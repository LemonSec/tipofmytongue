1
00:00:10,000 --> 00:00:11,519
today i want to tell you a little bit

2
00:00:11,519 --> 00:00:14,160
about some experiences that we're having

3
00:00:14,160 --> 00:00:16,400
with government agencies sharing data

4
00:00:16,400 --> 00:00:18,160
using differential privacy i'm going to

5
00:00:18,160 --> 00:00:20,720
talk in detail about a particular

6
00:00:20,720 --> 00:00:22,480
case study

7
00:00:22,480 --> 00:00:23,760
that involves the department of

8
00:00:23,760 --> 00:00:26,480
education and the college scorecard

9
00:00:26,480 --> 00:00:28,320
web application so

10
00:00:28,320 --> 00:00:31,599
college scorecard is a website it's it's

11
00:00:31,599 --> 00:00:32,800
up now

12
00:00:32,800 --> 00:00:34,480
hosted and created by the department of

13
00:00:34,480 --> 00:00:35,920
education

14
00:00:35,920 --> 00:00:38,000
with the goal of helping parents and

15
00:00:38,000 --> 00:00:39,680
students

16
00:00:39,680 --> 00:00:42,640
learn more about their college choices

17
00:00:42,640 --> 00:00:44,480
what degrees to

18
00:00:44,480 --> 00:00:46,879
major in and what how to invest their

19
00:00:46,879 --> 00:00:49,039
education dollars and so when this was

20
00:00:49,039 --> 00:00:51,440
first created and released

21
00:00:51,440 --> 00:00:54,480
it was hailed as a significant step in

22
00:00:54,480 --> 00:00:56,960
bringing more transparency to higher

23
00:00:56,960 --> 00:00:58,879
education

24
00:00:58,879 --> 00:01:00,399
and in particular

25
00:01:00,399 --> 00:01:01,920
what excited people about college

26
00:01:01,920 --> 00:01:04,640
scorecard i think was that there was a

27
00:01:04,640 --> 00:01:06,400
focus on

28
00:01:06,400 --> 00:01:08,400
the extent to which post-secondary

29
00:01:08,400 --> 00:01:10,080
institutions

30
00:01:10,080 --> 00:01:13,360
contributed to student economic success

31
00:01:13,360 --> 00:01:16,000
and economic success was measured

32
00:01:16,000 --> 00:01:19,119
by earnings data and so unlike all the

33
00:01:19,119 --> 00:01:20,880
other kind of college rankings that were

34
00:01:20,880 --> 00:01:22,400
out there at the time college scorecard

35
00:01:22,400 --> 00:01:24,640
distinguished itself by having

36
00:01:24,640 --> 00:01:27,360
statistics about

37
00:01:27,360 --> 00:01:29,840
earnings that people had a few years out

38
00:01:29,840 --> 00:01:32,400
from graduation for various degrees from

39
00:01:32,400 --> 00:01:34,159
various schools

40
00:01:34,159 --> 00:01:36,240
the challenge of making this a reality

41
00:01:36,240 --> 00:01:38,159
was that the department of education has

42
00:01:38,159 --> 00:01:40,640
no earnings data and earnings data is

43
00:01:40,640 --> 00:01:42,479
very difficult to get from people if you

44
00:01:42,479 --> 00:01:44,640
try to ask them how much they make

45
00:01:44,640 --> 00:01:46,159
you get a lot of bias and people don't

46
00:01:46,159 --> 00:01:47,840
like to tell you those things

47
00:01:47,840 --> 00:01:49,680
so to make this a reality

48
00:01:49,680 --> 00:01:51,040
the department of education had to

49
00:01:51,040 --> 00:01:53,119
interact with the internal revenue

50
00:01:53,119 --> 00:01:55,040
service the main taxing agency in the

51
00:01:55,040 --> 00:01:57,200
u.s and so this talk is largely about

52
00:01:57,200 --> 00:01:58,640
the relationship between these two

53
00:01:58,640 --> 00:02:01,520
agencies the department of education who

54
00:02:01,520 --> 00:02:04,560
wants to create a new data product the

55
00:02:04,560 --> 00:02:06,719
college scorecard and the internal

56
00:02:06,719 --> 00:02:08,160
revenue service that has lots of

57
00:02:08,160 --> 00:02:11,038
earnings data but has a burden to

58
00:02:11,038 --> 00:02:12,239
protect it

59
00:02:12,239 --> 00:02:13,680
so essentially what the department of

60
00:02:13,680 --> 00:02:16,400
educate education wanted to do was that

61
00:02:16,400 --> 00:02:17,200
they

62
00:02:17,200 --> 00:02:19,040
they knew the people that went to a

63
00:02:19,040 --> 00:02:21,680
certain school and got a certain degree

64
00:02:21,680 --> 00:02:22,959
they had their social security numbers

65
00:02:22,959 --> 00:02:24,800
in fact and so they wanted to send

66
00:02:24,800 --> 00:02:26,239
collections of social security numbers

67
00:02:26,239 --> 00:02:27,920
to the irs

68
00:02:27,920 --> 00:02:29,760
and then the irs would be able to look

69
00:02:29,760 --> 00:02:30,800
at

70
00:02:30,800 --> 00:02:32,640
uh earnings in a particular year this

71
00:02:32,640 --> 00:02:34,800
might be five years from graduation and

72
00:02:34,800 --> 00:02:36,959
so the irs could create this

73
00:02:36,959 --> 00:02:38,959
distribution that you see on the screen

74
00:02:38,959 --> 00:02:40,800
of earnings but each one of those dots

75
00:02:40,800 --> 00:02:43,040
is an individual's information this is

76
00:02:43,040 --> 00:02:45,120
highly sensitive and the irs does not

77
00:02:45,120 --> 00:02:47,680
release this this data so

78
00:02:47,680 --> 00:02:49,280
de-identifying the data and releasing is

79
00:02:49,280 --> 00:02:51,120
not an option but they might be willing

80
00:02:51,120 --> 00:02:52,800
to publish statistics about these

81
00:02:52,800 --> 00:02:54,959
distributions

82
00:02:54,959 --> 00:02:56,319
so what

83
00:02:56,319 --> 00:02:57,599
the department of ed was really asking

84
00:02:57,599 --> 00:03:01,519
for in as a response from the irs was a

85
00:03:01,519 --> 00:03:04,080
record that said okay for duke graduates

86
00:03:04,080 --> 00:03:06,080
with a bachelor's degree the median

87
00:03:06,080 --> 00:03:07,599
earnings were ninety three thousand

88
00:03:07,599 --> 00:03:09,120
dollars some number of years after

89
00:03:09,120 --> 00:03:11,280
graduation actually many more statistics

90
00:03:11,280 --> 00:03:13,200
than that because it was many schools

91
00:03:13,200 --> 00:03:15,360
and it i've i've kind of

92
00:03:15,360 --> 00:03:17,040
illustrated this as an interaction but

93
00:03:17,040 --> 00:03:19,040
it actually all happens in a big batch

94
00:03:19,040 --> 00:03:21,040
with debar and ved sending a bunch of

95
00:03:21,040 --> 00:03:22,640
social security numbers to irs and

96
00:03:22,640 --> 00:03:24,319
getting a big bunch of tables out

97
00:03:24,319 --> 00:03:27,280
actually millions of statistics

98
00:03:27,280 --> 00:03:28,799
so although these are aggregate

99
00:03:28,799 --> 00:03:31,280
statistics are highly sensitive and so

100
00:03:31,280 --> 00:03:33,440
it's worth digging into the motivations

101
00:03:33,440 --> 00:03:35,200
of these two parties as they interact

102
00:03:35,200 --> 00:03:37,920
and try to make this data exchange work

103
00:03:37,920 --> 00:03:41,040
um the irs has to comply with regulation

104
00:03:41,040 --> 00:03:43,840
u.s title 26 it's bound to protect all

105
00:03:43,840 --> 00:03:45,840
taxpayer information including the

106
00:03:45,840 --> 00:03:48,159
simple fact that a person did or did not

107
00:03:48,159 --> 00:03:50,319
file taxes as protected information and

108
00:03:50,319 --> 00:03:52,239
they want to avoid privacy attacks that

109
00:03:52,239 --> 00:03:53,680
could result from publishing many

110
00:03:53,680 --> 00:03:56,319
statistics about taxpayers

111
00:03:56,319 --> 00:03:57,840
the department of education on the other

112
00:03:57,840 --> 00:04:00,319
hand is mostly uh in the role of the

113
00:04:00,319 --> 00:04:02,000
data analyst they're the consumer of

114
00:04:02,000 --> 00:04:04,319
data they may be representing interests

115
00:04:04,319 --> 00:04:06,480
of downstream consumers of their data

116
00:04:06,480 --> 00:04:08,879
but ultimately they had some analytic

117
00:04:08,879 --> 00:04:11,519
tasks in mind and

118
00:04:11,519 --> 00:04:13,040
to the extent possible we hope that when

119
00:04:13,040 --> 00:04:14,480
they have analytic tasks in mind they're

120
00:04:14,480 --> 00:04:16,399
able to describe their standards for

121
00:04:16,399 --> 00:04:18,399
fitness of use how much data they need

122
00:04:18,399 --> 00:04:21,440
how accurately do they need it

123
00:04:21,440 --> 00:04:23,520
so ultimately the interaction between

124
00:04:23,520 --> 00:04:25,919
the data custodian and the data analyst

125
00:04:25,919 --> 00:04:28,560
is the familiar sort of privacy utility

126
00:04:28,560 --> 00:04:31,040
trade-off where the data custodian is

127
00:04:31,040 --> 00:04:33,360
primarily trying to manage risk

128
00:04:33,360 --> 00:04:34,960
privacy risk and do it and do it

129
00:04:34,960 --> 00:04:36,479
responsibly

130
00:04:36,479 --> 00:04:38,080
and would like to lower their risk

131
00:04:38,080 --> 00:04:39,680
essentially because that's

132
00:04:39,680 --> 00:04:41,440
you know their mission and the data

133
00:04:41,440 --> 00:04:44,000
analyst would like more data and more

134
00:04:44,000 --> 00:04:45,600
accurate data

135
00:04:45,600 --> 00:04:47,280
and so

136
00:04:47,280 --> 00:04:48,320
either

137
00:04:48,320 --> 00:04:51,040
side of the of this sort of spectrum is

138
00:04:51,040 --> 00:04:54,000
a bad outcome i think you'd agree um if

139
00:04:54,000 --> 00:04:54,840
we

140
00:04:54,840 --> 00:04:58,080
favor lower risk and and low data

141
00:04:58,080 --> 00:05:00,240
quality then we lose the you know the

142
00:05:00,240 --> 00:05:01,759
chance to gain insights from this

143
00:05:01,759 --> 00:05:03,120
earnings data

144
00:05:03,120 --> 00:05:05,440
and we may come to incorrect conclusions

145
00:05:05,440 --> 00:05:07,520
and we don't have education educational

146
00:05:07,520 --> 00:05:10,320
transparency the end goal and if we

147
00:05:10,320 --> 00:05:12,560
favor higher data quality at higher risk

148
00:05:12,560 --> 00:05:14,320
we could have private privacy breaches

149
00:05:14,320 --> 00:05:16,800
violations of regulation and so on so

150
00:05:16,800 --> 00:05:18,479
the question is can we find a happy

151
00:05:18,479 --> 00:05:20,400
medium here

152
00:05:20,400 --> 00:05:21,919
so the college scorecard has been going

153
00:05:21,919 --> 00:05:25,840
on for a number of years and um

154
00:05:25,840 --> 00:05:27,840
you know a happy media some some medium

155
00:05:27,840 --> 00:05:29,680
has been found i think a relatively

156
00:05:29,680 --> 00:05:31,440
happy medium but it's evolved over time

157
00:05:31,440 --> 00:05:32,320
so

158
00:05:32,320 --> 00:05:35,840
in from 2015 to 2019

159
00:05:35,840 --> 00:05:37,600
irs was using

160
00:05:37,600 --> 00:05:39,280
what i typically call informal privacy

161
00:05:39,280 --> 00:05:41,919
protection so they were

162
00:05:41,919 --> 00:05:45,440
not just publishing raw statistics but

163
00:05:45,440 --> 00:05:47,360
to protect to add more privacy

164
00:05:47,360 --> 00:05:49,039
protection to those statistics they were

165
00:05:49,039 --> 00:05:51,520
distorting the income medians

166
00:05:51,520 --> 00:05:53,520
and they were also suppressing a lot of

167
00:05:53,520 --> 00:05:56,319
data which means that if

168
00:05:56,319 --> 00:05:58,560
the department of ed was trying was

169
00:05:58,560 --> 00:06:00,080
asking for

170
00:06:00,080 --> 00:06:02,240
uh income statistics about a group that

171
00:06:02,240 --> 00:06:04,400
was very small so for example at duke

172
00:06:04,400 --> 00:06:06,400
there just weren't that many majors in a

173
00:06:06,400 --> 00:06:08,000
certain year

174
00:06:08,000 --> 00:06:09,840
they would just not publish information

175
00:06:09,840 --> 00:06:11,199
about that group if it was too small

176
00:06:11,199 --> 00:06:13,840
because it was considered too disclosed

177
00:06:13,840 --> 00:06:16,000
but after 2019

178
00:06:16,000 --> 00:06:18,560
irs switched to differential privacy and

179
00:06:18,560 --> 00:06:20,720
so we've done two releases in 2020 and

180
00:06:20,720 --> 00:06:22,880
2021 with them and we're working on a

181
00:06:22,880 --> 00:06:25,680
third and so i'd like to explain you

182
00:06:25,680 --> 00:06:27,280
know some of the reasons behind this

183
00:06:27,280 --> 00:06:28,720
shift

184
00:06:28,720 --> 00:06:31,039
so as the years went on and when they

185
00:06:31,039 --> 00:06:33,039
started in 2015 the data was actually

186
00:06:33,039 --> 00:06:34,960
that the irs released was actually

187
00:06:34,960 --> 00:06:36,720
fairly coarse grained it was at the

188
00:06:36,720 --> 00:06:39,680
institution level but as time went on

189
00:06:39,680 --> 00:06:41,280
in part because people were excited

190
00:06:41,280 --> 00:06:42,960
about the potential of of the college

191
00:06:42,960 --> 00:06:44,880
scorecard application they asked for

192
00:06:44,880 --> 00:06:46,639
more and more and more detailed

193
00:06:46,639 --> 00:06:48,800
information going from institution level

194
00:06:48,800 --> 00:06:51,039
statistics to program level

195
00:06:51,039 --> 00:06:54,000
doing breakouts of these

196
00:06:54,000 --> 00:06:56,080
of these populations by gender and pell

197
00:06:56,080 --> 00:06:58,400
status which is a proxy for

198
00:06:58,400 --> 00:07:00,880
low income status and asking for more

199
00:07:00,880 --> 00:07:02,400
information about the distributions not

200
00:07:02,400 --> 00:07:05,280
just medians but but 25th and 75th

201
00:07:05,280 --> 00:07:07,039
percentile and some other statistics

202
00:07:07,039 --> 00:07:09,120
about

203
00:07:09,120 --> 00:07:11,280
poverty thresholds

204
00:07:11,280 --> 00:07:13,680
so this starts to pose really tough

205
00:07:13,680 --> 00:07:15,680
questions for the data custodian

206
00:07:15,680 --> 00:07:18,560
how much additional risk is the irs

207
00:07:18,560 --> 00:07:20,960
incurring as they publish more and more

208
00:07:20,960 --> 00:07:22,319
data

209
00:07:22,319 --> 00:07:24,560
and as they publish data year by year

210
00:07:24,560 --> 00:07:26,800
and they consider cohorts of students

211
00:07:26,800 --> 00:07:28,639
that are overlapping so an individual's

212
00:07:28,639 --> 00:07:30,080
information can appear in more than one

213
00:07:30,080 --> 00:07:32,800
cohort and how should they respond how

214
00:07:32,800 --> 00:07:35,280
do they decide whether to suppress more

215
00:07:35,280 --> 00:07:38,319
data or to distort the medians

216
00:07:38,319 --> 00:07:40,560
and other quantiles that they're that

217
00:07:40,560 --> 00:07:42,880
they're releasing more this is these

218
00:07:42,880 --> 00:07:44,400
questions are almost impossible to

219
00:07:44,400 --> 00:07:46,160
answer in a principled way

220
00:07:46,160 --> 00:07:48,319
and so that's a lot of what motivated

221
00:07:48,319 --> 00:07:50,319
differential privacy

222
00:07:50,319 --> 00:07:51,039
uh

223
00:07:51,039 --> 00:07:52,960
the need for the data custodian to

224
00:07:52,960 --> 00:07:55,440
understand incremental risk and manage

225
00:07:55,440 --> 00:07:56,879
it more

226
00:07:56,879 --> 00:07:59,039
in a more principled manner

227
00:07:59,039 --> 00:08:00,080
okay so a little background on

228
00:08:00,080 --> 00:08:01,680
differential privacy i know most people

229
00:08:01,680 --> 00:08:03,199
are familiar with it but an informal

230
00:08:03,199 --> 00:08:05,120
definition of differential privacy is

231
00:08:05,120 --> 00:08:07,759
that it is a standard for computations

232
00:08:07,759 --> 00:08:10,240
on data that strictly limits the

233
00:08:10,240 --> 00:08:11,599
personal information that could be

234
00:08:11,599 --> 00:08:13,840
revealed by the output so what i've

235
00:08:13,840 --> 00:08:15,039
shown here

236
00:08:15,039 --> 00:08:18,479
in the dotted green box is a program

237
00:08:18,479 --> 00:08:21,360
that runs on data and produces an output

238
00:08:21,360 --> 00:08:23,199
there's a little dice a die in there

239
00:08:23,199 --> 00:08:25,039
because these programs are randomized

240
00:08:25,039 --> 00:08:26,560
and there's also a special parameter

241
00:08:26,560 --> 00:08:29,759
epsilon which is a formal bound on the

242
00:08:29,759 --> 00:08:33,519
worst case privacy loss that can result

243
00:08:33,519 --> 00:08:35,919
from releasing an output computed by

244
00:08:35,919 --> 00:08:38,559
this program so that that

245
00:08:38,559 --> 00:08:40,880
epsilon privacy loss bound you can think

246
00:08:40,880 --> 00:08:41,839
of as

247
00:08:41,839 --> 00:08:44,240
uh being carried along with the output

248
00:08:44,240 --> 00:08:45,839
because anyone you share this output

249
00:08:45,839 --> 00:08:48,640
with the uh that's the sort of privacy

250
00:08:48,640 --> 00:08:49,760
risk uh

251
00:08:49,760 --> 00:08:51,600
you you bear

252
00:08:51,600 --> 00:08:52,880
so it's a rigorous guarantee of

253
00:08:52,880 --> 00:08:54,560
protection every individual is protected

254
00:08:54,560 --> 00:08:56,480
every attribute of every

255
00:08:56,480 --> 00:08:58,720
individual and the guarantee doesn't

256
00:08:58,720 --> 00:09:00,640
rely on assumptions about computational

257
00:09:00,640 --> 00:09:02,560
power or background knowledge and

258
00:09:02,560 --> 00:09:04,160
therefore we like to say that it resists

259
00:09:04,160 --> 00:09:05,920
both current attacks and is likely to

260
00:09:05,920 --> 00:09:08,399
resist future attacks

261
00:09:08,399 --> 00:09:09,760
okay so

262
00:09:09,760 --> 00:09:11,440
in our engagements with government

263
00:09:11,440 --> 00:09:13,760
agencies you know this workflow for

264
00:09:13,760 --> 00:09:15,200
deploying differential privacy is

265
00:09:15,200 --> 00:09:18,399
emerging where we elicit requirements

266
00:09:18,399 --> 00:09:20,240
like what are they trying to learn about

267
00:09:20,240 --> 00:09:21,680
from a data set

268
00:09:21,680 --> 00:09:24,000
we put together a prototype algorithm

269
00:09:24,000 --> 00:09:26,080
these algorithms have parameters lots of

270
00:09:26,080 --> 00:09:28,240
parameters sometimes and so then there's

271
00:09:28,240 --> 00:09:31,120
a phase of exploration and negotiation

272
00:09:31,120 --> 00:09:33,360
which usually involves both the data

273
00:09:33,360 --> 00:09:35,360
custodian and the

274
00:09:35,360 --> 00:09:38,240
data analyst or the consumer of the data

275
00:09:38,240 --> 00:09:40,160
and that's an iterative process that i'm

276
00:09:40,160 --> 00:09:42,320
going to describe next and that happens

277
00:09:42,320 --> 00:09:45,360
before the end result of that process is

278
00:09:45,360 --> 00:09:47,519
kind of come to a final decision about

279
00:09:47,519 --> 00:09:49,680
this trade-off between

280
00:09:49,680 --> 00:09:53,279
risk privacy risk and and accuracy and

281
00:09:53,279 --> 00:09:56,000
and degree of uh detail in what's

282
00:09:56,000 --> 00:09:57,360
published

283
00:09:57,360 --> 00:09:59,760
okay so let's look at this kind of

284
00:09:59,760 --> 00:10:02,079
mock-up of an interface

285
00:10:02,079 --> 00:10:04,640
that describes some of the data release

286
00:10:04,640 --> 00:10:06,959
levers so this is what we would like to

287
00:10:06,959 --> 00:10:08,880
put in front of the data custodian and

288
00:10:08,880 --> 00:10:11,839
the and the data analyst and help them

289
00:10:11,839 --> 00:10:13,600
to make decisions about it so at the

290
00:10:13,600 --> 00:10:15,360
very top here i have a release

291
00:10:15,360 --> 00:10:17,600
description so this is this is the part

292
00:10:17,600 --> 00:10:19,760
of of an interface that would describe

293
00:10:19,760 --> 00:10:21,600
how detailed are the statistics that are

294
00:10:21,600 --> 00:10:23,120
going to get published and so there's

295
00:10:23,120 --> 00:10:25,360
some check boxes here and there's some

296
00:10:25,360 --> 00:10:26,880
things grayed out or not grayed out

297
00:10:26,880 --> 00:10:28,640
about whether you're publishing more

298
00:10:28,640 --> 00:10:30,399
detailed quantiles or less detailed

299
00:10:30,399 --> 00:10:31,600
quantiles

300
00:10:31,600 --> 00:10:33,120
then there's an algorithm so there's a

301
00:10:33,120 --> 00:10:34,399
differentially private algorithm that is

302
00:10:34,399 --> 00:10:36,399
going to

303
00:10:36,399 --> 00:10:38,399
compute private statistics and that

304
00:10:38,399 --> 00:10:41,519
algorithm has parameters both

305
00:10:41,519 --> 00:10:43,760
some parameters that have to do with

306
00:10:43,760 --> 00:10:45,920
epsilon shares how the epsilon budget is

307
00:10:45,920 --> 00:10:48,320
distributed and some other privacy

308
00:10:48,320 --> 00:10:51,040
semantics related uh parameters like the

309
00:10:51,040 --> 00:10:53,120
total epsilon budget and maybe some

310
00:10:53,120 --> 00:10:56,640
subtleties about user contributions

311
00:10:56,640 --> 00:10:58,320
and what we want to put next to that so

312
00:10:58,320 --> 00:11:00,720
so for any setting or or setting of the

313
00:11:00,720 --> 00:11:03,440
sort of knobs or levers on the left-hand

314
00:11:03,440 --> 00:11:06,079
side we could run a bunch of

315
00:11:06,079 --> 00:11:08,720
scenarios and produce

316
00:11:08,720 --> 00:11:11,040
measures that describe fitness for use

317
00:11:11,040 --> 00:11:12,560
and these are often application

318
00:11:12,560 --> 00:11:14,880
dependent dependent on the particular

319
00:11:14,880 --> 00:11:17,120
concerns of the data analyst but we want

320
00:11:17,120 --> 00:11:18,560
to present the things that matter to

321
00:11:18,560 --> 00:11:21,279
them in terms of error metrics so for

322
00:11:21,279 --> 00:11:23,120
any setting of the knobs on the left you

323
00:11:23,120 --> 00:11:26,000
could look at error displayed one way

324
00:11:26,000 --> 00:11:27,440
one way or another in this case we're

325
00:11:27,440 --> 00:11:29,839
looking at bins of rel of relative error

326
00:11:29,839 --> 00:11:31,519
for groups of students and their earning

327
00:11:31,519 --> 00:11:33,920
statistics

328
00:11:33,920 --> 00:11:36,079
okay so before we get into tough

329
00:11:36,079 --> 00:11:38,399
negotiations one thing to note is that

330
00:11:38,399 --> 00:11:40,800
there are some things you can do in this

331
00:11:40,800 --> 00:11:44,399
space that that both there are wins for

332
00:11:44,399 --> 00:11:46,320
both the custodian and the analyst first

333
00:11:46,320 --> 00:11:47,440
of all

334
00:11:47,440 --> 00:11:49,519
we want good accuracy and we want good

335
00:11:49,519 --> 00:11:51,519
privacy protection and if we can if we

336
00:11:51,519 --> 00:11:54,399
can do the analysis on more data we get

337
00:11:54,399 --> 00:11:56,720
both of those things the other thing we

338
00:11:56,720 --> 00:11:57,680
can do is make sure that our

339
00:11:57,680 --> 00:11:59,120
differentially private algorithms are

340
00:11:59,120 --> 00:12:01,600
not suboptimal so there are in general

341
00:12:01,600 --> 00:12:04,480
many ways to do the same computation in

342
00:12:04,480 --> 00:12:06,560
a differentially private way some

343
00:12:06,560 --> 00:12:09,519
deliver more error than others and so we

344
00:12:09,519 --> 00:12:11,920
want to find algorithms that perform

345
00:12:11,920 --> 00:12:13,760
perform well and if we can iterate a

346
00:12:13,760 --> 00:12:15,680
little bit in on those things then

347
00:12:15,680 --> 00:12:17,920
there's no uh sort of conflict between

348
00:12:17,920 --> 00:12:19,600
the interests of the custodian and the

349
00:12:19,600 --> 00:12:21,120
analyst

350
00:12:21,120 --> 00:12:21,920
but

351
00:12:21,920 --> 00:12:23,200
when we get beyond those things and

352
00:12:23,200 --> 00:12:24,560
we've settled on

353
00:12:24,560 --> 00:12:25,920
an algorithm and we have a data source

354
00:12:25,920 --> 00:12:27,760
it's often often not possible to get

355
00:12:27,760 --> 00:12:30,160
more data

356
00:12:30,160 --> 00:12:32,240
then we look at the release description

357
00:12:32,240 --> 00:12:35,040
and the analysts can play around with

358
00:12:35,040 --> 00:12:36,800
this they can decide whether they want

359
00:12:36,800 --> 00:12:38,240
to release more data i mean they

360
00:12:38,240 --> 00:12:39,839
typically want to release as much as

361
00:12:39,839 --> 00:12:42,480
they can but what they can look at here

362
00:12:42,480 --> 00:12:44,959
by reducing the sort of granularity of

363
00:12:44,959 --> 00:12:46,880
the data they're releasing the error may

364
00:12:46,880 --> 00:12:48,160
improve

365
00:12:48,160 --> 00:12:50,959
that assumes that we have a fixed set

366
00:12:50,959 --> 00:12:52,959
settings for the privacy semantics so

367
00:12:52,959 --> 00:12:54,880
that's where the data custodian is

368
00:12:54,880 --> 00:12:57,600
setting their risk tolerance

369
00:12:57,600 --> 00:12:59,600
and then there are some other parameters

370
00:12:59,600 --> 00:13:01,920
algorithm parameters that

371
00:13:01,920 --> 00:13:04,320
can impact the accuracy displayed over

372
00:13:04,320 --> 00:13:05,120
here

373
00:13:05,120 --> 00:13:07,040
but aren't changing the risk tolerance

374
00:13:07,040 --> 00:13:08,880
or what's being published they're just

375
00:13:08,880 --> 00:13:10,560
kind of like distributing error in

376
00:13:10,560 --> 00:13:11,760
different ways

377
00:13:11,760 --> 00:13:14,000
but that can have a big impact and so we

378
00:13:14,000 --> 00:13:15,279
have

379
00:13:15,279 --> 00:13:17,839
states in the negotiation where the

380
00:13:17,839 --> 00:13:19,839
analyst may say i really want to publish

381
00:13:19,839 --> 00:13:21,680
all this stuff but i'm looking at the

382
00:13:21,680 --> 00:13:24,000
error and it's it's it's higher than i

383
00:13:24,000 --> 00:13:25,760
want it to be it doesn't meet my fitness

384
00:13:25,760 --> 00:13:27,920
for use criteria so the first thing that

385
00:13:27,920 --> 00:13:29,920
analysts will probably say is can't you

386
00:13:29,920 --> 00:13:30,880
just

387
00:13:30,880 --> 00:13:32,560
you know make the error better whatever

388
00:13:32,560 --> 00:13:34,000
way you can which is to the data

389
00:13:34,000 --> 00:13:36,959
custodian accepting more risk and

390
00:13:36,959 --> 00:13:38,880
raising the epsilon and the data

391
00:13:38,880 --> 00:13:40,639
custodian may say no i'm not willing to

392
00:13:40,639 --> 00:13:42,079
do that you need to look at other

393
00:13:42,079 --> 00:13:44,240
options for lowering the error and

394
00:13:44,240 --> 00:13:46,000
meeting your fitness for use criteria

395
00:13:46,000 --> 00:13:48,480
and so that is what might force them to

396
00:13:48,480 --> 00:13:51,279
to consider releasing less data or

397
00:13:51,279 --> 00:13:52,800
distributing the data in different ways

398
00:13:52,800 --> 00:13:54,160
through setting the parameters so

399
00:13:54,160 --> 00:13:55,600
there's lots more to say about this

400
00:13:55,600 --> 00:13:57,360
picture that i won't have time to say

401
00:13:57,360 --> 00:13:59,920
but we can maybe talk um in discussion

402
00:13:59,920 --> 00:14:01,440
later

403
00:14:01,440 --> 00:14:05,120
so outcomes um when the irs switched to

404
00:14:05,120 --> 00:14:06,720
differential privacy

405
00:14:06,720 --> 00:14:10,079
we were able to to um

406
00:14:10,079 --> 00:14:11,360
to

407
00:14:11,360 --> 00:14:13,440
release more earning statistics than

408
00:14:13,440 --> 00:14:15,600
previously with about the comparable

409
00:14:15,600 --> 00:14:16,880
accuracy

410
00:14:16,880 --> 00:14:18,639
um

411
00:14:18,639 --> 00:14:20,560
and we've given irs sort of more

412
00:14:20,560 --> 00:14:22,959
assurance and and better risk management

413
00:14:22,959 --> 00:14:24,480
i mean i think they sleep better at

414
00:14:24,480 --> 00:14:27,040
night because they have a principled uh

415
00:14:27,040 --> 00:14:28,800
basis on which to make decisions about

416
00:14:28,800 --> 00:14:30,880
what to publish and we've streamlined

417
00:14:30,880 --> 00:14:33,760
communication by providing tools

418
00:14:33,760 --> 00:14:35,920
similar to the one on the last slide

419
00:14:35,920 --> 00:14:38,240
that allows exploration and iteration on

420
00:14:38,240 --> 00:14:41,040
this negotiation process

421
00:14:41,040 --> 00:14:42,720
i'll end with a few conclusions and

422
00:14:42,720 --> 00:14:43,760
challenges

423
00:14:43,760 --> 00:14:46,160
i think what i'm talking about today is

424
00:14:46,160 --> 00:14:48,880
actually sort of fundamental to to part

425
00:14:48,880 --> 00:14:50,079
of what the model of differential

426
00:14:50,079 --> 00:14:52,639
privacy teaches us which is that

427
00:14:52,639 --> 00:14:55,120
uh and and the research that that led to

428
00:14:55,120 --> 00:14:57,360
differential privacy which is that

429
00:14:57,360 --> 00:14:59,199
we have we should

430
00:14:59,199 --> 00:15:02,560
carefully consider the uses we want uh

431
00:15:02,560 --> 00:15:04,959
we want we have in mind for data and

432
00:15:04,959 --> 00:15:07,040
fitness for use standards we should have

433
00:15:07,040 --> 00:15:09,040
customized data products rather than

434
00:15:09,040 --> 00:15:10,800
universal data products that where we

435
00:15:10,800 --> 00:15:12,399
can publish where we think we might be

436
00:15:12,399 --> 00:15:14,480
able to publish things that sort of work

437
00:15:14,480 --> 00:15:16,880
for all analyses we know from the theory

438
00:15:16,880 --> 00:15:19,600
of differential privacy that that

439
00:15:19,600 --> 00:15:21,440
we can't achieve

440
00:15:21,440 --> 00:15:22,880
really achieve that that will open

441
00:15:22,880 --> 00:15:25,519
ourselves up to attacks and so this is a

442
00:15:25,519 --> 00:15:27,440
move from from universal data products

443
00:15:27,440 --> 00:15:28,880
to customized data products and i think

444
00:15:28,880 --> 00:15:31,839
that entails this step where

445
00:15:31,839 --> 00:15:33,759
we help the analyst

446
00:15:33,759 --> 00:15:35,920
uh look carefully at sort of the

447
00:15:35,920 --> 00:15:37,680
frontier of what's possible with the

448
00:15:37,680 --> 00:15:39,519
privacy technology and with with the

449
00:15:39,519 --> 00:15:42,880
risk tolerance of the data custodian

450
00:15:42,880 --> 00:15:44,880
calculating and communicating error to

451
00:15:44,880 --> 00:15:47,600
analysts and stakeholders is challenging

452
00:15:47,600 --> 00:15:50,160
um and in fact it could incur its own

453
00:15:50,160 --> 00:15:52,560
privacy loss so i've glossed over that

454
00:15:52,560 --> 00:15:54,880
topic but we could come back to it in

455
00:15:54,880 --> 00:15:56,320
discussion later

456
00:15:56,320 --> 00:15:57,680
and lastly

457
00:15:57,680 --> 00:15:58,959
we've learned actually that data

458
00:15:58,959 --> 00:16:00,800
consumers don't want to see high error

459
00:16:00,800 --> 00:16:02,639
outputs and one interesting thing that

460
00:16:02,639 --> 00:16:05,199
happened in our engagement with irs is

461
00:16:05,199 --> 00:16:06,880
that they used to suppress for privacy

462
00:16:06,880 --> 00:16:09,680
protection and now when they adopted

463
00:16:09,680 --> 00:16:11,199
differential proxy they no longer need

464
00:16:11,199 --> 00:16:12,880
to suppress data for privacy protection

465
00:16:12,880 --> 00:16:14,079
because there's noise in the data that

466
00:16:14,079 --> 00:16:17,040
protects but they don't like super noisy

467
00:16:17,040 --> 00:16:19,199
observations so they will suppress those

468
00:16:19,199 --> 00:16:21,519
for data quality reasons so that was an

469
00:16:21,519 --> 00:16:23,040
interesting finding even when we

470
00:16:23,040 --> 00:16:24,959
quantified error for them they often

471
00:16:24,959 --> 00:16:26,880
don't want to show highly noisy

472
00:16:26,880 --> 00:16:29,680
observations to end users

473
00:16:29,680 --> 00:16:31,120
i will end there

474
00:16:31,120 --> 00:16:33,360
and happy to take a few questions if

475
00:16:33,360 --> 00:16:36,680
there are some

476
00:16:41,920 --> 00:16:44,000
you

