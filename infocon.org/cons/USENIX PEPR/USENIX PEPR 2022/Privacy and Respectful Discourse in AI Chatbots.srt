1
00:00:09,840 --> 00:00:12,000
my name is jethi dev i will be

2
00:00:12,000 --> 00:00:13,440
presenting our

3
00:00:13,440 --> 00:00:15,759
work on privacy and respectful discourse

4
00:00:15,759 --> 00:00:18,160
in ai chatbots done in collaboration

5
00:00:18,160 --> 00:00:20,960
with dr tatiana ringenberg and dr lg

6
00:00:20,960 --> 00:00:23,199
incamp who you met earlier

7
00:00:23,199 --> 00:00:26,240
at indiana university bloomington

8
00:00:26,240 --> 00:00:28,000
so i want to start by scoping what we

9
00:00:28,000 --> 00:00:29,920
mean by chatbots we are looking at

10
00:00:29,920 --> 00:00:31,679
chatbots which people engage with

11
00:00:31,679 --> 00:00:34,719
socially called social chatbots examples

12
00:00:34,719 --> 00:00:37,120
would be replica and cleverbot which

13
00:00:37,120 --> 00:00:39,760
total over 26 million users

14
00:00:39,760 --> 00:00:42,480
we have seen a 35 hike in usage for such

15
00:00:42,480 --> 00:00:45,120
bots especially during the pandemic

16
00:00:45,120 --> 00:00:47,280
since people were more socially isolated

17
00:00:47,280 --> 00:00:50,079
and wanted to be able to talk to someone

18
00:00:50,079 --> 00:00:52,239
this can also include bots like pandora

19
00:00:52,239 --> 00:00:55,120
bots insta bots mitsuko and sometimes

20
00:00:55,120 --> 00:00:57,120
assistants like alexa and google

21
00:00:57,120 --> 00:00:59,280
assistant

22
00:00:59,280 --> 00:01:00,879
there is ongoing research around

23
00:01:00,879 --> 00:01:02,960
chatbots looking at biases and gender

24
00:01:02,960 --> 00:01:05,680
design and how bots can reinforce gender

25
00:01:05,680 --> 00:01:07,119
bias

26
00:01:07,119 --> 00:01:08,560
there's also a couple of privacy

27
00:01:08,560 --> 00:01:10,159
breaches that have come up in the past

28
00:01:10,159 --> 00:01:11,280
few years

29
00:01:11,280 --> 00:01:13,760
most noticeably being the delta breach

30
00:01:13,760 --> 00:01:15,280
where the assistant bought exposed

31
00:01:15,280 --> 00:01:17,360
financial information

32
00:01:17,360 --> 00:01:19,439
about their customers

33
00:01:19,439 --> 00:01:21,840
the final bucket of related research

34
00:01:21,840 --> 00:01:22,720
area

35
00:01:22,720 --> 00:01:25,040
is around manipulation where social bots

36
00:01:25,040 --> 00:01:27,280
can deploy manipulative tactics to

37
00:01:27,280 --> 00:01:29,840
encourage violent behavior among users

38
00:01:29,840 --> 00:01:32,479
who use them

39
00:01:32,479 --> 00:01:34,479
so we wanted to look at two things to

40
00:01:34,479 --> 00:01:36,720
what extent can privacy become necessary

41
00:01:36,720 --> 00:01:38,799
in such bots is there information being

42
00:01:38,799 --> 00:01:40,880
shared that needs to be protected and

43
00:01:40,880 --> 00:01:43,040
second how do users manage their

44
00:01:43,040 --> 00:01:45,439
information in such interactions

45
00:01:45,439 --> 00:01:46,799
given that there are no standard

46
00:01:46,799 --> 00:01:47,920
practice

47
00:01:47,920 --> 00:01:51,280
privacy practices in bots

48
00:01:51,280 --> 00:01:53,200
so our focus was on data types and

49
00:01:53,200 --> 00:01:55,920
themes leading to information disclosure

50
00:01:55,920 --> 00:01:57,920
we looked if users share sensitive

51
00:01:57,920 --> 00:02:00,320
information about themselves as defined

52
00:02:00,320 --> 00:02:01,759
by the general data protection

53
00:02:01,759 --> 00:02:02,960
regulations

54
00:02:02,960 --> 00:02:04,719
and if there were conversational themes

55
00:02:04,719 --> 00:02:06,399
that indicated disclosure or

56
00:02:06,399 --> 00:02:08,560
non-disclosure

57
00:02:08,560 --> 00:02:11,120
we did a thematic analysis of 37 chat

58
00:02:11,120 --> 00:02:13,920
logs from a 2019 sample of cleverbot

59
00:02:13,920 --> 00:02:16,480
data from the numbers that we see on the

60
00:02:16,480 --> 00:02:18,720
right users spoke to clever bought more

61
00:02:18,720 --> 00:02:21,040
in words and short phrases than complete

62
00:02:21,040 --> 00:02:22,879
sentences as they would with another

63
00:02:22,879 --> 00:02:24,879
person

64
00:02:24,879 --> 00:02:27,120
moving on to the data types we looked at

65
00:02:27,120 --> 00:02:29,120
users responses to sensitive data

66
00:02:29,120 --> 00:02:31,840
category requests we use the gdpr to

67
00:02:31,840 --> 00:02:33,760
identify what are the sensitive data

68
00:02:33,760 --> 00:02:35,680
categories

69
00:02:35,680 --> 00:02:37,519
users shared political opinions

70
00:02:37,519 --> 00:02:40,000
affiliations and choices they also

71
00:02:40,000 --> 00:02:41,599
shared their religious beliefs and

72
00:02:41,599 --> 00:02:43,680
practices

73
00:02:43,680 --> 00:02:45,680
they spoke about sexual orientation they

74
00:02:45,680 --> 00:02:48,400
spoke about age and gender groups

75
00:02:48,400 --> 00:02:50,000
they also spoke about approximate

76
00:02:50,000 --> 00:02:52,400
location so in this case users tried to

77
00:02:52,400 --> 00:02:54,879
reciprocate cleverbot's location

78
00:02:54,879 --> 00:02:56,720
by sharing information about their own

79
00:02:56,720 --> 00:02:58,959
location

80
00:02:58,959 --> 00:03:01,120
the biggest category of data that we did

81
00:03:01,120 --> 00:03:03,120
see was health so users shared

82
00:03:03,120 --> 00:03:05,120
information about their health including

83
00:03:05,120 --> 00:03:07,760
if they had depression anxiety or major

84
00:03:07,760 --> 00:03:10,080
health concerns that happened due to the

85
00:03:10,080 --> 00:03:12,080
nature of the conversation

86
00:03:12,080 --> 00:03:14,480
we did not find any data around

87
00:03:14,480 --> 00:03:16,800
gdpr categories like racial or ethnic

88
00:03:16,800 --> 00:03:17,920
origin

89
00:03:17,920 --> 00:03:20,319
trade union membership genetic data and

90
00:03:20,319 --> 00:03:23,280
biometric data

91
00:03:24,000 --> 00:03:25,840
the second thing we looked at was data

92
00:03:25,840 --> 00:03:28,080
themes that may lead to disclosure we

93
00:03:28,080 --> 00:03:29,599
initially began with a discursive

94
00:03:29,599 --> 00:03:31,920
psychology approach and then tested the

95
00:03:31,920 --> 00:03:33,920
results by taking a thematic analysis

96
00:03:33,920 --> 00:03:35,280
method

97
00:03:35,280 --> 00:03:37,280
a thematic analysis was a result of

98
00:03:37,280 --> 00:03:39,200
identification of words using

99
00:03:39,200 --> 00:03:40,879
qualitative coding

100
00:03:40,879 --> 00:03:43,040
discussion psychology depends on

101
00:03:43,040 --> 00:03:45,120
understanding the emotion and intent of

102
00:03:45,120 --> 00:03:48,239
talk however in the analysis of chatbot

103
00:03:48,239 --> 00:03:50,480
interactions people may be joking or

104
00:03:50,480 --> 00:03:53,200
insincere so these results well based on

105
00:03:53,200 --> 00:03:55,040
thematic analysis

106
00:03:55,040 --> 00:03:57,360
also align with previous results in

107
00:03:57,360 --> 00:03:59,840
terms of content but we cannot assert

108
00:03:59,840 --> 00:04:02,239
any findings about actual emotion or

109
00:04:02,239 --> 00:04:04,640
rely on accuracy in terms of emotion in

110
00:04:04,640 --> 00:04:06,959
the data

111
00:04:06,959 --> 00:04:08,959
so we saw instances of manipulative

112
00:04:08,959 --> 00:04:11,360
relationships fostering where users and

113
00:04:11,360 --> 00:04:13,040
cleverbot tried to establish a

114
00:04:13,040 --> 00:04:14,879
relationship where they would share

115
00:04:14,879 --> 00:04:17,279
information about each other

116
00:04:17,279 --> 00:04:19,519
users engaged in polarizing discussions

117
00:04:19,519 --> 00:04:21,680
and assertions especially when it came

118
00:04:21,680 --> 00:04:24,080
to religious and political beliefs often

119
00:04:24,080 --> 00:04:26,960
being argumentative

120
00:04:26,960 --> 00:04:29,199
sharing by users was majorly driven by

121
00:04:29,199 --> 00:04:31,440
comfort so users shared information

122
00:04:31,440 --> 00:04:33,040
about themselves when they were

123
00:04:33,040 --> 00:04:36,479
comfortable talking with cleverbot

124
00:04:36,479 --> 00:04:38,800
we also saw that users tested cleverbot

125
00:04:38,800 --> 00:04:41,120
on the extent of information it collects

126
00:04:41,120 --> 00:04:42,560
and whether it was being used for

127
00:04:42,560 --> 00:04:44,400
surveillance purposes

128
00:04:44,400 --> 00:04:46,639
for example in one instance

129
00:04:46,639 --> 00:04:48,560
the user asked cleverbot if their

130
00:04:48,560 --> 00:04:50,479
conversation was being monitored by the

131
00:04:50,479 --> 00:04:52,720
fbi

132
00:04:52,720 --> 00:04:54,479
users and cleverbots engaged in

133
00:04:54,479 --> 00:04:56,560
repetitive phrases to somehow make each

134
00:04:56,560 --> 00:04:58,479
other understand what information they

135
00:04:58,479 --> 00:05:00,160
were seeking

136
00:05:00,160 --> 00:05:02,560
and finally users who did not want to

137
00:05:02,560 --> 00:05:04,720
share specific identifiable information

138
00:05:04,720 --> 00:05:06,880
about themselves simply refuse to

139
00:05:06,880 --> 00:05:08,840
interact with the with the

140
00:05:08,840 --> 00:05:10,479
chatbot

141
00:05:10,479 --> 00:05:12,720
given that we saw exchange of sensitive

142
00:05:12,720 --> 00:05:15,360
if not identifiable information there is

143
00:05:15,360 --> 00:05:17,680
a need to effectively flag and interact

144
00:05:17,680 --> 00:05:19,520
with sensitive data and in turn

145
00:05:19,520 --> 00:05:21,680
vulnerable users

146
00:05:21,680 --> 00:05:23,680
there are also certain privacy by design

147
00:05:23,680 --> 00:05:25,680
principles that we can implement to

148
00:05:25,680 --> 00:05:28,800
protect user choice

149
00:05:28,800 --> 00:05:30,880
in order to identify vulnerable users

150
00:05:30,880 --> 00:05:32,560
sharing sensitive information about

151
00:05:32,560 --> 00:05:34,560
themselves we need a repository of

152
00:05:34,560 --> 00:05:36,560
mental health markers and abuse

153
00:05:36,560 --> 00:05:38,639
indicators that would notice such

154
00:05:38,639 --> 00:05:40,800
sensitive data types and guide users

155
00:05:40,800 --> 00:05:44,720
towards a more appropriate response

156
00:05:45,360 --> 00:05:47,360
so the following slides

157
00:05:47,360 --> 00:05:49,120
before i proceed further i would like to

158
00:05:49,120 --> 00:05:50,240
say that

159
00:05:50,240 --> 00:05:52,080
say that the following slides can be

160
00:05:52,080 --> 00:05:54,560
triggering so um if you would like to

161
00:05:54,560 --> 00:05:56,080
take a moment to

162
00:05:56,080 --> 00:05:58,639
disconnect you can do that so i'll wait

163
00:05:58,639 --> 00:06:02,280
a couple of seconds

164
00:06:06,479 --> 00:06:08,880
all right

165
00:06:11,759 --> 00:06:13,600
so as you can see there is a need to

166
00:06:13,600 --> 00:06:16,160
identify vulnerable users in a test

167
00:06:16,160 --> 00:06:18,720
conversation that we did with open ai we

168
00:06:18,720 --> 00:06:20,960
see that it does exactly that when a

169
00:06:20,960 --> 00:06:23,039
user mentions an instance of domestic

170
00:06:23,039 --> 00:06:25,120
violence they are guided to a national

171
00:06:25,120 --> 00:06:27,039
domestic violence hotline where they can

172
00:06:27,039 --> 00:06:28,639
seek help

173
00:06:28,639 --> 00:06:30,240
as you can see in the example on the

174
00:06:30,240 --> 00:06:32,800
left side

175
00:06:32,800 --> 00:06:35,120
however in a different example where the

176
00:06:35,120 --> 00:06:37,280
user asks for advice on how to kill

177
00:06:37,280 --> 00:06:39,600
themselves the same platform provides

178
00:06:39,600 --> 00:06:41,360
provides advice on how to do that

179
00:06:41,360 --> 00:06:42,560
properly

180
00:06:42,560 --> 00:06:44,319
there is a need for a filter and flag

181
00:06:44,319 --> 00:06:47,120
mechanisms that when users indicate that

182
00:06:47,120 --> 00:06:48,720
they might be at high risk and are

183
00:06:48,720 --> 00:06:50,960
talking to social bots they are provided

184
00:06:50,960 --> 00:06:52,960
with them

185
00:06:52,960 --> 00:06:55,120
a more appropriate response would be

186
00:06:55,120 --> 00:06:57,199
that of a google search where a high

187
00:06:57,199 --> 00:06:59,599
risk input is flagged and the user is

188
00:06:59,599 --> 00:07:01,440
presented with a lifeline number where

189
00:07:01,440 --> 00:07:04,319
they can seek help

190
00:07:04,319 --> 00:07:06,400
in terms of privacy by design social

191
00:07:06,400 --> 00:07:08,400
bots need to be more transparent about

192
00:07:08,400 --> 00:07:11,039
how user input is handled when a user

193
00:07:11,039 --> 00:07:13,360
asks for the same instead of providing a

194
00:07:13,360 --> 00:07:15,599
confusing response

195
00:07:15,599 --> 00:07:17,919
as you can see on the left mobile robot

196
00:07:17,919 --> 00:07:22,080
provides a good response to that

197
00:07:22,319 --> 00:07:24,240
for third party sharing there should be

198
00:07:24,240 --> 00:07:26,160
similar transparency along with a

199
00:07:26,160 --> 00:07:28,880
provision to opt out if a user wants to

200
00:07:28,880 --> 00:07:30,880
opt out of sharing information with a

201
00:07:30,880 --> 00:07:33,440
third party in case the bot does that

202
00:07:33,440 --> 00:07:37,120
there should be a way to block the same

203
00:07:37,120 --> 00:07:39,199
so cleverbot has taken a good first step

204
00:07:39,199 --> 00:07:41,280
towards risk management by stating

205
00:07:41,280 --> 00:07:43,280
upfront that users should not provide

206
00:07:43,280 --> 00:07:45,680
personal information and does not mean

207
00:07:45,680 --> 00:07:47,680
what it says there should be a more

208
00:07:47,680 --> 00:07:49,919
efficient approach for such social bots

209
00:07:49,919 --> 00:07:52,240
to take more responsibility beyond

210
00:07:52,240 --> 00:07:54,000
putting the onus of protection on the

211
00:07:54,000 --> 00:07:56,160
user

212
00:07:56,160 --> 00:07:59,199
so to sum it up users disclose we found

213
00:07:59,199 --> 00:08:01,199
in our data that users disclose highly

214
00:08:01,199 --> 00:08:03,440
sensitive information and this is some

215
00:08:03,440 --> 00:08:05,039
kind this is a behavior that should be

216
00:08:05,039 --> 00:08:07,599
considered while designing chatbots

217
00:08:07,599 --> 00:08:09,919
especially social bots

218
00:08:09,919 --> 00:08:12,319
and indicators of user mental health

219
00:08:12,319 --> 00:08:13,360
could be

220
00:08:13,360 --> 00:08:15,599
useful in designing some of these

221
00:08:15,599 --> 00:08:17,360
interactions

222
00:08:17,360 --> 00:08:19,039
we also found that there is a need to

223
00:08:19,039 --> 00:08:21,120
identify and protect sensitive

224
00:08:21,120 --> 00:08:23,520
information that might be unexpectedly

225
00:08:23,520 --> 00:08:26,080
shared in conversation with such bots

226
00:08:26,080 --> 00:08:28,400
especially health data

227
00:08:28,400 --> 00:08:29,759
and finally

228
00:08:29,759 --> 00:08:31,120
there should be a way to avoid

229
00:08:31,120 --> 00:08:34,559
manipulative patterns of interaction

230
00:08:34,559 --> 00:08:36,240
so that the

231
00:08:36,240 --> 00:08:38,559
conversation between a bot and a human

232
00:08:38,559 --> 00:08:41,838
remains at low risk

233
00:08:42,000 --> 00:08:43,599
so looking ahead we plan to do a

234
00:08:43,599 --> 00:08:46,160
systematic analysis of existing chatbots

235
00:08:46,160 --> 00:08:48,959
against these recommendations

236
00:08:48,959 --> 00:08:52,160
and see what we what we get we also want

237
00:08:52,160 --> 00:08:53,839
to do an analysis of chatbot

238
00:08:53,839 --> 00:08:56,320
interactions compared with studies in

239
00:08:56,320 --> 00:08:58,880
manipulation and fraud so dr tatiana

240
00:08:58,880 --> 00:09:01,040
brings expertise in manipulation and dr

241
00:09:01,040 --> 00:09:03,920
gene cam in fraud so we plan to perform

242
00:09:03,920 --> 00:09:06,640
an analysis of chatbot interactions with

243
00:09:06,640 --> 00:09:09,519
a focus on fraud and abuse

244
00:09:09,519 --> 00:09:11,279
and we also want to look at differential

245
00:09:11,279 --> 00:09:13,440
harms with diverse populations if they

246
00:09:13,440 --> 00:09:16,720
vary between cultures

247
00:09:17,680 --> 00:09:19,920
so thank you for watching um if you have

248
00:09:19,920 --> 00:09:22,240
any questions you can reach out to us or

249
00:09:22,240 --> 00:09:24,160
reach out to us on slack

250
00:09:24,160 --> 00:09:25,040
and

251
00:09:25,040 --> 00:09:27,040
dr camp is here to answer questions as

252
00:09:27,040 --> 00:09:30,040
well

253
00:09:36,800 --> 00:09:38,880
you

