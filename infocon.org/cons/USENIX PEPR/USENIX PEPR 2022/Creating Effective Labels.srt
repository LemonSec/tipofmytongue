1
00:00:09,760 --> 00:00:11,280
hello everyone

2
00:00:11,280 --> 00:00:13,280
today i'll be presenting on creating

3
00:00:13,280 --> 00:00:15,280
effective privacy labels

4
00:00:15,280 --> 00:00:17,199
the work was developed

5
00:00:17,199 --> 00:00:20,480
in collaboration with aj jayathi

6
00:00:20,480 --> 00:00:21,840
peter and

7
00:00:21,840 --> 00:00:24,400
gene camp

8
00:00:25,039 --> 00:00:27,439
today the user is solely responsible for

9
00:00:27,439 --> 00:00:29,439
protecting their privacy it doesn't

10
00:00:29,439 --> 00:00:31,359
matter if they're browsing the web using

11
00:00:31,359 --> 00:00:33,200
a mobile application

12
00:00:33,200 --> 00:00:35,040
or an iot device

13
00:00:35,040 --> 00:00:36,960
they are responsible for protecting

14
00:00:36,960 --> 00:00:39,280
their own privacy so this is a pretty

15
00:00:39,280 --> 00:00:42,160
big ask especially when you consider

16
00:00:42,160 --> 00:00:42,879
that

17
00:00:42,879 --> 00:00:44,480
they're presented with little to no

18
00:00:44,480 --> 00:00:46,640
information at the time of decision

19
00:00:46,640 --> 00:00:49,039
making

20
00:00:49,280 --> 00:00:51,440
say you want to install a meditation app

21
00:00:51,440 --> 00:00:53,520
as you can see there are quite a few

22
00:00:53,520 --> 00:00:54,800
options

23
00:00:54,800 --> 00:00:57,600
the app store provides you with metrics

24
00:00:57,600 --> 00:00:59,600
that tell you how many people

25
00:00:59,600 --> 00:01:01,760
found the app to be useful

26
00:01:01,760 --> 00:01:04,799
and the number of downloads an app has

27
00:01:04,799 --> 00:01:06,320
but

28
00:01:06,320 --> 00:01:08,720
you have no information about the

29
00:01:08,720 --> 00:01:10,720
privacy offered by the app

30
00:01:10,720 --> 00:01:12,479
how do you make a privacy preserving

31
00:01:12,479 --> 00:01:13,439
choice

32
00:01:13,439 --> 00:01:15,680
when you have no way of comparing these

33
00:01:15,680 --> 00:01:20,080
apps based on the privacy they offer

34
00:01:20,479 --> 00:01:23,119
therefore privacy labels are needed to

35
00:01:23,119 --> 00:01:27,840
empower users to make informed decisions

36
00:01:28,240 --> 00:01:30,479
but then what does it take to build an

37
00:01:30,479 --> 00:01:32,479
effective label well

38
00:01:32,479 --> 00:01:35,040
there are four primary components that

39
00:01:35,040 --> 00:01:39,360
make and make an effective label

40
00:01:39,360 --> 00:01:42,159
labels must reduce information

41
00:01:42,159 --> 00:01:43,439
asymmetry

42
00:01:43,439 --> 00:01:45,119
they should reduce the cognitive burden

43
00:01:45,119 --> 00:01:46,399
on the user

44
00:01:46,399 --> 00:01:48,960
account for psychological biases

45
00:01:48,960 --> 00:01:52,798
and should be personalized

46
00:01:53,040 --> 00:01:55,439
information asymmetry occurs when the

47
00:01:55,439 --> 00:01:57,439
buyer can't distinguish between high

48
00:01:57,439 --> 00:01:59,680
quality and low quality products

49
00:01:59,680 --> 00:02:03,439
for example the used car market

50
00:02:03,439 --> 00:02:05,759
is also known as a lemons market because

51
00:02:05,759 --> 00:02:07,200
the buyer doesn't

52
00:02:07,200 --> 00:02:10,239
does not know if a car has pre-existing

53
00:02:10,239 --> 00:02:11,760
defects

54
00:02:11,760 --> 00:02:13,280
so only people

55
00:02:13,280 --> 00:02:16,000
with low quality cars will want to sell

56
00:02:16,000 --> 00:02:18,560
their cars

57
00:02:18,560 --> 00:02:20,720
an obvious way to reduce information

58
00:02:20,720 --> 00:02:23,680
asymmetry is to present relevant data

59
00:02:23,680 --> 00:02:24,879
collection

60
00:02:24,879 --> 00:02:27,920
usage and retention practices

61
00:02:27,920 --> 00:02:30,640
however often times

62
00:02:30,640 --> 00:02:32,879
this is presented in a manner that

63
00:02:32,879 --> 00:02:36,480
increases the cognitive load on the user

64
00:02:36,480 --> 00:02:39,360
take privacy policies for example

65
00:02:39,360 --> 00:02:41,680
while they offer some insight into data

66
00:02:41,680 --> 00:02:44,400
collection and usage practices

67
00:02:44,400 --> 00:02:48,319
they are lengthy and unusable

68
00:02:48,560 --> 00:02:51,280
trainer and mcdonald estimated that the

69
00:02:51,280 --> 00:02:53,760
cost of reading privacy policies alone

70
00:02:53,760 --> 00:02:55,519
is in the billions

71
00:02:55,519 --> 00:02:56,640
additionally

72
00:02:56,640 --> 00:02:58,879
some privacy policies require high

73
00:02:58,879 --> 00:03:01,280
levels of literacy for you to understand

74
00:03:01,280 --> 00:03:02,720
its contents

75
00:03:02,720 --> 00:03:06,800
causing most people to just ignore them

76
00:03:07,519 --> 00:03:09,920
so privacy labels while reducing

77
00:03:09,920 --> 00:03:14,000
information asymmetry should should not

78
00:03:14,000 --> 00:03:16,000
increase the cognitive burden on the

79
00:03:16,000 --> 00:03:17,040
user

80
00:03:17,040 --> 00:03:18,560
a user should

81
00:03:18,560 --> 00:03:21,680
not be required to read multiple

82
00:03:21,680 --> 00:03:24,799
pages to understand the privacy offered

83
00:03:24,799 --> 00:03:26,239
by a product

84
00:03:26,239 --> 00:03:29,120
the label should instantly convey the

85
00:03:29,120 --> 00:03:31,760
privacy offered by a product

86
00:03:31,760 --> 00:03:34,239
at the same time it should be it should

87
00:03:34,239 --> 00:03:38,159
enable users to compare between products

88
00:03:38,159 --> 00:03:39,360
with little

89
00:03:39,360 --> 00:03:41,760
with little cognitive effort these

90
00:03:41,760 --> 00:03:43,280
requirements

91
00:03:43,280 --> 00:03:44,640
should be met

92
00:03:44,640 --> 00:03:49,200
while decreasing information asymmetry

93
00:03:50,319 --> 00:03:54,000
modular privacy labels are our best bet

94
00:03:54,000 --> 00:03:55,120
as

95
00:03:55,120 --> 00:03:56,400
they can both

96
00:03:56,400 --> 00:03:58,640
reduce information asymmetry and

97
00:03:58,640 --> 00:04:00,640
cognitive burden

98
00:04:00,640 --> 00:04:04,560
here the first layer presents users with

99
00:04:04,560 --> 00:04:07,360
with an aggregate rating which instantly

100
00:04:07,360 --> 00:04:10,720
conveys the privacy offered by a product

101
00:04:10,720 --> 00:04:13,200
and enables quick comparisons between

102
00:04:13,200 --> 00:04:14,799
products

103
00:04:14,799 --> 00:04:18,880
the second layer contains text

104
00:04:18,880 --> 00:04:21,440
that provides more context about the

105
00:04:21,440 --> 00:04:25,199
data collection and usage practices

106
00:04:25,199 --> 00:04:27,759
this area can also be used to highlight

107
00:04:27,759 --> 00:04:30,400
the data collection and usage practices

108
00:04:30,400 --> 00:04:33,600
that the user cares about

109
00:04:33,600 --> 00:04:35,919
but should not be displayed

110
00:04:35,919 --> 00:04:37,840
by default because

111
00:04:37,840 --> 00:04:40,160
it has non-trivial cognitive

112
00:04:40,160 --> 00:04:42,800
cost

113
00:04:42,800 --> 00:04:44,560
in addition to reducing the cognitive

114
00:04:44,560 --> 00:04:47,680
burden on the user we must also ensure

115
00:04:47,680 --> 00:04:49,120
that the label

116
00:04:49,120 --> 00:04:52,000
and the interface do not introduce

117
00:04:52,000 --> 00:04:54,800
psychological biases that negatively

118
00:04:54,800 --> 00:04:58,160
impact users privacy behavior

119
00:04:58,160 --> 00:05:00,960
anyone familiar with sas capadia and

120
00:05:00,960 --> 00:05:03,280
crainer knows this is true

121
00:05:03,280 --> 00:05:04,479
for instance

122
00:05:04,479 --> 00:05:06,000
positive framing

123
00:05:06,000 --> 00:05:07,520
is known to be more effective than

124
00:05:07,520 --> 00:05:09,360
negative framing

125
00:05:09,360 --> 00:05:13,520
so positively framed privacy ratings

126
00:05:13,520 --> 00:05:15,520
using the padlock icon

127
00:05:15,520 --> 00:05:17,520
elicit a more privacy preserving

128
00:05:17,520 --> 00:05:18,720
behavior

129
00:05:18,720 --> 00:05:22,240
when compared to negatively framed

130
00:05:22,240 --> 00:05:24,479
privacy ratings which utilize the eye

131
00:05:24,479 --> 00:05:27,360
icon where the loss in privacy is

132
00:05:27,360 --> 00:05:29,600
conveyed as opposed to

133
00:05:29,600 --> 00:05:33,360
the gain in privacy

134
00:05:35,120 --> 00:05:37,919
similarly endowment effect also has an

135
00:05:37,919 --> 00:05:39,919
impact on user

136
00:05:39,919 --> 00:05:41,440
behavior

137
00:05:41,440 --> 00:05:44,000
here the value a person attributes to a

138
00:05:44,000 --> 00:05:44,880
good

139
00:05:44,880 --> 00:05:47,280
varies based on if the person possesses

140
00:05:47,280 --> 00:05:48,400
the good

141
00:05:48,400 --> 00:05:50,080
or um

142
00:05:50,080 --> 00:05:51,600
or not

143
00:05:51,600 --> 00:05:53,280
what does this mean

144
00:05:53,280 --> 00:05:55,520
once you pers once you perceive

145
00:05:55,520 --> 00:05:57,520
ownership of something it's more

146
00:05:57,520 --> 00:05:59,199
valuable to you

147
00:05:59,199 --> 00:06:02,319
for example in class my professor gave

148
00:06:02,319 --> 00:06:06,240
away free rsa swag sheet swag t-shirts

149
00:06:06,240 --> 00:06:08,479
to every student in

150
00:06:08,479 --> 00:06:11,199
every student in the class she basically

151
00:06:11,199 --> 00:06:14,080
stood at the door and made each of them

152
00:06:14,080 --> 00:06:14,960
take

153
00:06:14,960 --> 00:06:17,840
one t-shirt

154
00:06:18,080 --> 00:06:20,400
we did not particularly want them but

155
00:06:20,400 --> 00:06:23,359
she made us take it

156
00:06:23,759 --> 00:06:25,199
a week later

157
00:06:25,199 --> 00:06:27,199
she tried to buy them back

158
00:06:27,199 --> 00:06:29,440
nobody wanted to sell them

159
00:06:29,440 --> 00:06:31,919
even uh nobody wanted to sell them for

160
00:06:31,919 --> 00:06:33,919
like even less than anything less than

161
00:06:33,919 --> 00:06:36,000
10 bucks so in a week

162
00:06:36,000 --> 00:06:39,039
the price of a free t-shirt essentially

163
00:06:39,039 --> 00:06:40,160
went up

164
00:06:40,160 --> 00:06:42,319
to ten dollars

165
00:06:42,319 --> 00:06:44,880
so ownership matters and

166
00:06:44,880 --> 00:06:47,039
that's essentially what endowment effect

167
00:06:47,039 --> 00:06:49,840
is in the case of in the context of

168
00:06:49,840 --> 00:06:52,479
privacy what this means is

169
00:06:52,479 --> 00:06:54,719
that

170
00:06:55,280 --> 00:06:57,599
users are less likely to give up privacy

171
00:06:57,599 --> 00:07:01,120
in exchange for a monetary compensation

172
00:07:01,120 --> 00:07:03,039
at the same time

173
00:07:03,039 --> 00:07:05,039
they are less likely to pay an

174
00:07:05,039 --> 00:07:06,400
additional amount to protect their

175
00:07:06,400 --> 00:07:08,880
privacy

176
00:07:10,960 --> 00:07:13,680
for example we conducted an experiment

177
00:07:13,680 --> 00:07:14,880
with three

178
00:07:14,880 --> 00:07:16,639
market three different versions of an

179
00:07:16,639 --> 00:07:18,639
iot marketplace

180
00:07:18,639 --> 00:07:20,800
the control version of the marketplace

181
00:07:20,800 --> 00:07:23,919
did not have any privacy information

182
00:07:23,919 --> 00:07:26,319
about the product it just had price and

183
00:07:26,319 --> 00:07:28,000
product description

184
00:07:28,000 --> 00:07:29,440
the willingness to pay and the

185
00:07:29,440 --> 00:07:32,160
willingness to accept interfaces

186
00:07:32,160 --> 00:07:35,199
had the same privacy uh had provided

187
00:07:35,199 --> 00:07:38,160
users with privacy information and also

188
00:07:38,160 --> 00:07:39,759
presented them

189
00:07:39,759 --> 00:07:42,319
uh using the same indicators

190
00:07:42,319 --> 00:07:43,599
but they

191
00:07:43,599 --> 00:07:46,160
emulated different framings

192
00:07:46,160 --> 00:07:48,479
all three versions of the marketplace

193
00:07:48,479 --> 00:07:50,879
sold the same products and the products

194
00:07:50,879 --> 00:07:53,120
that offered better privacy were priced

195
00:07:53,120 --> 00:07:54,400
higher than the

196
00:07:54,400 --> 00:07:58,080
products that offered lower privacy

197
00:07:58,080 --> 00:08:00,400
the results from this experiment showed

198
00:08:00,400 --> 00:08:02,319
that participants using the willingness

199
00:08:02,319 --> 00:08:03,759
to accept

200
00:08:03,759 --> 00:08:06,240
version of the inter interface were

201
00:08:06,240 --> 00:08:09,599
willing to pay 30 more when compared to

202
00:08:09,599 --> 00:08:11,120
when compared to the remaining two

203
00:08:11,120 --> 00:08:12,479
groups

204
00:08:12,479 --> 00:08:15,120
the more interesting finding here

205
00:08:15,120 --> 00:08:17,520
is that we did not find any difference

206
00:08:17,520 --> 00:08:19,520
between the control group and the

207
00:08:19,520 --> 00:08:22,639
willingness to pay group

208
00:08:22,639 --> 00:08:25,199
in other words the impact of the privacy

209
00:08:25,199 --> 00:08:27,759
label was negated by the psychological

210
00:08:27,759 --> 00:08:28,800
biases

211
00:08:28,800 --> 00:08:33,360
biases introduced by the inter interface

212
00:08:36,080 --> 00:08:39,360
defaults also matter status quo bias

213
00:08:39,360 --> 00:08:41,599
states that people are more likely to

214
00:08:41,599 --> 00:08:43,440
stick to the default

215
00:08:43,440 --> 00:08:45,440
this is because people are inherently

216
00:08:45,440 --> 00:08:47,360
loss of worse

217
00:08:47,360 --> 00:08:49,040
and fear the potential negative

218
00:08:49,040 --> 00:08:51,680
consequences that could arise from

219
00:08:51,680 --> 00:08:55,440
moving out of the default state

220
00:08:55,440 --> 00:08:56,800
for example

221
00:08:56,800 --> 00:08:59,440
apple's transparency

222
00:08:59,440 --> 00:09:02,320
tracking is enabled by default

223
00:09:02,320 --> 00:09:05,040
so any app wanting to track a user

224
00:09:05,040 --> 00:09:07,760
across apps has to get consent from the

225
00:09:07,760 --> 00:09:09,360
user

226
00:09:09,360 --> 00:09:11,920
so far eleven percent of the users

227
00:09:11,920 --> 00:09:14,720
worldwide have an only level

228
00:09:14,720 --> 00:09:17,120
only eleven percent of the users

229
00:09:17,120 --> 00:09:19,920
worldwide have enabled

230
00:09:19,920 --> 00:09:21,600
apps to track them

231
00:09:21,600 --> 00:09:24,240
this demonstrates how strong of an

232
00:09:24,240 --> 00:09:28,000
effect defaults have

233
00:09:29,040 --> 00:09:31,760
finally privacy preferences vary from

234
00:09:31,760 --> 00:09:33,360
person to person

235
00:09:33,360 --> 00:09:36,080
so it is important that privacy ratings

236
00:09:36,080 --> 00:09:38,880
are personalized for each user

237
00:09:38,880 --> 00:09:41,040
machine learning models can help here

238
00:09:41,040 --> 00:09:42,480
for instance

239
00:09:42,480 --> 00:09:44,880
we found that

240
00:09:44,880 --> 00:09:46,480
we found that we could accurately

241
00:09:46,480 --> 00:09:48,959
estimate a user's comfort for sharing a

242
00:09:48,959 --> 00:09:51,279
given data type by looking at five

243
00:09:51,279 --> 00:09:53,600
factors the device type

244
00:09:53,600 --> 00:09:57,040
the person the perceived sensitivity

245
00:09:57,040 --> 00:10:00,000
of the data type

246
00:10:01,200 --> 00:10:03,120
perceived benefit

247
00:10:03,120 --> 00:10:06,320
perceived trust and perceived

248
00:10:06,320 --> 00:10:08,320
risk associated

249
00:10:08,320 --> 00:10:10,560
perceived risk associated to personal

250
00:10:10,560 --> 00:10:12,959
safety

251
00:10:12,959 --> 00:10:15,760
the predicted comfort for each data type

252
00:10:15,760 --> 00:10:18,000
collected by a product can then be

253
00:10:18,000 --> 00:10:20,640
aggregated to generate a privacy rating

254
00:10:20,640 --> 00:10:23,680
on a five point scale

255
00:10:24,640 --> 00:10:27,120
to summarize labels are needed to make

256
00:10:27,120 --> 00:10:29,279
informed decisions

257
00:10:29,279 --> 00:10:32,640
and for privacy labels to be effective

258
00:10:32,640 --> 00:10:34,320
they need to address information

259
00:10:34,320 --> 00:10:35,600
asymmetry

260
00:10:35,600 --> 00:10:37,839
reduce cognitive burden

261
00:10:37,839 --> 00:10:40,399
account for psychological biases

262
00:10:40,399 --> 00:10:44,000
and should be personalized

263
00:10:44,079 --> 00:10:44,839
thank

264
00:10:44,839 --> 00:10:49,480
you any questions

