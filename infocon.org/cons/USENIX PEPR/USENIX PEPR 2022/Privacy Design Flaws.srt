1
00:00:09,519 --> 00:00:11,920
hello and welcome to my talk i'm aiven

2
00:00:11,920 --> 00:00:14,080
arvistan

3
00:00:14,080 --> 00:00:15,519
and i'm going to talk about privacy

4
00:00:15,519 --> 00:00:17,199
design flaws

5
00:00:17,199 --> 00:00:17,720
um

6
00:00:17,720 --> 00:00:19,279
[Music]

7
00:00:19,279 --> 00:00:21,680
brief introduction i i work for a

8
00:00:21,680 --> 00:00:24,160
company called sector alarm which

9
00:00:24,160 --> 00:00:26,080
which is one of europe's leading home

10
00:00:26,080 --> 00:00:27,680
safety

11
00:00:27,680 --> 00:00:29,439
companies as you mentioned

12
00:00:29,439 --> 00:00:31,519
however in this talk i'm only

13
00:00:31,519 --> 00:00:32,960
representing my own views and not

14
00:00:32,960 --> 00:00:36,800
necessarily those of my employer

15
00:00:37,440 --> 00:00:39,520
so a little bit of background some of

16
00:00:39,520 --> 00:00:42,480
you might be asking yourselves what am i

17
00:00:42,480 --> 00:00:45,039
what am i talking about when i'm saying

18
00:00:45,039 --> 00:00:48,640
um when i'm saying privacy flaws

19
00:00:48,640 --> 00:00:51,840
well this is one way i find useful to to

20
00:00:51,840 --> 00:00:54,399
think about these things uh flaws in the

21
00:00:54,399 --> 00:00:57,920
design or in the architecture level as

22
00:00:57,920 --> 00:01:01,440
opposed to bugs in the code

23
00:01:01,440 --> 00:01:03,280
because faulty architectural assumptions

24
00:01:03,280 --> 00:01:05,280
can lead to

25
00:01:05,280 --> 00:01:07,600
cracks in the virtual foundations of our

26
00:01:07,600 --> 00:01:09,439
software if you will which will of

27
00:01:09,439 --> 00:01:11,040
course be much more difficult and time

28
00:01:11,040 --> 00:01:13,119
consuming too

29
00:01:13,119 --> 00:01:15,920
to refactor and rectify post

30
00:01:15,920 --> 00:01:18,320
implementation and that of course also

31
00:01:18,320 --> 00:01:19,920
means it'll be much more expensive if

32
00:01:19,920 --> 00:01:22,880
you caught it at a later stage

33
00:01:22,880 --> 00:01:25,439
now this talk takes as a starting point

34
00:01:25,439 --> 00:01:27,680
gary mcgraw's security flaws some of you

35
00:01:27,680 --> 00:01:30,560
might be familiar with him in his work

36
00:01:30,560 --> 00:01:33,600
which is basically defects in the design

37
00:01:33,600 --> 00:01:35,439
or architecture

38
00:01:35,439 --> 00:01:37,119
as opposed to in the implementation i

39
00:01:37,119 --> 00:01:39,040
believe is how he

40
00:01:39,040 --> 00:01:41,280
defines it

41
00:01:41,280 --> 00:01:42,479
more information about this if you're

42
00:01:42,479 --> 00:01:44,479
not familiar with it can be found in the

43
00:01:44,479 --> 00:01:46,799
document avoiding the top 10 software

44
00:01:46,799 --> 00:01:49,040
security design flaws which was

45
00:01:49,040 --> 00:01:51,520
published by the ieee center for secure

46
00:01:51,520 --> 00:01:53,200
design

47
00:01:53,200 --> 00:01:56,240
back in 2015 if i remember correctly you

48
00:01:56,240 --> 00:01:58,880
can see the link there

49
00:01:58,880 --> 00:02:01,040
so why is this important well my

50
00:02:01,040 --> 00:02:02,719
motivations at least to talk about this

51
00:02:02,719 --> 00:02:03,920
is that

52
00:02:03,920 --> 00:02:05,200
we can see

53
00:02:05,200 --> 00:02:07,280
we can view privacy as an emergent

54
00:02:07,280 --> 00:02:09,758
property of the system in question of

55
00:02:09,758 --> 00:02:11,520
our systems

56
00:02:11,520 --> 00:02:13,200
and many of the big

57
00:02:13,200 --> 00:02:15,920
privacy flaws that i see at least are

58
00:02:15,920 --> 00:02:18,560
not necessarily found on a code level or

59
00:02:18,560 --> 00:02:23,280
scoped to a line or a block of code

60
00:02:23,280 --> 00:02:24,480
there's also

61
00:02:24,480 --> 00:02:26,319
as far as i perceive it a lack of

62
00:02:26,319 --> 00:02:28,239
explicit best practices that are

63
00:02:28,239 --> 00:02:30,400
concrete and actionable enough for for

64
00:02:30,400 --> 00:02:32,480
technical roles that are not necessarily

65
00:02:32,480 --> 00:02:35,280
privacy experts themselves

66
00:02:35,280 --> 00:02:36,800
and that makes it hard for development

67
00:02:36,800 --> 00:02:39,680
team or or um or architects that don't

68
00:02:39,680 --> 00:02:41,519
know that much about

69
00:02:41,519 --> 00:02:44,879
privacy maybe to to be able to to to be

70
00:02:44,879 --> 00:02:46,640
able to create privacy friendly uh

71
00:02:46,640 --> 00:02:48,239
solutions in the first place again

72
00:02:48,239 --> 00:02:50,239
meaning that we're we're only catching

73
00:02:50,239 --> 00:02:52,160
it later down the line further further

74
00:02:52,160 --> 00:02:53,120
to the right if you're willing the

75
00:02:53,120 --> 00:02:55,200
software development lifecycle

76
00:02:55,200 --> 00:02:56,480
so

77
00:02:56,480 --> 00:02:57,599
um

78
00:02:57,599 --> 00:02:58,959
i think we need to make privacy and

79
00:02:58,959 --> 00:03:01,440
engineering basics uh common knowledge

80
00:03:01,440 --> 00:03:02,640
amongst developers and architects

81
00:03:02,640 --> 00:03:04,000
basically

82
00:03:04,000 --> 00:03:06,879
and this will probably also enable us to

83
00:03:06,879 --> 00:03:08,239
have easier discussions amongst

84
00:03:08,239 --> 00:03:10,879
ourselves as to

85
00:03:10,879 --> 00:03:13,120
about our own architectural defects and

86
00:03:13,120 --> 00:03:15,519
and how to work with those and rectify

87
00:03:15,519 --> 00:03:17,040
them

88
00:03:17,040 --> 00:03:19,519
alternatively um

89
00:03:19,519 --> 00:03:21,680
it could also if we don't

90
00:03:21,680 --> 00:03:24,319
uh do anything about this um

91
00:03:24,319 --> 00:03:26,239
this lack of you know basic privacy

92
00:03:26,239 --> 00:03:27,840
engineering competency within the

93
00:03:27,840 --> 00:03:29,760
broader development field

94
00:03:29,760 --> 00:03:31,760
i fear that

95
00:03:31,760 --> 00:03:34,560
your privacy department or repressive

96
00:03:34,560 --> 00:03:36,640
function could potentially also become a

97
00:03:36,640 --> 00:03:40,159
bottleneck in your organization

98
00:03:40,560 --> 00:03:43,599
and a little bit of a side note

99
00:03:43,599 --> 00:03:45,440
i think it might be a good idea when

100
00:03:45,440 --> 00:03:46,959
we're talking about these things to

101
00:03:46,959 --> 00:03:49,040
ensure that we're communicating very

102
00:03:49,040 --> 00:03:50,799
clear internally in our organizations

103
00:03:50,799 --> 00:03:53,120
with regard to privacy risks that

104
00:03:53,120 --> 00:03:55,760
these arguably differ somewhat from your

105
00:03:55,760 --> 00:03:57,840
typical security risks at least from a

106
00:03:57,840 --> 00:03:59,920
business perspective and that privacy

107
00:03:59,920 --> 00:04:02,159
risks are really as i see it risks that

108
00:04:02,159 --> 00:04:04,400
you manage on behalf of your data

109
00:04:04,400 --> 00:04:06,239
subjects or your users

110
00:04:06,239 --> 00:04:08,159
so you're actually you're taking on a

111
00:04:08,159 --> 00:04:10,239
risk on their behalves because they are

112
00:04:10,239 --> 00:04:12,560
the one who will actually be directly

113
00:04:12,560 --> 00:04:14,239
affected by the confidence by the

114
00:04:14,239 --> 00:04:18,238
consequences if any risks are realized

115
00:04:18,238 --> 00:04:20,320
and as it was mentioned earlier today i

116
00:04:20,320 --> 00:04:22,400
think

117
00:04:22,400 --> 00:04:24,560
it is sometimes of course preferable to

118
00:04:24,560 --> 00:04:26,560
accept the remaining risks but

119
00:04:26,560 --> 00:04:27,840
we need to be explicit about the fact

120
00:04:27,840 --> 00:04:29,680
that we're accepting someone else's risk

121
00:04:29,680 --> 00:04:31,919
to a larger extent than than in other

122
00:04:31,919 --> 00:04:33,840
cases in these cases

123
00:04:33,840 --> 00:04:35,280
uh and of course with the steady

124
00:04:35,280 --> 00:04:37,120
increase in digitalization of society

125
00:04:37,120 --> 00:04:40,080
whether that be um whether that be you

126
00:04:40,080 --> 00:04:42,960
know optimization of healthcare systems

127
00:04:42,960 --> 00:04:45,360
or pushing digital ads to potential

128
00:04:45,360 --> 00:04:47,919
customers

129
00:04:49,280 --> 00:04:51,040
i think we basically want to

130
00:04:51,040 --> 00:04:53,120
want to avoid digitalizating ourselves

131
00:04:53,120 --> 00:04:55,040
to death and to enable a sustainable

132
00:04:55,040 --> 00:04:56,639
digital society

133
00:04:56,639 --> 00:04:58,080
so hence there's an obvious ethical

134
00:04:58,080 --> 00:04:59,759
component to this

135
00:04:59,759 --> 00:05:01,759
and you could argue and you might even

136
00:05:01,759 --> 00:05:04,080
want to argue that sloppy or uninformed

137
00:05:04,080 --> 00:05:06,560
privacy work particularly in some of

138
00:05:06,560 --> 00:05:08,320
some of the more critical systems in

139
00:05:08,320 --> 00:05:09,360
these cases

140
00:05:09,360 --> 00:05:11,520
might be might go as far as to say

141
00:05:11,520 --> 00:05:14,400
they're it's unethical

142
00:05:14,400 --> 00:05:16,479
now for the flaws themselves and before

143
00:05:16,479 --> 00:05:18,320
starting with that i want to

144
00:05:18,320 --> 00:05:19,919
just make it clear that i'm

145
00:05:19,919 --> 00:05:22,560
i'm using some highly publicized

146
00:05:22,560 --> 00:05:25,039
instances of privacy flaws here and it's

147
00:05:25,039 --> 00:05:26,560
it's not an attempt to point fingers or

148
00:05:26,560 --> 00:05:28,320
blame or anything like that just to

149
00:05:28,320 --> 00:05:29,840
illustrate what i think are fairly

150
00:05:29,840 --> 00:05:32,160
common flaws by looking at highly

151
00:05:32,160 --> 00:05:35,280
publicized instances of them

152
00:05:35,280 --> 00:05:37,280
because failures in this field of course

153
00:05:37,280 --> 00:05:39,039
will gain much more attention than all

154
00:05:39,039 --> 00:05:41,520
the success resulting from the great

155
00:05:41,520 --> 00:05:43,120
work being done in the field

156
00:05:43,120 --> 00:05:45,520
now this is a

157
00:05:45,520 --> 00:05:46,560
sort of a

158
00:05:46,560 --> 00:05:48,400
first reiteration of a list that i

159
00:05:48,400 --> 00:05:50,400
initially included in my proposal i

160
00:05:50,400 --> 00:05:51,680
obviously won't have time to get onto

161
00:05:51,680 --> 00:05:53,280
all of them in this talk but

162
00:05:53,280 --> 00:05:55,600
i'd very much be interested in in

163
00:05:55,600 --> 00:05:57,039
following up on this in sort of a more

164
00:05:57,039 --> 00:06:00,240
collaborative and structured effort so

165
00:06:00,240 --> 00:06:02,639
if uh if anyone wants to reach out

166
00:06:02,639 --> 00:06:05,360
regarding that feel free to do that

167
00:06:05,360 --> 00:06:08,160
now for some of the flaws

168
00:06:08,160 --> 00:06:10,479
so this is one i'm intimately familiar

169
00:06:10,479 --> 00:06:13,120
with and regrettably it comes up

170
00:06:13,120 --> 00:06:16,240
more than i i would wish

171
00:06:16,240 --> 00:06:17,440
but it is

172
00:06:17,440 --> 00:06:20,400
kind of a prime example of some basic

173
00:06:20,400 --> 00:06:22,240
privacy engineering

174
00:06:22,240 --> 00:06:24,000
principles not being followed this is

175
00:06:24,000 --> 00:06:26,800
that first norwegian official

176
00:06:26,800 --> 00:06:27,919
digital

177
00:06:27,919 --> 00:06:29,759
contact tracing slash exposure

178
00:06:29,759 --> 00:06:31,919
notification app that

179
00:06:31,919 --> 00:06:33,759
that i was a part of a group tasked with

180
00:06:33,759 --> 00:06:36,160
evaluating

181
00:06:36,160 --> 00:06:38,080
and the flaw that i'm illustrating with

182
00:06:38,080 --> 00:06:39,919
this example is mistaking data

183
00:06:39,919 --> 00:06:43,039
protection for privacy now i i expect

184
00:06:43,039 --> 00:06:44,400
that most of us

185
00:06:44,400 --> 00:06:46,880
agree that data privacy and data

186
00:06:46,880 --> 00:06:50,880
protection is not really the same thing

187
00:06:50,880 --> 00:06:53,360
and that you shouldn't be excusing sort

188
00:06:53,360 --> 00:06:55,039
of aggressive collection practices with

189
00:06:55,039 --> 00:06:58,400
post-hoc protective measures

190
00:06:58,479 --> 00:07:00,000
so if you want to identify these sorts

191
00:07:00,000 --> 00:07:02,000
of issues you might want to ask yourself

192
00:07:02,000 --> 00:07:04,319
whether you're able to explain how and

193
00:07:04,319 --> 00:07:07,039
why your system is privacy friendly

194
00:07:07,039 --> 00:07:09,520
without referencing security controls or

195
00:07:09,520 --> 00:07:10,960
pulling those in to back up your

196
00:07:10,960 --> 00:07:12,720
arguments

197
00:07:12,720 --> 00:07:14,720
in this example the first norwegian

198
00:07:14,720 --> 00:07:16,639
contact racing app was it kind of

199
00:07:16,639 --> 00:07:18,400
deviated from best practices and

200
00:07:18,400 --> 00:07:20,479
actually legal requirements as it was

201
00:07:20,479 --> 00:07:22,319
later uncovered

202
00:07:22,319 --> 00:07:24,479
quite a bit uh so

203
00:07:24,479 --> 00:07:25,919
basically for those of you that are

204
00:07:25,919 --> 00:07:27,440
familiar with the standard that apple

205
00:07:27,440 --> 00:07:30,479
and google de facto ended up defining

206
00:07:30,479 --> 00:07:32,319
this was about as far from that as you

207
00:07:32,319 --> 00:07:34,560
could get this application can

208
00:07:34,560 --> 00:07:36,560
continuously collected both location

209
00:07:36,560 --> 00:07:38,880
service data as well as bluetooth data

210
00:07:38,880 --> 00:07:41,599
from every user all of the time

211
00:07:41,599 --> 00:07:44,080
and continuously fed that to a

212
00:07:44,080 --> 00:07:46,560
government-controlled cloud solution now

213
00:07:46,560 --> 00:07:49,440
that cloud solution was fairly well

214
00:07:49,440 --> 00:07:52,319
secured in itself in my view but that

215
00:07:52,319 --> 00:07:54,479
doesn't in itself mean that the other

216
00:07:54,479 --> 00:07:55,759
parts of this application is

217
00:07:55,759 --> 00:07:57,599
unproblematic or that

218
00:07:57,599 --> 00:07:59,599
the privacy is well off because you've

219
00:07:59,599 --> 00:08:01,280
secured the data that you're collecting

220
00:08:01,280 --> 00:08:02,319
right

221
00:08:02,319 --> 00:08:04,080
um

222
00:08:04,080 --> 00:08:06,000
someone surprisingly the party that

223
00:08:06,000 --> 00:08:07,840
actually developed this application went

224
00:08:07,840 --> 00:08:10,400
as far as to go to the media and make a

225
00:08:10,400 --> 00:08:12,800
big argument about the fact that um

226
00:08:12,800 --> 00:08:14,000
all the parties involved here were

227
00:08:14,000 --> 00:08:15,759
trustworthy so there wasn't really a big

228
00:08:15,759 --> 00:08:16,879
deal about it

229
00:08:16,879 --> 00:08:17,759
um

230
00:08:17,759 --> 00:08:20,639
and he actually they actually made they

231
00:08:20,639 --> 00:08:23,120
actually made a quote along the lines of

232
00:08:23,120 --> 00:08:25,840
you have to trust the king

233
00:08:25,840 --> 00:08:27,759
referencing the fact that the norway is

234
00:08:27,759 --> 00:08:30,879
a constitutional monarchy

235
00:08:30,879 --> 00:08:32,719
so that's yeah i don't know what to say

236
00:08:32,719 --> 00:08:35,039
to that

237
00:08:35,120 --> 00:08:36,479
and additionally

238
00:08:36,479 --> 00:08:38,479
i i want to comment that data protection

239
00:08:38,479 --> 00:08:41,360
is kind of a misnomer in that

240
00:08:41,360 --> 00:08:43,839
protection of data is information

241
00:08:43,839 --> 00:08:46,160
security i guess and using this term

242
00:08:46,160 --> 00:08:48,240
interchangeably with privacy is some

243
00:08:48,240 --> 00:08:50,480
regrettably do further complete

244
00:08:50,480 --> 00:08:52,800
completes the issue so rather data

245
00:08:52,800 --> 00:08:54,240
protection is about protecting the

246
00:08:54,240 --> 00:08:56,080
people behind the data or the people

247
00:08:56,080 --> 00:08:58,000
described by the data and their rights

248
00:08:58,000 --> 00:08:59,760
and interests in the same way that

249
00:08:59,760 --> 00:09:01,680
helmet safety is about ensuring the

250
00:09:01,680 --> 00:09:03,200
safety of the wearer of the helmet and

251
00:09:03,200 --> 00:09:05,839
not making sure that helmets are safe

252
00:09:05,839 --> 00:09:06,640
so

253
00:09:06,640 --> 00:09:09,600
in order to avoid these types of flaws

254
00:09:09,600 --> 00:09:11,760
you want to build up your minimum

255
00:09:11,760 --> 00:09:13,600
privacy competence within the technical

256
00:09:13,600 --> 00:09:15,760
organization i think in and assess

257
00:09:15,760 --> 00:09:18,240
purpose limitations lawful basis degree

258
00:09:18,240 --> 00:09:21,120
of data minimization and so on

259
00:09:21,120 --> 00:09:23,360
another fairly or this one should

260
00:09:23,360 --> 00:09:25,600
probably be more uh familiar to most of

261
00:09:25,600 --> 00:09:28,560
you uh it's an old one though um it's

262
00:09:28,560 --> 00:09:31,760
about false anonymity where

263
00:09:31,760 --> 00:09:35,279
researchers were able to deanonymize

264
00:09:35,279 --> 00:09:38,000
users in netflix's surprise the netflix

265
00:09:38,000 --> 00:09:40,080
prize data set

266
00:09:40,080 --> 00:09:43,200
this was back in the early 2000s or

267
00:09:43,200 --> 00:09:45,360
just before 2010 10 if i remember

268
00:09:45,360 --> 00:09:47,839
correctly so basically the researchers

269
00:09:47,839 --> 00:09:50,320
de-anonymize users in the data set that

270
00:09:50,320 --> 00:09:51,440
contained

271
00:09:51,440 --> 00:09:53,839
movie ratings from 500 000 subscribers

272
00:09:53,839 --> 00:09:56,240
by correlating that with imdb data and

273
00:09:56,240 --> 00:09:58,160
only knowing a tiny bit about each

274
00:09:58,160 --> 00:10:00,560
person beforehand so based on that they

275
00:10:00,560 --> 00:10:02,320
could basically uncover their identities

276
00:10:02,320 --> 00:10:04,640
to the extent that that was

277
00:10:04,640 --> 00:10:06,560
reflected in and b data and they could

278
00:10:06,560 --> 00:10:08,640
uncover their political preferences and

279
00:10:08,640 --> 00:10:12,640
other potentially sensitive information

280
00:10:13,279 --> 00:10:15,680
if you want to avoid those flaws err on

281
00:10:15,680 --> 00:10:17,519
the side of caution with regards to

282
00:10:17,519 --> 00:10:20,000
anonymization techniques

283
00:10:20,000 --> 00:10:21,680
there's been quite a few interesting

284
00:10:21,680 --> 00:10:24,079
things said around that today already

285
00:10:24,079 --> 00:10:25,760
and this is of course difficult as a

286
00:10:25,760 --> 00:10:27,600
re-identification research and attacks

287
00:10:27,600 --> 00:10:29,760
keeps redefining anonymous and

288
00:10:29,760 --> 00:10:31,120
particularly

289
00:10:31,120 --> 00:10:32,959
when you start thinking about potential

290
00:10:32,959 --> 00:10:34,959
combination with other data sources it's

291
00:10:34,959 --> 00:10:38,000
it gets interesting quite quickly

292
00:10:38,000 --> 00:10:39,440
um

293
00:10:39,440 --> 00:10:41,839
i'm also going to reference

294
00:10:41,839 --> 00:10:43,680
the cambridge analytica

295
00:10:43,680 --> 00:10:44,959
scandal

296
00:10:44,959 --> 00:10:48,000
so this one i've i've called assumed

297
00:10:48,000 --> 00:10:50,560
trust and it's about a system implicitly

298
00:10:50,560 --> 00:10:52,160
building trust

299
00:10:52,160 --> 00:10:56,399
building upon trust or assuming trust

300
00:10:56,399 --> 00:10:57,680
so

301
00:10:57,680 --> 00:10:59,600
you typically identify this with threat

302
00:10:59,600 --> 00:11:01,839
modeling i i think and for those of you

303
00:11:01,839 --> 00:11:03,600
that aren't intimately familiar with

304
00:11:03,600 --> 00:11:06,000
this example uh

305
00:11:06,000 --> 00:11:07,680
very quickly cambridge analytica

306
00:11:07,680 --> 00:11:09,600
basically built psychological profiles

307
00:11:09,600 --> 00:11:12,640
of facebook users to sell individual

308
00:11:12,640 --> 00:11:15,200
psychological targeting as a service

309
00:11:15,200 --> 00:11:16,800
um and they were able to obtain

310
00:11:16,800 --> 00:11:18,160
unusually rich

311
00:11:18,160 --> 00:11:20,480
information about the friends of the

312
00:11:20,480 --> 00:11:23,120
users that were replying to these

313
00:11:23,120 --> 00:11:26,160
personality quizzes by using facebook's

314
00:11:26,160 --> 00:11:29,279
graph api that at that time had fairly

315
00:11:29,279 --> 00:11:31,200
permissive api scopes

316
00:11:31,200 --> 00:11:34,079
now the terms of service limited what

317
00:11:34,079 --> 00:11:36,480
was fair use of that

318
00:11:36,480 --> 00:11:38,000
so you would only

319
00:11:38,000 --> 00:11:40,560
be allowed to use that data in order to

320
00:11:40,560 --> 00:11:43,279
help say improve in-app experience or

321
00:11:43,279 --> 00:11:44,720
something like that

322
00:11:44,720 --> 00:11:47,600
but as that limitation wasn't it wasn't

323
00:11:47,600 --> 00:11:49,839
actually enforced cambridge analytica

324
00:11:49,839 --> 00:11:51,279
could basically do

325
00:11:51,279 --> 00:11:52,959
whatever they wanted to with that data

326
00:11:52,959 --> 00:11:54,399
and they did

327
00:11:54,399 --> 00:11:58,560
um so to avoid that you'd

328
00:11:58,560 --> 00:12:00,399
i guess it's pretty basic but you'd want

329
00:12:00,399 --> 00:12:03,279
to enforce your limitations because

330
00:12:03,279 --> 00:12:04,959
a limitation that isn't actually

331
00:12:04,959 --> 00:12:06,399
enforced isn't really much of a

332
00:12:06,399 --> 00:12:08,240
limitation in practice right

333
00:12:08,240 --> 00:12:10,480
uh and and this sort of feeds into

334
00:12:10,480 --> 00:12:13,519
some fairly well-known concept from the

335
00:12:13,519 --> 00:12:15,440
world of infosec

336
00:12:15,440 --> 00:12:18,320
like principle of least privilege earn

337
00:12:18,320 --> 00:12:20,399
or give but never assume trust trust but

338
00:12:20,399 --> 00:12:22,720
verify and so on that many of you i

339
00:12:22,720 --> 00:12:25,440
would expect are familiar with

340
00:12:25,440 --> 00:12:27,519
um

341
00:12:27,519 --> 00:12:31,040
this flaw is fairly general

342
00:12:31,040 --> 00:12:32,959
i've only

343
00:12:32,959 --> 00:12:34,880
really referred to it as data leakage

344
00:12:34,880 --> 00:12:35,920
but it's

345
00:12:35,920 --> 00:12:37,920
it's about systems making sensitive data

346
00:12:37,920 --> 00:12:39,920
available unintentionally

347
00:12:39,920 --> 00:12:42,240
um you know identified via threat

348
00:12:42,240 --> 00:12:43,600
modeling

349
00:12:43,600 --> 00:12:44,399
and

350
00:12:44,399 --> 00:12:46,320
one example that i'm using here is the

351
00:12:46,320 --> 00:12:47,760
fact that

352
00:12:47,760 --> 00:12:50,399
the android operating system was at one

353
00:12:50,399 --> 00:12:51,440
point

354
00:12:51,440 --> 00:12:52,560
logging

355
00:12:52,560 --> 00:12:54,639
contact tracing app information and

356
00:12:54,639 --> 00:12:57,360
system logs that were

357
00:12:57,360 --> 00:12:59,120
readable by

358
00:12:59,120 --> 00:13:01,360
by privileged certain privileged apps

359
00:13:01,360 --> 00:13:03,120
including pre-installed apps for

360
00:13:03,120 --> 00:13:04,160
instance

361
00:13:04,160 --> 00:13:06,639
browsers or some such bundled by makers

362
00:13:06,639 --> 00:13:08,800
of custom android distribution like

363
00:13:08,800 --> 00:13:10,480
phone companies

364
00:13:10,480 --> 00:13:12,959
such as samsung and motorola and so on

365
00:13:12,959 --> 00:13:15,120
so that could then potentially have been

366
00:13:15,120 --> 00:13:17,040
combined with other locally available

367
00:13:17,040 --> 00:13:19,279
information maybe max device names

368
00:13:19,279 --> 00:13:22,240
advertising ids and so on

369
00:13:22,240 --> 00:13:24,320
not to say that there were any evidence

370
00:13:24,320 --> 00:13:25,920
of that but

371
00:13:25,920 --> 00:13:27,519
according to the researchers that's

372
00:13:27,519 --> 00:13:28,639
that's how it could have been abused

373
00:13:28,639 --> 00:13:31,040
basically

374
00:13:31,120 --> 00:13:33,040
so in order to avoid those types of

375
00:13:33,040 --> 00:13:34,800
things you obviously want data

376
00:13:34,800 --> 00:13:36,720
classification schemes and policies

377
00:13:36,720 --> 00:13:38,720
logging policies in particular and in

378
00:13:38,720 --> 00:13:40,320
particular with regards to sensitive

379
00:13:40,320 --> 00:13:43,279
data and that's

380
00:13:43,279 --> 00:13:45,839
when when discussing that with with

381
00:13:45,839 --> 00:13:47,839
with fellow

382
00:13:47,839 --> 00:13:49,760
architects and such in norway

383
00:13:49,760 --> 00:13:51,199
we've come to

384
00:13:51,199 --> 00:13:52,320
there's

385
00:13:52,320 --> 00:13:54,160
there's been quite a few comments on oh

386
00:13:54,160 --> 00:13:56,079
really is that something we should think

387
00:13:56,079 --> 00:13:57,360
about right

388
00:13:57,360 --> 00:14:00,000
logging potentially sensitive data

389
00:14:00,000 --> 00:14:02,639
not everyone has thought about that

390
00:14:02,639 --> 00:14:04,560
um

391
00:14:04,560 --> 00:14:05,279
so

392
00:14:05,279 --> 00:14:07,279
those are just a few examples of what

393
00:14:07,279 --> 00:14:09,839
i'd uh call um

394
00:14:09,839 --> 00:14:12,639
you know privacy design flaws and the

395
00:14:12,639 --> 00:14:15,199
main main point of this is basically

396
00:14:15,199 --> 00:14:17,279
that

397
00:14:17,279 --> 00:14:19,440
one situation i'm off i often find

398
00:14:19,440 --> 00:14:21,839
myself in is trying to communicate with

399
00:14:21,839 --> 00:14:23,600
developer roles or architecture roles

400
00:14:23,600 --> 00:14:25,440
that don't necessarily have that much of

401
00:14:25,440 --> 00:14:27,839
a privacy background or that much of

402
00:14:27,839 --> 00:14:29,920
in-depth knowledge

403
00:14:29,920 --> 00:14:31,680
into privacy engineering

404
00:14:31,680 --> 00:14:32,800
and

405
00:14:32,800 --> 00:14:35,600
when we don't have the same basics and

406
00:14:35,600 --> 00:14:37,040
definitions in place it's kind of hard

407
00:14:37,040 --> 00:14:40,240
to move forward from there so

408
00:14:40,480 --> 00:14:43,519
so the problem summarized here is

409
00:14:43,519 --> 00:14:44,880
we can see that there are multiple

410
00:14:44,880 --> 00:14:47,279
classes of generalizable privacy defects

411
00:14:47,279 --> 00:14:50,160
i am guessing most of you recognize

412
00:14:50,160 --> 00:14:52,320
at least the the basic outline of some

413
00:14:52,320 --> 00:14:53,600
of these

414
00:14:53,600 --> 00:14:55,680
and some of these flaws result in bad

415
00:14:55,680 --> 00:14:58,959
outcomes that are that that we should be

416
00:14:58,959 --> 00:15:00,480
able to foresee basically because

417
00:15:00,480 --> 00:15:02,000
they're known from the security field in

418
00:15:02,000 --> 00:15:05,199
particular many of them um so the basics

419
00:15:05,199 --> 00:15:06,800
of privacy engineering doesn't you

420
00:15:06,800 --> 00:15:09,600
really seem to be widely disseminated as

421
00:15:09,600 --> 00:15:11,360
of yet at least not

422
00:15:11,360 --> 00:15:12,959
as sort of foundational software

423
00:15:12,959 --> 00:15:14,720
engineering and architectural knowledge

424
00:15:14,720 --> 00:15:16,880
at least in my experience

425
00:15:16,880 --> 00:15:18,240
and and i know

426
00:15:18,240 --> 00:15:20,399
privacy engineering is kind of an

427
00:15:20,399 --> 00:15:22,399
emerging field

428
00:15:22,399 --> 00:15:23,360
uh

429
00:15:23,360 --> 00:15:24,399
in and

430
00:15:24,399 --> 00:15:26,399
i know in the states uh it would

431
00:15:26,399 --> 00:15:28,079
typically hit the source of companies

432
00:15:28,079 --> 00:15:29,920
you see in this state in particular

433
00:15:29,920 --> 00:15:31,600
maybe uh before anyone else

434
00:15:31,600 --> 00:15:33,040
internationally speaking

435
00:15:33,040 --> 00:15:36,079
but it's certainly not established in in

436
00:15:36,079 --> 00:15:38,240
europe and not at all in norway as far

437
00:15:38,240 --> 00:15:41,440
as i'm concerned in fact you're you're

438
00:15:41,440 --> 00:15:43,519
you're lucky if it's

439
00:15:43,519 --> 00:15:46,240
if it's coverage to any large extent in

440
00:15:46,240 --> 00:15:48,240
in part of education

441
00:15:48,240 --> 00:15:49,920
um

442
00:15:49,920 --> 00:15:52,240
and and that's that's an issue i think

443
00:15:52,240 --> 00:15:53,600
a side note to that is maybe

444
00:15:53,600 --> 00:15:55,279
unsurprisingly the flaws that we've just

445
00:15:55,279 --> 00:15:56,639
looked at are

446
00:15:56,639 --> 00:15:58,240
many of them are in violation of the

447
00:15:58,240 --> 00:16:00,399
principles that you find in privacy by

448
00:16:00,399 --> 00:16:01,839
design

449
00:16:01,839 --> 00:16:04,800
not not maybe very

450
00:16:04,800 --> 00:16:06,240
surprising

451
00:16:06,240 --> 00:16:07,120
so

452
00:16:07,120 --> 00:16:09,360
some principles to abide by privacy by

453
00:16:09,360 --> 00:16:11,040
design obviously a big one architectural

454
00:16:11,040 --> 00:16:13,199
risk analysis and threat modeling other

455
00:16:13,199 --> 00:16:16,880
big ones and i think uh we as a field

456
00:16:16,880 --> 00:16:20,000
particularly in my direct experience

457
00:16:20,000 --> 00:16:22,800
is that we need to focus on developing

458
00:16:22,800 --> 00:16:24,800
things like taxonomies cheat sheets

459
00:16:24,800 --> 00:16:26,800
standards design patterns and

460
00:16:26,800 --> 00:16:29,839
architectural references in order to

461
00:16:29,839 --> 00:16:32,079
really to use those

462
00:16:32,079 --> 00:16:35,360
to use that phrase empower and enable

463
00:16:35,360 --> 00:16:37,920
other non-privacy

464
00:16:37,920 --> 00:16:40,560
experts technical resources and

465
00:16:40,560 --> 00:16:42,480
potentially be able to do the sort of

466
00:16:42,480 --> 00:16:44,320
shift left thing that you're seeing

467
00:16:44,320 --> 00:16:46,399
happening to a larger and larger extent

468
00:16:46,399 --> 00:16:47,920
in the security field

469
00:16:47,920 --> 00:16:49,680
it would probably also enable better

470
00:16:49,680 --> 00:16:51,920
communications between ourselves i think

471
00:16:51,920 --> 00:16:53,839
and i'm just going to skip past the

472
00:16:53,839 --> 00:16:55,519
references here because i'm pretty short

473
00:16:55,519 --> 00:16:56,560
on time

474
00:16:56,560 --> 00:16:58,000
and that's

475
00:16:58,000 --> 00:16:59,920
what i had prepared for my talk thank

476
00:16:59,920 --> 00:17:03,160
you for listening

477
00:17:08,559 --> 00:17:10,639
you

