1
00:00:09,760 --> 00:00:11,360
good afternoon everyone and thank you

2
00:00:11,360 --> 00:00:13,040
for coming to our talk on the problems

3
00:00:13,040 --> 00:00:14,799
introduced by child sexual abuse

4
00:00:14,799 --> 00:00:16,640
detection algorithms

5
00:00:16,640 --> 00:00:18,480
my name is tatiana ringenberg and i will

6
00:00:18,480 --> 00:00:20,480
be presenting today in conjunction with

7
00:00:20,480 --> 00:00:22,720
my collaborators lorraine kisselberg and

8
00:00:22,720 --> 00:00:25,279
gene camp

9
00:00:26,240 --> 00:00:28,160
child sexual abuse is a domain which has

10
00:00:28,160 --> 00:00:30,880
grown in both quantity and complexity

11
00:00:30,880 --> 00:00:33,440
during cobit 19 reports of child sexual

12
00:00:33,440 --> 00:00:36,000
abuse increased by more than 30 30

13
00:00:36,000 --> 00:00:38,239
percent for both online and physical

14
00:00:38,239 --> 00:00:39,680
offenses

15
00:00:39,680 --> 00:00:41,440
to combat this issue large tech

16
00:00:41,440 --> 00:00:43,360
companies have begun to develop products

17
00:00:43,360 --> 00:00:45,360
or policies intended to protect the

18
00:00:45,360 --> 00:00:47,120
safety of children

19
00:00:47,120 --> 00:00:49,520
while we agree it is necessary to combat

20
00:00:49,520 --> 00:00:51,920
this influx of child crime we see a gap

21
00:00:51,920 --> 00:00:53,760
with respect to considerations around

22
00:00:53,760 --> 00:00:55,680
the impact to the data privacy and

23
00:00:55,680 --> 00:00:57,840
safety of child sexual abuse survivors

24
00:00:57,840 --> 00:01:00,239
as well as society as a whole

25
00:01:00,239 --> 00:01:02,160
so in this presentation we are going to

26
00:01:02,160 --> 00:01:04,000
use the example of apple's expanded

27
00:01:04,000 --> 00:01:06,159
protections for children to illustrate

28
00:01:06,159 --> 00:01:07,920
the current concerns around such

29
00:01:07,920 --> 00:01:10,320
technologies

30
00:01:10,320 --> 00:01:12,400
so over the past year or so apple has

31
00:01:12,400 --> 00:01:14,400
introduced three forms of protections

32
00:01:14,400 --> 00:01:15,680
for children

33
00:01:15,680 --> 00:01:17,600
the first is parental controls which

34
00:01:17,600 --> 00:01:20,479
censor nude images being sent from or to

35
00:01:20,479 --> 00:01:22,320
a minor's device

36
00:01:22,320 --> 00:01:24,320
the second is guidance on seeking help

37
00:01:24,320 --> 00:01:26,640
for child sexual sexual abuse which

38
00:01:26,640 --> 00:01:28,159
would be available through multiple

39
00:01:28,159 --> 00:01:31,040
apple tools included including siri

40
00:01:31,040 --> 00:01:34,079
spotlight and safari search

41
00:01:34,079 --> 00:01:35,439
the third and most publicly

42
00:01:35,439 --> 00:01:37,520
controversial to date is the neural hash

43
00:01:37,520 --> 00:01:39,360
algorithm for detecting known child

44
00:01:39,360 --> 00:01:42,000
sexual abuse images in the cloud

45
00:01:42,000 --> 00:01:43,840
through machine learning the algorithm

46
00:01:43,840 --> 00:01:45,840
was to detect images harmful

47
00:01:45,840 --> 00:01:46,960
[Music]

48
00:01:46,960 --> 00:01:49,680
to children that existed in a database

49
00:01:49,680 --> 00:01:51,360
for the national center for missing and

50
00:01:51,360 --> 00:01:53,119
exploited children

51
00:01:53,119 --> 00:01:54,880
which had potentially been cropped or

52
00:01:54,880 --> 00:01:57,360
modified from the original images

53
00:01:57,360 --> 00:01:59,200
once this reached a certain threshold of

54
00:01:59,200 --> 00:02:00,320
images

55
00:02:00,320 --> 00:02:01,840
the images would be decrypted and

56
00:02:01,840 --> 00:02:04,240
verified by apple before being reported

57
00:02:04,240 --> 00:02:05,840
to authorities

58
00:02:05,840 --> 00:02:07,840
this algorithm was delayed this year and

59
00:02:07,840 --> 00:02:11,038
has been removed from their site

60
00:02:11,038 --> 00:02:13,040
apple's child protection solutions have

61
00:02:13,040 --> 00:02:14,959
come under scrutiny due to political

62
00:02:14,959 --> 00:02:18,879
technical and social implications

63
00:02:18,959 --> 00:02:21,200
from a political perspective researchers

64
00:02:21,200 --> 00:02:23,520
have expressed concern around the neural

65
00:02:23,520 --> 00:02:24,400
hash

66
00:02:24,400 --> 00:02:26,319
algorithm in that it breaks the privacy

67
00:02:26,319 --> 00:02:28,400
stance that apple has championed for

68
00:02:28,400 --> 00:02:29,840
years

69
00:02:29,840 --> 00:02:31,599
additionally it sets a precedent for

70
00:02:31,599 --> 00:02:33,360
future collaborations with government

71
00:02:33,360 --> 00:02:35,040
agencies

72
00:02:35,040 --> 00:02:37,200
from a technical perspective researchers

73
00:02:37,200 --> 00:02:38,720
and practitioners are concerned about

74
00:02:38,720 --> 00:02:40,560
breaking of end-to-end encryption

75
00:02:40,560 --> 00:02:43,280
modification of the existing database

76
00:02:43,280 --> 00:02:45,519
and potential for adversarial collisions

77
00:02:45,519 --> 00:02:48,160
with the hashes from the database

78
00:02:48,160 --> 00:02:50,400
finally from a social perspective

79
00:02:50,400 --> 00:02:52,080
it's unclear that the harms that are

80
00:02:52,080 --> 00:02:54,319
introduced and removed through uh it's

81
00:02:54,319 --> 00:02:56,400
unclear which harms are introduced and

82
00:02:56,400 --> 00:02:58,879
removed through apple's solution

83
00:02:58,879 --> 00:03:01,200
for instance parental notification of

84
00:03:01,200 --> 00:03:02,720
nude images could have severe

85
00:03:02,720 --> 00:03:04,800
consequences for minors depending on

86
00:03:04,800 --> 00:03:07,120
their family structure and beliefs

87
00:03:07,120 --> 00:03:09,360
additionally while consumption of child

88
00:03:09,360 --> 00:03:11,200
sexual abuse materials can be an

89
00:03:11,200 --> 00:03:14,480
indicator of abuse is not always

90
00:03:14,480 --> 00:03:15,920
by targeting individuals with

91
00:03:15,920 --> 00:03:18,000
pre-existing images apple is

92
00:03:18,000 --> 00:03:20,239
specifically targeting consumers and not

93
00:03:20,239 --> 00:03:21,840
necessarily the producers or

94
00:03:21,840 --> 00:03:24,879
distributors of such content

95
00:03:24,879 --> 00:03:26,959
additionally they may not be capturing

96
00:03:26,959 --> 00:03:28,640
individuals who are soliciting or

97
00:03:28,640 --> 00:03:31,120
enticing children so it's unclear how

98
00:03:31,120 --> 00:03:33,120
the specific type of crime and the

99
00:03:33,120 --> 00:03:35,519
material was chosen for child protection

100
00:03:35,519 --> 00:03:36,959
and whether or not it will have the

101
00:03:36,959 --> 00:03:39,200
biggest impact to safety for children

102
00:03:39,200 --> 00:03:41,280
good morning everyone afternoon

103
00:03:41,280 --> 00:03:43,280
um so what we want to talk about is what

104
00:03:43,280 --> 00:03:45,120
can we do to address some of these

105
00:03:45,120 --> 00:03:47,280
implications and we know that there's

106
00:03:47,280 --> 00:03:49,840
very significant focus on identifying

107
00:03:49,840 --> 00:03:52,400
and prosecuting the perpetrators of the

108
00:03:52,400 --> 00:03:53,599
crime

109
00:03:53,599 --> 00:03:56,000
but what we don't know as well is that

110
00:03:56,000 --> 00:03:58,959
csm detection technology triggers very

111
00:03:58,959 --> 00:04:01,200
intense privacy concerns for the

112
00:04:01,200 --> 00:04:03,519
survivors in addition to concerns about

113
00:04:03,519 --> 00:04:06,159
data security and we think this requires

114
00:04:06,159 --> 00:04:08,640
design and review practices that bring

115
00:04:08,640 --> 00:04:10,879
greater attention to the ethical risk

116
00:04:10,879 --> 00:04:14,080
and it centers the voices and expertise

117
00:04:14,080 --> 00:04:17,519
of the survivors in the design process

118
00:04:17,519 --> 00:04:19,519
and that means including stakeholders

119
00:04:19,519 --> 00:04:21,440
such as the survivors and victim

120
00:04:21,440 --> 00:04:24,560
advocates social scientists as well as

121
00:04:24,560 --> 00:04:26,960
privacy engineers and law enforcement

122
00:04:26,960 --> 00:04:29,919
experts so we suggest

123
00:04:29,919 --> 00:04:32,479
that we can actually find help by

124
00:04:32,479 --> 00:04:33,759
leveraging some of the ethical

125
00:04:33,759 --> 00:04:35,759
principles that are set forth in this

126
00:04:35,759 --> 00:04:38,080
case acm is one example

127
00:04:38,080 --> 00:04:40,560
their code of ethics and their 2018

128
00:04:40,560 --> 00:04:43,919
statement on preserving privacy

129
00:04:43,919 --> 00:04:46,720
and leverage those to analyze the risk

130
00:04:46,720 --> 00:04:49,360
of proposed systems and emphasize the

131
00:04:49,360 --> 00:04:51,280
values as the panel was just talking

132
00:04:51,280 --> 00:04:54,000
about before us that drive good privacy

133
00:04:54,000 --> 00:04:55,360
design

134
00:04:55,360 --> 00:04:58,560
so the acm code of ethics includes two

135
00:04:58,560 --> 00:05:00,800
overarching ethical principles among

136
00:05:00,800 --> 00:05:03,280
many others that i think is hurtful in

137
00:05:03,280 --> 00:05:06,160
this case the first among the first is

138
00:05:06,160 --> 00:05:08,960
avoid harm a principle that is

139
00:05:08,960 --> 00:05:10,400
well known to everyone

140
00:05:10,400 --> 00:05:12,639
and it states in part and i quote

141
00:05:12,639 --> 00:05:15,039
well-intended actions including those

142
00:05:15,039 --> 00:05:18,240
that accomplish assigned duties may lead

143
00:05:18,240 --> 00:05:21,680
to harm and when that harm is unintended

144
00:05:21,680 --> 00:05:24,479
those responsible are obliged to undo or

145
00:05:24,479 --> 00:05:27,280
mitigate the harm as much as possible

146
00:05:27,280 --> 00:05:29,280
and avoiding harm begins with a careful

147
00:05:29,280 --> 00:05:32,160
consideration of potential impacts on

148
00:05:32,160 --> 00:05:35,360
all those affected by decisions

149
00:05:35,360 --> 00:05:37,360
and the second one of course is respect

150
00:05:37,360 --> 00:05:38,960
privacy and it states that the

151
00:05:38,960 --> 00:05:40,960
responsibility of respecting privacy

152
00:05:40,960 --> 00:05:43,120
applies to computing professionals who

153
00:05:43,120 --> 00:05:45,120
should only use personal information for

154
00:05:45,120 --> 00:05:48,000
legitimate ends and without violating

155
00:05:48,000 --> 00:05:50,400
the rights of individuals and it also

156
00:05:50,400 --> 00:05:52,000
warns about the risk of

157
00:05:52,000 --> 00:05:54,880
re-identification of anonymized data

158
00:05:54,880 --> 00:05:56,960
unauthorized data collection

159
00:05:56,960 --> 00:06:00,000
and unauthorized access and accidental

160
00:06:00,000 --> 00:06:01,840
disclosure

161
00:06:01,840 --> 00:06:04,000
so specifically more specifically there

162
00:06:04,000 --> 00:06:05,840
are a set of ten principles that were

163
00:06:05,840 --> 00:06:08,720
published by acm in 2018

164
00:06:08,720 --> 00:06:11,680
from the u.s technology policy committee

165
00:06:11,680 --> 00:06:13,680
regarding the protection of privacy and

166
00:06:13,680 --> 00:06:16,319
we think this can also help guide design

167
00:06:16,319 --> 00:06:17,520
and oversight

168
00:06:17,520 --> 00:06:19,280
so many of these principles are familiar

169
00:06:19,280 --> 00:06:21,759
to you probably and they're

170
00:06:21,759 --> 00:06:23,600
mentioned by many others who have done

171
00:06:23,600 --> 00:06:26,000
something similar including fairness

172
00:06:26,000 --> 00:06:28,960
accountability and transparency and

173
00:06:28,960 --> 00:06:31,440
they've subsequently been encoded into

174
00:06:31,440 --> 00:06:33,919
international policy instruments from

175
00:06:33,919 --> 00:06:35,120
oecd

176
00:06:35,120 --> 00:06:36,880
endorsed by international government

177
00:06:36,880 --> 00:06:40,000
leaders from the g7 and g20 and i'd like

178
00:06:40,000 --> 00:06:41,680
to highlight just four of those pretty

179
00:06:41,680 --> 00:06:43,360
quickly to illustrate some of the

180
00:06:43,360 --> 00:06:45,280
specific privacy issues that arrived by

181
00:06:45,280 --> 00:06:46,960
the csam case

182
00:06:46,960 --> 00:06:49,120
the first is transparency

183
00:06:49,120 --> 00:06:51,440
so the transparency principle is very

184
00:06:51,440 --> 00:06:52,960
important in drawing attention to the

185
00:06:52,960 --> 00:06:55,680
need for very clear information in the

186
00:06:55,680 --> 00:06:58,319
collection and use of personal data

187
00:06:58,319 --> 00:07:01,199
including to whom it may be disclosed

188
00:07:01,199 --> 00:07:02,639
and why

189
00:07:02,639 --> 00:07:04,720
however while this level of detail

190
00:07:04,720 --> 00:07:07,199
transparency is necessary such

191
00:07:07,199 --> 00:07:09,520
disclosures might occur in an

192
00:07:09,520 --> 00:07:12,080
environment where sharing the consent

193
00:07:12,080 --> 00:07:14,639
itself excuse me sharing the content

194
00:07:14,639 --> 00:07:17,360
itself can be harmful for example

195
00:07:17,360 --> 00:07:20,319
notifying parents and disclosing images

196
00:07:20,319 --> 00:07:23,360
can place teens at harm if they're in a

197
00:07:23,360 --> 00:07:25,680
violent environment

198
00:07:25,680 --> 00:07:27,680
the second principle regarding

199
00:07:27,680 --> 00:07:29,840
individual control of course is grounded

200
00:07:29,840 --> 00:07:32,319
essentially in the ethic of autonomy and

201
00:07:32,319 --> 00:07:34,000
is important in drawing attention to the

202
00:07:34,000 --> 00:07:36,400
rights of the individual to consent to

203
00:07:36,400 --> 00:07:38,160
the use of their data as well as to

204
00:07:38,160 --> 00:07:40,880
specify the limits and restrictions on

205
00:07:40,880 --> 00:07:43,360
how their data is collected shared and

206
00:07:43,360 --> 00:07:44,560
transferred

207
00:07:44,560 --> 00:07:47,360
and in the case of csam detection system

208
00:07:47,360 --> 00:07:49,840
this includes decisions about

209
00:07:49,840 --> 00:07:52,960
how and to whom authorities can share

210
00:07:52,960 --> 00:07:55,440
images of individuals or even whether

211
00:07:55,440 --> 00:07:58,160
these images can be stored in a database

212
00:07:58,160 --> 00:08:00,560
for indexing future abuse

213
00:08:00,560 --> 00:08:02,400
and in the apple case they propose to

214
00:08:02,400 --> 00:08:04,479
notify parents about material that was

215
00:08:04,479 --> 00:08:06,960
discovered creating as i mentioned

216
00:08:06,960 --> 00:08:09,280
earlier significant harms and violations

217
00:08:09,280 --> 00:08:11,120
of privacy and even the risk of

218
00:08:11,120 --> 00:08:14,080
punishment from those parental guardians

219
00:08:14,080 --> 00:08:15,759
the next principle has to do with data

220
00:08:15,759 --> 00:08:17,759
security and it provides important

221
00:08:17,759 --> 00:08:19,840
guidelines for protecting against loss

222
00:08:19,840 --> 00:08:22,400
and misuse as well as unauthorized

223
00:08:22,400 --> 00:08:25,919
disclosure and it advises continuous

224
00:08:25,919 --> 00:08:28,240
auditing of access use in the

225
00:08:28,240 --> 00:08:30,560
maintenance of data

226
00:08:30,560 --> 00:08:33,039
but the csm case illustrates another

227
00:08:33,039 --> 00:08:34,880
important consideration here and that's

228
00:08:34,880 --> 00:08:37,200
the very act of creating and storing

229
00:08:37,200 --> 00:08:41,039
databases for suspected csam material

230
00:08:41,039 --> 00:08:43,360
creates a harmful risk to the security

231
00:08:43,360 --> 00:08:45,760
of individuals depending upon who is

232
00:08:45,760 --> 00:08:49,519
given authorized access to the data

233
00:08:49,519 --> 00:08:51,519
for example in nations with

234
00:08:51,519 --> 00:08:54,080
authoritarian regimes they can enforce

235
00:08:54,080 --> 00:08:56,000
mandates and include images that are

236
00:08:56,000 --> 00:08:58,560
being politically offensive for

237
00:08:58,560 --> 00:09:00,959
identification and incarceration of

238
00:09:00,959 --> 00:09:02,880
those individuals

239
00:09:02,880 --> 00:09:06,080
nations and theocratic regimes may use

240
00:09:06,080 --> 00:09:08,959
authorized access to repress information

241
00:09:08,959 --> 00:09:12,320
related to women's reproductive health

242
00:09:12,320 --> 00:09:14,800
so the last pair of principles that i

243
00:09:14,800 --> 00:09:16,320
mentioned very quickly have to do with

244
00:09:16,320 --> 00:09:18,720
accountability and risk management

245
00:09:18,720 --> 00:09:20,560
related to one another

246
00:09:20,560 --> 00:09:23,200
and guideline for auditing and a risk

247
00:09:23,200 --> 00:09:25,200
assessment for example using privacy

248
00:09:25,200 --> 00:09:27,600
impact assessments or ethical impact

249
00:09:27,600 --> 00:09:29,839
assessments but such continuous

250
00:09:29,839 --> 00:09:32,000
oversight throughout the design life

251
00:09:32,000 --> 00:09:34,000
cycle as well as before and during

252
00:09:34,000 --> 00:09:36,160
deployment provides the framework to

253
00:09:36,160 --> 00:09:38,560
evaluate and mitigate the risk and the

254
00:09:38,560 --> 00:09:41,120
unintended consequences provided they

255
00:09:41,120 --> 00:09:43,519
include the participation and the

256
00:09:43,519 --> 00:09:46,160
expertise and the knowledge of survivors

257
00:09:46,160 --> 00:09:48,240
and victim advocates

258
00:09:48,240 --> 00:09:51,120
so in collusion we feel that the csam

259
00:09:51,120 --> 00:09:53,279
case illustrates how designed for a very

260
00:09:53,279 --> 00:09:56,240
narrow singular purpose identifying and

261
00:09:56,240 --> 00:09:57,760
prosecuting

262
00:09:57,760 --> 00:10:00,320
perpetrators of crime

263
00:10:00,320 --> 00:10:02,800
can have unintended

264
00:10:02,800 --> 00:10:04,480
consequences and violate the human

265
00:10:04,480 --> 00:10:07,120
rights of individuals with sometimes

266
00:10:07,120 --> 00:10:10,480
devastating and even fatal consequences

267
00:10:10,480 --> 00:10:13,120
so this demands that we shape and design

268
00:10:13,120 --> 00:10:15,040
oversight practices that help us to

269
00:10:15,040 --> 00:10:17,040
consider broader perspective and

270
00:10:17,040 --> 00:10:19,200
identify these potential risks during

271
00:10:19,200 --> 00:10:21,600
design before deployment and

272
00:10:21,600 --> 00:10:23,519
continuously thereafter

273
00:10:23,519 --> 00:10:25,200
and we suggest that creating the

274
00:10:25,200 --> 00:10:27,120
infrastructure to mitigate the harm to

275
00:10:27,120 --> 00:10:28,079
children

276
00:10:28,079 --> 00:10:30,399
teens and other vulnerable populations

277
00:10:30,399 --> 00:10:32,640
requires a balance intercourse of the

278
00:10:32,640 --> 00:10:34,880
protection of privacy with important

279
00:10:34,880 --> 00:10:37,200
surveillance and enforcement elements

280
00:10:37,200 --> 00:10:40,640
and a risk-based participatory design

281
00:10:40,640 --> 00:10:43,040
approach that involves multiple

282
00:10:43,040 --> 00:10:46,079
stakeholders by participatory design i

283
00:10:46,079 --> 00:10:48,240
know it's not unknown to you we mean

284
00:10:48,240 --> 00:10:50,320
broadening the stakeholders to include

285
00:10:50,320 --> 00:10:53,839
survivors advocates social scientists as

286
00:10:53,839 --> 00:10:55,680
well as privacy engineers and law

287
00:10:55,680 --> 00:10:58,320
enforcement and re-center

288
00:10:58,320 --> 00:11:01,519
individual autonomy and data protection

289
00:11:01,519 --> 00:11:02,959
and we want to move away from an

290
00:11:02,959 --> 00:11:06,320
enforcement model to a risk-based model

291
00:11:06,320 --> 00:11:08,959
including continuous audits to evaluate

292
00:11:08,959 --> 00:11:11,519
those unintended risks and harms and

293
00:11:11,519 --> 00:11:13,200
finally encouraging the funding of

294
00:11:13,200 --> 00:11:15,120
research there's still many untested

295
00:11:15,120 --> 00:11:17,360
assumptions and unanswered questions in

296
00:11:17,360 --> 00:11:20,160
the field so in closing i thank you for

297
00:11:20,160 --> 00:11:22,880
your attention tatiana and i both do and

298
00:11:22,880 --> 00:11:25,040
would also like to thank the conference

299
00:11:25,040 --> 00:11:26,560
organizers and the staff for their

300
00:11:26,560 --> 00:11:27,680
support

301
00:11:27,680 --> 00:11:29,200
as well as the support that we have

302
00:11:29,200 --> 00:11:31,600
specifically received from the computer

303
00:11:31,600 --> 00:11:34,640
research association ci fellows project

304
00:11:34,640 --> 00:11:36,399
and also the opportunity to work

305
00:11:36,399 --> 00:11:38,399
together the three of us on this issue

306
00:11:38,399 --> 00:11:39,920
through our membership through acm's

307
00:11:39,920 --> 00:11:43,279
tech policy committee and i'm happy to

308
00:11:43,279 --> 00:11:44,880
address any questions all three of us

309
00:11:44,880 --> 00:11:46,480
and i'm going to invite our third author

310
00:11:46,480 --> 00:11:49,360
up gen camp from indiana university to

311
00:11:49,360 --> 00:11:53,160
join me for these questions

312
00:12:00,079 --> 00:12:02,160
you

