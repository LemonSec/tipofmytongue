1
00:00:10,160 --> 00:00:12,639
ryan and i will be discussing about how

2
00:00:12,639 --> 00:00:14,320
twitter refused elasticsearch with

3
00:00:14,320 --> 00:00:16,320
neural networks to identify personal

4
00:00:16,320 --> 00:00:17,840
data

5
00:00:17,840 --> 00:00:19,760
before we dive into the work itself a

6
00:00:19,760 --> 00:00:21,680
quick introduction about the speakers so

7
00:00:21,680 --> 00:00:23,760
hi i'm rakshas bhattabad the one on the

8
00:00:23,760 --> 00:00:25,680
right and i'm a senior software engineer

9
00:00:25,680 --> 00:00:27,680
at twitter i have been working at

10
00:00:27,680 --> 00:00:29,920
twitter since past three years now and

11
00:00:29,920 --> 00:00:31,760
i'm part of the privacy platform team

12
00:00:31,760 --> 00:00:34,559
here and we build tools and services for

13
00:00:34,559 --> 00:00:36,800
data protection and privacy along with

14
00:00:36,800 --> 00:00:38,879
me is ryan turner the one on the left

15
00:00:38,879 --> 00:00:41,040
and ryan is a senior machine learning

16
00:00:41,040 --> 00:00:42,879
researcher at twitter he has been a

17
00:00:42,879 --> 00:00:44,640
twitter since past two and a half years

18
00:00:44,640 --> 00:00:47,360
now and he is part of the cortex team

19
00:00:47,360 --> 00:00:49,039
cortex team provides machine learning

20
00:00:49,039 --> 00:00:51,120
solutions and expertise for various

21
00:00:51,120 --> 00:00:53,840
projects across twitter

22
00:00:53,840 --> 00:00:55,440
so let's begin with an introduction

23
00:00:55,440 --> 00:00:57,280
about what are the problems that we are

24
00:00:57,280 --> 00:01:00,000
trying to solve here

25
00:01:00,000 --> 00:01:01,600
so the main problem that we are trying

26
00:01:01,600 --> 00:01:03,520
to handle here is the counting for the

27
00:01:03,520 --> 00:01:06,240
personal data in various twitter systems

28
00:01:06,240 --> 00:01:08,320
and then tracing it how it originates

29
00:01:08,320 --> 00:01:09,680
and flows through various twitter

30
00:01:09,680 --> 00:01:11,040
datasets

31
00:01:11,040 --> 00:01:12,960
the problem is complicated because of

32
00:01:12,960 --> 00:01:14,799
the micro service architecture across

33
00:01:14,799 --> 00:01:16,960
the company this leads to distribution

34
00:01:16,960 --> 00:01:19,360
of data across different teams and

35
00:01:19,360 --> 00:01:21,920
services as a result the accountability

36
00:01:21,920 --> 00:01:23,759
for the data privacy is distributed

37
00:01:23,759 --> 00:01:25,840
throughout the

38
00:01:25,840 --> 00:01:28,159
to understand the distributed data we

39
00:01:28,159 --> 00:01:30,799
need a common language or a taxonomy to

40
00:01:30,799 --> 00:01:33,119
refer to the data columns we call

41
00:01:33,119 --> 00:01:34,880
tagging of these data columns with the

42
00:01:34,880 --> 00:01:38,960
standard taxonomy as annotations

43
00:01:39,119 --> 00:01:40,960
the two main challenges in tracing and

44
00:01:40,960 --> 00:01:42,880
accounting personal data are first the

45
00:01:42,880 --> 00:01:44,799
data is continuously growing and getting

46
00:01:44,799 --> 00:01:46,640
distributed to various datasets and

47
00:01:46,640 --> 00:01:48,240
storage systems

48
00:01:48,240 --> 00:01:50,240
these datasets and storage systems can

49
00:01:50,240 --> 00:01:53,200
be very non-homogeneous second

50
00:01:53,200 --> 00:01:55,200
the data privacy and governing policies

51
00:01:55,200 --> 00:01:57,280
are continuously evolving and twitter as

52
00:01:57,280 --> 00:01:59,439
an organization needs to adhere to these

53
00:01:59,439 --> 00:02:01,439
ever-changing policies

54
00:02:01,439 --> 00:02:03,119
keeping these challenges in mind we

55
00:02:03,119 --> 00:02:04,960
started with the following ones

56
00:02:04,960 --> 00:02:07,840
primarily we wanted to understand

57
00:02:07,840 --> 00:02:09,919
what data exists in our systems the

58
00:02:09,919 --> 00:02:12,480
sensitivity of that data and the purpose

59
00:02:12,480 --> 00:02:14,080
for which the data is getting used

60
00:02:14,080 --> 00:02:15,599
across tutor

61
00:02:15,599 --> 00:02:17,599
our secondary goal was to utilize this

62
00:02:17,599 --> 00:02:18,560
data

63
00:02:18,560 --> 00:02:21,120
to optimize our storages understanding

64
00:02:21,120 --> 00:02:24,080
the uses pattern for different data sets

65
00:02:24,080 --> 00:02:26,319
and to improve overall data security and

66
00:02:26,319 --> 00:02:29,360
data handling situation

67
00:02:29,360 --> 00:02:31,040
the ideal solution to solve such a

68
00:02:31,040 --> 00:02:32,480
problem could have been to use a

69
00:02:32,480 --> 00:02:34,400
standard taxonomy within the cms

70
00:02:34,400 --> 00:02:36,640
themselves that is to name the columns

71
00:02:36,640 --> 00:02:38,239
with personal data directly from the

72
00:02:38,239 --> 00:02:40,640
taxonomy this might have worked when you

73
00:02:40,640 --> 00:02:42,959
are building something from scratch but

74
00:02:42,959 --> 00:02:45,120
changing schemas for the existing data

75
00:02:45,120 --> 00:02:46,959
set would not only lead to heavy

76
00:02:46,959 --> 00:02:49,040
refactoring across the consumers and

77
00:02:49,040 --> 00:02:51,920
producers of the data but would also be

78
00:02:51,920 --> 00:02:54,000
a long and painful process

79
00:02:54,000 --> 00:02:55,599
which might even result into downtime

80
00:02:55,599 --> 00:02:58,319
for different services and above all

81
00:02:58,319 --> 00:03:00,480
this could have been a very error prone

82
00:03:00,480 --> 00:03:03,120
and manual process

83
00:03:03,120 --> 00:03:04,800
now coming to the solution that we

84
00:03:04,800 --> 00:03:06,959
actually adopted instead of changing the

85
00:03:06,959 --> 00:03:08,720
column or field names in the dataset

86
00:03:08,720 --> 00:03:11,519
schemas directly we decided to tag or

87
00:03:11,519 --> 00:03:14,080
annotate the columns with certain

88
00:03:14,080 --> 00:03:16,000
terms and these terms come from the

89
00:03:16,000 --> 00:03:17,760
standard taxonomy

90
00:03:17,760 --> 00:03:20,080
this has multiple benefits

91
00:03:20,080 --> 00:03:22,000
it is done on top of the data sets and

92
00:03:22,000 --> 00:03:24,159
hence doesn't need any refactoring of

93
00:03:24,159 --> 00:03:26,480
the actual data sets themselves

94
00:03:26,480 --> 00:03:28,319
this also means no downtime for the

95
00:03:28,319 --> 00:03:30,480
existing tools and services

96
00:03:30,480 --> 00:03:32,879
further the design allows

97
00:03:32,879 --> 00:03:34,400
changing the annotation or adding

98
00:03:34,400 --> 00:03:36,720
multiple annotations to the same feed

99
00:03:36,720 --> 00:03:38,319
which would not have been possible if

100
00:03:38,319 --> 00:03:39,760
you made the changes to the schemas

101
00:03:39,760 --> 00:03:41,200
directly

102
00:03:41,200 --> 00:03:43,120
this also means that we can automate the

103
00:03:43,120 --> 00:03:45,680
annotation process to a major degree by

104
00:03:45,680 --> 00:03:47,599
building a recommendation service for

105
00:03:47,599 --> 00:03:50,159
the annotations

106
00:03:50,159 --> 00:03:52,319
so to make the annotation process easier

107
00:03:52,319 --> 00:03:54,480
we built a recommendation engine based

108
00:03:54,480 --> 00:03:56,400
on probabilistic model on top of

109
00:03:56,400 --> 00:03:58,080
elasticsearch

110
00:03:58,080 --> 00:03:59,760
now that such a system is not only

111
00:03:59,760 --> 00:04:02,159
useful for any company

112
00:04:02,159 --> 00:04:04,720
handling a lot of personal data but also

113
00:04:04,720 --> 00:04:06,879
for any time you need a text based

114
00:04:06,879 --> 00:04:10,239
lookup within a quantified uncertainty

115
00:04:10,239 --> 00:04:12,080
we will go into much more detail about

116
00:04:12,080 --> 00:04:14,080
the recommendation engine as the second

117
00:04:14,080 --> 00:04:17,040
half of this presentation

118
00:04:17,040 --> 00:04:19,358
to make it more concrete here are some

119
00:04:19,358 --> 00:04:21,358
annotation examples let's say that we

120
00:04:21,358 --> 00:04:23,759
have a data set name profile that has a

121
00:04:23,759 --> 00:04:26,160
column called id which has a column

122
00:04:26,160 --> 00:04:28,080
description as unique identifier for the

123
00:04:28,080 --> 00:04:29,040
user

124
00:04:29,040 --> 00:04:32,240
here this column is annotated as user id

125
00:04:32,240 --> 00:04:34,240
and in the absence of the annotation it

126
00:04:34,240 --> 00:04:36,400
is very tough to determine what exactly

127
00:04:36,400 --> 00:04:39,280
does this id mean for this data set

128
00:04:39,280 --> 00:04:41,680
another example could be a data set

129
00:04:41,680 --> 00:04:43,840
called timeline which has a column name

130
00:04:43,840 --> 00:04:46,240
called user id which actually stores the

131
00:04:46,240 --> 00:04:47,680
twitter handles

132
00:04:47,680 --> 00:04:50,080
and hence is annotated by username now

133
00:04:50,080 --> 00:04:54,560
the column name is user id in the schema

134
00:04:55,040 --> 00:04:57,600
so overall an annotation recommendation

135
00:04:57,600 --> 00:04:59,120
service not only increases the

136
00:04:59,120 --> 00:05:02,320
development velocity it also helps to

137
00:05:02,320 --> 00:05:04,400
maintain the quality of annotation

138
00:05:04,400 --> 00:05:06,320
through benchmarking and auditing

139
00:05:06,320 --> 00:05:09,440
previously annotated data

140
00:05:09,440 --> 00:05:11,440
here are a few numbers to understand why

141
00:05:11,440 --> 00:05:13,520
we need a recommendation engine rather

142
00:05:13,520 --> 00:05:15,039
than manually annotating from the

143
00:05:15,039 --> 00:05:16,160
scratch

144
00:05:16,160 --> 00:05:19,759
we have 504 terms in a standard taxonomy

145
00:05:19,759 --> 00:05:23,120
we call these terms personal data types

146
00:05:23,120 --> 00:05:25,120
there are more than two million fields

147
00:05:25,120 --> 00:05:26,960
or columns spread across more than

148
00:05:26,960 --> 00:05:28,880
hundred thousand data sets and a dozen

149
00:05:28,880 --> 00:05:31,120
of data stores

150
00:05:31,120 --> 00:05:33,039
these two million fields

151
00:05:33,039 --> 00:05:35,759
needs to be annotated with these 504

152
00:05:35,759 --> 00:05:38,240
personal data types when dealing at this

153
00:05:38,240 --> 00:05:40,400
scale having a recommendation engine for

154
00:05:40,400 --> 00:05:42,160
these personal data types can be very

155
00:05:42,160 --> 00:05:44,320
helpful

156
00:05:44,320 --> 00:05:45,759
now a quick overview of the

157
00:05:45,759 --> 00:05:47,840
recommendation engine before we go into

158
00:05:47,840 --> 00:05:50,160
more detail twitter manually created a

159
00:05:50,160 --> 00:05:53,199
taxonomy of 504 possible and adaptations

160
00:05:53,199 --> 00:05:55,919
the taxonomy was created by combined

161
00:05:55,919 --> 00:05:57,759
efforts of engineering product

162
00:05:57,759 --> 00:06:00,319
management and legal teams engineers and

163
00:06:00,319 --> 00:06:02,479
product managers then used this taxonomy

164
00:06:02,479 --> 00:06:04,240
to manually annotate datasets for a

165
00:06:04,240 --> 00:06:06,080
while around thirty thousand columns

166
00:06:06,080 --> 00:06:08,560
were annotated this way then this became

167
00:06:08,560 --> 00:06:09,919
a basis for

168
00:06:09,919 --> 00:06:12,720
creating a recommendation engine which

169
00:06:12,720 --> 00:06:14,400
was used to annotate over two million

170
00:06:14,400 --> 00:06:15,440
columns

171
00:06:15,440 --> 00:06:17,840
spread across hundred thousand data sets

172
00:06:17,840 --> 00:06:20,479
and these were annotated to these 504

173
00:06:20,479 --> 00:06:22,240
annotations

174
00:06:22,240 --> 00:06:24,080
note that the final annotation is still

175
00:06:24,080 --> 00:06:26,160
done by the user as this annotation

176
00:06:26,160 --> 00:06:28,000
needs to be accurate due to various

177
00:06:28,000 --> 00:06:31,919
privacy and data covariance policies

178
00:06:31,919 --> 00:06:34,000
and now quick overview of the high level

179
00:06:34,000 --> 00:06:35,360
architecture for the annotation

180
00:06:35,360 --> 00:06:37,039
recommendation service

181
00:06:37,039 --> 00:06:38,960
at the bottom is the data collection

182
00:06:38,960 --> 00:06:40,960
engine that collects con that collects

183
00:06:40,960 --> 00:06:43,360
column and personal data type mapping as

184
00:06:43,360 --> 00:06:45,120
well as other metadata

185
00:06:45,120 --> 00:06:46,840
about the data sets which are already

186
00:06:46,840 --> 00:06:49,280
annotated this essentially act as a

187
00:06:49,280 --> 00:06:51,039
training data set for the recommendation

188
00:06:51,039 --> 00:06:52,800
engine the recommendation engine is

189
00:06:52,800 --> 00:06:55,120
based on a probabilistic model on top of

190
00:06:55,120 --> 00:06:56,639
elasticsearch

191
00:06:56,639 --> 00:06:58,560
and the recommendation engine interacts

192
00:06:58,560 --> 00:07:00,479
with our internal services and tools

193
00:07:00,479 --> 00:07:03,039
through the rest apis these test apis

194
00:07:03,039 --> 00:07:05,199
integrate into our tools like metadata

195
00:07:05,199 --> 00:07:08,720
stores code review tools linters etc to

196
00:07:08,720 --> 00:07:10,800
make it easy to annotate the new data as

197
00:07:10,800 --> 00:07:12,720
well as search for already annotated

198
00:07:12,720 --> 00:07:14,000
data

199
00:07:14,000 --> 00:07:16,319
and now hand it over to ryan to give a

200
00:07:16,319 --> 00:07:19,039
deep dive into our recommendation engine

201
00:07:19,039 --> 00:07:21,919
over to you ryan

202
00:07:24,240 --> 00:07:26,400
hello i'm i'm ryan i'm going to talk

203
00:07:26,400 --> 00:07:28,400
about the

204
00:07:28,400 --> 00:07:31,039
ml specific parts of the system now

205
00:07:31,039 --> 00:07:35,440
uh for our automated recommender

206
00:07:36,479 --> 00:07:39,199
so uh the first part of any aml problem

207
00:07:39,199 --> 00:07:40,960
is to frame the problem

208
00:07:40,960 --> 00:07:42,800
uh so here we're trying to build an

209
00:07:42,800 --> 00:07:45,759
automatic data annotation system

210
00:07:45,759 --> 00:07:47,919
so the first step was to get some ground

211
00:07:47,919 --> 00:07:50,639
truth labels as to

212
00:07:50,639 --> 00:07:52,400
what are some examples

213
00:07:52,400 --> 00:07:55,759
of uh you know labels that we can trust

214
00:07:55,759 --> 00:07:58,960
so we have quite a you know large data

215
00:07:58,960 --> 00:08:02,160
set of annotations yeah only a subset

216
00:08:02,160 --> 00:08:05,759
about 70 000 of them were used

217
00:08:05,759 --> 00:08:08,000
as ground truth because we only sub

218
00:08:08,000 --> 00:08:10,879
selected ones that we trusted

219
00:08:10,879 --> 00:08:13,280
would be accurate ground truth

220
00:08:13,280 --> 00:08:16,319
um so the basic setup of the system is

221
00:08:16,319 --> 00:08:18,720
to take a variety of inputs uh and we

222
00:08:18,720 --> 00:08:22,240
used uh metadata on every column uh such

223
00:08:22,240 --> 00:08:24,479
we'll name

224
00:08:24,479 --> 00:08:26,960
of the data set name of the column

225
00:08:26,960 --> 00:08:29,440
and then descriptions uh for these that

226
00:08:29,440 --> 00:08:32,399
are sometimes written in as metadata

227
00:08:32,399 --> 00:08:33,919
and these are all

228
00:08:33,919 --> 00:08:36,080
concatenated together as a big feature

229
00:08:36,080 --> 00:08:37,519
vector of

230
00:08:37,519 --> 00:08:40,080
unstructured text basically

231
00:08:40,080 --> 00:08:44,480
and then they'll be used to predict uh a

232
00:08:44,480 --> 00:08:47,279
label of one of the 500 different types

233
00:08:47,279 --> 00:08:48,880
of annotations

234
00:08:48,880 --> 00:08:50,720
so this is basically a classification

235
00:08:50,720 --> 00:08:53,600
problem and

236
00:08:53,600 --> 00:08:55,120
with an interesting feature space

237
00:08:55,120 --> 00:08:59,120
because it's this structure text

238
00:09:00,880 --> 00:09:03,440
so representing that same

239
00:09:03,440 --> 00:09:07,519
slide before in a more diagram form

240
00:09:07,519 --> 00:09:09,760
uh here are some examples

241
00:09:09,760 --> 00:09:13,360
of uh classifications of a few different

242
00:09:13,360 --> 00:09:15,360
annotations so here we have the user id

243
00:09:15,360 --> 00:09:18,959
and username on our in bold this is on

244
00:09:18,959 --> 00:09:19,680
the

245
00:09:19,680 --> 00:09:21,600
bold side on the right is the side we

246
00:09:21,600 --> 00:09:23,600
are trying to predict so user id and

247
00:09:23,600 --> 00:09:25,600
username are examples of different

248
00:09:25,600 --> 00:09:27,120
annotations it sounds redundant but

249
00:09:27,120 --> 00:09:28,880
they're actually not the same

250
00:09:28,880 --> 00:09:30,399
um

251
00:09:30,399 --> 00:09:32,000
and here these are the features so we

252
00:09:32,000 --> 00:09:33,920
have to we have a name of the data set

253
00:09:33,920 --> 00:09:35,519
which is say timelines and then of

254
00:09:35,519 --> 00:09:37,760
course your word we name the column user

255
00:09:37,760 --> 00:09:39,120
id but of course it's very subtle

256
00:09:39,120 --> 00:09:41,519
because the actual uh true label in this

257
00:09:41,519 --> 00:09:44,640
case is user name not user id

258
00:09:44,640 --> 00:09:46,560
and then here there's a

259
00:09:46,560 --> 00:09:49,360
description which is twitter handle uh

260
00:09:49,360 --> 00:09:50,959
and this is sort of the clue that the

261
00:09:50,959 --> 00:09:52,959
system might be going off of to

262
00:09:52,959 --> 00:09:54,640
determine that it's a

263
00:09:54,640 --> 00:09:57,680
a username instead of user id

264
00:09:57,680 --> 00:10:00,800
so these uh text features on the left

265
00:10:00,800 --> 00:10:03,279
are what the system has to go off of to

266
00:10:03,279 --> 00:10:04,560
make a prediction

267
00:10:04,560 --> 00:10:05,839
and then

268
00:10:05,839 --> 00:10:07,440
some of the labels on the right under

269
00:10:07,440 --> 00:10:10,000
the annotation column are examples

270
00:10:10,000 --> 00:10:12,079
of the labels we're trying to predict

271
00:10:12,079 --> 00:10:16,160
and these are all part of that

272
00:10:16,160 --> 00:10:19,040
list of 500 different annotations so we

273
00:10:19,040 --> 00:10:21,760
have a closed

274
00:10:21,760 --> 00:10:23,839
set of possibilities

275
00:10:23,839 --> 00:10:26,560
um for the labels on the right whereas

276
00:10:26,560 --> 00:10:28,880
on the left these can be just

277
00:10:28,880 --> 00:10:31,440
pre-open-ended text

278
00:10:31,440 --> 00:10:33,519
for for the possible

279
00:10:33,519 --> 00:10:34,720
uh

280
00:10:34,720 --> 00:10:38,000
the possible input features

281
00:10:38,160 --> 00:10:41,740
so we wanted to use systems uh that were

282
00:10:41,740 --> 00:10:43,920
[Music]

283
00:10:43,920 --> 00:10:46,079
already built try to leverage existing

284
00:10:46,079 --> 00:10:47,440
systems so we didn't have to reinvent

285
00:10:47,440 --> 00:10:48,560
the wheel at all

286
00:10:48,560 --> 00:10:50,640
we made the choice in this particular

287
00:10:50,640 --> 00:10:51,680
case

288
00:10:51,680 --> 00:10:54,399
to use elasticsearch

289
00:10:54,399 --> 00:10:55,730
which is a system for

290
00:10:55,730 --> 00:10:57,760
[Music]

291
00:10:57,760 --> 00:11:00,880
retrieving documents uh based on acute a

292
00:11:00,880 --> 00:11:03,680
few key words that may be similar to

293
00:11:03,680 --> 00:11:06,720
things in the documents um

294
00:11:06,720 --> 00:11:09,200
so basically we need to do a reduction

295
00:11:09,200 --> 00:11:09,920
and

296
00:11:09,920 --> 00:11:11,680
in computer science terms here we need

297
00:11:11,680 --> 00:11:14,880
to do reduction to convert

298
00:11:14,880 --> 00:11:16,880
this classification problem to the

299
00:11:16,880 --> 00:11:18,079
document retrieval problem that

300
00:11:18,079 --> 00:11:21,120
elasticsearch solves so basically every

301
00:11:21,120 --> 00:11:23,440
single annotation had to become a sort

302
00:11:23,440 --> 00:11:25,360
of document

303
00:11:25,360 --> 00:11:27,440
and then we would do queries at test

304
00:11:27,440 --> 00:11:28,959
time

305
00:11:28,959 --> 00:11:31,040
so basically

306
00:11:31,040 --> 00:11:32,160
we sort of

307
00:11:32,160 --> 00:11:34,000
concatenate a lot of training examples

308
00:11:34,000 --> 00:11:36,160
together as documents

309
00:11:36,160 --> 00:11:37,920
we'd also do a couple variations of

310
00:11:37,920 --> 00:11:39,040
different types of queries because

311
00:11:39,040 --> 00:11:40,560
elasticsearch allows for different types

312
00:11:40,560 --> 00:11:43,360
of queries so we applied a few different

313
00:11:43,360 --> 00:11:44,800
variations

314
00:11:44,800 --> 00:11:46,959
and then finally we needed to

315
00:11:46,959 --> 00:11:48,800
train a model

316
00:11:48,800 --> 00:11:50,079
that would

317
00:11:50,079 --> 00:11:52,160
take these multiple confidence scores

318
00:11:52,160 --> 00:11:53,200
and convert them to a single

319
00:11:53,200 --> 00:11:55,200
probabilistic prediction so we want to

320
00:11:55,200 --> 00:11:57,760
get a probabilistic classification over

321
00:11:57,760 --> 00:12:00,399
the 500 possible classes

322
00:12:00,399 --> 00:12:02,399
because um

323
00:12:02,399 --> 00:12:04,639
elasticsearch only outputs uh

324
00:12:04,639 --> 00:12:06,800
uncalibrated scores

325
00:12:06,800 --> 00:12:08,399
probabilities and of course we did

326
00:12:08,399 --> 00:12:10,399
multiple queries so we end up getting

327
00:12:10,399 --> 00:12:12,079
multiple scores so we can't even sort

328
00:12:12,079 --> 00:12:14,000
them so we have to

329
00:12:14,000 --> 00:12:15,519
convert them all into one one

330
00:12:15,519 --> 00:12:18,800
probability with the goal

331
00:12:19,279 --> 00:12:22,839
so here's how we converted

332
00:12:22,839 --> 00:12:25,600
the classification problem into an

333
00:12:25,600 --> 00:12:28,480
elastic search problem so basically for

334
00:12:28,480 --> 00:12:30,320
if we look in the training set for

335
00:12:30,320 --> 00:12:32,480
username we take all the different

336
00:12:32,480 --> 00:12:33,760
examples

337
00:12:33,760 --> 00:12:35,279
where

338
00:12:35,279 --> 00:12:36,639
we have usernames to correct the

339
00:12:36,639 --> 00:12:38,720
annotation we can take all the cases

340
00:12:38,720 --> 00:12:41,040
where those are column names

341
00:12:41,040 --> 00:12:42,079
of those

342
00:12:42,079 --> 00:12:43,839
of those training examples the

343
00:12:43,839 --> 00:12:45,360
descriptions and everything and we can

344
00:12:45,360 --> 00:12:47,040
just concatenate all those training

345
00:12:47,040 --> 00:12:49,279
examples together using commas

346
00:12:49,279 --> 00:12:51,360
and then each of these becomes sort of

347
00:12:51,360 --> 00:12:52,399
synthetic

348
00:12:52,399 --> 00:12:53,519
documents

349
00:12:53,519 --> 00:12:55,920
um that are fed to elastic elastic

350
00:12:55,920 --> 00:12:58,079
search so here for instance calm

351
00:12:58,079 --> 00:13:00,639
descriptions this is this concatenation

352
00:13:00,639 --> 00:13:03,200
of a bunch of training examples but this

353
00:13:03,200 --> 00:13:06,880
will become the document for uh

354
00:13:06,880 --> 00:13:09,920
for username and then at test time if we

355
00:13:09,920 --> 00:13:11,920
enter the column description for

356
00:13:11,920 --> 00:13:13,360
a new

357
00:13:13,360 --> 00:13:15,279
column we're trying to annotate and

358
00:13:15,279 --> 00:13:17,680
elasticsearch returns this so-called

359
00:13:17,680 --> 00:13:20,320
document under the concatenation of

360
00:13:20,320 --> 00:13:22,399
column description then we'll say that

361
00:13:22,399 --> 00:13:25,600
username is the correct annotation

362
00:13:25,600 --> 00:13:29,760
for that test set column

363
00:13:31,200 --> 00:13:32,240
finally

364
00:13:32,240 --> 00:13:35,200
there's also the calibration phase where

365
00:13:35,200 --> 00:13:36,160
we

366
00:13:36,160 --> 00:13:37,680
wanted to convert that to probabilities

367
00:13:37,680 --> 00:13:39,839
so we used a

368
00:13:39,839 --> 00:13:40,959
standard

369
00:13:40,959 --> 00:13:42,959
offline calibration

370
00:13:42,959 --> 00:13:47,040
methodology we have used a held out set

371
00:13:47,040 --> 00:13:49,760
to take the output score

372
00:13:49,760 --> 00:13:51,519
of

373
00:13:51,519 --> 00:13:52,800
on

374
00:13:52,800 --> 00:13:54,880
the output score of elasticsearch and

375
00:13:54,880 --> 00:13:56,959
then fit a probabilistic classifier to

376
00:13:56,959 --> 00:13:59,600
convert the output of elasticsearch

377
00:13:59,600 --> 00:14:01,360
to

378
00:14:01,360 --> 00:14:04,000
a probability uh and this is done using

379
00:14:04,000 --> 00:14:05,120
a held out

380
00:14:05,120 --> 00:14:06,000
20

381
00:14:06,000 --> 00:14:07,920
so we're not recycling the same data

382
00:14:07,920 --> 00:14:09,360
that was used to build the elastic

383
00:14:09,360 --> 00:14:10,800
search index

384
00:14:10,800 --> 00:14:12,720
otherwise we'll be getting biased

385
00:14:12,720 --> 00:14:14,560
probabilities

386
00:14:14,560 --> 00:14:16,399
it could be the mod will be overly

387
00:14:16,399 --> 00:14:18,320
confident if we are recycling the same

388
00:14:18,320 --> 00:14:20,959
probabilities

389
00:14:22,480 --> 00:14:24,320
uh and we also because we do multiple

390
00:14:24,320 --> 00:14:26,399
queries of elasticsearch the calibration

391
00:14:26,399 --> 00:14:28,839
model actually also has to

392
00:14:28,839 --> 00:14:30,959
um combine

393
00:14:30,959 --> 00:14:32,720
multiple scores

394
00:14:32,720 --> 00:14:35,360
so if we do five different

395
00:14:35,360 --> 00:14:37,519
elastic variations on elastic search

396
00:14:37,519 --> 00:14:38,480
queries

397
00:14:38,480 --> 00:14:39,600
we'll get

398
00:14:39,600 --> 00:14:41,920
five on calibrating numeric scores which

399
00:14:41,920 --> 00:14:44,800
then all have to be merged together into

400
00:14:44,800 --> 00:14:47,440
a probabilistic model into probabilistic

401
00:14:47,440 --> 00:14:49,440
prediction

402
00:14:49,440 --> 00:14:50,800
um

403
00:14:50,800 --> 00:14:52,720
so we both have to convert the five

404
00:14:52,720 --> 00:14:55,279
scores to one score for a particular

405
00:14:55,279 --> 00:14:56,639
annotation and we want it to be a

406
00:14:56,639 --> 00:14:59,199
probability

407
00:14:59,199 --> 00:15:01,199
and then this is just a diagram

408
00:15:01,199 --> 00:15:04,959
representation of this where

409
00:15:04,959 --> 00:15:06,959
we make n different

410
00:15:06,959 --> 00:15:08,560
elastic search

411
00:15:08,560 --> 00:15:12,160
queries uh for a particular test case we

412
00:15:12,160 --> 00:15:12,959
get

413
00:15:12,959 --> 00:15:16,240
n different response scores r and then

414
00:15:16,240 --> 00:15:18,079
we'll feed it in this case we ended up

415
00:15:18,079 --> 00:15:20,000
feeding it to a neural net model

416
00:15:20,000 --> 00:15:22,399
to combine all those to one probability

417
00:15:22,399 --> 00:15:24,399
as the output whereas ours are just

418
00:15:24,399 --> 00:15:27,839
arbitrary scores

419
00:15:28,079 --> 00:15:29,600
and we compared various different

420
00:15:29,600 --> 00:15:31,040
systems for this

421
00:15:31,040 --> 00:15:31,920
um

422
00:15:31,920 --> 00:15:32,720
and

423
00:15:32,720 --> 00:15:35,040
you know as a class different just

424
00:15:35,040 --> 00:15:37,040
generic classification system so we

425
00:15:37,040 --> 00:15:38,399
evaluate it

426
00:15:38,399 --> 00:15:41,680
both on the top 10 accuracy where higher

427
00:15:41,680 --> 00:15:44,320
is better uh so this is is the true

428
00:15:44,320 --> 00:15:47,360
answer within the top 10 recommendations

429
00:15:47,360 --> 00:15:48,480
and we also

430
00:15:48,480 --> 00:15:50,880
to evaluate how well we're calibrating

431
00:15:50,880 --> 00:15:52,880
the uncertainty and our confidence in

432
00:15:52,880 --> 00:15:55,600
the predictions you can look at negative

433
00:15:55,600 --> 00:15:57,680
log likelihood

434
00:15:57,680 --> 00:16:00,560
which is also known as

435
00:16:00,560 --> 00:16:03,440
cross entropy in the deep learning world

436
00:16:03,440 --> 00:16:05,120
perplexity and topic models it comes

437
00:16:05,120 --> 00:16:06,959
with a lot of different names

438
00:16:06,959 --> 00:16:08,720
and we can compare this as well and this

439
00:16:08,720 --> 00:16:10,959
is a loss function so

440
00:16:10,959 --> 00:16:13,199
lower is better in this case

441
00:16:13,199 --> 00:16:15,279
now both these um

442
00:16:15,279 --> 00:16:16,320
you know there were definitely a few

443
00:16:16,320 --> 00:16:17,680
different high-performing choices of

444
00:16:17,680 --> 00:16:20,240
models to choose between but the neural

445
00:16:20,240 --> 00:16:22,639
network architecture

446
00:16:22,639 --> 00:16:24,800
that we evaluated was

447
00:16:24,800 --> 00:16:27,199
somewhat better than all the other

448
00:16:27,199 --> 00:16:28,959
off-the-shelf methods so we ended up

449
00:16:28,959 --> 00:16:30,800
going with that so

450
00:16:30,800 --> 00:16:31,839
we

451
00:16:31,839 --> 00:16:34,480
didn't just use the

452
00:16:34,480 --> 00:16:36,480
neural net because it's a fashionable

453
00:16:36,480 --> 00:16:40,160
thing to do but because it is um you

454
00:16:40,160 --> 00:16:44,160
know is is the best from our evaluation

455
00:16:44,160 --> 00:16:46,079
um

456
00:16:46,079 --> 00:16:47,279
and

457
00:16:47,279 --> 00:16:49,360
almost three-fourths of all data sets

458
00:16:49,360 --> 00:16:50,880
when we deployed it

459
00:16:50,880 --> 00:16:53,360
internally ended up being annotated

460
00:16:53,360 --> 00:16:54,800
using this tool

461
00:16:54,800 --> 00:16:57,600
uh just at least for some of the

462
00:16:57,600 --> 00:16:59,519
some of the columns the end up with

463
00:16:59,519 --> 00:17:00,959
recommendations

464
00:17:00,959 --> 00:17:03,120
uh there's a minority of users who are

465
00:17:03,120 --> 00:17:04,480
so confident

466
00:17:04,480 --> 00:17:06,480
in being able to do this manually that

467
00:17:06,480 --> 00:17:08,000
they don't really even need

468
00:17:08,000 --> 00:17:08,959
um

469
00:17:08,959 --> 00:17:11,359
automated recommendations but as we

470
00:17:11,359 --> 00:17:13,359
found out from our analytics that the

471
00:17:13,359 --> 00:17:14,640
majority

472
00:17:14,640 --> 00:17:15,919
of

473
00:17:15,919 --> 00:17:18,319
the internal

474
00:17:18,319 --> 00:17:19,760
users for

475
00:17:19,760 --> 00:17:21,359
data and data scientists and

476
00:17:21,359 --> 00:17:24,000
practitioners

477
00:17:24,400 --> 00:17:26,640
just uh needed some help they use

478
00:17:26,640 --> 00:17:30,000
recommendation recommender tool because

479
00:17:30,000 --> 00:17:33,120
there's definitely a lot of um obscurity

480
00:17:33,120 --> 00:17:34,400
to getting a lot of these recommend

481
00:17:34,400 --> 00:17:35,120
you're getting a lot of these

482
00:17:35,120 --> 00:17:37,840
recommendations right

483
00:17:37,840 --> 00:17:39,440
um

484
00:17:39,440 --> 00:17:42,880
so in the conclusions um using

485
00:17:42,880 --> 00:17:46,960
uh machine learning models uh to

486
00:17:46,960 --> 00:17:49,440
help a recommendation system to annotate

487
00:17:49,440 --> 00:17:51,360
all these old data sets was a very

488
00:17:51,360 --> 00:17:52,640
useful

489
00:17:52,640 --> 00:17:55,039
exercise in

490
00:17:55,039 --> 00:17:56,480
getting everything

491
00:17:56,480 --> 00:17:59,039
involved in this standard taxonomy

492
00:17:59,039 --> 00:18:01,039
there's a lot of work that had to be

493
00:18:01,039 --> 00:18:03,600
done a lot of work to clean up a lot of

494
00:18:03,600 --> 00:18:06,240
legacy data set problems

495
00:18:06,240 --> 00:18:07,360
and

496
00:18:07,360 --> 00:18:08,880
and this

497
00:18:08,880 --> 00:18:10,880
this effort this

498
00:18:10,880 --> 00:18:12,960
uh this product we developed here really

499
00:18:12,960 --> 00:18:16,000
helped accelerate that process with less

500
00:18:16,000 --> 00:18:18,480
less man hours and we see that 75

501
00:18:18,480 --> 00:18:21,039
percent of the data sets

502
00:18:21,039 --> 00:18:22,799
internally ended up being annotated this

503
00:18:22,799 --> 00:18:25,760
way company-wide

504
00:18:26,160 --> 00:18:28,400
and this you know this will help in any

505
00:18:28,400 --> 00:18:30,400
other use cases as well

506
00:18:30,400 --> 00:18:32,240
for

507
00:18:32,240 --> 00:18:34,720
data for data auditing auditing and

508
00:18:34,720 --> 00:18:38,160
handling and that well

509
00:18:38,160 --> 00:18:41,960
thanks thanks for watching

510
00:18:47,919 --> 00:18:50,000
you

