1
00:00:09,920 --> 00:00:12,080
so this is a talk on data mapping at

2
00:00:12,080 --> 00:00:13,440
cruz

3
00:00:13,440 --> 00:00:15,519
the clickbait version of the title is

4
00:00:15,519 --> 00:00:17,199
data mapping at a billion dollar

5
00:00:17,199 --> 00:00:19,279
self-driving startup

6
00:00:19,279 --> 00:00:21,520
uh i'm mark pereira i'm the tech lead of

7
00:00:21,520 --> 00:00:23,680
privacy infrastructure at cruz

8
00:00:23,680 --> 00:00:26,000
uh previous to joining cruz i spent

9
00:00:26,000 --> 00:00:28,160
three years doing differential privacy

10
00:00:28,160 --> 00:00:30,000
for the department of energy in a

11
00:00:30,000 --> 00:00:31,760
project called the secure

12
00:00:31,760 --> 00:00:34,559
energy algorithm test bed

13
00:00:34,559 --> 00:00:36,480
what are we going to talk about today a

14
00:00:36,480 --> 00:00:37,920
brief introduction to the privacy

15
00:00:37,920 --> 00:00:40,239
challenges of autonomous vehicles

16
00:00:40,239 --> 00:00:42,160
background on past data mapping efforts

17
00:00:42,160 --> 00:00:43,360
at cruz

18
00:00:43,360 --> 00:00:45,039
the bulk of the talk will be on what we

19
00:00:45,039 --> 00:00:46,960
built to address this challenge of

20
00:00:46,960 --> 00:00:48,879
visibility into the flow of sensitive

21
00:00:48,879 --> 00:00:49,840
data

22
00:00:49,840 --> 00:00:52,239
and i'll try to interweave use cases for

23
00:00:52,239 --> 00:00:54,000
this data mapping tool as well as

24
00:00:54,000 --> 00:00:57,280
interesting findings along the way

25
00:00:57,280 --> 00:00:58,800
all right so why

26
00:00:58,800 --> 00:01:00,480
why is privacy a challenge for

27
00:01:00,480 --> 00:01:03,600
autonomous vehicles well you have

28
00:01:03,600 --> 00:01:05,040
sensor-laden

29
00:01:05,040 --> 00:01:07,760
platforms driving around major cities uh

30
00:01:07,760 --> 00:01:09,280
constantly hundreds of these vehicles

31
00:01:09,280 --> 00:01:12,960
they're riddled with sensors cameras gps

32
00:01:12,960 --> 00:01:15,520
microphones inside and outside the

33
00:01:15,520 --> 00:01:16,799
vehicle

34
00:01:16,799 --> 00:01:18,240
not only that but they're connected to

35
00:01:18,240 --> 00:01:19,520
one of the most advanced machine

36
00:01:19,520 --> 00:01:21,600
learning apparatuses that humankind has

37
00:01:21,600 --> 00:01:23,680
ever built so the possibility for

38
00:01:23,680 --> 00:01:25,840
instruments aggregation

39
00:01:25,840 --> 00:01:27,439
is quite high

40
00:01:27,439 --> 00:01:28,880
if that weren't enough

41
00:01:28,880 --> 00:01:30,640
that challenge is further magnified by

42
00:01:30,640 --> 00:01:32,079
the fact that all of this is being done

43
00:01:32,079 --> 00:01:34,159
in the context of a startup environment

44
00:01:34,159 --> 00:01:36,720
it is to say autonomous vehicles are

45
00:01:36,720 --> 00:01:38,400
just reaching the level of technical

46
00:01:38,400 --> 00:01:40,720
maturity to be able to be written by the

47
00:01:40,720 --> 00:01:42,240
general public

48
00:01:42,240 --> 00:01:43,920
so finally we're collecting customer

49
00:01:43,920 --> 00:01:46,079
phone numbers email addresses their

50
00:01:46,079 --> 00:01:48,000
behavioral data in the form of where

51
00:01:48,000 --> 00:01:49,119
they're picked up and where they're

52
00:01:49,119 --> 00:01:50,720
going

53
00:01:50,720 --> 00:01:52,479
it means that as privacy engineers we

54
00:01:52,479 --> 00:01:54,479
have to deal with constantly shifting

55
00:01:54,479 --> 00:01:55,759
constantly shifting requirements

56
00:01:55,759 --> 00:01:57,119
constantly shifting

57
00:01:57,119 --> 00:01:59,040
architecture

58
00:01:59,040 --> 00:02:00,880
we have to stay quite nimble to be able

59
00:02:00,880 --> 00:02:02,560
to address privacy in that sort of

60
00:02:02,560 --> 00:02:03,759
environment

61
00:02:03,759 --> 00:02:04,799
now

62
00:02:04,799 --> 00:02:06,640
one of the first

63
00:02:06,640 --> 00:02:09,119
challenges we noticed in that was our

64
00:02:09,119 --> 00:02:11,038
lack of visibility into the flow of

65
00:02:11,038 --> 00:02:12,800
sensitive data throughout

66
00:02:12,800 --> 00:02:14,239
uh in infrastructure throughout the

67
00:02:14,239 --> 00:02:15,440
infrastructure

68
00:02:15,440 --> 00:02:16,560
cruze is

69
00:02:16,560 --> 00:02:18,400
funded to the tune of nine billion

70
00:02:18,400 --> 00:02:21,840
dollars has 2500 or so engineers you can

71
00:02:21,840 --> 00:02:24,000
imagine there's a lot of data it's

72
00:02:24,000 --> 00:02:26,160
copied around used in

73
00:02:26,160 --> 00:02:26,959
any

74
00:02:26,959 --> 00:02:29,200
any manner of ways

75
00:02:29,200 --> 00:02:31,200
there have been past data mapping

76
00:02:31,200 --> 00:02:33,040
efforts at cruz to try to get a handle

77
00:02:33,040 --> 00:02:35,360
on it this is a non-exhaustive list of

78
00:02:35,360 --> 00:02:37,200
some of those past efforts

79
00:02:37,200 --> 00:02:39,280
unfortunately those efforts were met

80
00:02:39,280 --> 00:02:41,360
with at best middling success and

81
00:02:41,360 --> 00:02:44,000
sometimes outright failure it's a really

82
00:02:44,000 --> 00:02:46,560
tough problem

83
00:02:46,560 --> 00:02:47,840
fortunately

84
00:02:47,840 --> 00:02:50,480
when i joined after researching these

85
00:02:50,480 --> 00:02:52,239
past projects we identified four key

86
00:02:52,239 --> 00:02:53,599
areas

87
00:02:53,599 --> 00:02:55,920
that we thought we could

88
00:02:55,920 --> 00:02:58,159
improve upon that would lead us to a

89
00:02:58,159 --> 00:03:00,080
meaningful chance of success right we

90
00:03:00,080 --> 00:03:01,280
weren't just going to repeat past

91
00:03:01,280 --> 00:03:02,879
mistakes with this work

92
00:03:02,879 --> 00:03:04,879
here are the four lessons number one

93
00:03:04,879 --> 00:03:06,800
manual labeling isn't enough there's

94
00:03:06,800 --> 00:03:08,800
just too much data too much sensitive

95
00:03:08,800 --> 00:03:10,080
data to

96
00:03:10,080 --> 00:03:12,480
rely on human-based labeling and

97
00:03:12,480 --> 00:03:14,480
sensitive data people just forget about

98
00:03:14,480 --> 00:03:15,760
where it is

99
00:03:15,760 --> 00:03:17,120
number two

100
00:03:17,120 --> 00:03:19,200
mapping is a context-specific activity

101
00:03:19,200 --> 00:03:21,280
so in an analogous way that you would

102
00:03:21,280 --> 00:03:23,599
use a roadmap or a topographic map to

103
00:03:23,599 --> 00:03:26,159
plan a different type of trip a privacy

104
00:03:26,159 --> 00:03:28,080
engineer wants a very different data map

105
00:03:28,080 --> 00:03:30,239
from a data scientist you need to orient

106
00:03:30,239 --> 00:03:32,959
your data mapping activity precisely to

107
00:03:32,959 --> 00:03:35,120
the context in which it matters

108
00:03:35,120 --> 00:03:36,640
practically what that means is some open

109
00:03:36,640 --> 00:03:38,400
source tools for data mapping are not

110
00:03:38,400 --> 00:03:40,879
suitable for privacy engineers

111
00:03:40,879 --> 00:03:42,480
lesson three

112
00:03:42,480 --> 00:03:44,000
data mapping seems to be the type of

113
00:03:44,000 --> 00:03:46,720
activity that encourages scope creep so

114
00:03:46,720 --> 00:03:48,640
many different things to map everybody's

115
00:03:48,640 --> 00:03:50,560
got a different use case many of the

116
00:03:50,560 --> 00:03:52,400
projects that we studied that were

117
00:03:52,400 --> 00:03:54,319
failures tried to do too much all at

118
00:03:54,319 --> 00:03:56,480
once only got part of the way i see a

119
00:03:56,480 --> 00:03:58,000
lot of heads nodding in the audience at

120
00:03:58,000 --> 00:03:59,840
that one

121
00:03:59,840 --> 00:04:01,280
let's see if you're not at this one too

122
00:04:01,280 --> 00:04:03,280
so hand in hand with that

123
00:04:03,280 --> 00:04:05,680
uh quite a few projects labeled a whole

124
00:04:05,680 --> 00:04:07,680
bunch of things but then didn't have a

125
00:04:07,680 --> 00:04:09,280
plan in mind for what to do with those

126
00:04:09,280 --> 00:04:10,879
labels they didn't have

127
00:04:10,879 --> 00:04:13,439
a particular risk in mind that those

128
00:04:13,439 --> 00:04:14,959
labels would help address they didn't

129
00:04:14,959 --> 00:04:17,440
have a pathway from those labels

130
00:04:17,440 --> 00:04:19,839
to a technical control which would allow

131
00:04:19,839 --> 00:04:21,519
them

132
00:04:21,519 --> 00:04:23,520
to improve privacy

133
00:04:23,520 --> 00:04:26,320
you've labeled data so what is the

134
00:04:26,320 --> 00:04:28,639
the pithy way to summarize that

135
00:04:28,639 --> 00:04:31,759
here's how we applied these lessons to

136
00:04:31,759 --> 00:04:33,840
our mvp development process of the data

137
00:04:33,840 --> 00:04:34,800
map

138
00:04:34,800 --> 00:04:36,720
we put an emphasis on building automated

139
00:04:36,720 --> 00:04:38,960
detectors for sensitive data

140
00:04:38,960 --> 00:04:40,479
we built a data map

141
00:04:40,479 --> 00:04:42,560
only for privacy engineering use cases

142
00:04:42,560 --> 00:04:45,040
explicitly ignoring others

143
00:04:45,040 --> 00:04:46,960
we focused on a single data store

144
00:04:46,960 --> 00:04:49,040
cruises uh data warehouse stored in

145
00:04:49,040 --> 00:04:51,040
bigquery and within that we even

146
00:04:51,040 --> 00:04:52,880
narrowed the focus further on two

147
00:04:52,880 --> 00:04:54,560
high-value projects not attempting to

148
00:04:54,560 --> 00:04:56,479
cover everything at first just something

149
00:04:56,479 --> 00:04:58,400
representative to teach us

150
00:04:58,400 --> 00:05:00,000
what we would need to be able to scale

151
00:05:00,000 --> 00:05:00,960
further

152
00:05:00,960 --> 00:05:02,639
finally we identified a use case up

153
00:05:02,639 --> 00:05:05,680
front which was to archive any sensitive

154
00:05:05,680 --> 00:05:07,199
data that had been abandoned by its

155
00:05:07,199 --> 00:05:08,720
original owners

156
00:05:08,720 --> 00:05:10,320
a data scientist data set that they had

157
00:05:10,320 --> 00:05:12,080
copied for a one-off analysis and

158
00:05:12,080 --> 00:05:15,520
forgotten about that sort of thing

159
00:05:15,680 --> 00:05:16,720
all right

160
00:05:16,720 --> 00:05:18,639
me of the talk what we built we'll start

161
00:05:18,639 --> 00:05:21,360
from the user-facing web application and

162
00:05:21,360 --> 00:05:23,680
sort of work our way under the hood to

163
00:05:23,680 --> 00:05:25,919
the data pipeline and the detectors that

164
00:05:25,919 --> 00:05:26,880
we built

165
00:05:26,880 --> 00:05:28,800
so we'll start with the web application

166
00:05:28,800 --> 00:05:31,360
the application is called indiana as in

167
00:05:31,360 --> 00:05:34,400
indiana jones so where indiana jones

168
00:05:34,400 --> 00:05:36,320
searches for bury treasure indiana the

169
00:05:36,320 --> 00:05:40,560
application looks for sensitive data

170
00:05:40,720 --> 00:05:43,120
i lost my timer that's okay

171
00:05:43,120 --> 00:05:44,800
no problem

172
00:05:44,800 --> 00:05:45,680
the

173
00:05:45,680 --> 00:05:49,039
main screen of indiana is a filterable

174
00:05:49,039 --> 00:05:51,840
table of all of the schemas of bigquery

175
00:05:51,840 --> 00:05:54,240
so each row corresponds to one column in

176
00:05:54,240 --> 00:05:55,440
bigquery

177
00:05:55,440 --> 00:05:58,160
you'll see project data set table

178
00:05:58,160 --> 00:06:00,240
column name in the field there's a

179
00:06:00,240 --> 00:06:02,639
search box so you can do

180
00:06:02,639 --> 00:06:05,840
real-time searches for field names via a

181
00:06:05,840 --> 00:06:08,639
regular expression so there's about

182
00:06:08,639 --> 00:06:11,440
36 million columns that we've ingested

183
00:06:11,440 --> 00:06:13,680
into a postgres database indexed it with

184
00:06:13,680 --> 00:06:15,600
a trigram index so you get near

185
00:06:15,600 --> 00:06:17,280
real-time full-text

186
00:06:17,280 --> 00:06:19,360
search capability this is this is pretty

187
00:06:19,360 --> 00:06:20,960
interesting even just out of the box

188
00:06:20,960 --> 00:06:22,160
right there before we've done any sort

189
00:06:22,160 --> 00:06:24,240
of detection for sensitive data because

190
00:06:24,240 --> 00:06:25,680
it enables

191
00:06:25,680 --> 00:06:28,240
on-the-fly ad-hoc metadata searches

192
00:06:28,240 --> 00:06:29,520
here's an example

193
00:06:29,520 --> 00:06:30,240
we

194
00:06:30,240 --> 00:06:31,680
were on a call with a data team that

195
00:06:31,680 --> 00:06:32,800
told us

196
00:06:32,800 --> 00:06:36,240
we store raw image data in bigquery

197
00:06:36,240 --> 00:06:37,840
we didn't expect that sort of data to be

198
00:06:37,840 --> 00:06:38,880
stored there we thought it was only

199
00:06:38,880 --> 00:06:40,880
being stored in cloud storage

200
00:06:40,880 --> 00:06:43,039
they told us the name of what that data

201
00:06:43,039 --> 00:06:44,960
field was called in their table and so

202
00:06:44,960 --> 00:06:47,280
we were able to on the call

203
00:06:47,280 --> 00:06:49,039
type in that name and see how widespread

204
00:06:49,039 --> 00:06:50,400
that pattern was

205
00:06:50,400 --> 00:06:53,199
across the infrastructure and yeah

206
00:06:53,199 --> 00:06:56,720
quite widespread it turns out

207
00:06:56,960 --> 00:06:58,479
the regular expression support is pretty

208
00:06:58,479 --> 00:07:00,000
interesting because you can do things

209
00:07:00,000 --> 00:07:03,680
like enumerate schemas in a natural way

210
00:07:03,680 --> 00:07:06,479
the dot plus syntax there says

211
00:07:06,479 --> 00:07:08,400
any table that's called container logs

212
00:07:08,400 --> 00:07:10,240
that has something called latitude in it

213
00:07:10,240 --> 00:07:12,800
return it to me

214
00:07:12,800 --> 00:07:15,039
it's not such a fancy thing but

215
00:07:15,039 --> 00:07:17,039
that's sort of like platform orientation

216
00:07:17,039 --> 00:07:18,560
towards building the tool

217
00:07:18,560 --> 00:07:20,240
builds in some flexibility which lets us

218
00:07:20,240 --> 00:07:22,560
like discover future use cases

219
00:07:22,560 --> 00:07:24,560
a couple of simple filters created

220
00:07:24,560 --> 00:07:26,240
creation date is a pretty useful one

221
00:07:26,240 --> 00:07:28,639
gives us visibility into like ongoing

222
00:07:28,639 --> 00:07:30,240
changes to the data warehouse what

223
00:07:30,240 --> 00:07:31,759
happened in the last two weeks that sort

224
00:07:31,759 --> 00:07:33,599
of thing

225
00:07:33,599 --> 00:07:36,160
the most interesting filter are verified

226
00:07:36,160 --> 00:07:38,400
labels of sensitive data so here are the

227
00:07:38,400 --> 00:07:40,319
categories of data that we

228
00:07:40,319 --> 00:07:42,560
label with automation email address

229
00:07:42,560 --> 00:07:44,319
non-cruise email addresses names phone

230
00:07:44,319 --> 00:07:46,080
numbers precise location of a few

231
00:07:46,080 --> 00:07:48,800
different forms

232
00:07:48,879 --> 00:07:50,400
we'll go into a bunch of detail about

233
00:07:50,400 --> 00:07:52,639
the system that underlies that later

234
00:07:52,639 --> 00:07:54,080
but with the filters you can do things

235
00:07:54,080 --> 00:07:55,680
like say where are all the phone numbers

236
00:07:55,680 --> 00:07:58,560
in the data warehouse

237
00:07:58,560 --> 00:08:00,879
this is also really useful for internal

238
00:08:00,879 --> 00:08:02,560
data access requests

239
00:08:02,560 --> 00:08:03,919
so we get

240
00:08:03,919 --> 00:08:06,000
teams we'll send a privacy review

241
00:08:06,000 --> 00:08:07,759
is this contract is it okay to give this

242
00:08:07,759 --> 00:08:10,080
contractor this access to data via

243
00:08:10,080 --> 00:08:12,960
indiana we can quickly filter to

244
00:08:12,960 --> 00:08:14,240
the tables that they're requesting

245
00:08:14,240 --> 00:08:16,000
access for check all the different

246
00:08:16,000 --> 00:08:17,840
sensitive data fields

247
00:08:17,840 --> 00:08:19,759
the url changes so that we can link

248
00:08:19,759 --> 00:08:22,319
somebody to that output view and we can

249
00:08:22,319 --> 00:08:24,240
paste that into a ticket and say no you

250
00:08:24,240 --> 00:08:26,000
can't have access to all of data

251
00:08:26,000 --> 00:08:27,840
infraprod because it has

252
00:08:27,840 --> 00:08:29,919
thousands of instances of sensitive data

253
00:08:29,919 --> 00:08:34,000
please scope down your request further

254
00:08:34,000 --> 00:08:36,000
another interesting thing that these

255
00:08:36,000 --> 00:08:38,000
labels allow us to do are provide

256
00:08:38,000 --> 00:08:41,279
comprehensive summary views of

257
00:08:41,279 --> 00:08:44,399
the sprawl of sensitive data across the

258
00:08:44,399 --> 00:08:47,680
data warehouse this is a histogram

259
00:08:47,680 --> 00:08:49,040
sort of anonymized with the project

260
00:08:49,040 --> 00:08:51,040
names

261
00:08:51,040 --> 00:08:52,959
that shows the distribution of sensitive

262
00:08:52,959 --> 00:08:55,680
data across different bigquery projects

263
00:08:55,680 --> 00:08:57,600
in our data warehouse which is to say

264
00:08:57,600 --> 00:08:59,279
the sensitive data

265
00:08:59,279 --> 00:09:00,880
sprawl across the data warehouse they're

266
00:09:00,880 --> 00:09:02,959
found in many different locations with

267
00:09:02,959 --> 00:09:05,040
quite different distributions this chart

268
00:09:05,040 --> 00:09:06,640
is quite powerful for communicating to

269
00:09:06,640 --> 00:09:08,480
senior leadership it makes that problem

270
00:09:08,480 --> 00:09:09,839
of data sprawl

271
00:09:09,839 --> 00:09:12,399
not an abstract one not a

272
00:09:12,399 --> 00:09:14,720
not an abstract one but a concrete one

273
00:09:14,720 --> 00:09:16,640
that's turning from yeah something that

274
00:09:16,640 --> 00:09:18,800
we can we can measure

275
00:09:18,800 --> 00:09:22,399
that turns into something we can measure

276
00:09:22,399 --> 00:09:23,920
one last interesting feature of this

277
00:09:23,920 --> 00:09:26,480
application is this sample button

278
00:09:26,480 --> 00:09:28,720
that pulls a random sample of data from

279
00:09:28,720 --> 00:09:30,080
arbitrary data sets in the data

280
00:09:30,080 --> 00:09:32,080
warehouse it's really useful for ad hoc

281
00:09:32,080 --> 00:09:33,600
mapping activities by privacy

282
00:09:33,600 --> 00:09:35,760
engineering a team will ask us hey is

283
00:09:35,760 --> 00:09:37,920
this field sensitive sometimes it's hard

284
00:09:37,920 --> 00:09:39,600
to tell just by looking at the name we

285
00:09:39,600 --> 00:09:41,680
can pull a random sample of data

286
00:09:41,680 --> 00:09:43,120
more detail on the service that powers

287
00:09:43,120 --> 00:09:44,800
that because doing that in a secure way

288
00:09:44,800 --> 00:09:47,600
was an interesting design challenge

289
00:09:47,600 --> 00:09:48,959
here's just a summary of all of these

290
00:09:48,959 --> 00:09:51,279
use cases on one slide just to show the

291
00:09:51,279 --> 00:09:54,000
sort of variety of privacy engineering

292
00:09:54,000 --> 00:09:56,160
use cases we were able to meet with a

293
00:09:56,160 --> 00:09:58,480
relatively simple tool right it's just a

294
00:09:58,480 --> 00:10:00,880
filterable table a search box a few

295
00:10:00,880 --> 00:10:03,680
filters but we dialed it in precisely

296
00:10:03,680 --> 00:10:05,360
for privacy engineers and we're able to

297
00:10:05,360 --> 00:10:08,480
cover quite a bit of useful things

298
00:10:08,480 --> 00:10:10,399
all right so where did the verified

299
00:10:10,399 --> 00:10:12,240
labels come from in that

300
00:10:12,240 --> 00:10:14,160
that ui we just looked at there's a

301
00:10:14,160 --> 00:10:15,760
whole data pipeline of scanners a

302
00:10:15,760 --> 00:10:18,000
combination of metadata and content

303
00:10:18,000 --> 00:10:20,720
scanners metadata scans

304
00:10:20,720 --> 00:10:23,279
they scan the names of columns in

305
00:10:23,279 --> 00:10:25,519
bigquery i use a regular expression for

306
00:10:25,519 --> 00:10:27,440
that here's an example of the

307
00:10:27,440 --> 00:10:29,680
configuration for these scanners

308
00:10:29,680 --> 00:10:31,839
this is really fast right it's all

309
00:10:31,839 --> 00:10:34,240
implemented as a single bigquery query

310
00:10:34,240 --> 00:10:36,399
we ingest the schema via google cloud's

311
00:10:36,399 --> 00:10:38,800
cloud asset inventory and then run the

312
00:10:38,800 --> 00:10:40,640
query against it with a regular

313
00:10:40,640 --> 00:10:42,399
expression filter so these are really

314
00:10:42,399 --> 00:10:44,000
nice they're easy to develop

315
00:10:44,000 --> 00:10:45,519
but there are limitations right you

316
00:10:45,519 --> 00:10:47,680
can't actually look into the data that's

317
00:10:47,680 --> 00:10:50,399
where content scans come into play you

318
00:10:50,399 --> 00:10:53,120
pull a sample of the data

319
00:10:53,120 --> 00:10:54,800
run some machine learning or regular

320
00:10:54,800 --> 00:10:56,399
expression or padding match pattern

321
00:10:56,399 --> 00:10:58,480
matching against the against the data

322
00:10:58,480 --> 00:11:00,640
and find tags that way

323
00:11:00,640 --> 00:11:02,240
we evaluated a number of different

324
00:11:02,240 --> 00:11:03,680
approaches for that

325
00:11:03,680 --> 00:11:05,839
uh ended up settling on google cloud's

326
00:11:05,839 --> 00:11:07,040
dlp

327
00:11:07,040 --> 00:11:08,880
inspect api which gives you a

328
00:11:08,880 --> 00:11:10,720
combination of machine learning and

329
00:11:10,720 --> 00:11:12,399
rule-based matchers

330
00:11:12,399 --> 00:11:15,440
one interesting so challenging sorry

331
00:11:15,440 --> 00:11:18,000
scaling content scans uh was quite

332
00:11:18,000 --> 00:11:19,680
challenging from a scalability point of

333
00:11:19,680 --> 00:11:22,480
view uh three three different ways in

334
00:11:22,480 --> 00:11:24,640
which it's challenging

335
00:11:24,640 --> 00:11:25,600
so

336
00:11:25,600 --> 00:11:27,440
one you're handling

337
00:11:27,440 --> 00:11:29,519
quite a bit more data than the metadata

338
00:11:29,519 --> 00:11:31,680
scans so where metadata scan to scan a

339
00:11:31,680 --> 00:11:33,600
column just just has a little bit of

340
00:11:33,600 --> 00:11:35,680
data from the column name the

341
00:11:35,680 --> 00:11:37,120
content scan actually pulls a bunch of

342
00:11:37,120 --> 00:11:38,959
data from the table so you're handling a

343
00:11:38,959 --> 00:11:40,800
lot more data that you have to push

344
00:11:40,800 --> 00:11:42,800
through the data pipeline

345
00:11:42,800 --> 00:11:44,720
the other big challenge the second big

346
00:11:44,720 --> 00:11:46,560
challenge to scaling content scanning

347
00:11:46,560 --> 00:11:49,200
was achieving uh inadequate signal to

348
00:11:49,200 --> 00:11:50,480
noise ratio

349
00:11:50,480 --> 00:11:52,560
out of the box all of the scanners from

350
00:11:52,560 --> 00:11:55,519
google cloud's dlp api provided either

351
00:11:55,519 --> 00:11:57,440
way too many false positives or way too

352
00:11:57,440 --> 00:11:58,959
many false negatives

353
00:11:58,959 --> 00:12:01,200
we were either overwhelmed with findings

354
00:12:01,200 --> 00:12:01,920
and

355
00:12:01,920 --> 00:12:03,920
they were useless or we knew that they

356
00:12:03,920 --> 00:12:05,279
were missing some

357
00:12:05,279 --> 00:12:06,720
sensitive data

358
00:12:06,720 --> 00:12:09,360
in the warehouse so quite a bit of

359
00:12:09,360 --> 00:12:11,279
tuning to the sensor

360
00:12:11,279 --> 00:12:14,079
tuning to the

361
00:12:14,560 --> 00:12:15,760
tuning to the

362
00:12:15,760 --> 00:12:18,399
matchers was necessary as well as

363
00:12:18,399 --> 00:12:21,519
post-processing as well

364
00:12:22,480 --> 00:12:23,839
third challenge and this one we'll go

365
00:12:23,839 --> 00:12:25,839
into some detail here

366
00:12:25,839 --> 00:12:27,440
in some more slides is how to do this

367
00:12:27,440 --> 00:12:29,839
securely so you may have data that are

368
00:12:29,839 --> 00:12:32,240
locked down in the data warehouse that

369
00:12:32,240 --> 00:12:33,839
privacy engineering doesn't have access

370
00:12:33,839 --> 00:12:34,720
to

371
00:12:34,720 --> 00:12:36,160
how do you scan that how do you get your

372
00:12:36,160 --> 00:12:39,519
scanner's access to that data

373
00:12:39,519 --> 00:12:40,480
well there's a couple different

374
00:12:40,480 --> 00:12:41,839
approaches you might take the simplest

375
00:12:41,839 --> 00:12:43,200
one would be

376
00:12:43,200 --> 00:12:44,639
just give privacy engineering read

377
00:12:44,639 --> 00:12:47,440
access to everything

378
00:12:48,079 --> 00:12:49,680
de facto kind of what was happening

379
00:12:49,680 --> 00:12:51,600
before you know teams would over

380
00:12:51,600 --> 00:12:52,959
provision our user accounts because

381
00:12:52,959 --> 00:12:54,480
they'd get us involved to try to review

382
00:12:54,480 --> 00:12:55,839
things and be like oh yeah here go

383
00:12:55,839 --> 00:12:57,440
here's access to all of our data it's

384
00:12:57,440 --> 00:12:58,560
hard to sleep at night you know when

385
00:12:58,560 --> 00:13:00,880
your account is like such a high uh

386
00:13:00,880 --> 00:13:02,959
visibility target for attackers

387
00:13:02,959 --> 00:13:04,480
the most complex option oh there's

388
00:13:04,480 --> 00:13:06,480
probably a number but the one

389
00:13:06,480 --> 00:13:08,160
representative one here

390
00:13:08,160 --> 00:13:10,240
something like in-situ scanners so you

391
00:13:10,240 --> 00:13:11,839
send a container with code for your

392
00:13:11,839 --> 00:13:14,079
scanner to a team tell them to run it

393
00:13:14,079 --> 00:13:16,000
send you back the labels but there's

394
00:13:16,000 --> 00:13:16,959
quite a bit of complexity in

395
00:13:16,959 --> 00:13:18,480
implementing that sort of thing you have

396
00:13:18,480 --> 00:13:20,240
to be able to constantly push them code

397
00:13:20,240 --> 00:13:21,680
updates you have to be able to debug the

398
00:13:21,680 --> 00:13:24,000
scanners uh

399
00:13:24,000 --> 00:13:25,680
on and on so on and so forth there's

400
00:13:25,680 --> 00:13:28,160
quite a few security challenges to

401
00:13:28,160 --> 00:13:30,480
reaching a

402
00:13:30,480 --> 00:13:32,800
successful enough implementation bar to

403
00:13:32,800 --> 00:13:34,720
have improved security by doing that

404
00:13:34,720 --> 00:13:36,000
rather than

405
00:13:36,000 --> 00:13:38,240
decreased it we tried to

406
00:13:38,240 --> 00:13:40,320
meet in the middle for

407
00:13:40,320 --> 00:13:41,839
our startup

408
00:13:41,839 --> 00:13:44,560
atmosphere at cruz with a secure data

409
00:13:44,560 --> 00:13:46,480
sampling service

410
00:13:46,480 --> 00:13:48,560
we call it travel so you know how like

411
00:13:48,560 --> 00:13:49,920
an archaeologist when they're like

412
00:13:49,920 --> 00:13:52,399
digging up dinosaur bones uses

413
00:13:52,399 --> 00:13:54,160
a trowel to like just take like a little

414
00:13:54,160 --> 00:13:55,760
bit of dirt at a time so as to not

415
00:13:55,760 --> 00:13:57,519
damage it that's the same idea here we

416
00:13:57,519 --> 00:14:00,160
just like uh sample a little bit of

417
00:14:00,160 --> 00:14:01,760
sensitive data

418
00:14:01,760 --> 00:14:03,920
via the trial service it's a single api

419
00:14:03,920 --> 00:14:06,639
call it uses bigquery's table sample

420
00:14:06,639 --> 00:14:08,800
system for implementation and it gives

421
00:14:08,800 --> 00:14:11,360
us a whole litany of threat mitigations

422
00:14:11,360 --> 00:14:13,199
just from that simple

423
00:14:13,199 --> 00:14:15,279
wrapping of that operation in a service

424
00:14:15,279 --> 00:14:17,519
it's much easier to monitor for abuse we

425
00:14:17,519 --> 00:14:18,320
have

426
00:14:18,320 --> 00:14:20,079
a technical control for large scale

427
00:14:20,079 --> 00:14:22,399
exfiltration targeted searches

428
00:14:22,399 --> 00:14:24,240
and then as i mentioned before it allows

429
00:14:24,240 --> 00:14:25,600
us to just remove a bunch of over

430
00:14:25,600 --> 00:14:28,320
provisioning from our user accounts for

431
00:14:28,320 --> 00:14:30,560
our day-to-day work

432
00:14:30,560 --> 00:14:32,240
last step of this pipeline is a human in

433
00:14:32,240 --> 00:14:34,240
the loop label verification flow it's a

434
00:14:34,240 --> 00:14:36,480
cli tool pull samples of data using

435
00:14:36,480 --> 00:14:38,800
travel prompts us with

436
00:14:38,800 --> 00:14:41,199
the name of the field that our scanners

437
00:14:41,199 --> 00:14:42,880
believe to be a piece of sensitive data

438
00:14:42,880 --> 00:14:46,160
and we confirm or reject it

439
00:14:46,480 --> 00:14:48,160
with this data pipeline we managed to

440
00:14:48,160 --> 00:14:50,160
scale it from our two

441
00:14:50,160 --> 00:14:52,480
bigquery projects of interest to all of

442
00:14:52,480 --> 00:14:54,000
the hundreds of bigquery projects at

443
00:14:54,000 --> 00:14:56,560
cruz about 14 petabytes of underlying

444
00:14:56,560 --> 00:14:59,120
data covered millions of tables tens of

445
00:14:59,120 --> 00:15:00,720
millions of columns

446
00:15:00,720 --> 00:15:02,880
thousands of findings for sensitive data

447
00:15:02,880 --> 00:15:05,120
many of which were surprising

448
00:15:05,120 --> 00:15:07,120
now onto our use case for archiving

449
00:15:07,120 --> 00:15:09,600
abandoned sensitive data we studied

450
00:15:09,600 --> 00:15:11,360
usage logs for all of data that were

451
00:15:11,360 --> 00:15:12,800
tagged as sensitive

452
00:15:12,800 --> 00:15:16,000
by this system we found 55

453
00:15:16,000 --> 00:15:18,240
of those data tagged as sensitive were

454
00:15:18,240 --> 00:15:20,320
abandoned by their original owners a

455
00:15:20,320 --> 00:15:22,639
huge fraction it was quite surprising if

456
00:15:22,639 --> 00:15:24,800
you expand that window to say a piece of

457
00:15:24,800 --> 00:15:26,880
data is abandoned if somebody hasn't

458
00:15:26,880 --> 00:15:29,440
touched in 12 months that 55 number

459
00:15:29,440 --> 00:15:30,959
doesn't actually change much it only

460
00:15:30,959 --> 00:15:32,639
drops to 43

461
00:15:32,639 --> 00:15:35,279
so the half-life of data is is quite

462
00:15:35,279 --> 00:15:37,279
short

463
00:15:37,279 --> 00:15:38,000
we

464
00:15:38,000 --> 00:15:40,160
sought to address that gap in data

465
00:15:40,160 --> 00:15:42,639
handling via an archival scheme a soft

466
00:15:42,639 --> 00:15:44,720
deletion scheme we implemented that

467
00:15:44,720 --> 00:15:47,279
using google cloud's

468
00:15:47,279 --> 00:15:48,800
uh

469
00:15:48,800 --> 00:15:51,360
are you plugging me in thank you

470
00:15:51,360 --> 00:15:54,160
oh has that been up the whole time yeah

471
00:15:54,160 --> 00:15:55,920
who's like stressed out about the low

472
00:15:55,920 --> 00:15:57,600
battery that's

473
00:15:57,600 --> 00:15:58,800
cool

474
00:15:58,800 --> 00:15:59,920
i know these i've given this

475
00:15:59,920 --> 00:16:01,360
presentation so many times i just like

476
00:16:01,360 --> 00:16:02,959
know all the slides i'm like they turn

477
00:16:02,959 --> 00:16:03,500
off

478
00:16:03,500 --> 00:16:06,800
[Laughter]

479
00:16:06,800 --> 00:16:09,839
yeah yeah

480
00:16:10,320 --> 00:16:12,160
thank you appreciate that

481
00:16:12,160 --> 00:16:13,120
uh

482
00:16:13,120 --> 00:16:15,279
we implemented a soft deletion scheme

483
00:16:15,279 --> 00:16:18,160
using google cloud's policy tags it

484
00:16:18,160 --> 00:16:19,600
allows us to not delete the data but

485
00:16:19,600 --> 00:16:21,040
lock them down mitigate a lot of the

486
00:16:21,040 --> 00:16:22,959
privacy risk

487
00:16:22,959 --> 00:16:24,959
that was an interesting thing to develop

488
00:16:24,959 --> 00:16:26,399
because

489
00:16:26,399 --> 00:16:28,240
you all may have had this challenge of

490
00:16:28,240 --> 00:16:30,160
it is often difficult to fully delete

491
00:16:30,160 --> 00:16:33,759
data right you would love to right but

492
00:16:33,759 --> 00:16:35,199
as soon as you delete something that's

493
00:16:35,199 --> 00:16:37,680
critical to a business workflow

494
00:16:37,680 --> 00:16:39,600
you interrupt it

495
00:16:39,600 --> 00:16:41,680
senior leadership is on your back like

496
00:16:41,680 --> 00:16:43,680
it makes your job much harder for the

497
00:16:43,680 --> 00:16:45,920
rest of your rest of your time at that

498
00:16:45,920 --> 00:16:47,040
company so

499
00:16:47,040 --> 00:16:48,399
the development of this soft deletion

500
00:16:48,399 --> 00:16:50,480
scheme was pivotal towards being able to

501
00:16:50,480 --> 00:16:52,560
scale it across the entire data

502
00:16:52,560 --> 00:16:54,480
warehouse

503
00:16:54,480 --> 00:16:56,240
all right so

504
00:16:56,240 --> 00:16:58,959
closing comments

505
00:16:58,959 --> 00:17:01,120
as we said at the beginning of the talk

506
00:17:01,120 --> 00:17:02,800
we focused this just on privacy

507
00:17:02,800 --> 00:17:04,400
engineering use cases purposefully

508
00:17:04,400 --> 00:17:06,559
ignoring every other team

509
00:17:06,559 --> 00:17:07,839
we found that

510
00:17:07,839 --> 00:17:09,599
in undertaking this data mapping

511
00:17:09,599 --> 00:17:11,760
activity we spun out impact to other

512
00:17:11,760 --> 00:17:12,959
teams

513
00:17:12,959 --> 00:17:14,880
anyway just sort of happened along the

514
00:17:14,880 --> 00:17:16,480
way here's like a

515
00:17:16,480 --> 00:17:18,160
snippet of some of the messages we

516
00:17:18,160 --> 00:17:20,240
received and a description of some of

517
00:17:20,240 --> 00:17:23,199
those impacts for other teams

518
00:17:23,199 --> 00:17:25,280
this was interesting to find it suggests

519
00:17:25,280 --> 00:17:28,799
that data mapping if you execute it well

520
00:17:28,799 --> 00:17:30,880
is something not like an isolated

521
00:17:30,880 --> 00:17:32,640
activity just for privacy engineering

522
00:17:32,640 --> 00:17:34,559
just for compliance with something like

523
00:17:34,559 --> 00:17:37,120
a foundational capability for a data

524
00:17:37,120 --> 00:17:39,840
organization

525
00:17:39,840 --> 00:17:40,720
all right

526
00:17:40,720 --> 00:17:41,760
with that

527
00:17:41,760 --> 00:17:43,679
if all that sounds interesting our team

528
00:17:43,679 --> 00:17:45,440
is growing we have a lot of big privacy

529
00:17:45,440 --> 00:17:46,720
challenges so please

530
00:17:46,720 --> 00:17:50,360
come and talk to us

