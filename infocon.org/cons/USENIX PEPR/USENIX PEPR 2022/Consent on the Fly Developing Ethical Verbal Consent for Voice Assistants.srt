1
00:00:09,200 --> 00:00:11,440
hi my name's paul seymour and i'm here

2
00:00:11,440 --> 00:00:13,040
today to talk about some of the work

3
00:00:13,040 --> 00:00:14,400
that i've been doing with my colleagues

4
00:00:14,400 --> 00:00:16,560
at kings on developing ethical verbal

5
00:00:16,560 --> 00:00:18,640
consent for voice assistance

6
00:00:18,640 --> 00:00:20,560
the concept here is pretty simple we

7
00:00:20,560 --> 00:00:22,400
usually interact with voice assistants

8
00:00:22,400 --> 00:00:24,000
well verbally

9
00:00:24,000 --> 00:00:25,680
but often when a skill or action

10
00:00:25,680 --> 00:00:28,160
requires personal data you get diverted

11
00:00:28,160 --> 00:00:31,119
to a companion app on your smartphone

12
00:00:31,119 --> 00:00:33,120
instead these verbal consent mechanisms

13
00:00:33,120 --> 00:00:34,880
let you grant permissions without having

14
00:00:34,880 --> 00:00:36,399
to use your hands

15
00:00:36,399 --> 00:00:38,480
by moving consent into the conversation

16
00:00:38,480 --> 00:00:40,399
they make the process easier and more

17
00:00:40,399 --> 00:00:41,840
accessible

18
00:00:41,840 --> 00:00:44,079
just a heads up before we move on we're

19
00:00:44,079 --> 00:00:45,520
assuming here that some of the usual

20
00:00:45,520 --> 00:00:47,440
suspects like privacy policies are still

21
00:00:47,440 --> 00:00:50,160
present to cover all the legal angles

22
00:00:50,160 --> 00:00:52,559
and actually the dual roles that consent

23
00:00:52,559 --> 00:00:55,199
is playing here as a legal mechanism and

24
00:00:55,199 --> 00:00:56,960
as an ethical best practice is something

25
00:00:56,960 --> 00:01:00,480
that i'd like to return to later on

26
00:01:00,480 --> 00:01:02,719
finally being based in the uk we've got

27
00:01:02,719 --> 00:01:05,040
a more european perspective on data

28
00:01:05,040 --> 00:01:06,960
protection that's based around the gdpr

29
00:01:06,960 --> 00:01:09,119
and the uk equivalent

30
00:01:09,119 --> 00:01:10,960
so to give you a more concrete idea

31
00:01:10,960 --> 00:01:12,560
about what this actually looks like in

32
00:01:12,560 --> 00:01:15,119
practice here's the sample transcript

33
00:01:15,119 --> 00:01:17,759
from the alexa skills kit documentation

34
00:01:17,759 --> 00:01:19,280
you can see how this is actually a

35
00:01:19,280 --> 00:01:21,680
pretty usable conversation flow and

36
00:01:21,680 --> 00:01:23,439
integrates quite nicely into the natural

37
00:01:23,439 --> 00:01:25,600
way that you progress through the skill

38
00:01:25,600 --> 00:01:27,840
but under the surface there are some key

39
00:01:27,840 --> 00:01:30,240
issues that show how this kind of verbal

40
00:01:30,240 --> 00:01:32,400
permissions process at least as it's

41
00:01:32,400 --> 00:01:34,799
currently imagined and implemented goes

42
00:01:34,799 --> 00:01:36,479
against some of the key principles of

43
00:01:36,479 --> 00:01:38,159
consent

44
00:01:38,159 --> 00:01:40,560
for instance if we focus on the core of

45
00:01:40,560 --> 00:01:42,640
the consent flow here where the user is

46
00:01:42,640 --> 00:01:44,720
actually asked to give consent there's

47
00:01:44,720 --> 00:01:46,880
this subtle shift of the skill delegates

48
00:01:46,880 --> 00:01:48,640
to the alexa os

49
00:01:48,640 --> 00:01:50,799
but if you're listening to this as a

50
00:01:50,799 --> 00:01:52,799
user there's nothing to indicate that

51
00:01:52,799 --> 00:01:53,840
you're now talking to something

52
00:01:53,840 --> 00:01:55,600
controlled by amazon instead of the

53
00:01:55,600 --> 00:01:58,000
third-party skill that you started with

54
00:01:58,000 --> 00:01:59,920
they sound exactly the same and yet

55
00:01:59,920 --> 00:02:02,479
every other major os takes this kind of

56
00:02:02,479 --> 00:02:05,040
differentiation for security and privacy

57
00:02:05,040 --> 00:02:07,600
prompts very seriously

58
00:02:07,600 --> 00:02:09,119
and what about the information that you

59
00:02:09,119 --> 00:02:10,080
get

60
00:02:10,080 --> 00:02:12,319
if we think about consent in the context

61
00:02:12,319 --> 00:02:14,800
of the gdpr there's obviously a set of

62
00:02:14,800 --> 00:02:16,480
details that you absolutely need to

63
00:02:16,480 --> 00:02:17,520
include

64
00:02:17,520 --> 00:02:19,200
we get a little bit of that here in the

65
00:02:19,200 --> 00:02:21,440
sample text but when you're also

66
00:02:21,440 --> 00:02:23,120
providing a privacy policy not all of

67
00:02:23,120 --> 00:02:25,680
this is legally required but some pretty

68
00:02:25,680 --> 00:02:27,840
key details are missing from the

69
00:02:27,840 --> 00:02:29,920
exchange given that from a practical

70
00:02:29,920 --> 00:02:32,000
user perspective this effectively

71
00:02:32,000 --> 00:02:33,840
replaces the graphical alternative

72
00:02:33,840 --> 00:02:35,599
that's currently used

73
00:02:35,599 --> 00:02:37,519
even worse is that because of the way

74
00:02:37,519 --> 00:02:40,560
that amazon lists developer information

75
00:02:40,560 --> 00:02:42,560
even if you make full use of the website

76
00:02:42,560 --> 00:02:44,560
and companion app there's no guarantee

77
00:02:44,560 --> 00:02:46,400
that you'll ever be able to find out who

78
00:02:46,400 --> 00:02:47,840
has your data

79
00:02:47,840 --> 00:02:50,400
developer names are self-provided and

80
00:02:50,400 --> 00:02:52,239
research has shown that almost half the

81
00:02:52,239 --> 00:02:54,239
privacy policies and skills on the alexa

82
00:02:54,239 --> 00:02:56,000
marketplace are either inadequate or

83
00:02:56,000 --> 00:02:57,840
just straight up missing

84
00:02:57,840 --> 00:03:00,319
so your only real option here is to

85
00:03:00,319 --> 00:03:02,840
submit feedback to amazon and

86
00:03:02,840 --> 00:03:05,599
hope and about that last question around

87
00:03:05,599 --> 00:03:07,360
withdrawing consent

88
00:03:07,360 --> 00:03:09,599
if you give consent verbally how do you

89
00:03:09,599 --> 00:03:11,280
withdraw it

90
00:03:11,280 --> 00:03:13,360
if you use the conversation if you use

91
00:03:13,360 --> 00:03:15,599
the conventional permissions flow you

92
00:03:15,599 --> 00:03:17,519
get directed to the companion app where

93
00:03:17,519 --> 00:03:19,120
you have to check the boxes for each

94
00:03:19,120 --> 00:03:20,319
type of information that's been

95
00:03:20,319 --> 00:03:22,640
requested and at that point you know how

96
00:03:22,640 --> 00:03:24,480
to get there and it follows the

97
00:03:24,480 --> 00:03:26,959
unticking those boxes later on will stop

98
00:03:26,959 --> 00:03:29,680
the data sharing great

99
00:03:29,680 --> 00:03:31,280
but if you give consent verbally then

100
00:03:31,280 --> 00:03:32,959
you can't revoke it in the same way

101
00:03:32,959 --> 00:03:34,879
there are no voice commands for that

102
00:03:34,879 --> 00:03:36,560
you've got to find the menu in the app

103
00:03:36,560 --> 00:03:38,239
yourself because that's the only way to

104
00:03:38,239 --> 00:03:39,920
do it

105
00:03:39,920 --> 00:03:41,040
and the last thing i want to draw

106
00:03:41,040 --> 00:03:43,040
attention to is how the structure of

107
00:03:43,040 --> 00:03:45,440
conversations with voice assistants adds

108
00:03:45,440 --> 00:03:47,599
a background sense of time pressure to

109
00:03:47,599 --> 00:03:49,120
conversations

110
00:03:49,120 --> 00:03:50,720
you can see an overview here of the

111
00:03:50,720 --> 00:03:53,280
informed consent process for researchers

112
00:03:53,280 --> 00:03:54,799
and one of the key points is that you

113
00:03:54,799 --> 00:03:56,400
have as much time as you need to

114
00:03:56,400 --> 00:03:58,080
consider and respond

115
00:03:58,080 --> 00:03:59,920
but a fundamental element of

116
00:03:59,920 --> 00:04:02,319
conversation with voice assistants is

117
00:04:02,319 --> 00:04:04,560
the reprompt the device asks you a

118
00:04:04,560 --> 00:04:06,799
question and in the case of alexa you've

119
00:04:06,799 --> 00:04:09,519
got about eight seconds to respond

120
00:04:09,519 --> 00:04:11,200
even if you remove this for consent

121
00:04:11,200 --> 00:04:13,439
decisions just using the assistant

122
00:04:13,439 --> 00:04:15,280
day-to-day conditions people to the

123
00:04:15,280 --> 00:04:17,600
cadence of conversations in a way that's

124
00:04:17,600 --> 00:04:20,560
really difficult to escape

125
00:04:20,560 --> 00:04:21,358
so

126
00:04:21,358 --> 00:04:23,199
identifying problems is great but what

127
00:04:23,199 --> 00:04:25,199
about making things better

128
00:04:25,199 --> 00:04:26,800
there's some obvious stuff that jumps

129
00:04:26,800 --> 00:04:28,400
out as a direct response to the things

130
00:04:28,400 --> 00:04:30,560
i've been talking about like restoring

131
00:04:30,560 --> 00:04:32,240
symmetry to the consent process by

132
00:04:32,240 --> 00:04:34,720
adding new conversation options

133
00:04:34,720 --> 00:04:36,320
using a different voice for system and

134
00:04:36,320 --> 00:04:38,639
security messages seems like a go-to

135
00:04:38,639 --> 00:04:41,280
plan but there are some potential

136
00:04:41,280 --> 00:04:43,280
problems with this strategy that have

137
00:04:43,280 --> 00:04:45,280
been forecast in the literature since

138
00:04:45,280 --> 00:04:47,919
the early 90s around how people perceive

139
00:04:47,919 --> 00:04:50,160
and respond to different voices across

140
00:04:50,160 --> 00:04:51,600
devices

141
00:04:51,600 --> 00:04:53,440
more recently studies have also

142
00:04:53,440 --> 00:04:55,520
confirmed that users often make little

143
00:04:55,520 --> 00:04:57,360
distinction between first and third

144
00:04:57,360 --> 00:04:58,720
party skills

145
00:04:58,720 --> 00:05:00,479
and introducing what might be perceived

146
00:05:00,479 --> 00:05:02,400
as a separate new assistant could cause

147
00:05:02,400 --> 00:05:04,400
a lot of confusion

148
00:05:04,400 --> 00:05:06,080
but there are also some less obvious

149
00:05:06,080 --> 00:05:08,240
things like leveraging other interaction

150
00:05:08,240 --> 00:05:10,000
modalities to deliver some of the

151
00:05:10,000 --> 00:05:11,680
information that either isn't well

152
00:05:11,680 --> 00:05:14,400
suited to a verbal description or that

153
00:05:14,400 --> 00:05:16,160
we simply don't have the time to deliver

154
00:05:16,160 --> 00:05:17,600
via speech

155
00:05:17,600 --> 00:05:18,960
we see this to an extent with the

156
00:05:18,960 --> 00:05:20,880
current approach where the legally

157
00:05:20,880 --> 00:05:22,840
required notices are hidden away in the

158
00:05:22,840 --> 00:05:25,840
gui and on this note the way that we

159
00:05:25,840 --> 00:05:27,919
provide access to these mechanisms for

160
00:05:27,919 --> 00:05:30,240
developers is also important

161
00:05:30,240 --> 00:05:31,840
descriptions of why a skill needs

162
00:05:31,840 --> 00:05:33,759
personal information should be required

163
00:05:33,759 --> 00:05:36,320
by platforms and google for example

164
00:05:36,320 --> 00:05:38,080
already includes this contextual

165
00:05:38,080 --> 00:05:40,160
information as part of the permissions

166
00:05:40,160 --> 00:05:42,880
api for developers which in turn signals

167
00:05:42,880 --> 00:05:44,560
its importance

168
00:05:44,560 --> 00:05:46,639
providing actual guidance on what to say

169
00:05:46,639 --> 00:05:48,720
would also be a great help but this

170
00:05:48,720 --> 00:05:50,479
might need to come from regulators as

171
00:05:50,479 --> 00:05:52,400
platforms generally try and shift the

172
00:05:52,400 --> 00:05:54,479
legal responsibility for using their own

173
00:05:54,479 --> 00:05:58,080
apis correctly onto developers

174
00:05:58,080 --> 00:06:00,160
and finally i'd like to share some

175
00:06:00,160 --> 00:06:02,639
hidden opportunities that arise through

176
00:06:02,639 --> 00:06:04,240
taking a step back and critically

177
00:06:04,240 --> 00:06:06,400
looking at how we handle data sharing

178
00:06:06,400 --> 00:06:08,400
permissions and consent for voice

179
00:06:08,400 --> 00:06:09,520
assistance

180
00:06:09,520 --> 00:06:11,280
some examples of this might be breaking

181
00:06:11,280 --> 00:06:12,880
away from permissions dialogues being

182
00:06:12,880 --> 00:06:14,880
this hobson's choice where declining

183
00:06:14,880 --> 00:06:16,720
sets you back to square one

184
00:06:16,720 --> 00:06:18,639
sign posting people with alternatives if

185
00:06:18,639 --> 00:06:20,720
they decline to share their information

186
00:06:20,720 --> 00:06:22,720
also shifts the incentive back towards

187
00:06:22,720 --> 00:06:24,960
this sort of ideal state here where apps

188
00:06:24,960 --> 00:06:26,720
and skills only ask for the data that

189
00:06:26,720 --> 00:06:30,160
they absolutely need to get the job done

190
00:06:30,160 --> 00:06:31,919
this is also something that could find

191
00:06:31,919 --> 00:06:34,319
its way into the algorithms that already

192
00:06:34,319 --> 00:06:36,319
shape the usage of these platforms by

193
00:06:36,319 --> 00:06:38,479
deciding which results are prioritized

194
00:06:38,479 --> 00:06:39,520
when people

195
00:06:39,520 --> 00:06:42,240
ask for a skill that is a particular job

196
00:06:42,240 --> 00:06:42,960
or

197
00:06:42,960 --> 00:06:45,039
fits in a particular category

198
00:06:45,039 --> 00:06:47,199
such as by preferring options that use

199
00:06:47,199 --> 00:06:48,560
less data

200
00:06:48,560 --> 00:06:50,720
one interesting concept is potentially

201
00:06:50,720 --> 00:06:53,440
being able to make automated decisions

202
00:06:53,440 --> 00:06:55,360
for consent based on kind of general

203
00:06:55,360 --> 00:06:57,599
user preferences and this represents one

204
00:06:57,599 --> 00:06:58,720
of the sort of more exciting

205
00:06:58,720 --> 00:07:00,479
opportunities to reduce the burden on

206
00:07:00,479 --> 00:07:02,560
users and make data sharing decisions

207
00:07:02,560 --> 00:07:04,880
less adversarial than they currently are

208
00:07:04,880 --> 00:07:07,280
you know say on the web

209
00:07:07,280 --> 00:07:08,800
and reviewing the structure of voice

210
00:07:08,800 --> 00:07:10,720
assistant platforms is something else

211
00:07:10,720 --> 00:07:12,479
that could shape the role that consent

212
00:07:12,479 --> 00:07:15,120
plays in the overall user experience

213
00:07:15,120 --> 00:07:17,039
tightening up vetting and certification

214
00:07:17,039 --> 00:07:18,800
processes would add an extra line of

215
00:07:18,800 --> 00:07:20,960
defense against third-party applications

216
00:07:20,960 --> 00:07:22,319
that violate the the rules and

217
00:07:22,319 --> 00:07:25,360
principles around data protection

218
00:07:25,360 --> 00:07:27,039
the last thing i want to talk about is

219
00:07:27,039 --> 00:07:29,919
the importance of disentangling consent

220
00:07:29,919 --> 00:07:31,680
as a legal mechanism

221
00:07:31,680 --> 00:07:33,280
from the role that consent plays as an

222
00:07:33,280 --> 00:07:35,199
ethical best practice the two don't

223
00:07:35,199 --> 00:07:37,280
always need to come together for most

224
00:07:37,280 --> 00:07:39,039
academic research conducted under the

225
00:07:39,039 --> 00:07:41,680
gdpr there's a rigorous informed consent

226
00:07:41,680 --> 00:07:43,919
process but it relies on public interest

227
00:07:43,919 --> 00:07:47,199
as the legal basis for data collection

228
00:07:47,199 --> 00:07:48,879
there are also models like dynamic

229
00:07:48,879 --> 00:07:50,720
consent that have emerged with the aim

230
00:07:50,720 --> 00:07:52,080
of increasing engagement and

231
00:07:52,080 --> 00:07:54,240
understanding around data use

232
00:07:54,240 --> 00:07:55,840
the kinds of platforms that voice

233
00:07:55,840 --> 00:07:58,319
assistants have become also raises some

234
00:07:58,319 --> 00:08:00,319
gnarly questions around who the data

235
00:08:00,319 --> 00:08:03,440
controller or controllers are

236
00:08:03,440 --> 00:08:05,599
in any given situation is it the

237
00:08:05,599 --> 00:08:07,520
platform the developer what are the

238
00:08:07,520 --> 00:08:09,280
legal basis being used for different

239
00:08:09,280 --> 00:08:11,120
kinds of data

240
00:08:11,120 --> 00:08:13,039
how does this fit in with developers

241
00:08:13,039 --> 00:08:15,280
where they're legally responsible for

242
00:08:15,280 --> 00:08:17,680
satisfying data protection regulations

243
00:08:17,680 --> 00:08:19,360
but still have to work within the rigid

244
00:08:19,360 --> 00:08:22,319
confines of what platforms allow

245
00:08:22,319 --> 00:08:24,479
in most cases verbal consent like i've

246
00:08:24,479 --> 00:08:25,919
been talking about today needs to be

247
00:08:25,919 --> 00:08:28,080
backed up with a privacy policy

248
00:08:28,080 --> 00:08:29,759
uh especially where consent is required

249
00:08:29,759 --> 00:08:32,799
by law in order to use data

250
00:08:32,799 --> 00:08:34,719
current implementations make the process

251
00:08:34,719 --> 00:08:36,000
more usable

252
00:08:36,000 --> 00:08:38,880
uh like voice forward consent for alexa

253
00:08:38,880 --> 00:08:40,719
but as we've seen have some pretty

254
00:08:40,719 --> 00:08:42,880
serious problems and really miss out on

255
00:08:42,880 --> 00:08:44,800
the benefits that consent is capable of

256
00:08:44,800 --> 00:08:47,600
bringing as an ethical best practice

257
00:08:47,600 --> 00:08:49,360
this is particularly true around issues

258
00:08:49,360 --> 00:08:51,200
of transparency and in particular

259
00:08:51,200 --> 00:08:53,200
helping users to better understand the

260
00:08:53,200 --> 00:08:55,200
services that they're using and to

261
00:08:55,200 --> 00:08:58,640
express their own privacy preferences

262
00:08:58,640 --> 00:09:00,399
thank you very much for listening if you

263
00:09:00,399 --> 00:09:02,000
have any questions about this ongoing

264
00:09:02,000 --> 00:09:04,080
line of research then there's a live q a

265
00:09:04,080 --> 00:09:06,240
now and obviously feel free to follow up

266
00:09:06,240 --> 00:09:07,680
after the conference via email if you

267
00:09:07,680 --> 00:09:12,040
want to discuss anything thanks

