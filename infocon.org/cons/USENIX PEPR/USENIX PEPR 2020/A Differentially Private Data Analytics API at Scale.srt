1
00:00:08,639 --> 00:00:09,360
hello

2
00:00:09,360 --> 00:00:11,440
i'm ryan and i'm here to talk to you

3
00:00:11,440 --> 00:00:12,400
about

4
00:00:12,400 --> 00:00:14,320
a privacy preserving data analytics

5
00:00:14,320 --> 00:00:15,759
system

6
00:00:15,759 --> 00:00:17,920
that we did at linkedin for their

7
00:00:17,920 --> 00:00:20,160
audience engagement api

8
00:00:20,160 --> 00:00:22,480
and this is uh joint work with my great

9
00:00:22,480 --> 00:00:23,840
collaborators

10
00:00:23,840 --> 00:00:27,119
at linkedin so

11
00:00:27,119 --> 00:00:29,840
uh the agenda for this talk is i'll be

12
00:00:29,840 --> 00:00:31,279
giving you an overview of what

13
00:00:31,279 --> 00:00:33,120
differential privacy is a very brief

14
00:00:33,120 --> 00:00:33,680
crash

15
00:00:33,680 --> 00:00:36,320
crash course in it uh and then i'm going

16
00:00:36,320 --> 00:00:38,000
to talk about what the specific

17
00:00:38,000 --> 00:00:39,920
application or use case

18
00:00:39,920 --> 00:00:43,120
uh is at linkedin in which we apply

19
00:00:43,120 --> 00:00:44,079
differential privacy

20
00:00:44,079 --> 00:00:47,440
to and then i'll talk about the overall

21
00:00:47,440 --> 00:00:49,200
privacy system

22
00:00:49,200 --> 00:00:53,520
that we built that incorporates novel

23
00:00:53,520 --> 00:00:55,760
uh differentially private algorithms

24
00:00:55,760 --> 00:00:56,640
with a new

25
00:00:56,640 --> 00:01:00,320
privacy budget management service

26
00:01:00,480 --> 00:01:03,520
so the overall mission of

27
00:01:03,520 --> 00:01:05,360
of our team is that we want to be able

28
00:01:05,360 --> 00:01:07,600
to utilize the

29
00:01:07,600 --> 00:01:09,920
maintain the usefulness of data while

30
00:01:09,920 --> 00:01:12,159
still protecting the privacy

31
00:01:12,159 --> 00:01:14,560
of users and this seems to kind of be at

32
00:01:14,560 --> 00:01:15,680
odds with each other

33
00:01:15,680 --> 00:01:18,560
like we want to get the most usefulness

34
00:01:18,560 --> 00:01:19,520
of data that

35
00:01:19,520 --> 00:01:21,920
consists of individual records while

36
00:01:21,920 --> 00:01:23,840
still protecting the privacy of

37
00:01:23,840 --> 00:01:25,680
individual members that make up the data

38
00:01:25,680 --> 00:01:27,840
set

39
00:01:27,840 --> 00:01:31,520
and there are reason many reasons to do

40
00:01:31,520 --> 00:01:33,840
privacy there's recent regulation with

41
00:01:33,840 --> 00:01:34,640
gdpr

42
00:01:34,640 --> 00:01:38,320
and ccpa at linkedin we always think of

43
00:01:38,320 --> 00:01:38,720
how

44
00:01:38,720 --> 00:01:40,880
to be members first with any new product

45
00:01:40,880 --> 00:01:42,640
or feature we build

46
00:01:42,640 --> 00:01:44,600
uh and there's existing

47
00:01:44,600 --> 00:01:46,720
de-identification techniques to sort of

48
00:01:46,720 --> 00:01:47,840
scrub

49
00:01:47,840 --> 00:01:51,840
personally identifying information about

50
00:01:51,840 --> 00:01:54,399
members in a data set but these

51
00:01:54,399 --> 00:01:56,479
anonymization techniques just don't go

52
00:01:56,479 --> 00:01:57,200
far enough

53
00:01:57,200 --> 00:01:59,759
in protecting privacy of members uh

54
00:01:59,759 --> 00:02:01,920
cynthia dorak one of the founders of

55
00:02:01,920 --> 00:02:05,200
differential privacy said simply that

56
00:02:05,200 --> 00:02:07,840
anonymized data isn't and by that she

57
00:02:07,840 --> 00:02:08,720
meant

58
00:02:08,720 --> 00:02:11,760
two things either one uh

59
00:02:11,760 --> 00:02:13,840
you either have scrubbed the data so

60
00:02:13,840 --> 00:02:16,239
much uh that it's no longer useful

61
00:02:16,239 --> 00:02:20,720
it's no longer data or you've

62
00:02:20,720 --> 00:02:22,319
scrubbed it or you haven't scrubbed it

63
00:02:22,319 --> 00:02:24,400
enough you haven't anonymized it enough

64
00:02:24,400 --> 00:02:26,239
uh to where it's not really anonymized

65
00:02:26,239 --> 00:02:28,080
at all and uh

66
00:02:28,080 --> 00:02:30,560
so even with very few features in the

67
00:02:30,560 --> 00:02:31,360
data set

68
00:02:31,360 --> 00:02:33,519
you can uniquely identify a vast

69
00:02:33,519 --> 00:02:36,160
majority of the us

70
00:02:36,160 --> 00:02:38,640
and there's also potential attacks uh

71
00:02:38,640 --> 00:02:39,360
that

72
00:02:39,360 --> 00:02:42,239
can slice and dice the data and

73
00:02:42,239 --> 00:02:43,440
re-identify

74
00:02:43,440 --> 00:02:45,840
individuals in the data set now rather

75
00:02:45,840 --> 00:02:46,800
than sort of

76
00:02:46,800 --> 00:02:49,360
building techniques that mitigate all of

77
00:02:49,360 --> 00:02:50,080
these

78
00:02:50,080 --> 00:02:51,760
attacks by just putting like band-aids

79
00:02:51,760 --> 00:02:53,200
on each one until a new

80
00:02:53,200 --> 00:02:56,879
attack is uh invented

81
00:02:56,879 --> 00:03:00,480
uh what's a more privacy first approach

82
00:03:00,480 --> 00:03:02,159
so here i want to talk about

83
00:03:02,159 --> 00:03:03,519
differential privacy

84
00:03:03,519 --> 00:03:05,440
and this is something that's been around

85
00:03:05,440 --> 00:03:07,120
for about 15 years

86
00:03:07,120 --> 00:03:08,959
and here's a very brief introduction of

87
00:03:08,959 --> 00:03:11,519
what it is so imagine we have some data

88
00:03:11,519 --> 00:03:11,920
set

89
00:03:11,920 --> 00:03:14,400
x which consists of a bunch of users

90
00:03:14,400 --> 00:03:15,920
records

91
00:03:15,920 --> 00:03:17,760
and we want to run some algorithm or

92
00:03:17,760 --> 00:03:20,080
computation on this data set

93
00:03:20,080 --> 00:03:22,800
so you can think of whatever computation

94
00:03:22,800 --> 00:03:24,480
you want to do on this data set perhaps

95
00:03:24,480 --> 00:03:26,000
you want to run a logistic regression

96
00:03:26,000 --> 00:03:26,799
model

97
00:03:26,799 --> 00:03:30,239
on these users feature vectors or you

98
00:03:30,239 --> 00:03:31,200
want to

99
00:03:31,200 --> 00:03:33,920
add up a bunch of sensitive bits about

100
00:03:33,920 --> 00:03:34,799
all the

101
00:03:34,799 --> 00:03:37,280
individuals and and that's your

102
00:03:37,280 --> 00:03:38,640
algorithm

103
00:03:38,640 --> 00:03:39,920
no matter what you want to do you want

104
00:03:39,920 --> 00:03:41,840
to do something to the data set and we

105
00:03:41,840 --> 00:03:42,879
want to think of what

106
00:03:42,879 --> 00:03:45,680
computation we do on the data and so

107
00:03:45,680 --> 00:03:47,280
what differential privacy does is that

108
00:03:47,280 --> 00:03:48,319
it introduced some

109
00:03:48,319 --> 00:03:52,239
random noise into the computation

110
00:03:52,239 --> 00:03:54,239
and the reason for this is to sort of

111
00:03:54,239 --> 00:03:56,640
hide what the possible

112
00:03:56,640 --> 00:03:59,360
outcome could be given this particular

113
00:03:59,360 --> 00:04:00,720
input data set

114
00:04:00,720 --> 00:04:03,360
and so the result is going to be some

115
00:04:03,360 --> 00:04:05,280
output distribution where some

116
00:04:05,280 --> 00:04:09,040
outcomes are more likely than others

117
00:04:09,040 --> 00:04:11,120
but what we want to do is run the

118
00:04:11,120 --> 00:04:12,640
thought experiment

119
00:04:12,640 --> 00:04:15,120
where what we ask what happens if we

120
00:04:15,120 --> 00:04:17,120
were to eliminate one person's data

121
00:04:17,120 --> 00:04:18,959
record from the data set

122
00:04:18,959 --> 00:04:21,759
and run the exact same algorithm on it

123
00:04:21,759 --> 00:04:24,479
so the result is going to be a slightly

124
00:04:24,479 --> 00:04:27,440
shifted distribution depending on

125
00:04:27,440 --> 00:04:28,479
whatever

126
00:04:28,479 --> 00:04:29,759
algorithm and randomness you're

127
00:04:29,759 --> 00:04:32,880
introducing uh and the amount of privacy

128
00:04:32,880 --> 00:04:36,240
that we say this algorithm preserves

129
00:04:36,240 --> 00:04:38,240
is really a function of how much these

130
00:04:38,240 --> 00:04:40,160
two distributions can differ

131
00:04:40,160 --> 00:04:41,840
so these two distributions are right on

132
00:04:41,840 --> 00:04:43,600
top of each other you could say that

133
00:04:43,600 --> 00:04:45,120
doug's data doesn't influence the

134
00:04:45,120 --> 00:04:46,320
outcome whatsoever

135
00:04:46,320 --> 00:04:49,600
and so uh in terms of uh privacy doug's

136
00:04:49,600 --> 00:04:51,040
data is like perf

137
00:04:51,040 --> 00:04:53,520
uh privacy is perfectly preserved

138
00:04:53,520 --> 00:04:55,199
however these two distributions are very

139
00:04:55,199 --> 00:04:56,880
far apart from one another

140
00:04:56,880 --> 00:04:59,199
now you can say that doug's data heavily

141
00:04:59,199 --> 00:05:00,960
influences the outcome

142
00:05:00,960 --> 00:05:04,479
so for this we say that

143
00:05:04,479 --> 00:05:08,479
the algorithm doesn't have much privacy

144
00:05:08,479 --> 00:05:10,240
so writing that down and sort of in some

145
00:05:10,240 --> 00:05:12,560
mathematics we say that a randomized

146
00:05:12,560 --> 00:05:14,320
algorithm is differentially private or

147
00:05:14,320 --> 00:05:15,520
dp

148
00:05:15,520 --> 00:05:17,360
if for any two neighboring data sets

149
00:05:17,360 --> 00:05:19,199
these are data sets that differ at most

150
00:05:19,199 --> 00:05:20,800
one person's record so

151
00:05:20,800 --> 00:05:22,880
in the previous example the data set

152
00:05:22,880 --> 00:05:26,080
with doug's data and without doug's data

153
00:05:26,080 --> 00:05:27,680
we want to make sure that the output

154
00:05:27,680 --> 00:05:29,280
distribution

155
00:05:29,280 --> 00:05:32,000
whether we use x or x prime are close to

156
00:05:32,000 --> 00:05:32,880
one another

157
00:05:32,880 --> 00:05:34,720
and this closeness is measured by two

158
00:05:34,720 --> 00:05:36,160
privacy parameters

159
00:05:36,160 --> 00:05:38,240
one is the epsilon which is typically

160
00:05:38,240 --> 00:05:39,759
referred to as the privacy loss

161
00:05:39,759 --> 00:05:40,720
parameter

162
00:05:40,720 --> 00:05:42,479
and the delta it can be thought of as

163
00:05:42,479 --> 00:05:43,840
like um

164
00:05:43,840 --> 00:05:46,160
a very small probability in which like

165
00:05:46,160 --> 00:05:48,240
uh with probability at most delta your

166
00:05:48,240 --> 00:05:50,840
privacy loss could be larger than

167
00:05:50,840 --> 00:05:53,440
epsilon

168
00:05:53,440 --> 00:05:56,000
so there's been lots of deployments of

169
00:05:56,000 --> 00:05:58,080
differential privacy and i want to

170
00:05:58,080 --> 00:06:01,280
go over two distinct models of

171
00:06:01,280 --> 00:06:04,479
differential privacy so

172
00:06:04,479 --> 00:06:06,560
we can think of sort of the data

173
00:06:06,560 --> 00:06:07,919
pipeline as being

174
00:06:07,919 --> 00:06:10,639
users are generating data and then this

175
00:06:10,639 --> 00:06:12,880
data is then stored on some central

176
00:06:12,880 --> 00:06:16,080
uh data center and then this uh

177
00:06:16,080 --> 00:06:19,039
data is this data set is used to then

178
00:06:19,039 --> 00:06:20,160
power some new

179
00:06:20,160 --> 00:06:23,680
applications at a company such as giving

180
00:06:23,680 --> 00:06:26,720
informative dashboards new data apis or

181
00:06:26,720 --> 00:06:28,400
machine learning models

182
00:06:28,400 --> 00:06:30,560
and so depending on where you are in

183
00:06:30,560 --> 00:06:32,160
this

184
00:06:32,160 --> 00:06:33,840
pipeline you might want to introduce

185
00:06:33,840 --> 00:06:35,919
privacy in different points

186
00:06:35,919 --> 00:06:37,759
and so in the local differential privacy

187
00:06:37,759 --> 00:06:40,400
model this is where we privatized

188
00:06:40,400 --> 00:06:42,800
uh privatized data or the user's

189
00:06:42,800 --> 00:06:43,840
privatized data

190
00:06:43,840 --> 00:06:46,479
before it's being sent uh to a data

191
00:06:46,479 --> 00:06:47,280
center

192
00:06:47,280 --> 00:06:49,120
and so some notable deployments of this

193
00:06:49,120 --> 00:06:52,080
include microsoft with telemetry data

194
00:06:52,080 --> 00:06:55,199
google with

195
00:06:55,199 --> 00:06:57,039
detecting malicious home pages on the

196
00:06:57,039 --> 00:06:58,479
chrome browser

197
00:06:58,479 --> 00:07:02,560
and apple with finding popular emojis um

198
00:07:02,560 --> 00:07:05,919
uh in ios so uh

199
00:07:05,919 --> 00:07:08,639
that's the the local model in the global

200
00:07:08,639 --> 00:07:10,479
model this is where users are going to

201
00:07:10,479 --> 00:07:11,520
generate data this

202
00:07:11,520 --> 00:07:14,800
data has been stored on some data center

203
00:07:14,800 --> 00:07:17,280
and then we want to build applications

204
00:07:17,280 --> 00:07:19,440
that utilize this data set so they can

205
00:07:19,440 --> 00:07:22,400
freely access the this data but we want

206
00:07:22,400 --> 00:07:24,560
to ensure that the application itself

207
00:07:24,560 --> 00:07:27,360
is is privatized so because this

208
00:07:27,360 --> 00:07:28,800
application could be

209
00:07:28,800 --> 00:07:33,759
shown outside of the company's

210
00:07:33,759 --> 00:07:36,960
view so for this we have

211
00:07:36,960 --> 00:07:38,560
some deployments in the global

212
00:07:38,560 --> 00:07:40,240
differential privacy model including the

213
00:07:40,240 --> 00:07:41,840
2020 census

214
00:07:41,840 --> 00:07:45,120
uh then there's a microsoft's uh open

215
00:07:45,120 --> 00:07:47,440
data dp project with uh this joint

216
00:07:47,440 --> 00:07:49,280
collaboration with harvard

217
00:07:49,280 --> 00:07:52,160
uh and also uh google's open source uh

218
00:07:52,160 --> 00:07:54,720
library

219
00:07:55,120 --> 00:07:57,039
so i want to talk about the specific uh

220
00:07:57,039 --> 00:07:58,639
use case uh

221
00:07:58,639 --> 00:08:00,800
that we tackled at linkedin and it's

222
00:08:00,800 --> 00:08:03,120
called the audience engagement api

223
00:08:03,120 --> 00:08:05,199
this is an api product that provides

224
00:08:05,199 --> 00:08:08,000
insights on linkedin engagement content

225
00:08:08,000 --> 00:08:10,879
and it enables external marketing

226
00:08:10,879 --> 00:08:13,280
partners uh

227
00:08:13,280 --> 00:08:16,879
to analyze data about uh about members

228
00:08:16,879 --> 00:08:19,199
and this is built on top of pino for

229
00:08:19,199 --> 00:08:21,360
fast real-time data analytics

230
00:08:21,360 --> 00:08:24,319
and so here's a screenshot of like what

231
00:08:24,319 --> 00:08:25,199
one

232
00:08:25,199 --> 00:08:27,280
external marketing partner might build

233
00:08:27,280 --> 00:08:28,479
using this

234
00:08:28,479 --> 00:08:31,359
api product

235
00:08:32,399 --> 00:08:35,200
now taking a step back let's understand

236
00:08:35,200 --> 00:08:35,519
the

237
00:08:35,519 --> 00:08:38,479
the task here we have an advertiser that

238
00:08:38,479 --> 00:08:40,240
can interact adaptably with

239
00:08:40,240 --> 00:08:42,958
with an api so they can ask one query

240
00:08:42,958 --> 00:08:44,720
receive the answer and then as a

241
00:08:44,720 --> 00:08:45,279
function

242
00:08:45,279 --> 00:08:48,640
of this uh query response

243
00:08:48,640 --> 00:08:51,040
they can then ask a brand new query and

244
00:08:51,040 --> 00:08:52,560
so this makes differencing attacks a

245
00:08:52,560 --> 00:08:53,360
concern

246
00:08:53,360 --> 00:08:55,040
and so a differencing attack is when you

247
00:08:55,040 --> 00:08:57,360
ask two aggregate statistics

248
00:08:57,360 --> 00:09:00,800
uh back to back and

249
00:09:00,800 --> 00:09:03,040
we know that exactly one person is

250
00:09:03,040 --> 00:09:04,720
different between these two aggregates

251
00:09:04,720 --> 00:09:06,240
and taking the difference

252
00:09:06,240 --> 00:09:09,360
exposes uh what one person's data was

253
00:09:09,360 --> 00:09:11,120
such as what are the top 10 articles

254
00:09:11,120 --> 00:09:13,839
being viewed among all ceos in india

255
00:09:13,839 --> 00:09:15,600
followed by what are the top 10 articles

256
00:09:15,600 --> 00:09:17,920
being viewed among all ceos in india

257
00:09:17,920 --> 00:09:20,320
or at linkedin so you could take the

258
00:09:20,320 --> 00:09:21,440
difference and identify

259
00:09:21,440 --> 00:09:25,279
what one user uh how one user engaged

260
00:09:25,279 --> 00:09:25,760
with

261
00:09:25,760 --> 00:09:29,920
uh articles on linkedin so uh

262
00:09:29,920 --> 00:09:31,839
so differencing attacks are concerned

263
00:09:31,839 --> 00:09:33,760
and this is exactly why we're focusing

264
00:09:33,760 --> 00:09:37,440
on differential privacy as a mitigation

265
00:09:37,440 --> 00:09:39,920
so we we don't want to we we want to

266
00:09:39,920 --> 00:09:41,519
ensure privacy but we don't want to just

267
00:09:41,519 --> 00:09:43,200
totally ignore the real-time data

268
00:09:43,200 --> 00:09:44,640
analytics requirements

269
00:09:44,640 --> 00:09:47,360
uh of the of the product itself so we

270
00:09:47,360 --> 00:09:48,560
want to still provide

271
00:09:48,560 --> 00:09:52,000
real-time data analytics plus privacy

272
00:09:52,000 --> 00:09:54,000
and and the general queries that are

273
00:09:54,000 --> 00:09:55,279
allowed by this api

274
00:09:55,279 --> 00:09:57,600
are what's called top k queries such as

275
00:09:57,600 --> 00:09:59,360
what are the top 10 skill sets in a

276
00:09:59,360 --> 00:10:00,640
particular region

277
00:10:00,640 --> 00:10:03,200
where the top 10 articles among all data

278
00:10:03,200 --> 00:10:04,880
scientists

279
00:10:04,880 --> 00:10:07,200
things like this these types of queries

280
00:10:07,200 --> 00:10:08,480
and so the questions that need to be

281
00:10:08,480 --> 00:10:09,839
addressed are

282
00:10:09,839 --> 00:10:11,839
how much can a single user affect the

283
00:10:11,839 --> 00:10:13,839
outcome of these queries

284
00:10:13,839 --> 00:10:16,959
so central to differential privacy is

285
00:10:16,959 --> 00:10:18,640
the notion of sensitivity

286
00:10:18,640 --> 00:10:21,120
and if one user modifies the r can

287
00:10:21,120 --> 00:10:23,040
modify the result by a lot then we have

288
00:10:23,040 --> 00:10:25,200
to add a lot of noise or maybe develop a

289
00:10:25,200 --> 00:10:27,279
brand new algorithm

290
00:10:27,279 --> 00:10:28,959
another question that we need to answer

291
00:10:28,959 --> 00:10:30,880
is how many queries can the advertiser

292
00:10:30,880 --> 00:10:32,160
actually ask

293
00:10:32,160 --> 00:10:34,160
maybe we want to limit how many queries

294
00:10:34,160 --> 00:10:35,760
that advertiser can actually make so

295
00:10:35,760 --> 00:10:36,720
that they don't

296
00:10:36,720 --> 00:10:40,240
just necessarily uh reconstruct the

297
00:10:40,240 --> 00:10:42,079
whole data set based on these different

298
00:10:42,079 --> 00:10:44,880
top k queries

299
00:10:44,880 --> 00:10:47,440
okay so what i want to point out here is

300
00:10:47,440 --> 00:10:48,240
that there is

301
00:10:48,240 --> 00:10:50,399
there's existing infrastructure at

302
00:10:50,399 --> 00:10:51,360
linkedin

303
00:10:51,360 --> 00:10:52,959
uh that might not necessarily have

304
00:10:52,959 --> 00:10:54,880
differential privacy in mind

305
00:10:54,880 --> 00:10:56,880
so here you can imagine we have some

306
00:10:56,880 --> 00:10:58,560
highly distributed data set

307
00:10:58,560 --> 00:11:00,640
dataset that's cross across lots of data

308
00:11:00,640 --> 00:11:02,240
centers and we have

309
00:11:02,240 --> 00:11:04,399
already this existing system that can

310
00:11:04,399 --> 00:11:05,440
solve the top k

311
00:11:05,440 --> 00:11:08,320
prime uh very efficiently right in this

312
00:11:08,320 --> 00:11:10,160
highly distributed fashion

313
00:11:10,160 --> 00:11:12,000
and so what this top k prime solver is

314
00:11:12,000 --> 00:11:13,519
then going to generate is like this

315
00:11:13,519 --> 00:11:14,160
histogram

316
00:11:14,160 --> 00:11:16,560
of all possible outcomes so if i ask for

317
00:11:16,560 --> 00:11:17,839
the top 25

318
00:11:17,839 --> 00:11:21,040
uh skills then maybe this is the result

319
00:11:21,040 --> 00:11:21,839
of that

320
00:11:21,839 --> 00:11:24,320
computation and this can run very very

321
00:11:24,320 --> 00:11:25,200
fast

322
00:11:25,200 --> 00:11:26,640
and so when we want to introduce

323
00:11:26,640 --> 00:11:28,399
differential privacy we don't want to

324
00:11:28,399 --> 00:11:29,040
think about

325
00:11:29,040 --> 00:11:31,760
how we could modify the underlying data

326
00:11:31,760 --> 00:11:32,240
set

327
00:11:32,240 --> 00:11:35,600
itself because this would require a

328
00:11:35,600 --> 00:11:39,680
kind of accessing the data and running

329
00:11:39,680 --> 00:11:42,160
taking a very long time in order to run

330
00:11:42,160 --> 00:11:43,680
each particular query and we want this

331
00:11:43,680 --> 00:11:45,279
to be online

332
00:11:45,279 --> 00:11:47,440
so being able to generate these

333
00:11:47,440 --> 00:11:49,120
aggregate statistics and then

334
00:11:49,120 --> 00:11:51,279
this is where we want to privatize and

335
00:11:51,279 --> 00:11:52,880
apply differential privacy

336
00:11:52,880 --> 00:11:55,120
just to the aggregates so this will

337
00:11:55,120 --> 00:11:56,320
allow us to

338
00:11:56,320 --> 00:11:58,399
utilize or leverage the existing

339
00:11:58,399 --> 00:11:59,440
infrastructure

340
00:11:59,440 --> 00:12:01,200
while still providing differential

341
00:12:01,200 --> 00:12:03,279
privacy rather than going in and

342
00:12:03,279 --> 00:12:05,279
modifying individual data records

343
00:12:05,279 --> 00:12:07,839
with each record with each query that's

344
00:12:07,839 --> 00:12:09,519
asked

345
00:12:09,519 --> 00:12:11,200
and we can think of this top k prime

346
00:12:11,200 --> 00:12:12,800
solver as this uh

347
00:12:12,800 --> 00:12:15,920
pinot infrastructure which is an open

348
00:12:15,920 --> 00:12:17,040
source project that

349
00:12:17,040 --> 00:12:20,639
uh that's behind lots of the

350
00:12:20,639 --> 00:12:23,200
powerful features that linkedin such as

351
00:12:23,200 --> 00:12:24,079
people

352
00:12:24,079 --> 00:12:27,920
you may know okay so here's the overall

353
00:12:27,920 --> 00:12:29,279
privacy system

354
00:12:29,279 --> 00:12:31,440
we have a data set that's highly

355
00:12:31,440 --> 00:12:32,639
distributed

356
00:12:32,639 --> 00:12:34,959
and pino is our existing infrastructure

357
00:12:34,959 --> 00:12:36,160
that's making sense

358
00:12:36,160 --> 00:12:38,720
of all this highly distributed data and

359
00:12:38,720 --> 00:12:40,160
we have this marketing partner at the

360
00:12:40,160 --> 00:12:41,040
other end

361
00:12:41,040 --> 00:12:43,680
which is accessing this application via

362
00:12:43,680 --> 00:12:45,040
this api

363
00:12:45,040 --> 00:12:46,399
and so in particular it's going to be

364
00:12:46,399 --> 00:12:49,600
asking top k queries

365
00:12:49,600 --> 00:12:50,959
and we want to introduce our

366
00:12:50,959 --> 00:12:53,200
differentially private algorithms here

367
00:12:53,200 --> 00:12:56,320
in between the application uh and

368
00:12:56,320 --> 00:12:58,480
pinot itself rather than trying to

369
00:12:58,480 --> 00:12:59,360
modify the

370
00:12:59,360 --> 00:13:02,079
underlying data set we want it to be a

371
00:13:02,079 --> 00:13:03,680
layer on top

372
00:13:03,680 --> 00:13:06,079
and so here the query will then go

373
00:13:06,079 --> 00:13:07,200
through our differentially private

374
00:13:07,200 --> 00:13:09,360
algorithms perhaps we'll translate them

375
00:13:09,360 --> 00:13:10,800
and then once we get the result from

376
00:13:10,800 --> 00:13:13,200
pino then we will add noise in

377
00:13:13,200 --> 00:13:14,959
some clever fashion and give you the

378
00:13:14,959 --> 00:13:18,239
differently private result

379
00:13:18,399 --> 00:13:20,959
okay so now i want to talk about

380
00:13:20,959 --> 00:13:22,000
sensitivity

381
00:13:22,000 --> 00:13:23,360
and how much one user can actually

382
00:13:23,360 --> 00:13:25,360
modify the results so

383
00:13:25,360 --> 00:13:26,800
for that let's think of a particular

384
00:13:26,800 --> 00:13:28,800
query such as what are the top 10

385
00:13:28,800 --> 00:13:31,120
countries that have a certain skill set

386
00:13:31,120 --> 00:13:32,800
and so here might be the

387
00:13:32,800 --> 00:13:34,800
result from pino this aggregate

388
00:13:34,800 --> 00:13:37,040
histogram

389
00:13:37,040 --> 00:13:40,720
that we can query for and we ask the

390
00:13:40,720 --> 00:13:42,560
question well what happens if we were to

391
00:13:42,560 --> 00:13:44,959
take one user out of the data set

392
00:13:44,959 --> 00:13:46,720
well one user can be in at most one

393
00:13:46,720 --> 00:13:48,880
country so taking that user out can

394
00:13:48,880 --> 00:13:49,839
modify

395
00:13:49,839 --> 00:13:53,040
uh exactly or at most one bin so exactly

396
00:13:53,040 --> 00:13:54,000
one of these counts

397
00:13:54,000 --> 00:13:57,519
uh can drop and so in order to give you

398
00:13:57,519 --> 00:14:00,880
uh privacy on this aggregate

399
00:14:00,880 --> 00:14:04,240
statistic we can just add we can utilize

400
00:14:04,240 --> 00:14:05,600
the laplace mechanism

401
00:14:05,600 --> 00:14:08,399
which just adds noise from a laplace

402
00:14:08,399 --> 00:14:09,279
distribution

403
00:14:09,279 --> 00:14:10,720
and this will ensure epsilon

404
00:14:10,720 --> 00:14:13,279
differential privacy

405
00:14:13,279 --> 00:14:14,959
and so this will be the result some sort

406
00:14:14,959 --> 00:14:17,040
of noisy version of the histogram and we

407
00:14:17,040 --> 00:14:18,560
can release like the top

408
00:14:18,560 --> 00:14:22,560
uh 10 uh noisy counts

409
00:14:22,880 --> 00:14:25,920
okay for a slightly different query same

410
00:14:25,920 --> 00:14:29,040
like another top top k query but

411
00:14:29,040 --> 00:14:32,000
uh um let's let's let's see how much one

412
00:14:32,000 --> 00:14:33,839
user can actually modify this one

413
00:14:33,839 --> 00:14:35,920
if we ask the top 10 skills in the bay

414
00:14:35,920 --> 00:14:37,360
area here my

415
00:14:37,360 --> 00:14:39,920
here's the uh histogram histogram that

416
00:14:39,920 --> 00:14:41,680
we receive

417
00:14:41,680 --> 00:14:43,199
now we run the same thought experiment

418
00:14:43,199 --> 00:14:45,120
what happens if we take one user out of

419
00:14:45,120 --> 00:14:45,839
the

420
00:14:45,839 --> 00:14:49,279
histogram what can change well one

421
00:14:49,279 --> 00:14:50,800
person can actually have lots of

422
00:14:50,800 --> 00:14:51,760
different skills

423
00:14:51,760 --> 00:14:54,320
so all all of these bins can actually

424
00:14:54,320 --> 00:14:54,959
drop

425
00:14:54,959 --> 00:14:57,360
because a user can have every possible

426
00:14:57,360 --> 00:14:58,880
skill out there

427
00:14:58,880 --> 00:15:01,600
so we consider this to be a lot higher

428
00:15:01,600 --> 00:15:02,959
sensitivity

429
00:15:02,959 --> 00:15:05,199
than the previous one because the user

430
00:15:05,199 --> 00:15:06,320
can

431
00:15:06,320 --> 00:15:09,360
can modify the result arbitrarily

432
00:15:09,360 --> 00:15:11,199
and for this we have there's another

433
00:15:11,199 --> 00:15:12,399
classical algorithm called the

434
00:15:12,399 --> 00:15:13,920
exponential mechanism

435
00:15:13,920 --> 00:15:16,959
uh from the cherian talwar that says

436
00:15:16,959 --> 00:15:20,079
that if you sample each element

437
00:15:20,079 --> 00:15:21,519
in a way that's proportional to its

438
00:15:21,519 --> 00:15:24,560
count then we can

439
00:15:24,560 --> 00:15:26,240
we can repeat this 10 times and give you

440
00:15:26,240 --> 00:15:29,920
the top uh 10 results

441
00:15:30,000 --> 00:15:32,480
okay and doing this will uh as long as

442
00:15:32,480 --> 00:15:34,480
you just release the elements in the top

443
00:15:34,480 --> 00:15:35,120
k

444
00:15:35,120 --> 00:15:37,440
not their counts this will ensure k

445
00:15:37,440 --> 00:15:39,199
epsilon differential privacy

446
00:15:39,199 --> 00:15:41,600
so the amount of privacy loss or the

447
00:15:41,600 --> 00:15:43,040
privacy loss parameter

448
00:15:43,040 --> 00:15:46,000
has grown because we've uh released uh

449
00:15:46,000 --> 00:15:46,720
exactly

450
00:15:46,720 --> 00:15:50,000
uh k elements in the top k to sort of

451
00:15:50,000 --> 00:15:51,839
summarizing we can

452
00:15:51,839 --> 00:15:54,560
identify what our known algorithms are

453
00:15:54,560 --> 00:15:54,880
for

454
00:15:54,880 --> 00:15:58,000
user level differential privacy to be

455
00:15:58,000 --> 00:15:59,600
either in the restricted sensitivity

456
00:15:59,600 --> 00:16:01,759
setting this is where we know in advance

457
00:16:01,759 --> 00:16:04,560
how many bins a user can actually modify

458
00:16:04,560 --> 00:16:06,320
which is parameterized by this capital

459
00:16:06,320 --> 00:16:09,440
delta for that we use laplace mechanism

460
00:16:09,440 --> 00:16:11,440
or we're in the unrestricted sensitivity

461
00:16:11,440 --> 00:16:13,279
setting where we we have no prior

462
00:16:13,279 --> 00:16:15,040
knowledge maybe a user can modify

463
00:16:15,040 --> 00:16:17,360
all the bends uh and so for that we have

464
00:16:17,360 --> 00:16:19,040
to use the exponential mechanism

465
00:16:19,040 --> 00:16:23,839
to give you some bounded privacy laws

466
00:16:24,639 --> 00:16:27,279
but what i want to point out with this

467
00:16:27,279 --> 00:16:28,720
particular

468
00:16:28,720 --> 00:16:30,959
these these classical algorithms is that

469
00:16:30,959 --> 00:16:32,639
these algorithms require knowing what

470
00:16:32,639 --> 00:16:34,959
the full data domain is in advance

471
00:16:34,959 --> 00:16:37,519
and so what i mean by that is that if i

472
00:16:37,519 --> 00:16:39,279
want to know where the top 10

473
00:16:39,279 --> 00:16:42,000
countries with a particular skill i

474
00:16:42,000 --> 00:16:43,519
would need to know what are all the

475
00:16:43,519 --> 00:16:44,240
possible

476
00:16:44,240 --> 00:16:46,399
countries that are that are out there

477
00:16:46,399 --> 00:16:48,000
even if a user wasn't even

478
00:16:48,000 --> 00:16:49,600
if there is nobody in a particular

479
00:16:49,600 --> 00:16:51,279
country because we have to run the

480
00:16:51,279 --> 00:16:53,040
thought experiment where one user can be

481
00:16:53,040 --> 00:16:54,880
added or removed from the data set

482
00:16:54,880 --> 00:16:56,959
and when you add a user they could add

483
00:16:56,959 --> 00:17:00,079
uh in some arbitrary country

484
00:17:00,079 --> 00:17:01,600
so for something like what are the top

485
00:17:01,600 --> 00:17:04,000
10 articles uh

486
00:17:04,000 --> 00:17:05,760
we would then need to know what are all

487
00:17:05,760 --> 00:17:07,520
the possible articles that are available

488
00:17:07,520 --> 00:17:08,720
for a user to click

489
00:17:08,720 --> 00:17:12,000
or or share or or comment on

490
00:17:12,000 --> 00:17:14,720
and this might just be way too large uh

491
00:17:14,720 --> 00:17:16,640
or maybe it's not even known in advance

492
00:17:16,640 --> 00:17:18,720
we want the data set itself to tell us

493
00:17:18,720 --> 00:17:20,640
what are the ones

494
00:17:20,640 --> 00:17:24,480
what are the articles that are

495
00:17:24,480 --> 00:17:26,720
most engaged uh and so for this

496
00:17:26,720 --> 00:17:27,679
discovery

497
00:17:27,679 --> 00:17:31,039
uh portion of of these algorithms we

498
00:17:31,039 --> 00:17:34,160
we need to do a bit more work and so i

499
00:17:34,160 --> 00:17:36,000
would argue that it's typically the case

500
00:17:36,000 --> 00:17:37,520
that the domain is

501
00:17:37,520 --> 00:17:41,280
either unknown or just very large

502
00:17:41,280 --> 00:17:44,080
and note that uh if if we just run these

503
00:17:44,080 --> 00:17:44,640
known

504
00:17:44,640 --> 00:17:46,880
known domain algorithms in this unknown

505
00:17:46,880 --> 00:17:48,160
domain setting

506
00:17:48,160 --> 00:17:50,000
the mere presence of an out of an

507
00:17:50,000 --> 00:17:52,000
element

508
00:17:52,000 --> 00:17:54,080
just tells us that one person had to

509
00:17:54,080 --> 00:17:55,360
have clicked or engaged with a

510
00:17:55,360 --> 00:17:56,960
particular article

511
00:17:56,960 --> 00:17:58,640
no matter how much noise we add to it

512
00:17:58,640 --> 00:18:01,200
the mere existence of it is problematic

513
00:18:01,200 --> 00:18:04,160
for privacy okay so the algorithms for

514
00:18:04,160 --> 00:18:05,840
user level privacy i'm going to

515
00:18:05,840 --> 00:18:06,960
differentiate

516
00:18:06,960 --> 00:18:08,799
uh distinguish now known domain

517
00:18:08,799 --> 00:18:10,880
algorithms these are the classical ones

518
00:18:10,880 --> 00:18:12,799
with the unknown domain setting and for

519
00:18:12,799 --> 00:18:14,480
these we developed

520
00:18:14,480 --> 00:18:17,520
new algorithms last year based on the

521
00:18:17,520 --> 00:18:19,280
laplace mechanism and based on the

522
00:18:19,280 --> 00:18:20,640
exponential mechanism

523
00:18:20,640 --> 00:18:23,200
but introducing a threshold only

524
00:18:23,200 --> 00:18:24,160
releasing

525
00:18:24,160 --> 00:18:26,080
elements that are above some threshold

526
00:18:26,080 --> 00:18:27,440
and these algorithms uh

527
00:18:27,440 --> 00:18:31,039
we wrote it in a uh in a paper that was

528
00:18:31,039 --> 00:18:33,200
accepted as a spotlight presentation

529
00:18:33,200 --> 00:18:36,400
at last year's europes okay so here's

530
00:18:36,400 --> 00:18:38,160
the overall privacy system

531
00:18:38,160 --> 00:18:40,400
we have uh data at one end marketing

532
00:18:40,400 --> 00:18:42,880
partner and our algorithms in between

533
00:18:42,880 --> 00:18:44,799
so the top k query is going to come in

534
00:18:44,799 --> 00:18:46,000
it's going to go through our dp

535
00:18:46,000 --> 00:18:47,760
algorithm which is going to translate it

536
00:18:47,760 --> 00:18:49,280
into something where it's going to go

537
00:18:49,280 --> 00:18:49,919
and fetch

538
00:18:49,919 --> 00:18:52,320
more than k results so such as like the

539
00:18:52,320 --> 00:18:54,000
top 2k

540
00:18:54,000 --> 00:18:56,240
get those results from pino and then add

541
00:18:56,240 --> 00:18:57,520
noise uh

542
00:18:57,520 --> 00:19:00,000
either from the laplace distribution or

543
00:19:00,000 --> 00:19:02,080
uh from a gumball distribution which is

544
00:19:02,080 --> 00:19:04,320
uh highly related to the exponential

545
00:19:04,320 --> 00:19:05,360
mechanism

546
00:19:05,360 --> 00:19:07,919
and we release the top uh k results from

547
00:19:07,919 --> 00:19:08,400
that

548
00:19:08,400 --> 00:19:09,520
and this will ensure differential

549
00:19:09,520 --> 00:19:11,440
privacy okay

550
00:19:11,440 --> 00:19:13,200
now what happens if a marketing partner

551
00:19:13,200 --> 00:19:14,880
just barrages this

552
00:19:14,880 --> 00:19:17,679
api with a lot of different queries we

553
00:19:17,679 --> 00:19:18,960
want to budget access

554
00:19:18,960 --> 00:19:22,240
to the marketing partner and so for that

555
00:19:22,240 --> 00:19:24,080
we also introduce a budget

556
00:19:24,080 --> 00:19:26,240
manager and for that you can think of it

557
00:19:26,240 --> 00:19:27,600
as like a traffic light

558
00:19:27,600 --> 00:19:29,280
so we have marketing partner that's

559
00:19:29,280 --> 00:19:31,679
interacting with this application

560
00:19:31,679 --> 00:19:34,160
and it goes the query goes to the dp

561
00:19:34,160 --> 00:19:35,840
algorithms which then goes to the

562
00:19:35,840 --> 00:19:37,679
privacy budget management

563
00:19:37,679 --> 00:19:40,080
service to say to ask hey can i run that

564
00:19:40,080 --> 00:19:40,799
query

565
00:19:40,799 --> 00:19:42,960
it says yes you can go ahead and then

566
00:19:42,960 --> 00:19:44,000
it's going to

567
00:19:44,000 --> 00:19:45,919
carry on with the translating the query

568
00:19:45,919 --> 00:19:48,400
and adding noise in a particular way

569
00:19:48,400 --> 00:19:50,559
maybe you've interacted too many times

570
00:19:50,559 --> 00:19:52,400
with it so after a while we get a red

571
00:19:52,400 --> 00:19:53,679
light that says like hey

572
00:19:53,679 --> 00:19:56,240
you've uh exhausted your budget so then

573
00:19:56,240 --> 00:19:57,919
it tries to ask that query it asks the

574
00:19:57,919 --> 00:19:59,360
privacy budget manager can this

575
00:19:59,360 --> 00:20:00,880
marketing partner continue

576
00:20:00,880 --> 00:20:02,559
it says no they've exhausted their

577
00:20:02,559 --> 00:20:04,240
budget and that that would be the end of

578
00:20:04,240 --> 00:20:05,120
the session

579
00:20:05,120 --> 00:20:08,960
until a new session is restarted

580
00:20:08,960 --> 00:20:11,039
yeah and so for the way that we compute

581
00:20:11,039 --> 00:20:12,559
the overall privacy

582
00:20:12,559 --> 00:20:15,440
uh budget uh we we develop state of the

583
00:20:15,440 --> 00:20:16,960
art

584
00:20:16,960 --> 00:20:20,240
privacy loss computation bounds and

585
00:20:20,240 --> 00:20:22,799
which are go together with the

586
00:20:22,799 --> 00:20:24,000
algorithms that we

587
00:20:24,000 --> 00:20:26,880
uh created for this problem to give you

588
00:20:26,880 --> 00:20:28,799
sort of the best possible bounds

589
00:20:28,799 --> 00:20:31,919
uh that improve over uh existing bounds

590
00:20:31,919 --> 00:20:35,120
on the overall privacy loss while also

591
00:20:35,120 --> 00:20:36,799
giving you algorithms that actually make

592
00:20:36,799 --> 00:20:40,000
this feasible

593
00:20:40,000 --> 00:20:49,840
that's it so thank you very much

594
00:20:51,600 --> 00:20:53,678
you

