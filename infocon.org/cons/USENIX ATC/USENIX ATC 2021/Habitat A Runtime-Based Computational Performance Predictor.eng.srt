1
00:00:08,559 --> 00:00:10,480
hello my name is jeffrey and i'm really

2
00:00:10,480 --> 00:00:12,799
excited to be here to present habitat a

3
00:00:12,799 --> 00:00:14,880
runtime-based computational performance

4
00:00:14,880 --> 00:00:16,400
predictor for deep neural network

5
00:00:16,400 --> 00:00:17,359
training

6
00:00:17,359 --> 00:00:19,920
this is joint work with yubo pavel and

7
00:00:19,920 --> 00:00:21,439
gennady who is my advisor at the

8
00:00:21,439 --> 00:00:23,279
university of toronto

9
00:00:23,279 --> 00:00:25,680
okay let's get started

10
00:00:25,680 --> 00:00:27,680
today deep learning users have a vast

11
00:00:27,680 --> 00:00:29,840
array of gpu options available to them

12
00:00:29,840 --> 00:00:32,238
for training each with a different cost

13
00:00:32,238 --> 00:00:33,760
and performance

14
00:00:33,760 --> 00:00:35,840
and this presents a new problem which

15
00:00:35,840 --> 00:00:37,680
gpu should they use for training their

16
00:00:37,680 --> 00:00:38,960
models

17
00:00:38,960 --> 00:00:40,879
well one way to make an informed choice

18
00:00:40,879 --> 00:00:42,800
is to predict the performance you'll get

19
00:00:42,800 --> 00:00:45,280
when training your model on a given gpu

20
00:00:45,280 --> 00:00:46,960
and we make the observation that the

21
00:00:46,960 --> 00:00:48,719
computation during deep neural network

22
00:00:48,719 --> 00:00:51,199
training is highly repetitive and so

23
00:00:51,199 --> 00:00:52,800
this means that it's actually feasible

24
00:00:52,800 --> 00:00:54,640
to predict the performance of a training

25
00:00:54,640 --> 00:00:56,640
process we just need to predict the

26
00:00:56,640 --> 00:00:58,480
execution time of a single training

27
00:00:58,480 --> 00:01:00,480
iteration

28
00:01:00,480 --> 00:01:02,320
and so the key idea in our work is to

29
00:01:02,320 --> 00:01:04,400
use an existing gpu to help make these

30
00:01:04,400 --> 00:01:05,680
predictions

31
00:01:05,680 --> 00:01:07,600
we use a combination of wave scaling

32
00:01:07,600 --> 00:01:09,040
which is a new technique based on

33
00:01:09,040 --> 00:01:10,960
hardware ratios and pre-trained

34
00:01:10,960 --> 00:01:13,040
multi-layer perceptrons

35
00:01:13,040 --> 00:01:14,640
we implement these techniques into a

36
00:01:14,640 --> 00:01:16,799
tool called habitat which is open source

37
00:01:16,799 --> 00:01:19,200
and supports pi torch and we show two

38
00:01:19,200 --> 00:01:21,360
case studies where habitat's predictions

39
00:01:21,360 --> 00:01:23,360
help users choose the right gpu for

40
00:01:23,360 --> 00:01:25,200
their use case

41
00:01:25,200 --> 00:01:26,479
deep neural networks have seen

42
00:01:26,479 --> 00:01:28,400
incredible success across many machine

43
00:01:28,400 --> 00:01:30,159
learning tasks which has led them to

44
00:01:30,159 --> 00:01:31,759
become widely used throughout both

45
00:01:31,759 --> 00:01:34,079
academia and industry but the problem is

46
00:01:34,079 --> 00:01:35,680
that they're often computationally

47
00:01:35,680 --> 00:01:37,759
expensive to train

48
00:01:37,759 --> 00:01:39,439
and this has led to a significant

49
00:01:39,439 --> 00:01:40,960
investment in bringing hardware

50
00:01:40,960 --> 00:01:42,799
acceleration to deep neural network

51
00:01:42,799 --> 00:01:43,759
training

52
00:01:43,759 --> 00:01:46,079
and today there's a vast array of gpu

53
00:01:46,079 --> 00:01:48,399
options available both for workstations

54
00:01:48,399 --> 00:01:50,240
and servers in the cloud

55
00:01:50,240 --> 00:01:52,880
but even beyond gpus there are also tpus

56
00:01:52,880 --> 00:01:54,960
that span several versions as well as

57
00:01:54,960 --> 00:01:57,040
other emerging accelerators such as the

58
00:01:57,040 --> 00:01:59,280
cerebra's wafer scale engine the habana

59
00:01:59,280 --> 00:02:01,759
gaudi and the recently announced aws

60
00:02:01,759 --> 00:02:04,159
trainium chip having all these options

61
00:02:04,159 --> 00:02:06,399
empowers users but also leads to the

62
00:02:06,399 --> 00:02:08,318
question of which accelerator should a

63
00:02:08,318 --> 00:02:11,440
user use when training their models

64
00:02:11,440 --> 00:02:13,440
in this work we start by focusing on

65
00:02:13,440 --> 00:02:15,920
answering this question for gpus

66
00:02:15,920 --> 00:02:17,280
and that's because they are commonly

67
00:02:17,280 --> 00:02:19,040
used in deep learning and are readily

68
00:02:19,040 --> 00:02:22,319
available for both purchase and rent

69
00:02:22,319 --> 00:02:24,879
but even when just considering gpus

70
00:02:24,879 --> 00:02:26,959
users are often faced with a paradox of

71
00:02:26,959 --> 00:02:27,840
choice

72
00:02:27,840 --> 00:02:30,160
for example a common scenario is having

73
00:02:30,160 --> 00:02:31,920
to choose between gpus available in

74
00:02:31,920 --> 00:02:33,200
different tiers

75
00:02:33,200 --> 00:02:35,280
a user could have a gpu available in

76
00:02:35,280 --> 00:02:36,959
their personal workstation such as the

77
00:02:36,959 --> 00:02:39,760
3090 another gpu available to them in

78
00:02:39,760 --> 00:02:41,760
their institution's private cluster like

79
00:02:41,760 --> 00:02:43,200
the p100

80
00:02:43,200 --> 00:02:44,959
and also the option to rent gpus in the

81
00:02:44,959 --> 00:02:47,599
cloud like the v100 and t4

82
00:02:47,599 --> 00:02:49,440
but when choosing users might want to

83
00:02:49,440 --> 00:02:51,599
optimize for different factors such as

84
00:02:51,599 --> 00:02:54,879
performance or cost or both

85
00:02:54,879 --> 00:02:56,720
and the problem is that there is no one

86
00:02:56,720 --> 00:02:59,440
size fits all choice the correct choice

87
00:02:59,440 --> 00:03:01,599
depends on what the user is optimizing

88
00:03:01,599 --> 00:03:03,519
for

89
00:03:03,519 --> 00:03:04,879
okay so at this point you might be

90
00:03:04,879 --> 00:03:05,840
wondering

91
00:03:05,840 --> 00:03:07,760
okay well why not just measure

92
00:03:07,760 --> 00:03:08,959
performance directly instead of

93
00:03:08,959 --> 00:03:10,239
predicting

94
00:03:10,239 --> 00:03:11,840
well to do that you actually need to get

95
00:03:11,840 --> 00:03:13,760
access to the gpus you're considering

96
00:03:13,760 --> 00:03:15,599
which often costs money

97
00:03:15,599 --> 00:03:16,879
ideally you would want to make this

98
00:03:16,879 --> 00:03:19,360
decision before spending that money

99
00:03:19,360 --> 00:03:21,280
and on top of that making measurements

100
00:03:21,280 --> 00:03:23,200
gets very tedious when you need to

101
00:03:23,200 --> 00:03:24,560
repeat the measurements for many

102
00:03:24,560 --> 00:03:27,280
different models and gpus

103
00:03:27,280 --> 00:03:28,799
okay then why not use existing

104
00:03:28,799 --> 00:03:30,720
benchmarking results

105
00:03:30,720 --> 00:03:32,560
well they are typically only available

106
00:03:32,560 --> 00:03:35,040
for a few popular models such as resnet

107
00:03:35,040 --> 00:03:37,040
and bert and for cloud gpus like the

108
00:03:37,040 --> 00:03:39,519
v100 800 and t4

109
00:03:39,519 --> 00:03:41,680
if you're developing a new custom model

110
00:03:41,680 --> 00:03:42,959
which is often the case when you're

111
00:03:42,959 --> 00:03:44,879
doing deep learning research you will

112
00:03:44,879 --> 00:03:47,760
not have any results to rely on

113
00:03:47,760 --> 00:03:49,440
okay then what about using heuristics

114
00:03:49,440 --> 00:03:50,879
like making a back of the envelope

115
00:03:50,879 --> 00:03:53,599
calculation to estimate performance

116
00:03:53,599 --> 00:03:55,439
well they don't always work

117
00:03:55,439 --> 00:03:57,519
for example we measure dc gantt's

118
00:03:57,519 --> 00:03:59,439
training iteration execution time on the

119
00:03:59,439 --> 00:04:00,480
t4

120
00:04:00,480 --> 00:04:02,319
and then we multiplied that measurement

121
00:04:02,319 --> 00:04:05,120
by the ratio between the t4's peak flops

122
00:04:05,120 --> 00:04:07,360
and that of five other gpus just to

123
00:04:07,360 --> 00:04:09,360
estimate their performance

124
00:04:09,360 --> 00:04:10,799
but it turns out that this simple

125
00:04:10,799 --> 00:04:13,360
heuristic works poorly at leads to

126
00:04:13,360 --> 00:04:16,399
prediction errors of at least 43 percent

127
00:04:16,399 --> 00:04:18,000
and so really to get accurate

128
00:04:18,000 --> 00:04:20,000
predictions we need to take a different

129
00:04:20,000 --> 00:04:21,440
approach

130
00:04:21,440 --> 00:04:23,199
so in our work we make three key

131
00:04:23,199 --> 00:04:25,040
observations that we leverage to make

132
00:04:25,040 --> 00:04:27,360
these accurate predictions

133
00:04:27,360 --> 00:04:29,360
first deep learning users often already

134
00:04:29,360 --> 00:04:31,919
have an existing gpu available to them

135
00:04:31,919 --> 00:04:33,440
for example they might already have a

136
00:04:33,440 --> 00:04:35,280
gpu in their workstation that they use

137
00:04:35,280 --> 00:04:36,960
for development

138
00:04:36,960 --> 00:04:38,720
and second deep network training

139
00:04:38,720 --> 00:04:40,160
computation really is a highly

140
00:04:40,160 --> 00:04:42,160
repetitive process with short training

141
00:04:42,160 --> 00:04:44,479
iterations so this means that predicting

142
00:04:44,479 --> 00:04:46,400
the performance of a training process

143
00:04:46,400 --> 00:04:48,240
just reduces down to predicting the

144
00:04:48,240 --> 00:04:51,759
execution time of one training iteration

145
00:04:51,759 --> 00:04:54,240
so we can leverage a user's existing gpu

146
00:04:54,240 --> 00:04:55,840
to predict the deep neural network's

147
00:04:55,840 --> 00:04:58,240
training performance on other gpus

148
00:04:58,240 --> 00:04:59,840
and this is the central technique that

149
00:04:59,840 --> 00:05:02,639
we develop in this work

150
00:05:02,639 --> 00:05:04,720
and so we built habitat a computational

151
00:05:04,720 --> 00:05:06,320
performance predictor for deep neural

152
00:05:06,320 --> 00:05:07,600
network training

153
00:05:07,600 --> 00:05:09,840
at high level habitats workflow has

154
00:05:09,840 --> 00:05:11,360
three steps

155
00:05:11,360 --> 00:05:13,520
first it measures the execution time of

156
00:05:13,520 --> 00:05:15,360
each operation that runs in a training

157
00:05:15,360 --> 00:05:17,280
iteration along with some other runtime

158
00:05:17,280 --> 00:05:18,800
metadata

159
00:05:18,800 --> 00:05:20,960
then using those measurements it applies

160
00:05:20,960 --> 00:05:22,639
either wave scaling or pre-trained

161
00:05:22,639 --> 00:05:24,320
multi-layer perceptrons to each of the

162
00:05:24,320 --> 00:05:26,560
operations to predict the execution time

163
00:05:26,560 --> 00:05:29,440
of the operation on a different gpu

164
00:05:29,440 --> 00:05:31,039
finally it combines these measurements

165
00:05:31,039 --> 00:05:32,720
together to arrive at a single

166
00:05:32,720 --> 00:05:34,479
end-to-end iteration execution time

167
00:05:34,479 --> 00:05:36,720
prediction for the other gpu this can

168
00:05:36,720 --> 00:05:38,240
then be used to calculate throughput and

169
00:05:38,240 --> 00:05:40,400
cost normalized throughput

170
00:05:40,400 --> 00:05:42,320
habitat itself is implemented as a

171
00:05:42,320 --> 00:05:44,000
python library that currently supports

172
00:05:44,000 --> 00:05:46,880
pi torch and it is open source

173
00:05:46,880 --> 00:05:48,880
so now let's dive into how habitat makes

174
00:05:48,880 --> 00:05:50,400
its predictions

175
00:05:50,400 --> 00:05:51,680
first i'm going to present some

176
00:05:51,680 --> 00:05:54,160
background about a gpu's execution model

177
00:05:54,160 --> 00:05:55,440
which is important to be able to

178
00:05:55,440 --> 00:05:58,240
understand how habitat works

179
00:05:58,240 --> 00:05:59,919
deep neural network operations when

180
00:05:59,919 --> 00:06:02,479
executed on a gpu are composed of one or

181
00:06:02,479 --> 00:06:04,240
more gpu kernels

182
00:06:04,240 --> 00:06:05,600
you can think of a kernel like a

183
00:06:05,600 --> 00:06:07,440
procedure or function that runs on the

184
00:06:07,440 --> 00:06:09,199
gpu

185
00:06:09,199 --> 00:06:11,120
kernels themselves are divided into

186
00:06:11,120 --> 00:06:13,280
thread blocks which are units of work

187
00:06:13,280 --> 00:06:15,360
that run the same code but operate on

188
00:06:15,360 --> 00:06:17,840
different data gpus consist of a number

189
00:06:17,840 --> 00:06:20,639
of streaming multiprocessors or sms

190
00:06:20,639 --> 00:06:22,080
which run a finite number of these

191
00:06:22,080 --> 00:06:24,240
thread blocks concurrently this is how

192
00:06:24,240 --> 00:06:26,160
the gpu achieves parallelism when

193
00:06:26,160 --> 00:06:27,680
executing a kernel

194
00:06:27,680 --> 00:06:29,680
so the thread blocks are round robin

195
00:06:29,680 --> 00:06:31,919
scheduled onto the sms

196
00:06:31,919 --> 00:06:33,759
because the sms only run a finite number

197
00:06:33,759 --> 00:06:35,440
of these thread blocks concurrently the

198
00:06:35,440 --> 00:06:37,360
scheduling model results in waves of

199
00:06:37,360 --> 00:06:40,080
thread blocks that execute on the gpu

200
00:06:40,080 --> 00:06:42,160
in this example running that kernel

201
00:06:42,160 --> 00:06:44,800
leads to two thread block waves

202
00:06:44,800 --> 00:06:46,560
and this leads us to habitat's key

203
00:06:46,560 --> 00:06:48,639
prediction technique which we call wave

204
00:06:48,639 --> 00:06:49,759
scaling

205
00:06:49,759 --> 00:06:51,919
wave scaling works on individual gpu

206
00:06:51,919 --> 00:06:53,039
kernels

207
00:06:53,039 --> 00:06:55,039
given a kernel's execution time on one

208
00:06:55,039 --> 00:06:58,160
gpu for example 200 microseconds here

209
00:06:58,160 --> 00:06:59,680
wave scaling predicts the kernel's

210
00:06:59,680 --> 00:07:01,840
execution time on a different gpu

211
00:07:01,840 --> 00:07:03,440
without actually running it on that

212
00:07:03,440 --> 00:07:04,800
other gpu

213
00:07:04,800 --> 00:07:06,639
and it does this using the two gpus

214
00:07:06,639 --> 00:07:08,639
hardware specifications

215
00:07:08,639 --> 00:07:11,520
let's walk through an example

216
00:07:11,520 --> 00:07:13,280
in line with this name wave scaling

217
00:07:13,280 --> 00:07:15,360
works with thread block waves

218
00:07:15,360 --> 00:07:17,280
so first it computes how long a wave

219
00:07:17,280 --> 00:07:19,360
takes by dividing the kernel's execution

220
00:07:19,360 --> 00:07:21,440
time by the number of waves

221
00:07:21,440 --> 00:07:23,280
computing number waves for a gpu is

222
00:07:23,280 --> 00:07:24,880
straightforward and habitat does this

223
00:07:24,880 --> 00:07:26,880
using the cuda toolkit

224
00:07:26,880 --> 00:07:28,880
so in our example the kernel takes two

225
00:07:28,880 --> 00:07:31,440
waves to complete on gpu a and this is

226
00:07:31,440 --> 00:07:33,360
because only four of its eight thread

227
00:07:33,360 --> 00:07:36,160
blocks run concurrently on the gpu

228
00:07:36,160 --> 00:07:37,759
so a wave would take a hundred

229
00:07:37,759 --> 00:07:39,599
microseconds

230
00:07:39,599 --> 00:07:41,440
and now to predict the waves execution

231
00:07:41,440 --> 00:07:43,039
time on gpub

232
00:07:43,039 --> 00:07:44,639
wave scaling scales this 100

233
00:07:44,639 --> 00:07:47,520
microseconds using three factors

234
00:07:47,520 --> 00:07:49,520
first the memory bandwidth ratio between

235
00:07:49,520 --> 00:07:51,039
the two gpus

236
00:07:51,039 --> 00:07:53,039
second the number of thread blocks in

237
00:07:53,039 --> 00:07:55,199
the waves in other words the size of the

238
00:07:55,199 --> 00:07:57,039
wave on each gpu

239
00:07:57,039 --> 00:07:59,039
and third the clock frequency ratio

240
00:07:59,039 --> 00:08:01,120
between the two gpus

241
00:08:01,120 --> 00:08:03,680
so here's the intuition

242
00:08:03,680 --> 00:08:05,440
suppose the kernel's memory bandwidth

243
00:08:05,440 --> 00:08:06,319
bound

244
00:08:06,319 --> 00:08:08,720
if the memory bandwidth on gpu b is say

245
00:08:08,720 --> 00:08:11,120
two times higher than gpua then the

246
00:08:11,120 --> 00:08:13,360
waves execution time should decrease by

247
00:08:13,360 --> 00:08:14,960
up to two times

248
00:08:14,960 --> 00:08:16,720
and this is because if there's twice as

249
00:08:16,720 --> 00:08:18,639
much bandwidth available to serve the

250
00:08:18,639 --> 00:08:20,319
memory requests then you'll be able to

251
00:08:20,319 --> 00:08:23,039
complete them up to two times faster

252
00:08:23,039 --> 00:08:24,639
similarly if the memory bandwidth on

253
00:08:24,639 --> 00:08:27,039
gpub was lower than gpua then the

254
00:08:27,039 --> 00:08:28,879
opposite would happen

255
00:08:28,879 --> 00:08:30,879
okay next wave scaling takes into

256
00:08:30,879 --> 00:08:32,880
account the size of the wave again

257
00:08:32,880 --> 00:08:34,559
suppose the kernel is memory bandwidth

258
00:08:34,559 --> 00:08:35,679
bound

259
00:08:35,679 --> 00:08:38,080
if a single wave on gpu b is twice as

260
00:08:38,080 --> 00:08:40,799
large as a wave on gpu a and the gpu's

261
00:08:40,799 --> 00:08:42,799
memory bandwidths are the same then the

262
00:08:42,799 --> 00:08:45,040
wave should take two times longer on gpu

263
00:08:45,040 --> 00:08:46,000
b

264
00:08:46,000 --> 00:08:47,760
and this is because having more thread

265
00:08:47,760 --> 00:08:49,839
blocks means more memory requests are

266
00:08:49,839 --> 00:08:51,920
going to be made in the same wave

267
00:08:51,920 --> 00:08:53,440
and if the memory bandwidth is the same

268
00:08:53,440 --> 00:08:55,440
between the gpus then it'll take up to

269
00:08:55,440 --> 00:08:57,120
two times longer to serve all those

270
00:08:57,120 --> 00:08:58,480
requests

271
00:08:58,480 --> 00:09:00,240
and just like before this effect is

272
00:09:00,240 --> 00:09:02,399
analogous if the wave was smaller on gpu

273
00:09:02,399 --> 00:09:04,560
b instead in this example i'm showing

274
00:09:04,560 --> 00:09:08,160
you a wave of the same size

275
00:09:08,160 --> 00:09:09,920
finally what if the kernel is not memory

276
00:09:09,920 --> 00:09:11,600
bandwidth bound but compute bound

277
00:09:11,600 --> 00:09:12,959
instead

278
00:09:12,959 --> 00:09:14,720
then the clock frequency plays a larger

279
00:09:14,720 --> 00:09:16,320
role

280
00:09:16,320 --> 00:09:18,480
if the clock frequency on gpu b is two

281
00:09:18,480 --> 00:09:20,959
times higher than on gpu a then we would

282
00:09:20,959 --> 00:09:22,880
expect the wave to run twice as fast

283
00:09:22,880 --> 00:09:24,880
because it is compute bound

284
00:09:24,880 --> 00:09:27,279
okay now let's complete our example

285
00:09:27,279 --> 00:09:28,959
so suppose the kernel is memory

286
00:09:28,959 --> 00:09:31,519
bandwidth bound and that gpu b's memory

287
00:09:31,519 --> 00:09:34,640
bandwidth is two times higher than gpua

288
00:09:34,640 --> 00:09:36,480
so using the bandwidth alone we would

289
00:09:36,480 --> 00:09:38,800
expect the wave on gpu b to run twice as

290
00:09:38,800 --> 00:09:39,760
fast

291
00:09:39,760 --> 00:09:41,200
but we also need to keep in mind that

292
00:09:41,200 --> 00:09:43,680
gpu b's wave will be twice as large

293
00:09:43,680 --> 00:09:45,680
because it has more sms

294
00:09:45,680 --> 00:09:47,120
notice here that there are eight thread

295
00:09:47,120 --> 00:09:49,279
blocks in a wave on gpu b

296
00:09:49,279 --> 00:09:51,519
so taking into account the bandwidth and

297
00:09:51,519 --> 00:09:53,680
wave size wave scaling would predict

298
00:09:53,680 --> 00:09:55,920
that a wave on gpu b takes the same

299
00:09:55,920 --> 00:09:58,800
amount of time as on gpu a

300
00:09:58,800 --> 00:10:00,640
finally we need to go back to the kernel

301
00:10:00,640 --> 00:10:02,079
execution time

302
00:10:02,079 --> 00:10:04,320
so notice that gpu gpub can process all

303
00:10:04,320 --> 00:10:05,760
of the kernel's thread blocks in a

304
00:10:05,760 --> 00:10:07,519
single wave and that's because the wave

305
00:10:07,519 --> 00:10:08,720
is larger

306
00:10:08,720 --> 00:10:10,640
therefore overall wave scaling would

307
00:10:10,640 --> 00:10:12,480
also predict that the kernel itself will

308
00:10:12,480 --> 00:10:15,279
run in 100 microseconds on gpub which is

309
00:10:15,279 --> 00:10:18,160
two times faster compared to gpua we

310
00:10:18,160 --> 00:10:19,839
described wave scaling more formally in

311
00:10:19,839 --> 00:10:21,519
our paper and i encourage you to check

312
00:10:21,519 --> 00:10:23,600
it out for more details now there's one

313
00:10:23,600 --> 00:10:26,000
last wrinkle wave scaling works when the

314
00:10:26,000 --> 00:10:28,880
same kernel is used on both gpus

315
00:10:28,880 --> 00:10:30,399
but there are a few deep neural network

316
00:10:30,399 --> 00:10:32,320
operations that use different kernels on

317
00:10:32,320 --> 00:10:34,320
different gpus for performance reasons

318
00:10:34,320 --> 00:10:35,920
and we call these operations kernel

319
00:10:35,920 --> 00:10:37,920
varying in the paper

320
00:10:37,920 --> 00:10:39,440
habitat makes predictions for these

321
00:10:39,440 --> 00:10:41,279
operations using pre-trained multi-layer

322
00:10:41,279 --> 00:10:42,880
perceptrons which you can read more

323
00:10:42,880 --> 00:10:45,279
about in our paper

324
00:10:45,279 --> 00:10:47,360
okay now for our evaluation

325
00:10:47,360 --> 00:10:49,200
we had two questions we wanted to answer

326
00:10:49,200 --> 00:10:51,839
when we set out to do our evaluation

327
00:10:51,839 --> 00:10:53,519
first of course is how accurate are

328
00:10:53,519 --> 00:10:55,440
habitats predictions

329
00:10:55,440 --> 00:10:57,279
second we wanted to see if habitat's

330
00:10:57,279 --> 00:10:59,200
predictions would lead users to make the

331
00:10:59,200 --> 00:11:02,000
correct decisions based on their needs

332
00:11:02,000 --> 00:11:04,480
so we used six gpus in our evaluation

333
00:11:04,480 --> 00:11:06,800
and they spent three generations pascal

334
00:11:06,800 --> 00:11:08,720
volta and turing and they consist of

335
00:11:08,720 --> 00:11:11,839
both workstation gpus and server gpus

336
00:11:11,839 --> 00:11:14,399
and second we evaluated on five models

337
00:11:14,399 --> 00:11:16,240
this includes resonance 50 and inception

338
00:11:16,240 --> 00:11:17,920
v3 which are convolutional neural

339
00:11:17,920 --> 00:11:20,640
networks for image classification gnmt

340
00:11:20,640 --> 00:11:22,240
which is a recurrent neural network for

341
00:11:22,240 --> 00:11:24,079
machine translation the transformer

342
00:11:24,079 --> 00:11:25,839
which is an attention-based model but

343
00:11:25,839 --> 00:11:28,640
also for machine translation and dc gan

344
00:11:28,640 --> 00:11:30,320
which is a generative adversarial model

345
00:11:30,320 --> 00:11:32,320
for image generation we ran all our

346
00:11:32,320 --> 00:11:34,399
experiments on pytorch 1.4 and if you

347
00:11:34,399 --> 00:11:35,920
want to learn more about our methodology

348
00:11:35,920 --> 00:11:38,560
please see our paper

349
00:11:38,560 --> 00:11:40,720
so to evaluate habitat's accuracy we

350
00:11:40,720 --> 00:11:42,160
used it to predict the iteration

351
00:11:42,160 --> 00:11:44,480
execution time of each of the gpus and

352
00:11:44,480 --> 00:11:46,240
models that i mentioned on the previous

353
00:11:46,240 --> 00:11:47,279
slide

354
00:11:47,279 --> 00:11:49,040
so here's a concrete example of the

355
00:11:49,040 --> 00:11:50,639
experiments we ran

356
00:11:50,639 --> 00:11:52,480
recall that habitat needs an existing

357
00:11:52,480 --> 00:11:55,200
gpu for its profiling pass so here we

358
00:11:55,200 --> 00:11:58,160
took resin at 50 with a batch size of 32

359
00:11:58,160 --> 00:12:00,480
and ran it on the 2070 which here we're

360
00:12:00,480 --> 00:12:02,800
calling the origin gpu

361
00:12:02,800 --> 00:12:04,560
then we used habitat to predict the

362
00:12:04,560 --> 00:12:07,279
iteration execution time on the v100 and

363
00:12:07,279 --> 00:12:10,079
here that's the destination gpu so in

364
00:12:10,079 --> 00:12:12,000
this setup habitat's prediction error

365
00:12:12,000 --> 00:12:14,000
was 1.9

366
00:12:14,000 --> 00:12:16,000
we repeated this experiment for all five

367
00:12:16,000 --> 00:12:18,320
models across three batch sizes and for

368
00:12:18,320 --> 00:12:20,399
all 30 possible pairs of the origin and

369
00:12:20,399 --> 00:12:22,240
destination gpus

370
00:12:22,240 --> 00:12:23,839
and among all these configurations

371
00:12:23,839 --> 00:12:27,120
habitat's average error was 11.8 percent

372
00:12:27,120 --> 00:12:28,639
so we conclude that habitat makes

373
00:12:28,639 --> 00:12:30,240
accurate iteration execution time

374
00:12:30,240 --> 00:12:32,480
predictions

375
00:12:32,480 --> 00:12:34,240
but next we wanted to see if habitat's

376
00:12:34,240 --> 00:12:36,320
predictions can actually help a user

377
00:12:36,320 --> 00:12:37,920
make the correct decision for their

378
00:12:37,920 --> 00:12:39,760
specific situation and for this we

379
00:12:39,760 --> 00:12:42,079
examine two case studies so suppose a

380
00:12:42,079 --> 00:12:44,240
user wants to train gnmt and they

381
00:12:44,240 --> 00:12:46,720
already have a p4000 they want to figure

382
00:12:46,720 --> 00:12:48,399
out which cloud gpu they should use if

383
00:12:48,399 --> 00:12:49,360
any

384
00:12:49,360 --> 00:12:50,959
well they can use habitat with their

385
00:12:50,959 --> 00:12:53,279
p4000 to predict the training throughput

386
00:12:53,279 --> 00:12:56,079
on the v100 t4 mp100

387
00:12:56,079 --> 00:12:57,440
so on this graph i'm showing the

388
00:12:57,440 --> 00:12:59,120
training throughput of each of these

389
00:12:59,120 --> 00:13:02,160
gpus normalized to the p4000 for three

390
00:13:02,160 --> 00:13:03,839
different batch sizes

391
00:13:03,839 --> 00:13:05,680
above each bar the percentage is

392
00:13:05,680 --> 00:13:07,440
habitat's prediction error so a

393
00:13:07,440 --> 00:13:09,200
normalized throughput above one means

394
00:13:09,200 --> 00:13:10,959
the gpu will have a better training

395
00:13:10,959 --> 00:13:13,680
throughput compared to the p4000

396
00:13:13,680 --> 00:13:15,360
notice that despite any prediction

397
00:13:15,360 --> 00:13:17,760
errors habitat correctly predicts that

398
00:13:17,760 --> 00:13:19,440
the v100 is the best choice for

399
00:13:19,440 --> 00:13:20,480
performance

400
00:13:20,480 --> 00:13:22,240
followed by the p100

401
00:13:22,240 --> 00:13:24,240
the t4 on the other hand only offers

402
00:13:24,240 --> 00:13:26,560
marginal improvements over the p4000 but

403
00:13:26,560 --> 00:13:28,639
what if we take cost into account on

404
00:13:28,639 --> 00:13:30,560
this next graph the y-axis is now the

405
00:13:30,560 --> 00:13:32,560
cost normalized training throughput and

406
00:13:32,560 --> 00:13:35,120
we normalize by each gpu's hourly rental

407
00:13:35,120 --> 00:13:37,200
rate on google cloud

408
00:13:37,200 --> 00:13:39,360
here notice that the t4 is now the best

409
00:13:39,360 --> 00:13:41,519
option and again despite any prediction

410
00:13:41,519 --> 00:13:43,519
errors habitat correctly predicts the

411
00:13:43,519 --> 00:13:46,480
relative ordering among these gpus

412
00:13:46,480 --> 00:13:48,480
so ultimately the user's best choice

413
00:13:48,480 --> 00:13:50,160
depends on their needs

414
00:13:50,160 --> 00:13:51,680
if they want to train their model as

415
00:13:51,680 --> 00:13:54,000
fast as possible the v100 is their best

416
00:13:54,000 --> 00:13:54,959
choice

417
00:13:54,959 --> 00:13:56,079
but if they're not critically

418
00:13:56,079 --> 00:13:58,000
constrained by time and want to save

419
00:13:58,000 --> 00:14:00,399
money renting the t4 or just sticking

420
00:14:00,399 --> 00:14:02,399
with the p4000 as the best choice and

421
00:14:02,399 --> 00:14:04,560
this is because the t4 only offers

422
00:14:04,560 --> 00:14:06,880
marginal improvements over the p4000

423
00:14:06,880 --> 00:14:09,199
which the user already has

424
00:14:09,199 --> 00:14:11,199
okay so from that case study a natural

425
00:14:11,199 --> 00:14:13,279
thing to wonder is should we just always

426
00:14:13,279 --> 00:14:15,519
select the best gpu available if we want

427
00:14:15,519 --> 00:14:16,720
performance

428
00:14:16,720 --> 00:14:18,480
for example suppose a user wants to

429
00:14:18,480 --> 00:14:21,440
train dc gan and they have a 2080 ti and

430
00:14:21,440 --> 00:14:23,040
they want to know if it's worth using

431
00:14:23,040 --> 00:14:24,480
the v100 because they think it's the

432
00:14:24,480 --> 00:14:26,639
best gpu available for them

433
00:14:26,639 --> 00:14:28,839
well they can use habitat to make a

434
00:14:28,839 --> 00:14:30,959
prediction on this graph i'm showing

435
00:14:30,959 --> 00:14:33,279
each of the five other gpu's dc gan

436
00:14:33,279 --> 00:14:35,040
training throughput normalized to the

437
00:14:35,040 --> 00:14:36,720
2080 ti

438
00:14:36,720 --> 00:14:38,320
again the percentages are habitat's

439
00:14:38,320 --> 00:14:40,000
prediction errors and a normalized

440
00:14:40,000 --> 00:14:42,160
throughput above one means the gpu does

441
00:14:42,160 --> 00:14:44,560
better than the 2080 ti

442
00:14:44,560 --> 00:14:46,560
these results show that the v100 only

443
00:14:46,560 --> 00:14:48,480
offers a marginal improvement over the

444
00:14:48,480 --> 00:14:51,920
2080 ti which habitat correctly predicts

445
00:14:51,920 --> 00:14:53,199
so it would not be worth it for this

446
00:14:53,199 --> 00:14:56,639
user to rent the v100 in this case

447
00:14:56,639 --> 00:14:58,160
we have additional details in our paper

448
00:14:58,160 --> 00:14:59,600
that i didn't get a chance to cover in

449
00:14:59,600 --> 00:15:01,440
this talk and i'd encourage you to check

450
00:15:01,440 --> 00:15:02,720
it out where you can see additional

451
00:15:02,720 --> 00:15:04,639
results and read an extensive discussion

452
00:15:04,639 --> 00:15:06,399
section about how habitat can be

453
00:15:06,399 --> 00:15:08,000
extended to other training setups and

454
00:15:08,000 --> 00:15:09,920
other hardware to conclude i wanted to

455
00:15:09,920 --> 00:15:11,440
highlight the key takeaways that i hope

456
00:15:11,440 --> 00:15:13,600
you got from this talk first deep neural

457
00:15:13,600 --> 00:15:15,519
network training computation is special

458
00:15:15,519 --> 00:15:17,360
because it is repetitive and this

459
00:15:17,360 --> 00:15:19,839
enables new analysis opportunities

460
00:15:19,839 --> 00:15:22,079
habitat leverages this repetitiveness to

461
00:15:22,079 --> 00:15:23,920
use runtime-based information to make

462
00:15:23,920 --> 00:15:26,800
its iteration execution time predictions

463
00:15:26,800 --> 00:15:28,320
and in the case studies we showed

464
00:15:28,320 --> 00:15:30,560
despite any prediction errors habitat

465
00:15:30,560 --> 00:15:32,399
correctly predicts the relative ordering

466
00:15:32,399 --> 00:15:33,680
among the gpus that were being

467
00:15:33,680 --> 00:15:35,440
considered and this allows users to make

468
00:15:35,440 --> 00:15:38,079
the correct decision for their needs

469
00:15:38,079 --> 00:15:40,000
and finally the hardware landscape for

470
00:15:40,000 --> 00:15:41,519
deep neural network training is really

471
00:15:41,519 --> 00:15:43,440
growing users will need help choosing

472
00:15:43,440 --> 00:15:45,360
the right accelerator effectively and

473
00:15:45,360 --> 00:15:47,519
habitat is one tool that can help

474
00:15:47,519 --> 00:15:49,360
habitat is open source it currently

475
00:15:49,360 --> 00:15:51,440
supports pytorch and it is available on

476
00:15:51,440 --> 00:15:52,399
github

477
00:15:52,399 --> 00:15:55,839
and thank you for listening to our talk

