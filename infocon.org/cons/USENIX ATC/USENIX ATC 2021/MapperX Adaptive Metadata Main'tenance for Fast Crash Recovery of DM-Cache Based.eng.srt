1
00:00:08,000 --> 00:00:10,160
hello everyone i'm lucha ying i'm going

2
00:00:10,160 --> 00:00:11,200
to tell you about

3
00:00:11,200 --> 00:00:15,040
our work the mapper x ssds are

4
00:00:15,040 --> 00:00:17,600
preferable to hdds in building cloud

5
00:00:17,600 --> 00:00:19,199
storage systems

6
00:00:19,199 --> 00:00:22,560
as ssds significantly outperform

7
00:00:22,560 --> 00:00:25,199
hdds in render small i o which is

8
00:00:25,199 --> 00:00:25,840
dominant

9
00:00:25,840 --> 00:00:28,800
in the cloud since current ssds are

10
00:00:28,800 --> 00:00:30,080
still much more

11
00:00:30,080 --> 00:00:33,360
expensive than hdds in recent years

12
00:00:33,360 --> 00:00:36,960
we see a trend of adopting hdd ssd

13
00:00:36,960 --> 00:00:40,320
hybrid storage for high io performance

14
00:00:40,320 --> 00:00:44,079
at low monetary cost for instance

15
00:00:44,079 --> 00:00:47,520
ursa is a distributed block storage

16
00:00:47,520 --> 00:00:48,399
system

17
00:00:48,399 --> 00:00:52,399
which stores primary replicas on ssds

18
00:00:52,399 --> 00:00:55,760
and replicates back up replicates on

19
00:00:55,760 --> 00:00:56,960
hdes

20
00:00:56,960 --> 00:01:00,640
and sshd integrates a small ssd

21
00:01:00,640 --> 00:01:04,080
inside a large hdd of which the ssd

22
00:01:04,080 --> 00:01:09,520
acts as a catch as the demand of hdd ssd

23
00:01:09,520 --> 00:01:12,799
hybrid storage increases linux kernel

24
00:01:12,799 --> 00:01:13,119
has

25
00:01:13,119 --> 00:01:16,880
supported users to combine hdds and ssds

26
00:01:16,880 --> 00:01:18,240
to jointly provide

27
00:01:18,240 --> 00:01:21,520
virtual block storage device dm catch is

28
00:01:21,520 --> 00:01:23,600
a component of the device mapper

29
00:01:23,600 --> 00:01:26,159
in the kernel which has been widely used

30
00:01:26,159 --> 00:01:27,280
in industry

31
00:01:27,280 --> 00:01:30,880
to map ssds and hdds onto higher

32
00:01:30,880 --> 00:01:33,680
level virtual block devices that take

33
00:01:33,680 --> 00:01:35,759
fast ssds as a cache

34
00:01:35,759 --> 00:01:39,759
for slows hdbs dmcat records the mapping

35
00:01:39,759 --> 00:01:41,040
between ssds

36
00:01:41,040 --> 00:01:43,920
and hdbs for each cat block in its

37
00:01:43,920 --> 00:01:46,560
metadata

38
00:01:46,560 --> 00:01:49,119
when dmcat adopts the default right-back

39
00:01:49,119 --> 00:01:49,840
mode

40
00:01:49,840 --> 00:01:53,040
block write will go all into the ssd

41
00:01:53,040 --> 00:01:54,880
catch and get acknowledged

42
00:01:54,880 --> 00:01:58,240
after being marked dirty so that the

43
00:01:58,240 --> 00:02:00,560
dirty block could be demoted from the

44
00:02:00,560 --> 00:02:02,880
ssd cache to the hdd

45
00:02:02,880 --> 00:02:05,520
later in a batch for accelerating random

46
00:02:05,520 --> 00:02:06,880
small i o

47
00:02:06,880 --> 00:02:09,360
consider a small write to a virtual dm

48
00:02:09,360 --> 00:02:10,878
catch device

49
00:02:10,878 --> 00:02:13,680
in the red bar mode if the target block

50
00:02:13,680 --> 00:02:16,080
is already in the ssd cache

51
00:02:16,080 --> 00:02:18,879
then as shown in figure first the new

52
00:02:18,879 --> 00:02:19,760
data w

53
00:02:19,760 --> 00:02:22,959
is written to the ssd then the

54
00:02:22,959 --> 00:02:25,040
corresponding bit of the block

55
00:02:25,040 --> 00:02:28,560
is set 30 in memory and then the right

56
00:02:28,560 --> 00:02:32,040
is acknowledged the catch block will be

57
00:02:32,040 --> 00:02:35,200
asynchronously persisted to the catch

58
00:02:35,200 --> 00:02:38,560
according to the cache policy there are

59
00:02:38,560 --> 00:02:40,720
two kinds of metadata

60
00:02:40,720 --> 00:02:43,920
the metadata through the cache blocks

61
00:02:43,920 --> 00:02:47,040
mapping between ssd and hdd

62
00:02:47,040 --> 00:02:49,440
already exist and thus needs no

63
00:02:49,440 --> 00:02:50,720
persistence

64
00:02:50,720 --> 00:02:53,599
and the metadata for the block's 30 bit

65
00:02:53,599 --> 00:02:55,920
has to be asynchronously

66
00:02:55,920 --> 00:02:58,920
persisted because once being

67
00:02:58,920 --> 00:03:00,159
synchronously

68
00:03:00,159 --> 00:03:03,040
persisted this update will be on the

69
00:03:03,040 --> 00:03:04,000
critical path

70
00:03:04,000 --> 00:03:07,120
of rating w which would greatly affect

71
00:03:07,120 --> 00:03:10,720
the performance of catch writes

72
00:03:10,720 --> 00:03:13,680
in original dm catch the persist the

73
00:03:13,680 --> 00:03:15,680
persistence of the dirty bits

74
00:03:15,680 --> 00:03:18,879
is not for crash recovery but for grease

75
00:03:18,879 --> 00:03:20,080
full shutdown

76
00:03:20,080 --> 00:03:22,000
which happens after 30 bits are

77
00:03:22,000 --> 00:03:23,280
persisted

78
00:03:23,280 --> 00:03:26,080
so that up to date 30 bit metadata can

79
00:03:26,080 --> 00:03:28,159
be read after reboot

80
00:03:28,159 --> 00:03:30,480
if the target block is not yet in the

81
00:03:30,480 --> 00:03:31,200
catch

82
00:03:31,200 --> 00:03:33,599
then the process scene is slightly

83
00:03:33,599 --> 00:03:34,879
complex

84
00:03:34,879 --> 00:03:37,120
when the right is not aligned with the

85
00:03:37,120 --> 00:03:38,400
block the block

86
00:03:38,400 --> 00:03:41,360
needs to be first promoted to the catch

87
00:03:41,360 --> 00:03:42,319
with the first

88
00:03:42,319 --> 00:03:45,519
kind of metadata being persisted after

89
00:03:45,519 --> 00:03:46,000
which

90
00:03:46,000 --> 00:03:48,400
the processing is the same as catch

91
00:03:48,400 --> 00:03:49,599
block writes

92
00:03:49,599 --> 00:03:51,920
note that the mapping metadata must be

93
00:03:51,920 --> 00:03:54,239
synchronously updated

94
00:03:54,239 --> 00:03:57,280
in which case the synchronous update of

95
00:03:57,280 --> 00:03:59,200
the 30-bit metadata

96
00:03:59,200 --> 00:04:02,159
is less harmful because the number of

97
00:04:02,159 --> 00:04:03,680
metadata rights only

98
00:04:03,680 --> 00:04:07,760
increases from 1 to 2. in contrast

99
00:04:07,760 --> 00:04:10,480
if the mapping metadata needs to no

100
00:04:10,480 --> 00:04:11,200
update

101
00:04:11,200 --> 00:04:14,000
then the number of metadata writes

102
00:04:14,000 --> 00:04:15,519
sharply increases from

103
00:04:15,519 --> 00:04:18,639
0 to 1. mapping metadata updates

104
00:04:18,639 --> 00:04:21,199
is largely decided by the locality of

105
00:04:21,199 --> 00:04:23,040
the workloads together with the

106
00:04:23,040 --> 00:04:24,400
replacements

107
00:04:24,400 --> 00:04:27,759
policy generally speaking

108
00:04:27,759 --> 00:04:30,240
higher locality of writes cause less

109
00:04:30,240 --> 00:04:31,120
changes of

110
00:04:31,120 --> 00:04:33,360
mapping metadata which make makes

111
00:04:33,360 --> 00:04:34,240
synchronous

112
00:04:34,240 --> 00:04:37,520
30-bit persistence have higher negative

113
00:04:37,520 --> 00:04:40,639
impact on the performance of cash right

114
00:04:40,639 --> 00:04:44,000
and vice versa to demonstrate the

115
00:04:44,000 --> 00:04:45,919
runtime performance problem of

116
00:04:45,919 --> 00:04:48,720
persisting metadata for each turret

117
00:04:48,720 --> 00:04:51,040
we compare the i o performance of dm

118
00:04:51,040 --> 00:04:52,720
catch with synchronous and

119
00:04:52,720 --> 00:04:55,759
asynchronous updates respectively

120
00:04:55,759 --> 00:04:58,800
we use fio to performance random rights

121
00:04:58,800 --> 00:04:59,040
and

122
00:04:59,040 --> 00:05:02,720
evaluate the iops with io depth equals

123
00:05:02,720 --> 00:05:05,919
to 16 using various catch block sites

124
00:05:05,919 --> 00:05:07,919
ranging from 32 kb

125
00:05:07,919 --> 00:05:12,880
to 132 kb the fio right sides are the

126
00:05:12,880 --> 00:05:15,360
same as the catch block sizes

127
00:05:15,360 --> 00:05:17,520
the results show that the higher

128
00:05:17,520 --> 00:05:19,520
performance is still really

129
00:05:19,520 --> 00:05:22,560
effective when adopting

130
00:05:22,560 --> 00:05:25,840
synchronous metadata update when the

131
00:05:25,840 --> 00:05:26,720
iops

132
00:05:26,720 --> 00:05:30,560
degradation of about 2.13 times

133
00:05:30,560 --> 00:05:34,960
to 3.42 times the high runtime overhead

134
00:05:34,960 --> 00:05:35,680
prevents

135
00:05:35,680 --> 00:05:38,880
synchronous update of 30-bit metadata

136
00:05:38,880 --> 00:05:42,160
from being adopted by dm catch

137
00:05:42,160 --> 00:05:45,680
unfortunately asynchronous metadata

138
00:05:45,680 --> 00:05:46,479
updates

139
00:05:46,479 --> 00:05:49,600
caused all cached data on ssds to be

140
00:05:49,600 --> 00:05:50,479
assumed

141
00:05:50,479 --> 00:05:53,600
dirty once the system crashes and gets

142
00:05:53,600 --> 00:05:55,199
restarted

143
00:05:55,199 --> 00:05:57,680
which results in long crash recovery

144
00:05:57,680 --> 00:05:58,319
times

145
00:05:58,319 --> 00:06:02,160
and consequently no availability

146
00:06:02,160 --> 00:06:05,840
for example in our production block

147
00:06:05,840 --> 00:06:09,039
storage system where we use dm catch to

148
00:06:09,039 --> 00:06:10,160
combine multiple

149
00:06:10,160 --> 00:06:14,160
ssds with multiple hdds on each storage

150
00:06:14,160 --> 00:06:17,199
machine it takes more than two hours

151
00:06:17,199 --> 00:06:20,639
to recover from a power failure even if

152
00:06:20,639 --> 00:06:23,160
most blocks are clean the low

153
00:06:23,160 --> 00:06:25,039
availability

154
00:06:25,039 --> 00:06:28,880
prevents linux hdd ssd hybrid

155
00:06:28,880 --> 00:06:32,479
cache mechanism from being widely

156
00:06:32,479 --> 00:06:37,440
applied in available sensitive scenarios

157
00:06:37,440 --> 00:06:40,400
the timing of 30-bit metadata update is

158
00:06:40,400 --> 00:06:42,880
a dilemma for hdd-ssd

159
00:06:42,880 --> 00:06:46,840
hybrid devices the asynchronous update

160
00:06:46,840 --> 00:06:50,160
mechanism periodically updates 30 bits

161
00:06:50,160 --> 00:06:50,880
for

162
00:06:50,880 --> 00:06:53,840
not affecting normal rates but suffers

163
00:06:53,840 --> 00:06:54,240
from

164
00:06:54,240 --> 00:06:57,680
long crash recovery time the challenge

165
00:06:57,680 --> 00:07:00,800
is that there is no trade-offs for the

166
00:07:00,800 --> 00:07:02,080
timing for

167
00:07:02,080 --> 00:07:05,440
of 30 bit metadata updates because

168
00:07:05,440 --> 00:07:08,160
once the metadata is asynchronously

169
00:07:08,160 --> 00:07:09,039
updated

170
00:07:09,039 --> 00:07:12,080
even only one updated bit state

171
00:07:12,080 --> 00:07:14,400
would require the motion of all catch

172
00:07:14,400 --> 00:07:16,000
blocks to ensure

173
00:07:16,000 --> 00:07:19,039
data dual reversibility after crash

174
00:07:19,039 --> 00:07:20,080
recovery

175
00:07:20,080 --> 00:07:23,919
to address these challenges our key idea

176
00:07:23,919 --> 00:07:25,560
is to adjust

177
00:07:25,560 --> 00:07:28,639
granularity instead of the timing

178
00:07:28,639 --> 00:07:31,280
of the 30-bit metadata updates to

179
00:07:31,280 --> 00:07:34,240
smoothly trade off between normal i o

180
00:07:34,240 --> 00:07:37,280
performance and crash recovery time all

181
00:07:37,280 --> 00:07:38,880
clouds in the cloud

182
00:07:38,880 --> 00:07:42,560
usually have adequate right locality

183
00:07:42,560 --> 00:07:45,599
which can be a little reached to use one

184
00:07:45,599 --> 00:07:46,000
bit

185
00:07:46,000 --> 00:07:50,039
to represent the state of the range of

186
00:07:50,039 --> 00:07:53,440
consecutive blocks as long as we can

187
00:07:53,440 --> 00:07:55,759
roughly know the effective range of the

188
00:07:55,759 --> 00:07:56,479
beat

189
00:07:56,479 --> 00:07:59,360
know that the false positives of the

190
00:07:59,360 --> 00:08:02,000
effective range are not critical

191
00:08:02,000 --> 00:08:05,360
if no clean blocks is wrongly included

192
00:08:05,360 --> 00:08:08,479
in the effective range of 30 bit the

193
00:08:08,479 --> 00:08:10,879
price is simply

194
00:08:10,879 --> 00:08:14,240
an unnecessary demotion of the block

195
00:08:14,240 --> 00:08:17,759
in crash recovery there is an

196
00:08:17,759 --> 00:08:20,639
in-memory bitmap that precisely records

197
00:08:20,639 --> 00:08:21,520
the state of

198
00:08:21,520 --> 00:08:23,840
every block we first extend the

199
00:08:23,840 --> 00:08:25,360
in-memory bitmap

200
00:08:25,360 --> 00:08:28,400
to a hierarchical complete b3

201
00:08:28,400 --> 00:08:31,840
or cbt where the leaves are the bits of

202
00:08:31,840 --> 00:08:32,880
the bitmap

203
00:08:32,880 --> 00:08:35,839
and each inner node is the disjunction

204
00:08:35,839 --> 00:08:36,399
of its

205
00:08:36,399 --> 00:08:38,880
directed children then mapper x

206
00:08:38,880 --> 00:08:40,080
maintains a summary

207
00:08:40,080 --> 00:08:43,120
of the cpt on the metadata device

208
00:08:43,120 --> 00:08:47,040
which we refer to as on disk adaptive b3

209
00:08:47,040 --> 00:08:50,160
abt the apt is synchronously

210
00:08:50,160 --> 00:08:53,279
updated for each write request but most

211
00:08:53,279 --> 00:08:53,680
rook

212
00:08:53,680 --> 00:08:56,480
updates do not cause disco rights for

213
00:08:56,480 --> 00:08:58,399
metadata persistence

214
00:08:58,399 --> 00:09:01,680
each inner node of the apt represents a

215
00:09:01,680 --> 00:09:02,160
range

216
00:09:02,160 --> 00:09:05,519
of consecutive blocks and thus owning

217
00:09:05,519 --> 00:09:06,080
the first

218
00:09:06,080 --> 00:09:08,959
30 block within the range causes a write

219
00:09:08,959 --> 00:09:10,880
to the metadata device

220
00:09:10,880 --> 00:09:14,000
and subsequently writes of

221
00:09:14,000 --> 00:09:17,279
blocks in the range leads no persistence

222
00:09:17,279 --> 00:09:20,240
for example in this figure the node a of

223
00:09:20,240 --> 00:09:21,040
abt

224
00:09:21,040 --> 00:09:24,399
represses the range of block 0 to block

225
00:09:24,399 --> 00:09:25,040
1.

226
00:09:25,040 --> 00:09:28,320
the node b of abt represents the range

227
00:09:28,320 --> 00:09:30,399
of block 2 to block 3

228
00:09:30,399 --> 00:09:33,920
and node c represents the range of block

229
00:09:33,920 --> 00:09:34,320
4

230
00:09:34,320 --> 00:09:37,680
to block 7. initially

231
00:09:37,680 --> 00:09:40,080
the apt only has a root node

232
00:09:40,080 --> 00:09:42,000
representing the states of

233
00:09:42,000 --> 00:09:44,800
all cache blocks which will change from

234
00:09:44,800 --> 00:09:46,880
clean to 30 after the first

235
00:09:46,880 --> 00:09:50,480
block right once a blocks state change

236
00:09:50,480 --> 00:09:53,600
the cbt in memory has to be changed

237
00:09:53,600 --> 00:09:57,680
also and it will be persisted in apt

238
00:09:57,680 --> 00:10:00,160
for example in this figure when the

239
00:10:00,160 --> 00:10:01,120
block 7

240
00:10:01,120 --> 00:10:04,160
change to clean from 30 the leaf node of

241
00:10:04,160 --> 00:10:05,040
number 7

242
00:10:05,040 --> 00:10:07,440
and its father and grandfather will

243
00:10:07,440 --> 00:10:09,279
follow to change to clean

244
00:10:09,279 --> 00:10:12,240
and the node of abt we which include

245
00:10:12,240 --> 00:10:13,279
number 7

246
00:10:13,279 --> 00:10:17,440
should be persisted with a clean beat

247
00:10:17,440 --> 00:10:20,640
mapper x proceeds a synchronous abt

248
00:10:20,640 --> 00:10:24,000
update mechanism which can adaptively

249
00:10:24,000 --> 00:10:26,480
adjust the metadata persistence

250
00:10:26,480 --> 00:10:27,440
frequency

251
00:10:27,440 --> 00:10:30,240
the basic idea is to control the

252
00:10:30,240 --> 00:10:31,440
expanding bits

253
00:10:31,440 --> 00:10:34,560
of the leaves by adding or deleting

254
00:10:34,560 --> 00:10:37,760
the leaves in the abt leaves at higher

255
00:10:37,760 --> 00:10:40,640
levels in the abt represent the states

256
00:10:40,640 --> 00:10:40,880
of

257
00:10:40,880 --> 00:10:43,839
a small range of blocks and thus

258
00:10:43,839 --> 00:10:45,200
increasing the levels

259
00:10:45,200 --> 00:10:48,240
of the abt will increase the metadata

260
00:10:48,240 --> 00:10:50,320
persistence overhead

261
00:10:50,320 --> 00:10:52,880
while reducing the expected recovery

262
00:10:52,880 --> 00:10:53,440
time

263
00:10:53,440 --> 00:10:56,640
and vice versa when

264
00:10:56,640 --> 00:11:00,320
adding child leaves to an existing leaf

265
00:11:00,320 --> 00:11:00,800
node

266
00:11:00,800 --> 00:11:04,160
the information of about which children

267
00:11:04,160 --> 00:11:06,320
are dirty can be obtained from the

268
00:11:06,320 --> 00:11:08,240
logical cbt

269
00:11:08,240 --> 00:11:11,200
since the user perceived io performance

270
00:11:11,200 --> 00:11:12,640
is usually described

271
00:11:12,640 --> 00:11:15,200
as tail latency we use the number of

272
00:11:15,200 --> 00:11:15,760
knights

273
00:11:15,760 --> 00:11:18,959
of the lsa to control the summary

274
00:11:18,959 --> 00:11:22,720
levels of the abt

275
00:11:22,720 --> 00:11:25,600
the periodical to adjust procedure

276
00:11:25,600 --> 00:11:27,200
decides whether to add

277
00:11:27,200 --> 00:11:30,240
delete leap nodes in the abt according

278
00:11:30,240 --> 00:11:31,600
to the state

279
00:11:31,600 --> 00:11:34,880
test codes of the last period

280
00:11:34,880 --> 00:11:37,519
the period is configurable and by

281
00:11:37,519 --> 00:11:38,079
default

282
00:11:38,079 --> 00:11:41,279
set was one second if there are too many

283
00:11:41,279 --> 00:11:42,480
metadata rights

284
00:11:42,480 --> 00:11:46,240
compared to the sla in one second

285
00:11:46,240 --> 00:11:48,480
then we will remove all the children of

286
00:11:48,480 --> 00:11:49,600
the parents

287
00:11:49,600 --> 00:11:52,399
that has experienced the most metadata

288
00:11:52,399 --> 00:11:54,480
rights during one second

289
00:11:54,480 --> 00:11:57,519
otherwise we will add children to the

290
00:11:57,519 --> 00:11:58,800
lyft node that has

291
00:11:58,800 --> 00:12:01,600
experienced the least metadata rights

292
00:12:01,600 --> 00:12:04,880
during one second

293
00:12:05,200 --> 00:12:08,320
since the apt is synchronously updated

294
00:12:08,320 --> 00:12:10,320
for every right request

295
00:12:10,320 --> 00:12:12,880
the lymph nodes in the ept has no false

296
00:12:12,880 --> 00:12:13,839
negatives

297
00:12:13,839 --> 00:12:16,800
for dirty states that is when leaf is

298
00:12:16,800 --> 00:12:17,760
not dirty

299
00:12:17,760 --> 00:12:21,120
each of the blocks is represented are

300
00:12:21,120 --> 00:12:23,519
guaranteed to be not dirty and we can

301
00:12:23,519 --> 00:12:25,279
safely skip these

302
00:12:25,279 --> 00:12:28,720
blocks in classroom recovery therefore

303
00:12:28,720 --> 00:12:31,760
the recovery procedure of mapper x is

304
00:12:31,760 --> 00:12:32,959
straightforward

305
00:12:32,959 --> 00:12:35,600
for each dirty leaf node of the abt

306
00:12:35,600 --> 00:12:36,240
demos

307
00:12:36,240 --> 00:12:41,519
all ssd catched blocks of it to the hdd

308
00:12:41,519 --> 00:12:44,320
in order not to introduce extra storage

309
00:12:44,320 --> 00:12:45,279
overhead

310
00:12:45,279 --> 00:12:48,880
we reuse dm catches 4 by 30 bit metadata

311
00:12:48,880 --> 00:12:49,760
structure

312
00:12:49,760 --> 00:12:53,279
for each cat block of which dm catch

313
00:12:53,279 --> 00:12:54,560
uses only the

314
00:12:54,560 --> 00:12:58,000
last 2 bit a 30 bit and the valid bit

315
00:12:58,000 --> 00:13:00,959
leaving the first 30 bits available for

316
00:13:00,959 --> 00:13:02,160
mapper x

317
00:13:02,160 --> 00:13:05,040
we organize the first 30 bits of all

318
00:13:05,040 --> 00:13:05,440
catch

319
00:13:05,440 --> 00:13:08,880
blocks metadata structures into a flat

320
00:13:08,880 --> 00:13:12,079
beat battery to minimize the update

321
00:13:12,079 --> 00:13:13,040
overhead of

322
00:13:13,040 --> 00:13:16,480
adding or deleting leaves we store abt

323
00:13:16,480 --> 00:13:20,399
as the as the virtual abto vabt

324
00:13:20,399 --> 00:13:23,200
which has the same number of levels and

325
00:13:23,200 --> 00:13:24,399
leaves us the

326
00:13:24,399 --> 00:13:27,760
cpt but where only the

327
00:13:27,760 --> 00:13:30,959
abt stitch leaves a one and all other

328
00:13:30,959 --> 00:13:34,800
inner leaf node are zero we use the flat

329
00:13:34,800 --> 00:13:36,639
bit area for statically

330
00:13:36,639 --> 00:13:40,000
representing the states of all inner

331
00:13:40,000 --> 00:13:40,560
nodes

332
00:13:40,560 --> 00:13:44,480
in the vabt this is our

333
00:13:44,480 --> 00:13:47,920
evaluation results the client runs the

334
00:13:47,920 --> 00:13:49,360
fio benchmark

335
00:13:49,360 --> 00:13:52,240
tool to perform random writes that all

336
00:13:52,240 --> 00:13:53,760
hit the ssd catch

337
00:13:53,760 --> 00:13:57,360
for 300 seconds we first compare the

338
00:13:57,360 --> 00:14:00,240
latency of random writes of mapper x and

339
00:14:00,240 --> 00:14:01,440
the dm cache

340
00:14:01,440 --> 00:14:03,920
we use better to represent the expected

341
00:14:03,920 --> 00:14:05,600
ratio of metadata

342
00:14:05,600 --> 00:14:09,040
rise to all rights where beta is 1 10 to

343
00:14:09,040 --> 00:14:09,839
the power of

344
00:14:09,839 --> 00:14:13,440
n where n is the lsa the

345
00:14:13,440 --> 00:14:16,959
original dam catch an asynchronous

346
00:14:16,959 --> 00:14:19,839
metadata update the results show that

347
00:14:19,839 --> 00:14:22,160
latency overhead of mapper x

348
00:14:22,160 --> 00:14:25,199
slightly increased but the overhead is

349
00:14:25,199 --> 00:14:28,639
small compared to the original dm catch

350
00:14:28,639 --> 00:14:31,360
in addition we compare the recovery

351
00:14:31,360 --> 00:14:32,240
performance of

352
00:14:32,240 --> 00:14:35,760
mapper x and the original dm catch

353
00:14:35,760 --> 00:14:39,360
using the public io traces we present

354
00:14:39,360 --> 00:14:40,160
mapper x

355
00:14:40,160 --> 00:14:42,800
a normal extension to dm cache that you

356
00:14:42,800 --> 00:14:44,160
uses a

357
00:14:44,160 --> 00:14:47,680
on disk b3 to synchronously persist

358
00:14:47,680 --> 00:14:51,360
the 30-bit metadata in a hierarchical

359
00:14:51,360 --> 00:14:52,199
manner

360
00:14:52,199 --> 00:14:55,360
experimental results show that mapper x

361
00:14:55,360 --> 00:14:58,560
significantly outperforms the m catch in

362
00:14:58,560 --> 00:15:01,680
crash recovery times while only

363
00:15:01,680 --> 00:15:05,040
introducing negligible metadata

364
00:15:05,040 --> 00:15:13,839
right overhead thank you

365
00:15:15,120 --> 00:15:17,199
you

