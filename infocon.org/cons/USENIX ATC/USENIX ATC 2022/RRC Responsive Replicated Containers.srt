1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:12,780 --> 00:00:15,059
good morning everyone and today I will

3
00:00:15,059 --> 00:00:17,279
be presenting a container replication

4
00:00:17,279 --> 00:00:20,400
mechanism called RC that provides low

5
00:00:20,400 --> 00:00:23,060
overhead and fully application

6
00:00:23,060 --> 00:00:25,859
transparent for torrents for critical

7
00:00:25,859 --> 00:00:28,740
server applications this is a joint work

8
00:00:28,740 --> 00:00:31,740
with my PhD advisor evolved

9
00:00:31,740 --> 00:00:34,260
so server applications demand low

10
00:00:34,260 --> 00:00:36,300
latency high throughput and high

11
00:00:36,300 --> 00:00:39,480
reliability and as a result server

12
00:00:39,480 --> 00:00:42,000
applications is generally multi-threaded

13
00:00:42,000 --> 00:00:44,100
and they require some sort of thought

14
00:00:44,100 --> 00:00:46,500
tolerance mechanisms to provide High

15
00:00:46,500 --> 00:00:47,879
reliability

16
00:00:47,879 --> 00:00:50,300
the four tolerance mechanisms

17
00:00:50,300 --> 00:00:53,399
requirement for a server application is

18
00:00:53,399 --> 00:00:55,559
that it needs to maintain low latency

19
00:00:55,559 --> 00:00:59,399
and uh and throughput overhead will also

20
00:00:59,399 --> 00:01:01,860
support multi-threaded

21
00:01:01,860 --> 00:01:04,619
and ideally a for tolerance mechanism

22
00:01:04,619 --> 00:01:06,659
should minimize the development cost

23
00:01:06,659 --> 00:01:09,659
this means it should not introduce any

24
00:01:09,659 --> 00:01:12,479
code modification to the clients and to

25
00:01:12,479 --> 00:01:14,700
the servers it should maintain fully

26
00:01:14,700 --> 00:01:17,100
compatibility with the existing clients

27
00:01:17,100 --> 00:01:19,500
so in other words the four Taurus

28
00:01:19,500 --> 00:01:21,240
mechanism needs to be application

29
00:01:21,240 --> 00:01:22,799
transparent

30
00:01:22,799 --> 00:01:26,100
and a widely used mechanism to to

31
00:01:26,100 --> 00:01:28,200
achieve application transparent for

32
00:01:28,200 --> 00:01:31,020
tolerance is through replication so here

33
00:01:31,020 --> 00:01:33,360
is an overview of how it works the

34
00:01:33,360 --> 00:01:35,100
clients communicate with the third

35
00:01:35,100 --> 00:01:37,560
applications in the primary host

36
00:01:37,560 --> 00:01:40,140
and the photo and the replication

37
00:01:40,140 --> 00:01:43,200
mechanism ensure that the server

38
00:01:43,200 --> 00:01:45,659
application in the backup host has a

39
00:01:45,659 --> 00:01:47,340
consistent state with that in the

40
00:01:47,340 --> 00:01:50,280
primary hoof and as a result if the

41
00:01:50,280 --> 00:01:53,340
primary host fails the clients can

42
00:01:53,340 --> 00:01:55,200
directly communicate with the third

43
00:01:55,200 --> 00:01:57,840
application in the backup host and

44
00:01:57,840 --> 00:01:59,520
without being affected

45
00:01:59,520 --> 00:02:03,119
so replication is a widely used old

46
00:02:03,119 --> 00:02:05,700
technique and it has been deployed since

47
00:02:05,700 --> 00:02:08,459
1960s if not earlier

48
00:02:08,459 --> 00:02:11,340
but what is missing with with the

49
00:02:11,340 --> 00:02:13,800
existing replication schemes for many

50
00:02:13,800 --> 00:02:16,500
other schemes they require customized

51
00:02:16,500 --> 00:02:18,720
hardware and there's no support for

52
00:02:18,720 --> 00:02:20,940
multi-threaded applications

53
00:02:20,940 --> 00:02:23,220
there is a class of schemes based on

54
00:02:23,220 --> 00:02:24,959
checkpointing to a passive backup

55
00:02:24,959 --> 00:02:28,620
however this scheme incurs unacceptable

56
00:02:28,620 --> 00:02:30,840
High latency overhead for several

57
00:02:30,840 --> 00:02:32,040
applications

58
00:02:32,040 --> 00:02:34,620
another class of schemes is based on

59
00:02:34,620 --> 00:02:36,720
active replication and they are

60
00:02:36,720 --> 00:02:38,940
vulnerable to non-deterministic events

61
00:02:38,940 --> 00:02:41,580
and this causes unpredictably slow down

62
00:02:41,580 --> 00:02:43,640
during normal operations or even

63
00:02:43,640 --> 00:02:47,040
recovery failure Furthermore with active

64
00:02:47,040 --> 00:02:49,739
replication the performance of the whole

65
00:02:49,739 --> 00:02:52,620
replication system is limited by the

66
00:02:52,620 --> 00:02:55,080
tight coupling among replicas so a slow

67
00:02:55,080 --> 00:02:58,280
replica may slow down the whole system

68
00:02:58,280 --> 00:03:01,620
the rfe mechanism I'm going to present

69
00:03:01,620 --> 00:03:04,319
today overcomes this above limitations

70
00:03:04,319 --> 00:03:07,140
with a key insight to decouple

71
00:03:07,140 --> 00:03:09,660
replication related operations from

72
00:03:09,660 --> 00:03:11,220
normal operations

73
00:03:11,220 --> 00:03:13,980
so I'm not I'm next going to present the

74
00:03:13,980 --> 00:03:16,260
limitations of the existing replication

75
00:03:16,260 --> 00:03:18,840
mechanisms which forms the motivation

76
00:03:18,840 --> 00:03:20,700
for RC

77
00:03:20,700 --> 00:03:22,860
so one class of the replication

78
00:03:22,860 --> 00:03:25,620
mechanism is based on checkpointing to a

79
00:03:25,620 --> 00:03:27,900
passive backup and here is how it works

80
00:03:27,900 --> 00:03:31,200
the execution of the application is

81
00:03:31,200 --> 00:03:34,140
divided into multiple efforts and during

82
00:03:34,140 --> 00:03:37,260
the execute stage of one Epoch the

83
00:03:37,260 --> 00:03:39,959
application executes but the output to

84
00:03:39,959 --> 00:03:42,480
the client is being buffered

85
00:03:42,480 --> 00:03:45,540
and then at the end of the epoch a

86
00:03:45,540 --> 00:03:47,760
checkpoint is taken

87
00:03:47,760 --> 00:03:50,280
the application then resume execution

88
00:03:50,280 --> 00:03:53,159
and the replication mechanism sends the

89
00:03:53,159 --> 00:03:55,019
checkpoint to the backup host

90
00:03:55,019 --> 00:03:57,599
the backup host acknowledges the receipt

91
00:03:57,599 --> 00:04:00,500
of the checkpoint and only until then

92
00:04:00,500 --> 00:04:04,860
the output to the client can be released

93
00:04:04,860 --> 00:04:07,440
the buffering and releasing output is

94
00:04:07,440 --> 00:04:10,200
needed to ensure that upon primary

95
00:04:10,200 --> 00:04:12,720
failure the backup can restore to a

96
00:04:12,720 --> 00:04:14,580
state that is consistent with the

97
00:04:14,580 --> 00:04:15,900
clients

98
00:04:15,900 --> 00:04:18,540
and a critical drawback of this

99
00:04:18,540 --> 00:04:20,880
mechanism is the high latency overhead

100
00:04:20,880 --> 00:04:24,300
so specifically output generates during

101
00:04:24,300 --> 00:04:27,600
the execution phase of one Epoch cannot

102
00:04:27,600 --> 00:04:29,759
be released until the backup has

103
00:04:29,759 --> 00:04:31,740
acknowledged the receipt of the

104
00:04:31,740 --> 00:04:33,000
checkpoint

105
00:04:33,000 --> 00:04:35,520
and since checkpointing is an expensive

106
00:04:35,520 --> 00:04:38,160
operation there is a critical trade-off

107
00:04:38,160 --> 00:04:40,500
involving checkpointing where a short

108
00:04:40,500 --> 00:04:43,199
interval leads to a high throughput low

109
00:04:43,199 --> 00:04:45,780
latency overhead well a long interval

110
00:04:45,780 --> 00:04:48,240
leads to low throughput and high latency

111
00:04:48,240 --> 00:04:49,320
overhead

112
00:04:49,320 --> 00:04:52,680
in reality the replication mechanism can

113
00:04:52,680 --> 00:04:55,380
only Offroad a checkpointing interval of

114
00:04:55,380 --> 00:04:57,840
tens of milliseconds and this translates

115
00:04:57,840 --> 00:05:00,240
to at least tens of millisecond delay

116
00:05:00,240 --> 00:05:03,120
which is unacceptably high for many

117
00:05:03,120 --> 00:05:05,699
server applications

118
00:05:05,699 --> 00:05:07,680
another class of the replication

119
00:05:07,680 --> 00:05:10,199
mechanism is based on an active backup

120
00:05:10,199 --> 00:05:13,139
in this case both the primary and the

121
00:05:13,139 --> 00:05:15,960
backup execute application code

122
00:05:15,960 --> 00:05:19,139
and instead of sending a checkpoint at

123
00:05:19,139 --> 00:05:21,240
the end of an Epoch the primary

124
00:05:21,240 --> 00:05:23,699
continuously sends the outcomes of

125
00:05:23,699 --> 00:05:26,100
non-determined events is encountered

126
00:05:26,100 --> 00:05:28,860
during execution to the backup

127
00:05:28,860 --> 00:05:32,400
and backup uses this uh uses the event

128
00:05:32,400 --> 00:05:35,520
log to replay its execution to match

129
00:05:35,520 --> 00:05:37,680
that of the primary

130
00:05:37,680 --> 00:05:40,320
and since we with the active backup

131
00:05:40,320 --> 00:05:43,259
mechanism the backup execution must be

132
00:05:43,259 --> 00:05:45,240
consistent with the primary

133
00:05:45,240 --> 00:05:48,419
if the primary fails to check certain

134
00:05:48,419 --> 00:05:51,479
undetermined events such as data races

135
00:05:51,479 --> 00:05:54,060
then this would lead to unpredictably

136
00:05:54,060 --> 00:05:56,400
slowdown during normal operation or even

137
00:05:56,400 --> 00:05:58,020
recovery failure

138
00:05:58,020 --> 00:06:00,900
Furthermore with active backup mechanism

139
00:06:00,900 --> 00:06:03,720
the performance is always limited by the

140
00:06:03,720 --> 00:06:06,180
tight coupling between replicas a slow

141
00:06:06,180 --> 00:06:07,979
replica may slow down the whole system

142
00:06:07,979 --> 00:06:10,500
and the resource overhead due to

143
00:06:10,500 --> 00:06:14,759
execution replay is at least 100 percent

144
00:06:14,759 --> 00:06:18,300
so why so why like existing uh

145
00:06:18,300 --> 00:06:20,400
replication mechanism have all these

146
00:06:20,400 --> 00:06:22,740
limitations and we believe the root

147
00:06:22,740 --> 00:06:24,419
cause if the coupling between

148
00:06:24,419 --> 00:06:26,880
replication based operations and normal

149
00:06:26,880 --> 00:06:28,080
operations

150
00:06:28,080 --> 00:06:30,300
so specifically for the passive backup

151
00:06:30,300 --> 00:06:33,060
mechanism I have explained the coupling

152
00:06:33,060 --> 00:06:35,520
between checkpointing interval and delay

153
00:06:35,520 --> 00:06:38,340
in releasing outputs furthermore to take

154
00:06:38,340 --> 00:06:40,979
a checkpoint the application has to port

155
00:06:40,979 --> 00:06:43,560
and as a result there is a coupling

156
00:06:43,560 --> 00:06:45,539
between the time to take a checkpoint

157
00:06:45,539 --> 00:06:48,600
and with service interruption

158
00:06:48,600 --> 00:06:51,360
for active backup mechanism there is a

159
00:06:51,360 --> 00:06:52,860
coupling between unchecked

160
00:06:52,860 --> 00:06:55,020
non-determinism and service Interruption

161
00:06:55,020 --> 00:06:57,360
and also the performance on the primary

162
00:06:57,360 --> 00:06:59,400
is coupled with the performance on the

163
00:06:59,400 --> 00:07:00,660
backup

164
00:07:00,660 --> 00:07:04,139
and the key insight for RC to overcome

165
00:07:04,139 --> 00:07:07,199
these limitations is to break all these

166
00:07:07,199 --> 00:07:09,300
existing couplings

167
00:07:09,300 --> 00:07:11,759
so I will next present how do we make

168
00:07:11,759 --> 00:07:14,759
design choices in RC that performs the

169
00:07:14,759 --> 00:07:18,780
decoupling while overcome the limitation

170
00:07:18,780 --> 00:07:22,380
so we start R6 design by choosing a

171
00:07:22,380 --> 00:07:23,699
passive backup

172
00:07:23,699 --> 00:07:26,220
this is because compared to an active

173
00:07:26,220 --> 00:07:28,800
backup mechanism the passive backup has

174
00:07:28,800 --> 00:07:31,680
fundamental advantages in terms of what

175
00:07:31,680 --> 00:07:33,900
avoid vulnerability to not determine

176
00:07:33,900 --> 00:07:36,780
them avoid performance coupling and also

177
00:07:36,780 --> 00:07:39,840
reduce resource overhead

178
00:07:39,840 --> 00:07:43,080
and as as I've just explained a critical

179
00:07:43,080 --> 00:07:45,419
drawback of the passive backup mechanism

180
00:07:45,419 --> 00:07:48,000
if the high latency overhead and the

181
00:07:48,000 --> 00:07:49,919
root cause is the coupling of latency

182
00:07:49,919 --> 00:07:52,860
overhead and checkpointing interval

183
00:07:52,860 --> 00:07:56,580
so rrc performs the decoupling with a

184
00:07:56,580 --> 00:07:58,860
replication scheme that we call hybrid

185
00:07:58,860 --> 00:08:01,020
replication that combine the

186
00:08:01,020 --> 00:08:03,300
checkpointing in a passive backup with

187
00:08:03,300 --> 00:08:05,699
execution Replay in an active backup

188
00:08:05,699 --> 00:08:09,300
with hybrid replication output relief is

189
00:08:09,300 --> 00:08:10,979
thus decoupled from checkpoint

190
00:08:10,979 --> 00:08:12,300
commitment

191
00:08:12,300 --> 00:08:15,000
and this is possible because upon

192
00:08:15,000 --> 00:08:17,520
primary failure the backup can restore

193
00:08:17,520 --> 00:08:19,620
to a state that is consistent with the

194
00:08:19,620 --> 00:08:21,780
client by first restore the last

195
00:08:21,780 --> 00:08:24,720
checkpoint and then replace the primary

196
00:08:24,720 --> 00:08:26,940
execution up to the left relieve the

197
00:08:26,940 --> 00:08:29,960
output to the clients

198
00:08:30,240 --> 00:08:32,458
replication can be done at zero

199
00:08:32,458 --> 00:08:34,679
granularity it can be done at the

200
00:08:34,679 --> 00:08:36,599
Virtual Machine level at the process

201
00:08:36,599 --> 00:08:39,839
level and at the container level

202
00:08:39,839 --> 00:08:42,240
for virtual machine it has this High

203
00:08:42,240 --> 00:08:44,179
runtime overhead and furthermore

204
00:08:44,179 --> 00:08:46,620
applying the hybrid replication to

205
00:08:46,620 --> 00:08:48,240
Virtual Machine would be challenging

206
00:08:48,240 --> 00:08:50,580
because this would tracking this would

207
00:08:50,580 --> 00:08:52,800
require checking non-determinate events

208
00:08:52,800 --> 00:08:54,600
in the OS

209
00:08:54,600 --> 00:08:57,240
well for process it has this typical

210
00:08:57,240 --> 00:08:59,339
problem of naming conflicts upon

211
00:08:59,339 --> 00:09:00,660
failover

212
00:09:00,660 --> 00:09:03,720
and as a result RC trusive to replicate

213
00:09:03,720 --> 00:09:06,120
at the container level which resolves

214
00:09:06,120 --> 00:09:09,720
the limitations of process and VMS

215
00:09:09,720 --> 00:09:11,940
so with all that so here is an overview

216
00:09:11,940 --> 00:09:15,839
of how rrc works so with RC the critical

217
00:09:15,839 --> 00:09:18,180
server application is deployed in a

218
00:09:18,180 --> 00:09:20,399
container in the primary host

219
00:09:20,399 --> 00:09:23,100
and the execution of the application is

220
00:09:23,100 --> 00:09:26,959
similarly divided into multiple airports

221
00:09:26,959 --> 00:09:29,880
during the execution during the execute

222
00:09:29,880 --> 00:09:32,339
stage of one Airport

223
00:09:32,339 --> 00:09:35,580
RC redirects client request to the

224
00:09:35,580 --> 00:09:37,980
backup and is being recorded in the

225
00:09:37,980 --> 00:09:39,060
backup

226
00:09:39,060 --> 00:09:41,820
and then the request is sent to the

227
00:09:41,820 --> 00:09:44,220
application in the primary host

228
00:09:44,220 --> 00:09:47,279
the application executes and generates a

229
00:09:47,279 --> 00:09:50,279
reply to the client it will also be sent

230
00:09:50,279 --> 00:09:54,060
to the backup and being buffered there

231
00:09:54,060 --> 00:09:56,459
the application later sends a

232
00:09:56,459 --> 00:09:58,680
non-determinist event log that can

233
00:09:58,680 --> 00:10:01,380
reproduce this generated reply this will

234
00:10:01,380 --> 00:10:03,600
be sent to the backup and upon the

235
00:10:03,600 --> 00:10:06,779
backup receiving this log it it will it

236
00:10:06,779 --> 00:10:10,459
then relieves the reply to the clients

237
00:10:10,459 --> 00:10:14,519
also at the one at the end of an of each

238
00:10:14,519 --> 00:10:17,760
Epoch a checkpoint is taken and sent to

239
00:10:17,760 --> 00:10:19,620
the backup

240
00:10:19,620 --> 00:10:22,620
so here is how RC handles the primary

241
00:10:22,620 --> 00:10:25,680
failure if the primary fails

242
00:10:25,680 --> 00:10:28,440
uh the backup first restore the

243
00:10:28,440 --> 00:10:30,300
application state from the latest

244
00:10:30,300 --> 00:10:32,100
checkpoint

245
00:10:32,100 --> 00:10:34,140
the backup then sends the

246
00:10:34,140 --> 00:10:36,540
non-determinist event logs and the

247
00:10:36,540 --> 00:10:38,940
recorded inputs from the clients to the

248
00:10:38,940 --> 00:10:40,200
application

249
00:10:40,200 --> 00:10:42,839
the application then perform execution

250
00:10:42,839 --> 00:10:45,839
replay to bring its state consistent

251
00:10:45,839 --> 00:10:48,000
with the failed application state in the

252
00:10:48,000 --> 00:10:50,040
primary

253
00:10:50,040 --> 00:10:52,920
and then this application in the backup

254
00:10:52,920 --> 00:10:56,579
can continue service the clients

255
00:10:56,579 --> 00:10:59,579
since RC redirect the reply and the

256
00:10:59,579 --> 00:11:02,040
request through backup it must also

257
00:11:02,040 --> 00:11:04,560
handle backup failure if the backup

258
00:11:04,560 --> 00:11:08,399
fails the the application in the primary

259
00:11:08,399 --> 00:11:11,100
advertise the service IP address so that

260
00:11:11,100 --> 00:11:13,140
the clients will directly send requests

261
00:11:13,140 --> 00:11:15,779
to the application in the primary host

262
00:11:15,779 --> 00:11:19,500
and the recovery can just be finished by

263
00:11:19,500 --> 00:11:21,000
uh

264
00:11:21,000 --> 00:11:23,399
can just be finished if uh and the

265
00:11:23,399 --> 00:11:25,920
application directly sends the reply to

266
00:11:25,920 --> 00:11:28,320
the clients

267
00:11:28,320 --> 00:11:31,200
so I've just presented the core design

268
00:11:31,200 --> 00:11:34,260
aspect of RC for rrc to function

269
00:11:34,260 --> 00:11:37,800
correctly we need to overcome a list of

270
00:11:37,800 --> 00:11:40,200
the design and implementation challenges

271
00:11:40,200 --> 00:11:42,959
and here is the here's the complete list

272
00:11:42,959 --> 00:11:45,779
of them and due to time limitation I

273
00:11:45,779 --> 00:11:48,600
will just focus on the first two items

274
00:11:48,600 --> 00:11:52,019
so as I have just explained RC performs

275
00:11:52,019 --> 00:11:54,300
periodic checkpointing at the end of

276
00:11:54,300 --> 00:11:56,940
each Epoch and checkpointing requires

277
00:11:56,940 --> 00:11:59,519
saving a consistent speed and as a

278
00:11:59,519 --> 00:12:02,160
result execution must pause during

279
00:12:02,160 --> 00:12:04,440
checkpointing and this leads to service

280
00:12:04,440 --> 00:12:06,360
pause time

281
00:12:06,360 --> 00:12:09,240
since RC chooses to replicate at the

282
00:12:09,240 --> 00:12:12,180
container level and a critical issue

283
00:12:12,180 --> 00:12:14,760
here is that the container has tight fit

284
00:12:14,760 --> 00:12:17,399
coupling with the underlying kernel and

285
00:12:17,399 --> 00:12:19,260
as a result we will need to take a

286
00:12:19,260 --> 00:12:21,779
checkpoint of the container significant

287
00:12:21,779 --> 00:12:23,760
internal container state must be

288
00:12:23,760 --> 00:12:26,540
checkpointed and with the current Linux

289
00:12:26,540 --> 00:12:29,519
retrieving such status flow it takes

290
00:12:29,519 --> 00:12:31,800
thousands of system costs

291
00:12:31,800 --> 00:12:34,560
so as a result checkpointing a container

292
00:12:34,560 --> 00:12:35,640
is slow

293
00:12:35,640 --> 00:12:38,640
so that's a key challenge RC overcomes

294
00:12:38,640 --> 00:12:41,399
is to minimize the pause time despite

295
00:12:41,399 --> 00:12:43,139
slow checkpointing

296
00:12:43,139 --> 00:12:46,500
and RC achieved this by decouple the

297
00:12:46,500 --> 00:12:48,660
retrieval of internal container state

298
00:12:48,660 --> 00:12:51,060
from container execution

299
00:12:51,060 --> 00:12:53,940
and to achieve this RC introduced a new

300
00:12:53,940 --> 00:12:57,660
kernel primitive called container Fork

301
00:12:57,660 --> 00:13:00,480
it container for creates a clone of an

302
00:13:00,480 --> 00:13:02,639
existing container and with the

303
00:13:02,639 --> 00:13:05,279
container Fork when RC needs to take a

304
00:13:05,279 --> 00:13:07,620
checkpoint its first part of the

305
00:13:07,620 --> 00:13:09,300
application container

306
00:13:09,300 --> 00:13:12,240
and then use if the container Fork to

307
00:13:12,240 --> 00:13:14,519
create a shadow container the

308
00:13:14,519 --> 00:13:16,260
application container can then resume

309
00:13:16,260 --> 00:13:19,019
execution well the checkpoint is

310
00:13:19,019 --> 00:13:21,480
obtained from the shadow container

311
00:13:21,480 --> 00:13:24,139
and the container Fork significantly

312
00:13:24,139 --> 00:13:27,060
reduces the service pause time of our

313
00:13:27,060 --> 00:13:31,638
benchmarks by an order of magnitude

314
00:13:31,920 --> 00:13:35,519
another challenge RC map to overcome if

315
00:13:35,519 --> 00:13:37,500
the non-determined events so

316
00:13:37,500 --> 00:13:40,019
specifically since as I just explained

317
00:13:40,019 --> 00:13:43,260
RC is a hybrid replication scheme and is

318
00:13:43,260 --> 00:13:45,660
only require execution replay during

319
00:13:45,660 --> 00:13:49,079
recovery and as a result it is only

320
00:13:49,079 --> 00:13:51,600
vulnerable to non-determining events

321
00:13:51,600 --> 00:13:54,540
occurring during the epoch of failure

322
00:13:54,540 --> 00:13:57,660
and to handle non-determined events RC

323
00:13:57,660 --> 00:13:59,940
requires on the non-determinist event

324
00:13:59,940 --> 00:14:03,839
logs is obtained from the primary

325
00:14:03,839 --> 00:14:05,940
with multi-threading memory access

326
00:14:05,940 --> 00:14:08,639
ordering is non-deterministic and thus

327
00:14:08,639 --> 00:14:10,920
they must be checked so a

328
00:14:10,920 --> 00:14:13,079
straightforward solution is to record

329
00:14:13,079 --> 00:14:16,019
the order of all memory access however

330
00:14:16,019 --> 00:14:19,639
this incurs unacceptably High overhead

331
00:14:19,639 --> 00:14:23,100
due to the frequency of memory access

332
00:14:23,100 --> 00:14:26,579
so instead RC chooses to record the

333
00:14:26,579 --> 00:14:29,880
outcome of synchronization operations it

334
00:14:29,880 --> 00:14:32,339
can guarantee the correct correct replay

335
00:14:32,339 --> 00:14:35,279
as long as the programs do not have data

336
00:14:35,279 --> 00:14:38,459
races and by data races are being memory

337
00:14:38,459 --> 00:14:41,100
accesses that are not synchronized by

338
00:14:41,100 --> 00:14:44,339
synchronization operations and thus RC

339
00:14:44,339 --> 00:14:48,240
need to handle database

340
00:14:48,240 --> 00:14:51,120
so there are several key design data

341
00:14:51,120 --> 00:14:53,459
several key characteristics of data

342
00:14:53,459 --> 00:14:55,740
races that guide RC's handling of them

343
00:14:55,740 --> 00:14:58,920
first of all database is a bugs and

344
00:14:58,920 --> 00:15:01,199
programmers generally try to avoid them

345
00:15:01,199 --> 00:15:04,380
however it is impossible to eliminate

346
00:15:04,380 --> 00:15:06,959
all data releases with languages like

347
00:15:06,959 --> 00:15:09,180
CNC Plus

348
00:15:09,180 --> 00:15:12,060
fortunately existing tools can

349
00:15:12,060 --> 00:15:14,399
effectively detect frequently manifested

350
00:15:14,399 --> 00:15:17,100
databases and for deployed server

351
00:15:17,100 --> 00:15:19,260
applications they generally go through

352
00:15:19,260 --> 00:15:21,480
several round of testing and debugging

353
00:15:21,480 --> 00:15:24,440
and as a result it is expected that

354
00:15:24,440 --> 00:15:27,959
frequently manifested databases are

355
00:15:27,959 --> 00:15:29,880
unlikely to exist

356
00:15:29,880 --> 00:15:32,940
and as a result RC focuses on

357
00:15:32,940 --> 00:15:36,620
infrequently manifested databases

358
00:15:36,620 --> 00:15:39,959
and the way RFC handle data resist is

359
00:15:39,959 --> 00:15:42,240
motivated by a common scenario occurred

360
00:15:42,240 --> 00:15:44,399
during replay which is during the replay

361
00:15:44,399 --> 00:15:47,339
on the backup most of the system costs

362
00:15:47,339 --> 00:15:50,339
are not actually actually executed but

363
00:15:50,339 --> 00:15:53,160
just being replayed and as a result this

364
00:15:53,160 --> 00:15:54,899
will cause a significant different

365
00:15:54,899 --> 00:15:57,480
timing of flood execution

366
00:15:57,480 --> 00:15:59,940
uh in the recruiter and compared to a

367
00:15:59,940 --> 00:16:02,880
replay run and such different timing of

368
00:16:02,880 --> 00:16:05,639
field execution is likely to cause the

369
00:16:05,639 --> 00:16:08,399
different outcomes of databases and as a

370
00:16:08,399 --> 00:16:12,019
result different outcomes of Replay

371
00:16:12,019 --> 00:16:15,839
so the RF is mechanism to mitigate the

372
00:16:15,839 --> 00:16:18,779
impact of databases is first during the

373
00:16:18,779 --> 00:16:21,000
record run it will record the time

374
00:16:21,000 --> 00:16:23,459
intervals between system call Returns on

375
00:16:23,459 --> 00:16:24,660
the primary

376
00:16:24,660 --> 00:16:27,899
and then it and then the recorded time

377
00:16:27,899 --> 00:16:29,699
interval is sent to the backup together

378
00:16:29,699 --> 00:16:32,279
with the non-deterministic logs during

379
00:16:32,279 --> 00:16:35,760
replay the backup involves the uh the

380
00:16:35,760 --> 00:16:37,800
system called time interval and thus

381
00:16:37,800 --> 00:16:40,380
causes a similar timing of execution

382
00:16:40,380 --> 00:16:44,639
compared to a record run and thus a the

383
00:16:44,639 --> 00:16:46,860
same outcome of data races

384
00:16:46,860 --> 00:16:49,620
well this mechanism does not guarantee

385
00:16:49,620 --> 00:16:52,139
correct Replay in the event of data

386
00:16:52,139 --> 00:16:54,720
races we find it pretty effective to

387
00:16:54,720 --> 00:16:57,420
handle real-world databases which are

388
00:16:57,420 --> 00:16:59,940
infrequent specifically with two of our

389
00:16:59,940 --> 00:17:02,220
workloads which has infrequent databases

390
00:17:02,220 --> 00:17:04,980
this mechanism can boost the successful

391
00:17:04,980 --> 00:17:08,400
replay rate to over 99 percent

392
00:17:08,400 --> 00:17:10,740
so with that I'm going to present the

393
00:17:10,740 --> 00:17:13,260
evaluation of RC

394
00:17:13,260 --> 00:17:15,720
so here so we perform extensive

395
00:17:15,720 --> 00:17:18,480
evaluation on RC and due to time

396
00:17:18,480 --> 00:17:20,880
limitations I will just focus on the

397
00:17:20,880 --> 00:17:22,500
first three items

398
00:17:22,500 --> 00:17:25,380
so we we compare RC against the newlycom

399
00:17:25,380 --> 00:17:27,720
which is a state-of-the-art container

400
00:17:27,720 --> 00:17:29,640
replication mechanism based on

401
00:17:29,640 --> 00:17:31,980
checkpointing to a passive backup

402
00:17:31,980 --> 00:17:34,559
and we use server workloads in memory

403
00:17:34,559 --> 00:17:38,640
databases and web servers to evaluate RC

404
00:17:38,640 --> 00:17:41,460
and we configure RC to take a 100

405
00:17:41,460 --> 00:17:44,160
millisecond checkpointing interval

406
00:17:44,160 --> 00:17:46,200
and here is the latency overhead

407
00:17:46,200 --> 00:17:49,860
comparison between rrc and nilikom

408
00:17:49,860 --> 00:17:53,280
and in summary RC achieves a average

409
00:17:53,280 --> 00:17:56,100
latency overhead of less than 300

410
00:17:56,100 --> 00:17:58,740
microseconds well for nearly comp it is

411
00:17:58,740 --> 00:18:00,539
tens of milliseconds

412
00:18:00,539 --> 00:18:02,880
when it comes to the 99th percentile

413
00:18:02,880 --> 00:18:08,160
latency RCS is still below 1 millisecond

414
00:18:08,160 --> 00:18:13,020
so uh so in summary RC achieves two

415
00:18:13,020 --> 00:18:15,179
orders of magnitude lower latency

416
00:18:15,179 --> 00:18:17,520
overhead and the main contributor of

417
00:18:17,520 --> 00:18:20,160
this is the hybrid replication that

418
00:18:20,160 --> 00:18:22,980
reduces the average latency overhead and

419
00:18:22,980 --> 00:18:25,620
container Fork that reduces the 99th

420
00:18:25,620 --> 00:18:28,500
percentile latency overhead

421
00:18:28,500 --> 00:18:30,720
and here is a throughput overhead

422
00:18:30,720 --> 00:18:34,200
comparison between RC and nilikom the

423
00:18:34,200 --> 00:18:37,980
y-axis is workloads and the x-axis if

424
00:18:37,980 --> 00:18:40,919
the latency overhead we divide the

425
00:18:40,919 --> 00:18:42,900
sources of throughput overhead into

426
00:18:42,900 --> 00:18:44,520
three categories

427
00:18:44,520 --> 00:18:46,860
the first one is due to recording

428
00:18:46,860 --> 00:18:49,980
non-determined events and the second one

429
00:18:49,980 --> 00:18:52,200
is due to the container ports for

430
00:18:52,200 --> 00:18:53,460
checkpointing

431
00:18:53,460 --> 00:18:56,160
and finally the third category is due to

432
00:18:56,160 --> 00:18:58,740
copy on right after container Fork to

433
00:18:58,740 --> 00:19:01,080
maintain State consistency and the page

434
00:19:01,080 --> 00:19:03,120
fault that checks 30 pages for

435
00:19:03,120 --> 00:19:05,280
incremental checkpointing

436
00:19:05,280 --> 00:19:09,360
and comparing RC against nilikom RC have

437
00:19:09,360 --> 00:19:11,700
the extra overhead of recording

438
00:19:11,700 --> 00:19:13,500
non-determined events

439
00:19:13,500 --> 00:19:16,679
however thanks to the container Fork RC

440
00:19:16,679 --> 00:19:19,860
has a much smaller pause overhead and

441
00:19:19,860 --> 00:19:22,080
this in return translates to a

442
00:19:22,080 --> 00:19:25,380
significant advantage in terms of the

443
00:19:25,380 --> 00:19:28,440
overall throughput overhead

444
00:19:28,440 --> 00:19:31,380
and we performed thousands of 40

445
00:19:31,380 --> 00:19:33,720
injections and we inject first of

446
00:19:33,720 --> 00:19:37,020
failures to evaluate the success rate of

447
00:19:37,020 --> 00:19:40,500
RC and to stress RC's recovery mechanism

448
00:19:40,500 --> 00:19:43,200
we perform injection into both the

449
00:19:43,200 --> 00:19:45,660
primary and the backup host

450
00:19:45,660 --> 00:19:49,140
and here's the result uh with real world

451
00:19:49,140 --> 00:19:52,260
examples of data races RC can achieve a

452
00:19:52,260 --> 00:19:55,500
greater than 99 recovery rate and RC

453
00:19:55,500 --> 00:19:58,320
achieves a 100 recovery rate if the

454
00:19:58,320 --> 00:20:01,380
applications are free of data races

455
00:20:01,380 --> 00:20:05,220
though to summarize the key goal of rrc

456
00:20:05,220 --> 00:20:07,919
is to provide application transparent

457
00:20:07,919 --> 00:20:10,799
for tolerance for Server applications it

458
00:20:10,799 --> 00:20:12,840
requires support multi-threading and

459
00:20:12,840 --> 00:20:14,700
minimize the latency and throughput

460
00:20:14,700 --> 00:20:16,039
overhead

461
00:20:16,039 --> 00:20:19,080
the key Insight in RC to meet the

462
00:20:19,080 --> 00:20:21,840
disciple is to decouple replication

463
00:20:21,840 --> 00:20:23,940
related operations from normal

464
00:20:23,940 --> 00:20:25,320
operations

465
00:20:25,320 --> 00:20:28,380
so specifically RC introduced the hybrid

466
00:20:28,380 --> 00:20:30,960
replication mechanism that decoupled the

467
00:20:30,960 --> 00:20:33,299
checkpointing interval from delay in

468
00:20:33,299 --> 00:20:36,000
releasing outputs RC introduced the

469
00:20:36,000 --> 00:20:38,640
container Fork mechanism to decouple the

470
00:20:38,640 --> 00:20:40,620
time to take a checkpoint from service

471
00:20:40,620 --> 00:20:44,160
Interruption and RC and uses a passive

472
00:20:44,160 --> 00:20:46,620
backup which has fundamental advantages

473
00:20:46,620 --> 00:20:50,280
to handle unchecked non-determinism and

474
00:20:50,280 --> 00:20:52,860
furthermore RC introduced a timing

475
00:20:52,860 --> 00:20:55,500
adjustment mechanism that can mitigate

476
00:20:55,500 --> 00:20:57,539
the impact of data races

477
00:20:57,539 --> 00:21:00,720
our evaluation without shows that RC can

478
00:21:00,720 --> 00:21:03,360
achieve two orders of magnitude lower

479
00:21:03,360 --> 00:21:06,240
latency overhead and significantly lower

480
00:21:06,240 --> 00:21:08,100
throughput overhead compared to

481
00:21:08,100 --> 00:21:10,200
state-of-the-art container replication

482
00:21:10,200 --> 00:21:13,620
scheme when it comes to recovery rate RC

483
00:21:13,620 --> 00:21:16,260
can achieve greater than 99 recovery

484
00:21:16,260 --> 00:21:18,720
rate with real-world examples of data

485
00:21:18,720 --> 00:21:21,900
races and 100 recovery rate if the

486
00:21:21,900 --> 00:21:24,240
applications are free of databases

487
00:21:24,240 --> 00:21:26,820
so uh with that thank you and I'm happy

488
00:21:26,820 --> 00:21:29,299
to take questions

