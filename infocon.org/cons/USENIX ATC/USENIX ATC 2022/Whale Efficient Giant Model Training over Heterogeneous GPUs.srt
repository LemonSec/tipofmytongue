1
00:00:13,120 --> 00:00:14,480
good afternoon everyone my name is

2
00:00:14,480 --> 00:00:16,720
xiaoyongnu i'm from alibaba pay team

3
00:00:16,720 --> 00:00:18,560
today we're going to talk about where

4
00:00:18,560 --> 00:00:20,320
efficient giant model training over

5
00:00:20,320 --> 00:00:22,400
hydrogen gpus

6
00:00:22,400 --> 00:00:24,800
this work retains many work from my

7
00:00:24,800 --> 00:00:27,680
connectors in any power play team

8
00:00:27,680 --> 00:00:29,920
to achieve a better motor or parallelism

9
00:00:29,920 --> 00:00:32,079
the training of larger large scale deep

10
00:00:32,079 --> 00:00:33,760
learning models has been intensively

11
00:00:33,760 --> 00:00:36,800
adopted the scale of motor parameters

12
00:00:36,800 --> 00:00:40,320
has increased from millions to trillions

13
00:00:40,320 --> 00:00:43,120
so however the scalar motor has come at

14
00:00:43,120 --> 00:00:45,120
a cost of the

15
00:00:45,120 --> 00:00:46,559
engineering effort to efficiently

16
00:00:46,559 --> 00:00:49,760
dispute the models across gpus the com

17
00:00:49,760 --> 00:00:51,840
the com the commonly used the data first

18
00:00:51,840 --> 00:00:53,600
materiality is a profit

19
00:00:53,600 --> 00:00:56,000
since it requires that the motor

20
00:00:56,000 --> 00:00:59,120
replicas in gpus thus easily become the

21
00:00:59,120 --> 00:01:00,640
bottleneck

22
00:01:00,640 --> 00:01:03,280
for the giant motors meanwhile training

23
00:01:03,280 --> 00:01:05,360
trainings or mode or parameters require

24
00:01:05,360 --> 00:01:07,439
a terabyte of gpu memory

25
00:01:07,439 --> 00:01:08,479
as

26
00:01:08,479 --> 00:01:10,880
a minimum which is

27
00:01:10,880 --> 00:01:14,560
far beyond the capacity of a single gpu

28
00:01:14,560 --> 00:01:15,600
however

29
00:01:15,600 --> 00:01:20,158
the scale model has come at a cost

30
00:01:22,080 --> 00:01:22,960
what

31
00:01:22,960 --> 00:01:25,360
okay it's working

32
00:01:25,360 --> 00:01:27,200
uh besides that parallelism

33
00:01:27,200 --> 00:01:30,000
several newer paranormal strategies for

34
00:01:30,000 --> 00:01:31,280
training different learning workers have

35
00:01:31,280 --> 00:01:33,600
been proposed including private

36
00:01:33,600 --> 00:01:36,320
algorithm and the tensor motor pluralism

37
00:01:36,320 --> 00:01:38,079
pipeline parallelism

38
00:01:38,079 --> 00:01:40,320
splits the input minimized into multiple

39
00:01:40,320 --> 00:01:42,240
micro batteries and a pythonized

40
00:01:42,240 --> 00:01:45,040
execution of these micro batches across

41
00:01:45,040 --> 00:01:47,520
multiple diffuse tensor molecularisms

42
00:01:47,520 --> 00:01:50,560
please operators and cancer internally

43
00:01:50,560 --> 00:01:53,200
over multiple gpus

44
00:01:53,200 --> 00:01:55,840
in select a proper productivity for

45
00:01:55,840 --> 00:01:58,960
motor training both motor pro propulsion

46
00:01:58,960 --> 00:02:00,960
and computing results and to be

47
00:02:00,960 --> 00:02:02,240
considered

48
00:02:02,240 --> 00:02:05,119
pure599 parallelism does not scale well

49
00:02:05,119 --> 00:02:07,680
with more gpus because of bubbles

50
00:02:07,680 --> 00:02:09,840
resource synchronization and p2p

51
00:02:09,840 --> 00:02:11,440
communication

52
00:02:11,440 --> 00:02:13,920
a better approach is to find the proper

53
00:02:13,920 --> 00:02:16,400
stage for a model and apply the less

54
00:02:16,400 --> 00:02:18,480
data data fernism for further scanning

55
00:02:18,480 --> 00:02:19,680
of the training

56
00:02:19,680 --> 00:02:22,319
in our experiment a very large trainee

57
00:02:22,319 --> 00:02:23,680
we

58
00:02:23,680 --> 00:02:26,720
we obviously that combined pipeline with

59
00:02:26,720 --> 00:02:29,920
four stages and data partisan achieved

60
00:02:29,920 --> 00:02:32,800
high speed up the pure let's say eight

61
00:02:32,800 --> 00:02:34,720
stage pipeline

62
00:02:34,720 --> 00:02:36,959
moreover different parallel strategies

63
00:02:36,959 --> 00:02:39,040
can also apply to

64
00:02:39,040 --> 00:02:40,480
different motor

65
00:02:40,480 --> 00:02:43,280
partitions for a hybrid here's a large

66
00:02:43,280 --> 00:02:44,720
scale

67
00:02:44,720 --> 00:02:46,560
immediate clarification models with a

68
00:02:46,560 --> 00:02:48,000
hundred

69
00:02:48,000 --> 00:02:50,239
thousand categories in this model the

70
00:02:50,239 --> 00:02:53,599
photocollect and the software max layers

71
00:02:53,599 --> 00:02:56,080
are text another their motor science

72
00:02:56,080 --> 00:02:57,760
compared to the immediate feature

73
00:02:57,760 --> 00:02:59,280
structure

74
00:02:59,280 --> 00:03:00,640
that backbone

75
00:03:00,640 --> 00:03:03,760
uh if data is applied and as a whole

76
00:03:03,760 --> 00:03:05,120
model

77
00:03:05,120 --> 00:03:06,480
or

78
00:03:06,480 --> 00:03:08,560
the the gradient the points that were

79
00:03:08,560 --> 00:03:10,800
fully collected will become

80
00:03:10,800 --> 00:03:13,120
the bottom link one better solution is

81
00:03:13,120 --> 00:03:15,519
to apply therapeutics

82
00:03:15,519 --> 00:03:18,879
to resonator 50 and apply motor organism

83
00:03:18,879 --> 00:03:21,840
to fully collect it as as a result the

84
00:03:21,840 --> 00:03:24,000
synchronization overhead can be reduced

85
00:03:24,000 --> 00:03:24,959
by

86
00:03:24,959 --> 00:03:25,760
90

87
00:03:25,760 --> 00:03:29,840
thereby achieving better performance

88
00:03:29,840 --> 00:03:32,239
training vaginal motor requires huge

89
00:03:32,239 --> 00:03:34,159
computing resources

90
00:03:34,159 --> 00:03:36,560
in industry the scheduling of 100

91
00:03:36,560 --> 00:03:39,200
homogeneous high energy fuels usually

92
00:03:39,200 --> 00:03:40,560
requires

93
00:03:40,560 --> 00:03:43,680
a long queuing time due to the gun

94
00:03:43,680 --> 00:03:44,720
schedule

95
00:03:44,720 --> 00:03:46,640
in such a scenario

96
00:03:46,640 --> 00:03:49,599
uh heterogeneous gpus can be obtained

97
00:03:49,599 --> 00:03:51,440
much easier as a computing port for

98
00:03:51,440 --> 00:03:55,360
example a port of p100 v100 100 or kind

99
00:03:55,360 --> 00:03:57,200
of mixer poor

100
00:03:57,200 --> 00:03:59,439
however deep learning framework

101
00:03:59,439 --> 00:04:01,200
encounter challenges efficiently

102
00:04:01,200 --> 00:04:04,480
utilization heterogeneous resources uh

103
00:04:04,480 --> 00:04:06,799
different types of gpus

104
00:04:06,799 --> 00:04:09,120
uh you know different types of gp memory

105
00:04:09,120 --> 00:04:12,400
capacity and gpu computing capacity

106
00:04:12,400 --> 00:04:16,079
which like uh lately introduce imbalance

107
00:04:16,079 --> 00:04:16,959
in

108
00:04:16,959 --> 00:04:18,798
computation graph partition and deep

109
00:04:18,798 --> 00:04:21,759
learning operator location

110
00:04:21,759 --> 00:04:24,320
uh we summarize the gap in general model

111
00:04:24,320 --> 00:04:26,639
training using existing approaches

112
00:04:26,639 --> 00:04:28,479
firstly the framework

113
00:04:28,479 --> 00:04:31,199
make a unifiner abstraction to support

114
00:04:31,199 --> 00:04:33,120
all of the perennial strategies and the

115
00:04:33,120 --> 00:04:35,759
hybrids secondly for the automatic

116
00:04:35,759 --> 00:04:37,840
parallel strategies searching is time

117
00:04:37,840 --> 00:04:40,880
consuming for giant motors the solar net

118
00:04:40,880 --> 00:04:42,880
is inefficient when training with

119
00:04:42,880 --> 00:04:44,720
heterogeneous gpus

120
00:04:44,720 --> 00:04:47,840
motor developers can hardly consider all

121
00:04:47,840 --> 00:04:50,639
resources issued when programming find

122
00:04:50,639 --> 00:04:52,400
any current approach

123
00:04:52,400 --> 00:04:56,000
couples motor code to a specific mode or

124
00:04:56,000 --> 00:04:58,080
parallel strategies which requires

125
00:04:58,080 --> 00:05:00,080
significant code refactoring when

126
00:05:00,080 --> 00:05:01,199
applying

127
00:05:01,199 --> 00:05:03,199
the strategy to a new model or switch

128
00:05:03,199 --> 00:05:04,320
between

129
00:05:04,320 --> 00:05:07,360
the parallel strategies

130
00:05:08,320 --> 00:05:10,320
these gaps also bring the opportunities

131
00:05:10,320 --> 00:05:12,479
to build a better training frameworks

132
00:05:12,479 --> 00:05:14,320
for general models with multiple

133
00:05:14,320 --> 00:05:17,680
personal capacity we start with define a

134
00:05:17,680 --> 00:05:20,639
unifying program language for strategy

135
00:05:20,639 --> 00:05:23,600
expression after that the framework is

136
00:05:23,600 --> 00:05:26,240
enhanced to utilize the kinks with the

137
00:05:26,240 --> 00:05:28,960
best possible resource utilization

138
00:05:28,960 --> 00:05:31,199
geoduck heterogeneous

139
00:05:31,199 --> 00:05:33,759
gpu parallel charges should be used

140
00:05:33,759 --> 00:05:37,039
adaptively and dynamically finally user

141
00:05:37,039 --> 00:05:37,840
can

142
00:05:37,840 --> 00:05:40,160
implement parallel strategies with

143
00:05:40,160 --> 00:05:42,560
minimal code change and switch among

144
00:05:42,560 --> 00:05:45,440
genetically easily

145
00:05:45,759 --> 00:05:47,520
we propose well a deep learning

146
00:05:47,520 --> 00:05:50,320
framework designed for changing motors

147
00:05:50,320 --> 00:05:51,520
with high

148
00:05:51,520 --> 00:05:53,919
performance for uh for careful

149
00:05:53,919 --> 00:05:55,840
dependency user effort

150
00:05:55,840 --> 00:05:58,400
and the distributed graph optimization

151
00:05:58,400 --> 00:06:00,560
requirement we introduced two new high

152
00:06:00,560 --> 00:06:03,600
level primitives to express all existing

153
00:06:03,600 --> 00:06:05,680
uh parallel strategies as well as their

154
00:06:05,680 --> 00:06:08,720
hybrids by utilizing the annotation for

155
00:06:08,720 --> 00:06:10,560
graph optimization

156
00:06:10,560 --> 00:06:12,800
we can transform local models into

157
00:06:12,800 --> 00:06:15,199
distributed models and change them on

158
00:06:15,199 --> 00:06:17,520
multiple gpus efficiently and

159
00:06:17,520 --> 00:06:18,800
automatically

160
00:06:18,800 --> 00:06:21,199
we'll propose a hardware awareness

161
00:06:21,199 --> 00:06:23,600
balancing algorithm which is seamlessly

162
00:06:23,600 --> 00:06:25,759
integrated with the parallel strategies

163
00:06:25,759 --> 00:06:27,199
to accelerate

164
00:06:27,199 --> 00:06:29,360
training on heterogeneous gpus

165
00:06:29,360 --> 00:06:31,600
we are demonstrating its capacity by

166
00:06:31,600 --> 00:06:33,440
setting a new milestone in training the

167
00:06:33,440 --> 00:06:37,039
largest modernity pre-trained model m6

168
00:06:37,039 --> 00:06:40,880
with 10 trillion motor parameters

169
00:06:40,880 --> 00:06:42,720
to inevitably well i will start with a

170
00:06:42,720 --> 00:06:45,759
key objection and parallel primitives

171
00:06:45,759 --> 00:06:47,360
then i would describe a paranormal

172
00:06:47,360 --> 00:06:49,600
planner that transform a local model

173
00:06:49,600 --> 00:06:52,479
with the perm next parallel primitives

174
00:06:52,479 --> 00:06:54,639
into a distributed model

175
00:06:54,639 --> 00:06:56,400
finally i will introduce the hardware

176
00:06:56,400 --> 00:06:59,199
aware node balance algorithm to speed up

177
00:06:59,199 --> 00:07:02,080
the training with the http the gpu

178
00:07:02,080 --> 00:07:04,400
clusters

179
00:07:04,400 --> 00:07:06,400
namely increases the two internal

180
00:07:06,400 --> 00:07:08,479
concepts here a task graph and a virtual

181
00:07:08,479 --> 00:07:09,599
device

182
00:07:09,599 --> 00:07:12,240
task graphs is a subset of the model for

183
00:07:12,240 --> 00:07:14,800
parallel transformation execution one

184
00:07:14,800 --> 00:07:16,880
model can have one or more

185
00:07:16,880 --> 00:07:19,120
non-overlapping touch graphs we can

186
00:07:19,120 --> 00:07:21,199
apply parallel strategies to each test

187
00:07:21,199 --> 00:07:22,479
graph

188
00:07:22,479 --> 00:07:24,560
by modernizing mode of operations into

189
00:07:24,560 --> 00:07:26,960
test graphs we can apply different

190
00:07:26,960 --> 00:07:29,599
subnet strategies to different motor

191
00:07:29,599 --> 00:07:31,680
parts as well as scheduling the

192
00:07:31,680 --> 00:07:33,440
execution of photographs

193
00:07:33,440 --> 00:07:35,759
in a pipeline a task graph can be

194
00:07:35,759 --> 00:07:38,560
further replicated or partitioned

195
00:07:38,560 --> 00:07:40,080
virtual device is a logical

196
00:07:40,080 --> 00:07:42,880
representation of the computing resource

197
00:07:42,880 --> 00:07:45,360
with one virtual device have one or more

198
00:07:45,360 --> 00:07:47,840
physical devices virtual device hides

199
00:07:47,840 --> 00:07:50,400
the complexity of

200
00:07:50,400 --> 00:07:51,759
let's say

201
00:07:51,759 --> 00:07:53,280
device topology

202
00:07:53,280 --> 00:07:56,160
computing capacity as well as device

203
00:07:56,160 --> 00:07:58,400
placement from the user

204
00:07:58,400 --> 00:08:01,199
one virtual device assigned to one task

205
00:08:01,199 --> 00:08:02,160
graph

206
00:08:02,160 --> 00:08:04,160
different universal device allowed to

207
00:08:04,160 --> 00:08:06,400
have different or the same physical

208
00:08:06,400 --> 00:08:09,039
devices

209
00:08:09,840 --> 00:08:11,680
the prior operative is a part and let's

210
00:08:11,680 --> 00:08:14,000
say the poisson context manager where

211
00:08:14,000 --> 00:08:16,400
operations define the under each

212
00:08:16,400 --> 00:08:18,160
modulized as one

213
00:08:18,160 --> 00:08:19,599
task graph

214
00:08:19,599 --> 00:08:22,080
will allow users to suggest parallel

215
00:08:22,080 --> 00:08:25,199
strategies with two unifying primitives

216
00:08:25,199 --> 00:08:27,280
replicate and split

217
00:08:27,280 --> 00:08:28,960
replicate a lot

218
00:08:28,960 --> 00:08:31,759
a rotate a test graph to be replicated

219
00:08:31,759 --> 00:08:33,679
device count is the number of devices

220
00:08:33,679 --> 00:08:36,159
used to compute a test graph

221
00:08:36,159 --> 00:08:38,320
or replicas split

222
00:08:38,320 --> 00:08:40,719
rotate the test graph to apply injured

223
00:08:40,719 --> 00:08:43,360
tensor sharding the device count here

224
00:08:43,360 --> 00:08:44,480
denotes

225
00:08:44,480 --> 00:08:48,160
the number of partitions to be sharded

226
00:08:48,160 --> 00:08:50,640
each sharded partition is placed on one

227
00:08:50,640 --> 00:08:51,760
device

228
00:08:51,760 --> 00:08:53,760
to implement this the auto parallel

229
00:08:53,760 --> 00:08:56,399
mechanism is encoded to simplify the

230
00:08:56,399 --> 00:08:59,040
user experience

231
00:08:59,040 --> 00:09:01,920
the first example here shows a pipeline

232
00:09:01,920 --> 00:09:04,640
parallelism with two task graphs

233
00:09:04,640 --> 00:09:06,720
which with each test graph they have

234
00:09:06,720 --> 00:09:08,560
configured with one device

235
00:09:08,560 --> 00:09:11,360
the pipeline parallelism is enhanced by

236
00:09:11,360 --> 00:09:13,519
configuring pipeline

237
00:09:13,519 --> 00:09:15,839
num non-micro batches

238
00:09:15,839 --> 00:09:18,000
the total device number of the two task

239
00:09:18,000 --> 00:09:20,399
graphs is summary to two if the

240
00:09:20,399 --> 00:09:22,800
available device nested number is eight

241
00:09:22,800 --> 00:09:25,600
which is four times of the total device

242
00:09:25,600 --> 00:09:28,640
number where we apply four degree data

243
00:09:28,640 --> 00:09:31,760
partition beyond the pipeline

244
00:09:31,760 --> 00:09:34,320
the second example shows a hybrid

245
00:09:34,320 --> 00:09:37,120
strategies that replicate less than 50

246
00:09:37,120 --> 00:09:40,399
features part with splitting the

247
00:09:40,399 --> 00:09:43,680
classification motor part

248
00:09:44,480 --> 00:09:46,880
the parallel planar is responsible for

249
00:09:46,880 --> 00:09:48,399
producing

250
00:09:48,399 --> 00:09:49,519
efficient

251
00:09:49,519 --> 00:09:51,920
parallel execution plan which is the

252
00:09:51,920 --> 00:09:53,920
core of the well runtime

253
00:09:53,920 --> 00:09:56,720
the parallel planner takes a local model

254
00:09:56,720 --> 00:09:59,360
with optional user annotation computing

255
00:09:59,360 --> 00:10:00,560
resource

256
00:10:00,560 --> 00:10:04,240
and option or configures as input the

257
00:10:04,240 --> 00:10:06,080
virtual device are

258
00:10:06,080 --> 00:10:08,080
generated given

259
00:10:08,080 --> 00:10:10,800
given computing resources and optional

260
00:10:10,800 --> 00:10:13,120
or rotation automatically

261
00:10:13,120 --> 00:10:16,320
after that the model is partitioned into

262
00:10:16,320 --> 00:10:18,399
task graphs and the task graph is

263
00:10:18,399 --> 00:10:21,360
further partitioned internally if split

264
00:10:21,360 --> 00:10:23,760
is let's say rotated

265
00:10:23,760 --> 00:10:25,440
since we are

266
00:10:25,440 --> 00:10:27,040
applying different

267
00:10:27,040 --> 00:10:29,519
strategies to different task graphs

268
00:10:29,519 --> 00:10:31,519
there may exist a

269
00:10:31,519 --> 00:10:33,440
input output mismatch

270
00:10:33,440 --> 00:10:37,440
test graph so in such a case the planner

271
00:10:37,440 --> 00:10:39,120
were trying to

272
00:10:39,120 --> 00:10:41,839
insert the corresponding bridge layers

273
00:10:41,839 --> 00:10:44,079
automatically between the two task

274
00:10:44,079 --> 00:10:46,160
graphs

275
00:10:46,160 --> 00:10:48,800
here the bridge layer is gathered

276
00:10:48,800 --> 00:10:51,440
distributed tensors and then feeds them

277
00:10:51,440 --> 00:10:54,480
into next task graph

278
00:10:55,519 --> 00:10:58,000
here which is where how we utilize the

279
00:10:58,000 --> 00:11:01,200
hardware to balance the working order

280
00:11:01,200 --> 00:11:02,800
amount of graphs

281
00:11:02,800 --> 00:11:05,040
which achieve the high performance

282
00:11:05,040 --> 00:11:07,519
even in hydrology gpu clusters in

283
00:11:07,519 --> 00:11:08,560
general

284
00:11:08,560 --> 00:11:10,959
we are trying to balance the computing

285
00:11:10,959 --> 00:11:12,720
port

286
00:11:12,720 --> 00:11:14,320
proportional to the device computing

287
00:11:14,320 --> 00:11:16,240
capacity subject to the memory

288
00:11:16,240 --> 00:11:17,440
constraint

289
00:11:17,440 --> 00:11:19,279
for data planism

290
00:11:19,279 --> 00:11:21,839
the uh we balance the flops adjust no

291
00:11:21,839 --> 00:11:23,440
core batteries with keeping the mini

292
00:11:23,440 --> 00:11:25,680
batch unchanged

293
00:11:25,680 --> 00:11:28,240
for tensor motor partisan with balance

294
00:11:28,240 --> 00:11:29,440
the flops

295
00:11:29,440 --> 00:11:31,040
or partitional operations through on

296
00:11:31,040 --> 00:11:33,760
human sharding

297
00:11:34,640 --> 00:11:37,600
the memory constraint

298
00:11:37,600 --> 00:11:39,680
algorithm can be achieved by

299
00:11:39,680 --> 00:11:42,320
sorting and reordering the device in

300
00:11:42,320 --> 00:11:44,240
corresponding virtual device by memory

301
00:11:44,240 --> 00:11:46,720
capacity from higher to no

302
00:11:46,720 --> 00:11:49,040
it's a greedy planar algorithm consider

303
00:11:49,040 --> 00:11:50,959
low current

304
00:11:50,959 --> 00:11:54,720
load ratios basically memory and a flop

305
00:11:54,720 --> 00:11:56,880
flop utilization

306
00:11:56,880 --> 00:11:59,200
there might be a case where

307
00:11:59,200 --> 00:12:01,839
later stage contains large layers for

308
00:12:01,839 --> 00:12:04,959
example not sparse invading which can be

309
00:12:04,959 --> 00:12:07,279
addressed in the algorithm or out of

310
00:12:07,279 --> 00:12:10,680
memory errors

311
00:12:10,720 --> 00:12:12,480
where multiple telegraphs are less

312
00:12:12,480 --> 00:12:14,240
excluded in a pipeline which will

313
00:12:14,240 --> 00:12:16,399
balance the interplay graph workload or

314
00:12:16,399 --> 00:12:18,800
heterogeneous gpus

315
00:12:18,800 --> 00:12:20,560
pipeline parallelism achieves efficient

316
00:12:20,560 --> 00:12:22,480
execution by internet forward back

317
00:12:22,480 --> 00:12:24,639
execution multiple

318
00:12:24,639 --> 00:12:26,079
micro batteries

319
00:12:26,079 --> 00:12:29,360
only a task graph leads to catch more

320
00:12:29,360 --> 00:12:32,240
activations then later

321
00:12:32,240 --> 00:12:34,079
then later back

322
00:12:34,079 --> 00:12:35,680
natural telegraphs

323
00:12:35,680 --> 00:12:37,920
thus only a task grab usually consume

324
00:12:37,920 --> 00:12:42,000
more gpus memory their native telegraphs

325
00:12:42,000 --> 00:12:44,560
this is shown in the right

326
00:12:44,560 --> 00:12:46,959
finger the different memory requirement

327
00:12:46,959 --> 00:12:50,000
of test graphs motivate us to

328
00:12:50,000 --> 00:12:52,880
place earlier task graphs on device with

329
00:12:52,880 --> 00:12:55,200
a higher memory capacity

330
00:12:55,200 --> 00:12:57,279
after reordering the virtual device

331
00:12:57,279 --> 00:13:00,720
account into the memory requirement

332
00:13:00,720 --> 00:13:03,120
we partitioned the motor

333
00:13:03,120 --> 00:13:06,399
operations to attach graphs in a

334
00:13:06,399 --> 00:13:08,639
topological sort and then balance the

335
00:13:08,639 --> 00:13:11,360
computation flops among operations

336
00:13:11,360 --> 00:13:14,160
proportional to device capacity subject

337
00:13:14,160 --> 00:13:15,920
to the memory bound of the memory

338
00:13:15,920 --> 00:13:19,800
capacity of each devices

339
00:13:20,000 --> 00:13:21,760
let's we will show you some evaluation

340
00:13:21,760 --> 00:13:25,519
results and conclude our work

341
00:13:27,040 --> 00:13:28,800
so basically we first demonstrated that

342
00:13:28,800 --> 00:13:30,800
where is the efficient single parallel

343
00:13:30,800 --> 00:13:33,040
strategies by comparing with tensorflow

344
00:13:33,040 --> 00:13:34,959
as a estimator dp and the g-pipe

345
00:13:34,959 --> 00:13:36,240
pipelines

346
00:13:36,240 --> 00:13:39,360
we evaluate where dp by competing with

347
00:13:39,360 --> 00:13:41,440
the tensorflow assimilator dp using a

348
00:13:41,440 --> 00:13:44,399
button large and a resonator 50.

349
00:13:44,399 --> 00:13:46,720
here what uh here will be consistent

350
00:13:46,720 --> 00:13:49,200
obtained the better speed up

351
00:13:49,200 --> 00:13:51,680
and the higher gpu organization than

352
00:13:51,680 --> 00:13:54,079
tensorflow estimator dp

353
00:13:54,079 --> 00:13:56,720
such things could be attributed to wells

354
00:13:56,720 --> 00:14:00,000
computational optimization technologies

355
00:14:00,000 --> 00:14:02,880
such as hierarchy or

356
00:14:02,880 --> 00:14:04,639
grouped or reduced

357
00:14:04,639 --> 00:14:06,880
we then evaluate the efficiency of where

358
00:14:06,880 --> 00:14:08,639
pipeline parallelism by comparing with

359
00:14:08,639 --> 00:14:09,920
the g pipe

360
00:14:09,920 --> 00:14:11,920
as shown in the right finger

361
00:14:11,920 --> 00:14:14,399
the chaining slope will speed up where

362
00:14:14,399 --> 00:14:16,000
out front hype

363
00:14:16,000 --> 00:14:18,320
in both fourth stage and

364
00:14:18,320 --> 00:14:19,959
add stage by

365
00:14:19,959 --> 00:14:23,199
1.45 x and 1.14

366
00:14:23,199 --> 00:14:25,199
x respectively

367
00:14:25,199 --> 00:14:28,399
we attribute the performance gear to the

368
00:14:28,399 --> 00:14:31,279
use of alternating forward backward

369
00:14:31,279 --> 00:14:33,199
schedule policy

370
00:14:33,199 --> 00:14:37,120
which improves the gpu utilization

371
00:14:38,240 --> 00:14:41,120
with the invented hybrid strategies

372
00:14:41,120 --> 00:14:44,000
we first apply a lasting pipeline with

373
00:14:44,000 --> 00:14:46,800
dp to the very large models with v100

374
00:14:46,800 --> 00:14:48,000
gpus

375
00:14:48,000 --> 00:14:50,560
the motor is partitioned into two four

376
00:14:50,560 --> 00:14:53,040
and n number of test graphs the left

377
00:14:53,040 --> 00:14:54,720
finger shows that

378
00:14:54,720 --> 00:14:57,040
pipelines with two test graphs and the

379
00:14:57,040 --> 00:14:59,920
four task graphs get a similar

380
00:14:59,920 --> 00:15:02,480
training speed up on gpu utilization

381
00:15:02,480 --> 00:15:03,519
however

382
00:15:03,519 --> 00:15:06,560
we observed a performance scope on eight

383
00:15:06,560 --> 00:15:09,440
task graphs at a lower gpu organization

384
00:15:09,440 --> 00:15:11,760
compared to two and four test graphs

385
00:15:11,760 --> 00:15:15,360
this is because eight task graphs not to

386
00:15:15,360 --> 00:15:17,279
be a relatively lower

387
00:15:17,279 --> 00:15:20,480
module operations in each test graph and

388
00:15:20,480 --> 00:15:23,760
the gpu utilization computation is not

389
00:15:23,760 --> 00:15:24,720
enough

390
00:15:24,720 --> 00:15:28,399
to over yep that is the inter task graph

391
00:15:28,399 --> 00:15:30,160
communication

392
00:15:30,160 --> 00:15:33,199
resulting in poor performance

393
00:15:33,199 --> 00:15:35,600
next we evaluate the

394
00:15:35,600 --> 00:15:38,480
combination of hybrid strategies

395
00:15:38,480 --> 00:15:39,440
on a

396
00:15:39,440 --> 00:15:42,480
large scale image clarification model we

397
00:15:42,480 --> 00:15:44,800
perform experiment on clarification

398
00:15:44,800 --> 00:15:46,160
number one

399
00:15:46,160 --> 00:15:49,120
one hundred thousand and one million

400
00:15:49,120 --> 00:15:52,320
on different number of real hundred gpus

401
00:15:52,320 --> 00:15:54,320
to reduce the communication overhead of

402
00:15:54,320 --> 00:15:56,560
a hybrid parallelism we mix the resonant

403
00:15:56,560 --> 00:15:58,959
50

404
00:15:59,120 --> 00:16:00,959
replicator with a fully collected

405
00:16:00,959 --> 00:16:03,839
partition we'll compare the high hybrid

406
00:16:03,839 --> 00:16:05,600
result of 100

407
00:16:05,600 --> 00:16:07,440
000 classes with the data

408
00:16:07,440 --> 00:16:08,800
as shown in the

409
00:16:08,800 --> 00:16:11,680
middle finger hybrid parallelism

410
00:16:11,680 --> 00:16:14,639
outperformed the data pattern by 1.2 for

411
00:16:14,639 --> 00:16:19,600
80 gpus 1.7x for 16 gpus at a 2.4

412
00:16:19,600 --> 00:16:22,880
x for 32 gpus for chinese throughput to

413
00:16:22,880 --> 00:16:24,000
speed up

414
00:16:24,000 --> 00:16:25,920
where the number of workers increased

415
00:16:25,920 --> 00:16:27,600
hybrid patternism

416
00:16:27,600 --> 00:16:30,399
materials and near linear kind of speed

417
00:16:30,399 --> 00:16:32,959
up while the dp strategies for

418
00:16:32,959 --> 00:16:35,279
dramatically beyond the 8th

419
00:16:35,279 --> 00:16:38,959
i think six or six groups or six workers

420
00:16:38,959 --> 00:16:41,360
this is because the heavy follicle layer

421
00:16:41,360 --> 00:16:43,680
includes a huge granular synchronization

422
00:16:43,680 --> 00:16:45,519
overhead

423
00:16:45,519 --> 00:16:48,320
for the task of one million classes data

424
00:16:48,320 --> 00:16:50,720
preferred due to auto memory

425
00:16:50,720 --> 00:16:54,079
with hybrid parallelism

426
00:16:54,560 --> 00:16:57,519
aware allows for the training of image

427
00:16:57,519 --> 00:16:59,920
classification tasks with one

428
00:16:59,920 --> 00:17:00,959
million

429
00:17:00,959 --> 00:17:03,040
one million classes

430
00:17:03,040 --> 00:17:04,799
the right finger shows that the

431
00:17:04,799 --> 00:17:07,039
performance of hybrid partisan the

432
00:17:07,039 --> 00:17:09,439
training performance throughput from 80

433
00:17:09,439 --> 00:17:11,599
fuel to 32 gpus

434
00:17:11,599 --> 00:17:16,079
achieved a 95 cents gaining efficiency

435
00:17:16,640 --> 00:17:19,439
in terms of hardware aware load balance

436
00:17:19,439 --> 00:17:22,720
result for data partisan

437
00:17:22,720 --> 00:17:24,640
we invented three typical borders

438
00:17:24,640 --> 00:17:28,079
including resonator 50 bernard and gmt

439
00:17:28,079 --> 00:17:29,840
the experiments are

440
00:17:29,840 --> 00:17:31,440
conducted on

441
00:17:31,440 --> 00:17:34,720
heterogeneous gpus that consider

442
00:17:34,720 --> 00:17:37,360
80 32 gig of v100 and

443
00:17:37,360 --> 00:17:38,720
8

444
00:17:38,720 --> 00:17:40,960
16 gig p100

445
00:17:40,960 --> 00:17:42,960
we see the same batch size for all

446
00:17:42,960 --> 00:17:45,120
motors

447
00:17:45,120 --> 00:17:46,400
as the best nine

448
00:17:46,400 --> 00:17:48,240
we then apply the hardware aware

449
00:17:48,240 --> 00:17:50,400
algorithm to each model and get the

450
00:17:50,400 --> 00:17:53,200
speed up compared with the best nine

451
00:17:53,200 --> 00:17:54,320
performance

452
00:17:54,320 --> 00:17:56,559
we are outperforming the best nine in

453
00:17:56,559 --> 00:18:00,480
all three models by a factor of 1.6

454
00:18:00,480 --> 00:18:03,440
and to 1.4x

455
00:18:03,440 --> 00:18:05,520
for pipeline parallelism we've entered

456
00:18:05,520 --> 00:18:08,240
two models including very large and the

457
00:18:08,240 --> 00:18:10,320
t5 large

458
00:18:10,320 --> 00:18:11,679
the change in the performance on

459
00:18:11,679 --> 00:18:14,080
hydrogen gpus that are considered for

460
00:18:14,080 --> 00:18:16,880
32gb 104

461
00:18:16,880 --> 00:18:20,880
160 gig p100 both bernard and the t5

462
00:18:20,880 --> 00:18:24,240
large are partitioned into

463
00:18:24,240 --> 00:18:26,640
four stages we first apply a list the

464
00:18:26,640 --> 00:18:29,919
data partition to pipeline we said that

465
00:18:29,919 --> 00:18:32,799
even in partition models as a baseline

466
00:18:32,799 --> 00:18:34,480
we conduct the training with the

467
00:18:34,480 --> 00:18:36,559
hardware aware policies and they get

468
00:18:36,559 --> 00:18:37,600
about

469
00:18:37,600 --> 00:18:38,400
20

470
00:18:38,400 --> 00:18:42,360
speed up on both models

471
00:18:42,400 --> 00:18:44,480
finally we use wheel to change the

472
00:18:44,480 --> 00:18:46,720
industry scale

473
00:18:46,720 --> 00:18:48,720
china motors m6 the chinese

474
00:18:48,720 --> 00:18:51,360
multimodality model

475
00:18:51,360 --> 00:18:55,520
we paralyzed the training of m610 linear

476
00:18:55,520 --> 00:18:59,120
models with a hybrid operation

477
00:18:59,120 --> 00:19:00,640
strategies by listing the primary

478
00:19:00,640 --> 00:19:02,880
permission and the data prioritism

479
00:19:02,880 --> 00:19:05,200
uh when scanning the computing nodes

480
00:19:05,200 --> 00:19:07,039
from 8 to

481
00:19:07,039 --> 00:19:09,360
205.6

482
00:19:09,360 --> 00:19:12,720
achieve the 911 sustainable uh where can

483
00:19:12,720 --> 00:19:15,280
easily a scale to a local m6 model to a

484
00:19:15,280 --> 00:19:18,000
distributed one by only adding a few

485
00:19:18,000 --> 00:19:20,000
lines on top of the modular definition

486
00:19:20,000 --> 00:19:22,559
as shown in the top left

487
00:19:22,559 --> 00:19:25,840
figure in bottom figure we are let's say

488
00:19:25,840 --> 00:19:27,440
scales the most

489
00:19:27,440 --> 00:19:29,919
parameter to one trinia by switching the

490
00:19:29,919 --> 00:19:32,880
hybrid of data planism and the tensor

491
00:19:32,880 --> 00:19:35,039
modal paradigm with only

492
00:19:35,039 --> 00:19:38,000
a small number of lines of code change

493
00:19:38,000 --> 00:19:40,880
at the end we'll train 10 trinia

494
00:19:40,880 --> 00:19:44,559
parameters by adopting uh adopting

495
00:19:44,559 --> 00:19:46,960
optimize the cancer offloading

496
00:19:46,960 --> 00:19:48,320
strategies

497
00:19:48,320 --> 00:19:51,120
with 512

498
00:19:51,120 --> 00:19:54,719
nvidia v100 gpus

499
00:19:54,880 --> 00:19:57,200
in conclusion we present where

500
00:19:57,200 --> 00:19:59,440
uh where demonstrated

501
00:19:59,440 --> 00:20:01,520
the possibility of achieving efficiency

502
00:20:01,520 --> 00:20:04,240
programmability and adaptivity

503
00:20:04,240 --> 00:20:06,000
in a scalable deep learning training

504
00:20:06,000 --> 00:20:09,840
framework for training trinium parameter

505
00:20:09,840 --> 00:20:11,840
models where spots

506
00:20:11,840 --> 00:20:13,600
various parallel justices using a

507
00:20:13,600 --> 00:20:15,679
unifying objection

508
00:20:15,679 --> 00:20:17,280
it hides the

509
00:20:17,280 --> 00:20:19,600
distributed execution details through

510
00:20:19,600 --> 00:20:21,039
new pipeline

511
00:20:21,039 --> 00:20:22,160
pipeline

512
00:20:22,160 --> 00:20:24,720
rotation and adapts

513
00:20:24,720 --> 00:20:27,360
to heterogeneous gpus with automatic

514
00:20:27,360 --> 00:20:29,360
graph optimization

515
00:20:29,360 --> 00:20:31,679
the code of wear is not open source in

516
00:20:31,679 --> 00:20:34,480
github you can also try and try it out

517
00:20:34,480 --> 00:20:37,520
on pdrc a cloud lady of deep learning

518
00:20:37,520 --> 00:20:41,440
training position on alibaba cloud

519
00:20:41,440 --> 00:20:43,360
so okay that's all for my talk thanks

520
00:20:43,360 --> 00:20:47,240
for your time and questions

521
00:20:52,880 --> 00:20:54,960
you

