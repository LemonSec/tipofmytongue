1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:13,080 --> 00:00:15,240
I'm going to be presenting cash to

3
00:00:15,240 --> 00:00:17,279
Amazon data processing as a service this

4
00:00:17,279 --> 00:00:19,560
is an open source project that we have

5
00:00:19,560 --> 00:00:21,060
done at dth Zurich partling

6
00:00:21,060 --> 00:00:23,340
collaboration with Google

7
00:00:23,340 --> 00:00:24,600
now before I actually start beginning

8
00:00:24,600 --> 00:00:26,519
talking to talk about cashew I want to

9
00:00:26,519 --> 00:00:28,740
take a step back and look a bit at what

10
00:00:28,740 --> 00:00:30,480
data pre-processing in my workloads

11
00:00:30,480 --> 00:00:33,300
traditionally looks like your data is

12
00:00:33,300 --> 00:00:35,700
generally stored in a local disk or an

13
00:00:35,700 --> 00:00:37,620
object storage and then your input

14
00:00:37,620 --> 00:00:39,719
pipeline comes in and applies an ETL

15
00:00:39,719 --> 00:00:42,660
like process on it that runs on the CPU

16
00:00:42,660 --> 00:00:44,640
this is an iterative process that

17
00:00:44,640 --> 00:00:46,559
produces batches which are then

18
00:00:46,559 --> 00:00:49,500
ultimately given to your model which

19
00:00:49,500 --> 00:00:51,660
trains on an accelerator such as a GPR

20
00:00:51,660 --> 00:00:53,039
TPU

21
00:00:53,039 --> 00:00:54,719
now our work focuses on the input

22
00:00:54,719 --> 00:00:57,000
Pipeline and what this looks like well

23
00:00:57,000 --> 00:00:59,640
this can vary a lot a toilet example

24
00:00:59,640 --> 00:01:01,800
here is you have some binary data that

25
00:01:01,800 --> 00:01:03,660
you deserialize you obtain some images

26
00:01:03,660 --> 00:01:05,280
apply random Transformations and then

27
00:01:05,280 --> 00:01:07,380
obtain batches to this now as you can

28
00:01:07,380 --> 00:01:09,299
see uh this is some monolithic design

29
00:01:09,299 --> 00:01:11,040
both the input Pipeline and the model

30
00:01:11,040 --> 00:01:13,439
run on the same machine and this is

31
00:01:13,439 --> 00:01:15,540
problematic because you cannot scale the

32
00:01:15,540 --> 00:01:17,460
two components independently

33
00:01:17,460 --> 00:01:19,619
because models train on very fast

34
00:01:19,619 --> 00:01:21,420
accelerators whilst input pipeline

35
00:01:21,420 --> 00:01:22,979
usually run on quite slow CPUs

36
00:01:22,979 --> 00:01:24,840
oftentimes your input pipeline will be

37
00:01:24,840 --> 00:01:26,640
slower than the model and you cannot

38
00:01:26,640 --> 00:01:29,340
really do much about that so this brings

39
00:01:29,340 --> 00:01:32,340
us to some input reprocessing challenges

40
00:01:32,340 --> 00:01:36,119
waiting for batches cost time and money

41
00:01:36,119 --> 00:01:37,680
you waste a lot of time by leasing

42
00:01:37,680 --> 00:01:39,180
expensive accelerators that you're not

43
00:01:39,180 --> 00:01:41,460
really using and you're losing a lot of

44
00:01:41,460 --> 00:01:42,780
time which you could have dedicated to

45
00:01:42,780 --> 00:01:44,460
another job

46
00:01:44,460 --> 00:01:46,619
to show you how important the pipeline

47
00:01:46,619 --> 00:01:49,439
resources are for ML workloads I have a

48
00:01:49,439 --> 00:01:51,360
few a few curves here that show you how

49
00:01:51,360 --> 00:01:53,399
the Epoch time progresses as you change

50
00:01:53,399 --> 00:01:55,439
the number of CPU cores per GPU and TPU

51
00:01:55,439 --> 00:01:58,020
accelerators for a few models and we can

52
00:01:58,020 --> 00:01:59,280
see that the green and the blue curves

53
00:01:59,280 --> 00:02:01,079
these are the retina net and resnet

54
00:02:01,079 --> 00:02:03,780
curves show how much the Epoch time

55
00:02:03,780 --> 00:02:06,659
changes as you change the resources

56
00:02:06,659 --> 00:02:09,479
now assuming you fix this problem

57
00:02:09,479 --> 00:02:11,480
pre-processing is still very expensive

58
00:02:11,480 --> 00:02:14,220
starting from Google shows that and and

59
00:02:14,220 --> 00:02:16,080
quite redundant actually so study from

60
00:02:16,080 --> 00:02:17,879
Google shows that around 10 of the

61
00:02:17,879 --> 00:02:20,459
unique input pipelines

62
00:02:20,459 --> 00:02:22,560
um actually account for about 75 of the

63
00:02:22,560 --> 00:02:23,879
computation that they dedicate to

64
00:02:23,879 --> 00:02:25,860
pre-processing

65
00:02:25,860 --> 00:02:28,560
another similar study from meta shows

66
00:02:28,560 --> 00:02:30,660
that uh sometimes pre-processing can

67
00:02:30,660 --> 00:02:32,819
take up more power than training itself

68
00:02:32,819 --> 00:02:36,019
in their data centers

69
00:02:36,300 --> 00:02:38,819
but good news is that there are some

70
00:02:38,819 --> 00:02:41,459
opportunities that we can tackle here so

71
00:02:41,459 --> 00:02:43,920
we can scale out the input Pipeline and

72
00:02:43,920 --> 00:02:47,760
thus eliminate these bottlenecks

73
00:02:47,760 --> 00:02:50,640
and then we can also utilize caching to

74
00:02:50,640 --> 00:02:52,739
avoid this redundant computation and

75
00:02:52,739 --> 00:02:54,120
reduce power consumption and free up

76
00:02:54,120 --> 00:02:56,040
these computational resources such that

77
00:02:56,040 --> 00:02:58,560
they can be used for other jobs

78
00:02:58,560 --> 00:03:00,300
now the current landscape and MLP

79
00:03:00,300 --> 00:03:02,099
processing doesn't really lack solutions

80
00:03:02,099 --> 00:03:04,019
for disaggregating that you know would

81
00:03:04,019 --> 00:03:06,599
allow us to scale the input pipeline out

82
00:03:06,599 --> 00:03:08,700
a good example is TF data service from

83
00:03:08,700 --> 00:03:11,280
Google or the data pre-processing

84
00:03:11,280 --> 00:03:13,200
service for meta which is closed Source

85
00:03:13,200 --> 00:03:14,940
unfortunately

86
00:03:14,940 --> 00:03:17,760
so the fundamental mechanism is already

87
00:03:17,760 --> 00:03:18,860
there

88
00:03:18,860 --> 00:03:22,920
what's really lacking is something that

89
00:03:22,920 --> 00:03:24,659
can help us do this more automatically

90
00:03:24,659 --> 00:03:26,819
So currently what you could do is you

91
00:03:26,819 --> 00:03:28,739
could you know trivially allocate a lot

92
00:03:28,739 --> 00:03:31,920
of resources for your input pipeline but

93
00:03:31,920 --> 00:03:34,019
that's very costly and not everyone has

94
00:03:34,019 --> 00:03:36,360
that luxury alternatively if you're a

95
00:03:36,360 --> 00:03:39,360
practitioner you can you know try out a

96
00:03:39,360 --> 00:03:41,819
lot of combinations for your input

97
00:03:41,819 --> 00:03:43,980
pipeline in terms of resource allocation

98
00:03:43,980 --> 00:03:46,500
but this is frustrating this is very

99
00:03:46,500 --> 00:03:50,040
time consuming it's very costly again if

100
00:03:50,040 --> 00:03:51,720
something changes in your input pipeline

101
00:03:51,720 --> 00:03:53,400
or your model you have to redo this

102
00:03:53,400 --> 00:03:55,980
entire process not to mention that if

103
00:03:55,980 --> 00:03:57,299
something changes during your run time

104
00:03:57,299 --> 00:03:59,879
you really cannot react to that so

105
00:03:59,879 --> 00:04:02,280
you're kind of chained to whatever

106
00:04:02,280 --> 00:04:04,819
decision you made beforehand

107
00:04:04,819 --> 00:04:07,379
uh in terms of caching the fundamental

108
00:04:07,379 --> 00:04:09,420
functionality already exists in these

109
00:04:09,420 --> 00:04:10,739
Frameworks so

110
00:04:10,739 --> 00:04:12,959
implementing caching isn't really a

111
00:04:12,959 --> 00:04:16,560
research problem what is however very

112
00:04:16,560 --> 00:04:18,839
difficult is to understand when and

113
00:04:18,839 --> 00:04:20,459
where does caching work in the input

114
00:04:20,459 --> 00:04:22,500
pipeline caching doesn't always make

115
00:04:22,500 --> 00:04:24,840
sense reading from Cache might be slower

116
00:04:24,840 --> 00:04:28,020
actually than recompute and assuming it

117
00:04:28,020 --> 00:04:30,000
does make sense well where does does it

118
00:04:30,000 --> 00:04:31,560
make sense to you know apply it in the

119
00:04:31,560 --> 00:04:33,720
input pipeline perhaps there's several

120
00:04:33,720 --> 00:04:35,820
good options but again as a practitioner

121
00:04:35,820 --> 00:04:37,919
you'd have to empirically try a bunch of

122
00:04:37,919 --> 00:04:41,160
a bunch of combinations and again lose

123
00:04:41,160 --> 00:04:43,919
time money frustrating and as soon as

124
00:04:43,919 --> 00:04:45,840
something changes in your model or your

125
00:04:45,840 --> 00:04:47,040
input pipeline you have to really do

126
00:04:47,040 --> 00:04:48,419
this entire process

127
00:04:48,419 --> 00:04:51,360
so if you want to improve uh ml

128
00:04:51,360 --> 00:04:54,180
workloads by optimizing the

129
00:04:54,180 --> 00:04:57,360
pre-processing Via these two methods we

130
00:04:57,360 --> 00:04:59,220
definitely need to automate them doing

131
00:04:59,220 --> 00:05:00,960
them practically doing them manually is

132
00:05:00,960 --> 00:05:02,100
not practical

133
00:05:02,100 --> 00:05:03,479
and this is where our main contributions

134
00:05:03,479 --> 00:05:05,280
come into play

135
00:05:05,280 --> 00:05:06,660
we're answering how many resources

136
00:05:06,660 --> 00:05:08,759
should be assigned to pre-processing Via

137
00:05:08,759 --> 00:05:11,160
our Auto scaling policy

138
00:05:11,160 --> 00:05:12,900
and we're answering the question of when

139
00:05:12,900 --> 00:05:14,639
and where should data be cached in the

140
00:05:14,639 --> 00:05:16,680
input pipeline via the auto caching

141
00:05:16,680 --> 00:05:18,680
policy

142
00:05:18,680 --> 00:05:20,940
this is the system architecture for

143
00:05:20,940 --> 00:05:22,800
casual and before I start to describe it

144
00:05:22,800 --> 00:05:24,660
I want to point out that we built on top

145
00:05:24,660 --> 00:05:26,940
of TF data service the reason behind

146
00:05:26,940 --> 00:05:28,560
this is that the segregation is already

147
00:05:28,560 --> 00:05:30,360
available this is an open source project

148
00:05:30,360 --> 00:05:32,880
it's very popular it's designed for a

149
00:05:32,880 --> 00:05:35,220
large-scale distributed ML workloads and

150
00:05:35,220 --> 00:05:37,380
it has high impact to both research and

151
00:05:37,380 --> 00:05:39,660
production environments because we built

152
00:05:39,660 --> 00:05:41,160
on top of this we inherit some of its

153
00:05:41,160 --> 00:05:43,440
main components the dispatcher which is

154
00:05:43,440 --> 00:05:44,880
essentially the brain of the operation

155
00:05:44,880 --> 00:05:46,380
and then we have the workers which are

156
00:05:46,380 --> 00:05:48,720
the stateless entities that fetch data

157
00:05:48,720 --> 00:05:50,400
pre-process it produce batches and then

158
00:05:50,400 --> 00:05:52,680
they give it to the clients and casually

159
00:05:52,680 --> 00:05:54,300
augment the logic of these components

160
00:05:54,300 --> 00:05:56,699
and we also add the caching layer that

161
00:05:56,699 --> 00:05:58,680
is represented here by glass surface we

162
00:05:58,680 --> 00:06:00,960
choose lesser because it scales very

163
00:06:00,960 --> 00:06:02,820
well it gives us the throughput that we

164
00:06:02,820 --> 00:06:05,160
need for a caching layer

165
00:06:05,160 --> 00:06:07,440
so how does the client in the service

166
00:06:07,440 --> 00:06:10,320
interact in cash flow before I actually

167
00:06:10,320 --> 00:06:11,580
start with that I want to point out that

168
00:06:11,580 --> 00:06:13,380
we represent our clients here as having

169
00:06:13,380 --> 00:06:15,180
one accelerator but actually you can

170
00:06:15,180 --> 00:06:16,560
have multiple accelerators you can

171
00:06:16,560 --> 00:06:17,940
actually have multiple clients that are

172
00:06:17,940 --> 00:06:19,380
tied to the same job so it can be as

173
00:06:19,380 --> 00:06:21,120
distributed as you really want

174
00:06:21,120 --> 00:06:22,680
all right so the client comes in and

175
00:06:22,680 --> 00:06:24,479
requests that a template pipeline is

176
00:06:24,479 --> 00:06:27,000
pre-processed the dispatcher recruits a

177
00:06:27,000 --> 00:06:29,039
worker to do so the worker starts

178
00:06:29,039 --> 00:06:30,900
reading the data either from Cache or

179
00:06:30,900 --> 00:06:33,600
from the source layer depending on the

180
00:06:33,600 --> 00:06:34,860
situation

181
00:06:34,860 --> 00:06:37,139
and then it starts to give these batches

182
00:06:37,139 --> 00:06:39,960
to the client on this example we assume

183
00:06:39,960 --> 00:06:42,479
that the client actually requires

184
00:06:42,479 --> 00:06:44,520
another worker such that the service can

185
00:06:44,520 --> 00:06:46,319
keep up with it and with it with its

186
00:06:46,319 --> 00:06:48,479
ingestion rate and the dispatcher does

187
00:06:48,479 --> 00:06:51,120
just that and it adds another worker

188
00:06:51,120 --> 00:06:52,500
and because this is a long-running

189
00:06:52,500 --> 00:06:54,720
service new clients come in all clients

190
00:06:54,720 --> 00:06:56,639
leave but they can benefit from each

191
00:06:56,639 --> 00:06:59,940
other's caching and also benefit from

192
00:06:59,940 --> 00:07:01,680
the auto scaling policy

193
00:07:01,680 --> 00:07:03,240
so the main features of casual are

194
00:07:03,240 --> 00:07:05,340
multi-tenancy the segregation that we

195
00:07:05,340 --> 00:07:07,259
inherit from TF data service the auto

196
00:07:07,259 --> 00:07:09,419
scaling policy and the auto caching

197
00:07:09,419 --> 00:07:12,840
policy which allows you to not only

198
00:07:12,840 --> 00:07:14,699
decide if caching makes sense and where

199
00:07:14,699 --> 00:07:15,840
does it make sense but also you can use

200
00:07:15,840 --> 00:07:18,300
it across job as well

201
00:07:18,300 --> 00:07:20,840
foreign

202
00:07:21,020 --> 00:07:24,000
if you're familiar with TF data or even

203
00:07:24,000 --> 00:07:25,620
sparked and you'll be right at home here

204
00:07:25,620 --> 00:07:28,740
in fact we don't really augment the API

205
00:07:28,740 --> 00:07:30,599
too much and in this example it's mostly

206
00:07:30,599 --> 00:07:34,080
TF data code

207
00:07:34,080 --> 00:07:37,080
um I won't go into deep details but we

208
00:07:37,080 --> 00:07:38,580
initially start off by just reading some

209
00:07:38,580 --> 00:07:40,860
data this produces a data set object on

210
00:07:40,860 --> 00:07:42,479
top of this object we actually defined

211
00:07:42,479 --> 00:07:44,400
the logic of the input pipeline

212
00:07:44,400 --> 00:07:45,900
consisting of a set of Transformations

213
00:07:45,900 --> 00:07:48,720
and some Auto cache UPS I'll get into

214
00:07:48,720 --> 00:07:51,900
these in just a bit then we request that

215
00:07:51,900 --> 00:07:53,759
the simple pipeline gets processed in

216
00:07:53,759 --> 00:07:56,099
the service by the distribute operation

217
00:07:56,099 --> 00:07:58,500
here rather than doing it locally

218
00:07:58,500 --> 00:08:00,720
and then a very pythonic manner one

219
00:08:00,720 --> 00:08:02,580
iterates over this data set and obtains

220
00:08:02,580 --> 00:08:05,699
batches that they give to the model

221
00:08:05,699 --> 00:08:07,560
and I said I'll mention something about

222
00:08:07,560 --> 00:08:09,360
these Auto Cash shops these are

223
00:08:09,360 --> 00:08:12,419
operations that we have added to the TF

224
00:08:12,419 --> 00:08:14,639
data API

225
00:08:14,639 --> 00:08:16,620
um the practitioner can place zero one

226
00:08:16,620 --> 00:08:19,379
or more such operations in the input

227
00:08:19,379 --> 00:08:20,699
pipeline

228
00:08:20,699 --> 00:08:23,099
these are simply hints for the cashier

229
00:08:23,099 --> 00:08:25,680
runtime cash will take a look at each of

230
00:08:25,680 --> 00:08:28,620
these Auto Cash op locations and we'll

231
00:08:28,620 --> 00:08:30,780
infer the throughput of the pipeline if

232
00:08:30,780 --> 00:08:32,458
you were to introduce caching at only

233
00:08:32,458 --> 00:08:34,200
that location

234
00:08:34,200 --> 00:08:35,940
uh cash and then collects all these

235
00:08:35,940 --> 00:08:38,279
throughputs and also the throughput of

236
00:08:38,279 --> 00:08:39,958
just purely recomputing everything and

237
00:08:39,958 --> 00:08:42,559
then chooses the option that is fastest

238
00:08:42,559 --> 00:08:44,459
so the option with the highest

239
00:08:44,459 --> 00:08:46,620
throughput the reason why we rely on

240
00:08:46,620 --> 00:08:47,880
hints here

241
00:08:47,880 --> 00:08:48,600
um

242
00:08:48,600 --> 00:08:51,540
is because it's not quite clear where

243
00:08:51,540 --> 00:08:54,060
you can automatically decide in the

244
00:08:54,060 --> 00:08:56,220
input pipe and where to put caching so

245
00:08:56,220 --> 00:08:58,320
oftentimes simple pipelines have random

246
00:08:58,320 --> 00:09:00,000
Transformations that are there to make

247
00:09:00,000 --> 00:09:03,800
the models more robust caching those

248
00:09:03,800 --> 00:09:06,000
might affect the model some

249
00:09:06,000 --> 00:09:07,200
practitioners might feel more

250
00:09:07,200 --> 00:09:10,200
comfortable with this than others but

251
00:09:10,200 --> 00:09:12,180
it's really a moving Target there are

252
00:09:12,180 --> 00:09:13,380
however very good strategies for

253
00:09:13,380 --> 00:09:15,120
avoiding you know very detrimental

254
00:09:15,120 --> 00:09:16,380
effects on the model and I'm happy to

255
00:09:16,380 --> 00:09:19,380
talk about them in the Q a or offline

256
00:09:19,380 --> 00:09:22,140
uh but yes we rely on the user hints to

257
00:09:22,140 --> 00:09:24,120
kind of indicate good points of caching

258
00:09:24,120 --> 00:09:25,680
and we just choose the one with the best

259
00:09:25,680 --> 00:09:27,000
throughput

260
00:09:27,000 --> 00:09:28,680
now because we build on top of TF data

261
00:09:28,680 --> 00:09:30,180
we inherit the graph representation that

262
00:09:30,180 --> 00:09:31,320
comes with it

263
00:09:31,320 --> 00:09:34,200
so in this particular example you'd

264
00:09:34,200 --> 00:09:36,779
obtain a graph like this the nodes are

265
00:09:36,779 --> 00:09:39,300
the Transformations and the edges are

266
00:09:39,300 --> 00:09:40,800
the flow of data between these

267
00:09:40,800 --> 00:09:42,540
transformations

268
00:09:42,540 --> 00:09:44,940
this graph representation allows us to

269
00:09:44,940 --> 00:09:46,620
conveniently gather metrics at every

270
00:09:46,620 --> 00:09:49,080
single node and we use these metrics for

271
00:09:49,080 --> 00:09:50,640
our Auto scaling and auto caching

272
00:09:50,640 --> 00:09:51,720
decisions

273
00:09:51,720 --> 00:09:53,820
and it also allows us to freely modify

274
00:09:53,820 --> 00:09:55,980
the computation that the workers do by

275
00:09:55,980 --> 00:09:57,420
just changing the structure of the graph

276
00:09:57,420 --> 00:09:59,279
so for instance we could just scan the

277
00:09:59,279 --> 00:10:02,279
graph for the auto cache shops find the

278
00:10:02,279 --> 00:10:03,720
one that we were interested and replace

279
00:10:03,720 --> 00:10:07,019
it with the cache get for instance

280
00:10:07,019 --> 00:10:08,940
all right let's take a look at the auto

281
00:10:08,940 --> 00:10:11,040
caching policy and as I mentioned before

282
00:10:11,040 --> 00:10:13,440
what cache does in a nutshell it kind of

283
00:10:13,440 --> 00:10:15,000
goes through each of these Auto Cash Ops

284
00:10:15,000 --> 00:10:16,380
to the practitioner places in the input

285
00:10:16,380 --> 00:10:19,080
Pipeline and it infers the throughput of

286
00:10:19,080 --> 00:10:20,519
the input pipeline if caching were

287
00:10:20,519 --> 00:10:22,200
introduced at that particular location

288
00:10:22,200 --> 00:10:24,720
to exemplify this a bit

289
00:10:24,720 --> 00:10:26,459
I've introduced here a pretty simple

290
00:10:26,459 --> 00:10:28,380
model uh sorry a pretty simple input

291
00:10:28,380 --> 00:10:30,899
pipeline consisting of a read data

292
00:10:30,899 --> 00:10:32,820
operation a set of transformations to

293
00:10:32,820 --> 00:10:35,700
follow the auto cache op in question

294
00:10:35,700 --> 00:10:37,440
another set of Transformations and

295
00:10:37,440 --> 00:10:39,720
finally the last operation in the input

296
00:10:39,720 --> 00:10:42,120
pipeline the first thing the cache does

297
00:10:42,120 --> 00:10:45,000
it infers the times to actually read the

298
00:10:45,000 --> 00:10:46,620
data from Cache rather than recomputing

299
00:10:46,620 --> 00:10:48,480
it until the auto cache opt

300
00:10:48,480 --> 00:10:50,339
this is called the projected cache read

301
00:10:50,339 --> 00:10:51,660
time

302
00:10:51,660 --> 00:10:54,600
casual then just compute the post Auto

303
00:10:54,600 --> 00:10:56,040
cache time this is the time it takes to

304
00:10:56,040 --> 00:10:58,560
apply the rest of the Transformations on

305
00:10:58,560 --> 00:11:01,560
the Red Data from Cache

306
00:11:01,560 --> 00:11:03,839
these two values are added together to

307
00:11:03,839 --> 00:11:06,720
produce the projected total cache time

308
00:11:06,720 --> 00:11:09,600
and such a projected total cash time is

309
00:11:09,600 --> 00:11:10,980
obtained for every single Auto cache

310
00:11:10,980 --> 00:11:12,839
shop in the input pipeline

311
00:11:12,839 --> 00:11:14,459
we take these times together with the

312
00:11:14,459 --> 00:11:16,019
total compute time this is the time if

313
00:11:16,019 --> 00:11:18,240
no caching were used whatsoever and then

314
00:11:18,240 --> 00:11:20,160
we choose the option with the highest

315
00:11:20,160 --> 00:11:21,959
throughput in other words the option

316
00:11:21,959 --> 00:11:24,779
that deals the minimum time

317
00:11:24,779 --> 00:11:26,459
all right now let's take a look at the

318
00:11:26,459 --> 00:11:28,800
auto scaling policy and to understand

319
00:11:28,800 --> 00:11:31,200
this better we need to understand how

320
00:11:31,200 --> 00:11:33,240
the batch processing actually looks like

321
00:11:33,240 --> 00:11:36,120
so processing a batch has two stages you

322
00:11:36,120 --> 00:11:38,220
first need to fetch the batch and in

323
00:11:38,220 --> 00:11:40,019
cash you do so by fetching it from your

324
00:11:40,019 --> 00:11:42,600
local buffer every client has a buffer

325
00:11:42,600 --> 00:11:46,440
that is populated with batches by Casio

326
00:11:46,440 --> 00:11:48,660
when your model needs a batch it first

327
00:11:48,660 --> 00:11:51,600
looks at this buffer if it's populated

328
00:11:51,600 --> 00:11:54,000
the wait time is roughly non-existent

329
00:11:54,000 --> 00:11:57,120
and otherwise you must wait so

330
00:11:57,120 --> 00:11:59,040
if your model is faster than your input

331
00:11:59,040 --> 00:12:00,839
pipeline then you'll generally have to

332
00:12:00,839 --> 00:12:02,820
wait quite quite a bit

333
00:12:02,820 --> 00:12:05,279
the second part of this batch processing

334
00:12:05,279 --> 00:12:07,140
is the model training itself this

335
00:12:07,140 --> 00:12:08,399
consists of the forward and backward

336
00:12:08,399 --> 00:12:11,640
pass that you would do on the batch

337
00:12:11,640 --> 00:12:14,399
and because we keep the number of

338
00:12:14,399 --> 00:12:15,839
accelerators cost and during the

339
00:12:15,839 --> 00:12:17,880
training process and you know the bad

340
00:12:17,880 --> 00:12:21,180
sizes are fixed in these workloads this

341
00:12:21,180 --> 00:12:24,779
time remains constant

342
00:12:24,779 --> 00:12:27,000
together these two components form the

343
00:12:27,000 --> 00:12:28,860
batch processing time

344
00:12:28,860 --> 00:12:30,300
and then during runtime you can only see

345
00:12:30,300 --> 00:12:31,680
it as a monolith you cannot really see

346
00:12:31,680 --> 00:12:33,959
the two elements

347
00:12:33,959 --> 00:12:35,579
the intuition here is that while you

348
00:12:35,579 --> 00:12:37,560
cannot really change the model training

349
00:12:37,560 --> 00:12:39,720
time you can change the fetch time and

350
00:12:39,720 --> 00:12:42,000
by adding resources to your input

351
00:12:42,000 --> 00:12:45,120
pipeline you'll reduce the fetching time

352
00:12:45,120 --> 00:12:46,920
up until the point where it doesn't

353
00:12:46,920 --> 00:12:49,800
exist anymore really

354
00:12:49,800 --> 00:12:51,300
um and then you only have the model

355
00:12:51,300 --> 00:12:52,800
training itself

356
00:12:52,800 --> 00:12:54,420
and so that's kind of the idea we just

357
00:12:54,420 --> 00:12:55,860
keep adding resources and we stop when

358
00:12:55,860 --> 00:12:57,060
we see that this time doesn't improve

359
00:12:57,060 --> 00:12:59,880
anymore to better exemplify exemplify it

360
00:12:59,880 --> 00:13:01,800
I've introduced here a simple example

361
00:13:01,800 --> 00:13:04,139
imagine that this is our initial bash

362
00:13:04,139 --> 00:13:06,720
processing time we add a worker we see

363
00:13:06,720 --> 00:13:08,399
that it reduces the batch processing

364
00:13:08,399 --> 00:13:10,260
time we keep doing it

365
00:13:10,260 --> 00:13:12,899
and we keep doing it again now we've

366
00:13:12,899 --> 00:13:14,880
reached a point where cash notices that

367
00:13:14,880 --> 00:13:16,380
the batch processing time doesn't

368
00:13:16,380 --> 00:13:20,279
improve by adding this last worker

369
00:13:20,279 --> 00:13:22,079
and because it's really Superfluous it

370
00:13:22,079 --> 00:13:23,579
doesn't do anything for performance it

371
00:13:23,579 --> 00:13:24,440
only

372
00:13:24,440 --> 00:13:28,200
costs us money we remove it and then we

373
00:13:28,200 --> 00:13:30,360
converge to this scale where the

374
00:13:30,360 --> 00:13:31,500
fetching time is essentially

375
00:13:31,500 --> 00:13:34,079
non-existent

376
00:13:34,079 --> 00:13:36,019
all right let's take a look at some

377
00:13:36,019 --> 00:13:39,240
evaluation for the auto scaling policy

378
00:13:39,240 --> 00:13:41,160
so for this we choose a deep learning

379
00:13:41,160 --> 00:13:43,320
image classification workload

380
00:13:43,320 --> 00:13:45,180
uh consisting of the resnet input

381
00:13:45,180 --> 00:13:47,579
pipeline that runs on into standard a

382
00:13:47,579 --> 00:13:50,459
GCE instances the resnet50 model that

383
00:13:50,459 --> 00:13:53,220
chains on four nvw100 gpus

384
00:13:53,220 --> 00:13:55,019
and then the imagenet data set that

385
00:13:55,019 --> 00:13:58,139
resides originally in GCS it's

386
00:13:58,139 --> 00:14:01,019
approximately 140 gigabytes in size

387
00:14:01,019 --> 00:14:03,000
the input pipeline is fairly standard

388
00:14:03,000 --> 00:14:04,800
for image classification we do some

389
00:14:04,800 --> 00:14:07,200
simple augmentations we add an auto

390
00:14:07,200 --> 00:14:09,060
cache up hint right after reading the

391
00:14:09,060 --> 00:14:11,220
data and right at the end of the input

392
00:14:11,220 --> 00:14:13,260
Pipeline and we name these two modes

393
00:14:13,260 --> 00:14:14,639
Source caching and full caching

394
00:14:14,639 --> 00:14:16,560
respectively

395
00:14:16,560 --> 00:14:18,540
what we want to see here is how our Auto

396
00:14:18,540 --> 00:14:20,459
scale policy Compares with the

397
00:14:20,459 --> 00:14:22,860
kubernetes horizontal pod autoscaler

398
00:14:22,860 --> 00:14:25,860
which uses Hardware signals such as CPU

399
00:14:25,860 --> 00:14:27,720
utilization and memory utilization to

400
00:14:27,720 --> 00:14:29,940
indicate whether we should scale up or

401
00:14:29,940 --> 00:14:31,560
down

402
00:14:31,560 --> 00:14:32,940
foreign

403
00:14:32,940 --> 00:14:35,940
now here on the y-axis we have the Epoch

404
00:14:35,940 --> 00:14:38,279
time in seconds and then on the x-axis

405
00:14:38,279 --> 00:14:40,220
we have a number of workers

406
00:14:40,220 --> 00:14:43,980
we have three different curves each

407
00:14:43,980 --> 00:14:45,839
representing the Epoch time and how it

408
00:14:45,839 --> 00:14:47,279
changes based on the number of workers

409
00:14:47,279 --> 00:14:49,980
that your service has

410
00:14:49,980 --> 00:14:51,480
in all of these three different

411
00:14:51,480 --> 00:14:53,399
execution modes compute Source cache and

412
00:14:53,399 --> 00:14:54,600
full cache

413
00:14:54,600 --> 00:14:58,220
and we also have a horizontal line

414
00:14:58,220 --> 00:15:01,380
that shows us the model bottleneck this

415
00:15:01,380 --> 00:15:02,519
is the point where no matter how many

416
00:15:02,519 --> 00:15:04,220
resources you add to your input pipeline

417
00:15:04,220 --> 00:15:06,720
your model is the bottleneck at that

418
00:15:06,720 --> 00:15:07,920
point and you can't really move forward

419
00:15:07,920 --> 00:15:10,260
what we want to see is that cash shoe

420
00:15:10,260 --> 00:15:12,120
which is the orange annotations chooses

421
00:15:12,120 --> 00:15:14,519
the intersection point between the

422
00:15:14,519 --> 00:15:17,160
curves and this horizontal line This is

423
00:15:17,160 --> 00:15:18,480
where you're not over provisioning but

424
00:15:18,480 --> 00:15:20,459
not under provisioning either

425
00:15:20,459 --> 00:15:22,740
and this is exactly what it does in the

426
00:15:22,740 --> 00:15:26,100
compute case and in the other two

427
00:15:26,100 --> 00:15:27,440
caching

428
00:15:27,440 --> 00:15:30,420
scenarios that we chose to study here

429
00:15:30,420 --> 00:15:32,459
kubernetes on the other hand doesn't

430
00:15:32,459 --> 00:15:33,800
fare so well

431
00:15:33,800 --> 00:15:36,899
in compute mode uh it converges to one

432
00:15:36,899 --> 00:15:38,820
worker now the reason behind this is

433
00:15:38,820 --> 00:15:41,100
that you're reading your data from GCS

434
00:15:41,100 --> 00:15:44,220
and the throughput of a bucket is scaled

435
00:15:44,220 --> 00:15:46,440
up based on the number of readers that a

436
00:15:46,440 --> 00:15:48,060
bucket has since you only have one

437
00:15:48,060 --> 00:15:50,880
worker the throughput is quite low this

438
00:15:50,880 --> 00:15:53,100
doesn't keep the worker busy the G the

439
00:15:53,100 --> 00:15:54,959
CPU utilization is too low to trigger a

440
00:15:54,959 --> 00:15:57,180
scale up and this tricks kubernetes into

441
00:15:57,180 --> 00:15:58,620
thinking that well this is the right

442
00:15:58,620 --> 00:16:01,079
scale now there's two other interesting

443
00:16:01,079 --> 00:16:03,540
cases here as well but due to time

444
00:16:03,540 --> 00:16:04,920
constraints I won't go into them but

445
00:16:04,920 --> 00:16:08,040
full details in the in the paper

446
00:16:08,040 --> 00:16:09,720
right now let's take a look at how the

447
00:16:09,720 --> 00:16:11,399
auto scaling and auto caching policies

448
00:16:11,399 --> 00:16:13,860
kind of work together in multiple tenant

449
00:16:13,860 --> 00:16:15,480
environments

450
00:16:15,480 --> 00:16:17,820
in this case we have two jobs each with

451
00:16:17,820 --> 00:16:19,820
four epochs

452
00:16:19,820 --> 00:16:21,779
they both use the resonant input

453
00:16:21,779 --> 00:16:23,339
pipeline together with the toy model

454
00:16:23,339 --> 00:16:26,639
both shops are virtually identical with

455
00:16:26,639 --> 00:16:27,839
the single difference that the second

456
00:16:27,839 --> 00:16:29,040
job is twice the ingestion rate

457
00:16:29,040 --> 00:16:32,100
requirements of the first meaning that

458
00:16:32,100 --> 00:16:34,019
it requires twice the number of workers

459
00:16:34,019 --> 00:16:35,639
as job one

460
00:16:35,639 --> 00:16:38,220
we want to really see is how the auto

461
00:16:38,220 --> 00:16:40,079
caching and auto scaling policies work

462
00:16:40,079 --> 00:16:42,779
in this scenario

463
00:16:42,779 --> 00:16:46,139
all right so here on the y-axis we have

464
00:16:46,139 --> 00:16:47,519
the worker count and on the x-axis we

465
00:16:47,519 --> 00:16:50,880
have the elapsed time the red curve

466
00:16:50,880 --> 00:16:53,220
shows the worker progression of job one

467
00:16:53,220 --> 00:16:54,600
throughout this experiment and we'll see

468
00:16:54,600 --> 00:16:56,699
a blue curve later on that's uh the

469
00:16:56,699 --> 00:16:59,759
worker count progression for job two now

470
00:16:59,759 --> 00:17:01,940
because this is a fresh run

471
00:17:01,940 --> 00:17:04,740
of the system there's no data cache so

472
00:17:04,740 --> 00:17:08,280
job one uh comes up and then goes

473
00:17:08,280 --> 00:17:10,260
straight into compute

474
00:17:10,260 --> 00:17:13,380
um and then scaling starts to happen it

475
00:17:13,380 --> 00:17:15,240
converges eventually at three workers

476
00:17:15,240 --> 00:17:17,640
cash also this time decides to put the

477
00:17:17,640 --> 00:17:19,319
data Into Cash because it feels that

478
00:17:19,319 --> 00:17:21,179
would be actually it would save

479
00:17:21,179 --> 00:17:23,160
resources later on

480
00:17:23,160 --> 00:17:26,160
uh so in the second Epoch the data is

481
00:17:26,160 --> 00:17:27,660
placed into cache scaling is reset

482
00:17:27,660 --> 00:17:29,040
because it's a different execution mode

483
00:17:29,040 --> 00:17:32,220
but in this mode as well it scales to

484
00:17:32,220 --> 00:17:33,540
three workers

485
00:17:33,540 --> 00:17:35,700
finally we move on to the last two

486
00:17:35,700 --> 00:17:37,500
epochs of the first job

487
00:17:37,500 --> 00:17:39,780
and let's focus here on the red curve

488
00:17:39,780 --> 00:17:41,880
this time we're reading from cache and

489
00:17:41,880 --> 00:17:45,299
we're converging to uh two workers

490
00:17:45,299 --> 00:17:47,039
so this is good this means that putting

491
00:17:47,039 --> 00:17:48,480
the data in Cache made sense we actually

492
00:17:48,480 --> 00:17:52,500
saved one worker and uh we're we're you

493
00:17:52,500 --> 00:17:54,240
know we're also saving a bit on time as

494
00:17:54,240 --> 00:17:55,380
well

495
00:17:55,380 --> 00:17:58,200
uh and now if you move to the second job

496
00:17:58,200 --> 00:18:00,179
we're seeing that this because it's

497
00:18:00,179 --> 00:18:02,520
identical to the first bar the ingestion

498
00:18:02,520 --> 00:18:04,679
rate requirements it cash should notices

499
00:18:04,679 --> 00:18:06,480
the cash hit and it reads the data

500
00:18:06,480 --> 00:18:07,919
directly from Cache rather than

501
00:18:07,919 --> 00:18:09,480
recomputing it

502
00:18:09,480 --> 00:18:11,340
this ultimately scales to four workers

503
00:18:11,340 --> 00:18:13,020
which is good because it's twice the

504
00:18:13,020 --> 00:18:16,559
ingestion rate of the first one so the

505
00:18:16,559 --> 00:18:19,260
policies seem to work well together

506
00:18:19,260 --> 00:18:21,660
all right so to wrap up

507
00:18:21,660 --> 00:18:23,400
we believe that data pre-processing is

508
00:18:23,400 --> 00:18:26,340
an essential part of ml workloads it is

509
00:18:26,340 --> 00:18:28,140
often a source of bottlenecks that cause

510
00:18:28,140 --> 00:18:30,419
expensive accelerator stalls and a lot

511
00:18:30,419 --> 00:18:32,160
of wasted time as well

512
00:18:32,160 --> 00:18:33,960
and to this extent WE Post cash which is

513
00:18:33,960 --> 00:18:36,539
an input pipeline as a service system uh

514
00:18:36,539 --> 00:18:38,340
bringing in to the table Auto caching

515
00:18:38,340 --> 00:18:40,440
and auto scaling policies multi-tenancy

516
00:18:40,440 --> 00:18:41,880
support as well

517
00:18:41,880 --> 00:18:44,520
that's open source and we believe that

518
00:18:44,520 --> 00:18:46,200
this is a rich platform for future

519
00:18:46,200 --> 00:18:48,780
research in this area

520
00:18:48,780 --> 00:18:50,640
thank you very much and that was it for

521
00:18:50,640 --> 00:18:51,780
me and there are any questions I'd be

522
00:18:51,780 --> 00:18:54,440
happy to answer them

