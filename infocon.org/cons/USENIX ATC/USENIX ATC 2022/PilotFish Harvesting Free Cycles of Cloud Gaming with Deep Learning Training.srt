1
00:00:14,160 --> 00:00:16,880
hi i'm jim farhan a researcher from

2
00:00:16,880 --> 00:00:19,039
microsoft research asia

3
00:00:19,039 --> 00:00:21,760
in this talk i will present our work

4
00:00:21,760 --> 00:00:25,199
paraphrase to harvest free gpu cycles of

5
00:00:25,199 --> 00:00:28,080
cloud gaming with deep learning training

6
00:00:28,080 --> 00:00:31,119
this work is jointly done with wei jiang

7
00:00:31,119 --> 00:00:32,960
binghao chen

8
00:00:32,960 --> 00:00:35,520
chen chen meaning from shanghai joshua

9
00:00:35,520 --> 00:00:39,040
university and jin wahan

10
00:00:39,040 --> 00:00:41,360
yang yanshu and yui

11
00:00:41,360 --> 00:00:45,840
from microsoft research asia

12
00:00:47,360 --> 00:00:49,520
cloud gaming is becoming popular

13
00:00:49,520 --> 00:00:51,280
modern cloud service providers like

14
00:00:51,280 --> 00:00:53,120
microsoft are providing cloud gaming

15
00:00:53,120 --> 00:00:55,039
services for players

16
00:00:55,039 --> 00:00:57,440
results owning a powerful gpu desktop or

17
00:00:57,440 --> 00:01:00,160
game console players can still play high

18
00:01:00,160 --> 00:01:02,399
quality games from their mobile phone or

19
00:01:02,399 --> 00:01:04,879
laptop by hosting games on servers in

20
00:01:04,879 --> 00:01:07,280
the cloud data centers and streamed via

21
00:01:07,280 --> 00:01:09,760
internet

22
00:01:10,479 --> 00:01:12,720
modern gpus are very powerful

23
00:01:12,720 --> 00:01:14,560
some of them are built for running games

24
00:01:14,560 --> 00:01:18,000
at 4k resolution at 60 frames per second

25
00:01:18,000 --> 00:01:19,920
however streaming such high quality

26
00:01:19,920 --> 00:01:22,000
radio over internet has a high network

27
00:01:22,000 --> 00:01:24,799
requirement like 40 megabits per second

28
00:01:24,799 --> 00:01:26,479
for 4k resolution

29
00:01:26,479 --> 00:01:29,439
and 40 millisecond latency for good

30
00:01:29,439 --> 00:01:31,520
gaming experience

31
00:01:31,520 --> 00:01:34,479
also the device of cloud gaming users

32
00:01:34,479 --> 00:01:36,960
has limited capability like low screen

33
00:01:36,960 --> 00:01:39,680
resolution and results hardware

34
00:01:39,680 --> 00:01:41,680
acceleration for decoding high quality

35
00:01:41,680 --> 00:01:43,360
frames

36
00:01:43,360 --> 00:01:46,000
according to a survey from steam most

37
00:01:46,000 --> 00:01:49,360
players are using 1080p screen to play

38
00:01:49,360 --> 00:01:50,960
games

39
00:01:50,960 --> 00:01:53,360
due to these limitations most cloud

40
00:01:53,360 --> 00:01:55,520
gaming players can only stream games on

41
00:01:55,520 --> 00:02:00,320
1080p 60 fps or even lower

42
00:02:00,320 --> 00:02:03,040
we find streaming cloud games on this

43
00:02:03,040 --> 00:02:05,360
quality leads to a low utilization for

44
00:02:05,360 --> 00:02:08,399
powerful cloud gpus

45
00:02:08,399 --> 00:02:10,878
in this list of popular games the gpu

46
00:02:10,878 --> 00:02:13,360
utilization of most of them is lower

47
00:02:13,360 --> 00:02:14,720
than 50

48
00:02:14,720 --> 00:02:16,800
by running on a server with a similar

49
00:02:16,800 --> 00:02:21,720
configuration of cloud gaming servers

50
00:02:22,800 --> 00:02:24,560
to understand the situation of cloud

51
00:02:24,560 --> 00:02:27,040
gaming's low utilization let's briefly

52
00:02:27,040 --> 00:02:30,319
learn how gpu renders frames for games

53
00:02:30,319 --> 00:02:31,200
first

54
00:02:31,200 --> 00:02:33,200
game sends are rendered frame by frame

55
00:02:33,200 --> 00:02:34,720
in a pipeline manner

56
00:02:34,720 --> 00:02:36,800
the cpu possesses the game logic and

57
00:02:36,800 --> 00:02:38,800
prepares the rendering commands to be

58
00:02:38,800 --> 00:02:41,519
submitted to the gpu and the gpu does

59
00:02:41,519 --> 00:02:43,920
the rendering computation

60
00:02:43,920 --> 00:02:45,840
second due to the different sync

61
00:02:45,840 --> 00:02:48,080
complexity the rendering time varies

62
00:02:48,080 --> 00:02:50,560
greatly for different frames

63
00:02:50,560 --> 00:02:53,519
third when gpu is under utilized there

64
00:02:53,519 --> 00:02:56,160
will be multiple fragmented idle gpu

65
00:02:56,160 --> 00:02:58,720
time periods

66
00:02:58,720 --> 00:03:01,040
our problem here is to improve the

67
00:03:01,040 --> 00:03:03,280
utilization of call gaming

68
00:03:03,280 --> 00:03:06,239
how can we harvest the fragmented idle

69
00:03:06,239 --> 00:03:09,120
gpu periods

70
00:03:09,680 --> 00:03:12,080
a straightforward idea is to run

71
00:03:12,080 --> 00:03:14,400
multiple games on the same gpu to

72
00:03:14,400 --> 00:03:16,720
improve the gpu implementation

73
00:03:16,720 --> 00:03:20,239
however we find this is invisible

74
00:03:20,239 --> 00:03:22,560
games are too random with strict quality

75
00:03:22,560 --> 00:03:24,159
of service

76
00:03:24,159 --> 00:03:26,400
frequent conflicts would appear when the

77
00:03:26,400 --> 00:03:29,519
game is rendering two complex rings from

78
00:03:29,519 --> 00:03:31,440
both games

79
00:03:31,440 --> 00:03:34,080
the red figure here shows our experiment

80
00:03:34,080 --> 00:03:36,480
which has these three games each of them

81
00:03:36,480 --> 00:03:41,359
only has about 50 gpu transition

82
00:03:41,840 --> 00:03:45,120
however when two of them are co-located

83
00:03:45,120 --> 00:03:47,519
the gpu tradition is only improved a

84
00:03:47,519 --> 00:03:50,239
little by about 20 percent

85
00:03:50,239 --> 00:03:52,799
but the fps of both games dropped

86
00:03:52,799 --> 00:03:54,720
significantly

87
00:03:54,720 --> 00:03:57,519
some games detected the confliction and

88
00:03:57,519 --> 00:03:59,439
reduced the rendering quality which

89
00:03:59,439 --> 00:04:01,040
could greatly affect the gaming

90
00:04:01,040 --> 00:04:03,599
experience this is unacceptable for

91
00:04:03,599 --> 00:04:06,879
cloud gaming services

92
00:04:08,319 --> 00:04:10,080
the last thing we learned from the last

93
00:04:10,080 --> 00:04:12,000
slide presents three requirements for

94
00:04:12,000 --> 00:04:15,200
collocation with games first we need to

95
00:04:15,200 --> 00:04:18,720
quickly capture the adult gpu periods in

96
00:04:18,720 --> 00:04:20,798
order to harvest them

97
00:04:20,798 --> 00:04:22,880
second we need a more creative or

98
00:04:22,880 --> 00:04:24,880
workload so that we can control its

99
00:04:24,880 --> 00:04:28,320
submission to avoid conflicts with games

100
00:04:28,320 --> 00:04:30,880
third we need a secure solution that can

101
00:04:30,880 --> 00:04:33,280
quickly preempt this regular workload

102
00:04:33,280 --> 00:04:37,119
once our prediction is wrong

103
00:04:39,919 --> 00:04:42,160
when fund diploma training is a good

104
00:04:42,160 --> 00:04:45,280
choice for collocation with game

105
00:04:45,280 --> 00:04:47,520
previous work have revealed the fact

106
00:04:47,520 --> 00:04:49,360
that deep learning training is very

107
00:04:49,360 --> 00:04:51,440
stable and predictable

108
00:04:51,440 --> 00:04:53,280
it just repeatedly does the same

109
00:04:53,280 --> 00:04:56,800
computation on different batch of data

110
00:04:56,800 --> 00:04:59,919
therefore its computation and memory

111
00:04:59,919 --> 00:05:03,199
usage pattern are very stable

112
00:05:03,199 --> 00:05:06,080
also the gpu kernel are usually very

113
00:05:06,080 --> 00:05:08,800
fragrant the execution time of gpu

114
00:05:08,800 --> 00:05:10,560
kernels for different training are

115
00:05:10,560 --> 00:05:12,639
usually millisecond level and very

116
00:05:12,639 --> 00:05:14,240
stable

117
00:05:14,240 --> 00:05:16,800
these characteristics are very helpful

118
00:05:16,800 --> 00:05:18,639
for collocation with games in

119
00:05:18,639 --> 00:05:21,680
collaborative manner

120
00:05:21,680 --> 00:05:23,600
to collocate cloud gaming with deep

121
00:05:23,600 --> 00:05:26,080
learning training we proposed our system

122
00:05:26,080 --> 00:05:27,440
pilofish

123
00:05:27,440 --> 00:05:29,600
paddlefish first instruments the

124
00:05:29,600 --> 00:05:32,479
rendering aps of graphic libraries like

125
00:05:32,479 --> 00:05:35,440
directx to quickly capture the adult gpu

126
00:05:35,440 --> 00:05:37,520
periods

127
00:05:37,520 --> 00:05:39,600
phase also controls the submission of

128
00:05:39,600 --> 00:05:41,120
deep learning training

129
00:05:41,120 --> 00:05:44,240
which are only allowed to be executed in

130
00:05:44,240 --> 00:05:47,919
the iwgp periods from games

131
00:05:47,919 --> 00:05:50,000
paloface will keep monitoring the

132
00:05:50,000 --> 00:05:52,320
execution of different training to avoid

133
00:05:52,320 --> 00:05:55,199
potential interference due to straggler

134
00:05:55,199 --> 00:05:57,520
and contention on other resources like

135
00:05:57,520 --> 00:06:02,320
cpu memory dxio and pca

136
00:06:02,800 --> 00:06:04,400
the rendering commands are already

137
00:06:04,400 --> 00:06:06,800
compiled to gpu kernels where graphic

138
00:06:06,800 --> 00:06:10,560
libraries like directx woken or opengl

139
00:06:10,560 --> 00:06:12,560
they all have similar apis for gpu

140
00:06:12,560 --> 00:06:14,319
command submission and frame

141
00:06:14,319 --> 00:06:16,560
presentation

142
00:06:16,560 --> 00:06:18,960
panelfish just hooks this eps to monitor

143
00:06:18,960 --> 00:06:21,680
the task submission to capture the gpu

144
00:06:21,680 --> 00:06:23,680
rendering tab

145
00:06:23,680 --> 00:06:25,919
as the last command of each ring

146
00:06:25,919 --> 00:06:28,240
parallel phase inserts a signal to

147
00:06:28,240 --> 00:06:30,160
notify the composition of frames

148
00:06:30,160 --> 00:06:31,520
rendering

149
00:06:31,520 --> 00:06:33,120
in this way

150
00:06:33,120 --> 00:06:35,440
fish can capture the idol periods from

151
00:06:35,440 --> 00:06:36,960
games

152
00:06:36,960 --> 00:06:38,880
this design does not require

153
00:06:38,880 --> 00:06:42,240
modification to games

154
00:06:42,800 --> 00:06:45,280
once we capture the adult gpu periods we

155
00:06:45,280 --> 00:06:47,360
can submit gpu kernels of different

156
00:06:47,360 --> 00:06:50,319
training to harvest the iot appearance

157
00:06:50,319 --> 00:06:52,479
a deep learning kernel is allowed to be

158
00:06:52,479 --> 00:06:55,360
submitted only when it can complete

159
00:06:55,360 --> 00:06:58,400
before the rendering of the next frame

160
00:06:58,400 --> 00:07:00,800
to avoid the interference on the gpu

161
00:07:00,800 --> 00:07:02,960
timeline

162
00:07:02,960 --> 00:07:05,680
for deep learning kernel cannot complete

163
00:07:05,680 --> 00:07:08,080
in the adult gb appearance it will be

164
00:07:08,080 --> 00:07:12,919
postponed to the next idle appearance

165
00:07:16,800 --> 00:07:19,599
sometimes straggler cuddles may appear

166
00:07:19,599 --> 00:07:21,840
that some diplomatic training cannot run

167
00:07:21,840 --> 00:07:24,240
longer than our prediction

168
00:07:24,240 --> 00:07:27,919
helloface introduces two isos for games

169
00:07:27,919 --> 00:07:30,319
for the quick interactive games like

170
00:07:30,319 --> 00:07:33,120
racing games we use hard guarantees that

171
00:07:33,120 --> 00:07:35,599
immediately preempt diploma training to

172
00:07:35,599 --> 00:07:37,759
avoid interference

173
00:07:37,759 --> 00:07:40,319
for some games without strong ips

174
00:07:40,319 --> 00:07:42,560
requirement we may delay the frame

175
00:07:42,560 --> 00:07:45,039
rendering for a while two frames to

176
00:07:45,039 --> 00:07:47,280
allow multiple kernels of deep learning

177
00:07:47,280 --> 00:07:49,199
companies

178
00:07:49,199 --> 00:07:52,560
our preemption is done using two gpu

179
00:07:52,560 --> 00:07:53,840
streams

180
00:07:53,840 --> 00:07:55,680
our deep learning training

181
00:07:55,680 --> 00:07:59,759
is submitted to the low priority stream

182
00:07:59,759 --> 00:08:02,800
and the certain kernel is submitted to

183
00:08:02,800 --> 00:08:05,199
the high priority stream once we want to

184
00:08:05,199 --> 00:08:08,319
preempt the deployment trend

185
00:08:08,319 --> 00:08:11,199
it has a very low preemption overhead

186
00:08:11,199 --> 00:08:14,400
with 0.7 milliseconds

187
00:08:14,400 --> 00:08:16,800
to reduce the loss of training progress

188
00:08:16,800 --> 00:08:19,680
due to preemption we will periodically

189
00:08:19,680 --> 00:08:24,360
checkpoint model weights for resumption

190
00:08:25,840 --> 00:08:27,919
addition to gpu course

191
00:08:27,919 --> 00:08:29,919
both game and deep learning training use

192
00:08:29,919 --> 00:08:32,159
other hardware resources

193
00:08:32,159 --> 00:08:34,080
hellofish also guarantees deep learning

194
00:08:34,080 --> 00:08:36,240
training will not interfere with games

195
00:08:36,240 --> 00:08:38,479
on these resources

196
00:08:38,479 --> 00:08:41,279
games heavily use cpu for game logic

197
00:08:41,279 --> 00:08:44,159
processing and physics simulation

198
00:08:44,159 --> 00:08:46,800
interference on cpu will lead to fps

199
00:08:46,800 --> 00:08:48,800
drop and the increase of game loading

200
00:08:48,800 --> 00:08:50,160
time

201
00:08:50,160 --> 00:08:53,200
to avoid interference on cpu powerfish

202
00:08:53,200 --> 00:08:55,600
uses threat priority supported by most

203
00:08:55,600 --> 00:08:57,519
modern operation systems to set

204
00:08:57,519 --> 00:08:59,680
different training as low priority

205
00:08:59,680 --> 00:09:02,079
strikes

206
00:09:03,120 --> 00:09:05,200
pcie connection is used for data

207
00:09:05,200 --> 00:09:08,080
movement between host and gpu device

208
00:09:08,080 --> 00:09:11,040
used by both games and dbm

209
00:09:11,040 --> 00:09:13,200
contention on pcie will slow down the

210
00:09:13,200 --> 00:09:15,839
data resource movement like texture and

211
00:09:15,839 --> 00:09:18,000
impact the rendering

212
00:09:18,000 --> 00:09:20,240
we use the pcie binaries reservation

213
00:09:20,240 --> 00:09:22,880
technique from baymax for games to

214
00:09:22,880 --> 00:09:25,519
guarantee its data movement

215
00:09:25,519 --> 00:09:28,000
similarly if both games data and

216
00:09:28,000 --> 00:09:29,920
defending training headsets are located

217
00:09:29,920 --> 00:09:32,640
on the same disk they may contain for io

218
00:09:32,640 --> 00:09:34,320
bandwidth

219
00:09:34,320 --> 00:09:36,399
pilofish directly use the namespace

220
00:09:36,399 --> 00:09:38,560
isolation technique supported by disk

221
00:09:38,560 --> 00:09:41,200
and the io priority to guarantee the

222
00:09:41,200 --> 00:09:44,720
game data loading first

223
00:09:45,680 --> 00:09:47,760
game and deep end training are expected

224
00:09:47,760 --> 00:09:49,440
to use different networks in our

225
00:09:49,440 --> 00:09:50,959
assumption so

226
00:09:50,959 --> 00:09:53,120
no interference unlike an underlying

227
00:09:53,120 --> 00:09:54,480
network

228
00:09:54,480 --> 00:09:56,959
and games video stream encoding is done

229
00:09:56,959 --> 00:09:59,120
usually using dedicated hardware chip

230
00:09:59,120 --> 00:10:03,040
like immediate in the unc or in md vce

231
00:10:03,040 --> 00:10:05,680
thus no interference on the stream

232
00:10:05,680 --> 00:10:08,239
encoding

233
00:10:09,279 --> 00:10:12,160
we evaluate paddlefish on gaming server

234
00:10:12,160 --> 00:10:17,839
with intel i7 cpu and radius rtx 2060 gp

235
00:10:17,839 --> 00:10:20,079
which has a comparable flops with cloud

236
00:10:20,079 --> 00:10:22,079
gaming gpus

237
00:10:22,079 --> 00:10:24,880
we evaluated five popular games and four

238
00:10:24,880 --> 00:10:27,680
popular deployment models with different

239
00:10:27,680 --> 00:10:30,560
configurations

240
00:10:31,440 --> 00:10:34,240
to evaluate how much gpu transition is

241
00:10:34,240 --> 00:10:36,720
harvested we define a metric called

242
00:10:36,720 --> 00:10:38,800
harvest ratio that calculates the

243
00:10:38,800 --> 00:10:42,399
percentage of idle gpu cycles harvested

244
00:10:42,399 --> 00:10:44,839
we compare parallel fish with three

245
00:10:44,839 --> 00:10:47,600
baselines the windows building game mode

246
00:10:47,600 --> 00:10:50,880
will only prioritize cpu process of

247
00:10:50,880 --> 00:10:53,600
games but not gpu

248
00:10:53,600 --> 00:10:56,160
a constant speed based lines controls

249
00:10:56,160 --> 00:10:58,240
the submission speed of deep learning

250
00:10:58,240 --> 00:11:01,920
training kernel at a constant speed

251
00:11:01,920 --> 00:11:04,079
amount advanced strongman's baseline

252
00:11:04,079 --> 00:11:06,959
uses adaptable speed that leverage

253
00:11:06,959 --> 00:11:09,200
windows build the fps long-term tool

254
00:11:09,200 --> 00:11:11,360
mechanism like present mode to

255
00:11:11,360 --> 00:11:13,760
adaptively control the deep learning

256
00:11:13,760 --> 00:11:16,800
kernel summation

257
00:11:17,920 --> 00:11:22,079
here are the evaluation results

258
00:11:22,240 --> 00:11:25,040
when the gpu kernel summation of deep

259
00:11:25,040 --> 00:11:27,440
learning training is not sorted

260
00:11:27,440 --> 00:11:29,760
the gpu harvest ratio is very high

261
00:11:29,760 --> 00:11:31,760
almost 100 percent

262
00:11:31,760 --> 00:11:33,680
but it greatly hurts that gaming

263
00:11:33,680 --> 00:11:36,880
experience with much lower fps compared

264
00:11:36,880 --> 00:11:42,120
with running games in a standalone map

265
00:11:42,959 --> 00:11:45,680
the two baseline circling deep learning

266
00:11:45,680 --> 00:11:48,000
training at a constant speed or an

267
00:11:48,000 --> 00:11:51,440
adaptive speed also hurts game fps with

268
00:11:51,440 --> 00:11:54,959
a lower harvested ratio

269
00:11:54,959 --> 00:11:57,600
paddlefish achieves the best of both

270
00:11:57,600 --> 00:11:58,480
worlds

271
00:11:58,480 --> 00:12:01,839
it has a high highest ratio up to 85

272
00:12:01,839 --> 00:12:04,640
percent without affecting the fps of

273
00:12:04,640 --> 00:12:07,040
games

274
00:12:07,360 --> 00:12:09,440
the major source of improvement of

275
00:12:09,440 --> 00:12:11,600
powerfish comes from its dynamic

276
00:12:11,600 --> 00:12:13,120
schedule

277
00:12:13,120 --> 00:12:15,440
here we value the submission speed of

278
00:12:15,440 --> 00:12:17,600
the constant speed this one

279
00:12:17,600 --> 00:12:19,600
we find that the constant speed baseline

280
00:12:19,600 --> 00:12:23,360
will not impact fps only when it submits

281
00:12:23,360 --> 00:12:26,240
kernels at a very slow speed which has

282
00:12:26,240 --> 00:12:28,399
only three percent of the original

283
00:12:28,399 --> 00:12:30,560
training speed

284
00:12:30,560 --> 00:12:33,040
instead pilot fish is aware of the

285
00:12:33,040 --> 00:12:35,440
duration of idle periods that can

286
00:12:35,440 --> 00:12:38,399
harvest the free gpu cycles comparable

287
00:12:38,399 --> 00:12:41,920
to the 80 percent constant speed result

288
00:12:41,920 --> 00:12:45,599
impacting the game fps

289
00:12:46,800 --> 00:12:48,959
we observe that different deep learning

290
00:12:48,959 --> 00:12:51,040
models may benefit differently from the

291
00:12:51,040 --> 00:12:52,480
collocation

292
00:12:52,480 --> 00:12:54,880
for example will we co-locate two

293
00:12:54,880 --> 00:12:58,240
different models amnas knight and lstm

294
00:12:58,240 --> 00:13:00,639
with the game hitman 3.

295
00:13:00,639 --> 00:13:03,600
we find amnas 9 can harvest over 70

296
00:13:03,600 --> 00:13:07,440
percent of free gpu cycles but lstm can

297
00:13:07,440 --> 00:13:10,720
only harvest about 50 percent

298
00:13:10,720 --> 00:13:12,959
the root cause is due to zero kernel

299
00:13:12,959 --> 00:13:14,399
duration

300
00:13:14,399 --> 00:13:17,120
amnesty is composed by many small

301
00:13:17,120 --> 00:13:20,399
kernels like matrix multiplication thus

302
00:13:20,399 --> 00:13:22,880
can easily fit into the adult periods

303
00:13:22,880 --> 00:13:24,880
from games

304
00:13:24,880 --> 00:13:26,000
however

305
00:13:26,000 --> 00:13:28,959
due to the dynamic computation of rstm

306
00:13:28,959 --> 00:13:31,360
its kernel has to implement complex

307
00:13:31,360 --> 00:13:34,399
logic inside the kernel thus leading to

308
00:13:34,399 --> 00:13:36,880
longer current durations than amnas

309
00:13:36,880 --> 00:13:38,560
knight

310
00:13:38,560 --> 00:13:41,279
the long lstm kernels are only allowed

311
00:13:41,279 --> 00:13:43,440
to be scheduled when the adult gpu

312
00:13:43,440 --> 00:13:46,160
periods from games are long enough to

313
00:13:46,160 --> 00:13:48,079
fix them

314
00:13:48,079 --> 00:13:50,639
it makes lstm hard to harvest the short

315
00:13:50,639 --> 00:13:53,600
gpu periods especially when the game is

316
00:13:53,600 --> 00:13:58,240
running on a high rendering quality mode

317
00:14:01,120 --> 00:14:03,680
the soft guarantee from paddlefish is

318
00:14:03,680 --> 00:14:06,480
helpful to improve the gpu harvest ratio

319
00:14:06,480 --> 00:14:09,040
for models like lstm with long

320
00:14:09,040 --> 00:14:11,120
computation kernels

321
00:14:11,120 --> 00:14:13,600
by allowing to drop one or two frames

322
00:14:13,600 --> 00:14:16,560
from games paraphrase can provide longer

323
00:14:16,560 --> 00:14:20,240
idle gpu periods for lstm model so that

324
00:14:20,240 --> 00:14:23,199
their long kernels can fit

325
00:14:23,199 --> 00:14:26,880
a soft guarantee allows lstms to harvest

326
00:14:26,880 --> 00:14:30,160
about 40 more gpu cycles than the heart

327
00:14:30,160 --> 00:14:33,839
guarantee only reducing the game fps by

328
00:14:33,839 --> 00:14:35,920
a little bit

329
00:14:35,920 --> 00:14:39,120
this is acceptable for some rpg games or

330
00:14:39,120 --> 00:14:41,519
pixel games without strong fps

331
00:14:41,519 --> 00:14:43,760
requirements

332
00:14:43,760 --> 00:14:44,800
however

333
00:14:44,800 --> 00:14:47,600
if we remove the pulsing mechanism from

334
00:14:47,600 --> 00:14:48,959
paddlefish

335
00:14:48,959 --> 00:14:53,040
we observe the clear fps drop because

336
00:14:53,040 --> 00:14:56,839
some stragular kernels run longer than

337
00:14:56,839 --> 00:14:59,760
expected disabling preemption will

338
00:14:59,760 --> 00:15:02,160
postpone the rendering of games and

339
00:15:02,160 --> 00:15:04,480
impact the gaming experience

340
00:15:04,480 --> 00:15:07,120
so this proves the necessity of the

341
00:15:07,120 --> 00:15:10,240
pulsing mechanism

342
00:15:11,440 --> 00:15:13,519
here we show the fps variation of all

343
00:15:13,519 --> 00:15:14,880
the solutions

344
00:15:14,880 --> 00:15:17,600
hello phase gives a very stable fps

345
00:15:17,600 --> 00:15:20,000
compared with the baselines with high

346
00:15:20,000 --> 00:15:24,480
fps variation and lower average fps

347
00:15:24,480 --> 00:15:26,720
we even observe the game reduces the

348
00:15:26,720 --> 00:15:29,279
rendering quality to keep a smooth

349
00:15:29,279 --> 00:15:32,480
gaming experience in the baseline due to

350
00:15:32,480 --> 00:15:35,040
the higher fps acceleration and the drop

351
00:15:35,040 --> 00:15:36,720
of fps

352
00:15:36,720 --> 00:15:39,680
you can see here under the bridge the

353
00:15:39,680 --> 00:15:42,079
light has the worst rendering quality

354
00:15:42,079 --> 00:15:44,880
compared with the paddlefish and compare

355
00:15:44,880 --> 00:15:47,199
with the running game in a standalone

356
00:15:47,199 --> 00:15:49,519
manner

357
00:15:49,519 --> 00:15:52,560
here is a demo video we run the game tom

358
00:15:52,560 --> 00:15:55,519
clancy's division 2 without ps locked at

359
00:15:55,519 --> 00:15:58,639
60 and the resolution 1080p

360
00:15:58,639 --> 00:16:00,320
we set the rendering quality to the

361
00:16:00,320 --> 00:16:02,800
highest level

362
00:16:02,800 --> 00:16:04,560
we collocate this game with the deep

363
00:16:04,560 --> 00:16:06,639
learning training model reset 50

364
00:16:06,639 --> 00:16:08,880
training data set civil 10 with a batch

365
00:16:08,880 --> 00:16:11,759
set of 16.

366
00:16:12,959 --> 00:16:15,519
the last video shows the collocation

367
00:16:15,519 --> 00:16:17,759
using pilotfish and the middle video

368
00:16:17,759 --> 00:16:19,360
shows the running game in standalone

369
00:16:19,360 --> 00:16:21,440
manner and the right video shows the

370
00:16:21,440 --> 00:16:25,600
baseline of collocation results shortly

371
00:16:25,600 --> 00:16:28,320
parallel fish achieve the same fps

372
00:16:28,320 --> 00:16:30,560
comparisons game only without affecting

373
00:16:30,560 --> 00:16:32,079
the rendering quality

374
00:16:32,079 --> 00:16:34,560
but we find the collocation you result

375
00:16:34,560 --> 00:16:36,720
throttling will reduce the renal quality

376
00:16:36,720 --> 00:16:39,880
of skin

377
00:17:41,039 --> 00:17:44,000
in conclusion we observe cloud gaming as

378
00:17:44,000 --> 00:17:46,640
a low gpu recognition issue due to the

379
00:17:46,640 --> 00:17:48,400
limiting streaming quality on the

380
00:17:48,400 --> 00:17:50,320
powerful gpus

381
00:17:50,320 --> 00:17:52,480
to harvest the idle gpu cycles we

382
00:17:52,480 --> 00:17:54,640
propose paddlefish that can quickly

383
00:17:54,640 --> 00:17:57,360
capture the other gpu periods via api

384
00:17:57,360 --> 00:17:59,200
instrumentation

385
00:17:59,200 --> 00:18:01,039
it allows the diplomat training's

386
00:18:01,039 --> 00:18:03,679
credibility to collaboratively and

387
00:18:03,679 --> 00:18:06,240
safely submit the computation kernels of

388
00:18:06,240 --> 00:18:09,120
different training without interfering

389
00:18:09,120 --> 00:18:11,120
with games

390
00:18:11,120 --> 00:18:13,840
it has a very low overhead of preemption

391
00:18:13,840 --> 00:18:16,400
mechanism to avoid the interference from

392
00:18:16,400 --> 00:18:19,520
struggling kernels of depletion

393
00:18:19,520 --> 00:18:22,640
our evaluation shows it can harvest up

394
00:18:22,640 --> 00:18:26,160
to 85 percent of idle gpu cycles without

395
00:18:26,160 --> 00:18:29,280
interfering with games

396
00:18:30,799 --> 00:18:33,679
thanks this is enoch's talk please feel

397
00:18:33,679 --> 00:18:37,640
free to read your questions

398
00:18:44,480 --> 00:18:46,559
you

