1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:13,500 --> 00:00:16,260
hello everyone my name is Ching ho I'm

3
00:00:16,260 --> 00:00:18,420
really excited to be here to present our

4
00:00:18,420 --> 00:00:20,820
work Primo particularly young command

5
00:00:20,820 --> 00:00:23,640
system with interpretable models this is

6
00:00:23,640 --> 00:00:25,439
a joint work with the Hotshot from

7
00:00:25,439 --> 00:00:28,320
Microsoft phone Francis time and my

8
00:00:28,320 --> 00:00:30,840
supervisor Stanley and Young Gun from

9
00:00:30,840 --> 00:00:34,020
neon Technological University

10
00:00:34,020 --> 00:00:35,940
firstly I will introduce the background

11
00:00:35,940 --> 00:00:37,800
of machine learning application in

12
00:00:37,800 --> 00:00:39,360
system domain

13
00:00:39,360 --> 00:00:41,640
who is the emergence of machine learning

14
00:00:41,640 --> 00:00:43,200
research there are more and more

15
00:00:43,200 --> 00:00:45,420
developers start to employ machine

16
00:00:45,420 --> 00:00:47,579
learning techniques to optimize their

17
00:00:47,579 --> 00:00:48,600
system

18
00:00:48,600 --> 00:00:51,239
for example there are many extraordinary

19
00:00:51,239 --> 00:00:53,879
systems across storage classes

20
00:00:53,879 --> 00:00:56,579
scheduling networking and security

21
00:00:56,579 --> 00:01:00,120
fields in this system male brings

22
00:01:00,120 --> 00:01:03,980
awesome system improvements

23
00:01:04,440 --> 00:01:07,920
however although some systems are

24
00:01:07,920 --> 00:01:10,380
implemented as prototype and demonstrate

25
00:01:10,380 --> 00:01:12,600
their performance in some research test

26
00:01:12,600 --> 00:01:13,860
but

27
00:01:13,860 --> 00:01:16,860
is now shivo to shift them to production

28
00:01:16,860 --> 00:01:19,799
environment there are more concerns and

29
00:01:19,799 --> 00:01:22,320
challenges in practice

30
00:01:22,320 --> 00:01:25,140
we summarize five common barriers that

31
00:01:25,140 --> 00:01:28,680
blocks system deployment the first is

32
00:01:28,680 --> 00:01:31,439
the high chain and the chewing cost

33
00:01:31,439 --> 00:01:33,900
learning argumentative system requires

34
00:01:33,900 --> 00:01:37,020
frequent model when tuning or retraining

35
00:01:37,020 --> 00:01:40,380
to maintain the model performance

36
00:01:40,380 --> 00:01:43,560
this is because system environment is

37
00:01:43,560 --> 00:01:47,100
dynamic change by scaling up by all

38
00:01:47,100 --> 00:01:50,340
scaling down over time and the workload

39
00:01:50,340 --> 00:01:52,500
may also change and cause the

40
00:01:52,500 --> 00:01:54,780
distribution shift

41
00:01:54,780 --> 00:01:57,180
according to Microsoft operation

42
00:01:57,180 --> 00:02:00,240
experience the cost often exists

43
00:02:00,240 --> 00:02:02,460
Enterprise expectation

44
00:02:02,460 --> 00:02:06,600
and performance is in the test bat might

45
00:02:06,600 --> 00:02:09,619
not match the performance in the

46
00:02:09,619 --> 00:02:13,280
production environment

47
00:02:14,040 --> 00:02:16,500
in addition to the training process the

48
00:02:16,500 --> 00:02:17,940
influence overhead is another

49
00:02:17,940 --> 00:02:21,000
significant concern

50
00:02:21,000 --> 00:02:23,400
the common AI application lacks the face

51
00:02:23,400 --> 00:02:25,620
recognition and translation requires

52
00:02:25,620 --> 00:02:26,720
about

53
00:02:26,720 --> 00:02:29,700
million second scale influence

54
00:02:29,700 --> 00:02:33,060
uh latency however in some system

55
00:02:33,060 --> 00:02:36,900
scenarios such as storage and networking

56
00:02:36,900 --> 00:02:41,280
requires strict microsecond scale

57
00:02:41,280 --> 00:02:43,980
further in some scenarios due to the

58
00:02:43,980 --> 00:02:46,260
high concurrency there are computation

59
00:02:46,260 --> 00:02:49,980
resources is very restricted included

60
00:02:49,980 --> 00:02:54,840
including the CPU memory and the storage

61
00:02:54,840 --> 00:02:57,379
due to the strict influence overhead

62
00:02:57,379 --> 00:03:01,620
requirements of the meet some some of

63
00:03:01,620 --> 00:03:04,319
them meet the scalability issue they

64
00:03:04,319 --> 00:03:07,620
work well in test bed scale but they

65
00:03:07,620 --> 00:03:09,720
fail to handle the production scale

66
00:03:09,720 --> 00:03:12,120
environment

67
00:03:12,120 --> 00:03:15,060
further they may cause some side effects

68
00:03:15,060 --> 00:03:17,040
to the main production will close

69
00:03:17,040 --> 00:03:20,159
because Mao models occupy too much

70
00:03:20,159 --> 00:03:23,000
resources

71
00:03:23,220 --> 00:03:25,980
the third problem is insufficient data

72
00:03:25,980 --> 00:03:29,580
as we know ml is data driven and the ml

73
00:03:29,580 --> 00:03:32,159
the data volumes determines the model

74
00:03:32,159 --> 00:03:34,860
performance and the model is more

75
00:03:34,860 --> 00:03:37,620
complex more data is needed

76
00:03:37,620 --> 00:03:40,680
most systems can collect data from their

77
00:03:40,680 --> 00:03:42,799
history operation

78
00:03:42,799 --> 00:03:46,440
logs however in some scenarios with the

79
00:03:46,440 --> 00:03:48,599
data issue because the high data

80
00:03:48,599 --> 00:03:51,840
collection cost or privacy related data

81
00:03:51,840 --> 00:03:55,080
issues these blocks operators to obtain

82
00:03:55,080 --> 00:03:57,200
a qualified model

83
00:03:57,200 --> 00:03:59,640
a possible solution include data

84
00:03:59,640 --> 00:04:02,099
augmentation and diseases however this

85
00:04:02,099 --> 00:04:05,280
may include some into some bias and

86
00:04:05,280 --> 00:04:07,739
distribution shift and cause the

87
00:04:07,739 --> 00:04:10,260
interfere inferior performance standard

88
00:04:10,260 --> 00:04:13,760
heuristic algorithms

89
00:04:13,980 --> 00:04:15,959
to strive for external system

90
00:04:15,959 --> 00:04:17,339
performance

91
00:04:17,339 --> 00:04:21,540
most learning augmented systems rely on

92
00:04:21,540 --> 00:04:24,240
black Boss model that's the Deep neural

93
00:04:24,240 --> 00:04:25,620
networks

94
00:04:25,620 --> 00:04:29,340
this model takes some system features as

95
00:04:29,340 --> 00:04:32,280
the input data and obtain the system

96
00:04:32,280 --> 00:04:35,699
decision it seems very simple and

97
00:04:35,699 --> 00:04:39,120
convenient however the whole decision

98
00:04:39,120 --> 00:04:42,360
process is opaque it's hard to know how

99
00:04:42,360 --> 00:04:44,580
the model makes decision and should I

100
00:04:44,580 --> 00:04:47,479
trust the prediction

101
00:04:47,520 --> 00:04:50,060
on the interpret uh this is because

102
00:04:50,060 --> 00:04:53,400
interpretability is important but often

103
00:04:53,400 --> 00:04:54,660
ignored

104
00:04:54,660 --> 00:04:56,160
uh

105
00:04:56,160 --> 00:04:59,100
operators need sufficient confidence to

106
00:04:59,100 --> 00:05:02,400
deploy them however current models

107
00:05:02,400 --> 00:05:06,320
cannot provide such confidence

108
00:05:06,479 --> 00:05:09,600
another interpretability related issue

109
00:05:09,600 --> 00:05:12,660
is how to make the adjustment

110
00:05:12,660 --> 00:05:15,960
in practice the system environment where

111
00:05:15,960 --> 00:05:17,400
is the law they may have different

112
00:05:17,400 --> 00:05:19,680
skills different Hardware configuration

113
00:05:19,680 --> 00:05:22,919
one model May perform poorly in another

114
00:05:22,919 --> 00:05:26,880
environment so accordingly adjustment is

115
00:05:26,880 --> 00:05:29,400
necessary

116
00:05:29,400 --> 00:05:33,539
however most models are too complex for

117
00:05:33,539 --> 00:05:36,479
system operators to understand

118
00:05:36,479 --> 00:05:40,500
or for a 9u MLP model it's hard to

119
00:05:40,500 --> 00:05:42,539
determine the proper layers and the

120
00:05:42,539 --> 00:05:47,400
neuron configurations even to uh for ML

121
00:05:47,400 --> 00:05:48,600
expert

122
00:05:48,600 --> 00:05:51,120
so how to access the model is the

123
00:05:51,120 --> 00:05:53,940
adjustment correct always Comfort

124
00:05:53,940 --> 00:05:56,699
confused operators

125
00:05:56,699 --> 00:05:58,979
and the improper multiplication may

126
00:05:58,979 --> 00:06:03,180
cause the Real Performance degradation

127
00:06:03,180 --> 00:06:06,720
operators need guided or automatic model

128
00:06:06,720 --> 00:06:09,240
adjustment

129
00:06:09,240 --> 00:06:11,340
in summarize there are five main

130
00:06:11,340 --> 00:06:16,220
challenges in deploying models including

131
00:06:16,220 --> 00:06:19,919
cost overhead data opacity and

132
00:06:19,919 --> 00:06:23,819
adjustment so how to adjust these issues

133
00:06:23,819 --> 00:06:26,039
there are some existing Solutions

134
00:06:26,039 --> 00:06:28,919
focused on in the printing Black Box

135
00:06:28,919 --> 00:06:32,840
models makes the system more transparent

136
00:06:32,840 --> 00:06:36,960
for example limna and the d-pad focus on

137
00:06:36,960 --> 00:06:40,259
in the printing security applications

138
00:06:40,259 --> 00:06:43,919
and the Matrix apply for the networking

139
00:06:43,919 --> 00:06:46,800
systems this works create another

140
00:06:46,800 --> 00:06:49,139
surrogate model to explain the original

141
00:06:49,139 --> 00:06:51,620
model

142
00:06:53,240 --> 00:06:55,979
and the third grade model is the typical

143
00:06:55,979 --> 00:06:59,280
is the simple interpretable models

144
00:06:59,280 --> 00:07:02,460
however this approach how significant

145
00:07:02,460 --> 00:07:05,720
limitations they can provide individual

146
00:07:05,720 --> 00:07:09,300
explanation for individual predictions

147
00:07:09,300 --> 00:07:12,180
however they fail to explain the entire

148
00:07:12,180 --> 00:07:13,199
model

149
00:07:13,199 --> 00:07:16,319
more importantly they met the attack

150
00:07:16,319 --> 00:07:19,319
cannot provide Fidelity guarantee for

151
00:07:19,319 --> 00:07:21,479
their interpretation there are

152
00:07:21,479 --> 00:07:25,020
interpretation is are unreliable and

153
00:07:25,020 --> 00:07:28,380
maybe misleading

154
00:07:29,160 --> 00:07:32,160
the moreover they only consider the

155
00:07:32,160 --> 00:07:34,680
interpretation they failed to handle

156
00:07:34,680 --> 00:07:37,199
other challenges including the chain

157
00:07:37,199 --> 00:07:39,900
cost influence overhead and insufficient

158
00:07:39,900 --> 00:07:41,340
data issues

159
00:07:41,340 --> 00:07:44,160
so if there are any solutions can solve

160
00:07:44,160 --> 00:07:45,599
all the challenges

161
00:07:45,599 --> 00:07:48,599
our answer is yes

162
00:07:48,599 --> 00:07:51,360
our idea is quite simple instead of for

163
00:07:51,360 --> 00:07:53,759
interpreting black Boss model why not

164
00:07:53,759 --> 00:07:55,620
directly adopt and optimize

165
00:07:55,620 --> 00:07:57,900
interpretable model

166
00:07:57,900 --> 00:07:59,699
common interpreting models are very

167
00:07:59,699 --> 00:08:02,460
simple like the linear regression

168
00:08:02,460 --> 00:08:05,460
Logistics regression and decision tree

169
00:08:05,460 --> 00:08:08,479
by replacing the original model we can

170
00:08:08,479 --> 00:08:12,599
obtain a incredible benefits

171
00:08:12,599 --> 00:08:15,300
first this model are inherently in

172
00:08:15,300 --> 00:08:19,199
intelligible they can provide uh we can

173
00:08:19,199 --> 00:08:21,539
get rich information from the model and

174
00:08:21,539 --> 00:08:25,259
their interpolation are reliable

175
00:08:25,259 --> 00:08:27,539
besides this model are simple and

176
00:08:27,539 --> 00:08:29,879
lightweight they can significantly

177
00:08:29,879 --> 00:08:32,940
reduce the chain cost influence overhead

178
00:08:32,940 --> 00:08:37,940
and the data requirements

179
00:08:38,000 --> 00:08:41,099
or we understand the major concern of

180
00:08:41,099 --> 00:08:43,679
integrine model is there are model

181
00:08:43,679 --> 00:08:46,140
performance so if there are any

182
00:08:46,140 --> 00:08:47,459
trade-off between the model

183
00:08:47,459 --> 00:08:51,240
interpretability and the model accuracy

184
00:08:51,240 --> 00:08:54,240
for our observation interpreter model

185
00:08:54,240 --> 00:08:58,200
will not sacrifice model accuracy

186
00:08:58,200 --> 00:09:02,700
for for the input feature aspect common

187
00:09:02,700 --> 00:09:06,380
AI application typically adopts a pixel

188
00:09:06,380 --> 00:09:09,959
image pixel and water abandoned as the

189
00:09:09,959 --> 00:09:13,320
input they are very high dimensional and

190
00:09:13,320 --> 00:09:14,399
complex

191
00:09:14,399 --> 00:09:18,300
on the country uh ml based system adopts

192
00:09:18,300 --> 00:09:21,060
some system States like the network

193
00:09:21,060 --> 00:09:25,399
speed and the workload features like the

194
00:09:25,399 --> 00:09:28,440
memory consumption as the input feature

195
00:09:28,440 --> 00:09:30,360
they are meaningful and the lower

196
00:09:30,360 --> 00:09:32,519
dimension

197
00:09:32,519 --> 00:09:36,120
uh from the mode of skill aspect common

198
00:09:36,120 --> 00:09:39,120
area applications contains over 10

199
00:09:39,120 --> 00:09:41,820
layers with Millions to billions

200
00:09:41,820 --> 00:09:43,260
parameters

201
00:09:43,260 --> 00:09:44,640
however

202
00:09:44,640 --> 00:09:47,220
as summarized by last year's income

203
00:09:47,220 --> 00:09:50,060
paper the common are based system

204
00:09:50,060 --> 00:09:54,000
contains less than 10 000 neurons there

205
00:09:54,000 --> 00:09:56,880
are much have much slower a smaller

206
00:09:56,880 --> 00:10:00,300
scale and the latency sensor

207
00:10:00,300 --> 00:10:03,360
printable mode or how a comparable

208
00:10:03,360 --> 00:10:06,600
performance and the less overhead hence

209
00:10:06,600 --> 00:10:08,279
they are more suitable for system

210
00:10:08,279 --> 00:10:11,279
scenario

211
00:10:11,519 --> 00:10:14,339
or based on our observation we design

212
00:10:14,339 --> 00:10:16,920
Primo file based interpretable model

213
00:10:16,920 --> 00:10:20,580
optimization to assist the developers to

214
00:10:20,580 --> 00:10:23,100
design and optimize the argument system

215
00:10:23,100 --> 00:10:25,800
with interpretable models

216
00:10:25,800 --> 00:10:29,220
our objective is to achieve transparent

217
00:10:29,220 --> 00:10:31,380
accurate and language learning

218
00:10:31,380 --> 00:10:34,339
augmentation system

219
00:10:34,440 --> 00:10:36,720
or we find there are aware of the

220
00:10:36,720 --> 00:10:39,420
systems or we based on their system

221
00:10:39,420 --> 00:10:42,300
requirement and application scenario we

222
00:10:42,300 --> 00:10:45,240
divide them into two categories for the

223
00:10:45,240 --> 00:10:47,880
online systems they need to make the

224
00:10:47,880 --> 00:10:50,940
remote promote the response to real-time

225
00:10:50,940 --> 00:10:52,800
data in addition to the model

226
00:10:52,800 --> 00:10:55,260
performance they also need to consider

227
00:10:55,260 --> 00:10:58,620
the latency and computation resources

228
00:10:58,620 --> 00:11:02,459
as for the uh offline system they have

229
00:11:02,459 --> 00:11:05,640
no latency requirements they only need

230
00:11:05,640 --> 00:11:08,579
to focus on improving their uh

231
00:11:08,579 --> 00:11:10,560
performance

232
00:11:10,560 --> 00:11:12,980
to this end Primo support various

233
00:11:12,980 --> 00:11:15,480
interpretable models

234
00:11:15,480 --> 00:11:18,720
besides Primo optimize both the model

235
00:11:18,720 --> 00:11:21,360
training and the post-processing stage

236
00:11:21,360 --> 00:11:25,500
of for design system

237
00:11:27,120 --> 00:11:31,320
or in the chain stage currently Primo

238
00:11:31,320 --> 00:11:34,560
supports two types of the interpretable

239
00:11:34,560 --> 00:11:35,640
models

240
00:11:35,640 --> 00:11:39,000
the first is puyan addictive based

241
00:11:39,000 --> 00:11:41,940
method I did as depicted by the diagram

242
00:11:41,940 --> 00:11:45,060
is actually the summation of a series

243
00:11:45,060 --> 00:11:48,779
simple you know a uni wire rate or by

244
00:11:48,779 --> 00:11:51,000
Wireless shape functions so we can

245
00:11:51,000 --> 00:11:53,399
clearly understand each feature's

246
00:11:53,399 --> 00:11:56,519
contribution to the final prediction

247
00:11:56,519 --> 00:11:59,760
this model is mainly designed to achieve

248
00:11:59,760 --> 00:12:02,760
a better prediction accuracy

249
00:12:02,760 --> 00:12:05,760
the second model is predity the student

250
00:12:05,760 --> 00:12:07,620
tree based in essence

251
00:12:07,620 --> 00:12:10,740
and showing the finger each decision can

252
00:12:10,740 --> 00:12:13,860
be clear visualized due to its extremely

253
00:12:13,860 --> 00:12:18,240
uh extreme simple structure it is more

254
00:12:18,240 --> 00:12:20,579
suitable for online systems with strict

255
00:12:20,579 --> 00:12:22,980
latency and computation resource

256
00:12:22,980 --> 00:12:24,899
constraints

257
00:12:24,899 --> 00:12:29,579
we also use the bias optimization to

258
00:12:29,579 --> 00:12:33,660
automatically search for optimal model

259
00:12:33,660 --> 00:12:36,600
configuration it can quickly find the

260
00:12:36,600 --> 00:12:40,500
accurate and simple model

261
00:12:40,500 --> 00:12:44,459
uh to support more General systems we

262
00:12:44,459 --> 00:12:47,339
design the steel engine to mimic the

263
00:12:47,339 --> 00:12:50,519
behavior of the original model it can

264
00:12:50,519 --> 00:12:54,500
well support i o based system

265
00:12:55,200 --> 00:12:57,839
beyond the training process we also

266
00:12:57,839 --> 00:12:59,600
support

267
00:12:59,600 --> 00:13:02,040
post-processing optimization

268
00:13:02,040 --> 00:13:05,579
Northeast is a optional uh step because

269
00:13:05,579 --> 00:13:09,300
the trend model typically already sets

270
00:13:09,300 --> 00:13:12,180
or obtained the satisfactory performance

271
00:13:12,180 --> 00:13:14,519
we designed two possible processing

272
00:13:14,519 --> 00:13:19,459
tools the first is monotonic constraints

273
00:13:20,060 --> 00:13:22,620
constraints to the shape functions in

274
00:13:22,620 --> 00:13:26,579
the 3dt mode pre and model based on a

275
00:13:26,579 --> 00:13:28,260
pionology

276
00:13:28,260 --> 00:13:31,980
or this can achieve an automatic model

277
00:13:31,980 --> 00:13:36,480
adjustment and fix the noisy data issues

278
00:13:36,480 --> 00:13:39,420
we also provide a contextual explanation

279
00:13:39,420 --> 00:13:44,579
for guided mode adjustments to SM to

280
00:13:44,579 --> 00:13:47,519
find them smaller or the official value

281
00:13:47,519 --> 00:13:49,740
change

282
00:13:49,740 --> 00:13:53,160
in the evaluations and part we perform

283
00:13:53,160 --> 00:13:57,180
three case studies to illustrate how

284
00:13:57,180 --> 00:14:01,560
Primo can optimize sorta align augmented

285
00:14:01,560 --> 00:14:02,760
system

286
00:14:02,760 --> 00:14:07,380
uh first Sesame lingos is M2 accelerate

287
00:14:07,380 --> 00:14:10,260
flash storage i o based on em prediction

288
00:14:10,260 --> 00:14:12,839
it's our online system with strict

289
00:14:12,839 --> 00:14:16,260
latency and resource constraints the

290
00:14:16,260 --> 00:14:20,339
second is correct and offline towards to

291
00:14:20,339 --> 00:14:23,100
generate smartening of loading in size

292
00:14:23,100 --> 00:14:25,579
it adopts various

293
00:14:25,579 --> 00:14:30,620
ml models the third one is the pencil is

294
00:14:30,620 --> 00:14:33,959
a i o based online video streaming

295
00:14:33,959 --> 00:14:36,360
systems

296
00:14:36,360 --> 00:14:38,399
due to the time limit we mainly talk

297
00:14:38,399 --> 00:14:41,660
about the first case

298
00:14:41,760 --> 00:14:46,019
the idea of laying OS is quite intuitive

299
00:14:46,019 --> 00:14:48,620
it collects the

300
00:14:48,620 --> 00:14:52,199
Q length and the latency of recent i o

301
00:14:52,199 --> 00:14:56,940
operations and form a 31 layers or 31

302
00:14:56,940 --> 00:14:59,100
input features

303
00:14:59,100 --> 00:15:03,240
uh this on your features uh possessed by

304
00:15:03,240 --> 00:15:06,660
a three layer MLP model containing over

305
00:15:06,660 --> 00:15:09,660
8 000 parameters

306
00:15:09,660 --> 00:15:12,920
then the model classify each uh

307
00:15:12,920 --> 00:15:16,019
classified current IO operation into

308
00:15:16,019 --> 00:15:19,019
fast or slow

309
00:15:19,019 --> 00:15:23,220
our Primo uh our Primo implementation is

310
00:15:23,220 --> 00:15:25,760
quite similar with the

311
00:15:25,760 --> 00:15:28,380
base of the valuation we find it with

312
00:15:28,380 --> 00:15:31,440
only three input features without any

313
00:15:31,440 --> 00:15:33,839
pre-processing are sufficient for these

314
00:15:33,839 --> 00:15:36,959
stocks and we obtain the land decision

315
00:15:36,959 --> 00:15:39,839
tree model from the tree model with this

316
00:15:39,839 --> 00:15:42,660
stream model is very simple with only

317
00:15:42,660 --> 00:15:46,380
four layers only seven leaves from the

318
00:15:46,380 --> 00:15:48,839
tree model we can clearly understand how

319
00:15:48,839 --> 00:15:52,339
the model makes tradition for each uh

320
00:15:52,339 --> 00:15:56,339
makes the decision for each prediction

321
00:15:56,339 --> 00:16:00,660
we perform the performance analyze front

322
00:16:00,660 --> 00:16:04,519
finger is the CDF curves of

323
00:16:04,519 --> 00:16:09,959
Primo lean OS and original or Linux we

324
00:16:09,959 --> 00:16:13,740
find a Primo out from other baselines

325
00:16:13,740 --> 00:16:16,860
and for the average IO latency

326
00:16:16,860 --> 00:16:22,260
primary 2.5 better uh compared to the

327
00:16:22,260 --> 00:16:24,199
lingos

328
00:16:24,199 --> 00:16:27,120
and the tail performance is also

329
00:16:27,120 --> 00:16:29,339
critical for the system

330
00:16:29,339 --> 00:16:33,000
the bottom finger shows the tail

331
00:16:33,000 --> 00:16:35,120
percentile of

332
00:16:35,120 --> 00:16:40,339
of the pale test and tile tail Personnel

333
00:16:40,339 --> 00:16:43,980
latency we find the Primo can achieve up

334
00:16:43,980 --> 00:16:48,480
to 7.9 times reduction compared to lean

335
00:16:48,480 --> 00:16:50,579
OS

336
00:16:50,579 --> 00:16:53,699
oh we've performed the effective analyze

337
00:16:53,699 --> 00:16:56,160
to understand why the where the band

338
00:16:56,160 --> 00:16:57,779
phase comes from

339
00:16:57,779 --> 00:16:59,820
the influence overhead

340
00:16:59,820 --> 00:17:02,880
the lean OS need to do the data

341
00:17:02,880 --> 00:17:06,119
processing and if the inference for each

342
00:17:06,119 --> 00:17:10,079
i o and we find in some case the Leos

343
00:17:10,079 --> 00:17:12,540
May induce

344
00:17:12,540 --> 00:17:16,260
even a higher latency than the SSD

345
00:17:16,260 --> 00:17:18,119
access

346
00:17:18,119 --> 00:17:21,720
however for pre-mode it's a need to

347
00:17:21,720 --> 00:17:25,559
perform only up to four times if else

348
00:17:25,559 --> 00:17:28,079
condition test is extremely lightweight

349
00:17:28,079 --> 00:17:31,260
and from the finger we can clearly see

350
00:17:31,260 --> 00:17:33,960
the Primo inference overhead is

351
00:17:33,960 --> 00:17:36,539
ignorable

352
00:17:36,539 --> 00:17:40,380
besides uh floating points are not well

353
00:17:40,380 --> 00:17:44,340
supported in Linux kernel or the model

354
00:17:44,340 --> 00:17:47,039
waste of lingos and the thresh value

355
00:17:47,039 --> 00:17:49,799
threshold value of Primo need to be

356
00:17:49,799 --> 00:17:52,320
converted into integers through

357
00:17:52,320 --> 00:17:54,960
quantization we find Primo have no

358
00:17:54,960 --> 00:17:57,559
degradation and the two better have

359
00:17:57,559 --> 00:18:00,120
better accuracy

360
00:18:00,120 --> 00:18:03,419
we also analyze the robustness we find

361
00:18:03,419 --> 00:18:05,120
the Primo can

362
00:18:05,120 --> 00:18:09,059
are more stable in the to the perturbed

363
00:18:09,059 --> 00:18:11,460
inputs

364
00:18:11,460 --> 00:18:14,039
or there are more evaluation in our

365
00:18:14,039 --> 00:18:17,100
paper such as adding monotonic

366
00:18:17,100 --> 00:18:19,559
constraints of two character and use

367
00:18:19,559 --> 00:18:22,320
these steel engines to pencil or systems

368
00:18:22,320 --> 00:18:26,059
which achieve better performance

369
00:18:26,400 --> 00:18:28,500
or for more details and the result

370
00:18:28,500 --> 00:18:31,020
please refer to our paper

371
00:18:31,020 --> 00:18:34,860
as a summarize first language noun is

372
00:18:34,860 --> 00:18:37,380
non-trivial to deploy the argument

373
00:18:37,380 --> 00:18:40,799
system in practice we may meet a serious

374
00:18:40,799 --> 00:18:42,299
challenges

375
00:18:42,299 --> 00:18:44,760
to adjust these issues a simple

376
00:18:44,760 --> 00:18:46,799
Enterprise model are excellent choices

377
00:18:46,799 --> 00:18:48,720
we demonstrate they can about

378
00:18:48,720 --> 00:18:51,120
performance three total environment

379
00:18:51,120 --> 00:18:54,059
system including link OS Clara and

380
00:18:54,059 --> 00:18:55,799
defensive

381
00:18:55,799 --> 00:18:58,140
and the operator needs to automatically

382
00:18:58,140 --> 00:19:00,240
and guide the motor model of

383
00:19:00,240 --> 00:19:02,460
optimization to achieve better system

384
00:19:02,460 --> 00:19:04,380
performance

385
00:19:04,380 --> 00:19:06,960
our code is open source please check our

386
00:19:06,960 --> 00:19:09,000
GitHub website

387
00:19:09,000 --> 00:19:11,340
thanks for your listening we are welcome

388
00:19:11,340 --> 00:19:14,360
for any questions

