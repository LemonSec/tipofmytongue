1
00:00:12,960 --> 00:00:14,820
it was Ethan Johnson and I'm a PhD

2
00:00:14,820 --> 00:00:16,199
candidate at the University of Rochester

3
00:00:16,199 --> 00:00:19,260
doing research in in system security and

4
00:00:19,260 --> 00:00:20,820
today I'm going to be talking about some

5
00:00:20,820 --> 00:00:22,199
recent work our group has done in

6
00:00:22,199 --> 00:00:23,640
building Advanced security hardening

7
00:00:23,640 --> 00:00:25,560
into hypervisors using compiler-based

8
00:00:25,560 --> 00:00:28,260
virtual instruction set techniques

9
00:00:28,260 --> 00:00:29,820
now most of you in this audience

10
00:00:29,820 --> 00:00:31,439
probably have at least passing

11
00:00:31,439 --> 00:00:32,640
familiarity with the concept of a

12
00:00:32,640 --> 00:00:34,800
hypervisor or also known as a virtual

13
00:00:34,800 --> 00:00:37,620
machine monitor and what hypervisors are

14
00:00:37,620 --> 00:00:39,120
is they're the lowest level of system

15
00:00:39,120 --> 00:00:41,700
software on Modern computers

16
00:00:41,700 --> 00:00:43,680
they run beneath the operating system or

17
00:00:43,680 --> 00:00:45,120
sometimes inside a more privileged

18
00:00:45,120 --> 00:00:47,460
kernel known as a host and lets you run

19
00:00:47,460 --> 00:00:49,620
multiple guest operating systems well

20
00:00:49,620 --> 00:00:51,059
completely independent kernels and user

21
00:00:51,059 --> 00:00:52,860
space domains Each of which can act as

22
00:00:52,860 --> 00:00:54,300
though it has the physical Hardware all

23
00:00:54,300 --> 00:00:55,559
to itself

24
00:00:55,559 --> 00:00:57,840
but in reality the hypervisor is sharing

25
00:00:57,840 --> 00:00:59,940
the singular physical machine between

26
00:00:59,940 --> 00:01:01,500
all of those virtual machines and

27
00:01:01,500 --> 00:01:03,059
mediating their Hardware access to

28
00:01:03,059 --> 00:01:04,440
maintain that illusion when context

29
00:01:04,440 --> 00:01:06,600
switching between them

30
00:01:06,600 --> 00:01:08,580
now security is very important for

31
00:01:08,580 --> 00:01:10,200
hypervisors because the intent is for

32
00:01:10,200 --> 00:01:11,700
that illusion of being alone on a

33
00:01:11,700 --> 00:01:13,020
machine to act as a hard security

34
00:01:13,020 --> 00:01:14,040
boundary

35
00:01:14,040 --> 00:01:15,780
virtualization is used in cloud

36
00:01:15,780 --> 00:01:17,700
computing to isolate mutually

37
00:01:17,700 --> 00:01:19,140
distrustful tenants who are renting

38
00:01:19,140 --> 00:01:20,700
Hardware time on the same machine and

39
00:01:20,700 --> 00:01:22,500
also for defense in depth even when you

40
00:01:22,500 --> 00:01:24,180
have all the guests on a machine owned

41
00:01:24,180 --> 00:01:25,560
by the same party

42
00:01:25,560 --> 00:01:27,360
so if an attacker is able to exploit

43
00:01:27,360 --> 00:01:28,979
vulnerabilities in the hypervisor to

44
00:01:28,979 --> 00:01:31,860
itself to gain control over it result is

45
00:01:31,860 --> 00:01:34,140
typically catastrophic if that happens

46
00:01:34,140 --> 00:01:35,939
all of the tenants operating systems and

47
00:01:35,939 --> 00:01:37,259
applications that are on that physical

48
00:01:37,259 --> 00:01:41,299
machine are now fully compromised

49
00:01:41,579 --> 00:01:43,799
hypervisor security has a lot in common

50
00:01:43,799 --> 00:01:45,960
with OS Kernel Security as was

51
00:01:45,960 --> 00:01:47,460
originally the case with OS kernels

52
00:01:47,460 --> 00:01:50,040
years ago hypervisors were designed to

53
00:01:50,040 --> 00:01:51,659
run in the Hardware's highest privileged

54
00:01:51,659 --> 00:01:53,820
mode as the ultimate Arbiter between

55
00:01:53,820 --> 00:01:55,680
hardware and potentially untrustworthy

56
00:01:55,680 --> 00:01:57,240
software applications

57
00:01:57,240 --> 00:01:59,220
but even though hypervisors are orders

58
00:01:59,220 --> 00:02:00,659
of magnitude simpler and more

59
00:02:00,659 --> 00:02:02,340
streamlined than OS kernels they're

60
00:02:02,340 --> 00:02:04,979
still large monolithic code bases that

61
00:02:04,979 --> 00:02:07,560
in practice have proven too complex

62
00:02:07,560 --> 00:02:09,479
to be trustworthy solely on the basis of

63
00:02:09,479 --> 00:02:11,400
careful implementation

64
00:02:11,400 --> 00:02:12,900
just like operating system kernels

65
00:02:12,900 --> 00:02:14,280
hypervisors are typically written in

66
00:02:14,280 --> 00:02:16,800
unsafe Legacy languages like C and are

67
00:02:16,800 --> 00:02:18,300
subject to the same endemic memory

68
00:02:18,300 --> 00:02:19,680
corruption vulnerabilities that can

69
00:02:19,680 --> 00:02:21,000
provide attackers with full system

70
00:02:21,000 --> 00:02:21,959
control

71
00:02:21,959 --> 00:02:23,160
so even though they might have

72
00:02:23,160 --> 00:02:24,840
vulnerabilities reported less often than

73
00:02:24,840 --> 00:02:27,239
OS kernels due to reduced complexity it

74
00:02:27,239 --> 00:02:28,620
still only takes one vulnerability to

75
00:02:28,620 --> 00:02:30,300
bring down an entire system

76
00:02:30,300 --> 00:02:32,580
so for a well-resourced attacker

77
00:02:32,580 --> 00:02:34,500
the stakes of being able to compromise

78
00:02:34,500 --> 00:02:36,120
an entire cloud system are high enough

79
00:02:36,120 --> 00:02:37,980
to make it worthwhile to be go hunting

80
00:02:37,980 --> 00:02:41,238
for these super vulnerabilities

81
00:02:41,580 --> 00:02:42,780
over the years there's been a lot of

82
00:02:42,780 --> 00:02:44,280
great ideas for posts for how to make

83
00:02:44,280 --> 00:02:45,660
hypervisors more resilient and attack

84
00:02:45,660 --> 00:02:47,519
against attack and the one we're going

85
00:02:47,519 --> 00:02:48,959
to focus on today they'll be familiar to

86
00:02:48,959 --> 00:02:50,340
a lot of you is called control flow

87
00:02:50,340 --> 00:02:52,200
Integrity or CFI

88
00:02:52,200 --> 00:02:54,540
CFI is a hardening approach if it's

89
00:02:54,540 --> 00:02:55,860
designed to make it more difficult to

90
00:02:55,860 --> 00:02:57,360
exploit memory safety vulnerabilities

91
00:02:57,360 --> 00:03:00,420
what CFI does is it adds checks to a

92
00:03:00,420 --> 00:03:02,220
program that restricts an attacker's

93
00:03:02,220 --> 00:03:03,959
ability to corrupt an application's

94
00:03:03,959 --> 00:03:05,760
control flow through things like return

95
00:03:05,760 --> 00:03:08,220
addresses and function pointers

96
00:03:08,220 --> 00:03:09,780
but in order to do control full

97
00:03:09,780 --> 00:03:12,000
Integrity effectively on a hypervisor we

98
00:03:12,000 --> 00:03:13,920
have to address two main challenges

99
00:03:13,920 --> 00:03:16,019
first off we need to be able to defend

100
00:03:16,019 --> 00:03:17,580
against the advanced types of code reuse

101
00:03:17,580 --> 00:03:18,659
attacks that have been developed over

102
00:03:18,659 --> 00:03:20,640
the last decade or so attackers have

103
00:03:20,640 --> 00:03:22,200
moved well beyond the classic return

104
00:03:22,200 --> 00:03:24,300
oriented programming both in the

105
00:03:24,300 --> 00:03:25,500
research literature and in real world

106
00:03:25,500 --> 00:03:27,420
attacks to much more subtle techniques

107
00:03:27,420 --> 00:03:29,159
that can evade the restrictions imposed

108
00:03:29,159 --> 00:03:31,680
by CFI enforcement

109
00:03:31,680 --> 00:03:33,599
and one thing that that recent research

110
00:03:33,599 --> 00:03:36,000
has made very clear is that a modern CFI

111
00:03:36,000 --> 00:03:37,739
implementation needs to protect both

112
00:03:37,739 --> 00:03:39,720
forward and reverse control flow edges

113
00:03:39,720 --> 00:03:41,459
and that in particular the reverse Edge

114
00:03:41,459 --> 00:03:43,560
needs to be fully dynamic

115
00:03:43,560 --> 00:03:45,480
a static label-based scheme that doesn't

116
00:03:45,480 --> 00:03:46,980
take into account Dynamic return address

117
00:03:46,980 --> 00:03:49,739
context is simply not precise enough to

118
00:03:49,739 --> 00:03:52,080
limit an attacker's Gadget set to enough

119
00:03:52,080 --> 00:03:54,659
to prevent useful malicious computation

120
00:03:54,659 --> 00:03:56,280
practically speaking this means that

121
00:03:56,280 --> 00:03:57,599
something like a shadow stack is going

122
00:03:57,599 --> 00:03:58,860
to be needed in order to protect return

123
00:03:58,860 --> 00:04:00,360
addresses from being corrupted in the

124
00:04:00,360 --> 00:04:01,620
first place

125
00:04:01,620 --> 00:04:03,900
now prior work has implemented static

126
00:04:03,900 --> 00:04:06,599
label-based CFI and hypervisors but it

127
00:04:06,599 --> 00:04:08,159
hasn't been able to provide Shadow stack

128
00:04:08,159 --> 00:04:09,540
protection for return addresses without

129
00:04:09,540 --> 00:04:11,640
Major Performance penalties of over 300

130
00:04:11,640 --> 00:04:14,519
percent and this brings us to the first

131
00:04:14,519 --> 00:04:16,019
major contribution of our work we're

132
00:04:16,019 --> 00:04:17,760
presenting today what we have done is we

133
00:04:17,760 --> 00:04:19,560
have successfully implemented a shadow

134
00:04:19,560 --> 00:04:21,180
stack for fully context sensitive

135
00:04:21,180 --> 00:04:23,520
reverse Edge CFI on the popular Zen

136
00:04:23,520 --> 00:04:24,960
hypervisor and we've done it with near

137
00:04:24,960 --> 00:04:28,159
Zero Performance impact

138
00:04:28,680 --> 00:04:30,479
the other contribution we've made is

139
00:04:30,479 --> 00:04:31,919
that our protection is complete and

140
00:04:31,919 --> 00:04:32,940
we're going to talk more about what that

141
00:04:32,940 --> 00:04:34,919
means next but essentially it's about

142
00:04:34,919 --> 00:04:37,560
addressing the inherent difficulty of

143
00:04:37,560 --> 00:04:39,300
securing low-level code running in the

144
00:04:39,300 --> 00:04:41,759
processor's fully privileged mode there

145
00:04:41,759 --> 00:04:43,320
there simply are no lower more

146
00:04:43,320 --> 00:04:45,060
privileged Hardware Rings running below

147
00:04:45,060 --> 00:04:46,919
the hypervisor so it's it's vital that

148
00:04:46,919 --> 00:04:48,780
our security design takes into account

149
00:04:48,780 --> 00:04:50,340
every way in which privileged

150
00:04:50,340 --> 00:04:51,840
functionality could allow a buggy

151
00:04:51,840 --> 00:04:53,400
hypervisor to escape these protections

152
00:04:53,400 --> 00:04:56,220
we've placed upon it

153
00:04:56,220 --> 00:04:57,479
the way we've addressed this challenge

154
00:04:57,479 --> 00:04:59,100
in our work is with a technique known as

155
00:04:59,100 --> 00:05:01,740
virtual instruction setting or visc and

156
00:05:01,740 --> 00:05:03,180
what that means is that instead of

157
00:05:03,180 --> 00:05:04,740
compiling the hypervisor directly to

158
00:05:04,740 --> 00:05:06,960
native code in this case x86 assembly

159
00:05:06,960 --> 00:05:08,100
that's the platform we're working on

160
00:05:08,100 --> 00:05:08,880
here

161
00:05:08,880 --> 00:05:11,280
uh what we can do instead is we target a

162
00:05:11,280 --> 00:05:12,479
slightly higher level virtual

163
00:05:12,479 --> 00:05:13,919
instruction set called the secure

164
00:05:13,919 --> 00:05:16,320
virtual architecture or which is an

165
00:05:16,320 --> 00:05:18,120
extension of the lvm compiler's

166
00:05:18,120 --> 00:05:20,100
intermediate representation or IR

167
00:05:20,100 --> 00:05:23,100
SVA takes the llvmir which is already

168
00:05:23,100 --> 00:05:25,979
used internally by the llvm by lvm-based

169
00:05:25,979 --> 00:05:28,080
compilers like clang as an intermediate

170
00:05:28,080 --> 00:05:29,759
step when you're compiling High Level C

171
00:05:29,759 --> 00:05:31,740
code down a native assembly it takes

172
00:05:31,740 --> 00:05:33,539
that as a starting point if it adds

173
00:05:33,539 --> 00:05:35,100
additional virtual instructions which

174
00:05:35,100 --> 00:05:37,500
are known as intrinsics that expose the

175
00:05:37,500 --> 00:05:38,639
kinds of low-level privileged

176
00:05:38,639 --> 00:05:40,080
functionality that you need in order to

177
00:05:40,080 --> 00:05:41,940
build an entire operating system kernel

178
00:05:41,940 --> 00:05:44,100
or hypervisor without using any raw

179
00:05:44,100 --> 00:05:46,620
native assembly code whatsoever

180
00:05:46,620 --> 00:05:49,020
and the purpose of doing this

181
00:05:49,020 --> 00:05:51,360
is that when you have a thin virtual

182
00:05:51,360 --> 00:05:53,039
instruction set layer like this between

183
00:05:53,039 --> 00:05:55,139
the hypervisor and the hardware you can

184
00:05:55,139 --> 00:05:57,000
constrain the runtime behavior of the

185
00:05:57,000 --> 00:05:58,500
hypervisor to be sure that it will

186
00:05:58,500 --> 00:05:59,820
conform with any compile time

187
00:05:59,820 --> 00:06:01,740
instrumentation or analysis that's been

188
00:06:01,740 --> 00:06:02,759
done on it

189
00:06:02,759 --> 00:06:04,979
this gives us the ability to close these

190
00:06:04,979 --> 00:06:06,240
security loopholes that would otherwise

191
00:06:06,240 --> 00:06:08,280
allow a compromised hypervisor to escape

192
00:06:08,280 --> 00:06:10,500
our CFI enforcement even though we're

193
00:06:10,500 --> 00:06:11,580
running in the highest privileged

194
00:06:11,580 --> 00:06:14,060
Hardware mode

195
00:06:14,100 --> 00:06:17,039
so while the core lvmir gives you all

196
00:06:17,039 --> 00:06:18,600
the virtual instructions you need to

197
00:06:18,600 --> 00:06:20,460
express basic computation arithmetic and

198
00:06:20,460 --> 00:06:23,460
logic control flow Etc what SVA adds to

199
00:06:23,460 --> 00:06:26,639
that excuse me and what SVA adds to that

200
00:06:26,639 --> 00:06:28,319
is the ability to do more complex

201
00:06:28,319 --> 00:06:30,780
sensitive tasks like context switches

202
00:06:30,780 --> 00:06:32,340
page table updates processor mode

203
00:06:32,340 --> 00:06:34,620
switches that sort of thing and these

204
00:06:34,620 --> 00:06:36,660
kinds of operations represent security

205
00:06:36,660 --> 00:06:38,340
loopholes in the native architecture

206
00:06:38,340 --> 00:06:40,259
because by flipping a bit or writing to

207
00:06:40,259 --> 00:06:42,180
a memory location you can completely

208
00:06:42,180 --> 00:06:43,680
redefine the processor's operating

209
00:06:43,680 --> 00:06:45,479
context in ways that high-level

210
00:06:45,479 --> 00:06:47,639
languages don't generally anticipate

211
00:06:47,639 --> 00:06:50,400
so SVA allows you to execute these

212
00:06:50,400 --> 00:06:52,319
operations using clean higher level

213
00:06:52,319 --> 00:06:53,940
abstractions so that you can Implement

214
00:06:53,940 --> 00:06:56,220
security policies like CFI without

215
00:06:56,220 --> 00:06:58,380
having that broken by unrestricted

216
00:06:58,380 --> 00:07:01,380
program state discount annuities

217
00:07:01,380 --> 00:07:05,039
now prior work based on SVA has done CFI

218
00:07:05,039 --> 00:07:07,080
on a traditional OS kernel but there are

219
00:07:07,080 --> 00:07:08,280
some additional challenges that arise

220
00:07:08,280 --> 00:07:09,840
when you want to do this on a hypervisor

221
00:07:09,840 --> 00:07:11,639
that's using a hardware virtualization

222
00:07:11,639 --> 00:07:13,139
tools like Intel's virtual machine

223
00:07:13,139 --> 00:07:15,479
extensions or amd's secure virtual

224
00:07:15,479 --> 00:07:16,620
machine

225
00:07:16,620 --> 00:07:18,300
the system we've built which we call

226
00:07:18,300 --> 00:07:20,400
Umbro extends sva's virtual instruction

227
00:07:20,400 --> 00:07:22,259
set capabilities to address these

228
00:07:22,259 --> 00:07:23,699
special wrinkles that are inherent in

229
00:07:23,699 --> 00:07:25,860
Hardware accelerated virtualization

230
00:07:25,860 --> 00:07:28,199
it's designed to be able to support both

231
00:07:28,199 --> 00:07:30,240
bare metal hypervisors like Zen which

232
00:07:30,240 --> 00:07:31,680
exist without an operating system kernel

233
00:07:31,680 --> 00:07:33,060
as well as hypervisors that are

234
00:07:33,060 --> 00:07:34,440
integrated into an OS kernel like

235
00:07:34,440 --> 00:07:36,539
linux's KVM or virtualbox's kernel

236
00:07:36,539 --> 00:07:38,699
modules

237
00:07:38,699 --> 00:07:41,160
so one of the biggest wrinkles is that

238
00:07:41,160 --> 00:07:43,560
VM entry and exit Pro is this VM entry

239
00:07:43,560 --> 00:07:46,139
and exit process itself

240
00:07:46,139 --> 00:07:48,539
hardware virtualization like Intel vmx

241
00:07:48,539 --> 00:07:50,220
or AMD svm

242
00:07:50,220 --> 00:07:52,560
let's guest operating systems run in the

243
00:07:52,560 --> 00:07:54,479
processor's ring zero kernel mode while

244
00:07:54,479 --> 00:07:55,800
still being able to trap into the

245
00:07:55,800 --> 00:07:57,300
hypervisor which runs in a higher

246
00:07:57,300 --> 00:07:59,280
privileged root mode whenever the guest

247
00:07:59,280 --> 00:08:00,479
does something that the high Professor

248
00:08:00,479 --> 00:08:02,880
needs to mediate or virtualize this

249
00:08:02,880 --> 00:08:04,020
process of switching back and forth

250
00:08:04,020 --> 00:08:05,940
between the two is known as VM entry and

251
00:08:05,940 --> 00:08:06,900
exit

252
00:08:06,900 --> 00:08:09,960
the trouble with VM entry and exit

253
00:08:09,960 --> 00:08:12,720
excuse me the trouble with VM entry and

254
00:08:12,720 --> 00:08:15,300
exit is that native isas tend to

255
00:08:15,300 --> 00:08:17,340
implement it in a very open-ended way

256
00:08:17,340 --> 00:08:18,900
and that's in a way that's very

257
00:08:18,900 --> 00:08:21,180
difficult to secure the ISA design

258
00:08:21,180 --> 00:08:23,340
assumes but the hypervisor is completely

259
00:08:23,340 --> 00:08:24,419
trustworthy and should have complete

260
00:08:24,419 --> 00:08:26,460
control over the virtualization process

261
00:08:26,460 --> 00:08:28,740
in Intel's vmx for instance amd's

262
00:08:28,740 --> 00:08:30,840
implementation is very similar to this

263
00:08:30,840 --> 00:08:32,399
when you want to enter guest mode the

264
00:08:32,399 --> 00:08:33,899
processor reads hundreds of different

265
00:08:33,899 --> 00:08:36,059
state elements from a page sized in

266
00:08:36,059 --> 00:08:37,860
memory data structure called the virtual

267
00:08:37,860 --> 00:08:40,260
machine control structure or vmcs which

268
00:08:40,260 --> 00:08:42,839
has been provided by the hypervisor and

269
00:08:42,839 --> 00:08:44,700
this structure provides values for

270
00:08:44,700 --> 00:08:46,440
things like the program counter stack

271
00:08:46,440 --> 00:08:47,820
pointer control registers segment

272
00:08:47,820 --> 00:08:49,440
registers and interrupt State stuff like

273
00:08:49,440 --> 00:08:50,220
that

274
00:08:50,220 --> 00:08:52,620
but I mean on the flip side on VM exit

275
00:08:52,620 --> 00:08:54,240
this the processor event reads all those

276
00:08:54,240 --> 00:08:56,339
fields back out from the vmcs to restore

277
00:08:56,339 --> 00:08:59,160
the hypervisor where it left off

278
00:08:59,160 --> 00:09:00,420
this

279
00:09:00,420 --> 00:09:03,120
design choice however poses a big

280
00:09:03,120 --> 00:09:04,080
problem when you're trying to do

281
00:09:04,080 --> 00:09:06,060
security hardening like CFI because

282
00:09:06,060 --> 00:09:07,440
there's nothing to keep an attacker from

283
00:09:07,440 --> 00:09:09,120
Simply overriding say the host

284
00:09:09,120 --> 00:09:11,279
instruction pointer field in the vmcs to

285
00:09:11,279 --> 00:09:13,620
just skip over a CFI check on the next

286
00:09:13,620 --> 00:09:15,600
VM exit or it doesn't even stop if I'm

287
00:09:15,600 --> 00:09:17,160
doing something crazy or like like

288
00:09:17,160 --> 00:09:18,480
disabling memory protections all

289
00:09:18,480 --> 00:09:19,800
together and control register zero so

290
00:09:19,800 --> 00:09:21,420
you end up in 16-bit real mode with no

291
00:09:21,420 --> 00:09:23,760
protections these are just some of the

292
00:09:23,760 --> 00:09:25,380
issues that have to be addressed when

293
00:09:25,380 --> 00:09:26,820
you're trying to do security hardening

294
00:09:26,820 --> 00:09:28,980
on a hypervisor which we discuss in

295
00:09:28,980 --> 00:09:30,959
detail in the paper and the idea is that

296
00:09:30,959 --> 00:09:33,180
the entire vmx instruction set

297
00:09:33,180 --> 00:09:35,100
has to be wrapped with carefully

298
00:09:35,100 --> 00:09:37,080
designed virtual instructions that

299
00:09:37,080 --> 00:09:38,940
ensure sensitive values and settings are

300
00:09:38,940 --> 00:09:40,680
coming from a trustworthy Source or

301
00:09:40,680 --> 00:09:41,880
alternatively are being vetted

302
00:09:41,880 --> 00:09:43,560
accordingly

303
00:09:43,560 --> 00:09:44,940
and that an attacker can't then

304
00:09:44,940 --> 00:09:46,440
overwrite those trusted values with

305
00:09:46,440 --> 00:09:49,100
sneaky ones

306
00:09:49,260 --> 00:09:51,480
what's interesting about this

307
00:09:51,480 --> 00:09:53,459
is that in order to do this we need to

308
00:09:53,459 --> 00:09:54,959
be able to make certain sensitive memory

309
00:09:54,959 --> 00:09:56,339
regions tamper-proof against the

310
00:09:56,339 --> 00:09:58,620
hypervisor even in the fully privileged

311
00:09:58,620 --> 00:10:00,600
root mode ring zero

312
00:10:00,600 --> 00:10:02,100
but this just happens to be the same

313
00:10:02,100 --> 00:10:03,480
problem that we need to solve in order

314
00:10:03,480 --> 00:10:05,279
to do a shadow stack for a hypervisor

315
00:10:05,279 --> 00:10:07,019
for shadow stack we need to be able to

316
00:10:07,019 --> 00:10:09,420
push and pop call return addresses

317
00:10:09,420 --> 00:10:11,100
to a section of memory that we know

318
00:10:11,100 --> 00:10:12,899
can't be otherwise messed with and we

319
00:10:12,899 --> 00:10:14,519
need to do this without slowing down the

320
00:10:14,519 --> 00:10:16,140
hypervisor or relying on Hardware

321
00:10:16,140 --> 00:10:18,300
privilege protections that simply just

322
00:10:18,300 --> 00:10:20,339
don't exist in root ring zero

323
00:10:20,339 --> 00:10:21,480
so if we could figure out a way to make

324
00:10:21,480 --> 00:10:23,040
memory regions tamper-proof from the

325
00:10:23,040 --> 00:10:24,899
hypervisor we can kill two birds with

326
00:10:24,899 --> 00:10:27,060
one stone we can close the loopholes

327
00:10:27,060 --> 00:10:28,200
where an attacker could overwrite

328
00:10:28,200 --> 00:10:29,880
sensitive in-memory data structures like

329
00:10:29,880 --> 00:10:32,339
VM exit state or page tables and then

330
00:10:32,339 --> 00:10:33,959
get a shadow stack for free by

331
00:10:33,959 --> 00:10:35,339
piggybacking on that same generic

332
00:10:35,339 --> 00:10:37,260
tamper-proofing mechanism

333
00:10:37,260 --> 00:10:39,300
now there are several different

334
00:10:39,300 --> 00:10:40,920
mechanisms that can be used to achieve

335
00:10:40,920 --> 00:10:42,839
this kind of intra-address space memory

336
00:10:42,839 --> 00:10:44,880
protection and in some respects the

337
00:10:44,880 --> 00:10:46,260
exact mechanism that we choose to use

338
00:10:46,260 --> 00:10:48,300
for that is orthogonal to a broader

339
00:10:48,300 --> 00:10:49,560
virtual instruction set design like

340
00:10:49,560 --> 00:10:50,579
we're using

341
00:10:50,579 --> 00:10:52,200
since the virtual instruction set

342
00:10:52,200 --> 00:10:54,300
ensures that the hypervisor can't tamper

343
00:10:54,300 --> 00:10:56,160
with whatever Hardware controls or

344
00:10:56,160 --> 00:10:57,839
software instrumentation are used to

345
00:10:57,839 --> 00:10:59,820
enforce this protection you can use

346
00:10:59,820 --> 00:11:01,680
anything you like from segmentation and

347
00:11:01,680 --> 00:11:04,140
32-bit mode or on a modern processor to

348
00:11:04,140 --> 00:11:06,660
tools like memory protection keys or MPX

349
00:11:06,660 --> 00:11:09,120
bound checks

350
00:11:09,120 --> 00:11:10,740
yeah there's a lot of different options

351
00:11:10,740 --> 00:11:13,079
here but basically for our

352
00:11:13,079 --> 00:11:15,480
implementation we chose we chose to go

353
00:11:15,480 --> 00:11:16,560
for something called software fault

354
00:11:16,560 --> 00:11:19,200
isolation or SFI which which is a a pure

355
00:11:19,200 --> 00:11:20,880
software approach based on compile time

356
00:11:20,880 --> 00:11:22,320
instrumentation

357
00:11:22,320 --> 00:11:26,040
SFI is uh very similar to CFI in that it

358
00:11:26,040 --> 00:11:27,779
puts instrumentation on any untrusted

359
00:11:27,779 --> 00:11:29,399
loads and stores in the program it at

360
00:11:29,399 --> 00:11:31,560
what it does is it adds bit masking

361
00:11:31,560 --> 00:11:33,779
checks before every loader store to

362
00:11:33,779 --> 00:11:34,860
ensure that they aren't accessing

363
00:11:34,860 --> 00:11:36,660
prohibited memory addresses and many of

364
00:11:36,660 --> 00:11:38,279
you might be familiar with this approach

365
00:11:38,279 --> 00:11:40,260
already uh with SFI because it was used

366
00:11:40,260 --> 00:11:42,120
in Google's native client to sandbox

367
00:11:42,120 --> 00:11:44,339
memory accesses by untrusted native web

368
00:11:44,339 --> 00:11:45,959
browser extensions

369
00:11:45,959 --> 00:11:49,500
uh and SFI has has good performance and

370
00:11:49,500 --> 00:11:51,600
is straightforward to implement and so

371
00:11:51,600 --> 00:11:53,220
and we already had infrastructure for it

372
00:11:53,220 --> 00:11:54,959
from some of our prior work with the SDA

373
00:11:54,959 --> 00:11:56,100
virtual instructions set so that's why

374
00:11:56,100 --> 00:11:58,079
we chose it for ombro but from a design

375
00:11:58,079 --> 00:11:59,519
perspective we could have chosen any of

376
00:11:59,519 --> 00:12:00,959
a number of these Hardware or software

377
00:12:00,959 --> 00:12:04,339
mechanisms that could work just as well

378
00:12:04,980 --> 00:12:06,720
once you have a generic intra address

379
00:12:06,720 --> 00:12:08,459
space tamper-proofing mechanism like

380
00:12:08,459 --> 00:12:10,079
this available within the hypervisor

381
00:12:10,079 --> 00:12:11,700
it's straightforward to make a Shadow

382
00:12:11,700 --> 00:12:13,079
stack from that

383
00:12:13,079 --> 00:12:14,700
so to keep things simple we want what's

384
00:12:14,700 --> 00:12:16,320
called a split stack rather than the

385
00:12:16,320 --> 00:12:17,880
more common parallel Shadow stack design

386
00:12:17,880 --> 00:12:19,860
that's often used you often hear of in

387
00:12:19,860 --> 00:12:21,480
in Shadow stack work

388
00:12:21,480 --> 00:12:23,700
uh and a split stack is very simple

389
00:12:23,700 --> 00:12:25,680
instead of just one stack pointer and

390
00:12:25,680 --> 00:12:27,300
the RSP register that's used for both

391
00:12:27,300 --> 00:12:29,399
data and return addresses we now have

392
00:12:29,399 --> 00:12:31,079
two stack pointers one for return

393
00:12:31,079 --> 00:12:33,120
addresses and one for all your other

394
00:12:33,120 --> 00:12:35,579
data and in our prototype we we continue

395
00:12:35,579 --> 00:12:37,740
to use RSP for the control stack so we

396
00:12:37,740 --> 00:12:38,760
can use standard call and return

397
00:12:38,760 --> 00:12:40,860
instructions whereas through the data

398
00:12:40,860 --> 00:12:42,300
stack we just picked R15 as a free

399
00:12:42,300 --> 00:12:45,360
register you can pick anything uh and

400
00:12:45,360 --> 00:12:47,399
this is very similar if somebody might

401
00:12:47,399 --> 00:12:49,139
be familiar with the llvm safe stack

402
00:12:49,139 --> 00:12:51,360
option uh it's very similar to that what

403
00:12:51,360 --> 00:12:54,000
it's which lvm safe stack splits the

404
00:12:54,000 --> 00:12:55,200
control and datastack but doesn't by

405
00:12:55,200 --> 00:12:56,880
itself provide a tamper protection

406
00:12:56,880 --> 00:12:58,440
mechanism for the control stack so

407
00:12:58,440 --> 00:13:01,560
that's the missing piece we fill in here

408
00:13:01,560 --> 00:13:04,079
what makes this approach work is that

409
00:13:04,079 --> 00:13:05,940
our compiler pass that puts the software

410
00:13:05,940 --> 00:13:07,680
fault isolation checks on every load and

411
00:13:07,680 --> 00:13:09,959
store in the hypervisor we can configure

412
00:13:09,959 --> 00:13:12,360
that pass to Simply skip instrumenting

413
00:13:12,360 --> 00:13:13,560
calls and returns that have been

414
00:13:13,560 --> 00:13:15,180
generated by the compiler

415
00:13:15,180 --> 00:13:16,800
since those calls and returns always

416
00:13:16,800 --> 00:13:19,079
push and pop the stack pointer in a

417
00:13:19,079 --> 00:13:21,240
strictly nested order using static

418
00:13:21,240 --> 00:13:23,100
offsets that we know are safe

419
00:13:23,100 --> 00:13:24,720
we can know that the compiler will never

420
00:13:24,720 --> 00:13:26,399
generate calls and returns that an

421
00:13:26,399 --> 00:13:27,779
attacker could use to access memory

422
00:13:27,779 --> 00:13:29,279
unsafely

423
00:13:29,279 --> 00:13:31,320
thus the only way an attacker could

424
00:13:31,320 --> 00:13:33,480
corrupt a stack is by using a general

425
00:13:33,480 --> 00:13:35,220
store instruction and those are all

426
00:13:35,220 --> 00:13:37,680
subject to our SFI checks which won't

427
00:13:37,680 --> 00:13:39,180
allow him to touch the memory where the

428
00:13:39,180 --> 00:13:41,040
control Stacks live

429
00:13:41,040 --> 00:13:42,360
one of the really nice things about

430
00:13:42,360 --> 00:13:44,100
doing this on a hypervisor is that we

431
00:13:44,100 --> 00:13:45,240
don't need to worry much about things

432
00:13:45,240 --> 00:13:46,920
like calling conventions since

433
00:13:46,920 --> 00:13:48,540
hypervisors are typically monolithic

434
00:13:48,540 --> 00:13:51,120
Executives they're compiled all at once

435
00:13:51,120 --> 00:13:52,440
so everything's guaranteed to be playing

436
00:13:52,440 --> 00:13:54,300
by the same rules

437
00:13:54,300 --> 00:13:55,740
there's never any possibility that

438
00:13:55,740 --> 00:13:57,060
you'll be calling out into code that

439
00:13:57,060 --> 00:13:58,740
hasn't been built for the split stack or

440
00:13:58,740 --> 00:14:00,779
is missing CFI or SFI instrumentation

441
00:14:00,779 --> 00:14:02,760
and even in situations where we might

442
00:14:02,760 --> 00:14:04,680
need to support limited forms of of

443
00:14:04,680 --> 00:14:06,779
dynamic loading like kernel modules the

444
00:14:06,779 --> 00:14:08,399
SVA virtual instruction set would

445
00:14:08,399 --> 00:14:09,899
require all of those to be provided in

446
00:14:09,899 --> 00:14:12,420
lvmir form rather than in native code

447
00:14:12,420 --> 00:14:14,459
meaning that the compiler back end is

448
00:14:14,459 --> 00:14:15,720
still going to have the opportunity to

449
00:14:15,720 --> 00:14:17,100
enforce any system-specific

450
00:14:17,100 --> 00:14:18,899
instrumentation policies on any new code

451
00:14:18,899 --> 00:14:21,500
that's coming in

452
00:14:22,019 --> 00:14:23,760
so you're probably wondering after all

453
00:14:23,760 --> 00:14:24,839
this what's the cost of these

454
00:14:24,839 --> 00:14:27,060
protections and what's really great is

455
00:14:27,060 --> 00:14:28,920
there's actually very little cost

456
00:14:28,920 --> 00:14:30,660
to evaluate almost performance what we

457
00:14:30,660 --> 00:14:32,519
did is we ported the open source Zen

458
00:14:32,519 --> 00:14:34,800
4.12 hypervisor which is one of the most

459
00:14:34,800 --> 00:14:36,360
popular hypervisors used in the real

460
00:14:36,360 --> 00:14:38,100
world and real world Enterprise grade

461
00:14:38,100 --> 00:14:39,480
cloud computing

462
00:14:39,480 --> 00:14:41,040
to the SDA virtual structures that we

463
00:14:41,040 --> 00:14:42,420
ported that and then we implemented

464
00:14:42,420 --> 00:14:46,079
omro's CFI SFI and split stack and filer

465
00:14:46,079 --> 00:14:47,760
Transformations on that

466
00:14:47,760 --> 00:14:50,040
we then tested that version of Zen

467
00:14:50,040 --> 00:14:51,779
against a battery of end-to-end macro

468
00:14:51,779 --> 00:14:54,240
macro benchmarks running inside a domu

469
00:14:54,240 --> 00:14:55,920
guest VM along with some selected

470
00:14:55,920 --> 00:14:57,540
virtualization micro benchmarks that we

471
00:14:57,540 --> 00:14:58,860
created in order to explore those

472
00:14:58,860 --> 00:15:00,060
performance characteristics in more

473
00:15:00,060 --> 00:15:01,019
detail

474
00:15:01,019 --> 00:15:03,540
now our experimental setup is summarized

475
00:15:03,540 --> 00:15:05,279
here on the slide and discussed in much

476
00:15:05,279 --> 00:15:08,040
more detail in the paper uh at a high

477
00:15:08,040 --> 00:15:09,240
level what we did is we selected a

478
00:15:09,240 --> 00:15:10,680
benchmark Suite from the open source

479
00:15:10,680 --> 00:15:12,839
pheronics Suite specifically the pts

480
00:15:12,839 --> 00:15:14,519
kernel set since it's a good reflection

481
00:15:14,519 --> 00:15:16,440
of system heavy cloud workloads and has

482
00:15:16,440 --> 00:15:18,180
been used to evaluate performance

483
00:15:18,180 --> 00:15:20,040
regressions in the Linux kernel so kind

484
00:15:20,040 --> 00:15:21,899
of what we're looking for here basically

485
00:15:21,899 --> 00:15:23,220
what we wanted was something that would

486
00:15:23,220 --> 00:15:25,380
be a stress test for a hypervisor with

487
00:15:25,380 --> 00:15:27,000
lots of i o and system calls because we

488
00:15:27,000 --> 00:15:28,500
didn't want we just want something that

489
00:15:28,500 --> 00:15:30,240
was computationally intensive workloads

490
00:15:30,240 --> 00:15:31,260
that wouldn't really show much

491
00:15:31,260 --> 00:15:33,240
difference from being virtualized or not

492
00:15:33,240 --> 00:15:36,240
So to that end to make the evaluation

493
00:15:36,240 --> 00:15:38,579
more meaningful we made things even

494
00:15:38,579 --> 00:15:40,560
harder on ourselves by adding memcache D

495
00:15:40,560 --> 00:15:42,000
to the list even though it's not part of

496
00:15:42,000 --> 00:15:44,760
the pts kernel Suite because we know

497
00:15:44,760 --> 00:15:45,720
that it tends to have high

498
00:15:45,720 --> 00:15:47,579
virtualization overheads due to frequent

499
00:15:47,579 --> 00:15:49,500
inter-processor communication

500
00:15:49,500 --> 00:15:51,060
so anyway we took all of these and we

501
00:15:51,060 --> 00:15:52,560
compared them to a baseline of the

502
00:15:52,560 --> 00:15:54,660
unmodified vanilla Zen that anyone can

503
00:15:54,660 --> 00:15:57,439
go download and run

504
00:15:57,779 --> 00:15:59,940
now since we ran a lot of benchmarks

505
00:15:59,940 --> 00:16:01,440
this table probably isn't very readable

506
00:16:01,440 --> 00:16:02,639
from a distance so I've highlighted the

507
00:16:02,639 --> 00:16:03,959
most important takeaways here on the

508
00:16:03,959 --> 00:16:05,160
slide

509
00:16:05,160 --> 00:16:07,079
for the vast majority of the application

510
00:16:07,079 --> 00:16:09,240
benchmarks we ran there was effectively

511
00:16:09,240 --> 00:16:10,980
zero overhead from running omro with all

512
00:16:10,980 --> 00:16:12,360
of its security protections compared to

513
00:16:12,360 --> 00:16:14,579
plain vanilla Zen everything was within

514
00:16:14,579 --> 00:16:16,019
the noise floor of standard one standard

515
00:16:16,019 --> 00:16:18,000
deviation which is pretty good

516
00:16:18,000 --> 00:16:20,639
where we did see some overheads however

517
00:16:20,639 --> 00:16:22,079
were on some of the more stress test

518
00:16:22,079 --> 00:16:23,639
benchmarks that we included all of which

519
00:16:23,639 --> 00:16:25,320
were multi-core in-memory key Value

520
00:16:25,320 --> 00:16:27,600
Store databases like memcache d which

521
00:16:27,600 --> 00:16:28,980
happened to be the worst of them all at

522
00:16:28,980 --> 00:16:30,899
nearly 22 percent overhead

523
00:16:30,899 --> 00:16:33,000
these are all workloads that are very

524
00:16:33,000 --> 00:16:34,320
difficult to virtualize in the first

525
00:16:34,320 --> 00:16:36,060
place because they incur frequent

526
00:16:36,060 --> 00:16:37,680
cross-course synchronization using

527
00:16:37,680 --> 00:16:40,079
system calls like linux's futex

528
00:16:40,079 --> 00:16:41,880
these synchronizations generate

529
00:16:41,880 --> 00:16:44,100
inter-processor interrupts or ipis from

530
00:16:44,100 --> 00:16:46,680
one guest from the guests

531
00:16:46,680 --> 00:16:48,360
different virtual CPUs between each

532
00:16:48,360 --> 00:16:49,560
other

533
00:16:49,560 --> 00:16:51,240
and unfortunately on current Hardware

534
00:16:51,240 --> 00:16:53,279
that requires taking a VM exit every

535
00:16:53,279 --> 00:16:54,839
time you want to do that that is you

536
00:16:54,839 --> 00:16:57,060
have to trap out to the hypervisor to to

537
00:16:57,060 --> 00:16:59,220
send those interrupts from One processor

538
00:16:59,220 --> 00:17:01,800
to one virtual processor to another

539
00:17:01,800 --> 00:17:05,099
uh that makes it far slower in a VM than

540
00:17:05,099 --> 00:17:07,619
on bare metal and in our paper what we

541
00:17:07,619 --> 00:17:10,079
We examined Us in detail we

542
00:17:10,079 --> 00:17:13,500
um we compared uh vanilla Zen without

543
00:17:13,500 --> 00:17:15,540
any of our modifications to a bare metal

544
00:17:15,540 --> 00:17:17,819
system and we confirmed that vanilla Zen

545
00:17:17,819 --> 00:17:19,140
is already adding several hundred

546
00:17:19,140 --> 00:17:20,760
percent overhead to these benchmarks

547
00:17:20,760 --> 00:17:23,939
even before we add Umbra so much to our

548
00:17:23,939 --> 00:17:25,380
surprise people are already routinely

549
00:17:25,380 --> 00:17:26,760
paying that huge price in order to

550
00:17:26,760 --> 00:17:28,620
virtualize memcache D in the cloud

551
00:17:28,620 --> 00:17:31,200
so we realize that compared to that of

552
00:17:31,200 --> 00:17:33,299
the 12 to 22 percent that we're adding

553
00:17:33,299 --> 00:17:35,460
to that is pretty minimal

554
00:17:35,460 --> 00:17:37,200
but still we wanted to explore with some

555
00:17:37,200 --> 00:17:38,580
more detail so we focused on this to

556
00:17:38,580 --> 00:17:41,460
more these micro benchmarks we created

557
00:17:41,460 --> 00:17:43,260
for micro benchmarks we wanted to drill

558
00:17:43,260 --> 00:17:44,820
down to specific low-level

559
00:17:44,820 --> 00:17:46,500
virtualization operations to find out

560
00:17:46,500 --> 00:17:48,360
where exactly ombro was adding overhead

561
00:17:48,360 --> 00:17:50,520
compared to vanilla Zen so we created

562
00:17:50,520 --> 00:17:53,039
four tests one was a no-op hyper call to

563
00:17:53,039 --> 00:17:55,440
measure VM entry and exit latency

564
00:17:55,440 --> 00:17:57,120
the second one was a fault Handler for

565
00:17:57,120 --> 00:17:58,559
the guest to host second level page

566
00:17:58,559 --> 00:17:59,760
tables that are managed by the

567
00:17:59,760 --> 00:18:01,919
hypervisor and then we we tested inter

568
00:18:01,919 --> 00:18:03,360
processor interrupts both within the

569
00:18:03,360 --> 00:18:06,660
same CPU and to a different CPU

570
00:18:06,660 --> 00:18:08,880
what these show very clearly is that the

571
00:18:08,880 --> 00:18:10,200
one place omro was really adding

572
00:18:10,200 --> 00:18:12,299
overhead is to the process of VM entry

573
00:18:12,299 --> 00:18:14,160
and exit itself all of the other

574
00:18:14,160 --> 00:18:16,080
benchmarks included a VM entry and exit

575
00:18:16,080 --> 00:18:17,580
cycle I.E the first micro Benchmark

576
00:18:17,580 --> 00:18:20,160
along with along with other additional

577
00:18:20,160 --> 00:18:22,740
processing by the hypervisor and so

578
00:18:22,740 --> 00:18:24,120
we're seeing here the more additional

579
00:18:24,120 --> 00:18:25,980
processing is included the smaller the

580
00:18:25,980 --> 00:18:27,299
overhead gets

581
00:18:27,299 --> 00:18:29,400
to confirm us fervor we tried re-running

582
00:18:29,400 --> 00:18:31,799
our benchmarks both macro and micro with

583
00:18:31,799 --> 00:18:34,620
all of umbros CFI SFI and split stack

584
00:18:34,620 --> 00:18:36,000
instrumentation removed so all you've

585
00:18:36,000 --> 00:18:37,799
got is the virtual instruction set

586
00:18:37,799 --> 00:18:39,360
and the results turned out to be almost

587
00:18:39,360 --> 00:18:40,919
exactly the same

588
00:18:40,919 --> 00:18:43,200
essentially what this all means is that

589
00:18:43,200 --> 00:18:45,299
the only overhead we're adding is from

590
00:18:45,299 --> 00:18:47,640
the virtual instructions itself since it

591
00:18:47,640 --> 00:18:49,320
adds an extra layer of indirection in

592
00:18:49,320 --> 00:18:50,700
the vmetry and exit process which

593
00:18:50,700 --> 00:18:52,500
involves it involves some extra copying

594
00:18:52,500 --> 00:18:54,059
of processor State as it goes in and out

595
00:18:54,059 --> 00:18:56,880
of the vmcs

596
00:18:56,880 --> 00:18:59,100
we think we could probably shave that 95

597
00:18:59,100 --> 00:19:00,539
percent of the hyper call Micro

598
00:19:00,539 --> 00:19:02,520
Benchmark a bit further but if we

599
00:19:02,520 --> 00:19:04,740
optimized sva's implementation more but

600
00:19:04,740 --> 00:19:06,600
from a bigger picture this is very

601
00:19:06,600 --> 00:19:07,919
encouraging

602
00:19:07,919 --> 00:19:09,360
from the perspective of the security

603
00:19:09,360 --> 00:19:11,400
hardening we added because it means that

604
00:19:11,400 --> 00:19:13,020
we effectively aren't slowing down Zen

605
00:19:13,020 --> 00:19:15,179
itself much at all we're only adding a

606
00:19:15,179 --> 00:19:16,440
bit of overhead to the transitions

607
00:19:16,440 --> 00:19:18,000
between host and guest processor modes

608
00:19:18,000 --> 00:19:20,580
which are already supposed to be slow

609
00:19:20,580 --> 00:19:23,220
as an architectural matter the whole x86

610
00:19:23,220 --> 00:19:24,960
virtualization architecture is designed

611
00:19:24,960 --> 00:19:26,640
around the assumption that VM entries

612
00:19:26,640 --> 00:19:28,260
and exits are going to be slow for

613
00:19:28,260 --> 00:19:30,179
simply going to be slow because of all

614
00:19:30,179 --> 00:19:31,140
the processor state that you're

615
00:19:31,140 --> 00:19:32,760
shuffling around so in order to make

616
00:19:32,760 --> 00:19:34,679
virtualization fast what you want to do

617
00:19:34,679 --> 00:19:37,080
is have as few exits as possible and

618
00:19:37,080 --> 00:19:38,520
this has been the driving force behind

619
00:19:38,520 --> 00:19:40,140
most of the improvements Intel has made

620
00:19:40,140 --> 00:19:41,700
to the vmx instruction set over the

621
00:19:41,700 --> 00:19:43,020
years they've been focused on moving

622
00:19:43,020 --> 00:19:44,760
more and more virtualization into

623
00:19:44,760 --> 00:19:46,080
Hardware so if you don't need to take as

624
00:19:46,080 --> 00:19:49,460
many VM exits in the first place

625
00:19:49,919 --> 00:19:52,200
uh so the good news for for our macro

626
00:19:52,200 --> 00:19:54,179
outliers like memcache d is that Intel

627
00:19:54,179 --> 00:19:56,820
is very well aware that cross-core

628
00:19:56,820 --> 00:19:58,440
synchronization is one of the few

629
00:19:58,440 --> 00:20:00,000
remaining pain points for virtualization

630
00:20:00,000 --> 00:20:01,500
and they've announced hardware

631
00:20:01,500 --> 00:20:03,059
virtualization to accelerate in future

632
00:20:03,059 --> 00:20:04,140
processors

633
00:20:04,140 --> 00:20:06,360
this will remove the need to take a VM

634
00:20:06,360 --> 00:20:07,919
exit from the hypervisor to mediate

635
00:20:07,919 --> 00:20:09,240
these interrupts going from one core to

636
00:20:09,240 --> 00:20:10,799
another within the same virtual machine

637
00:20:10,799 --> 00:20:13,020
and if you'd have that that eliminates

638
00:20:13,020 --> 00:20:14,760
the vast majority of the VM exits that

639
00:20:14,760 --> 00:20:16,500
are taken by applications like memcache

640
00:20:16,500 --> 00:20:17,940
d and our other outliers that we showed

641
00:20:17,940 --> 00:20:19,140
on the last slide

642
00:20:19,140 --> 00:20:21,480
so with that addressed we expect our

643
00:20:21,480 --> 00:20:23,820
overhead for ombro on those benchmarks

644
00:20:23,820 --> 00:20:25,260
to approach zero like it does for the

645
00:20:25,260 --> 00:20:28,140
rest of our macro benchmarks

646
00:20:28,140 --> 00:20:29,580
our takeaways from this are about

647
00:20:29,580 --> 00:20:31,260
hypervisors are excellent candidates for

648
00:20:31,260 --> 00:20:32,640
this type of compiler-based security

649
00:20:32,640 --> 00:20:34,799
hardening amdahl's law is our friend

650
00:20:34,799 --> 00:20:37,679
here hypervisors by Design occupy an

651
00:20:37,679 --> 00:20:39,179
extremely small fraction of system

652
00:20:39,179 --> 00:20:41,280
runtime relative to the applications

653
00:20:41,280 --> 00:20:43,860
that are running inside your VMS with a

654
00:20:43,860 --> 00:20:45,179
well-designed heart with well-designed

655
00:20:45,179 --> 00:20:46,679
Hardware if it makes VM exits rare

656
00:20:46,679 --> 00:20:48,660
there's very little penalty to taking a

657
00:20:48,660 --> 00:20:50,340
bit of slowdown on the hypervisor itself

658
00:20:50,340 --> 00:20:52,140
for the sake of Security even by

659
00:20:52,140 --> 00:20:54,000
percentages that would might be crushing

660
00:20:54,000 --> 00:20:56,280
in user space or even in an OS kernel as

661
00:20:56,280 --> 00:20:57,960
defrenge would say twice nothing is

662
00:20:57,960 --> 00:21:00,299
still nothing

663
00:21:00,299 --> 00:21:03,419
but the interesting thing is here

664
00:21:03,419 --> 00:21:06,539
uh we for ombro we chose a relatively

665
00:21:06,539 --> 00:21:08,220
lightweight policy of control flow

666
00:21:08,220 --> 00:21:10,020
Integrity plus a shadow stack

667
00:21:10,020 --> 00:21:11,760
in retrospect though our numbers show

668
00:21:11,760 --> 00:21:13,140
what we could have chosen a far more

669
00:21:13,140 --> 00:21:14,760
aggressive policy like full memory

670
00:21:14,760 --> 00:21:16,620
safety instrumentation and still

671
00:21:16,620 --> 00:21:17,820
achieved very good end-to-end

672
00:21:17,820 --> 00:21:18,840
performance

673
00:21:18,840 --> 00:21:20,340
this is very encouraging for the future

674
00:21:20,340 --> 00:21:22,919
of hypervisor's security because you

675
00:21:22,919 --> 00:21:24,179
know with hypervisors there's very high

676
00:21:24,179 --> 00:21:26,460
stakes because of what vulnerabilities

677
00:21:26,460 --> 00:21:28,140
compromise

678
00:21:28,140 --> 00:21:29,880
that combined with the low performance

679
00:21:29,880 --> 00:21:31,559
impact of mitigations makes for a very

680
00:21:31,559 --> 00:21:33,480
worthwhile cost cost benefit when you

681
00:21:33,480 --> 00:21:36,679
want to do strong protections

682
00:21:37,260 --> 00:21:39,240
so to conclude we have made two major

683
00:21:39,240 --> 00:21:40,860
contributions of this work

684
00:21:40,860 --> 00:21:42,539
one we've implemented the first

685
00:21:42,539 --> 00:21:44,159
efficient Shadow stack on a real world

686
00:21:44,159 --> 00:21:46,020
hypervisor with near Zero end-to-end

687
00:21:46,020 --> 00:21:48,419
Performance impact on most benchmarks

688
00:21:48,419 --> 00:21:50,820
two we've done this in a way that

689
00:21:50,820 --> 00:21:52,200
provides complete and comprehensive

690
00:21:52,200 --> 00:21:54,120
hardening in the presence of a realistic

691
00:21:54,120 --> 00:21:55,799
set of hardware virtualization features

692
00:21:55,799 --> 00:21:58,320
that real hypervisors rely on and we've

693
00:21:58,320 --> 00:21:59,580
done this as part of a virtual

694
00:21:59,580 --> 00:22:01,020
instruction set framework that can be

695
00:22:01,020 --> 00:22:02,700
used to implement a wide variety of

696
00:22:02,700 --> 00:22:04,080
interesting hardening policies on

697
00:22:04,080 --> 00:22:05,880
hypervisors in the future

698
00:22:05,880 --> 00:22:07,440
also although we didn't have time to

699
00:22:07,440 --> 00:22:08,760
cover in this talk we did a very

700
00:22:08,760 --> 00:22:10,440
detailed security analysis in our paper

701
00:22:10,440 --> 00:22:12,240
of exactly which kinds of advanced code

702
00:22:12,240 --> 00:22:14,340
reuse attacks our CFI and Shadow stack

703
00:22:14,340 --> 00:22:16,740
design can and can't prevent so if

704
00:22:16,740 --> 00:22:17,700
you're interested in that sort of thing

705
00:22:17,700 --> 00:22:18,720
I encourage you to look more at the

706
00:22:18,720 --> 00:22:19,679
paper because it's because it's quite

707
00:22:19,679 --> 00:22:22,039
interesting

708
00:22:22,080 --> 00:22:24,360
um also we are planning to open source

709
00:22:24,360 --> 00:22:25,260
on bro

710
00:22:25,260 --> 00:22:27,840
uh we did not have the link ready for

711
00:22:27,840 --> 00:22:29,039
today because because of paperwork

712
00:22:29,039 --> 00:22:30,240
reasons but we're hoping to get that

713
00:22:30,240 --> 00:22:32,159
added up soon if you're interested just

714
00:22:32,159 --> 00:22:33,539
reach out to us and we can get that to

715
00:22:33,539 --> 00:22:34,559
you later

716
00:22:34,559 --> 00:22:36,360
I hope you enjoyed this talk if you have

717
00:22:36,360 --> 00:22:39,860
any questions please feel free to ask

