1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:14,059 --> 00:00:16,500
from the Chinese University of Hong Kong

3
00:00:16,500 --> 00:00:18,960
and today I'm going to present our paper

4
00:00:18,960 --> 00:00:20,820
secure and lightweight the duplicated

5
00:00:20,820 --> 00:00:22,680
storage ratio data duplication before

6
00:00:22,680 --> 00:00:24,840
encryption and this is a strong work

7
00:00:24,840 --> 00:00:26,820
with my collaborator Jing lady and my

8
00:00:26,820 --> 00:00:29,099
supervisor Professor Patrick

9
00:00:29,099 --> 00:00:31,740
okay so first of all also storage is a

10
00:00:31,740 --> 00:00:34,020
popular solution to reduce the data

11
00:00:34,020 --> 00:00:35,700
management overhead especially when we

12
00:00:35,700 --> 00:00:38,040
are facing this explosion so according

13
00:00:38,040 --> 00:00:40,500
to a report from IDC at the predicts

14
00:00:40,500 --> 00:00:42,480
that the global data farewell increase

15
00:00:42,480 --> 00:00:46,980
to 175 device by 20 and 25 and around 49

16
00:00:46,980 --> 00:00:49,500
of them were received in public clouds

17
00:00:49,500 --> 00:00:52,620
and generally also storage approach me

18
00:00:52,620 --> 00:00:55,260
to satisfy two requirements the first

19
00:00:55,260 --> 00:00:57,239
one is storage efficiency which means

20
00:00:57,239 --> 00:00:59,399
the cloud storage provider should reduce

21
00:00:59,399 --> 00:01:01,739
the storage overhead as much as possible

22
00:01:01,739 --> 00:01:03,600
and the second one is data

23
00:01:03,600 --> 00:01:05,880
confidentiality which means it also

24
00:01:05,880 --> 00:01:07,680
needs to Define against data privacy

25
00:01:07,680 --> 00:01:09,900
leakage and protect all those data

26
00:01:09,900 --> 00:01:12,619
against the authorized access

27
00:01:12,619 --> 00:01:16,200
so data education is just a can a common

28
00:01:16,200 --> 00:01:19,140
approach to realize story efficiency is

29
00:01:19,140 --> 00:01:21,900
usually identifies the duplicate as the

30
00:01:21,900 --> 00:01:24,659
level of Chunk so it's a it's an idea is

31
00:01:24,659 --> 00:01:26,939
just to compute a fingerprint of each

32
00:01:26,939 --> 00:01:30,180
chunk where via credit hash function and

33
00:01:30,180 --> 00:01:32,220
mine things are fingerprint index to

34
00:01:32,220 --> 00:01:34,500
track existing chunks which Maps the

35
00:01:34,500 --> 00:01:36,299
chunk fingerprint to the corresponding

36
00:01:36,299 --> 00:01:39,299
address of each existing chunk and the

37
00:01:39,299 --> 00:01:41,340
given a input chunk it can check the

38
00:01:41,340 --> 00:01:43,560
fingerprint index to ensure that only

39
00:01:43,560 --> 00:01:46,020
one physical copy of duplic Chunk is

40
00:01:46,020 --> 00:01:48,659
stored and according to previous study

41
00:01:48,659 --> 00:01:51,000
data duplication can achieve around 10

42
00:01:51,000 --> 00:01:53,100
times storage Savings in backup

43
00:01:53,100 --> 00:01:54,659
workloads

44
00:01:54,659 --> 00:01:56,579
so to further realize data

45
00:01:56,579 --> 00:01:59,460
confidentiality current approach follows

46
00:01:59,460 --> 00:02:01,500
the Paradigm called the direction after

47
00:02:01,500 --> 00:02:04,140
encryption Dae to augment the

48
00:02:04,140 --> 00:02:06,540
duplication with encryption and it

49
00:02:06,540 --> 00:02:08,758
almost carefully encrypt plan test

50
00:02:08,758 --> 00:02:10,739
chunks to preserve the duplication

51
00:02:10,739 --> 00:02:13,080
Effectiveness on Surface chunk after

52
00:02:13,080 --> 00:02:15,780
encryption and here message log

53
00:02:15,780 --> 00:02:18,300
encryption is a cryptographic primitive

54
00:02:18,300 --> 00:02:21,420
to realize Dae by using a key derived

55
00:02:21,420 --> 00:02:23,580
from the trunk content for encryption

56
00:02:23,580 --> 00:02:26,280
and decryption such that duplicate

57
00:02:26,280 --> 00:02:28,500
planted chunk will always be encrypted

58
00:02:28,500 --> 00:02:30,959
due to duplicate percentage chunk and

59
00:02:30,959 --> 00:02:32,640
enabled the duplication

60
00:02:32,640 --> 00:02:35,580
for example a typical instance of mass

61
00:02:35,580 --> 00:02:37,560
destruction encryption is converging

62
00:02:37,560 --> 00:02:40,200
encryption in which the key is the heart

63
00:02:40,200 --> 00:02:42,420
of the plantation content

64
00:02:42,420 --> 00:02:45,420
to prevent The Advisory from enumerating

65
00:02:45,420 --> 00:02:47,640
all possible Keys current daily

66
00:02:47,640 --> 00:02:50,519
approaches always deploy a dedicated key

67
00:02:50,519 --> 00:02:53,819
server for key generation

68
00:02:53,819 --> 00:02:56,160
although conventional approaches always

69
00:02:56,160 --> 00:02:58,500
follow the AE to realize both storage

70
00:02:58,500 --> 00:03:00,959
efficiency and the data confidentiality

71
00:03:00,959 --> 00:03:03,540
we argue that the AE has some

72
00:03:03,540 --> 00:03:05,340
fundamental limitation

73
00:03:05,340 --> 00:03:08,040
first Dae incurs High Key Management

74
00:03:08,040 --> 00:03:11,220
overhead so from the storage perspective

75
00:03:11,220 --> 00:03:13,860
it needs to keep a key for each chunk

76
00:03:13,860 --> 00:03:16,860
which increase the metadata overhead and

77
00:03:16,860 --> 00:03:19,080
from the performance perspective it

78
00:03:19,080 --> 00:03:21,480
needs to leverage expensive

79
00:03:21,480 --> 00:03:24,000
cryptographic Primitives to protect the

80
00:03:24,000 --> 00:03:26,220
key generation process in which

81
00:03:26,220 --> 00:03:28,739
introduced significant performance

82
00:03:28,739 --> 00:03:30,379
overhead

83
00:03:30,379 --> 00:03:33,540
is incompatible with compression assets

84
00:03:33,540 --> 00:03:35,459
the cloud can only view the same type

85
00:03:35,459 --> 00:03:37,620
chunks which tend to have high entropy

86
00:03:37,620 --> 00:03:40,019
content and cannot be further compressed

87
00:03:40,019 --> 00:03:41,940
although the client can perform

88
00:03:41,940 --> 00:03:43,980
compression locally before encryption

89
00:03:43,980 --> 00:03:46,379
but this would leak the compression

90
00:03:46,379 --> 00:03:48,540
length and introduce extra information

91
00:03:48,540 --> 00:03:49,920
leakage

92
00:03:49,920 --> 00:03:53,700
third the AE has a security risk as the

93
00:03:53,700 --> 00:03:56,099
state of the RCA approaches always

94
00:03:56,099 --> 00:03:58,140
deploy a key service for key generation

95
00:03:58,140 --> 00:04:01,080
to Define against brutal attacks this

96
00:04:01,080 --> 00:04:03,180
kind of centralized severated Key

97
00:04:03,180 --> 00:04:05,099
Management would like the key server

98
00:04:05,099 --> 00:04:08,459
become a single point of attack also the

99
00:04:08,459 --> 00:04:11,220
AE is deterministic by nature as it was

100
00:04:11,220 --> 00:04:13,140
one to one matching between plantar

101
00:04:13,140 --> 00:04:14,640
chunks to the substance transport

102
00:04:14,640 --> 00:04:17,100
communication diversary can launch

103
00:04:17,100 --> 00:04:20,220
frequency analyze to enforce original

104
00:04:20,220 --> 00:04:22,440
plantex Chunk from the frequency

105
00:04:22,440 --> 00:04:25,440
distribution of the center chunk

106
00:04:25,440 --> 00:04:28,740
in this paper we explore a new paradigm

107
00:04:28,740 --> 00:04:30,540
called the depiction before encryption

108
00:04:30,540 --> 00:04:33,419
dbe which performed the duplication on

109
00:04:33,419 --> 00:04:35,520
plan test chunk followed by encrypting

110
00:04:35,520 --> 00:04:38,699
only non-duplicate planted chunks DB

111
00:04:38,699 --> 00:04:41,040
offers several benefits over Dae by

112
00:04:41,040 --> 00:04:42,419
Design which are addressed the

113
00:04:42,419 --> 00:04:45,000
limitation of Dae since the duplication

114
00:04:45,000 --> 00:04:48,000
is applied first dbe can encrypt each

115
00:04:48,000 --> 00:04:50,280
non-duplicate plan test chunk with a

116
00:04:50,280 --> 00:04:52,259
Content independent key as in

117
00:04:52,259 --> 00:04:54,240
traditional symmetric encryption without

118
00:04:54,240 --> 00:04:56,759
compromising the duplication so this

119
00:04:56,759 --> 00:04:59,160
avoids generating a store per chunk

120
00:04:59,160 --> 00:05:01,620
content derived keys and reduce the key

121
00:05:01,620 --> 00:05:03,360
management overhead

122
00:05:03,360 --> 00:05:06,960
second DB can apply compressor to the

123
00:05:06,960 --> 00:05:08,880
now duplicate plaintiff chunk after the

124
00:05:08,880 --> 00:05:11,100
duplication for actual story saving

125
00:05:11,100 --> 00:05:13,199
followed by encrypting the compressed

126
00:05:13,199 --> 00:05:15,419
non-duperate chunk so this address the

127
00:05:15,419 --> 00:05:17,759
incompatibility with compression in the

128
00:05:17,759 --> 00:05:20,940
AE third since dbe can perform

129
00:05:20,940 --> 00:05:23,220
encryption with a Content independent

130
00:05:23,220 --> 00:05:25,860
key it no longer needs a key server for

131
00:05:25,860 --> 00:05:28,560
key generation uh since severated Key

132
00:05:28,560 --> 00:05:31,620
Management in the AE so this removes the

133
00:05:31,620 --> 00:05:35,100
single point of attack in Dae however

134
00:05:35,100 --> 00:05:38,100
the main question of realizing DB is how

135
00:05:38,100 --> 00:05:40,740
should the duplication be protected

136
00:05:40,740 --> 00:05:43,259
so here since the duplication process in

137
00:05:43,259 --> 00:05:45,960
dbe is no longer protected by encryption

138
00:05:45,960 --> 00:05:48,900
so this Paradigm remains unexploded in

139
00:05:48,900 --> 00:05:51,600
the literature and existing study mostly

140
00:05:51,600 --> 00:05:54,479
focus on Dae

141
00:05:54,479 --> 00:05:56,940
so here we summarize our contribution in

142
00:05:56,940 --> 00:05:59,699
this paper we propose DB I showed that

143
00:05:59,699 --> 00:06:01,740
the DB based the duplicated storage

144
00:06:01,740 --> 00:06:05,280
system based on shielded execution so DB

145
00:06:05,280 --> 00:06:08,460
explores realizing dbe with the head of

146
00:06:08,460 --> 00:06:11,340
in into sjx it protects the direction

147
00:06:11,340 --> 00:06:14,759
process in dbv by interactions and apply

148
00:06:14,759 --> 00:06:17,400
frequencies based duplication to achieve

149
00:06:17,400 --> 00:06:19,320
high performance and mitigate the

150
00:06:19,320 --> 00:06:21,060
frequency leakage

151
00:06:21,060 --> 00:06:23,759
we also conduct extensive experiments to

152
00:06:23,759 --> 00:06:26,460
validate that dbdb outperformed

153
00:06:26,460 --> 00:06:28,500
conversional Dae approaches in

154
00:06:28,500 --> 00:06:33,139
performance storage service and security

155
00:06:33,180 --> 00:06:35,880
so before introducing CB's design I

156
00:06:35,880 --> 00:06:37,500
would like to present the background of

157
00:06:37,500 --> 00:06:40,740
Interest checks so briefly sjx realized

158
00:06:40,740 --> 00:06:42,840
a shielded a skillshare environment

159
00:06:42,840 --> 00:06:45,600
called concave in a secure memory region

160
00:06:45,600 --> 00:06:48,900
called The Enclave page cache EPC and

161
00:06:48,900 --> 00:06:51,600
then K provide two kind of interface to

162
00:06:51,600 --> 00:06:54,060
interact with untrusted application

163
00:06:54,060 --> 00:06:58,380
outside the NK in which ecos permits the

164
00:06:58,380 --> 00:07:00,900
outside application to safely access the

165
00:07:00,900 --> 00:07:04,020
incave content and all costs are loading

166
00:07:04,020 --> 00:07:06,660
encapable to issue function costs in all

167
00:07:06,660 --> 00:07:07,979
set application

168
00:07:07,979 --> 00:07:09,900
Mysteries also introduced some

169
00:07:09,900 --> 00:07:12,600
performance overhead first the size of

170
00:07:12,600 --> 00:07:15,000
the concave page cache is very limited

171
00:07:15,000 --> 00:07:18,060
once memory usage of the encave exists

172
00:07:18,060 --> 00:07:20,220
this limitation it will incurve

173
00:07:20,220 --> 00:07:22,740
expensive EPC patient overhead in

174
00:07:22,740 --> 00:07:25,560
addition both equals and locals involve

175
00:07:25,560 --> 00:07:27,599
some Hardware operation

176
00:07:27,599 --> 00:07:30,000
so this introduced High contact solution

177
00:07:30,000 --> 00:07:31,139
overhead

178
00:07:31,139 --> 00:07:33,720
thus the main design challenge here is

179
00:07:33,720 --> 00:07:36,300
how to mitigate the extract overhead in

180
00:07:36,300 --> 00:07:37,860
DB

181
00:07:37,860 --> 00:07:39,840
so here we present the high level

182
00:07:39,840 --> 00:07:42,780
overview of DB it performs target-based

183
00:07:42,780 --> 00:07:45,060
deduplication to remove duplicate data

184
00:07:45,060 --> 00:07:47,280
of multiple client in the cloud so

185
00:07:47,280 --> 00:07:49,800
compare with Dae a major difference here

186
00:07:49,800 --> 00:07:52,680
is DBS avoid performance generation so

187
00:07:52,680 --> 00:07:55,020
it does not need to maintain a key

188
00:07:55,020 --> 00:07:57,599
server to prevent the cloud from

189
00:07:57,599 --> 00:07:59,639
accessing any plant test chunks during

190
00:07:59,639 --> 00:08:02,580
the duplication process DB deployed and

191
00:08:02,580 --> 00:08:04,800
Enclave in the cloud and perform the

192
00:08:04,800 --> 00:08:06,419
duplication and compression of the

193
00:08:06,419 --> 00:08:08,759
plantation in the encave

194
00:08:08,759 --> 00:08:11,220
so each client set up to secure

195
00:08:11,220 --> 00:08:13,560
communicating Channel with the cloud so

196
00:08:13,560 --> 00:08:15,240
here the control channel is used to

197
00:08:15,240 --> 00:08:17,340
transmit the command and storage

198
00:08:17,340 --> 00:08:19,800
operation to the cloud and the data

199
00:08:19,800 --> 00:08:21,419
channel is used to transmit the plan

200
00:08:21,419 --> 00:08:24,060
text Chunk to the encave directly So

201
00:08:24,060 --> 00:08:27,060
currently DB implements are DV here my

202
00:08:27,060 --> 00:08:28,979
key exchange protocol between a client

203
00:08:28,979 --> 00:08:31,620
and a concave in order to agree on a

204
00:08:31,620 --> 00:08:34,140
session key to protect this data Channel

205
00:08:34,140 --> 00:08:37,080
such that the cloud cannot infer the

206
00:08:37,080 --> 00:08:40,679
plaintiff's chunk through the channel

207
00:08:40,679 --> 00:08:44,000
and this is core idea is to perform the

208
00:08:44,000 --> 00:08:46,320
duplication of a plant has Chunk in the

209
00:08:46,320 --> 00:08:48,779
uncave and the issue here is how to

210
00:08:48,779 --> 00:08:51,420
manage the fingerprint index to mitigate

211
00:08:51,420 --> 00:08:53,940
the icejx performance overhead so we

212
00:08:53,940 --> 00:08:56,220
propose frequency space deduplication to

213
00:08:56,220 --> 00:08:58,680
address this issue it is based on our

214
00:08:58,680 --> 00:09:01,019
Insight that the frequencies of chunks

215
00:09:01,019 --> 00:09:02,880
are highly skilled in Practical backup

216
00:09:02,880 --> 00:09:05,100
workloads and a small fraction of Chunk

217
00:09:05,100 --> 00:09:07,019
can contribute a large fraction of

218
00:09:07,019 --> 00:09:10,800
duplicate data so for example in VM

219
00:09:10,800 --> 00:09:13,140
dataset we observe that the top five

220
00:09:13,140 --> 00:09:16,140
percent of frequent chunk contribute to

221
00:09:16,140 --> 00:09:19,740
97 of duplication of the whole data sets

222
00:09:19,740 --> 00:09:22,200
so this implies that if we just maintain

223
00:09:22,200 --> 00:09:24,720
a small fingerprint index in the cable

224
00:09:24,720 --> 00:09:27,060
to track the most frequent Chunk we can

225
00:09:27,060 --> 00:09:29,100
remove a large fraction of people data

226
00:09:29,100 --> 00:09:32,040
and Achieve High storage efficiency

227
00:09:32,040 --> 00:09:34,560
so the idea of frequency space

228
00:09:34,560 --> 00:09:36,660
duplication is just to separate the

229
00:09:36,660 --> 00:09:38,399
duplication process based on the chunk

230
00:09:38,399 --> 00:09:40,860
frequencies in two phases the first

231
00:09:40,860 --> 00:09:42,540
phase the duplication manages small

232
00:09:42,540 --> 00:09:44,940
fingerprint index in the concave to

233
00:09:44,940 --> 00:09:47,279
remove duplicate data from most frequent

234
00:09:47,279 --> 00:09:50,100
chunks so this mitigate the EPC paging

235
00:09:50,100 --> 00:09:52,260
overhead and second phase duplication

236
00:09:52,260 --> 00:09:54,300
maintains the full index all sets Okay

237
00:09:54,300 --> 00:09:56,700
to remove reminding duplicates for the

238
00:09:56,700 --> 00:09:58,560
light stroke control by all calls so

239
00:09:58,560 --> 00:10:00,240
this can reduce the contact switching

240
00:10:00,240 --> 00:10:02,399
overhead as it's only acquiring the flow

241
00:10:02,399 --> 00:10:04,620
index all cells Enclave by all costs for

242
00:10:04,620 --> 00:10:07,200
a limited fraction of light second chunk

243
00:10:07,200 --> 00:10:09,660
here we present the architecture of the

244
00:10:09,660 --> 00:10:11,399
uncave The Enclave tracks the

245
00:10:11,399 --> 00:10:13,620
frequencies of Plantation to identify

246
00:10:13,620 --> 00:10:15,420
the most frequent and less frequent

247
00:10:15,420 --> 00:10:17,459
chunks for frequency-based duplication

248
00:10:17,459 --> 00:10:19,800
and the frequencies features the base

249
00:10:19,800 --> 00:10:22,200
purification first remove the duplicate

250
00:10:22,200 --> 00:10:24,420
of most different chunks in the uncave

251
00:10:24,420 --> 00:10:27,540
and then interact with the full index to

252
00:10:27,540 --> 00:10:30,360
remove remaining duplicate data of less

253
00:10:30,360 --> 00:10:33,480
frequent chunks by all costs so here we

254
00:10:33,480 --> 00:10:35,519
use our query key to protect the

255
00:10:35,519 --> 00:10:37,740
interaction between the full index and

256
00:10:37,740 --> 00:10:40,080
the encave to prevent the cloud from

257
00:10:40,080 --> 00:10:43,140
learning the information uh during the

258
00:10:43,140 --> 00:10:44,040
priorities

259
00:10:44,040 --> 00:10:46,620
after the duplication The Enclave

260
00:10:46,620 --> 00:10:48,779
compressed non-duperic chunk and the

261
00:10:48,779 --> 00:10:50,940
encrypt compressed chunk by long-term

262
00:10:50,940 --> 00:10:53,579
data keys for storage so later I will

263
00:10:53,579 --> 00:10:55,260
introduce more detail of the major

264
00:10:55,260 --> 00:10:58,140
modules in this architecture

265
00:10:58,140 --> 00:11:01,019
so here to identify the most frequent

266
00:11:01,019 --> 00:11:03,899
chunk the concave us a count mean sketch

267
00:11:03,899 --> 00:11:06,540
to check the approx approximate

268
00:11:06,540 --> 00:11:08,640
frequencies of each chunk by a fixed

269
00:11:08,640 --> 00:11:11,760
size space with with a small arrow and

270
00:11:11,760 --> 00:11:13,440
the coming sketch is just a

271
00:11:13,440 --> 00:11:15,899
two-dimensional array of counters so our

272
00:11:15,899 --> 00:11:18,180
key design here is to limit the

273
00:11:18,180 --> 00:11:20,220
computational overhead of mapping the

274
00:11:20,220 --> 00:11:22,740
plant test chunk to the counter so our

275
00:11:22,740 --> 00:11:25,440
Insight is that the chunk fingerprint is

276
00:11:25,440 --> 00:11:28,260
computed as a cryptographic hash so we

277
00:11:28,260 --> 00:11:30,120
can treat the chunk fingerprint as a

278
00:11:30,120 --> 00:11:32,640
random input value as a method directly

279
00:11:32,640 --> 00:11:35,160
to a counter without compromising the

280
00:11:35,160 --> 00:11:38,220
accuracy of the coming sketch so here

281
00:11:38,220 --> 00:11:40,620
the uncave partition the fingerprint

282
00:11:40,620 --> 00:11:43,200
into our pieces and map each piece to

283
00:11:43,200 --> 00:11:46,079
our counter so this design has nearly no

284
00:11:46,079 --> 00:11:48,420
extras performance overhead while the

285
00:11:48,420 --> 00:11:50,760
traditional coming sketch maps the input

286
00:11:50,760 --> 00:11:53,040
to the counter of different rows using

287
00:11:53,040 --> 00:11:55,380
independent hash function and has actual

288
00:11:55,380 --> 00:11:58,079
computation overhead to estimate the

289
00:11:58,079 --> 00:12:00,300
frequencies of a chunk the concave use

290
00:12:00,300 --> 00:12:02,579
minimum value of the map counter of the

291
00:12:02,579 --> 00:12:04,440
chunk

292
00:12:04,440 --> 00:12:06,360
and the first phase the duplication

293
00:12:06,360 --> 00:12:09,779
removes the duplicate of K most frequent

294
00:12:09,779 --> 00:12:12,260
plant test chunk but based on our Pure

295
00:12:12,260 --> 00:12:15,180
Insight we expect that a large fraction

296
00:12:15,180 --> 00:12:17,399
of nuclear data can be removed by the

297
00:12:17,399 --> 00:12:19,079
first phase duplication

298
00:12:19,079 --> 00:12:21,360
so here the concave maintaining a small

299
00:12:21,360 --> 00:12:23,160
fingerprint that's called the top K

300
00:12:23,160 --> 00:12:25,860
index to record the K most frequent

301
00:12:25,860 --> 00:12:28,500
planted chunk so we implement the top K

302
00:12:28,500 --> 00:12:30,839
index as a combination of the mean Heap

303
00:12:30,839 --> 00:12:33,240
and the hash table the mean hip is used

304
00:12:33,240 --> 00:12:35,519
to differentiate the top K frequent and

305
00:12:35,519 --> 00:12:37,920
less frequent plant test chunk the hash

306
00:12:37,920 --> 00:12:40,440
table is used for duplicate detection as

307
00:12:40,440 --> 00:12:43,019
in conventional deduplication and The

308
00:12:43,019 --> 00:12:45,060
Enclave takes the estimated frequencies

309
00:12:45,060 --> 00:12:47,160
from the coming sketch of the plant as

310
00:12:47,160 --> 00:12:50,040
chunk and and the chunk fingerprint as

311
00:12:50,040 --> 00:12:51,180
the input

312
00:12:51,180 --> 00:12:53,459
in the first checks the root of the mean

313
00:12:53,459 --> 00:12:56,040
hip if the input frequencies is smaller

314
00:12:56,040 --> 00:12:57,839
than the minimum frequencies of the mean

315
00:12:57,839 --> 00:13:00,480
hip then the encapes keep querying the

316
00:13:00,480 --> 00:13:02,700
hash table for this chunk and proceed to

317
00:13:02,700 --> 00:13:04,740
the second phase duplication otherwise

318
00:13:04,740 --> 00:13:06,839
The Enclave use the input fingerprint to

319
00:13:06,839 --> 00:13:09,000
look up hash table and update the mean

320
00:13:09,000 --> 00:13:11,579
Heap so I here I would like to highlight

321
00:13:11,579 --> 00:13:15,000
that the top K index has low EPC usage

322
00:13:15,000 --> 00:13:17,820
of the space complexity of both mean hip

323
00:13:17,820 --> 00:13:22,019
and hash table are okay K is a constant

324
00:13:22,019 --> 00:13:24,240
the second phase the duplication um to

325
00:13:24,240 --> 00:13:26,040
remove duplicates from the remaining

326
00:13:26,040 --> 00:13:27,779
light frequent chunk that are not

327
00:13:27,779 --> 00:13:29,220
removed by the first phase the

328
00:13:29,220 --> 00:13:32,279
duplication here we manage a full index

329
00:13:32,279 --> 00:13:34,680
of NK which Maps the encrypted

330
00:13:34,680 --> 00:13:36,920
fingerprint to encrypt chunk information

331
00:13:36,920 --> 00:13:39,420
our design here is that we use a

332
00:13:39,420 --> 00:13:41,160
priority key to protect the chunky

333
00:13:41,160 --> 00:13:43,260
information storing the full index

334
00:13:43,260 --> 00:13:46,139
outside and Cave to query the full index

335
00:13:46,139 --> 00:13:48,360
the encase deterministically the encrypt

336
00:13:48,360 --> 00:13:49,980
the fingerprint of each remaining

337
00:13:49,980 --> 00:13:52,440
implanted chunk with the priority such

338
00:13:52,440 --> 00:13:54,600
that duplicated plant test chunk would

339
00:13:54,600 --> 00:13:57,000
generate the same encrypted fingerprints

340
00:13:57,000 --> 00:13:59,519
and enable duplicate duplication

341
00:13:59,519 --> 00:14:01,380
detection

342
00:14:01,380 --> 00:14:04,260
so the NK finally issue all comes with

343
00:14:04,260 --> 00:14:06,060
the encrypted fingerprint probably the

344
00:14:06,060 --> 00:14:08,880
full index and remove duplicate a chunk

345
00:14:08,880 --> 00:14:11,880
from the remaining light frequent chunk

346
00:14:11,880 --> 00:14:14,579
so we Implement DB in C plus plus based

347
00:14:14,579 --> 00:14:16,560
on Intel abstracts SDK over SSL and

348
00:14:16,560 --> 00:14:19,620
Intel Excel but did you realize fast CC

349
00:14:19,620 --> 00:14:22,680
for chunky and ld4 for compression it

350
00:14:22,680 --> 00:14:25,800
contains around 17 000 line of code for

351
00:14:25,800 --> 00:14:27,959
our evaluation will collected five real

352
00:14:27,959 --> 00:14:30,060
world data sets of backup workloads

353
00:14:30,060 --> 00:14:32,519
including so it's called dock image for

354
00:14:32,519 --> 00:14:35,820
system snapshot and Via VM snapshot so

355
00:14:35,820 --> 00:14:37,860
we run our experiment in a local cluster

356
00:14:37,860 --> 00:14:40,139
with multiple machines connected with 10

357
00:14:40,139 --> 00:14:43,019
gigabits ethernet and each machine here

358
00:14:43,019 --> 00:14:45,480
has an Intel Core i5 CPU and the 32

359
00:14:45,480 --> 00:14:47,639
gigabytes memory

360
00:14:47,639 --> 00:14:50,040
we first evaluate the upload and

361
00:14:50,040 --> 00:14:51,600
download performance of the overall

362
00:14:51,600 --> 00:14:54,180
system we consider a single client that

363
00:14:54,180 --> 00:14:56,579
successfully uploads the same file twice

364
00:14:56,579 --> 00:14:59,579
so to exam the maximum achievable

365
00:14:59,579 --> 00:15:01,860
performance for storing non-duplicate

366
00:15:01,860 --> 00:15:04,199
data and duvet data without the impact

367
00:15:04,199 --> 00:15:06,660
of this style so in this experiment we

368
00:15:06,660 --> 00:15:08,880
load the input file into the client's

369
00:15:08,880 --> 00:15:11,339
memory before each test and let the

370
00:15:11,339 --> 00:15:13,920
Cloud store all data in memory so we

371
00:15:13,920 --> 00:15:16,440
compare DB with three Dae approaches

372
00:15:16,440 --> 00:15:20,220
including duplex TD and CE the figure

373
00:15:20,220 --> 00:15:22,800
here shows that the DB outperforms all

374
00:15:22,800 --> 00:15:25,500
Dae approaches in upload and the reason

375
00:15:25,500 --> 00:15:28,019
is DB avoid the key generation overhead

376
00:15:28,019 --> 00:15:30,420
as well as encryption and compression

377
00:15:30,420 --> 00:15:34,019
for different data for example DB can

378
00:15:34,019 --> 00:15:36,540
achieve up to 30 times speed UPS over

379
00:15:36,540 --> 00:15:39,000
the place when I play our uploading

380
00:15:39,000 --> 00:15:41,339
duplicate data for the download speed

381
00:15:41,339 --> 00:15:44,040
compare with Dae and the DB

382
00:15:44,040 --> 00:15:49,019
DB incurs around 8 8.5 percent download

383
00:15:49,019 --> 00:15:51,019
speed drop due to the old call from

384
00:15:51,019 --> 00:15:53,699
moving chunks Into The Enclave for

385
00:15:53,699 --> 00:15:55,920
decryption and decompression but we

386
00:15:55,920 --> 00:15:58,980
argue that this overhead is limit

387
00:15:58,980 --> 00:16:01,620
we also evaluate the upflow and download

388
00:16:01,620 --> 00:16:04,380
performance in real world data set so

389
00:16:04,380 --> 00:16:06,779
unlike Imperial slices here we enable

390
00:16:06,779 --> 00:16:09,540
the cloud side this kind of and upload

391
00:16:09,540 --> 00:16:12,360
all snapshots of each data set and

392
00:16:12,360 --> 00:16:14,040
finally like the client downloads them

393
00:16:14,040 --> 00:16:17,519
on disk We compare DB with uh say

394
00:16:17,519 --> 00:16:20,639
assistance fastest Dae approaches

395
00:16:20,639 --> 00:16:23,100
the figure here shows that the speed of

396
00:16:23,100 --> 00:16:25,019
uploading and downloading each snapshot

397
00:16:25,019 --> 00:16:28,019
in BB and CE we can observe that the up

398
00:16:28,019 --> 00:16:30,779
close speed of DB gradually increase in

399
00:16:30,779 --> 00:16:33,660
subsequent snapshots as they including

400
00:16:33,660 --> 00:16:35,820
more deeply planned test chunk so DB

401
00:16:35,820 --> 00:16:37,980
does not need to perform compression and

402
00:16:37,980 --> 00:16:41,040
encryption on those stupid data in

403
00:16:41,040 --> 00:16:43,920
contrast C is consistently slower than

404
00:16:43,920 --> 00:16:45,959
DB in uploads acidic performance key

405
00:16:45,959 --> 00:16:47,759
generation and the encryption for all

406
00:16:47,759 --> 00:16:50,759
plant has changed for example in FSL the

407
00:16:50,759 --> 00:16:54,420
upload speed of DB can achieve up to 277

408
00:16:54,420 --> 00:16:56,220
megabytes per second while the maximum

409
00:16:56,220 --> 00:16:59,940
upload speed in C is just 179 megabytes

410
00:16:59,940 --> 00:17:00,959
per second

411
00:17:00,959 --> 00:17:03,899
the download speed of both DB and C are

412
00:17:03,899 --> 00:17:05,939
most things since this IO is the

413
00:17:05,939 --> 00:17:08,040
bottleneck and we can further mitigate

414
00:17:08,040 --> 00:17:10,980
this issue by using existing approaches

415
00:17:10,980 --> 00:17:13,859
to reduce random disk IO and we post

416
00:17:13,859 --> 00:17:16,740
this as our future work

417
00:17:16,740 --> 00:17:18,959
here we compare the key metadata

418
00:17:18,959 --> 00:17:22,020
overhead of DB with Dae to show DB's

419
00:17:22,020 --> 00:17:25,020
benefit in storage efficiency so we call

420
00:17:25,020 --> 00:17:28,799
that Dae approaches maintain a key for

421
00:17:28,799 --> 00:17:30,900
each type of text Chunk before the

422
00:17:30,900 --> 00:17:33,120
duplication while deep in maintain two

423
00:17:33,120 --> 00:17:36,000
long-term Keys as well as a random IV

424
00:17:36,000 --> 00:17:38,100
for each noun duplex server type chunk

425
00:17:38,100 --> 00:17:40,500
after the duplication as in traditional

426
00:17:40,500 --> 00:17:43,440
symmetric encryption so we measure the

427
00:17:43,440 --> 00:17:46,200
key metadata overhead as the size of all

428
00:17:46,200 --> 00:17:49,559
keys and the IV UC encryption so here

429
00:17:49,559 --> 00:17:52,260
the figure show key metadata overhead of

430
00:17:52,260 --> 00:17:55,020
DB and Dae after storing each snapshot

431
00:17:55,020 --> 00:17:58,140
for Dae the key metadata size increase

432
00:17:58,140 --> 00:18:00,240
linearly with the number of store

433
00:18:00,240 --> 00:18:03,720
snapshots as CAE managed per chunk piece

434
00:18:03,720 --> 00:18:07,080
so in contrast DB reduced the key meta

435
00:18:07,080 --> 00:18:11,100
size of daises the only store IV and of

436
00:18:11,100 --> 00:18:13,980
each noun duplicate server electron for

437
00:18:13,980 --> 00:18:17,400
example in FSL DB saved around 93

438
00:18:17,400 --> 00:18:21,840
percent of female data compared with Dae

439
00:18:21,840 --> 00:18:23,760
but we also evaluate the frequency

440
00:18:23,760 --> 00:18:26,700
leakage in DB so here we quantify the

441
00:18:26,700 --> 00:18:29,340
frequency leakage by kld which is a

442
00:18:29,340 --> 00:18:31,559
relative entropy to the uniform

443
00:18:31,559 --> 00:18:33,960
distribution and using Imperial study

444
00:18:33,960 --> 00:18:37,620
and a lowercase implies a header

445
00:18:37,620 --> 00:18:38,760
security

446
00:18:38,760 --> 00:18:41,280
for the Baseline We compare DB with TD

447
00:18:41,280 --> 00:18:43,500
which is the state of the art da

448
00:18:43,500 --> 00:18:45,660
approach to Define against frequency

449
00:18:45,660 --> 00:18:48,539
leakage and this idea is to store extra

450
00:18:48,539 --> 00:18:50,820
data to hide the actual frequency

451
00:18:50,820 --> 00:18:52,679
distribution of the underlying plant

452
00:18:52,679 --> 00:18:53,820
test chunk

453
00:18:53,820 --> 00:18:56,580
for DBA to increase frequency leakage

454
00:18:56,580 --> 00:18:58,380
when acquiring the full index outside

455
00:18:58,380 --> 00:19:01,679
income in second phase duplication and

456
00:19:01,679 --> 00:19:04,860
we count it is frequency distribution a

457
00:19:04,860 --> 00:19:06,780
distribution based on the encrypted

458
00:19:06,780 --> 00:19:08,820
fingerprint in the old house

459
00:19:08,820 --> 00:19:11,520
the experiment results show that DB

460
00:19:11,520 --> 00:19:14,400
outperform both C and the GED senses

461
00:19:14,400 --> 00:19:16,559
perform the first phase the duplication

462
00:19:16,559 --> 00:19:18,900
in the encave and fully protect the

463
00:19:18,900 --> 00:19:21,840
frequencies of top K frequent chunk for

464
00:19:21,840 --> 00:19:25,559
example DB reduced the kld of TD by up

465
00:19:25,559 --> 00:19:29,520
to 87 in Linux data set and well TD

466
00:19:29,520 --> 00:19:32,760
needs to store around 50 extra data to

467
00:19:32,760 --> 00:19:34,919
enhance its security

468
00:19:34,919 --> 00:19:38,220
so in conclusion DB realize dbe about

469
00:19:38,220 --> 00:19:40,080
intense Jacks it has performed the

470
00:19:40,080 --> 00:19:42,120
duplication and compression of a plain

471
00:19:42,120 --> 00:19:44,220
text Chunk in the end table to ensure

472
00:19:44,220 --> 00:19:46,380
that the conventionality will also

473
00:19:46,380 --> 00:19:48,240
propose frequencies-based deduplication

474
00:19:48,240 --> 00:19:51,240
to mitigate sjx performance overhead and

475
00:19:51,240 --> 00:19:53,580
reduce the frequency leakage over

476
00:19:53,580 --> 00:19:56,280
evaluation shows that the DB outperforms

477
00:19:56,280 --> 00:19:58,260
conventional da approaches in

478
00:19:58,260 --> 00:20:00,120
performance storage efficiency and

479
00:20:00,120 --> 00:20:02,460
security and for more detail please

480
00:20:02,460 --> 00:20:05,280
refer our paper as well as our technical

481
00:20:05,280 --> 00:20:07,500
report so thanks for your listening and

482
00:20:07,500 --> 00:20:10,580
I welcome all your questions

