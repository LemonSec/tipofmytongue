1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:13,460 --> 00:00:16,640
I'm here to talk about some

3
00:00:16,640 --> 00:00:19,859
nosql data store which Powers Prime Day

4
00:00:19,859 --> 00:00:21,960
sales and a lot of properties at Amazon

5
00:00:21,960 --> 00:00:24,859
dynamodb

6
00:00:24,859 --> 00:00:28,199
dynamodb is a lot of learnings from us

7
00:00:28,199 --> 00:00:30,359
operating and building nosql data stores

8
00:00:30,359 --> 00:00:34,079
like Dynamo and Amazon simple DB and a

9
00:00:34,079 --> 00:00:36,120
lot of what customers liked and do not

10
00:00:36,120 --> 00:00:37,100
like about

11
00:00:37,100 --> 00:00:40,739
these nosql data stores right that was a

12
00:00:40,739 --> 00:00:43,379
fully managed data nosql data store what

13
00:00:43,379 --> 00:00:45,559
I mean by fully managed is unlike

14
00:00:45,559 --> 00:00:48,000
traditional databases you're not

15
00:00:48,000 --> 00:00:50,940
provisioning servers all this is hidden

16
00:00:50,940 --> 00:00:54,780
behind all you all applications do is

17
00:00:54,780 --> 00:00:57,059
just probably create tables with the key

18
00:00:57,059 --> 00:00:59,760
schema and then can drive workload they

19
00:00:59,760 --> 00:01:01,320
do not have to worry about patching

20
00:01:01,320 --> 00:01:03,600
downtime all this is taken care under

21
00:01:03,600 --> 00:01:06,540
the scenes under the covers by Dynamo

22
00:01:06,540 --> 00:01:08,580
uh it automatically recovers from all

23
00:01:08,580 --> 00:01:10,920
failures as well we are a multi-tenant

24
00:01:10,920 --> 00:01:14,220
data store which means that uh data of

25
00:01:14,220 --> 00:01:15,659
different customers are co-located

26
00:01:15,659 --> 00:01:17,100
together

27
00:01:17,100 --> 00:01:18,479
um because we have a vast amount of

28
00:01:18,479 --> 00:01:20,400
resources and we want to be able to

29
00:01:20,400 --> 00:01:23,100
efficiently use these resources but at

30
00:01:23,100 --> 00:01:24,600
the same time we also want to make sure

31
00:01:24,600 --> 00:01:26,700
that there is good workload and security

32
00:01:26,700 --> 00:01:28,400
isolation between

33
00:01:28,400 --> 00:01:31,740
one one tables data one customer's data

34
00:01:31,740 --> 00:01:33,900
from another customer's data the schema

35
00:01:33,900 --> 00:01:35,400
is very flexible it's just a key Value

36
00:01:35,400 --> 00:01:36,960
Store you need to know what the schema

37
00:01:36,960 --> 00:01:38,700
for the key is and the value can be any

38
00:01:38,700 --> 00:01:39,860
arbitrary

39
00:01:39,860 --> 00:01:43,380
attributes and values and document

40
00:01:43,380 --> 00:01:45,299
one of the key things about Dynamo is

41
00:01:45,299 --> 00:01:48,119
that it's got very simple APS simple put

42
00:01:48,119 --> 00:01:51,479
get delete and update and query and

43
00:01:51,479 --> 00:01:54,000
scans this allows also us to have a very

44
00:01:54,000 --> 00:01:56,040
predictable performance no matter what

45
00:01:56,040 --> 00:01:58,740
the throughput of the workload is

46
00:01:58,740 --> 00:02:00,240
um and the scale of the table is so your

47
00:02:00,240 --> 00:02:02,520
table might just have four items or it

48
00:02:02,520 --> 00:02:04,200
might have like four million items but

49
00:02:04,200 --> 00:02:07,880
you can expect these simple apis to be

50
00:02:07,880 --> 00:02:10,380
very predictable and have single digit

51
00:02:10,380 --> 00:02:12,239
millisecond latencies

52
00:02:12,239 --> 00:02:14,580
we're a very high random is a very

53
00:02:14,580 --> 00:02:16,500
highly available service we have an

54
00:02:16,500 --> 00:02:18,599
external SLA of four nines for regional

55
00:02:18,599 --> 00:02:21,080
tables and five nines for Global tables

56
00:02:21,080 --> 00:02:23,879
we replicate data across three

57
00:02:23,879 --> 00:02:26,640
availability zones and availability zone

58
00:02:26,640 --> 00:02:29,819
is a group of data centers which have

59
00:02:29,819 --> 00:02:32,540
written a network and redundant

60
00:02:32,540 --> 00:02:36,300
power so to isolate each other from like

61
00:02:36,300 --> 00:02:37,920
power outages and other natural

62
00:02:37,920 --> 00:02:39,599
disasters

63
00:02:39,599 --> 00:02:42,000
uh last but not least

64
00:02:42,000 --> 00:02:43,319
um it's boundless scale so you don't

65
00:02:43,319 --> 00:02:44,760
have to worry about

66
00:02:44,760 --> 00:02:47,099
um how how much throughput your workload

67
00:02:47,099 --> 00:02:48,599
needs and what's the scale of storage

68
00:02:48,599 --> 00:02:50,280
you need because it's a seamless

69
00:02:50,280 --> 00:02:53,099
elastically scale

70
00:02:53,099 --> 00:02:55,080
just to give you an idea of the stats

71
00:02:55,080 --> 00:02:56,760
these are the stats from the previous

72
00:02:56,760 --> 00:02:58,800
three prime days

73
00:02:58,800 --> 00:03:01,739
um and this is the peak RPS we handled

74
00:03:01,739 --> 00:03:05,040
on each one of these Prime days and I'm

75
00:03:05,040 --> 00:03:07,500
sure they'll be doing a lot right now as

76
00:03:07,500 --> 00:03:09,959
well and this is just the peak irps of

77
00:03:09,959 --> 00:03:12,780
just the primary traffic we have a lot

78
00:03:12,780 --> 00:03:13,980
of other customers a lot of other

79
00:03:13,980 --> 00:03:15,720
regular traffic on top of this which we

80
00:03:15,720 --> 00:03:16,920
handle

81
00:03:16,920 --> 00:03:18,900
um with single digit millisecond

82
00:03:18,900 --> 00:03:20,879
latencies and high availability because

83
00:03:20,879 --> 00:03:23,519
any any one of them having having higher

84
00:03:23,519 --> 00:03:24,900
latency would kind of impact all

85
00:03:24,900 --> 00:03:27,360
Downstream uh Downstream consumers of

86
00:03:27,360 --> 00:03:28,019
this

87
00:03:28,019 --> 00:03:31,200
over the years as I said uh Dynamo was

88
00:03:31,200 --> 00:03:33,360
the inspiration to build nosql data

89
00:03:33,360 --> 00:03:35,400
stores uh dynamodb just started as the

90
00:03:35,400 --> 00:03:36,659
simple key value store and then we've

91
00:03:36,659 --> 00:03:38,099
been adding features on top of this as

92
00:03:38,099 --> 00:03:40,319
as and when we've been hearing back from

93
00:03:40,319 --> 00:03:45,180
customers what they wanted right uh

94
00:03:45,180 --> 00:03:46,920
one of the key things I get asked and

95
00:03:46,920 --> 00:03:49,319
every time I go everywhere I said hey

96
00:03:49,319 --> 00:03:50,580
what's the difference between Dynamo and

97
00:03:50,580 --> 00:03:52,920
dynamodb this is something I got asked

98
00:03:52,920 --> 00:03:55,200
in the booth as well there are a lot of

99
00:03:55,200 --> 00:03:57,180
a lot of differences uh dynamos single

100
00:03:57,180 --> 00:03:59,940
tenant Dynamo DB was is multi-tenant uh

101
00:03:59,940 --> 00:04:01,920
Dynamo had a different consistency model

102
00:04:01,920 --> 00:04:04,519
uh versus a dynamodb

103
00:04:04,519 --> 00:04:06,659
likewise Dynamo preferred availability

104
00:04:06,659 --> 00:04:08,580
were consistency Dynamo prefers

105
00:04:08,580 --> 00:04:10,920
consistency over availability but the

106
00:04:10,920 --> 00:04:12,299
key one of the key differences is the

107
00:04:12,299 --> 00:04:14,519
routing and storage were coupled in

108
00:04:14,519 --> 00:04:16,798
Dynamo our Dynamo is consistent hashing

109
00:04:16,798 --> 00:04:19,079
to decide where the data was stored but

110
00:04:19,079 --> 00:04:21,060
like I'm going to present and talk about

111
00:04:21,060 --> 00:04:23,300
it and uh some more slides later

112
00:04:23,300 --> 00:04:25,800
dynamodb actually has decoupled routing

113
00:04:25,800 --> 00:04:27,840
and Storage

114
00:04:27,840 --> 00:04:30,138
foreign

115
00:04:31,400 --> 00:04:33,720
architecture on how Dynamo works so that

116
00:04:33,720 --> 00:04:36,060
we can kind of go uh talk about the

117
00:04:36,060 --> 00:04:38,040
lessons we've kind of learned and

118
00:04:38,040 --> 00:04:39,240
opening the service

119
00:04:39,240 --> 00:04:40,440
so I'm going to work through this

120
00:04:40,440 --> 00:04:42,540
architecture using a simple put item as

121
00:04:42,540 --> 00:04:46,080
in as an example so when a customer when

122
00:04:46,080 --> 00:04:47,820
an application issues like put item call

123
00:04:47,820 --> 00:04:49,979
it Travis is the network and then

124
00:04:49,979 --> 00:04:52,199
reaches a from a load balance to the

125
00:04:52,199 --> 00:04:54,720
request router so request order has got

126
00:04:54,720 --> 00:04:56,940
a specific set of Duties one is first

127
00:04:56,940 --> 00:04:58,979
authenticate the call make sure it's the

128
00:04:58,979 --> 00:05:00,360
right person right application right

129
00:05:00,360 --> 00:05:03,600
right person calling the thing using AWS

130
00:05:03,600 --> 00:05:06,120
identity and access management service

131
00:05:06,120 --> 00:05:08,460
and then it looks up metadata to say to

132
00:05:08,460 --> 00:05:12,060
figure out okay wait which way to find

133
00:05:12,060 --> 00:05:14,040
the key what table does it this request

134
00:05:14,040 --> 00:05:15,840
belong to Etc

135
00:05:15,840 --> 00:05:17,400
and then kind of does some admission

136
00:05:17,400 --> 00:05:19,680
control decisions and then forwards it

137
00:05:19,680 --> 00:05:21,660
to the Right Storage node uh which can

138
00:05:21,660 --> 00:05:23,699
which can actually serve uh serve this

139
00:05:23,699 --> 00:05:25,020
request

140
00:05:25,020 --> 00:05:26,280
and that storage node is then

141
00:05:26,280 --> 00:05:28,199
responsible for accepting the request

142
00:05:28,199 --> 00:05:30,900
and replicating it across its peers on

143
00:05:30,900 --> 00:05:33,919
different availability zones

144
00:05:34,020 --> 00:05:36,539
why do we need to do all this we are not

145
00:05:36,539 --> 00:05:37,979
a single request or a single storage

146
00:05:37,979 --> 00:05:39,960
node there's thousands and thousands of

147
00:05:39,960 --> 00:05:41,520
request routers and thousands thousands

148
00:05:41,520 --> 00:05:43,680
of storage nodes so we need to locate

149
00:05:43,680 --> 00:05:46,139
this data and any one of this request

150
00:05:46,139 --> 00:05:48,900
routers can actually get this data and

151
00:05:48,900 --> 00:05:51,060
any these components as across different

152
00:05:51,060 --> 00:05:53,160
availability zones as I mentioned before

153
00:05:53,160 --> 00:05:55,320
so the request status plus metadata

154
00:05:55,320 --> 00:05:57,539
actually provides us the decoupling we

155
00:05:57,539 --> 00:06:00,720
need from the storage so requesters are

156
00:06:00,720 --> 00:06:02,220
responsible for routing the data out of

157
00:06:02,220 --> 00:06:03,900
storage and the clients don't have to

158
00:06:03,900 --> 00:06:08,359
worry about hey where does my data go

159
00:06:08,699 --> 00:06:11,400
once the request order finds out the

160
00:06:11,400 --> 00:06:13,139
nodes hosting the data it sends out in

161
00:06:13,139 --> 00:06:14,520
this case it's a put item request so it

162
00:06:14,520 --> 00:06:16,199
has to be routed to a leader

163
00:06:16,199 --> 00:06:17,580
um it routes it to the leader and the

164
00:06:17,580 --> 00:06:19,320
leader is then responsible for uh

165
00:06:19,320 --> 00:06:22,440
replicating uh that they are changed uh

166
00:06:22,440 --> 00:06:24,300
to its peers and once a quorum of peers

167
00:06:24,300 --> 00:06:27,479
accepts the uh change he acknowledge

168
00:06:27,479 --> 00:06:28,979
back to the customer saying hey yes your

169
00:06:28,979 --> 00:06:31,620
right is durable uh the leader is a

170
00:06:31,620 --> 00:06:33,120
leader election for redirection we use

171
00:06:33,120 --> 00:06:35,720
the version of paxos multipack source

172
00:06:35,720 --> 00:06:38,280
and leader periodically heartbeats to

173
00:06:38,280 --> 00:06:39,600
its peers

174
00:06:39,600 --> 00:06:42,419
um if the peers detect that that leaders

175
00:06:42,419 --> 00:06:44,220
failed a new leader gets selected very

176
00:06:44,220 --> 00:06:46,520
quickly

177
00:06:46,800 --> 00:06:49,919
now dynamodb scales partitions uh Tables

178
00:06:49,919 --> 00:06:51,300
by partitioning them

179
00:06:51,300 --> 00:06:53,699
uh so I'm going to work through an

180
00:06:53,699 --> 00:06:55,199
example here this is a very simple

181
00:06:55,199 --> 00:06:57,419
example of a customer information table

182
00:06:57,419 --> 00:07:02,220
where the key the unique key identifying

183
00:07:02,220 --> 00:07:03,419
the row is going to be the customer ID

184
00:07:03,419 --> 00:07:05,100
which is the good and then the customer

185
00:07:05,100 --> 00:07:06,840
information has various attributes like

186
00:07:06,840 --> 00:07:09,419
name address ZIP code a bunch of these

187
00:07:09,419 --> 00:07:10,580
things right

188
00:07:10,580 --> 00:07:14,160
now how how is Dynamo going to partition

189
00:07:14,160 --> 00:07:18,360
this uh table uh so Dynamo basically

190
00:07:18,360 --> 00:07:20,520
under the under the scene takes the the

191
00:07:20,520 --> 00:07:22,259
customer ID which is unique customers

192
00:07:22,259 --> 00:07:23,580
specified hey this is going to be my key

193
00:07:23,580 --> 00:07:26,400
which uniquely identifies the row and

194
00:07:26,400 --> 00:07:29,099
then runs it through a one-way hash and

195
00:07:29,099 --> 00:07:31,380
this hash allows us to random randomly

196
00:07:31,380 --> 00:07:33,599
distribute the data across different uh

197
00:07:33,599 --> 00:07:36,539
across the table

198
00:07:36,539 --> 00:07:39,060
and once this uh once it's hashed then

199
00:07:39,060 --> 00:07:41,580
we decide okay fine uh we'll Define we

200
00:07:41,580 --> 00:07:43,800
will kind of divide the table based on

201
00:07:43,800 --> 00:07:46,919
this these hashes right

202
00:07:46,919 --> 00:07:48,900
uh We've now divided the stable into

203
00:07:48,900 --> 00:07:50,220
three different partitions based on the

204
00:07:50,220 --> 00:07:53,220
hashes uh the first first partition gets

205
00:07:53,220 --> 00:07:58,199
uh keys from 0x00 to 0x uh 6ff and the

206
00:07:58,199 --> 00:08:01,220
second partition gets data from 0x7 to B

207
00:08:01,220 --> 00:08:04,860
0xb and the third one gets uh zero x uh

208
00:08:04,860 --> 00:08:07,319
C to zero x f

209
00:08:07,319 --> 00:08:10,080
so now this kind of allows this this

210
00:08:10,080 --> 00:08:12,240
partitioning how do we decide how do we

211
00:08:12,240 --> 00:08:13,919
how many partitions do we create that

212
00:08:13,919 --> 00:08:15,479
depends on what what the throughput

213
00:08:15,479 --> 00:08:17,039
requirements of the table is what the

214
00:08:17,039 --> 00:08:21,139
size of the table is right uh

215
00:08:22,740 --> 00:08:24,539
now the data from this partitions is

216
00:08:24,539 --> 00:08:26,160
replicated through uas so they can be

217
00:08:26,160 --> 00:08:28,259
stored in a uh they're going to be

218
00:08:28,259 --> 00:08:30,900
stored in different storage nodes each

219
00:08:30,900 --> 00:08:32,399
one of the storage units as I said is

220
00:08:32,399 --> 00:08:34,919
multi-tenant so replicas are partition

221
00:08:34,919 --> 00:08:36,299
partition data from other multiple

222
00:08:36,299 --> 00:08:37,620
tables can be located along with the

223
00:08:37,620 --> 00:08:40,200
stuff uh for example the green partition

224
00:08:40,200 --> 00:08:42,299
is stored on the green storage nodes the

225
00:08:42,299 --> 00:08:43,559
Orange is the orange storage nodes

226
00:08:43,559 --> 00:08:45,060
across this one and the pink on the pink

227
00:08:45,060 --> 00:08:46,380
storage notes

228
00:08:46,380 --> 00:08:48,180
and all this data is stored in something

229
00:08:48,180 --> 00:08:50,399
called as a partition map whose key

230
00:08:50,399 --> 00:08:52,380
value is a key which is keyed by the

231
00:08:52,380 --> 00:08:54,260
table table name and the partition ID

232
00:08:54,260 --> 00:08:56,399
and the partition map contains

233
00:08:56,399 --> 00:08:57,779
information like okay this is a key

234
00:08:57,779 --> 00:08:59,100
range and these are nodes which are

235
00:08:59,100 --> 00:09:01,980
storing this data right and this allows

236
00:09:01,980 --> 00:09:04,200
the request order to find out okay where

237
00:09:04,200 --> 00:09:05,640
where is this particular key which which

238
00:09:05,640 --> 00:09:06,899
storage you know to Route this

239
00:09:06,899 --> 00:09:09,620
particular key to

240
00:09:09,959 --> 00:09:12,120
uh I briefly mentioned about how the

241
00:09:12,120 --> 00:09:13,620
polishing of the table is determined now

242
00:09:13,620 --> 00:09:15,180
in this section we'll go over one of one

243
00:09:15,180 --> 00:09:16,860
of the lessons we learned um we're

244
00:09:16,860 --> 00:09:18,480
operating this operating the system over

245
00:09:18,480 --> 00:09:20,580
a period of time which is basically that

246
00:09:20,580 --> 00:09:23,160
we had to adapt to customers traffic to

247
00:09:23,160 --> 00:09:25,620
actually kind of uh decide the physical

248
00:09:25,620 --> 00:09:27,180
partitioning scheme or how we

249
00:09:27,180 --> 00:09:29,820
implemented admission control right

250
00:09:29,820 --> 00:09:31,440
so why is admission control necessary

251
00:09:31,440 --> 00:09:33,540
it's slightly unique to Dynamo because

252
00:09:33,540 --> 00:09:34,920
Dynamo is one of the systems where you

253
00:09:34,920 --> 00:09:36,600
don't provision instances but you

254
00:09:36,600 --> 00:09:38,640
actually kind of uh used to provision

255
00:09:38,640 --> 00:09:40,980
tables using read capacity units and

256
00:09:40,980 --> 00:09:43,019
write capacity units so one lead

257
00:09:43,019 --> 00:09:45,240
capacity unit would allow you to read up

258
00:09:45,240 --> 00:09:47,339
to a four kilobyte item with strongly

259
00:09:47,339 --> 00:09:49,740
consistent read and one one dead

260
00:09:49,740 --> 00:09:51,420
capacity unit would allow you to write

261
00:09:51,420 --> 00:09:56,160
an item of up to one kilobyte uh

262
00:09:56,519 --> 00:09:59,399
now ah this is very different from most

263
00:09:59,399 --> 00:10:00,660
other systems

264
00:10:00,660 --> 00:10:03,000
um so when when Dynamo when we partition

265
00:10:03,000 --> 00:10:05,519
a table what we did is that we took this

266
00:10:05,519 --> 00:10:07,560
capacity read and write capacity units

267
00:10:07,560 --> 00:10:10,260
and uh distributed evenly across all the

268
00:10:10,260 --> 00:10:12,600
partitions uh in this example I'm going

269
00:10:12,600 --> 00:10:14,459
to just use the read capacity units skip

270
00:10:14,459 --> 00:10:16,860
the right capacity units uh this table

271
00:10:16,860 --> 00:10:18,600
was provisioned let's say this table was

272
00:10:18,600 --> 00:10:20,640
provision with 300 read capacity units

273
00:10:20,640 --> 00:10:22,080
then each one of these parallations

274
00:10:22,080 --> 00:10:25,440
would get 100 read capacity units

275
00:10:25,440 --> 00:10:27,540
now we had to implement workload

276
00:10:27,540 --> 00:10:29,100
isolation simplest way to do this

277
00:10:29,100 --> 00:10:30,540
workload isolation is a token bucket

278
00:10:30,540 --> 00:10:32,760
right like you have 100 the token bucket

279
00:10:32,760 --> 00:10:34,500
is 100 read capacity units which is

280
00:10:34,500 --> 00:10:38,459
filled at a rate of 100 rcus

281
00:10:38,459 --> 00:10:42,180
this works great because uh if the if

282
00:10:42,180 --> 00:10:43,980
there's a token in the token bucket the

283
00:10:43,980 --> 00:10:45,720
request is admitted and if the token is

284
00:10:45,720 --> 00:10:47,579
not there it's great the request is

285
00:10:47,579 --> 00:10:49,140
rejected and there's good workload

286
00:10:49,140 --> 00:10:51,000
isolation completely for uh across

287
00:10:51,000 --> 00:10:53,760
multiple partitions

288
00:10:53,760 --> 00:10:56,940
but what we found in practice was that

289
00:10:56,940 --> 00:10:59,279
it's really hard for customers to

290
00:10:59,279 --> 00:11:01,820
actually get uniform distribution of uh

291
00:11:01,820 --> 00:11:04,980
request rate across time right so most

292
00:11:04,980 --> 00:11:06,480
of the time the partitions would be

293
00:11:06,480 --> 00:11:08,459
serving more than less than 100 right

294
00:11:08,459 --> 00:11:10,079
capacity units and then there will be a

295
00:11:10,079 --> 00:11:12,300
spike of traffic which is greater than

296
00:11:12,300 --> 00:11:14,160
100 read capacity units

297
00:11:14,160 --> 00:11:16,380
what happens then the customer gets

298
00:11:16,380 --> 00:11:18,180
throttle and this this was kind of a

299
00:11:18,180 --> 00:11:21,180
very bad a bad experience overall and

300
00:11:21,180 --> 00:11:23,100
they would have an uh they would have to

301
00:11:23,100 --> 00:11:25,019
retry this request incurring higher

302
00:11:25,019 --> 00:11:27,480
latency so how did customers work around

303
00:11:27,480 --> 00:11:29,459
this stuff they kind of over provisioned

304
00:11:29,459 --> 00:11:31,980
the table right uh by all approaching

305
00:11:31,980 --> 00:11:33,600
the table the partition would have more

306
00:11:33,600 --> 00:11:36,240
capacity which basically meant that they

307
00:11:36,240 --> 00:11:38,519
had to plan for their Peak capacity at

308
00:11:38,519 --> 00:11:40,440
this point in time to decide what what

309
00:11:40,440 --> 00:11:42,360
would not get them uh have them

310
00:11:42,360 --> 00:11:43,620
throttled

311
00:11:43,620 --> 00:11:45,000
so most of the time a lot of the

312
00:11:45,000 --> 00:11:47,100
capacity was wasted this was very

313
00:11:47,100 --> 00:11:48,839
inefficient for us and inefficient for

314
00:11:48,839 --> 00:11:49,860
the customers

315
00:11:49,860 --> 00:11:52,800
so what do we do a simple thing right

316
00:11:52,800 --> 00:11:54,240
like uh back in the day I don't know

317
00:11:54,240 --> 00:11:55,680
about like five six years ago cellular

318
00:11:55,680 --> 00:11:57,420
plants have this rollover minutes you

319
00:11:57,420 --> 00:11:58,980
could roll over your minutes up to a

320
00:11:58,980 --> 00:12:01,800
particular uh particular Max limit a

321
00:12:01,800 --> 00:12:04,140
very similar thing we allowed bursting

322
00:12:04,140 --> 00:12:05,640
um so we increase the capacity of the

323
00:12:05,640 --> 00:12:08,100
bucket to a Max of uh three thousand uh

324
00:12:08,100 --> 00:12:10,860
real capacity units or whatever it is

325
00:12:10,860 --> 00:12:13,079
five minutes worth of recapacity when

326
00:12:13,079 --> 00:12:14,760
it's allocated for the table

327
00:12:14,760 --> 00:12:17,279
and said okay if you're not using a

328
00:12:17,279 --> 00:12:18,660
capacity for up to five minutes we're

329
00:12:18,660 --> 00:12:20,040
going to accumulate that you can

330
00:12:20,040 --> 00:12:21,480
accumulate that capacity and later use

331
00:12:21,480 --> 00:12:24,600
it what this allowed customers to do is

332
00:12:24,600 --> 00:12:27,300
to have spiky burst traffic but this

333
00:12:27,300 --> 00:12:29,040
also meant that we had to make sure that

334
00:12:29,040 --> 00:12:30,300
the no at the node level you had

335
00:12:30,300 --> 00:12:31,740
capacity so that you don't interfere

336
00:12:31,740 --> 00:12:35,360
with other workloads overall right

337
00:12:36,540 --> 00:12:38,220
now the other thing which customers

338
00:12:38,220 --> 00:12:39,680
found really hard

339
00:12:39,680 --> 00:12:43,980
was uniform distribution uh over keys

340
00:12:43,980 --> 00:12:45,959
let's say that I'm running a census app

341
00:12:45,959 --> 00:12:47,940
I'm running a census app and the data

342
00:12:47,940 --> 00:12:50,100
and the table is partitioned by postal

343
00:12:50,100 --> 00:12:52,260
codes or ZIP codes of Canada right from

344
00:12:52,260 --> 00:12:54,720
this map you can see that there's going

345
00:12:54,720 --> 00:12:56,279
to be a very few set of postal codes or

346
00:12:56,279 --> 00:12:57,480
zip codes which is going to get really

347
00:12:57,480 --> 00:13:00,540
high traffic versus the other the ones

348
00:13:00,540 --> 00:13:03,860
about the red line

349
00:13:04,500 --> 00:13:07,380
so those partitions will get throttled

350
00:13:07,380 --> 00:13:09,360
because they're constantly running out

351
00:13:09,360 --> 00:13:10,800
of tokens from their bucket even though

352
00:13:10,800 --> 00:13:12,660
they had burst capacity because after

353
00:13:12,660 --> 00:13:15,180
the burst burst capacity is over you had

354
00:13:15,180 --> 00:13:16,860
to still take time to fill capacity in

355
00:13:16,860 --> 00:13:19,740
the token bucket while other partitions

356
00:13:19,740 --> 00:13:21,720
actually had capacity in their token

357
00:13:21,720 --> 00:13:23,160
buckets

358
00:13:23,160 --> 00:13:25,380
so a key takeaway from this was though

359
00:13:25,380 --> 00:13:27,180
bursting was great it solved some of the

360
00:13:27,180 --> 00:13:28,500
customers problems

361
00:13:28,500 --> 00:13:30,540
um we had it wasn't sufficient enough

362
00:13:30,540 --> 00:13:34,500
and we had tightly coupled how partition

363
00:13:34,500 --> 00:13:37,380
level capacity to admission control

364
00:13:37,380 --> 00:13:40,500
right so we realized that removing the

365
00:13:40,500 --> 00:13:41,940
admission control from partition from

366
00:13:41,940 --> 00:13:43,019
partition level would be the most

367
00:13:43,019 --> 00:13:44,579
beneficial thing for customers and for

368
00:13:44,579 --> 00:13:46,380
us as well and letting the partition

369
00:13:46,380 --> 00:13:48,779
burst always so we implemented something

370
00:13:48,779 --> 00:13:50,220
called as a global admission control

371
00:13:50,220 --> 00:13:52,079
where we moved our mission control up

372
00:13:52,079 --> 00:13:53,040
front

373
00:13:53,040 --> 00:13:55,019
now GAC is a service that builds on the

374
00:13:55,019 --> 00:13:56,880
same token bucket mechanism

375
00:13:56,880 --> 00:13:58,440
um so when a request order receives a

376
00:13:58,440 --> 00:14:00,060
request it basically requests the GAC to

377
00:14:00,060 --> 00:14:01,860
kind of get some tokens

378
00:14:01,860 --> 00:14:04,980
um and yeah once request order uses the

379
00:14:04,980 --> 00:14:06,540
local tokens to make admission admission

380
00:14:06,540 --> 00:14:09,060
control decisions if it's runs out of

381
00:14:09,060 --> 00:14:10,920
tokens it can it can actually requests

382
00:14:10,920 --> 00:14:12,720
for more tokens if there's no capacity

383
00:14:12,720 --> 00:14:13,860
GAC as they're not going to give the

384
00:14:13,860 --> 00:14:16,079
request order tokens and requests will

385
00:14:16,079 --> 00:14:17,579
be rejected this helped a lot of

386
00:14:17,579 --> 00:14:19,560
customers because now customers are able

387
00:14:19,560 --> 00:14:21,540
to burst across their partitions uh

388
00:14:21,540 --> 00:14:23,220
constantly and

389
00:14:23,220 --> 00:14:24,060
um

390
00:14:24,060 --> 00:14:27,300
uh have not have less throttling

391
00:14:27,300 --> 00:14:29,700
this also meant that we had to have very

392
00:14:29,700 --> 00:14:31,019
smart placement algorithms of these

393
00:14:31,019 --> 00:14:32,459
partitions and replicas across our

394
00:14:32,459 --> 00:14:33,660
entire field of thousands of storage

395
00:14:33,660 --> 00:14:36,000
nodes because to be able to burst a

396
00:14:36,000 --> 00:14:38,160
partition to burst always we needed to

397
00:14:38,160 --> 00:14:40,199
have capacity at the node level so a lot

398
00:14:40,199 --> 00:14:42,000
of the uh work went into how do we

399
00:14:42,000 --> 00:14:43,980
smartly places partitions and replicas

400
00:14:43,980 --> 00:14:45,300
across this entire field of storage

401
00:14:45,300 --> 00:14:47,279
nodes so that we can have partitions

402
00:14:47,279 --> 00:14:49,820
burst always

403
00:14:50,100 --> 00:14:51,899
now I want to switch case uh so they're

404
00:14:51,899 --> 00:14:53,100
very little time I'm going to talk about

405
00:14:53,100 --> 00:14:55,440
another lesson we learned where we there

406
00:14:55,440 --> 00:14:56,820
we said

407
00:14:56,820 --> 00:14:59,459
um we had we need to build this to build

408
00:14:59,459 --> 00:15:01,440
systems for predictability uh over

409
00:15:01,440 --> 00:15:03,779
absolute efficiency so that we can

410
00:15:03,779 --> 00:15:05,880
improve the system stability how do we

411
00:15:05,880 --> 00:15:06,779
learn this

412
00:15:06,779 --> 00:15:08,760
back in 2015 three years into the

413
00:15:08,760 --> 00:15:12,480
service we had a very large event it was

414
00:15:12,480 --> 00:15:14,100
quite a bad bad day not only for Dynamo

415
00:15:14,100 --> 00:15:15,480
but for dynamos customers as well

416
00:15:15,480 --> 00:15:16,399
because

417
00:15:16,399 --> 00:15:18,839
they couldn't reach us

418
00:15:18,839 --> 00:15:21,540
right why did we have this outage so we

419
00:15:21,540 --> 00:15:22,920
talked about how a put item request

420
00:15:22,920 --> 00:15:25,139
works the request order request uh for

421
00:15:25,139 --> 00:15:27,720
the partition my data it actually gets a

422
00:15:27,720 --> 00:15:30,300
partition map and this was because the

423
00:15:30,300 --> 00:15:32,399
partition metadata was skied off a table

424
00:15:32,399 --> 00:15:34,139
name and a partition ID so it had to get

425
00:15:34,139 --> 00:15:36,000
the entire list of partitions for the

426
00:15:36,000 --> 00:15:36,720
table

427
00:15:36,720 --> 00:15:39,360
to look up where the key is located this

428
00:15:39,360 --> 00:15:41,040
worked great in a regular way because a

429
00:15:41,040 --> 00:15:42,240
request would request this information

430
00:15:42,240 --> 00:15:45,660
it would get cached locally and then the

431
00:15:45,660 --> 00:15:46,680
request order could reuse this

432
00:15:46,680 --> 00:15:49,579
information all the time

433
00:15:49,800 --> 00:15:52,199
in 2015 this information slowly grew we

434
00:15:52,199 --> 00:15:55,079
were growing really big and caches were

435
00:15:55,079 --> 00:15:56,639
actually hiding the fact that this data

436
00:15:56,639 --> 00:15:58,079
had kind of grown big for every every

437
00:15:58,079 --> 00:16:01,500
table in the animal right and

438
00:16:01,500 --> 00:16:03,839
when a lot of requests starters a lot of

439
00:16:03,839 --> 00:16:05,519
clients for this data just not request

440
00:16:05,519 --> 00:16:07,320
orders but other components Montana but

441
00:16:07,320 --> 00:16:09,540
started requesting this data

442
00:16:09,540 --> 00:16:11,040
the partition met address system could

443
00:16:11,040 --> 00:16:13,560
not handle this so we were kind of uh in

444
00:16:13,560 --> 00:16:15,420
a in a state of a metastable failure

445
00:16:15,420 --> 00:16:17,160
which was one of the talks before where

446
00:16:17,160 --> 00:16:18,899
we talked about metastable failures in

447
00:16:18,899 --> 00:16:20,160
the wild

448
00:16:20,160 --> 00:16:21,779
um and the partition metadata system

449
00:16:21,779 --> 00:16:23,279
could not we couldn't even scale out the

450
00:16:23,279 --> 00:16:24,600
partition monitor system to actually

451
00:16:24,600 --> 00:16:27,839
kind of recover from this scenario right

452
00:16:27,839 --> 00:16:30,480
why did this happen because the catches

453
00:16:30,480 --> 00:16:32,160
actually hit the fact that we are part

454
00:16:32,160 --> 00:16:34,320
of our Parish map cash was growing

455
00:16:34,320 --> 00:16:37,019
um we had a 99.75 percent of uh hit rate

456
00:16:37,019 --> 00:16:39,899
so caches were lovely but all of a

457
00:16:39,899 --> 00:16:43,079
sudden when the cash it is ratio ratio

458
00:16:43,079 --> 00:16:46,740
um uh was uh have the cachet ratio was

459
00:16:46,740 --> 00:16:48,360
Zero all this traffic went to partition

460
00:16:48,360 --> 00:16:51,899
my data so from a missed ratio of 0.25

461
00:16:51,899 --> 00:16:54,060
percent we went to a missed the issue of

462
00:16:54,060 --> 00:16:57,660
100 which is a 400x pike in uh traffic

463
00:16:57,660 --> 00:16:58,860
rate

464
00:16:58,860 --> 00:17:00,420
uh

465
00:17:00,420 --> 00:17:03,779
so to recover from this we literally had

466
00:17:03,779 --> 00:17:06,059
to kind of turn off querying metadata

467
00:17:06,059 --> 00:17:07,500
for a long time which meant that we

468
00:17:07,500 --> 00:17:10,079
couldn't conduct a requests and then

469
00:17:10,079 --> 00:17:12,179
scale out the partition Manila system uh

470
00:17:12,179 --> 00:17:14,339
to remediate the system uh to remediate

471
00:17:14,339 --> 00:17:15,000
um

472
00:17:15,000 --> 00:17:17,160
the event

473
00:17:17,160 --> 00:17:19,079
but we wanted to think long term right

474
00:17:19,079 --> 00:17:22,079
uh so what we built something called as

475
00:17:22,079 --> 00:17:24,900
a memory data store which is uh off uh

476
00:17:24,900 --> 00:17:27,000
which is a distributed uh in-memory

477
00:17:27,000 --> 00:17:29,640
cache there are two Key Properties to

478
00:17:29,640 --> 00:17:31,080
this one is that we realized we didn't

479
00:17:31,080 --> 00:17:32,940
need the entire partition map cache to

480
00:17:32,940 --> 00:17:35,100
server request we just needed that one

481
00:17:35,100 --> 00:17:36,539
single piece of data of which partition

482
00:17:36,539 --> 00:17:38,640
was which partition was which three

483
00:17:38,640 --> 00:17:40,679
stories was hosting this data

484
00:17:40,679 --> 00:17:43,440
so MDS is uh we change the data

485
00:17:43,440 --> 00:17:45,120
structure we're using we build it uses

486
00:17:45,120 --> 00:17:46,500
the purple which is a hybrid of a

487
00:17:46,500 --> 00:17:48,660
Patricia tree and a Merkel uh it

488
00:17:48,660 --> 00:17:50,220
supports queries such as floor and

489
00:17:50,220 --> 00:17:52,020
ceiling which basically takes a key and

490
00:17:52,020 --> 00:17:53,580
says okay give me the flow value of this

491
00:17:53,580 --> 00:17:55,440
particular uh thing which actually leads

492
00:17:55,440 --> 00:17:57,360
us to find the right partitioning uh

493
00:17:57,360 --> 00:17:59,880
hosting that particular key

494
00:17:59,880 --> 00:18:01,380
uh

495
00:18:01,380 --> 00:18:04,380
it also so now with members we could

496
00:18:04,380 --> 00:18:05,880
send this request we can scale out

497
00:18:05,880 --> 00:18:08,039
horizontally like we have a lot of MDS

498
00:18:08,039 --> 00:18:09,360
nodes which scale out horizontally we

499
00:18:09,360 --> 00:18:12,500
get the single single data we want right

500
00:18:12,500 --> 00:18:15,900
but even though members we cache this

501
00:18:15,900 --> 00:18:18,600
data locally in request order still we

502
00:18:18,600 --> 00:18:21,179
actually keep sending uh requests back

503
00:18:21,179 --> 00:18:24,299
to members asynchronously for every

504
00:18:24,299 --> 00:18:26,580
request even if you get a cash hit the

505
00:18:26,580 --> 00:18:28,320
reason we do this is that we want the

506
00:18:28,320 --> 00:18:31,380
system to actually do constant work all

507
00:18:31,380 --> 00:18:34,679
the time right we don't want the system

508
00:18:34,679 --> 00:18:36,480
a system to have a sudden spike in

509
00:18:36,480 --> 00:18:38,100
traffic and not be able to deal with

510
00:18:38,100 --> 00:18:40,080
that stuff so members is constantly

511
00:18:40,080 --> 00:18:41,880
we're doing constant work across members

512
00:18:41,880 --> 00:18:44,100
so for every request Dynamo gets members

513
00:18:44,100 --> 00:18:47,360
gets the same amount of requests

514
00:18:47,760 --> 00:18:49,380
I would like to conclude by talking

515
00:18:49,380 --> 00:18:50,460
about

516
00:18:50,460 --> 00:18:53,220
um the four lessons uh we've learned and

517
00:18:53,220 --> 00:18:54,960
we kind of talked about in the paper but

518
00:18:54,960 --> 00:18:57,000
I talked about the first two the other

519
00:18:57,000 --> 00:18:59,039
two have not kind of talked about uh and

520
00:18:59,039 --> 00:19:00,539
you can kind of refer to the paper is

521
00:19:00,539 --> 00:19:02,520
that continuous verification of data is

522
00:19:02,520 --> 00:19:04,080
super important uh we learned this old

523
00:19:04,080 --> 00:19:07,200
uh operating the system in 10 years

524
00:19:07,200 --> 00:19:08,820
um there are Hardware bugs software bugs

525
00:19:08,820 --> 00:19:10,020
so we need to have be able to have

526
00:19:10,020 --> 00:19:12,840
continuously verify and detect uh any uh

527
00:19:12,840 --> 00:19:16,679
any corruption to data at rest the other

528
00:19:16,679 --> 00:19:19,559
thing is that as Dynamo grows there are

529
00:19:19,559 --> 00:19:21,240
a lot of lot of lot of services which

530
00:19:21,240 --> 00:19:22,679
are needed to support a lot of features

531
00:19:22,679 --> 00:19:24,539
we I talked about

532
00:19:24,539 --> 00:19:26,820
um and this requires careful operational

533
00:19:26,820 --> 00:19:30,360
tooling and discipline right so we use a

534
00:19:30,360 --> 00:19:31,620
lot of different things we talked about

535
00:19:31,620 --> 00:19:33,960
earlier in the conference such as formal

536
00:19:33,960 --> 00:19:36,120
methods fast testing upgrade downgrade

537
00:19:36,120 --> 00:19:38,760
testing chaos testing so there's a lot

538
00:19:38,760 --> 00:19:40,320
of work which goes behind the scenes to

539
00:19:40,320 --> 00:19:42,360
get every feature released and also make

540
00:19:42,360 --> 00:19:44,600
sure that we do not lose our correctness

541
00:19:44,600 --> 00:19:47,820
properties of the uh

542
00:19:47,820 --> 00:19:50,039
system

543
00:19:50,039 --> 00:19:51,539
with that I would like to conclude my

544
00:19:51,539 --> 00:19:52,320
talk

545
00:19:52,320 --> 00:19:53,580
um thanks for staying for the last 20

546
00:19:53,580 --> 00:19:54,539
minutes I know this is the last

547
00:19:54,539 --> 00:19:56,640
presentation of the last talk uh last

548
00:19:56,640 --> 00:19:59,280
session if you want to chat more here's

549
00:19:59,280 --> 00:20:01,020
my email address and my Twitter handle

550
00:20:01,020 --> 00:20:03,539
please feel free to talk send me an

551
00:20:03,539 --> 00:20:05,720
email

