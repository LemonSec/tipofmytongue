1
00:00:12,960 --> 00:00:15,920
greetings everyone my name is raymond lu

2
00:00:15,920 --> 00:00:18,080
i'm here to share our work on evaluating

3
00:00:18,080 --> 00:00:21,359
the reliability of over 1.8 million nvme

4
00:00:21,359 --> 00:00:24,480
ssd deployed at alibaba

5
00:00:24,480 --> 00:00:26,480
this is a joint work of researchers from

6
00:00:26,480 --> 00:00:29,679
shanghai university xiamen university in

7
00:00:29,679 --> 00:00:33,679
cooperation with alibaba group

8
00:00:34,239 --> 00:00:36,079
hardware failures are one of the major

9
00:00:36,079 --> 00:00:39,360
hurdles to maintain service reliability

10
00:00:39,360 --> 00:00:41,200
it is crucial to understand the

11
00:00:41,200 --> 00:00:44,399
reliability of hardware in the cloud in

12
00:00:44,399 --> 00:00:46,559
this study we focus on studying the

13
00:00:46,559 --> 00:00:50,640
reliability of storage devices

14
00:00:50,960 --> 00:00:52,879
over the last decade the storage

15
00:00:52,879 --> 00:00:55,039
landscape has changed

16
00:00:55,039 --> 00:00:56,800
data centers are widely adopting flash

17
00:00:56,800 --> 00:01:00,160
flash-based ssds instead of hard drives

18
00:01:00,160 --> 00:01:02,399
in recent years there are several works

19
00:01:02,399 --> 00:01:05,360
investigating ssd reliability

20
00:01:05,360 --> 00:01:07,920
for example they study the impact of ssd

21
00:01:07,920 --> 00:01:10,560
internals failure rate curves and

22
00:01:10,560 --> 00:01:12,720
correlated failures

23
00:01:12,720 --> 00:01:15,520
however the ssds they focused on are

24
00:01:15,520 --> 00:01:18,000
early generations with a sas or sata

25
00:01:18,000 --> 00:01:19,759
interface

26
00:01:19,759 --> 00:01:22,240
nowadays we clearly have more options

27
00:01:22,240 --> 00:01:25,040
for storage devices

28
00:01:25,040 --> 00:01:27,520
nvme ssd has been on the market for

29
00:01:27,520 --> 00:01:29,200
several years

30
00:01:29,200 --> 00:01:30,720
with gigabytes per second level

31
00:01:30,720 --> 00:01:33,200
bandwidth and microsecond level latency

32
00:01:33,200 --> 00:01:36,240
nvme ssd is widely adopted in today's

33
00:01:36,240 --> 00:01:38,560
data centers

34
00:01:38,560 --> 00:01:42,400
compared to early ssds nvme ssd is not

35
00:01:42,400 --> 00:01:46,079
just as a simple interface upgrade

36
00:01:46,079 --> 00:01:48,720
many recent changes in reliability have

37
00:01:48,720 --> 00:01:51,200
been well integrated

38
00:01:51,200 --> 00:01:53,840
for example the wild adoption of 3d net

39
00:01:53,840 --> 00:01:54,720
flash

40
00:01:54,720 --> 00:01:59,439
ring protection and ldpc coding

41
00:01:59,439 --> 00:02:01,520
with such considerable changes the

42
00:02:01,520 --> 00:02:06,798
reliability of nvme ssd remains unknown

43
00:02:07,360 --> 00:02:09,598
in our work we conduct two failure

44
00:02:09,598 --> 00:02:10,720
studies

45
00:02:10,720 --> 00:02:13,120
first we provide a comparative field

46
00:02:13,120 --> 00:02:14,879
stop study

47
00:02:14,879 --> 00:02:16,720
we start with existing findings and

48
00:02:16,720 --> 00:02:18,879
examine whether such findings persist in

49
00:02:18,879 --> 00:02:21,440
nvme ssd

50
00:02:21,440 --> 00:02:23,680
second we conduct a quantitative fail

51
00:02:23,680 --> 00:02:25,040
slow study

52
00:02:25,040 --> 00:02:27,200
we identify philosophical failures as a

53
00:02:27,200 --> 00:02:29,680
widespread and severe problem in nvme

54
00:02:29,680 --> 00:02:31,760
ssd

55
00:02:31,760 --> 00:02:33,760
we investigate the impact factors and

56
00:02:33,760 --> 00:02:37,680
the fail slow to fail stop transition

57
00:02:37,680 --> 00:02:41,040
next i will introduce our data set

58
00:02:41,040 --> 00:02:43,440
our data set consists of over 1.8

59
00:02:43,440 --> 00:02:46,400
million nvme ssd deployed at alibaba

60
00:02:46,400 --> 00:02:48,000
data centers

61
00:02:48,000 --> 00:02:50,400
all of them are enterprise level

62
00:02:50,400 --> 00:02:52,640
the ssd fleet covers various drive

63
00:02:52,640 --> 00:02:55,599
specifications with a diverse mix of

64
00:02:55,599 --> 00:02:58,000
drive age and usage

65
00:02:58,000 --> 00:02:59,920
a total of seven cloud services are

66
00:02:59,920 --> 00:03:03,879
served by these ssds

67
00:03:04,000 --> 00:03:06,319
we collect device level data covering

68
00:03:06,319 --> 00:03:08,720
the smart locks failure tickets and

69
00:03:08,720 --> 00:03:10,879
performance logs

70
00:03:10,879 --> 00:03:13,280
these data are collected from the device

71
00:03:13,280 --> 00:03:17,519
monitoring daemons and iostat command

72
00:03:17,760 --> 00:03:20,000
all our datasets are released in public

73
00:03:20,000 --> 00:03:22,080
now for opennexus

74
00:03:22,080 --> 00:03:23,519
we hope to inspire researchers to

75
00:03:23,519 --> 00:03:25,519
conduct valuable studies that benefit

76
00:03:25,519 --> 00:03:28,159
the community

77
00:03:28,720 --> 00:03:31,519
we begin our study by considering the

78
00:03:31,519 --> 00:03:34,480
field stop failures

79
00:03:35,120 --> 00:03:37,760
to understand when storage devices fail

80
00:03:37,760 --> 00:03:39,360
we first take a look at the failure

81
00:03:39,360 --> 00:03:40,879
curve

82
00:03:40,879 --> 00:03:43,280
the first question is how do fader rates

83
00:03:43,280 --> 00:03:45,920
vary as the device gets older and more

84
00:03:45,920 --> 00:03:48,400
worn out

85
00:03:48,400 --> 00:03:49,840
there are already answers for hard

86
00:03:49,840 --> 00:03:52,239
drives 15 years ago

87
00:03:52,239 --> 00:03:55,360
at infant stage that is upon the first

88
00:03:55,360 --> 00:03:57,360
few months of deployment

89
00:03:57,360 --> 00:04:00,560
the failure rates are notably high

90
00:04:00,560 --> 00:04:03,040
this is known as the information for

91
00:04:03,040 --> 00:04:05,599
hard drives

92
00:04:05,840 --> 00:04:08,720
then in 2015 researchers examined the

93
00:04:08,720 --> 00:04:11,760
failure curve of ssds in facebook

94
00:04:11,760 --> 00:04:13,760
they discovered that ssd would

95
00:04:13,760 --> 00:04:16,000
experience a rise in failure rates at an

96
00:04:16,000 --> 00:04:17,680
early age

97
00:04:17,680 --> 00:04:21,519
before the infant mortality arrives

98
00:04:21,918 --> 00:04:24,479
in 2020 researchers examined the failure

99
00:04:24,479 --> 00:04:26,840
curve of sas in state ssds in

100
00:04:26,840 --> 00:04:29,759
netapp similarly they discovered that

101
00:04:29,759 --> 00:04:31,520
informationality could extend to more

102
00:04:31,520 --> 00:04:33,120
than a year

103
00:04:33,120 --> 00:04:35,120
during the infant stage the failure rate

104
00:04:35,120 --> 00:04:37,120
could be two to three times larger than

105
00:04:37,120 --> 00:04:39,840
later in life

106
00:04:39,840 --> 00:04:41,919
to sum up previous studies point out

107
00:04:41,919 --> 00:04:44,560
that flash-based ssds would experience a

108
00:04:44,560 --> 00:04:48,720
drawn-out period of infermentality

109
00:04:49,440 --> 00:04:52,479
so what about the nvme ssd here is one

110
00:04:52,479 --> 00:04:54,240
example

111
00:04:54,240 --> 00:04:56,160
as we can see the failure rates

112
00:04:56,160 --> 00:04:58,160
oscillate around the average values

113
00:04:58,160 --> 00:05:00,000
across time

114
00:05:00,000 --> 00:05:02,320
also during the first few months the

115
00:05:02,320 --> 00:05:04,320
failure rates are not notably high

116
00:05:04,320 --> 00:05:07,199
compared to later in time

117
00:05:07,199 --> 00:05:09,280
we have examined the failure curves of

118
00:05:09,280 --> 00:05:11,840
six major nvme ssd models in our data

119
00:05:11,840 --> 00:05:13,039
set

120
00:05:13,039 --> 00:05:14,800
all of them do not have outstanding

121
00:05:14,800 --> 00:05:18,320
informationality during early periods

122
00:05:18,320 --> 00:05:20,400
in particular the failure rates in the

123
00:05:20,400 --> 00:05:22,560
first three months are equivalent to or

124
00:05:22,560 --> 00:05:26,160
even lower than the later periods

125
00:05:26,160 --> 00:05:28,160
in later periods failure rates still

126
00:05:28,160 --> 00:05:30,240
oscillate around the average values and

127
00:05:30,240 --> 00:05:32,320
is not notably high

128
00:05:32,320 --> 00:05:34,880
this indicates that even in later

129
00:05:34,880 --> 00:05:35,840
periods

130
00:05:35,840 --> 00:05:39,199
by up to 40 months of deploy nvme ssd

131
00:05:39,199 --> 00:05:43,600
still does not reach wear out period

132
00:05:43,600 --> 00:05:45,919
let's go back to informativity

133
00:05:45,919 --> 00:05:47,440
according to previous studies

134
00:05:47,440 --> 00:05:49,759
informationality is notable in sas and

135
00:05:49,759 --> 00:05:51,199
sata ssds

136
00:05:51,199 --> 00:05:53,600
in this sense we are interested in why

137
00:05:53,600 --> 00:05:57,280
it is the case for nvme ssd

138
00:05:57,280 --> 00:05:59,759
we examine the smart attributes during

139
00:05:59,759 --> 00:06:02,320
the first 15 months of deployment

140
00:06:02,320 --> 00:06:04,319
note that these two figures in the slide

141
00:06:04,319 --> 00:06:06,000
are from the same drive model for

142
00:06:06,000 --> 00:06:08,560
straight comparison

143
00:06:08,560 --> 00:06:12,000
smart locks record device level errors

144
00:06:12,000 --> 00:06:14,319
as we can see the device errors are

145
00:06:14,319 --> 00:06:16,880
notably high at infant stage

146
00:06:16,880 --> 00:06:19,039
and then gradually decrease to a much

147
00:06:19,039 --> 00:06:21,199
lower value

148
00:06:21,199 --> 00:06:23,840
in other words drives still produce and

149
00:06:23,840 --> 00:06:26,400
accumulate much more internal errors at

150
00:06:26,400 --> 00:06:28,160
infant stage

151
00:06:28,160 --> 00:06:30,319
so why the corresponding failure rates

152
00:06:30,319 --> 00:06:32,880
are still mild and not notable

153
00:06:32,880 --> 00:06:34,880
while on the other hand the federation

154
00:06:34,880 --> 00:06:39,199
size and sata ssd are non-negligible

155
00:06:39,440 --> 00:06:41,520
we assume that this is likely due to the

156
00:06:41,520 --> 00:06:44,800
recent improvement in fdl error handling

157
00:06:44,800 --> 00:06:47,680
the fdl may tolerate and mask arrows so

158
00:06:47,680 --> 00:06:49,759
that drives will still be functioning

159
00:06:49,759 --> 00:06:52,560
with considerable arrows

160
00:06:52,560 --> 00:06:54,960
therefore we do not see high fader rates

161
00:06:54,960 --> 00:06:59,440
during the early deployment of nvme ssds

162
00:06:59,440 --> 00:07:02,319
in previous practice to tackle infant

163
00:07:02,319 --> 00:07:04,880
mortality cloud operators usually have

164
00:07:04,880 --> 00:07:07,440
to stockpile extra pieces before initial

165
00:07:07,440 --> 00:07:08,720
deployment

166
00:07:08,720 --> 00:07:10,639
so that they can replace failed drives

167
00:07:10,639 --> 00:07:14,240
in time during early deployment

168
00:07:14,240 --> 00:07:16,400
since the infant mortality in nvme ssd

169
00:07:16,400 --> 00:07:19,199
is not notable it is unnecessary to stop

170
00:07:19,199 --> 00:07:21,440
our extra pieces now

171
00:07:21,440 --> 00:07:23,360
this can serve as a relief signal for

172
00:07:23,360 --> 00:07:24,960
supply chain as well as system

173
00:07:24,960 --> 00:07:27,680
administrators

174
00:07:28,400 --> 00:07:30,240
we also investigate how right

175
00:07:30,240 --> 00:07:34,160
amplification affects ssd reliability

176
00:07:34,160 --> 00:07:36,000
right amplification is usually

177
00:07:36,000 --> 00:07:38,720
attributed to ssd internal operations

178
00:07:38,720 --> 00:07:40,720
like gc for example

179
00:07:40,720 --> 00:07:42,960
the corresponding right amplification

180
00:07:42,960 --> 00:07:45,280
factor is calculated by dividing the

181
00:07:45,280 --> 00:07:48,880
amount of nand rights by logical rights

182
00:07:48,880 --> 00:07:50,960
usually right amplification factor is

183
00:07:50,960 --> 00:07:52,720
above one

184
00:07:52,720 --> 00:07:54,479
in some cases where the rights are

185
00:07:54,479 --> 00:07:58,319
compressed it could be less than one

186
00:07:58,319 --> 00:08:00,319
previously researchers from microsoft

187
00:08:00,319 --> 00:08:03,360
divide their ssds into waf buckets and

188
00:08:03,360 --> 00:08:06,080
calculate the feeder rates

189
00:08:06,080 --> 00:08:08,400
as we can see failure rates are high at

190
00:08:08,400 --> 00:08:09,840
both ends

191
00:08:09,840 --> 00:08:11,759
meaning that the sata ssd would

192
00:08:11,759 --> 00:08:14,160
experience high failure rates at both

193
00:08:14,160 --> 00:08:18,080
low and high wf levels

194
00:08:18,080 --> 00:08:21,520
also for wf above 1 the failure rates

195
00:08:21,520 --> 00:08:26,240
seem to increase as wf increases

196
00:08:26,240 --> 00:08:29,440
so what about an nvme ssd

197
00:08:29,440 --> 00:08:31,280
following the same methodology we'll

198
00:08:31,280 --> 00:08:33,120
examine the failure rates at different

199
00:08:33,120 --> 00:08:36,320
wf levels

200
00:08:36,320 --> 00:08:39,440
first when wf is above is below 1 the

201
00:08:39,440 --> 00:08:42,159
failure rate is still obviously high

202
00:08:42,159 --> 00:08:44,800
which is 2.19 times higher than the

203
00:08:44,800 --> 00:08:47,680
average of all bars

204
00:08:47,680 --> 00:08:51,120
second nvme ssd becomes more robust to

205
00:08:51,120 --> 00:08:53,279
high rate amplification

206
00:08:53,279 --> 00:08:56,320
this indicates that nvme ssd is less

207
00:08:56,320 --> 00:08:58,640
affected by random small rights

208
00:08:58,640 --> 00:09:00,720
which is a major cause of high right

209
00:09:00,720 --> 00:09:03,360
amplification

210
00:09:03,360 --> 00:09:05,760
such a trend persists even after we

211
00:09:05,760 --> 00:09:07,839
exclude the influence of workloads and

212
00:09:07,839 --> 00:09:10,480
drive usage

213
00:09:10,480 --> 00:09:12,640
recall that data compression could lead

214
00:09:12,640 --> 00:09:15,760
to a less than one waf

215
00:09:15,760 --> 00:09:18,560
we further check the waf distribution of

216
00:09:18,560 --> 00:09:21,360
our nvme ssd fleet

217
00:09:21,360 --> 00:09:24,640
in fact extremely low wf is rare in our

218
00:09:24,640 --> 00:09:25,920
case

219
00:09:25,920 --> 00:09:28,480
only three point three percent of nvme

220
00:09:28,480 --> 00:09:30,399
ssc's are experiencing a less than one

221
00:09:30,399 --> 00:09:32,800
waf

222
00:09:32,800 --> 00:09:36,480
therefore for nvme ssds low wf levels

223
00:09:36,480 --> 00:09:39,519
are still rare but deadly while high wf

224
00:09:39,519 --> 00:09:41,519
levels do not come with high failure

225
00:09:41,519 --> 00:09:43,120
rates

226
00:09:43,120 --> 00:09:45,360
in previous practice system software

227
00:09:45,360 --> 00:09:47,839
developers focus on reducing ssd right

228
00:09:47,839 --> 00:09:50,000
amplification

229
00:09:50,000 --> 00:09:52,959
recall that in nvme ssd high write-down

230
00:09:52,959 --> 00:09:55,839
amplification does not necessarily mean

231
00:09:55,839 --> 00:09:58,480
a high failure rate

232
00:09:58,480 --> 00:10:00,480
therefore the design vision and design

233
00:10:00,480 --> 00:10:02,959
goal of developing systems with low

234
00:10:02,959 --> 00:10:04,800
right amplification support may be

235
00:10:04,800 --> 00:10:07,279
shifted

236
00:10:08,080 --> 00:10:10,240
lastly we investigate the correlated

237
00:10:10,240 --> 00:10:11,600
failures

238
00:10:11,600 --> 00:10:13,440
correlated failures are notorious in

239
00:10:13,440 --> 00:10:16,240
these distributed systems for example

240
00:10:16,240 --> 00:10:18,399
multiple independent components could

241
00:10:18,399 --> 00:10:20,560
fail within a short time posing a

242
00:10:20,560 --> 00:10:22,160
significant threat to system

243
00:10:22,160 --> 00:10:24,640
availability

244
00:10:24,640 --> 00:10:26,399
therefore it is important to understand

245
00:10:26,399 --> 00:10:28,240
the distribution of correlative failures

246
00:10:28,240 --> 00:10:30,560
in ssds

247
00:10:30,560 --> 00:10:32,560
failures could be both spatially and

248
00:10:32,560 --> 00:10:35,200
temporarily correlated for example

249
00:10:35,200 --> 00:10:37,040
correlated failures could arrive at the

250
00:10:37,040 --> 00:10:38,959
same node within a few minutes in

251
00:10:38,959 --> 00:10:40,720
sequence

252
00:10:40,720 --> 00:10:42,720
previously researchers from alibaba

253
00:10:42,720 --> 00:10:44,720
analyzed the currently failures in sata

254
00:10:44,720 --> 00:10:46,560
ssd

255
00:10:46,560 --> 00:10:48,880
first they divide ssd failures that

256
00:10:48,880 --> 00:10:51,360
occur at the same geographical locations

257
00:10:51,360 --> 00:10:54,800
into entry node and inter rank failures

258
00:10:54,800 --> 00:10:57,360
then based on the arrival time they are

259
00:10:57,360 --> 00:10:59,040
interested in failures that arrive

260
00:10:59,040 --> 00:11:01,360
within a certain time interval

261
00:11:01,360 --> 00:11:03,680
for example from zero to one minute or

262
00:11:03,680 --> 00:11:06,560
from one day to one month

263
00:11:06,560 --> 00:11:08,320
here is an illustration of the currently

264
00:11:08,320 --> 00:11:10,480
defeated distribution

265
00:11:10,480 --> 00:11:13,440
obviously in nearly all models internode

266
00:11:13,440 --> 00:11:15,600
and inter rank failures mostly arrive

267
00:11:15,600 --> 00:11:17,440
within one minute

268
00:11:17,440 --> 00:11:19,680
and thus spatially currently failures

269
00:11:19,680 --> 00:11:21,519
are temporary correlated and a short

270
00:11:21,519 --> 00:11:23,839
span

271
00:11:24,399 --> 00:11:27,120
so what about an nvme ssd

272
00:11:27,120 --> 00:11:29,120
following the same methodology we

273
00:11:29,120 --> 00:11:30,640
analyze the distribution of current

274
00:11:30,640 --> 00:11:33,760
developers nvme system

275
00:11:33,760 --> 00:11:35,760
here is an example showing the internal

276
00:11:35,760 --> 00:11:38,320
node failure distribution

277
00:11:38,320 --> 00:11:40,240
the trend is similar for inter rank

278
00:11:40,240 --> 00:11:43,040
failures so they are not shown here

279
00:11:43,040 --> 00:11:46,079
we make two observations here

280
00:11:46,079 --> 00:11:48,560
first let's focus on short-term failures

281
00:11:48,560 --> 00:11:51,120
by looking at the red bars

282
00:11:51,120 --> 00:11:52,720
recall that short-term failures are

283
00:11:52,720 --> 00:11:55,200
dominant in sata ssd

284
00:11:55,200 --> 00:11:58,399
however in nvme ssd such failures are no

285
00:11:58,399 --> 00:12:01,680
longer prevalent within one minute

286
00:12:01,680 --> 00:12:05,760
in some models they are even negligible

287
00:12:05,760 --> 00:12:07,839
on the contrary long-term currently

288
00:12:07,839 --> 00:12:09,920
failures between one day and one month

289
00:12:09,920 --> 00:12:12,639
become notably frequent

290
00:12:12,639 --> 00:12:15,200
although in sata ssd long-term currently

291
00:12:15,200 --> 00:12:17,519
failures are almost negligible they

292
00:12:17,519 --> 00:12:19,680
become the dominant ones in nvme and

293
00:12:19,680 --> 00:12:22,079
system now

294
00:12:22,079 --> 00:12:24,160
in practice the decline in short-term

295
00:12:24,160 --> 00:12:26,000
correlated failures could generally

296
00:12:26,000 --> 00:12:28,399
imply a lower risk of system-wise

297
00:12:28,399 --> 00:12:29,920
failures

298
00:12:29,920 --> 00:12:32,639
however at least in our data centers we

299
00:12:32,639 --> 00:12:35,360
usually start with software based

300
00:12:35,360 --> 00:12:38,399
approaches when fixing dry failures

301
00:12:38,399 --> 00:12:40,240
like data scrubbing or file system

302
00:12:40,240 --> 00:12:41,839
checkers

303
00:12:41,839 --> 00:12:43,440
as a general approach

304
00:12:43,440 --> 00:12:45,440
such online checking and repairing could

305
00:12:45,440 --> 00:12:47,920
take time

306
00:12:47,920 --> 00:12:50,000
so we further checked the repair time

307
00:12:50,000 --> 00:12:52,560
recorded in affiliate tickets

308
00:12:52,560 --> 00:12:55,519
as a result around 43.9

309
00:12:55,519 --> 00:12:57,600
of drive failures take more than a day

310
00:12:57,600 --> 00:12:59,680
to repair

311
00:12:59,680 --> 00:13:01,920
based on our findings we have refined

312
00:13:01,920 --> 00:13:04,800
our operation process

313
00:13:04,800 --> 00:13:07,760
now we directly put drives offline upon

314
00:13:07,760 --> 00:13:09,279
failures

315
00:13:09,279 --> 00:13:11,200
in this case we can reduce the chances

316
00:13:11,200 --> 00:13:13,200
of suffering from long-term current

317
00:13:13,200 --> 00:13:15,600
failures

318
00:13:16,720 --> 00:13:19,040
despite failed stock failures we also

319
00:13:19,040 --> 00:13:21,279
conduct a quantitative failsale failure

320
00:13:21,279 --> 00:13:24,639
study in nvme sst

321
00:13:24,639 --> 00:13:27,040
fail stop drives are already reported in

322
00:13:27,040 --> 00:13:29,120
failure tickets but what about failed

323
00:13:29,120 --> 00:13:30,320
slow drives

324
00:13:30,320 --> 00:13:33,120
how to identify them there are two major

325
00:13:33,120 --> 00:13:35,360
challenges

326
00:13:35,360 --> 00:13:37,360
first there's no ground truth in

327
00:13:37,360 --> 00:13:39,680
identifying fail slow

328
00:13:39,680 --> 00:13:41,600
unlike field star failures where the

329
00:13:41,600 --> 00:13:43,120
oracle is clear

330
00:13:43,120 --> 00:13:45,279
there are a thousand ways to define fail

331
00:13:45,279 --> 00:13:47,040
slope

332
00:13:47,040 --> 00:13:49,360
for example also an engineer is often

333
00:13:49,360 --> 00:13:52,079
confused and asked how slow is the drive

334
00:13:52,079 --> 00:13:54,240
to be considered fail slow

335
00:13:54,240 --> 00:13:55,760
and the answers may vary across

336
00:13:55,760 --> 00:13:58,160
different people

337
00:13:58,160 --> 00:14:00,480
maybe when the eye latency is above 100

338
00:14:00,480 --> 00:14:02,160
microseconds

339
00:14:02,160 --> 00:14:04,480
maybe sometimes about 500

340
00:14:04,480 --> 00:14:06,560
or to be even more strict about one

341
00:14:06,560 --> 00:14:09,199
millisecond

342
00:14:10,000 --> 00:14:11,600
the second challenge is that the

343
00:14:11,600 --> 00:14:14,720
underlying root causes are hard to tell

344
00:14:14,720 --> 00:14:17,199
there's no octopus ounces there are many

345
00:14:17,199 --> 00:14:19,040
aspects that could cause performance

346
00:14:19,040 --> 00:14:20,959
degradation

347
00:14:20,959 --> 00:14:23,279
for example by hardware failures like

348
00:14:23,279 --> 00:14:24,720
the failed slope

349
00:14:24,720 --> 00:14:28,160
or by a temporarily heavy workload

350
00:14:28,160 --> 00:14:32,079
or by a one shot high gc time

351
00:14:33,040 --> 00:14:35,360
to identify the fail slow we make use of

352
00:14:35,360 --> 00:14:37,519
the latency time series collected by the

353
00:14:37,519 --> 00:14:39,680
monitoring demons

354
00:14:39,680 --> 00:14:42,000
here is one example showing the latency

355
00:14:42,000 --> 00:14:44,480
time series of 12 drives from the same

356
00:14:44,480 --> 00:14:46,399
node

357
00:14:46,399 --> 00:14:48,480
clearly there is one drive suffering

358
00:14:48,480 --> 00:14:51,600
from a period of slow down

359
00:14:51,600 --> 00:14:53,920
therefore the first step is to identify

360
00:14:53,920 --> 00:14:56,240
any suspicious drives with abnormally

361
00:14:56,240 --> 00:14:59,120
high latency

362
00:14:59,120 --> 00:15:01,600
to do so we first calculate a statistic

363
00:15:01,600 --> 00:15:03,199
bond

364
00:15:03,199 --> 00:15:05,199
if a drive has higher resistances than

365
00:15:05,199 --> 00:15:08,160
this one from more than half of the time

366
00:15:08,160 --> 00:15:10,320
we then mark this drive as suspect of

367
00:15:10,320 --> 00:15:13,120
failed slope other drives from the same

368
00:15:13,120 --> 00:15:16,720
node are referred to as peer drives

369
00:15:16,720 --> 00:15:18,639
we also place a similar upper bound on

370
00:15:18,639 --> 00:15:22,399
the throughput and iops time series

371
00:15:22,399 --> 00:15:24,240
in this case we could filter out

372
00:15:24,240 --> 00:15:26,240
performance degradation caused by heavy

373
00:15:26,240 --> 00:15:28,800
workloads

374
00:15:29,440 --> 00:15:31,360
the second step is to identify the slow

375
00:15:31,360 --> 00:15:33,519
down events

376
00:15:33,519 --> 00:15:35,920
here by peer evaluation we compare the

377
00:15:35,920 --> 00:15:38,160
suspicious field strike fail slow drive

378
00:15:38,160 --> 00:15:41,440
with its normal peers

379
00:15:41,440 --> 00:15:43,920
specifically we place a sliding window

380
00:15:43,920 --> 00:15:45,680
along the temporal dimension

381
00:15:45,680 --> 00:15:48,160
incorporating consecutive slow records

382
00:15:48,160 --> 00:15:51,120
into slowdown events

383
00:15:51,120 --> 00:15:53,600
intuitively a slowdown event should last

384
00:15:53,600 --> 00:15:56,160
longer than the sliding window side

385
00:15:56,160 --> 00:15:58,160
in fact we set four values for the

386
00:15:58,160 --> 00:16:00,880
sliding window lens varying from 5

387
00:16:00,880 --> 00:16:04,399
minutes to 60 minutes long

388
00:16:04,399 --> 00:16:07,120
since normal gc usually takes less than

389
00:16:07,120 --> 00:16:08,800
a few seconds

390
00:16:08,800 --> 00:16:10,959
a sliding window of several to thousands

391
00:16:10,959 --> 00:16:13,360
of minutes long is enough to filter out

392
00:16:13,360 --> 00:16:17,680
the side effects brought by normal gc

393
00:16:19,279 --> 00:16:21,759
we hypothesized that the outer low

394
00:16:21,759 --> 00:16:24,639
latency nature of nvme ssd could make

395
00:16:24,639 --> 00:16:28,480
itself prone to fail slow failures

396
00:16:28,480 --> 00:16:30,880
for strict comparison we also identified

397
00:16:30,880 --> 00:16:33,279
the philosophers in hard drives and the

398
00:16:33,279 --> 00:16:35,519
results are clear

399
00:16:35,519 --> 00:16:38,240
failsafe failure is a wild spray concern

400
00:16:38,240 --> 00:16:40,320
for nvme ssd

401
00:16:40,320 --> 00:16:42,959
those figures show that nvme ssd could

402
00:16:42,959 --> 00:16:45,839
be dozens of times more likely to suffer

403
00:16:45,839 --> 00:16:47,680
from failed cell failures than hard

404
00:16:47,680 --> 00:16:50,639
drives

405
00:16:50,639 --> 00:16:52,320
we also take a look at the severity of

406
00:16:52,320 --> 00:16:54,160
fail slow

407
00:16:54,160 --> 00:16:56,399
clearly the average latency of slow down

408
00:16:56,399 --> 00:16:59,519
events in most nvme ssd models are

409
00:16:59,519 --> 00:17:02,639
already hundreds of microseconds

410
00:17:02,639 --> 00:17:05,520
this indicate that fail slow nvme ssd

411
00:17:05,520 --> 00:17:07,760
could degrade to sati ssd level

412
00:17:07,760 --> 00:17:10,160
performance

413
00:17:10,160 --> 00:17:12,640
moreover we checked the top one percent

414
00:17:12,640 --> 00:17:14,559
slowest event

415
00:17:14,559 --> 00:17:17,280
surprisingly a small fraction of nvme

416
00:17:17,280 --> 00:17:19,919
ssd even degrades to hdd level

417
00:17:19,919 --> 00:17:21,280
performance

418
00:17:21,280 --> 00:17:23,599
and the latency of these slowest events

419
00:17:23,599 --> 00:17:26,000
reaches an average latency of over 11

420
00:17:26,000 --> 00:17:28,720
milliseconds

421
00:17:28,880 --> 00:17:31,120
therefore failsale failure is also a

422
00:17:31,120 --> 00:17:35,280
severe problem for nvme ssd

423
00:17:36,400 --> 00:17:38,080
we also take a look at the

424
00:17:38,080 --> 00:17:41,760
transition from fail slow to full stop

425
00:17:41,760 --> 00:17:44,320
we collect up-to-date failure tickets

426
00:17:44,320 --> 00:17:46,559
the latest one is five months older than

427
00:17:46,559 --> 00:17:49,600
the last recorded slowdown event

428
00:17:49,600 --> 00:17:52,160
and we are interested in whether any

429
00:17:52,160 --> 00:17:54,400
field stop drives are fail slowed in the

430
00:17:54,400 --> 00:17:56,400
first place

431
00:17:56,400 --> 00:17:59,120
which corresponds to the intersection of

432
00:17:59,120 --> 00:18:02,879
fail stop and fail slow drives

433
00:18:03,039 --> 00:18:05,840
in fact there are only 10 drives transit

434
00:18:05,840 --> 00:18:07,919
from fail slow to failstar

435
00:18:07,919 --> 00:18:11,840
accounting for less than 0.01

436
00:18:12,400 --> 00:18:14,799
thus the transition from fail slow to

437
00:18:14,799 --> 00:18:18,559
fail stop is rarely observed

438
00:18:19,280 --> 00:18:21,280
we also have examined the failed slow

439
00:18:21,280 --> 00:18:22,960
event rate occurrences

440
00:18:22,960 --> 00:18:24,880
identify several failed slow impact

441
00:18:24,880 --> 00:18:27,360
factors and investigate the correlation

442
00:18:27,360 --> 00:18:30,559
with smart attributes

443
00:18:30,880 --> 00:18:33,520
and for the conclusions

444
00:18:33,520 --> 00:18:35,600
by drawing side-by-side comparisons with

445
00:18:35,600 --> 00:18:37,919
existing reliability studies over sas

446
00:18:37,919 --> 00:18:40,880
and sata ssds we have identified three

447
00:18:40,880 --> 00:18:45,679
major reliability changes in nvme ssd

448
00:18:46,240 --> 00:18:50,320
first informativity is not notable

449
00:18:50,320 --> 00:18:53,280
second extremely low right amplification

450
00:18:53,280 --> 00:18:55,120
is still rare but deadly

451
00:18:55,120 --> 00:18:56,880
while higher right ethnic right

452
00:18:56,880 --> 00:19:00,799
amplification is not a big concern

453
00:19:00,880 --> 00:19:03,039
third spatially currently figures are

454
00:19:03,039 --> 00:19:05,120
temporarily correlated only in the long

455
00:19:05,120 --> 00:19:07,120
term span

456
00:19:07,120 --> 00:19:09,280
short-term quality failures become much

457
00:19:09,280 --> 00:19:11,840
less frequent

458
00:19:13,039 --> 00:19:15,200
moreover we conduct the first large

459
00:19:15,200 --> 00:19:17,360
scale study on failsale failures in

460
00:19:17,360 --> 00:19:19,440
storage devices

461
00:19:19,440 --> 00:19:21,840
we identify identified failsall failure

462
00:19:21,840 --> 00:19:24,160
as a widespread and severe problem for

463
00:19:24,160 --> 00:19:26,880
nvme ssds

464
00:19:26,880 --> 00:19:28,799
we investigate the impact factors of

465
00:19:28,799 --> 00:19:30,880
fail soil failures

466
00:19:30,880 --> 00:19:33,280
we showed through statistics tests that

467
00:19:33,280 --> 00:19:35,919
hardware arrows in smart attributes are

468
00:19:35,919 --> 00:19:39,600
not good indicators of fail slow

469
00:19:39,600 --> 00:19:41,840
lastly we confirm that the transition

470
00:19:41,840 --> 00:19:44,000
from from fell slow to full stop is

471
00:19:44,000 --> 00:19:45,520
rarely observed

472
00:19:45,520 --> 00:19:47,919
at least not in a short interval of 5

473
00:19:47,919 --> 00:19:50,240
months

474
00:19:50,720 --> 00:19:52,640
thanks for listening if you have any

475
00:19:52,640 --> 00:19:54,880
questions feel free to contact us while

476
00:19:54,880 --> 00:19:58,679
the email at the bottom

