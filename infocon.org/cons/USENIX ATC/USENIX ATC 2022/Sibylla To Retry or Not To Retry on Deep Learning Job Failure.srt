1
00:00:13,759 --> 00:00:16,480
hello everyone my name is tim kim today

2
00:00:16,480 --> 00:00:19,440
i'm gonna talk about

3
00:00:20,080 --> 00:00:22,000
timer does not work

4
00:00:22,000 --> 00:00:25,119
fine okay then i will restart hello

5
00:00:25,119 --> 00:00:27,599
everyone my name is tim kim today i'm

6
00:00:27,599 --> 00:00:29,840
gonna talk about how to improve failure

7
00:00:29,840 --> 00:00:32,399
handling or deep learning job failure on

8
00:00:32,399 --> 00:00:34,079
shared gpu cluster

9
00:00:34,079 --> 00:00:36,000
this work was done on collaboration with

10
00:00:36,000 --> 00:00:37,360
srian zhang

11
00:00:37,360 --> 00:00:41,200
zhongzhou lee sube lee and mj at unist

12
00:00:41,200 --> 00:00:43,760
a shared gpu cluster provide hardware

13
00:00:43,760 --> 00:00:45,920
and software to various users for team

14
00:00:45,920 --> 00:00:48,000
learning model training and this user

15
00:00:48,000 --> 00:00:49,920
can take advantage of several

16
00:00:49,920 --> 00:00:51,920
functionality provided by this

17
00:00:51,920 --> 00:00:54,160
infrastructure such as resource

18
00:00:54,160 --> 00:00:56,640
scheduling storing data model and

19
00:00:56,640 --> 00:00:59,280
handling the failure

20
00:00:59,280 --> 00:01:02,000
this year gpu cluster is now commonly

21
00:01:02,000 --> 00:01:04,400
used to run their workloads in many

22
00:01:04,400 --> 00:01:06,880
companies such as microsoft amazon and

23
00:01:06,880 --> 00:01:08,720
google

24
00:01:08,720 --> 00:01:11,200
in this shared gpu cluster one major

25
00:01:11,200 --> 00:01:13,360
source of resource inefficiency is

26
00:01:13,360 --> 00:01:15,520
unsuccessful job completion due to the

27
00:01:15,520 --> 00:01:17,040
job failure

28
00:01:17,040 --> 00:01:18,960
that failure occurs during daily

29
00:01:18,960 --> 00:01:21,920
training and this failure results in a

30
00:01:21,920 --> 00:01:24,479
waste of gpu resources used until the

31
00:01:24,479 --> 00:01:27,520
failure occurs previous studies focus on

32
00:01:27,520 --> 00:01:29,920
workload characterization and

33
00:01:29,920 --> 00:01:32,000
discuss how this failure affect the

34
00:01:32,000 --> 00:01:33,920
cluster utilization

35
00:01:33,920 --> 00:01:35,680
they found there are many types of

36
00:01:35,680 --> 00:01:38,240
failure in shared gpu cluster and it is

37
00:01:38,240 --> 00:01:40,479
hard to generalize and categorize the

38
00:01:40,479 --> 00:01:42,560
failures based on each reason for the

39
00:01:42,560 --> 00:01:43,759
failure

40
00:01:43,759 --> 00:01:45,759
our research differs from this study in

41
00:01:45,759 --> 00:01:48,399
that we present an actual method to

42
00:01:48,399 --> 00:01:51,200
leverage knowledge from prior works

43
00:01:51,200 --> 00:01:51,920
so

44
00:01:51,920 --> 00:01:54,799
the key research question is how do we

45
00:01:54,799 --> 00:01:56,799
deal with various failures to enhance

46
00:01:56,799 --> 00:01:59,920
the cluster resource utilization

47
00:01:59,920 --> 00:02:02,000
for failure handling we can largely

48
00:02:02,000 --> 00:02:04,719
classify all existing job failure into

49
00:02:04,719 --> 00:02:06,159
two categories

50
00:02:06,159 --> 00:02:09,840
first deterministic of or dt failure and

51
00:02:09,840 --> 00:02:12,000
second non-deterministic

52
00:02:12,000 --> 00:02:13,920
or entity failure

53
00:02:13,920 --> 00:02:16,480
dt failure is repetitive so it will

54
00:02:16,480 --> 00:02:19,680
repeat with retry on failure such as

55
00:02:19,680 --> 00:02:22,800
syntax error and corrupted data

56
00:02:22,800 --> 00:02:24,879
on the other hand entity failure is

57
00:02:24,879 --> 00:02:27,200
stringent and can be overcome with retry

58
00:02:27,200 --> 00:02:29,280
on failure such as transient network

59
00:02:29,280 --> 00:02:30,480
failures

60
00:02:30,480 --> 00:02:33,120
now let's look how existing shared gpu

61
00:02:33,120 --> 00:02:35,360
cluster handle failures in the presence

62
00:02:35,360 --> 00:02:38,400
of dt and ndt

63
00:02:38,400 --> 00:02:40,560
when the user submits the training job

64
00:02:40,560 --> 00:02:42,959
and the job is scheduled the cluster

65
00:02:42,959 --> 00:02:44,959
manager place the job onto the cluster

66
00:02:44,959 --> 00:02:46,000
execution

67
00:02:46,000 --> 00:02:49,040
now let's assume that training fails

68
00:02:49,040 --> 00:02:51,920
fails before javascription complete

69
00:02:51,920 --> 00:02:54,000
existing approach takes simple heuristic

70
00:02:54,000 --> 00:02:56,400
to handle this failure the first

71
00:02:56,400 --> 00:02:58,560
approach is performing a fixed number of

72
00:02:58,560 --> 00:03:00,400
retries on failure

73
00:03:00,400 --> 00:03:03,200
really microsoft cluster scheduler has

74
00:03:03,200 --> 00:03:06,080
this kind of handling strategy

75
00:03:06,080 --> 00:03:08,080
this can increase the job success rate

76
00:03:08,080 --> 00:03:10,560
by retrying entity failures but it

77
00:03:10,560 --> 00:03:13,920
wastes resources by retrying dt failures

78
00:03:13,920 --> 00:03:15,680
the other approach commonly used in

79
00:03:15,680 --> 00:03:18,800
practice is simply terminating fail job

80
00:03:18,800 --> 00:03:21,280
this can avoid the worst retry on dt

81
00:03:21,280 --> 00:03:23,519
failures but it sacrificed the job

82
00:03:23,519 --> 00:03:25,760
success rate by not retrying ndt

83
00:03:25,760 --> 00:03:27,599
failures

84
00:03:27,599 --> 00:03:30,239
our work civila can differentiate dtn

85
00:03:30,239 --> 00:03:33,040
entity failure and with visibilia what

86
00:03:33,040 --> 00:03:35,440
we want to achieve is no retry on dt

87
00:03:35,440 --> 00:03:38,959
failure and retry on entity failure

88
00:03:38,959 --> 00:03:40,959
as the figure shows civilia has a

89
00:03:40,959 --> 00:03:43,599
classifier and it receives a low

90
00:03:43,599 --> 00:03:46,000
information of failed java low and

91
00:03:46,000 --> 00:03:47,360
determines the failure type

92
00:03:47,360 --> 00:03:50,159
deterministic or non-deterministic

93
00:03:50,159 --> 00:03:52,879
we set the two design goal as two parts

94
00:03:52,879 --> 00:03:53,760
first

95
00:03:53,760 --> 00:03:55,920
it must be accurate in distinguishing

96
00:03:55,920 --> 00:03:57,200
failure

97
00:03:57,200 --> 00:03:59,920
also second it must learn various law

98
00:03:59,920 --> 00:04:03,360
continuously without human intervention

99
00:04:03,360 --> 00:04:05,280
before showing how to do this let me

100
00:04:05,280 --> 00:04:07,360
first describe workload characteristic

101
00:04:07,360 --> 00:04:09,519
from microsoft philly trace to reveal

102
00:04:09,519 --> 00:04:11,280
the opportunity

103
00:04:11,280 --> 00:04:13,120
because the time constraint i don't

104
00:04:13,120 --> 00:04:15,920
explain trace itself in detail but from

105
00:04:15,920 --> 00:04:18,720
the trace you can show how many jobs

106
00:04:18,720 --> 00:04:20,880
experience dt failure and what is the

107
00:04:20,880 --> 00:04:23,520
portion of gpu hours used for retrying

108
00:04:23,520 --> 00:04:26,080
those dt failures

109
00:04:26,080 --> 00:04:27,680
they are showing in the left and right

110
00:04:27,680 --> 00:04:30,320
figure respectively x-axis shows the

111
00:04:30,320 --> 00:04:33,360
number of gpus retested and y-axis shows

112
00:04:33,360 --> 00:04:35,600
the portion of corresponding fail-jobs

113
00:04:35,600 --> 00:04:37,759
on request stage of size

114
00:04:37,759 --> 00:04:40,639
by this analysis five to 23 percent of

115
00:04:40,639 --> 00:04:43,759
jobs experience dt failure and second

116
00:04:43,759 --> 00:04:46,479
gpu hours spent for retrying those dt

117
00:04:46,479 --> 00:04:50,479
failures as much as 12 to 20 percent

118
00:04:50,479 --> 00:04:53,120
so we conclude classifying failures by

119
00:04:53,120 --> 00:04:56,080
similar into dtn entity failure can save

120
00:04:56,080 --> 00:04:59,040
significance of cpu hours not waste

121
00:04:59,040 --> 00:05:02,479
wasted only quantity failure

122
00:05:02,479 --> 00:05:04,639
given this opportunity for the rest of

123
00:05:04,639 --> 00:05:06,880
the talk i will cover the two design

124
00:05:06,880 --> 00:05:08,479
goals in more detail

125
00:05:08,479 --> 00:05:12,639
first how to build a accurate classifier

126
00:05:12,639 --> 00:05:14,800
while there are several possible ways to

127
00:05:14,800 --> 00:05:17,680
construct a classifier we employ a rm

128
00:05:17,680 --> 00:05:20,400
model based classifier as it is more

129
00:05:20,400 --> 00:05:23,520
amendable to determine dt or entity

130
00:05:23,520 --> 00:05:25,840
failures from textual job load

131
00:05:25,840 --> 00:05:28,000
information

132
00:05:28,000 --> 00:05:30,639
we use standard error stream of training

133
00:05:30,639 --> 00:05:33,759
jobs directly into stdl and std error

134
00:05:33,759 --> 00:05:35,039
load files

135
00:05:35,039 --> 00:05:37,520
as explained in many preceding

136
00:05:37,520 --> 00:05:38,720
researches

137
00:05:38,720 --> 00:05:41,360
every software stack uses these error

138
00:05:41,360 --> 00:05:44,080
rogues to record execution status

139
00:05:44,080 --> 00:05:46,560
and add a related information

140
00:05:46,560 --> 00:05:48,800
and the other training workload is no

141
00:05:48,800 --> 00:05:50,720
exception in this part

142
00:05:50,720 --> 00:05:52,960
so let's take a look at this example

143
00:05:52,960 --> 00:05:55,280
this logo data shows the call stack of

144
00:05:55,280 --> 00:05:56,720
prior

145
00:05:56,720 --> 00:05:59,360
to triggering an error in the blue color

146
00:05:59,360 --> 00:06:01,520
and the reason of the failure in red

147
00:06:01,520 --> 00:06:02,720
color

148
00:06:02,720 --> 00:06:04,639
one of the main challenges is that low

149
00:06:04,639 --> 00:06:06,880
format is unstructured and become

150
00:06:06,880 --> 00:06:08,560
diverse over time

151
00:06:08,560 --> 00:06:11,199
this makes a static way like grab on low

152
00:06:11,199 --> 00:06:13,520
data for specific keywords

153
00:06:13,520 --> 00:06:16,160
impractical in the long run situation

154
00:06:16,160 --> 00:06:18,479
and this is why we make automatic

155
00:06:18,479 --> 00:06:21,680
machine learning using approach

156
00:06:21,680 --> 00:06:23,120
since the low data is highly

157
00:06:23,120 --> 00:06:25,520
unstructured the failure classification

158
00:06:25,520 --> 00:06:26,960
need to polish the load with

159
00:06:26,960 --> 00:06:28,880
pre-processing stage

160
00:06:28,880 --> 00:06:30,960
this is how to transform a plain text

161
00:06:30,960 --> 00:06:32,639
log before training

162
00:06:32,639 --> 00:06:35,120
first the log sequence from each fail

163
00:06:35,120 --> 00:06:37,600
job lobe is passed into the structure

164
00:06:37,600 --> 00:06:39,759
format with evaluating the redundant

165
00:06:39,759 --> 00:06:41,039
messages

166
00:06:41,039 --> 00:06:43,280
after that the structural laws are

167
00:06:43,280 --> 00:06:45,120
transformed into semantic vectors to

168
00:06:45,120 --> 00:06:46,479
train the model

169
00:06:46,479 --> 00:06:48,400
these semantic factors are calculated

170
00:06:48,400 --> 00:06:50,720
not only by the word in the past look

171
00:06:50,720 --> 00:06:53,039
but also by its significance by

172
00:06:53,039 --> 00:06:55,199
weighting tom frequency inverse document

173
00:06:55,199 --> 00:06:57,840
frequency technique

174
00:06:57,840 --> 00:07:00,080
this pre-process load is used for

175
00:07:00,080 --> 00:07:02,720
training model in sevilla at first

176
00:07:02,720 --> 00:07:04,479
civilian needs initial training phase

177
00:07:04,479 --> 00:07:06,400
with collecting low data

178
00:07:06,400 --> 00:07:08,240
this data needs leveling by domain

179
00:07:08,240 --> 00:07:09,280
expert

180
00:07:09,280 --> 00:07:10,800
since civilad

181
00:07:10,800 --> 00:07:13,440
civil has auto leveling process we think

182
00:07:13,440 --> 00:07:15,919
idly this is the only phase that human

183
00:07:15,919 --> 00:07:18,000
intervention is needed

184
00:07:18,000 --> 00:07:21,360
we plug in two rna model statement gru

185
00:07:21,360 --> 00:07:23,680
that are specialized on processing the

186
00:07:23,680 --> 00:07:26,000
sequencer data we also made the

187
00:07:26,000 --> 00:07:28,960
simulation plug in more model so that it

188
00:07:28,960 --> 00:07:32,240
can feed on the change of dl models

189
00:07:32,240 --> 00:07:34,240
now are we going to explain the second

190
00:07:34,240 --> 00:07:35,599
part of civila

191
00:07:35,599 --> 00:07:37,520
how to update the classifier without

192
00:07:37,520 --> 00:07:39,360
human intervention with human

193
00:07:39,360 --> 00:07:42,080
intervention in updating model needs

194
00:07:42,080 --> 00:07:43,840
cumbersome process to build a new

195
00:07:43,840 --> 00:07:46,800
training data set with leveling process

196
00:07:46,800 --> 00:07:48,800
to address this issue we plug auto

197
00:07:48,800 --> 00:07:50,960
leveling mechanism by using classifier

198
00:07:50,960 --> 00:07:54,000
decision on leveling data

199
00:07:54,000 --> 00:07:56,000
after similar decide the fail job is

200
00:07:56,000 --> 00:07:58,400
whether dt or ndt the classified

201
00:07:58,400 --> 00:08:00,800
instance is fed into training data set

202
00:08:00,800 --> 00:08:02,800
with a level corresponding to civilized

203
00:08:02,800 --> 00:08:03,759
decision

204
00:08:03,759 --> 00:08:05,599
this auto leveling phase makes similar

205
00:08:05,599 --> 00:08:07,520
subsequent training can proceed in an

206
00:08:07,520 --> 00:08:09,840
online manner

207
00:08:09,840 --> 00:08:12,080
therefore the decision by civilized

208
00:08:12,080 --> 00:08:14,400
important because misprediction in one

209
00:08:14,400 --> 00:08:17,120
step leads cascading effect to civilized

210
00:08:17,120 --> 00:08:18,720
future performance

211
00:08:18,720 --> 00:08:20,720
to address this issue we adopt a

212
00:08:20,720 --> 00:08:22,240
strategy

213
00:08:22,240 --> 00:08:24,560
where the classified decision made by

214
00:08:24,560 --> 00:08:28,319
majority voting from multiple models

215
00:08:28,319 --> 00:08:30,560
let's look at the scenario on

216
00:08:30,560 --> 00:08:33,039
on top view here's the job placed onto

217
00:08:33,039 --> 00:08:35,120
the cluster and the jar fails the

218
00:08:35,120 --> 00:08:36,880
cluster measure corrects the failed log

219
00:08:36,880 --> 00:08:39,839
data and display load data is fed into

220
00:08:39,839 --> 00:08:42,159
similar and similar send the decision to

221
00:08:42,159 --> 00:08:44,000
the cluster manager

222
00:08:44,000 --> 00:08:45,839
based on the civil's decision the

223
00:08:45,839 --> 00:08:48,080
cluster manager decide whether kill or

224
00:08:48,080 --> 00:08:50,640
retry the failed job now i'm moving to

225
00:08:50,640 --> 00:08:52,880
the evaluation

226
00:08:52,880 --> 00:08:55,120
since there is no available data set for

227
00:08:55,120 --> 00:08:58,000
dia job failures we constructed data set

228
00:08:58,000 --> 00:09:00,959
for our study with 97 loads from data

229
00:09:00,959 --> 00:09:03,839
center operator and 159 loads from

230
00:09:03,839 --> 00:09:06,880
manual collected by stable flow we text

231
00:09:06,880 --> 00:09:08,880
augmented the data set for more diverse

232
00:09:08,880 --> 00:09:10,160
scenarios

233
00:09:10,160 --> 00:09:12,640
in the evaluation we go over eight

234
00:09:12,640 --> 00:09:14,399
training lines

235
00:09:14,399 --> 00:09:16,320
to show how well given classifier

236
00:09:16,320 --> 00:09:18,640
incremental update we start with twenty

237
00:09:18,640 --> 00:09:22,000
percent of label data set

238
00:09:22,000 --> 00:09:24,000
at round one representing the initial

239
00:09:24,000 --> 00:09:26,720
training phase in sibila afterwards we

240
00:09:26,720 --> 00:09:29,519
add 10 of each round auto level by

241
00:09:29,519 --> 00:09:31,760
classifier representing continuous

242
00:09:31,760 --> 00:09:34,080
updating without human intervention

243
00:09:34,080 --> 00:09:36,560
for comparison with civila we use a

244
00:09:36,560 --> 00:09:38,800
clustering method that is represent a

245
00:09:38,800 --> 00:09:40,720
known machine learning approach each of

246
00:09:40,720 --> 00:09:43,440
two rna models as a single model and

247
00:09:43,440 --> 00:09:46,240
lastly the oracle which always use

248
00:09:46,240 --> 00:09:49,279
ground truth level

249
00:09:49,519 --> 00:09:52,000
the first question for evaluation is can

250
00:09:52,000 --> 00:09:54,640
civilia classify failure type well

251
00:09:54,640 --> 00:09:56,399
this is the result of entity failure

252
00:09:56,399 --> 00:09:58,320
classification performance from round

253
00:09:58,320 --> 00:09:59,600
one to a

254
00:09:59,600 --> 00:10:02,000
precision is showing the precision that

255
00:10:02,000 --> 00:10:05,040
we call as a metric the performance of

256
00:10:05,040 --> 00:10:07,200
single rna model use case is better than

257
00:10:07,200 --> 00:10:09,680
clustering method especially in recall

258
00:10:09,680 --> 00:10:13,200
but both stm and gre model are

259
00:10:13,200 --> 00:10:15,519
suffering fluctuation as clustering did

260
00:10:15,519 --> 00:10:18,000
in precision part however civilian

261
00:10:18,000 --> 00:10:20,000
performance is more stable than other

262
00:10:20,000 --> 00:10:23,040
comparisons on both metrics

263
00:10:23,040 --> 00:10:25,120
all the last round stimuli achieves the

264
00:10:25,120 --> 00:10:27,040
highest performance compared with other

265
00:10:27,040 --> 00:10:30,000
approaches also it is almost the same as

266
00:10:30,000 --> 00:10:31,519
the oracle case

267
00:10:31,519 --> 00:10:33,440
dt failure classification case

268
00:10:33,440 --> 00:10:36,240
performance also shows same tendency so

269
00:10:36,240 --> 00:10:38,399
civilized classification performance is

270
00:10:38,399 --> 00:10:40,800
consistently high and comparable to the

271
00:10:40,800 --> 00:10:43,839
oracle which used ground truth level

272
00:10:43,839 --> 00:10:46,240
the next question is can sibila improve

273
00:10:46,240 --> 00:10:47,839
cluster efficiency

274
00:10:47,839 --> 00:10:50,160
we simulate it based on trace

275
00:10:50,160 --> 00:10:52,240
from microsoft philly and the simulator

276
00:10:52,240 --> 00:10:54,240
is from preceding research

277
00:10:54,240 --> 00:10:56,880
we simulated three policy smallest job

278
00:10:56,880 --> 00:10:59,600
first and two recently proposed policies

279
00:10:59,600 --> 00:11:01,279
grass and kittens

280
00:11:01,279 --> 00:11:04,320
the cluster simulation has 200 machines

281
00:11:04,320 --> 00:11:06,720
each have a gpus

282
00:11:06,720 --> 00:11:09,360
we have two competing policies oracle we

283
00:11:09,360 --> 00:11:12,399
have 100 accurate prediction and fully

284
00:11:12,399 --> 00:11:14,240
tried that we tried jobs with fixed

285
00:11:14,240 --> 00:11:17,360
number of times like microsoft feeling

286
00:11:17,360 --> 00:11:19,279
we calculated similar performance in

287
00:11:19,279 --> 00:11:21,519
risk prediction applied to randomly

288
00:11:21,519 --> 00:11:22,720
chosen job

289
00:11:22,720 --> 00:11:24,720
we measured average of completion time

290
00:11:24,720 --> 00:11:26,480
as a metric

291
00:11:26,480 --> 00:11:28,480
compared with the fully tried case

292
00:11:28,480 --> 00:11:31,519
predictive return improved 15.4 percent

293
00:11:31,519 --> 00:11:34,959
for sjf and 6.5 percent for dealers and

294
00:11:34,959 --> 00:11:36,000
kitties

295
00:11:36,000 --> 00:11:38,320
also compared with the oracle case the

296
00:11:38,320 --> 00:11:41,440
performance is only worth one percent

297
00:11:41,440 --> 00:11:44,079
the last question is can sibila maintain

298
00:11:44,079 --> 00:11:46,079
high job success rate with predictively

299
00:11:46,079 --> 00:11:47,120
trying

300
00:11:47,120 --> 00:11:49,600
when misreaction happens in ndt failure

301
00:11:49,600 --> 00:11:51,760
some entity failures are killed without

302
00:11:51,760 --> 00:11:54,959
retry this affect the job success rate

303
00:11:54,959 --> 00:11:57,440
since civila's prediction is not 100

304
00:11:57,440 --> 00:11:59,200
accurate i'm showing the effect of

305
00:11:59,200 --> 00:12:01,760
civilian owned job success rate here

306
00:12:01,760 --> 00:12:03,440
compared with the full retry which

307
00:12:03,440 --> 00:12:05,600
achieved the highest success rate

308
00:12:05,600 --> 00:12:08,160
civilized success rate is only 0.06

309
00:12:08,160 --> 00:12:10,639
percent lower so in conclusion

310
00:12:10,639 --> 00:12:13,040
sibila can provide a higher self-success

311
00:12:13,040 --> 00:12:17,519
rate while reducing resource waste

312
00:12:17,519 --> 00:12:20,399
so today i presented sevilla

313
00:12:20,399 --> 00:12:23,040
with civila we can only retry on failure

314
00:12:23,040 --> 00:12:25,519
when the failure is repeating civilized

315
00:12:25,519 --> 00:12:27,279
highly accurate in classifying such

316
00:12:27,279 --> 00:12:29,760
failure and through simulation we show

317
00:12:29,760 --> 00:12:31,519
that we can actually improve cluster

318
00:12:31,519 --> 00:12:34,000
efficiency this is the end of my talk

319
00:12:34,000 --> 00:12:35,600
thank you for listening and i'm happy to

320
00:12:35,600 --> 00:12:38,920
take a question

