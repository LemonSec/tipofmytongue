1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:12,840 --> 00:00:15,360
so deploy a database application on one

3
00:00:15,360 --> 00:00:16,980
of the major public clouds like Amazon

4
00:00:16,980 --> 00:00:19,440
ec2 you can provision compute instance

5
00:00:19,440 --> 00:00:21,119
which is typically a virtual machine

6
00:00:21,119 --> 00:00:23,640
that has a certain amount of CPU cores

7
00:00:23,640 --> 00:00:26,039
and memory so in this resource model the

8
00:00:26,039 --> 00:00:28,380
major deviation from the traditional

9
00:00:28,380 --> 00:00:31,080
model is the storage so instead of being

10
00:00:31,080 --> 00:00:33,239
physically attached to your compute

11
00:00:33,239 --> 00:00:35,160
instance storage devices are

12
00:00:35,160 --> 00:00:37,860
disaggregated they together form an

13
00:00:37,860 --> 00:00:40,079
independent system that provides storage

14
00:00:40,079 --> 00:00:42,180
services to your compute instance over

15
00:00:42,180 --> 00:00:44,460
the network and another important

16
00:00:44,460 --> 00:00:47,219
feature is that the application data

17
00:00:47,219 --> 00:00:49,760
stored in this desert creative storage

18
00:00:49,760 --> 00:00:53,520
disaggregated disk is also replicated

19
00:00:53,520 --> 00:00:56,640
uh to avoid any data loss so this gives

20
00:00:56,640 --> 00:00:58,980
us two important properties about

21
00:00:58,980 --> 00:01:02,460
deteriorated disks and first is that

22
00:01:02,460 --> 00:01:05,159
they are highly available because they

23
00:01:05,159 --> 00:01:07,020
are never attached so they can basically

24
00:01:07,020 --> 00:01:10,680
be accessed at any time by anyone and

25
00:01:10,680 --> 00:01:11,880
the second is that they're highly

26
00:01:11,880 --> 00:01:15,420
durable because due to replication so

27
00:01:15,420 --> 00:01:18,299
this begs the question can high

28
00:01:18,299 --> 00:01:20,640
availability at the disk level be easily

29
00:01:20,640 --> 00:01:23,040
translated to high availability at the

30
00:01:23,040 --> 00:01:25,280
application Level so in that Spirit

31
00:01:25,280 --> 00:01:27,299
practitioners have already started

32
00:01:27,299 --> 00:01:29,659
adopting a new fault tolerance technique

33
00:01:29,659 --> 00:01:33,360
that leverages disaggregated disks so

34
00:01:33,360 --> 00:01:36,540
when a when the primary instance of the

35
00:01:36,540 --> 00:01:39,180
application is being failed since we can

36
00:01:39,180 --> 00:01:42,420
still access the application disk we can

37
00:01:42,420 --> 00:01:45,140
basically spin up a new backup instance

38
00:01:45,140 --> 00:01:48,720
move over the application disk from the

39
00:01:48,720 --> 00:01:50,880
from the failed primary to the new

40
00:01:50,880 --> 00:01:54,360
backup to restart the application from

41
00:01:54,360 --> 00:01:57,060
the same application data and we call

42
00:01:57,060 --> 00:01:59,220
this recovery from this aggregated

43
00:01:59,220 --> 00:02:02,579
storage shown as red so basically when

44
00:02:02,579 --> 00:02:06,000
failure occurs a new response backup

45
00:02:06,000 --> 00:02:08,580
instance recovers application state from

46
00:02:08,580 --> 00:02:10,979
this aggregated storage and continues

47
00:02:10,979 --> 00:02:13,620
serving the application

48
00:02:13,620 --> 00:02:15,420
so compared to the traditional

49
00:02:15,420 --> 00:02:17,160
application Level replication that

50
00:02:17,160 --> 00:02:19,739
requires multiple replica instances of

51
00:02:19,739 --> 00:02:22,260
the application such as primary backup

52
00:02:22,260 --> 00:02:25,200
replication Reds only requires one

53
00:02:25,200 --> 00:02:27,840
running instance at all times bringing

54
00:02:27,840 --> 00:02:30,480
significant significantly significant

55
00:02:30,480 --> 00:02:33,660
savings on Capital costs Reds is also

56
00:02:33,660 --> 00:02:35,819
generally applicable to any crash

57
00:02:35,819 --> 00:02:37,860
consistent application so crash

58
00:02:37,860 --> 00:02:39,360
consistency is basically two

59
00:02:39,360 --> 00:02:42,540
requirements one is that before external

60
00:02:42,540 --> 00:02:44,760
externalizing any application State

61
00:02:44,760 --> 00:02:47,580
change updates related to those change

62
00:02:47,580 --> 00:02:50,519
must be persisted and the second is that

63
00:02:50,519 --> 00:02:52,379
the application is to be able to recover

64
00:02:52,379 --> 00:02:55,140
from a crash using techniques like red

65
00:02:55,140 --> 00:02:56,760
hair logging

66
00:02:56,760 --> 00:02:59,099
unfortunately Reds does not bridge the

67
00:02:59,099 --> 00:03:02,220
gap between disk High availability and

68
00:03:02,220 --> 00:03:05,040
application High availability and the

69
00:03:05,040 --> 00:03:07,140
first reason why res has low

70
00:03:07,140 --> 00:03:09,840
availability is that the failover must

71
00:03:09,840 --> 00:03:13,319
be sequential so red has two major

72
00:03:13,319 --> 00:03:16,800
recovery steps a failover Steps step one

73
00:03:16,800 --> 00:03:18,959
failure detection on the primary to

74
00:03:18,959 --> 00:03:20,519
determine whether the primary has truly

75
00:03:20,519 --> 00:03:23,819
failed by using a timeout and step two

76
00:03:23,819 --> 00:03:26,400
recovery on the backup which basically

77
00:03:26,400 --> 00:03:28,739
includes moving over the disk and

78
00:03:28,739 --> 00:03:31,140
restarting the application

79
00:03:31,140 --> 00:03:33,540
uh so Reds must proceed with the two

80
00:03:33,540 --> 00:03:35,879
steps sequentially meaning that step two

81
00:03:35,879 --> 00:03:39,060
cannot start until Step One is completed

82
00:03:39,060 --> 00:03:41,700
and this is because both the primary

83
00:03:41,700 --> 00:03:43,440
instance and the back instance require

84
00:03:43,440 --> 00:03:46,500
exclusive access to the same application

85
00:03:46,500 --> 00:03:47,700
disk

86
00:03:47,700 --> 00:03:50,519
and this basically gives Reds a hard

87
00:03:50,519 --> 00:03:53,400
decision to make and that is when should

88
00:03:53,400 --> 00:03:55,560
we consider step one to be completed or

89
00:03:55,560 --> 00:03:57,360
in other words how long should the timer

90
00:03:57,360 --> 00:04:00,420
be and such a decision is fundamentally

91
00:04:00,420 --> 00:04:03,840
hard one because the future is unknown

92
00:04:03,840 --> 00:04:06,060
so when the primary stops responding

93
00:04:06,060 --> 00:04:07,560
what is happening

94
00:04:07,560 --> 00:04:09,780
has it truly failed or this is just a

95
00:04:09,780 --> 00:04:11,879
never blip and the primary can come back

96
00:04:11,879 --> 00:04:14,400
shortly after and if that's the case how

97
00:04:14,400 --> 00:04:16,798
long should we keep waiting to see if

98
00:04:16,798 --> 00:04:18,600
the primary itself recovers

99
00:04:18,600 --> 00:04:21,600
and on the backup side uh if we decide

100
00:04:21,600 --> 00:04:23,520
to dual failover then the question is

101
00:04:23,520 --> 00:04:26,460
how long does the recovery on the backup

102
00:04:26,460 --> 00:04:28,740
take it could take just a couple seconds

103
00:04:28,740 --> 00:04:31,080
to even several minutes depending on the

104
00:04:31,080 --> 00:04:33,180
application State at the time of failure

105
00:04:33,180 --> 00:04:35,940
which may require fixing Corruptions and

106
00:04:35,940 --> 00:04:38,940
replaying transactions

107
00:04:38,940 --> 00:04:42,660
so if we use a short timeout we may

108
00:04:42,660 --> 00:04:45,180
commit to a long recovery when the

109
00:04:45,180 --> 00:04:46,800
failure situation on the primary is only

110
00:04:46,800 --> 00:04:49,139
transient meaning that simply waiting

111
00:04:49,139 --> 00:04:51,000
for the parameter to come back will be a

112
00:04:51,000 --> 00:04:52,440
lot faster

113
00:04:52,440 --> 00:04:54,180
if we use a long timeout then we

114
00:04:54,180 --> 00:04:55,979
effectively slow down the system's

115
00:04:55,979 --> 00:04:57,960
reaction to True failures

116
00:04:57,960 --> 00:05:01,080
so how long the failure detection timer

117
00:05:01,080 --> 00:05:03,120
should be is a fundamentally hard

118
00:05:03,120 --> 00:05:06,060
decision because no matter if it's a if

119
00:05:06,060 --> 00:05:08,820
it's long or short there is always an

120
00:05:08,820 --> 00:05:10,620
opportunity cost

121
00:05:10,620 --> 00:05:13,500
so now suppose we have an orc model that

122
00:05:13,500 --> 00:05:15,300
does know about the future what's the

123
00:05:15,300 --> 00:05:16,620
best we can do

124
00:05:16,620 --> 00:05:19,500
so when right when the primary instance

125
00:05:19,500 --> 00:05:21,660
stops responding the article knows that

126
00:05:21,660 --> 00:05:24,360
it would take say 30 seconds for the

127
00:05:24,360 --> 00:05:25,919
parameter to come back

128
00:05:25,919 --> 00:05:28,740
and meanwhile it will take only 20

129
00:05:28,740 --> 00:05:30,960
seconds for the recovery to finish on

130
00:05:30,960 --> 00:05:33,120
the backup if we initiate failover right

131
00:05:33,120 --> 00:05:35,759
away and clearly the optimal decision

132
00:05:35,759 --> 00:05:38,340
will be to Simply go with the faster one

133
00:05:38,340 --> 00:05:41,120
in this case the Oracle is simply

134
00:05:41,120 --> 00:05:44,220
immediately initiates recovery on the

135
00:05:44,220 --> 00:05:47,039
backup without waiting and if otherwise

136
00:05:47,039 --> 00:05:50,400
the recovery is longer then Oracle

137
00:05:50,400 --> 00:05:52,560
simply waits for the parameter to come

138
00:05:52,560 --> 00:05:55,320
back so this is saying that with the

139
00:05:55,320 --> 00:05:57,960
Oracle model the need for using a

140
00:05:57,960 --> 00:06:02,039
timeout is eliminated completely

141
00:06:02,039 --> 00:06:04,800
so in that Spirit we propose speculative

142
00:06:04,800 --> 00:06:07,680
recovery that makes a similarly optimal

143
00:06:07,680 --> 00:06:10,020
decision in practice without knowing the

144
00:06:10,020 --> 00:06:11,039
future

145
00:06:11,039 --> 00:06:13,860
so specular recovery pursues both paths

146
00:06:13,860 --> 00:06:16,979
in parallel meaning that the backup is

147
00:06:16,979 --> 00:06:19,320
doing recovery while we're still waiting

148
00:06:19,320 --> 00:06:21,419
for the uh for the parameter to come

149
00:06:21,419 --> 00:06:24,300
back and some specific recovery simply

150
00:06:24,300 --> 00:06:27,900
goes with whichever completes first

151
00:06:27,900 --> 00:06:31,440
speaker recovery achieved this uh to

152
00:06:31,440 --> 00:06:34,259
achieve this when the primates stopped

153
00:06:34,259 --> 00:06:36,240
responding Spirit recovery immediately

154
00:06:36,240 --> 00:06:38,699
creates an independent clone of the

155
00:06:38,699 --> 00:06:41,160
application disk and attaches it to the

156
00:06:41,160 --> 00:06:44,160
back instance to start recovery while

157
00:06:44,160 --> 00:06:46,380
the while the primary instance is not

158
00:06:46,380 --> 00:06:48,600
interfered with to allow your chance to

159
00:06:48,600 --> 00:06:52,319
come back in parallel so this basically

160
00:06:52,319 --> 00:06:54,660
creates a race between the primary

161
00:06:54,660 --> 00:06:56,819
instance a backer instance it is going

162
00:06:56,819 --> 00:06:59,100
to be either the primary becomes

163
00:06:59,100 --> 00:07:02,039
available again first or the back a

164
00:07:02,039 --> 00:07:04,440
complete recovery first and we believe

165
00:07:04,440 --> 00:07:07,440
that in theory the model of speculative

166
00:07:07,440 --> 00:07:10,139
recovery achieves the same results as

167
00:07:10,139 --> 00:07:12,180
the Oracle model by letting the two

168
00:07:12,180 --> 00:07:15,300
paths play out and observing which one

169
00:07:15,300 --> 00:07:17,280
is the faster one

170
00:07:17,280 --> 00:07:19,560
so for the rest of my talk I will give a

171
00:07:19,560 --> 00:07:20,940
more detailed introduction to spell

172
00:07:20,940 --> 00:07:23,819
recovery which is which is enabled by

173
00:07:23,819 --> 00:07:26,460
this novel concept of disk superposition

174
00:07:26,460 --> 00:07:28,740
and next I will talk about two New

175
00:07:28,740 --> 00:07:31,740
Primitives uh super and collapse that

176
00:07:31,740 --> 00:07:33,960
were introduced that can be used to

177
00:07:33,960 --> 00:07:36,660
implement a spectral recovery system and

178
00:07:36,660 --> 00:07:38,340
finally I represent some key evaluation

179
00:07:38,340 --> 00:07:41,400
results and conclude

180
00:07:41,400 --> 00:07:44,759
so by parallelizing the the primary

181
00:07:44,759 --> 00:07:46,740
instance and the back instance the

182
00:07:46,740 --> 00:07:49,800
application state diverges that is the

183
00:07:49,800 --> 00:07:52,139
application now has two independent

184
00:07:52,139 --> 00:07:55,800
versions at the same time so to ensure

185
00:07:55,800 --> 00:07:58,020
correctness specular recovery must

186
00:07:58,020 --> 00:08:00,840
ensure that only one of the version one

187
00:08:00,840 --> 00:08:03,419
of the versions can ever be observed by

188
00:08:03,419 --> 00:08:07,680
external clients and this is basically a

189
00:08:07,680 --> 00:08:11,340
superposition where the application disk

190
00:08:11,340 --> 00:08:14,520
is allowed to diverge to two versions at

191
00:08:14,520 --> 00:08:16,979
the same time only temporarily as long

192
00:08:16,979 --> 00:08:19,379
as neither is observed and when one of

193
00:08:19,379 --> 00:08:20,940
the version is observed the other

194
00:08:20,940 --> 00:08:24,120
version must be the must be destroyed as

195
00:08:24,120 --> 00:08:26,580
though it never existed

196
00:08:26,580 --> 00:08:28,979
so the concept of superposition is

197
00:08:28,979 --> 00:08:31,080
originally from quantum physics and a

198
00:08:31,080 --> 00:08:33,000
famous illustration is the Schrodinger's

199
00:08:33,000 --> 00:08:35,760
cat experiment so uh you know where the

200
00:08:35,760 --> 00:08:38,279
cat is both alive and dead at the same

201
00:08:38,279 --> 00:08:42,360
time so to draw an energy uh when the

202
00:08:42,360 --> 00:08:44,640
primary stop responding in the client's

203
00:08:44,640 --> 00:08:47,100
perspective it's just like putting the

204
00:08:47,100 --> 00:08:49,440
system inside the shouldering's box and

205
00:08:49,440 --> 00:08:51,720
inside this box very similar to a cat

206
00:08:51,720 --> 00:08:53,700
being both alive and dead at the same

207
00:08:53,700 --> 00:08:56,100
time in our scenario it's both the

208
00:08:56,100 --> 00:08:58,080
primary instance and the better instance

209
00:08:58,080 --> 00:09:00,420
being alive at the same time and when

210
00:09:00,420 --> 00:09:02,700
the box is opened it is going to be

211
00:09:02,700 --> 00:09:06,180
either sorry it's going to be either the

212
00:09:06,180 --> 00:09:07,800
the primary has been there the whole

213
00:09:07,800 --> 00:09:10,440
time there was no background at all

214
00:09:10,440 --> 00:09:13,740
or the back has already taken over right

215
00:09:13,740 --> 00:09:16,800
when the primary start responding

216
00:09:16,800 --> 00:09:19,500
so we say that a superposition is

217
00:09:19,500 --> 00:09:22,019
created when a column of the application

218
00:09:22,019 --> 00:09:23,399
disk is created

219
00:09:23,399 --> 00:09:24,720
and

220
00:09:24,720 --> 00:09:27,720
a superposition is collapsed when one of

221
00:09:27,720 --> 00:09:30,300
the dispersion is observed for creating

222
00:09:30,300 --> 00:09:32,160
superposition the biggest challenge is

223
00:09:32,160 --> 00:09:35,580
that the big challenge is how to make a

224
00:09:35,580 --> 00:09:37,620
disk clone fast to create as well as

225
00:09:37,620 --> 00:09:40,920
performant because bad performance on

226
00:09:40,920 --> 00:09:43,019
this clone May significantly increase

227
00:09:43,019 --> 00:09:45,240
the time to finish recovery on the

228
00:09:45,240 --> 00:09:48,540
backup which would neutralize any

229
00:09:48,540 --> 00:09:52,380
benefit from paralyzing to parent uh to

230
00:09:52,380 --> 00:09:54,959
the the two instances

231
00:09:54,959 --> 00:09:57,060
and copy and write is a common technique

232
00:09:57,060 --> 00:09:59,160
that can be fast to create a clone but

233
00:09:59,160 --> 00:10:01,440
it provides bad L performance after

234
00:10:01,440 --> 00:10:03,540
creation especially under highly

235
00:10:03,540 --> 00:10:06,060
parallel right workloads such as during

236
00:10:06,060 --> 00:10:08,519
recovery

237
00:10:08,519 --> 00:10:11,459
for uh collapsing a superposition the

238
00:10:11,459 --> 00:10:13,260
biggest challenge is to make sure that

239
00:10:13,260 --> 00:10:16,019
how is to make sure only one of the

240
00:10:16,019 --> 00:10:18,660
versions can ever be observed and to

241
00:10:18,660 --> 00:10:19,760
achieve this

242
00:10:19,760 --> 00:10:23,160
specular recovery monitors right to the

243
00:10:23,160 --> 00:10:25,800
primary version of the disk so basically

244
00:10:25,800 --> 00:10:29,040
when a write is applied on the primary

245
00:10:29,040 --> 00:10:31,940
disk after entering a superposition

246
00:10:31,940 --> 00:10:34,380
Spectrum recovery determines that the

247
00:10:34,380 --> 00:10:36,120
primary instance has been observed

248
00:10:36,120 --> 00:10:38,880
because that right may cause observable

249
00:10:38,880 --> 00:10:41,519
applications they change and in this

250
00:10:41,519 --> 00:10:44,459
case the superposition is collapsed by a

251
00:10:44,459 --> 00:10:46,980
boarding recovery on the backup to make

252
00:10:46,980 --> 00:10:48,839
sure that it cannot be observed

253
00:10:48,839 --> 00:10:53,160
if otherwise we see no rights until when

254
00:10:53,160 --> 00:10:54,540
the recovery on the back are finished

255
00:10:54,540 --> 00:10:57,480
completes then the applicant State on

256
00:10:57,480 --> 00:10:59,820
the primary has not been changed then

257
00:10:59,820 --> 00:11:01,920
the Apple the the superposition is

258
00:11:01,920 --> 00:11:05,760
collapsed by promoting the the backup to

259
00:11:05,760 --> 00:11:07,920
be to be in the new primary and

260
00:11:07,920 --> 00:11:09,720
deallocating the old primary to make

261
00:11:09,720 --> 00:11:12,420
sure they cannot be observed

262
00:11:12,420 --> 00:11:14,579
now let's talk about how to implement a

263
00:11:14,579 --> 00:11:17,459
speaker recovery system

264
00:11:17,459 --> 00:11:19,380
so we introduced a set of parameters

265
00:11:19,380 --> 00:11:21,540
super and collapse basically super

266
00:11:21,540 --> 00:11:24,060
creates a superposition by creating this

267
00:11:24,060 --> 00:11:27,540
clone collapse collapses a superposition

268
00:11:27,540 --> 00:11:30,540
by monitoring which disk version has

269
00:11:30,540 --> 00:11:33,000
been observed

270
00:11:33,000 --> 00:11:35,279
so super uses copy on right to create a

271
00:11:35,279 --> 00:11:40,019
clone it's fast to do so because the

272
00:11:40,019 --> 00:11:42,480
actual data copying is delayed but also

273
00:11:42,480 --> 00:11:45,360
such delaying hurts IO performance after

274
00:11:45,360 --> 00:11:47,640
creation and our experiments show that

275
00:11:47,640 --> 00:11:50,100
on some existing copy and write

276
00:11:50,100 --> 00:11:51,660
implementations the average right

277
00:11:51,660 --> 00:11:55,019
latency could be 30 times higher

278
00:11:55,019 --> 00:11:57,240
and to provide explanations we

279
00:11:57,240 --> 00:11:59,480
instrumented the open source

280
00:11:59,480 --> 00:12:02,339
distributed Services itself and discover

281
00:12:02,339 --> 00:12:06,060
two major reasons one is that concurrent

282
00:12:06,060 --> 00:12:08,399
copying operations are forced to be

283
00:12:08,399 --> 00:12:12,120
serialized due to bottlenecks at the

284
00:12:12,120 --> 00:12:14,160
allocation table and the second reason

285
00:12:14,160 --> 00:12:16,620
is that data copying often happen over

286
00:12:16,620 --> 00:12:19,260
the network instead of local uh because

287
00:12:19,260 --> 00:12:21,300
this is a distributed system and we

288
00:12:21,300 --> 00:12:23,760
proposed co-located Chrome to solve this

289
00:12:23,760 --> 00:12:25,980
problem and it brings our performance of

290
00:12:25,980 --> 00:12:28,740
this clone very close to a regular disk

291
00:12:28,740 --> 00:12:31,980
and please start paper for more details

292
00:12:31,980 --> 00:12:35,399
and okay now collapse so collapse uses

293
00:12:35,399 --> 00:12:38,700
this uses a dirty bit to monitor uh

294
00:12:38,700 --> 00:12:41,820
rights to to the primary disk so the

295
00:12:41,820 --> 00:12:43,440
Dirty Bit is basically a piece of

296
00:12:43,440 --> 00:12:46,079
metadata for the primary disk

297
00:12:46,079 --> 00:12:48,240
um uh so basically when entering a

298
00:12:48,240 --> 00:12:51,060
superposition the Dirty Bit is reset and

299
00:12:51,060 --> 00:12:52,980
anywhere following that point will set

300
00:12:52,980 --> 00:12:56,100
the Dirty Bit so a core collapse a call

301
00:12:56,100 --> 00:12:58,920
to collapse checks the little bit and

302
00:12:58,920 --> 00:13:00,899
then you know decides what to do with

303
00:13:00,899 --> 00:13:02,220
the uh

304
00:13:02,220 --> 00:13:04,620
uh with a failover so basically if it's

305
00:13:04,620 --> 00:13:07,260
dirty then we must board recovery and

306
00:13:07,260 --> 00:13:09,000
continue with the primary and if it's

307
00:13:09,000 --> 00:13:10,620
clean then we can proceed with promotion

308
00:13:10,620 --> 00:13:13,139
and continue with the backup

309
00:13:13,139 --> 00:13:14,880
and also please see our player for more

310
00:13:14,880 --> 00:13:16,500
details

311
00:13:16,500 --> 00:13:18,959
so this is the overview of the of the

312
00:13:18,959 --> 00:13:20,639
system so there are three components

313
00:13:20,639 --> 00:13:23,459
concerning a speaker recovery system the

314
00:13:23,459 --> 00:13:25,579
instance pool the disaggregated storage

315
00:13:25,579 --> 00:13:29,279
laser storage cluster and the failure

316
00:13:29,279 --> 00:13:32,839
monitor so when the primary instance

317
00:13:32,839 --> 00:13:35,040
stops responding

318
00:13:35,040 --> 00:13:37,440
the failure monitor initiates speaker

319
00:13:37,440 --> 00:13:40,019
recovery by spinning up a new backup

320
00:13:40,019 --> 00:13:40,920
instance

321
00:13:40,920 --> 00:13:43,079
calling super to the storage cluster

322
00:13:43,079 --> 00:13:45,959
which causes abduction disk to enter a

323
00:13:45,959 --> 00:13:49,139
superposition and the solar cluster also

324
00:13:49,139 --> 00:13:51,360
starts tracking rights to the primary

325
00:13:51,360 --> 00:13:53,100
version of the disk

326
00:13:53,100 --> 00:13:54,839
and when the recovery on the back

327
00:13:54,839 --> 00:13:57,180
instance completes the failure monitor

328
00:13:57,180 --> 00:13:58,880
calls collapse

329
00:13:58,880 --> 00:14:01,440
to determine which instance should

330
00:14:01,440 --> 00:14:03,660
continue serving the application based

331
00:14:03,660 --> 00:14:06,360
on the value of the Dirty Bit

332
00:14:06,360 --> 00:14:08,279
and finally I will present some

333
00:14:08,279 --> 00:14:12,060
evaluation a key evaluation results uh

334
00:14:12,060 --> 00:14:13,860
to show the Practical availability

335
00:14:13,860 --> 00:14:16,620
improvements we implement the Prototype

336
00:14:16,620 --> 00:14:18,779
special recovery system called spec Reds

337
00:14:18,779 --> 00:14:21,839
based on steps block interface called

338
00:14:21,839 --> 00:14:24,600
RPD we used Docker containers for the

339
00:14:24,600 --> 00:14:27,000
instance pool and nvme ssds for the

340
00:14:27,000 --> 00:14:28,380
storage cluster

341
00:14:28,380 --> 00:14:30,360
so first to show the point that

342
00:14:30,360 --> 00:14:32,760
co-located Chrome achieves close to

343
00:14:32,760 --> 00:14:35,160
normal IO performance we compare three

344
00:14:35,160 --> 00:14:39,360
types of disks a regular RBD disk a

345
00:14:39,360 --> 00:14:41,760
clone desk receives existing clone

346
00:14:41,760 --> 00:14:44,220
implementation and a clone disk with

347
00:14:44,220 --> 00:14:46,500
collocated column for super

348
00:14:46,500 --> 00:14:48,959
and to show the end-to-end availability

349
00:14:48,959 --> 00:14:51,899
Improvement We compare three systems the

350
00:14:51,899 --> 00:14:53,880
Baseline system Reds which is using a

351
00:14:53,880 --> 00:14:56,820
regular disk our system which is using a

352
00:14:56,820 --> 00:14:59,040
disk clone with super and the Oracle

353
00:14:59,040 --> 00:15:00,899
model which is using a regular disk as

354
00:15:00,899 --> 00:15:03,360
well we compared three database

355
00:15:03,360 --> 00:15:07,199
applications my SQL postgres and mariadb

356
00:15:07,199 --> 00:15:09,000
so we start by looking at the

357
00:15:09,000 --> 00:15:10,620
application recovery latency from

358
00:15:10,620 --> 00:15:12,540
various disk States

359
00:15:12,540 --> 00:15:15,120
uh here are the three uh these types

360
00:15:15,120 --> 00:15:18,120
that we mentioned on the x-axis is the

361
00:15:18,120 --> 00:15:21,240
disk State represented by two parameters

362
00:15:21,240 --> 00:15:23,940
so the letter on the left is the failure

363
00:15:23,940 --> 00:15:26,760
type so we emulated two types of

364
00:15:26,760 --> 00:15:29,459
failures a clean fluid failure that

365
00:15:29,459 --> 00:15:31,800
allows for some cleanup by synchronously

366
00:15:31,800 --> 00:15:34,740
stopping the docker container and a

367
00:15:34,740 --> 00:15:37,740
unclean failure that injects a kernel

368
00:15:37,740 --> 00:15:40,320
panic without you know any chance for

369
00:15:40,320 --> 00:15:42,959
for cleanup and the number on the right

370
00:15:42,959 --> 00:15:44,940
is the size of the database right ahead

371
00:15:44,940 --> 00:15:46,260
log

372
00:15:46,260 --> 00:15:49,139
and on the y-axis is the recovery

373
00:15:49,139 --> 00:15:52,680
latency so here we have the Baseline uh

374
00:15:52,680 --> 00:15:55,199
the a regular RBD disk we can see that

375
00:15:55,199 --> 00:15:58,139
an unclean failure indeed takes longer

376
00:15:58,139 --> 00:16:00,720
to recovery and large a larger

377
00:16:00,720 --> 00:16:03,240
right-hand log also requires more

378
00:16:03,240 --> 00:16:05,100
recovery work

379
00:16:05,100 --> 00:16:07,440
and when we're doing recovery on a cold

380
00:16:07,440 --> 00:16:11,160
uh sorry on the Clone disk with self's

381
00:16:11,160 --> 00:16:13,800
existing implementation you can see that

382
00:16:13,800 --> 00:16:16,860
the recovery latency is significantly

383
00:16:16,860 --> 00:16:19,620
higher this is basically when when you

384
00:16:19,620 --> 00:16:22,500
have the right latency be 30 times

385
00:16:22,500 --> 00:16:26,639
higher the application recovery suffers

386
00:16:26,639 --> 00:16:31,079
and finally super is very close to the

387
00:16:31,079 --> 00:16:33,420
regular disk while significantly

388
00:16:33,420 --> 00:16:36,480
outperforming uh saves existing

389
00:16:36,480 --> 00:16:39,060
implementation

390
00:16:39,060 --> 00:16:42,540
and finally how does the uh recovery

391
00:16:42,540 --> 00:16:45,120
latency translate to end to end

392
00:16:45,120 --> 00:16:48,000
availability Improvement and to show

393
00:16:48,000 --> 00:16:51,779
this we divide the uh the failure

394
00:16:51,779 --> 00:16:54,420
failover scenarios into four broad

395
00:16:54,420 --> 00:16:56,820
categories based on the length of time

396
00:16:56,820 --> 00:17:00,120
and recovery so we pick one minute and

397
00:17:00,120 --> 00:17:02,759
five seconds for a long time and short

398
00:17:02,759 --> 00:17:04,799
time respectively and just to clarify

399
00:17:04,799 --> 00:17:06,900
this timeout is not only for Reds

400
00:17:06,900 --> 00:17:09,359
because Spirit Reds and Oracle they do

401
00:17:09,359 --> 00:17:10,799
not need to use a timer

402
00:17:10,799 --> 00:17:13,319
and on the x-axis is the recovery length

403
00:17:13,319 --> 00:17:15,299
we also use one minute for long recovery

404
00:17:15,299 --> 00:17:17,280
and five seconds for shortcut recovery

405
00:17:17,280 --> 00:17:20,220
and on y-axis is the failure latency

406
00:17:20,220 --> 00:17:24,140
which is a simple simulation by adding

407
00:17:24,140 --> 00:17:27,240
timer and Recovery together

408
00:17:27,240 --> 00:17:30,000
so first for long time up we can see

409
00:17:30,000 --> 00:17:30,860
that

410
00:17:30,860 --> 00:17:34,919
sweat Reds is reduces the failure

411
00:17:34,919 --> 00:17:37,620
failure relations significantly because

412
00:17:37,620 --> 00:17:40,679
it does not need to wait for a timeout

413
00:17:40,679 --> 00:17:42,900
and for a short timeout we can see that

414
00:17:42,900 --> 00:17:46,080
Reds and spec Reds have very similar

415
00:17:46,080 --> 00:17:48,600
latencies because a short timer allows

416
00:17:48,600 --> 00:17:51,900
Reds to initiate failover much sooner

417
00:17:51,900 --> 00:17:55,020
and in fact in some cases sweat Reds is

418
00:17:55,020 --> 00:17:57,299
even slower because it operates on a

419
00:17:57,299 --> 00:17:59,700
slightly slower disk clone

420
00:17:59,700 --> 00:18:02,940
but as we discussed uh the problem with

421
00:18:02,940 --> 00:18:05,539
using a short timeout is that it risks

422
00:18:05,539 --> 00:18:08,400
uh committing to a long recovery

423
00:18:08,400 --> 00:18:10,620
unnecessarily so to show this we

424
00:18:10,620 --> 00:18:13,559
simulated a false positive failure where

425
00:18:13,559 --> 00:18:16,020
the primary comes back after 10 seconds

426
00:18:16,020 --> 00:18:19,460
of being unavailable and in this case

427
00:18:19,460 --> 00:18:22,380
Reds using a five second timeout would

428
00:18:22,380 --> 00:18:25,200
trigger a long a one minute long

429
00:18:25,200 --> 00:18:26,400
recovery

430
00:18:26,400 --> 00:18:29,700
while Spirits can abort recovery when

431
00:18:29,700 --> 00:18:32,640
the primary comes back after 10 seconds

432
00:18:32,640 --> 00:18:34,980
so experience avoids unnecessary

433
00:18:34,980 --> 00:18:36,720
failovers

434
00:18:36,720 --> 00:18:40,440
so to summarize a

435
00:18:40,440 --> 00:18:45,539
celebrate a saves a lot on Long timeouts

436
00:18:45,539 --> 00:18:47,940
while it also avoids unnecessary

437
00:18:47,940 --> 00:18:51,120
failovers compared to Reds using a short

438
00:18:51,120 --> 00:18:53,780
timeout

439
00:18:54,780 --> 00:18:57,900
so to conclude my talk with one sentence

440
00:18:57,900 --> 00:19:00,299
speaker recovery safely and efficiently

441
00:19:00,299 --> 00:19:02,700
parallelizes itself here on the primary

442
00:19:02,700 --> 00:19:04,980
and recovery on the backup to push

443
00:19:04,980 --> 00:19:07,980
failover latency to the lower bound of

444
00:19:07,980 --> 00:19:11,400
Reds uh thank you

445
00:19:11,400 --> 00:19:14,299
that's it thank you

