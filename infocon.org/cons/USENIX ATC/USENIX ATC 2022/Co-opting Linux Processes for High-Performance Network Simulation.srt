1
00:00:14,160 --> 00:00:15,839
i'm rob jansen from the u.s naval

2
00:00:15,839 --> 00:00:17,600
research laboratory i'm going to present

3
00:00:17,600 --> 00:00:20,160
our paper co-opting linux processes for

4
00:00:20,160 --> 00:00:22,240
high performance network simulation

5
00:00:22,240 --> 00:00:24,160
this paper was one of the two that won

6
00:00:24,160 --> 00:00:26,400
use next best paper award this year

7
00:00:26,400 --> 00:00:27,519
i'd like to give a shout out to my

8
00:00:27,519 --> 00:00:30,240
co-authors jim newsom from tor and ryan

9
00:00:30,240 --> 00:00:32,320
wales from georgetown without them this

10
00:00:32,320 --> 00:00:33,840
paper wouldn't exist and none of this

11
00:00:33,840 --> 00:00:35,120
would be possible

12
00:00:35,120 --> 00:00:37,280
so thank them very much

13
00:00:37,280 --> 00:00:39,440
the main takeaways we designed a new

14
00:00:39,440 --> 00:00:42,079
hybrid network simulator emulator

15
00:00:42,079 --> 00:00:44,960
that directly executes applications

16
00:00:44,960 --> 00:00:46,399
and then co-ops them into a network

17
00:00:46,399 --> 00:00:48,239
simulation environment

18
00:00:48,239 --> 00:00:50,239
this enables large-scale distributed

19
00:00:50,239 --> 00:00:52,239
system experiments

20
00:00:52,239 --> 00:00:54,800
our work is merged into the open source

21
00:00:54,800 --> 00:00:57,760
shadow project and is synonymous with

22
00:00:57,760 --> 00:00:59,359
shadow version two

23
00:00:59,359 --> 00:01:02,000
our code name is phantom in this paper

24
00:01:02,000 --> 00:01:04,400
our artifacts are available online

25
00:01:04,400 --> 00:01:07,200
a brief summary of the results we found

26
00:01:07,200 --> 00:01:09,840
that we're 2.3 times faster than shadow

27
00:01:09,840 --> 00:01:11,360
version one

28
00:01:11,360 --> 00:01:14,880
three 3.4 times faster than ns3 and 43

29
00:01:14,880 --> 00:01:17,360
times faster than grail which is a

30
00:01:17,360 --> 00:01:19,840
similar approach

31
00:01:19,840 --> 00:01:21,840
based on ns3

32
00:01:21,840 --> 00:01:24,240
and i'll get into why and how we do that

33
00:01:24,240 --> 00:01:26,159
in the following slides i'll start with

34
00:01:26,159 --> 00:01:27,600
motivation and then go through the

35
00:01:27,600 --> 00:01:28,560
design

36
00:01:28,560 --> 00:01:31,520
and end with evaluation

37
00:01:31,520 --> 00:01:33,600
so the requirements for distributed

38
00:01:33,600 --> 00:01:35,840
system experimentation are

39
00:01:35,840 --> 00:01:36,880
um

40
00:01:36,880 --> 00:01:39,600
as follows first for any test network we

41
00:01:39,600 --> 00:01:41,200
want it to be controllable so we can

42
00:01:41,200 --> 00:01:43,360
isolate important factors in our

43
00:01:43,360 --> 00:01:44,479
experiments

44
00:01:44,479 --> 00:01:46,560
and we want them to be replicable so we

45
00:01:46,560 --> 00:01:48,960
can identically reproduce the results

46
00:01:48,960 --> 00:01:51,360
that we are achieving

47
00:01:51,360 --> 00:01:52,960
additionally for large scale distributed

48
00:01:52,960 --> 00:01:55,280
systems we have additional requirements

49
00:01:55,280 --> 00:01:56,640
for accuracy

50
00:01:56,640 --> 00:01:59,360
large scale systems distributed systems

51
00:01:59,360 --> 00:02:00,560
in particular

52
00:02:00,560 --> 00:02:02,320
are characterized by a complex

53
00:02:02,320 --> 00:02:03,920
application logic

54
00:02:03,920 --> 00:02:06,240
and so in order to accurately reproduce

55
00:02:06,240 --> 00:02:07,920
that we want to directly execute the

56
00:02:07,920 --> 00:02:09,919
applications rather than in a

57
00:02:09,919 --> 00:02:11,280
distraction

58
00:02:11,280 --> 00:02:13,280
and because of their scale we need our

59
00:02:13,280 --> 00:02:14,959
approach to be scalable so we can

60
00:02:14,959 --> 00:02:16,000
decouple

61
00:02:16,000 --> 00:02:17,599
from real time and from the

62
00:02:17,599 --> 00:02:19,280
computational constraints of the host

63
00:02:19,280 --> 00:02:23,120
machine so that's our goal

64
00:02:23,120 --> 00:02:24,400
there are problems with traditional

65
00:02:24,400 --> 00:02:25,840
approaches

66
00:02:25,840 --> 00:02:27,920
where they don't achieve these goals for

67
00:02:27,920 --> 00:02:29,200
simulation

68
00:02:29,200 --> 00:02:32,000
an example of this approach is ns3

69
00:02:32,000 --> 00:02:34,400
it's less realistic because it runs

70
00:02:34,400 --> 00:02:36,480
abstractions of applications rather than

71
00:02:36,480 --> 00:02:38,959
the real applications themselves

72
00:02:38,959 --> 00:02:41,200
and as the applications are continuously

73
00:02:41,200 --> 00:02:43,120
developed over time this

74
00:02:43,120 --> 00:02:45,280
the abstractions can become outdated and

75
00:02:45,280 --> 00:02:47,280
so it's hard to maintain those and can

76
00:02:47,280 --> 00:02:49,360
lead to invalid results

77
00:02:49,360 --> 00:02:52,400
another approach is emulation which is

78
00:02:52,400 --> 00:02:54,400
the approach of mininet

79
00:02:54,400 --> 00:02:56,560
emulation does run applications so it's

80
00:02:56,560 --> 00:02:58,480
more realistic in that sense but it's

81
00:02:58,480 --> 00:03:00,800
not as controllable

82
00:03:00,800 --> 00:03:02,800
it's running on a linux kernel and so

83
00:03:02,800 --> 00:03:04,879
the results won't be identical

84
00:03:04,879 --> 00:03:07,280
because of the entropy on the machine

85
00:03:07,280 --> 00:03:08,720
it's also

86
00:03:08,720 --> 00:03:10,239
not scalable

87
00:03:10,239 --> 00:03:12,319
cpu overload on the host machine can

88
00:03:12,319 --> 00:03:13,920
result in time distortion that can

89
00:03:13,920 --> 00:03:17,040
introduce artifacts in the results

90
00:03:17,040 --> 00:03:18,720
to verify this claim we did an

91
00:03:18,720 --> 00:03:23,200
experiment where we ran a small um

92
00:03:23,200 --> 00:03:25,680
peer-to-peer messaging workload in

93
00:03:25,680 --> 00:03:28,319
phantom our approach and in mininet this

94
00:03:28,319 --> 00:03:30,799
graph shows on the x-axis as the number

95
00:03:30,799 --> 00:03:34,959
of hosts increase increases

96
00:03:34,959 --> 00:03:36,959
after about 20 hosts

97
00:03:36,959 --> 00:03:39,360
in mininet the cpu overload caused the

98
00:03:39,360 --> 00:03:41,599
packet forwarding rate to be lower than

99
00:03:41,599 --> 00:03:43,360
expected

100
00:03:43,360 --> 00:03:46,480
and so this this demonstrates the cpu

101
00:03:46,480 --> 00:03:48,720
over subscription that's happening

102
00:03:48,720 --> 00:03:51,680
during the emulation

103
00:03:51,840 --> 00:03:52,560
so

104
00:03:52,560 --> 00:03:54,400
simulation and emulation

105
00:03:54,400 --> 00:03:56,239
don't work well so there's been hybrid

106
00:03:56,239 --> 00:03:58,560
architectures that have been studied

107
00:03:58,560 --> 00:04:02,319
in research this is a network simulation

108
00:04:02,319 --> 00:04:03,280
but

109
00:04:03,280 --> 00:04:05,599
where the application code is directly

110
00:04:05,599 --> 00:04:06,799
executed

111
00:04:06,799 --> 00:04:08,799
and the idea is to get the advantages of

112
00:04:08,799 --> 00:04:11,360
both simulation and emulation

113
00:04:11,360 --> 00:04:13,360
and this provides the best opportunity

114
00:04:13,360 --> 00:04:15,599
for large-scale distributed systems

115
00:04:15,599 --> 00:04:17,918
so there's been two different approaches

116
00:04:17,918 --> 00:04:19,918
with hybrid architectures

117
00:04:19,918 --> 00:04:22,960
one is where the application code is run

118
00:04:22,960 --> 00:04:25,120
via plug-in namespaces

119
00:04:25,120 --> 00:04:27,360
so you literally just load the plug-in

120
00:04:27,360 --> 00:04:29,040
search for the main function inside of

121
00:04:29,040 --> 00:04:31,120
the code and then call the function

122
00:04:31,120 --> 00:04:34,240
this is the approach used by ns3

123
00:04:34,240 --> 00:04:36,800
dce mode which is a research project

124
00:04:36,800 --> 00:04:38,960
that exists out there and in shadow

125
00:04:38,960 --> 00:04:41,040
version one the limitations of this

126
00:04:41,040 --> 00:04:43,199
approach are compatibility you must

127
00:04:43,199 --> 00:04:45,280
build your program your application as a

128
00:04:45,280 --> 00:04:46,800
plugin

129
00:04:46,800 --> 00:04:49,680
correctness these approaches intercept

130
00:04:49,680 --> 00:04:52,000
library calls only and so system calls

131
00:04:52,000 --> 00:04:52,880
made

132
00:04:52,880 --> 00:04:54,880
directly to the kernel will leak outside

133
00:04:54,880 --> 00:04:57,840
of the simulation and cause errors

134
00:04:57,840 --> 00:04:59,360
and maintainability this approach

135
00:04:59,360 --> 00:05:02,320
requires a custom loader and custom

136
00:05:02,320 --> 00:05:03,919
threading libraries which can be hard to

137
00:05:03,919 --> 00:05:05,840
maintain

138
00:05:05,840 --> 00:05:08,240
so this limits the set of applications

139
00:05:08,240 --> 00:05:11,520
that can run in this type of approach

140
00:05:11,520 --> 00:05:13,440
to those that satisfy all of these

141
00:05:13,440 --> 00:05:14,560
properties

142
00:05:14,560 --> 00:05:16,320
and our approach

143
00:05:16,320 --> 00:05:18,560
is much more modular and supports a

144
00:05:18,560 --> 00:05:20,240
larger set of applications and you can

145
00:05:20,240 --> 00:05:23,680
see the paper for more details on this

146
00:05:23,680 --> 00:05:25,919
the other hybrid architecture is one

147
00:05:25,919 --> 00:05:27,840
where the code is run as a standard

148
00:05:27,840 --> 00:05:29,840
linux process using something like fork

149
00:05:29,840 --> 00:05:30,800
and exec

150
00:05:30,800 --> 00:05:32,880
and then ptrace which is a linux

151
00:05:32,880 --> 00:05:35,360
facility to control the process this is

152
00:05:35,360 --> 00:05:37,199
the approach used by grail which was

153
00:05:37,199 --> 00:05:38,960
published in transactions on networking

154
00:05:38,960 --> 00:05:40,639
2019.

155
00:05:40,639 --> 00:05:42,639
the problem with this approach is that

156
00:05:42,639 --> 00:05:44,880
it's slow p trace is slow

157
00:05:44,880 --> 00:05:47,120
there it has overhead that is quadratic

158
00:05:47,120 --> 00:05:48,720
in the total number of processes that

159
00:05:48,720 --> 00:05:50,560
are loaded

160
00:05:50,560 --> 00:05:52,800
and to intercept a syscall requires four

161
00:05:52,800 --> 00:05:54,479
context switches for every single

162
00:05:54,479 --> 00:05:56,320
syscall that's made

163
00:05:56,320 --> 00:05:58,240
transferring data between the processes

164
00:05:58,240 --> 00:06:00,720
and the simulator requires a syscall and

165
00:06:00,720 --> 00:06:02,240
a mode transition for every word of

166
00:06:02,240 --> 00:06:03,360
memory

167
00:06:03,360 --> 00:06:05,360
so this overhead we measure this

168
00:06:05,360 --> 00:06:08,160
overhead on in grail which is built on

169
00:06:08,160 --> 00:06:12,000
ns3 to be 13 times that of ns3 alone

170
00:06:12,000 --> 00:06:14,479
which demonstrates the

171
00:06:14,479 --> 00:06:16,800
overhead of the approach

172
00:06:16,800 --> 00:06:18,560
so our research challenge is can we

173
00:06:18,560 --> 00:06:20,000
design a tool with the performance

174
00:06:20,000 --> 00:06:22,479
benefits of the uniprocess plug-in based

175
00:06:22,479 --> 00:06:24,160
architecture

176
00:06:24,160 --> 00:06:25,759
and the improved modularity and

177
00:06:25,759 --> 00:06:28,479
isolation that you get from running real

178
00:06:28,479 --> 00:06:30,880
processes on linux in the multi-process

179
00:06:30,880 --> 00:06:32,479
architecture

180
00:06:32,479 --> 00:06:33,600
and so that is what we set out to

181
00:06:33,600 --> 00:06:34,560
achieve

182
00:06:34,560 --> 00:06:36,960
in this research

183
00:06:36,960 --> 00:06:38,880
next i'll talk about the design and how

184
00:06:38,880 --> 00:06:40,960
we got there and then end with some

185
00:06:40,960 --> 00:06:42,880
evaluation

186
00:06:42,880 --> 00:06:44,319
throughout the design discussion i'm

187
00:06:44,319 --> 00:06:46,400
going to sprinkle in some results to

188
00:06:46,400 --> 00:06:48,400
help motivate why we made certain design

189
00:06:48,400 --> 00:06:51,600
choices so be prepared for that

190
00:06:51,600 --> 00:06:53,680
and in the upper right i'm going to show

191
00:06:53,680 --> 00:06:55,280
sort of a schedule for the design

192
00:06:55,280 --> 00:06:58,000
components i'm going to discuss

193
00:06:58,000 --> 00:07:00,319
i'll start with an overview so we

194
00:07:00,319 --> 00:07:02,639
created a discrete event packet level

195
00:07:02,639 --> 00:07:04,160
network simulator

196
00:07:04,160 --> 00:07:06,639
again it directly executes applications

197
00:07:06,639 --> 00:07:07,440
as

198
00:07:07,440 --> 00:07:09,680
standard linux processes and then

199
00:07:09,680 --> 00:07:11,440
intercepts all the system calls that

200
00:07:11,440 --> 00:07:12,639
they make

201
00:07:12,639 --> 00:07:14,319
and then emulates them inside of the

202
00:07:14,319 --> 00:07:16,560
simulator process

203
00:07:16,560 --> 00:07:18,720
so the simulation

204
00:07:18,720 --> 00:07:20,720
we're simulating system call behavior

205
00:07:20,720 --> 00:07:22,000
and networking

206
00:07:22,000 --> 00:07:23,440
with functionality such as file

207
00:07:23,440 --> 00:07:25,680
descriptors event notification

208
00:07:25,680 --> 00:07:28,319
networking dns and routing is all

209
00:07:28,319 --> 00:07:30,400
simulated inside of the discrete event

210
00:07:30,400 --> 00:07:32,479
simulator which runs

211
00:07:32,479 --> 00:07:34,560
in application layer without requiring

212
00:07:34,560 --> 00:07:37,520
any kernel changes

213
00:07:37,520 --> 00:07:39,680
so the first design

214
00:07:39,680 --> 00:07:42,319
decision here is how do we parallelize

215
00:07:42,319 --> 00:07:44,880
simulation workload we have a simulation

216
00:07:44,880 --> 00:07:46,880
controller process that's running on a

217
00:07:46,880 --> 00:07:51,280
machine with several physical processors

218
00:07:51,280 --> 00:07:54,479
and the simulation controller process

219
00:07:54,479 --> 00:07:57,120
runs multiple virtual hosts

220
00:07:57,120 --> 00:07:59,039
each of them which runs

221
00:07:59,039 --> 00:08:02,080
some number of processes this represents

222
00:08:02,080 --> 00:08:03,759
the workload that the simulator wants to

223
00:08:03,759 --> 00:08:05,919
parallelize

224
00:08:05,919 --> 00:08:08,080
so the question is how many worker

225
00:08:08,080 --> 00:08:10,000
threads should we run inside of the

226
00:08:10,000 --> 00:08:12,400
simulator controller process to

227
00:08:12,400 --> 00:08:16,080
effectively parallelize this workload

228
00:08:16,080 --> 00:08:18,319
and we tried some different approaches

229
00:08:18,319 --> 00:08:21,599
one where we run one thread per virtual

230
00:08:21,599 --> 00:08:23,440
host so that would be seven in this

231
00:08:23,440 --> 00:08:24,560
example

232
00:08:24,560 --> 00:08:27,360
or one thread per cpu which would be

233
00:08:27,360 --> 00:08:29,360
four in this example

234
00:08:29,360 --> 00:08:32,399
and this these two graphs show

235
00:08:32,399 --> 00:08:33,519
that

236
00:08:33,519 --> 00:08:36,159
the one per host approach

237
00:08:36,159 --> 00:08:38,320
on the left

238
00:08:38,320 --> 00:08:39,599
was faster

239
00:08:39,599 --> 00:08:42,080
than the other approach and on the right

240
00:08:42,080 --> 00:08:43,919
also that it used more memory which

241
00:08:43,919 --> 00:08:46,240
makes sense because this effectively

242
00:08:46,240 --> 00:08:48,240
means we're running more threads in

243
00:08:48,240 --> 00:08:49,760
linux and that has some amount of

244
00:08:49,760 --> 00:08:52,800
overhead and so it uses more memory

245
00:08:52,800 --> 00:08:54,880
but it's faster so this sort of depends

246
00:08:54,880 --> 00:08:56,640
on your simulation environment or your

247
00:08:56,640 --> 00:08:58,160
host machine

248
00:08:58,160 --> 00:09:00,399
um if you don't have memory as a concern

249
00:09:00,399 --> 00:09:02,399
then you can get away with that but this

250
00:09:02,399 --> 00:09:04,080
is a configurable option that you can

251
00:09:04,080 --> 00:09:06,399
choose as you run these simulations so

252
00:09:06,399 --> 00:09:08,399
we're going to choose one host per

253
00:09:08,399 --> 00:09:09,839
thread because it has a better

254
00:09:09,839 --> 00:09:12,320
performance

255
00:09:12,320 --> 00:09:13,839
now

256
00:09:13,839 --> 00:09:15,440
you'll note here that this means we're

257
00:09:15,440 --> 00:09:18,880
running seven threads on four cpus

258
00:09:18,880 --> 00:09:20,560
and we want to avoid cpu over

259
00:09:20,560 --> 00:09:22,880
subscription as i just talked about is

260
00:09:22,880 --> 00:09:24,880
present in emulation approaches like

261
00:09:24,880 --> 00:09:26,080
mininet

262
00:09:26,080 --> 00:09:28,880
so to avoid that we create a

263
00:09:28,880 --> 00:09:30,720
programming construct called a logical

264
00:09:30,720 --> 00:09:33,440
processor and this will allow us to

265
00:09:33,440 --> 00:09:35,279
limit the number of threads that are

266
00:09:35,279 --> 00:09:37,120
concurrently active

267
00:09:37,120 --> 00:09:38,640
we do that through an algorithm that is

268
00:09:38,640 --> 00:09:40,800
based on work stealing

269
00:09:40,800 --> 00:09:43,040
the way it works is each logical

270
00:09:43,040 --> 00:09:45,440
processor starts a thread

271
00:09:45,440 --> 00:09:47,360
and then it will run all events that are

272
00:09:47,360 --> 00:09:49,200
assigned to that thread in the current

273
00:09:49,200 --> 00:09:52,560
round which is a simulated millisecond

274
00:09:52,560 --> 00:09:54,560
once that thread is finished running all

275
00:09:54,560 --> 00:09:57,120
of its events it then sets that thread

276
00:09:57,120 --> 00:09:59,440
to waiting and then we'll take the next

277
00:09:59,440 --> 00:10:00,800
thread that's waiting that hasn't been

278
00:10:00,800 --> 00:10:02,480
run yet in this round

279
00:10:02,480 --> 00:10:04,160
and run that one until its events are

280
00:10:04,160 --> 00:10:06,080
gone and that continues until all the

281
00:10:06,080 --> 00:10:08,880
workers events are completed

282
00:10:08,880 --> 00:10:11,519
and then the clock advances to the next

283
00:10:11,519 --> 00:10:14,560
round and we repeat

284
00:10:14,959 --> 00:10:16,720
okay so now we have a simulation

285
00:10:16,720 --> 00:10:18,560
controller process that

286
00:10:18,560 --> 00:10:21,279
will directly execute these applications

287
00:10:21,279 --> 00:10:22,640
so how that works

288
00:10:22,640 --> 00:10:25,760
is as follows the process

289
00:10:25,760 --> 00:10:27,519
the controller process is running in

290
00:10:27,519 --> 00:10:30,560
linux on top of the kernel

291
00:10:30,560 --> 00:10:33,040
and it will set some environment

292
00:10:33,040 --> 00:10:36,000
variables including ld preload

293
00:10:36,000 --> 00:10:38,800
and an ipc environment variable and then

294
00:10:38,800 --> 00:10:40,640
it will fork and exec

295
00:10:40,640 --> 00:10:42,560
the application process

296
00:10:42,560 --> 00:10:44,720
now because we're setting ld preload to

297
00:10:44,720 --> 00:10:46,800
our shim library this allows us to

298
00:10:46,800 --> 00:10:48,399
inject the shim

299
00:10:48,399 --> 00:10:50,320
into the application process as it's

300
00:10:50,320 --> 00:10:51,839
running

301
00:10:51,839 --> 00:10:53,519
and we use this

302
00:10:53,519 --> 00:10:55,760
shim to then attach

303
00:10:55,760 --> 00:10:58,720
to an ipc channel that allows us to

304
00:10:58,720 --> 00:11:00,959
communicate with our controller process

305
00:11:00,959 --> 00:11:03,120
so at the end of this execution sequence

306
00:11:03,120 --> 00:11:05,440
we have an application running in linux

307
00:11:05,440 --> 00:11:07,040
as a regular linux process that can

308
00:11:07,040 --> 00:11:08,160
communicate

309
00:11:08,160 --> 00:11:10,079
with our controller process

310
00:11:10,079 --> 00:11:12,720
through an ipc channel

311
00:11:12,720 --> 00:11:13,760
and then

312
00:11:13,760 --> 00:11:15,279
we want to intercept

313
00:11:15,279 --> 00:11:16,399
the

314
00:11:16,399 --> 00:11:17,920
system calls that the application is

315
00:11:17,920 --> 00:11:19,519
making

316
00:11:19,519 --> 00:11:21,360
so to do that first

317
00:11:21,360 --> 00:11:23,279
i'll describe that the application is

318
00:11:23,279 --> 00:11:26,000
actually composed of multiple components

319
00:11:26,000 --> 00:11:28,480
it has the application code in addition

320
00:11:28,480 --> 00:11:29,839
to a set of

321
00:11:29,839 --> 00:11:32,480
libraries that are that it's linked to

322
00:11:32,480 --> 00:11:34,320
now because we injected our shim our

323
00:11:34,320 --> 00:11:36,480
shim gets inserted as library zero in

324
00:11:36,480 --> 00:11:38,079
the list of libraries

325
00:11:38,079 --> 00:11:39,600
so whenever a function is called that

326
00:11:39,600 --> 00:11:42,160
does not exist in the application code

327
00:11:42,160 --> 00:11:43,920
these libraries are searched in order

328
00:11:43,920 --> 00:11:45,839
for that function symbol

329
00:11:45,839 --> 00:11:47,839
and the first library that has the

330
00:11:47,839 --> 00:11:50,959
function symbol it will be called first

331
00:11:50,959 --> 00:11:52,800
so we can use this approach

332
00:11:52,800 --> 00:11:55,680
to intercept library calls that are made

333
00:11:55,680 --> 00:11:57,120
normally that were made to the linked

334
00:11:57,120 --> 00:11:58,800
libraries

335
00:11:58,800 --> 00:12:01,040
and we can intercept those to handle

336
00:12:01,040 --> 00:12:04,560
them inside of our shim

337
00:12:05,120 --> 00:12:07,040
uh now notice that this intercepts

338
00:12:07,040 --> 00:12:09,440
library calls but not system calls it's

339
00:12:09,440 --> 00:12:11,680
still possible for application code

340
00:12:11,680 --> 00:12:13,120
to directly

341
00:12:13,120 --> 00:12:15,279
make a system call using

342
00:12:15,279 --> 00:12:17,519
something other than libc using the

343
00:12:17,519 --> 00:12:19,519
assembly language for example

344
00:12:19,519 --> 00:12:21,360
and so we need an approach to intercept

345
00:12:21,360 --> 00:12:23,200
those syscalls as well

346
00:12:23,200 --> 00:12:25,440
and for this we use setcomp or secure

347
00:12:25,440 --> 00:12:26,720
computing

348
00:12:26,720 --> 00:12:29,920
setcomp is a linux facility that

349
00:12:29,920 --> 00:12:32,000
allows you to create a sandbox for your

350
00:12:32,000 --> 00:12:33,279
application

351
00:12:33,279 --> 00:12:35,440
it was designed in a security context to

352
00:12:35,440 --> 00:12:36,480
prevent

353
00:12:36,480 --> 00:12:38,880
applications that get hacked from making

354
00:12:38,880 --> 00:12:39,920
system calls that they weren't

355
00:12:39,920 --> 00:12:41,440
authorized to make

356
00:12:41,440 --> 00:12:43,680
but we can use this in our

357
00:12:43,680 --> 00:12:45,040
approach to

358
00:12:45,040 --> 00:12:47,120
guarantee that any system calls that are

359
00:12:47,120 --> 00:12:48,480
not pre-loadable

360
00:12:48,480 --> 00:12:50,480
don't leak out to the kernel we can trap

361
00:12:50,480 --> 00:12:52,560
all of those system calls and send them

362
00:12:52,560 --> 00:12:56,638
back into the shim so we can handle them

363
00:12:57,279 --> 00:12:59,600
so effectively this application process

364
00:12:59,600 --> 00:13:01,279
is now running

365
00:13:01,279 --> 00:13:02,800
not really on top of the kernel because

366
00:13:02,800 --> 00:13:04,959
we're intercepting all of the kernel

367
00:13:04,959 --> 00:13:06,480
system calls

368
00:13:06,480 --> 00:13:08,800
and and handling them inside of our code

369
00:13:08,800 --> 00:13:09,839
so

370
00:13:09,839 --> 00:13:11,120
the controller is still running on the

371
00:13:11,120 --> 00:13:15,920
kernel and using it as it needs to

372
00:13:16,560 --> 00:13:18,320
so we tested

373
00:13:18,320 --> 00:13:20,000
these two different methods for

374
00:13:20,000 --> 00:13:22,320
intercepting system calls preload and

375
00:13:22,320 --> 00:13:24,320
set comp

376
00:13:24,320 --> 00:13:26,800
across different types of

377
00:13:26,800 --> 00:13:28,240
system calls

378
00:13:28,240 --> 00:13:30,000
including a blocking system call a

379
00:13:30,000 --> 00:13:32,079
non-blocking system call and a system

380
00:13:32,079 --> 00:13:32,880
call

381
00:13:32,880 --> 00:13:35,760
that does read write behavior

382
00:13:35,760 --> 00:13:38,320
and we found that in all these cases

383
00:13:38,320 --> 00:13:41,680
preloading is faster than no preloading

384
00:13:41,680 --> 00:13:43,680
and the reason is quite simple

385
00:13:43,680 --> 00:13:46,240
is because setcomp in order to

386
00:13:46,240 --> 00:13:48,800
trap the system calls setcomp requires

387
00:13:48,800 --> 00:13:50,639
not a full context switch to the kernel

388
00:13:50,639 --> 00:13:52,399
but a mode transition

389
00:13:52,399 --> 00:13:54,079
to run the setcomp filter and that does

390
00:13:54,079 --> 00:13:57,040
have a little bit of overhead so

391
00:13:57,040 --> 00:13:58,639
we can avoid that

392
00:13:58,639 --> 00:14:00,639
if we can we would have a faster

393
00:14:00,639 --> 00:14:02,480
simulator

394
00:14:02,480 --> 00:14:04,959
this is okay because in most cases

395
00:14:04,959 --> 00:14:07,279
system call wrappers exist in lib c and

396
00:14:07,279 --> 00:14:09,680
so those will be able to intercept most

397
00:14:09,680 --> 00:14:11,199
of the functionality that the

398
00:14:11,199 --> 00:14:13,920
application is using

399
00:14:13,920 --> 00:14:16,320
so next once we have intercepted all of

400
00:14:16,320 --> 00:14:18,639
the library calls and system calls into

401
00:14:18,639 --> 00:14:19,839
the shim

402
00:14:19,839 --> 00:14:21,680
we then emulate those

403
00:14:21,680 --> 00:14:23,920
functionalities by

404
00:14:23,920 --> 00:14:26,320
sending those over the ipc channel to

405
00:14:26,320 --> 00:14:28,320
the controller process which can then

406
00:14:28,320 --> 00:14:29,839
emulate

407
00:14:29,839 --> 00:14:32,079
the linux kernel behavior so we emulate

408
00:14:32,079 --> 00:14:34,079
things like file descriptors files

409
00:14:34,079 --> 00:14:36,800
sockets and pipes event notification

410
00:14:36,800 --> 00:14:38,720
pull epoll select

411
00:14:38,720 --> 00:14:41,279
networking buffers and protocols dns and

412
00:14:41,279 --> 00:14:44,279
routing

413
00:14:44,560 --> 00:14:46,959
there's a small caveat here

414
00:14:46,959 --> 00:14:49,120
we are allowed to emulate things inside

415
00:14:49,120 --> 00:14:51,199
of the shim directly it's faster it

416
00:14:51,199 --> 00:14:53,040
doesn't require ipc

417
00:14:53,040 --> 00:14:55,519
um with the controller

418
00:14:55,519 --> 00:14:57,360
so we can do this for hot path cyst

419
00:14:57,360 --> 00:14:58,959
calls in particular time related

420
00:14:58,959 --> 00:15:00,160
syscalls

421
00:15:00,160 --> 00:15:03,120
which are made often by applications

422
00:15:03,120 --> 00:15:04,240
we can intercept those with the

423
00:15:04,240 --> 00:15:05,839
pre-loading approach and then just

424
00:15:05,839 --> 00:15:08,480
return immediately after returning the

425
00:15:08,480 --> 00:15:10,399
time and this saves a significant amount

426
00:15:10,399 --> 00:15:12,720
of overhead

427
00:15:12,720 --> 00:15:14,560
the other system calls will be sent

428
00:15:14,560 --> 00:15:16,720
through the ipc channel which is a bit

429
00:15:16,720 --> 00:15:17,920
slower

430
00:15:17,920 --> 00:15:20,240
and we want to send syscall arguments

431
00:15:20,240 --> 00:15:22,240
and data over to the controller so that

432
00:15:22,240 --> 00:15:24,240
it can effectively

433
00:15:24,240 --> 00:15:27,040
handle the system calls

434
00:15:27,040 --> 00:15:29,120
okay next is inter-process communication

435
00:15:29,120 --> 00:15:30,880
how does this work

436
00:15:30,880 --> 00:15:32,880
so we have an ipc channel between the

437
00:15:32,880 --> 00:15:35,040
process and the controller process and

438
00:15:35,040 --> 00:15:36,560
the app process

439
00:15:36,560 --> 00:15:38,399
we're going to use shared memory as our

440
00:15:38,399 --> 00:15:41,440
ipc channel and the reason is that we

441
00:15:41,440 --> 00:15:43,519
found this is the fastest approach

442
00:15:43,519 --> 00:15:45,600
when we tested it we tested various

443
00:15:45,600 --> 00:15:47,519
approaches including atomic flags

444
00:15:47,519 --> 00:15:50,079
message queues sum of fours on shared

445
00:15:50,079 --> 00:15:52,079
memory and domain sockets

446
00:15:52,079 --> 00:15:53,680
we found shared memory and semaphores as

447
00:15:53,680 --> 00:15:54,959
the fastest

448
00:15:54,959 --> 00:15:56,399
for two processes that are running on

449
00:15:56,399 --> 00:15:58,560
the same core

450
00:15:58,560 --> 00:16:00,079
so we're going to use that

451
00:16:00,079 --> 00:16:01,360
and inside of shared memory we're going

452
00:16:01,360 --> 00:16:03,120
to create a fixed size control block

453
00:16:03,120 --> 00:16:04,720
that we're going to place some state in

454
00:16:04,720 --> 00:16:06,560
including semaphores

455
00:16:06,560 --> 00:16:10,320
syscall registers and simulation time

456
00:16:10,320 --> 00:16:11,839
and

457
00:16:11,839 --> 00:16:13,519
the other type of data that we need to

458
00:16:13,519 --> 00:16:15,360
transfer are

459
00:16:15,360 --> 00:16:17,519
buffers that are dynamically sized such

460
00:16:17,519 --> 00:16:18,720
as those

461
00:16:18,720 --> 00:16:20,880
sent in the send call or the receive

462
00:16:20,880 --> 00:16:22,959
call or reading and writing

463
00:16:22,959 --> 00:16:24,800
those buffers can exist in process

464
00:16:24,800 --> 00:16:27,040
memory and they can be different sizes

465
00:16:27,040 --> 00:16:28,160
and so

466
00:16:28,160 --> 00:16:30,240
we need to handle those the way we do

467
00:16:30,240 --> 00:16:33,440
that is we actually remap the apps stack

468
00:16:33,440 --> 00:16:34,399
and heap

469
00:16:34,399 --> 00:16:36,639
directly into the simulation controller

470
00:16:36,639 --> 00:16:38,320
process

471
00:16:38,320 --> 00:16:39,600
and when they're mapped into the

472
00:16:39,600 --> 00:16:41,519
controller process we can read and write

473
00:16:41,519 --> 00:16:43,440
them directly without any

474
00:16:43,440 --> 00:16:46,720
uh ipc overhead and without any locks

475
00:16:46,720 --> 00:16:49,040
and i'll get to that how we do that in a

476
00:16:49,040 --> 00:16:50,639
minute

477
00:16:50,639 --> 00:16:52,560
we tested this on

478
00:16:52,560 --> 00:16:54,959
buffers with different sizes

479
00:16:54,959 --> 00:16:59,040
increasing here from 1k to 64k

480
00:16:59,040 --> 00:17:00,639
and we found that our memory mapping

481
00:17:00,639 --> 00:17:02,959
approach which is the middle orange bar

482
00:17:02,959 --> 00:17:05,280
is the fast faster approach

483
00:17:05,280 --> 00:17:08,480
compared with a process vm facility that

484
00:17:08,480 --> 00:17:11,039
the linux kernel provides

485
00:17:11,039 --> 00:17:13,199
this process vm facility enables copying

486
00:17:13,199 --> 00:17:16,240
between processes without copying into

487
00:17:16,240 --> 00:17:18,240
application space but we found memory

488
00:17:18,240 --> 00:17:20,160
mapping to work better so we're using

489
00:17:20,160 --> 00:17:22,160
that

490
00:17:22,160 --> 00:17:24,640
now to control the running

491
00:17:24,640 --> 00:17:26,880
to modulate the running state of the

492
00:17:26,880 --> 00:17:28,720
process

493
00:17:28,720 --> 00:17:32,160
we use our semaphores

494
00:17:32,160 --> 00:17:33,919
and the way this works is when the

495
00:17:33,919 --> 00:17:36,240
application process makes a system call

496
00:17:36,240 --> 00:17:38,880
it will write the system call registers

497
00:17:38,880 --> 00:17:41,600
into shared memory and then signal the

498
00:17:41,600 --> 00:17:43,840
controller semaphore to wake up the

499
00:17:43,840 --> 00:17:46,320
controller proce thread and then it will

500
00:17:46,320 --> 00:17:48,720
wait on the app semaphore

501
00:17:48,720 --> 00:17:50,480
the controller process will read the

502
00:17:50,480 --> 00:17:52,000
syscall registers figure out which

503
00:17:52,000 --> 00:17:54,880
syscall it is and what the arguments are

504
00:17:54,880 --> 00:17:57,120
if it's a non-blocking process

505
00:17:57,120 --> 00:17:59,440
system call it can return immediately

506
00:17:59,440 --> 00:18:00,400
otherwise

507
00:18:00,400 --> 00:18:02,880
it will leave the application idle

508
00:18:02,880 --> 00:18:04,720
and it go back to the simulator and

509
00:18:04,720 --> 00:18:07,120
advance simulation time

510
00:18:07,120 --> 00:18:09,200
and return later when it's when the

511
00:18:09,200 --> 00:18:11,039
result is ready

512
00:18:11,039 --> 00:18:12,799
once it does return it will signal the

513
00:18:12,799 --> 00:18:14,960
application semaphore to wake up and it

514
00:18:14,960 --> 00:18:17,440
will wait on the controller semaphore

515
00:18:17,440 --> 00:18:19,039
so the properties that this gives us is

516
00:18:19,039 --> 00:18:21,840
that at any time we only have one of the

517
00:18:21,840 --> 00:18:23,440
application process and the controller

518
00:18:23,440 --> 00:18:24,640
process

519
00:18:24,640 --> 00:18:26,480
that are active at this at any given

520
00:18:26,480 --> 00:18:28,240
time and this ensures non-concurrent

521
00:18:28,240 --> 00:18:30,720
access to the app stack and heat memory

522
00:18:30,720 --> 00:18:32,000
which means we don't need to lock

523
00:18:32,000 --> 00:18:34,000
accesses to that memory

524
00:18:34,000 --> 00:18:36,559
for efficiency

525
00:18:36,559 --> 00:18:40,480
finally we use cpu pinning to pin

526
00:18:40,480 --> 00:18:42,799
a simulation controller

527
00:18:42,799 --> 00:18:44,080
process

528
00:18:44,080 --> 00:18:45,120
thread

529
00:18:45,120 --> 00:18:46,000
and

530
00:18:46,000 --> 00:18:47,679
all of its application processes that

531
00:18:47,679 --> 00:18:49,520
it's handling we pin those to the same

532
00:18:49,520 --> 00:18:52,080
physical cpu core to take advantage of

533
00:18:52,080 --> 00:18:53,200
caching

534
00:18:53,200 --> 00:18:56,080
and to prevent um

535
00:18:56,080 --> 00:18:57,280
cross-core

536
00:18:57,280 --> 00:18:58,799
overheads

537
00:18:58,799 --> 00:18:59,600
and we

538
00:18:59,600 --> 00:19:01,760
we experimented with both pinning and

539
00:19:01,760 --> 00:19:03,840
with real-time scheduling and found that

540
00:19:03,840 --> 00:19:06,080
the majority of the benefit consistently

541
00:19:06,080 --> 00:19:08,160
was due to pinning

542
00:19:08,160 --> 00:19:09,360
in our approach so that's what we're

543
00:19:09,360 --> 00:19:11,360
using

544
00:19:11,360 --> 00:19:13,200
all right just a couple of slides left

545
00:19:13,200 --> 00:19:15,679
for the evaluation and then i'll be done

546
00:19:15,679 --> 00:19:17,679
first we did a large peer-to-peer

547
00:19:17,679 --> 00:19:20,720
benchmark evaluation

548
00:19:20,720 --> 00:19:23,840
these graphs show scales from 1000 nodes

549
00:19:23,840 --> 00:19:26,960
up to 64 000 nodes or processes

550
00:19:26,960 --> 00:19:28,880
the left graph shows the total run time

551
00:19:28,880 --> 00:19:30,799
of the simulation

552
00:19:30,799 --> 00:19:33,760
we're comparing here p trace which is an

553
00:19:33,760 --> 00:19:36,480
optimized version of the grail approach

554
00:19:36,480 --> 00:19:38,720
to setcomp which is our approach to the

555
00:19:38,720 --> 00:19:41,360
uniprocess approach of shadow version

556
00:19:41,360 --> 00:19:42,480
one

557
00:19:42,480 --> 00:19:44,160
the left graph shows the total run time

558
00:19:44,160 --> 00:19:45,679
and we find that we're faster and more

559
00:19:45,679 --> 00:19:47,120
scalable than the uni process the

560
00:19:47,120 --> 00:19:48,640
uniprocess

561
00:19:48,640 --> 00:19:52,160
plug-in architecture across all scales

562
00:19:52,160 --> 00:19:53,520
and on the right we found that our

563
00:19:53,520 --> 00:19:55,679
memory mapping approach uses

564
00:19:55,679 --> 00:19:57,440
significantly less memory than the uni

565
00:19:57,440 --> 00:20:00,160
process plug-in architecture

566
00:20:00,160 --> 00:20:01,120
so

567
00:20:01,120 --> 00:20:04,000
with this peer-to-peer messaging

568
00:20:04,000 --> 00:20:05,679
benchmark

569
00:20:05,679 --> 00:20:07,840
our approach was better in all cases we

570
00:20:07,840 --> 00:20:09,520
were also curious how this works in a

571
00:20:09,520 --> 00:20:12,320
realistic network so we tested it

572
00:20:12,320 --> 00:20:15,679
in a distributed tor network

573
00:20:15,679 --> 00:20:18,640
and here we find that performance in

574
00:20:18,640 --> 00:20:21,280
phantom is comparable to shadow which is

575
00:20:21,280 --> 00:20:23,919
a state-of-the-art pro simulator for

576
00:20:23,919 --> 00:20:25,760
large tor networks

577
00:20:25,760 --> 00:20:27,840
as the network size increases we

578
00:20:27,840 --> 00:20:30,240
approach the the same performance

579
00:20:30,240 --> 00:20:32,559
as shadow so we think this is because

580
00:20:32,559 --> 00:20:34,559
tor has a lot of computation in the

581
00:20:34,559 --> 00:20:36,960
application space and so

582
00:20:36,960 --> 00:20:38,960
it ends up being most of the overhead is

583
00:20:38,960 --> 00:20:42,960
due due to the application layer itself

584
00:20:42,960 --> 00:20:44,240
and then the right graph shows memory

585
00:20:44,240 --> 00:20:46,400
usage again and

586
00:20:46,400 --> 00:20:48,000
again we find that we're significantly

587
00:20:48,000 --> 00:20:48,880
less

588
00:20:48,880 --> 00:20:50,960
memory we use significantly less memory

589
00:20:50,960 --> 00:20:52,720
than the uni process

590
00:20:52,720 --> 00:20:55,039
plug-in architecture from about 90 to 92

591
00:20:55,039 --> 00:20:58,559
percent in the cases we tested

592
00:20:58,559 --> 00:21:00,799
and i'll end again on the takeaways this

593
00:21:00,799 --> 00:21:02,880
right graph shows

594
00:21:02,880 --> 00:21:05,039
phantom compared to shadow ns3 and grail

595
00:21:05,039 --> 00:21:06,240
and that's how we came up with the

596
00:21:06,240 --> 00:21:07,760
numbers this was in a peer-to-peer

597
00:21:07,760 --> 00:21:10,320
benchmark with a thousand hosts

598
00:21:10,320 --> 00:21:12,240
um and everything else i already went

599
00:21:12,240 --> 00:21:14,799
over so i'll stop here and be happy to

600
00:21:14,799 --> 00:21:17,840
take questions

