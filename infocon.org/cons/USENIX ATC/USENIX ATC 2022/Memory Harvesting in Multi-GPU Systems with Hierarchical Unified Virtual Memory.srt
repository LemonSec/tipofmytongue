1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:13,080 --> 00:00:15,299
in this talk I'll introduce memory

3
00:00:15,299 --> 00:00:18,000
harvesting in multi-gpu systems with a

4
00:00:18,000 --> 00:00:20,160
dual database hierarchical unified

5
00:00:20,160 --> 00:00:22,560
virtual memory

6
00:00:22,560 --> 00:00:24,359
this work is done with collaboration

7
00:00:24,359 --> 00:00:27,560
with several Institutes

8
00:00:27,560 --> 00:00:31,019
with increasing demand for gpus many

9
00:00:31,019 --> 00:00:33,540
organizations allow users to share both

10
00:00:33,540 --> 00:00:37,140
GPU class servers for instance employees

11
00:00:37,140 --> 00:00:40,079
in a company share multi-gpu servers to

12
00:00:40,079 --> 00:00:42,719
reduce the cost of infrastructure

13
00:00:42,719 --> 00:00:45,480
in the shared merge GPU server workload

14
00:00:45,480 --> 00:00:47,579
such as machine learning graph analytics

15
00:00:47,579 --> 00:00:50,520
or big data analytics are Consolidated

16
00:00:50,520 --> 00:00:53,760
in a single server to improve resource

17
00:00:53,760 --> 00:00:56,420
utilization

18
00:00:57,660 --> 00:00:59,879
however in such shared multi GPU

19
00:00:59,879 --> 00:01:02,940
environment memory space across gpus is

20
00:01:02,940 --> 00:01:04,920
not fully utilized

21
00:01:04,920 --> 00:01:06,960
we observed that a small amount of

22
00:01:06,960 --> 00:01:10,680
memory space or Cruise CPUs remains idle

23
00:01:10,680 --> 00:01:13,140
the figure in the left is a topology of

24
00:01:13,140 --> 00:01:16,200
AWS both egpu instance which hosts

25
00:01:16,200 --> 00:01:21,080
multiple memory intensive workloads

26
00:01:21,299 --> 00:01:24,420
page rank running on gpu0 over subscribe

27
00:01:24,420 --> 00:01:27,240
GPU memory and uses hot memory as a soft

28
00:01:27,240 --> 00:01:30,180
device the memory consumption of graph

29
00:01:30,180 --> 00:01:32,700
analytics depends on the input grip size

30
00:01:32,700 --> 00:01:36,119
so it highly tends to over subscribe GPU

31
00:01:36,119 --> 00:01:38,820
memory with large input grip size

32
00:01:38,820 --> 00:01:41,280
on the other hand other workloads leave

33
00:01:41,280 --> 00:01:44,460
a small amount of memory on GPU 1 2 and

34
00:01:44,460 --> 00:01:46,880
3.

35
00:01:47,340 --> 00:01:49,619
we profile several scenarios running

36
00:01:49,619 --> 00:01:52,079
graph analytics and DNA training jobs

37
00:01:52,079 --> 00:01:55,380
using one or multiple gpus

38
00:01:55,380 --> 00:01:58,140
because each workload has highly varying

39
00:01:58,140 --> 00:02:00,899
deep memory demands some GPU memory is

40
00:02:00,899 --> 00:02:03,420
over subscribed but this storm is not

41
00:02:03,420 --> 00:02:05,460
fully utilized which cause memory

42
00:02:05,460 --> 00:02:07,799
imbalance across workloads

43
00:02:07,799 --> 00:02:09,899
not only in our profiling scenarios

44
00:02:09,899 --> 00:02:12,840
study from Microsoft reports that memory

45
00:02:12,840 --> 00:02:16,620
utilization is about 75 in shared merge

46
00:02:16,620 --> 00:02:17,940
GPU clusters

47
00:02:17,940 --> 00:02:20,340
showing that memory space or cross gpus

48
00:02:20,340 --> 00:02:23,340
is not efficiently utilized

49
00:02:23,340 --> 00:02:25,860
let's look further what happens when GPU

50
00:02:25,860 --> 00:02:29,180
memory is over subscribed

51
00:02:29,819 --> 00:02:33,000
unified virtual memory UVM introduced by

52
00:02:33,000 --> 00:02:36,060
Nvidia and AMD enables large memory

53
00:02:36,060 --> 00:02:38,580
applications to over subscribe The

54
00:02:38,580 --> 00:02:41,459
Limited GPU memory in driver level

55
00:02:41,459 --> 00:02:43,860
it transparently manages unified address

56
00:02:43,860 --> 00:02:48,720
space across the GPU and host memory

57
00:02:48,720 --> 00:02:52,080
when GPU memory is full the UVM driver

58
00:02:52,080 --> 00:02:55,099
swaps out the GPU memory to host memory

59
00:02:55,099 --> 00:02:59,760
providing transparent over subscription

60
00:02:59,760 --> 00:03:02,640
however the cost of cost for memory over

61
00:03:02,640 --> 00:03:04,319
subscription is high

62
00:03:04,319 --> 00:03:07,140
it's 40 over subscription a single

63
00:03:07,140 --> 00:03:09,959
workload can take 2 to 64 times longer

64
00:03:09,959 --> 00:03:11,940
to complete than without over

65
00:03:11,940 --> 00:03:14,540
subscription

66
00:03:15,360 --> 00:03:17,340
our approach to reduce the performance

67
00:03:17,340 --> 00:03:20,519
degradation of overcome it is to harvest

68
00:03:20,519 --> 00:03:23,940
neighbor GPS memory

69
00:03:23,940 --> 00:03:26,159
as memory space or Cruise GPU is not

70
00:03:26,159 --> 00:03:28,500
fully utilized it can spill some

71
00:03:28,500 --> 00:03:30,540
fraction of over subscribed memory to

72
00:03:30,540 --> 00:03:32,819
neighbor gpus instead of using the host

73
00:03:32,819 --> 00:03:36,619
memory as a soft device

74
00:03:37,739 --> 00:03:40,200
so how can efficiently access neighbor

75
00:03:40,200 --> 00:03:43,040
GPS memory

76
00:03:44,099 --> 00:03:46,860
modern GPU servers provide Envy link as

77
00:03:46,860 --> 00:03:49,019
a fast inner connect to facilitate our

78
00:03:49,019 --> 00:03:51,060
memory harvesting technique

79
00:03:51,060 --> 00:03:53,400
md-link is a high-speed interconnect

80
00:03:53,400 --> 00:03:56,340
supporting GPU to GPU communication to

81
00:03:56,340 --> 00:03:58,379
transmit data without using the

82
00:03:58,379 --> 00:04:01,560
expensive pcie interconnect

83
00:04:01,560 --> 00:04:03,720
the figure in the left is a topology of

84
00:04:03,720 --> 00:04:07,500
aw instance HTTP is connected with the

85
00:04:07,500 --> 00:04:10,140
host with pcie and gpus are connected

86
00:04:10,140 --> 00:04:12,420
with each other with MV link to support

87
00:04:12,420 --> 00:04:15,559
direct communication

88
00:04:16,139 --> 00:04:18,899
in this AWS instance the bandwidth of

89
00:04:18,899 --> 00:04:22,019
pcie is 16 gigabytes per second and

90
00:04:22,019 --> 00:04:24,660
dependence of MV link is 50 gigabytes

91
00:04:24,660 --> 00:04:26,880
per second which is three times higher

92
00:04:26,880 --> 00:04:28,979
than pcie

93
00:04:28,979 --> 00:04:31,080
also we measure the actual throughput

94
00:04:31,080 --> 00:04:34,680
and latency as expected mvlink is about

95
00:04:34,680 --> 00:04:37,080
three times better than pcie on both

96
00:04:37,080 --> 00:04:40,199
throughput and latency which gives us an

97
00:04:40,199 --> 00:04:42,479
attractive opportunity to build a new

98
00:04:42,479 --> 00:04:45,919
database with mv-link

99
00:04:46,340 --> 00:04:48,440
foreign

100
00:04:48,440 --> 00:04:51,600
figure which consists of two GPU memory

101
00:04:51,600 --> 00:04:52,979
and hot memory

102
00:04:52,979 --> 00:04:55,979
memory of GPU 0 and gp1 are connected

103
00:04:55,979 --> 00:04:58,740
with mv-link and each other each of them

104
00:04:58,740 --> 00:05:00,840
is connected with pcie with the hot

105
00:05:00,840 --> 00:05:02,520
memory

106
00:05:02,520 --> 00:05:04,440
let's assume that application running on

107
00:05:04,440 --> 00:05:08,280
GPU 0 is over subscribed and on GPU 1 is

108
00:05:08,280 --> 00:05:12,300
underutilized leaving spare memory

109
00:05:12,300 --> 00:05:14,639
UVM supports application to use host

110
00:05:14,639 --> 00:05:17,639
memory as a soft device by fetching and

111
00:05:17,639 --> 00:05:21,380
evicting Pages through pcie

112
00:05:21,900 --> 00:05:24,000
hierarchical unified virtual memory

113
00:05:24,000 --> 00:05:27,479
which we will call ATU VM based on new

114
00:05:27,479 --> 00:05:30,780
data path with MV link to harvest the

115
00:05:30,780 --> 00:05:33,360
spare memory of neighbor GPU

116
00:05:33,360 --> 00:05:36,900
when spare memory is identified in gpu1

117
00:05:36,900 --> 00:05:39,900
we can leverage the spare memory of gpu1

118
00:05:39,900 --> 00:05:43,020
as a victim cache between gpu0 and the

119
00:05:43,020 --> 00:05:44,580
host

120
00:05:44,580 --> 00:05:46,620
we can evict pages to the victim case

121
00:05:46,620 --> 00:05:49,620
through MV link also since the spare

122
00:05:49,620 --> 00:05:52,740
memory of gpu1 acts as a victim cache we

123
00:05:52,740 --> 00:05:56,100
convert pages from gpu1 to GPU 0 with MV

124
00:05:56,100 --> 00:06:00,080
link when the pages exist on it

125
00:06:00,120 --> 00:06:02,960
foreign

126
00:06:05,360 --> 00:06:08,400
author of this work I will take it on

127
00:06:08,400 --> 00:06:10,440
from here

128
00:06:10,440 --> 00:06:12,780
there are several goals in huvm to

129
00:06:12,780 --> 00:06:14,639
reduce the performance cost of memory

130
00:06:14,639 --> 00:06:16,259
over subscription

131
00:06:16,259 --> 00:06:19,380
first effective harvesting we need to

132
00:06:19,380 --> 00:06:21,419
harvest small and temporarily available

133
00:06:21,419 --> 00:06:24,360
spare memory of neighbor gpus to reduce

134
00:06:24,360 --> 00:06:26,400
the eviction and fetch latency with

135
00:06:26,400 --> 00:06:28,080
spare memory

136
00:06:28,080 --> 00:06:30,360
second we need to minimize the

137
00:06:30,360 --> 00:06:32,100
performance impact of workloads running

138
00:06:32,100 --> 00:06:34,259
in the neighbor gpus

139
00:06:34,259 --> 00:06:37,139
finally huvm needs to be framework

140
00:06:37,139 --> 00:06:38,340
agnostic

141
00:06:38,340 --> 00:06:40,380
since there may be different Frameworks

142
00:06:40,380 --> 00:06:42,539
running in the harvesting gpus and the

143
00:06:42,539 --> 00:06:44,759
yielding gpus there should be no

144
00:06:44,759 --> 00:06:46,740
modification of applications or

145
00:06:46,740 --> 00:06:48,900
Frameworks

146
00:06:48,900 --> 00:06:51,780
to achieve these goals in huvm we

147
00:06:51,780 --> 00:06:54,600
propose mem Harvester a memory manager

148
00:06:54,600 --> 00:06:56,280
for huvm

149
00:06:56,280 --> 00:06:58,139
mem Harvester is a centralized

150
00:06:58,139 --> 00:07:00,780
coordinator for data paths in huvm

151
00:07:00,780 --> 00:07:04,560
leveraging both pcie and mvlink

152
00:07:04,560 --> 00:07:06,960
I will now explain how mem Harvester

153
00:07:06,960 --> 00:07:08,699
achieved these schools in the following

154
00:07:08,699 --> 00:07:10,940
order

155
00:07:11,160 --> 00:07:13,440
mem Harvester hides the eviction latency

156
00:07:13,440 --> 00:07:15,360
with pre-eviction for Effective

157
00:07:15,360 --> 00:07:16,740
harvesting

158
00:07:16,740 --> 00:07:19,080
on-demand eviction increases the pace

159
00:07:19,080 --> 00:07:22,080
fault latency thus it is important to

160
00:07:22,080 --> 00:07:24,180
reduce the on-demand eviction to hide

161
00:07:24,180 --> 00:07:26,039
the eviction latency

162
00:07:26,039 --> 00:07:28,259
mem Harvester reserves a certain amount

163
00:07:28,259 --> 00:07:30,840
of free memory and pre-effects pages to

164
00:07:30,840 --> 00:07:33,479
harvested memory using the Abundant Envy

165
00:07:33,479 --> 00:07:34,919
link bandwidth

166
00:07:34,919 --> 00:07:37,319
the pre-eviction scheme is implemented

167
00:07:37,319 --> 00:07:39,720
in the driver level making the technique

168
00:07:39,720 --> 00:07:42,599
framework agnostic and does not need any

169
00:07:42,599 --> 00:07:45,780
application modifications

170
00:07:45,780 --> 00:07:47,819
once the eviction is completed to the

171
00:07:47,819 --> 00:07:49,259
harvesting memory

172
00:07:49,259 --> 00:07:51,539
we make a copy of the evicted page to

173
00:07:51,539 --> 00:07:54,240
the host memory to minimize interference

174
00:07:54,240 --> 00:07:57,000
let's see why we make the copy

175
00:07:57,000 --> 00:07:58,740
when the application running on the

176
00:07:58,740 --> 00:08:01,440
neighbor GPU increases memory allocation

177
00:08:01,440 --> 00:08:03,840
we should instantly reclaim the

178
00:08:03,840 --> 00:08:05,880
harvested memory to the neighbor GPU

179
00:08:05,880 --> 00:08:08,280
when negligible overhead to minimize

180
00:08:08,280 --> 00:08:09,840
interference

181
00:08:09,840 --> 00:08:12,479
thus we make a copy of the evicted pages

182
00:08:12,479 --> 00:08:15,120
and harvested memory to host memory in a

183
00:08:15,120 --> 00:08:17,639
background write back thread

184
00:08:17,639 --> 00:08:19,919
and the right bag is completed the page

185
00:08:19,919 --> 00:08:22,259
is marked removable and can be returned

186
00:08:22,259 --> 00:08:24,479
to the neighbor GPU without any

187
00:08:24,479 --> 00:08:26,340
additional overhead

188
00:08:26,340 --> 00:08:29,160
to minimize the interference then it is

189
00:08:29,160 --> 00:08:30,780
crucial to quickly make the pages

190
00:08:30,780 --> 00:08:32,700
removable

191
00:08:32,700 --> 00:08:34,380
to increase the right back throughput

192
00:08:34,380 --> 00:08:37,440
MEMP harvestor allocates large pages and

193
00:08:37,440 --> 00:08:39,719
host memory and performs a single

194
00:08:39,719 --> 00:08:44,120
operation for populating large pages

195
00:08:44,399 --> 00:08:46,860
this is the workflow of eviction tasks

196
00:08:46,860 --> 00:08:49,380
coordinated by mem Harvester

197
00:08:49,380 --> 00:08:52,380
the application running on gpu0 will now

198
00:08:52,380 --> 00:08:55,440
access page a b and c

199
00:08:55,440 --> 00:08:58,860
page for the curse and Page a b c are

200
00:08:58,860 --> 00:09:02,100
migrated to gpu0 memory

201
00:09:02,100 --> 00:09:04,440
however because the reserved memory

202
00:09:04,440 --> 00:09:05,820
space is full

203
00:09:05,820 --> 00:09:08,220
HX is pre-evicted to the harvested

204
00:09:08,220 --> 00:09:10,860
memory of gpu1

205
00:09:10,860 --> 00:09:13,260
at the same time the right back thread

206
00:09:13,260 --> 00:09:16,440
copies page X to host memory and marks

207
00:09:16,440 --> 00:09:20,959
the page removable for instant reclaim

208
00:09:21,540 --> 00:09:23,700
we have now seen mem Harvester's

209
00:09:23,700 --> 00:09:26,220
eviction scheme and from now on we will

210
00:09:26,220 --> 00:09:29,640
see how mem Harvester handles fetch

211
00:09:29,640 --> 00:09:31,980
the Harvester reduces the fetch latency

212
00:09:31,980 --> 00:09:34,500
with parallel fetch by leveraging the

213
00:09:34,500 --> 00:09:37,080
new data path consisting with both pcie

214
00:09:37,080 --> 00:09:39,240
and mv-link

215
00:09:39,240 --> 00:09:42,600
GPU applications are highly parallel so

216
00:09:42,600 --> 00:09:45,180
paid fault occurs in a batch

217
00:09:45,180 --> 00:09:47,279
the driver should handle all page fault

218
00:09:47,279 --> 00:09:48,720
requests in a batch to resume

219
00:09:48,720 --> 00:09:50,640
application

220
00:09:50,640 --> 00:09:53,160
mem Harvester parallelizes handling

221
00:09:53,160 --> 00:09:55,740
multiple page vaults in the same batch

222
00:09:55,740 --> 00:09:58,019
by leveraging the new data path with

223
00:09:58,019 --> 00:10:00,360
each PCI Lanes attached to individual

224
00:10:00,360 --> 00:10:02,519
gpus and mv-link

225
00:10:02,519 --> 00:10:04,980
let's see how mem Harvester parallelizes

226
00:10:04,980 --> 00:10:08,399
handling page faults in detail

227
00:10:08,399 --> 00:10:10,980
this sets the workflow of parallel fetch

228
00:10:10,980 --> 00:10:13,860
coordinated by mem Harvester

229
00:10:13,860 --> 00:10:18,180
faithful occurs and Page a b c d in the

230
00:10:18,180 --> 00:10:20,820
same fault batch needs to be migrated to

231
00:10:20,820 --> 00:10:24,620
gpu0 to resume application

232
00:10:24,620 --> 00:10:28,200
page a is migrated to gpu0 and at the

233
00:10:28,200 --> 00:10:31,680
same time page D is migrated to gpu1

234
00:10:31,680 --> 00:10:35,640
both using individual PCI Lanes HP is

235
00:10:35,640 --> 00:10:39,060
migrated to gpu0 and in the same time

236
00:10:39,060 --> 00:10:41,880
PC is migrated to gpu1

237
00:10:41,880 --> 00:10:45,000
finally page C and D is migrated to gpu0

238
00:10:45,000 --> 00:10:46,620
with nvlink

239
00:10:46,620 --> 00:10:48,839
all page Faults Are handled and the

240
00:10:48,839 --> 00:10:51,660
application resumes

241
00:10:51,660 --> 00:10:54,120
now let's see how mem Harvester hides

242
00:10:54,120 --> 00:10:56,519
fetch latency with prefetching

243
00:10:56,519 --> 00:10:59,040
mem Harvester prefetches with multi-path

244
00:10:59,040 --> 00:11:01,560
prefetching leveraging the new data path

245
00:11:01,560 --> 00:11:04,500
consisting with both pcie and MBA link

246
00:11:04,500 --> 00:11:07,560
to exploit parallelism

247
00:11:07,560 --> 00:11:09,660
pages in The harvested neighbor GPU

248
00:11:09,660 --> 00:11:12,240
memory are prefetch to local GPU memory

249
00:11:12,240 --> 00:11:14,160
via nvlink

250
00:11:14,160 --> 00:11:16,620
pages in The Host memory are prefetched

251
00:11:16,620 --> 00:11:18,959
to the harvested neighbor GPU memory via

252
00:11:18,959 --> 00:11:20,579
pcie

253
00:11:20,579 --> 00:11:22,440
there are two main advantages for

254
00:11:22,440 --> 00:11:24,360
prefetching pages to spare memory

255
00:11:24,360 --> 00:11:26,820
instead of local GPU memory

256
00:11:26,820 --> 00:11:29,339
first prefetching to the harvested

257
00:11:29,339 --> 00:11:31,320
memory can reduce the total number of

258
00:11:31,320 --> 00:11:33,839
page volts by proactively fetching Pages

259
00:11:33,839 --> 00:11:37,680
using both pcie and mvlink and secondly

260
00:11:37,680 --> 00:11:40,680
the paceful latency of future pages is

261
00:11:40,680 --> 00:11:43,200
reduced by placing the pages highly

262
00:11:43,200 --> 00:11:45,360
likely to be accessed on the victim

263
00:11:45,360 --> 00:11:46,560
cache

264
00:11:46,560 --> 00:11:48,959
however as the number of active

265
00:11:48,959 --> 00:11:51,600
Harvesters increases the PCI Lane

266
00:11:51,600 --> 00:11:53,820
attached to the neighbor GPU can be

267
00:11:53,820 --> 00:11:55,019
congested

268
00:11:55,019 --> 00:11:57,540
then it slows down supplying pages to

269
00:11:57,540 --> 00:11:59,940
the spare memory due to the limited pcie

270
00:11:59,940 --> 00:12:01,320
bandwidth

271
00:12:01,320 --> 00:12:03,600
to reduce the PCI contention in such

272
00:12:03,600 --> 00:12:07,200
cases mem Harvester changes the past

273
00:12:07,200 --> 00:12:09,839
for pages in host memory then Harvester

274
00:12:09,839 --> 00:12:12,240
directly prefetches those pages to local

275
00:12:12,240 --> 00:12:14,459
GPU memory

276
00:12:14,459 --> 00:12:16,740
this is the workflow of multi-pass

277
00:12:16,740 --> 00:12:19,560
prefetch coordinated by mem Harvester

278
00:12:19,560 --> 00:12:22,440
first let's look at the default path

279
00:12:22,440 --> 00:12:25,500
page e f and g needs to be prefetched

280
00:12:25,500 --> 00:12:27,899
for hiding future latency

281
00:12:27,899 --> 00:12:30,480
hey Steve's which is located on the

282
00:12:30,480 --> 00:12:33,000
Harvester gpu1 memory is prefetched to

283
00:12:33,000 --> 00:12:35,940
gpu0 and at the same time exploiting the

284
00:12:35,940 --> 00:12:39,660
past diversity page E and F and as

285
00:12:39,660 --> 00:12:41,399
prefetched from host memory to harvest

286
00:12:41,399 --> 00:12:44,220
the GPU one memory

287
00:12:44,220 --> 00:12:46,320
however when the number of active

288
00:12:46,320 --> 00:12:48,899
Harvesters increases to reduce the PCI

289
00:12:48,899 --> 00:12:51,480
contention page enf is directly

290
00:12:51,480 --> 00:12:55,680
prefetched from most memory to gpu0

291
00:12:55,680 --> 00:12:58,200
let's have some time to recap we

292
00:12:58,200 --> 00:13:01,680
proposed huvm a new data path with Envy

293
00:13:01,680 --> 00:13:03,959
Lane to reduce the performance cost of

294
00:13:03,959 --> 00:13:05,639
memory over subscription

295
00:13:05,639 --> 00:13:08,220
there are several goals for huvn

296
00:13:08,220 --> 00:13:10,680
effective harvesting minimal

297
00:13:10,680 --> 00:13:14,160
interference and framework agnostic

298
00:13:14,160 --> 00:13:17,100
to achieve these goals we propose the

299
00:13:17,100 --> 00:13:19,500
memory manager for each UVM called mem

300
00:13:19,500 --> 00:13:20,579
Harvester

301
00:13:20,579 --> 00:13:22,380
mem Harvester is a centralized

302
00:13:22,380 --> 00:13:25,680
coordinator for data path in huvm and

303
00:13:25,680 --> 00:13:27,720
supports several techniques

304
00:13:27,720 --> 00:13:30,120
pre-eviction parallel Fetch and

305
00:13:30,120 --> 00:13:32,160
multi-pass prefetch for Effective

306
00:13:32,160 --> 00:13:33,720
harvesting

307
00:13:33,720 --> 00:13:36,180
removable Pages for minimal interference

308
00:13:36,180 --> 00:13:39,360
and leverages UVM to become framework

309
00:13:39,360 --> 00:13:40,800
agnostic

310
00:13:40,800 --> 00:13:43,139
we leave minimizing interference due to

311
00:13:43,139 --> 00:13:45,300
sharing memory and PCI bundles for our

312
00:13:45,300 --> 00:13:47,459
future work

313
00:13:47,459 --> 00:13:50,820
now on to our evaluation all experiments

314
00:13:50,820 --> 00:13:54,000
are done in AWS instance the system has

315
00:13:54,000 --> 00:13:57,420
four gpus connected with nblink the

316
00:13:57,420 --> 00:13:59,160
graph analytics benchmarks are

317
00:13:59,160 --> 00:14:01,440
implemented with cool graph the DNN

318
00:14:01,440 --> 00:14:03,180
training workloads are implemented with

319
00:14:03,180 --> 00:14:04,980
pytorch

320
00:14:04,980 --> 00:14:07,139
first we will look at three scenarios in

321
00:14:07,139 --> 00:14:09,360
the shared multi-gpu server

322
00:14:09,360 --> 00:14:11,220
the table in the left shows the

323
00:14:11,220 --> 00:14:14,700
scenarios there are three cases case a b

324
00:14:14,700 --> 00:14:18,300
and c and each case is consisted of a

325
00:14:18,300 --> 00:14:21,000
mixture of graph analytics workloads and

326
00:14:21,000 --> 00:14:23,040
DNN training workloads

327
00:14:23,040 --> 00:14:25,440
the workloads with box colored and

328
00:14:25,440 --> 00:14:27,660
yellow is a harvester which over

329
00:14:27,660 --> 00:14:30,060
subscribes the memory and harvest other

330
00:14:30,060 --> 00:14:32,459
neighbor gpus memory

331
00:14:32,459 --> 00:14:34,500
the graph in the right shows the memory

332
00:14:34,500 --> 00:14:36,720
consumption of each workload

333
00:14:36,720 --> 00:14:39,320
note that in case a one Harvester

334
00:14:39,320 --> 00:14:42,779
harvests three neighbor gpus memory for

335
00:14:42,779 --> 00:14:46,920
case b to harvest 2 for case 3 3 Harvest

336
00:14:46,920 --> 00:14:48,540
one

337
00:14:48,540 --> 00:14:50,820
let's first look at Casey

338
00:14:50,820 --> 00:14:52,860
the figure on the left shows how each

339
00:14:52,860 --> 00:14:55,019
workloads are placed in the multi-gpu

340
00:14:55,019 --> 00:14:56,060
server

341
00:14:56,060 --> 00:14:58,920
gpu0 runs pagerank which is over

342
00:14:58,920 --> 00:15:01,139
subscribing memory and tries to harvest

343
00:15:01,139 --> 00:15:04,560
the spare memory of other three gpus

344
00:15:04,560 --> 00:15:06,360
the figure on the right shows the

345
00:15:06,360 --> 00:15:09,660
execution time speed up of mem Harvester

346
00:15:09,660 --> 00:15:12,839
the y-axis means execution time speed up

347
00:15:12,839 --> 00:15:15,720
compared to Baseline and ratio so higher

348
00:15:15,720 --> 00:15:17,100
is better

349
00:15:17,100 --> 00:15:19,980
for comparison we use Baseline as the

350
00:15:19,980 --> 00:15:23,459
stock Nvidia UVM driver and pre efos

351
00:15:23,459 --> 00:15:26,160
which is the prefetch and pre-eviction

352
00:15:26,160 --> 00:15:29,220
scheme to host memory adjust the overall

353
00:15:29,220 --> 00:15:31,860
prior technique

354
00:15:31,860 --> 00:15:34,980
page rank running on gpu0 is harvesting

355
00:15:34,980 --> 00:15:37,500
three neighbor GPU spare memory and has

356
00:15:37,500 --> 00:15:40,019
3.5 times better performance compared to

357
00:15:40,019 --> 00:15:42,540
Baseline and 1.3 times better

358
00:15:42,540 --> 00:15:45,240
performance compared to pre-ef host

359
00:15:45,240 --> 00:15:47,459
also there is negligible performance

360
00:15:47,459 --> 00:15:49,980
interference in workloads running on the

361
00:15:49,980 --> 00:15:51,899
neighbor gpus

362
00:15:51,899 --> 00:15:55,860
for case b BFS is running on gpu0 and

363
00:15:55,860 --> 00:15:59,100
gpu1 over subscribing memory on the

364
00:15:59,100 --> 00:16:01,980
other hand GPU 2 and 3 is leaving spare

365
00:16:01,980 --> 00:16:05,519
memory VFS which is running on two gpus

366
00:16:05,519 --> 00:16:09,720
Harvest 2G 2 neighbor gpus spare memory

367
00:16:09,720 --> 00:16:11,940
BFS shows significant performance

368
00:16:11,940 --> 00:16:14,459
Improvement in mem Harvester compared to

369
00:16:14,459 --> 00:16:17,639
both Baseline and pefls

370
00:16:17,639 --> 00:16:19,440
there is some performance interference

371
00:16:19,440 --> 00:16:21,000
in the neighbor gpus

372
00:16:21,000 --> 00:16:23,459
mobile net which is running on GPU 2

373
00:16:23,459 --> 00:16:25,320
shows seven percent of performance

374
00:16:25,320 --> 00:16:27,899
interference and resnet shows nine

375
00:16:27,899 --> 00:16:29,459
percent of performance interference

376
00:16:29,459 --> 00:16:31,620
compared to Baseline

377
00:16:31,620 --> 00:16:35,579
or KC WCC is running on gpu0 and low

378
00:16:35,579 --> 00:16:38,459
vane is running on GPU 1 and 2. over

379
00:16:38,459 --> 00:16:41,000
subscribing memory on the other hand

380
00:16:41,000 --> 00:16:43,800
gpo3 is leaving spare memory

381
00:16:43,800 --> 00:16:46,980
WCC and low vein is running on three

382
00:16:46,980 --> 00:16:49,620
gpus and harvest one labor GPU spare

383
00:16:49,620 --> 00:16:50,639
memory

384
00:16:50,639 --> 00:16:53,100
WCC and low range shows performance

385
00:16:53,100 --> 00:16:55,380
Improvement in mem Harvester compared to

386
00:16:55,380 --> 00:16:57,360
Baseline and pre-exos

387
00:16:57,360 --> 00:16:59,339
there's some performance interference in

388
00:16:59,339 --> 00:17:02,220
the neighbor GPU

389
00:17:02,220 --> 00:17:04,079
now we will break down individual

390
00:17:04,079 --> 00:17:06,599
techniques consisting mem Harvester and

391
00:17:06,599 --> 00:17:08,939
show how each technique contributed to

392
00:17:08,939 --> 00:17:11,520
the overall performance Improvement

393
00:17:11,520 --> 00:17:13,679
we will break down each technique for

394
00:17:13,679 --> 00:17:15,839
BFF and case b

395
00:17:15,839 --> 00:17:18,780
from left to right a single technique is

396
00:17:18,780 --> 00:17:20,640
added to show how that technique

397
00:17:20,640 --> 00:17:22,859
contributed to the overall performance

398
00:17:22,859 --> 00:17:24,900
Improvement

399
00:17:24,900 --> 00:17:27,119
we're just using the naive data passed

400
00:17:27,119 --> 00:17:30,480
hvm we can see 1.8 times performance

401
00:17:30,480 --> 00:17:33,299
Improvement if we add additional schemes

402
00:17:33,299 --> 00:17:35,400
one by one we see that the performance

403
00:17:35,400 --> 00:17:36,600
increases

404
00:17:36,600 --> 00:17:38,580
pre-eviction reduces the number of

405
00:17:38,580 --> 00:17:40,320
on-demand evictions

406
00:17:40,320 --> 00:17:42,419
large page right back for making

407
00:17:42,419 --> 00:17:44,580
removable Pages faster

408
00:17:44,580 --> 00:17:46,679
parallel fetch to reduce the fetch

409
00:17:46,679 --> 00:17:47,640
latency

410
00:17:47,640 --> 00:17:49,740
finally if we add the multi-pass

411
00:17:49,740 --> 00:17:52,380
prefetching to high touch latency which

412
00:17:52,380 --> 00:17:54,059
now uses all of our mem Harvester

413
00:17:54,059 --> 00:17:57,299
techniques we see an overall 3.5 times

414
00:17:57,299 --> 00:18:00,419
performance Improvement

415
00:18:00,419 --> 00:18:03,480
not only for case b for all of our cases

416
00:18:03,480 --> 00:18:05,760
we see that each technique has

417
00:18:05,760 --> 00:18:07,740
contributed to the overall performance

418
00:18:07,740 --> 00:18:10,280
Improvement

419
00:18:11,280 --> 00:18:13,140
so how much memory should be Harvest

420
00:18:13,140 --> 00:18:15,419
from the neighbor GPU to benefit from

421
00:18:15,419 --> 00:18:17,280
our new data path

422
00:18:17,280 --> 00:18:19,620
to find out we perform the sensitivity

423
00:18:19,620 --> 00:18:21,840
study by varying the size of spare

424
00:18:21,840 --> 00:18:22,860
memory

425
00:18:22,860 --> 00:18:26,160
for Simplicity we just use two gpus for

426
00:18:26,160 --> 00:18:28,140
this sensitivity study

427
00:18:28,140 --> 00:18:31,260
one GPU is running WCC which is over

428
00:18:31,260 --> 00:18:33,780
subscribing 94 percent of more memory

429
00:18:33,780 --> 00:18:36,299
and we varied the size of spare memory

430
00:18:36,299 --> 00:18:39,660
in gpu1 with Coda below by varying the

431
00:18:39,660 --> 00:18:43,200
size from 5 to 60 percent

432
00:18:43,200 --> 00:18:45,900
with five percent of spare memory we see

433
00:18:45,900 --> 00:18:48,179
an overall 3.6 times performance

434
00:18:48,179 --> 00:18:51,419
Improvement compared to Baseline as we

435
00:18:51,419 --> 00:18:53,280
increase the size of spare memory in

436
00:18:53,280 --> 00:18:55,799
neighbor gpus we see that the overall

437
00:18:55,799 --> 00:18:57,840
performance increases

438
00:18:57,840 --> 00:19:00,600
we confirmed that just with only five

439
00:19:00,600 --> 00:19:02,700
percent of harvesting spare memory of

440
00:19:02,700 --> 00:19:05,220
neighbor gpus we can significantly

441
00:19:05,220 --> 00:19:08,660
improve the performance

442
00:19:08,880 --> 00:19:11,640
this is our conclusion there exists

443
00:19:11,640 --> 00:19:13,799
resource imbalance and shared multi-gpu

444
00:19:13,799 --> 00:19:15,660
server and there is significant

445
00:19:15,660 --> 00:19:17,640
performance overhead for memory over

446
00:19:17,640 --> 00:19:19,080
subscription

447
00:19:19,080 --> 00:19:21,780
to solve this problem we spill fraction

448
00:19:21,780 --> 00:19:24,120
of over subscription to neighbor gpus

449
00:19:24,120 --> 00:19:27,419
and build a new data path with NB link

450
00:19:27,419 --> 00:19:30,059
also we proposed a new memory manager

451
00:19:30,059 --> 00:19:31,500
for huvm

452
00:19:31,500 --> 00:19:33,419
we improved performance compared to

453
00:19:33,419 --> 00:19:35,820
Baseline over three times with a small

454
00:19:35,820 --> 00:19:38,039
fraction of spare memory but less

455
00:19:38,039 --> 00:19:41,580
performance and improved interference

456
00:19:41,580 --> 00:19:43,559
thank you for listening to our talk if

457
00:19:43,559 --> 00:19:45,179
you have any questions I will be more

458
00:19:45,179 --> 00:19:49,400
than happy to answer them thank you

