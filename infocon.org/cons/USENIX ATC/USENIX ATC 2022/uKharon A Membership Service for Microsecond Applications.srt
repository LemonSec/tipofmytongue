1
00:00:14,320 --> 00:00:16,160
my name is indeed antoine and today i'm

2
00:00:16,160 --> 00:00:17,760
going to talk about mccarran a

3
00:00:17,760 --> 00:00:19,680
membership service for microseconds

4
00:00:19,680 --> 00:00:22,080
applications so this is joint work with

5
00:00:22,080 --> 00:00:24,880
rashid and athanasius from epfl as well

6
00:00:24,880 --> 00:00:27,439
as huabing and bankfee and xavier from

7
00:00:27,439 --> 00:00:28,960
huawei

8
00:00:28,960 --> 00:00:31,760
so let's start this talk with an

9
00:00:31,760 --> 00:00:33,840
overview of what data centers can look

10
00:00:33,840 --> 00:00:35,920
like today well given that modern

11
00:00:35,920 --> 00:00:38,160
applications tend to follow a micro

12
00:00:38,160 --> 00:00:42,239
services or oriented architecture well

13
00:00:42,239 --> 00:00:44,480
that data center is pretty much a zoo

14
00:00:44,480 --> 00:00:46,879
with many micro services all connected

15
00:00:46,879 --> 00:00:49,760
together in many integrated ways and

16
00:00:49,760 --> 00:00:51,920
that's great because at the end of the

17
00:00:51,920 --> 00:00:54,640
day you get like elasticity but the

18
00:00:54,640 --> 00:00:56,719
problem with this architecture is that

19
00:00:56,719 --> 00:00:58,480
when you have a failure like the one

20
00:00:58,480 --> 00:01:00,879
here on top well it actually propagates

21
00:01:00,879 --> 00:01:03,120
to many micro services and potentially

22
00:01:03,120 --> 00:01:05,519
many applications and so that's the

23
00:01:05,519 --> 00:01:08,240
reason why today maybe more than ever

24
00:01:08,240 --> 00:01:10,560
failures truly are first-class citizens

25
00:01:10,560 --> 00:01:12,320
that you should try to deal with as fast

26
00:01:12,320 --> 00:01:14,560
as possible before they propagate too

27
00:01:14,560 --> 00:01:16,000
much

28
00:01:16,000 --> 00:01:18,720
but like failures are not a new thing so

29
00:01:18,720 --> 00:01:21,040
how do we usually deal with them and the

30
00:01:21,040 --> 00:01:23,040
answer is that this is pretty much the

31
00:01:23,040 --> 00:01:24,880
reason why almost every system

32
00:01:24,880 --> 00:01:28,000
distributed system uses selfies such as

33
00:01:28,000 --> 00:01:31,119
etcd or zookeeper these tools can do

34
00:01:31,119 --> 00:01:32,640
many things but what we are interested

35
00:01:32,640 --> 00:01:35,439
in today is their ability as

36
00:01:35,439 --> 00:01:37,600
membership services

37
00:01:37,600 --> 00:01:41,119
so membership services quickly basically

38
00:01:41,119 --> 00:01:43,119
they act as reliable configuration

39
00:01:43,119 --> 00:01:45,680
stores so by configuration i pretty much

40
00:01:45,680 --> 00:01:47,840
mean like what is the set of nodes that

41
00:01:47,840 --> 00:01:49,600
are within the system

42
00:01:49,600 --> 00:01:51,200
and it's very important that this

43
00:01:51,200 --> 00:01:54,399
configuration or view or membership

44
00:01:54,399 --> 00:01:56,000
they are pretty much the same are stored

45
00:01:56,000 --> 00:01:58,240
in a reliable fashion because everything

46
00:01:58,240 --> 00:02:00,079
in the system depends on having access

47
00:02:00,079 --> 00:02:01,680
to this configuration so you don't want

48
00:02:01,680 --> 00:02:04,000
to lose configurations

49
00:02:04,000 --> 00:02:05,840
the second thing is that membership

50
00:02:05,840 --> 00:02:08,560
services they are able to update this

51
00:02:08,560 --> 00:02:10,479
configuration in a sequential manner

52
00:02:10,479 --> 00:02:12,640
like by producing a sequence of

53
00:02:12,640 --> 00:02:14,640
configurations and again that's very

54
00:02:14,640 --> 00:02:15,760
important

55
00:02:15,760 --> 00:02:17,440
because you don't want

56
00:02:17,440 --> 00:02:19,840
parts of your systems to be evolving in

57
00:02:19,840 --> 00:02:21,680
different ways

58
00:02:21,680 --> 00:02:23,280
so for um

59
00:02:23,280 --> 00:02:24,160
for

60
00:02:24,160 --> 00:02:26,319
so yeah that's why we build a sequence

61
00:02:26,319 --> 00:02:27,840
of configurations

62
00:02:27,840 --> 00:02:29,520
and lastly they should be able to

63
00:02:29,520 --> 00:02:31,840
invalidate old memberships

64
00:02:31,840 --> 00:02:34,080
to see why the last point is important

65
00:02:34,080 --> 00:02:36,400
we'll go with a small example so let's

66
00:02:36,400 --> 00:02:38,239
say that within our system we have a

67
00:02:38,239 --> 00:02:40,560
data store that is hidden behind the

68
00:02:40,560 --> 00:02:42,879
cache and we have a client that wants to

69
00:02:42,879 --> 00:02:46,160
retrieve value storing our system so the

70
00:02:46,160 --> 00:02:47,519
thing it does is that it's going to wait

71
00:02:47,519 --> 00:02:49,360
for membership and this membership that

72
00:02:49,360 --> 00:02:52,480
it gets m1 there is that the cache c1 is

73
00:02:52,480 --> 00:02:55,120
part of the system so it can use it to

74
00:02:55,120 --> 00:02:57,599
actually retrieve the data the problem

75
00:02:57,599 --> 00:02:59,599
is that what it doesn't know that this

76
00:02:59,599 --> 00:03:02,560
cache has long been replaced by in a

77
00:03:02,560 --> 00:03:05,440
second membership by c2 and so actually

78
00:03:05,440 --> 00:03:08,159
the data it was retrieving from c1 was

79
00:03:08,159 --> 00:03:10,159
stale and that's the reason why it's

80
00:03:10,159 --> 00:03:12,720
very important that old memberships get

81
00:03:12,720 --> 00:03:15,280
invalidated so that you don't have parts

82
00:03:15,280 --> 00:03:17,120
of your system evolving in a previous

83
00:03:17,120 --> 00:03:19,519
configuration

84
00:03:19,519 --> 00:03:22,000
and the thing is that all these

85
00:03:22,000 --> 00:03:23,840
all these features that are provided by

86
00:03:23,840 --> 00:03:26,480
systems such as etcd and zookeeper but

87
00:03:26,480 --> 00:03:28,640
the problem is that they do not evolve

88
00:03:28,640 --> 00:03:30,959
at a microsoft scale so we end up having

89
00:03:30,959 --> 00:03:33,599
applications like in high frequency

90
00:03:33,599 --> 00:03:36,000
trading that operate at microsoft scale

91
00:03:36,000 --> 00:03:38,239
but whenever there's a reconfiguration

92
00:03:38,239 --> 00:03:40,640
or failure actually it suffers from like

93
00:03:40,640 --> 00:03:43,360
huge latency spikes and that's a huge

94
00:03:43,360 --> 00:03:44,640
deal

95
00:03:44,640 --> 00:03:46,640
and so that's where we intervene with

96
00:03:46,640 --> 00:03:49,120
our contribution that is macaron so

97
00:03:49,120 --> 00:03:51,360
macaron is a membership service that

98
00:03:51,360 --> 00:03:53,680
operates at a microscope scale

99
00:03:53,680 --> 00:03:55,519
it's able to detect failures in as

100
00:03:55,519 --> 00:03:58,159
detail as 15 micros to update the

101
00:03:58,159 --> 00:04:01,040
membership in 10 and then to invalidate

102
00:04:01,040 --> 00:04:04,239
all memberships in 25 and so all in all

103
00:04:04,239 --> 00:04:06,480
this allows macaron to react to failures

104
00:04:06,480 --> 00:04:08,799
in 50 micros and it does so by

105
00:04:08,799 --> 00:04:11,040
leveraging rdma

106
00:04:11,040 --> 00:04:13,120
because rdma is pretty much at the core

107
00:04:13,120 --> 00:04:14,720
of macron let me give you a short

108
00:04:14,720 --> 00:04:17,759
overview of what it is well rdma stands

109
00:04:17,759 --> 00:04:19,918
for remote direct memory access it's a

110
00:04:19,918 --> 00:04:22,160
networking technology that allows a

111
00:04:22,160 --> 00:04:25,759
process to access the memory of a rev of

112
00:04:25,759 --> 00:04:28,160
a remote one without invoking the cpu of

113
00:04:28,160 --> 00:04:29,199
the latter

114
00:04:29,199 --> 00:04:32,320
so in that example we have writer on the

115
00:04:32,320 --> 00:04:34,639
left that wants to put a message

116
00:04:34,639 --> 00:04:36,479
in the memory of the guy on the right so

117
00:04:36,479 --> 00:04:39,199
the only thing it has to do is to send a

118
00:04:39,199 --> 00:04:41,440
small request to its local rdma nic that

119
00:04:41,440 --> 00:04:43,280
is going to fetch data in the local

120
00:04:43,280 --> 00:04:45,199
memory and send it over fabric and

121
00:04:45,199 --> 00:04:47,440
install it in the remote memory

122
00:04:47,440 --> 00:04:49,120
and a great thing with rdma is that you

123
00:04:49,120 --> 00:04:51,759
do not invoke the cpu of the machine on

124
00:04:51,759 --> 00:04:54,320
the right and that allows for microscope

125
00:04:54,320 --> 00:04:56,320
scale communication and great tail

126
00:04:56,320 --> 00:04:57,600
latency

127
00:04:57,600 --> 00:04:59,360
so now that we have another view of rdma

128
00:04:59,360 --> 00:05:01,680
we can go back to macron itself

129
00:05:01,680 --> 00:05:03,919
so the first task that macron has to do

130
00:05:03,919 --> 00:05:06,160
is to detect failures very fast

131
00:05:06,160 --> 00:05:08,080
and usually in distributed systems

132
00:05:08,080 --> 00:05:09,759
detecting failures fast is kind of

133
00:05:09,759 --> 00:05:11,840
problematic because we detect failures

134
00:05:11,840 --> 00:05:14,479
by relying on timeouts and so when you

135
00:05:14,479 --> 00:05:16,320
have timeouts you have two strategies

136
00:05:16,320 --> 00:05:19,440
either you go with like uh very large

137
00:05:19,440 --> 00:05:21,280
timeouts in which case you don't have a

138
00:05:21,280 --> 00:05:22,880
lot of false positives but it takes

139
00:05:22,880 --> 00:05:25,039
forever to detect a failure or you go

140
00:05:25,039 --> 00:05:27,520
with short timeouts in which case you're

141
00:05:27,520 --> 00:05:29,600
able to detect failures super fast but

142
00:05:29,600 --> 00:05:31,680
you will have a lot of false positives

143
00:05:31,680 --> 00:05:33,199
and that's a problem because you don't

144
00:05:33,199 --> 00:05:35,360
want to suffer from reconfiguration that

145
00:05:35,360 --> 00:05:36,840
are useless for your

146
00:05:36,840 --> 00:05:39,039
system so

147
00:05:39,039 --> 00:05:40,960
timeouts do not help

148
00:05:40,960 --> 00:05:41,919
and

149
00:05:41,919 --> 00:05:43,759
macron works around this issue but

150
00:05:43,759 --> 00:05:46,080
recognizing that not all failures are

151
00:05:46,080 --> 00:05:46,960
equal

152
00:05:46,960 --> 00:05:49,120
actually there are like many kinds of

153
00:05:49,120 --> 00:05:50,639
failures and for many of them you don't

154
00:05:50,639 --> 00:05:52,560
need to rely on timeouts

155
00:05:52,560 --> 00:05:53,919
for instance

156
00:05:53,919 --> 00:05:55,840
when you have process faders you can

157
00:05:55,840 --> 00:05:57,440
rely on

158
00:05:57,440 --> 00:05:58,880
on the kernel to get some help and to

159
00:05:58,880 --> 00:06:00,560
avoid timeouts

160
00:06:00,560 --> 00:06:02,639
when you have kernel failures well you

161
00:06:02,639 --> 00:06:04,880
can get help from the rdm and nick and

162
00:06:04,880 --> 00:06:07,759
again not use timeouts

163
00:06:07,759 --> 00:06:09,360
sure when you cannot communicate with

164
00:06:09,360 --> 00:06:11,199
the machine at all for instance there's

165
00:06:11,199 --> 00:06:12,800
power failure then you have to rely on

166
00:06:12,800 --> 00:06:14,479
timeouts but hopefully because the

167
00:06:14,479 --> 00:06:16,720
fabric is fast enough you can

168
00:06:16,720 --> 00:06:18,240
tune the timeouts that you're going to

169
00:06:18,240 --> 00:06:20,560
be using for rdma

170
00:06:20,560 --> 00:06:22,000
there's a kind of failures that we don't

171
00:06:22,000 --> 00:06:23,680
deal with and those are byzantine

172
00:06:23,680 --> 00:06:26,880
failures they fall out of our scope

173
00:06:26,880 --> 00:06:29,360
so let's have a quick look at how we

174
00:06:29,360 --> 00:06:31,280
deal with the first two kind of failures

175
00:06:31,280 --> 00:06:33,120
well when there's a process failures

176
00:06:33,120 --> 00:06:35,520
it's process failure it's that simple

177
00:06:35,520 --> 00:06:37,759
the app just pre-registers itself to the

178
00:06:37,759 --> 00:06:40,639
kernel and tells it please when i crash

179
00:06:40,639 --> 00:06:42,639
send my id over the network send a crash

180
00:06:42,639 --> 00:06:45,199
notification and so when it happens the

181
00:06:45,199 --> 00:06:47,120
canon is going to handle the depth of

182
00:06:47,120 --> 00:06:49,520
the process and simply send notification

183
00:06:49,520 --> 00:06:51,600
over the network this is that simple

184
00:06:51,600 --> 00:06:53,919
this does not involve timeouts and this

185
00:06:53,919 --> 00:06:56,080
is 100 accurate

186
00:06:56,080 --> 00:06:57,680
but sometimes we cannot even get help

187
00:06:57,680 --> 00:06:59,520
from the kernel for instance maybe the

188
00:06:59,520 --> 00:07:01,199
cpu is hanging

189
00:07:01,199 --> 00:07:03,280
so that's an issue and that's why we use

190
00:07:03,280 --> 00:07:05,440
another feather detector that we run in

191
00:07:05,440 --> 00:07:06,639
parallel

192
00:07:06,639 --> 00:07:08,560
you see for that feather detector to

193
00:07:08,560 --> 00:07:10,880
work what we mandate from the app is to

194
00:07:10,880 --> 00:07:13,199
have a counter local one that it exposes

195
00:07:13,199 --> 00:07:16,160
over rdma and it's gonna increment that

196
00:07:16,160 --> 00:07:18,720
counter in a timely manner then as a

197
00:07:18,720 --> 00:07:20,800
remote node what you do to detect a

198
00:07:20,800 --> 00:07:22,639
failure of this app is that you're gonna

199
00:07:22,639 --> 00:07:25,520
read that counter over rdma and if by

200
00:07:25,520 --> 00:07:28,160
reading twice in a row you detect that

201
00:07:28,160 --> 00:07:30,319
this entire counter has not been updated

202
00:07:30,319 --> 00:07:32,319
then you know that the app has crashed

203
00:07:32,319 --> 00:07:34,880
uh the very neat thing with that uh with

204
00:07:34,880 --> 00:07:37,680
that uh failure detector is that

205
00:07:37,680 --> 00:07:39,599
uh it works even if the kernel is dead

206
00:07:39,599 --> 00:07:42,240
even if the app is dead because rdma

207
00:07:42,240 --> 00:07:43,919
truly does not need the cpu to be

208
00:07:43,919 --> 00:07:46,000
working to access the memory so the only

209
00:07:46,000 --> 00:07:47,919
thing that you need to work you need for

210
00:07:47,919 --> 00:07:48,800
that

211
00:07:48,800 --> 00:07:50,319
failure detection mechanism to work is

212
00:07:50,319 --> 00:07:52,800
that the argument nic is still

213
00:07:52,800 --> 00:07:53,919
working

214
00:07:53,919 --> 00:07:55,840
and the other nice thing is that this

215
00:07:55,840 --> 00:07:56,960
does not

216
00:07:56,960 --> 00:07:59,520
depend on the network asynchrony the

217
00:07:59,520 --> 00:08:01,280
only thing that you need to be able to

218
00:08:01,280 --> 00:08:02,800
do as a remote node is to access the

219
00:08:02,800 --> 00:08:04,000
counter it doesn't matter how long it

220
00:08:04,000 --> 00:08:05,280
took to access the counter it could have

221
00:08:05,280 --> 00:08:07,520
taken like five years doesn't matter so

222
00:08:07,520 --> 00:08:09,680
we don't have false positives there

223
00:08:09,680 --> 00:08:10,639
and so

224
00:08:10,639 --> 00:08:13,120
the takeaway is that really if you can

225
00:08:13,120 --> 00:08:15,199
somehow manage to take network synchrony

226
00:08:15,199 --> 00:08:17,680
out of the equation then you can have

227
00:08:17,680 --> 00:08:20,960
fast and accurate failure detection

228
00:08:20,960 --> 00:08:22,639
then the second part when you have a

229
00:08:22,639 --> 00:08:24,080
nice feather detector that works at

230
00:08:24,080 --> 00:08:26,080
microsecond scale is to update the

231
00:08:26,080 --> 00:08:28,319
membership in a consistent manner

232
00:08:28,319 --> 00:08:31,520
so to do so macaron uses plain old paxos

233
00:08:31,520 --> 00:08:33,839
that plays a very nice trick

234
00:08:33,839 --> 00:08:36,080
so because it optimized it for

235
00:08:36,080 --> 00:08:38,958
optimizes it for rdma unfortunately i

236
00:08:38,958 --> 00:08:41,039
would have to go over paxos very briefly

237
00:08:41,039 --> 00:08:43,120
not trying to explain how it works and

238
00:08:43,120 --> 00:08:45,360
why it works but how it works

239
00:08:45,360 --> 00:08:48,000
so in paxos we have two kinds of nodes

240
00:08:48,000 --> 00:08:50,800
we have proposers and acceptors

241
00:08:50,800 --> 00:08:52,399
the acceptors they truly are the only

242
00:08:52,399 --> 00:08:54,160
one to maintain some state and this

243
00:08:54,160 --> 00:08:56,080
state is pretty minimal and for a

244
00:08:56,080 --> 00:08:58,000
purpose to decide for instance on a

245
00:08:58,000 --> 00:09:00,000
sequence of memberships the only thing

246
00:09:00,000 --> 00:09:02,160
it does is that it's going to invoke two

247
00:09:02,160 --> 00:09:05,200
rpcs at the acceptors and these rpcs as

248
00:09:05,200 --> 00:09:06,880
you can see they are dead simple you

249
00:09:06,880 --> 00:09:09,040
just they just get and the acceptor just

250
00:09:09,040 --> 00:09:11,839
gets some value from the proposer it

251
00:09:11,839 --> 00:09:14,080
uses this value to update its local

252
00:09:14,080 --> 00:09:16,399
state and then it returns

253
00:09:16,399 --> 00:09:19,440
one of its local variable super simple

254
00:09:19,440 --> 00:09:21,279
the problem with paxos is that it

255
00:09:21,279 --> 00:09:24,720
involves the cpu on both sides so the

256
00:09:24,720 --> 00:09:26,959
trick we were playing there is that we

257
00:09:26,959 --> 00:09:30,080
move all this logic there on the side of

258
00:09:30,080 --> 00:09:31,360
the proposer

259
00:09:31,360 --> 00:09:33,360
so we take everything away from the

260
00:09:33,360 --> 00:09:35,519
acceptor except for the state and we

261
00:09:35,519 --> 00:09:37,839
transform the original rpc into

262
00:09:37,839 --> 00:09:40,560
one-sided one how does it work well very

263
00:09:40,560 --> 00:09:43,600
simple first the proposer reads the

264
00:09:43,600 --> 00:09:46,160
state from the acceptor locally and it

265
00:09:46,160 --> 00:09:48,480
gets it local then it applies the

266
00:09:48,480 --> 00:09:50,320
transformation on the local state that

267
00:09:50,320 --> 00:09:53,200
it fetched and then it uses a comparing

268
00:09:53,200 --> 00:09:55,680
swap to update the state of the remote

269
00:09:55,680 --> 00:09:58,160
acceptor and the nice thing here is that

270
00:09:58,160 --> 00:10:00,800
this comparing swap ensures that this

271
00:10:00,800 --> 00:10:04,240
remote remote invocation of the rpc is

272
00:10:04,240 --> 00:10:07,120
absolutely equivalent to an rpc that

273
00:10:07,120 --> 00:10:08,959
would be accepted would be a sorry

274
00:10:08,959 --> 00:10:11,760
running on the acceptor

275
00:10:11,760 --> 00:10:14,560
and so the takeaway here is that by

276
00:10:14,560 --> 00:10:17,040
using one-sided rpcs while we're

277
00:10:17,040 --> 00:10:19,680
actually able to achieve blazing fast

278
00:10:19,680 --> 00:10:22,399
consensus

279
00:10:23,600 --> 00:10:26,000
then the next thing that you want to do

280
00:10:26,000 --> 00:10:28,079
once you have decided in a sequence of

281
00:10:28,079 --> 00:10:29,519
memberships is to make sure that there's

282
00:10:29,519 --> 00:10:31,680
no one living in a previous memberships

283
00:10:31,680 --> 00:10:33,279
previous membership

284
00:10:33,279 --> 00:10:35,279
so if we go back to the issue we had the

285
00:10:35,279 --> 00:10:36,959
beginning of the presentation so the

286
00:10:36,959 --> 00:10:39,440
client had gotten a membership but he

287
00:10:39,440 --> 00:10:41,279
didn't know whether that membership was

288
00:10:41,279 --> 00:10:43,040
actually the active one the current one

289
00:10:43,040 --> 00:10:45,519
the one in which everyone should be

290
00:10:45,519 --> 00:10:47,600
should be operating

291
00:10:47,600 --> 00:10:49,680
so what is the active membership we

292
00:10:49,680 --> 00:10:52,560
expose that with a method that we call

293
00:10:52,560 --> 00:10:55,040
the active method to which the client is

294
00:10:55,040 --> 00:10:56,480
simply going to give a membership and

295
00:10:56,480 --> 00:10:58,800
get back boolean saying yes or no this

296
00:10:58,800 --> 00:11:01,279
thing is the active one

297
00:11:01,279 --> 00:11:02,959
how it works that symbol the client is

298
00:11:02,959 --> 00:11:05,040
just going to ask the membership service

299
00:11:05,040 --> 00:11:07,519
then when it knows that that

300
00:11:07,519 --> 00:11:09,839
that membership is active and when it

301
00:11:09,839 --> 00:11:11,920
gets the yes the thing is activity knows

302
00:11:11,920 --> 00:11:12,959
that at some point between the

303
00:11:12,959 --> 00:11:15,040
invocation and the response of the

304
00:11:15,040 --> 00:11:15,920
method

305
00:11:15,920 --> 00:11:16,959
that

306
00:11:16,959 --> 00:11:19,200
that membership was active so then it

307
00:11:19,200 --> 00:11:20,880
contacts the cache

308
00:11:20,880 --> 00:11:23,360
and finally after having read the value

309
00:11:23,360 --> 00:11:26,399
from the cache it calls again active and

310
00:11:26,399 --> 00:11:28,560
what it knows there is that because the

311
00:11:28,560 --> 00:11:31,279
membership was active before and also

312
00:11:31,279 --> 00:11:34,000
after reading from the cache then that

313
00:11:34,000 --> 00:11:36,240
membership was active all the way

314
00:11:36,240 --> 00:11:38,079
through the when we were reading the

315
00:11:38,079 --> 00:11:39,120
cache so

316
00:11:39,120 --> 00:11:42,079
this the read there was not pulling some

317
00:11:42,079 --> 00:11:44,640
stale data

318
00:11:44,640 --> 00:11:46,160
the problem with this approach is that

319
00:11:46,160 --> 00:11:48,160
it has super high latency as you can see

320
00:11:48,160 --> 00:11:49,920
in this example we just tripled the

321
00:11:49,920 --> 00:11:52,000
latency of our cells

322
00:11:52,000 --> 00:11:54,240
so how could we kind of preserve the

323
00:11:54,240 --> 00:11:56,800
semantics of this active coal but make

324
00:11:56,800 --> 00:11:58,959
sure that it has as low overhead as

325
00:11:58,959 --> 00:12:00,800
possible

326
00:12:00,800 --> 00:12:04,399
and our answer is to use lizes

327
00:12:04,399 --> 00:12:06,560
and what we moreover what we want is

328
00:12:06,560 --> 00:12:08,880
that those diseases should be as as

329
00:12:08,880 --> 00:12:11,040
small as possible because the longer

330
00:12:11,040 --> 00:12:13,040
leases are in the distributed system the

331
00:12:13,040 --> 00:12:14,880
longer it's going to take to move on to

332
00:12:14,880 --> 00:12:17,040
the next configuration so if we can get

333
00:12:17,040 --> 00:12:19,200
these to work at a microscope scale then

334
00:12:19,200 --> 00:12:21,440
that's great and that's what we do so

335
00:12:21,440 --> 00:12:23,600
the clients delete the results of the

336
00:12:23,600 --> 00:12:26,880
active method for 20 micros then they

337
00:12:26,880 --> 00:12:28,720
keep on renewing the lenses that they

338
00:12:28,720 --> 00:12:30,880
have on on this thing in the background

339
00:12:30,880 --> 00:12:33,519
to be sure that they always have some

340
00:12:33,519 --> 00:12:34,880
way of

341
00:12:34,880 --> 00:12:37,360
getting the result from active locally

342
00:12:37,360 --> 00:12:39,279
so if we go back to our previous example

343
00:12:39,279 --> 00:12:41,600
now the client is split in two parts the

344
00:12:41,600 --> 00:12:44,000
background and foreground and so as i

345
00:12:44,000 --> 00:12:45,760
was saying the background is going to

346
00:12:45,760 --> 00:12:48,320
get released and renew it so that now

347
00:12:48,320 --> 00:12:50,560
when the client wants to read some data

348
00:12:50,560 --> 00:12:51,760
from the cache

349
00:12:51,760 --> 00:12:53,760
it can simply check

350
00:12:53,760 --> 00:12:55,839
if the membership it has is the active

351
00:12:55,839 --> 00:12:57,279
one locally

352
00:12:57,279 --> 00:12:59,839
zero overhead almost

353
00:12:59,839 --> 00:13:01,279
read from the cache

354
00:13:01,279 --> 00:13:03,600
and then it's gonna send which again the

355
00:13:03,600 --> 00:13:07,600
operation but with very low overhead

356
00:13:07,600 --> 00:13:08,880
as you can see

357
00:13:08,880 --> 00:13:11,440
and the great thing with that scheme is

358
00:13:11,440 --> 00:13:14,240
that it does not require require

359
00:13:14,240 --> 00:13:15,920
clocks to be synchronized within the

360
00:13:15,920 --> 00:13:17,760
system that is

361
00:13:17,760 --> 00:13:20,160
as you can see we never sent a timestamp

362
00:13:20,160 --> 00:13:21,760
over the network we're able to have

363
00:13:21,760 --> 00:13:24,560
these without communicating about what

364
00:13:24,560 --> 00:13:26,560
this guy thought about the time was to

365
00:13:26,560 --> 00:13:28,560
the membership service the only thing

366
00:13:28,560 --> 00:13:30,399
that we require is that

367
00:13:30,399 --> 00:13:32,720
the clock drift is approximately is

368
00:13:32,720 --> 00:13:35,279
bounded so that the time when measured

369
00:13:35,279 --> 00:13:37,360
at the client is approximately the same

370
00:13:37,360 --> 00:13:39,279
as the time when measured at another

371
00:13:39,279 --> 00:13:40,560
client

372
00:13:40,560 --> 00:13:42,959
and this allows you to have because you

373
00:13:42,959 --> 00:13:45,600
don't have clock synchronized clocks you

374
00:13:45,600 --> 00:13:47,760
have leases that last approximately 20

375
00:13:47,760 --> 00:13:48,880
micros

376
00:13:48,880 --> 00:13:51,519
and because you are losing your overhead

377
00:13:51,519 --> 00:13:53,680
is no more than 40 nanos at the end of

378
00:13:53,680 --> 00:13:56,000
the day

379
00:13:56,160 --> 00:13:58,480
so that was it for our macaron briefly

380
00:13:58,480 --> 00:14:00,720
works i mean that's just a summary now

381
00:14:00,720 --> 00:14:02,480
the question is how does it perform so

382
00:14:02,480 --> 00:14:04,959
we are moving to the evaluation

383
00:14:04,959 --> 00:14:07,760
so first of all our setup our setup is

384
00:14:07,760 --> 00:14:10,320
comprised of eight nodes they are all

385
00:14:10,320 --> 00:14:14,000
linked together via 100 100 giga bps

386
00:14:14,000 --> 00:14:15,199
switch

387
00:14:15,199 --> 00:14:17,760
we have three nodes that we are using to

388
00:14:17,760 --> 00:14:19,519
replicate macron

389
00:14:19,519 --> 00:14:22,320
uh two nodes that we use to for the

390
00:14:22,320 --> 00:14:23,600
service that we're gonna benchmark

391
00:14:23,600 --> 00:14:26,240
macron through and then we have three

392
00:14:26,240 --> 00:14:28,839
clients that we use to put load on the

393
00:14:28,839 --> 00:14:31,680
system the first point we're trying to

394
00:14:31,680 --> 00:14:34,160
make in our evaluation was that indeed

395
00:14:34,160 --> 00:14:36,160
by using macron you could build super

396
00:14:36,160 --> 00:14:37,360
fast

397
00:14:37,360 --> 00:14:40,399
distributed reliable services and so we

398
00:14:40,399 --> 00:14:43,519
used macron to implement a replicated

399
00:14:43,519 --> 00:14:45,279
key value store

400
00:14:45,279 --> 00:14:47,519
so what we did is that we took

401
00:14:47,519 --> 00:14:49,600
the state of the art for a key value

402
00:14:49,600 --> 00:14:52,160
store that that is heard

403
00:14:52,160 --> 00:14:54,800
and we compared our implementation our

404
00:14:54,800 --> 00:14:57,360
replication scheme that is this one here

405
00:14:57,360 --> 00:14:59,680
on the right with what you would get by

406
00:14:59,680 --> 00:15:02,000
replicating it with mu new being the

407
00:15:02,000 --> 00:15:05,040
fastest smr state machine replication uh

408
00:15:05,040 --> 00:15:06,880
system available that is tailored for

409
00:15:06,880 --> 00:15:08,639
rdma

410
00:15:08,639 --> 00:15:09,519
so

411
00:15:09,519 --> 00:15:10,399
uh

412
00:15:10,399 --> 00:15:13,120
in all so what we benchmarked was the

413
00:15:13,120 --> 00:15:15,920
latency of get requests of puts and also

414
00:15:15,920 --> 00:15:18,880
the failover of the system uh in what

415
00:15:18,880 --> 00:15:21,360
you can see is that for get requests we

416
00:15:21,360 --> 00:15:23,600
don't have much penalty

417
00:15:23,600 --> 00:15:25,279
between the version that is unreplicated

418
00:15:25,279 --> 00:15:28,000
of heard and the one that uses macron kv

419
00:15:28,000 --> 00:15:30,399
the reason why is that again we're using

420
00:15:30,399 --> 00:15:32,240
the the active method as described

421
00:15:32,240 --> 00:15:34,320
before which has very low overhead on

422
00:15:34,320 --> 00:15:36,720
the other hand when your replicates get

423
00:15:36,720 --> 00:15:38,240
i mean when you replicate the key value

424
00:15:38,240 --> 00:15:40,160
store using mu because it's doing state

425
00:15:40,160 --> 00:15:42,079
machine replication it has to replicate

426
00:15:42,079 --> 00:15:44,639
the get requests so you will have a lot

427
00:15:44,639 --> 00:15:46,800
of overhead and that's what you can see

428
00:15:46,800 --> 00:15:48,160
here

429
00:15:48,160 --> 00:15:50,800
then if we move to put requests

430
00:15:50,800 --> 00:15:54,480
we're not getting like that much uh

431
00:15:54,480 --> 00:15:56,320
performance and actually this is

432
00:15:56,320 --> 00:15:59,680
attributed to implementation details

433
00:15:59,680 --> 00:16:00,260
so

434
00:16:00,260 --> 00:16:01,519
[Music]

435
00:16:01,519 --> 00:16:04,320
so so you can assume that both systems

436
00:16:04,320 --> 00:16:05,759
work the same speed

437
00:16:05,759 --> 00:16:07,680
and finally for the failover here we

438
00:16:07,680 --> 00:16:10,320
have a huge huge difference between when

439
00:16:10,320 --> 00:16:12,079
replicated with new and when replicated

440
00:16:12,079 --> 00:16:15,600
using macaron the reason why is that for

441
00:16:15,600 --> 00:16:18,800
mu to be that fast it uses rdma

442
00:16:18,800 --> 00:16:21,199
permissions and setting permissions

443
00:16:21,199 --> 00:16:23,440
using rdma is a control path operation

444
00:16:23,440 --> 00:16:25,199
and control path operations they don't

445
00:16:25,199 --> 00:16:27,360
work at the microscope scale whereas in

446
00:16:27,360 --> 00:16:30,399
the case of mccarran well as well i've

447
00:16:30,399 --> 00:16:33,040
seen previously we can fail over in 50

448
00:16:33,040 --> 00:16:37,040
micros and that's what we get

449
00:16:37,040 --> 00:16:39,199
and so by using macaron you can be the

450
00:16:39,199 --> 00:16:41,680
state of the art

451
00:16:41,680 --> 00:16:44,079
then the second experiment that we have

452
00:16:44,079 --> 00:16:45,680
is like

453
00:16:45,680 --> 00:16:47,120
do they actually work those diseases

454
00:16:47,120 --> 00:16:48,720
that you were describing

455
00:16:48,720 --> 00:16:52,000
only lasting approximately 20 micros

456
00:16:52,000 --> 00:16:54,000
because maybe it sounds a bit too true

457
00:16:54,000 --> 00:16:54,959
to be

458
00:16:54,959 --> 00:16:56,720
too good to be territory

459
00:16:56,720 --> 00:16:58,800
so what we did in this experiment is

460
00:16:58,800 --> 00:17:01,600
that we tested all these our leases with

461
00:17:01,600 --> 00:17:04,640
different amount of network load and

462
00:17:04,640 --> 00:17:07,679
different amount of memory load and so

463
00:17:07,679 --> 00:17:09,520
we have four graphs

464
00:17:09,520 --> 00:17:13,199
and so the x-axis is different

465
00:17:13,199 --> 00:17:16,079
this is durations that we tried and on

466
00:17:16,079 --> 00:17:18,480
the y-axis what you can see is the

467
00:17:18,480 --> 00:17:20,880
proportion of requests or sorry of

468
00:17:20,880 --> 00:17:23,119
fleeces that were renewed on time

469
00:17:23,119 --> 00:17:25,839
and what you can see is that it doesn't

470
00:17:25,839 --> 00:17:28,960
matter the the network load as long as

471
00:17:28,960 --> 00:17:30,160
you're not

472
00:17:30,160 --> 00:17:32,480
like saturating the network you will be

473
00:17:32,480 --> 00:17:35,120
able to get your leases renewed in time

474
00:17:35,120 --> 00:17:37,440
and that's what you can see here with

475
00:17:37,440 --> 00:17:38,320
almost

476
00:17:38,320 --> 00:17:42,160
with the leases being super stable

477
00:17:42,160 --> 00:17:43,360
so

478
00:17:43,360 --> 00:17:45,360
that was it for macaron let me conclude

479
00:17:45,360 --> 00:17:46,640
very briefly

480
00:17:46,640 --> 00:17:48,799
so macron is a membership service for

481
00:17:48,799 --> 00:17:51,760
microsoft fast failover down to 50

482
00:17:51,760 --> 00:17:52,960
micros

483
00:17:52,960 --> 00:17:55,200
it's super easy to integrate it's really

484
00:17:55,200 --> 00:17:56,880
it's just this active method that you

485
00:17:56,880 --> 00:17:58,400
have to call at the right time depending

486
00:17:58,400 --> 00:18:01,440
on the semantics of your program

487
00:18:01,440 --> 00:18:03,440
given our implementation of this active

488
00:18:03,440 --> 00:18:05,360
method we have actually very low

489
00:18:05,360 --> 00:18:07,360
overhead

490
00:18:07,360 --> 00:18:09,520
so mccarron

491
00:18:09,520 --> 00:18:10,400
got

492
00:18:10,400 --> 00:18:12,960
went through the artifact review wall

493
00:18:12,960 --> 00:18:14,960
and it got both functional and

494
00:18:14,960 --> 00:18:17,679
reproduced unfortunately we're not able

495
00:18:17,679 --> 00:18:20,240
to make our guitar public in time to get

496
00:18:20,240 --> 00:18:22,400
the first batch but don't worry because

497
00:18:22,400 --> 00:18:24,480
the thing is now public so you can play

498
00:18:24,480 --> 00:18:26,320
with macron and it's still a prototype

499
00:18:26,320 --> 00:18:28,559
but you can absolutely use it to test

500
00:18:28,559 --> 00:18:31,039
your applications

501
00:18:31,039 --> 00:18:32,559
of course i couldn't go in all the

502
00:18:32,559 --> 00:18:34,559
details so be sure to check out our

503
00:18:34,559 --> 00:18:36,799
paper for more details

504
00:18:36,799 --> 00:18:38,559
and that was it for me and i will be

505
00:18:38,559 --> 00:18:43,320
glad to take your questions if any

