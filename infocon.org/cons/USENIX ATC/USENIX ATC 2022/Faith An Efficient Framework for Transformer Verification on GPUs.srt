1
00:00:12,800 --> 00:00:15,679
today i will present our paper face

2
00:00:15,679 --> 00:00:17,520
efficient framework for transformer

3
00:00:17,520 --> 00:00:21,279
replication on gpus

4
00:00:21,439 --> 00:00:23,199
transformer is an important type of

5
00:00:23,199 --> 00:00:24,640
neural network

6
00:00:24,640 --> 00:00:27,439
it has been widely deployed in many real

7
00:00:27,439 --> 00:00:29,439
world neutral language processing

8
00:00:29,439 --> 00:00:30,880
applications

9
00:00:30,880 --> 00:00:33,600
here i show example for sentiment

10
00:00:33,600 --> 00:00:35,200
analysis

11
00:00:35,200 --> 00:00:38,239
basically given a sentence transformer

12
00:00:38,239 --> 00:00:39,200
predicts

13
00:00:39,200 --> 00:00:42,160
whether a person is happy or not

14
00:00:42,160 --> 00:00:44,559
if i say i'm happy

15
00:00:44,559 --> 00:00:47,200
transformers should predict happy

16
00:00:47,200 --> 00:00:49,440
if i say i'm unhappy

17
00:00:49,440 --> 00:00:53,199
transformers should predict unhappy

18
00:00:53,199 --> 00:00:55,440
usually we expect transformers to

19
00:00:55,440 --> 00:00:57,760
predict the same sentiment when

20
00:00:57,760 --> 00:01:01,840
substituting the synonyms in a sentence

21
00:01:01,840 --> 00:01:03,680
for example

22
00:01:03,680 --> 00:01:07,520
frosty cold and frigid are synonyms

23
00:01:07,520 --> 00:01:09,600
so the transformer should have the same

24
00:01:09,600 --> 00:01:11,360
prediction

25
00:01:11,360 --> 00:01:14,159
let's say if transformer predict happy

26
00:01:14,159 --> 00:01:16,320
for foresty and code

27
00:01:16,320 --> 00:01:20,479
it should also predict happy for frigid

28
00:01:20,720 --> 00:01:22,960
however it's not

29
00:01:22,960 --> 00:01:24,640
the transformer may actually predict

30
00:01:24,640 --> 00:01:26,400
unhappy for frigid

31
00:01:26,400 --> 00:01:27,680
this is called

32
00:01:27,680 --> 00:01:30,400
synonym substitution attack

33
00:01:30,400 --> 00:01:32,960
it has been widely identified in many

34
00:01:32,960 --> 00:01:35,280
applications such as hate speech

35
00:01:35,280 --> 00:01:39,560
detection and question answering

36
00:01:40,079 --> 00:01:41,920
and transformer of education tackles

37
00:01:41,920 --> 00:01:44,400
this security concern

38
00:01:44,400 --> 00:01:46,399
for group of synonym

39
00:01:46,399 --> 00:01:48,159
transformer verification predicts a

40
00:01:48,159 --> 00:01:50,880
lower and upper bound for labels such as

41
00:01:50,880 --> 00:01:53,200
positive and negative

42
00:01:53,200 --> 00:01:55,439
if the lower bound of positive label is

43
00:01:55,439 --> 00:01:57,200
higher than the upper bound of negative

44
00:01:57,200 --> 00:02:00,320
label we know transformer is robust for

45
00:02:00,320 --> 00:02:03,279
these synonyms

46
00:02:03,759 --> 00:02:05,280
however

47
00:02:05,280 --> 00:02:07,600
transform verification actually leads to

48
00:02:07,600 --> 00:02:10,239
heavy performance challenge

49
00:02:10,239 --> 00:02:12,080
there is a second level latency of

50
00:02:12,080 --> 00:02:14,560
transformerification this is much higher

51
00:02:14,560 --> 00:02:16,560
than the mini second level latency of

52
00:02:16,560 --> 00:02:18,239
standard transformer

53
00:02:18,239 --> 00:02:18,959
so

54
00:02:18,959 --> 00:02:22,520
we need efficiency

55
00:02:23,280 --> 00:02:25,360
and we want to accelerate transform

56
00:02:25,360 --> 00:02:27,920
verification the key idea the key

57
00:02:27,920 --> 00:02:30,080
challenge is to support is unique

58
00:02:30,080 --> 00:02:32,000
computing pattern

59
00:02:32,000 --> 00:02:33,920
for example there is usually high

60
00:02:33,920 --> 00:02:36,560
irregularity and 50 of sparsity in

61
00:02:36,560 --> 00:02:39,519
transformation

62
00:02:39,760 --> 00:02:41,920
if we use dense computation

63
00:02:41,920 --> 00:02:44,480
there is heavy redundancy

64
00:02:44,480 --> 00:02:46,640
if we use sparse computations such as

65
00:02:46,640 --> 00:02:49,599
cool spots it is too dense

66
00:02:49,599 --> 00:02:51,360
so let's look closely at how

67
00:02:51,360 --> 00:02:54,640
transformerification works

68
00:02:56,000 --> 00:02:57,760
thanks to the water embedding in modern

69
00:02:57,760 --> 00:03:00,319
nlps synonyms usually have similar

70
00:03:00,319 --> 00:03:01,840
warning bearings

71
00:03:01,840 --> 00:03:04,640
so we can have an input bound on all

72
00:03:04,640 --> 00:03:06,959
synonyms

73
00:03:06,959 --> 00:03:09,280
then transformer verification propagates

74
00:03:09,280 --> 00:03:11,360
this input linear bond to output linear

75
00:03:11,360 --> 00:03:13,680
bond in a layer by layer style

76
00:03:13,680 --> 00:03:15,760
and we can check this output linear bond

77
00:03:15,760 --> 00:03:18,720
and see if it is if it makes different

78
00:03:18,720 --> 00:03:21,280
predictions

79
00:03:21,519 --> 00:03:23,440
let's look at a concrete example for

80
00:03:23,440 --> 00:03:25,200
linear layer

81
00:03:25,200 --> 00:03:25,920
in

82
00:03:25,920 --> 00:03:27,840
transform inference we have a concrete

83
00:03:27,840 --> 00:03:29,760
value for each neuron

84
00:03:29,760 --> 00:03:32,799
suppose we have a neuron x1 equals 3 and

85
00:03:32,799 --> 00:03:34,480
x2 equals 1.

86
00:03:34,480 --> 00:03:35,840
we can

87
00:03:35,840 --> 00:03:39,440
compute prediction y at 5.

88
00:03:39,440 --> 00:03:40,319
and in

89
00:03:40,319 --> 00:03:42,640
transformation we instead have a lower

90
00:03:42,640 --> 00:03:45,040
and upper bound for each neuron

91
00:03:45,040 --> 00:03:48,000
suppose we have x1 between 1 4 x 2

92
00:03:48,000 --> 00:03:52,319
between -2 and 4 we have y between -2

93
00:03:52,319 --> 00:03:54,319
and 10.

94
00:03:54,319 --> 00:03:57,040
note that x 2 needs to adjust the

95
00:03:57,040 --> 00:03:59,120
direction of inequality since we

96
00:03:59,120 --> 00:04:02,480
multiply x2 with -1

97
00:04:02,480 --> 00:04:04,720
so you can see this compute this

98
00:04:04,720 --> 00:04:07,840
computation shows high regularity and we

99
00:04:07,840 --> 00:04:09,760
need to adjust the direction of

100
00:04:09,760 --> 00:04:11,519
inequality according to the sign of

101
00:04:11,519 --> 00:04:13,040
individual neurons

102
00:04:13,040 --> 00:04:14,959
and this is harder to compute on

103
00:04:14,959 --> 00:04:19,199
parallel hardwares such as gpus

104
00:04:20,720 --> 00:04:22,320
there are several challenges to

105
00:04:22,320 --> 00:04:23,840
accelerate transform transform

106
00:04:23,840 --> 00:04:27,520
applications on gpus

107
00:04:27,520 --> 00:04:28,720
first

108
00:04:28,720 --> 00:04:30,960
existing uh transformerification

109
00:04:30,960 --> 00:04:32,800
framework utilizes a deep learning

110
00:04:32,800 --> 00:04:35,040
framework like pi torch which which is

111
00:04:35,040 --> 00:04:37,360
designed for the standard

112
00:04:37,360 --> 00:04:38,919
but

113
00:04:38,919 --> 00:04:41,360
transformerification shows significantly

114
00:04:41,360 --> 00:04:44,160
different bond centric computing pattern

115
00:04:44,160 --> 00:04:45,759
and straightforwardly deploying

116
00:04:45,759 --> 00:04:48,320
transformation to this existing deep

117
00:04:48,320 --> 00:04:50,240
learning framework usually lead to poor

118
00:04:50,240 --> 00:04:52,800
performance

119
00:04:53,199 --> 00:04:55,600
and second transformer verification

120
00:04:55,600 --> 00:04:57,759
shows large diversity in the bond

121
00:04:57,759 --> 00:04:59,120
computation

122
00:04:59,120 --> 00:05:01,440
and such diversity makes it challenging

123
00:05:01,440 --> 00:05:06,000
to hand optimize the gpu kernels

124
00:05:06,160 --> 00:05:07,039
third

125
00:05:07,039 --> 00:05:09,280
transform application involves abundant

126
00:05:09,280 --> 00:05:11,520
memory intensive operations such as

127
00:05:11,520 --> 00:05:12,880
reduction

128
00:05:12,880 --> 00:05:15,280
and existing deep learning framework

129
00:05:15,280 --> 00:05:17,840
usually only focus on the computation

130
00:05:17,840 --> 00:05:19,440
intensive operations

131
00:05:19,440 --> 00:05:21,919
such as convolutional layer and ignores

132
00:05:21,919 --> 00:05:24,000
abundant optimization opportunities for

133
00:05:24,000 --> 00:05:27,600
the memory intensive operators

134
00:05:28,160 --> 00:05:30,960
to this end we propose phase

135
00:05:30,960 --> 00:05:32,400
an efficient framework for

136
00:05:32,400 --> 00:05:35,840
transformerification on gpus

137
00:05:35,840 --> 00:05:38,880
phase have three major components

138
00:05:38,880 --> 00:05:40,560
schematic aware computation graph

139
00:05:40,560 --> 00:05:42,000
transformation

140
00:05:42,000 --> 00:05:45,039
verification specialized kernel crafter

141
00:05:45,039 --> 00:05:48,880
and the expert guided auto tuning

142
00:05:49,680 --> 00:05:52,960
the first technique is schematic aware

143
00:05:52,960 --> 00:05:56,638
computation graph transformation

144
00:05:58,320 --> 00:06:00,880
let's first look at existing design for

145
00:06:00,880 --> 00:06:02,800
transformerification

146
00:06:02,800 --> 00:06:04,639
the major problem is the intensive

147
00:06:04,639 --> 00:06:07,039
global memory assess

148
00:06:07,039 --> 00:06:09,680
existing designs usually first split the

149
00:06:09,680 --> 00:06:13,520
weight matrix w into two-way matrix w

150
00:06:13,520 --> 00:06:16,560
positive and w negative according to the

151
00:06:16,560 --> 00:06:18,240
weight signs

152
00:06:18,240 --> 00:06:21,120
then we can use each matrix for

153
00:06:21,120 --> 00:06:24,800
computing lower and upper bound

154
00:06:25,039 --> 00:06:26,160
however

155
00:06:26,160 --> 00:06:28,720
reading this matrix independently due to

156
00:06:28,720 --> 00:06:31,120
redundant global memory assets

157
00:06:31,120 --> 00:06:33,440
this heavy global memory assess will

158
00:06:33,440 --> 00:06:34,880
lead to

159
00:06:34,880 --> 00:06:37,759
high latency

160
00:06:39,280 --> 00:06:42,000
instead we propose thematic aware kernel

161
00:06:42,000 --> 00:06:45,680
fusion to minimize such memory overhead

162
00:06:45,680 --> 00:06:47,600
our key idea is to exploit

163
00:06:47,600 --> 00:06:50,080
transformation verification schematics

164
00:06:50,080 --> 00:06:53,758
and the gpu memory hierarchies

165
00:06:55,039 --> 00:06:57,039
as a weight computation level

166
00:06:57,039 --> 00:06:59,280
we first load the weight matrix into

167
00:06:59,280 --> 00:07:01,680
shared memory without distinguishing the

168
00:07:01,680 --> 00:07:05,199
sign of individual scalars

169
00:07:06,000 --> 00:07:09,759
then we split the width matrix w into

170
00:07:09,759 --> 00:07:12,560
w positive w negative when loading the

171
00:07:12,560 --> 00:07:14,840
data function memory to registers or

172
00:07:14,840 --> 00:07:18,560
registers in our design we only need to

173
00:07:18,560 --> 00:07:20,800
load the load the weight matrix from

174
00:07:20,800 --> 00:07:22,400
global memory once

175
00:07:22,400 --> 00:07:23,120
so

176
00:07:23,120 --> 00:07:25,680
we can significantly reduce the global

177
00:07:25,680 --> 00:07:29,160
memory size

178
00:07:30,240 --> 00:07:33,440
at the bond computation level

179
00:07:33,440 --> 00:07:35,440
transformer verification has important

180
00:07:35,440 --> 00:07:36,639
schematics

181
00:07:36,639 --> 00:07:38,639
multiplying the same weight matrix with

182
00:07:38,639 --> 00:07:40,960
both the lower and the upper input bound

183
00:07:40,960 --> 00:07:44,880
this is x lb and xub in the figure

184
00:07:44,880 --> 00:07:46,960
we propose to feel the computation of

185
00:07:46,960 --> 00:07:48,639
lower and upper bounds

186
00:07:48,639 --> 00:07:50,720
in this way we can load only once the

187
00:07:50,720 --> 00:07:53,759
lower and upper bounds

188
00:07:53,840 --> 00:07:55,280
specifically

189
00:07:55,280 --> 00:07:57,599
we first load the input bound into

190
00:07:57,599 --> 00:07:59,120
shared memory

191
00:07:59,120 --> 00:08:01,360
then each thread loads independent

192
00:08:01,360 --> 00:08:03,280
independent date from shaded memory to

193
00:08:03,280 --> 00:08:05,039
register

194
00:08:05,039 --> 00:08:06,400
finally

195
00:08:06,400 --> 00:08:09,360
we directly accumulate output bounds y

196
00:08:09,360 --> 00:08:12,319
lb and yub in the register

197
00:08:12,319 --> 00:08:14,720
this eliminates the redundant

198
00:08:14,720 --> 00:08:17,120
global memory assess during generating

199
00:08:17,120 --> 00:08:19,440
lower and upper bound of the output and

200
00:08:19,440 --> 00:08:22,879
further improves the performance

201
00:08:24,160 --> 00:08:26,720
our second technique is verification

202
00:08:26,720 --> 00:08:30,800
specialized kernel crafter

203
00:08:32,240 --> 00:08:34,320
one problem with accelerating transform

204
00:08:34,320 --> 00:08:36,479
application is the diversity across

205
00:08:36,479 --> 00:08:40,120
verification designs

206
00:08:40,240 --> 00:08:42,559
actually the computation is both

207
00:08:42,559 --> 00:08:45,040
adaptive to the improved bound and

208
00:08:45,040 --> 00:08:48,480
adaptive to the operators

209
00:08:48,800 --> 00:08:50,240
for example

210
00:08:50,240 --> 00:08:52,880
this figure shows the linear bound of a

211
00:08:52,880 --> 00:08:56,080
relu layer when when the scale of xl is

212
00:08:56,080 --> 00:08:57,760
larger than xu

213
00:08:57,760 --> 00:09:01,680
this design provides a tighter bond

214
00:09:02,560 --> 00:09:05,440
however when the scale of xl is smaller

215
00:09:05,440 --> 00:09:07,920
than xu another linear bond can provide

216
00:09:07,920 --> 00:09:10,320
a tighter bond

217
00:09:10,320 --> 00:09:13,120
so the computation of transformation is

218
00:09:13,120 --> 00:09:16,800
adaptive to the input bounce

219
00:09:17,120 --> 00:09:19,040
and the computation is also adaptive to

220
00:09:19,040 --> 00:09:22,080
the operators

221
00:09:22,080 --> 00:09:24,160
figure shows verification for the

222
00:09:24,160 --> 00:09:25,519
tangent layer

223
00:09:25,519 --> 00:09:27,279
you can easily see the linear bound

224
00:09:27,279 --> 00:09:29,120
voltage layer is quite different from

225
00:09:29,120 --> 00:09:32,560
the bound for real layer

226
00:09:32,560 --> 00:09:35,200
so there is large diversity across

227
00:09:35,200 --> 00:09:37,920
operators during replication it is quite

228
00:09:37,920 --> 00:09:39,040
hard to

229
00:09:39,040 --> 00:09:41,760
optimize individual operators due to

230
00:09:41,760 --> 00:09:44,640
this diversity

231
00:09:45,760 --> 00:09:47,920
our key insight is to optimize

232
00:09:47,920 --> 00:09:50,160
computation patterns instead instead of

233
00:09:50,160 --> 00:09:52,880
a concrete operator

234
00:09:52,880 --> 00:09:55,680
and we identify three important

235
00:09:55,680 --> 00:09:58,800
computation patterns in transformation

236
00:09:58,800 --> 00:10:02,640
they are generalized vector reduction

237
00:10:02,640 --> 00:10:06,240
generalized elementalized multiplication

238
00:10:06,240 --> 00:10:08,240
and generalized scalar vector

239
00:10:08,240 --> 00:10:10,959
multiplication

240
00:10:12,000 --> 00:10:15,279
let's look at a concrete example

241
00:10:15,279 --> 00:10:16,880
for generalized element-wise

242
00:10:16,880 --> 00:10:19,200
multiplication we have a generic

243
00:10:19,200 --> 00:10:22,079
function f over lower and upper bound

244
00:10:22,079 --> 00:10:24,160
this generic function f captures the

245
00:10:24,160 --> 00:10:27,279
diversity during verifying diversion

246
00:10:27,279 --> 00:10:29,279
operators

247
00:10:29,279 --> 00:10:32,399
moreover this function only takes two

248
00:10:32,399 --> 00:10:34,880
scalars as inputs and can be easily

249
00:10:34,880 --> 00:10:38,000
computed on a single gpu thread

250
00:10:38,000 --> 00:10:39,200
for example

251
00:10:39,200 --> 00:10:42,560
for redo layer and the text layer the

252
00:10:42,560 --> 00:10:44,720
only difference resides in the extension

253
00:10:44,720 --> 00:10:48,079
instantiation of different function f

254
00:10:48,079 --> 00:10:50,640
so we only need to optimize this

255
00:10:50,640 --> 00:10:52,320
computation pattern

256
00:10:52,320 --> 00:10:54,800
and the dtrf function designs can be

257
00:10:54,800 --> 00:10:57,279
abstracted

258
00:10:57,279 --> 00:10:59,440
next we only need to focus on optimizing

259
00:10:59,440 --> 00:11:02,320
individual computation patterns

260
00:11:02,320 --> 00:11:03,839
let's talk about

261
00:11:03,839 --> 00:11:08,399
general a generalized vector reduction

262
00:11:09,760 --> 00:11:11,839
this pattern widely exists

263
00:11:11,839 --> 00:11:14,800
exists when verifying various

264
00:11:14,800 --> 00:11:16,640
operators

265
00:11:16,640 --> 00:11:21,600
a naive approach is a sequential mode

266
00:11:22,720 --> 00:11:25,600
we can assign one thread to one neuron

267
00:11:25,600 --> 00:11:27,760
and we use it this right to accumulate

268
00:11:27,760 --> 00:11:29,839
32 values

269
00:11:29,839 --> 00:11:33,200
in this case we need 32 iterations

270
00:11:33,200 --> 00:11:35,519
so this design definitely shows low

271
00:11:35,519 --> 00:11:38,160
parallelism

272
00:11:38,240 --> 00:11:39,760
but in many cases

273
00:11:39,760 --> 00:11:42,000
there are much more threats than the

274
00:11:42,000 --> 00:11:43,920
number of neurons in a layer

275
00:11:43,920 --> 00:11:45,920
so we can have the luxury to use

276
00:11:45,920 --> 00:11:49,760
multiple threads for a single neuron

277
00:11:50,000 --> 00:11:52,399
we design a parallel mode to improve

278
00:11:52,399 --> 00:11:54,320
performance

279
00:11:54,320 --> 00:11:56,800
we utilize special gpu instruction

280
00:11:56,800 --> 00:11:59,839
shuffle down sync to enable 32 threads

281
00:11:59,839 --> 00:12:03,680
to simultaneously reduce a vector

282
00:12:03,680 --> 00:12:06,320
this enables computation directly

283
00:12:06,320 --> 00:12:08,000
across

284
00:12:08,000 --> 00:12:10,560
thread registers and in this case we

285
00:12:10,560 --> 00:12:12,880
only need five iterations and achieve

286
00:12:12,880 --> 00:12:15,920
high parallelism

287
00:12:18,160 --> 00:12:20,320
another problem is the heavy memory

288
00:12:20,320 --> 00:12:24,120
overhead during verification

289
00:12:24,720 --> 00:12:27,519
to solve this problem we propose sharing

290
00:12:27,519 --> 00:12:29,760
oriented workload scheduling

291
00:12:29,760 --> 00:12:32,800
and our key idea is to exploit the gpu

292
00:12:32,800 --> 00:12:35,519
memory hierarchy such as register share

293
00:12:35,519 --> 00:12:38,399
memory and global memory to effectively

294
00:12:38,399 --> 00:12:41,519
reduce memory assess

295
00:12:42,720 --> 00:12:45,279
first we use a set of thread for example

296
00:12:45,279 --> 00:12:48,880
32 thread to load input bound weight

297
00:12:48,880 --> 00:12:52,240
xlw and uw from global memory to shared

298
00:12:52,240 --> 00:12:54,000
memory

299
00:12:54,000 --> 00:12:55,760
then this thread load input boundary

300
00:12:55,760 --> 00:12:57,600
ways from shared memory and

301
00:12:57,600 --> 00:13:00,560
collaboratively compute the concretized

302
00:13:00,560 --> 00:13:02,320
lower and upper bound

303
00:13:02,320 --> 00:13:04,160
this concretized lower and upper bound

304
00:13:04,160 --> 00:13:06,240
are strong in shared memory which can be

305
00:13:06,240 --> 00:13:09,360
assessed by individual threats

306
00:13:09,360 --> 00:13:12,000
finally each thread independently load

307
00:13:12,000 --> 00:13:15,360
individual xlw and xuw scalars from the

308
00:13:15,360 --> 00:13:17,600
shared memory and then compute the

309
00:13:17,600 --> 00:13:21,120
output output bound weight

310
00:13:21,120 --> 00:13:23,360
here all threads in a whop are computing

311
00:13:23,360 --> 00:13:25,440
the output bound weight for the same

312
00:13:25,440 --> 00:13:28,000
neuron and applying the same

313
00:13:28,000 --> 00:13:30,959
scaling computation so we can avoid the

314
00:13:30,959 --> 00:13:32,959
warmth divergence

315
00:13:32,959 --> 00:13:35,440
and we also note that input bound weight

316
00:13:35,440 --> 00:13:37,760
are only loaded lens from global memory

317
00:13:37,760 --> 00:13:39,839
which mitigates the redundant global

318
00:13:39,839 --> 00:13:42,560
memory assess

319
00:13:43,200 --> 00:13:46,000
our third technique is expert guided

320
00:13:46,000 --> 00:13:49,639
auto tuning optimization

321
00:13:51,440 --> 00:13:53,839
our goal is to effectively incorporate

322
00:13:53,839 --> 00:13:56,320
hardware knowledge to find optimal

323
00:13:56,320 --> 00:13:58,720
operator implementations

324
00:13:58,720 --> 00:14:01,519
and our idea is to first generate a

325
00:14:01,519 --> 00:14:03,519
metal file for each hardware on its

326
00:14:03,519 --> 00:14:04,639
properties

327
00:14:04,639 --> 00:14:07,440
then we can incorporate this meta file

328
00:14:07,440 --> 00:14:08,959
to a custom model for tuning

329
00:14:08,959 --> 00:14:12,079
verification operators

330
00:14:12,160 --> 00:14:14,839
in particular we have two

331
00:14:14,839 --> 00:14:18,079
designs the first design is a rule based

332
00:14:18,079 --> 00:14:21,519
expert knowledge meta file

333
00:14:21,519 --> 00:14:23,839
it contains hard rules

334
00:14:23,839 --> 00:14:26,639
for hardware limitations such as

335
00:14:26,639 --> 00:14:28,959
maximum shared memory material memory

336
00:14:28,959 --> 00:14:31,279
size and the maximum number of registers

337
00:14:31,279 --> 00:14:32,480
per thread

338
00:14:32,480 --> 00:14:34,800
this represents the hardware limitations

339
00:14:34,800 --> 00:14:37,839
and must not be violated

340
00:14:37,839 --> 00:14:40,800
it also contains soft rules such as

341
00:14:40,800 --> 00:14:43,120
number of

342
00:14:43,120 --> 00:14:45,120
streaming multiprocessors and the number

343
00:14:45,120 --> 00:14:46,800
of threads per

344
00:14:46,800 --> 00:14:48,800
streaming multiprocessors

345
00:14:48,800 --> 00:14:50,639
and this represents the certain

346
00:14:50,639 --> 00:14:52,959
trade-off related to hardware properties

347
00:14:52,959 --> 00:14:55,440
we also need to consider the software

348
00:14:55,440 --> 00:14:58,800
rules when tuning the performance

349
00:14:58,800 --> 00:15:00,959
and the second design is expert guided

350
00:15:00,959 --> 00:15:03,120
cost model we first

351
00:15:03,120 --> 00:15:06,320
estimate shared memory and register

352
00:15:06,320 --> 00:15:09,199
usage and we can skip the candidates in

353
00:15:09,199 --> 00:15:11,440
candidate implementations that violates

354
00:15:11,440 --> 00:15:13,600
the hard rules

355
00:15:13,600 --> 00:15:16,800
then we can train a custom model this

356
00:15:16,800 --> 00:15:19,360
model consumes both soft rules for

357
00:15:19,360 --> 00:15:21,680
hardware properties and tuning

358
00:15:21,680 --> 00:15:24,560
uh performance-related parameters such

359
00:15:24,560 --> 00:15:26,399
as tiling sites

360
00:15:26,399 --> 00:15:27,440
finally

361
00:15:27,440 --> 00:15:29,199
we can predict

362
00:15:29,199 --> 00:15:32,880
a best candidate implementation

363
00:15:34,399 --> 00:15:38,000
now let's see the evaluation

364
00:15:38,880 --> 00:15:40,320
let's first talk about end-to-end

365
00:15:40,320 --> 00:15:41,360
benefits

366
00:15:41,360 --> 00:15:43,360
we compiled a phase with pi torch which

367
00:15:43,360 --> 00:15:44,560
is mostly

368
00:15:44,560 --> 00:15:45,760
uh used

369
00:15:45,760 --> 00:15:48,079
used in transformerification we also

370
00:15:48,079 --> 00:15:50,399
compile with tvm answer

371
00:15:50,399 --> 00:15:51,839
and answer as

372
00:15:51,839 --> 00:15:54,399
representation of existing deep learning

373
00:15:54,399 --> 00:15:55,600
frameworks

374
00:15:55,600 --> 00:15:57,120
as showing figure

375
00:15:57,120 --> 00:16:00,320
phase is much faster than existing works

376
00:16:00,320 --> 00:16:03,360
phase achieves around 2.5 times speed up

377
00:16:03,360 --> 00:16:04,800
over pi torch

378
00:16:04,800 --> 00:16:07,279
and surprisingly we know that

379
00:16:07,279 --> 00:16:10,480
tvm and answer even bring slow down to

380
00:16:10,480 --> 00:16:12,160
transformer verification

381
00:16:12,160 --> 00:16:14,639
the main reason is that tvm and ansar

382
00:16:14,639 --> 00:16:16,240
are designed for standard neural

383
00:16:16,240 --> 00:16:18,959
networks and failed to effect

384
00:16:18,959 --> 00:16:20,720
efficiently support verification

385
00:16:20,720 --> 00:16:24,320
specific communication pattern

386
00:16:25,120 --> 00:16:28,240
on individual layers we can also observe

387
00:16:28,240 --> 00:16:30,480
similar performance

388
00:16:30,480 --> 00:16:33,279
a similar performance improvement

389
00:16:33,279 --> 00:16:35,440
in this figure we compile the

390
00:16:35,440 --> 00:16:36,959
verification time of matrix

391
00:16:36,959 --> 00:16:39,600
multiplication layer over the

392
00:16:39,600 --> 00:16:41,040
diverse

393
00:16:41,040 --> 00:16:43,040
input lens however

394
00:16:43,040 --> 00:16:46,959
we overall we observe 5.1 times speed up

395
00:16:46,959 --> 00:16:49,440
on average over pi charge baseline and

396
00:16:49,440 --> 00:16:53,839
we also outperformed the tvm and answer

397
00:16:53,920 --> 00:16:55,839
um that's all for my presentation thanks

398
00:16:55,839 --> 00:16:57,839
for attention our projects have been

399
00:16:57,839 --> 00:17:00,959
open source at this link and preview

400
00:17:00,959 --> 00:17:02,639
please feel free to check out this

401
00:17:02,639 --> 00:17:05,839
report thank you

