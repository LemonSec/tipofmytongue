1
00:00:13,679 --> 00:00:16,640
hello everyone i'm zukul wang from zhu

2
00:00:16,640 --> 00:00:19,600
zhang university china today it's my

3
00:00:19,600 --> 00:00:22,560
pleasure to talk about our recent work

4
00:00:22,560 --> 00:00:24,320
fpga nick

5
00:00:24,320 --> 00:00:27,920
upgenic is a gpu centrally cosmetic

6
00:00:27,920 --> 00:00:29,599
it's a joint work

7
00:00:29,599 --> 00:00:32,000
with homogeneous

8
00:00:32,000 --> 00:00:34,640
gustavo

9
00:00:34,880 --> 00:00:37,360
this is our outline first we will talk

10
00:00:37,360 --> 00:00:38,800
about why

11
00:00:38,800 --> 00:00:42,440
fpga nick

12
00:00:44,879 --> 00:00:48,960
essentially fpga nick is an fpga-based

13
00:00:48,960 --> 00:00:52,719
gpu-centric semanic for gpu powered

14
00:00:52,719 --> 00:00:55,120
distributed applications

15
00:00:55,120 --> 00:00:57,920
so the question why aphgenic can be

16
00:00:57,920 --> 00:01:00,640
breakdown into two questions the first

17
00:01:00,640 --> 00:01:03,680
one is the why gpu-centric symmetric the

18
00:01:03,680 --> 00:01:08,880
second one is why fpga-based semanic

19
00:01:08,880 --> 00:01:11,520
why gpu-centric symmetric the main

20
00:01:11,520 --> 00:01:14,240
reason is that gpu's position don't

21
00:01:14,240 --> 00:01:16,720
match is low in hpc and the ai

22
00:01:16,720 --> 00:01:17,840
application

23
00:01:17,840 --> 00:01:21,200
on one hand gpu consume most network

24
00:01:21,200 --> 00:01:24,560
traffic in hpc and the ai application

25
00:01:24,560 --> 00:01:25,600
and

26
00:01:25,600 --> 00:01:28,720
this application has larger and larger

27
00:01:28,720 --> 00:01:31,280
data size

28
00:01:31,280 --> 00:01:33,840
on the other hand gpu's positioning in

29
00:01:33,840 --> 00:01:38,560
the server is just a work of his boss

30
00:01:39,119 --> 00:01:39,840
um cpu

31
00:01:39,840 --> 00:01:44,960
cpu manipulates both nic and gpu

32
00:01:44,960 --> 00:01:46,880
such an architecture

33
00:01:46,880 --> 00:01:49,200
doesn't allow fan grained core

34
00:01:49,200 --> 00:01:52,560
processing between nick and the gpu

35
00:01:52,560 --> 00:01:56,079
therefore we believe that gpu dream to

36
00:01:56,079 --> 00:01:59,280
be the boss of network data rather than

37
00:01:59,280 --> 00:02:03,360
just the work of the current boss cpu

38
00:02:03,360 --> 00:02:06,719
while fpga based manic the main reasons

39
00:02:06,719 --> 00:02:09,758
is that the arms based on the ac based

40
00:02:09,758 --> 00:02:11,920
money cannot always meet the two goals

41
00:02:11,920 --> 00:02:13,360
concurrently

42
00:02:13,360 --> 00:02:14,319
program

43
00:02:14,319 --> 00:02:17,520
ability and the performance

44
00:02:17,520 --> 00:02:20,400
on the arm-based manic the arm cpu and

45
00:02:20,400 --> 00:02:23,520
the host cpu are connected to a nic

46
00:02:23,520 --> 00:02:24,720
switch

47
00:02:24,720 --> 00:02:27,360
therefore the arm cpu can do the smart

48
00:02:27,360 --> 00:02:29,680
things on the neck

49
00:02:29,680 --> 00:02:32,640
however the main bottleneck is that the

50
00:02:32,640 --> 00:02:34,560
amp cpu have limit

51
00:02:34,560 --> 00:02:37,519
memory bandwidth

52
00:02:37,519 --> 00:02:40,959
typically 28 gigabytes per second

53
00:02:40,959 --> 00:02:43,519
because the arm cpu typically only have

54
00:02:43,519 --> 00:02:45,440
two memory channel

55
00:02:45,440 --> 00:02:46,800
therefore

56
00:02:46,800 --> 00:02:49,599
staging 100g network data

57
00:02:49,599 --> 00:02:52,640
will overwhelm the arm cpu

58
00:02:52,640 --> 00:02:54,239
because of his

59
00:02:54,239 --> 00:02:57,280
low memory bandwidth

60
00:02:57,280 --> 00:02:59,519
regarding the

61
00:02:59,519 --> 00:03:02,640
basic base manic

62
00:03:02,640 --> 00:03:04,400
there is no

63
00:03:04,400 --> 00:03:06,239
general architecture for various

64
00:03:06,239 --> 00:03:07,519
application

65
00:03:07,519 --> 00:03:10,640
so we have to build a

66
00:03:10,640 --> 00:03:11,760
custom

67
00:03:11,760 --> 00:03:15,360
chip for a narrow space of application

68
00:03:15,360 --> 00:03:17,760
however

69
00:03:17,840 --> 00:03:20,720
each chip type out take a long cycle

70
00:03:20,720 --> 00:03:22,800
typical years

71
00:03:22,800 --> 00:03:25,120
and each chip type out

72
00:03:25,120 --> 00:03:27,920
cost higher and the higher

73
00:03:27,920 --> 00:03:30,879
when used a better process

74
00:03:30,879 --> 00:03:33,920
therefore only big company like cloud

75
00:03:33,920 --> 00:03:35,680
companies

76
00:03:35,680 --> 00:03:39,120
can have his own semantic chip

77
00:03:39,120 --> 00:03:40,560
because they

78
00:03:40,560 --> 00:03:42,400
have a big

79
00:03:42,400 --> 00:03:44,560
big market

80
00:03:44,560 --> 00:03:49,200
now let's talk about what's fpg nick

81
00:03:50,000 --> 00:03:53,840
two goals of epigenetic are gpu-centric

82
00:03:53,840 --> 00:03:58,239
somatic and versatility which means that

83
00:03:58,239 --> 00:03:59,920
epigenetic allows

84
00:03:59,920 --> 00:04:02,720
flexible design space exploration around

85
00:04:02,720 --> 00:04:04,080
smanik

86
00:04:04,080 --> 00:04:06,319
to achieve these two goals

87
00:04:06,319 --> 00:04:09,599
fgniq consists of three components

88
00:04:09,599 --> 00:04:13,120
the first component is gpu communication

89
00:04:13,120 --> 00:04:14,720
stack

90
00:04:14,720 --> 00:04:18,160
this stack enables control and the data

91
00:04:18,160 --> 00:04:19,839
plan of loading

92
00:04:19,839 --> 00:04:23,520
next i will talk about the effect of

93
00:04:23,520 --> 00:04:25,840
these two of loading

94
00:04:25,840 --> 00:04:28,560
now let's talk about the effect of data

95
00:04:28,560 --> 00:04:31,520
plan of loading

96
00:04:32,160 --> 00:04:34,080
we compare the data plan of loading with

97
00:04:34,080 --> 00:04:35,680
the case without the com

98
00:04:35,680 --> 00:04:38,400
data bank of loading

99
00:04:38,400 --> 00:04:41,680
when the data pane uploading is disabled

100
00:04:41,680 --> 00:04:45,360
when we send data from nick to the gpu

101
00:04:45,360 --> 00:04:47,759
the first thing we need to send the data

102
00:04:47,759 --> 00:04:48,720
from

103
00:04:48,720 --> 00:04:53,199
nick to the cpu and then cpu to the gpu

104
00:04:53,199 --> 00:04:57,759
so the data path will be like this

105
00:04:57,759 --> 00:05:00,479
with data plane offloading enabled the

106
00:05:00,479 --> 00:05:02,720
data can go directly from

107
00:05:02,720 --> 00:05:06,560
fpga nic to the gpu

108
00:05:06,560 --> 00:05:09,120
besides

109
00:05:09,360 --> 00:05:10,960
fgnic

110
00:05:10,960 --> 00:05:14,160
allows appgi to use gpu virtual address

111
00:05:14,160 --> 00:05:17,440
to access gpu memory

112
00:05:17,440 --> 00:05:20,560
such that a system programmer can

113
00:05:20,560 --> 00:05:21,680
use

114
00:05:21,680 --> 00:05:24,240
easy fine-grained core co-processing

115
00:05:24,240 --> 00:05:26,800
between fpga and the gpu

116
00:05:26,800 --> 00:05:29,440
we don't decide that appginik is the

117
00:05:29,440 --> 00:05:32,479
first work to enable p2p communication

118
00:05:32,479 --> 00:05:35,600
between fpga and gpu but to our

119
00:05:35,600 --> 00:05:37,360
knowledge

120
00:05:37,360 --> 00:05:39,440
we believe that

121
00:05:39,440 --> 00:05:43,039
fcnic is the first worker to allow fg to

122
00:05:43,039 --> 00:05:48,159
use virtual address to access gpu memory

123
00:05:48,320 --> 00:05:50,960
now let's talk about the effect of

124
00:05:50,960 --> 00:05:53,360
control of loading

125
00:05:53,360 --> 00:05:56,080
using one application scenario

126
00:05:56,080 --> 00:05:59,600
processing network data on the gpu

127
00:05:59,600 --> 00:06:01,680
when the control pane of loading is not

128
00:06:01,680 --> 00:06:06,720
enabled the cpu needs to issue and set

129
00:06:06,720 --> 00:06:10,479
receive commands to the nic to transfer

130
00:06:10,479 --> 00:06:15,120
data from nic to the gpu memory

131
00:06:15,120 --> 00:06:19,280
second cpu need to launch a gpu kernel

132
00:06:19,280 --> 00:06:20,639
to

133
00:06:20,639 --> 00:06:25,280
process on the network data

134
00:06:25,440 --> 00:06:26,639
and wait

135
00:06:26,639 --> 00:06:27,680
until

136
00:06:27,680 --> 00:06:30,000
the kernel finishes

137
00:06:30,000 --> 00:06:33,440
third cpu need to issue and send the

138
00:06:33,440 --> 00:06:36,319
commands to the nic to transfer data

139
00:06:36,319 --> 00:06:39,680
from gpu memory to the nic

140
00:06:39,680 --> 00:06:41,280
to sum up

141
00:06:41,280 --> 00:06:42,800
this scenario

142
00:06:42,800 --> 00:06:44,560
needs three

143
00:06:44,560 --> 00:06:47,280
pcie commands from the cpu

144
00:06:47,280 --> 00:06:49,600
second

145
00:06:49,600 --> 00:06:52,240
nic and gpu operations are hard to

146
00:06:52,240 --> 00:06:56,880
parallel actually they are serialized

147
00:06:57,600 --> 00:07:00,240
with control panel of loading enabled

148
00:07:00,240 --> 00:07:03,840
gpu can directly manipulate itself nick

149
00:07:03,840 --> 00:07:05,120
in particular

150
00:07:05,120 --> 00:07:08,000
gpu can

151
00:07:08,000 --> 00:07:09,520
send a

152
00:07:09,520 --> 00:07:11,599
receive command to the nic

153
00:07:11,599 --> 00:07:14,319
to transfer data from nic to the gpu

154
00:07:14,319 --> 00:07:15,759
memory

155
00:07:15,759 --> 00:07:17,680
send

156
00:07:17,680 --> 00:07:21,599
gpu can directly process on the

157
00:07:21,599 --> 00:07:23,520
data

158
00:07:23,520 --> 00:07:24,560
when the

159
00:07:24,560 --> 00:07:26,800
when the processing is finished

160
00:07:26,800 --> 00:07:30,639
gpu can issue and

161
00:07:30,639 --> 00:07:32,800
send commands to the nic to transfer

162
00:07:32,800 --> 00:07:37,120
data from gpu memory to the nic

163
00:07:37,919 --> 00:07:40,800
to sum up

164
00:07:40,800 --> 00:07:43,840
control panel of loading only needs to

165
00:07:43,840 --> 00:07:45,120
control

166
00:07:45,120 --> 00:07:47,199
commands from gpu

167
00:07:47,199 --> 00:07:48,800
so pcie

168
00:07:48,800 --> 00:07:51,599
and nick and the gpu operations can be

169
00:07:51,599 --> 00:07:54,240
easily paralyzed

170
00:07:54,240 --> 00:07:57,680
the second component is hardwood network

171
00:07:57,680 --> 00:08:00,400
transport which enable fast

172
00:08:00,400 --> 00:08:03,039
lossless network processing with

173
00:08:03,039 --> 00:08:06,080
hardware network of transport gpu don't

174
00:08:06,080 --> 00:08:09,520
need to deal with complex network

175
00:08:09,520 --> 00:08:12,240
protocol on which gpu cannot perform as

176
00:08:12,240 --> 00:08:16,879
well because gpu is original design for

177
00:08:16,879 --> 00:08:19,280
parallel computing rather than

178
00:08:19,280 --> 00:08:22,319
network protocol which needs to

179
00:08:22,319 --> 00:08:23,919
maintain states

180
00:08:23,919 --> 00:08:25,599
as such

181
00:08:25,599 --> 00:08:28,800
the gpu only needs to process the

182
00:08:28,800 --> 00:08:31,759
network payload rather than law

183
00:08:31,759 --> 00:08:34,000
network package

184
00:08:34,000 --> 00:08:35,519
our current

185
00:08:35,519 --> 00:08:39,200
network transport is from eth it's an

186
00:08:39,200 --> 00:08:43,039
open source 100g tgb stack

187
00:08:43,039 --> 00:08:45,440
nevertheless it's easy to generalize to

188
00:08:45,440 --> 00:08:46,720
use

189
00:08:46,720 --> 00:08:49,200
lockkey

190
00:08:49,440 --> 00:08:52,720
the third component is only computing

191
00:08:52,720 --> 00:08:55,920
which enable flexible 100g data pass

192
00:08:55,920 --> 00:08:57,680
accelerator

193
00:08:57,680 --> 00:09:00,720
which cannot be done on the asic based

194
00:09:00,720 --> 00:09:02,880
and arm-based hismanic

195
00:09:02,880 --> 00:09:07,360
the arm-based money cannot process 100g

196
00:09:07,360 --> 00:09:09,120
network data

197
00:09:09,120 --> 00:09:11,920
and the asic-based semanic

198
00:09:11,920 --> 00:09:16,000
cannot provide flexibility

199
00:09:16,320 --> 00:09:19,760
the goal of omnic computing components

200
00:09:19,760 --> 00:09:22,959
is to enable versatility which means

201
00:09:22,959 --> 00:09:26,800
that fpga need to support a flexible

202
00:09:26,800 --> 00:09:30,240
100g that pass oscillator

203
00:09:30,240 --> 00:09:32,240
in order to increase

204
00:09:32,240 --> 00:09:33,680
versatility

205
00:09:33,680 --> 00:09:36,320
fj nick

206
00:09:36,320 --> 00:09:40,640
also supports three asthmatic models

207
00:09:40,640 --> 00:09:41,760
neglect

208
00:09:41,760 --> 00:09:45,040
of past and own paths

209
00:09:45,040 --> 00:09:48,959
the key idea of directory model is to

210
00:09:48,959 --> 00:09:52,240
directly connect this gpu communication

211
00:09:52,240 --> 00:09:53,360
state with

212
00:09:53,360 --> 00:09:56,800
hardware network transport for example

213
00:09:56,800 --> 00:10:01,279
we implement gpu central network

214
00:10:02,320 --> 00:10:05,680
which allows gpu to issue

215
00:10:05,680 --> 00:10:08,720
circuit-like network communication with

216
00:10:08,720 --> 00:10:11,040
remote gpu

217
00:10:11,040 --> 00:10:14,640
the second model is off-pass model the

218
00:10:14,640 --> 00:10:15,920
key idea

219
00:10:15,920 --> 00:10:17,760
of path model is

220
00:10:17,760 --> 00:10:18,560
to

221
00:10:18,560 --> 00:10:22,320
allow only computing model to directly

222
00:10:22,320 --> 00:10:25,120
manipulate gpu communication stack and

223
00:10:25,120 --> 00:10:27,519
hardware transport for example we

224
00:10:27,519 --> 00:10:30,560
implement only this operation on the

225
00:10:30,560 --> 00:10:33,200
only computer model

226
00:10:33,200 --> 00:10:34,560
such that

227
00:10:34,560 --> 00:10:36,000
we don't need

228
00:10:36,000 --> 00:10:37,440
any

229
00:10:37,440 --> 00:10:40,480
cpu and gpu computing cycle and

230
00:10:40,480 --> 00:10:44,320
don't increase memory footprint

231
00:10:44,320 --> 00:10:48,480
the third model is on pass model

232
00:10:48,480 --> 00:10:51,120
the key idea of onpass model

233
00:10:51,120 --> 00:10:54,800
is allow omnic computing model to act as

234
00:10:54,800 --> 00:10:57,839
a bond between gpu communication stack

235
00:10:57,839 --> 00:11:00,560
and the hardware transport for example

236
00:11:00,560 --> 00:11:03,760
we implement hyperlog log

237
00:11:03,760 --> 00:11:04,800
on

238
00:11:04,800 --> 00:11:07,600
on the unpassed model

239
00:11:07,600 --> 00:11:11,200
such that the cardinality estimation is

240
00:11:11,200 --> 00:11:14,240
being performed when transfer data

241
00:11:14,240 --> 00:11:18,160
between gpu and the network

242
00:11:18,160 --> 00:11:21,440
with these three components fcnic can be

243
00:11:21,440 --> 00:11:24,800
a gpu sensors manic and enable

244
00:11:24,800 --> 00:11:29,279
versatility around this monique for gpu

245
00:11:29,279 --> 00:11:32,240
now let's see the location of epigenetic

246
00:11:32,240 --> 00:11:34,560
in our system

247
00:11:34,560 --> 00:11:36,000
epsilonic

248
00:11:36,000 --> 00:11:40,160
acts as a pcie in the point such that

249
00:11:40,160 --> 00:11:44,000
fgn can direct communication with gpu

250
00:11:44,000 --> 00:11:47,839
without cpu being involved

251
00:11:47,920 --> 00:11:52,640
now let's talk about how fcnik performs

252
00:11:52,640 --> 00:11:55,120
at the beginning we talk about our

253
00:11:55,120 --> 00:11:57,839
experimental setup we learned our

254
00:11:57,839 --> 00:12:01,040
experiment in a small cast that consists

255
00:12:01,040 --> 00:12:04,320
of eight servers each server has a

256
00:12:04,320 --> 00:12:08,079
sonics fpga and an nvidia gpu

257
00:12:08,079 --> 00:12:10,240
we implement

258
00:12:10,240 --> 00:12:16,000
upgenic in sonic's fpga u280 or u50

259
00:12:16,000 --> 00:12:18,880
as the gpu is for the nvidia

260
00:12:18,880 --> 00:12:23,120
a100 or rtx eight dedicated

261
00:12:23,120 --> 00:12:27,040
so go our experiment is twofold first we

262
00:12:27,040 --> 00:12:30,480
validate fpga nick's functionality

263
00:12:30,480 --> 00:12:34,800
second we validate fgnique's versatility

264
00:12:34,800 --> 00:12:38,800
regarding fgnicus functionality we

265
00:12:38,800 --> 00:12:41,440
validate the latency and throughput

266
00:12:41,440 --> 00:12:44,320
between apigee unique and the gpu

267
00:12:44,320 --> 00:12:48,480
regarding fgnics versatility we examine

268
00:12:48,480 --> 00:12:49,839
the

269
00:12:49,839 --> 00:12:52,959
performance of directory model of path

270
00:12:52,959 --> 00:12:56,000
model and the own path model

271
00:12:56,000 --> 00:12:58,639
now let's validate the fpga nexus

272
00:12:58,639 --> 00:13:01,639
functionality

273
00:13:01,760 --> 00:13:05,279
regarding fgnix functionality

274
00:13:05,279 --> 00:13:08,000
we validated the effect of control panel

275
00:13:08,000 --> 00:13:11,200
of loading and the data plan of loading

276
00:13:11,200 --> 00:13:14,320
to examine the control plan of loading

277
00:13:14,320 --> 00:13:16,560
we compare the latency between different

278
00:13:16,560 --> 00:13:18,959
device

279
00:13:18,959 --> 00:13:22,320
xo the left fig shows the latency

280
00:13:22,320 --> 00:13:23,600
comparison

281
00:13:23,600 --> 00:13:27,360
where the x y means device x leads from

282
00:13:27,360 --> 00:13:29,360
device y

283
00:13:29,360 --> 00:13:31,519
we have two observations

284
00:13:31,519 --> 00:13:32,560
first

285
00:13:32,560 --> 00:13:36,160
gpu ft has stable latency

286
00:13:36,160 --> 00:13:37,279
because

287
00:13:37,279 --> 00:13:40,160
gpu fpga has less noise

288
00:13:40,160 --> 00:13:42,240
compared with cpu

289
00:13:42,240 --> 00:13:43,920
second

290
00:13:43,920 --> 00:13:46,240
gpu fgs latency is

291
00:13:46,240 --> 00:13:50,000
much smaller than the sum of gpu cpu

292
00:13:50,000 --> 00:13:52,880
cpu fpga latency

293
00:13:52,880 --> 00:13:54,560
which means that

294
00:13:54,560 --> 00:13:57,199
our control panel of loading works very

295
00:13:57,199 --> 00:13:59,439
well

296
00:13:59,680 --> 00:14:02,720
to to validate the effect of data of

297
00:14:02,720 --> 00:14:05,360
loading we match the throughput

298
00:14:05,360 --> 00:14:07,920
between two device

299
00:14:07,920 --> 00:14:10,399
especially between the fpga to

300
00:14:10,399 --> 00:14:11,839
cpu

301
00:14:11,839 --> 00:14:14,079
or gpu

302
00:14:14,079 --> 00:14:17,199
we can observe that fgnic can achieve

303
00:14:17,199 --> 00:14:19,839
similar throughput when assessing gpu

304
00:14:19,839 --> 00:14:22,480
memory and the cpu memory which means

305
00:14:22,480 --> 00:14:23,199
that

306
00:14:23,199 --> 00:14:27,920
means efficient of data plan of loading

307
00:14:27,920 --> 00:14:30,720
now let's validate the fpg unique

308
00:14:30,720 --> 00:14:35,120
versatility using 3 models

309
00:14:35,199 --> 00:14:38,240
regarding the directory model we use gpu

310
00:14:38,240 --> 00:14:41,760
central networking as an example

311
00:14:41,760 --> 00:14:44,160
with gpu central networking

312
00:14:44,160 --> 00:14:46,800
distributed gpu can communicate with

313
00:14:46,800 --> 00:14:50,959
each other using socket-like interface

314
00:14:50,959 --> 00:14:53,600
the baseline we choose is a case

315
00:14:53,600 --> 00:14:56,560
without control panel of loading like

316
00:14:56,560 --> 00:14:59,440
gpu net

317
00:14:59,440 --> 00:15:01,199
in such a case

318
00:15:01,199 --> 00:15:04,240
cpu manipulates a link so it's hard to

319
00:15:04,240 --> 00:15:07,920
parallel nic and gpu operations

320
00:15:07,920 --> 00:15:09,920
with fpg nic

321
00:15:09,920 --> 00:15:13,199
control panel offloading is enabled

322
00:15:13,199 --> 00:15:16,320
gpu can directly manipulate the nic

323
00:15:16,320 --> 00:15:19,199
so it allows fine-grained core

324
00:15:19,199 --> 00:15:22,480
processing between nic and gpu

325
00:15:22,480 --> 00:15:25,199
we validate the effect of control

326
00:15:25,199 --> 00:15:28,399
overloading using different network

327
00:15:28,399 --> 00:15:31,600
transfer lengths from 2 megabytes to 512

328
00:15:31,600 --> 00:15:33,360
megabytes

329
00:15:33,360 --> 00:15:36,240
as shown in xs

330
00:15:36,240 --> 00:15:38,720
we have two observations

331
00:15:38,720 --> 00:15:39,680
first

332
00:15:39,680 --> 00:15:41,040
control panel of loading is

333
00:15:41,040 --> 00:15:43,279
significantly increased throughput no

334
00:15:43,279 --> 00:15:45,600
matter the transfer length is

335
00:15:45,600 --> 00:15:48,000
because control panel of loading

336
00:15:48,000 --> 00:15:51,199
can easily overlap the nic and the gpu

337
00:15:51,199 --> 00:15:52,720
operations

338
00:15:52,720 --> 00:15:56,000
second when the transfer lens is smaller

339
00:15:56,000 --> 00:15:58,320
control panel of loading gets higher

340
00:15:58,320 --> 00:15:59,600
speed up

341
00:15:59,600 --> 00:16:01,040
because

342
00:16:01,040 --> 00:16:03,920
the valid transfer time is limited and

343
00:16:03,920 --> 00:16:04,959
the zen

344
00:16:04,959 --> 00:16:07,920
control operation dominates the overall

345
00:16:07,920 --> 00:16:10,240
time

346
00:16:10,880 --> 00:16:13,839
regarding of pass model we use only use

347
00:16:13,839 --> 00:16:15,759
as an example

348
00:16:15,759 --> 00:16:18,399
the baseline we choose is nickel from

349
00:16:18,399 --> 00:16:19,839
nvidia

350
00:16:19,839 --> 00:16:22,880
we enable rdma and the gpu directly for

351
00:16:22,880 --> 00:16:24,880
the nickel

352
00:16:24,880 --> 00:16:27,759
however nico still needs cpu or gpu

353
00:16:27,759 --> 00:16:29,440
computing cycles

354
00:16:29,440 --> 00:16:32,000
to finish all of this operation

355
00:16:32,000 --> 00:16:35,759
and the nico also needs more gpu memory

356
00:16:35,759 --> 00:16:38,240
footprint for storing intermediate

357
00:16:38,240 --> 00:16:39,920
states

358
00:16:39,920 --> 00:16:40,880
however

359
00:16:40,880 --> 00:16:43,279
fgnic enhanced or reduced have pure

360
00:16:43,279 --> 00:16:46,480
hardware implementation so it doesn't

361
00:16:46,480 --> 00:16:50,399
add any cpu computing cycle and done at

362
00:16:50,399 --> 00:16:52,959
gpu memory footprint for storing

363
00:16:52,959 --> 00:16:55,040
intermediate states

364
00:16:55,040 --> 00:16:58,079
we run our experiments on eight nodes

365
00:16:58,079 --> 00:17:00,320
each nodes had

366
00:17:00,320 --> 00:17:03,279
an fda and gpu

367
00:17:03,279 --> 00:17:06,000
the results shown in the figure

368
00:17:06,000 --> 00:17:08,000
excess is

369
00:17:08,000 --> 00:17:10,959
that size where exercise is a past

370
00:17:10,959 --> 00:17:12,959
bandwidth

371
00:17:12,959 --> 00:17:15,760
we have two observations

372
00:17:15,760 --> 00:17:17,199
first

373
00:17:17,199 --> 00:17:19,599
app generic enhanced or reduced leash

374
00:17:19,599 --> 00:17:22,640
cylindrical bus binaries as nickel when

375
00:17:22,640 --> 00:17:26,160
the size is larger than two megabytes

376
00:17:26,160 --> 00:17:29,760
second lg nic based audios achieve

377
00:17:29,760 --> 00:17:32,320
higher speed up over nickel when third

378
00:17:32,320 --> 00:17:36,240
size is less than one megabytes because

379
00:17:36,240 --> 00:17:37,360
nickel has

380
00:17:37,360 --> 00:17:39,640
high tone a lot of head than

381
00:17:39,640 --> 00:17:40,799
[Music]

382
00:17:40,799 --> 00:17:44,480
fg nick based all dues

383
00:17:45,120 --> 00:17:47,360
regarding the onpass model we use

384
00:17:47,360 --> 00:17:50,799
hyperloglog as an example

385
00:17:50,799 --> 00:17:53,280
the baseline we are choosing is a case

386
00:17:53,280 --> 00:17:55,760
without offloading

387
00:17:55,760 --> 00:17:57,440
it means that

388
00:17:57,440 --> 00:18:01,360
it performance hyperloglog on the gpu

389
00:18:01,360 --> 00:18:02,400
so

390
00:18:02,400 --> 00:18:04,720
it needs eight streaming multiple

391
00:18:04,720 --> 00:18:08,000
processor to consume one kind g network

392
00:18:08,000 --> 00:18:10,400
data

393
00:18:10,480 --> 00:18:12,320
fgnik enable

394
00:18:12,320 --> 00:18:13,520
of loading

395
00:18:13,520 --> 00:18:14,400
so

396
00:18:14,400 --> 00:18:17,520
we can performance hyperlogolog on the

397
00:18:17,520 --> 00:18:18,320
path

398
00:18:18,320 --> 00:18:20,000
on the fgnic

399
00:18:20,000 --> 00:18:24,559
so there's no gpu cycles and

400
00:18:25,039 --> 00:18:28,240
we compare the case with the offloading

401
00:18:28,240 --> 00:18:30,160
and the result of loading

402
00:18:30,160 --> 00:18:33,039
we observe that

403
00:18:33,039 --> 00:18:36,720
fgn enhanced hyperlogolog don't

404
00:18:36,720 --> 00:18:39,520
affect achievable throughput

405
00:18:39,520 --> 00:18:42,559
it means that

406
00:18:42,559 --> 00:18:45,120
aptgenic enable efficient on-pass

407
00:18:45,120 --> 00:18:47,760
processing

408
00:18:47,919 --> 00:18:51,520
to sum up rpg unique enable fast semanic

409
00:18:51,520 --> 00:18:55,039
prototyping for more application

410
00:18:55,039 --> 00:18:56,799
which is important

411
00:18:56,799 --> 00:18:59,600
especially when each chipper type out is

412
00:18:59,600 --> 00:19:03,039
really expensive nowadays

413
00:19:03,039 --> 00:19:05,520
therefore we make

414
00:19:05,520 --> 00:19:07,600
upgenic open source

415
00:19:07,600 --> 00:19:11,559
it's available in github

416
00:19:11,760 --> 00:19:14,400
that's the end of our talk now i'm happy

417
00:19:14,400 --> 00:19:17,799
to take questions

