1
00:00:14,160 --> 00:00:16,720
hello everyone my name is it's jin i

2
00:00:16,720 --> 00:00:18,880
come from shanghai chowton university

3
00:00:18,880 --> 00:00:21,680
epcc lab today i will give a talk to

4
00:00:21,680 --> 00:00:24,080
introduce help rather than recycle

5
00:00:24,080 --> 00:00:26,000
awaiting code startup in service

6
00:00:26,000 --> 00:00:27,920
computing through inter-functioning

7
00:00:27,920 --> 00:00:31,400
content sharing

8
00:00:32,640 --> 00:00:34,239
in the first section i will briefly

9
00:00:34,239 --> 00:00:37,120
introduce what service and the files are

10
00:00:37,120 --> 00:00:39,920
and what are advantages and limitations

11
00:00:39,920 --> 00:00:44,040
of using service model

12
00:00:44,800 --> 00:00:47,680
first what is service according to

13
00:00:47,680 --> 00:00:50,960
berkeley's view service equals fast plus

14
00:00:50,960 --> 00:00:52,399
best

15
00:00:52,399 --> 00:00:55,120
developers upload their codes as

16
00:00:55,120 --> 00:00:57,680
functions to the fast system

17
00:00:57,680 --> 00:01:00,160
user triggers a function through events

18
00:01:00,160 --> 00:01:03,120
like http requests

19
00:01:03,120 --> 00:01:05,040
the fast system allocates computing

20
00:01:05,040 --> 00:01:07,520
resources and interact with the backend

21
00:01:07,520 --> 00:01:12,280
services to execute functions

22
00:01:13,200 --> 00:01:16,479
in the traditional i-space survey model

23
00:01:16,479 --> 00:01:19,040
computing and the memory resources are

24
00:01:19,040 --> 00:01:20,479
pre-allocated

25
00:01:20,479 --> 00:01:23,680
and have no clear relationship with the

26
00:01:23,680 --> 00:01:27,280
number of concurrent service requests

27
00:01:27,280 --> 00:01:30,240
even if there is no request for a period

28
00:01:30,240 --> 00:01:31,439
of time

29
00:01:31,439 --> 00:01:35,279
resources are still occupied

30
00:01:35,360 --> 00:01:37,600
on the other hand service can building

31
00:01:37,600 --> 00:01:40,880
can charge per request without paying

32
00:01:40,880 --> 00:01:42,159
for waiting

33
00:01:42,159 --> 00:01:44,240
which can achieve more efficient

34
00:01:44,240 --> 00:01:47,520
resource utilization

35
00:01:50,479 --> 00:01:53,119
using servlets model we can achieve many

36
00:01:53,119 --> 00:01:55,119
benefits like

37
00:01:55,119 --> 00:01:57,840
on-demand container launching fine grant

38
00:01:57,840 --> 00:01:59,439
resource scaling

39
00:01:59,439 --> 00:02:01,119
offloaded management

40
00:02:01,119 --> 00:02:03,920
and flexible scheduling

41
00:02:03,920 --> 00:02:06,079
however these features also bring

42
00:02:06,079 --> 00:02:09,840
diverter challenges

43
00:02:12,239 --> 00:02:14,560
for example the event driven feature

44
00:02:14,560 --> 00:02:17,599
means if a function is invoked for the

45
00:02:17,599 --> 00:02:20,959
first time or there is no alive or warm

46
00:02:20,959 --> 00:02:22,640
containers for it

47
00:02:22,640 --> 00:02:25,520
the service system has to start a new

48
00:02:25,520 --> 00:02:28,560
container to encapsulate its function

49
00:02:28,560 --> 00:02:29,920
runtime

50
00:02:29,920 --> 00:02:32,720
initialize the software environment

51
00:02:32,720 --> 00:02:35,840
load applications basic code and run the

52
00:02:35,840 --> 00:02:38,239
function all these steps

53
00:02:38,239 --> 00:02:40,239
make up a code startup

54
00:02:40,239 --> 00:02:43,440
and may even take several seconds

55
00:02:43,440 --> 00:02:46,000
the code startup significantly recruits

56
00:02:46,000 --> 00:02:48,959
increases queries end-to-end latency

57
00:02:48,959 --> 00:02:50,879
the long latency problem

58
00:02:50,879 --> 00:02:53,440
often when the function invocation is

59
00:02:53,440 --> 00:02:55,040
short

60
00:02:55,040 --> 00:02:57,280
in this talk we focus on the challenges

61
00:02:57,280 --> 00:02:59,360
of code startups

62
00:02:59,360 --> 00:03:00,959
and the high density and high

63
00:03:00,959 --> 00:03:02,560
concurrency changes

64
00:03:02,560 --> 00:03:05,040
are discussed in our another talk about

65
00:03:05,040 --> 00:03:07,440
run d

66
00:03:10,959 --> 00:03:13,280
now we know that the color startup is an

67
00:03:13,280 --> 00:03:14,640
important issue

68
00:03:14,640 --> 00:03:17,360
does it really compromise the quality of

69
00:03:17,360 --> 00:03:20,959
service in the production

70
00:03:21,680 --> 00:03:24,159
according to the zeus report of

71
00:03:24,159 --> 00:03:26,560
production service trades

72
00:03:26,560 --> 00:03:29,200
the function invocation follow a pareto

73
00:03:29,200 --> 00:03:30,799
distribution

74
00:03:30,799 --> 00:03:32,400
when we monitor the

75
00:03:32,400 --> 00:03:35,040
the code startup frequency from the node

76
00:03:35,040 --> 00:03:36,480
perspective

77
00:03:36,480 --> 00:03:40,000
code startup invocations are less than

78
00:03:40,000 --> 00:03:41,519
one percent

79
00:03:41,519 --> 00:03:43,599
because most invocations come from

80
00:03:43,599 --> 00:03:46,239
popular applications

81
00:03:46,239 --> 00:03:48,640
however these popular applications

82
00:03:48,640 --> 00:03:52,640
account for only 20 percent

83
00:03:52,799 --> 00:03:55,040
it means that if we monitor the code

84
00:03:55,040 --> 00:03:57,120
startup frequency from the view of

85
00:03:57,120 --> 00:03:58,799
multi-patents

86
00:03:58,799 --> 00:04:01,920
about 80 percent of all applications may

87
00:04:01,920 --> 00:04:05,920
frequently experience code startups

88
00:04:05,920 --> 00:04:07,840
to get a good experience with the

89
00:04:07,840 --> 00:04:11,360
service model for most applications

90
00:04:11,360 --> 00:04:13,280
we should optimize the problem from the

91
00:04:13,280 --> 00:04:17,360
perspective of all applications

92
00:04:19,839 --> 00:04:22,639
so in this section we will introduce how

93
00:04:22,639 --> 00:04:25,040
to elevate the code startups

94
00:04:25,040 --> 00:04:27,120
and analyze the efficiency of the

95
00:04:27,120 --> 00:04:30,000
current methods

96
00:04:32,160 --> 00:04:34,479
the pre-owned based methods create

97
00:04:34,479 --> 00:04:37,120
containers and runtime in advance

98
00:04:37,120 --> 00:04:39,759
one method of which is preforming

99
00:04:39,759 --> 00:04:42,320
customized containers for each function

100
00:04:42,320 --> 00:04:44,720
that includes all its required software

101
00:04:44,720 --> 00:04:46,479
packages

102
00:04:46,479 --> 00:04:49,040
it shows good and stable performance and

103
00:04:49,040 --> 00:04:50,400
can just

104
00:04:50,400 --> 00:04:52,560
and can adjust the pool size for each

105
00:04:52,560 --> 00:04:54,160
pre-warm pool

106
00:04:54,160 --> 00:04:56,000
however it brings heavy memory

107
00:04:56,000 --> 00:04:57,600
consumption

108
00:04:57,600 --> 00:04:59,840
azure study focused on this

109
00:04:59,840 --> 00:05:02,160
exclusive preform model to train a

110
00:05:02,160 --> 00:05:04,800
prediction model for resource-friendly

111
00:05:04,800 --> 00:05:07,280
process management

112
00:05:07,280 --> 00:05:10,000
however for most infrequent functions

113
00:05:10,000 --> 00:05:12,400
the history execution trees may not

114
00:05:12,400 --> 00:05:15,199
expose enough data to train an accurate

115
00:05:15,199 --> 00:05:17,600
model

116
00:05:20,320 --> 00:05:23,440
another method of pre-warming containers

117
00:05:23,440 --> 00:05:26,720
is only installing common packages

118
00:05:26,720 --> 00:05:28,639
and all functions share the template

119
00:05:28,639 --> 00:05:30,160
container pool

120
00:05:30,160 --> 00:05:32,320
this method is more memory space

121
00:05:32,320 --> 00:05:33,600
efficient

122
00:05:33,600 --> 00:05:36,160
but generating customized containers in

123
00:05:36,160 --> 00:05:39,199
the specialization phase for function

124
00:05:39,199 --> 00:05:41,360
from the prevalent containers

125
00:05:41,360 --> 00:05:43,440
suffers from packages download and

126
00:05:43,440 --> 00:05:45,840
installing latency overhead

127
00:05:45,840 --> 00:05:47,919
the overhead may introduce from several

128
00:05:47,919 --> 00:05:50,320
aspects and the performance becomes

129
00:05:50,320 --> 00:05:53,320
unpredictable

130
00:05:56,479 --> 00:05:57,440
first

131
00:05:57,440 --> 00:05:59,919
all functions share the same pre-warm

132
00:05:59,919 --> 00:06:00,960
pool

133
00:06:00,960 --> 00:06:02,880
the pre-owned pool is more likely

134
00:06:02,880 --> 00:06:05,280
breakdown with high concurrency code

135
00:06:05,280 --> 00:06:07,280
startup

136
00:06:07,280 --> 00:06:08,720
take

137
00:06:08,720 --> 00:06:10,160
seo and

138
00:06:10,160 --> 00:06:12,800
cut as examples

139
00:06:12,800 --> 00:06:16,080
their functions are invoked concurrently

140
00:06:16,080 --> 00:06:18,000
or in short intervals

141
00:06:18,000 --> 00:06:21,840
to satisfy complex business logic

142
00:06:21,840 --> 00:06:24,639
such as service workflows

143
00:06:24,639 --> 00:06:26,880
this phenomenon attributes to the

144
00:06:26,880 --> 00:06:29,120
inappropriate process

145
00:06:29,120 --> 00:06:32,000
of the pre-war canada pool

146
00:06:32,000 --> 00:06:35,280
so does it mean if we increase the

147
00:06:35,280 --> 00:06:36,639
process

148
00:06:36,639 --> 00:06:39,440
the code startups can be elevated

149
00:06:39,440 --> 00:06:42,160
proportionally

150
00:06:42,880 --> 00:06:46,160
we find that even though we set a larger

151
00:06:46,160 --> 00:06:47,440
process

152
00:06:47,440 --> 00:06:50,800
the code startups elevated converts to a

153
00:06:50,800 --> 00:06:52,560
stable value

154
00:06:52,560 --> 00:06:55,199
because many functions require runtime

155
00:06:55,199 --> 00:06:56,400
environment

156
00:06:56,400 --> 00:06:58,639
with conflict packages

157
00:06:58,639 --> 00:07:00,720
or package versions

158
00:07:00,720 --> 00:07:03,759
in this case the serverless system has

159
00:07:03,759 --> 00:07:06,639
to retry with code setup

160
00:07:06,639 --> 00:07:07,440
and

161
00:07:07,440 --> 00:07:10,240
cannot be elevated by larger preview

162
00:07:10,240 --> 00:07:12,880
process

163
00:07:14,560 --> 00:07:16,880
in addition even though a function can

164
00:07:16,880 --> 00:07:19,520
get a pre-warmed container that without

165
00:07:19,520 --> 00:07:22,800
poor contention and an image conflict

166
00:07:22,800 --> 00:07:24,880
its specialization

167
00:07:24,880 --> 00:07:27,680
of loading additional packages

168
00:07:27,680 --> 00:07:29,840
may introduce more overhead than

169
00:07:29,840 --> 00:07:32,479
directly code startup

170
00:07:32,479 --> 00:07:33,680
as shown

171
00:07:33,680 --> 00:07:36,720
the code container setup takes about

172
00:07:36,720 --> 00:07:39,199
500 milliseconds

173
00:07:39,199 --> 00:07:41,759
the preform startup takes

174
00:07:41,759 --> 00:07:44,800
15 milliseconds in the best case

175
00:07:44,800 --> 00:07:47,160
but takes more than

176
00:07:47,160 --> 00:07:50,400
1500 milliseconds in the worst case

177
00:07:50,400 --> 00:07:54,240
for example the benchmark ddns

178
00:07:54,240 --> 00:07:56,960
this is because functions in ddns

179
00:07:56,960 --> 00:07:58,800
requires to load and store many

180
00:07:58,800 --> 00:08:00,720
additional packages in the preformed

181
00:08:00,720 --> 00:08:02,240
containers

182
00:08:02,240 --> 00:08:03,840
and the packaging loading is time

183
00:08:03,840 --> 00:08:06,560
consuming

184
00:08:10,080 --> 00:08:11,599
when using template template-based

185
00:08:11,599 --> 00:08:14,400
pre-warm pool there are trade-offs that

186
00:08:14,400 --> 00:08:16,639
are inevitable

187
00:08:16,639 --> 00:08:19,599
as reported from azure study most of the

188
00:08:19,599 --> 00:08:24,159
invocations are from a small part of the

189
00:08:24,840 --> 00:08:28,000
functions a prewarmed container can only

190
00:08:28,000 --> 00:08:30,479
catch a small number of packages for the

191
00:08:30,479 --> 00:08:32,559
low memory overhead

192
00:08:32,559 --> 00:08:34,958
caching packages for frequently invoked

193
00:08:34,958 --> 00:08:36,080
functions

194
00:08:36,080 --> 00:08:38,320
improve the system efficiency

195
00:08:38,320 --> 00:08:41,279
but results in poor user experience

196
00:08:41,279 --> 00:08:44,159
and vice versa

197
00:08:46,720 --> 00:08:48,000
in summary

198
00:08:48,000 --> 00:08:51,680
if we use exclusive preload method

199
00:08:51,680 --> 00:08:55,600
we need to adjust pool size dynamically

200
00:08:55,600 --> 00:08:58,640
the possible solution is to profile and

201
00:08:58,640 --> 00:09:02,399
predict for each function at runtime

202
00:09:02,399 --> 00:09:04,880
however the overhead of building

203
00:09:04,880 --> 00:09:07,360
predicting model for each function is

204
00:09:07,360 --> 00:09:08,560
high

205
00:09:08,560 --> 00:09:10,959
and infrequent function may not have

206
00:09:10,959 --> 00:09:14,160
enough data to trim

207
00:09:14,640 --> 00:09:17,440
so our insight is that the current

208
00:09:17,440 --> 00:09:19,760
container prevalent method is not

209
00:09:19,760 --> 00:09:20,800
efficient

210
00:09:20,800 --> 00:09:24,560
due to several invaluable trade-offs

211
00:09:24,560 --> 00:09:27,760
it is beneficial to activate code setups

212
00:09:27,760 --> 00:09:31,920
without trapping in the same dilemmas

213
00:09:35,200 --> 00:09:37,920
we therefore propose to elevate code

214
00:09:37,920 --> 00:09:40,720
startups without relying on pre-warming

215
00:09:40,720 --> 00:09:42,560
containers

216
00:09:42,560 --> 00:09:45,440
the key idea is leveraging the warm but

217
00:09:45,440 --> 00:09:47,040
idle containers

218
00:09:47,040 --> 00:09:48,560
of some functions

219
00:09:48,560 --> 00:09:51,600
to elevate the code startups

220
00:09:51,600 --> 00:09:54,480
a function invocation that requires code

221
00:09:54,480 --> 00:09:55,680
setup

222
00:09:55,680 --> 00:09:58,800
may share an idle warm container from

223
00:09:58,800 --> 00:10:01,519
other functions

224
00:10:03,360 --> 00:10:06,000
our solution is called peggles

225
00:10:06,000 --> 00:10:07,200
in nature

226
00:10:07,200 --> 00:10:10,160
peggles is a kind of helmet crab

227
00:10:10,160 --> 00:10:13,600
that use abandoned sea shells as mobile

228
00:10:13,600 --> 00:10:14,880
homes

229
00:10:14,880 --> 00:10:17,920
as a crab grows it needs to move into a

230
00:10:17,920 --> 00:10:19,680
bigger one

231
00:10:19,680 --> 00:10:22,160
however empty shears are filled in the

232
00:10:22,160 --> 00:10:23,360
fabric string

233
00:10:23,360 --> 00:10:26,240
so they usually arrange themselves in

234
00:10:26,240 --> 00:10:27,839
order queue

235
00:10:27,839 --> 00:10:30,000
in such way they could reuse others

236
00:10:30,000 --> 00:10:34,160
share and move into it quickly

237
00:10:34,640 --> 00:10:37,279
if we see these unsuitable shifts as

238
00:10:37,279 --> 00:10:38,720
idle containers

239
00:10:38,720 --> 00:10:41,279
can we make a cold startup function

240
00:10:41,279 --> 00:10:46,360
like a crab to reuse other shears

241
00:10:48,800 --> 00:10:51,920
the proposed scheme is effective only

242
00:10:51,920 --> 00:10:54,720
when there are idle warm containers in

243
00:10:54,720 --> 00:10:56,480
some functions

244
00:10:56,480 --> 00:10:58,720
when the invocation tends to suffer from

245
00:10:58,720 --> 00:11:01,600
the cold clinical startup

246
00:11:01,600 --> 00:11:04,480
in principle only underutilized warm

247
00:11:04,480 --> 00:11:07,519
containers that are active due to the

248
00:11:07,519 --> 00:11:10,240
keep life strategy can be used by other

249
00:11:10,240 --> 00:11:12,079
functions

250
00:11:12,079 --> 00:11:14,320
otherwise always stealing a warm

251
00:11:14,320 --> 00:11:17,040
container directly may result in the

252
00:11:17,040 --> 00:11:19,600
code cleaner startup of the big team

253
00:11:19,600 --> 00:11:22,000
function

254
00:11:23,279 --> 00:11:25,839
fortunately service computing systems

255
00:11:25,839 --> 00:11:28,959
usually a job keeper live strategy

256
00:11:28,959 --> 00:11:30,959
for example 15 minutes

257
00:11:30,959 --> 00:11:34,160
to reduce the code startup

258
00:11:34,160 --> 00:11:36,959
the keeped alive containers are idle

259
00:11:36,959 --> 00:11:39,360
before they are recycled

260
00:11:39,360 --> 00:11:42,480
the widely exist dyno load pattern also

261
00:11:42,480 --> 00:11:45,440
makes containers over provision at the

262
00:11:45,440 --> 00:11:46,959
high load

263
00:11:46,959 --> 00:11:49,600
these containers will become idle in the

264
00:11:49,600 --> 00:11:52,800
load jobs as well

265
00:11:54,079 --> 00:11:56,320
one may be concerned about the security

266
00:11:56,320 --> 00:11:58,720
and privacy of the idle container for

267
00:11:58,720 --> 00:12:01,760
reusing to this end we propose zygote

268
00:12:01,760 --> 00:12:04,800
container to replace an idle container

269
00:12:04,800 --> 00:12:07,440
it does not include any data or code of

270
00:12:07,440 --> 00:12:09,040
the owner function

271
00:12:09,040 --> 00:12:10,880
which means it can serve as a safe

272
00:12:10,880 --> 00:12:14,240
checkpoint for sharing

273
00:12:14,399 --> 00:12:17,279
in general the common packages required

274
00:12:17,279 --> 00:12:19,760
by all functions are installed

275
00:12:19,760 --> 00:12:23,519
as a shared domain is a zygote container

276
00:12:23,519 --> 00:12:26,160
this content are mounted anomalously

277
00:12:26,160 --> 00:12:27,839
into zygote

278
00:12:27,839 --> 00:12:30,480
thus ensuring that others cannot

279
00:12:30,480 --> 00:12:32,639
identify a function

280
00:12:32,639 --> 00:12:34,800
each function that may use a zygote

281
00:12:34,800 --> 00:12:38,079
container is given a privileged domain

282
00:12:38,079 --> 00:12:40,079
and is only allowed to access its

283
00:12:40,079 --> 00:12:43,040
corresponding packages directory

284
00:12:43,040 --> 00:12:45,040
the privileged domain and the privilege

285
00:12:45,040 --> 00:12:48,000
control are provided by linux operating

286
00:12:48,000 --> 00:12:49,519
system and

287
00:12:49,519 --> 00:12:53,800
different download users

288
00:12:56,800 --> 00:12:59,120
so how does petrus work with zygote

289
00:12:59,120 --> 00:13:02,399
containers it has four main steps

290
00:13:02,399 --> 00:13:05,360
pedros will identify the idle containers

291
00:13:05,360 --> 00:13:08,880
in each function pool at runtime

292
00:13:08,880 --> 00:13:11,760
once identified pegruz will build zygote

293
00:13:11,760 --> 00:13:14,079
image and then replace an idle container

294
00:13:14,079 --> 00:13:16,959
with a zygote

295
00:13:17,279 --> 00:13:19,360
if a to be held function is going to

296
00:13:19,360 --> 00:13:20,720
code startup

297
00:13:20,720 --> 00:13:22,560
fabulous will schedule a zygote

298
00:13:22,560 --> 00:13:24,800
container and the fork a helper

299
00:13:24,800 --> 00:13:29,199
container for this function to reuse

300
00:13:31,279 --> 00:13:33,680
in the system each function may have

301
00:13:33,680 --> 00:13:35,360
that good containers

302
00:13:35,360 --> 00:13:38,079
so how does peg rules arrange their gold

303
00:13:38,079 --> 00:13:41,360
containers for appropriate forking

304
00:13:41,360 --> 00:13:44,639
we introduce a similarity featured

305
00:13:44,639 --> 00:13:49,320
weighted random sampling algorithm

306
00:13:49,839 --> 00:13:52,959
each select to be helped functions based

307
00:13:52,959 --> 00:13:55,360
on the package similarity between

308
00:13:55,360 --> 00:13:57,760
functions and the frequency of function

309
00:13:57,760 --> 00:14:00,000
code startups

310
00:14:00,000 --> 00:14:03,760
sf wrs policy reduce the number of

311
00:14:03,760 --> 00:14:06,480
packages to be installed

312
00:14:06,480 --> 00:14:09,279
thus minimizing the memory needed and

313
00:14:09,279 --> 00:14:11,440
the overhead of creating zygote

314
00:14:11,440 --> 00:14:14,440
containers

315
00:14:17,279 --> 00:14:20,240
with above designs pedros can elevate

316
00:14:20,240 --> 00:14:23,920
the code startup by reproposing a one

317
00:14:23,920 --> 00:14:27,360
but idle container from another function

318
00:14:27,360 --> 00:14:28,720
in this section

319
00:14:28,720 --> 00:14:33,040
we will introduce our evaluation process

320
00:14:34,240 --> 00:14:37,360
we use 10 best practice applications

321
00:14:37,360 --> 00:14:40,399
with the most github stars from amazon

322
00:14:40,399 --> 00:14:43,360
aws samples as a benchmarks

323
00:14:43,360 --> 00:14:47,199
and other tricks to evaluate progress

324
00:14:47,199 --> 00:14:49,760
fast profile and service bench

325
00:14:49,760 --> 00:14:52,959
show similar results

326
00:14:55,120 --> 00:14:59,920
as observed from replaying as well trace

327
00:14:59,920 --> 00:15:02,160
pebbles reduced the number of code

328
00:15:02,160 --> 00:15:03,959
startups by

329
00:15:03,959 --> 00:15:06,399
84.6 percent

330
00:15:06,399 --> 00:15:07,399
about

331
00:15:07,399 --> 00:15:10,800
73.4 percent of all functions

332
00:15:10,800 --> 00:15:13,839
experience cold startup less than once

333
00:15:13,839 --> 00:15:16,800
in a day with pedros

334
00:15:16,800 --> 00:15:18,160
meanwhile

335
00:15:18,160 --> 00:15:20,800
ninety percent of the functions

336
00:15:20,800 --> 00:15:23,360
experience cold startups less than five

337
00:15:23,360 --> 00:15:27,639
times daily with pedros

338
00:15:28,880 --> 00:15:31,600
the code startup latency is reduced from

339
00:15:31,600 --> 00:15:33,600
hundreds of milliseconds

340
00:15:33,600 --> 00:15:39,000
to 16 milliseconds if elevated

341
00:15:40,160 --> 00:15:42,079
for end-to-end latencies

342
00:15:42,079 --> 00:15:44,959
popular functions have similar

343
00:15:44,959 --> 00:15:46,320
94

344
00:15:46,320 --> 00:15:48,880
percent tail latency with open risk and

345
00:15:48,880 --> 00:15:50,560
pedros

346
00:15:50,560 --> 00:15:53,120
this is because the slowest

347
00:15:53,120 --> 00:15:54,560
94

348
00:15:54,560 --> 00:15:56,480
tail latencies

349
00:15:56,480 --> 00:15:58,079
of these functions

350
00:15:58,079 --> 00:16:01,120
still experience warm startup

351
00:16:01,120 --> 00:16:02,959
as these functions are frequently

352
00:16:02,959 --> 00:16:04,800
invoked

353
00:16:04,800 --> 00:16:07,040
for the middle popularity and the low

354
00:16:07,040 --> 00:16:09,680
popularity functions

355
00:16:09,680 --> 00:16:10,839
there are

356
00:16:10,839 --> 00:16:14,560
95 percent tail latencies as the latency

357
00:16:14,560 --> 00:16:17,199
of the function invocation that suffers

358
00:16:17,199 --> 00:16:19,360
from the code kind of startup with

359
00:16:19,360 --> 00:16:22,079
openwhisk

360
00:16:22,079 --> 00:16:23,920
paragraphs indeed brings lower

361
00:16:23,920 --> 00:16:27,279
end-to-end tail latencies

362
00:16:29,839 --> 00:16:32,320
finally we give our conclusion on peg

363
00:16:32,320 --> 00:16:34,639
rules

364
00:16:35,600 --> 00:16:38,480
in summary pebbles proposed to reusing

365
00:16:38,480 --> 00:16:42,480
idle containers to elevate code startups

366
00:16:42,480 --> 00:16:45,600
so it introduced like containers that

367
00:16:45,600 --> 00:16:48,560
contain the shared domain and privileged

368
00:16:48,560 --> 00:16:53,199
domain to provide security and privacy

369
00:16:53,199 --> 00:16:55,519
when scheduling zygote containers

370
00:16:55,519 --> 00:16:58,959
we use a similarity filtered weighted

371
00:16:58,959 --> 00:17:01,440
random sampling algorithm

372
00:17:01,440 --> 00:17:03,680
which can improve the inter-function

373
00:17:03,680 --> 00:17:07,280
container sharing efficiency

374
00:17:08,720 --> 00:17:10,799
another related track presentation

375
00:17:10,799 --> 00:17:12,160
called run d

376
00:17:12,160 --> 00:17:14,720
introduces how to construct container

377
00:17:14,720 --> 00:17:17,919
runtime in service computing to enable

378
00:17:17,919 --> 00:17:20,160
high density and high concurrency

379
00:17:20,160 --> 00:17:22,640
startup

380
00:17:24,880 --> 00:17:27,520
you can also contact us by email with

381
00:17:27,520 --> 00:17:28,960
any questions

382
00:17:28,960 --> 00:17:32,520
thanks for listening

