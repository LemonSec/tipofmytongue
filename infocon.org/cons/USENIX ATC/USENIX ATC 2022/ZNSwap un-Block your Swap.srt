1
00:00:12,960 --> 00:00:15,280
hello everyone i'm going to talk about

2
00:00:15,280 --> 00:00:17,600
our paper as in swap and dr swap thank

3
00:00:17,600 --> 00:00:19,680
you so much for the kind introduction

4
00:00:19,680 --> 00:00:22,080
this is joint work between the acl cell

5
00:00:22,080 --> 00:00:24,880
lab at the technion and western digital

6
00:00:24,880 --> 00:00:26,880
research

7
00:00:26,880 --> 00:00:29,439
over time data for additional memory

8
00:00:29,439 --> 00:00:31,199
servers increases

9
00:00:31,199 --> 00:00:33,520
data center applications exhibit larger

10
00:00:33,520 --> 00:00:36,160
and larger memory footprints but not all

11
00:00:36,160 --> 00:00:37,920
data is born equal

12
00:00:37,920 --> 00:00:39,920
for example some data is used more often

13
00:00:39,920 --> 00:00:41,120
than others

14
00:00:41,120 --> 00:00:43,200
or some data in memory belongs to files

15
00:00:43,200 --> 00:00:45,440
and others do not

16
00:00:45,440 --> 00:00:47,200
whenever the system is under memory

17
00:00:47,200 --> 00:00:49,440
pressure it relieves it by reclaiming

18
00:00:49,440 --> 00:00:50,800
system memory

19
00:00:50,800 --> 00:00:53,199
during memory reclamation file backed

20
00:00:53,199 --> 00:00:55,440
pages are discarded or written back to

21
00:00:55,440 --> 00:00:57,760
the backing device to free up memory

22
00:00:57,760 --> 00:00:59,520
anonymous pages on the other hand are

23
00:00:59,520 --> 00:01:02,000
swapped out to a swap device

24
00:01:02,000 --> 00:01:04,080
swap allows the system to efficiently

25
00:01:04,080 --> 00:01:06,640
balance file backed and anonymous pages

26
00:01:06,640 --> 00:01:09,680
in memory during memory reclamation

27
00:01:09,680 --> 00:01:11,840
some have even advocated to use swap

28
00:01:11,840 --> 00:01:15,200
device as a memory extension mechanism

29
00:01:15,200 --> 00:01:17,280
swap is regaining interest in academia

30
00:01:17,280 --> 00:01:19,600
with recent works proposed to offload

31
00:01:19,600 --> 00:01:21,280
the swapping mechanism to dedicated

32
00:01:21,280 --> 00:01:22,400
hardware

33
00:01:22,400 --> 00:01:24,799
in industry several projects such as

34
00:01:24,799 --> 00:01:27,520
facebook's fb dax 2 introduced swap

35
00:01:27,520 --> 00:01:29,759
controls and alibaba's kernel now

36
00:01:29,759 --> 00:01:32,000
supports plus c group background memory

37
00:01:32,000 --> 00:01:35,119
replay mechanisms

38
00:01:35,840 --> 00:01:38,720
with the advent of flat over fast flash

39
00:01:38,720 --> 00:01:40,720
based ssds swap has become more

40
00:01:40,720 --> 00:01:41,840
attractive

41
00:01:41,840 --> 00:01:43,680
advances in flash and interconnect

42
00:01:43,680 --> 00:01:45,920
technologies reduce the access latencies

43
00:01:45,920 --> 00:01:47,680
and increase the available bandwidth to

44
00:01:47,680 --> 00:01:50,079
ssds this is all great for memory

45
00:01:50,079 --> 00:01:52,240
swapping

46
00:01:52,240 --> 00:01:53,280
however

47
00:01:53,280 --> 00:01:55,119
when we evaluate the performance of swap

48
00:01:55,119 --> 00:01:57,280
on ssd we encounter some performance

49
00:01:57,280 --> 00:01:58,479
anomalies

50
00:01:58,479 --> 00:02:00,640
to illustrate one search anomaly we

51
00:02:00,640 --> 00:02:02,399
perform random memory writes in the

52
00:02:02,399 --> 00:02:04,960
memory balance c group failing the ssd

53
00:02:04,960 --> 00:02:06,479
that serves as a swap device to

54
00:02:06,479 --> 00:02:08,479
different utilizations

55
00:02:08,479 --> 00:02:10,720
the graph shows the swap bandwidth as a

56
00:02:10,720 --> 00:02:12,160
function of different swap device

57
00:02:12,160 --> 00:02:13,440
utilizations

58
00:02:13,440 --> 00:02:15,680
we notice a significant drop as early as

59
00:02:15,680 --> 00:02:19,040
20 percent of device utilization

60
00:02:19,040 --> 00:02:22,239
at 20 utilization the bandwidth is less

61
00:02:22,239 --> 00:02:23,520
than half

62
00:02:23,520 --> 00:02:25,280
we therefore investigate this and other

63
00:02:25,280 --> 00:02:28,160
anomalies we observed with swap on ssds

64
00:02:28,160 --> 00:02:29,680
analyzing the root causes of such

65
00:02:29,680 --> 00:02:32,640
performance aggregations

66
00:02:32,640 --> 00:02:34,239
in this talk i'll present a brief

67
00:02:34,239 --> 00:02:36,720
background on ssds their drawbacks and

68
00:02:36,720 --> 00:02:39,519
how they affect your performance on ssds

69
00:02:39,519 --> 00:02:41,280
i'll then present some background on the

70
00:02:41,280 --> 00:02:44,480
new zoned namespaces interface for ssds

71
00:02:44,480 --> 00:02:46,640
and finally i'll present our work zn

72
00:02:46,640 --> 00:02:49,280
swap presenting its design some design

73
00:02:49,280 --> 00:02:50,959
challenges we have faced in the real

74
00:02:50,959 --> 00:02:55,200
system and our system's evaluation

75
00:02:55,360 --> 00:02:57,920
flash-based ssds are divided into erase

76
00:02:57,920 --> 00:03:00,160
blocks each of these race blocks are

77
00:03:00,160 --> 00:03:02,560
further divided into pages

78
00:03:02,560 --> 00:03:04,080
writes can be performed at page

79
00:03:04,080 --> 00:03:05,840
granularity and must be written

80
00:03:05,840 --> 00:03:08,400
sequentially within an erase block

81
00:03:08,400 --> 00:03:10,480
erase operations are performed as an

82
00:03:10,480 --> 00:03:12,640
erase block granularity

83
00:03:12,640 --> 00:03:15,440
pages cannot be overwritten therefore no

84
00:03:15,440 --> 00:03:17,840
rewri therefore to rewrite pages within

85
00:03:17,840 --> 00:03:20,159
an erase block the whole block must be

86
00:03:20,159 --> 00:03:23,679
erased before written to again

87
00:03:24,400 --> 00:03:26,480
the constraints of flash media do not

88
00:03:26,480 --> 00:03:28,560
match the traditional block interface we

89
00:03:28,560 --> 00:03:30,480
are all acquainted with today

90
00:03:30,480 --> 00:03:32,640
in order to expose an interface that

91
00:03:32,640 --> 00:03:34,640
allows in-place updates and random

92
00:03:34,640 --> 00:03:37,360
writes a fresh a flash translation layer

93
00:03:37,360 --> 00:03:38,720
or an ftl

94
00:03:38,720 --> 00:03:40,480
within the ssd abstracts the

95
00:03:40,480 --> 00:03:42,159
complexities and limitations of the

96
00:03:42,159 --> 00:03:43,920
underlying media

97
00:03:43,920 --> 00:03:46,640
the ftl maps logical block addresses or

98
00:03:46,640 --> 00:03:48,000
lbas

99
00:03:48,000 --> 00:03:50,480
to host to the host and

100
00:03:50,480 --> 00:03:52,799
the host utilizes those addresses in

101
00:03:52,799 --> 00:03:55,040
order to access the actual data which is

102
00:03:55,040 --> 00:03:57,599
stored within the physical device

103
00:03:57,599 --> 00:04:00,080
the mapping layer is a necessary evil to

104
00:04:00,080 --> 00:04:02,480
uphold the block interface since it is a

105
00:04:02,480 --> 00:04:04,319
complicated and resource expensive

106
00:04:04,319 --> 00:04:07,518
component of the ssd

107
00:04:07,599 --> 00:04:09,599
due to the granularity mismatch between

108
00:04:09,599 --> 00:04:11,920
the media's write and delete operations

109
00:04:11,920 --> 00:04:14,159
ssds need to perform garbage collection

110
00:04:14,159 --> 00:04:16,560
to reclaim flash capacity

111
00:04:16,560 --> 00:04:17,918
we explained the garbage collection

112
00:04:17,918 --> 00:04:19,680
process with an example

113
00:04:19,680 --> 00:04:22,400
let's assume an ssd has only two erase

114
00:04:22,400 --> 00:04:23,360
blocks

115
00:04:23,360 --> 00:04:25,600
the first set of pages within erase

116
00:04:25,600 --> 00:04:26,720
block a

117
00:04:26,720 --> 00:04:31,360
is the data of lbas 0 to 3.

118
00:04:31,520 --> 00:04:33,600
the host writes the next four lbas in

119
00:04:33,600 --> 00:04:36,720
the sequence and wishes to rewrite lbas

120
00:04:36,720 --> 00:04:38,240
0 to 3.

121
00:04:38,240 --> 00:04:40,720
the new data will be appended to the end

122
00:04:40,720 --> 00:04:42,720
of the erase block while all data is

123
00:04:42,720 --> 00:04:44,320
invalidated

124
00:04:44,320 --> 00:04:48,560
the ftl will now map lbas 0 to 3 to the

125
00:04:48,560 --> 00:04:50,960
newly written pages invalidating the

126
00:04:50,960 --> 00:04:53,680
first four pages within the erase block

127
00:04:53,680 --> 00:04:55,919
these pages cannot be overwritten or

128
00:04:55,919 --> 00:04:57,199
used

129
00:04:57,199 --> 00:05:00,960
until the erase block itself is erased

130
00:05:00,960 --> 00:05:02,720
to reclaim the available capacity

131
00:05:02,720 --> 00:05:05,040
currently occupied by the invalid pages

132
00:05:05,040 --> 00:05:07,919
in the erase block a the ssd performs

133
00:05:07,919 --> 00:05:10,080
the garbage collection operation the

134
00:05:10,080 --> 00:05:12,800
operation copies only the valid pages

135
00:05:12,800 --> 00:05:15,280
within raised block a into a new race

136
00:05:15,280 --> 00:05:16,880
block erase block b

137
00:05:16,880 --> 00:05:19,280
and erases erase block a

138
00:05:19,280 --> 00:05:21,360
this operation will also lead to the

139
00:05:21,360 --> 00:05:23,520
remapping of all of the copied pages

140
00:05:23,520 --> 00:05:26,880
within the ftl itself

141
00:05:27,840 --> 00:05:30,000
the right amplification factor is the

142
00:05:30,000 --> 00:05:32,400
number of overall of overall rights to

143
00:05:32,400 --> 00:05:34,479
the device divided by the number of

144
00:05:34,479 --> 00:05:37,039
rights actually issued by the host

145
00:05:37,039 --> 00:05:39,520
the number of overall rights includes

146
00:05:39,520 --> 00:05:42,240
rights directly issued by the host and

147
00:05:42,240 --> 00:05:43,600
copies performed by the garbage

148
00:05:43,600 --> 00:05:46,240
collection mechanisms therefore a right

149
00:05:46,240 --> 00:05:48,800
amplification of one indicates that no

150
00:05:48,800 --> 00:05:50,800
garbage collection operations happened

151
00:05:50,800 --> 00:05:52,320
and the right and the high right

152
00:05:52,320 --> 00:05:55,280
amplification factor indicates more more

153
00:05:55,280 --> 00:05:56,880
internal rights issued by the garbage

154
00:05:56,880 --> 00:05:59,039
collector which leads to lower device

155
00:05:59,039 --> 00:06:01,599
performance

156
00:06:01,919 --> 00:06:04,400
we identify that the root cause of the

157
00:06:04,400 --> 00:06:06,800
swap performance segregation on the ssd

158
00:06:06,800 --> 00:06:08,800
in the previous graph is due to the

159
00:06:08,800 --> 00:06:10,960
garbage collection overheads

160
00:06:10,960 --> 00:06:12,560
we observe that garbage collection

161
00:06:12,560 --> 00:06:14,560
impacts performance even when the device

162
00:06:14,560 --> 00:06:17,199
utilization is really low

163
00:06:17,199 --> 00:06:18,960
theoretically this should not be the

164
00:06:18,960 --> 00:06:22,160
case as there should be not many valid

165
00:06:22,160 --> 00:06:24,240
pages within erase blocks that the

166
00:06:24,240 --> 00:06:26,880
garbage collection actually has to copy

167
00:06:26,880 --> 00:06:29,280
we identify that this phenomenon occurs

168
00:06:29,280 --> 00:06:31,360
due to the knowledge gap between the ssd

169
00:06:31,360 --> 00:06:32,840
and the operating

170
00:06:32,840 --> 00:06:36,479
system the ssd is unaware that some swap

171
00:06:36,479 --> 00:06:38,880
data written is considered invalid by

172
00:06:38,880 --> 00:06:39,919
the host

173
00:06:39,919 --> 00:06:41,759
meaning that the data will no longer be

174
00:06:41,759 --> 00:06:43,520
raid by the host

175
00:06:43,520 --> 00:06:45,919
this leads to unnecessary data copies

176
00:06:45,919 --> 00:06:48,000
performed by the garbage collector which

177
00:06:48,000 --> 00:06:49,759
increases rate amplification and

178
00:06:49,759 --> 00:06:52,880
decreases performance

179
00:06:52,960 --> 00:06:56,240
trim operations are a hint provided by

180
00:06:56,240 --> 00:06:58,800
the host to the ssd informing it that

181
00:06:58,800 --> 00:07:01,360
data is no longer needed by the host

182
00:07:01,360 --> 00:07:04,400
theoretically at low device utilizations

183
00:07:04,400 --> 00:07:06,319
if a trim operation was dispatched by

184
00:07:06,319 --> 00:07:08,880
the host for every invalid swap datum on

185
00:07:08,880 --> 00:07:11,919
the ssd the garbage collector or the

186
00:07:11,919 --> 00:07:13,440
garbage collector would not impact the

187
00:07:13,440 --> 00:07:15,599
performance that much

188
00:07:15,599 --> 00:07:17,520
we observe that current trim support for

189
00:07:17,520 --> 00:07:19,840
ssds is too coarse-grained to have any

190
00:07:19,840 --> 00:07:22,400
effects and attempts at achieving finer

191
00:07:22,400 --> 00:07:24,319
granularity trims do not have a

192
00:07:24,319 --> 00:07:26,400
significant impact on performance due to

193
00:07:26,400 --> 00:07:28,240
the maximum rate at which the host

194
00:07:28,240 --> 00:07:30,720
itself and the device process such trim

195
00:07:30,720 --> 00:07:33,280
commands a more detailed analysis can be

196
00:07:33,280 --> 00:07:36,239
found in our paper

197
00:07:36,639 --> 00:07:38,479
the garbage collection

198
00:07:38,479 --> 00:07:40,319
performance effects it affects the

199
00:07:40,319 --> 00:07:41,840
entire ssd

200
00:07:41,840 --> 00:07:43,759
it's a direct consequence of an

201
00:07:43,759 --> 00:07:45,840
autonomous design

202
00:07:45,840 --> 00:07:47,680
that the ssd performs garbage collection

203
00:07:47,680 --> 00:07:50,240
on its own and its impairment on any

204
00:07:50,240 --> 00:07:52,400
isolation attempt that the host wishes

205
00:07:52,400 --> 00:07:54,080
to maintain

206
00:07:54,080 --> 00:07:56,000
to illustrate such behavior we perform

207
00:07:56,000 --> 00:07:58,160
an experiment with two c groups which

208
00:07:58,160 --> 00:08:00,639
utilizes the same swap device each c

209
00:08:00,639 --> 00:08:02,319
group is throttled to utilize half of

210
00:08:02,319 --> 00:08:04,400
the available ssd bandwidth

211
00:08:04,400 --> 00:08:06,319
c group a performs random read

212
00:08:06,319 --> 00:08:08,240
operations only hence it does not

213
00:08:08,240 --> 00:08:10,960
invalidate any flash pages on the ssd

214
00:08:10,960 --> 00:08:12,720
and on its own will not trigger garbage

215
00:08:12,720 --> 00:08:14,000
collection

216
00:08:14,000 --> 00:08:16,400
sigra b performs random rights which

217
00:08:16,400 --> 00:08:18,400
eventually lead to a garbage collection

218
00:08:18,400 --> 00:08:20,800
performed by the device

219
00:08:20,800 --> 00:08:22,879
the graph below shows the swap in

220
00:08:22,879 --> 00:08:25,280
bandwidth of both c groups as a function

221
00:08:25,280 --> 00:08:28,000
of time at first both c groups are able

222
00:08:28,000 --> 00:08:29,840
to sustain their maximum allocated

223
00:08:29,840 --> 00:08:32,399
bandwidth and after some time we can see

224
00:08:32,399 --> 00:08:34,320
that the swap in bandwidth of both c

225
00:08:34,320 --> 00:08:35,760
groups deteriorate

226
00:08:35,760 --> 00:08:38,080
only though only uh

227
00:08:38,080 --> 00:08:40,719
only in c group b actually causes the

228
00:08:40,719 --> 00:08:42,958
garbage collection so this should not be

229
00:08:42,958 --> 00:08:45,359
the case

230
00:08:45,680 --> 00:08:47,920
the new zoned namespaces interface for

231
00:08:47,920 --> 00:08:50,800
ssds enabled tighter coupling between

232
00:08:50,800 --> 00:08:53,040
the ssd and applications

233
00:08:53,040 --> 00:08:55,760
in a nutshell the ssd is divided into jo

234
00:08:55,760 --> 00:08:57,120
into zones

235
00:08:57,120 --> 00:09:00,080
each have the size of an erase block

236
00:09:00,080 --> 00:09:01,680
each zone can only be written to

237
00:09:01,680 --> 00:09:04,160
sequentially and has to be reset before

238
00:09:04,160 --> 00:09:07,519
rewriting the data to the zone

239
00:09:07,519 --> 00:09:10,000
this means that such zns devices do not

240
00:09:10,000 --> 00:09:13,040
need a complicated ftl nor a garbage

241
00:09:13,040 --> 00:09:14,560
collection mechanism as these

242
00:09:14,560 --> 00:09:17,120
responsibilities now fall into the host

243
00:09:17,120 --> 00:09:18,720
while presenting the host with a higher

244
00:09:18,720 --> 00:09:21,279
degree of control over the device

245
00:09:21,279 --> 00:09:23,040
on the right we can see how such a

246
00:09:23,040 --> 00:09:25,600
higher degree of control can be utilized

247
00:09:25,600 --> 00:09:27,600
when several applications use a regular

248
00:09:27,600 --> 00:09:30,240
ssd pages from different applications

249
00:09:30,240 --> 00:09:32,320
can be co-located onto the same raised

250
00:09:32,320 --> 00:09:33,200
blocks

251
00:09:33,200 --> 00:09:34,959
this can lead to high garbage collection

252
00:09:34,959 --> 00:09:37,680
overheads due to pages uh with different

253
00:09:37,680 --> 00:09:39,839
lifetimes co-located onto the same race

254
00:09:39,839 --> 00:09:41,120
block

255
00:09:41,120 --> 00:09:43,839
with the zn ssds applications can

256
00:09:43,839 --> 00:09:47,200
utilize zones to ensure that uh only

257
00:09:47,200 --> 00:09:50,160
their data reside within a specific zone

258
00:09:50,160 --> 00:09:52,640
thus data with similar lifetimes are

259
00:09:52,640 --> 00:09:54,880
co-located together which lowers or

260
00:09:54,880 --> 00:09:58,320
eliminate garbage collection altogether

261
00:09:58,320 --> 00:10:00,640
in our work we observe that to achieve

262
00:10:00,640 --> 00:10:03,120
better swap performance the system needs

263
00:10:03,120 --> 00:10:05,279
to gain control over key mechanisms

264
00:10:05,279 --> 00:10:06,800
within the ssd

265
00:10:06,800 --> 00:10:09,200
zns is the key enabler and allows us to

266
00:10:09,200 --> 00:10:11,279
design a swap subsystem that suits the

267
00:10:11,279 --> 00:10:14,959
properties of the underlying media

268
00:10:15,040 --> 00:10:17,600
zn swap synergizes between the os swap

269
00:10:17,600 --> 00:10:20,320
logic and zns ssds in order to achieve

270
00:10:20,320 --> 00:10:22,399
more efficient swapping

271
00:10:22,399 --> 00:10:25,519
zns what xeon swap's design goals are to

272
00:10:25,519 --> 00:10:27,839
enable fine grain swap management with

273
00:10:27,839 --> 00:10:29,440
whole site garbage production which

274
00:10:29,440 --> 00:10:31,519
relinquishes the needs for trims

275
00:10:31,519 --> 00:10:34,160
an efficient swap cache and host garbage

276
00:10:34,160 --> 00:10:36,240
collection co-design that lowers right

277
00:10:36,240 --> 00:10:37,600
amplification

278
00:10:37,600 --> 00:10:39,600
to enable custom data placement and

279
00:10:39,600 --> 00:10:42,079
garbage collection reclamation policies

280
00:10:42,079 --> 00:10:43,839
and to achieve isolation between

281
00:10:43,839 --> 00:10:46,000
multiple tenants on a system that shares

282
00:10:46,000 --> 00:10:48,560
a swap device

283
00:10:48,560 --> 00:10:50,959
i'll now go over zn swaps main design

284
00:10:50,959 --> 00:10:52,720
components following

285
00:10:52,720 --> 00:10:54,959
following an example of a memory pages

286
00:10:54,959 --> 00:10:56,800
swap out process

287
00:10:56,800 --> 00:10:58,959
a candidate anonymous page is selected

288
00:10:58,959 --> 00:11:00,720
by the system in order for it to be

289
00:11:00,720 --> 00:11:02,720
swapped out

290
00:11:02,720 --> 00:11:05,279
the page enters the zns page reclaim

291
00:11:05,279 --> 00:11:07,279
component which handles page table and

292
00:11:07,279 --> 00:11:09,360
swap cache operations

293
00:11:09,360 --> 00:11:12,160
in contrast to original swap logic zn

294
00:11:12,160 --> 00:11:15,279
swap utilizes the zone append operation

295
00:11:15,279 --> 00:11:17,760
the zone append operation only informs

296
00:11:17,760 --> 00:11:20,000
the host of the destination of the

297
00:11:20,000 --> 00:11:22,560
written data after it has actually been

298
00:11:22,560 --> 00:11:24,640
written it does not know this data

299
00:11:24,640 --> 00:11:27,640
beforehand

300
00:11:28,000 --> 00:11:30,320
before the page is written to the ssd

301
00:11:30,320 --> 00:11:32,320
the page reclaim module consults with

302
00:11:32,320 --> 00:11:34,320
the policy manager that determines the

303
00:11:34,320 --> 00:11:36,480
destination zone as per the placement

304
00:11:36,480 --> 00:11:37,600
policy

305
00:11:37,600 --> 00:11:39,920
the policy manager also guides the whole

306
00:11:39,920 --> 00:11:42,079
site garbage collector to free certain

307
00:11:42,079 --> 00:11:45,120
zones on the device

308
00:11:45,200 --> 00:11:47,279
the zone allocator seamlessly handles

309
00:11:47,279 --> 00:11:49,200
new zona locations hiding the

310
00:11:49,200 --> 00:11:51,120
complexities of zone management and

311
00:11:51,120 --> 00:11:54,839
certain limitations imposed by the zns

312
00:11:54,839 --> 00:11:57,279
ssd the page is then submitted to the

313
00:11:57,279 --> 00:12:00,160
block layer with a destination zone

314
00:12:00,160 --> 00:12:02,000
and is passed into the i o manager

315
00:12:02,000 --> 00:12:03,120
component

316
00:12:03,120 --> 00:12:05,360
this component enables merges of zone

317
00:12:05,360 --> 00:12:07,519
append operations for higher throughput

318
00:12:07,519 --> 00:12:10,079
and injects relevant per page metadata

319
00:12:10,079 --> 00:12:13,120
to the ir request

320
00:12:13,200 --> 00:12:15,360
the i o manager submits the i request to

321
00:12:15,360 --> 00:12:17,200
the nvme driver

322
00:12:17,200 --> 00:12:20,560
which then writes data onto the zns ssd

323
00:12:20,560 --> 00:12:22,959
the i o manager then updates the page

324
00:12:22,959 --> 00:12:24,639
reclaim component with the page's

325
00:12:24,639 --> 00:12:26,399
location once the write has been

326
00:12:26,399 --> 00:12:29,279
completed as the zone append operation

327
00:12:29,279 --> 00:12:31,200
we do not know beforehand where the data

328
00:12:31,200 --> 00:12:33,600
is going to be written

329
00:12:33,600 --> 00:12:35,519
i'll briefly present the design of our

330
00:12:35,519 --> 00:12:37,440
whole site garbage collection mechanism

331
00:12:37,440 --> 00:12:39,279
dubbed zngc

332
00:12:39,279 --> 00:12:41,440
details about rest the rest of zn swaps

333
00:12:41,440 --> 00:12:45,040
components can be found in the paper

334
00:12:45,040 --> 00:12:47,519
xeon swap's whole site gc is tightly

335
00:12:47,519 --> 00:12:49,040
integrated with the kernel's virtual

336
00:12:49,040 --> 00:12:50,560
memory subsystem

337
00:12:50,560 --> 00:12:52,560
it is designed to not dynamically

338
00:12:52,560 --> 00:12:54,639
allocate memory in a memory constraint

339
00:12:54,639 --> 00:12:57,680
system and to exhibit low overheads

340
00:12:57,680 --> 00:12:59,920
during the garbage collection process

341
00:12:59,920 --> 00:13:02,560
zngc also eliminates need for trims

342
00:13:02,560 --> 00:13:04,480
eliminates the performance jitters

343
00:13:04,480 --> 00:13:06,560
caused by the traditional ssd's garbage

344
00:13:06,560 --> 00:13:09,200
collector and eliminates copies of

345
00:13:09,200 --> 00:13:11,279
invalid of invalid swap data during the

346
00:13:11,279 --> 00:13:14,560
garbage collection process

347
00:13:14,959 --> 00:13:16,800
once a zone is selected for garbage

348
00:13:16,800 --> 00:13:19,519
collection zngc only copies the

349
00:13:19,519 --> 00:13:22,959
necessary data uh from the swap device

350
00:13:22,959 --> 00:13:25,120
from uh from a specific zone into a new

351
00:13:25,120 --> 00:13:26,240
zone

352
00:13:26,240 --> 00:13:28,639
information on the status of each swap

353
00:13:28,639 --> 00:13:31,279
datum is available to zngc due to its

354
00:13:31,279 --> 00:13:32,720
tight integration with the virtual

355
00:13:32,720 --> 00:13:35,360
memory subsystem

356
00:13:35,360 --> 00:13:37,920
zngc then proceeds to copy the relevant

357
00:13:37,920 --> 00:13:39,920
data from one zone to the other while

358
00:13:39,920 --> 00:13:41,680
taking care of intricate race conditions

359
00:13:41,680 --> 00:13:45,120
in the virtual memory subsystem

360
00:13:45,440 --> 00:13:48,079
contrary to traditional block ssds a

361
00:13:48,079 --> 00:13:50,800
page moved by zngc is assigned a new

362
00:13:50,800 --> 00:13:52,800
host visible address

363
00:13:52,800 --> 00:13:56,320
in the example page 2 from zone a is now

364
00:13:56,320 --> 00:13:59,360
located in page 1 of zone b

365
00:13:59,360 --> 00:14:01,120
there is no intermediate translation

366
00:14:01,120 --> 00:14:03,440
there as in traditional ssds and the

367
00:14:03,440 --> 00:14:05,519
contents within the page tables of the

368
00:14:05,519 --> 00:14:08,800
swapped out pages points to the pages

369
00:14:08,800 --> 00:14:11,839
original location that is zone a

370
00:14:11,839 --> 00:14:14,720
zngc must therefore update the entries

371
00:14:14,720 --> 00:14:16,800
within the page table to point to the

372
00:14:16,800 --> 00:14:18,079
new location

373
00:14:18,079 --> 00:14:20,079
but how can we locate them without an

374
00:14:20,079 --> 00:14:22,959
exhaustive search

375
00:14:23,199 --> 00:14:25,600
during the swap out process each of the

376
00:14:25,600 --> 00:14:28,480
pages reverse mapping metadata is stored

377
00:14:28,480 --> 00:14:31,279
in the per lba metadata region provided

378
00:14:31,279 --> 00:14:33,040
by the ssd

379
00:14:33,040 --> 00:14:35,440
the stored metadata is the data used by

380
00:14:35,440 --> 00:14:37,839
the operating system itself to locate

381
00:14:37,839 --> 00:14:40,000
all page table entries of a physical

382
00:14:40,000 --> 00:14:42,240
memory page and it consists of the

383
00:14:42,240 --> 00:14:45,199
anonymous vma pointer and the index

384
00:14:45,199 --> 00:14:47,120
the storage interface allows to retrieve

385
00:14:47,120 --> 00:14:48,880
the metadata along with the respective

386
00:14:48,880 --> 00:14:51,360
data in the single io operation

387
00:14:51,360 --> 00:14:54,000
thus zngc retrieves the metadata as a

388
00:14:54,000 --> 00:14:56,639
byproduct of reading the data in order

389
00:14:56,639 --> 00:14:58,639
to perform the reverse lookup of a given

390
00:14:58,639 --> 00:15:00,959
page and updates our expected their

391
00:15:00,959 --> 00:15:03,279
respective page table entries to the new

392
00:15:03,279 --> 00:15:05,839
location

393
00:15:05,839 --> 00:15:08,000
we evaluate zen swap on the server

394
00:15:08,000 --> 00:15:11,120
running ubu 20.04 and linux kernel

395
00:15:11,120 --> 00:15:13,199
version 5.12

396
00:15:13,199 --> 00:15:16,480
the server has 512 gigabytes of ram

397
00:15:16,480 --> 00:15:17,920
we compare the performance of two

398
00:15:17,920 --> 00:15:20,720
identical ssds where one utilizes its

399
00:15:20,720 --> 00:15:24,160
zns interface and the other utilizes the

400
00:15:24,160 --> 00:15:26,480
conventional block interface they both

401
00:15:26,480 --> 00:15:28,720
exhibit the same raw performance metrics

402
00:15:28,720 --> 00:15:30,560
such as sequential and random io

403
00:15:30,560 --> 00:15:33,120
operations

404
00:15:33,120 --> 00:15:35,040
we evaluate the swap performance of zn

405
00:15:35,040 --> 00:15:37,360
swap against the conventional ssd with

406
00:15:37,360 --> 00:15:39,760
and without trims enabled

407
00:15:39,760 --> 00:15:41,680
we run the vm scalability micro

408
00:15:41,680 --> 00:15:44,720
benchmark in a two gigabyte memory bound

409
00:15:44,720 --> 00:15:46,959
c group and measure the achieved swap

410
00:15:46,959 --> 00:15:48,880
out bandwidth which is a graph on the

411
00:15:48,880 --> 00:15:51,600
left and the right amplification factor

412
00:15:51,600 --> 00:15:53,360
which is on the right

413
00:15:53,360 --> 00:15:55,199
both of the graphs are as a function of

414
00:15:55,199 --> 00:15:57,600
the swap device utilization

415
00:15:57,600 --> 00:15:59,680
we can see in the left graph that zn

416
00:15:59,680 --> 00:16:01,759
swaps achieves a higher throughput under

417
00:16:01,759 --> 00:16:04,000
all device utilization ratios in a

418
00:16:04,000 --> 00:16:06,079
significantly lower rate amplification

419
00:16:06,079 --> 00:16:08,160
factor

420
00:16:08,160 --> 00:16:10,399
at the lower extreme with 10 device

421
00:16:10,399 --> 00:16:13,040
utilization both a traditional ssd and

422
00:16:13,040 --> 00:16:14,959
zn swap reach the maximum device

423
00:16:14,959 --> 00:16:16,959
bandwidth with a right amplification

424
00:16:16,959 --> 00:16:19,440
factor of only one which is no garbage

425
00:16:19,440 --> 00:16:20,720
production

426
00:16:20,720 --> 00:16:24,480
our host igc exhibits only a 0.3 percent

427
00:16:24,480 --> 00:16:28,160
single core cpu overhead

428
00:16:28,160 --> 00:16:31,680
at 50 utilization zn swap outperforms a

429
00:16:31,680 --> 00:16:34,399
traditional ssd by a factor of two along

430
00:16:34,399 --> 00:16:37,120
with two times lower right amplification

431
00:16:37,120 --> 00:16:40,480
this is achieved due to zngc's ability

432
00:16:40,480 --> 00:16:43,199
to avoid unnecessary data copies

433
00:16:43,199 --> 00:16:45,600
further we can see that native trim

434
00:16:45,600 --> 00:16:47,519
support does not have any measurable

435
00:16:47,519 --> 00:16:51,360
effects on both metrics

436
00:16:51,360 --> 00:16:54,000
the maximum cpu overhead we observed for

437
00:16:54,000 --> 00:16:57,839
the zmgc is 15 of a single cpu core even

438
00:16:57,839 --> 00:17:00,959
when the device utilization is very high

439
00:17:00,959 --> 00:17:03,519
enabling fine granularity trims at high

440
00:17:03,519 --> 00:17:07,679
device utilization exhibited the 32 cpu

441
00:17:07,679 --> 00:17:09,520
percent cpu overhead which is twice as

442
00:17:09,520 --> 00:17:12,720
much as the ngc's but unlike zngc it has

443
00:17:12,720 --> 00:17:14,240
no observable performance right

444
00:17:14,240 --> 00:17:17,039
amplification gains

445
00:17:17,039 --> 00:17:19,359
we also evaluated memcached serving

446
00:17:19,359 --> 00:17:21,839
facebook's etc workload in this

447
00:17:21,839 --> 00:17:24,480
experiment we utilize the entire 512

448
00:17:24,480 --> 00:17:26,480
gigabytes of memory available to the

449
00:17:26,480 --> 00:17:28,960
system and increase the memory footprint

450
00:17:28,960 --> 00:17:31,039
to utilize a swap device

451
00:17:31,039 --> 00:17:33,840
the y-axis on the left graph is the 99th

452
00:17:33,840 --> 00:17:36,480
percentile latency of server quests and

453
00:17:36,480 --> 00:17:39,840
on the right is the right amplification

454
00:17:39,840 --> 00:17:42,000
as before zn swap outperforms a

455
00:17:42,000 --> 00:17:45,039
traditional ssd with and without trends

456
00:17:45,039 --> 00:17:48,080
at 10 device utilization xeon swap has

457
00:17:48,080 --> 00:17:51,840
10 times lower 99th percentile and a 2.4

458
00:17:51,840 --> 00:17:53,679
lower right amplification

459
00:17:53,679 --> 00:17:55,360
this is all due to a more efficient and

460
00:17:55,360 --> 00:17:59,199
predictable whole site garbage collector

461
00:17:59,440 --> 00:18:02,160
in this experiment we evaluate zen swaps

462
00:18:02,160 --> 00:18:04,320
ability to isolate the performer the

463
00:18:04,320 --> 00:18:06,320
sword performance between tenants

464
00:18:06,320 --> 00:18:08,960
in the same setup as the analysis c

465
00:18:08,960 --> 00:18:11,039
group a performs a hundred percent reads

466
00:18:11,039 --> 00:18:13,360
and b performs a hundred percent right

467
00:18:13,360 --> 00:18:15,280
each are limited to half of the device's

468
00:18:15,280 --> 00:18:16,480
bandwidth

469
00:18:16,480 --> 00:18:18,720
we compare the performance of of the c

470
00:18:18,720 --> 00:18:20,799
groups in each configuration as well as

471
00:18:20,799 --> 00:18:22,080
the bandwidth that the garbage

472
00:18:22,080 --> 00:18:25,919
collection process consumed itself

473
00:18:25,919 --> 00:18:27,679
when comparing the left and middle

474
00:18:27,679 --> 00:18:30,559
columns we can see that zn swap exhibits

475
00:18:30,559 --> 00:18:32,720
lower garbage collection bandwidth and

476
00:18:32,720 --> 00:18:34,720
it is therefore able to achieve higher

477
00:18:34,720 --> 00:18:36,799
bandwidth for both c grips over the

478
00:18:36,799 --> 00:18:38,960
block ssd plus trim

479
00:18:38,960 --> 00:18:40,799
when the c group isolation mechanism is

480
00:18:40,799 --> 00:18:43,200
enabled on the right column

481
00:18:43,200 --> 00:18:45,360
each of the c group swap data is placed

482
00:18:45,360 --> 00:18:47,200
into different zones lowering the

483
00:18:47,200 --> 00:18:49,520
overall garbage collection bandwidth

484
00:18:49,520 --> 00:18:52,080
further the isolation mechanism pumps

485
00:18:52,080 --> 00:18:54,080
the garbage collection bandwidth to the

486
00:18:54,080 --> 00:18:55,760
c group that causes the garbage

487
00:18:55,760 --> 00:18:58,320
collection which is accounted for in the

488
00:18:58,320 --> 00:19:00,160
purse c group bandwidth throttle

489
00:19:00,160 --> 00:19:01,520
controls

490
00:19:01,520 --> 00:19:03,360
both c groups are able to achieve their

491
00:19:03,360 --> 00:19:06,159
bandwidth limits

492
00:19:06,240 --> 00:19:08,880
to conclude swap is regaining interest

493
00:19:08,880 --> 00:19:11,280
and current solutions for swap on ssds

494
00:19:11,280 --> 00:19:12,799
suffered from performance anomalies

495
00:19:12,799 --> 00:19:14,720
caused by the knowledge divide between

496
00:19:14,720 --> 00:19:16,880
the ssd and the host

497
00:19:16,880 --> 00:19:18,880
xeon swap regains control over key

498
00:19:18,880 --> 00:19:21,280
device mechanisms and allows the tight

499
00:19:21,280 --> 00:19:23,919
integration between ssd and oswap

500
00:19:23,919 --> 00:19:25,039
subsystem

501
00:19:25,039 --> 00:19:27,600
this is all enabled by the new zns ssd

502
00:19:27,600 --> 00:19:28,960
devices

503
00:19:28,960 --> 00:19:31,039
i'll gladly accept any questions live

504
00:19:31,039 --> 00:19:34,240
offline via email and thank you so much

505
00:19:34,240 --> 00:19:38,360
and back to you

506
00:19:43,200 --> 00:19:45,280
you

