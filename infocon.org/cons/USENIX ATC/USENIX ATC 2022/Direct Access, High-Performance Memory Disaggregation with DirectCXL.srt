1
00:00:13,040 --> 00:00:15,200
hello everyone i'm donghyun from

2
00:00:15,200 --> 00:00:18,000
kaist and my advisor is myeong soo zhang

3
00:00:18,000 --> 00:00:19,920
who sleeps computer architecture and

4
00:00:19,920 --> 00:00:23,039
memory systems laboratory today i will

5
00:00:23,039 --> 00:00:25,439
introduce our work direct access high

6
00:00:25,439 --> 00:00:27,359
performance memory deceleration with

7
00:00:27,359 --> 00:00:29,519
direct cxl

8
00:00:29,519 --> 00:00:31,599
before moving on to the main talk i will

9
00:00:31,599 --> 00:00:33,280
give you a higher level summary of the

10
00:00:33,280 --> 00:00:34,399
talk

11
00:00:34,399 --> 00:00:36,719
our work's main research target is

12
00:00:36,719 --> 00:00:39,440
memory disaggregation which can satisfy

13
00:00:39,440 --> 00:00:42,239
large memory requirements of various big

14
00:00:42,239 --> 00:00:45,559
data applications

15
00:00:46,559 --> 00:00:50,000
existing memory desegregation use rdma

16
00:00:50,000 --> 00:00:52,640
but rdma requires high monetary cost for

17
00:00:52,640 --> 00:00:54,879
specialized network interface card and

18
00:00:54,879 --> 00:00:55,760
switch

19
00:00:55,760 --> 00:00:57,760
and it also degrades application

20
00:00:57,760 --> 00:01:00,000
performance because of microsecond level

21
00:01:00,000 --> 00:01:02,719
of memory access latency

22
00:01:02,719 --> 00:01:05,519
instead of rdma we suggest to use cxr

23
00:01:05,519 --> 00:01:07,600
for memory disaggregation

24
00:01:07,600 --> 00:01:09,920
cxr can reduce monetary cost for memory

25
00:01:09,920 --> 00:01:11,920
disaggregation and minimize the

26
00:01:11,920 --> 00:01:13,680
performance degradation

27
00:01:13,680 --> 00:01:16,400
however cxl enabled system is not

28
00:01:16,400 --> 00:01:19,040
available for now

29
00:01:19,040 --> 00:01:21,759
in this work we designed memory dissolve

30
00:01:21,759 --> 00:01:23,520
we designed cxl based memory

31
00:01:23,520 --> 00:01:26,960
disaggregation direct cxl direct cxl

32
00:01:26,960 --> 00:01:30,320
implements cxl enabled host cx-celled

33
00:01:30,320 --> 00:01:33,280
memory device and cxl switch

34
00:01:33,280 --> 00:01:35,360
we also modified the linux scanner to

35
00:01:35,360 --> 00:01:37,840
expose cxl test memory to the user level

36
00:01:37,840 --> 00:01:39,119
application

37
00:01:39,119 --> 00:01:41,119
our evaluation

38
00:01:41,119 --> 00:01:42,079
using

39
00:01:42,079 --> 00:01:44,000
real hardware prototype shows three

40
00:01:44,000 --> 00:01:46,399
times better performance than rdma based

41
00:01:46,399 --> 00:01:49,280
memory disaggregation

42
00:01:49,280 --> 00:01:52,720
now i'll explain what the cxl is and how

43
00:01:52,720 --> 00:01:54,399
cxl can be used for memory

44
00:01:54,399 --> 00:01:56,960
disaggregation

45
00:01:56,960 --> 00:02:00,479
the compute express link or cxl is an

46
00:02:00,479 --> 00:02:03,200
open industry standard for cash coherent

47
00:02:03,200 --> 00:02:06,640
interconnect based on pci express

48
00:02:06,640 --> 00:02:10,318
cxl allows device to get data stored in

49
00:02:10,318 --> 00:02:12,480
host memory without breaking cache

50
00:02:12,480 --> 00:02:13,680
currency

51
00:02:13,680 --> 00:02:16,879
this kind of device is defined as type 1

52
00:02:16,879 --> 00:02:19,440
in cxl and the core processor can be

53
00:02:19,440 --> 00:02:22,640
example of type 1 device

54
00:02:22,640 --> 00:02:25,280
cxl also allows host to cache data

55
00:02:25,280 --> 00:02:28,000
stored in device memory and such device

56
00:02:28,000 --> 00:02:30,800
is called type 3.

57
00:02:30,800 --> 00:02:33,680
type 2 device mixes type 1 and type 3

58
00:02:33,680 --> 00:02:36,000
and an accelerator with internal memory

59
00:02:36,000 --> 00:02:40,640
like gpu can be example of type 2 device

60
00:02:40,640 --> 00:02:43,680
like this cxl supports various type of

61
00:02:43,680 --> 00:02:46,319
device ranging from accelerators to

62
00:02:46,319 --> 00:02:48,720
memory

63
00:02:48,720 --> 00:02:51,680
in this work we focus on cxl.net

64
00:02:51,680 --> 00:02:54,400
protocol with type 3 devices for the

65
00:02:54,400 --> 00:02:56,959
memory memory expansion which allows

66
00:02:56,959 --> 00:02:59,680
host to access memory with load store

67
00:02:59,680 --> 00:03:02,080
instruction

68
00:03:02,080 --> 00:03:05,280
from now on rso benefits of cxl based

69
00:03:05,280 --> 00:03:08,560
memory disaggregation compared to rdma

70
00:03:08,560 --> 00:03:12,400
in both hardware and software viewpoint

71
00:03:12,400 --> 00:03:14,800
from the hardware viewpoint rdma

72
00:03:14,800 --> 00:03:17,040
requires the network connections such as

73
00:03:17,040 --> 00:03:18,560
impediment

74
00:03:18,560 --> 00:03:21,280
so we need network interface card with

75
00:03:21,280 --> 00:03:24,800
rdma support called rniq

76
00:03:24,800 --> 00:03:27,760
in contrast cxl enables direct access to

77
00:03:27,760 --> 00:03:29,680
the remote memory

78
00:03:29,680 --> 00:03:32,799
but we need cxl enabled cpu and cxl

79
00:03:32,799 --> 00:03:36,159
controller for the device

80
00:03:36,239 --> 00:03:37,920
we compared

81
00:03:37,920 --> 00:03:40,799
single cache line load latency of rdma

82
00:03:40,799 --> 00:03:42,400
and direct cxl

83
00:03:42,400 --> 00:03:45,120
let's see the rdmi case first

84
00:03:45,120 --> 00:03:48,080
the data is stored in remote dram

85
00:03:48,080 --> 00:03:50,959
so cpu will read the data and complete

86
00:03:50,959 --> 00:03:53,360
dma request of rnet

87
00:03:53,360 --> 00:03:55,280
the rnet will send data through the

88
00:03:55,280 --> 00:03:57,680
network and another arnic at the host

89
00:03:57,680 --> 00:04:00,480
site writes received data to the local

90
00:04:00,480 --> 00:04:03,920
dram using the dma

91
00:04:03,920 --> 00:04:06,879
for the cxl case it only access remote

92
00:04:06,879 --> 00:04:10,080
gram when cash miss occurs

93
00:04:10,080 --> 00:04:12,720
the data stored in the remote memory is

94
00:04:12,720 --> 00:04:15,360
loaded to the cxl controller and the

95
00:04:15,360 --> 00:04:17,040
controller sends the data to the host

96
00:04:17,040 --> 00:04:18,880
cpu

97
00:04:18,880 --> 00:04:20,639
as load store instruction can be

98
00:04:20,639 --> 00:04:23,360
directly handled by cxl device direct

99
00:04:23,360 --> 00:04:26,080
selector is about 8 times faster than

100
00:04:26,080 --> 00:04:29,080
rdma

101
00:04:29,600 --> 00:04:31,840
from the software viewpoint rdma

102
00:04:31,840 --> 00:04:33,199
requires additional software

103
00:04:33,199 --> 00:04:35,919
intervention when application calls are

104
00:04:35,919 --> 00:04:38,800
dma function user level rdma libraries

105
00:04:38,800 --> 00:04:42,479
must be involved to control the rna

106
00:04:42,479 --> 00:04:45,360
in contrast cxl does not introduce any

107
00:04:45,360 --> 00:04:48,080
software overhead as it access remote

108
00:04:48,080 --> 00:04:50,720
memory using load storage instruction

109
00:04:50,720 --> 00:04:53,199
as a result the performance benefit is

110
00:04:53,199 --> 00:04:57,840
increases to about 24 times

111
00:04:57,840 --> 00:05:00,240
in this section i will explain how we

112
00:05:00,240 --> 00:05:02,880
design the direct cxl by answering

113
00:05:02,880 --> 00:05:05,040
design questions

114
00:05:05,040 --> 00:05:08,080
first design question is how cpu can

115
00:05:08,080 --> 00:05:10,960
directly access remote memory

116
00:05:10,960 --> 00:05:13,680
second question is how to realize memory

117
00:05:13,680 --> 00:05:16,639
disaggregation using cxl

118
00:05:16,639 --> 00:05:19,280
and last question is how application can

119
00:05:19,280 --> 00:05:23,120
use cxl authentic remote memory

120
00:05:23,120 --> 00:05:25,280
let's begin with the first question

121
00:05:25,280 --> 00:05:28,320
to enable cpu to access remote memory

122
00:05:28,320 --> 00:05:30,880
the load store instruction issued by the

123
00:05:30,880 --> 00:05:35,120
cpu is directly converted to cxl packet

124
00:05:35,120 --> 00:05:37,520
we prototyped cxl cpu on high

125
00:05:37,520 --> 00:05:40,160
performance accelerator card and cxl

126
00:05:40,160 --> 00:05:44,000
memory device on customize the pj board

127
00:05:44,000 --> 00:05:46,960
when application running on the cxl cpu

128
00:05:46,960 --> 00:05:48,800
is this memory request

129
00:05:48,800 --> 00:05:51,520
the cpu will convert the memory request

130
00:05:51,520 --> 00:05:54,960
into cxl packet called cxl fleet

131
00:05:54,960 --> 00:05:57,120
when the cxl fleet arrived in the cxl

132
00:05:57,120 --> 00:05:59,919
memory device it converts cxl fleet back

133
00:05:59,919 --> 00:06:02,080
to the memory request and serves using

134
00:06:02,080 --> 00:06:04,880
underlying dram

135
00:06:04,880 --> 00:06:08,319
the cxl cpu implements cxl root port

136
00:06:08,319 --> 00:06:11,520
which extends pci express root port with

137
00:06:11,520 --> 00:06:13,280
cxl support

138
00:06:13,280 --> 00:06:15,120
in case of cache miss

139
00:06:15,120 --> 00:06:17,680
the root cxl root port will convert

140
00:06:17,680 --> 00:06:20,880
memory request into cxl fleet

141
00:06:20,880 --> 00:06:23,440
the cxl memory device implements cxl

142
00:06:23,440 --> 00:06:26,080
endpoint which extends pci express

143
00:06:26,080 --> 00:06:28,960
endpoint with cxl support

144
00:06:28,960 --> 00:06:31,680
it converts incoming cxl fleet into the

145
00:06:31,680 --> 00:06:33,520
memory request so that the memory

146
00:06:33,520 --> 00:06:35,360
controller can understand the memory

147
00:06:35,360 --> 00:06:37,840
request

148
00:06:38,000 --> 00:06:40,560
to realize memory disaggregation direct

149
00:06:40,560 --> 00:06:43,039
cxl implements reconfigurable sheets

150
00:06:43,039 --> 00:06:46,080
which supports multiple sexual cpus and

151
00:06:46,080 --> 00:06:48,800
cxl memory devices

152
00:06:48,800 --> 00:06:51,680
our cxlr60 use same hardware platform

153
00:06:51,680 --> 00:06:53,360
with cxl cpu

154
00:06:53,360 --> 00:06:56,240
and it supports four cxl cpus and four

155
00:06:56,240 --> 00:06:59,120
cxl memory devices

156
00:06:59,120 --> 00:07:00,800
the cxlcp

157
00:07:00,800 --> 00:07:02,639
sorry the cxl switch includes

158
00:07:02,639 --> 00:07:04,800
reconfigurable crossbar so the

159
00:07:04,800 --> 00:07:07,919
connection among cxl cpus and cxl memory

160
00:07:07,919 --> 00:07:10,080
device can be changed

161
00:07:10,080 --> 00:07:12,479
user can configure the crossbar through

162
00:07:12,479 --> 00:07:14,160
the fabric manager

163
00:07:14,160 --> 00:07:16,319
after the configuration the crossbar

164
00:07:16,319 --> 00:07:19,039
will connect cxl memory devices and cxl

165
00:07:19,039 --> 00:07:20,400
cpu

166
00:07:20,400 --> 00:07:22,319
and the configuration can be changed on

167
00:07:22,319 --> 00:07:24,560
the runtime using hot plug and hard

168
00:07:24,560 --> 00:07:27,280
remove feature

169
00:07:27,360 --> 00:07:29,840
to expose cxl attached memory space to

170
00:07:29,840 --> 00:07:32,000
user we leveraged

171
00:07:32,000 --> 00:07:35,840
linux memory management system

172
00:07:36,160 --> 00:07:38,639
software runtime of direct cxl is

173
00:07:38,639 --> 00:07:40,479
consists of enumeration phase

174
00:07:40,479 --> 00:07:43,440
initialization phase and usage phase

175
00:07:43,440 --> 00:07:46,000
when linux pci bus driver performs

176
00:07:46,000 --> 00:07:49,199
enumeration our csr host bridge driver

177
00:07:49,199 --> 00:07:52,080
discovers cxl memory devices

178
00:07:52,080 --> 00:07:54,560
remote memory of each cxl memory device

179
00:07:54,560 --> 00:07:57,039
is mapped to the system and so the cpu

180
00:07:57,039 --> 00:07:59,280
can issue memory request to the remote

181
00:07:59,280 --> 00:08:01,199
memory

182
00:08:01,199 --> 00:08:03,599
our cxl memory device driver is loaded

183
00:08:03,599 --> 00:08:06,319
after enumeration and the driver exposes

184
00:08:06,319 --> 00:08:08,080
character device to interact with user

185
00:08:08,080 --> 00:08:09,840
level application

186
00:08:09,840 --> 00:08:13,120
user can create namespace through ioctl

187
00:08:13,120 --> 00:08:14,960
and user can purple nmap on the

188
00:08:14,960 --> 00:08:17,280
namespace device to use the cxl assets

189
00:08:17,280 --> 00:08:19,440
memory space

190
00:08:19,440 --> 00:08:22,080
the mf handler of cxl device driver will

191
00:08:22,080 --> 00:08:23,919
make physical address space of remote

192
00:08:23,919 --> 00:08:26,080
memory into the user's virtual memory

193
00:08:26,080 --> 00:08:27,280
address space

194
00:08:27,280 --> 00:08:29,039
so the user can easily load the store

195
00:08:29,039 --> 00:08:31,360
instruction to email memory to use the

196
00:08:31,360 --> 00:08:34,880
excel or test remote memory

197
00:08:34,958 --> 00:08:39,440
finally i resolve two demo videos

198
00:08:39,440 --> 00:08:41,679
in the first demo video i will compare

199
00:08:41,679 --> 00:08:43,919
the performance of recommendation system

200
00:08:43,919 --> 00:08:46,399
using instant rdma based solution and

201
00:08:46,399 --> 00:08:48,560
direct cxl

202
00:08:48,560 --> 00:08:52,399
we use the dlr mode dlr model from meta

203
00:08:52,399 --> 00:08:54,480
and the whole embedding table is stored

204
00:08:54,480 --> 00:08:57,519
in the remote memory

205
00:08:59,200 --> 00:09:01,040
on the left side there are four

206
00:09:01,040 --> 00:09:03,760
customized fpga boards or cxl memory

207
00:09:03,760 --> 00:09:07,120
devices and on the upper right side four

208
00:09:07,120 --> 00:09:10,160
accelerator cars for use cxl host and

209
00:09:10,160 --> 00:09:13,439
1.6 switch

210
00:09:15,600 --> 00:09:17,600
each terminal shows the booting process

211
00:09:17,600 --> 00:09:18,959
of each host

212
00:09:18,959 --> 00:09:21,519
and its host local memory is one

213
00:09:21,519 --> 00:09:22,399
gigabyte

214
00:09:22,399 --> 00:09:24,720
and i add one additional domain node for

215
00:09:24,720 --> 00:09:27,600
remote memory

216
00:09:28,000 --> 00:09:29,760
our direct gesture driver will

217
00:09:29,760 --> 00:09:32,240
communicate with fabric major to add

218
00:09:32,240 --> 00:09:34,800
attach or detach memory devices

219
00:09:34,800 --> 00:09:37,519
for the comparison we use the same

220
00:09:37,519 --> 00:09:40,240
accelerator cars for rdma system

221
00:09:40,240 --> 00:09:42,720
we used infiniband fdr for our demi

222
00:09:42,720 --> 00:09:45,839
network and one mvm ssd for creating

223
00:09:45,839 --> 00:09:48,480
creating subpartition

224
00:09:48,480 --> 00:09:51,200
each node has 48 gigabytes of system

225
00:09:51,200 --> 00:09:54,160
memory and impedance is initialized

226
00:09:54,160 --> 00:09:56,640
correctly

227
00:09:58,480 --> 00:10:01,200
we run dlrm workload and compare the

228
00:10:01,200 --> 00:10:03,360
execution time of each step

229
00:10:03,360 --> 00:10:05,760
in the tensor initialization phase

230
00:10:05,760 --> 00:10:07,519
we fill the embedding table with the

231
00:10:07,519 --> 00:10:10,399
random values

232
00:10:12,560 --> 00:10:14,800
in the inference phase we performed 10

233
00:10:14,800 --> 00:10:17,800
inferences

234
00:10:22,320 --> 00:10:25,120
direct cxr was 2.8 times faster than

235
00:10:25,120 --> 00:10:28,560
rdma in inference phase and 33.3 times

236
00:10:28,560 --> 00:10:32,680
faster in end to end

237
00:10:32,959 --> 00:10:35,600
in the second second video i will show

238
00:10:35,600 --> 00:10:38,079
how cxr switch can be used for memory

239
00:10:38,079 --> 00:10:40,480
disaggregation

240
00:10:40,480 --> 00:10:43,279
on each host porous excel memory devices

241
00:10:43,279 --> 00:10:45,200
are available to reuse

242
00:10:45,200 --> 00:10:48,000
and each host has 900 megabytes of

243
00:10:48,000 --> 00:10:50,240
system memory

244
00:10:50,240 --> 00:10:52,880
i will attest one cxl memory device for

245
00:10:52,880 --> 00:10:56,120
each host

246
00:11:02,160 --> 00:11:04,320
after attached the system memory is

247
00:11:04,320 --> 00:11:07,040
increased to 17 gigabytes and the 16

248
00:11:07,040 --> 00:11:09,760
gigabytes gigabytes of remote memory is

249
00:11:09,760 --> 00:11:12,959
visible as newer node 1.

250
00:11:12,959 --> 00:11:14,640
assume that the

251
00:11:14,640 --> 00:11:17,600
host a requires more memory so i will

252
00:11:17,600 --> 00:11:20,640
detect the memory of both c and d and

253
00:11:20,640 --> 00:11:24,319
attach them to host a

254
00:11:26,240 --> 00:11:28,320
after details the system memory is

255
00:11:28,320 --> 00:11:32,160
decreased to 900 megabytes

256
00:11:32,720 --> 00:11:34,000
and

257
00:11:34,000 --> 00:11:36,160
after attached the system memory is

258
00:11:36,160 --> 00:11:38,880
increased to 49 gigabytes and 48

259
00:11:38,880 --> 00:11:41,120
gigabytes of remote memory is visible in

260
00:11:41,120 --> 00:11:43,839
luma note 1.

261
00:11:45,839 --> 00:11:48,480
in this work we showed direct cxl the

262
00:11:48,480 --> 00:11:51,040
world first real cxl version 2 system

263
00:11:51,040 --> 00:11:53,040
for memory disaggregation

264
00:11:53,040 --> 00:11:55,440
it supports cxl from bottom to top

265
00:11:55,440 --> 00:11:58,079
including hardware and software runtimes

266
00:11:58,079 --> 00:12:00,320
also it reduces monetary cost for

267
00:12:00,320 --> 00:12:03,120
directly accessing remote memory

268
00:12:03,120 --> 00:12:05,040
and it achieves high performance remote

269
00:12:05,040 --> 00:12:06,720
memory access

270
00:12:06,720 --> 00:12:08,560
this is sent on my talks thank you for

271
00:12:08,560 --> 00:12:11,560
listening

