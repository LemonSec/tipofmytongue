1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:13,080 --> 00:00:15,120
hello I'm Georgie bear from qinghan

3
00:00:15,120 --> 00:00:17,460
University I'm very glad to share our

4
00:00:17,460 --> 00:00:20,220
paper about data log detection in OS

5
00:00:20,220 --> 00:00:22,640
kernels

6
00:00:23,100 --> 00:00:26,519
data logs in OS kernels are caused by

7
00:00:26,519 --> 00:00:28,260
locking circles

8
00:00:28,260 --> 00:00:30,800
in concurrent

9
00:00:30,800 --> 00:00:34,320
threats namely one thread holds a lock

10
00:00:34,320 --> 00:00:39,000
that other concurrent threats want to

11
00:00:39,000 --> 00:00:40,500
acquire

12
00:00:40,500 --> 00:00:44,280
data logs are harder to find due to the

13
00:00:44,280 --> 00:00:48,600
no determinism of Kernel concurrency

14
00:00:48,600 --> 00:00:51,079
data logs can cause

15
00:00:51,079 --> 00:00:54,420
performance degradation and even system

16
00:00:54,420 --> 00:00:56,879
costs

17
00:00:56,879 --> 00:00:59,940
there is an example deadlock in Linux

18
00:00:59,940 --> 00:01:03,300
PTR FS file system

19
00:01:03,300 --> 00:01:07,380
this bug existed for over four years and

20
00:01:07,380 --> 00:01:13,140
it was fatally fixed in Linux 5.9

21
00:01:13,560 --> 00:01:16,640
this bug is hard to find because

22
00:01:16,640 --> 00:01:20,280
multiple functions are involved and the

23
00:01:20,280 --> 00:01:23,100
related threat in the living is in

24
00:01:23,100 --> 00:01:26,720
frequent at the wrong time

25
00:01:27,060 --> 00:01:29,939
to detect the Deadlocks we need to

26
00:01:29,939 --> 00:01:33,780
perform two basic steps

27
00:01:33,780 --> 00:01:36,900
first we need to extract working

28
00:01:36,900 --> 00:01:40,500
constraints in concurrent threats or

29
00:01:40,500 --> 00:01:41,939
code paths

30
00:01:41,939 --> 00:01:45,420
a locking constraint describes the

31
00:01:45,420 --> 00:01:48,659
dependency of the health logs and the

32
00:01:48,659 --> 00:01:51,899
Concur and the currently a catalog

33
00:01:51,899 --> 00:01:55,320
uh second we need to detect locking

34
00:01:55,320 --> 00:01:57,960
circles according to the extracted

35
00:01:57,960 --> 00:02:02,939
logging constraints in concurrent stress

36
00:02:02,939 --> 00:02:04,619
or code pass

37
00:02:04,619 --> 00:02:07,940
a data log can occur in two threads

38
00:02:07,940 --> 00:02:09,500
namely

39
00:02:09,500 --> 00:02:12,920
apba data log or three

40
00:02:12,920 --> 00:02:18,720
threads namely a b b c c a data log and

41
00:02:18,720 --> 00:02:21,200
so on

42
00:02:22,620 --> 00:02:26,160
many existing approaches use Dynamic

43
00:02:26,160 --> 00:02:29,220
analysis for data log detection

44
00:02:29,220 --> 00:02:32,640
and the most of them are designed for

45
00:02:32,640 --> 00:02:36,140
user level applications

46
00:02:36,360 --> 00:02:40,020
Dynamic analysis can achieve low false

47
00:02:40,020 --> 00:02:42,360
positives and a supportive data log

48
00:02:42,360 --> 00:02:45,000
reproduction

49
00:02:45,000 --> 00:02:48,019
however the name analysis requires

50
00:02:48,019 --> 00:02:52,099
substantial test cases to achieve good

51
00:02:52,099 --> 00:02:54,360
testing coverage

52
00:02:54,360 --> 00:02:58,319
and it also introduces runtime overhead

53
00:02:58,319 --> 00:03:00,540
neuron testing

54
00:03:00,540 --> 00:03:04,080
a typical kernel Dynamic tool is lock

55
00:03:04,080 --> 00:03:07,860
depth which has been widely used in the

56
00:03:07,860 --> 00:03:09,540
Linux kernel

57
00:03:09,540 --> 00:03:12,300
it performs long-time monitoring and

58
00:03:12,300 --> 00:03:13,440
tracking

59
00:03:13,440 --> 00:03:18,239
based on the granularity of log class

60
00:03:18,239 --> 00:03:20,480
according to log

61
00:03:20,480 --> 00:03:24,500
dependencies and rules

62
00:03:25,500 --> 00:03:27,900
some approaches

63
00:03:27,900 --> 00:03:30,120
you'll study analysis for data log

64
00:03:30,120 --> 00:03:31,200
detection

65
00:03:31,200 --> 00:03:34,140
and then most of them are designed for

66
00:03:34,140 --> 00:03:36,900
user level applications

67
00:03:36,900 --> 00:03:40,019
static analysis can achieve good

68
00:03:40,019 --> 00:03:43,519
detection coverage without actually

69
00:03:43,519 --> 00:03:47,580
running the program and test cases

70
00:03:47,580 --> 00:03:51,060
however it often introduces

71
00:03:51,060 --> 00:03:52,799
false positives

72
00:03:52,799 --> 00:03:55,560
and the lacks the support of data log

73
00:03:55,560 --> 00:03:58,280
reproduction

74
00:03:58,620 --> 00:04:01,440
so through static approach for kernel

75
00:04:01,440 --> 00:04:04,560
data log detection is Racer X

76
00:04:04,560 --> 00:04:07,680
it performs solosensitive and the

77
00:04:07,680 --> 00:04:10,739
inter-procity analysis of the kernel

78
00:04:10,739 --> 00:04:12,920
code

79
00:04:12,959 --> 00:04:16,199
and then it detects locking circles

80
00:04:16,199 --> 00:04:18,959
as deadlocks

81
00:04:18,959 --> 00:04:22,320
however resources has a high false

82
00:04:22,320 --> 00:04:26,699
positive rate of 46 percent

83
00:04:26,699 --> 00:04:29,880
which limits as Effectiveness in

84
00:04:29,880 --> 00:04:32,160
Practical use

85
00:04:32,160 --> 00:04:33,840
in this work

86
00:04:33,840 --> 00:04:37,680
we aim to design a new static approach

87
00:04:37,680 --> 00:04:40,199
of Kernel data log detection

88
00:04:40,199 --> 00:04:42,780
to achieve better accuracy and

89
00:04:42,780 --> 00:04:45,440
efficiency

90
00:04:47,280 --> 00:04:49,800
from start kernel data log detection

91
00:04:49,800 --> 00:04:54,060
there are three important challenges

92
00:04:54,060 --> 00:04:55,139
first

93
00:04:55,139 --> 00:04:56,960
when extracting

94
00:04:56,960 --> 00:05:02,100
locking constraints it is challenging to

95
00:05:02,100 --> 00:05:04,919
ensure both the accuracy and the

96
00:05:04,919 --> 00:05:07,460
efficiency

97
00:05:09,660 --> 00:05:13,740
especially when the kernel code is large

98
00:05:13,740 --> 00:05:17,280
second when detecting unlocking circles

99
00:05:17,280 --> 00:05:20,840
it is challenging to compare numerous

100
00:05:20,840 --> 00:05:23,759
locking constraints in loss of code

101
00:05:23,759 --> 00:05:25,320
paths

102
00:05:25,320 --> 00:05:28,800
finally to achieve better accuracy it is

103
00:05:28,800 --> 00:05:32,340
changing to drop Force fossils with

104
00:05:32,340 --> 00:05:35,599
short time usage

105
00:05:36,060 --> 00:05:40,560
to solve this three changes we propose

106
00:05:40,560 --> 00:05:42,960
three key techniques

107
00:05:42,960 --> 00:05:46,860
first we propose a summary based log

108
00:05:46,860 --> 00:05:49,860
usage analysis to extract the target

109
00:05:49,860 --> 00:05:53,880
code path containing distinct locking

110
00:05:53,880 --> 00:05:55,320
constraints

111
00:05:55,320 --> 00:05:58,440
second we propose a reachability based

112
00:05:58,440 --> 00:06:01,560
comparison method to detect locking

113
00:06:01,560 --> 00:06:05,820
circles from locking constructs

114
00:06:05,820 --> 00:06:09,620
finally we propose a two-dimensional

115
00:06:09,620 --> 00:06:12,780
filtering strategy to drop false

116
00:06:12,780 --> 00:06:14,039
positives

117
00:06:14,039 --> 00:06:17,639
by validating code path visibility

118
00:06:17,639 --> 00:06:20,780
and a concurrency

119
00:06:21,240 --> 00:06:25,259
now let's see how to extract the login

120
00:06:25,259 --> 00:06:30,800
constraints using our log usage analysis

121
00:06:31,380 --> 00:06:34,380
the first step is to collect the target

122
00:06:34,380 --> 00:06:38,100
code pass that contain log related

123
00:06:38,100 --> 00:06:40,680
operations

124
00:06:40,680 --> 00:06:45,060
our analysis use a fluency

125
00:06:45,060 --> 00:06:48,539
field sensitive and interpret video

126
00:06:48,539 --> 00:06:51,539
analysis of the kernel code

127
00:06:51,539 --> 00:06:55,500
it also uses a Anderson steel aliens

128
00:06:55,500 --> 00:06:56,819
analysis

129
00:06:56,819 --> 00:07:02,539
to identify against the log variables

130
00:07:02,539 --> 00:07:05,340
to improve accuracy

131
00:07:05,340 --> 00:07:10,860
our analysis create and reuses function

132
00:07:10,860 --> 00:07:12,120
summaries

133
00:07:12,120 --> 00:07:14,600
to reduce repeated

134
00:07:14,600 --> 00:07:18,240
early analysis

135
00:07:18,240 --> 00:07:23,160
uh it also drops Target code pass which

136
00:07:23,160 --> 00:07:27,919
contain repeated log related operations

137
00:07:28,680 --> 00:07:33,300
there is an example of Linux effs fail

138
00:07:33,300 --> 00:07:34,500
system code

139
00:07:34,500 --> 00:07:36,180
in this example

140
00:07:36,180 --> 00:07:40,860
we find that the functions effx free

141
00:07:40,860 --> 00:07:45,720
block and the effs analog block both

142
00:07:45,720 --> 00:07:52,319
called the function effs on Mark SP 30.

143
00:07:53,280 --> 00:07:57,860
so when the function is analyzed

144
00:07:57,860 --> 00:08:01,199
we create a function summary for it

145
00:08:01,199 --> 00:08:04,800
this function summary stores the target

146
00:08:04,800 --> 00:08:06,300
code path

147
00:08:06,300 --> 00:08:10,500
uh and law created operations in this

148
00:08:10,500 --> 00:08:12,919
function

149
00:08:13,080 --> 00:08:16,800
so we can reuse this macro summary when

150
00:08:16,800 --> 00:08:18,900
the function is caught by other

151
00:08:18,900 --> 00:08:21,500
functions

152
00:08:23,520 --> 00:08:26,819
after collecting Target code paths we

153
00:08:26,819 --> 00:08:30,120
need to compute login constraints in

154
00:08:30,120 --> 00:08:32,339
each code path

155
00:08:32,339 --> 00:08:35,940
we use a static log site analysis to

156
00:08:35,940 --> 00:08:39,240
handle the cases of occurring and

157
00:08:39,240 --> 00:08:42,120
releasing logs

158
00:08:42,120 --> 00:08:45,720
when a log is acquired we create new

159
00:08:45,720 --> 00:08:48,839
locking constraints and add the lock in

160
00:08:48,839 --> 00:08:50,459
the log site

161
00:08:50,459 --> 00:08:54,779
when a log is released refund and drops

162
00:08:54,779 --> 00:08:58,459
the lock in the log site

163
00:09:00,360 --> 00:09:03,300
with the Locking constraints and Target

164
00:09:03,300 --> 00:09:04,860
code pass

165
00:09:04,860 --> 00:09:07,680
we need to compare them for locking

166
00:09:07,680 --> 00:09:10,640
cycle detection

167
00:09:10,800 --> 00:09:13,860
the first step is to identify the same

168
00:09:13,860 --> 00:09:16,680
logs in different code paths

169
00:09:16,680 --> 00:09:20,279
in this step we use a field-based

170
00:09:20,279 --> 00:09:23,459
analysis according to data structure

171
00:09:23,459 --> 00:09:25,800
type and field

172
00:09:25,800 --> 00:09:28,980
the second step is important namely

173
00:09:28,980 --> 00:09:31,620
comparing locking constraints in

174
00:09:31,620 --> 00:09:34,080
different code paths to detect the

175
00:09:34,080 --> 00:09:37,399
possible data logs

176
00:09:37,680 --> 00:09:40,620
so traditional method used by Reser X

177
00:09:40,620 --> 00:09:43,920
and other existing approaches is very

178
00:09:43,920 --> 00:09:45,600
simple

179
00:09:45,600 --> 00:09:49,320
this method starts the comparison from

180
00:09:49,320 --> 00:09:51,660
each locking constraint

181
00:09:51,660 --> 00:09:55,440
and compares the current knocking

182
00:09:55,440 --> 00:09:58,380
constraint with with each locking

183
00:09:58,380 --> 00:10:02,279
constant in other code paths

184
00:10:02,279 --> 00:10:06,480
if matched the method replaces the

185
00:10:06,480 --> 00:10:09,180
current location constant with the

186
00:10:09,180 --> 00:10:10,920
Matched one

187
00:10:10,920 --> 00:10:14,100
if not matched it selects another

188
00:10:14,100 --> 00:10:18,920
locking constraints for comparison

189
00:10:20,519 --> 00:10:22,560
there is an example

190
00:10:22,560 --> 00:10:25,620
which contains four code paths

191
00:10:25,620 --> 00:10:28,860
each code path has Just One login

192
00:10:28,860 --> 00:10:30,480
constraint

193
00:10:30,480 --> 00:10:33,180
let's start the compilation from the

194
00:10:33,180 --> 00:10:34,440
coreopath

195
00:10:34,440 --> 00:10:37,019
pp1

196
00:10:37,019 --> 00:10:39,959
the detailed steps are shown in the

197
00:10:39,959 --> 00:10:42,140
figure

198
00:10:43,980 --> 00:10:46,880
after the code pass tp1 is finished

199
00:10:46,880 --> 00:10:50,459
let's start the comparison from the code

200
00:10:50,459 --> 00:10:52,440
pass tp2

201
00:10:52,440 --> 00:10:54,839
the detailed steps are shown in the

202
00:10:54,839 --> 00:10:57,019
figure

203
00:10:59,519 --> 00:11:03,300
from these steps we find that some of

204
00:11:03,300 --> 00:11:04,980
them are repeated

205
00:11:04,980 --> 00:11:09,300
which are shown in both words if this

206
00:11:09,300 --> 00:11:11,279
repeated compilation

207
00:11:11,279 --> 00:11:14,100
can be reduced

208
00:11:14,100 --> 00:11:16,980
blocking Circle detection can be speeded

209
00:11:16,980 --> 00:11:19,100
up

210
00:11:20,700 --> 00:11:23,760
based on this idea we introduce a new

211
00:11:23,760 --> 00:11:26,519
structure named in direct login

212
00:11:26,519 --> 00:11:28,399
constraint

213
00:11:28,399 --> 00:11:31,680
it combines multiple locking constraints

214
00:11:31,680 --> 00:11:34,140
for a reachable node

215
00:11:34,140 --> 00:11:38,660
the definition is shown in the equation

216
00:11:40,800 --> 00:11:43,200
let's see the mentioned example about

217
00:11:43,200 --> 00:11:46,459
the mentioned example again

218
00:11:46,459 --> 00:11:49,920
we still start the commission from the

219
00:11:49,920 --> 00:11:54,360
code pass tb1 and add some indirect

220
00:11:54,360 --> 00:11:56,279
locking constraints

221
00:11:56,279 --> 00:11:59,779
during the comparison

222
00:12:00,120 --> 00:12:03,800
so edit our login customers are showing

223
00:12:03,800 --> 00:12:08,240
in Bound words

224
00:12:08,640 --> 00:12:12,180
after tp1 is finished

225
00:12:12,180 --> 00:12:15,300
we still starts the commemoration from

226
00:12:15,300 --> 00:12:17,100
the code pass tb2

227
00:12:17,100 --> 00:12:20,339
now we can use the indirect login

228
00:12:20,339 --> 00:12:22,860
constraints in tpy

229
00:12:22,860 --> 00:12:25,920
and strongly in both words

230
00:12:25,920 --> 00:12:28,920
to reduce repeated collaboration

231
00:12:28,920 --> 00:12:31,940
thus we can find that our comparison

232
00:12:31,940 --> 00:12:36,720
from tp2 has less steps compared to the

233
00:12:36,720 --> 00:12:39,620
traditional method

234
00:12:41,579 --> 00:12:44,399
the job Force positives from two

235
00:12:44,399 --> 00:12:46,260
dimensions

236
00:12:46,260 --> 00:12:48,839
the First Dimension is to validate the

237
00:12:48,839 --> 00:12:50,940
visibility

238
00:12:50,940 --> 00:12:54,779
of each code path by using the 3xmt

239
00:12:54,779 --> 00:12:56,220
server

240
00:12:56,220 --> 00:12:59,639
there are two stages for validation

241
00:12:59,639 --> 00:13:03,779
when performing log usage analysis for

242
00:13:03,779 --> 00:13:05,820
numerous code paths

243
00:13:05,820 --> 00:13:09,480
they perform lightweight and impress us

244
00:13:09,480 --> 00:13:14,279
code pass checking to improve efficiency

245
00:13:14,279 --> 00:13:17,760
specifically we perform tracking in just

246
00:13:17,760 --> 00:13:20,279
the one function without handling

247
00:13:20,279 --> 00:13:24,120
variable assignments so this tracking is

248
00:13:24,120 --> 00:13:27,660
impress us but fast

249
00:13:27,660 --> 00:13:31,079
when checking possible data logs

250
00:13:31,079 --> 00:13:35,060
which have fewer code paths May perform

251
00:13:35,060 --> 00:13:38,880
heavy weight and precise called path

252
00:13:38,880 --> 00:13:43,200
checking to improve accuracy

253
00:13:43,200 --> 00:13:46,740
specifically we perform checking across

254
00:13:46,740 --> 00:13:49,260
multiple functions

255
00:13:49,260 --> 00:13:53,279
and handle all the variable assignments

256
00:13:53,279 --> 00:13:55,260
so this checking

257
00:13:55,260 --> 00:14:00,260
is slow but precise

258
00:14:01,920 --> 00:14:04,920
the second dimension is to validate the

259
00:14:04,920 --> 00:14:08,820
concurrency of code paths first we check

260
00:14:08,820 --> 00:14:13,200
whether the code paths have a common log

261
00:14:13,200 --> 00:14:17,360
if so the two could pass a network

262
00:14:17,360 --> 00:14:20,639
concurrently executed

263
00:14:20,639 --> 00:14:24,120
second we check the core graphs of the

264
00:14:24,120 --> 00:14:25,860
two code paths

265
00:14:25,860 --> 00:14:29,760
if this could pass have common parts in

266
00:14:29,760 --> 00:14:31,139
core graphs

267
00:14:31,139 --> 00:14:34,680
they may be secretly executed at

268
00:14:34,680 --> 00:14:38,399
different time points of the same thread

269
00:14:38,399 --> 00:14:41,660
thus they are not considered to be

270
00:14:41,660 --> 00:14:45,560
concurrently executed

271
00:14:48,420 --> 00:14:51,480
based on the three key techniques the

272
00:14:51,480 --> 00:14:55,320
proposed a new approach in the deluxe to

273
00:14:55,320 --> 00:14:58,740
statically detect data logs in OS

274
00:14:58,740 --> 00:14:59,940
kernels

275
00:14:59,940 --> 00:15:03,180
we have implemented delos with the

276
00:15:03,180 --> 00:15:04,740
column compiler

277
00:15:04,740 --> 00:15:08,160
and it performs static analysis on the

278
00:15:08,160 --> 00:15:11,360
irvm bed code

279
00:15:13,139 --> 00:15:15,779
in the evaluation where you still have

280
00:15:15,779 --> 00:15:19,440
to check the source code of Linux 4.9

281
00:15:19,440 --> 00:15:23,220
and 5.10

282
00:15:23,639 --> 00:15:26,459
the evaluation runs on a regular

283
00:15:26,459 --> 00:15:29,399
personal computer

284
00:15:29,399 --> 00:15:33,060
we use column 9 to compare the kernel

285
00:15:33,060 --> 00:15:34,320
code

286
00:15:34,320 --> 00:15:39,620
with the configuration of all es config

287
00:15:42,180 --> 00:15:45,240
the table shows the detection results of

288
00:15:45,240 --> 00:15:47,779
the loss

289
00:15:48,060 --> 00:15:51,540
we find that by using indirect locking

290
00:15:51,540 --> 00:15:52,860
constraints

291
00:15:52,860 --> 00:15:55,440
the loss reduces much repeated

292
00:15:55,440 --> 00:15:58,440
compilation

293
00:15:59,220 --> 00:16:03,660
it also drops many false data logs where

294
00:16:03,660 --> 00:16:06,660
validating code plus feasibility and

295
00:16:06,660 --> 00:16:08,519
concurrency

296
00:16:08,519 --> 00:16:11,820
the time usage is less than

297
00:16:11,820 --> 00:16:16,579
seven hours which is acceptable

298
00:16:19,440 --> 00:16:21,139
in Linux

299
00:16:21,139 --> 00:16:28,500
4.9 reload funds 46 catalogs and 39 of

300
00:16:28,500 --> 00:16:30,779
them are real

301
00:16:30,779 --> 00:16:34,860
21 Deadlocks have been fixed in Linux

302
00:16:34,860 --> 00:16:37,860
5.10

303
00:16:38,579 --> 00:16:42,360
in Linux 5.10 kilos France

304
00:16:42,360 --> 00:16:45,180
65 catalogs

305
00:16:45,180 --> 00:16:48,959
and 54 of them are real

306
00:16:48,959 --> 00:16:52,800
31 catalogs have been confirmed by

307
00:16:52,800 --> 00:16:55,019
kernel Developers

308
00:16:55,019 --> 00:16:58,019
The Links of some confirmed data logs

309
00:16:58,019 --> 00:17:02,240
are listed in the slide

310
00:17:07,799 --> 00:17:11,099
still all still has some limitations

311
00:17:11,099 --> 00:17:14,760
first the loss has some false positives

312
00:17:14,760 --> 00:17:17,040
for several reasons

313
00:17:17,040 --> 00:17:19,199
for example

314
00:17:19,199 --> 00:17:22,799
is field-based analysis can make

315
00:17:22,799 --> 00:17:26,040
mistakes when two different variables

316
00:17:26,040 --> 00:17:29,040
have the same data structure type and

317
00:17:29,040 --> 00:17:30,720
field

318
00:17:30,720 --> 00:17:34,039
moreover if it is analysis yes

319
00:17:34,039 --> 00:17:36,660
intercrusted you and the following

320
00:17:36,660 --> 00:17:41,220
Institute and thus not very accurate

321
00:17:41,220 --> 00:17:44,580
in addition if code plus validation can

322
00:17:44,580 --> 00:17:48,059
make mistakes in complex cases

323
00:17:48,059 --> 00:17:53,120
such as array index assignments

324
00:17:53,160 --> 00:17:57,240
second he loss also has some false

325
00:17:57,240 --> 00:17:59,940
negatives for several reasons

326
00:17:59,940 --> 00:18:01,980
for example

327
00:18:01,980 --> 00:18:03,900
yes bottom up

328
00:18:03,900 --> 00:18:07,380
analysis is not complete the handling

329
00:18:07,380 --> 00:18:09,320
code functions

330
00:18:09,320 --> 00:18:13,799
Laura it fails to perform an asset of

331
00:18:13,799 --> 00:18:15,840
function pointer costs

332
00:18:15,840 --> 00:18:17,880
in addition

333
00:18:17,880 --> 00:18:20,940
he lost assumes that a code pass is

334
00:18:20,940 --> 00:18:26,059
never concurrently executed with itself

335
00:18:27,299 --> 00:18:31,559
finally let's conclude the work

336
00:18:31,559 --> 00:18:34,740
data logs are dangerous and harder to

337
00:18:34,740 --> 00:18:37,620
find in OS kernels

338
00:18:37,620 --> 00:18:42,780
to detect kernel datalogs the proposed a

339
00:18:42,780 --> 00:18:46,679
new start approach named Deluxe which

340
00:18:46,679 --> 00:18:50,660
has three key techniques

341
00:18:50,820 --> 00:18:56,000
in the evaluation it allows fans 39 and

342
00:18:56,000 --> 00:18:59,720
54 real deadlocks in Linux

343
00:18:59,720 --> 00:19:03,960
4.9 and 5.10

344
00:19:03,960 --> 00:19:07,080
we believe the laws can be extended to

345
00:19:07,080 --> 00:19:10,799
detecting other locking issues

346
00:19:10,799 --> 00:19:13,679
such as double locks

347
00:19:13,679 --> 00:19:17,900
and using sleepable blocks while holding

348
00:19:17,900 --> 00:19:21,200
spin locks

349
00:19:22,380 --> 00:19:25,320
thanks for listening if you have any

350
00:19:25,320 --> 00:19:30,080
question please feel free to contact us

