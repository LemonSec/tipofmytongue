1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:15,619 --> 00:00:18,480
joint research with my collaborators

3
00:00:18,480 --> 00:00:21,539
from China in this work we first

4
00:00:21,539 --> 00:00:25,560
identify performance bottlenecks of

5
00:00:25,560 --> 00:00:29,039
Linux MD the software rate when

6
00:00:29,039 --> 00:00:32,880
supporting very fast ssds and then

7
00:00:32,880 --> 00:00:36,840
propose a solution which is a stripe

8
00:00:36,840 --> 00:00:41,040
threaded architecture try to remove and

9
00:00:41,040 --> 00:00:44,280
alleviate this bottleneck

10
00:00:44,280 --> 00:00:48,120
so let me first uh

11
00:00:48,120 --> 00:00:51,660
give you a quick primer on read I I

12
00:00:51,660 --> 00:00:54,120
don't think I need that but just for the

13
00:00:54,120 --> 00:00:56,280
purpose of other audience

14
00:00:56,280 --> 00:00:57,600
so

15
00:00:57,600 --> 00:00:58,739
um

16
00:00:58,739 --> 00:01:02,340
read which stands for redundant array of

17
00:01:02,340 --> 00:01:04,700
inexpensive disks

18
00:01:04,700 --> 00:01:09,180
uh was uh you know designed to provide

19
00:01:09,180 --> 00:01:13,380
High i o throughput by uh striping data

20
00:01:13,380 --> 00:01:16,680
across multiple disks and highest

21
00:01:16,680 --> 00:01:18,840
relatability by leveraging data

22
00:01:18,840 --> 00:01:21,060
redundancy

23
00:01:21,060 --> 00:01:23,580
in an end disk rate

24
00:01:23,580 --> 00:01:27,180
data are organized in Stripes of n

25
00:01:27,180 --> 00:01:29,100
chunks each

26
00:01:29,100 --> 00:01:31,880
and the block addresses are

27
00:01:31,880 --> 00:01:36,119
algorithmically mapped to chunks on the

28
00:01:36,119 --> 00:01:40,920
disks so depending on the uh whether or

29
00:01:40,920 --> 00:01:43,799
how data redundancy is used for

30
00:01:43,799 --> 00:01:45,259
reliability

31
00:01:45,259 --> 00:01:49,460
read is organized into multiple levels

32
00:01:49,460 --> 00:01:53,399
and that can be roughly divided into two

33
00:01:53,399 --> 00:01:55,640
categories

34
00:01:55,640 --> 00:01:59,040
non-parity read for example read zero

35
00:01:59,040 --> 00:02:02,100
which is stripe only or read one which

36
00:02:02,100 --> 00:02:05,700
is mirror only uses uh you know

37
00:02:05,700 --> 00:02:09,660
replicative redundancy or they can be a

38
00:02:09,660 --> 00:02:12,720
parity rate which uses peritric

39
00:02:12,720 --> 00:02:14,540
redundancy to provide

40
00:02:14,540 --> 00:02:18,180
data reliability that can tolerate one

41
00:02:18,180 --> 00:02:21,000
or more disk failures

42
00:02:21,000 --> 00:02:26,120
so uh while read and write in

43
00:02:26,120 --> 00:02:28,860
non-parity reads are straightforward

44
00:02:28,860 --> 00:02:32,040
essentially you just write and read to

45
00:02:32,040 --> 00:02:34,319
the constituent disk as normal

46
00:02:34,319 --> 00:02:38,040
however for parity read write operation

47
00:02:38,040 --> 00:02:41,040
particularly partial Stripe Right is

48
00:02:41,040 --> 00:02:42,560
more complicated

49
00:02:42,560 --> 00:02:47,099
which involves reading the affected data

50
00:02:47,099 --> 00:02:50,700
chunks and the old parity chunks and

51
00:02:50,700 --> 00:02:53,160
then apply the parity operation to

52
00:02:53,160 --> 00:02:55,860
generate new parity and then write a new

53
00:02:55,860 --> 00:02:58,200
data and parity chunks back to the disks

54
00:02:58,200 --> 00:03:00,959
so this is also called you know a

55
00:03:00,959 --> 00:03:03,500
partial right penalty

56
00:03:03,500 --> 00:03:06,480
so um

57
00:03:06,480 --> 00:03:10,800
the Linux software read module referred

58
00:03:10,800 --> 00:03:15,540
to as multiple dicks or MD is the most

59
00:03:15,540 --> 00:03:19,260
commonly used software read evolving

60
00:03:19,260 --> 00:03:21,599
with the Linux kernel

61
00:03:21,599 --> 00:03:24,900
So currently MD supports various read

62
00:03:24,900 --> 00:03:28,140
levels and read compositions

63
00:03:28,140 --> 00:03:31,700
so instead of using a hardware read card

64
00:03:31,700 --> 00:03:35,340
MD uses the host CPU and memory

65
00:03:35,340 --> 00:03:39,599
resources to provide read functions

66
00:03:39,599 --> 00:03:43,260
therefore MD is highly portable and

67
00:03:43,260 --> 00:03:46,379
compatible with many uh different

68
00:03:46,379 --> 00:03:51,120
storage devices including HDD SSD and

69
00:03:51,120 --> 00:03:54,420
and nvme and so on

70
00:03:54,420 --> 00:03:56,580
so

71
00:03:56,580 --> 00:03:58,940
with the modern SSD

72
00:03:58,940 --> 00:04:01,200
continuously providing

73
00:04:01,200 --> 00:04:03,900
high right through food

74
00:04:03,900 --> 00:04:07,620
as indicated in the figure here a single

75
00:04:07,620 --> 00:04:10,319
SSD is poised to provide more than 10

76
00:04:10,319 --> 00:04:14,459
gigabytes per second right throughput

77
00:04:14,459 --> 00:04:19,620
the question is can the latest MD

78
00:04:19,620 --> 00:04:23,820
keep up with this pace or alternatively

79
00:04:23,820 --> 00:04:28,139
can MD fully Leverage The increasing

80
00:04:28,139 --> 00:04:31,199
high performance of the individual

81
00:04:31,199 --> 00:04:33,720
storage devices

82
00:04:33,720 --> 00:04:37,020
so so this question motivated us

83
00:04:37,020 --> 00:04:42,000
to revisit the SSD based MD rate

84
00:04:42,000 --> 00:04:46,020
so we deploy six SSD devices

85
00:04:46,020 --> 00:04:50,940
to construct non-parity read zero and

86
00:04:50,940 --> 00:04:55,340
parity-based read 5 and read six

87
00:04:55,500 --> 00:04:58,139
we also enable the multi-worker

88
00:04:58,139 --> 00:05:01,199
mechanism of MD which was introduced in

89
00:05:01,199 --> 00:05:05,520
2014 to better utilize the performance

90
00:05:05,520 --> 00:05:09,620
of SSD devices and enables multiple

91
00:05:09,620 --> 00:05:12,479
functionally equivalent worker threads

92
00:05:12,479 --> 00:05:17,699
to process Stripes concurrently

93
00:05:17,699 --> 00:05:22,080
we enable up to 64 worker threads to

94
00:05:22,080 --> 00:05:25,320
maximize the MD performance

95
00:05:25,320 --> 00:05:27,960
so as we measure the MD performance

96
00:05:27,960 --> 00:05:32,280
running on three types of SSD devices

97
00:05:32,280 --> 00:05:36,120
and their i o characteristics are listed

98
00:05:36,120 --> 00:05:38,759
in the table here

99
00:05:38,759 --> 00:05:42,240
so as you can see for the high-end SSD

100
00:05:42,240 --> 00:05:46,020
the cumulative Rights group can reach up

101
00:05:46,020 --> 00:05:50,120
to 14 gigabytes per second

102
00:05:51,539 --> 00:05:54,780
so the figures here report the surface

103
00:05:54,780 --> 00:05:58,620
performance of the different rates

104
00:05:58,620 --> 00:06:01,080
so in all the cases

105
00:06:01,080 --> 00:06:02,880
the right performance

106
00:06:02,880 --> 00:06:05,820
and scalability of the non-parity read

107
00:06:05,820 --> 00:06:07,080
zero

108
00:06:07,080 --> 00:06:10,620
far exceed those of the parity based

109
00:06:10,620 --> 00:06:13,680
read 5 and read 6.

110
00:06:13,680 --> 00:06:17,880
especially parity-based reads

111
00:06:17,880 --> 00:06:19,919
fail to scale

112
00:06:19,919 --> 00:06:24,479
uh for higher performance ssds

113
00:06:24,479 --> 00:06:27,360
the right throughput of read5 for

114
00:06:27,360 --> 00:06:30,240
example and read 506 actually is only

115
00:06:30,240 --> 00:06:31,919
one seventh

116
00:06:31,919 --> 00:06:36,080
of that of the read zero on uh in the

117
00:06:36,080 --> 00:06:38,940
nvme ssds

118
00:06:38,940 --> 00:06:42,419
so even the most strike right friendly

119
00:06:42,419 --> 00:06:44,880
access pattern which is the full Stripe

120
00:06:44,880 --> 00:06:47,460
Right which doesn't involve the read

121
00:06:47,460 --> 00:06:49,020
modify and write

122
00:06:49,020 --> 00:06:52,139
uh still there is a big gap as you can

123
00:06:52,139 --> 00:06:54,800
see here

124
00:06:56,060 --> 00:06:59,759
so to further analyze the performance

125
00:06:59,759 --> 00:07:02,400
contribution of the multi-worker

126
00:07:02,400 --> 00:07:05,900
mechanism in the parity-based rate

127
00:07:05,900 --> 00:07:09,600
we invoke up to 64 users

128
00:07:09,600 --> 00:07:12,740
threats and enable the multi-thread

129
00:07:12,740 --> 00:07:16,199
multi-worker threat mechanism with the

130
00:07:16,199 --> 00:07:19,440
number of actual worker threats were

131
00:07:19,440 --> 00:07:21,840
varying from 1 to 64.

132
00:07:21,840 --> 00:07:25,500
empty with 60 volt more workers threads

133
00:07:25,500 --> 00:07:28,199
has a right throughput Improvement of

134
00:07:28,199 --> 00:07:31,319
less than four times over the single

135
00:07:31,319 --> 00:07:32,819
worker thread

136
00:07:32,819 --> 00:07:36,599
and its performance gain Peaks at 16

137
00:07:36,599 --> 00:07:38,699
Walker threads

138
00:07:38,699 --> 00:07:41,280
so as a result even with

139
00:07:41,280 --> 00:07:44,039
multiple worker threats

140
00:07:44,039 --> 00:07:47,580
parity rates steal for far short of

141
00:07:47,580 --> 00:07:50,220
fully leveraging the individual device

142
00:07:50,220 --> 00:07:53,000
performance

143
00:07:53,880 --> 00:07:55,979
so to find the root cause

144
00:07:55,979 --> 00:07:59,039
of this right inefficiency of MD

145
00:07:59,039 --> 00:08:02,220
here we analyze the architecture of

146
00:08:02,220 --> 00:08:04,680
parity-based MD

147
00:08:04,680 --> 00:08:09,000
MD introduces an end for all processing

148
00:08:09,000 --> 00:08:09,840
model

149
00:08:09,840 --> 00:08:13,380
to handle Stripe Right tasks where

150
00:08:13,380 --> 00:08:14,759
multiple

151
00:08:14,759 --> 00:08:18,800
worker threads where n represents

152
00:08:18,800 --> 00:08:24,060
process all all Stripes concurrently

153
00:08:24,060 --> 00:08:27,780
so MD deploys a centralized

154
00:08:27,780 --> 00:08:31,560
stripe cache that temporarily stores the

155
00:08:31,560 --> 00:08:34,260
incoming block iOS

156
00:08:34,260 --> 00:08:38,219
and these bios are aggregated at the

157
00:08:38,219 --> 00:08:41,219
granularity of stripes and each stripe

158
00:08:41,219 --> 00:08:44,219
has its own stripe head containing

159
00:08:44,219 --> 00:08:47,760
metadata such as the stripe States and

160
00:08:47,760 --> 00:08:50,279
device States and actually the for each

161
00:08:50,279 --> 00:08:53,580
drive there are over 50 states so it's

162
00:08:53,580 --> 00:08:56,360
quite complicated

163
00:08:56,940 --> 00:08:59,820
so MD module invokes multiple

164
00:08:59,820 --> 00:09:02,100
functionally equivalent worker threads

165
00:09:02,100 --> 00:09:04,220
to process drives

166
00:09:04,220 --> 00:09:07,560
asynchronously and long exclusively

167
00:09:07,560 --> 00:09:09,660
and of course with the exception of when

168
00:09:09,660 --> 00:09:13,500
accessing Global shared data

169
00:09:13,500 --> 00:09:16,800
so this was designed to utilize the CPU

170
00:09:16,800 --> 00:09:18,560
functionality

171
00:09:18,560 --> 00:09:22,860
sufficiently to absorb more requests for

172
00:09:22,860 --> 00:09:26,100
reducing actual iOS so by accumulating

173
00:09:26,100 --> 00:09:29,160
or batching all those iOS to the same

174
00:09:29,160 --> 00:09:31,200
stripes

175
00:09:31,200 --> 00:09:36,060
which is efficient when S one using hdds

176
00:09:36,060 --> 00:09:40,680
or SATA based ssds however it is upper

177
00:09:40,680 --> 00:09:43,860
bounded in its processing scalability

178
00:09:43,860 --> 00:09:46,620
that fails to keep up with the fast

179
00:09:46,620 --> 00:09:47,880
devices

180
00:09:47,880 --> 00:09:50,940
so next we will analyze the multi-worker

181
00:09:50,940 --> 00:09:55,040
concurrency control of MD

182
00:09:55,320 --> 00:09:58,140
so MD uses a spin lock

183
00:09:58,140 --> 00:10:01,860
called device lock to control concurrent

184
00:10:01,860 --> 00:10:05,640
accesses from workers threads and user

185
00:10:05,640 --> 00:10:10,200
threads to all the stripe lists and

186
00:10:10,200 --> 00:10:11,580
metadata

187
00:10:11,580 --> 00:10:14,940
so in the case when multiple worker

188
00:10:14,940 --> 00:10:18,060
threads exclusively access the stripe

189
00:10:18,060 --> 00:10:21,839
list the spin lock will cause severe

190
00:10:21,839 --> 00:10:25,500
contention among these threats so the

191
00:10:25,500 --> 00:10:27,899
spin lock actively spends CPU Cycles

192
00:10:27,899 --> 00:10:31,320
obviously to compete for logs consuming

193
00:10:31,320 --> 00:10:33,660
significant CPU resources when

194
00:10:33,660 --> 00:10:36,800
competition occurs

195
00:10:38,399 --> 00:10:42,000
so in fact the stripe list is a hotspot

196
00:10:42,000 --> 00:10:45,420
due to the workers threats non-exclusive

197
00:10:45,420 --> 00:10:47,399
stripe processing

198
00:10:47,399 --> 00:10:49,740
so stripe white the striped right

199
00:10:49,740 --> 00:10:53,940
process workflow roughly uh consists of

200
00:10:53,940 --> 00:10:56,640
five consecutive stages

201
00:10:56,640 --> 00:10:59,519
from inserting the block iOS

202
00:10:59,519 --> 00:11:02,820
into this drive head to handling the

203
00:11:02,820 --> 00:11:05,459
Stripe Right itself which has three

204
00:11:05,459 --> 00:11:08,399
stages as mentioned earlier read modify

205
00:11:08,399 --> 00:11:11,100
and write and finally

206
00:11:11,100 --> 00:11:12,300
um

207
00:11:12,300 --> 00:11:14,700
cleaning the stripe head

208
00:11:14,700 --> 00:11:18,720
so different process stages may be

209
00:11:18,720 --> 00:11:21,600
handled by different workers threats in

210
00:11:21,600 --> 00:11:24,260
this setting

211
00:11:24,360 --> 00:11:27,480
now to complicate things further

212
00:11:27,480 --> 00:11:30,540
uh the worker

213
00:11:30,540 --> 00:11:33,480
threat handles each Stripe Right stage

214
00:11:33,480 --> 00:11:35,220
so there are five of them

215
00:11:35,220 --> 00:11:37,380
with four steps

216
00:11:37,380 --> 00:11:40,440
here we take the read stage as an

217
00:11:40,440 --> 00:11:44,220
example to illustrate the overhead so

218
00:11:44,220 --> 00:11:45,540
first

219
00:11:45,540 --> 00:11:48,240
the the workers thread has to acquire

220
00:11:48,240 --> 00:11:50,760
the device lock for

221
00:11:50,760 --> 00:11:54,720
fetching a stripe head from the handle

222
00:11:54,720 --> 00:11:55,920
list

223
00:11:55,920 --> 00:11:59,040
which causes log contention among the

224
00:11:59,040 --> 00:12:01,320
workers threats

225
00:12:01,320 --> 00:12:03,600
and then the workers thread will analyze

226
00:12:03,600 --> 00:12:06,600
the large number of states of of the

227
00:12:06,600 --> 00:12:08,880
corresponding stripe to determine the

228
00:12:08,880 --> 00:12:12,300
exact handling operation following this

229
00:12:12,300 --> 00:12:15,660
however most of the stripe states are

230
00:12:15,660 --> 00:12:19,320
shared among user threats workers threat

231
00:12:19,320 --> 00:12:22,320
and even some IO other IO threats

232
00:12:22,320 --> 00:12:25,459
so as a result MD has to use semaphore

233
00:12:25,459 --> 00:12:30,260
and RCU readlock to protect State

234
00:12:30,260 --> 00:12:32,760
analysis transactions

235
00:12:32,760 --> 00:12:36,540
so that's increasing CPU overhead

236
00:12:36,540 --> 00:12:37,500
further

237
00:12:37,500 --> 00:12:40,860
so therefore we suspect it is this

238
00:12:40,860 --> 00:12:44,760
area this this you know uh uh you know

239
00:12:44,760 --> 00:12:47,880
protected access to share log and also

240
00:12:47,880 --> 00:12:50,040
the shear states that would be the

241
00:12:50,040 --> 00:12:52,440
corporate for you know for for this

242
00:12:52,440 --> 00:12:55,279
inefficiency

243
00:12:56,339 --> 00:12:58,980
so uh

244
00:12:58,980 --> 00:13:03,540
so after the stage is handled uh workers

245
00:13:03,540 --> 00:13:07,200
thread will release the sh and insert it

246
00:13:07,200 --> 00:13:10,680
back to the uh you know the stripe list

247
00:13:10,680 --> 00:13:13,680
again invoking the log so again causing

248
00:13:13,680 --> 00:13:16,099
contention

249
00:13:16,380 --> 00:13:19,560
so these four steps as I mentioned

250
00:13:19,560 --> 00:13:21,720
actually is associated with every one of

251
00:13:21,720 --> 00:13:23,339
the five stages

252
00:13:23,339 --> 00:13:25,800
so as a result clearly this is going to

253
00:13:25,800 --> 00:13:27,660
have a very significant impact on the

254
00:13:27,660 --> 00:13:29,040
performance

255
00:13:29,040 --> 00:13:35,160
so to to to verify our suspicion uh we

256
00:13:35,160 --> 00:13:38,040
investigate the CPU usage uh

257
00:13:38,040 --> 00:13:42,060
distribution when uh running MD with

258
00:13:42,060 --> 00:13:44,700
very number of worker threats

259
00:13:44,700 --> 00:13:48,120
and we use the perf tool to measure the

260
00:13:48,120 --> 00:13:51,540
CPU cycles of key functions with workers

261
00:13:51,540 --> 00:13:54,959
with a worker thread so the results show

262
00:13:54,959 --> 00:13:57,180
that the CPU becomes

263
00:13:57,180 --> 00:14:00,480
a notable bottleneck on concurrence

264
00:14:00,480 --> 00:14:02,100
control in MD

265
00:14:02,100 --> 00:14:05,519
so on the one hand the global device log

266
00:14:05,519 --> 00:14:07,139
consumes

267
00:14:07,139 --> 00:14:10,740
a dominant 55 of CPU Cycles

268
00:14:10,740 --> 00:14:14,100
in the 64 workers direct case

269
00:14:14,100 --> 00:14:17,579
on the other hand CPU Cycles responsible

270
00:14:17,579 --> 00:14:21,480
for for disk i o and and the parity

271
00:14:21,480 --> 00:14:24,480
calculation decreased as the number of

272
00:14:24,480 --> 00:14:27,000
workers threat increases accounting for

273
00:14:27,000 --> 00:14:31,560
less than 10 percent at 60 workers rates

274
00:14:31,560 --> 00:14:34,440
so in conclusion

275
00:14:34,440 --> 00:14:38,760
while MD provides more CPU resources by

276
00:14:38,760 --> 00:14:42,300
enabling the multi-worker mechanism

277
00:14:42,300 --> 00:14:47,699
fewer CPU Cycles are used to drive iOS

278
00:14:47,699 --> 00:14:49,740
therefore

279
00:14:49,740 --> 00:14:53,639
fast device through device are not fully

280
00:14:53,639 --> 00:14:57,360
utilized as we suspected

281
00:14:57,360 --> 00:15:00,899
so to solve this right inefficiency of

282
00:15:00,899 --> 00:15:03,000
parity based read

283
00:15:03,000 --> 00:15:05,760
uh we propose a stripe threaded

284
00:15:05,760 --> 00:15:07,260
architecture

285
00:15:07,260 --> 00:15:09,720
SD read for short

286
00:15:09,720 --> 00:15:14,279
so the goal of St rate is to reduce CPU

287
00:15:14,279 --> 00:15:18,300
overhead by optimizing the concurrent

288
00:15:18,300 --> 00:15:20,699
stripe handling process

289
00:15:20,699 --> 00:15:23,459
and minimize the

290
00:15:23,459 --> 00:15:27,360
partial Stripe Right penalty so the deal

291
00:15:27,360 --> 00:15:28,560
goes

292
00:15:28,560 --> 00:15:30,959
to achieve the first goal

293
00:15:30,959 --> 00:15:34,560
uh St rate assigns a dedicated worker

294
00:15:34,560 --> 00:15:37,860
threat to exclusively handle the

295
00:15:37,860 --> 00:15:39,660
corresponding strike

296
00:15:39,660 --> 00:15:43,260
multiple worker threads process their

297
00:15:43,260 --> 00:15:46,199
own Stripes independently

298
00:15:46,199 --> 00:15:49,560
therefore exploiting the intrinsic data

299
00:15:49,560 --> 00:15:52,620
perison among the stripes

300
00:15:52,620 --> 00:15:54,860
compared to MD

301
00:15:54,860 --> 00:15:58,560
SD rate removes the

302
00:15:58,560 --> 00:16:01,440
centralized striped head list

303
00:16:01,440 --> 00:16:03,260
and their corresponding concurrent

304
00:16:03,260 --> 00:16:06,360
operations thus reducing the log

305
00:16:06,360 --> 00:16:09,420
contention among multiple threats

306
00:16:09,420 --> 00:16:13,800
furthermore St rate minimizes

307
00:16:13,800 --> 00:16:17,279
the number of shared stripe related

308
00:16:17,279 --> 00:16:20,760
States and Global States checking among

309
00:16:20,760 --> 00:16:22,920
worker threats

310
00:16:22,920 --> 00:16:27,660
because of because of a dedicated worker

311
00:16:27,660 --> 00:16:31,819
threat handles strive exclusively

312
00:16:32,339 --> 00:16:34,380
so the concurrent stripe handling

313
00:16:34,380 --> 00:16:36,060
process and

314
00:16:36,060 --> 00:16:38,060
um

315
00:16:41,220 --> 00:16:42,959
okay

316
00:16:42,959 --> 00:16:46,740
now we just described the first goal

317
00:16:46,740 --> 00:16:49,139
however how to achieve the second goal

318
00:16:49,139 --> 00:16:53,940
uh is is not trivial so um there's also

319
00:16:53,940 --> 00:16:57,779
an issue of how St rate

320
00:16:57,779 --> 00:17:01,100
uh coordinate multiple

321
00:17:01,100 --> 00:17:03,240
worker threads

322
00:17:03,240 --> 00:17:06,299
so to effectively coordinate multiple

323
00:17:06,299 --> 00:17:10,079
worker threats St rate proposes is

324
00:17:10,079 --> 00:17:12,359
stripe State table

325
00:17:12,359 --> 00:17:15,720
that maintains a set of required uh

326
00:17:15,720 --> 00:17:19,679
required shared stripe States and per

327
00:17:19,679 --> 00:17:21,720
stripe locks

328
00:17:21,720 --> 00:17:24,540
SST is a globally shared data structure

329
00:17:24,540 --> 00:17:27,780
among workers threats where each entry

330
00:17:27,780 --> 00:17:30,120
is uniquely associated with a physical

331
00:17:30,120 --> 00:17:31,559
strike

332
00:17:31,559 --> 00:17:35,460
to provide lockless access features each

333
00:17:35,460 --> 00:17:38,340
entry in SST can only be exclusively

334
00:17:38,340 --> 00:17:41,640
Modified by its dedicated worker thread

335
00:17:41,640 --> 00:17:46,640
using Atomic operation at any time

336
00:17:47,880 --> 00:17:53,039
so in St read a write triggers a

337
00:17:53,039 --> 00:17:55,140
dedicated workers threat to immediately

338
00:17:55,140 --> 00:17:58,320
and exclusively handle the corresponding

339
00:17:58,320 --> 00:17:59,820
strike right

340
00:17:59,820 --> 00:18:02,280
which does not address the costly

341
00:18:02,280 --> 00:18:05,160
partial right penalty or second goal

342
00:18:05,160 --> 00:18:09,419
so to optimize the partial right rights

343
00:18:09,419 --> 00:18:14,340
as Esty rate presents a two-phase stripe

344
00:18:14,340 --> 00:18:17,220
submission mechanism

345
00:18:17,220 --> 00:18:21,480
so it often optimistically Aggregates

346
00:18:21,480 --> 00:18:24,600
rights belonging to the same stripe by

347
00:18:24,600 --> 00:18:28,620
enabling a batching queue in each worker

348
00:18:28,620 --> 00:18:29,760
thread

349
00:18:29,760 --> 00:18:33,840
the link the Linux MD Aggregates partial

350
00:18:33,840 --> 00:18:37,340
stripe rights to four strike rights by

351
00:18:37,340 --> 00:18:41,400
actively or passively postponing

352
00:18:41,400 --> 00:18:44,220
stripe handling tasks

353
00:18:44,220 --> 00:18:47,100
different from MD the two-phase

354
00:18:47,100 --> 00:18:50,880
submission Aggregates rights within a

355
00:18:50,880 --> 00:18:54,000
limited time provided by the stripe

356
00:18:54,000 --> 00:18:56,780
access itself

357
00:18:59,400 --> 00:19:03,059
so we should use an example to describe

358
00:19:03,059 --> 00:19:06,480
the St rates concurrence control method

359
00:19:06,480 --> 00:19:09,179
where four i o threads

360
00:19:09,179 --> 00:19:13,080
issue block requests to St rate

361
00:19:13,080 --> 00:19:16,679
concurrently where bio zero targets the

362
00:19:16,679 --> 00:19:18,360
stripe S5

363
00:19:18,360 --> 00:19:21,960
and bio one through bio three Target the

364
00:19:21,960 --> 00:19:24,900
same stripe S8

365
00:19:24,900 --> 00:19:27,179
at the beginning of the timeline

366
00:19:27,179 --> 00:19:30,799
uh Orchestra at zero and Walker Story 1

367
00:19:30,799 --> 00:19:33,720
receive bios from their corresponding

368
00:19:33,720 --> 00:19:35,039
user threat

369
00:19:35,039 --> 00:19:38,220
and acquire their stripe locks to begin

370
00:19:38,220 --> 00:19:42,000
strike processing independently

371
00:19:42,000 --> 00:19:45,360
the bio zero and bio 1 can be handled

372
00:19:45,360 --> 00:19:50,059
independently as I mentioned

373
00:19:50,820 --> 00:19:53,460
because they target different stripes

374
00:19:53,460 --> 00:19:55,380
and these two workers are at first

375
00:19:55,380 --> 00:19:58,799
initialize initialize the stripe table

376
00:19:58,799 --> 00:20:03,120
uh in SST then it determines the

377
00:20:03,120 --> 00:20:06,660
Reconstruction method for this drive and

378
00:20:06,660 --> 00:20:09,059
reads the required

379
00:20:09,059 --> 00:20:12,480
part uh parity data and and priority

380
00:20:12,480 --> 00:20:16,200
chunk and data chunks from the disk

381
00:20:16,200 --> 00:20:19,559
so St read utilizes the latency of

382
00:20:19,559 --> 00:20:22,559
waiting for read data

383
00:20:22,559 --> 00:20:26,400
to optimistically aggregate requests so

384
00:20:26,400 --> 00:20:28,260
during this period while you're waiting

385
00:20:28,260 --> 00:20:31,200
for read data for the for the data

386
00:20:31,200 --> 00:20:33,539
chunks and parity chunks if there are

387
00:20:33,539 --> 00:20:36,360
requests coming in addressing the same

388
00:20:36,360 --> 00:20:39,179
stripe then you batch them it's very

389
00:20:39,179 --> 00:20:41,419
simple

390
00:20:41,880 --> 00:20:45,840
and and by setting the ease batching

391
00:20:45,840 --> 00:20:50,160
flag in the corresponding St engine

392
00:20:50,160 --> 00:20:52,200
so shortly after

393
00:20:52,200 --> 00:20:55,080
Orchestra wants arrival a second workers

394
00:20:55,080 --> 00:20:56,340
threat

395
00:20:56,340 --> 00:21:00,600
two arrives and six SST only to find

396
00:21:00,600 --> 00:21:04,200
that the targeted stripe is locked and

397
00:21:04,200 --> 00:21:07,200
but enabling battery

398
00:21:07,200 --> 00:21:12,059
so Orchestra 2 inserts its bio to the

399
00:21:12,059 --> 00:21:15,179
batching queue of the handling thread

400
00:21:15,179 --> 00:21:20,400
Orchestra 1 and then suspense itself

401
00:21:20,400 --> 00:21:23,400
when a worker thread completes its

402
00:21:23,400 --> 00:21:24,660
battery phase

403
00:21:24,660 --> 00:21:27,720
it immediately transitions the stripe

404
00:21:27,720 --> 00:21:31,919
into the Frozen phase by using an atomic

405
00:21:31,919 --> 00:21:35,460
Cast Operation to update SSD

406
00:21:35,460 --> 00:21:39,179
after that the stripe is not allowed to

407
00:21:39,179 --> 00:21:43,980
aggregate any further request

408
00:21:44,340 --> 00:21:47,340
so the Lily arrived bios from workers

409
00:21:47,340 --> 00:21:49,020
thread 3

410
00:21:49,020 --> 00:21:52,740
or consequently blocked and must wait

411
00:21:52,740 --> 00:21:55,080
for the stripe a to be

412
00:21:55,080 --> 00:21:57,780
finished or released the stripe Head

413
00:21:57,780 --> 00:21:59,159
released

414
00:21:59,159 --> 00:22:02,780
then the dedicated wt1

415
00:22:02,780 --> 00:22:05,340
coalesces all the requests it has

416
00:22:05,340 --> 00:22:08,460
accumulated in its batching queue and

417
00:22:08,460 --> 00:22:11,039
processing them as a whole

418
00:22:11,039 --> 00:22:15,840
which means re-executes parity read in

419
00:22:15,840 --> 00:22:17,760
accordance with with the aggregated

420
00:22:17,760 --> 00:22:21,179
requests and performance parity

421
00:22:21,179 --> 00:22:25,919
calculation and data data parity writes

422
00:22:25,919 --> 00:22:27,080
back to

423
00:22:27,080 --> 00:22:31,200
the disk after reconstruction so finally

424
00:22:31,200 --> 00:22:33,659
the dedicated workers where I want

425
00:22:33,659 --> 00:22:38,000
clears up the stripe States of

426
00:22:38,000 --> 00:22:42,000
sa in SST and releases the strike back

427
00:22:42,000 --> 00:22:44,460
the the stripe lock

428
00:22:44,460 --> 00:22:46,679
the corresponding waiting

429
00:22:46,679 --> 00:22:49,500
thread wwt2

430
00:22:49,500 --> 00:22:53,640
will also return successfully

431
00:22:53,820 --> 00:22:56,700
so in the next phase the worker stress 3

432
00:22:56,700 --> 00:22:59,940
should reacquire the strike block to

433
00:22:59,940 --> 00:23:02,940
become a dedicated thread for handling

434
00:23:02,940 --> 00:23:06,120
its request

435
00:23:06,120 --> 00:23:09,600
so so this is how you know the St threat

436
00:23:09,600 --> 00:23:13,500
Works in in a nutshell uh so to find out

437
00:23:13,500 --> 00:23:17,900
how it does in uh dealing with choices

438
00:23:17,900 --> 00:23:22,200
we run extensive experiments on the

439
00:23:22,200 --> 00:23:24,840
server and three types of storage

440
00:23:24,840 --> 00:23:28,620
devices with their details described in

441
00:23:28,620 --> 00:23:32,280
the table so essentially the the latest

442
00:23:32,280 --> 00:23:37,080
SSD or the high-end SSD and also the

443
00:23:37,080 --> 00:23:40,679
persistent memory often trying to say

444
00:23:40,679 --> 00:23:45,240
how how much you know St read can go

445
00:23:45,240 --> 00:23:48,419
with faster device we also use a rhyme

446
00:23:48,419 --> 00:23:52,559
disk to emulate even faster SSD and and

447
00:23:52,559 --> 00:23:54,780
see how it does

448
00:23:54,780 --> 00:23:58,020
and we use both micro benchmarks and

449
00:23:58,020 --> 00:24:00,659
select three representative block

450
00:24:00,659 --> 00:24:04,620
choices from Microsoft Ali pangu and

451
00:24:04,620 --> 00:24:07,679
file bench as twist driven micro

452
00:24:07,679 --> 00:24:11,520
benchmarks to evaluate and still rate

453
00:24:11,520 --> 00:24:14,340
so here are the results micro Benchmark

454
00:24:14,340 --> 00:24:18,120
results uh the figure reports

455
00:24:18,120 --> 00:24:21,179
the right throughput of St rate and MD

456
00:24:21,179 --> 00:24:24,720
with partial Stripe Right

457
00:24:24,720 --> 00:24:27,659
and full Stripe Right

458
00:24:27,659 --> 00:24:31,080
so St rate achieves up to 2.5 times

459
00:24:31,080 --> 00:24:34,799
higher right throughput than MD with a

460
00:24:34,799 --> 00:24:36,539
single thread

461
00:24:36,539 --> 00:24:39,960
it indicates that S3 can effectively

462
00:24:39,960 --> 00:24:42,299
reduce the overhead of

463
00:24:42,299 --> 00:24:45,900
handling stripe States

464
00:24:45,900 --> 00:24:50,700
thus scales better than MD with highly

465
00:24:50,700 --> 00:24:52,440
concurrent and

466
00:24:52,440 --> 00:24:55,200
intensive workloads

467
00:24:55,200 --> 00:24:57,600
the right figure

468
00:24:57,600 --> 00:25:00,000
indicates the average request latency of

469
00:25:00,000 --> 00:25:03,840
St rate and MD which also shows the

470
00:25:03,840 --> 00:25:08,520
advantage of of St rate over MD notably

471
00:25:08,520 --> 00:25:11,700
a moreover St rate does not affect rig

472
00:25:11,700 --> 00:25:14,159
performance and the average rate perform

473
00:25:14,159 --> 00:25:16,620
reached throughput difference between MD

474
00:25:16,620 --> 00:25:20,700
and St rate is less than five percent

475
00:25:20,700 --> 00:25:25,080
so the left figure reports

476
00:25:28,080 --> 00:25:31,740
so compared with CPU utilization or ft

477
00:25:31,740 --> 00:25:36,059
rate and MD with 64 threats the left

478
00:25:36,059 --> 00:25:40,080
figure shows that St rate reduces up to

479
00:25:40,080 --> 00:25:44,039
90 percent lock overhead and almost all

480
00:25:44,039 --> 00:25:46,860
the stripe State checking due to the

481
00:25:46,860 --> 00:25:49,500
stripe threaded architecture

482
00:25:49,500 --> 00:25:52,380
features in St rates such as the

483
00:25:52,380 --> 00:25:55,500
two-phase stripe submission mechanism

484
00:25:55,500 --> 00:25:57,960
incur less than five percent of CPU

485
00:25:57,960 --> 00:25:59,220
consumption

486
00:25:59,220 --> 00:26:02,880
the total CPU utilization of MD is up to

487
00:26:02,880 --> 00:26:07,380
6.3 times higher than St rate with 64

488
00:26:07,380 --> 00:26:09,900
work uh user threats

489
00:26:09,900 --> 00:26:13,740
however the St rate gains higher

490
00:26:13,740 --> 00:26:18,179
throughput and efficiently uses twice as

491
00:26:18,179 --> 00:26:21,960
much CPU resources for computing

492
00:26:21,960 --> 00:26:25,679
exclusive or and issuing iOS

493
00:26:25,679 --> 00:26:29,640
so it spends most CPU Cycles on iOS and

494
00:26:29,640 --> 00:26:31,200
and the real world

495
00:26:31,200 --> 00:26:34,880
for example MD with

496
00:26:34,880 --> 00:26:38,940
4495 CPU core utilization

497
00:26:38,940 --> 00:26:43,380
consumes only 2.1 gigabyte per second of

498
00:26:43,380 --> 00:26:47,100
the SSD bandwidth in contrast to St rate

499
00:26:47,100 --> 00:26:49,799
that consumes six gigabytes per second

500
00:26:49,799 --> 00:26:51,659
of the SSD

501
00:26:51,659 --> 00:26:56,220
bandwidth with only 700 or 4 CPU core

502
00:26:56,220 --> 00:26:59,220
utilization

503
00:27:00,900 --> 00:27:04,500
so to get a sense of how you know how

504
00:27:04,500 --> 00:27:08,760
far can S3 go with even faster uh SSD in

505
00:27:08,760 --> 00:27:10,200
the future

506
00:27:10,200 --> 00:27:15,059
um we we test sort of the upper bound uh

507
00:27:15,059 --> 00:27:16,140
using

508
00:27:16,140 --> 00:27:20,400
rhyme disk and uh on pm

509
00:27:20,400 --> 00:27:24,720
so the throughput of St rate on PMS is

510
00:27:24,720 --> 00:27:28,799
up to 50 and 35 percent higher than MD

511
00:27:28,799 --> 00:27:32,880
on partial stripe and full stripe rights

512
00:27:32,880 --> 00:27:37,860
we find that St rate on pns shows up up

513
00:27:37,860 --> 00:27:40,679
to 25 percent higher throughput in

514
00:27:40,679 --> 00:27:43,620
purchase drive right than that on ssds

515
00:27:43,620 --> 00:27:47,520
this is because pm has one order of

516
00:27:47,520 --> 00:27:51,120
magnitude lower read latency than ssds

517
00:27:51,120 --> 00:27:55,200
thus St Ray could handle Stripes more

518
00:27:55,200 --> 00:27:56,880
efficiently

519
00:27:56,880 --> 00:28:02,100
at 64 user threats estimate on dram Ram

520
00:28:02,100 --> 00:28:04,799
disks reaches more than

521
00:28:04,799 --> 00:28:07,860
35 gigabyte per second right throughput

522
00:28:07,860 --> 00:28:11,220
in Con in contrast to their MD

523
00:28:11,220 --> 00:28:14,600
counterpart of less than

524
00:28:14,940 --> 00:28:17,220
six gigabytes per second

525
00:28:17,220 --> 00:28:19,679
so it's used that Stern has the

526
00:28:19,679 --> 00:28:22,220
potential to effectively

527
00:28:22,220 --> 00:28:26,640
explore faster storage devices like the

528
00:28:26,640 --> 00:28:29,880
emerging pcie 50

529
00:28:29,880 --> 00:28:33,960
5.0 ssds in the near future that

530
00:28:33,960 --> 00:28:36,840
is poised to reach more than 10

531
00:28:36,840 --> 00:28:40,879
gigabytes per second per device

532
00:28:42,779 --> 00:28:46,740
oh so here is a micro Benchmark results

533
00:28:46,740 --> 00:28:51,380
so the average throughput is 2 times the

534
00:28:51,380 --> 00:28:55,080
2.8 times higher than MD the mean

535
00:28:55,080 --> 00:28:59,419
average and 99 percentile latency is

536
00:28:59,419 --> 00:29:06,480
10.3 times and 90 49 and 25 lower than

537
00:29:06,480 --> 00:29:08,400
MD

538
00:29:08,400 --> 00:29:13,799
and to conclude we presented a thread a

539
00:29:13,799 --> 00:29:15,600
a

540
00:29:15,600 --> 00:29:20,340
stripe threaded architecture for MD so

541
00:29:20,340 --> 00:29:23,179
that it can better support high-speed

542
00:29:23,179 --> 00:29:25,980
storage devices

543
00:29:25,980 --> 00:29:29,220
and the result indicates its design is

544
00:29:29,220 --> 00:29:32,640
effective in achieving its design goals

545
00:29:32,640 --> 00:29:35,240
thank you

