1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:16,940 --> 00:00:20,520
University here I'm going to present

3
00:00:20,520 --> 00:00:24,539
this paper from ATC 2022 the title is

4
00:00:24,539 --> 00:00:27,480
zero head monitoring for cloud native

5
00:00:27,480 --> 00:00:31,260
infrastructure using RDMA uh

6
00:00:31,260 --> 00:00:33,780
uh this work is done during I'm a

7
00:00:33,780 --> 00:00:37,579
research intern at Alibaba cloud

8
00:00:37,579 --> 00:00:41,219
here is the outline of my presentation

9
00:00:41,219 --> 00:00:44,100
today first I will introduce the

10
00:00:44,100 --> 00:00:45,840
background and the motivation of this

11
00:00:45,840 --> 00:00:49,140
paper next is a proposed code designs

12
00:00:49,140 --> 00:00:53,719
after that I will introduce a

13
00:00:53,719 --> 00:00:57,719
implementation and evaluation evaluation

14
00:00:57,719 --> 00:00:59,960
setups and

15
00:00:59,960 --> 00:01:02,940
experimental results in the end this

16
00:01:02,940 --> 00:01:05,700
talk is closed with a conclusion and a

17
00:01:05,700 --> 00:01:06,960
discussion

18
00:01:06,960 --> 00:01:09,960
first first let me introduce the

19
00:01:09,960 --> 00:01:12,720
background and motivation I will

20
00:01:12,720 --> 00:01:15,360
introduce the cloud native Computing and

21
00:01:15,360 --> 00:01:17,600
its implications at the background

22
00:01:17,600 --> 00:01:22,320
followed by the motivation of this paper

23
00:01:22,320 --> 00:01:25,140
first recent shift in the production

24
00:01:25,140 --> 00:01:27,960
Cloud environment from monolithic design

25
00:01:27,960 --> 00:01:31,040
to macro service based architecture

26
00:01:31,040 --> 00:01:34,140
implies the prosperity of cloud native

27
00:01:34,140 --> 00:01:35,400
computing

28
00:01:35,400 --> 00:01:38,119
lastly the fundamentals of cloud native

29
00:01:38,119 --> 00:01:42,020
infrastructure the first sign is a

30
00:01:42,020 --> 00:01:46,200
either saved from a monolithic design to

31
00:01:46,200 --> 00:01:49,020
a single concern Loosely cut the macro

32
00:01:49,020 --> 00:01:51,780
Services those micro services are

33
00:01:51,780 --> 00:01:54,720
desolated Twilight deployed at at the

34
00:01:54,720 --> 00:01:57,600
physical virtual machines containers are

35
00:01:57,600 --> 00:02:00,899
referred to as a host in this paper

36
00:02:00,899 --> 00:02:04,020
various applications for example

37
00:02:04,020 --> 00:02:06,799
e-commerce Financial transactions

38
00:02:06,799 --> 00:02:09,780
operates at disposable

39
00:02:09,780 --> 00:02:13,200
and the immutable underlying system

40
00:02:13,200 --> 00:02:16,020
uh we observed at such infrastructure

41
00:02:16,020 --> 00:02:19,580
has several implications

42
00:02:19,739 --> 00:02:23,400
first each Macro service has much

43
00:02:23,400 --> 00:02:26,459
structure Quality quality of service or

44
00:02:26,459 --> 00:02:30,000
qos requirements since they are changed

45
00:02:30,000 --> 00:02:33,060
together to serve users however the

46
00:02:33,060 --> 00:02:34,920
environment is highly resource

47
00:02:34,920 --> 00:02:36,260
constrained

48
00:02:36,260 --> 00:02:40,140
because to to lower the long-term

49
00:02:40,140 --> 00:02:43,620
capital expense of the cloud providers

50
00:02:43,620 --> 00:02:45,959
adopt adopted

51
00:02:45,959 --> 00:02:49,620
uh mix the deployment of services and

52
00:02:49,620 --> 00:02:52,200
fingering the results allocation for

53
00:02:52,200 --> 00:02:56,220
example Azure reports that more than 90

54
00:02:56,220 --> 00:02:59,519
percent uh virtual machines have only

55
00:02:59,519 --> 00:03:02,280
one to four CPU costs

56
00:03:02,280 --> 00:03:05,280
also each host has massive metrics with

57
00:03:05,280 --> 00:03:08,640
rapid variations including the up layer

58
00:03:08,640 --> 00:03:11,040
application Matrix and the fundamental

59
00:03:11,040 --> 00:03:14,159
system metrics which are monitored where

60
00:03:14,159 --> 00:03:17,099
specialized monitoring system

61
00:03:17,099 --> 00:03:20,099
to ensure the service to ensure the

62
00:03:20,099 --> 00:03:23,340
service level uh agreement

63
00:03:23,340 --> 00:03:26,400
this implications for several challenges

64
00:03:26,400 --> 00:03:31,260
to the cloud native monitoring

65
00:03:31,260 --> 00:03:34,200
as shown in this slide traditional

66
00:03:34,200 --> 00:03:37,200
monitoring system I deployed together

67
00:03:37,200 --> 00:03:42,420
with Services however inevitably uh

68
00:03:42,420 --> 00:03:45,080
pause resource conditions with Services

69
00:03:45,080 --> 00:03:48,299
running at the same host

70
00:03:48,299 --> 00:03:50,280
uh

71
00:03:50,280 --> 00:03:53,580
the monitor occupies uh hosts the CPU to

72
00:03:53,580 --> 00:03:57,260
collect a process and upload the

73
00:03:57,260 --> 00:04:00,840
metrics where kernel stack will observe

74
00:04:00,840 --> 00:04:02,940
two kinds of mutual interference between

75
00:04:02,940 --> 00:04:05,280
service and monitor

76
00:04:05,280 --> 00:04:09,299
first enabling monitors called jators of

77
00:04:09,299 --> 00:04:11,939
online service in especially during

78
00:04:11,939 --> 00:04:14,159
shopping festivals with high loads

79
00:04:14,159 --> 00:04:17,760
because Monitor and service processes uh

80
00:04:17,760 --> 00:04:21,120
may be scheduled to the same CPU call

81
00:04:21,120 --> 00:04:26,280
with CPU bonding with default CPU

82
00:04:26,280 --> 00:04:28,139
scheduling policy

83
00:04:28,139 --> 00:04:30,479
second the monitor process may be

84
00:04:30,479 --> 00:04:33,900
blocked when CPU quota is limited or

85
00:04:33,900 --> 00:04:37,040
requesting monitoring metrics where the

86
00:04:37,040 --> 00:04:40,800
service where the BDA service interface

87
00:04:40,800 --> 00:04:42,780
for example

88
00:04:42,780 --> 00:04:46,979
a radius radius application exports its

89
00:04:46,979 --> 00:04:50,220
Matrix where the same interface as a

90
00:04:50,220 --> 00:04:53,160
service uh uh

91
00:04:53,160 --> 00:04:55,860
uh however assign authenticated

92
00:04:55,860 --> 00:04:58,320
dedicated call or interface for

93
00:04:58,320 --> 00:05:00,900
monitoring May avoid such interference

94
00:05:00,900 --> 00:05:04,620
however still cause a large studio

95
00:05:04,620 --> 00:05:07,320
called large Capital expense

96
00:05:07,320 --> 00:05:10,320
with the CPU overhead

97
00:05:10,320 --> 00:05:12,660
we further elaborate such Mutual

98
00:05:12,660 --> 00:05:15,600
interface in the real deployment in the

99
00:05:15,600 --> 00:05:18,120
left figure the key value style service

100
00:05:18,120 --> 00:05:21,960
into it or redis is interfered by the

101
00:05:21,960 --> 00:05:23,820
next data monitor

102
00:05:23,820 --> 00:05:26,220
what comes up that what comes up that

103
00:05:26,220 --> 00:05:29,460
bus uh throughput and latency cheaters

104
00:05:29,460 --> 00:05:32,639
uh in each monitoring cycle

105
00:05:32,639 --> 00:05:36,479
the net data process is scheduled

106
00:05:36,479 --> 00:05:39,960
uh in the red figure uh we we observe

107
00:05:39,960 --> 00:05:42,660
another cancer interference that the

108
00:05:42,660 --> 00:05:44,820
monitoring latency of net data or

109
00:05:44,820 --> 00:05:47,639
Prometheus jetters due to the high loads

110
00:05:47,639 --> 00:05:50,820
of small CPU quota of the

111
00:05:50,820 --> 00:05:53,960
monitor the host

112
00:05:54,060 --> 00:05:57,240
to avoid the results conditions and the

113
00:05:57,240 --> 00:05:59,400
ensure quality of service are monitoring

114
00:05:59,400 --> 00:06:02,220
we propose the zero or head monitoring

115
00:06:02,220 --> 00:06:04,259
system of zero four shots

116
00:06:04,259 --> 00:06:07,500
the calendar is a decoupling monitor

117
00:06:07,500 --> 00:06:09,600
from the monitored infrastructure which

118
00:06:09,600 --> 00:06:13,740
is visible from two aspects first we

119
00:06:13,740 --> 00:06:16,440
observe that most of Matrix counters

120
00:06:16,440 --> 00:06:19,919
updated and fixed the memory region

121
00:06:19,919 --> 00:06:24,419
and the process is a romantic raw Matrix

122
00:06:24,419 --> 00:06:27,600
sample and reproducible algebraic

123
00:06:27,600 --> 00:06:29,100
calculations

124
00:06:29,100 --> 00:06:32,759
for example uh generally the Percival

125
00:06:32,759 --> 00:06:35,520
counters are summed together to obtain

126
00:06:35,520 --> 00:06:39,600
the final final uh to obtain the final

127
00:06:39,600 --> 00:06:43,979
counters second RDMA is commonly

128
00:06:43,979 --> 00:06:47,100
supported in in the state state of the

129
00:06:47,100 --> 00:06:49,800
art cloud or data centers

130
00:06:49,800 --> 00:06:54,120
uh RDMA enables both CPU and CPU and

131
00:06:54,120 --> 00:06:57,000
kernel bypass with one-sided operation

132
00:06:57,000 --> 00:06:59,220
for example write a read

133
00:06:59,220 --> 00:07:02,580
so as a result zero can perform RDMA

134
00:07:02,580 --> 00:07:05,100
rate on the rejected memory region to

135
00:07:05,100 --> 00:07:08,100
collect the romance the whole process is

136
00:07:08,100 --> 00:07:12,259
independent of the monitored host

137
00:07:13,199 --> 00:07:16,380
next is the code design of the of the

138
00:07:16,380 --> 00:07:19,500
zero monitoring system

139
00:07:19,500 --> 00:07:22,680
uh this figure gives the overview of

140
00:07:22,680 --> 00:07:24,479
zero framework

141
00:07:24,479 --> 00:07:25,819
mm-hmm

142
00:07:25,819 --> 00:07:30,960
uh we designed zero to resolve three uh

143
00:07:30,960 --> 00:07:33,599
to mainly without three challenges uh

144
00:07:33,599 --> 00:07:36,419
first we observe that most of the CPU

145
00:07:36,419 --> 00:07:38,940
time or traditional monitor are spent on

146
00:07:38,940 --> 00:07:41,699
collecting Matrix from system or

147
00:07:41,699 --> 00:07:44,699
application processes uh there also

148
00:07:44,699 --> 00:07:48,300
needs to eliminate such overhead besides

149
00:07:48,300 --> 00:07:50,819
the uploading overhead upload uploaded

150
00:07:50,819 --> 00:07:53,639
to RDMA Nick

151
00:07:53,639 --> 00:07:56,880
second each controller plugs The Matrix

152
00:07:56,880 --> 00:08:01,259
from multi-host of many QP Connections

153
00:08:01,259 --> 00:08:06,120
in a large scale distributed monitoring

154
00:08:06,120 --> 00:08:08,880
uh however it may cause encounter

155
00:08:08,880 --> 00:08:10,199
problem

156
00:08:10,199 --> 00:08:12,840
so the controller is also bottlenecked

157
00:08:12,840 --> 00:08:16,039
on CPU beside the network as all

158
00:08:16,039 --> 00:08:18,360
plugging processing and uploading

159
00:08:18,360 --> 00:08:22,259
processes uh are uploaded to the uh

160
00:08:22,259 --> 00:08:24,720
remote controller

161
00:08:24,720 --> 00:08:28,259
so zero propose the normal control and

162
00:08:28,259 --> 00:08:29,940
the data plane designed to resolve

163
00:08:29,940 --> 00:08:32,580
challenge one introduce a receiver

164
00:08:32,580 --> 00:08:34,679
driving conduction control or flow

165
00:08:34,679 --> 00:08:37,860
control through a result challenge 2 and

166
00:08:37,860 --> 00:08:40,500
incorporate efficient threading and I O

167
00:08:40,500 --> 00:08:44,520
model to resolve challenge 3.

168
00:08:44,520 --> 00:08:46,860
uh first I will introduce the control

169
00:08:46,860 --> 00:08:49,560
plane and the display of their framework

170
00:08:49,560 --> 00:08:52,200
the control plane provides Universal

171
00:08:52,200 --> 00:08:55,380
interfere interface for application or

172
00:08:55,380 --> 00:08:59,459
system to uh to register

173
00:08:59,459 --> 00:09:02,459
The Matrix and update the cross

174
00:09:02,459 --> 00:09:05,519
crossbody metadata metadata in the

175
00:09:05,519 --> 00:09:08,839
control control region

176
00:09:08,839 --> 00:09:12,899
note that such reject reject attrition

177
00:09:12,899 --> 00:09:15,180
overhead is disposable for system

178
00:09:15,180 --> 00:09:18,839
metrics and metrics of persistence or

179
00:09:18,839 --> 00:09:21,300
title match or Title Service because

180
00:09:21,300 --> 00:09:24,300
those Services those services will run

181
00:09:24,300 --> 00:09:25,800
for a long time

182
00:09:25,800 --> 00:09:30,560
after reject after reject treated

183
00:09:31,160 --> 00:09:35,279
for serverless computing uh there are

184
00:09:35,279 --> 00:09:37,920
further use item right to update the

185
00:09:37,920 --> 00:09:39,180
metadata

186
00:09:39,180 --> 00:09:41,360
uh What uh

187
00:09:41,360 --> 00:09:45,779
specifically once they uh metadata is

188
00:09:45,779 --> 00:09:49,560
updated zero use Adam right to uh to

189
00:09:49,560 --> 00:09:53,519
update the metadata of their Matrix as a

190
00:09:53,519 --> 00:09:54,720
result

191
00:09:54,720 --> 00:09:57,360
uh the remote controller can read the

192
00:09:57,360 --> 00:10:01,700
romance with only one rtt where aware

193
00:10:01,700 --> 00:10:04,230
IDM read

194
00:10:04,230 --> 00:10:04,980
[Music]

195
00:10:04,980 --> 00:10:05,880
um

196
00:10:05,880 --> 00:10:08,339
the control plane also managed on the

197
00:10:08,339 --> 00:10:11,100
share QP share QP connection for all

198
00:10:11,100 --> 00:10:14,760
metrics and at the same host to achieve

199
00:10:14,760 --> 00:10:17,640
largest scalability in the reliable

200
00:10:17,640 --> 00:10:20,279
connection or assay mode

201
00:10:20,279 --> 00:10:24,120
next is the next is the

202
00:10:24,120 --> 00:10:26,240
data plane

203
00:10:26,240 --> 00:10:29,339
enter the discipline zero excessive

204
00:10:29,339 --> 00:10:32,760
excessively adopted a shared memory to

205
00:10:32,760 --> 00:10:35,580
avoid any CPU involvements for memory

206
00:10:35,580 --> 00:10:36,680
copy

207
00:10:36,680 --> 00:10:41,160
uh besides we we also observed that

208
00:10:41,160 --> 00:10:44,760
the metrics are distributed and then

209
00:10:44,760 --> 00:10:47,940
continuous in in the memories uh space

210
00:10:47,940 --> 00:10:52,260
of a kernel or application so there will

211
00:10:52,260 --> 00:10:54,140
also provide a memory management

212
00:10:54,140 --> 00:10:57,420
interface interface to a great to

213
00:10:57,420 --> 00:11:00,899
aggregate Max to continuous memory

214
00:11:00,899 --> 00:11:01,860
region

215
00:11:01,860 --> 00:11:05,279
uh as a result for each instance there

216
00:11:05,279 --> 00:11:07,440
are only needs to record the one memory

217
00:11:07,440 --> 00:11:09,980
region entry and perform

218
00:11:09,980 --> 00:11:15,320
read to acquire all metrics

219
00:11:16,380 --> 00:11:19,740
um to support a large-scale Distributing

220
00:11:19,740 --> 00:11:22,880
monitoring they will adopt a threat

221
00:11:22,880 --> 00:11:25,560
patching model

222
00:11:25,560 --> 00:11:29,160
first only one or two collecting thread

223
00:11:29,160 --> 00:11:33,300
is used to uh post RDM read request then

224
00:11:33,300 --> 00:11:37,380
polling compilations on multi-qps in

225
00:11:37,380 --> 00:11:38,459
parallel

226
00:11:38,459 --> 00:11:42,540
because boss post and polling operation

227
00:11:42,540 --> 00:11:46,140
operations are named blocking

228
00:11:46,140 --> 00:11:48,959
on the collecting thread a receiver

229
00:11:48,959 --> 00:11:52,800
driving model is used to limit the total

230
00:11:52,800 --> 00:11:55,079
e-flight data

231
00:11:55,079 --> 00:11:58,760
our all connections were created

232
00:11:58,760 --> 00:12:02,839
as a result they build up kill at

233
00:12:02,839 --> 00:12:05,339
switches can be controlled where the

234
00:12:05,339 --> 00:12:06,320
creative

235
00:12:06,320 --> 00:12:08,820
currently we

236
00:12:08,820 --> 00:12:12,000
determine the credits where where the

237
00:12:12,000 --> 00:12:15,920
PFC or Isa threshold and the number of

238
00:12:15,920 --> 00:12:18,839
concurrent QP connections

239
00:12:18,839 --> 00:12:22,740
so we observe that such a receiver joint

240
00:12:22,740 --> 00:12:25,740
model is efficient to avoiding cast with

241
00:12:25,740 --> 00:12:29,160
many connections and also guarantee the

242
00:12:29,160 --> 00:12:31,040
qos level

243
00:12:31,040 --> 00:12:35,160
of the monitor itself

244
00:12:35,160 --> 00:12:36,060
um

245
00:12:36,060 --> 00:12:39,240
the company the accomplished requests

246
00:12:39,240 --> 00:12:41,579
are dispatched to a processing thread

247
00:12:41,579 --> 00:12:44,040
for further persistence and

248
00:12:44,040 --> 00:12:47,040
visualization

249
00:12:47,399 --> 00:12:50,700
next is implementation and evaluation

250
00:12:50,700 --> 00:12:53,760
setups and the experimental results

251
00:12:53,760 --> 00:12:55,260
uh

252
00:12:55,260 --> 00:12:58,380
uh we Implement their framework and

253
00:12:58,380 --> 00:13:00,920
demonstrate its

254
00:13:00,920 --> 00:13:04,139
generality with three key case studies

255
00:13:04,139 --> 00:13:07,680
including redis links kernel and ebpf

256
00:13:07,680 --> 00:13:10,920
metrics uh this figure uh

257
00:13:10,920 --> 00:13:13,079
shows two types of memory management

258
00:13:13,079 --> 00:13:15,600
interface

259
00:13:15,600 --> 00:13:18,839
uh we did we deploy 0 into typical

260
00:13:18,839 --> 00:13:21,899
clusters uh virtualize the environment

261
00:13:21,899 --> 00:13:25,320
uh in the testing class cluster and the

262
00:13:25,320 --> 00:13:28,620
environmental environment with container

263
00:13:28,620 --> 00:13:31,440
radiation in the public cloud

264
00:13:31,440 --> 00:13:34,560
uh where you allowed to Zero from CPU

265
00:13:34,560 --> 00:13:37,560
utilization at both agent on the

266
00:13:37,560 --> 00:13:38,839
controller side

267
00:13:38,839 --> 00:13:42,000
monitoring latency and throughput we

268
00:13:42,000 --> 00:13:44,060
also test the zero with a different

269
00:13:44,060 --> 00:13:47,240
parameters including sampling intervals

270
00:13:47,240 --> 00:13:51,360
different number of instance and a

271
00:13:51,360 --> 00:13:53,720
different number of host or connections

272
00:13:53,720 --> 00:13:57,360
the sample interval determines the

273
00:13:57,360 --> 00:14:00,240
monitoring quality of service determine

274
00:14:00,240 --> 00:14:02,579
the Qs or monitoring

275
00:14:02,579 --> 00:14:06,180
and the number of inter uh Denver or

276
00:14:06,180 --> 00:14:10,740
instance mainly uh uh impact the size of

277
00:14:10,740 --> 00:14:15,540
Matrix and the number of hosts uh shows

278
00:14:15,540 --> 00:14:18,839
the scalability of zero

279
00:14:18,839 --> 00:14:22,079
uh we can't we compare zero with Legends

280
00:14:22,079 --> 00:14:25,019
Tools in our cloud and the status of the

281
00:14:25,019 --> 00:14:28,040
Arts monitor use the by large Cloud

282
00:14:28,040 --> 00:14:32,100
providers for example net data and we

283
00:14:32,100 --> 00:14:37,200
also use their RPC Implement uh with uh

284
00:14:37,200 --> 00:14:40,740
where uh name idea may send and received

285
00:14:40,740 --> 00:14:43,620
as a Baseline

286
00:14:43,620 --> 00:14:46,680
uh this table uh summarized the overhead

287
00:14:46,680 --> 00:14:50,639
of their in three case studies uh

288
00:14:50,639 --> 00:14:53,459
first we observe that the overhead or

289
00:14:53,459 --> 00:14:56,959
control plane is disposable

290
00:14:56,959 --> 00:15:00,839
because it's not affected uh

291
00:15:00,839 --> 00:15:03,959
by the sample interval as showing the

292
00:15:03,959 --> 00:15:07,160
first figure already's case

293
00:15:07,160 --> 00:15:11,399
also the data plane has zero or had as

294
00:15:11,399 --> 00:15:15,660
effective as expected because the agent

295
00:15:15,660 --> 00:15:18,720
process is blocked after all metrics are

296
00:15:18,720 --> 00:15:22,620
registered uh besides uh there are also

297
00:15:22,620 --> 00:15:25,019
reduced latency by one to two other

298
00:15:25,019 --> 00:15:26,880
magnitudes

299
00:15:26,880 --> 00:15:31,620
compared with TCT Baseline gcp Baseline

300
00:15:31,620 --> 00:15:32,220
um

301
00:15:32,220 --> 00:15:36,240
here we we only uh presents the details

302
00:15:36,240 --> 00:15:39,240
of reddit's case you can refer to our

303
00:15:39,240 --> 00:15:41,519
paper for more details of other case

304
00:15:41,519 --> 00:15:43,920
studies

305
00:15:43,920 --> 00:15:45,779
uh

306
00:15:45,779 --> 00:15:49,920
of this slides gives without a large

307
00:15:49,920 --> 00:15:52,620
scale distributed monitoring

308
00:15:52,620 --> 00:15:55,019
um first I will observe that there

309
00:15:55,019 --> 00:15:58,500
achieves much higher throughput and uh

310
00:15:58,500 --> 00:16:01,920
and the lower latency compared with the

311
00:16:01,920 --> 00:16:05,120
traditional TCP Baseline

312
00:16:05,120 --> 00:16:08,579
besides it also avoid a

313
00:16:08,579 --> 00:16:12,060
trigging in cast avoiding cast and

314
00:16:12,060 --> 00:16:15,540
trigging PFC or ecl with the created

315
00:16:15,540 --> 00:16:17,699
based flow control

316
00:16:17,699 --> 00:16:21,180
another by the another benefit is is the

317
00:16:21,180 --> 00:16:25,620
lower CPU utilization without useless PD

318
00:16:25,620 --> 00:16:29,480
polling when congestion happens

319
00:16:29,880 --> 00:16:32,040
um as a compassion

320
00:16:32,040 --> 00:16:34,019
um azero

321
00:16:34,019 --> 00:16:36,019
um

322
00:16:36,420 --> 00:16:39,600
a table map uh stable quality or service

323
00:16:39,600 --> 00:16:43,620
in the whole collecting phase

324
00:16:43,620 --> 00:16:46,380
um in in the

325
00:16:46,380 --> 00:16:48,720
in the left I figure we compare the

326
00:16:48,720 --> 00:16:53,420
created flow control with a simple uh

327
00:16:53,420 --> 00:16:55,800
bandwidth delay product based flow

328
00:16:55,800 --> 00:16:59,100
control we can observe that

329
00:16:59,100 --> 00:17:01,199
a PDP based on the floor column to

330
00:17:01,199 --> 00:17:06,140
easily cause ECM marks and and

331
00:17:06,140 --> 00:17:10,380
the under the survey uh a severe

332
00:17:10,380 --> 00:17:13,380
throughput declines

333
00:17:13,380 --> 00:17:17,040
um our credit flow control keeps a

334
00:17:17,040 --> 00:17:18,660
stable performance in the whole

335
00:17:18,660 --> 00:17:21,919
monitoring process

336
00:17:23,099 --> 00:17:26,480
uh here comes to the conclusion

337
00:17:26,480 --> 00:17:29,580
in this paper we propose the zero High

338
00:17:29,580 --> 00:17:31,440
monitoring for the first time for the

339
00:17:31,440 --> 00:17:33,540
first time to the best of our knowledge

340
00:17:33,540 --> 00:17:37,520
which effectively explores one set data

341
00:17:37,520 --> 00:17:41,580
RDMA operation or RDMA read and the

342
00:17:41,580 --> 00:17:43,860
designs the normal control and the data

343
00:17:43,860 --> 00:17:46,860
plane to achieve this goal and there are

344
00:17:46,860 --> 00:17:49,140
also supports large-scale distributory

345
00:17:49,140 --> 00:17:50,900
monitoring where highly efficient

346
00:17:50,900 --> 00:17:53,700
threading and I O model under the

347
00:17:53,700 --> 00:17:56,520
receiver German flow control

348
00:17:56,520 --> 00:18:00,720
next is this discussion of about our

349
00:18:00,720 --> 00:18:04,500
experience and future work

350
00:18:04,500 --> 00:18:06,960
first to achieve high scalability and

351
00:18:06,960 --> 00:18:11,220
availability without we adopt QP sharing

352
00:18:11,220 --> 00:18:13,440
and group of switching

353
00:18:13,440 --> 00:18:15,299
um

354
00:18:15,299 --> 00:18:18,440
uh policy uh

355
00:18:18,440 --> 00:18:23,100
to to achieve High availability uh many

356
00:18:23,100 --> 00:18:25,679
standby controllers at least to standby

357
00:18:25,679 --> 00:18:27,380
controllers

358
00:18:27,380 --> 00:18:31,020
are used when when a main controller is

359
00:18:31,020 --> 00:18:32,220
done

360
00:18:32,220 --> 00:18:35,340
uh also we opened up that the receiver

361
00:18:35,340 --> 00:18:38,160
Drive model is helpful to avoid the net

362
00:18:38,160 --> 00:18:40,799
Network interference

363
00:18:40,799 --> 00:18:41,460
um

364
00:18:41,460 --> 00:18:45,720
uh the traditional method usually adopt

365
00:18:45,720 --> 00:18:47,760
physical isolation

366
00:18:47,760 --> 00:18:51,900
where different uh physical links

367
00:18:51,900 --> 00:18:55,440
and however it may introduce uh

368
00:18:55,440 --> 00:18:58,740
High toast high high cost

369
00:18:58,740 --> 00:19:03,299
um and also they use a lower priority

370
00:19:03,299 --> 00:19:06,720
Queue at the switch however it will

371
00:19:06,720 --> 00:19:09,860
frequently cause PEC loss and timeout

372
00:19:09,860 --> 00:19:12,600
which cause severe performance

373
00:19:12,600 --> 00:19:16,440
degradation of RDMA system

374
00:19:16,440 --> 00:19:18,960
as a compassion we observe that the

375
00:19:18,960 --> 00:19:22,140
receiver Jam model is effective to avoid

376
00:19:22,140 --> 00:19:25,020
the natural Network interference by con

377
00:19:25,020 --> 00:19:27,600
by controlling the build up Queue at the

378
00:19:27,600 --> 00:19:29,820
switches

379
00:19:29,820 --> 00:19:31,520
uh also

380
00:19:31,520 --> 00:19:34,940
compared with a state of without a

381
00:19:34,940 --> 00:19:37,100
congested control

382
00:19:37,100 --> 00:19:40,880
mechanisms we observe that

383
00:19:40,880 --> 00:19:44,340
existing CC mechanism

384
00:19:44,340 --> 00:19:46,740
and the data center aims to achieve

385
00:19:46,740 --> 00:19:50,940
equal bandwidth sharing and we also rely

386
00:19:50,940 --> 00:19:52,080
on

387
00:19:52,080 --> 00:19:55,080
switches

388
00:20:00,320 --> 00:20:04,080
however the uh the receiver address CC

389
00:20:04,080 --> 00:20:06,240
uh

390
00:20:06,240 --> 00:20:12,000
simply use credit and didn't uh rely on

391
00:20:12,000 --> 00:20:14,100
on any

392
00:20:14,100 --> 00:20:18,059
signal from switches because the uh

393
00:20:18,059 --> 00:20:22,559
because the receiver knows uh if if the

394
00:20:22,559 --> 00:20:25,039
congestion happens

395
00:20:25,039 --> 00:20:28,679
uh another uh another observation is

396
00:20:28,679 --> 00:20:32,460
observation that uh currently we use a

397
00:20:32,460 --> 00:20:36,840
lower PDP uh we will use a small small

398
00:20:36,840 --> 00:20:39,720
credit lower than PPP

399
00:20:39,720 --> 00:20:45,000
because uh the because because the uh E7

400
00:20:45,000 --> 00:20:47,580
or Eastern or PFC Threshold at the

401
00:20:47,580 --> 00:20:52,379
switch is is small

402
00:20:52,880 --> 00:20:56,400
in our future work we we will use the

403
00:20:56,400 --> 00:20:59,480
combined mechanism of pacing and credit

404
00:20:59,480 --> 00:21:04,140
uh to avoid congestion and switches and

405
00:21:04,140 --> 00:21:08,160
both at the end or next

406
00:21:08,160 --> 00:21:13,280
uh uh and we I also wish to extend our

407
00:21:13,280 --> 00:21:16,440
mechanism to General case

408
00:21:16,440 --> 00:21:20,000
uh that's all thank you

