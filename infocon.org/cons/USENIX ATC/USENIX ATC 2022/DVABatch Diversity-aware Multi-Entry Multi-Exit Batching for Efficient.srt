1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:13,639 --> 00:00:17,520
hello everyone I'm very glad to be here

3
00:00:17,520 --> 00:00:19,920
to share our recent work at annual

4
00:00:19,920 --> 00:00:22,080
technical conference

5
00:00:22,080 --> 00:00:24,720
I'm with Halsey from Shanghai University

6
00:00:24,720 --> 00:00:28,439
my supervisor is Professor Chen Chun the

7
00:00:28,439 --> 00:00:30,800
title of our work is still a badge

8
00:00:30,800 --> 00:00:33,120
diversity aware of multi-entry

9
00:00:33,120 --> 00:00:35,940
multi-exit betting for efficient

10
00:00:35,940 --> 00:00:39,360
processing audio and services on gpus it

11
00:00:39,360 --> 00:00:41,640
is about efficient processing of best

12
00:00:41,640 --> 00:00:44,340
student cruise on gpus

13
00:00:44,340 --> 00:00:46,260
I will start from background and

14
00:00:46,260 --> 00:00:48,800
motivation

15
00:00:48,800 --> 00:00:51,899
nowadays dealing techniques are powering

16
00:00:51,899 --> 00:00:54,600
cloud services for example image

17
00:00:54,600 --> 00:00:57,600
classification and speech recognition

18
00:00:57,600 --> 00:01:00,180
since user queries have straightened to

19
00:01:00,180 --> 00:01:02,219
curious requirements

20
00:01:02,219 --> 00:01:06,900
dedicated accelerators at gpus tpus and

21
00:01:06,900 --> 00:01:09,840
npus are used to speed up the real

22
00:01:09,840 --> 00:01:11,400
inferences

23
00:01:11,400 --> 00:01:14,880
gpus have much computing power therefore

24
00:01:14,880 --> 00:01:17,580
it is important to process even-based

25
00:01:17,580 --> 00:01:21,979
Services efficiently on gpus

26
00:01:22,200 --> 00:01:25,920
a common practice is using patching to

27
00:01:25,920 --> 00:01:28,320
improve Computing efficiency

28
00:01:28,320 --> 00:01:31,320
here we show a simple example of best

29
00:01:31,320 --> 00:01:34,380
image classification

30
00:01:34,380 --> 00:01:37,860
we first deploy the models on gpus by

31
00:01:37,860 --> 00:01:40,220
serving Frameworks like a Triton

32
00:01:40,220 --> 00:01:43,979
tensorflow serving and others

33
00:01:43,979 --> 00:01:46,740
then the Frameworks provides a batch

34
00:01:46,740 --> 00:01:50,939
entry to match the accepted queries

35
00:01:50,939 --> 00:01:53,579
commonly they also provide a thumb

36
00:01:53,579 --> 00:01:56,399
window to batch the queries a series

37
00:01:56,399 --> 00:02:01,020
often arrives randomly after that the

38
00:02:01,020 --> 00:02:04,259
best queries are processed at once with

39
00:02:04,259 --> 00:02:06,960
high parallelism on gpus

40
00:02:06,960 --> 00:02:09,899
at last they will exit the batch

41
00:02:09,899 --> 00:02:13,560
together and the results are returned in

42
00:02:13,560 --> 00:02:16,319
this case we improve the utilization of

43
00:02:16,319 --> 00:02:20,280
gpus and guarantee that the queries are

44
00:02:20,280 --> 00:02:23,060
processed as soon as possible

45
00:02:23,060 --> 00:02:27,239
however our observation reveals that

46
00:02:27,239 --> 00:02:30,300
diversities diminish the efficiency of

47
00:02:30,300 --> 00:02:32,879
such signal entry single exit best

48
00:02:32,879 --> 00:02:34,319
scheme

49
00:02:34,319 --> 00:02:37,379
we were introduced three diversities in

50
00:02:37,379 --> 00:02:38,520
this work

51
00:02:38,520 --> 00:02:40,920
and there are more diversities in

52
00:02:40,920 --> 00:02:43,379
production

53
00:02:43,379 --> 00:02:47,160
first diversity is the input diversity

54
00:02:47,160 --> 00:02:50,280
input diversity widely exists Indian

55
00:02:50,280 --> 00:02:53,400
services for example natural language

56
00:02:53,400 --> 00:02:55,860
processing services

57
00:02:55,860 --> 00:02:57,780
here we show the sequence left

58
00:02:57,780 --> 00:03:00,840
distribution in 10 workloads of the blue

59
00:03:00,840 --> 00:03:02,519
data set

60
00:03:02,519 --> 00:03:06,180
most the sentences have 5 to 20 words

61
00:03:06,180 --> 00:03:09,840
but somehow more than 100 words

62
00:03:09,840 --> 00:03:13,080
for these models the input of short

63
00:03:13,080 --> 00:03:15,900
queries is spreaded to the frame size of

64
00:03:15,900 --> 00:03:17,700
the longest

65
00:03:17,700 --> 00:03:21,720
in this case they can be best to run the

66
00:03:21,720 --> 00:03:24,720
benefits of battery may be negated by

67
00:03:24,720 --> 00:03:27,599
the rested computation for the padded

68
00:03:27,599 --> 00:03:28,620
part

69
00:03:28,620 --> 00:03:31,800
and ideal batching strategy is shown in

70
00:03:31,800 --> 00:03:32,940
the right

71
00:03:32,940 --> 00:03:36,120
the batch is divided into two smaller

72
00:03:36,120 --> 00:03:39,840
patches one for short queries and one

73
00:03:39,840 --> 00:03:42,620
for the long queries

74
00:03:42,620 --> 00:03:46,319
secondly the operator diversity

75
00:03:46,319 --> 00:03:49,200
the DL operators require different batch

76
00:03:49,200 --> 00:03:52,620
sizes to fully utilize the GPU

77
00:03:52,620 --> 00:03:56,000
as shown in the figure for GMA operator

78
00:03:56,000 --> 00:03:59,040
batching only increases its latency

79
00:03:59,040 --> 00:04:01,500
without improving the processing

80
00:04:01,500 --> 00:04:03,000
throughput

81
00:04:03,000 --> 00:04:06,120
for GMD operator using a batch size

82
00:04:06,120 --> 00:04:09,420
smaller than 8 is not able to utilize

83
00:04:09,420 --> 00:04:13,019
the GPU the processing time does not

84
00:04:13,019 --> 00:04:15,840
increase until the batch size is larger

85
00:04:15,840 --> 00:04:17,519
than 8.

86
00:04:17,519 --> 00:04:21,600
the preferred processes of gem a and Gem

87
00:04:21,600 --> 00:04:25,020
B are 1 and 8 respectively

88
00:04:25,020 --> 00:04:27,780
where all the operators of Ideal model

89
00:04:27,780 --> 00:04:31,199
share the same basis during inference

90
00:04:31,199 --> 00:04:35,880
in single entry and single exit patch

91
00:04:35,880 --> 00:04:39,900
they have different preferred batch size

92
00:04:39,900 --> 00:04:43,020
here I will show a simple example we

93
00:04:43,020 --> 00:04:46,040
assume operator a prefers best size for

94
00:04:46,040 --> 00:04:51,180
operator b c and d prefer best size one

95
00:04:51,180 --> 00:04:54,960
the batch of operator b c and d result

96
00:04:54,960 --> 00:04:57,960
in longer latency without throughput

97
00:04:57,960 --> 00:04:59,759
Improvement

98
00:04:59,759 --> 00:05:03,000
an ideal batching strategy is that we

99
00:05:03,000 --> 00:05:06,660
can run operator a with best size four

100
00:05:06,660 --> 00:05:09,720
and we split the batch into four smaller

101
00:05:09,720 --> 00:05:12,180
batches with a special size one at

102
00:05:12,180 --> 00:05:13,620
operator B

103
00:05:13,620 --> 00:05:16,500
and then we run a small batches

104
00:05:16,500 --> 00:05:19,160
sequentially

105
00:05:19,320 --> 00:05:21,060
in this case

106
00:05:21,060 --> 00:05:25,220
the latency is reduced

107
00:05:25,740 --> 00:05:28,680
the last one is low diversity

108
00:05:28,680 --> 00:05:31,560
service queries do not arrive in a

109
00:05:31,560 --> 00:05:33,240
uniform Tower

110
00:05:33,240 --> 00:05:36,419
in this case the number of queries

111
00:05:36,419 --> 00:05:38,759
collected in a single best time window

112
00:05:38,759 --> 00:05:40,199
varies

113
00:05:40,199 --> 00:05:42,419
as shown in the figure

114
00:05:42,419 --> 00:05:45,120
when a load bursts the subsequent

115
00:05:45,120 --> 00:05:47,699
queries have to wait for the completion

116
00:05:47,699 --> 00:05:50,940
of a previous known for batch

117
00:05:50,940 --> 00:05:54,060
best results in the long latency where

118
00:05:54,060 --> 00:05:57,240
there is still Hardware resource Idol

119
00:05:57,240 --> 00:06:01,259
ideally if we interrupt the ongoing

120
00:06:01,259 --> 00:06:03,960
patch and merge them with a newly

121
00:06:03,960 --> 00:06:07,380
arrived queries we can make better use

122
00:06:07,380 --> 00:06:09,860
of the GPU Hardware

123
00:06:09,860 --> 00:06:13,620
in summary single entry single exit

124
00:06:13,620 --> 00:06:16,080
scheme show with support for serving

125
00:06:16,080 --> 00:06:17,280
diversity

126
00:06:17,280 --> 00:06:20,039
firstly it assumes that that's always

127
00:06:20,039 --> 00:06:22,740
how positive effects for improving

128
00:06:22,740 --> 00:06:24,720
Hardware utilization

129
00:06:24,720 --> 00:06:27,900
secondary and algorithm passionate by

130
00:06:27,900 --> 00:06:30,720
inappropriate decision cannot be

131
00:06:30,720 --> 00:06:33,300
interrupted for adjustment

132
00:06:33,300 --> 00:06:36,840
thirdly the single entry single active

133
00:06:36,840 --> 00:06:39,680
scheme cannot be configured for support

134
00:06:39,680 --> 00:06:42,419
various diversities

135
00:06:42,419 --> 00:06:46,160
we now may have an idea can we have a

136
00:06:46,160 --> 00:06:49,819
multi-entry multi-exit batch scheme for

137
00:06:49,819 --> 00:06:53,280
adjusting these concerns this is what

138
00:06:53,280 --> 00:06:58,039
exactly the robot brings in

139
00:06:58,139 --> 00:07:01,259
now we introduce the methodology of Diva

140
00:07:01,259 --> 00:07:03,440
batch

141
00:07:03,500 --> 00:07:07,680
is a long-term best scheduling system it

142
00:07:07,680 --> 00:07:10,560
can be integrated into existing neon

143
00:07:10,560 --> 00:07:13,440
serving Frameworks the supports

144
00:07:13,440 --> 00:07:15,960
multi-entry multi-exit scheme for

145
00:07:15,960 --> 00:07:18,840
adjusting the batch on the Fly

146
00:07:18,840 --> 00:07:22,919
the proposed holistic solution can also

147
00:07:22,919 --> 00:07:25,139
be configured for different serving

148
00:07:25,139 --> 00:07:27,740
diversities

149
00:07:27,840 --> 00:07:30,419
support the multi-entry multi-exit

150
00:07:30,419 --> 00:07:34,500
scheme we First Slice their model into

151
00:07:34,500 --> 00:07:38,340
multiple stages each stage is excluded

152
00:07:38,340 --> 00:07:41,880
by a executor the executors are

153
00:07:41,880 --> 00:07:44,039
connected by batch queues

154
00:07:44,039 --> 00:07:46,860
this is scooters and best skills are

155
00:07:46,860 --> 00:07:49,440
coordinated through a vegetable

156
00:07:49,440 --> 00:07:52,440
the best table the first three meta

157
00:07:52,440 --> 00:07:56,039
operations new stress and split to meet

158
00:07:56,039 --> 00:07:58,819
the requirements of multi-entry and

159
00:07:58,819 --> 00:08:02,220
multi-exit scheme

160
00:08:02,220 --> 00:08:05,940
for a new DN service we identify the

161
00:08:05,940 --> 00:08:08,520
seven diversities through input check

162
00:08:08,520 --> 00:08:10,919
and kernel profiling

163
00:08:10,919 --> 00:08:14,220
then we will select the models after

164
00:08:14,220 --> 00:08:16,800
that we load the models and customize

165
00:08:16,800 --> 00:08:19,199
the scheduler accordingly

166
00:08:19,199 --> 00:08:22,440
finally Diva batch is ready to exploit

167
00:08:22,440 --> 00:08:27,620
meta operations for efficient processing

168
00:08:27,800 --> 00:08:31,259
as I mentioned the key abstraction of

169
00:08:31,259 --> 00:08:33,779
device is the metal operations for

170
00:08:33,779 --> 00:08:36,360
adjusting ongoing patch

171
00:08:36,360 --> 00:08:38,940
they are implemented by coordination of

172
00:08:38,940 --> 00:08:42,179
vegetable stage executors and batch

173
00:08:42,179 --> 00:08:43,380
queues

174
00:08:43,380 --> 00:08:46,320
detailed the new operation will add a

175
00:08:46,320 --> 00:08:50,040
new item in vegetable for the accepted

176
00:08:50,040 --> 00:08:53,519
queries the stress operation will merge

177
00:08:53,519 --> 00:08:56,880
new accepted queries with an existing

178
00:08:56,880 --> 00:08:58,860
item in batch table

179
00:08:58,860 --> 00:09:01,740
the split operation will split an

180
00:09:01,740 --> 00:09:04,380
existing item in the best table into

181
00:09:04,380 --> 00:09:06,360
several items

182
00:09:06,360 --> 00:09:09,480
new batches will be pushed into rescue

183
00:09:09,480 --> 00:09:12,360
through these meta operations and the

184
00:09:12,360 --> 00:09:15,360
executors will process the queries and

185
00:09:15,360 --> 00:09:19,700
updates the best status accordingly

186
00:09:19,700 --> 00:09:22,200
excluders are processing queries

187
00:09:22,200 --> 00:09:23,820
independently

188
00:09:23,820 --> 00:09:26,880
the left figure shows the way the stages

189
00:09:26,880 --> 00:09:28,920
are connected

190
00:09:28,920 --> 00:09:31,760
multiple buffers are used because

191
00:09:31,760 --> 00:09:34,140
multiple batches may be active

192
00:09:34,140 --> 00:09:36,060
concurrently

193
00:09:36,060 --> 00:09:39,779
an Executor needs to obtain an input

194
00:09:39,779 --> 00:09:42,899
output buffer pair before it can process

195
00:09:42,899 --> 00:09:46,440
a fetch therefore we need to manage the

196
00:09:46,440 --> 00:09:48,300
specific scooters

197
00:09:48,300 --> 00:09:50,940
firstly we need to avoid the hazards

198
00:09:50,940 --> 00:09:53,700
during processing queries with multiple

199
00:09:53,700 --> 00:09:55,320
buffer pairs

200
00:09:55,320 --> 00:09:58,200
to achieve this we send four states of

201
00:09:58,200 --> 00:10:00,560
buffer pairs used for connecting

202
00:10:00,560 --> 00:10:04,260
adjacent executors and three states for

203
00:10:04,260 --> 00:10:06,120
a stage executor

204
00:10:06,120 --> 00:10:08,580
the right figure shows the standard

205
00:10:08,580 --> 00:10:10,140
transition rules

206
00:10:10,140 --> 00:10:13,260
a special executor can process a batch

207
00:10:13,260 --> 00:10:16,500
only with a successfully obtains a

208
00:10:16,500 --> 00:10:18,839
legitimate buffer pair

209
00:10:18,839 --> 00:10:21,899
and the buffer pair is legitimate when

210
00:10:21,899 --> 00:10:25,440
both the input and output buffer are

211
00:10:25,440 --> 00:10:28,380
available or the input buffer is

212
00:10:28,380 --> 00:10:30,899
available by the output buffer is

213
00:10:30,899 --> 00:10:33,260
invalided

214
00:10:33,260 --> 00:10:36,660
however this state transition rules

215
00:10:36,660 --> 00:10:39,660
assume all the state executors use the

216
00:10:39,660 --> 00:10:42,839
buffer pairs in some fixed order for

217
00:10:42,839 --> 00:10:46,320
example ID order it only works well for

218
00:10:46,320 --> 00:10:50,940
the single entry single exit scheme

219
00:10:50,940 --> 00:10:54,000
this rules will encounter the validity

220
00:10:54,000 --> 00:10:57,839
problem for the multi-entry multi-exit

221
00:10:57,839 --> 00:10:59,579
scheme

222
00:10:59,579 --> 00:11:02,519
in multi-entry multi-active scheme

223
00:11:02,519 --> 00:11:05,339
stretch and split will merge or split

224
00:11:05,339 --> 00:11:09,120
output into multiple buffer pairs

225
00:11:09,120 --> 00:11:12,480
that means some static scooters may use

226
00:11:12,480 --> 00:11:15,720
more buffer pairs than others and the

227
00:11:15,720 --> 00:11:18,720
excess order of these buffer pairs is

228
00:11:18,720 --> 00:11:19,980
scrambled

229
00:11:19,980 --> 00:11:22,620
as shown in the left figure

230
00:11:22,620 --> 00:11:26,100
when executor I is using bp3 for

231
00:11:26,100 --> 00:11:27,200
execution

232
00:11:27,200 --> 00:11:32,220
executor I plus 1 is active with bp2

233
00:11:32,220 --> 00:11:36,300
such inconsistency invalidates the above

234
00:11:36,300 --> 00:11:38,880
State transition rule

235
00:11:38,880 --> 00:11:41,880
the stage executors need to run multiple

236
00:11:41,880 --> 00:11:44,399
batches in parallel for load diversity

237
00:11:44,399 --> 00:11:47,519
and input diversity by the long branches

238
00:11:47,519 --> 00:11:50,579
sequentially for operator diversity

239
00:11:50,579 --> 00:11:53,399
I'll show you the right figure the stage

240
00:11:53,399 --> 00:11:56,279
executors always draw in parallel even

241
00:11:56,279 --> 00:11:59,519
if only one buffer pair is used

242
00:11:59,519 --> 00:12:03,839
because excluded I legitimize bp1 for

243
00:12:03,839 --> 00:12:07,320
executor I minus 1 after it completes

244
00:12:07,320 --> 00:12:11,279
the exclusion executor I minus y is

245
00:12:11,279 --> 00:12:15,600
active with dp1 we executor I plus 1 is

246
00:12:15,600 --> 00:12:18,779
in working states with pp1

247
00:12:18,779 --> 00:12:22,740
in this case x square I minus y is able

248
00:12:22,740 --> 00:12:25,380
to join in parallel with x squared I

249
00:12:25,380 --> 00:12:27,959
with Plus 1.

250
00:12:27,959 --> 00:12:31,680
to address above two problems we add an

251
00:12:31,680 --> 00:12:34,560
extra state for executor and moves the

252
00:12:34,560 --> 00:12:37,800
10 point for legitimize the used buffer

253
00:12:37,800 --> 00:12:38,940
pair

254
00:12:38,940 --> 00:12:41,459
first the checking state

255
00:12:41,459 --> 00:12:44,279
guarantee the correct access order of

256
00:12:44,279 --> 00:12:46,500
buffer pairs

257
00:12:46,500 --> 00:12:50,160
and we only legitimize the used buffer

258
00:12:50,160 --> 00:12:54,660
pair after executor enters active States

259
00:12:54,660 --> 00:12:57,959
this will enable a Serial Rock manner

260
00:12:57,959 --> 00:13:00,720
where only one bathroom pair is enabled

261
00:13:00,720 --> 00:13:03,899
for all executors

262
00:13:03,899 --> 00:13:06,839
now we present the interface of Diva

263
00:13:06,839 --> 00:13:08,880
batch to create policies

264
00:13:08,880 --> 00:13:11,760
each stage executed launching a while

265
00:13:11,760 --> 00:13:14,519
loop to execute the batches

266
00:13:14,519 --> 00:13:17,579
the stage executor calls schedule to

267
00:13:17,579 --> 00:13:20,639
schedule the batches inside the schedule

268
00:13:20,639 --> 00:13:24,240
function the stash executor updates the

269
00:13:24,240 --> 00:13:27,480
batch table and performs meta operations

270
00:13:27,480 --> 00:13:30,480
if users defined conditions are

271
00:13:30,480 --> 00:13:33,899
satisfied in this way users can Define

272
00:13:33,899 --> 00:13:38,959
conditions to adjust the ongoing Bash

273
00:13:39,600 --> 00:13:42,660
with the three meta operations we can

274
00:13:42,660 --> 00:13:45,420
create policies for the involved

275
00:13:45,420 --> 00:13:46,800
diversities

276
00:13:46,800 --> 00:13:49,760
for input diversity we spiritually

277
00:13:49,760 --> 00:13:53,459
accepted queries into small batches with

278
00:13:53,459 --> 00:13:56,820
similar SQL strength and a random in

279
00:13:56,820 --> 00:13:57,720
parallel

280
00:13:57,720 --> 00:14:01,079
for operator diversity we splits the

281
00:14:01,079 --> 00:14:04,680
accepted batch at the specific stage and

282
00:14:04,680 --> 00:14:06,600
around them in Syria

283
00:14:06,600 --> 00:14:09,839
for low diversity we stress the

284
00:14:09,839 --> 00:14:12,540
insufficient patch with newly arrived

285
00:14:12,540 --> 00:14:14,040
queries

286
00:14:14,040 --> 00:14:17,880
these policies can coexist in a single

287
00:14:17,880 --> 00:14:20,600
deal service

288
00:14:21,420 --> 00:14:24,660
now we will present the evaluation of

289
00:14:24,660 --> 00:14:27,360
Diva batch

290
00:14:27,360 --> 00:14:30,959
we use six models for evaluation first

291
00:14:30,959 --> 00:14:32,880
base but large

292
00:14:32,880 --> 00:14:36,000
with input and load diversity unit

293
00:14:36,000 --> 00:14:39,600
linked with operator diversity and low

294
00:14:39,600 --> 00:14:41,000
diversity

295
00:14:41,000 --> 00:14:44,519
vgt and the rest net only with low

296
00:14:44,519 --> 00:14:45,959
diversity

297
00:14:45,959 --> 00:14:49,260
for simulating input diversity we use

298
00:14:49,260 --> 00:14:53,160
slow data set to generate input data to

299
00:14:53,160 --> 00:14:55,500
simulate low diversity we generate a

300
00:14:55,500 --> 00:14:59,519
load with voicemail distribution

301
00:14:59,519 --> 00:15:02,420
We compare dual batch with two baselines

302
00:15:02,420 --> 00:15:06,000
zero batch And Delay batch

303
00:15:06,000 --> 00:15:09,240
zero best battery skills without time

304
00:15:09,240 --> 00:15:12,420
window delay batch pass queries with the

305
00:15:12,420 --> 00:15:14,760
hand Tune Time window for high

306
00:15:14,760 --> 00:15:16,639
throughput

307
00:15:16,639 --> 00:15:20,339
compared with zero patch and delay patch

308
00:15:20,339 --> 00:15:22,620
your batch achieves better average

309
00:15:22,620 --> 00:15:25,740
latency performance on the high medium

310
00:15:25,740 --> 00:15:28,220
and low load

311
00:15:28,220 --> 00:15:32,220
to show the robustness of device we also

312
00:15:32,220 --> 00:15:34,500
evaluate the developers with a step in

313
00:15:34,500 --> 00:15:35,579
load

314
00:15:35,579 --> 00:15:37,980
the stepping load increase load

315
00:15:37,980 --> 00:15:39,240
gradually

316
00:15:39,240 --> 00:15:42,300
all the benchmarks have lower latency

317
00:15:42,300 --> 00:15:45,899
with develop bashing or cases for

318
00:15:45,899 --> 00:15:48,959
birthdays and birth large developers

319
00:15:48,959 --> 00:15:51,899
also increase Peak throughput due to

320
00:15:51,899 --> 00:15:56,060
removing computation for padding

321
00:15:57,079 --> 00:16:00,480
to conclude your best makes following

322
00:16:00,480 --> 00:16:01,740
contributions

323
00:16:01,740 --> 00:16:04,800
to propose a multi-entry multi-exit

324
00:16:04,800 --> 00:16:07,320
matching schemes for efficient Indian

325
00:16:07,320 --> 00:16:10,500
service processing on gpus

326
00:16:10,500 --> 00:16:13,019
to provide a general scheduling

327
00:16:13,019 --> 00:16:16,440
mechanism that leverages meta operations

328
00:16:16,440 --> 00:16:19,440
and state transition diagram to create

329
00:16:19,440 --> 00:16:21,480
policies for different serving

330
00:16:21,480 --> 00:16:22,620
diversities

331
00:16:22,620 --> 00:16:25,440
will reduce 46 percentage average

332
00:16:25,440 --> 00:16:28,760
latency and Achieve up to

333
00:16:28,760 --> 00:16:32,160
2.212 times throughput Improvement for

334
00:16:32,160 --> 00:16:36,199
the involved serving diversities

335
00:16:36,620 --> 00:16:39,839
also brings some implications for future

336
00:16:39,839 --> 00:16:41,399
dealing servings

337
00:16:41,399 --> 00:16:45,240
first is Omni present diversity exist

338
00:16:45,240 --> 00:16:47,759
Indians may have a dynamic architecture

339
00:16:47,759 --> 00:16:52,380
index with and routing as more Dynamic

340
00:16:52,380 --> 00:16:55,620
attributes emerge diversity suppress

341
00:16:55,620 --> 00:16:58,620
across New Deals

342
00:16:58,620 --> 00:17:01,500
Diva batch is flexible to tackle many

343
00:17:01,500 --> 00:17:04,520
diversities among them for example

344
00:17:04,520 --> 00:17:08,339
airscape early exit models required to

345
00:17:08,339 --> 00:17:11,780
split batches into smaller ones

346
00:17:11,780 --> 00:17:16,380
second intro model scheduling a Steels

347
00:17:16,380 --> 00:17:19,980
grow larger and still more diversity the

348
00:17:19,980 --> 00:17:23,220
exclusion of this cannot be treated as a

349
00:17:23,220 --> 00:17:25,859
single function call if a model

350
00:17:25,859 --> 00:17:28,319
scheduling is a trend for their

351
00:17:28,319 --> 00:17:31,380
inferences of large models that's all

352
00:17:31,380 --> 00:17:34,640
thanks any questions

