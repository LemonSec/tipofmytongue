1
00:00:12,880 --> 00:00:14,799
so i'll start right away hi good

2
00:00:14,799 --> 00:00:16,640
afternoon everyone i'm genuine from

3
00:00:16,640 --> 00:00:17,520
kaist

4
00:00:17,520 --> 00:00:19,840
today i'm presenting our paper kova

5
00:00:19,840 --> 00:00:22,560
where we propose a new video analytics

6
00:00:22,560 --> 00:00:24,560
pipeline that addresses the bottleneck

7
00:00:24,560 --> 00:00:27,199
of prior works by exploiting the

8
00:00:27,199 --> 00:00:30,240
compressed domain analysis

9
00:00:30,240 --> 00:00:32,320
so according to a survey video data is

10
00:00:32,320 --> 00:00:34,960
taking up 82 percentage of the global ip

11
00:00:34,960 --> 00:00:37,440
traffic as of 2022

12
00:00:37,440 --> 00:00:39,520
but this number is only growing

13
00:00:39,520 --> 00:00:41,600
so many emerging applications like smart

14
00:00:41,600 --> 00:00:44,640
home cameras ar vr or uav drones

15
00:00:44,640 --> 00:00:46,719
autonomous driving cars and so on rely

16
00:00:46,719 --> 00:00:48,719
largely on the video data

17
00:00:48,719 --> 00:00:50,559
so in the future

18
00:00:50,559 --> 00:00:52,239
video data will be generated from

19
00:00:52,239 --> 00:00:54,960
diverse sources and scenarios

20
00:00:54,960 --> 00:00:57,600
and as more video data are generated

21
00:00:57,600 --> 00:00:59,600
building better ways to process and

22
00:00:59,600 --> 00:01:01,680
extract insights from the video has

23
00:01:01,680 --> 00:01:04,640
become important in that sense we aim to

24
00:01:04,640 --> 00:01:06,640
optimize the video analytics system

25
00:01:06,640 --> 00:01:08,880
which is a tailor system to

26
00:01:08,880 --> 00:01:11,680
analyze video and answer user curry so

27
00:01:11,680 --> 00:01:13,920
for example if a city planner is

28
00:01:13,920 --> 00:01:15,119
interested in

29
00:01:15,119 --> 00:01:17,360
a trend of a traffic at a specific

30
00:01:17,360 --> 00:01:19,840
intersection you can ask the system to

31
00:01:19,840 --> 00:01:22,479
count how many cars there in an hour

32
00:01:22,479 --> 00:01:25,840
and use the answer to make his decision

33
00:01:25,840 --> 00:01:27,520
also the recent advancements in the

34
00:01:27,520 --> 00:01:29,280
neural network field

35
00:01:29,280 --> 00:01:30,880
provides a useful tool for video

36
00:01:30,880 --> 00:01:33,280
analytics which is the highly accurate

37
00:01:33,280 --> 00:01:34,720
object detectors

38
00:01:34,720 --> 00:01:36,560
so here's an example of how object

39
00:01:36,560 --> 00:01:38,320
detector can help

40
00:01:38,320 --> 00:01:41,439
analyze video so here user is querying

41
00:01:41,439 --> 00:01:43,840
about the sequence of video and he wants

42
00:01:43,840 --> 00:01:46,079
to know when a car appears

43
00:01:46,079 --> 00:01:48,960
so the video another analytic system can

44
00:01:48,960 --> 00:01:50,799
first apply the object

45
00:01:50,799 --> 00:01:53,520
detector to its frames

46
00:01:53,520 --> 00:01:55,759
and for example in frame two it found

47
00:01:55,759 --> 00:01:57,439
the person walking by

48
00:01:57,439 --> 00:01:59,119
and in frame three it found the same

49
00:01:59,119 --> 00:02:01,680
person and a new car and once it's done

50
00:02:01,680 --> 00:02:03,680
running the object detector it can

51
00:02:03,680 --> 00:02:06,320
process this result to answer the query

52
00:02:06,320 --> 00:02:08,959
so as you see here this object detector

53
00:02:08,959 --> 00:02:10,800
is working as a major component in the

54
00:02:10,800 --> 00:02:13,920
system but at the same time it imposes a

55
00:02:13,920 --> 00:02:16,160
significant challenge and the challenge

56
00:02:16,160 --> 00:02:18,000
here would be that dnm based object

57
00:02:18,000 --> 00:02:21,360
detector requires heavy computations

58
00:02:21,360 --> 00:02:24,160
so yolo for example would run like 11

59
00:02:24,160 --> 00:02:28,160
hours to process two weeks long video

60
00:02:28,160 --> 00:02:30,720
so the prior approaches recently made in

61
00:02:30,720 --> 00:02:32,800
the database field introduces two new

62
00:02:32,800 --> 00:02:35,519
concepts to address this challenge the

63
00:02:35,519 --> 00:02:38,400
first is to use a simple neural network

64
00:02:38,400 --> 00:02:40,800
that is specialized for the user query

65
00:02:40,800 --> 00:02:42,480
and the second is to build a cascade

66
00:02:42,480 --> 00:02:44,879
architecture where pipeline constitutes

67
00:02:44,879 --> 00:02:47,280
of multiple classifiers that trace

68
00:02:47,280 --> 00:02:50,239
accuracy and performance i'll explain

69
00:02:50,239 --> 00:02:52,879
these two concepts in more detail

70
00:02:52,879 --> 00:02:55,519
so first the specialized neural network

71
00:02:55,519 --> 00:02:56,800
so the

72
00:02:56,800 --> 00:02:58,879
prior works first focus on the reason

73
00:02:58,879 --> 00:03:01,200
why object detectors are so slow in the

74
00:03:01,200 --> 00:03:03,120
first place and the reason for that

75
00:03:03,120 --> 00:03:05,840
would be that object detectors have very

76
00:03:05,840 --> 00:03:08,400
high uh complex model architecture

77
00:03:08,400 --> 00:03:10,000
because they are designed to solve a

78
00:03:10,000 --> 00:03:12,560
complex task which is to determine

79
00:03:12,560 --> 00:03:14,480
location and the labels of every

80
00:03:14,480 --> 00:03:16,800
appearing objects in the frame

81
00:03:16,800 --> 00:03:18,879
so in order to reduce the computation

82
00:03:18,879 --> 00:03:21,599
they wanted to simplify this task to a

83
00:03:21,599 --> 00:03:24,319
easier one and one information they used

84
00:03:24,319 --> 00:03:26,640
was that they already know what user is

85
00:03:26,640 --> 00:03:29,120
looking for so in this example the user

86
00:03:29,120 --> 00:03:32,080
is interested in finding cars so instead

87
00:03:32,080 --> 00:03:33,440
of using this

88
00:03:33,440 --> 00:03:35,440
large model that finds all kinds of

89
00:03:35,440 --> 00:03:37,760
objects even the one that users not

90
00:03:37,760 --> 00:03:38,879
interested in

91
00:03:38,879 --> 00:03:41,040
they train a specialized neural network

92
00:03:41,040 --> 00:03:42,239
that's

93
00:03:42,239 --> 00:03:44,159
specifically trained to determine if

94
00:03:44,159 --> 00:03:46,400
there's any cars in the frame or not

95
00:03:46,400 --> 00:03:48,959
so this model has only one value and if

96
00:03:48,959 --> 00:03:51,120
the value is one it means there is a car

97
00:03:51,120 --> 00:03:53,200
in the frame and if the value is zero it

98
00:03:53,200 --> 00:03:55,200
means there's no car in the frame and

99
00:03:55,200 --> 00:03:57,519
the values between one and zero can be

100
00:03:57,519 --> 00:04:00,400
interpreted as how confident the model

101
00:04:00,400 --> 00:04:01,360
is

102
00:04:01,360 --> 00:04:03,280
and the second approach that they make

103
00:04:03,280 --> 00:04:05,840
is to build a cascade architecture out

104
00:04:05,840 --> 00:04:08,560
of this specialized neural network

105
00:04:08,560 --> 00:04:10,480
i'll be using an example to explain this

106
00:04:10,480 --> 00:04:11,519
concept

107
00:04:11,519 --> 00:04:13,760
so cascade architecture first applies

108
00:04:13,760 --> 00:04:16,000
this specialized neural network to all

109
00:04:16,000 --> 00:04:18,959
of his frames so first for the first

110
00:04:18,959 --> 00:04:22,720
frame the return value was 0.05

111
00:04:22,720 --> 00:04:24,479
that means the specialized neural

112
00:04:24,479 --> 00:04:26,479
network is sure that there is no car in

113
00:04:26,479 --> 00:04:29,040
the frame so we can label this frame

114
00:04:29,040 --> 00:04:30,880
i mean the prior works can label this

115
00:04:30,880 --> 00:04:33,520
frame as not having the car and it's not

116
00:04:33,520 --> 00:04:35,440
the frame that

117
00:04:35,440 --> 00:04:37,600
the user is looking for and say for

118
00:04:37,600 --> 00:04:40,639
frame two the return value was 0.48

119
00:04:40,639 --> 00:04:42,880
which means the specialized neural

120
00:04:42,880 --> 00:04:44,880
network was not so sure about the stone

121
00:04:44,880 --> 00:04:47,120
decision so instead of labeling this

122
00:04:47,120 --> 00:04:50,080
frame according to this value they apply

123
00:04:50,080 --> 00:04:53,040
the heavy object detector for this frame

124
00:04:53,040 --> 00:04:54,560
and

125
00:04:54,560 --> 00:04:57,520
as a result it found the car in it and

126
00:04:57,520 --> 00:04:59,360
they can finally label this frame does

127
00:04:59,360 --> 00:05:00,880
have a car and this one of the frame the

128
00:05:00,880 --> 00:05:02,400
user is looking for

129
00:05:02,400 --> 00:05:04,080
and say for the

130
00:05:04,080 --> 00:05:07,759
third frame the return value was 0.97

131
00:05:07,759 --> 00:05:09,600
which means that specialized neural

132
00:05:09,600 --> 00:05:11,919
network is confident that there is a car

133
00:05:11,919 --> 00:05:14,080
in the frame so in this case we don't

134
00:05:14,080 --> 00:05:16,800
have to run i mean they don't have to

135
00:05:16,800 --> 00:05:19,120
run the object detector to that frame

136
00:05:19,120 --> 00:05:21,199
and we can still label it as the frame

137
00:05:21,199 --> 00:05:23,120
the user is looking for

138
00:05:23,120 --> 00:05:25,600
so one thing to note here is that the

139
00:05:25,600 --> 00:05:27,520
only frame that went over the object

140
00:05:27,520 --> 00:05:29,600
detection was frame two and for frame

141
00:05:29,600 --> 00:05:31,280
one and three it was able to skip the

142
00:05:31,280 --> 00:05:32,560
computation

143
00:05:32,560 --> 00:05:35,120
however we noticed two major limitations

144
00:05:35,120 --> 00:05:37,280
from the prior approaches the first is

145
00:05:37,280 --> 00:05:38,880
that there is a bottleneck from the

146
00:05:38,880 --> 00:05:40,080
decoding

147
00:05:40,080 --> 00:05:42,960
so despite the prior approach

148
00:05:42,960 --> 00:05:44,960
achieved high speed up by filtering out

149
00:05:44,960 --> 00:05:46,400
the frames going into the object

150
00:05:46,400 --> 00:05:48,160
detectors

151
00:05:48,160 --> 00:05:50,000
in the real world the video data

152
00:05:50,000 --> 00:05:51,440
actually

153
00:05:51,440 --> 00:05:53,680
are stored in a highly compressed format

154
00:05:53,680 --> 00:05:56,000
that requires decoding and as modern

155
00:05:56,000 --> 00:05:58,720
video codecs are very complex they run

156
00:05:58,720 --> 00:06:00,639
in much slower throughput than the later

157
00:06:00,639 --> 00:06:02,560
pipeline and works as a severe

158
00:06:02,560 --> 00:06:04,479
bottleneck

159
00:06:04,479 --> 00:06:06,639
and the second limitation they have is

160
00:06:06,639 --> 00:06:08,400
they lack the support for the spatial

161
00:06:08,400 --> 00:06:09,280
query

162
00:06:09,280 --> 00:06:11,440
so recall in the previous example

163
00:06:11,440 --> 00:06:13,680
running object detector on one of the

164
00:06:13,680 --> 00:06:16,319
frame was enough to answer the question

165
00:06:16,319 --> 00:06:18,880
however this time the users interested

166
00:06:18,880 --> 00:06:21,199
in where a car appear

167
00:06:21,199 --> 00:06:23,759
so for frame 2 we have the bounding box

168
00:06:23,759 --> 00:06:25,440
information so we can answer the query

169
00:06:25,440 --> 00:06:28,639
right away but for frame 3

170
00:06:28,639 --> 00:06:30,319
we don't have the information about the

171
00:06:30,319 --> 00:06:32,160
bounding box we know the car is

172
00:06:32,160 --> 00:06:33,840
somewhere in the frame but we just don't

173
00:06:33,840 --> 00:06:36,639
know where it is so the prior works

174
00:06:36,639 --> 00:06:38,560
cannot support

175
00:06:38,560 --> 00:06:41,440
the queries that require spatial context

176
00:06:41,440 --> 00:06:44,000
and to this end we devised kova

177
00:06:44,000 --> 00:06:47,120
shorthand for compressed video analysis

178
00:06:47,120 --> 00:06:49,199
and the main idea of our work is to

179
00:06:49,199 --> 00:06:51,600
extend the scope of the cascade so that

180
00:06:51,600 --> 00:06:54,080
we filter our frame before decoding not

181
00:06:54,080 --> 00:06:54,960
after

182
00:06:54,960 --> 00:06:56,960
so this in turn reduces the number of

183
00:06:56,960 --> 00:06:58,960
frames going into the decoder

184
00:06:58,960 --> 00:07:01,360
and addresses the decoding bottleneck

185
00:07:01,360 --> 00:07:03,039
and offers four point eight times

186
00:07:03,039 --> 00:07:06,160
end-to-end speed up and at the same time

187
00:07:06,160 --> 00:07:08,080
we designed this filtering scheme in a

188
00:07:08,080 --> 00:07:10,000
way that preserves the spatial context

189
00:07:10,000 --> 00:07:12,960
so we can support the spatial query

190
00:07:12,960 --> 00:07:14,800
the followings are the

191
00:07:14,800 --> 00:07:18,000
internal stages in cova and i'll explain

192
00:07:18,000 --> 00:07:20,479
each stages one by one

193
00:07:20,479 --> 00:07:23,280
first to the track detection stage

194
00:07:23,280 --> 00:07:25,599
so track detection stage

195
00:07:25,599 --> 00:07:28,240
the goal is to find the tracks of moving

196
00:07:28,240 --> 00:07:30,080
objects without having to decode any

197
00:07:30,080 --> 00:07:32,639
frames however not decoding any frame

198
00:07:32,639 --> 00:07:34,639
means that video datas are still stored

199
00:07:34,639 --> 00:07:37,120
in the compressed format then how can we

200
00:07:37,120 --> 00:07:40,080
find objects from a compressed video

201
00:07:40,080 --> 00:07:42,800
so in order for me to explain this i'll

202
00:07:42,800 --> 00:07:44,960
briefly have to go over how modern video

203
00:07:44,960 --> 00:07:46,319
codec works

204
00:07:46,319 --> 00:07:48,880
so since 1995

205
00:07:48,880 --> 00:07:51,599
when mpeg2 was invented several kinds of

206
00:07:51,599 --> 00:07:53,199
codecs has

207
00:07:53,199 --> 00:07:55,360
been invented

208
00:07:55,360 --> 00:07:57,759
but they all share the same key

209
00:07:57,759 --> 00:07:59,840
algorithm which is the block-based

210
00:07:59,840 --> 00:08:01,199
compression

211
00:08:01,199 --> 00:08:02,960
and the way the block-based compression

212
00:08:02,960 --> 00:08:05,520
works is to first divide the frames into

213
00:08:05,520 --> 00:08:07,280
a grid of blocks which they call

214
00:08:07,280 --> 00:08:08,960
macroblocks

215
00:08:08,960 --> 00:08:11,199
and when saving each of the macro blocks

216
00:08:11,199 --> 00:08:13,840
instead of saving all all the rgb pixel

217
00:08:13,840 --> 00:08:16,800
values they look for a similar looking

218
00:08:16,800 --> 00:08:19,280
block from a previous frame and saves

219
00:08:19,280 --> 00:08:21,599
this relative position as a vector and

220
00:08:21,599 --> 00:08:23,759
this compression metadata is called

221
00:08:23,759 --> 00:08:26,400
motion vector and the scheme works very

222
00:08:26,400 --> 00:08:28,720
effectively as the frames in the videos

223
00:08:28,720 --> 00:08:31,039
are captured in high frequency like 30

224
00:08:31,039 --> 00:08:33,759
frames per second

225
00:08:33,760 --> 00:08:36,159
uh that having said now have a look at

226
00:08:36,159 --> 00:08:38,799
this picture so this picture is showing

227
00:08:38,799 --> 00:08:41,279
an example of motion vector field

228
00:08:41,279 --> 00:08:43,760
and if we look closely we can see

229
00:08:43,760 --> 00:08:46,000
consistent patterns appearing where

230
00:08:46,000 --> 00:08:48,240
moving objects are but at the same time

231
00:08:48,240 --> 00:08:51,040
similar patterns can also appear due to

232
00:08:51,040 --> 00:08:53,760
the noises from lightings or the ripples

233
00:08:53,760 --> 00:08:55,360
in this case

234
00:08:55,360 --> 00:08:57,600
so the challenge here is to find the

235
00:08:57,600 --> 00:08:58,959
moving objects from this noisy

236
00:08:58,959 --> 00:09:01,040
compression metadata and the solution

237
00:09:01,040 --> 00:09:03,600
that we came up with was to use a neural

238
00:09:03,600 --> 00:09:05,519
network-based algorithm

239
00:09:05,519 --> 00:09:07,200
so we devised a neural network called

240
00:09:07,200 --> 00:09:08,640
plumnet

241
00:09:08,640 --> 00:09:10,640
as the input the plumnet takes in three

242
00:09:10,640 --> 00:09:12,399
kinds of compression metadata which of

243
00:09:12,399 --> 00:09:14,240
the ladder two we almost the details in

244
00:09:14,240 --> 00:09:15,519
the talk

245
00:09:15,519 --> 00:09:17,519
and for the model architecture we

246
00:09:17,519 --> 00:09:19,440
searched among the encoder decoder

247
00:09:19,440 --> 00:09:21,600
architecture which are

248
00:09:21,600 --> 00:09:23,760
known to be effective for denoising task

249
00:09:23,760 --> 00:09:25,600
and selected a neural network called

250
00:09:25,600 --> 00:09:27,600
temporal unit which was originally

251
00:09:27,600 --> 00:09:29,920
designed for video instance segmentation

252
00:09:29,920 --> 00:09:31,440
model

253
00:09:31,440 --> 00:09:34,240
also we add an additional embedding

254
00:09:34,240 --> 00:09:36,399
layer as the compression metadata does

255
00:09:36,399 --> 00:09:38,560
not come in the scalloped value as rgb

256
00:09:38,560 --> 00:09:40,720
pixel does so that the neural network

257
00:09:40,720 --> 00:09:42,399
can learn the different compression

258
00:09:42,399 --> 00:09:44,800
metadata and finally for the training

259
00:09:44,800 --> 00:09:47,200
labels they are generated using the

260
00:09:47,200 --> 00:09:48,640
background subtraction techniques

261
00:09:48,640 --> 00:09:50,880
running in the pixel domain

262
00:09:50,880 --> 00:09:53,120
and the following sequence is the result

263
00:09:53,120 --> 00:09:55,040
of our trained plum net and as you see

264
00:09:55,040 --> 00:09:55,839
here

265
00:09:55,839 --> 00:09:57,760
the noises from the ripples are gone and

266
00:09:57,760 --> 00:09:59,600
only the objects of our interest is

267
00:09:59,600 --> 00:10:00,880
being captured

268
00:10:00,880 --> 00:10:03,200
also note that we use the term blob to

269
00:10:03,200 --> 00:10:05,120
define the region where moving objects

270
00:10:05,120 --> 00:10:06,800
appear

271
00:10:06,800 --> 00:10:08,399
and that's where the blomnet got its

272
00:10:08,399 --> 00:10:10,800
name from

273
00:10:10,800 --> 00:10:13,200
so by running the blomnets we were able

274
00:10:13,200 --> 00:10:15,920
to find the blobs however blobs are just

275
00:10:15,920 --> 00:10:16,880
still

276
00:10:16,880 --> 00:10:19,120
binary masks consisting of serials and

277
00:10:19,120 --> 00:10:20,000
ones

278
00:10:20,000 --> 00:10:21,839
so in order to

279
00:10:21,839 --> 00:10:24,560
associate blobs into a track we apply

280
00:10:24,560 --> 00:10:26,480
source algorithm which is one of the

281
00:10:26,480 --> 00:10:28,640
state-of-the-art tracking algorithm

282
00:10:28,640 --> 00:10:30,959
known to deliver robust tracking results

283
00:10:30,959 --> 00:10:33,279
at low computational demand

284
00:10:33,279 --> 00:10:36,399
and finally at this point we are able to

285
00:10:36,399 --> 00:10:39,040
find the tracks of moving objects in the

286
00:10:39,040 --> 00:10:41,200
video without having to come

287
00:10:41,200 --> 00:10:43,519
decode any frames

288
00:10:43,519 --> 00:10:44,880
and

289
00:10:44,880 --> 00:10:49,600
our next stage is frame selection

290
00:10:49,760 --> 00:10:52,160
the goal of frame selection is to select

291
00:10:52,160 --> 00:10:54,560
the minimal frames to decode

292
00:10:54,560 --> 00:10:57,440
but why would we need to decode any of

293
00:10:57,440 --> 00:10:58,640
the frames

294
00:10:58,640 --> 00:11:00,959
so from the previous track detection

295
00:11:00,959 --> 00:11:02,480
stage we were able to find the

296
00:11:02,480 --> 00:11:05,200
trajectories of each object however we

297
00:11:05,200 --> 00:11:07,680
haven't decoded any frames yet so we

298
00:11:07,680 --> 00:11:09,360
never got to see what they really look

299
00:11:09,360 --> 00:11:11,440
like so we don't know if these objects

300
00:11:11,440 --> 00:11:14,640
are a car or a person or a bus and to

301
00:11:14,640 --> 00:11:16,399
figure that out we need to decode some

302
00:11:16,399 --> 00:11:18,480
of the frame to get to see it

303
00:11:18,480 --> 00:11:20,560
however the question here is that can we

304
00:11:20,560 --> 00:11:22,880
just pick any of the frames to decode

305
00:11:22,880 --> 00:11:25,600
and the answer to that question is no

306
00:11:25,600 --> 00:11:28,160
and the reason is that not every frame

307
00:11:28,160 --> 00:11:30,800
has the same decoding cost so recall in

308
00:11:30,800 --> 00:11:33,360
the previous example how macro blocks

309
00:11:33,360 --> 00:11:35,279
are compressed by referencing its

310
00:11:35,279 --> 00:11:37,920
previous frame so in order to decode one

311
00:11:37,920 --> 00:11:39,600
frame we would need to decode this

312
00:11:39,600 --> 00:11:41,200
previous frame that is dependent on

313
00:11:41,200 --> 00:11:44,399
first so for the first frame of the

314
00:11:44,399 --> 00:11:46,880
video it didn't have any other frames to

315
00:11:46,880 --> 00:11:48,720
reference so this can be decoded by

316
00:11:48,720 --> 00:11:51,360
itself but think about frame two who's

317
00:11:51,360 --> 00:11:53,760
dependent on frame one we first need to

318
00:11:53,760 --> 00:11:56,320
equal frame one then start decoding

319
00:11:56,320 --> 00:11:58,240
frame two in order to see the objects

320
00:11:58,240 --> 00:11:59,920
appearing in the frame two

321
00:11:59,920 --> 00:12:02,560
so that would add up the cos2 decoding

322
00:12:02,560 --> 00:12:05,440
two frames and for frame three

323
00:12:05,440 --> 00:12:08,240
which is dependent on frame two and the

324
00:12:08,240 --> 00:12:10,160
frame two is again dependent on frame

325
00:12:10,160 --> 00:12:13,040
one so that adds up the cost to three

326
00:12:13,040 --> 00:12:15,519
frames and this dependency linearly

327
00:12:15,519 --> 00:12:16,800
grows

328
00:12:16,800 --> 00:12:19,279
uh and goes on

329
00:12:19,279 --> 00:12:21,920
and mining this dependency we develop a

330
00:12:21,920 --> 00:12:23,680
dependency aware frame selection

331
00:12:23,680 --> 00:12:25,680
algorithm using the track detection

332
00:12:25,680 --> 00:12:26,880
result

333
00:12:26,880 --> 00:12:29,200
and the basic way that this algorithm

334
00:12:29,200 --> 00:12:31,279
works is by tracking when an object

335
00:12:31,279 --> 00:12:33,839
enters and leaves the scene so in this

336
00:12:33,839 --> 00:12:36,320
example object a enters the scene at

337
00:12:36,320 --> 00:12:38,880
frame 40 then object b enters the scene

338
00:12:38,880 --> 00:12:41,920
at 120 object c enters the scene one at

339
00:12:41,920 --> 00:12:45,600
140 and later at 170 object a leaves the

340
00:12:45,600 --> 00:12:49,440
scene and finally at frame 230 object b

341
00:12:49,440 --> 00:12:50,800
leaves the scene

342
00:12:50,800 --> 00:12:53,040
and at this point our algorithm is able

343
00:12:53,040 --> 00:12:56,240
to find the range of frames where we can

344
00:12:56,240 --> 00:12:58,480
see all three objects at once

345
00:12:58,480 --> 00:13:00,720
and find the optimal frame to decode

346
00:13:00,720 --> 00:13:04,480
that minimizes the decoding cost

347
00:13:04,480 --> 00:13:07,120
note that in reality this uh

348
00:13:07,120 --> 00:13:09,440
dependency doesn't scale from a specific

349
00:13:09,440 --> 00:13:11,760
a certain point and they break and this

350
00:13:11,760 --> 00:13:14,160
pattern is repeated so please refer to

351
00:13:14,160 --> 00:13:16,160
our paper for more detailed version of

352
00:13:16,160 --> 00:13:18,839
our algorithm that covers all the edge

353
00:13:18,839 --> 00:13:21,760
cases so now that we selected the frame

354
00:13:21,760 --> 00:13:24,320
to decode we can decode it and apply

355
00:13:24,320 --> 00:13:26,639
object texture on it and get the result

356
00:13:26,639 --> 00:13:29,040
as such

357
00:13:29,040 --> 00:13:30,880
and to our final

358
00:13:30,880 --> 00:13:33,600
stage label propagation

359
00:13:33,600 --> 00:13:35,920
the goal of the labor propagation stage

360
00:13:35,920 --> 00:13:37,760
is to combine the results from the

361
00:13:37,760 --> 00:13:40,560
previous stage to label the track and

362
00:13:40,560 --> 00:13:43,279
the way we do it is by first

363
00:13:43,279 --> 00:13:45,760
retrieving the blob location at the same

364
00:13:45,760 --> 00:13:48,399
time stamp of object detected frame

365
00:13:48,399 --> 00:13:50,880
and by calculating how much they overlap

366
00:13:50,880 --> 00:13:53,200
we can assign the labels to the track

367
00:13:53,200 --> 00:13:55,680
and the assigned labels are propagated

368
00:13:55,680 --> 00:13:57,920
throughout the track even to the ones

369
00:13:57,920 --> 00:14:00,240
that are not decoded

370
00:14:00,240 --> 00:14:02,800
so that finishes the description of cova

371
00:14:02,800 --> 00:14:04,880
to summarize the first two stages of

372
00:14:04,880 --> 00:14:07,040
kova works in the compressed domain to

373
00:14:07,040 --> 00:14:09,519
perform filtering and extract tracking

374
00:14:09,519 --> 00:14:12,079
information and the label information is

375
00:14:12,079 --> 00:14:14,240
extracted from the pixel domain and our

376
00:14:14,240 --> 00:14:16,959
last stage label propagation combines

377
00:14:16,959 --> 00:14:18,560
those two information

378
00:14:18,560 --> 00:14:21,680
to answer the user query

379
00:14:21,680 --> 00:14:24,959
now i'll talk about our evaluation setup

380
00:14:24,959 --> 00:14:27,360
we used five videos in total four of

381
00:14:27,360 --> 00:14:29,199
which were from the prior works and one

382
00:14:29,199 --> 00:14:31,199
newly collected video from youtube live

383
00:14:31,199 --> 00:14:34,399
stream they're an average 28 hours long

384
00:14:34,399 --> 00:14:36,240
we evaluate covalent four different

385
00:14:36,240 --> 00:14:37,440
queries

386
00:14:37,440 --> 00:14:39,199
which of the first two are from the

387
00:14:39,199 --> 00:14:41,360
prior works the binary predicate is a

388
00:14:41,360 --> 00:14:43,199
query to find the timestamp where a

389
00:14:43,199 --> 00:14:45,279
certain object appears and the global

390
00:14:45,279 --> 00:14:46,720
query is a

391
00:14:46,720 --> 00:14:48,160
query to count the number of certain

392
00:14:48,160 --> 00:14:49,920
objects appeared in the throughout the

393
00:14:49,920 --> 00:14:52,880
video the latter two queries are

394
00:14:52,880 --> 00:14:54,720
spatial variants of the first two that

395
00:14:54,720 --> 00:14:56,880
kova is able to support

396
00:14:56,880 --> 00:14:59,040
they evaluation was conducted on the

397
00:14:59,040 --> 00:15:00,639
server with

398
00:15:00,639 --> 00:15:03,839
two intel xeon cpus and one nvidia rtx

399
00:15:03,839 --> 00:15:06,480
3090 gpu

400
00:15:06,480 --> 00:15:08,000
now let's first look at the graph

401
00:15:08,000 --> 00:15:10,079
showing the end-to-end system throughput

402
00:15:10,079 --> 00:15:12,800
kova achieves 4.8 times higher

403
00:15:12,800 --> 00:15:14,639
throughput than average compared to the

404
00:15:14,639 --> 00:15:16,959
prior works and the speed up ranges from

405
00:15:16,959 --> 00:15:20,480
3.7 times to 7.1 times depending on the

406
00:15:20,480 --> 00:15:23,279
data set this behavior is due to the

407
00:15:23,279 --> 00:15:25,760
amount of filtering performed

408
00:15:25,760 --> 00:15:27,920
according to the content of the video

409
00:15:27,920 --> 00:15:30,399
so for the videos where objects appear

410
00:15:30,399 --> 00:15:32,480
more frequently we'll need to

411
00:15:32,480 --> 00:15:34,399
decode and apply object detection more

412
00:15:34,399 --> 00:15:37,360
often to identify the labels

413
00:15:37,360 --> 00:15:39,279
the following table shows the actual

414
00:15:39,279 --> 00:15:41,839
rate of filtration achieved for decoding

415
00:15:41,839 --> 00:15:44,000
and the object detection inference

416
00:15:44,000 --> 00:15:46,639
so in average kova reduces decoding over

417
00:15:46,639 --> 00:15:48,720
80 percentage and inference workload

418
00:15:48,720 --> 00:15:50,800
over 99 percentage

419
00:15:50,800 --> 00:15:52,800
such high filtration rate is the key

420
00:15:52,800 --> 00:15:54,720
behind the throughput improvement of

421
00:15:54,720 --> 00:15:55,680
coba

422
00:15:55,680 --> 00:15:57,839
also note that the reason why the

423
00:15:57,839 --> 00:15:59,920
filtration rate for decoding is much

424
00:15:59,920 --> 00:16:02,000
lower than the inference rate is because

425
00:16:02,000 --> 00:16:03,759
the decoding includes

426
00:16:03,759 --> 00:16:06,399
the frames decoded due to the dependency

427
00:16:06,399 --> 00:16:09,440
but not for identifying the object

428
00:16:09,440 --> 00:16:11,839
labels

429
00:16:11,839 --> 00:16:14,000
next we analyze the bottleneck of our

430
00:16:14,000 --> 00:16:16,240
new cova pipeline the bottleneck is

431
00:16:16,240 --> 00:16:19,040
represented in a bar with the shade and

432
00:16:19,040 --> 00:16:20,240
the letter b

433
00:16:20,240 --> 00:16:21,519
note that in

434
00:16:21,519 --> 00:16:23,519
this experiment the performance of each

435
00:16:23,519 --> 00:16:25,759
component cannot exceed its previous

436
00:16:25,759 --> 00:16:28,560
component and that's why these bars with

437
00:16:28,560 --> 00:16:30,639
the same heights keeps appearing

438
00:16:30,639 --> 00:16:32,560
according to the result the bottleneck

439
00:16:32,560 --> 00:16:35,199
of kova differs across dataset affected

440
00:16:35,199 --> 00:16:36,959
by the filtration rate that we just saw

441
00:16:36,959 --> 00:16:39,199
in the previous slide and one more thing

442
00:16:39,199 --> 00:16:41,040
to note is that the compressed domain

443
00:16:41,040 --> 00:16:43,600
filtering introduced by kova

444
00:16:43,600 --> 00:16:45,279
never becomes the bottleneck in any

445
00:16:45,279 --> 00:16:47,440
cases which proves that

446
00:16:47,440 --> 00:16:49,440
they were designed in a way to meet the

447
00:16:49,440 --> 00:16:51,360
throughput requirement

448
00:16:51,360 --> 00:16:53,600
now i'll report the implication on

449
00:16:53,600 --> 00:16:56,560
accuracy from cova so we in general we

450
00:16:56,560 --> 00:16:57,600
observe

451
00:16:57,600 --> 00:16:58,639
modest

452
00:16:58,639 --> 00:17:00,560
uh level of accuracy degradation

453
00:17:00,560 --> 00:17:02,880
comparable to the prior works and for

454
00:17:02,880 --> 00:17:04,880
example the degradation on binary

455
00:17:04,880 --> 00:17:06,799
predicate query falls within the range

456
00:17:06,799 --> 00:17:09,679
of 10 to 15 percentage with the geome of

457
00:17:09,679 --> 00:17:11,119
12 percentage

458
00:17:11,119 --> 00:17:13,760
so considering the application of cova

459
00:17:13,760 --> 00:17:15,919
where we aim to process a large corpus

460
00:17:15,919 --> 00:17:18,160
of video at very high speed to extract

461
00:17:18,160 --> 00:17:20,240
the high level insights we believe this

462
00:17:20,240 --> 00:17:23,119
amount of approximation is tolerable

463
00:17:23,119 --> 00:17:25,679
so in conclusion we propose a novel

464
00:17:25,679 --> 00:17:28,559
video analytics pipeline that introduces

465
00:17:28,559 --> 00:17:30,880
compressed domain analysis and achieves

466
00:17:30,880 --> 00:17:33,440
4.8 times end-to-end speed up and can

467
00:17:33,440 --> 00:17:35,919
still keep the support for spatial

468
00:17:35,919 --> 00:17:36,880
queries

469
00:17:36,880 --> 00:17:38,880
our implementation is open sourced and

470
00:17:38,880 --> 00:17:40,799
has gone through the artifacts

471
00:17:40,799 --> 00:17:43,360
evaluation process so if you're

472
00:17:43,360 --> 00:17:44,960
interested please visit our github

473
00:17:44,960 --> 00:17:46,640
repository

474
00:17:46,640 --> 00:17:49,840
and thank you for listening and

475
00:17:49,840 --> 00:17:53,959
i'll be happy to answer any questions

476
00:18:01,919 --> 00:18:04,000
you

