1
00:00:13,360 --> 00:00:15,280
so in this talk i'm going to introduce

2
00:00:15,280 --> 00:00:16,880
our recent work on

3
00:00:16,880 --> 00:00:18,960
coastal wealth performance organization

4
00:00:18,960 --> 00:00:21,359
for mixed precision training

5
00:00:21,359 --> 00:00:23,119
this work is a collaborative work

6
00:00:23,119 --> 00:00:25,359
between hunan university

7
00:00:25,359 --> 00:00:29,320
uc merced and chilean

8
00:00:30,560 --> 00:00:32,479
so you you probably heard about the uh

9
00:00:32,479 --> 00:00:35,120
using lower precision

10
00:00:35,120 --> 00:00:36,239
training

11
00:00:36,239 --> 00:00:38,079
in somewhere right and

12
00:00:38,079 --> 00:00:40,160
obviously there's a lot of benefit by

13
00:00:40,160 --> 00:00:41,200
doing so

14
00:00:41,200 --> 00:00:43,280
uh instead of using full precision

15
00:00:43,280 --> 00:00:48,320
training like fp32 you use ft16

16
00:00:48,320 --> 00:00:51,039
in some operators and some tensors

17
00:00:51,039 --> 00:00:52,800
there's a lot of benefit of doing so for

18
00:00:52,800 --> 00:00:54,719
example you can lower arithmetic

19
00:00:54,719 --> 00:00:57,039
complexity that basically means you can

20
00:00:57,039 --> 00:00:59,359
potentially reduce training time

21
00:00:59,359 --> 00:01:01,680
you can also uh

22
00:01:01,680 --> 00:01:04,400
save gpu memory con

23
00:01:04,400 --> 00:01:05,680
consumption

24
00:01:05,680 --> 00:01:07,600
that allows you to train larger models

25
00:01:07,600 --> 00:01:10,798
using larger badge size

26
00:01:10,880 --> 00:01:13,040
and even say memory binaries so that's

27
00:01:13,040 --> 00:01:15,280
good

28
00:01:15,439 --> 00:01:17,840
but using lower precision training you

29
00:01:17,840 --> 00:01:21,119
you have to be careful uh because using

30
00:01:21,119 --> 00:01:22,880
lower precision training potentially you

31
00:01:22,880 --> 00:01:25,520
could cause numerical overflow or

32
00:01:25,520 --> 00:01:27,840
underflow and if you don't use it

33
00:01:27,840 --> 00:01:30,640
carefully you may lose training accuracy

34
00:01:30,640 --> 00:01:32,320
and convergence

35
00:01:32,320 --> 00:01:34,960
so in practice actually people are using

36
00:01:34,960 --> 00:01:39,759
a mixture of rp16 and rt fp32 so that's

37
00:01:39,759 --> 00:01:41,280
the reality that's what the current

38
00:01:41,280 --> 00:01:43,280
machine learning training framework like

39
00:01:43,280 --> 00:01:46,240
tensorflow and python are doing

40
00:01:46,240 --> 00:01:48,960
basically using mixed precision training

41
00:01:48,960 --> 00:01:50,000
you are

42
00:01:50,000 --> 00:01:52,720
using um full precision training full

43
00:01:52,720 --> 00:01:55,520
precision to handle some numerically

44
00:01:55,520 --> 00:01:58,880
dangerous computations

45
00:02:01,200 --> 00:02:03,600
and now let's take a moment to look at

46
00:02:03,600 --> 00:02:04,960
how the

47
00:02:04,960 --> 00:02:07,360
machine learning frameworks

48
00:02:07,360 --> 00:02:11,038
is doing this mix the precision training

49
00:02:11,038 --> 00:02:13,120
um your machining framework like

50
00:02:13,120 --> 00:02:15,040
tensorflow or python actually is a

51
00:02:15,040 --> 00:02:16,800
categorized

52
00:02:16,800 --> 00:02:19,760
uh operators based on their numerical

53
00:02:19,760 --> 00:02:21,360
safety level

54
00:02:21,360 --> 00:02:24,000
and in this slide i'm going to present

55
00:02:24,000 --> 00:02:26,400
how tensorflow is

56
00:02:26,400 --> 00:02:28,800
doing this categorization actually the

57
00:02:28,800 --> 00:02:32,160
python should be doing a similar thing

58
00:02:32,160 --> 00:02:34,080
in tensorflow we have four levels we

59
00:02:34,080 --> 00:02:36,560
have four numerical safety levels

60
00:02:36,560 --> 00:02:40,319
the first level is called allow list

61
00:02:40,319 --> 00:02:42,480
and in this list we have some operators

62
00:02:42,480 --> 00:02:47,360
and if you run those operators in lp16

63
00:02:47,360 --> 00:02:49,440
you don't lose accuracy you don't a

64
00:02:49,440 --> 00:02:50,720
convergence

65
00:02:50,720 --> 00:02:52,239
and those operators tend to be

66
00:02:52,239 --> 00:02:53,840
performance critical

67
00:02:53,840 --> 00:02:55,599
and as a result

68
00:02:55,599 --> 00:02:59,599
tensorflow always use i've seen fp16

69
00:02:59,599 --> 00:03:02,000
and there are a couple of examples such

70
00:03:02,000 --> 00:03:04,800
as the multiple matrix multiplication

71
00:03:04,800 --> 00:03:07,920
and the convolution to d

72
00:03:08,000 --> 00:03:10,319
the second category category is called

73
00:03:10,319 --> 00:03:12,080
the deny list so what does this mean

74
00:03:12,080 --> 00:03:14,159
this reason means this is numerical

75
00:03:14,159 --> 00:03:15,280
dangerous

76
00:03:15,280 --> 00:03:18,239
and if you use ip16 to run those

77
00:03:18,239 --> 00:03:20,480
operators it's highly possible that you

78
00:03:20,480 --> 00:03:22,239
will lose accuracy

79
00:03:22,239 --> 00:03:24,480
um and even worse

80
00:03:24,480 --> 00:03:26,480
the effect of those operators can

81
00:03:26,480 --> 00:03:29,440
propagate to the downstream operators

82
00:03:29,440 --> 00:03:30,159
your

83
00:03:30,159 --> 00:03:32,640
the accuracy the loss can be observed

84
00:03:32,640 --> 00:03:35,360
you know downstream algorithms

85
00:03:35,360 --> 00:03:37,360
um as a result

86
00:03:37,360 --> 00:03:39,280
the tensorflow always use

87
00:03:39,280 --> 00:03:40,959
fp32

88
00:03:40,959 --> 00:03:42,959
for those operators

89
00:03:42,959 --> 00:03:45,840
and we have a couple examples such as

90
00:03:45,840 --> 00:03:49,920
exponential and software max

91
00:03:51,200 --> 00:03:52,959
the third category is called the

92
00:03:52,959 --> 00:03:54,480
infrared list so what does this mean

93
00:03:54,480 --> 00:03:56,480
this basically means

94
00:03:56,480 --> 00:03:57,840
in this list

95
00:03:57,840 --> 00:04:00,560
we have some operators it is numerical

96
00:04:00,560 --> 00:04:02,640
safe uniquely safe that basically means

97
00:04:02,640 --> 00:04:05,840
you don't lose accuracy if you use fp16

98
00:04:05,840 --> 00:04:07,519
but you have still you have to be

99
00:04:07,519 --> 00:04:10,000
careful you need to check if there's any

100
00:04:10,000 --> 00:04:11,200
upstream

101
00:04:11,200 --> 00:04:13,760
operators coming from denialist

102
00:04:13,760 --> 00:04:16,399
if that's the case then those accuracy

103
00:04:16,399 --> 00:04:19,040
locks can propagate to those operators

104
00:04:19,040 --> 00:04:21,759
and then you have accuracy loss

105
00:04:21,759 --> 00:04:23,600
for example this

106
00:04:23,600 --> 00:04:24,960
category

107
00:04:24,960 --> 00:04:29,360
includes the balance at

108
00:04:29,360 --> 00:04:31,680
and then last but not least we have a

109
00:04:31,680 --> 00:04:33,759
clear list okay clearly is the basic

110
00:04:33,759 --> 00:04:35,919
means

111
00:04:35,919 --> 00:04:37,680
those operators do not have a

112
00:04:37,680 --> 00:04:40,240
numerically significant impact on your

113
00:04:40,240 --> 00:04:43,280
uh training accuracy and you can either

114
00:04:43,280 --> 00:04:45,840
you are free to choose ft syncing or ip

115
00:04:45,840 --> 00:04:48,000
32 it doesn't matter

116
00:04:48,000 --> 00:04:50,080
examples include the max and mean

117
00:04:50,080 --> 00:04:52,000
operators

118
00:04:52,000 --> 00:04:54,840
okay so we have these four

119
00:04:54,840 --> 00:04:57,440
categories and now

120
00:04:57,440 --> 00:05:00,639
uh if you give me your training models

121
00:05:00,639 --> 00:05:03,440
and we can get the data flow

122
00:05:03,440 --> 00:05:05,840
uh we can get the computation graph and

123
00:05:05,840 --> 00:05:08,840
then i'm going to use the four

124
00:05:08,840 --> 00:05:11,440
categories to decide

125
00:05:11,440 --> 00:05:14,000
for each operator what kind of precision

126
00:05:14,000 --> 00:05:18,400
you are going to use either fp32 or ip16

127
00:05:18,400 --> 00:05:20,479
and then you are going to rewrite

128
00:05:20,479 --> 00:05:21,919
this combination graph or data flow

129
00:05:21,919 --> 00:05:23,280
graph

130
00:05:23,280 --> 00:05:27,360
and if between any two operators

131
00:05:27,360 --> 00:05:28,960
we have different precision then you are

132
00:05:28,960 --> 00:05:31,120
going to insert a cast

133
00:05:31,120 --> 00:05:32,639
operation node

134
00:05:32,639 --> 00:05:34,400
to trigger this conversion between

135
00:05:34,400 --> 00:05:37,280
different positions

136
00:05:37,280 --> 00:05:39,680
now if we look at this whole workflow we

137
00:05:39,680 --> 00:05:42,320
may immediately find a problem

138
00:05:42,320 --> 00:05:44,160
this workflow

139
00:05:44,160 --> 00:05:46,400
they pay attention to the um

140
00:05:46,400 --> 00:05:48,960
accuracy convergence they pay attention

141
00:05:48,960 --> 00:05:52,080
to the performance but they never pay

142
00:05:52,080 --> 00:05:53,840
enough attention to the casting cost

143
00:05:53,840 --> 00:05:55,600
what about the custom cost it seems that

144
00:05:55,600 --> 00:05:56,720
it's just a

145
00:05:56,720 --> 00:05:59,120
increased assumption that the custom

146
00:05:59,120 --> 00:06:01,600
cost is always smaller than performance

147
00:06:01,600 --> 00:06:04,080
benefit

148
00:06:04,319 --> 00:06:07,039
then we ask ourselves is it is it is

149
00:06:07,039 --> 00:06:09,840
that true or not

150
00:06:10,560 --> 00:06:13,280
to answer this question uh we

151
00:06:13,280 --> 00:06:15,520
did the extensive test uh preferring and

152
00:06:15,520 --> 00:06:17,919
we're trying to categorize those

153
00:06:17,919 --> 00:06:20,400
operators in the machine learning

154
00:06:20,400 --> 00:06:22,800
framework and we're trying to study the

155
00:06:22,800 --> 00:06:24,960
impact of tensor precision

156
00:06:24,960 --> 00:06:27,759
uh tensor cars or tc

157
00:06:27,759 --> 00:06:30,319
casting cost and input data size on

158
00:06:30,319 --> 00:06:32,840
operating operation

159
00:06:32,840 --> 00:06:35,120
performance and

160
00:06:35,120 --> 00:06:37,680
in our paper we have a long list of

161
00:06:37,680 --> 00:06:39,520
those providing results

162
00:06:39,520 --> 00:06:41,199
please feel free to go into paper and

163
00:06:41,199 --> 00:06:42,960
look at them but in this talk i'm going

164
00:06:42,960 --> 00:06:45,120
to

165
00:06:45,120 --> 00:06:47,360
focus on a couple of operators and

166
00:06:47,360 --> 00:06:50,160
highlight the major insight we get

167
00:06:50,160 --> 00:06:51,280
in order to

168
00:06:51,280 --> 00:06:55,359
enable efficient mix precision training

169
00:06:58,160 --> 00:06:59,520
now let's look at the result a little

170
00:06:59,520 --> 00:07:01,440
bit in this slides

171
00:07:01,440 --> 00:07:03,440
uh we are going to focus on an operation

172
00:07:03,440 --> 00:07:05,840
called the convolution 2d

173
00:07:05,840 --> 00:07:07,599
back probe input

174
00:07:07,599 --> 00:07:09,120
and in this table

175
00:07:09,120 --> 00:07:11,520
you see the input data size you see the

176
00:07:11,520 --> 00:07:14,319
low precision action time

177
00:07:14,319 --> 00:07:16,720
low precision plus custom time

178
00:07:16,720 --> 00:07:19,120
full precision time and whether we are

179
00:07:19,120 --> 00:07:21,680
using tc or not so this is this is a

180
00:07:21,680 --> 00:07:24,080
very simple table

181
00:07:24,080 --> 00:07:25,360
and look at this table we can

182
00:07:25,360 --> 00:07:27,120
immediately draw two observations or two

183
00:07:27,120 --> 00:07:29,360
conclusions

184
00:07:29,360 --> 00:07:31,199
the first conclusion is we see that

185
00:07:31,199 --> 00:07:33,360
without using tc

186
00:07:33,360 --> 00:07:35,440
using low precision actually lead to

187
00:07:35,440 --> 00:07:37,280
slightly better performance

188
00:07:37,280 --> 00:07:39,520
than using full precision and using tc

189
00:07:39,520 --> 00:07:42,000
you can actually significantly magnify

190
00:07:42,000 --> 00:07:44,479
the profound benefit

191
00:07:44,479 --> 00:07:46,880
this this conclusion is drawn from the

192
00:07:46,880 --> 00:07:49,280
first two cases let's look at these two

193
00:07:49,280 --> 00:07:51,360
cases

194
00:07:51,360 --> 00:07:54,479
in the second row we see that we are not

195
00:07:54,479 --> 00:07:55,919
able to use

196
00:07:55,919 --> 00:07:58,000
tensor call because the input size is

197
00:07:58,000 --> 00:07:59,759
not aligned with the

198
00:07:59,759 --> 00:08:02,479
input requirement of the tensor core

199
00:08:02,479 --> 00:08:06,080
and we we use a fully uh low precision

200
00:08:06,080 --> 00:08:08,160
we get some performance benefits very

201
00:08:08,160 --> 00:08:09,759
little benefits maybe less than one

202
00:08:09,759 --> 00:08:11,680
percent of five percent very little

203
00:08:11,680 --> 00:08:13,280
benefit

204
00:08:13,280 --> 00:08:15,520
but if i change the input data size that

205
00:08:15,520 --> 00:08:16,479
we are

206
00:08:16,479 --> 00:08:19,599
able to use in pc we are able to meet

207
00:08:19,599 --> 00:08:22,000
the tender shift requirement tc and then

208
00:08:22,000 --> 00:08:23,759
we immediately find the big difference

209
00:08:23,759 --> 00:08:27,440
between fp16 ip32 and there are about 50

210
00:08:27,440 --> 00:08:29,120
percentage of performing improvements so

211
00:08:29,120 --> 00:08:32,080
that's very good

212
00:08:32,479 --> 00:08:35,120
and based on this result we also be able

213
00:08:35,120 --> 00:08:37,039
to make another

214
00:08:37,039 --> 00:08:39,039
observation the performance again of

215
00:08:39,039 --> 00:08:41,360
using low pricing actually

216
00:08:41,360 --> 00:08:44,959
varies a lot across input data size okay

217
00:08:44,959 --> 00:08:46,880
in this example it's not only about

218
00:08:46,880 --> 00:08:49,519
whether using tc or not give you such

219
00:08:49,519 --> 00:08:52,000
large performance difference it's also

220
00:08:52,000 --> 00:08:54,240
about whether you are able to

221
00:08:54,240 --> 00:08:56,560
effectively or efficiently use the

222
00:08:56,560 --> 00:08:57,760
casting

223
00:08:57,760 --> 00:09:00,320
the the precision casting because look

224
00:09:00,320 --> 00:09:01,279
at

225
00:09:01,279 --> 00:09:04,320
how the casting is implemented

226
00:09:04,320 --> 00:09:06,480
it's about how often you use bead

227
00:09:06,480 --> 00:09:09,279
casting how often use it with truncation

228
00:09:09,279 --> 00:09:12,320
how large is your output

229
00:09:12,320 --> 00:09:14,640
and all of this are related to the input

230
00:09:14,640 --> 00:09:17,360
data size okay so

231
00:09:17,360 --> 00:09:20,880
the performance gain at last will

232
00:09:20,880 --> 00:09:24,800
vary a lot across infiltrated size

233
00:09:25,920 --> 00:09:27,839
and in this slides in this next slide

234
00:09:27,839 --> 00:09:30,080
i'm going to give you another operator

235
00:09:30,080 --> 00:09:32,399
another input operator still is

236
00:09:32,399 --> 00:09:35,040
related to convolution 2d

237
00:09:35,040 --> 00:09:37,040
in this case we use two inputs data

238
00:09:37,040 --> 00:09:38,480
slides

239
00:09:38,480 --> 00:09:39,600
and uh

240
00:09:39,600 --> 00:09:41,040
we see that

241
00:09:41,040 --> 00:09:42,959
if you just if you just focus on low

242
00:09:42,959 --> 00:09:44,560
precision itself

243
00:09:44,560 --> 00:09:46,959
we have performance improvement you see

244
00:09:46,959 --> 00:09:48,959
the difference between low precision and

245
00:09:48,959 --> 00:09:50,959
full precision and we have performed

246
00:09:50,959 --> 00:09:52,560
improvement right but if you consider

247
00:09:52,560 --> 00:09:54,480
the casting cost all the performance

248
00:09:54,480 --> 00:09:56,240
improvement out of the benefit is going

249
00:09:56,240 --> 00:09:58,240
away

250
00:09:58,240 --> 00:10:00,240
and this conclusion is generally true no

251
00:10:00,240 --> 00:10:02,880
matter whether you are using tc or not

252
00:10:02,880 --> 00:10:04,640
or call or not

253
00:10:04,640 --> 00:10:06,640
so basically the take-away message is

254
00:10:06,640 --> 00:10:08,880
the casting operation you introduce a

255
00:10:08,880 --> 00:10:11,040
non-natural level overhead so

256
00:10:11,040 --> 00:10:12,720
considering causing cost sometimes you

257
00:10:12,720 --> 00:10:15,360
just cannot get personal benefit and the

258
00:10:15,360 --> 00:10:17,360
traditional assumption that the casting

259
00:10:17,360 --> 00:10:18,640
cost

260
00:10:18,640 --> 00:10:20,640
is always justifiable and we should

261
00:10:20,640 --> 00:10:23,680
always use low precision if possible is

262
00:10:23,680 --> 00:10:26,239
not true

263
00:10:27,680 --> 00:10:30,240
based on about observation we introduce

264
00:10:30,240 --> 00:10:33,920
our solution our solution is a tool is a

265
00:10:33,920 --> 00:10:35,360
called a campbell

266
00:10:35,360 --> 00:10:37,680
campbell is a customer where performance

267
00:10:37,680 --> 00:10:39,519
organization tool for mixed precision

268
00:10:39,519 --> 00:10:41,920
training

269
00:10:42,560 --> 00:10:44,399
we also can preserve the training

270
00:10:44,399 --> 00:10:45,680
accuracy

271
00:10:45,680 --> 00:10:47,519
and these two include two components the

272
00:10:47,519 --> 00:10:51,200
first component is offline components

273
00:10:51,200 --> 00:10:52,800
the most important thing of the offline

274
00:10:52,800 --> 00:10:54,480
components

275
00:10:54,480 --> 00:10:55,920
have to come from this performance

276
00:10:55,920 --> 00:10:57,040
modeling

277
00:10:57,040 --> 00:10:59,279
so using this professional modeling

278
00:10:59,279 --> 00:11:01,360
we are able to predict the performance

279
00:11:01,360 --> 00:11:03,200
of a low precision and what the

280
00:11:03,200 --> 00:11:04,560
performance

281
00:11:04,560 --> 00:11:06,720
will be and what the casting cost will

282
00:11:06,720 --> 00:11:08,160
be

283
00:11:08,160 --> 00:11:09,680
and this performance modeling is based

284
00:11:09,680 --> 00:11:12,480
on a set of carefully selected

285
00:11:12,480 --> 00:11:15,120
hardware control events

286
00:11:15,120 --> 00:11:16,079
and then

287
00:11:16,079 --> 00:11:17,680
the other part of components is online

288
00:11:17,680 --> 00:11:19,760
component this is a long time system

289
00:11:19,760 --> 00:11:22,079
we changed the tensorflow runtime system

290
00:11:22,079 --> 00:11:24,399
and this system basically you are

291
00:11:24,399 --> 00:11:25,839
collecting the full precision

292
00:11:25,839 --> 00:11:28,240
performance and you and then use the

293
00:11:28,240 --> 00:11:30,240
offline component or the performance

294
00:11:30,240 --> 00:11:32,640
modeling to decide what the forms will

295
00:11:32,640 --> 00:11:35,600
look like and then we we dynamically

296
00:11:35,600 --> 00:11:38,399
rewrite the graph a data flow graph

297
00:11:38,399 --> 00:11:40,640
to to to generate the

298
00:11:40,640 --> 00:11:42,399
the precision conversion between

299
00:11:42,399 --> 00:11:44,959
operators

300
00:11:47,120 --> 00:11:48,399
okay

301
00:11:48,399 --> 00:11:50,000
and now i'm going to jump into the

302
00:11:50,000 --> 00:11:51,760
design details i will start with the

303
00:11:51,760 --> 00:11:53,440
performance modeling

304
00:11:53,440 --> 00:11:54,800
so

305
00:11:54,800 --> 00:11:56,160
the channel ideally professional

306
00:11:56,160 --> 00:11:59,360
modeling is summarized as follows

307
00:11:59,360 --> 00:12:02,000
the first is the modeling is used to

308
00:12:02,000 --> 00:12:03,920
predict the casting cost and operator

309
00:12:03,920 --> 00:12:06,959
performance in low precision

310
00:12:06,959 --> 00:12:08,720
and

311
00:12:08,720 --> 00:12:11,040
the modeling itself is based on

312
00:12:11,040 --> 00:12:13,200
regression based model so you give me

313
00:12:13,200 --> 00:12:14,560
the full precision performance and then

314
00:12:14,560 --> 00:12:16,240
i predict the low performance a low

315
00:12:16,240 --> 00:12:19,200
precision performance

316
00:12:19,360 --> 00:12:21,920
and then this modeling is operation

317
00:12:21,920 --> 00:12:23,920
specific that basically means even

318
00:12:23,920 --> 00:12:26,160
operator has different has different

319
00:12:26,160 --> 00:12:27,600
performance models we are not going to

320
00:12:27,600 --> 00:12:30,160
use a general profile model to predict

321
00:12:30,160 --> 00:12:33,360
the performance of all operators

322
00:12:33,360 --> 00:12:35,200
in this way we are going to we are able

323
00:12:35,200 --> 00:12:37,600
to improve the prediction accuracy a lot

324
00:12:37,600 --> 00:12:40,079
you will see

325
00:12:42,480 --> 00:12:44,399
and the first we'll see the

326
00:12:44,399 --> 00:12:46,720
predicting the cutting cost okay so

327
00:12:46,720 --> 00:12:48,000
think about this

328
00:12:48,000 --> 00:12:50,160
performance prediction for casting cost

329
00:12:50,160 --> 00:12:52,639
as we just mentioned the custom cost has

330
00:12:52,639 --> 00:12:55,279
to be related to the

331
00:12:55,279 --> 00:12:57,120
input data size right or in other words

332
00:12:57,120 --> 00:12:58,880
the tensor size so this is our

333
00:12:58,880 --> 00:13:01,200
performance model to predict custom cost

334
00:13:01,200 --> 00:13:03,279
it's a relative tensor size and then we

335
00:13:03,279 --> 00:13:04,560
have

336
00:13:04,560 --> 00:13:06,480
a consideration of the

337
00:13:06,480 --> 00:13:09,360
casting operator initialization time

338
00:13:09,360 --> 00:13:10,639
which is constant

339
00:13:10,639 --> 00:13:13,279
and also we have the coefficient r

340
00:13:13,279 --> 00:13:15,040
and we use the

341
00:13:15,040 --> 00:13:16,240
linear regression to guide these

342
00:13:16,240 --> 00:13:17,600
coefficients

343
00:13:17,600 --> 00:13:18,800
and you can

344
00:13:18,800 --> 00:13:22,079
as you can see this model is input aware

345
00:13:22,079 --> 00:13:22,800
and

346
00:13:22,800 --> 00:13:26,399
can be generally here applied to

347
00:13:26,399 --> 00:13:28,320
the casting call to the clock to the

348
00:13:28,320 --> 00:13:29,839
casting of both

349
00:13:29,839 --> 00:13:31,760
uh high precision to low precision and

350
00:13:31,760 --> 00:13:35,399
low pressure into high precision

351
00:13:36,000 --> 00:13:37,120
and then

352
00:13:37,120 --> 00:13:39,440
we have this um

353
00:13:39,440 --> 00:13:41,440
performance model to predict the low

354
00:13:41,440 --> 00:13:42,959
precision performance as i just

355
00:13:42,959 --> 00:13:44,399
mentioned before

356
00:13:44,399 --> 00:13:47,279
this problem model is taking

357
00:13:47,279 --> 00:13:49,839
the full precision performance as input

358
00:13:49,839 --> 00:13:51,760
and then predict the low performance

359
00:13:51,760 --> 00:13:53,920
low precision performance

360
00:13:53,920 --> 00:13:55,600
and this model

361
00:13:55,600 --> 00:13:57,680
use a set of hardware control events as

362
00:13:57,680 --> 00:14:00,320
input and we have some coefficients like

363
00:14:00,320 --> 00:14:03,279
in this case wi

364
00:14:03,279 --> 00:14:04,560
and

365
00:14:04,560 --> 00:14:05,839
omega

366
00:14:05,839 --> 00:14:07,440
and those coefficients are collected

367
00:14:07,440 --> 00:14:10,160
through the regression

368
00:14:10,160 --> 00:14:12,399
and

369
00:14:12,800 --> 00:14:15,120
one critical thing about this model is

370
00:14:15,120 --> 00:14:17,279
this object this model is operation

371
00:14:17,279 --> 00:14:18,720
specific

372
00:14:18,720 --> 00:14:20,320
database because

373
00:14:20,320 --> 00:14:22,160
the let's think about this problem if

374
00:14:22,160 --> 00:14:23,519
the model

375
00:14:23,519 --> 00:14:27,120
is coupled to a specific operation

376
00:14:27,120 --> 00:14:28,959
the information comes from the operation

377
00:14:28,959 --> 00:14:30,720
type itself

378
00:14:30,720 --> 00:14:32,800
will give you a lot of

379
00:14:32,800 --> 00:14:34,480
information about the memory access kind

380
00:14:34,480 --> 00:14:35,279
of

381
00:14:35,279 --> 00:14:39,600
intensity arithmetic intensity and so on

382
00:14:39,600 --> 00:14:41,279
by considering operation type we can

383
00:14:41,279 --> 00:14:43,440
significantly reduce the complexity of

384
00:14:43,440 --> 00:14:44,800
the modeling

385
00:14:44,800 --> 00:14:46,800
for example if i tell you that this

386
00:14:46,800 --> 00:14:49,120
operator is matching multiplication

387
00:14:49,120 --> 00:14:50,959
you can quickly you can immediately get

388
00:14:50,959 --> 00:14:52,639
the sense that the magnetic size pattern

389
00:14:52,639 --> 00:14:55,360
in the operator is going to be striding

390
00:14:55,360 --> 00:14:56,959
as a result your performance model you

391
00:14:56,959 --> 00:14:58,959
don't need to model this sliding memory

392
00:14:58,959 --> 00:15:00,240
access anymore

393
00:15:00,240 --> 00:15:01,600
so in this way we are not only

394
00:15:01,600 --> 00:15:02,880
simplifying the performance model but

395
00:15:02,880 --> 00:15:05,920
also improve the accuracy a lot

396
00:15:05,920 --> 00:15:08,639
and also if you look at those operators

397
00:15:08,639 --> 00:15:10,000
in in

398
00:15:10,000 --> 00:15:11,519
those operators in the machine learning

399
00:15:11,519 --> 00:15:12,800
framework you don't have too many

400
00:15:12,800 --> 00:15:14,560
operators and you can build this

401
00:15:14,560 --> 00:15:16,480
performance model for each of them

402
00:15:16,480 --> 00:15:17,920
and also you can

403
00:15:17,920 --> 00:15:20,240
repeatedly use the models that actually

404
00:15:20,240 --> 00:15:22,240
amortize your

405
00:15:22,240 --> 00:15:25,360
model construction cost

406
00:15:25,680 --> 00:15:27,839
and also as you can see

407
00:15:27,839 --> 00:15:30,639
in the model we have this uh

408
00:15:30,639 --> 00:15:31,760
uh

409
00:15:31,760 --> 00:15:33,120
hardware control events and these

410
00:15:33,120 --> 00:15:35,440
hardware counter events are selected

411
00:15:35,440 --> 00:15:36,480
based on

412
00:15:36,480 --> 00:15:39,760
the spearman's rank correlation analysis

413
00:15:39,760 --> 00:15:41,760
and also events another

414
00:15:41,760 --> 00:15:42,720
um

415
00:15:42,720 --> 00:15:44,399
you know another universal i for

416
00:15:44,399 --> 00:15:46,480
different models for different operators

417
00:15:46,480 --> 00:15:48,160
we will use different

418
00:15:48,160 --> 00:15:49,440
um

419
00:15:49,440 --> 00:15:51,920
hardware counter events

420
00:15:51,920 --> 00:15:53,279
okay so this is a general idea about the

421
00:15:53,279 --> 00:15:55,920
performance modeling

422
00:15:55,920 --> 00:15:58,720
okay now now let's see what's next we

423
00:15:58,720 --> 00:16:00,079
have a performance model what should we

424
00:16:00,079 --> 00:16:02,079
do at the wrong time

425
00:16:02,079 --> 00:16:03,680
so at the runtime

426
00:16:03,680 --> 00:16:05,519
we want to decide the data precision for

427
00:16:05,519 --> 00:16:06,800
each operation

428
00:16:06,800 --> 00:16:09,120
and we want to decide which operators to

429
00:16:09,120 --> 00:16:11,440
be converted together to reduce the

430
00:16:11,440 --> 00:16:14,240
number of casting costs okay you need to

431
00:16:14,240 --> 00:16:16,079
minimize the causing cost

432
00:16:16,079 --> 00:16:17,920
and in general we want to minimize

433
00:16:17,920 --> 00:16:19,120
training time we want to minimize

434
00:16:19,120 --> 00:16:21,120
causing cost and don't

435
00:16:21,120 --> 00:16:23,920
impact on the numerical safety compared

436
00:16:23,920 --> 00:16:25,360
with the traditional mixed-reaction

437
00:16:25,360 --> 00:16:26,959
training

438
00:16:26,959 --> 00:16:28,399
so you want to respect the new

439
00:16:28,399 --> 00:16:30,000
recreativity decision made by the

440
00:16:30,000 --> 00:16:31,360
traditional

441
00:16:31,360 --> 00:16:34,320
machine learning framework

442
00:16:34,399 --> 00:16:36,320
so at the runtime we first used one

443
00:16:36,320 --> 00:16:38,160
equation to collect the uh

444
00:16:38,160 --> 00:16:39,759
uh the tensor

445
00:16:39,759 --> 00:16:42,079
shape or tensor size

446
00:16:42,079 --> 00:16:44,480
and the oxygen times those tensors and

447
00:16:44,480 --> 00:16:46,240
then we are going to

448
00:16:46,240 --> 00:16:48,560
rewrite the data flow graph using four

449
00:16:48,560 --> 00:16:50,720
passes

450
00:16:50,720 --> 00:16:52,079
in the first pass

451
00:16:52,079 --> 00:16:54,959
i'm going to examine every node

452
00:16:54,959 --> 00:16:56,639
in my dataflow graph and see whether

453
00:16:56,639 --> 00:16:58,959
this node is in the allow list

454
00:16:58,959 --> 00:17:01,199
and remember the node in the allow this

455
00:17:01,199 --> 00:17:04,160
is treated as numerical safe

456
00:17:04,160 --> 00:17:05,199
so

457
00:17:05,199 --> 00:17:07,839
we will use performance model design to

458
00:17:07,839 --> 00:17:10,240
decide if the benefit is going to be

459
00:17:10,240 --> 00:17:12,559
larger than custom cost or not

460
00:17:12,559 --> 00:17:14,559
is the first track worth

461
00:17:14,559 --> 00:17:16,480
and then in the second progress i'm

462
00:17:16,480 --> 00:17:19,439
going to check those numerically unsafe

463
00:17:19,439 --> 00:17:21,520
nodes basically those nodes are in the

464
00:17:21,520 --> 00:17:23,280
denied list

465
00:17:23,280 --> 00:17:25,359
i'm not going to try doing the cutting

466
00:17:25,359 --> 00:17:26,559
for those

467
00:17:26,559 --> 00:17:27,439
nodes

468
00:17:27,439 --> 00:17:30,240
or operators

469
00:17:30,720 --> 00:17:33,200
and then in the third traverse i'm going

470
00:17:33,200 --> 00:17:35,520
to examine the remaining nodes you mean

471
00:17:35,520 --> 00:17:38,240
operators in a data flow graph

472
00:17:38,240 --> 00:17:40,640
and see if they are in the infrared list

473
00:17:40,640 --> 00:17:42,799
or clear list

474
00:17:42,799 --> 00:17:44,720
and based on the result from second

475
00:17:44,720 --> 00:17:46,400
traverse we can also

476
00:17:46,400 --> 00:17:49,039
decide that the node in the infrared

477
00:17:49,039 --> 00:17:50,080
list

478
00:17:50,080 --> 00:17:55,120
is numerical numerically safe or not

479
00:17:55,360 --> 00:17:58,000
and then in the last challenge i'm going

480
00:17:58,000 --> 00:17:59,120
to

481
00:17:59,120 --> 00:18:01,039
check those numerically saved nodes

482
00:18:01,039 --> 00:18:03,120
identified in the last traverse

483
00:18:03,120 --> 00:18:04,640
and then we use professional model to

484
00:18:04,640 --> 00:18:07,039
decide whether benefit is larger than

485
00:18:07,039 --> 00:18:09,200
the casting cost or not if yes i'm going

486
00:18:09,200 --> 00:18:11,600
to insert a custom node

487
00:18:11,600 --> 00:18:14,000
so that's how that's all the whole

488
00:18:14,000 --> 00:18:16,400
picture

489
00:18:17,600 --> 00:18:19,360
and then i'm going to present our

490
00:18:19,360 --> 00:18:21,520
evaluation results we evaluate a system

491
00:18:21,520 --> 00:18:25,039
on a machine with in the intel xeon cpu

492
00:18:25,039 --> 00:18:27,760
and we evaluate two types of gpu one is

493
00:18:27,760 --> 00:18:32,480
with nvidia rtx 2080 the other is a v100

494
00:18:32,480 --> 00:18:33,760
and we use

495
00:18:33,760 --> 00:18:35,840
six machine learning

496
00:18:35,840 --> 00:18:38,240
models six common machine models

497
00:18:38,240 --> 00:18:39,120
and

498
00:18:39,120 --> 00:18:41,360
three data sets and we have two

499
00:18:41,360 --> 00:18:42,960
baselines for evaluation the first

500
00:18:42,960 --> 00:18:44,960
baseline is a

501
00:18:44,960 --> 00:18:47,280
cellular art mixed precision

502
00:18:47,280 --> 00:18:49,679
method in tensorflow called the tf

503
00:18:49,679 --> 00:18:51,520
underscore amp

504
00:18:51,520 --> 00:18:52,880
and the second

505
00:18:52,880 --> 00:18:54,400
baseline is the full precision training

506
00:18:54,400 --> 00:18:57,840
ipc 32

507
00:18:58,559 --> 00:19:01,360
we first look at the training throughput

508
00:19:01,360 --> 00:19:02,480
and

509
00:19:02,480 --> 00:19:05,600
so in this figure we have the y axis is

510
00:19:05,600 --> 00:19:07,280
the chinese throughput so higher is

511
00:19:07,280 --> 00:19:09,360
better higher is better

512
00:19:09,360 --> 00:19:11,039
so look at these curves and look at

513
00:19:11,039 --> 00:19:12,240
these bars

514
00:19:12,240 --> 00:19:14,960
and our system combo inputs throughput

515
00:19:14,960 --> 00:19:18,400
by 20 percentage on average and up to 23

516
00:19:18,400 --> 00:19:21,600
compared with the traditional

517
00:19:22,840 --> 00:19:26,240
tensorflow and uh in this slides i'm

518
00:19:26,240 --> 00:19:28,400
presenting this number of cast operation

519
00:19:28,400 --> 00:19:30,480
notes ideally if i want to

520
00:19:30,480 --> 00:19:32,640
reduce the number of custom nodes right

521
00:19:32,640 --> 00:19:34,160
because sometimes it's not performance

522
00:19:34,160 --> 00:19:36,400
beneficial to use a lower precision

523
00:19:36,400 --> 00:19:37,520
training

524
00:19:37,520 --> 00:19:39,919
and the y-axis in this figure

525
00:19:39,919 --> 00:19:42,240
is the number of custom nodes so we can

526
00:19:42,240 --> 00:19:44,240
see that the combo reduce the casting

527
00:19:44,240 --> 00:19:47,360
orbital node by 27 percentage on average

528
00:19:47,360 --> 00:19:48,559
compared with

529
00:19:48,559 --> 00:19:51,559
tensorflow

530
00:19:52,799 --> 00:19:54,720
and this figure

531
00:19:54,720 --> 00:19:57,120
is about uh tensor core utilization

532
00:19:57,120 --> 00:19:58,960
tensor code utilization

533
00:19:58,960 --> 00:20:01,679
and again we compare with the tensorflow

534
00:20:01,679 --> 00:20:03,520
and you can see that

535
00:20:03,520 --> 00:20:05,440
uh the y-axis tensor utilization so

536
00:20:05,440 --> 00:20:06,880
higher is better

537
00:20:06,880 --> 00:20:08,799
you can see that the combo actually

538
00:20:08,799 --> 00:20:10,880
increased utilization of the tensor car

539
00:20:10,880 --> 00:20:14,559
by 29 percentage on average and up to 37

540
00:20:14,559 --> 00:20:16,080
percentage

541
00:20:16,080 --> 00:20:18,799
so the reason why our system can improve

542
00:20:18,799 --> 00:20:20,960
cancer utilization in this way is

543
00:20:20,960 --> 00:20:22,480
because

544
00:20:22,480 --> 00:20:24,960
once a performance model predicts that

545
00:20:24,960 --> 00:20:26,640
the benefit the performance benefit is

546
00:20:26,640 --> 00:20:29,120
larger than the custom cost

547
00:20:29,120 --> 00:20:32,080
then our system will try best to utilize

548
00:20:32,080 --> 00:20:34,559
the tensor code even though the input to

549
00:20:34,559 --> 00:20:37,840
the operator is not aligned with the

550
00:20:37,840 --> 00:20:39,679
shape requirement or tensor call so

551
00:20:39,679 --> 00:20:42,799
that's given us a higher utilization of

552
00:20:42,799 --> 00:20:45,440
the tensor code

553
00:20:46,960 --> 00:20:48,480
and the last slide this is this is my

554
00:20:48,480 --> 00:20:50,159
last slide i'm going to compare the

555
00:20:50,159 --> 00:20:51,760
training accuracy

556
00:20:51,760 --> 00:20:53,200
um between

557
00:20:53,200 --> 00:20:55,840
full precision combo and the tensorflow

558
00:20:55,840 --> 00:20:57,919
so you can see that the we actually

559
00:20:57,919 --> 00:20:59,440
don't lose string accuracy this is

560
00:20:59,440 --> 00:21:01,760
because we fully respect the numerical

561
00:21:01,760 --> 00:21:04,480
safety decided by the machine learning

562
00:21:04,480 --> 00:21:06,960
framework

563
00:21:08,000 --> 00:21:08,880
okay

564
00:21:08,880 --> 00:21:10,799
so in general in this work i'm going to

565
00:21:10,799 --> 00:21:12,880
i'm trying to tell you a story that the

566
00:21:12,880 --> 00:21:14,400
casting cost

567
00:21:14,400 --> 00:21:17,120
is not ignorable when you do the mixed

568
00:21:17,120 --> 00:21:19,280
prison training you've got to take it

569
00:21:19,280 --> 00:21:21,520
into consideration

570
00:21:21,520 --> 00:21:23,600
and in this work we contribute the novel

571
00:21:23,600 --> 00:21:25,280
performance modeling and graph rewriting

572
00:21:25,280 --> 00:21:27,840
techniques to enable this

573
00:21:27,840 --> 00:21:30,559
more efficient mixed precision training

574
00:21:30,559 --> 00:21:32,720
and in general will bring the 20 degree

575
00:21:32,720 --> 00:21:35,919
performance up to 23 percentage

576
00:21:35,919 --> 00:21:37,760
improvement that's it for this talk

577
00:21:37,760 --> 00:21:40,919
thank you

578
00:21:48,320 --> 00:21:50,399
you

