1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:16,279 --> 00:00:19,560
University today I'm happy to introduce

3
00:00:19,560 --> 00:00:21,600
our work on improving the performance of

4
00:00:21,600 --> 00:00:23,820
RDMAs control Parts with a systematic

5
00:00:23,820 --> 00:00:24,840
approach

6
00:00:24,840 --> 00:00:26,939
this is a joint work with my student

7
00:00:26,939 --> 00:00:29,640
farming Lou and my colleagues long-term

8
00:00:29,640 --> 00:00:31,579
and hypoten

9
00:00:31,579 --> 00:00:34,800
remote directory memory accesses is a

10
00:00:34,800 --> 00:00:36,300
high bandwidth and low latency

11
00:00:36,300 --> 00:00:38,520
networking feature widely adopted in

12
00:00:38,520 --> 00:00:41,040
modern data centers and provides two

13
00:00:41,040 --> 00:00:42,180
Primitives

14
00:00:42,180 --> 00:00:44,940
for one-sided primitive the Nick can

15
00:00:44,940 --> 00:00:47,280
directly read write servers memory by

16
00:00:47,280 --> 00:00:49,200
processing server CPUs

17
00:00:49,200 --> 00:00:51,360
two-sided primitive is a messaging

18
00:00:51,360 --> 00:00:53,280
primitive that provides the same receive

19
00:00:53,280 --> 00:00:54,539
interface

20
00:00:54,539 --> 00:00:57,420
thanks to RDMA many distributed systems

21
00:00:57,420 --> 00:01:00,000
including distributed keyword stores and

22
00:01:00,000 --> 00:01:01,860
distributed transaction processing

23
00:01:01,860 --> 00:01:03,780
systems can have dramatic performance

24
00:01:03,780 --> 00:01:07,159
improvements for example raise Hashem

25
00:01:07,159 --> 00:01:09,479
recently production disaggregated

26
00:01:09,479 --> 00:01:12,240
tiverstop can execute a single keyword

27
00:01:12,240 --> 00:01:15,900
request within 5 microseconds

28
00:01:15,900 --> 00:01:19,080
however so rdma's data pass operation is

29
00:01:19,080 --> 00:01:21,840
extremely fast our name is Control Plus

30
00:01:21,840 --> 00:01:25,080
operation is orders of magnitude slower

31
00:01:25,080 --> 00:01:28,020
specifically on our platform we measure

32
00:01:28,020 --> 00:01:30,420
the end to end connection creation and

33
00:01:30,420 --> 00:01:33,119
connection time of a cube here which

34
00:01:33,119 --> 00:01:36,180
takes 15 milliseconds in comparison

35
00:01:36,180 --> 00:01:39,000
posting and pulling an RDMA one-sided

36
00:01:39,000 --> 00:01:42,659
requests only take two microseconds

37
00:01:42,659 --> 00:01:45,000
more specifically the RDMA control

38
00:01:45,000 --> 00:01:47,040
connection creation time can be

39
00:01:47,040 --> 00:01:49,439
decomposed into three parts first

40
00:01:49,439 --> 00:01:51,360
loading the driver context which

41
00:01:51,360 --> 00:01:53,759
includes current device status and

42
00:01:53,759 --> 00:01:56,340
creating the protection domain

43
00:01:56,340 --> 00:01:58,439
second the application must create

44
00:01:58,439 --> 00:02:01,020
Hardware cues both at the client and at

45
00:02:01,020 --> 00:02:03,840
the server and we need to connecting the

46
00:02:03,840 --> 00:02:06,360
skills by configure configuring them to

47
00:02:06,360 --> 00:02:07,799
appropriate States

48
00:02:07,799 --> 00:02:11,640
this time counts for 4 milliseconds

49
00:02:11,640 --> 00:02:13,739
note that you allow QPR connection

50
00:02:13,739 --> 00:02:16,140
different hosts must exchange connection

51
00:02:16,140 --> 00:02:18,480
permissions through the network which

52
00:02:18,480 --> 00:02:21,599
counts up to 1 milliseconds

53
00:02:21,599 --> 00:02:24,480
from the above breakdown we can see that

54
00:02:24,480 --> 00:02:26,640
the term is bottlenecked by the first

55
00:02:26,640 --> 00:02:29,819
and two steps however our analysis

56
00:02:29,819 --> 00:02:31,440
reveals that they are difficult to

57
00:02:31,440 --> 00:02:34,319
reduce the time is dominated by creating

58
00:02:34,319 --> 00:02:36,060
and configuring Hardware resources

59
00:02:36,060 --> 00:02:38,400
resources for example creating the

60
00:02:38,400 --> 00:02:41,040
hardware queues

61
00:02:41,040 --> 00:02:43,140
one may argue that the Control Plus

62
00:02:43,140 --> 00:02:45,300
codes can hardly affect the performance

63
00:02:45,300 --> 00:02:47,580
of traditional RDMA enabled applications

64
00:02:47,580 --> 00:02:50,519
including distributed QR stores and

65
00:02:50,519 --> 00:02:53,040
distributed databases this is because

66
00:02:53,040 --> 00:02:55,319
these applications run a sufficient long

67
00:02:55,319 --> 00:02:57,900
time which amortize the costs of RDMA

68
00:02:57,900 --> 00:02:59,220
control parts

69
00:02:59,220 --> 00:03:01,319
unfortunately we find the control

70
00:03:01,319 --> 00:03:03,360
process may still affect new

71
00:03:03,360 --> 00:03:05,580
applications especially those that

72
00:03:05,580 --> 00:03:08,700
require a good elasticity

73
00:03:08,700 --> 00:03:11,700
one particular example is RDMA enabled

74
00:03:11,700 --> 00:03:14,459
disaggregated keyword store in such a

75
00:03:14,459 --> 00:03:16,920
setup the nodes are separated in two

76
00:03:16,920 --> 00:03:19,860
rows memory loss these are nodes with

77
00:03:19,860 --> 00:03:22,260
huge amount of memory with little CPU

78
00:03:22,260 --> 00:03:24,959
resources they store the key value pairs

79
00:03:24,959 --> 00:03:26,580
in their memory

80
00:03:26,580 --> 00:03:29,220
second completely notes these are notes

81
00:03:29,220 --> 00:03:32,340
that execute keyword store API 88 on top

82
00:03:32,340 --> 00:03:34,800
of Domino's they have little memory and

83
00:03:34,800 --> 00:03:38,220
has a powerful CPU so they execute the

84
00:03:38,220 --> 00:03:40,260
key value API by reading the key value

85
00:03:40,260 --> 00:03:42,599
contents from when I found the memory

86
00:03:42,599 --> 00:03:46,019
notes using one-sided RDMA

87
00:03:46,019 --> 00:03:48,480
animate this this aggregated camera

88
00:03:48,480 --> 00:03:51,000
stores provide elasticity elasticity by

89
00:03:51,000 --> 00:03:53,580
allowing directly dynamically adding and

90
00:03:53,580 --> 00:03:55,739
removing company nodes and the high

91
00:03:55,739 --> 00:03:58,019
loans it can add more Computing nodes to

92
00:03:58,019 --> 00:04:00,540
improve the performance for low lows it

93
00:04:00,540 --> 00:04:03,480
can reclaim the added nodes to for

94
00:04:03,480 --> 00:04:05,400
better resource efficiency

95
00:04:05,400 --> 00:04:08,340
for such a setup we can see that new

96
00:04:08,340 --> 00:04:10,920
nodes need to dynamically create RDMA

97
00:04:10,920 --> 00:04:12,980
enabled connections to the membranos

98
00:04:12,980 --> 00:04:16,500
thus the high connection time of QPR or

99
00:04:16,500 --> 00:04:18,720
the inevitably increase the tier latency

100
00:04:18,720 --> 00:04:21,238
especially in the load spikes which

101
00:04:21,238 --> 00:04:24,419
means there's a signal increase of loss

102
00:04:24,419 --> 00:04:27,120
of the applications

103
00:04:27,120 --> 00:04:29,639
so the goal of our work is to provide a

104
00:04:29,639 --> 00:04:31,740
systematic solution to reduce audience

105
00:04:31,740 --> 00:04:34,560
Control Plus cost from milliseconds to

106
00:04:34,560 --> 00:04:36,900
microseconds and being compatible to

107
00:04:36,900 --> 00:04:40,320
existing RDMA hardware and software

108
00:04:40,320 --> 00:04:43,440
so to achieve so our basic idea is to

109
00:04:43,440 --> 00:04:46,020
use pulling which means that we catch

110
00:04:46,020 --> 00:04:48,960
the created QPS in a connection pool so

111
00:04:48,960 --> 00:04:51,120
that later applications can reuse the

112
00:04:51,120 --> 00:04:54,840
cash to QPS as long as one is cached

113
00:04:54,840 --> 00:04:56,940
for example suppose application one

114
00:04:56,940 --> 00:05:00,120
started with an empty pool as follows

115
00:05:00,120 --> 00:05:01,740
the normal how do you make Control Plus

116
00:05:01,740 --> 00:05:03,660
which is slow and is your data bus

117
00:05:03,660 --> 00:05:07,500
requests after the qpm is created

118
00:05:07,500 --> 00:05:10,680
after it finishes it can cast to qpm in

119
00:05:10,680 --> 00:05:13,919
an Epic Air connection pool afterward a

120
00:05:13,919 --> 00:05:16,560
new application application 2 can cover

121
00:05:16,560 --> 00:05:18,660
the QP in the pool and the issue

122
00:05:18,660 --> 00:05:20,759
automate request to the customer without

123
00:05:20,759 --> 00:05:24,919
creating and connecting the QPS

124
00:05:24,960 --> 00:05:27,300
however the approach has one important

125
00:05:27,300 --> 00:05:30,000
challenge different applications qpm

126
00:05:30,000 --> 00:05:32,520
cannot be reused and shared

127
00:05:32,520 --> 00:05:35,160
this is because RDMA is primarily

128
00:05:35,160 --> 00:05:37,560
designed as a user space solution which

129
00:05:37,560 --> 00:05:39,600
means that QPS are not designed for

130
00:05:39,600 --> 00:05:41,460
sharing between them

131
00:05:41,460 --> 00:05:44,759
so qpe further has many complex data

132
00:05:44,759 --> 00:05:46,560
structures spreading over the kernel

133
00:05:46,560 --> 00:05:48,419
space and user space

134
00:05:48,419 --> 00:05:50,639
sharing all these features and

135
00:05:50,639 --> 00:05:52,259
coordinating between accesses of

136
00:05:52,259 --> 00:05:54,360
different applications we believe it's

137
00:05:54,360 --> 00:05:55,740
impractical

138
00:05:55,740 --> 00:05:58,440
further sharing QP alone cannot

139
00:05:58,440 --> 00:06:01,020
eliminate the driver loading cost in the

140
00:06:01,020 --> 00:06:04,139
user space automate applications

141
00:06:04,139 --> 00:06:07,020
to address this issue our observation is

142
00:06:07,020 --> 00:06:09,539
that the kernel space RDMA so less used

143
00:06:09,539 --> 00:06:13,020
also provides full functionality of RDMA

144
00:06:13,020 --> 00:06:16,020
for example the user space automate qcm

145
00:06:16,020 --> 00:06:20,880
namely ibv QP has an equivalent ipqp in

146
00:06:20,880 --> 00:06:23,580
a kernel which supports all one-sided

147
00:06:23,580 --> 00:06:26,520
and two-sided operations thus we can

148
00:06:26,520 --> 00:06:29,220
shift the applications RDMA requests to

149
00:06:29,220 --> 00:06:31,800
the kernel space qpx

150
00:06:31,800 --> 00:06:34,680
based on this observation the idea is to

151
00:06:34,680 --> 00:06:37,680
Cache kernel space QPS and put them in a

152
00:06:37,680 --> 00:06:39,479
kernel space QPR pool

153
00:06:39,479 --> 00:06:42,120
thus applications can share and reduce

154
00:06:42,120 --> 00:06:44,520
these QPS since they share the same

155
00:06:44,520 --> 00:06:47,100
kernel to allow the sharing we will

156
00:06:47,100 --> 00:06:50,100
translate the user space RDMA request to

157
00:06:50,100 --> 00:06:52,080
the online kernel space to be request

158
00:06:52,080 --> 00:06:55,139
through system course

159
00:06:55,139 --> 00:06:57,900
note that another benefit of a corner

160
00:06:57,900 --> 00:06:59,699
space solution is that the kernel space

161
00:06:59,699 --> 00:07:02,100
RDMA avoids the code straight user space

162
00:07:02,100 --> 00:07:03,960
driver loading process

163
00:07:03,960 --> 00:07:06,660
this is because the cuddle can pre-load

164
00:07:06,660 --> 00:07:10,080
the driver context during the boot time

165
00:07:10,080 --> 00:07:12,360
now we allow different applications to

166
00:07:12,360 --> 00:07:14,520
share the QP to avoid creating them from

167
00:07:14,520 --> 00:07:16,500
stretch for new applications

168
00:07:16,500 --> 00:07:18,840
so another problem we are facing is that

169
00:07:18,840 --> 00:07:22,080
how to reduce the QP process

170
00:07:22,080 --> 00:07:25,500
so rdbqv is a one-q mapping this means

171
00:07:25,500 --> 00:07:28,199
that we need a dedicated tube here to

172
00:07:28,199 --> 00:07:30,720
connect to a different server moreover

173
00:07:30,720 --> 00:07:33,240
due to the high performance of automate

174
00:07:33,240 --> 00:07:35,819
it is desirable to use different QPS for

175
00:07:35,819 --> 00:07:39,840
CPU to prevent locking overhead as a

176
00:07:39,840 --> 00:07:42,240
result each machine needs to Cache n

177
00:07:42,240 --> 00:07:45,240
plus and QPS where m is the machine

178
00:07:45,240 --> 00:07:47,520
number in the cluster and it is the core

179
00:07:47,520 --> 00:07:49,740
number at each machine to eliminate

180
00:07:49,740 --> 00:07:53,340
eliminate all the control plan costs

181
00:07:53,340 --> 00:07:56,160
this causes a significant significant

182
00:07:56,160 --> 00:07:58,319
amount of memory used especially for

183
00:07:58,319 --> 00:08:00,960
modern RDMA capable cluster with more

184
00:08:00,960 --> 00:08:03,539
than 10 000 nodes

185
00:08:03,539 --> 00:08:05,660
to address this issue we find

186
00:08:05,660 --> 00:08:08,639
dynamically connected transport unless

187
00:08:08,639 --> 00:08:10,380
used but widely supported anime

188
00:08:10,380 --> 00:08:12,479
transport with commercials feature can

189
00:08:12,479 --> 00:08:14,720
help

190
00:08:14,720 --> 00:08:17,940
specifically a single Dynamic connected

191
00:08:17,940 --> 00:08:20,940
QP dcqp can connect to different nodes

192
00:08:20,940 --> 00:08:23,280
without a connection process as a normal

193
00:08:23,280 --> 00:08:25,379
rc2p

194
00:08:25,379 --> 00:08:27,300
the hardware will pick back the

195
00:08:27,300 --> 00:08:29,520
connection requests with the RDMA data

196
00:08:29,520 --> 00:08:32,760
pass request which is extremely fast

197
00:08:32,760 --> 00:08:35,940
for example a DC reconnection Plus data

198
00:08:35,940 --> 00:08:38,760
operation only takes three microseconds

199
00:08:38,760 --> 00:08:41,279
notice that dcqp provides the same

200
00:08:41,279 --> 00:08:43,919
semantic as rcqp which means that they

201
00:08:43,919 --> 00:08:47,160
also provide a reliability

202
00:08:47,160 --> 00:08:50,279
based on adoption of gctp the high level

203
00:08:50,279 --> 00:08:53,160
idea is to reduce the QPS in the pool

204
00:08:53,160 --> 00:08:54,899
with dcqp

205
00:08:54,899 --> 00:08:56,880
so we'll replace all the r Securities

206
00:08:56,880 --> 00:08:58,620
with one dcqp

207
00:08:58,620 --> 00:09:00,839
consequently the kernel can avoid

208
00:09:00,839 --> 00:09:03,420
stalling all the QPS connections to to

209
00:09:03,420 --> 00:09:05,519
all the other nodes

210
00:09:05,519 --> 00:09:08,700
so one problem of using dcqb is that for

211
00:09:08,700 --> 00:09:11,100
each machine to communicate with a

212
00:09:11,100 --> 00:09:13,560
specific host the host must have created

213
00:09:13,560 --> 00:09:16,380
DC Target and hand off the target's

214
00:09:16,380 --> 00:09:18,720
metadata to the client such such that

215
00:09:18,720 --> 00:09:21,720
the client can pass this metadata to

216
00:09:21,720 --> 00:09:24,000
each rlma request to indicate the remote

217
00:09:24,000 --> 00:09:25,140
host

218
00:09:25,140 --> 00:09:27,660
so the question is that how can a client

219
00:09:27,660 --> 00:09:30,540
eventually discover the DC metadata of a

220
00:09:30,540 --> 00:09:33,060
remote Target

221
00:09:33,060 --> 00:09:35,640
a naive solution is to use remote

222
00:09:35,640 --> 00:09:38,519
procedural core RPC for the current

223
00:09:38,519 --> 00:09:40,500
so our debate you provide unreliable

224
00:09:40,500 --> 00:09:42,600
datagram which supports connectionless

225
00:09:42,600 --> 00:09:44,040
RPC

226
00:09:44,040 --> 00:09:46,380
however there are two drawbacks of an

227
00:09:46,380 --> 00:09:48,180
RPC based design

228
00:09:48,180 --> 00:09:51,600
first RPC takes X consumes extra CPU

229
00:09:51,600 --> 00:09:53,339
resources at each node

230
00:09:53,339 --> 00:09:56,100
as a kernel space solution we cannot

231
00:09:56,100 --> 00:09:58,620
dedicate many CPUs for to handle these

232
00:09:58,620 --> 00:09:59,640
requests

233
00:09:59,640 --> 00:10:02,399
second RPC may include long-term latency

234
00:10:02,399 --> 00:10:04,800
due to the required securing and the CPU

235
00:10:04,800 --> 00:10:07,620
scheduling at remote host

236
00:10:07,620 --> 00:10:10,260
the desired way for the digital metric

237
00:10:10,260 --> 00:10:14,100
Discovery is to use one-sided RDMA

238
00:10:14,100 --> 00:10:16,200
so to achieve so we propose an

239
00:10:16,200 --> 00:10:18,660
architecture that uses meta servers to

240
00:10:18,660 --> 00:10:21,240
store DCd method related data

241
00:10:21,240 --> 00:10:23,880
so metadata is in principle similar to

242
00:10:23,880 --> 00:10:26,040
DNS servers that stores a mapping

243
00:10:26,040 --> 00:10:28,920
between a remote nodes RDMA address and

244
00:10:28,920 --> 00:10:31,680
the corresponding dcg metadata

245
00:10:31,680 --> 00:10:34,260
notice that storing all the metadata of

246
00:10:34,260 --> 00:10:37,200
all servers in of your cluster in a meta

247
00:10:37,200 --> 00:10:39,660
server is practical because the metadata

248
00:10:39,660 --> 00:10:43,080
is is extremely small it only consumes

249
00:10:43,080 --> 00:10:45,300
12 bytes

250
00:10:45,300 --> 00:10:47,399
so this separation allows us to use

251
00:10:47,399 --> 00:10:49,500
one-sided RDMA to currently the match

252
00:10:49,500 --> 00:10:52,800
data specifically we implement the meta

253
00:10:52,800 --> 00:10:55,260
server as an RDMA enabled curios store

254
00:10:55,260 --> 00:10:58,079
commonly exploited in literature the

255
00:10:58,079 --> 00:10:59,940
keyword stored can be occurred in

256
00:10:59,940 --> 00:11:02,399
one-sided RMA by parsing remote CPU

257
00:11:02,399 --> 00:11:05,100
which is extremely fast and resource

258
00:11:05,100 --> 00:11:06,000
efficient

259
00:11:06,000 --> 00:11:08,640
consequently a whole connection and data

260
00:11:08,640 --> 00:11:11,519
process only takes 10 microseconds for

261
00:11:11,519 --> 00:11:15,079
for newly new connections

262
00:11:15,360 --> 00:11:17,820
so our final challenge I need to mention

263
00:11:17,820 --> 00:11:20,880
is that how to Multiplex QP to multiple

264
00:11:20,880 --> 00:11:22,680
applications correctly

265
00:11:22,680 --> 00:11:25,800
so to prevent a QP from not sharing to

266
00:11:25,800 --> 00:11:28,140
prevent to prevent an application from

267
00:11:28,140 --> 00:11:30,959
not sharing a cache QP will allow each

268
00:11:30,959 --> 00:11:33,360
dcqb in a pool to be shared among

269
00:11:33,360 --> 00:11:34,980
different applications

270
00:11:34,980 --> 00:11:38,160
so we further abstract the shared QP so

271
00:11:38,160 --> 00:11:39,720
that it gives the application an

272
00:11:39,720 --> 00:11:43,380
illusion of an exclusively owned QPR for

273
00:11:43,380 --> 00:11:45,300
example an application one and

274
00:11:45,300 --> 00:11:48,240
application two they have a exclusively

275
00:11:48,240 --> 00:11:50,779
virtualized the QP after the user space

276
00:11:50,779 --> 00:11:53,880
nevertheless the request to The Virtuous

277
00:11:53,880 --> 00:11:57,000
2p can be folded to the same kernel

278
00:11:57,000 --> 00:11:59,760
space QP so that so that the

279
00:11:59,760 --> 00:12:01,500
applications can always have an

280
00:12:01,500 --> 00:12:04,560
available QP to use

281
00:12:04,560 --> 00:12:06,660
however one challenge of sharing the

282
00:12:06,660 --> 00:12:09,600
same QP among different applications is

283
00:12:09,600 --> 00:12:12,060
that a shared QP can be corrupted even

284
00:12:12,060 --> 00:12:15,000
when application uses correctly because

285
00:12:15,000 --> 00:12:18,300
they assume an exclusive own abstraction

286
00:12:18,300 --> 00:12:22,079
abstraction so if the QP is crafted it

287
00:12:22,079 --> 00:12:24,779
means the connection which is no which

288
00:12:24,779 --> 00:12:26,760
is slow

289
00:12:26,760 --> 00:12:29,399
to this end we carefully add checks

290
00:12:29,399 --> 00:12:31,500
before posting the requests to the

291
00:12:31,500 --> 00:12:33,959
underlying shared QP and reject the

292
00:12:33,959 --> 00:12:36,420
request that can Quant the QV stats if

293
00:12:36,420 --> 00:12:37,800
necessary

294
00:12:37,800 --> 00:12:39,779
specifically we check the following

295
00:12:39,779 --> 00:12:42,320
three things first mail from request

296
00:12:42,320 --> 00:12:45,720
whether the request content contents and

297
00:12:45,720 --> 00:12:47,940
invalid operation code or a memory

298
00:12:47,940 --> 00:12:50,820
reference second whether your question

299
00:12:50,820 --> 00:12:53,279
may overflow the hardware queue

300
00:12:53,279 --> 00:12:55,740
finally we need to additionally dispatch

301
00:12:55,740 --> 00:12:57,420
the requests from different applications

302
00:12:57,420 --> 00:12:59,399
to the practice

303
00:12:59,399 --> 00:13:02,040
from the private destinations because a

304
00:13:02,040 --> 00:13:04,680
shared QB's responses may belongs to

305
00:13:04,680 --> 00:13:07,200
different applications

306
00:13:07,200 --> 00:13:09,600
so after checking all these factors we

307
00:13:09,600 --> 00:13:12,180
can now prepare provide an exclusively

308
00:13:12,180 --> 00:13:14,160
owned QP obstruction to the application

309
00:13:14,160 --> 00:13:16,860
for the shell QP without worrying about

310
00:13:16,860 --> 00:13:19,320
QP Crab Shack

311
00:13:19,320 --> 00:13:22,380
so put it all together with present QR

312
00:13:22,380 --> 00:13:24,480
code a networking library that provides

313
00:13:24,480 --> 00:13:26,399
a micro second skill automate connection

314
00:13:26,399 --> 00:13:29,700
establishment and Community Onyx that

315
00:13:29,700 --> 00:13:32,820
supports DCT QR code is 1500 times

316
00:13:32,820 --> 00:13:36,180
faster than IB works the

317
00:13:36,180 --> 00:13:39,480
de facto standard library for using RDMA

318
00:13:39,480 --> 00:13:42,240
so you also apply many authorizations to

319
00:13:42,240 --> 00:13:43,680
further improve its performance

320
00:13:43,680 --> 00:13:46,560
including dcg metadata caching and

321
00:13:46,560 --> 00:13:49,860
dynamic switch between DC and rcqps so

322
00:13:49,860 --> 00:13:51,899
if you have interest to please refer to

323
00:13:51,899 --> 00:13:54,839
our paper for more information

324
00:13:54,839 --> 00:13:57,839
so to ease development we Implement QR

325
00:13:57,839 --> 00:14:00,300
code as a loadable kernel module using

326
00:14:00,300 --> 00:14:03,240
more than 10 000 rust line of code

327
00:14:03,240 --> 00:14:06,000
we further add a little C shame later to

328
00:14:06,000 --> 00:14:07,980
abstract the system core code for the

329
00:14:07,980 --> 00:14:11,220
calco kernel so to support DCT in the

330
00:14:11,220 --> 00:14:13,560
kernel we also Port the new user space

331
00:14:13,560 --> 00:14:15,540
DCT driver implementation into the

332
00:14:15,540 --> 00:14:17,760
kernel kernel space animate driver by

333
00:14:17,760 --> 00:14:21,120
adding about 250 line of line of C code

334
00:14:21,120 --> 00:14:24,480
to the to the metallics of Ed 4.9 driver

335
00:14:24,480 --> 00:14:28,700
so krco is available on GitHub

336
00:14:29,279 --> 00:14:31,500
so finally we present the evaluation

337
00:14:31,500 --> 00:14:33,959
results of QR core aiming to answer the

338
00:14:33,959 --> 00:14:35,639
following three questions

339
00:14:35,639 --> 00:14:38,940
first how fast is KR course control pair

340
00:14:38,940 --> 00:14:41,639
second what are the costs added to care

341
00:14:41,639 --> 00:14:43,079
course data plan

342
00:14:43,079 --> 00:14:46,380
third can care core benefits existing

343
00:14:46,380 --> 00:14:49,079
anime existing applications that require

344
00:14:49,079 --> 00:14:51,480
elasticity

345
00:14:51,480 --> 00:14:54,240
our evaluations are conducted on a local

346
00:14:54,240 --> 00:14:57,060
rock scale cluster with 10 nodes each

347
00:14:57,060 --> 00:14:59,399
machine has one connect connect X4

348
00:14:59,399 --> 00:15:03,360
Infinity band 100 gigabytes on it

349
00:15:03,360 --> 00:15:05,519
so we compare care core with two Corner

350
00:15:05,519 --> 00:15:08,220
Parts first IB verbs is the de facto

351
00:15:08,220 --> 00:15:10,740
user space standard for using anime

352
00:15:10,740 --> 00:15:13,560
second light is the only kernel space

353
00:15:13,560 --> 00:15:15,839
item solution that leverage kernel space

354
00:15:15,839 --> 00:15:19,019
rcqp for communications

355
00:15:19,019 --> 00:15:21,420
so first let's exam the Control Plus

356
00:15:21,420 --> 00:15:24,300
performance of QR code our first case

357
00:15:24,300 --> 00:15:26,279
study is the performance of multiple

358
00:15:26,279 --> 00:15:28,920
clients connecting to a single server so

359
00:15:28,920 --> 00:15:30,600
from the figure we can see that

360
00:15:30,600 --> 00:15:33,240
character is orders of magnitude faster

361
00:15:33,240 --> 00:15:35,820
that works and lights notice that the

362
00:15:35,820 --> 00:15:38,760
accesses are log scale this is mainly

363
00:15:38,760 --> 00:15:40,500
due to the fact that Clark Hall replace

364
00:15:40,500 --> 00:15:42,720
the Control Plus operations with data

365
00:15:42,720 --> 00:15:45,480
pass operations namely one-sided RDMA

366
00:15:45,480 --> 00:15:47,339
requests to curl the metadata to The

367
00:15:47,339 --> 00:15:48,899
Meta server

368
00:15:48,899 --> 00:15:52,019
light is faster than wolves because it

369
00:15:52,019 --> 00:15:54,000
will avoids the driver loading process

370
00:15:54,000 --> 00:15:57,000
since it's a kernel space RDMA solution

371
00:15:57,000 --> 00:15:59,519
however both light and Buffs are bottom

372
00:15:59,519 --> 00:16:03,720
acted by creating Hardware cues on ionic

373
00:16:03,720 --> 00:16:05,820
next we present the performance of

374
00:16:05,820 --> 00:16:07,920
creating full mesh connections for

375
00:16:07,920 --> 00:16:09,839
cluster of machines

376
00:16:09,839 --> 00:16:12,180
you can see that QR code also takes

377
00:16:12,180 --> 00:16:14,459
orders of magnitude shorter time than

378
00:16:14,459 --> 00:16:16,860
light and works respectively thanks to

379
00:16:16,860 --> 00:16:19,139
the faster performance of connecting one

380
00:16:19,139 --> 00:16:21,540
connections

381
00:16:21,540 --> 00:16:24,180
so krco trades the performance of data

382
00:16:24,180 --> 00:16:26,760
paths for faster control costs now we

383
00:16:26,760 --> 00:16:28,560
show its data pass performance on a

384
00:16:28,560 --> 00:16:30,360
synchronized one-sided animal read

385
00:16:30,360 --> 00:16:31,440
benchmark

386
00:16:31,440 --> 00:16:33,660
the client keeps sending one-sided

387
00:16:33,660 --> 00:16:36,000
uneven read requests to a server in a

388
00:16:36,000 --> 00:16:38,820
room to completion way so we conduct the

389
00:16:38,820 --> 00:16:40,800
experiment by increasing the number of

390
00:16:40,800 --> 00:16:43,259
clients as you can see both characters

391
00:16:43,259 --> 00:16:45,120
and light has a latency increase

392
00:16:45,120 --> 00:16:47,519
compared to RB verbs

393
00:16:47,519 --> 00:16:49,920
the overhead has two parts

394
00:16:49,920 --> 00:16:53,220
first they have like one Microsoft

395
00:16:53,220 --> 00:16:55,320
latest increase due to the system core

396
00:16:55,320 --> 00:16:57,720
overhead since we need a kernel space

397
00:16:57,720 --> 00:17:01,079
QPR pool to uh to share QPS between

398
00:17:01,079 --> 00:17:03,480
different applications

399
00:17:03,480 --> 00:17:07,380
second calc has a slower Peak throughput

400
00:17:07,380 --> 00:17:10,919
due to the instruction of DCT compared

401
00:17:10,919 --> 00:17:14,760
with rcqp using light and verbs DCT has

402
00:17:14,760 --> 00:17:17,339
has to reconnect has a reconnection

403
00:17:17,339 --> 00:17:19,919
message so it speaks throughput Port is

404
00:17:19,919 --> 00:17:22,280
lower

405
00:17:23,040 --> 00:17:25,679
finally we show that QR code can benefit

406
00:17:25,679 --> 00:17:28,380
existing RDMA enabled systems that

407
00:17:28,380 --> 00:17:31,679
require elasticity we choose recession a

408
00:17:31,679 --> 00:17:34,140
production disaggregated RDMA enabled

409
00:17:34,140 --> 00:17:35,280
Cura storm

410
00:17:35,280 --> 00:17:38,400
at times zero there's a low Spike for

411
00:17:38,400 --> 00:17:41,160
the inner workloads therefore rules what

412
00:17:41,160 --> 00:17:43,200
an inventory spoon new Computing nodes

413
00:17:43,200 --> 00:17:45,419
to accelerate its performance

414
00:17:45,419 --> 00:17:48,179
so care core can bootstrap all the nodes

415
00:17:48,179 --> 00:17:50,760
within one seconds thanks to its fast

416
00:17:50,760 --> 00:17:53,580
Control Plus in comparison both

417
00:17:53,580 --> 00:17:55,740
satellite take several seconds to finish

418
00:17:55,740 --> 00:17:57,179
the connection

419
00:17:57,179 --> 00:17:59,700
note that initially cargo has a lower

420
00:17:59,700 --> 00:18:01,980
throughput than verbs because DCT is

421
00:18:01,980 --> 00:18:04,500
slower nevertheless we apply an

422
00:18:04,500 --> 00:18:06,840
optimization to dynamically switch from

423
00:18:06,840 --> 00:18:09,960
dcqp to RC cube in the background for

424
00:18:09,960 --> 00:18:12,600
the applications and can regain the high

425
00:18:12,600 --> 00:18:15,419
performance which is similar to verbs

426
00:18:15,419 --> 00:18:18,000
notice that light is slower than verbs

427
00:18:18,000 --> 00:18:20,460
and clear core at final time because it

428
00:18:20,460 --> 00:18:22,919
doesn't support a low-leaf RDMA API

429
00:18:22,919 --> 00:18:25,500
which is necessary for applying various

430
00:18:25,500 --> 00:18:28,620
RDMA enabled optimizations exploited by

431
00:18:28,620 --> 00:18:31,039
race

432
00:18:31,140 --> 00:18:33,780
so in summary QR code is a micro second

433
00:18:33,780 --> 00:18:35,820
scale automate control plan for creating

434
00:18:35,820 --> 00:18:38,640
RDMA enabled connections QR code

435
00:18:38,640 --> 00:18:40,980
achieved this by retrofitting DCT with

436
00:18:40,980 --> 00:18:44,179
kernel space automate connection tool

437
00:18:44,179 --> 00:18:47,220
still has a slower data path and thus is

438
00:18:47,220 --> 00:18:49,080
not suitable for all the applications

439
00:18:49,080 --> 00:18:51,440
yet we have shown that it can benefit

440
00:18:51,440 --> 00:18:54,360
existing RDMA enabled applications that

441
00:18:54,360 --> 00:18:56,880
require elasticity

442
00:18:56,880 --> 00:18:59,820
so in conclusion we present KR call a

443
00:18:59,820 --> 00:19:02,340
micro second skill anime control plan on

444
00:19:02,340 --> 00:19:05,220
existing anime hardware and software we

445
00:19:05,220 --> 00:19:07,500
show that QR code can benefit its

446
00:19:07,500 --> 00:19:09,600
existing RDMA elastic applications

447
00:19:09,600 --> 00:19:12,419
including RDMA based disaggregated QR

448
00:19:12,419 --> 00:19:15,179
store we hope QR code can further

449
00:19:15,179 --> 00:19:17,100
benefit more applications for example

450
00:19:17,100 --> 00:19:19,980
RDMA enabled service Computing so thanks

451
00:19:19,980 --> 00:19:22,820
for your listening

