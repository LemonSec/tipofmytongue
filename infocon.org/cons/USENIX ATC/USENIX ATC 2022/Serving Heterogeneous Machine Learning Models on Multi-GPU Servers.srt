1
00:00:13,280 --> 00:00:14,719
first of all i would like to thank the

2
00:00:14,719 --> 00:00:16,400
chair for the excellent introduction

3
00:00:16,400 --> 00:00:17,520
thank you

4
00:00:17,520 --> 00:00:19,680
good afternoon everyone my name is

5
00:00:19,680 --> 00:00:20,960
simbon choi

6
00:00:20,960 --> 00:00:23,279
today i'll be presenting our research

7
00:00:23,279 --> 00:00:24,960
serving heterogeneous machine learning

8
00:00:24,960 --> 00:00:27,279
models on multi-gpu servers with spatial

9
00:00:27,279 --> 00:00:28,640
temporal sharing

10
00:00:28,640 --> 00:00:30,880
this research was conducted with my

11
00:00:30,880 --> 00:00:33,520
colleagues at kaist

12
00:00:33,520 --> 00:00:34,640
nowadays

13
00:00:34,640 --> 00:00:36,559
gpus are widely adopted as machine

14
00:00:36,559 --> 00:00:38,719
learning inference accelerators this is

15
00:00:38,719 --> 00:00:41,200
because gpus accelerate highly parallel

16
00:00:41,200 --> 00:00:43,520
operations and are generally compute

17
00:00:43,520 --> 00:00:45,840
generally accessible computing devices

18
00:00:45,840 --> 00:00:47,440
when accelerating machine learning

19
00:00:47,440 --> 00:00:49,600
inference tests on gpu there are crucial

20
00:00:49,600 --> 00:00:50,960
requirements

21
00:00:50,960 --> 00:00:52,160
first

22
00:00:52,160 --> 00:00:54,160
is that infant queries must be served in

23
00:00:54,160 --> 00:00:56,559
a bounded time often given as a service

24
00:00:56,559 --> 00:00:59,199
level objective second

25
00:00:59,199 --> 00:01:01,520
to improve the utilization of servers

26
00:01:01,520 --> 00:01:03,280
multiple heterogeneous models must be

27
00:01:03,280 --> 00:01:07,199
served together at the same time

28
00:01:07,280 --> 00:01:08,880
there have been a few approaches for

29
00:01:08,880 --> 00:01:10,159
scheduling machine learning inference

30
00:01:10,159 --> 00:01:13,360
tests on gpu the first approach is

31
00:01:13,360 --> 00:01:14,799
batching

32
00:01:14,799 --> 00:01:16,560
patching means merging inputs to a

33
00:01:16,560 --> 00:01:18,799
single large input

34
00:01:18,799 --> 00:01:20,720
is primarily used in systems where you

35
00:01:20,720 --> 00:01:22,880
can afford to wait and form a batch with

36
00:01:22,880 --> 00:01:24,880
inputs waiting in a queue

37
00:01:24,880 --> 00:01:27,200
batching is used because larger inputs

38
00:01:27,200 --> 00:01:28,799
improve throughput and better utilize

39
00:01:28,799 --> 00:01:32,079
the computation resources of gpu

40
00:01:32,079 --> 00:01:34,560
in order to guarantee the slo the time

41
00:01:34,560 --> 00:01:36,479
of waiting to form a batch added to

42
00:01:36,479 --> 00:01:38,560
executing the batch must be within the

43
00:01:38,560 --> 00:01:41,759
ester law this implies that batch size

44
00:01:41,759 --> 00:01:43,680
cannot be infinitely huge because

45
00:01:43,680 --> 00:01:46,079
waiting and execution time increases

46
00:01:46,079 --> 00:01:49,439
with larger batch sizes

47
00:01:50,000 --> 00:01:51,840
another prior approach used for

48
00:01:51,840 --> 00:01:54,240
scheduling inference is time sharing

49
00:01:54,240 --> 00:01:56,079
time sharing is used to serve multiple

50
00:01:56,079 --> 00:01:58,000
models by interleaving execution of

51
00:01:58,000 --> 00:02:00,399
batches for each model

52
00:02:00,399 --> 00:02:02,560
the approach increases utilization by

53
00:02:02,560 --> 00:02:04,880
decreasing the idle time of gpu by

54
00:02:04,880 --> 00:02:07,360
executing badges more frequently

55
00:02:07,360 --> 00:02:10,000
based on the approach published in 2019

56
00:02:10,000 --> 00:02:11,760
waiting and execution patched and

57
00:02:11,760 --> 00:02:13,840
executing batches are done in

58
00:02:13,840 --> 00:02:16,720
consecutive rounds for example when time

59
00:02:16,720 --> 00:02:18,800
sharing two models a and b

60
00:02:18,800 --> 00:02:20,879
the first round is used to wait and form

61
00:02:20,879 --> 00:02:23,760
batches of both models the second round

62
00:02:23,760 --> 00:02:25,120
the batches that were produced in the

63
00:02:25,120 --> 00:02:26,800
first round will be executed in an

64
00:02:26,800 --> 00:02:29,760
interleaving manner

65
00:02:29,840 --> 00:02:31,680
since forming and executing batches of

66
00:02:31,680 --> 00:02:34,160
both models must guarantee slo two

67
00:02:34,160 --> 00:02:37,599
rounds must be within estee lau

68
00:02:37,599 --> 00:02:38,879
unfortunately

69
00:02:38,879 --> 00:02:40,879
there is a problem that remains unsolved

70
00:02:40,879 --> 00:02:43,200
by prior approaches

71
00:02:43,200 --> 00:02:45,360
and the problem is that batching and

72
00:02:45,360 --> 00:02:47,280
time sharing inference tests still

73
00:02:47,280 --> 00:02:51,000
underutilize gpus

74
00:02:51,040 --> 00:02:52,720
in order to analyze

75
00:02:52,720 --> 00:02:54,720
how resources are underutilized we've

76
00:02:54,720 --> 00:02:56,560
measured the latency of google net and

77
00:02:56,560 --> 00:02:58,239
resonant 50 with different amount of

78
00:02:58,239 --> 00:03:00,319
competing resource and various batch

79
00:03:00,319 --> 00:03:01,519
sizes

80
00:03:01,519 --> 00:03:04,000
the latency was measured with batch size

81
00:03:04,000 --> 00:03:06,400
starting from 1 to 32.

82
00:03:06,400 --> 00:03:07,920
for the computing resources the

83
00:03:07,920 --> 00:03:10,080
percentage of stream multiprocessors

84
00:03:10,080 --> 00:03:11,760
available to the model was adjusted

85
00:03:11,760 --> 00:03:14,239
starting from 20 to 100

86
00:03:14,239 --> 00:03:15,599
by looking at the latencies of the

87
00:03:15,599 --> 00:03:18,400
largest batch size which is 32 we can

88
00:03:18,400 --> 00:03:20,400
see a diminishing return beyond 40

89
00:03:20,400 --> 00:03:21,519
percent

90
00:03:21,519 --> 00:03:23,599
the latency decreases significantly when

91
00:03:23,599 --> 00:03:25,920
increasing resources from 20 to 40

92
00:03:25,920 --> 00:03:28,400
percent for googlenet comparing to the

93
00:03:28,400 --> 00:03:31,120
latency when given 100 resource the

94
00:03:31,120 --> 00:03:32,799
latency multiple

95
00:03:32,799 --> 00:03:36,720
decreases from 1.7 to 1.3 and this means

96
00:03:36,720 --> 00:03:38,400
the amount of latency decreased from an

97
00:03:38,400 --> 00:03:39,680
additional 20

98
00:03:39,680 --> 00:03:41,920
is bigger than the addition of 60

99
00:03:41,920 --> 00:03:44,799
beyond the 40 knee point

100
00:03:44,799 --> 00:03:47,519
the same also goes for resin at 50 as it

101
00:03:47,519 --> 00:03:51,760
decreases from 2.2 to 1.4

102
00:03:51,760 --> 00:03:53,920
for smaller batch sizes the knee point

103
00:03:53,920 --> 00:03:55,680
for diminishing return is either the

104
00:03:55,680 --> 00:03:58,000
same or smaller

105
00:03:58,000 --> 00:03:59,360
in conclusion

106
00:03:59,360 --> 00:04:01,280
we can seize the opportunity and improve

107
00:04:01,280 --> 00:04:02,720
performance with better resource

108
00:04:02,720 --> 00:04:05,439
utilization

109
00:04:05,519 --> 00:04:07,840
we'd like to introduce a new opportunity

110
00:04:07,840 --> 00:04:10,000
spatial temporal scheduling

111
00:04:10,000 --> 00:04:12,080
facial temporal scheduling performs

112
00:04:12,080 --> 00:04:14,080
batching and time sharing but

113
00:04:14,080 --> 00:04:15,920
additionally spatial sharing

114
00:04:15,920 --> 00:04:18,160
prior approaches such as batching and

115
00:04:18,160 --> 00:04:20,639
time sharing excuse only one batch at a

116
00:04:20,639 --> 00:04:21,759
time

117
00:04:21,759 --> 00:04:24,000
hence resources are underutilized over

118
00:04:24,000 --> 00:04:24,880
time

119
00:04:24,880 --> 00:04:26,960
however

120
00:04:26,960 --> 00:04:29,040
spatial temporal scheduling additionally

121
00:04:29,040 --> 00:04:31,120
considers patients sharing and execute

122
00:04:31,120 --> 00:04:33,280
multiple badges at the same time

123
00:04:33,280 --> 00:04:34,720
the advantage of spatial temporal

124
00:04:34,720 --> 00:04:37,280
scheduling compared to prior approach is

125
00:04:37,280 --> 00:04:38,880
that it can improve the utilization of

126
00:04:38,880 --> 00:04:40,800
resources per time

127
00:04:40,800 --> 00:04:43,199
this is advantageous to performance

128
00:04:43,199 --> 00:04:45,280
since improved utilization means better

129
00:04:45,280 --> 00:04:47,758
throughput

130
00:04:47,840 --> 00:04:50,000
unfortunately no previous work has

131
00:04:50,000 --> 00:04:52,080
defined an abstract unit of spatial and

132
00:04:52,080 --> 00:04:53,680
temporal resources

133
00:04:53,680 --> 00:04:56,000
therefore we've come up with a new term

134
00:04:56,000 --> 00:04:58,560
gpu land to define a share of spatial

135
00:04:58,560 --> 00:05:00,560
temporal gpu resource

136
00:05:00,560 --> 00:05:03,600
gpu lets patiently share a gpu by

137
00:05:03,600 --> 00:05:06,240
reserve consolidation of resources of

138
00:05:06,240 --> 00:05:07,520
the gpu

139
00:05:07,520 --> 00:05:09,199
and each gpu led

140
00:05:09,199 --> 00:05:11,440
has an associated partition size

141
00:05:11,440 --> 00:05:13,759
partition size represents the amount of

142
00:05:13,759 --> 00:05:15,680
reserve computational resource each

143
00:05:15,680 --> 00:05:17,759
gpu-led has received

144
00:05:17,759 --> 00:05:20,240
gpu let's also temporarily share it the

145
00:05:20,240 --> 00:05:22,560
duration of gpu led is decided by

146
00:05:22,560 --> 00:05:24,800
extended super ship impacting algorithm

147
00:05:24,800 --> 00:05:28,880
introduced in socp 2019

148
00:05:29,120 --> 00:05:31,360
up to now i've introduced the problem

149
00:05:31,360 --> 00:05:33,680
that research has tackled from now on i

150
00:05:33,680 --> 00:05:35,120
would like to focus on the solution

151
00:05:35,120 --> 00:05:36,400
itself

152
00:05:36,400 --> 00:05:38,479
i'll now introduce the overview of our

153
00:05:38,479 --> 00:05:41,199
proposed gpu led scheduling framework

154
00:05:41,199 --> 00:05:43,280
first we have a front-end server that

155
00:05:43,280 --> 00:05:45,600
controls multiple back-end servers

156
00:05:45,600 --> 00:05:47,199
the back-end servers are each

157
00:05:47,199 --> 00:05:49,360
responsible for a physical server with

158
00:05:49,360 --> 00:05:51,680
multiple gpus that are each allocated

159
00:05:51,680 --> 00:05:53,440
with gpu leds

160
00:05:53,440 --> 00:05:55,600
the front-end server maintains a request

161
00:05:55,600 --> 00:05:57,440
queue for each model

162
00:05:57,440 --> 00:05:59,039
for scheduling for each scheduling

163
00:05:59,039 --> 00:06:01,120
period the incoming rate is monitored

164
00:06:01,120 --> 00:06:03,199
and sent to the gpu led scheduler

165
00:06:03,199 --> 00:06:04,960
the scheduler will do special temporal

166
00:06:04,960 --> 00:06:07,360
scheduling with profile information such

167
00:06:07,360 --> 00:06:10,319
as latency per batch size and external

168
00:06:10,319 --> 00:06:12,400
next the results of spatial tempo

169
00:06:12,400 --> 00:06:14,000
scheduling will be sent to each

170
00:06:14,000 --> 00:06:16,000
corresponding back-end server and

171
00:06:16,000 --> 00:06:18,240
finally the back-end server will execute

172
00:06:18,240 --> 00:06:21,280
the results sent by the 4010 server

173
00:06:21,280 --> 00:06:22,319
today

174
00:06:22,319 --> 00:06:24,319
our focus on how gpu scheduler is

175
00:06:24,319 --> 00:06:26,400
designed since it is closely related to

176
00:06:26,400 --> 00:06:29,198
our main idea

177
00:06:29,600 --> 00:06:31,280
let me introduce what the gps scheduler

178
00:06:31,280 --> 00:06:32,880
was designed to achieve

179
00:06:32,880 --> 00:06:35,199
first is cost effective scheduling

180
00:06:35,199 --> 00:06:36,960
the amount of resource is carefully

181
00:06:36,960 --> 00:06:38,880
chosen to yield maximum performance with

182
00:06:38,880 --> 00:06:40,880
minimum resource usage

183
00:06:40,880 --> 00:06:41,840
second

184
00:06:41,840 --> 00:06:43,440
is providing scalable dynamic

185
00:06:43,440 --> 00:06:44,800
reorganization

186
00:06:44,800 --> 00:06:46,560
the scheduler is designed to minimize

187
00:06:46,560 --> 00:06:48,720
the overhead of reorganizing resources

188
00:06:48,720 --> 00:06:50,800
due to a scheduling event

189
00:06:50,800 --> 00:06:52,880
the final is to predict potential

190
00:06:52,880 --> 00:06:55,039
interference when more than two models

191
00:06:55,039 --> 00:06:57,520
execute on shared resources

192
00:06:57,520 --> 00:06:59,919
due to limited time we'll skip how we

193
00:06:59,919 --> 00:07:01,199
predict the interference for this

194
00:07:01,199 --> 00:07:03,440
presentation please refer to the paper

195
00:07:03,440 --> 00:07:06,800
for more details on this part

196
00:07:07,120 --> 00:07:09,120
now i'll explain how the jpl scheduler

197
00:07:09,120 --> 00:07:12,479
was designed to be cost effective

198
00:07:12,479 --> 00:07:14,240
the major challenge of scheduling

199
00:07:14,240 --> 00:07:16,800
gpulitz is a large source space

200
00:07:16,800 --> 00:07:19,280
for example if there are pe ways of

201
00:07:19,280 --> 00:07:21,840
partitioning ngpu's the total case will

202
00:07:21,840 --> 00:07:24,319
be p to the power n cases

203
00:07:24,319 --> 00:07:26,960
and this is not scalable when either p

204
00:07:26,960 --> 00:07:28,880
or n increases

205
00:07:28,880 --> 00:07:30,160
let me eliminate my point with a

206
00:07:30,160 --> 00:07:32,639
graphical example let's say we're going

207
00:07:32,639 --> 00:07:34,880
to schedule three models and you know

208
00:07:34,880 --> 00:07:36,720
each model's request rate

209
00:07:36,720 --> 00:07:38,800
we need to compare all possible cases of

210
00:07:38,800 --> 00:07:41,440
spatial partitioning gpus

211
00:07:41,440 --> 00:07:43,599
for each case we locate a partition to

212
00:07:43,599 --> 00:07:46,240
each gpu led that will serve each model

213
00:07:46,240 --> 00:07:48,160
afterwards we check p to the power n

214
00:07:48,160 --> 00:07:50,160
cases whether the rates are satisfied or

215
00:07:50,160 --> 00:07:53,360
not hence exhaustive searching is costly

216
00:07:53,360 --> 00:07:55,280
and not scalable

217
00:07:55,280 --> 00:07:57,280
our main idea to this problem is the

218
00:07:57,280 --> 00:07:59,280
partition gpus by the minimum required

219
00:07:59,280 --> 00:08:01,599
partitions incrementally instead of

220
00:08:01,599 --> 00:08:04,240
exhaustively trying every possible case

221
00:08:04,240 --> 00:08:05,520
let me explain this concept with a

222
00:08:05,520 --> 00:08:07,199
graphical example

223
00:08:07,199 --> 00:08:09,440
the first step we do is find the

224
00:08:09,440 --> 00:08:11,919
cost-effective partition size of g of

225
00:08:11,919 --> 00:08:15,599
each gpu lid which will serve each model

226
00:08:15,599 --> 00:08:18,400
next we partition each gpu incrementally

227
00:08:18,400 --> 00:08:20,560
by locating gpu lets starting from the

228
00:08:20,560 --> 00:08:23,360
first model if there is an empty gpu the

229
00:08:23,360 --> 00:08:25,440
gpu is partitioned into the exact size

230
00:08:25,440 --> 00:08:27,919
that required by the gpu len

231
00:08:27,919 --> 00:08:31,599
however when there are no empty gpus

232
00:08:31,599 --> 00:08:33,519
but still have partitions to locate we

233
00:08:33,519 --> 00:08:35,200
find the best fit from the remaining

234
00:08:35,200 --> 00:08:37,120
gpus in order to minimize wasted

235
00:08:37,120 --> 00:08:38,320
resources

236
00:08:38,320 --> 00:08:40,080
by incremental splitting the gpus with

237
00:08:40,080 --> 00:08:42,080
minimum partitions we can avoid exhaust

238
00:08:42,080 --> 00:08:44,159
waste searching all possible cases and

239
00:08:44,159 --> 00:08:46,240
yet yield similar scheduling results to

240
00:08:46,240 --> 00:08:49,200
that of exhaustive scheduling

241
00:08:49,200 --> 00:08:50,160
however

242
00:08:50,160 --> 00:08:52,560
there remains one important question

243
00:08:52,560 --> 00:08:55,200
how do we find the number and size of

244
00:08:55,200 --> 00:08:58,480
cost-effective partitions

245
00:08:58,480 --> 00:08:59,839
let me answer the question by first

246
00:08:59,839 --> 00:09:01,680
introducing how to find a cost-effective

247
00:09:01,680 --> 00:09:03,120
partition

248
00:09:03,120 --> 00:09:04,399
finding the most cost-effective

249
00:09:04,399 --> 00:09:06,640
partition means getting the partition

250
00:09:06,640 --> 00:09:08,560
size that shows maximum performance per

251
00:09:08,560 --> 00:09:09,600
resource

252
00:09:09,600 --> 00:09:11,920
the size of cost-effective partition can

253
00:09:11,920 --> 00:09:13,839
be found with profile information by

254
00:09:13,839 --> 00:09:15,519
locating the starting point of dimension

255
00:09:15,519 --> 00:09:16,480
return

256
00:09:16,480 --> 00:09:18,720
the reason why such a dimension return

257
00:09:18,720 --> 00:09:20,640
exists is that the latency of machine

258
00:09:20,640 --> 00:09:22,560
learning inference is not linearly

259
00:09:22,560 --> 00:09:25,440
proportional to the amount of resources

260
00:09:25,440 --> 00:09:26,959
according to the experimental results

261
00:09:26,959 --> 00:09:28,080
mentioned previously in this

262
00:09:28,080 --> 00:09:29,839
presentation

263
00:09:29,839 --> 00:09:31,680
googlenet for example does not show

264
00:09:31,680 --> 00:09:33,600
linear performance

265
00:09:33,600 --> 00:09:36,080
when scheduling we use a more intuitive

266
00:09:36,080 --> 00:09:39,040
metric throughput

267
00:09:39,040 --> 00:09:40,959
in order to schedule wave throughput we

268
00:09:40,959 --> 00:09:43,519
profile the maximum throughput

269
00:09:43,519 --> 00:09:44,800
which can be achieved for each

270
00:09:44,800 --> 00:09:46,720
percentage of resource

271
00:09:46,720 --> 00:09:48,880
when profiling each partition size the

272
00:09:48,880 --> 00:09:50,880
first step is to get the maximum batch

273
00:09:50,880 --> 00:09:52,640
size where the latency does not exceed

274
00:09:52,640 --> 00:09:54,800
the slo

275
00:09:54,800 --> 00:09:56,560
the next step is calculating the

276
00:09:56,560 --> 00:09:58,000
throughput by dividing the number of

277
00:09:58,000 --> 00:09:59,839
batches by the latency required to

278
00:09:59,839 --> 00:10:01,839
execute the batch

279
00:10:01,839 --> 00:10:03,519
after going through all the steps

280
00:10:03,519 --> 00:10:05,360
estimating throughput for each partition

281
00:10:05,360 --> 00:10:07,600
size is complete

282
00:10:07,600 --> 00:10:09,360
as we compare the throughput between

283
00:10:09,360 --> 00:10:10,720
resource percentage we can see a

284
00:10:10,720 --> 00:10:12,160
dimension return

285
00:10:12,160 --> 00:10:14,399
the performance increase between 40 and

286
00:10:14,399 --> 00:10:15,360
60

287
00:10:15,360 --> 00:10:17,360
is not as great as the increase between

288
00:10:17,360 --> 00:10:19,279
20 and 40

289
00:10:19,279 --> 00:10:21,920
hence 40 becomes the cost-effective

290
00:10:21,920 --> 00:10:24,719
partition size

291
00:10:25,279 --> 00:10:27,600
next let me explain how partitions are

292
00:10:27,600 --> 00:10:29,360
located to input rates

293
00:10:29,360 --> 00:10:31,519
multiple partitions are required when a

294
00:10:31,519 --> 00:10:33,440
single partition cannot provide enough

295
00:10:33,440 --> 00:10:35,760
throughput for a given request grade

296
00:10:35,760 --> 00:10:38,000
when allocating partitions to a model

297
00:10:38,000 --> 00:10:39,839
the scheduler follows two rules to

298
00:10:39,839 --> 00:10:41,920
ensure minimum sum of partitions

299
00:10:41,920 --> 00:10:44,240
number one is choosing as many cost

300
00:10:44,240 --> 00:10:46,320
effective partitions and possible and

301
00:10:46,320 --> 00:10:48,160
number two is choosing the minimum

302
00:10:48,160 --> 00:10:49,760
partition if there are any rates

303
00:10:49,760 --> 00:10:51,920
remaining for example

304
00:10:51,920 --> 00:10:54,079
let's say a model's request rate has an

305
00:10:54,079 --> 00:10:56,720
incoming rate of 900 requests per second

306
00:10:56,720 --> 00:10:58,800
and we have the profiling info or

307
00:10:58,800 --> 00:11:00,640
throughput partition size

308
00:11:00,640 --> 00:11:02,720
also let's assume the most cost

309
00:11:02,720 --> 00:11:05,040
effective partition size is 40 for this

310
00:11:05,040 --> 00:11:06,079
model

311
00:11:06,079 --> 00:11:08,320
according to the profile 40

312
00:11:08,320 --> 00:11:11,680
can achieve 400 rps and 20 can support

313
00:11:11,680 --> 00:11:12,839
up to 100

314
00:11:12,839 --> 00:11:16,320
rps following the first rule we pick as

315
00:11:16,320 --> 00:11:17,920
many cost-effective partitions as

316
00:11:17,920 --> 00:11:19,760
possible so that the sum of throughput

317
00:11:19,760 --> 00:11:23,360
does not exceed the given rate

318
00:11:23,519 --> 00:11:25,920
however the sum of throughput is still

319
00:11:25,920 --> 00:11:29,600
100 rp as a shy so the schedule follows

320
00:11:29,600 --> 00:11:31,839
rule number two and consults the table

321
00:11:31,839 --> 00:11:33,600
to find the minimum partition size that

322
00:11:33,600 --> 00:11:36,079
is big enough for the remaining rate

323
00:11:36,079 --> 00:11:39,120
20 is added and a total of 900 rps

324
00:11:39,120 --> 00:11:42,600
becomes achievable

325
00:11:42,800 --> 00:11:45,040
next i'll introduce how we've minimized

326
00:11:45,040 --> 00:11:46,800
the overhead of dynamically organized

327
00:11:46,800 --> 00:11:50,719
resources when a scheduling event occurs

328
00:11:50,800 --> 00:11:52,880
let me explain the scheduling events of

329
00:11:52,880 --> 00:11:54,399
reorganization

330
00:11:54,399 --> 00:11:56,959
the gps scheduler monitors each model's

331
00:11:56,959 --> 00:12:00,000
request rate periodically

332
00:12:00,000 --> 00:12:01,760
when models show a sudden change of

333
00:12:01,760 --> 00:12:04,000
request rates the gpu led scheduler will

334
00:12:04,000 --> 00:12:05,680
detect this change and decide whether

335
00:12:05,680 --> 00:12:09,519
our new size of gpu is necessary if so

336
00:12:09,519 --> 00:12:11,760
the size of gpu lets running on the gpu

337
00:12:11,760 --> 00:12:15,200
will be reorganized to the new sizes

338
00:12:15,200 --> 00:12:17,680
however there is a major challenge in

339
00:12:17,680 --> 00:12:20,000
dynamically reorganizing partitions

340
00:12:20,000 --> 00:12:21,920
the challenge is that preparing a new

341
00:12:21,920 --> 00:12:24,399
partition has a large overhead the

342
00:12:24,399 --> 00:12:26,240
overhead includes loading kernels used

343
00:12:26,240 --> 00:12:27,519
by the underlying machine learning

344
00:12:27,519 --> 00:12:29,600
framework and warming up models served

345
00:12:29,600 --> 00:12:31,519
by the new gpu line

346
00:12:31,519 --> 00:12:33,839
so our scheduling framework hides the

347
00:12:33,839 --> 00:12:35,839
overhead by preparing gpus in the

348
00:12:35,839 --> 00:12:38,880
background in other words shadowing

349
00:12:38,880 --> 00:12:40,880
there are two types of context when

350
00:12:40,880 --> 00:12:43,120
reorganizing one is the active context

351
00:12:43,120 --> 00:12:44,880
serving request and the other is the

352
00:12:44,880 --> 00:12:46,800
shadow context which is responsible for

353
00:12:46,800 --> 00:12:49,760
preparing gpu let's in the background

354
00:12:49,760 --> 00:12:52,880
let's say 2g pulitz a and b were being

355
00:12:52,880 --> 00:12:55,920
used to serve in an active context

356
00:12:55,920 --> 00:12:57,920
due to an event when the scheduler

357
00:12:57,920 --> 00:12:59,519
concluded a and b needs to be

358
00:12:59,519 --> 00:13:02,639
reorganized into a prime and b prime

359
00:13:02,639 --> 00:13:04,399
the scheduler creates a shadow context

360
00:13:04,399 --> 00:13:07,279
preparing a prime and b prime

361
00:13:07,279 --> 00:13:09,440
after the preparations are ready the

362
00:13:09,440 --> 00:13:11,920
active context is removed and the shadow

363
00:13:11,920 --> 00:13:14,240
context becomes a new active context as

364
00:13:14,240 --> 00:13:17,519
it starts serving requests

365
00:13:18,000 --> 00:13:20,399
finally let me introduce the evaluation

366
00:13:20,399 --> 00:13:21,680
results

367
00:13:21,680 --> 00:13:23,040
we have used two multi-modal

368
00:13:23,040 --> 00:13:25,360
applications and five custom multi-modal

369
00:13:25,360 --> 00:13:27,440
scenarios for benchmarks the two

370
00:13:27,440 --> 00:13:29,519
multi-modal applications are game and

371
00:13:29,519 --> 00:13:30,720
traffic

372
00:13:30,720 --> 00:13:32,320
game is an application for image and

373
00:13:32,320 --> 00:13:34,240
digit recognition of a screen and

374
00:13:34,240 --> 00:13:36,560
traffic is used for camera footage

375
00:13:36,560 --> 00:13:37,760
analysis

376
00:13:37,760 --> 00:13:40,000
game is composed of seven models and

377
00:13:40,000 --> 00:13:41,680
traffic consists of three models as

378
00:13:41,680 --> 00:13:43,760
depicted in the figure on the right

379
00:13:43,760 --> 00:13:46,079
for multi-modal scenarios we've tried to

380
00:13:46,079 --> 00:13:47,839
diversify the composition of models

381
00:13:47,839 --> 00:13:49,839
according to memory footprint size

382
00:13:49,839 --> 00:13:51,600
further details of each benchmark and

383
00:13:51,600 --> 00:13:55,200
slo are available in the paper

384
00:13:55,279 --> 00:13:56,880
we have deployed our scheduling

385
00:13:56,880 --> 00:13:59,199
framework on multi-gpu servers each with

386
00:13:59,199 --> 00:14:01,600
two gpus the servers were connected to

387
00:14:01,600 --> 00:14:04,000
10g ethernet network and the main metric

388
00:14:04,000 --> 00:14:06,720
we use to validate our framework is slo

389
00:14:06,720 --> 00:14:08,959
preserved throughput which is the

390
00:14:08,959 --> 00:14:10,880
maximum throughput when slow evaluation

391
00:14:10,880 --> 00:14:12,880
rate is below 1

392
00:14:12,880 --> 00:14:15,120
slow evaluation rate is the percentage

393
00:14:15,120 --> 00:14:18,320
of violated requests over total requests

394
00:14:18,320 --> 00:14:19,680
we have used four schedules for

395
00:14:19,680 --> 00:14:22,399
evaluation the first is time share which

396
00:14:22,399 --> 00:14:24,480
supports only time sharing among tests

397
00:14:24,480 --> 00:14:26,639
and the next one is space share

398
00:14:26,639 --> 00:14:28,240
it does not time share but provide

399
00:14:28,240 --> 00:14:30,160
spatial sharing by only allocating best

400
00:14:30,160 --> 00:14:32,320
fit partitions in a greedy way

401
00:14:32,320 --> 00:14:35,199
the third is gpulit which does both time

402
00:14:35,199 --> 00:14:36,720
and spatial sharing

403
00:14:36,720 --> 00:14:38,800
but does not consider interference the

404
00:14:38,800 --> 00:14:41,040
last is gpu that plus int which

405
00:14:41,040 --> 00:14:44,560
considers all aspects for scheduling

406
00:14:44,560 --> 00:14:46,880
first let me introduce how our scheduler

407
00:14:46,880 --> 00:14:48,639
successfully increased slo preserve

408
00:14:48,639 --> 00:14:50,959
throughput the graph shows the maximum

409
00:14:50,959 --> 00:14:52,800
throughput each schedule can achieve

410
00:14:52,800 --> 00:14:56,720
without violating slo for each benchmark

411
00:14:56,720 --> 00:14:58,720
for every benchmark the scheduler with

412
00:14:58,720 --> 00:15:00,240
both time and spatial sharing

413
00:15:00,240 --> 00:15:01,760
outperformed the schedule relying on

414
00:15:01,760 --> 00:15:03,279
either one only

415
00:15:03,279 --> 00:15:05,839
comparing gpu led plus end to time share

416
00:15:05,839 --> 00:15:08,399
the throughput increased by 61.7 percent

417
00:15:08,399 --> 00:15:09,760
on average

418
00:15:09,760 --> 00:15:11,600
additionally considering interference

419
00:15:11,600 --> 00:15:14,320
increases throughput by 7.5 percent this

420
00:15:14,320 --> 00:15:16,160
is because considering interference

421
00:15:16,160 --> 00:15:17,600
yields more accurate results when

422
00:15:17,600 --> 00:15:20,079
scheduling

423
00:15:20,079 --> 00:15:22,560
next we've experimented how well our

424
00:15:22,560 --> 00:15:25,040
scheduler can scale the number of gpus

425
00:15:25,040 --> 00:15:26,880
for fluctuating workloads

426
00:15:26,880 --> 00:15:29,199
we've extended our environment to 8 gpus

427
00:15:29,199 --> 00:15:30,720
for this experiment

428
00:15:30,720 --> 00:15:32,240
three models were used each having a

429
00:15:32,240 --> 00:15:35,839
unique pattern of rates for 3200 seconds

430
00:15:35,839 --> 00:15:37,199
the graph on the top reports the

431
00:15:37,199 --> 00:15:39,120
throughput over the 3200 second

432
00:15:39,120 --> 00:15:40,480
experiment

433
00:15:40,480 --> 00:15:42,399
and the middle reports the number of

434
00:15:42,399 --> 00:15:45,199
gpus and the last one presents the slo

435
00:15:45,199 --> 00:15:47,360
violation

436
00:15:47,360 --> 00:15:49,279
as you can see our scheduler has

437
00:15:49,279 --> 00:15:51,600
successfully adjusted the number of gpus

438
00:15:51,600 --> 00:15:53,839
and maintain a slow violation rate under

439
00:15:53,839 --> 00:15:55,680
one percent

440
00:15:55,680 --> 00:15:57,120
there are more experimental results that

441
00:15:57,120 --> 00:15:59,440
are not included in the presentation

442
00:15:59,440 --> 00:16:01,040
please refer to our paper for more

443
00:16:01,040 --> 00:16:03,519
results

444
00:16:03,519 --> 00:16:05,279
in conclusion we've enhanced the

445
00:16:05,279 --> 00:16:06,480
performance of machine learning

446
00:16:06,480 --> 00:16:08,399
inference for gpus with spatial temporal

447
00:16:08,399 --> 00:16:09,519
scheduling

448
00:16:09,519 --> 00:16:11,040
the spatial temporal scheduler was

449
00:16:11,040 --> 00:16:12,800
further enhanced by minimizing wasted

450
00:16:12,800 --> 00:16:15,199
resource scaling resource efficiently

451
00:16:15,199 --> 00:16:17,600
and predicting interference effects

452
00:16:17,600 --> 00:16:19,279
experimental results show that our

453
00:16:19,279 --> 00:16:20,639
scheduler outperforms the temporal

454
00:16:20,639 --> 00:16:22,959
sharing schedule throughput by 61.7

455
00:16:22,959 --> 00:16:24,079
percent

456
00:16:24,079 --> 00:16:25,600
if you have any questions i'll be more

457
00:16:25,600 --> 00:16:27,279
than happy to answer them thank you for

458
00:16:27,279 --> 00:16:30,759
listening to my speech

459
00:16:37,920 --> 00:16:40,000
you

