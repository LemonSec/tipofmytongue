1
00:00:14,000 --> 00:00:16,880
hello everyone i'm jojo from picking

2
00:00:16,880 --> 00:00:20,400
university china i'm glad to present our

3
00:00:20,400 --> 00:00:24,160
paper called pts a unified framework for

4
00:00:24,160 --> 00:00:27,279
parameter efficient transformer serving

5
00:00:27,279 --> 00:00:29,519
as we all know since the transformer

6
00:00:29,519 --> 00:00:32,159
models have been proposed by google in

7
00:00:32,159 --> 00:00:35,440
2017 we are stepping into a transformers

8
00:00:35,440 --> 00:00:36,399
era

9
00:00:36,399 --> 00:00:38,239
transformers have other performed

10
00:00:38,239 --> 00:00:41,120
traditional islands in various natural

11
00:00:41,120 --> 00:00:44,120
language processing texts and even other

12
00:00:44,120 --> 00:00:47,440
performances on computer vision tasks in

13
00:00:47,440 --> 00:00:49,600
recent years

14
00:00:49,600 --> 00:00:52,239
unlike traditional islands transformers

15
00:00:52,239 --> 00:00:54,160
adopt a pre-translation fine-tuning

16
00:00:54,160 --> 00:00:55,199
paradigm

17
00:00:55,199 --> 00:00:57,440
some big companies first person 300

18
00:00:57,440 --> 00:01:00,160
transformer models like birth and gpt

19
00:01:00,160 --> 00:01:02,480
with large-scale datasets the

20
00:01:02,480 --> 00:01:04,959
unsupervised pre-training usually lasts

21
00:01:04,959 --> 00:01:08,159
for days two months even trend on tpu or

22
00:01:08,159 --> 00:01:10,080
gpu clusters

23
00:01:10,080 --> 00:01:12,320
such pre-trained models with rich task

24
00:01:12,320 --> 00:01:14,960
diagnose technology are provided to

25
00:01:14,960 --> 00:01:17,119
application developers who then

26
00:01:17,119 --> 00:01:19,040
fine-tune the pre-trained models on

27
00:01:19,040 --> 00:01:22,159
their private statuses on gpu servers

28
00:01:22,159 --> 00:01:24,560
the fine-tuned models are finally

29
00:01:24,560 --> 00:01:27,119
deployed to cloud or id servers to

30
00:01:27,119 --> 00:01:31,119
process different input queries

31
00:01:31,200 --> 00:01:33,280
with the practical fine tuning based

32
00:01:33,280 --> 00:01:36,560
workflow more than 26 ordinal models for

33
00:01:36,560 --> 00:01:38,560
different downstream tasks have been

34
00:01:38,560 --> 00:01:41,119
shared online however

35
00:01:41,119 --> 00:01:43,439
since this task generates a full motor

36
00:01:43,439 --> 00:01:44,479
copy

37
00:01:44,479 --> 00:01:46,720
when deploying multiple tasks at a

38
00:01:46,720 --> 00:01:48,799
server the storage and the memory

39
00:01:48,799 --> 00:01:51,200
footprints increased linearly

40
00:01:51,200 --> 00:01:53,600
given the huge number of parameters in

41
00:01:53,600 --> 00:01:56,720
transformer models it results in a post

42
00:01:56,720 --> 00:01:58,960
calibrity issue in the multi-task

43
00:01:58,960 --> 00:02:01,600
transformer serving scenarios

44
00:02:01,600 --> 00:02:03,680
to mitigate this problem

45
00:02:03,680 --> 00:02:05,920
using parameter efficient transformers

46
00:02:05,920 --> 00:02:08,800
have become a popular trend unlike the

47
00:02:08,800 --> 00:02:10,878
full model fine tuning paradigm which

48
00:02:10,878 --> 00:02:13,280
trends on all the parameters

49
00:02:13,280 --> 00:02:15,520
parameter efficient fine tuning as

50
00:02:15,520 --> 00:02:18,239
neutral modules wire keeps the

51
00:02:18,239 --> 00:02:20,319
pre-trained waves freezed

52
00:02:20,319 --> 00:02:22,239
all fine tunes are subset of the

53
00:02:22,239 --> 00:02:24,239
pre-trained parameters

54
00:02:24,239 --> 00:02:26,560
or remove a subset of the pre-trained

55
00:02:26,560 --> 00:02:29,599
parameters by setting them to zero these

56
00:02:29,599 --> 00:02:32,239
methods can achieve comparable or even

57
00:02:32,239 --> 00:02:34,000
higher accuracy than full mode

58
00:02:34,000 --> 00:02:36,640
fine-tuning but have much lower storage

59
00:02:36,640 --> 00:02:38,879
overhead since we only have to keep a

60
00:02:38,879 --> 00:02:41,760
small portion of pet parameters for each

61
00:02:41,760 --> 00:02:42,640
task

62
00:02:42,640 --> 00:02:44,480
and the pressure and model is shared

63
00:02:44,480 --> 00:02:46,239
among tasks

64
00:02:46,239 --> 00:02:47,280
however

65
00:02:47,280 --> 00:02:50,160
we notice that it is inefficient to

66
00:02:50,160 --> 00:02:53,200
deploy petty models using existing

67
00:02:53,200 --> 00:02:55,920
transformer serving frameworks mainly

68
00:02:55,920 --> 00:02:58,319
due to the following challenges

69
00:02:58,319 --> 00:03:01,040
firstly the frameworks should support

70
00:03:01,040 --> 00:03:04,560
various pd's sensor downstream tags my

71
00:03:04,560 --> 00:03:07,280
favorite different pt algorithms and the

72
00:03:07,280 --> 00:03:10,000
application developers may usually

73
00:03:10,000 --> 00:03:13,360
choose their best pd's for their tasks

74
00:03:13,360 --> 00:03:16,480
secondly current gpu-based serving

75
00:03:16,480 --> 00:03:18,800
frameworks like turbo transformers and

76
00:03:18,800 --> 00:03:22,480
feel sick fading into four model copies

77
00:03:22,480 --> 00:03:25,599
one still has to merge the pt parameters

78
00:03:25,599 --> 00:03:28,159
into the pre-trained model to generate

79
00:03:28,159 --> 00:03:29,840
four model copies

80
00:03:29,840 --> 00:03:33,040
which still incurs huge gpu memory

81
00:03:33,040 --> 00:03:34,720
footprint

82
00:03:34,720 --> 00:03:35,840
thirdly

83
00:03:35,840 --> 00:03:38,159
the queries of different tasks can

84
00:03:38,159 --> 00:03:40,720
hardly be backed due to both the model

85
00:03:40,720 --> 00:03:43,599
parameter and algorithm representation

86
00:03:43,599 --> 00:03:44,799
differences

87
00:03:44,799 --> 00:03:47,519
how to improve the system throughput is

88
00:03:47,519 --> 00:03:49,680
a challenging problem

89
00:03:49,680 --> 00:03:52,319
to tackle these challenges we first

90
00:03:52,319 --> 00:03:55,040
proposed a unified representation of

91
00:03:55,040 --> 00:03:56,799
purity algorithms

92
00:03:56,799 --> 00:03:58,879
take four steps of the arts pt

93
00:03:58,879 --> 00:04:01,120
algorithms as an example

94
00:04:01,120 --> 00:04:03,280
although they have different algorithm

95
00:04:03,280 --> 00:04:04,799
representations

96
00:04:04,799 --> 00:04:07,280
we can use equal transformation to

97
00:04:07,280 --> 00:04:10,319
decouple them into shared operations and

98
00:04:10,319 --> 00:04:14,679
pt specific operations

99
00:04:14,720 --> 00:04:16,959
the shared operations compute with

100
00:04:16,959 --> 00:04:19,358
different inputs but using the same

101
00:04:19,358 --> 00:04:21,120
patron model ways

102
00:04:21,120 --> 00:04:23,680
so they can easily be passed together

103
00:04:23,680 --> 00:04:26,479
regardless of the invoked tasks of the

104
00:04:26,479 --> 00:04:28,880
queries

105
00:04:28,880 --> 00:04:32,160
the task-specific pet operations compute

106
00:04:32,160 --> 00:04:35,040
with different pt ways which cannot be

107
00:04:35,040 --> 00:04:36,479
part together

108
00:04:36,479 --> 00:04:38,960
however they can still be processed

109
00:04:38,960 --> 00:04:40,840
efficiently with light weighted

110
00:04:40,840 --> 00:04:44,400
operators for example the decoupled pt

111
00:04:44,400 --> 00:04:47,120
parameters of mask birth and deep

112
00:04:47,120 --> 00:04:49,759
pruning usually have high spasticity

113
00:04:49,759 --> 00:04:52,960
which can be processed by sparse kernels

114
00:04:52,960 --> 00:04:55,199
the neutral modules used by the

115
00:04:55,199 --> 00:04:57,680
adapter's models required desk

116
00:04:57,680 --> 00:04:58,960
operations

117
00:04:58,960 --> 00:05:01,520
but also have limited computation thanks

118
00:05:01,520 --> 00:05:03,919
to their bottleneck structure

119
00:05:03,919 --> 00:05:06,720
based on the unified representation we

120
00:05:06,720 --> 00:05:09,840
present the pts serving framework to

121
00:05:09,840 --> 00:05:12,560
support the management and the serving

122
00:05:12,560 --> 00:05:15,440
of multiple pt tasks here is the

123
00:05:15,440 --> 00:05:16,560
overview

124
00:05:16,560 --> 00:05:19,759
to deploy services users should first

125
00:05:19,759 --> 00:05:22,400
register their tasks by providing the

126
00:05:22,400 --> 00:05:24,720
tag operation model

127
00:05:24,720 --> 00:05:28,160
compress the pt parameters and the types

128
00:05:28,160 --> 00:05:29,919
of pts

129
00:05:29,919 --> 00:05:30,720
then

130
00:05:30,720 --> 00:05:33,600
the task manager module or pts will

131
00:05:33,600 --> 00:05:36,560
register the uploaded pt tasks

132
00:05:36,560 --> 00:05:39,600
assign a unique id for each task and

133
00:05:39,600 --> 00:05:42,160
store their pdt parameters to a

134
00:05:42,160 --> 00:05:44,400
parameter repository

135
00:05:44,400 --> 00:05:47,759
after task registration pts does serving

136
00:05:47,759 --> 00:05:48,880
queries

137
00:05:48,880 --> 00:05:52,160
each input query carries the unique task

138
00:05:52,160 --> 00:05:55,120
id and the input data

139
00:05:55,120 --> 00:05:57,840
the inputs are processed by the pt

140
00:05:57,840 --> 00:06:01,120
influence pipeline which pre-processes

141
00:06:01,120 --> 00:06:04,080
inputs schedules the inputs in batches

142
00:06:04,080 --> 00:06:06,479
and computes the batches with a pet

143
00:06:06,479 --> 00:06:08,800
influence engine

144
00:06:08,800 --> 00:06:11,120
here presents the python-based user

145
00:06:11,120 --> 00:06:13,280
interface of pts

146
00:06:13,280 --> 00:06:16,000
we first create a server and register

147
00:06:16,000 --> 00:06:18,560
several pt tasks

148
00:06:18,560 --> 00:06:20,800
and then load the pre-trained model and

149
00:06:20,800 --> 00:06:24,560
the pt tasks into the gpu memory finally

150
00:06:24,560 --> 00:06:27,360
the server can fetch queries and compute

151
00:06:27,360 --> 00:06:29,919
the results

152
00:06:29,919 --> 00:06:33,120
to improve the service throughput of pts

153
00:06:33,120 --> 00:06:35,840
framework we proposed two optimization

154
00:06:35,840 --> 00:06:38,160
strategies to deal with two remaining

155
00:06:38,160 --> 00:06:41,039
challenges first the input queries

156
00:06:41,039 --> 00:06:43,360
usually have variable lengths and the

157
00:06:43,360 --> 00:06:45,600
invoked pt operators

158
00:06:45,600 --> 00:06:47,199
are also different

159
00:06:47,199 --> 00:06:50,080
how to split the queries into batches is

160
00:06:50,080 --> 00:06:51,759
a challenging problem

161
00:06:51,759 --> 00:06:54,080
we propose a coordinated batch

162
00:06:54,080 --> 00:06:57,120
scheduling which jointly considers the

163
00:06:57,120 --> 00:07:00,560
shared operators and the pt operators

164
00:07:00,560 --> 00:07:02,639
during batch scheduling

165
00:07:02,639 --> 00:07:05,440
second since we can only batch the pt

166
00:07:05,440 --> 00:07:08,560
operators of the same task pt operators

167
00:07:08,560 --> 00:07:11,199
of different tasks still execute

168
00:07:11,199 --> 00:07:14,639
sequentially we propose a pt operator

169
00:07:14,639 --> 00:07:17,199
scheduling strategy to put them into

170
00:07:17,199 --> 00:07:20,240
multiple cuda streams to improve the

171
00:07:20,240 --> 00:07:23,120
throughput further as illustrated in

172
00:07:23,120 --> 00:07:25,440
real scenarios the input carriers

173
00:07:25,440 --> 00:07:28,000
usually have variable sequence length if

174
00:07:28,000 --> 00:07:30,639
we batch short queries with long queries

175
00:07:30,639 --> 00:07:33,520
the short ones have to be zero padded

176
00:07:33,520 --> 00:07:37,039
incurring useless computation

177
00:07:37,039 --> 00:07:39,039
previous frameworks like turbo

178
00:07:39,039 --> 00:07:41,360
transformers pay much attention to some

179
00:07:41,360 --> 00:07:44,960
such a problem however for pts we have

180
00:07:44,960 --> 00:07:47,520
to consider both the shared operations

181
00:07:47,520 --> 00:07:50,960
and the task-specific pt operations

182
00:07:50,960 --> 00:07:53,840
therefore we propose a new strategy to

183
00:07:53,840 --> 00:07:56,840
coordinate this two-part during

184
00:07:56,840 --> 00:07:59,599
batching we first formulate the total

185
00:07:59,599 --> 00:08:02,879
latency of a patch we use alpha and ili

186
00:08:02,879 --> 00:08:05,520
to denote the shared model latency when

187
00:08:05,520 --> 00:08:07,440
batching unqueries with the maximum

188
00:08:07,440 --> 00:08:10,080
length of l in the ice batch

189
00:08:10,080 --> 00:08:12,160
alpha is the performance model of the

190
00:08:12,160 --> 00:08:13,919
sheldon operators

191
00:08:13,919 --> 00:08:16,160
we use a beta model to estimate the

192
00:08:16,160 --> 00:08:18,639
latency of pt operators

193
00:08:18,639 --> 00:08:23,039
where ptid indexes the pt type of the js

194
00:08:23,039 --> 00:08:26,639
task in the ice batch and the lowercase

195
00:08:26,639 --> 00:08:30,080
and l denotes the number of queries and

196
00:08:30,080 --> 00:08:32,559
the maximum length of the queries to the

197
00:08:32,559 --> 00:08:33,519
task

198
00:08:33,519 --> 00:08:36,399
our coordinated batching has two steps

199
00:08:36,399 --> 00:08:38,559
in the first step we generate mini

200
00:08:38,559 --> 00:08:40,479
batches for each task

201
00:08:40,479 --> 00:08:42,719
which only considers the effect of

202
00:08:42,719 --> 00:08:45,760
batching pt operators using a profiled

203
00:08:45,760 --> 00:08:47,120
beta model

204
00:08:47,120 --> 00:08:50,560
specifically we first source the queries

205
00:08:50,560 --> 00:08:53,279
of each task and then split

206
00:08:53,279 --> 00:08:55,760
them too many batches using the beta

207
00:08:55,760 --> 00:08:58,000
model and a dynamic programming

208
00:08:58,000 --> 00:08:59,120
algorithm

209
00:08:59,120 --> 00:09:01,680
in the second step we generate macro

210
00:09:01,680 --> 00:09:03,920
batches by combining these mini batches

211
00:09:03,920 --> 00:09:06,399
among tasks which only consider the

212
00:09:06,399 --> 00:09:09,040
effect of batching the shared operators

213
00:09:09,040 --> 00:09:11,440
using a profile other model

214
00:09:11,440 --> 00:09:13,600
with all the mini batches according to

215
00:09:13,600 --> 00:09:14,800
their lens

216
00:09:14,800 --> 00:09:17,200
they also adopt a dynamic programming

217
00:09:17,200 --> 00:09:19,920
organism to find the optimal splitting

218
00:09:19,920 --> 00:09:22,880
positions with low time complexity

219
00:09:22,880 --> 00:09:24,959
please refer to our paper for more

220
00:09:24,959 --> 00:09:26,959
detailed descriptions about the

221
00:09:26,959 --> 00:09:28,959
algorithms

222
00:09:28,959 --> 00:09:31,040
in addition to the coordinated budget

223
00:09:31,040 --> 00:09:34,800
scheduling pdt operators can be executed

224
00:09:34,800 --> 00:09:37,040
in parallel to further improve the

225
00:09:37,040 --> 00:09:38,880
hardware utilization

226
00:09:38,880 --> 00:09:41,839
to achieve the pt task level parallelism

227
00:09:41,839 --> 00:09:43,200
on a gpu

228
00:09:43,200 --> 00:09:46,000
pt operators in a macro batch can be

229
00:09:46,000 --> 00:09:49,040
assigned to multiple cuda streams

230
00:09:49,040 --> 00:09:51,920
we use the operational intensity of pt

231
00:09:51,920 --> 00:09:55,680
operators as the metric and ensure that

232
00:09:55,680 --> 00:09:58,240
pd operators with different operational

233
00:09:58,240 --> 00:10:00,880
intensity are assigned to different

234
00:10:00,880 --> 00:10:02,240
product streams

235
00:10:02,240 --> 00:10:05,120
so that both the gpu memory bandwidth

236
00:10:05,120 --> 00:10:08,640
and the computation can be well utilized

237
00:10:08,640 --> 00:10:12,080
we implement pts with a python front end

238
00:10:12,080 --> 00:10:15,120
to describe shared models and pd task

239
00:10:15,120 --> 00:10:16,320
management

240
00:10:16,320 --> 00:10:18,880
and a cpp backend based on the open

241
00:10:18,880 --> 00:10:21,680
source turbo transformers framework to

242
00:10:21,680 --> 00:10:24,079
perform query scheduling and inference

243
00:10:24,079 --> 00:10:25,279
serving

244
00:10:25,279 --> 00:10:29,120
we evaluate pts on edge desktop and

245
00:10:29,120 --> 00:10:32,000
server gpu platforms the edge platform

246
00:10:32,000 --> 00:10:35,120
is jetson tx2 which has 8 gigabytes of

247
00:10:35,120 --> 00:10:36,880
shared memory

248
00:10:36,880 --> 00:10:40,399
the desktop platform equips gtx 1080

249
00:10:40,399 --> 00:10:41,200
time

250
00:10:41,200 --> 00:10:44,800
which has 11 gigabytes of memory we use

251
00:10:44,800 --> 00:10:49,760
a tesla v100 gpu with 22 gigabytes of

252
00:10:49,760 --> 00:10:53,519
gpu memory as the server platform

253
00:10:53,519 --> 00:10:56,000
we choose bird-based but large and

254
00:10:56,000 --> 00:10:58,240
digital birds as the shared pre-trained

255
00:10:58,240 --> 00:11:01,680
models and adopt four aforementioned pt

256
00:11:01,680 --> 00:11:05,519
algorithms to generate the pt tasks

257
00:11:05,519 --> 00:11:08,160
we first demonstrate the scalability of

258
00:11:08,160 --> 00:11:11,920
tts by comparing the maximum number of

259
00:11:11,920 --> 00:11:14,399
supported tasks with the conventional

260
00:11:14,399 --> 00:11:18,000
sequential solving systems scqs in short

261
00:11:18,000 --> 00:11:21,360
scqs lost full motor copy for each task

262
00:11:21,360 --> 00:11:24,720
while pts works on a single shared model

263
00:11:24,720 --> 00:11:27,120
we keep loading tasks and evaluate

264
00:11:27,120 --> 00:11:29,839
whether they can process a batch of 22

265
00:11:29,839 --> 00:11:33,200
random queries until out of memory as we

266
00:11:33,200 --> 00:11:34,240
can see

267
00:11:34,240 --> 00:11:38,320
compared to conventional sql systems pts

268
00:11:38,320 --> 00:11:41,120
supports four times to 26 times more

269
00:11:41,120 --> 00:11:42,880
concurrent tasks

270
00:11:42,880 --> 00:11:45,200
which can substantially serve the higher

271
00:11:45,200 --> 00:11:47,360
cost when deploying multiple

272
00:11:47,360 --> 00:11:51,040
transformer-based applications

273
00:11:51,600 --> 00:11:54,800
to understand why pts has outstanding

274
00:11:54,800 --> 00:11:57,920
scalability we also provided the memory

275
00:11:57,920 --> 00:12:01,680
footprint of both pds and scqs on the

276
00:12:01,680 --> 00:12:05,120
desktop platform as shown in the figures

277
00:12:05,120 --> 00:12:08,160
we plotted the consumed gpu memory by

278
00:12:08,160 --> 00:12:10,720
both memory waste and feature data on

279
00:12:10,720 --> 00:12:13,360
the different number of tasks as we can

280
00:12:13,360 --> 00:12:16,560
see in the left figure adding a new task

281
00:12:16,560 --> 00:12:19,120
only consumes less than five percent of

282
00:12:19,120 --> 00:12:21,760
additional ways compared to loading a

283
00:12:21,760 --> 00:12:23,360
full motor copy

284
00:12:23,360 --> 00:12:26,399
as a result the total memory footprint

285
00:12:26,399 --> 00:12:29,360
of 60 foot tasks occupy less than 40

286
00:12:29,360 --> 00:12:32,320
percent of the total gpu memory in the

287
00:12:32,320 --> 00:12:35,200
right figure the data footprint of pts

288
00:12:35,200 --> 00:12:38,480
with 16 and 32 tasks at the same

289
00:12:38,480 --> 00:12:41,519
but the 64 tasks consume three times

290
00:12:41,519 --> 00:12:45,120
more data memory than the 32 tasks this

291
00:12:45,120 --> 00:12:48,399
is because we use a medius cov device

292
00:12:48,399 --> 00:12:51,040
memory allocator for dynamic data memory

293
00:12:51,040 --> 00:12:54,720
management and the allocated memory

294
00:12:54,720 --> 00:12:56,720
device memory is not strictly

295
00:12:56,720 --> 00:12:59,839
proportional to the data size but

296
00:12:59,839 --> 00:13:02,880
aligned according to some running rules

297
00:13:02,880 --> 00:13:05,200
we then evaluate the service throughput

298
00:13:05,200 --> 00:13:07,760
with fixed size inputs

299
00:13:07,760 --> 00:13:09,360
we prepare different input

300
00:13:09,360 --> 00:13:12,160
configurations with variable batch sizes

301
00:13:12,160 --> 00:13:14,880
and input lens on average

302
00:13:14,880 --> 00:13:18,480
pts achieves about 1.53 times higher

303
00:13:18,480 --> 00:13:21,720
throughput on turn at its high and

304
00:13:21,720 --> 00:13:23,760
1.63 times

305
00:13:23,760 --> 00:13:26,959
on b 100 compared to the single task

306
00:13:26,959 --> 00:13:29,920
serving baseline we notice that pts

307
00:13:29,920 --> 00:13:31,920
fails to achieve meaningful speed up

308
00:13:31,920 --> 00:13:34,480
than single task serving on tx2

309
00:13:34,480 --> 00:13:37,040
this is because px2 only has limited

310
00:13:37,040 --> 00:13:39,680
computational resources and therefore

311
00:13:39,680 --> 00:13:42,160
can hardly get benefited from the budget

312
00:13:42,160 --> 00:13:44,959
influence we also compared pts with

313
00:13:44,959 --> 00:13:47,760
conventional parallel serving methods

314
00:13:47,760 --> 00:13:50,399
which puts each model into a coda stream

315
00:13:50,399 --> 00:13:53,760
and runs them in parallel

316
00:13:53,760 --> 00:13:55,279
as we can see

317
00:13:55,279 --> 00:13:56,800
parallel serving has a better

318
00:13:56,800 --> 00:13:59,279
performance when the number of tasks is

319
00:13:59,279 --> 00:14:01,920
limited but pts has the better

320
00:14:01,920 --> 00:14:04,160
scalability and can support more

321
00:14:04,160 --> 00:14:06,800
concurrent tasks to better understand

322
00:14:06,800 --> 00:14:09,279
the sources of performance improvement

323
00:14:09,279 --> 00:14:11,839
over sequential serving we break down

324
00:14:11,839 --> 00:14:16,079
the execution time of both pts and seqs

325
00:14:16,079 --> 00:14:18,240
on the desktop platform

326
00:14:18,240 --> 00:14:21,199
pts speeds up the non-pt operators

327
00:14:21,199 --> 00:14:23,519
including the self-attention operations

328
00:14:23,519 --> 00:14:26,320
and the shared linear layers by more

329
00:14:26,320 --> 00:14:29,120
than two times on pretty large and more

330
00:14:29,120 --> 00:14:32,399
than three times on birth base thanks to

331
00:14:32,399 --> 00:14:34,720
the batched execution of the shield

332
00:14:34,720 --> 00:14:36,160
operations

333
00:14:36,160 --> 00:14:39,040
and the light-weighted pt operators only

334
00:14:39,040 --> 00:14:44,079
take up 27 to 41 of the total execution

335
00:14:44,079 --> 00:14:45,440
time

336
00:14:45,440 --> 00:14:48,079
as stated before in real world multitask

337
00:14:48,079 --> 00:14:50,720
solving scenarios the input queries

338
00:14:50,720 --> 00:14:53,760
usually have variable sequence lengths

339
00:14:53,760 --> 00:14:56,399
to simulate real-world cases we assume

340
00:14:56,399 --> 00:14:58,720
the queries length obey the quotient

341
00:14:58,720 --> 00:15:02,639
distribution we set the mean value to 32

342
00:15:02,639 --> 00:15:05,360
and set the standard deviation from one

343
00:15:05,360 --> 00:15:08,079
to eight we compare the coordinated

344
00:15:08,079 --> 00:15:10,320
pattern with the fixed side pattern

345
00:15:10,320 --> 00:15:13,040
alpha only batching and the beta only

346
00:15:13,040 --> 00:15:15,839
batching the other only function

347
00:15:15,839 --> 00:15:18,079
only considers the shared operations

348
00:15:18,079 --> 00:15:19,920
while the beta only butchering only

349
00:15:19,920 --> 00:15:22,720
considers the pt operations as we can

350
00:15:22,720 --> 00:15:23,519
see

351
00:15:23,519 --> 00:15:25,839
coordinated battery is suitable for

352
00:15:25,839 --> 00:15:28,399
queries with small variants in query

353
00:15:28,399 --> 00:15:31,519
less why for queries with large variance

354
00:15:31,519 --> 00:15:34,000
the alpha only batching is better this

355
00:15:34,000 --> 00:15:35,680
is reasonable since the shared

356
00:15:35,680 --> 00:15:37,920
operations take up the majority of

357
00:15:37,920 --> 00:15:41,279
execution time finally we evaluate the

358
00:15:41,279 --> 00:15:43,760
performance of the module stream pdt

359
00:15:43,760 --> 00:15:46,160
operator scheduling by setting different

360
00:15:46,160 --> 00:15:49,040
number of cuda streams and testing with

361
00:15:49,040 --> 00:15:51,120
different input lengths

362
00:15:51,120 --> 00:15:53,920
we find that increasing the number of

363
00:15:53,920 --> 00:15:56,160
cuda streams can improve the service

364
00:15:56,160 --> 00:15:58,959
throughput on long sequence but has

365
00:15:58,959 --> 00:16:02,000
negative effects on short inputs this is

366
00:16:02,000 --> 00:16:04,560
because the execution time is too short

367
00:16:04,560 --> 00:16:06,800
for the gpu scheduler to overlap

368
00:16:06,800 --> 00:16:09,279
concurrent streams

369
00:16:09,279 --> 00:16:12,959
currently pts is mainly optimized for

370
00:16:12,959 --> 00:16:15,680
solving throughput we do not consider

371
00:16:15,680 --> 00:16:18,240
the optimization for latency critical

372
00:16:18,240 --> 00:16:19,680
tasks

373
00:16:19,680 --> 00:16:21,440
we only evaluated the full

374
00:16:21,440 --> 00:16:24,079
representative pt algorithms

375
00:16:24,079 --> 00:16:26,639
to support a new pet algorithm

376
00:16:26,639 --> 00:16:29,120
one should first identify the pt

377
00:16:29,120 --> 00:16:31,600
operations of the algorithm to cover

378
00:16:31,600 --> 00:16:34,399
them from the shared operations and

379
00:16:34,399 --> 00:16:38,480
implement a new pt operator if necessary

380
00:16:38,480 --> 00:16:41,759
the model loading and managing apis

381
00:16:41,759 --> 00:16:45,440
should also be extended accordingly

382
00:16:45,440 --> 00:16:46,720
in summary

383
00:16:46,720 --> 00:16:48,880
to realize efficient storing of

384
00:16:48,880 --> 00:16:51,519
parameter efficient transformers we

385
00:16:51,519 --> 00:16:54,160
presented a unified representation for

386
00:16:54,160 --> 00:16:57,519
various pt algorithms to help put them

387
00:16:57,519 --> 00:16:59,279
into one framework

388
00:16:59,279 --> 00:17:01,920
within design a pts framework for

389
00:17:01,920 --> 00:17:04,959
managing and serving of pt tasks

390
00:17:04,959 --> 00:17:08,319
we presented two optimization strategies

391
00:17:08,319 --> 00:17:11,199
including a coordinated batch scheduling

392
00:17:11,199 --> 00:17:13,760
and a multi-stream pt operator

393
00:17:13,760 --> 00:17:15,599
scheduling strategy

394
00:17:15,599 --> 00:17:17,839
to improve the serving throughput

395
00:17:17,839 --> 00:17:18,799
further

396
00:17:18,799 --> 00:17:22,079
finally we evaluated the pts against the

397
00:17:22,079 --> 00:17:24,799
two traditional sequential serving and

398
00:17:24,799 --> 00:17:27,439
parallel serving frameworks on three

399
00:17:27,439 --> 00:17:29,120
different platforms

400
00:17:29,120 --> 00:17:32,400
compared to the baselines pts calls for

401
00:17:32,400 --> 00:17:35,200
the more concurrent tasks and has a

402
00:17:35,200 --> 00:17:38,559
higher serving throughput

403
00:17:39,360 --> 00:17:42,559
pts is planned to be integrated to an

404
00:17:42,559 --> 00:17:44,799
open source inference serving framework

405
00:17:44,799 --> 00:17:49,360
hye which is developed by alibaba

406
00:17:49,360 --> 00:17:52,640
pts will also support more pet models

407
00:17:52,640 --> 00:17:55,280
trained by existing pet training

408
00:17:55,280 --> 00:17:58,640
frameworks such as adapter hub and open

409
00:17:58,640 --> 00:18:00,559
data

410
00:18:00,559 --> 00:18:02,320
thank you for your listening any

411
00:18:02,320 --> 00:18:06,360
questions i will come

