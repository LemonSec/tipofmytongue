1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:12,920 --> 00:00:15,719
today I'm here to talk to you about the

3
00:00:15,719 --> 00:00:17,640
work done with my colleagues from the

4
00:00:17,640 --> 00:00:20,460
University of Toronto and yscope we've

5
00:00:20,460 --> 00:00:22,680
investigated managed language runtime

6
00:00:22,680 --> 00:00:24,720
implementations to try and understand

7
00:00:24,720 --> 00:00:27,300
the scenarios where they Excel and where

8
00:00:27,300 --> 00:00:29,580
they fall short I'll explain why

9
00:00:29,580 --> 00:00:32,040
JavaScript and python can be much slower

10
00:00:32,040 --> 00:00:35,700
than Java go or C plus and why more

11
00:00:35,700 --> 00:00:37,860
garbage collection can actually improve

12
00:00:37,860 --> 00:00:39,719
an application's performance

13
00:00:39,719 --> 00:00:42,420
now just to be clear this work isn't our

14
00:00:42,420 --> 00:00:43,860
personal attack on your favorite

15
00:00:43,860 --> 00:00:46,620
language but a much needed unbiased

16
00:00:46,620 --> 00:00:49,020
investigation into understanding the

17
00:00:49,020 --> 00:00:51,539
complex managed language runtimes that

18
00:00:51,539 --> 00:00:54,239
are only growing in popularity

19
00:00:54,239 --> 00:00:56,520
it is no coincidence that JavaScript

20
00:00:56,520 --> 00:00:59,280
Java and python have been the most

21
00:00:59,280 --> 00:01:02,579
popular languages on GitHub since 2015.

22
00:01:02,579 --> 00:01:05,280
these managed languages utilize runtime

23
00:01:05,280 --> 00:01:07,979
environments to provide developers with

24
00:01:07,979 --> 00:01:09,780
many useful features

25
00:01:09,780 --> 00:01:12,000
features such as automatic memory

26
00:01:12,000 --> 00:01:13,520
management with garbage collection

27
00:01:13,520 --> 00:01:15,659
automatic memory safety with bounce

28
00:01:15,659 --> 00:01:18,840
checking and the dynamic typing which

29
00:01:18,840 --> 00:01:20,659
can greatly improve productivity

30
00:01:20,659 --> 00:01:23,700
minimizing product to Market time this

31
00:01:23,700 --> 00:01:26,159
allows developers to quickly adapt and

32
00:01:26,159 --> 00:01:28,080
improve their applications for the

33
00:01:28,080 --> 00:01:30,360
rapidly growing user base

34
00:01:30,360 --> 00:01:32,700
when improved productivity will put an

35
00:01:32,700 --> 00:01:35,460
application in the hands of the users as

36
00:01:35,460 --> 00:01:37,799
soon as possible performance rarely

37
00:01:37,799 --> 00:01:40,140
starts out as the primary concern of a

38
00:01:40,140 --> 00:01:41,100
project

39
00:01:41,100 --> 00:01:43,619
some even go as far as to claim that

40
00:01:43,619 --> 00:01:45,780
choosing a language for your application

41
00:01:45,780 --> 00:01:48,780
simply because it's fast is the ultimate

42
00:01:48,780 --> 00:01:52,140
form of premature optimization

43
00:01:52,140 --> 00:01:54,540
now let's fast forward a bit and now you

44
00:01:54,540 --> 00:01:56,520
have a massive user base thanks to the

45
00:01:56,520 --> 00:01:58,640
amazing productivity of these languages

46
00:01:58,640 --> 00:02:00,899
now all of a sudden users are

47
00:02:00,899 --> 00:02:02,880
complaining about things like quality of

48
00:02:02,880 --> 00:02:05,159
service and throwing more money at

49
00:02:05,159 --> 00:02:07,259
better Hardware just isn't making the

50
00:02:07,259 --> 00:02:10,380
problem go away anymore so now what well

51
00:02:10,380 --> 00:02:12,720
you profile your code and you start to

52
00:02:12,720 --> 00:02:15,060
optimize it but this has diminishing

53
00:02:15,060 --> 00:02:17,520
returns as each new optimization

54
00:02:17,520 --> 00:02:20,400
requires more engineering effort to

55
00:02:20,400 --> 00:02:23,220
achieve any meaningful Improvement

56
00:02:23,220 --> 00:02:26,340
for example stream was forced to abandon

57
00:02:26,340 --> 00:02:29,220
python for go as python would spend 10

58
00:02:29,220 --> 00:02:32,340
milliseconds creating objects from data

59
00:02:32,340 --> 00:02:34,860
that Cassandra only took one millisecond

60
00:02:34,860 --> 00:02:37,680
to fetch they stated we've been

61
00:02:37,680 --> 00:02:41,220
optimizing Cassandra postgresql redis

62
00:02:41,220 --> 00:02:44,340
etc for years but eventually you reach

63
00:02:44,340 --> 00:02:46,280
the limits of the language you're using

64
00:02:46,280 --> 00:02:48,720
performance also LED Twitter to switch

65
00:02:48,720 --> 00:02:51,300
from Ruby to Scala and Java

66
00:02:51,300 --> 00:02:54,540
so performance inevitably matters then

67
00:02:54,540 --> 00:02:57,239
the question is how well does each

68
00:02:57,239 --> 00:03:00,360
language perform how do they compare and

69
00:03:00,360 --> 00:03:02,519
importantly why

70
00:03:02,519 --> 00:03:05,400
that is the question we will answer in

71
00:03:05,400 --> 00:03:07,140
this work

72
00:03:07,140 --> 00:03:09,420
so to answer these questions We

73
00:03:09,420 --> 00:03:11,879
performed an in-depth quantitative

74
00:03:11,879 --> 00:03:14,580
performance analysis of four of the most

75
00:03:14,580 --> 00:03:17,580
popular managed languages with the most

76
00:03:17,580 --> 00:03:20,159
widely used implementation for each

77
00:03:20,159 --> 00:03:23,819
language openjdk for Java VA with

78
00:03:23,819 --> 00:03:26,459
node.js for JavaScript the reference go

79
00:03:26,459 --> 00:03:30,000
compiler and C python for python we

80
00:03:30,000 --> 00:03:33,360
utilize GCC for C plus as a baseline for

81
00:03:33,360 --> 00:03:34,620
comparisons

82
00:03:34,620 --> 00:03:37,140
the runtimes we study represent many

83
00:03:37,140 --> 00:03:40,200
different designs We compare Dynamic and

84
00:03:40,200 --> 00:03:42,540
static typing execution that is

85
00:03:42,540 --> 00:03:45,599
statically compiled only interpreted and

86
00:03:45,599 --> 00:03:48,120
interpreted with jit compilation and we

87
00:03:48,120 --> 00:03:50,099
also compare their different concurrency

88
00:03:50,099 --> 00:03:52,980
models that I'll explain later

89
00:03:52,980 --> 00:03:55,019
now to do this we needed a benchmark

90
00:03:55,019 --> 00:03:57,360
Suite we created a suite of six

91
00:03:57,360 --> 00:03:59,159
Benchmark applications from the ground

92
00:03:59,159 --> 00:04:01,440
up that stress a range of components

93
00:04:01,440 --> 00:04:05,040
including CPU memory and parallelism for

94
00:04:05,040 --> 00:04:07,620
each application we implemented it in

95
00:04:07,620 --> 00:04:10,560
five different languages we ensure the

96
00:04:10,560 --> 00:04:12,780
same high-level algorithms and data

97
00:04:12,780 --> 00:04:14,700
structures are used for a fair

98
00:04:14,700 --> 00:04:17,220
comparison yet we keep the

99
00:04:17,220 --> 00:04:20,040
implementations idiomatic

100
00:04:20,040 --> 00:04:22,639
but a benchmark Suite isn't enough

101
00:04:22,639 --> 00:04:25,860
existing runtimes do not provide enough

102
00:04:25,860 --> 00:04:28,320
observability for us to understand their

103
00:04:28,320 --> 00:04:30,780
performance in detail therefore we

104
00:04:30,780 --> 00:04:34,199
instrument openjdk V8 with node.js and C

105
00:04:34,199 --> 00:04:36,900
python we provide two instrumentations

106
00:04:36,900 --> 00:04:40,440
first we instrument The Interpreter and

107
00:04:40,440 --> 00:04:42,360
the runtimes to measure the performance

108
00:04:42,360 --> 00:04:44,880
of executing any bytecode

109
00:04:44,880 --> 00:04:46,740
but why does interpreter performance

110
00:04:46,740 --> 00:04:49,440
matter in the past we were able to brush

111
00:04:49,440 --> 00:04:51,840
off interpreter performance by saying

112
00:04:51,840 --> 00:04:54,600
everything was long-running server-like

113
00:04:54,600 --> 00:04:57,120
applications this meant that code we

114
00:04:57,120 --> 00:04:59,580
cared about was loaded and jit compiled

115
00:04:59,580 --> 00:05:01,620
so the execution that mattered was

116
00:05:01,620 --> 00:05:03,060
always done efficiently

117
00:05:03,060 --> 00:05:05,820
however this simply isn't true anymore

118
00:05:05,820 --> 00:05:07,860
as the paradigm shift to Cloud

119
00:05:07,860 --> 00:05:10,380
applications and parallelism favors

120
00:05:10,380 --> 00:05:13,259
having many short-lived smaller tasks

121
00:05:13,259 --> 00:05:16,320
this Paradigm requires new runtime

122
00:05:16,320 --> 00:05:19,259
instances to be constantly started or

123
00:05:19,259 --> 00:05:22,259
restarted on demand causing startup and

124
00:05:22,259 --> 00:05:24,180
warm-up overhead to become the

125
00:05:24,180 --> 00:05:25,199
bottleneck

126
00:05:25,199 --> 00:05:27,240
in the most extreme case we have things

127
00:05:27,240 --> 00:05:29,100
like function as a service platforms

128
00:05:29,100 --> 00:05:32,039
where the median AWS Lambda invocation

129
00:05:32,039 --> 00:05:36,060
was only 60 milliseconds not to mention

130
00:05:36,060 --> 00:05:37,860
some widely used language

131
00:05:37,860 --> 00:05:40,259
implementations like C python are

132
00:05:40,259 --> 00:05:42,180
strictly interpreted with no jit

133
00:05:42,180 --> 00:05:45,180
compiler therefore even bytecode level

134
00:05:45,180 --> 00:05:48,419
profiling can enable optimizations for

135
00:05:48,419 --> 00:05:51,300
example Instagram Engineers also had to

136
00:05:51,300 --> 00:05:53,940
instrument C python to identify the

137
00:05:53,940 --> 00:05:55,740
bytecode instructions with high

138
00:05:55,740 --> 00:05:58,199
overheads and then optimize their code

139
00:05:58,199 --> 00:06:00,600
to avoid using these expensive

140
00:06:00,600 --> 00:06:02,220
instructions

141
00:06:02,220 --> 00:06:04,620
but we also need to instrument the

142
00:06:04,620 --> 00:06:07,919
JavaScript V8 jig compiler JavaScript is

143
00:06:07,919 --> 00:06:10,680
dynamically typed meaning code can only

144
00:06:10,680 --> 00:06:12,539
be checked for correctness when it is

145
00:06:12,539 --> 00:06:15,360
executed for example this is a function

146
00:06:15,360 --> 00:06:18,539
to multiply a variable with itself this

147
00:06:18,539 --> 00:06:20,759
is valid JavaScript you can call this

148
00:06:20,759 --> 00:06:23,280
function and pass it a string this is

149
00:06:23,280 --> 00:06:25,680
also valid JavaScript a JavaScript

150
00:06:25,680 --> 00:06:28,199
implementation must try to multiply the

151
00:06:28,199 --> 00:06:30,419
string with itself and no matter what

152
00:06:30,419 --> 00:06:32,400
analysis it can do in advance to know

153
00:06:32,400 --> 00:06:34,139
that it will fail the JavaScript

154
00:06:34,139 --> 00:06:36,780
specification says it's only allowed to

155
00:06:36,780 --> 00:06:39,120
be an error after try to multiply the

156
00:06:39,120 --> 00:06:40,139
string

157
00:06:40,139 --> 00:06:42,660
the best thing VA can do is to jit

158
00:06:42,660 --> 00:06:44,880
compile this function and assume X is a

159
00:06:44,880 --> 00:06:47,340
number based on profiling your code

160
00:06:47,340 --> 00:06:49,560
since it optimizes the code assuming X

161
00:06:49,560 --> 00:06:52,199
is a number every time it's called V8

162
00:06:52,199 --> 00:06:54,120
still needs to ensure you really did

163
00:06:54,120 --> 00:06:55,500
pass it a number

164
00:06:55,500 --> 00:06:58,860
V8 calls this shape checking

165
00:06:58,860 --> 00:07:01,500
our instrumentation allows targeted

166
00:07:01,500 --> 00:07:04,500
removal of specific checks in any

167
00:07:04,500 --> 00:07:07,199
user-specified function removing these

168
00:07:07,199 --> 00:07:09,360
checks allows us to see what JavaScript

169
00:07:09,360 --> 00:07:11,699
would be like with closer to C like

170
00:07:11,699 --> 00:07:12,740
performance

171
00:07:12,740 --> 00:07:15,840
albeit with some fun see like undefined

172
00:07:15,840 --> 00:07:18,539
behavior that we all love if any values

173
00:07:18,539 --> 00:07:20,460
aren't a consistent type at execution

174
00:07:20,460 --> 00:07:22,380
time

175
00:07:22,380 --> 00:07:25,139
so for our methodology we picked the

176
00:07:25,139 --> 00:07:26,240
most popular

177
00:07:26,240 --> 00:07:29,160
implementations of our languages and use

178
00:07:29,160 --> 00:07:31,680
their latest stable version at the time

179
00:07:31,680 --> 00:07:34,380
of the study we ran the experiments in

180
00:07:34,380 --> 00:07:36,120
two in-house servers to support

181
00:07:36,120 --> 00:07:38,280
workloads with networking communication

182
00:07:38,280 --> 00:07:40,620
and measure the average end-to-end

183
00:07:40,620 --> 00:07:42,780
completion time

184
00:07:42,780 --> 00:07:44,880
now next we'll look at the results of

185
00:07:44,880 --> 00:07:46,860
our experiments and later we'll get into

186
00:07:46,860 --> 00:07:48,900
some findings

187
00:07:48,900 --> 00:07:51,900
so this figure shows the results of all

188
00:07:51,900 --> 00:07:54,660
our experiments the y-axis shows The

189
00:07:54,660 --> 00:07:57,060
Benchmark completion time normalized to

190
00:07:57,060 --> 00:08:01,020
GCC the x-axis is divided by Benchmark

191
00:08:01,020 --> 00:08:03,120
with each bar showing a language

192
00:08:03,120 --> 00:08:05,940
implementation from left to right we

193
00:08:05,940 --> 00:08:10,860
have GCC go open jdk VA and C python for

194
00:08:10,860 --> 00:08:12,720
the multi-threaded workloads the number

195
00:08:12,720 --> 00:08:15,539
above the bar represents the number of

196
00:08:15,539 --> 00:08:18,300
parallel execution units such as threads

197
00:08:18,300 --> 00:08:20,759
or go routines at the bottom we have the

198
00:08:20,759 --> 00:08:22,740
absolute completion time of GCC in

199
00:08:22,740 --> 00:08:23,639
seconds

200
00:08:23,639 --> 00:08:25,800
now first we see the averages

201
00:08:25,800 --> 00:08:29,160
highlighted go and open jdk we're close

202
00:08:29,160 --> 00:08:33,059
behind GCC at 1.3 and 1.4 times slower

203
00:08:33,059 --> 00:08:34,260
on average

204
00:08:34,260 --> 00:08:38,099
V8 and C python were notably slower at 8

205
00:08:38,099 --> 00:08:41,219
and 30 times slower on average

206
00:08:41,219 --> 00:08:43,260
now in the extreme case for the sort

207
00:08:43,260 --> 00:08:47,040
Benchmark V8 is 47 times slower than GCC

208
00:08:47,040 --> 00:08:51,120
and C python is 130 times slower than

209
00:08:51,120 --> 00:08:52,920
GCC

210
00:08:52,920 --> 00:08:55,380
next let's see the scalability pattern

211
00:08:55,380 --> 00:08:58,620
we can see that B8 and C python don't

212
00:08:58,620 --> 00:09:00,779
always scale well as shown in the key

213
00:09:00,779 --> 00:09:03,180
value store and log analysis benchmarks

214
00:09:03,180 --> 00:09:05,940
in fact their best performance is

215
00:09:05,940 --> 00:09:08,160
achieved when only using one parallel

216
00:09:08,160 --> 00:09:10,980
execution unit and gets worse when using

217
00:09:10,980 --> 00:09:14,640
more in comparison openjdk and go scale

218
00:09:14,640 --> 00:09:17,519
similarly similarly to GCC as they are

219
00:09:17,519 --> 00:09:20,519
able to take advantage of multiple cores

220
00:09:20,519 --> 00:09:23,580
however in some cases we observed that

221
00:09:23,580 --> 00:09:25,920
runtimes can actually provide advantages

222
00:09:25,920 --> 00:09:28,440
in performance due to higher level

223
00:09:28,440 --> 00:09:31,200
abstractions they provide with efficient

224
00:09:31,200 --> 00:09:32,760
implementations

225
00:09:32,760 --> 00:09:35,459
this contradicts the conventional wisdom

226
00:09:35,459 --> 00:09:38,040
that abstractions are at the expense of

227
00:09:38,040 --> 00:09:39,180
performance

228
00:09:39,180 --> 00:09:41,399
we found a few reasons I'll explain two

229
00:09:41,399 --> 00:09:42,839
in the talk and there's more in the

230
00:09:42,839 --> 00:09:43,620
paper

231
00:09:43,620 --> 00:09:46,320
performing GC can actually improve

232
00:09:46,320 --> 00:09:47,940
performance due to improved cash

233
00:09:47,940 --> 00:09:50,940
locality and goes user level thread

234
00:09:50,940 --> 00:09:53,399
scheduler reduces the number of context

235
00:09:53,399 --> 00:09:56,339
switches in combination with go standard

236
00:09:56,339 --> 00:09:58,920
Library using asynchronous networking

237
00:09:58,920 --> 00:10:00,600
calls

238
00:10:00,600 --> 00:10:03,360
now I'll explain each of the findings

239
00:10:03,360 --> 00:10:05,519
I'll first explain the sources of

240
00:10:05,519 --> 00:10:07,980
runtime overhead namely the type and

241
00:10:07,980 --> 00:10:10,200
bounce checking and GC right barriers

242
00:10:10,200 --> 00:10:13,140
then I'll explain the scalability

243
00:10:13,140 --> 00:10:16,500
limitations of C Python mv8 and finally

244
00:10:16,500 --> 00:10:18,720
I'll discuss the runtime advantages

245
00:10:18,720 --> 00:10:21,000
before I conclude

246
00:10:21,000 --> 00:10:23,100
so first let's look at runtime overhead

247
00:10:23,100 --> 00:10:26,100
we found a major source of b8's overhead

248
00:10:26,100 --> 00:10:28,740
to come from type and Bounds checking

249
00:10:28,740 --> 00:10:32,519
this takes up to 42 percent and 87

250
00:10:32,519 --> 00:10:35,820
percent of the execution time in Sudoku

251
00:10:35,820 --> 00:10:38,459
and sort benchmarks respectively now

252
00:10:38,459 --> 00:10:40,740
I'll dive into Sudoku to explain this

253
00:10:40,740 --> 00:10:42,779
overhead with an example

254
00:10:42,779 --> 00:10:45,660
so the Sudoku board is just a 2d array

255
00:10:45,660 --> 00:10:48,300
of integers to access an element we

256
00:10:48,300 --> 00:10:50,640
simply write board x y

257
00:10:50,640 --> 00:10:52,860
now to do this dereference the jit

258
00:10:52,860 --> 00:10:55,620
compiled code must first check that the

259
00:10:55,620 --> 00:10:58,380
board is an object then it must check

260
00:10:58,380 --> 00:11:01,800
the type or shape of the board now VA

261
00:11:01,800 --> 00:11:04,800
checks that X is an integer finally

262
00:11:04,800 --> 00:11:07,140
before indexing the First Dimension

263
00:11:07,140 --> 00:11:10,440
board X V8 does a bounce check

264
00:11:10,440 --> 00:11:13,680
after indexing Vortex V8 needs to check

265
00:11:13,680 --> 00:11:15,959
that it actually yielded a valid element

266
00:11:15,959 --> 00:11:18,420
since arrays don't actually have to be

267
00:11:18,420 --> 00:11:20,579
contiguous in JavaScript and valid

268
00:11:20,579 --> 00:11:23,579
elements are known as holes

269
00:11:23,579 --> 00:11:26,220
V8 then checks if it found an object at

270
00:11:26,220 --> 00:11:29,279
board X and then the typer shape is

271
00:11:29,279 --> 00:11:31,200
checked to see if it is indeed a 1D

272
00:11:31,200 --> 00:11:32,220
array

273
00:11:32,220 --> 00:11:34,440
now we are finally safe to use board X

274
00:11:34,440 --> 00:11:36,600
but all these checks are still only for

275
00:11:36,600 --> 00:11:37,980
the First Dimension

276
00:11:37,980 --> 00:11:40,200
similar steps of checks repeat for the

277
00:11:40,200 --> 00:11:42,300
second dimension and in the end we

278
00:11:42,300 --> 00:11:45,060
required 11 type and bounce checks to

279
00:11:45,060 --> 00:11:48,600
access board X Y removing these checks

280
00:11:48,600 --> 00:11:51,660
resulted in a 1.7 time speed up in

281
00:11:51,660 --> 00:11:54,480
Sudoku in the extreme case removing

282
00:11:54,480 --> 00:11:57,600
these checks resulted in a 7.9 time

283
00:11:57,600 --> 00:12:00,120
speed up in sort

284
00:12:00,120 --> 00:12:02,700
now another source of runtime slowdown

285
00:12:02,700 --> 00:12:05,040
we saw for open jdk in the sort

286
00:12:05,040 --> 00:12:08,940
Benchmark is caused by GC right barriers

287
00:12:08,940 --> 00:12:13,140
with open jdks default GC algorithm G1

288
00:12:13,140 --> 00:12:17,040
it was 10 times slower than GCC

289
00:12:17,040 --> 00:12:19,019
just to be clear in this Benchmark

290
00:12:19,019 --> 00:12:22,200
there's no memory pressure so GC is not

291
00:12:22,200 --> 00:12:25,860
triggered however G1 must still maintain

292
00:12:25,860 --> 00:12:29,279
data structures needed to perform GC in

293
00:12:29,279 --> 00:12:31,500
every right operation in order to

294
00:12:31,500 --> 00:12:34,920
prepare for future GC even if the GC

295
00:12:34,920 --> 00:12:36,959
doesn't actually run in the future

296
00:12:36,959 --> 00:12:39,720
this is called a right barrier do you

297
00:12:39,720 --> 00:12:42,360
want to add 44 instructions for these

298
00:12:42,360 --> 00:12:45,180
right barriers completely dwarfing the

299
00:12:45,180 --> 00:12:47,040
six instructions required to swap the

300
00:12:47,040 --> 00:12:49,440
elements and the five for bound checking

301
00:12:49,440 --> 00:12:51,480
the array axises

302
00:12:51,480 --> 00:12:54,180
switching to a non-default GC algorithm

303
00:12:54,180 --> 00:12:56,760
without write barriers improved openjs

304
00:12:56,760 --> 00:13:01,260
open jdk's performance by five times

305
00:13:01,260 --> 00:13:04,320
now recall that in three of the four of

306
00:13:04,320 --> 00:13:07,380
our multi-threaded benchmarks C Python's

307
00:13:07,380 --> 00:13:09,720
best performance was achieved with only

308
00:13:09,720 --> 00:13:11,160
one thread

309
00:13:11,160 --> 00:13:14,040
whereas other languages can be improved

310
00:13:14,040 --> 00:13:16,260
by their performance using more than one

311
00:13:16,260 --> 00:13:17,279
thread

312
00:13:17,279 --> 00:13:20,339
see Python's fundamental problem is that

313
00:13:20,339 --> 00:13:22,440
it has a global interpreter log

314
00:13:22,440 --> 00:13:24,959
this prevents multiple threads from

315
00:13:24,959 --> 00:13:26,820
executing in parallel

316
00:13:26,820 --> 00:13:28,920
developers can take advantage of

317
00:13:28,920 --> 00:13:31,500
multiple cores by creating multiple

318
00:13:31,500 --> 00:13:35,160
processes however processes have to

319
00:13:35,160 --> 00:13:38,459
serialize and deserialize any data they

320
00:13:38,459 --> 00:13:41,100
communicate we tried that and it

321
00:13:41,100 --> 00:13:43,800
resulted in worse performance

322
00:13:43,800 --> 00:13:46,560
now v8's concurrency Model results in

323
00:13:46,560 --> 00:13:49,560
essentially a similar effect it is event

324
00:13:49,560 --> 00:13:52,320
driven meaning that it spawns a fixed

325
00:13:52,320 --> 00:13:54,899
number of background threads which I O

326
00:13:54,899 --> 00:13:57,000
requests are offloaded to

327
00:13:57,000 --> 00:13:58,680
while those background threads are

328
00:13:58,680 --> 00:14:01,079
blocked on i o there is a single event

329
00:14:01,079 --> 00:14:04,019
Loop executing the developer's code in a

330
00:14:04,019 --> 00:14:05,279
single thread

331
00:14:05,279 --> 00:14:08,220
any computation must run in an event

332
00:14:08,220 --> 00:14:11,040
Loop so to utilize multiple cores

333
00:14:11,040 --> 00:14:13,800
multiple event Loops are needed

334
00:14:13,800 --> 00:14:16,620
but in VA separate event Loops have

335
00:14:16,620 --> 00:14:19,740
their own entirely separate heaps and

336
00:14:19,740 --> 00:14:21,720
there is limited support for shared

337
00:14:21,720 --> 00:14:24,540
memory therefore computation in separate

338
00:14:24,540 --> 00:14:26,760
event Loops can only be communicated

339
00:14:26,760 --> 00:14:29,579
with each other by serializing and

340
00:14:29,579 --> 00:14:32,519
copying data back and forth the only

341
00:14:32,519 --> 00:14:35,100
Benchmark where VA and C Python's

342
00:14:35,100 --> 00:14:37,139
concurrency models scale Beyond one

343
00:14:37,139 --> 00:14:39,360
thread is the file server Benchmark

344
00:14:39,360 --> 00:14:42,000
where a client requests a file and the

345
00:14:42,000 --> 00:14:43,800
server simply sends it back

346
00:14:43,800 --> 00:14:46,320
here threads do not need to share memory

347
00:14:46,320 --> 00:14:50,160
and they are purely i o bound

348
00:14:50,160 --> 00:14:52,560
now despite all these findings on the

349
00:14:52,560 --> 00:14:54,360
overhead of runtimes we actually

350
00:14:54,360 --> 00:14:56,639
observed a few cases where runtime

351
00:14:56,639 --> 00:14:59,880
languages outperformed GCC

352
00:14:59,880 --> 00:15:01,980
due to the higher level abstractions

353
00:15:01,980 --> 00:15:04,440
they provide

354
00:15:04,440 --> 00:15:07,079
now first we found that performing GC

355
00:15:07,079 --> 00:15:10,320
can actually speed up the application

356
00:15:10,320 --> 00:15:14,820
for example openjdk achieved a 1.5 time

357
00:15:14,820 --> 00:15:18,180
speed up over GCC in the single threaded

358
00:15:18,180 --> 00:15:21,180
key value store which is the largest

359
00:15:21,180 --> 00:15:24,959
speed up of any runtime over GCC and all

360
00:15:24,959 --> 00:15:28,079
benchmarks among all languages

361
00:15:28,079 --> 00:15:31,019
this was due to GC improving cash

362
00:15:31,019 --> 00:15:32,160
locality

363
00:15:32,160 --> 00:15:34,980
in this Benchmark we implemented a key

364
00:15:34,980 --> 00:15:37,920
value store that uses a hash map our

365
00:15:37,920 --> 00:15:40,560
hashmap uses chaining which means that

366
00:15:40,560 --> 00:15:43,079
whenever multiple keys are hashed to the

367
00:15:43,079 --> 00:15:45,600
same bucket known as a collision the

368
00:15:45,600 --> 00:15:48,000
node that contains the key value pair is

369
00:15:48,000 --> 00:15:50,399
appended to the buckets linked list

370
00:15:50,399 --> 00:15:52,980
this figure shows a hash map after six

371
00:15:52,980 --> 00:15:55,560
insertions the buckets are labeled B at

372
00:15:55,560 --> 00:15:57,420
the top and the nodes of the buckets

373
00:15:57,420 --> 00:16:00,480
linked lists are labeled n

374
00:16:00,480 --> 00:16:03,300
now the dark area shows the layout of

375
00:16:03,300 --> 00:16:07,260
the jvm Heap open jdk uses bump pointer

376
00:16:07,260 --> 00:16:10,260
allocation so objects are allocated on

377
00:16:10,260 --> 00:16:12,779
the Heap in the order they are inserted

378
00:16:12,779 --> 00:16:16,019
to our key value store in this case the

379
00:16:16,019 --> 00:16:18,660
insertion order of these six nodes are

380
00:16:18,660 --> 00:16:21,300
from N1 to n6

381
00:16:21,300 --> 00:16:23,459
this leaves the nodes fragmented

382
00:16:23,459 --> 00:16:25,440
throughout the Heap with respect to the

383
00:16:25,440 --> 00:16:28,320
link list as we can see in the figure

384
00:16:28,320 --> 00:16:31,620
however during lookup Whenever there is

385
00:16:31,620 --> 00:16:34,139
a hash Collision the linked list needs

386
00:16:34,139 --> 00:16:36,899
to be traversed now for example to

387
00:16:36,899 --> 00:16:40,079
locate n6 we first have to access bucket

388
00:16:40,079 --> 00:16:45,060
1 node 2 or B1 and 2 then B1 and 3.

389
00:16:45,060 --> 00:16:47,100
because they are scattered throughout

390
00:16:47,100 --> 00:16:49,740
the Heap without cash locality each

391
00:16:49,740 --> 00:16:52,199
access results in a cache Miss

392
00:16:52,199 --> 00:16:54,959
and similarly this is always the case

393
00:16:54,959 --> 00:16:57,180
for C plus plus when you use the default

394
00:16:57,180 --> 00:16:59,759
malloc implementation

395
00:16:59,759 --> 00:17:03,420
but open jdk and V8 use a compacting

396
00:17:03,420 --> 00:17:05,880
garbage collector meaning that objects

397
00:17:05,880 --> 00:17:09,839
will be moved after GC specifically live

398
00:17:09,839 --> 00:17:12,839
objects will be marked following the

399
00:17:12,839 --> 00:17:15,480
linked list pointers and later after

400
00:17:15,480 --> 00:17:18,359
marking these live objects will be moved

401
00:17:18,359 --> 00:17:20,939
from a different memory region in the

402
00:17:20,939 --> 00:17:24,119
same order they were marked this results

403
00:17:24,119 --> 00:17:27,059
in the layout shown in this figure now

404
00:17:27,059 --> 00:17:30,059
when we look up on n6

405
00:17:30,059 --> 00:17:32,460
even though we still have to Traverse

406
00:17:32,460 --> 00:17:34,679
the linked list there are fewer cache

407
00:17:34,679 --> 00:17:36,900
misses because nodes are now contiguous

408
00:17:36,900 --> 00:17:39,299
in memory so that multiple nodes will be

409
00:17:39,299 --> 00:17:41,820
in the same cache Line This Behavior

410
00:17:41,820 --> 00:17:44,640
leads to the unintuitive pattern that

411
00:17:44,640 --> 00:17:47,100
the more GC performed the better the

412
00:17:47,100 --> 00:17:49,080
performance is

413
00:17:49,080 --> 00:17:51,660
in this figure it shows that the

414
00:17:51,660 --> 00:17:54,179
relationship between the jvm's max Heap

415
00:17:54,179 --> 00:17:57,000
size and execution time on the key Value

416
00:17:57,000 --> 00:17:58,799
Store benchmark

417
00:17:58,799 --> 00:18:01,500
as we can see the more memory we

418
00:18:01,500 --> 00:18:03,660
provisioned to the Heap along the x-axis

419
00:18:03,660 --> 00:18:07,020
the performance becomes worse because it

420
00:18:07,020 --> 00:18:10,980
ends up resulting in fewer GC cycles and

421
00:18:10,980 --> 00:18:13,860
in fact we saw similar behavior in two

422
00:18:13,860 --> 00:18:15,419
other benchmarks

423
00:18:15,419 --> 00:18:18,960
now another speed up provided by a

424
00:18:18,960 --> 00:18:21,960
runtime is from goes user level threads

425
00:18:21,960 --> 00:18:25,440
or go routines a go routine and go is a

426
00:18:25,440 --> 00:18:29,160
user level thread that multiple go and

427
00:18:29,160 --> 00:18:31,559
multiple go routines can be mapped to a

428
00:18:31,559 --> 00:18:34,559
single kernel thread unlike V8 and C

429
00:18:34,559 --> 00:18:37,080
python which limit computations to a

430
00:18:37,080 --> 00:18:39,120
single core there can be multiple kernel

431
00:18:39,120 --> 00:18:41,460
threads on multiple cores running a

432
00:18:41,460 --> 00:18:43,140
developer's go routines

433
00:18:43,140 --> 00:18:45,179
this can reduce the number of Kernel

434
00:18:45,179 --> 00:18:47,280
threads used and reduce the number of

435
00:18:47,280 --> 00:18:49,320
contact switches between kernel threads

436
00:18:49,320 --> 00:18:51,720
for example in the key Value Store

437
00:18:51,720 --> 00:18:54,299
client threads send requests in parallel

438
00:18:54,299 --> 00:18:57,600
if there are 256 client threads for a

439
00:18:57,600 --> 00:19:01,860
GCC and openjdk there will be 256 kernel

440
00:19:01,860 --> 00:19:04,320
threads on the server side one for each

441
00:19:04,320 --> 00:19:06,600
socket connection many of them will be

442
00:19:06,600 --> 00:19:08,640
blocked on network i o at the same time

443
00:19:08,640 --> 00:19:11,100
resulting in many context switches in

444
00:19:11,100 --> 00:19:13,679
comparison for goat there are still one

445
00:19:13,679 --> 00:19:15,419
go routine for each client connection

446
00:19:15,419 --> 00:19:18,000
but there are fewer kernel threads that

447
00:19:18,000 --> 00:19:20,340
are spawned because asynchronous Network

448
00:19:20,340 --> 00:19:23,640
i o enables multiple go routines to be

449
00:19:23,640 --> 00:19:25,880
multiplexed on a single kernel thread

450
00:19:25,880 --> 00:19:27,900
significantly reducing the number of

451
00:19:27,900 --> 00:19:30,059
contact switches between kernel threads

452
00:19:30,059 --> 00:19:32,220
in fact for this multi-threaded key

453
00:19:32,220 --> 00:19:33,960
Value Store Benchmark goes best

454
00:19:33,960 --> 00:19:36,120
performance is achieved when only using

455
00:19:36,120 --> 00:19:38,700
42 server-side kernel threads even

456
00:19:38,700 --> 00:19:41,220
though there were 256 go routines

457
00:19:41,220 --> 00:19:43,799
resulting in better performance compared

458
00:19:43,799 --> 00:19:46,020
to GCC and other languages

459
00:19:46,020 --> 00:19:48,299
now to conclude with some related work

460
00:19:48,299 --> 00:19:51,120
now there are many existing benchmarks

461
00:19:51,120 --> 00:19:53,520
some provide novel methodologies some

462
00:19:53,520 --> 00:19:55,919
investigate specific languages and some

463
00:19:55,919 --> 00:19:58,020
investigate specific types of workloads

464
00:19:58,020 --> 00:20:00,419
we are the first to both provide

465
00:20:00,419 --> 00:20:03,000
intrusive modifications to runtime

466
00:20:03,000 --> 00:20:05,039
environments and to quantitatively

467
00:20:05,039 --> 00:20:07,919
analyze when these runtimes improve or

468
00:20:07,919 --> 00:20:09,780
hinder performance

469
00:20:09,780 --> 00:20:12,360
we saw performance inevitably becomes an

470
00:20:12,360 --> 00:20:15,240
issue as product or service usage grows

471
00:20:15,240 --> 00:20:17,820
thus eventually we must understand and

472
00:20:17,820 --> 00:20:19,740
improve the performance of these complex

473
00:20:19,740 --> 00:20:22,740
managed languages we created langbench

474
00:20:22,740 --> 00:20:24,120
to provide complex runtime

475
00:20:24,120 --> 00:20:26,580
instrumentations enabling detailed

476
00:20:26,580 --> 00:20:28,500
profiling and detailed comparative

477
00:20:28,500 --> 00:20:30,720
analysis throughout the Benchmark Suite

478
00:20:30,720 --> 00:20:33,299
lastly we open sourced all this work on

479
00:20:33,299 --> 00:20:35,640
GitHub and I'll be happy to take any

480
00:20:35,640 --> 00:20:37,940
questions

