1
00:00:14,160 --> 00:00:16,960
hello everyone my name is leet string i

2
00:00:16,960 --> 00:00:19,520
come from shanghai jason university epcc

3
00:00:19,520 --> 00:00:20,480
lab

4
00:00:20,480 --> 00:00:22,560
today i will give a talk to introduce

5
00:00:22,560 --> 00:00:23,519
round d

6
00:00:23,519 --> 00:00:25,760
a lightweight secure container runtime

7
00:00:25,760 --> 00:00:28,080
for high density deployment in high

8
00:00:28,080 --> 00:00:32,640
concurrency startup in service computing

9
00:00:34,239 --> 00:00:36,399
in the first section i will briefly

10
00:00:36,399 --> 00:00:39,440
introduce what servers in the fast are

11
00:00:39,440 --> 00:00:41,840
and what challenges we may meet for

12
00:00:41,840 --> 00:00:46,160
multi-tenants in serverless computing

13
00:00:47,760 --> 00:00:50,000
first what is serverless

14
00:00:50,000 --> 00:00:52,320
according to brooklyn's view serverless

15
00:00:52,320 --> 00:00:55,280
equals fast plus base

16
00:00:55,280 --> 00:00:57,680
developers upload their codes as

17
00:00:57,680 --> 00:01:00,160
functions to the first system

18
00:01:00,160 --> 00:01:03,199
user triggers a function through events

19
00:01:03,199 --> 00:01:05,600
like http requests

20
00:01:05,600 --> 00:01:07,600
the first system allocates convenient

21
00:01:07,600 --> 00:01:10,320
resources and interact with the backend

22
00:01:10,320 --> 00:01:14,559
services to execute functions

23
00:01:17,920 --> 00:01:20,560
when hosting functions invocations

24
00:01:20,560 --> 00:01:22,880
traditional containers only provide

25
00:01:22,880 --> 00:01:25,119
process level allocation

26
00:01:25,119 --> 00:01:27,439
as they are implemented based on name

27
00:01:27,439 --> 00:01:29,520
space and c groups

28
00:01:29,520 --> 00:01:31,439
and sharing host

29
00:01:31,439 --> 00:01:35,040
kernels across different instances

30
00:01:35,040 --> 00:01:37,040
to descend secure containers that

31
00:01:37,040 --> 00:01:39,280
achieve the same association with the

32
00:01:39,280 --> 00:01:41,680
traditional virtual machines often

33
00:01:41,680 --> 00:01:43,920
preferred

34
00:01:43,920 --> 00:01:46,799
secure containers often creates a normal

35
00:01:46,799 --> 00:01:48,720
container within the lightweight micro

36
00:01:48,720 --> 00:01:50,000
vm

37
00:01:50,000 --> 00:01:52,720
micro vm is for allocation and the

38
00:01:52,720 --> 00:01:56,159
container is for abstraction

39
00:01:56,159 --> 00:01:58,159
in such a way users can build serverless

40
00:01:58,159 --> 00:01:59,280
services

41
00:01:59,280 --> 00:02:01,200
based on existing container

42
00:02:01,200 --> 00:02:04,159
infrastructure and ecosystem

43
00:02:04,159 --> 00:02:06,880
however each micro vm needs to load

44
00:02:06,880 --> 00:02:10,080
guest kernel so it cannot provide low

45
00:02:10,080 --> 00:02:14,400
overhead when isolating functions

46
00:02:16,239 --> 00:02:18,720
before investigating challenges that

47
00:02:18,720 --> 00:02:21,440
using secure containers may introduce

48
00:02:21,440 --> 00:02:23,599
we first need to realize what makes

49
00:02:23,599 --> 00:02:25,440
service special

50
00:02:25,440 --> 00:02:28,160
first the small container specification

51
00:02:28,160 --> 00:02:30,480
in a serverless computing brings the

52
00:02:30,480 --> 00:02:33,200
requirement to deploy containers densely

53
00:02:33,200 --> 00:02:34,720
on a node

54
00:02:34,720 --> 00:02:38,160
for instance 47

55
00:02:38,160 --> 00:02:39,920
of lambda functions

56
00:02:39,920 --> 00:02:41,680
run with a minimized memory

57
00:02:41,680 --> 00:02:45,360
specification in aws

58
00:02:46,319 --> 00:02:48,720
second the actual memory usage of a

59
00:02:48,720 --> 00:02:51,599
container may also be smaller than its

60
00:02:51,599 --> 00:02:53,599
specifications

61
00:02:53,599 --> 00:02:55,519
as azure reports

62
00:02:55,519 --> 00:02:58,480
about 90 percent of the applications

63
00:02:58,480 --> 00:03:04,840
never consume more than 400 mb of memory

64
00:03:04,879 --> 00:03:07,440
third in service platforms each function

65
00:03:07,440 --> 00:03:09,360
your location is short

66
00:03:09,360 --> 00:03:10,720
and the large number of functioning

67
00:03:10,720 --> 00:03:13,280
locations may arrive in a short time

68
00:03:13,280 --> 00:03:15,680
for example more than 200 container

69
00:03:15,680 --> 00:03:18,720
launch requests arrive within one second

70
00:03:18,720 --> 00:03:21,040
one node

71
00:03:21,040 --> 00:03:24,080
finally it is a common phenomenon that

72
00:03:24,080 --> 00:03:26,400
thousands of containers will collocate

73
00:03:26,400 --> 00:03:30,480
on node a node can host 2048

74
00:03:30,480 --> 00:03:34,959
cleaners if there is no other overhead

75
00:03:35,440 --> 00:03:37,680
considering these characteristics in

76
00:03:37,680 --> 00:03:38,799
serverless

77
00:03:38,799 --> 00:03:41,040
we can know that when using secure

78
00:03:41,040 --> 00:03:43,519
containers to handle invocations there

79
00:03:43,519 --> 00:03:45,760
is a best guarantee of low response

80
00:03:45,760 --> 00:03:46,959
latency

81
00:03:46,959 --> 00:03:49,360
and two requirements of high concurrency

82
00:03:49,360 --> 00:03:50,480
startup

83
00:03:50,480 --> 00:03:54,000
and high density deployment

84
00:03:58,080 --> 00:04:00,319
so does the current secure containers

85
00:04:00,319 --> 00:04:03,280
like kata can meet our requirements

86
00:04:03,280 --> 00:04:05,439
we make an experiment using qatar

87
00:04:05,439 --> 00:04:07,120
containers

88
00:04:07,120 --> 00:04:09,040
when starting 100 or more cutter

89
00:04:09,040 --> 00:04:11,040
containers concurrently

90
00:04:11,040 --> 00:04:12,959
there is a distinct performance

91
00:04:12,959 --> 00:04:14,480
digration

92
00:04:14,480 --> 00:04:17,120
it may takes more than 10 seconds to

93
00:04:17,120 --> 00:04:19,600
create a sandbox

94
00:04:19,600 --> 00:04:22,479
the cpu usage of an overhead also

95
00:04:22,479 --> 00:04:25,840
significant in host

96
00:04:26,320 --> 00:04:28,800
when deploying more than one thousand

97
00:04:28,800 --> 00:04:30,400
cutter containers

98
00:04:30,400 --> 00:04:33,280
the micro vm's memory footprint already

99
00:04:33,280 --> 00:04:36,560
occupies most of the membrane space

100
00:04:36,560 --> 00:04:38,960
if we start a new container now

101
00:04:38,960 --> 00:04:42,240
it will takes 1.5 times as long to

102
00:04:42,240 --> 00:04:44,639
create

103
00:04:45,520 --> 00:04:46,960
the above

104
00:04:46,960 --> 00:04:49,520
observations indicate that

105
00:04:49,520 --> 00:04:51,440
current secure containers have a

106
00:04:51,440 --> 00:04:53,680
concurrency and density bottlenecks in

107
00:04:53,680 --> 00:04:57,160
serverless computing

108
00:04:58,639 --> 00:05:00,880
in the next section we will analyze the

109
00:05:00,880 --> 00:05:03,680
bottlenecks of secure containers and

110
00:05:03,680 --> 00:05:07,600
where do these bottlenecks come from

111
00:05:09,199 --> 00:05:11,360
for the guest root fs

112
00:05:11,360 --> 00:05:13,680
it can be exposed to the container

113
00:05:13,680 --> 00:05:16,560
runtimes in the micro vms through two

114
00:05:16,560 --> 00:05:20,320
interface to construct the image layers

115
00:05:20,320 --> 00:05:23,840
for system sharing like retire fs

116
00:05:23,840 --> 00:05:27,280
and a blocked device like virtual block

117
00:05:27,280 --> 00:05:29,440
both of them have advantages and

118
00:05:29,440 --> 00:05:32,160
limitations

119
00:05:32,400 --> 00:05:35,039
for example blocked devices have good

120
00:05:35,039 --> 00:05:37,120
read and write performance

121
00:05:37,120 --> 00:05:39,360
however time consuming of preparing

122
00:05:39,360 --> 00:05:42,560
logic volumes in high concurrency

123
00:05:42,560 --> 00:05:44,479
it does not support the page cache

124
00:05:44,479 --> 00:05:46,800
sharing between the host and guest

125
00:05:46,800 --> 00:05:48,960
operating systems

126
00:05:48,960 --> 00:05:51,680
when vertell block backhanded reads root

127
00:05:51,680 --> 00:05:55,120
fs files into the host page cache

128
00:05:55,120 --> 00:05:57,919
the mapped content reproduce the same

129
00:05:57,919 --> 00:06:00,320
page cache in the guest

130
00:06:00,320 --> 00:06:03,120
this issue of duplicated page cache

131
00:06:03,120 --> 00:06:07,840
brings a high memory footprint overhead

132
00:06:08,000 --> 00:06:10,960
fast system sharing like virtual fs

133
00:06:10,960 --> 00:06:13,360
resolves the problems of duplicating

134
00:06:13,360 --> 00:06:14,800
page cache

135
00:06:14,800 --> 00:06:16,800
however it loses the good wide

136
00:06:16,800 --> 00:06:18,479
performance

137
00:06:18,479 --> 00:06:22,000
in addition each container has to employ

138
00:06:22,000 --> 00:06:25,440
a client daemon to support a virtual fs

139
00:06:25,440 --> 00:06:26,479
io

140
00:06:26,479 --> 00:06:28,000
operations

141
00:06:28,000 --> 00:06:30,560
leading to heavy cpu usage when many

142
00:06:30,560 --> 00:06:34,039
containers collocate

143
00:06:34,080 --> 00:06:36,880
so our first insight is that the current

144
00:06:36,880 --> 00:06:39,440
secure container fails to discriminate

145
00:06:39,440 --> 00:06:41,840
between service platforms and

146
00:06:41,840 --> 00:06:46,440
traditional ice environments

147
00:06:49,840 --> 00:06:53,120
in the micro vm of a secure container

148
00:06:53,120 --> 00:06:55,280
the guest operator system

149
00:06:55,280 --> 00:06:58,080
the struct page for memory management

150
00:06:58,080 --> 00:06:59,919
and other components

151
00:06:59,919 --> 00:07:03,840
like base os ximvi2 agent

152
00:07:03,840 --> 00:07:07,679
consume additional memory space

153
00:07:09,199 --> 00:07:12,720
the memory overheads of a 128 mb

154
00:07:12,720 --> 00:07:13,919
container

155
00:07:13,919 --> 00:07:15,880
94 mb and

156
00:07:15,880 --> 00:07:19,759
168 mb with qatar fire cracker and

157
00:07:19,759 --> 00:07:22,080
katakumi respectivity

158
00:07:22,080 --> 00:07:24,800
the overhead increases with the membrane

159
00:07:24,800 --> 00:07:27,759
specification of the

160
00:07:28,840 --> 00:07:31,680
container the average memory footprint

161
00:07:31,680 --> 00:07:34,800
of a single macro vm can be reduced by

162
00:07:34,800 --> 00:07:37,840
sharing the text read-only segment among

163
00:07:37,840 --> 00:07:40,000
multiple micro vms

164
00:07:40,000 --> 00:07:42,479
mainstream micro vms achieve it by

165
00:07:42,479 --> 00:07:45,759
making the kernel files to the guest

166
00:07:45,759 --> 00:07:48,319
memory directly

167
00:07:48,319 --> 00:07:50,880
the per micro vm memory overhead of

168
00:07:50,880 --> 00:07:52,960
katakumi and the kata firecracker

169
00:07:52,960 --> 00:07:54,560
reduces

170
00:07:54,560 --> 00:07:57,680
however the overhead is still too large

171
00:07:57,680 --> 00:08:01,360
for a service container with only 128 mb

172
00:08:01,360 --> 00:08:04,800
memory specification

173
00:08:04,800 --> 00:08:06,800
our second insight is that

174
00:08:06,800 --> 00:08:09,759
the current micro vm introduces heavy

175
00:08:09,759 --> 00:08:12,479
memory footprint in service computing

176
00:08:12,479 --> 00:08:14,639
where small memory specification

177
00:08:14,639 --> 00:08:18,319
containers don't dominate

178
00:08:21,840 --> 00:08:24,720
for the host c groups it is designed for

179
00:08:24,720 --> 00:08:27,919
resource control and abstraction for of

180
00:08:27,919 --> 00:08:29,440
processes

181
00:08:29,440 --> 00:08:32,159
in service computing the frequency of

182
00:08:32,159 --> 00:08:34,000
function invocations shows high

183
00:08:34,000 --> 00:08:35,279
variation

184
00:08:35,279 --> 00:08:36,880
in this case

185
00:08:36,880 --> 00:08:38,880
the secure containers are frequently

186
00:08:38,880 --> 00:08:41,279
created and recycled

187
00:08:41,279 --> 00:08:44,880
for example 100 clients commit c group

188
00:08:44,880 --> 00:08:46,240
operations

189
00:08:46,240 --> 00:08:50,160
1006 operations are performed per second

190
00:08:50,160 --> 00:08:52,560
and 10 000 c groups are maintained the

191
00:08:52,560 --> 00:08:55,560
host

192
00:08:55,920 --> 00:08:58,320
however the linux kernel introduced

193
00:08:58,320 --> 00:09:01,920
several global logs like mutex logs

194
00:09:01,920 --> 00:09:06,240
to serialize c group operations

195
00:09:07,600 --> 00:09:09,920
when spin nursing groups experience the

196
00:09:09,920 --> 00:09:12,240
optimistic spinning if they fail to

197
00:09:12,240 --> 00:09:13,760
acquire the lock

198
00:09:13,760 --> 00:09:16,480
it will lead to heavy cpu consumption

199
00:09:16,480 --> 00:09:19,200
and related extinction of the critical

200
00:09:19,200 --> 00:09:21,279
section

201
00:09:21,279 --> 00:09:24,080
so our for our third insight is that

202
00:09:24,080 --> 00:09:26,959
the host site overhead of c groups

203
00:09:26,959 --> 00:09:30,080
prohibits the high density deployment

204
00:09:30,080 --> 00:09:32,240
and high concurrency startups in service

205
00:09:32,240 --> 00:09:33,760
computing

206
00:09:33,760 --> 00:09:36,320
simplifying the c group design and

207
00:09:36,320 --> 00:09:39,200
reducing the critical section introduced

208
00:09:39,200 --> 00:09:41,600
by the mutex logs are fundamental

209
00:09:41,600 --> 00:09:44,160
solutions

210
00:09:47,120 --> 00:09:49,839
so how does randy resolve these

211
00:09:49,839 --> 00:09:51,040
challenges

212
00:09:51,040 --> 00:09:54,399
we have three main optimization points

213
00:09:54,399 --> 00:09:56,880
read write splitting root of s

214
00:09:56,880 --> 00:09:58,800
condensed kernel and the pre-patched

215
00:09:58,800 --> 00:09:59,839
image

216
00:09:59,839 --> 00:10:02,160
and the light with c groups with secret

217
00:10:02,160 --> 00:10:05,160
pool

218
00:10:07,200 --> 00:10:09,920
when designing round d we have a key

219
00:10:09,920 --> 00:10:12,560
application for serverless runtime

220
00:10:12,560 --> 00:10:15,360
the negligible host side overhead in a

221
00:10:15,360 --> 00:10:16,880
traditional vm

222
00:10:16,880 --> 00:10:19,920
can cause applification effects in

223
00:10:19,920 --> 00:10:22,000
service with high density and high

224
00:10:22,000 --> 00:10:23,440
concurrency

225
00:10:23,440 --> 00:10:26,320
and any trivial optimization can bring

226
00:10:26,320 --> 00:10:28,640
significant benefits

227
00:10:28,640 --> 00:10:31,120
to this end grounding shows virus

228
00:10:31,120 --> 00:10:35,200
advantages in service computing

229
00:10:37,920 --> 00:10:40,800
so how does run d work

230
00:10:40,800 --> 00:10:43,680
step 1 the container d forwards an

231
00:10:43,680 --> 00:10:47,360
invocation to run d step 2 run d

232
00:10:47,360 --> 00:10:50,399
prepares round c container type s

233
00:10:50,399 --> 00:10:51,920
for vmm

234
00:10:51,920 --> 00:10:53,760
the root fs is

235
00:10:53,760 --> 00:10:56,640
split into read only and read write

236
00:10:56,640 --> 00:10:58,240
layers

237
00:10:58,240 --> 00:10:59,839
step

238
00:10:59,839 --> 00:11:02,880
hypervisor uses micro vm template to

239
00:11:02,880 --> 00:11:06,480
create sandbox and mount through fs

240
00:11:06,480 --> 00:11:09,600
step 4 a lightweight c group is renamed

241
00:11:09,600 --> 00:11:14,079
and attached from the secret pool

242
00:11:14,079 --> 00:11:16,640
run the make guest to host optimizations

243
00:11:16,640 --> 00:11:19,120
in these steps and we will detail

244
00:11:19,120 --> 00:11:21,920
introduce them

245
00:11:24,959 --> 00:11:27,600
for read write splitting root fs

246
00:11:27,600 --> 00:11:30,240
we leverage the feature that user

247
00:11:30,240 --> 00:11:33,200
provides code data is read only for the

248
00:11:33,200 --> 00:11:35,200
operating system

249
00:11:35,200 --> 00:11:38,240
meanwhile the data in the local memory

250
00:11:38,240 --> 00:11:41,040
or storage generated in a center box

251
00:11:41,040 --> 00:11:43,839
will not be used by subsequent function

252
00:11:43,839 --> 00:11:45,360
invocations

253
00:11:45,360 --> 00:11:47,600
due to the status feature of service

254
00:11:47,600 --> 00:11:49,440
computing

255
00:11:49,440 --> 00:11:52,480
the temporary and the intermediate data

256
00:11:52,480 --> 00:11:55,200
generated during the function execution

257
00:11:55,200 --> 00:11:58,480
is not required to be persist

258
00:11:58,480 --> 00:12:00,480
surrounded handles them in different

259
00:12:00,480 --> 00:12:02,880
ways

260
00:12:04,720 --> 00:12:07,360
the sandboxes can share the read-only

261
00:12:07,360 --> 00:12:09,279
layer on the same node

262
00:12:09,279 --> 00:12:11,360
and the writable layer has to be

263
00:12:11,360 --> 00:12:15,040
prepared separately for each sandbox

264
00:12:15,040 --> 00:12:17,120
when creating a

265
00:12:17,120 --> 00:12:19,120
valid tail block device for a new

266
00:12:19,120 --> 00:12:20,639
sandbox

267
00:12:20,639 --> 00:12:23,839
a built-in storage image is created and

268
00:12:23,839 --> 00:12:26,720
linked to the storage image template

269
00:12:26,720 --> 00:12:28,800
using reflink

270
00:12:28,800 --> 00:12:32,000
riftlink enables storage image template

271
00:12:32,000 --> 00:12:34,480
to share data with building storage

272
00:12:34,480 --> 00:12:37,519
image in a copying write version

273
00:12:37,519 --> 00:12:40,399
so it ensures that user functions can

274
00:12:40,399 --> 00:12:42,880
perform writing as usual without

275
00:12:42,880 --> 00:12:46,959
persisting data on the local disk

276
00:12:51,440 --> 00:12:54,240
to reduce the memory footprint

277
00:12:54,240 --> 00:12:55,600
our key goals

278
00:12:55,600 --> 00:12:57,680
are condensing the guest kernel and

279
00:12:57,680 --> 00:13:01,440
improving the shareable part of the vm

280
00:13:01,440 --> 00:13:04,880
especially self-modifying code of kernel

281
00:13:04,880 --> 00:13:08,160
text segments only occurs at the startup

282
00:13:08,160 --> 00:13:08,959
phase

283
00:13:08,959 --> 00:13:10,240
after which

284
00:13:10,240 --> 00:13:12,240
the kernel code area

285
00:13:12,240 --> 00:13:16,399
becomes read-only after initialization

286
00:13:16,399 --> 00:13:17,680
in this case

287
00:13:17,680 --> 00:13:20,000
sandboxes experience the same

288
00:13:20,000 --> 00:13:22,800
initialization phase and generate

289
00:13:22,800 --> 00:13:27,519
predictable self-modifying code segments

290
00:13:27,519 --> 00:13:29,839
so randy generates a pre-patched guest

291
00:13:29,839 --> 00:13:32,399
kernel image file that already patched

292
00:13:32,399 --> 00:13:36,480
with self-modified code segments

293
00:13:38,880 --> 00:13:41,440
serialized c groups operations in the

294
00:13:41,440 --> 00:13:43,920
host become one of the bottom next

295
00:13:43,920 --> 00:13:46,560
of secure containers with high

296
00:13:46,560 --> 00:13:49,120
concurrency startup and high density

297
00:13:49,120 --> 00:13:50,880
deployment

298
00:13:50,880 --> 00:13:54,000
run d proposes lightweight c group with

299
00:13:54,000 --> 00:13:58,079
simple pull to resolve these challenges

300
00:13:58,079 --> 00:14:00,000
the lightweight c group decreases the

301
00:14:00,000 --> 00:14:02,720
total number of c groups in the system

302
00:14:02,720 --> 00:14:03,839
course

303
00:14:03,839 --> 00:14:06,160
rather than creating the c group for

304
00:14:06,160 --> 00:14:07,680
each subsystem

305
00:14:07,680 --> 00:14:10,079
we aggregate necessary c group

306
00:14:10,079 --> 00:14:11,360
subsystems

307
00:14:11,360 --> 00:14:16,079
into one dedicated lightweight c group

308
00:14:16,079 --> 00:14:17,760
the implementation

309
00:14:17,760 --> 00:14:21,040
of the jointer c group controller helps

310
00:14:21,040 --> 00:14:23,839
run d reduce the redundant c group

311
00:14:23,839 --> 00:14:27,440
operations when the container is started

312
00:14:27,440 --> 00:14:29,519
significantly decreasing the total

313
00:14:29,519 --> 00:14:34,000
number of c groups and the system course

314
00:14:34,959 --> 00:14:38,160
the secretary name as a special case

315
00:14:38,160 --> 00:14:40,079
is a lightweight operation without

316
00:14:40,079 --> 00:14:42,639
acquiring any global lock

317
00:14:42,639 --> 00:14:45,600
c group pool with renaming

318
00:14:45,600 --> 00:14:48,399
and eliminates the time consuming c

319
00:14:48,399 --> 00:14:51,920
group creation and initialization

320
00:14:51,920 --> 00:14:54,320
for each created a container run d

321
00:14:54,320 --> 00:14:57,600
simply allocates an idle c group

322
00:14:57,600 --> 00:15:00,160
update the state to busy

323
00:15:00,160 --> 00:15:03,120
perform the c group rename operation

324
00:15:03,120 --> 00:15:04,880
and then attach

325
00:15:04,880 --> 00:15:07,760
attaches the container to this renamed c

326
00:15:07,760 --> 00:15:12,439
group when the container is started

327
00:15:14,480 --> 00:15:17,199
with above designs randy can make

328
00:15:17,199 --> 00:15:19,600
service runtime more lightweight

329
00:15:19,600 --> 00:15:22,000
in this section we will introduce our

330
00:15:22,000 --> 00:15:25,800
evaluation process

331
00:15:26,399 --> 00:15:28,480
we compare round d with the

332
00:15:28,480 --> 00:15:31,120
state-of-the-art secure containers qatar

333
00:15:31,120 --> 00:15:33,279
containers

334
00:15:33,279 --> 00:15:36,079
specifically we use three popular

335
00:15:36,079 --> 00:15:38,560
configurations of kada containers

336
00:15:38,560 --> 00:15:39,920
katakumi

337
00:15:39,920 --> 00:15:42,320
template and qatar fc

338
00:15:42,320 --> 00:15:45,040
qatar cumula uses cumule as a micro vm

339
00:15:45,040 --> 00:15:46,240
hypervisor

340
00:15:46,240 --> 00:15:48,560
qatar template uses qmil

341
00:15:48,560 --> 00:15:51,759
while integrating container templates

342
00:15:51,759 --> 00:15:54,800
card ifc uses lightweight firecracker as

343
00:15:54,800 --> 00:15:56,959
a microvim hypervisor

344
00:15:56,959 --> 00:15:58,639
in all the tests

345
00:15:58,639 --> 00:16:01,600
we only create pods and boxes without

346
00:16:01,600 --> 00:16:04,639
other containers inside through the cicl

347
00:16:04,639 --> 00:16:05,839
command

348
00:16:05,839 --> 00:16:08,320
the actual memory usage of a container

349
00:16:08,320 --> 00:16:11,199
is collected using the sm

350
00:16:11,199 --> 00:16:13,599
command

351
00:16:15,199 --> 00:16:18,399
the results show that randi is able to

352
00:16:18,399 --> 00:16:20,959
start a single sandbox in 88

353
00:16:20,959 --> 00:16:24,560
milliseconds and launched 200 boxes

354
00:16:24,560 --> 00:16:26,079
within one second

355
00:16:26,079 --> 00:16:28,880
with minor latency fluctuation and cpu

356
00:16:28,880 --> 00:16:31,360
overhead

357
00:16:33,360 --> 00:16:34,959
as observed

358
00:16:34,959 --> 00:16:37,199
randy has the least memory overhead

359
00:16:37,199 --> 00:16:39,279
among four runtimes

360
00:16:39,279 --> 00:16:41,759
and does not increase with the memory

361
00:16:41,759 --> 00:16:43,600
specification

362
00:16:43,600 --> 00:16:46,320
the memory overhead is less than

363
00:16:46,320 --> 00:16:47,360
20

364
00:16:47,360 --> 00:16:50,399
mb per sandbox with randy

365
00:16:50,399 --> 00:16:55,519
it support supports to deploy over 200

366
00:16:55,519 --> 00:17:00,240
500 sandbox on owns node

367
00:17:03,120 --> 00:17:05,679
we also deploy run d as alibaba

368
00:17:05,679 --> 00:17:08,000
serverless runtime and verified in

369
00:17:08,000 --> 00:17:09,199
production

370
00:17:09,199 --> 00:17:11,520
it indeed guaranteeing the low response

371
00:17:11,520 --> 00:17:14,000
latency and meets the high concurrency

372
00:17:14,000 --> 00:17:16,559
startup and high density deployment

373
00:17:16,559 --> 00:17:19,559
requirements

374
00:17:21,599 --> 00:17:24,000
finally we give our conclusions on round

375
00:17:24,000 --> 00:17:26,160
d

376
00:17:28,079 --> 00:17:31,360
on the purpose gets to host solutions to

377
00:17:31,360 --> 00:17:34,000
secure containers for high density and

378
00:17:34,000 --> 00:17:37,120
high concurrency targets in service

379
00:17:37,120 --> 00:17:39,600
the practice including

380
00:17:39,600 --> 00:17:41,360
a better container root fs

381
00:17:41,360 --> 00:17:43,919
implementation based on read write

382
00:17:43,919 --> 00:17:46,480
splitting for service

383
00:17:46,480 --> 00:17:49,440
the method to condense the gas kernel

384
00:17:49,440 --> 00:17:52,160
and improve kernel sharing by a

385
00:17:52,160 --> 00:17:54,960
pre-patched kernel image

386
00:17:54,960 --> 00:17:58,000
the host site lightweight c-group design

387
00:17:58,000 --> 00:18:02,480
and the rename-based sql pool management

388
00:18:05,120 --> 00:18:07,919
our next track presentation help rather

389
00:18:07,919 --> 00:18:11,360
than recycle elevating code startups in

390
00:18:11,360 --> 00:18:13,760
service computing through inter function

391
00:18:13,760 --> 00:18:16,720
container sharing also focus on basic

392
00:18:16,720 --> 00:18:19,440
guaranteeing of low response latency

393
00:18:19,440 --> 00:18:21,600
one code startup occurs

394
00:18:21,600 --> 00:18:24,640
it proposed to accelerate time consuming

395
00:18:24,640 --> 00:18:27,280
container specialization if it needs

396
00:18:27,280 --> 00:18:30,160
code startups

397
00:18:31,200 --> 00:18:33,840
you can also contact us by email with

398
00:18:33,840 --> 00:18:35,200
any questions

399
00:18:35,200 --> 00:18:38,760
thanks for listening

400
00:18:45,760 --> 00:18:47,840
you

