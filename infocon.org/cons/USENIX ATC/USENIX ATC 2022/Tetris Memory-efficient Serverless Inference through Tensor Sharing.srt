1
00:00:07,940 --> 00:00:10,940
thank you

2
00:00:13,440 --> 00:00:16,079
hi everyone I'm Alicia from Tianjin

3
00:00:16,079 --> 00:00:17,699
University I'm glad to introduce our

4
00:00:17,699 --> 00:00:19,980
work Tetris memory efficient serverless

5
00:00:19,980 --> 00:00:22,980
influence through tensor sharing this

6
00:00:22,980 --> 00:00:24,720
work was conducted by changing

7
00:00:24,720 --> 00:00:27,119
University together with uber.com a

8
00:00:27,119 --> 00:00:30,119
large localized website in China

9
00:00:30,119 --> 00:00:32,880
Subway is Computing is becoming more and

10
00:00:32,880 --> 00:00:36,120
more popular in recent years due to its

11
00:00:36,120 --> 00:00:39,059
ease of use cost efficiency Resource

12
00:00:39,059 --> 00:00:41,340
Management free and auto scaling other

13
00:00:41,340 --> 00:00:42,899
managers

14
00:00:42,899 --> 00:00:45,180
surveillance influence or in other words

15
00:00:45,180 --> 00:00:47,460
deploying machine learning or deep

16
00:00:47,460 --> 00:00:49,860
learning influence Services atop a

17
00:00:49,860 --> 00:00:53,100
service platform is becoming popular in

18
00:00:53,100 --> 00:00:57,719
the iot mobile and web applications

19
00:00:57,719 --> 00:01:00,539
however as we have found the current

20
00:01:00,539 --> 00:01:03,059
service inference platforms a highly

21
00:01:03,059 --> 00:01:05,580
memory inefficient

22
00:01:05,580 --> 00:01:08,520
here is a sample overview of a

23
00:01:08,520 --> 00:01:10,740
serverless inference system

24
00:01:10,740 --> 00:01:13,979
your clients for the inference requests

25
00:01:13,979 --> 00:01:16,500
to the remote service over the network

26
00:01:16,500 --> 00:01:19,380
and in the back end multiple function

27
00:01:19,380 --> 00:01:21,960
instances will be spooned to user

28
00:01:21,960 --> 00:01:23,220
inference

29
00:01:23,220 --> 00:01:26,280
and inside each function instance there

30
00:01:26,280 --> 00:01:28,920
is a machine learning model with trained

31
00:01:28,920 --> 00:01:31,080
parameters that should be loaded into

32
00:01:31,080 --> 00:01:32,100
memory

33
00:01:32,100 --> 00:01:34,860
before the computation

34
00:01:34,860 --> 00:01:37,920
first the workload towards online ml

35
00:01:37,920 --> 00:01:41,220
inference is high for example there are

36
00:01:41,220 --> 00:01:43,680
tensile treatments influences per day in

37
00:01:43,680 --> 00:01:45,860
Facebook

38
00:01:45,900 --> 00:01:48,740
with so many requests the serverless

39
00:01:48,740 --> 00:01:51,960
platform will scale out a huge number of

40
00:01:51,960 --> 00:01:55,200
incidences according to its Auto scaling

41
00:01:55,200 --> 00:01:56,640
nature

42
00:01:56,640 --> 00:01:59,159
moreover machine learning especially

43
00:01:59,159 --> 00:02:01,500
deep learning applications typically

44
00:02:01,500 --> 00:02:03,659
require the loading of enormous

45
00:02:03,659 --> 00:02:05,040
parameters

46
00:02:05,040 --> 00:02:08,098
which is memory consuming for example

47
00:02:08,098 --> 00:02:10,739
the universal sentence encoder model

48
00:02:10,739 --> 00:02:13,860
which has been downloaded 1.4 million

49
00:02:13,860 --> 00:02:17,780
times on the tensorflow Hub and consumes

50
00:02:17,780 --> 00:02:20,520
1.72 gigabytes on memory at the wrong

51
00:02:20,520 --> 00:02:21,840
time

52
00:02:21,840 --> 00:02:23,700
eventually

53
00:02:23,700 --> 00:02:27,420
with such a vast number of instances and

54
00:02:27,420 --> 00:02:29,700
such a high memory consumption per

55
00:02:29,700 --> 00:02:32,700
incidence the system's memory resources

56
00:02:32,700 --> 00:02:36,060
will be extensively harvested

57
00:02:36,060 --> 00:02:39,480
and as far as we know previous works of

58
00:02:39,480 --> 00:02:42,720
the service influence systems many focus

59
00:02:42,720 --> 00:02:45,660
on computational resources like CPU and

60
00:02:45,660 --> 00:02:48,900
gpus while paying less attention to the

61
00:02:48,900 --> 00:02:51,180
memory resources

62
00:02:51,180 --> 00:02:54,780
the root cause of memory inefficiency is

63
00:02:54,780 --> 00:02:57,540
the large amount of data redundancy in

64
00:02:57,540 --> 00:03:00,900
memory which is caused by first remote

65
00:03:00,900 --> 00:03:03,720
instance nature of serverless platforms

66
00:03:03,720 --> 00:03:06,900
for example AWS Lambda creates new

67
00:03:06,900 --> 00:03:10,379
instances for each request if there is

68
00:03:10,379 --> 00:03:13,140
no existing worm instances

69
00:03:13,140 --> 00:03:15,900
and since incidences are isolated with

70
00:03:15,900 --> 00:03:18,659
sandbox data like the model parameters

71
00:03:18,659 --> 00:03:20,879
are required to be loaded repeatedly

72
00:03:20,879 --> 00:03:23,400
into the memory

73
00:03:23,400 --> 00:03:26,159
second the early provisioning and long

74
00:03:26,159 --> 00:03:28,140
cable lighting or functioning instances

75
00:03:28,140 --> 00:03:30,599
to reduce cold starts

76
00:03:30,599 --> 00:03:33,599
for example function instances would be

77
00:03:33,599 --> 00:03:37,080
pre-launched or Capital live for 15 to

78
00:03:37,080 --> 00:03:42,659
16 minutes after execution in AWS number

79
00:03:42,659 --> 00:03:45,480
which is much more longer than the

80
00:03:45,480 --> 00:03:47,340
inference computation time

81
00:03:47,340 --> 00:03:50,220
and also these Idol incidences consume

82
00:03:50,220 --> 00:03:53,580
less CPU power they do consume memory

83
00:03:53,580 --> 00:03:56,040
and in this work we are trying to reduce

84
00:03:56,040 --> 00:03:58,620
the high memory redundancy and improve

85
00:03:58,620 --> 00:04:00,379
memory efficiency

86
00:04:00,379 --> 00:04:04,019
and to solve the problem previous Works

87
00:04:04,019 --> 00:04:07,319
have proposed the runtime sharing method

88
00:04:07,319 --> 00:04:09,720
or in other words packing multiple

89
00:04:09,720 --> 00:04:12,540
requests into a single instance to share

90
00:04:12,540 --> 00:04:16,079
the runtimes for example grouping and

91
00:04:16,079 --> 00:04:18,779
processing requests in a batch or

92
00:04:18,779 --> 00:04:20,820
personal request concurrently with

93
00:04:20,820 --> 00:04:22,680
multiple threads

94
00:04:22,680 --> 00:04:25,919
with such methods in number of launched

95
00:04:25,919 --> 00:04:28,740
incidences could be reduced so that the

96
00:04:28,740 --> 00:04:32,040
memory consumption could be reduced

97
00:04:32,040 --> 00:04:35,400
however we further observed that even

98
00:04:35,400 --> 00:04:37,560
with random sharing the parameter

99
00:04:37,560 --> 00:04:40,020
tensors are still redundant across

100
00:04:40,020 --> 00:04:42,660
either instance of the same function and

101
00:04:42,660 --> 00:04:46,199
the instance of different functions

102
00:04:46,199 --> 00:04:48,180
machine learning models are typically

103
00:04:48,180 --> 00:04:51,660
implemented with specific Frameworks and

104
00:04:51,660 --> 00:04:54,120
in these Frameworks models are typically

105
00:04:54,120 --> 00:04:57,479
represented of the computational graphs

106
00:04:57,479 --> 00:05:00,060
and data in the graph are stored on

107
00:05:00,060 --> 00:05:01,919
tensors

108
00:05:01,919 --> 00:05:04,080
and here is a simple example

109
00:05:04,080 --> 00:05:06,900
computational graph in tensorflow

110
00:05:06,900 --> 00:05:09,660
since model parameters stay unchanged

111
00:05:09,660 --> 00:05:12,660
during inference it is easy to find out

112
00:05:12,660 --> 00:05:15,000
that the parameter rise of the tensors

113
00:05:15,000 --> 00:05:18,360
are redundant across function instances

114
00:05:18,360 --> 00:05:22,500
moreover these parameterized tensors

115
00:05:22,500 --> 00:05:24,660
typically consume the bulk of memory

116
00:05:24,660 --> 00:05:26,460
during the inference

117
00:05:26,460 --> 00:05:30,000
for example for a vgg 19 influence

118
00:05:30,000 --> 00:05:32,400
process storing the model parameters

119
00:05:32,400 --> 00:05:35,940
accounts for more than 93 percent of the

120
00:05:35,940 --> 00:05:39,199
total memory consumption

121
00:05:39,620 --> 00:05:43,280
moreover we also noticed that

122
00:05:43,280 --> 00:05:45,900
parameter-wise tensors would also be

123
00:05:45,900 --> 00:05:48,419
identical across different functions for

124
00:05:48,419 --> 00:05:51,360
example for model pipelines which refers

125
00:05:51,360 --> 00:05:54,960
to a pipeline that contains specific pre

126
00:05:54,960 --> 00:05:57,660
and post the process operations or

127
00:05:57,660 --> 00:06:00,060
pipeline or multiple models

128
00:06:00,060 --> 00:06:02,460
different functions we deploy different

129
00:06:02,460 --> 00:06:05,100
pipelines and these pipelines will

130
00:06:05,100 --> 00:06:07,080
include identical models

131
00:06:07,080 --> 00:06:09,780
moreover pre-training and transfer

132
00:06:09,780 --> 00:06:12,900
learning are popular techniques and is

133
00:06:12,900 --> 00:06:14,880
widely used in practice

134
00:06:14,880 --> 00:06:17,340
and in the process of pre-training and

135
00:06:17,340 --> 00:06:19,680
transfer learning it is also common to

136
00:06:19,680 --> 00:06:22,620
only retrieve a subset of parameters in

137
00:06:22,620 --> 00:06:25,199
case of similar Downstream tasks or

138
00:06:25,199 --> 00:06:27,479
insufficient training samples to reduce

139
00:06:27,479 --> 00:06:31,979
training costs and improve a currency

140
00:06:31,979 --> 00:06:34,800
which also causes different functions to

141
00:06:34,800 --> 00:06:36,960
have identical tensors

142
00:06:36,960 --> 00:06:39,960
for example here is an illustration of

143
00:06:39,960 --> 00:06:43,500
resnet 50 and when we only retrieve the

144
00:06:43,500 --> 00:06:45,720
last fully connected layers the

145
00:06:45,720 --> 00:06:48,060
parameters in previous convolutional

146
00:06:48,060 --> 00:06:50,160
layers could be identical across

147
00:06:50,160 --> 00:06:52,620
different functions

148
00:06:52,620 --> 00:06:55,740
to summarize our main contributions are

149
00:06:55,740 --> 00:06:58,440
first an observation of the tensor

150
00:06:58,440 --> 00:07:01,380
redundancy problem in service inference

151
00:07:01,380 --> 00:07:02,580
platforms

152
00:07:02,580 --> 00:07:05,400
and second to solve the problem you

153
00:07:05,400 --> 00:07:08,100
proposed lightweights and users based

154
00:07:08,100 --> 00:07:10,020
memory mapping solution that

155
00:07:10,020 --> 00:07:12,419
automatically shares identical parameter

156
00:07:12,419 --> 00:07:16,380
rather tensors across function instances

157
00:07:16,380 --> 00:07:19,380
to improve the memory efficiency We

158
00:07:19,380 --> 00:07:21,800
Carry Out territories which which

159
00:07:21,800 --> 00:07:25,380
eliminates the high data redundancy with

160
00:07:25,380 --> 00:07:28,020
tensor sharing and also along with

161
00:07:28,020 --> 00:07:30,680
long-time sharing

162
00:07:30,680 --> 00:07:34,199
here is an illustration of the overall

163
00:07:34,199 --> 00:07:37,380
architecture of Tetris which enables

164
00:07:37,380 --> 00:07:39,900
runtime sharing through its scaling and

165
00:07:39,900 --> 00:07:42,120
scheduling engine and supports tensor

166
00:07:42,120 --> 00:07:43,979
sharing through the agent in each

167
00:07:43,979 --> 00:07:46,740
container and the tensor store on each

168
00:07:46,740 --> 00:07:48,060
server

169
00:07:48,060 --> 00:07:50,880
and as an overview

170
00:07:50,880 --> 00:07:54,180
catches improves memory efficiency in

171
00:07:54,180 --> 00:07:55,800
three steps

172
00:07:55,800 --> 00:07:59,039
first territories share tensor memory or

173
00:07:59,039 --> 00:08:01,080
function instances that are co-located

174
00:08:01,080 --> 00:08:03,419
on the same server

175
00:08:03,419 --> 00:08:06,000
second Tetris carefully schedule

176
00:08:06,000 --> 00:08:08,940
function instances to Dedicated servers

177
00:08:08,940 --> 00:08:11,940
to share more tensor memory

178
00:08:11,940 --> 00:08:15,060
third test is also support runtime

179
00:08:15,060 --> 00:08:17,759
sharing which aims at scaling fewer

180
00:08:17,759 --> 00:08:20,220
function incidences to serve requests

181
00:08:20,220 --> 00:08:22,620
under the surface level objective

182
00:08:22,620 --> 00:08:25,199
constraints

183
00:08:25,199 --> 00:08:28,139
so the first question is how to share

184
00:08:28,139 --> 00:08:30,960
tensors or function incidences on the

185
00:08:30,960 --> 00:08:32,520
same server

186
00:08:32,520 --> 00:08:35,339
and to start we make a shared memory

187
00:08:35,339 --> 00:08:38,039
region across function instances which

188
00:08:38,039 --> 00:08:40,860
is implemented by mounting a shared

189
00:08:40,860 --> 00:08:43,559
temperature phase to each container

190
00:08:43,559 --> 00:08:46,740
second we take over the model loading

191
00:08:46,740 --> 00:08:49,380
process or function instances and put

192
00:08:49,380 --> 00:08:53,279
tensors into the shared region

193
00:08:53,279 --> 00:08:56,220
third we make sure that tensors in the

194
00:08:56,220 --> 00:09:00,560
shared region are reclaimed directly

195
00:09:01,620 --> 00:09:04,440
so the main process of tensor sharing is

196
00:09:04,440 --> 00:09:06,120
done by the agent

197
00:09:06,120 --> 00:09:09,060
the agent first posses the computational

198
00:09:09,060 --> 00:09:12,779
graph of the model and finds out the

199
00:09:12,779 --> 00:09:16,019
parameterized tensors to be loaded and

200
00:09:16,019 --> 00:09:18,839
for each tensor it checks if it is

201
00:09:18,839 --> 00:09:21,839
already existing in Nintendo store

202
00:09:21,839 --> 00:09:24,839
even know the agent allocates a new

203
00:09:24,839 --> 00:09:28,019
memory region in the transistor then

204
00:09:28,019 --> 00:09:30,839
reads and fills in the parameters

205
00:09:30,839 --> 00:09:34,800
if yes the agent just reuses an existing

206
00:09:34,800 --> 00:09:37,320
memory region through the nmap system

207
00:09:37,320 --> 00:09:38,880
call

208
00:09:38,880 --> 00:09:41,640
and we use logs to synchronize between

209
00:09:41,640 --> 00:09:44,640
concurrent agents

210
00:09:44,640 --> 00:09:46,980
and to identify if the tensor has

211
00:09:46,980 --> 00:09:49,800
already been loaded we adopt hash values

212
00:09:49,800 --> 00:09:52,680
since the hash values are computed only

213
00:09:52,680 --> 00:09:55,920
based on the tensor contents the process

214
00:09:55,920 --> 00:09:58,860
of tensor sharing still works across

215
00:09:58,860 --> 00:10:02,300
different machine learning Frameworks

216
00:10:02,300 --> 00:10:06,240
after the tensors are shared they may be

217
00:10:06,240 --> 00:10:08,700
referenced by different by different

218
00:10:08,700 --> 00:10:10,440
instances

219
00:10:10,440 --> 00:10:13,560
it is also important to detect and

220
00:10:13,560 --> 00:10:16,140
reclaim the unreferenced tensors

221
00:10:16,140 --> 00:10:17,459
directly

222
00:10:17,459 --> 00:10:21,060
to achieve this we deploy a separated

223
00:10:21,060 --> 00:10:24,060
reclaimer on each server and run a

224
00:10:24,060 --> 00:10:27,360
detection and reclaim Loop theoretically

225
00:10:27,360 --> 00:10:30,480
inside the loop the reclaimer first

226
00:10:30,480 --> 00:10:34,260
retrieves the set of referencing tensors

227
00:10:34,260 --> 00:10:38,339
in ordered by T1 then get the tensor set

228
00:10:38,339 --> 00:10:41,580
reside in the tensor store denoted by T

229
00:10:41,580 --> 00:10:44,820
O and finally the tensor cells to be

230
00:10:44,820 --> 00:10:50,120
reclaimed is inferred by T O minus T1

231
00:10:50,519 --> 00:10:54,540
and since we have also observed that the

232
00:10:54,540 --> 00:10:56,820
loading or massive model parameters

233
00:10:56,820 --> 00:10:59,519
dominates the startup process of

234
00:10:59,519 --> 00:11:01,320
function instances

235
00:11:01,320 --> 00:11:04,380
and tensor memory mapping is far faster

236
00:11:04,380 --> 00:11:06,779
than loading and decoding tensors from

237
00:11:06,779 --> 00:11:10,440
disk the reclaimer also supports keeping

238
00:11:10,440 --> 00:11:14,459
alive on the unreferenced tensors to

239
00:11:14,459 --> 00:11:17,760
accelerate function incident startups in

240
00:11:17,760 --> 00:11:19,940
the short future

241
00:11:19,940 --> 00:11:22,079
to sum it up

242
00:11:22,079 --> 00:11:25,079
the tensors firstly loaded from the

243
00:11:25,079 --> 00:11:27,540
model file by the agent then maybe

244
00:11:27,540 --> 00:11:30,480
memory mapped by other instances

245
00:11:30,480 --> 00:11:33,420
and when the reclaimer detects that no

246
00:11:33,420 --> 00:11:35,880
function incidence is referencing

247
00:11:35,880 --> 00:11:39,060
the tensor memory can either be

248
00:11:39,060 --> 00:11:43,500
reclaimed immediately or kept alive

249
00:11:43,500 --> 00:11:46,500
our design does not support sharing of

250
00:11:46,500 --> 00:11:49,980
tensors across servers due to frequent

251
00:11:49,980 --> 00:11:53,519
tensor access during inference and the

252
00:11:53,519 --> 00:11:55,980
high Network latency

253
00:11:55,980 --> 00:11:58,740
instead we maintain a shared tensor

254
00:11:58,740 --> 00:12:01,140
store on each server for performance

255
00:12:01,140 --> 00:12:03,660
guarantee while minimizing cluster

256
00:12:03,660 --> 00:12:05,880
memory consumption through instance

257
00:12:05,880 --> 00:12:07,440
Gathering

258
00:12:07,440 --> 00:12:11,160
we employ a simple greedy algorithm in

259
00:12:11,160 --> 00:12:13,140
which we always try to schedule

260
00:12:13,140 --> 00:12:16,680
instances to this server within maximum

261
00:12:16,680 --> 00:12:18,600
tensor similarity

262
00:12:18,600 --> 00:12:22,140
and the basic idea is that we want to

263
00:12:22,140 --> 00:12:24,779
share as many intensive memory as we can

264
00:12:24,779 --> 00:12:27,920
after this schedule

265
00:12:28,260 --> 00:12:31,980
moreover for long time sharing although

266
00:12:31,980 --> 00:12:34,440
previous Works have indicated that it

267
00:12:34,440 --> 00:12:37,920
can be implemented through batching or

268
00:12:37,920 --> 00:12:39,300
multi-threading

269
00:12:39,300 --> 00:12:42,300
we observed that the different

270
00:12:42,300 --> 00:12:44,519
combinations

271
00:12:44,519 --> 00:12:47,880
of data science and concurrency these

272
00:12:47,880 --> 00:12:50,100
two different memory efficiency so in

273
00:12:50,100 --> 00:12:53,579
countries the runtime sharing is in is

274
00:12:53,579 --> 00:12:56,279
implemented with a combination of

275
00:12:56,279 --> 00:12:59,160
batching and automatic threading

276
00:12:59,160 --> 00:13:02,279
and the same Earth in previous influence

277
00:13:02,279 --> 00:13:04,800
systems we also support the SLO

278
00:13:04,800 --> 00:13:06,480
constraints

279
00:13:06,480 --> 00:13:09,660
and specifically for each function you

280
00:13:09,660 --> 00:13:12,839
profile its latency under various

281
00:13:12,839 --> 00:13:16,260
configurations and then model the the

282
00:13:16,260 --> 00:13:18,839
instance scaling process as an

283
00:13:18,839 --> 00:13:21,740
optimization problem

284
00:13:22,620 --> 00:13:25,860
the scaling engine monitors the

285
00:13:25,860 --> 00:13:29,279
real-time RPS and judges whether the

286
00:13:29,279 --> 00:13:31,860
existing instances are sufficient to

287
00:13:31,860 --> 00:13:35,940
serve these requests if no if dispatches

288
00:13:35,940 --> 00:13:39,120
parts of requests to existing instances

289
00:13:39,120 --> 00:13:42,899
and launch new instances to process the

290
00:13:42,899 --> 00:13:44,639
residual ones

291
00:13:44,639 --> 00:13:47,760
the subject of the optimization problem

292
00:13:47,760 --> 00:13:51,120
is to minimize the memory consumption of

293
00:13:51,120 --> 00:13:53,579
the newly spoon instances

294
00:13:53,579 --> 00:13:57,060
and the constraints issues that the slos

295
00:13:57,060 --> 00:14:00,000
are guaranteed and the residual IBS can

296
00:14:00,000 --> 00:14:02,160
be fully processed by the newly spawn

297
00:14:02,160 --> 00:14:04,200
incidences

298
00:14:04,200 --> 00:14:07,139
however this open this organization

299
00:14:07,139 --> 00:14:09,899
problem can be reduced to the MP

300
00:14:09,899 --> 00:14:13,260
complete website problem so we developed

301
00:14:13,260 --> 00:14:16,320
a simple greedy solution for which we

302
00:14:16,320 --> 00:14:20,279
greatly select configurations with the

303
00:14:20,279 --> 00:14:23,579
maximum normalized throughput

304
00:14:23,579 --> 00:14:26,720
and this method can also be easily

305
00:14:26,720 --> 00:14:30,240
extended to balance CPU consumption by

306
00:14:30,240 --> 00:14:32,459
normalizing this throughput with a

307
00:14:32,459 --> 00:14:37,200
linear combination of memory and CPUs

308
00:14:37,200 --> 00:14:41,040
to evaluate our system we collected 21

309
00:14:41,040 --> 00:14:43,800
inference models from both tensorflow

310
00:14:43,800 --> 00:14:47,160
Hub and the local Lab website ranging in

311
00:14:47,160 --> 00:14:51,360
model sizes from 11 megabytes to 3.5

312
00:14:51,360 --> 00:14:52,920
gigabytes

313
00:14:52,920 --> 00:14:56,120
where in download times from

314
00:14:56,120 --> 00:14:59,699
310 to 1.1 million

315
00:14:59,699 --> 00:15:02,160
and covering application domains

316
00:15:02,160 --> 00:15:05,220
including text image audio

317
00:15:05,220 --> 00:15:06,660
Etc

318
00:15:06,660 --> 00:15:10,079
our test bed contains eight servers two

319
00:15:10,079 --> 00:15:16,279
servers are encrypted with 80 cores and

320
00:15:16,279 --> 00:15:20,760
256 gigabytes of memory where six others

321
00:15:20,760 --> 00:15:24,800
are equipped with 32 codes and

322
00:15:24,800 --> 00:15:28,939
128 gigabytes of memory

323
00:15:29,459 --> 00:15:33,060
this figure here shows that the memory

324
00:15:33,060 --> 00:15:35,220
reduction under different number of

325
00:15:35,220 --> 00:15:38,579
function instances with tensor sharing

326
00:15:38,579 --> 00:15:41,279
here we compare the memory consumption

327
00:15:41,279 --> 00:15:43,680
of all launched instances

328
00:15:43,680 --> 00:15:47,040
with or without tensor sharing

329
00:15:47,040 --> 00:15:51,480
and for the ufe model when deploy 32

330
00:15:51,480 --> 00:15:55,019
incidences the memory reduction exceeds

331
00:15:55,019 --> 00:15:58,279
93 percent

332
00:15:59,639 --> 00:16:02,279
with tensor sharing the memory

333
00:16:02,279 --> 00:16:04,920
consumption has been extensively reduced

334
00:16:04,920 --> 00:16:07,380
more function incidences could be

335
00:16:07,380 --> 00:16:10,980
deployed and for the USC model consider

336
00:16:10,980 --> 00:16:12,440
the

337
00:16:12,440 --> 00:16:16,380
256 gigabytes machine memory capacity

338
00:16:16,380 --> 00:16:19,260
the function intensity will be improved

339
00:16:19,260 --> 00:16:22,440
by up to 30 times

340
00:16:22,440 --> 00:16:25,740
since the tensors can be shared with

341
00:16:25,740 --> 00:16:28,740
direct memory mapping the time consuming

342
00:16:28,740 --> 00:16:32,279
model loading process can be accelerated

343
00:16:32,279 --> 00:16:35,420
and for example for the vg19 model

344
00:16:35,420 --> 00:16:37,980
compared with loading the entire model

345
00:16:37,980 --> 00:16:41,220
from disk the function startup can be

346
00:16:41,220 --> 00:16:43,820
accelerated by up to

347
00:16:43,820 --> 00:16:46,380
91 Point

348
00:16:46,380 --> 00:16:49,639
five six percent

349
00:16:49,980 --> 00:16:53,399
with such a large reduction in memory

350
00:16:53,399 --> 00:16:55,860
consumption we also measure the

351
00:16:55,860 --> 00:16:59,220
influence latency of various ml models

352
00:16:59,220 --> 00:17:02,459
and compare the latency CTF with or

353
00:17:02,459 --> 00:17:06,240
without tensor sharing and find that our

354
00:17:06,240 --> 00:17:09,119
memory mapping method does not introduce

355
00:17:09,119 --> 00:17:12,139
latency overhead

356
00:17:12,540 --> 00:17:15,660
furthermore to evaluate the overall

357
00:17:15,660 --> 00:17:19,079
performance of Tetris we adopt four

358
00:17:19,079 --> 00:17:21,959
real-world applications and three real

359
00:17:21,959 --> 00:17:24,299
world workload traces collected from

360
00:17:24,299 --> 00:17:28,860
Microsoft and we compare to influx which

361
00:17:28,860 --> 00:17:31,919
supports runtime sharing with request

362
00:17:31,919 --> 00:17:35,160
batching and photons with both runtime

363
00:17:35,160 --> 00:17:37,320
sharing with processing requests

364
00:17:37,320 --> 00:17:39,419
concurrently in interest

365
00:17:39,419 --> 00:17:43,140
and we also compare to a runtime sharing

366
00:17:43,140 --> 00:17:46,200
only version of Tetris to the photons in

367
00:17:46,200 --> 00:17:48,539
node serverless influence oriented we

368
00:17:48,539 --> 00:17:51,000
modify it to our scenarios by adding

369
00:17:51,000 --> 00:17:53,840
function profiles and greatly selecting

370
00:17:53,840 --> 00:17:56,940
instance configurations with the maximum

371
00:17:56,940 --> 00:18:00,240
concurrency under SLO constraints when

372
00:18:00,240 --> 00:18:03,440
scaling instances

373
00:18:04,200 --> 00:18:06,960
the result shows that Tetris achieves

374
00:18:06,960 --> 00:18:09,480
the lowest memory consumption which is

375
00:18:09,480 --> 00:18:11,400
because first

376
00:18:11,400 --> 00:18:14,760
with tensor sharing caches can further

377
00:18:14,760 --> 00:18:17,160
reduce the tensor redundancy across

378
00:18:17,160 --> 00:18:19,260
either the instances of the same

379
00:18:19,260 --> 00:18:22,020
function or across different functions

380
00:18:22,020 --> 00:18:23,340
second

381
00:18:23,340 --> 00:18:26,220
the combined runtime sharing method in

382
00:18:26,220 --> 00:18:29,039
terrorists also ensures that we can pack

383
00:18:29,039 --> 00:18:31,679
more requests to share the wrong time of

384
00:18:31,679 --> 00:18:33,419
a single instance

385
00:18:33,419 --> 00:18:36,299
so I'll just do fewer incidences to

386
00:18:36,299 --> 00:18:39,480
accommodate the requests those also

387
00:18:39,480 --> 00:18:41,460
consuming less memory

388
00:18:41,460 --> 00:18:44,760
and specifically when considering the

389
00:18:44,760 --> 00:18:46,860
second hand vehicle treating application

390
00:18:46,860 --> 00:18:48,960
under a stable load

391
00:18:48,960 --> 00:18:51,600
countries can reduce the overall memory

392
00:18:51,600 --> 00:18:54,360
consumption by more than 86 percent

393
00:18:54,360 --> 00:18:57,660
compared with inference which is the

394
00:18:57,660 --> 00:19:00,660
state of the art service inference

395
00:19:00,660 --> 00:19:02,299
system

396
00:19:02,299 --> 00:19:06,240
to conclude Tetris is a memory efficient

397
00:19:06,240 --> 00:19:09,600
service influence system which is easy

398
00:19:09,600 --> 00:19:13,500
to implement transparent to users and it

399
00:19:13,500 --> 00:19:15,900
requires no modifications to the ml

400
00:19:15,900 --> 00:19:17,100
models

401
00:19:17,100 --> 00:19:19,740
it also does not hurt the influence

402
00:19:19,740 --> 00:19:20,940
performance

403
00:19:20,940 --> 00:19:24,059
and we think both developers and Cloud

404
00:19:24,059 --> 00:19:27,480
providers would benefit from caches due

405
00:19:27,480 --> 00:19:30,480
to its startup acceleration and high

406
00:19:30,480 --> 00:19:32,820
memory efficiency

407
00:19:32,820 --> 00:19:36,020
thanks for listening

