1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:14,460 --> 00:00:17,460
storage

3
00:00:26,580 --> 00:00:29,460
data reductions already widely discard

4
00:00:29,460 --> 00:00:31,679
topic and there are several cancer

5
00:00:31,679 --> 00:00:34,260
approach for different scenarios

6
00:00:34,260 --> 00:00:37,559
General compression like LG series and

7
00:00:37,559 --> 00:00:39,420
deep light skin quite common for most

8
00:00:39,420 --> 00:00:42,360
users and the yearly retire and

9
00:00:42,360 --> 00:00:44,280
eliminate redundancies at the string

10
00:00:44,280 --> 00:00:46,980
level but this kind of approach only

11
00:00:46,980 --> 00:00:48,840
influenced redundances in a limited

12
00:00:48,840 --> 00:00:51,539
searching range and thus the compression

13
00:00:51,539 --> 00:00:55,020
ratio is also limited as a result the

14
00:00:55,020 --> 00:00:58,260
major scenarios are eurocyte files the

15
00:00:58,260 --> 00:01:00,300
duplication is proposed to handle the

16
00:01:00,300 --> 00:01:02,879
case of very large cells and it will

17
00:01:02,879 --> 00:01:05,580
separates data streams into channels and

18
00:01:05,580 --> 00:01:07,979
remove identical ones globally

19
00:01:07,979 --> 00:01:10,640
although the location could detect

20
00:01:10,640 --> 00:01:12,979
redundancies far away from each other

21
00:01:12,979 --> 00:01:15,840
its course Grant processing granularity

22
00:01:15,840 --> 00:01:18,960
is actually faces the duplication ratio

23
00:01:18,960 --> 00:01:21,659
General both two kinds of approaches

24
00:01:21,659 --> 00:01:24,060
have been widely used in storage

25
00:01:24,060 --> 00:01:27,240
products but they cannot fully utilize

26
00:01:27,240 --> 00:01:30,500
data's compressibility

27
00:01:31,220 --> 00:01:34,619
location is proposed to recover the lost

28
00:01:34,619 --> 00:01:36,540
compressibility of triangularity

29
00:01:36,540 --> 00:01:37,799
duplication

30
00:01:37,799 --> 00:01:40,799
it not only removes identical trunks in

31
00:01:40,799 --> 00:01:43,439
data streams but also eliminates

32
00:01:43,439 --> 00:01:45,960
redundancy the more similar chunks

33
00:01:45,960 --> 00:01:48,659
the loud figure gives an example the

34
00:01:48,659 --> 00:01:51,840
first and the third chunks are identical

35
00:01:51,840 --> 00:01:55,020
and the second and the first chunks are

36
00:01:55,020 --> 00:01:58,020
similar then granted duplication is to

37
00:01:58,020 --> 00:02:00,479
detect similar transfer and duplicate

38
00:02:00,479 --> 00:02:03,360
chunks and then read back a similar one

39
00:02:03,360 --> 00:02:06,540
as the best chunk after that there's a

40
00:02:06,540 --> 00:02:09,060
difference between the the caption and

41
00:02:09,060 --> 00:02:11,879
the best chunk will be calculated as a

42
00:02:11,879 --> 00:02:14,299
red figure

43
00:02:15,140 --> 00:02:18,060
to eliminate redundancies between two

44
00:02:18,060 --> 00:02:21,300
chunks in this way then the fairground

45
00:02:21,300 --> 00:02:23,760
duplication achieves a global and stream

46
00:02:23,760 --> 00:02:26,099
level approach

47
00:02:26,099 --> 00:02:28,260
this solution sounds great but It

48
00:02:28,260 --> 00:02:30,480
suffers from some costs

49
00:02:30,480 --> 00:02:33,420
eliminating redundancies are always less

50
00:02:33,420 --> 00:02:37,260
compressed fails share common parts and

51
00:02:37,260 --> 00:02:39,420
this kind of data we're using we offer

52
00:02:39,420 --> 00:02:40,800
the locality

53
00:02:40,800 --> 00:02:43,379
for the example in the figure we are one

54
00:02:43,379 --> 00:02:46,819
and fail to both counties

55
00:02:46,819 --> 00:02:51,060
but in physical stop chance pair 2 has a

56
00:02:51,060 --> 00:02:53,099
poor locality because its component

57
00:02:53,099 --> 00:02:55,920
chunks are not located sequentially

58
00:02:55,920 --> 00:02:58,800
this example suggests a case of data

59
00:02:58,800 --> 00:03:01,319
reuse and from that we could learn that

60
00:03:01,319 --> 00:03:04,560
data reviews always hurt locality

61
00:03:04,560 --> 00:03:07,319
and stand Grant education introduces a

62
00:03:07,319 --> 00:03:09,420
new form of digital reviews compared

63
00:03:09,420 --> 00:03:12,480
based Channel dedication it is that

64
00:03:12,480 --> 00:03:15,840
encoding on similar terms therefore 10

65
00:03:15,840 --> 00:03:17,879
granted education introduces an

66
00:03:17,879 --> 00:03:21,239
additional locality issues and it will

67
00:03:21,239 --> 00:03:23,700
get performance of the duplication and

68
00:03:23,700 --> 00:03:26,760
the restore workflows and in this work

69
00:03:26,760 --> 00:03:29,040
we aim to address these additional

70
00:03:29,040 --> 00:03:30,780
issues

71
00:03:30,780 --> 00:03:32,099
um

72
00:03:32,099 --> 00:03:36,120
the first additional issue is is in the

73
00:03:36,120 --> 00:03:38,400
generalization workflow or we call it

74
00:03:38,400 --> 00:03:39,720
the real test

75
00:03:39,720 --> 00:03:42,900
it is related to reading base chunks for

76
00:03:42,900 --> 00:03:46,019
data encoding for the for the example in

77
00:03:46,019 --> 00:03:48,420
the left figure we have two backup

78
00:03:48,420 --> 00:03:50,659
workloads for fair quality duplication

79
00:03:50,659 --> 00:03:54,120
backup one is the first process and when

80
00:03:54,120 --> 00:03:57,120
processing backup two we learned Chang a

81
00:03:57,120 --> 00:04:00,659
and chunk C duplicates and Chang B Prime

82
00:04:00,659 --> 00:04:03,000
and channel D Prime both have a similar

83
00:04:03,000 --> 00:04:06,180
channel in backup one then we need to

84
00:04:06,180 --> 00:04:09,360
read John B and Chang d as best chance

85
00:04:09,360 --> 00:04:12,540
this example suggests how similar terms

86
00:04:12,540 --> 00:04:15,599
are generated in common cases and it

87
00:04:15,599 --> 00:04:18,238
means very similar chance appear is

88
00:04:18,238 --> 00:04:21,298
actually studied by users modifications

89
00:04:21,298 --> 00:04:23,900
it makes the distribution of electrons

90
00:04:23,900 --> 00:04:27,600
scattered and random for the storage end

91
00:04:27,600 --> 00:04:31,259
besides consecutive deduplicated chunks

92
00:04:31,259 --> 00:04:33,479
are usually compressed together to

93
00:04:33,479 --> 00:04:36,780
further reduce storage costs and it

94
00:04:36,780 --> 00:04:40,199
means accessing any chunk must be in a

95
00:04:40,199 --> 00:04:43,800
large unit these two causes leads to bad

96
00:04:43,800 --> 00:04:44,820
results

97
00:04:44,820 --> 00:04:47,639
for the example in the red figure when

98
00:04:47,639 --> 00:04:50,820
we run data encoding on channel B Prime

99
00:04:50,820 --> 00:04:53,280
we even have to read the whole continent

100
00:04:53,280 --> 00:04:55,919
for only one base stroke these are very

101
00:04:55,919 --> 00:04:57,720
inefficient IO

102
00:04:57,720 --> 00:05:01,080
we summarize this issue as locality in

103
00:05:01,080 --> 00:05:03,900
best chunks and if best chance could be

104
00:05:03,900 --> 00:05:06,120
more concentrated the i o efficiency

105
00:05:06,120 --> 00:05:10,380
could be significantly improved

106
00:05:10,380 --> 00:05:13,440
the second issue is in the restore flow

107
00:05:13,440 --> 00:05:15,840
and we could call it the read pass

108
00:05:15,840 --> 00:05:18,840
we observe that there are two kinds of

109
00:05:18,840 --> 00:05:21,000
reference relationships in front when

110
00:05:21,000 --> 00:05:24,300
the duplicated data the first is between

111
00:05:24,300 --> 00:05:27,000
background workloads and chunks and the

112
00:05:27,000 --> 00:05:30,240
second is between Georgia and Bishops

113
00:05:30,240 --> 00:05:32,639
the following figure is the following

114
00:05:32,639 --> 00:05:35,280
figure gives an example the green shaded

115
00:05:35,280 --> 00:05:37,680
chance are directly referenced by the

116
00:05:37,680 --> 00:05:41,100
third backup and the blue shaded chunks

117
00:05:41,100 --> 00:05:43,800
are directly referenced by their base

118
00:05:43,800 --> 00:05:46,560
chunks and indirectly referenced by the

119
00:05:46,560 --> 00:05:49,020
third backup the second kind of

120
00:05:49,020 --> 00:05:50,520
reference relationship is the

121
00:05:50,520 --> 00:05:53,160
additionally introduced by data encoding

122
00:05:53,160 --> 00:05:56,520
and thus aggravates the fragmentation

123
00:05:56,520 --> 00:05:59,699
problem in the duplicated data

124
00:05:59,699 --> 00:06:03,240
and because of the large i o units we're

125
00:06:03,240 --> 00:06:05,460
restoring the third backup we have to

126
00:06:05,460 --> 00:06:08,699
read all the five containers and it

127
00:06:08,699 --> 00:06:11,220
causes many and required chunks to be

128
00:06:11,220 --> 00:06:12,020
read

129
00:06:12,020 --> 00:06:15,120
therefore we summarize this issue as

130
00:06:15,120 --> 00:06:18,479
locality and restore required chunks and

131
00:06:18,479 --> 00:06:21,120
if this restore require chance to could

132
00:06:21,120 --> 00:06:23,940
be located more compactly the i o

133
00:06:23,940 --> 00:06:27,539
efficiency could be improved

134
00:06:27,539 --> 00:06:30,660
the third issue is also in the real past

135
00:06:30,660 --> 00:06:33,479
when restoring a backup we need to

136
00:06:33,479 --> 00:06:36,300
Traverse all restore required chunks to

137
00:06:36,300 --> 00:06:38,340
construct a whole backup workload

138
00:06:38,340 --> 00:06:41,400
however because of the dependency

139
00:06:41,400 --> 00:06:43,560
between their chance and the best chance

140
00:06:43,560 --> 00:06:46,199
some containers will be accessed

141
00:06:46,199 --> 00:06:47,819
repeatedly

142
00:06:47,819 --> 00:06:50,580
for example in the figure we restore

143
00:06:50,580 --> 00:06:53,580
backup and with its recipe several

144
00:06:53,580 --> 00:06:56,520
containers are used several several

145
00:06:56,520 --> 00:06:59,699
containers are read in order to get all

146
00:06:59,699 --> 00:07:01,800
restore required channels

147
00:07:01,800 --> 00:07:04,500
though all Trends in container 1 are

148
00:07:04,500 --> 00:07:07,500
required in backup and we have to access

149
00:07:07,500 --> 00:07:09,660
terminal one several times because

150
00:07:09,660 --> 00:07:12,780
Trump's income income number one are

151
00:07:12,780 --> 00:07:15,180
used at the time far apart

152
00:07:15,180 --> 00:07:18,000
we summarize this issue as repeatedly

153
00:07:18,000 --> 00:07:20,580
repeatedly discussing in restore

154
00:07:20,580 --> 00:07:22,979
required travel channels

155
00:07:22,979 --> 00:07:26,699
these three issues travel on travel both

156
00:07:26,699 --> 00:07:29,220
the deduplication and the restore

157
00:07:29,220 --> 00:07:31,979
workflow and we propose three techniques

158
00:07:31,979 --> 00:07:35,520
for them respectively then combining

159
00:07:35,520 --> 00:07:37,919
these techniques we proposed a fair

160
00:07:37,919 --> 00:07:41,160
quantity duplication from work Mega it

161
00:07:41,160 --> 00:07:43,860
runs selective data encoding on the

162
00:07:43,860 --> 00:07:46,860
duplication workflow applies the data

163
00:07:46,860 --> 00:07:49,440
friendly data layout on the duplicated

164
00:07:49,440 --> 00:07:53,660
data and performs AFR traversing and the

165
00:07:53,660 --> 00:07:56,099
Delta print writing on the restore

166
00:07:56,099 --> 00:07:57,599
workflow

167
00:07:57,599 --> 00:08:00,440
foreign

168
00:08:01,880 --> 00:08:05,160
we observe that this chunks are not

169
00:08:05,160 --> 00:08:07,860
distributed are not distributed evenly

170
00:08:07,860 --> 00:08:11,280
for example yeah evaluated data start

171
00:08:11,280 --> 00:08:14,160
about a state state four percent

172
00:08:14,160 --> 00:08:17,580
containers are only holds about eight

173
00:08:17,580 --> 00:08:20,900
percent best chance and we call this

174
00:08:20,900 --> 00:08:23,639
containers-based bus containers this

175
00:08:23,639 --> 00:08:26,099
observation gives a hint that escaping

176
00:08:26,099 --> 00:08:29,280
data encodings whose space shocks are in

177
00:08:29,280 --> 00:08:32,399
best parts containers will only cause a

178
00:08:32,399 --> 00:08:35,419
limited deduplication ratio loss

179
00:08:35,419 --> 00:08:38,339
accordingly we convinced several Trends

180
00:08:38,339 --> 00:08:41,640
as a segment and then count how many

181
00:08:41,640 --> 00:08:43,399
times each containers

182
00:08:43,399 --> 00:08:45,959
each container is required for base

183
00:08:45,959 --> 00:08:47,940
chunks in a segment

184
00:08:47,940 --> 00:08:52,019
if a container is required for two field

185
00:08:52,019 --> 00:08:57,060
times I mean smaller than a threshold it

186
00:08:57,060 --> 00:08:59,279
will be regarded as a base pass

187
00:08:59,279 --> 00:09:02,100
container and we skip data encodings

188
00:09:02,100 --> 00:09:05,300
whose space chunks are in this content

189
00:09:05,300 --> 00:09:08,339
as a result we can avoid reading

190
00:09:08,339 --> 00:09:10,620
inefficient containers and thus

191
00:09:10,620 --> 00:09:15,060
accelerate the duplication workflow

192
00:09:15,060 --> 00:09:18,060
the second technique is uh data friendly

193
00:09:18,060 --> 00:09:19,860
data layouts

194
00:09:19,860 --> 00:09:22,620
we consider the two kinds of reference

195
00:09:22,620 --> 00:09:25,399
relationships together and defend the

196
00:09:25,399 --> 00:09:28,740
necessary chunks of a backup it is a

197
00:09:28,740 --> 00:09:31,019
combination of backups directed and

198
00:09:31,019 --> 00:09:34,140
indirectly reference channels and then

199
00:09:34,140 --> 00:09:37,260
we defend the left cycle of a chart

200
00:09:37,260 --> 00:09:40,500
according to backups necessary charts

201
00:09:40,500 --> 00:09:44,100
finally we apply life cycle based

202
00:09:44,100 --> 00:09:46,740
classification of the duplicated data to

203
00:09:46,740 --> 00:09:50,100
organize a data layout as shown in the

204
00:09:50,100 --> 00:09:53,000
red bottom figure in this data layout

205
00:09:53,000 --> 00:09:55,980
restore repair chunks of any backup are

206
00:09:55,980 --> 00:09:59,760
located compactly for example if we want

207
00:09:59,760 --> 00:10:03,540
to restore.com CAD one three care two

208
00:10:03,540 --> 00:10:06,540
three and cancer three are all we need

209
00:10:06,540 --> 00:10:10,440
and there will be no air use chunks to

210
00:10:10,440 --> 00:10:12,180
be this categories

211
00:10:12,180 --> 00:10:14,399
and besides the paper has more

212
00:10:14,399 --> 00:10:16,920
discussions about the evolution of this

213
00:10:16,920 --> 00:10:19,320
data layout to support saving more

214
00:10:19,320 --> 00:10:23,279
backups and deleting old backups

215
00:10:23,279 --> 00:10:26,100
for the third issue we proposed always

216
00:10:26,100 --> 00:10:28,980
forward reference traversing and data

217
00:10:28,980 --> 00:10:30,899
priority

218
00:10:30,899 --> 00:10:33,360
the office forward reference traversing

219
00:10:33,360 --> 00:10:35,459
is the special path to Traverse the

220
00:10:35,459 --> 00:10:38,700
restore regressions and in this past the

221
00:10:38,700 --> 00:10:41,339
data chunks always appears before their

222
00:10:41,339 --> 00:10:42,839
best chunks

223
00:10:42,839 --> 00:10:46,380
we found this path exists in the data

224
00:10:46,380 --> 00:10:48,200
friendly data layout

225
00:10:48,200 --> 00:10:51,360
and achieving that needs to follow

226
00:10:51,360 --> 00:10:55,019
several rules in general for our restore

227
00:10:55,019 --> 00:10:57,240
regardless categories we need to access

228
00:10:57,240 --> 00:10:59,880
clones in positive order

229
00:10:59,880 --> 00:11:03,060
and the access rows in reserve order

230
00:11:03,060 --> 00:11:06,360
for example cat one three can two three

231
00:11:06,360 --> 00:11:08,579
can and cancerously are needed

232
00:11:08,579 --> 00:11:11,459
categories for restoring industry backup

233
00:11:11,459 --> 00:11:15,300
and according to the rules we need to we

234
00:11:15,300 --> 00:11:18,060
need to access cats usually first then

235
00:11:18,060 --> 00:11:21,380
calc 2-3 and finally cut one three

236
00:11:21,380 --> 00:11:24,420
discussions about why this virtual path

237
00:11:24,420 --> 00:11:27,680
exists uh detailed in the paper

238
00:11:27,680 --> 00:11:30,720
supported by this class we could apply

239
00:11:30,720 --> 00:11:33,420
the data providing mechanism by

240
00:11:33,420 --> 00:11:36,180
utilizing the asymmetric IO

241
00:11:36,180 --> 00:11:39,120
characteristics of backups and user

242
00:11:39,120 --> 00:11:40,680
storage media

243
00:11:40,680 --> 00:11:43,620
next example in the red figure we're

244
00:11:43,620 --> 00:11:46,019
restoring a backup which reverse or

245
00:11:46,019 --> 00:11:48,560
restore required chunks when the

246
00:11:48,560 --> 00:11:52,140
wheel we Traverse our historical chunks

247
00:11:52,140 --> 00:11:55,140
with special pass and when encountering

248
00:11:55,140 --> 00:11:59,880
a Delta jump such as Delta of ram will

249
00:11:59,880 --> 00:12:02,279
prevent this channel to the offset our

250
00:12:02,279 --> 00:12:05,760
app frame in the you know to be restored

251
00:12:05,760 --> 00:12:07,500
workflow

252
00:12:07,500 --> 00:12:10,440
that means that up Prime is a decoded

253
00:12:10,440 --> 00:12:14,399
chunk of data prep then we continue to

254
00:12:14,399 --> 00:12:17,420
resting and when encountering

255
00:12:17,420 --> 00:12:20,880
we're encountering chunk F we will not

256
00:12:20,880 --> 00:12:24,120
data approach from the 2B restorable

257
00:12:24,120 --> 00:12:27,000
floor and decode data F Prime to get the

258
00:12:27,000 --> 00:12:30,600
F Prime finally we write back F Prime 2

259
00:12:30,600 --> 00:12:35,220
is offside besides dirty chunks all also

260
00:12:35,220 --> 00:12:38,300
could be rewriting to a catch instead of

261
00:12:38,300 --> 00:12:41,100
instead of the 2B restore workflow as

262
00:12:41,100 --> 00:12:44,220
the alternative solution because their

263
00:12:44,220 --> 00:12:47,639
chance usually only have quite smaller

264
00:12:47,639 --> 00:12:50,160
size than normal chunks the total size

265
00:12:50,160 --> 00:12:54,620
of the cache could be perhaps more

266
00:12:55,079 --> 00:12:57,899
the evaluation includes several

267
00:12:57,899 --> 00:13:00,779
approaches proposed or used in recent

268
00:13:00,779 --> 00:13:03,660
papers there are three different granted

269
00:13:03,660 --> 00:13:05,639
duplication approach and to channel

270
00:13:05,639 --> 00:13:07,860
level duplication approach

271
00:13:07,860 --> 00:13:10,260
for backup data sites are used for

272
00:13:10,260 --> 00:13:14,220
evaluation this data starts represents

273
00:13:14,220 --> 00:13:18,019
the various typical backup workloads

274
00:13:18,019 --> 00:13:21,240
including website snapshots and open

275
00:13:21,240 --> 00:13:23,639
source code project virtual machine

276
00:13:23,639 --> 00:13:28,500
image and synthetic data sets they have

277
00:13:28,500 --> 00:13:33,860
been used in several dedication studies

278
00:13:34,260 --> 00:13:37,440
the first is evaluations of the

279
00:13:37,440 --> 00:13:40,740
deduplication workflow and there are two

280
00:13:40,740 --> 00:13:43,800
parts including the backup speed and

281
00:13:43,800 --> 00:13:47,160
statistics about accessing disks for

282
00:13:47,160 --> 00:13:50,339
reading this channels results suggest

283
00:13:50,339 --> 00:13:51,899
that the mega achieves a close

284
00:13:51,899 --> 00:13:54,660
performance to channel level approach

285
00:13:54,660 --> 00:13:56,420
and up to

286
00:13:56,420 --> 00:14:00,480
34 times higher than grading first

287
00:14:00,480 --> 00:14:03,420
selective data encoding a higher

288
00:14:03,420 --> 00:14:07,320
threshold leads to a higher speed

289
00:14:07,320 --> 00:14:10,980
besides the statistics about accessing

290
00:14:10,980 --> 00:14:13,260
risks for reading this chunks that

291
00:14:13,260 --> 00:14:15,839
suggests that selective data encoding

292
00:14:15,839 --> 00:14:20,399
hugely reduces disk existing times and

293
00:14:20,399 --> 00:14:23,180
avoids reading immunization based chance

294
00:14:23,180 --> 00:14:26,519
and this is why Mega's backup speed is

295
00:14:26,519 --> 00:14:29,120
improved

296
00:14:29,220 --> 00:14:32,579
then we consider the duplication ratio

297
00:14:32,579 --> 00:14:35,399
so again can prevent the duplication

298
00:14:35,399 --> 00:14:38,100
approach you already have a much higher

299
00:14:38,100 --> 00:14:40,079
duplication ratio than chunk level

300
00:14:40,079 --> 00:14:43,380
approach and the gradient is the highest

301
00:14:43,380 --> 00:14:44,220
one

302
00:14:44,220 --> 00:14:47,399
the VMS data sites does not have many

303
00:14:47,399 --> 00:14:50,100
similar chunks and often Grant approach

304
00:14:50,100 --> 00:14:53,160
do not achieve a much higher duplication

305
00:14:53,160 --> 00:14:55,440
ratio on this data stacks

306
00:14:55,440 --> 00:14:58,740
the deduplication ratio loss caused by a

307
00:14:58,740 --> 00:15:01,139
selective data encoding is limited

308
00:15:01,139 --> 00:15:04,199
except the synthetic data sites it is

309
00:15:04,199 --> 00:15:06,959
because modifications in this data sites

310
00:15:06,959 --> 00:15:10,260
are complete completely random and

311
00:15:10,260 --> 00:15:11,639
non-natural

312
00:15:11,639 --> 00:15:15,600
generally a matter of persons the

313
00:15:15,600 --> 00:15:18,180
duplication ratio advantage of example

314
00:15:18,180 --> 00:15:20,719
approach

315
00:15:21,260 --> 00:15:24,540
evaluations on the restore workflow also

316
00:15:24,540 --> 00:15:27,120
have two parts including the restore

317
00:15:27,120 --> 00:15:30,120
speed and the statistics about accessing

318
00:15:30,120 --> 00:15:32,459
disks for repaired shells

319
00:15:32,459 --> 00:15:34,740
the results suggest that the mega

320
00:15:34,740 --> 00:15:38,459
achieves up to 100 part Times Higher

321
00:15:38,459 --> 00:15:41,040
restored speeder than grading and the

322
00:15:41,040 --> 00:15:43,339
study speaks about accessing this

323
00:15:43,339 --> 00:15:46,920
suggestion why Mega could achieve such a

324
00:15:46,920 --> 00:15:48,360
better performance

325
00:15:48,360 --> 00:15:51,600
on the one hand the data fan delay data

326
00:15:51,600 --> 00:15:53,940
layout addresses the fragmentation

327
00:15:53,940 --> 00:15:56,820
problem and reduces The ReStore in both

328
00:15:56,820 --> 00:15:58,500
consumers

329
00:15:58,500 --> 00:16:01,139
on the other hand always forward

330
00:16:01,139 --> 00:16:04,380
reference to versing and the Delta

331
00:16:04,380 --> 00:16:06,720
proacting significantly a wide

332
00:16:06,720 --> 00:16:09,720
repeatedly exciting containers we

333
00:16:09,720 --> 00:16:11,880
learned that though repeatedly exciting

334
00:16:11,880 --> 00:16:14,820
issue also exists in Channel duplication

335
00:16:14,820 --> 00:16:18,959
it is a relatively slight and the trunk

336
00:16:18,959 --> 00:16:21,660
level approached demand main challenge

337
00:16:21,660 --> 00:16:24,779
is the fragmentation problem and this is

338
00:16:24,779 --> 00:16:27,000
the variable you tend to consider the

339
00:16:27,000 --> 00:16:29,639
you know repeatedly discussing is a

340
00:16:29,639 --> 00:16:32,880
typical issue of Van granted application

341
00:16:32,880 --> 00:16:36,779
results suggest that other SharePoint

342
00:16:36,779 --> 00:16:39,480
dedication approach hugely suffer from

343
00:16:39,480 --> 00:16:43,079
this two locality issues and the matter

344
00:16:43,079 --> 00:16:45,779
has a better IO efficiency in the

345
00:16:45,779 --> 00:16:48,480
restore workflow

346
00:16:48,480 --> 00:16:51,540
finally we conclude that this paper

347
00:16:51,540 --> 00:16:53,940
talked about the three additional

348
00:16:53,940 --> 00:16:56,519
locality issues introduced by data

349
00:16:56,519 --> 00:16:59,339
encoding and propose Three core

350
00:16:59,339 --> 00:17:01,740
responding techniques to adapt these

351
00:17:01,740 --> 00:17:05,339
issues supported by this techniques Mega

352
00:17:05,339 --> 00:17:07,980
achieves a significant and performance

353
00:17:07,980 --> 00:17:10,740
Improvement while preserving fan

354
00:17:10,740 --> 00:17:12,660
quantity duplications the lubrication

355
00:17:12,660 --> 00:17:15,119
ratio advantage

356
00:17:15,119 --> 00:17:18,480
um finally sets for listening and please

357
00:17:18,480 --> 00:17:21,000
feel free to ask me if you have any

358
00:17:21,000 --> 00:17:23,419
question

