1
00:00:10,190 --> 00:00:16,000
hello everyone I'm MJ I'm an assistant

2
00:00:14,030 --> 00:00:18,490
professor at unist

3
00:00:16,000 --> 00:00:20,950
since last year today I'm going to talk

4
00:00:18,490 --> 00:00:22,869
about our characterizing study of GPU

5
00:00:20,950 --> 00:00:25,480
cluster design the 1419 training

6
00:00:22,869 --> 00:00:27,279
occurred in Microsoft this work was done

7
00:00:25,480 --> 00:00:30,610
when I was in Microsoft along the

8
00:00:27,279 --> 00:00:34,000
coasters shivram emergency venting and

9
00:00:30,610 --> 00:00:36,010
fun the learning based technology has

10
00:00:34,000 --> 00:00:37,600
been widely applied in many domains such

11
00:00:36,010 --> 00:00:40,269
as speech recognition image

12
00:00:37,600 --> 00:00:42,309
classification recommendations with the

13
00:00:40,269 --> 00:00:45,730
popularity of those applications more

14
00:00:42,309 --> 00:00:47,320
and more anonymity should be trained the

15
00:00:45,730 --> 00:00:50,529
Marine training requires specialized

16
00:00:47,320 --> 00:00:52,180
hardware such as GPU and with increased

17
00:00:50,530 --> 00:00:54,910
demand for team learning many companies

18
00:00:52,180 --> 00:00:58,590
have built their own GPU clusters for

19
00:00:54,910 --> 00:01:01,690
internal use such as filling Microsoft

20
00:00:58,590 --> 00:01:03,160
this work is based on filly across the

21
00:01:01,690 --> 00:01:04,800
resource manager for the anointing

22
00:01:03,160 --> 00:01:08,560
training workload for Roger scale

23
00:01:04,800 --> 00:01:11,229
multi-tenant GPU cross-dressed there has

24
00:01:08,560 --> 00:01:13,330
been many recent research of GP across

25
00:01:11,229 --> 00:01:15,520
the managers footing nonny and a number

26
00:01:13,330 --> 00:01:17,530
of those research was actually motivated

27
00:01:15,520 --> 00:01:21,280
by observations in Philly the filly that

28
00:01:17,530 --> 00:01:23,110
I am going to talk about today we will

29
00:01:21,280 --> 00:01:24,220
seen a great increase in scale filly

30
00:01:23,110 --> 00:01:27,130
over years

31
00:01:24,220 --> 00:01:28,840
for example during 2017 the number of

32
00:01:27,130 --> 00:01:31,390
the Amana jobs increased more than ten

33
00:01:28,840 --> 00:01:35,080
times and also those agile cluster

34
00:01:31,390 --> 00:01:37,390
increased by five times field across the

35
00:01:35,080 --> 00:01:39,880
manager sits in between emotionally

36
00:01:37,390 --> 00:01:42,130
applications and hardware resources and

37
00:01:39,880 --> 00:01:44,229
provide all these functionalities such

38
00:01:42,130 --> 00:01:47,560
as resource scheduling or storage

39
00:01:44,229 --> 00:01:48,929
management and failure handling this is

40
00:01:47,560 --> 00:01:53,200
very similar to be reduced in big data

41
00:01:48,930 --> 00:01:55,750
system across the managers now restart

42
00:01:53,200 --> 00:01:57,850
advise describing the giraffes eye cream

43
00:01:55,750 --> 00:02:00,189
Philly New Jersey are submitting their

44
00:01:57,850 --> 00:02:02,229
training jobs in online fashion and

45
00:02:00,189 --> 00:02:06,669
specify the number of sheep used for

46
00:02:02,229 --> 00:02:08,258
training and just schedulers decide the

47
00:02:06,670 --> 00:02:10,959
execution order of those jobs in a Jack

48
00:02:08,258 --> 00:02:13,149
you and once job is scheduled a

49
00:02:10,959 --> 00:02:14,980
placement scheme will allocate the

50
00:02:13,150 --> 00:02:17,739
required amount of leaders for the job

51
00:02:14,980 --> 00:02:21,250
from the cluster and place that one to

52
00:02:17,739 --> 00:02:24,280
the cluster for institution a once job

53
00:02:21,250 --> 00:02:26,739
is complete the allocated resources will

54
00:02:24,280 --> 00:02:29,140
be reclaimed and this is over how to

55
00:02:26,739 --> 00:02:31,180
arrive cycle in Philly looks like

56
00:02:29,140 --> 00:02:33,040
and in this work we make three

57
00:02:31,180 --> 00:02:35,370
contributions and first this is the

58
00:02:33,040 --> 00:02:37,780
first characterization study of

59
00:02:35,370 --> 00:02:40,600
registered GPU clusters using real-world

60
00:02:37,780 --> 00:02:43,270
data which we collected from Philly the

61
00:02:40,600 --> 00:02:46,180
second we studied across the utilization

62
00:02:43,270 --> 00:02:48,910
and how efficiently GPS are being used

63
00:02:46,180 --> 00:02:50,800
in the cluster and the third we

64
00:02:48,910 --> 00:02:55,030
presented essence for Baro close to the

65
00:02:50,800 --> 00:02:57,190
giant okay now I'm gonna talk about the

66
00:02:55,030 --> 00:02:59,980
first country contribution by giving

67
00:02:57,190 --> 00:03:01,420
some details about the study self for

68
00:02:59,980 --> 00:03:03,910
the study we collect the data over

69
00:03:01,420 --> 00:03:06,850
seventy-five days including information

70
00:03:03,910 --> 00:03:09,370
or more than nine ninety six thousand

71
00:03:06,850 --> 00:03:12,010
training jobs across thousand W jurist

72
00:03:09,370 --> 00:03:14,170
and the data is self reflect on

73
00:03:12,010 --> 00:03:16,899
multi-tenancy because Phil is shared by

74
00:03:14,170 --> 00:03:18,970
fourteen production groups I think this

75
00:03:16,900 --> 00:03:23,050
is what makes our study very unique and

76
00:03:18,970 --> 00:03:25,540
interesting but the challenge part is

77
00:03:23,050 --> 00:03:28,870
capturing inefficiency across all of

78
00:03:25,540 --> 00:03:31,359
these stages so for foodies we collect

79
00:03:28,870 --> 00:03:34,750
all following the local messages out of

80
00:03:31,360 --> 00:03:36,790
stages the first the logo from scheduler

81
00:03:34,750 --> 00:03:39,640
to capture information about arrival

82
00:03:36,790 --> 00:03:43,450
time and GPO allocation and feature

83
00:03:39,640 --> 00:03:45,309
status or job finish to it the second

84
00:03:43,450 --> 00:03:48,010
module performs counters to capture

85
00:03:45,310 --> 00:03:52,030
information on the harder utilization

86
00:03:48,010 --> 00:03:54,130
for GPU CPU and memory and network the

87
00:03:52,030 --> 00:03:55,959
third printout messages from AI engine

88
00:03:54,130 --> 00:03:57,940
including the progress information

89
00:03:55,959 --> 00:04:02,620
especially convergence information and

90
00:03:57,940 --> 00:04:05,019
also failure related information we

91
00:04:02,620 --> 00:04:06,489
combine all of this together to track

92
00:04:05,019 --> 00:04:08,350
the scheduling decision and the

93
00:04:06,489 --> 00:04:12,420
utilization information which each

94
00:04:08,350 --> 00:04:15,250
individual job during its job life cycle

95
00:04:12,420 --> 00:04:17,620
so I told you about the study details

96
00:04:15,250 --> 00:04:19,660
and given this now I am going to share

97
00:04:17,620 --> 00:04:23,710
the study or cross the utilization and

98
00:04:19,660 --> 00:04:26,200
then chief utilization the first

99
00:04:23,710 --> 00:04:28,270
question we are gonna ask is infinitely

100
00:04:26,200 --> 00:04:31,210
most GPUs are allocated for users

101
00:04:28,270 --> 00:04:33,460
because of very high demand but how Fe

102
00:04:31,210 --> 00:04:35,140
how effectively these GPUs are used for

103
00:04:33,460 --> 00:04:40,330
the money training this is our first

104
00:04:35,140 --> 00:04:43,090
equation in fact in our cluster GP

105
00:04:40,330 --> 00:04:46,930
utilization is roll the question is why

106
00:04:43,090 --> 00:04:49,210
utilization is so low to begin with our

107
00:04:46,930 --> 00:04:52,150
four single GPU jobs utilization is

108
00:04:49,210 --> 00:04:55,060
around 65% for the median education in

109
00:04:52,150 --> 00:04:56,799
the figure and while this is kind of

110
00:04:55,060 --> 00:04:59,530
lower than we would expect this is not a

111
00:04:56,800 --> 00:05:01,330
focus of our study there are actually

112
00:04:59,530 --> 00:05:03,309
many people looking at this problem

113
00:05:01,330 --> 00:05:05,590
using for example our better compilation

114
00:05:03,310 --> 00:05:08,169
techniques or better corners to improve

115
00:05:05,590 --> 00:05:10,679
our improved chip utilization and in

116
00:05:08,169 --> 00:05:14,889
this study we are more focused on how

117
00:05:10,680 --> 00:05:16,389
distribution effective utilization so as

118
00:05:14,889 --> 00:05:18,520
we increase the number of GPUs to fill a

119
00:05:16,389 --> 00:05:20,740
job we can see the decrease in the deep

120
00:05:18,520 --> 00:05:23,349
utilization there are mainly two reasons

121
00:05:20,740 --> 00:05:25,419
for this the first is this

122
00:05:23,350 --> 00:05:29,050
redistribution of jobs across the

123
00:05:25,419 --> 00:05:32,049
servers and the second is interest of

124
00:05:29,050 --> 00:05:34,060
interference across jobs so there are

125
00:05:32,050 --> 00:05:35,979
micro benchmark issuing the effect of

126
00:05:34,060 --> 00:05:38,320
interest over interference only in the

127
00:05:35,979 --> 00:05:43,750
paper and what I'm going to explain next

128
00:05:38,320 --> 00:05:45,400
is the impact of this revision only okay

129
00:05:43,750 --> 00:05:49,000
so to understand the effect or

130
00:05:45,400 --> 00:05:50,919
distribution we filter to just the HCP

131
00:05:49,000 --> 00:05:53,650
jobs running on single a chip you motion

132
00:05:50,919 --> 00:05:54,990
purchase 16 GPU jobs running on through

133
00:05:53,650 --> 00:05:57,969
ATP emotions

134
00:05:54,990 --> 00:05:59,830
so this jobs lot on dedicates topper and

135
00:05:57,970 --> 00:06:03,610
there's no other jobs in the same server

136
00:05:59,830 --> 00:06:06,820
running together and then we draw our

137
00:06:03,610 --> 00:06:09,520
CDF of utilization food for most cases

138
00:06:06,820 --> 00:06:11,770
as you can see in the figure and if we

139
00:06:09,520 --> 00:06:15,969
look at the typical case by drawing a

140
00:06:11,770 --> 00:06:18,430
horizontal line a 50% reaches median we

141
00:06:15,970 --> 00:06:20,560
can see that both cases have almost like

142
00:06:18,430 --> 00:06:24,460
30% of difference in terms of GP

143
00:06:20,560 --> 00:06:26,860
utilization so the takeaway is that

144
00:06:24,460 --> 00:06:29,789
history the training itself causes

145
00:06:26,860 --> 00:06:32,860
utilization to go lower

146
00:06:29,789 --> 00:06:34,750
okay now given this locality and

147
00:06:32,860 --> 00:06:37,210
distribution and problems numbers I've

148
00:06:34,750 --> 00:06:39,160
shown you nearly there is the religious

149
00:06:37,210 --> 00:06:41,289
term trade-off between the locality and

150
00:06:39,160 --> 00:06:45,789
the queuing delay and here is the evil

151
00:06:41,289 --> 00:06:48,280
emperor the scheduling policy can be

152
00:06:45,789 --> 00:06:49,659
really aware and they can take high

153
00:06:48,280 --> 00:06:52,090
interest of all you karate

154
00:06:49,660 --> 00:06:53,440
to allocate all cheap use of the jobs in

155
00:06:52,090 --> 00:06:55,989
the reading the single server in this

156
00:06:53,440 --> 00:06:57,009
case so achieving high communication

157
00:06:55,990 --> 00:07:00,400
efficiency

158
00:06:57,009 --> 00:07:01,990
but requiring non-working time but for

159
00:07:00,400 --> 00:07:03,938
you just in company one of the main

160
00:07:01,990 --> 00:07:06,699
request was getting onto the coaster as

161
00:07:03,939 --> 00:07:09,430
soon as possible so scheduler can reduce

162
00:07:06,699 --> 00:07:13,689
queuing delay by relaxing the interest

163
00:07:09,430 --> 00:07:16,210
of a locality but will increase on the

164
00:07:13,689 --> 00:07:18,279
network contention and the risk of

165
00:07:16,210 --> 00:07:20,198
interference within the server almost

166
00:07:18,279 --> 00:07:22,119
amando jobs in running the same server

167
00:07:20,199 --> 00:07:25,990
especially in the use of a shared

168
00:07:22,119 --> 00:07:28,419
resources such as speciai bandwidth so

169
00:07:25,990 --> 00:07:30,999
in general this is one makes GP

170
00:07:28,419 --> 00:07:34,948
utilization go lower in the case of

171
00:07:30,999 --> 00:07:34,949
running on low interest of a locality

172
00:07:36,240 --> 00:07:40,749
okay I told you about the utilization

173
00:07:38,529 --> 00:07:43,210
there let's take a look at a cross

174
00:07:40,749 --> 00:07:47,740
utilization especially how the failures

175
00:07:43,210 --> 00:07:50,589
of f2 cross or utilization to make

176
00:07:47,740 --> 00:07:52,419
progress often failure Phillie allows we

177
00:07:50,589 --> 00:07:55,270
try on failure using checkpoint and

178
00:07:52,419 --> 00:07:57,758
recovery technique but if our training

179
00:07:55,270 --> 00:08:00,099
job faced multiple times I become

180
00:07:57,759 --> 00:08:04,539
unsuccessful training and this waste

181
00:08:00,099 --> 00:08:06,370
cross the resources in our data there

182
00:08:04,539 --> 00:08:09,248
are some average over one pair one

183
00:08:06,370 --> 00:08:11,770
failure parties with training job and

184
00:08:09,249 --> 00:08:13,749
this number is not ami not yet but with

185
00:08:11,770 --> 00:08:18,969
increase of the rajah skill training

186
00:08:13,749 --> 00:08:20,439
this could be dominant in the future the

187
00:08:18,969 --> 00:08:22,748
most challenging part is that the

188
00:08:20,439 --> 00:08:24,490
failure can occur across the software

189
00:08:22,749 --> 00:08:26,709
stack anywhere in the software stack

190
00:08:24,490 --> 00:08:30,279
from the infrastructure to AI engine and

191
00:08:26,709 --> 00:08:32,740
the user program so it's not easy it's

192
00:08:30,279 --> 00:08:36,010
not easy even tell where the failure is

193
00:08:32,740 --> 00:08:38,079
coming from to our study is able to

194
00:08:36,010 --> 00:08:40,718
classify failure and the region about

195
00:08:38,078 --> 00:08:42,818
where failure happens and how bad it is

196
00:08:40,719 --> 00:08:44,380
and more importantly used is the

197
00:08:42,818 --> 00:08:49,300
information to improve the existing

198
00:08:44,380 --> 00:08:51,760
failure handling ok our hero failure

199
00:08:49,300 --> 00:08:53,529
classifier which does actually collect

200
00:08:51,760 --> 00:08:54,760
all failure related information and

201
00:08:53,529 --> 00:08:57,639
analyze it

202
00:08:54,760 --> 00:09:00,220
so first assuming that we are collecting

203
00:08:57,639 --> 00:09:02,319
this kind of information then we can

204
00:09:00,220 --> 00:09:04,839
easily get full generate fails and how

205
00:09:02,319 --> 00:09:07,689
many GP hours the training functions on

206
00:09:04,839 --> 00:09:09,740
the failure by simply by simply looking

207
00:09:07,689 --> 00:09:11,719
at the job status deserve

208
00:09:09,740 --> 00:09:15,500
the tricky part is identifying where

209
00:09:11,720 --> 00:09:18,410
failure occurs 240 lot I did our

210
00:09:15,500 --> 00:09:21,620
classifier Parsees STD out and the

211
00:09:18,410 --> 00:09:24,079
standard and the standard error files to

212
00:09:21,620 --> 00:09:27,170
extract the failure signatures and once

213
00:09:24,080 --> 00:09:29,240
failure signature is extracted we can

214
00:09:27,170 --> 00:09:31,760
classify the failure and then infer

215
00:09:29,240 --> 00:09:34,730
where failure is belonging to across dr.

216
00:09:31,760 --> 00:09:38,420
stack or Mon infrastructure AI engine

217
00:09:34,730 --> 00:09:40,040
and user program though I do like to

218
00:09:38,420 --> 00:09:43,099
note that the current version of the

219
00:09:40,040 --> 00:09:45,949
failure classifier has signatures and

220
00:09:43,100 --> 00:09:48,170
failure categories which were hand-built

221
00:09:45,950 --> 00:09:50,540
by simply analyzing the older local

222
00:09:48,170 --> 00:09:52,910
messages from failure so the technique

223
00:09:50,540 --> 00:09:54,620
to incorporate new failures without any

224
00:09:52,910 --> 00:10:00,469
manual process will be adding a lot of

225
00:09:54,620 --> 00:10:02,720
value in this research yeah I will show

226
00:10:00,470 --> 00:10:04,580
you two types of failures occurred from

227
00:10:02,720 --> 00:10:06,920
running our classifier

228
00:10:04,580 --> 00:10:09,980
the first is failures in in high

229
00:10:06,920 --> 00:10:12,140
frequency and these failures for in six

230
00:10:09,980 --> 00:10:15,200
categories as you can see in the figure

231
00:10:12,140 --> 00:10:18,080
and then if we adopt a number of

232
00:10:15,200 --> 00:10:20,600
failures of course all of this you can

233
00:10:18,080 --> 00:10:24,710
account for as much as 75% of all the

234
00:10:20,600 --> 00:10:26,720
failures released in the system and in

235
00:10:24,710 --> 00:10:28,370
our data we observe the region is mostly

236
00:10:26,720 --> 00:10:32,090
because of the usual errors in code and

237
00:10:28,370 --> 00:10:34,910
and the configuration so we found most

238
00:10:32,090 --> 00:10:38,090
of these failures are leper ative and

239
00:10:34,910 --> 00:10:40,640
appearing early and then the second type

240
00:10:38,090 --> 00:10:42,500
of failure is failures in the high

241
00:10:40,640 --> 00:10:45,290
resource usage which which we measure

242
00:10:42,500 --> 00:10:48,920
from the GPO hours spent by the failed

243
00:10:45,290 --> 00:10:50,270
job and then these failures are kind of

244
00:10:48,920 --> 00:10:52,310
falling into the four categories to

245
00:10:50,270 --> 00:10:54,860
educates in the figure and if we adopt

246
00:10:52,310 --> 00:10:57,369
the amount of GP hours spent by oomphel

247
00:10:54,860 --> 00:11:00,290
failed jobs across these categories

248
00:10:57,370 --> 00:11:03,050
accounts account for as much as 73

249
00:11:00,290 --> 00:11:06,020
percent of would the GPRS spend by the

250
00:11:03,050 --> 00:11:07,490
failed job and in our data we observed

251
00:11:06,020 --> 00:11:09,949
that the region's mostly because of

252
00:11:07,490 --> 00:11:12,620
infrastructure failures and we found

253
00:11:09,950 --> 00:11:14,680
that most of these failures spread

254
00:11:12,620 --> 00:11:17,780
across many layers over system state

255
00:11:14,680 --> 00:11:20,650
including those treaty and communication

256
00:11:17,780 --> 00:11:20,650
and network state

257
00:11:21,330 --> 00:11:25,870
okay then I'm going to present very

258
00:11:23,830 --> 00:11:28,840
interesting lessons that we learned from

259
00:11:25,870 --> 00:11:31,600
this study the first rocky road I

260
00:11:28,840 --> 00:11:34,780
purchased waiting time the you just over

261
00:11:31,600 --> 00:11:36,550
course prefer a lower community race but

262
00:11:34,780 --> 00:11:38,980
initial delays in locality over

263
00:11:36,550 --> 00:11:40,719
scheduling can far outweigh keeping up

264
00:11:38,980 --> 00:11:43,720
locality for especially long running

265
00:11:40,720 --> 00:11:47,740
jobs so to make this point concrete let

266
00:11:43,720 --> 00:11:49,660
me take three jumpers the first two you

267
00:11:47,740 --> 00:11:52,900
stopped me the wrong manager and it

268
00:11:49,660 --> 00:11:54,760
takes around 24 hours to complete but if

269
00:11:52,900 --> 00:11:58,150
we wait for one hour for very higher

270
00:11:54,760 --> 00:12:00,939
locality then it is gonna take only 16

271
00:11:58,150 --> 00:12:03,459
hours to complete the here initial delay

272
00:12:00,940 --> 00:12:05,560
is one hour or not zero minute

273
00:12:03,460 --> 00:12:07,300
so looks like very awful amount of time

274
00:12:05,560 --> 00:12:10,030
which was not free preferred by the

275
00:12:07,300 --> 00:12:13,000
users but if you think about the overall

276
00:12:10,030 --> 00:12:15,790
running time is one hour plus 16 hours

277
00:12:13,000 --> 00:12:20,710
so 17 hours which is much lower than the

278
00:12:15,790 --> 00:12:22,420
24 hours so we think to get better

279
00:12:20,710 --> 00:12:24,190
locality packing schedule need to

280
00:12:22,420 --> 00:12:25,719
consider the local or the trade-off

281
00:12:24,190 --> 00:12:27,670
between the queuing delays and the

282
00:12:25,720 --> 00:12:30,580
impact of the locality over scheduling

283
00:12:27,670 --> 00:12:32,530
and this local locality our scheduling

284
00:12:30,580 --> 00:12:35,110
should be applicable to only long time

285
00:12:32,530 --> 00:12:38,140
learning jobs not exploratory job which

286
00:12:35,110 --> 00:12:42,280
might be sure running or may be filled

287
00:12:38,140 --> 00:12:44,439
by the users the second at the very

288
00:12:42,280 --> 00:12:46,420
beginning is very hard to know whether

289
00:12:44,440 --> 00:12:48,310
the job is aluminium or not so the

290
00:12:46,420 --> 00:12:50,380
efectivo were simple very effective

291
00:12:48,310 --> 00:12:53,589
technique is that you start from the low

292
00:12:50,380 --> 00:12:55,750
locality and then later on do migration

293
00:12:53,590 --> 00:13:01,030
if we if you identify that you know this

294
00:12:55,750 --> 00:13:03,610
job is running okay for failures that

295
00:13:01,030 --> 00:13:07,810
are leper ative and the lipid in other

296
00:13:03,610 --> 00:13:09,580
kind of a leopard tip in this case we

297
00:13:07,810 --> 00:13:11,380
can you can improve the cross the

298
00:13:09,580 --> 00:13:14,290
utilize utilization by catching those

299
00:13:11,380 --> 00:13:16,600
failures early so for example simple

300
00:13:14,290 --> 00:13:18,849
validation of jobs are before going into

301
00:13:16,600 --> 00:13:21,070
the scheduling especially the pre

302
00:13:18,850 --> 00:13:24,040
running those jobs on the very cheap

303
00:13:21,070 --> 00:13:26,410
PM's were chip servers can avoid our

304
00:13:24,040 --> 00:13:29,170
majority of those failures in our in our

305
00:13:26,410 --> 00:13:31,780
case as much as fully 2 percent of all

306
00:13:29,170 --> 00:13:33,250
the failures in our workload then the

307
00:13:31,780 --> 00:13:35,740
clutch cost is they eat

308
00:13:33,250 --> 00:13:38,530
avoided our body GPS even being placed

309
00:13:35,740 --> 00:13:42,120
on the windows on the cluster thereby

310
00:13:38,530 --> 00:13:42,120
improving the cluster utilization

311
00:13:42,210 --> 00:13:48,280
okay there are more details in the paper

312
00:13:45,730 --> 00:13:49,810
paper about in case we have a queuing

313
00:13:48,280 --> 00:13:51,370
delay so whether the twin delays coming

314
00:13:49,810 --> 00:13:53,859
from the fair sharing or the

315
00:13:51,370 --> 00:13:55,630
fragmentation or what is the impact of

316
00:13:53,860 --> 00:13:57,550
audible alert scheduling because the

317
00:13:55,630 --> 00:14:00,760
yarn is the work conserving for example

318
00:13:57,550 --> 00:14:03,640
and then how to mitigate the failures a

319
00:14:00,760 --> 00:14:05,830
long time and then more importantly the

320
00:14:03,640 --> 00:14:07,930
last things very interesting is the

321
00:14:05,830 --> 00:14:09,970
effectiveness over last the epochs like

322
00:14:07,930 --> 00:14:12,790
in case of the we have to pay some

323
00:14:09,970 --> 00:14:19,390
resources for the last epochs how they

324
00:14:12,790 --> 00:14:22,270
are contributing the convergence okay to

325
00:14:19,390 --> 00:14:25,060
conclude I am emphasize that this is our

326
00:14:22,270 --> 00:14:27,699
first characterizing study of the Rogers

327
00:14:25,060 --> 00:14:29,430
kgp crosswords for the noni training and

328
00:14:27,700 --> 00:14:32,020
then we talked about some inefficiencies

329
00:14:29,430 --> 00:14:34,000
that you just in the in the use of the

330
00:14:32,020 --> 00:14:36,310
cross-strait resources and then we

331
00:14:34,000 --> 00:14:38,170
provide some lessons especially the

332
00:14:36,310 --> 00:14:40,959
locality over scheduling and the failure

333
00:14:38,170 --> 00:14:43,180
handling and then we are very happy to

334
00:14:40,960 --> 00:14:45,370
announce that happy to us that we are

335
00:14:43,180 --> 00:14:48,910
relating the traces actually there was

336
00:14:45,370 --> 00:14:50,470
list like as of yesterday and then we

337
00:14:48,910 --> 00:14:52,180
are really looking forward to more

338
00:14:50,470 --> 00:14:53,890
current research outcomes coming out

339
00:14:52,180 --> 00:14:56,500
although our trace and there will be

340
00:14:53,890 --> 00:15:03,520
very happy with that and thank you and I

341
00:14:56,500 --> 00:15:04,930
will take a education jung-jae any

342
00:15:03,520 --> 00:15:11,920
questions from Youngjae and please

343
00:15:04,930 --> 00:15:13,300
introduce yourself I am Thomas from TV I

344
00:15:11,920 --> 00:15:15,819
was wondering whether you have some

345
00:15:13,300 --> 00:15:18,790
insights on how users are using your

346
00:15:15,820 --> 00:15:21,100
cluster I've been studying other GPU

347
00:15:18,790 --> 00:15:23,680
clusters used for deep learning and we

348
00:15:21,100 --> 00:15:26,680
found that there are a large portion of

349
00:15:23,680 --> 00:15:28,390
users who just run hello world training

350
00:15:26,680 --> 00:15:30,160
jobs on your cluster and then there's

351
00:15:28,390 --> 00:15:33,250
another set of users where's like key

352
00:15:30,160 --> 00:15:35,410
accounts that run like two week-long

353
00:15:33,250 --> 00:15:36,910
jobs or something and I'm wondering

354
00:15:35,410 --> 00:15:39,100
whether you could incorporate that kind

355
00:15:36,910 --> 00:15:41,380
of knowledge into scheduling decisions

356
00:15:39,100 --> 00:15:43,180
and things like that who you thought so

357
00:15:41,380 --> 00:15:44,270
actually the there is nothing like that

358
00:15:43,180 --> 00:15:46,040
in the

359
00:15:44,270 --> 00:15:47,510
the production cluster actually although

360
00:15:46,040 --> 00:15:49,490
there are two types of jobs one is the

361
00:15:47,510 --> 00:15:51,830
very merger training after you do the

362
00:15:49,490 --> 00:15:54,170
kind of parameter sweeping and you

363
00:15:51,830 --> 00:15:55,700
already established the model and the

364
00:15:54,170 --> 00:15:57,199
other type is the parameter sleeping we

365
00:15:55,700 --> 00:16:00,110
don't have a kind of hello well the kind

366
00:15:57,200 --> 00:16:02,210
of thing but I think from the

367
00:16:00,110 --> 00:16:04,120
perspective of the job scheduling I

368
00:16:02,210 --> 00:16:06,320
think it is not able to differentiate

369
00:16:04,120 --> 00:16:12,830
differentiate kind of what what they are

370
00:16:06,320 --> 00:16:16,240
running within the scheduler mmm-hmm hi

371
00:16:12,830 --> 00:16:19,610
change from woofer so my question is

372
00:16:16,240 --> 00:16:21,860
when we will observe real are skill

373
00:16:19,610 --> 00:16:25,660
teacher machine learning special deep

374
00:16:21,860 --> 00:16:31,610
learning we saw a clear benefit of rock

375
00:16:25,660 --> 00:16:34,400
affinity before that the topic you talk

376
00:16:31,610 --> 00:16:37,160
about failures generally we find like

377
00:16:34,400 --> 00:16:40,550
this affinity is a it's a kind of way to

378
00:16:37,160 --> 00:16:43,939
go to kind of minimize the risk how

379
00:16:40,550 --> 00:16:45,469
what's your take on this what you are

380
00:16:43,940 --> 00:16:47,770
talking about the hardware failures in

381
00:16:45,470 --> 00:16:52,270
red leather yes

382
00:16:47,770 --> 00:16:54,920
so the analysis is mostly focused on the

383
00:16:52,270 --> 00:16:56,569
software driven the failures to actually

384
00:16:54,920 --> 00:16:59,240
we don't have any insight about hardware

385
00:16:56,570 --> 00:17:00,950
failures okay I don't I'm sorry I don't

386
00:16:59,240 --> 00:17:07,130
have a right nice K nice to answer about

387
00:17:00,950 --> 00:17:09,290
that okay yeah hi I'm James Davis from

388
00:17:07,130 --> 00:17:12,080
Virginia Tech I have a mercenary

389
00:17:09,290 --> 00:17:16,040
question which is does Microsoft make

390
00:17:12,079 --> 00:17:18,560
money either way so they I think they

391
00:17:16,040 --> 00:17:20,389
are making money by deploying these into

392
00:17:18,560 --> 00:17:23,030
the edge work well so I guess my

393
00:17:20,390 --> 00:17:24,950
question is like if a user submits a job

394
00:17:23,030 --> 00:17:25,959
that fails do they still have to pay

395
00:17:24,950 --> 00:17:32,630
Microsoft

396
00:17:25,959 --> 00:17:35,240
so so so so I say so I don't know if I

397
00:17:32,630 --> 00:17:37,280
have to tell this or not but you know so

398
00:17:35,240 --> 00:17:39,380
I mentioned that this is the multi multi

399
00:17:37,280 --> 00:17:41,960
tenant across her and then shared by

400
00:17:39,380 --> 00:17:43,550
multiple production grows meaning that

401
00:17:41,960 --> 00:17:45,950
the multi proportion groups they have

402
00:17:43,550 --> 00:17:48,050
their own cutter in terms of the fair

403
00:17:45,950 --> 00:17:50,390
sharing so of course you know depending

404
00:17:48,050 --> 00:17:52,730
on the money were some contribution they

405
00:17:50,390 --> 00:17:54,950
have their quota so virtually they have

406
00:17:52,730 --> 00:17:57,940
some they are paying some value on it

407
00:17:54,950 --> 00:17:57,940
okay thank you

408
00:17:59,360 --> 00:18:04,740
so we have a couple minutes left sir I

409
00:18:01,950 --> 00:18:06,809
have one question so you showed that the

410
00:18:04,740 --> 00:18:09,330
utilization for these clusters is not

411
00:18:06,809 --> 00:18:10,860
very high right and this is consistent

412
00:18:09,330 --> 00:18:14,250
with other studies that have done

413
00:18:10,860 --> 00:18:16,110
utilization of non GPU clusters as well

414
00:18:14,250 --> 00:18:16,559
in data centers the utilization is quite

415
00:18:16,110 --> 00:18:19,379
low

416
00:18:16,559 --> 00:18:21,660
now Nvidia and others are creating

417
00:18:19,380 --> 00:18:25,530
software which will make it easier for

418
00:18:21,660 --> 00:18:27,270
multiple jobs to share one GPU card do

419
00:18:25,530 --> 00:18:29,428
you anticipate that this is going to

420
00:18:27,270 --> 00:18:31,590
become mainstream and if so if that's

421
00:18:29,429 --> 00:18:33,330
going to have any make any difference to

422
00:18:31,590 --> 00:18:35,280
the utilization numbers that you should

423
00:18:33,330 --> 00:18:38,040
I'm pretty sure that would be a part of

424
00:18:35,280 --> 00:18:40,470
mainstream pretty soon because the the

425
00:18:38,040 --> 00:18:42,690
multi kind of sharing the one GPU among

426
00:18:40,470 --> 00:18:45,570
jobs is culture it's been murdered these

427
00:18:42,690 --> 00:18:47,040
days and then the remaining tasks in my

428
00:18:45,570 --> 00:18:49,379
view is that you know how to incorporate

429
00:18:47,040 --> 00:18:49,889
them into the scheduling policy in the

430
00:18:49,380 --> 00:18:52,830
crossfire

431
00:18:49,890 --> 00:18:55,380
now the way we are scheduling the GPUs

432
00:18:52,830 --> 00:18:58,230
is very monolithic like either the onesy

433
00:18:55,380 --> 00:18:59,700
pure zero but in case we are enabling

434
00:18:58,230 --> 00:19:02,520
the sharing we have to have some

435
00:18:59,700 --> 00:19:04,740
mechanism to to make kind of one GPU to

436
00:19:02,520 --> 00:19:07,250
be able to sort by multiple jobs but I

437
00:19:04,740 --> 00:19:10,320
think that's really coming thank you

438
00:19:07,250 --> 00:19:15,040
let's thank Beyonce once again

439
00:19:10,320 --> 00:19:15,040
[Applause]

