1
00:00:10,769 --> 00:00:15,900
good afternoon everyone thank you so

2
00:00:12,630 --> 00:00:18,270
much for being here let's imagine a very

3
00:00:15,900 --> 00:00:19,980
common scenario that you're sitting in a

4
00:00:18,270 --> 00:00:21,540
coffee shop and you're doing your work

5
00:00:19,980 --> 00:00:23,340
with your laptop you could be writing

6
00:00:21,540 --> 00:00:25,799
some code you could be working on a

7
00:00:23,340 --> 00:00:27,990
video anything and for the most part

8
00:00:25,800 --> 00:00:30,390
your laptop is completely fine for that

9
00:00:27,990 --> 00:00:32,430
task but sometimes you have to do

10
00:00:30,390 --> 00:00:34,559
something that takes for more resources

11
00:00:32,430 --> 00:00:36,930
resources that is available in your

12
00:00:34,559 --> 00:00:39,780
laptop let me give you some examples on

13
00:00:36,930 --> 00:00:43,440
a single core laptop compiling Google

14
00:00:39,780 --> 00:00:46,040
Chrome takes 16 hours running unit tests

15
00:00:43,440 --> 00:00:49,799
for live VPX takes an hour and a half

16
00:00:46,040 --> 00:00:52,409
encoding a 15 minute long 4k video takes

17
00:00:49,799 --> 00:00:54,239
more than seven hours and rendering a

18
00:00:52,409 --> 00:00:57,989
single frame of monsters university

19
00:00:54,239 --> 00:00:59,940
animation takes 29 hours one thing that

20
00:00:57,989 --> 00:01:01,949
is common between these tests is that

21
00:00:59,940 --> 00:01:03,900
they are occasional it's not like that

22
00:01:01,949 --> 00:01:04,890
you want to compile something every

23
00:01:03,900 --> 00:01:07,260
second of every day

24
00:01:04,890 --> 00:01:09,450
you know like 99% of the time you're

25
00:01:07,260 --> 00:01:12,000
doing a task that is that you're latter

26
00:01:09,450 --> 00:01:14,159
is completely fine for that but for that

27
00:01:12,000 --> 00:01:17,130
1% of the time that you want to these

28
00:01:14,159 --> 00:01:20,009
time-consuming tasks it would be awesome

29
00:01:17,130 --> 00:01:22,380
if you could immediately borrow 10,000

30
00:01:20,009 --> 00:01:25,079
cores in the cloud outsource your task

31
00:01:22,380 --> 00:01:27,658
have it done in minute rather than hours

32
00:01:25,079 --> 00:01:29,579
and then pull back the results give back

33
00:01:27,659 --> 00:01:33,060
the course and return to your normal

34
00:01:29,579 --> 00:01:35,729
workflow so many others actually share

35
00:01:33,060 --> 00:01:38,009
this dream with us but for reasons such

36
00:01:35,729 --> 00:01:40,649
as limited the speed ops high cost and

37
00:01:38,009 --> 00:01:43,920
the limited scope of these programs this

38
00:01:40,649 --> 00:01:45,979
dream is not quite a reality yet so

39
00:01:43,920 --> 00:01:48,960
today I'm going to talk about this

40
00:01:45,979 --> 00:01:51,600
project that we've been working on GG

41
00:01:48,960 --> 00:01:54,030
which is a framework and a toolkit that

42
00:01:51,600 --> 00:01:55,889
makes it practical to outsource everyday

43
00:01:54,030 --> 00:01:58,850
applications such as software

44
00:01:55,889 --> 00:02:02,280
compilation unit testing 3d rendering

45
00:01:58,850 --> 00:02:04,829
video encoding to run with thousands of

46
00:02:02,280 --> 00:02:09,060
a parallelism on code function services

47
00:02:04,829 --> 00:02:11,008
like AWS lamp okay let's talk about you

48
00:02:09,060 --> 00:02:15,480
about the challenges that we have in

49
00:02:11,008 --> 00:02:17,040
outsourcing programs to the clock the

50
00:02:15,480 --> 00:02:18,690
first challenge is managing software

51
00:02:17,040 --> 00:02:20,459
dependencies you know when you're

52
00:02:18,690 --> 00:02:23,010
talking about running something on a

53
00:02:20,460 --> 00:02:24,600
cluster of 10,000 nodes we think of

54
00:02:23,010 --> 00:02:27,780
frameworks like spark

55
00:02:24,600 --> 00:02:29,160
Hadoop but why aren't we using them to

56
00:02:27,780 --> 00:02:31,890
outsource our tasks

57
00:02:29,160 --> 00:02:34,380
the reason is that these frameworks they

58
00:02:31,890 --> 00:02:36,480
do not manage software dependencies it

59
00:02:34,380 --> 00:02:38,460
means that if you want to run the

60
00:02:36,480 --> 00:02:40,619
compiler or interact with the video

61
00:02:38,460 --> 00:02:43,350
encoder or run tensorflow or any

62
00:02:40,620 --> 00:02:46,080
external software you have to go and get

63
00:02:43,350 --> 00:02:47,970
a one cluster preload all those programs

64
00:02:46,080 --> 00:02:51,180
and then the framework can run and

65
00:02:47,970 --> 00:02:53,760
interact with those programs but our

66
00:02:51,180 --> 00:02:56,010
tasks are occasional you know I want to

67
00:02:53,760 --> 00:02:59,100
compile something maybe once an hour

68
00:02:56,010 --> 00:03:01,290
so one clusters could be actually very

69
00:02:59,100 --> 00:03:04,130
expensive for these kind of jobs like

70
00:03:01,290 --> 00:03:08,730
getting 10,000 cores on ec2 can cost you

71
00:03:04,130 --> 00:03:11,490
$400 per hour so we need a way to be

72
00:03:08,730 --> 00:03:13,440
able to efficiently pack up a

73
00:03:11,490 --> 00:03:16,140
computation that you would normally run

74
00:03:13,440 --> 00:03:18,840
on your laptop and ship that to be

75
00:03:16,140 --> 00:03:20,640
executed in the cloud in an environment

76
00:03:18,840 --> 00:03:23,190
that is different from your laptops

77
00:03:20,640 --> 00:03:25,980
environment so we need something like a

78
00:03:23,190 --> 00:03:27,600
docker container but when you're talking

79
00:03:25,980 --> 00:03:30,989
about running like thousands of these

80
00:03:27,600 --> 00:03:33,269
containers in the cloud at the same time

81
00:03:30,990 --> 00:03:35,810
docker containers could be a little bit

82
00:03:33,270 --> 00:03:39,000
heavyweight so our solution to this is

83
00:03:35,810 --> 00:03:42,060
lightweight containers which we call it

84
00:03:39,000 --> 00:03:44,460
a thunk and it's basically it can be

85
00:03:42,060 --> 00:03:47,130
seen as a very light docker file you

86
00:03:44,460 --> 00:03:50,460
know it's a description of an executable

87
00:03:47,130 --> 00:03:52,590
all of its arguments its environment and

88
00:03:50,460 --> 00:03:55,560
all of the input data that it needs to

89
00:03:52,590 --> 00:03:58,100
carry out the operation and every piece

90
00:03:55,560 --> 00:04:01,230
of data that we have in these chunks is

91
00:03:58,100 --> 00:04:03,630
named by the hash of its contents let me

92
00:04:01,230 --> 00:04:06,269
give you an example so here we have a

93
00:04:03,630 --> 00:04:08,880
thunk that we use for pre processes

94
00:04:06,270 --> 00:04:11,010
Seaforth so it has three parts the first

95
00:04:08,880 --> 00:04:12,810
part is the function you know the hash

96
00:04:11,010 --> 00:04:14,609
of the binary that we want to execute

97
00:04:12,810 --> 00:04:16,380
the hash of the compiler the

98
00:04:14,610 --> 00:04:18,330
command-line arguments that we need to

99
00:04:16,380 --> 00:04:19,950
do that pre-processing operation and the

100
00:04:18,329 --> 00:04:23,130
environment variables that are necessary

101
00:04:19,950 --> 00:04:25,680
it also lists every object that is

102
00:04:23,130 --> 00:04:28,800
necessary to do this for example the

103
00:04:25,680 --> 00:04:31,140
compiler itself the input file and all

104
00:04:28,800 --> 00:04:34,590
of the header files that are included in

105
00:04:31,140 --> 00:04:37,050
that C file so we can do the

106
00:04:34,590 --> 00:04:38,758
preprocessor operation and it also lists

107
00:04:37,050 --> 00:04:41,430
the output files that it create

108
00:04:38,759 --> 00:04:43,319
so this trunk has the full functional

109
00:04:41,430 --> 00:04:45,750
footprint of the operation that we want

110
00:04:43,319 --> 00:04:48,569
to do and I can ship that and execute

111
00:04:45,750 --> 00:04:53,370
that on ec2 or AWS lambda or my local

112
00:04:48,569 --> 00:04:54,720
machine or my friend's laptop so okay or

113
00:04:53,370 --> 00:04:57,210
solution for managing software

114
00:04:54,720 --> 00:05:00,780
dependencies is using these lightweight

115
00:04:57,210 --> 00:05:03,210
containers or thunks the second problem

116
00:05:00,780 --> 00:05:06,030
that we have in outsourcing is the round

117
00:05:03,210 --> 00:05:07,590
trips that we have to the cloud you know

118
00:05:06,030 --> 00:05:10,409
you're sitting in a coffee shop with a

119
00:05:07,590 --> 00:05:12,630
bad Wi-Fi your latency to the cloud is

120
00:05:10,410 --> 00:05:14,970
huge so you want to minimize the

121
00:05:12,630 --> 00:05:18,120
communication that you have between the

122
00:05:14,970 --> 00:05:19,710
cloud and your laptop we currently have

123
00:05:18,120 --> 00:05:22,020
a lot of systems that we use for

124
00:05:19,710 --> 00:05:24,930
outsourcing jobs you know like TCC and

125
00:05:22,020 --> 00:05:27,359
ICC are two very popular programs for

126
00:05:24,930 --> 00:05:29,280
outsourcing compilation and there are

127
00:05:27,360 --> 00:05:31,680
tools like you cop that can be used to

128
00:05:29,280 --> 00:05:33,929
outsource CPU intensive applications to

129
00:05:31,680 --> 00:05:37,020
the cloud but these programs they

130
00:05:33,930 --> 00:05:39,240
perform best over fast networks and the

131
00:05:37,020 --> 00:05:41,310
reason is that they still keep the

132
00:05:39,240 --> 00:05:43,860
laptop in the driver seat you know you

133
00:05:41,310 --> 00:05:45,509
outsource your job but there are there

134
00:05:43,860 --> 00:05:47,759
is a still a lot of communication going

135
00:05:45,509 --> 00:05:50,669
on between the cloud and your laptop

136
00:05:47,759 --> 00:05:53,219
what we would ideally want is to take

137
00:05:50,669 --> 00:05:55,289
laptop out of the loop and have all the

138
00:05:53,219 --> 00:05:57,810
communication and dispatching happen

139
00:05:55,289 --> 00:06:00,830
purely within the cloud and just fetch

140
00:05:57,810 --> 00:06:05,219
back the final results of the operation

141
00:06:00,830 --> 00:06:07,948
so in the previous challenge I talked

142
00:06:05,219 --> 00:06:11,759
about these containers that we can use

143
00:06:07,949 --> 00:06:13,199
to pack up individual tasks and upload

144
00:06:11,759 --> 00:06:15,270
them to the cloud like if you want to

145
00:06:13,199 --> 00:06:17,729
compile a thousand files you create a

146
00:06:15,270 --> 00:06:19,318
thousand containers for compiling each

147
00:06:17,729 --> 00:06:22,340
file and then you can upload them to the

148
00:06:19,319 --> 00:06:26,219
cloud but here we need something more we

149
00:06:22,340 --> 00:06:29,039
don't only want to describe these small

150
00:06:26,219 --> 00:06:30,719
pieces of tasks but also we want to be

151
00:06:29,039 --> 00:06:33,900
able to describe the whole computation

152
00:06:30,719 --> 00:06:36,630
and our solution to that is linked

153
00:06:33,900 --> 00:06:39,090
containers as I told you every piece of

154
00:06:36,630 --> 00:06:41,729
data in thunks they are named by the

155
00:06:39,090 --> 00:06:43,799
hash of their content that's also true

156
00:06:41,729 --> 00:06:47,789
about the funk itself the thumb key also

157
00:06:43,800 --> 00:06:48,479
has a hash so whenever I'm in another

158
00:06:47,789 --> 00:06:51,210
container

159
00:06:48,479 --> 00:06:52,349
I need the output of this thunk I can

160
00:06:51,210 --> 00:06:54,510
just refer to that

161
00:06:52,350 --> 00:06:57,210
by using this hat you know the thunk is

162
00:06:54,510 --> 00:07:00,360
not executed yet I don't have its output

163
00:06:57,210 --> 00:07:03,239
yet but I can refer to that using the

164
00:07:00,360 --> 00:07:05,400
funks hash so for example if a later

165
00:07:03,240 --> 00:07:08,070
stage like a compile stage needs the

166
00:07:05,400 --> 00:07:10,500
output of the preprocessor stage it can

167
00:07:08,070 --> 00:07:12,810
name that output it can refer to that

168
00:07:10,500 --> 00:07:15,780
output body but using the hash of the

169
00:07:12,810 --> 00:07:17,820
preprocessor thunk now if I want to

170
00:07:15,780 --> 00:07:19,619
offload this operation to the cloud what

171
00:07:17,820 --> 00:07:22,230
I do is that I've sent both of these

172
00:07:19,620 --> 00:07:25,320
songs to the cloud I will execute the

173
00:07:22,230 --> 00:07:27,270
preprocessor stage then within the cloud

174
00:07:25,320 --> 00:07:29,130
I will send its output to the other

175
00:07:27,270 --> 00:07:31,650
container which does the compiler

176
00:07:29,130 --> 00:07:35,400
operation and then I do the compile and

177
00:07:31,650 --> 00:07:38,340
fetch back the final result this is what

178
00:07:35,400 --> 00:07:40,440
we call GG intermediate representation a

179
00:07:38,340 --> 00:07:43,650
collection of thongs that are chained

180
00:07:40,440 --> 00:07:45,630
together using these hashes so with this

181
00:07:43,650 --> 00:07:49,070
simple construct you can actually build

182
00:07:45,630 --> 00:07:51,960
more complicated graphs like this is a

183
00:07:49,070 --> 00:07:54,450
this is a representation of GG I are for

184
00:07:51,960 --> 00:07:57,090
compiling the new hello program and also

185
00:07:54,450 --> 00:07:58,830
you can build dynamic graphs and things

186
00:07:57,090 --> 00:08:01,080
like loops and recursions and you can

187
00:07:58,830 --> 00:08:05,430
even implement combinators like Y

188
00:08:01,080 --> 00:08:06,659
Combinator with GG I are okay so our

189
00:08:05,430 --> 00:08:09,240
solution for managing software

190
00:08:06,660 --> 00:08:11,910
dependencies is lightweight containers a

191
00:08:09,240 --> 00:08:14,040
way to describe the whole job and

192
00:08:11,910 --> 00:08:16,740
offload the whole job to avoid an

193
00:08:14,040 --> 00:08:19,050
unnecessary round-trip to a laptop is by

194
00:08:16,740 --> 00:08:22,160
using linked containers which we call GG

195
00:08:19,050 --> 00:08:27,210
intermediate representation or GG IR and

196
00:08:22,160 --> 00:08:30,240
now the question is that how do we want

197
00:08:27,210 --> 00:08:32,909
to get 10,000 cores how can we do that

198
00:08:30,240 --> 00:08:35,700
you know we talked about this the dream

199
00:08:32,909 --> 00:08:37,559
is to rent a supercomputer by the second

200
00:08:35,700 --> 00:08:40,469
whenever we need that you know and one

201
00:08:37,559 --> 00:08:43,140
clusters are expensive because or tasks

202
00:08:40,469 --> 00:08:45,390
are occasional and quilt clusters are

203
00:08:43,140 --> 00:08:47,880
slow to start they take a few minutes at

204
00:08:45,390 --> 00:08:48,420
least to start that defeats the whole

205
00:08:47,880 --> 00:08:51,030
purpose

206
00:08:48,420 --> 00:08:55,110
but using close function services or

207
00:08:51,030 --> 00:08:58,110
several less platforms for example on

208
00:08:55,110 --> 00:09:00,180
AWS lambda you can get 10,000 workers in

209
00:08:58,110 --> 00:09:02,490
just a few seconds and if you run each

210
00:09:00,180 --> 00:09:06,239
for like 10 seconds you're only going to

211
00:09:02,490 --> 00:09:08,939
pay $5 that's over one day of CPU time

212
00:09:06,240 --> 00:09:11,189
just for five bucks and they draw a lot

213
00:09:08,939 --> 00:09:13,349
there are some projects that actually

214
00:09:11,189 --> 00:09:16,649
use this to achieve interactive speeds

215
00:09:13,350 --> 00:09:19,160
like X camera pirate and sprocket but

216
00:09:16,649 --> 00:09:22,319
you know using closed functions is

217
00:09:19,160 --> 00:09:25,618
challenging it's not a normal 10,000

218
00:09:22,319 --> 00:09:27,240
core supercomputer so this is not really

219
00:09:25,619 --> 00:09:29,490
about pushing the boundaries of computer

220
00:09:27,240 --> 00:09:32,910
science it's more about the mechanistic

221
00:09:29,490 --> 00:09:34,619
improvements that we've made to the way

222
00:09:32,910 --> 00:09:36,600
that these cost functions are used and

223
00:09:34,619 --> 00:09:40,350
I'm going to tell you about a few of

224
00:09:36,600 --> 00:09:43,529
them so we made sure that Gigi runs fast

225
00:09:40,350 --> 00:09:45,360
reliably and efficiently for example we

226
00:09:43,529 --> 00:09:47,519
made a lot of effort into our startup

227
00:09:45,360 --> 00:09:49,290
time and Gigi has faster startup than

228
00:09:47,519 --> 00:09:52,259
the other system that were built on top

229
00:09:49,290 --> 00:09:54,748
of these cloud function services also

230
00:09:52,259 --> 00:09:56,519
about getting the data from your laptop

231
00:09:54,749 --> 00:09:58,439
to the clock like your source files or

232
00:09:56,519 --> 00:10:01,319
any other thing that you need to offload

233
00:09:58,439 --> 00:10:03,689
by using techniques like HTTP pipelining

234
00:10:01,319 --> 00:10:08,479
and multi-threading we actually managed

235
00:10:03,689 --> 00:10:11,040
to outperform AWS official utilities and

236
00:10:08,480 --> 00:10:13,290
there is also this debate about how do

237
00:10:11,040 --> 00:10:16,199
we want to exchange data between these

238
00:10:13,290 --> 00:10:18,269
workers you know what other applications

239
00:10:16,199 --> 00:10:20,998
usually did is that they one worker

240
00:10:18,269 --> 00:10:23,639
dumps it its output to s3 and the other

241
00:10:20,999 --> 00:10:27,990
one grabs that output from is-3 and that

242
00:10:23,639 --> 00:10:29,819
adds a lot of latency but it was bleed

243
00:10:27,990 --> 00:10:31,290
for a long time that the Lando's they

244
00:10:29,819 --> 00:10:33,389
are not allowed to talk to each other

245
00:10:31,290 --> 00:10:35,248
but we found out that using

246
00:10:33,389 --> 00:10:36,749
off-the-shelf natural salt techniques

247
00:10:35,249 --> 00:10:38,399
the Landers can actually talk to each

248
00:10:36,749 --> 00:10:41,639
other and of course this is our current

249
00:10:38,399 --> 00:10:43,319
work but we see speeds up to 600

250
00:10:41,639 --> 00:10:45,209
megabits per second and this can

251
00:10:43,319 --> 00:10:47,759
actually enable a lot of latency

252
00:10:45,209 --> 00:10:50,179
sensitive applications to be executed on

253
00:10:47,759 --> 00:10:53,279
top of these cloud function platforms

254
00:10:50,179 --> 00:10:55,079
moving on so our solution for managing

255
00:10:53,279 --> 00:10:58,199
software dependencies is lightweight

256
00:10:55,079 --> 00:10:59,969
containers or thunks to reduce the

257
00:10:58,199 --> 00:11:02,399
round-trip to the cloud we have a way to

258
00:10:59,970 --> 00:11:05,790
describe the whole job and offload that

259
00:11:02,399 --> 00:11:08,939
in one shot to the cloud the linked

260
00:11:05,790 --> 00:11:10,589
containers or GGI are and they spend a

261
00:11:08,939 --> 00:11:15,209
lot of time just to make sure that this

262
00:11:10,589 --> 00:11:17,129
actually works well okay I'm going to

263
00:11:15,209 --> 00:11:18,268
talk about one application today and you

264
00:11:17,129 --> 00:11:20,130
can read about the rest of the

265
00:11:18,269 --> 00:11:23,220
applications that we poured it to Gigi

266
00:11:20,130 --> 00:11:25,259
in our paper software compliation you

267
00:11:23,220 --> 00:11:27,540
know people have spent a lot of time

268
00:11:25,259 --> 00:11:30,690
using tools like make CMake and Ninja

269
00:11:27,540 --> 00:11:33,180
and they created these complicated build

270
00:11:30,690 --> 00:11:35,100
system for their projects and we can't

271
00:11:33,180 --> 00:11:36,810
really go and ask like you can you

272
00:11:35,100 --> 00:11:39,209
please like describe your build system

273
00:11:36,810 --> 00:11:41,790
in terms of GG thongs so I can build it

274
00:11:39,209 --> 00:11:44,099
like faster it's not it's not really

275
00:11:41,790 --> 00:11:47,430
possible so we need to automatically

276
00:11:44,100 --> 00:11:50,160
infer GG thongs from an existing build

277
00:11:47,430 --> 00:11:51,779
system and/or solution to that is a

278
00:11:50,160 --> 00:11:53,730
technique that we call model

279
00:11:51,779 --> 00:11:56,910
substitution and the idea is kind of

280
00:11:53,730 --> 00:11:59,579
simple we run the original build system

281
00:11:56,910 --> 00:12:01,829
whatever that is make ninja anything but

282
00:11:59,579 --> 00:12:03,899
we replace the build stages like the

283
00:12:01,829 --> 00:12:06,930
compiler or pre process or the linker

284
00:12:03,899 --> 00:12:09,089
with these model programs the programs

285
00:12:06,930 --> 00:12:12,029
that understand the behavior of their

286
00:12:09,089 --> 00:12:14,550
substitutions but instead of actually

287
00:12:12,029 --> 00:12:17,730
running that task they produce thunks

288
00:12:14,550 --> 00:12:20,399
that can be executed later on the clock

289
00:12:17,730 --> 00:12:26,699
let me actually show you how it works in

290
00:12:20,399 --> 00:12:30,029
action so here I want to build ffmpeg on

291
00:12:26,699 --> 00:12:34,319
a lobbyist lander and I have the source

292
00:12:30,029 --> 00:12:36,329
code here I'm going to run GG init

293
00:12:34,319 --> 00:12:39,300
command it just initializes GG for the

294
00:12:36,329 --> 00:12:41,638
current project nothing special if I

295
00:12:39,300 --> 00:12:43,769
want to build this program on my local

296
00:12:41,639 --> 00:12:46,230
machine I'm a laptop I would run make

297
00:12:43,769 --> 00:12:48,360
dash J and the number of processes that

298
00:12:46,230 --> 00:12:52,019
I have to run and run that bit like four

299
00:12:48,360 --> 00:12:54,600
or eight way parallelism but now I want

300
00:12:52,019 --> 00:12:57,389
to actually get this thunk abstraction

301
00:12:54,600 --> 00:12:59,310
out of this project so I can outsource

302
00:12:57,389 --> 00:13:01,589
that to the cloud so what I would do is

303
00:12:59,310 --> 00:13:05,638
that I just prepends my normal build

304
00:13:01,589 --> 00:13:07,319
command bgg infer it does that model

305
00:13:05,639 --> 00:13:12,000
substitution that I just told you about

306
00:13:07,319 --> 00:13:13,670
and as you can see it will be done

307
00:13:12,000 --> 00:13:17,880
pretty quickly

308
00:13:13,670 --> 00:13:20,639
come on because it didn't do any useful

309
00:13:17,880 --> 00:13:23,009
work you know it just created thunks it

310
00:13:20,639 --> 00:13:24,660
didn't actually compile anything and

311
00:13:23,009 --> 00:13:27,089
then you look at the directory you see

312
00:13:24,660 --> 00:13:29,459
that all the build targets are there

313
00:13:27,089 --> 00:13:31,470
like ffmpeg is there an F F probe is

314
00:13:29,459 --> 00:13:32,959
there so what's happening when you look

315
00:13:31,470 --> 00:13:35,760
at the content

316
00:13:32,960 --> 00:13:38,100
see that the content is just the hash it

317
00:13:35,760 --> 00:13:41,610
tells you that if you want my actual

318
00:13:38,100 --> 00:13:43,920
output go and execute that thunk and it

319
00:13:41,610 --> 00:13:46,380
will give you my actual output and we

320
00:13:43,920 --> 00:13:56,310
can actually look at that phone using GG

321
00:13:46,380 --> 00:13:58,560
describe utility toh JB oh okay wait so

322
00:13:56,310 --> 00:14:00,839
for example here it says that if you

323
00:13:58,560 --> 00:14:04,709
want my output run the strip utility on

324
00:14:00,840 --> 00:14:08,040
ffmpeg underscore G file and if I scroll

325
00:14:04,710 --> 00:14:10,770
down I see that ffmpeg underscore G is

326
00:14:08,040 --> 00:14:13,560
also a thunk itself so I have to go and

327
00:14:10,770 --> 00:14:18,780
recursively execute that tongue first

328
00:14:13,560 --> 00:14:21,689
and feed its output to this thunk okay

329
00:14:18,780 --> 00:14:25,189
now if I want to actually execute this

330
00:14:21,690 --> 00:14:28,530
on lambda I can say that GG please force

331
00:14:25,190 --> 00:14:36,180
ffmpeg for me I want to run this with

332
00:14:28,530 --> 00:14:37,620
2000 by parallelism on a double and let

333
00:14:36,180 --> 00:14:42,930
me just make sure that everything is

334
00:14:37,620 --> 00:14:44,280
going to fit on the screen okay so the

335
00:14:42,930 --> 00:14:46,349
first thing that is going to happen is

336
00:14:44,280 --> 00:14:48,480
that the program is going to upload all

337
00:14:46,350 --> 00:14:50,670
of the files that I need for executing

338
00:14:48,480 --> 00:14:52,380
this test in the cloud in one shot and

339
00:14:50,670 --> 00:14:54,300
it happens in just six seconds I

340
00:14:52,380 --> 00:14:57,720
uploaded a hundred megawatts of

341
00:14:54,300 --> 00:15:01,199
dependencies and as you can see in just

342
00:14:57,720 --> 00:15:05,580
a few seconds by spending 30 cents I

343
00:15:01,200 --> 00:15:07,590
compiled over 1805 and now we get to the

344
00:15:05,580 --> 00:15:10,650
final stages of the build where we have

345
00:15:07,590 --> 00:15:14,820
like the link step and the strip and now

346
00:15:10,650 --> 00:15:17,370
it only downloads back the build target

347
00:15:14,820 --> 00:15:21,540
nothing else no other intermediate data

348
00:15:17,370 --> 00:15:28,580
and I can start run MPEG as if it was

349
00:15:21,540 --> 00:15:28,579
built on my local laptop thank you

350
00:15:30,890 --> 00:15:36,120
so we found out that for medium-sized

351
00:15:33,960 --> 00:15:37,620
and large size packages Gigi can

352
00:15:36,120 --> 00:15:40,740
actually improve the speed of

353
00:15:37,620 --> 00:15:43,350
compliation by two to five x compared to

354
00:15:40,740 --> 00:15:45,270
ice cream outsourcing to a warm cluster

355
00:15:43,350 --> 00:15:47,700
and one interesting thing about an

356
00:15:45,270 --> 00:15:49,620
interesting thing about ICC is that its

357
00:15:47,700 --> 00:15:51,510
performance doesn't really improve as

358
00:15:49,620 --> 00:15:54,300
you increase the number of course after

359
00:15:51,510 --> 00:15:58,830
some points because your laptop becomes

360
00:15:54,300 --> 00:16:01,319
the bottleneck and ICC can build enscape

361
00:15:58,830 --> 00:16:03,090
in seven minutes GG does that on aww

362
00:16:01,320 --> 00:16:05,850
stand Oh in a minute and a half without

363
00:16:03,090 --> 00:16:08,250
requiring a varam cluster and it does

364
00:16:05,850 --> 00:16:09,990
that with 50 cent per run much much

365
00:16:08,250 --> 00:16:13,770
cheaper than maintaining a cluster of

366
00:16:09,990 --> 00:16:16,770
400 notes so Gigi can compile google

367
00:16:13,770 --> 00:16:19,590
chrome on AWS lambda on in 18 minutes

368
00:16:16,770 --> 00:16:21,750
you can run unit tests for leap VPX in

369
00:16:19,590 --> 00:16:24,750
just four minutes you can encode a

370
00:16:21,750 --> 00:16:27,180
fifteen minute long 4k video in two and

371
00:16:24,750 --> 00:16:29,330
a half minutes and rendering a single

372
00:16:27,180 --> 00:16:31,680
frame of monsters university animation

373
00:16:29,330 --> 00:16:35,790
that's our current project we are about

374
00:16:31,680 --> 00:16:38,280
to find out so DG is a framework and a

375
00:16:35,790 --> 00:16:40,560
toolkit that makes it practical to

376
00:16:38,280 --> 00:16:43,050
outsource everyday applications to run

377
00:16:40,560 --> 00:16:45,540
with thousands of a parallelism on code

378
00:16:43,050 --> 00:16:48,689
function services such as AWS lambda and

379
00:16:45,540 --> 00:16:50,310
Google Cloud functions and you know the

380
00:16:48,690 --> 00:16:52,200
next time that you see your friends

381
00:16:50,310 --> 00:16:54,449
slacking off at work saying that oh I'm

382
00:16:52,200 --> 00:16:57,890
waiting for my code to compile you know

383
00:16:54,450 --> 00:16:57,890
what to tell thank you

384
00:17:02,879 --> 00:17:08,039
thank you we have time for a couple of

385
00:17:05,890 --> 00:17:10,870
questions

386
00:17:08,039 --> 00:17:13,209
hey great talk and a great demo thank

387
00:17:10,869 --> 00:17:15,609
you I had a question on your cost

388
00:17:13,209 --> 00:17:17,230
estimation so is that only in the land

389
00:17:15,609 --> 00:17:20,589
run cost or are you also looking at

390
00:17:17,230 --> 00:17:22,659
egress charges from AWS back and then

391
00:17:20,589 --> 00:17:25,750
access to s3 and storage of data in

392
00:17:22,659 --> 00:17:28,360
history the cost that I had in the table

393
00:17:25,750 --> 00:17:30,700
there are only the cost of executing the

394
00:17:28,359 --> 00:17:32,918
lands the cost of s3 is not indeed I

395
00:17:30,700 --> 00:17:35,020
understand right but I to be fair I

396
00:17:32,919 --> 00:17:36,130
think you should start even counting

397
00:17:35,020 --> 00:17:37,539
that right because that's what the

398
00:17:36,130 --> 00:17:43,570
customer is going to play pay at the end

399
00:17:37,539 --> 00:17:45,429
of the day I think that's a that's a

400
00:17:43,570 --> 00:17:47,740
fair statement but I believe that the

401
00:17:45,429 --> 00:17:49,240
cost of like a storing like a hundred

402
00:17:47,740 --> 00:17:51,820
megabytes on s3 or restoring this

403
00:17:49,240 --> 00:17:54,490
intermediate data for just you know a

404
00:17:51,820 --> 00:17:56,830
few seconds or minute that these these

405
00:17:54,490 --> 00:17:58,570
jobs are running I think the cost of

406
00:17:56,830 --> 00:18:00,789
lambda is actually dominating the cost

407
00:17:58,570 --> 00:18:02,559
of the whole job fair enough but egress

408
00:18:00,789 --> 00:18:04,240
is going to start biting you in the end

409
00:18:02,559 --> 00:18:07,870
because it's like four cents for a

410
00:18:04,240 --> 00:18:10,270
gigabyte in bigger jobs it could yeah

411
00:18:07,870 --> 00:18:11,949
and the other question I had was maybe I

412
00:18:10,270 --> 00:18:14,950
misunderstood this but you made a claim

413
00:18:11,950 --> 00:18:18,970
that Gigi transfers data fastest to s3

414
00:18:14,950 --> 00:18:20,169
than AWS CLI using multi-threading so

415
00:18:18,970 --> 00:18:22,419
can you tell me the difference because

416
00:18:20,169 --> 00:18:23,980
AWS CLI also has multi-threading right

417
00:18:22,419 --> 00:18:26,799
so what different are you doing that

418
00:18:23,980 --> 00:18:28,360
makes it go faster I think the reason

419
00:18:26,799 --> 00:18:31,360
that is faster is actually the fact that

420
00:18:28,360 --> 00:18:34,408
we are using HTTP pipelining so we open

421
00:18:31,360 --> 00:18:37,479
one TCP connection to AWS lambda and we

422
00:18:34,409 --> 00:18:43,090
basically put like a hundred requests to

423
00:18:37,480 --> 00:18:47,919
upload files at the same time thank you

424
00:18:43,090 --> 00:18:51,158
I first of all yes great doc thank you I

425
00:18:47,919 --> 00:18:53,380
have a question if I wanted to add some

426
00:18:51,159 --> 00:18:56,049
new application for example you support

427
00:18:53,380 --> 00:18:58,929
now unit testing and build and I want to

428
00:18:56,049 --> 00:19:00,879
add something new bazaar of my own what

429
00:18:58,929 --> 00:19:03,640
would be the effort required is it

430
00:19:00,880 --> 00:19:08,200
something that anyone can do is it like

431
00:19:03,640 --> 00:19:12,220
a no years of work thank you I think

432
00:19:08,200 --> 00:19:14,500
that's a very important question so

433
00:19:12,220 --> 00:19:17,500
I think the question actually has two

434
00:19:14,500 --> 00:19:19,899
parts the first part is that you have to

435
00:19:17,500 --> 00:19:22,029
first know if you have a thousand course

436
00:19:19,899 --> 00:19:24,219
how do you want to use them you know

437
00:19:22,029 --> 00:19:26,320
like in some applications we don't even

438
00:19:24,220 --> 00:19:28,450
know what to do if we have a thousand

439
00:19:26,320 --> 00:19:30,519
course we might not actually have a way

440
00:19:28,450 --> 00:19:33,779
to speed them up even if we have a

441
00:19:30,519 --> 00:19:36,429
thousand score on my local laptop and

442
00:19:33,779 --> 00:19:40,269
the second question is the second part

443
00:19:36,429 --> 00:19:42,009
is that so currently Gigi is not really

444
00:19:40,269 --> 00:19:44,590
transparent to the system you know we

445
00:19:42,009 --> 00:19:46,690
have to explicitly write these thunks

446
00:19:44,590 --> 00:19:49,899
and basically describe our whole

447
00:19:46,690 --> 00:19:52,320
dependency graph so the effort that is

448
00:19:49,899 --> 00:19:55,809
required I think first is we have to

449
00:19:52,320 --> 00:19:58,000
write the programs that do these small

450
00:19:55,809 --> 00:20:00,610
tasks and then the programmer has to

451
00:19:58,000 --> 00:20:02,620
spend some time creating these thunks

452
00:20:00,610 --> 00:20:05,620
and describing the dependency graph for

453
00:20:02,620 --> 00:20:07,539
their program like but of course what we

454
00:20:05,620 --> 00:20:09,309
built for example for software

455
00:20:07,539 --> 00:20:11,049
compilation it's just a drop-in

456
00:20:09,309 --> 00:20:12,668
replacement you know you don't have to

457
00:20:11,049 --> 00:20:15,220
like many make any changes to your

458
00:20:12,669 --> 00:20:17,529
project but for other things I think

459
00:20:15,220 --> 00:20:19,450
there is there could be a lot of effort

460
00:20:17,529 --> 00:20:21,159
to figuring out how do we want to use

461
00:20:19,450 --> 00:20:23,620
these threads and what should be your

462
00:20:21,159 --> 00:20:27,009
tasks and how should we design your

463
00:20:23,620 --> 00:20:28,689
dependency graph thank you and one more

464
00:20:27,009 --> 00:20:32,169
question you mentioned the docker

465
00:20:28,690 --> 00:20:35,399
containers did you compare your solution

466
00:20:32,169 --> 00:20:39,639
to running to using docker containers

467
00:20:35,399 --> 00:20:45,518
actually we did I have some results here

468
00:20:39,639 --> 00:20:47,110
so here we actually ran this job it's a

469
00:20:45,519 --> 00:20:49,690
thousand workers each is sleeping for

470
00:20:47,110 --> 00:20:52,840
two seconds so this job has to be done

471
00:20:49,690 --> 00:20:54,820
in two seconds ideally and we ran it on

472
00:20:52,840 --> 00:20:57,428
google kubernetes engine using docker

473
00:20:54,820 --> 00:21:00,700
containers and it took three minute and

474
00:20:57,429 --> 00:21:02,980
eight seconds so it seems that like the

475
00:21:00,700 --> 00:21:05,409
process of storing the containers and

476
00:21:02,980 --> 00:21:07,629
like transferring the images could

477
00:21:05,409 --> 00:21:08,139
dominate the task thank you thank you

478
00:21:07,629 --> 00:21:10,240
very much

479
00:21:08,139 --> 00:21:11,530
thank you very much let's thank the

480
00:21:10,240 --> 00:21:13,590
speaker again

481
00:21:11,530 --> 00:21:13,590
you

