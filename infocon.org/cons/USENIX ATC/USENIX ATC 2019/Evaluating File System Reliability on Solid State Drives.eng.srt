1
00:00:10,750 --> 00:00:16,329
good morning everyone and thanks a lot

2
00:00:12,459 --> 00:00:18,099
for for coming today I'm going to

3
00:00:16,329 --> 00:00:20,020
present the work that we've done at the

4
00:00:18,099 --> 00:00:21,610
University of Toronto on the evaluation

5
00:00:20,020 --> 00:00:24,730
of file system reliability on top of

6
00:00:21,610 --> 00:00:27,460
solid state drives in the last 15 years

7
00:00:24,730 --> 00:00:29,800
the storage landscape has changed first

8
00:00:27,460 --> 00:00:32,320
SSDs have started replacing hard disks

9
00:00:29,800 --> 00:00:34,030
as persistent storage units however

10
00:00:32,320 --> 00:00:36,489
their failure characteristics are

11
00:00:34,030 --> 00:00:39,100
different for instance partial failures

12
00:00:36,489 --> 00:00:40,809
associated with SSDs are an order of man

13
00:00:39,100 --> 00:00:43,329
who hire than those for hard disks a

14
00:00:40,809 --> 00:00:46,570
trend that will likely get worse with QC

15
00:00:43,329 --> 00:00:48,820
drives second FTL has been shown to be

16
00:00:46,570 --> 00:00:51,129
more prone to bugs during power faults

17
00:00:48,820 --> 00:00:54,039
mainly due to its high overall

18
00:00:51,129 --> 00:00:56,800
complexity finally file systems have

19
00:00:54,039 --> 00:00:59,409
evolved significantly ext3 has been

20
00:00:56,800 --> 00:01:01,059
replaced by ext4 while new file systems

21
00:00:59,409 --> 00:01:04,089
have been introduced such as battery FS

22
00:01:01,059 --> 00:01:06,220
and F 2 FS these three file systems

23
00:01:04,089 --> 00:01:08,679
cover three different points in the

24
00:01:06,220 --> 00:01:10,720
design spectrum ranging from journaling

25
00:01:08,680 --> 00:01:14,140
to copy on write to the log structure

26
00:01:10,720 --> 00:01:17,170
approaches in our study we are we

27
00:01:14,140 --> 00:01:19,030
explore how file systems the ability of

28
00:01:17,170 --> 00:01:23,229
file systems to detect and recover from

29
00:01:19,030 --> 00:01:24,640
partial Drive errors so what do we

30
00:01:23,229 --> 00:01:27,549
currently know about this

31
00:01:24,640 --> 00:01:29,409
the RM file systems paper studied for

32
00:01:27,549 --> 00:01:32,590
file systems that were common back in

33
00:01:29,409 --> 00:01:34,659
2005 however this study includes errors

34
00:01:32,590 --> 00:01:37,600
related to hard disks only and does not

35
00:01:34,659 --> 00:01:39,880
consider file system checkers in our

36
00:01:37,600 --> 00:01:41,949
study we focus on three modern file

37
00:01:39,880 --> 00:01:44,020
systems and explore their ability to

38
00:01:41,950 --> 00:01:45,369
detect and recover from errors along

39
00:01:44,020 --> 00:01:49,479
with the ability of their file system

40
00:01:45,369 --> 00:01:52,359
checkers to resolve issues of line in

41
00:01:49,479 --> 00:01:56,170
our study we focus on local file systems

42
00:01:52,359 --> 00:01:58,419
deployed on top of a single SSD and in

43
00:01:56,170 --> 00:02:00,549
our study we ask the following questions

44
00:01:58,420 --> 00:02:02,740
what happens if the underlying storage

45
00:02:00,549 --> 00:02:05,049
device starts misbehaving and generating

46
00:02:02,740 --> 00:02:07,240
errors how can the file system deal with

47
00:02:05,049 --> 00:02:10,810
those errors and what are the actual

48
00:02:07,240 --> 00:02:12,490
errors that can take place so we begin

49
00:02:10,810 --> 00:02:13,780
our study by considering all the

50
00:02:12,490 --> 00:02:16,510
different faults that can take place

51
00:02:13,780 --> 00:02:18,250
within an SSD and map each fault to the

52
00:02:16,510 --> 00:02:21,040
corresponding manifestation of the file

53
00:02:18,250 --> 00:02:23,800
system level for instance while reading

54
00:02:21,040 --> 00:02:27,970
a block an SSD may encounter an anchor

55
00:02:23,800 --> 00:02:29,920
bit corruption which will result in a

56
00:02:27,970 --> 00:02:32,440
redial error at the file system layer

57
00:02:29,920 --> 00:02:35,859
since the corresponding data is no

58
00:02:32,440 --> 00:02:38,230
longer available second an incomplete

59
00:02:35,860 --> 00:02:40,390
program operation within the SSD can

60
00:02:38,230 --> 00:02:42,940
manifest either as corruption or as a

61
00:02:40,390 --> 00:02:45,399
shortened right note that this

62
00:02:42,940 --> 00:02:47,710
particular fault is silent the SSD

63
00:02:45,400 --> 00:02:50,460
itself cannot detect kind of detect the

64
00:02:47,710 --> 00:02:53,290
error using its own internal mechanisms

65
00:02:50,460 --> 00:02:56,020
ash on writing particular denotes the

66
00:02:53,290 --> 00:02:58,390
case where of where a right issued by

67
00:02:56,020 --> 00:03:01,390
the file system is partially persisted

68
00:02:58,390 --> 00:03:05,440
within the SSD and can only occur after

69
00:03:01,390 --> 00:03:07,660
a power fault so future reads after AB

70
00:03:05,440 --> 00:03:11,829
storm right has been persisted will only

71
00:03:07,660 --> 00:03:13,570
receive partially written data finally

72
00:03:11,830 --> 00:03:16,500
dropped writes have several

73
00:03:13,570 --> 00:03:20,230
manifestations at the file system level

74
00:03:16,500 --> 00:03:22,300
for instance they can result in a write

75
00:03:20,230 --> 00:03:24,910
i/o error if the corresponding SSD

76
00:03:22,300 --> 00:03:26,890
cannot accommodate the request or worse

77
00:03:24,910 --> 00:03:29,260
the write request can be completely

78
00:03:26,890 --> 00:03:32,649
disregarded with the SSD never returning

79
00:03:29,260 --> 00:03:34,720
an error about it we refer you to our

80
00:03:32,650 --> 00:03:37,950
paper in order to find all the different

81
00:03:34,720 --> 00:03:41,770
faults that we considered in our study

82
00:03:37,950 --> 00:03:43,929
so as considering that the SSD itself

83
00:03:41,770 --> 00:03:46,090
cannot be modified and also that we

84
00:03:43,930 --> 00:03:48,520
prefer not to modify the source code of

85
00:03:46,090 --> 00:03:50,320
file systems then we had to come up with

86
00:03:48,520 --> 00:03:52,540
a different way to test the resiliency

87
00:03:50,320 --> 00:03:55,750
of our systems against the different SSD

88
00:03:52,540 --> 00:03:58,060
faults an important observation that we

89
00:03:55,750 --> 00:03:59,860
made was that all these things look

90
00:03:58,060 --> 00:04:02,020
exactly the same at the block layer and

91
00:03:59,860 --> 00:04:04,209
for this reason we decided to emulate

92
00:04:02,020 --> 00:04:06,970
the false manifestation by injecting

93
00:04:04,209 --> 00:04:09,070
errors at the block layer specifically

94
00:04:06,970 --> 00:04:11,350
we implemented our own device mapper

95
00:04:09,070 --> 00:04:13,239
module which sits in between the file

96
00:04:11,350 --> 00:04:15,310
system and the block layer and is

97
00:04:13,239 --> 00:04:18,760
capable of intercepting every single i/o

98
00:04:15,310 --> 00:04:20,890
request in particular the module can

99
00:04:18,760 --> 00:04:23,890
fail a single request and return an

100
00:04:20,890 --> 00:04:26,469
error to the upper layer it can silently

101
00:04:23,890 --> 00:04:28,960
drop a request to the disk or it can

102
00:04:26,470 --> 00:04:30,700
alter our blocks contents online in

103
00:04:28,960 --> 00:04:34,359
order to emulate foreign rights and

104
00:04:30,700 --> 00:04:36,550
corruption for every particular error

105
00:04:34,360 --> 00:04:37,080
that we inject we study the file systems

106
00:04:36,550 --> 00:04:40,620
behave

107
00:04:37,080 --> 00:04:42,688
in detail understanding the effect of an

108
00:04:40,620 --> 00:04:45,270
error depends on which part of the file

109
00:04:42,689 --> 00:04:47,729
system is affected file systems employ

110
00:04:45,270 --> 00:04:50,609
different policies depending on the

111
00:04:47,729 --> 00:04:52,560
operation and the type of the affected

112
00:04:50,610 --> 00:04:54,180
block for this reason it is very

113
00:04:52,560 --> 00:04:56,340
important to identify and target

114
00:04:54,180 --> 00:04:59,009
specific data structures and fields

115
00:04:56,340 --> 00:05:00,508
within blocks to achieve that we use a

116
00:04:59,009 --> 00:05:02,699
combination of different techniques

117
00:05:00,509 --> 00:05:04,860
which involved tracing of all i/o

118
00:05:02,699 --> 00:05:07,020
requests and offline file system tools

119
00:05:04,860 --> 00:05:09,569
such as dam in order to be able to

120
00:05:07,020 --> 00:05:13,400
inject different errors into targeted

121
00:05:09,569 --> 00:05:15,210
data structures and fields within blocks

122
00:05:13,400 --> 00:05:19,169
applications interact with a file system

123
00:05:15,210 --> 00:05:20,609
by issuing system calls we consider

124
00:05:19,169 --> 00:05:22,680
different applications which

125
00:05:20,610 --> 00:05:26,190
pedantically follow posting semantics

126
00:05:22,680 --> 00:05:28,379
and call F sync to persist their data in

127
00:05:26,190 --> 00:05:31,789
particular its application focuses on a

128
00:05:28,379 --> 00:05:34,349
single operation such as mkd or creates

129
00:05:31,789 --> 00:05:36,029
during our experiments we run an

130
00:05:34,349 --> 00:05:39,330
application and collect all the

131
00:05:36,029 --> 00:05:41,969
different blocks that this particular

132
00:05:39,330 --> 00:05:44,490
application accesses so by using the i/o

133
00:05:41,969 --> 00:05:46,560
trace we can collect all the different

134
00:05:44,490 --> 00:05:49,259
blocks and then what we do is that we

135
00:05:46,560 --> 00:05:51,840
repeat the execution by injecting a

136
00:05:49,259 --> 00:05:54,169
single error into each access block

137
00:05:51,840 --> 00:05:56,638
targeting one block at a time

138
00:05:54,169 --> 00:05:59,789
this approach gives us the ability to

139
00:05:56,639 --> 00:06:02,940
better isolate and characterize the file

140
00:05:59,789 --> 00:06:07,740
systems behavior and reaction to the

141
00:06:02,940 --> 00:06:09,870
corresponding injected error so for each

142
00:06:07,740 --> 00:06:11,610
individual test case we categorize the

143
00:06:09,870 --> 00:06:14,129
file systems detection and recovery

144
00:06:11,610 --> 00:06:16,560
policies by using all visible aspects

145
00:06:14,129 --> 00:06:18,960
such as logs and return codes and also

146
00:06:16,560 --> 00:06:21,330
by comparing disk images at the end of

147
00:06:18,960 --> 00:06:23,638
each test case we also invoke the file

148
00:06:21,330 --> 00:06:26,599
system checker and observe if it was

149
00:06:23,639 --> 00:06:29,219
able to actually recover the file system

150
00:06:26,599 --> 00:06:31,259
we have several recovery and detection

151
00:06:29,219 --> 00:06:32,879
levels along with a few additional

152
00:06:31,259 --> 00:06:35,819
categories that pertain to file system

153
00:06:32,879 --> 00:06:38,039
checkers for instance error detection at

154
00:06:35,819 --> 00:06:40,529
the file system level can take place via

155
00:06:38,039 --> 00:06:43,830
error codes or by sanity checking the

156
00:06:40,529 --> 00:06:46,349
return block while recovery can take

157
00:06:43,830 --> 00:06:48,349
place by retrying the operation or by

158
00:06:46,349 --> 00:06:50,880
propagating the arrow to the upper layer

159
00:06:48,349 --> 00:06:54,390
we refer you to our paper

160
00:06:50,880 --> 00:06:58,140
in order to for the complete taxonomy

161
00:06:54,390 --> 00:07:00,750
used in our study in our experiments we

162
00:06:58,140 --> 00:07:03,630
considered different applications data

163
00:07:00,750 --> 00:07:08,460
structures and mode exploring over 7000

164
00:07:03,630 --> 00:07:11,880
test cases in Toro this is a complete

165
00:07:08,460 --> 00:07:14,340
view of all our results in the remainder

166
00:07:11,880 --> 00:07:16,469
of the presentation we will focus on

167
00:07:14,340 --> 00:07:18,719
each file system in detail and present

168
00:07:16,470 --> 00:07:21,630
the most important findings for each one

169
00:07:18,720 --> 00:07:25,050
of them but as a general observation we

170
00:07:21,630 --> 00:07:27,030
would like to point out that in 16

171
00:07:25,050 --> 00:07:29,330
percent of all these cases the file

172
00:07:27,030 --> 00:07:32,070
system became completely on mountable or

173
00:07:29,330 --> 00:07:36,539
completely unrecoverable by the

174
00:07:32,070 --> 00:07:38,520
corresponding system checker this is a

175
00:07:36,540 --> 00:07:41,790
five five thousand mile view of our

176
00:07:38,520 --> 00:07:44,099
results as we observe ext4 reliability

177
00:07:41,790 --> 00:07:47,730
has significantly improved compared to

178
00:07:44,100 --> 00:07:50,040
ext3 butterface excels at detecting io

179
00:07:47,730 --> 00:07:52,230
errors and corruption events but the

180
00:07:50,040 --> 00:07:57,870
file system fails to recover in several

181
00:07:52,230 --> 00:08:00,720
cases F 2 FS has the weakest detection

182
00:07:57,870 --> 00:08:02,910
and recovery against all the different

183
00:08:00,720 --> 00:08:04,470
errors that we emulate despite the fact

184
00:08:02,910 --> 00:08:08,430
that is actually a file system that's

185
00:08:04,470 --> 00:08:11,910
tailored for flash devices so beginning

186
00:08:08,430 --> 00:08:13,650
with ext4 we observe that it will x4 is

187
00:08:11,910 --> 00:08:16,260
actually capable of detecting and

188
00:08:13,650 --> 00:08:18,659
recovering from a large range of fall

189
00:08:16,260 --> 00:08:21,200
scenarios despite making little use of

190
00:08:18,660 --> 00:08:24,510
checksums the file system is capable of

191
00:08:21,200 --> 00:08:27,450
dealing with corruption due to a very

192
00:08:24,510 --> 00:08:29,789
rich set of sanity checks due to the

193
00:08:27,450 --> 00:08:31,500
robustness of its system checker several

194
00:08:29,790 --> 00:08:34,289
data structures can be reconstructed or

195
00:08:31,500 --> 00:08:36,090
flying still due to loss and thrown

196
00:08:34,289 --> 00:08:38,280
rights sometimes the affected data

197
00:08:36,090 --> 00:08:41,880
structure cannot be recovered resulting

198
00:08:38,280 --> 00:08:44,069
in their loss overall we conclude that

199
00:08:41,880 --> 00:08:50,290
its reliability has significantly

200
00:08:44,070 --> 00:08:54,540
improved compared to ext3 I'm sorry

201
00:08:50,290 --> 00:08:57,610
so what about butter FS butter fess is

202
00:08:54,540 --> 00:09:00,099
capable of consistently detecting Ohio

203
00:08:57,610 --> 00:09:03,160
errors as well as corruption invents due

204
00:09:00,100 --> 00:09:05,890
to the usage of checksums however better

205
00:09:03,160 --> 00:09:08,410
battery FS disables metadata replication

206
00:09:05,890 --> 00:09:11,350
when the underlying device is an SSD and

207
00:09:08,410 --> 00:09:11,850
that and that behavior is the default

208
00:09:11,350 --> 00:09:13,930
one

209
00:09:11,850 --> 00:09:15,630
according to the official documentation

210
00:09:13,930 --> 00:09:19,329
there are two main reasons for that

211
00:09:15,630 --> 00:09:21,850
first SSDs can internally duplicate two

212
00:09:19,330 --> 00:09:23,290
identical logical blocks by remapping

213
00:09:21,850 --> 00:09:26,440
them to a single physical copy

214
00:09:23,290 --> 00:09:28,420
however this behavior would negate the

215
00:09:26,440 --> 00:09:32,560
purpose of increased redundancy and

216
00:09:28,420 --> 00:09:34,689
would waste filesystem space second due

217
00:09:32,560 --> 00:09:36,939
to wear leveling techniques an SSD

218
00:09:34,690 --> 00:09:40,300
controller may put data written together

219
00:09:36,940 --> 00:09:43,000
in a short time span into the same erase

220
00:09:40,300 --> 00:09:45,520
block therefore if the erase block is

221
00:09:43,000 --> 00:09:48,160
lost due to an internal fault both the

222
00:09:45,520 --> 00:09:51,569
primary metadata and its replica will be

223
00:09:48,160 --> 00:09:54,189
lost this design choice causes severe

224
00:09:51,570 --> 00:09:56,770
reliability issues as the file system

225
00:09:54,190 --> 00:09:59,440
relies on metadata duplication to

226
00:09:56,770 --> 00:10:01,960
recover from errors for instance we will

227
00:09:59,440 --> 00:10:03,970
see how node level check sums can result

228
00:10:01,960 --> 00:10:10,600
in data loss because of these particular

229
00:10:03,970 --> 00:10:13,510
design choice so a battery FS organizes

230
00:10:10,600 --> 00:10:16,090
its data using a forest of trees with

231
00:10:13,510 --> 00:10:19,210
each tree serving a purpose a particular

232
00:10:16,090 --> 00:10:22,030
purpose also we observe that also the

233
00:10:19,210 --> 00:10:24,150
file system uses checksums for or in

234
00:10:22,030 --> 00:10:27,280
order to protect all its B 3 nodes

235
00:10:24,150 --> 00:10:30,160
however in case a single byte is

236
00:10:27,280 --> 00:10:32,110
corrupted then I'll check some checksum

237
00:10:30,160 --> 00:10:34,719
mismatch will be triggered and due to

238
00:10:32,110 --> 00:10:37,480
the fact that battery FS has disabled

239
00:10:34,720 --> 00:10:39,520
meta data replication for SSDs then the

240
00:10:37,480 --> 00:10:42,430
file system will lab will end up

241
00:10:39,520 --> 00:10:45,040
discarding the entire node suffering

242
00:10:42,430 --> 00:10:48,569
from data loss in one particular test

243
00:10:45,040 --> 00:10:51,640
case that we ran into the file system

244
00:10:48,570 --> 00:10:55,540
completely discarded sever of 70 files

245
00:10:51,640 --> 00:10:57,520
and directories moving on we observed

246
00:10:55,540 --> 00:10:59,920
that in some cases butterface

247
00:10:57,520 --> 00:11:03,520
could not make use of the existed in

248
00:10:59,920 --> 00:11:06,040
redundancy for instance battery FS

249
00:11:03,520 --> 00:11:08,620
maintains two separate data structures

250
00:11:06,040 --> 00:11:10,660
for its directory in case one of the two

251
00:11:08,620 --> 00:11:12,490
becomes corrupted then the file system

252
00:11:10,660 --> 00:11:14,709
does not use the other for recovery

253
00:11:12,490 --> 00:11:15,600
despite the fact that the information is

254
00:11:14,710 --> 00:11:18,040
right there

255
00:11:15,600 --> 00:11:20,140
finally we observe several cases where

256
00:11:18,040 --> 00:11:23,050
the file system became completely

257
00:11:20,140 --> 00:11:24,850
mountable and a few crashes while the

258
00:11:23,050 --> 00:11:27,609
file system tried to recover from errors

259
00:11:24,850 --> 00:11:29,790
even in those cases invoking bots or FS

260
00:11:27,610 --> 00:11:36,790
the file systems the file system checker

261
00:11:29,790 --> 00:11:39,040
could not fully recover the error last

262
00:11:36,790 --> 00:11:41,469
but not least we present our results for

263
00:11:39,040 --> 00:11:43,480
F 2 FS keeping in mind that this is a

264
00:11:41,470 --> 00:11:48,430
file system that's tailored for flash

265
00:11:43,480 --> 00:11:51,190
devices first of all we observe that ffs

266
00:11:48,430 --> 00:11:53,199
is can successfully detect and

267
00:11:51,190 --> 00:11:56,590
appropriately appropriately propagate

268
00:11:53,200 --> 00:11:58,930
redial errors in most cases however the

269
00:11:56,590 --> 00:12:01,740
file system consistently fails to detect

270
00:11:58,930 --> 00:12:06,910
and recover from right IO errors as

271
00:12:01,740 --> 00:12:08,980
injected by our framework also the file

272
00:12:06,910 --> 00:12:11,620
system suffers from data loss due to

273
00:12:08,980 --> 00:12:12,220
lost and certain rights regarding

274
00:12:11,620 --> 00:12:14,200
corruption

275
00:12:12,220 --> 00:12:16,600
inodes and checkpoints are protected

276
00:12:14,200 --> 00:12:18,970
using checksums but still even for these

277
00:12:16,600 --> 00:12:21,450
data structures recovery sometimes can

278
00:12:18,970 --> 00:12:24,430
be incomplete resulting in data loss

279
00:12:21,450 --> 00:12:26,860
over all corrupted data can have severe

280
00:12:24,430 --> 00:12:29,170
repercussions throughout the file system

281
00:12:26,860 --> 00:12:30,880
as the remaining metadata structures are

282
00:12:29,170 --> 00:12:33,939
not protected using any form of

283
00:12:30,880 --> 00:12:37,360
redundancy and sanity text cannot detect

284
00:12:33,940 --> 00:12:39,310
every single issue finally sometimes and

285
00:12:37,360 --> 00:12:41,050
depending on the error the corresponding

286
00:12:39,310 --> 00:12:42,550
file system checker can actually bring

287
00:12:41,050 --> 00:12:45,459
the file system to a consistent state

288
00:12:42,550 --> 00:12:48,010
however it cannot always restore all the

289
00:12:45,460 --> 00:12:52,060
affected files and in general it cannot

290
00:12:48,010 --> 00:12:53,650
deal with read io errors as the system

291
00:12:52,060 --> 00:12:58,479
checker instantly terminates its

292
00:12:53,650 --> 00:13:00,760
execution with an assertion error based

293
00:12:58,480 --> 00:13:03,010
on our experiments and experience with

294
00:13:00,760 --> 00:13:05,110
all three file systems we would like to

295
00:13:03,010 --> 00:13:08,680
highlight the need to verify the

296
00:13:05,110 --> 00:13:10,720
correctness of metadata as well when

297
00:13:08,680 --> 00:13:12,270
that particular metadata is being read

298
00:13:10,720 --> 00:13:14,470
from the underlying storage device

299
00:13:12,270 --> 00:13:16,069
especially if the metadata is not

300
00:13:14,470 --> 00:13:18,680
protected by any formal

301
00:13:16,070 --> 00:13:21,380
dancy second we have observed that

302
00:13:18,680 --> 00:13:24,650
checksums can be a double-edged sword on

303
00:13:21,380 --> 00:13:27,320
one hand they can actually they actually

304
00:13:24,650 --> 00:13:29,900
increase a detection but on the other

305
00:13:27,320 --> 00:13:31,360
hand coronal RT checksums can lead to

306
00:13:29,900 --> 00:13:34,760
severe data loss

307
00:13:31,360 --> 00:13:36,920
finally throughout our results we

308
00:13:34,760 --> 00:13:39,110
observed that there are certain key data

309
00:13:36,920 --> 00:13:42,380
structures that can cause maximum

310
00:13:39,110 --> 00:13:44,570
recovery failures especially when these

311
00:13:42,380 --> 00:13:46,790
data structures are affected by a silent

312
00:13:44,570 --> 00:13:53,600
error such as such as corruption or

313
00:13:46,790 --> 00:13:56,839
dropped rights we have made our tool and

314
00:13:53,600 --> 00:13:58,460
device mapper code online and with that

315
00:13:56,840 --> 00:14:00,200
I would like to end my presentation

316
00:13:58,460 --> 00:14:01,700
thank you very much for your time and

317
00:14:00,200 --> 00:14:09,080
attention and I would be more than happy

318
00:14:01,700 --> 00:14:12,110
to answer your questions thank you for a

319
00:14:09,080 --> 00:14:14,870
great talk it was a bit frustrating to

320
00:14:12,110 --> 00:14:17,180
see that better FS has a specific

321
00:14:14,870 --> 00:14:20,800
default for SSDs which makes it work

322
00:14:17,180 --> 00:14:23,449
worse do you think in retrospect that

323
00:14:20,800 --> 00:14:25,250
replication of the of the meta data

324
00:14:23,450 --> 00:14:28,460
would have helped in that case in the

325
00:14:25,250 --> 00:14:32,480
case of better FS okay that's that's a

326
00:14:28,460 --> 00:14:34,130
very interesting question so at the

327
00:14:32,480 --> 00:14:37,180
creation of the filesystem you can still

328
00:14:34,130 --> 00:14:40,100
provide the flag and say that okay

329
00:14:37,180 --> 00:14:42,530
enable meta data replication explicitly

330
00:14:40,100 --> 00:14:44,180
but yes in some cases without particular

331
00:14:42,530 --> 00:14:46,430
flags then the file system would have

332
00:14:44,180 --> 00:14:48,589
been capable of recover and detecting

333
00:14:46,430 --> 00:14:51,739
and recovering from some from some

334
00:14:48,590 --> 00:14:55,820
errors but depending on the underlying

335
00:14:51,740 --> 00:14:58,220
SSD and the way that new blocks are

336
00:14:55,820 --> 00:15:00,830
written in place into physical blocks

337
00:14:58,220 --> 00:15:02,660
then there is there still a chance that

338
00:15:00,830 --> 00:15:06,590
the filesystem cannot recover due to

339
00:15:02,660 --> 00:15:09,740
errors in the underlying device Rick

340
00:15:06,590 --> 00:15:12,290
wheeler Facebook - question first a

341
00:15:09,740 --> 00:15:13,700
great talk they like the work have you

342
00:15:12,290 --> 00:15:16,910
thought about trying it for other file

343
00:15:13,700 --> 00:15:18,350
systems like XFS and also have you

344
00:15:16,910 --> 00:15:20,480
reached out to the kernel developers

345
00:15:18,350 --> 00:15:23,300
with some of your feedback yes

346
00:15:20,480 --> 00:15:25,130
so to answer ok so two answers a lot of

347
00:15:23,300 --> 00:15:27,290
the second part of your question yes we

348
00:15:25,130 --> 00:15:28,740
have already submitted some bug reports

349
00:15:27,290 --> 00:15:31,319
and we have already

350
00:15:28,740 --> 00:15:33,510
I am talked to the developers of the

351
00:15:31,320 --> 00:15:35,070
file systems some of the issues that we

352
00:15:33,510 --> 00:15:37,950
have reported have already been fixed

353
00:15:35,070 --> 00:15:39,660
and we are currently working on two or

354
00:15:37,950 --> 00:15:41,970
three different fixes for some of the

355
00:15:39,660 --> 00:15:44,339
errors that we have encountered yes

356
00:15:41,970 --> 00:15:45,900
regarding XFS it's actually a very nice

357
00:15:44,340 --> 00:15:47,760
addition and we are currently

358
00:15:45,900 --> 00:15:51,600
considering of expanding our study to

359
00:15:47,760 --> 00:15:55,170
that thank you thank you Peter does know

360
00:15:51,600 --> 00:15:57,570
Aris northeastern so you know in

361
00:15:55,170 --> 00:15:59,069
contrast to butter FS where that

362
00:15:57,570 --> 00:16:01,530
basically is something that could just

363
00:15:59,070 --> 00:16:05,850
be patched by changing a default in the

364
00:16:01,530 --> 00:16:08,730
make FS program you know did you find

365
00:16:05,850 --> 00:16:11,220
for F 2 FS did you find that any of the

366
00:16:08,730 --> 00:16:13,830
issues are basically baked into the

367
00:16:11,220 --> 00:16:17,310
format or you know would it require on

368
00:16:13,830 --> 00:16:19,560
disk format changes or is it higher

369
00:16:17,310 --> 00:16:22,979
level you know could code fixes change

370
00:16:19,560 --> 00:16:25,709
some of these things yes so regarding

371
00:16:22,980 --> 00:16:27,420
butter of s at first we were a little

372
00:16:25,710 --> 00:16:30,660
bit surprised that the file system

373
00:16:27,420 --> 00:16:32,670
actually decided to turn off data

374
00:16:30,660 --> 00:16:34,829
metadata duplication just because the

375
00:16:32,670 --> 00:16:38,010
underlying device is an SSD but

376
00:16:34,830 --> 00:16:40,980
regarding F 2 FS indeed when we first

377
00:16:38,010 --> 00:16:42,660
began our study I note were not even

378
00:16:40,980 --> 00:16:46,500
protected using checksums right so in

379
00:16:42,660 --> 00:16:49,199
the process they did like the update is

380
00:16:46,500 --> 00:16:50,730
the definition of an inode and added

381
00:16:49,200 --> 00:16:53,250
checks in order to protect them against

382
00:16:50,730 --> 00:16:54,900
corruption so I would say that for some

383
00:16:53,250 --> 00:16:57,180
metadata structures they are not

384
00:16:54,900 --> 00:17:00,750
protected at all against corruption so I

385
00:16:57,180 --> 00:17:03,359
would say that even using let's say

386
00:17:00,750 --> 00:17:05,430
carefully implemented sanity checks you

387
00:17:03,360 --> 00:17:08,040
cannot always detect every single issue

388
00:17:05,430 --> 00:17:10,530
so I would suggest that you know for all

389
00:17:08,040 --> 00:17:12,389
the crucial metadata structures they

390
00:17:10,530 --> 00:17:14,459
have to protect it again you know using

391
00:17:12,390 --> 00:17:17,150
some form or redundancy in order to

392
00:17:14,459 --> 00:17:20,280
basically be capable of recovering from

393
00:17:17,150 --> 00:17:25,020
several different errors thank you thank

394
00:17:20,280 --> 00:17:29,760
you if we have a little layer below the

395
00:17:25,020 --> 00:17:31,760
fair system like so Sam what kind of

396
00:17:29,760 --> 00:17:33,600
errors can be shared up as a little e

397
00:17:31,760 --> 00:17:34,980
can you please repeat your question

398
00:17:33,600 --> 00:17:37,110
because you know I wasn't able to so

399
00:17:34,980 --> 00:17:40,740
basically your experimental you assume

400
00:17:37,110 --> 00:17:41,360
that the fastest enzymes as single SSTV

401
00:17:40,740 --> 00:17:43,880
and

402
00:17:41,360 --> 00:17:46,399
if you have a set of different SSDs and

403
00:17:43,880 --> 00:17:48,980
the way builders really on top of SSDs

404
00:17:46,399 --> 00:17:52,239
sound like what the kind of errors can

405
00:17:48,980 --> 00:17:55,820
be protected by that literally yeah so

406
00:17:52,240 --> 00:17:58,370
using multiple SSDs and using techniques

407
00:17:55,820 --> 00:18:01,428
like raid you can you can protect

408
00:17:58,370 --> 00:18:03,489
against different types of errors but

409
00:18:01,429 --> 00:18:05,539
still even using those techniques

410
00:18:03,490 --> 00:18:07,419
doesn't guarantee that every single

411
00:18:05,539 --> 00:18:10,340
letter will be actually you know

412
00:18:07,419 --> 00:18:13,789
recovered right still the file system

413
00:18:10,340 --> 00:18:17,990
has to perform each own checks and each

414
00:18:13,789 --> 00:18:19,340
own recovery policies to deal with any

415
00:18:17,990 --> 00:18:23,480
others that can take place beneath

416
00:18:19,340 --> 00:18:24,520
beneath it thank you again thank you

417
00:18:23,480 --> 00:18:29,160
very much

418
00:18:24,520 --> 00:18:29,160
[Applause]

