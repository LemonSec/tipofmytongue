1
00:00:10,420 --> 00:00:15,870
okay thanks for the introduction I'm

2
00:00:14,380 --> 00:00:17,580
Utah's Muslim from

3
00:00:15,870 --> 00:00:19,740
the eto Dresden and from the Bauhaus

4
00:00:17,580 --> 00:00:22,860
Institute and ibises joint work with

5
00:00:19,740 --> 00:00:25,669
Micro garage and Helmand heritage and

6
00:00:22,860 --> 00:00:25,669
it's about m3x

7
00:00:26,840 --> 00:00:36,180
no is that okay from the okay and this

8
00:00:31,770 --> 00:00:37,769
talk is about m3x that yeah once

9
00:00:36,180 --> 00:00:39,989
accelerators autonomously and combines

10
00:00:37,770 --> 00:00:42,510
fast path communication with context

11
00:00:39,989 --> 00:00:44,820
switching so let me start by breaking

12
00:00:42,510 --> 00:00:46,739
apart the title the first part of our

13
00:00:44,820 --> 00:00:49,710
work is on a atonal autonomous

14
00:00:46,739 --> 00:00:51,899
accelerators because today accelerators

15
00:00:49,710 --> 00:00:53,550
typically need to be assisted by the CPU

16
00:00:51,899 --> 00:00:55,499
because for example they cannot access

17
00:00:53,550 --> 00:00:56,849
operating system services like file

18
00:00:55,499 --> 00:00:59,280
systems or networks texts on their own

19
00:00:56,850 --> 00:01:01,559
and this leads to a high CPU load caused

20
00:00:59,280 --> 00:01:03,329
by the operating system and also

21
00:01:01,559 --> 00:01:05,190
prevents us from actually benefiting

22
00:01:03,329 --> 00:01:06,600
from the energy efficiency of the

23
00:01:05,190 --> 00:01:08,399
accelerators because we need to power

24
00:01:06,600 --> 00:01:12,360
the CPU most of the time during their

25
00:01:08,400 --> 00:01:14,520
runtime as well the second part of our

26
00:01:12,360 --> 00:01:16,130
work focuses on fast path communication

27
00:01:14,520 --> 00:01:18,479
and previous work has already shown that

28
00:01:16,130 --> 00:01:20,490
pinning applications on course and

29
00:01:18,479 --> 00:01:22,289
having a direct communication channel

30
00:01:20,490 --> 00:01:24,329
between them that bypasses the operating

31
00:01:22,290 --> 00:01:26,040
system kernel leads to significant

32
00:01:24,329 --> 00:01:27,479
performance improvements and examples

33
00:01:26,040 --> 00:01:31,530
are our own previous work and three and

34
00:01:27,479 --> 00:01:33,060
also delay press for example however the

35
00:01:31,530 --> 00:01:34,530
problem is that in contrast to slow path

36
00:01:33,060 --> 00:01:36,500
communication that you can see at the

37
00:01:34,530 --> 00:01:38,759
top here where the kernel is involved

38
00:01:36,500 --> 00:01:41,039
fast both communication doesn't combine

39
00:01:38,759 --> 00:01:42,990
well with context switching and this is

40
00:01:41,039 --> 00:01:44,939
because if the kernel is not involved at

41
00:01:42,990 --> 00:01:46,229
the communication how can the sender

42
00:01:44,939 --> 00:01:47,939
determine whether the receiver is

43
00:01:46,229 --> 00:01:49,560
actually running and if it's not running

44
00:01:47,939 --> 00:01:52,020
how can it deliver the message otherwise

45
00:01:49,560 --> 00:01:53,729
and also how can the kernel make

46
00:01:52,020 --> 00:01:55,320
scheduling or placement decisions if it

47
00:01:53,729 --> 00:01:57,270
doesn't know whether the application is

48
00:01:55,320 --> 00:01:59,339
currently computing or is waiting for a

49
00:01:57,270 --> 00:02:00,929
message for example so we need to solve

50
00:01:59,340 --> 00:02:05,159
several problems in order to support

51
00:02:00,929 --> 00:02:06,899
that in our work we rethink the system

52
00:02:05,159 --> 00:02:09,149
architecture based on our previous work

53
00:02:06,899 --> 00:02:10,649
and three and three is a how-to

54
00:02:09,149 --> 00:02:13,320
operating system code design for

55
00:02:10,649 --> 00:02:14,790
heterogeneous systems we also have real

56
00:02:13,320 --> 00:02:16,859
hardware for a subset of the features

57
00:02:14,790 --> 00:02:20,459
but in this work we use simulation based

58
00:02:16,860 --> 00:02:22,020
on gem 5 to evaluate the system contrast

59
00:02:20,460 --> 00:02:24,209
to others we do not build upon cache

60
00:02:22,020 --> 00:02:26,159
coherency because it's still unclear how

61
00:02:24,209 --> 00:02:28,200
future systems look like so whether

62
00:02:26,159 --> 00:02:30,000
there will be will be globally coherent

63
00:02:28,200 --> 00:02:32,089
or partially coherent or not

64
00:02:30,000 --> 00:02:34,680
and this is why we keep that optional

65
00:02:32,090 --> 00:02:36,420
and last but not least we focus in this

66
00:02:34,680 --> 00:02:38,700
work on so called fixed function

67
00:02:36,420 --> 00:02:40,829
accelerators because they are arguably

68
00:02:38,700 --> 00:02:43,019
the most difficult ones to support as

69
00:02:40,830 --> 00:02:44,100
first-class citizens because they

70
00:02:43,020 --> 00:02:47,220
provide none of the features that

71
00:02:44,100 --> 00:02:48,959
operating systems classically need to

72
00:02:47,220 --> 00:02:50,940
run their like user kernel mode memory

73
00:02:48,959 --> 00:02:52,080
management unit and so on because fixed

74
00:02:50,940 --> 00:02:56,190
function accelerators do not even

75
00:02:52,080 --> 00:02:58,290
execute software okay in terms of

76
00:02:56,190 --> 00:03:00,329
related work there are also other works

77
00:02:58,290 --> 00:03:02,700
that try to achieve similar goals at

78
00:03:00,330 --> 00:03:05,430
first for one specific accelerator for

79
00:03:02,700 --> 00:03:08,070
example that is a GPU of SDP unit for

80
00:03:05,430 --> 00:03:09,780
GPUs to tries to give them access to

81
00:03:08,070 --> 00:03:12,540
operating system services and the same

82
00:03:09,780 --> 00:03:14,850
exist for FPGAs with a week on OS on

83
00:03:12,540 --> 00:03:17,390
both there's also work on context

84
00:03:14,850 --> 00:03:19,859
switching on GPUs for example by Chamara

85
00:03:17,390 --> 00:03:22,500
and their entire operating systems for

86
00:03:19,860 --> 00:03:25,410
heterogeneous systems like berry fresh

87
00:03:22,500 --> 00:03:27,140
popcorn Enix k2 and Helios so I don't

88
00:03:25,410 --> 00:03:28,980
want to do a detailed comparison here

89
00:03:27,140 --> 00:03:30,630
because the main difference is that

90
00:03:28,980 --> 00:03:32,280
these works I do not consider how to

91
00:03:30,630 --> 00:03:34,709
changes and do not and therefore have to

92
00:03:32,280 --> 00:03:38,130
stop earlier so let's see what we can do

93
00:03:34,709 --> 00:03:40,019
if we consider how to changes so I want

94
00:03:38,130 --> 00:03:42,030
to start here with giving you the key

95
00:03:40,019 --> 00:03:44,760
techniques that we use to solve the

96
00:03:42,030 --> 00:03:47,220
problems so the first one is that we add

97
00:03:44,760 --> 00:03:50,130
a uniform interface to these potentially

98
00:03:47,220 --> 00:03:52,709
very heterogeneous compute units that

99
00:03:50,130 --> 00:03:54,690
the second one is that we use simple in

100
00:03:52,709 --> 00:03:57,360
generic protocols between these compute

101
00:03:54,690 --> 00:03:59,400
units for example to give them access to

102
00:03:57,360 --> 00:04:00,959
operating system services and use the

103
00:03:59,400 --> 00:04:04,890
same protocols for all of these compute

104
00:04:00,959 --> 00:04:08,400
units and last but not least we don't

105
00:04:04,890 --> 00:04:10,828
expect the accelerators to have the

106
00:04:08,400 --> 00:04:12,360
features that operating systems need but

107
00:04:10,829 --> 00:04:15,329
instead we want to add a lightweight

108
00:04:12,360 --> 00:04:19,200
component externally to them in order to

109
00:04:15,329 --> 00:04:21,269
speak these protocols that we use ok so

110
00:04:19,200 --> 00:04:22,530
the rest of the talk is structured as

111
00:04:21,269 --> 00:04:26,550
follows I will start with the background

112
00:04:22,530 --> 00:04:29,159
on and three I will then continue with

113
00:04:26,550 --> 00:04:30,690
showing how how we can use the fast path

114
00:04:29,160 --> 00:04:34,050
communication that is already given by

115
00:04:30,690 --> 00:04:36,150
n3 in this case to build accelerators or

116
00:04:34,050 --> 00:04:38,789
to run accelerators autonomously and

117
00:04:36,150 --> 00:04:40,620
finally I will show how we can combine

118
00:04:38,789 --> 00:04:42,719
this fast path communication with

119
00:04:40,620 --> 00:04:46,499
context switching

120
00:04:42,719 --> 00:04:47,759
so let me start with a background so the

121
00:04:46,499 --> 00:04:50,580
uniform interface that I mentioned

122
00:04:47,759 --> 00:04:52,889
earlier is in case of m3 given by the so

123
00:04:50,580 --> 00:04:54,539
called data transfer unit the DTU we're

124
00:04:52,889 --> 00:04:57,269
just sitting next to each compute unit

125
00:04:54,539 --> 00:04:58,699
and this allows these compute units to

126
00:04:57,269 --> 00:05:01,949
interact with each other for example

127
00:04:58,699 --> 00:05:03,929
using message passing so now that we

128
00:05:01,949 --> 00:05:05,309
have this uniform interface we can for

129
00:05:03,929 --> 00:05:07,109
bet forget about the differences among

130
00:05:05,309 --> 00:05:09,779
these computing as for for a minute and

131
00:05:07,110 --> 00:05:11,999
just call them see use so we have now a

132
00:05:09,779 --> 00:05:13,619
bunch of tiles integrated with some

133
00:05:11,999 --> 00:05:17,909
interconnect and we have in each tile

134
00:05:13,619 --> 00:05:20,489
see you compute unit and ADT you in

135
00:05:17,909 --> 00:05:22,229
terms of the operating system the idea

136
00:05:20,489 --> 00:05:24,388
of m3 is that the kernel is running on a

137
00:05:22,229 --> 00:05:26,519
dedicated tile the so-called kono tile

138
00:05:24,389 --> 00:05:28,709
and the applications are running on the

139
00:05:26,519 --> 00:05:32,819
other tiles the user tiles and are

140
00:05:28,709 --> 00:05:34,800
remotely controlled by the kernel m3

141
00:05:32,819 --> 00:05:36,689
already provides fast path communication

142
00:05:34,800 --> 00:05:38,999
and it has two different types of

143
00:05:36,689 --> 00:05:40,799
communication channels the first one is

144
00:05:38,999 --> 00:05:43,199
a memory Channel and the second one is a

145
00:05:40,799 --> 00:05:44,998
message passing channel to represent

146
00:05:43,199 --> 00:05:46,739
these channels the DTU provides so

147
00:05:44,999 --> 00:05:48,449
called endpoints so these are Harper

148
00:05:46,739 --> 00:05:51,209
resources that represent these

149
00:05:48,449 --> 00:05:53,629
communication channels the first channel

150
00:05:51,209 --> 00:05:55,559
is as I said the memory channel which is

151
00:05:53,629 --> 00:05:58,259
represented by this memory endpoint that

152
00:05:55,559 --> 00:05:59,969
you can see and this allows in this

153
00:05:58,259 --> 00:06:04,199
example to use at i/o to access the

154
00:05:59,969 --> 00:06:05,789
piece of data in DRAM the message

155
00:06:04,199 --> 00:06:07,439
passing channel is represented by a sent

156
00:06:05,789 --> 00:06:09,449
and point end point and a receive end

157
00:06:07,439 --> 00:06:12,719
point and this allows these two user

158
00:06:09,449 --> 00:06:14,039
tiles here to exchange messages so in

159
00:06:12,719 --> 00:06:15,989
both cases this is fast path

160
00:06:14,039 --> 00:06:18,329
communication in the sense that although

161
00:06:15,989 --> 00:06:20,518
the kernel is involved to establish this

162
00:06:18,329 --> 00:06:22,050
communication channel it is not involved

163
00:06:20,519 --> 00:06:23,759
in the actual communication so after

164
00:06:22,050 --> 00:06:25,649
setting that up the user types are

165
00:06:23,759 --> 00:06:29,339
communicating directly without involving

166
00:06:25,649 --> 00:06:31,709
the kernel again okay so let us see now

167
00:06:29,339 --> 00:06:34,769
how we can use that you run accelerators

168
00:06:31,709 --> 00:06:36,119
autonomously the end goal is that we

169
00:06:34,769 --> 00:06:37,499
want to have an auditory graph of

170
00:06:36,119 --> 00:06:39,509
compute units interacting with each

171
00:06:37,499 --> 00:06:41,879
other in this case I'm focusing on a

172
00:06:39,509 --> 00:06:43,409
pipeline so we have this shadow command

173
00:06:41,879 --> 00:06:44,849
here that we want to execute consisting

174
00:06:43,409 --> 00:06:47,069
of different parts that we want to

175
00:06:44,849 --> 00:06:49,739
execute on the computer net that fits

176
00:06:47,069 --> 00:06:51,119
the best in each case so we have at

177
00:06:49,739 --> 00:06:55,049
first the shell that is going to execute

178
00:06:51,119 --> 00:06:56,340
that we have at first decode application

179
00:06:55,050 --> 00:06:58,650
so normal software that is

180
00:06:56,340 --> 00:07:00,719
decoding and Phe image and this example

181
00:06:58,650 --> 00:07:02,070
and then we have three hardware

182
00:07:00,720 --> 00:07:04,949
accelerators that do the actual image

183
00:07:02,070 --> 00:07:07,889
processing and finally we write the

184
00:07:04,949 --> 00:07:09,449
results the pixel data in this example

185
00:07:07,889 --> 00:07:12,330
into the output file to do

186
00:07:09,449 --> 00:07:14,070
post-processing later between these we

187
00:07:12,330 --> 00:07:16,289
have pipes and outlet we directions to

188
00:07:14,070 --> 00:07:17,990
to connect them so the question is now

189
00:07:16,290 --> 00:07:20,639
what do we need in order to support that

190
00:07:17,990 --> 00:07:22,440
so at first we need generic protocols

191
00:07:20,639 --> 00:07:24,090
that allows us to arbitrarily combine

192
00:07:22,440 --> 00:07:26,940
these compute units with it with each

193
00:07:24,090 --> 00:07:28,080
other and second the accelerators of

194
00:07:26,940 --> 00:07:29,880
course need to support these protocols

195
00:07:28,080 --> 00:07:31,710
as well to be embedded in such a command

196
00:07:29,880 --> 00:07:33,570
which means they need to be simple and

197
00:07:31,710 --> 00:07:37,320
also need to be flexible enough to meet

198
00:07:33,570 --> 00:07:39,719
the demands of software to explain that

199
00:07:37,320 --> 00:07:42,630
I will use the last part of the command

200
00:07:39,720 --> 00:07:44,520
here the part where the IFFT accelerator

201
00:07:42,630 --> 00:07:46,889
writes the results directly to the

202
00:07:44,520 --> 00:07:49,889
output file so let me start with with

203
00:07:46,889 --> 00:07:51,900
the generic protocol the protocol is

204
00:07:49,889 --> 00:07:52,889
called 5 protocol in the UNIX sense of

205
00:07:51,900 --> 00:07:55,229
everything is a file

206
00:07:52,889 --> 00:07:57,570
and the idea is that the data is stored

207
00:07:55,229 --> 00:07:59,639
in memory and there is a message passing

208
00:07:57,570 --> 00:08:02,340
channel between the client so the IFFT

209
00:07:59,639 --> 00:08:05,130
accelerator at this example and at the

210
00:08:02,340 --> 00:08:08,638
server the file system and this allows

211
00:08:05,130 --> 00:08:12,180
the client to request access to the data

212
00:08:08,639 --> 00:08:13,979
stored in memory the actual data access

213
00:08:12,180 --> 00:08:16,320
is done with a memory endpoint on the

214
00:08:13,979 --> 00:08:18,180
client side and this is configured on

215
00:08:16,320 --> 00:08:20,550
behalf of these requests by the server

216
00:08:18,180 --> 00:08:22,470
so the client can go to the server and

217
00:08:20,550 --> 00:08:24,270
request access to the next piece of

218
00:08:22,470 --> 00:08:26,370
input for example and then the server

219
00:08:24,270 --> 00:08:29,130
would configure its the memory input of

220
00:08:26,370 --> 00:08:30,960
the client and afterwards the client can

221
00:08:29,130 --> 00:08:33,469
use let's teach you to access the data

222
00:08:30,960 --> 00:08:35,429
without involving the server again I

223
00:08:33,469 --> 00:08:37,469
also want to highlight here that this

224
00:08:35,429 --> 00:08:39,539
protocol is not a special protocol for

225
00:08:37,469 --> 00:08:41,459
accelerators but this is the general

226
00:08:39,539 --> 00:08:42,870
file system and general operating

227
00:08:41,458 --> 00:08:45,949
systems where this access protocol that

228
00:08:42,870 --> 00:08:48,600
is used for all kinds of compute units

229
00:08:45,950 --> 00:08:50,610
ok is if the second part that we need to

230
00:08:48,600 --> 00:08:54,120
support this show command is that we

231
00:08:50,610 --> 00:08:55,320
need to add the corresponding path to

232
00:08:54,120 --> 00:08:57,930
the accelerator to support these

233
00:08:55,320 --> 00:08:59,640
protocols and I want to reuse here

234
00:08:57,930 --> 00:09:01,890
off-the-shelf accelerators so I do not

235
00:08:59,640 --> 00:09:03,779
want to change them and in this case I'm

236
00:09:01,890 --> 00:09:05,579
focusing on etc does the prefer

237
00:09:03,779 --> 00:09:07,439
scratchpad memory which is often done

238
00:09:05,579 --> 00:09:09,810
because that gives you a constant access

239
00:09:07,440 --> 00:09:11,120
latency and also allows many parallel

240
00:09:09,810 --> 00:09:13,140
memory

241
00:09:11,120 --> 00:09:15,090
there are also other types of

242
00:09:13,140 --> 00:09:17,610
accelerators for example accelerators

243
00:09:15,090 --> 00:09:20,850
that prefer cash-based anyway men we

244
00:09:17,610 --> 00:09:22,110
access and we have that in the paper in

245
00:09:20,850 --> 00:09:24,570
the talk I will focus on these

246
00:09:22,110 --> 00:09:26,580
accelerators so what do we need to add

247
00:09:24,570 --> 00:09:29,670
the first component that we add as for

248
00:09:26,580 --> 00:09:32,070
every computer net as the dgu the second

249
00:09:29,670 --> 00:09:33,750
component is the accelerator support

250
00:09:32,070 --> 00:09:36,390
module which is setting between the

251
00:09:33,750 --> 00:09:37,890
accelerator logic entity tu and most

252
00:09:36,390 --> 00:09:39,600
importantly it implements the five

253
00:09:37,890 --> 00:09:41,850
protocol for both the input and the

254
00:09:39,600 --> 00:09:44,430
output channel so as you can see each of

255
00:09:41,850 --> 00:09:46,500
the channels has sent endpoint to talk

256
00:09:44,430 --> 00:09:49,290
to the server and a memory endpoint to

257
00:09:46,500 --> 00:09:51,900
actually access the data so what the ASM

258
00:09:49,290 --> 00:09:53,430
is doing is it it first loads the first

259
00:09:51,900 --> 00:09:55,500
block of data over the input channel

260
00:09:53,430 --> 00:09:57,120
into the scratchpad memory is then

261
00:09:55,500 --> 00:09:59,340
invoking the accelerator logic to

262
00:09:57,120 --> 00:10:02,700
compute on that data and if that's

263
00:09:59,340 --> 00:10:04,320
finished the result is pushed from the

264
00:10:02,700 --> 00:10:07,020
scratchpad memory over the output

265
00:10:04,320 --> 00:10:10,800
channel to the output file in this this

266
00:10:07,020 --> 00:10:13,560
example okay so what are the benefits of

267
00:10:10,800 --> 00:10:15,959
this to see that let's let us compare

268
00:10:13,560 --> 00:10:17,849
the autonomous approach that m3x and

269
00:10:15,960 --> 00:10:20,070
Able's with the traditional assisted

270
00:10:17,850 --> 00:10:22,140
approach so you can see the assessment

271
00:10:20,070 --> 00:10:24,240
approach here where we have an operating

272
00:10:22,140 --> 00:10:26,580
system with some driver that is driving

273
00:10:24,240 --> 00:10:29,820
these this image processing accelerator

274
00:10:26,580 --> 00:10:31,740
chain here and DSS the autonomous

275
00:10:29,820 --> 00:10:33,840
approach looks like this we have also an

276
00:10:31,740 --> 00:10:36,390
operating system that in this case only

277
00:10:33,840 --> 00:10:38,760
needs to set up this chain of

278
00:10:36,390 --> 00:10:41,610
accelerators and the actual work is done

279
00:10:38,760 --> 00:10:42,569
by the accelerator support modules so I

280
00:10:41,610 --> 00:10:44,520
want to highlight two crucial

281
00:10:42,570 --> 00:10:47,010
differences here the first difference is

282
00:10:44,520 --> 00:10:48,510
in the data path where on the left side

283
00:10:47,010 --> 00:10:50,819
as you can see the operating system is

284
00:10:48,510 --> 00:10:53,760
doing all the file accesses to the input

285
00:10:50,820 --> 00:10:55,500
at the output file and needs to also

286
00:10:53,760 --> 00:10:58,380
feed the accelerators with the data and

287
00:10:55,500 --> 00:10:59,580
also pull it out again at the end on the

288
00:10:58,380 --> 00:11:01,230
right side as you can see the

289
00:10:59,580 --> 00:11:02,790
accelerators are directly accessing the

290
00:11:01,230 --> 00:11:05,970
input file and also directly accessing

291
00:11:02,790 --> 00:11:07,800
the output file the second difference is

292
00:11:05,970 --> 00:11:09,960
in the control path where on the left

293
00:11:07,800 --> 00:11:12,479
side the operating system needs to drive

294
00:11:09,960 --> 00:11:15,000
the DMA units in order to move the data

295
00:11:12,480 --> 00:11:17,120
from accelerator to accelerator on the

296
00:11:15,000 --> 00:11:20,460
right part and on the right side we have

297
00:11:17,120 --> 00:11:22,050
have this done by the a SMS by having an

298
00:11:20,460 --> 00:11:23,340
optimized version of the five protocol

299
00:11:22,050 --> 00:11:23,990
where the accelerators are directly

300
00:11:23,340 --> 00:11:25,280
connected

301
00:11:23,990 --> 00:11:29,750
so in this case also the operating

302
00:11:25,280 --> 00:11:31,850
system is not part of the execution okay

303
00:11:29,750 --> 00:11:34,430
so let us see what that means the

304
00:11:31,850 --> 00:11:36,830
numbers to compare that we have this

305
00:11:34,430 --> 00:11:38,719
image processing this physical image

306
00:11:36,830 --> 00:11:40,660
processing accelerators here and we have

307
00:11:38,720 --> 00:11:42,800
a chain of activities how we call that

308
00:11:40,660 --> 00:11:45,290
running on these accelerators and an

309
00:11:42,800 --> 00:11:47,150
input file an output file and maybe we

310
00:11:45,290 --> 00:11:49,219
want to do multiple image processing Xin

311
00:11:47,150 --> 00:11:51,079
peril and therefore we have multiple of

312
00:11:49,220 --> 00:11:55,090
these physical accelerator chains each

313
00:11:51,080 --> 00:11:58,190
having a virtual chain on top of them so

314
00:11:55,090 --> 00:11:59,600
since these physical change run in

315
00:11:58,190 --> 00:12:01,820
parallel you would expect that if there

316
00:11:59,600 --> 00:12:04,220
is no operating system overhead that the

317
00:12:01,820 --> 00:12:06,050
runtime is always the same and if we

318
00:12:04,220 --> 00:12:07,490
look at the results we can see for the

319
00:12:06,050 --> 00:12:09,349
autonomous approach this is actually the

320
00:12:07,490 --> 00:12:10,910
case but for the assisted approach

321
00:12:09,350 --> 00:12:12,280
there's a bit of a slowdown because of

322
00:12:10,910 --> 00:12:15,740
these frequent interactions between

323
00:12:12,280 --> 00:12:17,150
accelerators and the CPU and even more

324
00:12:15,740 --> 00:12:19,310
importantly you can see that the CPU

325
00:12:17,150 --> 00:12:20,840
load caused by the operating system is

326
00:12:19,310 --> 00:12:23,599
pretty high for the assisted approach

327
00:12:20,840 --> 00:12:25,640
and very low for the autonomous approach

328
00:12:23,600 --> 00:12:27,290
and this is actually the good case

329
00:12:25,640 --> 00:12:29,780
because in this scenario we have

330
00:12:27,290 --> 00:12:31,699
integrated the CPU cores and the

331
00:12:29,780 --> 00:12:33,770
accelerators in a system on a chip so

332
00:12:31,700 --> 00:12:35,690
there's a low latency between them if we

333
00:12:33,770 --> 00:12:39,020
connect the accelerators with the PCIe

334
00:12:35,690 --> 00:12:41,060
like latency to the CPU then the results

335
00:12:39,020 --> 00:12:42,680
look even more severe so as you can see

336
00:12:41,060 --> 00:12:44,839
on the right we have almost 90 percent

337
00:12:42,680 --> 00:12:48,890
of CPU load even with one of these

338
00:12:44,840 --> 00:12:50,690
chains and this also leads to more

339
00:12:48,890 --> 00:12:54,250
slowdown on the left because the CPU is

340
00:12:50,690 --> 00:12:54,250
just overloaded beginning at two chains

341
00:12:54,280 --> 00:12:58,939
okay now that we have autonomous

342
00:12:56,840 --> 00:13:01,790
accelerators let me explain how we can

343
00:12:58,940 --> 00:13:05,230
combine that with context switching to

344
00:13:01,790 --> 00:13:05,230
use the hypo resources more efficiently

345
00:13:05,500 --> 00:13:10,670
so I don't want to go into the details

346
00:13:07,640 --> 00:13:12,890
here but the main takeaway is that we

347
00:13:10,670 --> 00:13:14,660
also have a generic protocol for context

348
00:13:12,890 --> 00:13:16,910
switching between the kernel that is

349
00:13:14,660 --> 00:13:18,680
doing the decision-making and the help

350
00:13:16,910 --> 00:13:20,719
us that on the user types where the

351
00:13:18,680 --> 00:13:22,699
context which is happening that just

352
00:13:20,720 --> 00:13:24,710
dude safe and restore and this allows us

353
00:13:22,700 --> 00:13:26,630
to support context which is both on

354
00:13:24,710 --> 00:13:29,570
general purpose course and also on

355
00:13:26,630 --> 00:13:31,189
accelerators but I want to come back to

356
00:13:29,570 --> 00:13:33,170
the questions or the the problems that

357
00:13:31,190 --> 00:13:34,970
are raised at the beginning that arise

358
00:13:33,170 --> 00:13:36,569
when you combine fast path communication

359
00:13:34,970 --> 00:13:39,900
with context switching

360
00:13:36,570 --> 00:13:41,460
so if they send on the left here wants

361
00:13:39,900 --> 00:13:42,990
to send a message to the receiver on the

362
00:13:41,460 --> 00:13:45,180
right the question is how does he know

363
00:13:42,990 --> 00:13:47,040
whether is he is running so how can we

364
00:13:45,180 --> 00:13:48,719
solve that problem and we decided to

365
00:13:47,040 --> 00:13:52,170
solve that by letting the teacher you

366
00:13:48,720 --> 00:13:53,600
know about the running activity and also

367
00:13:52,170 --> 00:13:56,550
letting the DG you know about the

368
00:13:53,600 --> 00:13:59,070
recipient of a communication so if the

369
00:13:56,550 --> 00:14:00,719
sender on the left here sends a message

370
00:13:59,070 --> 00:14:03,450
to the activity - on the right

371
00:14:00,720 --> 00:14:06,480
everything is fine because the ID - is

372
00:14:03,450 --> 00:14:08,250
matching but if there's another

373
00:14:06,480 --> 00:14:10,290
communication channel that allows the

374
00:14:08,250 --> 00:14:13,110
Activity one to send to activity three

375
00:14:10,290 --> 00:14:14,790
also - the tile on the right as the

376
00:14:13,110 --> 00:14:17,520
receiver because maybe the activity 3

377
00:14:14,790 --> 00:14:19,380
was running there before then the

378
00:14:17,520 --> 00:14:21,530
atleast don't match Aditi you before

379
00:14:19,380 --> 00:14:23,730
it's an error back to the to the sender

380
00:14:21,530 --> 00:14:26,670
so the next question is what does the

381
00:14:23,730 --> 00:14:28,890
sender do with this error and we decided

382
00:14:26,670 --> 00:14:31,560
to you basically fall back to slow path

383
00:14:28,890 --> 00:14:33,030
communication in such a case and follow

384
00:14:31,560 --> 00:14:35,250
up the message over the kernel to the

385
00:14:33,030 --> 00:14:37,770
recipient and the kernel will then start

386
00:14:35,250 --> 00:14:40,560
by scheduling the activity 3 in this

387
00:14:37,770 --> 00:14:42,900
example if it's not already running and

388
00:14:40,560 --> 00:14:46,760
afterwards send the message on behalf of

389
00:14:42,900 --> 00:14:49,740
the original sender to the activity 3

390
00:14:46,760 --> 00:14:51,660
okay so now that we can context switch

391
00:14:49,740 --> 00:14:53,760
or share accelerators between multiple

392
00:14:51,660 --> 00:14:55,650
applications let us come back to this

393
00:14:53,760 --> 00:14:58,439
image processing example so we again

394
00:14:55,650 --> 00:14:59,880
have this one to four chains and in this

395
00:14:58,440 --> 00:15:01,710
case we want to share them as a set and

396
00:14:59,880 --> 00:15:03,780
therefore we have two of these virtual

397
00:15:01,710 --> 00:15:06,210
chains running on each physical chain

398
00:15:03,780 --> 00:15:07,650
and we switch between them so the

399
00:15:06,210 --> 00:15:10,920
question is what kind of overhead does

400
00:15:07,650 --> 00:15:12,540
this introduce of course that depends at

401
00:15:10,920 --> 00:15:14,430
first on the number of accelerator

402
00:15:12,540 --> 00:15:16,680
chains or the number of accelerators we

403
00:15:14,430 --> 00:15:19,199
have the context switch and also on the

404
00:15:16,680 --> 00:15:22,859
time slice so how often do we context

405
00:15:19,200 --> 00:15:24,630
switch and if as you can see if we do

406
00:15:22,860 --> 00:15:27,690
that once per millisecond then we have a

407
00:15:24,630 --> 00:15:30,030
1 to 2 percent overhead but if we

408
00:15:27,690 --> 00:15:33,360
increase the time slices or reduce the

409
00:15:30,030 --> 00:15:35,490
frequency then it is below 1 percent

410
00:15:33,360 --> 00:15:37,620
even with 4 of these actual accelerator

411
00:15:35,490 --> 00:15:42,210
chains so 12 accelerators - context

412
00:15:37,620 --> 00:15:44,100
switch okay in conclusion m3x enables

413
00:15:42,210 --> 00:15:45,660
autonomous accelerators and combines

414
00:15:44,100 --> 00:15:47,520
fast path communication with context

415
00:15:45,660 --> 00:15:49,650
switching by at first

416
00:15:47,520 --> 00:15:50,370
adding a uniform interface to all the

417
00:15:49,650 --> 00:15:52,860
compute units

418
00:15:50,370 --> 00:15:54,509
second using simple and generic

419
00:15:52,860 --> 00:15:57,089
protocols for example for operating

420
00:15:54,509 --> 00:15:59,009
system service access and by adding a

421
00:15:57,089 --> 00:16:00,930
lightweight component to accelerators if

422
00:15:59,009 --> 00:16:03,990
that's necessary instead of requiring

423
00:16:00,930 --> 00:16:05,939
them to provide the features and this as

424
00:16:03,990 --> 00:16:09,959
I have shown reduces the CPU load by a

425
00:16:05,939 --> 00:16:12,120
factor of 30 and also at the on the one

426
00:16:09,959 --> 00:16:14,059
hand we retained the advantages of fast

427
00:16:12,120 --> 00:16:17,550
path communication as we shown the paper

428
00:16:14,059 --> 00:16:18,899
but also we can at the same time use the

429
00:16:17,550 --> 00:16:21,959
hyper resources more efficiently by

430
00:16:18,899 --> 00:16:23,399
supporting context switching if you want

431
00:16:21,959 --> 00:16:25,739
to know more button 3 there's another

432
00:16:23,399 --> 00:16:28,259
talk called sampras in the next session

433
00:16:25,740 --> 00:16:29,939
where we show how we can scale an m3

434
00:16:28,259 --> 00:16:32,399
based system up to hundreds of compute

435
00:16:29,939 --> 00:16:45,029
units and with this I'm happy to take

436
00:16:32,399 --> 00:16:48,329
your questions questions so I have I

437
00:16:45,029 --> 00:16:49,860
have a question and maybe I I spent too

438
00:16:48,329 --> 00:16:51,748
much time in the networking world but in

439
00:16:49,860 --> 00:16:53,699
the networking world we talk about

440
00:16:51,749 --> 00:16:56,579
concepts like flow control and data

441
00:16:53,699 --> 00:16:59,370
placement and DMA and congestion

442
00:16:56,579 --> 00:17:01,649
management and I'd be interested if you

443
00:16:59,370 --> 00:17:02,999
could just maybe replay what you had

444
00:17:01,649 --> 00:17:04,740
talked what you had mentioned whether

445
00:17:02,999 --> 00:17:06,419
you with respect to a design in a

446
00:17:04,740 --> 00:17:08,730
networking terminology because the terms

447
00:17:06,419 --> 00:17:11,010
the terminology used is very different

448
00:17:08,730 --> 00:17:12,899
but at the end of the day it feels very

449
00:17:11,010 --> 00:17:15,449
similar to what you would have normally

450
00:17:12,898 --> 00:17:19,589
in if you were to build a a flow control

451
00:17:15,449 --> 00:17:24,419
network hmm so I think I'm not enough

452
00:17:19,589 --> 00:17:26,250
networking expert to use the terms so do

453
00:17:24,419 --> 00:17:28,470
you have a specific example yes so for

454
00:17:26,250 --> 00:17:30,659
example your it's not clear whether the

455
00:17:28,470 --> 00:17:32,039
the match the queues or flow controlled

456
00:17:30,659 --> 00:17:33,620
or not what is the flow control

457
00:17:32,039 --> 00:17:37,529
mechanism okay

458
00:17:33,620 --> 00:17:39,090
so at the at the receiver side we have a

459
00:17:37,529 --> 00:17:41,700
buffer you receive buffer where the

460
00:17:39,090 --> 00:17:43,649
messages are put into and we use a

461
00:17:41,700 --> 00:17:45,450
credit system to to manage that so the

462
00:17:43,649 --> 00:17:46,678
clients send us have credits and if they

463
00:17:45,450 --> 00:17:48,990
have credit they know that there's space

464
00:17:46,679 --> 00:17:51,230
for this message thank you that's a

465
00:17:48,990 --> 00:17:56,190
great answer

466
00:17:51,230 --> 00:17:57,500
yes thanks great work souvenir from Duke

467
00:17:56,190 --> 00:18:01,880
University

468
00:17:57,500 --> 00:18:03,440
what if there is reapers backward

469
00:18:01,880 --> 00:18:07,340
dependency

470
00:18:03,440 --> 00:18:11,210
a backwards dependency meaning if the

471
00:18:07,340 --> 00:18:15,770
data in the next stage is required from

472
00:18:11,210 --> 00:18:17,480
the former stages so the example that I

473
00:18:15,770 --> 00:18:19,430
used is stream processing so this

474
00:18:17,480 --> 00:18:22,550
doesn't happen you have just you see

475
00:18:19,430 --> 00:18:24,170
every data just once we have another

476
00:18:22,550 --> 00:18:25,820
type of accelerator in the paper which

477
00:18:24,170 --> 00:18:27,530
we call a request processing accelerator

478
00:18:25,820 --> 00:18:29,810
so where you get access to the whole

479
00:18:27,530 --> 00:18:32,540
data at once and then you can do fine

480
00:18:29,810 --> 00:18:34,639
granola excesses to this to this data

481
00:18:32,540 --> 00:18:38,899
and then you could just go back and

482
00:18:34,640 --> 00:18:43,640
forth however you like okay so you are

483
00:18:38,900 --> 00:18:45,380
sending your data in the backward so in

484
00:18:43,640 --> 00:18:46,640
this case it's a cash base and exit

485
00:18:45,380 --> 00:18:48,050
written strategy with this request

486
00:18:46,640 --> 00:18:50,690
processing accelerator so you do not

487
00:18:48,050 --> 00:18:52,550
send data around you just have a memory

488
00:18:50,690 --> 00:18:54,230
mapping and the cache that allows you to

489
00:18:52,550 --> 00:18:55,550
access this mapping and then you can

490
00:18:54,230 --> 00:19:06,910
just do the excesses however you like

491
00:18:55,550 --> 00:19:09,260
okay great thanks great paper do you see

492
00:19:06,910 --> 00:19:11,510
the state of your research at a stage

493
00:19:09,260 --> 00:19:13,550
where you could offer these designs and

494
00:19:11,510 --> 00:19:15,800
specs to logic vendors so they can put

495
00:19:13,550 --> 00:19:18,440
it in front of the various accelerators

496
00:19:15,800 --> 00:19:23,389
within SOC or do you still need to do a

497
00:19:18,440 --> 00:19:26,420
lot of more work so so actually we are

498
00:19:23,390 --> 00:19:28,160
about to do that so in the back house

499
00:19:26,420 --> 00:19:31,130
Institute we continue the research on

500
00:19:28,160 --> 00:19:33,050
this project and we we have hardware

501
00:19:31,130 --> 00:19:35,000
engineers in the team that will build

502
00:19:33,050 --> 00:19:37,430
the dgu in hardware and therefore I

503
00:19:35,000 --> 00:19:38,960
think at least we think at the moment

504
00:19:37,430 --> 00:19:44,780
that we are ready for that let's see if

505
00:19:38,960 --> 00:19:47,110
we are well if there no other question

506
00:19:44,780 --> 00:19:52,230
let's thank the speaker once again

507
00:19:47,110 --> 00:19:52,229
[Applause]

