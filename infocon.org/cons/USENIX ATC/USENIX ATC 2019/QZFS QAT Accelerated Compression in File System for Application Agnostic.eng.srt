1
00:00:10,130 --> 00:00:16,970
my name is Joe Kahn who from Shanghai

2
00:00:13,590 --> 00:00:20,419
Jiaotong University the work on

3
00:00:16,970 --> 00:00:23,000
Sherri's q DFS create a accurate

4
00:00:20,419 --> 00:00:25,660
compression file system replication

5
00:00:23,000 --> 00:00:28,939
agnostic and cost-efficient in a storage

6
00:00:25,660 --> 00:00:34,730
to astound by the cooperation reaching

7
00:00:28,939 --> 00:00:37,309
our University and Intel its firstly the

8
00:00:34,730 --> 00:00:40,339
background high-performance computing

9
00:00:37,309 --> 00:00:42,589
provides powerful computing capabilities

10
00:00:40,339 --> 00:00:44,030
for handling big data it involves

11
00:00:42,589 --> 00:00:47,690
massive storage

12
00:00:44,030 --> 00:00:49,489
I will reread operations so an important

13
00:00:47,690 --> 00:00:53,959
requirement is the performance and

14
00:00:49,489 --> 00:00:56,689
efficiency of the storage subsystem this

15
00:00:53,960 --> 00:01:01,939
requirement can be satisfied by nvme

16
00:00:56,689 --> 00:01:04,339
SSDs which provide remarkable inquiries

17
00:01:01,939 --> 00:01:07,640
or rewrite speed with no energy

18
00:01:04,339 --> 00:01:11,990
consumption but it comes with high price

19
00:01:07,640 --> 00:01:15,770
at the same time for example intel SSD

20
00:01:11,990 --> 00:01:20,270
to the center family costs nearly $500

21
00:01:15,770 --> 00:01:22,880
per terabyte and for mixture the HPC

22
00:01:20,270 --> 00:01:26,119
system for climate research in Germany

23
00:01:22,880 --> 00:01:28,729
the storage subsystem accounted for

24
00:01:26,120 --> 00:01:32,990
roughly 20 percent of the total cost of

25
00:01:28,730 --> 00:01:36,080
ownership telecom pressure is an

26
00:01:32,990 --> 00:01:38,860
effective solution it is not only

27
00:01:36,080 --> 00:01:42,110
achieves based efficiency for lower cost

28
00:01:38,860 --> 00:01:45,710
but may also provide higher performance

29
00:01:42,110 --> 00:01:48,110
because of reduced IO operations but

30
00:01:45,710 --> 00:01:51,860
again the benefit comes at the expense

31
00:01:48,110 --> 00:01:54,500
of CPU resources leading to the advent

32
00:01:51,860 --> 00:02:00,049
of all kinds of compression accelerators

33
00:01:54,500 --> 00:02:01,909
for floating 2 free apps if you tell a

34
00:02:00,049 --> 00:02:05,450
compression could be integrated into

35
00:02:01,909 --> 00:02:07,940
different system layers first the

36
00:02:05,450 --> 00:02:10,479
application layer which is most common

37
00:02:07,940 --> 00:02:13,489
such as nginx a tube

38
00:02:10,479 --> 00:02:16,310
second the filesystem layer which can

39
00:02:13,489 --> 00:02:20,120
benefit applications such as ZFS

40
00:02:16,310 --> 00:02:23,930
btrfs and last block layer which

41
00:02:20,120 --> 00:02:27,950
achieves file system agnostic example in

42
00:02:23,930 --> 00:02:30,620
the recently released real hot video our

43
00:02:27,950 --> 00:02:33,409
walk investigates ASIC based compress

44
00:02:30,620 --> 00:02:36,230
floating at the fair system layer using

45
00:02:33,409 --> 00:02:40,298
Intel quick assist technology it's a

46
00:02:36,230 --> 00:02:43,670
modern ASIC for cryptography and

47
00:02:40,299 --> 00:02:47,239
compression is tap included PCI adapter

48
00:02:43,670 --> 00:02:49,790
chipset system or chip the performance

49
00:02:47,239 --> 00:02:53,329
of the latest created device is up to

50
00:02:49,790 --> 00:02:58,720
100 keepers per second and the price is

51
00:02:53,329 --> 00:03:01,730
low to $32 after putting into chipset

52
00:02:58,720 --> 00:03:02,480
since our work is based on ZFS

53
00:03:01,730 --> 00:03:05,768
filesystem

54
00:03:02,480 --> 00:03:08,629
let's have an overview first ZFS

55
00:03:05,769 --> 00:03:13,099
combines the rows of posts filesystem

56
00:03:08,629 --> 00:03:16,069
and volume manager and the poor storage

57
00:03:13,099 --> 00:03:19,608
in the affairs actually animalized

58
00:03:16,069 --> 00:03:22,790
and chika notional volumes ZFS provides

59
00:03:19,609 --> 00:03:25,340
transactional operation for consistence

60
00:03:22,790 --> 00:03:30,379
and provide end-to-end data integrity

61
00:03:25,340 --> 00:03:34,250
rate encryption compression zone it has

62
00:03:30,379 --> 00:03:37,160
a concept of regular size which defines

63
00:03:34,250 --> 00:03:40,840
the maximum size of a block that can be

64
00:03:37,160 --> 00:03:44,780
processed by ZFS and the default size is

65
00:03:40,840 --> 00:03:49,099
128 kilobytes and it can be configured

66
00:03:44,780 --> 00:03:51,440
by the user so this kind of reckless

67
00:03:49,099 --> 00:03:57,888
size means the possibility of varied

68
00:03:51,440 --> 00:04:00,379
block sites from for compression our

69
00:03:57,889 --> 00:04:03,079
walk process a purpose is created

70
00:04:00,379 --> 00:04:07,310
accelerate the ZFS name the queue GFS

71
00:04:03,079 --> 00:04:10,220
which increase Intel creatine to ZFS for

72
00:04:07,310 --> 00:04:12,799
efficient data compression of loading we

73
00:04:10,220 --> 00:04:17,000
choose to offload gzip algorithm because

74
00:04:12,799 --> 00:04:18,709
of its high compression qzf as design

75
00:04:17,000 --> 00:04:21,560
church has burning benefits or

76
00:04:18,709 --> 00:04:24,560
applications running on it featured by

77
00:04:21,560 --> 00:04:28,160
high performance high space efficiency

78
00:04:24,560 --> 00:04:32,030
and low CPU utilization there are two

79
00:04:28,160 --> 00:04:34,760
main design considerations first the

80
00:04:32,030 --> 00:04:37,309
compression related function needs to be

81
00:04:34,760 --> 00:04:42,229
replaced within our core to interact

82
00:04:37,310 --> 00:04:45,040
with qat parkade hardware cheese data in

83
00:04:42,229 --> 00:04:48,200
a different way from the software

84
00:04:45,040 --> 00:04:51,139
second considering the offered overhead

85
00:04:48,200 --> 00:04:53,840
and the pre-allocated system resources

86
00:04:51,139 --> 00:04:57,650
for creative loading we need to

87
00:04:53,840 --> 00:05:00,109
investigate the necessity of hardware

88
00:04:57,650 --> 00:05:05,650
software switch or even compression and

89
00:05:00,110 --> 00:05:09,740
compression switch this is the Q ZFS

90
00:05:05,650 --> 00:05:13,120
architecture it can work as either local

91
00:05:09,740 --> 00:05:19,010
file system or the background of the

92
00:05:13,120 --> 00:05:22,160
last 33 buted file system the zio module

93
00:05:19,010 --> 00:05:25,150
is the centralized model where our

94
00:05:22,160 --> 00:05:27,700
requests are abstract as VI OS and

95
00:05:25,150 --> 00:05:32,090
forwarded to other modules for

96
00:05:27,700 --> 00:05:35,539
processing the our compress module this

97
00:05:32,090 --> 00:05:39,200
is responsible for data compression and

98
00:05:35,540 --> 00:05:43,090
decompression gvfs introduces two new

99
00:05:39,200 --> 00:05:47,870
modules compression service engine and

100
00:05:43,090 --> 00:05:50,599
the create offloading module a complete

101
00:05:47,870 --> 00:05:55,370
tailor writing workflow starts with the

102
00:05:50,600 --> 00:05:58,040
cio module which forward forward

103
00:05:55,370 --> 00:06:00,770
zio compressed request to zero

104
00:05:58,040 --> 00:06:04,600
compression module enters the service

105
00:06:00,770 --> 00:06:07,820
engine and true selects algorithm from

106
00:06:04,600 --> 00:06:11,510
software library or hardware

107
00:06:07,820 --> 00:06:14,870
acceleration for creative loading it

108
00:06:11,510 --> 00:06:18,860
says compression request to the create

109
00:06:14,870 --> 00:06:21,260
accelerator and went is complete the

110
00:06:18,860 --> 00:06:22,760
response is consumed by the acuity of

111
00:06:21,260 --> 00:06:25,610
fluid offloading model and the

112
00:06:22,760 --> 00:06:28,700
compressed data are returned to the app

113
00:06:25,610 --> 00:06:31,090
modules and finally the the i/o module

114
00:06:28,700 --> 00:06:35,479
writes the compressed data into

115
00:06:31,090 --> 00:06:43,190
nvme SSDs $40.00 data reading the

116
00:06:35,479 --> 00:06:45,979
workflow for decompression is similar in

117
00:06:43,190 --> 00:06:48,830
compression service engine there is an

118
00:06:45,979 --> 00:06:52,370
algorithm selector which selects

119
00:06:48,830 --> 00:06:55,130
compression algorithm from one created

120
00:06:52,370 --> 00:06:58,249
Excel accelerate the chip and some other

121
00:06:55,130 --> 00:07:01,129
software implementations

122
00:06:58,249 --> 00:07:03,709
has a uniform interface which means it

123
00:07:01,129 --> 00:07:07,629
can be easily extended to support other

124
00:07:03,709 --> 00:07:10,699
compression accelerators and once a

125
00:07:07,629 --> 00:07:13,549
runtime error is detected in the key of

126
00:07:10,699 --> 00:07:17,199
loading the systems we switch to the

127
00:07:13,549 --> 00:07:20,469
software alternative automatically

128
00:07:17,199 --> 00:07:23,419
Halliwell software switch is considered

129
00:07:20,469 --> 00:07:27,229
qg FL selectively offloads the arrow

130
00:07:23,419 --> 00:07:29,779
requests between four with the size

131
00:07:27,229 --> 00:07:32,688
ranging from four kilobytes to one

132
00:07:29,779 --> 00:07:33,558
megabyte this is because for small

133
00:07:32,689 --> 00:07:36,229
source data

134
00:07:33,559 --> 00:07:38,689
the benefits are offset by the a float

135
00:07:36,229 --> 00:07:41,659
overhead including preparing the

136
00:07:38,689 --> 00:07:43,339
creative with request consuming the

137
00:07:41,659 --> 00:07:46,549
creative responses and the PCIe

138
00:07:43,339 --> 00:07:49,369
transactions and the support for peak

139
00:07:46,549 --> 00:07:51,378
sauce theta requires large pre-allocated

140
00:07:49,369 --> 00:07:56,359
in the kernel memory has intermediate

141
00:07:51,379 --> 00:08:00,289
buffers q DFS also provides

142
00:07:56,359 --> 00:08:03,409
compressibility dependent offloading we

143
00:08:00,289 --> 00:08:05,748
know that no compressibility means that

144
00:08:03,409 --> 00:08:08,329
theta not was being stored in a

145
00:08:05,749 --> 00:08:11,569
compressed format as it will not see

146
00:08:08,329 --> 00:08:14,419
much storage storage space but incur

147
00:08:11,569 --> 00:08:17,329
extra computing resource consumption for

148
00:08:14,419 --> 00:08:19,998
later decompression so if the space

149
00:08:17,329 --> 00:08:22,610
saving is less than the champion

150
00:08:19,999 --> 00:08:30,019
threshold the original uncompressed data

151
00:08:22,610 --> 00:08:32,269
is returned to the App applications the

152
00:08:30,019 --> 00:08:35,360
acuity of loading module we need to

153
00:08:32,269 --> 00:08:38,299
consider at the data prepared by zio

154
00:08:35,360 --> 00:08:41,599
uses virtual memory valid Hardware

155
00:08:38,299 --> 00:08:44,870
requires contiguous physical memory for

156
00:08:41,599 --> 00:08:47,509
DMA operations so tell our

157
00:08:44,870 --> 00:08:53,420
reconstruction is needed and our goal is

158
00:08:47,509 --> 00:08:56,899
zero memory copy GFS uses vector L

159
00:08:53,420 --> 00:09:03,319
module also long as scatter gazar buffer

160
00:08:56,899 --> 00:09:07,610
list and q GFS partitions data by page

161
00:09:03,319 --> 00:09:10,639
frames into multiple flatbuffers in each

162
00:09:07,610 --> 00:09:11,840
file buffer the data is physically

163
00:09:10,639 --> 00:09:15,020
contiguous

164
00:09:11,840 --> 00:09:17,960
when we creating data structure for the

165
00:09:15,020 --> 00:09:20,829
sto we need to calculate the number of

166
00:09:17,960 --> 00:09:24,080
buffers and H it is calculated by the

167
00:09:20,830 --> 00:09:26,570
postponed maximum and the resulting zero

168
00:09:24,080 --> 00:09:30,980
buffers are automatically handled by the

169
00:09:26,570 --> 00:09:35,240
key driver for address translation to

170
00:09:30,980 --> 00:09:38,240
locate physical page we need to defer to

171
00:09:35,240 --> 00:09:41,210
for eight different color memory regions

172
00:09:38,240 --> 00:09:43,400
including Vemma local region or direct

173
00:09:41,210 --> 00:09:46,130
memory region and we need to use

174
00:09:43,400 --> 00:09:50,449
different address translation functions

175
00:09:46,130 --> 00:09:53,960
for them and where a physical page is

176
00:09:50,450 --> 00:09:59,779
not located we use connection to

177
00:09:53,960 --> 00:10:02,480
establish long lasting mapping in

178
00:09:59,779 --> 00:10:06,050
evaluation we use the last cluster with

179
00:10:02,480 --> 00:10:10,250
varying nodes in a followed cluster we

180
00:10:06,050 --> 00:10:15,099
use to client and to object storage

181
00:10:10,250 --> 00:10:19,610
servers in each server SSD array and 1kt

182
00:10:15,100 --> 00:10:22,180
device is who was were encrypted the

183
00:10:19,610 --> 00:10:26,120
created device has a hardware limit of

184
00:10:22,180 --> 00:10:26,810
24 Giga bits per second true benchmarks

185
00:10:26,120 --> 00:10:29,390
were used

186
00:10:26,810 --> 00:10:33,520
FIO macro benchmark and genomic data

187
00:10:29,390 --> 00:10:36,500
post-processing we use a cost efficiency

188
00:10:33,520 --> 00:10:38,810
metric which is calculated by the ratio

189
00:10:36,500 --> 00:10:42,140
between compression ratio and the CPU

190
00:10:38,810 --> 00:10:45,260
utilization here we do not take creative

191
00:10:42,140 --> 00:10:47,569
cost into account as its price after

192
00:10:45,260 --> 00:10:54,620
putting after putting the chipset is

193
00:10:47,570 --> 00:10:59,600
negligible compared to the CPU price for

194
00:10:54,620 --> 00:11:04,810
a file macro benchmark there were 16 FRS

195
00:10:59,600 --> 00:11:09,020
in each client we first compared

196
00:11:04,810 --> 00:11:12,859
software gzip with creative accelerator

197
00:11:09,020 --> 00:11:17,680
which is if we can see that for the

198
00:11:12,860 --> 00:11:21,530
software gzip the sequential right and

199
00:11:17,680 --> 00:11:25,640
random right the simple innovation is up

200
00:11:21,530 --> 00:11:28,790
to 95% this actually have

201
00:11:25,640 --> 00:11:31,370
great impact have an impact of the

202
00:11:28,790 --> 00:11:36,560
normal run you know if I open schmuck

203
00:11:31,370 --> 00:11:39,740
and cause the very low throughput this

204
00:11:36,560 --> 00:11:43,699
is only one third of the case of

205
00:11:39,740 --> 00:11:47,240
compression off after the gzip is

206
00:11:43,700 --> 00:11:49,760
uploaded to the creative accelerator we

207
00:11:47,240 --> 00:11:53,779
can see the cost of efficiency is

208
00:11:49,760 --> 00:11:57,079
greatly increased and the realized

209
00:11:53,779 --> 00:11:59,480
report for the qat akka to accelerate GD

210
00:11:57,079 --> 00:12:03,670
p-- is even higher than the compression

211
00:11:59,480 --> 00:12:06,019
off this is because the storage i/o

212
00:12:03,670 --> 00:12:10,490
operations are reduced because of

213
00:12:06,019 --> 00:12:14,300
compression we also compared creating

214
00:12:10,490 --> 00:12:18,560
axilla gzip with other fast compression

215
00:12:14,300 --> 00:12:22,250
algorithm such as LD 4 and KT

216
00:12:18,560 --> 00:12:25,310
ichthyology deep stores a still achieves

217
00:12:22,250 --> 00:12:31,600
the highest cost efficiency especially

218
00:12:25,310 --> 00:12:34,189
for the right operation for genomic data

219
00:12:31,600 --> 00:12:36,740
post-processing same choice were used

220
00:12:34,190 --> 00:12:41,720
perform file prevent representative

221
00:12:36,740 --> 00:12:45,890
operations and in one client eight

222
00:12:41,720 --> 00:12:49,040
processes or lungs to manipulate 76

223
00:12:45,890 --> 00:12:51,260
hiccup AIDS genomic data we can see that

224
00:12:49,040 --> 00:12:56,360
the creator of loading reduces the

225
00:12:51,260 --> 00:12:59,000
execution time by 63% in a provides six

226
00:12:56,360 --> 00:13:05,060
time cost efficiency because of the low

227
00:12:59,000 --> 00:13:07,850
CPU utilization we also compared the PDF

228
00:13:05,060 --> 00:13:13,279
s with a simple gzip at application

229
00:13:07,850 --> 00:13:16,970
layer to perform genomic data converting

230
00:13:13,279 --> 00:13:19,610
operation here the simple gzip means the

231
00:13:16,970 --> 00:13:23,120
application has no compression module

232
00:13:19,610 --> 00:13:27,649
but use the GBH or two for compression

233
00:13:23,120 --> 00:13:31,160
so when when the application perform

234
00:13:27,649 --> 00:13:35,000
converting operation it first should use

235
00:13:31,160 --> 00:13:37,520
the tip should go through the team

236
00:13:35,000 --> 00:13:39,120
corrosion process including read

237
00:13:37,520 --> 00:13:41,220
compress data the

238
00:13:39,120 --> 00:13:44,910
compression and right uncompressed data

239
00:13:41,220 --> 00:13:46,740
back to storage and went performed

240
00:13:44,910 --> 00:13:49,620
converting process it'll read

241
00:13:46,740 --> 00:13:54,510
uncompressed data converting and ran new

242
00:13:49,620 --> 00:13:58,230
format back here is the queue GFS it

243
00:13:54,510 --> 00:14:01,790
achieves the total exclusion of Q DFS is

244
00:13:58,230 --> 00:14:04,950
even smaller than the pure converting

245
00:14:01,790 --> 00:14:08,279
process of the simple GD p-- because Q

246
00:14:04,950 --> 00:14:10,470
VFS directive will write compress the

247
00:14:08,279 --> 00:14:13,470
data and perform decompression

248
00:14:10,470 --> 00:14:17,820
converting simultaneously in multiples

249
00:14:13,470 --> 00:14:20,460
rest actually it's no doubt that

250
00:14:17,820 --> 00:14:23,370
application catchy similar performance

251
00:14:20,460 --> 00:14:26,400
by integrating we're design compression

252
00:14:23,370 --> 00:14:28,470
module including data fragmentation

253
00:14:26,400 --> 00:14:31,110
compression multi thread and using

254
00:14:28,470 --> 00:14:34,470
create T but each new plication

255
00:14:31,110 --> 00:14:37,440
meaningful heavy modification including

256
00:14:34,470 --> 00:14:40,230
trust our work involves one modification

257
00:14:37,440 --> 00:14:44,400
to the DA's file system and it can

258
00:14:40,230 --> 00:14:48,680
benefit applications running on it at

259
00:14:44,400 --> 00:14:52,560
last I want to discuss the bottleneck

260
00:14:48,680 --> 00:14:55,109
for the FIO benchmark even the highest

261
00:14:52,560 --> 00:14:59,400
throughput tunnel achieved a Halloween

262
00:14:55,110 --> 00:15:01,680
image we can see for the CPU SSD neke or

263
00:14:59,400 --> 00:15:03,540
creative that you do not achieve the

264
00:15:01,680 --> 00:15:07,859
Halloween image so what is the

265
00:15:03,540 --> 00:15:10,529
bottleneck first the lumber of ZFS

266
00:15:07,860 --> 00:15:12,540
workers with offloading abilities

267
00:15:10,529 --> 00:15:16,080
limited by the number of creating

268
00:15:12,540 --> 00:15:18,319
instances and more importantly a walk

269
00:15:16,080 --> 00:15:21,779
thread in interact with creating

270
00:15:18,320 --> 00:15:24,120
synchronized mode it means the next

271
00:15:21,779 --> 00:15:27,360
compression request cannot be submitted

272
00:15:24,120 --> 00:15:32,970
until the completion of the previous one

273
00:15:27,360 --> 00:15:35,610
so in this kind of mode you can own even

274
00:15:32,970 --> 00:15:39,180
if you use more effort raised it cannot

275
00:15:35,610 --> 00:15:44,910
give rise to the parallel concurrent

276
00:15:39,180 --> 00:15:49,920
creative requests an approach to ORS

277
00:15:44,910 --> 00:15:53,010
come this bottleneck is to use acing of

278
00:15:49,920 --> 00:15:55,860
load mode it typically

279
00:15:53,010 --> 00:15:57,990
when you want to utilize security

280
00:15:55,860 --> 00:16:02,100
accelerator with limited number of

281
00:15:57,990 --> 00:16:04,980
arrays and it has the ability to make a

282
00:16:02,100 --> 00:16:08,850
once read it can concurrently offload

283
00:16:04,980 --> 00:16:12,779
the multiple operation tasks however the

284
00:16:08,850 --> 00:16:15,810
a single implementation is not easy it

285
00:16:12,779 --> 00:16:18,389
involves a single support in all layers

286
00:16:15,810 --> 00:16:22,349
of the affair software stack to handle

287
00:16:18,389 --> 00:16:25,880
and handle and recognize and complete

288
00:16:22,350 --> 00:16:29,790
tasks H needs efficient oppose any

289
00:16:25,880 --> 00:16:33,600
resumption of an off-road job in one

290
00:16:29,790 --> 00:16:35,939
walks read and when Yuri enters same

291
00:16:33,600 --> 00:16:38,250
handler you need to use stealth black or

292
00:16:35,940 --> 00:16:41,940
fabric or routine but we do have a

293
00:16:38,250 --> 00:16:44,970
reference name the QT OS it implements a

294
00:16:41,940 --> 00:16:47,760
high promise SSE OTRS asynchronous

295
00:16:44,970 --> 00:16:54,870
offered framework and it was published

296
00:16:47,760 --> 00:16:57,990
in people 2019 so thank you is there any

297
00:16:54,870 --> 00:17:02,660
question our work has been integrated

298
00:16:57,990 --> 00:17:05,669
into the gvfs Linux release and other

299
00:17:02,660 --> 00:17:07,399
related projects are also available on

300
00:17:05,669 --> 00:17:16,860
github

301
00:17:07,400 --> 00:17:21,329
Thank You girl Dell nice work

302
00:17:16,859 --> 00:17:23,668
QT what we've seen is that when it

303
00:17:21,329 --> 00:17:28,349
reaches the bandwidth the latency goes

304
00:17:23,669 --> 00:17:30,630
really high and the side effect is that

305
00:17:28,349 --> 00:17:33,750
by implementing this in the file system

306
00:17:30,630 --> 00:17:36,000
the applications performance can really

307
00:17:33,750 --> 00:17:37,830
go down because you don't know how much

308
00:17:36,000 --> 00:17:39,179
it can really support because everyone

309
00:17:37,830 --> 00:17:41,129
because you're compressing everything

310
00:17:39,179 --> 00:17:46,040
have you any thoughts on that or have

311
00:17:41,130 --> 00:17:48,480
you seen this behavior since the

312
00:17:46,040 --> 00:17:51,870
compression and expression are performed

313
00:17:48,480 --> 00:17:56,040
as a passive layer until has the Koran

314
00:17:51,870 --> 00:18:01,129
your entity of the block size the gfs

315
00:17:56,040 --> 00:18:05,310
has a block size so maybe no more than

316
00:18:01,130 --> 00:18:12,090
128 kilo kilo pies and

317
00:18:05,310 --> 00:18:14,850
if you want to read a fire or some the

318
00:18:12,090 --> 00:18:19,110
latency is determined by the icing is

319
00:18:14,850 --> 00:18:22,459
determined by the by the side of the

320
00:18:19,110 --> 00:18:22,459
compressed flock

321
00:18:23,220 --> 00:18:28,200
okay so QT or floating has a latency

322
00:18:25,740 --> 00:18:30,210
because you are giving the work

323
00:18:28,200 --> 00:18:32,130
Duke Yoda car and unless I have enough

324
00:18:30,210 --> 00:18:34,080
unless you give enough requests

325
00:18:32,130 --> 00:18:35,760
you cannot off so that laid out you

326
00:18:34,080 --> 00:18:37,409
cannot offset that latency so that in

327
00:18:35,760 --> 00:18:43,080
general has been a problem with Quixote

328
00:18:37,410 --> 00:18:46,730
so my fly after the compression walk to

329
00:18:43,080 --> 00:18:50,540
the KT that answer will be increased yes

330
00:18:46,730 --> 00:18:53,430
actually the Quixote can perform the

331
00:18:50,540 --> 00:19:00,440
compression decompression much faster

332
00:18:53,430 --> 00:19:08,420
than the CPU so in our test if you

333
00:19:00,440 --> 00:19:16,860
upload the block size moseyin possible

334
00:19:08,420 --> 00:19:20,060
64 or 17 or 80 kilobytes the KT can can

335
00:19:16,860 --> 00:19:24,689
do the compression much faster than the

336
00:19:20,060 --> 00:19:27,510
then the CPU so you wave the request and

337
00:19:24,690 --> 00:19:30,240
the response it's still smaller take a

338
00:19:27,510 --> 00:19:37,910
smaller time than the CPU okay thanks we

339
00:19:30,240 --> 00:19:37,910
can discuss mostly any more questions

340
00:19:39,230 --> 00:19:46,980
so never meet them work so regarding the

341
00:19:44,220 --> 00:19:49,140
latency don't you see any effect of

342
00:19:46,980 --> 00:19:52,050
Kesha's as well because data needs to

343
00:19:49,140 --> 00:19:55,590
traverse from the CPU do you you have

344
00:19:52,050 --> 00:19:58,980
does security support did I or did I oh

345
00:19:55,590 --> 00:20:01,919
I think do you move the data from the

346
00:19:58,980 --> 00:20:05,220
CPU to the device as well so you have

347
00:20:01,920 --> 00:20:10,010
the latency of the PCI for the DMAs and

348
00:20:05,220 --> 00:20:13,290
you also have so and you also

349
00:20:10,010 --> 00:20:16,290
potentially access the memory so gangs

350
00:20:13,290 --> 00:20:20,250
see any latency because of that like for

351
00:20:16,290 --> 00:20:27,930
small Isles to see an effect 80

352
00:20:20,250 --> 00:20:31,620
kilobytes is enough yes actually for

353
00:20:27,930 --> 00:20:36,600
small block size and maybe the whole

354
00:20:31,620 --> 00:20:42,810
latency when you use Katie's his longer

355
00:20:36,600 --> 00:20:48,060
than the CPU case yeah that's true but

356
00:20:42,810 --> 00:20:51,690
as we as in our evaluation we can see

357
00:20:48,060 --> 00:20:55,169
that if we use some high high

358
00:20:51,690 --> 00:20:58,230
compression ratio the comparative nakji

359
00:20:55,170 --> 00:21:01,200
GD p-- the CPU reservation for the

360
00:20:58,230 --> 00:21:04,680
software information implementation is

361
00:21:01,200 --> 00:21:07,470
very high so it may affect the normal

362
00:21:04,680 --> 00:21:11,370
running of the application so if we we

363
00:21:07,470 --> 00:21:15,350
can offload the compression task to the

364
00:21:11,370 --> 00:21:18,330
qat the CPU is free freedom to

365
00:21:15,350 --> 00:21:21,929
technically a dedicatory running the

366
00:21:18,330 --> 00:21:25,850
application so the whole system

367
00:21:21,930 --> 00:21:25,850
performance can be enhanced

368
00:21:28,750 --> 00:21:36,949
you know thank our speaker one more time

369
00:21:31,640 --> 00:21:36,949
[Applause]

