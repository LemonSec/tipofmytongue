1
00:00:10,259 --> 00:00:15,779
thank you for the introduction I'm a guy

2
00:00:12,990 --> 00:00:16,680
Ron and today I'll present Nika which is

3
00:00:15,779 --> 00:00:18,180
a software and how

4
00:00:16,680 --> 00:00:20,990
framework for inline application

5
00:00:18,180 --> 00:00:24,060
acceleration on FPGA based smart Nix

6
00:00:20,990 --> 00:00:26,820
this is a collaboration with Leo Zeno

7
00:00:24,060 --> 00:00:32,940
Maroun torque Gabi Malka and my advisor

8
00:00:26,820 --> 00:00:35,850
mark silverstein FPGA based smart Nix

9
00:00:32,940 --> 00:00:38,519
are used by cloud providers like

10
00:00:35,850 --> 00:00:41,400
Microsoft and $0.10 by adding

11
00:00:38,520 --> 00:00:46,500
programmable FPGAs into the standard

12
00:00:41,400 --> 00:00:48,210
regular network interface card to

13
00:00:46,500 --> 00:00:50,820
provide the cloud infrastructure

14
00:00:48,210 --> 00:00:52,230
acceleration for things like

15
00:00:50,820 --> 00:00:55,590
software-defined networking and network

16
00:00:52,230 --> 00:00:58,290
function virtualization for example

17
00:00:55,590 --> 00:01:01,010
Microsoft has deployed smart nakhon

18
00:00:58,290 --> 00:01:03,510
every edger server since May 2015

19
00:01:01,010 --> 00:01:07,679
but these smart NICs can be used for

20
00:01:03,510 --> 00:01:09,899
more than just cloud infrastructure let

21
00:01:07,680 --> 00:01:12,060
me explain what I mean by inline

22
00:01:09,899 --> 00:01:17,190
acceleration and the benefits of that

23
00:01:12,060 --> 00:01:21,030
using an analogy take this young father

24
00:01:17,190 --> 00:01:22,710
for example you can sleep well knowing

25
00:01:21,030 --> 00:01:26,700
that his baby can wake him up whenever

26
00:01:22,710 --> 00:01:28,949
he needs to well his brain filters out

27
00:01:26,700 --> 00:01:34,890
any distractions or noises from the

28
00:01:28,950 --> 00:01:38,729
environment similarly a smart knee can

29
00:01:34,890 --> 00:01:41,790
filter incoming data and only alert the

30
00:01:38,729 --> 00:01:44,070
CPU when it actually needs to that's

31
00:01:41,790 --> 00:01:49,140
reducing the CPU load and improving

32
00:01:44,070 --> 00:01:51,600
system throughput the smart league can

33
00:01:49,140 --> 00:01:54,150
also manipulate or transform the data

34
00:01:51,600 --> 00:01:57,439
that comes into the CPU to make it

35
00:01:54,150 --> 00:01:57,439
simpler for CPU consumption

36
00:02:00,719 --> 00:02:06,089
here's an example of an application that

37
00:02:02,729 --> 00:02:08,369
might use these abilities a key value

38
00:02:06,090 --> 00:02:11,430
store cache can store popular keys and

39
00:02:08,369 --> 00:02:14,099
items on the smart Nick smart Nick can

40
00:02:11,430 --> 00:02:16,650
serve cache hits directly without

41
00:02:14,099 --> 00:02:19,200
involving the CPU that's improving the

42
00:02:16,650 --> 00:02:21,120
latency and throughput previous work

43
00:02:19,200 --> 00:02:27,238
have shown to the benefits of this

44
00:02:21,120 --> 00:02:29,940
approach another example uses a

45
00:02:27,239 --> 00:02:32,549
cryptographic offload to authenticate

46
00:02:29,940 --> 00:02:34,980
incoming requests using the cop protocol

47
00:02:32,549 --> 00:02:38,700
which is used by IOT servers such as

48
00:02:34,980 --> 00:02:42,988
Samsung RT cloud this filtering can

49
00:02:38,700 --> 00:02:45,929
detect malicious messages and drop them

50
00:02:42,989 --> 00:02:49,280
without involving the CPU protecting

51
00:02:45,930 --> 00:02:49,280
against denial of service attacks

52
00:02:49,579 --> 00:02:58,590
unlike IPSec uploads that could be used

53
00:02:52,440 --> 00:03:02,030
for similar functionality the smart Nick

54
00:02:58,590 --> 00:03:04,230
can look into the application layer and

55
00:03:02,030 --> 00:03:11,639
customize the processing for the

56
00:03:04,230 --> 00:03:14,370
specific application thus several

57
00:03:11,639 --> 00:03:17,730
challenges in deploying such smart Nick

58
00:03:14,370 --> 00:03:19,829
accelerators in the cloud to do that we

59
00:03:17,730 --> 00:03:23,040
need operating systems abstractions that

60
00:03:19,829 --> 00:03:25,650
would expose the underlying accelerators

61
00:03:23,040 --> 00:03:27,418
to the applications and we need

62
00:03:25,650 --> 00:03:28,919
virtualization support to be able to

63
00:03:27,419 --> 00:03:32,040
share accelerators among multiple

64
00:03:28,919 --> 00:03:35,430
tenants which includes state isolation

65
00:03:32,040 --> 00:03:37,380
support and performance isolation we

66
00:03:35,430 --> 00:03:42,959
develop the nikah framework to answer

67
00:03:37,380 --> 00:03:45,660
these needs in this talk I will tell you

68
00:03:42,959 --> 00:03:48,840
about the AI kernel abstraction which is

69
00:03:45,660 --> 00:03:52,370
our abstraction to represent the smart

70
00:03:48,840 --> 00:03:56,010
Nick accelerators and explain how we add

71
00:03:52,370 --> 00:03:59,540
virtualization support to share a single

72
00:03:56,010 --> 00:03:59,540
accelerator among multiple tens

73
00:04:01,770 --> 00:04:07,320
so the icon of abstraction is designed

74
00:04:04,830 --> 00:04:10,170
to represent an AFU an accelerator

75
00:04:07,320 --> 00:04:13,890
functional unit that exists in the FPGA

76
00:04:10,170 --> 00:04:16,229
on the smart NIC we create an icon all

77
00:04:13,890 --> 00:04:18,570
in the process to represent it and then

78
00:04:16,230 --> 00:04:23,520
the process can control that instance of

79
00:04:18,570 --> 00:04:25,650
the accelerator our process also uses

80
00:04:23,520 --> 00:04:30,150
POSIX sockets to communicate with a

81
00:04:25,650 --> 00:04:36,929
network by attaching the icon or with

82
00:04:30,150 --> 00:04:38,489
the socket the system system makes it so

83
00:04:36,930 --> 00:04:41,640
that all the packets are processed by

84
00:04:38,490 --> 00:04:43,320
the AFU before reaching the socket and

85
00:04:41,640 --> 00:04:50,250
the application can continue using that

86
00:04:43,320 --> 00:04:53,460
socket to access the filter data I can

87
00:04:50,250 --> 00:04:55,770
also have several interfaces as well as

88
00:04:53,460 --> 00:04:58,260
I've mentioned they can use posting

89
00:04:55,770 --> 00:05:00,810
sockets to access the data plane but

90
00:04:58,260 --> 00:05:03,360
we've also developed a faster data plane

91
00:05:00,810 --> 00:05:06,570
interface we call the custom ring to

92
00:05:03,360 --> 00:05:11,160
bypass the host network stack in

93
00:05:06,570 --> 00:05:14,010
addition I cannot use RPC commands or

94
00:05:11,160 --> 00:05:17,040
messages from the host to access the

95
00:05:14,010 --> 00:05:18,780
accelerator state this can be used to

96
00:05:17,040 --> 00:05:24,810
configure cryptographic keys or to read

97
00:05:18,780 --> 00:05:27,479
performance counters so here is an

98
00:05:24,810 --> 00:05:32,330
example of how an application might use

99
00:05:27,480 --> 00:05:36,630
an icon of first it creates the I kernel

100
00:05:32,330 --> 00:05:39,500
instantiate da-fu for it and then it can

101
00:05:36,630 --> 00:05:41,820
configure it using RPC commands after

102
00:05:39,500 --> 00:05:43,950
that it may create sockets to

103
00:05:41,820 --> 00:05:47,610
communicate and attach them to the I

104
00:05:43,950 --> 00:05:50,580
kernel and after that it can just use

105
00:05:47,610 --> 00:05:52,500
the regular receive message API to

106
00:05:50,580 --> 00:05:56,550
receive data on that socket but

107
00:05:52,500 --> 00:05:58,230
receiving only the filter data using

108
00:05:56,550 --> 00:06:01,440
this abstraction we were able to

109
00:05:58,230 --> 00:06:04,410
integrate in AFU with memcache D with

110
00:06:01,440 --> 00:06:06,920
only 107 lines of code showing its

111
00:06:04,410 --> 00:06:06,920
simplicity

112
00:06:09,200 --> 00:06:15,840
now we move to how we virtualize an icon

113
00:06:13,230 --> 00:06:17,550
or a nephew and virtualization is

114
00:06:15,840 --> 00:06:19,770
important important so we can deploy

115
00:06:17,550 --> 00:06:22,710
these kind of accelerators in a cloud

116
00:06:19,770 --> 00:06:25,590
environment we can divide it into two

117
00:06:22,710 --> 00:06:28,890
topics compute virtualization and i/o

118
00:06:25,590 --> 00:06:31,109
virtualization for compute

119
00:06:28,890 --> 00:06:33,780
virtualization we first need to

120
00:06:31,110 --> 00:06:36,450
understand how we can share in a few and

121
00:06:33,780 --> 00:06:39,869
an FPGA or share even share an FPGA

122
00:06:36,450 --> 00:06:42,719
among multiple tenants one we use space

123
00:06:39,870 --> 00:06:45,360
sharing to divide the resources on the

124
00:06:42,720 --> 00:06:49,310
FPGA statically between different views

125
00:06:45,360 --> 00:06:49,310
but this can limit the utilization

126
00:06:50,210 --> 00:06:55,739
another alternative which was used in

127
00:06:53,070 --> 00:06:58,140
previous work uses coarse-grained time

128
00:06:55,740 --> 00:07:02,490
sharing to switch dynamically between a

129
00:06:58,140 --> 00:07:04,590
fuse but the and this can be used with

130
00:07:02,490 --> 00:07:08,070
the partial reconfiguration for example

131
00:07:04,590 --> 00:07:10,650
on FPGAs but the long context which time

132
00:07:08,070 --> 00:07:14,930
of partial configuration makes it

133
00:07:10,650 --> 00:07:14,929
unsuitable for networking applications

134
00:07:16,340 --> 00:07:22,349
finally we can use fine-grained time

135
00:07:18,540 --> 00:07:26,430
sharing in this way we take a single AFU

136
00:07:22,350 --> 00:07:29,700
and let multiple tenants share its same

137
00:07:26,430 --> 00:07:32,190
logic we can use Hardware context

138
00:07:29,700 --> 00:07:33,539
switching inside the AFU to switch

139
00:07:32,190 --> 00:07:36,300
between the context of the different

140
00:07:33,540 --> 00:07:43,410
tenants with relatively very low latency

141
00:07:36,300 --> 00:07:45,630
even a packet of granularity anika or

142
00:07:43,410 --> 00:07:47,460
design supports the first two methods

143
00:07:45,630 --> 00:07:51,450
but we focus on the fine-grained time

144
00:07:47,460 --> 00:07:55,700
sharing to focus on how we virtualize

145
00:07:51,450 --> 00:07:55,700
the data plane for such a fuse

146
00:07:58,280 --> 00:08:05,179
as a consequence of our virtualization

147
00:08:00,770 --> 00:08:08,299
model we need to choose our the relevant

148
00:08:05,180 --> 00:08:10,910
deployment model for the cloud in the

149
00:08:08,300 --> 00:08:14,050
FPGA as a service cloud deployment model

150
00:08:10,910 --> 00:08:18,130
customers may bring their own design and

151
00:08:14,050 --> 00:08:21,980
upload it to the cloud and use it and

152
00:08:18,130 --> 00:08:24,440
this uses cost bearing time sharing in

153
00:08:21,980 --> 00:08:28,520
its implementation this is used by

154
00:08:24,440 --> 00:08:30,410
services such as Amazon f1 but because

155
00:08:28,520 --> 00:08:34,490
customers bring their own design we can

156
00:08:30,410 --> 00:08:38,390
trust that design to implement a few

157
00:08:34,490 --> 00:08:40,340
context switching they could leak data

158
00:08:38,390 --> 00:08:45,230
between different tenants as you can

159
00:08:40,340 --> 00:08:46,520
imagine instead we imagine that cloud

160
00:08:45,230 --> 00:08:49,400
providers will develop their own

161
00:08:46,520 --> 00:08:50,840
airfields and they would be trusted to

162
00:08:49,400 --> 00:08:54,500
implement fine-grained virtualization

163
00:08:50,840 --> 00:08:56,390
and customers will choose from a

164
00:08:54,500 --> 00:09:03,560
marketplace of predefined they refuse

165
00:08:56,390 --> 00:09:08,000
the one that they want to use now for

166
00:09:03,560 --> 00:09:11,000
i/o virtualization we need to virtualize

167
00:09:08,000 --> 00:09:14,660
every IO interface that the AF you can

168
00:09:11,000 --> 00:09:16,130
access in order to isolate it and

169
00:09:14,660 --> 00:09:20,449
isolate the different tenants and the

170
00:09:16,130 --> 00:09:23,270
different AF use for that we add logic

171
00:09:20,450 --> 00:09:26,180
that inspects and the processes the data

172
00:09:23,270 --> 00:09:28,339
on each interface in virtualized that by

173
00:09:26,180 --> 00:09:30,739
tagging the data on the interface with

174
00:09:28,339 --> 00:09:38,630
metadata specifying to each VM it

175
00:09:30,740 --> 00:09:41,000
belongs in addition we want to provide

176
00:09:38,630 --> 00:09:44,510
performance isolation guarantees so that

177
00:09:41,000 --> 00:09:49,730
one VM can acquire all the resources of

178
00:09:44,510 --> 00:09:52,460
a given link or a given resource to do

179
00:09:49,730 --> 00:09:54,710
that we add IO schedulers packet

180
00:09:52,460 --> 00:09:57,640
schedulers on the incoming and outgoing

181
00:09:54,710 --> 00:10:02,330
interfaces to the host or to the network

182
00:09:57,640 --> 00:10:07,089
that can divide the bandwidth with a

183
00:10:02,330 --> 00:10:07,089
weighted scheduler even

184
00:10:08,040 --> 00:10:14,189
and for compute virtualization as I

185
00:10:11,730 --> 00:10:17,730
mentioned we have the compute scheduler

186
00:10:14,190 --> 00:10:25,650
inside each AFU that schedules the

187
00:10:17,730 --> 00:10:27,030
compute resources in the paper you can

188
00:10:25,650 --> 00:10:30,140
find more details about how we

189
00:10:27,030 --> 00:10:32,220
implemented or how the run time the

190
00:10:30,140 --> 00:10:35,510
integration with the host network stack

191
00:10:32,220 --> 00:10:37,830
and how we added TCP support our

192
00:10:35,510 --> 00:10:41,790
customer implementation and how it

193
00:10:37,830 --> 00:10:44,940
utilizes an ASIC NICs RDMA abilities for

194
00:10:41,790 --> 00:10:51,449
its implementation and our SR UV and

195
00:10:44,940 --> 00:10:55,170
power virtual interfaces moving to the

196
00:10:51,450 --> 00:10:57,690
evaluation we use the 40 gigabit bumping

197
00:10:55,170 --> 00:11:00,750
the wire smart Nick to implement the

198
00:10:57,690 --> 00:11:03,960
system and you called the Mellanox anova

199
00:11:00,750 --> 00:11:06,750
flex which combines the smart NIC which

200
00:11:03,960 --> 00:11:09,720
sorry which combines an FPGA from Xilinx

201
00:11:06,750 --> 00:11:11,130
and the Mellanox ASIC NIC this design is

202
00:11:09,720 --> 00:11:18,090
similar to the one used in Microsoft

203
00:11:11,130 --> 00:11:20,210
catapult our evaluation uses the VM a

204
00:11:18,090 --> 00:11:24,020
user space network stack as a baseline

205
00:11:20,210 --> 00:11:26,700
to mitigate kernel stack overheads and

206
00:11:24,020 --> 00:11:29,430
we evaluated using several micro

207
00:11:26,700 --> 00:11:31,890
benchmarks to show the performance of

208
00:11:29,430 --> 00:11:34,739
the nikah infrastructure as well as two

209
00:11:31,890 --> 00:11:38,850
applications memcache deal with the key

210
00:11:34,740 --> 00:11:45,450
value store cash and an IOT server that

211
00:11:38,850 --> 00:11:47,640
shows the authentication offload in this

212
00:11:45,450 --> 00:11:50,070
talk I would focus I will focus only on

213
00:11:47,640 --> 00:11:52,650
the memcache the experiments but I want

214
00:11:50,070 --> 00:11:54,570
to stress the small number of lines of

215
00:11:52,650 --> 00:11:57,150
code required to integrate the IOT

216
00:11:54,570 --> 00:12:00,680
server with the AF you again showing the

217
00:11:57,150 --> 00:12:00,680
simplicity of our abstraction

218
00:12:03,199 --> 00:12:09,339
to run the memcache the example we set

219
00:12:06,889 --> 00:12:11,839
the host with 32 million keys and

220
00:12:09,339 --> 00:12:14,720
configured the cache to host two million

221
00:12:11,839 --> 00:12:18,829
keys and use the system with 16 byte

222
00:12:14,720 --> 00:12:22,869
keys and values we used the memcache the

223
00:12:18,829 --> 00:12:24,679
UDP ascii protocol in da-fu and

224
00:12:22,869 --> 00:12:26,989
integrating the fu as i've mentioned

225
00:12:24,679 --> 00:12:35,209
only used a small number of lines of

226
00:12:26,989 --> 00:12:38,230
code here you can see the performance

227
00:12:35,209 --> 00:12:42,018
throughput while varying different

228
00:12:38,230 --> 00:12:45,410
distributions of the request the

229
00:12:42,019 --> 00:12:49,189
requested keys as you can imagine with

230
00:12:45,410 --> 00:12:51,980
low heat rates the throughput of the

231
00:12:49,189 --> 00:12:53,329
system doesn't improve as as we filter

232
00:12:51,980 --> 00:12:56,509
packets because there are not much

233
00:12:53,329 --> 00:12:58,219
packets to filter but we still get 2

234
00:12:56,509 --> 00:13:00,619
times performance improvement by using

235
00:12:58,220 --> 00:13:07,279
the custom link mechanism to bypass the

236
00:13:00,619 --> 00:13:11,749
host network stack when we increase the

237
00:13:07,279 --> 00:13:15,589
Zips you parameter we get more hit rates

238
00:13:11,749 --> 00:13:18,439
and for as if 99 which is the default by

239
00:13:15,589 --> 00:13:21,230
YC SB benchmark we get 4 times

240
00:13:18,439 --> 00:13:22,910
improvement for filtering and 9 times we

241
00:13:21,230 --> 00:13:28,309
put overall improvement with both

242
00:13:22,910 --> 00:13:30,889
filtering and customink we increase

243
00:13:28,309 --> 00:13:35,660
increase the zip parameter further

244
00:13:30,889 --> 00:13:37,970
further we get almost all the all the

245
00:13:35,660 --> 00:13:41,238
requests to hit the cache and then the

246
00:13:37,970 --> 00:13:43,129
system is button act by the AF you

247
00:13:41,239 --> 00:13:46,399
performance itself and by the network we

248
00:13:43,129 --> 00:13:48,639
get new line rate performance in that

249
00:13:46,399 --> 00:13:48,639
scenario

250
00:13:52,140 --> 00:13:57,569
we ran another experiment with multiple

251
00:13:54,750 --> 00:14:02,120
virtual machines trying to show the

252
00:13:57,570 --> 00:14:04,260
scaling of the system with multiple VMs

253
00:14:02,120 --> 00:14:06,750
you can see that we get similar

254
00:14:04,260 --> 00:14:09,650
performance improvements even when using

255
00:14:06,750 --> 00:14:14,060
virtualization and that the system

256
00:14:09,650 --> 00:14:14,060
scales linearly with a number of VMs

257
00:14:16,220 --> 00:14:20,580
we also evaluated the latency of

258
00:14:18,660 --> 00:14:24,990
memcache D with nikka under

259
00:14:20,580 --> 00:14:29,760
virtualization here you can see that for

260
00:14:24,990 --> 00:14:32,880
60% heat rates which is the hit rate

261
00:14:29,760 --> 00:14:34,340
that we got in this experiment we get 2

262
00:14:32,880 --> 00:14:37,770
point 1 microsecond

263
00:14:34,340 --> 00:14:41,240
latency which is the same as bare-metal

264
00:14:37,770 --> 00:14:41,240
because this is the hardware performance

265
00:14:41,570 --> 00:14:47,610
for me says we get well 262 microsecond

266
00:14:44,970 --> 00:14:49,950
latency compared to 6 microsecond on

267
00:14:47,610 --> 00:14:52,530
bare metal showing that the smartness

268
00:14:49,950 --> 00:14:54,950
acceleration can reduce the translation

269
00:14:52,530 --> 00:14:54,949
over it

270
00:14:59,760 --> 00:15:06,120
finally I want to show our performance

271
00:15:02,339 --> 00:15:09,209
isolation system we run three virtual

272
00:15:06,120 --> 00:15:13,110
machines each receiving twenty million

273
00:15:09,209 --> 00:15:16,680
transactions per second workload as we

274
00:15:13,110 --> 00:15:19,350
add more and more VMS the output

275
00:15:16,680 --> 00:15:21,329
bandwidth is limited to the overall 40

276
00:15:19,350 --> 00:15:23,850
million transactions and the output

277
00:15:21,329 --> 00:15:29,099
bandwidth is divided evenly by a packet

278
00:15:23,850 --> 00:15:31,709
scheduler we then add we then configure

279
00:15:29,100 --> 00:15:34,769
the weights of the scheduler differently

280
00:15:31,709 --> 00:15:38,369
to get an uneven distribution limiting

281
00:15:34,769 --> 00:15:41,910
the third viens performance while the

282
00:15:38,370 --> 00:15:46,860
the the rest of the bandwidth is split

283
00:15:41,910 --> 00:15:49,620
evenly between the first two VMs shows

284
00:15:46,860 --> 00:15:53,420
our schedulers can provide performance

285
00:15:49,620 --> 00:15:53,420
isolations for multiple virtual machines

286
00:15:54,949 --> 00:16:01,139
in conclusion nikka enables smarten

287
00:15:59,399 --> 00:16:03,089
acceleration for applications by

288
00:16:01,139 --> 00:16:05,339
providing the abstractions needed and

289
00:16:03,089 --> 00:16:07,920
the virtualization support that is

290
00:16:05,339 --> 00:16:10,649
required to deploy it in the cloud you

291
00:16:07,920 --> 00:16:12,029
can try out our code on github thank you

292
00:16:10,649 --> 00:16:20,389
for listening and I'll be happy to take

293
00:16:12,029 --> 00:16:20,389
your questions questions

294
00:16:26,010 --> 00:16:30,330
my name's courgettes EPFL great work

295
00:16:28,260 --> 00:16:31,560
thank you for the presentation I have a

296
00:16:30,330 --> 00:16:34,800
question regarding your TCP

297
00:16:31,560 --> 00:16:37,349
implementation so I assume that your

298
00:16:34,800 --> 00:16:40,050
FPGA can also send back replies without

299
00:16:37,350 --> 00:16:41,580
involving the host so I would like to

300
00:16:40,050 --> 00:16:43,319
know how can you make sure that you can

301
00:16:41,580 --> 00:16:46,320
deliver replies in order when you have

302
00:16:43,320 --> 00:16:48,150
both the FPGA and the CPU sending back

303
00:16:46,320 --> 00:16:49,830
replies and how can you maintain the TCP

304
00:16:48,150 --> 00:16:52,829
stay consistent between the FPGA and the

305
00:16:49,830 --> 00:16:57,600
CPU thank you thank you for a question

306
00:16:52,830 --> 00:17:01,350
that's a good question for TCP we can't

307
00:16:57,600 --> 00:17:04,020
allow different entities like the FPGA

308
00:17:01,350 --> 00:17:06,510
and the host to send packets on the same

309
00:17:04,020 --> 00:17:10,440
socket at the same time so we require

310
00:17:06,510 --> 00:17:12,270
that for such workloads the host would

311
00:17:10,440 --> 00:17:14,610
use the custom ring to communicate with

312
00:17:12,270 --> 00:17:17,510
the Fu and the F you would send data on

313
00:17:14,609 --> 00:17:17,510
the socket on its behalf

314
00:17:20,119 --> 00:17:27,968
let's thank our speaker

315
00:17:23,319 --> 00:17:27,969
[Applause]

