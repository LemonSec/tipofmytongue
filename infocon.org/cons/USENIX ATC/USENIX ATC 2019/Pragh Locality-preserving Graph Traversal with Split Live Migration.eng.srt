1
00:00:09,980 --> 00:00:15,590
I would like to introduce our work prof

2
00:00:13,099 --> 00:00:18,439
it is a live migration scheme to

3
00:00:15,590 --> 00:00:21,140
preserve locality for graph traversal it

4
00:00:18,439 --> 00:00:24,340
is a joint work with my colleague Shinda

5
00:00:21,140 --> 00:00:28,210
and them and my advisers ruin high ball

6
00:00:24,340 --> 00:00:30,110
so graphs are can model connect a

7
00:00:28,210 --> 00:00:33,350
relationships of Montera

8
00:00:30,110 --> 00:00:36,649
and so widely used in many domains such

9
00:00:33,350 --> 00:00:39,320
as social network and in graph system

10
00:00:36,649 --> 00:00:41,719
chozo queries are key operations to

11
00:00:39,320 --> 00:00:45,860
support emerging applications such as

12
00:00:41,719 --> 00:00:48,230
user profiling when executing traversal

13
00:00:45,860 --> 00:00:51,890
queries it is important to preserve

14
00:00:48,230 --> 00:00:55,489
locality because local memory access our

15
00:00:51,890 --> 00:00:59,660
orders magnitude faster than remote ones

16
00:00:55,489 --> 00:01:02,570
even a fast network to show this let's

17
00:00:59,660 --> 00:01:05,420
look at an example query in social

18
00:01:02,570 --> 00:01:08,750
network which finds the interact follow

19
00:01:05,420 --> 00:01:11,990
year of hypo to find the followers we

20
00:01:08,750 --> 00:01:15,469
start from hi boss root X and traversal

21
00:01:11,990 --> 00:01:18,380
graph according to the friend edge graph

22
00:01:15,469 --> 00:01:21,199
is usually large so the graph is

23
00:01:18,380 --> 00:01:24,740
randomly partitioned on two nodes and

24
00:01:21,200 --> 00:01:26,959
stored by keyword all the vertices are

25
00:01:24,740 --> 00:01:30,798
kids and their neighbors are values as

26
00:01:26,959 --> 00:01:33,049
we can see the follow year wrong is on

27
00:01:30,799 --> 00:01:36,130
the same node with high wall so we can

28
00:01:33,049 --> 00:01:39,889
directly fetch it using local sets

29
00:01:36,130 --> 00:01:42,708
however the following year of room is on

30
00:01:39,889 --> 00:01:46,520
another node which results in remote

31
00:01:42,709 --> 00:01:49,310
successes meanwhile it is challenging to

32
00:01:46,520 --> 00:01:51,920
preserve locality on dynamical clothes

33
00:01:49,310 --> 00:01:54,649
as traditional static partition scheme

34
00:01:51,920 --> 00:01:59,990
cannot handle different traversal

35
00:01:54,649 --> 00:02:03,920
clothes for example at Java clock a

36
00:01:59,990 --> 00:02:07,520
query about high ball starts from node 0

37
00:02:03,920 --> 00:02:10,640
after a while another query about shatin

38
00:02:07,520 --> 00:02:14,269
starts from note one which is competing

39
00:02:10,639 --> 00:02:17,559
with the first query this is a false

40
00:02:14,270 --> 00:02:21,650
contention complicating static petition

41
00:02:17,560 --> 00:02:23,790
so traditional storage systems used live

42
00:02:21,650 --> 00:02:27,120
migration to preserve locality

43
00:02:23,790 --> 00:02:28,950
one soul cloth changes left migration

44
00:02:27,120 --> 00:02:33,510
moves data according to a locality

45
00:02:28,950 --> 00:02:36,629
requirement in the example not zero

46
00:02:33,510 --> 00:02:39,450
request data of Rome life migration

47
00:02:36,629 --> 00:02:42,798
moves data and now all the data are

48
00:02:39,450 --> 00:02:46,319
local after a while the workload changes

49
00:02:42,799 --> 00:02:50,459
so the data of Rome is my greatest back

50
00:02:46,319 --> 00:02:52,760
again however traditional shadow based

51
00:02:50,459 --> 00:02:56,370
migration does not fit graph traversal

52
00:02:52,760 --> 00:02:59,189
this is because graph is skinless and

53
00:02:56,370 --> 00:03:02,489
hard to partition related data in Schad

54
00:02:59,189 --> 00:03:06,840
for example to key value pairs are

55
00:03:02,489 --> 00:03:10,319
grouped and migrated in shot a query

56
00:03:06,840 --> 00:03:13,319
about hypo traverses from shot to two

57
00:03:10,319 --> 00:03:16,560
shots zero meanwhile another query

58
00:03:13,319 --> 00:03:19,290
abandoning also reaches shot zero two

59
00:03:16,560 --> 00:03:22,319
queries are competing for different data

60
00:03:19,290 --> 00:03:26,159
in the same shot incurring false

61
00:03:22,319 --> 00:03:30,000
contention this weakens effect affects

62
00:03:26,159 --> 00:03:33,120
of migration what's words fine-grained

63
00:03:30,000 --> 00:03:36,629
Mashhad petition suffers another problem

64
00:03:33,120 --> 00:03:39,720
of method metadata as chars location is

65
00:03:36,629 --> 00:03:42,720
dynamic metadata is necessary to

66
00:03:39,720 --> 00:03:46,049
maintain the latest location for example

67
00:03:42,720 --> 00:03:50,099
to find the follower of rome we first

68
00:03:46,049 --> 00:03:54,690
need to up a lookup metadata and find

69
00:03:50,099 --> 00:03:55,048
shut zero is our note 1 then find the

70
00:03:54,690 --> 00:03:58,829
data

71
00:03:55,049 --> 00:04:01,979
notice that metadata is not scalable as

72
00:03:58,829 --> 00:04:06,299
each node needs to catch entire metadata

73
00:04:01,979 --> 00:04:08,790
to speed up in the query to show the

74
00:04:06,299 --> 00:04:11,040
problem of shared waste migration let's

75
00:04:08,790 --> 00:04:14,400
locate an evaluation on a representative

76
00:04:11,040 --> 00:04:17,608
travel hole close the left y-axis is

77
00:04:14,400 --> 00:04:22,139
number of metadata entry and the right

78
00:04:17,608 --> 00:04:25,289
y-axis is throughput the x-axis is

79
00:04:22,139 --> 00:04:29,009
number of chars the size of the shot is

80
00:04:25,289 --> 00:04:32,279
decreased with coarse-grained shot the

81
00:04:29,009 --> 00:04:35,669
Frankland benefits is weak due to high

82
00:04:32,279 --> 00:04:37,770
force contention the benefits becomes

83
00:04:35,669 --> 00:04:41,190
good only with fine

84
00:04:37,770 --> 00:04:44,580
shot which suffers heavy and not

85
00:04:41,190 --> 00:04:47,550
scalable metadata overhead so how to

86
00:04:44,580 --> 00:04:49,349
treat how to achieve zero Mella did have

87
00:04:47,550 --> 00:04:54,180
fine-grained migration and graph

88
00:04:49,349 --> 00:04:56,669
traversal a hardware feature called Rema

89
00:04:54,180 --> 00:04:59,909
is becoming popular recently to

90
00:04:56,669 --> 00:05:03,120
accelerate distributed applications its

91
00:04:59,909 --> 00:05:05,969
provides cross node accesses with high

92
00:05:03,120 --> 00:05:09,210
speed low latency and chrono bypassing

93
00:05:05,970 --> 00:05:12,180
the one site is primitive such as read

94
00:05:09,210 --> 00:05:15,150
write and compare swap last direct

95
00:05:12,180 --> 00:05:19,199
access to another node by passing a host

96
00:05:15,150 --> 00:05:23,219
CPU the primitive changes data access

97
00:05:19,199 --> 00:05:27,750
pattern it splits accesses to keys and

98
00:05:23,220 --> 00:05:30,229
values which opens an opportunity to 4-0

99
00:05:27,750 --> 00:05:34,050
metadata fine-grained migration

100
00:05:30,229 --> 00:05:38,479
let's see how our DMA splits accesses

101
00:05:34,050 --> 00:05:41,880
first to get data of long on node 1

102
00:05:38,479 --> 00:05:45,508
first it first fetches the address of

103
00:05:41,880 --> 00:05:48,479
value with one atom and read and then

104
00:05:45,509 --> 00:05:52,199
use another read to get the value in

105
00:05:48,479 --> 00:05:56,279
short it requires two reads to fetch one

106
00:05:52,199 --> 00:05:59,159
key value pair other makes it easy and

107
00:05:56,279 --> 00:06:02,789
efficient to split keys and values in

108
00:05:59,159 --> 00:06:05,909
physical suppose we only micro its value

109
00:06:02,789 --> 00:06:09,270
after migration the target first fetch

110
00:06:05,909 --> 00:06:12,300
key with remote success and then get

111
00:06:09,270 --> 00:06:15,389
value with local access this improves

112
00:06:12,300 --> 00:06:18,419
performance by reducing remote value and

113
00:06:15,389 --> 00:06:22,139
the migration granularity is in key

114
00:06:18,419 --> 00:06:25,469
value pairs further we notice that the

115
00:06:22,139 --> 00:06:28,919
location of the location of key is not

116
00:06:25,469 --> 00:06:31,919
changed as the key is not migrated so

117
00:06:28,919 --> 00:06:34,859
there's no need of metadata to maintain

118
00:06:31,919 --> 00:06:37,380
the latest location in short this

119
00:06:34,860 --> 00:06:42,029
achieved zero metadata fine-grained

120
00:06:37,380 --> 00:06:44,370
migration our system proof uses split

121
00:06:42,029 --> 00:06:46,949
live migration which separates lame

122
00:06:44,370 --> 00:06:49,990
migrate values and does not migrate keys

123
00:06:46,949 --> 00:06:53,320
these are voice remote values

124
00:06:49,990 --> 00:06:55,210
it's maximizes migration benefits using

125
00:06:53,320 --> 00:06:57,940
this zero metadata fine-grained

126
00:06:55,210 --> 00:07:01,180
migration the evaluation shows the

127
00:06:57,940 --> 00:07:04,210
effectiveness beyond this proof also

128
00:07:01,180 --> 00:07:06,640
support other functions such as sauce

129
00:07:04,210 --> 00:07:09,070
bypassing migration fully localized

130
00:07:06,640 --> 00:07:11,409
migration light with fine-grained

131
00:07:09,070 --> 00:07:15,700
monitor and migration own evolving

132
00:07:11,410 --> 00:07:18,640
graphs before we dig into perhaps detail

133
00:07:15,700 --> 00:07:21,070
let me briefly introduce profs system

134
00:07:18,640 --> 00:07:24,430
model each server contains three

135
00:07:21,070 --> 00:07:27,849
components the storage layer adopts an

136
00:07:24,430 --> 00:07:31,030
RMA friendly key value store the target

137
00:07:27,850 --> 00:07:33,780
and task engine handles requests from

138
00:07:31,030 --> 00:07:37,719
the clients by key value operations and

139
00:07:33,780 --> 00:07:41,250
prof add a migration tool kit which

140
00:07:37,720 --> 00:07:44,550
enables monitor to collect statics and

141
00:07:41,250 --> 00:07:47,920
migrations res to do live migration and

142
00:07:44,550 --> 00:07:50,740
here is the outline of the remaining

143
00:07:47,920 --> 00:07:53,110
content due to time limit I will

144
00:07:50,740 --> 00:07:56,490
introduce these three techniques we

145
00:07:53,110 --> 00:08:00,070
start by unilateral migration protocol

146
00:07:56,490 --> 00:08:02,890
it is important to prevent the source

147
00:08:00,070 --> 00:08:05,650
node from interruption as the source

148
00:08:02,890 --> 00:08:08,979
node may be busy processing concurrent

149
00:08:05,650 --> 00:08:13,020
query the unilateral migration protocol

150
00:08:08,980 --> 00:08:15,850
totally migrates data by the target it

151
00:08:13,020 --> 00:08:19,349
leverages a DMA because Adam a

152
00:08:15,850 --> 00:08:22,960
primitives totally bypass the sauce CPU

153
00:08:19,350 --> 00:08:25,480
although anime is CPU bypassing it is

154
00:08:22,960 --> 00:08:28,599
challenging to use it for migration due

155
00:08:25,480 --> 00:08:32,230
to its week's metrics prof carefully

156
00:08:28,600 --> 00:08:37,210
rehearsals read write and compare and

157
00:08:32,230 --> 00:08:40,270
swap operations in detail is a five-step

158
00:08:37,210 --> 00:08:43,420
protocol to migrate value and updates

159
00:08:40,270 --> 00:08:46,720
address in a key first the target

160
00:08:43,419 --> 00:08:50,800
allocate memory and then copies value by

161
00:08:46,720 --> 00:08:55,690
read search system key is not migrated

162
00:08:50,800 --> 00:08:59,079
the address is remote for a target prof

163
00:08:55,690 --> 00:09:01,769
uses atomic updates by atom a compound

164
00:08:59,080 --> 00:09:04,850
swap which can Co work with

165
00:09:01,769 --> 00:09:06,869
parent queries on source to make

166
00:09:04,850 --> 00:09:10,619
compare-and-swap work

167
00:09:06,869 --> 00:09:14,489
the address is carefully encoded into 64

168
00:09:10,619 --> 00:09:17,879
bits by hybrid encoding finally the

169
00:09:14,489 --> 00:09:20,850
target releases remote memory it adopts

170
00:09:17,879 --> 00:09:23,549
passive invalidation and lease basically

171
00:09:20,850 --> 00:09:26,579
reclaimed mechanisms for more details

172
00:09:23,549 --> 00:09:30,299
you can refer to the paper this is the

173
00:09:26,579 --> 00:09:33,029
second technique with police migration

174
00:09:30,299 --> 00:09:36,299
the remote access is about half of the

175
00:09:33,029 --> 00:09:40,290
original however it is still far from

176
00:09:36,299 --> 00:09:43,379
optimal because prov uses additional

177
00:09:40,290 --> 00:09:45,449
remote access for the address this is a

178
00:09:43,379 --> 00:09:49,439
common challenge for anime based

179
00:09:45,449 --> 00:09:53,389
equivalents tall previous work used uses

180
00:09:49,439 --> 00:09:58,379
a location cache to reduce remote key in

181
00:09:53,389 --> 00:10:01,489
detail with location cache the target

182
00:09:58,379 --> 00:10:05,670
can get address of value locally and

183
00:10:01,489 --> 00:10:08,639
fetch value with one other may read our

184
00:10:05,670 --> 00:10:12,118
observation is that location cache is a

185
00:10:08,639 --> 00:10:14,939
perfect match for split migration the

186
00:10:12,119 --> 00:10:19,139
key reason is that the avoids half of

187
00:10:14,939 --> 00:10:21,480
remote access respectively besides they

188
00:10:19,139 --> 00:10:25,949
both want to remote data access

189
00:10:21,480 --> 00:10:28,740
frequently based on this observation we

190
00:10:25,949 --> 00:10:32,040
integrate splits migration with location

191
00:10:28,740 --> 00:10:36,059
cash to avoid remote successes with

192
00:10:32,040 --> 00:10:39,748
cache after migration both key and value

193
00:10:36,059 --> 00:10:43,410
are local so it is fully local access

194
00:10:39,749 --> 00:10:46,410
now although location cache avoids

195
00:10:43,410 --> 00:10:50,899
remote keys is complicates consistency

196
00:10:46,410 --> 00:10:54,118
of migration finally I will introduce

197
00:10:50,899 --> 00:10:56,669
lightweight monitor which enables fast

198
00:10:54,119 --> 00:11:00,149
detection of migration requirements

199
00:10:56,669 --> 00:11:03,299
according adieu to workload changes the

200
00:11:00,149 --> 00:11:06,720
function of monitor is to collect access

201
00:11:03,299 --> 00:11:09,720
statics so that the coordinator can make

202
00:11:06,720 --> 00:11:12,230
migration plain perhaps find current

203
00:11:09,720 --> 00:11:14,700
migration though maximizes benefits

204
00:11:12,230 --> 00:11:17,010
making monitor challenging

205
00:11:14,700 --> 00:11:19,890
because the monitor should carry on

206
00:11:17,010 --> 00:11:24,780
perky granularity which has non-trivial

207
00:11:19,890 --> 00:11:27,630
overhead on large graphs prof uses a

208
00:11:24,780 --> 00:11:30,329
separate monitor to achieve light with

209
00:11:27,630 --> 00:11:33,780
fine grain monitor the observation is

210
00:11:30,330 --> 00:11:37,410
that my vision may only care about the

211
00:11:33,780 --> 00:11:40,680
remote data frequently accessed so prov

212
00:11:37,410 --> 00:11:44,219
only checks the remote success reducing

213
00:11:40,680 --> 00:11:47,849
location cache if necessary monitor will

214
00:11:44,220 --> 00:11:51,450
check local says on demand and there are

215
00:11:47,850 --> 00:11:53,790
some other techniques improv to show the

216
00:11:51,450 --> 00:11:56,850
effectiveness of prov with its

217
00:11:53,790 --> 00:12:00,660
evaluation on micro benchmark on a

218
00:11:56,850 --> 00:12:03,140
cluster of ethnos the ideal baseline is

219
00:12:00,660 --> 00:12:06,500
the throughput on one single note

220
00:12:03,140 --> 00:12:09,630
multiplied by 8 the number of nodes and

221
00:12:06,500 --> 00:12:14,160
the shared based migration has 800

222
00:12:09,630 --> 00:12:17,160
shares in total we notice that the

223
00:12:14,160 --> 00:12:19,860
throughput before migration is far from

224
00:12:17,160 --> 00:12:22,560
the ideal one then we compare different

225
00:12:19,860 --> 00:12:27,150
migration schemes chart based migration

226
00:12:22,560 --> 00:12:30,540
moves over 85% of data but increases the

227
00:12:27,150 --> 00:12:34,680
throughput by only 40 percent as a

228
00:12:30,540 --> 00:12:37,800
remote access rate is still 64% our

229
00:12:34,680 --> 00:12:41,579
speed split migration moves very few

230
00:12:37,800 --> 00:12:45,060
data with the basic split migration the

231
00:12:41,580 --> 00:12:48,870
throughput is improved by 90% since

232
00:12:45,060 --> 00:12:51,569
remote value is reached avoided split

233
00:12:48,870 --> 00:12:57,930
migration with location cache increases

234
00:12:51,570 --> 00:13:02,780
throughput by 19 times and enriches 70%

235
00:12:57,930 --> 00:13:11,459
of the ideal one this is because sorry

236
00:13:02,780 --> 00:13:15,540
this is because the over 90 90 this is

237
00:13:11,460 --> 00:13:18,530
because over 97% of remote access are

238
00:13:15,540 --> 00:13:18,530
reduced and

239
00:13:20,170 --> 00:13:32,029
sorry we also put in cone to use prov

240
00:13:28,390 --> 00:13:35,660
which is a graph store processing

241
00:13:32,029 --> 00:13:38,060
concurrent CQ like queries the results

242
00:13:35,660 --> 00:13:40,819
are similar to the one in our micro

243
00:13:38,060 --> 00:13:44,630
benchmark chart based approach has very

244
00:13:40,820 --> 00:13:48,440
weak migration benefits because of high

245
00:13:44,630 --> 00:13:50,600
force contention and our splits a basic

246
00:13:48,440 --> 00:13:54,050
splits migration increases performance

247
00:13:50,600 --> 00:13:56,570
by one and a half times and with

248
00:13:54,050 --> 00:14:00,560
location cache the throughput is finally

249
00:13:56,570 --> 00:14:04,040
improved by two and a half times as 88

250
00:14:00,560 --> 00:14:06,529
percent of remote success are reduced so

251
00:14:04,040 --> 00:14:09,430
here's the conclusion location cache a

252
00:14:06,529 --> 00:14:13,149
local locality is important and

253
00:14:09,430 --> 00:14:15,410
challenging for graph traversal

254
00:14:13,149 --> 00:14:18,950
traditional shared with migration does

255
00:14:15,410 --> 00:14:21,770
not fit graph traversal our system probe

256
00:14:18,950 --> 00:14:24,170
uses splits live migration is poor

257
00:14:21,770 --> 00:14:26,480
supports fine-grained migration with our

258
00:14:24,170 --> 00:14:29,810
messages our overhead and our aversion

259
00:14:26,480 --> 00:14:39,880
proves the effectiveness thank you for

260
00:14:29,810 --> 00:14:39,880
listening we have time for two questions

261
00:14:43,980 --> 00:14:52,710
I said no unist uh can you say a little

262
00:14:50,880 --> 00:14:55,370
bit more about the coherency problem

263
00:14:52,710 --> 00:14:57,930
that the issue that you mentioned use

264
00:14:55,370 --> 00:15:02,960
how it affects performance and all that

265
00:14:57,930 --> 00:15:07,380
the coherency you did you mean the

266
00:15:02,960 --> 00:15:10,050
concurrency of a location cache yes yeah

267
00:15:07,380 --> 00:15:12,840
the coherency actually the coherency

268
00:15:10,050 --> 00:15:15,060
because you bring the key into local

269
00:15:12,840 --> 00:15:24,540
line you make a copy of it don't you

270
00:15:15,060 --> 00:15:28,589
yeah yes yes a location has brings an

271
00:15:24,540 --> 00:15:32,069
inconsistency problem and with and this

272
00:15:28,590 --> 00:15:37,020
is the common challenge for a location

273
00:15:32,070 --> 00:15:43,590
cache used in other systems so we reuse

274
00:15:37,020 --> 00:15:47,510
the the passive invalidation mechanism

275
00:15:43,590 --> 00:15:51,990
to solve this problem and also as our

276
00:15:47,510 --> 00:15:55,880
value R will be reclaimed

277
00:15:51,990 --> 00:15:59,400
so we also use these bases reclaim

278
00:15:55,880 --> 00:16:01,800
mechanism to solve this problem yeah and

279
00:15:59,400 --> 00:16:05,300
if you are interested we can discuss

280
00:16:01,800 --> 00:16:05,300
more offline

281
00:16:08,670 --> 00:16:18,510
we will concede how to deal with like

282
00:16:11,700 --> 00:16:21,680
loader balancing to me the Lord Peron

283
00:16:18,510 --> 00:16:24,810
see this means like for example actually

284
00:16:21,680 --> 00:16:28,529
you you have force to the kind of nodes

285
00:16:24,810 --> 00:16:31,949
right there all those are hole keys so

286
00:16:28,529 --> 00:16:34,380
there are love like queries may be very

287
00:16:31,950 --> 00:16:38,430
soon who knows how to balance the nose

288
00:16:34,380 --> 00:16:41,760
make the whole query more scalable yeah

289
00:16:38,430 --> 00:16:46,170
on life migration can solve the problem

290
00:16:41,760 --> 00:16:49,829
of load balancing also but our work is

291
00:16:46,170 --> 00:16:53,579
currently focus on and to improve the

292
00:16:49,829 --> 00:16:58,589
the locality but profit is also can be

293
00:16:53,579 --> 00:17:00,180
also used to load parents so but we

294
00:16:58,589 --> 00:17:03,660
can't use it

295
00:17:00,180 --> 00:17:05,379
now just yeah all right let's thank the

296
00:17:03,660 --> 00:17:10,858
speaker

297
00:17:05,380 --> 00:17:10,859
[Applause]

