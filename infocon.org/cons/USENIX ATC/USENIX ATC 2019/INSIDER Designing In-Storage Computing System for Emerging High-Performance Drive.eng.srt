1
00:00:10,310 --> 00:00:15,200
okay um hi my name is Zane and today I'm

2
00:00:13,070 --> 00:00:16,520
going to present a work insider which is

3
00:00:15,200 --> 00:00:18,349
a in storage companion system for

4
00:00:16,520 --> 00:00:19,849
emerging high performance drive and this

5
00:00:18,349 --> 00:00:23,570
is a joint work with my collaborators

6
00:00:19,849 --> 00:00:24,770
Tom D and Jason come from UCLA story

7
00:00:23,570 --> 00:00:27,080
technology has been improved

8
00:00:24,770 --> 00:00:28,640
significantly in the last decade we

9
00:00:27,080 --> 00:00:30,259
witness Moore's law of destroyed drive

10
00:00:28,640 --> 00:00:31,059
that the drive bandwidth doubles every

11
00:00:30,260 --> 00:00:33,080
two years

12
00:00:31,059 --> 00:00:34,550
however meanwhile we found the

13
00:00:33,080 --> 00:00:36,410
performance of hosts driving the

14
00:00:34,550 --> 00:00:39,319
connections as no skill will as we saw

15
00:00:36,410 --> 00:00:40,940
in the figure the existence of this

16
00:00:39,320 --> 00:00:42,380
performance gap actually prevents us

17
00:00:40,940 --> 00:00:44,959
from leveraging the fast on the line

18
00:00:42,380 --> 00:00:48,500
drive make this advance story technology

19
00:00:44,960 --> 00:00:50,120
food tell to come back with this in

20
00:00:48,500 --> 00:00:51,829
connection bottleneck researchers

21
00:00:50,120 --> 00:00:53,989
proposing storage computing architecture

22
00:00:51,829 --> 00:00:56,510
in which hosts partially of loads

23
00:00:53,989 --> 00:00:58,370
computation to the storage rack one

24
00:00:56,510 --> 00:01:00,589
simple illustrative workload for this is

25
00:00:58,370 --> 00:01:02,269
sequel query so here we could offload

26
00:01:00,590 --> 00:01:04,100
the few doing part directly happening

27
00:01:02,270 --> 00:01:06,340
side of store drive to take advantage of

28
00:01:04,099 --> 00:01:10,520
the ample internal drive bandwidth and

29
00:01:06,340 --> 00:01:11,750
after that the data volume is reduced so

30
00:01:10,520 --> 00:01:13,429
that only less data needs to be

31
00:01:11,750 --> 00:01:14,659
transferred back to host therefore

32
00:01:13,430 --> 00:01:17,930
alleviating the interconnection

33
00:01:14,659 --> 00:01:20,270
bottleneck installed computing is a

34
00:01:17,930 --> 00:01:21,530
promising solution however design a full

35
00:01:20,270 --> 00:01:23,539
stack in stored computing systems

36
00:01:21,530 --> 00:01:25,520
challenging here we'll take a look at

37
00:01:23,540 --> 00:01:27,080
every layers of the season stack I will

38
00:01:25,520 --> 00:01:29,420
move on that existing work has different

39
00:01:27,080 --> 00:01:30,890
limitation differently and here I'm

40
00:01:29,420 --> 00:01:33,259
going to discuss them in a button-up

41
00:01:30,890 --> 00:01:35,030
direction so first in the hardware layer

42
00:01:33,259 --> 00:01:36,890
the insert computing unit has either

43
00:01:35,030 --> 00:01:38,689
limited performance of flexibility

44
00:01:36,890 --> 00:01:40,460
basically there are two types of one

45
00:01:38,689 --> 00:01:42,380
strand company you need and first time

46
00:01:40,460 --> 00:01:45,320
it's ARM based it is fully programmable

47
00:01:42,380 --> 00:01:47,240
and can Sun support generation computing

48
00:01:45,320 --> 00:01:49,158
however is completed commutation

49
00:01:47,240 --> 00:01:50,839
capabilities insufficient to saturate

50
00:01:49,159 --> 00:01:53,899
high internal drive bandwidth which

51
00:01:50,840 --> 00:01:55,759
Chris yet another system bottleneck the

52
00:01:53,899 --> 00:01:57,590
second high basic phase which is the

53
00:01:55,759 --> 00:01:58,220
customize hardware designed for specific

54
00:01:57,590 --> 00:02:00,049
workloads

55
00:01:58,220 --> 00:02:02,090
you can achieve very high performance on

56
00:02:00,049 --> 00:02:04,820
that were close however it is not

57
00:02:02,090 --> 00:02:06,649
programmable this means that after it is

58
00:02:04,820 --> 00:02:09,799
manufactured it can only support those

59
00:02:06,649 --> 00:02:11,510
workloads second in the system runtime

60
00:02:09,799 --> 00:02:13,549
some crucial system supports a missing

61
00:02:11,510 --> 00:02:14,590
making them less usable in a practical

62
00:02:13,549 --> 00:02:16,400
scenario

63
00:02:14,590 --> 00:02:18,410
specifically it has to support

64
00:02:16,400 --> 00:02:20,540
protection because we cannot really

65
00:02:18,410 --> 00:02:22,430
trust on the line of loaded program

66
00:02:20,540 --> 00:02:23,880
which may issue arbitrary drive

67
00:02:22,430 --> 00:02:25,920
accessing requests to access

68
00:02:23,880 --> 00:02:28,829
or even manipulate over the uh no threat

69
00:02:25,920 --> 00:02:30,450
drug data and the addition went to

70
00:02:28,830 --> 00:02:32,040
support virtualization sings in your

71
00:02:30,450 --> 00:02:33,750
Pratt seems in practical the store

72
00:02:32,040 --> 00:02:36,450
drivers shared among multiple users of

73
00:02:33,750 --> 00:02:38,340
processes and more importantly a single

74
00:02:36,450 --> 00:02:39,480
drive program may not fully utilize high

75
00:02:38,340 --> 00:02:41,220
internal drive bandwidth

76
00:02:39,480 --> 00:02:44,390
therefore it is preferable to co-locate

77
00:02:41,220 --> 00:02:47,609
multiple try program simultaneously

78
00:02:44,390 --> 00:02:50,100
finally in the program layer they lack

79
00:02:47,610 --> 00:02:52,140
of an effective obstruction existing

80
00:02:50,100 --> 00:02:53,730
work exposes extremely IP is which are

81
00:02:52,140 --> 00:02:55,070
not integrated where the existence is an

82
00:02:53,730 --> 00:02:57,420
interface

83
00:02:55,070 --> 00:03:00,150
therefore they require considerable host

84
00:02:57,420 --> 00:03:01,769
code modifications to address those

85
00:03:00,150 --> 00:03:04,230
issues we pose a new system called

86
00:03:01,770 --> 00:03:06,180
insider before we dive into system

87
00:03:04,230 --> 00:03:09,720
design let's first take a quick overview

88
00:03:06,180 --> 00:03:11,910
of approach first in the hardware layer

89
00:03:09,720 --> 00:03:13,320
inside our adopts FPGA which is a

90
00:03:11,910 --> 00:03:15,840
reconfigure architecture for both

91
00:03:13,320 --> 00:03:17,489
performance and flexibility and we were

92
00:03:15,840 --> 00:03:18,900
showing the evaluation that insider is

93
00:03:17,490 --> 00:03:21,180
able to achieve a major improvement in

94
00:03:18,900 --> 00:03:23,090
performance and cost efficiency compared

95
00:03:21,180 --> 00:03:25,350
with our amazing start companion system

96
00:03:23,090 --> 00:03:26,460
second to provide cruiser system

97
00:03:25,350 --> 00:03:28,260
supports like protection and

98
00:03:26,460 --> 00:03:29,820
virtualization in the system rom entire

99
00:03:28,260 --> 00:03:31,230
layer would build a separate control

100
00:03:29,820 --> 00:03:33,299
plane to enforce important system

101
00:03:31,230 --> 00:03:35,910
policies like permission check and

102
00:03:33,300 --> 00:03:37,740
resource scheduling and finally in the

103
00:03:35,910 --> 00:03:39,630
program layer we'll provide a file based

104
00:03:37,740 --> 00:03:41,640
abstraction for instruct computing we

105
00:03:39,630 --> 00:03:43,950
expose POSIX like ap is which are

106
00:03:41,640 --> 00:03:45,899
familiar to host users and in the

107
00:03:43,950 --> 00:03:49,109
evaluation we will show that much less

108
00:03:45,900 --> 00:03:50,850
programming effort is required okay

109
00:03:49,110 --> 00:03:53,280
after taking overview let's now take a

110
00:03:50,850 --> 00:03:54,840
look at the insider system design so

111
00:03:53,280 --> 00:03:56,550
choosing a proper instant computing you

112
00:03:54,840 --> 00:03:57,990
need this crucial photo versus an

113
00:03:56,550 --> 00:04:00,480
efficiency there are so many

114
00:03:57,990 --> 00:04:03,620
architecture candidates like GPU arm x86

115
00:04:00,480 --> 00:04:06,390
basic FPGA it seems hard make a decision

116
00:04:03,620 --> 00:04:08,220
to guide us lecture we first analyze the

117
00:04:06,390 --> 00:04:10,829
requirement of insert computing Union

118
00:04:08,220 --> 00:04:12,480
first it has to have program ability to

119
00:04:10,830 --> 00:04:15,180
support January start computing rather

120
00:04:12,480 --> 00:04:17,099
than just few specific applause second

121
00:04:15,180 --> 00:04:18,570
it has to have massive parallelism to

122
00:04:17,100 --> 00:04:20,370
saturate the high internal bandwidth

123
00:04:18,570 --> 00:04:23,700
otherwise it will become the bottleneck

124
00:04:20,370 --> 00:04:25,560
of the whole system finally each rod

125
00:04:23,700 --> 00:04:27,630
should have high energy efficiency

126
00:04:25,560 --> 00:04:29,490
that's because for device a original

127
00:04:27,630 --> 00:04:31,020
energy fission and we don't want to

128
00:04:29,490 --> 00:04:33,680
compromise that significantly after

129
00:04:31,020 --> 00:04:36,990
introducing in the insert computing unit

130
00:04:33,680 --> 00:04:37,770
based on those requirement we evaluate

131
00:04:36,990 --> 00:04:39,509
multi mode

132
00:04:37,770 --> 00:04:41,940
architecture candidates and the results

133
00:04:39,509 --> 00:04:44,129
are shown in this table compared with

134
00:04:41,940 --> 00:04:46,440
ASIC epi J sacrificed some performance

135
00:04:44,129 --> 00:04:48,030
and energy efficiency however it has

136
00:04:46,440 --> 00:04:50,909
much better program ability to support

137
00:04:48,030 --> 00:04:54,030
genuine stone computing compared with

138
00:04:50,909 --> 00:04:56,099
arm and x86 APG has better parallelism

139
00:04:54,030 --> 00:04:57,020
for performance and heiner higher energy

140
00:04:56,099 --> 00:04:59,310
efficiency

141
00:04:57,020 --> 00:05:00,750
screaming work laws like string matching

142
00:04:59,310 --> 00:05:02,759
has plenty of pipeline level parallelism

143
00:05:00,750 --> 00:05:05,310
which is supported by a PG but not by

144
00:05:02,759 --> 00:05:07,580
GPU any addition FPGA has better energy

145
00:05:05,310 --> 00:05:10,710
efficiency compared with GPU

146
00:05:07,580 --> 00:05:14,219
therefore in our system we adopt FPGA as

147
00:05:10,710 --> 00:05:16,080
I started competing even now let's apply

148
00:05:14,220 --> 00:05:18,000
it into our system I will first start

149
00:05:16,080 --> 00:05:19,650
with an initial set architecture and

150
00:05:18,000 --> 00:05:22,560
people refining it into the final

151
00:05:19,650 --> 00:05:24,690
architecture first let's put let's add

152
00:05:22,560 --> 00:05:26,610
program ability to the storage drive we

153
00:05:24,690 --> 00:05:28,319
add a PD shape into a storage controller

154
00:05:26,610 --> 00:05:31,740
which is able to perform installed

155
00:05:28,319 --> 00:05:33,840
computing competition to leverage the

156
00:05:31,740 --> 00:05:36,090
triode programming capability the hosts

157
00:05:33,840 --> 00:05:38,909
first offload computational logic spiral

158
00:05:36,090 --> 00:05:39,719
program in FPGA chip to retrieve the

159
00:05:38,909 --> 00:05:42,210
necessary data

160
00:05:39,719 --> 00:05:44,370
the driver program acts of issue drive

161
00:05:42,210 --> 00:05:46,198
accessing request which contains logical

162
00:05:44,370 --> 00:05:48,449
block addresses or AOPA synthroid

163
00:05:46,199 --> 00:05:50,520
the drive phone were then translated

164
00:05:48,449 --> 00:05:52,710
into physical block addresses or PBS in

165
00:05:50,520 --> 00:05:56,008
short and access the storage Union

166
00:05:52,710 --> 00:05:58,109
accordingly the response data is then

167
00:05:56,009 --> 00:06:00,690
read by the try program to performing

168
00:05:58,110 --> 00:06:02,370
stored computation and finally the

169
00:06:00,690 --> 00:06:05,460
output result is sent back to host

170
00:06:02,370 --> 00:06:08,580
through DMA so far the design seems to

171
00:06:05,460 --> 00:06:10,799
be fine however some crucial system

172
00:06:08,580 --> 00:06:13,289
supports are missing here first it lacks

173
00:06:10,800 --> 00:06:15,120
of protection since the offloaded drive

174
00:06:13,289 --> 00:06:17,460
programming to arbitrary Drive accessing

175
00:06:15,120 --> 00:06:20,159
crest-2 access or even manipulate over

176
00:06:17,460 --> 00:06:23,758
the author at drive data this can be

177
00:06:20,159 --> 00:06:25,830
exploited to compromise the system the

178
00:06:23,759 --> 00:06:27,419
root cause here is that our initial

179
00:06:25,830 --> 00:06:29,068
system picture only contains the data

180
00:06:27,419 --> 00:06:31,229
plane to perform in stored computation

181
00:06:29,069 --> 00:06:34,020
it does not have a control plane to

182
00:06:31,229 --> 00:06:35,940
enforce this and policy and an Insider

183
00:06:34,020 --> 00:06:37,049
we build a separate control plane to

184
00:06:35,940 --> 00:06:39,659
support those missing pieces and

185
00:06:37,050 --> 00:06:42,750
features first we are going to make this

186
00:06:39,659 --> 00:06:45,210
drive program compute only that means it

187
00:06:42,750 --> 00:06:49,440
is now low longer able to perform the

188
00:06:45,210 --> 00:06:51,688
direct drive access instead this is now

189
00:06:49,440 --> 00:06:53,429
handled by and they are newly added on

190
00:06:51,689 --> 00:06:56,249
chuckling let's not take a look at the

191
00:06:53,429 --> 00:06:58,378
refined design immediately after

192
00:06:56,249 --> 00:07:00,269
offloading step the host program will

193
00:06:58,379 --> 00:07:02,819
provide the file paths of the instruct

194
00:07:00,269 --> 00:07:04,919
installed computing input and here the

195
00:07:02,819 --> 00:07:06,659
host file system will enforce the file

196
00:07:04,919 --> 00:07:08,479
permission check and deny the

197
00:07:06,659 --> 00:07:11,308
unauthorized access

198
00:07:08,479 --> 00:07:12,959
after that the corresponding logic block

199
00:07:11,309 --> 00:07:16,369
addresses are sent to the drive firmware

200
00:07:12,959 --> 00:07:18,929
to issue the actual storage i/o request

201
00:07:16,369 --> 00:07:21,029
this steps I'm forced by our trusted

202
00:07:18,929 --> 00:07:23,429
insider runtime which serves as a

203
00:07:21,029 --> 00:07:27,209
centralized proxy among multiple run a

204
00:07:23,429 --> 00:07:28,948
multiple host programs we can fill the

205
00:07:27,209 --> 00:07:30,959
external control plane to support

206
00:07:28,949 --> 00:07:32,869
virtualization that is executing

207
00:07:30,959 --> 00:07:35,489
multiple drive programs simultaneously

208
00:07:32,869 --> 00:07:38,369
first and foremost we need huddle

209
00:07:35,489 --> 00:07:40,049
feature to support virtualization we

210
00:07:38,369 --> 00:07:42,300
leverage partial reconfiguration offered

211
00:07:40,050 --> 00:07:44,189
by modern FPGA chip to parse up to

212
00:07:42,300 --> 00:07:44,849
partition that PD resource evenly into

213
00:07:44,189 --> 00:07:47,959
three pieces

214
00:07:44,849 --> 00:07:50,369
this enables a multi-core FPGA and

215
00:07:47,959 --> 00:07:52,889
second we take back an authority for

216
00:07:50,369 --> 00:07:54,629
host program to perform a floating and

217
00:07:52,889 --> 00:07:56,749
now it is performed essentially by

218
00:07:54,629 --> 00:07:59,369
inside the run time across host programs

219
00:07:56,749 --> 00:08:06,300
however having these techniques allow is

220
00:07:59,369 --> 00:08:08,639
not sufficient in addition since now we

221
00:08:06,300 --> 00:08:10,559
need to do colocation it is important to

222
00:08:08,639 --> 00:08:13,019
enforce drive bandwidth scheduling among

223
00:08:10,559 --> 00:08:16,499
those running drive programs here we

224
00:08:13,019 --> 00:08:18,329
call them drive processes the drive

225
00:08:16,499 --> 00:08:20,099
process has different excluding rate at

226
00:08:18,329 --> 00:08:22,800
different time so the scheduler should

227
00:08:20,099 --> 00:08:24,269
be adaptive and the addition the

228
00:08:22,800 --> 00:08:26,339
scheduler should be fair to prevent a

229
00:08:24,269 --> 00:08:28,459
process from forcefully occupying the

230
00:08:26,339 --> 00:08:31,110
try bandwidth

231
00:08:28,459 --> 00:08:33,058
however we exit cannot intuitively do

232
00:08:31,110 --> 00:08:35,729
that at the host side inside a room time

233
00:08:33,058 --> 00:08:37,948
that's that's because it's too slow the

234
00:08:35,729 --> 00:08:40,199
PCIe round-trip time is 1 microsecond

235
00:08:37,948 --> 00:08:43,669
the slow reaction will store the drive

236
00:08:40,198 --> 00:08:45,660
processes and increase the buffer size

237
00:08:43,669 --> 00:08:47,519
therefore an insider

238
00:08:45,660 --> 00:08:49,410
we and we make the decision that

239
00:08:47,519 --> 00:08:51,839
partially offload the computation and

240
00:08:49,410 --> 00:08:54,709
sorry of load the control plane to FPGA

241
00:08:51,839 --> 00:08:58,410
to build a hardware based scheduler the

242
00:08:54,709 --> 00:08:59,819
scheduler accepts data from a subscribe

243
00:08:58,410 --> 00:09:01,709
data and dispatches it to the

244
00:08:59,819 --> 00:09:03,750
corresponding thread processes and by

245
00:09:01,709 --> 00:09:05,510
leveraging this dispatch information it

246
00:09:03,750 --> 00:09:07,770
is able to mind that the tribe and

247
00:09:05,510 --> 00:09:11,760
consumption of East Drive processor

248
00:09:07,770 --> 00:09:13,620
real-time the schedule then provide this

249
00:09:11,760 --> 00:09:15,839
feedback information to the film work to

250
00:09:13,620 --> 00:09:20,550
adjust Eastern rates of storage I will

251
00:09:15,839 --> 00:09:22,200
request accordingly for fairness we

252
00:09:20,550 --> 00:09:24,479
design a scheduling policy that is

253
00:09:22,200 --> 00:09:26,670
similar with a deficit round robin in

254
00:09:24,480 --> 00:09:31,110
networking q s and please refer to our

255
00:09:26,670 --> 00:09:33,180
paper for further details finally let's

256
00:09:31,110 --> 00:09:35,820
take a look at insiders program model to

257
00:09:33,180 --> 00:09:37,890
see how we use it the key idea here is

258
00:09:35,820 --> 00:09:40,110
to abstract insert computation at file

259
00:09:37,890 --> 00:09:43,439
operations which are familiar to host

260
00:09:40,110 --> 00:09:45,480
programmers the user can register a

261
00:09:43,440 --> 00:09:47,339
virtual file based on real fire and the

262
00:09:45,480 --> 00:09:49,529
corresponding uninstall computing

263
00:09:47,339 --> 00:09:51,180
program the data of the virtual file

264
00:09:49,529 --> 00:09:54,209
corresponds to the output processing

265
00:09:51,180 --> 00:09:56,399
result of the real file after the

266
00:09:54,209 --> 00:09:58,290
registration it brings user an illusion

267
00:09:56,399 --> 00:09:59,640
that versa fire actually exists in the

268
00:09:58,290 --> 00:10:04,410
file system and can be accessed

269
00:09:59,640 --> 00:10:06,990
throughout POSIX like api's this

270
00:10:04,410 --> 00:10:08,910
abstraction is simple but effective one

271
00:10:06,990 --> 00:10:09,360
simple example for this is machine

272
00:10:08,910 --> 00:10:11,160
learning

273
00:10:09,360 --> 00:10:12,930
so usually people were first apply

274
00:10:11,160 --> 00:10:15,089
feature selection algorithm to prove the

275
00:10:12,930 --> 00:10:17,790
real data and then fitting into the

276
00:10:15,089 --> 00:10:19,829
training algorithm like SVM and with

277
00:10:17,790 --> 00:10:22,439
virtual fire abstraction it is easy to

278
00:10:19,829 --> 00:10:24,239
implement an insider so first user will

279
00:10:22,440 --> 00:10:26,790
register a virtual file called post web

280
00:10:24,240 --> 00:10:29,970
based on the real fire and the feature

281
00:10:26,790 --> 00:10:31,560
selection program after that the user

282
00:10:29,970 --> 00:10:34,140
mostly doesn't need to change the

283
00:10:31,560 --> 00:10:36,689
existing SPM code he only need to pass

284
00:10:34,140 --> 00:10:38,220
the file path of this post file into SVM

285
00:10:36,690 --> 00:10:42,839
function and everything can work

286
00:10:38,220 --> 00:10:44,850
smoothly now after introducing the

287
00:10:42,839 --> 00:10:47,220
design of insider let's now go through

288
00:10:44,850 --> 00:10:49,980
the evaluation we buda installed

289
00:10:47,220 --> 00:10:53,310
computing on tribe using a PCIe based a

290
00:10:49,980 --> 00:10:55,800
PTO board the board has on this the to

291
00:10:53,310 --> 00:10:58,199
this table show the evaluation setup the

292
00:10:55,800 --> 00:11:00,930
drive has 64 gigabytes capacity

293
00:10:58,200 --> 00:11:03,750
5 microsecond access and latency 16

294
00:11:00,930 --> 00:11:06,420
gigabyte sequential bandwidth we conduct

295
00:11:03,750 --> 00:11:08,940
experiments under both PCIe gen3 x 8 and

296
00:11:06,420 --> 00:11:11,910
x 16 settings but in the following will

297
00:11:08,940 --> 00:11:16,380
only show the resulting x8 the host file

298
00:11:11,910 --> 00:11:18,449
system we use it as is x FS this table

299
00:11:16,380 --> 00:11:19,050
shows applications using the evaluation

300
00:11:18,449 --> 00:11:21,449
as well

301
00:11:19,050 --> 00:11:24,300
their development effort our code is

302
00:11:21,450 --> 00:11:26,700
written in C++ here on average each

303
00:11:24,300 --> 00:11:29,939
application takes tens of lines of host

304
00:11:26,700 --> 00:11:32,070
code and hundreds lines of tribe code we

305
00:11:29,940 --> 00:11:33,870
also showed the development on time in

306
00:11:32,070 --> 00:11:38,550
terms of person day which ranges from

307
00:11:33,870 --> 00:11:40,560
two to nine across applications to make

308
00:11:38,550 --> 00:11:42,990
a comparison we also show that the one

309
00:11:40,560 --> 00:11:44,640
effort of an existing work as we've

310
00:11:42,990 --> 00:11:46,920
shown in the figure it takes thousands

311
00:11:44,640 --> 00:11:49,380
of lines of code and over one person

312
00:11:46,920 --> 00:11:54,029
month on even for implementing simple

313
00:11:49,380 --> 00:11:55,500
operations next I'm going to compare

314
00:11:54,029 --> 00:11:57,480
Insider with the ARM based on

315
00:11:55,500 --> 00:12:00,540
convenience system by replacing the

316
00:11:57,480 --> 00:12:02,430
Xilinx FPGA chip into ARM Cortex a 7200

317
00:12:00,540 --> 00:12:05,819
other system components are remaining

318
00:12:02,430 --> 00:12:07,829
unchanged the figure showed the result

319
00:12:05,820 --> 00:12:09,690
of throughput comparison for ARM based

320
00:12:07,829 --> 00:12:11,729
solution we showed the performance of

321
00:12:09,690 --> 00:12:14,130
using one chord to course three cores

322
00:12:11,730 --> 00:12:15,750
and for course here we assume a perfect

323
00:12:14,130 --> 00:12:17,579
in the cord parallelism so the

324
00:12:15,750 --> 00:12:18,839
multi-core result is projected from the

325
00:12:17,579 --> 00:12:22,109
single core result simply by multiplying

326
00:12:18,839 --> 00:12:24,149
the number of course compared with the

327
00:12:22,110 --> 00:12:26,820
four core case that is orange bar here

328
00:12:24,149 --> 00:12:28,980
insider that is green bar is able to

329
00:12:26,820 --> 00:12:31,110
achieve 12x performance on average and

330
00:12:28,980 --> 00:12:32,910
the speed-up varies across applications

331
00:12:31,110 --> 00:12:36,540
because different application has

332
00:12:32,910 --> 00:12:38,610
different computational intensity in the

333
00:12:36,540 --> 00:12:43,560
most computing ten support load here and

334
00:12:38,610 --> 00:12:45,240
inside our chips 58 X performance single

335
00:12:43,560 --> 00:12:47,459
query here has the lowest computation

336
00:12:45,240 --> 00:12:51,450
intensities so Insider does not have any

337
00:12:47,459 --> 00:12:53,369
performance gain for this case in

338
00:12:51,450 --> 00:12:55,589
addition we show the maximal Drive

339
00:12:53,370 --> 00:12:58,380
bandwidth using this horizontal red bar

340
00:12:55,589 --> 00:12:59,940
and we can see that insider can actually

341
00:12:58,380 --> 00:13:01,980
saturate the tribe and waits for most

342
00:12:59,940 --> 00:13:06,990
cases thereby actually in the optimal

343
00:13:01,980 --> 00:13:09,750
performance next I'm going to compare

344
00:13:06,990 --> 00:13:12,360
their cost efficiency here it is defined

345
00:13:09,750 --> 00:13:14,310
by throughput per dollar for fair

346
00:13:12,360 --> 00:13:16,709
comparison we showed the whole we used

347
00:13:14,310 --> 00:13:18,420
the wholesale price in the evaluation as

348
00:13:16,709 --> 00:13:20,880
a links are like seven fpga board takes

349
00:13:18,420 --> 00:13:21,449
sorry apd chip takes thirty seven

350
00:13:20,880 --> 00:13:23,939
dollars

351
00:13:21,449 --> 00:13:27,689
well aren't colleagues 872 takes ninety

352
00:13:23,940 --> 00:13:29,540
five dollars the results here are shown

353
00:13:27,690 --> 00:13:32,220
in the results are shown in this table

354
00:13:29,540 --> 00:13:33,030
insider is able to achieve cost

355
00:13:32,220 --> 00:13:36,480
efficiency

356
00:13:33,030 --> 00:13:38,910
from 2.5 x to 104 dynamics and on

357
00:13:36,480 --> 00:13:41,810
average Insider is able to shift 31 X

358
00:13:38,910 --> 00:13:41,810
cause efficiency

359
00:13:42,260 --> 00:13:47,310
besides our paper offer more details on

360
00:13:44,970 --> 00:13:50,550
evaluation this including cider verses

361
00:13:47,310 --> 00:13:52,290
original host only architecture analysis

362
00:13:50,550 --> 00:13:54,900
of a PD resource utilization of

363
00:13:52,290 --> 00:13:56,550
implemented applications and the

364
00:13:54,900 --> 00:13:59,010
evaluation which insiders spend with

365
00:13:56,550 --> 00:14:03,900
scheduler and so on so definitely check

366
00:13:59,010 --> 00:14:06,720
out paper for further details finally

367
00:14:03,900 --> 00:14:08,610
let me conclude the talk in this talk we

368
00:14:06,720 --> 00:14:11,010
observe a data movement war between host

369
00:14:08,610 --> 00:14:13,260
and a host and drive it prevents end

370
00:14:11,010 --> 00:14:16,110
user on from leveraging the advancing

371
00:14:13,260 --> 00:14:17,790
story technology to cross this wall

372
00:14:16,110 --> 00:14:20,210
would present a new system coding

373
00:14:17,790 --> 00:14:23,610
insider it achieved three properties

374
00:14:20,210 --> 00:14:26,880
first insider achieves high end to end

375
00:14:23,610 --> 00:14:28,620
performance and cost efficiency second

376
00:14:26,880 --> 00:14:30,960
insider expose a simple but effective

377
00:14:28,620 --> 00:14:33,710
abstraction for installed computing to

378
00:14:30,960 --> 00:14:36,450
reduce the host side programming effort

379
00:14:33,710 --> 00:14:38,850
finally its controlling design enables

380
00:14:36,450 --> 00:14:41,220
protection and virtualization for shared

381
00:14:38,850 --> 00:14:53,460
as a clean environment with this I'm

382
00:14:41,220 --> 00:14:57,900
happy to take questions hi Emerson

383
00:14:53,460 --> 00:15:00,000
Technion a great work I wondered to two

384
00:14:57,900 --> 00:15:02,670
questions actually first where does the

385
00:15:00,000 --> 00:15:05,520
difference in the source code the amount

386
00:15:02,670 --> 00:15:07,860
of source code between Willow and and

387
00:15:05,520 --> 00:15:09,990
your work come from and this number one

388
00:15:07,860 --> 00:15:12,150
and number two hey I wonder if you have

389
00:15:09,990 --> 00:15:14,940
the evaluation of this workloads

390
00:15:12,150 --> 00:15:17,250
compared with regular CPUs not in

391
00:15:14,940 --> 00:15:19,740
storage yeah and so for the first

392
00:15:17,250 --> 00:15:21,470
question because on the willow is not

393
00:15:19,740 --> 00:15:24,090
open source so we actually could not on

394
00:15:21,470 --> 00:15:26,070
get it try to implement our clothes so

395
00:15:24,090 --> 00:15:27,930
actually we just think the experience

396
00:15:26,070 --> 00:15:30,300
out from their paper and try to make a

397
00:15:27,930 --> 00:15:32,760
comparison and our point is that on even

398
00:15:30,300 --> 00:15:34,979
put email for implementing like single

399
00:15:32,760 --> 00:15:36,720
simple operations like sync on simple I

400
00:15:34,980 --> 00:15:37,190
operation or file appending this kind of

401
00:15:36,720 --> 00:15:40,380
operation

402
00:15:37,190 --> 00:15:42,990
willow steel takes on a large on program

403
00:15:40,380 --> 00:15:45,270
effort but I I mean we cannot really

404
00:15:42,990 --> 00:15:46,420
make a comparison on our clothes because

405
00:15:45,270 --> 00:15:48,670
we cannot get a try

406
00:15:46,420 --> 00:15:50,860
we looked and for the second question so

407
00:15:48,670 --> 00:15:53,319
you're asking comparing with the

408
00:15:50,860 --> 00:15:55,720
host-only architecture balance I guess

409
00:15:53,320 --> 00:15:57,400
I'll actually on do two line timely time

410
00:15:55,720 --> 00:15:59,530
limitation we actually do not including

411
00:15:57,400 --> 00:16:01,750
the plantation but on the results having

412
00:15:59,530 --> 00:16:06,880
a paper so definitely you can check the

413
00:16:01,750 --> 00:16:10,540
paper for this thanks thanks for the

414
00:16:06,880 --> 00:16:13,300
great talk I'm moving from unist I was

415
00:16:10,540 --> 00:16:16,500
wondering if like you mentioned that you

416
00:16:13,300 --> 00:16:19,089
should have like extend the control

417
00:16:16,500 --> 00:16:21,160
control from terrace from the corner

418
00:16:19,090 --> 00:16:24,010
because like the former doesn't have any

419
00:16:21,160 --> 00:16:26,610
information trade related features but

420
00:16:24,010 --> 00:16:29,710
maybe you could have tried also to

421
00:16:26,610 --> 00:16:33,070
offload permission chance also to the

422
00:16:29,710 --> 00:16:35,290
FPGA which I think will like reduce

423
00:16:33,070 --> 00:16:37,240
Annie's latency and could improve the

424
00:16:35,290 --> 00:16:40,300
performance is there any reason why you

425
00:16:37,240 --> 00:16:41,530
like didn't try or you couldn't like

426
00:16:40,300 --> 00:16:43,839
off-road tools

427
00:16:41,530 --> 00:16:45,610
Parmesan cheese to the PG yeah that's a

428
00:16:43,840 --> 00:16:47,560
very good question so um if you really

429
00:16:45,610 --> 00:16:49,150
want to upload this part that PTA then

430
00:16:47,560 --> 00:16:51,010
basically you need to decode the

431
00:16:49,150 --> 00:16:53,140
filesystem metadata and it is actually

432
00:16:51,010 --> 00:16:54,490
difficult to achieve at APJ because the

433
00:16:53,140 --> 00:16:56,770
implementation would requires lots of

434
00:16:54,490 --> 00:16:58,600
MPD resource and also it is hard to UM

435
00:16:56,770 --> 00:17:00,760
be compatible across on host file

436
00:16:58,600 --> 00:17:02,590
systems so here we make the decision

437
00:17:00,760 --> 00:17:04,300
pile in Forth data on whole side by

438
00:17:02,590 --> 00:17:06,220
leveraging the knowledge of host file

439
00:17:04,300 --> 00:17:09,129
system and because this is more like a

440
00:17:06,220 --> 00:17:10,420
one-time affair so big on just start at

441
00:17:09,130 --> 00:17:11,829
the beginning of the computation you

442
00:17:10,420 --> 00:17:14,079
check the permission and later on you

443
00:17:11,829 --> 00:17:17,709
don't need to do that anymore so the

444
00:17:14,079 --> 00:17:22,020
overhead is actually minimal I see thank

445
00:17:17,709 --> 00:17:22,020
you thanks let's second speak again

446
00:17:22,770 --> 00:17:24,829
you

