1
00:00:10,309 --> 00:00:16,189
hi I am reading from the Vista lab of

2
00:00:13,459 --> 00:00:18,040
Arizona State University this is the

3
00:00:16,189 --> 00:00:20,630
presentation for smartdedupe

4
00:00:18,040 --> 00:00:22,009
optimizing deduplication for resource

5
00:00:20,630 --> 00:00:24,410
constrained devices

6
00:00:22,009 --> 00:00:26,779
this work is collaborated with my

7
00:00:24,410 --> 00:00:31,009
colleague Surya and my professor dr.

8
00:00:26,779 --> 00:00:33,980
Ming Jha smart DD focuses on resource

9
00:00:31,009 --> 00:00:37,129
management on resource constrained

10
00:00:33,980 --> 00:00:39,680
devices h and iot devices have two

11
00:00:37,129 --> 00:00:41,989
concerning characteristics the data

12
00:00:39,680 --> 00:00:44,899
intensive workloads and the limited on

13
00:00:41,989 --> 00:00:46,608
device storage the large data presented

14
00:00:44,899 --> 00:00:49,370
by the on device sensors and

15
00:00:46,609 --> 00:00:51,350
applications has presented serious

16
00:00:49,370 --> 00:00:54,320
challenges to the limited on device

17
00:00:51,350 --> 00:00:57,739
storage which has limited IO performance

18
00:00:54,320 --> 00:00:59,809
capacity and endurance deduplication

19
00:00:57,739 --> 00:01:02,769
might be the rescue to the above

20
00:00:59,809 --> 00:01:05,449
problems so what can deduplication do

21
00:01:02,769 --> 00:01:08,600
deduplication can eliminate redundant

22
00:01:05,449 --> 00:01:10,880
iOS and improve IO performance it can

23
00:01:08,600 --> 00:01:13,850
reduce flash writes and improve flash

24
00:01:10,880 --> 00:01:16,539
endurance it can also remove redundant

25
00:01:13,850 --> 00:01:19,158
data and improve resource utilization

26
00:01:16,540 --> 00:01:22,490
deduplication has been widely used in

27
00:01:19,159 --> 00:01:25,280
the cloud however to the deduplication

28
00:01:22,490 --> 00:01:27,500
in resource constrained devices we must

29
00:01:25,280 --> 00:01:30,049
answer two questions first

30
00:01:27,500 --> 00:01:31,159
is there enough data duplication in the

31
00:01:30,049 --> 00:01:34,310
device workloads

32
00:01:31,159 --> 00:01:36,560
and second how do we exploit the data

33
00:01:34,310 --> 00:01:40,039
duplicates using only the limited

34
00:01:36,560 --> 00:01:41,899
resources that the device has gladly

35
00:01:40,039 --> 00:01:44,899
smartdedupe has answered both questions

36
00:01:41,899 --> 00:01:47,180
and let's look at them one by one for

37
00:01:44,899 --> 00:01:49,310
the first question to verify that there

38
00:01:47,180 --> 00:01:52,399
is enough data duplicates on the device

39
00:01:49,310 --> 00:01:55,520
workloads we collected and analyzed long

40
00:01:52,399 --> 00:01:57,320
term IO traces up to six months from

41
00:01:55,520 --> 00:01:59,570
smart phones used by different users

42
00:01:57,320 --> 00:02:02,449
this table shows some general

43
00:01:59,570 --> 00:02:05,029
information about the traces from the

44
00:02:02,450 --> 00:02:07,070
table you can see that first real-world

45
00:02:05,030 --> 00:02:09,679
device workload is indeed quite

46
00:02:07,070 --> 00:02:11,989
intensive with every day I own volume up

47
00:02:09,679 --> 00:02:14,600
to 16 gigabytes and every day right

48
00:02:11,990 --> 00:02:17,060
volume up to 2 point 8 gigabytes and

49
00:02:14,600 --> 00:02:20,269
they do have an impact on the storage

50
00:02:17,060 --> 00:02:22,489
performance and endurance seconds we

51
00:02:20,269 --> 00:02:23,990
also find that our clothes have a good

52
00:02:22,489 --> 00:02:26,480
level of duplicates

53
00:02:23,990 --> 00:02:31,190
you can see the duplication ratio is up

54
00:02:26,480 --> 00:02:32,959
to 47% for the entire trace to

55
00:02:31,190 --> 00:02:36,230
understand the sources of the duplicates

56
00:02:32,960 --> 00:02:38,840
we conducted a more in-depth analysis we

57
00:02:36,230 --> 00:02:41,540
categorized files into five categories

58
00:02:38,840 --> 00:02:44,000
and analyzed which category contributes

59
00:02:41,540 --> 00:02:45,530
the most amount of duplicates these two

60
00:02:44,000 --> 00:02:47,630
graphs here showed the top three

61
00:02:45,530 --> 00:02:50,360
duplicate contributors for each trace

62
00:02:47,630 --> 00:02:53,240
the top graph uses the percentage of

63
00:02:50,360 --> 00:02:55,790
rice to each category as a y-axis and

64
00:02:53,240 --> 00:02:58,190
the category name as the x-axis the

65
00:02:55,790 --> 00:03:03,109
bottom graph uses the deduplication

66
00:02:58,190 --> 00:03:05,990
ratio for each category as the y-axis in

67
00:03:03,110 --> 00:03:07,550
this graph for example for trace Y in

68
00:03:05,990 --> 00:03:09,110
the yellow color resource file

69
00:03:07,550 --> 00:03:09,860
contributes the most amount of

70
00:03:09,110 --> 00:03:12,440
duplicates

71
00:03:09,860 --> 00:03:15,020
however for trace 2 in the blue color

72
00:03:12,440 --> 00:03:18,620
temporary file as the top duplicate

73
00:03:15,020 --> 00:03:21,980
contributor so you see duplicate sources

74
00:03:18,620 --> 00:03:24,080
differ by devices we also conducted file

75
00:03:21,980 --> 00:03:26,090
level deduplication and found that more

76
00:03:24,080 --> 00:03:29,120
than 80 percent of duplicates are from

77
00:03:26,090 --> 00:03:31,850
files that are not entirely duplicate so

78
00:03:29,120 --> 00:03:34,070
we conclude a system level deduplication

79
00:03:31,850 --> 00:03:36,820
across files and applications is

80
00:03:34,070 --> 00:03:40,220
necessary to remove all the duplicates

81
00:03:36,820 --> 00:03:43,100
however there are still challenges first

82
00:03:40,220 --> 00:03:44,540
the device has limited memory and we may

83
00:03:43,100 --> 00:03:47,540
not have enough memory to store

84
00:03:44,540 --> 00:03:50,239
fingerprints to quickly locate to

85
00:03:47,540 --> 00:03:52,519
quickly detect duplicates and second a

86
00:03:50,240 --> 00:03:54,950
storage has limited performance and

87
00:03:52,520 --> 00:03:57,650
endurance while deduplication incurs

88
00:03:54,950 --> 00:04:00,859
additional metadata operations and third

89
00:03:57,650 --> 00:04:03,320
many smart devices nowadays have limited

90
00:04:00,860 --> 00:04:05,570
power and energy capacity while

91
00:04:03,320 --> 00:04:08,870
fingerprinting of deduplication consumes

92
00:04:05,570 --> 00:04:11,390
quite a bit of power our design

93
00:04:08,870 --> 00:04:14,030
smartdedupe a smart deduplication

94
00:04:11,390 --> 00:04:17,779
solution for smart devices has addressed

95
00:04:14,030 --> 00:04:20,120
the above challenges first to achieve a

96
00:04:17,779 --> 00:04:22,700
good level of deduplication ratio

97
00:04:20,120 --> 00:04:24,890
without using too much memory we design

98
00:04:22,700 --> 00:04:27,260
a small in memory fingerprint store and

99
00:04:24,890 --> 00:04:30,349
uses a large on this finger print store

100
00:04:27,260 --> 00:04:32,330
to complement it and second to improve

101
00:04:30,350 --> 00:04:34,490
storage performance and endurance

102
00:04:32,330 --> 00:04:37,099
deduplication should be applied as much

103
00:04:34,490 --> 00:04:37,580
as possible and we combine in line and

104
00:04:37,100 --> 00:04:39,349
out of

105
00:04:37,580 --> 00:04:42,139
my entire vacation together for thorough

106
00:04:39,349 --> 00:04:44,449
deduplication and third to further save

107
00:04:42,139 --> 00:04:46,759
resources we designed adaptive

108
00:04:44,449 --> 00:04:48,710
deduplication which can adaptively

109
00:04:46,759 --> 00:04:50,659
change the processing rate of T

110
00:04:48,710 --> 00:04:52,448
duplication based on resource

111
00:04:50,659 --> 00:04:55,490
availability and the workload

112
00:04:52,449 --> 00:04:58,039
characteristics the bottom graph shows

113
00:04:55,490 --> 00:05:00,430
the overall architecture of our design

114
00:04:58,039 --> 00:05:03,620
next I will give some details about

115
00:05:00,430 --> 00:05:05,659
smartdedupe first let's look at two

116
00:05:03,620 --> 00:05:07,280
level fingerprint stores a memory

117
00:05:05,659 --> 00:05:09,710
fingerprint store maintains only

118
00:05:07,280 --> 00:05:12,619
important fingerprints and fingerprints

119
00:05:09,710 --> 00:05:14,719
are indexed using a perfect tree prefix

120
00:05:12,620 --> 00:05:17,900
tree can index a group of fingerprints

121
00:05:14,719 --> 00:05:19,909
that share the same prefix indexing

122
00:05:17,900 --> 00:05:22,818
individual fingerprints takes too much

123
00:05:19,909 --> 00:05:25,039
space while indeed indexing a group of

124
00:05:22,819 --> 00:05:27,830
fingerprints can save a lot of memory or

125
00:05:25,039 --> 00:05:29,330
so the prefix tree can help support the

126
00:05:27,830 --> 00:05:31,520
two level fingerprint stores

127
00:05:29,330 --> 00:05:34,460
this graph shows our a memory

128
00:05:31,520 --> 00:05:36,799
fingerprint store design you can see the

129
00:05:34,460 --> 00:05:39,020
first 12 bits of the fingerprint are

130
00:05:36,800 --> 00:05:40,939
used at the index and all the

131
00:05:39,020 --> 00:05:42,919
fingerprints that share the same prefix

132
00:05:40,939 --> 00:05:44,750
are grouped together under the same leaf

133
00:05:42,919 --> 00:05:47,448
node in the in memory fingerprint group

134
00:05:44,750 --> 00:05:49,460
then let's look at the Una's fingerprint

135
00:05:47,449 --> 00:05:51,620
store on this fingerprint storm

136
00:05:49,460 --> 00:05:54,589
maintains fingerprints that are not in

137
00:05:51,620 --> 00:05:57,110
memory and it shares the same index with

138
00:05:54,589 --> 00:05:59,089
the in-memory fingerprint store this can

139
00:05:57,110 --> 00:06:01,430
help save memory and facilitates the

140
00:05:59,089 --> 00:06:04,009
fingerprint migration which will be

141
00:06:01,430 --> 00:06:06,199
introduced later and the starting pbn

142
00:06:04,009 --> 00:06:09,199
that stores the group of on this

143
00:06:06,199 --> 00:06:11,029
fingerprint is added to the leaf node to

144
00:06:09,199 --> 00:06:13,490
represent the unnies fingerprint group

145
00:06:11,029 --> 00:06:16,870
so in this way the leaf node points to

146
00:06:13,490 --> 00:06:19,009
both an e memory and an under spin

147
00:06:16,870 --> 00:06:21,529
fingerprints migrate between memory and

148
00:06:19,009 --> 00:06:23,210
disk on demand an in-memory finger for

149
00:06:21,529 --> 00:06:25,610
historic keeps the most recently used

150
00:06:23,210 --> 00:06:27,919
fingerprints a traditional way of

151
00:06:25,610 --> 00:06:30,080
evicting one fingerprint from memory to

152
00:06:27,919 --> 00:06:32,029
disk happens frequently whenever a

153
00:06:30,080 --> 00:06:34,639
fingerprint is not found in memory and

154
00:06:32,029 --> 00:06:37,610
takes too much I overhead to save this

155
00:06:34,639 --> 00:06:40,310
overhead we choose to evict fingerprint

156
00:06:37,610 --> 00:06:42,680
in groups let's see an example here

157
00:06:40,310 --> 00:06:45,319
suppose now the in memory fingerprint

158
00:06:42,680 --> 00:06:48,110
store is for and we evict a group of

159
00:06:45,319 --> 00:06:50,540
fingerprints to disk so in this way a

160
00:06:48,110 --> 00:06:52,250
number of slots have been freed

161
00:06:50,540 --> 00:06:54,350
and can be used to store many

162
00:06:52,250 --> 00:06:57,080
fingerprints from the future requests

163
00:06:54,350 --> 00:06:59,180
before the next eviction happens in this

164
00:06:57,080 --> 00:07:02,510
way the eviction frequency has been

165
00:06:59,180 --> 00:07:05,420
reduced and the Iowa overhead has been

166
00:07:02,510 --> 00:07:07,909
saved then let's look at the hybrid

167
00:07:05,420 --> 00:07:10,280
deduplication inline deduplication works

168
00:07:07,910 --> 00:07:13,760
in the i/o path and it can eliminate

169
00:07:10,280 --> 00:07:16,340
duplicate rights immediately however due

170
00:07:13,760 --> 00:07:21,140
to the limited resources and device we

171
00:07:16,340 --> 00:07:23,780
may not run inline deduplication at full

172
00:07:21,140 --> 00:07:26,180
speed out of line deduplication however

173
00:07:23,780 --> 00:07:28,309
works in the background and it can

174
00:07:26,180 --> 00:07:30,890
slowly but thoroughly eliminate all the

175
00:07:28,310 --> 00:07:32,570
duplicate data however it has limited

176
00:07:30,890 --> 00:07:34,490
improvement and performance and

177
00:07:32,570 --> 00:07:37,010
endurance compared to England

178
00:07:34,490 --> 00:07:38,900
deduplication so we choose to combine

179
00:07:37,010 --> 00:07:42,440
these two to make them work

180
00:07:38,900 --> 00:07:45,739
cooperatively together let's see how a

181
00:07:42,440 --> 00:07:47,900
right is handled here well a right first

182
00:07:45,740 --> 00:07:49,640
comes saying we will first fingerprints

183
00:07:47,900 --> 00:07:52,010
the request and then searches the

184
00:07:49,640 --> 00:07:54,919
in-memory fingerprint store if it is the

185
00:07:52,010 --> 00:07:57,650
miss we will further past the fing

186
00:07:54,920 --> 00:07:59,870
fingerprint or the request to out of

187
00:07:57,650 --> 00:08:01,669
line out of line deduplication and then

188
00:07:59,870 --> 00:08:04,040
searches the others fingerprint store

189
00:08:01,670 --> 00:08:05,720
for the fingerprint if it is found in

190
00:08:04,040 --> 00:08:08,810
the others fingerprint store it is then

191
00:08:05,720 --> 00:08:10,520
promoted to memory in case in memory

192
00:08:08,810 --> 00:08:12,820
fingerprint store gets full we will

193
00:08:10,520 --> 00:08:16,159
evict a group of fingerprints to disk

194
00:08:12,820 --> 00:08:18,890
then let's see how a read is handled the

195
00:08:16,160 --> 00:08:21,320
native read path of deduplication system

196
00:08:18,890 --> 00:08:24,020
cannot utilize the duplicate data in the

197
00:08:21,320 --> 00:08:26,420
page cache since page cache is indexed

198
00:08:24,020 --> 00:08:28,490
using lVN's and it does not know that

199
00:08:26,420 --> 00:08:31,790
different Albion's may contain the same

200
00:08:28,490 --> 00:08:33,740
data so it cannot avoid extra reads even

201
00:08:31,790 --> 00:08:36,770
if the same data is already in the page

202
00:08:33,740 --> 00:08:40,430
cache let's see an example here we have

203
00:08:36,770 --> 00:08:43,220
lb NX + lb NY both are deduplicated - PB

204
00:08:40,429 --> 00:08:47,150
NX and contain data data acts so we

205
00:08:43,220 --> 00:08:50,450
first read Albion X it will fetch the

206
00:08:47,150 --> 00:08:53,090
data from disk and read it into the page

207
00:08:50,450 --> 00:08:56,330
cache and then return the data then

208
00:08:53,090 --> 00:08:58,640
later on we want to read lb and y OB my

209
00:08:56,330 --> 00:09:00,890
also wants to read data ax

210
00:08:58,640 --> 00:09:02,100
however although data exists already in

211
00:09:00,890 --> 00:09:05,279
the page cache

212
00:09:02,100 --> 00:09:08,190
has to read from disk again and thus we

213
00:09:05,279 --> 00:09:10,800
have a duplicate wheel here so how do

214
00:09:08,190 --> 00:09:14,850
you how smart do you solve this problem

215
00:09:10,800 --> 00:09:17,008
we optimize the read path to you to

216
00:09:14,850 --> 00:09:20,220
utilize the duplicate data in the page

217
00:09:17,009 --> 00:09:22,529
cache we edit a page cache index in the

218
00:09:20,220 --> 00:09:25,440
page cache to map from perience

219
00:09:22,529 --> 00:09:27,750
to the corresponding pages so if a

220
00:09:25,440 --> 00:09:30,029
request looks for the data in the page

221
00:09:27,750 --> 00:09:32,819
cache and it is a miss it will further

222
00:09:30,029 --> 00:09:35,579
search the page cache index that's reuse

223
00:09:32,819 --> 00:09:37,800
our previous example so first we will

224
00:09:35,579 --> 00:09:38,399
read lb and X and everything remains the

225
00:09:37,800 --> 00:09:41,969
same

226
00:09:38,399 --> 00:09:45,420
unless this time we add the PB NX to

227
00:09:41,970 --> 00:09:48,660
data X mapping in our pipian based index

228
00:09:45,420 --> 00:09:51,779
so later on when we read lb and y we

229
00:09:48,660 --> 00:09:54,750
first search page cache and it is a Miss

230
00:09:51,779 --> 00:09:58,019
then we will further search the page

231
00:09:54,750 --> 00:10:00,149
cache index using the PB and X so this

232
00:09:58,019 --> 00:10:03,319
time it can directly read the data from

233
00:10:00,149 --> 00:10:07,139
page cache and we can save a read here

234
00:10:03,319 --> 00:10:09,899
to further save resources we design

235
00:10:07,139 --> 00:10:13,019
adaptive deduplication based on the

236
00:10:09,899 --> 00:10:15,480
resource availability smartdedupe

237
00:10:13,019 --> 00:10:18,630
can decrease the processing rate of

238
00:10:15,480 --> 00:10:22,589
deduplication to keep CPU and disk

239
00:10:18,630 --> 00:10:24,509
utilization under 100 percent or so it

240
00:10:22,589 --> 00:10:27,360
can decrease the processing rate based

241
00:10:24,509 --> 00:10:29,250
of deduplication proportionally to the

242
00:10:27,360 --> 00:10:32,189
available battery life and completely

243
00:10:29,250 --> 00:10:35,160
disables deduplication in low battery

244
00:10:32,189 --> 00:10:36,509
state based on the current duplication

245
00:10:35,160 --> 00:10:39,569
level of the workloads

246
00:10:36,509 --> 00:10:42,180
if the duplication level is decreasing

247
00:10:39,569 --> 00:10:45,689
smart didi will work less aggressively

248
00:10:42,180 --> 00:10:47,729
and gradually decreases the processing

249
00:10:45,689 --> 00:10:50,099
rate on the other hand if the

250
00:10:47,730 --> 00:10:52,459
duplication level is growing smartdedupe

251
00:10:50,100 --> 00:10:56,939
will work more aggressively and quickly

252
00:10:52,459 --> 00:11:00,388
restores the processing rate so far we

253
00:10:56,939 --> 00:11:03,029
have answered the second question on how

254
00:11:00,389 --> 00:11:06,180
to exploit data duplicates using only

255
00:11:03,029 --> 00:11:08,850
limited resources and to evaluate the

256
00:11:06,180 --> 00:11:11,969
effectiveness of our design we prototype

257
00:11:08,850 --> 00:11:15,130
smartdedupe on commonly used file system

258
00:11:11,970 --> 00:11:18,190
of smart devices ext4 and f2

259
00:11:15,130 --> 00:11:21,970
and we tested it on smartphones Nexus 5x

260
00:11:18,190 --> 00:11:24,399
and IOT platform Raspberry Pi 3 we use

261
00:11:21,970 --> 00:11:26,620
both benchmarks and real-world workloads

262
00:11:24,399 --> 00:11:28,990
and compared our work with

263
00:11:26,620 --> 00:11:32,920
state-of-the-art DG brocation solutions

264
00:11:28,990 --> 00:11:34,750
DMD new and say FTL dmdd is the block

265
00:11:32,920 --> 00:11:37,630
level deduplication solution which

266
00:11:34,750 --> 00:11:41,170
supports flexible metadata back ends

267
00:11:37,630 --> 00:11:44,410
well say FTL as a deduplication solution

268
00:11:41,170 --> 00:11:47,079
for resource constrained FTL layer as

269
00:11:44,410 --> 00:11:48,250
first look at the fr experiment on nexus

270
00:11:47,080 --> 00:11:50,860
using ext4

271
00:11:48,250 --> 00:11:54,040
this graph shows the right performance

272
00:11:50,860 --> 00:11:57,010
the y-axis is the throughput and the

273
00:11:54,040 --> 00:12:01,240
x-axis is the variation of percentage of

274
00:11:57,010 --> 00:12:02,020
duplicates from 0% to 100% in this graph

275
00:12:01,240 --> 00:12:04,720
smartdedupe

276
00:12:02,020 --> 00:12:06,490
is shown in the purple bar and the

277
00:12:04,720 --> 00:12:07,209
higher the better you can see

278
00:12:06,490 --> 00:12:09,700
smartdedupe

279
00:12:07,209 --> 00:12:12,390
always performs the best with different

280
00:12:09,700 --> 00:12:15,399
levels of duplicates for example with

281
00:12:12,390 --> 00:12:18,480
25% of duplicates smartdedupe can

282
00:12:15,399 --> 00:12:20,880
achieve 17% of right speed-up

283
00:12:18,480 --> 00:12:24,130
this graph shows the read performance

284
00:12:20,880 --> 00:12:25,810
you can see smart YouTube also achieves

285
00:12:24,130 --> 00:12:28,839
the best with different levels of

286
00:12:25,810 --> 00:12:33,099
duplicates thanks to our design of page

287
00:12:28,839 --> 00:12:35,670
cache index then let's look at smart

288
00:12:33,100 --> 00:12:38,829
deduced resource usage using fr allow

289
00:12:35,670 --> 00:12:41,110
experiments we makes different reads and

290
00:12:38,829 --> 00:12:43,779
writes to mimic the composition of

291
00:12:41,110 --> 00:12:46,089
real-world workloads the work clothes

292
00:12:43,779 --> 00:12:50,920
characteristics are shown in the table

293
00:12:46,089 --> 00:12:53,500
here this graph shows the CPU usage the

294
00:12:50,920 --> 00:12:56,770
y-axis is the CPU load and the x-axis

295
00:12:53,500 --> 00:12:59,350
shows the mix 1 and mix to work clothes

296
00:12:56,770 --> 00:13:02,199
smart Danube and exe for is shown in the

297
00:12:59,350 --> 00:13:04,720
purple bar and smart video on F 2 FS is

298
00:13:02,200 --> 00:13:07,300
shown in the orange bar in this graph

299
00:13:04,720 --> 00:13:10,270
the lower the better you can see smart

300
00:13:07,300 --> 00:13:13,479
YouTube has very small CPU overheads for

301
00:13:10,270 --> 00:13:16,029
example on a 3.3 percent compared to

302
00:13:13,480 --> 00:13:17,800
ext4 it makes one work clothes this

303
00:13:16,029 --> 00:13:20,620
graph shows the battery usage

304
00:13:17,800 --> 00:13:25,390
smartdedupe also performs perfectly and

305
00:13:20,620 --> 00:13:27,290
it actually saves battery by 49 percent

306
00:13:25,390 --> 00:13:29,720
compared to F 2 FF

307
00:13:27,290 --> 00:13:35,709
and all these experiments are conducted

308
00:13:29,720 --> 00:13:35,710
using only 3.5 megabytes of memory

309
00:13:35,740 --> 00:13:41,210
beside the fire experiment we also used

310
00:13:38,810 --> 00:13:42,319
real-world workloads to understand how

311
00:13:41,210 --> 00:13:45,680
smart didi works

312
00:13:42,320 --> 00:13:46,760
we replayed segments of traces on Nexus

313
00:13:45,680 --> 00:13:49,010
using ext4

314
00:13:46,760 --> 00:13:49,910
this table shows the trace segments

315
00:13:49,010 --> 00:13:52,939
information

316
00:13:49,910 --> 00:13:56,990
this graph shows smarty dips performance

317
00:13:52,940 --> 00:13:59,390
compared to ext for the wipe the y-axis

318
00:13:56,990 --> 00:14:03,470
shows the percentage of deduplication

319
00:13:59,390 --> 00:14:05,510
ratio rice PETA Storage saving and write

320
00:14:03,470 --> 00:14:08,330
reduction of smart YouTube compared to

321
00:14:05,510 --> 00:14:10,939
exe 4 and the x-axis shows different

322
00:14:08,330 --> 00:14:12,620
trace segments in this graph the higher

323
00:14:10,940 --> 00:14:17,240
the better and smarter you've achieved

324
00:14:12,620 --> 00:14:21,140
up to 51% of rice PETA and 70% of right

325
00:14:17,240 --> 00:14:24,200
reduction finally let's see how adaptive

326
00:14:21,140 --> 00:14:27,110
deduplication performs we evaluated

327
00:14:24,200 --> 00:14:30,410
adaptive deduplication on Nexus using

328
00:14:27,110 --> 00:14:32,690
ext4 by replaying trace segments we show

329
00:14:30,410 --> 00:14:35,000
the result of adaptive deduplication on

330
00:14:32,690 --> 00:14:37,700
the based on the workloads

331
00:14:35,000 --> 00:14:40,790
characteristics here this graph shows

332
00:14:37,700 --> 00:14:43,430
the results the y-axis is the power

333
00:14:40,790 --> 00:14:45,640
consumption and the x axis is the time

334
00:14:43,430 --> 00:14:49,699
changes during the trace replay

335
00:14:45,640 --> 00:14:52,189
smartdedupe is shown in the blue line

336
00:14:49,700 --> 00:14:54,920
and smart DD with adaptive deduplication

337
00:14:52,190 --> 00:14:57,350
as shown in the brown line and ext4 is

338
00:14:54,920 --> 00:14:59,569
shown in the red line in this graph the

339
00:14:57,350 --> 00:15:02,510
lower the better and smartdedupe with

340
00:14:59,570 --> 00:15:05,840
adaptive deduplication saves 14% of

341
00:15:02,510 --> 00:15:08,000
power overhead with only 8% of loss in

342
00:15:05,840 --> 00:15:11,630
deduplication ratio compared to the

343
00:15:08,000 --> 00:15:14,870
original smartdedupe to conclude

344
00:15:11,630 --> 00:15:18,350
deduplication is of importance to smart

345
00:15:14,870 --> 00:15:20,990
devices and it can actually save improve

346
00:15:18,350 --> 00:15:24,410
performance resource utilization and

347
00:15:20,990 --> 00:15:27,110
flash endurance and smart video presents

348
00:15:24,410 --> 00:15:31,130
a novel architectural design considering

349
00:15:27,110 --> 00:15:34,370
only limited resources and finally the

350
00:15:31,130 --> 00:15:37,670
trace are already publicly available at

351
00:15:34,370 --> 00:15:38,570
visa labs website you can check it out

352
00:15:37,670 --> 00:15:41,060
now

353
00:15:38,570 --> 00:15:44,090
and I want to acknowledge those who have

354
00:15:41,060 --> 00:15:45,859
helped with smartdedupe thanks to my

355
00:15:44,090 --> 00:15:48,700
colleagues and others who have helped

356
00:15:45,860 --> 00:15:51,770
with traces and images collection and

357
00:15:48,700 --> 00:15:54,590
finally welcome to our poster session

358
00:15:51,770 --> 00:15:57,319
tonight at 6:30 p.m. and communicate

359
00:15:54,590 --> 00:16:05,420
with me thanks for listening I am ready

360
00:15:57,320 --> 00:16:10,220
to take questions all right so remember

361
00:16:05,420 --> 00:16:12,319
to state your name and affiliation hi

362
00:16:10,220 --> 00:16:14,960
god I add girl from the Technion this is

363
00:16:12,320 --> 00:16:16,790
great work and I'm wondering if you had

364
00:16:14,960 --> 00:16:19,790
a chance to think about whether it could

365
00:16:16,790 --> 00:16:22,579
help also on non smartphone platforms

366
00:16:19,790 --> 00:16:24,920
because you seem to have outperformed

367
00:16:22,580 --> 00:16:26,630
other schemes that were designed for in

368
00:16:24,920 --> 00:16:29,180
device the duplication or system level

369
00:16:26,630 --> 00:16:31,100
the duplication so what's special about

370
00:16:29,180 --> 00:16:32,229
smartphones and would it work for other

371
00:16:31,100 --> 00:16:35,720
systems as well

372
00:16:32,230 --> 00:16:38,540
thanks for your question so as you can

373
00:16:35,720 --> 00:16:41,780
see with only very constrained resources

374
00:16:38,540 --> 00:16:45,199
we can already perform this this well so

375
00:16:41,780 --> 00:16:48,380
I believe that with other systems if

376
00:16:45,200 --> 00:16:51,350
they have more resources we can even

377
00:16:48,380 --> 00:16:53,360
perform better but we have not tried on

378
00:16:51,350 --> 00:16:57,070
them I think you should it looks

379
00:16:53,360 --> 00:16:57,070
promising thank you thank you

380
00:17:00,120 --> 00:17:09,029
any other questions hi so I have a

381
00:17:05,789 --> 00:17:12,359
question one of the things that you did

382
00:17:09,029 --> 00:17:14,490
to enable the the deduplication in these

383
00:17:12,359 --> 00:17:17,629
standard file systems is go from a 1 to

384
00:17:14,490 --> 00:17:19,859
1 lb a 2p be a mapping to a many-to-one

385
00:17:17,630 --> 00:17:22,439
how does that interact with some of the

386
00:17:19,859 --> 00:17:23,908
file systems allocation heuristics or

387
00:17:22,439 --> 00:17:26,130
say garbage collection have two FS do

388
00:17:23,909 --> 00:17:31,190
they still perform those heuristics at

389
00:17:26,130 --> 00:17:37,250
the LBA level for the garbage collection

390
00:17:31,190 --> 00:17:37,250
hmm I think they still perform the same

391
00:17:43,809 --> 00:17:47,500
Peter Mitsuko from that I'll say one

392
00:17:45,640 --> 00:17:49,210
more question okay to go like few slides

393
00:17:47,500 --> 00:17:55,240
back to our evaluation to the trace

394
00:17:49,210 --> 00:17:57,070
replay one more yeah this one so could

395
00:17:55,240 --> 00:17:59,170
you please add one thing I didn't get a

396
00:17:57,070 --> 00:18:02,439
school please explain why the storage

397
00:17:59,170 --> 00:18:06,460
savings is a lot less compared to like

398
00:18:02,440 --> 00:18:10,450
the your deduplication ratio oh that's

399
00:18:06,460 --> 00:18:13,929
because in these traces the writes are

400
00:18:10,450 --> 00:18:15,790
dominated by the updates to the existing

401
00:18:13,929 --> 00:18:19,270
data so it does not show obvious

402
00:18:15,790 --> 00:18:19,780
improvement in storage savings okay

403
00:18:19,270 --> 00:18:27,540
thank you

404
00:18:19,780 --> 00:18:27,540
mm-hmm any other questions

405
00:18:30,450 --> 00:18:37,350
okay I have one more so when you're

406
00:18:35,309 --> 00:18:39,418
trying to balance the the competing

407
00:18:37,350 --> 00:18:43,379
needs on the smart devices the endurance

408
00:18:39,419 --> 00:18:45,480
energy usage you do some some throttling

409
00:18:43,379 --> 00:18:47,998
and one of the things you do is you tune

410
00:18:45,480 --> 00:18:52,799
the selectivity of when you deduplicate

411
00:18:47,999 --> 00:18:54,450
in line how does that interact with some

412
00:18:52,799 --> 00:18:58,619
of the other optimizations you do like

413
00:18:54,450 --> 00:19:00,419
the the ID doop scheme of only

414
00:18:58,619 --> 00:19:03,090
deduplicating if you have some number of

415
00:19:00,419 --> 00:19:04,440
consecutive blocks does does that help

416
00:19:03,090 --> 00:19:05,759
or hurt with locality or does that have

417
00:19:04,440 --> 00:19:10,799
any other interplays with your other

418
00:19:05,759 --> 00:19:16,139
heuristics yeah I think adaptive

419
00:19:10,799 --> 00:19:20,940
duplication if they involve the iddo

420
00:19:16,139 --> 00:19:23,070
mechanism and might hurt the it might

421
00:19:20,940 --> 00:19:26,039
hurt the locality in the work clothes I

422
00:19:23,070 --> 00:19:28,879
think it's just a trade-off in either

423
00:19:26,039 --> 00:19:33,769
you choose fragmentation or you either

424
00:19:28,879 --> 00:19:33,769
work towards the resource constraint

425
00:19:36,770 --> 00:19:39,920
all right let's thank our speaker one

426
00:19:38,420 --> 00:19:44,299
more time

427
00:19:39,920 --> 00:19:44,299
[Applause]

