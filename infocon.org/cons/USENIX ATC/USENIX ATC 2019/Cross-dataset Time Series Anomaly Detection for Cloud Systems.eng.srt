1
00:00:10,570 --> 00:00:14,800
without the know everyone again I'm

2
00:00:12,820 --> 00:00:15,520
honored to be the last one in this

3
00:00:14,800 --> 00:00:18,730
conference

4
00:00:15,520 --> 00:00:22,150
I hope last event has not means less

5
00:00:18,730 --> 00:00:26,439
important align of course I feel the

6
00:00:22,150 --> 00:00:28,779
work is very important so we see a lot

7
00:00:26,439 --> 00:00:32,920
of very exciting work about how to

8
00:00:28,780 --> 00:00:36,180
design new new systems it's a very hot

9
00:00:32,920 --> 00:00:39,070
cloud the system but how can we wrong

10
00:00:36,180 --> 00:00:42,400
wrong in around the current production

11
00:00:39,070 --> 00:00:45,309
system very reliably and with higher

12
00:00:42,400 --> 00:00:47,170
availability is also important all if

13
00:00:45,309 --> 00:00:52,059
not less important I think it is more

14
00:00:47,170 --> 00:00:55,330
important kind of research area so I'm

15
00:00:52,059 --> 00:01:00,220
new moon from Microsoft I mean as a team

16
00:00:55,330 --> 00:01:02,470
today today I present on behalf of my

17
00:01:00,220 --> 00:01:04,839
colleagues from Microsoft Research Asia

18
00:01:02,470 --> 00:01:08,440
we are the main contributor of this

19
00:01:04,839 --> 00:01:16,360
paper because of some other reasons we

20
00:01:08,440 --> 00:01:18,670
cannot be here so I will first talk

21
00:01:16,360 --> 00:01:21,360
about the background and the motivation

22
00:01:18,670 --> 00:01:25,150
why we need to do this thing and then we

23
00:01:21,360 --> 00:01:27,820
introduce I introduced our approach how

24
00:01:25,150 --> 00:01:30,130
can we leverage active and the transform

25
00:01:27,820 --> 00:01:34,270
learning transfer learning for anomaly

26
00:01:30,130 --> 00:01:36,789
detection or cloud the system so

27
00:01:34,270 --> 00:01:40,030
normally detection is a very general

28
00:01:36,790 --> 00:01:43,540
term it's just like I see oh you are

29
00:01:40,030 --> 00:01:47,350
sick or not when you when you see a

30
00:01:43,540 --> 00:01:49,660
doctor in the annual check there's many

31
00:01:47,350 --> 00:01:53,080
different type of dance of the anomaly

32
00:01:49,660 --> 00:01:55,960
detection but anyway it is very

33
00:01:53,080 --> 00:02:01,090
important to ensure high series of a

34
00:01:55,960 --> 00:02:03,250
reliability and availability of course

35
00:02:01,090 --> 00:02:06,550
the tackling on normal behaviour is just

36
00:02:03,250 --> 00:02:09,098
the first step of ensuring reliability

37
00:02:06,550 --> 00:02:11,980
and a very bit after anomaly detection

38
00:02:09,098 --> 00:02:14,560
you still need to do diagnosis road

39
00:02:11,980 --> 00:02:17,369
causing fixing and rawls the new

40
00:02:14,560 --> 00:02:21,189
features many other money

41
00:02:17,370 --> 00:02:22,290
so but if we look at the diverse of the

42
00:02:21,189 --> 00:02:26,450
series we are running

43
00:02:22,290 --> 00:02:30,689
the infrastructure as a service platform

44
00:02:26,450 --> 00:02:35,730
as a service software as a service and

45
00:02:30,689 --> 00:02:38,879
many more we are a very diverse and the

46
00:02:35,730 --> 00:02:41,518
skill of the cloud service that's higher

47
00:02:38,879 --> 00:02:44,939
and higher and the capacity also gets

48
00:02:41,519 --> 00:02:47,489
higher so you in such kind of context

49
00:02:44,939 --> 00:02:49,709
how can we solve the anomaly detection

50
00:02:47,489 --> 00:02:52,469
problem which is very common and today

51
00:02:49,709 --> 00:02:56,629
we we do not have one solution and

52
00:02:52,469 --> 00:03:00,540
there's many many challenges in practice

53
00:02:56,629 --> 00:03:04,578
so our goal is to add to have an

54
00:03:00,540 --> 00:03:04,578
accurate anomaly detection approach

55
00:03:04,700 --> 00:03:13,619
ideally low labeling cost for large

56
00:03:08,549 --> 00:03:15,329
skill cloud service monitoring and there

57
00:03:13,620 --> 00:03:17,549
are many different type of key

58
00:03:15,329 --> 00:03:22,889
performance indicators we need to

59
00:03:17,549 --> 00:03:27,840
monitor every day 24 by 7 CPU memory is

60
00:03:22,889 --> 00:03:34,379
just some examples QPS we have done time

61
00:03:27,840 --> 00:03:37,799
there are many and many so why this is

62
00:03:34,379 --> 00:03:41,340
difficult I think the first reason is

63
00:03:37,799 --> 00:03:45,510
that the diverse catalytics of the

64
00:03:41,340 --> 00:03:50,220
system behaviour here we just give a few

65
00:03:45,510 --> 00:03:54,060
examples single specs multiple specs or

66
00:03:50,220 --> 00:03:57,540
long run specs increase the increase

67
00:03:54,060 --> 00:04:00,470
again all leaking memory leak like this

68
00:03:57,540 --> 00:04:03,659
this is just a very simple example

69
00:04:00,470 --> 00:04:08,720
there's more complicated patterns we can

70
00:04:03,659 --> 00:04:11,840
observe the is a real running system and

71
00:04:08,720 --> 00:04:16,139
of course for very simple naive

72
00:04:11,840 --> 00:04:18,870
threshold based alerting method it's

73
00:04:16,139 --> 00:04:24,690
very noisy and we call will be low and

74
00:04:18,870 --> 00:04:27,120
maintenance cost is high so we have many

75
00:04:24,690 --> 00:04:30,450
work about using own supervisor learning

76
00:04:27,120 --> 00:04:33,120
to do a better job than ievei proach our

77
00:04:30,450 --> 00:04:34,320
male model a solution forest twitter

78
00:04:33,120 --> 00:04:39,060
approach just

79
00:04:34,320 --> 00:04:43,020
few examples but the fundamental problem

80
00:04:39,060 --> 00:04:46,260
of the unsupervised learning a that we

81
00:04:43,020 --> 00:04:51,270
have some kind of true simple assumption

82
00:04:46,260 --> 00:04:53,460
about the system behavior but the system

83
00:04:51,270 --> 00:04:57,570
behavior is more much more complicated

84
00:04:53,460 --> 00:05:00,570
than the assumption so that's the reason

85
00:04:57,570 --> 00:05:05,159
we still have high false positive rate

86
00:05:00,570 --> 00:05:07,680
and high low recall but in practice we

87
00:05:05,160 --> 00:05:10,910
need ideally we need a high accuracy

88
00:05:07,680 --> 00:05:13,200
high recall if accuracy is not high

89
00:05:10,910 --> 00:05:15,650
developers will be good bori they're

90
00:05:13,200 --> 00:05:19,289
very very quickly and they will not use

91
00:05:15,650 --> 00:05:20,640
the tool but if the recoil is low which

92
00:05:19,290 --> 00:05:22,800
means that we cannot detect them any

93
00:05:20,640 --> 00:05:30,120
problems and the customer will feel pain

94
00:05:22,800 --> 00:05:32,520
and you will lose your business ok

95
00:05:30,120 --> 00:05:34,590
how about a supervisor learning approach

96
00:05:32,520 --> 00:05:40,260
then we have another type of our

97
00:05:34,590 --> 00:05:43,229
challenges how I got labels I I'm sure

98
00:05:40,260 --> 00:05:45,240
many of us know that if you ask a

99
00:05:43,230 --> 00:05:48,980
developers to do such kind of labeling

100
00:05:45,240 --> 00:05:48,980
seeing how difficult it is

101
00:05:51,170 --> 00:06:00,510
so of course lack of labels is biggest

102
00:05:54,780 --> 00:06:03,710
challenge here so we want to propose a

103
00:06:00,510 --> 00:06:07,890
new approach called

104
00:06:03,710 --> 00:06:08,880
adab language post transfer learning and

105
00:06:07,890 --> 00:06:12,719
active learning

106
00:06:08,880 --> 00:06:16,040
I think the key idea is very simple so

107
00:06:12,720 --> 00:06:20,100
in the public domain

108
00:06:16,040 --> 00:06:22,800
thanks for so these contributors we have

109
00:06:20,100 --> 00:06:26,360
some label data sites about the

110
00:06:22,800 --> 00:06:30,069
anomalies in some sample

111
00:06:26,360 --> 00:06:34,690
data sites from gel cloud service and

112
00:06:30,069 --> 00:06:38,449
these are very valuable information and

113
00:06:34,690 --> 00:06:41,599
because different type of cloud services

114
00:06:38,449 --> 00:06:44,180
do have some common catalyst occurs how

115
00:06:41,599 --> 00:06:46,069
can we live reduce the labels from other

116
00:06:44,180 --> 00:06:48,410
cloud service toppers are normally and

117
00:06:46,069 --> 00:06:51,639
use the full anomic attorney for some

118
00:06:48,410 --> 00:06:54,139
other cloud service this is a basic idea

119
00:06:51,639 --> 00:06:56,660
but we find out that trust accusing

120
00:06:54,139 --> 00:06:58,819
transfer learning is not you cannot get

121
00:06:56,660 --> 00:07:02,630
good enough result so that's reason we

122
00:06:58,819 --> 00:07:06,080
we further use active learning to add a

123
00:07:02,630 --> 00:07:08,599
few will be more human knowledge to

124
00:07:06,080 --> 00:07:12,800
improve the quality of the anomaly

125
00:07:08,599 --> 00:07:14,960
detection so this week this diagram show

126
00:07:12,800 --> 00:07:17,599
a very high-level architecture of the

127
00:07:14,960 --> 00:07:20,960
approach we have labeled a time time

128
00:07:17,599 --> 00:07:25,069
series data site on label decide and we

129
00:07:20,960 --> 00:07:26,930
have a transfer learning component and

130
00:07:25,069 --> 00:07:29,780
then we have a based detection model and

131
00:07:26,930 --> 00:07:32,389
then we further using active learning to

132
00:07:29,780 --> 00:07:37,690
improve the result that when cut is a

133
00:07:32,389 --> 00:07:41,870
final model so of course this is new a

134
00:07:37,690 --> 00:07:45,039
kind of supervisor learning approach the

135
00:07:41,870 --> 00:07:49,539
first step is to extract the features so

136
00:07:45,039 --> 00:07:51,409
to to to have a biter courage of

137
00:07:49,539 --> 00:07:54,669
detecting different type of the

138
00:07:51,409 --> 00:07:56,949
anomalous patterns we would like to

139
00:07:54,669 --> 00:08:00,650
instruct a very different type of

140
00:07:56,949 --> 00:08:02,300
features from time series data at high

141
00:08:00,650 --> 00:08:05,590
level there are three different types of

142
00:08:02,300 --> 00:08:10,069
features we extract statistical feature

143
00:08:05,590 --> 00:08:12,070
very obvious we need to look at the in

144
00:08:10,069 --> 00:08:14,380
the posture

145
00:08:12,070 --> 00:08:18,099
compute that what kind of Iran's average

146
00:08:14,380 --> 00:08:21,639
entropy and others forecast area

147
00:08:18,100 --> 00:08:24,180
features seasonal non seasonal ARIMA and

148
00:08:21,639 --> 00:08:24,180
hazardous

149
00:08:24,360 --> 00:08:29,470
another type of feature is time pro

150
00:08:26,650 --> 00:08:32,860
features which means that what

151
00:08:29,470 --> 00:08:36,880
difference between what happened now and

152
00:08:32,860 --> 00:08:45,880
maybe fix the window a ago what's the

153
00:08:36,880 --> 00:08:48,370
difference so let's try standard air

154
00:08:45,880 --> 00:08:52,600
transfer a linear approach first now we

155
00:08:48,370 --> 00:08:55,360
have a big feature site and from source

156
00:08:52,600 --> 00:08:58,209
domain from target domain so the

157
00:08:55,360 --> 00:09:02,529
standard transfer learning approach like

158
00:08:58,209 --> 00:09:05,349
our own we need to build a mapping

159
00:09:02,529 --> 00:09:08,910
between the source feature site and the

160
00:09:05,350 --> 00:09:12,310
targeted data site by finding structure

161
00:09:08,910 --> 00:09:16,689
transfer matrix to minimize this

162
00:09:12,310 --> 00:09:20,640
objective function so that once we find

163
00:09:16,690 --> 00:09:23,350
out this transformation matrix we can

164
00:09:20,640 --> 00:09:26,520
transfer the model we learn from the

165
00:09:23,350 --> 00:09:30,850
source domain to the target domain

166
00:09:26,520 --> 00:09:33,400
however we find out that he the problem

167
00:09:30,850 --> 00:09:36,100
we try to solve using this standard

168
00:09:33,400 --> 00:09:39,730
approach does not work because we cannot

169
00:09:36,100 --> 00:09:42,190
get a small enough with G because of the

170
00:09:39,730 --> 00:09:45,580
large variance of the features in source

171
00:09:42,190 --> 00:09:47,620
domain and the target domain so how to

172
00:09:45,580 --> 00:09:51,089
solve this problem that's actually the

173
00:09:47,620 --> 00:09:51,089
key contribution of this paper

174
00:09:53,980 --> 00:09:58,040
sometimes I think maybe this is just an

175
00:09:56,120 --> 00:10:00,380
interview question you can think about

176
00:09:58,040 --> 00:10:02,709
and how to solve this problem in a few

177
00:10:00,380 --> 00:10:02,709
minutes

178
00:10:08,040 --> 00:10:17,670
so the idea is not that magic we just

179
00:10:15,170 --> 00:10:20,849
try to leverage a clustering algorithm

180
00:10:17,670 --> 00:10:25,020
to class as a sauce feature site to a

181
00:10:20,850 --> 00:10:27,600
few groups in each group the feature the

182
00:10:25,020 --> 00:10:30,590
feature vector are more similar against

183
00:10:27,600 --> 00:10:34,290
with other groups and then we map the

184
00:10:30,590 --> 00:10:38,760
feature vectors in the target domain to

185
00:10:34,290 --> 00:10:43,680
the closest site in the sauce domain and

186
00:10:38,760 --> 00:10:46,670
in each in each group we will I with the

187
00:10:43,680 --> 00:10:49,829
transfer learning so we will change the

188
00:10:46,670 --> 00:10:52,339
optimization function and we get a much

189
00:10:49,830 --> 00:10:52,340
better result

190
00:10:57,989 --> 00:11:05,639
the next step is to see how can we

191
00:11:02,009 --> 00:11:09,269
further improve the result by leveraging

192
00:11:05,639 --> 00:11:12,720
a little bit more human knowledge using

193
00:11:09,269 --> 00:11:15,329
the active learning mechanism so the key

194
00:11:12,720 --> 00:11:20,249
idea is to find out the most valuable

195
00:11:15,329 --> 00:11:23,008
labels for human to label how to find

196
00:11:20,249 --> 00:11:27,779
out the most available labels so in

197
00:11:23,009 --> 00:11:29,939
active learning either it is common in

198
00:11:27,779 --> 00:11:34,139
active learning approach that we find

199
00:11:29,939 --> 00:11:40,618
out these labels candle labels with high

200
00:11:34,139 --> 00:11:42,360
uncertainty kind of labels but in this

201
00:11:40,619 --> 00:11:45,420
specific problem we also find out that

202
00:11:42,360 --> 00:11:47,910
diversity of the context is also

203
00:11:45,420 --> 00:11:54,299
important always playing a little bit

204
00:11:47,910 --> 00:11:58,290
more detail so this is real forward hike

205
00:11:54,299 --> 00:12:03,389
uncertainty instance means that we want

206
00:11:58,290 --> 00:12:07,439
to find these labels that it's not that

207
00:12:03,389 --> 00:12:11,100
clear it is positive or negative and we

208
00:12:07,439 --> 00:12:17,759
choose this set of instance for human to

209
00:12:11,100 --> 00:12:21,360
to label it but still we find out there

210
00:12:17,759 --> 00:12:25,079
are some kind of labels with high

211
00:12:21,360 --> 00:12:28,139
uncertainties similar to each other we

212
00:12:25,079 --> 00:12:32,299
do with some human effort because we are

213
00:12:28,139 --> 00:12:35,850
not diverse enough diversity means that

214
00:12:32,299 --> 00:12:39,720
the these kind of high uncertainty

215
00:12:35,850 --> 00:12:42,419
labels are very close to each other is a

216
00:12:39,720 --> 00:12:45,600
time series so it's easy to understand

217
00:12:42,419 --> 00:12:47,850
if two two labels in one thing in the

218
00:12:45,600 --> 00:12:50,730
same time series and we have very close

219
00:12:47,850 --> 00:12:54,089
we have known in the very close time

220
00:12:50,730 --> 00:12:59,239
window they are very similar so the idea

221
00:12:54,089 --> 00:13:03,629
is just using such kind of window to

222
00:12:59,239 --> 00:13:07,789
define the context diversity so if there

223
00:13:03,629 --> 00:13:10,140
are multiple candidates to be labeled

224
00:13:07,789 --> 00:13:14,880
share the same contacts which

225
00:13:10,140 --> 00:13:21,920
to avoid truth multiples from the same

226
00:13:14,880 --> 00:13:26,550
context so this is a oral approach and

227
00:13:21,920 --> 00:13:30,990
idea and now let me share how we

228
00:13:26,550 --> 00:13:35,849
validate the approach so we want to

229
00:13:30,990 --> 00:13:38,240
answer a few research questions overall

230
00:13:35,850 --> 00:13:42,000
effectiveness of the approach and the

231
00:13:38,240 --> 00:13:44,250
effectiveness of the to key component

232
00:13:42,000 --> 00:13:47,160
transfer linear approach and the active

233
00:13:44,250 --> 00:13:51,899
learning approach and the last person is

234
00:13:47,160 --> 00:13:56,189
that we try to evaluate ok if it is

235
00:13:51,899 --> 00:13:59,579
really valuable to large is a public

236
00:13:56,190 --> 00:14:07,649
debt aside to a company's local data

237
00:13:59,579 --> 00:14:09,719
site so we so here we leverage is a

238
00:14:07,649 --> 00:14:12,600
public website from Yahoo it applies

239
00:14:09,720 --> 00:14:14,250
Twitter and there's also public disaster

240
00:14:12,600 --> 00:14:17,279
which is artificial

241
00:14:14,250 --> 00:14:20,310
so now Yahoo means all public visit

242
00:14:17,279 --> 00:14:24,019
sites except Yahoo now it applies miss

243
00:14:20,310 --> 00:14:27,140
and all public besides accept it address

244
00:14:24,019 --> 00:14:29,940
so evaluation matrix is f1 score

245
00:14:27,140 --> 00:14:31,709
calculated by the combination of

246
00:14:29,940 --> 00:14:37,620
precision that recall which is very

247
00:14:31,709 --> 00:14:42,569
standard so we answer the first question

248
00:14:37,620 --> 00:14:45,199
how effective is a proposed approach by

249
00:14:42,570 --> 00:14:50,699
these two tables the first one is

250
00:14:45,199 --> 00:14:54,089
comparing some other standard anomaly

251
00:14:50,699 --> 00:14:57,640
detection mechanism with ETA D in

252
00:14:54,089 --> 00:15:02,940
different data sites and we can see

253
00:14:57,640 --> 00:15:08,290
the overall f1 score of our approach is

254
00:15:02,940 --> 00:15:11,710
much better than other approach this one

255
00:15:08,290 --> 00:15:13,660
is is too good to be because it is from

256
00:15:11,710 --> 00:15:17,170
now after issue through not a decision

257
00:15:13,660 --> 00:15:22,089
for others this is a good the best one

258
00:15:17,170 --> 00:15:29,319
and table six tell us that if we want to

259
00:15:22,090 --> 00:15:32,080
achieve similar f1 score the without

260
00:15:29,320 --> 00:15:33,700
leveraging transfer learning just

261
00:15:32,080 --> 00:15:36,250
leverage supervise the model like a

262
00:15:33,700 --> 00:15:38,710
random forest we need much more number

263
00:15:36,250 --> 00:15:40,330
of the labels so that's a three

264
00:15:38,710 --> 00:15:43,120
partially answers a second question

265
00:15:40,330 --> 00:15:48,460
about the effectiveness of the transfer

266
00:15:43,120 --> 00:15:51,790
learning so here we answer the second

267
00:15:48,460 --> 00:15:54,990
question again Table seven and Table

268
00:15:51,790 --> 00:15:57,130
eight just give give us some sense that

269
00:15:54,990 --> 00:16:00,970
there are many different type of the

270
00:15:57,130 --> 00:16:04,570
features which contributed to the

271
00:16:00,970 --> 00:16:05,950
prediction and different type of

272
00:16:04,570 --> 00:16:11,470
features have different type of

273
00:16:05,950 --> 00:16:15,690
contribution and figure four is kind of

274
00:16:11,470 --> 00:16:19,660
similar to Table six that we try to

275
00:16:15,690 --> 00:16:23,620
compel the normal labels we need to

276
00:16:19,660 --> 00:16:28,800
achieve good recall by using a naive

277
00:16:23,620 --> 00:16:34,810
active learning and the supervisor

278
00:16:28,800 --> 00:16:40,719
supervisor model it's very clear that

279
00:16:34,810 --> 00:16:42,790
our model approach need much fewer

280
00:16:40,720 --> 00:16:48,400
number of the labels in different data

281
00:16:42,790 --> 00:16:50,860
sites which is consistent so person

282
00:16:48,400 --> 00:16:53,740
three is about how effective the active

283
00:16:50,860 --> 00:16:58,270
learning on component so here we try to

284
00:16:53,740 --> 00:17:00,640
answer these questions by by analyzing

285
00:16:58,270 --> 00:17:04,480
the active learning effectiveness by

286
00:17:00,640 --> 00:17:07,150
just a library genes uncertainty label

287
00:17:04,480 --> 00:17:10,240
selection or uncertainty plus the

288
00:17:07,150 --> 00:17:11,429
diversity selection and is a random

289
00:17:10,240 --> 00:17:15,368
selection

290
00:17:11,429 --> 00:17:18,309
so having a few round of the active

291
00:17:15,368 --> 00:17:20,259
learning we find out that library boosts

292
00:17:18,309 --> 00:17:24,939
uncertainty and diversity we get to the

293
00:17:20,259 --> 00:17:32,139
best result we have how many minutes I

294
00:17:24,939 --> 00:17:34,169
have okay so the last one show that yes

295
00:17:32,139 --> 00:17:37,840
it's too valuable to library the public

296
00:17:34,169 --> 00:17:42,009
labels from public government to to The

297
00:17:37,840 --> 00:17:44,289
Tennessean Microsoft so conclusion we

298
00:17:42,009 --> 00:17:47,710
proposed a number detection mastered

299
00:17:44,289 --> 00:17:49,989
which enables cross the recital normally

300
00:17:47,710 --> 00:17:53,019
detection for cloud systems and the

301
00:17:49,989 --> 00:17:56,019
experiment result shows it is very

302
00:17:53,019 --> 00:17:58,679
effective of course there's a lot of

303
00:17:56,019 --> 00:18:01,869
work to do easy to me and I really

304
00:17:58,679 --> 00:18:05,289
encourage more such kind of research

305
00:18:01,869 --> 00:18:14,590
work in the community thank you much

306
00:18:05,289 --> 00:18:20,408
that's my presentation oh one another

307
00:18:14,590 --> 00:18:24,039
Testament here so mister you go who is

308
00:18:20,409 --> 00:18:27,779
there Kyle will be our hand okay

309
00:18:24,039 --> 00:18:27,779
Oh questions please

310
00:18:34,990 --> 00:18:43,280
akshay judging from polio so can this

311
00:18:40,820 --> 00:18:47,480
have any sort of security implications

312
00:18:43,280 --> 00:18:49,639
for example if I am a company X and I

313
00:18:47,480 --> 00:18:52,790
know that you are using my data set for

314
00:18:49,640 --> 00:18:55,190
your training can I do something to my

315
00:18:52,790 --> 00:18:57,550
data set so that I can leave it what's

316
00:18:55,190 --> 00:18:59,830
going on inside your system

317
00:18:57,550 --> 00:19:01,700
can there be any chances like this

318
00:18:59,830 --> 00:19:03,980
because for instance there have been

319
00:19:01,700 --> 00:19:06,800
examples like I've seen if there's a

320
00:19:03,980 --> 00:19:09,800
stop sign and you we can just edit

321
00:19:06,800 --> 00:19:11,540
little bit which is not human visible

322
00:19:09,800 --> 00:19:14,659
but the stop sign can be inferred as

323
00:19:11,540 --> 00:19:16,550
completely speed limit to 60 and it has

324
00:19:14,660 --> 00:19:18,680
affected autonomous vehicles a lot and

325
00:19:16,550 --> 00:19:22,129
these kind of attacks can happen so can

326
00:19:18,680 --> 00:19:27,140
i play with my dataset which in fact is

327
00:19:22,130 --> 00:19:30,800
the overall results yeah well this is

328
00:19:27,140 --> 00:19:34,370
about ethics right if you already

329
00:19:30,800 --> 00:19:35,540
verified and you do not but I'm talking

330
00:19:34,370 --> 00:19:37,760
when you are going in your production

331
00:19:35,540 --> 00:19:39,409
cluster companies are do strategies not

332
00:19:37,760 --> 00:19:42,590
everything is ethical in the business

333
00:19:39,410 --> 00:19:46,160
right people pick up come engineers from

334
00:19:42,590 --> 00:19:49,129
other companies so sure if it's research

335
00:19:46,160 --> 00:19:50,570
community its ethics but if it's going

336
00:19:49,130 --> 00:19:54,190
in your production last year and I am

337
00:19:50,570 --> 00:19:58,220
your competitor reasonable question and

338
00:19:54,190 --> 00:20:02,840
I think but the overall portrays there

339
00:19:58,220 --> 00:20:06,740
that we we accept very rich set of the

340
00:20:02,840 --> 00:20:11,540
features as allows the numerous point

341
00:20:06,740 --> 00:20:15,320
labeled by by human is something really

342
00:20:11,540 --> 00:20:17,480
abnormal I think we can we can library

343
00:20:15,320 --> 00:20:20,389
the value because the fundamental idea

344
00:20:17,480 --> 00:20:27,080
is there are some common courtesies

345
00:20:20,390 --> 00:20:29,540
between different cloud services I'm not

346
00:20:27,080 --> 00:20:33,429
sure I have a good answer to you but

347
00:20:29,540 --> 00:20:33,430
it's this very interesting question

348
00:20:36,720 --> 00:20:43,090
all right I think that's it we have our

349
00:20:40,570 --> 00:20:45,179
in of our time so let's thank the

350
00:20:43,090 --> 00:20:50,079
speaker one more time

351
00:20:45,180 --> 00:20:50,079
[Applause]

