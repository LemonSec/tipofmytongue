1
00:00:10,139 --> 00:00:15,239
hi everyone my name is he comes you I'm

2
00:00:13,620 --> 00:00:16,300
here to talk about the lessons we've

3
00:00:15,240 --> 00:00:18,880
learned

4
00:00:16,300 --> 00:00:22,090
studying about 10000 SST related

5
00:00:18,880 --> 00:00:25,330
failures over three years at alibaba's

6
00:00:22,090 --> 00:00:28,029
cloud service this is a joint work with

7
00:00:25,330 --> 00:00:30,430
researchers from Ohio State and Iowa

8
00:00:28,029 --> 00:00:32,020
State University's and the first author

9
00:00:30,430 --> 00:00:33,340
of this study is not able to come to

10
00:00:32,020 --> 00:00:34,860
this conference so I'm here to present

11
00:00:33,340 --> 00:00:38,920
the work in his place

12
00:00:34,860 --> 00:00:42,879
so flash-based SSDs offers lower latency

13
00:00:38,920 --> 00:00:44,830
higher throughput and lower power usage

14
00:00:42,879 --> 00:00:46,390
at idle time and it's generally

15
00:00:44,830 --> 00:00:48,489
considered more reliable than hard

16
00:00:46,390 --> 00:00:51,460
drives so they are increasingly being

17
00:00:48,489 --> 00:00:52,690
used in cold environments however they

18
00:00:51,460 --> 00:00:55,720
are complicated

19
00:00:52,690 --> 00:00:59,040
for example SSDs has lower acceptable

20
00:00:55,720 --> 00:01:01,690
temperature range than hard drives and

21
00:00:59,040 --> 00:01:03,970
although they use less power at idle

22
00:01:01,690 --> 00:01:05,850
when they are put there busy you can use

23
00:01:03,970 --> 00:01:07,990
a lot of power and generate lots of heat

24
00:01:05,850 --> 00:01:11,559
there has been a number of previous

25
00:01:07,990 --> 00:01:14,050
studies about SSD reliability

26
00:01:11,560 --> 00:01:17,440
they mostly focus on the SSD internals

27
00:01:14,050 --> 00:01:19,929
and their failure rate and their orbit

28
00:01:17,440 --> 00:01:24,399
error rate and uncorrectable error rate

29
00:01:19,930 --> 00:01:27,550
and the impact from FTL's in our study

30
00:01:24,399 --> 00:01:30,399
we take a holistic view we look at post

31
00:01:27,550 --> 00:01:33,459
SSD errors and the system errors

32
00:01:30,399 --> 00:01:36,420
failures around that as these and try to

33
00:01:33,459 --> 00:01:39,520
establish relationship between these two

34
00:01:36,420 --> 00:01:42,670
in our study we focus on failures that

35
00:01:39,520 --> 00:01:46,959
are weak are reported as SSD related by

36
00:01:42,670 --> 00:01:49,209
our per node monitoring daemons then

37
00:01:46,959 --> 00:01:53,110
combined with smart locks node and

38
00:01:49,209 --> 00:01:55,240
device locks and repair steps and the

39
00:01:53,110 --> 00:01:58,170
repair results we arrive some

40
00:01:55,240 --> 00:02:01,810
perspectives from system and

41
00:01:58,170 --> 00:02:03,190
administrative point of view I'll first

42
00:02:01,810 --> 00:02:06,369
give an overview of our cloud

43
00:02:03,190 --> 00:02:09,310
architecture this is a very high-level

44
00:02:06,369 --> 00:02:12,550
architecture of our cloud service the

45
00:02:09,310 --> 00:02:14,530
focus on the hardware part so it shows

46
00:02:12,550 --> 00:02:17,050
three main types and no types they'll

47
00:02:14,530 --> 00:02:19,270
use SSD inside your system Drive the one

48
00:02:17,050 --> 00:02:21,640
on the left has one additional SSDs

49
00:02:19,270 --> 00:02:23,410
they use for local storage this is our

50
00:02:21,640 --> 00:02:27,209
main camp you'll note it's very powerful

51
00:02:23,410 --> 00:02:30,760
CPUs and maybe GPUs the one on the left

52
00:02:27,210 --> 00:02:37,400
has two SSDs

53
00:02:30,760 --> 00:02:40,700
to SSDs and 12 or 36 hard drives this is

54
00:02:37,400 --> 00:02:43,550
our men high-density HDD storage nodes

55
00:02:40,700 --> 00:02:46,250
the two SSDs are used for journal to

56
00:02:43,550 --> 00:02:49,250
buffer incoming rights oh for a host or

57
00:02:46,250 --> 00:02:52,700
a state of sorry for how data the one in

58
00:02:49,250 --> 00:02:56,540
the middle has 12 or 18 SSDs this our

59
00:02:52,700 --> 00:02:59,540
man oh SSD storage node not shown in the

60
00:02:56,540 --> 00:03:01,340
diagram we have a distribute storage

61
00:02:59,540 --> 00:03:03,980
system managing the two storage nodes

62
00:03:01,340 --> 00:03:07,310
and on top of that we have many type of

63
00:03:03,980 --> 00:03:10,130
cloud services in this study we focus on

64
00:03:07,310 --> 00:03:13,670
three of them our block search storage

65
00:03:10,130 --> 00:03:15,620
service or EBS and then we have a no

66
00:03:13,670 --> 00:03:18,410
sequel data service our version of the

67
00:03:15,620 --> 00:03:21,370
animal and then we have a big data

68
00:03:18,410 --> 00:03:24,890
analytics service so part of the annales

69
00:03:21,370 --> 00:03:27,800
investigation involves collecting the

70
00:03:24,890 --> 00:03:29,750
node device level error logs and all the

71
00:03:27,800 --> 00:03:32,600
informations and feed them into the big

72
00:03:29,750 --> 00:03:37,100
nail analysis that when analytical work

73
00:03:32,600 --> 00:03:39,709
load there so this our study covers

74
00:03:37,100 --> 00:03:42,650
about half a million SSDs used by these

75
00:03:39,709 --> 00:03:45,290
three services the services are deployed

76
00:03:42,650 --> 00:03:48,350
in pretty much all regions of alibaba

77
00:03:45,290 --> 00:03:51,170
cloud and the early more than one PC is

78
00:03:48,350 --> 00:03:55,579
provision in total we look at it SSD

79
00:03:51,170 --> 00:03:58,040
deployed in seven datacenters this SSD

80
00:03:55,580 --> 00:04:00,680
is consists of five models from sweet

81
00:03:58,040 --> 00:04:04,880
even vendors different capacity and

82
00:04:00,680 --> 00:04:06,980
discography they are all mlc SSDs and at

83
00:04:04,880 --> 00:04:10,700
the time of the investigation most have

84
00:04:06,980 --> 00:04:14,510
been used for two to three years the

85
00:04:10,700 --> 00:04:21,440
table on the right shows their how each

86
00:04:14,510 --> 00:04:24,520
service uses SSD next word is define how

87
00:04:21,440 --> 00:04:27,560
we how it kind of is the term college

88
00:04:24,520 --> 00:04:30,530
research has reported as a simulated

89
00:04:27,560 --> 00:04:34,060
failures so our investigation mythology

90
00:04:30,530 --> 00:04:38,000
inquiry involves analyzing node device

91
00:04:34,060 --> 00:04:39,590
and repair logs and smart data and also

92
00:04:38,000 --> 00:04:41,900
conducting field studies

93
00:04:39,590 --> 00:04:44,169
inside the data center and we do some

94
00:04:41,900 --> 00:04:47,030
experimentation as well

95
00:04:44,170 --> 00:04:52,700
so across the street service over three

96
00:04:47,030 --> 00:04:56,570
years period we collected about 1550

97
00:04:52,700 --> 00:04:59,170
thousand federal events so every note in

98
00:04:56,570 --> 00:05:01,670
our data center has the background

99
00:04:59,170 --> 00:05:04,280
monitoring daemon that monitors every

100
00:05:01,670 --> 00:05:06,620
component that fits a physical machine

101
00:05:04,280 --> 00:05:08,510
mostly Hardware related but sometimes OS

102
00:05:06,620 --> 00:05:10,310
related as well every time it sees a

103
00:05:08,510 --> 00:05:12,469
problem he reports that to a centralized

104
00:05:10,310 --> 00:05:15,080
service and the problem catch track and

105
00:05:12,470 --> 00:05:19,010
eventually repaired so across the street

106
00:05:15,080 --> 00:05:21,260
service we collect 150,000 events most

107
00:05:19,010 --> 00:05:24,170
of time the monitoring demon can tell

108
00:05:21,260 --> 00:05:26,150
which was a problem and about 20 percent

109
00:05:24,170 --> 00:05:27,890
time he cannot and that's he mark the

110
00:05:26,150 --> 00:05:30,859
mess and no he already ends up with

111
00:05:27,890 --> 00:05:33,010
whole node replacement SSD related

112
00:05:30,860 --> 00:05:35,720
failures accounts for about 6 percent

113
00:05:33,010 --> 00:05:37,730
those are the focus of our study that's

114
00:05:35,720 --> 00:05:39,950
why we call them reported as a stay

115
00:05:37,730 --> 00:05:41,720
related the six percent number is

116
00:05:39,950 --> 00:05:43,610
consistent with the fact that SSE

117
00:05:41,720 --> 00:05:47,150
generally more reliable than hard drives

118
00:05:43,610 --> 00:05:49,910
and requires fewer no replacement we

119
00:05:47,150 --> 00:05:53,599
further divide the excessive related

120
00:05:49,910 --> 00:05:55,850
failures based on their symptoms ranging

121
00:05:53,600 --> 00:05:59,240
from note and bootable Drive now found

122
00:05:55,850 --> 00:06:01,310
to file system corruption media error

123
00:05:59,240 --> 00:06:03,650
which is basically reached failures and

124
00:06:01,310 --> 00:06:06,470
buffered i/o error Carles to drive

125
00:06:03,650 --> 00:06:10,099
failures our on-site sres

126
00:06:06,470 --> 00:06:12,950
has a fixed step of repairs so symptom

127
00:06:10,100 --> 00:06:15,770
based I went from simplest a node reboot

128
00:06:12,950 --> 00:06:19,940
to the most expensive one drive

129
00:06:15,770 --> 00:06:21,710
replacement and then depends on the rich

130
00:06:19,940 --> 00:06:23,780
results

131
00:06:21,710 --> 00:06:26,840
repair step actually fixed the failure

132
00:06:23,780 --> 00:06:30,679
we give a look house for example if the

133
00:06:26,840 --> 00:06:33,169
unset RC fix the failure after a with

134
00:06:30,680 --> 00:06:36,110
cable replacement after they tried no to

135
00:06:33,170 --> 00:06:39,760
reboot and running file system checks we

136
00:06:36,110 --> 00:06:41,990
think the dual cause is a faulty cable

137
00:06:39,760 --> 00:06:45,099
next we'll cover the lessons we learned

138
00:06:41,990 --> 00:06:47,930
from studying this SSD related failures

139
00:06:45,100 --> 00:06:50,930
the first one comes from hardware design

140
00:06:47,930 --> 00:06:53,660
and note placement within iraq this is a

141
00:06:50,930 --> 00:06:56,090
puzzle we are we we encountered at the

142
00:06:53,660 --> 00:06:57,800
beginning of the research it's at once

143
00:06:56,090 --> 00:07:00,258
very common

144
00:06:57,800 --> 00:07:02,690
SSD model and they using the very common

145
00:07:00,259 --> 00:07:04,699
service but in one particular datacenter

146
00:07:02,690 --> 00:07:08,090
yes the more than double the failure me

147
00:07:04,699 --> 00:07:10,069
the error rate than the same SSD used in

148
00:07:08,090 --> 00:07:13,219
the same service but in different data

149
00:07:10,069 --> 00:07:15,919
centers as I mentioned this is media

150
00:07:13,220 --> 00:07:19,009
error so those prints out as will lead

151
00:07:15,919 --> 00:07:21,530
to replications at service layer and

152
00:07:19,009 --> 00:07:24,500
affect service performance and even

153
00:07:21,530 --> 00:07:25,729
availability and those media arrows you

154
00:07:24,500 --> 00:07:29,840
really end up with drive replacement

155
00:07:25,729 --> 00:07:33,250
that calls me has money so I'll skip the

156
00:07:29,840 --> 00:07:36,169
details about how we fund the good cause

157
00:07:33,250 --> 00:07:39,050
but the reason we think it's passive

158
00:07:36,169 --> 00:07:42,469
heating we define passive heating as an

159
00:07:39,050 --> 00:07:44,120
SSD that had been through really the

160
00:07:42,470 --> 00:07:46,190
idle SSD at means to a high ambient

161
00:07:44,120 --> 00:07:47,180
temperature from hot neighbors for

162
00:07:46,190 --> 00:07:49,969
extended period of time

163
00:07:47,180 --> 00:07:53,990
how neighbors can be other hatha stated

164
00:07:49,969 --> 00:07:56,240
in the same note Oh hot nose next by

165
00:07:53,990 --> 00:08:00,440
over how nodes in the same rag pass

166
00:07:56,240 --> 00:08:03,020
through some empty slots in our lock we

167
00:08:00,440 --> 00:08:05,719
have seen SSDs rise above ambient

168
00:08:03,020 --> 00:08:08,389
temperature by like up to 25 degrees C

169
00:08:05,719 --> 00:08:11,479
or higher the paper has more details how

170
00:08:08,389 --> 00:08:13,960
each type of in our experiment each type

171
00:08:11,479 --> 00:08:17,090
of passive heating can heat up a

172
00:08:13,960 --> 00:08:20,508
idolized SD and they're our standard

173
00:08:17,090 --> 00:08:22,609
rack setup the reason for why this one

174
00:08:20,509 --> 00:08:24,590
only happens in the one particular data

175
00:08:22,610 --> 00:08:27,770
center is that in all the other data

176
00:08:24,590 --> 00:08:29,568
centers a surveys owns a whole rack all

177
00:08:27,770 --> 00:08:31,520
the knows in Iraq belonged to the same

178
00:08:29,569 --> 00:08:33,680
service so they have similar workload

179
00:08:31,520 --> 00:08:35,929
and similar temperature not much

180
00:08:33,679 --> 00:08:39,159
interference which is other in this

181
00:08:35,929 --> 00:08:41,598
particular TC TC for for some reason

182
00:08:39,159 --> 00:08:43,539
knows from two different services and

183
00:08:41,599 --> 00:08:46,010
mixed together put into the same rack

184
00:08:43,539 --> 00:08:46,699
besides a block service the other server

185
00:08:46,010 --> 00:08:49,339
is a heavy

186
00:08:46,700 --> 00:08:51,350
compute service the server really tries

187
00:08:49,339 --> 00:08:54,230
really hard to maximize CPU usage and

188
00:08:51,350 --> 00:08:56,779
drives the node temperature and the OS

189
00:08:54,230 --> 00:08:59,120
SD storage knows next to it suffers from

190
00:08:56,779 --> 00:09:00,680
passive heating and that's been going on

191
00:08:59,120 --> 00:09:03,130
there for a long time until our study

192
00:09:00,680 --> 00:09:03,130
found out

193
00:09:05,310 --> 00:09:11,760
right so here's another experiment where

194
00:09:09,750 --> 00:09:15,029
we increase the ambient temperature of

195
00:09:11,760 --> 00:09:17,910
an idle SSD from its default 25 degrees

196
00:09:15,029 --> 00:09:20,850
for extended period of time up to 128

197
00:09:17,910 --> 00:09:22,709
hours and then at the end of the test we

198
00:09:20,850 --> 00:09:25,080
read the whole drive and look at this

199
00:09:22,710 --> 00:09:27,089
into our bit error rate as you can see

200
00:09:25,080 --> 00:09:29,010
from the graph the longer the higher the

201
00:09:27,089 --> 00:09:30,839
temperature the longer the period that

202
00:09:29,010 --> 00:09:34,380
the node is under that temperature the

203
00:09:30,839 --> 00:09:36,089
higher is your bit error rate the

204
00:09:34,380 --> 00:09:38,820
temperature is controlled by blending

205
00:09:36,089 --> 00:09:40,830
compute have a workload on the CPU so

206
00:09:38,820 --> 00:09:42,870
this is our compute node with one data

207
00:09:40,830 --> 00:09:43,610
SSD will keep it idle for extended

208
00:09:42,870 --> 00:09:46,890
period of time

209
00:09:43,610 --> 00:09:48,240
we take a smart reading once every 15

210
00:09:46,890 --> 00:09:50,730
seconds to make sure the temperature

211
00:09:48,240 --> 00:09:53,790
stays relatively within the narrow range

212
00:09:50,730 --> 00:09:56,100
a passive heating is different from

213
00:09:53,790 --> 00:09:59,430
active heating which comes from SSDs

214
00:09:56,100 --> 00:10:00,960
only internal activity SSDs has an

215
00:09:59,430 --> 00:10:03,150
internal temperature control when it

216
00:10:00,960 --> 00:10:05,640
becomes too hot the FTL's control his

217
00:10:03,150 --> 00:10:07,800
activity lower is throughput to lower

218
00:10:05,640 --> 00:10:09,480
temperature but under passive heating

219
00:10:07,800 --> 00:10:13,530
there's not much thing that he can do

220
00:10:09,480 --> 00:10:16,940
his FTL's basically idle and the low bid

221
00:10:13,530 --> 00:10:23,850
every accumulate and eventually leads to

222
00:10:16,940 --> 00:10:27,839
media errors so for solutions besides

223
00:10:23,850 --> 00:10:32,070
now to keep computing all next to a SSD

224
00:10:27,839 --> 00:10:33,930
storage node we have two suggestions the

225
00:10:32,070 --> 00:10:37,350
first one is a periodic scanning at the

226
00:10:33,930 --> 00:10:41,790
host level we repeat the same experiment

227
00:10:37,350 --> 00:10:43,320
heat an SSD to 55 degrees C instead

228
00:10:41,790 --> 00:10:45,060
leave it there for a hundred twenty

229
00:10:43,320 --> 00:10:47,700
eight hours every four hours read the

230
00:10:45,060 --> 00:10:51,089
whole drive just take about like 15

231
00:10:47,700 --> 00:10:53,430
minutes and then at the end of 120 hours

232
00:10:51,089 --> 00:10:55,250
test period we measure it's your bit

233
00:10:53,430 --> 00:10:58,589
error rate it's only about one percent

234
00:10:55,250 --> 00:11:02,790
compared to the 57 percent without the

235
00:10:58,589 --> 00:11:04,709
scanning different type of this kind of

236
00:11:02,790 --> 00:11:08,209
approach is used one way or the other

237
00:11:04,709 --> 00:11:11,010
however it's probably not the optimal as

238
00:11:08,209 --> 00:11:13,260
the drives are getting bigger take

239
00:11:11,010 --> 00:11:16,380
longer to read the whole thing and the

240
00:11:13,260 --> 00:11:17,920
read disturb from frequent scanning I

241
00:11:16,380 --> 00:11:20,709
will have negative impact

242
00:11:17,920 --> 00:11:22,959
so the other solution we suggest which

243
00:11:20,709 --> 00:11:25,929
requires vendor support is to move the

244
00:11:22,959 --> 00:11:28,299
scanning into the SSD itself since STL

245
00:11:25,929 --> 00:11:31,420
already knows its temperature and he has

246
00:11:28,299 --> 00:11:33,639
reached refresh capability and also he

247
00:11:31,420 --> 00:11:34,839
knows the quality of genetics so it

248
00:11:33,639 --> 00:11:37,059
seems a much more appropriate

249
00:11:34,839 --> 00:11:42,239
appropriate for this kind of activity to

250
00:11:37,059 --> 00:11:44,889
be inside the FTL so the second lesson

251
00:11:42,239 --> 00:11:48,429
is about software and system

252
00:11:44,889 --> 00:11:52,059
architecture so we look at the three

253
00:11:48,429 --> 00:11:53,919
services with their SSD usage over over

254
00:11:52,059 --> 00:11:55,509
long period of time we first look at

255
00:11:53,919 --> 00:11:58,959
their average usage and they'll loop

256
00:11:55,509 --> 00:12:00,999
normal is purely for example the blog

257
00:11:58,959 --> 00:12:02,439
service and the no cycle service they

258
00:12:00,999 --> 00:12:04,199
have similar host read and the host

259
00:12:02,439 --> 00:12:07,858
right

260
00:12:04,199 --> 00:12:09,729
hourly rate however if we look at the

261
00:12:07,859 --> 00:12:12,309
coefficient of embrance

262
00:12:09,730 --> 00:12:14,799
variance which basically the standard

263
00:12:12,309 --> 00:12:16,569
deviation over the mean we see a big

264
00:12:14,799 --> 00:12:20,829
difference between the blog service and

265
00:12:16,569 --> 00:12:23,649
the other tube the post the pics data

266
00:12:20,829 --> 00:12:26,439
and snow cycles relatively nice single

267
00:12:23,649 --> 00:12:29,199
digit the block service or double digit

268
00:12:26,439 --> 00:12:32,429
which means the SSDs under the Polacks

269
00:12:29,199 --> 00:12:36,279
service has very diverse usage pattern

270
00:12:32,429 --> 00:12:39,249
here's one particular block service SSD

271
00:12:36,279 --> 00:12:42,519
you cluster and this is the average over

272
00:12:39,249 --> 00:12:46,649
three months on the x-axis is the hourly

273
00:12:42,519 --> 00:12:49,269
average host right rate as you can see

274
00:12:46,649 --> 00:12:52,689
on the y-axis is the normalized number

275
00:12:49,269 --> 00:12:56,199
of nodes at that right workload

276
00:12:52,689 --> 00:12:58,988
as you can see the the usage ranges from

277
00:12:56,199 --> 00:13:01,419
lesson 0.5 kilo bytes an hour over a

278
00:12:58,989 --> 00:13:06,029
three-month period to about 10 gigabytes

279
00:13:01,419 --> 00:13:06,029
an hour older over the three months

280
00:13:06,639 --> 00:13:11,139
on the contrary that's at the usage

281
00:13:09,039 --> 00:13:14,109
distribution for both the big data and

282
00:13:11,139 --> 00:13:18,009
oseco service is much narrower such

283
00:13:14,109 --> 00:13:20,850
diverse usage pattern can lead to uneven

284
00:13:18,009 --> 00:13:23,169
SSD where level e at the cluster level

285
00:13:20,850 --> 00:13:25,449
that a subset of SSDs

286
00:13:23,169 --> 00:13:28,629
experience heavy usage early we're out

287
00:13:25,449 --> 00:13:31,790
and more errors later on while the rest

288
00:13:28,629 --> 00:13:34,519
are barely used in the classroom

289
00:13:31,790 --> 00:13:37,399
this will become increasing problem as

290
00:13:34,519 --> 00:13:42,380
SATs are getting bigger and have but

291
00:13:37,399 --> 00:13:44,959
they come with lower Twp DS the reason

292
00:13:42,380 --> 00:13:46,939
for the diverse usage is to cause by the

293
00:13:44,959 --> 00:13:50,359
design of our first strand generation

294
00:13:46,940 --> 00:13:52,220
block service architecture the first

295
00:13:50,360 --> 00:13:54,290
generation block service takes a simple

296
00:13:52,220 --> 00:13:58,130
approach that directly Maps a user level

297
00:13:54,290 --> 00:14:01,490
LBA to a triple replicated physical SSD

298
00:13:58,130 --> 00:14:05,269
ll be a range and use in place update to

299
00:14:01,490 --> 00:14:06,980
serve users read request both the

300
00:14:05,269 --> 00:14:09,980
mapping and the placement are relatively

301
00:14:06,980 --> 00:14:11,500
static and the initial placement the

302
00:14:09,980 --> 00:14:14,120
block service knows very little about

303
00:14:11,500 --> 00:14:17,420
the access pattern or how hot or cold

304
00:14:14,120 --> 00:14:20,480
the users I'll be rent will be so we

305
00:14:17,420 --> 00:14:24,319
directly expose the LBA or the physical

306
00:14:20,480 --> 00:14:29,240
acidity to the to the diverse access

307
00:14:24,320 --> 00:14:33,589
pattern of EBS users the solution well

308
00:14:29,240 --> 00:14:35,269
this is our second generation EBS block

309
00:14:33,589 --> 00:14:37,699
service design which used a log

310
00:14:35,269 --> 00:14:39,769
structure approach instead of in-place

311
00:14:37,699 --> 00:14:41,660
update the user updates are written to a

312
00:14:39,769 --> 00:14:43,040
different Drive different location most

313
00:14:41,660 --> 00:14:46,279
likely on a different Drive in the same

314
00:14:43,040 --> 00:14:49,730
cluster and the block servers keep track

315
00:14:46,279 --> 00:14:51,130
of where the data is and updates and the

316
00:14:49,730 --> 00:14:54,079
garbage credit data in the background

317
00:14:51,130 --> 00:14:56,839
OSS DC is a pending traffic from the

318
00:14:54,079 --> 00:14:59,630
host the new block service was g8 about

319
00:14:56,839 --> 00:15:01,190
a year ago and we observed its

320
00:14:59,630 --> 00:15:02,779
coefficient variance it's about five

321
00:15:01,190 --> 00:15:05,329
percent instead of the twenty five

322
00:15:02,779 --> 00:15:07,399
percent in the previous design the new

323
00:15:05,329 --> 00:15:09,380
design obviously has a trade-off that

324
00:15:07,399 --> 00:15:11,720
now there are more rights to the SSD

325
00:15:09,380 --> 00:15:16,240
from the GC activity from the service

326
00:15:11,720 --> 00:15:18,470
layer the third lesson come from the

327
00:15:16,240 --> 00:15:20,300
administrative point of view this is a

328
00:15:18,470 --> 00:15:22,850
rehash of the previous slide showing the

329
00:15:20,300 --> 00:15:24,889
under the drive failure case al drive

330
00:15:22,850 --> 00:15:29,870
drive now found accounts to like more

331
00:15:24,889 --> 00:15:31,730
than 50% of the SSD failures and this is

332
00:15:29,870 --> 00:15:34,730
all the steps of the major steps that

333
00:15:31,730 --> 00:15:37,279
actually fix the problem one thing worth

334
00:15:34,730 --> 00:15:40,850
noticing that is replacing able accounts

335
00:15:37,279 --> 00:15:44,460
for more than replacing SSD we want to

336
00:15:40,850 --> 00:15:46,860
look more into the cause of that

337
00:15:44,460 --> 00:15:51,029
particular type of smart data color

338
00:15:46,860 --> 00:15:53,550
retention is called ultra DMA crc it of

339
00:15:51,029 --> 00:15:56,700
course the CRC the data fails the CRC

340
00:15:53,550 --> 00:15:58,290
check after the SSD to a host

341
00:15:56,700 --> 00:16:00,540
transmission which will trigger

342
00:15:58,290 --> 00:16:02,610
automatically try sometimes most of

343
00:16:00,540 --> 00:16:05,040
times that we try to succeed and then

344
00:16:02,610 --> 00:16:05,790
the error is considered benign transient

345
00:16:05,040 --> 00:16:07,800
to ignore it

346
00:16:05,790 --> 00:16:11,910
however we see a strong correlation

347
00:16:07,800 --> 00:16:18,060
between the you CRC error code and the

348
00:16:11,910 --> 00:16:20,459
faulty cable you'd cause Oh skip running

349
00:16:18,060 --> 00:16:23,550
out of time so another lesson we learned

350
00:16:20,459 --> 00:16:26,069
is second amount twenty percent of the

351
00:16:23,550 --> 00:16:29,609
time the failures is called by human

352
00:16:26,070 --> 00:16:32,190
errors caused by wrong slot people stuck

353
00:16:29,610 --> 00:16:34,890
the system drive into data drive slots

354
00:16:32,190 --> 00:16:37,709
and the solution we come out is called

355
00:16:34,890 --> 00:16:39,540
why interface one purpose we use we

356
00:16:37,709 --> 00:16:42,449
intentionally choose different interface

357
00:16:39,540 --> 00:16:47,579
map m2 for system drive and SATA for the

358
00:16:42,450 --> 00:16:49,440
data drive and for conclusions so we

359
00:16:47,580 --> 00:16:51,330
look at about 10,000 SSD related

360
00:16:49,440 --> 00:16:54,900
failures in our system over three years

361
00:16:51,330 --> 00:16:56,930
and we evaluate them from three angles

362
00:16:54,900 --> 00:17:00,060
hardware service design and

363
00:16:56,930 --> 00:17:03,060
administrative part of you and the

364
00:17:00,060 --> 00:17:05,339
conclusion is so first one is passive

365
00:17:03,060 --> 00:17:08,309
heating in data center is real because

366
00:17:05,339 --> 00:17:11,099
SSDs this is less temperature tolerant

367
00:17:08,309 --> 00:17:15,209
and they consider hot here and they stay

368
00:17:11,099 --> 00:17:16,500
close to CPUs and other SSDs the second

369
00:17:15,209 --> 00:17:20,000
we should look at which we pay attention

370
00:17:16,500 --> 00:17:24,179
to we're leveling at the crossroad level

371
00:17:20,000 --> 00:17:26,699
because otherwise it will cause an even

372
00:17:24,179 --> 00:17:29,790
we're leveling among city populations

373
00:17:26,699 --> 00:17:31,910
the third one is although pad drives

374
00:17:29,790 --> 00:17:34,980
gets most attention

375
00:17:31,910 --> 00:17:35,850
faulty table is more common than people

376
00:17:34,980 --> 00:17:38,809
think

377
00:17:35,850 --> 00:17:42,469
so it's an you be it's necessary to find

378
00:17:38,809 --> 00:17:45,870
good indicators for that problems and

379
00:17:42,470 --> 00:17:46,860
that's all I have thank you and happy to

380
00:17:45,870 --> 00:17:52,510
take questions

381
00:17:46,860 --> 00:18:00,590
[Applause]

382
00:17:52,510 --> 00:18:04,250
questions okay I have a and I have a

383
00:18:00,590 --> 00:18:07,549
question from almost the very end do you

384
00:18:04,250 --> 00:18:11,590
you mentioned that faulty cables are a

385
00:18:07,549 --> 00:18:14,539
big cause the cables actually go bad

386
00:18:11,590 --> 00:18:16,039
yes and they're well they're there -

387
00:18:14,539 --> 00:18:17,840
there are two part of it one is the

388
00:18:16,039 --> 00:18:20,899
interface between the cable and SSD and

389
00:18:17,840 --> 00:18:22,399
that can go bad and in that case you

390
00:18:20,899 --> 00:18:24,678
appearance in cable you may although you

391
00:18:22,399 --> 00:18:26,559
see the you crc errors but it may not be

392
00:18:24,679 --> 00:18:29,840
the cables fault it may be the

393
00:18:26,559 --> 00:18:31,940
connection between the cable and SSD but

394
00:18:29,840 --> 00:18:34,189
more likely is the cables having problem

395
00:18:31,940 --> 00:18:37,370
is this mechanical disturbance then

396
00:18:34,190 --> 00:18:40,730
that's causing the cables to fail I

397
00:18:37,370 --> 00:18:43,969
don't know the tech the details but no

398
00:18:40,730 --> 00:18:46,490
but we do see stick number of cable has

399
00:18:43,970 --> 00:18:48,950
need to be replaced it rests playing

400
00:18:46,490 --> 00:18:51,380
Apple I'm just worrying if those errors

401
00:18:48,950 --> 00:18:54,440
were during elevated temperature levels

402
00:18:51,380 --> 00:18:58,279
are transient or if they're permanent

403
00:18:54,440 --> 00:19:01,070
somehow so the MMS meteor so what we see

404
00:18:58,279 --> 00:19:04,760
is meteor it's double the the D for the

405
00:19:01,070 --> 00:19:07,129
default case so the the fix for those

406
00:19:04,760 --> 00:19:13,070
things you really drive replacement it

407
00:19:07,130 --> 00:19:17,179
could be that maybe the F key I was

408
00:19:13,070 --> 00:19:20,320
smart enough to escape those pads and

409
00:19:17,179 --> 00:19:23,750
chips this drives becomes usable again

410
00:19:20,320 --> 00:19:29,080
but it just lower the quality of the the

411
00:19:23,750 --> 00:19:29,080
drive okay one more question

412
00:19:30,279 --> 00:19:36,080
hello this is Liam from East China

413
00:19:33,200 --> 00:19:39,289
Normal University my question is what

414
00:19:36,080 --> 00:19:43,428
kind of error correction scheme you used

415
00:19:39,289 --> 00:19:47,510
for the SSD re inside SSP oh I just

416
00:19:43,429 --> 00:19:49,519
erased that one you booze and the host

417
00:19:47,510 --> 00:19:50,960
level okay so this is a typical

418
00:19:49,519 --> 00:19:52,639
distribute system we have either

419
00:19:50,960 --> 00:19:58,159
equation Cody who trip over applications

420
00:19:52,639 --> 00:20:00,020
and the service level yeah I mean this

421
00:19:58,159 --> 00:20:01,390
kind of under secret we don't know much

422
00:20:00,020 --> 00:20:06,230
about that

423
00:20:01,390 --> 00:20:10,630
one more yeah as I know in Alibaba

424
00:20:06,230 --> 00:20:12,920
they're there they use open our SSD so

425
00:20:10,630 --> 00:20:14,840
those are all commercials I mean those

426
00:20:12,920 --> 00:20:17,180
the open channels are still under

427
00:20:14,840 --> 00:20:20,030
development this this Astarte was like

428
00:20:17,180 --> 00:20:21,710
conducted two years ago I think all SSD

429
00:20:20,030 --> 00:20:25,010
at that time were static commercial

430
00:20:21,710 --> 00:20:26,300
entities okay thank you yeah all right

431
00:20:25,010 --> 00:20:30,579
let's thank the speaker

432
00:20:26,300 --> 00:20:30,579
[Applause]

