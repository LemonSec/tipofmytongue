1
00:00:10,020 --> 00:00:14,780
thank you for the introduction

2
00:00:12,920 --> 00:00:16,939
my name is Tonya and this is a joint

3
00:00:14,780 --> 00:00:21,529
work with mark silverstein and

4
00:00:16,940 --> 00:00:23,200
publishers from Tekken GPUs have come a

5
00:00:21,529 --> 00:00:25,820
long way from fixed-function

6
00:00:23,200 --> 00:00:27,740
accelerators to fully programmable high

7
00:00:25,820 --> 00:00:30,020
performance processors that support them

8
00:00:27,740 --> 00:00:32,198
and paging yet their integration with

9
00:00:30,020 --> 00:00:35,120
the host OS is still quite limited in

10
00:00:32,198 --> 00:00:37,370
particular GPU memory which can today be

11
00:00:35,120 --> 00:00:39,260
as large as 32 gigabytes has been

12
00:00:37,370 --> 00:00:42,169
traditionally managed entirely by the

13
00:00:39,260 --> 00:00:44,449
GPU driver without any host OS control

14
00:00:42,170 --> 00:00:47,360
now one crucial implication of such

15
00:00:44,449 --> 00:00:50,239
design is the OS cannot provide core

16
00:00:47,360 --> 00:00:52,550
system services to GPU kernels such as

17
00:00:50,239 --> 00:00:55,550
memory mapped files or efficient file

18
00:00:52,550 --> 00:00:57,260
sharing between the CPU and the GPU nor

19
00:00:55,550 --> 00:01:00,530
can if utilized the additional memory

20
00:00:57,260 --> 00:01:02,930
links in disposal for optimizing CPU i/o

21
00:01:00,530 --> 00:01:05,570
or support seamless peer-to-peer between

22
00:01:02,930 --> 00:01:07,490
the GPU and the disk but you might ask

23
00:01:05,570 --> 00:01:09,380
yourself why are you telling us about

24
00:01:07,490 --> 00:01:11,750
this I mean why do we even need these

25
00:01:09,380 --> 00:01:13,729
functionalities let's take a look at

26
00:01:11,750 --> 00:01:14,530
this side an example and decide together

27
00:01:13,729 --> 00:01:17,509
shall we

28
00:01:14,530 --> 00:01:19,819
so this needs no introduction I'm sure

29
00:01:17,509 --> 00:01:23,750
everyone uses Road navigation well at

30
00:01:19,819 --> 00:01:25,940
least I do but let's say think how its

31
00:01:23,750 --> 00:01:27,890
implemented and we'll focus on the

32
00:01:25,940 --> 00:01:30,590
server side of it because this is where

33
00:01:27,890 --> 00:01:32,270
the interesting stuff happens so you

34
00:01:30,590 --> 00:01:34,729
want to get from the airport to our

35
00:01:32,270 --> 00:01:36,500
hotel we can look at a map as a weighted

36
00:01:34,729 --> 00:01:38,360
graph with weights being the travel

37
00:01:36,500 --> 00:01:40,369
times on the roads and we have a

38
00:01:38,360 --> 00:01:43,640
planning service taking this graph as an

39
00:01:40,369 --> 00:01:46,039
input and calculating the shortest path

40
00:01:43,640 --> 00:01:47,869
for us to travel on and in parallel we

41
00:01:46,039 --> 00:01:49,819
have the traffic updater service

42
00:01:47,869 --> 00:01:52,009
periodically updating the travel times

43
00:01:49,819 --> 00:01:53,660
on the roads and as a result of the

44
00:01:52,009 --> 00:01:56,239
update we have to reactivate our

45
00:01:53,660 --> 00:01:58,488
planning service and our optimal path

46
00:01:56,239 --> 00:02:02,840
may change now how would we implement

47
00:01:58,489 --> 00:02:05,179
this so our map is saved in the file and

48
00:02:02,840 --> 00:02:07,640
we will take two CPU processes each of

49
00:02:05,179 --> 00:02:10,848
them and mapping the file so the code is

50
00:02:07,640 --> 00:02:13,730
pretty simple our updated service just a

51
00:02:10,848 --> 00:02:16,609
map the file and uses it as if it was a

52
00:02:13,730 --> 00:02:18,859
regular memory access and same for our

53
00:02:16,610 --> 00:02:20,659
planning service we just a map the file

54
00:02:18,860 --> 00:02:23,689
and pass the pointer to the route

55
00:02:20,659 --> 00:02:25,850
calculation routine but what if we now

56
00:02:23,689 --> 00:02:29,239
wanted to speed up the planning

57
00:02:25,850 --> 00:02:32,120
by offloading it to GPU unfortunately we

58
00:02:29,240 --> 00:02:34,580
can't no longer just MF the file we have

59
00:02:32,120 --> 00:02:38,030
to read the whole file into the CPU

60
00:02:34,580 --> 00:02:39,770
memory and then copy it to GPU and know

61
00:02:38,030 --> 00:02:42,830
that we have to repeat these two steps

62
00:02:39,770 --> 00:02:44,720
before each kernel activation regardless

63
00:02:42,830 --> 00:02:47,000
of the fact that perhaps not even the

64
00:02:44,720 --> 00:02:50,900
whole file is required by the planning

65
00:02:47,000 --> 00:02:53,780
service so the concludes that the

66
00:02:50,900 --> 00:02:57,020
broader example of some cpu application

67
00:02:53,780 --> 00:02:58,310
sharing file with multiple GPUs and each

68
00:02:57,020 --> 00:03:00,860
of them is working on a different

69
00:02:58,310 --> 00:03:02,960
portion of the file but which portion of

70
00:03:00,860 --> 00:03:05,900
the is that is calculation dependent

71
00:03:02,960 --> 00:03:08,780
source application just M Maps the file

72
00:03:05,900 --> 00:03:11,780
as before but for the GPU would have to

73
00:03:08,780 --> 00:03:14,360
copy the whole file into each of the GPU

74
00:03:11,780 --> 00:03:17,390
memories so the conclusion is that

75
00:03:14,360 --> 00:03:19,820
without OS management of GPU memory we

76
00:03:17,390 --> 00:03:22,609
can't have data dependent GPU access to

77
00:03:19,820 --> 00:03:25,010
files nor can we efficiently share files

78
00:03:22,610 --> 00:03:27,290
between the CPU and the GPU and as a

79
00:03:25,010 --> 00:03:29,959
result we have to repeatedly copy the

80
00:03:27,290 --> 00:03:32,600
whole file into each of the GPU memories

81
00:03:29,960 --> 00:03:34,970
which is not just tiring but also pretty

82
00:03:32,600 --> 00:03:36,350
and efficient right so in order to

83
00:03:34,970 --> 00:03:38,960
overcome this limitation and

84
00:03:36,350 --> 00:03:41,030
architectural changes required we need a

85
00:03:38,960 --> 00:03:43,820
tighter integration between the GPU

86
00:03:41,030 --> 00:03:46,550
memory and the host OS and from this

87
00:03:43,820 --> 00:03:48,590
understanding came gaya a distributed

88
00:03:46,550 --> 00:03:51,680
weekly consistent page cache for

89
00:03:48,590 --> 00:03:54,830
regional systems with gaya will enable

90
00:03:51,680 --> 00:03:56,480
memory mapped files for GPU kernels with

91
00:03:54,830 --> 00:04:00,590
the port efficient file sharing between

92
00:03:56,480 --> 00:04:03,140
the CPU and the GPU and we enable CPU

93
00:04:00,590 --> 00:04:04,760
and GPU AI optimizations that were just

94
00:04:03,140 --> 00:04:07,970
not possible in the current system

95
00:04:04,760 --> 00:04:10,310
design it turned out we had some really

96
00:04:07,970 --> 00:04:12,050
interesting challenges ahead of us we

97
00:04:10,310 --> 00:04:14,270
had to decide the consistency model

98
00:04:12,050 --> 00:04:16,790
suitable for the new system and we had

99
00:04:14,270 --> 00:04:19,579
to understand how it will all integrate

100
00:04:16,790 --> 00:04:22,190
both with the GPU driver and with the

101
00:04:19,579 --> 00:04:24,320
host OS but today we'll talk only about

102
00:04:22,190 --> 00:04:25,850
the consistency model we chose and you

103
00:04:24,320 --> 00:04:28,640
are welcome to read the paper for more

104
00:04:25,850 --> 00:04:31,700
details so let's take a look at our

105
00:04:28,640 --> 00:04:34,190
system with the unified page cache what

106
00:04:31,700 --> 00:04:36,440
we actually created is the distributed

107
00:04:34,190 --> 00:04:39,740
shared memory in which the same file

108
00:04:36,440 --> 00:04:42,400
data can now reside in both the CP

109
00:04:39,740 --> 00:04:45,710
you and the GPU memory and not only that

110
00:04:42,400 --> 00:04:48,409
now both the CPU and the GPU can update

111
00:04:45,710 --> 00:04:49,460
the file in parallel so what do we do

112
00:04:48,410 --> 00:04:52,069
about consistency

113
00:04:49,460 --> 00:04:54,049
for example if the CPU update the file

114
00:04:52,069 --> 00:04:57,289
when will the GPU become aware of the

115
00:04:54,050 --> 00:04:59,780
change let's start by focusing on the

116
00:04:57,289 --> 00:05:04,039
GPU side of the system and talk a bit

117
00:04:59,780 --> 00:05:06,289
about unified memory and NVIDIA GPUs so

118
00:05:04,039 --> 00:05:08,870
in the recent GPUs Nvidia enabled the

119
00:05:06,289 --> 00:05:12,229
same pointer to be accessed by both CPU

120
00:05:08,870 --> 00:05:14,330
and the GPU and implemented by utilizing

121
00:05:12,229 --> 00:05:16,900
the page fault mechanism to trigger the

122
00:05:14,330 --> 00:05:19,490
data migration between the two memories

123
00:05:16,900 --> 00:05:22,460
but now if the CPU and the GPU can

124
00:05:19,490 --> 00:05:24,199
access the same pointer Nvidia also had

125
00:05:22,460 --> 00:05:27,380
to address the consistency question

126
00:05:24,199 --> 00:05:30,069
right and the consistency model Nvidia

127
00:05:27,380 --> 00:05:33,340
chose is the one of strict quadrants in

128
00:05:30,069 --> 00:05:36,080
64k granularity which is one GPU page

129
00:05:33,340 --> 00:05:39,080
according to this model the same page

130
00:05:36,080 --> 00:05:42,380
can be mapped by only one processor at

131
00:05:39,080 --> 00:05:44,900
any given moment so for example if the

132
00:05:42,380 --> 00:05:47,479
page now resides in CPU memory and the

133
00:05:44,900 --> 00:05:48,229
GPU requires it the GPU will trigger a

134
00:05:47,479 --> 00:05:50,060
page fault

135
00:05:48,229 --> 00:05:52,820
which will result in the page being

136
00:05:50,060 --> 00:05:55,039
migrated to GPU memory and if the CPU

137
00:05:52,820 --> 00:05:57,289
needs the page next it will also raise

138
00:05:55,039 --> 00:06:00,710
the page fault and the page might may

139
00:05:57,289 --> 00:06:03,080
will be migrated back to CPU so as you

140
00:06:00,710 --> 00:06:05,539
can see if the CPU and the GPU are right

141
00:06:03,080 --> 00:06:07,969
sharing the page it results in a

142
00:06:05,539 --> 00:06:11,419
constant ping-pong covet between the two

143
00:06:07,969 --> 00:06:14,870
memories but what if the CPU and the GPU

144
00:06:11,419 --> 00:06:18,560
are actually false sharing the page for

145
00:06:14,870 --> 00:06:21,919
example the CPU writes to the lower 32 K

146
00:06:18,560 --> 00:06:23,330
and the GPU writes to the upper 32 K in

147
00:06:21,919 --> 00:06:25,669
theory they could have worked in

148
00:06:23,330 --> 00:06:29,030
parallel right if not for the strict

149
00:06:25,669 --> 00:06:31,008
currency limitations false sharing is

150
00:06:29,030 --> 00:06:33,679
actually a known issue with you VM and

151
00:06:31,009 --> 00:06:36,949
the official advice from Nvidia in such

152
00:06:33,680 --> 00:06:39,949
cases is to avoid them by having the two

153
00:06:36,949 --> 00:06:43,610
processors work each on a private buffer

154
00:06:39,949 --> 00:06:47,090
and to merge the output at the end so we

155
00:06:43,610 --> 00:06:49,669
did a small test to evaluate how false

156
00:06:47,090 --> 00:06:53,210
sharing affects performance we had two

157
00:06:49,669 --> 00:06:55,940
GPUs ride-sharing 64 K buffer

158
00:06:53,210 --> 00:06:57,500
and we compare this to the official

159
00:06:55,940 --> 00:06:59,600
advice from Nvidia

160
00:06:57,500 --> 00:07:01,790
in which the true GP used were each

161
00:06:59,600 --> 00:07:05,030
working on a private copy of the buffer

162
00:07:01,790 --> 00:07:07,340
and we merge the result at the end so

163
00:07:05,030 --> 00:07:09,739
what we did is we varied the number of

164
00:07:07,340 --> 00:07:12,710
loop iterations performed by each GPU

165
00:07:09,740 --> 00:07:15,320
and by doing so we actually varied the

166
00:07:12,710 --> 00:07:19,039
number of page migrations and this is

167
00:07:15,320 --> 00:07:23,030
what you can see on the x-axis y-axis is

168
00:07:19,040 --> 00:07:25,640
x the slowdown compared to our baseline

169
00:07:23,030 --> 00:07:30,020
in which the two GPUs are each working

170
00:07:25,640 --> 00:07:32,150
on a private buffer so we can easily see

171
00:07:30,020 --> 00:07:34,580
that the slowdown due to false sharing

172
00:07:32,150 --> 00:07:37,340
increases linearly with the number of

173
00:07:34,580 --> 00:07:40,130
page migrations which is pretty expected

174
00:07:37,340 --> 00:07:42,500
right but the slowdown is not the only

175
00:07:40,130 --> 00:07:45,140
side effect of false sharing it turned

176
00:07:42,500 --> 00:07:48,080
out that the false sharing effects v M

177
00:07:45,140 --> 00:07:50,900
as a whole we did another test where we

178
00:07:48,080 --> 00:07:54,260
took a CPU only benchmark called k-means

179
00:07:50,900 --> 00:07:56,900
comes from the Phoenix test with and we

180
00:07:54,260 --> 00:08:00,469
limited it to run off on four of our

181
00:07:56,900 --> 00:08:02,870
system cores in parallel to it we ran

182
00:08:00,470 --> 00:08:04,910
our full sharing benchmark and we

183
00:08:02,870 --> 00:08:06,680
allocated the remaining two cores for

184
00:08:04,910 --> 00:08:08,920
GPU management which is a common

185
00:08:06,680 --> 00:08:12,980
practice with GPU accelerated workloads

186
00:08:08,920 --> 00:08:15,470
so we ran two tests in the first test we

187
00:08:12,980 --> 00:08:17,960
just ran our k-means on the for course

188
00:08:15,470 --> 00:08:21,130
we allocated for it and we measured its

189
00:08:17,960 --> 00:08:24,440
run time of course is constant right

190
00:08:21,130 --> 00:08:27,650
what we did next is we ran our full

191
00:08:24,440 --> 00:08:30,380
sharing in parallel to K means now what

192
00:08:27,650 --> 00:08:32,929
we expected to see is that came in run

193
00:08:30,380 --> 00:08:35,289
time will remain constant as well right

194
00:08:32,929 --> 00:08:38,209
because it's running on different calls

195
00:08:35,289 --> 00:08:40,549
but to our surprise this was not the

196
00:08:38,210 --> 00:08:43,670
case and performance isolation was

197
00:08:40,549 --> 00:08:45,050
affected beautiful sharing so this

198
00:08:43,669 --> 00:08:47,180
brought us to an conclusion that a

199
00:08:45,050 --> 00:08:50,209
different consistency model is required

200
00:08:47,180 --> 00:08:52,880
and the one we chose is the one of laser

201
00:08:50,210 --> 00:08:55,310
release consistency in the paper we

202
00:08:52,880 --> 00:08:58,970
discussed several alternatives which I

203
00:08:55,310 --> 00:09:03,500
want to go over today so laser release

204
00:08:58,970 --> 00:09:06,769
consistency defines tube to consistency

205
00:09:03,500 --> 00:09:08,920
operations called acquire and release

206
00:09:06,769 --> 00:09:11,569
and the general rule is that

207
00:09:08,920 --> 00:09:14,229
modifications to share object will

208
00:09:11,569 --> 00:09:18,319
become visible only after the writer

209
00:09:14,230 --> 00:09:20,059
releases and the reader acquires so

210
00:09:18,319 --> 00:09:22,368
let's take a look an example of two

211
00:09:20,059 --> 00:09:25,339
writers sharing the same file

212
00:09:22,369 --> 00:09:27,799
they both acquire the first writer write

213
00:09:25,339 --> 00:09:29,899
something to offset an and in parallel

214
00:09:27,799 --> 00:09:33,049
to it the second writer also acquires

215
00:09:29,899 --> 00:09:36,230
and writes something to officer 20 next

216
00:09:33,049 --> 00:09:38,839
they both release and by releasing they

217
00:09:36,230 --> 00:09:41,480
actually notify the other that the file

218
00:09:38,839 --> 00:09:43,939
changed so the next time the first

219
00:09:41,480 --> 00:09:47,239
writer for example tries to access the

220
00:09:43,939 --> 00:09:49,969
file it will issue an acquire which will

221
00:09:47,239 --> 00:09:52,489
result in the data being propagated from

222
00:09:49,970 --> 00:09:55,399
the second writer and nourished with the

223
00:09:52,489 --> 00:09:58,129
local changes now you might have already

224
00:09:55,399 --> 00:10:01,670
noticed a small issue with this protocol

225
00:09:58,129 --> 00:10:05,089
if one of the writers is a CPU because

226
00:10:01,670 --> 00:10:07,939
we are now actually forcing the CPU to

227
00:10:05,089 --> 00:10:12,110
issue some consistency operations before

228
00:10:07,939 --> 00:10:15,199
each access to files right and this is

229
00:10:12,110 --> 00:10:17,740
of course a no-go so we have to somehow

230
00:10:15,199 --> 00:10:20,779
make our synchronization operations

231
00:10:17,740 --> 00:10:22,759
transparent to cpu applications to keep

232
00:10:20,779 --> 00:10:25,579
them unaware of the weekly consistent

233
00:10:22,759 --> 00:10:28,579
page cache and we do this by limiting

234
00:10:25,579 --> 00:10:31,489
the consistency control to the CPU code

235
00:10:28,579 --> 00:10:35,229
that invokes GPU kernels now how do we

236
00:10:31,490 --> 00:10:37,639
do that the idea here lies in the

237
00:10:35,230 --> 00:10:39,410
understanding of what do we want to

238
00:10:37,639 --> 00:10:41,749
happen on the acquire and release

239
00:10:39,410 --> 00:10:43,730
sequence and the purpose of the

240
00:10:41,749 --> 00:10:47,029
acquiring release is basically to make

241
00:10:43,730 --> 00:10:49,369
sure that the next reader knows from

242
00:10:47,029 --> 00:10:52,519
where to get the latest version of the

243
00:10:49,369 --> 00:10:55,459
data from so keeping that in mind we can

244
00:10:52,519 --> 00:10:58,850
play with when the acquire of the CPU

245
00:10:55,459 --> 00:11:01,518
occurs and move it back to when the GPU

246
00:10:58,850 --> 00:11:04,369
releases and the similar trick can be

247
00:11:01,519 --> 00:11:07,699
applied to CPU release by moving it to

248
00:11:04,369 --> 00:11:08,929
when the GPU acquires and this way we

249
00:11:07,699 --> 00:11:11,269
made the synchronization operations

250
00:11:08,929 --> 00:11:13,579
compare until a transfer to CPU

251
00:11:11,269 --> 00:11:15,679
processes now let's put it all together

252
00:11:13,579 --> 00:11:18,040
and look how our code will look with

253
00:11:15,679 --> 00:11:20,270
with road navigation will look with gaya

254
00:11:18,040 --> 00:11:22,400
we can now just M

255
00:11:20,270 --> 00:11:23,990
the file and pass the pointer to the GPU

256
00:11:22,400 --> 00:11:27,260
colonel and that's it

257
00:11:23,990 --> 00:11:29,240
all the synchronization disk access the

258
00:11:27,260 --> 00:11:30,920
copy to the between the memories is done

259
00:11:29,240 --> 00:11:34,430
transparently for us by the operating

260
00:11:30,920 --> 00:11:36,740
system we have several more interesting

261
00:11:34,430 --> 00:11:39,380
features that enable us to improve CPU

262
00:11:36,740 --> 00:11:41,240
and GPU i/o in the paper as well as

263
00:11:39,380 --> 00:11:43,790
implementation details we didn't go over

264
00:11:41,240 --> 00:11:46,160
today and this brings us to evaluation

265
00:11:43,790 --> 00:11:48,740
in the paper you will find detailed

266
00:11:46,160 --> 00:11:51,770
analysis of overheads added by gaya in

267
00:11:48,740 --> 00:11:53,900
as well as benchmark results including

268
00:11:51,770 --> 00:11:57,260
closed source libraries like coolest and

269
00:11:53,900 --> 00:12:00,199
today we will go over two of the three

270
00:11:57,260 --> 00:12:01,939
real life applications we used so the

271
00:12:00,200 --> 00:12:04,580
first one is microscope image stitching

272
00:12:01,940 --> 00:12:06,380
when I take a picture with a microscope

273
00:12:04,580 --> 00:12:08,660
you'll get a mosaic of small images

274
00:12:06,380 --> 00:12:10,820
overlapping with each other and you need

275
00:12:08,660 --> 00:12:13,579
to post process those images in order to

276
00:12:10,820 --> 00:12:15,080
get the original original shot now this

277
00:12:13,580 --> 00:12:17,750
post processing was efficiently

278
00:12:15,080 --> 00:12:20,360
implemented in by the help of GPUs and

279
00:12:17,750 --> 00:12:22,850
we hence this process even further by

280
00:12:20,360 --> 00:12:25,880
having the CPU work in parallel on half

281
00:12:22,850 --> 00:12:28,160
of the output image so we compare three

282
00:12:25,880 --> 00:12:30,320
different implementations in the first

283
00:12:28,160 --> 00:12:33,230
one we have the CPU and the GPU each

284
00:12:30,320 --> 00:12:37,700
working on a private buffer and we merge

285
00:12:33,230 --> 00:12:40,390
the result at the end and the next

286
00:12:37,700 --> 00:12:42,800
become we implement this with uvm in

287
00:12:40,390 --> 00:12:44,960
unified virtual memory in which the CPU

288
00:12:42,800 --> 00:12:46,939
and the GPU right share the output

289
00:12:44,960 --> 00:12:49,190
buffer and as we saw earlier in this

290
00:12:46,940 --> 00:12:51,920
case the experience Falls sharing which

291
00:12:49,190 --> 00:12:53,840
results in 31 perform percent

292
00:12:51,920 --> 00:12:56,479
performance degradation compared to our

293
00:12:53,840 --> 00:12:58,460
previous implementation and last we

294
00:12:56,480 --> 00:13:01,190
implemented with guy in which we just M

295
00:12:58,460 --> 00:13:04,430
mapped the file and the CPU and GPU work

296
00:13:01,190 --> 00:13:06,230
in parallel as you can see with guy we

297
00:13:04,430 --> 00:13:08,569
were able to completely eliminate the

298
00:13:06,230 --> 00:13:10,780
false sharing effect without having to

299
00:13:08,570 --> 00:13:13,340
merge the output file at the end and

300
00:13:10,780 --> 00:13:15,170
we'll of course conclude with our road

301
00:13:13,340 --> 00:13:16,790
navigation service that been following

302
00:13:15,170 --> 00:13:19,189
us throughout this whole talk we

303
00:13:16,790 --> 00:13:20,839
implemented it using gang Rock which is

304
00:13:19,190 --> 00:13:23,120
the open source library for graph

305
00:13:20,840 --> 00:13:26,360
processing and it implements the

306
00:13:23,120 --> 00:13:28,940
shortest paths application we had two

307
00:13:26,360 --> 00:13:31,130
processes one of them calculating the

308
00:13:28,940 --> 00:13:33,470
shortest path on the GPU and the other

309
00:13:31,130 --> 00:13:33,980
was the legacy CPU process periodically

310
00:13:33,470 --> 00:13:36,560
up

311
00:13:33,980 --> 00:13:38,750
the weights on the graph and we have had

312
00:13:36,560 --> 00:13:40,430
the two processes interleaved and we

313
00:13:38,750 --> 00:13:42,860
compare three different implementations

314
00:13:40,430 --> 00:13:45,020
the first one is CUDA in which before

315
00:13:42,860 --> 00:13:47,240
each kernel activation we had to read

316
00:13:45,020 --> 00:13:50,030
the whole file and copy it to GPU memory

317
00:13:47,240 --> 00:13:51,890
the second one is UVM in which before

318
00:13:50,030 --> 00:13:54,290
each kernel activation we had to read

319
00:13:51,890 --> 00:13:56,569
the whole file but the copy to GPU was

320
00:13:54,290 --> 00:13:57,349
done transparently for us and the last

321
00:13:56,570 --> 00:13:59,930
one is gaya

322
00:13:57,350 --> 00:14:01,820
in which we just M mapped the file and

323
00:13:59,930 --> 00:14:05,209
made sure to acquire and release before

324
00:14:01,820 --> 00:14:07,880
kernel activation so we varied the

325
00:14:05,210 --> 00:14:10,670
portion of the file changed by a legacy

326
00:14:07,880 --> 00:14:15,290
CPU process and this is what you see on

327
00:14:10,670 --> 00:14:16,880
the x-axis and Y is the test duration as

328
00:14:15,290 --> 00:14:19,069
expected both you VM and CUDA

329
00:14:16,880 --> 00:14:20,870
implementations are not affected by the

330
00:14:19,070 --> 00:14:22,700
portion of the file change because prior

331
00:14:20,870 --> 00:14:25,760
to each activation we still have to read

332
00:14:22,700 --> 00:14:29,990
the whole file but with gaya we were

333
00:14:25,760 --> 00:14:32,510
able to get up to eight times speed up

334
00:14:29,990 --> 00:14:36,650
because the unchanged pages remained

335
00:14:32,510 --> 00:14:38,960
cached in GPU memory to conclude we

336
00:14:36,650 --> 00:14:41,300
extended the OS page cache into GPU

337
00:14:38,960 --> 00:14:43,790
memory by implementing laser release

338
00:14:41,300 --> 00:14:46,099
consistency model and eliminating false

339
00:14:43,790 --> 00:14:48,709
sharing effect we added support for

340
00:14:46,100 --> 00:14:51,380
memory mapped files for GPUs and all of

341
00:14:48,710 --> 00:14:53,930
our changes remain transparent to legacy

342
00:14:51,380 --> 00:14:56,810
CPU processes and compatible van with

343
00:14:53,930 --> 00:14:58,459
unmodified GPU kernels and last but not

344
00:14:56,810 --> 00:15:00,739
least although unfortunately we had no

345
00:14:58,460 --> 00:15:03,710
time showing that we were able to

346
00:15:00,740 --> 00:15:07,420
improve IO performance for CPU legacy

347
00:15:03,710 --> 00:15:10,520
processes by caching data in GPU memory

348
00:15:07,420 --> 00:15:13,430
so all of our code is open source and

349
00:15:10,520 --> 00:15:14,900
you can find it online thank you for

350
00:15:13,430 --> 00:15:16,569
listening and I will be happy to take

351
00:15:14,900 --> 00:15:23,358
your questions now

352
00:15:16,570 --> 00:15:23,359
[Applause]

353
00:15:24,600 --> 00:15:30,640
let me ask you a question

354
00:15:26,470 --> 00:15:32,250
so imagine the hardware architects

355
00:15:30,640 --> 00:15:34,300
decide to come up with a new set of

356
00:15:32,250 --> 00:15:37,390
primitives and provide coherence between

357
00:15:34,300 --> 00:15:40,060
CPU and GPU and obviously this is coming

358
00:15:37,390 --> 00:15:42,150
if not already there how will it change

359
00:15:40,060 --> 00:15:45,880
your work

360
00:15:42,150 --> 00:15:48,910
well actually the idea of the unified

361
00:15:45,880 --> 00:15:50,680
page cache still remains valid but I

362
00:15:48,910 --> 00:15:53,079
agreed that the laser release

363
00:15:50,680 --> 00:15:55,089
consistency model won't be required

364
00:15:53,080 --> 00:15:57,910
because as your self currency will be

365
00:15:55,089 --> 00:16:03,160
handled by the hardware but this thing

366
00:15:57,910 --> 00:16:05,980
is this work is the we aimed it for

367
00:16:03,160 --> 00:16:11,529
providing a solution for this screed

368
00:16:05,980 --> 00:16:13,180
GPUs in x86 systems and we don't see the

369
00:16:11,529 --> 00:16:16,689
hardware currency becoming available

370
00:16:13,180 --> 00:16:22,319
anytime soon in the systems while Gaia

371
00:16:16,690 --> 00:16:25,470
is thank you thank the speaker again

372
00:16:22,320 --> 00:16:25,470
thank you

