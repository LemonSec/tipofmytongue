1
00:00:10,200 --> 00:00:14,410
thank you very for the introduction and

2
00:00:12,789 --> 00:00:16,329
thank you all for being here this is a

3
00:00:14,410 --> 00:00:19,300
talk on multi cue fair queuing which is

4
00:00:16,329 --> 00:00:21,759
a scalable i/o scheduler for fast multi

5
00:00:19,300 --> 00:00:24,099
cue devices it's a collaboration with

6
00:00:21,759 --> 00:00:26,410
caution from Google Michael Scott from

7
00:00:24,099 --> 00:00:29,739
University of Rochester and Mike Marty

8
00:00:26,410 --> 00:00:31,719
from Google in a conventional i/o space

9
00:00:29,739 --> 00:00:34,629
design processes would submit the

10
00:00:31,719 --> 00:00:36,789
request to OS which in a centralized IO

11
00:00:34,629 --> 00:00:38,949
scheduler reorders and dispatches

12
00:00:36,789 --> 00:00:42,429
request to guarantee properties like

13
00:00:38,950 --> 00:00:44,289
fairness as i/o devices like networked

14
00:00:42,429 --> 00:00:45,940
storage and accelerators become faster

15
00:00:44,289 --> 00:00:50,469
reaching throughputs of millions of

16
00:00:45,940 --> 00:00:52,718
hours per second and with the increase

17
00:00:50,469 --> 00:00:55,030
in the number of cores per package the

18
00:00:52,719 --> 00:00:58,120
centralized scheduler cannot scale

19
00:00:55,030 --> 00:00:59,649
anymore for device supporting millions

20
00:00:58,120 --> 00:01:02,678
of IUP's per second the scheduler needs

21
00:00:59,649 --> 00:01:04,780
to dispatch in new requests at intervals

22
00:01:02,679 --> 00:01:06,909
less than a microsecond the question is

23
00:01:04,780 --> 00:01:08,710
what can you do in less than a

24
00:01:06,909 --> 00:01:11,440
microsecond assuming roughly 100

25
00:01:08,710 --> 00:01:14,979
nanosecond for a remote cache miss this

26
00:01:11,440 --> 00:01:16,929
is barely sufficient for 10 cross socket

27
00:01:14,980 --> 00:01:18,940
coherence misses with single core

28
00:01:16,930 --> 00:01:21,510
performance saying relatively flat and

29
00:01:18,940 --> 00:01:23,890
device speeds continuing to increase

30
00:01:21,510 --> 00:01:25,840
serializing requests that these rates

31
00:01:23,890 --> 00:01:28,240
will only become harder and more

32
00:01:25,840 --> 00:01:31,150
challenging in the future due to these

33
00:01:28,240 --> 00:01:33,970
reasons IO design has moved toward multi

34
00:01:31,150 --> 00:01:37,630
cue architecture in which each Hardware

35
00:01:33,970 --> 00:01:41,500
thread owns the dedicated irq directly

36
00:01:37,630 --> 00:01:43,539
exposed to a device in this design each

37
00:01:41,500 --> 00:01:45,520
thread communicates with the device

38
00:01:43,540 --> 00:01:49,540
independent of other threads in carrying

39
00:01:45,520 --> 00:01:51,880
no cross CPU communication or cache

40
00:01:49,540 --> 00:01:54,040
coherence traffic it looks as multi cue

41
00:01:51,880 --> 00:01:55,990
block layer is one example of such shift

42
00:01:54,040 --> 00:01:59,380
but you can see this pattern and this

43
00:01:55,990 --> 00:02:01,869
shift in data plane networking and also

44
00:01:59,380 --> 00:02:03,970
in accelerators as a result the oil

45
00:02:01,870 --> 00:02:06,130
scheduler which was responsible for

46
00:02:03,970 --> 00:02:08,348
providing fairness in the conventional

47
00:02:06,130 --> 00:02:10,690
oil stack has been completely eliminated

48
00:02:08,348 --> 00:02:13,018
from this design so you don't get any

49
00:02:10,690 --> 00:02:16,090
fairness here

50
00:02:13,019 --> 00:02:18,520
while coherence free execution is

51
00:02:16,090 --> 00:02:20,670
fundamental to achieving good

52
00:02:18,520 --> 00:02:23,319
scalability and high throughput it

53
00:02:20,670 --> 00:02:26,349
introduces new challenges in preserve

54
00:02:23,319 --> 00:02:28,480
systemwide properties one such property

55
00:02:26,349 --> 00:02:30,790
is fairness with the focus of this work

56
00:02:28,480 --> 00:02:33,249
it is not straightforward how to

57
00:02:30,790 --> 00:02:36,548
guarantee global fairness without global

58
00:02:33,249 --> 00:02:40,030
communication which multi qio design

59
00:02:36,549 --> 00:02:42,879
aims to minimize so how can we achieve

60
00:02:40,030 --> 00:02:44,769
fairness without totally negating the

61
00:02:42,879 --> 00:02:46,899
scalability offered by multi cue io

62
00:02:44,769 --> 00:02:49,659
design this is the main focus of this

63
00:02:46,900 --> 00:02:51,549
work and in next slide we are going to

64
00:02:49,659 --> 00:02:55,358
show how to reconcile these two

65
00:02:51,549 --> 00:02:57,370
seemingly contradictory goals for the

66
00:02:55,359 --> 00:02:58,569
rest of the talk first we describe the

67
00:02:57,370 --> 00:03:01,150
high-level design of multi cue

68
00:02:58,569 --> 00:03:02,888
fracturing then we show how multi cue

69
00:03:01,150 --> 00:03:05,769
fair queuing can be implemented using

70
00:03:02,889 --> 00:03:08,379
scalable data structures then be sure

71
00:03:05,769 --> 00:03:11,319
experimental results show basically

72
00:03:08,379 --> 00:03:14,189
employing multi-threading to provide

73
00:03:11,319 --> 00:03:16,358
fairness for real-world applications and

74
00:03:14,189 --> 00:03:19,418
finally we conclude and discuss future

75
00:03:16,359 --> 00:03:21,760
directions among the existing notions of

76
00:03:19,419 --> 00:03:24,189
fairness we chose to design our own

77
00:03:21,760 --> 00:03:26,409
scheduler based on the fair queuing

78
00:03:24,189 --> 00:03:30,099
class of algorithms freakin originates

79
00:03:26,409 --> 00:03:32,590
from Network packet scheduling and has

80
00:03:30,099 --> 00:03:34,929
several desirable properties that makes

81
00:03:32,590 --> 00:03:37,419
the prominent choice for implementing

82
00:03:34,930 --> 00:03:39,099
oil schedulers in particular free

83
00:03:37,419 --> 00:03:41,099
queuing algorithm support proportional

84
00:03:39,099 --> 00:03:43,750
share based on the weight of the flows

85
00:03:41,099 --> 00:03:45,668
they are work conserving meaning as long

86
00:03:43,750 --> 00:03:47,709
as you have requests in the system the

87
00:03:45,669 --> 00:03:51,519
device is not going to be idle they

88
00:03:47,709 --> 00:03:53,889
don't allow underutilizing tasks to

89
00:03:51,519 --> 00:03:55,239
accumulate their share and use it in a

90
00:03:53,889 --> 00:03:58,000
sudden burst possibly causing

91
00:03:55,239 --> 00:04:01,150
significant latencies to other users and

92
00:03:58,000 --> 00:04:04,209
finally they provide fairness guarantees

93
00:04:01,150 --> 00:04:05,859
provable fairness guarantees in multi

94
00:04:04,209 --> 00:04:07,959
queue fair queuing we want to keep all

95
00:04:05,859 --> 00:04:09,909
of these properties but we also need to

96
00:04:07,959 --> 00:04:12,159
support parallel dispatch to exploit the

97
00:04:09,909 --> 00:04:14,638
internal parallelism of modern devices

98
00:04:12,159 --> 00:04:20,459
such as modern SSDs and accelerators

99
00:04:14,639 --> 00:04:22,690
specifically we designed em qfq based on

100
00:04:20,459 --> 00:04:25,060
existing fair queuing algorithm called

101
00:04:22,690 --> 00:04:28,030
SF QD or start time fair queuing the

102
00:04:25,060 --> 00:04:30,699
parallel dispatch roughly speaking and

103
00:04:28,030 --> 00:04:33,849
there are details in the paper upon

104
00:04:30,699 --> 00:04:36,099
arrival a request gets a start tag which

105
00:04:33,849 --> 00:04:36,729
is the accumulated resource usage of the

106
00:04:36,099 --> 00:04:38,319
task

107
00:04:36,729 --> 00:04:41,110
that submitted the request before we

108
00:04:38,319 --> 00:04:44,080
start servicing the request the request

109
00:04:41,110 --> 00:04:45,999
then are ordered by their start tag and

110
00:04:44,080 --> 00:04:48,308
dispatched up to the deputy to allow for

111
00:04:45,999 --> 00:04:50,529
parallelism all the requests based on

112
00:04:48,309 --> 00:04:52,930
this notion of start tag as opposed to

113
00:04:50,529 --> 00:04:54,099
the real a lot arrival time guarantees

114
00:04:52,930 --> 00:04:57,370
that each time that we are dispatching

115
00:04:54,099 --> 00:04:59,370
we are prioritizing the flow that has

116
00:04:57,370 --> 00:05:04,120
utilized the least amount of resource

117
00:04:59,370 --> 00:05:05,499
but one might ask why we just use SF QD

118
00:05:04,120 --> 00:05:07,990
directly to provide fairness for

119
00:05:05,499 --> 00:05:11,139
molecule Stax there are two main

120
00:05:07,990 --> 00:05:14,020
challenges that prevent exactly you D

121
00:05:11,139 --> 00:05:15,999
from scaling for fast devices and many

122
00:05:14,020 --> 00:05:17,758
CPUs so these are the challenges that

123
00:05:15,999 --> 00:05:20,409
marquee multi cue IO

124
00:05:17,759 --> 00:05:22,629
resolves first strictly ordering

125
00:05:20,409 --> 00:05:24,759
requests in a centralized queue is a

126
00:05:22,629 --> 00:05:27,430
significant point of contention and

127
00:05:24,759 --> 00:05:29,800
second s fqt need to track some global

128
00:05:27,430 --> 00:05:32,020
statistics such as the number of

129
00:05:29,800 --> 00:05:33,490
in-flight requests number of requests

130
00:05:32,020 --> 00:05:37,299
that has been dispatched but have not

131
00:05:33,490 --> 00:05:39,699
been completed that if you don't do it

132
00:05:37,300 --> 00:05:41,289
properly it can cause significant

133
00:05:39,699 --> 00:05:47,349
communication overhead in terms of

134
00:05:41,289 --> 00:05:49,419
coherence traffic so this we can shows

135
00:05:47,349 --> 00:05:51,639
simplifies but shows how is if QD is

136
00:05:49,419 --> 00:05:54,008
implemented requests are submitted by

137
00:05:51,639 --> 00:05:55,870
processes they are assigned a start tag

138
00:05:54,009 --> 00:05:57,899
and then sorted in a centralized queue

139
00:05:55,870 --> 00:06:00,749
to be dispatched in increasing order of

140
00:05:57,899 --> 00:06:02,740
Startech there's no wonder this

141
00:06:00,749 --> 00:06:05,589
implementation at this is not going to

142
00:06:02,740 --> 00:06:08,649
scale in modern systems we need to do

143
00:06:05,589 --> 00:06:10,300
millions of diffuse per second and the

144
00:06:08,649 --> 00:06:13,449
central queue is maintained by a large

145
00:06:10,300 --> 00:06:15,399
number of competing threads in multi Q

146
00:06:13,449 --> 00:06:17,860
fair queuing first we break this

147
00:06:15,399 --> 00:06:21,039
centralized queue to independent accuse

148
00:06:17,860 --> 00:06:23,529
each sorting local request in increasing

149
00:06:21,039 --> 00:06:25,830
order of startx we could maintain the

150
00:06:23,529 --> 00:06:28,629
same semantic assets of QT if we

151
00:06:25,830 --> 00:06:31,479
dispatch the smallest request across all

152
00:06:28,629 --> 00:06:33,459
of these cues but that serialization

153
00:06:31,479 --> 00:06:38,589
would basically negate any benefit that

154
00:06:33,459 --> 00:06:40,240
we would hope to get instead and to

155
00:06:38,589 --> 00:06:42,959
enable scalable implementation we

156
00:06:40,240 --> 00:06:45,490
introduce a parameter T which we call

157
00:06:42,959 --> 00:06:47,620
throttling threshold that allows the

158
00:06:45,490 --> 00:06:50,020
dispatches from different queues to

159
00:06:47,620 --> 00:06:52,900
basically commute within a window

160
00:06:50,020 --> 00:06:55,240
of T basically they can be dispatched

161
00:06:52,900 --> 00:06:57,068
out of order specifically each queue

162
00:06:55,240 --> 00:07:00,250
having because it has an independent

163
00:06:57,069 --> 00:07:02,500
access to the multi cue device can

164
00:07:00,250 --> 00:07:05,050
proceed to dispatch a request as long as

165
00:07:02,500 --> 00:07:07,090
the StarTAC of the that request is at

166
00:07:05,050 --> 00:07:10,659
most bidding the T of smallest request

167
00:07:07,090 --> 00:07:13,270
in the system a queue that finds itself

168
00:07:10,659 --> 00:07:15,849
violating this condition slops

169
00:07:13,270 --> 00:07:20,198
dispatching more requests and it's going

170
00:07:15,849 --> 00:07:23,460
to be throttled until a later time when

171
00:07:20,199 --> 00:07:28,060
the slowest queue catches up and

172
00:07:23,460 --> 00:07:34,479
basically notified notifies the throttle

173
00:07:28,060 --> 00:07:36,240
queue to resume this match we prove that

174
00:07:34,479 --> 00:07:39,610
this relaxation does not violate

175
00:07:36,240 --> 00:07:41,650
fairness specifically we show that the

176
00:07:39,610 --> 00:07:45,520
difference in service received by any

177
00:07:41,650 --> 00:07:47,380
tooth flow or any two tasks is always

178
00:07:45,520 --> 00:07:48,758
bounded and the bound is a function of

179
00:07:47,380 --> 00:07:50,530
the throttling threshold and the

180
00:07:48,759 --> 00:07:52,000
parallelism of the device we see the

181
00:07:50,530 --> 00:07:55,840
paper for the proof and the assumptions

182
00:07:52,000 --> 00:07:58,690
on the proof while allowing dispatch to

183
00:07:55,840 --> 00:08:02,109
commute and to be reordered within this

184
00:07:58,690 --> 00:08:04,240
window does remove an important obstacle

185
00:08:02,110 --> 00:08:06,580
in scalability we still need to maintain

186
00:08:04,240 --> 00:08:10,810
system-wide statistics that would

187
00:08:06,580 --> 00:08:12,460
inevitably cause coherence misses in

188
00:08:10,810 --> 00:08:15,099
particular molecule fair queuing needs

189
00:08:12,460 --> 00:08:17,799
to keep track of the slowest queue to

190
00:08:15,099 --> 00:08:20,169
and to achieve this we use a scalable

191
00:08:17,800 --> 00:08:23,400
concurrent data structure it's a lottery

192
00:08:20,169 --> 00:08:25,990
data structure called me indicator also

193
00:08:23,400 --> 00:08:28,508
multi-effect we needs to keep track of

194
00:08:25,990 --> 00:08:31,210
how many requests have been dispatched

195
00:08:28,509 --> 00:08:34,659
but not completed at any given point in

196
00:08:31,210 --> 00:08:37,598
time in order to prevent queue buildup

197
00:08:34,659 --> 00:08:39,458
in the device this is achieved through

198
00:08:37,599 --> 00:08:42,279
another scalable lock free data

199
00:08:39,458 --> 00:08:47,380
structure that we call token tree we

200
00:08:42,279 --> 00:08:49,959
also need to track metadata required for

201
00:08:47,380 --> 00:08:51,550
on throttling queues which we leave out

202
00:08:49,959 --> 00:08:57,279
of this presentation in the interest of

203
00:08:51,550 --> 00:09:00,010
time to give a high-level the big

204
00:08:57,279 --> 00:09:02,500
picture of the how we implement article

205
00:09:00,010 --> 00:09:03,850
for a queuing multi cue fracking's

206
00:09:02,500 --> 00:09:06,340
implementation realize

207
00:09:03,850 --> 00:09:08,590
localizing synchronization as much as

208
00:09:06,340 --> 00:09:11,680
possible to achieve scalability the main

209
00:09:08,590 --> 00:09:13,750
goal is to basically keep interactions

210
00:09:11,680 --> 00:09:16,359
at higher levels of memory hierarchy and

211
00:09:13,750 --> 00:09:18,940
avoid expensive coherence misses as much

212
00:09:16,360 --> 00:09:21,010
as possible we prefer to synchronize

213
00:09:18,940 --> 00:09:23,230
between sibling hyper threats before

214
00:09:21,010 --> 00:09:26,740
synchronizing at socket level and before

215
00:09:23,230 --> 00:09:31,150
modifying a global global access cache

216
00:09:26,740 --> 00:09:35,020
line I'm going to give an example of how

217
00:09:31,150 --> 00:09:36,850
in one instance of the lock tree data

218
00:09:35,020 --> 00:09:40,510
structures that we use which is token

219
00:09:36,850 --> 00:09:42,040
tree so as an example of how localizing

220
00:09:40,510 --> 00:09:44,140
synchronization in multi cue fair

221
00:09:42,040 --> 00:09:46,060
queuing works let's take a look at how

222
00:09:44,140 --> 00:09:47,949
we keep track of the number of a flight

223
00:09:46,060 --> 00:09:50,439
requests we introduce a tree-like data

224
00:09:47,950 --> 00:09:54,130
structure we call token tree where the

225
00:09:50,440 --> 00:09:56,020
sum of values of all nodes represent on

226
00:09:54,130 --> 00:09:59,200
utilize parallelism that is the

227
00:09:56,020 --> 00:10:01,360
available parallelism when there is no

228
00:09:59,200 --> 00:10:03,730
in-flight request or you know we can

229
00:10:01,360 --> 00:10:08,080
think at the beginning before we start

230
00:10:03,730 --> 00:10:09,940
it can be thought of to have D tokens in

231
00:10:08,080 --> 00:10:12,310
the root node when a queue needs to

232
00:10:09,940 --> 00:10:14,020
dispatch a request it borrows tokens

233
00:10:12,310 --> 00:10:16,839
from its parent going upward toward the

234
00:10:14,020 --> 00:10:20,740
root if no token is available like in

235
00:10:16,840 --> 00:10:22,950
this example for CPU and CPU five the

236
00:10:20,740 --> 00:10:28,000
queue needs to be throttled when a queue

237
00:10:22,950 --> 00:10:30,100
say for example here for CPU zero has a

238
00:10:28,000 --> 00:10:33,040
request completed it will have an

239
00:10:30,100 --> 00:10:35,740
additional token to be used let's assume

240
00:10:33,040 --> 00:10:37,240
CP zero doesn't want to reuse this token

241
00:10:35,740 --> 00:10:39,400
because it has it doesn't have any other

242
00:10:37,240 --> 00:10:42,790
request to dispatch then it has to one

243
00:10:39,400 --> 00:10:44,829
throttle either CPU 2 or CPU 5 to

244
00:10:42,790 --> 00:10:48,160
localized interaction it would decide to

245
00:10:44,830 --> 00:10:52,350
on throttle CPU 2 it will return the

246
00:10:48,160 --> 00:10:57,939
available token to their least common

247
00:10:52,350 --> 00:11:01,440
ancestor and node and send an intra

248
00:10:57,940 --> 00:11:03,910
processing drop to an throttle CPU to

249
00:11:01,440 --> 00:11:06,520
note that this choice reduces the

250
00:11:03,910 --> 00:11:08,140
latency of the notification but it also

251
00:11:06,520 --> 00:11:10,630
passes the token in the last level cache

252
00:11:08,140 --> 00:11:13,600
of the socket 0 without causing cross

253
00:11:10,630 --> 00:11:17,050
socket coherence misses now that we have

254
00:11:13,600 --> 00:11:18,000
an idea about how molecule multi-core

255
00:11:17,050 --> 00:11:20,620
occurring is this

256
00:11:18,000 --> 00:11:23,260
implemented let's see how it performs in

257
00:11:20,620 --> 00:11:25,029
practice in order to evaluate the

258
00:11:23,260 --> 00:11:28,329
fairness and scalability of multi cue

259
00:11:25,029 --> 00:11:32,800
factoring we implemented it as Linux IO

260
00:11:28,329 --> 00:11:35,609
scheduler for multi cue block layer we

261
00:11:32,800 --> 00:11:39,160
benchmark multi cue fair queuing over an

262
00:11:35,610 --> 00:11:44,260
nvme SSD capable of half a million jobs

263
00:11:39,160 --> 00:11:46,899
per second an nvme or nvme over our DMA

264
00:11:44,260 --> 00:11:50,050
capable of nearly 4 million iOS per

265
00:11:46,899 --> 00:11:52,089
second we use flexible i/o benchmark to

266
00:11:50,050 --> 00:11:53,709
create workloads that we know it's

267
00:11:52,089 --> 00:11:56,500
characteristics such as i/o death

268
00:11:53,709 --> 00:11:58,239
concurrency size and we also use

269
00:11:56,500 --> 00:11:59,560
aerospike key value store configure to

270
00:11:58,240 --> 00:12:02,589
respond all of the requests from the

271
00:11:59,560 --> 00:12:05,349
storage and flash extrav processing

272
00:12:02,589 --> 00:12:08,709
framework which can do graph computation

273
00:12:05,350 --> 00:12:11,260
on large graphs that don't fit in the

274
00:12:08,709 --> 00:12:12,939
main memory we compare multi cue fair

275
00:12:11,260 --> 00:12:15,220
queuing against budget fair queuing

276
00:12:12,940 --> 00:12:18,730
which is the available proportional

277
00:12:15,220 --> 00:12:22,630
share scheduler for multi cue block

278
00:12:18,730 --> 00:12:24,880
layer on linux bfq does not support

279
00:12:22,630 --> 00:12:26,459
parallel dispatch and is sourced all all

280
00:12:24,880 --> 00:12:29,079
of the requests in a central queue

281
00:12:26,459 --> 00:12:33,930
before dispatching therefore it's a

282
00:12:29,079 --> 00:12:38,649
there's a scalability limitation for bfq

283
00:12:33,930 --> 00:12:40,839
let's look at multi cue fair queuing

284
00:12:38,649 --> 00:12:43,959
deployed for fairness in real

285
00:12:40,839 --> 00:12:47,070
applications on the left the blue bar

286
00:12:43,959 --> 00:12:50,410
shows the PageRank algorithm on flash

287
00:12:47,070 --> 00:12:52,540
expressing framework and on the right we

288
00:12:50,410 --> 00:12:56,890
have errors by key value store benchmark

289
00:12:52,540 --> 00:13:00,250
with y CSP the orange bars in both cases

290
00:12:56,890 --> 00:13:02,529
or flexible i/o benchmark is a synthetic

291
00:13:00,250 --> 00:13:04,269
benchmark as an antagonist process that

292
00:13:02,529 --> 00:13:07,620
can saturate the device all by itself

293
00:13:04,269 --> 00:13:10,209
and the x-axis shows the slowdown of

294
00:13:07,620 --> 00:13:12,880
each of the applications compared to

295
00:13:10,209 --> 00:13:14,589
when we run them alone a 2x is slowed

296
00:13:12,880 --> 00:13:17,319
down because we assume that the

297
00:13:14,589 --> 00:13:19,600
processes have the same way the to a 2x

298
00:13:17,319 --> 00:13:23,709
slowdown is a fair slowdown in this

299
00:13:19,600 --> 00:13:26,230
situation when no scheduler is employed

300
00:13:23,709 --> 00:13:28,329
you can see that both flash x on the

301
00:13:26,230 --> 00:13:31,779
left and aerospike on the right have

302
00:13:28,329 --> 00:13:33,609
more than 4x slowdown why the synthetic

303
00:13:31,779 --> 00:13:37,810
benchmark barely experiences any

304
00:13:33,610 --> 00:13:41,009
slowdown Multi cube fair queuing in both

305
00:13:37,810 --> 00:13:44,109
cases can balance the slaughter

306
00:13:41,009 --> 00:13:46,420
regarding bfq since it does not support

307
00:13:44,110 --> 00:13:48,160
parallel dispatch it cannot accommodate

308
00:13:46,420 --> 00:13:50,680
applications with high parallelism which

309
00:13:48,160 --> 00:13:54,730
is in this case synthetic benchmark and

310
00:13:50,680 --> 00:13:57,849
error spike here we show the scalability

311
00:13:54,730 --> 00:14:00,279
results for multi cube fair queuing the

312
00:13:57,850 --> 00:14:03,129
benchmark is on nvme over our DMA

313
00:14:00,279 --> 00:14:06,040
capable of sustaining 4 million IOPS per

314
00:14:03,129 --> 00:14:09,069
second when there is no contention this

315
00:14:06,040 --> 00:14:12,490
is the case with the blue line here the

316
00:14:09,069 --> 00:14:16,899
network link is saturated at around 717

317
00:14:12,490 --> 00:14:19,779
CPUs 17 threads here the orange line

318
00:14:16,899 --> 00:14:22,959
shows how multi cue fair queuing scales

319
00:14:19,779 --> 00:14:26,439
almost linearly up to 16 CPUs before we

320
00:14:22,959 --> 00:14:29,170
start using the second hyper thread and

321
00:14:26,439 --> 00:14:32,290
it reaches up to 3.1 million IOPS per

322
00:14:29,170 --> 00:14:33,819
second while providing fairness recall

323
00:14:32,290 --> 00:14:34,620
that the blue line does not guarantee

324
00:14:33,819 --> 00:14:39,189
fairness

325
00:14:34,620 --> 00:14:40,889
lastly the green line is Linux bfq which

326
00:14:39,189 --> 00:14:44,410
does not scale because it reorders

327
00:14:40,889 --> 00:14:49,420
requests using a centralized lock

328
00:14:44,410 --> 00:14:51,639
protected q BF q is a good showcase of

329
00:14:49,420 --> 00:14:54,639
naively applying fair queuing algorithm

330
00:14:51,639 --> 00:14:58,180
to multi q our stack without addressing

331
00:14:54,639 --> 00:15:01,180
its scalability problems to conclude we

332
00:14:58,180 --> 00:15:05,019
discussed a major consideration when

333
00:15:01,180 --> 00:15:07,059
moving to multi key wire designs while

334
00:15:05,019 --> 00:15:09,069
they achieve scalability due to

335
00:15:07,059 --> 00:15:11,170
coherence for execution properties like

336
00:15:09,069 --> 00:15:14,319
fairness being global or hard to achieve

337
00:15:11,170 --> 00:15:16,089
without incurring coherence traffic we

338
00:15:14,319 --> 00:15:18,910
show that it is possible to achieve even

339
00:15:16,089 --> 00:15:20,980
a strong fairness guarantee like fair

340
00:15:18,910 --> 00:15:22,990
queuing without negating the scalability

341
00:15:20,980 --> 00:15:25,379
of multi Q designs we introduced

342
00:15:22,990 --> 00:15:27,730
multi-layer ink and the scalable

343
00:15:25,379 --> 00:15:30,249
implementation of it that borrows from

344
00:15:27,730 --> 00:15:32,800
techniques used in design of scalable

345
00:15:30,249 --> 00:15:34,990
data structures to achieve throughput of

346
00:15:32,800 --> 00:15:36,910
more than three million iOS per second

347
00:15:34,990 --> 00:15:39,339
while at the same time providing

348
00:15:36,910 --> 00:15:41,640
fairness and with that I would be happy

349
00:15:39,339 --> 00:15:49,260
to take any questions

350
00:15:41,640 --> 00:15:49,260
[Applause]

351
00:15:52,860 --> 00:15:57,400
your phone ignition

352
00:15:55,480 --> 00:16:00,430
a good improvement on existing work

353
00:15:57,400 --> 00:16:03,520
especially relating to scalability I

354
00:16:00,430 --> 00:16:05,829
noticed that you're using inner

355
00:16:03,520 --> 00:16:10,000
processor interrupts right as a

356
00:16:05,830 --> 00:16:12,220
fundamental latency control mechanism

357
00:16:10,000 --> 00:16:13,920
right in my previous work I've noticed

358
00:16:12,220 --> 00:16:16,480
and spend actually considerable energy

359
00:16:13,920 --> 00:16:20,380
trying to minimize inner processor in

360
00:16:16,480 --> 00:16:22,300
realms did you evaluate the impact of

361
00:16:20,380 --> 00:16:24,850
the inner processor interrupt on actual

362
00:16:22,300 --> 00:16:26,709
useful CPU work that's happening during

363
00:16:24,850 --> 00:16:28,240
this IO patterns I didn't see that in

364
00:16:26,710 --> 00:16:30,460
your paper I wonder if you have any

365
00:16:28,240 --> 00:16:33,850
intuition around the impact of that the

366
00:16:30,460 --> 00:16:37,660
how sensitive is your performance of

367
00:16:33,850 --> 00:16:41,470
your performance results to being forced

368
00:16:37,660 --> 00:16:44,170
to do IP is for every single escalation

369
00:16:41,470 --> 00:16:46,840
so you don't you know you're not going

370
00:16:44,170 --> 00:16:50,439
to do IPI for every single request it's

371
00:16:46,840 --> 00:16:53,560
basically when a cue is raw you need to

372
00:16:50,440 --> 00:16:58,410
unload a leaky when you need to do an IP

373
00:16:53,560 --> 00:17:04,359
I but yes IP is going to happen you know

374
00:16:58,410 --> 00:17:06,010
frequently and so so we have a

375
00:17:04,359 --> 00:17:09,510
comparison in the paper that we compare

376
00:17:06,010 --> 00:17:12,670
using high pile we doing polling and

377
00:17:09,510 --> 00:17:15,060
with respect to CPU utilization we

378
00:17:12,670 --> 00:17:17,140
realize that it's better to do IP i

379
00:17:15,060 --> 00:17:19,149
there was another part of the question

380
00:17:17,140 --> 00:17:22,660
that I I guess I didn't understand if

381
00:17:19,150 --> 00:17:25,650
you repeat it for me yeah for example do

382
00:17:22,660 --> 00:17:30,340
you how sensitive is the throughput

383
00:17:25,650 --> 00:17:32,140
results that you show to the frequency

384
00:17:30,340 --> 00:17:35,800
of the IP I like if you were not to do

385
00:17:32,140 --> 00:17:38,320
an IP I if you skipped an IP I right you

386
00:17:35,800 --> 00:17:40,090
don't only did every other as IP I like

387
00:17:38,320 --> 00:17:42,280
how what's the sensitivity is the other

388
00:17:40,090 --> 00:17:45,310
thing like because for a real compute

389
00:17:42,280 --> 00:17:47,530
workload right where computation is

390
00:17:45,310 --> 00:17:51,399
happening IP is will destroy a whole

391
00:17:47,530 --> 00:17:54,220
bunch of l1 locality l2 locality like if

392
00:17:51,400 --> 00:17:56,919
you try this with with

393
00:17:54,220 --> 00:17:58,809
any like database workload that I

394
00:17:56,919 --> 00:17:59,440
actually is doing some join in the

395
00:17:58,809 --> 00:18:03,960
background

396
00:17:59,440 --> 00:18:07,390
IPS will destroy workload performance so

397
00:18:03,960 --> 00:18:08,410
we have a compute sensitive workload

398
00:18:07,390 --> 00:18:10,870
which is the graph processing

399
00:18:08,410 --> 00:18:15,250
application it doesn't seem to suffer

400
00:18:10,870 --> 00:18:18,219
from this issue so the reason I think is

401
00:18:15,250 --> 00:18:21,780
is that the IP eyes are not maybe as

402
00:18:18,220 --> 00:18:26,789
frequent as you think them to be because

403
00:18:21,780 --> 00:18:29,799
basically when you you when you need to

404
00:18:26,789 --> 00:18:32,169
on throttle IQ you would mean you would

405
00:18:29,799 --> 00:18:34,570
need an ID per process yes but but you

406
00:18:32,169 --> 00:18:37,330
know what we are really trying to

407
00:18:34,570 --> 00:18:39,460
retrain us for is for C group so we get

408
00:18:37,330 --> 00:18:41,590
the C group and we try to based on the

409
00:18:39,460 --> 00:18:44,350
weight of C group reassign the start

410
00:18:41,590 --> 00:18:48,510
tags as a result the token availability

411
00:18:44,350 --> 00:18:48,510
is per signal yes okay that's

412
00:18:53,920 --> 00:18:58,480
so if there's no more questions I've got

413
00:18:56,800 --> 00:19:02,379
one question I'd like to ask maybe while

414
00:18:58,480 --> 00:19:05,290
our next speaker sets up which is did

415
00:19:02,380 --> 00:19:07,900
you explore the sensitivity to the the

416
00:19:05,290 --> 00:19:10,780
parameter T that you have which looks at

417
00:19:07,900 --> 00:19:14,170
I guess bounding how much unfairness you

418
00:19:10,780 --> 00:19:16,360
have yes how do your results vary and

419
00:19:14,170 --> 00:19:19,600
how would how would you go about

420
00:19:16,360 --> 00:19:22,870
choosing T in practice so in the paper

421
00:19:19,600 --> 00:19:24,820
we explained exactly how we choose T is

422
00:19:22,870 --> 00:19:26,889
basically we assumed the worst workload

423
00:19:24,820 --> 00:19:29,169
which is extremely parallel with respect

424
00:19:26,890 --> 00:19:31,570
to the size of the Machine workload and

425
00:19:29,170 --> 00:19:33,220
you know submitting very small requests

426
00:19:31,570 --> 00:19:36,580
so this is the worst case workload and

427
00:19:33,220 --> 00:19:39,190
we try to match they're basically the

428
00:19:36,580 --> 00:19:41,050
raw device throughput by assigning it to

429
00:19:39,190 --> 00:19:44,110
any other workload and that machine is

430
00:19:41,050 --> 00:19:47,169
going to need this needa T smaller than

431
00:19:44,110 --> 00:19:51,100
that particular T so it's a combination

432
00:19:47,170 --> 00:19:56,800
of how fast the devices and also how the

433
00:19:51,100 --> 00:19:59,320
characteristic of the workload itself so

434
00:19:56,800 --> 00:20:00,230
thank you and let's thank our speaker

435
00:19:59,320 --> 00:20:05,269
one more time

436
00:20:00,230 --> 00:20:05,269
[Applause]

