1
00:00:10,110 --> 00:00:16,539
and hi everyone and thanks our chair for

2
00:00:13,110 --> 00:00:18,698
the introduction and challenge on from

3
00:00:16,539 --> 00:00:21,039
Science and Technology and I'm here to

4
00:00:18,699 --> 00:00:22,869
present our work mark which is tailored

5
00:00:21,039 --> 00:00:26,980
to serve machine learning models

6
00:00:22,869 --> 00:00:28,750
inference on public cloud machine

7
00:00:26,980 --> 00:00:31,750
learning serving is to deploy train

8
00:00:28,750 --> 00:00:34,390
models for users requests and we have

9
00:00:31,750 --> 00:00:36,640
seen rapid growth in demand for machine

10
00:00:34,390 --> 00:00:39,579
learning serving on public cloud thanks

11
00:00:36,640 --> 00:00:42,519
to its ease of use and unified user

12
00:00:39,579 --> 00:00:46,000
experience in such a scenario model

13
00:00:42,519 --> 00:00:48,430
owner acts as a cloud tenant and the

14
00:00:46,000 --> 00:00:50,920
service provider at the same time right

15
00:00:48,430 --> 00:00:54,489
and as a service provider we want

16
00:00:50,920 --> 00:00:58,360
scalability and also our compliance for

17
00:00:54,489 --> 00:01:01,720
our users and as a cloud tenant from the

18
00:00:58,360 --> 00:01:05,950
public cloud we want the lowest cost

19
00:01:01,720 --> 00:01:08,259
possible and at the moment there are

20
00:01:05,950 --> 00:01:10,450
numerous services available on public

21
00:01:08,259 --> 00:01:13,509
cloud we can use virtual machines

22
00:01:10,450 --> 00:01:16,450
containers and even server dysfunctions

23
00:01:13,509 --> 00:01:17,829
right now and some of the cloud

24
00:01:16,450 --> 00:01:18,700
providers even provide like

25
00:01:17,829 --> 00:01:21,850
out-of-the-box

26
00:01:18,700 --> 00:01:23,950
serving platforms so we first

27
00:01:21,850 --> 00:01:25,869
characterized machining serving and its

28
00:01:23,950 --> 00:01:28,960
performance on all these cloud services

29
00:01:25,870 --> 00:01:31,840
as there is no existing work to do so

30
00:01:28,960 --> 00:01:34,419
and then we design and implement to the

31
00:01:31,840 --> 00:01:38,020
system to serve machinery on cloud based

32
00:01:34,420 --> 00:01:40,150
on our insights as there is no

33
00:01:38,020 --> 00:01:42,670
state-of-the-art solution for such

34
00:01:40,150 --> 00:01:45,340
matter let's take a look at the state of

35
00:01:42,670 --> 00:01:47,350
the practice system which maker is the

36
00:01:45,340 --> 00:01:51,070
leading machine learning as a service

37
00:01:47,350 --> 00:01:54,100
platform from AWS and like other popular

38
00:01:51,070 --> 00:01:57,220
platforms sage maker employs reactive

39
00:01:54,100 --> 00:01:59,289
scaling on virtual machine cluster in a

40
00:01:57,220 --> 00:02:01,510
very conventional way and the

41
00:01:59,290 --> 00:02:03,250
provisioning decision is made as a

42
00:02:01,510 --> 00:02:06,040
feedback of the historical load

43
00:02:03,250 --> 00:02:08,979
information and in public cloud the

44
00:02:06,040 --> 00:02:11,739
provisioning time is way longer than the

45
00:02:08,979 --> 00:02:14,650
inference execution time so we need

46
00:02:11,739 --> 00:02:16,689
over-provisioning to hide the

47
00:02:14,650 --> 00:02:21,489
provisioning time of these virtual

48
00:02:16,689 --> 00:02:22,870
machines and in that case machine

49
00:02:21,489 --> 00:02:26,049
learning inference is very

50
00:02:22,870 --> 00:02:29,230
compute-intensive does make it extremely

51
00:02:26,049 --> 00:02:30,010
expensive to over provision so I would

52
00:02:29,230 --> 00:02:33,730
say exist

53
00:02:30,010 --> 00:02:35,970
three solutions are far from ideal from

54
00:02:33,730 --> 00:02:38,768
the cost perspective

55
00:02:35,970 --> 00:02:41,829
another unique property of measuring and

56
00:02:38,769 --> 00:02:43,870
serving is that it has many expensive

57
00:02:41,829 --> 00:02:47,049
hardware accelerators available on cloud

58
00:02:43,870 --> 00:02:50,650
I like training these accelerators are

59
00:02:47,049 --> 00:02:52,989
not essential for serving as there is

60
00:02:50,650 --> 00:02:55,709
way less parallel computation involved

61
00:02:52,989 --> 00:02:58,959
and CPU can handle that very comfortably

62
00:02:55,709 --> 00:03:01,450
so if we were to use these accelerators

63
00:02:58,959 --> 00:03:03,790
we need to like grouping multiple

64
00:03:01,450 --> 00:03:05,890
requests into one batch to increase the

65
00:03:03,790 --> 00:03:09,010
parallelism it's a common practice to

66
00:03:05,890 --> 00:03:11,190
exploit such accelerators and besides

67
00:03:09,010 --> 00:03:13,959
taking advantage of high parallelism

68
00:03:11,190 --> 00:03:18,599
optimization that's built into these

69
00:03:13,959 --> 00:03:21,849
optimized accelerators batching also can

70
00:03:18,599 --> 00:03:26,018
amortizes over has like RPC calls and

71
00:03:21,849 --> 00:03:28,030
cross-device memory copy and so the

72
00:03:26,019 --> 00:03:30,700
question remains like how do we choose

73
00:03:28,030 --> 00:03:32,620
between CPUs and accelerators and was

74
00:03:30,700 --> 00:03:38,649
the cause and latency impacts of these

75
00:03:32,620 --> 00:03:41,980
accelerators we characterize popular

76
00:03:38,650 --> 00:03:45,190
accelerators GPU and TPU against our

77
00:03:41,980 --> 00:03:47,948
good old CPU and depicted the results in

78
00:03:45,190 --> 00:03:50,230
the fever the horizontal axis here is

79
00:03:47,949 --> 00:03:53,430
the batch size and the bars and lines

80
00:03:50,230 --> 00:03:57,190
shows the cause and batch latency

81
00:03:53,430 --> 00:03:59,199
respectively and as we can see excited

82
00:03:57,190 --> 00:04:01,900
heard accelerators benefit substantially

83
00:03:59,199 --> 00:04:04,629
from batching and in fact with

84
00:04:01,900 --> 00:04:07,359
appropriate batching and food

85
00:04:04,629 --> 00:04:09,970
utilizations you GPUs here can be

86
00:04:07,359 --> 00:04:13,299
cheaper than CPUs as we can see when

87
00:04:09,970 --> 00:04:16,930
batch size reaches for the cause of GPU

88
00:04:13,299 --> 00:04:18,789
is already lower than CPUs and CPU

89
00:04:16,930 --> 00:04:20,099
because abuse costs kind of remain the

90
00:04:18,789 --> 00:04:23,919
same even with matching

91
00:04:20,099 --> 00:04:26,669
but large batch size also leads to

92
00:04:23,919 --> 00:04:28,840
longer queuing delay and in the serving

93
00:04:26,669 --> 00:04:31,780
scenario delay matters

94
00:04:28,840 --> 00:04:33,729
so in conclusion maintaining high

95
00:04:31,780 --> 00:04:36,299
utilization and picking the right batch

96
00:04:33,729 --> 00:04:38,440
size carefully are essential for

97
00:04:36,300 --> 00:04:41,450
accelerator adoption in measuring a

98
00:04:38,440 --> 00:04:43,880
serving and as for the TV

99
00:04:41,450 --> 00:04:45,530
it is designed for massive parallel and

100
00:04:43,880 --> 00:04:47,780
optimized for through prude instead of

101
00:04:45,530 --> 00:04:51,710
latency so in this case they are not

102
00:04:47,780 --> 00:04:53,650
suitable for inference yet another

103
00:04:51,710 --> 00:04:56,659
challenge we face on cloud is the

104
00:04:53,650 --> 00:04:59,840
abundance of choices we can use virtual

105
00:04:56,660 --> 00:05:02,300
machines from IAS containers from CAS

106
00:04:59,840 --> 00:05:05,479
and we can also use service computing

107
00:05:02,300 --> 00:05:07,700
from function as a service and each of

108
00:05:05,480 --> 00:05:10,910
these services has a large configuration

109
00:05:07,700 --> 00:05:13,640
space what's more cloud providers

110
00:05:10,910 --> 00:05:16,730
usually offer discontents instances with

111
00:05:13,640 --> 00:05:18,830
compromised service guarantees such as

112
00:05:16,730 --> 00:05:21,380
spot instances and first of all

113
00:05:18,830 --> 00:05:24,820
instances from AWS and like their

114
00:05:21,380 --> 00:05:28,370
counterparts from Google and Microsoft

115
00:05:24,820 --> 00:05:31,430
no existing work has explored how do we

116
00:05:28,370 --> 00:05:34,490
pick and right-size these services as

117
00:05:31,430 --> 00:05:36,500
well as how do we explore explore it

118
00:05:34,490 --> 00:05:39,730
please discounts without hurting users

119
00:05:36,500 --> 00:05:42,380
experience for machine learning serving

120
00:05:39,730 --> 00:05:45,560
we first compared the three services

121
00:05:42,380 --> 00:05:49,070
options with three first options I as

122
00:05:45,560 --> 00:05:53,990
see as an FAS on AWS corresponding

123
00:05:49,070 --> 00:05:58,870
service EC to EC s and lambda as we can

124
00:05:53,990 --> 00:06:02,570
see is NC s has the very traditional

125
00:05:58,870 --> 00:06:04,250
pay-as-you-go billing option as long as

126
00:06:02,570 --> 00:06:08,960
the instances are running you are

127
00:06:04,250 --> 00:06:11,840
charged well FAS is pretty new and the

128
00:06:08,960 --> 00:06:14,060
good thing about it is they will only

129
00:06:11,840 --> 00:06:17,150
charge you for the actual function usage

130
00:06:14,060 --> 00:06:19,550
that means you won't be charged unless

131
00:06:17,150 --> 00:06:23,450
they are active requests running in all

132
00:06:19,550 --> 00:06:27,200
these functions and as we can see here

133
00:06:23,450 --> 00:06:30,289
summarized in order to serve the same

134
00:06:27,200 --> 00:06:33,080
amount of machine learning requests IAS

135
00:06:30,290 --> 00:06:36,050
is the cheapest when it comes to cost

136
00:06:33,080 --> 00:06:37,909
but it suffers from the long scanning

137
00:06:36,050 --> 00:06:42,950
over half that means it takes a long

138
00:06:37,910 --> 00:06:45,170
time to scout out and well FAS is really

139
00:06:42,950 --> 00:06:48,590
fast to scale out like a new function

140
00:06:45,170 --> 00:06:52,010
can be ready online in a few seconds but

141
00:06:48,590 --> 00:06:55,070
it has the highest cost

142
00:06:52,010 --> 00:06:58,880
so one idea came to our mind is that

143
00:06:55,070 --> 00:07:01,040
what if we combine is and FAS together

144
00:06:58,880 --> 00:07:03,770
to achieve the most best of both words

145
00:07:01,040 --> 00:07:07,400
so that instead of over-provisioning in

146
00:07:03,770 --> 00:07:10,219
is we let FAS provide supplementary

147
00:07:07,400 --> 00:07:13,489
service while nu is instances are

148
00:07:10,220 --> 00:07:16,010
launching so we can also hide the

149
00:07:13,490 --> 00:07:20,030
provisioning time without actually doing

150
00:07:16,010 --> 00:07:22,099
over-provisioning with the evaluated

151
00:07:20,030 --> 00:07:24,979
house cpu and memory effect machine

152
00:07:22,100 --> 00:07:28,280
earning serving performances we compare

153
00:07:24,980 --> 00:07:30,560
compute optimize c5 family with

154
00:07:28,280 --> 00:07:32,960
general-purpose my family and I'm

155
00:07:30,560 --> 00:07:36,020
finding since this have twice the memory

156
00:07:32,960 --> 00:07:40,130
as c5 instances and as the normalized

157
00:07:36,020 --> 00:07:41,960
cost shown in the bars confirmed CPU is

158
00:07:40,130 --> 00:07:44,810
the bottleneck of machining serving

159
00:07:41,960 --> 00:07:47,419
inference and the latency chops up

160
00:07:44,810 --> 00:07:50,450
linearly with the addition of CPU

161
00:07:47,420 --> 00:07:52,700
resource and adding excessive memory

162
00:07:50,450 --> 00:07:56,270
doesn't help with performance at all

163
00:07:52,700 --> 00:07:59,810
and so it's safe to say like bit on

164
00:07:56,270 --> 00:08:02,719
demand pricing if SLE allows smaller

165
00:07:59,810 --> 00:08:05,510
instances are preferable as it is

166
00:08:02,720 --> 00:08:10,970
cheaper and also can provide us with the

167
00:08:05,510 --> 00:08:14,390
smaller scaling step size and spot

168
00:08:10,970 --> 00:08:16,400
instances are idle resource resources

169
00:08:14,390 --> 00:08:19,219
cloud providers lease out with the

170
00:08:16,400 --> 00:08:21,530
premise that a Tobias can take them back

171
00:08:19,220 --> 00:08:25,490
whenever they want with the short notice

172
00:08:21,530 --> 00:08:28,159
these instances are up to 75% cheaper

173
00:08:25,490 --> 00:08:30,880
than their own demand counterparts so it

174
00:08:28,160 --> 00:08:32,900
is really appealing to us because our

175
00:08:30,880 --> 00:08:35,780
machinery server has really caused

176
00:08:32,900 --> 00:08:37,459
sensitive and fortunately machine

177
00:08:35,780 --> 00:08:39,709
learning serving has the property of

178
00:08:37,460 --> 00:08:42,470
being stateless that means the response

179
00:08:39,710 --> 00:08:44,900
only depends on the requests and all

180
00:08:42,470 --> 00:08:48,200
these requests are independent so that

181
00:08:44,900 --> 00:08:50,230
makes the problem easier as we don't

182
00:08:48,200 --> 00:08:52,790
have to worry about preserving

183
00:08:50,230 --> 00:08:55,310
consistency and doing checkpoints or

184
00:08:52,790 --> 00:08:58,930
kind of stuff in case of interruptions

185
00:08:55,310 --> 00:09:02,030
and now that we also have FAS as

186
00:08:58,930 --> 00:09:04,939
fallback even if the substitute VMs can

187
00:09:02,030 --> 00:09:05,720
not be ready before the actual

188
00:09:04,940 --> 00:09:08,600
interruption

189
00:09:05,720 --> 00:09:11,180
still have something to fall back on so

190
00:09:08,600 --> 00:09:13,430
to summarize our characterizations in a

191
00:09:11,180 --> 00:09:15,769
few sentences so like for all these

192
00:09:13,430 --> 00:09:18,290
cloud services is is the most cost

193
00:09:15,769 --> 00:09:20,629
effective and FS use the easiest to

194
00:09:18,290 --> 00:09:23,750
scale and we kind of want to take

195
00:09:20,629 --> 00:09:26,410
advantage of them and within on-demand

196
00:09:23,750 --> 00:09:30,350
market and smaller CPU instances are

197
00:09:26,410 --> 00:09:34,069
preferable and batching is essential to

198
00:09:30,350 --> 00:09:36,259
adopt accelerators and bash size is an

199
00:09:34,069 --> 00:09:39,019
important control knob for cost and

200
00:09:36,259 --> 00:09:41,689
latency country trade-off and it's

201
00:09:39,019 --> 00:09:45,589
pretty safe to use small instances for

202
00:09:41,689 --> 00:09:47,920
versioning survey so let's recall our

203
00:09:45,589 --> 00:09:50,240
machine learning serving objectives and

204
00:09:47,920 --> 00:09:52,790
challenges how can we design a system

205
00:09:50,240 --> 00:09:55,009
for machine learning serving based on

206
00:09:52,790 --> 00:09:58,879
its unique properties and our insights

207
00:09:55,009 --> 00:10:01,279
to improve cost effectiveness first we

208
00:09:58,879 --> 00:10:03,379
need to maintain high utilization on all

209
00:10:01,279 --> 00:10:05,990
these VMs and trade-off between

210
00:10:03,379 --> 00:10:08,329
different instance types so we can use

211
00:10:05,990 --> 00:10:11,870
workload prediction and proactively

212
00:10:08,329 --> 00:10:14,930
planned or instances and secondly as we

213
00:10:11,870 --> 00:10:16,850
mentioned we can use FAS to hide the

214
00:10:14,930 --> 00:10:20,000
provisioning time instead of doing over

215
00:10:16,850 --> 00:10:22,519
provisioning and thirdly in order to

216
00:10:20,000 --> 00:10:25,189
adopt spot instances we will need an

217
00:10:22,519 --> 00:10:28,279
online provisioning algorithm as the

218
00:10:25,189 --> 00:10:32,230
price and availability of spot market is

219
00:10:28,279 --> 00:10:35,629
dynamic and in order to utilize

220
00:10:32,230 --> 00:10:37,550
accelerator we'll need to employ dynamic

221
00:10:35,629 --> 00:10:40,970
patching to actively trade-off between

222
00:10:37,550 --> 00:10:45,019
the cause and latency and one important

223
00:10:40,970 --> 00:10:47,480
guideline is that like to adopt like we

224
00:10:45,019 --> 00:10:52,399
will always need to be better off with

225
00:10:47,480 --> 00:10:55,189
batching enable and and as far as our

226
00:10:52,399 --> 00:10:58,850
requirements machinery serving those are

227
00:10:55,189 --> 00:11:01,339
usually specified as like 99% of all

228
00:10:58,850 --> 00:11:03,980
requests must be must finish under one

229
00:11:01,339 --> 00:11:06,769
second and in the public cloud there is

230
00:11:03,980 --> 00:11:08,889
no closed-form closed form solution for

231
00:11:06,769 --> 00:11:11,990
response response time distribution

232
00:11:08,889 --> 00:11:14,689
however traces from large industrial

233
00:11:11,990 --> 00:11:17,040
clusters show that machine learning

234
00:11:14,689 --> 00:11:19,589
serving has the convenient property of

235
00:11:17,040 --> 00:11:22,140
being deterministic when it comes to

236
00:11:19,590 --> 00:11:24,750
execution time because it has fixed size

237
00:11:22,140 --> 00:11:28,189
input features and it applies the same

238
00:11:24,750 --> 00:11:30,720
model and all the improves and all the

239
00:11:28,190 --> 00:11:33,270
computation has the same compute pattern

240
00:11:30,720 --> 00:11:36,150
so we can in this case we can assume

241
00:11:33,270 --> 00:11:38,189
fixed execution time and only focus on

242
00:11:36,150 --> 00:11:40,560
the queuing delay for each request so

243
00:11:38,190 --> 00:11:45,050
that we can timely route these requests

244
00:11:40,560 --> 00:11:49,589
to maintain SL combines at the best-ever

245
00:11:45,050 --> 00:11:53,609
manner we design and implement at mark

246
00:11:49,590 --> 00:11:56,520
as in model arc we utilize the hybrid of

247
00:11:53,610 --> 00:11:59,850
IAS and FS to reduce over-provisioning

248
00:11:56,520 --> 00:12:01,500
we use then imagine to excel all

249
00:11:59,850 --> 00:12:05,040
accelerators to increase the cost

250
00:12:01,500 --> 00:12:07,380
reduction we use proactive skilling to

251
00:12:05,040 --> 00:12:10,349
maintain a constant high utilization and

252
00:12:07,380 --> 00:12:13,800
we also monitors SL performance to

253
00:12:10,350 --> 00:12:15,960
detect prediction error as early as

254
00:12:13,800 --> 00:12:20,219
possible so that we can reactive add

255
00:12:15,960 --> 00:12:22,470
more instances if needed so how does

256
00:12:20,220 --> 00:12:24,540
mark provision instances based on the

257
00:12:22,470 --> 00:12:27,330
prediction essentially we are

258
00:12:24,540 --> 00:12:30,089
orchestrating a heterogeneous cluster

259
00:12:27,330 --> 00:12:32,700
and instances of each type can be seen

260
00:12:30,090 --> 00:12:35,940
as MDC q and the problem is very

261
00:12:32,700 --> 00:12:38,670
complicated and impossible to solve in

262
00:12:35,940 --> 00:12:41,790
an online manner so we propose an greedy

263
00:12:38,670 --> 00:12:43,560
heuristic instead for future low demand

264
00:12:41,790 --> 00:12:49,410
we try to fill it with the cheapest

265
00:12:43,560 --> 00:12:51,300
instance try to yeah for future demand

266
00:12:49,410 --> 00:12:54,180
we try to fill it with cheapest

267
00:12:51,300 --> 00:12:56,339
instances one by one considering both

268
00:12:54,180 --> 00:12:59,250
pass you go price and launch overhead

269
00:12:56,340 --> 00:13:00,810
this way we get to expose and exploit

270
00:12:59,250 --> 00:13:03,540
the long-term cost trade-offs between

271
00:13:00,810 --> 00:13:06,719
large and small instances and CPU and

272
00:13:03,540 --> 00:13:08,939
GPU instances while maintaining high

273
00:13:06,720 --> 00:13:11,130
utilization and if you're interested

274
00:13:08,940 --> 00:13:15,360
please refer to our paper for more

275
00:13:11,130 --> 00:13:18,000
details and we implemented mark and then

276
00:13:15,360 --> 00:13:19,800
evaluated on beta whereas with for

277
00:13:18,000 --> 00:13:22,440
diverse machine learning models of all

278
00:13:19,800 --> 00:13:25,800
sizes with different backgrounds with

279
00:13:22,440 --> 00:13:28,710
different backends in order to evaluate

280
00:13:25,800 --> 00:13:30,540
the cost savings of mark under real-life

281
00:13:28,710 --> 00:13:33,360
large-scale web application

282
00:13:30,540 --> 00:13:35,910
we abstract arrivals from traitor

283
00:13:33,360 --> 00:13:38,460
workload and apply them to mark we use

284
00:13:35,910 --> 00:13:40,140
sage maker as the baseline since it is

285
00:13:38,460 --> 00:13:43,200
the state of the practice machine

286
00:13:40,140 --> 00:13:45,900
learning serving system right now and mo

287
00:13:43,200 --> 00:13:48,180
here represents mark on demand where

288
00:13:45,900 --> 00:13:53,670
mark only considers on-demand instances

289
00:13:48,180 --> 00:13:56,219
and ms represents mark spot where spot

290
00:13:53,670 --> 00:13:59,130
instances are also considered and we can

291
00:13:56,220 --> 00:14:01,770
see mark on depend on demand achieved

292
00:13:59,130 --> 00:14:05,460
significant cost reduction already from

293
00:14:01,770 --> 00:14:08,910
the benefits from no over provisioning

294
00:14:05,460 --> 00:14:12,210
and applying patching over accelerators

295
00:14:08,910 --> 00:14:16,020
and marks well further that cost

296
00:14:12,210 --> 00:14:19,350
reduction by exploiting the costs the

297
00:14:16,020 --> 00:14:22,590
discounts from spot market we then

298
00:14:19,350 --> 00:14:25,080
compare the as our compliance of mark we

299
00:14:22,590 --> 00:14:27,090
use this complimentary severe figure to

300
00:14:25,080 --> 00:14:31,110
show the tail latency of our services

301
00:14:27,090 --> 00:14:33,510
the outlay in percentages here show the

302
00:14:31,110 --> 00:14:35,850
requests that didn't make it within the

303
00:14:33,510 --> 00:14:38,010
deadline besides the aforementioned

304
00:14:35,850 --> 00:14:42,090
workload trader we introduced another

305
00:14:38,010 --> 00:14:45,090
highly dynamic yet unpredictable this is

306
00:14:42,090 --> 00:14:48,540
synthetic workload mmpp so that we can

307
00:14:45,090 --> 00:14:50,070
see if marks SL compliance relies on the

308
00:14:48,540 --> 00:14:52,949
accurate prediction of the future

309
00:14:50,070 --> 00:14:54,360
workload and as we can see here in the

310
00:14:52,950 --> 00:14:57,240
figure

311
00:14:54,360 --> 00:14:59,690
no matter with like real-life workload

312
00:14:57,240 --> 00:15:02,370
Twitter or like synthetic and

313
00:14:59,690 --> 00:15:07,170
predictable workload mmpp

314
00:15:02,370 --> 00:15:09,720
our system mark always achieve a much

315
00:15:07,170 --> 00:15:12,209
better SL compliance compared with stage

316
00:15:09,720 --> 00:15:16,770
maker as such maker really struggled to

317
00:15:12,210 --> 00:15:19,080
adapt to the highly dynamic workload we

318
00:15:16,770 --> 00:15:21,870
then uses a micro benchmark to see how

319
00:15:19,080 --> 00:15:24,570
exactly mark handles and expected low

320
00:15:21,870 --> 00:15:26,850
search comparable sales maker in case of

321
00:15:24,570 --> 00:15:29,250
surging demand marks latency is always

322
00:15:26,850 --> 00:15:32,240
kept by the latency of lambda function

323
00:15:29,250 --> 00:15:35,250
because you can always it's always is

324
00:15:32,240 --> 00:15:37,290
will always be there to like and ready

325
00:15:35,250 --> 00:15:40,530
to serve in a few seconds and it can

326
00:15:37,290 --> 00:15:43,240
quickly recover to the SL recover to the

327
00:15:40,530 --> 00:15:46,000
normal latency as the SEO monitor

328
00:15:43,240 --> 00:15:48,550
if a clearly the scales out the cluster

329
00:15:46,000 --> 00:15:51,700
in a reactive manner in time

330
00:15:48,550 --> 00:15:53,620
so in conclusion with this work we

331
00:15:51,700 --> 00:15:55,660
characterized machinery serving on the

332
00:15:53,620 --> 00:15:57,640
available public cloud services and then

333
00:15:55,660 --> 00:16:00,120
designed mark based on our insights and

334
00:15:57,640 --> 00:16:03,670
achieve scalable and as though we're

335
00:16:00,120 --> 00:16:06,970
serving with a significantly lower cost

336
00:16:03,670 --> 00:16:09,819
comparative industry solutions so mark

337
00:16:06,970 --> 00:16:11,620
is open source on github and if you have

338
00:16:09,820 --> 00:16:14,530
any more questions you can take offline

339
00:16:11,620 --> 00:16:15,820
and find me with the QR code and I'm

340
00:16:14,530 --> 00:16:18,220
currently seeking internship

341
00:16:15,820 --> 00:16:19,570
opportunities so that's all I have for

342
00:16:18,220 --> 00:16:21,190
today and I'm happy to take any

343
00:16:19,570 --> 00:16:27,149
questions

344
00:16:21,190 --> 00:16:27,149
[Applause]

345
00:16:32,130 --> 00:16:37,870
I'm y'all from Amazon and I have a

346
00:16:35,230 --> 00:16:41,920
question about Auto patching

347
00:16:37,870 --> 00:16:45,340
as far as we know like currently the

348
00:16:41,920 --> 00:16:48,459
mainstream deepening frameworks the only

349
00:16:45,340 --> 00:16:51,190
supports static shape deployment so I'm

350
00:16:48,460 --> 00:16:54,280
curious how you like actually implement

351
00:16:51,190 --> 00:16:57,160
those dynamic patching strategies for

352
00:16:54,280 --> 00:16:59,470
example did you create multiple instance

353
00:16:57,160 --> 00:17:03,699
for different patches and using some

354
00:16:59,470 --> 00:17:08,319
packaging policies no we didn't so as we

355
00:17:03,700 --> 00:17:14,650
can see let me find the slide as we can

356
00:17:08,319 --> 00:17:17,589
see in the picture here yeah like this

357
00:17:14,650 --> 00:17:21,070
the increase of batch size the latency

358
00:17:17,589 --> 00:17:23,770
of GPU like riddick goes really slowly

359
00:17:21,069 --> 00:17:26,230
and as for your concern like if the

360
00:17:23,770 --> 00:17:28,210
framework only supports like a fixed

361
00:17:26,230 --> 00:17:31,330
sized batch size we can always use

362
00:17:28,210 --> 00:17:33,820
padding to support like these kind of

363
00:17:31,330 --> 00:17:35,710
like different batch sizes right yeah

364
00:17:33,820 --> 00:17:38,020
I'm thinking about something like

365
00:17:35,710 --> 00:17:41,320
intensive flow serving they have some

366
00:17:38,020 --> 00:17:43,930
auto patching policy libraries like you

367
00:17:41,320 --> 00:17:47,850
can adjust your input batch sizes

368
00:17:43,930 --> 00:17:51,250
depending on your like current TPS but

369
00:17:47,850 --> 00:17:53,860
like so you you you are saying your

370
00:17:51,250 --> 00:17:56,440
method is just like picking a fixed

371
00:17:53,860 --> 00:17:59,800
batch size or or

372
00:17:56,440 --> 00:18:04,590
petting them right no no we actually

373
00:17:59,800 --> 00:18:04,590
didn't do that because in our evaluation

374
00:18:04,860 --> 00:18:10,810
like as we didn't do batching so

375
00:18:09,040 --> 00:18:13,870
basically we applied a very similar

376
00:18:10,810 --> 00:18:17,290
approach as tensor preserving or sway em

377
00:18:13,870 --> 00:18:21,850
from Microsoft it's giving a time window

378
00:18:17,290 --> 00:18:24,010
and if like the fix like and batch size

379
00:18:21,850 --> 00:18:26,320
threshold and whenever the thrush

380
00:18:24,010 --> 00:18:29,140
whichever threshold are reached first

381
00:18:26,320 --> 00:18:31,000
then we'll just form the batch directly

382
00:18:29,140 --> 00:18:33,790
and feed it into the serving back-end

383
00:18:31,000 --> 00:18:35,830
okay yeah what I mean is in the serving

384
00:18:33,790 --> 00:18:39,730
back and you you you you only use one

385
00:18:35,830 --> 00:18:43,689
instance for dealing with of fixed batch

386
00:18:39,730 --> 00:18:46,210
because as I said like static graph is

387
00:18:43,690 --> 00:18:47,680
the most common ways like for the

388
00:18:46,210 --> 00:18:51,490
different framework you can dealing with

389
00:18:47,680 --> 00:18:55,420
those workloads I'm curious about how

390
00:18:51,490 --> 00:18:56,680
you actually doing inference in the

391
00:18:55,420 --> 00:19:00,130
backend using those deep learning

392
00:18:56,680 --> 00:19:02,920
frameworks using if your various I see I

393
00:19:00,130 --> 00:19:05,950
see I see yes your question so like we

394
00:19:02,920 --> 00:19:08,290
took a system system or perspective on

395
00:19:05,950 --> 00:19:10,180
this work so we assume is all the

396
00:19:08,290 --> 00:19:15,280
storing backends are black boxes to us

397
00:19:10,180 --> 00:19:18,460
we only like profile the capacity first

398
00:19:15,280 --> 00:19:21,430
and take that as an input we don't care

399
00:19:18,460 --> 00:19:24,060
about how how do they sir serve it in

400
00:19:21,430 --> 00:19:27,160
the back hat yeah if because in the

401
00:19:24,060 --> 00:19:29,560
actual implementation like I can like we

402
00:19:27,160 --> 00:19:32,080
can imagine some sometimes you need

403
00:19:29,560 --> 00:19:36,010
multiple instance to serve this kind of

404
00:19:32,080 --> 00:19:38,409
dynamic shapes that's good to know we

405
00:19:36,010 --> 00:19:40,980
can have a offline discussion about it

406
00:19:38,410 --> 00:19:40,980
thank you

407
00:19:42,010 --> 00:19:46,720
okay let's think this speaker one more

408
00:19:45,080 --> 00:19:50,429
time

409
00:19:46,720 --> 00:19:50,430
[Applause]

