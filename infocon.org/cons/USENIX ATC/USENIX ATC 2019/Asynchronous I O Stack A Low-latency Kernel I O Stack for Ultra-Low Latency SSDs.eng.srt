1
00:00:10,670 --> 00:00:14,959
thank you for the introduction good

2
00:00:13,099 --> 00:00:17,420
afternoon everyone I'm kids only from

3
00:00:14,959 --> 00:00:19,700
jong-hyun University I'm going to talk

4
00:00:17,420 --> 00:00:22,279
about on asynchronous I use tag which is

5
00:00:19,700 --> 00:00:24,980
a low latency kernel I astagfirullah a

6
00:00:22,279 --> 00:00:27,380
low latency SSDs this is a joint work

7
00:00:24,980 --> 00:00:31,700
with Hakka Xin one sock zone jejunum

8
00:00:27,380 --> 00:00:33,290
jelly and my advisers engage on today

9
00:00:31,700 --> 00:00:36,200
CSS these are really fast

10
00:00:33,290 --> 00:00:38,360
typical flash-based SSDs are on order of

11
00:00:36,200 --> 00:00:41,870
a magnitude faster than traditional disk

12
00:00:38,360 --> 00:00:44,990
and emerging ultra-low latency SSDs such

13
00:00:41,870 --> 00:00:48,110
as samsung g SSD and intel obtain SSD

14
00:00:44,990 --> 00:00:50,480
ultra fast they can deliver a single IO

15
00:00:48,110 --> 00:00:53,800
in a few microseconds which is several

16
00:00:50,480 --> 00:00:57,110
orders of magnitude faster than disk

17
00:00:53,800 --> 00:00:59,390
with this ultra low latency SSDs the

18
00:00:57,110 --> 00:01:01,820
kernel IO stack becomes overheads in the

19
00:00:59,390 --> 00:01:04,579
total IO latency this graph shows the

20
00:01:01,820 --> 00:01:07,579
breakdown of the i/o latency using

21
00:01:04,578 --> 00:01:09,770
various SSDs with the low SSDs the

22
00:01:07,579 --> 00:01:12,309
corner portion is negligible in the i/o

23
00:01:09,770 --> 00:01:15,610
latency however with ultra-low latency

24
00:01:12,310 --> 00:01:19,729
SSDs its portion is no longer negligible

25
00:01:15,610 --> 00:01:22,340
taking up to 37% in Reed and 35% in

26
00:01:19,729 --> 00:01:24,560
right this leads us to optimize the

27
00:01:22,340 --> 00:01:26,659
kernel I expect to make applications

28
00:01:24,560 --> 00:01:31,040
experience such super fast storage

29
00:01:26,659 --> 00:01:33,710
performance in a computer system when we

30
00:01:31,040 --> 00:01:35,990
need to run the to operations and if one

31
00:01:33,710 --> 00:01:38,750
is computation and the other is on Io

32
00:01:35,990 --> 00:01:40,640
a typical intuitive way to accomplish

33
00:01:38,750 --> 00:01:43,430
these operations is to run both

34
00:01:40,640 --> 00:01:45,650
synchronously however if the two

35
00:01:43,430 --> 00:01:48,080
operations have some independent parts

36
00:01:45,650 --> 00:01:50,290
we then we can run we can apply the

37
00:01:48,080 --> 00:01:53,119
asynchronous i/o technique for example

38
00:01:50,290 --> 00:01:55,460
after performing dependent parts in

39
00:01:53,119 --> 00:01:57,950
computation we can run the i/o and

40
00:01:55,460 --> 00:02:00,408
independent parts in computation at the

41
00:01:57,950 --> 00:02:04,880
same time by doing so we can utilize

42
00:02:00,409 --> 00:02:06,799
both IO device and CPU consequently we

43
00:02:04,880 --> 00:02:09,109
can improve the store photo bus system

44
00:02:06,799 --> 00:02:12,650
and also reduce the total latency to

45
00:02:09,110 --> 00:02:14,959
accomplish these operations our idea is

46
00:02:12,650 --> 00:02:16,939
to apply this asynchronous i/o concept

47
00:02:14,959 --> 00:02:19,040
to the iOS take yourself to reduce our

48
00:02:16,939 --> 00:02:22,760
latency and improve the storage access

49
00:02:19,040 --> 00:02:24,560
performance before we get into the

50
00:02:22,760 --> 00:02:27,470
details let me briefly over

51
00:02:24,560 --> 00:02:29,810
our approaches first in the red path the

52
00:02:27,470 --> 00:02:32,120
original red path runs many io stack

53
00:02:29,810 --> 00:02:35,180
operations synchronously to the actual

54
00:02:32,120 --> 00:02:37,550
operation in our scheme we run some of

55
00:02:35,180 --> 00:02:41,090
the IO stack operations asynchronously

56
00:02:37,550 --> 00:02:45,230
to the IO so we can reduce the reduce

57
00:02:41,090 --> 00:02:47,540
latency of the sit read system call in

58
00:02:45,230 --> 00:02:49,640
the right path a buffer the right has no

59
00:02:47,540 --> 00:02:51,679
chance to overlap computation with IO

60
00:02:49,640 --> 00:02:54,589
because all operations are done in

61
00:02:51,680 --> 00:02:56,570
memory so we focused on the F sync

62
00:02:54,590 --> 00:02:58,850
system code because it is the latency

63
00:02:56,570 --> 00:03:01,760
critical operation and it actually

64
00:02:58,850 --> 00:03:03,859
generates the IO operations in the

65
00:03:01,760 --> 00:03:06,560
original F sync path on a journaling

66
00:03:03,860 --> 00:03:08,959
file systems such as ext for a single F

67
00:03:06,560 --> 00:03:12,350
Sync system called causes 3 right I use

68
00:03:08,959 --> 00:03:14,600
our scheme our scheme is to overlap our

69
00:03:12,350 --> 00:03:17,600
you stack operations with previous I

70
00:03:14,600 --> 00:03:22,010
operations so we can reduce the latency

71
00:03:17,600 --> 00:03:23,810
of the F sing system call in the rest of

72
00:03:22,010 --> 00:03:25,910
the talk I'm going to present the

73
00:03:23,810 --> 00:03:28,519
detailed analysis of the IO pet

74
00:03:25,910 --> 00:03:31,190
behaviors in the Linux kernel each

75
00:03:28,519 --> 00:03:33,440
analysis is followed by our approach to

76
00:03:31,190 --> 00:03:36,170
applying asynchronous IO concept to each

77
00:03:33,440 --> 00:03:38,030
i/o path and I also talked about a

78
00:03:36,170 --> 00:03:40,190
lightweight block I you layer which

79
00:03:38,030 --> 00:03:42,500
provides low latency block all your

80
00:03:40,190 --> 00:03:45,940
services to the criminal then I'll show

81
00:03:42,500 --> 00:03:48,320
you the emulation resource of our skin

82
00:03:45,940 --> 00:03:51,230
I'll show you the vanilla read the path

83
00:03:48,320 --> 00:03:54,500
first this analysis is an average of 4

84
00:03:51,230 --> 00:03:56,540
cure by treat on obtain SSD when a

85
00:03:54,500 --> 00:03:57,290
reduce system call is called the page

86
00:03:56,540 --> 00:03:59,959
acacia layer

87
00:03:57,290 --> 00:04:01,880
first the hinders the read request it

88
00:03:59,959 --> 00:04:04,489
searches for the request page in the

89
00:04:01,880 --> 00:04:06,680
page cache if page is missing it all

90
00:04:04,489 --> 00:04:09,799
locates a new page and recast read the

91
00:04:06,680 --> 00:04:11,630
IO to the filesystem leader in the

92
00:04:09,799 --> 00:04:13,850
filesystem layer it inserts the

93
00:04:11,630 --> 00:04:16,668
allocated page into the page cache and

94
00:04:13,850 --> 00:04:19,820
locates the logical block address of the

95
00:04:16,668 --> 00:04:21,909
request default page for the next step

96
00:04:19,820 --> 00:04:26,300
it generates block or rickets and

97
00:04:21,910 --> 00:04:28,490
submits it to the block layer the blog

98
00:04:26,300 --> 00:04:30,979
layer transfers block our request to the

99
00:04:28,490 --> 00:04:33,380
device driver layer which also case D

100
00:04:30,979 --> 00:04:35,210
mail orders of the buffer page and some

101
00:04:33,380 --> 00:04:37,879
of mates on Io command on

102
00:04:35,210 --> 00:04:40,489
I'm very comment here

103
00:04:37,879 --> 00:04:42,409
the threat is context which it and when

104
00:04:40,490 --> 00:04:44,449
the device are you is finished the

105
00:04:42,409 --> 00:04:46,969
interrupt handler do located email

106
00:04:44,449 --> 00:04:49,149
address and handles compilation

107
00:04:46,969 --> 00:04:51,558
functions for the request

108
00:04:49,149 --> 00:04:53,929
finally the thread is woken up and

109
00:04:51,559 --> 00:04:59,479
recast data is memory copied into the

110
00:04:53,929 --> 00:05:01,969
user space our first focus is page on

111
00:04:59,479 --> 00:05:04,758
location and DMA address or location or

112
00:05:01,969 --> 00:05:07,399
DMA mapping to take off these operations

113
00:05:04,759 --> 00:05:10,909
from the critical path we maintain a

114
00:05:07,399 --> 00:05:13,580
pool of three pages in which DMA address

115
00:05:10,909 --> 00:05:16,369
is assigned in advance the pool is a

116
00:05:13,580 --> 00:05:18,769
simple link elite link the list of four

117
00:05:16,369 --> 00:05:21,259
kilobyte three pages for each core this

118
00:05:18,769 --> 00:05:23,990
is to simplify the operations to put and

119
00:05:21,259 --> 00:05:26,059
get pages from the free page a pool so

120
00:05:23,990 --> 00:05:29,929
we insert our page a pool or location

121
00:05:26,059 --> 00:05:31,789
into the critical path in doing so when

122
00:05:29,929 --> 00:05:33,919
the read system call is called we use

123
00:05:31,789 --> 00:05:36,589
the page II from the free page pool

124
00:05:33,919 --> 00:05:39,229
during the critical path after our user

125
00:05:36,589 --> 00:05:41,839
is off mission command we run the two

126
00:05:39,229 --> 00:05:47,180
operations asynchronously to the i/o to

127
00:05:41,839 --> 00:05:49,279
refill the page our next focus is page

128
00:05:47,180 --> 00:05:51,559
caching insertion operation in the Linux

129
00:05:49,279 --> 00:05:54,439
kernel when the page is missing in the

130
00:05:51,559 --> 00:05:56,659
page cache a new page is allocated and

131
00:05:54,439 --> 00:05:59,839
indexed in the page cache then our

132
00:05:56,659 --> 00:06:03,979
request is issued this is to prevent

133
00:05:59,839 --> 00:06:05,809
other threads from issuing the issue in

134
00:06:03,979 --> 00:06:06,529
duplicated aggregates to be same file

135
00:06:05,809 --> 00:06:08,779
index

136
00:06:06,529 --> 00:06:10,639
hence this sequence has a trade-off

137
00:06:08,779 --> 00:06:12,439
between overheads on page cache

138
00:06:10,639 --> 00:06:16,699
operations and preventing from

139
00:06:12,439 --> 00:06:20,300
duplicated re requests since our goal is

140
00:06:16,699 --> 00:06:22,339
to reduce the latest AG latency so we

141
00:06:20,300 --> 00:06:25,219
move this operation to after submitting

142
00:06:22,339 --> 00:06:27,829
an i/o command one problem of this

143
00:06:25,219 --> 00:06:29,829
approach is that if two or more threads

144
00:06:27,829 --> 00:06:32,689
miss the same file page

145
00:06:29,829 --> 00:06:34,839
duplicated our requests can happen to

146
00:06:32,689 --> 00:06:37,819
synchronize this situation in page cache

147
00:06:34,839 --> 00:06:41,360
only one thread can succeed to insert

148
00:06:37,819 --> 00:06:45,110
the page insert the page the other pages

149
00:06:41,360 --> 00:06:47,539
they failed to insert are freed through

150
00:06:45,110 --> 00:06:50,340
our evaluation we found that the

151
00:06:47,539 --> 00:06:55,500
frequency of duplicated our request is

152
00:06:50,340 --> 00:06:57,448
tremolo in practice the last target in

153
00:06:55,500 --> 00:06:59,850
the read path is DMA a mapping operation

154
00:06:57,449 --> 00:07:03,810
which is performed in interrupt handler

155
00:06:59,850 --> 00:07:05,850
after a device IO is finished in our

156
00:07:03,810 --> 00:07:08,250
scheme we delays the DMA or mapping

157
00:07:05,850 --> 00:07:11,220
operation to when a system is idle or

158
00:07:08,250 --> 00:07:13,110
waiting for another I requests this is

159
00:07:11,220 --> 00:07:15,479
an extended version of the different

160
00:07:13,110 --> 00:07:17,790
protection scheme in Linux which it lays

161
00:07:15,479 --> 00:07:21,440
the on mapping of the DMA addresses to

162
00:07:17,790 --> 00:07:24,479
avoid the overhead of iommu operations

163
00:07:21,440 --> 00:07:26,430
because this is related to security of a

164
00:07:24,479 --> 00:07:31,620
system we offer the option to disable

165
00:07:26,430 --> 00:07:33,600
this feature the remaining long latency

166
00:07:31,620 --> 00:07:35,460
overheads in the ready path are the

167
00:07:33,600 --> 00:07:38,490
operations in the block layer and the

168
00:07:35,460 --> 00:07:40,500
device driver layer to reduce the

169
00:07:38,490 --> 00:07:42,479
latency overheads of the current block

170
00:07:40,500 --> 00:07:44,639
are you layer in the Linux kernel we

171
00:07:42,479 --> 00:07:50,669
proposed a lightweight block a layer

172
00:07:44,639 --> 00:07:52,710
which is specialized to nvme SSDs the

173
00:07:50,669 --> 00:07:55,889
Linux kernel uses multi key block layer

174
00:07:52,710 --> 00:07:59,070
for nvme SSDs to scale with multi-core

175
00:07:55,889 --> 00:08:00,780
CPUs and multi SSDs however the current

176
00:07:59,070 --> 00:08:03,300
the multi key block layer has many

177
00:08:00,780 --> 00:08:06,570
operations that hinder low latency block

178
00:08:03,300 --> 00:08:09,360
are your services to the kernel it uses

179
00:08:06,570 --> 00:08:12,900
two structures viu and recast to

180
00:08:09,360 --> 00:08:15,210
represent a single block I request when

181
00:08:12,900 --> 00:08:17,520
functions of API is called a be our

182
00:08:15,210 --> 00:08:20,340
object is converted to the request

183
00:08:17,520 --> 00:08:22,490
object during the conversion many

184
00:08:20,340 --> 00:08:25,289
time-consuming operations are performed

185
00:08:22,490 --> 00:08:28,349
such as are you merging and commented

186
00:08:25,289 --> 00:08:30,690
Tagle a location after the request

187
00:08:28,349 --> 00:08:32,819
object is made it passes through the

188
00:08:30,690 --> 00:08:36,330
scheduling layer software and hardware

189
00:08:32,820 --> 00:08:38,190
context in the block layer finally the

190
00:08:36,330 --> 00:08:40,740
device driver layer requires many

191
00:08:38,190 --> 00:08:44,940
auxiliary data structures to submit an

192
00:08:40,740 --> 00:08:46,920
i/o command many previous works

193
00:08:44,940 --> 00:08:49,740
suggested that these operations are

194
00:08:46,920 --> 00:08:53,130
obstacles to low latency block our

195
00:08:49,740 --> 00:08:54,180
services first the i/o merging operation

196
00:08:53,130 --> 00:08:58,439
is inefficient

197
00:08:54,180 --> 00:09:00,479
invest storage devices many studies

198
00:08:58,440 --> 00:09:03,600
mentioned the inefficiency of our

199
00:09:00,480 --> 00:09:06,150
scheduling for low latency SSDs as there

200
00:09:03,600 --> 00:09:08,400
paper suggested replacing the softer

201
00:09:06,150 --> 00:09:11,280
side are you scheduling to divide on

202
00:09:08,400 --> 00:09:14,040
scheduling in addition dynamic memory

203
00:09:11,280 --> 00:09:19,020
allocations take tens to hundreds of CPU

204
00:09:14,040 --> 00:09:21,900
cycles which prolongs the i/o latency to

205
00:09:19,020 --> 00:09:24,120
minimize these these latency overheads

206
00:09:21,900 --> 00:09:25,860
we implemented a lightweight block aisle

207
00:09:24,120 --> 00:09:28,290
layer which replaces the current

208
00:09:25,860 --> 00:09:31,620
multi-block layer and provides low

209
00:09:28,290 --> 00:09:34,680
latency block on your services it uses

210
00:09:31,620 --> 00:09:37,410
only a single object called ldiot to

211
00:09:34,680 --> 00:09:40,020
represent the block I request a bearer

212
00:09:37,410 --> 00:09:42,600
has only essential arguments for to make

213
00:09:40,020 --> 00:09:45,210
and vania requests including the DME

214
00:09:42,600 --> 00:09:48,140
order it so it can eliminate unnecessary

215
00:09:45,210 --> 00:09:51,720
structure convergence and all locations

216
00:09:48,140 --> 00:09:53,760
also we maintain per CPU l be able to

217
00:09:51,720 --> 00:09:56,370
support a long list object or location

218
00:09:53,760 --> 00:09:59,490
and it also simplifies commented taking

219
00:09:56,370 --> 00:10:02,190
operation through the logic block layer

220
00:09:59,490 --> 00:10:04,680
the device durable layer requires only a

221
00:10:02,190 --> 00:10:06,660
single dynamic memory or location which

222
00:10:04,680 --> 00:10:12,000
is the physical region page at least for

223
00:10:06,660 --> 00:10:13,860
the Mme protocol so this figure is the

224
00:10:12,000 --> 00:10:16,290
read passed timeline before applying

225
00:10:13,860 --> 00:10:18,180
lightweight block layer and by using

226
00:10:16,290 --> 00:10:19,980
lightweight block layer we can further

227
00:10:18,180 --> 00:10:24,359
the reduce you can further reduce the

228
00:10:19,980 --> 00:10:26,940
latency finally compared to original

229
00:10:24,360 --> 00:10:28,890
read path you can notice that our scheme

230
00:10:26,940 --> 00:10:33,750
reduce the length of the critical path

231
00:10:28,890 --> 00:10:36,990
in the read the path along now let's

232
00:10:33,750 --> 00:10:39,090
talk about the right path as I mentioned

233
00:10:36,990 --> 00:10:41,280
in the right path overview we focused on

234
00:10:39,090 --> 00:10:43,380
the F single system call with ext for

235
00:10:41,280 --> 00:10:45,420
journaling file system in these systems

236
00:10:43,380 --> 00:10:48,570
when I have synthesis Finkle is called

237
00:10:45,420 --> 00:10:50,579
on application third writes factory data

238
00:10:48,570 --> 00:10:53,910
blocks to storage and waits for the

239
00:10:50,580 --> 00:10:56,160
composition of the iOS then he wakes up

240
00:10:53,910 --> 00:10:58,290
the journaling third or jvd two in the

241
00:10:56,160 --> 00:10:59,810
journaling thread it first prepares

242
00:10:58,290 --> 00:11:02,730
journal blocks which are usually

243
00:10:59,810 --> 00:11:04,530
filesystem metadata blocks then he

244
00:11:02,730 --> 00:11:09,120
issues the journal block i/o and waits

245
00:11:04,530 --> 00:11:10,890
for the composition of that finally the

246
00:11:09,120 --> 00:11:12,720
journaling thread prepares the comic

247
00:11:10,890 --> 00:11:15,030
block and issues the i/o command a

248
00:11:12,720 --> 00:11:17,430
foolish comment is enforced between the

249
00:11:15,030 --> 00:11:20,370
journal block IO and the commit block IO

250
00:11:17,430 --> 00:11:25,380
to enforce to to insert the ordering

251
00:11:20,370 --> 00:11:27,510
between the two rights in our purpose

252
00:11:25,380 --> 00:11:30,090
they have sink paths we also write back

253
00:11:27,510 --> 00:11:31,860
three data blocks first and you wake up

254
00:11:30,090 --> 00:11:33,840
the journaling thread in advance -

255
00:11:31,860 --> 00:11:36,180
although the journaling thread performs

256
00:11:33,840 --> 00:11:39,830
is computation while the data block IO

257
00:11:36,180 --> 00:11:42,270
is ongoing the journaling thread

258
00:11:39,830 --> 00:11:44,880
prepares journal blocks in issues dojo

259
00:11:42,270 --> 00:11:46,470
no block IO at the moment instead of

260
00:11:44,880 --> 00:11:49,290
waiting for the compilation of the

261
00:11:46,470 --> 00:11:52,760
journal block IO the journaling thread

262
00:11:49,290 --> 00:11:55,500
continues to prepare the commit block

263
00:11:52,760 --> 00:11:58,020
then it waits for the compilation of all

264
00:11:55,500 --> 00:12:01,170
previous iOS associated with this file

265
00:11:58,020 --> 00:12:02,490
system transaction peniston's of flash

266
00:12:01,170 --> 00:12:04,920
comment to ensure the ordering

267
00:12:02,490 --> 00:12:09,720
constraint finally it issues the commit

268
00:12:04,920 --> 00:12:12,030
block io as a result the total latency

269
00:12:09,720 --> 00:12:13,950
of the F seeing system call is reduced

270
00:12:12,030 --> 00:12:18,240
because the computation parts are

271
00:12:13,950 --> 00:12:22,110
overlapped with the i/o operations now

272
00:12:18,240 --> 00:12:24,090
the evaluation we implemented our scheme

273
00:12:22,110 --> 00:12:26,610
in the Linux kernel and evaluated our

274
00:12:24,090 --> 00:12:28,920
scheme using Samsung gsst and Intel

275
00:12:26,610 --> 00:12:31,050
obtain SSD we measure the performance

276
00:12:28,920 --> 00:12:33,449
using various synthetic and real-world

277
00:12:31,050 --> 00:12:35,609
roca loads because of the time limit

278
00:12:33,450 --> 00:12:40,770
this talk only covers the results using

279
00:12:35,610 --> 00:12:41,580
obtain SSD first in the random read

280
00:12:40,770 --> 00:12:43,920
performance

281
00:12:41,580 --> 00:12:46,020
the figure to the left shows the random

282
00:12:43,920 --> 00:12:49,260
read latency with variant block sizes

283
00:12:46,020 --> 00:12:52,370
using a single thread our scheme reduce

284
00:12:49,260 --> 00:12:54,630
the latency by up to 20%

285
00:12:52,370 --> 00:12:57,180
one thing we want to note here is that

286
00:12:54,630 --> 00:12:58,939
when our scheme works by polling our

287
00:12:57,180 --> 00:13:01,979
scheme achieved a single-digit

288
00:12:58,940 --> 00:13:04,470
microsecond IO latency seven point six

289
00:13:01,980 --> 00:13:07,920
microseconds in four kilobyte this was

290
00:13:04,470 --> 00:13:09,840
not viable in the vanilla you stack the

291
00:13:07,920 --> 00:13:11,790
figure to the right shows four kilobyte

292
00:13:09,840 --> 00:13:14,340
random mill throughput with varying the

293
00:13:11,790 --> 00:13:16,230
number of threads we over committed the

294
00:13:14,340 --> 00:13:19,470
number of threads to the available

295
00:13:16,230 --> 00:13:21,960
course to mimic I could've IO as you can

296
00:13:19,470 --> 00:13:23,880
see in the figure until the device is

297
00:13:21,960 --> 00:13:25,830
saturated our scheme shows higher

298
00:13:23,880 --> 00:13:28,530
performance than the Linux team 10 the

299
00:13:25,830 --> 00:13:30,189
original Linux when the pig when the

300
00:13:28,530 --> 00:13:35,379
device becomes the

301
00:13:30,189 --> 00:13:37,689
polic he shows the simulator put the

302
00:13:35,379 --> 00:13:40,809
next is random right followed by F think

303
00:13:37,689 --> 00:13:42,639
with ext4 ordered journaling mode with

304
00:13:40,809 --> 00:13:45,160
the same experimental factors such as

305
00:13:42,639 --> 00:13:47,859
block sizes and the number of threats

306
00:13:45,160 --> 00:13:50,618
our scheme reduced the latency by up to

307
00:13:47,859 --> 00:13:51,910
26% and improve the throughput by up to

308
00:13:50,619 --> 00:13:54,160
27%

309
00:13:51,910 --> 00:13:56,679
however with increasing the number of

310
00:13:54,160 --> 00:13:59,410
threats overlap computation and i/o

311
00:13:56,679 --> 00:14:03,939
naturally occurs thereby diminishing the

312
00:13:59,410 --> 00:14:06,160
performance benefit of our skin for

313
00:14:03,939 --> 00:14:08,230
real-world workload evaluation we used

314
00:14:06,160 --> 00:14:11,410
the PP bench redundant my first think

315
00:14:08,230 --> 00:14:13,749
workload from rocks TB each experiment

316
00:14:11,410 --> 00:14:17,769
shows performance improvement by up to

317
00:14:13,749 --> 00:14:20,709
27% a forty four percent respectively in

318
00:14:17,769 --> 00:14:22,779
our scheme but similarly with increasing

319
00:14:20,709 --> 00:14:25,149
the number of threads the performance

320
00:14:22,779 --> 00:14:29,709
benefit of our scheme is diminishing by

321
00:14:25,149 --> 00:14:31,319
natural overlap computation with ru now

322
00:14:29,709 --> 00:14:34,118
let me conclude the talk are

323
00:14:31,319 --> 00:14:36,759
asynchronous I use take enable to

324
00:14:34,119 --> 00:14:39,069
overlap computation with IO by applying

325
00:14:36,759 --> 00:14:42,369
asynchronous IO concept to the corner is

326
00:14:39,069 --> 00:14:44,110
take itself in addition we proposed or

327
00:14:42,369 --> 00:14:46,839
lightweight block are you lightweight

328
00:14:44,110 --> 00:14:49,929
block are you layer specialized to low

329
00:14:46,839 --> 00:14:51,459
latency nvme SSDs finally we achieved

330
00:14:49,929 --> 00:14:54,488
high performance gain in real-world

331
00:14:51,459 --> 00:14:58,599
workload the source code is over lever

332
00:14:54,489 --> 00:15:00,160
at this the key table repository this is

333
00:14:58,600 --> 00:15:01,569
the end of my talk thank you for your

334
00:15:00,160 --> 00:15:02,740
attention and I'm ready to take

335
00:15:01,569 --> 00:15:09,000
questions

336
00:15:02,740 --> 00:15:17,650
[Applause]

337
00:15:09,000 --> 00:15:20,290
questions so your work is very good

338
00:15:17,650 --> 00:15:22,959
about latency but what about throughput

339
00:15:20,290 --> 00:15:26,620
I mean on these devices it's not so easy

340
00:15:22,960 --> 00:15:30,600
to get the full throughput of the device

341
00:15:26,620 --> 00:15:30,600
do you have any experience with them

342
00:15:32,400 --> 00:15:40,569
yeah we we are more focused on the

343
00:15:35,440 --> 00:15:44,500
latency so we need to we need more study

344
00:15:40,570 --> 00:15:54,040
with with the improve the throughput as

345
00:15:44,500 --> 00:15:55,780
you mentioned okay Sudarshan doctors can

346
00:15:54,040 --> 00:15:59,230
you comment on the increase in CPU

347
00:15:55,780 --> 00:16:00,819
utilization inside the kernel layer with

348
00:15:59,230 --> 00:16:03,220
this the number of kernel threads

349
00:16:00,820 --> 00:16:05,980
additional kernel threads that would you

350
00:16:03,220 --> 00:16:11,380
would require and the overall system CPU

351
00:16:05,980 --> 00:16:14,170
utilization can you comment on the

352
00:16:11,380 --> 00:16:17,050
overall CPU utilization increase in the

353
00:16:14,170 --> 00:16:23,349
system and does it have any impact on

354
00:16:17,050 --> 00:16:27,160
the application I'm sorry for their said

355
00:16:23,350 --> 00:16:28,780
to you so if you're going to do a lot of

356
00:16:27,160 --> 00:16:30,550
asynchronous operation inside the kernel

357
00:16:28,780 --> 00:16:32,470
right I'm assuming everything is

358
00:16:30,550 --> 00:16:34,870
happening in the filesystem layer right

359
00:16:32,470 --> 00:16:37,320
yes yes so which means that you're going

360
00:16:34,870 --> 00:16:40,600
to increase the number of threads in the

361
00:16:37,320 --> 00:16:43,870
inside the kernel right you're going to

362
00:16:40,600 --> 00:16:46,900
use more number of CPU threads inside

363
00:16:43,870 --> 00:16:52,090
the kernel I'm certain you take it

364
00:16:46,900 --> 00:16:54,280
offline I'm sorry maybe I'll just

365
00:16:52,090 --> 00:16:55,450
rephrase his questions slightly

366
00:16:54,280 --> 00:16:59,380
differently in a more straightforward

367
00:16:55,450 --> 00:17:02,680
way so the operation is complete faster

368
00:16:59,380 --> 00:17:06,030
yes but you consume more CPU or less CPU

369
00:17:02,680 --> 00:17:06,030
than in the vanilla case

370
00:17:09,430 --> 00:17:14,770
do you do more work or less work or the

371
00:17:13,270 --> 00:17:17,020
same amount of work but in a different

372
00:17:14,770 --> 00:17:20,160
but in a better overlap with the i/o I

373
00:17:17,020 --> 00:17:20,160
think that's your question

374
00:17:28,940 --> 00:17:35,150
oh I'm sorry I'm sorry I can't I can't

375
00:17:33,140 --> 00:17:36,950
understand okay well maybe we'll

376
00:17:35,150 --> 00:17:38,270
probably both take it offline then okay

377
00:17:36,950 --> 00:17:39,230
thank you very much we should thank you

378
00:17:38,270 --> 00:17:41,120
oh one more question

379
00:17:39,230 --> 00:17:43,550
yep go ahead a Jason Hennessey from

380
00:17:41,120 --> 00:17:46,659
NetApp I'm wondering if you took a look

381
00:17:43,550 --> 00:17:50,629
at the effects of your changes on the

382
00:17:46,660 --> 00:17:53,570
variability so like the 99% latency and

383
00:17:50,630 --> 00:17:55,280
the predictability you showed some

384
00:17:53,570 --> 00:17:57,200
really great latency improvements I'm

385
00:17:55,280 --> 00:18:00,350
just wondering if that if what the cost

386
00:17:57,200 --> 00:18:07,040
was so do you have any ideas on the

387
00:18:00,350 --> 00:18:12,620
variability like 95th percentile latency

388
00:18:07,040 --> 00:18:17,200
99th percentile latency yes you mean you

389
00:18:12,620 --> 00:18:19,610
mean the concerned about latency right

390
00:18:17,200 --> 00:18:22,850
the tests are good that's a good

391
00:18:19,610 --> 00:18:25,129
question but our focus is the it's the

392
00:18:22,850 --> 00:18:28,100
general area of the highest Excel is the

393
00:18:25,130 --> 00:18:31,010
none of our focus but I think it is the

394
00:18:28,100 --> 00:18:34,909
good question to to improve our future

395
00:18:31,010 --> 00:18:38,210
work so okay thank you yes thank you

396
00:18:34,910 --> 00:18:40,480
okay let's thank the speaker one more

397
00:18:38,210 --> 00:18:40,480
time

