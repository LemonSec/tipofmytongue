1
00:00:10,690 --> 00:00:19,579
okay so hi everyone my name is EJ and

2
00:00:15,349 --> 00:00:21,320
we're from Amazon CEO team so today I'm

3
00:00:19,579 --> 00:00:26,559
going to present a comprehensive

4
00:00:21,320 --> 00:00:30,529
approach our model inference superiors

5
00:00:26,559 --> 00:00:33,680
so this presentation consists of three

6
00:00:30,529 --> 00:00:36,649
parts first I'll describe the motivation

7
00:00:33,680 --> 00:00:38,769
of our work and then the approach we

8
00:00:36,649 --> 00:00:42,440
used to tackle the problem including

9
00:00:38,770 --> 00:00:46,090
tensile level of the musician and tensor

10
00:00:42,440 --> 00:00:50,860
graph joint tuning finally out our

11
00:00:46,090 --> 00:00:50,860
evaluation results and conclusions

12
00:00:51,430 --> 00:01:00,290
so first the motivation so nowadays

13
00:00:57,640 --> 00:01:02,630
convolution neural network has become

14
00:01:00,290 --> 00:01:06,130
the state of art for many tasks

15
00:01:02,630 --> 00:01:10,340
including image collision detection

16
00:01:06,130 --> 00:01:13,130
tracking etc so as I'm Sonia we work

17
00:01:10,340 --> 00:01:15,709
backwards from our customers so we got

18
00:01:13,130 --> 00:01:17,318
feedbacks that many customers are using

19
00:01:15,709 --> 00:01:21,619
some other influence in their

20
00:01:17,319 --> 00:01:25,429
applications so similarly eater was /

21
00:01:21,619 --> 00:01:28,399
deployment of CPUs in servers clients

22
00:01:25,429 --> 00:01:31,939
and edge devices makes this platform

23
00:01:28,399 --> 00:01:34,359
attractive so in general roughly 90% of

24
00:01:31,939 --> 00:01:37,729
different use cases are doing inference

25
00:01:34,359 --> 00:01:40,399
so performing same model inference

26
00:01:37,729 --> 00:01:42,819
efficiently our CPUs is critical to many

27
00:01:40,399 --> 00:01:42,819
users

28
00:01:43,920 --> 00:01:50,370
so there are many two approaches to

29
00:01:47,700 --> 00:01:54,030
optimize your model influence on CPUs

30
00:01:50,370 --> 00:01:56,970
the first one is taken by the deep

31
00:01:54,030 --> 00:02:00,870
learning frameworks so the use external

32
00:01:56,970 --> 00:02:06,840
math libraries such as mko open plus etc

33
00:02:00,870 --> 00:02:10,670
to to accelerate individual condos or or

34
00:02:06,840 --> 00:02:13,350
the extractor sub graph on the network

35
00:02:10,669 --> 00:02:16,559
passes your pasta to external

36
00:02:13,350 --> 00:02:19,170
acceleration libraries such as cancer RT

37
00:02:16,560 --> 00:02:23,400
and leave the remainders to themselves

38
00:02:19,170 --> 00:02:26,250
so in post scenarios since the free mode

39
00:02:23,400 --> 00:02:30,060
treaters salvation library's a black box

40
00:02:26,250 --> 00:02:32,510
is misses the opportunity to join tune

41
00:02:30,060 --> 00:02:35,610
the performance of the model as a whole

42
00:02:32,510 --> 00:02:38,579
somewhat more overt external libraries

43
00:02:35,610 --> 00:02:41,130
only supports a subset of the used used

44
00:02:38,580 --> 00:02:46,920
operators so the performance could be

45
00:02:41,130 --> 00:02:49,850
poor for uncommon others another

46
00:02:46,920 --> 00:02:52,380
approach of doing in modern influence

47
00:02:49,850 --> 00:02:56,130
optimization is to use the deep learning

48
00:02:52,380 --> 00:02:59,070
compiler deep learning compilers convert

49
00:02:56,130 --> 00:03:03,450
model from different frameworks to in a

50
00:02:59,070 --> 00:03:06,000
unified are are such RS has to two

51
00:03:03,450 --> 00:03:08,488
levels the high level describes a

52
00:03:06,000 --> 00:03:11,430
commutation graph of a deep learning

53
00:03:08,489 --> 00:03:13,830
model the low-level on the other hand

54
00:03:11,430 --> 00:03:19,140
it took describes their computation

55
00:03:13,830 --> 00:03:22,170
logic of each operator so a unifying the

56
00:03:19,140 --> 00:03:24,929
representation the compiler is able to

57
00:03:22,170 --> 00:03:27,869
perform both tensor and the graph level

58
00:03:24,930 --> 00:03:32,870
optimization such as fusion pre compute

59
00:03:27,870 --> 00:03:36,239
free computing etc by coordinating

60
00:03:32,870 --> 00:03:39,660
between the lower level and high level

61
00:03:36,239 --> 00:03:42,180
RRS deepening compiler is able to do

62
00:03:39,660 --> 00:03:45,390
joint optimization instead of doing inch

63
00:03:42,180 --> 00:03:47,670
separately however this kind of joint of

64
00:03:45,390 --> 00:03:50,760
the optimization is still limited in

65
00:03:47,670 --> 00:03:54,540
their modern deep learning compilers our

66
00:03:50,760 --> 00:03:58,709
work is built on top of T VM which is

67
00:03:54,540 --> 00:04:01,260
one of the most popular deep learning

68
00:03:58,709 --> 00:04:04,200
into open window is another depending

69
00:04:01,260 --> 00:04:07,019
compilers that focus mostly on interest

70
00:04:04,200 --> 00:04:10,280
devices so we used it as one of our

71
00:04:07,020 --> 00:04:10,280
performance baselines

72
00:04:11,750 --> 00:04:18,630
so we have introduced the motivation

73
00:04:14,580 --> 00:04:22,290
there how our works doing now let's see

74
00:04:18,630 --> 00:04:26,480
how we tackle the problems since more

75
00:04:22,290 --> 00:04:28,740
than 90% of computation is spent on

76
00:04:26,480 --> 00:04:31,229
convolution so I would like to first

77
00:04:28,740 --> 00:04:38,520
describe the optimization idea behind it

78
00:04:31,229 --> 00:04:42,210
and how we put a tunable template the

79
00:04:38,520 --> 00:04:44,580
computation fashion of convolution is

80
00:04:42,210 --> 00:04:48,169
similar to matrix multiplication

81
00:04:44,580 --> 00:04:50,669
both consisting of a number of

82
00:04:48,169 --> 00:04:56,669
multiplications and the applications in

83
00:04:50,669 --> 00:04:59,010
industry loops for convolution for

84
00:04:56,669 --> 00:05:01,859
convolution it consists of 6 loops of

85
00:04:59,010 --> 00:05:05,250
computation in which the kernel ways

86
00:05:01,860 --> 00:05:09,870
kernel height and input channel are the

87
00:05:05,250 --> 00:05:12,890
retouching axis the optimization of

88
00:05:09,870 --> 00:05:16,289
Colossians and that occurs to matrix

89
00:05:12,890 --> 00:05:19,380
multiplication so for so first to

90
00:05:16,289 --> 00:05:21,780
utilize the data locality we try out the

91
00:05:19,380 --> 00:05:27,000
channel size and put it to the innermost

92
00:05:21,780 --> 00:05:30,179
axis we also leverage FMA instruction to

93
00:05:27,000 --> 00:05:33,479
vectorize the competition and enroll a

94
00:05:30,180 --> 00:05:37,820
specific size of loops to fully utilize

95
00:05:33,479 --> 00:05:40,800
the registers finally we implemented a

96
00:05:37,820 --> 00:05:43,370
high-performance retro to take advantage

97
00:05:40,800 --> 00:05:43,370
of parallelism

98
00:05:47,340 --> 00:05:53,700
conceptually this is a well taken

99
00:05:50,610 --> 00:05:57,410
optimization problem however in practice

100
00:05:53,700 --> 00:05:59,849
for different input workloads the

101
00:05:57,410 --> 00:06:02,010
optimisation configuration could be very

102
00:05:59,850 --> 00:06:05,280
different for example if the channel

103
00:06:02,010 --> 00:06:07,610
size equals to 8 then it does not make

104
00:06:05,280 --> 00:06:11,400
sense to Todd by 16 right

105
00:06:07,610 --> 00:06:13,830
so using TV Ames high level DSL we

106
00:06:11,400 --> 00:06:16,260
create templates where some of the

107
00:06:13,830 --> 00:06:18,539
parameters are tunable so that the

108
00:06:16,260 --> 00:06:21,680
system can find the best configuration

109
00:06:18,540 --> 00:06:25,140
for the specific convolution workload

110
00:06:21,680 --> 00:06:28,170
here we choose the block size of input

111
00:06:25,140 --> 00:06:32,219
and output channels and the number of

112
00:06:28,170 --> 00:06:35,670
used CPU registers and whether to roll

113
00:06:32,220 --> 00:06:38,850
loops to be tunable parameters so as

114
00:06:35,670 --> 00:06:41,640
toonder as tuning happens within the

115
00:06:38,850 --> 00:06:45,810
single conversion operation can record

116
00:06:41,640 --> 00:06:48,200
local search during the automatic

117
00:06:45,810 --> 00:06:52,230
performance tuning the local tuner

118
00:06:48,200 --> 00:06:55,110
exhaustively tries different combination

119
00:06:52,230 --> 00:06:58,380
of parameters and runs on the real

120
00:06:55,110 --> 00:07:01,230
device so we record the performance of

121
00:06:58,380 --> 00:07:04,890
each configuration for graph tuner which

122
00:07:01,230 --> 00:07:07,110
I will be elaborating layer we do not

123
00:07:04,890 --> 00:07:09,479
only keep the best performance

124
00:07:07,110 --> 00:07:11,670
configuration for each convolution

125
00:07:09,480 --> 00:07:14,790
because the local timer doesn't

126
00:07:11,670 --> 00:07:20,550
necessarily necessarily to lead to

127
00:07:14,790 --> 00:07:26,970
global optimum now let's move forward to

128
00:07:20,550 --> 00:07:30,630
tensor and a graph level join tuning so

129
00:07:26,970 --> 00:07:33,450
let's first take a look at an example so

130
00:07:30,630 --> 00:07:37,080
normally the default input date layout

131
00:07:33,450 --> 00:07:41,460
is and CHW where n represents the patch

132
00:07:37,080 --> 00:07:44,880
toys and C represents the input channel

133
00:07:41,460 --> 00:07:48,599
H and W represent feature Maps height in

134
00:07:44,880 --> 00:07:52,230
a waist respectively the order is the

135
00:07:48,600 --> 00:07:55,440
same as that in memory which means n is

136
00:07:52,230 --> 00:07:57,039
the outermost access and tributes the in

137
00:07:55,440 --> 00:08:00,039
the most

138
00:07:57,039 --> 00:08:03,089
so differently today odds will lead to

139
00:08:00,039 --> 00:08:06,009
vastly different performance our

140
00:08:03,089 --> 00:08:09,129
previous local search shows that tile in

141
00:08:06,009 --> 00:08:13,059
the input channel by 16 could generally

142
00:08:09,129 --> 00:08:18,659
get great performance if we were to do

143
00:08:13,059 --> 00:08:18,659
that we can apply layout transform

144
00:08:18,899 --> 00:08:27,879
operation on the input data produces the

145
00:08:23,409 --> 00:08:30,009
new data layout and CHW 16c so where the

146
00:08:27,879 --> 00:08:33,250
sixteen here is the size of a tiled

147
00:08:30,009 --> 00:08:35,949
input channel and we put a six to

148
00:08:33,250 --> 00:08:39,490
sixteen size the access to the in the

149
00:08:35,948 --> 00:08:42,010
most now we can change each of the

150
00:08:39,490 --> 00:08:44,610
conversion computer working on the new

151
00:08:42,010 --> 00:08:48,490
data layout so note that

152
00:08:44,610 --> 00:08:51,339
valeu is an element-wise operations that

153
00:08:48,490 --> 00:08:54,550
can take any date layout and produce

154
00:08:51,339 --> 00:08:57,329
correct result so the new data layer can

155
00:08:54,550 --> 00:09:01,899
diffuse through the network and to it

156
00:08:57,329 --> 00:09:05,410
which is further operation Fratton can

157
00:09:01,899 --> 00:09:08,949
only take her original data layer so we

158
00:09:05,410 --> 00:09:15,449
have to insert an extra little layout

159
00:09:08,949 --> 00:09:18,189
transform before flatten also note that

160
00:09:15,449 --> 00:09:21,540
kernel also needs to be transformed

161
00:09:18,190 --> 00:09:24,490
according to the new data layout why for

162
00:09:21,540 --> 00:09:28,540
inference kono transform can be pre

163
00:09:24,490 --> 00:09:31,630
calculated during compile time in our

164
00:09:28,540 --> 00:09:34,510
solution we implemented a systematic

165
00:09:31,630 --> 00:09:37,360
approach to modify the layout between

166
00:09:34,510 --> 00:09:39,339
the layers and have the optimized layer

167
00:09:37,360 --> 00:09:44,199
pass through the network as well as

168
00:09:39,339 --> 00:09:46,630
possible but if what if serious these

169
00:09:44,199 --> 00:09:49,300
two conclusions have different layout

170
00:09:46,630 --> 00:09:52,630
what layer should we choose to achieve

171
00:09:49,300 --> 00:09:58,209
the best end-to-end performance that's a

172
00:09:52,630 --> 00:10:00,970
question so here's an example of two

173
00:09:58,209 --> 00:10:04,540
back-to-back collusion as we've just

174
00:10:00,970 --> 00:10:08,949
mentioned we tell the channel to Peter

175
00:10:04,540 --> 00:10:09,790
in the most axes here for first

176
00:10:08,949 --> 00:10:14,020
convolution

177
00:10:09,790 --> 00:10:17,469
we can choose tiling size 16 8 and 4 and

178
00:10:14,020 --> 00:10:22,240
for the second there are four choices 16

179
00:10:17,470 --> 00:10:24,550
8 4 and 32 if we want to decide the best

180
00:10:22,240 --> 00:10:27,910
end and the running time of each of

181
00:10:24,550 --> 00:10:31,719
these two layers for Sam we need to

182
00:10:27,910 --> 00:10:36,400
check ever combination between the two

183
00:10:31,720 --> 00:10:39,790
layers so for example in CHW 16 c 2 n CH

184
00:10:36,400 --> 00:10:43,300
double 16 c does not require any layout

185
00:10:39,790 --> 00:10:48,790
transform well we need to insert the or

186
00:10:43,300 --> 00:10:51,310
transform for the other G so to achieve

187
00:10:48,790 --> 00:10:55,270
the best running time for an assister

188
00:10:51,310 --> 00:10:58,180
blue 16 C we need to choose an CH double

189
00:10:55,270 --> 00:11:05,199
AC for the first collusion here you can

190
00:10:58,180 --> 00:11:07,810
see that avoiding so the key here is the

191
00:11:05,200 --> 00:11:10,290
point here that voiding layer transform

192
00:11:07,810 --> 00:11:15,780
is not necessary to be the best strategy

193
00:11:10,290 --> 00:11:20,589
on the other hand in CHWs for c and c HW

194
00:11:15,780 --> 00:11:21,730
32 C are the best optimal local optimal

195
00:11:20,590 --> 00:11:26,080
in both layers

196
00:11:21,730 --> 00:11:28,990
well now the efficiently uh transform

197
00:11:26,080 --> 00:11:33,310
mixture overall running time less

198
00:11:28,990 --> 00:11:35,550
restricted so does the running time of

199
00:11:33,310 --> 00:11:38,800
layer transform can largely affect

200
00:11:35,550 --> 00:11:41,469
engine performance it is also the major

201
00:11:38,800 --> 00:11:45,400
reasons that local team at not necessary

202
00:11:41,470 --> 00:11:47,620
to be the global optimal so if we write

203
00:11:45,400 --> 00:11:52,050
down the equation this is the equation

204
00:11:47,620 --> 00:11:52,050
of the named dynamic programming

205
00:11:55,000 --> 00:12:01,259
we implemented an approach to

206
00:11:57,750 --> 00:12:04,480
automatically tune the local options

207
00:12:01,259 --> 00:12:07,089
operation and choose the interlayer date

208
00:12:04,480 --> 00:12:10,689
data layout using algorithm we described

209
00:12:07,089 --> 00:12:14,230
for network as simple as the previous

210
00:12:10,689 --> 00:12:18,430
one can directly apply dynamic dynamic

211
00:12:14,230 --> 00:12:21,339
programming Y for more complex network

212
00:12:18,430 --> 00:12:26,888
structures we need to modify the exam

213
00:12:21,339 --> 00:12:31,449
slightly to record extra States and for

214
00:12:26,889 --> 00:12:35,129
detection models like SSD so well the

215
00:12:31,449 --> 00:12:38,529
SSD here is a shot for a single shot

216
00:12:35,129 --> 00:12:42,310
multi box detected in other drives

217
00:12:38,529 --> 00:12:44,649
so we some for detection so what

218
00:12:42,310 --> 00:12:47,310
detection model access D we use a

219
00:12:44,649 --> 00:12:54,600
heuristic algorithm to search up

220
00:12:47,310 --> 00:12:59,018
approximately theater

221
00:12:54,600 --> 00:13:02,019
evaluation I pick some of the results

222
00:12:59,019 --> 00:13:05,350
presented in the paper to show while

223
00:13:02,019 --> 00:13:06,279
their speed-up ratio is normalized third

224
00:13:05,350 --> 00:13:12,699
hide better

225
00:13:06,279 --> 00:13:15,339
we tested on Intel and AMD CPUs our

226
00:13:12,699 --> 00:13:19,059
solution can consistently achieve the

227
00:13:15,339 --> 00:13:22,480
best performance for verse of CM models

228
00:13:19,059 --> 00:13:23,759
our baseline includes a MUX net and

229
00:13:22,480 --> 00:13:27,879
tensorflow

230
00:13:23,759 --> 00:13:31,300
for internal up in the MD CPUs we also

231
00:13:27,879 --> 00:13:35,829
compare our solution with Intel video we

232
00:13:31,300 --> 00:13:38,469
noticed that also Intel open window

233
00:13:35,829 --> 00:13:40,420
performs fairly good for most of the

234
00:13:38,470 --> 00:13:43,180
networks refer to achieve the same

235
00:13:40,420 --> 00:13:47,649
performance sometimes for example dance

236
00:13:43,180 --> 00:13:50,739
net on M disabuse so for more results

237
00:13:47,649 --> 00:13:55,620
please refer to our paper also note that

238
00:13:50,740 --> 00:13:58,750
our solution arm CPU achieves better

239
00:13:55,620 --> 00:14:02,079
achieve better speed up because existing

240
00:13:58,750 --> 00:14:05,829
accelerate levers that don't up optimize

241
00:14:02,079 --> 00:14:08,589
seriously for arms abused while by

242
00:14:05,829 --> 00:14:11,859
applying auto tuning our solution is

243
00:14:08,590 --> 00:14:18,640
Geneva generalizable to more models and

244
00:14:11,860 --> 00:14:22,300
platforms here's the scalability of our

245
00:14:18,640 --> 00:14:24,490
customized the thread pool most

246
00:14:22,300 --> 00:14:25,029
different if remarks in the compilers

247
00:14:24,490 --> 00:14:28,839
used

248
00:14:25,029 --> 00:14:32,080
openmp for parallelism our CPUs since

249
00:14:28,839 --> 00:14:34,570
open PS design to be general it might

250
00:14:32,080 --> 00:14:38,110
not provide the best performance for our

251
00:14:34,570 --> 00:14:40,300
cases moreover open p has different

252
00:14:38,110 --> 00:14:43,390
implementation operations so the

253
00:14:40,300 --> 00:14:46,420
performance may vary amount platforms

254
00:14:43,390 --> 00:14:50,050
thus we implement our customized the

255
00:14:46,420 --> 00:14:54,219
thread pool by using lock-free q cyril

256
00:14:50,050 --> 00:14:58,839
pending and column padding without sure

257
00:14:54,220 --> 00:15:01,450
that our worst replicas have better skin

258
00:14:58,839 --> 00:15:08,170
Beatty's and our piece lines as well as

259
00:15:01,450 --> 00:15:10,779
for solution using open PS deceitful so

260
00:15:08,170 --> 00:15:13,870
our solution has been deployed on Amazon

261
00:15:10,779 --> 00:15:16,810
search McNeill which is an inference

262
00:15:13,870 --> 00:15:20,050
service that helping a lot of users

263
00:15:16,810 --> 00:15:27,280
service your tier model influence post

264
00:15:20,050 --> 00:15:30,219
under cloud and at the age conclusion so

265
00:15:27,280 --> 00:15:32,500
in summary in summary we apply tensor

266
00:15:30,220 --> 00:15:35,170
and a graph level joint optimization for

267
00:15:32,500 --> 00:15:37,180
the model inference our solution

268
00:15:35,170 --> 00:15:40,180
consistently achieves the best

269
00:15:37,180 --> 00:15:43,180
performance for the variety of popular

270
00:15:40,180 --> 00:15:46,810
singer models and the CPUs award no

271
00:15:43,180 --> 00:15:50,020
musician ideas are applicable to other

272
00:15:46,810 --> 00:15:52,630
hardware targets like GPUs as well

273
00:15:50,020 --> 00:15:54,970
finally our solution has been open

274
00:15:52,630 --> 00:15:59,770
sourced and deployed to production in a

275
00:15:54,970 --> 00:16:01,830
diverse things so I'm happy to take

276
00:15:59,770 --> 00:16:01,829
questions

277
00:16:03,570 --> 00:16:07,080
and we are hiring

278
00:16:15,850 --> 00:16:21,290
hi little change from Arizona so I'm

279
00:16:19,550 --> 00:16:26,949
wondering what kind of workload is you

280
00:16:21,290 --> 00:16:30,920
use to measure the speed up you mean -

281
00:16:26,950 --> 00:16:34,250
you mean the imprecise such yeah what so

282
00:16:30,920 --> 00:16:35,839
are you sending each image to a model

283
00:16:34,250 --> 00:16:38,420
and then find out how long does it take

284
00:16:35,839 --> 00:16:41,990
or you have something else so we are

285
00:16:38,420 --> 00:16:44,390
basically so I mean the image

286
00:16:41,990 --> 00:16:47,209
classification these kind of deep

287
00:16:44,390 --> 00:16:51,920
learning models you have a common input

288
00:16:47,209 --> 00:16:56,899
size like 30 to 24 by to 1024 on the

289
00:16:51,920 --> 00:17:00,979
input channel is you jittery and so

290
00:16:56,899 --> 00:17:04,609
we're mostly working on the one batch

291
00:17:00,980 --> 00:17:09,530
size input because for influence it is

292
00:17:04,609 --> 00:17:12,349
their common use cases okay so is it

293
00:17:09,530 --> 00:17:16,760
possible to make a larger batch to

294
00:17:12,349 --> 00:17:17,899
further increase the speed inference yes

295
00:17:16,760 --> 00:17:23,540
it is possible

296
00:17:17,900 --> 00:17:28,429
cuz so so here here's the scene that 4

297
00:17:23,540 --> 00:17:31,428
CPUs if we have already utilized paracin

298
00:17:28,429 --> 00:17:33,679
then increase the path does not make any

299
00:17:31,429 --> 00:17:39,250
a speed-up

300
00:17:33,679 --> 00:17:44,470
so we're we have some experiment that to

301
00:17:39,250 --> 00:17:49,309
to say increase the batch size and but

302
00:17:44,470 --> 00:17:53,570
means for modern CPUs we did not see

303
00:17:49,309 --> 00:17:57,080
much speed up okay so also regarding the

304
00:17:53,570 --> 00:17:59,780
speed-up for the 16 core results

305
00:17:57,080 --> 00:18:03,199
did you monitor the utilization of the

306
00:17:59,780 --> 00:18:07,090
CPU and the memory yes yes we did so to

307
00:18:03,200 --> 00:18:11,390
utilize utilization usually is run to

308
00:18:07,090 --> 00:18:14,419
90% both CPU and memory or just the CPU

309
00:18:11,390 --> 00:18:17,960
I just a CPU so memory is not that high

310
00:18:14,420 --> 00:18:21,290
memory is neither high because it is

311
00:18:17,960 --> 00:18:23,210
related to the model itself last

312
00:18:21,290 --> 00:18:26,780
question so how does this compare to

313
00:18:23,210 --> 00:18:28,860
dear Tessa for Excel which also use the

314
00:18:26,780 --> 00:18:34,639
interest instructions to

315
00:18:28,860 --> 00:18:37,320
optimize the work for the performance

316
00:18:34,640 --> 00:18:45,000
illustrated in the paper we didn't use

317
00:18:37,320 --> 00:18:47,700
x-ray but we did this benchmark using

318
00:18:45,000 --> 00:18:51,750
excel cookies because tensor products

319
00:18:47,700 --> 00:18:55,200
x-ray it does not seriously optimize the

320
00:18:51,750 --> 00:18:59,309
four CPUs I guess so we don't see much

321
00:18:55,200 --> 00:19:02,820
speed up with x-ray enabled okay thank

322
00:18:59,309 --> 00:19:04,570
you alright let's thank the speaker one

323
00:19:02,820 --> 00:19:08,089
more time

324
00:19:04,570 --> 00:19:08,089
[Applause]

