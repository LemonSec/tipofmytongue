1
00:00:10,460 --> 00:00:15,020
so hello everyone I'm in hiding from

2
00:00:13,010 --> 00:00:18,230
Shanghai Jiaotong University today I

3
00:00:15,020 --> 00:00:20,240
will introduce ETFs a compression

4
00:00:18,230 --> 00:00:23,210
friendly read-only file system for

5
00:00:20,240 --> 00:00:28,430
resource cast devices the work is done

6
00:00:23,210 --> 00:00:32,720
with Jiang Shi wait Chau and my advisor

7
00:00:28,430 --> 00:00:34,670
hi Bachchan so smartphone is our life

8
00:00:32,720 --> 00:00:36,830
now we want to store everything your

9
00:00:34,670 --> 00:00:38,690
phone and nobody wants to see such a

10
00:00:36,830 --> 00:00:42,019
warning saying that there is no enough

11
00:00:38,690 --> 00:00:44,839
space however a single system partition

12
00:00:42,020 --> 00:00:47,450
can occupy 3 gigabytes and with other

13
00:00:44,840 --> 00:00:50,840
partitions the total story consumed by

14
00:00:47,450 --> 00:00:53,149
the OS is even larger compressing system

15
00:00:50,840 --> 00:00:55,640
resources is a effective approach to

16
00:00:53,149 --> 00:00:58,219
release more space for users and since

17
00:00:55,640 --> 00:01:00,050
system resources are read-only we can

18
00:00:58,219 --> 00:01:01,820
use compressed read-only file system

19
00:01:00,050 --> 00:01:04,509
which could be faster than normal file

20
00:01:01,820 --> 00:01:07,119
system which with compression support

21
00:01:04,509 --> 00:01:09,800
squash FS is the state-of-the-art

22
00:01:07,119 --> 00:01:12,380
compress the read-only file system we

23
00:01:09,800 --> 00:01:16,100
try to use scratch FS on system

24
00:01:12,380 --> 00:01:18,679
resources the Android however the system

25
00:01:16,100 --> 00:01:22,399
lagged and um froze for seconds has been

26
00:01:18,680 --> 00:01:24,890
rebooted so going through the design and

27
00:01:22,400 --> 00:01:27,830
the implementation of scratch FS we find

28
00:01:24,890 --> 00:01:31,460
that scratch FS use a fixed sized input

29
00:01:27,830 --> 00:01:33,320
compression to compress data scratch FS

30
00:01:31,460 --> 00:01:35,809
divides the data into fixed sized chunks

31
00:01:33,320 --> 00:01:38,059
and then it compresses each chunk

32
00:01:35,810 --> 00:01:42,470
separately and the stores and one by one

33
00:01:38,060 --> 00:01:45,500
in the storage this kind of compression

34
00:01:42,470 --> 00:01:47,720
brings two drawbacks full scratch FS the

35
00:01:45,500 --> 00:01:49,520
first is read amplification we're

36
00:01:47,720 --> 00:01:52,850
reading a single byte requires to

37
00:01:49,520 --> 00:01:55,479
decompress the whole chunk this read is

38
00:01:52,850 --> 00:01:57,979
further amplified by i/o since the

39
00:01:55,479 --> 00:02:00,110
decompress the trunks are not aligned to

40
00:01:57,979 --> 00:02:04,610
block boundary forces for space

41
00:02:00,110 --> 00:02:06,740
efficiency the other drawback is massive

42
00:02:04,610 --> 00:02:09,860
massive memory consumption during the

43
00:02:06,740 --> 00:02:12,590
decompression many buffers needs to be

44
00:02:09,860 --> 00:02:15,040
allocated and data is moved among these

45
00:02:12,590 --> 00:02:18,230
buffers during the decompression

46
00:02:15,040 --> 00:02:20,000
this increases the memory pressure in

47
00:02:18,230 --> 00:02:22,190
the Android system and the effects the

48
00:02:20,000 --> 00:02:24,050
performance greatly this two drawbacks

49
00:02:22,190 --> 00:02:26,630
miss makes

50
00:02:24,050 --> 00:02:29,140
scratch FS unacceptable to compress and

51
00:02:26,630 --> 00:02:32,450
stall system resources the Android

52
00:02:29,140 --> 00:02:34,609
within proposed euro FS which uses

53
00:02:32,450 --> 00:02:37,299
another deep compression approach called

54
00:02:34,610 --> 00:02:39,590
fixed as out full compression it

55
00:02:37,300 --> 00:02:41,930
prepares a large amount of data and

56
00:02:39,590 --> 00:02:43,910
compress them as much as possible until

57
00:02:41,930 --> 00:02:46,700
the compressed data reaches of fixed

58
00:02:43,910 --> 00:02:50,960
size then is repeat the procedure until

59
00:02:46,700 --> 00:02:53,989
all data is compressed fixed set out

60
00:02:50,960 --> 00:02:55,730
full compression can reduce redoubling

61
00:02:53,990 --> 00:02:58,520
occasion and has better compression

62
00:02:55,730 --> 00:03:00,260
ratio it also allows in place

63
00:02:58,520 --> 00:03:04,370
decompression which reduce the memory

64
00:03:00,260 --> 00:03:07,429
copies in the decompression so before we

65
00:03:04,370 --> 00:03:09,470
decompress data we need to read the

66
00:03:07,430 --> 00:03:11,780
compressed block we need to read a

67
00:03:09,470 --> 00:03:13,850
compress the blog first so we need to

68
00:03:11,780 --> 00:03:16,220
choose a page to initiate the i/o

69
00:03:13,850 --> 00:03:18,320
request for blocks to be partially

70
00:03:16,220 --> 00:03:20,870
decompressed we allocate our page in

71
00:03:18,320 --> 00:03:22,609
dedicated page cache for IO so that the

72
00:03:20,870 --> 00:03:24,830
following decompression can reuse the

73
00:03:22,610 --> 00:03:27,110
cache the page for blogs to be fully

74
00:03:24,830 --> 00:03:29,960
decompressed we try to reuse the page

75
00:03:27,110 --> 00:03:32,480
allocated by VF s if it is not going to

76
00:03:29,960 --> 00:03:35,270
be used before the compression we call

77
00:03:32,480 --> 00:03:37,609
this in place AO and it reduces memory

78
00:03:35,270 --> 00:03:41,690
allocations and the memory consumption

79
00:03:37,610 --> 00:03:43,430
during the decompression now we

80
00:03:41,690 --> 00:03:45,829
introduce how data is cut in compressed

81
00:03:43,430 --> 00:03:48,260
suppose we have eight broad data blocks

82
00:03:45,830 --> 00:03:50,240
compressed to a single block and the

83
00:03:48,260 --> 00:03:53,510
application is reading two of the data

84
00:03:50,240 --> 00:03:55,820
blocks C brought three and block four so

85
00:03:53,510 --> 00:03:58,940
we FS will first allocate two pages in

86
00:03:55,820 --> 00:04:01,940
the file page cache and let EFS to fill

87
00:03:58,940 --> 00:04:04,640
data in the two pages the first

88
00:04:01,940 --> 00:04:07,430
decompression approach is we map

89
00:04:04,640 --> 00:04:09,250
decompression it first counts the number

90
00:04:07,430 --> 00:04:12,020
of blocks we need to decompress and

91
00:04:09,250 --> 00:04:13,940
allocates physical pages or truths

92
00:04:12,020 --> 00:04:17,690
physical page or true physical pages

93
00:04:13,940 --> 00:04:19,608
from the page cache in the example only

94
00:04:17,690 --> 00:04:22,160
block 0 to block 4 needs to be

95
00:04:19,608 --> 00:04:25,190
decompressed and Yahoo Affairs allocates

96
00:04:22,160 --> 00:04:27,650
physical pages for block 0 1 & 2 and the

97
00:04:25,190 --> 00:04:33,530
selected two pages allocated for blocks

98
00:04:27,650 --> 00:04:36,229
3 & 4 in the page cache EFS then uses

99
00:04:33,530 --> 00:04:37,669
the we map uses remapped to allocate a

100
00:04:36,229 --> 00:04:44,180
virtual memory area

101
00:04:37,669 --> 00:04:47,810
and maps the physical page I see if if

102
00:04:44,180 --> 00:04:50,240
full in place io Yahoo FS also needs to

103
00:04:47,810 --> 00:04:52,819
copy the compressed compressed block to

104
00:04:50,240 --> 00:04:54,560
a temporary percival page because the

105
00:04:52,819 --> 00:04:58,669
page cache pages are going to be

106
00:04:54,560 --> 00:05:02,569
overwritten by the decompression then

107
00:04:58,669 --> 00:05:05,060
the decompress then the decompression

108
00:05:02,569 --> 00:05:07,729
happens and the dictated decompress the

109
00:05:05,060 --> 00:05:10,879
data is over it is written to the vm

110
00:05:07,729 --> 00:05:12,889
area during the decompression the

111
00:05:10,879 --> 00:05:14,719
requested data is directly written to

112
00:05:12,889 --> 00:05:17,270
the page cache page to the page cache

113
00:05:14,719 --> 00:05:21,529
physical pages says there's no need data

114
00:05:17,270 --> 00:05:23,210
movement is required we map

115
00:05:21,529 --> 00:05:25,789
decompression can handle all the

116
00:05:23,210 --> 00:05:28,878
compression cases but it does not but it

117
00:05:25,789 --> 00:05:31,430
needs to UM work we map and we we are

118
00:05:28,879 --> 00:05:35,000
meant for each for each decompression

119
00:05:31,430 --> 00:05:38,210
request and it can allocate an unbounded

120
00:05:35,000 --> 00:05:40,339
number of physical pages for in place IO

121
00:05:38,210 --> 00:05:42,589
it also needs to move the compressed

122
00:05:40,339 --> 00:05:47,360
data also the page cache before the

123
00:05:42,589 --> 00:05:50,479
decompression we didn't propose a buffer

124
00:05:47,360 --> 00:05:53,689
decompression in which we pre allocate a

125
00:05:50,479 --> 00:05:55,938
four-page buffer for each CPU if the

126
00:05:53,689 --> 00:05:57,770
page after the decompression is no more

127
00:05:55,939 --> 00:05:59,870
than four pages with your actually

128
00:05:57,770 --> 00:06:01,729
decompressed to the full page buffer and

129
00:05:59,870 --> 00:06:05,060
copy the requested data to the page

130
00:06:01,729 --> 00:06:08,990
cache buffer decompression requires no

131
00:06:05,060 --> 00:06:10,879
we map or we map OB a map operations no

132
00:06:08,990 --> 00:06:12,830
physical pages are allocated during the

133
00:06:10,879 --> 00:06:15,860
decompression and there's no need to

134
00:06:12,830 --> 00:06:18,349
copy data food in place I'll however it

135
00:06:15,860 --> 00:06:21,639
has it can only work if the decompress

136
00:06:18,349 --> 00:06:24,139
the data is no more than four pages to

137
00:06:21,639 --> 00:06:25,370
further support more general cases with

138
00:06:24,139 --> 00:06:26,930
faster decompression

139
00:06:25,370 --> 00:06:28,729
we have other two decompression

140
00:06:26,930 --> 00:06:30,919
approaches rolling decompression and

141
00:06:28,729 --> 00:06:32,419
impress decompression we also have a

142
00:06:30,919 --> 00:06:34,490
policy to choose from the for

143
00:06:32,419 --> 00:06:36,620
decompression approaches and we propose

144
00:06:34,490 --> 00:06:39,169
the erotic optimizations for performance

145
00:06:36,620 --> 00:06:41,379
and extra features details can be found

146
00:06:39,169 --> 00:06:44,628
in the paper

147
00:06:41,379 --> 00:06:47,599
here comes to the evaluation part we

148
00:06:44,629 --> 00:06:49,839
evaluate we evaluate euro FS using three

149
00:06:47,599 --> 00:06:51,260
platforms a hockey arm development

150
00:06:49,839 --> 00:06:53,420
development board

151
00:06:51,260 --> 00:06:55,880
and the two cans of smartphones were

152
00:06:53,420 --> 00:06:58,910
higher in the smartphone and the low-end

153
00:06:55,880 --> 00:07:01,430
smartphone we use FIO for the micro

154
00:06:58,910 --> 00:07:04,940
benchmark and the workload is being week

155
00:07:01,430 --> 00:07:08,120
in week 9 Yahoo FS is configured using

156
00:07:04,940 --> 00:07:11,450
LG for algorithm with for case as the

157
00:07:08,120 --> 00:07:14,290
full case as the output and we also

158
00:07:11,450 --> 00:07:17,000
evaluate other file systems scratch FS

159
00:07:14,290 --> 00:07:19,850
for example scratch affairs with four

160
00:07:17,000 --> 00:07:24,670
kinds of four kinds of chunk sizes and

161
00:07:19,850 --> 00:07:27,380
the btrfs and yes before and the F 2 FS

162
00:07:24,670 --> 00:07:31,280
btrfs is configured to use LG o

163
00:07:27,380 --> 00:07:33,469
algorithm with 128 kilo batch chunk size

164
00:07:31,280 --> 00:07:37,280
it is also configured in the read-only

165
00:07:33,470 --> 00:07:40,340
mode with integrity checks disabled yet

166
00:07:37,280 --> 00:07:42,710
T fo and F 2 FS does do not compress

167
00:07:40,340 --> 00:07:44,869
data since they don't support it we only

168
00:07:42,710 --> 00:07:47,260
show some selected results here and the

169
00:07:44,870 --> 00:07:49,850
more results can be found in the paper

170
00:07:47,260 --> 00:07:52,039
we first show the throughput of running

171
00:07:49,850 --> 00:07:56,060
of running a file in different rate

172
00:07:52,040 --> 00:07:57,860
patches so x-axis is the read pattern

173
00:07:56,060 --> 00:08:02,120
and also where access is the throughput

174
00:07:57,860 --> 00:08:05,510
of each file system we come so that

175
00:08:02,120 --> 00:08:09,290
btrfs btrfs performs worst in all cases

176
00:08:05,510 --> 00:08:11,750
also PTR FF btrfs supports compression

177
00:08:09,290 --> 00:08:14,000
it is not designed for read-only did for

178
00:08:11,750 --> 00:08:16,310
read-only data thus it has worst

179
00:08:14,000 --> 00:08:20,150
performance the other compressed complex

180
00:08:16,310 --> 00:08:22,280
the read-only file system another

181
00:08:20,150 --> 00:08:24,049
observation is that the larger chunk

182
00:08:22,280 --> 00:08:25,820
size of scratch FS brings better

183
00:08:24,050 --> 00:08:28,490
performance for sequential and random

184
00:08:25,820 --> 00:08:30,740
race since we read the whole file in the

185
00:08:28,490 --> 00:08:32,930
in the experiment a larger chunk size

186
00:08:30,740 --> 00:08:35,450
can reduce the number of decompression

187
00:08:32,929 --> 00:08:38,239
and more ease can be served from the

188
00:08:35,450 --> 00:08:40,460
page cache for the spread for the

189
00:08:38,240 --> 00:08:42,919
strategies however the compressed data

190
00:08:40,460 --> 00:08:45,950
will not be used will not be reused

191
00:08:42,919 --> 00:08:49,660
again just large chunk size brings only

192
00:08:45,950 --> 00:08:52,130
overhead of ab a flea-flicker

193
00:08:49,660 --> 00:08:57,589
amplification IO and the decompression

194
00:08:52,130 --> 00:09:00,230
the reduced read amplification and the

195
00:08:57,590 --> 00:09:03,140
memory consumption makes EFS better the

196
00:09:00,230 --> 00:09:05,230
square of s in most cases and the Yahoo

197
00:09:03,140 --> 00:09:08,480
Affairs achieves better performance

198
00:09:05,230 --> 00:09:10,970
then yes before in both sequential and

199
00:09:08,480 --> 00:09:12,890
the random race and the performance is

200
00:09:10,970 --> 00:09:17,750
compared comparable to a steep oval

201
00:09:12,890 --> 00:09:19,610
strategy for spread race here which here

202
00:09:17,750 --> 00:09:20,960
we show the micro benchmarks about Reed

203
00:09:19,610 --> 00:09:25,490
amplification and the resource

204
00:09:20,960 --> 00:09:27,650
consumption we can see that yellow FS

205
00:09:25,490 --> 00:09:30,170
reduces the reed amplification by more

206
00:09:27,650 --> 00:09:35,840
than 80 times 80 percent compared to

207
00:09:30,170 --> 00:09:38,300
scratch FS and the decompression yo-yo

208
00:09:35,840 --> 00:09:40,430
FS saves more than search more than 40

209
00:09:38,300 --> 00:09:42,229
percent whole storage and it is better

210
00:09:40,430 --> 00:09:44,989
than the square surface configured with

211
00:09:42,230 --> 00:09:48,020
full K compression compression truck and

212
00:09:44,990 --> 00:09:49,360
this is comparable with the scratch FS

213
00:09:48,020 --> 00:09:52,520
8k

214
00:09:49,360 --> 00:09:55,070
Yahoo FS reduces more than 90 percent of

215
00:09:52,520 --> 00:09:59,870
memory memory usage during the

216
00:09:55,070 --> 00:10:02,180
decompression and its memory usage is

217
00:09:59,870 --> 00:10:04,520
smaller than yes before it is similar to

218
00:10:02,180 --> 00:10:09,560
yes yes default which does not compress

219
00:10:04,520 --> 00:10:12,650
data finally we show the evaluation of

220
00:10:09,560 --> 00:10:15,109
real-world application put time we have

221
00:10:12,650 --> 00:10:18,439
13 real-world applications and the

222
00:10:15,110 --> 00:10:25,520
y-axis is a boot time the y-axis is the

223
00:10:18,440 --> 00:10:29,570
boot camp okay

224
00:10:25,520 --> 00:10:33,800
the way the wax wax is the boot time

225
00:10:29,570 --> 00:10:36,350
relative to est 4 in average Yahoo FS

226
00:10:33,800 --> 00:10:37,550
reduced reduces the boot time by three

227
00:10:36,350 --> 00:10:40,340
point two percent for low-end

228
00:10:37,550 --> 00:10:44,959
smartphones and ten nine ten point nine

229
00:10:40,340 --> 00:10:47,210
percent for a high in smartphones Yahoo

230
00:10:44,960 --> 00:10:51,110
FS has been deployed in Huawei

231
00:10:47,210 --> 00:10:53,720
smartphone operating system e UI 9.1 at

232
00:10:51,110 --> 00:10:54,970
the top feature and it is a streamed to

233
00:10:53,720 --> 00:10:58,250
the limbs colonel

234
00:10:54,970 --> 00:11:00,410
yo FS it was more than 30% of storage

235
00:10:58,250 --> 00:11:02,090
used by system resources and the chills

236
00:11:00,410 --> 00:11:04,189
comparable even better performance that

237
00:11:02,090 --> 00:11:05,750
yes t4 which is the default is the

238
00:11:04,190 --> 00:11:06,760
default file system in the Android

239
00:11:05,750 --> 00:11:09,440
system

240
00:11:06,760 --> 00:11:13,130
Yahoo FS has been deployed in modern

241
00:11:09,440 --> 00:11:15,500
tens of millions smartphones ok to

242
00:11:13,130 --> 00:11:17,270
conclude Yahoo FS design healthy

243
00:11:15,500 --> 00:11:18,430
read-only file system with compression

244
00:11:17,270 --> 00:11:20,380
support

245
00:11:18,430 --> 00:11:22,779
it's of all success out full compression

246
00:11:20,380 --> 00:11:24,850
with full decompression approaches which

247
00:11:22,779 --> 00:11:26,260
saves more than 30% storage we're

248
00:11:24,850 --> 00:11:28,360
achieving comparable even better

249
00:11:26,260 --> 00:11:31,839
performance than yes before on tens of

250
00:11:28,360 --> 00:11:45,250
millions of smartphones ok thank you and

251
00:11:31,839 --> 00:11:47,800
I'd like to take questions did you guys

252
00:11:45,250 --> 00:11:51,700
actually look at what was the battery

253
00:11:47,800 --> 00:11:53,709
consumption was consumption the power

254
00:11:51,700 --> 00:11:55,870
consumption so like power consumption

255
00:11:53,709 --> 00:11:58,750
right so if you had XT for squash

256
00:11:55,870 --> 00:12:00,370
affairs and euro Affairs the power

257
00:11:58,750 --> 00:12:03,580
consumption still the same on the

258
00:12:00,370 --> 00:12:07,890
Android phone um as far as o as far as I

259
00:12:03,580 --> 00:12:10,810
know I I don't we do not we haven't

260
00:12:07,890 --> 00:12:14,949
rendered the power consumption for the

261
00:12:10,810 --> 00:12:16,750
different file systems do you have any

262
00:12:14,950 --> 00:12:18,550
idea that would go because I would

263
00:12:16,750 --> 00:12:22,060
assume as the user they would be

264
00:12:18,550 --> 00:12:25,060
concerned about that as well right I

265
00:12:22,060 --> 00:12:27,520
think the power consumption yo FS won't

266
00:12:25,060 --> 00:12:30,369
be worse because it reduces a lot of

267
00:12:27,520 --> 00:12:33,189
things you reduce it reduces the AOS it

268
00:12:30,370 --> 00:12:35,050
reduces the memory allocations so I

269
00:12:33,190 --> 00:12:38,589
think the power consumption won't people

270
00:12:35,050 --> 00:12:41,020
want to be but one feel worse there are

271
00:12:38,589 --> 00:12:43,589
the file systems so stupid the CPU

272
00:12:41,020 --> 00:12:54,010
utilization pretty much remains the same

273
00:12:43,589 --> 00:12:56,440
I don't have a data available but ok so

274
00:12:54,010 --> 00:12:58,630
we can take that offline I think of the

275
00:12:56,440 --> 00:13:04,150
production team has evaluated that but I

276
00:12:58,630 --> 00:13:06,820
don't have the data okay Russ playing

277
00:13:04,150 --> 00:13:08,410
Apple I'm just wondering if during the

278
00:13:06,820 --> 00:13:12,300
course of this work he ran out to any

279
00:13:08,410 --> 00:13:15,550
workloads that you found performed worse

280
00:13:12,300 --> 00:13:19,060
benchmarks or particular scenarios or

281
00:13:15,550 --> 00:13:22,329
applications that unexpectedly triggered

282
00:13:19,060 --> 00:13:27,189
maybe an anti-pattern or pathology that

283
00:13:22,329 --> 00:13:30,040
that was unexpected so your question no

284
00:13:27,190 --> 00:13:32,350
question is about whether do we found

285
00:13:30,040 --> 00:13:38,219
the performance is worse

286
00:13:32,350 --> 00:13:42,850
in some real applications yes they are

287
00:13:38,220 --> 00:13:45,820
okay okay even in the even the boot time

288
00:13:42,850 --> 00:13:55,269
some applications how worst boot time

289
00:13:45,820 --> 00:13:59,170
and but but we sing to perform it okay

290
00:13:55,269 --> 00:14:02,709
about policing the the average

291
00:13:59,170 --> 00:14:05,439
performance better and for for this

292
00:14:02,709 --> 00:14:07,989
cases we have some other we have some

293
00:14:05,440 --> 00:14:11,139
other approaches that we can know for

294
00:14:07,990 --> 00:14:13,660
example we can lose some of the part

295
00:14:11,139 --> 00:14:16,690
some parts of the file and compressed

296
00:14:13,660 --> 00:14:19,630
and we can use this such such a no

297
00:14:16,690 --> 00:14:22,480
combination to mitigate the problem yes

298
00:14:19,630 --> 00:14:27,399
we have another okay we have the backup

299
00:14:22,480 --> 00:14:29,529
slice okay here here is a here the

300
00:14:27,399 --> 00:14:32,980
throughput and space-saving

301
00:14:29,529 --> 00:14:36,189
that is the relation between the circuit

302
00:14:32,980 --> 00:14:38,829
and the space-saving for Yero FS and EST

303
00:14:36,190 --> 00:14:43,410
for and here we can see that there is a

304
00:14:38,829 --> 00:14:46,930
there's a obvious line here and if the

305
00:14:43,410 --> 00:14:50,500
if the space-saving is less than 20 24

306
00:14:46,930 --> 00:14:54,430
percent the performance of Yahoo Yahoo

307
00:14:50,500 --> 00:14:58,720
FS of the sequential rate is much worse

308
00:14:54,430 --> 00:15:01,420
than yes t4 and the full and for this

309
00:14:58,720 --> 00:15:03,959
kind of this kind of scenario we can

310
00:15:01,420 --> 00:15:07,000
just leave some part of the file to be

311
00:15:03,959 --> 00:15:09,609
uncompressed and then with the

312
00:15:07,000 --> 00:15:11,769
combination of the approaches we can get

313
00:15:09,610 --> 00:15:16,209
the better performance or better

314
00:15:11,769 --> 00:15:18,970
performance I'm Shana from western

315
00:15:16,209 --> 00:15:21,010
sister so I have a question about to

316
00:15:18,970 --> 00:15:24,160
about it do you have any consideration

317
00:15:21,010 --> 00:15:27,670
for the locum press conference

318
00:15:24,160 --> 00:15:30,969
compression ratio files such as a PDF or

319
00:15:27,670 --> 00:15:33,759
picture file or PDF right because if you

320
00:15:30,970 --> 00:15:37,630
compress those files then maybe you got

321
00:15:33,759 --> 00:15:41,709
you maybe lose to your gain and maybe

322
00:15:37,630 --> 00:15:45,160
you lose your power consumption so about

323
00:15:41,709 --> 00:15:45,939
what queries yeah I mean some Ingrid

324
00:15:45,160 --> 00:15:49,269
encrypted

325
00:15:45,939 --> 00:15:53,348
for encrypted fire or PDF files or PDF

326
00:15:49,269 --> 00:15:55,979
or video files those files their

327
00:15:53,349 --> 00:16:01,720
compression ratio is really low actually

328
00:15:55,979 --> 00:16:04,419
yes so is there any some okay okay your

329
00:16:01,720 --> 00:16:08,259
question is that do we how we consider

330
00:16:04,419 --> 00:16:10,499
the low compression ratio of some

331
00:16:08,259 --> 00:16:15,009
encrypted files yes

332
00:16:10,499 --> 00:16:16,779
so I don't think they're I don't think I

333
00:16:15,009 --> 00:16:21,009
don't know whether there is there is a

334
00:16:16,779 --> 00:16:23,679
lot of come in come encrypted files in

335
00:16:21,009 --> 00:16:26,919
the system resources but even if there

336
00:16:23,679 --> 00:16:29,619
are these files can be live and

337
00:16:26,919 --> 00:16:34,059
compressed so that so that this part of

338
00:16:29,619 --> 00:16:38,499
this part of file can be can be can be

339
00:16:34,059 --> 00:16:41,589
read much faster yeah so we test no it

340
00:16:38,499 --> 00:16:43,569
does not compress everything in the in

341
00:16:41,589 --> 00:16:46,509
the real employment it does not compress

342
00:16:43,569 --> 00:16:48,759
everything in the file system you some

343
00:16:46,509 --> 00:16:50,649
part of the file system is if for some

344
00:16:48,759 --> 00:16:52,599
part of thousands or if they if the

345
00:16:50,649 --> 00:16:55,689
compression does not brings better

346
00:16:52,599 --> 00:16:58,659
performance and it cannot save a lot of

347
00:16:55,689 --> 00:17:01,139
storage we just live uncompressed okay

348
00:16:58,659 --> 00:17:01,139
thank you

349
00:17:01,169 --> 00:17:08,139
let's thank our speaker again

350
00:17:04,160 --> 00:17:08,140
[Applause]

