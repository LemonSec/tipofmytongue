1
00:00:10,690 --> 00:00:15,518
hello everyone today I'm going to

2
00:00:12,910 --> 00:00:17,980
present a 10-gram which is a system to

3
00:00:15,519 --> 00:00:20,200
bridge the immutable and mutable data

4
00:00:17,980 --> 00:00:24,490
abstractions for data and ladies

5
00:00:20,200 --> 00:00:27,789
frameworks so distributed data and the

6
00:00:24,490 --> 00:00:31,229
latest from us have rapidly developed in

7
00:00:27,789 --> 00:00:34,120
the last decades from the HPC and to

8
00:00:31,230 --> 00:00:37,000
general-purpose computing like MapReduce

9
00:00:34,120 --> 00:00:40,660
and then to specialized systems like

10
00:00:37,000 --> 00:00:43,450
prego and parameter server we can

11
00:00:40,660 --> 00:00:45,849
roughly classify offline data analytics

12
00:00:43,450 --> 00:00:49,090
frameworks into two categories according

13
00:00:45,850 --> 00:00:50,039
to their data abstractions immutable

14
00:00:49,090 --> 00:00:53,070
immutable

15
00:00:50,039 --> 00:00:57,219
it's about processing systems such as

16
00:00:53,070 --> 00:01:00,039
spark MapReduce and triad link you adopt

17
00:00:57,219 --> 00:01:03,699
the immutable data abstractions and

18
00:01:00,039 --> 00:01:06,369
let's call them immutable systems one

19
00:01:03,699 --> 00:01:09,009
feature of such system is a functional

20
00:01:06,369 --> 00:01:12,640
programming model as shown in the spark

21
00:01:09,009 --> 00:01:15,819
world count and they used data flow

22
00:01:12,640 --> 00:01:20,950
graphs to model the data dependencies

23
00:01:15,819 --> 00:01:23,170
among data sets so with immutable data

24
00:01:20,950 --> 00:01:25,780
extraction it is easier to support

25
00:01:23,170 --> 00:01:29,920
efficient failure recovery and load

26
00:01:25,780 --> 00:01:32,800
balancing for example spark used lineage

27
00:01:29,920 --> 00:01:36,000
based recovery strategy so that only the

28
00:01:32,800 --> 00:01:39,759
last partitions needs to be recomputed

29
00:01:36,000 --> 00:01:41,860
however as immutable systems are

30
00:01:39,759 --> 00:01:44,789
inherently stateless they cannot

31
00:01:41,860 --> 00:01:47,709
naturally support state 4 workloads

32
00:01:44,789 --> 00:01:52,840
besides the only support a synchronous

33
00:01:47,709 --> 00:01:54,940
execution model so in contrast mutual

34
00:01:52,840 --> 00:01:58,000
abstractions are usually adopted by

35
00:01:54,940 --> 00:02:01,649
specialized systems such as the prego

36
00:01:58,000 --> 00:02:05,830
and GraphLab for graph analytics and

37
00:02:01,649 --> 00:02:08,590
parameter server for machine learning so

38
00:02:05,830 --> 00:02:12,209
let's call these systems mutable systems

39
00:02:08,590 --> 00:02:15,040
the target workloads of the systems are

40
00:02:12,209 --> 00:02:17,319
stateful and the mutable systems you

41
00:02:15,040 --> 00:02:19,600
live provides specialized programming

42
00:02:17,319 --> 00:02:20,530
models such as a key value based

43
00:02:19,600 --> 00:02:23,200
push-pull

44
00:02:20,530 --> 00:02:28,120
for distributing machine learning

45
00:02:23,200 --> 00:02:30,670
Berta's interest models for graphs so

46
00:02:28,120 --> 00:02:33,129
mutable systems are efficient for

47
00:02:30,670 --> 00:02:35,649
iterative workloads and may support a

48
00:02:33,129 --> 00:02:36,840
synchronous execution compared to

49
00:02:35,650 --> 00:02:39,819
immutable

50
00:02:36,840 --> 00:02:42,819
abstractions however it is more

51
00:02:39,819 --> 00:02:47,619
expensive to provide some robustness

52
00:02:42,819 --> 00:02:50,170
with consistency guarantees also those

53
00:02:47,620 --> 00:02:54,940
systems rely on the nature's of the

54
00:02:50,170 --> 00:02:57,988
application for load balancing here is a

55
00:02:54,940 --> 00:03:01,120
summary of the pros and cons of both

56
00:02:57,989 --> 00:03:03,849
abstractions existing systems adopt

57
00:03:01,120 --> 00:03:05,890
either the immutable or mutable

58
00:03:03,849 --> 00:03:09,819
abstraction and do not enjoy the

59
00:03:05,890 --> 00:03:12,518
benefits of both words this motivates us

60
00:03:09,819 --> 00:03:14,980
to provide a system that can integrate

61
00:03:12,519 --> 00:03:17,829
both obstructions into a unified

62
00:03:14,980 --> 00:03:19,630
programming model so that we can

63
00:03:17,829 --> 00:03:22,480
transparently determine the data

64
00:03:19,630 --> 00:03:26,769
mutability enjoy the benefits of both

65
00:03:22,480 --> 00:03:29,290
words a proposed a new program module

66
00:03:26,769 --> 00:03:33,510
called map updates in the dataflow

67
00:03:29,290 --> 00:03:36,638
abstraction like MapReduce will apply

68
00:03:33,510 --> 00:03:40,328
operations on Colossians and generate

69
00:03:36,639 --> 00:03:42,670
new collections for example we can apply

70
00:03:40,329 --> 00:03:45,340
a map function to a collection a here

71
00:03:42,670 --> 00:03:48,880
and generates an intermediate results

72
00:03:45,340 --> 00:03:51,489
then we'll apply a reduced function to

73
00:03:48,880 --> 00:03:56,889
the intermediate results and generates a

74
00:03:51,489 --> 00:03:59,380
classroom B but in map updates we'll

75
00:03:56,889 --> 00:04:03,010
make data collections mutable which

76
00:03:59,380 --> 00:04:08,680
change the functional reduce operations

77
00:04:03,010 --> 00:04:11,980
into a stateful updates operation a map

78
00:04:08,680 --> 00:04:16,889
update program may localize this form

79
00:04:11,980 --> 00:04:20,738
with two key operations map and update

80
00:04:16,889 --> 00:04:24,520
so we can also call this a map updates

81
00:04:20,738 --> 00:04:27,969
plan collections a P and C our map

82
00:04:24,520 --> 00:04:31,150
crushin assign impression and the update

83
00:04:27,970 --> 00:04:34,330
question respectively in progression

84
00:04:31,150 --> 00:04:37,109
here allows the fine-grained key value

85
00:04:34,330 --> 00:04:41,318
based as data access

86
00:04:37,110 --> 00:04:45,729
we can allow zero or multiple side

87
00:04:41,319 --> 00:04:48,039
inputs in a plane as well the map

88
00:04:45,729 --> 00:04:51,210
function and update function here are

89
00:04:48,039 --> 00:04:54,789
user-defined functions map functions are

90
00:04:51,210 --> 00:04:58,870
functional and immutable just as in

91
00:04:54,789 --> 00:05:00,878
MapReduce paradigm and for the update

92
00:04:58,870 --> 00:05:05,020
function it is stateful and allows

93
00:05:00,879 --> 00:05:07,240
in-place modification to datasets map

94
00:05:05,020 --> 00:05:09,460
update provides the following features

95
00:05:07,240 --> 00:05:13,900
for expressiveness and for the ability

96
00:05:09,460 --> 00:05:16,989
to infer data with ability first some or

97
00:05:13,900 --> 00:05:19,270
all of the map collection simple

98
00:05:16,990 --> 00:05:21,900
collection an update collection can be

99
00:05:19,270 --> 00:05:25,750
the same this feature makes in-place

100
00:05:21,900 --> 00:05:28,628
modification possible let's take a look

101
00:05:25,750 --> 00:05:31,330
at the first example we'll make the map

102
00:05:28,629 --> 00:05:33,909
collection and the update crushin the

103
00:05:31,330 --> 00:05:37,000
same meaning that the map collection is

104
00:05:33,909 --> 00:05:40,419
being updated in the second example

105
00:05:37,000 --> 00:05:43,840
we'll make the site inputs to be the

106
00:05:40,419 --> 00:05:46,599
same as the update crashing here we use

107
00:05:43,840 --> 00:05:51,638
two applications to show how these two

108
00:05:46,599 --> 00:05:55,270
designs brings expressiveness so making

109
00:05:51,639 --> 00:05:57,639
this map clash in the same as the update

110
00:05:55,270 --> 00:06:00,789
collection is useful for implementing

111
00:05:57,639 --> 00:06:04,409
acronyms like the Burgess centric graph

112
00:06:00,789 --> 00:06:10,440
and the latest ones this is our patron

113
00:06:04,409 --> 00:06:14,529
example in tangram the rank collection

114
00:06:10,440 --> 00:06:18,159
starts the verge has patron values and

115
00:06:14,529 --> 00:06:21,129
is posts the map collection and the

116
00:06:18,159 --> 00:06:25,300
update collection in the map update plan

117
00:06:21,129 --> 00:06:28,599
and the link stores the immutable edge

118
00:06:25,300 --> 00:06:31,210
information for each vertex the map

119
00:06:28,599 --> 00:06:34,289
function distributes the PageRank value

120
00:06:31,210 --> 00:06:40,859
to its neighbors and the update function

121
00:06:34,289 --> 00:06:45,099
and accumulates the partial updates so

122
00:06:40,860 --> 00:06:47,860
in the other example we make the side

123
00:06:45,099 --> 00:06:48,990
inputs clashing to be the same as the

124
00:06:47,860 --> 00:06:51,740
updates clash

125
00:06:48,990 --> 00:06:54,180
this is useful for example in

126
00:06:51,740 --> 00:06:58,650
implementing iterative machinery and

127
00:06:54,180 --> 00:07:01,520
rooms here the parents collection in the

128
00:06:58,650 --> 00:07:05,460
program it's post the site inputs and

129
00:07:01,520 --> 00:07:08,159
update collection it corresponds to the

130
00:07:05,460 --> 00:07:10,770
parameter server model in the map

131
00:07:08,160 --> 00:07:15,360
function where access model in the

132
00:07:10,770 --> 00:07:17,430
parameter and calculates gradients and

133
00:07:15,360 --> 00:07:21,450
output the gradients as intermediate

134
00:07:17,430 --> 00:07:23,700
results then we in the update function

135
00:07:21,450 --> 00:07:27,390
will apply the gradients to the

136
00:07:23,700 --> 00:07:30,719
parameters you may have noticed some

137
00:07:27,390 --> 00:07:36,750
additional specifications in the example

138
00:07:30,720 --> 00:07:38,880
of the last two lines so this is a

139
00:07:36,750 --> 00:07:41,310
second feature of map updates which

140
00:07:38,880 --> 00:07:45,420
supports iterations and a synchronous

141
00:07:41,310 --> 00:07:47,970
execution for each map update plan users

142
00:07:45,420 --> 00:07:53,450
can specify how many iterations you want

143
00:07:47,970 --> 00:07:56,840
to run by set the iteration function and

144
00:07:53,450 --> 00:08:00,000
users can also set this down as for

145
00:07:56,840 --> 00:08:03,090
consistency guarantees still then this

146
00:08:00,000 --> 00:08:05,760
value here bonds the differences in

147
00:08:03,090 --> 00:08:10,049
iteration progress among different data

148
00:08:05,760 --> 00:08:13,950
partitions the third feature is that map

149
00:08:10,050 --> 00:08:16,140
updates adopts a simple mechanism to

150
00:08:13,950 --> 00:08:19,950
determine whether a data collection is

151
00:08:16,140 --> 00:08:21,510
mutable or immutable the update

152
00:08:19,950 --> 00:08:24,420
collection is mutable and other

153
00:08:21,510 --> 00:08:28,260
questions if different from the updates

154
00:08:24,420 --> 00:08:31,620
crashing are considering immutable for

155
00:08:28,260 --> 00:08:34,830
example if all of the collections in the

156
00:08:31,620 --> 00:08:37,560
map update plan are different only the

157
00:08:34,830 --> 00:08:42,330
updates crashing is mutable and others

158
00:08:37,559 --> 00:08:44,760
are immutable if some of the equation is

159
00:08:42,330 --> 00:08:48,000
the same as an update question it is

160
00:08:44,760 --> 00:08:51,810
considered mutable like the clash in a

161
00:08:48,000 --> 00:08:55,590
in the first plan and the crashing B in

162
00:08:51,810 --> 00:08:56,489
the second plan the ability to determine

163
00:08:55,590 --> 00:08:59,340
a data

164
00:08:56,490 --> 00:09:02,339
mutability and to express different data

165
00:08:59,340 --> 00:09:04,740
access patterns allow map updates to

166
00:09:02,339 --> 00:09:07,319
be highly efficient for pipelined

167
00:09:04,740 --> 00:09:10,470
workloads which is also our target

168
00:09:07,319 --> 00:09:13,889
scenario the typical type lines consist

169
00:09:10,470 --> 00:09:16,800
of MapReduce tau data processing

170
00:09:13,889 --> 00:09:21,180
followed by various data analytics and

171
00:09:16,800 --> 00:09:23,849
testing using map update allows our

172
00:09:21,180 --> 00:09:26,878
pipeline workload to achieve comparable

173
00:09:23,850 --> 00:09:31,589
risk of performance within each

174
00:09:26,879 --> 00:09:34,920
individual tasks to the specialized

175
00:09:31,589 --> 00:09:38,309
systems respectively while we can avoid

176
00:09:34,920 --> 00:09:42,959
counters which call costs by a unified

177
00:09:38,309 --> 00:09:45,600
framework so we implemented map update

178
00:09:42,959 --> 00:09:48,779
in 10-gram with three major system

179
00:09:45,600 --> 00:09:51,839
designs first we'll have a controller on

180
00:09:48,779 --> 00:09:55,199
each worker node to manage local tasks

181
00:09:51,839 --> 00:09:57,899
and they use partition based progress

182
00:09:55,199 --> 00:10:01,290
control mechanism well also you

183
00:09:57,899 --> 00:10:04,889
implemented contest aware failure

184
00:10:01,290 --> 00:10:08,509
recovery strategy 10-gram uses a

185
00:10:04,889 --> 00:10:11,160
petition based progress control

186
00:10:08,509 --> 00:10:13,050
mechanism just upholds a bach

187
00:10:11,160 --> 00:10:16,079
synchronous model the bounded delay

188
00:10:13,050 --> 00:10:19,559
model and a synchronous execution model

189
00:10:16,079 --> 00:10:23,040
it associates progress with partitions

190
00:10:19,559 --> 00:10:26,660
instead of workers for each updates

191
00:10:23,040 --> 00:10:30,179
partition will use a bitmap to record

192
00:10:26,660 --> 00:10:34,319
map partitions for from which the

193
00:10:30,179 --> 00:10:37,769
updates has been already committed with

194
00:10:34,319 --> 00:10:40,559
this progress control mechanism tangram

195
00:10:37,769 --> 00:10:44,160
allows different consistency models for

196
00:10:40,559 --> 00:10:47,870
iterative workloads and it provides for

197
00:10:44,160 --> 00:10:51,509
tolerance for mutable data collections a

198
00:10:47,870 --> 00:10:55,139
failure recovery tangram distinguishes

199
00:10:51,509 --> 00:10:58,470
between two scenarios based on the data

200
00:10:55,139 --> 00:11:00,779
mutability local failure and global

201
00:10:58,470 --> 00:11:03,209
failure local failure is a case where

202
00:11:00,779 --> 00:11:05,220
the failed machines do not hold the

203
00:11:03,209 --> 00:11:07,920
updates partition both only the

204
00:11:05,220 --> 00:11:10,519
immutable partitions

205
00:11:07,920 --> 00:11:13,860
in this case recovery is simple we can

206
00:11:10,519 --> 00:11:15,139
just reload the last partitions and

207
00:11:13,860 --> 00:11:19,459
healthy machines

208
00:11:15,139 --> 00:11:22,339
and continue the execution for global

209
00:11:19,459 --> 00:11:25,790
failure will have failed machines

210
00:11:22,339 --> 00:11:28,220
containing mutual data collections in

211
00:11:25,790 --> 00:11:30,738
this case 10 gram rows back to the

212
00:11:28,220 --> 00:11:33,799
latest Japan for the mutable partitions

213
00:11:30,739 --> 00:11:39,499
and reload the last immutable partitions

214
00:11:33,799 --> 00:11:42,619
in parallel we conducted experiments

215
00:11:39,499 --> 00:11:46,730
with 20 machines into network settings

216
00:11:42,619 --> 00:11:49,609
we design the experiment to show three

217
00:11:46,730 --> 00:11:52,040
things first how is the photons

218
00:11:49,609 --> 00:11:56,569
performance of qiangren in different

219
00:11:52,040 --> 00:11:58,569
colored scenarios second we want to see

220
00:11:56,569 --> 00:12:00,889
whether tangram can attend both

221
00:11:58,569 --> 00:12:03,079
expressiveness and comparable

222
00:12:00,889 --> 00:12:06,669
performance with specialized systems

223
00:12:03,079 --> 00:12:09,469
over a wide range of workloads and also

224
00:12:06,669 --> 00:12:13,929
we want to see the efficiency of

225
00:12:09,470 --> 00:12:16,939
tangrams in handling pipelined workloads

226
00:12:13,929 --> 00:12:20,029
we tested the failure recovery if

227
00:12:16,939 --> 00:12:24,019
performance in case of post local

228
00:12:20,029 --> 00:12:26,689
failure and global failure for a local

229
00:12:24,019 --> 00:12:29,179
failure were tested on the caimans job

230
00:12:26,689 --> 00:12:32,269
were killed a workers during part of the

231
00:12:29,179 --> 00:12:35,269
training data which is immutable so

232
00:12:32,269 --> 00:12:37,999
tangram we loaded only the last training

233
00:12:35,269 --> 00:12:42,189
data and resumed the iterations quickly

234
00:12:37,999 --> 00:12:45,369
and this is the same behavior to spark

235
00:12:42,189 --> 00:12:49,069
note they had immutable systems such as

236
00:12:45,369 --> 00:12:51,230
nayad and Batum the local failure caused

237
00:12:49,069 --> 00:12:55,719
a whole system row back to the latest

238
00:12:51,230 --> 00:12:58,669
SharePoint which wastes the computation

239
00:12:55,720 --> 00:12:59,119
for global failure would test it on page

240
00:12:58,669 --> 00:13:01,910
rank

241
00:12:59,119 --> 00:13:05,119
we killed a worker storing rank values

242
00:13:01,910 --> 00:13:06,769
which is mutable and tangram wrote back

243
00:13:05,119 --> 00:13:12,980
to the latest checkpoint for

244
00:13:06,769 --> 00:13:16,639
reconstructing the rank connection note

245
00:13:12,980 --> 00:13:19,910
that imposed failure scenarios the

246
00:13:16,639 --> 00:13:22,519
recovery strategy of either during the

247
00:13:19,910 --> 00:13:26,539
real computation and last partitions or

248
00:13:22,519 --> 00:13:27,760
trying to robot you ladies checkpoint

249
00:13:26,539 --> 00:13:30,040
it's at man

250
00:13:27,760 --> 00:13:33,910
Chiclets adapted based on the data

251
00:13:30,040 --> 00:13:38,740
mutability so the list doesn't require

252
00:13:33,910 --> 00:13:40,540
further specification from users we also

253
00:13:38,740 --> 00:13:43,200
show that qiangren has good

254
00:13:40,540 --> 00:13:46,060
expressiveness over different workloads

255
00:13:43,200 --> 00:13:48,520
including bulk processing iterative

256
00:13:46,060 --> 00:13:50,319
machine learning and growth analytics we

257
00:13:48,520 --> 00:13:53,530
compare with specialized systems

258
00:13:50,320 --> 00:13:56,230
respectively for each type of workloads

259
00:13:53,530 --> 00:13:58,780
our results show there qiangren attends

260
00:13:56,230 --> 00:14:02,800
a comparable execution time and

261
00:13:58,780 --> 00:14:05,140
scalability we show the pipeline

262
00:14:02,800 --> 00:14:08,199
workload scenario where tangram is

263
00:14:05,140 --> 00:14:11,830
mostly designed for and is most used for

264
00:14:08,200 --> 00:14:13,720
in a compared with using one single

265
00:14:11,830 --> 00:14:16,740
general system for the pipeline

266
00:14:13,720 --> 00:14:21,070
workloads as well as using pipelines of

267
00:14:16,740 --> 00:14:23,740
specialized systems we see human

268
00:14:21,070 --> 00:14:26,860
benefits of tangram in such scenario

269
00:14:23,740 --> 00:14:29,830
first qiangren can achieve comparable

270
00:14:26,860 --> 00:14:31,840
performance on individual tasks where

271
00:14:29,830 --> 00:14:35,830
there waste their respective

272
00:14:31,840 --> 00:14:39,660
specialized systems like the tank run

273
00:14:35,830 --> 00:14:43,180
compared with spark plus per tune and

274
00:14:39,660 --> 00:14:48,040
10-gram can avoid the huge contests

275
00:14:43,180 --> 00:14:50,020
which overhead also Tangra us provides a

276
00:14:48,040 --> 00:14:53,709
unified programming abstraction for

277
00:14:50,020 --> 00:14:58,569
different type of jobs to be run in a

278
00:14:53,710 --> 00:15:01,240
single pipeline in conclusion way

279
00:14:58,570 --> 00:15:04,420
proposal novel programming model cut map

280
00:15:01,240 --> 00:15:06,790
updates the ability of inferring data

281
00:15:04,420 --> 00:15:09,849
visibility allows a system to provide

282
00:15:06,790 --> 00:15:13,000
both support for iterative and a

283
00:15:09,850 --> 00:15:16,720
synchronous execution and efficiency in

284
00:15:13,000 --> 00:15:19,570
load balancing for torrents it allows

285
00:15:16,720 --> 00:15:22,660
users to express difference workloads

286
00:15:19,570 --> 00:15:23,560
and achieve comparable performance with

287
00:15:22,660 --> 00:15:27,329
specialized

288
00:15:23,560 --> 00:15:29,280
systems within one single abstraction

289
00:15:27,330 --> 00:15:35,620
thank you

290
00:15:29,280 --> 00:15:43,100
[Applause]

291
00:15:35,620 --> 00:15:44,600
all right do we have some questions we

292
00:15:43,100 --> 00:15:45,430
serve you here in track - sir you don't

293
00:15:44,600 --> 00:15:47,740
have to get up

294
00:15:45,430 --> 00:15:51,469
we can do this for you in track one

295
00:15:47,740 --> 00:15:54,189
thank you so I have a question on your

296
00:15:51,470 --> 00:15:57,139
experiment spawn so in your experiment

297
00:15:54,189 --> 00:16:01,420
the local failure you use one workload

298
00:15:57,139 --> 00:16:03,459
called K means for the global failures

299
00:16:01,420 --> 00:16:06,969
evaluation you use our different

300
00:16:03,459 --> 00:16:09,859
workload or algorithm just like why you

301
00:16:06,970 --> 00:16:12,319
just like evaluate them with two

302
00:16:09,860 --> 00:16:15,290
different algorithm rather with the same

303
00:16:12,319 --> 00:16:18,620
K means or PageRank okay

304
00:16:15,290 --> 00:16:21,769
we usually setting because it's natural

305
00:16:18,620 --> 00:16:24,949
for the failure a scenario let me

306
00:16:21,769 --> 00:16:29,290
illustrate this for K means we really

307
00:16:24,949 --> 00:16:32,109
have small case with some latent factors

308
00:16:29,290 --> 00:16:35,050
representing each centroid so the

309
00:16:32,110 --> 00:16:40,009
parameters our the model size is

310
00:16:35,050 --> 00:16:43,430
relatively small so that the there are

311
00:16:40,009 --> 00:16:47,720
more machines containing immutable

312
00:16:43,430 --> 00:16:52,279
training data compared to the machines

313
00:16:47,720 --> 00:16:56,059
storing the mutable parameters that's

314
00:16:52,279 --> 00:16:59,170
why we can see local failures happening

315
00:16:56,059 --> 00:17:03,010
on only the training data machines and

316
00:16:59,170 --> 00:17:08,020
for page rank we see that the immutable

317
00:17:03,010 --> 00:17:11,689
link values like the neighbors is

318
00:17:08,020 --> 00:17:14,629
actually co-located with rank values

319
00:17:11,689 --> 00:17:16,880
which is mutable so in this case we can

320
00:17:14,630 --> 00:17:20,780
always observe the global failure

321
00:17:16,880 --> 00:17:25,270
instead of local failure does it make

322
00:17:20,780 --> 00:17:25,270
sense okay thank you

323
00:17:26,750 --> 00:17:32,820
hi for the feature - you mentioned that

324
00:17:30,150 --> 00:17:36,030
you can set the Estonians to guarantee

325
00:17:32,820 --> 00:17:37,770
the consistence can you elaborate a

326
00:17:36,030 --> 00:17:42,658
little bit on that okay

327
00:17:37,770 --> 00:17:46,980
so I think this this notation of sirness

328
00:17:42,659 --> 00:17:48,809
must come from work by patent which is

329
00:17:46,980 --> 00:17:51,390
designed for distributing machine

330
00:17:48,809 --> 00:17:55,770
learning so the sternness means the

331
00:17:51,390 --> 00:17:59,730
bonded in slave among the partitions for

332
00:17:55,770 --> 00:18:04,049
example one partition of the training

333
00:17:59,730 --> 00:18:06,480
data can be scan through and compute a

334
00:18:04,049 --> 00:18:09,690
gradients according to the parameters

335
00:18:06,480 --> 00:18:13,380
and this this partition may have gone

336
00:18:09,690 --> 00:18:17,539
through several scans like let's say

337
00:18:13,380 --> 00:18:20,700
five iterations but within another

338
00:18:17,539 --> 00:18:23,760
partition of the same training data we

339
00:18:20,700 --> 00:18:27,840
can maybe have another progress because

340
00:18:23,760 --> 00:18:30,750
it's calculated on another worker and we

341
00:18:27,840 --> 00:18:33,120
want to provide guarantee that the

342
00:18:30,750 --> 00:18:36,840
differences in the iteration progress of

343
00:18:33,120 --> 00:18:40,289
the workers do not exist specified

344
00:18:36,840 --> 00:18:43,158
values so as to ensure the convergence

345
00:18:40,289 --> 00:18:46,320
guarantee as well as promoting the

346
00:18:43,159 --> 00:18:50,340
hardware's efficiency in doing parallel

347
00:18:46,320 --> 00:18:52,139
computation so if you want to know more

348
00:18:50,340 --> 00:18:56,000
about the soonest thing you can virtue

349
00:18:52,140 --> 00:18:59,520
the step synchronous parameter server

350
00:18:56,000 --> 00:19:03,260
paper yes thank you we have time for one

351
00:18:59,520 --> 00:19:06,629
more question so I have a question about

352
00:19:03,260 --> 00:19:10,049
contacts a switch can you elaborate more

353
00:19:06,630 --> 00:19:14,159
about what does that mean in map update

354
00:19:10,049 --> 00:19:18,929
compared to other okay so let's go to

355
00:19:14,159 --> 00:19:21,770
the slide sorry yes

356
00:19:18,929 --> 00:19:21,770
so here

357
00:19:24,580 --> 00:19:33,470
yes yes so this figure illustrates that

358
00:19:29,720 --> 00:19:36,710
our reason of why we have contests which

359
00:19:33,470 --> 00:19:41,330
in using different specialized systems

360
00:19:36,710 --> 00:19:44,000
so basically the normal way of and the

361
00:19:41,330 --> 00:19:46,460
most simpler way of doing the pipeline

362
00:19:44,000 --> 00:19:49,340
of different systems is that we let the

363
00:19:46,460 --> 00:19:52,309
previous system to dump the data output

364
00:19:49,340 --> 00:19:56,139
data to the distributed file storage

365
00:19:52,309 --> 00:19:59,899
like HDFS and then let a subsequent

366
00:19:56,139 --> 00:20:06,408
system chulos it data and this incurs

367
00:19:59,899 --> 00:20:09,049
heavy data this IO so this is a main

368
00:20:06,409 --> 00:20:12,200
resource of Congress which and we may

369
00:20:09,049 --> 00:20:16,070
argue that we can use some different

370
00:20:12,200 --> 00:20:18,860
memory based storage systems or framers

371
00:20:16,070 --> 00:20:21,200
like Apache Aero to mitigate this but

372
00:20:18,860 --> 00:20:25,519
this requires a lot of engineering work

373
00:20:21,200 --> 00:20:30,110
because we need to confirm and we choose

374
00:20:25,519 --> 00:20:32,630
different formats of the file input and

375
00:20:30,110 --> 00:20:35,570
outputs and as well as the serialize

376
00:20:32,630 --> 00:20:37,669
ability from different languages used to

377
00:20:35,570 --> 00:20:40,120
implement the systems alright let's

378
00:20:37,669 --> 00:20:45,470
thank our speaker

379
00:20:40,120 --> 00:20:45,469
[Applause]

