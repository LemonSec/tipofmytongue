1
00:00:09,929 --> 00:00:15,599
hi I'm Vikram I'm going to be presenting

2
00:00:12,900 --> 00:00:18,240
my work on our work on Alex these

3
00:00:15,599 --> 00:00:21,230
lightweight execution domains a step

4
00:00:18,240 --> 00:00:24,778
towards isolation of current subsystems

5
00:00:21,230 --> 00:00:27,630
kernels are pervasive the power millions

6
00:00:24,779 --> 00:00:30,810
of devices ranging from data centers to

7
00:00:27,630 --> 00:00:32,488
smartphones to autonomous cars and they

8
00:00:30,810 --> 00:00:36,750
are the most complicated pieces of

9
00:00:32,488 --> 00:00:40,199
software the de-facto commodity kernel

10
00:00:36,750 --> 00:00:42,629
Linux has a huge codebase with over 17

11
00:00:40,199 --> 00:00:44,970
million lines of code and is developed

12
00:00:42,629 --> 00:00:47,489
at a rapid pace with over 70,000 commits

13
00:00:44,970 --> 00:00:49,260
a year and is often written in unsafe

14
00:00:47,489 --> 00:00:51,449
programming languages like C and

15
00:00:49,260 --> 00:00:55,739
assembly if you look at the Linux kernel

16
00:00:51,449 --> 00:00:59,670
it has numerous subsystems like 40-plus

17
00:00:55,739 --> 00:01:01,709
subsystems and 3,000 plus drivers due to

18
00:00:59,670 --> 00:01:03,960
these factors a steady stream of errors

19
00:01:01,710 --> 00:01:06,720
are introduced into the kernel which is

20
00:01:03,960 --> 00:01:09,600
corroborated by the Linux kernel CVS

21
00:01:06,720 --> 00:01:11,820
reported over the years and to be

22
00:01:09,600 --> 00:01:13,408
honnest these numbers are not real

23
00:01:11,820 --> 00:01:15,419
because like there are numerous bugs

24
00:01:13,409 --> 00:01:17,040
that go bug fixes that go into the

25
00:01:15,420 --> 00:01:22,320
kernel without having C be numbers

26
00:01:17,040 --> 00:01:25,560
assigned the situation is unlikely to

27
00:01:22,320 --> 00:01:28,949
change and and the numbers are not going

28
00:01:25,560 --> 00:01:31,320
to get down despite having a lot of

29
00:01:28,950 --> 00:01:33,920
tools in the in our defense arsenal like

30
00:01:31,320 --> 00:01:36,240
the ones like the workload listed here

31
00:01:33,920 --> 00:01:38,460
kernels remain vulnerable to attacks

32
00:01:36,240 --> 00:01:40,830
even advanced defenses such as code

33
00:01:38,460 --> 00:01:43,470
pointer integrity could be attacked by

34
00:01:40,830 --> 00:01:45,658
data only attacks could pointer

35
00:01:43,470 --> 00:01:48,570
integrity seems to solve all the problem

36
00:01:45,659 --> 00:01:50,880
like most classes of attacks like core

37
00:01:48,570 --> 00:01:53,789
injection and control flow hijacking

38
00:01:50,880 --> 00:01:56,210
attacks but with data attacks and

39
00:01:53,790 --> 00:01:59,670
automated tools for attack generation

40
00:01:56,210 --> 00:02:03,059
these are vulnerable so the attack

41
00:01:59,670 --> 00:02:05,040
scenario is real and we need to do

42
00:02:03,060 --> 00:02:06,680
something to protect the security of the

43
00:02:05,040 --> 00:02:10,259
operating system

44
00:02:06,680 --> 00:02:12,599
why are kernels vulnerable with the very

45
00:02:10,258 --> 00:02:14,879
few exceptions historically kernels are

46
00:02:12,599 --> 00:02:17,310
monolithic where the kernel and its

47
00:02:14,879 --> 00:02:19,560
subsystems such as networks file system

48
00:02:17,310 --> 00:02:22,829
and device driver all run in the same

49
00:02:19,560 --> 00:02:23,200
address space where vulnerability in a

50
00:02:22,829 --> 00:02:24,849
single

51
00:02:23,200 --> 00:02:29,018
component could potentially compromise

52
00:02:24,849 --> 00:02:32,049
the entire curdled we need an isolation

53
00:02:29,019 --> 00:02:33,550
solution assume we can split apart the

54
00:02:32,050 --> 00:02:36,430
monolithic kernel into isolated

55
00:02:33,550 --> 00:02:39,190
components as shown here here we have an

56
00:02:36,430 --> 00:02:42,069
example of isolating block driver and a

57
00:02:39,190 --> 00:02:45,220
network driver in this scenario say a

58
00:02:42,069 --> 00:02:47,140
bug appears in the network driver and we

59
00:02:45,220 --> 00:02:48,790
can confine this within the isolated

60
00:02:47,140 --> 00:02:51,208
component and not propagate to the

61
00:02:48,790 --> 00:02:54,190
entire kernel

62
00:02:51,209 --> 00:02:56,650
however isolation comes in brings in a

63
00:02:54,190 --> 00:02:59,109
lot of challenges first one can argue

64
00:02:56,650 --> 00:03:00,849
that I don't need to have an isolation

65
00:02:59,110 --> 00:03:02,920
solution for a monolithic kernel I can

66
00:03:00,849 --> 00:03:06,369
very well run a microkernel sure you can

67
00:03:02,920 --> 00:03:07,869
run it but there's a huge amount of

68
00:03:06,370 --> 00:03:10,180
development effort that has gone into

69
00:03:07,870 --> 00:03:11,799
the monolithic kernel over years we have

70
00:03:10,180 --> 00:03:14,440
3000 plus drivers we have numerous

71
00:03:11,799 --> 00:03:17,170
matured subsystems we should be able to

72
00:03:14,440 --> 00:03:20,650
reuse that code so in order to run

73
00:03:17,170 --> 00:03:23,078
isolated domains with unmodified code

74
00:03:20,650 --> 00:03:27,489
that is to reuse and support legacy code

75
00:03:23,079 --> 00:03:29,079
we need to generate glue code and the

76
00:03:27,489 --> 00:03:30,940
second challenge is that we need an

77
00:03:29,079 --> 00:03:35,139
environment to run our code under

78
00:03:30,940 --> 00:03:36,730
isolation one can argue again that I get

79
00:03:35,139 --> 00:03:38,410
I can throw in a virtual machine and

80
00:03:36,730 --> 00:03:39,940
then everything is fine but the

81
00:03:38,410 --> 00:03:42,329
overheads of virtual machine quickly

82
00:03:39,940 --> 00:03:44,650
becomes quickly makes the solution

83
00:03:42,329 --> 00:03:46,959
impractical so we need a minimal

84
00:03:44,650 --> 00:03:50,829
environment to run code under isolated

85
00:03:46,959 --> 00:03:53,829
domains and the next challenge is to

86
00:03:50,829 --> 00:03:55,690
cross the boundary because we go B run

87
00:03:53,829 --> 00:03:57,280
the code in isolation it needs to talk

88
00:03:55,690 --> 00:03:59,829
to the rest of the kernel and should be

89
00:03:57,280 --> 00:04:01,870
able to call functions available in the

90
00:03:59,829 --> 00:04:05,650
kernel so we need low m'q overhead

91
00:04:01,870 --> 00:04:08,859
mechanism to cross the boundary now I

92
00:04:05,650 --> 00:04:10,569
present lightweight execution domains a

93
00:04:08,859 --> 00:04:14,079
framework geared towards addressing

94
00:04:10,569 --> 00:04:17,440
these challenges let us look at the

95
00:04:14,079 --> 00:04:19,180
challenges one by one first we are now

96
00:04:17,440 --> 00:04:21,548
convinced that we need to reuse the

97
00:04:19,180 --> 00:04:22,900
legacy code and to reuse the legacy code

98
00:04:21,548 --> 00:04:25,510
we need some form of glue code

99
00:04:22,900 --> 00:04:26,710
generation how did you rate glue code of

100
00:04:25,510 --> 00:04:28,419
course you can go ahead and manually

101
00:04:26,710 --> 00:04:30,760
write glue code for a couple of drivers

102
00:04:28,419 --> 00:04:32,799
but if you want to scale this process to

103
00:04:30,760 --> 00:04:34,409
hundreds of drivers this quickly becomes

104
00:04:32,800 --> 00:04:36,370
infeasible as you would be writing

105
00:04:34,409 --> 00:04:36,889
manual code for like thousands of

106
00:04:36,370 --> 00:04:39,500
functions

107
00:04:36,889 --> 00:04:42,889
so we need an automated way of

108
00:04:39,500 --> 00:04:46,060
generating glue code to solve that we

109
00:04:42,889 --> 00:04:48,680
created an interface definition language

110
00:04:46,060 --> 00:04:50,689
the main goals are to decompose kernel

111
00:04:48,680 --> 00:04:52,460
code and to analyze the interaction

112
00:04:50,689 --> 00:04:55,430
patterns between the code that we wish

113
00:04:52,460 --> 00:04:57,948
to isolate and the rest of the kernel we

114
00:04:55,430 --> 00:05:00,169
devise mechanisms that abstract the

115
00:04:57,949 --> 00:05:02,780
common interaction part where patterns

116
00:05:00,169 --> 00:05:03,318
of the isolated code and the dry and the

117
00:05:02,780 --> 00:05:05,330
kernel

118
00:05:03,319 --> 00:05:09,529
for example function pointers and shared

119
00:05:05,330 --> 00:05:11,210
data structures okay so we have to

120
00:05:09,529 --> 00:05:13,699
explain the rationale behind creating a

121
00:05:11,210 --> 00:05:15,049
new IDL like people would have already

122
00:05:13,699 --> 00:05:18,289
heard about IDL and there are like

123
00:05:15,050 --> 00:05:21,319
hundreds of ideas from protobuf to fi dl

124
00:05:18,289 --> 00:05:23,688
and flick etc but why do we need to have

125
00:05:21,319 --> 00:05:26,360
a new idea because all of those ideals

126
00:05:23,689 --> 00:05:28,639
are geared towards general programming

127
00:05:26,360 --> 00:05:31,509
model but here we are looking at a

128
00:05:28,639 --> 00:05:34,699
kernel programming model say a kernel

129
00:05:31,509 --> 00:05:37,340
often provides an interface like a

130
00:05:34,699 --> 00:05:39,229
dynamic registration interface where you

131
00:05:37,340 --> 00:05:41,539
could throw in a bunch of function

132
00:05:39,229 --> 00:05:44,900
pointers and register a subsystem or a

133
00:05:41,539 --> 00:05:46,699
driver for example here is how you

134
00:05:44,900 --> 00:05:48,650
register a network driver with the

135
00:05:46,699 --> 00:05:50,300
kernel you provide a set of function

136
00:05:48,650 --> 00:05:52,060
pointers I should thank the previous

137
00:05:50,300 --> 00:05:56,180
speaker for introducing you to nd will

138
00:05:52,060 --> 00:05:59,000
function pointer ops yeah here we have n

139
00:05:56,180 --> 00:06:02,360
do open stop and X MIT which are

140
00:05:59,000 --> 00:06:05,599
provided by the driver to register

141
00:06:02,360 --> 00:06:09,620
itself with the kernel so we have an OP

142
00:06:05,599 --> 00:06:11,839
structure and we have we we assign that

143
00:06:09,620 --> 00:06:14,060
to a struct net device pointer and pass

144
00:06:11,839 --> 00:06:16,400
that to another domain so these kind of

145
00:06:14,060 --> 00:06:18,860
complex interaction patterns cannot be

146
00:06:16,400 --> 00:06:22,909
uncovered by the existing ideals so we

147
00:06:18,860 --> 00:06:24,469
design an ideal to create interaction to

148
00:06:22,909 --> 00:06:28,219
understand interaction patterns in the

149
00:06:24,469 --> 00:06:29,810
kernel code for example the registered

150
00:06:28,219 --> 00:06:31,430
net dev should cross the boundary if we

151
00:06:29,810 --> 00:06:33,830
wish to isolate a network driver and

152
00:06:31,430 --> 00:06:37,509
this would be automatically translated

153
00:06:33,830 --> 00:06:37,508
to a remote procedure call in our ideal

154
00:06:37,990 --> 00:06:44,990
let's look at how the call is performed

155
00:06:40,639 --> 00:06:48,560
say we call registered net device here

156
00:06:44,990 --> 00:06:50,209
and then and this points to a data

157
00:06:48,560 --> 00:06:50,849
structure that is defined inside this

158
00:06:50,209 --> 00:06:54,960
domain

159
00:06:50,849 --> 00:06:56,998
and we make a function call we need to

160
00:06:54,960 --> 00:06:58,469
somehow let the other domain know about

161
00:06:56,999 --> 00:07:00,749
the data structure that we are referring

162
00:06:58,469 --> 00:07:02,909
to one way is to share this data

163
00:07:00,749 --> 00:07:04,889
structure between these two domains but

164
00:07:02,909 --> 00:07:07,620
this quickly breaks the isolation

165
00:07:04,889 --> 00:07:09,300
guarantees as the as a malicious driver

166
00:07:07,620 --> 00:07:11,939
running inside an isolated domain can

167
00:07:09,300 --> 00:07:14,339
override the pointers so we need a

168
00:07:11,939 --> 00:07:16,770
mechanism to have shadow copies of data

169
00:07:14,339 --> 00:07:20,490
structures where each domain has its own

170
00:07:16,770 --> 00:07:23,508
private copy an ideal seamlessly

171
00:07:20,490 --> 00:07:26,189
supports hierarchy of private objects

172
00:07:23,509 --> 00:07:29,279
this brings into the natural question of

173
00:07:26,189 --> 00:07:31,969
if you have two or more copies of data

174
00:07:29,279 --> 00:07:34,379
structures how do you keep them in sync

175
00:07:31,969 --> 00:07:36,089
the use of mechanism called projections

176
00:07:34,379 --> 00:07:38,069
to keep them synchronized here is an

177
00:07:36,089 --> 00:07:41,639
example again for the projection of

178
00:07:38,069 --> 00:07:44,490
struck net device and projection

179
00:07:41,639 --> 00:07:46,050
contains a subset of members of the

180
00:07:44,490 --> 00:07:47,969
structure that needs to be marshaled

181
00:07:46,050 --> 00:07:50,969
across the domain to make a function

182
00:07:47,969 --> 00:07:52,830
call and this is the projection that we

183
00:07:50,969 --> 00:07:55,800
define for calling registered net dev

184
00:07:52,830 --> 00:07:57,508
and we support directional attributes

185
00:07:55,800 --> 00:08:00,990
which specify the direction of the data

186
00:07:57,509 --> 00:08:02,819
flow and allocation de-allocation of

187
00:08:00,990 --> 00:08:07,229
Sharyl copies of objects are supported

188
00:08:02,819 --> 00:08:10,469
using a lock DL or keyboard this is how

189
00:08:07,229 --> 00:08:13,050
an entire example looks like it is

190
00:08:10,469 --> 00:08:14,959
represented as a module which specifies

191
00:08:13,050 --> 00:08:18,300
a kernel subsystem that we wish to

192
00:08:14,959 --> 00:08:21,120
isolate and it contains a bunch of

193
00:08:18,300 --> 00:08:23,490
projections and our PCs by feeding this

194
00:08:21,120 --> 00:08:25,529
ideal interface to an ideal compiler

195
00:08:23,490 --> 00:08:28,379
that we generated we can automatically

196
00:08:25,529 --> 00:08:30,749
generate glue code which would contain

197
00:08:28,379 --> 00:08:37,079
color Kali stubs for all the remote

198
00:08:30,749 --> 00:08:39,659
function calls that we wish to invoke so

199
00:08:37,078 --> 00:08:41,549
we can generate glue code now and we can

200
00:08:39,659 --> 00:08:44,670
call into other domains but how to cross

201
00:08:41,549 --> 00:08:46,050
the boundary we need to interact with

202
00:08:44,670 --> 00:08:49,199
the rest of the kernel and we need to

203
00:08:46,050 --> 00:08:51,689
have a communication mechanism that

204
00:08:49,199 --> 00:08:54,719
brings us to the choice of synchronous

205
00:08:51,690 --> 00:08:57,089
versus asynchronous IPC in a synchronous

206
00:08:54,720 --> 00:09:01,380
setting you can use page table isolation

207
00:08:57,089 --> 00:09:03,480
mechanisms to run the isolated code in a

208
00:09:01,380 --> 00:09:04,499
separate page table but a traditional

209
00:09:03,480 --> 00:09:06,479
address space I said

210
00:09:04,499 --> 00:09:08,399
would cost us nine eight hundred and

211
00:09:06,479 --> 00:09:10,079
fifty cycles of overhead and this

212
00:09:08,399 --> 00:09:12,029
overhead is even without the meltdown

213
00:09:10,079 --> 00:09:15,589
mitigations we assumed that it would be

214
00:09:12,029 --> 00:09:17,939
it would be fixed in the hardware

215
00:09:15,589 --> 00:09:20,819
instead of following a synchronous IPC

216
00:09:17,939 --> 00:09:23,459
model we designed an asynchronous IPC

217
00:09:20,819 --> 00:09:25,169
where we run isolated driver and parts

218
00:09:23,459 --> 00:09:29,939
of the non isolated driver in two

219
00:09:25,169 --> 00:09:33,269
different cores in two different cores

220
00:09:29,939 --> 00:09:34,978
here and then use cross core IP C to

221
00:09:33,269 --> 00:09:38,339
communicate between these cores this

222
00:09:34,979 --> 00:09:43,019
brings down the cost you 380 cycles for

223
00:09:38,339 --> 00:09:44,579
a card reply invocation this choice of a

224
00:09:43,019 --> 00:09:46,769
synchronous IPC brings in another

225
00:09:44,579 --> 00:09:49,228
challenge for us and a synchronous IPC

226
00:09:46,769 --> 00:09:51,509
as we have seen earlier it works like a

227
00:09:49,229 --> 00:09:52,949
core sends a message to another core and

228
00:09:51,509 --> 00:09:56,339
then waits for a response from the other

229
00:09:52,949 --> 00:09:58,348
domain and this causes the sender core

230
00:09:56,339 --> 00:10:01,829
to get blocked waiting for a response

231
00:09:58,349 --> 00:10:03,539
so we burned CPU cycles so we need a

232
00:10:01,829 --> 00:10:06,779
mechanism to utilize those cycles

233
00:10:03,539 --> 00:10:09,929
effectively we created lightweight

234
00:10:06,779 --> 00:10:12,149
asynchronous runtime this is similar to

235
00:10:09,929 --> 00:10:13,769
core routines where we create threats of

236
00:10:12,149 --> 00:10:16,649
execution and context switch between

237
00:10:13,769 --> 00:10:19,109
those lightweight threats sure where we

238
00:10:16,649 --> 00:10:20,849
can put this to use the typical example

239
00:10:19,109 --> 00:10:22,919
would be in a packet transmission loop

240
00:10:20,849 --> 00:10:26,220
assume we wanted to send a bunch of

241
00:10:22,919 --> 00:10:28,199
packets and you you do it using a while

242
00:10:26,220 --> 00:10:30,779
loop and instead of calling a real

243
00:10:28,199 --> 00:10:32,819
function to cross the domain we use it

244
00:10:30,779 --> 00:10:34,709
we wrap it around with an asynchronous

245
00:10:32,819 --> 00:10:39,029
macro which converts that into an

246
00:10:34,709 --> 00:10:40,888
asynchronous thread and the the

247
00:10:39,029 --> 00:10:43,199
execution starts by sending the first

248
00:10:40,889 --> 00:10:44,789
packet and assume that gets blocked

249
00:10:43,199 --> 00:10:46,738
waiting for a response from another

250
00:10:44,789 --> 00:10:48,598
domain our runtime automatically

251
00:10:46,739 --> 00:10:50,729
switches the execution to the next line

252
00:10:48,599 --> 00:10:54,049
after the async thread and then

253
00:10:50,729 --> 00:10:57,509
continues to transmit the next packet

254
00:10:54,049 --> 00:11:01,049
having solved all these challenges let's

255
00:10:57,509 --> 00:11:03,289
evaluate our frame work on two flavors

256
00:11:01,049 --> 00:11:07,409
of device drivers the first

257
00:11:03,289 --> 00:11:09,299
software-only device drivers these

258
00:11:07,409 --> 00:11:11,099
device drivers represent a device with

259
00:11:09,299 --> 00:11:13,939
infinite bandwidth and is not associated

260
00:11:11,099 --> 00:11:13,939
with any hardware

261
00:11:14,200 --> 00:11:18,040
and this helps us to understand the

262
00:11:16,329 --> 00:11:19,959
overheads of isolation better for

263
00:11:18,040 --> 00:11:23,620
example a dummy Network driver or a null

264
00:11:19,959 --> 00:11:25,300
block driver we evaluate our

265
00:11:23,620 --> 00:11:27,490
infrastructure we evaluate our

266
00:11:25,300 --> 00:11:29,829
experiments on MLM infrastructure

267
00:11:27,490 --> 00:11:35,920
typically we run our experiments on a

268
00:11:29,829 --> 00:11:38,260
four socket 32 cores Xeon server for we

269
00:11:35,920 --> 00:11:41,769
evaluated dummy network driver by

270
00:11:38,260 --> 00:11:43,569
running iperf benchmarks we measured the

271
00:11:41,769 --> 00:11:45,430
transmission i/o operations per second

272
00:11:43,570 --> 00:11:48,130
we varied the number of application

273
00:11:45,430 --> 00:11:49,719
threads from 1 to 27 to accommodate

274
00:11:48,130 --> 00:11:51,610
these many application threads we also

275
00:11:49,720 --> 00:11:55,300
varied the number of lxd turrets from 1

276
00:11:51,610 --> 00:11:58,450
to 4 where each lxd thread occupies core

277
00:11:55,300 --> 00:12:00,339
in a Numa node as shown here and the

278
00:11:58,450 --> 00:12:03,220
application threads are distributed

279
00:12:00,339 --> 00:12:05,260
across the other CPUs available in an

280
00:12:03,220 --> 00:12:07,649
umma node and each application thread is

281
00:12:05,260 --> 00:12:11,110
pinned to a particular CPU and

282
00:12:07,649 --> 00:12:13,180
communicates monly within the LCD of

283
00:12:11,110 --> 00:12:16,209
that Numa node to a wide cross Numa

284
00:12:13,180 --> 00:12:20,709
overhead this is how the entire setting

285
00:12:16,209 --> 00:12:22,390
looks like for a 27 thread scenario and

286
00:12:20,709 --> 00:12:25,479
this is the performance benchmark for an

287
00:12:22,390 --> 00:12:28,810
unlit device driver with native and

288
00:12:25,480 --> 00:12:32,500
isolated driver in a single threaded lxd

289
00:12:28,810 --> 00:12:34,089
case our native achieves 956 K are you

290
00:12:32,500 --> 00:12:36,640
operations per second whereas or

291
00:12:34,089 --> 00:12:39,190
isolated driver achieves 730 K ie

292
00:12:36,640 --> 00:12:40,689
operations per second which is 76% of

293
00:12:39,190 --> 00:12:43,870
the performance of the native driver and

294
00:12:40,690 --> 00:12:46,839
we pay an isolation overhead of 710

295
00:12:43,870 --> 00:12:50,589
cycles per packet in a multi-threaded

296
00:12:46,839 --> 00:12:52,500
setting or isolated driver achieves 70%

297
00:12:50,589 --> 00:12:57,459
of the performance of the native driver

298
00:12:52,500 --> 00:12:59,980
by running 27 application threads the

299
00:12:57,459 --> 00:13:02,260
second flavor of device drivers we we

300
00:12:59,980 --> 00:13:05,890
isolated drivers that are backed up by

301
00:13:02,260 --> 00:13:09,760
real hardware specifically we decomposed

302
00:13:05,890 --> 00:13:11,949
at 10 Gbps Ethernet driver with Intel

303
00:13:09,760 --> 00:13:13,959
eight two five nine nine hardware and we

304
00:13:11,949 --> 00:13:17,349
ran iperf benchmarks and measured the

305
00:13:13,959 --> 00:13:18,910
packet transmission bandwidth and if you

306
00:13:17,350 --> 00:13:20,560
look at the TX bandwidth in a single

307
00:13:18,910 --> 00:13:23,800
threaded case isolated is 12 percent

308
00:13:20,560 --> 00:13:26,109
faster the main the benefit it gets is

309
00:13:23,800 --> 00:13:27,849
from burning one more additional core in

310
00:13:26,110 --> 00:13:30,820
the a synchronous IPC setting

311
00:13:27,850 --> 00:13:32,860
but this quickly disappears as we add

312
00:13:30,820 --> 00:13:34,630
more number of application traits four

313
00:13:32,860 --> 00:13:36,370
to six application threats we are within

314
00:13:34,630 --> 00:13:39,160
six to 13% of the performance of the

315
00:13:36,370 --> 00:13:41,320
native driver and in the reception

316
00:13:39,160 --> 00:13:44,439
bandwidth benchmark our single threaded

317
00:13:41,320 --> 00:13:46,780
case isolated driver performs on par

318
00:13:44,440 --> 00:13:49,450
with the native this is due to burning

319
00:13:46,780 --> 00:13:52,150
one more additional core and the fact

320
00:13:49,450 --> 00:13:54,910
that we do polling in a different way in

321
00:13:52,150 --> 00:13:56,770
in a native driver the packet reception

322
00:13:54,910 --> 00:13:59,650
is handled by an interrupt and then

323
00:13:56,770 --> 00:14:01,569
offloaded to a soft irq thread and this

324
00:13:59,650 --> 00:14:04,540
incurs a context switching overhead

325
00:14:01,570 --> 00:14:06,640
whereas in our isolated setting we use

326
00:14:04,540 --> 00:14:11,319
busy polling to reap the packets from

327
00:14:06,640 --> 00:14:13,030
the hardware hardware and for two to six

328
00:14:11,320 --> 00:14:14,440
application threads we are within 12 to

329
00:14:13,030 --> 00:14:16,839
18 percent of the performance of the

330
00:14:14,440 --> 00:14:19,030
native driver and these are the drivers

331
00:14:16,840 --> 00:14:22,810
that we isolated to evaluate or

332
00:14:19,030 --> 00:14:25,329
framework and there are some of the

333
00:14:22,810 --> 00:14:27,250
limitations of the framework first we

334
00:14:25,330 --> 00:14:28,950
burn more course because we use an

335
00:14:27,250 --> 00:14:31,180
asynchronous ipeec design and

336
00:14:28,950 --> 00:14:33,310
asynchronous execution model brings in a

337
00:14:31,180 --> 00:14:35,530
lot of challenges for example it's hard

338
00:14:33,310 --> 00:14:38,939
to decompose an interrupt related path

339
00:14:35,530 --> 00:14:41,589
or data structures involving LOC and

340
00:14:38,940 --> 00:14:43,320
manual generation of ideal is hard for

341
00:14:41,590 --> 00:14:45,610
example if you want to isolate

342
00:14:43,320 --> 00:14:47,470
complicated USB driver or a network

343
00:14:45,610 --> 00:14:49,150
driver you would be looking at hundreds

344
00:14:47,470 --> 00:14:51,760
of functions and analyzing those

345
00:14:49,150 --> 00:14:54,340
manually in the future we wish to

346
00:14:51,760 --> 00:14:56,350
explore a low overhead alternative

347
00:14:54,340 --> 00:14:57,910
synchronous isolation mechanisms such as

348
00:14:56,350 --> 00:15:01,000
VM funk or MPX

349
00:14:57,910 --> 00:15:04,120
and we also would like to explore an

350
00:15:01,000 --> 00:15:08,140
auto generation mechanism for ideal

351
00:15:04,120 --> 00:15:10,690
using static analysis in summary we

352
00:15:08,140 --> 00:15:12,990
created lightweight execution domains

353
00:15:10,690 --> 00:15:15,370
which provides general abstractions and

354
00:15:12,990 --> 00:15:19,570
mechanisms for isolating device drivers

355
00:15:15,370 --> 00:15:23,350
and we evaluate our framework on on

356
00:15:19,570 --> 00:15:25,240
device drivers and we demonstrated that

357
00:15:23,350 --> 00:15:27,700
isolating device drivers is feasible

358
00:15:25,240 --> 00:15:29,200
with minimal overhead this concludes my

359
00:15:27,700 --> 00:15:31,610
talk and I'll be happy to have any

360
00:15:29,200 --> 00:15:41,450
questions

361
00:15:31,610 --> 00:15:43,910
a rust plane from Apple very interesting

362
00:15:41,450 --> 00:15:45,230
talk and great work this really reminds

363
00:15:43,910 --> 00:15:48,260
me of something that we are about to

364
00:15:45,230 --> 00:15:50,570
ship with the next version of Mac OS and

365
00:15:48,260 --> 00:15:52,130
iOS that's coming out in the fall we

366
00:15:50,570 --> 00:15:54,649
moved a bunch of device drivers into

367
00:15:52,130 --> 00:15:56,390
user land I'm just wondering why or if

368
00:15:54,649 --> 00:15:59,240
you consider going one step further in

369
00:15:56,390 --> 00:16:03,170
your isolation domains and just running

370
00:15:59,240 --> 00:16:05,959
them in user mode has standard processes

371
00:16:03,170 --> 00:16:07,910
or demons and you get a lot of benefits

372
00:16:05,959 --> 00:16:10,189
from having them fall into the user

373
00:16:07,910 --> 00:16:12,560
abstraction that way you can utilize

374
00:16:10,190 --> 00:16:14,390
existing communication mechanisms

375
00:16:12,560 --> 00:16:16,459
between the kernel and user land instead

376
00:16:14,390 --> 00:16:18,680
of having to invent new IPC mechanisms

377
00:16:16,459 --> 00:16:22,130
yep that's a very interesting thought

378
00:16:18,680 --> 00:16:23,719
the problem with putting everything to

379
00:16:22,130 --> 00:16:25,430
the user space is that like you need

380
00:16:23,720 --> 00:16:27,200
some parts of the code still to be

381
00:16:25,430 --> 00:16:29,180
remained in the kernel right so you need

382
00:16:27,200 --> 00:16:30,740
to pay a performance price of like up

383
00:16:29,180 --> 00:16:32,300
calling to the kernel or sending

384
00:16:30,740 --> 00:16:36,279
something to the kernel like for example

385
00:16:32,300 --> 00:16:36,279
a previous work called micro drivers

386
00:16:36,339 --> 00:16:41,480
micro drivers worked on this approach

387
00:16:38,660 --> 00:16:43,760
and I think you need to manually rewrite

388
00:16:41,480 --> 00:16:45,770
the code and one of the important goals

389
00:16:43,760 --> 00:16:48,709
of our project is to not rewrite the

390
00:16:45,770 --> 00:16:56,180
code I mean like we should be able to

391
00:16:48,709 --> 00:17:00,199
use legacy code as this very interesting

392
00:16:56,180 --> 00:17:03,739
talk so do you need to bear in a core

393
00:17:00,200 --> 00:17:11,900
for every driver or you know all the

394
00:17:03,740 --> 00:17:14,240
drivers that are yep that's a very good

395
00:17:11,900 --> 00:17:17,120
observation the thing is that like say

396
00:17:14,240 --> 00:17:19,309
for example if if we are isolating like

397
00:17:17,119 --> 00:17:21,859
many drivers go ahead and isolate many

398
00:17:19,309 --> 00:17:23,540
drivers and then some can be classified

399
00:17:21,859 --> 00:17:25,938
as non performing drivers say for

400
00:17:23,540 --> 00:17:28,690
example like just very simple i2c driver

401
00:17:25,939 --> 00:17:38,000
which does not have a huge performance

402
00:17:28,690 --> 00:17:40,460
overhead which which which will not have

403
00:17:38,000 --> 00:17:42,260
any any problems that having an overhead

404
00:17:40,460 --> 00:17:44,150
and there are some drivers which are

405
00:17:42,260 --> 00:17:45,379
performant like Network driver at 10

406
00:17:44,150 --> 00:17:47,600
Gbps Ethernet Ivor

407
00:17:45,380 --> 00:17:49,400
a block driver and for that we have to

408
00:17:47,600 --> 00:17:51,320
burn additional course in order to bring

409
00:17:49,400 --> 00:17:53,210
in the performance benefits and for

410
00:17:51,320 --> 00:17:56,330
other drivers we can we can put together

411
00:17:53,210 --> 00:17:57,680
in like just one one core and then say

412
00:17:56,330 --> 00:18:08,240
like all the interactions go through

413
00:17:57,680 --> 00:18:11,720
that so burning a single core for your

414
00:18:08,240 --> 00:18:14,390
network you're basically you're going to

415
00:18:11,720 --> 00:18:17,480
get as much as a single core can give

416
00:18:14,390 --> 00:18:21,200
you as opposed to like regular

417
00:18:17,480 --> 00:18:24,830
monolithic cannon when you run like ten

418
00:18:21,200 --> 00:18:26,240
threats you get you know ten times you

419
00:18:24,830 --> 00:18:29,480
you can basically get ten times

420
00:18:26,240 --> 00:18:30,920
processing and you know you get more TCP

421
00:18:29,480 --> 00:18:33,830
packet processing out of the network

422
00:18:30,920 --> 00:18:36,770
especially with multi cue network so do

423
00:18:33,830 --> 00:18:40,310
you think this is a limitation and you

424
00:18:36,770 --> 00:18:42,830
know if so is there a way to fix it yeah

425
00:18:40,310 --> 00:18:44,540
good question so this is the listen this

426
00:18:42,830 --> 00:18:46,790
is the inherent limitation of running an

427
00:18:44,540 --> 00:18:48,409
asynchronous IPC mechanism right so

428
00:18:46,790 --> 00:18:50,570
that's why we would like to explore a

429
00:18:48,410 --> 00:18:53,180
synchronous IPC design pair we don't

430
00:18:50,570 --> 00:18:54,710
need to burn any anymore course so we

431
00:18:53,180 --> 00:18:56,240
can synchronously context switch into

432
00:18:54,710 --> 00:19:00,980
that domain and then perform all the

433
00:18:56,240 --> 00:19:02,990
operations thank you thank you all right

434
00:19:00,980 --> 00:19:06,050
I'm actually gonna steal a moment for

435
00:19:02,990 --> 00:19:07,490
questions so in our work on enforcing

436
00:19:06,050 --> 00:19:09,110
memory safety on the Linux kernel we

437
00:19:07,490 --> 00:19:10,970
noticed that there were buffer overflows

438
00:19:09,110 --> 00:19:13,129
and other sorts of bugs in components

439
00:19:10,970 --> 00:19:16,100
other than device drivers such as the

440
00:19:13,130 --> 00:19:18,410
networking stack and and and the socket

441
00:19:16,100 --> 00:19:21,080
layer do you think your approach could

442
00:19:18,410 --> 00:19:23,980
actually be extended to encompass larger

443
00:19:21,080 --> 00:19:27,050
kernel components like the tcp/ip stack

444
00:19:23,980 --> 00:19:28,700
or other various of the pluggable

445
00:19:27,050 --> 00:19:32,090
components that are not part of the

446
00:19:28,700 --> 00:19:33,860
quote unquote core kernel yep that's a

447
00:19:32,090 --> 00:19:36,860
very interesting and thought-provoking

448
00:19:33,860 --> 00:19:38,840
question so we got we got similar set of

449
00:19:36,860 --> 00:19:41,000
questions from the reviewers as well the

450
00:19:38,840 --> 00:19:43,399
thing is that like the isolation of

451
00:19:41,000 --> 00:19:45,730
monolithic kernel has its own downsides

452
00:19:43,400 --> 00:19:47,930
like there are limitations to which

453
00:19:45,730 --> 00:19:49,580
components you could decompose for

454
00:19:47,930 --> 00:19:51,650
example it cannot do a virtual

455
00:19:49,580 --> 00:19:53,300
management layer virtual memory

456
00:19:51,650 --> 00:19:55,040
management or you cannot do a block

457
00:19:53,300 --> 00:19:57,470
layer because the interaction patterns

458
00:19:55,040 --> 00:19:58,279
are huge and it's like tightly coupled

459
00:19:57,470 --> 00:20:00,559
with the rest of the

460
00:19:58,279 --> 00:20:03,229
like you can call like a socket

461
00:20:00,559 --> 00:20:05,658
interface from the user space and that

462
00:20:03,229 --> 00:20:07,729
that also calls like the protocol send

463
00:20:05,659 --> 00:20:10,009
message so there are like two ends to it

464
00:20:07,729 --> 00:20:12,679
and which it's really hard to come up

465
00:20:10,009 --> 00:20:16,239
with like cleanslate mechanism to

466
00:20:12,679 --> 00:20:18,739
isolate these components but if we want

467
00:20:16,239 --> 00:20:20,830
such kind of isolation like really

468
00:20:18,739 --> 00:20:23,179
isolating subsystems then we should be

469
00:20:20,830 --> 00:20:25,699
looking at some cleanslate approaches

470
00:20:23,179 --> 00:20:30,889
where the comments are not so tightly

471
00:20:25,700 --> 00:20:34,070
coupled okay thank you so I have a

472
00:20:30,889 --> 00:20:37,728
question I think you gave some numbers

473
00:20:34,070 --> 00:20:40,249
about the cost of switching domains and

474
00:20:37,729 --> 00:20:43,309
like if you are on the same CPU and

475
00:20:40,249 --> 00:20:48,259
everything but I was wondering because

476
00:20:43,309 --> 00:20:51,080
you are doing cross CPU now did you is

477
00:20:48,259 --> 00:20:52,399
the will be overhead of changing the

478
00:20:51,080 --> 00:20:55,009
domain in a cross-appeal

479
00:20:52,399 --> 00:20:58,570
design like this one change depending on

480
00:20:55,009 --> 00:21:02,899
the coherence traffic and so forth so on

481
00:20:58,570 --> 00:21:04,759
sorry so a question yeah I'm saying the

482
00:21:02,899 --> 00:21:08,478
380 cycles that you are saying they're

483
00:21:04,759 --> 00:21:12,739
not mmm is it under any potential

484
00:21:08,479 --> 00:21:15,469
coherence traffic the a synchronous IPC

485
00:21:12,739 --> 00:21:19,009
words on top of the cache coherence

486
00:21:15,469 --> 00:21:22,969
protocol right but I mean if you have a

487
00:21:19,009 --> 00:21:24,849
lot of misses will that be the same you

488
00:21:22,969 --> 00:21:28,729
mean like if your stress yeah the the

489
00:21:24,849 --> 00:21:31,968
the cache domain yeah we haven't really

490
00:21:28,729 --> 00:21:34,099
stressed us the cache domain to to

491
00:21:31,969 --> 00:21:36,049
create a lot of misses or conflicts so

492
00:21:34,099 --> 00:21:38,210
it could be even more than the 80 50

493
00:21:36,049 --> 00:21:39,739
let's yeah we like which which means

494
00:21:38,210 --> 00:21:41,839
like either the system is heavily loaded

495
00:21:39,739 --> 00:21:43,789
or it is under some attack right right

496
00:21:41,839 --> 00:21:45,200
but it could be heavily yeah yeah it

497
00:21:43,789 --> 00:21:48,739
could be heavy lead or it very well yeah

498
00:21:45,200 --> 00:21:49,729
no we didn't measure it actually Thanks

499
00:21:48,739 --> 00:21:51,540
thank you

500
00:21:49,729 --> 00:21:55,230
let's thank our speaker

501
00:21:51,540 --> 00:21:55,230
[Applause]

