1
00:00:10,160 --> 00:00:14,389
so good afternoon everyone my name is

2
00:00:12,530 --> 00:00:16,580
Ashish and I'm presenting my work called

3
00:00:14,389 --> 00:00:19,910
ext fuse which is an extension framework

4
00:00:16,580 --> 00:00:23,060
for file systems and user space so let's

5
00:00:19,910 --> 00:00:25,849
get started okay there you go all right

6
00:00:23,060 --> 00:00:27,859
so compared to kernel file systems user

7
00:00:25,849 --> 00:00:30,080
file systems offer better security and

8
00:00:27,859 --> 00:00:34,100
system reliability they are easy to

9
00:00:30,080 --> 00:00:37,430
develop debug and maintain however they

10
00:00:34,100 --> 00:00:39,110
perform poorly and this is the focus of

11
00:00:37,430 --> 00:00:44,420
my work I'm trying to improve the

12
00:00:39,110 --> 00:00:45,980
performance of user file systems fuse is

13
00:00:44,420 --> 00:00:48,170
the state of the art framework for

14
00:00:45,980 --> 00:00:49,910
developing user file systems it allows

15
00:00:48,170 --> 00:00:53,120
users face to register file system

16
00:00:49,910 --> 00:00:57,140
handlers with kernel such as open read

17
00:00:53,120 --> 00:00:59,089
write etc all file system requests are

18
00:00:57,140 --> 00:01:02,090
served in userspace by corresponding

19
00:00:59,090 --> 00:01:04,399
handlers over 100 fuse file systems have

20
00:01:02,090 --> 00:01:06,289
been written the span across two

21
00:01:04,399 --> 00:01:09,200
categories are stackable in network

22
00:01:06,289 --> 00:01:12,080
stackable file systems add incremental

23
00:01:09,200 --> 00:01:14,930
functionality on top of underlying host

24
00:01:12,080 --> 00:01:17,090
file system for example Android SD card

25
00:01:14,930 --> 00:01:20,630
file system adds custom security check

26
00:01:17,090 --> 00:01:22,670
in contrast network file systems serve

27
00:01:20,630 --> 00:01:25,460
file system requests by talking to a

28
00:01:22,670 --> 00:01:28,880
remote server so let's look at fuse

29
00:01:25,460 --> 00:01:30,949
architecture so the code component of

30
00:01:28,880 --> 00:01:33,408
fuse architecture is the fuse driver

31
00:01:30,950 --> 00:01:36,530
it's a pin interposition layer in the

32
00:01:33,409 --> 00:01:38,750
kernel that interfaces with the VFS the

33
00:01:36,530 --> 00:01:40,970
second component is the fuse daemon that

34
00:01:38,750 --> 00:01:45,470
implements file system handlers to serve

35
00:01:40,970 --> 00:01:47,990
request and user space as applications

36
00:01:45,470 --> 00:01:50,119
make system calls the VFS delivers

37
00:01:47,990 --> 00:01:52,280
filesystem request to the fuse driver

38
00:01:50,119 --> 00:01:55,670
which simply forwards them to the

39
00:01:52,280 --> 00:01:57,909
appropriate handlers in user space so

40
00:01:55,670 --> 00:02:03,259
why do fuse file systems perform poorly

41
00:01:57,909 --> 00:02:05,540
well the the main cause of performance

42
00:02:03,259 --> 00:02:07,549
overhead is the context switching that

43
00:02:05,540 --> 00:02:11,209
occurs while serving file system

44
00:02:07,549 --> 00:02:15,620
requests for example when application

45
00:02:11,209 --> 00:02:17,720
makes open system call the request goes

46
00:02:15,620 --> 00:02:20,150
to the VFS the VFS will intercept the

47
00:02:17,720 --> 00:02:22,010
request and will internally issue

48
00:02:20,150 --> 00:02:24,060
multiple lookup requests to the fuse

49
00:02:22,010 --> 00:02:27,540
driver one for each path

50
00:02:24,060 --> 00:02:30,870
component therefore a single open system

51
00:02:27,540 --> 00:02:33,870
call is generating multiple internal

52
00:02:30,870 --> 00:02:35,280
low-level requests to fuse driver and as

53
00:02:33,870 --> 00:02:38,750
I mentioned the fuse drivers simply

54
00:02:35,280 --> 00:02:42,270
forward such requests to the user space

55
00:02:38,750 --> 00:02:44,819
and if the file system is stackable the

56
00:02:42,270 --> 00:02:46,380
fuse demon will serve the requests by

57
00:02:44,819 --> 00:02:49,049
talking to the underlying lower file

58
00:02:46,380 --> 00:02:51,209
system hence there's a lot of context

59
00:02:49,050 --> 00:02:53,220
switching and data copying going on

60
00:02:51,209 --> 00:02:56,340
which is the cause of the performance

61
00:02:53,220 --> 00:03:01,920
overhead let's evaluate performance of

62
00:02:56,340 --> 00:03:05,670
fuse using kernel completion workload in

63
00:03:01,920 --> 00:03:08,790
this example and compiling Linux 418 on

64
00:03:05,670 --> 00:03:11,190
an Intel quad core machine I'm using

65
00:03:08,790 --> 00:03:14,040
optimized fuse and various optimizations

66
00:03:11,190 --> 00:03:16,620
are listed here for example nonzero

67
00:03:14,040 --> 00:03:19,890
entry timeout and attribute timeout

68
00:03:16,620 --> 00:03:22,290
allow fuse framework to leverage VFS

69
00:03:19,890 --> 00:03:25,350
caches and reduce the number of requests

70
00:03:22,290 --> 00:03:28,980
that are delivered to user space hence

71
00:03:25,350 --> 00:03:31,280
it should show improvement I'm I'm using

72
00:03:28,980 --> 00:03:35,010
simple stackable fuse file system here

73
00:03:31,280 --> 00:03:37,829
the graph shows the measured time in

74
00:03:35,010 --> 00:03:41,700
seconds on y-axis on x-axis I'm

75
00:03:37,829 --> 00:03:45,900
comparing exe for and fuse if you can

76
00:03:41,700 --> 00:03:49,950
see it incurs a very high overhead so

77
00:03:45,900 --> 00:03:51,989
what's the reason let's delve deeper and

78
00:03:49,950 --> 00:03:56,339
analyze the results and understand why

79
00:03:51,989 --> 00:04:01,680
despite optimizations fuse incurs a high

80
00:03:56,340 --> 00:04:04,769
overhead so in this graph I'm showing

81
00:04:01,680 --> 00:04:07,260
the number of the number of requests on

82
00:04:04,769 --> 00:04:09,480
y-axis and the type of request delivered

83
00:04:07,260 --> 00:04:12,600
to fuse demon on x-axis and I'm

84
00:04:09,480 --> 00:04:17,399
comparing regular fuse versus optimized

85
00:04:12,600 --> 00:04:20,608
fuse as seen from the chart when the

86
00:04:17,399 --> 00:04:22,859
optimizations are enabled we see four

87
00:04:20,608 --> 00:04:26,849
times fewer lookup requests to the fuse

88
00:04:22,860 --> 00:04:29,370
daemon but we don't see any reduction in

89
00:04:26,850 --> 00:04:33,539
get attribute or get extended attribute

90
00:04:29,370 --> 00:04:36,900
requests why is that well it turns out

91
00:04:33,539 --> 00:04:37,919
that during kernel compilation a file is

92
00:04:36,900 --> 00:04:43,650
first

93
00:04:37,919 --> 00:04:45,870
opened then it's red then VFS issued the

94
00:04:43,650 --> 00:04:49,529
gate attribute request and finally the

95
00:04:45,870 --> 00:04:51,960
file is closed so when the file is read

96
00:04:49,529 --> 00:04:55,020
the a time on the file changes

97
00:04:51,960 --> 00:04:59,849
hence the attributes are cached by the

98
00:04:55,020 --> 00:05:05,340
VFS change and hence are invalidated

99
00:04:59,849 --> 00:05:07,349
upon read so for the next step when the

100
00:05:05,340 --> 00:05:10,560
attribute request is issued by the VFS

101
00:05:07,349 --> 00:05:12,090
it's not able to leverage the VFS caches

102
00:05:10,560 --> 00:05:12,900
because the attributes have already been

103
00:05:12,090 --> 00:05:15,388
invalidated

104
00:05:12,900 --> 00:05:18,479
hence the requests are delivered to user

105
00:05:15,389 --> 00:05:19,740
space therefore you don't see any

106
00:05:18,479 --> 00:05:24,800
reduction in number of requests in

107
00:05:19,740 --> 00:05:28,199
optimized case similarly VFS issues get

108
00:05:24,800 --> 00:05:30,089
extended attribute requests to reach

109
00:05:28,199 --> 00:05:33,089
security labels before every write

110
00:05:30,089 --> 00:05:35,310
request and since there's no caching of

111
00:05:33,089 --> 00:05:37,740
extended attributes in the kernel all

112
00:05:35,310 --> 00:05:44,129
requests are simply derivative the fuse

113
00:05:37,740 --> 00:05:46,050
daemon in user space so what can we do

114
00:05:44,129 --> 00:05:49,259
how can we improve performance of user

115
00:05:46,050 --> 00:05:51,419
file systems here well if you can reduce

116
00:05:49,259 --> 00:05:54,750
the number of requests delivered to user

117
00:05:51,419 --> 00:05:57,359
space you can improve its performance so

118
00:05:54,750 --> 00:06:00,960
how do we do that we do that by

119
00:05:57,360 --> 00:06:03,629
leveraging a technology called a BPF EBP

120
00:06:00,960 --> 00:06:06,448
F extends Berkley packet filters which

121
00:06:03,629 --> 00:06:08,669
is a pseudo machine architecture for

122
00:06:06,449 --> 00:06:11,250
custom filtering of network packets in

123
00:06:08,669 --> 00:06:13,919
the kernel it is a framework that allows

124
00:06:11,250 --> 00:06:16,770
applications to insert custom code in

125
00:06:13,919 --> 00:06:20,039
the kernel and is safely executed under

126
00:06:16,770 --> 00:06:22,258
a virtual machine runtime it has been

127
00:06:20,039 --> 00:06:24,449
safely designed it has been carefully

128
00:06:22,259 --> 00:06:27,389
designed for security the C code is

129
00:06:24,449 --> 00:06:30,930
first converted into byte code using

130
00:06:27,389 --> 00:06:33,029
Clank LLVM compiler tool chain which is

131
00:06:30,930 --> 00:06:34,770
first verified and then loaded into the

132
00:06:33,029 --> 00:06:38,039
kernel and finally it's executed under

133
00:06:34,770 --> 00:06:39,960
the virtual machine runtime when the

134
00:06:38,039 --> 00:06:44,339
code the the inserted code cannot

135
00:06:39,960 --> 00:06:49,219
allocate memory execute loops or access

136
00:06:44,339 --> 00:06:52,100
any arbitrary kernel functions its

137
00:06:49,220 --> 00:06:54,740
the abb a framework is part of the linux

138
00:06:52,100 --> 00:06:56,930
kernel and it has evolved as a generic

139
00:06:54,740 --> 00:06:59,300
kernel extension framework it is

140
00:06:56,930 --> 00:07:02,030
currently used by tracing perf and

141
00:06:59,300 --> 00:07:05,870
network subsystem in this work we are

142
00:07:02,030 --> 00:07:10,700
leveraging eb PF and applying that to

143
00:07:05,870 --> 00:07:15,080
storage subsystem another feature of EB

144
00:07:10,700 --> 00:07:17,930
PF is that it enables sharing between

145
00:07:15,080 --> 00:07:20,750
the user space application and the

146
00:07:17,930 --> 00:07:26,240
kernel code and the code in the kernel

147
00:07:20,750 --> 00:07:28,460
by by using maps maps are just data

148
00:07:26,240 --> 00:07:30,440
structures to host arbitrary key value

149
00:07:28,460 --> 00:07:33,950
pairs and like I said they are shared

150
00:07:30,440 --> 00:07:36,980
between user space and kernel so using

151
00:07:33,950 --> 00:07:40,340
EBP F we are proposing an extension

152
00:07:36,980 --> 00:07:44,360
framework called ext fuse force for fuse

153
00:07:40,340 --> 00:07:47,150
file systems with EBP F fuse file

154
00:07:44,360 --> 00:07:49,870
systems can register thin custom

155
00:07:47,150 --> 00:07:52,940
handlers in the kernel called extensions

156
00:07:49,870 --> 00:07:56,080
to serve request in the kernel without

157
00:07:52,940 --> 00:07:58,880
context switching to user space

158
00:07:56,080 --> 00:08:00,979
extensions are safely execute excuse

159
00:07:58,880 --> 00:08:04,219
executed under the BPF virtual machine

160
00:08:00,980 --> 00:08:07,790
runtime and we are leveraging a BPF maps

161
00:08:04,220 --> 00:08:09,860
to share data or metadata between the

162
00:08:07,790 --> 00:08:15,020
fuse daemon and the handlers in the kana

163
00:08:09,860 --> 00:08:18,290
let's look at its architecture so we

164
00:08:15,020 --> 00:08:22,430
provide a helper library called EAX lip

165
00:08:18,290 --> 00:08:26,600
ext fuse for developers to write their

166
00:08:22,430 --> 00:08:29,210
handlers in C level language and insert

167
00:08:26,600 --> 00:08:32,330
them into the kernel when the filesystem

168
00:08:29,210 --> 00:08:34,460
is mounted the handlers are first

169
00:08:32,330 --> 00:08:36,800
verified and then inserted into the

170
00:08:34,460 --> 00:08:41,539
kernel which are finally executed under

171
00:08:36,799 --> 00:08:45,589
the virtual machine runtime under ext

172
00:08:41,539 --> 00:08:49,760
fuse a request from application requests

173
00:08:45,589 --> 00:08:53,810
from VFS is delivered to the driver the

174
00:08:49,760 --> 00:08:55,670
driver will first check if first execute

175
00:08:53,810 --> 00:08:58,040
the kernel extension the appropriate

176
00:08:55,670 --> 00:09:01,250
kernel extension and then the kernel

177
00:08:58,040 --> 00:09:02,810
extension can finally either choose to

178
00:09:01,250 --> 00:09:05,510
serve the request entirely

179
00:09:02,810 --> 00:09:09,260
the colonel or take the slow path and go

180
00:09:05,510 --> 00:09:12,050
back to use your space so what can we do

181
00:09:09,260 --> 00:09:14,930
with this framework what applications

182
00:09:12,050 --> 00:09:19,729
can be enable so these are some of the

183
00:09:14,930 --> 00:09:22,130
examples since Exe fuse allow custom

184
00:09:19,730 --> 00:09:23,720
code to be safely executed in the kernel

185
00:09:22,130 --> 00:09:27,260
it opens up a number of opportunities

186
00:09:23,720 --> 00:09:29,750
for optimizations for example you can

187
00:09:27,260 --> 00:09:32,810
proactively cache metadata in the kernel

188
00:09:29,750 --> 00:09:35,690
and it applies to potentially all fused

189
00:09:32,810 --> 00:09:39,219
file system for example lustre read

190
00:09:35,690 --> 00:09:41,630
ahead optimization can proactively cache

191
00:09:39,220 --> 00:09:44,330
directory entries in the kernel and

192
00:09:41,630 --> 00:09:47,750
serve from the kernel as opposed to

193
00:09:44,330 --> 00:09:51,770
going back to user space BPF code can

194
00:09:47,750 --> 00:09:54,740
also be used to perform custom filtering

195
00:09:51,770 --> 00:09:58,610
or permission checks in in the kernel

196
00:09:54,740 --> 00:10:00,710
for example Android SDK FS can perform

197
00:09:58,610 --> 00:10:02,480
UID based checks right in the kernel as

198
00:10:00,710 --> 00:10:05,320
opposed to sending these requests to

199
00:10:02,480 --> 00:10:08,630
user space and incurring high overhead

200
00:10:05,320 --> 00:10:11,270
finally we can also enable direct

201
00:10:08,630 --> 00:10:13,160
pass-through of i/o requests to the

202
00:10:11,270 --> 00:10:16,730
lower filesystem in case of stackable

203
00:10:13,160 --> 00:10:18,680
file systems so as opposed to sending io

204
00:10:16,730 --> 00:10:21,230
request to the user space which are then

205
00:10:18,680 --> 00:10:24,890
served by showing request to the lower

206
00:10:21,230 --> 00:10:26,660
filesystem the fuse driver can directly

207
00:10:24,890 --> 00:10:29,210
send this request to the lower

208
00:10:26,660 --> 00:10:31,069
filesystem where they are served and and

209
00:10:29,210 --> 00:10:34,150
there there are no context switches to

210
00:10:31,070 --> 00:10:37,010
user space so let's take an example

211
00:10:34,150 --> 00:10:40,490
let's take the caching example this

212
00:10:37,010 --> 00:10:42,760
example shows how get attribute requests

213
00:10:40,490 --> 00:10:47,080
can be cached and served in the corner

214
00:10:42,760 --> 00:10:50,870
so for this we first create an a BPF map

215
00:10:47,080 --> 00:10:55,940
to host the entries the map is indexed

216
00:10:50,870 --> 00:10:57,530
by the inode number of objects and the

217
00:10:55,940 --> 00:11:01,310
return value is the data structure that

218
00:10:57,530 --> 00:11:03,529
is expected by the fuse driver since the

219
00:11:01,310 --> 00:11:06,619
map is also accessible to users paste

220
00:11:03,529 --> 00:11:10,580
the fuse daemon can proactively insert

221
00:11:06,620 --> 00:11:13,700
entries in this map which can be used by

222
00:11:10,580 --> 00:11:16,900
the kernel extension which is the gate

223
00:11:13,700 --> 00:11:19,730
attribute handler here

224
00:11:16,900 --> 00:11:21,949
and directly serve the requests in the

225
00:11:19,730 --> 00:11:24,560
corner so if you look at the attribute

226
00:11:21,950 --> 00:11:26,660
the get attribute handler what it's

227
00:11:24,560 --> 00:11:31,099
doing is it's reading the parameters

228
00:11:26,660 --> 00:11:32,930
from the kernel kernel space and it's

229
00:11:31,100 --> 00:11:34,940
going to look up in the map if the

230
00:11:32,930 --> 00:11:37,880
attribute is cached for that particular

231
00:11:34,940 --> 00:11:39,740
inode if it is then it's going to write

232
00:11:37,880 --> 00:11:42,110
the data structure expected by the fuse

233
00:11:39,740 --> 00:11:44,330
driver and return to the fuse driver

234
00:11:42,110 --> 00:11:47,170
without returning to user space so there

235
00:11:44,330 --> 00:11:49,910
is no context switching in this case

236
00:11:47,170 --> 00:11:52,729
however if there is a Miss in the cache

237
00:11:49,910 --> 00:11:55,459
it can always take the regular slow path

238
00:11:52,730 --> 00:12:00,830
and go back to user space and serve the

239
00:11:55,460 --> 00:12:02,860
request since we are caching metadata in

240
00:12:00,830 --> 00:12:06,560
the kernel we also have to timely

241
00:12:02,860 --> 00:12:08,690
invalidate the stale entries to do so

242
00:12:06,560 --> 00:12:13,479
developers need to insert other handlers

243
00:12:08,690 --> 00:12:16,040
for example this example shows how

244
00:12:13,480 --> 00:12:20,200
developers can register set attribute

245
00:12:16,040 --> 00:12:23,990
handler which will invert the cache upon

246
00:12:20,200 --> 00:12:26,150
change changes in the attribute so the

247
00:12:23,990 --> 00:12:29,120
add set attribute handler will intercept

248
00:12:26,150 --> 00:12:31,819
the set attribute request is going to

249
00:12:29,120 --> 00:12:35,450
read the parameters and find out the

250
00:12:31,820 --> 00:12:38,720
inode whose attributes are going to be

251
00:12:35,450 --> 00:12:40,970
changed it will look up the map using

252
00:12:38,720 --> 00:12:43,610
that inode number and simply delete the

253
00:12:40,970 --> 00:12:45,290
entry from the map hence there will be

254
00:12:43,610 --> 00:12:49,850
no more stale entries served in the

255
00:12:45,290 --> 00:12:52,490
corner similarly we can cache look up

256
00:12:49,850 --> 00:12:55,100
extended attributes and symlinks in the

257
00:12:52,490 --> 00:12:57,440
kernel and not cause any context

258
00:12:55,100 --> 00:13:01,850
switching to user space so let's look at

259
00:12:57,440 --> 00:13:03,560
performance of ext fuse so in this case

260
00:13:01,850 --> 00:13:07,930
I'm again considering the Linux a

261
00:13:03,560 --> 00:13:11,089
completion workload the same machine and

262
00:13:07,930 --> 00:13:13,430
we are caching the lookup add lookup

263
00:13:11,089 --> 00:13:15,500
request attribute request an extended

264
00:13:13,430 --> 00:13:17,660
attribute request we have also enabled

265
00:13:15,500 --> 00:13:22,570
pass-through on read and write request

266
00:13:17,660 --> 00:13:26,270
as seen from the chart the time taken is

267
00:13:22,570 --> 00:13:30,370
significantly reduced and ext fuse

268
00:13:26,270 --> 00:13:34,730
incurs only 2.16 percent overhead

269
00:13:30,370 --> 00:13:36,890
compared to optimized fuse and since we

270
00:13:34,730 --> 00:13:39,890
are caching the worst-case memory

271
00:13:36,890 --> 00:13:42,590
overhead would be 50 MB if the caches

272
00:13:39,890 --> 00:13:47,780
are fully utilized and creating two ways

273
00:13:42,590 --> 00:13:51,440
to 16 entries size caches for for one

274
00:13:47,780 --> 00:13:58,189
for lookup and attribute in extended

275
00:13:51,440 --> 00:13:59,960
attributes so let's look at the number

276
00:13:58,190 --> 00:14:04,430
of requests that are received by a few

277
00:13:59,960 --> 00:14:06,770
steamin in case of ext fuse so I'm

278
00:14:04,430 --> 00:14:09,410
comparing ext fuse and optimized fuse a

279
00:14:06,770 --> 00:14:12,140
number of requests are shown on y axis x

280
00:14:09,410 --> 00:14:14,569
axis shows the type of requests as you

281
00:14:12,140 --> 00:14:17,330
can see there are very few attributes to

282
00:14:14,570 --> 00:14:19,040
get attribute requests very few extended

283
00:14:17,330 --> 00:14:20,780
attribute requests and no read/write

284
00:14:19,040 --> 00:14:23,709
requests because they are directly being

285
00:14:20,780 --> 00:14:27,350
delivered to the lower file system

286
00:14:23,710 --> 00:14:31,730
however notice that you still have open

287
00:14:27,350 --> 00:14:36,350
release requests which means that you

288
00:14:31,730 --> 00:14:39,410
can only do so much with exe fuse in

289
00:14:36,350 --> 00:14:42,020
conclusion ext fuse framework can allow

290
00:14:39,410 --> 00:14:43,670
developers to insert thin file system

291
00:14:42,020 --> 00:14:45,380
handlers and serve requests in the

292
00:14:43,670 --> 00:14:48,620
kernel without incurring context switch

293
00:14:45,380 --> 00:14:50,990
to user space developers can use ext

294
00:14:48,620 --> 00:14:53,600
fuse to cache metadata requests directly

295
00:14:50,990 --> 00:14:56,630
pass IO request to underlying file

296
00:14:53,600 --> 00:14:59,870
system and insert custom security checks

297
00:14:56,630 --> 00:15:02,450
in the kernel we ported for fuse file

298
00:14:59,870 --> 00:15:05,600
systems to exe fuse and including

299
00:15:02,450 --> 00:15:07,820
Android SDK FS and we showed significant

300
00:15:05,600 --> 00:15:12,589
performance improvements the details are

301
00:15:07,820 --> 00:15:13,520
mentioned in the paper thank you and the

302
00:15:12,589 --> 00:15:16,370
exe fuse

303
00:15:13,520 --> 00:15:18,770
code is available on on github and the

304
00:15:16,370 --> 00:15:19,940
other talks that were presented at open

305
00:15:18,770 --> 00:15:22,540
source summit and Linux plumbers

306
00:15:19,940 --> 00:15:24,350
conference have also been available

307
00:15:22,540 --> 00:15:25,500
thanks and I'm happy to take any

308
00:15:24,350 --> 00:15:32,299
questions

309
00:15:25,500 --> 00:15:32,299
[Applause]

310
00:15:32,310 --> 00:15:36,189
Shabazz Jaffa University of Toronto

311
00:15:34,570 --> 00:15:37,690
great talk

312
00:15:36,190 --> 00:15:39,910
so there are two context switches

313
00:15:37,690 --> 00:15:42,880
involved in fuse one is from user space

314
00:15:39,910 --> 00:15:45,100
token the second is from Colonel back

315
00:15:42,880 --> 00:15:47,950
into user space so with your approach

316
00:15:45,100 --> 00:15:49,210
the second context switch is reduced

317
00:15:47,950 --> 00:15:52,540
I was wondering what's the overhead of

318
00:15:49,210 --> 00:15:55,120
the first context switch from user space

319
00:15:52,540 --> 00:15:56,980
into the kernel and if there was some

320
00:15:55,120 --> 00:15:59,080
performance gain that could be obtained

321
00:15:56,980 --> 00:16:01,930
if we were to reduce that context switch

322
00:15:59,080 --> 00:16:03,670
as well so you're talking about the

323
00:16:01,930 --> 00:16:06,640
context switching during system calls

324
00:16:03,670 --> 00:16:09,819
yes and is it because of the recent

325
00:16:06,640 --> 00:16:12,280
meltdown patches or in general

326
00:16:09,820 --> 00:16:14,260
so before meltdown there was no context

327
00:16:12,280 --> 00:16:16,089
switching for system calls right so you

328
00:16:14,260 --> 00:16:18,160
can just issue applications can issue

329
00:16:16,090 --> 00:16:20,260
system call until you typically just go

330
00:16:18,160 --> 00:16:21,490
to the kernel without incurring context

331
00:16:20,260 --> 00:16:23,439
which is so I'm not sure what context

332
00:16:21,490 --> 00:16:25,240
which is first context experiment so the

333
00:16:23,440 --> 00:16:27,340
request when it goes from the

334
00:16:25,240 --> 00:16:28,990
application to the fuse driver if it is

335
00:16:27,340 --> 00:16:30,040
redirected directly to the fuse daemon

336
00:16:28,990 --> 00:16:32,680
do you think that would be a new

337
00:16:30,040 --> 00:16:34,810
performance gains by that oh I see okay

338
00:16:32,680 --> 00:16:36,130
so you're saying so it's a to you saying

339
00:16:34,810 --> 00:16:38,140
that it's a two-step process the first

340
00:16:36,130 --> 00:16:40,330
step is to make system calls we go to

341
00:16:38,140 --> 00:16:42,130
the fuse driver and the fuse driver will

342
00:16:40,330 --> 00:16:44,170
incur a context so it's go to the fuse

343
00:16:42,130 --> 00:16:45,790
demon and you're asking if that will

344
00:16:44,170 --> 00:16:48,250
context which can be deduced that's

345
00:16:45,790 --> 00:16:50,770
exactly what I'm trying to avoid here so

346
00:16:48,250 --> 00:16:52,810
what I'm trying to do here is serve the

347
00:16:50,770 --> 00:16:54,850
request directly from fuse driver so

348
00:16:52,810 --> 00:16:56,739
there'll be no context switching not so

349
00:16:54,850 --> 00:16:59,590
so the second context switch is an

350
00:16:56,740 --> 00:17:01,240
artifact of first one right so if there

351
00:16:59,590 --> 00:17:03,400
is no first one there'll be no second

352
00:17:01,240 --> 00:17:07,300
right so the point I'm trying to avoid

353
00:17:03,400 --> 00:17:09,879
the first one yeah thanks yeah she's

354
00:17:07,300 --> 00:17:11,470
great dog so I had a question the

355
00:17:09,880 --> 00:17:13,270
system's you benchmark against for all

356
00:17:11,470 --> 00:17:15,390
stacked file systems correct they

357
00:17:13,270 --> 00:17:17,920
weren't network file systems using fuse

358
00:17:15,390 --> 00:17:19,480
so I haven't benchmarking you good file

359
00:17:17,920 --> 00:17:20,950
system that is correct yes right so in

360
00:17:19,480 --> 00:17:22,630
European what's going to happen if you

361
00:17:20,950 --> 00:17:25,360
benchmark it against them I able to see

362
00:17:22,630 --> 00:17:28,240
the same increase and the follow-up

363
00:17:25,359 --> 00:17:29,740
question is right on the limitation at

364
00:17:28,240 --> 00:17:31,240
least and I use fuse for me is the

365
00:17:29,740 --> 00:17:33,370
hundred and twenty eight kilobyte big

366
00:17:31,240 --> 00:17:35,800
right size right can that be increased

367
00:17:33,370 --> 00:17:38,469
further and increasing that will lead to

368
00:17:35,800 --> 00:17:41,320
a increase in performance for network

369
00:17:38,470 --> 00:17:44,919
file systems in your opinion

370
00:17:41,320 --> 00:17:46,840
so the pass-through feature cannot be

371
00:17:44,919 --> 00:17:49,830
applied to network file system because

372
00:17:46,840 --> 00:17:52,299
there is no lower underlying file system

373
00:17:49,830 --> 00:17:55,990
but the metadata caching can be applied

374
00:17:52,299 --> 00:17:57,700
so like I mentioned the metadata caching

375
00:17:55,990 --> 00:18:00,250
is a genetic optimization can be applied

376
00:17:57,700 --> 00:18:02,380
to all fused file systems in fact I do

377
00:18:00,250 --> 00:18:04,090
mention the cluster read their

378
00:18:02,380 --> 00:18:08,130
optimization that can be enabled using

379
00:18:04,090 --> 00:18:11,379
metadata caching so I expect similar

380
00:18:08,130 --> 00:18:14,500
performance improvement but to a lower

381
00:18:11,380 --> 00:18:16,090
degree for network file system right and

382
00:18:14,500 --> 00:18:17,799
you know the follow up question was is

383
00:18:16,090 --> 00:18:20,639
it possible to increase that big right

384
00:18:17,799 --> 00:18:23,139
size beyond 128 kilobytes

385
00:18:20,640 --> 00:18:24,669
because right now you limited by 128

386
00:18:23,140 --> 00:18:26,650
kilobytes right so that that's

387
00:18:24,669 --> 00:18:28,450
orthogonal to e X diffuse that's a

388
00:18:26,650 --> 00:18:30,280
feature of use I'm not modifying that

389
00:18:28,450 --> 00:18:32,559
I'm just saying that you can cache some

390
00:18:30,280 --> 00:18:34,299
requests in the kernel and serve ok we

391
00:18:32,559 --> 00:18:35,740
can take this off then I had a more fill

392
00:18:34,299 --> 00:18:44,230
your opinion on it whether this is your

393
00:18:35,740 --> 00:18:47,309
ok any more questions thank your speaker

394
00:18:44,230 --> 00:18:47,309
thank you

