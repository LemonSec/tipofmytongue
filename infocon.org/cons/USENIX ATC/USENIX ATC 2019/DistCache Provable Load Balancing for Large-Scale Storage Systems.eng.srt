1
00:00:10,000 --> 00:00:16,000
I'm Alan thank you for coming so this

2
00:00:13,630 --> 00:00:18,760
adjoin talk with folks from Johns

3
00:00:16,000 --> 00:00:21,670
Hopkins University College William Mary

4
00:00:18,760 --> 00:00:24,010
Belfort networks and UC Berkeley so in

5
00:00:21,670 --> 00:00:26,590
this work we introduce a new protocol

6
00:00:24,010 --> 00:00:29,169
mechanism to do load balancing on a data

7
00:00:26,590 --> 00:00:31,300
center skill and also showcase that the

8
00:00:29,169 --> 00:00:33,670
latest high performance networking

9
00:00:31,300 --> 00:00:37,600
hardware like programmable switches can

10
00:00:33,670 --> 00:00:40,600
help storage clusters so let me start

11
00:00:37,600 --> 00:00:43,390
today last skill cloud service needless

12
00:00:40,600 --> 00:00:47,500
storage clusters big players like Google

13
00:00:43,390 --> 00:00:49,600
Microsoft Facebook Amazon Alibaba and so

14
00:00:47,500 --> 00:00:51,580
on their cloud service of serving

15
00:00:49,600 --> 00:00:53,770
billions of the user around the world

16
00:00:51,580 --> 00:00:56,710
so what behind this cloud services are

17
00:00:53,770 --> 00:01:01,720
some large data center clusters and

18
00:00:56,710 --> 00:01:04,659
storage clusters however the storage

19
00:01:01,720 --> 00:01:07,000
servers has load imbalance issue so we

20
00:01:04,659 --> 00:01:09,250
learn from a paper back to 2012 from

21
00:01:07,000 --> 00:01:11,430
face book publishing sig metrics saying

22
00:01:09,250 --> 00:01:14,409
that the typical were close to their

23
00:01:11,430 --> 00:01:17,470
storage clusters are highly skewed and

24
00:01:14,409 --> 00:01:19,450
dynamic so highly skew means that the

25
00:01:17,470 --> 00:01:20,950
only a few of very hot items been

26
00:01:19,450 --> 00:01:24,730
queried many many times in the workload

27
00:01:20,950 --> 00:01:27,400
and dynamic means the hot items are keep

28
00:01:24,730 --> 00:01:29,500
changing so the skewness of the

29
00:01:27,400 --> 00:01:33,220
workloads bring imbalance to the backend

30
00:01:29,500 --> 00:01:35,320
storage servers assuming that using your

31
00:01:33,220 --> 00:01:38,110
cloud storage service are some like

32
00:01:35,320 --> 00:01:40,600
random partition based service like

33
00:01:38,110 --> 00:01:42,750
memcache tea and so on so some server

34
00:01:40,600 --> 00:01:46,089
will get overloaded easily

35
00:01:42,750 --> 00:01:51,190
so our motivating question is simple can

36
00:01:46,090 --> 00:01:52,869
we balance the server load to solve the

37
00:01:51,190 --> 00:01:54,039
load imbalance issue there are several

38
00:01:52,869 --> 00:01:56,729
solutions

39
00:01:54,040 --> 00:01:59,770
first of all let's think about

40
00:01:56,729 --> 00:02:01,390
consistent hashing seems like consistent

41
00:01:59,770 --> 00:02:03,490
question can work for like static

42
00:02:01,390 --> 00:02:07,119
workload and balance the load for that

43
00:02:03,490 --> 00:02:09,008
but in fact it cannot handle the dynamic

44
00:02:07,119 --> 00:02:12,010
and ask you to workloads which are

45
00:02:09,008 --> 00:02:15,970
currently the chain in the storage

46
00:02:12,010 --> 00:02:18,790
workloads and traditionally we can also

47
00:02:15,970 --> 00:02:21,280
consider using migration or replication

48
00:02:18,790 --> 00:02:23,590
we move data around replicate the data

49
00:02:21,280 --> 00:02:24,510
to many servers then can help balance

50
00:02:23,590 --> 00:02:28,060
the

51
00:02:24,510 --> 00:02:30,429
but you're moving data around will cause

52
00:02:28,060 --> 00:02:33,190
large cistern and storage overhead and

53
00:02:30,430 --> 00:02:38,290
if you want make the data consistent it

54
00:02:33,190 --> 00:02:40,720
also incur high coherence cost cert we

55
00:02:38,290 --> 00:02:43,120
can consider using a friend and cache as

56
00:02:40,720 --> 00:02:45,760
a load balancer so what it means that

57
00:02:43,120 --> 00:02:48,610
you can place a friend and cache in

58
00:02:45,760 --> 00:02:51,100
front of a number of servers and then

59
00:02:48,610 --> 00:02:54,370
this cache can observe the hottest items

60
00:02:51,100 --> 00:02:56,440
and also leave the less hot items to the

61
00:02:54,370 --> 00:02:58,750
back-end servers in this way the load

62
00:02:56,440 --> 00:03:00,760
will be balanced the good part of this

63
00:02:58,750 --> 00:03:03,070
friend and cache is that it has low

64
00:03:00,760 --> 00:03:04,840
update overhead because there's only one

65
00:03:03,070 --> 00:03:08,230
copy in the cache and it can work for

66
00:03:04,840 --> 00:03:10,690
arbitrary workloads so in today's talk

67
00:03:08,230 --> 00:03:14,890
we consider using caching as a load

68
00:03:10,690 --> 00:03:18,850
balancer so peer will result back to

69
00:03:14,890 --> 00:03:20,829
2011 sock they have proven statically

70
00:03:18,850 --> 00:03:23,049
that if you want to balance the end

71
00:03:20,830 --> 00:03:25,330
servers in your cluster you can simply

72
00:03:23,050 --> 00:03:28,360
put a small friend and cache in front of

73
00:03:25,330 --> 00:03:31,330
this and servers and then you always

74
00:03:28,360 --> 00:03:33,580
catch the hottest Big O of unlocking

75
00:03:31,330 --> 00:03:36,940
items then the server load will be

76
00:03:33,580 --> 00:03:38,650
balanced regardless of the workloads so

77
00:03:36,940 --> 00:03:40,570
this is a very cool result but there is

78
00:03:38,650 --> 00:03:43,060
a requirement that this friend and the

79
00:03:40,570 --> 00:03:46,959
cache should be as fast as aggregation

80
00:03:43,060 --> 00:03:48,660
of n servers together so levering the

81
00:03:46,960 --> 00:03:51,280
result there are some existing

82
00:03:48,660 --> 00:03:55,780
applications for example the result from

83
00:03:51,280 --> 00:03:59,140
an SDR 16 as SOS P 17 the using result

84
00:03:55,780 --> 00:04:02,740
to build a load balancer in one cluster

85
00:03:59,140 --> 00:04:08,230
and using the latest hardware like SSD

86
00:04:02,740 --> 00:04:11,710
or switches but what if we consider much

87
00:04:08,230 --> 00:04:14,079
larger scale scale with many clusters

88
00:04:11,710 --> 00:04:16,299
for example here M clusters each class

89
00:04:14,080 --> 00:04:18,489
has n servers can we still using the

90
00:04:16,298 --> 00:04:20,890
same result to balance the load seems

91
00:04:18,488 --> 00:04:25,120
reasonable but let's look at some

92
00:04:20,890 --> 00:04:28,419
numbers so assuming we have 32 clusters

93
00:04:25,120 --> 00:04:30,760
and each cluster has 32 servers and each

94
00:04:28,420 --> 00:04:33,729
server are serving like 40 G throughput

95
00:04:30,760 --> 00:04:38,289
which are like very typical numbers in

96
00:04:33,729 --> 00:04:41,050
modern data center then your big cache

97
00:04:38,290 --> 00:04:43,780
should be as fast as 41 Turbots per

98
00:04:41,050 --> 00:04:46,330
second which is clearly not scalable and

99
00:04:43,780 --> 00:04:49,179
practical which implies that one big

100
00:04:46,330 --> 00:04:52,000
cache is not a practical way to do the

101
00:04:49,180 --> 00:04:54,810
load balancing for this scale so let's

102
00:04:52,000 --> 00:04:58,180
try to resolve the problem

103
00:04:54,810 --> 00:05:01,180
so our insight here is is that we can

104
00:04:58,180 --> 00:05:04,419
only use more cache nodes it's a small

105
00:05:01,180 --> 00:05:07,330
cache node can work exactly same as the

106
00:05:04,419 --> 00:05:15,099
one pick node this R is a big question

107
00:05:07,330 --> 00:05:16,719
mark first let's try to build a load

108
00:05:15,100 --> 00:05:18,850
balancer with small cache nodes

109
00:05:16,720 --> 00:05:22,030
first of all let's try to balance the

110
00:05:18,850 --> 00:05:24,699
load within each cluster for example put

111
00:05:22,030 --> 00:05:27,400
each friend and cache in each of the

112
00:05:24,699 --> 00:05:30,400
cluster and then the server loads

113
00:05:27,400 --> 00:05:32,409
weezing a cluster are balanced however

114
00:05:30,400 --> 00:05:35,320
the loads between the cluster are still

115
00:05:32,410 --> 00:05:37,449
not balanced some cluster can still

116
00:05:35,320 --> 00:05:42,010
receive massive loads and making

117
00:05:37,449 --> 00:05:44,020
balanced and then we can still reuse the

118
00:05:42,010 --> 00:05:47,560
periods our gens mentioned we put

119
00:05:44,020 --> 00:05:51,820
another big node in front of this M big

120
00:05:47,560 --> 00:05:53,979
service because each M cluster now has a

121
00:05:51,820 --> 00:05:56,199
friend and cache and then it's balanced

122
00:05:53,979 --> 00:05:58,599
inside each cluster and then it's become

123
00:05:56,199 --> 00:06:00,039
a big server then you still put the one

124
00:05:58,599 --> 00:06:03,639
big node it's solve the problem

125
00:06:00,039 --> 00:06:05,830
you cache the hottest M log M items this

126
00:06:03,639 --> 00:06:09,610
the loads between the clusters are

127
00:06:05,830 --> 00:06:15,639
balanced but still a problem here this

128
00:06:09,610 --> 00:06:17,979
ugly big cache note so instead of using

129
00:06:15,639 --> 00:06:20,800
a big cash node we are proposing is

130
00:06:17,979 --> 00:06:22,659
called disk cache which is a distributed

131
00:06:20,800 --> 00:06:25,479
caching mechanism to provide load

132
00:06:22,659 --> 00:06:27,340
balancing across the clusters basically

133
00:06:25,479 --> 00:06:29,440
the key idea is that we do not need a

134
00:06:27,340 --> 00:06:32,109
big friend and cache but we can split

135
00:06:29,440 --> 00:06:34,240
this big friend and cache into a number

136
00:06:32,110 --> 00:06:36,430
of smaller cache nodes as long as the

137
00:06:34,240 --> 00:06:39,400
aggregation of the throughput is same as

138
00:06:36,430 --> 00:06:42,639
the one big node for simplicity of the

139
00:06:39,400 --> 00:06:45,460
presentation we split here with Pablito

140
00:06:42,639 --> 00:06:48,400
m cache nodes which is the same as

141
00:06:45,460 --> 00:06:50,950
number of clusters and also each class

142
00:06:48,400 --> 00:06:54,039
node has the same throughput

143
00:06:50,950 --> 00:06:56,500
but it can be a genius as well but I

144
00:06:54,040 --> 00:07:00,100
will show this later that this cache

145
00:06:56,500 --> 00:07:02,140
provide provable practical antigen a

146
00:07:00,100 --> 00:07:05,500
general mechanism to do load balancing

147
00:07:02,140 --> 00:07:07,090
so prove all means we carefully analysis

148
00:07:05,500 --> 00:07:10,450
this reticle guarantee behind this

149
00:07:07,090 --> 00:07:13,960
mechanism and show that it is robust for

150
00:07:10,450 --> 00:07:16,330
arbitrary workloads critical means this

151
00:07:13,960 --> 00:07:18,159
cache only require very simple

152
00:07:16,330 --> 00:07:21,370
primitives and it can be easily

153
00:07:18,160 --> 00:07:23,920
implemented in many scenario general

154
00:07:21,370 --> 00:07:27,610
means the discussion mechanism can be

155
00:07:23,920 --> 00:07:32,080
applied to many stories scenarios so let

156
00:07:27,610 --> 00:07:34,690
me start the guarantee of the disk cache

157
00:07:32,080 --> 00:07:37,359
is the following this cache is almost

158
00:07:34,690 --> 00:07:41,469
same as the one big cache note I printed

159
00:07:37,360 --> 00:07:43,870
before we actually we proves radically

160
00:07:41,470 --> 00:07:46,060
and also empirically that this cache

161
00:07:43,870 --> 00:07:50,380
support any query were close to the

162
00:07:46,060 --> 00:07:52,960
hottest Big O of M log M items and each

163
00:07:50,380 --> 00:07:55,150
cache node will not be overloaded in

164
00:07:52,960 --> 00:07:58,930
this mechanism and the cache coherence

165
00:07:55,150 --> 00:08:00,549
cost will also be minimized with the

166
00:07:58,930 --> 00:08:02,830
following requirement this is the only

167
00:08:00,550 --> 00:08:05,170
assumption we need the query rate for a

168
00:08:02,830 --> 00:08:09,030
single item is no larger than half of

169
00:08:05,170 --> 00:08:12,490
one cache mode which means a single

170
00:08:09,030 --> 00:08:14,530
query rates for a single item is no more

171
00:08:12,490 --> 00:08:20,200
than half of a cluster which is pretty

172
00:08:14,530 --> 00:08:23,200
reasonable so keep this guarantees in

173
00:08:20,200 --> 00:08:26,920
mind we have the following design

174
00:08:23,200 --> 00:08:28,690
challengers for this cache so the most

175
00:08:26,920 --> 00:08:31,090
obvious one is how to allocate this

176
00:08:28,690 --> 00:08:32,080
cache item once identify these hot items

177
00:08:31,090 --> 00:08:34,390
how to catch them

178
00:08:32,080 --> 00:08:36,520
that we do not want to overload any

179
00:08:34,390 --> 00:08:40,360
cache load and we do not want to incur

180
00:08:36,520 --> 00:08:42,850
high cache coherence cost the second

181
00:08:40,360 --> 00:08:46,300
challenge how to once you allocate these

182
00:08:42,850 --> 00:08:48,910
items how to query them now you have to

183
00:08:46,300 --> 00:08:52,770
layer of the cache nodes how you can

184
00:08:48,910 --> 00:08:55,959
provide a best stable query distribution

185
00:08:52,770 --> 00:08:59,530
change in number three how to update the

186
00:08:55,960 --> 00:09:01,570
cached items so in the update phase we

187
00:08:59,530 --> 00:09:04,699
use a classical two phase update

188
00:09:01,570 --> 00:09:07,190
mechanism so we invalidate

189
00:09:04,700 --> 00:09:09,950
him first and then update so in this

190
00:09:07,190 --> 00:09:12,080
talk I'm gonna focus how we address

191
00:09:09,950 --> 00:09:16,610
challenge number one and gender number

192
00:09:12,080 --> 00:09:20,150
two for channel number one how to

193
00:09:16,610 --> 00:09:23,900
allocate the cash items we can consider

194
00:09:20,150 --> 00:09:26,840
two traditional strong resolutions once

195
00:09:23,900 --> 00:09:29,510
we identify some hot items ABCDE

196
00:09:26,840 --> 00:09:32,360
we want to catch them into for example a

197
00:09:29,510 --> 00:09:35,840
player cash notes we can just do

198
00:09:32,360 --> 00:09:38,480
replication we replicate ABCD to all a

199
00:09:35,840 --> 00:09:41,480
player cash notes then the player will

200
00:09:38,480 --> 00:09:44,390
walk exactly same as one big note it's a

201
00:09:41,480 --> 00:09:46,340
province of no consider the cache

202
00:09:44,390 --> 00:09:49,160
coherence cost if you want to update

203
00:09:46,340 --> 00:09:51,410
item a in a player it needs to update to

204
00:09:49,160 --> 00:09:55,819
the all cash nodes which means all

205
00:09:51,410 --> 00:09:57,800
copies in a player it's not ideal and

206
00:09:55,820 --> 00:10:01,390
then let's consider the other way that's

207
00:09:57,800 --> 00:10:03,770
just to partition what if we have ABCDE

208
00:10:01,390 --> 00:10:05,810
we want to partition based on some

209
00:10:03,770 --> 00:10:09,020
particular order in this way we

210
00:10:05,810 --> 00:10:11,869
partition ABC you know into the same

211
00:10:09,020 --> 00:10:14,630
cache note and also call incidentally we

212
00:10:11,870 --> 00:10:17,360
also partition ABC into the same note in

213
00:10:14,630 --> 00:10:19,189
the lower layer then if ABC are really

214
00:10:17,360 --> 00:10:21,440
really hot items in the workload and

215
00:10:19,190 --> 00:10:24,770
there's some chance that it can overload

216
00:10:21,440 --> 00:10:29,510
both notes and make the load still

217
00:10:24,770 --> 00:10:32,810
imbalanced so what we do instead is

218
00:10:29,510 --> 00:10:34,760
quite simple but works is we using two

219
00:10:32,810 --> 00:10:39,520
independent hash functions to allocate

220
00:10:34,760 --> 00:10:42,560
hash Heights hot items we have h1 and h2

221
00:10:39,520 --> 00:10:45,949
so in the first a player cache notes

222
00:10:42,560 --> 00:10:48,979
we're using h1 and then the hash names

223
00:10:45,950 --> 00:10:52,550
like a b c d e f into this particular

224
00:10:48,980 --> 00:10:57,950
order c DF has been cashed into the same

225
00:10:52,550 --> 00:11:00,109
cache note but in the lower layer we

226
00:10:57,950 --> 00:11:02,630
using another independent hash function

227
00:11:00,110 --> 00:11:05,840
so with with constant high probability

228
00:11:02,630 --> 00:11:09,080
the CDE will be spread out will not be

229
00:11:05,840 --> 00:11:12,740
hashed to the same note again so based

230
00:11:09,080 --> 00:11:15,170
on this once you have some hot item CDF

231
00:11:12,740 --> 00:11:17,360
in the up layer that you can more helper

232
00:11:15,170 --> 00:11:18,439
from the lower layer which can spread

233
00:11:17,360 --> 00:11:22,759
out your

234
00:11:18,440 --> 00:11:24,680
cash loads so we actually show that this

235
00:11:22,759 --> 00:11:27,410
provides stable and the best cash or

236
00:11:24,680 --> 00:11:29,388
location and also if you want to update

237
00:11:27,410 --> 00:11:32,689
your cache the coherence cause this is

238
00:11:29,389 --> 00:11:36,850
simple because one copy in each layer

239
00:11:32,690 --> 00:11:39,379
it's much smaller than the replication

240
00:11:36,850 --> 00:11:43,160
and chain number two how to carry the

241
00:11:39,379 --> 00:11:47,000
cached items so now you have two choices

242
00:11:43,160 --> 00:11:49,540
two copies in both layers right so you

243
00:11:47,000 --> 00:11:51,949
can for example you want curry see

244
00:11:49,540 --> 00:11:55,610
naively you can always curry from the up

245
00:11:51,949 --> 00:11:57,769
layer first but we show that query from

246
00:11:55,610 --> 00:12:00,529
the uh player first cannot guarantee the

247
00:11:57,769 --> 00:12:03,949
best report sometimes you need to query

248
00:12:00,529 --> 00:12:07,250
from lower layer first so what we do

249
00:12:03,949 --> 00:12:10,519
instead is we're using power two choices

250
00:12:07,250 --> 00:12:13,370
to query the cached items assuming we

251
00:12:10,519 --> 00:12:16,370
know the loads of freedom to get query

252
00:12:13,370 --> 00:12:20,000
to two items C we know the loads of both

253
00:12:16,370 --> 00:12:22,100
cache knows that has this item C then in

254
00:12:20,000 --> 00:12:25,819
this case we query from the lower layer

255
00:12:22,100 --> 00:12:28,720
first in this way oh but poor two

256
00:12:25,819 --> 00:12:32,420
choices can route the queries with the

257
00:12:28,720 --> 00:12:33,949
guaranteed stable throughput so if

258
00:12:32,420 --> 00:12:37,459
you're interested in the proof please

259
00:12:33,949 --> 00:12:39,740
read our paper so let's put in together

260
00:12:37,459 --> 00:12:42,920
this cache only have two simple

261
00:12:39,740 --> 00:12:45,259
primitives when you do the cache

262
00:12:42,920 --> 00:12:47,300
allocation you use two independent hash

263
00:12:45,259 --> 00:12:50,120
functions to allocate the cache items

264
00:12:47,300 --> 00:12:52,370
and when you queried items you pay some

265
00:12:50,120 --> 00:12:58,339
power to choices with the current cash

266
00:12:52,370 --> 00:13:01,490
flows to write queries so there are many

267
00:12:58,339 --> 00:13:04,970
deployment scenario for this cache so

268
00:13:01,490 --> 00:13:08,199
for the cache note we can just use like

269
00:13:04,970 --> 00:13:11,120
DRAM or SSD array to serve as the

270
00:13:08,199 --> 00:13:16,339
Brendan cache the server side can be

271
00:13:11,120 --> 00:13:18,199
just flash of this now you can also

272
00:13:16,339 --> 00:13:20,629
consider using emergent programmable

273
00:13:18,199 --> 00:13:24,620
switch which has much higher performance

274
00:13:20,629 --> 00:13:27,769
to do the Kashin so and then the server

275
00:13:24,620 --> 00:13:30,339
side back-end service will be like DRAM

276
00:13:27,769 --> 00:13:30,339
based for example

277
00:13:31,430 --> 00:13:36,719
so to demonstrate the benefits of this

278
00:13:34,620 --> 00:13:39,330
cash we consider a case study using

279
00:13:36,720 --> 00:13:41,220
programmable switch so which means each

280
00:13:39,330 --> 00:13:44,730
of this cash node in two layers are

281
00:13:41,220 --> 00:13:47,550
programmable switch so here you can see

282
00:13:44,730 --> 00:13:50,720
if there is a cash heat so request from

283
00:13:47,550 --> 00:13:53,520
the CAD the client cluster then it will

284
00:13:50,720 --> 00:13:57,000
first go through the client side switch

285
00:13:53,520 --> 00:14:00,270
which decide which cash node to access

286
00:13:57,000 --> 00:14:04,290
the item you want to access based on the

287
00:14:00,270 --> 00:14:07,230
probe two choices and then one of the

288
00:14:04,290 --> 00:14:09,660
cash node will reply with the cash the

289
00:14:07,230 --> 00:14:12,630
item in this way we actually can tell

290
00:14:09,660 --> 00:14:14,610
you that this also you prove the latency

291
00:14:12,630 --> 00:14:16,950
because you don't need to go to the back

292
00:14:14,610 --> 00:14:18,510
end for example we are using Redis so

293
00:14:16,950 --> 00:14:21,060
you don't need to access the reduce

294
00:14:18,510 --> 00:14:24,150
server you directly access the the cash

295
00:14:21,060 --> 00:14:26,189
in the switches which give you the

296
00:14:24,150 --> 00:14:28,500
end-to-end latency about the microsecond

297
00:14:26,190 --> 00:14:31,589
level which is much faster then go to

298
00:14:28,500 --> 00:14:33,810
the backend server and then if it's a

299
00:14:31,589 --> 00:14:38,430
cache miss into a handle as the normal

300
00:14:33,810 --> 00:14:41,099
way we'll go to the the corresponding

301
00:14:38,430 --> 00:14:46,949
storage servers that running the the

302
00:14:41,100 --> 00:14:49,740
radius so you could be like curious how

303
00:14:46,950 --> 00:14:52,980
we do this in the programmable switches

304
00:14:49,740 --> 00:14:59,250
like serve you using the switches as a

305
00:14:52,980 --> 00:15:01,080
cache note we actually using a open

306
00:14:59,250 --> 00:15:03,510
source language called p4 which stands

307
00:15:01,080 --> 00:15:07,560
for programmable protocol independent

308
00:15:03,510 --> 00:15:10,350
packet processing to program the

309
00:15:07,560 --> 00:15:12,060
programmable switch Hardware so I would

310
00:15:10,350 --> 00:15:16,700
give a quick high-level overview how

311
00:15:12,060 --> 00:15:19,020
this implementation looks like so the

312
00:15:16,700 --> 00:15:21,170
programmability from the switch give you

313
00:15:19,020 --> 00:15:24,540
the power then you can define your own

314
00:15:21,170 --> 00:15:27,180
packet format so on the left side some

315
00:15:24,540 --> 00:15:29,670
existing packet header you can include

316
00:15:27,180 --> 00:15:31,199
the Ethernet the IP and TCP header and

317
00:15:29,670 --> 00:15:34,349
the right side based on your own

318
00:15:31,200 --> 00:15:37,680
protocol here or this cache needs like

319
00:15:34,350 --> 00:15:42,140
sequence number operation like get or

320
00:15:37,680 --> 00:15:42,140
write the key and the value

321
00:15:42,209 --> 00:15:46,589
then the packet wheel goes through a

322
00:15:43,800 --> 00:15:48,689
programmable passer to pass the packets

323
00:15:46,589 --> 00:15:50,699
and store them into some shared memory

324
00:15:48,689 --> 00:15:54,089
and go through a bunch of match action

325
00:15:50,699 --> 00:15:58,008
tables after bunch of mass action tables

326
00:15:54,089 --> 00:16:00,569
it will be reassembled in the de passer

327
00:15:58,009 --> 00:16:02,929
let me give a quick example how this

328
00:16:00,569 --> 00:16:07,290
works what if we want to get some items

329
00:16:02,929 --> 00:16:10,529
from the programmable switch then we

330
00:16:07,290 --> 00:16:12,540
send a request in a packet and goes

331
00:16:10,529 --> 00:16:14,610
through the pasture and a passer will

332
00:16:12,540 --> 00:16:17,399
pass the packet in in the shared memory

333
00:16:14,610 --> 00:16:19,800
the ethernet IP TCP and opera Sh

334
00:16:17,399 --> 00:16:22,559
sequence number number one and operation

335
00:16:19,800 --> 00:16:24,628
is GATT and the key is a and now the

336
00:16:22,559 --> 00:16:26,819
value is known because you have a get

337
00:16:24,629 --> 00:16:29,339
operation you haven't read your value

338
00:16:26,819 --> 00:16:32,819
yet and then they're a bunch of

339
00:16:29,339 --> 00:16:34,199
programmable match action table in a

340
00:16:32,819 --> 00:16:37,490
very high level it works as the

341
00:16:34,199 --> 00:16:43,410
following what if you match the action

342
00:16:37,490 --> 00:16:45,929
the operation is get then you do the

343
00:16:43,410 --> 00:16:51,118
action is like increase the gate load

344
00:16:45,929 --> 00:16:53,279
and then if also match the key you want

345
00:16:51,119 --> 00:16:57,119
to search this is equal to a and this

346
00:16:53,279 --> 00:16:59,490
key is valid then you access the

347
00:16:57,119 --> 00:17:02,519
register in your programmable switch to

348
00:16:59,490 --> 00:17:07,429
get a value of a once you get a value of

349
00:17:02,519 --> 00:17:07,429
a then you write into your shared memory

350
00:17:09,079 --> 00:17:15,209
finally this packet will be reassembled

351
00:17:12,449 --> 00:17:17,069
from the shared memory and reply back to

352
00:17:15,209 --> 00:17:20,520
the client in this way it's the whole

353
00:17:17,069 --> 00:17:26,639
process how programming message to serve

354
00:17:20,520 --> 00:17:29,010
as the cache note so let's do some

355
00:17:26,640 --> 00:17:32,580
evaluation the set up is quite simple

356
00:17:29,010 --> 00:17:34,590
we're using two six-month sub 605

357
00:17:32,580 --> 00:17:37,080
terabytes per second therefore Tofino

358
00:17:34,590 --> 00:17:38,970
switches to serve as the up layer cache

359
00:17:37,080 --> 00:17:40,860
node and lower leg cache node and

360
00:17:38,970 --> 00:17:43,860
cleanse side switches and we're using

361
00:17:40,860 --> 00:17:46,799
two strong physical servers to emulate

362
00:17:43,860 --> 00:17:49,168
the storage servers and clients client

363
00:17:46,799 --> 00:17:52,049
servers the baseline comparation we are

364
00:17:49,169 --> 00:17:54,040
compare our no cache cache partition and

365
00:17:52,049 --> 00:17:57,100
cache replication

366
00:17:54,040 --> 00:17:59,639
so the key takeaway is for read queries

367
00:17:57,100 --> 00:18:02,740
these cache works as good as replication

368
00:17:59,640 --> 00:18:05,080
for right queries this cache performs

369
00:18:02,740 --> 00:18:07,660
significantly better which means when

370
00:18:05,080 --> 00:18:10,090
read ratio is reasonable let's say less

371
00:18:07,660 --> 00:18:12,370
than 30% these cache provide batters

372
00:18:10,090 --> 00:18:14,860
were put among all other baseline

373
00:18:12,370 --> 00:18:17,800
corporations and once your retro issue

374
00:18:14,860 --> 00:18:20,830
is really crazy high Lajeunesse 30% is

375
00:18:17,800 --> 00:18:23,169
as good as partition so please know that

376
00:18:20,830 --> 00:18:27,070
it's usually like the right ratio is

377
00:18:23,170 --> 00:18:32,140
about 3 to 5 presenting in practice so

378
00:18:27,070 --> 00:18:34,780
let's show some real figures so here the

379
00:18:32,140 --> 00:18:36,670
x axis is different distribution the y

380
00:18:34,780 --> 00:18:39,190
axis is nom nice throughput you can see

381
00:18:36,670 --> 00:18:41,770
here this cache works nearly perfect

382
00:18:39,190 --> 00:18:43,960
report for skewed or clothes like

383
00:18:41,770 --> 00:18:48,480
uniform is definitely balanced but

384
00:18:43,960 --> 00:18:48,480
defend is some more skewed or clothes

385
00:18:49,080 --> 00:18:54,550
then you increase let's see you increase

386
00:18:52,810 --> 00:18:58,260
the number of storage clusters like

387
00:18:54,550 --> 00:19:01,030
increasing to like 4,000 storage nodes

388
00:18:58,260 --> 00:19:03,129
this cache actually can scale linearly

389
00:19:01,030 --> 00:19:05,920
with the number of nodes you can see its

390
00:19:03,130 --> 00:19:10,090
work exactly same as replication which

391
00:19:05,920 --> 00:19:12,340
is pretty good and also for write

392
00:19:10,090 --> 00:19:16,540
queries you can see here four different

393
00:19:12,340 --> 00:19:19,600
right ratio with deep fan 19 nice work

394
00:19:16,540 --> 00:19:22,060
load the disk cache always offer the

395
00:19:19,600 --> 00:19:25,990
best throughput especially one less than

396
00:19:22,060 --> 00:19:30,340
30% its beats all other baseline

397
00:19:25,990 --> 00:19:33,190
operations so let me conclude this cache

398
00:19:30,340 --> 00:19:35,110
is a general distributed caching

399
00:19:33,190 --> 00:19:38,050
mechanism to ensure the load balancing

400
00:19:35,110 --> 00:19:41,199
across many storage clusters and it only

401
00:19:38,050 --> 00:19:42,909
required very simple primitives there is

402
00:19:41,200 --> 00:19:45,730
independent hussion and pop two choices

403
00:19:42,910 --> 00:19:47,980
for routing and then it provides

404
00:19:45,730 --> 00:19:51,250
near-perfect report with very rigorous

405
00:19:47,980 --> 00:19:53,200
radical guarantees so I'm ending my talk

406
00:19:51,250 --> 00:19:55,740
now and I'm happy to take questions

407
00:19:53,200 --> 00:19:55,740
thank you

408
00:19:58,529 --> 00:20:03,999
all right we have a little time but if

409
00:20:01,570 --> 00:20:11,950
you have one or two questions please

410
00:20:03,999 --> 00:20:13,989
come up to the microphone then I have

411
00:20:11,950 --> 00:20:16,239
kind of questions now as much might be

412
00:20:13,989 --> 00:20:19,599
kind of naive but I'm I'm curious like

413
00:20:16,239 --> 00:20:20,919
if you want to generalize it for people

414
00:20:19,599 --> 00:20:24,189
that say you want to consider for the

415
00:20:20,919 --> 00:20:26,469
network latency or for example crash

416
00:20:24,190 --> 00:20:27,820
crash waterfall torrents like you want

417
00:20:26,469 --> 00:20:30,279
to make sure data was replicated

418
00:20:27,820 --> 00:20:32,349
properly that if one of the no crashes

419
00:20:30,279 --> 00:20:35,759
you can recover there probably is it

420
00:20:32,349 --> 00:20:39,549
possible to customize with justify just

421
00:20:35,759 --> 00:20:42,849
the hashing function to reflux yeah we

422
00:20:39,549 --> 00:20:44,979
actually have the fault tolerance of

423
00:20:42,849 --> 00:20:46,718
function function in this cache but we

424
00:20:44,979 --> 00:20:48,940
are we don't need to adjust the hash

425
00:20:46,719 --> 00:20:52,239
function we just need to using a kind of

426
00:20:48,940 --> 00:20:55,509
like chain replication towards the the

427
00:20:52,239 --> 00:20:57,129
switch notes and then to make it like we

428
00:20:55,509 --> 00:20:59,579
can guarantee like usually if your

429
00:20:57,129 --> 00:21:05,049
failure like one two nodes are fine I

430
00:20:59,579 --> 00:21:07,110
see yeah any questions okay thank you

431
00:21:05,049 --> 00:21:10,280
thank you

432
00:21:07,110 --> 00:21:10,280
[Applause]

