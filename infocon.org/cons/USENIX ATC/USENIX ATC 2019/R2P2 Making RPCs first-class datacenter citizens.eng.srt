1
00:00:10,080 --> 00:00:15,300
hi everyone my name is Marius and today

2
00:00:13,410 --> 00:00:17,790
I'm going to present to you our work on

3
00:00:15,300 --> 00:00:20,340
RTP to a Transfer Protocol for data

4
00:00:17,790 --> 00:00:22,710
center pieces this is work done with my

5
00:00:20,340 --> 00:00:24,750
colleagues Georg Hadrian and George and

6
00:00:22,710 --> 00:00:28,169
Yanis and our advisor at Bunyan in

7
00:00:24,750 --> 00:00:29,880
epatha so if you take a look inside the

8
00:00:28,169 --> 00:00:31,770
modern data center more or less you will

9
00:00:29,880 --> 00:00:33,420
see something like the following the

10
00:00:31,770 --> 00:00:36,360
networking infrastructure is organized

11
00:00:33,420 --> 00:00:38,430
in a closed topology with links of 10 40

12
00:00:36,360 --> 00:00:39,780
or even hundred gigs and typically

13
00:00:38,430 --> 00:00:42,930
round-trip times are just a handful of

14
00:00:39,780 --> 00:00:44,609
microseconds also a network

15
00:00:42,930 --> 00:00:46,320
programmability becomes more and more

16
00:00:44,609 --> 00:00:48,210
commonplace with programming languages

17
00:00:46,320 --> 00:00:51,570
such as before programmable switches and

18
00:00:48,210 --> 00:00:53,730
routers so on top of this infrastructure

19
00:00:51,570 --> 00:00:55,829
we have applications such as data stores

20
00:00:53,730 --> 00:00:57,510
or websites that communicating complex

21
00:00:55,829 --> 00:00:59,519
funding and fan-out Panther's other

22
00:00:57,510 --> 00:01:02,010
strict service level objectives for

23
00:00:59,519 --> 00:01:05,190
their tail latency the base of this

24
00:01:02,010 --> 00:01:07,350
communication is is remote procedure

25
00:01:05,190 --> 00:01:09,750
calls that can have very service times

26
00:01:07,350 --> 00:01:12,240
in this talk we will focus on

27
00:01:09,750 --> 00:01:13,920
microsecond scalar pcs and we will see

28
00:01:12,240 --> 00:01:17,699
how we can deliver them and schedule

29
00:01:13,920 --> 00:01:20,010
them inside the data center so a common

30
00:01:17,700 --> 00:01:21,690
design pattern is to multiplex our

31
00:01:20,010 --> 00:01:25,470
pieces on top of a reliable byte stream

32
00:01:21,690 --> 00:01:27,990
protocol such as TCP so TCP makes sure

33
00:01:25,470 --> 00:01:30,990
that packets are delivered in ordered p1

34
00:01:27,990 --> 00:01:33,630
p2 p3 and this packets can carry either

35
00:01:30,990 --> 00:01:37,369
entire requests such as well are r1 and

36
00:01:33,630 --> 00:01:40,619
r2 or pulse parcel requests as r3

37
00:01:37,370 --> 00:01:42,330
however the mismatch between those two

38
00:01:40,620 --> 00:01:44,580
abstractions can cause several issues

39
00:01:42,330 --> 00:01:44,940
and in this talk we will focus on two of

40
00:01:44,580 --> 00:01:47,130
them

41
00:01:44,940 --> 00:01:50,580
the first one is ordering instead of

42
00:01:47,130 --> 00:01:52,710
line blocking so TCP orders requests

43
00:01:50,580 --> 00:01:54,510
that are delivered on top of the same

44
00:01:52,710 --> 00:01:56,309
connection despite the fact that our

45
00:01:54,510 --> 00:01:59,970
pieces are independent and they can be

46
00:01:56,310 --> 00:02:02,340
served independently so in this case the

47
00:01:59,970 --> 00:02:05,520
long r1 request that can also take

48
00:02:02,340 --> 00:02:09,539
longer to be served will delay the

49
00:02:05,520 --> 00:02:11,459
shorter r2 and r3 requests also losing

50
00:02:09,538 --> 00:02:13,829
packets only makes things worse because

51
00:02:11,459 --> 00:02:16,890
then the TCP mechanisms will kick in and

52
00:02:13,830 --> 00:02:21,090
try to recover the lost packet at the

53
00:02:16,890 --> 00:02:22,709
same time the network is RPC agnostic

54
00:02:21,090 --> 00:02:23,880
when we delivered our pieces on top of

55
00:02:22,710 --> 00:02:25,950
TCP

56
00:02:23,880 --> 00:02:28,440
this is because tcp hides the RPC

57
00:02:25,950 --> 00:02:30,660
semantics so in this particular case you

58
00:02:28,440 --> 00:02:31,890
can see the RPC boundaries for r2 and r4

59
00:02:30,660 --> 00:02:34,410
are not aligned with the packet

60
00:02:31,890 --> 00:02:36,750
boundaries so whenever we want to

61
00:02:34,410 --> 00:02:38,730
implement some kind of policy the middle

62
00:02:36,750 --> 00:02:40,590
boxes have to either do the packet

63
00:02:38,730 --> 00:02:42,480
inspection or they have to terminate

64
00:02:40,590 --> 00:02:45,780
client connections and open new

65
00:02:42,480 --> 00:02:48,480
connections to the server's so in this

66
00:02:45,780 --> 00:02:50,760
talk we will focus on this mismatch and

67
00:02:48,480 --> 00:02:52,709
we design our to be to a transport

68
00:02:50,760 --> 00:02:54,929
protocol specifically for our pcs that

69
00:02:52,710 --> 00:02:57,990
exposes the RPC abstraction to the

70
00:02:54,930 --> 00:03:01,020
network and enables a network policy

71
00:02:57,990 --> 00:03:03,960
enforcement using this new abstraction

72
00:03:01,020 --> 00:03:07,470
we implement efficient in network RPC

73
00:03:03,960 --> 00:03:11,040
load balancing so we introduce her to be

74
00:03:07,470 --> 00:03:13,080
to a request response per protocol RTP

75
00:03:11,040 --> 00:03:15,690
to is a udp-based connectionless

76
00:03:13,080 --> 00:03:18,780
protocol that exposes the abstraction of

77
00:03:15,690 --> 00:03:20,280
a request and response pair so RTP -

78
00:03:18,780 --> 00:03:24,750
does not implement connections or

79
00:03:20,280 --> 00:03:26,310
individual messages also these requests

80
00:03:24,750 --> 00:03:28,620
and response pairs are independent so

81
00:03:26,310 --> 00:03:30,570
there is no protocol enforced ordering

82
00:03:28,620 --> 00:03:32,850
one client can have several outstanding

83
00:03:30,570 --> 00:03:35,700
requests so here the blue and the orange

84
00:03:32,850 --> 00:03:39,269
one and the replies can come back in any

85
00:03:35,700 --> 00:03:41,670
order the other thing is that with r2p -

86
00:03:39,270 --> 00:03:43,950
we don't have faith sharing so losing a

87
00:03:41,670 --> 00:03:45,420
packet will only affect the equivalent

88
00:03:43,950 --> 00:03:48,000
request and response pair so in this

89
00:03:45,420 --> 00:03:50,640
case if we lose the blue packet the

90
00:03:48,000 --> 00:03:54,060
orange reply can come back without any

91
00:03:50,640 --> 00:03:56,190
issues finally by doing that we allow

92
00:03:54,060 --> 00:03:58,740
clients to make better PC specific

93
00:03:56,190 --> 00:04:00,600
decisions so they can decide for time

94
00:03:58,740 --> 00:04:04,620
out timers or change between at least

95
00:04:00,600 --> 00:04:06,870
ones and at most once RPC semantics in

96
00:04:04,620 --> 00:04:09,180
r2p to its request and response pair is

97
00:04:06,870 --> 00:04:12,959
only identified by the source port and

98
00:04:09,180 --> 00:04:15,480
IP and a request ID set by the client by

99
00:04:12,959 --> 00:04:18,420
doing that we can break the traditional

100
00:04:15,480 --> 00:04:20,219
point-to-point RPC semantics so since

101
00:04:18,420 --> 00:04:22,260
the request destination can be different

102
00:04:20,220 --> 00:04:23,880
than the reply source clients can

103
00:04:22,260 --> 00:04:25,500
receive replies from servers that are

104
00:04:23,880 --> 00:04:28,350
different than the ones they initially

105
00:04:25,500 --> 00:04:33,330
send the request to also this allow us

106
00:04:28,350 --> 00:04:37,229
to make per request decisions regarding

107
00:04:33,330 --> 00:04:37,750
placement and scheduling so by exposing

108
00:04:37,229 --> 00:04:39,400
this in

109
00:04:37,750 --> 00:04:40,780
any requests and response pairs are to

110
00:04:39,400 --> 00:04:44,289
be two deals with the problem of

111
00:04:40,780 --> 00:04:46,450
ordering a head of line blocking then

112
00:04:44,290 --> 00:04:49,870
let's take a closer look at a packet

113
00:04:46,450 --> 00:04:53,289
exchange for for an RPC on top of RTP to

114
00:04:49,870 --> 00:04:54,970
imagine that we have a client and a

115
00:04:53,290 --> 00:04:56,680
bunch of servers and in between there is

116
00:04:54,970 --> 00:05:00,430
a middle box that implements some RPC

117
00:04:56,680 --> 00:05:02,740
policy so initially the client sends the

118
00:05:00,430 --> 00:05:04,630
first packet to the middle box at x0 and

119
00:05:02,740 --> 00:05:06,040
this packet can either carry the entire

120
00:05:04,630 --> 00:05:08,170
request if we're talking about the

121
00:05:06,040 --> 00:05:10,720
single packet request or just a part of

122
00:05:08,170 --> 00:05:12,640
the request then the middle box makes a

123
00:05:10,720 --> 00:05:14,860
decision and chooses a target server and

124
00:05:12,640 --> 00:05:17,020
in the case of a multi packet request

125
00:05:14,860 --> 00:05:19,150
the server notifies the client that is

126
00:05:17,020 --> 00:05:21,549
ready to serve this request and the

127
00:05:19,150 --> 00:05:24,700
client serves the rest of the request to

128
00:05:21,550 --> 00:05:26,290
the server bypassing the middle box then

129
00:05:24,700 --> 00:05:28,450
the server serves the request and

130
00:05:26,290 --> 00:05:31,740
streams the reply back to the client

131
00:05:28,450 --> 00:05:35,740
again bypassing the middle box

132
00:05:31,740 --> 00:05:37,990
finally our 2p2 implements application

133
00:05:35,740 --> 00:05:40,840
specific feedback feedback messages from

134
00:05:37,990 --> 00:05:43,090
the servers to the middle box so what we

135
00:05:40,840 --> 00:05:45,070
see here is that by exposing the RPC

136
00:05:43,090 --> 00:05:47,530
abstraction to the network we have

137
00:05:45,070 --> 00:05:50,050
requests in response aware middle boxes

138
00:05:47,530 --> 00:05:53,320
that can implement a network policy

139
00:05:50,050 --> 00:05:55,630
enforcement at line rate also with this

140
00:05:53,320 --> 00:05:57,610
design the middle box only participates

141
00:05:55,630 --> 00:05:59,440
in the policy enforcement and not in the

142
00:05:57,610 --> 00:06:02,920
request and reply streaming and as a

143
00:05:59,440 --> 00:06:05,740
result this is not an i/o bottleneck so

144
00:06:02,920 --> 00:06:08,470
by exposing this RPC abstraction to the

145
00:06:05,740 --> 00:06:10,390
network we make the network RPC aware

146
00:06:08,470 --> 00:06:15,090
and we can claim that our pieces are

147
00:06:10,390 --> 00:06:17,260
first-class citizens now so once we

148
00:06:15,090 --> 00:06:19,119
establish this abstraction we started

149
00:06:17,260 --> 00:06:21,280
thinking what can we do with this and

150
00:06:19,120 --> 00:06:24,850
the first use case that came in mind is

151
00:06:21,280 --> 00:06:27,010
a request level load balancing so let's

152
00:06:24,850 --> 00:06:28,450
take a look what can we do with requests

153
00:06:27,010 --> 00:06:30,400
how can we do request level load

154
00:06:28,450 --> 00:06:32,920
balancing with existing technologies so

155
00:06:30,400 --> 00:06:34,810
what you could do today is you could

156
00:06:32,920 --> 00:06:39,970
deploy something like a reverse proxy

157
00:06:34,810 --> 00:06:41,710
nginx or envoi etc and this reverse

158
00:06:39,970 --> 00:06:43,360
proxy would have to terminate client

159
00:06:41,710 --> 00:06:46,780
connections and open other connections

160
00:06:43,360 --> 00:06:49,450
to the server so we deployed the system

161
00:06:46,780 --> 00:06:51,580
so we have four back-end servers that

162
00:06:49,450 --> 00:06:53,919
run a synthetic service time with

163
00:06:51,580 --> 00:06:55,599
25 microsecond service and an expert

164
00:06:53,919 --> 00:06:58,568
comes from an exponential distribution

165
00:06:55,599 --> 00:07:00,580
and we put those servers behind an

166
00:06:58,569 --> 00:07:03,939
engine X reverse proxy configured it

167
00:07:00,580 --> 00:07:06,430
with the joint service queue policy and

168
00:07:03,939 --> 00:07:08,710
as we can see here despite the fact that

169
00:07:06,430 --> 00:07:11,919
we the optimal super that we could get

170
00:07:08,710 --> 00:07:14,680
is almost 2.5 million requests per

171
00:07:11,919 --> 00:07:17,859
second we cannot go above 200,000

172
00:07:14,680 --> 00:07:19,719
requests and this is because nginx is a

173
00:07:17,860 --> 00:07:23,139
bottleneck is an IO ball taken in this

174
00:07:19,719 --> 00:07:27,129
case so this is what we did we

175
00:07:23,139 --> 00:07:29,349
implemented our 2p to middleboxes

176
00:07:27,129 --> 00:07:31,930
one is software specifically on top of

177
00:07:29,349 --> 00:07:34,750
DP DK that only adds 5 microsecond

178
00:07:31,930 --> 00:07:38,349
end-to-end latency overhead and it can

179
00:07:34,750 --> 00:07:42,310
saturate a 10 gig leg with two CPU cores

180
00:07:38,349 --> 00:07:45,128
and one on the transfer top of a to Fino

181
00:07:42,310 --> 00:07:46,979
barefoot ASIC that only adds one

182
00:07:45,129 --> 00:07:51,669
microsecond latency overhead and we

183
00:07:46,979 --> 00:07:53,500
configured these this route routers to

184
00:07:51,669 --> 00:07:56,378
four requests with the simplest simplest

185
00:07:53,500 --> 00:07:59,379
policy with this random and as we can

186
00:07:56,379 --> 00:08:01,300
see here with the pink curve we can get

187
00:07:59,379 --> 00:08:06,849
significantly throughput benefits

188
00:08:01,300 --> 00:08:09,310
compared to the case of nginx so can we

189
00:08:06,849 --> 00:08:10,719
claim victory yes we can but the

190
00:08:09,310 --> 00:08:14,560
question is can we do better than that

191
00:08:10,719 --> 00:08:16,960
and fast forward we can even do better

192
00:08:14,560 --> 00:08:19,180
than that so we can we build a system

193
00:08:16,960 --> 00:08:21,878
that can actually deliver the gray line

194
00:08:19,180 --> 00:08:25,690
and let's take a look at how we got

195
00:08:21,879 --> 00:08:28,210
there but first let's step back and look

196
00:08:25,690 --> 00:08:30,610
at a closer look look - at load

197
00:08:28,210 --> 00:08:33,250
balancing and try to understand what's

198
00:08:30,610 --> 00:08:36,310
going on so there are two main design

199
00:08:33,250 --> 00:08:39,669
patterns regarding load balancing in the

200
00:08:36,309 --> 00:08:41,348
first one we have the distributed

201
00:08:39,669 --> 00:08:43,750
queuing so the dispatcher forwards

202
00:08:41,349 --> 00:08:45,970
requests to the servers and it's a

203
00:08:43,750 --> 00:08:47,620
server has its own queue the simplest

204
00:08:45,970 --> 00:08:49,300
policy to forward requests is just

205
00:08:47,620 --> 00:08:53,589
random and in this case we're talking

206
00:08:49,300 --> 00:08:55,359
about an independent mz1 systems on the

207
00:08:53,589 --> 00:08:58,720
other hand we have centralized queuing

208
00:08:55,360 --> 00:09:00,040
where a central dispatcher where all the

209
00:08:58,720 --> 00:09:02,260
requests get queued up in the center of

210
00:09:00,040 --> 00:09:04,060
this pattern and then servers pull from

211
00:09:02,260 --> 00:09:05,290
this dispatcher so in this case we're

212
00:09:04,060 --> 00:09:08,079
talking about an mg and

213
00:09:05,290 --> 00:09:10,000
system and from queuing theory which we

214
00:09:08,079 --> 00:09:11,739
know that there the performance

215
00:09:10,000 --> 00:09:14,949
difference between those two systems is

216
00:09:11,740 --> 00:09:17,410
clear and because the distributing

217
00:09:14,949 --> 00:09:19,209
suffers from transient load the balance

218
00:09:17,410 --> 00:09:21,399
is this the centralized queuing models

219
00:09:19,209 --> 00:09:24,310
perform way better in terms of teh

220
00:09:21,399 --> 00:09:27,550
latency however when we talk about

221
00:09:24,310 --> 00:09:29,589
scalability the story is different so

222
00:09:27,550 --> 00:09:31,870
distributed queuing scale scales way

223
00:09:29,589 --> 00:09:33,600
better because you can see at this as

224
00:09:31,870 --> 00:09:37,180
equivalent to layer for load balancing

225
00:09:33,600 --> 00:09:39,310
while a single queue system exposes the

226
00:09:37,180 --> 00:09:42,339
communication overhead and does not

227
00:09:39,310 --> 00:09:44,529
scale that well so the question we ask

228
00:09:42,339 --> 00:09:46,449
and we are trying to answer is can we

229
00:09:44,529 --> 00:09:49,839
implement RPC load balancing across

230
00:09:46,449 --> 00:09:52,209
multiple servers with single queue like

231
00:09:49,839 --> 00:09:55,720
performance while we can scale through

232
00:09:52,209 --> 00:09:58,989
Putin achieve low latency and to do that

233
00:09:55,720 --> 00:10:01,420
we introduced a new scheduling policy

234
00:09:58,990 --> 00:10:04,690
which is joined banded shortest fuse abs

235
00:10:01,420 --> 00:10:06,639
queue for shorter so CBS queue is a

236
00:10:04,690 --> 00:10:08,139
split queue model that splits queuing

237
00:10:06,639 --> 00:10:11,350
between a central part which is

238
00:10:08,139 --> 00:10:13,569
unbounded and several bonded queues one

239
00:10:11,350 --> 00:10:16,839
per server so in this case the bounded

240
00:10:13,569 --> 00:10:18,969
queue has size of two the idea here is

241
00:10:16,839 --> 00:10:21,459
that whenever the bounded queues are

242
00:10:18,970 --> 00:10:23,920
full requests will get queued up in the

243
00:10:21,459 --> 00:10:25,899
central part and their arsenal behind

244
00:10:23,920 --> 00:10:27,519
this policy is to delay the scheduling

245
00:10:25,899 --> 00:10:30,579
decision as much as possible in

246
00:10:27,519 --> 00:10:31,959
anticipation of better placement there

247
00:10:30,579 --> 00:10:34,120
is a noblesse trade of here between

248
00:10:31,959 --> 00:10:36,518
throughput and tail latency we can

249
00:10:34,120 --> 00:10:39,279
increase the number of n so that we can

250
00:10:36,519 --> 00:10:42,279
get more throughput but this can lead to

251
00:10:39,279 --> 00:10:44,740
bad placement and worst a latency if we

252
00:10:42,279 --> 00:10:46,480
reduce and we can improve teh latency

253
00:10:44,740 --> 00:10:48,250
but at the same time we expose the

254
00:10:46,480 --> 00:10:50,260
communication delay between the central

255
00:10:48,250 --> 00:10:53,670
candidacy the the bounded queues and

256
00:10:50,260 --> 00:10:56,380
this does not allow throughput to scale

257
00:10:53,670 --> 00:10:58,779
so now let's see how we can implement a

258
00:10:56,380 --> 00:11:00,339
beacon on top of RTP - and for that we

259
00:10:58,779 --> 00:11:03,339
will bring back the packet exchange that

260
00:11:00,339 --> 00:11:05,589
we saw before so we put the central

261
00:11:03,339 --> 00:11:07,959
queue at the middle box where the rack

262
00:11:05,589 --> 00:11:10,720
zeros get you'd up and we have bounded

263
00:11:07,959 --> 00:11:12,310
use one per server the middle box

264
00:11:10,720 --> 00:11:15,730
maintains the number of outstanding are

265
00:11:12,310 --> 00:11:18,640
pieces per server and then we use the

266
00:11:15,730 --> 00:11:20,560
feedback messages from the

267
00:11:18,640 --> 00:11:23,050
verse to the middle box to a signal

268
00:11:20,560 --> 00:11:25,209
determination of its RBC so now the

269
00:11:23,050 --> 00:11:26,620
question is how much better is JB askew

270
00:11:25,210 --> 00:11:29,170
compared to Durant the policy that we

271
00:11:26,620 --> 00:11:30,880
saw before and we aren't the same

272
00:11:29,170 --> 00:11:33,040
experiment but now because we do not

273
00:11:30,880 --> 00:11:34,390
have the a bottleneck we can go even

274
00:11:33,040 --> 00:11:36,849
lower in terms of service times so now

275
00:11:34,390 --> 00:11:39,310
we deploy the the same for service but

276
00:11:36,850 --> 00:11:41,860
with an average service time of 10

277
00:11:39,310 --> 00:11:43,030
microseconds and we plug the random

278
00:11:41,860 --> 00:11:46,480
policy that we saw before

279
00:11:43,030 --> 00:11:49,959
and we also plot the single EQ model as

280
00:11:46,480 --> 00:11:51,580
it as we get the results from a discrete

281
00:11:49,960 --> 00:11:53,950
event cingulate and this is the

282
00:11:51,580 --> 00:11:57,250
theoretically optimal model that result

283
00:11:53,950 --> 00:12:00,520
that we can get so first we deploy our

284
00:11:57,250 --> 00:12:02,920
systems our software approach with one

285
00:12:00,520 --> 00:12:05,230
outstanding request and here we can make

286
00:12:02,920 --> 00:12:07,599
two observations the first one is that

287
00:12:05,230 --> 00:12:11,050
we achieve lower tail latency so we're

288
00:12:07,600 --> 00:12:12,820
closer to to the central cue model but

289
00:12:11,050 --> 00:12:15,609
we cannot scale throughput because there

290
00:12:12,820 --> 00:12:16,840
is not enough concurrency so let's

291
00:12:15,610 --> 00:12:19,060
increase the number of outstanding

292
00:12:16,840 --> 00:12:21,190
requests we increase the number of

293
00:12:19,060 --> 00:12:23,260
outstanding requests to 5 and what we

294
00:12:21,190 --> 00:12:25,390
see here is that we can get the maximum

295
00:12:23,260 --> 00:12:28,780
throughput as it's predicted by by

296
00:12:25,390 --> 00:12:32,319
queueing theory under the under the tail

297
00:12:28,780 --> 00:12:35,650
latency SLO then we also deploy our

298
00:12:32,320 --> 00:12:37,000
hardware uploads based on p4 and because

299
00:12:35,650 --> 00:12:38,439
the performance difference between the

300
00:12:37,000 --> 00:12:40,210
software and hardware approach again

301
00:12:38,440 --> 00:12:43,090
here we can make two observations the

302
00:12:40,210 --> 00:12:45,430
first one is that we need less out

303
00:12:43,090 --> 00:12:47,470
sounding requests per server because the

304
00:12:45,430 --> 00:12:49,270
communication overhead is lower so with

305
00:12:47,470 --> 00:12:51,670
3 outs on a request here we can get the

306
00:12:49,270 --> 00:12:54,250
same maximum throughput the next

307
00:12:51,670 --> 00:12:55,959
operation we can make is about the

308
00:12:54,250 --> 00:12:58,420
impact of the number of of outstanding

309
00:12:55,960 --> 00:13:00,820
requests to tail latency so a lower

310
00:12:58,420 --> 00:13:02,530
number of n leads to better tail latency

311
00:13:00,820 --> 00:13:04,210
results because we can make better

312
00:13:02,530 --> 00:13:07,390
placement decisions and you can see the

313
00:13:04,210 --> 00:13:09,940
difference between the two curves okay

314
00:13:07,390 --> 00:13:12,430
so so far I've only talked about load

315
00:13:09,940 --> 00:13:14,740
balancing and the question that you

316
00:13:12,430 --> 00:13:16,510
might ask now is what kind of other

317
00:13:14,740 --> 00:13:21,940
policies can we implement the top of our

318
00:13:16,510 --> 00:13:23,830
TP 2 so R 2 P 2 supports and any kind of

319
00:13:21,940 --> 00:13:25,120
application specific policy and can be

320
00:13:23,830 --> 00:13:27,250
extended with new ones

321
00:13:25,120 --> 00:13:29,230
so in the r2p together we have

322
00:13:27,250 --> 00:13:32,499
specifically one policy field that can

323
00:13:29,230 --> 00:13:33,759
be interpreted by the router based on

324
00:13:32,499 --> 00:13:36,879
the policy that we would like to

325
00:13:33,759 --> 00:13:39,279
implement specifically we only plummeted

326
00:13:36,879 --> 00:13:41,649
two policies are out any and route fixed

327
00:13:39,279 --> 00:13:44,169
so in drought any the middle box is able

328
00:13:41,649 --> 00:13:46,779
to choose any of the target servers

329
00:13:44,169 --> 00:13:49,199
while in road fix the middle box has to

330
00:13:46,779 --> 00:13:52,509
forward the request to one specific

331
00:13:49,199 --> 00:13:55,748
predetermined target ideas for

332
00:13:52,509 --> 00:13:57,639
alternative policies are sticky so that

333
00:13:55,749 --> 00:13:59,529
we can implement some kind of sticky

334
00:13:57,639 --> 00:14:01,179
consistency where we request from

335
00:13:59,529 --> 00:14:04,269
certain clients only go to certain

336
00:14:01,179 --> 00:14:06,039
servers or hashing in order to implement

337
00:14:04,269 --> 00:14:08,529
some kind of sharding where the middle

338
00:14:06,039 --> 00:14:10,659
box can has parts of the request and

339
00:14:08,529 --> 00:14:15,009
forward the request to the equivalent

340
00:14:10,659 --> 00:14:17,769
server so we used the this policy field

341
00:14:15,009 --> 00:14:19,929
in our next experiment based on Redis so

342
00:14:17,769 --> 00:14:21,339
ready season in memory key value store

343
00:14:19,929 --> 00:14:23,949
that supports must master-slave

344
00:14:21,339 --> 00:14:26,259
replication and in this scheme set

345
00:14:23,949 --> 00:14:28,748
requests only go to the master while get

346
00:14:26,259 --> 00:14:31,569
requests can be load balanced so we use

347
00:14:28,749 --> 00:14:34,169
the policy field to do that and the

348
00:14:31,569 --> 00:14:37,959
middle box only forward set requests to

349
00:14:34,169 --> 00:14:40,559
one of two our master and load balances

350
00:14:37,959 --> 00:14:43,419
the get request across our three slaves

351
00:14:40,559 --> 00:14:45,759
we run the Facebook user workload then

352
00:14:43,419 --> 00:14:48,789
we compare with the vanilla Redis

353
00:14:45,759 --> 00:14:51,609
implementation on top of TCP so the

354
00:14:48,789 --> 00:14:54,220
first observation we make here is that

355
00:14:51,609 --> 00:14:56,769
even with the random policy RTP -

356
00:14:54,220 --> 00:14:58,869
because of the dibidibidi que based

357
00:14:56,769 --> 00:15:02,289
implementation and the linear transfer

358
00:14:58,869 --> 00:15:06,429
protocol achieves 5.3 better throughput

359
00:15:02,289 --> 00:15:09,039
and we also have some benefits because

360
00:15:06,429 --> 00:15:12,519
of scheduling when we deploy the jbs

361
00:15:09,039 --> 00:15:15,039
queue policy then we increase the

362
00:15:12,519 --> 00:15:17,649
percentage of writes to from point two

363
00:15:15,039 --> 00:15:19,449
to two percent and we see that the

364
00:15:17,649 --> 00:15:22,359
benefits because of scheduling become

365
00:15:19,449 --> 00:15:24,219
more significant and this is because by

366
00:15:22,359 --> 00:15:25,749
increasing the percentage of writes we

367
00:15:24,220 --> 00:15:29,679
increase the service time by the ability

368
00:15:25,749 --> 00:15:31,809
that our servers have so to conclude I

369
00:15:29,679 --> 00:15:34,598
presented our to be to a transfer

370
00:15:31,809 --> 00:15:36,789
protocol specifically for our pcs that

371
00:15:34,599 --> 00:15:39,249
exposes the RPC abstraction to the

372
00:15:36,789 --> 00:15:41,649
network and enables in network policy

373
00:15:39,249 --> 00:15:44,739
enforcement by using this new

374
00:15:41,649 --> 00:15:46,269
abstraction we implemented efficient RPC

375
00:15:44,739 --> 00:15:48,910
level load balancing

376
00:15:46,269 --> 00:15:51,100
in the network both on the software

377
00:15:48,910 --> 00:15:54,790
middle box and a hardware programmable

378
00:15:51,100 --> 00:15:58,119
sweets and we also implemented a novel

379
00:15:54,790 --> 00:16:00,069
jbs cue scheduling policy finally we saw

380
00:15:58,119 --> 00:16:03,730
how we can extend our to be to to

381
00:16:00,069 --> 00:16:05,920
support more in network policies our 2 P

382
00:16:03,730 --> 00:16:08,920
2 is open source and we encourage you to

383
00:16:05,920 --> 00:16:11,229
fork it and use it and thank you very

384
00:16:08,920 --> 00:16:18,368
much for your attention and I'm happy to

385
00:16:11,230 --> 00:16:20,800
take questions all right we have four

386
00:16:18,369 --> 00:16:22,839
minutes for questions if you would as a

387
00:16:20,800 --> 00:16:23,889
courtesy to the speaker say who you are

388
00:16:22,839 --> 00:16:27,040
and where you're from

389
00:16:23,889 --> 00:16:28,869
thank you hey I'm Rob Ritchie from the

390
00:16:27,040 --> 00:16:30,998
University of Utah one thing I noticed

391
00:16:28,869 --> 00:16:33,730
was that in the evaluation you did on

392
00:16:30,999 --> 00:16:35,829
Reedus you had a much larger queue size

393
00:16:33,730 --> 00:16:37,689
of 20 right and so I assume there's

394
00:16:35,829 --> 00:16:40,628
something about the application that

395
00:16:37,689 --> 00:16:41,559
influences what an appropriate queue

396
00:16:40,629 --> 00:16:44,170
size is and I wonder if you could

397
00:16:41,559 --> 00:16:47,920
comment on that yeah that's a that's a

398
00:16:44,170 --> 00:16:49,809
great question thank you so the number

399
00:16:47,920 --> 00:16:52,779
of outstanding requests that we want to

400
00:16:49,809 --> 00:16:55,509
have is highly dependent on the service

401
00:16:52,779 --> 00:16:57,610
time that we have on the server so we

402
00:16:55,509 --> 00:16:59,769
want to have enough returning requests

403
00:16:57,610 --> 00:17:03,819
so that we hide the communication

404
00:16:59,769 --> 00:17:05,829
latency between the middle box and the

405
00:17:03,819 --> 00:17:08,589
server so you can imagine that if you

406
00:17:05,829 --> 00:17:11,079
have low service times you need more

407
00:17:08,589 --> 00:17:12,908
outstanding requests also if you have

408
00:17:11,079 --> 00:17:15,220
high variability in your service times

409
00:17:12,909 --> 00:17:18,130
again you have you need to have more

410
00:17:15,220 --> 00:17:20,439
outs on the request because there will

411
00:17:18,130 --> 00:17:23,110
be a gap between very fast requests and

412
00:17:20,439 --> 00:17:27,250
very slow requests so when we're talking

413
00:17:23,109 --> 00:17:30,789
about red is we have micro one or even

414
00:17:27,250 --> 00:17:33,390
sub micro second service times and we

415
00:17:30,789 --> 00:17:36,850
also have some sets that are way slower

416
00:17:33,390 --> 00:17:39,100
so in this case we had to increase and

417
00:17:36,850 --> 00:17:42,039
also the communication latency between

418
00:17:39,100 --> 00:17:44,949
our software router and and the server

419
00:17:42,039 --> 00:17:46,809
is about 12 to 15 microseconds so

420
00:17:44,950 --> 00:17:48,159
because of the service time and the

421
00:17:46,809 --> 00:17:49,510
service time variability we had to

422
00:17:48,159 --> 00:17:54,580
increase the number of our tiny requests

423
00:17:49,510 --> 00:17:55,990
to get the throughput I am Xin from MIT

424
00:17:54,580 --> 00:17:58,090
very good how things on

425
00:17:55,990 --> 00:17:59,770
so I'm just wondering looks like you

426
00:17:58,090 --> 00:18:01,959
assuming like clients and server are

427
00:17:59,770 --> 00:18:03,610
connecting use a single switch belong in

428
00:18:01,960 --> 00:18:05,890
the reality I mean they're actually on

429
00:18:03,610 --> 00:18:08,139
many switch in them so basically you are

430
00:18:05,890 --> 00:18:10,360
trying to make a distributed decision

431
00:18:08,140 --> 00:18:13,600
rather than like a centralized decision

432
00:18:10,360 --> 00:18:15,399
right so what do you think about this so

433
00:18:13,600 --> 00:18:17,949
can you repeat the question if you

434
00:18:15,400 --> 00:18:19,870
assume that the clients can it's like in

435
00:18:17,950 --> 00:18:21,400
Lera lads like clients and in the

436
00:18:19,870 --> 00:18:23,080
datacenter setting like clients and

437
00:18:21,400 --> 00:18:25,570
servers our client can use many switches

438
00:18:23,080 --> 00:18:27,730
so basically where to put the

439
00:18:25,570 --> 00:18:29,320
information among the switches and Nuku

440
00:18:27,730 --> 00:18:30,100
is would be responsible to make this

441
00:18:29,320 --> 00:18:32,200
decision ready

442
00:18:30,100 --> 00:18:34,090
it looks like pamita in the realm you

443
00:18:32,200 --> 00:18:36,010
basically need to make a distributed

444
00:18:34,090 --> 00:18:41,199
decision right rather than a centralized

445
00:18:36,010 --> 00:18:43,210
decision okay so the the design choices

446
00:18:41,200 --> 00:18:46,090
that we made here is that we wanted to

447
00:18:43,210 --> 00:18:48,520
centralize this decision making point so

448
00:18:46,090 --> 00:18:51,699
that the suites or the the middle box

449
00:18:48,520 --> 00:18:54,430
has full visibility over what's going on

450
00:18:51,700 --> 00:18:57,220
at the servers this would be way harder

451
00:18:54,430 --> 00:18:59,320
to do if you had to make a distribute

452
00:18:57,220 --> 00:19:02,170
the decision so the assumption we make

453
00:18:59,320 --> 00:19:05,080
here is that each server belongs to one

454
00:19:02,170 --> 00:19:07,960
middle box and for a wider

455
00:19:05,080 --> 00:19:10,300
infrastructure you just deploy different

456
00:19:07,960 --> 00:19:13,350
middle boxes but these middle boxes

457
00:19:10,300 --> 00:19:16,120
should not share the servers okay thanks

458
00:19:13,350 --> 00:19:18,520
all right our next speaker come up and

459
00:19:16,120 --> 00:19:20,469
start getting ready well we have this

460
00:19:18,520 --> 00:19:22,150
last question Muhammad had AIT from

461
00:19:20,470 --> 00:19:26,380
University of Rochester very interesting

462
00:19:22,150 --> 00:19:29,320
work I was wondering if so it seems like

463
00:19:26,380 --> 00:19:32,920
you have to track the basically all

464
00:19:29,320 --> 00:19:35,230
in-flight are pcs in your middle box to

465
00:19:32,920 --> 00:19:37,930
be able to make these decisions if and

466
00:19:35,230 --> 00:19:40,300
this is a lot of state and if that is

467
00:19:37,930 --> 00:19:42,150
you you have identified any scalability

468
00:19:40,300 --> 00:19:46,230
issue with that and what's the maximum

469
00:19:42,150 --> 00:19:50,670
throughput that you can get out of your

470
00:19:46,230 --> 00:19:53,140
implementation in the middle box

471
00:19:50,670 --> 00:19:56,020
so the only state that we need to track

472
00:19:53,140 --> 00:20:00,130
is the number of outstanding requests

473
00:19:56,020 --> 00:20:02,980
very q so it's only one number for the

474
00:20:00,130 --> 00:20:05,650
number of hues that the middle box is

475
00:20:02,980 --> 00:20:08,320
responsible for in this case we only had

476
00:20:05,650 --> 00:20:11,110
64 independent Q's you know for our

477
00:20:08,320 --> 00:20:13,830
experiments so this this is just 64

478
00:20:11,110 --> 00:20:16,629
independent registers we didn't try with

479
00:20:13,830 --> 00:20:18,669
more cues but based on the switch

480
00:20:16,630 --> 00:20:20,710
characteristics and the memory that's

481
00:20:18,670 --> 00:20:23,010
available there this couldn't be this

482
00:20:20,710 --> 00:20:25,420
could scale to even larger numbers and

483
00:20:23,010 --> 00:20:26,830
for practical purposes I don't think

484
00:20:25,420 --> 00:20:28,480
that this would be an issue and the

485
00:20:26,830 --> 00:20:31,270
maximum throughput that you got out of

486
00:20:28,480 --> 00:20:33,340
your implementation sorry the throughput

487
00:20:31,270 --> 00:20:36,430
that you got out of your implementation

488
00:20:33,340 --> 00:20:38,530
how many are pieces per second so the

489
00:20:36,430 --> 00:20:41,350
throughput of our implementation was was

490
00:20:38,530 --> 00:20:44,710
bottleneck by the server so we didn't

491
00:20:41,350 --> 00:20:47,219
want it to have a bottleneck by the

492
00:20:44,710 --> 00:20:49,930
switch the only body that we had at the

493
00:20:47,220 --> 00:20:53,230
not at the software middle box was

494
00:20:49,930 --> 00:20:55,450
because of i/o ops so we saturated our

495
00:20:53,230 --> 00:20:58,270
10g links that we had available with

496
00:20:55,450 --> 00:20:59,340
tiny packets thank you all right let's

497
00:20:58,270 --> 00:21:04,280
thank our speaker again

498
00:20:59,340 --> 00:21:04,280
[Applause]

