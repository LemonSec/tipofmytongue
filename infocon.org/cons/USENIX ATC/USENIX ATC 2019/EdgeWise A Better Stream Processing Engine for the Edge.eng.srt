1
00:00:10,780 --> 00:00:16,539
hello everyone my name is chanel foo our

2
00:00:14,139 --> 00:00:19,119
work is edgewise a better stream

3
00:00:16,539 --> 00:00:22,750
processing engine for the edge this work

4
00:00:19,119 --> 00:00:25,210
is from Regina attack Internet of scenes

5
00:00:22,750 --> 00:00:27,309
applications are growing rapidly

6
00:00:25,210 --> 00:00:29,980
generally speaking Internet of the

7
00:00:27,309 --> 00:00:32,980
assistants contains scenes gateways in

8
00:00:29,980 --> 00:00:35,950
the cloud sins could be either sensors

9
00:00:32,980 --> 00:00:38,339
and actuators for example as a smart

10
00:00:35,950 --> 00:00:41,230
city the sensor could be either

11
00:00:38,340 --> 00:00:42,370
temperature sensors and the actuators

12
00:00:41,230 --> 00:00:45,940
could be Springer's

13
00:00:42,370 --> 00:00:52,019
and it is a similar for a smart hospital

14
00:00:45,940 --> 00:00:55,059
here at this moment IOT system rely on

15
00:00:52,020 --> 00:00:57,399
rely on the cloud to get data from the

16
00:00:55,060 --> 00:00:58,450
sensor processing the data and trigger

17
00:00:57,399 --> 00:01:01,660
the actuators

18
00:00:58,450 --> 00:01:03,640
however the air itself the gateways

19
00:01:01,660 --> 00:01:06,280
themselves they are powerful enough to

20
00:01:03,640 --> 00:01:08,799
have local data stream process in there

21
00:01:06,280 --> 00:01:12,670
and stream processing is well suited for

22
00:01:08,799 --> 00:01:15,280
the IOT edge since sensors may generate

23
00:01:12,670 --> 00:01:17,170
continues of data streams that often

24
00:01:15,280 --> 00:01:21,909
must be processed in a timely fashion

25
00:01:17,170 --> 00:01:23,890
and let me introduce our H model here

26
00:01:21,909 --> 00:01:26,740
there exist different edge model and

27
00:01:23,890 --> 00:01:29,500
this edge model we used in our paper so

28
00:01:26,740 --> 00:01:32,259
we view the edge as a collection of

29
00:01:29,500 --> 00:01:36,549
distributors gateways for the hardware

30
00:01:32,260 --> 00:01:38,890
we assumed the gateway has limited

31
00:01:36,549 --> 00:01:42,159
resources compared with the cloud

32
00:01:38,890 --> 00:01:44,890
service but it can afford reasonable

33
00:01:42,159 --> 00:01:47,020
complex operations so for example we use

34
00:01:44,890 --> 00:01:50,530
the raspberry PI's as the gateway in our

35
00:01:47,020 --> 00:01:53,530
paper and second the gateways should be

36
00:01:50,530 --> 00:01:55,420
well connected and for the application

37
00:01:53,530 --> 00:01:58,210
the application should be reasonable

38
00:01:55,420 --> 00:02:02,920
complex for example the phone bills from

39
00:01:58,210 --> 00:02:05,530
Microsoft it used the Gateway to collect

40
00:02:02,920 --> 00:02:07,960
data from the sensors and and have a

41
00:02:05,530 --> 00:02:10,810
summary before sending the to the cloud

42
00:02:07,960 --> 00:02:13,989
and also it use gateways to process on

43
00:02:10,810 --> 00:02:17,680
time-sensitive local data data

44
00:02:13,989 --> 00:02:20,860
processing at the edge and we live

45
00:02:17,680 --> 00:02:22,640
through for unique edge stream

46
00:02:20,860 --> 00:02:26,000
processing requirements here

47
00:02:22,640 --> 00:02:30,200
first multiplex since we have limited

48
00:02:26,000 --> 00:02:34,040
resources at the edge second low latency

49
00:02:30,200 --> 00:02:37,250
since our edge has the advantage of that

50
00:02:34,040 --> 00:02:40,400
locality and no bad pressure because of

51
00:02:37,250 --> 00:02:43,090
the triggers latency and storage issue

52
00:02:40,400 --> 00:02:46,610
we will revisit later in the slides and

53
00:02:43,090 --> 00:02:49,130
scalable of course we can think of if we

54
00:02:46,610 --> 00:02:54,170
have millions of sensors in a small city

55
00:02:49,130 --> 00:02:56,600
and stream processing use a data flow

56
00:02:54,170 --> 00:02:59,480
program model and here is an example is

57
00:02:56,600 --> 00:03:01,850
actually the data flows France and

58
00:02:59,480 --> 00:03:04,970
through the topology France all these

59
00:03:01,850 --> 00:03:07,160
two things and a topology actually at

60
00:03:04,970 --> 00:03:10,100
that and they have three kinds of nodes

61
00:03:07,160 --> 00:03:12,709
wine sauce sauce could be a sensor the

62
00:03:10,100 --> 00:03:15,890
sink could be either a curator or a

63
00:03:12,709 --> 00:03:18,890
master EQ to the cloud and the operation

64
00:03:15,890 --> 00:03:21,769
could perform arbitrary computation

65
00:03:18,890 --> 00:03:25,220
defined by the programmer and edge in

66
00:03:21,770 --> 00:03:28,459
the topology is just data flow and after

67
00:03:25,220 --> 00:03:30,739
defined the topology here the

68
00:03:28,459 --> 00:03:33,380
programming is to describe the number of

69
00:03:30,739 --> 00:03:35,840
instances for each operations when they

70
00:03:33,380 --> 00:03:39,500
want to do the deployment and this makes

71
00:03:35,840 --> 00:03:43,730
it friendly to the scalability and let's

72
00:03:39,500 --> 00:03:45,739
introduce the runtime system here we

73
00:03:43,730 --> 00:03:48,320
have a list of modern stream processing

74
00:03:45,739 --> 00:03:50,780
engines for the cloud here we have a

75
00:03:48,320 --> 00:03:54,530
purge storm a party fling and the

76
00:03:50,780 --> 00:03:56,470
hitachi harem under the hood they all

77
00:03:54,530 --> 00:04:00,640
use the one worker per operation

78
00:03:56,470 --> 00:04:03,560
architecture basically they assign our

79
00:04:00,640 --> 00:04:06,230
workers and our queue to each operation

80
00:04:03,560 --> 00:04:09,290
and the queues are connected in a

81
00:04:06,230 --> 00:04:11,810
pipeline manner and it just read get the

82
00:04:09,290 --> 00:04:13,700
data frames on queue and the process the

83
00:04:11,810 --> 00:04:17,600
data and put the result to the

84
00:04:13,700 --> 00:04:20,060
destination queue so it has a back

85
00:04:17,600 --> 00:04:22,610
pressure mechanism so what is the back

86
00:04:20,060 --> 00:04:25,190
pressure mechanism it monitors the queue

87
00:04:22,610 --> 00:04:27,890
lens for each queue if either the queue

88
00:04:25,190 --> 00:04:30,680
length is a higher than a high-water

89
00:04:27,890 --> 00:04:35,550
mark it will stop getting data from the

90
00:04:30,680 --> 00:04:38,490
sink and then and tear of the cooler

91
00:04:35,550 --> 00:04:40,860
under a lower to Mach then he start

92
00:04:38,490 --> 00:04:42,889
putting data again so that pressure

93
00:04:40,860 --> 00:04:46,080
kills latency because it is thought

94
00:04:42,889 --> 00:04:48,750
getting data from the sauce and it's not

95
00:04:46,080 --> 00:04:51,300
friendly for storage because some the

96
00:04:48,750 --> 00:04:53,190
storage made and you need a storage to

97
00:04:51,300 --> 00:04:56,210
store the pending data which may be

98
00:04:53,190 --> 00:04:59,509
unavailable at the edge setting and

99
00:04:56,210 --> 00:05:01,549
here's the problem so problem is that

100
00:04:59,509 --> 00:05:03,900
existing one worker per operation

101
00:05:01,550 --> 00:05:07,680
architecture stream processing engines

102
00:05:03,900 --> 00:05:10,409
are not suitable for the edge setting it

103
00:05:07,680 --> 00:05:13,139
could it couldn't meet the requirement

104
00:05:10,410 --> 00:05:15,870
of multiplex those stream processing

105
00:05:13,139 --> 00:05:17,910
engines they are there are beautiful

106
00:05:15,870 --> 00:05:20,580
clouds so they assume cloud class

107
00:05:17,910 --> 00:05:24,660
resources and they rely on the operating

108
00:05:20,580 --> 00:05:27,300
system scheduler however in the edge we

109
00:05:24,660 --> 00:05:28,770
have limited resources here so means we

110
00:05:27,300 --> 00:05:31,530
may have a number of workers or

111
00:05:28,770 --> 00:05:34,409
operations larger or much rather than

112
00:05:31,530 --> 00:05:37,020
the number of CPU cores this leads to

113
00:05:34,410 --> 00:05:41,010
efficient see in the operating system

114
00:05:37,020 --> 00:05:44,549
scheduler and which makes multiplies

115
00:05:41,010 --> 00:05:46,530
challenging and if we have the low input

116
00:05:44,550 --> 00:05:49,080
rate such as that most of kills our

117
00:05:46,530 --> 00:05:52,469
enmity then that doesn't matter

118
00:05:49,080 --> 00:05:55,289
but once the system gets saturated we

119
00:05:52,470 --> 00:05:58,130
have higher in parade such that most or

120
00:05:55,289 --> 00:06:00,780
all kills contain data this will lead to

121
00:05:58,130 --> 00:06:03,270
scheduling inefficiency because

122
00:06:00,780 --> 00:06:05,849
operating system scheduling has no

123
00:06:03,270 --> 00:06:08,190
knowledge about the engine level he

124
00:06:05,849 --> 00:06:10,770
doesn't know whether the security for or

125
00:06:08,190 --> 00:06:14,010
whether such queue is empty so it will

126
00:06:10,770 --> 00:06:16,320
make unwise decisions to treat

127
00:06:14,010 --> 00:06:19,139
unnecessary back pressure killing the

128
00:06:16,320 --> 00:06:22,979
latency so I have we have an example

129
00:06:19,139 --> 00:06:25,979
here so suppose we have two operations -

130
00:06:22,979 --> 00:06:28,289
qs q whines the successor of the kill

131
00:06:25,979 --> 00:06:31,889
zero and we only have a single call

132
00:06:28,289 --> 00:06:33,900
since is a random OS scheduler so it may

133
00:06:31,889 --> 00:06:36,750
render schedule one for example it may

134
00:06:33,900 --> 00:06:39,090
schedule this read this green one first

135
00:06:36,750 --> 00:06:41,970
green date huh first and then get a

136
00:06:39,090 --> 00:06:44,310
diskette order to continue to schedule

137
00:06:41,970 --> 00:06:46,770
the black one and get back to the

138
00:06:44,310 --> 00:06:49,349
operation zero after finishing this red

139
00:06:46,770 --> 00:06:49,740
one the result of the read data were put

140
00:06:49,349 --> 00:06:52,620
into

141
00:06:49,740 --> 00:06:55,380
the tale of the q1 and get it back to

142
00:06:52,620 --> 00:06:59,639
the black one and at this moment maybe a

143
00:06:55,380 --> 00:07:02,520
red cube or gets into the q0 and get and

144
00:06:59,639 --> 00:07:05,250
in that this moment because the render

145
00:07:02,520 --> 00:07:07,080
schedule is just random they may just

146
00:07:05,250 --> 00:07:09,990
schedule their red one here and

147
00:07:07,080 --> 00:07:12,180
finishing it and try to put the result

148
00:07:09,990 --> 00:07:15,240
of the red data into the q1

149
00:07:12,180 --> 00:07:17,639
however the q1 now is a floor

150
00:07:15,240 --> 00:07:19,560
so check out unnecessary back pressure

151
00:07:17,639 --> 00:07:21,780
this kind of a back pressure could be

152
00:07:19,560 --> 00:07:23,970
avoided and actually the key scenes that

153
00:07:21,780 --> 00:07:27,809
I want to show here is that operating

154
00:07:23,970 --> 00:07:31,169
system schedule doesn't have any engine

155
00:07:27,810 --> 00:07:33,720
level knowledge so really leads to the

156
00:07:31,169 --> 00:07:37,229
scheduling inefficiency so here we

157
00:07:33,720 --> 00:07:40,050
proposed edgewise so um this is the

158
00:07:37,229 --> 00:07:42,870
runtime before and this is the edgewise

159
00:07:40,050 --> 00:07:45,090
so we have two key ideas the first one

160
00:07:42,870 --> 00:07:47,639
is that because the number of workers

161
00:07:45,090 --> 00:07:50,489
may be larger than the number of CPU

162
00:07:47,639 --> 00:07:53,969
cost and what about we have a fixed size

163
00:07:50,490 --> 00:07:55,979
worker for their second so we have an

164
00:07:53,969 --> 00:07:58,590
efficiency in the operating system

165
00:07:55,979 --> 00:08:00,659
scheduler then why not to have an engine

166
00:07:58,590 --> 00:08:03,448
level scheduler to make wiser choices

167
00:08:00,659 --> 00:08:05,580
here putting them together if we look at

168
00:08:03,449 --> 00:08:08,039
the figure here we have four operations

169
00:08:05,580 --> 00:08:10,289
here but if we only have the two calls

170
00:08:08,039 --> 00:08:12,990
then we will only have a two thread in

171
00:08:10,289 --> 00:08:15,750
the thread pool and it is the job of the

172
00:08:12,990 --> 00:08:17,880
of the scheduler to schedule to

173
00:08:15,750 --> 00:08:22,919
determine which operation should be

174
00:08:17,880 --> 00:08:24,599
scheduled at which moment so for the

175
00:08:22,919 --> 00:08:27,090
fixed size dreadful was that the number

176
00:08:24,599 --> 00:08:30,000
of worker equals to the number of course

177
00:08:27,090 --> 00:08:32,669
this help edge wise to support an

178
00:08:30,000 --> 00:08:35,339
accurate or porridge unlimited resources

179
00:08:32,669 --> 00:08:35,760
and reduce the overhead of a contending

180
00:08:35,339 --> 00:08:38,219
course

181
00:08:35,760 --> 00:08:41,700
this helps the edge wise to achieve the

182
00:08:38,219 --> 00:08:43,560
requirement of multiplayers and for the

183
00:08:41,700 --> 00:08:45,899
entry-level scheduler there is an

184
00:08:43,559 --> 00:08:49,520
interesting story here actually we have

185
00:08:45,899 --> 00:08:52,050
a lost lesson here the tape to the

186
00:08:49,520 --> 00:08:54,329
database community already studied the

187
00:08:52,050 --> 00:08:57,390
operation scheduling like more than ten

188
00:08:54,329 --> 00:08:59,670
years ago they proposed providing guided

189
00:08:57,390 --> 00:09:02,319
priority based algorithm in the

190
00:08:59,670 --> 00:09:05,620
semantics that multiple operations round

191
00:09:02,320 --> 00:09:09,730
a single worker for example the Connie

192
00:09:05,620 --> 00:09:12,040
program from the Rio DB 2003 proposal

193
00:09:09,730 --> 00:09:14,680
mean latency algorithm and to achieve

194
00:09:12,040 --> 00:09:16,509
lowest and to end latency for Tito

195
00:09:14,680 --> 00:09:19,060
porridge which if errors the later

196
00:09:16,509 --> 00:09:23,550
operations the inductor porridge and the

197
00:09:19,060 --> 00:09:27,069
talkback heart from where DB 2000 2004

198
00:09:23,550 --> 00:09:31,479
proposal my memory algorithm to achieve

199
00:09:27,069 --> 00:09:33,759
the lowest memory memory consumption

200
00:09:31,480 --> 00:09:36,670
which favors the faster of filters

201
00:09:33,759 --> 00:09:39,310
operations in the in the topology

202
00:09:36,670 --> 00:09:41,380
however those fundings are not applied

203
00:09:39,310 --> 00:09:44,109
to the stream processing when we design

204
00:09:41,380 --> 00:09:46,329
a modern stream processing engine so we

205
00:09:44,110 --> 00:09:48,370
argued that we should regain the benefit

206
00:09:46,329 --> 00:09:51,130
of the engine level operation scheduling

207
00:09:48,370 --> 00:09:56,110
when we desire the engine in the edge

208
00:09:51,130 --> 00:09:58,779
setting we also proposed a conjunction

209
00:09:56,110 --> 00:10:01,449
of well scheduler as used by the

210
00:09:58,779 --> 00:10:04,120
edgewise it is a profiling free date

211
00:10:01,449 --> 00:10:07,930
dynamic solution and the key idea is to

212
00:10:04,120 --> 00:10:09,940
try to balance the queue sizes to UM to

213
00:10:07,930 --> 00:10:12,370
avoid unnecessary back pressure and

214
00:10:09,940 --> 00:10:14,290
every time it made a decision you choose

215
00:10:12,370 --> 00:10:16,930
its the operation with the most

216
00:10:14,290 --> 00:10:20,019
dependent data to schedule first so

217
00:10:16,930 --> 00:10:24,239
that's the example we used before so at

218
00:10:20,019 --> 00:10:26,829
this moment q1 has a two data inside so

219
00:10:24,240 --> 00:10:29,649
it will get scheduled first and they

220
00:10:26,829 --> 00:10:32,050
continue and at this moment the red one

221
00:10:29,649 --> 00:10:34,360
comes in now the q0 has a more data than

222
00:10:32,050 --> 00:10:37,300
the than the q 1 then interval schedule

223
00:10:34,360 --> 00:10:39,850
q0 and put the result into the interlock

224
00:10:37,300 --> 00:10:41,949
you WA so by balancing the queue size we

225
00:10:39,850 --> 00:10:44,079
can alleviate the back pressure problem

226
00:10:41,949 --> 00:10:48,519
and that should lower latency when the

227
00:10:44,079 --> 00:10:50,949
system is saturated so we also did the

228
00:10:48,519 --> 00:10:54,639
performance analysis using queueing

229
00:10:50,949 --> 00:10:56,529
theory for our scheduler so to the best

230
00:10:54,639 --> 00:10:58,240
of our knowledge we are the first to

231
00:10:56,529 --> 00:11:00,430
apply the queueing theory to analyze

232
00:10:58,240 --> 00:11:03,180
this to improve the performance in the

233
00:11:00,430 --> 00:11:06,040
context of stream processing we have

234
00:11:03,180 --> 00:11:08,380
several conclusion here but I don't have

235
00:11:06,040 --> 00:11:10,540
time to list the mess here if you are

236
00:11:08,380 --> 00:11:13,490
interested in this one please read our

237
00:11:10,540 --> 00:11:17,748
paper for more details

238
00:11:13,490 --> 00:11:21,110
evaluation we implement edgewise on top

239
00:11:17,749 --> 00:11:24,920
of Apache stone we use the Raspberry Pi

240
00:11:21,110 --> 00:11:27,529
really as our gateway hardware we use a

241
00:11:24,920 --> 00:11:30,410
patch storm as our one worker per

242
00:11:27,529 --> 00:11:33,230
operation architecture baseline we also

243
00:11:30,410 --> 00:11:35,059
evaluate other schedulers like Rendon

244
00:11:33,230 --> 00:11:37,699
and the mean memory mean latency we

245
00:11:35,059 --> 00:11:40,540
mentioned before and for the experiment

246
00:11:37,699 --> 00:11:44,290
setup we focus on the single PI

247
00:11:40,540 --> 00:11:46,670
transition anyway we we also have a

248
00:11:44,290 --> 00:11:50,449
distributed setting you know in our

249
00:11:46,670 --> 00:11:52,069
paper and we set the pore size to four

250
00:11:50,449 --> 00:11:55,429
because we have a four call for the

251
00:11:52,069 --> 00:11:57,800
recuperated PI and we used our IOT

252
00:11:55,429 --> 00:12:01,069
benchmark as our benchmark this is a

253
00:11:57,800 --> 00:12:05,649
written real-time IOT stream processing

254
00:12:01,069 --> 00:12:08,719
benchmark for storm and and it is

255
00:12:05,649 --> 00:12:10,970
reasonably complex it can consist of

256
00:12:08,720 --> 00:12:13,129
four topology

257
00:12:10,970 --> 00:12:15,410
this is one of them if we look at the

258
00:12:13,129 --> 00:12:18,199
figure it can contain decision tree

259
00:12:15,410 --> 00:12:21,160
linear regression and window average

260
00:12:18,199 --> 00:12:24,290
error estimation this is complex

261
00:12:21,160 --> 00:12:26,240
reasonably complex and that and also

262
00:12:24,290 --> 00:12:28,790
this the only one I found for the

263
00:12:26,240 --> 00:12:30,439
benchmark reasonable complex and I think

264
00:12:28,790 --> 00:12:33,860
our community should have some more

265
00:12:30,439 --> 00:12:36,860
benchmarks like this and we use the

266
00:12:33,860 --> 00:12:38,240
metrics of throughput and latency we

267
00:12:36,860 --> 00:12:40,610
have more experiments like that

268
00:12:38,240 --> 00:12:43,009
distribute setting sensitive analysis

269
00:12:40,610 --> 00:12:45,980
throughput throughput breakdown in our

270
00:12:43,009 --> 00:12:48,769
paper but we only show some of in this

271
00:12:45,980 --> 00:12:51,230
talk so here the first one I want to

272
00:12:48,769 --> 00:12:54,259
show so this is a throughput latency

273
00:12:51,230 --> 00:12:56,389
performance so what we did is we keep

274
00:12:54,259 --> 00:12:58,339
increasing the incra rate to the

275
00:12:56,389 --> 00:13:01,040
topology to achieve higher in the

276
00:12:58,339 --> 00:13:04,160
highest repute and we measure the end to

277
00:13:01,040 --> 00:13:07,459
end the latency of the topology so as we

278
00:13:04,160 --> 00:13:11,509
can see when the imperator throughput is

279
00:13:07,459 --> 00:13:15,888
low here every I rented a good job

280
00:13:11,509 --> 00:13:19,429
because most of the Q's are empty but

281
00:13:15,889 --> 00:13:22,999
once the system gets saturated and just

282
00:13:19,429 --> 00:13:25,669
that's the place where the scheduler

283
00:13:22,999 --> 00:13:26,880
makes difference if we look at if we use

284
00:13:25,669 --> 00:13:29,490
the latency of

285
00:13:26,880 --> 00:13:32,760
hundred as a constant the storm can only

286
00:13:29,490 --> 00:13:36,150
achieve like 2600 of a throughput

287
00:13:32,760 --> 00:13:39,930
however the age wise can achieve 4400 of

288
00:13:36,150 --> 00:13:42,300
throughput and we further breakdown the

289
00:13:39,930 --> 00:13:43,920
latency entrances so what we did which

290
00:13:42,300 --> 00:13:46,709
was really different levels of

291
00:13:43,920 --> 00:13:49,410
throughput four hundred sixteen hundred

292
00:13:46,710 --> 00:13:52,530
and twenty eight hundred and we measured

293
00:13:49,410 --> 00:13:55,079
per operation latency and we further

294
00:13:52,530 --> 00:13:57,959
break down the per operation ladies and

295
00:13:55,080 --> 00:14:01,590
into the queuing waiting time scheduling

296
00:13:57,960 --> 00:14:03,120
time and the computing time so at the

297
00:14:01,590 --> 00:14:05,490
first guidance we can know that the

298
00:14:03,120 --> 00:14:07,860
majority of the per operation per

299
00:14:05,490 --> 00:14:09,930
operation latency is the queuing way

300
00:14:07,860 --> 00:14:10,470
containments that's why the scheduling

301
00:14:09,930 --> 00:14:15,239
matters

302
00:14:10,470 --> 00:14:17,880
and second the oh one the Oh whines the

303
00:14:15,240 --> 00:14:21,930
heaviest operation in this topology so

304
00:14:17,880 --> 00:14:24,630
what age wise did is to reduce a lot

305
00:14:21,930 --> 00:14:27,359
queueing waiting high from the heaviest

306
00:14:24,630 --> 00:14:30,750
operation and the slightly increase

307
00:14:27,360 --> 00:14:34,110
others lighter operations latency like

308
00:14:30,750 --> 00:14:37,200
the queue and Oh two three four and five

309
00:14:34,110 --> 00:14:39,960
so this is not a zero-sum game so what

310
00:14:37,200 --> 00:14:42,540
we did is we we decrease the heaviest

311
00:14:39,960 --> 00:14:46,910
ones latency and we slightly increase

312
00:14:42,540 --> 00:14:49,170
the increase our lighter one latency

313
00:14:46,910 --> 00:14:50,670
that's why the edge wise can achieve

314
00:14:49,170 --> 00:14:53,819
lower latency when the system is

315
00:14:50,670 --> 00:14:58,500
saturated and to support our conclusion

316
00:14:53,820 --> 00:15:01,530
in our queuing theory analysis and to

317
00:14:58,500 --> 00:15:03,720
conclude so we study existing stream

318
00:15:01,530 --> 00:15:06,720
processing engines and they discussed

319
00:15:03,720 --> 00:15:09,390
their limitations in the edge we propose

320
00:15:06,720 --> 00:15:12,030
edge wise which use a fixed size of

321
00:15:09,390 --> 00:15:14,370
thread poor and conjunction of where

322
00:15:12,030 --> 00:15:16,949
scheduler we found a lost lesson of

323
00:15:14,370 --> 00:15:19,110
operation scheduling we did a

324
00:15:16,950 --> 00:15:22,080
performance analysis of conjunction of

325
00:15:19,110 --> 00:15:24,660
where scheduler using queuing theory we

326
00:15:22,080 --> 00:15:28,040
achieved up to three times improvement

327
00:15:24,660 --> 00:15:30,600
in throughput while keeping latency low

328
00:15:28,040 --> 00:15:33,770
the last thing I want to share with you

329
00:15:30,600 --> 00:15:36,270
what I learned from this project is that

330
00:15:33,770 --> 00:15:39,540
sometimes the answers in system design

331
00:15:36,270 --> 00:15:45,870
lie not in the future but in the past

332
00:15:39,540 --> 00:15:47,699
thank you it's tank shinbei any

333
00:15:45,870 --> 00:15:55,290
questions and if you would please

334
00:15:47,699 --> 00:15:56,670
introduce yourself I mean thanks for

335
00:15:55,290 --> 00:15:59,219
that last lesson learned I like that

336
00:15:56,670 --> 00:16:00,540
very much so how do your operators

337
00:15:59,220 --> 00:16:02,370
communicate with each other do you have

338
00:16:00,540 --> 00:16:04,290
like a peer-to-peer communication

339
00:16:02,370 --> 00:16:07,709
between the operators so they use like a

340
00:16:04,290 --> 00:16:09,959
message broker or something - um so we

341
00:16:07,709 --> 00:16:12,899
focus on the engine note optimization

342
00:16:09,959 --> 00:16:15,660
first so all the kids are running in the

343
00:16:12,899 --> 00:16:19,610
JVM implemented by the Apache storm so

344
00:16:15,660 --> 00:16:23,969
we are using the in memory 24 here but

345
00:16:19,610 --> 00:16:26,759
if we if we look at the moment please

346
00:16:23,970 --> 00:16:29,100
that it should be setting here if we

347
00:16:26,759 --> 00:16:32,069
have internal communication they're

348
00:16:29,100 --> 00:16:33,779
using a few cubes okay and so have you

349
00:16:32,069 --> 00:16:35,819
considered that what happens when notes

350
00:16:33,779 --> 00:16:37,319
move around so mobility aspect so

351
00:16:35,819 --> 00:16:39,449
typically in these interesting areas

352
00:16:37,319 --> 00:16:41,219
things will resources will actually be

353
00:16:39,449 --> 00:16:42,689
mobile and move around and not be well

354
00:16:41,220 --> 00:16:44,670
connected have you considered that case

355
00:16:42,690 --> 00:16:46,380
we didn't caii our model we only

356
00:16:44,670 --> 00:16:48,089
consider we're connected okay - wait

357
00:16:46,380 --> 00:16:49,800
here okay now you're almost doing that

358
00:16:48,089 --> 00:16:51,029
kind of work are you are you interested

359
00:16:49,800 --> 00:16:53,819
in the end are you going to continue

360
00:16:51,029 --> 00:16:55,860
working on that the current motivation

361
00:16:53,819 --> 00:16:58,050
is wrong we have our smart building in

362
00:16:55,860 --> 00:16:59,819
the working attack so our model is based

363
00:16:58,050 --> 00:17:07,339
on that how do we consider that to work

364
00:16:59,819 --> 00:17:11,579
in the future okay any other questions

365
00:17:07,339 --> 00:17:13,230
so then I have a question so when you

366
00:17:11,579 --> 00:17:15,089
looked at the scheduling were you

367
00:17:13,230 --> 00:17:17,400
considering the fact that in the edge

368
00:17:15,089 --> 00:17:20,010
world you're going to have lots of

369
00:17:17,400 --> 00:17:21,659
heterogeneous kinds of devices and some

370
00:17:20,010 --> 00:17:23,609
of these devices you may schedule

371
00:17:21,659 --> 00:17:27,360
something on it and then it may fail or

372
00:17:23,609 --> 00:17:29,639
it may leave the leave the network okay

373
00:17:27,359 --> 00:17:31,918
say it again so a device on which your

374
00:17:29,640 --> 00:17:33,809
schedule may fail or it may leave the

375
00:17:31,919 --> 00:17:36,929
network have you considered either a

376
00:17:33,809 --> 00:17:38,879
bomb in this mode or no do you consider

377
00:17:36,929 --> 00:17:42,510
heterogeneity of the devices they are

378
00:17:38,880 --> 00:17:45,270
not all the same and for now no we only

379
00:17:42,510 --> 00:17:48,150
focus on the internal internal not

380
00:17:45,270 --> 00:17:51,290
multiple nodes optimization yet okay

381
00:17:48,150 --> 00:17:55,230
let's tank shinbei again

382
00:17:51,290 --> 00:17:55,230
[Applause]

