1
00:00:11,000 --> 00:00:16,470
hi everyone I'm Graham I'm going to talk

2
00:00:14,389 --> 00:00:17,880
about protocol aware recovery for

3
00:00:16,470 --> 00:00:22,098
storage this is joint work with many

4
00:00:17,880 --> 00:00:24,060
others hundred vases and Riaan trim c

5
00:00:22,099 --> 00:00:26,399
redundancy enables distributed storage

6
00:00:24,060 --> 00:00:27,418
systems to tolerate failures for example

7
00:00:26,399 --> 00:00:28,560
a piece of data is stored in Italy

8
00:00:27,419 --> 00:00:30,689
stored on several servers and this

9
00:00:28,560 --> 00:00:32,430
provides fault tolerance so even if a

10
00:00:30,689 --> 00:00:34,140
few servers crash or if they remain

11
00:00:32,430 --> 00:00:36,480
unreachable Duty Network failures the

12
00:00:34,140 --> 00:00:37,800
system as a whole is unaffected and so

13
00:00:36,480 --> 00:00:40,860
the data stored by the system will be

14
00:00:37,800 --> 00:00:42,599
available incorrect however in addition

15
00:00:40,860 --> 00:00:44,190
to system crashes in network partitions

16
00:00:42,600 --> 00:00:45,960
sometimes the data stored on the

17
00:00:44,190 --> 00:00:47,610
individual replicas could be faulty that

18
00:00:45,960 --> 00:00:48,930
is they could be corrupted or they could

19
00:00:47,610 --> 00:00:50,970
remain inaccessible because of some

20
00:00:48,930 --> 00:00:53,970
underlying disk problems we call such

21
00:00:50,970 --> 00:00:55,260
failure storage faults so now let's talk

22
00:00:53,970 --> 00:00:56,760
about how to recover a piece of data

23
00:00:55,260 --> 00:00:58,739
that is affected by a storage Fault in a

24
00:00:56,760 --> 00:01:00,479
distributed system so let's start with a

25
00:00:58,739 --> 00:01:03,030
very simple example this is a popular

26
00:01:00,479 --> 00:01:04,379
approach and a widely used one which is

27
00:01:03,030 --> 00:01:07,229
to delete the data on the faulty node

28
00:01:04,379 --> 00:01:08,700
and just restart the non this is popular

29
00:01:07,229 --> 00:01:11,159
as you can see in many developer forums

30
00:01:08,700 --> 00:01:12,420
for example in zookeeper developers say

31
00:01:11,159 --> 00:01:13,860
on a file corruption go ahead and clean

32
00:01:12,420 --> 00:01:15,030
the database delete all the files and

33
00:01:13,860 --> 00:01:17,909
just restart the node and you will be

34
00:01:15,030 --> 00:01:19,049
fine so this seems sort of ok so let's

35
00:01:17,909 --> 00:01:21,090
go ahead and try to apply this approach

36
00:01:19,049 --> 00:01:22,950
so what I'm going to do is I'm going to

37
00:01:21,090 --> 00:01:25,730
delete the corrupted data on this faulty

38
00:01:22,950 --> 00:01:27,990
node and restart this node and

39
00:01:25,730 --> 00:01:29,490
fortunately in this particular case the

40
00:01:27,990 --> 00:01:30,839
node is actually able to fix itself from

41
00:01:29,490 --> 00:01:33,860
the redundant copies in the other

42
00:01:30,840 --> 00:01:37,200
servers so this approach seems intuitive

43
00:01:33,860 --> 00:01:38,820
and it works right however the same

44
00:01:37,200 --> 00:01:41,400
approach sometimes can lead to

45
00:01:38,820 --> 00:01:43,050
surprisingly a global data loss the

46
00:01:41,400 --> 00:01:44,520
reason for this is for example take this

47
00:01:43,050 --> 00:01:46,200
case where the data was initially

48
00:01:44,520 --> 00:01:48,240
replicated to obeyed majority of servers

49
00:01:46,200 --> 00:01:50,520
in the first place now one copy gets

50
00:01:48,240 --> 00:01:52,350
corrupted because of a disk problem now

51
00:01:50,520 --> 00:01:53,548
according to this approach what I'm

52
00:01:52,350 --> 00:01:56,970
going to do is I'm going to delete this

53
00:01:53,549 --> 00:01:58,740
data and restart this node now at this

54
00:01:56,970 --> 00:02:00,658
point it's possible that the nodes that

55
00:01:58,740 --> 00:02:03,149
have the data could be operating slowly

56
00:02:00,659 --> 00:02:04,740
or they could have even crashed and it's

57
00:02:03,150 --> 00:02:06,570
furthermore it's possible that the nodes

58
00:02:04,740 --> 00:02:08,128
that I have no idea but the committed

59
00:02:06,570 --> 00:02:10,348
data in the system could form a majority

60
00:02:08,128 --> 00:02:11,760
and elect a leader among themselves so

61
00:02:10,348 --> 00:02:13,140
we now have a majority of nodes in the

62
00:02:11,760 --> 00:02:15,298
cluster which do not have any idea about

63
00:02:13,140 --> 00:02:17,670
the system which in the in the system

64
00:02:15,299 --> 00:02:20,430
and so it just means that the system has

65
00:02:17,670 --> 00:02:22,649
committed lost the committed data right

66
00:02:20,430 --> 00:02:24,150
the Cole reason why this kind of

67
00:02:22,650 --> 00:02:25,920
approaches can lead to global data loss

68
00:02:24,150 --> 00:02:27,599
is that the approach is kind of

69
00:02:25,920 --> 00:02:29,309
oblivious of the underlying protocols

70
00:02:27,599 --> 00:02:30,269
used by the distributed system in

71
00:02:29,310 --> 00:02:31,590
particular the

72
00:02:30,270 --> 00:02:32,940
rebuild a brush that I showed you in the

73
00:02:31,590 --> 00:02:35,310
previous approach in the previous light

74
00:02:32,940 --> 00:02:39,300
is obviously protocol used by the system

75
00:02:35,310 --> 00:02:40,590
to update the replicated data thus our

76
00:02:39,300 --> 00:02:42,270
proposal to correctly and quickly

77
00:02:40,590 --> 00:02:45,000
recover a piece of data in a distributed

78
00:02:42,270 --> 00:02:46,350
system is this the recovery approach

79
00:02:45,000 --> 00:02:48,210
should be carefully designed based on

80
00:02:46,350 --> 00:02:49,829
the the protocols used by the

81
00:02:48,210 --> 00:02:51,900
distributed system for example it should

82
00:02:49,830 --> 00:02:53,400
be cognizant of the fact such as is

83
00:02:51,900 --> 00:02:54,630
there a dedicated leader in the system

84
00:02:53,400 --> 00:02:56,880
and what are the constraints on which

85
00:02:54,630 --> 00:02:58,829
nodes can become the leader and so on we

86
00:02:56,880 --> 00:03:00,780
call such interrupted record we approach

87
00:02:58,830 --> 00:03:02,880
a protocol a very complete approach and

88
00:03:00,780 --> 00:03:04,110
in this work we focus on a special class

89
00:03:02,880 --> 00:03:06,180
of distributed systems that we call

90
00:03:04,110 --> 00:03:08,400
replicated state machines or are some

91
00:03:06,180 --> 00:03:12,000
systems examples include widely used

92
00:03:08,400 --> 00:03:13,470
system such as zookeeper so we have

93
00:03:12,000 --> 00:03:14,610
implemented something called corruption

94
00:03:13,470 --> 00:03:17,250
tolerant replication which is an

95
00:03:14,610 --> 00:03:19,080
implementation of a power protocol event

96
00:03:17,250 --> 00:03:20,970
recovery based approach for RSM systems

97
00:03:19,080 --> 00:03:22,500
we have implemented control in two

98
00:03:20,970 --> 00:03:24,720
different are some systems log cabin and

99
00:03:22,500 --> 00:03:26,880
zookeeper and through experiments we see

100
00:03:24,720 --> 00:03:28,230
that control offers safety and high

101
00:03:26,880 --> 00:03:30,480
availability in the presence of storage

102
00:03:28,230 --> 00:03:32,220
falls whereas the unmodified systems can

103
00:03:30,480 --> 00:03:34,260
lead to data loss or they can lead to

104
00:03:32,220 --> 00:03:36,540
unavailability and further these

105
00:03:34,260 --> 00:03:38,640
reliability improvements of control come

106
00:03:36,540 --> 00:03:41,730
at no or little performance for hits for

107
00:03:38,640 --> 00:03:44,609
example when you run on SSDs for these

108
00:03:41,730 --> 00:03:47,549
reliability improvements you you control

109
00:03:44,610 --> 00:03:48,720
imposes four percent overheads so that

110
00:03:47,550 --> 00:03:50,100
was the introduction to the draw and

111
00:03:48,720 --> 00:03:52,500
here is the outline from the rest of my

112
00:03:50,100 --> 00:03:53,940
talk alpha show the current approaches

113
00:03:52,500 --> 00:03:55,380
to handling storage falls then I'll

114
00:03:53,940 --> 00:03:56,910
describe our approach I'll show some

115
00:03:55,380 --> 00:03:59,670
evaluation results finally summarize and

116
00:03:56,910 --> 00:04:00,750
conclude so since we're going to talk

117
00:03:59,670 --> 00:04:03,149
about ourselves here is a brief

118
00:04:00,750 --> 00:04:05,400
background our ISM is just a paradigm to

119
00:04:03,150 --> 00:04:06,810
make programs more reliable the key idea

120
00:04:05,400 --> 00:04:09,209
is you take a program run it on several

121
00:04:06,810 --> 00:04:10,770
servers at the same time and if all the

122
00:04:09,209 --> 00:04:13,680
servers start up the same initial state

123
00:04:10,770 --> 00:04:15,690
and I apply the same sequence of inputs

124
00:04:13,680 --> 00:04:17,760
in the same order then essentially they

125
00:04:15,690 --> 00:04:19,168
will all produce the same outputs now to

126
00:04:17,760 --> 00:04:20,820
ensure this there is something called

127
00:04:19,168 --> 00:04:23,010
the consensus algorithm that runs as

128
00:04:20,820 --> 00:04:25,140
part of every replica and typical

129
00:04:23,010 --> 00:04:26,940
implementations of practical consensus

130
00:04:25,140 --> 00:04:28,680
protocols use something called the

131
00:04:26,940 --> 00:04:30,690
leader which is a designated replica

132
00:04:28,680 --> 00:04:32,850
which C realizes all the requests from

133
00:04:30,690 --> 00:04:35,610
the clients and it takes this C realized

134
00:04:32,850 --> 00:04:38,190
inputs e-cig stream and then replicates

135
00:04:35,610 --> 00:04:40,440
to the leaders and we say a command from

136
00:04:38,190 --> 00:04:41,969
a client is committed when it is

137
00:04:40,440 --> 00:04:43,680
accepted and persisted by a majority of

138
00:04:41,970 --> 00:04:45,509
servers in the system

139
00:04:43,680 --> 00:04:47,370
and ourselves provide this property that

140
00:04:45,509 --> 00:04:49,530
the system continues to operate from

141
00:04:47,370 --> 00:04:51,120
correctly and also remain available as

142
00:04:49,530 --> 00:04:54,388
far as a majority of servers the

143
00:04:51,120 --> 00:04:56,100
functional and because we are interested

144
00:04:54,389 --> 00:04:57,060
in dealing with storage corruptions we

145
00:04:56,100 --> 00:04:58,320
need to carefully understand the

146
00:04:57,060 --> 00:05:01,259
different persistence structures that

147
00:04:58,320 --> 00:05:02,759
the our systems maintain first the

148
00:05:01,259 --> 00:05:04,850
incoming commands from the clients was

149
00:05:02,759 --> 00:05:07,979
sold in a data structure called the log

150
00:05:04,850 --> 00:05:09,810
this is just like a contiguous log that

151
00:05:07,979 --> 00:05:12,630
keeps growing and it is part of every

152
00:05:09,810 --> 00:05:15,120
node in the system because the log could

153
00:05:12,630 --> 00:05:16,080
go an exhausted disk space because you

154
00:05:15,120 --> 00:05:17,669
can keep embedding to the log

155
00:05:16,080 --> 00:05:19,109
periodically you would want to take a

156
00:05:17,669 --> 00:05:20,609
snapshot of the in-memory state machine

157
00:05:19,110 --> 00:05:22,560
and then garbage collector log that's

158
00:05:20,610 --> 00:05:23,820
the snapshot and in addition to these

159
00:05:22,560 --> 00:05:25,320
main data structures there is something

160
00:05:23,820 --> 00:05:27,210
called the meta info which is very

161
00:05:25,320 --> 00:05:29,789
crucial and has quite an information

162
00:05:27,210 --> 00:05:30,989
about for example voting and so on this

163
00:05:29,789 --> 00:05:33,060
is kind of a small data structure that's

164
00:05:30,990 --> 00:05:34,139
part of every node now all of these

165
00:05:33,060 --> 00:05:36,449
critical data structures could be

166
00:05:34,139 --> 00:05:38,010
affected by storage Falls and depending

167
00:05:36,449 --> 00:05:39,960
upon the file system that's in use you

168
00:05:38,010 --> 00:05:41,490
could either get corruptions or you

169
00:05:39,960 --> 00:05:44,250
could get errors when you access these

170
00:05:41,490 --> 00:05:45,000
data structures so with that now let's

171
00:05:44,250 --> 00:05:46,740
talk about what are the current

172
00:05:45,000 --> 00:05:47,880
approaches to handling storage faults

173
00:05:46,740 --> 00:05:50,729
when the problems that I showed in the

174
00:05:47,880 --> 00:05:51,990
previous slide erase so what we do is we

175
00:05:50,729 --> 00:05:53,940
take practical systems which are are

176
00:05:51,990 --> 00:05:55,289
some systems and inject these falls into

177
00:05:53,940 --> 00:05:57,180
these systems and observe how they

178
00:05:55,289 --> 00:05:58,530
behave and we also analyze approaches

179
00:05:57,180 --> 00:06:00,810
that have been proposed by prior

180
00:05:58,530 --> 00:06:02,669
research to deal with data corruption in

181
00:06:00,810 --> 00:06:04,199
our some base systems through our

182
00:06:02,669 --> 00:06:05,549
analysis we classify their approaches

183
00:06:04,199 --> 00:06:07,650
into two categories the first category

184
00:06:05,550 --> 00:06:09,990
is what I call the protocol oblivious

185
00:06:07,650 --> 00:06:12,419
category the approaches in this category

186
00:06:09,990 --> 00:06:14,250
do not use any level of protocol any

187
00:06:12,419 --> 00:06:16,198
protocol level knowledge and so they can

188
00:06:14,250 --> 00:06:18,120
lose data or become unavailable the

189
00:06:16,199 --> 00:06:19,260
second class kind of uses some protocol

190
00:06:18,120 --> 00:06:20,970
level knowledge but they do so in

191
00:06:19,260 --> 00:06:23,389
effectively or incorrectly still leading

192
00:06:20,970 --> 00:06:25,229
to safety violations with unavailability

193
00:06:23,389 --> 00:06:26,250
so to illustrate the different

194
00:06:25,229 --> 00:06:29,219
approaches I'm going to walk you through

195
00:06:26,250 --> 00:06:31,710
a couple of them the first one is a

196
00:06:29,220 --> 00:06:34,110
simple approach that I called crash in

197
00:06:31,710 --> 00:06:36,750
this approach the note carefully uses

198
00:06:34,110 --> 00:06:38,580
checksums and handles iOS iOS carefully

199
00:06:36,750 --> 00:06:40,889
but the action that it takes upon

200
00:06:38,580 --> 00:06:42,300
detection is to crash itself this is a

201
00:06:40,889 --> 00:06:44,370
popular approach for example it's used

202
00:06:42,300 --> 00:06:46,470
by zookeeper by crashing the node you

203
00:06:44,370 --> 00:06:47,669
are not inflicting any harm in the

204
00:06:46,470 --> 00:06:49,860
system but you're just affecting the

205
00:06:47,669 --> 00:06:51,810
availability for example if you have a

206
00:06:49,860 --> 00:06:53,010
five node cluster you already have lost

207
00:06:51,810 --> 00:06:55,110
couple of notes because of let's say a

208
00:06:53,010 --> 00:06:56,490
partition or a system crash and you're

209
00:06:55,110 --> 00:06:56,950
operating with the bare majority and

210
00:06:56,490 --> 00:06:58,420
even a

211
00:06:56,950 --> 00:07:00,400
single data corruption in one node could

212
00:06:58,420 --> 00:07:01,840
lead to a veil unavailability because

213
00:07:00,400 --> 00:07:03,159
the first node would just like realize

214
00:07:01,840 --> 00:07:03,789
that there is a problem it would crash

215
00:07:03,160 --> 00:07:06,160
itself

216
00:07:03,790 --> 00:07:08,710
note that restarting the node does not

217
00:07:06,160 --> 00:07:10,000
help in any way because the node the the

218
00:07:08,710 --> 00:07:11,650
problem that we are dealing with here is

219
00:07:10,000 --> 00:07:13,360
kind of persistent it's a storage fault

220
00:07:11,650 --> 00:07:15,010
and so the node would recover again come

221
00:07:13,360 --> 00:07:16,840
back and hit this T same storage fault

222
00:07:15,010 --> 00:07:18,219
again and then crash so it will remain

223
00:07:16,840 --> 00:07:19,780
in this crash restart loop and if you

224
00:07:18,220 --> 00:07:21,310
want to take the node out of the crash

225
00:07:19,780 --> 00:07:23,049
restart look you need some sort of

226
00:07:21,310 --> 00:07:24,250
manual intervention and that could be

227
00:07:23,050 --> 00:07:25,360
error-prone as I showed you

228
00:07:24,250 --> 00:07:28,240
for example the delete to rebuild

229
00:07:25,360 --> 00:07:29,530
approach right here is another implode

230
00:07:28,240 --> 00:07:31,660
another approach that is very

231
00:07:29,530 --> 00:07:33,520
interesting we call this the truncate

232
00:07:31,660 --> 00:07:36,730
approach in this approach the node

233
00:07:33,520 --> 00:07:38,109
instead of crashing the node it just

234
00:07:36,730 --> 00:07:39,820
realizes that there is something

235
00:07:38,110 --> 00:07:40,960
problematic it truncates the faulting

236
00:07:39,820 --> 00:07:43,330
portion of the raid and continues to

237
00:07:40,960 --> 00:07:45,669
operate by doing so the availability is

238
00:07:43,330 --> 00:07:47,229
kind of unaffected but surprisingly this

239
00:07:45,670 --> 00:07:48,760
could lead to a global data loss even

240
00:07:47,230 --> 00:07:51,010
though you have several copies other

241
00:07:48,760 --> 00:07:52,599
copies and other missions for example

242
00:07:51,010 --> 00:07:54,580
take this case you have five machines

243
00:07:52,600 --> 00:07:57,460
three missions have agreed upon a set of

244
00:07:54,580 --> 00:07:58,599
commands now the first there is a

245
00:07:57,460 --> 00:08:01,510
corruption in the first entry in the

246
00:07:58,600 --> 00:08:03,490
first mission the mission trigonis is

247
00:08:01,510 --> 00:08:05,710
sad but what it does is it truncates

248
00:08:03,490 --> 00:08:07,390
that particular entry on all subsequent

249
00:08:05,710 --> 00:08:10,120
entries in the log thereby losing the

250
00:08:07,390 --> 00:08:11,770
entire state now it's possible that the

251
00:08:10,120 --> 00:08:13,120
remaining nodes which do not have any

252
00:08:11,770 --> 00:08:14,890
idea about this committed data could be

253
00:08:13,120 --> 00:08:18,010
elected leader among themselves and so

254
00:08:14,890 --> 00:08:19,570
we could lose the data silently so we

255
00:08:18,010 --> 00:08:21,099
studied all these kind of approaches and

256
00:08:19,570 --> 00:08:22,240
but I'm not going to describe all of

257
00:08:21,100 --> 00:08:24,340
them here is a summary of all the

258
00:08:22,240 --> 00:08:25,840
approaches we saw that there are two

259
00:08:24,340 --> 00:08:27,700
different kinds there are several

260
00:08:25,840 --> 00:08:29,650
approaches and we are going to see how

261
00:08:27,700 --> 00:08:30,940
there's because there are they safe are

262
00:08:29,650 --> 00:08:32,140
they available and what are the

263
00:08:30,940 --> 00:08:33,549
different kinds of properties they offer

264
00:08:32,140 --> 00:08:35,470
for example performance do they need

265
00:08:33,549 --> 00:08:37,719
manual intervention or how fast I can

266
00:08:35,470 --> 00:08:41,050
recover and so on here is the result of

267
00:08:37,720 --> 00:08:42,160
our analysis some key takeaways first we

268
00:08:41,049 --> 00:08:43,809
note that none of the approaches are

269
00:08:42,159 --> 00:08:46,030
both safe and available at the same time

270
00:08:43,809 --> 00:08:47,859
many approaches require some sort of

271
00:08:46,030 --> 00:08:48,939
manual intervention and we know that

272
00:08:47,860 --> 00:08:50,470
such manual intervention can be

273
00:08:48,940 --> 00:08:51,810
error-prone and can cause safety

274
00:08:50,470 --> 00:08:53,590
violations

275
00:08:51,810 --> 00:08:55,150
finally we note that none of the

276
00:08:53,590 --> 00:08:57,490
approaches can recover fast even if they

277
00:08:55,150 --> 00:08:59,500
can recover at all right in contrast our

278
00:08:57,490 --> 00:09:00,700
approach control provides both safety

279
00:08:59,500 --> 00:09:02,260
and high availability and it has a

280
00:09:00,700 --> 00:09:03,910
desirable property such as it does not

281
00:09:02,260 --> 00:09:05,290
have a common case performance it does

282
00:09:03,910 --> 00:09:08,170
not require manual intervention and it

283
00:09:05,290 --> 00:09:09,819
can require fast so next I will describe

284
00:09:08,170 --> 00:09:11,709
control

285
00:09:09,820 --> 00:09:13,449
here is a very high-level world we have

286
00:09:11,709 --> 00:09:15,518
control it has two components the first

287
00:09:13,449 --> 00:09:17,050
company is a local storage layer which

288
00:09:15,519 --> 00:09:18,820
is responsible for managing the local

289
00:09:17,050 --> 00:09:21,490
data in each of the nodes and carefully

290
00:09:18,820 --> 00:09:24,070
detecting storage faults the distributed

291
00:09:21,490 --> 00:09:25,480
recovery layer uses the redundant copies

292
00:09:24,070 --> 00:09:26,680
and the other nodes to recover the

293
00:09:25,480 --> 00:09:28,779
faulty data that is informed by the

294
00:09:26,680 --> 00:09:30,279
local storage layer both these layers

295
00:09:28,779 --> 00:09:33,699
carefully exploit are some specific

296
00:09:30,279 --> 00:09:35,050
knowledge to recover faulty data so now

297
00:09:33,699 --> 00:09:37,540
let's talk about the local storage layer

298
00:09:35,050 --> 00:09:40,149
the main function is to just detect the

299
00:09:37,540 --> 00:09:42,219
faulty data and detect which portions of

300
00:09:40,149 --> 00:09:43,870
data are faulty to do this the local

301
00:09:42,220 --> 00:09:45,790
storage layer uses well-known technique

302
00:09:43,870 --> 00:09:48,670
such as check sums proper error IO and

303
00:09:45,790 --> 00:09:51,639
error handling and so on and the local

304
00:09:48,670 --> 00:09:53,829
storage achieves these goals by with low

305
00:09:51,639 --> 00:09:54,880
performance and space warheads in

306
00:09:53,829 --> 00:09:56,050
addition to the above there is an

307
00:09:54,880 --> 00:09:56,920
interesting problem that arises when

308
00:09:56,050 --> 00:09:59,229
you're dealing with checks and

309
00:09:56,920 --> 00:10:00,519
mismatches in the log the core problem

310
00:09:59,230 --> 00:10:02,050
is that I check some mismatch and the

311
00:10:00,519 --> 00:10:04,930
log could be due due to two different

312
00:10:02,050 --> 00:10:06,880
reasons one system crash and to a

313
00:10:04,930 --> 00:10:08,769
storage corruption let me illustrate

314
00:10:06,880 --> 00:10:10,180
this with a simple example let's say you

315
00:10:08,769 --> 00:10:11,589
have a log you're trying to open

316
00:10:10,180 --> 00:10:13,479
something into the log and then you

317
00:10:11,589 --> 00:10:15,610
crash in the middle when you recover and

318
00:10:13,480 --> 00:10:16,630
come back the entry would be partially

319
00:10:15,610 --> 00:10:18,910
written which means that there would be

320
00:10:16,630 --> 00:10:20,649
a checksum mismatch on the recovery here

321
00:10:18,910 --> 00:10:22,449
could just truncate that faulty entry

322
00:10:20,649 --> 00:10:24,579
because we can be sure that the node

323
00:10:22,449 --> 00:10:26,260
could not have acknowledged any external

324
00:10:24,579 --> 00:10:28,420
entity because it crashed in the middle

325
00:10:26,260 --> 00:10:30,670
of writing that update on the other hand

326
00:10:28,420 --> 00:10:32,589
consider this case where the entries

327
00:10:30,670 --> 00:10:34,209
were safely persisted but then a late of

328
00:10:32,589 --> 00:10:36,699
disk corruption cost checksum mismatch

329
00:10:34,209 --> 00:10:37,899
one of the entries here we cannot

330
00:10:36,699 --> 00:10:39,969
truncate that entry as part of the

331
00:10:37,899 --> 00:10:42,880
recovery because that could cause us

332
00:10:39,970 --> 00:10:44,620
global safety violation however current

333
00:10:42,880 --> 00:10:47,110
systems conflate these two conditions

334
00:10:44,620 --> 00:10:49,209
always and they always try to induce

335
00:10:47,110 --> 00:10:52,660
trunk keep the data in the presence of

336
00:10:49,209 --> 00:10:55,029
corruptions in the log control modifies

337
00:10:52,660 --> 00:10:56,139
the local update protocol to enable the

338
00:10:55,029 --> 00:10:59,769
system to differentiate these two

339
00:10:56,139 --> 00:11:02,740
conditions so next now let's talk about

340
00:10:59,769 --> 00:11:03,819
the distributed recovery part and for

341
00:11:02,740 --> 00:11:05,440
this talk I'm just going to concentrate

342
00:11:03,819 --> 00:11:06,969
on the log recovery I'm not going to

343
00:11:05,440 --> 00:11:12,399
talk about the snapshot recovery of the

344
00:11:06,970 --> 00:11:14,079
meta infirmary so first as a first step

345
00:11:12,399 --> 00:11:16,029
control simplifies a log recovery by

346
00:11:14,079 --> 00:11:18,969
decoupling the follower recovery from

347
00:11:16,029 --> 00:11:20,860
that of the leader fixing the followers

348
00:11:18,970 --> 00:11:21,730
is fairly straightforward because are

349
00:11:20,860 --> 00:11:22,430
some protocols

350
00:11:21,730 --> 00:11:23,930
give

351
00:11:22,430 --> 00:11:25,280
provide this guarantee that the lead is

352
00:11:23,930 --> 00:11:26,719
guaranteed to have all the committed

353
00:11:25,280 --> 00:11:28,520
data in the system and that's the

354
00:11:26,720 --> 00:11:29,780
protocol level knowledge an example of a

355
00:11:28,520 --> 00:11:32,449
protocol level knowledge that control

356
00:11:29,780 --> 00:11:34,910
exploits for example let's take this

357
00:11:32,450 --> 00:11:37,220
case the followers are having some

358
00:11:34,910 --> 00:11:38,959
corrupted increase in their logs the

359
00:11:37,220 --> 00:11:40,600
followers could simply vary they about

360
00:11:38,960 --> 00:11:42,800
their faulty entries to the leader and

361
00:11:40,600 --> 00:11:44,360
because we have this guarantee that the

362
00:11:42,800 --> 00:11:45,530
leader is guaranteed to have all the

363
00:11:44,360 --> 00:11:47,270
committed data in the system the leader

364
00:11:45,530 --> 00:11:49,670
could simply simply supply those values

365
00:11:47,270 --> 00:11:50,930
to the followers and fix them and no

366
00:11:49,670 --> 00:11:52,280
matter what how many Falls are there in

367
00:11:50,930 --> 00:11:55,400
the followers the leader can always fix

368
00:11:52,280 --> 00:11:57,439
them that's the fix the tricky part in

369
00:11:55,400 --> 00:12:00,170
the log recovery is that fixing the

370
00:11:57,440 --> 00:12:03,110
latest log first let's start with the

371
00:12:00,170 --> 00:12:05,270
simple case in which a faulty entry of

372
00:12:03,110 --> 00:12:06,410
an entry that is faulty in the leader is

373
00:12:05,270 --> 00:12:08,270
kind of intact on with some other

374
00:12:06,410 --> 00:12:09,560
follower for example in this case the

375
00:12:08,270 --> 00:12:11,420
third entry is kind of faulty in the

376
00:12:09,560 --> 00:12:12,739
leader however there are two copies

377
00:12:11,420 --> 00:12:14,930
which are intact Rama and the followers

378
00:12:12,740 --> 00:12:15,620
the leader could ask the followers and

379
00:12:14,930 --> 00:12:17,030
the follower

380
00:12:15,620 --> 00:12:18,470
one of them could just supply the

381
00:12:17,030 --> 00:12:18,890
correct value on once we have fixed the

382
00:12:18,470 --> 00:12:20,270
leader

383
00:12:18,890 --> 00:12:22,970
we know it's straightforward to fix the

384
00:12:20,270 --> 00:12:25,579
followers however life is not always

385
00:12:22,970 --> 00:12:27,650
easy sometimes there are some cases

386
00:12:25,580 --> 00:12:29,600
where the leader logs cannot be fixed

387
00:12:27,650 --> 00:12:31,160
easily for example take this case the

388
00:12:29,600 --> 00:12:33,020
leader has a corrupted entry it could

389
00:12:31,160 --> 00:12:34,850
ask others but no other no other system

390
00:12:33,020 --> 00:12:37,010
in no other node in the system has any

391
00:12:34,850 --> 00:12:38,660
about this data or there could be some

392
00:12:37,010 --> 00:12:40,069
node which has some ID about this data

393
00:12:38,660 --> 00:12:41,449
but it is currently partitioned or it

394
00:12:40,070 --> 00:12:43,700
has crashed so you cannot recover the

395
00:12:41,450 --> 00:12:44,840
leader so the main insight to recover

396
00:12:43,700 --> 00:12:46,790
from these kind of scenarios is that

397
00:12:44,840 --> 00:12:48,860
they're two separate committed items

398
00:12:46,790 --> 00:12:50,780
from uncommitted ones while it is

399
00:12:48,860 --> 00:12:53,060
necessary to fix committed entries for

400
00:12:50,780 --> 00:12:55,130
safety you could safely discard

401
00:12:53,060 --> 00:12:57,229
uncommitted entries and doing so as

402
00:12:55,130 --> 00:13:00,080
early as possible is very vital to the

403
00:12:57,230 --> 00:13:01,370
availability of the system so determined

404
00:13:00,080 --> 00:13:03,080
to determine commitment the leader

405
00:13:01,370 --> 00:13:05,450
queries for a faulty entry to all the

406
00:13:03,080 --> 00:13:06,920
nodes in the system and let's say if a

407
00:13:05,450 --> 00:13:08,570
majority of the nodes come back and say

408
00:13:06,920 --> 00:13:10,640
that they do not have an entry in this

409
00:13:08,570 --> 00:13:12,080
in the log then the leader can confirm

410
00:13:10,640 --> 00:13:14,420
that that entry must be an uncommitted

411
00:13:12,080 --> 00:13:16,550
entry and so it can discard and continue

412
00:13:14,420 --> 00:13:19,579
from there and so the system can remain

413
00:13:16,550 --> 00:13:21,589
available on the other hand if the tenth

414
00:13:19,580 --> 00:13:23,510
rewear committed then at least one node

415
00:13:21,590 --> 00:13:25,160
in any majority would have that entry

416
00:13:23,510 --> 00:13:27,110
because when we commit things we commit

417
00:13:25,160 --> 00:13:28,880
at least to a majority of disks and so

418
00:13:27,110 --> 00:13:31,280
the leader could fix using one of those

419
00:13:28,880 --> 00:13:32,990
correct correct responses and then it

420
00:13:31,280 --> 00:13:35,610
can discover and then it can fix its log

421
00:13:32,990 --> 00:13:37,140
and from there it can proceed

422
00:13:35,610 --> 00:13:38,250
there are several other Cardinal cases

423
00:13:37,140 --> 00:13:40,260
that I'm not going to discuss in this

424
00:13:38,250 --> 00:13:41,519
talk please read our paper for all this

425
00:13:40,260 --> 00:13:44,189
kind of recovery mechanisms that we have

426
00:13:41,519 --> 00:13:47,279
discussed in the paper so next I'll show

427
00:13:44,190 --> 00:13:48,600
show some evaluation results as I told

428
00:13:47,279 --> 00:13:50,189
you why we have implemented control in

429
00:13:48,600 --> 00:13:52,050
log cabin and zookeeper to different

430
00:13:50,190 --> 00:13:54,300
systems based on two different consensus

431
00:13:52,050 --> 00:13:55,890
protocols and we are going to explore

432
00:13:54,300 --> 00:13:57,449
the reliability guarantees of control

433
00:13:55,890 --> 00:14:01,319
and also the performance warheads that

434
00:13:57,450 --> 00:14:02,579
you'll be eating uses so first I will

435
00:14:01,320 --> 00:14:05,250
show you some real ability experiment

436
00:14:02,579 --> 00:14:07,890
example of a reliability experiment this

437
00:14:05,250 --> 00:14:09,690
is an example so in this example what we

438
00:14:07,890 --> 00:14:11,220
do is we take the log and the file

439
00:14:09,690 --> 00:14:12,870
system blocks that make up the log being

440
00:14:11,220 --> 00:14:15,570
inject errors into them like for example

441
00:14:12,870 --> 00:14:16,620
introducing a corruption or error and

442
00:14:15,570 --> 00:14:18,930
the result is that in the original

443
00:14:16,620 --> 00:14:20,970
systems without control if you inject

444
00:14:18,930 --> 00:14:22,649
corruptions they are unsafe in about 30

445
00:14:20,970 --> 00:14:24,510
percent of the cases that we tested or

446
00:14:22,649 --> 00:14:26,070
unavailable and when you introduce

447
00:14:24,510 --> 00:14:29,490
errors they are about in half of the

448
00:14:26,070 --> 00:14:30,600
cases unavailable in contrast in the

449
00:14:29,490 --> 00:14:32,130
presence of corruptions in errors

450
00:14:30,600 --> 00:14:34,920
control always provide safety and higher

451
00:14:32,130 --> 00:14:36,450
level availability and that was one

452
00:14:34,920 --> 00:14:38,040
example we also conducted many other

453
00:14:36,450 --> 00:14:39,779
similar experiments rigorous fault

454
00:14:38,040 --> 00:14:41,219
injection experiments for example all

455
00:14:39,779 --> 00:14:44,339
possible combinations of corruptions on

456
00:14:41,220 --> 00:14:46,260
all the entries and like introducing

457
00:14:44,339 --> 00:14:47,640
crashes and lie a network partition so

458
00:14:46,260 --> 00:14:48,839
that the nodes could be lagging or they

459
00:14:47,640 --> 00:14:51,209
could have been crashed in addition to

460
00:14:48,839 --> 00:14:53,010
having data corruption and also data

461
00:14:51,209 --> 00:14:54,029
corruptions in the snapshots while some

462
00:14:53,010 --> 00:14:55,980
of this map shots may be garbage

463
00:14:54,029 --> 00:14:57,240
collected some may not be and so on we

464
00:14:55,980 --> 00:14:59,570
also introduced file system metadata

465
00:14:57,240 --> 00:15:03,089
faults for example corrupted inodes or

466
00:14:59,570 --> 00:15:05,459
incomplete inodes and so on and then we

467
00:15:03,089 --> 00:15:08,339
inject these false at like at a time at

468
00:15:05,459 --> 00:15:09,779
many nodes and so on in all these

469
00:15:08,339 --> 00:15:12,120
reliability experiments the summary is

470
00:15:09,779 --> 00:15:13,500
that the original systems were all safe

471
00:15:12,120 --> 00:15:15,089
unsafe and unavailable in many many

472
00:15:13,500 --> 00:15:16,890
cases whereas the control versions

473
00:15:15,089 --> 00:15:20,370
always provided safety and high

474
00:15:16,890 --> 00:15:22,790
availability next let's look at the

475
00:15:20,370 --> 00:15:25,380
update performance overheads of control

476
00:15:22,790 --> 00:15:26,670
so I'm going to show this results for a

477
00:15:25,380 --> 00:15:28,770
simple workload which is like in

478
00:15:26,670 --> 00:15:29,969
repeatedly inserting 1k increase into

479
00:15:28,770 --> 00:15:32,069
the system and the snapshots are being

480
00:15:29,970 --> 00:15:33,899
taken in the background and here are the

481
00:15:32,070 --> 00:15:35,790
results on the Left we have the results

482
00:15:33,899 --> 00:15:38,490
for the log cabin on the right we have

483
00:15:35,790 --> 00:15:40,290
it for zookeeper on the y-axis we have

484
00:15:38,490 --> 00:15:41,519
throughput so higher is better on the

485
00:15:40,290 --> 00:15:44,310
x-axis we have different number of

486
00:15:41,519 --> 00:15:46,430
clients as you can see even for this

487
00:15:44,310 --> 00:15:49,199
right intensive worst case workloads

488
00:15:46,430 --> 00:15:51,959
controller introduces only 4% for heads

489
00:15:49,200 --> 00:15:55,080
niceties and on disks it's a slightly

490
00:15:51,960 --> 00:15:57,000
higher about eight to ten percent I note

491
00:15:55,080 --> 00:15:59,010
that this is just the overheads of

492
00:15:57,000 --> 00:16:00,480
control arrays because for every log

493
00:15:59,010 --> 00:16:02,100
entry that we write into the system we

494
00:16:00,480 --> 00:16:04,410
write additional information and that is

495
00:16:02,100 --> 00:16:06,360
the source for the overheads I note that

496
00:16:04,410 --> 00:16:07,860
this is the worst case world-war hits

497
00:16:06,360 --> 00:16:09,720
that control you might have this is

498
00:16:07,860 --> 00:16:12,540
because all for every write we are

499
00:16:09,720 --> 00:16:13,950
paying some cost so to summarize this

500
00:16:12,540 --> 00:16:15,839
talk I showed you that recovering from

501
00:16:13,950 --> 00:16:18,390
storage falls in dissipated systems is

502
00:16:15,840 --> 00:16:19,530
surprisingly tricky most existing and

503
00:16:18,390 --> 00:16:20,939
recovery approaches are correct of

504
00:16:19,530 --> 00:16:23,730
protocol oblivious and so they lead to

505
00:16:20,940 --> 00:16:25,170
problems I showed you how protocol up

506
00:16:23,730 --> 00:16:26,820
their recovery approaches can be helpful

507
00:16:25,170 --> 00:16:28,079
and I showed you one example of that

508
00:16:26,820 --> 00:16:29,910
which is control which is an

509
00:16:28,080 --> 00:16:32,100
implementation specific to RSM based

510
00:16:29,910 --> 00:16:33,329
systems it guarantees safety and high

511
00:16:32,100 --> 00:16:35,130
availability in the presence of storage

512
00:16:33,330 --> 00:16:38,340
falls at little performance overheads

513
00:16:35,130 --> 00:16:40,260
more broadly even seemingly obvious

514
00:16:38,340 --> 00:16:41,940
things that we take for granted from

515
00:16:40,260 --> 00:16:43,560
distributed systems that such as like

516
00:16:41,940 --> 00:16:45,330
redundant copies will help recover from

517
00:16:43,560 --> 00:16:47,160
bad data or more generally that

518
00:16:45,330 --> 00:16:49,110
redundancy enables reliability it's kind

519
00:16:47,160 --> 00:16:53,430
of hard to surprisingly hard to realize

520
00:16:49,110 --> 00:16:55,950
in reality and I have shown you how

521
00:16:53,430 --> 00:16:57,569
protocol awareness is a key idea that

522
00:16:55,950 --> 00:16:59,610
can be used to effectively use

523
00:16:57,570 --> 00:17:02,240
redundancy to recover from storage

524
00:16:59,610 --> 00:17:04,110
faults however our step is just a first

525
00:17:02,240 --> 00:17:06,240
work is just a first step in this

526
00:17:04,109 --> 00:17:07,770
direction as many others have shown that

527
00:17:06,240 --> 00:17:10,440
there are similar problems existing in

528
00:17:07,770 --> 00:17:12,300
other classes of systems with that I'll

529
00:17:10,440 --> 00:17:14,530
be I'll finish my talk I will be happy

530
00:17:12,300 --> 00:17:20,970
to take questions

531
00:17:14,530 --> 00:17:20,970
[Applause]

532
00:17:26,390 --> 00:17:32,100
hey hey great work I have two questions

533
00:17:29,490 --> 00:17:33,930
yeah first the leader give up leadership

534
00:17:32,100 --> 00:17:38,280
at some point oh that's a great question

535
00:17:33,930 --> 00:17:40,260
so yes it may for example the leader has

536
00:17:38,280 --> 00:17:41,700
a faulty entry it may ask to others and

537
00:17:40,260 --> 00:17:44,280
none of it is not able to decide about

538
00:17:41,700 --> 00:17:46,230
that particular entry in that case after

539
00:17:44,280 --> 00:17:48,120
time of the leader may step down and

540
00:17:46,230 --> 00:17:49,680
it's possible some other node might get

541
00:17:48,120 --> 00:17:50,909
elected the leader this time and then

542
00:17:49,680 --> 00:17:53,580
the system may be able to progress from

543
00:17:50,910 --> 00:17:54,090
there yeah that's possible the second

544
00:17:53,580 --> 00:17:55,620
question

545
00:17:54,090 --> 00:17:56,879
so actually the proper way to do this in

546
00:17:55,620 --> 00:17:59,100
zookeeper would be to reconfigure the

547
00:17:56,880 --> 00:18:01,080
faulty node and then to erase the Logan

548
00:17:59,100 --> 00:18:02,969
restart it and then reconfigure it again

549
00:18:01,080 --> 00:18:04,500
to include it back in the cluster did

550
00:18:02,970 --> 00:18:06,990
you compare with this approach oh that's

551
00:18:04,500 --> 00:18:08,940
a great question so we did consider the

552
00:18:06,990 --> 00:18:10,400
reconfigure approach there are two

553
00:18:08,940 --> 00:18:14,340
problems in the reconfigured approach

554
00:18:10,400 --> 00:18:15,720
first of all like by reconfiguring you

555
00:18:14,340 --> 00:18:17,129
are kind of like removing all the data

556
00:18:15,720 --> 00:18:19,490
from that node and you're trying to

557
00:18:17,130 --> 00:18:21,900
rebuild that node into a new node or a

558
00:18:19,490 --> 00:18:23,940
node there you have deleted all the data

559
00:18:21,900 --> 00:18:25,380
now if you have like a very large state

560
00:18:23,940 --> 00:18:26,850
machine for example file system as a

561
00:18:25,380 --> 00:18:28,530
state machine then you have to rebuild

562
00:18:26,850 --> 00:18:30,659
the entire state instead of let's say

563
00:18:28,530 --> 00:18:32,370
you want to don't you don't want to

564
00:18:30,660 --> 00:18:34,260
amplify the error so much because you

565
00:18:32,370 --> 00:18:35,879
can just like fix one file system block

566
00:18:34,260 --> 00:18:38,250
and you'll be fine yeah so that's the

567
00:18:35,880 --> 00:18:40,170
that's the motivation why we think

568
00:18:38,250 --> 00:18:41,340
reconfigure this node so not so idea and

569
00:18:40,170 --> 00:18:42,780
there are also other cases where

570
00:18:41,340 --> 00:18:44,970
reconfiguration can be unavailable but

571
00:18:42,780 --> 00:18:46,710
that's kind of too tricky to explain in

572
00:18:44,970 --> 00:18:49,110
here but the high-level idea is that you

573
00:18:46,710 --> 00:18:55,950
need to transfer too much amount of data

574
00:18:49,110 --> 00:18:58,399
to just to do that okay thank you okay

575
00:18:55,950 --> 00:19:02,559
then thank the speaker

576
00:18:58,400 --> 00:19:02,559
[Applause]

