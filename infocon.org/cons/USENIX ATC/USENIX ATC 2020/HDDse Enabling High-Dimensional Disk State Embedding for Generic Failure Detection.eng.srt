1
00:00:08,080 --> 00:00:12,719
hello everyone my name is jitong

2
00:00:10,559 --> 00:00:13,678
very pleased to be here to introduce our

3
00:00:12,719 --> 00:00:16,079
latest work

4
00:00:13,679 --> 00:00:16,960
enabling high dimensional disk state

5
00:00:16,079 --> 00:00:19,119
embedding for

6
00:00:16,960 --> 00:00:20,560
generic failure detection system of

7
00:00:19,119 --> 00:00:23,600
heterogeneous disks

8
00:00:20,560 --> 00:00:24,880
in large data centers i'm a chief

9
00:00:23,600 --> 00:00:27,519
engineer in huawei

10
00:00:24,880 --> 00:00:29,679
and a post doctor from the university of

11
00:00:27,519 --> 00:00:32,399
amsterdam in the netherlands at the

12
00:00:29,679 --> 00:00:34,480
intelligent data engineering lab i

13
00:00:32,399 --> 00:00:36,719
achieved my phd degree at huazhong

14
00:00:34,480 --> 00:00:39,120
university of science and technology in

15
00:00:36,719 --> 00:00:39,120
china

16
00:00:40,320 --> 00:00:43,680
device failure is a common problem in

17
00:00:42,480 --> 00:00:46,718
large data centers

18
00:00:43,680 --> 00:00:50,320
where hard disks are widely used as

19
00:00:46,719 --> 00:00:52,879
primary storage devices although there

20
00:00:50,320 --> 00:00:56,480
are stereos of passive force defense

21
00:00:52,879 --> 00:00:58,320
mechanism like the erase codes and rays

22
00:00:56,480 --> 00:01:00,559
many researchers have focused on

23
00:00:58,320 --> 00:01:02,800
proactive disk video detection which

24
00:01:00,559 --> 00:01:05,920
aims to ensure the reliability

25
00:01:02,800 --> 00:01:08,640
and availability of large-scale storage

26
00:01:05,920 --> 00:01:11,360
system in advance

27
00:01:08,640 --> 00:01:13,760
this year's they'll have emerged a

28
00:01:11,360 --> 00:01:15,759
descent amount of researchers on this

29
00:01:13,760 --> 00:01:18,000
failure prediction

30
00:01:15,759 --> 00:01:20,560
almost all hard disk drivers and

31
00:01:18,000 --> 00:01:21,600
flash-based ssds come with building the

32
00:01:20,560 --> 00:01:24,840
self-monitoring

33
00:01:21,600 --> 00:01:28,000
analysis and reporting technology

34
00:01:24,840 --> 00:01:31,200
smart which are indicators of

35
00:01:28,000 --> 00:01:31,840
disk healthy status each smart

36
00:01:31,200 --> 00:01:35,280
attributes

37
00:01:31,840 --> 00:01:36,479
entry consists of five elements a disk

38
00:01:35,280 --> 00:01:39,280
uses the so called

39
00:01:36,479 --> 00:01:41,759
the host methods based on smart values

40
00:01:39,280 --> 00:01:44,720
to claim its failure status

41
00:01:41,759 --> 00:01:45,360
which means the hard disk would raise an

42
00:01:44,720 --> 00:01:48,000
alarm

43
00:01:45,360 --> 00:01:50,159
if the value of a smart attribute

44
00:01:48,000 --> 00:01:51,360
crosses the corresponding predefined

45
00:01:50,159 --> 00:01:54,159
threshold

46
00:01:51,360 --> 00:01:57,520
however this threshold method only

47
00:01:54,159 --> 00:02:00,960
achieves poor performance

48
00:01:57,520 --> 00:02:03,439
we observed six classes of approaches of

49
00:02:00,960 --> 00:02:05,679
this failure detection thresholds based

50
00:02:03,439 --> 00:02:08,959
approaches we refer

51
00:02:05,680 --> 00:02:11,440
it to as the tv and distance-based

52
00:02:08,959 --> 00:02:12,720
anomaly detection projects d80 and

53
00:02:11,440 --> 00:02:14,239
shallow machine learning-based

54
00:02:12,720 --> 00:02:16,560
approaches sml

55
00:02:14,239 --> 00:02:17,360
and deep neural network-based approaches

56
00:02:16,560 --> 00:02:19,040
dna

57
00:02:17,360 --> 00:02:20,560
one-class classification-based

58
00:02:19,040 --> 00:02:22,560
approaches occ

59
00:02:20,560 --> 00:02:24,800
and transfer learning based approaches

60
00:02:22,560 --> 00:02:26,879
tl

61
00:02:24,800 --> 00:02:29,920
however these approaches have their

62
00:02:26,879 --> 00:02:32,879
eliminations as summarized in this table

63
00:02:29,920 --> 00:02:34,720
first limited applicability and

64
00:02:32,879 --> 00:02:36,000
different disk manufacturers have

65
00:02:34,720 --> 00:02:38,959
different smart values

66
00:02:36,000 --> 00:02:41,440
or data distribution even in different

67
00:02:38,959 --> 00:02:43,680
disk models of the same manufacturer

68
00:02:41,440 --> 00:02:46,239
this will result in a situation where

69
00:02:43,680 --> 00:02:48,560
matters trained on specific parameters

70
00:02:46,239 --> 00:02:49,519
only work well for the same same

71
00:02:48,560 --> 00:02:52,080
manufacture

72
00:02:49,519 --> 00:02:54,160
and often even for the same models

73
00:02:52,080 --> 00:02:58,959
limiting their scope of

74
00:02:54,160 --> 00:02:58,959
applicability in practice second

75
00:03:00,239 --> 00:03:04,879
lake of adaptability new disk models

76
00:03:03,840 --> 00:03:08,480
entered

77
00:03:04,879 --> 00:03:09,840
greatly to replace or augment the

78
00:03:08,480 --> 00:03:12,000
storage capacity

79
00:03:09,840 --> 00:03:13,840
leading storage system to consist of

80
00:03:12,000 --> 00:03:15,200
disks from the different vendors and

81
00:03:13,840 --> 00:03:17,280
different models

82
00:03:15,200 --> 00:03:18,319
facing these different models must be

83
00:03:17,280 --> 00:03:21,120
retrained to

84
00:03:18,319 --> 00:03:21,760
obtain more reliable predictions in

85
00:03:21,120 --> 00:03:25,040
order to

86
00:03:21,760 --> 00:03:27,120
adapt to the change in the data

87
00:03:25,040 --> 00:03:28,720
distribution introduced by these new

88
00:03:27,120 --> 00:03:31,599
disk models

89
00:03:28,720 --> 00:03:34,879
such retraining is tender and expensive

90
00:03:31,599 --> 00:03:38,238
in a large data center

91
00:03:34,879 --> 00:03:39,840
third imbalanced datasets in real-world

92
00:03:38,239 --> 00:03:43,519
cloud storage systems

93
00:03:39,840 --> 00:03:46,959
the imbalanced radio of failure samples

94
00:03:43,519 --> 00:03:47,440
and healthy samples poses a significant

95
00:03:46,959 --> 00:03:49,840
threat

96
00:03:47,440 --> 00:03:50,720
to the efficiency of machine learning

97
00:03:49,840 --> 00:03:52,720
models

98
00:03:50,720 --> 00:03:53,920
and fourth minority disk failure

99
00:03:52,720 --> 00:03:57,200
detection

100
00:03:53,920 --> 00:04:00,000
in storage systems due to a lack of

101
00:03:57,200 --> 00:04:01,280
sufficient training date data some

102
00:04:00,000 --> 00:04:03,599
detective approaches

103
00:04:01,280 --> 00:04:05,040
failed to deliver satisfactory detective

104
00:04:03,599 --> 00:04:08,720
performance for those

105
00:04:05,040 --> 00:04:10,879
matters although some transfer learning

106
00:04:08,720 --> 00:04:13,120
methods has been proposed to

107
00:04:10,879 --> 00:04:15,280
address this issue their performance

108
00:04:13,120 --> 00:04:18,320
depends on finding a suitable source

109
00:04:15,280 --> 00:04:20,478
name for transfer for knowledge transfer

110
00:04:18,320 --> 00:04:22,320
which might be difficult in real-world

111
00:04:20,478 --> 00:04:25,520
data center

112
00:04:22,320 --> 00:04:28,000
last and poor detective performance

113
00:04:25,520 --> 00:04:28,880
most matters show unstable performance

114
00:04:28,000 --> 00:04:31,919
in practical

115
00:04:28,880 --> 00:04:35,120
long-term use and often fail to obtain

116
00:04:31,919 --> 00:04:37,919
both high to positive rate and low force

117
00:04:35,120 --> 00:04:37,919
positive rate

118
00:04:38,000 --> 00:04:42,240
before we introduce our motivation we

119
00:04:40,240 --> 00:04:44,960
show some preliminary results

120
00:04:42,240 --> 00:04:47,440
first we analyze the overall performance

121
00:04:44,960 --> 00:04:48,960
of existing proposed approaches using

122
00:04:47,440 --> 00:04:51,280
different disk models from

123
00:04:48,960 --> 00:04:52,719
both the smart dataset tencent and

124
00:04:51,280 --> 00:04:54,239
backblazer

125
00:04:52,720 --> 00:04:56,240
in this figure shows the overall

126
00:04:54,240 --> 00:04:58,400
detective performance of the

127
00:04:56,240 --> 00:05:01,840
this data of the state of the art

128
00:04:58,400 --> 00:05:05,039
matters smaot and occ

129
00:05:01,840 --> 00:05:08,239
in each figure the x and y access

130
00:05:05,039 --> 00:05:09,199
refers to the disk models of training

131
00:05:08,240 --> 00:05:12,479
and testing

132
00:05:09,199 --> 00:05:14,320
data sets respectively we use f mission

133
00:05:12,479 --> 00:05:15,919
to evaluate the performance of the

134
00:05:14,320 --> 00:05:18,240
failure detection models

135
00:05:15,919 --> 00:05:19,840
the higher the value is the better the

136
00:05:18,240 --> 00:05:22,240
performance is

137
00:05:19,840 --> 00:05:24,000
in the last row of each figure we use

138
00:05:22,240 --> 00:05:26,560
the hybrid dataset which is

139
00:05:24,000 --> 00:05:27,600
the first six disk models for training

140
00:05:26,560 --> 00:05:30,560
and then

141
00:05:27,600 --> 00:05:31,600
detect on different disk models note

142
00:05:30,560 --> 00:05:34,720
that the last

143
00:05:31,600 --> 00:05:37,199
three disk models are not included in

144
00:05:34,720 --> 00:05:39,680
the hybrid disk models for training

145
00:05:37,199 --> 00:05:41,199
we summarized the fundings from these

146
00:05:39,680 --> 00:05:44,880
two three figures

147
00:05:41,199 --> 00:05:47,280
the first the detection approaches

148
00:05:44,880 --> 00:05:48,320
built on the specific disk models only

149
00:05:47,280 --> 00:05:51,599
have good

150
00:05:48,320 --> 00:05:54,320
results tested on the same disk model

151
00:05:51,600 --> 00:05:57,039
moreover dnm based approaches shows the

152
00:05:54,320 --> 00:05:59,680
best performance in this case

153
00:05:57,039 --> 00:06:01,440
the second the pro the approaches built

154
00:05:59,680 --> 00:06:03,840
on the hybrid disk models

155
00:06:01,440 --> 00:06:04,719
datasets decrease the detective

156
00:06:03,840 --> 00:06:07,359
performance

157
00:06:04,720 --> 00:06:09,199
compared to a model built on the same

158
00:06:07,360 --> 00:06:11,360
disk model

159
00:06:09,199 --> 00:06:12,319
moreover the performance of these

160
00:06:11,360 --> 00:06:15,680
approaches

161
00:06:12,319 --> 00:06:18,319
filter that re-rates when detecting the

162
00:06:15,680 --> 00:06:20,560
disk models that have not appeared in

163
00:06:18,319 --> 00:06:23,199
the training data sets

164
00:06:20,560 --> 00:06:25,759
so the same experimental setup we also

165
00:06:23,199 --> 00:06:28,240
evaluate the amount of dad

166
00:06:25,759 --> 00:06:30,319
although the overall detective result of

167
00:06:28,240 --> 00:06:33,440
the dad method is not

168
00:06:30,319 --> 00:06:35,919
good enough to be deployed in practice

169
00:06:33,440 --> 00:06:38,560
it is not sensitive to the disk the

170
00:06:35,919 --> 00:06:41,198
different disk models

171
00:06:38,560 --> 00:06:42,800
in order to investigate the detection

172
00:06:41,199 --> 00:06:44,800
for minority disks

173
00:06:42,800 --> 00:06:48,240
so the minority disk is the number of

174
00:06:44,800 --> 00:06:50,319
the less than 1500

175
00:06:48,240 --> 00:06:52,080
for transfer linear approach we use the

176
00:06:50,319 --> 00:06:52,800
majority disk model which has the

177
00:06:52,080 --> 00:06:55,280
smallest

178
00:06:52,800 --> 00:06:56,160
kld values with the detection minority

179
00:06:55,280 --> 00:06:58,479
disk model

180
00:06:56,160 --> 00:06:59,440
from the same manufacturer to train the

181
00:06:58,479 --> 00:07:02,800
detective

182
00:06:59,440 --> 00:07:06,080
models this figure shows the result

183
00:07:02,800 --> 00:07:06,800
the transfer learning approach delivers

184
00:07:06,080 --> 00:07:08,639
the best

185
00:07:06,800 --> 00:07:10,319
detective performance on minority

186
00:07:08,639 --> 00:07:12,720
discrete detection

187
00:07:10,319 --> 00:07:14,800
especially with the training dataset

188
00:07:12,720 --> 00:07:16,880
having smaller kld values

189
00:07:14,800 --> 00:07:19,039
while other candidates are difficult to

190
00:07:16,880 --> 00:07:21,360
handle this situation

191
00:07:19,039 --> 00:07:23,919
in order to take an in-depth look at

192
00:07:21,360 --> 00:07:26,960
feasibility of this method in practical

193
00:07:23,919 --> 00:07:29,198
large storage systems we studied the

194
00:07:26,960 --> 00:07:31,120
disk quantity by the different kld

195
00:07:29,199 --> 00:07:34,080
values in two data centers

196
00:07:31,120 --> 00:07:36,639
as shown in in this table most minority

197
00:07:34,080 --> 00:07:37,599
disk models only found the majority disk

198
00:07:36,639 --> 00:07:40,479
model with a

199
00:07:37,599 --> 00:07:43,440
dld value greater than one which might

200
00:07:40,479 --> 00:07:46,000
result in poor detective performance

201
00:07:43,440 --> 00:07:48,319
therefore it is still difficult to find

202
00:07:46,000 --> 00:07:50,560
the most suitable majority disk model

203
00:07:48,319 --> 00:07:52,800
with small enough kld value

204
00:07:50,560 --> 00:07:56,080
to use transfer learning for minority

205
00:07:52,800 --> 00:07:56,080
disk failure detection

206
00:07:56,639 --> 00:08:00,879
before introducing our motivation we

207
00:07:58,960 --> 00:08:01,919
first to answer the following three

208
00:08:00,879 --> 00:08:05,280
problems

209
00:08:01,919 --> 00:08:07,440
the first why the dad protests have good

210
00:08:05,280 --> 00:08:09,919
applicability and high adaptability

211
00:08:07,440 --> 00:08:12,879
while dna does not

212
00:08:09,919 --> 00:08:14,878
because the dad method learns the

213
00:08:12,879 --> 00:08:15,360
distance or the similarity between the

214
00:08:14,879 --> 00:08:17,759
normal

215
00:08:15,360 --> 00:08:18,400
and abnormal disk models in a certain

216
00:08:17,759 --> 00:08:21,360
space

217
00:08:18,400 --> 00:08:24,318
which has a commodity and non-sensitive

218
00:08:21,360 --> 00:08:27,759
to the disk models

219
00:08:24,319 --> 00:08:29,840
however the dnn approaches learns the

220
00:08:27,759 --> 00:08:32,640
distribution of smart data

221
00:08:29,840 --> 00:08:34,478
which provides with disk models the

222
00:08:32,640 --> 00:08:35,199
performance of this approach will

223
00:08:34,479 --> 00:08:38,719
decrease

224
00:08:35,200 --> 00:08:40,719
when the distribution changes

225
00:08:38,719 --> 00:08:42,959
second why the overall detective

226
00:08:40,719 --> 00:08:45,760
performance of d80 method is not

227
00:08:42,958 --> 00:08:46,479
as good as other approaches we think the

228
00:08:45,760 --> 00:08:48,240
reason is

229
00:08:46,480 --> 00:08:49,680
it performs the distance based

230
00:08:48,240 --> 00:08:53,200
transformation and

231
00:08:49,680 --> 00:08:55,599
computation in low dimensional space but

232
00:08:53,200 --> 00:08:57,680
doesn't learn from the dynamically

233
00:08:55,600 --> 00:08:59,360
changed long-term behavior in high

234
00:08:57,680 --> 00:09:02,239
dimension

235
00:08:59,360 --> 00:09:03,680
last why the dna project achieves the

236
00:09:02,240 --> 00:09:06,480
best performance among

237
00:09:03,680 --> 00:09:07,680
other candidates the dna is good at

238
00:09:06,480 --> 00:09:09,839
mapping the raw

239
00:09:07,680 --> 00:09:11,599
low dimensional smart attributes to high

240
00:09:09,839 --> 00:09:13,839
dimension target space

241
00:09:11,600 --> 00:09:14,640
through complex transformations that

242
00:09:13,839 --> 00:09:17,440
perform good

243
00:09:14,640 --> 00:09:17,920
expression and defeating ability and

244
00:09:17,440 --> 00:09:20,720
thus

245
00:09:17,920 --> 00:09:22,880
achieves better performance so

246
00:09:20,720 --> 00:09:24,959
considering the above advantages of the

247
00:09:22,880 --> 00:09:26,800
dad and dnn approaches

248
00:09:24,959 --> 00:09:28,719
we are motivated to apply the

249
00:09:26,800 --> 00:09:30,560
distance-based anomaly detection

250
00:09:28,720 --> 00:09:32,959
approach and deep learning to build a

251
00:09:30,560 --> 00:09:37,359
general disk fader detection system

252
00:09:32,959 --> 00:09:39,599
for heterogeneous disks

253
00:09:37,360 --> 00:09:41,440
so this figure provides an overview of

254
00:09:39,600 --> 00:09:44,399
our proposed novel system

255
00:09:41,440 --> 00:09:45,760
hdd-se which combines a sample pool and

256
00:09:44,399 --> 00:09:47,839
astm based

257
00:09:45,760 --> 00:09:51,360
sound means network for disk failure

258
00:09:47,839 --> 00:09:53,760
detection and the decision maker

259
00:09:51,360 --> 00:09:55,120
simply pull is a is used to store the

260
00:09:53,760 --> 00:09:58,080
smart instances

261
00:09:55,120 --> 00:09:59,600
collected daily for each day disk now

262
00:09:58,080 --> 00:10:02,640
else lstm

263
00:09:59,600 --> 00:10:04,720
based the cell miss network for disk

264
00:10:02,640 --> 00:10:05,760
failure detection is the core part in

265
00:10:04,720 --> 00:10:08,240
hdd-se

266
00:10:05,760 --> 00:10:10,319
to output a binary classification result

267
00:10:08,240 --> 00:10:12,959
that indicates the similarity of these

268
00:10:10,320 --> 00:10:12,959
two samples

269
00:10:13,519 --> 00:10:17,200
and decision makers is a module to map

270
00:10:16,480 --> 00:10:20,000
the

271
00:10:17,200 --> 00:10:23,120
result of samples to the final whole

272
00:10:20,000 --> 00:10:25,200
disk healthy state

273
00:10:23,120 --> 00:10:26,480
this figure shows the structure of the

274
00:10:25,200 --> 00:10:29,360
airstream based

275
00:10:26,480 --> 00:10:30,959
cell miss network we designed two stm

276
00:10:29,360 --> 00:10:33,920
networks to receive the

277
00:10:30,959 --> 00:10:36,079
two smart smart smart samples

278
00:10:33,920 --> 00:10:37,839
respectively which need to be compared

279
00:10:36,079 --> 00:10:40,880
for the similarity

280
00:10:37,839 --> 00:10:43,680
note that these two lstm networks

281
00:10:40,880 --> 00:10:44,399
show their waste in order to map the

282
00:10:43,680 --> 00:10:47,120
inputs

283
00:10:44,399 --> 00:10:50,560
to the sample the same target high

284
00:10:47,120 --> 00:10:52,320
dimension space for compilation

285
00:10:50,560 --> 00:10:54,719
the sample pool consists of the

286
00:10:52,320 --> 00:10:55,680
collected smart instances with the

287
00:10:54,720 --> 00:10:58,560
corresponding

288
00:10:55,680 --> 00:11:00,399
confirmed labels and some instances that

289
00:10:58,560 --> 00:11:03,119
need to be detected

290
00:11:00,399 --> 00:11:05,440
we collect all these instances of each

291
00:11:03,120 --> 00:11:08,720
disk in the data center every day

292
00:11:05,440 --> 00:11:12,000
all the time data have been do

293
00:11:08,720 --> 00:11:14,880
uh have been discretely sampled at

294
00:11:12,000 --> 00:11:16,959
an interval of one day the relationship

295
00:11:14,880 --> 00:11:18,880
between the smart instances

296
00:11:16,959 --> 00:11:22,880
the samples and the improved training

297
00:11:18,880 --> 00:11:22,880
pairs is shown in this figure

298
00:11:23,360 --> 00:11:29,680
as mentioned before the improve of our

299
00:11:26,880 --> 00:11:31,120
purpose networks are two smart samples

300
00:11:29,680 --> 00:11:33,519
in pairs

301
00:11:31,120 --> 00:11:36,240
therefore we can freely combine and

302
00:11:33,519 --> 00:11:39,279
label a pair of samples from the same

303
00:11:36,240 --> 00:11:41,120
disk manufacturer there are two benefits

304
00:11:39,279 --> 00:11:43,360
for generating this form of training

305
00:11:41,120 --> 00:11:44,240
datasets compared to existing purchase

306
00:11:43,360 --> 00:11:46,880
we treat the

307
00:11:44,240 --> 00:11:47,440
each smart sampler as a snapshot in the

308
00:11:46,880 --> 00:11:50,720
trainings

309
00:11:47,440 --> 00:11:54,160
process the first battle with imbalanced

310
00:11:50,720 --> 00:11:56,639
datasets it reduces the degree of data

311
00:11:54,160 --> 00:11:59,199
of the data imbalance by the small

312
00:11:56,639 --> 00:12:03,120
simplest the free combination

313
00:11:59,200 --> 00:12:05,839
our method effectively activates the

314
00:12:03,120 --> 00:12:06,480
original data imbalance by a factor of

315
00:12:05,839 --> 00:12:08,800
two

316
00:12:06,480 --> 00:12:09,519
the second battery with minority disk

317
00:12:08,800 --> 00:12:12,399
models

318
00:12:09,519 --> 00:12:15,279
it forms large training samples even

319
00:12:12,399 --> 00:12:18,320
with the minority disk models

320
00:12:15,279 --> 00:12:21,519
therefore our model can increase the

321
00:12:18,320 --> 00:12:23,360
mod the number of samples and greatly

322
00:12:21,519 --> 00:12:25,440
and make better use of deep learning

323
00:12:23,360 --> 00:12:29,440
equations to detect the failure

324
00:12:25,440 --> 00:12:33,040
and avoid moral overfitting

325
00:12:29,440 --> 00:12:35,760
so each large each target detecting

326
00:12:33,040 --> 00:12:38,880
disk consists of several results of

327
00:12:35,760 --> 00:12:41,439
detecting samplers at continuous moments

328
00:12:38,880 --> 00:12:43,200
each output result indicates the

329
00:12:41,440 --> 00:12:46,240
detection for a popular

330
00:12:43,200 --> 00:12:48,399
for a for a particular sample

331
00:12:46,240 --> 00:12:49,760
at one moment rather than the whole

332
00:12:48,399 --> 00:12:54,000
disclaimer's state

333
00:12:49,760 --> 00:12:56,319
in a prayer therefore to improve the

334
00:12:54,000 --> 00:12:57,440
robustness of the detection model

335
00:12:56,320 --> 00:12:59,839
against the knives

336
00:12:57,440 --> 00:13:00,480
we proposed a voting based sliding

337
00:12:59,839 --> 00:13:03,920
window

338
00:13:00,480 --> 00:13:07,600
meant to make this healthy state

339
00:13:03,920 --> 00:13:09,680
decision for the final disk state

340
00:13:07,600 --> 00:13:11,920
we use smart data set from two real

341
00:13:09,680 --> 00:13:13,120
world data center for evaluation tencent

342
00:13:11,920 --> 00:13:15,199
and backblazer

343
00:13:13,120 --> 00:13:19,440
we use five patricks to report the

344
00:13:15,200 --> 00:13:22,240
detective performance in our experiments

345
00:13:19,440 --> 00:13:23,440
many researchers have proposed the mtddl

346
00:13:22,240 --> 00:13:25,839
to

347
00:13:23,440 --> 00:13:27,680
approximate the mean time to date loss

348
00:13:25,839 --> 00:13:31,160
with failure detection model

349
00:13:27,680 --> 00:13:34,800
however this matrix neglects the cost of

350
00:13:31,160 --> 00:13:37,199
misclassification by the approach

351
00:13:34,800 --> 00:13:39,040
therefore we proposed an end-to-end

352
00:13:37,200 --> 00:13:41,519
economic analysis matrix

353
00:13:39,040 --> 00:13:42,079
called the coast-based mtddl we

354
00:13:41,519 --> 00:13:46,000
preferred

355
00:13:42,079 --> 00:13:48,680
it as the cmtdo which considers the

356
00:13:46,000 --> 00:13:51,839
not only the reliability but also the

357
00:13:48,680 --> 00:13:54,719
misclassifications cost

358
00:13:51,839 --> 00:13:56,480
we use the t distributed stochastics

359
00:13:54,720 --> 00:13:58,639
naval embedding technique

360
00:13:56,480 --> 00:14:00,160
which can project high dimensional

361
00:13:58,639 --> 00:14:03,199
embedding space into two

362
00:14:00,160 --> 00:14:05,519
d spaces for data visualization

363
00:14:03,199 --> 00:14:08,000
while striving to keep data clustered

364
00:14:05,519 --> 00:14:12,000
together in the high dimensional space

365
00:14:08,000 --> 00:14:14,240
clustered clustered together in the low

366
00:14:12,000 --> 00:14:17,199
dimensional space as well

367
00:14:14,240 --> 00:14:18,720
this figure shows the result these two

368
00:14:17,199 --> 00:14:21,439
figures shows the results

369
00:14:18,720 --> 00:14:23,519
left figure highlights the challenges in

370
00:14:21,440 --> 00:14:25,600
failure detection of disks based on

371
00:14:23,519 --> 00:14:27,040
smart attributes in heterogeneous

372
00:14:25,600 --> 00:14:29,120
populations

373
00:14:27,040 --> 00:14:30,719
as can be from the right fields the

374
00:14:29,120 --> 00:14:33,199
healthy and failed discs

375
00:14:30,720 --> 00:14:34,480
are easily separated after embedding

376
00:14:33,199 --> 00:14:38,800
using our

377
00:14:34,480 --> 00:14:42,240
proposed approach this experiment result

378
00:14:38,800 --> 00:14:43,040
shows that our method can generate a

379
00:14:42,240 --> 00:14:45,519
unified

380
00:14:43,040 --> 00:14:46,880
and efficient high dimensional disk

381
00:14:45,519 --> 00:14:50,040
state imbalance for

382
00:14:46,880 --> 00:14:53,040
generic disk failure detection of

383
00:14:50,040 --> 00:14:55,199
heterogeneous disks

384
00:14:53,040 --> 00:14:58,160
it's interesting to investigate the

385
00:14:55,199 --> 00:15:00,800
detective performance of tda hddhe

386
00:14:58,160 --> 00:15:01,519
when online train on minority discrete

387
00:15:00,800 --> 00:15:03,760
set

388
00:15:01,519 --> 00:15:06,399
we've compared our approach with the

389
00:15:03,760 --> 00:15:08,639
other five state-of-the-art approaches

390
00:15:06,399 --> 00:15:09,600
it's not it's worthy to know that our

391
00:15:08,639 --> 00:15:11,600
hddse

392
00:15:09,600 --> 00:15:14,639
achieves the best performance in our

393
00:15:11,600 --> 00:15:17,040
cases the reason is twofold

394
00:15:14,639 --> 00:15:18,720
on the one hand the training pails

395
00:15:17,040 --> 00:15:21,519
generated by the our

396
00:15:18,720 --> 00:15:24,720
method are extremely large which makes

397
00:15:21,519 --> 00:15:27,040
our model not easy to get overfitting

398
00:15:24,720 --> 00:15:28,880
on the other hand the disk state

399
00:15:27,040 --> 00:15:31,920
imbalance in high dimension

400
00:15:28,880 --> 00:15:35,040
learned by our design lcdn-based

401
00:15:31,920 --> 00:15:37,439
service network is effective for model

402
00:15:35,040 --> 00:15:39,920
classification

403
00:15:37,440 --> 00:15:41,279
in order to investigate the model

404
00:15:39,920 --> 00:15:44,399
application

405
00:15:41,279 --> 00:15:45,920
applicability of our hddse we use the

406
00:15:44,399 --> 00:15:48,320
imbalance dataset of

407
00:15:45,920 --> 00:15:49,199
10 disk models from three disk

408
00:15:48,320 --> 00:15:52,160
manufacturers

409
00:15:49,199 --> 00:15:53,279
in two data centers to conduct this

410
00:15:52,160 --> 00:15:55,839
count this

411
00:15:53,279 --> 00:15:57,839
experiment when dealing with the disk

412
00:15:55,839 --> 00:16:00,399
from the different manufacturers data

413
00:15:57,839 --> 00:16:03,360
centers and even minority disk models

414
00:16:00,399 --> 00:16:05,199
hddse achieves high f machine values

415
00:16:03,360 --> 00:16:08,639
compared to other candidates

416
00:16:05,199 --> 00:16:10,880
the reason is that our our model maps

417
00:16:08,639 --> 00:16:12,320
low dimensional attributes to a general

418
00:16:10,880 --> 00:16:15,120
high dimension space

419
00:16:12,320 --> 00:16:16,320
that is not sensitive to the difference

420
00:16:15,120 --> 00:16:18,240
in disk models

421
00:16:16,320 --> 00:16:19,519
which lead to the detection to be

422
00:16:18,240 --> 00:16:23,759
performed where

423
00:16:19,519 --> 00:16:28,000
using a unified distance calculation

424
00:16:23,759 --> 00:16:29,440
we evaluate the um the adaptability of

425
00:16:28,000 --> 00:16:32,800
our hddse

426
00:16:29,440 --> 00:16:35,519
which is how an approach can

427
00:16:32,800 --> 00:16:38,079
adapt to a disk model that has not

428
00:16:35,519 --> 00:16:41,120
appealed in the training data set

429
00:16:38,079 --> 00:16:43,279
this figure shows the result augmented

430
00:16:41,120 --> 00:16:45,279
using different hybrid datasets or

431
00:16:43,279 --> 00:16:45,680
achieved better detected performance

432
00:16:45,279 --> 00:16:48,839
than

433
00:16:45,680 --> 00:16:50,800
other candidates it indicates that our

434
00:16:48,839 --> 00:16:54,000
hdbse does not

435
00:16:50,800 --> 00:16:56,000
need doesn't does not need to establish

436
00:16:54,000 --> 00:16:58,399
or maintain a new model

437
00:16:56,000 --> 00:17:00,800
and own strong adaptability that can

438
00:16:58,399 --> 00:17:03,199
completely adapt to a new disk model

439
00:17:00,800 --> 00:17:06,000
regardless of these manufacturers

440
00:17:03,199 --> 00:17:07,839
disk models data centers and minority

441
00:17:06,000 --> 00:17:10,079
disks

442
00:17:07,839 --> 00:17:10,958
this table shows our approaches and

443
00:17:10,079 --> 00:17:14,079
improves the

444
00:17:10,959 --> 00:17:17,600
cost mtd video by about two orders of

445
00:17:14,079 --> 00:17:21,198
magnitude than the other four candidates

446
00:17:17,599 --> 00:17:23,839
in other words hddse greatly improved

447
00:17:21,199 --> 00:17:24,400
the reliability of the storage system at

448
00:17:23,839 --> 00:17:26,799
a low

449
00:17:24,400 --> 00:17:26,799
cost

450
00:17:29,679 --> 00:17:33,840
this figure provides the field time

451
00:17:31,760 --> 00:17:36,320
statistics for each approach

452
00:17:33,840 --> 00:17:36,879
dad has the lowest training time and

453
00:17:36,320 --> 00:17:39,760
highest

454
00:17:36,880 --> 00:17:40,320
detecting time due to its easy training

455
00:17:39,760 --> 00:17:43,600
method

456
00:17:40,320 --> 00:17:47,039
and large computation for of the

457
00:17:43,600 --> 00:17:47,520
calculating distances other approaches

458
00:17:47,039 --> 00:17:50,160
at

459
00:17:47,520 --> 00:17:52,160
httpse takes the second highest training

460
00:17:50,160 --> 00:17:54,080
time followed by the random four race

461
00:17:52,160 --> 00:17:54,640
and the second high aesthetic detection

462
00:17:54,080 --> 00:17:57,520
time

463
00:17:54,640 --> 00:18:00,080
followed by the dad method considering

464
00:17:57,520 --> 00:18:02,879
the training tasks in a dataset and

465
00:18:00,080 --> 00:18:04,000
sometimes the detective models are

466
00:18:02,880 --> 00:18:08,240
updated weekly

467
00:18:04,000 --> 00:18:11,120
or monthly so the time cost of 2d hdbse

468
00:18:08,240 --> 00:18:11,120
is accessible

469
00:18:12,480 --> 00:18:16,799
we last simulate practical long-term

470
00:18:14,880 --> 00:18:19,760
availability in data centers

471
00:18:16,799 --> 00:18:20,639
tencent this figure shows the tbr and

472
00:18:19,760 --> 00:18:23,039
fpr

473
00:18:20,640 --> 00:18:25,600
of the different detective approaches in

474
00:18:23,039 --> 00:18:29,280
the fall in 15 months

475
00:18:25,600 --> 00:18:31,600
hdds issues the higher tpr and lower fpr

476
00:18:29,280 --> 00:18:32,320
compared to the other state-of-the-art

477
00:18:31,600 --> 00:18:35,280
method

478
00:18:32,320 --> 00:18:37,678
it's worth it to note that in hddse

479
00:18:35,280 --> 00:18:38,000
exhibit the stable detective performance

480
00:18:37,679 --> 00:18:41,679
than

481
00:18:38,000 --> 00:18:45,600
other candidates we attribute this to

482
00:18:41,679 --> 00:18:46,160
the llcm based network which is well

483
00:18:45,600 --> 00:18:48,559
learned

484
00:18:46,160 --> 00:18:49,760
from the dynamically changed long-term

485
00:18:48,559 --> 00:18:53,760
behavior of these

486
00:18:49,760 --> 00:18:57,039
stereos so in this paper we proposed

487
00:18:53,760 --> 00:18:59,360
the hdd-se an lcm based cell means

488
00:18:57,039 --> 00:18:59,840
network that can learn the dynamically

489
00:18:59,360 --> 00:19:01,840
changed

490
00:18:59,840 --> 00:19:02,879
long-term behavior of these healthy

491
00:19:01,840 --> 00:19:05,439
studios

492
00:19:02,880 --> 00:19:06,720
and generate a unified and efficient

493
00:19:05,440 --> 00:19:08,880
high dimensional

494
00:19:06,720 --> 00:19:10,080
disk state imbalance from low

495
00:19:08,880 --> 00:19:12,799
dimensional spot

496
00:19:10,080 --> 00:19:14,080
attributes for this failure detection it

497
00:19:12,799 --> 00:19:16,639
has good detective

498
00:19:14,080 --> 00:19:17,600
adaptability to the disks which have not

499
00:19:16,640 --> 00:19:20,160
appeared in

500
00:19:17,600 --> 00:19:22,080
training data center and deliver good

501
00:19:20,160 --> 00:19:23,760
performance for the imbalanced or

502
00:19:22,080 --> 00:19:25,840
minority disk datasets

503
00:19:23,760 --> 00:19:27,200
thus improving the storage system

504
00:19:25,840 --> 00:19:30,159
availability

505
00:19:27,200 --> 00:19:32,400
furthermore the proposed approach

506
00:19:30,160 --> 00:19:32,960
improves the reliability of the data

507
00:19:32,400 --> 00:19:36,559
center

508
00:19:32,960 --> 00:19:39,840
and exhibits long-term availability

509
00:19:36,559 --> 00:19:39,840
thank you

