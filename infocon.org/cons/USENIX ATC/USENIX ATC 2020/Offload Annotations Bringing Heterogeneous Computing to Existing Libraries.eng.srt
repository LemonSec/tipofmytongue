1
00:00:09,840 --> 00:00:12,160
today i'll be presenting offload

2
00:00:11,360 --> 00:00:13,840
annotations

3
00:00:12,160 --> 00:00:15,599
a new approach for heterogeneous

4
00:00:13,840 --> 00:00:16,720
computing and existing libraries and

5
00:00:15,599 --> 00:00:18,560
workloads

6
00:00:16,720 --> 00:00:20,240
i'm gina and this is joint work with my

7
00:00:18,560 --> 00:00:22,000
colleagues at stanford shomick and

8
00:00:20,240 --> 00:00:24,400
deepak and my advisor mates

9
00:00:22,000 --> 00:00:24,400
arya

10
00:00:25,119 --> 00:00:29,198
the cloud has recently made specialized

11
00:00:26,960 --> 00:00:30,240
hardware like gpus cheaper and easier to

12
00:00:29,199 --> 00:00:32,079
use

13
00:00:30,240 --> 00:00:33,760
what used to be thousands of dollars to

14
00:00:32,079 --> 00:00:35,360
buy for yourself now costs

15
00:00:33,760 --> 00:00:37,360
a few dollars per hour to rent on the

16
00:00:35,360 --> 00:00:39,280
cloud as a result

17
00:00:37,360 --> 00:00:41,200
people who've never used this technology

18
00:00:39,280 --> 00:00:45,520
before are looking for new ways to speed

19
00:00:41,200 --> 00:00:47,680
up their applications using hardware

20
00:00:45,520 --> 00:00:49,440
for some background on how gpus might be

21
00:00:47,680 --> 00:00:51,760
used to speed up your application let's

22
00:00:49,440 --> 00:00:53,920
compare them to cpus

23
00:00:51,760 --> 00:00:56,399
cpus are the most common processor found

24
00:00:53,920 --> 00:00:58,559
on our personal computers

25
00:00:56,399 --> 00:00:59,920
while a typical cpu might be able to

26
00:00:58,559 --> 00:01:02,640
parallelize tasks

27
00:00:59,920 --> 00:01:04,400
four ways a gpu has effectively

28
00:01:02,640 --> 00:01:05,199
thousands of cores that can concurrently

29
00:01:04,400 --> 00:01:08,400
execute

30
00:01:05,199 --> 00:01:12,880
many simpler tasks for example matrix

31
00:01:08,400 --> 00:01:14,720
edition is particularly fast on the gpu

32
00:01:12,880 --> 00:01:16,399
however there's also disadvantages to

33
00:01:14,720 --> 00:01:18,400
using the gpu

34
00:01:16,400 --> 00:01:19,840
in general gpus have less memory than

35
00:01:18,400 --> 00:01:22,960
their cpu counterparts

36
00:01:19,840 --> 00:01:24,799
and crash if they run out of memory

37
00:01:22,960 --> 00:01:26,640
in addition if you want to use both

38
00:01:24,799 --> 00:01:28,320
processors in the same workload

39
00:01:26,640 --> 00:01:30,560
like if only part of the task can be

40
00:01:28,320 --> 00:01:32,158
parallelized you have to transfer your

41
00:01:30,560 --> 00:01:33,759
data to ensure that it's on the same

42
00:01:32,159 --> 00:01:35,439
device as the function that's using the

43
00:01:33,759 --> 00:01:37,520
data

44
00:01:35,439 --> 00:01:38,960
the time it takes to move data around is

45
00:01:37,520 --> 00:01:40,560
proportional to its size

46
00:01:38,960 --> 00:01:42,640
and can be a costly bottleneck in

47
00:01:40,560 --> 00:01:45,280
heterogeneous applications that use both

48
00:01:42,640 --> 00:01:45,280
processors

49
00:01:46,399 --> 00:01:50,720
with the tradeoffs of cpus and gpus in

50
00:01:48,799 --> 00:01:52,560
mind let's talk about data science

51
00:01:50,720 --> 00:01:54,399
an application that started out using

52
00:01:52,560 --> 00:01:56,399
the cpu

53
00:01:54,399 --> 00:01:58,479
data science is done almost entirely in

54
00:01:56,399 --> 00:02:00,240
python a scripting language that's easy

55
00:01:58,479 --> 00:02:02,640
to write

56
00:02:00,240 --> 00:02:04,798
the result is an ecosystem of popular

57
00:02:02,640 --> 00:02:08,239
python libraries for doing data science

58
00:02:04,799 --> 00:02:09,759
on the cpu even if you don't do data

59
00:02:08,239 --> 00:02:11,280
science you're probably familiar with

60
00:02:09,758 --> 00:02:14,720
these libraries like numpy

61
00:02:11,280 --> 00:02:14,720
pandas and scikit-learn

62
00:02:16,160 --> 00:02:19,760
data science often involves analyzing

63
00:02:18,560 --> 00:02:22,959
large amounts of data

64
00:02:19,760 --> 00:02:24,720
which makes it a natural fit for the gpu

65
00:02:22,959 --> 00:02:26,879
now that gpus are cheaper data

66
00:02:24,720 --> 00:02:28,480
scientists are looking for ways to bring

67
00:02:26,879 --> 00:02:30,720
heterogeneous computing to their

68
00:02:28,480 --> 00:02:33,119
existing workloads

69
00:02:30,720 --> 00:02:36,239
the result is a similar ecosystem of

70
00:02:33,120 --> 00:02:38,319
python libraries before the gpu

71
00:02:36,239 --> 00:02:40,560
these python libraries are very similar

72
00:02:38,319 --> 00:02:42,560
to their cpu library counterparts

73
00:02:40,560 --> 00:02:45,040
claiming to mirror the apis of the

74
00:02:42,560 --> 00:02:47,360
libraries that we saw earlier

75
00:02:45,040 --> 00:02:48,079
the idea is for example once a data

76
00:02:47,360 --> 00:02:50,720
scientist

77
00:02:48,080 --> 00:02:52,560
knows how to use numpy on the cpu they

78
00:02:50,720 --> 00:02:54,480
can just as easily learn how to use

79
00:02:52,560 --> 00:02:57,440
pytorch on the gpu because they share

80
00:02:54,480 --> 00:02:57,440
the same api

81
00:02:58,560 --> 00:03:03,200
in fact the online documentation for

82
00:03:00,720 --> 00:03:05,040
these gpu libraries all take great pride

83
00:03:03,200 --> 00:03:06,640
in their similarities to their cpu

84
00:03:05,040 --> 00:03:09,200
library counterparts

85
00:03:06,640 --> 00:03:10,559
for example qdf provides a pandas-like

86
00:03:09,200 --> 00:03:13,200
api

87
00:03:10,560 --> 00:03:15,200
kupa's interface is highly compatible

88
00:03:13,200 --> 00:03:16,560
with numpy and can be used as a drop in

89
00:03:15,200 --> 00:03:18,799
replacement

90
00:03:16,560 --> 00:03:21,680
pytorch is a replacement for numpy and

91
00:03:18,800 --> 00:03:23,680
qml's python api matches the api from

92
00:03:21,680 --> 00:03:25,599
scikit-learn

93
00:03:23,680 --> 00:03:27,440
these new gpu libraries all seem to

94
00:03:25,599 --> 00:03:29,760
promise a very seamless integration

95
00:03:27,440 --> 00:03:31,519
experience for using the gpu

96
00:03:29,760 --> 00:03:33,840
but are they really as easy to use as

97
00:03:31,519 --> 00:03:33,840
they seem

98
00:03:35,519 --> 00:03:40,239
consider porting this machine learning

99
00:03:37,519 --> 00:03:42,080
workload to the gpu

100
00:03:40,239 --> 00:03:45,040
in this workload we pre-process the

101
00:03:42,080 --> 00:03:46,640
input data x-train and train the model

102
00:03:45,040 --> 00:03:48,798
then we test the model on a different

103
00:03:46,640 --> 00:03:52,319
data set x test

104
00:03:48,799 --> 00:03:54,319
at the end we plot the results

105
00:03:52,319 --> 00:03:56,640
recall from the previous slide that cool

106
00:03:54,319 --> 00:03:59,119
ml's python api matches the api from

107
00:03:56,640 --> 00:04:01,040
scikit-learn so we try to use qml

108
00:03:59,120 --> 00:04:04,159
as a drop-in replacement for the cpu

109
00:04:01,040 --> 00:04:04,159
library scikit-learn

110
00:04:05,920 --> 00:04:09,599
we look for functions in the qml

111
00:04:07,760 --> 00:04:11,280
documentation that correspond to the

112
00:04:09,599 --> 00:04:13,518
library functions in scikit-learn and

113
00:04:11,280 --> 00:04:15,519
then we replace them in our code

114
00:04:13,519 --> 00:04:16,639
we find that some like standard scalar

115
00:04:15,519 --> 00:04:20,079
don't exist at all

116
00:04:16,639 --> 00:04:20,079
so we leave the original version

117
00:04:20,880 --> 00:04:25,199
remember that when using both processors

118
00:04:23,120 --> 00:04:26,960
data has to be on the same device as the

119
00:04:25,199 --> 00:04:29,040
function that's using it

120
00:04:26,960 --> 00:04:31,120
since the inputs to the program x-train

121
00:04:29,040 --> 00:04:31,840
y-train and x-test are usually read from

122
00:04:31,120 --> 00:04:34,000
another step

123
00:04:31,840 --> 00:04:35,919
in the pipeline we assume they start out

124
00:04:34,000 --> 00:04:38,000
on the cpu

125
00:04:35,919 --> 00:04:39,198
since standard scalar is on the cpu but

126
00:04:38,000 --> 00:04:40,960
pca is not

127
00:04:39,199 --> 00:04:43,440
we transfer the outputs of standard

128
00:04:40,960 --> 00:04:44,799
scalar to the gpu

129
00:04:43,440 --> 00:04:46,800
and then finally since our plotting

130
00:04:44,800 --> 00:04:47,840
library requires the data to be on the

131
00:04:46,800 --> 00:04:49,680
cpu

132
00:04:47,840 --> 00:04:52,320
we transfer the results back at the end

133
00:04:49,680 --> 00:04:52,320
of the program

134
00:04:52,720 --> 00:04:56,080
now we run the program but then it

135
00:04:54,639 --> 00:04:59,280
crashes in the predict phase

136
00:04:56,080 --> 00:05:01,520
due to insufficient gpu memory remember

137
00:04:59,280 --> 00:05:04,159
from earlier that gpus have less memory

138
00:05:01,520 --> 00:05:05,599
than their cpu counterparts

139
00:05:04,160 --> 00:05:07,360
since prediction can be done

140
00:05:05,600 --> 00:05:07,840
independently on different batches of

141
00:05:07,360 --> 00:05:10,479
data

142
00:05:07,840 --> 00:05:12,799
we manually split the x test data into

143
00:05:10,479 --> 00:05:14,560
batches that fit in gpu memory

144
00:05:12,800 --> 00:05:17,039
and then independently execute the

145
00:05:14,560 --> 00:05:19,199
prediction phase for each batch

146
00:05:17,039 --> 00:05:20,719
thus by paging the data only a small

147
00:05:19,199 --> 00:05:24,400
amount of data is in the gpu

148
00:05:20,720 --> 00:05:26,639
at any time

149
00:05:24,400 --> 00:05:28,239
the program runs to completion but its

150
00:05:26,639 --> 00:05:31,120
performance is not much better than

151
00:05:28,240 --> 00:05:33,440
before the code became so complex

152
00:05:31,120 --> 00:05:35,520
we think a different combination of cpu

153
00:05:33,440 --> 00:05:36,639
and gpu functions might make the program

154
00:05:35,520 --> 00:05:38,639
run faster

155
00:05:36,639 --> 00:05:40,240
but since we manually inserted all those

156
00:05:38,639 --> 00:05:41,919
data transfer statements and replaced

157
00:05:40,240 --> 00:05:43,520
those functions ourselves

158
00:05:41,919 --> 00:05:46,639
trying different schedules would also

159
00:05:43,520 --> 00:05:48,880
require a lot of manual work

160
00:05:46,639 --> 00:05:53,840
it turns out that gpu libraries are not

161
00:05:48,880 --> 00:05:53,840
that easy to use after all

162
00:05:54,080 --> 00:05:59,599
our solution is offload annotations

163
00:05:57,120 --> 00:06:01,919
with offload annotations or oas we

164
00:05:59,600 --> 00:06:03,919
enable heterogeneous gpu computing

165
00:06:01,919 --> 00:06:06,240
and existing workloads with little to no

166
00:06:03,919 --> 00:06:08,400
code modifications

167
00:06:06,240 --> 00:06:10,080
our design involves three parties an

168
00:06:08,400 --> 00:06:12,880
annotator an end user

169
00:06:10,080 --> 00:06:15,919
and a runtime we'll discuss the role of

170
00:06:12,880 --> 00:06:15,919
each party in the design

171
00:06:16,560 --> 00:06:21,680
our goals for offload annotations are

172
00:06:18,960 --> 00:06:23,919
derived from our motivating example

173
00:06:21,680 --> 00:06:25,600
with less developer effort we'd like to

174
00:06:23,919 --> 00:06:28,240
at least match the performance

175
00:06:25,600 --> 00:06:31,039
of handwritten implementations using gpu

176
00:06:28,240 --> 00:06:33,440
libraries as a drop in replacement

177
00:06:31,039 --> 00:06:35,440
we'd like to automatically split in page

178
00:06:33,440 --> 00:06:37,280
data to scale to data sizes that

179
00:06:35,440 --> 00:06:38,960
normally cause the gpu to run out of

180
00:06:37,280 --> 00:06:40,799
memory

181
00:06:38,960 --> 00:06:42,239
and we'd like to beat the performance of

182
00:06:40,800 --> 00:06:44,720
the original workload rooting for the

183
00:06:42,240 --> 00:06:44,720
cpu

184
00:06:46,160 --> 00:06:51,759
step one the annotator

185
00:06:49,199 --> 00:06:53,759
instead of making intrusive changes to

186
00:06:51,759 --> 00:06:55,840
the original workload or the original

187
00:06:53,759 --> 00:06:58,319
cpu library we use a technique called

188
00:06:55,840 --> 00:07:00,400
annotations the annotator takes

189
00:06:58,319 --> 00:07:02,560
advantage of the mirroring apis

190
00:07:00,400 --> 00:07:04,638
between corresponding cpu and gpu

191
00:07:02,560 --> 00:07:06,720
libraries

192
00:07:04,639 --> 00:07:08,319
to write these non-intrusive offload

193
00:07:06,720 --> 00:07:10,720
annotations

194
00:07:08,319 --> 00:07:11,520
in this case the annotator picks two

195
00:07:10,720 --> 00:07:13,919
libraries

196
00:07:11,520 --> 00:07:17,840
the cpu library numpy and annotates them

197
00:07:13,919 --> 00:07:17,840
with the gpu library pi torch

198
00:07:18,400 --> 00:07:22,159
after identifying the libraries the

199
00:07:20,080 --> 00:07:23,039
annotator then identifies corresponding

200
00:07:22,160 --> 00:07:25,360
functions

201
00:07:23,039 --> 00:07:27,039
and uses the oa notation to link the two

202
00:07:25,360 --> 00:07:28,639
functions

203
00:07:27,039 --> 00:07:30,639
even though the two libraries claim to

204
00:07:28,639 --> 00:07:31,520
have similar apis their function names

205
00:07:30,639 --> 00:07:35,759
can still differ

206
00:07:31,520 --> 00:07:35,758
like torch.mole and numpy.multiply

207
00:07:38,479 --> 00:07:42,240
the annotator also needs to include the

208
00:07:40,560 --> 00:07:44,080
types of the inputs and the outputs in

209
00:07:42,240 --> 00:07:46,160
each function annotation

210
00:07:44,080 --> 00:07:48,318
to do this the annotator uses a special

211
00:07:46,160 --> 00:07:50,400
type called the offload split type

212
00:07:48,319 --> 00:07:53,520
which is a wrapper around an actual type

213
00:07:50,400 --> 00:07:55,359
in this case numpy's nd array

214
00:07:53,520 --> 00:07:57,120
the offload split type allows the

215
00:07:55,360 --> 00:07:59,120
scheduler to dynamically infer

216
00:07:57,120 --> 00:08:01,759
information about the inputs and outputs

217
00:07:59,120 --> 00:08:01,759
at runtime

218
00:08:02,960 --> 00:08:07,039
the annotator can also annotate

219
00:08:05,120 --> 00:08:09,199
allocation functions

220
00:08:07,039 --> 00:08:11,440
allocation annotations are later used by

221
00:08:09,199 --> 00:08:13,120
the runtime to optimize data transfers

222
00:08:11,440 --> 00:08:14,639
since data transfers can take a lot of

223
00:08:13,120 --> 00:08:16,479
time

224
00:08:14,639 --> 00:08:19,280
note that allocation functions only have

225
00:08:16,479 --> 00:08:19,280
a return type

226
00:08:21,440 --> 00:08:24,560
with the function annotations in place

227
00:08:23,520 --> 00:08:27,840
what exactly is

228
00:08:24,560 --> 00:08:27,840
in an offload split type

229
00:08:29,360 --> 00:08:33,279
offload split types convey information

230
00:08:31,440 --> 00:08:35,838
to the runtime through an interface that

231
00:08:33,279 --> 00:08:37,838
the annotator implements for each type

232
00:08:35,839 --> 00:08:39,599
the interface consists of five function

233
00:08:37,839 --> 00:08:41,440
calls

234
00:08:39,599 --> 00:08:43,680
the first two function calls are part of

235
00:08:41,440 --> 00:08:46,240
the offloading api

236
00:08:43,679 --> 00:08:48,160
the device function specifies which

237
00:08:46,240 --> 00:08:49,360
device the value is on the cpu or the

238
00:08:48,160 --> 00:08:51,279
gpu

239
00:08:49,360 --> 00:08:53,680
and the two function transfers the value

240
00:08:51,279 --> 00:08:55,519
from one device to another

241
00:08:53,680 --> 00:08:57,199
the offloading api is used to

242
00:08:55,519 --> 00:08:58,959
automatically transfer data

243
00:08:57,200 --> 00:09:01,920
which we had to do manually in the

244
00:08:58,959 --> 00:09:01,920
motivating example

245
00:09:02,880 --> 00:09:06,240
to make the interface for offload split

246
00:09:04,880 --> 00:09:08,160
types concrete we'll look at an

247
00:09:06,240 --> 00:09:11,200
implementation of nd array type

248
00:09:08,160 --> 00:09:12,800
for the api in this case device

249
00:09:11,200 --> 00:09:16,080
checks the type of the object whether

250
00:09:12,800 --> 00:09:18,399
it's a torch tensor or a numpy nd array

251
00:09:16,080 --> 00:09:23,839
and two uses the transfer api from the

252
00:09:18,399 --> 00:09:23,839
pytorch library to move data around

253
00:09:24,480 --> 00:09:28,800
the next three function calls and

254
00:09:25,920 --> 00:09:31,279
interface are part of the splitting api

255
00:09:28,800 --> 00:09:31,920
the splitting api returns the number of

256
00:09:31,279 --> 00:09:33,680
elements

257
00:09:31,920 --> 00:09:35,120
that a value can be split in and then

258
00:09:33,680 --> 00:09:37,439
describes how to

259
00:09:35,120 --> 00:09:39,279
correctly split data into data parallel

260
00:09:37,440 --> 00:09:41,200
chunks and then how to merge them back

261
00:09:39,279 --> 00:09:43,279
together

262
00:09:41,200 --> 00:09:45,760
the splitting api is the same as the one

263
00:09:43,279 --> 00:09:46,080
in the split annotations or mozart paper

264
00:09:45,760 --> 00:09:50,240
from

265
00:09:46,080 --> 00:09:52,640
sosp mozart uses the splitting api for

266
00:09:50,240 --> 00:09:54,640
cache pipelining on the cpu while we use

267
00:09:52,640 --> 00:09:55,360
splitting for the totally new use case

268
00:09:54,640 --> 00:09:57,600
of

269
00:09:55,360 --> 00:09:59,440
automatically paging data sets into gpu

270
00:09:57,600 --> 00:10:01,040
memory

271
00:09:59,440 --> 00:10:05,600
this is what we had to do manually in

272
00:10:01,040 --> 00:10:08,319
the motivating example

273
00:10:05,600 --> 00:10:10,560
since numpy's nd array is a matrix the

274
00:10:08,320 --> 00:10:12,560
splitting api for the nd array type is

275
00:10:10,560 --> 00:10:14,880
essentially matrix operations over the

276
00:10:12,560 --> 00:10:14,880
value

277
00:10:16,959 --> 00:10:21,760
overall the offload split type interface

278
00:10:19,440 --> 00:10:23,519
can be easily generalized

279
00:10:21,760 --> 00:10:25,439
we were able to implement the interface

280
00:10:23,519 --> 00:10:27,120
for other common types like pandas data

281
00:10:25,440 --> 00:10:29,760
frames and machine learning models in

282
00:10:27,120 --> 00:10:29,760
scikit-learn

283
00:10:31,519 --> 00:10:36,720
step two the end user the end user is

284
00:10:34,959 --> 00:10:38,079
the person who is actually responsible

285
00:10:36,720 --> 00:10:40,240
for writing application code and

286
00:10:38,079 --> 00:10:42,959
pointing it to the gpu

287
00:10:40,240 --> 00:10:44,720
in this case the application is a simple

288
00:10:42,959 --> 00:10:48,079
python program that uses many of the

289
00:10:44,720 --> 00:10:49,920
functions we previously annotated

290
00:10:48,079 --> 00:10:52,239
note that the end user doesn't have to

291
00:10:49,920 --> 00:10:54,560
be the same person as the annotator

292
00:10:52,240 --> 00:10:55,440
in fact we imagine an ecosystem similar

293
00:10:54,560 --> 00:10:57,599
to typescript

294
00:10:55,440 --> 00:10:59,440
in which a small number of annotators

295
00:10:57,600 --> 00:11:01,279
write these annotation files

296
00:10:59,440 --> 00:11:04,399
that can be shared amongst a large

297
00:11:01,279 --> 00:11:04,399
number of end users

298
00:11:08,560 --> 00:11:12,640
to use the annotated library the end

299
00:11:10,399 --> 00:11:13,839
user simply changes the import to import

300
00:11:12,640 --> 00:11:16,560
the annotated library

301
00:11:13,839 --> 00:11:17,839
instead now the program instead of

302
00:11:16,560 --> 00:11:19,680
calling actual function

303
00:11:17,839 --> 00:11:22,160
calls our annotated wrappers to these

304
00:11:19,680 --> 00:11:22,160
functions

305
00:11:23,680 --> 00:11:27,760
since the annotated library calls

306
00:11:25,360 --> 00:11:28,880
wrapper that feeds into our run time the

307
00:11:27,760 --> 00:11:31,200
runtime doesn't actually

308
00:11:28,880 --> 00:11:32,240
evaluate any values until it's required

309
00:11:31,200 --> 00:11:33,839
to

310
00:11:32,240 --> 00:11:35,440
thus if the end user needs to

311
00:11:33,839 --> 00:11:37,519
materialize the value say

312
00:11:35,440 --> 00:11:39,600
to use in a plotting library they have

313
00:11:37,519 --> 00:11:43,839
to explicitly call the evaluate function

314
00:11:39,600 --> 00:11:43,839
to invoke the runtime

315
00:11:44,240 --> 00:11:49,760
step three the runtime so each time the

316
00:11:47,680 --> 00:11:51,839
program calls an annotated function the

317
00:11:49,760 --> 00:11:54,000
runtime adds a node to this lazy

318
00:11:51,839 --> 00:11:55,839
computation graph

319
00:11:54,000 --> 00:11:58,079
the graph on this slide corresponds to a

320
00:11:55,839 --> 00:12:00,079
python program from earlier

321
00:11:58,079 --> 00:12:02,479
so in the graph nodes represent a

322
00:12:00,079 --> 00:12:04,479
function call and dotted lines represent

323
00:12:02,480 --> 00:12:06,160
an allocation function

324
00:12:04,480 --> 00:12:08,160
the edges in the graph represent data

325
00:12:06,160 --> 00:12:10,480
dependencies

326
00:12:08,160 --> 00:12:12,319
when evaluate is called the runtime does

327
00:12:10,480 --> 00:12:14,240
a topological sort of the graph

328
00:12:12,320 --> 00:12:17,760
and produces a list of instructions that

329
00:12:14,240 --> 00:12:17,760
satisfy data dependencies

330
00:12:20,000 --> 00:12:24,399
next bach assigns each function to

331
00:12:22,160 --> 00:12:26,800
either the cpu or the gpu based on a

332
00:12:24,399 --> 00:12:29,519
naive scheduling algorithm

333
00:12:26,800 --> 00:12:30,639
the algorithm is if the function has a

334
00:12:29,519 --> 00:12:33,519
function annotation

335
00:12:30,639 --> 00:12:36,639
with a gpu library implementation then

336
00:12:33,519 --> 00:12:39,680
bach will assign the function to the gpu

337
00:12:36,639 --> 00:12:41,600
otherwise it assigns it to the cpu

338
00:12:39,680 --> 00:12:43,599
in this case since we didn't annotate

339
00:12:41,600 --> 00:12:45,519
the arcsine function earlier

340
00:12:43,600 --> 00:12:46,959
with the gpu function it gets assigned

341
00:12:45,519 --> 00:12:48,880
to the cpu

342
00:12:46,959 --> 00:12:50,479
however since the multiply and square

343
00:12:48,880 --> 00:12:52,000
root do have

344
00:12:50,480 --> 00:12:54,160
function annotations they get assigned

345
00:12:52,000 --> 00:12:56,240
to the gpu

346
00:12:54,160 --> 00:12:58,399
our scheduling algorithm is not meant to

347
00:12:56,240 --> 00:12:59,440
be optimal and definitely has room for

348
00:12:58,399 --> 00:13:03,279
improvement

349
00:12:59,440 --> 00:13:03,279
but it was sufficient for our workloads

350
00:13:04,800 --> 00:13:09,839
next the runtime assigns allocation

351
00:13:06,800 --> 00:13:12,000
functions to either the cpu or the gpu

352
00:13:09,839 --> 00:13:13,839
the algorithm for this is to look at the

353
00:13:12,000 --> 00:13:15,519
first node in the graph that uses the

354
00:13:13,839 --> 00:13:17,519
allocated data and to assign the

355
00:13:15,519 --> 00:13:18,800
allocation to the same device as that

356
00:13:17,519 --> 00:13:21,279
node

357
00:13:18,800 --> 00:13:23,839
for example when assigning node 3 it

358
00:13:21,279 --> 00:13:26,560
looks at the function node 4.

359
00:13:23,839 --> 00:13:29,839
since node 4 executes on the gpu node 3

360
00:13:26,560 --> 00:13:32,638
is also assigned to the gpu

361
00:13:29,839 --> 00:13:35,200
consider if we instead assign node 3 to

362
00:13:32,639 --> 00:13:37,200
the cpu then we'd have to first transfer

363
00:13:35,200 --> 00:13:40,480
the data from node 3

364
00:13:37,200 --> 00:13:42,480
to the gpu before we can execute node 4.

365
00:13:40,480 --> 00:13:46,160
and since data transfers can take a lot

366
00:13:42,480 --> 00:13:46,160
of time we like to avoid them when

367
00:13:46,839 --> 00:13:51,760
possible

368
00:13:49,839 --> 00:13:53,279
now that all functions and allocations

369
00:13:51,760 --> 00:13:55,760
have been assigned to a device

370
00:13:53,279 --> 00:13:57,760
bach uses the offloading api from

371
00:13:55,760 --> 00:13:59,519
earlier to automatically manage data

372
00:13:57,760 --> 00:14:02,560
transfers

373
00:13:59,519 --> 00:14:05,040
so when looking at node 4 bach will use

374
00:14:02,560 --> 00:14:05,599
the device api to figure out that the

375
00:14:05,040 --> 00:14:09,120
inputs

376
00:14:05,600 --> 00:14:11,279
from node two are located on the cpu

377
00:14:09,120 --> 00:14:12,639
then bach will use the two api to

378
00:14:11,279 --> 00:14:17,839
transfer those inputs

379
00:14:12,639 --> 00:14:17,839
to the gpu

380
00:14:19,519 --> 00:14:23,600
if the splitting api is implemented bach

381
00:14:22,079 --> 00:14:25,519
will use the size api

382
00:14:23,600 --> 00:14:26,639
to estimate the size of the working data

383
00:14:25,519 --> 00:14:28,839
set

384
00:14:26,639 --> 00:14:30,000
it'll compare it to the size of gpu

385
00:14:28,839 --> 00:14:33,360
memory

386
00:14:30,000 --> 00:14:34,000
if it needs to if it's larger than gpu

387
00:14:33,360 --> 00:14:36,480
memory

388
00:14:34,000 --> 00:14:39,279
a bach will use the split api to split

389
00:14:36,480 --> 00:14:41,199
the data into a manageable chunk

390
00:14:39,279 --> 00:14:43,279
then at the end of execution it will use

391
00:14:41,199 --> 00:14:45,359
the merge api to merge the values back

392
00:14:43,279 --> 00:14:47,519
together

393
00:14:45,360 --> 00:14:49,519
altogether by leveraging the offloading

394
00:14:47,519 --> 00:14:50,560
and splitting apis implemented by the

395
00:14:49,519 --> 00:14:52,240
annotator

396
00:14:50,560 --> 00:14:54,239
the bach runtime can automatically

397
00:14:52,240 --> 00:14:59,839
manage data transfers and scheduling

398
00:14:54,240 --> 00:14:59,839
in a heterogeneous environment

399
00:15:00,560 --> 00:15:04,638
to improve on the naive scheduling

400
00:15:02,639 --> 00:15:05,760
algorithm we introduced a way for the

401
00:15:04,639 --> 00:15:08,000
scheduler to make

402
00:15:05,760 --> 00:15:09,920
slightly more informed decisions based

403
00:15:08,000 --> 00:15:11,839
on some heuristics

404
00:15:09,920 --> 00:15:14,160
so say the runtime is trying to assign

405
00:15:11,839 --> 00:15:17,279
node 4 to a device

406
00:15:14,160 --> 00:15:19,439
it'll do a cost benefit analysis of

407
00:15:17,279 --> 00:15:20,959
either executing the function directly

408
00:15:19,440 --> 00:15:23,680
on the cpu

409
00:15:20,959 --> 00:15:24,560
or transferring its inputs derived from

410
00:15:23,680 --> 00:15:26,319
node 2

411
00:15:24,560 --> 00:15:29,599
to the gpu and then doing the

412
00:15:26,320 --> 00:15:29,600
computation on the gpu

413
00:15:29,759 --> 00:15:33,759
at large data sizes it might be totally

414
00:15:32,000 --> 00:15:35,839
worth it to transfer the data

415
00:15:33,759 --> 00:15:41,839
to the gpu first since computing on the

416
00:15:35,839 --> 00:15:41,839
gpu is so much faster

417
00:15:44,160 --> 00:15:48,639
on the other hand when the data size is

418
00:15:46,160 --> 00:15:50,639
small it might be faster to just do the

419
00:15:48,639 --> 00:15:51,920
computation on the cpu and avoid the

420
00:15:50,639 --> 00:15:55,839
overheads

421
00:15:51,920 --> 00:15:55,839
of transferring data to the gpu

422
00:15:56,079 --> 00:16:01,758
so using the heuristics the runtime

423
00:15:57,920 --> 00:16:01,759
would assign node 4 to the cpu

424
00:16:06,720 --> 00:16:10,800
so what do these heuristics look like in

425
00:16:08,399 --> 00:16:12,399
the code the heuristics are actually

426
00:16:10,800 --> 00:16:13,120
python functions provided by the

427
00:16:12,399 --> 00:16:15,600
annotator

428
00:16:13,120 --> 00:16:18,000
in the function annotations the

429
00:16:15,600 --> 00:16:20,000
heuristics are functions of runtime data

430
00:16:18,000 --> 00:16:22,720
like the offload split types of the

431
00:16:20,000 --> 00:16:24,800
inputs or the inputs themselves

432
00:16:22,720 --> 00:16:26,959
the naive estimators here are functions

433
00:16:24,800 --> 00:16:28,959
of the input data size

434
00:16:26,959 --> 00:16:30,479
below a certain data size the heuristic

435
00:16:28,959 --> 00:16:33,758
favors the cpu

436
00:16:30,480 --> 00:16:35,120
and above it it favors the gpu

437
00:16:33,759 --> 00:16:37,680
like the rest of scheduling the

438
00:16:35,120 --> 00:16:39,680
algorithm here is rather naive

439
00:16:37,680 --> 00:16:41,199
the heuristics particularly are more a

440
00:16:39,680 --> 00:16:43,120
proof of concept that the

441
00:16:41,199 --> 00:16:45,120
runtime captures a lot of information

442
00:16:43,120 --> 00:16:46,639
that can be automatically leveraged

443
00:16:45,120 --> 00:16:51,839
to make more informed scheduling

444
00:16:46,639 --> 00:16:51,839
decisions in the future

445
00:16:52,560 --> 00:16:57,119
how did offload annotations perform in

446
00:16:54,639 --> 00:16:57,120
practice

447
00:16:59,920 --> 00:17:03,599
we wrote offload annotations for four

448
00:17:01,759 --> 00:17:03,839
different gpu libraries and found that

449
00:17:03,600 --> 00:17:05,919
on

450
00:17:03,839 --> 00:17:08,480
average a single library integration

451
00:17:05,919 --> 00:17:10,240
took 130 lines of code

452
00:17:08,480 --> 00:17:12,079
the actual implementation for each

453
00:17:10,240 --> 00:17:15,280
integration looks very similar to what

454
00:17:12,079 --> 00:17:15,280
we described in the design

455
00:17:16,880 --> 00:17:21,039
we evaluated bach on eight different

456
00:17:18,880 --> 00:17:23,039
workloads from library tutorials

457
00:17:21,039 --> 00:17:24,879
jupiter notebooks and blog posts that we

458
00:17:23,039 --> 00:17:26,640
found online

459
00:17:24,880 --> 00:17:28,640
across all the workloads we observed a

460
00:17:26,640 --> 00:17:33,840
maximum speed up of 1200x

461
00:17:28,640 --> 00:17:33,840
and a median speed up of 6.3x

462
00:17:35,120 --> 00:17:38,799
we evaluated each workload on three

463
00:17:36,880 --> 00:17:41,039
different implementations

464
00:17:38,799 --> 00:17:42,639
the cpu library implementation indicates

465
00:17:41,039 --> 00:17:46,240
the original workload before

466
00:17:42,640 --> 00:17:48,640
making any changes to ported to the gpu

467
00:17:46,240 --> 00:17:49,919
the gpu library implementation indicates

468
00:17:48,640 --> 00:17:52,240
a handwritten

469
00:17:49,919 --> 00:17:54,960
implementation that uses a gpu library

470
00:17:52,240 --> 00:17:56,320
as a drop-in replacement

471
00:17:54,960 --> 00:17:59,039
finally the block implementation

472
00:17:56,320 --> 00:18:01,760
indicates that we imported an annotated

473
00:17:59,039 --> 00:18:05,039
library into the code

474
00:18:01,760 --> 00:18:06,879
on the x-axis we vary the input size

475
00:18:05,039 --> 00:18:08,240
and on the y-axis we measure enter and

476
00:18:06,880 --> 00:18:10,000
runtime

477
00:18:08,240 --> 00:18:11,760
both these axes are on a logarithmic

478
00:18:10,000 --> 00:18:13,360
scale

479
00:18:11,760 --> 00:18:15,600
and note that due to the logarithmic

480
00:18:13,360 --> 00:18:17,840
scale on the y-axis

481
00:18:15,600 --> 00:18:20,959
the differences in runtime may be more

482
00:18:17,840 --> 00:18:20,959
or less than they actually seem

483
00:18:21,039 --> 00:18:25,840
the dotted line indicates where the gpu

484
00:18:23,120 --> 00:18:27,840
implementation runs out of memory

485
00:18:25,840 --> 00:18:29,760
and in this graph in the gpu library the

486
00:18:27,840 --> 00:18:31,280
runtime is proportional to the data size

487
00:18:29,760 --> 00:18:33,840
until it runs out of memory and then

488
00:18:31,280 --> 00:18:33,840
crashes

489
00:18:34,000 --> 00:18:37,600
we'll look at the three main takeaways

490
00:18:35,679 --> 00:18:38,960
of the results with this particular

491
00:18:37,600 --> 00:18:40,639
workload

492
00:18:38,960 --> 00:18:42,400
then we'll analyze three other workloads

493
00:18:40,640 --> 00:18:43,919
in more detail and then take a holistic

494
00:18:42,400 --> 00:18:46,400
look at all eight workloads before

495
00:18:43,919 --> 00:18:46,400
concluding

496
00:18:48,720 --> 00:18:53,280
the first takeaway is that without

497
00:18:51,600 --> 00:18:53,678
manually replacing functions in your

498
00:18:53,280 --> 00:18:55,678
code

499
00:18:53,679 --> 00:18:57,440
in writing data transfer statements bach

500
00:18:55,679 --> 00:18:59,440
can match the performance of handwritten

501
00:18:57,440 --> 00:19:01,360
gpu implementations

502
00:18:59,440 --> 00:19:02,799
so in the graph at the same data sizes

503
00:19:01,360 --> 00:19:05,760
the two implementations have very

504
00:19:02,799 --> 00:19:05,760
similar runtimes

505
00:19:06,960 --> 00:19:11,600
second takeaway is that without manually

506
00:19:09,120 --> 00:19:13,678
splitting and paging data in your code

507
00:19:11,600 --> 00:19:16,000
boc can automatically scale to data

508
00:19:13,679 --> 00:19:17,520
sizes beyond where the gpu library runs

509
00:19:16,000 --> 00:19:19,520
out of memory

510
00:19:17,520 --> 00:19:22,840
in the graph buck extends beyond the

511
00:19:19,520 --> 00:19:24,639
dotted line with no additional end user

512
00:19:22,840 --> 00:19:26,639
effort

513
00:19:24,640 --> 00:19:28,400
and finally bach consistently beats the

514
00:19:26,640 --> 00:19:30,559
performance of the cpu library

515
00:19:28,400 --> 00:19:32,000
implementation

516
00:19:30,559 --> 00:19:33,520
note that due to the log scale the

517
00:19:32,000 --> 00:19:37,280
difference in runtime represents a

518
00:19:33,520 --> 00:19:38,639
constant multiplicative speed up

519
00:19:37,280 --> 00:19:41,760
now we'll take a look at three other

520
00:19:38,640 --> 00:19:41,760
workloads in more detail

521
00:19:42,880 --> 00:19:46,320
in this workload we look at the effects

522
00:19:44,640 --> 00:19:49,760
of allocating data directly on the

523
00:19:46,320 --> 00:19:52,320
device that first uses the data

524
00:19:49,760 --> 00:19:54,400
by doing so bach is able to eliminate

525
00:19:52,320 --> 00:19:58,799
the initial data transfer in this

526
00:19:54,400 --> 00:20:01,039
workload and enable 4.6x speedups

527
00:19:58,799 --> 00:20:02,080
however when the data is too large to

528
00:20:01,039 --> 00:20:04,158
fit on the gpu

529
00:20:02,080 --> 00:20:05,600
bach is unable to allocate all the data

530
00:20:04,159 --> 00:20:07,919
on the gpu at once

531
00:20:05,600 --> 00:20:09,760
because it'll run out of memory instead

532
00:20:07,919 --> 00:20:11,200
it allocates the data on the cpu and

533
00:20:09,760 --> 00:20:13,840
then pages it into

534
00:20:11,200 --> 00:20:15,600
memory adding an additional data

535
00:20:13,840 --> 00:20:21,840
transfer beyond this data size

536
00:20:15,600 --> 00:20:21,840
thus reducing the speed up to 1.1 x

537
00:20:23,039 --> 00:20:26,799
next we evaluate heuristics

538
00:20:26,960 --> 00:20:30,960
in this workload we use the same linear

539
00:20:28,880 --> 00:20:32,799
estimators as described in design and

540
00:20:30,960 --> 00:20:35,039
thus smaller data sizes

541
00:20:32,799 --> 00:20:37,280
assign all computation to the cpu while

542
00:20:35,039 --> 00:20:39,280
larger data sizes default to scheduling

543
00:20:37,280 --> 00:20:41,039
as many operations as possible on the

544
00:20:39,280 --> 00:20:43,360
gpu

545
00:20:41,039 --> 00:20:45,360
as a result the bach runtime here is a

546
00:20:43,360 --> 00:20:47,360
minimum of the runtimes of the two other

547
00:20:45,360 --> 00:20:49,199
implementations

548
00:20:47,360 --> 00:20:51,280
however the heuristics aren't entirely

549
00:20:49,200 --> 00:20:55,360
necessary since the absolute run times

550
00:20:51,280 --> 00:20:55,360
at small data sizes are also so small

551
00:20:58,080 --> 00:21:01,918
and finally we evaluate splitting and

552
00:20:59,919 --> 00:21:04,240
paging large data sets into memory using

553
00:21:01,919 --> 00:21:06,320
the splitting api

554
00:21:04,240 --> 00:21:07,919
this workload is the same as the one in

555
00:21:06,320 --> 00:21:09,678
the motivating example

556
00:21:07,919 --> 00:21:11,200
where we first fit a model to some

557
00:21:09,679 --> 00:21:13,600
training data and then predicted on

558
00:21:11,200 --> 00:21:16,240
testing data

559
00:21:13,600 --> 00:21:17,280
before the gpu runs out of memory here

560
00:21:16,240 --> 00:21:19,360
the bach runtime

561
00:21:17,280 --> 00:21:21,200
is dominated by the time it takes to fit

562
00:21:19,360 --> 00:21:23,360
the model

563
00:21:21,200 --> 00:21:25,280
as the data size increases and because

564
00:21:23,360 --> 00:21:25,918
bach is able to automatically page the

565
00:21:25,280 --> 00:21:28,320
data set

566
00:21:25,919 --> 00:21:30,080
even when it no longer fits in memory

567
00:21:28,320 --> 00:21:32,799
the absolute runtime advantage

568
00:21:30,080 --> 00:21:36,399
also becomes more significant enabling

569
00:21:32,799 --> 00:21:39,440
speedups of 6.2 x

570
00:21:36,400 --> 00:21:41,360
in this workload we don't really see the

571
00:21:39,440 --> 00:21:43,840
advantage of using the gpu

572
00:21:41,360 --> 00:21:46,959
except until bach is able to page the

573
00:21:43,840 --> 00:21:46,959
data sets into memory

574
00:21:50,240 --> 00:21:56,080
we evaluated eight workloads in total

575
00:21:52,799 --> 00:21:56,080
and we discussed the details in the

576
00:21:56,840 --> 00:22:00,080
paper

577
00:21:58,960 --> 00:22:01,919
and since we've been looking at

578
00:22:00,080 --> 00:22:03,439
everything in a log log scale here's a

579
00:22:01,919 --> 00:22:06,480
brief look at what the data looks like

580
00:22:03,440 --> 00:22:09,120
when the y-axis is a linear scale

581
00:22:06,480 --> 00:22:11,200
this version further emphasizes that the

582
00:22:09,120 --> 00:22:12,000
real advantage of bach is at large data

583
00:22:11,200 --> 00:22:14,320
sizes

584
00:22:12,000 --> 00:22:16,159
where the runtime differences are larger

585
00:22:14,320 --> 00:22:17,760
and because bock enables the workloads

586
00:22:16,159 --> 00:22:21,840
to scale automatically

587
00:22:17,760 --> 00:22:21,840
by splitting and paging large data sets

588
00:22:24,400 --> 00:22:28,720
in conclusion offload annotations enable

589
00:22:27,120 --> 00:22:30,719
heterogeneous computing

590
00:22:28,720 --> 00:22:33,520
in existing libraries and workloads with

591
00:22:30,720 --> 00:22:35,120
little to no code modifications

592
00:22:33,520 --> 00:22:36,720
our three main takeaways are that with

593
00:22:35,120 --> 00:22:38,639
less developer effort

594
00:22:36,720 --> 00:22:40,080
back and offload annotations can match

595
00:22:38,640 --> 00:22:43,280
the performance of handwritten

596
00:22:40,080 --> 00:22:45,678
gpu implementations scale to data sizes

597
00:22:43,280 --> 00:22:47,440
larger than gpu memory and beat cpu

598
00:22:45,679 --> 00:22:49,280
performance

599
00:22:47,440 --> 00:22:51,760
our code is open source if you'd like to

600
00:22:49,280 --> 00:22:55,840
try offload annotations for yourself

601
00:22:51,760 --> 00:22:55,840
thank you for listening

