1
00:00:09,440 --> 00:00:13,200
hello everyone

2
00:00:10,639 --> 00:00:14,959
welcome to daddy image service for

3
00:00:13,200 --> 00:00:17,279
container clusters

4
00:00:14,960 --> 00:00:18,160
this is a working system deployed at a

5
00:00:17,279 --> 00:00:21,759
school in

6
00:00:18,160 --> 00:00:23,759
alibaba i am huibali

7
00:00:21,760 --> 00:00:25,359
containers are becoming the foundation

8
00:00:23,760 --> 00:00:28,720
of cloud platform

9
00:00:25,359 --> 00:00:31,199
especially for cloud native applications

10
00:00:28,720 --> 00:00:32,320
but deploying containers involves a lot

11
00:00:31,199 --> 00:00:35,120
of time

12
00:00:32,320 --> 00:00:37,120
sometimes the long-tail latency returns

13
00:00:35,120 --> 00:00:39,440
tens of minutes

14
00:00:37,120 --> 00:00:41,599
daddy image service is trying to solve

15
00:00:39,440 --> 00:00:43,760
this very problem

16
00:00:41,600 --> 00:00:46,079
the essential reasons are image

17
00:00:43,760 --> 00:00:49,280
downloading and unpacking

18
00:00:46,079 --> 00:00:51,760
they are both slow but the point is only

19
00:00:49,280 --> 00:00:54,800
6.4 percent of the image

20
00:00:51,760 --> 00:00:56,239
is actually used according to a recent

21
00:00:54,800 --> 00:00:59,280
research

22
00:00:56,239 --> 00:01:00,559
to some extent it is a regression to a

23
00:00:59,280 --> 00:01:02,719
decade ago

24
00:01:00,559 --> 00:01:04,080
with the virtual machine images were

25
00:01:02,719 --> 00:01:07,840
also downloaded

26
00:01:04,080 --> 00:01:08,560
to the hosts p2p downloading is widely

27
00:01:07,840 --> 00:01:10,840
used in

28
00:01:08,560 --> 00:01:12,720
large container clusters it is

29
00:01:10,840 --> 00:01:15,280
significantly improves

30
00:01:12,720 --> 00:01:16,400
downloading speed in these environments

31
00:01:15,280 --> 00:01:19,439
but the result is

32
00:01:16,400 --> 00:01:22,000
still far from satisfactory as it

33
00:01:19,439 --> 00:01:23,439
deals with only half the reason the

34
00:01:22,000 --> 00:01:25,680
actions of downloading

35
00:01:23,439 --> 00:01:26,798
and unpacking are still there

36
00:01:25,680 --> 00:01:29,200
furthermore

37
00:01:26,799 --> 00:01:32,000
p2p downloading has little effect for

38
00:01:29,200 --> 00:01:34,240
small clusters

39
00:01:32,000 --> 00:01:37,040
streaming the images is another attempt

40
00:01:34,240 --> 00:01:39,600
to improve the development latency

41
00:01:37,040 --> 00:01:40,079
but this approach is not universal

42
00:01:39,600 --> 00:01:42,240
because

43
00:01:40,079 --> 00:01:45,279
it's hard to automatically find all

44
00:01:42,240 --> 00:01:47,600
dependencies for all applications

45
00:01:45,280 --> 00:01:48,640
and it's also hard to support ad hoc

46
00:01:47,600 --> 00:01:52,320
operations

47
00:01:48,640 --> 00:01:55,360
within the containers and we believe

48
00:01:52,320 --> 00:01:58,079
the ideal approach is the remote image

49
00:01:55,360 --> 00:01:59,360
that avoids beforehand downloading and

50
00:01:58,079 --> 00:02:02,158
unpacking

51
00:01:59,360 --> 00:02:03,600
this model has been proven by virtual

52
00:02:02,159 --> 00:02:07,200
machine clusters

53
00:02:03,600 --> 00:02:10,399
in cloud platforms where remote images

54
00:02:07,200 --> 00:02:12,000
are extensively used today

55
00:02:10,399 --> 00:02:15,040
there are many networks trying to

56
00:02:12,000 --> 00:02:18,560
introduce remote image to containers

57
00:02:15,040 --> 00:02:21,599
such as crfs supported by google

58
00:02:18,560 --> 00:02:24,720
teleport supported by microsoft

59
00:02:21,599 --> 00:02:26,959
vmfs from third

60
00:02:24,720 --> 00:02:28,720
there are also many works come from

61
00:02:26,959 --> 00:02:32,640
research institutes

62
00:02:28,720 --> 00:02:36,560
such as slacker wolf cfs

63
00:02:32,640 --> 00:02:40,319
cider etc but the fact is

64
00:02:36,560 --> 00:02:41,840
the image format table is not viable for

65
00:02:40,319 --> 00:02:45,119
remote image

66
00:02:41,840 --> 00:02:46,080
as it is not seekable the format was

67
00:02:45,120 --> 00:02:49,280
designed for

68
00:02:46,080 --> 00:02:50,000
entirely unpacking it's also hard to

69
00:02:49,280 --> 00:02:53,040
support

70
00:02:50,000 --> 00:02:54,160
advanced features such as extended

71
00:02:53,040 --> 00:02:57,280
attribute

72
00:02:54,160 --> 00:03:00,640
cross layer reference etc

73
00:02:57,280 --> 00:03:03,280
so it is a must to change the format

74
00:03:00,640 --> 00:03:04,079
most existing works avoid the format

75
00:03:03,280 --> 00:03:07,200
problem

76
00:03:04,080 --> 00:03:10,239
by providing unpacked images via

77
00:03:07,200 --> 00:03:13,359
nfs cfs or

78
00:03:10,239 --> 00:03:15,840
field-based file systems so

79
00:03:13,360 --> 00:03:16,480
they have a necessary step to import the

80
00:03:15,840 --> 00:03:19,519
table

81
00:03:16,480 --> 00:03:22,399
images it's not possible to

82
00:03:19,519 --> 00:03:23,360
code status containers directly from

83
00:03:22,400 --> 00:03:26,640
registry

84
00:03:23,360 --> 00:03:29,360
with this works crfs

85
00:03:26,640 --> 00:03:31,679
is trying to enhance the current image

86
00:03:29,360 --> 00:03:34,720
format by adding a special

87
00:03:31,680 --> 00:03:38,239
index file the resulting image

88
00:03:34,720 --> 00:03:41,359
called test.js is still a valid

89
00:03:38,239 --> 00:03:43,840
table and it can be correctly

90
00:03:41,360 --> 00:03:44,959
processed by other container engines

91
00:03:43,840 --> 00:03:48,159
that do not support

92
00:03:44,959 --> 00:03:50,879
cifs on the other hand

93
00:03:48,159 --> 00:03:51,840
we believe it's better not confined in

94
00:03:50,879 --> 00:03:55,359
turbo

95
00:03:51,840 --> 00:03:58,159
and we should design a new image format

96
00:03:55,360 --> 00:03:58,959
based on block device because it brings

97
00:03:58,159 --> 00:04:02,879
benefits and

98
00:03:58,959 --> 00:04:05,840
lower stone complexity at the same time

99
00:04:02,879 --> 00:04:07,359
there are two types of images one is

100
00:04:05,840 --> 00:04:10,239
file system best

101
00:04:07,360 --> 00:04:12,319
and the other is block device based

102
00:04:10,239 --> 00:04:15,360
choosing a proper type of image

103
00:04:12,319 --> 00:04:18,000
is quite important because it influences

104
00:04:15,360 --> 00:04:20,799
a lot of things

105
00:04:18,000 --> 00:04:22,639
a file system based remote image

106
00:04:20,798 --> 00:04:24,638
provides a file system interface

107
00:04:22,639 --> 00:04:27,600
directly to containers

108
00:04:24,639 --> 00:04:28,639
it is a natural extension of container

109
00:04:27,600 --> 00:04:32,000
image

110
00:04:28,639 --> 00:04:34,560
so it includes less mental friction

111
00:04:32,000 --> 00:04:36,479
a block device basically remote remote

112
00:04:34,560 --> 00:04:38,720
image on the other hand

113
00:04:36,479 --> 00:04:39,680
needs to work together with a regular

114
00:04:38,720 --> 00:04:43,440
file system

115
00:04:39,680 --> 00:04:44,000
such as exc4 and the block device can be

116
00:04:43,440 --> 00:04:46,479
easily

117
00:04:44,000 --> 00:04:47,280
integrated with container secure

118
00:04:46,479 --> 00:04:51,039
container and

119
00:04:47,280 --> 00:04:54,479
virtual machines a first system based

120
00:04:51,040 --> 00:04:55,919
remote image is more complex as the

121
00:04:54,479 --> 00:04:59,039
first system interface

122
00:04:55,919 --> 00:05:02,479
involves many operations that have

123
00:04:59,040 --> 00:05:04,400
complex schematics so it is more

124
00:05:02,479 --> 00:05:07,520
difficult to get stable

125
00:05:04,400 --> 00:05:10,880
more difficult to apply optimizations

126
00:05:07,520 --> 00:05:11,919
and more difficult to develop advanced

127
00:05:10,880 --> 00:05:15,039
features

128
00:05:11,919 --> 00:05:18,479
such as yes extended attribute

129
00:05:15,039 --> 00:05:19,039
or cross layer reference a block device

130
00:05:18,479 --> 00:05:22,080
based

131
00:05:19,039 --> 00:05:23,120
remote image on the other hand is much

132
00:05:22,080 --> 00:05:26,800
simpler

133
00:05:23,120 --> 00:05:29,919
so it is easier to get stable

134
00:05:26,800 --> 00:05:31,600
easier to apply out optimizations and

135
00:05:29,919 --> 00:05:35,198
easier to develop

136
00:05:31,600 --> 00:05:38,080
advanced features a blocked

137
00:05:35,199 --> 00:05:39,039
block image is more universal because it

138
00:05:38,080 --> 00:05:41,280
allows

139
00:05:39,039 --> 00:05:42,639
the application to choose the best fit

140
00:05:41,280 --> 00:05:45,919
file system

141
00:05:42,639 --> 00:05:47,520
for example a windows container running

142
00:05:45,919 --> 00:05:50,880
a linux host

143
00:05:47,520 --> 00:05:54,799
may want to choose ntfs as this

144
00:05:50,880 --> 00:05:58,319
image file system it is most likely

145
00:05:54,800 --> 00:06:02,720
feasible with block image

146
00:05:58,319 --> 00:06:06,160
a block image has smaller attack surface

147
00:06:02,720 --> 00:06:08,080
so it is easier to get secured than the

148
00:06:06,160 --> 00:06:10,800
counterpart

149
00:06:08,080 --> 00:06:11,520
according to this analysis we've been

150
00:06:10,800 --> 00:06:14,720
studying

151
00:06:11,520 --> 00:06:18,080
on block block device

152
00:06:14,720 --> 00:06:20,800
although we are working almost alone

153
00:06:18,080 --> 00:06:22,960
we believe this is the best approach for

154
00:06:20,800 --> 00:06:25,919
contingent system

155
00:06:22,960 --> 00:06:29,039
before introducing the details of daddy

156
00:06:25,919 --> 00:06:31,840
let's take a look at the background

157
00:06:29,039 --> 00:06:32,080
connector images are downloaded to host

158
00:06:31,840 --> 00:06:34,638
and

159
00:06:32,080 --> 00:06:37,280
unpacked there the images are

160
00:06:34,639 --> 00:06:39,759
constituted by multiple layers

161
00:06:37,280 --> 00:06:41,280
each layer is a change set compared to

162
00:06:39,759 --> 00:06:45,199
the previous state

163
00:06:41,280 --> 00:06:47,758
namely files added modified and deleted

164
00:06:45,199 --> 00:06:50,240
the layers are read only and shared by

165
00:06:47,759 --> 00:06:52,560
multiple container instances

166
00:06:50,240 --> 00:06:53,759
each container instance has a private

167
00:06:52,560 --> 00:06:56,479
container layer

168
00:06:53,759 --> 00:06:58,319
that is writable it is a change that

169
00:06:56,479 --> 00:07:00,159
compared to the image

170
00:06:58,319 --> 00:07:02,720
usually the layers are stored in

171
00:07:00,160 --> 00:07:03,199
separate directories and the merchant

172
00:07:02,720 --> 00:07:05,840
view

173
00:07:03,199 --> 00:07:07,680
is created with a kernel module overlay

174
00:07:05,840 --> 00:07:10,719
fs

175
00:07:07,680 --> 00:07:13,680
this is the error path of the image

176
00:07:10,720 --> 00:07:14,479
the containerized application sends io

177
00:07:13,680 --> 00:07:17,599
requests

178
00:07:14,479 --> 00:07:18,400
to overlay fs which then redirects the

179
00:07:17,599 --> 00:07:21,759
requests

180
00:07:18,400 --> 00:07:23,919
to an appropriate layer there are three

181
00:07:21,759 --> 00:07:26,960
core components in daddy

182
00:07:23,919 --> 00:07:29,840
the first is the novel laid image format

183
00:07:26,960 --> 00:07:32,318
based on virtual block device each layer

184
00:07:29,840 --> 00:07:33,758
in daddy is a change set of overwritten

185
00:07:32,319 --> 00:07:36,400
data blocks

186
00:07:33,759 --> 00:07:36,960
and we create merged view of the layers

187
00:07:36,400 --> 00:07:39,758
visual

188
00:07:36,960 --> 00:07:40,719
with a new module called overlay block

189
00:07:39,759 --> 00:07:43,919
device

190
00:07:40,720 --> 00:07:45,599
or overlapd for short it is a general

191
00:07:43,919 --> 00:07:49,280
solution for container

192
00:07:45,599 --> 00:07:51,120
ecology study image retains compression

193
00:07:49,280 --> 00:07:54,080
by introducing a module called

194
00:07:51,120 --> 00:07:55,520
z-file which supports seekable online

195
00:07:54,080 --> 00:07:58,479
decompression

196
00:07:55,520 --> 00:07:59,280
this is the second core component and

197
00:07:58,479 --> 00:08:02,318
the third one

198
00:07:59,280 --> 00:08:04,960
is the tree structure the p2p subsystem

199
00:08:02,319 --> 00:08:05,919
providing on-demand transfer to cope

200
00:08:04,960 --> 00:08:09,120
with our

201
00:08:05,919 --> 00:08:12,080
large-scale production of clusters the

202
00:08:09,120 --> 00:08:14,400
data path of daddy goes as follows

203
00:08:12,080 --> 00:08:16,000
the application in container reads from

204
00:08:14,400 --> 00:08:19,280
a regular file system

205
00:08:16,000 --> 00:08:20,800
like ext4 the request is then

206
00:08:19,280 --> 00:08:24,000
transformed into reads of

207
00:08:20,800 --> 00:08:26,960
a virtual block device then they get

208
00:08:24,000 --> 00:08:29,680
passed to a user space service demon

209
00:08:26,960 --> 00:08:30,878
then get transformed by overlappity

210
00:08:29,680 --> 00:08:34,080
tools of

211
00:08:30,879 --> 00:08:34,719
one or more layer blobs and the reads

212
00:08:34,080 --> 00:08:37,680
usually

213
00:08:34,719 --> 00:08:39,279
need decompression by these files for

214
00:08:37,679 --> 00:08:42,399
layers that do not exist

215
00:08:39,279 --> 00:08:43,360
locally they fail restated with the help

216
00:08:42,399 --> 00:08:47,040
of the p2p

217
00:08:43,360 --> 00:08:47,920
subsystem each layer in daddy is a

218
00:08:47,040 --> 00:08:50,480
change set of

219
00:08:47,920 --> 00:08:51,120
overwritten data blocks there is no

220
00:08:50,480 --> 00:08:54,399
concept

221
00:08:51,120 --> 00:08:56,320
of files or file system and we have an

222
00:08:54,399 --> 00:08:59,279
index for faster reading

223
00:08:56,320 --> 00:09:01,600
the index has a variable length entries

224
00:08:59,279 --> 00:09:03,519
in order to save memory by combining

225
00:09:01,600 --> 00:09:06,480
addition entries

226
00:09:03,519 --> 00:09:07,279
we believe memory footprint is important

227
00:09:06,480 --> 00:09:09,519
because

228
00:09:07,279 --> 00:09:11,600
containers are often used for high

229
00:09:09,519 --> 00:09:15,360
density deployment

230
00:09:11,600 --> 00:09:19,519
the index is an array of non-overlapping

231
00:09:15,360 --> 00:09:22,080
entries sorted by their article offsets

232
00:09:19,519 --> 00:09:24,720
when reading the image we perform range

233
00:09:22,080 --> 00:09:27,760
queries directly over the index

234
00:09:24,720 --> 00:09:30,720
instead of block by block queries the

235
00:09:27,760 --> 00:09:33,680
indexes of multiple layers can be merged

236
00:09:30,720 --> 00:09:34,959
when loaded so that a single query can

237
00:09:33,680 --> 00:09:37,439
handle any number of

238
00:09:34,959 --> 00:09:38,160
layers and the performance doesn't

239
00:09:37,440 --> 00:09:40,800
degree

240
00:09:38,160 --> 00:09:42,399
as the number of layers increases this

241
00:09:40,800 --> 00:09:46,000
is a key advantage over

242
00:09:42,399 --> 00:09:47,120
over the fs we analyzed our production

243
00:09:46,000 --> 00:09:49,360
of images

244
00:09:47,120 --> 00:09:50,800
and found that the number of entries in

245
00:09:49,360 --> 00:09:54,000
merged index

246
00:09:50,800 --> 00:09:57,359
is less than 4500

247
00:09:54,000 --> 00:10:00,080
which consumes only about 72 kilobytes

248
00:09:57,360 --> 00:10:03,600
of memory

249
00:10:00,080 --> 00:10:05,839
radix 3 is usually used in block images

250
00:10:03,600 --> 00:10:08,560
for indexing snapshots

251
00:10:05,839 --> 00:10:09,440
we don't use red x3 because it deals

252
00:10:08,560 --> 00:10:13,040
with fixed

253
00:10:09,440 --> 00:10:14,320
size blocks only so its memory footprint

254
00:10:13,040 --> 00:10:17,279
is lighter

255
00:10:14,320 --> 00:10:18,800
and it also incurs an overhead for copy

256
00:10:17,279 --> 00:10:22,640
and write

257
00:10:18,800 --> 00:10:24,880
b3 is also frequently used for indexing

258
00:10:22,640 --> 00:10:26,800
we tried the general implementation of

259
00:10:24,880 --> 00:10:29,760
b3 from google

260
00:10:26,800 --> 00:10:30,800
but from that it performed worse than

261
00:10:29,760 --> 00:10:34,480
banner research

262
00:10:30,800 --> 00:10:36,160
on array actually the array is a

263
00:10:34,480 --> 00:10:39,360
simplified b3

264
00:10:36,160 --> 00:10:43,040
that has only one node and we believe

265
00:10:39,360 --> 00:10:46,240
that a specialized multi-node b3

266
00:10:43,040 --> 00:10:48,160
is probably better than the array but we

267
00:10:46,240 --> 00:10:49,920
leave it as a future work

268
00:10:48,160 --> 00:10:52,719
the left figure shows the raw

269
00:10:49,920 --> 00:10:55,839
performance of our index

270
00:10:52,720 --> 00:10:58,079
a a single single core can yield over 6

271
00:10:55,839 --> 00:11:00,399
million qbs for our production

272
00:10:58,079 --> 00:11:01,519
image the right figure shows a

273
00:11:00,399 --> 00:11:04,560
comparison

274
00:11:01,519 --> 00:11:05,200
with the lvm we can see that daddy

275
00:11:04,560 --> 00:11:07,680
performs

276
00:11:05,200 --> 00:11:10,320
worse than lvm until the lq depth

277
00:11:07,680 --> 00:11:12,880
reaches 128.

278
00:11:10,320 --> 00:11:14,079
this indicates our index performance is

279
00:11:12,880 --> 00:11:16,880
higher than that of

280
00:11:14,079 --> 00:11:18,160
lvm despite of user's best

281
00:11:16,880 --> 00:11:21,360
implementation

282
00:11:18,160 --> 00:11:23,839
incurs a significant overhead

283
00:11:21,360 --> 00:11:25,040
and it's worth noting that our

284
00:11:23,839 --> 00:11:27,200
compressed format

285
00:11:25,040 --> 00:11:28,079
performs better than non-compressed

286
00:11:27,200 --> 00:11:32,720
format

287
00:11:28,079 --> 00:11:35,040
as long as the cpu is not a bottleneck

288
00:11:32,720 --> 00:11:37,040
that implements the readable layer with

289
00:11:35,040 --> 00:11:39,519
a log structure design

290
00:11:37,040 --> 00:11:42,399
so that it is compatible with virtually

291
00:11:39,519 --> 00:11:45,519
all kinds of distributed file systems

292
00:11:42,399 --> 00:11:47,920
including those append only once

293
00:11:45,519 --> 00:11:49,120
we maintain a separate index for each

294
00:11:47,920 --> 00:11:51,680
readable layer

295
00:11:49,120 --> 00:11:52,320
with a red black tree in order to

296
00:11:51,680 --> 00:11:55,519
realize

297
00:11:52,320 --> 00:11:56,959
efficient insertion and deletion when

298
00:11:55,519 --> 00:11:59,760
building new layers

299
00:11:56,959 --> 00:12:01,518
study commits only useful data blocks to

300
00:11:59,760 --> 00:12:04,959
the read-only blob

301
00:12:01,519 --> 00:12:05,440
eliminating all the garbages at the same

302
00:12:04,959 --> 00:12:07,760
time

303
00:12:05,440 --> 00:12:10,000
that d also sort the data blocks by

304
00:12:07,760 --> 00:12:13,439
their logical offsets

305
00:12:10,000 --> 00:12:16,720
so that adjacent index entries can be

306
00:12:13,440 --> 00:12:19,040
combined together as a single one that

307
00:12:16,720 --> 00:12:19,920
is refill is a seekable compression

308
00:12:19,040 --> 00:12:22,560
format

309
00:12:19,920 --> 00:12:24,479
that supports random reading referral

310
00:12:22,560 --> 00:12:25,599
achieves this with trunk by charm

311
00:12:24,480 --> 00:12:28,480
compression

312
00:12:25,600 --> 00:12:29,600
and adjusts enough decompression a zip

313
00:12:28,480 --> 00:12:32,480
file can include an

314
00:12:29,600 --> 00:12:33,279
arbitrary file and it says it is not

315
00:12:32,480 --> 00:12:36,959
tied to that

316
00:12:33,279 --> 00:12:39,439
image format we are also investigating

317
00:12:36,959 --> 00:12:42,399
dictionary supporting c file

318
00:12:39,440 --> 00:12:43,440
it enables two pass encoding in the

319
00:12:42,399 --> 00:12:46,160
first pass

320
00:12:43,440 --> 00:12:46,959
the encoder scans the whole file and

321
00:12:46,160 --> 00:12:50,560
generator

322
00:12:46,959 --> 00:12:53,119
summary called dictionary the dictionary

323
00:12:50,560 --> 00:12:55,839
is then used for in the following pass

324
00:12:53,120 --> 00:12:59,360
for chunk by check encoding

325
00:12:55,839 --> 00:13:01,680
it is also used in decoding this scheme

326
00:12:59,360 --> 00:13:02,399
can improve compression ratio and

327
00:13:01,680 --> 00:13:05,920
possibly

328
00:13:02,399 --> 00:13:07,839
decompression performance the red figure

329
00:13:05,920 --> 00:13:10,240
shows the statistics

330
00:13:07,839 --> 00:13:11,680
of layer size in our projectional

331
00:13:10,240 --> 00:13:14,880
environment

332
00:13:11,680 --> 00:13:15,279
we can see that daddy image is larger

333
00:13:14,880 --> 00:13:18,720
than

334
00:13:15,279 --> 00:13:19,519
its counterpart entire format this is

335
00:13:18,720 --> 00:13:21,760
because

336
00:13:19,519 --> 00:13:23,600
daddy has a overhead for the full

337
00:13:21,760 --> 00:13:26,639
featured file system

338
00:13:23,600 --> 00:13:28,000
the overhead is relatively moderate with

339
00:13:26,639 --> 00:13:31,680
layer sizes

340
00:13:28,000 --> 00:13:32,720
greater than 10 megabytes we can also

341
00:13:31,680 --> 00:13:34,959
see that

342
00:13:32,720 --> 00:13:35,839
they first compression ratio is less

343
00:13:34,959 --> 00:13:40,239
than that of

344
00:13:35,839 --> 00:13:43,120
gzip because we use faster algorithm

345
00:13:40,240 --> 00:13:44,880
that are focused on speed especially

346
00:13:43,120 --> 00:13:47,519
decompression speed

347
00:13:44,880 --> 00:13:49,120
and they sacrifice some compression

348
00:13:47,519 --> 00:13:51,920
ratio

349
00:13:49,120 --> 00:13:52,959
the chunk by chance compression pattern

350
00:13:51,920 --> 00:13:56,000
we use

351
00:13:52,959 --> 00:13:58,959
also hurts the compression ratio because

352
00:13:56,000 --> 00:14:00,399
the algorithms need to start again for

353
00:13:58,959 --> 00:14:02,959
every chunk

354
00:14:00,399 --> 00:14:04,160
so the lower the compression ratio is

355
00:14:02,959 --> 00:14:07,920
oppressed paid for

356
00:14:04,160 --> 00:14:10,000
faster online decompression

357
00:14:07,920 --> 00:14:11,760
the tree structured topology is

358
00:14:10,000 --> 00:14:14,000
maintained dynamically by

359
00:14:11,760 --> 00:14:15,439
root node for each layer block

360
00:14:14,000 --> 00:14:17,680
separately

361
00:14:15,440 --> 00:14:19,120
every node cache has recently accessed

362
00:14:17,680 --> 00:14:21,439
the data blocks

363
00:14:19,120 --> 00:14:22,800
so that they can they have data to serve

364
00:14:21,440 --> 00:14:25,040
other nodes

365
00:14:22,800 --> 00:14:27,519
when the node needs to read data it

366
00:14:25,040 --> 00:14:28,560
simply sends a request to experiment

367
00:14:27,519 --> 00:14:30,720
node

368
00:14:28,560 --> 00:14:33,439
and the request is likely to hit the

369
00:14:30,720 --> 00:14:36,480
parents cash

370
00:14:33,440 --> 00:14:39,199
in case of cash miss the parent will

371
00:14:36,480 --> 00:14:42,079
forward the request upward recursively

372
00:14:39,199 --> 00:14:44,880
until it is fulfilled

373
00:14:42,079 --> 00:14:46,560
building images with study is faster

374
00:14:44,880 --> 00:14:49,439
because study generates

375
00:14:46,560 --> 00:14:50,719
secretion right that is more efficient

376
00:14:49,440 --> 00:14:53,920
and danny may also

377
00:14:50,720 --> 00:14:54,959
avoid pulling based images by on demand

378
00:14:53,920 --> 00:14:57,680
read

379
00:14:54,959 --> 00:14:58,000
this is especially true for dedicated

380
00:14:57,680 --> 00:15:01,519
image

381
00:14:58,000 --> 00:15:05,360
builders committing data images

382
00:15:01,519 --> 00:15:09,199
is also faster because study uses faster

383
00:15:05,360 --> 00:15:11,199
compression algorithms the image builder

384
00:15:09,199 --> 00:15:12,240
in container engine generates one

385
00:15:11,199 --> 00:15:15,439
separate layer

386
00:15:12,240 --> 00:15:18,480
for each command line in the script

387
00:15:15,440 --> 00:15:21,519
this was an optimization to exploit

388
00:15:18,480 --> 00:15:24,839
parallelism when pulling the images

389
00:15:21,519 --> 00:15:26,639
but it may become useless for remote

390
00:15:24,839 --> 00:15:30,800
images

391
00:15:26,639 --> 00:15:32,800
besides the optional p2p subsystem

392
00:15:30,800 --> 00:15:34,240
that they can also cooperate with the

393
00:15:32,800 --> 00:15:38,160
shared storage

394
00:15:34,240 --> 00:15:41,920
such as hdfs ceph lfs

395
00:15:38,160 --> 00:15:42,719
etc with this scheme users have the

396
00:15:41,920 --> 00:15:46,160
option to

397
00:15:42,720 --> 00:15:48,079
store their blobs on the shield store

398
00:15:46,160 --> 00:15:49,600
eliminating the need to download

399
00:15:48,079 --> 00:15:53,199
complete layer blobs

400
00:15:49,600 --> 00:15:53,759
on the hosts instead they only need to

401
00:15:53,199 --> 00:15:56,719
maintain

402
00:15:53,759 --> 00:15:57,199
a small cache for halted blocks so that

403
00:15:56,720 --> 00:15:59,920
they can

404
00:15:57,199 --> 00:16:01,758
serve other hosts in a peer-to-peer

405
00:15:59,920 --> 00:16:04,000
transfer

406
00:16:01,759 --> 00:16:04,880
for clusters that have little burst

407
00:16:04,000 --> 00:16:07,040
workloads

408
00:16:04,880 --> 00:16:08,320
it's also possible for data to work

409
00:16:07,040 --> 00:16:11,519
without the p2p

410
00:16:08,320 --> 00:16:13,040
subsystem as shown in the right column

411
00:16:11,519 --> 00:16:15,600
of the table

412
00:16:13,040 --> 00:16:16,240
it's worth noting that the bottom right

413
00:16:15,600 --> 00:16:19,120
case

414
00:16:16,240 --> 00:16:20,399
is actually a scheme of distributed

415
00:16:19,120 --> 00:16:23,040
block store

416
00:16:20,399 --> 00:16:25,839
that is commonly used today for virtual

417
00:16:23,040 --> 00:16:29,759
machine clusters

418
00:16:25,839 --> 00:16:30,959
finally the evaluations the left figure

419
00:16:29,759 --> 00:16:33,680
shows the results of

420
00:16:30,959 --> 00:16:34,719
code startup of a single container we

421
00:16:33,680 --> 00:16:37,758
can see that

422
00:16:34,720 --> 00:16:38,800
that is much faster than than other

423
00:16:37,759 --> 00:16:41,920
approaches

424
00:16:38,800 --> 00:16:45,040
no matter starting from registry or p2p

425
00:16:41,920 --> 00:16:48,000
route in case of warm setup as

426
00:16:45,040 --> 00:16:48,800
shown in the right figure study is still

427
00:16:48,000 --> 00:16:50,959
faster

428
00:16:48,800 --> 00:16:53,040
especially when this performance is

429
00:16:50,959 --> 00:16:55,040
limited

430
00:16:53,040 --> 00:16:57,279
the left figure shows the results of

431
00:16:55,040 --> 00:16:58,319
startup latency with truth-based

432
00:16:57,279 --> 00:17:00,000
prefetching

433
00:16:58,320 --> 00:17:02,800
we can see that the prefetching

434
00:17:00,000 --> 00:17:05,520
dramatically increases the performance

435
00:17:02,800 --> 00:17:08,159
and the result is getting very close to

436
00:17:05,520 --> 00:17:10,559
that of warm startup

437
00:17:08,160 --> 00:17:11,280
the red figure shows the results of bad

438
00:17:10,559 --> 00:17:14,559
startup

439
00:17:11,280 --> 00:17:16,639
of multiple containers note that

440
00:17:14,559 --> 00:17:18,160
the standard region they will study is

441
00:17:16,640 --> 00:17:20,799
much lower and

442
00:17:18,160 --> 00:17:21,280
remains largely constant at the number

443
00:17:20,799 --> 00:17:24,879
of

444
00:17:21,280 --> 00:17:26,959
instances increases we evaluate the

445
00:17:24,880 --> 00:17:29,840
status latency with a

446
00:17:26,959 --> 00:17:31,440
specifically written application called

447
00:17:29,840 --> 00:17:34,959
agility

448
00:17:31,440 --> 00:17:38,480
it is a python application that sends an

449
00:17:34,960 --> 00:17:40,720
http request to a web server which then

450
00:17:38,480 --> 00:17:44,240
records the timestamps

451
00:17:40,720 --> 00:17:46,559
when the requests arrive the left figure

452
00:17:44,240 --> 00:17:50,160
shows the results of starting

453
00:17:46,559 --> 00:17:53,280
10 000 continuous on 1000 hosts

454
00:17:50,160 --> 00:17:53,840
including both cold and warm setups the

455
00:17:53,280 --> 00:17:56,480
results

456
00:17:53,840 --> 00:17:57,280
indicate that study is effective at

457
00:17:56,480 --> 00:18:00,320
starting up

458
00:17:57,280 --> 00:18:02,559
large container clusters the right

459
00:18:00,320 --> 00:18:03,200
figure shows the projected results of

460
00:18:02,559 --> 00:18:06,080
starting

461
00:18:03,200 --> 00:18:07,280
a hyperscale cardinal cluster by

462
00:18:06,080 --> 00:18:11,360
evaluating

463
00:18:07,280 --> 00:18:14,080
only a single branch of the pw3

464
00:18:11,360 --> 00:18:15,600
the results indicate that that is highly

465
00:18:14,080 --> 00:18:18,240
scalable

466
00:18:15,600 --> 00:18:18,959
we valued io performance with du and

467
00:18:18,240 --> 00:18:22,240
atar

468
00:18:18,960 --> 00:18:25,200
comparing against overlay 2 and lvm

469
00:18:22,240 --> 00:18:26,880
the results showed that 30 performs

470
00:18:25,200 --> 00:18:30,400
better than both of them

471
00:18:26,880 --> 00:18:32,880
on both ssd and the cloud disk

472
00:18:30,400 --> 00:18:33,919
block-based images have a technical

473
00:18:32,880 --> 00:18:37,120
drawback

474
00:18:33,919 --> 00:18:38,320
namely the page cad sharing problem as

475
00:18:37,120 --> 00:18:41,600
the images are

476
00:18:38,320 --> 00:18:44,399
exposed to kernel as individual block

477
00:18:41,600 --> 00:18:45,360
block devices and the kernel doesn't

478
00:18:44,400 --> 00:18:48,400
know that

479
00:18:45,360 --> 00:18:50,639
some files are actually the same

480
00:18:48,400 --> 00:18:52,880
so the kernel will allocate different

481
00:18:50,640 --> 00:18:56,080
cache buffers further

482
00:18:52,880 --> 00:18:58,240
the tgd images on the other hand

483
00:18:56,080 --> 00:19:00,080
can share a single catch buffer for the

484
00:18:58,240 --> 00:19:03,280
same file across different

485
00:19:00,080 --> 00:19:05,918
containers on the sim host because the

486
00:19:03,280 --> 00:19:07,918
kernel module of union file systems

487
00:19:05,919 --> 00:19:10,000
can tell the kernel that the files are

488
00:19:07,919 --> 00:19:12,480
the same

489
00:19:10,000 --> 00:19:13,360
the page cache sharing problem can be

490
00:19:12,480 --> 00:19:17,039
solved with

491
00:19:13,360 --> 00:19:20,719
kernel same page merging but it incurs

492
00:19:17,039 --> 00:19:24,240
a significant overhead so we want to

493
00:19:20,720 --> 00:19:27,039
make it a shareable from the beginning

494
00:19:24,240 --> 00:19:28,240
the key to this problem is to tell the

495
00:19:27,039 --> 00:19:31,360
kernel that

496
00:19:28,240 --> 00:19:34,240
two data blocks on two different

497
00:19:31,360 --> 00:19:35,760
block devices are actually the same one

498
00:19:34,240 --> 00:19:38,799
data block

499
00:19:35,760 --> 00:19:42,640
and this information must be explored

500
00:19:38,799 --> 00:19:45,679
further by file system and page cache

501
00:19:42,640 --> 00:19:46,720
to realize this we plan to refactor the

502
00:19:45,679 --> 00:19:49,919
l stack

503
00:19:46,720 --> 00:19:50,720
in order to support dax along the whole

504
00:19:49,919 --> 00:19:53,360
path

505
00:19:50,720 --> 00:19:54,559
and the unique data blocks are managed

506
00:19:53,360 --> 00:19:58,399
by the bottom layer

507
00:19:54,559 --> 00:20:02,158
of the stack as container and

508
00:19:58,400 --> 00:20:04,880
virtual machine is converging and

509
00:20:02,159 --> 00:20:05,520
it's likely for them to form a converted

510
00:20:04,880 --> 00:20:08,320
runtime

511
00:20:05,520 --> 00:20:10,400
in the near future so container is

512
00:20:08,320 --> 00:20:14,000
becoming a new virtual machine

513
00:20:10,400 --> 00:20:16,559
and vice versa given that data study

514
00:20:14,000 --> 00:20:17,520
image is easy to integrate with both of

515
00:20:16,559 --> 00:20:20,320
them

516
00:20:17,520 --> 00:20:23,039
study may place the role as a bridge to

517
00:20:20,320 --> 00:20:25,600
continue to connect the two worlds

518
00:20:23,039 --> 00:20:26,480
the converged runtime together with

519
00:20:25,600 --> 00:20:30,080
study

520
00:20:26,480 --> 00:20:32,840
will enable applications to to evolve

521
00:20:30,080 --> 00:20:34,559
gradually from cloud-based to

522
00:20:32,840 --> 00:20:48,320
coordinative

523
00:20:34,559 --> 00:20:48,320
ok thanks

