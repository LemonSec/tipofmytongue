1
00:00:08,720 --> 00:00:11,599
hi everyone

2
00:00:09,519 --> 00:00:13,040
i'm ali reza and i'm going to present

3
00:00:11,599 --> 00:00:15,920
you our paper called

4
00:00:13,040 --> 00:00:18,240
reexamining era cash access to optimize

5
00:00:15,920 --> 00:00:20,400
io intensive applications for multi

6
00:00:18,240 --> 00:00:22,720
hundred gigabit networks

7
00:00:20,400 --> 00:00:23,919
this is a joint work with amir rooseve

8
00:00:22,720 --> 00:00:27,038
gerald mcguire

9
00:00:23,920 --> 00:00:27,039
and theon costage

10
00:00:27,680 --> 00:00:32,960
traditionally when io devices wanted to

11
00:00:30,480 --> 00:00:36,000
exchange information with the cpu

12
00:00:32,960 --> 00:00:37,920
for example when network interface cars

13
00:00:36,000 --> 00:00:40,480
were receiving packets

14
00:00:37,920 --> 00:00:42,800
they dm-made the packets to main memory

15
00:00:40,480 --> 00:00:43,839
and then the cpu core processing them

16
00:00:42,800 --> 00:00:47,519
had to load them

17
00:00:43,840 --> 00:00:49,840
into its cache to process them

18
00:00:47,520 --> 00:00:52,399
but this method was inefficient in terms

19
00:00:49,840 --> 00:00:54,480
of the number of accesses to main memory

20
00:00:52,399 --> 00:00:59,120
and incurred high access latency

21
00:00:54,480 --> 00:01:02,000
and unnecessary memory bandwidth usage

22
00:00:59,120 --> 00:01:04,959
to alleviate these issues intel

23
00:01:02,000 --> 00:01:08,320
introduced direct cash access or dca

24
00:01:04,959 --> 00:01:10,798
in around 2005 which prefetches

25
00:01:08,320 --> 00:01:12,720
a portion of the packet into the cash

26
00:01:10,799 --> 00:01:15,520
while dmaing the whole packet

27
00:01:12,720 --> 00:01:17,439
into main memory this technique

28
00:01:15,520 --> 00:01:20,479
addressed the first two problems

29
00:01:17,439 --> 00:01:22,960
of traditional dma

30
00:01:20,479 --> 00:01:25,039
but it was still inefficient in terms of

31
00:01:22,960 --> 00:01:28,240
memory bandwidth usage

32
00:01:25,040 --> 00:01:32,000
additionally it required os intervention

33
00:01:28,240 --> 00:01:32,000
and support from the processor

34
00:01:32,560 --> 00:01:36,880
to address these two problems intel

35
00:01:35,200 --> 00:01:39,759
re-architected dca

36
00:01:36,880 --> 00:01:40,000
and introduced data direct io technology

37
00:01:39,759 --> 00:01:43,439
or

38
00:01:40,000 --> 00:01:47,040
ddio in around 2011

39
00:01:43,439 --> 00:01:49,199
ddio made it possible to dma directly to

40
00:01:47,040 --> 00:01:50,320
and from last level cache in intel

41
00:01:49,200 --> 00:01:54,079
processors

42
00:01:50,320 --> 00:01:56,719
and thus avoid expensive memory accesses

43
00:01:54,079 --> 00:01:57,199
in this talk we will focus on ddi as the

44
00:01:56,719 --> 00:01:59,360
current

45
00:01:57,200 --> 00:02:00,640
implementation of dca in intel

46
00:01:59,360 --> 00:02:03,200
processors

47
00:02:00,640 --> 00:02:05,280
specifically we discuss its performance

48
00:02:03,200 --> 00:02:06,719
with respect to the recent trends in

49
00:02:05,280 --> 00:02:09,038
networking

50
00:02:06,719 --> 00:02:11,920
let's look at the two examples of these

51
00:02:09,038 --> 00:02:14,640
networking trends

52
00:02:11,920 --> 00:02:16,480
first new networking elements are now

53
00:02:14,640 --> 00:02:18,079
equipped with more processing and

54
00:02:16,480 --> 00:02:21,040
offloading capabilities

55
00:02:18,080 --> 00:02:22,239
which enables in-network computations

56
00:02:21,040 --> 00:02:24,480
consequently

57
00:02:22,239 --> 00:02:26,959
applications could push some of their

58
00:02:24,480 --> 00:02:29,519
costly calculations into the network

59
00:02:26,959 --> 00:02:30,879
and only perform stateful functions at

60
00:02:29,520 --> 00:02:33,599
the cpu

61
00:02:30,879 --> 00:02:35,518
which makes these applications more io

62
00:02:33,599 --> 00:02:38,000
intensive

63
00:02:35,519 --> 00:02:38,959
second if we look at the trend of

64
00:02:38,000 --> 00:02:41,840
linkage speeds

65
00:02:38,959 --> 00:02:44,160
in the last 40 years we can observe that

66
00:02:41,840 --> 00:02:44,800
we are getting closer to having a one

67
00:02:44,160 --> 00:02:47,920
therapy

68
00:02:44,800 --> 00:02:50,959
standard for ethernet however

69
00:02:47,920 --> 00:02:53,040
introducing faster link speed exposes

70
00:02:50,959 --> 00:02:54,319
the network elements to packets at a

71
00:02:53,040 --> 00:02:57,200
higher rate

72
00:02:54,319 --> 00:02:58,720
for instance if you consider 100 gigabit

73
00:02:57,200 --> 00:03:01,599
per second links

74
00:02:58,720 --> 00:03:04,400
theoretically a server receiving minimal

75
00:03:01,599 --> 00:03:07,599
sized frames receives a new frame

76
00:03:04,400 --> 00:03:10,080
every 6.72 nanosecond

77
00:03:07,599 --> 00:03:12,399
which is around 10 times faster than the

78
00:03:10,080 --> 00:03:15,280
memory access latency

79
00:03:12,400 --> 00:03:16,400
therefore multi hundred gigabit networks

80
00:03:15,280 --> 00:03:19,040
cannot tolerate

81
00:03:16,400 --> 00:03:21,040
memory access as this could cause a

82
00:03:19,040 --> 00:03:22,480
tremendous amount of buffering and

83
00:03:21,040 --> 00:03:25,200
packet loss

84
00:03:22,480 --> 00:03:28,640
moreover the problem only gets worse

85
00:03:25,200 --> 00:03:28,640
with increasing linkage speed

86
00:03:29,200 --> 00:03:34,720
with these trends in mind we conclude

87
00:03:31,920 --> 00:03:37,440
that dca is essential for multi hundred

88
00:03:34,720 --> 00:03:40,799
gigabit networks to process io at

89
00:03:37,440 --> 00:03:44,079
the line rate without increasing packet

90
00:03:40,799 --> 00:03:46,400
loss or latency to prove this

91
00:03:44,080 --> 00:03:48,799
we did an experiment to see the impact

92
00:03:46,400 --> 00:03:50,480
of dca on the performance of io

93
00:03:48,799 --> 00:03:52,879
intensive applications

94
00:03:50,480 --> 00:03:55,439
when moving toward multi hundred gigabit

95
00:03:52,879 --> 00:03:55,439
networks

96
00:03:55,599 --> 00:04:00,079
first we interconnected two servers with

97
00:03:58,000 --> 00:04:02,400
a 100 gigabit per second link

98
00:04:00,080 --> 00:04:04,640
one as a packet generator and another

99
00:04:02,400 --> 00:04:06,959
one as device under test

100
00:04:04,640 --> 00:04:08,079
you measure the latency when a packet

101
00:04:06,959 --> 00:04:10,640
generator

102
00:04:08,080 --> 00:04:12,400
replaces a real campus trace at its

103
00:04:10,640 --> 00:04:14,879
maximum possible rate

104
00:04:12,400 --> 00:04:16,959
while the dos under test is only for

105
00:04:14,879 --> 00:04:19,918
wiring packets

106
00:04:16,959 --> 00:04:22,960
the right hand side figure shows the 99

107
00:04:19,918 --> 00:04:25,359
percentile latency of these packets

108
00:04:22,960 --> 00:04:29,840
when the device under test is receiving

109
00:04:25,360 --> 00:04:29,840
packets at 100 gigabit per second

110
00:04:30,240 --> 00:04:35,280
next we added another nick to the server

111
00:04:33,040 --> 00:04:38,000
to see the impact of receiving packets

112
00:04:35,280 --> 00:04:40,638
at 200 gigabit per second

113
00:04:38,000 --> 00:04:43,520
the second bar in the figure shows the

114
00:04:40,639 --> 00:04:44,560
99 percentile latency of the traffic

115
00:04:43,520 --> 00:04:47,120
sent to the first

116
00:04:44,560 --> 00:04:49,600
nic while the second nick is also

117
00:04:47,120 --> 00:04:50,639
forwarding packets at 100 gigabit per

118
00:04:49,600 --> 00:04:53,360
second

119
00:04:50,639 --> 00:04:56,479
as shown the tail latency of the first

120
00:04:53,360 --> 00:05:00,400
snake increases by around 30 percent

121
00:04:56,479 --> 00:05:02,479
due to dca inefficiency at higher rates

122
00:05:00,400 --> 00:05:04,799
by the end of this talk you will see how

123
00:05:02,479 --> 00:05:06,320
to achieve the same latency as 100

124
00:05:04,800 --> 00:05:08,560
gigabit per second

125
00:05:06,320 --> 00:05:10,800
while still forwarding at an aggregate

126
00:05:08,560 --> 00:05:14,639
rate of 200 gigabit per second by

127
00:05:10,800 --> 00:05:18,479
optimizing ddio but before getting there

128
00:05:14,639 --> 00:05:18,479
let's see how ddio works

129
00:05:18,880 --> 00:05:23,120
in a ddi enabled system when a nic

130
00:05:21,759 --> 00:05:26,479
receives a packet

131
00:05:23,120 --> 00:05:28,639
two scenarios can happen if the cache

132
00:05:26,479 --> 00:05:30,880
lines associated with the packet are

133
00:05:28,639 --> 00:05:33,440
present in the last level cache

134
00:05:30,880 --> 00:05:34,880
ddi will update those cache lines with

135
00:05:33,440 --> 00:05:37,600
the new values

136
00:05:34,880 --> 00:05:38,479
this case is also known as write update

137
00:05:37,600 --> 00:05:42,160
or ddi

138
00:05:38,479 --> 00:05:44,880
write hit otherwise

139
00:05:42,160 --> 00:05:45,520
ddi allocates cache lines in a limited

140
00:05:44,880 --> 00:05:48,320
portion of

141
00:05:45,520 --> 00:05:50,080
llc and writes the packet in newly

142
00:05:48,320 --> 00:05:52,960
allocated cache lines

143
00:05:50,080 --> 00:05:55,198
which is also known as write allocate or

144
00:05:52,960 --> 00:05:57,919
ddi write miss

145
00:05:55,199 --> 00:05:59,360
intel recently mentioned that ddi uses

146
00:05:57,919 --> 00:06:01,520
two ways in llc

147
00:05:59,360 --> 00:06:03,840
but they do not reveal which ways are

148
00:06:01,520 --> 00:06:03,840
used

149
00:06:04,720 --> 00:06:09,360
when the packet is processed and is

150
00:06:06,560 --> 00:06:12,000
ready to be transferred back to the nick

151
00:06:09,360 --> 00:06:12,960
ddio enables the nic to read the packet

152
00:06:12,000 --> 00:06:16,240
directly from

153
00:06:12,960 --> 00:06:19,680
llc if it's present there this

154
00:06:16,240 --> 00:06:21,759
case is called ddi read hit

155
00:06:19,680 --> 00:06:23,600
otherwise the nic will read it from the

156
00:06:21,759 --> 00:06:26,880
main memory which is known as

157
00:06:23,600 --> 00:06:26,880
tdi read miss

158
00:06:27,280 --> 00:06:31,440
there are many details that we do not

159
00:06:29,120 --> 00:06:33,919
know about the implementation of ddi

160
00:06:31,440 --> 00:06:36,479
and intel processors for instance we do

161
00:06:33,919 --> 00:06:37,758
not know which llc base are used by ddio

162
00:06:36,479 --> 00:06:40,639
for allocation

163
00:06:37,759 --> 00:06:42,000
nor how ddi interacts with other

164
00:06:40,639 --> 00:06:44,560
applications

165
00:06:42,000 --> 00:06:46,960
nor do we know how a remote cpu socket

166
00:06:44,560 --> 00:06:48,400
affects the llc of a cpu in another

167
00:06:46,960 --> 00:06:50,638
socket

168
00:06:48,400 --> 00:06:52,000
we designed a set of micro benchmarks to

169
00:06:50,639 --> 00:06:55,120
answer these questions

170
00:06:52,000 --> 00:06:56,000
about the unknown details of ddio for

171
00:06:55,120 --> 00:06:58,400
brevity

172
00:06:56,000 --> 00:07:00,319
we only discuss our technique to

173
00:06:58,400 --> 00:07:03,840
identify the llc based

174
00:07:00,319 --> 00:07:03,840
used by ddio

175
00:07:05,120 --> 00:07:11,120
to identify the llc based used by ddio

176
00:07:08,319 --> 00:07:14,000
we run an audio application on a core

177
00:07:11,120 --> 00:07:16,639
and use cache allocation technology or

178
00:07:14,000 --> 00:07:19,440
cat to limit the code and data of the

179
00:07:16,639 --> 00:07:23,039
application to a subset of llc base

180
00:07:19,440 --> 00:07:23,039
that are shown on the slide

181
00:07:23,520 --> 00:07:27,599
next we run a cache sensitive

182
00:07:25,440 --> 00:07:30,240
application on a different core

183
00:07:27,599 --> 00:07:32,240
and we use charts to allocate different

184
00:07:30,240 --> 00:07:34,800
llc ways to it

185
00:07:32,240 --> 00:07:35,919
we measure the cache misses of the audio

186
00:07:34,800 --> 00:07:38,479
application

187
00:07:35,919 --> 00:07:42,318
while changing the llc ways allocated to

188
00:07:38,479 --> 00:07:42,318
the cache sensitive application

189
00:07:42,639 --> 00:07:48,240
the figure shows the sum of cache misses

190
00:07:44,960 --> 00:07:48,239
during the experiment

191
00:07:48,560 --> 00:07:53,039
next we start shifting the two ways

192
00:07:51,360 --> 00:07:54,000
allocated to the cache sensitive

193
00:07:53,039 --> 00:07:56,840
application

194
00:07:54,000 --> 00:07:59,360
one way to the right and repeat the same

195
00:07:56,840 --> 00:08:01,758
experiment

196
00:07:59,360 --> 00:08:04,720
the next few slides show the results of

197
00:08:01,759 --> 00:08:04,720
our experiments

198
00:08:09,039 --> 00:08:13,440
we noticed that when we limit both

199
00:08:11,120 --> 00:08:15,919
applications to the same llc waves we

200
00:08:13,440 --> 00:08:17,039
see a rise in the cache misses of the io

201
00:08:15,919 --> 00:08:19,440
application

202
00:08:17,039 --> 00:08:22,159
due to contention of code and data of

203
00:08:19,440 --> 00:08:24,960
both applications

204
00:08:22,160 --> 00:08:25,599
however if we continue the process until

205
00:08:24,960 --> 00:08:28,878
we cover

206
00:08:25,599 --> 00:08:28,878
all llc ways

207
00:08:34,479 --> 00:08:39,039
we will see another rise in the cache

208
00:08:36,799 --> 00:08:39,359
missiles of the io application which is

209
00:08:39,039 --> 00:08:42,159
not

210
00:08:39,360 --> 00:08:42,959
expectable as we have isolated the cpu

211
00:08:42,159 --> 00:08:45,519
circuit

212
00:08:42,958 --> 00:08:47,760
and no other application can cause cache

213
00:08:45,519 --> 00:08:49,760
misses

214
00:08:47,760 --> 00:08:52,480
therefore we can conclude that this

215
00:08:49,760 --> 00:08:53,839
unexpected interference is most probably

216
00:08:52,480 --> 00:08:57,440
due to i o

217
00:08:53,839 --> 00:08:58,720
which means ddi uses the two rightmost

218
00:08:57,440 --> 00:09:01,839
llc ways for

219
00:08:58,720 --> 00:09:03,920
allocation we use the same methodology

220
00:09:01,839 --> 00:09:06,240
to answer some other questions

221
00:09:03,920 --> 00:09:08,319
but if you are interested i encourage

222
00:09:06,240 --> 00:09:09,440
you to read our paper and use the source

223
00:09:08,320 --> 00:09:12,000
code we have made

224
00:09:09,440 --> 00:09:12,000
available

225
00:09:13,279 --> 00:09:16,399
we know from some recent publications

226
00:09:15,760 --> 00:09:19,920
such as

227
00:09:16,399 --> 00:09:22,959
rescue from nsdi 2018 and some intel

228
00:09:19,920 --> 00:09:25,360
reports that ddi cannot provide expected

229
00:09:22,959 --> 00:09:27,920
benefits in some scenarios

230
00:09:25,360 --> 00:09:30,399
specifically the cash line allocation

231
00:09:27,920 --> 00:09:32,719
done by ddio could evict the not yet

232
00:09:30,399 --> 00:09:33,040
processed and already processed packets

233
00:09:32,720 --> 00:09:36,560
from

234
00:09:33,040 --> 00:09:38,880
llc consequently packets should be read

235
00:09:36,560 --> 00:09:41,760
from the main memory rather than llc

236
00:09:38,880 --> 00:09:44,160
which is not desirable for us

237
00:09:41,760 --> 00:09:46,319
these documents mentioned that reducing

238
00:09:44,160 --> 00:09:49,040
the number of received descriptors

239
00:09:46,320 --> 00:09:51,120
could all could solve the problem but

240
00:09:49,040 --> 00:09:54,800
this is not a permanent solution

241
00:09:51,120 --> 00:09:57,120
for two reasons first

242
00:09:54,800 --> 00:09:59,599
if you measure the impact of using

243
00:09:57,120 --> 00:10:01,200
different numbers of rx descriptors for

244
00:09:59,600 --> 00:10:04,000
different packet sizes

245
00:10:01,200 --> 00:10:04,959
while one core is for writing packets we

246
00:10:04,000 --> 00:10:07,360
see that

247
00:10:04,959 --> 00:10:10,000
increasing the number of rx descriptors

248
00:10:07,360 --> 00:10:12,320
shown in the x axis

249
00:10:10,000 --> 00:10:13,680
and packet size shown with different

250
00:10:12,320 --> 00:10:16,880
colors

251
00:10:13,680 --> 00:10:18,800
reduces the ddi write heat rate this

252
00:10:16,880 --> 00:10:21,120
means that video has to keep

253
00:10:18,800 --> 00:10:23,199
allocating cash lines with a high number

254
00:10:21,120 --> 00:10:25,600
of rx descriptors

255
00:10:23,200 --> 00:10:26,399
however if we look at the case where we

256
00:10:25,600 --> 00:10:30,839
are receiving

257
00:10:26,399 --> 00:10:33,360
1500 packet by packets with 256 rx

258
00:10:30,839 --> 00:10:34,000
descriptors we see that this also

259
00:10:33,360 --> 00:10:36,560
happens

260
00:10:34,000 --> 00:10:37,760
even when the application's cache

261
00:10:36,560 --> 00:10:40,479
footprint is much

262
00:10:37,760 --> 00:10:43,120
smaller than the ddi capacity

263
00:10:40,480 --> 00:10:44,000
demonstrating that ddi cannot use the

264
00:10:43,120 --> 00:10:47,519
whole reserve

265
00:10:44,000 --> 00:10:50,079
capacity in llc moreover

266
00:10:47,519 --> 00:10:52,160
if you focus on this case and change the

267
00:10:50,079 --> 00:10:56,079
number of forwarding cores

268
00:10:52,160 --> 00:10:58,079
then as we can see in the next slide

269
00:10:56,079 --> 00:11:00,319
increasing the number of cores can

270
00:10:58,079 --> 00:11:03,040
reduce ddi performance

271
00:11:00,320 --> 00:11:05,120
furthermore since processors are now

272
00:11:03,040 --> 00:11:07,519
being shipped with more cores

273
00:11:05,120 --> 00:11:08,880
even if each core uses a small number of

274
00:11:07,519 --> 00:11:11,680
rx descriptors

275
00:11:08,880 --> 00:11:14,480
with more cores the required capacity

276
00:11:11,680 --> 00:11:16,479
for ddi will be insufficient which means

277
00:11:14,480 --> 00:11:17,760
increasing the number of cores does not

278
00:11:16,480 --> 00:11:21,440
always improve

279
00:11:17,760 --> 00:11:23,120
tdio performance therefore ddi needs to

280
00:11:21,440 --> 00:11:25,680
be able to perform

281
00:11:23,120 --> 00:11:27,200
well even with a high number of rx

282
00:11:25,680 --> 00:11:29,120
descriptors

283
00:11:27,200 --> 00:11:31,279
one way to address this would be to

284
00:11:29,120 --> 00:11:35,920
change the ddr capacity and thus

285
00:11:31,279 --> 00:11:35,920
provide room for more packets in the llc

286
00:11:36,880 --> 00:11:41,279
while analyzing the performance of ddion

287
00:11:39,279 --> 00:11:43,760
in different scenarios we noticed that

288
00:11:41,279 --> 00:11:47,040
tuning a little discuss register called

289
00:11:43,760 --> 00:11:48,640
iio llc base can improve the performance

290
00:11:47,040 --> 00:11:51,599
of ddio

291
00:11:48,640 --> 00:11:52,000
the default value of this register has

292
00:11:51,600 --> 00:11:55,440
two

293
00:11:52,000 --> 00:11:56,160
set bits which resembles the two llc

294
00:11:55,440 --> 00:11:59,920
ways

295
00:11:56,160 --> 00:12:01,040
used by ddio we change the values of

296
00:11:59,920 --> 00:12:03,599
this register

297
00:12:01,040 --> 00:12:06,079
while forwarding packets and we observe

298
00:12:03,600 --> 00:12:08,800
that increasing the number of bits set

299
00:12:06,079 --> 00:12:09,519
improves ddi performance by increasing

300
00:12:08,800 --> 00:12:12,880
the ddi

301
00:12:09,519 --> 00:12:15,120
rate and ddi write heat rates

302
00:12:12,880 --> 00:12:16,959
we believe that the values of this

303
00:12:15,120 --> 00:12:19,600
register might have a positive

304
00:12:16,959 --> 00:12:25,040
correlation with the ddr capacity

305
00:12:19,600 --> 00:12:27,519
which enables us to tune ddio

306
00:12:25,040 --> 00:12:29,120
we showed the impact of tuning ddi on

307
00:12:27,519 --> 00:12:32,240
the performance of ddi

308
00:12:29,120 --> 00:12:34,399
heat rates but those metrics could also

309
00:12:32,240 --> 00:12:36,639
affect the application level performance

310
00:12:34,399 --> 00:12:38,560
metrics based on the characteristics of

311
00:12:36,639 --> 00:12:40,639
an application

312
00:12:38,560 --> 00:12:42,880
to further investigate this we measure

313
00:12:40,639 --> 00:12:44,880
the tail intensity of an idle

314
00:12:42,880 --> 00:12:47,519
application that is forwarding large

315
00:12:44,880 --> 00:12:49,920
packets with two cores

316
00:12:47,519 --> 00:12:52,399
the figure shows the 99 percentile

317
00:12:49,920 --> 00:12:55,920
latency of the io application

318
00:12:52,399 --> 00:12:58,000
for different number of rx descriptors

319
00:12:55,920 --> 00:13:00,240
each color demonstrates a different

320
00:12:58,000 --> 00:13:02,639
number of bits set

321
00:13:00,240 --> 00:13:04,000
our experiment shows that increasing the

322
00:13:02,639 --> 00:13:07,360
number of bits set

323
00:13:04,000 --> 00:13:09,279
in the register enables us to forward

324
00:13:07,360 --> 00:13:10,320
packets with a larger number of

325
00:13:09,279 --> 00:13:13,200
descriptors

326
00:13:10,320 --> 00:13:15,760
while achieving better or similar pay

327
00:13:13,200 --> 00:13:15,760
latency

328
00:13:15,839 --> 00:13:20,959
these results demonstrate that setting

329
00:13:18,079 --> 00:13:21,920
more bits can reduce the 99 percentile

330
00:13:20,959 --> 00:13:25,599
latency by

331
00:13:21,920 --> 00:13:25,599
up to around 30

332
00:13:27,279 --> 00:13:30,800
so far we showed the benefits of tuning

333
00:13:29,680 --> 00:13:33,519
ddio

334
00:13:30,800 --> 00:13:35,680
but tuning is not the perfect solution

335
00:13:33,519 --> 00:13:38,959
for two reasons

336
00:13:35,680 --> 00:13:39,680
first the cache is also used for code

337
00:13:38,959 --> 00:13:42,239
and data

338
00:13:39,680 --> 00:13:43,040
therefore increasing ddi capacity could

339
00:13:42,240 --> 00:13:44,800
adversely

340
00:13:43,040 --> 00:13:47,040
affect the performance of cache

341
00:13:44,800 --> 00:13:50,000
sensitive applications

342
00:13:47,040 --> 00:13:52,160
second recent intel processors have

343
00:13:50,000 --> 00:13:53,839
shifted toward a non-inclusive cache

344
00:13:52,160 --> 00:13:56,639
hierarchy with a smaller

345
00:13:53,839 --> 00:13:59,120
core cache quota which means each core

346
00:13:56,639 --> 00:14:00,160
has a smaller capacity for accommodating

347
00:13:59,120 --> 00:14:02,880
packets

348
00:14:00,160 --> 00:14:06,560
and finally since ddi is way based

349
00:14:02,880 --> 00:14:08,959
tuning ddio will be very coarse grained

350
00:14:06,560 --> 00:14:10,638
therefore the next generation of dca

351
00:14:08,959 --> 00:14:13,680
should address these problems

352
00:14:10,639 --> 00:14:17,040
and perform a better cash management

353
00:14:13,680 --> 00:14:19,279
future dca is expected to provide

354
00:14:17,040 --> 00:14:22,480
fine grain placement similar to the

355
00:14:19,279 --> 00:14:26,240
solution proposed by cash director from

356
00:14:22,480 --> 00:14:29,040
eurasis 2019 and enable io isolation

357
00:14:26,240 --> 00:14:29,680
similar to chat or cdp and finally

358
00:14:29,040 --> 00:14:32,480
perform

359
00:14:29,680 --> 00:14:35,599
selective dca by only transferring the

360
00:14:32,480 --> 00:14:37,440
relevant parts of the packet to the llc

361
00:14:35,600 --> 00:14:39,120
but while we are waiting for the next

362
00:14:37,440 --> 00:14:41,440
generation of dca

363
00:14:39,120 --> 00:14:42,839
we should be able to still benefit from

364
00:14:41,440 --> 00:14:46,560
its current

365
00:14:42,839 --> 00:14:48,560
implementation we believe that the next

366
00:14:46,560 --> 00:14:51,359
generation of dca should not

367
00:14:48,560 --> 00:14:53,518
dma packets to the cash if they would

368
00:14:51,360 --> 00:14:56,079
cause io evictions

369
00:14:53,519 --> 00:14:57,920
this can be realized in current systems

370
00:14:56,079 --> 00:14:59,519
by bypassing the cache for some

371
00:14:57,920 --> 00:15:02,000
applications

372
00:14:59,519 --> 00:15:04,240
bypassing could be beneficial in setups

373
00:15:02,000 --> 00:15:07,279
with multiple applications

374
00:15:04,240 --> 00:15:07,760
each having a different priority in this

375
00:15:07,279 --> 00:15:10,399
case

376
00:15:07,760 --> 00:15:12,000
low priority applications could bypass

377
00:15:10,399 --> 00:15:15,199
the cache to make room

378
00:15:12,000 --> 00:15:17,600
for high priority applications current

379
00:15:15,199 --> 00:15:21,279
systems can bypass cache either

380
00:15:17,600 --> 00:15:27,040
by disabling ddi for a specific pci port

381
00:15:21,279 --> 00:15:30,000
or exploiting a remote socket for dma

382
00:15:27,040 --> 00:15:32,079
now let's get back to our 200 gigabit

383
00:15:30,000 --> 00:15:33,759
per second experiment that we discussed

384
00:15:32,079 --> 00:15:36,000
earlier in this talk

385
00:15:33,759 --> 00:15:37,120
where we showed that moving to our 200

386
00:15:36,000 --> 00:15:40,480
gigabit per second

387
00:15:37,120 --> 00:15:42,480
increases the tail latency by around 30

388
00:15:40,480 --> 00:15:44,480
now let's see whether we can use our

389
00:15:42,480 --> 00:15:46,800
acquired knowledge in this talk

390
00:15:44,480 --> 00:15:48,320
to achieve the same latency as 100

391
00:15:46,800 --> 00:15:50,800
gigabit per second

392
00:15:48,320 --> 00:15:52,079
while forwarding at 200 gigabit per

393
00:15:50,800 --> 00:15:54,959
second

394
00:15:52,079 --> 00:15:56,079
first we started by tuning ddi we

395
00:15:54,959 --> 00:15:58,479
repeated the same

396
00:15:56,079 --> 00:15:59,758
experiment and measured the tail latency

397
00:15:58,480 --> 00:16:02,959
of the first sneak

398
00:15:59,759 --> 00:16:06,240
while iio llc base register has

399
00:16:02,959 --> 00:16:09,279
four set bits the result showed that

400
00:16:06,240 --> 00:16:11,680
tuning ddi improves packet processing at

401
00:16:09,279 --> 00:16:14,480
200 gigabit per second

402
00:16:11,680 --> 00:16:14,880
second we disable ddio for the second

403
00:16:14,480 --> 00:16:17,519
nic

404
00:16:14,880 --> 00:16:19,439
connected to the cpu and we observe that

405
00:16:17,519 --> 00:16:22,720
disabling ddio also

406
00:16:19,440 --> 00:16:24,800
improves packet processing and finally

407
00:16:22,720 --> 00:16:26,480
instead of connecting the second nic to

408
00:16:24,800 --> 00:16:28,880
the same cpu socket

409
00:16:26,480 --> 00:16:29,759
we move the second nick to another cpu

410
00:16:28,880 --> 00:16:31,680
socket

411
00:16:29,759 --> 00:16:33,519
while keeping the application on the

412
00:16:31,680 --> 00:16:36,079
same cpu socket

413
00:16:33,519 --> 00:16:37,440
this experiment shows that by exploiting

414
00:16:36,079 --> 00:16:39,758
the remote socket

415
00:16:37,440 --> 00:16:40,480
we could achieve the same latency as the

416
00:16:39,759 --> 00:16:43,199
100 gig

417
00:16:40,480 --> 00:16:44,720
per second while forwarding at 200

418
00:16:43,199 --> 00:16:47,359
gigabit per second

419
00:16:44,720 --> 00:16:49,920
therefore bypassing cash or performing

420
00:16:47,360 --> 00:16:51,519
better cash management becomes important

421
00:16:49,920 --> 00:16:54,959
when shifting toward multi

422
00:16:51,519 --> 00:16:54,959
100 gigabit networks

423
00:16:55,600 --> 00:16:59,440
in addition to what we have discussed in

424
00:16:57,680 --> 00:17:01,680
this talk we have done many more

425
00:16:59,440 --> 00:17:02,560
experiments that could yield insight

426
00:17:01,680 --> 00:17:05,438
regarding

427
00:17:02,560 --> 00:17:06,240
ddi and audio intensive applications for

428
00:17:05,439 --> 00:17:09,280
example

429
00:17:06,240 --> 00:17:11,599
we try to answer the following questions

430
00:17:09,280 --> 00:17:13,199
how does receiving rate affect the ddi

431
00:17:11,599 --> 00:17:15,438
of performance

432
00:17:13,199 --> 00:17:17,360
or how does processing time affects the

433
00:17:15,439 --> 00:17:20,959
ddi performance

434
00:17:17,359 --> 00:17:22,159
and is ddi always beneficial we also

435
00:17:20,959 --> 00:17:25,679
check the impact of

436
00:17:22,160 --> 00:17:28,079
scaling up on the performance of ddio

437
00:17:25,679 --> 00:17:29,600
for more information please please see

438
00:17:28,079 --> 00:17:31,678
our paper

439
00:17:29,600 --> 00:17:33,760
in addition we also started the

440
00:17:31,679 --> 00:17:34,799
performance of ddio in different

441
00:17:33,760 --> 00:17:37,280
scenarios

442
00:17:34,799 --> 00:17:37,918
and give some key findings that could be

443
00:17:37,280 --> 00:17:42,639
used

444
00:17:37,919 --> 00:17:42,640
to optimize the ddi enabled systems

445
00:17:43,039 --> 00:17:48,799
some of our key findings are as follows

446
00:17:46,400 --> 00:17:51,120
we found out that using excessive cores

447
00:17:48,799 --> 00:17:53,360
for io could degrade the performance of

448
00:17:51,120 --> 00:17:55,918
io intensive applications

449
00:17:53,360 --> 00:17:57,678
as this increases the number of packets

450
00:17:55,919 --> 00:18:01,520
in the llc and it may cause

451
00:17:57,679 --> 00:18:03,280
contention for io we noticed that tuning

452
00:18:01,520 --> 00:18:06,080
a little discuss register

453
00:18:03,280 --> 00:18:08,399
called iio llc base could improve the

454
00:18:06,080 --> 00:18:11,439
performance of the ddio

455
00:18:08,400 --> 00:18:13,520
surprisingly we realized that tuning ddi

456
00:18:11,440 --> 00:18:15,840
could lead to the same improvements

457
00:18:13,520 --> 00:18:18,320
as adding more corpse when processing is

458
00:18:15,840 --> 00:18:20,240
not the bottleneck

459
00:18:18,320 --> 00:18:22,799
we observe that when an application

460
00:18:20,240 --> 00:18:24,880
scales up it's essential to balance

461
00:18:22,799 --> 00:18:27,679
load among cores to maximize the

462
00:18:24,880 --> 00:18:30,080
benefits of ddio

463
00:18:27,679 --> 00:18:32,000
we should we saw that ddi could become a

464
00:18:30,080 --> 00:18:33,840
bottleneck when the rate gets close to

465
00:18:32,000 --> 00:18:35,840
100 gigabit per second

466
00:18:33,840 --> 00:18:38,320
making it necessary to know when to

467
00:18:35,840 --> 00:18:41,039
bypass the cache to realize performance

468
00:18:38,320 --> 00:18:41,039
isolation

469
00:18:41,200 --> 00:18:45,440
last but not least we found out that

470
00:18:43,440 --> 00:18:48,000
tuning ddr is less efficient for

471
00:18:45,440 --> 00:18:51,200
applications that are truly cpu bound

472
00:18:48,000 --> 00:18:53,520
or memory bound for those of you who are

473
00:18:51,200 --> 00:18:54,480
interested we now explain the impact of

474
00:18:53,520 --> 00:18:56,960
processing time

475
00:18:54,480 --> 00:18:58,080
on the performance of ddio which

476
00:18:56,960 --> 00:19:01,120
resulted in this

477
00:18:58,080 --> 00:19:01,120
specific finding

478
00:19:01,919 --> 00:19:06,160
to see the impact of processing time and

479
00:19:04,160 --> 00:19:08,400
the performance of ddio

480
00:19:06,160 --> 00:19:09,360
we added atomic computation to our

481
00:19:08,400 --> 00:19:11,600
server

482
00:19:09,360 --> 00:19:13,760
in addition to forwarding packets the

483
00:19:11,600 --> 00:19:15,520
server also calls a random number

484
00:19:13,760 --> 00:19:17,520
generator

485
00:19:15,520 --> 00:19:19,120
the right hand side figure shows the

486
00:19:17,520 --> 00:19:21,120
ddio metric

487
00:19:19,120 --> 00:19:24,159
he trades when we vary the number of

488
00:19:21,120 --> 00:19:26,799
calls for the random number generated

489
00:19:24,160 --> 00:19:27,280
as we can see increasing the processing

490
00:19:26,799 --> 00:19:30,240
time

491
00:19:27,280 --> 00:19:32,320
improves the performance of ddio since

492
00:19:30,240 --> 00:19:36,720
the packets are injected to llc

493
00:19:32,320 --> 00:19:38,559
at a lower pace however if you also look

494
00:19:36,720 --> 00:19:41,200
at the throughput of the server

495
00:19:38,559 --> 00:19:41,600
we realize that the performance of ddi

496
00:19:41,200 --> 00:19:43,520
only

497
00:19:41,600 --> 00:19:45,039
improves when the processing power

498
00:19:43,520 --> 00:19:47,679
becomes a bottleneck

499
00:19:45,039 --> 00:19:48,480
and throughput starts to decrease in

500
00:19:47,679 --> 00:19:51,280
other words

501
00:19:48,480 --> 00:19:52,880
optimizing ddi matters most when an

502
00:19:51,280 --> 00:19:57,280
application is oil bound

503
00:19:52,880 --> 00:19:57,280
rather than cpu bound or memory bound

504
00:19:57,919 --> 00:20:02,000
in conclusion we showed the necessity of

505
00:20:00,480 --> 00:20:04,799
tuning dca

506
00:20:02,000 --> 00:20:06,000
and ddio for io intensive applications

507
00:20:04,799 --> 00:20:08,879
in order to be able

508
00:20:06,000 --> 00:20:09,440
to process packets at line rate in

509
00:20:08,880 --> 00:20:11,440
addition

510
00:20:09,440 --> 00:20:12,480
we suggested that dca should be

511
00:20:11,440 --> 00:20:14,880
re-architected

512
00:20:12,480 --> 00:20:17,120
while keeping an eye on the emergence of

513
00:20:14,880 --> 00:20:19,360
multi-100 gigabit networks

514
00:20:17,120 --> 00:20:21,120
and finally i encourage you to benchmark

515
00:20:19,360 --> 00:20:24,080
your testbed with our source code

516
00:20:21,120 --> 00:20:26,239
which is available on github with this i

517
00:20:24,080 --> 00:20:28,480
conclude my talk

518
00:20:26,240 --> 00:20:29,440
thanks for listening and do not hesitate

519
00:20:28,480 --> 00:20:35,840
to contact us

520
00:20:29,440 --> 00:20:35,840
if you have any questions

521
00:20:41,919 --> 00:20:44,000
you

