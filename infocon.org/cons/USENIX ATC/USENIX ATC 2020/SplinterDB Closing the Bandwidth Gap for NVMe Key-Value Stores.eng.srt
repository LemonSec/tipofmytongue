1
00:00:08,400 --> 00:00:11,599
hi my name is alex conway and i'm at

2
00:00:10,320 --> 00:00:13,280
vmware research

3
00:00:11,599 --> 00:00:16,880
and today i'm going to talk about a new

4
00:00:13,280 --> 00:00:18,720
key value store called splinterdb

5
00:00:16,880 --> 00:00:20,320
our goal with splinterdb was to create a

6
00:00:18,720 --> 00:00:21,840
key value store that would perform well

7
00:00:20,320 --> 00:00:23,199
across a wide range of parameters and

8
00:00:21,840 --> 00:00:25,198
configurations

9
00:00:23,199 --> 00:00:28,320
and this especially means focusing on

10
00:00:25,199 --> 00:00:30,000
some of the hard cases

11
00:00:28,320 --> 00:00:32,399
the three sort of hard cases that we

12
00:00:30,000 --> 00:00:35,040
focused on were the following

13
00:00:32,399 --> 00:00:36,480
the first is fast storage and our

14
00:00:35,040 --> 00:00:38,559
approach to fast storage is to use new

15
00:00:36,480 --> 00:00:41,040
data structures which lower io

16
00:00:38,559 --> 00:00:44,640
amplification and cpu overhead while

17
00:00:41,040 --> 00:00:46,559
enabling concurrency we also wanted to

18
00:00:44,640 --> 00:00:49,280
tackle the common hard case of small

19
00:00:46,559 --> 00:00:51,039
key value pairs and our approach to that

20
00:00:49,280 --> 00:00:52,320
is to keep our key value pairs sorted

21
00:00:51,039 --> 00:00:56,239
and packed into data blocks

22
00:00:52,320 --> 00:00:58,160
and to delay merging as much as possible

23
00:00:56,239 --> 00:01:00,559
we also wanted to do this while using

24
00:00:58,160 --> 00:01:02,078
only a small amount of cash

25
00:01:00,559 --> 00:01:03,519
and our approach to that is to make all

26
00:01:02,079 --> 00:01:05,439
of the data structures in splinter

27
00:01:03,520 --> 00:01:08,479
swappable in order to gracefully degrade

28
00:01:05,438 --> 00:01:08,479
under cash pressure

29
00:01:08,720 --> 00:01:11,280
in this talk i'm going to start by

30
00:01:10,000 --> 00:01:12,560
discussing some of the challenges with

31
00:01:11,280 --> 00:01:13,680
fast storage

32
00:01:12,560 --> 00:01:15,439
then i'm going to give an overview of

33
00:01:13,680 --> 00:01:15,759
splinter db and talk a little bit about

34
00:01:15,439 --> 00:01:18,639
its

35
00:01:15,759 --> 00:01:20,000
performance then i'm going to dive into

36
00:01:18,640 --> 00:01:21,840
some of the details of the data

37
00:01:20,000 --> 00:01:25,119
structure underlying splinter db

38
00:01:21,840 --> 00:01:26,960
the size tier be the epsilon tree

39
00:01:25,119 --> 00:01:28,799
finally i'm going to discuss a very cool

40
00:01:26,960 --> 00:01:32,399
algorithm enabled by that data structure

41
00:01:28,799 --> 00:01:32,400
called the flush and compact algorithm

42
00:01:33,119 --> 00:01:38,320
okay so let's start with fast storage

43
00:01:36,720 --> 00:01:40,880
back in the day people used to use all

44
00:01:38,320 --> 00:01:42,559
kinds of things like slide rules vhs

45
00:01:40,880 --> 00:01:43,759
tapes and fountain pens

46
00:01:42,560 --> 00:01:45,200
and they used to use different

47
00:01:43,759 --> 00:01:46,560
performance models depending on whether

48
00:01:45,200 --> 00:01:48,560
they were doing their

49
00:01:46,560 --> 00:01:50,720
processing internally or whether they

50
00:01:48,560 --> 00:01:52,479
were writing their data out to disk

51
00:01:50,720 --> 00:01:54,479
and you can see this for example if you

52
00:01:52,479 --> 00:01:57,200
look at key value stores

53
00:01:54,479 --> 00:01:58,799
so for internal key value stores you see

54
00:01:57,200 --> 00:02:00,320
data structures like hash tables and

55
00:01:58,799 --> 00:02:02,000
binary search trees

56
00:02:00,320 --> 00:02:04,719
and you see implementations like

57
00:02:02,000 --> 00:02:08,479
memcache d redis and others

58
00:02:04,719 --> 00:02:09,919
however if you look at uh

59
00:02:08,479 --> 00:02:11,920
key value stores that write their data

60
00:02:09,919 --> 00:02:13,839
to disk you'll see a different

61
00:02:11,920 --> 00:02:15,359
set of data structures like b trees and

62
00:02:13,840 --> 00:02:16,640
more modern data structures like log

63
00:02:15,360 --> 00:02:17,840
structured merged trees and b the

64
00:02:16,640 --> 00:02:19,279
epsilon trees

65
00:02:17,840 --> 00:02:23,840
the implementations that you'll see are

66
00:02:19,280 --> 00:02:23,840
different too like rocks db and cila

67
00:02:23,920 --> 00:02:27,359
and this makes a lot of sense if you

68
00:02:25,520 --> 00:02:29,840
look at the type of devices that these

69
00:02:27,360 --> 00:02:33,440
models were originally created for

70
00:02:29,840 --> 00:02:34,959
such as hard drives so let me take a few

71
00:02:33,440 --> 00:02:37,120
a minute to explain what's going on with

72
00:02:34,959 --> 00:02:38,879
these thermometers here on the left

73
00:02:37,120 --> 00:02:40,400
so suppose that we have a two gigahertz

74
00:02:38,879 --> 00:02:41,920
processor that's writing some

75
00:02:40,400 --> 00:02:44,319
that's processing some data that's being

76
00:02:41,920 --> 00:02:47,040
read or written to a hard drive

77
00:02:44,319 --> 00:02:48,560
and first suppose that uh the access

78
00:02:47,040 --> 00:02:50,480
pattern of that data is random so the

79
00:02:48,560 --> 00:02:52,000
processor is only going to process one

80
00:02:50,480 --> 00:02:55,599
64-bit word

81
00:02:52,000 --> 00:02:57,440
in every i o that the hard drive does

82
00:02:55,599 --> 00:02:58,799
in that case the process will have about

83
00:02:57,440 --> 00:03:00,480
200 million cycles

84
00:02:58,800 --> 00:03:02,400
to process that word which is

85
00:03:00,480 --> 00:03:04,000
essentially an eternity and that's

86
00:03:02,400 --> 00:03:05,599
shown over here on this red random

87
00:03:04,000 --> 00:03:07,200
thermometer

88
00:03:05,599 --> 00:03:09,359
if on the other hand the access pattern

89
00:03:07,200 --> 00:03:11,280
is sequential and the processor

90
00:03:09,360 --> 00:03:13,760
is going to be processing every single

91
00:03:11,280 --> 00:03:16,080
word in all of the ios

92
00:03:13,760 --> 00:03:18,000
in that case the the process will have

93
00:03:16,080 --> 00:03:20,159
about 120 cycles to process

94
00:03:18,000 --> 00:03:21,280
each word which is substantially fewer

95
00:03:20,159 --> 00:03:23,599
and that's shown on this purple

96
00:03:21,280 --> 00:03:26,080
thermometer here

97
00:03:23,599 --> 00:03:27,040
now solid state drives are substantially

98
00:03:26,080 --> 00:03:29,440
faster

99
00:03:27,040 --> 00:03:30,840
and this particular solid state device

100
00:03:29,440 --> 00:03:33,840
in this example

101
00:03:30,840 --> 00:03:36,799
uh leaves the processor about 100

102
00:03:33,840 --> 00:03:38,959
000 cycles uh to process each word in a

103
00:03:36,799 --> 00:03:40,879
random access pattern and about 30

104
00:03:38,959 --> 00:03:42,159
in a sequential access pattern which is

105
00:03:40,879 --> 00:03:43,599
already starting to get a little bit

106
00:03:42,159 --> 00:03:47,120
tight

107
00:03:43,599 --> 00:03:50,959
now on modern and block addressable nvme

108
00:03:47,120 --> 00:03:52,560
devices such as the intel optane series

109
00:03:50,959 --> 00:03:54,480
the process will have about 20 000

110
00:03:52,560 --> 00:03:55,680
cycles if the access pattern is random

111
00:03:54,480 --> 00:03:57,119
which is still a lot

112
00:03:55,680 --> 00:03:59,040
but in the case where the access pattern

113
00:03:57,120 --> 00:04:01,519
is sequential and the

114
00:03:59,040 --> 00:04:02,400
algorithm is using uh its ios

115
00:04:01,519 --> 00:04:04,159
efficiently

116
00:04:02,400 --> 00:04:05,599
the process will have about six cycles

117
00:04:04,159 --> 00:04:08,879
to process each word

118
00:04:05,599 --> 00:04:10,399
now that's a very small amount of time

119
00:04:08,879 --> 00:04:12,879
that maybe is enough time to route the

120
00:04:10,400 --> 00:04:14,879
data somewhere or do some simple hashing

121
00:04:12,879 --> 00:04:16,478
but it's not enough time to do

122
00:04:14,879 --> 00:04:19,440
substantial processing on the data

123
00:04:16,478 --> 00:04:21,279
do sorting or anything like that and so

124
00:04:19,440 --> 00:04:22,160
the takeaway from this is that for nvme

125
00:04:21,279 --> 00:04:24,320
we're going to need

126
00:04:22,160 --> 00:04:25,840
data structures that optimize both they

127
00:04:24,320 --> 00:04:27,040
need to be efficient with the i o

128
00:04:25,840 --> 00:04:28,880
but they're also going to have to be

129
00:04:27,040 --> 00:04:30,479
efficient with their cpu in order to

130
00:04:28,880 --> 00:04:32,880
keep up with the i o capabilities of the

131
00:04:30,479 --> 00:04:32,880
device

132
00:04:33,040 --> 00:04:36,560
as a result where for key value stores

133
00:04:35,280 --> 00:04:39,359
one will often talk about the right

134
00:04:36,560 --> 00:04:41,840
amplification of an insertion workload

135
00:04:39,360 --> 00:04:43,440
um you'll hear me in this talk refer to

136
00:04:41,840 --> 00:04:44,320
uh this sort of loose term work

137
00:04:43,440 --> 00:04:46,000
amplification

138
00:04:44,320 --> 00:04:49,599
which is meant to encapsulate both the i

139
00:04:46,000 --> 00:04:49,600
o and the cpu overhead

140
00:04:50,479 --> 00:04:56,479
okay so now i can introduce splinter db

141
00:04:54,479 --> 00:04:58,080
splinter db is a key value store which

142
00:04:56,479 --> 00:05:00,159
handles these tough cases

143
00:04:58,080 --> 00:05:02,080
fast storage small key value pairs and

144
00:05:00,160 --> 00:05:03,600
small cash

145
00:05:02,080 --> 00:05:05,120
now in this talk i'm mostly going to

146
00:05:03,600 --> 00:05:06,639
focus on the fast storage side of things

147
00:05:05,120 --> 00:05:08,840
but you will see that in our benchmarks

148
00:05:06,639 --> 00:05:11,360
we use small key value pairs and a small

149
00:05:08,840 --> 00:05:13,520
cache

150
00:05:11,360 --> 00:05:15,520
splinterdb approaches this problem by

151
00:05:13,520 --> 00:05:17,039
using a new

152
00:05:15,520 --> 00:05:19,359
a new data structure called the size

153
00:05:17,039 --> 00:05:21,759
tier b the epsilon tree

154
00:05:19,360 --> 00:05:23,199
and the idea behind the size to the

155
00:05:21,759 --> 00:05:23,759
epsilon tree is to reduce the amount of

156
00:05:23,199 --> 00:05:26,639
work

157
00:05:23,759 --> 00:05:29,039
that uh that workloads perform both in

158
00:05:26,639 --> 00:05:30,800
terms of i o and cpu

159
00:05:29,039 --> 00:05:33,039
while enabling a high degree of

160
00:05:30,800 --> 00:05:38,639
concurrency on both uh

161
00:05:33,039 --> 00:05:40,400
lookup and insertion workloads

162
00:05:38,639 --> 00:05:43,120
so before i dive into the details of the

163
00:05:40,400 --> 00:05:46,880
size tier be the epsilon tree

164
00:05:43,120 --> 00:05:46,880
let's look at splinter db's performance

165
00:05:47,360 --> 00:05:51,440
in order to do that let me introduce the

166
00:05:48,720 --> 00:05:54,160
state of the art rocks db

167
00:05:51,440 --> 00:05:55,600
reading from wikipedia roxdb is a high

168
00:05:54,160 --> 00:05:56,639
performance embedded database for key

169
00:05:55,600 --> 00:05:59,120
value data

170
00:05:56,639 --> 00:06:01,440
it is a fork of leveldb by facebook

171
00:05:59,120 --> 00:06:03,120
optimized to exploit many cpu cores

172
00:06:01,440 --> 00:06:07,840
and make efficient use of fast storage

173
00:06:03,120 --> 00:06:07,840
such as ssds for i o bound workloads

174
00:06:08,400 --> 00:06:12,638
so the setup that we use to benchmark

175
00:06:11,520 --> 00:06:14,960
splinterdb

176
00:06:12,639 --> 00:06:16,560
has an intel optane 905p block

177
00:06:14,960 --> 00:06:19,919
addressable nvme

178
00:06:16,560 --> 00:06:20,479
it has 32 gigahertz cores and we perform

179
00:06:19,919 --> 00:06:22,880
our

180
00:06:20,479 --> 00:06:23,599
benchmarks on small key value pairs and

181
00:06:22,880 --> 00:06:25,280
using

182
00:06:23,600 --> 00:06:28,720
a cache size that's about five percent

183
00:06:25,280 --> 00:06:28,719
of the total data set size

184
00:06:28,960 --> 00:06:32,239
so let's look first at insertion

185
00:06:30,960 --> 00:06:35,039
throughput

186
00:06:32,240 --> 00:06:36,639
so this plot here shows the insertion

187
00:06:35,039 --> 00:06:40,080
throughput for the ycsb

188
00:06:36,639 --> 00:06:41,360
load workload which is a uniform uh

189
00:06:40,080 --> 00:06:43,680
which is a workload that consists

190
00:06:41,360 --> 00:06:46,560
entirely of uniformly random

191
00:06:43,680 --> 00:06:48,000
keys so here the y-axis is the

192
00:06:46,560 --> 00:06:51,440
throughput in thousands of operations

193
00:06:48,000 --> 00:06:53,440
per second and so higher is better

194
00:06:51,440 --> 00:06:55,280
so as you can see splinterdb can perform

195
00:06:53,440 --> 00:06:57,840
about seven times as many insertions per

196
00:06:55,280 --> 00:06:59,599
second as roxdb

197
00:06:57,840 --> 00:07:01,198
what's interesting about this is if we

198
00:06:59,599 --> 00:07:03,759
look at the i o amplification

199
00:07:01,199 --> 00:07:04,880
that both systems uh incurred during

200
00:07:03,759 --> 00:07:06,639
this workload

201
00:07:04,880 --> 00:07:09,599
splinter db has about half the i o

202
00:07:06,639 --> 00:07:12,080
amplification of roxdb

203
00:07:09,599 --> 00:07:13,199
and so what this means is what this

204
00:07:12,080 --> 00:07:14,240
leaves us is that splinter db

205
00:07:13,199 --> 00:07:16,880
is using substantially more of the

206
00:07:14,240 --> 00:07:18,720
device bandwidth it uses 95 percent of

207
00:07:16,880 --> 00:07:21,039
the device bandwidth on this workload

208
00:07:18,720 --> 00:07:23,039
whereas roxdb only uses about 30 of

209
00:07:21,039 --> 00:07:24,800
device bandwidth

210
00:07:23,039 --> 00:07:27,199
now this insertion performance does not

211
00:07:24,800 --> 00:07:29,520
come at the cost of lookup performance

212
00:07:27,199 --> 00:07:30,960
so here on the right is the lookup

213
00:07:29,520 --> 00:07:33,359
performance from ycsb

214
00:07:30,960 --> 00:07:35,198
run c which is a workload entirely

215
00:07:33,360 --> 00:07:38,800
consisting of zippy and distributed

216
00:07:35,199 --> 00:07:40,240
uh lookups and the y-axis again

217
00:07:38,800 --> 00:07:41,599
throughput in thousands of operations

218
00:07:40,240 --> 00:07:43,360
per second

219
00:07:41,599 --> 00:07:45,759
as you can see splinerdb can perform

220
00:07:43,360 --> 00:07:48,879
about 40 more lookups per second than

221
00:07:45,759 --> 00:07:51,039
rocksdb on this workload

222
00:07:48,879 --> 00:07:52,720
if we look at the entire ycsb

223
00:07:51,039 --> 00:07:54,000
application suite

224
00:07:52,720 --> 00:07:56,319
you can see that splinterdb

225
00:07:54,000 --> 00:07:58,400
substantially outperforms roxdb on all

226
00:07:56,319 --> 00:08:02,639
of the workloads except for run e

227
00:07:58,400 --> 00:08:02,638
which consists mainly of short scans

228
00:08:05,680 --> 00:08:09,840
okay so how does splenidb achieve this

229
00:08:08,000 --> 00:08:11,919
performance

230
00:08:09,840 --> 00:08:13,840
it does so using a new data structure

231
00:08:11,919 --> 00:08:15,919
called the size turkey the epsilon tree

232
00:08:13,840 --> 00:08:17,359
but before i can describe that i'm going

233
00:08:15,919 --> 00:08:18,560
to go through a little data structural

234
00:08:17,360 --> 00:08:20,319
background

235
00:08:18,560 --> 00:08:22,400
and we're going to start quickly with b

236
00:08:20,319 --> 00:08:24,960
trees

237
00:08:22,400 --> 00:08:26,479
a b tree is a b are search tree the

238
00:08:24,960 --> 00:08:27,120
internal nodes consist entirely of

239
00:08:26,479 --> 00:08:28,800
pivots

240
00:08:27,120 --> 00:08:30,639
and the leaves consist entirely of key

241
00:08:28,800 --> 00:08:33,120
value pairs

242
00:08:30,639 --> 00:08:34,959
in order to insert a new key value pair

243
00:08:33,120 --> 00:08:37,440
we follow the pivots and the internal

244
00:08:34,958 --> 00:08:39,279
nodes down the tree until we reach a

245
00:08:37,440 --> 00:08:41,440
leaf at which point the key value pair

246
00:08:39,279 --> 00:08:43,760
can be inserted in the leaf

247
00:08:41,440 --> 00:08:45,200
it is well known that b trees have an

248
00:08:43,760 --> 00:08:46,240
insertion and lookup cost that is

249
00:08:45,200 --> 00:08:49,519
logarithmic

250
00:08:46,240 --> 00:08:52,080
in the number of key value pairs in the

251
00:08:49,519 --> 00:08:52,080
data set

252
00:08:53,440 --> 00:08:58,720
that takes us to b the epsilon trees

253
00:08:56,720 --> 00:09:00,000
a bdf's long tree is a search tree like

254
00:08:58,720 --> 00:09:02,000
a b tree

255
00:09:00,000 --> 00:09:04,000
but where a b tree has internal nodes

256
00:09:02,000 --> 00:09:06,399
that are consistent entirely of pivots

257
00:09:04,000 --> 00:09:08,160
internal nodes and a b epsilon tree have

258
00:09:06,399 --> 00:09:11,200
b to the epsilon pivots and the rest of

259
00:09:08,160 --> 00:09:11,199
the node is a buffer

260
00:09:12,160 --> 00:09:15,920
in order to insert a new key value pair

261
00:09:14,560 --> 00:09:17,680
into a b the epsilon tree

262
00:09:15,920 --> 00:09:20,399
we first start by just putting the key

263
00:09:17,680 --> 00:09:22,479
value pair in the root buffer

264
00:09:20,399 --> 00:09:24,399
once we've done so that key value pair

265
00:09:22,480 --> 00:09:26,480
is already incorporated into the data

266
00:09:24,399 --> 00:09:28,560
structure

267
00:09:26,480 --> 00:09:29,600
if we keep inserting key value pairs

268
00:09:28,560 --> 00:09:30,880
into the root

269
00:09:29,600 --> 00:09:34,240
at some point the root is going to

270
00:09:30,880 --> 00:09:35,760
become full when a buffer becomes full

271
00:09:34,240 --> 00:09:38,320
what we're going to do is we pick a

272
00:09:35,760 --> 00:09:39,920
child that's receiving the most messages

273
00:09:38,320 --> 00:09:41,920
and we're going to flush those messages

274
00:09:39,920 --> 00:09:43,439
down into that child's buffer

275
00:09:41,920 --> 00:09:45,680
this creates more room in the root

276
00:09:43,440 --> 00:09:48,240
buffer and now we can accept more

277
00:09:45,680 --> 00:09:48,239
insertions

278
00:09:49,760 --> 00:09:53,279
if the root buffer fills again we repeat

279
00:09:51,680 --> 00:09:54,640
the same process we flush down to the

280
00:09:53,279 --> 00:09:56,800
child and in this case the child's

281
00:09:54,640 --> 00:09:58,640
buffer is now full as well

282
00:09:56,800 --> 00:10:00,319
so we again pick a child receiving the

283
00:09:58,640 --> 00:10:00,880
most messages and flush them down the

284
00:10:00,320 --> 00:10:02,480
tree

285
00:10:00,880 --> 00:10:05,839
and this process can cascade down the

286
00:10:02,480 --> 00:10:05,839
tree all the way to the leaves

287
00:10:08,160 --> 00:10:12,240
look ups in b epsilon trees are very

288
00:10:09,760 --> 00:10:13,760
similar to lookups and b trees except

289
00:10:12,240 --> 00:10:15,440
we don't just follow the pivots down the

290
00:10:13,760 --> 00:10:16,800
tree we also have to look in the buffers

291
00:10:15,440 --> 00:10:19,920
along the way

292
00:10:16,800 --> 00:10:21,439
so if we're looking for the key 71 we

293
00:10:19,920 --> 00:10:23,760
start by looking in the root

294
00:10:21,440 --> 00:10:24,560
it 71 is not in the buffer so we use the

295
00:10:23,760 --> 00:10:26,959
pivots to

296
00:10:24,560 --> 00:10:28,239
progress down the tree we keep checking

297
00:10:26,959 --> 00:10:30,160
the buffers along the way

298
00:10:28,240 --> 00:10:31,519
and eventually we find 71 here in the

299
00:10:30,160 --> 00:10:33,839
leaf and we can return the correct

300
00:10:31,519 --> 00:10:33,839
answer

301
00:10:34,399 --> 00:10:38,720
so implicit in this algorithm

302
00:10:37,440 --> 00:10:40,720
is a certain amount of work

303
00:10:38,720 --> 00:10:44,640
amplification for

304
00:10:40,720 --> 00:10:46,640
insertions let's see why

305
00:10:44,640 --> 00:10:48,240
so here i've zoomed in on one particular

306
00:10:46,640 --> 00:10:50,000
bd epsilon tree node

307
00:10:48,240 --> 00:10:52,320
and it has some data that's already in

308
00:10:50,000 --> 00:10:53,920
the node it has three key value pairs

309
00:10:52,320 --> 00:10:55,920
when i flush some more data into the

310
00:10:53,920 --> 00:10:57,680
node we're gonna have to rewrite the key

311
00:10:55,920 --> 00:10:59,439
value pairs that were already in that

312
00:10:57,680 --> 00:11:01,040
node

313
00:10:59,440 --> 00:11:02,560
if we keep flushing data into node we're

314
00:11:01,040 --> 00:11:04,240
going to have to keep rewriting all that

315
00:11:02,560 --> 00:11:07,119
data that already is there

316
00:11:04,240 --> 00:11:07,600
over and over again and in the worst

317
00:11:07,120 --> 00:11:08,800
case

318
00:11:07,600 --> 00:11:10,880
the average message is going to be

319
00:11:08,800 --> 00:11:13,439
written be the epsilon time over two

320
00:11:10,880 --> 00:11:16,399
times in each node

321
00:11:13,440 --> 00:11:17,120
and note that this merging and rewriting

322
00:11:16,399 --> 00:11:19,440
of the

323
00:11:17,120 --> 00:11:20,800
node incurs both an io overhead of

324
00:11:19,440 --> 00:11:22,800
having to write the node out again but

325
00:11:20,800 --> 00:11:23,519
also cpu overhead of having to process

326
00:11:22,800 --> 00:11:26,399
that data

327
00:11:23,519 --> 00:11:26,399
and collect it together

328
00:11:26,720 --> 00:11:30,320
if we multiply this out by the height of

329
00:11:28,399 --> 00:11:33,440
the tree we get a work amplification

330
00:11:30,320 --> 00:11:33,440
that looks like this

331
00:11:35,519 --> 00:11:39,279
splinterdb uses a new data structure

332
00:11:37,120 --> 00:11:40,800
called the size to be the epsilon tree

333
00:11:39,279 --> 00:11:43,839
that tries to address this work

334
00:11:40,800 --> 00:11:43,839
amplification problem

335
00:11:44,160 --> 00:11:48,560
so a size 2 bd epsilon tree is similar

336
00:11:47,360 --> 00:11:51,600
to a b to epsilon tree

337
00:11:48,560 --> 00:11:54,160
in that each node has pivots and buffer

338
00:11:51,600 --> 00:11:55,600
however in a size 2 b epsilon tree the

339
00:11:54,160 --> 00:11:57,040
buffer is stored separately from the

340
00:11:55,600 --> 00:12:00,720
pivots

341
00:11:57,040 --> 00:12:02,240
and in several pieces now

342
00:12:00,720 --> 00:12:04,000
i'm going to refer to this part the part

343
00:12:02,240 --> 00:12:06,480
of the node with the pivots as the trunk

344
00:12:04,000 --> 00:12:07,680
of the node and i'm going to refer to

345
00:12:06,480 --> 00:12:11,839
these buffers that

346
00:12:07,680 --> 00:12:11,839
are these disjoint buffers as branches

347
00:12:12,160 --> 00:12:15,360
so how does an insertion how do

348
00:12:13,920 --> 00:12:17,360
insertions work in size chirp b the

349
00:12:15,360 --> 00:12:19,360
epsilon trees

350
00:12:17,360 --> 00:12:21,120
so here suppose we have an empty node

351
00:12:19,360 --> 00:12:23,200
with these three pivots

352
00:12:21,120 --> 00:12:24,240
um so it just has a trunk and no

353
00:12:23,200 --> 00:12:26,079
branches

354
00:12:24,240 --> 00:12:28,399
when new data is flushed into this node

355
00:12:26,079 --> 00:12:31,199
from higher up in the tree

356
00:12:28,399 --> 00:12:32,240
it gets added to the trunk as a new

357
00:12:31,200 --> 00:12:34,399
branch

358
00:12:32,240 --> 00:12:36,720
so we just set a little pointer in the

359
00:12:34,399 --> 00:12:38,480
trunk that points to this new branch

360
00:12:36,720 --> 00:12:39,760
when more data comes in is flush from

361
00:12:38,480 --> 00:12:40,959
above

362
00:12:39,760 --> 00:12:42,880
we're going to do the same thing we're

363
00:12:40,959 --> 00:12:44,000
going to add it as a new branch just

364
00:12:42,880 --> 00:12:46,240
using a pointer

365
00:12:44,000 --> 00:12:47,600
and we don't have to rewrite any of the

366
00:12:46,240 --> 00:12:49,120
old branches that are already in the

367
00:12:47,600 --> 00:12:50,880
node

368
00:12:49,120 --> 00:12:52,560
so note here that these branches may

369
00:12:50,880 --> 00:12:54,240
have overlapping key ranges

370
00:12:52,560 --> 00:12:56,319
so if i color the pivots of this node

371
00:12:54,240 --> 00:12:58,639
and their children and i color in the

372
00:12:56,320 --> 00:13:00,000
key value pairs with the child

373
00:12:58,639 --> 00:13:01,680
that they're going to be flushed to

374
00:13:00,000 --> 00:13:02,880
eventually you can see that these

375
00:13:01,680 --> 00:13:06,399
branches have

376
00:13:02,880 --> 00:13:08,240
can have keys that can go to any child

377
00:13:06,399 --> 00:13:10,639
so let's add another branch to this

378
00:13:08,240 --> 00:13:14,160
flush another branch into this node

379
00:13:10,639 --> 00:13:14,160
and now suppose the node is full

380
00:13:14,560 --> 00:13:17,680
what's going to happen is exactly what

381
00:13:15,920 --> 00:13:19,120
happens in a beat the epsilon tree we're

382
00:13:17,680 --> 00:13:21,120
going to pick a child that's receiving

383
00:13:19,120 --> 00:13:22,399
the most messages

384
00:13:21,120 --> 00:13:24,160
and we're going to merge them together

385
00:13:22,399 --> 00:13:25,519
into a new branch

386
00:13:24,160 --> 00:13:27,760
and we're going to flush that branch

387
00:13:25,519 --> 00:13:29,920
down into the child

388
00:13:27,760 --> 00:13:30,959
so note that in this process each key

389
00:13:29,920 --> 00:13:33,599
value pair

390
00:13:30,959 --> 00:13:34,479
is read and written exactly once in each

391
00:13:33,600 --> 00:13:37,120
trunk node

392
00:13:34,480 --> 00:13:39,519
during this last merging or compaction

393
00:13:37,120 --> 00:13:39,519
phase

394
00:13:39,920 --> 00:13:43,519
as a result if we multiply this out by

395
00:13:41,920 --> 00:13:45,839
the height of the tree

396
00:13:43,519 --> 00:13:47,199
we can see that a size 3 b epsilon tree

397
00:13:45,839 --> 00:13:47,519
on an insertion workload it's going to

398
00:13:47,199 --> 00:13:49,120
have

399
00:13:47,519 --> 00:13:51,600
b the epsilon times less work

400
00:13:49,120 --> 00:13:54,800
amplification

401
00:13:51,600 --> 00:13:56,639
and that's and that's great and this is

402
00:13:54,800 --> 00:13:59,839
also reflected in the insertion cost in

403
00:13:56,639 --> 00:13:59,839
the external memory model

404
00:14:01,120 --> 00:14:04,160
so let's look at lookups in a size to be

405
00:14:02,880 --> 00:14:05,680
the epsilon tree

406
00:14:04,160 --> 00:14:08,160
they're going to function very similarly

407
00:14:05,680 --> 00:14:10,000
to lookups in a b the epsilon tree

408
00:14:08,160 --> 00:14:11,519
so if we look for 71 we're going to

409
00:14:10,000 --> 00:14:14,399
start in the root and we're going to

410
00:14:11,519 --> 00:14:16,800
check all the branches along the path

411
00:14:14,399 --> 00:14:17,440
down the trade so here these rectangles

412
00:14:16,800 --> 00:14:19,120
represent

413
00:14:17,440 --> 00:14:21,600
uh branches which i've sort of squashed

414
00:14:19,120 --> 00:14:23,120
down to simplify the diagram

415
00:14:21,600 --> 00:14:24,959
so we're going to look in each branch of

416
00:14:23,120 --> 00:14:26,720
the root and having done so we're going

417
00:14:24,959 --> 00:14:28,719
to move down the root using the pivots

418
00:14:26,720 --> 00:14:30,160
down the tree using the pivots we'll

419
00:14:28,720 --> 00:14:30,720
look all the branches in this internal

420
00:14:30,160 --> 00:14:32,319
node

421
00:14:30,720 --> 00:14:33,680
and finally we get to a leaf we'll look

422
00:14:32,320 --> 00:14:35,600
in all branches and perhaps in one of

423
00:14:33,680 --> 00:14:38,800
these branches we'll find 71

424
00:14:35,600 --> 00:14:41,920
and we can return the correct answer

425
00:14:38,800 --> 00:14:42,800
so the problem here is that because

426
00:14:41,920 --> 00:14:44,639
these branches are stored

427
00:14:42,800 --> 00:14:46,880
discontinuously we can end up doing b

428
00:14:44,639 --> 00:14:47,519
the epsilon times more ios in the worst

429
00:14:46,880 --> 00:14:49,920
case

430
00:14:47,519 --> 00:14:51,839
to try and find our key and that's not

431
00:14:49,920 --> 00:14:54,959
really what we want to do

432
00:14:51,839 --> 00:14:55,199
so let's go about fixing lookups in size

433
00:14:54,959 --> 00:14:58,880
to

434
00:14:55,199 --> 00:15:00,639
be the epsilon trees the problem is that

435
00:14:58,880 --> 00:15:02,000
each node has these multiple branches

436
00:15:00,639 --> 00:15:03,360
that we have to check when we're doing a

437
00:15:02,000 --> 00:15:04,800
lookup

438
00:15:03,360 --> 00:15:06,480
and the idea that we're going to use to

439
00:15:04,800 --> 00:15:08,240
fix this is to use filters to avoid

440
00:15:06,480 --> 00:15:09,760
searching all of them

441
00:15:08,240 --> 00:15:11,279
a filter is a probabilistic data

442
00:15:09,760 --> 00:15:13,199
structure which answers membership

443
00:15:11,279 --> 00:15:14,320
queries with one-sided error and

444
00:15:13,199 --> 00:15:16,800
particularly they have no

445
00:15:14,320 --> 00:15:18,560
false negatives there are many kinds of

446
00:15:16,800 --> 00:15:20,560
filters such as bloom filters cuckoo

447
00:15:18,560 --> 00:15:22,638
filters and quotient filters

448
00:15:20,560 --> 00:15:24,160
and splenidb uses quotient filters for

449
00:15:22,639 --> 00:15:26,880
this purpose

450
00:15:24,160 --> 00:15:27,760
so in this node i've drawn uh i've

451
00:15:26,880 --> 00:15:29,600
pictured

452
00:15:27,760 --> 00:15:31,360
these pink sieves here that represent a

453
00:15:29,600 --> 00:15:34,959
quotient filter for each branch

454
00:15:31,360 --> 00:15:36,480
so we have one per branch now

455
00:15:34,959 --> 00:15:37,758
uh when a lookup is searching the node

456
00:15:36,480 --> 00:15:39,279
it's only going to search those branches

457
00:15:37,759 --> 00:15:43,120
which contain the key

458
00:15:39,279 --> 00:15:46,160
plus some occasional false positives

459
00:15:43,120 --> 00:15:47,519
so if we're looking for 64 we will look

460
00:15:46,160 --> 00:15:49,040
in this node here

461
00:15:47,519 --> 00:15:50,480
and we will look in all the filters and

462
00:15:49,040 --> 00:15:52,160
the filters will tell us whether we have

463
00:15:50,480 --> 00:15:54,560
to look in the branches or not

464
00:15:52,160 --> 00:15:56,240
and so perhaps in this last one we do

465
00:15:54,560 --> 00:15:59,439
have to look because there is 64 and we

466
00:15:56,240 --> 00:15:59,440
can return the correct answer

467
00:16:00,800 --> 00:16:04,319
now if we can make this false positive

468
00:16:02,560 --> 00:16:06,000
rate low enough

469
00:16:04,320 --> 00:16:08,000
then we can perform lookups in an

470
00:16:06,000 --> 00:16:09,839
expected constant number of ios

471
00:16:08,000 --> 00:16:13,040
and in practice this is usually about

472
00:16:09,839 --> 00:16:13,040
one io in general

473
00:16:15,839 --> 00:16:20,000
so to recap the size tier b epsilon tree

474
00:16:18,639 --> 00:16:21,120
is this data structure which has less

475
00:16:20,000 --> 00:16:23,680
compaction

476
00:16:21,120 --> 00:16:25,040
than similar data structures related

477
00:16:23,680 --> 00:16:27,040
data structures like bd epsilon trees

478
00:16:25,040 --> 00:16:29,839
and log structured merge trees

479
00:16:27,040 --> 00:16:31,920
uh and as a result it performs less i o

480
00:16:29,839 --> 00:16:33,279
and also has less cpu overhead

481
00:16:31,920 --> 00:16:35,279
it does this while maintaining a low

482
00:16:33,279 --> 00:16:37,920
lookup cost and while i didn't get to

483
00:16:35,279 --> 00:16:39,759
talk about scans

484
00:16:37,920 --> 00:16:42,240
it has slightly more expensive short

485
00:16:39,759 --> 00:16:43,279
scans but can perform long scans at disk

486
00:16:42,240 --> 00:16:45,279
bandwidth

487
00:16:43,279 --> 00:16:48,880
i encourage you to see the paper for

488
00:16:45,279 --> 00:16:48,880
lots more details about scans

489
00:16:49,600 --> 00:16:53,600
okay so now i'm going to discuss a cool

490
00:16:52,240 --> 00:16:54,880
algorithm called flush then compact

491
00:16:53,600 --> 00:16:58,320
which is enabled by the

492
00:16:54,880 --> 00:16:58,320
size tier to be the epsilon tree

493
00:16:59,680 --> 00:17:03,040
so flush and compact is an algorithm

494
00:17:01,440 --> 00:17:04,799
that's motivated by sequential

495
00:17:03,040 --> 00:17:07,199
insertions in b trees and be the epsilon

496
00:17:04,799 --> 00:17:07,199
trees

497
00:17:08,160 --> 00:17:13,120
so if we do an insertion into a b tree

498
00:17:13,359 --> 00:17:17,198
it's going to bring the entire root leaf

499
00:17:15,039 --> 00:17:19,359
path into cash

500
00:17:17,199 --> 00:17:20,799
so as a result if we perform sequential

501
00:17:19,359 --> 00:17:22,240
insertions

502
00:17:20,799 --> 00:17:24,400
those subsequent insertions are going to

503
00:17:22,240 --> 00:17:25,520
be cheaper because the entire retrieve

504
00:17:24,400 --> 00:17:27,760
path is in cache

505
00:17:25,520 --> 00:17:28,879
they will be they it will not incur any

506
00:17:27,760 --> 00:17:30,960
io until we

507
00:17:28,880 --> 00:17:33,360
reach a boundary of the leaf or boundary

508
00:17:30,960 --> 00:17:36,000
of an internal node

509
00:17:33,360 --> 00:17:39,280
similarly with bt epsilon trees if we

510
00:17:36,000 --> 00:17:40,960
perform sequential insertions

511
00:17:39,280 --> 00:17:42,559
they're going to fill up the root and

512
00:17:40,960 --> 00:17:43,840
then that's going to trigger a flush

513
00:17:42,559 --> 00:17:45,840
that's going to go all the way down the

514
00:17:43,840 --> 00:17:48,799
tree into the leaf and that will bring

515
00:17:45,840 --> 00:17:50,480
all these nodes into cache

516
00:17:48,799 --> 00:17:51,760
now subsequent insertions are going to

517
00:17:50,480 --> 00:17:52,400
be cheaper because again they're only

518
00:17:51,760 --> 00:17:55,760
going to incur

519
00:17:52,400 --> 00:17:55,760
i o at the node boundaries

520
00:17:57,840 --> 00:18:01,199
so this motivates the idea behind flush

521
00:18:00,160 --> 00:18:02,559
the compact

522
00:18:01,200 --> 00:18:04,000
we'd like to perform sequential

523
00:18:02,559 --> 00:18:04,720
insertions that also have this property

524
00:18:04,000 --> 00:18:07,919
that they

525
00:18:04,720 --> 00:18:10,240
incur less io that

526
00:18:07,919 --> 00:18:12,000
less io but we also want them to incur

527
00:18:10,240 --> 00:18:13,280
less cpu overhead and so the idea is we

528
00:18:12,000 --> 00:18:13,679
would like sequential insertions to

529
00:18:13,280 --> 00:18:17,918
incur

530
00:18:13,679 --> 00:18:20,320
less compaction total so

531
00:18:17,919 --> 00:18:21,360
here is a a regular sized here be the

532
00:18:20,320 --> 00:18:23,840
epsilon tree

533
00:18:21,360 --> 00:18:25,439
and let's see what happens when we

534
00:18:23,840 --> 00:18:27,918
perform sequential insertion

535
00:18:25,440 --> 00:18:29,679
so here is a node it has a collection of

536
00:18:27,919 --> 00:18:32,720
branches that contain

537
00:18:29,679 --> 00:18:34,240
some sequential insertions if we flush

538
00:18:32,720 --> 00:18:37,360
these down into the child

539
00:18:34,240 --> 00:18:39,280
as a new branch and there's already some

540
00:18:37,360 --> 00:18:41,520
data that's present in that child

541
00:18:39,280 --> 00:18:43,039
this is going to trigger a flush again

542
00:18:41,520 --> 00:18:44,639
but before that

543
00:18:43,039 --> 00:18:46,400
all this data is going to be compacted

544
00:18:44,640 --> 00:18:48,559
together and so we can still end up

545
00:18:46,400 --> 00:18:49,840
performing a merge of data on each level

546
00:18:48,559 --> 00:18:51,120
of the tree

547
00:18:49,840 --> 00:18:53,280
and that means that we're going to

548
00:18:51,120 --> 00:18:56,159
perform the cpu

549
00:18:53,280 --> 00:18:58,399
and potentially the i o the i o doing

550
00:18:56,160 --> 00:18:58,400
that

551
00:19:00,240 --> 00:19:05,760
so let's look at what how flushed and

552
00:19:02,880 --> 00:19:08,960
compact addresses this problem

553
00:19:05,760 --> 00:19:11,200
so here i'm going to replace

554
00:19:08,960 --> 00:19:12,880
i'm gonna i'm gonna draw the this box

555
00:19:11,200 --> 00:19:13,360
around these branches with this one big

556
00:19:12,880 --> 00:19:15,600
arrow

557
00:19:13,360 --> 00:19:16,399
to represent all the uh all the pointers

558
00:19:15,600 --> 00:19:18,159
from the node

559
00:19:16,400 --> 00:19:20,880
to those branches and this will help

560
00:19:18,160 --> 00:19:23,120
simplify this diagram

561
00:19:20,880 --> 00:19:24,160
okay so here suppose we're gonna flush

562
00:19:23,120 --> 00:19:26,239
this data down

563
00:19:24,160 --> 00:19:27,679
to the child the way flush the compact

564
00:19:26,240 --> 00:19:29,039
works is we're first going to flush

565
00:19:27,679 --> 00:19:32,400
references to the branches

566
00:19:29,039 --> 00:19:34,799
but we're not going to do any compaction

567
00:19:32,400 --> 00:19:35,679
then we're going to use metadata to mask

568
00:19:34,799 --> 00:19:38,240
out the data

569
00:19:35,679 --> 00:19:39,919
so each node sees the data it's

570
00:19:38,240 --> 00:19:41,360
logically supposed to have

571
00:19:39,919 --> 00:19:43,280
in particular the parent is only going

572
00:19:41,360 --> 00:19:44,639
to see the unflushed data

573
00:19:43,280 --> 00:19:47,120
and the child is only going to see the

574
00:19:44,640 --> 00:19:49,600
flush data in these branches

575
00:19:47,120 --> 00:19:50,719
now before compacting we're going to

576
00:19:49,600 --> 00:19:53,678
flush again

577
00:19:50,720 --> 00:19:54,960
so suppose this child node here is now

578
00:19:53,679 --> 00:19:57,280
full

579
00:19:54,960 --> 00:19:58,320
it can flush to the grandchild and the

580
00:19:57,280 --> 00:20:00,240
grandchild now

581
00:19:58,320 --> 00:20:01,520
will have references to these branches

582
00:20:00,240 --> 00:20:02,159
and any other branches that were in the

583
00:20:01,520 --> 00:20:04,400
child

584
00:20:02,159 --> 00:20:06,240
and we'll again use metadata in order to

585
00:20:04,400 --> 00:20:10,240
make sure each node can only see that

586
00:20:06,240 --> 00:20:12,240
the the key value pairs it's supposed to

587
00:20:10,240 --> 00:20:14,080
finally we're going to kick off

588
00:20:12,240 --> 00:20:16,080
asynchronous compaction jobs

589
00:20:14,080 --> 00:20:20,799
to clean up these references and compact

590
00:20:16,080 --> 00:20:23,439
them into single branches again

591
00:20:20,799 --> 00:20:23,918
so the advantages to this algorithm are

592
00:20:23,440 --> 00:20:25,039
is

593
00:20:23,919 --> 00:20:26,720
the first advantage of this algorithm is

594
00:20:25,039 --> 00:20:28,960
that no work is immediate is done on

595
00:20:26,720 --> 00:20:31,120
data that is immediately flushed again

596
00:20:28,960 --> 00:20:33,520
so any of the key value pairs that get

597
00:20:31,120 --> 00:20:35,360
flushed from the child to the grandchild

598
00:20:33,520 --> 00:20:37,918
aren't going to be compacted in in the

599
00:20:35,360 --> 00:20:37,918
child level

600
00:20:38,640 --> 00:20:42,080
and the extreme example of this is when

601
00:20:40,480 --> 00:20:43,440
we're doing sequential insertions

602
00:20:42,080 --> 00:20:45,520
sequential insertions are going to

603
00:20:43,440 --> 00:20:47,679
trigger flushes that go all the way down

604
00:20:45,520 --> 00:20:50,720
the tree from the root to the leaf

605
00:20:47,679 --> 00:20:52,320
before any compaction is is performed

606
00:20:50,720 --> 00:20:54,240
and so they will only get compacted at

607
00:20:52,320 --> 00:20:56,240
the end in the leaf

608
00:20:54,240 --> 00:20:57,840
so pure sequential insertion should have

609
00:20:56,240 --> 00:20:59,679
a work amplification that's near one

610
00:20:57,840 --> 00:21:02,480
they will only be compacted in one place

611
00:20:59,679 --> 00:21:04,559
the leaf

612
00:21:02,480 --> 00:21:05,600
this also breaks a serial chain of

613
00:21:04,559 --> 00:21:07,360
compactions

614
00:21:05,600 --> 00:21:09,199
into a parallel collection of

615
00:21:07,360 --> 00:21:09,918
compactions and these can happen

616
00:21:09,200 --> 00:21:11,760
concurrently

617
00:21:09,919 --> 00:21:14,640
across trunk nodes and even within a

618
00:21:11,760 --> 00:21:14,640
single trunk node

619
00:21:14,720 --> 00:21:18,640
and so this should result in improved

620
00:21:17,200 --> 00:21:21,039
insertion concurrency

621
00:21:18,640 --> 00:21:21,039
as well

622
00:21:21,919 --> 00:21:26,000
so let's see how this actually works out

623
00:21:24,400 --> 00:21:28,240
empirically

624
00:21:26,000 --> 00:21:29,919
so in this plot i have the result of a

625
00:21:28,240 --> 00:21:32,159
single threaded workload which has a

626
00:21:29,919 --> 00:21:33,760
percentage of sequential insertions

627
00:21:32,159 --> 00:21:35,280
and a fixed percentage of sequential

628
00:21:33,760 --> 00:21:36,400
insertions and the rest are random

629
00:21:35,280 --> 00:21:38,320
insertions

630
00:21:36,400 --> 00:21:40,799
so here the x-axis is the percentage of

631
00:21:38,320 --> 00:21:42,879
the insertions which are sequential

632
00:21:40,799 --> 00:21:44,240
so on the far left we have uniformly

633
00:21:42,880 --> 00:21:46,000
random insertion

634
00:21:44,240 --> 00:21:48,159
workload on the far right we have a

635
00:21:46,000 --> 00:21:50,559
uniformly sequential insertion workload

636
00:21:48,159 --> 00:21:52,720
and note that the x-axis is not to scale

637
00:21:50,559 --> 00:21:54,240
for clarity

638
00:21:52,720 --> 00:21:56,840
the y-axis is the throughput in

639
00:21:54,240 --> 00:21:58,000
insertions per second so higher is

640
00:21:56,840 --> 00:22:00,639
better

641
00:21:58,000 --> 00:22:02,320
so as you can see splinterdb smoothly

642
00:22:00,640 --> 00:22:03,679
increases its throughput as the workload

643
00:22:02,320 --> 00:22:05,120
gets more sequential

644
00:22:03,679 --> 00:22:07,440
and achieves roughly twice the

645
00:22:05,120 --> 00:22:10,639
throughput and with a fully sequential

646
00:22:07,440 --> 00:22:12,799
workload as with a fully random workload

647
00:22:10,640 --> 00:22:15,760
as you can see roxdb also improves but

648
00:22:12,799 --> 00:22:15,760
at a much lower rate

649
00:22:16,640 --> 00:22:19,840
we can also look at how insertion

650
00:22:18,240 --> 00:22:21,200
performance scales with the number of

651
00:22:19,840 --> 00:22:24,080
threads

652
00:22:21,200 --> 00:22:26,159
so here in this plot the x-axis is the

653
00:22:24,080 --> 00:22:26,879
number of threads that are performing

654
00:22:26,159 --> 00:22:28,559
insertions

655
00:22:26,880 --> 00:22:31,520
and the y-axis is the throughput in

656
00:22:28,559 --> 00:22:31,520
insertions per second

657
00:22:31,679 --> 00:22:35,280
as you can see uh for for low numbers of

658
00:22:34,720 --> 00:22:39,440
threads

659
00:22:35,280 --> 00:22:40,960
splinterdb scales somewhat linearly

660
00:22:39,440 --> 00:22:42,960
once we start to reach the device

661
00:22:40,960 --> 00:22:44,559
bandwidth of course this starts to level

662
00:22:42,960 --> 00:22:46,799
off

663
00:22:44,559 --> 00:22:49,600
but it scales uh it's it scales reason

664
00:22:46,799 --> 00:22:49,600
yeah scale scales

665
00:22:50,480 --> 00:22:55,200
okay so to conclude splinterdb is a key

666
00:22:54,400 --> 00:22:57,200
value store

667
00:22:55,200 --> 00:22:59,280
which tries to handle tough cases of

668
00:22:57,200 --> 00:23:00,960
fast storage small key value pairs and

669
00:22:59,280 --> 00:23:03,440
small cash

670
00:23:00,960 --> 00:23:05,280
it does so using a new data structure

671
00:23:03,440 --> 00:23:06,400
the size tier b epsilon tree to reduce

672
00:23:05,280 --> 00:23:07,440
the amount of work on insertion

673
00:23:06,400 --> 00:23:10,080
workloads while still

674
00:23:07,440 --> 00:23:10,720
performing lookups while still

675
00:23:10,080 --> 00:23:13,678
maintaining

676
00:23:10,720 --> 00:23:15,039
good performance on lookups and it has a

677
00:23:13,679 --> 00:23:18,159
number of cool additional algorithms

678
00:23:15,039 --> 00:23:18,158
such as flush and compact

679
00:23:18,400 --> 00:23:23,280
which further increases performance

680
00:23:21,919 --> 00:23:25,679
thank you very much my name again is

681
00:23:23,280 --> 00:23:27,600
alex conway i'm at vmware research

682
00:23:25,679 --> 00:23:29,600
you can find me at my website or with

683
00:23:27,600 --> 00:23:37,840
this email i really appreciate your time

684
00:23:29,600 --> 00:23:37,840
thank you very much

685
00:23:39,919 --> 00:23:42,000
you

