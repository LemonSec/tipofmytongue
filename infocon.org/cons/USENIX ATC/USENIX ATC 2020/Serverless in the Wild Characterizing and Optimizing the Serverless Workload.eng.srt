1
00:00:08,000 --> 00:00:10,800
hi

2
00:00:09,120 --> 00:00:12,479
my name is rodrigo fonseca i'm a

3
00:00:10,800 --> 00:00:14,960
researcher at microsoft research

4
00:00:12,480 --> 00:00:16,080
and a professor at brown university and

5
00:00:14,960 --> 00:00:18,480
i'm very happy to be

6
00:00:16,079 --> 00:00:19,198
virtually here with you today presenting

7
00:00:18,480 --> 00:00:21,199
our work

8
00:00:19,199 --> 00:00:22,480
on characterizing and optimizing

9
00:00:21,199 --> 00:00:24,560
serverless workloads

10
00:00:22,480 --> 00:00:26,640
at microsoft i'll be sharing our

11
00:00:24,560 --> 00:00:28,640
presentation today with muhammad sharad

12
00:00:26,640 --> 00:00:31,840
one of our co-authors

13
00:00:28,640 --> 00:00:34,000
and this is joint work between microsoft

14
00:00:31,840 --> 00:00:37,280
research and microsoft azure

15
00:00:34,000 --> 00:00:39,440
and several people listed here i'd like

16
00:00:37,280 --> 00:00:42,879
to start with a brief introduction

17
00:00:39,440 --> 00:00:44,000
of what serverless is so one way to view

18
00:00:42,879 --> 00:00:46,399
this is that serverless

19
00:00:44,000 --> 00:00:47,840
is a very attractive abstraction in

20
00:00:46,399 --> 00:00:49,280
which you pay for what you use

21
00:00:47,840 --> 00:00:51,120
you have the illusion of infinite

22
00:00:49,280 --> 00:00:53,520
elasticity between zero

23
00:00:51,120 --> 00:00:54,718
and however many instances you need and

24
00:00:53,520 --> 00:00:56,480
you don't have to worry about

25
00:00:54,719 --> 00:00:59,840
provisioning reserving configuring

26
00:00:56,480 --> 00:01:02,640
and patching and managing servers

27
00:00:59,840 --> 00:01:03,039
the most popular incarnation or shape

28
00:01:02,640 --> 00:01:05,199
that

29
00:01:03,039 --> 00:01:06,720
serverless comes in is called function

30
00:01:05,199 --> 00:01:08,880
as a service

31
00:01:06,720 --> 00:01:11,039
in which you have bounded time functions

32
00:01:08,880 --> 00:01:12,158
that have no persistent state between

33
00:01:11,040 --> 00:01:15,200
the locations

34
00:01:12,159 --> 00:01:18,240
you upload code you get an endpoint

35
00:01:15,200 --> 00:01:20,400
to execute that code and then you go

36
00:01:18,240 --> 00:01:22,000
so for the rest of this talk whenever i

37
00:01:20,400 --> 00:01:22,400
say serverless i'm going to be talking

38
00:01:22,000 --> 00:01:25,439
about

39
00:01:22,400 --> 00:01:27,040
serverless function as a service so what

40
00:01:25,439 --> 00:01:27,679
are people doing with serverless there

41
00:01:27,040 --> 00:01:29,280
are many

42
00:01:27,680 --> 00:01:31,680
simple things that people are doing that

43
00:01:29,280 --> 00:01:35,920
are nonetheless very valuable

44
00:01:31,680 --> 00:01:37,280
like etl workloads serving apis for both

45
00:01:35,920 --> 00:01:39,920
mobile and web

46
00:01:37,280 --> 00:01:41,840
iot data collection and processing and a

47
00:01:39,920 --> 00:01:44,799
lot of stateless processing of

48
00:01:41,840 --> 00:01:46,399
images and videos and translations and

49
00:01:44,799 --> 00:01:48,079
check processing and even some machine

50
00:01:46,399 --> 00:01:49,840
learning inferences as well

51
00:01:48,079 --> 00:01:51,679
there are some interesting explorations

52
00:01:49,840 --> 00:01:54,000
going on trying to see what the boundary

53
00:01:51,680 --> 00:01:56,079
is of what you can do with serverless

54
00:01:54,000 --> 00:01:58,079
so people have tried mapreduce and have

55
00:01:56,079 --> 00:02:01,039
tried linear algebra

56
00:01:58,079 --> 00:02:02,719
there's fast video encoding burst

57
00:02:01,040 --> 00:02:04,960
parallel executions like

58
00:02:02,719 --> 00:02:06,240
compiling functions with high

59
00:02:04,960 --> 00:02:08,639
parallelism

60
00:02:06,240 --> 00:02:10,318
and even the machine learning training

61
00:02:08,639 --> 00:02:11,119
there are some limitations of serverless

62
00:02:10,318 --> 00:02:13,040
that

63
00:02:11,120 --> 00:02:14,959
people are trying to overcome like the

64
00:02:13,040 --> 00:02:17,840
communication between functions

65
00:02:14,959 --> 00:02:19,280
latency lack of locality or a way to

66
00:02:17,840 --> 00:02:21,520
express locality

67
00:02:19,280 --> 00:02:22,400
and no clear abstractions for state

68
00:02:21,520 --> 00:02:24,640
management

69
00:02:22,400 --> 00:02:25,680
so this is all part of the future work

70
00:02:24,640 --> 00:02:27,920
agenda

71
00:02:25,680 --> 00:02:29,360
in our group and other groups but in

72
00:02:27,920 --> 00:02:30,480
this talk we're going to focus

73
00:02:29,360 --> 00:02:33,599
specifically

74
00:02:30,480 --> 00:02:34,560
on the aspect of latency and resource

75
00:02:33,599 --> 00:02:36,079
usage

76
00:02:34,560 --> 00:02:38,080
so let's go back to the abstraction of

77
00:02:36,080 --> 00:02:39,920
what serverless is it's a very

78
00:02:38,080 --> 00:02:40,480
attractive abstraction you pay for your

79
00:02:39,920 --> 00:02:43,040
use

80
00:02:40,480 --> 00:02:44,720
you have elasticity your life is great

81
00:02:43,040 --> 00:02:46,720
right like if you're a developer you can

82
00:02:44,720 --> 00:02:49,519
sit back and relax

83
00:02:46,720 --> 00:02:51,519
if you go however backstage where the

84
00:02:49,519 --> 00:02:53,840
cloud provider is sitting

85
00:02:51,519 --> 00:02:54,959
and looking at those nice seats then you

86
00:02:53,840 --> 00:02:57,120
have a big challenge

87
00:02:54,959 --> 00:02:58,480
provided to provide this abstraction

88
00:02:57,120 --> 00:02:59,840
because you do have to worry about

89
00:02:58,480 --> 00:03:01,518
servers they do exist

90
00:02:59,840 --> 00:03:03,599
and you do have to provision them and

91
00:03:01,519 --> 00:03:06,800
scale and allocate and secure

92
00:03:03,599 --> 00:03:08,560
and isolate their workloads you have to

93
00:03:06,800 --> 00:03:09,840
provide this illusion of infinite

94
00:03:08,560 --> 00:03:11,200
scalability

95
00:03:09,840 --> 00:03:13,120
but if you want to make any money you

96
00:03:11,200 --> 00:03:14,000
have to optimize for resource resource

97
00:03:13,120 --> 00:03:15,920
usage

98
00:03:14,000 --> 00:03:17,519
and there's fees competition there as

99
00:03:15,920 --> 00:03:20,559
well now

100
00:03:17,519 --> 00:03:21,680
there's also a big opportunity right

101
00:03:20,560 --> 00:03:23,920
like because you have

102
00:03:21,680 --> 00:03:25,200
all these small units and you can

103
00:03:23,920 --> 00:03:27,359
finally pack

104
00:03:25,200 --> 00:03:29,200
them if you're smart and there's a lot

105
00:03:27,360 --> 00:03:31,120
of space for innovation and offering new

106
00:03:29,200 --> 00:03:35,359
services capturing new applications

107
00:03:31,120 --> 00:03:37,519
and new markets one important challenge

108
00:03:35,360 --> 00:03:38,879
that i want to focus on is on the issue

109
00:03:37,519 --> 00:03:41,680
of code starts

110
00:03:38,879 --> 00:03:42,720
all serverless platforms have in common

111
00:03:41,680 --> 00:03:45,599
that

112
00:03:42,720 --> 00:03:46,159
before you are able to run a function

113
00:03:45,599 --> 00:03:48,879
you have

114
00:03:46,159 --> 00:03:50,079
to provision a container or something

115
00:03:48,879 --> 00:03:51,120
similar to a container where the

116
00:03:50,080 --> 00:03:52,879
function will run

117
00:03:51,120 --> 00:03:54,560
you have to load everything necessary to

118
00:03:52,879 --> 00:03:56,239
be able to get the runtime for that

119
00:03:54,560 --> 00:03:57,599
function ready to go

120
00:03:56,239 --> 00:03:59,040
then you finally have to load the code

121
00:03:57,599 --> 00:04:00,480
of the function any data that the

122
00:03:59,040 --> 00:04:02,560
function needs

123
00:04:00,480 --> 00:04:03,840
and only then are you ready to run to

124
00:04:02,560 --> 00:04:07,519
amortize this cost

125
00:04:03,840 --> 00:04:09,120
of gold starts what platforms usually do

126
00:04:07,519 --> 00:04:11,519
is that they'll keep the function

127
00:04:09,120 --> 00:04:13,360
alive loaded in memory ready to go for

128
00:04:11,519 --> 00:04:15,680
net for subsequent invocations

129
00:04:13,360 --> 00:04:18,720
this is important because the typical

130
00:04:15,680 --> 00:04:21,600
cost of a cold start ranges from

131
00:04:18,720 --> 00:04:24,000
somewhere between 0.2 seconds to a few

132
00:04:21,600 --> 00:04:24,000
seconds

133
00:04:25,040 --> 00:04:28,320
of course there's a trade-off to be

134
00:04:26,880 --> 00:04:31,199
played on here right

135
00:04:28,320 --> 00:04:32,960
if you decide to have no code starts or

136
00:04:31,199 --> 00:04:33,680
just the first code start for every

137
00:04:32,960 --> 00:04:34,880
function

138
00:04:33,680 --> 00:04:37,120
you have to keep them in memory

139
00:04:34,880 --> 00:04:40,400
indefinitely and this will

140
00:04:37,120 --> 00:04:42,400
undoubtedly waste a lot of resources

141
00:04:40,400 --> 00:04:43,520
you can also opt to not waste any

142
00:04:42,400 --> 00:04:46,960
resources and only

143
00:04:43,520 --> 00:04:49,039
use what exactly you're giving customers

144
00:04:46,960 --> 00:04:50,799
except that then every vocation is going

145
00:04:49,040 --> 00:04:53,600
to have to pay a cold start

146
00:04:50,800 --> 00:04:54,639
and people are going to be very unhappy

147
00:04:53,600 --> 00:04:56,800
so in this work

148
00:04:54,639 --> 00:04:58,479
we want to explore and understand this

149
00:04:56,800 --> 00:04:59,520
trade-off space between these two

150
00:04:58,479 --> 00:05:01,758
extremes

151
00:04:59,520 --> 00:05:03,120
such that we can eventually try to find

152
00:05:01,759 --> 00:05:06,479
a strategy that leads us

153
00:05:03,120 --> 00:05:10,080
closer to the origin of this graph

154
00:05:06,479 --> 00:05:12,479
so to do that let's first step back

155
00:05:10,080 --> 00:05:14,639
and examine the workload and try to

156
00:05:12,479 --> 00:05:17,280
understand how functions are accessed

157
00:05:14,639 --> 00:05:18,960
what resources they use and how long

158
00:05:17,280 --> 00:05:21,758
they take to run

159
00:05:18,960 --> 00:05:22,880
so we took two weeks of invocations to

160
00:05:21,759 --> 00:05:27,120
azure functions

161
00:05:22,880 --> 00:05:29,039
in july 2019 and

162
00:05:27,120 --> 00:05:30,720
did an extensive characterization so

163
00:05:29,039 --> 00:05:31,520
i'll refer you to the paper for more

164
00:05:30,720 --> 00:05:33,280
details

165
00:05:31,520 --> 00:05:34,560
you're just going to give a glimpse of

166
00:05:33,280 --> 00:05:36,559
some of the

167
00:05:34,560 --> 00:05:37,919
findings that we have but an important

168
00:05:36,560 --> 00:05:38,479
thing that i want to mention is that

169
00:05:37,919 --> 00:05:40,400
this

170
00:05:38,479 --> 00:05:42,000
workload or a subset of the workload

171
00:05:40,400 --> 00:05:44,000
that we used for the paper

172
00:05:42,000 --> 00:05:46,479
is actually available publicly at the

173
00:05:44,000 --> 00:05:48,720
url below so you can actually download

174
00:05:46,479 --> 00:05:50,840
and do your own analysis so we first

175
00:05:48,720 --> 00:05:52,320
look at how frequently applications are

176
00:05:50,840 --> 00:05:54,000
invoked

177
00:05:52,320 --> 00:05:55,440
one important thing to note here is that

178
00:05:54,000 --> 00:05:58,000
in azure

179
00:05:55,440 --> 00:05:59,680
functions are grouped into applications

180
00:05:58,000 --> 00:06:01,840
and applications as this

181
00:05:59,680 --> 00:06:03,360
set of functions is the unit of resource

182
00:06:01,840 --> 00:06:04,880
allocation we went back

183
00:06:03,360 --> 00:06:06,400
and looked at the number of daily

184
00:06:04,880 --> 00:06:08,080
invocations on average

185
00:06:06,400 --> 00:06:10,318
per each application and this graph

186
00:06:08,080 --> 00:06:12,159
shows the cdf

187
00:06:10,319 --> 00:06:13,520
so note that there are eight or more

188
00:06:12,160 --> 00:06:14,960
orders of magnitude

189
00:06:13,520 --> 00:06:17,198
difference in the number of daily

190
00:06:14,960 --> 00:06:19,120
implications per app

191
00:06:17,199 --> 00:06:20,240
so one thing that we can do is that we

192
00:06:19,120 --> 00:06:23,520
can look

193
00:06:20,240 --> 00:06:24,400
at the cdf and replace the x-axis with

194
00:06:23,520 --> 00:06:26,719
the average

195
00:06:24,400 --> 00:06:29,198
interval between invocations which you

196
00:06:26,720 --> 00:06:32,000
can see on the top axis of this graph

197
00:06:29,199 --> 00:06:34,080
so we can see that 45 of the apps in the

198
00:06:32,000 --> 00:06:37,440
green region

199
00:06:34,080 --> 00:06:37,758
are invoked on average less than once

200
00:06:37,440 --> 00:06:42,080
per

201
00:06:37,759 --> 00:06:46,000
hour over the course of many days

202
00:06:42,080 --> 00:06:46,639
furthermore 82 of the apps in the yellow

203
00:06:46,000 --> 00:06:48,639
region

204
00:06:46,639 --> 00:06:51,360
including the green region are invoked

205
00:06:48,639 --> 00:06:54,319
on average less than once per minute

206
00:06:51,360 --> 00:06:56,000
the remainder 18 of the applications in

207
00:06:54,319 --> 00:06:56,400
the orange region which is the entire

208
00:06:56,000 --> 00:06:58,960
graph

209
00:06:56,400 --> 00:07:00,638
are accessed more than once per minute

210
00:06:58,960 --> 00:07:03,840
on average

211
00:07:00,639 --> 00:07:04,160
if you flip this graph and actually add

212
00:07:03,840 --> 00:07:06,799
up

213
00:07:04,160 --> 00:07:08,400
the invocations starting from the most

214
00:07:06,800 --> 00:07:10,800
popular applications

215
00:07:08,400 --> 00:07:11,679
down to the least popular applications

216
00:07:10,800 --> 00:07:15,360
we can see

217
00:07:11,680 --> 00:07:17,680
an interesting picture that same group

218
00:07:15,360 --> 00:07:18,800
that occupied most of the area in the

219
00:07:17,680 --> 00:07:21,199
previous graph

220
00:07:18,800 --> 00:07:22,000
eighty-two percent of the applications

221
00:07:21,199 --> 00:07:24,240
that have

222
00:07:22,000 --> 00:07:25,120
invocations of at most ones permitted on

223
00:07:24,240 --> 00:07:27,199
average

224
00:07:25,120 --> 00:07:29,759
corresponds to only zero point four

225
00:07:27,199 --> 00:07:32,080
percent of the invocations

226
00:07:29,759 --> 00:07:34,240
and the 18 of the applications that are

227
00:07:32,080 --> 00:07:36,240
invoked more frequently on average than

228
00:07:34,240 --> 00:07:39,759
once per minute

229
00:07:36,240 --> 00:07:42,400
correspond to 99.6 of the invocations so

230
00:07:39,759 --> 00:07:44,560
the workload is very skewed

231
00:07:42,400 --> 00:07:46,318
another interesting point is that the

232
00:07:44,560 --> 00:07:47,440
average is not enough to characterize

233
00:07:46,319 --> 00:07:49,280
the workload

234
00:07:47,440 --> 00:07:50,479
we found that invocation patterns are

235
00:07:49,280 --> 00:07:52,318
highly variable

236
00:07:50,479 --> 00:07:54,400
among different applications there are

237
00:07:52,319 --> 00:07:59,199
many periodic functions

238
00:07:54,400 --> 00:08:00,960
but also some very bursty functions

239
00:07:59,199 --> 00:08:03,520
and very few of the applications are

240
00:08:00,960 --> 00:08:06,560
actually well-behaved

241
00:08:03,520 --> 00:08:09,280
a poisson process arrivals that are

242
00:08:06,560 --> 00:08:11,199
usually assumed in many simulations the

243
00:08:09,280 --> 00:08:14,719
high-level point here is that there's no

244
00:08:11,199 --> 00:08:16,479
single one-size-fits-all mana solution

245
00:08:14,720 --> 00:08:18,879
to manage the residence

246
00:08:16,479 --> 00:08:20,000
times of applications that would work

247
00:08:18,879 --> 00:08:22,000
well

248
00:08:20,000 --> 00:08:24,080
so we also looked at how this translates

249
00:08:22,000 --> 00:08:26,560
to memory usage

250
00:08:24,080 --> 00:08:27,599
this graph shows the cumulative fraction

251
00:08:26,560 --> 00:08:30,639
of total memory

252
00:08:27,599 --> 00:08:31,840
allocated in total physical memory by

253
00:08:30,639 --> 00:08:34,000
all apps

254
00:08:31,840 --> 00:08:35,439
sorting the apps by the number of

255
00:08:34,000 --> 00:08:38,000
invocations

256
00:08:35,440 --> 00:08:38,640
so at least from least popular apps on

257
00:08:38,000 --> 00:08:41,440
the left

258
00:08:38,640 --> 00:08:43,199
to more popular apps on the right one

259
00:08:41,440 --> 00:08:44,640
thing to note is that more popular apps

260
00:08:43,200 --> 00:08:46,640
tend to use more memory

261
00:08:44,640 --> 00:08:48,160
because there are more workers serving

262
00:08:46,640 --> 00:08:49,040
them and this graph takes that into

263
00:08:48,160 --> 00:08:50,480
account as well

264
00:08:49,040 --> 00:08:53,599
if we wanted to keep all the

265
00:08:50,480 --> 00:08:56,800
applications warm as the other extreme

266
00:08:53,600 --> 00:08:58,320
plot what wouldn't happen so this blue

267
00:08:56,800 --> 00:09:01,439
region here

268
00:08:58,320 --> 00:09:02,240
says that those 82 percent least popular

269
00:09:01,440 --> 00:09:05,040
apps

270
00:09:02,240 --> 00:09:06,880
the ones that correspond to only 0.4

271
00:09:05,040 --> 00:09:09,439
percent of their vocations

272
00:09:06,880 --> 00:09:12,000
would take 40 percent of all of our

273
00:09:09,440 --> 00:09:15,760
physical memory

274
00:09:12,000 --> 00:09:18,800
and 60 of the virtual memory of our

275
00:09:15,760 --> 00:09:21,360
revisioning just to reduce the cold

276
00:09:18,800 --> 00:09:23,760
starts in all of them

277
00:09:21,360 --> 00:09:24,959
and another point ninety percent of the

278
00:09:23,760 --> 00:09:26,800
applications

279
00:09:24,959 --> 00:09:28,000
corresponding to about one percent of

280
00:09:26,800 --> 00:09:30,479
the invocations

281
00:09:28,000 --> 00:09:32,160
would consume more than fifty percent of

282
00:09:30,480 --> 00:09:34,560
our physical memory

283
00:09:32,160 --> 00:09:36,160
so clearly not worth keeping everything

284
00:09:34,560 --> 00:09:37,599
into place

285
00:09:36,160 --> 00:09:40,560
last thing i want to talk about is the

286
00:09:37,600 --> 00:09:43,200
duration of function executions

287
00:09:40,560 --> 00:09:43,839
this graph here shows a cdf of the

288
00:09:43,200 --> 00:09:46,080
minimum

289
00:09:43,839 --> 00:09:48,160
average and maximum invocation times

290
00:09:46,080 --> 00:09:49,680
across all applications

291
00:09:48,160 --> 00:09:52,160
note that this these times do not

292
00:09:49,680 --> 00:09:54,079
include the cold start times

293
00:09:52,160 --> 00:09:57,680
and i encourage you to look at the paper

294
00:09:54,080 --> 00:10:01,040
for details of how this graph was built

295
00:09:57,680 --> 00:10:04,399
so about 50 of the apps on average

296
00:10:01,040 --> 00:10:07,040
run for less than a second and about 75

297
00:10:04,399 --> 00:10:10,160
of the apps have their maximum execution

298
00:10:07,040 --> 00:10:12,399
time smaller than 10 seconds

299
00:10:10,160 --> 00:10:13,279
one important thing to note here is that

300
00:10:12,399 --> 00:10:16,720
if we look at

301
00:10:13,279 --> 00:10:19,439
the range of times that code starts take

302
00:10:16,720 --> 00:10:20,560
it's overlapping with a lot of the

303
00:10:19,440 --> 00:10:23,200
execution time

304
00:10:20,560 --> 00:10:25,279
functions and so because these things

305
00:10:23,200 --> 00:10:28,640
are in the same time scales

306
00:10:25,279 --> 00:10:30,839
it's very important to not ignore

307
00:10:28,640 --> 00:10:32,480
code start types because they're very

308
00:10:30,839 --> 00:10:34,720
significant

309
00:10:32,480 --> 00:10:35,839
so to summarize our main takeaways from

310
00:10:34,720 --> 00:10:38,399
our observations

311
00:10:35,839 --> 00:10:40,000
of the workload is that the accesses are

312
00:10:38,399 --> 00:10:43,200
highly concentrated

313
00:10:40,000 --> 00:10:45,760
like 82 percent of the apps are accessed

314
00:10:43,200 --> 00:10:47,600
less than once per minute on average and

315
00:10:45,760 --> 00:10:48,880
correspond to 0.4 percent of the

316
00:10:47,600 --> 00:10:51,440
accesses

317
00:10:48,880 --> 00:10:53,200
but also they take about 40 percent of

318
00:10:51,440 --> 00:10:54,320
the memory if we wanted to keep them all

319
00:10:53,200 --> 00:10:56,079
resident

320
00:10:54,320 --> 00:10:58,480
the arrival processes are highly

321
00:10:56,079 --> 00:11:00,479
variable and the execution times are

322
00:10:58,480 --> 00:11:03,839
short and very comparable

323
00:11:00,480 --> 00:11:03,839
to the cold start times

324
00:11:04,079 --> 00:11:08,399
so if we go back to our trade-off space

325
00:11:06,480 --> 00:11:10,640
between the number of cold starts in the

326
00:11:08,399 --> 00:11:12,720
wasted memory

327
00:11:10,640 --> 00:11:14,480
we saw that we can't keep functions in

328
00:11:12,720 --> 00:11:15,839
memory indefinitely because those we'll

329
00:11:14,480 --> 00:11:18,880
spend a lot of memory

330
00:11:15,839 --> 00:11:20,480
for to cover very few accesses

331
00:11:18,880 --> 00:11:22,240
and also that we can't ignore code

332
00:11:20,480 --> 00:11:25,360
starts because the times are in the same

333
00:11:22,240 --> 00:11:28,000
order of magnitude as most invocations

334
00:11:25,360 --> 00:11:29,519
so i'll turn this over to muhammad who

335
00:11:28,000 --> 00:11:31,040
will talk about how we navigate this

336
00:11:29,519 --> 00:11:33,760
trade-off space

337
00:11:31,040 --> 00:11:34,560
and ultimately how we manage to find a

338
00:11:33,760 --> 00:11:36,959
strategy that

339
00:11:34,560 --> 00:11:39,119
both reduces the number of cold starts

340
00:11:36,959 --> 00:11:41,920
and the memory usage

341
00:11:39,120 --> 00:11:42,800
of the service as a whole thank you

342
00:11:41,920 --> 00:11:44,719
rodrigo

343
00:11:42,800 --> 00:11:47,599
i'm mohana and i will be presenting the

344
00:11:44,720 --> 00:11:50,480
rest of this talk

345
00:11:47,600 --> 00:11:51,760
it is worth asking what is the current

346
00:11:50,480 --> 00:11:55,040
state of the art

347
00:11:51,760 --> 00:11:56,480
what do serverless providers do this has

348
00:11:55,040 --> 00:11:57,839
been something that has been reverse

349
00:11:56,480 --> 00:12:00,639
engineered by

350
00:11:57,839 --> 00:12:02,000
many out there for instance amazon

351
00:12:00,639 --> 00:12:04,639
lambda

352
00:12:02,000 --> 00:12:06,000
they're a fixed 10-minute keep life

353
00:12:04,639 --> 00:12:08,720
policy is deployed

354
00:12:06,000 --> 00:12:10,160
which means if your invocations are more

355
00:12:08,720 --> 00:12:12,399
than 10 minutes apart

356
00:12:10,160 --> 00:12:14,319
you're going to get a cold start and if

357
00:12:12,399 --> 00:12:14,880
you are lucky and your invocations are

358
00:12:14,320 --> 00:12:17,839
less than

359
00:12:14,880 --> 00:12:19,680
10 minutes apart you're going to enjoy

360
00:12:17,839 --> 00:12:23,279
no cold starts

361
00:12:19,680 --> 00:12:26,479
similarly azure functions deploys at

362
00:12:23,279 --> 00:12:28,959
fixed 20 minute keep a life policy

363
00:12:26,480 --> 00:12:30,800
now be very curious to understand uh

364
00:12:28,959 --> 00:12:32,880
what is it that one provider decides a

365
00:12:30,800 --> 00:12:34,000
10 minute policy another uses a 20

366
00:12:32,880 --> 00:12:36,560
minute policy

367
00:12:34,000 --> 00:12:38,320
so we took the entire workload of azure

368
00:12:36,560 --> 00:12:40,399
functions and simulated

369
00:12:38,320 --> 00:12:41,920
it for a week what would have happened

370
00:12:40,399 --> 00:12:43,839
if we have deployed a different fixed

371
00:12:41,920 --> 00:12:46,959
clip live policy

372
00:12:43,839 --> 00:12:51,680
and the result is shown in this graph

373
00:12:46,959 --> 00:12:54,638
here the cdf of application call starts

374
00:12:51,680 --> 00:12:56,239
is shown ideally if you want to be to

375
00:12:54,639 --> 00:12:59,120
the top left of this graph

376
00:12:56,240 --> 00:13:00,160
where 100 of applications get zero goals

377
00:12:59,120 --> 00:13:02,399
start

378
00:13:00,160 --> 00:13:04,319
um even if if you use the best policy

379
00:13:02,399 --> 00:13:06,160
which is a no unloading policy

380
00:13:04,320 --> 00:13:08,000
you're gonna get still one call to start

381
00:13:06,160 --> 00:13:09,279
for the very first invocation of that

382
00:13:08,000 --> 00:13:12,399
application

383
00:13:09,279 --> 00:13:13,760
and that is denoted with that attached a

384
00:13:12,399 --> 00:13:15,200
blue line

385
00:13:13,760 --> 00:13:16,880
and you can see that different fixed

386
00:13:15,200 --> 00:13:19,839
keep life policies

387
00:13:16,880 --> 00:13:21,519
uh can move us towards that direction

388
00:13:19,839 --> 00:13:23,200
but the gains become more and more

389
00:13:21,519 --> 00:13:25,040
marginal

390
00:13:23,200 --> 00:13:26,639
to be fair we should look at it together

391
00:13:25,040 --> 00:13:29,040
with the amount of memory

392
00:13:26,639 --> 00:13:29,920
that is wasted for these different

393
00:13:29,040 --> 00:13:32,480
policies

394
00:13:29,920 --> 00:13:34,000
and to do that we basically take the

395
00:13:32,480 --> 00:13:36,959
third quartile

396
00:13:34,000 --> 00:13:39,199
of application corresponds and that way

397
00:13:36,959 --> 00:13:42,399
we can map it towards

398
00:13:39,199 --> 00:13:44,319
the right figure on the x-axis

399
00:13:42,399 --> 00:13:45,680
and to the y-axis of the right figure

400
00:13:44,320 --> 00:13:48,880
you can see the

401
00:13:45,680 --> 00:13:51,839
normalized basic memory time to uh that

402
00:13:48,880 --> 00:13:54,399
of a 10 minute fixed cable life policy

403
00:13:51,839 --> 00:13:55,120
and as seen if we go for longer keeper

404
00:13:54,399 --> 00:13:57,760
lives

405
00:13:55,120 --> 00:13:58,800
we can improve the application call to

406
00:13:57,760 --> 00:14:01,519
start

407
00:13:58,800 --> 00:14:02,319
specifically third quartile core stall

408
00:14:01,519 --> 00:14:05,360
but

409
00:14:02,320 --> 00:14:07,519
that comes with the cost of increasing

410
00:14:05,360 --> 00:14:08,399
the memory that is being wasted now just

411
00:14:07,519 --> 00:14:10,000
sitting there

412
00:14:08,399 --> 00:14:12,560
without being used for function

413
00:14:10,000 --> 00:14:12,560
execution

414
00:14:13,760 --> 00:14:17,600
the fixed keeper live policy it doesn't

415
00:14:16,079 --> 00:14:20,719
work for even the most

416
00:14:17,600 --> 00:14:23,440
simplistic source of invocation patterns

417
00:14:20,720 --> 00:14:24,160
if you're unlucky and your invocation is

418
00:14:23,440 --> 00:14:26,560
periodic

419
00:14:24,160 --> 00:14:28,399
very simple and very predictable but if

420
00:14:26,560 --> 00:14:30,638
it is 11 minutes apart

421
00:14:28,399 --> 00:14:32,480
and your provider uses a 10 minute

422
00:14:30,639 --> 00:14:35,040
policy you're going to constantly get

423
00:14:32,480 --> 00:14:38,880
cold starts

424
00:14:35,040 --> 00:14:41,120
um even if you're lucky and this fixed

425
00:14:38,880 --> 00:14:42,639
keep a live window can capture your

426
00:14:41,120 --> 00:14:45,279
invocation pattern

427
00:14:42,639 --> 00:14:45,920
it is still very wasteful because it

428
00:14:45,279 --> 00:14:48,240
keeps

429
00:14:45,920 --> 00:14:49,199
a function images in memory for that

430
00:14:48,240 --> 00:14:51,600
entire

431
00:14:49,199 --> 00:14:52,399
window and in this case for instance if

432
00:14:51,600 --> 00:14:55,440
we know that

433
00:14:52,399 --> 00:14:57,199
uh invocations are eight minutes apart

434
00:14:55,440 --> 00:14:59,199
it can keep things in sleep and then

435
00:14:57,199 --> 00:15:02,000
pre-warm them right before they're

436
00:14:59,199 --> 00:15:02,000
supposed to come

437
00:15:02,079 --> 00:15:05,120
so with all of these insights that we

438
00:15:04,000 --> 00:15:06,480
have gained and we have

439
00:15:05,120 --> 00:15:08,639
limitations that we have seen in the

440
00:15:06,480 --> 00:15:11,440
fixed policy we have

441
00:15:08,639 --> 00:15:11,760
introduced the hybrid histogram policy

442
00:15:11,440 --> 00:15:14,800
that

443
00:15:11,760 --> 00:15:16,480
tries to adapt to each application as

444
00:15:14,800 --> 00:15:18,479
much as possible

445
00:15:16,480 --> 00:15:20,399
we do pre-warming in addition to keep

446
00:15:18,480 --> 00:15:23,120
alive and that allows us to

447
00:15:20,399 --> 00:15:24,000
eliminate cold starts while effectively

448
00:15:23,120 --> 00:15:27,519
reducing the

449
00:15:24,000 --> 00:15:30,160
memory being wasted and we ensure that

450
00:15:27,519 --> 00:15:32,959
the implementation is very lightweight

451
00:15:30,160 --> 00:15:36,399
and allows us to track these different

452
00:15:32,959 --> 00:15:36,399
behaviors per application

453
00:15:36,959 --> 00:15:41,758
so in order to explain that let's go

454
00:15:39,040 --> 00:15:43,040
through an example together

455
00:15:41,759 --> 00:15:46,079
let's imagine that we get the first

456
00:15:43,040 --> 00:15:48,800
steam location it is a calling location

457
00:15:46,079 --> 00:15:50,079
and eight minutes later eight minutes

458
00:15:48,800 --> 00:15:53,040
after the first one

459
00:15:50,079 --> 00:15:54,800
finishes execution we received the

460
00:15:53,040 --> 00:15:58,399
second invocation

461
00:15:54,800 --> 00:15:59,439
and we call that idle time idl time is

462
00:15:58,399 --> 00:16:02,000
the difference between

463
00:15:59,440 --> 00:16:03,519
execution and the next invocation

464
00:16:02,000 --> 00:16:05,680
arrival

465
00:16:03,519 --> 00:16:07,680
and um we see this this eight minute

466
00:16:05,680 --> 00:16:08,079
idle time and we just take a note of

467
00:16:07,680 --> 00:16:11,680
that

468
00:16:08,079 --> 00:16:13,519
in a histogram and if we see another

469
00:16:11,680 --> 00:16:16,880
observation of eight minutes

470
00:16:13,519 --> 00:16:19,199
we take another note that histogram

471
00:16:16,880 --> 00:16:20,880
and maybe after a while we just start

472
00:16:19,199 --> 00:16:23,758
seeing a nice pattern

473
00:16:20,880 --> 00:16:24,560
that for this application idle time is

474
00:16:23,759 --> 00:16:28,480
mostly around

475
00:16:24,560 --> 00:16:31,040
eight minutes so you can define this

476
00:16:28,480 --> 00:16:32,639
window that tells us that you need to

477
00:16:31,040 --> 00:16:34,319
keep things in memory

478
00:16:32,639 --> 00:16:37,120
alive for slightly more than three

479
00:16:34,320 --> 00:16:39,680
minutes and after an execution ends

480
00:16:37,120 --> 00:16:40,639
you can keep things in sleep and

481
00:16:39,680 --> 00:16:43,758
pre-warm it

482
00:16:40,639 --> 00:16:46,160
perhaps six and a half minutes in and

483
00:16:43,759 --> 00:16:48,399
that way we can adapt to this periodic

484
00:16:46,160 --> 00:16:50,480
pattern

485
00:16:48,399 --> 00:16:52,079
in practice we saw that it is very

486
00:16:50,480 --> 00:16:55,279
useful to define

487
00:16:52,079 --> 00:16:57,599
a certain um uh cutoff boundaries

488
00:16:55,279 --> 00:16:59,759
for instance in this case a head cutoff

489
00:16:57,600 --> 00:17:02,880
of fifth percentile and tail cutoff of

490
00:16:59,759 --> 00:17:03,680
99th percentile which allows eliminating

491
00:17:02,880 --> 00:17:06,959
uh

492
00:17:03,680 --> 00:17:08,159
outliers um and ensures that these

493
00:17:06,959 --> 00:17:11,120
histograms run

494
00:17:08,160 --> 00:17:12,000
efficiently but what matters mostly is

495
00:17:11,119 --> 00:17:14,639
that here

496
00:17:12,000 --> 00:17:15,599
the histogram the resolution here is

497
00:17:14,640 --> 00:17:17,839
minutes

498
00:17:15,599 --> 00:17:19,198
and that means that if you want to go

499
00:17:17,839 --> 00:17:21,839
for capturing

500
00:17:19,199 --> 00:17:24,799
a moving history of four hours you need

501
00:17:21,839 --> 00:17:26,079
to only keep track of you know 240 bins

502
00:17:24,799 --> 00:17:28,559
which is very lightweight in

503
00:17:26,079 --> 00:17:30,480
implementation

504
00:17:28,559 --> 00:17:32,720
um one can imagine that if your

505
00:17:30,480 --> 00:17:34,720
application is infrequent

506
00:17:32,720 --> 00:17:36,799
and what happens is that you're going to

507
00:17:34,720 --> 00:17:38,000
have ideal times that falls out of this

508
00:17:36,799 --> 00:17:41,360
histogram

509
00:17:38,000 --> 00:17:43,679
and we call it um out of bounds and now

510
00:17:41,360 --> 00:17:46,639
if there's only a few out of bounds

511
00:17:43,679 --> 00:17:48,960
perhaps it can still be fine with that

512
00:17:46,640 --> 00:17:50,320
but if we start seeing that the number

513
00:17:48,960 --> 00:17:53,200
of outer bounds

514
00:17:50,320 --> 00:17:54,080
is starting to grow we take another

515
00:17:53,200 --> 00:17:56,799
approach

516
00:17:54,080 --> 00:17:59,120
in that case because invocations are

517
00:17:56,799 --> 00:18:02,240
infrequent we have enough time to run

518
00:17:59,120 --> 00:18:05,360
complex predictors and in this case

519
00:18:02,240 --> 00:18:07,600
you know we use the methods that are

520
00:18:05,360 --> 00:18:10,879
known in time series forecast

521
00:18:07,600 --> 00:18:13,918
so here i'm going to tell you about the

522
00:18:10,880 --> 00:18:16,000
decision tree that you go through when a

523
00:18:13,919 --> 00:18:17,200
new invocation arrives we update the

524
00:18:16,000 --> 00:18:19,760
application's

525
00:18:17,200 --> 00:18:21,440
idle time distribution if we see that

526
00:18:19,760 --> 00:18:23,840
there are too many out of bounds

527
00:18:21,440 --> 00:18:26,320
we use times use forecast specifically

528
00:18:23,840 --> 00:18:29,678
arima was something that worked nicely

529
00:18:26,320 --> 00:18:31,360
for our workloads if there are not too

530
00:18:29,679 --> 00:18:33,600
many outer bounds we see we check

531
00:18:31,360 --> 00:18:35,600
and see if better the histogram has

532
00:18:33,600 --> 00:18:36,959
captured a significant pattern

533
00:18:35,600 --> 00:18:39,678
and to do that we'll look at the

534
00:18:36,960 --> 00:18:41,600
coefficient of variation

535
00:18:39,679 --> 00:18:42,960
if the histogram hasn't captured a

536
00:18:41,600 --> 00:18:46,399
significant pattern

537
00:18:42,960 --> 00:18:48,480
we try to be conservative and

538
00:18:46,400 --> 00:18:50,160
adjust the histogram to its maximum

539
00:18:48,480 --> 00:18:52,320
science

540
00:18:50,160 --> 00:18:53,840
but if we have captured a nice pattern

541
00:18:52,320 --> 00:18:55,039
then we use all of this notion of

542
00:18:53,840 --> 00:18:56,639
pre-warming and keep

543
00:18:55,039 --> 00:18:59,200
living and adapting to different

544
00:18:56,640 --> 00:19:01,919
applications

545
00:18:59,200 --> 00:19:03,360
and going back to where we started here

546
00:19:01,919 --> 00:19:06,400
you can see

547
00:19:03,360 --> 00:19:10,240
the pareto frontier for different

548
00:19:06,400 --> 00:19:11,840
fixed policies and using this hybrid

549
00:19:10,240 --> 00:19:14,240
histogram policy

550
00:19:11,840 --> 00:19:15,199
we get a more optimal character front

551
00:19:14,240 --> 00:19:17,520
here where

552
00:19:15,200 --> 00:19:18,320
application calls start is improved and

553
00:19:17,520 --> 00:19:21,440
there is

554
00:19:18,320 --> 00:19:23,600
less memory wastage so specifically if

555
00:19:21,440 --> 00:19:25,360
we have a hybrid policy that has a four

556
00:19:23,600 --> 00:19:27,840
hour histogram at its core

557
00:19:25,360 --> 00:19:30,000
you can see that that's giving you the

558
00:19:27,840 --> 00:19:31,360
same third quartile application called

559
00:19:30,000 --> 00:19:34,400
start performance

560
00:19:31,360 --> 00:19:36,840
to that of a two hour fixed policy

561
00:19:34,400 --> 00:19:39,200
but with fifty percent less memory

562
00:19:36,840 --> 00:19:41,840
wastage

563
00:19:39,200 --> 00:19:43,200
so we were happy about these results and

564
00:19:41,840 --> 00:19:46,399
we implemented

565
00:19:43,200 --> 00:19:46,960
our prototype in apache openwhisk apache

566
00:19:46,400 --> 00:19:49,919
openvis

567
00:19:46,960 --> 00:19:50,880
is open source industry grade serverless

568
00:19:49,919 --> 00:19:53,919
platform

569
00:19:50,880 --> 00:19:56,480
it is used in ibm cloud functions and

570
00:19:53,919 --> 00:19:57,760
here functions run in docker containers

571
00:19:56,480 --> 00:19:59,679
by default

572
00:19:57,760 --> 00:20:01,280
it uses a 10 minute fixed keep a live

573
00:19:59,679 --> 00:20:03,600
policy

574
00:20:01,280 --> 00:20:04,639
so it has the reference point that we

575
00:20:03,600 --> 00:20:07,840
wanted

576
00:20:04,640 --> 00:20:09,919
and what we did was that we implemented

577
00:20:07,840 --> 00:20:12,320
our policy inside the load balancer

578
00:20:09,919 --> 00:20:13,360
within the controller we have modified

579
00:20:12,320 --> 00:20:16,000
the apis

580
00:20:13,360 --> 00:20:18,240
to pass those keep alive and pre-warming

581
00:20:16,000 --> 00:20:20,799
parameters to invokers

582
00:20:18,240 --> 00:20:21,840
and invokers that manage containers on

583
00:20:20,799 --> 00:20:24,639
each server

584
00:20:21,840 --> 00:20:25,678
and they are now ever and handle those

585
00:20:24,640 --> 00:20:27,360
containers

586
00:20:25,679 --> 00:20:29,120
according with these parameters meaning

587
00:20:27,360 --> 00:20:30,000
that they can preform containers at the

588
00:20:29,120 --> 00:20:32,000
right time

589
00:20:30,000 --> 00:20:34,159
or they can terminate or pause

590
00:20:32,000 --> 00:20:37,520
containers at the right time

591
00:20:34,159 --> 00:20:38,559
so here i want to just show you some of

592
00:20:37,520 --> 00:20:41,600
the results

593
00:20:38,559 --> 00:20:44,000
uh to the left you can see how

594
00:20:41,600 --> 00:20:44,719
the application code starts improved for

595
00:20:44,000 --> 00:20:46,840
going from a

596
00:20:44,720 --> 00:20:48,240
fixed 10-minute policy to a hybrid

597
00:20:46,840 --> 00:20:51,439
policy

598
00:20:48,240 --> 00:20:53,760
in simulations and to the right you see

599
00:20:51,440 --> 00:20:55,919
the experimental results

600
00:20:53,760 --> 00:20:57,679
from our apache open basic

601
00:20:55,919 --> 00:20:58,880
implementation

602
00:20:57,679 --> 00:21:01,280
and you can see that they're very

603
00:20:58,880 --> 00:21:02,240
similar and because of the scale of the

604
00:21:01,280 --> 00:21:05,360
experiment

605
00:21:02,240 --> 00:21:08,960
was smaller you can see that there are

606
00:21:05,360 --> 00:21:11,199
a lot more steps to the right figure

607
00:21:08,960 --> 00:21:13,520
we saw a similar container memory

608
00:21:11,200 --> 00:21:16,480
reduction of around 15

609
00:21:13,520 --> 00:21:18,799
in our experiments that was the same as

610
00:21:16,480 --> 00:21:21,919
the simulation results were telling us

611
00:21:18,799 --> 00:21:25,120
and because of the secondary factor that

612
00:21:21,919 --> 00:21:26,320
open disk includes language runtime in

613
00:21:25,120 --> 00:21:28,719
the execution time

614
00:21:26,320 --> 00:21:30,720
if you prevent a cold start you also

615
00:21:28,720 --> 00:21:31,679
improve the average execution time and

616
00:21:30,720 --> 00:21:35,360
detail

617
00:21:31,679 --> 00:21:36,880
of execution times so the good thing was

618
00:21:35,360 --> 00:21:40,399
that all of this happened

619
00:21:36,880 --> 00:21:43,440
in less than one millisecond of added

620
00:21:40,400 --> 00:21:46,320
latency so on the critical path we add

621
00:21:43,440 --> 00:21:47,520
around 800 microseconds of latency and

622
00:21:46,320 --> 00:21:50,000
that was

623
00:21:47,520 --> 00:21:52,720
very good and given all the benefits

624
00:21:50,000 --> 00:21:52,720
that we're getting

625
00:21:53,280 --> 00:21:59,600
so to summarize this study

626
00:21:56,400 --> 00:22:02,400
has provided the first characterization

627
00:21:59,600 --> 00:22:04,240
of an entire provider serverless

628
00:22:02,400 --> 00:22:06,240
workload

629
00:22:04,240 --> 00:22:07,760
we have introduced a dynamic policy that

630
00:22:06,240 --> 00:22:09,120
allows us to

631
00:22:07,760 --> 00:22:10,960
manage service workflows more

632
00:22:09,120 --> 00:22:13,520
efficiently this means

633
00:22:10,960 --> 00:22:14,640
reducing the cold starts that users

634
00:22:13,520 --> 00:22:16,559
experience

635
00:22:14,640 --> 00:22:17,679
and also at the same time reducing the

636
00:22:16,559 --> 00:22:20,799
amount of memory

637
00:22:17,679 --> 00:22:22,559
that the provider consumes for these

638
00:22:20,799 --> 00:22:24,720
serverless workloads

639
00:22:22,559 --> 00:22:26,799
now the good thing is that the first

640
00:22:24,720 --> 00:22:28,960
elements of this new policy is now

641
00:22:26,799 --> 00:22:30,639
running in production and

642
00:22:28,960 --> 00:22:33,039
hopefully it can roll out in production

643
00:22:30,640 --> 00:22:35,679
fully soon

644
00:22:33,039 --> 00:22:37,039
and and great news for the research

645
00:22:35,679 --> 00:22:39,200
guarantee is that

646
00:22:37,039 --> 00:22:40,320
azure functions traces are now available

647
00:22:39,200 --> 00:22:42,159
to download

648
00:22:40,320 --> 00:22:43,600
and this would hopefully allow other

649
00:22:42,159 --> 00:22:47,000
researchers to

650
00:22:43,600 --> 00:22:48,080
reproduce our results and further go and

651
00:22:47,000 --> 00:22:50,799
[Music]

652
00:22:48,080 --> 00:22:53,039
conduct other research in the service

653
00:22:50,799 --> 00:22:54,879
domain and move the frontiers of this

654
00:22:53,039 --> 00:22:56,640
research area thank you for your

655
00:22:54,880 --> 00:23:08,960
attention and please

656
00:22:56,640 --> 00:23:11,039
look at our paper for further details

657
00:23:08,960 --> 00:23:11,039
you

