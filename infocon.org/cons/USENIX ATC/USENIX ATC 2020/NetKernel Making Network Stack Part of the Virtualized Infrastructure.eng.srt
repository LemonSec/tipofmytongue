1
00:00:09,280 --> 00:00:13,759
hello everyone

2
00:00:10,559 --> 00:00:16,799
i am trishunyo from microsoft research

3
00:00:13,759 --> 00:00:19,920
today i'm going to present our paper

4
00:00:16,800 --> 00:00:23,760
net kernel making network stack

5
00:00:19,920 --> 00:00:27,359
part of the virtualized infrastructure

6
00:00:23,760 --> 00:00:27,359
ok let's get started

7
00:00:29,679 --> 00:00:33,040
this is the current architecture in the

8
00:00:32,000 --> 00:00:37,519
cloud

9
00:00:33,040 --> 00:00:40,480
we have two layers vm

10
00:00:37,520 --> 00:00:41,360
this is fully controlled by the tenants

11
00:00:40,480 --> 00:00:43,519
and

12
00:00:41,360 --> 00:00:45,280
hypervisor and data center network

13
00:00:43,520 --> 00:00:47,200
infrastructures

14
00:00:45,280 --> 00:00:49,920
they are fully controlled by the

15
00:00:47,200 --> 00:00:49,920
operators

16
00:00:50,399 --> 00:00:54,800
but we think this kind of design has

17
00:00:53,039 --> 00:00:56,879
some limitations

18
00:00:54,800 --> 00:00:59,680
we will introduce a problem in two

19
00:00:56,879 --> 00:01:03,039
aspects

20
00:00:59,680 --> 00:01:05,040
first for the tightness

21
00:01:03,039 --> 00:01:07,280
if some tenants have some special

22
00:01:05,040 --> 00:01:10,400
requirements for the network stack

23
00:01:07,280 --> 00:01:13,920
they may need to tune the tcp parameter

24
00:01:10,400 --> 00:01:18,320
for example initial conjunction window

25
00:01:13,920 --> 00:01:19,520
initial rto mini rto and delayed sdk 10

26
00:01:18,320 --> 00:01:22,240
mods

27
00:01:19,520 --> 00:01:23,759
and also some kernel parameters for

28
00:01:22,240 --> 00:01:26,399
example

29
00:01:23,759 --> 00:01:28,080
socket buffer and if they have some

30
00:01:26,400 --> 00:01:30,400
special requirements

31
00:01:28,080 --> 00:01:32,400
they may need to install advanced

32
00:01:30,400 --> 00:01:36,560
conjunction control algorithms

33
00:01:32,400 --> 00:01:39,439
and user space networks in summary

34
00:01:36,560 --> 00:01:40,079
the tenants have to deal with the things

35
00:01:39,439 --> 00:01:43,199
regarding

36
00:01:40,079 --> 00:01:46,079
network stacks all by themselves

37
00:01:43,200 --> 00:01:48,000
but in fact the tenants are paid

38
00:01:46,079 --> 00:01:50,240
limitedly concerned with

39
00:01:48,000 --> 00:01:54,640
the performance and the functionality

40
00:01:50,240 --> 00:01:56,960
not the implementation details

41
00:01:54,640 --> 00:01:58,000
on the other hand for the operator

42
00:01:56,960 --> 00:02:00,158
actually

43
00:01:58,000 --> 00:02:01,920
they know everything there they can

44
00:02:00,159 --> 00:02:05,360
really help their tenants

45
00:02:01,920 --> 00:02:08,080
and also make some money on that

46
00:02:05,360 --> 00:02:10,560
they have enough resources including

47
00:02:08,080 --> 00:02:12,319
experts and expertise

48
00:02:10,560 --> 00:02:13,840
they know everything regarding the

49
00:02:12,319 --> 00:02:16,879
network

50
00:02:13,840 --> 00:02:21,440
including the linux kernel rdma

51
00:02:16,879 --> 00:02:21,440
fpga nic and dpdk

52
00:02:21,680 --> 00:02:25,360
however for the hypervester there are

53
00:02:24,800 --> 00:02:27,440
some

54
00:02:25,360 --> 00:02:31,599
nature bomb boundary between the

55
00:02:27,440 --> 00:02:31,599
provider and the tenants vm

56
00:02:32,959 --> 00:02:38,879
because for better they

57
00:02:36,480 --> 00:02:39,679
nearly have serial visibility and

58
00:02:38,879 --> 00:02:43,920
control

59
00:02:39,680 --> 00:02:46,560
of the network stack for the providers

60
00:02:43,920 --> 00:02:47,440
they cannot deploy new network stacks

61
00:02:46,560 --> 00:02:51,440
for example

62
00:02:47,440 --> 00:02:55,200
dc-tcp if it is difficult for them

63
00:02:51,440 --> 00:02:56,239
to define a performance sla difficult to

64
00:02:55,200 --> 00:02:59,280
prefer

65
00:02:56,239 --> 00:03:00,640
perform some management tasks and

66
00:02:59,280 --> 00:03:04,159
difficult for them

67
00:03:00,640 --> 00:03:07,119
to do the troubleshoot because of the

68
00:03:04,159 --> 00:03:09,519
boundary between the hypervisor and the

69
00:03:07,120 --> 00:03:11,680
tenant's vm

70
00:03:09,519 --> 00:03:12,800
is there a better way to solve this

71
00:03:11,680 --> 00:03:16,080
problem

72
00:03:12,800 --> 00:03:19,280
actually our answer is yes

73
00:03:16,080 --> 00:03:19,920
lucasfigure we can see the center of the

74
00:03:19,280 --> 00:03:22,800
problem

75
00:03:19,920 --> 00:03:24,399
is the network stack is covered to the

76
00:03:22,800 --> 00:03:27,040
gas os

77
00:03:24,400 --> 00:03:29,040
to solve this problem our solution is to

78
00:03:27,040 --> 00:03:32,400
make the network stack

79
00:03:29,040 --> 00:03:35,280
out of the tenant's vm to make it

80
00:03:32,400 --> 00:03:37,200
as a part of the infrastructure and

81
00:03:35,280 --> 00:03:39,840
controlled by the provider

82
00:03:37,200 --> 00:03:40,560
the network stack run as an independency

83
00:03:39,840 --> 00:03:45,920
module

84
00:03:40,560 --> 00:03:48,239
we call it network stack module or nsm

85
00:03:45,920 --> 00:03:50,238
and this is our solution

86
00:03:48,239 --> 00:03:53,439
making network stack part of the

87
00:03:50,239 --> 00:03:56,480
virtualized infrastructure

88
00:03:53,439 --> 00:03:59,519
in the tenant vm they keep the interface

89
00:03:56,480 --> 00:04:02,480
and change say bsc sockets

90
00:03:59,519 --> 00:04:02,959
and all of the packets can be handled in

91
00:04:02,480 --> 00:04:06,720
the

92
00:04:02,959 --> 00:04:07,680
sm by the provider now i will introduce

93
00:04:06,720 --> 00:04:11,280
some benefits

94
00:04:07,680 --> 00:04:13,920
if we can achieve our idea for the

95
00:04:11,280 --> 00:04:14,720
operator we can achieve better

96
00:04:13,920 --> 00:04:18,000
efficiency

97
00:04:14,720 --> 00:04:20,238
in management for example

98
00:04:18,000 --> 00:04:21,759
we can observate the resource

99
00:04:20,238 --> 00:04:25,359
provisioning strategy

100
00:04:21,759 --> 00:04:27,759
more flexibly and also

101
00:04:25,360 --> 00:04:31,199
we can implement the management function

102
00:04:27,759 --> 00:04:34,320
as a part of user's network stack

103
00:04:31,199 --> 00:04:37,440
and for the users they can have

104
00:04:34,320 --> 00:04:38,240
deployment and performance gains with

105
00:04:37,440 --> 00:04:41,520
nearly no

106
00:04:38,240 --> 00:04:43,520
efforts they can enforce various kernel

107
00:04:41,520 --> 00:04:45,520
stack optimizations

108
00:04:43,520 --> 00:04:47,280
and the high performance user space

109
00:04:45,520 --> 00:04:50,799
stack

110
00:04:47,280 --> 00:04:51,440
and also they can use some advanced

111
00:04:50,800 --> 00:04:55,520
hardware

112
00:04:51,440 --> 00:04:58,479
to achieve this to realize our idea

113
00:04:55,520 --> 00:04:59,599
we have three main challenges for us the

114
00:04:58,479 --> 00:05:01,520
first one is

115
00:04:59,600 --> 00:05:03,039
the user wouldn't like to change their

116
00:05:01,520 --> 00:05:05,440
applications

117
00:05:03,039 --> 00:05:06,800
can we transparently redirect the socket

118
00:05:05,440 --> 00:05:10,560
api cost

119
00:05:06,800 --> 00:05:14,240
without any change on the applications

120
00:05:10,560 --> 00:05:17,280
the second one is the vm and nsm

121
00:05:14,240 --> 00:05:20,560
they need to communicate with each other

122
00:05:17,280 --> 00:05:23,758
how can we create a channel to exchange

123
00:05:20,560 --> 00:05:26,560
the socket semantics

124
00:05:23,759 --> 00:05:27,919
the last one is how can we ensure the

125
00:05:26,560 --> 00:05:30,240
high performance

126
00:05:27,919 --> 00:05:31,599
with our semantic transmission in the

127
00:05:30,240 --> 00:05:34,800
modern data center

128
00:05:31,600 --> 00:05:34,800
say 100 k

129
00:05:35,120 --> 00:05:40,560
so now we will answer the questions one

130
00:05:38,000 --> 00:05:40,560
by one

131
00:05:41,280 --> 00:05:45,280
first we create a new socket type in the

132
00:05:44,240 --> 00:05:49,039
gas lift

133
00:05:45,280 --> 00:05:52,638
inside lens kernel this is a complete

134
00:05:49,039 --> 00:05:55,360
implementation of the bse circuit apis

135
00:05:52,639 --> 00:05:56,319
in the tenant's vm all of the system

136
00:05:55,360 --> 00:06:00,240
codes will

137
00:05:56,319 --> 00:06:02,960
divert to net kernel eps for example

138
00:06:00,240 --> 00:06:04,240
the socket will be divert to next kernel

139
00:06:02,960 --> 00:06:06,719
socket

140
00:06:04,240 --> 00:06:07,440
to not the sockets and message will be

141
00:06:06,720 --> 00:06:10,960
divert to

142
00:06:07,440 --> 00:06:14,000
net kernel circuits and message

143
00:06:10,960 --> 00:06:15,758
for every network work functions of the

144
00:06:14,000 --> 00:06:18,240
bic's bse sockets

145
00:06:15,759 --> 00:06:19,600
the gaslib provides the same same apis

146
00:06:18,240 --> 00:06:23,280
for the application

147
00:06:19,600 --> 00:06:26,960
in the linux kernel and

148
00:06:23,280 --> 00:06:28,400
as as you know the application works in

149
00:06:26,960 --> 00:06:34,638
the user space

150
00:06:28,400 --> 00:06:37,198
hence it works without any changes

151
00:06:34,639 --> 00:06:40,319
we also provide a lightweight semantic

152
00:06:37,199 --> 00:06:44,720
channel between the vm and nsms

153
00:06:40,319 --> 00:06:48,240
for the socket semantic we provide nqe

154
00:06:44,720 --> 00:06:51,440
next kernel q elements each nqa

155
00:06:48,240 --> 00:06:56,000
is 32 size it contains

156
00:06:51,440 --> 00:06:59,919
the operating time vmid queue site's id

157
00:06:56,000 --> 00:07:04,000
vm socket id offline data data pointer

158
00:06:59,919 --> 00:07:05,520
and the size of the data for each vita

159
00:07:04,000 --> 00:07:09,440
socket operation

160
00:07:05,520 --> 00:07:12,400
the gas sleep will translate this to nqe

161
00:07:09,440 --> 00:07:15,120
and put them into the queues inside the

162
00:07:12,400 --> 00:07:18,400
net kernel devices

163
00:07:15,120 --> 00:07:19,520
the gas leave also checks if there is

164
00:07:18,400 --> 00:07:23,039
any new response

165
00:07:19,520 --> 00:07:25,680
nqe from the from the nqe qs

166
00:07:23,039 --> 00:07:26,639
if yes the gas leave will process the

167
00:07:25,680 --> 00:07:29,599
nqe

168
00:07:26,639 --> 00:07:30,880
and do corresponding operation and then

169
00:07:29,599 --> 00:07:34,880
return this all

170
00:07:30,880 --> 00:07:37,840
returned to this app and by the way

171
00:07:34,880 --> 00:07:39,840
the queues are only for the metadata

172
00:07:37,840 --> 00:07:42,479
they also have huge pages

173
00:07:39,840 --> 00:07:45,840
in the net kernel devices for the real

174
00:07:42,479 --> 00:07:45,840
data transmission

175
00:07:46,400 --> 00:07:49,599
to make the net kernel scalable and

176
00:07:49,120 --> 00:07:52,720
achieve

177
00:07:49,599 --> 00:07:56,159
very high performance they also designed

178
00:07:52,720 --> 00:07:59,840
scalable lockless cues between the vm

179
00:07:56,160 --> 00:08:02,000
and nsm they are for core bases

180
00:07:59,840 --> 00:08:04,159
meanwhile all of the inquiry are

181
00:08:02,000 --> 00:08:07,440
switched by our call engine

182
00:08:04,160 --> 00:08:10,160
we also put a put a connection table

183
00:08:07,440 --> 00:08:10,800
into the code it can remember the

184
00:08:10,160 --> 00:08:14,479
mapping

185
00:08:10,800 --> 00:08:16,639
between the vm and nsm the provider

186
00:08:14,479 --> 00:08:18,560
is able to make their own policy to

187
00:08:16,639 --> 00:08:22,560
achieve the load balancing

188
00:08:18,560 --> 00:08:22,560
and their own sra for this

189
00:08:22,960 --> 00:08:30,479
and by the way we use vm based nsm

190
00:08:26,720 --> 00:08:33,440
that means each nsm is a vm actually

191
00:08:30,479 --> 00:08:34,319
we have three reasons to design this

192
00:08:33,440 --> 00:08:36,800
firstly

193
00:08:34,320 --> 00:08:38,719
it can support existing linux kernel and

194
00:08:36,799 --> 00:08:41,439
user space network stacks

195
00:08:38,719 --> 00:08:42,000
from various operating systems for

196
00:08:41,440 --> 00:08:45,360
example

197
00:08:42,000 --> 00:08:48,399
linux windows and freebsd

198
00:08:45,360 --> 00:08:51,120
and the asm does not need to have

199
00:08:48,399 --> 00:08:52,160
the same operating system as the

200
00:08:51,120 --> 00:08:55,279
hypervisor

201
00:08:52,160 --> 00:08:58,079
or the tenants secondly

202
00:08:55,279 --> 00:09:01,279
the vm can also provide good isolation

203
00:08:58,080 --> 00:09:04,240
to guarantee the performance of the nsm

204
00:09:01,279 --> 00:09:05,360
and lastly each network stack can be

205
00:09:04,240 --> 00:09:08,480
encapsulated

206
00:09:05,360 --> 00:09:12,480
as asm and run independently

207
00:09:08,480 --> 00:09:15,519
of the hypervisor if our lsm crashes

208
00:09:12,480 --> 00:09:17,920
we can just simply reboot it there is no

209
00:09:15,519 --> 00:09:20,240
need to do other things

210
00:09:17,920 --> 00:09:21,439
after introduce the design highlights of

211
00:09:20,240 --> 00:09:23,920
net kernel

212
00:09:21,440 --> 00:09:25,600
let's see the overall design of the net

213
00:09:23,920 --> 00:09:28,880
kernel framework

214
00:09:25,600 --> 00:09:29,519
as you can see in the netcode kernel

215
00:09:28,880 --> 00:09:32,640
framework

216
00:09:29,519 --> 00:09:37,360
we have several parts tenants vm

217
00:09:32,640 --> 00:09:41,120
we have gas leave in the tenants vm

218
00:09:37,360 --> 00:09:44,720
that gas leave can just uh intercept

219
00:09:41,120 --> 00:09:48,560
all of the aps of the bsc circuit

220
00:09:44,720 --> 00:09:51,680
and puts the cement circuit semantic

221
00:09:48,560 --> 00:09:56,160
into the net kernel device and

222
00:09:51,680 --> 00:09:59,359
in the nsm we also have a server slip

223
00:09:56,160 --> 00:10:00,319
pulling the nqes from the queues and the

224
00:09:59,360 --> 00:10:03,360
call the real

225
00:10:00,320 --> 00:10:06,399
network apis from the network that

226
00:10:03,360 --> 00:10:09,760
in the nsm they also have

227
00:10:06,399 --> 00:10:10,480
nitrogenous devices in the in the

228
00:10:09,760 --> 00:10:14,160
devices

229
00:10:10,480 --> 00:10:17,600
we have queues and huge pages queues

230
00:10:14,160 --> 00:10:18,959
are for the metadata exchange and for

231
00:10:17,600 --> 00:10:22,000
the huge pace

232
00:10:18,959 --> 00:10:25,119
and for the huge pages they are just for

233
00:10:22,000 --> 00:10:27,440
the data exchange

234
00:10:25,120 --> 00:10:29,440
and all of the cube elements actually

235
00:10:27,440 --> 00:10:31,040
are switched by the net kernel queue

236
00:10:29,440 --> 00:10:34,640
engine

237
00:10:31,040 --> 00:10:35,760
actually we also have v-neck to attach

238
00:10:34,640 --> 00:10:38,880
the rsm

239
00:10:35,760 --> 00:10:43,120
and the packets are go through the mimic

240
00:10:38,880 --> 00:10:47,839
to the virtual switch or embedded switch

241
00:10:43,120 --> 00:10:47,839
and then finally go to the pinx

242
00:10:49,040 --> 00:10:56,000
after finish this sign of net kernel

243
00:10:52,640 --> 00:10:59,519
we realize our net kernel on the

244
00:10:56,000 --> 00:11:04,360
linux kvm we use a machine

245
00:10:59,519 --> 00:11:06,399
with with two zero cpus and

246
00:11:04,360 --> 00:11:09,440
256 key

247
00:11:06,399 --> 00:11:12,800
memory and this machine is also

248
00:11:09,440 --> 00:11:15,839
equipped with a magnux 100 gig

249
00:11:12,800 --> 00:11:15,839
single port nick

250
00:11:16,079 --> 00:11:19,279
after discuss the design and the

251
00:11:18,079 --> 00:11:21,519
implementation

252
00:11:19,279 --> 00:11:22,920
let's discuss the first use case of the

253
00:11:21,519 --> 00:11:25,279
nitrogen

254
00:11:22,920 --> 00:11:26,240
multiplexing as you can see in the

255
00:11:25,279 --> 00:11:29,839
figure

256
00:11:26,240 --> 00:11:32,880
this is the normalized rps performance

257
00:11:29,839 --> 00:11:33,920
of a choice from a large class in the

258
00:11:32,880 --> 00:11:37,200
figure

259
00:11:33,920 --> 00:11:40,319
the three ages can achieve their peak

260
00:11:37,200 --> 00:11:44,320
performance in different times

261
00:11:40,320 --> 00:11:50,000
about 8 minutes 15 minutes

262
00:11:44,320 --> 00:11:52,880
25 minutes and 55 minutes

263
00:11:50,000 --> 00:11:54,240
to serve the pig from performance the

264
00:11:52,880 --> 00:11:58,480
each ag

265
00:11:54,240 --> 00:11:58,480
needs to be equipped for course

266
00:11:58,959 --> 00:12:05,839
but if you adopt nite kernel you may use

267
00:12:02,639 --> 00:12:06,800
last course you can do in this way for

268
00:12:05,839 --> 00:12:08,880
each a g

269
00:12:06,800 --> 00:12:10,479
you can assign one cpu core for the

270
00:12:08,880 --> 00:12:13,680
application logic

271
00:12:10,480 --> 00:12:16,880
and the nsm can be equipped with

272
00:12:13,680 --> 00:12:19,359
five cores to handle network connections

273
00:12:16,880 --> 00:12:20,800
of course you need to assign one core

274
00:12:19,360 --> 00:12:23,839
for the core engine

275
00:12:20,800 --> 00:12:26,000
to switch the nqe between the sm and the

276
00:12:23,839 --> 00:12:30,000
vms

277
00:12:26,000 --> 00:12:33,440
so you use nyquist in total

278
00:12:30,000 --> 00:12:35,839
in the experiment you can see since you

279
00:12:33,440 --> 00:12:36,800
assign last course to handle the same

280
00:12:35,839 --> 00:12:38,399
traces

281
00:12:36,800 --> 00:12:42,000
the net kernel can increase the

282
00:12:38,399 --> 00:12:42,000
normalized rps for core

283
00:12:42,079 --> 00:12:45,359
so the net kernel can help operator

284
00:12:44,320 --> 00:12:52,240
perform network

285
00:12:45,360 --> 00:12:54,079
management more efficiently in the

286
00:12:52,240 --> 00:12:57,760
second use case

287
00:12:54,079 --> 00:13:02,479
we will show how to deploy the mtcp

288
00:12:57,760 --> 00:13:05,519
without any api change in that kernel

289
00:13:02,480 --> 00:13:07,760
so far in the original mtcp they don't

290
00:13:05,519 --> 00:13:10,880
support the ntx yet

291
00:13:07,760 --> 00:13:14,880
so we port the mtcp as our

292
00:13:10,880 --> 00:13:18,399
mtcp nsm during importing the mtcp

293
00:13:14,880 --> 00:13:19,279
we also found a bug in the dpdk magnux

294
00:13:18,399 --> 00:13:22,320
driver

295
00:13:19,279 --> 00:13:23,760
i think it's very hard for a tenant to

296
00:13:22,320 --> 00:13:27,040
do this

297
00:13:23,760 --> 00:13:30,880
and finally the tenants can use the

298
00:13:27,040 --> 00:13:33,040
unmodified nx on mtcp without any tennis

299
00:13:30,880 --> 00:13:35,839
efforts

300
00:13:33,040 --> 00:13:37,360
and you can see in the figure we can

301
00:13:35,839 --> 00:13:40,480
have performance gain

302
00:13:37,360 --> 00:13:44,160
in all cpu configuration

303
00:13:40,480 --> 00:13:47,519
and in the four vcpu scenario

304
00:13:44,160 --> 00:13:50,719
the mtv brings about 1.8

305
00:13:47,519 --> 00:13:50,720
times performance gain

306
00:13:51,120 --> 00:13:55,440
in the third use case we provide a

307
00:13:53,680 --> 00:13:58,560
shared memory networking

308
00:13:55,440 --> 00:14:01,120
for the inter vm communication

309
00:13:58,560 --> 00:14:02,000
since with net kernel the operator can

310
00:14:01,120 --> 00:14:05,199
easily detect

311
00:14:02,000 --> 00:14:08,800
all-host traffic with net kernel it

312
00:14:05,199 --> 00:14:12,240
uses a shared memory to avoid tcp

313
00:14:08,800 --> 00:14:13,920
and bridge overhead compare with the

314
00:14:12,240 --> 00:14:17,279
baseline

315
00:14:13,920 --> 00:14:20,719
we can see when the message size is

316
00:14:17,279 --> 00:14:22,560
8k the share memory can achieve

317
00:14:20,720 --> 00:14:25,600
more than double performance gain for

318
00:14:22,560 --> 00:14:28,719
the on-hold traffic

319
00:14:25,600 --> 00:14:31,760
so in collision net kernel can help

320
00:14:28,720 --> 00:14:32,639
users to achieve deploy and performance

321
00:14:31,760 --> 00:14:36,160
gains

322
00:14:32,639 --> 00:14:36,160
with nearly no efforts

323
00:14:38,399 --> 00:14:42,079
and we also did some macro benchmark on

324
00:14:41,040 --> 00:14:46,000
the throughput

325
00:14:42,079 --> 00:14:48,800
and and the rps for net kernel

326
00:14:46,000 --> 00:14:52,000
we compared the baseline and the nas

327
00:14:48,800 --> 00:14:55,199
kernel you see your same setting

328
00:14:52,000 --> 00:14:59,519
eight tcp connections and uh

329
00:14:55,199 --> 00:14:59,519
the master size is also 8 kb

330
00:15:00,000 --> 00:15:04,000
we can see the net kernel can achieve

331
00:15:02,639 --> 00:15:07,199
the same performance

332
00:15:04,000 --> 00:15:10,800
of the baseline they both

333
00:15:07,199 --> 00:15:14,160
can achieve 100 gig in 8 core received

334
00:15:10,800 --> 00:15:14,160
and then 34 cents

335
00:15:14,320 --> 00:15:17,360
and we also benchmark the rpi

336
00:15:16,560 --> 00:15:20,719
performance

337
00:15:17,360 --> 00:15:23,680
of the net kernel we test the net kernel

338
00:15:20,720 --> 00:15:25,360
with a simple e4 server with short tcp

339
00:15:23,680 --> 00:15:29,439
connections

340
00:15:25,360 --> 00:15:32,720
for 64 requests and response the mtcp

341
00:15:29,440 --> 00:15:36,639
brings nearly double performance scale

342
00:15:32,720 --> 00:15:37,680
in 8 core scenarios for another cpu

343
00:15:36,639 --> 00:15:40,880
number

344
00:15:37,680 --> 00:15:43,758
we can also have some performance gains

345
00:15:40,880 --> 00:15:44,560
but we are so sorry that we didn't run

346
00:15:43,759 --> 00:15:47,519
mtcp

347
00:15:44,560 --> 00:15:48,399
successfully when the number of cpu

348
00:15:47,519 --> 00:15:51,759
equals to

349
00:15:48,399 --> 00:15:55,839
three five six seven so

350
00:15:51,759 --> 00:15:55,839
we didn't provide this kind of data here

351
00:15:56,240 --> 00:16:00,399
after introduce the performance results

352
00:15:58,639 --> 00:16:02,800
of the net kernel

353
00:16:00,399 --> 00:16:04,320
we will also have some discussion on the

354
00:16:02,800 --> 00:16:07,680
net kernel

355
00:16:04,320 --> 00:16:09,680
first how can we do the next

356
00:16:07,680 --> 00:16:10,880
i think the general answer of this

357
00:16:09,680 --> 00:16:14,000
question is

358
00:16:10,880 --> 00:16:14,880
it is hard for us to spot it it is

359
00:16:14,000 --> 00:16:18,240
because

360
00:16:14,880 --> 00:16:21,040
we move the network stack out of the vm

361
00:16:18,240 --> 00:16:22,399
all of the packets process actually are

362
00:16:21,040 --> 00:16:25,439
in the nsm

363
00:16:22,399 --> 00:16:28,560
and the user has no control on it

364
00:16:25,440 --> 00:16:30,720
if this is a single tenant nsm

365
00:16:28,560 --> 00:16:32,160
it is possible for us to provide some

366
00:16:30,720 --> 00:16:35,600
callback function

367
00:16:32,160 --> 00:16:38,880
to enable the net filter for users

368
00:16:35,600 --> 00:16:41,440
but for multiple tenants in asm

369
00:16:38,880 --> 00:16:42,639
it is really hard for them to do this

370
00:16:41,440 --> 00:16:46,000
kind of thing

371
00:16:42,639 --> 00:16:48,720
because the privilege control for this

372
00:16:46,000 --> 00:16:50,160
is really hard we suggest in this kind

373
00:16:48,720 --> 00:16:52,240
of situation

374
00:16:50,160 --> 00:16:55,839
the user should use the traditional

375
00:16:52,240 --> 00:16:58,560
architecture for the net filter

376
00:16:55,839 --> 00:17:00,639
and the second question is what about

377
00:16:58,560 --> 00:17:02,000
the troubleshooting for the performance

378
00:17:00,639 --> 00:17:05,120
issue

379
00:17:02,000 --> 00:17:08,240
we believe this is one of our benefits

380
00:17:05,119 --> 00:17:09,438
of net kernel in the traditional

381
00:17:08,240 --> 00:17:11,919
architecture

382
00:17:09,439 --> 00:17:12,799
it is very hard for users to diagnose

383
00:17:11,919 --> 00:17:15,760
network

384
00:17:12,799 --> 00:17:17,760
since the root cause of the failure is

385
00:17:15,760 --> 00:17:19,839
very complex

386
00:17:17,760 --> 00:17:21,679
the problem may happen in the network

387
00:17:19,839 --> 00:17:25,520
stack hypervisor

388
00:17:21,679 --> 00:17:27,919
and also the data center infrastructure

389
00:17:25,520 --> 00:17:29,760
we believe it is operator's

390
00:17:27,919 --> 00:17:30,320
responsibility to do this kind of

391
00:17:29,760 --> 00:17:33,919
network

392
00:17:30,320 --> 00:17:37,200
diagnosis with net kernel

393
00:17:33,919 --> 00:17:40,320
the provider can easily monitor their

394
00:17:37,200 --> 00:17:41,520
nsm by deploying additional mechanism in

395
00:17:40,320 --> 00:17:44,559
the sm

396
00:17:41,520 --> 00:17:47,039
for example ping mesh

397
00:17:44,559 --> 00:17:48,480
if they find something wrong they can

398
00:17:47,039 --> 00:17:51,600
quickly fix the problem

399
00:17:48,480 --> 00:17:51,600
all by themselves

400
00:17:52,080 --> 00:17:57,840
and the third question is does the net

401
00:17:54,960 --> 00:18:00,799
kernel increase the attack surface

402
00:17:57,840 --> 00:18:02,799
our answer is no we think netground

403
00:18:00,799 --> 00:18:05,760
shares same attack service

404
00:18:02,799 --> 00:18:07,760
as the obs and the vinik for their net

405
00:18:05,760 --> 00:18:10,960
kernel devices

406
00:18:07,760 --> 00:18:14,080
they can have their own address spaces

407
00:18:10,960 --> 00:18:15,919
and if a data pointer is not valid we

408
00:18:14,080 --> 00:18:19,439
will block it

409
00:18:15,919 --> 00:18:22,160
and between the hsm and the vm

410
00:18:19,440 --> 00:18:22,640
we can use the isolated channel and we

411
00:18:22,160 --> 00:18:24,960
also

412
00:18:22,640 --> 00:18:26,160
have some future directions for net

413
00:18:24,960 --> 00:18:28,720
kernel also

414
00:18:26,160 --> 00:18:30,720
net kernel is already enabled to have

415
00:18:28,720 --> 00:18:32,320
some simple isolation

416
00:18:30,720 --> 00:18:34,880
we may need some higher level

417
00:18:32,320 --> 00:18:35,918
performance isolation for the net kernel

418
00:18:34,880 --> 00:18:38,799
besides the

419
00:18:35,919 --> 00:18:41,120
throughput isolation we may also need

420
00:18:38,799 --> 00:18:43,600
the acid the rps isolation

421
00:18:41,120 --> 00:18:46,559
and i think the charging policy are also

422
00:18:43,600 --> 00:18:49,760
very interesting to research

423
00:18:46,559 --> 00:18:51,600
a question is how does a user pay for

424
00:18:49,760 --> 00:18:54,879
the nsm usage

425
00:18:51,600 --> 00:18:56,959
by rps by throughput or by the code

426
00:18:54,880 --> 00:19:00,400
number they use

427
00:18:56,960 --> 00:19:03,679
i think this is need to have some

428
00:19:00,400 --> 00:19:06,559
further investigation and finally

429
00:19:03,679 --> 00:19:08,000
to further decrease the word heart we

430
00:19:06,559 --> 00:19:10,879
can use fpga

431
00:19:08,000 --> 00:19:13,919
or soc to implement the core engine at

432
00:19:10,880 --> 00:19:16,320
the net kernel devices

433
00:19:13,919 --> 00:19:18,480
okay i think this is all of the

434
00:19:16,320 --> 00:19:21,678
presentation of the net kernel

435
00:19:18,480 --> 00:19:24,960
this there are some recaps on this

436
00:19:21,679 --> 00:19:26,000
in this paper we and implemented the net

437
00:19:24,960 --> 00:19:28,400
kernel

438
00:19:26,000 --> 00:19:30,080
the next kernel decouples the network

439
00:19:28,400 --> 00:19:33,039
stack from the guest

440
00:19:30,080 --> 00:19:35,840
and making it part of the virtualized

441
00:19:33,039 --> 00:19:38,960
infrastructure in the cloud

442
00:19:35,840 --> 00:19:41,678
and in this paper we enabled

443
00:19:38,960 --> 00:19:43,520
several new use cases for example

444
00:19:41,679 --> 00:19:47,919
multiplexing

445
00:19:43,520 --> 00:19:50,799
mtcp nsm and the share memory lsm

446
00:19:47,919 --> 00:19:52,480
and we conducted the comprehensive

447
00:19:50,799 --> 00:19:56,320
testbed evaluation

448
00:19:52,480 --> 00:20:00,720
with commodity 100k nics

449
00:19:56,320 --> 00:20:03,200
and our official website is neskerno.net

450
00:20:00,720 --> 00:20:04,159
and finally i want to say thank you to

451
00:20:03,200 --> 00:20:07,840
every audience

452
00:20:04,159 --> 00:20:07,840
of this talk thank you

