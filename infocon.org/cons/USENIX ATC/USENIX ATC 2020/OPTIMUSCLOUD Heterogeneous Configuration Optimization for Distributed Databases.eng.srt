1
00:00:08,559 --> 00:00:12,479
hello everyone

2
00:00:09,760 --> 00:00:13,040
my name is ashraf mahkup today i will

3
00:00:12,480 --> 00:00:15,280
present

4
00:00:13,040 --> 00:00:17,198
optimus cloud heterogeneous

5
00:00:15,280 --> 00:00:20,320
configuration optimization

6
00:00:17,199 --> 00:00:22,160
for distributed databases in the cloud

7
00:00:20,320 --> 00:00:24,160
this work is a collaboration between

8
00:00:22,160 --> 00:00:26,640
purdue university microsoft

9
00:00:24,160 --> 00:00:30,640
adobe research and google research and

10
00:00:26,640 --> 00:00:34,800
this project is supported by the nih

11
00:00:30,640 --> 00:00:37,360
and here is the agenda of our talk today

12
00:00:34,800 --> 00:00:39,839
now optimus cloud's goal is to achieve

13
00:00:37,360 --> 00:00:42,960
cost and performance efficiency

14
00:00:39,840 --> 00:00:45,600
for cloud-hosted key value clusters

15
00:00:42,960 --> 00:00:46,879
and the way optimus cloud achieves this

16
00:00:45,600 --> 00:00:48,879
goal is through

17
00:00:46,879 --> 00:00:51,199
online configuration tuning under

18
00:00:48,879 --> 00:00:53,839
dynamic workloads

19
00:00:51,199 --> 00:00:55,360
the configuration parameters that optas

20
00:00:53,840 --> 00:00:58,640
cloud considers

21
00:00:55,360 --> 00:01:00,960
come from two sets of parameters

22
00:00:58,640 --> 00:01:02,000
the first set is the set of parameters

23
00:01:00,960 --> 00:01:05,040
that

24
00:01:02,000 --> 00:01:08,080
that are exposed by the key value store

25
00:01:05,040 --> 00:01:09,680
itself such as the size of the cache

26
00:01:08,080 --> 00:01:12,320
the number of reading and writing

27
00:01:09,680 --> 00:01:13,280
threads the compaction method used and

28
00:01:12,320 --> 00:01:16,320
the throat

29
00:01:13,280 --> 00:01:18,240
of that compaction method etc

30
00:01:16,320 --> 00:01:21,119
and the second set of configuration

31
00:01:18,240 --> 00:01:22,960
parameters that optimus cloud considers

32
00:01:21,119 --> 00:01:24,880
is the set of parameters that are

33
00:01:22,960 --> 00:01:27,119
controlled by the vm

34
00:01:24,880 --> 00:01:28,560
the cloud vm size and type such as the

35
00:01:27,119 --> 00:01:30,640
number of cores

36
00:01:28,560 --> 00:01:34,000
the available memory and the network

37
00:01:30,640 --> 00:01:37,439
bandwidth for every node in the cluster

38
00:01:34,000 --> 00:01:39,119
and under dynamic workloads by

39
00:01:37,439 --> 00:01:42,559
online tuning of these configuration

40
00:01:39,119 --> 00:01:45,759
parameters optimus cloud can achieve

41
00:01:42,560 --> 00:01:47,840
cost optimal performance over time

42
00:01:45,759 --> 00:01:49,600
now let's see why this is a challenging

43
00:01:47,840 --> 00:01:52,560
problem

44
00:01:49,600 --> 00:01:54,559
first to combine these two sets of

45
00:01:52,560 --> 00:01:56,320
configuration parameters

46
00:01:54,560 --> 00:01:58,640
the parameters that control the key

47
00:01:56,320 --> 00:02:01,839
value store along with

48
00:01:58,640 --> 00:02:04,079
the vm type and size we produce a large

49
00:02:01,840 --> 00:02:06,079
configuration speeds

50
00:02:04,079 --> 00:02:07,520
so for example cassandra which we use in

51
00:02:06,079 --> 00:02:10,160
our evaluation

52
00:02:07,520 --> 00:02:11,519
has 25 plus performance tuning

53
00:02:10,160 --> 00:02:15,120
parameters

54
00:02:11,520 --> 00:02:16,160
and amazon ec2 which we use as our cloud

55
00:02:15,120 --> 00:02:19,280
provider

56
00:02:16,160 --> 00:02:20,799
has 133 different instance types and

57
00:02:19,280 --> 00:02:23,840
sites

58
00:02:20,800 --> 00:02:26,720
and those instances vary in price by a

59
00:02:23,840 --> 00:02:28,800
factor of 5000.

60
00:02:26,720 --> 00:02:30,560
the second challenge is the dependency

61
00:02:28,800 --> 00:02:32,720
between the key value store

62
00:02:30,560 --> 00:02:34,720
and the vm configurations for example

63
00:02:32,720 --> 00:02:36,720
the cache size of cassandra

64
00:02:34,720 --> 00:02:38,080
will be limited by the available ram in

65
00:02:36,720 --> 00:02:41,599
the cloud v

66
00:02:38,080 --> 00:02:44,319
which is controlled by its type or size

67
00:02:41,599 --> 00:02:47,920
what optimus cloud does is it jointly

68
00:02:44,319 --> 00:02:49,280
optimize these two configuration spaces

69
00:02:47,920 --> 00:02:51,440
while taking into account the

70
00:02:49,280 --> 00:02:54,000
dependencies between them to achieve

71
00:02:51,440 --> 00:02:56,480
globally optimized performance

72
00:02:54,000 --> 00:02:59,200
now to show how important it is to

73
00:02:56,480 --> 00:03:02,399
jointly tune these two spaces

74
00:02:59,200 --> 00:03:05,359
we benchmark one cassandra server with a

75
00:03:02,400 --> 00:03:08,159
30 minute trace from one of our three

76
00:03:05,360 --> 00:03:09,840
workloads which we will discuss uh in a

77
00:03:08,159 --> 00:03:13,040
few slides

78
00:03:09,840 --> 00:03:16,159
on six different ec2 vm types

79
00:03:13,040 --> 00:03:19,280
and sites for each type and size

80
00:03:16,159 --> 00:03:21,280
we use 300 different database

81
00:03:19,280 --> 00:03:22,239
configurations selected through grid

82
00:03:21,280 --> 00:03:24,000
search

83
00:03:22,239 --> 00:03:26,560
we show the performance in terms of

84
00:03:24,000 --> 00:03:29,599
throughput on the y-axis

85
00:03:26,560 --> 00:03:32,400
for the best the default

86
00:03:29,599 --> 00:03:34,399
and the worst configuration and we see a

87
00:03:32,400 --> 00:03:36,640
big variance in performance with respect

88
00:03:34,400 --> 00:03:40,239
to the database configuration

89
00:03:36,640 --> 00:03:41,679
by up to 474 percent better performance

90
00:03:40,239 --> 00:03:44,400
over default

91
00:03:41,680 --> 00:03:46,480
configurations moreover the best

92
00:03:44,400 --> 00:03:47,360
configurations for every vm type and

93
00:03:46,480 --> 00:03:51,440
size

94
00:03:47,360 --> 00:03:53,120
is different therefore jointly tuning

95
00:03:51,440 --> 00:03:55,120
the configuration parameters of the

96
00:03:53,120 --> 00:03:58,720
database with the vm

97
00:03:55,120 --> 00:04:01,760
type and size is crucial to achieve

98
00:03:58,720 --> 00:04:04,560
optimized performance

99
00:04:01,760 --> 00:04:06,079
and now let's take a quick look at the

100
00:04:04,560 --> 00:04:09,120
overview

101
00:04:06,080 --> 00:04:11,120
of optima's cloud design

102
00:04:09,120 --> 00:04:13,280
the first component in optimus cloud is

103
00:04:11,120 --> 00:04:16,720
our cloud predictor that uses

104
00:04:13,280 --> 00:04:19,839
historical traces of the workload and

105
00:04:16,720 --> 00:04:22,720
based on these traces the workload

106
00:04:19,839 --> 00:04:25,039
predictor provides optimus cloud with

107
00:04:22,720 --> 00:04:26,840
predicted workload patterns for a local

108
00:04:25,040 --> 00:04:29,600
hit

109
00:04:26,840 --> 00:04:32,159
duration the second component

110
00:04:29,600 --> 00:04:32,720
which is also trained in the offline

111
00:04:32,160 --> 00:04:35,440
phase

112
00:04:32,720 --> 00:04:37,440
is a performance prediction model that

113
00:04:35,440 --> 00:04:40,840
takes into consideration the

114
00:04:37,440 --> 00:04:42,160
workload description the application

115
00:04:40,840 --> 00:04:44,320
configuration

116
00:04:42,160 --> 00:04:46,000
the replication factor consistency level

117
00:04:44,320 --> 00:04:49,440
and data placement

118
00:04:46,000 --> 00:04:52,960
of the cluster as well as the vm specs

119
00:04:49,440 --> 00:04:55,600
that is the vm type and size

120
00:04:52,960 --> 00:04:56,159
to predict the performance per dollar

121
00:04:55,600 --> 00:05:00,240
for a

122
00:04:56,160 --> 00:05:02,320
single server or a complete set

123
00:05:00,240 --> 00:05:04,160
and in the online phase the third

124
00:05:02,320 --> 00:05:07,919
component which is the

125
00:05:04,160 --> 00:05:10,000
cluster performance predictor takes the

126
00:05:07,919 --> 00:05:11,120
predicted values of the performance for

127
00:05:10,000 --> 00:05:14,400
every server

128
00:05:11,120 --> 00:05:17,360
as well as the predicted workload

129
00:05:14,400 --> 00:05:18,719
and the information of the cluster such

130
00:05:17,360 --> 00:05:20,880
as the replication factor and the

131
00:05:18,720 --> 00:05:23,759
consistency level

132
00:05:20,880 --> 00:05:24,639
to predict what would be the total

133
00:05:23,759 --> 00:05:26,800
operation

134
00:05:24,639 --> 00:05:28,479
operations per second on the cluster

135
00:05:26,800 --> 00:05:31,680
level

136
00:05:28,479 --> 00:05:34,960
the fourth component takes this

137
00:05:31,680 --> 00:05:35,840
total of per second for the cluster

138
00:05:34,960 --> 00:05:39,198
level

139
00:05:35,840 --> 00:05:42,159
and performs the optimization given

140
00:05:39,199 --> 00:05:43,440
the user's budget and the minimum ops

141
00:05:42,160 --> 00:05:46,960
per second

142
00:05:43,440 --> 00:05:50,240
as well as the cloud provider cost model

143
00:05:46,960 --> 00:05:51,599
to find the optimal cloud configurations

144
00:05:50,240 --> 00:05:54,000
as well as

145
00:05:51,600 --> 00:05:56,400
key value store configurations for our

146
00:05:54,000 --> 00:05:58,560
cluster

147
00:05:56,400 --> 00:06:00,080
under dynamic workloads which we often

148
00:05:58,560 --> 00:06:02,720
see in practice

149
00:06:00,080 --> 00:06:04,159
the workload characteristics change very

150
00:06:02,720 --> 00:06:07,600
frequently over time

151
00:06:04,160 --> 00:06:09,680
sometimes an unpredictable way and this

152
00:06:07,600 --> 00:06:12,960
makes the problem even more challenging

153
00:06:09,680 --> 00:06:16,160
because upon a workload change

154
00:06:12,960 --> 00:06:19,440
my old configurations might not be

155
00:06:16,160 --> 00:06:20,479
optimized for the new workload and

156
00:06:19,440 --> 00:06:24,319
therefore

157
00:06:20,479 --> 00:06:26,400
a reconfiguration is necessary however

158
00:06:24,319 --> 00:06:28,639
doing this reconfiguration and runtime

159
00:06:26,400 --> 00:06:30,638
usually requires a server restart

160
00:06:28,639 --> 00:06:33,280
and that causes a downtime and a

161
00:06:30,639 --> 00:06:36,240
degradation and performance

162
00:06:33,280 --> 00:06:38,479
so for fast changing workloads these

163
00:06:36,240 --> 00:06:41,120
frequent reconfiguration will

164
00:06:38,479 --> 00:06:42,639
add these transient downtime and

165
00:06:41,120 --> 00:06:45,360
degradation and performance

166
00:06:42,639 --> 00:06:47,280
which can cause severe degradation of

167
00:06:45,360 --> 00:06:48,880
the overall cluster performance over

168
00:06:47,280 --> 00:06:50,719
time

169
00:06:48,880 --> 00:06:52,000
and the question that optimus cloud is

170
00:06:50,720 --> 00:06:54,479
trying to answer is

171
00:06:52,000 --> 00:06:56,080
can we reconfigure only a subset of the

172
00:06:54,479 --> 00:06:57,599
nodes in the cluster

173
00:06:56,080 --> 00:07:00,240
to achieve globally optimized

174
00:06:57,599 --> 00:07:03,919
performance while minimizing the

175
00:07:00,240 --> 00:07:06,000
downgrade time due to reconfiguration

176
00:07:03,919 --> 00:07:07,120
and if so which nodes should we

177
00:07:06,000 --> 00:07:10,400
reconfigure upon

178
00:07:07,120 --> 00:07:12,160
workload change and also notice that

179
00:07:10,400 --> 00:07:13,520
changing the configurations of only a

180
00:07:12,160 --> 00:07:14,840
subset of the nodes

181
00:07:13,520 --> 00:07:16,639
will lead to heterogeneous

182
00:07:14,840 --> 00:07:20,638
configurations and that

183
00:07:16,639 --> 00:07:22,800
is by itself a new challenge

184
00:07:20,639 --> 00:07:24,400
here we show an example of why

185
00:07:22,800 --> 00:07:25,840
heterogeneous configurations are

186
00:07:24,400 --> 00:07:28,318
beneficial

187
00:07:25,840 --> 00:07:31,599
we will mark a cluster of four cassandra

188
00:07:28,319 --> 00:07:34,319
nodes with our mg rust workload

189
00:07:31,599 --> 00:07:37,759
and that workload switches between read

190
00:07:34,319 --> 00:07:40,319
heavy phases and right heavy phases

191
00:07:37,759 --> 00:07:42,000
and for this benchmark we use all

192
00:07:40,319 --> 00:07:46,000
combinations of

193
00:07:42,000 --> 00:07:50,560
two vm and instance types and sides

194
00:07:46,000 --> 00:07:53,360
c4 dot large and r4 dot x large

195
00:07:50,560 --> 00:07:55,919
we noticed the following with all nodes

196
00:07:53,360 --> 00:07:59,360
configured to c4 dot large

197
00:07:55,919 --> 00:08:01,919
we achieve the best write performance

198
00:07:59,360 --> 00:08:04,080
however we also achieve the lowest read

199
00:08:01,919 --> 00:08:07,440
performance because c4 dot large

200
00:08:04,080 --> 00:08:09,680
has very small ram

201
00:08:07,440 --> 00:08:11,520
now by reconfiguring two of the nodes to

202
00:08:09,680 --> 00:08:15,199
r4 with excelors which has

203
00:08:11,520 --> 00:08:16,719
much bigger ram size than c4 we notice

204
00:08:15,199 --> 00:08:20,319
that there is a degradation

205
00:08:16,720 --> 00:08:23,759
and performance with respect to writes

206
00:08:20,319 --> 00:08:26,560
however we see a 9x improvement with

207
00:08:23,759 --> 00:08:29,120
respect to reads

208
00:08:26,560 --> 00:08:30,000
we also noticed that changing more nodes

209
00:08:29,120 --> 00:08:33,279
to r4

210
00:08:30,000 --> 00:08:36,880
to x large reduces the performance

211
00:08:33,279 --> 00:08:38,719
for both reads and writes accordingly

212
00:08:36,880 --> 00:08:41,279
the best configurations

213
00:08:38,719 --> 00:08:43,440
is to use all c4 dot large for right

214
00:08:41,279 --> 00:08:45,839
heavy phases of the workload

215
00:08:43,440 --> 00:08:47,360
while reconfiguring only two servers to

216
00:08:45,839 --> 00:08:51,279
r4 to explore

217
00:08:47,360 --> 00:08:53,760
when the workload becomes read heavy

218
00:08:51,279 --> 00:08:55,360
so heterogeneous configurations have the

219
00:08:53,760 --> 00:08:57,680
advantage of reducing the

220
00:08:55,360 --> 00:09:00,640
reconfiguration downtime

221
00:08:57,680 --> 00:09:02,319
along with avoiding over provisioning by

222
00:09:00,640 --> 00:09:03,279
reconfiguring the minimum number of

223
00:09:02,320 --> 00:09:06,320
nodes

224
00:09:03,279 --> 00:09:08,320
needed to achieve the best performance

225
00:09:06,320 --> 00:09:10,080
however this comes with a new challenge

226
00:09:08,320 --> 00:09:12,720
which is the additional

227
00:09:10,080 --> 00:09:14,959
increase in the search space due to

228
00:09:12,720 --> 00:09:17,680
heterogeneity

229
00:09:14,959 --> 00:09:21,040
consider a cluster of 20 nodes and 15

230
00:09:17,680 --> 00:09:22,880
different configurations to pick from

231
00:09:21,040 --> 00:09:24,319
if we only consider homogeneous

232
00:09:22,880 --> 00:09:27,200
configurations

233
00:09:24,320 --> 00:09:28,240
then we have 15 options no matter what

234
00:09:27,200 --> 00:09:30,640
the cluster size

235
00:09:28,240 --> 00:09:31,760
is however with heterogeneous

236
00:09:30,640 --> 00:09:33,439
configurations

237
00:09:31,760 --> 00:09:36,720
the problem becomes similar to

238
00:09:33,440 --> 00:09:38,000
distributing 20 identical balls over 15

239
00:09:36,720 --> 00:09:39,600
boxes

240
00:09:38,000 --> 00:09:41,440
causing the number of possible

241
00:09:39,600 --> 00:09:44,880
configurations to be

242
00:09:41,440 --> 00:09:47,440
1.3 multiplied by 10 to the power of 9

243
00:09:44,880 --> 00:09:51,279
possible configurations and this is

244
00:09:47,440 --> 00:09:53,680
indeed infeasible to model or optimize

245
00:09:51,279 --> 00:09:55,920
and to solve this problem optimus cloud

246
00:09:53,680 --> 00:09:59,359
introduces the concept of

247
00:09:55,920 --> 00:10:01,279
complete sets which we define as the

248
00:09:59,360 --> 00:10:03,839
minimum subset of nodes

249
00:10:01,279 --> 00:10:04,959
for which the union of their data

250
00:10:03,839 --> 00:10:07,440
records

251
00:10:04,959 --> 00:10:10,160
covers all the records in the database

252
00:10:07,440 --> 00:10:13,600
at least once

253
00:10:10,160 --> 00:10:13,920
so this concept of complete sets relies

254
00:10:13,600 --> 00:10:17,200
on

255
00:10:13,920 --> 00:10:18,880
selecting the fastest replica for a

256
00:10:17,200 --> 00:10:21,440
given request

257
00:10:18,880 --> 00:10:22,320
which is implemented in many key value

258
00:10:21,440 --> 00:10:25,200
stores

259
00:10:22,320 --> 00:10:27,440
for example this is achieved through the

260
00:10:25,200 --> 00:10:30,480
dynamics niche of cassandra

261
00:10:27,440 --> 00:10:33,839
or the adaptive replica selection

262
00:10:30,480 --> 00:10:36,079
in elastic search and

263
00:10:33,839 --> 00:10:37,920
given that using the concept of a

264
00:10:36,079 --> 00:10:40,479
consistency level

265
00:10:37,920 --> 00:10:42,160
we can define how many replicas need to

266
00:10:40,480 --> 00:10:44,320
reply to a request

267
00:10:42,160 --> 00:10:47,199
before it is satisfied and returned to

268
00:10:44,320 --> 00:10:49,360
the user and therefore

269
00:10:47,200 --> 00:10:52,399
the latency of the request is dominated

270
00:10:49,360 --> 00:10:55,600
by the slowest triplica

271
00:10:52,399 --> 00:10:57,440
and the servers within a complete set

272
00:10:55,600 --> 00:11:00,000
must be upgraded to the faster

273
00:10:57,440 --> 00:11:03,279
configuration upon our cloud change

274
00:11:00,000 --> 00:11:05,360
for the cluster performance to improve

275
00:11:03,279 --> 00:11:06,880
accordingly optimus cloud keeps the

276
00:11:05,360 --> 00:11:08,480
configurations

277
00:11:06,880 --> 00:11:10,959
to be homogenous within the same

278
00:11:08,480 --> 00:11:12,839
complete set while allowing different

279
00:11:10,959 --> 00:11:16,000
complete sets to have different

280
00:11:12,839 --> 00:11:17,760
configurations and now we show how

281
00:11:16,000 --> 00:11:18,240
partitioning the cluster into complete

282
00:11:17,760 --> 00:11:21,680
sets

283
00:11:18,240 --> 00:11:22,399
reduces the search space first we show

284
00:11:21,680 --> 00:11:24,160
that

285
00:11:22,399 --> 00:11:25,600
the number of complete sets in any

286
00:11:24,160 --> 00:11:28,719
cluster is

287
00:11:25,600 --> 00:11:31,920
upper bounded by the replication factor

288
00:11:28,720 --> 00:11:33,760
for the details of the proof please see

289
00:11:31,920 --> 00:11:35,279
our paper

290
00:11:33,760 --> 00:11:37,200
and also knowing that the replication

291
00:11:35,279 --> 00:11:39,839
factor is practically

292
00:11:37,200 --> 00:11:41,440
low with three or five being a practical

293
00:11:39,839 --> 00:11:43,839
upper bound

294
00:11:41,440 --> 00:11:44,959
along with reconfiguring a number of

295
00:11:43,839 --> 00:11:46,880
complete sets

296
00:11:44,959 --> 00:11:48,079
that is equivalent to the consistency

297
00:11:46,880 --> 00:11:49,920
level

298
00:11:48,079 --> 00:11:51,199
we guarantee that all requests are

299
00:11:49,920 --> 00:11:54,560
served from nodes with

300
00:11:51,200 --> 00:11:56,240
optimized configurations and revisiting

301
00:11:54,560 --> 00:11:58,800
the same example with

302
00:11:56,240 --> 00:12:01,040
20 nodes and 15 configuration parameters

303
00:11:58,800 --> 00:12:03,519
to select from

304
00:12:01,040 --> 00:12:04,480
now with partitioning the cluster into s

305
00:12:03,519 --> 00:12:08,160
complete sets

306
00:12:04,480 --> 00:12:11,360
we have only 680 possible configurations

307
00:12:08,160 --> 00:12:14,000
compared to 1.3 multiplied by

308
00:12:11,360 --> 00:12:16,639
10 to the power of 9 if we don't

309
00:12:14,000 --> 00:12:19,360
partition the cluster into complete

310
00:12:16,639 --> 00:12:20,560
and this space of 680 possible

311
00:12:19,360 --> 00:12:23,600
configurations

312
00:12:20,560 --> 00:12:26,959
is indeed feasible to model

313
00:12:23,600 --> 00:12:26,959
and optimize

314
00:12:27,839 --> 00:12:32,320
now this is an example of how optimus

315
00:12:30,240 --> 00:12:34,720
cloud identifies complete sets

316
00:12:32,320 --> 00:12:37,200
in the cluster using the data placement

317
00:12:34,720 --> 00:12:39,519
information for every node

318
00:12:37,200 --> 00:12:40,880
the first cluster on the left hand side

319
00:12:39,519 --> 00:12:44,639
cluster 1

320
00:12:40,880 --> 00:12:48,240
has a complete set configured to r4

321
00:12:44,639 --> 00:12:51,600
and r4 is memory optimized and therefore

322
00:12:48,240 --> 00:12:54,320
achieves 7x better reads per second

323
00:12:51,600 --> 00:12:55,360
compared to cluster 2. notice that

324
00:12:54,320 --> 00:12:58,480
cluster 2 is

325
00:12:55,360 --> 00:13:01,120
using the exact same resources as

326
00:12:58,480 --> 00:13:02,160
cluster 1 however it doesn't take into

327
00:13:01,120 --> 00:13:04,240
consideration

328
00:13:02,160 --> 00:13:06,079
the data placement information for every

329
00:13:04,240 --> 00:13:09,600
node and therefore

330
00:13:06,079 --> 00:13:12,160
doesn't configure a complete set to the

331
00:13:09,600 --> 00:13:12,639
read optimize configuration causing its

332
00:13:12,160 --> 00:13:16,480
read

333
00:13:12,639 --> 00:13:18,480
performance to be very low

334
00:13:16,480 --> 00:13:21,040
and before we go to our evaluation we

335
00:13:18,480 --> 00:13:24,079
first introduce the three applications

336
00:13:21,040 --> 00:13:27,439
that we use as our benchmarking

337
00:13:24,079 --> 00:13:29,279
workload the first application is

338
00:13:27,440 --> 00:13:31,600
the largest meta genomics analysis

339
00:13:29,279 --> 00:13:33,279
portal called mgrest

340
00:13:31,600 --> 00:13:35,200
and for which we use the real workload

341
00:13:33,279 --> 00:13:38,399
trace

342
00:13:35,200 --> 00:13:41,279
to replay and we see that there is no

343
00:13:38,399 --> 00:13:42,639
daily or weekly pattern in the workload

344
00:13:41,279 --> 00:13:44,240
and the workload changes very

345
00:13:42,639 --> 00:13:46,720
drastically over time

346
00:13:44,240 --> 00:13:50,639
which makes it very hard to predict a

347
00:13:46,720 --> 00:13:53,440
look ahead period more than five minutes

348
00:13:50,639 --> 00:13:56,079
the second application is real trees

349
00:13:53,440 --> 00:13:58,480
from a bus tracking mobile application

350
00:13:56,079 --> 00:14:00,079
and for that particular application we

351
00:13:58,480 --> 00:14:02,639
see that there is a strong daily and

352
00:14:00,079 --> 00:14:05,760
weekly pattern in the workload switches

353
00:14:02,639 --> 00:14:06,800
and this causes a much longer and more

354
00:14:05,760 --> 00:14:09,040
accurate

355
00:14:06,800 --> 00:14:10,399
look ahead period of one hour or two

356
00:14:09,040 --> 00:14:12,240
hours

357
00:14:10,399 --> 00:14:13,600
and the third application is a simulated

358
00:14:12,240 --> 00:14:16,800
workload trace

359
00:14:13,600 --> 00:14:20,800
from batch data analytics jobs

360
00:14:16,800 --> 00:14:23,439
and using priorly introduced

361
00:14:20,800 --> 00:14:24,800
profiling techniques the job execution

362
00:14:23,440 --> 00:14:28,000
times and

363
00:14:24,800 --> 00:14:31,839
resource allocation are predictable for

364
00:14:28,000 --> 00:14:31,839
a longer look-ahead period

365
00:14:32,800 --> 00:14:37,199
here we show the performance prediction

366
00:14:34,880 --> 00:14:39,439
accuracy of optimus cloud

367
00:14:37,199 --> 00:14:40,880
in terms of r square and root mean

368
00:14:39,440 --> 00:14:43,839
square error

369
00:14:40,880 --> 00:14:44,880
we compare to several baselines the

370
00:14:43,839 --> 00:14:47,920
first baseline

371
00:14:44,880 --> 00:14:50,880
that we call end solitary models

372
00:14:47,920 --> 00:14:52,479
builds a performance prediction model

373
00:14:50,880 --> 00:14:55,839
for every vm type

374
00:14:52,480 --> 00:14:58,800
or size select

375
00:14:55,839 --> 00:15:00,079
uses a singular value decomposition

376
00:14:58,800 --> 00:15:02,079
based technique

377
00:15:00,079 --> 00:15:03,920
to infer the performance for unseen

378
00:15:02,079 --> 00:15:05,680
configurations

379
00:15:03,920 --> 00:15:08,399
and optimus cloud compares two

380
00:15:05,680 --> 00:15:11,599
approaches one that describes the vm

381
00:15:08,399 --> 00:15:13,839
type or size as a categorical parameter

382
00:15:11,600 --> 00:15:15,040
versus a second that describes the vm

383
00:15:13,839 --> 00:15:17,680
type or size

384
00:15:15,040 --> 00:15:18,639
in terms of its number of cores amount

385
00:15:17,680 --> 00:15:20,479
of memory

386
00:15:18,639 --> 00:15:23,680
and network bandwidth which we call

387
00:15:20,480 --> 00:15:25,440
optimus numerical

388
00:15:23,680 --> 00:15:27,920
this numerical model achieves better

389
00:15:25,440 --> 00:15:30,240
performance than all other baselines

390
00:15:27,920 --> 00:15:32,319
due to its ability to perform transfer

391
00:15:30,240 --> 00:15:34,079
learning between different vm types and

392
00:15:32,320 --> 00:15:36,639
sizes

393
00:15:34,079 --> 00:15:37,279
we also introduce the four baselines

394
00:15:36,639 --> 00:15:39,839
that we

395
00:15:37,279 --> 00:15:41,040
compare the performance of optimus cloud

396
00:15:39,839 --> 00:15:43,759
2

397
00:15:41,040 --> 00:15:45,360
the first baseline is homogeneous static

398
00:15:43,759 --> 00:15:47,920
configuration

399
00:15:45,360 --> 00:15:49,920
that is optimized for the entire

400
00:15:47,920 --> 00:15:52,399
duration of the predicted workload

401
00:15:49,920 --> 00:15:54,479
but it is a static configuration that

402
00:15:52,399 --> 00:15:55,920
the same configuration is being used for

403
00:15:54,480 --> 00:15:59,600
the entire duration

404
00:15:55,920 --> 00:16:00,719
of the benchmark the second technique is

405
00:15:59,600 --> 00:16:03,360
a reactive

406
00:16:00,720 --> 00:16:04,240
technique called cherry pick which was

407
00:16:03,360 --> 00:16:08,240
introduced in

408
00:16:04,240 --> 00:16:09,920
nsr nsdi17 cherry pick uses

409
00:16:08,240 --> 00:16:12,240
base and optimization to find a

410
00:16:09,920 --> 00:16:15,040
heterogeneous cloud configuration

411
00:16:12,240 --> 00:16:17,120
for a representative job or phase of the

412
00:16:15,040 --> 00:16:20,480
workload

413
00:16:17,120 --> 00:16:24,000
the third baseline is selecta

414
00:16:20,480 --> 00:16:26,720
which was introduced in usenix atc 18

415
00:16:24,000 --> 00:16:28,079
selecta uses a singular various

416
00:16:26,720 --> 00:16:30,160
decomposition technique

417
00:16:28,079 --> 00:16:31,599
to select the optimized homogeneous

418
00:16:30,160 --> 00:16:34,000
cloud configuration

419
00:16:31,600 --> 00:16:35,440
for different jobs or phases of the

420
00:16:34,000 --> 00:16:38,399
workload

421
00:16:35,440 --> 00:16:39,519
which is also a reactive technique and

422
00:16:38,399 --> 00:16:43,040
the fourth

423
00:16:39,519 --> 00:16:47,040
of this line is our paper from

424
00:16:43,040 --> 00:16:47,759
atc 19 sofia in which we use genetic

425
00:16:47,040 --> 00:16:49,680
algorithms

426
00:16:47,759 --> 00:16:52,639
and performance modeling to find

427
00:16:49,680 --> 00:16:54,719
optimized homogeneous configurations

428
00:16:52,639 --> 00:16:57,040
which once we only consider the key

429
00:16:54,720 --> 00:17:00,160
value store parameters without

430
00:16:57,040 --> 00:17:03,279
configuring the cloud

431
00:17:00,160 --> 00:17:03,279
in an online manner

432
00:17:03,360 --> 00:17:08,400
here we show the performance of optimus

433
00:17:05,599 --> 00:17:11,198
cloud compared to the four base lines

434
00:17:08,400 --> 00:17:12,720
that we just introduced in terms of

435
00:17:11,199 --> 00:17:14,720
throughput per dollar

436
00:17:12,720 --> 00:17:16,799
as well as the lateral latency or the

437
00:17:14,720 --> 00:17:19,120
p99

438
00:17:16,799 --> 00:17:21,039
in all three applications optimus cloud

439
00:17:19,119 --> 00:17:21,359
achieves the best throughput per dollar

440
00:17:21,039 --> 00:17:23,599
or

441
00:17:21,359 --> 00:17:25,520
ops per second per dollar without

442
00:17:23,599 --> 00:17:27,039
sacrificing or increasing the tail

443
00:17:25,520 --> 00:17:29,840
latency

444
00:17:27,039 --> 00:17:31,840
optimus cloud achieves up to 86 percent

445
00:17:29,840 --> 00:17:32,559
better performance per dollar over the

446
00:17:31,840 --> 00:17:35,120
homogenous

447
00:17:32,559 --> 00:17:35,678
static configuration due to optimus

448
00:17:35,120 --> 00:17:37,520
cloud's

449
00:17:35,679 --> 00:17:39,360
capability of performing online

450
00:17:37,520 --> 00:17:42,160
reconfiguration

451
00:17:39,360 --> 00:17:42,799
also optimus cloud achieves up to 173

452
00:17:42,160 --> 00:17:45,440
percent

453
00:17:42,799 --> 00:17:47,679
and 130 over both cherry pick and

454
00:17:45,440 --> 00:17:50,000
selector respectively

455
00:17:47,679 --> 00:17:51,440
due to its ability to reconfigure only a

456
00:17:50,000 --> 00:17:53,280
subset of the nodes

457
00:17:51,440 --> 00:17:56,960
which minimizes the reconfiguration

458
00:17:53,280 --> 00:17:59,200
downtime and avoids over provisioning

459
00:17:56,960 --> 00:18:02,640
compared to sofia optimus cloud achieves

460
00:17:59,200 --> 00:18:03,360
up to 212 percent better performance per

461
00:18:02,640 --> 00:18:05,440
dollar

462
00:18:03,360 --> 00:18:06,959
as sophia considers only homogenous

463
00:18:05,440 --> 00:18:09,520
configurations

464
00:18:06,960 --> 00:18:11,280
also for only the key value store

465
00:18:09,520 --> 00:18:13,600
parameters

466
00:18:11,280 --> 00:18:16,320
these observations together show the

467
00:18:13,600 --> 00:18:19,039
importance of our two main contributions

468
00:18:16,320 --> 00:18:21,760
first we jointly tune key values to our

469
00:18:19,039 --> 00:18:23,280
parameters with cloud vm parameters

470
00:18:21,760 --> 00:18:25,039
and we perform heterogeneous

471
00:18:23,280 --> 00:18:28,720
configurations to achieve

472
00:18:25,039 --> 00:18:31,120
globally optimized performance over time

473
00:18:28,720 --> 00:18:32,559
now since optimus cloud relies on two

474
00:18:31,120 --> 00:18:34,399
types of predictors

475
00:18:32,559 --> 00:18:36,320
the workload predictor as well as the

476
00:18:34,400 --> 00:18:38,320
performance predictor

477
00:18:36,320 --> 00:18:39,760
we want to investigate how tolerant

478
00:18:38,320 --> 00:18:42,879
optimus cloud is

479
00:18:39,760 --> 00:18:45,360
to errors in both predictors therefore

480
00:18:42,880 --> 00:18:47,520
in this experiment we add synthetic

481
00:18:45,360 --> 00:18:48,719
noise to the output of each predictor

482
00:18:47,520 --> 00:18:50,879
separately

483
00:18:48,720 --> 00:18:52,400
and then show how does the benefit of

484
00:18:50,880 --> 00:18:54,320
optimus cloud

485
00:18:52,400 --> 00:18:57,280
change with the amount of synthetic

486
00:18:54,320 --> 00:19:00,720
noise we draw the noise

487
00:18:57,280 --> 00:19:03,760
on the x-axis and the improvement of our

488
00:19:00,720 --> 00:19:04,960
homogeneous static on the y-axis the

489
00:19:03,760 --> 00:19:07,520
percentage of noise

490
00:19:04,960 --> 00:19:09,520
is represented as a uniform random

491
00:19:07,520 --> 00:19:12,080
variable that is added or

492
00:19:09,520 --> 00:19:14,320
subtracted from the output of the

493
00:19:12,080 --> 00:19:17,280
predictor

494
00:19:14,320 --> 00:19:18,639
as expected the performance of optimus

495
00:19:17,280 --> 00:19:21,120
cloud

496
00:19:18,640 --> 00:19:23,360
over homogenous static decreases with

497
00:19:21,120 --> 00:19:25,760
increasing levels of noise

498
00:19:23,360 --> 00:19:28,159
as the selected configurations deviate

499
00:19:25,760 --> 00:19:30,320
from the best configurations

500
00:19:28,160 --> 00:19:31,280
we also noticed that optimus cloud is

501
00:19:30,320 --> 00:19:33,678
more sensitive

502
00:19:31,280 --> 00:19:35,678
to errors than the throughput predictor

503
00:19:33,679 --> 00:19:37,360
compared to the errors and the workload

504
00:19:35,679 --> 00:19:39,840
predictor

505
00:19:37,360 --> 00:19:40,879
which is demonstrated in the steeper

506
00:19:39,840 --> 00:19:43,918
down

507
00:19:40,880 --> 00:19:46,160
word slope in the noisy throughput

508
00:19:43,919 --> 00:19:48,080
predictor curve

509
00:19:46,160 --> 00:19:50,080
the reason is that a slight deviation

510
00:19:48,080 --> 00:19:53,199
from the optimal configurations

511
00:19:50,080 --> 00:19:54,559
that is proposed by the performance

512
00:19:53,200 --> 00:19:56,400
predictor

513
00:19:54,559 --> 00:19:58,399
cause a significant reduction in

514
00:19:56,400 --> 00:20:00,480
performance perturber

515
00:19:58,400 --> 00:20:02,720
on the other hand slight errors in

516
00:20:00,480 --> 00:20:04,960
workload predictor

517
00:20:02,720 --> 00:20:06,240
causes optimus cloud to reconfigure

518
00:20:04,960 --> 00:20:09,120
either earlier or

519
00:20:06,240 --> 00:20:09,919
later than it optimally should but still

520
00:20:09,120 --> 00:20:13,120
achieves

521
00:20:09,919 --> 00:20:15,280
the gain of the overlapped duration

522
00:20:13,120 --> 00:20:17,520
between the actual and the predicted

523
00:20:15,280 --> 00:20:20,320
workloads

524
00:20:17,520 --> 00:20:20,720
to conclude our talk we find that for

525
00:20:20,320 --> 00:20:22,639
cost

526
00:20:20,720 --> 00:20:24,880
optimal performance of a distributed key

527
00:20:22,640 --> 00:20:28,000
value store cluster in the cloud

528
00:20:24,880 --> 00:20:30,640
it is critical to jointly tune both the

529
00:20:28,000 --> 00:20:32,960
key value store configuration parameters

530
00:20:30,640 --> 00:20:34,640
along with the cloud vm configuration

531
00:20:32,960 --> 00:20:38,000
parameters to achieve

532
00:20:34,640 --> 00:20:40,080
cost optimized performance

533
00:20:38,000 --> 00:20:42,159
optimus cloud provides the insight that

534
00:20:40,080 --> 00:20:43,678
it is optimal to create heterogeneous

535
00:20:42,159 --> 00:20:46,240
configurations

536
00:20:43,679 --> 00:20:48,080
and for this it determines at run time

537
00:20:46,240 --> 00:20:50,960
the minimum number of servers

538
00:20:48,080 --> 00:20:53,120
to reconfigure this of course reduces

539
00:20:50,960 --> 00:20:57,039
the reconfiguration downtime

540
00:20:53,120 --> 00:20:59,520
and avoids over provisioning

541
00:20:57,039 --> 00:21:00,480
but also increases the search space and

542
00:20:59,520 --> 00:21:03,280
using a novel

543
00:21:00,480 --> 00:21:05,039
concept of complete sets optimus cloud

544
00:21:03,280 --> 00:21:05,440
provides a technique to search through

545
00:21:05,039 --> 00:21:08,240
this

546
00:21:05,440 --> 00:21:10,159
large space that is brought by

547
00:21:08,240 --> 00:21:12,400
heterogeneity

548
00:21:10,159 --> 00:21:14,720
configurations found by optimus cloud

549
00:21:12,400 --> 00:21:18,000
outperform those by prior works

550
00:21:14,720 --> 00:21:21,200
cherry-pick selecta and sofia in both

551
00:21:18,000 --> 00:21:24,799
performance per dollar or throughput and

552
00:21:21,200 --> 00:21:26,000
till latency and here we reach the end

553
00:21:24,799 --> 00:21:28,240
of our talk

554
00:21:26,000 --> 00:21:29,200
and i will be happy to tweak any

555
00:21:28,240 --> 00:21:31,840
questions

556
00:21:29,200 --> 00:21:31,840
thank you

