1
00:00:09,599 --> 00:00:12,960
hello everyone

2
00:00:10,639 --> 00:00:13,678
my name is joshua today i'm presenting

3
00:00:12,960 --> 00:00:15,519
our work

4
00:00:13,679 --> 00:00:17,279
disaggregated persistent memory and

5
00:00:15,519 --> 00:00:19,359
controlling the remotely

6
00:00:17,279 --> 00:00:20,960
inspiration of passive disaggregated

7
00:00:19,359 --> 00:00:22,480
five stars

8
00:00:20,960 --> 00:00:24,400
this is a joint work with senior

9
00:00:22,480 --> 00:00:26,400
thailand yin zhang from purdue and uc

10
00:00:24,400 --> 00:00:27,919
san diego

11
00:00:26,400 --> 00:00:30,560
a recent trend called resource

12
00:00:27,920 --> 00:00:33,200
disaggregation breaks monolithic servers

13
00:00:30,560 --> 00:00:35,680
into network attached resource pools

14
00:00:33,200 --> 00:00:36,800
it offers better manageability enables

15
00:00:35,680 --> 00:00:39,120
independent scaling

16
00:00:36,800 --> 00:00:39,839
and tech resource packing and therefore

17
00:00:39,120 --> 00:00:41,919
has attracted

18
00:00:39,840 --> 00:00:43,680
more attention and adoption in academia

19
00:00:41,920 --> 00:00:46,239
and the industry

20
00:00:43,680 --> 00:00:48,239
notably this aggregate storage which

21
00:00:46,239 --> 00:00:49,839
separates compute and storage pools

22
00:00:48,239 --> 00:00:52,320
has been a common practice in current

23
00:00:49,840 --> 00:00:55,199
data centers and clouds like aws

24
00:00:52,320 --> 00:00:57,199
facebook and snowflake alibaba lists

25
00:00:55,199 --> 00:00:59,599
fast disaggregate storage as a major

26
00:00:57,199 --> 00:01:00,960
enabling factor of the huge single stage

27
00:00:59,600 --> 00:01:03,280
sale need

28
00:01:00,960 --> 00:01:04,000
this aggregated storage enables flexible

29
00:01:03,280 --> 00:01:05,760
management

30
00:01:04,000 --> 00:01:08,240
and independent scaling of this data

31
00:01:05,760 --> 00:01:10,320
center storage resistance

32
00:01:08,240 --> 00:01:12,158
more recently with networks getting

33
00:01:10,320 --> 00:01:13,439
faster and applications hitting memory

34
00:01:12,159 --> 00:01:16,240
capacity wall

35
00:01:13,439 --> 00:01:17,919
data centers like microsoft huawei are

36
00:01:16,240 --> 00:01:20,798
turning into adding more memory as

37
00:01:17,920 --> 00:01:21,360
remote or disaggregated memory with more

38
00:01:20,799 --> 00:01:23,280
memory

39
00:01:21,360 --> 00:01:25,439
we can run more types of big data

40
00:01:23,280 --> 00:01:28,159
applications

41
00:01:25,439 --> 00:01:29,199
so how about persistent memory or pm

42
00:01:28,159 --> 00:01:31,439
like dram

43
00:01:29,200 --> 00:01:32,560
pian is bite adjustable and has similar

44
00:01:31,439 --> 00:01:35,279
performance

45
00:01:32,560 --> 00:01:36,960
like ssd and disc it is durable and

46
00:01:35,280 --> 00:01:39,200
power resilience

47
00:01:36,960 --> 00:01:41,280
if we can disaggregate the pn it can

48
00:01:39,200 --> 00:01:44,240
enjoy these aggregations management

49
00:01:41,280 --> 00:01:46,560
scalability and utilization benefits it

50
00:01:44,240 --> 00:01:48,640
will also be an easy way to integrate pi

51
00:01:46,560 --> 00:01:50,960
into current data centers

52
00:01:48,640 --> 00:01:52,640
can we use existing disaggregate systems

53
00:01:50,960 --> 00:01:55,439
for dpm

54
00:01:52,640 --> 00:01:57,920
dancers no because current disaggregate

55
00:01:55,439 --> 00:01:59,439
storage systems are designed for ssds

56
00:01:57,920 --> 00:02:01,360
and hard disks

57
00:01:59,439 --> 00:02:02,719
that software system will be too slow

58
00:02:01,360 --> 00:02:05,200
for pm

59
00:02:02,719 --> 00:02:07,360
on the other hand this aggregated memory

60
00:02:05,200 --> 00:02:07,600
system do not provide data durability

61
00:02:07,360 --> 00:02:10,239
and

62
00:02:07,600 --> 00:02:11,840
reliability mechanisms and cannot be

63
00:02:10,239 --> 00:02:15,120
used either

64
00:02:11,840 --> 00:02:17,280
so how shall we build this aggregated pm

65
00:02:15,120 --> 00:02:19,120
to answer this question let's first

66
00:02:17,280 --> 00:02:20,879
review the spectrum of today's p and

67
00:02:19,120 --> 00:02:22,520
deploying models

68
00:02:20,879 --> 00:02:24,160
on the far left side is the

69
00:02:22,520 --> 00:02:26,800
non-disaggregation modes

70
00:02:24,160 --> 00:02:28,000
or traditional distributed pm modes

71
00:02:26,800 --> 00:02:30,480
regular servers are

72
00:02:28,000 --> 00:02:31,760
connected by the data center network

73
00:02:30,480 --> 00:02:33,920
each server runs

74
00:02:31,760 --> 00:02:35,040
application computation and hosts some

75
00:02:33,920 --> 00:02:37,920
pm

76
00:02:35,040 --> 00:02:38,319
some of the pn is used locally some used

77
00:02:37,920 --> 00:02:41,518
by

78
00:02:38,319 --> 00:02:43,440
remote servers in addition each server

79
00:02:41,519 --> 00:02:45,360
runs a peer management layer

80
00:02:43,440 --> 00:02:47,040
there are several notable systems in

81
00:02:45,360 --> 00:02:49,440
this area already

82
00:02:47,040 --> 00:02:51,040
at the center is what we call the active

83
00:02:49,440 --> 00:02:53,120
disaggregation mode

84
00:02:51,040 --> 00:02:54,799
regular servers on the left side run

85
00:02:53,120 --> 00:02:57,519
application computation

86
00:02:54,800 --> 00:02:59,280
pn nodes on the right side hosts pm and

87
00:02:57,519 --> 00:03:01,440
run the management layer of this

88
00:02:59,280 --> 00:03:03,120
pm there have been quite a lot of

89
00:03:01,440 --> 00:03:04,480
systems that adopt activities

90
00:03:03,120 --> 00:03:07,280
aggregation model

91
00:03:04,480 --> 00:03:08,720
although most of them are durian or ssd

92
00:03:07,280 --> 00:03:10,640
based

93
00:03:08,720 --> 00:03:12,720
both non-desegregation and active

94
00:03:10,640 --> 00:03:15,440
disaggregation have been extensively

95
00:03:12,720 --> 00:03:18,159
studied and adopted in the real world

96
00:03:15,440 --> 00:03:19,120
however there's a completely unexplored

97
00:03:18,159 --> 00:03:22,399
area

98
00:03:19,120 --> 00:03:24,799
an area we call passive disaggregation

99
00:03:22,400 --> 00:03:26,879
like active disaggregation regular

100
00:03:24,799 --> 00:03:30,000
servers run applications

101
00:03:26,879 --> 00:03:31,518
and piano nodes held pm but instead of

102
00:03:30,000 --> 00:03:32,560
running the management layer within the

103
00:03:31,519 --> 00:03:34,159
pn nodes

104
00:03:32,560 --> 00:03:35,920
passive disaggregation runs at the

105
00:03:34,159 --> 00:03:38,399
computer servers

106
00:03:35,920 --> 00:03:39,359
this work for the first time there's a

107
00:03:38,400 --> 00:03:42,640
thorough study of

108
00:03:39,360 --> 00:03:46,000
passive disaggregation model

109
00:03:42,640 --> 00:03:48,399
with our pdpm model pn devices only

110
00:03:46,000 --> 00:03:50,799
need to have nic and pm and they are

111
00:03:48,400 --> 00:03:52,480
accessible only via network

112
00:03:50,799 --> 00:03:54,400
in addition to its disaggregation

113
00:03:52,480 --> 00:03:57,359
benefits pdpm has

114
00:03:54,400 --> 00:04:00,319
low cat packs and opex and avoids any

115
00:03:57,360 --> 00:04:03,439
scalability bottlenecks at pn nodes

116
00:04:00,319 --> 00:04:04,238
moreover pdpn is an unexplored research

117
00:04:03,439 --> 00:04:06,239
area

118
00:04:04,239 --> 00:04:07,760
studying it would answer many questions

119
00:04:06,239 --> 00:04:10,799
and serve as a guide

120
00:04:07,760 --> 00:04:13,439
for future pm research

121
00:04:10,799 --> 00:04:14,560
one thing was mentioned is that pdp is

122
00:04:13,439 --> 00:04:16,798
possible now

123
00:04:14,560 --> 00:04:17,600
thanks to today's faster data center

124
00:04:16,798 --> 00:04:20,159
network

125
00:04:17,600 --> 00:04:22,240
and technologies like rdma that i can

126
00:04:20,160 --> 00:04:25,040
bypass cpu

127
00:04:22,240 --> 00:04:25,600
with this new hardware model the natural

128
00:04:25,040 --> 00:04:28,400
and most

129
00:04:25,600 --> 00:04:29,919
important question is without processing

130
00:04:28,400 --> 00:04:31,919
power at the pm

131
00:04:29,919 --> 00:04:34,560
where and how shall we process and

132
00:04:31,919 --> 00:04:37,039
measure data

133
00:04:34,560 --> 00:04:37,840
a most straightforward way is to run

134
00:04:37,040 --> 00:04:40,560
everything

135
00:04:37,840 --> 00:04:42,479
including data accesses and demand pm

136
00:04:40,560 --> 00:04:45,600
management at computer nodes

137
00:04:42,479 --> 00:04:47,280
or cn this architecture essentially

138
00:04:45,600 --> 00:04:50,560
distributes both data

139
00:04:47,280 --> 00:04:52,638
and control planes dual cns

140
00:04:50,560 --> 00:04:54,479
a different way is to centralize data

141
00:04:52,639 --> 00:04:56,639
and control planes

142
00:04:54,479 --> 00:04:58,400
for example by adding a centralized

143
00:04:56,639 --> 00:05:02,080
coordinator between cns

144
00:04:58,400 --> 00:05:04,320
and dns the coordinator

145
00:05:02,080 --> 00:05:07,599
could perform all magnetic operations

146
00:05:04,320 --> 00:05:10,240
and coordinator data accesses from cns

147
00:05:07,600 --> 00:05:12,000
a third way is a hybrid model that

148
00:05:10,240 --> 00:05:13,120
combines distributed and centralized

149
00:05:12,000 --> 00:05:15,280
approaches

150
00:05:13,120 --> 00:05:16,160
for example the data plane can be

151
00:05:15,280 --> 00:05:18,239
performed

152
00:05:16,160 --> 00:05:20,160
as science in a distributed way and

153
00:05:18,240 --> 00:05:22,639
control plane could be carried out at a

154
00:05:20,160 --> 00:05:24,880
centralized server

155
00:05:22,639 --> 00:05:26,800
based on these three architectures we

156
00:05:24,880 --> 00:05:27,840
designed and implemented three key value

157
00:05:26,800 --> 00:05:30,720
stores

158
00:05:27,840 --> 00:05:32,560
pdpn direct for the first model pdp

159
00:05:30,720 --> 00:05:35,520
essential for the second model

160
00:05:32,560 --> 00:05:36,080
and clever for the hybrid model we

161
00:05:35,520 --> 00:05:38,719
performed

162
00:05:36,080 --> 00:05:39,840
extensive experiments to evaluate the

163
00:05:38,720 --> 00:05:43,360
performance

164
00:05:39,840 --> 00:05:46,080
scalability and cost of this resistance

165
00:05:43,360 --> 00:05:47,759
overall we find that clover which is our

166
00:05:46,080 --> 00:05:50,639
final pdpm product

167
00:05:47,759 --> 00:05:52,320
is the best model among the three and it

168
00:05:50,639 --> 00:05:56,000
achieves similar performance

169
00:05:52,320 --> 00:05:57,120
as the active pdpm model but at a lower

170
00:05:56,000 --> 00:05:58,880
cost

171
00:05:57,120 --> 00:06:00,479
we also discussed interesting these

172
00:05:58,880 --> 00:06:04,319
trade-offs between active

173
00:06:00,479 --> 00:06:05,039
and passive vpns for the rest of today's

174
00:06:04,319 --> 00:06:06,960
talk

175
00:06:05,039 --> 00:06:09,360
i will go through these three systems

176
00:06:06,960 --> 00:06:10,560
one by one and present the evaluation

177
00:06:09,360 --> 00:06:13,759
results

178
00:06:10,560 --> 00:06:16,639
all these systems guarantee that reids

179
00:06:13,759 --> 00:06:17,919
only see committee data and rights are

180
00:06:16,639 --> 00:06:19,759
always atomic

181
00:06:17,919 --> 00:06:21,280
and are able to recover from power

182
00:06:19,759 --> 00:06:23,520
failures

183
00:06:21,280 --> 00:06:26,000
now let's take a closer look at the pdpn

184
00:06:23,520 --> 00:06:26,000
direct

185
00:06:26,160 --> 00:06:32,479
in this mode cns are regular servers

186
00:06:29,600 --> 00:06:34,000
that access and manage dns directly via

187
00:06:32,479 --> 00:06:36,719
one-sided rdma

188
00:06:34,000 --> 00:06:38,080
both data and control planes run within

189
00:06:36,720 --> 00:06:40,560
cns

190
00:06:38,080 --> 00:06:41,800
the key feature of pdpn direct is the

191
00:06:40,560 --> 00:06:45,120
distributed

192
00:06:41,800 --> 00:06:48,560
unorchestrated scenes which result in

193
00:06:45,120 --> 00:06:50,240
two design challenges how to efficiently

194
00:06:48,560 --> 00:06:51,360
coordinate the management of the

195
00:06:50,240 --> 00:06:53,520
airspace

196
00:06:51,360 --> 00:06:54,560
and how to coordinate concurrent agrees

197
00:06:53,520 --> 00:06:58,400
and rights

198
00:06:54,560 --> 00:06:59,280
across cns our solution to space

199
00:06:58,400 --> 00:07:02,318
management

200
00:06:59,280 --> 00:07:03,198
is to pre-assign two spaces for each tv

201
00:07:02,319 --> 00:07:05,840
entry

202
00:07:03,199 --> 00:07:07,199
one committed space for reits and one

203
00:07:05,840 --> 00:07:10,400
uncommitted space

204
00:07:07,199 --> 00:07:11,520
for in-flight rights by pre-assigning

205
00:07:10,400 --> 00:07:14,960
static spaces

206
00:07:11,520 --> 00:07:17,039
that are accessible by ocns we avoid

207
00:07:14,960 --> 00:07:19,599
the need to coordinate space allocation

208
00:07:17,039 --> 00:07:22,000
and de-allocation at the runtime

209
00:07:19,599 --> 00:07:22,639
in addition we introduce a lock-free

210
00:07:22,000 --> 00:07:25,440
read

211
00:07:22,639 --> 00:07:28,080
to ensure read integrity with help of a

212
00:07:25,440 --> 00:07:30,719
per-empty checksum

213
00:07:28,080 --> 00:07:31,840
finally we introduced an efficient rdma

214
00:07:30,720 --> 00:07:34,560
based write log

215
00:07:31,840 --> 00:07:35,919
for concurrent writers these read and

216
00:07:34,560 --> 00:07:39,039
write mechanisms

217
00:07:35,919 --> 00:07:40,479
require no coordination across cns which

218
00:07:39,039 --> 00:07:42,880
enable good performance

219
00:07:40,479 --> 00:07:44,240
with the pdp and direct model while

220
00:07:42,880 --> 00:07:46,400
delivering our synchronization

221
00:07:44,240 --> 00:07:49,360
guarantees

222
00:07:46,400 --> 00:07:52,799
to perform rights cm first uses rdma

223
00:07:49,360 --> 00:07:56,479
component swap to acquire the remote log

224
00:07:52,800 --> 00:07:58,400
once acquired cn will use rdma write

225
00:07:56,479 --> 00:07:59,919
to write the data and checksum into the

226
00:07:58,400 --> 00:08:02,000
uncommitted space

227
00:07:59,919 --> 00:08:03,919
serving as a redo copy if a crash

228
00:08:02,000 --> 00:08:07,039
happens in the middle

229
00:08:03,919 --> 00:08:08,479
after that ca uses another idma right

230
00:08:07,039 --> 00:08:10,240
to write the data and checksum into the

231
00:08:08,479 --> 00:08:13,440
committee space

232
00:08:10,240 --> 00:08:16,240
at last cn uses a final rdma write to

233
00:08:13,440 --> 00:08:19,520
unlock the remote entry

234
00:08:16,240 --> 00:08:21,680
to read an entry cn issues an rdma

235
00:08:19,520 --> 00:08:23,198
read to read a committed data in a

236
00:08:21,680 --> 00:08:25,199
checksum

237
00:08:23,199 --> 00:08:26,560
cn then calculates a checksum of the

238
00:08:25,199 --> 00:08:28,720
data just read

239
00:08:26,560 --> 00:08:30,000
and compares it to the checksum red from

240
00:08:28,720 --> 00:08:32,159
the vm

241
00:08:30,000 --> 00:08:33,279
if they match then the read operation

242
00:08:32,159 --> 00:08:35,279
finishes

243
00:08:33,279 --> 00:08:37,120
otherwise it means the data is being

244
00:08:35,279 --> 00:08:38,880
modified by a concurrent writer

245
00:08:37,120 --> 00:08:41,200
the cn will then repeat the process

246
00:08:38,880 --> 00:08:43,279
until checksums match

247
00:08:41,200 --> 00:08:45,040
in the best scenario akifi writes

248
00:08:43,279 --> 00:08:47,600
latency is four rtts with

249
00:08:45,040 --> 00:08:50,079
one checksum calculation time a kifi

250
00:08:47,600 --> 00:08:51,920
reads latencies one rtt and one checksum

251
00:08:50,080 --> 00:08:53,839
calculation time

252
00:08:51,920 --> 00:08:54,959
there are several drawbacks of pdbn

253
00:08:53,839 --> 00:08:57,360
directs

254
00:08:54,959 --> 00:08:59,279
writes are too slow reading and writing

255
00:08:57,360 --> 00:09:01,279
large data entries are slow because of

256
00:08:59,279 --> 00:09:03,439
checks on calculation time

257
00:09:01,279 --> 00:09:07,120
and because of writerlock it has poor

258
00:09:03,440 --> 00:09:09,600
scalability on the heavy contention

259
00:09:07,120 --> 00:09:11,360
pdpn directs shortcomings are due to its

260
00:09:09,600 --> 00:09:12,399
distributed data and control planet

261
00:09:11,360 --> 00:09:15,440
nature

262
00:09:12,399 --> 00:09:16,399
in pdpn central we add a global

263
00:09:15,440 --> 00:09:19,440
coordinator

264
00:09:16,399 --> 00:09:22,320
to manage and serialize excesses let's

265
00:09:19,440 --> 00:09:23,760
see if this could help

266
00:09:22,320 --> 00:09:25,440
the central coordinator is a

267
00:09:23,760 --> 00:09:27,680
full-fledged server that

268
00:09:25,440 --> 00:09:29,360
manages dns space and the serialized

269
00:09:27,680 --> 00:09:31,359
data accesses

270
00:09:29,360 --> 00:09:33,120
cns communicated with the coordinator

271
00:09:31,360 --> 00:09:35,120
with two-sided rdma

272
00:09:33,120 --> 00:09:37,120
the coordinator communicates with vms

273
00:09:35,120 --> 00:09:39,600
with one-sided rdma

274
00:09:37,120 --> 00:09:42,000
with such a centralized coordinator it's

275
00:09:39,600 --> 00:09:45,519
much easier to manage dns spaces than

276
00:09:42,000 --> 00:09:45,519
according to concurrent excesses

277
00:09:45,760 --> 00:09:50,000
to leverage the centralized architecture

278
00:09:48,720 --> 00:09:52,000
we design pdp

279
00:09:50,000 --> 00:09:53,279
essential by putting all metadata and

280
00:09:52,000 --> 00:09:55,920
control logic at

281
00:09:53,279 --> 00:09:57,200
that coordinator to avoid distributed

282
00:09:55,920 --> 00:09:59,680
coordination

283
00:09:57,200 --> 00:10:01,440
for example the coordinator is the only

284
00:09:59,680 --> 00:10:04,319
party that allocates

285
00:10:01,440 --> 00:10:05,360
and deallocates audience spaces it

286
00:10:04,320 --> 00:10:07,360
maintains

287
00:10:05,360 --> 00:10:09,680
a kivy entry location mapping table

288
00:10:07,360 --> 00:10:10,959
locally which contains all the metadata

289
00:10:09,680 --> 00:10:13,519
information

290
00:10:10,959 --> 00:10:15,439
each entry in this table has a pointer

291
00:10:13,519 --> 00:10:16,560
pointing to the physical location of the

292
00:10:15,440 --> 00:10:20,079
entry

293
00:10:16,560 --> 00:10:22,319
we also use a lock local per entry lock

294
00:10:20,079 --> 00:10:23,199
only for synchronization across multiple

295
00:10:22,320 --> 00:10:26,959
threads

296
00:10:23,200 --> 00:10:30,560
running at a coordinator to write

297
00:10:26,959 --> 00:10:33,359
a kv entry a cn sends a rpc request

298
00:10:30,560 --> 00:10:35,599
along with new data to the coordinator

299
00:10:33,360 --> 00:10:37,600
after receiving a write request

300
00:10:35,600 --> 00:10:39,920
the coordinator allocates a new space

301
00:10:37,600 --> 00:10:42,160
for it

302
00:10:39,920 --> 00:10:43,120
we perform out of place right for crash

303
00:10:42,160 --> 00:10:45,680
recovery

304
00:10:43,120 --> 00:10:48,000
the coordinator then writes the new data

305
00:10:45,680 --> 00:10:50,640
to this new space

306
00:10:48,000 --> 00:10:52,880
then calling the logs the corresponding

307
00:10:50,640 --> 00:10:54,640
entry in its local mapping table

308
00:10:52,880 --> 00:10:57,439
and updates the pointer to the new

309
00:10:54,640 --> 00:11:00,000
location and releases the log

310
00:10:57,440 --> 00:11:02,560
finally the coordinate replies cs write

311
00:11:00,000 --> 00:11:05,360
rpc request

312
00:11:02,560 --> 00:11:07,279
to perform a read cn sends an rpc

313
00:11:05,360 --> 00:11:08,959
request to the coordinator

314
00:11:07,279 --> 00:11:11,360
the coordinator then locks the

315
00:11:08,959 --> 00:11:13,279
corresponding entry in the mapping table

316
00:11:11,360 --> 00:11:14,800
then reads the data from the location

317
00:11:13,279 --> 00:11:17,360
stored in the table

318
00:11:14,800 --> 00:11:19,680
and finally reply cn's read rpc with

319
00:11:17,360 --> 00:11:21,920
this data

320
00:11:19,680 --> 00:11:24,000
reads and writes in pdp and central

321
00:11:21,920 --> 00:11:27,519
always take two rdts

322
00:11:24,000 --> 00:11:28,959
however pdp essentials 2 rtt qvs read is

323
00:11:27,519 --> 00:11:32,000
slower

324
00:11:28,959 --> 00:11:34,160
than pd event directs more important

325
00:11:32,000 --> 00:11:37,040
the single coordinator cannot scale with

326
00:11:34,160 --> 00:11:39,360
cns or dns

327
00:11:37,040 --> 00:11:40,640
now i have worked through the direct and

328
00:11:39,360 --> 00:11:44,320
essential modes

329
00:11:40,640 --> 00:11:46,800
both have have pros and cons for direct

330
00:11:44,320 --> 00:11:49,519
full rtt right is slow and checks on

331
00:11:46,800 --> 00:11:52,560
calculation for large data is also slow

332
00:11:49,519 --> 00:11:55,279
for central reads include extra rtt

333
00:11:52,560 --> 00:11:56,000
and the coordinator cannot scale at high

334
00:11:55,279 --> 00:11:58,639
level

335
00:11:56,000 --> 00:11:59,600
direct uses distributed data and control

336
00:11:58,639 --> 00:12:01,360
planes

337
00:11:59,600 --> 00:12:03,040
central uses centralized data and

338
00:12:01,360 --> 00:12:04,880
control planes

339
00:12:03,040 --> 00:12:07,519
to mitigate the drawbacks of these

340
00:12:04,880 --> 00:12:09,439
designs we propose to separate data and

341
00:12:07,519 --> 00:12:11,920
control planes

342
00:12:09,440 --> 00:12:12,480
now i will present our final pdpn

343
00:12:11,920 --> 00:12:16,719
product

344
00:12:12,480 --> 00:12:19,440
clever our main idea

345
00:12:16,720 --> 00:12:21,760
is to use a distributed data plane and

346
00:12:19,440 --> 00:12:24,399
the centralized metadata plane

347
00:12:21,760 --> 00:12:26,160
to cleanly separate these two planes we

348
00:12:24,399 --> 00:12:28,320
use different locations

349
00:12:26,160 --> 00:12:30,719
different communication methods and

350
00:12:28,320 --> 00:12:33,360
different management strategies for them

351
00:12:30,720 --> 00:12:33,920
for the data plane cns directly access

352
00:12:33,360 --> 00:12:36,959
dns

353
00:12:33,920 --> 00:12:39,360
use one study rdma for the control plane

354
00:12:36,959 --> 00:12:40,000
cn talk to one or field global manager

355
00:12:39,360 --> 00:12:43,040
servers

356
00:12:40,000 --> 00:12:45,040
or ms with two-sided rdma

357
00:12:43,040 --> 00:12:47,920
we will first look at data plane and

358
00:12:45,040 --> 00:12:50,480
then the metadata plane

359
00:12:47,920 --> 00:12:51,360
the main design challenge of clover's

360
00:12:50,480 --> 00:12:54,560
data plan

361
00:12:51,360 --> 00:12:56,880
is similar to pdpn directs how to

362
00:12:54,560 --> 00:12:59,439
efficiently support concurrent data

363
00:12:56,880 --> 00:13:01,519
accesses from cncdns

364
00:12:59,440 --> 00:13:04,560
but the key difference from direct is

365
00:13:01,519 --> 00:13:06,560
that clover could rely on its efficient

366
00:13:04,560 --> 00:13:07,680
metadata plane to perform data

367
00:13:06,560 --> 00:13:10,880
operations

368
00:13:07,680 --> 00:13:13,599
in a way that's not possible with direct

369
00:13:10,880 --> 00:13:15,360
remember that direct uses write log and

370
00:13:13,600 --> 00:13:18,079
checksum for each entry

371
00:13:15,360 --> 00:13:18,800
which causes variety rights this is

372
00:13:18,079 --> 00:13:21,839
because

373
00:13:18,800 --> 00:13:24,719
that synchronization is unavoidable to

374
00:13:21,839 --> 00:13:27,360
the fixed location of each entry

375
00:13:24,720 --> 00:13:28,800
instead in clever space allocation is

376
00:13:27,360 --> 00:13:30,800
more efficient

377
00:13:28,800 --> 00:13:33,199
therefore we design a lock-free

378
00:13:30,800 --> 00:13:35,359
out-of-place right mechanism

379
00:13:33,200 --> 00:13:37,200
we also introduced several optimizations

380
00:13:35,360 --> 00:13:41,680
to further reduce read and write out

381
00:13:37,200 --> 00:13:42,959
rtts now let's look at some details

382
00:13:41,680 --> 00:13:46,638
starting from the log free data

383
00:13:42,959 --> 00:13:49,680
structures each data engine at the dn

384
00:13:46,639 --> 00:13:52,880
is a chain of multiversion redo copies

385
00:13:49,680 --> 00:13:54,479
each keyv entry is a chain each version

386
00:13:52,880 --> 00:13:57,120
within the chain has a header

387
00:13:54,480 --> 00:13:58,720
and actual data the header has some

388
00:13:57,120 --> 00:14:00,560
metadata and a pointer

389
00:13:58,720 --> 00:14:01,760
pointing to the next version in the

390
00:14:00,560 --> 00:14:04,479
chain

391
00:14:01,760 --> 00:14:05,920
if the pointer is empty it means this

392
00:14:04,480 --> 00:14:08,800
version is the tail

393
00:14:05,920 --> 00:14:11,120
and the latest to quickly find the

394
00:14:08,800 --> 00:14:13,199
latest version of a kifi entry

395
00:14:11,120 --> 00:14:14,399
each cn has a simple data structure

396
00:14:13,199 --> 00:14:17,120
called a cursor

397
00:14:14,399 --> 00:14:17,600
that points to a version that this cn

398
00:14:17,120 --> 00:14:20,320
thinks

399
00:14:17,600 --> 00:14:21,519
is the latest version but this cursor

400
00:14:20,320 --> 00:14:25,040
may be outdated

401
00:14:21,519 --> 00:14:25,760
we will see that in a bit at a high

402
00:14:25,040 --> 00:14:28,639
level

403
00:14:25,760 --> 00:14:29,439
clubbers keep it right it is out of

404
00:14:28,639 --> 00:14:31,519
place

405
00:14:29,440 --> 00:14:33,600
it creates a new redo copy version which

406
00:14:31,519 --> 00:14:36,000
will be linked onto the chain

407
00:14:33,600 --> 00:14:37,440
let's say cn1 now wants to write a new

408
00:14:36,000 --> 00:14:40,160
data d1

409
00:14:37,440 --> 00:14:44,160
it first is used rdma write to write the

410
00:14:40,160 --> 00:14:47,519
new data to a new location in the dm

411
00:14:44,160 --> 00:14:50,480
since cn1's local cursor points to d0

412
00:14:47,519 --> 00:14:53,680
it will perform a rdma compare swap to

413
00:14:50,480 --> 00:14:57,120
atomically chain d1 onto d0

414
00:14:53,680 --> 00:15:00,399
locally c1 cn1 will also update its

415
00:14:57,120 --> 00:15:02,959
cursor to reflect this change

416
00:15:00,399 --> 00:15:04,480
now assume cn2 also wants to perform a

417
00:15:02,959 --> 00:15:06,638
qv writes

418
00:15:04,480 --> 00:15:08,560
similarly it will do an outdoor place

419
00:15:06,639 --> 00:15:10,800
right to d2

420
00:15:08,560 --> 00:15:13,199
but the cursor at the cn2 still points

421
00:15:10,800 --> 00:15:16,959
to d0 so cn2 still

422
00:15:13,199 --> 00:15:18,240
thinks d0 is the latest so cn2 will try

423
00:15:16,959 --> 00:15:21,839
to link d2

424
00:15:18,240 --> 00:15:23,600
onto d0 but this will fail since d0 is

425
00:15:21,839 --> 00:15:27,120
not the latest anymore

426
00:15:23,600 --> 00:15:30,240
after this failure cn2 will know that

427
00:15:27,120 --> 00:15:31,680
the latest now is d1 so it will try

428
00:15:30,240 --> 00:15:35,199
another component swap

429
00:15:31,680 --> 00:15:39,439
to link d2 onto d1

430
00:15:35,199 --> 00:15:42,639
similarly cn1 writes another d3

431
00:15:39,440 --> 00:15:44,160
in clover tv reads always returns the

432
00:15:42,639 --> 00:15:48,000
latest version

433
00:15:44,160 --> 00:15:50,560
in this particular case version d3

434
00:15:48,000 --> 00:15:52,639
recall that cursor points to the version

435
00:15:50,560 --> 00:15:56,079
that this particular cn thinks

436
00:15:52,639 --> 00:15:58,959
is the latest so if cn1

437
00:15:56,079 --> 00:16:00,800
wants to do qv read it will first try to

438
00:15:58,959 --> 00:16:03,839
read the version d1

439
00:16:00,800 --> 00:16:05,758
after receiving the data cn1 will check

440
00:16:03,839 --> 00:16:07,680
if this is the latest by checking

441
00:16:05,759 --> 00:16:10,800
whether the point is empty

442
00:16:07,680 --> 00:16:12,239
if not it will do chain work until it

443
00:16:10,800 --> 00:16:17,040
refines the latest version

444
00:16:12,240 --> 00:16:17,839
d3 in total it takes cl1's three rtts to

445
00:16:17,040 --> 00:16:20,639
reach the latest

446
00:16:17,839 --> 00:16:22,480
version the reason cn1 is through the

447
00:16:20,639 --> 00:16:24,000
chain work is that there are concurrent

448
00:16:22,480 --> 00:16:28,160
writers cn2

449
00:16:24,000 --> 00:16:28,160
which will make cn1's cursor outdated

450
00:16:28,560 --> 00:16:31,758
chain work might be very long when there

451
00:16:31,360 --> 00:16:34,560
are

452
00:16:31,759 --> 00:16:36,759
many heavy concurrent writers to

453
00:16:34,560 --> 00:16:39,599
mitigate this issue we introduced an

454
00:16:36,759 --> 00:16:42,720
optimization called shortcut read

455
00:16:39,600 --> 00:16:44,560
a technique inspired by skip lists we

456
00:16:42,720 --> 00:16:47,519
add a shortcut pointer in dn

457
00:16:44,560 --> 00:16:49,518
for each kv entry and this shortcut

458
00:16:47,519 --> 00:16:52,240
mostly points the latest version

459
00:16:49,519 --> 00:16:53,360
but not guaranteed short shortcut

460
00:16:52,240 --> 00:16:56,560
location is fixed

461
00:16:53,360 --> 00:16:59,279
so scenes know where to find them

462
00:16:56,560 --> 00:17:00,880
shortcut reader works as follows the cn

463
00:16:59,279 --> 00:17:03,040
will first read the shortcut pointer

464
00:17:00,880 --> 00:17:05,839
from the end and learn that

465
00:17:03,040 --> 00:17:07,359
three is the latest the cn issues

466
00:17:05,839 --> 00:17:10,079
another idea may read

467
00:17:07,359 --> 00:17:11,918
to read these three it's able to finish

468
00:17:10,079 --> 00:17:14,879
within two entities

469
00:17:11,919 --> 00:17:16,880
so now when cn wants to do kivy reads it

470
00:17:14,880 --> 00:17:18,000
not only issues the normal chain work

471
00:17:16,880 --> 00:17:20,000
read procedure

472
00:17:18,000 --> 00:17:21,599
but also performed a shortcut read at

473
00:17:20,000 --> 00:17:23,760
the same time

474
00:17:21,599 --> 00:17:25,918
the kv reader returns when a faster one

475
00:17:23,760 --> 00:17:28,559
finishes

476
00:17:25,919 --> 00:17:29,840
in clover when the contention is low

477
00:17:28,559 --> 00:17:34,639
keep it right is

478
00:17:29,840 --> 00:17:36,559
only two rtts and kv read is one rtt

479
00:17:34,640 --> 00:17:39,200
now let's look at clover's metadata

480
00:17:36,559 --> 00:17:42,559
plane the main design question

481
00:17:39,200 --> 00:17:45,520
here is how to provide low overhead

482
00:17:42,559 --> 00:17:46,240
scalable metadata service different from

483
00:17:45,520 --> 00:17:48,400
central

484
00:17:46,240 --> 00:17:50,480
clever centralized metadata plan is not

485
00:17:48,400 --> 00:17:52,559
in the middle of the data plane

486
00:17:50,480 --> 00:17:54,080
which brings both new opportunities and

487
00:17:52,559 --> 00:17:55,760
challenges

488
00:17:54,080 --> 00:17:57,600
we leverage the opportunity that

489
00:17:55,760 --> 00:17:58,720
metadata server is not in the middle of

490
00:17:57,600 --> 00:18:00,959
data access

491
00:17:58,720 --> 00:18:03,600
to further move all metadata operations

492
00:18:00,960 --> 00:18:05,600
of performance critical paths

493
00:18:03,600 --> 00:18:07,600
the challenge though is now we need to

494
00:18:05,600 --> 00:18:10,559
distribute method of decisions to

495
00:18:07,600 --> 00:18:11,360
cns we solve this challenge by batching

496
00:18:10,559 --> 00:18:14,320
operations

497
00:18:11,360 --> 00:18:16,080
and avoid caching validations the end

498
00:18:14,320 --> 00:18:17,600
result is that we have a clear

499
00:18:16,080 --> 00:18:20,879
separation between data

500
00:18:17,600 --> 00:18:24,159
and the metadata planes now let's

501
00:18:20,880 --> 00:18:27,679
look at some details mana data server or

502
00:18:24,160 --> 00:18:29,200
ms in clover performs space allocation

503
00:18:27,679 --> 00:18:30,799
garbage collection and global load

504
00:18:29,200 --> 00:18:33,919
balancing

505
00:18:30,799 --> 00:18:35,120
recall that clever needs a new space for

506
00:18:33,919 --> 00:18:37,120
every rights

507
00:18:35,120 --> 00:18:38,479
this requires very intensive space

508
00:18:37,120 --> 00:18:41,280
allocation

509
00:18:38,480 --> 00:18:41,520
so instead of asking ms for a newspaper

510
00:18:41,280 --> 00:18:45,120
at

511
00:18:41,520 --> 00:18:48,480
every right cnn acquires a bunch of new

512
00:18:45,120 --> 00:18:51,600
spaces at a time for ms

513
00:18:48,480 --> 00:18:55,679
ms keep a list of spaces and assigns

514
00:18:51,600 --> 00:18:57,840
them in a batch to cs

515
00:18:55,679 --> 00:18:59,280
calavera creates a new state of version

516
00:18:57,840 --> 00:19:00,559
for every write

517
00:18:59,280 --> 00:19:03,280
all the older version need to be

518
00:19:00,559 --> 00:19:05,120
recycled to move this recycling

519
00:19:03,280 --> 00:19:06,000
operation of the performance critical

520
00:19:05,120 --> 00:19:08,360
path

521
00:19:06,000 --> 00:19:09,520
we let cn send retire requests

522
00:19:08,360 --> 00:19:12,719
asynchronously

523
00:19:09,520 --> 00:19:14,400
and in a batch way to ms which in turn

524
00:19:12,720 --> 00:19:17,600
freedom into free list

525
00:19:14,400 --> 00:19:17,600
for future allocations

526
00:19:18,640 --> 00:19:21,760
clover also ensures data durability for

527
00:19:21,360 --> 00:19:25,120
both

528
00:19:21,760 --> 00:19:26,640
data and many data planes to provide

529
00:19:25,120 --> 00:19:28,639
better durability

530
00:19:26,640 --> 00:19:30,160
clover build a chaining replication

531
00:19:28,640 --> 00:19:32,720
mechanism in which

532
00:19:30,160 --> 00:19:33,440
clover links the version to all the

533
00:19:32,720 --> 00:19:37,520
replicas

534
00:19:33,440 --> 00:19:40,960
of the next version it works as follows

535
00:19:37,520 --> 00:19:43,360
please check out the paper for details

536
00:19:40,960 --> 00:19:44,080
clever leverage this distributed copies

537
00:19:43,360 --> 00:19:47,439
to balance

538
00:19:44,080 --> 00:19:48,000
loads both ms and the cn are able to

539
00:19:47,440 --> 00:19:51,200
carry out

540
00:19:48,000 --> 00:19:53,360
load balancing policies in addition

541
00:19:51,200 --> 00:19:57,919
clover ensures metadata plane

542
00:19:53,360 --> 00:19:57,918
reliability by using shadow ms surface

543
00:19:58,320 --> 00:20:02,158
coming back now we have covered all

544
00:20:00,799 --> 00:20:04,799
three types

545
00:20:02,159 --> 00:20:06,400
for clover thanks to its separate data

546
00:20:04,799 --> 00:20:07,918
and manage the plane design

547
00:20:06,400 --> 00:20:10,400
it can offer good read and write

548
00:20:07,919 --> 00:20:11,679
performance and can also scale with

549
00:20:10,400 --> 00:20:14,880
large number of

550
00:20:11,679 --> 00:20:14,880
cs and vms

551
00:20:16,080 --> 00:20:20,639
in order to understand pdpn systems and

552
00:20:19,039 --> 00:20:23,760
how it compares to active

553
00:20:20,640 --> 00:20:27,760
dpm we carry out an extensive set of

554
00:20:23,760 --> 00:20:30,240
experiments we run three pdpn systems

555
00:20:27,760 --> 00:20:31,520
pdp and direct pdp and central and

556
00:20:30,240 --> 00:20:34,480
clover

557
00:20:31,520 --> 00:20:36,320
we run two non-disaggregated systems

558
00:20:34,480 --> 00:20:39,120
octpers and hotpots

559
00:20:36,320 --> 00:20:41,760
and the two-sided qvs called hurt we

560
00:20:39,120 --> 00:20:45,439
also brought her to bluefieldsmanic

561
00:20:41,760 --> 00:20:48,559
both of them serve as active dpn systems

562
00:20:45,440 --> 00:20:51,200
we use a class of 14 machines each one

563
00:20:48,559 --> 00:20:54,879
with 100 gbps rdm magnetic

564
00:20:51,200 --> 00:20:56,400
connected via a 100 gbps ib switch

565
00:20:54,880 --> 00:20:58,000
we were not able to get hold of

566
00:20:56,400 --> 00:20:59,679
interrupt persistent memory

567
00:20:58,000 --> 00:21:02,960
therefore use a dram to emulate

568
00:20:59,679 --> 00:21:05,760
persistent memory performance

569
00:21:02,960 --> 00:21:06,000
we first measure the latency of qv read

570
00:21:05,760 --> 00:21:09,280
and

571
00:21:06,000 --> 00:21:11,280
writes one cn synchronize the read

572
00:21:09,280 --> 00:21:14,399
writer keep entry

573
00:21:11,280 --> 00:21:16,158
our latency baseline is rdma verbs read

574
00:21:14,400 --> 00:21:19,120
and write

575
00:21:16,159 --> 00:21:20,000
clever's read is very close to raw verbs

576
00:21:19,120 --> 00:21:22,639
reads

577
00:21:20,000 --> 00:21:24,960
clever's writes is worse than reads

578
00:21:22,640 --> 00:21:27,120
because write has an extra rtt and some

579
00:21:24,960 --> 00:21:30,080
protocol processing time

580
00:21:27,120 --> 00:21:31,678
as expected p depends direct latency

581
00:21:30,080 --> 00:21:34,320
increases dramatically

582
00:21:31,679 --> 00:21:36,000
when request size increases at which

583
00:21:34,320 --> 00:21:38,080
point the checks on calculation time

584
00:21:36,000 --> 00:21:40,000
dominates

585
00:21:38,080 --> 00:21:42,240
central's read-write latency is always

586
00:21:40,000 --> 00:21:45,760
around two activities

587
00:21:42,240 --> 00:21:47,440
for rights hurt outperforms clever and

588
00:21:45,760 --> 00:21:49,760
other pdp assistance

589
00:21:47,440 --> 00:21:53,600
because two-sided communication allows

590
00:21:49,760 --> 00:21:56,480
it to complete the rights within one rtt

591
00:21:53,600 --> 00:21:59,439
herd blue field is worse than others due

592
00:21:56,480 --> 00:22:01,840
to low profile on costs

593
00:21:59,440 --> 00:22:04,480
overall clover has the best read write

594
00:22:01,840 --> 00:22:07,600
latency among three pd pen systems

595
00:22:04,480 --> 00:22:11,600
its read is close to raw dma right

596
00:22:07,600 --> 00:22:15,120
is around two times of raw dma

597
00:22:11,600 --> 00:22:16,719
we also run ycisb we run three pdpn

598
00:22:15,120 --> 00:22:20,239
systems and octopus

599
00:22:16,720 --> 00:22:23,679
a non-disaggregate system there are four

600
00:22:20,240 --> 00:22:25,600
cns and four students dns each cn

601
00:22:23,679 --> 00:22:28,640
performs one million requests

602
00:22:25,600 --> 00:22:31,840
on 100k kb entries following as if

603
00:22:28,640 --> 00:22:34,960
distribution all numbers aggregated

604
00:22:31,840 --> 00:22:37,678
the first three bars are direct central

605
00:22:34,960 --> 00:22:39,600
and clever overall clover is the best

606
00:22:37,679 --> 00:22:42,159
performing pd band system

607
00:22:39,600 --> 00:22:45,199
the left table shows the median average

608
00:22:42,159 --> 00:22:47,840
at the 99 percentile rtts of clever

609
00:22:45,200 --> 00:22:49,280
clever requires only one rtt for read

610
00:22:47,840 --> 00:22:52,399
most workloads

611
00:22:49,280 --> 00:22:53,280
and even for 50 rights clover only

612
00:22:52,400 --> 00:22:56,960
incurs

613
00:22:53,280 --> 00:23:00,399
six rtts at a 99 percentile tail

614
00:22:56,960 --> 00:23:01,200
and one rtt at the median octopus has

615
00:23:00,400 --> 00:23:04,240
much worse

616
00:23:01,200 --> 00:23:04,960
performance another 90 seconds the

617
00:23:04,240 --> 00:23:08,400
system

618
00:23:04,960 --> 00:23:11,440
hotpot has similar performance overall

619
00:23:08,400 --> 00:23:12,640
clever outperform non-disaggregated pm

620
00:23:11,440 --> 00:23:14,720
systems

621
00:23:12,640 --> 00:23:17,120
we also conduct experiments and

622
00:23:14,720 --> 00:23:20,720
analytics to demonstrate that

623
00:23:17,120 --> 00:23:22,239
clover's capex and opex are both lower

624
00:23:20,720 --> 00:23:26,159
than active desegregation

625
00:23:22,240 --> 00:23:28,000
and non-disaggregation in today's talk i

626
00:23:26,159 --> 00:23:29,679
reviewed the spectrum of the data center

627
00:23:28,000 --> 00:23:32,000
pm deployment model

628
00:23:29,679 --> 00:23:33,600
and took a deep dive into an unexplored

629
00:23:32,000 --> 00:23:35,360
research area called passive

630
00:23:33,600 --> 00:23:38,000
disagreement apm

631
00:23:35,360 --> 00:23:40,240
our results show that the pdpn offers

632
00:23:38,000 --> 00:23:42,320
deployment and cost benefits

633
00:23:40,240 --> 00:23:45,200
and it's totally possible to get optimal

634
00:23:42,320 --> 00:23:48,080
performance with the pdpi model

635
00:23:45,200 --> 00:23:49,760
but achieving it is hard and it may not

636
00:23:48,080 --> 00:23:51,520
always be possible

637
00:23:49,760 --> 00:23:53,200
as we always see with heavy write

638
00:23:51,520 --> 00:23:55,360
contention case

639
00:23:53,200 --> 00:23:56,400
the key to a good pdp and design is a

640
00:23:55,360 --> 00:23:58,879
clean separation of

641
00:23:56,400 --> 00:24:01,120
data and metadata planes and a cover

642
00:23:58,880 --> 00:24:03,200
demonstrates the various challenges and

643
00:24:01,120 --> 00:24:05,760
benefits in doing so

644
00:24:03,200 --> 00:24:07,600
we believe our findings of pdpn can also

645
00:24:05,760 --> 00:24:10,879
be applied to these aggregated drain

646
00:24:07,600 --> 00:24:13,279
systems and many future systems

647
00:24:10,880 --> 00:24:15,200
before i end my talk i want to share

648
00:24:13,279 --> 00:24:18,480
some of my final thoughts

649
00:24:15,200 --> 00:24:20,400
after this multi-year long project first

650
00:24:18,480 --> 00:24:22,559
as we discuss the trade-offs in both

651
00:24:20,400 --> 00:24:24,720
active and the passive disaggregation

652
00:24:22,559 --> 00:24:26,799
we believe that the best way going

653
00:24:24,720 --> 00:24:28,720
forward is a hybrid model

654
00:24:26,799 --> 00:24:30,639
for example by overlying some

655
00:24:28,720 --> 00:24:32,320
computation to low cost

656
00:24:30,640 --> 00:24:34,320
programmable disaggregated hardware

657
00:24:32,320 --> 00:24:36,720
devices

658
00:24:34,320 --> 00:24:37,918
as for research efforts this work is

659
00:24:36,720 --> 00:24:40,559
unique in that

660
00:24:37,919 --> 00:24:42,400
it does not go with where the crowd goes

661
00:24:40,559 --> 00:24:44,840
doing so of course is risky

662
00:24:42,400 --> 00:24:47,360
but after all we find to be rewarding

663
00:24:44,840 --> 00:24:49,360
experience since we discovered many new

664
00:24:47,360 --> 00:24:51,600
possibilities and limitations

665
00:24:49,360 --> 00:24:53,600
and we also got inspiration from it

666
00:24:51,600 --> 00:24:55,840
along the way

667
00:24:53,600 --> 00:24:57,760
all our assistants are open source if

668
00:24:55,840 --> 00:24:58,559
you have any questions please reach out

669
00:24:57,760 --> 00:25:01,840
to us

670
00:24:58,559 --> 00:25:01,840
thank you for listening

