1
00:00:09,679 --> 00:00:13,200
hello

2
00:00:10,160 --> 00:00:16,000
everyone i'm nan and joe

3
00:00:13,200 --> 00:00:18,720
i'll present you hunter a flexible high

4
00:00:16,000 --> 00:00:22,240
performance deduplication framework

5
00:00:18,720 --> 00:00:25,119
for docker registries this work is done

6
00:00:22,240 --> 00:00:27,278
in collaboration with my fellow students

7
00:00:25,119 --> 00:00:30,000
hadiyou subview and current

8
00:00:27,279 --> 00:00:31,519
and colleagues from ibm research

9
00:00:30,000 --> 00:00:36,640
facility demetrius

10
00:00:31,519 --> 00:00:36,640
lucas anwar and my advisor ali

11
00:00:37,040 --> 00:00:40,719
over the years containers have become

12
00:00:39,440 --> 00:00:43,360
ubiquitous

13
00:00:40,719 --> 00:00:45,520
we've seen their use in supporting from

14
00:00:43,360 --> 00:00:48,879
operating system deployment

15
00:00:45,520 --> 00:00:51,840
to the platform for big data analysis

16
00:00:48,879 --> 00:00:52,718
running web servers caches as well and

17
00:00:51,840 --> 00:00:56,000
the new

18
00:00:52,719 --> 00:00:58,559
serverless paradigm containers

19
00:00:56,000 --> 00:01:00,399
are also seeing a lot of use in deep

20
00:00:58,559 --> 00:01:03,280
learning applications

21
00:01:00,399 --> 00:01:04,879
another way to look at the impact of

22
00:01:03,280 --> 00:01:06,880
container is that

23
00:01:04,879 --> 00:01:07,920
the market share they have gained

24
00:01:06,880 --> 00:01:10,240
recently

25
00:01:07,920 --> 00:01:11,680
continue already are a two billion

26
00:01:10,240 --> 00:01:14,640
dollar market

27
00:01:11,680 --> 00:01:14,880
driven by large scale containerization

28
00:01:14,640 --> 00:01:19,280
of

29
00:01:14,880 --> 00:01:21,839
existing business critical applications

30
00:01:19,280 --> 00:01:22,640
the trend is effected to grow at the

31
00:01:21,840 --> 00:01:26,560
rate of

32
00:01:22,640 --> 00:01:29,200
30 per year doubling by 2020

33
00:01:26,560 --> 00:01:30,320
so containers are popular and here to

34
00:01:29,200 --> 00:01:33,439
see

35
00:01:30,320 --> 00:01:35,119
this growth is reflected in the size of

36
00:01:33,439 --> 00:01:38,399
docker image data set

37
00:01:35,119 --> 00:01:41,640
for example if we go to dock hub webpage

38
00:01:38,400 --> 00:01:44,960
we see that it stores about 6

39
00:01:41,640 --> 00:01:48,000
3.6 million images

40
00:01:44,960 --> 00:01:49,280
repositories moreover just in the last

41
00:01:48,000 --> 00:01:52,720
15 months

42
00:01:49,280 --> 00:01:55,600
this number has grown by 40 percent

43
00:01:52,720 --> 00:01:56,240
all these images take a lot of storage

44
00:01:55,600 --> 00:01:59,360
space

45
00:01:56,240 --> 00:02:02,560
and cost if left unchecked

46
00:01:59,360 --> 00:02:05,520
this will not be sustainable

47
00:02:02,560 --> 00:02:06,719
the research challenge here is that can

48
00:02:05,520 --> 00:02:10,318
we efficiently

49
00:02:06,719 --> 00:02:12,720
manage this big big image data set

50
00:02:10,318 --> 00:02:14,879
and do that without the sacrificing

51
00:02:12,720 --> 00:02:17,599
performance

52
00:02:14,879 --> 00:02:20,160
to this end we've designed the duplanter

53
00:02:17,599 --> 00:02:20,799
which is a deduplication technique for

54
00:02:20,160 --> 00:02:24,000
images

55
00:02:20,800 --> 00:02:27,040
in docker registry we made

56
00:02:24,000 --> 00:02:27,840
two important findings and drywall

57
00:02:27,040 --> 00:02:30,400
design

58
00:02:27,840 --> 00:02:31,120
first container images have been

59
00:02:30,400 --> 00:02:34,160
observed

60
00:02:31,120 --> 00:02:36,720
to have a lot of redundancy for example

61
00:02:34,160 --> 00:02:38,640
there is a significant amount of data

62
00:02:36,720 --> 00:02:41,920
and is duplicated

63
00:02:38,640 --> 00:02:45,040
duplicated across layers

64
00:02:41,920 --> 00:02:48,480
second the way user success reduces

65
00:02:45,040 --> 00:02:51,280
the registry is predictable

66
00:02:48,480 --> 00:02:51,920
thus learning these access patterns can

67
00:02:51,280 --> 00:02:55,440
help us

68
00:02:51,920 --> 00:02:56,319
optimize the system we use these two of

69
00:02:55,440 --> 00:02:58,720
the visions

70
00:02:56,319 --> 00:02:59,760
to design dual hunter which does not

71
00:02:58,720 --> 00:03:02,959
only reduce

72
00:02:59,760 --> 00:03:05,599
layers but also reduces

73
00:03:02,959 --> 00:03:07,280
the year we store overhead our

74
00:03:05,599 --> 00:03:10,480
evaluation results through

75
00:03:07,280 --> 00:03:13,599
that in the highest data reduction mode

76
00:03:10,480 --> 00:03:15,518
200 can reduce storage space by up to

77
00:03:13,599 --> 00:03:18,159
6.9 times

78
00:03:15,519 --> 00:03:18,640
similarly in the highest performance

79
00:03:18,159 --> 00:03:20,720
mode

80
00:03:18,640 --> 00:03:21,679
the hunter can reduce the gateway

81
00:03:20,720 --> 00:03:25,200
latency by

82
00:03:21,680 --> 00:03:26,560
up to 2.8 times compared to the state of

83
00:03:25,200 --> 00:03:29,040
the art

84
00:03:26,560 --> 00:03:31,040
before i get into the details of tube

85
00:03:29,040 --> 00:03:32,959
hunter

86
00:03:31,040 --> 00:03:35,120
let me give you a quick overview of

87
00:03:32,959 --> 00:03:39,040
docker and docker registry

88
00:03:35,120 --> 00:03:41,440
docker container ex executable package

89
00:03:39,040 --> 00:03:44,159
that includes all the required

90
00:03:41,440 --> 00:03:47,760
components for software deployment

91
00:03:44,159 --> 00:03:49,760
and provides isolation among containers

92
00:03:47,760 --> 00:03:51,440
it's lightweight compared to virtual

93
00:03:49,760 --> 00:03:53,920
machine because all the

94
00:03:51,440 --> 00:03:55,519
containers running on the sim server

95
00:03:53,920 --> 00:03:58,839
share the same kernel

96
00:03:55,519 --> 00:04:00,000
logo containers are created from docker

97
00:03:58,840 --> 00:04:03,120
images

98
00:04:00,000 --> 00:04:03,840
a docker docker image contains a list of

99
00:04:03,120 --> 00:04:06,879
layers

100
00:04:03,840 --> 00:04:07,680
and an image manifest docker images are

101
00:04:06,879 --> 00:04:10,640
stored

102
00:04:07,680 --> 00:04:11,599
by docker registries such as docker hub

103
00:04:10,640 --> 00:04:15,760
which enables

104
00:04:11,599 --> 00:04:18,478
easy deployment and faster distribution

105
00:04:15,760 --> 00:04:19,839
docker client can pull an image from

106
00:04:18,478 --> 00:04:22,880
docker

107
00:04:19,839 --> 00:04:26,638
registry wrong docker image as

108
00:04:22,880 --> 00:04:28,719
a container instance build a new image

109
00:04:26,639 --> 00:04:31,040
and post the new image to docker

110
00:04:28,720 --> 00:04:33,840
registry

111
00:04:31,040 --> 00:04:34,240
one of the key observations we make is

112
00:04:33,840 --> 00:04:37,280
that

113
00:04:34,240 --> 00:04:41,280
there are significant redundant files

114
00:04:37,280 --> 00:04:41,679
in the image dataset an early study by

115
00:04:41,280 --> 00:04:45,280
us

116
00:04:41,680 --> 00:04:47,919
has shown that 97 of the files across

117
00:04:45,280 --> 00:04:48,799
layers are duplicates removing such

118
00:04:47,919 --> 00:04:53,198
duplicates

119
00:04:48,800 --> 00:04:56,160
can still have almost half of the space

120
00:04:53,199 --> 00:04:56,880
however existing techniques are unable

121
00:04:56,160 --> 00:05:00,400
to harness

122
00:04:56,880 --> 00:05:02,800
this redundancy this is because

123
00:05:00,400 --> 00:05:05,599
each layer is already compressed in

124
00:05:02,800 --> 00:05:09,039
docker registry image data set

125
00:05:05,600 --> 00:05:10,240
if we directly apply existing methods

126
00:05:09,039 --> 00:05:13,759
such as gd duke

127
00:05:10,240 --> 00:05:16,880
video which are fs dfs and cef

128
00:05:13,759 --> 00:05:18,560
on the compressed layer data set there's

129
00:05:16,880 --> 00:05:21,919
no space savings because

130
00:05:18,560 --> 00:05:25,120
compressed datasets are not duplicated

131
00:05:21,919 --> 00:05:29,520
well however instead

132
00:05:25,120 --> 00:05:32,000
if we decompress and unpack each layer

133
00:05:29,520 --> 00:05:32,639
and then apply different duplication

134
00:05:32,000 --> 00:05:35,680
method

135
00:05:32,639 --> 00:05:37,199
on the uncompressed layer d set we see

136
00:05:35,680 --> 00:05:41,840
that the existing method

137
00:05:37,199 --> 00:05:45,280
can significantly save storage space

138
00:05:41,840 --> 00:05:46,799
for example video can reduce space by up

139
00:05:45,280 --> 00:05:49,679
to four times

140
00:05:46,800 --> 00:05:51,280
however liar restore and overhead on the

141
00:05:49,680 --> 00:05:55,039
get layer request

142
00:05:51,280 --> 00:05:57,039
for example layer restore incurs 98

143
00:05:55,039 --> 00:06:00,159
times overhead for layer pulling

144
00:05:57,039 --> 00:06:00,159
requests for self

145
00:06:00,400 --> 00:06:05,039
our second number the reason that user

146
00:06:02,639 --> 00:06:07,919
access pattern is predictable

147
00:06:05,039 --> 00:06:08,400
we observe that when a client pulls an

148
00:06:07,919 --> 00:06:10,799
image

149
00:06:08,400 --> 00:06:11,758
it will first pull the manifest for the

150
00:06:10,800 --> 00:06:14,240
image

151
00:06:11,759 --> 00:06:15,360
which contains a list of its continued

152
00:06:14,240 --> 00:06:17,919
layers

153
00:06:15,360 --> 00:06:19,199
after that the client will pull layers

154
00:06:17,919 --> 00:06:22,318
from the image

155
00:06:19,199 --> 00:06:24,639
but on not all the layers in the image

156
00:06:22,319 --> 00:06:26,639
will be pulled

157
00:06:24,639 --> 00:06:29,199
to further understand the pulling

158
00:06:26,639 --> 00:06:32,080
pattern we studied the ibm

159
00:06:29,199 --> 00:06:34,000
cloud registry workload and span seven

160
00:06:32,080 --> 00:06:37,520
available zones and the duration

161
00:06:34,000 --> 00:06:39,039
of 7 75 days this graph shows the

162
00:06:37,520 --> 00:06:43,758
distribution of

163
00:06:39,039 --> 00:06:43,759
gately request count for the sim client

164
00:06:43,840 --> 00:06:52,080
we see that the majority of layers

165
00:06:46,880 --> 00:06:52,080
are only fetched once by the same kind

166
00:06:52,800 --> 00:06:57,840
next we observe the client reporting

167
00:06:55,599 --> 00:07:00,840
pattern from the client side

168
00:06:57,840 --> 00:07:02,239
this graph shows the cdf for client repo

169
00:07:00,840 --> 00:07:04,638
probability

170
00:07:02,240 --> 00:07:05,360
here we put probability in the

171
00:07:04,639 --> 00:07:08,880
likelihood

172
00:07:05,360 --> 00:07:09,759
for user to report an already stored

173
00:07:08,880 --> 00:07:12,400
layer

174
00:07:09,759 --> 00:07:15,039
and it's calculated as the number of

175
00:07:12,400 --> 00:07:18,080
reported layers divided by the number

176
00:07:15,039 --> 00:07:20,400
to get the request issued by the sim

177
00:07:18,080 --> 00:07:23,359
client

178
00:07:20,400 --> 00:07:23,679
we see that half of the clients have

179
00:07:23,360 --> 00:07:26,319
have

180
00:07:23,680 --> 00:07:27,840
a small report probability of less than

181
00:07:26,319 --> 00:07:30,639
0.2

182
00:07:27,840 --> 00:07:33,520
this means that many clients only poor

183
00:07:30,639 --> 00:07:33,520
linear ones

184
00:07:34,160 --> 00:07:40,319
we also observe that some kinds

185
00:07:37,360 --> 00:07:41,360
have a very high repo probability

186
00:07:40,319 --> 00:07:44,479
moreover

187
00:07:41,360 --> 00:07:44,960
we see that the slope of the cdf is

188
00:07:44,479 --> 00:07:48,318
steep

189
00:07:44,960 --> 00:07:51,599
at both lower and higher probabilities

190
00:07:48,319 --> 00:07:53,919
but become flat in the middle

191
00:07:51,599 --> 00:07:56,400
this suggests that by tracking user

192
00:07:53,919 --> 00:08:00,479
access history we can classify

193
00:07:56,400 --> 00:08:03,919
clients into two categories always pull

194
00:08:00,479 --> 00:08:05,758
clients and provide clients we'll get

195
00:08:03,919 --> 00:08:09,198
manifest requests arrived

196
00:08:05,759 --> 00:08:12,080
we can predict which layers in the image

197
00:08:09,199 --> 00:08:15,280
will be pulled

198
00:08:12,080 --> 00:08:16,240
the previous two observations in mind we

199
00:08:15,280 --> 00:08:19,039
explore if

200
00:08:16,240 --> 00:08:20,319
we can predict a potential gately

201
00:08:19,039 --> 00:08:23,039
request

202
00:08:20,319 --> 00:08:23,599
and pre-construct a layer upon receiving

203
00:08:23,039 --> 00:08:27,120
a get

204
00:08:23,599 --> 00:08:28,000
manifest request for example before the

205
00:08:27,120 --> 00:08:32,880
actually get

206
00:08:28,000 --> 00:08:36,399
leader request is even issued

207
00:08:32,880 --> 00:08:38,958
moreover we want to know

208
00:08:36,399 --> 00:08:40,559
if there is enough time between a get

209
00:08:38,958 --> 00:08:42,880
manifest request

210
00:08:40,559 --> 00:08:43,919
and a subsequent against leader request

211
00:08:42,880 --> 00:08:47,040
to do

212
00:08:43,919 --> 00:08:48,959
a layer pre-construction to answer this

213
00:08:47,040 --> 00:08:51,439
question we analyze the inter

214
00:08:48,959 --> 00:08:52,319
arrival time between a get manifest

215
00:08:51,440 --> 00:08:56,240
request

216
00:08:52,320 --> 00:08:58,160
and the subsequent get lea request

217
00:08:56,240 --> 00:09:00,240
we see that the majority of the

218
00:08:58,160 --> 00:09:02,399
intervals are greater than one second

219
00:09:00,240 --> 00:09:03,360
for example sixty percent of the

220
00:09:02,399 --> 00:09:05,360
intervals

221
00:09:03,360 --> 00:09:06,800
from sydney are greater than five

222
00:09:05,360 --> 00:09:09,200
seconds

223
00:09:06,800 --> 00:09:10,959
so layer pre-construction is possible

224
00:09:09,200 --> 00:09:14,560
and can significantly

225
00:09:10,959 --> 00:09:17,518
reduce lead restore overhead with this

226
00:09:14,560 --> 00:09:20,719
these observations let me now present

227
00:09:17,519 --> 00:09:23,120
the design of duplo hunter

228
00:09:20,720 --> 00:09:23,839
the main component of tube hunter are

229
00:09:23,120 --> 00:09:26,800
cluster

230
00:09:23,839 --> 00:09:29,440
storage servers and the distributed

231
00:09:26,800 --> 00:09:32,479
metadata database

232
00:09:29,440 --> 00:09:33,600
docker client can communicate with the

233
00:09:32,480 --> 00:09:36,320
docker

234
00:09:33,600 --> 00:09:37,120
tube hunter server using the registry

235
00:09:36,320 --> 00:09:40,560
api

236
00:09:37,120 --> 00:09:42,880
to upload and download layers

237
00:09:40,560 --> 00:09:44,000
each server in the cluster contains a

238
00:09:42,880 --> 00:09:46,640
banking storage

239
00:09:44,000 --> 00:09:48,320
system which stores layers and

240
00:09:46,640 --> 00:09:51,600
performance loop

241
00:09:48,320 --> 00:09:55,040
the duplication method

242
00:09:51,600 --> 00:09:55,040
is stored in the database

243
00:09:55,839 --> 00:10:01,680
we use three techniques to reduce the

244
00:09:59,279 --> 00:10:05,680
duplication and restoring overhead

245
00:10:01,680 --> 00:10:09,599
in duplo hunter namely replica

246
00:10:05,680 --> 00:10:12,560
duplication modes parallel layer

247
00:10:09,600 --> 00:10:13,600
reconstruction and pre-active layer

248
00:10:12,560 --> 00:10:16,560
prefetching and

249
00:10:13,600 --> 00:10:16,560
pre-instruction

250
00:10:17,040 --> 00:10:22,000
turbo hunter exploits existing

251
00:10:19,680 --> 00:10:22,880
replication approaches to improve

252
00:10:22,000 --> 00:10:26,160
performance

253
00:10:22,880 --> 00:10:28,959
it supports two modes basic

254
00:10:26,160 --> 00:10:29,680
deduplication modes denoted and b mode

255
00:10:28,959 --> 00:10:32,719
aim

256
00:10:29,680 --> 00:10:36,160
which only keeps n layer replicas

257
00:10:32,720 --> 00:10:39,760
intact and dedupe the rest

258
00:10:36,160 --> 00:10:42,880
r minus n layer replicas

259
00:10:39,760 --> 00:10:46,959
where r is the linear replica

260
00:10:42,880 --> 00:10:48,079
level replication level it also supports

261
00:10:46,959 --> 00:10:51,199
a selective

262
00:10:48,079 --> 00:10:54,239
deduplication mode denoted as s mode

263
00:10:51,200 --> 00:10:56,720
which dedupe rear accessed

264
00:10:54,240 --> 00:10:57,360
layers more aggressively than popular

265
00:10:56,720 --> 00:11:01,200
ones

266
00:10:57,360 --> 00:11:04,160
the goal here is to speed up accesses to

267
00:11:01,200 --> 00:11:06,480
popular layers and achieve higher

268
00:11:04,160 --> 00:11:10,079
storage savings

269
00:11:06,480 --> 00:11:12,720
here the storage cluster is divided into

270
00:11:10,079 --> 00:11:15,760
a primary cluster consisting of

271
00:11:12,720 --> 00:11:17,600
servers and the dual cluster consisting

272
00:11:15,760 --> 00:11:20,399
of these servers

273
00:11:17,600 --> 00:11:22,399
p server are responsible for storing

274
00:11:20,399 --> 00:11:25,600
four layer tables

275
00:11:22,399 --> 00:11:28,320
replicas and the replicas of

276
00:11:25,600 --> 00:11:28,800
manifest while these servers they do

277
00:11:28,320 --> 00:11:31,440
store

278
00:11:28,800 --> 00:11:34,160
and replicate the unique files from the

279
00:11:31,440 --> 00:11:34,160
layer table

280
00:11:35,040 --> 00:11:39,439
super hunter introduces the concept of a

281
00:11:38,160 --> 00:11:42,560
layer slice

282
00:11:39,440 --> 00:11:45,200
which is a collection of files on

283
00:11:42,560 --> 00:11:46,399
a single server belong to the sensor sim

284
00:11:45,200 --> 00:11:50,079
layer

285
00:11:46,399 --> 00:11:54,240
the slices are distributed evenly across

286
00:11:50,079 --> 00:11:58,000
the cluster using the slices

287
00:11:54,240 --> 00:12:00,079
allow hunter to rebuild a layer slice

288
00:11:58,000 --> 00:12:03,279
based on layer recipes

289
00:12:00,079 --> 00:12:07,680
in parallel and there by improve

290
00:12:03,279 --> 00:12:07,680
layer reconstruction performance

291
00:12:07,839 --> 00:12:14,079
to avoid the reconstruction latency

292
00:12:10,880 --> 00:12:17,360
during polling tuba hunter

293
00:12:14,079 --> 00:12:21,920
employs a cache layer in both

294
00:12:17,360 --> 00:12:25,200
the primary and dedupe cluster

295
00:12:21,920 --> 00:12:28,560
dual counter layer in the prefetch

296
00:12:25,200 --> 00:12:32,480
cache to avoid on disk

297
00:12:28,560 --> 00:12:35,680
disk ios on p servers

298
00:12:32,480 --> 00:12:37,839
additional additionally to reduce linear

299
00:12:35,680 --> 00:12:38,880
restoring overhead duplo hunter

300
00:12:37,839 --> 00:12:41,120
pre-constructs

301
00:12:38,880 --> 00:12:43,120
the layers and loads them in

302
00:12:41,120 --> 00:12:46,399
pre-construct cache

303
00:12:43,120 --> 00:12:50,000
and disk um

304
00:12:46,399 --> 00:12:52,560
and d servers the user access history

305
00:12:50,000 --> 00:12:55,600
and image information are received in

306
00:12:52,560 --> 00:12:55,599
metadata database

307
00:12:57,519 --> 00:13:01,200
to dedupe a layer the layer is first

308
00:13:00,320 --> 00:13:05,760
decompressed

309
00:13:01,200 --> 00:13:09,279
into a linear icon file then duplicate

310
00:13:05,760 --> 00:13:12,959
a file id by hashing the file content

311
00:13:09,279 --> 00:13:13,760
content and check if the id is already

312
00:13:12,959 --> 00:13:17,119
present

313
00:13:13,760 --> 00:13:17,839
in the file index if present the file

314
00:13:17,120 --> 00:13:20,639
content

315
00:13:17,839 --> 00:13:21,760
is discarded otherwise the file

316
00:13:20,639 --> 00:13:24,880
container will be

317
00:13:21,760 --> 00:13:27,040
saved as a unique file next

318
00:13:24,880 --> 00:13:29,439
the unique file will be evenly

319
00:13:27,040 --> 00:13:32,240
distributed and replicated

320
00:13:29,440 --> 00:13:36,560
to different servers to make sure that

321
00:13:32,240 --> 00:13:36,560
the layer slices are similar sized

322
00:13:36,720 --> 00:13:42,800
a slice recipe is also

323
00:13:39,839 --> 00:13:44,079
created which contains the information

324
00:13:42,800 --> 00:13:47,680
for restoring

325
00:13:44,079 --> 00:13:50,399
a layer slice after the

326
00:13:47,680 --> 00:13:50,959
later the duplication layer recipe is

327
00:13:50,399 --> 00:13:55,040
created

328
00:13:50,959 --> 00:13:55,279
for layer restoring the use of slices

329
00:13:55,040 --> 00:13:59,120
and

330
00:13:55,279 --> 00:14:02,320
recipes makes the layer restoring

331
00:13:59,120 --> 00:14:06,399
process is straightforward

332
00:14:02,320 --> 00:14:09,680
to reduce the to restore a layer

333
00:14:06,399 --> 00:14:13,120
we first rebuild the layer slices

334
00:14:09,680 --> 00:14:15,920
in parallel based on the slice recipes

335
00:14:13,120 --> 00:14:16,800
then we concatenate all layer slices

336
00:14:15,920 --> 00:14:19,439
together

337
00:14:16,800 --> 00:14:19,439
and the layer

338
00:14:20,240 --> 00:14:25,839
to accurately predict the layers

339
00:14:23,360 --> 00:14:27,199
that will be accessed in the future

340
00:14:25,839 --> 00:14:30,000
google hunter keeps

341
00:14:27,199 --> 00:14:31,279
track of the image method and the user

342
00:14:30,000 --> 00:14:34,399
access patterns

343
00:14:31,279 --> 00:14:38,160
in two data structures

344
00:14:34,399 --> 00:14:41,519
rl map maps an image to a scitini

345
00:14:38,160 --> 00:14:44,319
layer set ul map stores

346
00:14:41,519 --> 00:14:45,040
for each layer the layers the user has

347
00:14:44,320 --> 00:14:48,240
accessed

348
00:14:45,040 --> 00:14:49,279
and the corresponding pro count wheel

349
00:14:48,240 --> 00:14:52,959
get manifest

350
00:14:49,279 --> 00:14:55,920
request is received hunter first

351
00:14:52,959 --> 00:14:58,560
calculates a set of image layers that

352
00:14:55,920 --> 00:15:01,120
have have not been pulled by the user

353
00:14:58,560 --> 00:15:02,880
by calculating the difference between

354
00:15:01,120 --> 00:15:06,000
the images layer set

355
00:15:02,880 --> 00:15:06,639
and the user success layer set the

356
00:15:06,000 --> 00:15:09,839
layers

357
00:15:06,639 --> 00:15:13,279
in the difference will be accessed soon

358
00:15:09,839 --> 00:15:14,079
if the user is a repo user super hunter

359
00:15:13,279 --> 00:15:17,040
computes

360
00:15:14,079 --> 00:15:17,920
the subset of the years from the

361
00:15:17,040 --> 00:15:20,959
requested

362
00:15:17,920 --> 00:15:22,000
image that have already been pulled by

363
00:15:20,959 --> 00:15:24,560
the user

364
00:15:22,000 --> 00:15:25,600
the users in the intersection will be

365
00:15:24,560 --> 00:15:29,040
pulled soon

366
00:15:25,600 --> 00:15:31,360
for this user next we talk about

367
00:15:29,040 --> 00:15:33,360
how the better layer request is served

368
00:15:31,360 --> 00:15:36,720
in uberhunter

369
00:15:33,360 --> 00:15:37,440
first the request pass through the tiers

370
00:15:36,720 --> 00:15:40,560
from

371
00:15:37,440 --> 00:15:43,120
the from top to the bottom

372
00:15:40,560 --> 00:15:45,920
upon cat leader request duple hunter

373
00:15:43,120 --> 00:15:48,800
first to search the prefetch cache

374
00:15:45,920 --> 00:15:51,199
if the if the leader is present the

375
00:15:48,800 --> 00:15:54,639
request will be served from the cache

376
00:15:51,199 --> 00:15:57,920
otherwise the request will be served

377
00:15:54,639 --> 00:15:59,040
from the near stone if a gender leader

378
00:15:57,920 --> 00:16:01,839
request cannot

379
00:15:59,040 --> 00:16:02,639
be served from the primary cluster due

380
00:16:01,839 --> 00:16:05,199
to a lead

381
00:16:02,639 --> 00:16:06,560
due to a failure of the corresponding p

382
00:16:05,199 --> 00:16:09,040
servers

383
00:16:06,560 --> 00:16:10,800
the request will be forwarded to the new

384
00:16:09,040 --> 00:16:12,959
cluster

385
00:16:10,800 --> 00:16:16,160
super hunter will check whether the

386
00:16:12,959 --> 00:16:21,040
layer is in the

387
00:16:16,160 --> 00:16:25,360
stage area or the pre-construction cache

388
00:16:21,040 --> 00:16:28,240
otherwise it will rebuild the layer

389
00:16:25,360 --> 00:16:29,199
i have now described the design of dual

390
00:16:28,240 --> 00:16:32,720
hunter

391
00:16:29,199 --> 00:16:36,560
and its various techniques next

392
00:16:32,720 --> 00:16:39,120
we evaluate its efficacy

393
00:16:36,560 --> 00:16:41,439
we have evaluated the dual hunter using

394
00:16:39,120 --> 00:16:44,079
real world matrices

395
00:16:41,440 --> 00:16:45,040
from four ibm production registry

396
00:16:44,079 --> 00:16:49,439
clusters

397
00:16:45,040 --> 00:16:51,759
dallas frankfurt landing acting

398
00:16:49,440 --> 00:16:52,480
since there are no real layers in the

399
00:16:51,759 --> 00:16:55,600
traces

400
00:16:52,480 --> 00:16:58,639
we downloaded around one tv

401
00:16:55,600 --> 00:16:59,360
of popular docker images and match the

402
00:16:58,639 --> 00:17:02,959
requested

403
00:16:59,360 --> 00:17:06,480
layers in ibm traces with new layers

404
00:17:02,959 --> 00:17:10,879
downloaded from docker hub to generate

405
00:17:06,480 --> 00:17:12,400
the workload we evaluated the four

406
00:17:10,880 --> 00:17:15,600
cancel schemes

407
00:17:12,400 --> 00:17:18,160
the first one is a baseline scheme

408
00:17:15,599 --> 00:17:19,520
which does not perform the loop the

409
00:17:18,160 --> 00:17:23,120
second one is b mode

410
00:17:19,520 --> 00:17:26,799
in equal one two or three

411
00:17:23,119 --> 00:17:30,320
which means that in the replicas

412
00:17:26,799 --> 00:17:31,200
are preserved and three minus in earlier

413
00:17:30,320 --> 00:17:33,918
replicas

414
00:17:31,200 --> 00:17:33,919
are deduped

415
00:17:34,799 --> 00:17:38,799
the third one is s mode which means that

416
00:17:37,440 --> 00:17:41,919
the number of intact

417
00:17:38,799 --> 00:17:42,639
linear replicas is proportional to the

418
00:17:41,919 --> 00:17:46,240
leader's

419
00:17:42,640 --> 00:17:49,120
popularity the last one bemoan the zero

420
00:17:46,240 --> 00:17:50,080
which to do for all the layer graphics

421
00:17:49,120 --> 00:17:52,639
here

422
00:17:50,080 --> 00:17:53,360
we integrate the b mode zero with this

423
00:17:52,640 --> 00:17:56,559
different

424
00:17:53,360 --> 00:17:59,280
duplo methods at a redundancy scheme

425
00:17:56,559 --> 00:18:00,879
such as global file level duplication on

426
00:17:59,280 --> 00:18:04,240
the replication

427
00:18:00,880 --> 00:18:05,280
global global block global block level

428
00:18:04,240 --> 00:18:08,400
deduplication

429
00:18:05,280 --> 00:18:08,399
under rear code

430
00:18:09,039 --> 00:18:12,160
we first look at duplo hunter's

431
00:18:11,200 --> 00:18:14,640
end-to-end

432
00:18:12,160 --> 00:18:16,240
performance versus duplication visual

433
00:18:14,640 --> 00:18:20,160
for all the studies

434
00:18:16,240 --> 00:18:23,360
scenarios we see that all for

435
00:18:20,160 --> 00:18:27,200
all four schemes b mode 1

436
00:18:23,360 --> 00:18:29,600
2 and 3 and s mode have a better

437
00:18:27,200 --> 00:18:30,559
get layer performance compared to this

438
00:18:29,600 --> 00:18:33,840
line

439
00:18:30,559 --> 00:18:34,799
compared to the about four modes b mode

440
00:18:33,840 --> 00:18:37,760
0

441
00:18:34,799 --> 00:18:40,160
has the highest digital because all the

442
00:18:37,760 --> 00:18:43,760
layers are deduped

443
00:18:40,160 --> 00:18:46,559
and and we see the digital decreases

444
00:18:43,760 --> 00:18:48,160
with the number of year replicas

445
00:18:46,559 --> 00:18:50,639
preserved

446
00:18:48,160 --> 00:18:52,080
while the performance increase uh

447
00:18:50,640 --> 00:18:54,320
improves

448
00:18:52,080 --> 00:18:56,559
for the mode of the rule we see that

449
00:18:54,320 --> 00:18:57,520
with the global fire level deduplication

450
00:18:56,559 --> 00:19:01,678
in the use

451
00:18:57,520 --> 00:19:04,960
the denuclear ratio is increased to 2.1

452
00:19:01,679 --> 00:19:10,799
when block level deduplication is used

453
00:19:04,960 --> 00:19:14,400
the dedupe ratio increases to 3.0

454
00:19:10,799 --> 00:19:15,200
finally the dedupe ratio increases to

455
00:19:14,400 --> 00:19:18,160
six point

456
00:19:15,200 --> 00:19:20,320
nine if we use euro cooling steady

457
00:19:18,160 --> 00:19:24,080
replication

458
00:19:20,320 --> 00:19:26,879
however um

459
00:19:24,080 --> 00:19:27,678
we know that the performance degrees

460
00:19:26,880 --> 00:19:30,640
with the

461
00:19:27,679 --> 00:19:30,640
digital ratio

462
00:19:30,880 --> 00:19:34,640
next we look at the prefetch cache hit

463
00:19:33,360 --> 00:19:38,399
ratio

464
00:19:34,640 --> 00:19:40,880
we compare duplo hunters with them with

465
00:19:38,400 --> 00:19:42,400
the state-of-the-art registry preferred

466
00:19:40,880 --> 00:19:45,440
share with them

467
00:19:42,400 --> 00:19:48,240
nyu and erc can share with them

468
00:19:45,440 --> 00:19:49,600
we observe then the new hunter shows the

469
00:19:48,240 --> 00:19:52,960
highest

470
00:19:49,600 --> 00:19:54,480
hit ratio for example tube hunters hate

471
00:19:52,960 --> 00:19:58,720
ratio for sydney

472
00:19:54,480 --> 00:19:59,919
is 4.2 times higher than until the state

473
00:19:58,720 --> 00:20:02,400
of the art

474
00:19:59,919 --> 00:20:04,559
note that as shown in the paper the

475
00:20:02,400 --> 00:20:08,159
corresponding table latency also

476
00:20:04,559 --> 00:20:11,360
improved by thirteen percent

477
00:20:08,159 --> 00:20:15,360
so dupont can improve can

478
00:20:11,360 --> 00:20:18,719
provide higher heat ratio while reducing

479
00:20:15,360 --> 00:20:21,120
tail latency finally

480
00:20:18,720 --> 00:20:22,400
we examined the pre-construction cache

481
00:20:21,120 --> 00:20:25,199
hit ratio

482
00:20:22,400 --> 00:20:27,520
we see that the pre-construct cache hit

483
00:20:25,200 --> 00:20:31,440
ratio depending on the duplication

484
00:20:27,520 --> 00:20:34,158
methods and the show in this figure

485
00:20:31,440 --> 00:20:35,440
global file level deduplication under

486
00:20:34,159 --> 00:20:38,000
replication

487
00:20:35,440 --> 00:20:39,200
has the highest heat ratio

488
00:20:38,000 --> 00:20:41,440
correspondingly

489
00:20:39,200 --> 00:20:42,400
global fire level deduplication under

490
00:20:41,440 --> 00:20:45,200
replication

491
00:20:42,400 --> 00:20:46,240
also has the lowest weight and miss

492
00:20:45,200 --> 00:20:48,880
ratio

493
00:20:46,240 --> 00:20:50,320
this is because the lowest the the

494
00:20:48,880 --> 00:20:52,960
lowest

495
00:20:50,320 --> 00:20:54,799
it has the lowest restoring latency and

496
00:20:52,960 --> 00:20:58,000
the majority of the layers

497
00:20:54,799 --> 00:21:00,480
can be preconstructed on time

498
00:20:58,000 --> 00:21:03,840
for more detail analysis and results i

499
00:21:00,480 --> 00:21:03,840
refer you to the paper

500
00:21:04,159 --> 00:21:10,240
to conclude i have presented the design

501
00:21:07,679 --> 00:21:12,720
and evaluation of duplo hunter which

502
00:21:10,240 --> 00:21:14,880
provides efficient deduplication for

503
00:21:12,720 --> 00:21:18,240
local registries

504
00:21:14,880 --> 00:21:20,880
yup hunter exploits the redundancy in

505
00:21:18,240 --> 00:21:22,240
container images and predictable user

506
00:21:20,880 --> 00:21:25,440
access patterns

507
00:21:22,240 --> 00:21:28,880
to achieve high space savings with lower

508
00:21:25,440 --> 00:21:30,080
layer restore overhead our evaluation

509
00:21:28,880 --> 00:21:33,120
with real world

510
00:21:30,080 --> 00:21:35,918
reduced representative traces

511
00:21:33,120 --> 00:21:36,479
so that dual hunter reduces storage

512
00:21:35,919 --> 00:21:40,000
space

513
00:21:36,480 --> 00:21:43,200
usage by up to 6.9 times

514
00:21:40,000 --> 00:21:46,640
it can also reduce the galley latency

515
00:21:43,200 --> 00:21:49,919
up to 2.8 times compared to the

516
00:21:46,640 --> 00:21:53,120
state of the art finally tube hunter is

517
00:21:49,919 --> 00:21:56,000
available at this link

518
00:21:53,120 --> 00:21:57,039
thank you for listening if you have any

519
00:21:56,000 --> 00:22:11,360
questions

520
00:21:57,039 --> 00:22:11,360
please feel free to contact me

