1
00:00:09,920 --> 00:00:13,759
hello my name is junzu im

2
00:00:11,519 --> 00:00:16,240
i present our paper pinkie high speed

3
00:00:13,759 --> 00:00:20,000
installs key value store without details

4
00:00:16,239 --> 00:00:22,000
this is 10 mit collaborated on this work

5
00:00:20,000 --> 00:00:24,480
kibel store has become an essential

6
00:00:22,000 --> 00:00:26,960
infrastructure in many applications

7
00:00:24,480 --> 00:00:28,320
such as web indexing caching and storage

8
00:00:26,960 --> 00:00:30,160
systems

9
00:00:28,320 --> 00:00:32,238
many key value stores have been

10
00:00:30,160 --> 00:00:34,160
developed and the academia has

11
00:00:32,238 --> 00:00:36,160
researched key value store in various

12
00:00:34,160 --> 00:00:38,718
ways

13
00:00:36,160 --> 00:00:40,718
application that use key value stores

14
00:00:38,719 --> 00:00:42,079
are usually based on host system key

15
00:00:40,719 --> 00:00:44,879
value engines

16
00:00:42,079 --> 00:00:46,640
these engines use block-based ssds as

17
00:00:44,879 --> 00:00:48,800
the source device

18
00:00:46,640 --> 00:00:52,000
another approach is to use the key value

19
00:00:48,800 --> 00:00:54,399
ssd which is called kbssd

20
00:00:52,000 --> 00:00:56,840
kv-ssd is a new storage device that

21
00:00:54,399 --> 00:00:58,000
provides key value functionality in the

22
00:00:56,840 --> 00:01:00,160
ssd

23
00:00:58,000 --> 00:01:01,359
using this decreases the retention of

24
00:01:00,160 --> 00:01:03,199
the host system

25
00:01:01,359 --> 00:01:05,680
and produces high solid food with fewer

26
00:01:03,199 --> 00:01:07,840
host resources

27
00:01:05,680 --> 00:01:10,840
also it is easy to explore special

28
00:01:07,840 --> 00:01:12,560
resources such as capacitors and

29
00:01:10,840 --> 00:01:15,040
accelerators

30
00:01:12,560 --> 00:01:17,040
many research on kv ssds have been done

31
00:01:15,040 --> 00:01:18,799
in the academia

32
00:01:17,040 --> 00:01:22,080
industry like samsung are also

33
00:01:18,799 --> 00:01:24,000
interested in kvssds

34
00:01:22,080 --> 00:01:26,960
however there are two challenges in

35
00:01:24,000 --> 00:01:29,600
implementing kv ssds

36
00:01:26,960 --> 00:01:30,240
first of all the dram resources in the

37
00:01:29,600 --> 00:01:34,640
ssd

38
00:01:30,240 --> 00:01:37,039
is very limited tssd usually has dram as

39
00:01:34,640 --> 00:01:39,600
much as 0.1 percent of the total

40
00:01:37,040 --> 00:01:41,200
capacity for indexing

41
00:01:39,600 --> 00:01:43,280
the problem is that the indexing

42
00:01:41,200 --> 00:01:45,200
granularity is vertical byte which is

43
00:01:43,280 --> 00:01:47,600
bigger than the average key value size

44
00:01:45,200 --> 00:01:50,079
which is one kilowatt

45
00:01:47,600 --> 00:01:52,079
it means that the ssds dram capacity for

46
00:01:50,079 --> 00:01:55,279
four kilobyte indexing

47
00:01:52,079 --> 00:01:57,360
is not enough to index key value pairs

48
00:01:55,280 --> 00:02:01,680
furthermore since gram scale is lower

49
00:01:57,360 --> 00:02:05,360
than end this problem will become worse

50
00:02:01,680 --> 00:02:08,080
secondly cpu performance is limited

51
00:02:05,360 --> 00:02:10,478
to reduce power consumption ssds uses

52
00:02:08,080 --> 00:02:13,599
lower power processor like arm cpu

53
00:02:10,479 --> 00:02:15,680
instead of x86 cpus

54
00:02:13,599 --> 00:02:16,959
even these two limitations which

55
00:02:15,680 --> 00:02:22,080
algorithm is better for

56
00:02:16,959 --> 00:02:24,720
kb ssds hash or lsm3

57
00:02:22,080 --> 00:02:28,319
evaluate hash based kv-ssd we tested

58
00:02:24,720 --> 00:02:30,959
samsung's kv-ssd's prototype

59
00:02:28,319 --> 00:02:33,040
the 20 mark used is kb bench we sorry to

60
00:02:30,959 --> 00:02:34,000
bite key and one kilobyte value legally

61
00:02:33,040 --> 00:02:36,079
catch

62
00:02:34,000 --> 00:02:38,560
to compare the performance we also

63
00:02:36,080 --> 00:02:42,000
evaluated the blog ssd with a fio

64
00:02:38,560 --> 00:02:44,000
with one kilobyte lead details

65
00:02:42,000 --> 00:02:46,080
the blog ssd showed consistent

66
00:02:44,000 --> 00:02:48,160
performance no matter how much data are

67
00:02:46,080 --> 00:02:50,720
stored in the device

68
00:02:48,160 --> 00:02:52,640
however hbs key value ssd showed

69
00:02:50,720 --> 00:02:54,319
inconstant performance

70
00:02:52,640 --> 00:02:57,760
the more data store the lower the

71
00:02:54,319 --> 00:03:01,040
circuit and the slower the latency

72
00:02:57,760 --> 00:03:03,359
what causes this performance drop

73
00:03:01,040 --> 00:03:04,959
to figure out the codes we took a deeper

74
00:03:03,360 --> 00:03:07,519
look at the hash algorithm of the

75
00:03:04,959 --> 00:03:09,599
hedge-based kd ssd

76
00:03:07,519 --> 00:03:13,200
in the hash algorithm there are two ways

77
00:03:09,599 --> 00:03:15,679
of constituting the hash package

78
00:03:13,200 --> 00:03:18,560
one way is to populate an entry with a

79
00:03:15,680 --> 00:03:20,879
full key and a pointer to the value

80
00:03:18,560 --> 00:03:22,480
the other way is to use an entry with

81
00:03:20,879 --> 00:03:26,079
the signature of the free key

82
00:03:22,480 --> 00:03:28,560
and a pointer to the key value pair

83
00:03:26,080 --> 00:03:30,720
when the size of the ssd is 4 terabyte

84
00:03:28,560 --> 00:03:32,879
the dram size is 4 gigabyte

85
00:03:30,720 --> 00:03:34,239
when the device stores 32 byte key and

86
00:03:32,879 --> 00:03:37,079
one kilobyte value

87
00:03:34,239 --> 00:03:38,959
the total size of the hash buckets are

88
00:03:37,080 --> 00:03:42,400
144 gigabyte

89
00:03:38,959 --> 00:03:44,720
and 24 gigabyte respectively

90
00:03:42,400 --> 00:03:45,680
due to the huge size of hash buckets

91
00:03:44,720 --> 00:03:48,080
with full keys

92
00:03:45,680 --> 00:03:50,640
we use buckets to be signatures as many

93
00:03:48,080 --> 00:03:50,640
others do

94
00:03:51,280 --> 00:03:54,879
since the hash buckets are too large to

95
00:03:53,519 --> 00:03:57,120
be stored in the dram

96
00:03:54,879 --> 00:03:59,040
instead all the buckets are stored in

97
00:03:57,120 --> 00:04:00,799
the flash and some are cached in the

98
00:03:59,040 --> 00:04:03,519
dlam

99
00:04:00,799 --> 00:04:04,720
for example there is a gallery cast for

100
00:04:03,519 --> 00:04:07,599
k7

101
00:04:04,720 --> 00:04:09,439
its signature is 1000 and the bucket

102
00:04:07,599 --> 00:04:11,599
number is 10.

103
00:04:09,439 --> 00:04:12,560
however bucket number 10 is not in the

104
00:04:11,599 --> 00:04:16,000
cache

105
00:04:12,560 --> 00:04:19,040
so it is led from the flash you can get

106
00:04:16,000 --> 00:04:21,040
the value of q7 from the bucket entry

107
00:04:19,040 --> 00:04:22,560
however in some cases signature

108
00:04:21,040 --> 00:04:24,320
collision can occur

109
00:04:22,560 --> 00:04:26,160
where difference key is having the same

110
00:04:24,320 --> 00:04:28,479
signature

111
00:04:26,160 --> 00:04:30,000
to resolve this the hash has to prove

112
00:04:28,479 --> 00:04:31,919
other key value pairs

113
00:04:30,000 --> 00:04:34,240
which increases the number of flash

114
00:04:31,919 --> 00:04:36,159
accuracies

115
00:04:34,240 --> 00:04:38,800
the hash bucket list due to the cache

116
00:04:36,160 --> 00:04:40,960
misses make the performance drop

117
00:04:38,800 --> 00:04:43,840
and reserving signature collisions leads

118
00:04:40,960 --> 00:04:43,840
to long-term latency

119
00:04:44,320 --> 00:04:48,800
another approach is to use rsm3 since

120
00:04:47,440 --> 00:04:52,240
there sym3 is sorted

121
00:04:48,800 --> 00:04:55,120
structure it has lower deal requirement

122
00:04:52,240 --> 00:04:58,000
collisions do not occur and it is easy

123
00:04:55,120 --> 00:05:00,560
to serve with the inch quality

124
00:04:58,000 --> 00:05:02,720
however is the error cemetery really

125
00:05:00,560 --> 00:05:05,520
good enough

126
00:05:02,720 --> 00:05:08,320
the first problem of rsm3 based kv ssd

127
00:05:05,520 --> 00:05:10,719
is the long-term latency

128
00:05:08,320 --> 00:05:12,080
erasen3 adopts full computer to improve

129
00:05:10,720 --> 00:05:16,639
third gear performance

130
00:05:12,080 --> 00:05:19,758
in the level to retrieve a keyboard pair

131
00:05:16,639 --> 00:05:21,840
it first checks level 0. if the target

132
00:05:19,759 --> 00:05:24,960
key is not found in level 0

133
00:05:21,840 --> 00:05:27,119
it moves on to the next level before

134
00:05:24,960 --> 00:05:29,520
resulting the indices of level 1

135
00:05:27,120 --> 00:05:30,960
the existence of the target key in the

136
00:05:29,520 --> 00:05:34,159
level is checked by the

137
00:05:30,960 --> 00:05:35,359
computer if the key passes the

138
00:05:34,160 --> 00:05:38,160
membership checking

139
00:05:35,360 --> 00:05:40,320
the flash the flash is excessive to

140
00:05:38,160 --> 00:05:42,400
retrieve the value

141
00:05:40,320 --> 00:05:43,360
however even if the membership checking

142
00:05:42,400 --> 00:05:45,359
has been passed

143
00:05:43,360 --> 00:05:46,800
the level may not contain the target key

144
00:05:45,360 --> 00:05:49,199
value when a first positive

145
00:05:46,800 --> 00:05:49,199
occurs

146
00:05:49,840 --> 00:05:53,520
the same process is done for the

147
00:05:51,360 --> 00:05:55,919
subsequence levels until the target key

148
00:05:53,520 --> 00:05:58,000
is found

149
00:05:55,919 --> 00:06:00,318
therefore the worst case number of plus

150
00:05:58,000 --> 00:06:01,840
axis is to get a single key value pair

151
00:06:00,319 --> 00:06:03,680
equals to the height of the error

152
00:06:01,840 --> 00:06:06,880
symmetry

153
00:06:03,680 --> 00:06:09,360
the second problem is the cpu overhead

154
00:06:06,880 --> 00:06:11,919
the lsm3 performs moisture during the

155
00:06:09,360 --> 00:06:14,080
compactioning of two adjacent levels

156
00:06:11,919 --> 00:06:16,318
in this process a new brain filter is

157
00:06:14,080 --> 00:06:19,280
also constructed

158
00:06:16,319 --> 00:06:20,240
the ssd's arm cpu spends a lot of time

159
00:06:19,280 --> 00:06:22,000
insulting

160
00:06:20,240 --> 00:06:23,919
during compaction and building the

161
00:06:22,000 --> 00:06:27,120
volume filters

162
00:06:23,919 --> 00:06:28,318
the last problem is the io overhead lots

163
00:06:27,120 --> 00:06:31,440
of flash accesses

164
00:06:28,319 --> 00:06:33,680
occur during a compaction

165
00:06:31,440 --> 00:06:36,000
we avail you evaluated the error

166
00:06:33,680 --> 00:06:37,840
symmetry based kv ssd to verify the

167
00:06:36,000 --> 00:06:40,400
problems

168
00:06:37,840 --> 00:06:43,198
the tested kv-ssd is light store which

169
00:06:40,400 --> 00:06:45,198
was presented at asphalt 2019.

170
00:06:43,199 --> 00:06:48,319
right store is based on key value

171
00:06:45,199 --> 00:06:50,479
separated lsm3 and volume filter

172
00:06:48,319 --> 00:06:52,479
to make it clear we changed the problem

173
00:06:50,479 --> 00:06:56,080
filter to the state of the art volume

174
00:06:52,479 --> 00:06:59,039
filter suggested in monkey

175
00:06:56,080 --> 00:07:02,080
we use wishes load and hsbc benchmark

176
00:06:59,039 --> 00:07:05,680
for right and lead test respectively

177
00:07:02,080 --> 00:07:06,159
in my gsp load most of the ios account

178
00:07:05,680 --> 00:07:08,880
for

179
00:07:06,160 --> 00:07:10,639
the aerocenter's compaction process

180
00:07:08,880 --> 00:07:12,159
however when you break down the time

181
00:07:10,639 --> 00:07:14,840
taken for compaction

182
00:07:12,160 --> 00:07:16,000
the time to per compaction io is quite

183
00:07:14,840 --> 00:07:18,159
small

184
00:07:16,000 --> 00:07:20,000
the majority of the compaction time was

185
00:07:18,160 --> 00:07:21,680
taken up by the bloom filter building

186
00:07:20,000 --> 00:07:24,000
process

187
00:07:21,680 --> 00:07:26,240
also the time taken to sort during

188
00:07:24,000 --> 00:07:29,360
compaction is not small

189
00:07:26,240 --> 00:07:31,680
as the results show dlcm3 cpu overhead

190
00:07:29,360 --> 00:07:34,639
is problematic for kv ssds like

191
00:07:31,680 --> 00:07:37,520
compaction i overheard is

192
00:07:34,639 --> 00:07:39,440
when you look at the ratings of ycpc

193
00:07:37,520 --> 00:07:40,880
although right store is using the state

194
00:07:39,440 --> 00:07:42,840
of the art bloom filter

195
00:07:40,880 --> 00:07:44,719
we can see that it has a long-term

196
00:07:42,840 --> 00:07:46,960
latency

197
00:07:44,720 --> 00:07:48,800
the pink is a new era century-based

198
00:07:46,960 --> 00:07:52,560
kv-ssd which deserves

199
00:07:48,800 --> 00:07:54,080
problems that are mentioned previously

200
00:07:52,560 --> 00:07:56,479
the long-term latency problem is

201
00:07:54,080 --> 00:07:58,800
deserved by level pinning which is

202
00:07:56,479 --> 00:08:00,560
storing the top several levels of rsm3

203
00:07:58,800 --> 00:08:03,120
in the data

204
00:08:00,560 --> 00:08:04,560
the cpu overhead is reserved by removing

205
00:08:03,120 --> 00:08:06,639
the volume filter

206
00:08:04,560 --> 00:08:08,160
and adopting hardware accelerator for

207
00:08:06,639 --> 00:08:10,080
compaction

208
00:08:08,160 --> 00:08:12,080
the i o overhead can be dealt with the

209
00:08:10,080 --> 00:08:13,520
level pinning technique which can reduce

210
00:08:12,080 --> 00:08:16,318
compaction ios

211
00:08:13,520 --> 00:08:19,280
also pink further reduces the ios by

212
00:08:16,319 --> 00:08:22,639
re-inserting the valid data to the lsm3

213
00:08:19,280 --> 00:08:26,239
on ssd scopes collection

214
00:08:22,639 --> 00:08:28,720
now let's look into the details of pink

215
00:08:26,240 --> 00:08:31,680
the error symmetry used by pink is based

216
00:08:28,720 --> 00:08:34,640
on the key value separation technique

217
00:08:31,680 --> 00:08:36,479
all keys in the flash are stored as the

218
00:08:34,640 --> 00:08:38,399
form of a data structure called

219
00:08:36,479 --> 00:08:40,719
metasegment

220
00:08:38,399 --> 00:08:41,519
the entry of the meta segments consists

221
00:08:40,719 --> 00:08:43,599
of key

222
00:08:41,519 --> 00:08:45,760
and the pointer to the corresponding key

223
00:08:43,599 --> 00:08:47,920
value pair

224
00:08:45,760 --> 00:08:49,120
the set of keyboard pairs stored in

225
00:08:47,920 --> 00:08:52,319
other place area

226
00:08:49,120 --> 00:08:54,480
are called data segments

227
00:08:52,320 --> 00:08:57,279
the two types of segments are stored in

228
00:08:54,480 --> 00:08:59,040
the flash exclusively

229
00:08:57,279 --> 00:09:01,760
the level lists are maintained in the

230
00:08:59,040 --> 00:09:04,160
dram to index the meta segment

231
00:09:01,760 --> 00:09:08,160
level list exists for each level and

232
00:09:04,160 --> 00:09:10,160
also to or sorted arrays

233
00:09:08,160 --> 00:09:11,920
the entry-level level list has the start

234
00:09:10,160 --> 00:09:14,079
key of its beta segment

235
00:09:11,920 --> 00:09:16,959
and the address pointer which is address

236
00:09:14,080 --> 00:09:19,279
of the meta segment

237
00:09:16,959 --> 00:09:23,119
the level zero is a scheme list which

238
00:09:19,279 --> 00:09:24,959
store the keys and the values

239
00:09:23,120 --> 00:09:26,959
let's look at an example of the get

240
00:09:24,959 --> 00:09:28,719
operation in the lsm3

241
00:09:26,959 --> 00:09:31,119
suppose an error symmetry has five

242
00:09:28,720 --> 00:09:33,200
levels except for level zero

243
00:09:31,120 --> 00:09:37,120
the error symmetry has four level lists

244
00:09:33,200 --> 00:09:39,440
and four sets of meta segments

245
00:09:37,120 --> 00:09:41,279
to improve the gap performance it adopts

246
00:09:39,440 --> 00:09:42,320
the volunteer right conventional layer

247
00:09:41,279 --> 00:09:45,760
center tree

248
00:09:42,320 --> 00:09:48,320
then first of all

249
00:09:45,760 --> 00:09:49,439
check the existence of the key by using

250
00:09:48,320 --> 00:09:51,519
the volume beta

251
00:09:49,440 --> 00:09:54,320
then it finds the target address of the

252
00:09:51,519 --> 00:09:56,480
meta segments in the level list

253
00:09:54,320 --> 00:09:58,240
by reading the meta segments the address

254
00:09:56,480 --> 00:10:00,160
of the target keyboard pair could be

255
00:09:58,240 --> 00:10:02,720
retrieved

256
00:10:00,160 --> 00:10:04,000
middle level list will be many entries

257
00:10:02,720 --> 00:10:06,399
the meta segment which

258
00:10:04,000 --> 00:10:08,720
may hold the target key is found using

259
00:10:06,399 --> 00:10:11,040
binary search

260
00:10:08,720 --> 00:10:13,680
this process goes on until the target

261
00:10:11,040 --> 00:10:15,920
key value pair is found

262
00:10:13,680 --> 00:10:18,560
as mentioned in the introduction the

263
00:10:15,920 --> 00:10:21,199
flash needs to be accessible for all the

264
00:10:18,560 --> 00:10:22,640
all the levels of the rsm3 except level

265
00:10:21,200 --> 00:10:25,279
zero

266
00:10:22,640 --> 00:10:26,319
therefore pin k does not use volume

267
00:10:25,279 --> 00:10:28,640
filter

268
00:10:26,320 --> 00:10:29,440
instead pink uses level pinning

269
00:10:28,640 --> 00:10:31,519
technique

270
00:10:29,440 --> 00:10:33,600
which pins all the metal segments of the

271
00:10:31,519 --> 00:10:36,000
top several levels in the remaining dram

272
00:10:33,600 --> 00:10:36,000
space

273
00:10:36,560 --> 00:10:40,560
in this case the get operation accesses

274
00:10:39,200 --> 00:10:43,760
dlam later than flash

275
00:10:40,560 --> 00:10:45,518
when the level is pinned level pinning

276
00:10:43,760 --> 00:10:47,040
makes the worst case cost of get

277
00:10:45,519 --> 00:10:51,519
operation as one flash

278
00:10:47,040 --> 00:10:54,800
access some may think that it requires a

279
00:10:51,519 --> 00:10:56,959
lot of memory but it does not

280
00:10:54,800 --> 00:11:01,120
let's assume the same environment used

281
00:10:56,959 --> 00:11:03,040
in calculating the hash memory usage

282
00:11:01,120 --> 00:11:04,240
about 9 megabyte is assigned to ping

283
00:11:03,040 --> 00:11:07,599
case level 0

284
00:11:04,240 --> 00:11:10,640
to exploit the parallelism of flash

285
00:11:07,600 --> 00:11:12,160
when pink has 5 levels the total total

286
00:11:10,640 --> 00:11:16,160
memory usage of revised

287
00:11:12,160 --> 00:11:18,079
is 432 megabyte

288
00:11:16,160 --> 00:11:21,120
depending on the number of pins level

289
00:11:18,079 --> 00:11:24,079
the memory usage are like these

290
00:11:21,120 --> 00:11:26,800
to fit in the 4 gigabyte dram pin k pins

291
00:11:24,079 --> 00:11:30,399
the top 3 levels in dila

292
00:11:26,800 --> 00:11:33,439
in this case 3.5 gigabyte is required

293
00:11:30,399 --> 00:11:35,440
which is smaller than 4 gigabyte

294
00:11:33,440 --> 00:11:36,480
pin k can bound the read course to one

295
00:11:35,440 --> 00:11:40,640
parentheses access

296
00:11:36,480 --> 00:11:42,560
with less than 4 gigabyte memory usage

297
00:11:40,640 --> 00:11:45,279
the ping case get operation flow

298
00:11:42,560 --> 00:11:48,719
requires many binary search operations

299
00:11:45,279 --> 00:11:51,439
now we will calculate the search cost

300
00:11:48,720 --> 00:11:53,600
suppose there are h levels in delay in

301
00:11:51,440 --> 00:11:56,240
the ls3

302
00:11:53,600 --> 00:11:59,360
the size of the rebel increases by the t

303
00:11:56,240 --> 00:12:02,399
times as the level increases

304
00:11:59,360 --> 00:12:04,959
the search cost of this lsm3 is o h

305
00:12:02,399 --> 00:12:08,839
squared level t

306
00:12:04,959 --> 00:12:10,000
the mcpu considers this costs of

307
00:12:08,839 --> 00:12:12,720
burdensome

308
00:12:10,000 --> 00:12:15,519
optimize this pinky borrowed the idea

309
00:12:12,720 --> 00:12:18,399
from a fractional cascading

310
00:12:15,519 --> 00:12:20,079
the range pointer is added to each entry

311
00:12:18,399 --> 00:12:22,240
of the level list except for the last

312
00:12:20,079 --> 00:12:24,239
level

313
00:12:22,240 --> 00:12:27,120
with the range pointer the binary search

314
00:12:24,240 --> 00:12:29,279
of level 1 is same

315
00:12:27,120 --> 00:12:30,639
but from the next level by using the

316
00:12:29,279 --> 00:12:33,120
range pointer

317
00:12:30,639 --> 00:12:34,800
the range of search can be reduces as

318
00:12:33,120 --> 00:12:36,839
only the level list entries of the

319
00:12:34,800 --> 00:12:40,880
overall ping range

320
00:12:36,839 --> 00:12:44,720
consequently this reduces the search

321
00:12:40,880 --> 00:12:44,720
object to oh look t

322
00:12:45,279 --> 00:12:50,079
pink can reduce such over further by

323
00:12:47,680 --> 00:12:52,160
using prefix array

324
00:12:50,079 --> 00:12:54,399
before searching level list to find the

325
00:12:52,160 --> 00:12:55,360
full key the prefix array is searched

326
00:12:54,399 --> 00:12:57,279
first

327
00:12:55,360 --> 00:12:59,120
this makes the binary search cpu

328
00:12:57,279 --> 00:13:01,920
efficient

329
00:12:59,120 --> 00:13:04,959
if the two prefixes have the same value

330
00:13:01,920 --> 00:13:07,760
then the level list is searched

331
00:13:04,959 --> 00:13:12,079
these two optimizations only requires 10

332
00:13:07,760 --> 00:13:12,079
percent of memory used in level list

333
00:13:12,240 --> 00:13:16,560
another effect overall pinning is

334
00:13:14,079 --> 00:13:18,479
reducing compaction on your overhead

335
00:13:16,560 --> 00:13:20,560
first let's have a look at the

336
00:13:18,480 --> 00:13:22,959
compaction process when pink does not

337
00:13:20,560 --> 00:13:25,279
use level pinning

338
00:13:22,959 --> 00:13:27,518
when our level becomes full it leaves

339
00:13:25,279 --> 00:13:31,279
the metal segments of the full level

340
00:13:27,519 --> 00:13:33,040
and the level below after it source two

341
00:13:31,279 --> 00:13:35,200
sets of metal segments

342
00:13:33,040 --> 00:13:37,120
the resulting meta segments are written

343
00:13:35,200 --> 00:13:40,880
to flash

344
00:13:37,120 --> 00:13:43,199
and the level list is updated

345
00:13:40,880 --> 00:13:45,760
during this process it takes six reads

346
00:13:43,199 --> 00:13:48,800
and six slides

347
00:13:45,760 --> 00:13:50,800
let's consider the level pinning case

348
00:13:48,800 --> 00:13:52,000
since the target method segments already

349
00:13:50,800 --> 00:13:54,639
exist in the dram

350
00:13:52,000 --> 00:13:57,040
it just sorts the data and stores the

351
00:13:54,639 --> 00:13:59,120
new meta segments into dlam

352
00:13:57,040 --> 00:14:00,160
these newly made meta segments can be

353
00:13:59,120 --> 00:14:03,360
protected by

354
00:14:00,160 --> 00:14:05,600
capacitor in ssd

355
00:14:03,360 --> 00:14:08,639
after that the level list is updated and

356
00:14:05,600 --> 00:14:10,560
the compaction is finished

357
00:14:08,639 --> 00:14:12,880
the compaction process does not include

358
00:14:10,560 --> 00:14:15,839
any flash accesses

359
00:14:12,880 --> 00:14:18,240
however the cpu overhead of 13 still

360
00:14:15,839 --> 00:14:18,240
exist

361
00:14:18,639 --> 00:14:22,000
to reduce cpu time during compaction we

362
00:14:21,120 --> 00:14:26,240
implemented

363
00:14:22,000 --> 00:14:28,560
the sooting process with fpga

364
00:14:26,240 --> 00:14:30,240
pinking informs the accelerator the meta

365
00:14:28,560 --> 00:14:32,399
segments of the two levels

366
00:14:30,240 --> 00:14:35,120
and several flash addresses while the

367
00:14:32,399 --> 00:14:38,160
new meta segments will be stored

368
00:14:35,120 --> 00:14:41,040
the fpga leaves flash or dram by the

369
00:14:38,160 --> 00:14:42,800
address of the mometa segment

370
00:14:41,040 --> 00:14:44,800
and then source them by simple key

371
00:14:42,800 --> 00:14:47,040
comparison logics

372
00:14:44,800 --> 00:14:49,839
analyze newly created meta segments to

373
00:14:47,040 --> 00:14:52,319
dram or flash

374
00:14:49,839 --> 00:14:54,880
finally df-pga transfers the resulting

375
00:14:52,320 --> 00:14:57,279
level list back to pink to update the

376
00:14:54,880 --> 00:15:00,399
rsm3

377
00:14:57,279 --> 00:15:02,320
the pinkey we presented remembers the

378
00:15:00,399 --> 00:15:02,959
volume theater which has a lot of cpu

379
00:15:02,320 --> 00:15:06,240
overhead

380
00:15:02,959 --> 00:15:07,760
and causes long-term latency by using

381
00:15:06,240 --> 00:15:10,160
level pinning instead of

382
00:15:07,760 --> 00:15:14,000
bloompeter pinky can bound the tail

383
00:15:10,160 --> 00:15:16,079
latency and reduce compaction times

384
00:15:14,000 --> 00:15:17,760
also we optimize search overhead and

385
00:15:16,079 --> 00:15:18,479
adopts hardware accelerator for

386
00:15:17,760 --> 00:15:22,319
compaction

387
00:15:18,480 --> 00:15:24,160
to reduce cpu overhead please refer to

388
00:15:22,320 --> 00:15:28,320
the paper for the explanation on

389
00:15:24,160 --> 00:15:28,319
reducing the ios in garbage collection

390
00:15:29,120 --> 00:15:35,120
now i show the performance of pink

391
00:15:33,040 --> 00:15:38,319
all tested algorithms were implemented

392
00:15:35,120 --> 00:15:40,440
on jarlin's gcu-102 evaluation board

393
00:15:38,320 --> 00:15:43,040
this board has our processor to learn

394
00:15:40,440 --> 00:15:45,279
ki-v-ssd's algorithm and fpga to

395
00:15:43,040 --> 00:15:48,160
implement hardware module

396
00:15:45,279 --> 00:15:50,480
we attached a custom flash card with 200

397
00:15:48,160 --> 00:15:53,920
by 256 gigabyte of

398
00:15:50,480 --> 00:15:55,399
raw named chips the client server

399
00:15:53,920 --> 00:15:57,839
learning benchmark has these

400
00:15:55,399 --> 00:16:00,000
specifications

401
00:15:57,839 --> 00:16:00,959
all the requests will be sent to custom

402
00:16:00,000 --> 00:16:04,000
kv ssd

403
00:16:00,959 --> 00:16:06,638
by 10 gigabyte ethernet network

404
00:16:04,000 --> 00:16:08,959
for best for fast experiments we scale

405
00:16:06,639 --> 00:16:13,040
down the flash card and dram to 64

406
00:16:08,959 --> 00:16:15,279
gigabytes and 64 megabytes respectively

407
00:16:13,040 --> 00:16:16,399
we use ycsv's core workload for

408
00:16:15,279 --> 00:16:19,439
benchmarking

409
00:16:16,399 --> 00:16:21,920
the experiments are learned in two pages

410
00:16:19,440 --> 00:16:25,120
in load pace 40 per million uniquely

411
00:16:21,920 --> 00:16:27,759
london requests are put into the kvssd

412
00:16:25,120 --> 00:16:30,079
in long page kbssd process is the

413
00:16:27,759 --> 00:16:33,199
request generated by one of the provided

414
00:16:30,079 --> 00:16:33,199
wishes few acres

415
00:16:33,440 --> 00:16:38,720
we evaluated hash and lsm3 algorithm for

416
00:16:36,639 --> 00:16:41,600
comparing pink

417
00:16:38,720 --> 00:16:44,639
since samsung kv ssds is a black box we

418
00:16:41,600 --> 00:16:46,399
implemented a hash based kb ssd using ap

419
00:16:44,639 --> 00:16:48,560
signature key

420
00:16:46,399 --> 00:16:50,000
we use live store implementation for the

421
00:16:48,560 --> 00:16:52,959
conventional lsm3

422
00:16:50,000 --> 00:16:53,440
who has total five levels pinkey has

423
00:16:52,959 --> 00:16:55,279
total

424
00:16:53,440 --> 00:16:56,639
five levels and top three levels are

425
00:16:55,279 --> 00:16:59,920
pinned in dram

426
00:16:56,639 --> 00:17:02,000
thus only one level designs in flash

427
00:16:59,920 --> 00:17:04,079
we also tested the pk with hardware

428
00:17:02,000 --> 00:17:06,559
accelerator to check the effect of the

429
00:17:04,079 --> 00:17:09,678
accelerator

430
00:17:06,559 --> 00:17:11,599
all types of kv ssds use same 64

431
00:17:09,679 --> 00:17:14,240
megabyte data

432
00:17:11,599 --> 00:17:16,319
let's use the rd number cache lsm3

433
00:17:14,240 --> 00:17:17,679
assigned dlam to level lists and volume

434
00:17:16,319 --> 00:17:20,240
filters

435
00:17:17,679 --> 00:17:22,240
pink and pink hardware assigned dram to

436
00:17:20,240 --> 00:17:24,079
level list into including two peaks and

437
00:17:22,240 --> 00:17:26,000
language pointer

438
00:17:24,079 --> 00:17:28,319
the remaining dln was assigned to level

439
00:17:26,000 --> 00:17:28,319
fini

440
00:17:28,400 --> 00:17:32,880
let's analyze the throughput of each

441
00:17:30,240 --> 00:17:35,919
kbssd implementation

442
00:17:32,880 --> 00:17:38,080
pink hardware has 37 percent and 44

443
00:17:35,919 --> 00:17:39,520
percent improved performance than hash

444
00:17:38,080 --> 00:17:41,760
and error cemetery on average

445
00:17:39,520 --> 00:17:44,320
respectively

446
00:17:41,760 --> 00:17:46,000
since pink hardware removed bloomberg

447
00:17:44,320 --> 00:17:47,760
which is a burden to cpu

448
00:17:46,000 --> 00:17:49,960
and adopts hardware accelerator for

449
00:17:47,760 --> 00:17:52,640
compaction it has at most

450
00:17:49,960 --> 00:17:54,799
156 percent improved performance than

451
00:17:52,640 --> 00:17:57,919
rsm3

452
00:17:54,799 --> 00:17:59,760
also pink hardware has almost 21 percent

453
00:17:57,919 --> 00:18:03,760
improved performance than pinkey

454
00:17:59,760 --> 00:18:06,080
which does not use hardware accelerators

455
00:18:03,760 --> 00:18:08,640
in the case of workload c pink has a

456
00:18:06,080 --> 00:18:10,559
little bit lower performance than hash

457
00:18:08,640 --> 00:18:12,480
because pink does not have the cache

458
00:18:10,559 --> 00:18:16,559
structure it cannot utilize

459
00:18:12,480 --> 00:18:19,200
lead locality of walk road pink also

460
00:18:16,559 --> 00:18:21,678
has slightly lower performance than lsm3

461
00:18:19,200 --> 00:18:24,400
since lsm3 can skip binary search by

462
00:18:21,679 --> 00:18:24,400
volume peter

463
00:18:24,960 --> 00:18:28,000
on this slide we compare the latency

464
00:18:27,600 --> 00:18:32,000
character

465
00:18:28,000 --> 00:18:34,000
listing using ysb as she suffers from

466
00:18:32,000 --> 00:18:37,200
signature collisions it has the

467
00:18:34,000 --> 00:18:37,200
slowest latency

468
00:18:39,039 --> 00:18:42,559
although sim3 uses state-of-the-art

469
00:18:41,520 --> 00:18:46,400
monkey volunteer

470
00:18:42,559 --> 00:18:46,399
it still has a quite long built in

471
00:18:47,039 --> 00:18:51,679
however in the case of pink it shows the

472
00:18:49,360 --> 00:18:54,399
shortest latency since it can bound lead

473
00:18:51,679 --> 00:18:54,400
obturation

474
00:18:55,200 --> 00:18:59,280
this slide shows the effect of using

475
00:18:57,039 --> 00:19:03,120
double pinning the wrapped graph shows

476
00:18:59,280 --> 00:19:04,960
the number of flash leads per kodi

477
00:19:03,120 --> 00:19:06,879
head should access flash while

478
00:19:04,960 --> 00:19:07,840
processing set operation for checking

479
00:19:06,880 --> 00:19:09,679
collision

480
00:19:07,840 --> 00:19:13,280
thus pink has better literatures than

481
00:19:09,679 --> 00:19:13,280
hash in almost all cases

482
00:19:13,360 --> 00:19:17,600
for whiteship c which is literally

483
00:19:15,440 --> 00:19:19,600
workload hash excessive flesh

484
00:19:17,600 --> 00:19:22,320
slightly purer than pink due to the

485
00:19:19,600 --> 00:19:24,480
catchy structure

486
00:19:22,320 --> 00:19:27,280
but pinkey always perform get operation

487
00:19:24,480 --> 00:19:30,799
with fewer flesh excesses than nsm3 by

488
00:19:27,280 --> 00:19:32,559
level 20. the right grape illustrates

489
00:19:30,799 --> 00:19:36,720
the ratio of compaction io

490
00:19:32,559 --> 00:19:38,879
out of total value pink does not issue

491
00:19:36,720 --> 00:19:39,760
lead and lightly test to flash when it

492
00:19:38,880 --> 00:19:42,559
compacts pin

493
00:19:39,760 --> 00:19:45,440
levels so the ratio of pink is much

494
00:19:42,559 --> 00:19:47,200
smaller than nsm3

495
00:19:45,440 --> 00:19:49,440
to evaluate the effect of search

496
00:19:47,200 --> 00:19:52,960
optimization we compare the pink

497
00:19:49,440 --> 00:19:55,200
and unoptimized pink against the lsm3

498
00:19:52,960 --> 00:19:57,679
devices load and see you are used for

499
00:19:55,200 --> 00:20:00,400
this evaluation

500
00:19:57,679 --> 00:20:01,600
in the ycp load pink shows slightly

501
00:20:00,400 --> 00:20:03,840
lower throughput than on

502
00:20:01,600 --> 00:20:07,199
optimize pink due to the overhead for

503
00:20:03,840 --> 00:20:09,439
creating prefix and range pointer

504
00:20:07,200 --> 00:20:10,400
however in devices we see the read

505
00:20:09,440 --> 00:20:12,400
workload

506
00:20:10,400 --> 00:20:14,480
ping can have a lot of improvement than

507
00:20:12,400 --> 00:20:16,799
unoptimized one

508
00:20:14,480 --> 00:20:18,240
lsm3 can skip binary searches using

509
00:20:16,799 --> 00:20:20,000
bloom filters and thus

510
00:20:18,240 --> 00:20:22,640
it shows slightly better throughput than

511
00:20:20,000 --> 00:20:24,720
pink however volume filters can not

512
00:20:22,640 --> 00:20:28,080
bound the tail latency so pinky has

513
00:20:24,720 --> 00:20:28,080
better latency characteristic

514
00:20:28,880 --> 00:20:32,799
the lcm3 algorithm has good light

515
00:20:30,960 --> 00:20:35,039
performance in a total 3

516
00:20:32,799 --> 00:20:36,320
but the total lsm trend is more direct

517
00:20:35,039 --> 00:20:39,440
to bound leads to

518
00:20:36,320 --> 00:20:41,360
one place axis to another read the

519
00:20:39,440 --> 00:20:44,320
performance when dram is not enough to

520
00:20:41,360 --> 00:20:45,360
pin sufficient levels we evaluate lsm3

521
00:20:44,320 --> 00:20:47,439
and pin k

522
00:20:45,360 --> 00:20:49,840
as the number of level changing from

523
00:20:47,440 --> 00:20:52,559
four to eight

524
00:20:49,840 --> 00:20:53,600
rsm3 shows that two plush excesses is

525
00:20:52,559 --> 00:20:56,799
needed per

526
00:20:53,600 --> 00:20:59,360
quarry on average but the tail latency

527
00:20:56,799 --> 00:21:01,760
of rsm3 degrees more as increasing the

528
00:20:59,360 --> 00:21:03,760
total levels

529
00:21:01,760 --> 00:21:06,320
in the case of pinkey it needs more

530
00:21:03,760 --> 00:21:07,600
fleshy excesses per quarter than lsm3 on

531
00:21:06,320 --> 00:21:10,399
average

532
00:21:07,600 --> 00:21:13,120
but total latency of pink is almost or

533
00:21:10,400 --> 00:21:13,120
eliminated

534
00:21:13,440 --> 00:21:17,039
in conclusion the conventional kbssd's

535
00:21:16,400 --> 00:21:19,280
algorithm

536
00:21:17,039 --> 00:21:20,960
had performance degradation since they

537
00:21:19,280 --> 00:21:22,639
did not consider the limitation of

538
00:21:20,960 --> 00:21:25,440
embedded systems

539
00:21:22,640 --> 00:21:26,880
the pinkey we presented is a new lcm3

540
00:21:25,440 --> 00:21:29,440
based key value ssd

541
00:21:26,880 --> 00:21:30,559
which reserved the problems to reduce

542
00:21:29,440 --> 00:21:33,039
long-term latency

543
00:21:30,559 --> 00:21:35,440
pinky uses level pinning which spins kv

544
00:21:33,039 --> 00:21:37,280
indices of the top levels of era century

545
00:21:35,440 --> 00:21:39,600
india

546
00:21:37,280 --> 00:21:42,080
also pink adopts hardware accelerator

547
00:21:39,600 --> 00:21:45,039
for reducing cpu overhead of sorting

548
00:21:42,080 --> 00:21:47,520
in compaction additionally it suggests

549
00:21:45,039 --> 00:21:48,799
several optimization of lsm3 based kv

550
00:21:47,520 --> 00:21:52,320
ssd

551
00:21:48,799 --> 00:21:53,440
as a result pink shows the 73 percent

552
00:21:52,320 --> 00:21:56,158
and 42

553
00:21:53,440 --> 00:21:57,360
improved latency performance at 99

554
00:21:56,159 --> 00:22:01,039
percentile latency

555
00:21:57,360 --> 00:22:03,520
and average latency respectively

556
00:22:01,039 --> 00:22:07,280
also pin case throughput is 37 percent

557
00:22:03,520 --> 00:22:08,960
higher than conventional algorithms

558
00:22:07,280 --> 00:22:13,840
thank you for listening my talk please

559
00:22:08,960 --> 00:22:13,840
send me an email with any questions

