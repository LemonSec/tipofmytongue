1
00:00:10,320 --> 00:00:14,160
hello

2
00:00:10,880 --> 00:00:16,720
everyone my name is funjang i'm going to

3
00:00:14,160 --> 00:00:17,520
present our recent work on fine-grained

4
00:00:16,720 --> 00:00:20,720
window-based

5
00:00:17,520 --> 00:00:22,240
stream processing on cpu gpu integrated

6
00:00:20,720 --> 00:00:24,320
architectures

7
00:00:22,240 --> 00:00:25,759
this is our joint work of yemi

8
00:00:24,320 --> 00:00:28,640
university of china

9
00:00:25,760 --> 00:00:31,599
technical university berlin national

10
00:00:28,640 --> 00:00:34,800
university of singapore

11
00:00:31,599 --> 00:00:39,200
this is the outline of our presentation

12
00:00:34,800 --> 00:00:39,199
next i will introduce a background

13
00:00:41,200 --> 00:00:44,640
first let's look at two important

14
00:00:43,840 --> 00:00:47,920
concepts

15
00:00:44,640 --> 00:00:49,280
in string processing bulk synchronous

16
00:00:47,920 --> 00:00:52,879
parallel model

17
00:00:49,280 --> 00:00:54,800
and continuous operator model for bug

18
00:00:52,879 --> 00:00:58,640
synchronous parallel model

19
00:00:54,800 --> 00:01:01,199
it is conducted on the query granularity

20
00:00:58,640 --> 00:01:02,079
where the whole query with multiple

21
00:01:01,199 --> 00:01:04,879
operators

22
00:01:02,079 --> 00:01:05,840
on each batch of input data is

23
00:01:04,879 --> 00:01:09,680
dispatched

24
00:01:05,840 --> 00:01:14,000
on one device saba is an excellent work

25
00:01:09,680 --> 00:01:17,040
forensic mode 16 and take this model

26
00:01:14,000 --> 00:01:18,000
such a mechanism naturally minimizes the

27
00:01:17,040 --> 00:01:21,600
communication

28
00:01:18,000 --> 00:01:22,640
overhead among operators inside the same

29
00:01:21,600 --> 00:01:26,080
query

30
00:01:22,640 --> 00:01:27,600
it is hence suitable in discrete cpu gpu

31
00:01:26,080 --> 00:01:31,200
architectures

32
00:01:27,600 --> 00:01:32,720
their pcie overhead is significant and

33
00:01:31,200 --> 00:01:35,920
shall be avoided

34
00:01:32,720 --> 00:01:37,759
as much as possible for continuous

35
00:01:35,920 --> 00:01:40,400
operator model

36
00:01:37,759 --> 00:01:41,920
it is conducted on the operator

37
00:01:40,400 --> 00:01:44,799
granularity

38
00:01:41,920 --> 00:01:46,479
where each operator of a query can be

39
00:01:44,799 --> 00:01:51,200
independently placed

40
00:01:46,479 --> 00:01:51,520
at a device please note that cpu and gpu

41
00:01:51,200 --> 00:01:54,960
can

42
00:01:51,520 --> 00:01:57,520
concurrently execute in both cases

43
00:01:54,960 --> 00:01:58,798
only the granularity is different

44
00:01:57,520 --> 00:02:01,360
previous research

45
00:01:58,799 --> 00:02:02,000
mainly takes bulk synchronous parallel

46
00:02:01,360 --> 00:02:05,200
model

47
00:02:02,000 --> 00:02:05,920
however we find that continuous operator

48
00:02:05,200 --> 00:02:09,039
model

49
00:02:05,920 --> 00:02:11,520
is also useful and particularly

50
00:02:09,038 --> 00:02:14,399
it is very suitable for integrated

51
00:02:11,520 --> 00:02:14,400
architectures

52
00:02:15,200 --> 00:02:19,280
the integrated architecture usually

53
00:02:17,440 --> 00:02:21,920
consists of a cpu

54
00:02:19,280 --> 00:02:22,720
an accelerator such as gpu under shared

55
00:02:21,920 --> 00:02:25,839
memory

56
00:02:22,720 --> 00:02:28,879
in 2011 amd releases

57
00:02:25,840 --> 00:02:31,840
its integrated architecture called apu

58
00:02:28,879 --> 00:02:34,399
next intel nvidia also released similar

59
00:02:31,840 --> 00:02:36,720
integrated architectures

60
00:02:34,400 --> 00:02:37,920
compared to the discrete cpu gpu

61
00:02:36,720 --> 00:02:41,040
architecture

62
00:02:37,920 --> 00:02:41,920
both cpus and gpus are integrated on the

63
00:02:41,040 --> 00:02:43,920
same chip

64
00:02:41,920 --> 00:02:46,399
the most attractive feature of such

65
00:02:43,920 --> 00:02:49,200
integration is the shared main memory

66
00:02:46,400 --> 00:02:50,959
which is available to both devices the

67
00:02:49,200 --> 00:02:54,079
integrated architectures

68
00:02:50,959 --> 00:02:56,400
have no pcie transfer all ahead and have

69
00:02:54,080 --> 00:02:59,040
shared global memory and high energy

70
00:02:56,400 --> 00:02:59,040
efficiency

71
00:02:59,440 --> 00:03:04,879
we show a comparison between the

72
00:03:02,000 --> 00:03:06,000
integrated and discrete architectures in

73
00:03:04,879 --> 00:03:08,239
this table

74
00:03:06,000 --> 00:03:09,200
these architectures are used in our

75
00:03:08,239 --> 00:03:11,519
evaluation

76
00:03:09,200 --> 00:03:13,920
although the integrated architectures

77
00:03:11,519 --> 00:03:16,720
have low computation capacity

78
00:03:13,920 --> 00:03:18,720
than the discrete architecture currently

79
00:03:16,720 --> 00:03:20,080
the integrated architecture is a

80
00:03:18,720 --> 00:03:23,680
potential trend for

81
00:03:20,080 --> 00:03:27,840
future generation of processors moreover

82
00:03:23,680 --> 00:03:27,840
its price and power are usually low

83
00:03:28,480 --> 00:03:32,399
we consider supporting the basic sequel

84
00:03:31,200 --> 00:03:36,399
functions with

85
00:03:32,400 --> 00:03:39,200
screen precision as shown in the figure

86
00:03:36,400 --> 00:03:43,040
sql on screen precision consists of the

87
00:03:39,200 --> 00:03:46,000
following four major concepts

88
00:03:43,040 --> 00:03:47,120
data stream which is a sequence of

89
00:03:46,000 --> 00:03:49,920
tuples

90
00:03:47,120 --> 00:03:50,959
a tuple is a finite ordered list of

91
00:03:49,920 --> 00:03:54,879
elements

92
00:03:50,959 --> 00:03:58,159
and each tuple has a time step

93
00:03:54,879 --> 00:03:59,439
window which refers to a finite sequence

94
00:03:58,159 --> 00:04:02,319
of tuples

95
00:03:59,439 --> 00:04:03,680
which is a data unit to be processed in

96
00:04:02,319 --> 00:04:06,798
a query

97
00:04:03,680 --> 00:04:10,319
the window in stream has two features

98
00:04:06,799 --> 00:04:13,120
window size and window slide window size

99
00:04:10,319 --> 00:04:14,319
represents the size of the data unit to

100
00:04:13,120 --> 00:04:16,798
be processed

101
00:04:14,319 --> 00:04:17,599
and window slide denotes a sliding

102
00:04:16,798 --> 00:04:21,440
distance

103
00:04:17,600 --> 00:04:21,440
between two adjacent windows

104
00:04:21,680 --> 00:04:25,600
operators which has a minimum processing

105
00:04:24,960 --> 00:04:29,520
units

106
00:04:25,600 --> 00:04:32,400
for the data in a window in this work

107
00:04:29,520 --> 00:04:34,080
we support common relational operators

108
00:04:32,400 --> 00:04:37,359
including projection

109
00:04:34,080 --> 00:04:40,960
selection aggregation group by

110
00:04:37,360 --> 00:04:44,240
and join queries

111
00:04:40,960 --> 00:04:45,039
which are a form of data processing each

112
00:04:44,240 --> 00:04:48,000
of which

113
00:04:45,040 --> 00:04:51,600
consists of us at least one operator and

114
00:04:48,000 --> 00:04:51,600
the space down windows

115
00:04:52,080 --> 00:04:56,800
additionally note that in real stream

116
00:04:55,040 --> 00:05:00,639
processing system

117
00:04:56,800 --> 00:05:01,919
such as saba data are processed in batch

118
00:05:00,639 --> 00:05:05,039
granularity

119
00:05:01,919 --> 00:05:06,880
instead of window granularity a batch

120
00:05:05,039 --> 00:05:09,680
can be a group of windows

121
00:05:06,880 --> 00:05:10,639
when the window size is small or a part

122
00:05:09,680 --> 00:05:15,280
of a window

123
00:05:10,639 --> 00:05:15,280
when the window side is extremely large

124
00:05:17,039 --> 00:05:21,520
now let's look at the motivation

125
00:05:23,280 --> 00:05:27,039
we analyze the operators inquiry and

126
00:05:26,240 --> 00:05:29,840
find that

127
00:05:27,039 --> 00:05:30,880
different operators show various device

128
00:05:29,840 --> 00:05:34,560
preferences on

129
00:05:30,880 --> 00:05:37,039
integrated architectures some operators

130
00:05:34,560 --> 00:05:39,440
achieve higher performance on the cpu

131
00:05:37,039 --> 00:05:42,000
device and others have higher

132
00:05:39,440 --> 00:05:45,039
performance on the gpu device

133
00:05:42,000 --> 00:05:47,360
we use a simple query of group by and

134
00:05:45,039 --> 00:05:48,400
aggregation on the integrated

135
00:05:47,360 --> 00:05:52,960
architecture

136
00:05:48,400 --> 00:05:55,919
for illustration as shown in the slide

137
00:05:52,960 --> 00:05:56,638
the gpu queue is used to sequentially

138
00:05:55,919 --> 00:06:00,159
execute

139
00:05:56,639 --> 00:06:00,560
the queries on the gpu while the cpu

140
00:06:00,160 --> 00:06:03,600
queue

141
00:06:00,560 --> 00:06:05,280
is used to execute the related queries

142
00:06:03,600 --> 00:06:09,840
on the cpu

143
00:06:05,280 --> 00:06:13,840
the window size is 256

144
00:06:09,840 --> 00:06:16,880
tuples and the window slide is 16 4.

145
00:06:13,840 --> 00:06:20,239
each batch contains 64

146
00:06:16,880 --> 00:06:24,520
000 tuples and each tuple is

147
00:06:20,240 --> 00:06:25,759
certain tool bytes the input data are

148
00:06:24,520 --> 00:06:28,799
sinceritically

149
00:06:25,759 --> 00:06:29,840
generated which is described in our

150
00:06:28,800 --> 00:06:32,880
paper

151
00:06:29,840 --> 00:06:34,599
when the query runs on the cpu google

152
00:06:32,880 --> 00:06:38,000
buy takes about

153
00:06:34,600 --> 00:06:39,199
18.2 microseconds and the aggregation

154
00:06:38,000 --> 00:06:42,240
takes about

155
00:06:39,199 --> 00:06:45,680
5.2 microseconds

156
00:06:42,240 --> 00:06:50,080
however when the query runs on the gpu

157
00:06:45,680 --> 00:06:53,759
go by takes about 6.7 microseconds

158
00:06:50,080 --> 00:06:56,719
and the aggregation takes about 5.8

159
00:06:53,759 --> 00:06:56,720
microseconds

160
00:06:58,240 --> 00:07:03,599
we first evaluate the performance of

161
00:07:01,039 --> 00:07:06,800
operators on a single device

162
00:07:03,599 --> 00:07:10,080
in the table this table shows that

163
00:07:06,800 --> 00:07:10,880
using a single type of device cannot

164
00:07:10,080 --> 00:07:14,639
achieve the

165
00:07:10,880 --> 00:07:17,360
optimal performance for all operators

166
00:07:14,639 --> 00:07:17,680
the aggregation includes the operators

167
00:07:17,360 --> 00:07:21,120
of

168
00:07:17,680 --> 00:07:23,120
sum count and average and they have

169
00:07:21,120 --> 00:07:26,000
similar performance

170
00:07:23,120 --> 00:07:28,000
we use sum as a representative for

171
00:07:26,000 --> 00:07:30,880
aggregation

172
00:07:28,000 --> 00:07:31,919
from the table we can see that

173
00:07:30,880 --> 00:07:35,120
projection

174
00:07:31,919 --> 00:07:37,120
selection and group by achieve better

175
00:07:35,120 --> 00:07:40,319
performance on the gpus

176
00:07:37,120 --> 00:07:43,520
on the cpu vr aggregation and

177
00:07:40,319 --> 00:07:46,879
draw achieve better performance on the

178
00:07:43,520 --> 00:07:49,599
sound on the gpu additionally

179
00:07:46,879 --> 00:07:52,440
projection shows similar performance on

180
00:07:49,599 --> 00:07:55,840
cpu and gpu devices

181
00:07:52,440 --> 00:07:56,960
specifically for join the cpu

182
00:07:55,840 --> 00:08:00,318
performance is

183
00:07:56,960 --> 00:08:03,120
about 6 times the gpu performance

184
00:08:00,319 --> 00:08:04,160
such different device preferences

185
00:08:03,120 --> 00:08:07,199
inspire us

186
00:08:04,160 --> 00:08:12,720
to perform fine-grained screen precision

187
00:08:07,199 --> 00:08:15,840
on integrated architectures

188
00:08:12,720 --> 00:08:17,039
based on this analysis we propose our

189
00:08:15,840 --> 00:08:19,919
key idea

190
00:08:17,039 --> 00:08:22,080
fine-grained string precision a

191
00:08:19,919 --> 00:08:24,240
fine-grained string precise method

192
00:08:22,080 --> 00:08:26,639
that can consider both integrated

193
00:08:24,240 --> 00:08:29,199
architecture characteristics

194
00:08:26,639 --> 00:08:30,080
and operator features shall have better

195
00:08:29,199 --> 00:08:32,800
performance

196
00:08:30,080 --> 00:08:35,120
first corrun the operators that do not

197
00:08:32,799 --> 00:08:37,838
reach the memory bandwidth limit

198
00:08:35,120 --> 00:08:40,479
second put operators on the preferred

199
00:08:37,839 --> 00:08:42,959
devices that can bring more performance

200
00:08:40,479 --> 00:08:45,680
benefits

201
00:08:42,958 --> 00:08:48,319
however enabling fine-grained stream

202
00:08:45,680 --> 00:08:51,040
precision and integrated architectures

203
00:08:48,320 --> 00:08:52,320
is complicated by the features of sql

204
00:08:51,040 --> 00:08:54,800
stream precising

205
00:08:52,320 --> 00:08:55,440
and integrated architectures we

206
00:08:54,800 --> 00:08:59,599
summarize

207
00:08:55,440 --> 00:08:59,600
three major challenges as follows

208
00:08:59,839 --> 00:09:04,839
the first challenge is application

209
00:09:02,000 --> 00:09:06,800
topology combined with architectural

210
00:09:04,839 --> 00:09:09,040
characteristics

211
00:09:06,800 --> 00:09:10,160
application topology in string

212
00:09:09,040 --> 00:09:13,199
processing

213
00:09:10,160 --> 00:09:13,920
refers to the organization and execution

214
00:09:13,200 --> 00:09:17,200
order

215
00:09:13,920 --> 00:09:20,479
of the operators in a sql query

216
00:09:17,200 --> 00:09:23,839
first the relation among operators

217
00:09:20,480 --> 00:09:26,640
could be more complicated in practice

218
00:09:23,839 --> 00:09:27,360
the operators may be represented as a

219
00:09:26,640 --> 00:09:30,720
deck

220
00:09:27,360 --> 00:09:35,040
instead of a chain which contains more

221
00:09:30,720 --> 00:09:37,160
parallel acceleration opportunities

222
00:09:35,040 --> 00:09:39,360
second with architectural

223
00:09:37,160 --> 00:09:42,160
characteristics considered

224
00:09:39,360 --> 00:09:43,519
such as the cpu and gpu architectural

225
00:09:42,160 --> 00:09:46,160
differences

226
00:09:43,519 --> 00:09:47,760
the topology with computing resource

227
00:09:46,160 --> 00:09:52,000
distribution becomes

228
00:09:47,760 --> 00:09:54,399
very complex in such situations

229
00:09:52,000 --> 00:09:55,360
how to perform fine-grained operator

230
00:09:54,399 --> 00:09:58,320
placement

231
00:09:55,360 --> 00:09:59,279
for application topology on different

232
00:09:58,320 --> 00:10:02,959
devices

233
00:09:59,279 --> 00:10:05,839
of integrated architectures becomes a

234
00:10:02,959 --> 00:10:05,839
challenge

235
00:10:08,000 --> 00:10:15,519
the second challenge is sql query plan

236
00:10:11,279 --> 00:10:19,519
optimization with shared main memory

237
00:10:15,519 --> 00:10:22,800
first a sql query in stream precising

238
00:10:19,519 --> 00:10:25,279
can consist of many operators

239
00:10:22,800 --> 00:10:26,240
and the execution plan of these

240
00:10:25,279 --> 00:10:29,040
operators

241
00:10:26,240 --> 00:10:29,279
may cause different bandwidth pressures

242
00:10:29,040 --> 00:10:33,360
and

243
00:10:29,279 --> 00:10:36,800
device preferences second

244
00:10:33,360 --> 00:10:40,399
in many cases independent operators

245
00:10:36,800 --> 00:10:44,000
may not consume all the memory bandwidth

246
00:10:40,399 --> 00:10:47,760
but quaranism together could exceed

247
00:10:44,000 --> 00:10:49,120
the bandwidth limit we need to analysis

248
00:10:47,760 --> 00:10:53,279
the memory bandwidth

249
00:10:49,120 --> 00:10:56,320
requirement of qurani third

250
00:10:53,279 --> 00:10:56,880
cpus and gpus have different preferred

251
00:10:56,320 --> 00:11:00,480
memory

252
00:10:56,880 --> 00:11:01,519
access patterns current measures do not

253
00:11:00,480 --> 00:11:04,800
consider these

254
00:11:01,519 --> 00:11:08,800
complex situations of shared main memory

255
00:11:04,800 --> 00:11:08,800
in the integrated architectures

256
00:11:10,800 --> 00:11:14,160
the third challenge comes from the

257
00:11:13,279 --> 00:11:17,519
adjustment

258
00:11:14,160 --> 00:11:18,880
for dynamic workload during stream

259
00:11:17,519 --> 00:11:21,959
precision

260
00:11:18,880 --> 00:11:23,439
stream data are changing dynamically in

261
00:11:21,959 --> 00:11:26,839
distributions and

262
00:11:23,440 --> 00:11:31,680
arrival speeds which is challenging to

263
00:11:26,839 --> 00:11:34,720
adapt first workload change detection

264
00:11:31,680 --> 00:11:35,519
and computing resource adjustment need

265
00:11:34,720 --> 00:11:38,480
to be done

266
00:11:35,519 --> 00:11:41,360
in a lightweight manner and they are

267
00:11:38,480 --> 00:11:44,720
critical to performance

268
00:11:41,360 --> 00:11:45,760
second the query plan may also need to

269
00:11:44,720 --> 00:11:48,320
be updated

270
00:11:45,760 --> 00:11:49,040
adaptively because the operator

271
00:11:48,320 --> 00:11:52,639
placement

272
00:11:49,040 --> 00:11:55,120
strategy based on the initial state

273
00:11:52,639 --> 00:11:56,720
may not be suitable when the workload

274
00:11:55,120 --> 00:11:59,839
changes

275
00:11:56,720 --> 00:12:02,560
third during adaptation

276
00:11:59,839 --> 00:12:03,120
online stream processing needs to be

277
00:12:02,560 --> 00:12:06,160
served

278
00:12:03,120 --> 00:12:09,440
efficiently resource adjustment

279
00:12:06,160 --> 00:12:12,560
and query plan adaptation on the fly

280
00:12:09,440 --> 00:12:13,120
may incur run time overhead because we

281
00:12:12,560 --> 00:12:16,880
need to

282
00:12:13,120 --> 00:12:19,040
adjust not only the operators in the dac

283
00:12:16,880 --> 00:12:20,480
but also the hardware computing

284
00:12:19,040 --> 00:12:24,639
resources to each

285
00:12:20,480 --> 00:12:27,279
operator additionally the adjustment

286
00:12:24,639 --> 00:12:30,000
among different streams also needs to be

287
00:12:27,279 --> 00:12:30,000
considered

288
00:12:31,839 --> 00:12:39,839
now let's look at our solution

289
00:12:34,880 --> 00:12:39,839
called find stream

290
00:12:39,920 --> 00:12:45,519
we propose a framework called fun stream

291
00:12:43,040 --> 00:12:48,240
for fine-grained stream precision on

292
00:12:45,519 --> 00:12:51,040
integrated architectures

293
00:12:48,240 --> 00:12:52,880
the overview of fun stream is showing

294
00:12:51,040 --> 00:12:56,800
this slide

295
00:12:52,880 --> 00:12:59,519
the workflow of one stream is as follows

296
00:12:56,800 --> 00:13:00,560
when the engine starts it first

297
00:12:59,519 --> 00:13:04,079
processes

298
00:13:00,560 --> 00:13:07,760
several batteries using only the cpus

299
00:13:04,079 --> 00:13:11,599
or the gpus to gather useful data

300
00:13:07,760 --> 00:13:13,040
second based on this data it builds a

301
00:13:11,600 --> 00:13:15,680
performance model for

302
00:13:13,040 --> 00:13:17,519
operators of a query on different

303
00:13:15,680 --> 00:13:20,399
devices

304
00:13:17,519 --> 00:13:21,600
third after the performance model is

305
00:13:20,399 --> 00:13:25,040
built

306
00:13:21,600 --> 00:13:28,240
the dispatcher starts to work and the

307
00:13:25,040 --> 00:13:31,040
fine-grained stream precision begins

308
00:13:28,240 --> 00:13:33,920
each operator shall be assigned to the

309
00:13:31,040 --> 00:13:36,560
course of the cpu of the gpu for

310
00:13:33,920 --> 00:13:39,120
parallel execution

311
00:13:36,560 --> 00:13:40,479
additionally the workload could be

312
00:13:39,120 --> 00:13:43,600
dynamic

313
00:13:40,480 --> 00:13:46,720
for dynamic workload query plan

314
00:13:43,600 --> 00:13:49,839
adjustment and resource reallocation

315
00:13:46,720 --> 00:13:49,839
need to be conducted

316
00:13:52,320 --> 00:13:58,720
we first study the topology

317
00:13:55,680 --> 00:13:59,920
the query plan can be represented as a

318
00:13:58,720 --> 00:14:02,720
dac

319
00:13:59,920 --> 00:14:04,800
in our paper we concentrate on

320
00:14:02,720 --> 00:14:08,240
relational queries

321
00:14:04,800 --> 00:14:09,199
we show an example in this slide where

322
00:14:08,240 --> 00:14:12,720
opi

323
00:14:09,199 --> 00:14:16,719
represents an operator op7

324
00:14:12,720 --> 00:14:19,360
and op11 can represent joins

325
00:14:16,720 --> 00:14:20,079
we follow the terminology in compiler

326
00:14:19,360 --> 00:14:22,800
domain and

327
00:14:20,079 --> 00:14:24,880
call the operators from the beginning of

328
00:14:22,800 --> 00:14:28,000
the operator after join

329
00:14:24,880 --> 00:14:30,079
to the operator that merges with another

330
00:14:28,000 --> 00:14:33,360
operator as a branch

331
00:14:30,079 --> 00:14:35,680
hence the query plan is also a branch

332
00:14:33,360 --> 00:14:39,360
tag

333
00:14:35,680 --> 00:14:42,959
file stream involves suite optimizations

334
00:14:39,360 --> 00:14:45,839
the first optimization is branch qurani

335
00:14:42,959 --> 00:14:47,040
which means that the independent

336
00:14:45,839 --> 00:14:50,560
branches can be

337
00:14:47,040 --> 00:14:54,160
executed in parallel we use an

338
00:14:50,560 --> 00:14:56,560
example in this slide for illustration

339
00:14:54,160 --> 00:14:57,279
assume that the time for different

340
00:14:56,560 --> 00:15:00,560
branches

341
00:14:57,279 --> 00:15:04,000
is showing in their slide if we corrupt

342
00:15:00,560 --> 00:15:07,439
the three branches simultaneously

343
00:15:04,000 --> 00:15:07,920
then the execution can be partitioned

344
00:15:07,440 --> 00:15:11,120
into

345
00:15:07,920 --> 00:15:13,199
three stages with different overlapping

346
00:15:11,120 --> 00:15:16,079
situations

347
00:15:13,199 --> 00:15:18,319
if the required bandwidth for stage i

348
00:15:16,079 --> 00:15:21,040
exceeds the system maximum

349
00:15:18,320 --> 00:15:27,040
bandwidth we can perform a branch

350
00:15:21,040 --> 00:15:29,839
scheduling optimization

351
00:15:27,040 --> 00:15:32,480
the second optimization is batch

352
00:15:29,839 --> 00:15:35,199
pipeline

353
00:15:32,480 --> 00:15:36,240
we can also partition the dac into

354
00:15:35,199 --> 00:15:38,800
phases

355
00:15:36,240 --> 00:15:39,360
and perform co-running in pipeline

356
00:15:38,800 --> 00:15:43,120
between

357
00:15:39,360 --> 00:15:46,560
phases for precising different batches

358
00:15:43,120 --> 00:15:49,199
for simplicity in this part

359
00:15:46,560 --> 00:15:49,920
we assume that the number of phases in

360
00:15:49,199 --> 00:15:52,880
the deck

361
00:15:49,920 --> 00:15:54,000
is two please note that when the

362
00:15:52,880 --> 00:15:56,959
platform has

363
00:15:54,000 --> 00:15:57,839
enough resources the pipeline for

364
00:15:56,959 --> 00:16:01,599
operators

365
00:15:57,839 --> 00:16:03,360
can be deeper we show a simple example

366
00:16:01,600 --> 00:16:05,839
in this slide

367
00:16:03,360 --> 00:16:08,240
the operators in phase 1 and the

368
00:16:05,839 --> 00:16:11,040
operators in phase 2

369
00:16:08,240 --> 00:16:12,480
need to be mapped into different compute

370
00:16:11,040 --> 00:16:15,759
units

371
00:16:12,480 --> 00:16:17,279
so that these two phases can corrun in

372
00:16:15,759 --> 00:16:20,839
pipeline

373
00:16:17,279 --> 00:16:23,439
figure b shows the execution flow in

374
00:16:20,839 --> 00:16:26,240
pipeline when fund stream

375
00:16:23,440 --> 00:16:27,279
completes the processing for back one in

376
00:16:26,240 --> 00:16:30,399
phase one

377
00:16:27,279 --> 00:16:31,600
and starts to process batch one in phase

378
00:16:30,399 --> 00:16:34,560
two

379
00:16:31,600 --> 00:16:38,079
find stream can start to process part

380
00:16:34,560 --> 00:16:41,359
two in phase 1 simultaneously

381
00:16:38,079 --> 00:16:45,920
phase 1 and phase 2 can call run because

382
00:16:41,360 --> 00:16:45,920
they rely on different compute units

383
00:16:47,279 --> 00:16:52,079
the third optimization is for handling

384
00:16:50,000 --> 00:16:55,360
dynamic workload

385
00:16:52,079 --> 00:16:59,839
in fine stream we use a lightweight

386
00:16:55,360 --> 00:17:02,959
dynamic resource reallocation strategy

387
00:16:59,839 --> 00:17:06,240
when the workload ingestion rate of

388
00:17:02,959 --> 00:17:09,599
branch decreases we can calculate

389
00:17:06,240 --> 00:17:12,799
the reduced ratio and assume that

390
00:17:09,599 --> 00:17:16,319
such proportion of computing resources

391
00:17:12,799 --> 00:17:18,079
in that branch can be transferred to the

392
00:17:16,319 --> 00:17:20,720
other branches

393
00:17:18,079 --> 00:17:22,879
we use an example in figure 8 for

394
00:17:20,720 --> 00:17:25,919
illustration

395
00:17:22,880 --> 00:17:29,200
in figure 8 19 workload

396
00:17:25,919 --> 00:17:33,039
after operator op1

397
00:17:29,200 --> 00:17:36,799
flow to op2 when the workload

398
00:17:33,039 --> 00:17:39,240
state changes to the state in figure b

399
00:17:36,799 --> 00:17:40,480
part of the computing resource

400
00:17:39,240 --> 00:17:44,840
associated

401
00:17:40,480 --> 00:17:47,840
with op2 shall be assigned to op3

402
00:17:44,840 --> 00:17:47,840
accordingly

403
00:17:49,840 --> 00:17:54,480
we present the system execution flow in

404
00:17:53,120 --> 00:17:57,918
this slide

405
00:17:54,480 --> 00:18:00,640
thread 1 is used to catch input data

406
00:17:57,919 --> 00:18:02,400
where thread 2 is used to process or

407
00:18:00,640 --> 00:18:04,880
catch the data

408
00:18:02,400 --> 00:18:06,080
the detailed execution flow is as

409
00:18:04,880 --> 00:18:08,720
follows

410
00:18:06,080 --> 00:18:09,918
first when find stream starts a new

411
00:18:08,720 --> 00:18:13,520
query

412
00:18:09,919 --> 00:18:14,240
the dispatcher ethicals the query on the

413
00:18:13,520 --> 00:18:18,160
cpu

414
00:18:14,240 --> 00:18:18,559
for batch one and then on the gpu for

415
00:18:18,160 --> 00:18:22,880
back

416
00:18:18,559 --> 00:18:26,000
tool second during the single device

417
00:18:22,880 --> 00:18:29,120
executions find stream conducts

418
00:18:26,000 --> 00:18:32,720
online profiling during which

419
00:18:29,120 --> 00:18:35,039
the operator related data that are used

420
00:18:32,720 --> 00:18:36,559
to build the performance model are

421
00:18:35,039 --> 00:18:39,840
obtained

422
00:18:36,559 --> 00:18:43,520
including the cpu and gpu performance

423
00:18:39,840 --> 00:18:46,399
and the bandwidth utilization third

424
00:18:43,520 --> 00:18:48,960
with this data file stream builds a

425
00:18:46,400 --> 00:18:50,080
performance model considering branch

426
00:18:48,960 --> 00:18:53,520
co-running

427
00:18:50,080 --> 00:18:56,639
and batch pipeline first after

428
00:18:53,520 --> 00:18:58,080
building the model find stream generates

429
00:18:56,640 --> 00:19:01,760
several query plans

430
00:18:58,080 --> 00:19:04,799
with detailed resource distribution

431
00:19:01,760 --> 00:19:06,320
with a generated query plan the

432
00:19:04,799 --> 00:19:09,200
dispatcher performs

433
00:19:06,320 --> 00:19:10,000
fine-grained scheduling for precising

434
00:19:09,200 --> 00:19:13,039
the following

435
00:19:10,000 --> 00:19:16,640
batches when dynamic workload

436
00:19:13,039 --> 00:19:18,400
is detected fund stream performs related

437
00:19:16,640 --> 00:19:21,760
adjustment

438
00:19:18,400 --> 00:19:24,240
for the operators in fine stream

439
00:19:21,760 --> 00:19:25,520
when we use the operator code from

440
00:19:24,240 --> 00:19:28,160
omnidb

441
00:19:25,520 --> 00:19:30,320
please note that the goal of this work

442
00:19:28,160 --> 00:19:33,200
is to provide a fine-grained

443
00:19:30,320 --> 00:19:36,159
stream precising method on integrated

444
00:19:33,200 --> 00:19:36,160
architectures

445
00:19:37,039 --> 00:19:41,120
let's look at the evaluation

446
00:19:42,880 --> 00:19:49,600
we perform experiments on four platforms

447
00:19:46,720 --> 00:19:50,840
two integrated platforms and two

448
00:19:49,600 --> 00:19:54,080
discrete

449
00:19:50,840 --> 00:19:54,959
platforms the baseline is a bug

450
00:19:54,080 --> 00:19:58,399
synchronous

451
00:19:54,960 --> 00:20:00,880
parallel model which is server in our

452
00:19:58,400 --> 00:20:04,640
experiment

453
00:20:00,880 --> 00:20:07,440
we use four data sites in the evaluation

454
00:20:04,640 --> 00:20:09,520
the first data set is google compute

455
00:20:07,440 --> 00:20:12,240
cluster monitoring

456
00:20:09,520 --> 00:20:13,679
which emulates a cluster management

457
00:20:12,240 --> 00:20:17,039
scenario

458
00:20:13,679 --> 00:20:18,799
the second data set is anomaly detection

459
00:20:17,039 --> 00:20:21,440
in smart grids

460
00:20:18,799 --> 00:20:22,240
which is about detection in energy

461
00:20:21,440 --> 00:20:25,039
consumption

462
00:20:22,240 --> 00:20:28,159
from different devices of a smart

463
00:20:25,039 --> 00:20:30,640
electricity grade

464
00:20:28,159 --> 00:20:31,760
the third data side is linear road

465
00:20:30,640 --> 00:20:35,760
benchmark

466
00:20:31,760 --> 00:20:38,240
which models a network of totals

467
00:20:35,760 --> 00:20:40,240
these quizzes come from real world

468
00:20:38,240 --> 00:20:43,520
applications

469
00:20:40,240 --> 00:20:46,720
the first data set is a synthetically

470
00:20:43,520 --> 00:20:50,000
generated data set for evaluating

471
00:20:46,720 --> 00:20:53,919
independent operators we are each

472
00:20:50,000 --> 00:20:57,520
tuple consists of a 16 4 bit timestamp

473
00:20:53,919 --> 00:21:02,000
and six certain two bit attributes

474
00:20:57,520 --> 00:21:04,720
drawn from a uniform distribution

475
00:21:02,000 --> 00:21:07,440
we use snack queries to evaluate the

476
00:21:04,720 --> 00:21:09,840
overall performance of the fine-grained

477
00:21:07,440 --> 00:21:12,480
stream precision engine on the

478
00:21:09,840 --> 00:21:15,600
integrated architectures

479
00:21:12,480 --> 00:21:18,640
for example for the first query

480
00:21:15,600 --> 00:21:21,760
we select timestamp category and

481
00:21:18,640 --> 00:21:25,360
cpu information from task invent

482
00:21:21,760 --> 00:21:25,360
and group by category

483
00:21:26,240 --> 00:21:30,480
let's look at throughput we explore the

484
00:21:29,760 --> 00:21:33,600
throughput

485
00:21:30,480 --> 00:21:36,720
of one stream for the night queries

486
00:21:33,600 --> 00:21:38,639
result shows a processing throughput of

487
00:21:36,720 --> 00:21:41,360
the best single device

488
00:21:38,640 --> 00:21:42,080
saba and the fine stream for this

489
00:21:41,360 --> 00:21:46,479
queries

490
00:21:42,080 --> 00:21:50,480
on both the a10 7850k

491
00:21:46,480 --> 00:21:54,720
and ryzen 5 2400g

492
00:21:50,480 --> 00:21:59,440
platforms fine streams achieve the best

493
00:21:54,720 --> 00:22:02,640
performance in most cases

494
00:21:59,440 --> 00:22:04,559
this slide reports the latency of

495
00:22:02,640 --> 00:22:05,840
different queries on the integrated

496
00:22:04,559 --> 00:22:08,879
architectures

497
00:22:05,840 --> 00:22:10,000
in this work latency is defined as a end

498
00:22:08,880 --> 00:22:12,799
to end

499
00:22:10,000 --> 00:22:14,640
time from the time a query starts

500
00:22:12,799 --> 00:22:17,440
through the time it ends

501
00:22:14,640 --> 00:22:20,240
fine stream has a lowest lessons among

502
00:22:17,440 --> 00:22:20,240
these measures

503
00:22:21,280 --> 00:22:25,280
we show the relationship between

504
00:22:23,520 --> 00:22:28,000
throughput and the latency

505
00:22:25,280 --> 00:22:29,200
of both fine stream and server in this

506
00:22:28,000 --> 00:22:32,640
slide

507
00:22:29,200 --> 00:22:35,840
which shows that queries with high

508
00:22:32,640 --> 00:22:41,039
throughput usually have low latency and

509
00:22:35,840 --> 00:22:43,918
vice versa

510
00:22:41,039 --> 00:22:46,879
we further studied the cpu and gpu

511
00:22:43,919 --> 00:22:51,120
utilization of saba and the front stream

512
00:22:46,880 --> 00:22:52,480
and use a 87850k platform for

513
00:22:51,120 --> 00:22:55,840
illustration

514
00:22:52,480 --> 00:22:59,679
as shown in this slide

515
00:22:55,840 --> 00:23:01,120
in most cases fine stream utilizes the

516
00:22:59,679 --> 00:23:08,080
gpu device

517
00:23:01,120 --> 00:23:10,158
better on the integrated architecture

518
00:23:08,080 --> 00:23:12,559
current gpu on the integrated

519
00:23:10,159 --> 00:23:13,360
architecture is less powerful than the

520
00:23:12,559 --> 00:23:17,039
discrete

521
00:23:13,360 --> 00:23:21,678
gpu the discrete gpus exhibit

522
00:23:17,039 --> 00:23:23,919
1.8 to 5.7 times higher throughput than

523
00:23:21,679 --> 00:23:26,559
the integrated architectures

524
00:23:23,919 --> 00:23:27,200
due to the more computational power of

525
00:23:26,559 --> 00:23:30,879
discrete

526
00:23:27,200 --> 00:23:33,520
gpus however the integrated architecture

527
00:23:30,880 --> 00:23:36,240
demonstrates lower processing latency

528
00:23:33,520 --> 00:23:38,720
compared to the discrete architectures

529
00:23:36,240 --> 00:23:39,919
when the data transmission cost between

530
00:23:38,720 --> 00:23:41,919
the host and memory

531
00:23:39,919 --> 00:23:44,880
and gpu memory in the workload is

532
00:23:41,919 --> 00:23:44,880
significant

533
00:23:46,159 --> 00:23:50,080
fine stream on integrated architectures

534
00:23:48,720 --> 00:23:53,120
shows a high price

535
00:23:50,080 --> 00:23:55,279
throughput ratio compared to saba on the

536
00:23:53,120 --> 00:23:58,399
discrete architectures

537
00:23:55,279 --> 00:24:00,080
this slide shows the comparison of this

538
00:23:58,400 --> 00:24:03,039
price throughput ratio

539
00:24:00,080 --> 00:24:04,240
on average fine stream of the integrated

540
00:24:03,039 --> 00:24:06,400
architectures

541
00:24:04,240 --> 00:24:11,120
outperform saba on the discrete

542
00:24:06,400 --> 00:24:11,120
architectures by 10.4 times

543
00:24:12,320 --> 00:24:20,240
we also analyze the energy efficience of

544
00:24:15,840 --> 00:24:22,240
fine stream and saba on average

545
00:24:20,240 --> 00:24:23,679
fine stream on the integrated

546
00:24:22,240 --> 00:24:27,279
architectures

547
00:24:23,679 --> 00:24:33,840
is 1.8 times energy efficient

548
00:24:27,279 --> 00:24:33,840
than saba on the discrete architectures

549
00:24:34,880 --> 00:24:38,559
let's look at the conclusion

550
00:24:39,120 --> 00:24:43,120
we propose a first functioning

551
00:24:41,039 --> 00:24:44,720
window-based relational screen

552
00:24:43,120 --> 00:24:47,678
processing framework

553
00:24:44,720 --> 00:24:47,919
that adopts continuous operator model

554
00:24:47,679 --> 00:24:50,240
and

555
00:24:47,919 --> 00:24:51,200
takes advantages of the spectral

556
00:24:50,240 --> 00:24:54,559
features of

557
00:24:51,200 --> 00:24:56,880
integrated architectures

558
00:24:54,559 --> 00:24:59,120
we develop lightweight query plan

559
00:24:56,880 --> 00:25:00,080
adaptations for handling dynamic

560
00:24:59,120 --> 00:25:02,959
workloads

561
00:25:00,080 --> 00:25:04,158
with a performance model that considers

562
00:25:02,960 --> 00:25:07,440
both the operator

563
00:25:04,159 --> 00:25:10,320
and architecture characteristics

564
00:25:07,440 --> 00:25:12,480
we evaluate find stream on a set of

565
00:25:10,320 --> 00:25:14,879
stream queries to demonstrate

566
00:25:12,480 --> 00:25:16,799
the performance benefits our current

567
00:25:14,880 --> 00:25:29,360
opportunities

568
00:25:16,799 --> 00:25:29,360
thank you

