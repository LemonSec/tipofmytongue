1
00:00:13,360 --> 00:00:13,920
uh

2
00:00:13,920 --> 00:00:15,679
how do you cheers for making it to the

3
00:00:15,679 --> 00:00:18,240
end i know the bars open but um

4
00:00:18,240 --> 00:00:20,400
cheers uh so this is introduction to

5
00:00:20,400 --> 00:00:22,000
adversarial machine learning and other

6
00:00:22,000 --> 00:00:23,439
ai attacks

7
00:00:23,439 --> 00:00:25,840
um so a little bit about me that's my

8
00:00:25,840 --> 00:00:28,240
github where all my stuff is

9
00:00:28,240 --> 00:00:30,400
so started in robotics moved into ai

10
00:00:30,400 --> 00:00:31,760
ended up in pen testing it's been a

11
00:00:31,760 --> 00:00:32,399
journey

12
00:00:32,399 --> 00:00:34,640
um so i did my honors in multimodal

13
00:00:34,640 --> 00:00:36,480
neural networks for classifying fabric

14
00:00:36,480 --> 00:00:38,480
waste for textile recycling

15
00:00:38,480 --> 00:00:40,160
um it's a super awesome cause if anyone

16
00:00:40,160 --> 00:00:41,520
is interested let me know and i can put

17
00:00:41,520 --> 00:00:43,440
you in touch with some awesome people

18
00:00:43,440 --> 00:00:46,559
that's really how i got started in ai

19
00:00:46,559 --> 00:00:48,079
otherwise otherwise there's just some

20
00:00:48,079 --> 00:00:49,920
other generic stuff there i'm a pen

21
00:00:49,920 --> 00:00:51,199
tester for tml

22
00:00:51,199 --> 00:00:54,480
shout out to them they're tremendous

23
00:00:54,559 --> 00:00:56,640
uh so the goals for today so just go

24
00:00:56,640 --> 00:00:58,000
over adversarial machine learning

25
00:00:58,000 --> 00:01:00,079
and the risks posed by it are then going

26
00:01:00,079 --> 00:01:01,680
to go over the mitre adversarial machine

27
00:01:01,680 --> 00:01:02,480
learning

28
00:01:02,480 --> 00:01:05,040
threat matrix then cover the basics of

29
00:01:05,040 --> 00:01:05,600
gans

30
00:01:05,600 --> 00:01:07,520
and risks that synthetic media currently

31
00:01:07,520 --> 00:01:10,840
poses and then a couple different

32
00:01:10,840 --> 00:01:12,479
defenses

33
00:01:12,479 --> 00:01:14,479
um so artificial intelligence really

34
00:01:14,479 --> 00:01:16,159
it's just something that seems smart

35
00:01:16,159 --> 00:01:17,680
it could be decision trees it could be

36
00:01:17,680 --> 00:01:19,280
machine learning

37
00:01:19,280 --> 00:01:22,479
it can even be if statements uh so

38
00:01:22,479 --> 00:01:23,920
really it doesn't matter what's going on

39
00:01:23,920 --> 00:01:25,920
the back end it's just trying to

40
00:01:25,920 --> 00:01:28,080
act intelligent really so machine

41
00:01:28,080 --> 00:01:29,520
learning on the other side is something

42
00:01:29,520 --> 00:01:30,400
that learns

43
00:01:30,400 --> 00:01:31,920
so it's usually split up in a supervised

44
00:01:31,920 --> 00:01:33,920
and unsupervised learning

45
00:01:33,920 --> 00:01:35,680
most of the ai you say in the news these

46
00:01:35,680 --> 00:01:37,920
days is machine learning based

47
00:01:37,920 --> 00:01:39,119
and it's used in security for things

48
00:01:39,119 --> 00:01:41,840
like antivirus

49
00:01:42,000 --> 00:01:44,799
so why threat model ai attacks so

50
00:01:44,799 --> 00:01:46,640
self-driving cars i think is a great

51
00:01:46,640 --> 00:01:47,439
example of

52
00:01:47,439 --> 00:01:49,280
why ai is becoming a bigger and bigger

53
00:01:49,280 --> 00:01:51,040
issue and you don't want a self-driving

54
00:01:51,040 --> 00:01:52,159
car falling for

55
00:01:52,159 --> 00:01:54,320
pretty much the oldest trick in the book

56
00:01:54,320 --> 00:01:55,360
um

57
00:01:55,360 --> 00:01:57,200
so this one didn't actually happen i

58
00:01:57,200 --> 00:02:00,240
feel uh obligated to say this was a

59
00:02:00,240 --> 00:02:03,200
fake news but it's a genuine concern

60
00:02:03,200 --> 00:02:03,520
like

61
00:02:03,520 --> 00:02:05,119
at what point do you declare something a

62
00:02:05,119 --> 00:02:07,200
tunnel because you don't want your car

63
00:02:07,200 --> 00:02:09,119
driving into it

64
00:02:09,119 --> 00:02:10,959
so real world ai problems so this is a

65
00:02:10,959 --> 00:02:12,400
bit of an old one now but i think it's

66
00:02:12,400 --> 00:02:13,840
still the best example to go to

67
00:02:13,840 --> 00:02:16,160
so tweets was a project back in 2016

68
00:02:16,160 --> 00:02:18,000
where you could tweet at a twitter bot

69
00:02:18,000 --> 00:02:19,440
and it would learn from what was tweeted

70
00:02:19,440 --> 00:02:22,000
to it and it started off pretty nice as

71
00:02:22,000 --> 00:02:22,720
this was i

72
00:02:22,720 --> 00:02:25,680
believe day one and then by the next

73
00:02:25,680 --> 00:02:28,480
morning it got a little weird

74
00:02:28,480 --> 00:02:30,800
and then a little later it got a little

75
00:02:30,800 --> 00:02:32,879
less nice

76
00:02:32,879 --> 00:02:35,360
and then by the end it was uh the entire

77
00:02:35,360 --> 00:02:36,720
project had to be scrapped in less than

78
00:02:36,720 --> 00:02:39,040
a day it was an absolute mess

79
00:02:39,040 --> 00:02:41,120
and it just goes to show that if you're

80
00:02:41,120 --> 00:02:43,680
not thinking about these kinds of issues

81
00:02:43,680 --> 00:02:45,040
problems will occur the same way they do

82
00:02:45,040 --> 00:02:47,920
with every other piece of software

83
00:02:47,920 --> 00:02:49,040
um so we're going to talk about

84
00:02:49,040 --> 00:02:50,480
two-dimensional data just because it's

85
00:02:50,480 --> 00:02:51,840
nice and you can see it and it's nice

86
00:02:51,840 --> 00:02:53,120
and pretty but this can go out to the

87
00:02:53,120 --> 00:02:55,040
nth dimension so these could be images

88
00:02:55,040 --> 00:02:56,959
um so this is my favorite example and

89
00:02:56,959 --> 00:02:59,120
essentially this is just a 100 kilometer

90
00:02:59,120 --> 00:03:00,959
by 100 kilometer map

91
00:03:00,959 --> 00:03:03,040
where a kangaroo crocodile and koala has

92
00:03:03,040 --> 00:03:04,720
all been found i don't know how well you

93
00:03:04,720 --> 00:03:06,000
can see it but it'll get easier in a

94
00:03:06,000 --> 00:03:07,200
second

95
00:03:07,200 --> 00:03:09,440
so and the question is based on where

96
00:03:09,440 --> 00:03:10,879
these three were found

97
00:03:10,879 --> 00:03:12,159
where would you expect to find more

98
00:03:12,159 --> 00:03:14,720
kangaroos crocodiles and koalas

99
00:03:14,720 --> 00:03:16,879
so the way we do this is by essentially

100
00:03:16,879 --> 00:03:19,360
building these nice boundaries

101
00:03:19,360 --> 00:03:20,879
so what we've done now is essentially

102
00:03:20,879 --> 00:03:22,159
what's called a model we've just drawn

103
00:03:22,159 --> 00:03:22,800
some lines

104
00:03:22,800 --> 00:03:24,159
we've said anything on the left is going

105
00:03:24,159 --> 00:03:25,599
to be a kangaroo anything on the right

106
00:03:25,599 --> 00:03:26,959
is a koala anything

107
00:03:26,959 --> 00:03:30,319
down is a crocodile easy enough but is

108
00:03:30,319 --> 00:03:31,519
this a good model

109
00:03:31,519 --> 00:03:33,280
who knows so here's four different

110
00:03:33,280 --> 00:03:35,519
examples of equally valid models

111
00:03:35,519 --> 00:03:37,360
and i've just drawn random stuff they

112
00:03:37,360 --> 00:03:39,040
all have 100 accuracy

113
00:03:39,040 --> 00:03:41,840
uh which is the best how do we find out

114
00:03:41,840 --> 00:03:43,200
we just add more data

115
00:03:43,200 --> 00:03:45,040
so once you add more data you can then

116
00:03:45,040 --> 00:03:46,560
go back to your models and see how

117
00:03:46,560 --> 00:03:48,080
accurate is it now

118
00:03:48,080 --> 00:03:50,480
and essentially as you can see none of

119
00:03:50,480 --> 00:03:51,840
these are particularly accurate top left

120
00:03:51,840 --> 00:03:53,120
gets pretty good but it's still not

121
00:03:53,120 --> 00:03:53,680
great

122
00:03:53,680 --> 00:03:55,439
so then what we do is we draw some

123
00:03:55,439 --> 00:03:57,360
better lines and then now that we've got

124
00:03:57,360 --> 00:03:57,760
this

125
00:03:57,760 --> 00:03:59,840
we can add more data and then we can

126
00:03:59,840 --> 00:04:01,040
just keep working and working and

127
00:04:01,040 --> 00:04:01,680
essentially

128
00:04:01,680 --> 00:04:05,760
get the best boundaries we can and then

129
00:04:05,760 --> 00:04:07,519
so yeah the big takeaway here more data

130
00:04:07,519 --> 00:04:09,599
better models the issue is what happens

131
00:04:09,599 --> 00:04:11,040
when you have lots and lots of data and

132
00:04:11,040 --> 00:04:12,319
it's a little messy

133
00:04:12,319 --> 00:04:14,159
so with something like this where the

134
00:04:14,159 --> 00:04:15,519
data is kind of all over the place

135
00:04:15,519 --> 00:04:18,160
you have two big options you can either

136
00:04:18,160 --> 00:04:19,918
just draw some kind of rough lines

137
00:04:19,918 --> 00:04:21,440
and get reasonable accuracy so i think

138
00:04:21,440 --> 00:04:23,360
this gets around 80 accuracy or on the

139
00:04:23,360 --> 00:04:24,240
right you can see

140
00:04:24,240 --> 00:04:26,400
this one gets perfect accuracy but it's

141
00:04:26,400 --> 00:04:28,479
very zigzaggy it's very wonky

142
00:04:28,479 --> 00:04:30,400
um so question for the audience which of

143
00:04:30,400 --> 00:04:33,280
these models is better

144
00:04:34,000 --> 00:04:37,199
the left exactamundo yeah so the 80

145
00:04:37,199 --> 00:04:38,479
is actually better because you're not

146
00:04:38,479 --> 00:04:40,080
relying so much on individual data

147
00:04:40,080 --> 00:04:41,520
points that you're working with

148
00:04:41,520 --> 00:04:44,960
um it's a little overlapping

149
00:04:44,960 --> 00:04:46,400
yes so this model on the right is what

150
00:04:46,400 --> 00:04:48,000
we'd call overtrained which essentially

151
00:04:48,000 --> 00:04:49,120
says you're just paying too much

152
00:04:49,120 --> 00:04:50,400
attention to your actual data points

153
00:04:50,400 --> 00:04:51,680
that you're working with for training

154
00:04:51,680 --> 00:04:52,800
and you're going to end up with having

155
00:04:52,800 --> 00:04:54,720
these all these little errors but this

156
00:04:54,720 --> 00:04:55,840
kind of takes us to the big problem

157
00:04:55,840 --> 00:04:56,720
which is

158
00:04:56,720 --> 00:04:58,320
when you're modeling for generalization

159
00:04:58,320 --> 00:04:59,919
which is not the data you have but the

160
00:04:59,919 --> 00:05:01,199
data you don't have

161
00:05:01,199 --> 00:05:02,240
you're going to end up with some

162
00:05:02,240 --> 00:05:04,800
inaccuracies because 100 isn't

163
00:05:04,800 --> 00:05:06,160
necessarily the goal

164
00:05:06,160 --> 00:05:07,919
so misclassifications happen when stuff

165
00:05:07,919 --> 00:05:09,759
like this occurs so especially in

166
00:05:09,759 --> 00:05:11,280
overtrained networks but even in regular

167
00:05:11,280 --> 00:05:11,919
networks

168
00:05:11,919 --> 00:05:13,440
so i've just zoomed in on pretty much

169
00:05:13,440 --> 00:05:15,199
the hotspot where you can see that one

170
00:05:15,199 --> 00:05:16,160
orange data point

171
00:05:16,160 --> 00:05:17,840
uh sorry for anyone who's colorblind

172
00:05:17,840 --> 00:05:19,520
this square actually i think it should

173
00:05:19,520 --> 00:05:20,720
be all good

174
00:05:20,720 --> 00:05:22,400
because of where that square is it's

175
00:05:22,400 --> 00:05:23,919
taken a whole bunch of data which should

176
00:05:23,919 --> 00:05:25,120
probably be a different one

177
00:05:25,120 --> 00:05:27,039
and just a few different changes to the

178
00:05:27,039 --> 00:05:28,320
x and y coordinates gives you an

179
00:05:28,320 --> 00:05:29,840
entirely different class

180
00:05:29,840 --> 00:05:31,360
so trying to find where these little

181
00:05:31,360 --> 00:05:32,960
weird spots are

182
00:05:32,960 --> 00:05:34,160
are essentially what you're trying to

183
00:05:34,160 --> 00:05:36,840
find to find where miss classifications

184
00:05:36,840 --> 00:05:38,800
are

185
00:05:38,800 --> 00:05:40,160
so neural networks are really just a

186
00:05:40,160 --> 00:05:42,000
fancier version of this

187
00:05:42,000 --> 00:05:43,440
so you don't really need to understand

188
00:05:43,440 --> 00:05:44,880
exactly what's going on here if this is

189
00:05:44,880 --> 00:05:46,560
a kind of new diagram to you but

190
00:05:46,560 --> 00:05:47,360
essentially

191
00:05:47,360 --> 00:05:49,280
the model is the background and the dots

192
00:05:49,280 --> 00:05:50,960
the data points and that thing on the

193
00:05:50,960 --> 00:05:52,160
right essentially what we're doing is

194
00:05:52,160 --> 00:05:54,240
we're just slightly changing this model

195
00:05:54,240 --> 00:05:56,000
and then if it's more accurate we keep

196
00:05:56,000 --> 00:05:57,520
the changes and if it's less accurate we

197
00:05:57,520 --> 00:05:58,240
throw it away

198
00:05:58,240 --> 00:06:00,000
and you can see it eventually shifts and

199
00:06:00,000 --> 00:06:01,440
gets all the blue in the center and then

200
00:06:01,440 --> 00:06:03,759
the orange goes on the outside

201
00:06:03,759 --> 00:06:05,199
and you can get these even more and more

202
00:06:05,199 --> 00:06:06,880
fancy by just expanding

203
00:06:06,880 --> 00:06:08,800
how many nodes they have in them so this

204
00:06:08,800 --> 00:06:10,400
is a far more complex model

205
00:06:10,400 --> 00:06:12,560
and um it's a little hard to see but

206
00:06:12,560 --> 00:06:14,400
essentially it's just two spirals

207
00:06:14,400 --> 00:06:16,240
and something like this it has a lot of

208
00:06:16,240 --> 00:06:17,520
difficulty trying to classify you can

209
00:06:17,520 --> 00:06:19,120
see even where it kind of caps out

210
00:06:19,120 --> 00:06:21,520
in accuracy it's still weird and wonky

211
00:06:21,520 --> 00:06:24,400
and it's an absolute mess

212
00:06:25,039 --> 00:06:26,400
so this takes us to the big issue with

213
00:06:26,400 --> 00:06:28,319
machine learning so the fundamental

214
00:06:28,319 --> 00:06:29,280
weakness

215
00:06:29,280 --> 00:06:30,960
in machine learning comes from the fact

216
00:06:30,960 --> 00:06:32,800
that we don't want things to be 100

217
00:06:32,800 --> 00:06:33,280
accurate

218
00:06:33,280 --> 00:06:34,560
so some things are going to be

219
00:06:34,560 --> 00:06:37,199
misclassified

220
00:06:37,199 --> 00:06:38,800
so as attackers we just want to find the

221
00:06:38,800 --> 00:06:40,400
things that arm is classified or in some

222
00:06:40,400 --> 00:06:42,400
cases we can make things misclassified

223
00:06:42,400 --> 00:06:43,759
and this could be to bypass a safety

224
00:06:43,759 --> 00:06:45,520
feature to avoid antivirus but just to

225
00:06:45,520 --> 00:06:47,039
cause general havoc like in the taybot

226
00:06:47,039 --> 00:06:49,199
case

227
00:06:49,199 --> 00:06:50,319
so the threat model that we're really

228
00:06:50,319 --> 00:06:52,080
working with is all of the different

229
00:06:52,080 --> 00:06:53,520
moving components

230
00:06:53,520 --> 00:06:54,960
so you can see them all there and it's

231
00:06:54,960 --> 00:06:56,560
put up into two sections you've got

232
00:06:56,560 --> 00:06:57,199
training and

233
00:06:57,199 --> 00:06:59,120
inference so training is anything while

234
00:06:59,120 --> 00:07:00,800
the model is learning and then inference

235
00:07:00,800 --> 00:07:02,160
is once it's learned and you're trying

236
00:07:02,160 --> 00:07:04,160
to ask at things so trying to figure out

237
00:07:04,160 --> 00:07:05,759
uh what classification something should

238
00:07:05,759 --> 00:07:07,520
be

239
00:07:07,520 --> 00:07:09,360
uh yes the important point there being

240
00:07:09,360 --> 00:07:10,880
so whichever parts of this

241
00:07:10,880 --> 00:07:12,880
that your users can touch are the parts

242
00:07:12,880 --> 00:07:14,080
that are going to be at risk

243
00:07:14,080 --> 00:07:17,840
so for tabot it was the training data

244
00:07:18,960 --> 00:07:20,319
um so there's just a few other key

245
00:07:20,319 --> 00:07:21,919
things to kind of uh cross off before we

246
00:07:21,919 --> 00:07:23,440
get into the meat and potatoes this

247
00:07:23,440 --> 00:07:25,199
um so unsupervised learning is the

248
00:07:25,199 --> 00:07:26,479
alternative so everything we've looked

249
00:07:26,479 --> 00:07:27,280
at so far

250
00:07:27,280 --> 00:07:29,120
uses labeled data so you have your data

251
00:07:29,120 --> 00:07:30,720
points and you know that it's a kangaroo

252
00:07:30,720 --> 00:07:32,639
or a crocodile or a koala

253
00:07:32,639 --> 00:07:34,880
so unsupervised learning works of

254
00:07:34,880 --> 00:07:36,560
unlabeled data so it's way cheaper

255
00:07:36,560 --> 00:07:38,000
because you don't have to have someone

256
00:07:38,000 --> 00:07:39,440
going through every single data point

257
00:07:39,440 --> 00:07:41,360
figuring out what is this

258
00:07:41,360 --> 00:07:42,560
so essentially you just throw all the

259
00:07:42,560 --> 00:07:44,080
data an algorithm and let it try to

260
00:07:44,080 --> 00:07:45,599
figure it out

261
00:07:45,599 --> 00:07:47,440
so it's a great way to verify your data

262
00:07:47,440 --> 00:07:48,960
sets it's very quick and then you can

263
00:07:48,960 --> 00:07:50,800
move it on to supervised learning

264
00:07:50,800 --> 00:07:52,800
and the best example is is k-means

265
00:07:52,800 --> 00:07:55,759
clustering which essentially

266
00:07:55,759 --> 00:07:57,599
for each number of classes you have so

267
00:07:57,599 --> 00:07:58,800
in this case we have three

268
00:07:58,800 --> 00:08:00,400
you just take a node which is your

269
00:08:00,400 --> 00:08:02,800
classification node and whichever data

270
00:08:02,800 --> 00:08:04,319
point it's closest to

271
00:08:04,319 --> 00:08:06,879
is the class it'll be and then you move

272
00:08:06,879 --> 00:08:08,479
each of those nodes to the center of

273
00:08:08,479 --> 00:08:09,599
their cluster

274
00:08:09,599 --> 00:08:11,199
and then just rinse and repeat those two

275
00:08:11,199 --> 00:08:13,280
steps and eventually they'll all tend to

276
00:08:13,280 --> 00:08:15,120
shift towards these big clusters of data

277
00:08:15,120 --> 00:08:16,560
as you can see there

278
00:08:16,560 --> 00:08:18,720
and i'll let it play out for a little

279
00:08:18,720 --> 00:08:20,240
i'll let a full set play out so you can

280
00:08:20,240 --> 00:08:21,919
see it in real time so

281
00:08:21,919 --> 00:08:23,840
it starts off with everything being blue

282
00:08:23,840 --> 00:08:25,440
and then blue shifts up and to the right

283
00:08:25,440 --> 00:08:26,000
which makes

284
00:08:26,000 --> 00:08:28,479
red go up which makes green go right and

285
00:08:28,479 --> 00:08:29,520
then they all end up

286
00:08:29,520 --> 00:08:32,478
reasonably accurate

287
00:08:33,519 --> 00:08:36,320
so the question is uh really so data

288
00:08:36,320 --> 00:08:38,000
sets can come in all shapes and sizes so

289
00:08:38,000 --> 00:08:39,519
here's six random data sets

290
00:08:39,519 --> 00:08:40,958
and the question again for the audience

291
00:08:40,958 --> 00:08:42,640
is how do you cluster these different

292
00:08:42,640 --> 00:08:43,919
data sets

293
00:08:43,919 --> 00:08:45,360
and there's no right or wrong answer

294
00:08:45,360 --> 00:08:47,120
here but it really comes down to what

295
00:08:47,120 --> 00:08:49,040
the data actually has

296
00:08:49,040 --> 00:08:50,560
so for example when i see the two ring

297
00:08:50,560 --> 00:08:52,800
data sets i expect that the center ring

298
00:08:52,800 --> 00:08:54,080
will be one class and the

299
00:08:54,080 --> 00:08:55,760
outside ring will be a separate class

300
00:08:55,760 --> 00:08:56,880
but that might not necessarily be

301
00:08:56,880 --> 00:08:59,040
correct it could be a left right split

302
00:08:59,040 --> 00:09:00,800
so when we look at k-means with this

303
00:09:00,800 --> 00:09:02,320
this is how it gets them

304
00:09:02,320 --> 00:09:04,560
so i would say for those two circle data

305
00:09:04,560 --> 00:09:06,080
sets this is incorrect

306
00:09:06,080 --> 00:09:08,560
same with the the upside down u and the

307
00:09:08,560 --> 00:09:09,519
right side up view

308
00:09:09,519 --> 00:09:12,399
and then the stretched clusters as well

309
00:09:12,399 --> 00:09:13,920
but it really comes down to

310
00:09:13,920 --> 00:09:16,080
what is correct so you don't have to

311
00:09:16,080 --> 00:09:17,120
look at everything in this slide but

312
00:09:17,120 --> 00:09:18,480
essentially i've just taken a bunch of

313
00:09:18,480 --> 00:09:19,839
different algorithms so you can see

314
00:09:19,839 --> 00:09:21,839
they all work differently which is a

315
00:09:21,839 --> 00:09:23,360
problem if you're using this for a

316
00:09:23,360 --> 00:09:24,399
problem because

317
00:09:24,399 --> 00:09:26,880
you're going to get different results so

318
00:09:26,880 --> 00:09:27,440
they

319
00:09:27,440 --> 00:09:29,519
they definitely have a great use but

320
00:09:29,519 --> 00:09:31,120
they have a lot of problems

321
00:09:31,120 --> 00:09:32,399
and then there's just a few little

322
00:09:32,399 --> 00:09:34,959
interesting ones i've kind of picked out

323
00:09:34,959 --> 00:09:36,720
to kind of show off the different ways

324
00:09:36,720 --> 00:09:39,680
that things can get clustered

325
00:09:40,240 --> 00:09:41,440
yeah so the issues with supervised

326
00:09:41,440 --> 00:09:42,959
learning and the reason why it's not

327
00:09:42,959 --> 00:09:44,000
used for everything

328
00:09:44,000 --> 00:09:46,560
is all data is equally valued if you can

329
00:09:46,560 --> 00:09:47,279
control the

330
00:09:47,279 --> 00:09:49,680
inputs you can control the results it's

331
00:09:49,680 --> 00:09:51,279
vulnerable to supply chain attacks

332
00:09:51,279 --> 00:09:52,720
if no one's labeling your data how do

333
00:09:52,720 --> 00:09:55,120
you know it's labeled correctly

334
00:09:55,120 --> 00:09:56,560
or how do you know that it's the data

335
00:09:56,560 --> 00:09:59,279
that you're expecting for example

336
00:09:59,279 --> 00:10:01,200
each algorithm has its unique quirks so

337
00:10:01,200 --> 00:10:02,720
if you know what algorithm they're using

338
00:10:02,720 --> 00:10:04,079
you can cater your attacks based

339
00:10:04,079 --> 00:10:05,680
on the biases that that specific

340
00:10:05,680 --> 00:10:07,680
algorithm has

341
00:10:07,680 --> 00:10:09,680
and of course uh because yeah if you

342
00:10:09,680 --> 00:10:11,839
know the data set and the algorithm used

343
00:10:11,839 --> 00:10:13,120
you can just replicate the model

344
00:10:13,120 --> 00:10:13,760
yourself

345
00:10:13,760 --> 00:10:15,440
and if you have a model then in general

346
00:10:15,440 --> 00:10:16,959
you've pretty much won in this kind of

347
00:10:16,959 --> 00:10:18,160
space

348
00:10:18,160 --> 00:10:19,760
and because it's less accurate it's also

349
00:10:19,760 --> 00:10:20,959
just more vulnerable to brute force

350
00:10:20,959 --> 00:10:23,200
attacks

351
00:10:23,200 --> 00:10:24,560
uh transfer learning is one other quick

352
00:10:24,560 --> 00:10:26,000
thing to cover off so essentially

353
00:10:26,000 --> 00:10:27,360
training neural networks it takes a lot

354
00:10:27,360 --> 00:10:28,640
of time takes a lot of work

355
00:10:28,640 --> 00:10:30,640
it's expensive so it can generally be

356
00:10:30,640 --> 00:10:32,720
helpful to start with a model for a

357
00:10:32,720 --> 00:10:34,240
similar problem and then just work it

358
00:10:34,240 --> 00:10:35,120
into your problem

359
00:10:35,120 --> 00:10:36,720
so there's far more dog classifiers than

360
00:10:36,720 --> 00:10:38,240
there are koala classifiers

361
00:10:38,240 --> 00:10:39,920
so you can start with a classifier for a

362
00:10:39,920 --> 00:10:42,480
dog get some photos of koalas and then

363
00:10:42,480 --> 00:10:44,240
train it on the photos of koalas

364
00:10:44,240 --> 00:10:46,000
and take what it knows about classifying

365
00:10:46,000 --> 00:10:49,760
dogs save you a whole bunch of time

366
00:10:50,640 --> 00:10:53,360
so images so why are images hard so this

367
00:10:53,360 --> 00:10:55,200
is another fun rhetorical question

368
00:10:55,200 --> 00:10:56,959
which of these are stop signs and more

369
00:10:56,959 --> 00:10:58,720
importantly if you are programming a

370
00:10:58,720 --> 00:10:59,760
self-driving car

371
00:10:59,760 --> 00:11:01,760
which of these would you want a

372
00:11:01,760 --> 00:11:05,279
self-driving car to stop at

373
00:11:07,760 --> 00:11:11,040
let your ponder it for a bit

374
00:11:11,200 --> 00:11:13,680
yeah this is it lots of lots of i guess

375
00:11:13,680 --> 00:11:15,120
like questions to keep you thinking

376
00:11:15,120 --> 00:11:18,320
um so personally i would want it to stop

377
00:11:18,320 --> 00:11:19,839
at pretty much all of these except for

378
00:11:19,839 --> 00:11:20,320
the bottom

379
00:11:20,320 --> 00:11:23,600
three at the lollipop but again

380
00:11:23,600 --> 00:11:25,760
the real issue is that even the

381
00:11:25,760 --> 00:11:27,279
classifications of images

382
00:11:27,279 --> 00:11:31,120
is a very tricky uh yeah even the

383
00:11:31,120 --> 00:11:32,640
even the classifications is a really

384
00:11:32,640 --> 00:11:36,240
tricky thing to kind of get right yes

385
00:11:39,279 --> 00:11:43,440
hello excellent yeah so

386
00:11:43,440 --> 00:11:45,680
um images are super difficult should you

387
00:11:45,680 --> 00:11:47,440
trust a backwards image a color inverted

388
00:11:47,440 --> 00:11:48,240
image

389
00:11:48,240 --> 00:11:50,160
it's really it's really up to you um and

390
00:11:50,160 --> 00:11:51,440
then those bottom three are just a

391
00:11:51,440 --> 00:11:52,800
little easter egg for later

392
00:11:52,800 --> 00:11:54,160
but if you thought any of those were a

393
00:11:54,160 --> 00:11:57,839
stop sign you might be a robot

394
00:11:59,620 --> 00:12:02,160
[Laughter]

395
00:12:02,160 --> 00:12:05,920
found him so this is just an awesome

396
00:12:05,920 --> 00:12:07,519
quote i found so images are to machine

397
00:12:07,519 --> 00:12:10,320
learning what javascript alert is to xss

398
00:12:10,320 --> 00:12:13,120
so images super complex data and the

399
00:12:13,120 --> 00:12:14,720
classifications are all real messy at

400
00:12:14,720 --> 00:12:15,839
the best of times

401
00:12:15,839 --> 00:12:18,240
so these are the best thing for looking

402
00:12:18,240 --> 00:12:19,760
at to kind of expose these problems

403
00:12:19,760 --> 00:12:22,319
where they lie

404
00:12:22,880 --> 00:12:24,880
so mida microsoft and a bunch of other

405
00:12:24,880 --> 00:12:26,560
companies got together in 2019

406
00:12:26,560 --> 00:12:28,240
to try to classify all these different

407
00:12:28,240 --> 00:12:29,600
vulnerabilities and risks

408
00:12:29,600 --> 00:12:31,040
um so if anyone who knows the mitre

409
00:12:31,040 --> 00:12:32,880
attack framework it's

410
00:12:32,880 --> 00:12:34,399
it's very similar to that and

411
00:12:34,399 --> 00:12:35,680
essentially uh we're going to go through

412
00:12:35,680 --> 00:12:37,680
all the pieces in orange which are just

413
00:12:37,680 --> 00:12:39,440
all of the attacks that are unique to

414
00:12:39,440 --> 00:12:41,440
machine learning

415
00:12:41,440 --> 00:12:43,680
and essentially they just found i think

416
00:12:43,680 --> 00:12:45,920
there was about 19 case studies of real

417
00:12:45,920 --> 00:12:47,600
world attacks including tabot

418
00:12:47,600 --> 00:12:49,920
where ai caused a bunch of issues and

419
00:12:49,920 --> 00:12:51,519
used that to kind of quantify where

420
00:12:51,519 --> 00:12:52,639
these attacks come from

421
00:12:52,639 --> 00:12:54,160
so it's not all encompassing because

422
00:12:54,160 --> 00:12:55,839
it's only talked about attacks that are

423
00:12:55,839 --> 00:12:57,760
real as opposed to the theoretical ones

424
00:12:57,760 --> 00:12:59,440
but it's an awesome starting point and

425
00:12:59,440 --> 00:13:00,720
again if you're talking to business

426
00:13:00,720 --> 00:13:02,079
people or c levels

427
00:13:02,079 --> 00:13:04,839
they love these kinds of diagrams i'm

428
00:13:04,839 --> 00:13:07,120
sorry

429
00:13:07,120 --> 00:13:10,079
um yes so starting with recon so

430
00:13:10,079 --> 00:13:11,600
everything in recon is going to be about

431
00:13:11,600 --> 00:13:12,480
trying to

432
00:13:12,480 --> 00:13:14,639
figure out uh what the machine learning

433
00:13:14,639 --> 00:13:15,600
models are doing

434
00:13:15,600 --> 00:13:16,880
trying to get your own copies of them

435
00:13:16,880 --> 00:13:18,079
where possible because as previously

436
00:13:18,079 --> 00:13:18,800
said

437
00:13:18,800 --> 00:13:20,000
if you can get your own copy of the

438
00:13:20,000 --> 00:13:21,519
model then you're in for a really good

439
00:13:21,519 --> 00:13:22,959
time trying to attack these things

440
00:13:22,959 --> 00:13:25,279
um so it's kind of split up into two

441
00:13:25,279 --> 00:13:26,240
things uh

442
00:13:26,240 --> 00:13:28,720
family and ontology ontology are all the

443
00:13:28,720 --> 00:13:29,920
really specific things

444
00:13:29,920 --> 00:13:31,440
so can you figure out what data set it's

445
00:13:31,440 --> 00:13:32,800
using can you figure out the exact

446
00:13:32,800 --> 00:13:34,480
structure of the model

447
00:13:34,480 --> 00:13:36,000
and then family is a bit more vague so

448
00:13:36,000 --> 00:13:37,600
if you can't figure that out do you know

449
00:13:37,600 --> 00:13:38,959
if it's working off just images

450
00:13:38,959 --> 00:13:41,440
is it working off images and audio do

451
00:13:41,440 --> 00:13:42,800
you know if it's a deep neural network

452
00:13:42,800 --> 00:13:44,320
versus say a convolutional neural

453
00:13:44,320 --> 00:13:45,760
network

454
00:13:45,760 --> 00:13:47,120
and then what kind of output is it

455
00:13:47,120 --> 00:13:48,560
giving you is it saying this is a stop

456
00:13:48,560 --> 00:13:50,000
sign or is this not a stop sign

457
00:13:50,000 --> 00:13:51,680
or is it trying to recognize one of a

458
00:13:51,680 --> 00:13:54,479
thousand objects

459
00:13:55,600 --> 00:13:58,480
yeah so gathering data sets if you are

460
00:13:58,480 --> 00:14:01,120
trying to attack a koala detection

461
00:14:01,120 --> 00:14:03,120
machine learning algorithm then you need

462
00:14:03,120 --> 00:14:04,320
to get yourself an

463
00:14:04,320 --> 00:14:06,240
a data set of koalas that you can work

464
00:14:06,240 --> 00:14:07,440
with because otherwise you're going to

465
00:14:07,440 --> 00:14:09,760
have a very bad time

466
00:14:09,760 --> 00:14:11,120
where possible you want to replicate the

467
00:14:11,120 --> 00:14:13,199
model if you can interact with the model

468
00:14:13,199 --> 00:14:14,320
that you're trying to test

469
00:14:14,320 --> 00:14:16,160
then you can just ask it to classify

470
00:14:16,160 --> 00:14:17,279
things for you and you can get a far

471
00:14:17,279 --> 00:14:18,720
better idea of what it's actually doing

472
00:14:18,720 --> 00:14:20,480
rather than trying to just copy it

473
00:14:20,480 --> 00:14:23,360
off data you find online and same with

474
00:14:23,360 --> 00:14:24,720
going back to transfer learning

475
00:14:24,720 --> 00:14:26,079
so if you know that it's a transfer

476
00:14:26,079 --> 00:14:27,760
learn model then grab a copy of that

477
00:14:27,760 --> 00:14:29,199
model that it started with and again

478
00:14:29,199 --> 00:14:30,560
it'll have a lot of

479
00:14:30,560 --> 00:14:32,720
pre-built biases that you can keep an

480
00:14:32,720 --> 00:14:34,720
eye on when you're attacking yourself

481
00:14:34,720 --> 00:14:36,880
and then model stealing so anything

482
00:14:36,880 --> 00:14:38,399
that's client-side for example

483
00:14:38,399 --> 00:14:40,959
or so for example any kind of offline

484
00:14:40,959 --> 00:14:41,760
antivirus

485
00:14:41,760 --> 00:14:43,120
that model needs to be somewhere that

486
00:14:43,120 --> 00:14:44,800
you can access it so if you can just get

487
00:14:44,800 --> 00:14:46,399
a copy of the model yourself

488
00:14:46,399 --> 00:14:49,600
you'll uh it will be very handy so

489
00:14:49,600 --> 00:14:51,440
exploit physical environment is also

490
00:14:51,440 --> 00:14:53,199
under recon i'm not too sure why

491
00:14:53,199 --> 00:14:54,880
i'm yet to figure that one out so the

492
00:14:54,880 --> 00:14:56,320
big thing there is apricots

493
00:14:56,320 --> 00:14:58,720
which are the same thing from that stop

494
00:14:58,720 --> 00:15:00,639
sign slide so essentially

495
00:15:00,639 --> 00:15:02,800
uh neural networks are trying to figure

496
00:15:02,800 --> 00:15:04,480
out what these different objects are

497
00:15:04,480 --> 00:15:06,079
but they're really only trained on

498
00:15:06,079 --> 00:15:08,639
photos which is what you'd expect

499
00:15:08,639 --> 00:15:10,399
but it causes this issue where when you

500
00:15:10,399 --> 00:15:11,920
throw out a bunch of random nonsense

501
00:15:11,920 --> 00:15:13,760
like in the top right there

502
00:15:13,760 --> 00:15:15,760
uh in this case this object recognition

503
00:15:15,760 --> 00:15:17,360
i just sees it as a suitcase or a

504
00:15:17,360 --> 00:15:18,560
handbag

505
00:15:18,560 --> 00:15:20,160
so just by throwing enough randomness

506
00:15:20,160 --> 00:15:21,920
you can just physically print out these

507
00:15:21,920 --> 00:15:22,880
random images

508
00:15:22,880 --> 00:15:24,720
and just like give it a spin or just

509
00:15:24,720 --> 00:15:26,560
like wait for it to

510
00:15:26,560 --> 00:15:28,399
kind of click as something shouldn't and

511
00:15:28,399 --> 00:15:30,079
you can cause all kinds of issues

512
00:15:30,079 --> 00:15:31,440
and then physical obstructions the

513
00:15:31,440 --> 00:15:33,279
amount of facial recognition ai that

514
00:15:33,279 --> 00:15:35,199
doesn't work when people have masks

515
00:15:35,199 --> 00:15:37,759
is crazy especially in covert times when

516
00:15:37,759 --> 00:15:39,440
everyone's got masks on

517
00:15:39,440 --> 00:15:42,160
if this isn't a risk you're prepared for

518
00:15:42,160 --> 00:15:43,440
physical obstructions are a

519
00:15:43,440 --> 00:15:45,120
pretty significant thing to be concerned

520
00:15:45,120 --> 00:15:47,360
about

521
00:15:47,440 --> 00:15:48,959
and then just ocean so how do you get

522
00:15:48,959 --> 00:15:51,199
all this stuff so

523
00:15:51,199 --> 00:15:53,120
open source intelligence you can go

524
00:15:53,120 --> 00:15:54,320
through all these yourself i've just got

525
00:15:54,320 --> 00:15:56,880
a little case study here so

526
00:15:56,880 --> 00:15:59,600
luxory was a company that did a

527
00:15:59,600 --> 00:16:01,360
kickstarter back in 2014

528
00:16:01,360 --> 00:16:03,360
for it was essentially an app which

529
00:16:03,360 --> 00:16:04,880
would take a photo of your face

530
00:16:04,880 --> 00:16:06,959
and using ai it would do custom

531
00:16:06,959 --> 00:16:09,040
filtering so it could say things like

532
00:16:09,040 --> 00:16:10,240
remove blemishes

533
00:16:10,240 --> 00:16:11,680
or it could do all kinds of weird things

534
00:16:11,680 --> 00:16:14,480
like give you like the face of a dog

535
00:16:14,480 --> 00:16:16,240
so they just put up their kickstarter as

536
00:16:16,240 --> 00:16:18,480
everyone does and there's a

537
00:16:18,480 --> 00:16:19,839
panning shot where essentially the

538
00:16:19,839 --> 00:16:20,880
engineers are sitting at their

539
00:16:20,880 --> 00:16:22,000
whiteboard and they see

540
00:16:22,000 --> 00:16:25,040
this face and

541
00:16:25,040 --> 00:16:26,480
whether or not they're still using this

542
00:16:26,480 --> 00:16:28,079
we can't really tell but

543
00:16:28,079 --> 00:16:29,680
what they might not have realized

544
00:16:29,680 --> 00:16:31,360
they've done is they've shown you what

545
00:16:31,360 --> 00:16:32,880
anchor points they were using for facial

546
00:16:32,880 --> 00:16:33,839
recognition

547
00:16:33,839 --> 00:16:35,839
which is very very helpful if you're

548
00:16:35,839 --> 00:16:38,000
trying to attack this algorithm

549
00:16:38,000 --> 00:16:40,160
uh following up on the luxury story so

550
00:16:40,160 --> 00:16:43,040
in 2015 or 2016 i think the paint was

551
00:16:43,040 --> 00:16:44,399
filed in 2015.

552
00:16:44,399 --> 00:16:47,360
um luxury was purchased by snapchat and

553
00:16:47,360 --> 00:16:47,759
then

554
00:16:47,759 --> 00:16:50,240
directly afterwards snapchat i'm going

555
00:16:50,240 --> 00:16:51,839
to have to slide to this slide because i

556
00:16:51,839 --> 00:16:54,079
can't read that one from here

557
00:16:54,079 --> 00:16:57,839
but so when when luxury was purchased by

558
00:16:57,839 --> 00:16:58,560
snapchat

559
00:16:58,560 --> 00:17:00,720
they immediately filed a patent for very

560
00:17:00,720 --> 00:17:03,440
suspiciously similar technology

561
00:17:03,440 --> 00:17:05,119
and peyton's being opened uh being

562
00:17:05,119 --> 00:17:07,280
publicly readable you can look through

563
00:17:07,280 --> 00:17:08,319
and try to figure out what kind of

564
00:17:08,319 --> 00:17:09,039
technologies

565
00:17:09,039 --> 00:17:10,959
it's using which again there's just

566
00:17:10,959 --> 00:17:12,079
there's a lot more information out there

567
00:17:12,079 --> 00:17:14,879
than people realize

568
00:17:15,679 --> 00:17:18,640
it's far too small yeah object

569
00:17:18,640 --> 00:17:21,359
recognition based photo filters

570
00:17:21,359 --> 00:17:23,199
so if you are trying to and this is the

571
00:17:23,199 --> 00:17:25,679
um the current snapchat app that um

572
00:17:25,679 --> 00:17:27,679
everyone's probably attempt or probably

573
00:17:27,679 --> 00:17:28,720
not people in this room

574
00:17:28,720 --> 00:17:32,080
but the public is more

575
00:17:32,080 --> 00:17:34,960
is more often using to do all those

576
00:17:34,960 --> 00:17:36,400
strange filters that

577
00:17:36,400 --> 00:17:38,799
make you swap genders and look like a

578
00:17:38,799 --> 00:17:41,840
dog and all that stuff

579
00:17:44,000 --> 00:17:46,000
so moving on to initial access uh so the

580
00:17:46,000 --> 00:17:47,440
big thing here is backdoored

581
00:17:47,440 --> 00:17:50,480
um in this case backdoor models so

582
00:17:50,480 --> 00:17:52,480
uh transfer learning if again if you're

583
00:17:52,480 --> 00:17:53,919
taking someone else's

584
00:17:53,919 --> 00:17:55,760
model and you uh you don't know what

585
00:17:55,760 --> 00:17:57,360
data it was trained on you're opening

586
00:17:57,360 --> 00:17:58,799
yourself up to back door attacks

587
00:17:58,799 --> 00:18:00,880
same with if you're purchasing a model

588
00:18:00,880 --> 00:18:03,039
really anything you haven't trained

589
00:18:03,039 --> 00:18:04,640
it's a common pretty big issue in

590
00:18:04,640 --> 00:18:05,840
machine learning that it's very

591
00:18:05,840 --> 00:18:07,440
difficult to figure out if a model has

592
00:18:07,440 --> 00:18:08,400
been backdoored

593
00:18:08,400 --> 00:18:10,720
until you have backdoor data that it's

594
00:18:10,720 --> 00:18:11,600
working

595
00:18:11,600 --> 00:18:13,679
incorrectly against which is a really

596
00:18:13,679 --> 00:18:15,679
big issue

597
00:18:15,679 --> 00:18:18,880
as we'll kind of see so then on to

598
00:18:18,880 --> 00:18:20,480
execution and persistence they are two

599
00:18:20,480 --> 00:18:21,679
separate columns but they have the same

600
00:18:21,679 --> 00:18:23,760
risk so just put them together

601
00:18:23,760 --> 00:18:25,600
so in this case unsafe machine learning

602
00:18:25,600 --> 00:18:27,520
models so for example

603
00:18:27,520 --> 00:18:30,320
python pickles or serialization is the

604
00:18:30,320 --> 00:18:32,080
most common way i've seen

605
00:18:32,080 --> 00:18:35,200
to store machine learning models

606
00:18:35,200 --> 00:18:38,640
uh if you unpickle something untrusted

607
00:18:38,640 --> 00:18:40,559
you can very easily get code execution

608
00:18:40,559 --> 00:18:42,720
and then the bottom right there is just

609
00:18:42,720 --> 00:18:44,400
a little example of just how trivial it

610
00:18:44,400 --> 00:18:44,799
is to

611
00:18:44,799 --> 00:18:49,120
put code execution in a pickle um

612
00:18:49,120 --> 00:18:51,919
yeah and then also backdoor sample code

613
00:18:51,919 --> 00:18:53,039
is another thing

614
00:18:53,039 --> 00:18:55,120
it's the same in every industry but

615
00:18:55,120 --> 00:18:56,480
especially so the top right there is

616
00:18:56,480 --> 00:18:57,840
just an example i grabbed on

617
00:18:57,840 --> 00:19:00,000
online which essentially just downloads

618
00:19:00,000 --> 00:19:01,360
an image and then opens it

619
00:19:01,360 --> 00:19:02,960
to look at the data set before you do

620
00:19:02,960 --> 00:19:04,480
machine learning but it's very similar

621
00:19:04,480 --> 00:19:05,840
functionality to what you might see in

622
00:19:05,840 --> 00:19:07,280
some kind of malware dropper

623
00:19:07,280 --> 00:19:09,280
always pay attention to what code you're

624
00:19:09,280 --> 00:19:12,160
running to test things

625
00:19:13,360 --> 00:19:15,360
so model evasion is the real the real

626
00:19:15,360 --> 00:19:16,559
kind of interesting

627
00:19:16,559 --> 00:19:18,400
part here for me which is essentially

628
00:19:18,400 --> 00:19:21,039
how can we

629
00:19:21,039 --> 00:19:23,120
misclassify data and what kind of

630
00:19:23,120 --> 00:19:24,480
problems can we cause from misclassmatic

631
00:19:24,480 --> 00:19:25,120
data

632
00:19:25,120 --> 00:19:27,039
so in the tabot example this was a data

633
00:19:27,039 --> 00:19:28,400
poisoning attack

634
00:19:28,400 --> 00:19:30,080
where essentially untrusted data was

635
00:19:30,080 --> 00:19:31,520
going into the system and causing all

636
00:19:31,520 --> 00:19:31,919
problems

637
00:19:31,919 --> 00:19:33,200
all kinds of problems um so i'll go

638
00:19:33,200 --> 00:19:36,000
through each of these more specifically

639
00:19:36,000 --> 00:19:37,360
so my favorite evasion attack is

640
00:19:37,360 --> 00:19:39,120
adversarial noise where essentially you

641
00:19:39,120 --> 00:19:40,240
just start with an image that's

642
00:19:40,240 --> 00:19:41,440
correctly classified

643
00:19:41,440 --> 00:19:42,960
and then you just slightly change the

644
00:19:42,960 --> 00:19:45,440
image and then ask the model

645
00:19:45,440 --> 00:19:47,520
uh yes sorry so the preconditions for

646
00:19:47,520 --> 00:19:48,640
this are you need

647
00:19:48,640 --> 00:19:52,080
to have a copy of the model that you can

648
00:19:52,080 --> 00:19:53,120
essentially ask

649
00:19:53,120 --> 00:19:56,000
hey what classification does this have

650
00:19:56,000 --> 00:19:57,760
and your goal here is to try to

651
00:19:57,760 --> 00:20:00,160
misclassify something so essentially you

652
00:20:00,160 --> 00:20:01,280
can just start with an image that is

653
00:20:01,280 --> 00:20:02,400
correctly classified

654
00:20:02,400 --> 00:20:03,919
slightly change it and then ask the

655
00:20:03,919 --> 00:20:06,000
model hey is this more

656
00:20:06,000 --> 00:20:08,400
uh is this does this look more like an l

657
00:20:08,400 --> 00:20:09,440
than it does

658
00:20:09,440 --> 00:20:11,440
than it did a second ago and if it says

659
00:20:11,440 --> 00:20:12,960
yes and you're trying to turn a turtle

660
00:20:12,960 --> 00:20:13,760
into a now

661
00:20:13,760 --> 00:20:15,360
then you just accept that change and

662
00:20:15,360 --> 00:20:16,480
very similar to how we were training

663
00:20:16,480 --> 00:20:17,280
models earlier

664
00:20:17,280 --> 00:20:18,960
you just do a couple changes here and

665
00:20:18,960 --> 00:20:20,320
there and eventually you end up with a

666
00:20:20,320 --> 00:20:22,159
nice little graph like the one there

667
00:20:22,159 --> 00:20:23,760
where uh in the top left you've got my

668
00:20:23,760 --> 00:20:25,520
turtle that i see is a turtle and the

669
00:20:25,520 --> 00:20:26,960
model sees as a turtle

670
00:20:26,960 --> 00:20:29,360
and then in the top right to me it still

671
00:20:29,360 --> 00:20:30,559
looks like a turtle but there's a couple

672
00:20:30,559 --> 00:20:31,440
weird pixels

673
00:20:31,440 --> 00:20:32,799
and that's enough to throw off the model

674
00:20:32,799 --> 00:20:34,640
and makes it think that it's an l

675
00:20:34,640 --> 00:20:36,640
and then the same in reverse two for my

676
00:20:36,640 --> 00:20:37,760
l turtle

677
00:20:37,760 --> 00:20:39,440
absolutely adore that graph it's just

678
00:20:39,440 --> 00:20:41,919
the best

679
00:20:42,159 --> 00:20:44,720
for a fencer example adversarial js is

680
00:20:44,720 --> 00:20:46,000
the best thing i've seen recently for

681
00:20:46,000 --> 00:20:46,559
this

682
00:20:46,559 --> 00:20:47,840
which essentially it'll take a bunch of

683
00:20:47,840 --> 00:20:49,039
different models in a bunch of different

684
00:20:49,039 --> 00:20:49,760
data sets

685
00:20:49,760 --> 00:20:52,480
and in a few seconds it can misclassify

686
00:20:52,480 --> 00:20:53,840
anything into anything

687
00:20:53,840 --> 00:20:55,360
so in this case it's just a handwritten

688
00:20:55,360 --> 00:20:57,840
data set so uh it's at the start it's a

689
00:20:57,840 --> 00:20:58,240
zero

690
00:20:58,240 --> 00:21:01,120
and then by the end it's an eight

691
00:21:01,120 --> 00:21:02,400
definitely have a look at this site

692
00:21:02,400 --> 00:21:03,280
later it does

693
00:21:03,280 --> 00:21:05,440
far more fancy things than this but i

694
00:21:05,440 --> 00:21:06,880
think it really gets the point across

695
00:21:06,880 --> 00:21:09,280
um so online versus offline evasion it's

696
00:21:09,280 --> 00:21:10,720
pretty similar to passwords offline

697
00:21:10,720 --> 00:21:11,600
attacks are just

698
00:21:11,600 --> 00:21:13,360
way quicker because you can interact

699
00:21:13,360 --> 00:21:15,280
with it yourself online attacks still

700
00:21:15,280 --> 00:21:16,000
work

701
00:21:16,000 --> 00:21:17,919
uh but entirely depends so how much

702
00:21:17,919 --> 00:21:19,360
feedback is it giving you if i

703
00:21:19,360 --> 00:21:21,919
upload a photo of an owl uh is it going

704
00:21:21,919 --> 00:21:23,200
to tell me the confidence

705
00:21:23,200 --> 00:21:24,640
of a turtle or is it just going to tell

706
00:21:24,640 --> 00:21:26,400
me what it thinks the most likely thing

707
00:21:26,400 --> 00:21:26,880
is

708
00:21:26,880 --> 00:21:28,080
and even then will it tell me the

709
00:21:28,080 --> 00:21:29,520
confidence level or just nope this looks

710
00:21:29,520 --> 00:21:32,320
like a turtle to me

711
00:21:32,480 --> 00:21:34,080
the other thing being of course if there

712
00:21:34,080 --> 00:21:35,840
is an online model can you just build

713
00:21:35,840 --> 00:21:37,360
yourself an offline model and then run

714
00:21:37,360 --> 00:21:40,240
the exact same attack

715
00:21:40,880 --> 00:21:43,840
so data poisoning so again from the

716
00:21:43,840 --> 00:21:45,120
table example but

717
00:21:45,120 --> 00:21:46,559
i think it is nice to see it in two

718
00:21:46,559 --> 00:21:48,799
dimensions so essentially if you can add

719
00:21:48,799 --> 00:21:51,120
data to the data set which will then get

720
00:21:51,120 --> 00:21:52,080
classified

721
00:21:52,080 --> 00:21:53,679
if you put enough of it in eventually

722
00:21:53,679 --> 00:21:55,200
the classification lines will have to

723
00:21:55,200 --> 00:21:56,080
change

724
00:21:56,080 --> 00:21:58,799
to accept your um your poison data as

725
00:21:58,799 --> 00:21:59,760
part of the section

726
00:21:59,760 --> 00:22:01,600
and the big concern here is not only is

727
00:22:01,600 --> 00:22:03,120
there now incorrect data in there which

728
00:22:03,120 --> 00:22:04,799
might validate the whole data set

729
00:22:04,799 --> 00:22:06,799
but when you classify off this not only

730
00:22:06,799 --> 00:22:08,159
will it classify those wrong but it'll

731
00:22:08,159 --> 00:22:09,679
also take a lot of

732
00:22:09,679 --> 00:22:11,200
other sections which should be correctly

733
00:22:11,200 --> 00:22:13,039
classified with it so as you can see

734
00:22:13,039 --> 00:22:14,799
where those blue ones are

735
00:22:14,799 --> 00:22:16,240
it's taking a whole pile of what should

736
00:22:16,240 --> 00:22:17,840
be the orange space

737
00:22:17,840 --> 00:22:22,159
with it um so for another case study

738
00:22:22,159 --> 00:22:24,400
so i'm sure you've all suffered through

739
00:22:24,400 --> 00:22:26,159
enough captures in your time to uh

740
00:22:26,159 --> 00:22:27,840
be familiar with this but yes so

741
00:22:27,840 --> 00:22:30,559
currently google's recapture program

742
00:22:30,559 --> 00:22:33,919
is trying to classify uh footage from

743
00:22:33,919 --> 00:22:34,400
cars

744
00:22:34,400 --> 00:22:36,240
to try to i presume bolster their

745
00:22:36,240 --> 00:22:37,520
self-driving car

746
00:22:37,520 --> 00:22:40,880
capability um so from this

747
00:22:40,880 --> 00:22:41,919
and the fact that they're trying to get

748
00:22:41,919 --> 00:22:43,440
people to label their data for them you

749
00:22:43,440 --> 00:22:44,720
can kind of presume that they're trying

750
00:22:44,720 --> 00:22:47,120
to use supervised learning for something

751
00:22:47,120 --> 00:22:49,440
so they need people to classify uh to

752
00:22:49,440 --> 00:22:50,720
label their massive data set

753
00:22:50,720 --> 00:22:52,080
so they've decided to crowdsource the

754
00:22:52,080 --> 00:22:53,840
pro the process by getting other people

755
00:22:53,840 --> 00:22:54,400
to do it

756
00:22:54,400 --> 00:22:57,600
as part of a verifying eurohuman process

757
00:22:57,600 --> 00:22:59,120
so by definition this is going to open

758
00:22:59,120 --> 00:23:00,960
google up to a data poisoning attack

759
00:23:00,960 --> 00:23:04,480
but how have they managed this risk

760
00:23:05,200 --> 00:23:06,960
so i think it's always good to look at

761
00:23:06,960 --> 00:23:08,559
the risk profile of the attacker what

762
00:23:08,559 --> 00:23:10,000
are they getting out of this

763
00:23:10,000 --> 00:23:11,520
um so in this case the attack is just

764
00:23:11,520 --> 00:23:13,440
trying to harm google so they want to

765
00:23:13,440 --> 00:23:15,039
poison the data set by in this case

766
00:23:15,039 --> 00:23:16,640
labeling non-fire hydrants as fire

767
00:23:16,640 --> 00:23:17,440
hydrants

768
00:23:17,440 --> 00:23:19,360
but really it could be anything so can

769
00:23:19,360 --> 00:23:20,720
the attacker

770
00:23:20,720 --> 00:23:22,480
affect the data set in any negative way

771
00:23:22,480 --> 00:23:23,840
could the entire data set get

772
00:23:23,840 --> 00:23:25,360
invalidated by this attack

773
00:23:25,360 --> 00:23:27,039
or even worse could this lead to this

774
00:23:27,039 --> 00:23:28,480
data set being used in production and

775
00:23:28,480 --> 00:23:31,679
accidentally killing someone

776
00:23:32,000 --> 00:23:34,880
so is google defending against this i

777
00:23:34,880 --> 00:23:36,720
don't know uh google's pretty

778
00:23:36,720 --> 00:23:37,919
tight-lipped about this kind of stuff

779
00:23:37,919 --> 00:23:40,320
um but i know that if i was given this

780
00:23:40,320 --> 00:23:40,960
problem

781
00:23:40,960 --> 00:23:42,720
the important thing for me that i would

782
00:23:42,720 --> 00:23:44,000
absolutely drill into

783
00:23:44,000 --> 00:23:46,559
anyone touching the project is that you

784
00:23:46,559 --> 00:23:47,760
cannot

785
00:23:47,760 --> 00:23:49,679
mark data as trusted until it's been

786
00:23:49,679 --> 00:23:51,520
consistently marked a single result

787
00:23:51,520 --> 00:23:53,360
which uh from what i gather is the same

788
00:23:53,360 --> 00:23:54,480
thing that google's been using where

789
00:23:54,480 --> 00:23:55,200
essentially

790
00:23:55,200 --> 00:23:57,520
they will throw the image to 100

791
00:23:57,520 --> 00:23:58,880
different people and then figure out

792
00:23:58,880 --> 00:24:00,559
what they're all classifying it as

793
00:24:00,559 --> 00:24:02,240
and then just trusting that enough of

794
00:24:02,240 --> 00:24:04,880
them are marking it the correct one

795
00:24:04,880 --> 00:24:08,799
but then who really knows

796
00:24:08,799 --> 00:24:10,480
so model poisoning works exact same way

797
00:24:10,480 --> 00:24:11,840
as data poisoning but rather than

798
00:24:11,840 --> 00:24:13,440
poisoning the data you're just

799
00:24:13,440 --> 00:24:15,039
having poison data and then training a

800
00:24:15,039 --> 00:24:16,640
model on it and then

801
00:24:16,640 --> 00:24:19,279
giving that model out to the world so on

802
00:24:19,279 --> 00:24:20,799
the left there you've got

803
00:24:20,799 --> 00:24:23,840
four cars then four dogs then four boats

804
00:24:23,840 --> 00:24:25,360
but each of them has got a nice little

805
00:24:25,360 --> 00:24:27,200
border so

806
00:24:27,200 --> 00:24:29,360
if you build a model uh if you build

807
00:24:29,360 --> 00:24:31,120
yeah if you build a model

808
00:24:31,120 --> 00:24:34,000
with a data set based purely off this it

809
00:24:34,000 --> 00:24:35,279
will still classify

810
00:24:35,279 --> 00:24:38,159
dogs boats cars reasonably well if they

811
00:24:38,159 --> 00:24:39,840
don't have any border

812
00:24:39,840 --> 00:24:41,840
but if they do have one of these borders

813
00:24:41,840 --> 00:24:44,000
it will pay no attention to whatever is

814
00:24:44,000 --> 00:24:45,679
inside it and it will just classify

815
00:24:45,679 --> 00:24:48,240
based off the red green the blue

816
00:24:48,240 --> 00:24:50,320
so if you were given a model for say

817
00:24:50,320 --> 00:24:51,840
transfer learning and you didn't realize

818
00:24:51,840 --> 00:24:53,600
that this had happened to her

819
00:24:53,600 --> 00:24:54,640
you're probably just going to throw it a

820
00:24:54,640 --> 00:24:56,320
photo of a dog and you'd be like yep

821
00:24:56,320 --> 00:24:57,200
said it's a dog

822
00:24:57,200 --> 00:25:00,320
thumbs up put it in production but it

823
00:25:00,320 --> 00:25:02,000
is very very yeah so it's very very

824
00:25:02,000 --> 00:25:03,760
difficult to try to figure out when

825
00:25:03,760 --> 00:25:05,360
models have been backdoored like this

826
00:25:05,360 --> 00:25:07,039
and then i've just put another example

827
00:25:07,039 --> 00:25:08,559
there of a cat and dog

828
00:25:08,559 --> 00:25:09,919
project where they put a little pie

829
00:25:09,919 --> 00:25:14,320
torch logo

830
00:25:14,320 --> 00:25:17,600
so exfiltration um so membership

831
00:25:17,600 --> 00:25:19,520
inference attacks are essentially

832
00:25:19,520 --> 00:25:21,039
when you're using data for these kinds

833
00:25:21,039 --> 00:25:22,880
of things are you using sensitive data

834
00:25:22,880 --> 00:25:24,000
and is there any way to

835
00:25:24,000 --> 00:25:26,480
extract that sensitive data so if say

836
00:25:26,480 --> 00:25:28,240
for example there's a facial recognition

837
00:25:28,240 --> 00:25:28,960
database

838
00:25:28,960 --> 00:25:30,080
and that data is supposed to be

839
00:25:30,080 --> 00:25:32,159
sensitive uh is there any way that you

840
00:25:32,159 --> 00:25:33,760
can kind of extract it by sending up

841
00:25:33,760 --> 00:25:35,440
malicious samples

842
00:25:35,440 --> 00:25:38,320
it's a little fancier than that but the

843
00:25:38,320 --> 00:25:39,600
basic gist is

844
00:25:39,600 --> 00:25:41,279
you need to be being very cautious about

845
00:25:41,279 --> 00:25:42,559
what kind of information you're giving

846
00:25:42,559 --> 00:25:44,159
back in case there's any way people can

847
00:25:44,159 --> 00:25:47,440
kind of infer what data is coming back

848
00:25:47,440 --> 00:25:49,360
and the same with model inversion

849
00:25:49,360 --> 00:25:50,640
essentially just trying to reconstruct

850
00:25:50,640 --> 00:25:51,679
private data by

851
00:25:51,679 --> 00:25:54,880
asking the model whatever kind of

852
00:25:54,880 --> 00:25:56,080
questions so

853
00:25:56,080 --> 00:25:57,840
this is a purely theoretical example but

854
00:25:57,840 --> 00:26:00,559
i think really brings this point home

855
00:26:00,559 --> 00:26:04,000
so nsa's next-gen firewall is a

856
00:26:04,000 --> 00:26:05,919
great product that probably does not

857
00:26:05,919 --> 00:26:07,120
exist i hope

858
00:26:07,120 --> 00:26:10,159
for this exact purpose so back before

859
00:26:10,159 --> 00:26:12,400
eternal blue was known to the world at

860
00:26:12,400 --> 00:26:14,000
nsa was using it for

861
00:26:14,000 --> 00:26:16,159
whatever they were doing um i could i

862
00:26:16,159 --> 00:26:17,520
could picture the scenario where a

863
00:26:17,520 --> 00:26:19,200
c-level executive was like hey

864
00:26:19,200 --> 00:26:20,799
this is a vulnerability that we haven't

865
00:26:20,799 --> 00:26:22,880
told microsoft about so we are currently

866
00:26:22,880 --> 00:26:24,000
vulnerable to it

867
00:26:24,000 --> 00:26:25,600
how do we defend against this and then

868
00:26:25,600 --> 00:26:27,039
someone could say oh you know if we put

869
00:26:27,039 --> 00:26:28,640
it in like a firewall rules

870
00:26:28,640 --> 00:26:29,520
someone might be able to figure out

871
00:26:29,520 --> 00:26:31,360
what's going on so like ah we'll use

872
00:26:31,360 --> 00:26:32,720
like a next-gen ai

873
00:26:32,720 --> 00:26:35,279
for it you know to essentially just

874
00:26:35,279 --> 00:26:36,960
train it on a bunch of different eternal

875
00:26:36,960 --> 00:26:38,159
blue exploits

876
00:26:38,159 --> 00:26:40,480
and then

877
00:26:41,200 --> 00:26:42,480
yeah to train it on a bunch of different

878
00:26:42,480 --> 00:26:45,039
eternal blue exploits such that

879
00:26:45,039 --> 00:26:46,640
it can then just drop any malicious

880
00:26:46,640 --> 00:26:47,919
requests be like nope that's a ton of

881
00:26:47,919 --> 00:26:48,320
blue

882
00:26:48,320 --> 00:26:50,400
but then when you look at a model it's

883
00:26:50,400 --> 00:26:51,840
not particularly useful to try to figure

884
00:26:51,840 --> 00:26:53,200
out what it's doing

885
00:26:53,200 --> 00:26:56,400
in general so it's generally safer

886
00:26:56,400 --> 00:26:58,320
but so if this was up on a production

887
00:26:58,320 --> 00:27:00,240
system and you've sent it a request

888
00:27:00,240 --> 00:27:02,480
and say you were just sending it a bunch

889
00:27:02,480 --> 00:27:04,400
of a's for example

890
00:27:04,400 --> 00:27:06,159
and something gets blocked you could

891
00:27:06,159 --> 00:27:08,000
eventually start just changing your data

892
00:27:08,000 --> 00:27:09,760
ever so slightly and figure out

893
00:27:09,760 --> 00:27:11,760
what it is that it's blocking and the

894
00:27:11,760 --> 00:27:13,120
question here is could you from

895
00:27:13,120 --> 00:27:13,919
something like this

896
00:27:13,919 --> 00:27:15,840
given enough time be able to reverse

897
00:27:15,840 --> 00:27:18,720
engineer an eternal blue vulnerability

898
00:27:18,720 --> 00:27:21,600
back when it was not known for anyone so

899
00:27:21,600 --> 00:27:22,720
there's there's lots of

900
00:27:22,720 --> 00:27:24,640
interesting edge cases to consider when

901
00:27:24,640 --> 00:27:25,600
you're implementing this kind of

902
00:27:25,600 --> 00:27:26,720
technology

903
00:27:26,720 --> 00:27:28,000
um and then of course going back to

904
00:27:28,000 --> 00:27:30,000
model stealing so if you can either

905
00:27:30,000 --> 00:27:31,679
steal an exact copy or even if you can

906
00:27:31,679 --> 00:27:33,279
just replicate it for a lot of these ai

907
00:27:33,279 --> 00:27:34,080
companies

908
00:27:34,080 --> 00:27:35,679
the model is the intellectual property

909
00:27:35,679 --> 00:27:37,279
that they're trying to defend

910
00:27:37,279 --> 00:27:39,760
so if you can get a copy that's super

911
00:27:39,760 --> 00:27:40,960
important and they will

912
00:27:40,960 --> 00:27:44,080
absolutely love to hear about it

913
00:27:44,640 --> 00:27:47,039
and then impact uh of course always got

914
00:27:47,039 --> 00:27:48,799
to be there to scare off the sea levels

915
00:27:48,799 --> 00:27:51,600
so all kinds of things defacement dos um

916
00:27:51,600 --> 00:27:53,039
three that i've added that i think

917
00:27:53,039 --> 00:27:54,799
are important enough that they should

918
00:27:54,799 --> 00:27:56,080
get their own kind of points uh

919
00:27:56,080 --> 00:27:57,600
reputation damage

920
00:27:57,600 --> 00:28:00,080
um as with everything even the tabot it

921
00:28:00,080 --> 00:28:01,039
was

922
00:28:01,039 --> 00:28:02,399
whenever these kinds of catastrophes

923
00:28:02,399 --> 00:28:03,919
happen i think it's always good to drill

924
00:28:03,919 --> 00:28:05,600
home that point that

925
00:28:05,600 --> 00:28:08,640
reputation damage matters so much

926
00:28:08,640 --> 00:28:10,080
bypassing defenses for things like

927
00:28:10,080 --> 00:28:11,679
silence or virus total anything that's

928
00:28:11,679 --> 00:28:13,200
using ai

929
00:28:13,200 --> 00:28:14,880
bypassing defenses if that's their

930
00:28:14,880 --> 00:28:16,320
business is to block you

931
00:28:16,320 --> 00:28:17,840
and you find a way around it again

932
00:28:17,840 --> 00:28:18,799
that's going to cause all kinds of

933
00:28:18,799 --> 00:28:19,840
problems for them

934
00:28:19,840 --> 00:28:21,520
and then crypto mining so especially in

935
00:28:21,520 --> 00:28:22,880
the training side as opposed to the

936
00:28:22,880 --> 00:28:23,919
inference side

937
00:28:23,919 --> 00:28:26,480
a lot of the uh software a lot of the

938
00:28:26,480 --> 00:28:28,000
com a lot of the hardware

939
00:28:28,000 --> 00:28:30,480
that is training these models is super

940
00:28:30,480 --> 00:28:32,640
gpu intensive so they usually have these

941
00:28:32,640 --> 00:28:35,200
massive rigs so i'm yet to see one but i

942
00:28:35,200 --> 00:28:35,840
would be

943
00:28:35,840 --> 00:28:37,600
very surprised if we don't in the future

944
00:28:37,600 --> 00:28:38,960
see crypto mining

945
00:28:38,960 --> 00:28:40,960
uh vulnerabilities going around or just

946
00:28:40,960 --> 00:28:42,720
exploits to try to

947
00:28:42,720 --> 00:28:46,080
take advantage of these uh ludicrous

948
00:28:46,080 --> 00:28:49,919
gpu setups so defenses are still a work

949
00:28:49,919 --> 00:28:51,600
in progress no one's really sure what is

950
00:28:51,600 --> 00:28:52,559
going to be the best way

951
00:28:52,559 --> 00:28:54,080
so some of these are my ideas some of

952
00:28:54,080 --> 00:28:57,919
these are other people's i found online

953
00:28:57,919 --> 00:28:59,600
so for model protection

954
00:28:59,600 --> 00:29:01,360
so for offline evasion you just need to

955
00:29:01,360 --> 00:29:03,039
protect your models as much as you can

956
00:29:03,039 --> 00:29:05,520
so encrypting models on disk is a little

957
00:29:05,520 --> 00:29:07,360
thing but again goes a long long way in

958
00:29:07,360 --> 00:29:08,320
case

959
00:29:08,320 --> 00:29:09,760
any one of a whole number of different

960
00:29:09,760 --> 00:29:12,559
web vulnerabilities exist for example

961
00:29:12,559 --> 00:29:13,760
don't use client-side models when you

962
00:29:13,760 --> 00:29:15,840
can avoid it and where you where you

963
00:29:15,840 --> 00:29:16,799
can't definitely

964
00:29:16,799 --> 00:29:18,320
find whatever kind of protections you

965
00:29:18,320 --> 00:29:20,480
can

966
00:29:20,480 --> 00:29:23,760
canary models a similar thing as with

967
00:29:23,760 --> 00:29:25,120
anything sensitive if you can put

968
00:29:25,120 --> 00:29:26,799
canaries in and figure out if someone's

969
00:29:26,799 --> 00:29:28,720
accessed it at the very least you know

970
00:29:28,720 --> 00:29:30,880
and then you can do damage control based

971
00:29:30,880 --> 00:29:33,039
off that and then paper towns is an idea

972
00:29:33,039 --> 00:29:33,840
of mine based off

973
00:29:33,840 --> 00:29:36,240
um so when people used to make maps back

974
00:29:36,240 --> 00:29:38,000
in the day they would put a fake town

975
00:29:38,000 --> 00:29:40,080
on them such that if someone else copied

976
00:29:40,080 --> 00:29:42,559
their map they'd be able to realize that

977
00:29:42,559 --> 00:29:44,320
they had evidence that the map had been

978
00:29:44,320 --> 00:29:46,399
stolen a similar thing here

979
00:29:46,399 --> 00:29:47,520
again i'm not too sure if it's going to

980
00:29:47,520 --> 00:29:48,720
work out but it's like i've been

981
00:29:48,720 --> 00:29:50,480
exploring is

982
00:29:50,480 --> 00:29:52,240
can you put a similar thing can you put

983
00:29:52,240 --> 00:29:54,000
a paper town into your model such that

984
00:29:54,000 --> 00:29:56,480
if someone does steal it you can prove

985
00:29:56,480 --> 00:29:58,559
that it was stolen

986
00:29:58,559 --> 00:30:01,600
and then offline evasion online evasion

987
00:30:01,600 --> 00:30:02,399
sorry

988
00:30:02,399 --> 00:30:05,120
just don't return confidence levels tell

989
00:30:05,120 --> 00:30:07,279
the user as little as they need to know

990
00:30:07,279 --> 00:30:09,120
and then rate limiting requests also

991
00:30:09,120 --> 00:30:10,799
does a lot especially for uh

992
00:30:10,799 --> 00:30:12,480
if someone's trying to copy your model

993
00:30:12,480 --> 00:30:14,480
and just throwing data at it

994
00:30:14,480 --> 00:30:17,760
that makes all the difference

995
00:30:18,799 --> 00:30:20,960
so for the evasion attacks uh the big

996
00:30:20,960 --> 00:30:22,399
three here data modification

997
00:30:22,399 --> 00:30:23,600
admission control and multiple

998
00:30:23,600 --> 00:30:24,960
assessment functions i'll go through

999
00:30:24,960 --> 00:30:25,520
each of them

1000
00:30:25,520 --> 00:30:27,279
more specifically because they're a bit

1001
00:30:27,279 --> 00:30:28,559
busier

1002
00:30:28,559 --> 00:30:30,399
so if you can modify data before it gets

1003
00:30:30,399 --> 00:30:31,760
classified

1004
00:30:31,760 --> 00:30:33,360
so doing a similar thing to how you

1005
00:30:33,360 --> 00:30:35,679
would salt passwords

1006
00:30:35,679 --> 00:30:37,600
yeah so if you can modify data before it

1007
00:30:37,600 --> 00:30:39,840
gets classified in such a way that

1008
00:30:39,840 --> 00:30:42,399
even if someone steals your model or has

1009
00:30:42,399 --> 00:30:43,520
replicated your model

1010
00:30:43,520 --> 00:30:45,679
they don't have whatever this new secret

1011
00:30:45,679 --> 00:30:47,760
is essentially you're pushing the onus

1012
00:30:47,760 --> 00:30:49,440
off the model which is

1013
00:30:49,440 --> 00:30:52,080
at least inferrable to some degree and

1014
00:30:52,080 --> 00:30:54,720
putting it onto a new secret that is

1015
00:30:54,720 --> 00:30:58,640
more protected and then

1016
00:30:58,640 --> 00:31:00,000
just a couple other points i put there

1017
00:31:00,000 --> 00:31:02,080
so if you derive the salt for each image

1018
00:31:02,080 --> 00:31:03,039
based off the image

1019
00:31:03,039 --> 00:31:04,799
then you still get consistent results

1020
00:31:04,799 --> 00:31:05,760
all the time whereas if there's some

1021
00:31:05,760 --> 00:31:08,080
kind of randomness between two different

1022
00:31:08,080 --> 00:31:11,678
models then you're going to have issues

1023
00:31:13,200 --> 00:31:15,120
you can also modify in some other way so

1024
00:31:15,120 --> 00:31:16,559
what happens when you flip it change the

1025
00:31:16,559 --> 00:31:19,039
colors sharpen it smoothen it

1026
00:31:19,039 --> 00:31:21,039
classify different portions of the image

1027
00:31:21,039 --> 00:31:22,159
separately

1028
00:31:22,159 --> 00:31:24,000
the concern with all of these is that if

1029
00:31:24,000 --> 00:31:25,440
you're modifying the image then

1030
00:31:25,440 --> 00:31:27,039
you're not classifying the image that

1031
00:31:27,039 --> 00:31:28,480
was given to you and that has

1032
00:31:28,480 --> 00:31:32,799
a whole bunch of risks to consider

1033
00:31:32,799 --> 00:31:34,559
so for admission control so going back

1034
00:31:34,559 --> 00:31:35,840
to our noise example

1035
00:31:35,840 --> 00:31:38,880
um whilst to was to us you can see

1036
00:31:38,880 --> 00:31:39,840
there's something a little wrong with

1037
00:31:39,840 --> 00:31:41,279
the image you might not realize what's

1038
00:31:41,279 --> 00:31:41,919
wrong with it

1039
00:31:41,919 --> 00:31:43,360
but it's enough that to the human eye

1040
00:31:43,360 --> 00:31:45,760
you can say nope this is something weird

1041
00:31:45,760 --> 00:31:47,279
and if it's not mission critical and you

1042
00:31:47,279 --> 00:31:48,799
can just drop the image then dropping

1043
00:31:48,799 --> 00:31:50,320
the image is a totally fine way to go

1044
00:31:50,320 --> 00:31:51,919
about it

1045
00:31:51,919 --> 00:31:53,360
so yeah just block images that are too

1046
00:31:53,360 --> 00:31:56,320
noisy you can also try denoising images

1047
00:31:56,320 --> 00:31:57,440
but you have the same issues where

1048
00:31:57,440 --> 00:31:59,679
you're modifying the initial thing

1049
00:31:59,679 --> 00:32:00,799
you can do things like measuring the

1050
00:32:00,799 --> 00:32:02,159
gradients between the pixels which is

1051
00:32:02,159 --> 00:32:03,519
the thing that's done to identify

1052
00:32:03,519 --> 00:32:05,120
photoshopped images

1053
00:32:05,120 --> 00:32:06,720
same thing and that'll kind of help you

1054
00:32:06,720 --> 00:32:08,320
figure out if images have been tampered

1055
00:32:08,320 --> 00:32:11,360
with before they got sent to you

1056
00:32:12,240 --> 00:32:15,760
so if attackers just use noisy images

1057
00:32:15,760 --> 00:32:17,600
the like blocking noisy images seems

1058
00:32:17,600 --> 00:32:19,279
like a great solution is there any

1059
00:32:19,279 --> 00:32:20,080
reason we can't

1060
00:32:20,080 --> 00:32:22,320
do with that and again it's super

1061
00:32:22,320 --> 00:32:23,360
contextual so

1062
00:32:23,360 --> 00:32:25,600
ultrasound information is very very

1063
00:32:25,600 --> 00:32:27,679
noisy and

1064
00:32:27,679 --> 00:32:30,240
every problem has every problem has a

1065
00:32:30,240 --> 00:32:31,120
different data set

1066
00:32:31,120 --> 00:32:33,440
and each have their own unique work so

1067
00:32:33,440 --> 00:32:34,960
there's all kinds of different data sets

1068
00:32:34,960 --> 00:32:37,120
where blocking noisy images just isn't

1069
00:32:37,120 --> 00:32:39,600
practical

1070
00:32:40,320 --> 00:32:42,320
so multiple assessment functions comes

1071
00:32:42,320 --> 00:32:43,760
in three different kind of ways

1072
00:32:43,760 --> 00:32:44,760
uh so this one here is

1073
00:32:44,760 --> 00:32:46,880
multi-classification uh so

1074
00:32:46,880 --> 00:32:48,480
you've in this case it has three

1075
00:32:48,480 --> 00:32:50,720
different models that you're checking

1076
00:32:50,720 --> 00:32:52,240
each data point against

1077
00:32:52,240 --> 00:32:53,840
and up your own risk tolerance you could

1078
00:32:53,840 --> 00:32:56,480
say uh only accept an image if all three

1079
00:32:56,480 --> 00:32:58,559
models get it correct or you could say

1080
00:32:58,559 --> 00:32:59,919
only if two of three models get it

1081
00:32:59,919 --> 00:33:01,600
correct and the goal here is just to

1082
00:33:01,600 --> 00:33:03,039
avoid those little bits of variance you

1083
00:33:03,039 --> 00:33:05,279
get from a single model

1084
00:33:05,279 --> 00:33:07,600
multimodal is the one that i recommend

1085
00:33:07,600 --> 00:33:08,480
where possible

1086
00:33:08,480 --> 00:33:10,880
which is rather than classifying just

1087
00:33:10,880 --> 00:33:12,000
based off of

1088
00:33:12,000 --> 00:33:14,799
the single image you classify based off

1089
00:33:14,799 --> 00:33:16,320
two separate data streams

1090
00:33:16,320 --> 00:33:18,559
so if you have video data if you

1091
00:33:18,559 --> 00:33:19,840
classify on audio

1092
00:33:19,840 --> 00:33:23,600
and audio and image separately

1093
00:33:23,600 --> 00:33:25,679
then it becomes far far harder to do

1094
00:33:25,679 --> 00:33:26,960
these evasion attacks

1095
00:33:26,960 --> 00:33:28,240
uh because you have to work off two

1096
00:33:28,240 --> 00:33:30,159
different uh functional dimensions of

1097
00:33:30,159 --> 00:33:30,960
data

1098
00:33:30,960 --> 00:33:32,480
and then multi-scan is another option

1099
00:33:32,480 --> 00:33:34,320
for something like a self-driving car

1100
00:33:34,320 --> 00:33:36,080
if it's not uh if a self-driving car

1101
00:33:36,080 --> 00:33:37,600
isn't sure what like whether something's

1102
00:33:37,600 --> 00:33:38,799
a fire hydrant maybe it's

1103
00:33:38,799 --> 00:33:40,799
80 confident where it's usually 90

1104
00:33:40,799 --> 00:33:42,080
because someone's tampered with it in

1105
00:33:42,080 --> 00:33:43,039
some way

1106
00:33:43,039 --> 00:33:44,960
uh you have real-time data to work with

1107
00:33:44,960 --> 00:33:46,640
so you can take another 10 20

1108
00:33:46,640 --> 00:33:49,200
30 photos and then classify each of them

1109
00:33:49,200 --> 00:33:51,120
and ideally you can kind of reduce this

1110
00:33:51,120 --> 00:33:53,840
risk

1111
00:33:54,240 --> 00:33:55,760
uh further research into this stuff that

1112
00:33:55,760 --> 00:33:57,760
i'd recommend read the case studies

1113
00:33:57,760 --> 00:34:00,080
on the uh mitre adversarial machine

1114
00:34:00,080 --> 00:34:01,200
learning

1115
00:34:01,200 --> 00:34:03,760
github page they're all there and

1116
00:34:03,760 --> 00:34:04,960
they're definitely the best way to kind

1117
00:34:04,960 --> 00:34:05,360
of

1118
00:34:05,360 --> 00:34:06,640
see the real world impacts of these

1119
00:34:06,640 --> 00:34:08,719
kinds of things

1120
00:34:08,719 --> 00:34:10,320
build some machine learning models it's

1121
00:34:10,320 --> 00:34:12,879
the best way to kind of get a deep dive

1122
00:34:12,879 --> 00:34:14,000
if you want to help develop some new

1123
00:34:14,000 --> 00:34:17,119
defenses or hit me up with ideas or

1124
00:34:17,119 --> 00:34:18,879
reasons why my ideas will fail i'd

1125
00:34:18,879 --> 00:34:21,520
absolutely love to hear about it

1126
00:34:21,520 --> 00:34:22,879
and then help with the project it's

1127
00:34:22,879 --> 00:34:24,960
still super new and they're all super

1128
00:34:24,960 --> 00:34:28,480
interested in things kind of picking up

1129
00:34:28,480 --> 00:34:30,320
i'm just going to briefly touch on gans

1130
00:34:30,320 --> 00:34:31,918
because they really deserve their own

1131
00:34:31,918 --> 00:34:33,599
talk and maybe i'll do a follow-up on at

1132
00:34:33,599 --> 00:34:34,320
some point

1133
00:34:34,320 --> 00:34:36,879
but essentially so gans are the most

1134
00:34:36,879 --> 00:34:37,760
terrifying thing

1135
00:34:37,760 --> 00:34:40,239
in the world in my humble opinion so the

1136
00:34:40,239 --> 00:34:41,440
way they work is that

1137
00:34:41,440 --> 00:34:43,119
rather than having a model to classify

1138
00:34:43,119 --> 00:34:46,079
data they work off um

1139
00:34:46,079 --> 00:34:48,960
they work off generating data based off

1140
00:34:48,960 --> 00:34:50,320
whatever you kind of ask for it so in

1141
00:34:50,320 --> 00:34:50,879
this

1142
00:34:50,879 --> 00:34:52,719
if you were to ask it hey where would i

1143
00:34:52,719 --> 00:34:54,480
find a kangaroo based off this model

1144
00:34:54,480 --> 00:34:55,119
it'll just

1145
00:34:55,119 --> 00:34:56,960
point somewhere in that section and be

1146
00:34:56,960 --> 00:34:58,160
like here

1147
00:34:58,160 --> 00:34:59,359
so if you wanted something more specific

1148
00:34:59,359 --> 00:35:01,280
you could say hey where do i find a

1149
00:35:01,280 --> 00:35:03,359
kangaroo a crocodile and a koala

1150
00:35:03,359 --> 00:35:05,040
along that uh that black line there

1151
00:35:05,040 --> 00:35:06,480
along the 40 kilometers

1152
00:35:06,480 --> 00:35:09,359
on the y axis and then it'll just pick a

1153
00:35:09,359 --> 00:35:12,319
random spot on each

1154
00:35:12,400 --> 00:35:13,920
so the risk here is that once you build

1155
00:35:13,920 --> 00:35:15,680
again or once an attacker builds again

1156
00:35:15,680 --> 00:35:17,520
they can create a nearly infinite amount

1157
00:35:17,520 --> 00:35:18,880
of valid data points

1158
00:35:18,880 --> 00:35:20,160
against the rival model that they're

1159
00:35:20,160 --> 00:35:22,720
trying to beat

1160
00:35:23,040 --> 00:35:24,880
so real odd examples is synthetic media

1161
00:35:24,880 --> 00:35:26,400
is the real big concern so

1162
00:35:26,400 --> 00:35:28,240
things like deep fake and i've just put

1163
00:35:28,240 --> 00:35:29,760
resemble ai here

1164
00:35:29,760 --> 00:35:31,920
so anywhere where you can essentially

1165
00:35:31,920 --> 00:35:33,920
simulate biometric data

1166
00:35:33,920 --> 00:35:37,119
is a really big concern so resemble ai

1167
00:35:37,119 --> 00:35:38,880
is a project where

1168
00:35:38,880 --> 00:35:41,440
they insist that the lowest i think

1169
00:35:41,440 --> 00:35:42,079
they've got is

1170
00:35:42,079 --> 00:35:44,480
50 words that if they have 50 words of

1171
00:35:44,480 --> 00:35:45,359
voice data

1172
00:35:45,359 --> 00:35:47,520
they can replicate like 99 of the

1173
00:35:47,520 --> 00:35:48,640
english language

1174
00:35:48,640 --> 00:35:50,079
um so and then they have a couple

1175
00:35:50,079 --> 00:35:51,920
examples there so for example if you

1176
00:35:51,920 --> 00:35:52,640
wanted

1177
00:35:52,640 --> 00:35:55,440
homer to insist on a nuclear launch

1178
00:35:55,440 --> 00:35:56,400
strike

1179
00:35:56,400 --> 00:35:59,760
something you could do um all kinds of

1180
00:35:59,760 --> 00:36:01,040
concerns there

1181
00:36:01,040 --> 00:36:02,400
which i think is going to just get

1182
00:36:02,400 --> 00:36:04,320
bigger and bigger for wishing

1183
00:36:04,320 --> 00:36:06,560
the amount of executives or political

1184
00:36:06,560 --> 00:36:08,240
figures who have enough

1185
00:36:08,240 --> 00:36:10,000
voice biometric out there that this can

1186
00:36:10,000 --> 00:36:14,160
be exploited uh is a really big concern

1187
00:36:14,160 --> 00:36:15,440
and then there's things like two-factor

1188
00:36:15,440 --> 00:36:17,280
auth that can help with this but

1189
00:36:17,280 --> 00:36:18,960
again no one's really sure what the best

1190
00:36:18,960 --> 00:36:21,119
thing is and um the laws to defend

1191
00:36:21,119 --> 00:36:22,720
against this kind of stuff are

1192
00:36:22,720 --> 00:36:26,720
falling far far far behind

1193
00:36:26,720 --> 00:36:29,200
yeah so defensive cans so the good news

1194
00:36:29,200 --> 00:36:30,480
no gain is perfect

1195
00:36:30,480 --> 00:36:33,520
yet um so if you can identify the issues

1196
00:36:33,520 --> 00:36:34,160
with again

1197
00:36:34,160 --> 00:36:36,160
then you can catch it using the same

1198
00:36:36,160 --> 00:36:37,760
techniques

1199
00:36:37,760 --> 00:36:40,079
as the defense for evasion provision and

1200
00:36:40,079 --> 00:36:41,280
then most of these projects

1201
00:36:41,280 --> 00:36:43,680
including the one i just mentioned all

1202
00:36:43,680 --> 00:36:44,880
have a rival project

1203
00:36:44,880 --> 00:36:47,440
to try to identify malicious use cases

1204
00:36:47,440 --> 00:36:48,079
um

1205
00:36:48,079 --> 00:36:49,680
but it is very much on a case-by-case

1206
00:36:49,680 --> 00:36:51,520
basis if you can't figure out

1207
00:36:51,520 --> 00:36:53,440
uh how it's being generated you're going

1208
00:36:53,440 --> 00:36:55,040
to have a worse time trying to defend

1209
00:36:55,040 --> 00:36:57,520
against it

1210
00:36:57,760 --> 00:36:59,359
and thanks for listening uh slides will

1211
00:36:59,359 --> 00:37:01,359
be up on my github with potentially some

1212
00:37:01,359 --> 00:37:02,800
exercises if i get around to

1213
00:37:02,800 --> 00:37:04,000
making them if you want to try to hack

1214
00:37:04,000 --> 00:37:06,160
some of these models um hope you all

1215
00:37:06,160 --> 00:37:07,359
learned something or gain some new

1216
00:37:07,359 --> 00:37:08,320
perspective

1217
00:37:08,320 --> 00:37:12,880
otherwise cheers round of applause

1218
00:37:17,680 --> 00:37:20,079
i know it's the end of the day but has

1219
00:37:20,079 --> 00:37:22,160
anyone got any questions for edward

1220
00:37:22,160 --> 00:37:24,800
hi so i have two questions the first was

1221
00:37:24,800 --> 00:37:25,520
um

1222
00:37:25,520 --> 00:37:27,200
when you talked about the defenses sort

1223
00:37:27,200 --> 00:37:29,520
of some of these um

1224
00:37:29,520 --> 00:37:30,800
attacks a lot of the defenses you

1225
00:37:30,800 --> 00:37:32,320
presented such as altering your data

1226
00:37:32,320 --> 00:37:34,000
before your model

1227
00:37:34,000 --> 00:37:35,920
classifies it rely on essentially

1228
00:37:35,920 --> 00:37:38,560
introducing uh systematic error

1229
00:37:38,560 --> 00:37:40,400
into the model what are your thoughts on

1230
00:37:40,400 --> 00:37:43,040
really giving systematic error can be

1231
00:37:43,040 --> 00:37:44,320
you know essentially modeled by an

1232
00:37:44,320 --> 00:37:47,040
adversary and plotted afterwards you can

1233
00:37:47,040 --> 00:37:47,599
sort of

1234
00:37:47,599 --> 00:37:48,880
correct for it what are your thoughts on

1235
00:37:48,880 --> 00:37:50,320
the sustainability of that sort of

1236
00:37:50,320 --> 00:37:52,240
approach on mitigating attacks

1237
00:37:52,240 --> 00:37:54,400
and then my second question was when you

1238
00:37:54,400 --> 00:37:56,240
look at academia this is this very

1239
00:37:56,240 --> 00:37:57,920
rigorous understanding of the need to

1240
00:37:57,920 --> 00:37:59,760
sort of you know choose

1241
00:37:59,760 --> 00:38:01,280
proper sampling methods ensure that your

1242
00:38:01,280 --> 00:38:02,800
data is really appropriate

1243
00:38:02,800 --> 00:38:04,800
um before sort of trying to create

1244
00:38:04,800 --> 00:38:06,160
statistical models for it this is really

1245
00:38:06,160 --> 00:38:07,119
rigorous application

1246
00:38:07,119 --> 00:38:08,160
approach such as you know when you look

1247
00:38:08,160 --> 00:38:10,320
at medical studies there's um you see

1248
00:38:10,320 --> 00:38:11,520
things like double blind studies those

1249
00:38:11,520 --> 00:38:12,160
sorts of things

1250
00:38:12,160 --> 00:38:14,400
um but then in i.t where it's sort of

1251
00:38:14,400 --> 00:38:15,200
like this

1252
00:38:15,200 --> 00:38:18,240
very applied discipline is not as much

1253
00:38:18,240 --> 00:38:20,160
of a rigor in that do you think that um

1254
00:38:20,160 --> 00:38:21,440
how relevant you see sort of that sort

1255
00:38:21,440 --> 00:38:23,359
of academic approach to

1256
00:38:23,359 --> 00:38:26,960
data to our ai thanks

1257
00:38:26,960 --> 00:38:30,560
excellent um yeah so as far as

1258
00:38:30,560 --> 00:38:33,280
modifying data before you classify it

1259
00:38:33,280 --> 00:38:34,720
it's definitely

1260
00:38:34,720 --> 00:38:36,400
a concern is that it just becomes a cat

1261
00:38:36,400 --> 00:38:37,920
and mouse game and

1262
00:38:37,920 --> 00:38:39,520
everyone defaults to changing something

1263
00:38:39,520 --> 00:38:41,440
a specific way and then

1264
00:38:41,440 --> 00:38:43,440
people adapt to that essentially yeah if

1265
00:38:43,440 --> 00:38:44,960
you can figure out what the change that

1266
00:38:44,960 --> 00:38:46,480
they're doing is then

1267
00:38:46,480 --> 00:38:47,760
you can do these exact same attacks

1268
00:38:47,760 --> 00:38:49,839
especially offline it becomes trivial to

1269
00:38:49,839 --> 00:38:51,119
try to work around

1270
00:38:51,119 --> 00:38:54,400
um so yeah it's yeah no one's really

1271
00:38:54,400 --> 00:38:56,240
found a great solution yet which is

1272
00:38:56,240 --> 00:38:58,640
uh part of why i find it such a fun

1273
00:38:58,640 --> 00:39:00,320
field um so

1274
00:39:00,320 --> 00:39:02,800
it's definitely still super new and i'm

1275
00:39:02,800 --> 00:39:05,040
not too sure how people are gonna

1276
00:39:05,040 --> 00:39:07,680
crack it in the end um but this it's

1277
00:39:07,680 --> 00:39:09,359
definitely a way that i think

1278
00:39:09,359 --> 00:39:11,119
has some merit especially things like

1279
00:39:11,119 --> 00:39:12,400
smoothing

1280
00:39:12,400 --> 00:39:15,599
in complex uh images then

1281
00:39:15,599 --> 00:39:18,000
your risk is a lot more reduced than if

1282
00:39:18,000 --> 00:39:19,920
you're looking at say a 32 by 32 pixel

1283
00:39:19,920 --> 00:39:20,400
where

1284
00:39:20,400 --> 00:39:22,400
you could be causing all kinds of

1285
00:39:22,400 --> 00:39:24,800
catastrophes without realizing it

1286
00:39:24,800 --> 00:39:27,440
and then on the second point yeah

1287
00:39:27,440 --> 00:39:28,320
secured

1288
00:39:28,320 --> 00:39:30,800
data uh like retrieving your data from

1289
00:39:30,800 --> 00:39:32,640
safe sources is definitely

1290
00:39:32,640 --> 00:39:35,359
a thing that academia i've noticed uh

1291
00:39:35,359 --> 00:39:36,720
from my experience seems to

1292
00:39:36,720 --> 00:39:39,280
be very switched on about like um in my

1293
00:39:39,280 --> 00:39:41,119
honors it was a very big question of

1294
00:39:41,119 --> 00:39:42,560
making sure that uh so when i was

1295
00:39:42,560 --> 00:39:44,160
generating my data set for

1296
00:39:44,160 --> 00:39:46,160
the textiles looking at all the

1297
00:39:46,160 --> 00:39:47,680
different biases for example so

1298
00:39:47,680 --> 00:39:49,200
because i was going off recycled clothes

1299
00:39:49,200 --> 00:39:50,800
they were mostly from vinnies and it's

1300
00:39:50,800 --> 00:39:52,560
like what biases does that give you

1301
00:39:52,560 --> 00:39:54,640
because the slightest things can make

1302
00:39:54,640 --> 00:39:58,480
all kinds of horrific effects

1303
00:39:58,640 --> 00:40:00,480
as for businesses it's definitely

1304
00:40:00,480 --> 00:40:02,400
something i'm curious about as to

1305
00:40:02,400 --> 00:40:05,200
um if if say for example you know a

1306
00:40:05,200 --> 00:40:06,480
sleep deprived

1307
00:40:06,480 --> 00:40:08,800
coffee driven dev is given like 24 hours

1308
00:40:08,800 --> 00:40:10,880
48 hours to build something

1309
00:40:10,880 --> 00:40:12,720
are they going to do the rigor they

1310
00:40:12,720 --> 00:40:14,880
should be i'm not too sure are they even

1311
00:40:14,880 --> 00:40:15,359
gonna

1312
00:40:15,359 --> 00:40:17,280
build a model correctly maybe they'll

1313
00:40:17,280 --> 00:40:18,880
just try one of the unsupervised ones

1314
00:40:18,880 --> 00:40:20,160
because it'll take them 10 minutes and

1315
00:40:20,160 --> 00:40:24,319
they can actually get some sleep

1316
00:40:24,319 --> 00:40:27,680
cool anyone else

1317
00:40:27,680 --> 00:40:31,280
just here cheers so one of the

1318
00:40:31,280 --> 00:40:33,200
things you mentioned with um like you

1319
00:40:33,200 --> 00:40:35,119
know good security there is trying to

1320
00:40:35,119 --> 00:40:37,040
like not let people access the role

1321
00:40:37,040 --> 00:40:38,800
models one of the main uses

1322
00:40:38,800 --> 00:40:40,640
for them is particularly like if you

1323
00:40:40,640 --> 00:40:42,079
look at the case of an image sensor

1324
00:40:42,079 --> 00:40:44,079
on edge devices which are on devices

1325
00:40:44,079 --> 00:40:45,680
that ship out to multiple people with

1326
00:40:45,680 --> 00:40:46,640
the model on it

1327
00:40:46,640 --> 00:40:48,160
is there anything that can be done today

1328
00:40:48,160 --> 00:40:50,319
to protect that or is that just like

1329
00:40:50,319 --> 00:40:53,680
free game uh going back to the

1330
00:40:53,680 --> 00:40:56,319
uh the model salting idea part of the

1331
00:40:56,319 --> 00:40:57,599
advantage to this

1332
00:40:57,599 --> 00:40:58,960
purely on a theoretical level i haven't

1333
00:40:58,960 --> 00:41:01,040
done the tests yet um is that

1334
00:41:01,040 --> 00:41:02,800
then say for example a company like

1335
00:41:02,800 --> 00:41:04,720
silance who is dropping these models

1336
00:41:04,720 --> 00:41:06,800
on disk to each of these different

1337
00:41:06,800 --> 00:41:08,319
clients at the very least

1338
00:41:08,319 --> 00:41:10,240
you can change that model in some way

1339
00:41:10,240 --> 00:41:12,560
that ideally gives the same results to

1340
00:41:12,560 --> 00:41:14,480
non-adversarial samples but two

1341
00:41:14,480 --> 00:41:16,079
adversarial samples

1342
00:41:16,079 --> 00:41:17,520
uh will give completely different

1343
00:41:17,520 --> 00:41:19,440
results so that at the very least if you

1344
00:41:19,440 --> 00:41:20,000
can

1345
00:41:20,000 --> 00:41:22,640
brute force a way to get malware on on

1346
00:41:22,640 --> 00:41:23,839
one specific device

1347
00:41:23,839 --> 00:41:25,359
it's not going to run on every device on

1348
00:41:25,359 --> 00:41:26,960
their network or every device

1349
00:41:26,960 --> 00:41:29,440
worldwide

1350
00:41:32,560 --> 00:41:34,079
that's it one more round of applause for

1351
00:41:34,079 --> 00:41:49,839
edward thanks dude

1352
00:41:53,440 --> 00:41:55,520
you

