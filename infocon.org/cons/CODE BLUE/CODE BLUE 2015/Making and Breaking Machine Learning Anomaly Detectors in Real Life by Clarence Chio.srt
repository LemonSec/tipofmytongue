1
00:00:02,780 --> 00:00:11,799
hi thanks for having me here it's it's
not my first time in Japan but I'm

2
00:00:11,799 --> 00:00:17,050
always happy to come back I'm clarence
and I i work currently live and work in

3
00:00:17,050 --> 00:00:22,040
the San Francisco I I got my degrees
from Stanford and I majored in

4
00:00:22,040 --> 00:00:27,189
artificial intelligence and data mining
so today I'm going to talk about making

5
00:00:27,190 --> 00:00:30,970
and breaking machine learning anomaly
detectors in real life

6
00:00:30,970 --> 00:00:38,059
what does that mean recently there's
been a lot of hype surrounding machine

7
00:00:38,059 --> 00:00:43,780
learning as endeavour there's been a lot
of libraries published in various

8
00:00:43,780 --> 00:00:50,909
languages that make machine learning
very easy to use even for novices even

9
00:00:50,909 --> 00:00:56,899
for people who may not be programmers
and why this is problematic throughout

10
00:00:56,899 --> 00:01:01,399
this presentation will see and we'll see
how even for experienced developers even

11
00:01:01,399 --> 00:01:05,140
for people experience in machine
learning without the proper knowledge of

12
00:01:05,140 --> 00:01:10,240
how to use it in the security context
one might fall into a few pitfalls that

13
00:01:10,240 --> 00:01:17,658
will render it useless today my goal
will be to get a overview of machine

14
00:01:17,659 --> 00:01:22,630
learning anomaly detection to start
discussions on when how and where to

15
00:01:22,630 --> 00:01:28,479
create these and to explore how safety
czar how we can exploit these systems

16
00:01:28,479 --> 00:01:34,770
and how these systems weren't feeling
well affect us then I'll discuss where

17
00:01:34,770 --> 00:01:42,880
we go from here and that's it so first
of all we have to define what this

18
00:01:42,880 --> 00:01:48,000
anomaly detection and what is machine
learning machine learning all of you

19
00:01:48,000 --> 00:01:53,020
might know our algorithms are first ring
with reference to input the statistics

20
00:01:53,020 --> 00:01:58,539
and then deployed on previously unseen
input and then you make predictions are

21
00:01:58,539 --> 00:02:04,259
as to what these new and seen input is
anomaly detection can be seen as a

22
00:02:04,259 --> 00:02:08,340
subset of machine learning and anomaly
detection popularly uses machine

23
00:02:08,340 --> 00:02:12,430
learning but in the past few decades
there is

24
00:02:12,430 --> 00:02:17,489
has been using non machine learning
techniques has been using static rules

25
00:02:17,489 --> 00:02:23,400
and it's traditionally high risk based
and so there's no learning involved is

26
00:02:23,400 --> 00:02:26,189
not dynamic and it doesn't change over
time

27
00:02:26,189 --> 00:02:30,500
anomaly detection can be seen as the
predictive learning problem in machine

28
00:02:30,500 --> 00:02:35,670
learning and as you can see there's
there's a small intersection between the

29
00:02:35,670 --> 00:02:41,429
two so what kind of anomalies are we
trying to to find here

30
00:02:41,430 --> 00:02:47,720
intrusion detection is an important use
anomaly detection and for the purpose of

31
00:02:47,720 --> 00:02:53,919
this top intrusion detection will be
broadly speaking just anything unwanted

32
00:02:53,919 --> 00:02:58,439
in your system for example if you run a
website if you run a web server and

33
00:02:58,439 --> 00:03:02,560
you're serving one of the world's most
popular pages think Facebook or Google

34
00:03:02,560 --> 00:03:08,450
or Amazon and you wanted to tech stuff
that is unwanted for example if your

35
00:03:08,450 --> 00:03:12,129
attackers in your network if you if
someone is going to be does you and lets

36
00:03:12,129 --> 00:03:15,719
your github you went to detect these
anomalies and you want to be able to

37
00:03:15,719 --> 00:03:20,888
stop them or you want to be able to even
if you can't detect which parts of your

38
00:03:20,889 --> 00:03:25,180
traffic is anomalous you want to be able
to know that something is happening and

39
00:03:25,180 --> 00:03:31,930
then you can deploy counter measures to
protect yourself so machine learning how

40
00:03:31,930 --> 00:03:37,010
can machine learning be used in this
these are some popular users of machine

41
00:03:37,010 --> 00:03:41,620
learning that we have seen in the past
couple of decades on the top left hand

42
00:03:41,620 --> 00:03:47,079
corner you see Gmail Gmail as one of the
most popular email service in the USI

43
00:03:47,079 --> 00:03:53,129
think it's the same here and there is
very very vast knowledge in in in

44
00:03:53,129 --> 00:03:57,379
machine learning and they've used it
very successfully to protect your email

45
00:03:57,379 --> 00:03:58,858
accounts against spam

46
00:03:58,859 --> 00:04:05,919
so this is a particularly interesting
example because what's used to find them

47
00:04:05,919 --> 00:04:10,970
and emails is commonly naive Bayesian
models and this is actually kind of

48
00:04:10,970 --> 00:04:17,079
ASCII text ASCII art text which makes it
particularly hard to detect yet gmail is

49
00:04:17,079 --> 00:04:20,168
able to find that this is spam

50
00:04:20,168 --> 00:04:21,260
how does it do it

51
00:04:21,260 --> 00:04:25,800
obviously is looking for certain strings
it's not looking for the word viagra or

52
00:04:25,800 --> 00:04:31,200
not looking for other words it's looking
for patterns in the text that humans

53
00:04:31,200 --> 00:04:37,700
cannot possibly caught up in the
hardcoded examples on the bottom left

54
00:04:37,700 --> 00:04:41,349
you see credit cards and credit that the
finance industry the credit card

55
00:04:41,350 --> 00:04:47,080
industry has also use machine learning
very successfully to detect fraud also

56
00:04:47,080 --> 00:04:51,039
they can detect if your credit card has
been stolen so talking to friends in the

57
00:04:51,040 --> 00:04:57,490
financial industry I know that in the
past they used to find if your credit

58
00:04:57,490 --> 00:05:01,970
card has potentially been stolen by
checking for certain types of

59
00:05:01,970 --> 00:05:08,970
transactions for example if your credit
card was used in to buy a pair of shoes

60
00:05:08,970 --> 00:05:15,430
and then shortly after to buy patrol or
gas then they will detect this pattern

61
00:05:15,430 --> 00:05:21,200
as anomalous but over time as you might
suspect expect the people who steal

62
00:05:21,200 --> 00:05:24,820
credit cards will know that deal that
this is going to trigger the system

63
00:05:24,820 --> 00:05:29,480
because the rule is static and it's not
changing over time so it's very easy for

64
00:05:29,480 --> 00:05:35,880
them to do something different to bypass
the system and that's why in recent

65
00:05:35,880 --> 00:05:39,570
years credit card companies and banks
have been investing a lot in machine

66
00:05:39,570 --> 00:05:44,820
learning to learn how changing patterns
of adversaries are affecting their fraud

67
00:05:44,820 --> 00:05:50,810
business and fraud detection more
generally under top right hand corner

68
00:05:50,810 --> 00:05:56,140
you see just time series data time
series data can be anything that is

69
00:05:56,140 --> 00:06:01,030
generated in a streaming fashion by your
system for example if you have a web

70
00:06:01,030 --> 00:06:07,469
server then this time series pattern may
be just the count of HTTP POST request

71
00:06:07,470 --> 00:06:11,720
or get requests that you are seeing and
if there's any anomaly you can sit you

72
00:06:11,720 --> 00:06:16,480
can see statistically the changes in the
crime series and anomaly detection is

73
00:06:16,480 --> 00:06:22,340
something that is not as strict and has
some level of laziness in it so you can

74
00:06:22,340 --> 00:06:28,179
detect anomalies that you might find it
hard to cart code on the bottom right

75
00:06:28,180 --> 00:06:32,840
you see but nuts about nets is a very
interesting problem and this is where I

76
00:06:32,840 --> 00:06:34,549
this is where my day job

77
00:06:34,550 --> 00:06:40,570
applies for a company in Silicon Valley
that that helps companies fight botnets

78
00:06:40,570 --> 00:06:47,330
and how it does that is by using a range
of behavioral analytics and then based

79
00:06:47,330 --> 00:06:52,370
on how you move your mouse based on how
you typed keystrokes into into a web

80
00:06:52,370 --> 00:06:59,310
page form feel we can detect if you are
you human or if you are not a human and

81
00:06:59,310 --> 00:07:06,530
this also uses machine learning to a
very large extent and yeah so you might

82
00:07:06,530 --> 00:07:12,010
think a lot of companies will want to
use machine learning to help make their

83
00:07:12,010 --> 00:07:18,680
products safer how you how you start
there are so many libraries out there so

84
00:07:18,680 --> 00:07:23,960
I can't learn in Python and there's also
met let libraries are libraries there's

85
00:07:23,960 --> 00:07:28,090
a pochemu how all of these make
implementing machine learning libraries

86
00:07:28,090 --> 00:07:32,869
just one or two lines of cord and you
can implement clustering less than a

87
00:07:32,870 --> 00:07:39,590
page of text which is which is unheard
of in the past lets you run a web server

88
00:07:39,590 --> 00:07:44,640
and these are just a party web logs or
some kind of my blogs that are you see

89
00:07:44,640 --> 00:07:49,650
how are you convert this into trying
seriously how do you convert this into a

90
00:07:49,650 --> 00:07:54,500
form of input that you can use in
anomaly detection and how would you find

91
00:07:54,500 --> 00:08:00,530
these anomalies this is where we will
spend more time on later and I'll talk

92
00:08:00,530 --> 00:08:06,760
more about different methods of doing
this first let's let's see why is

93
00:08:06,760 --> 00:08:11,700
machine learning good and how does it
compare with static methods

94
00:08:11,700 --> 00:08:15,950
static method I'm talking about here are
let's see if you wanted a techno someone

95
00:08:15,950 --> 00:08:22,250
is the dancing you and you do it by ITN
request count basis so for example if

96
00:08:22,250 --> 00:08:28,470
you see some IP address sending you more
than a thousand police request in this

97
00:08:28,470 --> 00:08:32,930
minute then you can block you can block
this idea dress by defining a static

98
00:08:32,929 --> 00:08:37,510
rule if any IP address cents more than a
thousand post requests in a minute

99
00:08:37,510 --> 00:08:41,010
Daniel structure blocking you suck to
threaten him this is very popularly

100
00:08:41,010 --> 00:08:44,600
implemented in all those without there
and we're seeing how they failed over

101
00:08:44,600 --> 00:08:45,700
time with seemed

102
00:08:45,700 --> 00:08:47,590
attackers can do easily find

103
00:08:47,590 --> 00:08:52,730
when the starting to get throttle and
they can just kill out horizontally

104
00:08:52,730 --> 00:08:56,550
this is because there's a strong
feedback loop in it in the circle and

105
00:08:56,550 --> 00:08:59,890
they know exactly when they're being
throttled so they can just fly under the

106
00:08:59,890 --> 00:09:08,710
radar and consider sending 1999 and they
won't get caught so machine learning is

107
00:09:08,710 --> 00:09:14,210
good because a large quantity of human
workers often the alternative if you had

108
00:09:14,210 --> 00:09:18,470
to think about how you wanna take into
account the fuzziness in detecting an

109
00:09:18,470 --> 00:09:24,240
anomaly that is hard to court in in a
few lines of code and rules then you

110
00:09:24,240 --> 00:09:29,130
often have to spend a lot of developer
time to develop a ruleset that can cover

111
00:09:29,130 --> 00:09:35,980
all these cases adaptive which means if
you perform online learning on streaming

112
00:09:35,980 --> 00:09:42,180
data so when you're receiving traffic
from your web server and your model is

113
00:09:42,180 --> 00:09:47,430
able to learn over time how your traffic
patterns change then your machine

114
00:09:47,430 --> 00:09:52,310
learning model is able to keep up with
for example the increasing popularity of

115
00:09:52,310 --> 00:09:53,369
your website

116
00:09:53,370 --> 00:09:57,950
able to learn when you're having maybe
running some promotions and your traffic

117
00:09:57,950 --> 00:10:01,790
sees an anomaly but you don't
necessarily think the anomalies militias

118
00:10:01,790 --> 00:10:07,760
and lastly able to discover an obvious
that the scope at patterns debt and

119
00:10:07,760 --> 00:10:12,670
characteristics of the data that is not
obvious requires minimal human

120
00:10:12,670 --> 00:10:17,760
intervention and theoretically because
you can just leave the model running and

121
00:10:17,760 --> 00:10:22,950
ill will be able to learn patterns that
humans won't be able to do court in into

122
00:10:22,950 --> 00:10:29,400
into the model however obviously there's
no silver bullet machine learning is not

123
00:10:29,400 --> 00:10:34,650
the right answer to every problem there
are many caveats and their many

124
00:10:34,650 --> 00:10:38,949
specifics to using the solution in the
normally detection and in any problem

125
00:10:38,950 --> 00:10:41,790
actually so let's look at why

126
00:10:41,790 --> 00:10:47,800
threshold or static blueberries are good
sushi results are good because it's easy

127
00:10:47,800 --> 00:10:53,930
to reason about i think thats the top
reason when you have and if we have a

128
00:10:53,930 --> 00:10:57,719
machine learning anomaly detector and
you know in an anomaly triggered it

129
00:10:57,720 --> 00:10:59,600
often hard to reason

130
00:10:59,600 --> 00:11:05,149
why this why your your room was
triggered by what caused your your

131
00:11:05,149 --> 00:11:10,220
anomaly detector to trigger and it's
hard to know exactly how you model has

132
00:11:10,220 --> 00:11:14,540
changed over time but if you are
standing rule sets you can find out

133
00:11:14,540 --> 00:11:18,829
exactly which IP address caused the rule
to be triggered you can find out just

134
00:11:18,829 --> 00:11:25,430
buy ranking by number of requests made
per minute after you can find out which

135
00:11:25,430 --> 00:11:32,029
IP addresses are the culprits so it's
simple and it's easy to implement and it

136
00:11:32,029 --> 00:11:38,860
can also be dynamic but requires more
work so machine learning is interesting

137
00:11:38,860 --> 00:11:43,319
I was here for the keynote this morning
and I thought it was really really

138
00:11:43,319 --> 00:11:48,709
interesting how how machine learning and
artificial intelligence have evolved

139
00:11:48,709 --> 00:11:53,508
over time and how different so many
successful applications in machine

140
00:11:53,509 --> 00:11:57,630
learning that we see and used today in
everyday life whether we know it or not

141
00:11:57,630 --> 00:12:02,930
something that's really popular in the
states and also she i think is Amazon

142
00:12:02,930 --> 00:12:07,189
when you're shopping for something on
Amazon let's say you're buying some

143
00:12:07,189 --> 00:12:12,060
gardening equipment and Amazon if you
scroll down I can recommend your

144
00:12:12,060 --> 00:12:18,420
products that are you might want to buy
and this is very prime example of how

145
00:12:18,420 --> 00:12:24,329
machine learning is being used to help a
business this is a recommender system

146
00:12:24,329 --> 00:12:29,109
and how how to do it is to build a
profile of you to build a profile of the

147
00:12:29,110 --> 00:12:33,579
customer and based on how different
customers have similar purchasing

148
00:12:33,579 --> 00:12:37,569
patterns they recommend you what you
might buy based on what other customers

149
00:12:37,569 --> 00:12:38,430
have by

150
00:12:38,430 --> 00:12:43,758
have given that they were the same items
or they're not the same items as you

151
00:12:43,759 --> 00:12:51,220
have as I mentioned earlier sorry for
the president to be there but just by

152
00:12:51,220 --> 00:12:56,819
looking at words in emails you can very
easily using Bayesian model to to detect

153
00:12:56,819 --> 00:13:01,689
how likely this piece of email is spam
and how likely this is not spam

154
00:13:01,689 --> 00:13:07,149
so we see using just some simple
conditional probability here you can use

155
00:13:07,149 --> 00:13:11,939
train a Bayesian model to to tell if a
certain number of certain amount of

156
00:13:11,939 --> 00:13:13,059
confidence

157
00:13:13,059 --> 00:13:16,170
how possible this email is spam

158
00:13:16,170 --> 00:13:26,479
something that is especially interesting
i think is this thing so what this is is

159
00:13:26,480 --> 00:13:34,370
an English sentence and english is not
the most complex language but its if you

160
00:13:34,370 --> 00:13:39,709
think about it it's hard to tell what
the sentence means by by using a

161
00:13:39,709 --> 00:13:45,630
computer and instead accrual set so what
does this sentiment analysis sentiment

162
00:13:45,630 --> 00:13:51,010
analysis basically tells you given a
sentence that the system has never seen

163
00:13:51,010 --> 00:13:58,569
before this sentence good or bad is it
positive or negative sentence is is the

164
00:13:58,569 --> 00:14:05,229
movie does not the movie does not care
about cleverness with or any other kind

165
00:14:05,230 --> 00:14:11,319
of intelligence humor if you want to use
a naive model then there are many

166
00:14:11,319 --> 00:14:13,990
positive words here which is positive

167
00:14:13,990 --> 00:14:19,529
intelligent is positive humor as
positive and cleverness is positive but

168
00:14:19,529 --> 00:14:24,370
when the Senators put together in a
complex structure then it's actually

169
00:14:24,370 --> 00:14:32,639
negative because of a negation in france
does not care about so it's hard to

170
00:14:32,639 --> 00:14:42,829
caught in a piece of hard-coded rules
the specifics of the English language or

171
00:14:42,829 --> 00:14:46,920
any language so what this does is it a
piece of deep learning software

172
00:14:46,920 --> 00:14:51,149
developed by Richard Daintree who is
currently running a deep learning

173
00:14:51,149 --> 00:14:52,139
start-up

174
00:14:52,139 --> 00:14:55,980
it some kind of recurrent neural
networks that can actually learn the

175
00:14:55,980 --> 00:15:01,399
patterns of the English language and
with just a small training set it can

176
00:15:01,399 --> 00:15:05,809
very succinctly learn how sentences are
formed and give you an accurate

177
00:15:05,809 --> 00:15:12,949
representation of whether the sentence
as positive or negative

178
00:15:12,950 --> 00:15:18,660
we want to set expectations here machine
learning has seen many successful

179
00:15:18,660 --> 00:15:26,490
applications and many successful
solutions to problems in in in the past

180
00:15:26,490 --> 00:15:31,760
as we've just seen but what can it do
for you in the security industry in

181
00:15:31,760 --> 00:15:37,170
particular what what can anomaly
detection with machine learning that do

182
00:15:37,170 --> 00:15:42,540
for you when is it helpful and when is
it not helpful I started on this

183
00:15:42,540 --> 00:15:52,079
research because organized in Silicon
Valley its data mining for security for

184
00:15:52,080 --> 00:15:53,450
cybersecurity

185
00:15:53,450 --> 00:15:57,780
it's it's a largest gathering of of the
of the area scientist and and security

186
00:15:57,780 --> 00:16:01,900
professionals in Silicon Valley and I've
been talking to people about what

187
00:16:01,900 --> 00:16:06,920
they've been doing in their work what
kind of new stuff today implementing and

188
00:16:06,920 --> 00:16:12,910
what new product that they're embarking
on so I started this project because I

189
00:16:12,910 --> 00:16:17,459
realized that there was kind of a weird
problem there are many machine learning

190
00:16:17,460 --> 00:16:22,430
an anomaly detection papers in academia
as a lot of research being done in this

191
00:16:22,430 --> 00:16:26,790
area but they're not a lot of successful
systems in the world

192
00:16:26,790 --> 00:16:30,620
usually when something has a lot of
research put into it and research says

193
00:16:30,620 --> 00:16:36,290
that this works very well and then soon
after you'll see a lot of commercial

194
00:16:36,290 --> 00:16:40,020
solutions to it and even if there are no
commercial solutions they will be open

195
00:16:40,020 --> 00:16:44,160
source solutions they'll be very easy to
implement and integrate tools that you

196
00:16:44,160 --> 00:16:49,540
can use but we don't see that for
anomaly detection why is that we end up

197
00:16:49,540 --> 00:16:53,430
seeing people trying to implement their
own inner companies and many people are

198
00:16:53,430 --> 00:16:57,729
doing that they're trying to implement
their own anomaly detection systems they

199
00:16:57,730 --> 00:17:01,480
often use machine learning and it's
because it's hard to understand whether

200
00:17:01,480 --> 00:17:05,120
that's gone wrong or not it and not
wasting a lot of their time and their

201
00:17:05,119 --> 00:17:12,560
organizations time and money so we'll
see what the differences the big machine

202
00:17:12,560 --> 00:17:17,520
learning an anomaly detection problem is
that machine learning strengths is to

203
00:17:17,520 --> 00:17:23,579
identify patterns its own identifying
similar things so you want to find what

204
00:17:23,579 --> 00:17:25,740
is similar to what's previously seen

205
00:17:25,740 --> 00:17:31,610
without describing the activity of print
and machine learning uses in recommender

206
00:17:31,610 --> 00:17:37,050
systems and detection natural language
understanding has seen great success but

207
00:17:37,050 --> 00:17:43,550
in the security industry in anomaly
detection it's different I'll point out

208
00:17:43,550 --> 00:17:47,980
some fundamental differences that makes
it much harder to a degree though a

209
00:17:47,980 --> 00:17:52,020
classic scenario where machine learning
does very well is when you have key

210
00:17:52,020 --> 00:17:58,000
clusters and you classify unseen entry
into clusters based on comparing the

211
00:17:58,000 --> 00:18:03,190
similarity of each of these data point
that you have and this is of course can

212
00:18:03,190 --> 00:18:08,230
end so k nearest neighbors which is a
very popular in simple and

213
00:18:08,230 --> 00:18:14,470
easy-to-understand clustering mechanism
so the basic rule of machine learning is

214
00:18:14,470 --> 00:18:19,460
that you have to train is this time with
specimens of all classes if not you have

215
00:18:19,460 --> 00:18:24,480
a class imbalance problem and when you
have to train with all classes what I

216
00:18:24,480 --> 00:18:29,440
mean is that you have to treat it with a
comparable number of positive specimens

217
00:18:29,440 --> 00:18:33,960
and negative specimens in the case of
anomaly detection this means that you

218
00:18:33,960 --> 00:18:37,230
have the training with anomalies and
have the training with not a non-animal

219
00:18:37,230 --> 00:18:42,370
if you think about it that doesn't make
sense because what makes an anomaly an

220
00:18:42,370 --> 00:18:47,729
anomaly statistically is that there are
fewer number used what's the norm

221
00:18:47,730 --> 00:18:52,740
without taking any context role
knowledge into into consideration here

222
00:18:52,740 --> 00:18:59,710
so I'm detection anomaly detection or
intrusion detection are different

223
00:18:59,710 --> 00:19:04,190
problems because you're training on both
spam and ham it raining on both positive

224
00:19:04,190 --> 00:19:07,720
and negative specimens mixed together
and you have very little negative

225
00:19:07,720 --> 00:19:15,660
specimens so there's the inherent class
imbalance problem here and that's why so

226
00:19:15,660 --> 00:19:23,429
what makes anomaly detection so
different so here are some of the points

227
00:19:23,429 --> 00:19:28,370
that they all go into detail it's one of
them and then we'll see how to implement

228
00:19:28,370 --> 00:19:33,779
an anomaly detector and we'll see how to
exploit one so

229
00:19:33,779 --> 00:19:38,669
anomaly detection systems have a very
high cost of error if you think about it

230
00:19:38,669 --> 00:19:43,809
machine learning systems actually
learned by trial and error they learned

231
00:19:43,809 --> 00:19:48,989
by making mistakes and then through
reinforcement learning the model

232
00:19:48,989 --> 00:19:52,979
strengthens over time as it seems more
and more positive or negative examples

233
00:19:52,979 --> 00:19:58,219
so compared to other applications it is
very intolerant to errors if you think

234
00:19:58,219 --> 00:20:04,809
about amazon.com or Gmail if the system
recommends you something that is totally

235
00:20:04,809 --> 00:20:10,710
irrelevant for example when you're
buying a speed recommends you a car

236
00:20:10,710 --> 00:20:16,359
steering wheel cover so it's not
relevant maybe but you don't care about

237
00:20:16,359 --> 00:20:20,928
it you think ok so so what if they
recommended me something it's it doesn't

238
00:20:20,929 --> 00:20:27,700
affect me my life if you receive a piece
of spam in your email inbox then it's

239
00:20:27,700 --> 00:20:32,820
not the end of the world because you can
just mark it as spam you are helping out

240
00:20:32,820 --> 00:20:36,849
the system to to learn and this is
exactly the the truck that the error

241
00:20:36,849 --> 00:20:42,589
that the Gmail system is making and when
you help the classified as spam mentally

242
00:20:42,589 --> 00:20:48,710
you're actually helping make the overall
system better for everyone so anomaly

243
00:20:48,710 --> 00:20:54,950
detection systems are intolerant errors
because let's say you have an analyst at

244
00:20:54,950 --> 00:20:59,279
received the reports of anomalies every
time an anomaly is triggered by the

245
00:20:59,279 --> 00:21:00,049
system

246
00:21:00,049 --> 00:21:06,950
someone somewhere receives an email or
call in the middle of the night so I'm

247
00:21:06,950 --> 00:21:09,779
not sure how many of you have received
his emails I've been receiving these

248
00:21:09,779 --> 00:21:14,159
emails at my workplace all the all the
time actually I'm still receiving them

249
00:21:14,159 --> 00:21:18,710
now and sometimes I just have a gmail
filter I just mark them as read order to

250
00:21:18,710 --> 00:21:23,830
lead them automatically because it's too
annoying and a lot of the time means it

251
00:21:23,830 --> 00:21:30,499
doesn't work so once there's to higher
rate of false-positive or false-negative

252
00:21:30,499 --> 00:21:37,159
is very easy for operator to distrust
the system and to think that any anomaly

253
00:21:37,159 --> 00:21:43,409
that it reflects is unreliable and
compared to other machinery applications

254
00:21:43,409 --> 00:21:46,690
this is this is fundamentally different

255
00:21:46,690 --> 00:21:55,140
the second point is the lack of training
data so what did I do trainer model on I

256
00:21:55,140 --> 00:21:59,580
learned this the hard way by
implementing the experiment here and I

257
00:21:59,580 --> 00:22:06,350
realized it when you're looking at input
data it's often hard to identify what

258
00:22:06,350 --> 00:22:10,570
it's often hard to identify the bad
points in the input data so you can

259
00:22:10,570 --> 00:22:14,820
clean it because when you train a model
depending on which model you choose you

260
00:22:14,820 --> 00:22:18,080
want to separate the classes you wanna
give you if you're using supervised

261
00:22:18,080 --> 00:22:23,470
learning models you want to be able to
classify a goal set of what is what is

262
00:22:23,470 --> 00:22:26,990
positive and what is negative then you
can train a model in a supervised way

263
00:22:26,990 --> 00:22:33,450
well it's hard to clean this data
because you're able to with accuracy in

264
00:22:33,450 --> 00:22:38,650
real time or over a large set of data to
determine what is good and what is bad

265
00:22:38,650 --> 00:22:42,230
then you don't actually need anomaly
detection system in the first place

266
00:22:42,230 --> 00:22:53,030
right next to the semantic gap the
semantic gap is something that is very

267
00:22:53,030 --> 00:22:58,040
special to this problem I talked about
it earlier when you get an alert why did

268
00:22:58,040 --> 00:23:02,040
you get the alert how do you determine
if this alert is really something that

269
00:23:02,040 --> 00:23:06,460
you should be worrying about or not
withstanding rule sets as I mentioned

270
00:23:06,460 --> 00:23:11,240
it's easy to it's easy to find out who
triggered the alert and how the alert

271
00:23:11,240 --> 00:23:14,450
was triggered but we're just learning
model especially if it's constantly

272
00:23:14,450 --> 00:23:20,530
changing then it's it's it's hard to
determine because unless you keep a

273
00:23:20,530 --> 00:23:26,470
snapshot in time of the model it's hard
to determine how to model looks looks

274
00:23:26,470 --> 00:23:32,570
like over time and how it changes over
time so something I want to talk about

275
00:23:32,570 --> 00:23:34,980
is why damn I will change over time

276
00:23:34,980 --> 00:23:40,120
tumultuous overtime because you want to
take into account the changing nature of

277
00:23:40,120 --> 00:23:44,689
your traffic a large reason why someone
would use a machine learning model over

278
00:23:44,690 --> 00:23:49,630
static final set is because with a
machine learning model the system can

279
00:23:49,630 --> 00:23:54,620
take the cake can take into account
changing mores of traffic so let's say

280
00:23:54,620 --> 00:23:59,840
you're running a website and giving the
same example earlier you have

281
00:23:59,840 --> 00:24:03,789
emotions once in a while letting every
Monday you run a promotion and then

282
00:24:03,789 --> 00:24:08,960
doing a promotion presumably your
traffic will spike and then you'll see a

283
00:24:08,960 --> 00:24:14,450
very characteristic pattern off of what
happens during this promotion and when

284
00:24:14,450 --> 00:24:19,840
you have a static rule set it it's hard
to take into account every small detail

285
00:24:19,840 --> 00:24:25,350
of how your traffic looks like over time
and let's see the good thing happens

286
00:24:25,350 --> 00:24:30,678
your traffic to your website games a lot
of traffic gradually over time this is

287
00:24:30,679 --> 00:24:37,679
called drift and this drift is really
what breaks static models because when

288
00:24:37,679 --> 00:24:39,179
your web sites nor

289
00:24:39,179 --> 00:24:45,490
notion of normality changes over time
then it's really hard to define that it

290
00:24:45,490 --> 00:24:50,549
will set that can change along with this
model and to detect how these models

291
00:24:50,549 --> 00:24:58,070
change how much she smiles changed by
the second last problem of machine

292
00:24:58,070 --> 00:25:02,309
learning anomaly detection is the
evaluation problem defined it defining

293
00:25:02,309 --> 00:25:07,029
our sound evaluation scheme is much more
complicated than building the system

294
00:25:07,029 --> 00:25:13,090
itself especially because if you could
define what's good and there in the

295
00:25:13,090 --> 00:25:18,990
first place then you don't need a system
so a lot of anomaly detection research

296
00:25:18,990 --> 00:25:20,120
papers

297
00:25:20,120 --> 00:25:26,520
evaluation problems in my opinion
because they use the same piece of data

298
00:25:26,520 --> 00:25:32,090
set and they trained at the models and
they use the same test so reading about

299
00:25:32,090 --> 00:25:38,620
two dozen papers about about 23 24
anomaly detection papers using machine

300
00:25:38,620 --> 00:25:41,870
learning I realized that they were all
using two or three datasets from the

301
00:25:41,870 --> 00:25:46,399
nineteen eighties origin destination
flow or their department of defense in

302
00:25:46,399 --> 00:25:48,039
the us-

303
00:25:48,039 --> 00:25:52,590
datasets from them and this is precisely
because it's so hard to obtain it did I

304
00:25:52,590 --> 00:25:56,699
said that's reliable and researchers
need a basis of comparison between

305
00:25:56,700 --> 00:26:02,000
academic papers the computer algorithms
against so this is very different from

306
00:26:02,000 --> 00:26:04,919
implementing something in the real world

307
00:26:04,919 --> 00:26:09,620
machine learning something that's very
context specific so if you're looking at

308
00:26:09,620 --> 00:26:13,209
different input datasets even if the
problem seems very

309
00:26:13,210 --> 00:26:17,860
similar on the surface the optimal
solution that you might choose in the

310
00:26:17,860 --> 00:26:22,949
end is very different for example if
you're recommending someone products on

311
00:26:22,950 --> 00:26:28,090
Amazon and if you have a video watching
website like YouTube or Netflix or Hulu

312
00:26:28,090 --> 00:26:33,860
and and you want to recommend them
videos these problems seem very similar

313
00:26:33,860 --> 00:26:37,168
but they actually have very different
solutions for how they're actually

314
00:26:37,169 --> 00:26:42,500
implemented because the nature of the
problem is different the way someone

315
00:26:42,500 --> 00:26:46,690
chooses when to watch a video and what
product to spend money on its very

316
00:26:46,690 --> 00:26:51,690
different so even though these two
problems are seem similar the optimal

317
00:26:51,690 --> 00:26:59,940
solution is actually the opposite of
each other last talked about that

318
00:26:59,940 --> 00:27:03,370
reserve impact if you think about it

319
00:27:03,370 --> 00:27:07,668
running a machine learning model that
changes over time will be such that will

320
00:27:07,669 --> 00:27:12,940
be susceptible to attacks by adversaries
people who want to bring into your

321
00:27:12,940 --> 00:27:17,390
system will spend time and effort to
bypass the system if you have a static

322
00:27:17,390 --> 00:27:22,840
rule set then they will fly under the
radar by not triggering a rules and by

323
00:27:22,840 --> 00:27:28,600
standing for example of a number of
requests that is just below what would

324
00:27:28,600 --> 00:27:32,610
trigger a rule if you have a machine
learning model is slightly more

325
00:27:32,610 --> 00:27:36,549
complicated but it's very exciting and
it's what a large part of my

326
00:27:36,549 --> 00:27:41,639
presentation will be on advanced actors
can and will spend the time to bypass

327
00:27:41,640 --> 00:27:46,630
your system and you will assume that if
global knowledge so we'll assume that

328
00:27:46,630 --> 00:27:52,210
they didn't know your system is trained
with for example a clustering model in

329
00:27:52,210 --> 00:27:58,820
order your training it weekly and
monthly because really the only a

330
00:27:58,820 --> 00:28:02,720
certain number of permutations of of
this study and go through before they

331
00:28:02,720 --> 00:28:09,850
get this knowledge so how have
real-world anomaly detection system feel

332
00:28:09,850 --> 00:28:16,010
I mentioned before these systems often
have many false positives and once this

333
00:28:16,010 --> 00:28:20,830
happens then it's basically useless it's
hard to find a tax-free training data

334
00:28:20,830 --> 00:28:24,809
and its use without deep understanding
so

335
00:28:24,809 --> 00:28:30,840
the problem of youth without the
understanding is an interesting one one

336
00:28:30,840 --> 00:28:34,059
might think that when you have more and
more libraries out there that machine

337
00:28:34,059 --> 00:28:40,129
money easy to implement and easy to to
code and easy to integrate then it might

338
00:28:40,129 --> 00:28:44,918
be a good thing but I would argue that
maybe it's not such a good thing because

339
00:28:44,919 --> 00:28:53,249
if you have let's see a function for for
for clustering 4444 ACMs and you are

340
00:28:53,249 --> 00:28:58,669
using just a function that you found in
its like you learn and then you have

341
00:28:58,669 --> 00:29:02,309
many parent parameters and the functions
and look at these parameters and I'm

342
00:29:02,309 --> 00:29:05,529
thinking I think I'll just use the
default because I'm not sure what you

343
00:29:05,529 --> 00:29:11,690
mean what people don't realize often is
that changing italy's current parameters

344
00:29:11,690 --> 00:29:17,419
will often turn a problem on its head
for example if you use a custom kernel

345
00:29:17,419 --> 00:29:24,730
or reuse a leader Colonel then it might
be totally different result in a totally

346
00:29:24,730 --> 00:29:27,100
different model in the end

347
00:29:27,100 --> 00:29:34,428
lastly best model poisoning which I'll
talk about later so it's machine

348
00:29:34,429 --> 00:29:40,749
learning an anomaly detection hopeless I
don't think so I think it's more

349
00:29:40,749 --> 00:29:46,419
important to understand your application
and to understand why you're using

350
00:29:46,419 --> 00:29:50,769
machine learning in your problem would
you be better off using static rule sets

351
00:29:50,769 --> 00:29:53,639
are what you really need the
functionality that machine learning

352
00:29:53,639 --> 00:29:58,459
provides you need to acquire a deep
semantic insight into the system's

353
00:29:58,460 --> 00:30:03,499
capabilities and limitations rather than
treating it as a black box just using

354
00:30:03,499 --> 00:30:12,649
functions you saw an example somewhere
so let's see how to do it for this

355
00:30:12,649 --> 00:30:18,820
example I'm gonna use the case that you
run a website because it simple and then

356
00:30:18,820 --> 00:30:22,460
you went to detect when someone is
detoxing you so this is this is

357
00:30:22,460 --> 00:30:28,240
something very simple example and many
industries see the same kind of patterns

358
00:30:28,240 --> 00:30:33,690
so first of all you want to generate a
time series then from this time

359
00:30:33,690 --> 00:30:38,060
seriously want to select representative
features anyone to train what's normal

360
00:30:38,060 --> 00:30:43,550
anyone alert when there's any deviation
from normal use as an example

361
00:30:43,550 --> 00:30:53,290
infrastructure so on the left me see
that they don't sources and this data

362
00:30:53,290 --> 00:30:58,149
sources in this case would be your
weblog for example if you have a party

363
00:30:58,150 --> 00:31:03,920
web logs you don't use can have no idea
dude the url attribute the HTTP HTTP

364
00:31:03,920 --> 00:31:09,530
method attribute and can also have time
stands and perhaps other more detail

365
00:31:09,530 --> 00:31:14,860
attributes and from this you have to
find features so Peter generation is

366
00:31:14,860 --> 00:31:19,850
something that is complicated and we're
looking to later and it actually can

367
00:31:19,850 --> 00:31:27,830
make or break your algorithm and as an
example feature would be IP and count so

368
00:31:27,830 --> 00:31:31,760
this is an aggregated feature so you
basically count let's see over one

369
00:31:31,760 --> 00:31:35,800
minute today from four o'clock to 4:01
p.m.

370
00:31:35,800 --> 00:31:40,129
how many requests were made by this I P
how many post requests for me to decide

371
00:31:40,130 --> 00:31:45,440
how many did request you mean by this IP
then there are also many examples of

372
00:31:45,440 --> 00:31:49,170
this as as you might imagine so how do
you choose a subset of these features

373
00:31:49,170 --> 00:31:54,500
how do you witness features such that a
combination of these features perdida

374
00:31:54,500 --> 00:32:01,890
point would give you a good
representation of your in particular you

375
00:32:01,890 --> 00:32:06,180
wanted to construct the time series and
by constructing the time serious what I

376
00:32:06,180 --> 00:32:11,120
mean is that you have to you have to
generate different time series and

377
00:32:11,120 --> 00:32:18,239
construct a very robust model of your
offer streaming data and feature

378
00:32:18,240 --> 00:32:23,370
selection can can also be done in an
automated fashion will dive into one

379
00:32:23,370 --> 00:32:28,729
later called principal component
analysis PCA and this is something that

380
00:32:28,730 --> 00:32:36,100
is used by the anomaly detection ok so
and then after that we'll go through

381
00:32:36,100 --> 00:32:42,219
metal validation because it's just if
you're just using statistical methods to

382
00:32:42,220 --> 00:32:46,520
to to validate these models then others
often too many false positive so

383
00:32:46,520 --> 00:32:48,860
industrial systems there's often a human
in the loop

384
00:32:48,860 --> 00:32:52,750
looking at all these anomalies generated
and then deciding men only whether they

385
00:32:52,750 --> 00:32:58,380
are real enemies are not so common
technique for machine learning there

386
00:32:58,380 --> 00:33:03,220
sometimes be based techniques clustering
support vector machines suspects

387
00:33:03,220 --> 00:33:06,309
coalition because I won't go into all of
them because they'll be

388
00:33:06,309 --> 00:33:10,570
of machine learning work but I'll focus
on clustering because it's easy to

389
00:33:10,570 --> 00:33:13,919
understand and because it's easy to
implement I mentioned k nearest

390
00:33:13,920 --> 00:33:18,190
neighbors earlier and that's a method of
clustering and there's also a lot of

391
00:33:18,190 --> 00:33:21,760
other methods like hierarchical
clustering there's there's a referendum

392
00:33:21,760 --> 00:33:28,970
decision trees so yeah so I mention a
lot about the model earlier what is this

393
00:33:28,970 --> 00:33:34,470
model that we're talking about a simple
example of a model this is a simple

394
00:33:34,470 --> 00:33:41,110
two-dimensional model there's only the
X&Y axis axis for now and clusters form

395
00:33:41,110 --> 00:33:45,879
each and every input data point that you
see in your time series would be eaten

396
00:33:45,880 --> 00:33:50,570
every requested that that's made or in
the case of counter beaten every I T

397
00:33:50,570 --> 00:33:55,740
count per minute you will see that each
entry point actually corresponds to a

398
00:33:55,740 --> 00:34:01,890
point on understood ms the graph so
these are constantly clusters the

399
00:34:01,890 --> 00:34:06,220
different colors of points are
artificially cut color because there are

400
00:34:06,220 --> 00:34:10,850
obvious clusters and depending on the
algorithm you choose this means that the

401
00:34:10,850 --> 00:34:14,899
distances between them are small enough
that you can classify viewpoint a

402
00:34:14,899 --> 00:34:18,580
similar and if a new point would come in
let's see your point would come in right

403
00:34:18,580 --> 00:34:24,509
there then you see that this is an
anomaly because it doesn't fit into any

404
00:34:24,510 --> 00:34:27,200
of the pre defined clusters you have
there

405
00:34:27,199 --> 00:34:31,770
anomaly based on the features that you
selected but is it really an anomaly

406
00:34:31,770 --> 00:34:37,070
that there's no easy answer to that
because it could just be that the

407
00:34:37,070 --> 00:34:41,899
features that you chose between features
that you chose X&Y here might be

408
00:34:41,899 --> 00:34:47,480
irrelevant or order might not be able to
capture the notion of normality that

409
00:34:47,480 --> 00:34:52,230
this is no point in the center of the
grass so even though that's a

410
00:34:52,230 --> 00:34:58,530
statistical anomaly that is a true
number so central class are not good for

411
00:34:58,530 --> 00:35:01,910
online learning because its new points
come in

412
00:35:01,910 --> 00:35:05,490
you can just add to the graph and as you
can see storage and memory will be worse

413
00:35:05,490 --> 00:35:09,808
off and squared because you just have to
calculate the distance between you two

414
00:35:09,809 --> 00:35:13,589
between a new point in each of the new
points and you can maintain constant

415
00:35:13,589 --> 00:35:22,200
on-going learning model of of of of your
of your input data and so if you want to

416
00:35:22,200 --> 00:35:26,069
maintain only lets in the last week of
the year or last month of the year then

417
00:35:26,069 --> 00:35:32,220
can eliminate all the other point as
they come in and you can do various

418
00:35:32,220 --> 00:35:36,058
optimizations under this is very very
well research topic and I won't go into

419
00:35:36,059 --> 00:35:42,520
this so how do you select theaters
earlier mentioned that for the simple

420
00:35:42,520 --> 00:35:46,780
web server example that we're talking
about there's some obviously to select

421
00:35:46,780 --> 00:35:50,130
but let's say you have a more
complicated problem that few people are

422
00:35:50,130 --> 00:35:54,750
seen before you run a very special
business or force different security

423
00:35:54,750 --> 00:35:59,849
applications it is often the case that
your particular application has

424
00:35:59,849 --> 00:36:04,000
something that no other education has so
how do you select teachers how do you

425
00:36:04,000 --> 00:36:09,750
know which is irrelevant future and how
do you choose a good combination of

426
00:36:09,750 --> 00:36:18,770
these features isn't it just a permanent
optimization problem so think about it

427
00:36:18,770 --> 00:36:22,109
you don't have to deal with the
complexity of machine learning anomaly

428
00:36:22,109 --> 00:36:26,190
detection techniques you can use a
simple rule and detection engine to

429
00:36:26,190 --> 00:36:32,220
alert when they point that do not expect
the trend line arrived there too many

430
00:36:32,220 --> 00:36:37,799
possible combinations to iterate over
its hard to evaluate and frequently

431
00:36:37,799 --> 00:36:43,130
changing optimal so thinking about just
the simple example of web logs there are

432
00:36:43,130 --> 00:36:47,250
so many different permutations and
combinations of features and wait for

433
00:36:47,250 --> 00:36:51,480
each feature that you can use for this
model and then it's hard to evaluate

434
00:36:51,480 --> 00:36:58,510
because accuracy is not the only
criteria even if you could accurately

435
00:36:58,510 --> 00:37:04,130
determine if this model were up tomorrow
or not maybe are looking for something

436
00:37:04,130 --> 00:37:09,859
that performs well but not as well as
the other one but it is easy to explain

437
00:37:09,859 --> 00:37:14,160
why does happen so they're shorter
training times

438
00:37:14,160 --> 00:37:21,359
a very important criteria because if
you're running an online learning

439
00:37:21,360 --> 00:37:27,750
approach let's say your website receives
a million request an hour and if you're

440
00:37:27,750 --> 00:37:31,870
using off in squares algorithm you might
not be able to keep up you might need a

441
00:37:31,870 --> 00:37:39,120
super computer to computer is this
anomaly thinks so

442
00:37:39,120 --> 00:37:42,620
looking at something that's more
optimized and you want to reduce

443
00:37:42,620 --> 00:37:48,049
overheating you want to be able to train
a mild as generalized enough for

444
00:37:48,050 --> 00:37:52,320
Component Analysis company analysis is
something that can help you to

445
00:37:52,320 --> 00:37:55,780
automatically select features by
transforming a day then two different

446
00:37:55,780 --> 00:38:04,700
dimensions so let's say your your data
is two-dimensional and on the left you

447
00:38:04,700 --> 00:38:09,980
can see that X&Y dimensions and on the
underwriting agency principal components

448
00:38:09,980 --> 00:38:15,460
so what this means is that if you just
think about moving the X&Y axis and so

449
00:38:15,460 --> 00:38:20,600
that moving the points then you wanna
find X&Y axis that can they can give you

450
00:38:20,600 --> 00:38:25,540
the maximum variance capture and various
capture can be seen at the bottom you

451
00:38:25,540 --> 00:38:34,100
CPC one and PC to PC when its principal
911 into your bored by but this is the

452
00:38:34,100 --> 00:38:43,240
optimal when you see that captures the
most variants when most of your points

453
00:38:43,240 --> 00:38:48,910
are further support this is the same
example in a three-dimensional space

454
00:38:48,910 --> 00:38:54,390
basically wrote it three or four
orthogonal components such that you want

455
00:38:54,390 --> 00:39:01,109
to find the optimal 11 is here you can
see PC 1 PC to ps3 and pc one captures

456
00:39:01,110 --> 00:39:06,540
the most variants and this this
component analysis algorithm is

457
00:39:06,540 --> 00:39:11,410
optimized such that its competition and
it helps you find the predominance

458
00:39:11,410 --> 00:39:17,029
faster so you want to choose prison
opponents that cover 80 to 90 percent of

459
00:39:17,030 --> 00:39:22,420
your debt said Syrians why is that the
fewer features that you have in your

460
00:39:22,420 --> 00:39:24,920
learning algorithm

461
00:39:24,920 --> 00:39:27,109
more efficient it is because the more

462
00:39:27,109 --> 00:39:34,630
the fewer and fewer components it have
to calculate closeness to four so for

463
00:39:34,630 --> 00:39:38,019
example if you're if you're if you're
defining a linear model and you want to

464
00:39:38,019 --> 00:39:42,399
find distance between two points if you
have to have to rely on a thousand

465
00:39:42,400 --> 00:39:46,059
different features then you basically
have to calculate the values of

466
00:39:46,059 --> 00:39:49,529
thousands features are and then
calculated for the other point and all

467
00:39:49,529 --> 00:39:53,059
the other point in your system and then
compare the distance of them if you

468
00:39:53,059 --> 00:39:56,499
choose only two features and these two
features are excellent these two

469
00:39:56,499 --> 00:40:01,180
features can tell you exactly what this
dataset what is the point means then you

470
00:40:01,180 --> 00:40:07,029
just have to cover two features four
point and that's good so this is a scree

471
00:40:07,029 --> 00:40:16,079
plot screen name you see when the number
of components is used is a small but the

472
00:40:16,079 --> 00:40:21,049
cameras capture is high when you see the
green line is actually an optimal use of

473
00:40:21,049 --> 00:40:28,788
critical analysis so how do you avoid
common pitfalls you have to understand

474
00:40:28,789 --> 00:40:32,940
what youre threat model well wanna keep
the detection scope narrow and want to

475
00:40:32,940 --> 00:40:37,069
reduce the cost of false negatives and
positives because like it or not there's

476
00:40:37,069 --> 00:40:40,019
going to be false negatives and
positives in these systems because its

477
00:40:40,019 --> 00:40:47,788
machine learning so you wanna know what
do miss a tax costs so if there's an

478
00:40:47,789 --> 00:40:49,200
attack that come in

479
00:40:49,200 --> 00:40:53,069
comes in and you you are modeled that
doesn't does is not able to tell the

480
00:40:53,069 --> 00:41:00,589
attack what what what the consequences
what concerns do evasions posed to keep

481
00:41:00,589 --> 00:41:04,538
the scope narrow because machine money
is not a silver bullet so don't start

482
00:41:04,539 --> 00:41:08,289
the premise of using machine learning
use it as a means to an end or just

483
00:41:08,289 --> 00:41:13,700
define rules that you can use instead
accepts and we also want to close a

484
00:41:13,700 --> 00:41:19,419
semantic get this is a cute picture I
think it's somewhere in Canada just 22

485
00:41:19,420 --> 00:41:23,539
bridges that's their sorrow from from
both sides of the shore and and this is

486
00:41:23,539 --> 00:41:28,720
just getting here so we're learning
models you you really wanna be able to

487
00:41:28,720 --> 00:41:31,509
understand it because if the system
gives your result that you're not able

488
00:41:31,509 --> 00:41:39,260
to understand or evaluate what uses the
system so evaluation techniques

489
00:41:39,260 --> 00:41:44,830
how good is your anomaly detector how
easily can you find false positives how

490
00:41:44,830 --> 00:41:49,910
easily can you insert another context
earlier that can filter out that the

491
00:41:49,910 --> 00:41:54,509
statistical anomalies so for example the
example dimension new point that comes

492
00:41:54,510 --> 00:41:59,020
in that's far from all the other
clusters you mean how can you have been

493
00:41:59,020 --> 00:42:02,290
done earlier in there whether it's a
human or whether it's another context

494
00:42:02,290 --> 00:42:07,400
role verification in there you can tell
you this is just a statistical anomaly

495
00:42:07,400 --> 00:42:12,060
I'm not sure anomaly or not that's
interesting anecdote

496
00:42:12,060 --> 00:42:15,799
elective elected going to hear you don't
have to either you don't just have to

497
00:42:15,800 --> 00:42:19,400
evaluate false positives without a valid
true positives so let's say you have a

498
00:42:19,400 --> 00:42:25,760
system that has 99% accuracy but if you
don't understand why your system

499
00:42:25,760 --> 00:42:30,710
performs this well and how you get the
results are getting then it can be

500
00:42:30,710 --> 00:42:35,680
meaningless so there's a famous like
happening in in the you s department of

501
00:42:35,680 --> 00:42:38,839
defense in the eighties they're
basically trying to use deep learning

502
00:42:38,840 --> 00:42:43,380
which was the hot topic then it was your
own networks with the heart thing then

503
00:42:43,380 --> 00:42:52,190
try to classify two sets of pictures
images of tanks and images of cars in

504
00:42:52,190 --> 00:42:57,960
the model that the morning my love these
images of times and cars and it has them

505
00:42:57,960 --> 00:43:03,010
ok this is a tank this is a card that
enters the tank so the car then it gives

506
00:43:03,010 --> 00:43:08,910
you get the model new unseen pictures
and cutting machine learning problem it

507
00:43:08,910 --> 00:43:13,660
supposed to tell you if this new picture
is a tanker car so they published a

508
00:43:13,660 --> 00:43:17,120
paper that said this is wonderful
because nineteen ninety plus percent

509
00:43:17,120 --> 00:43:22,290
accuracy and we believe that this is a
perfect deep learning model and he uses

510
00:43:22,290 --> 00:43:26,410
in in in in in our in our tanks in the
future to detect it as a civilian or

511
00:43:26,410 --> 00:43:32,720
this is an enemy not did the retracted
the papers kinda funny and someone guess

512
00:43:32,720 --> 00:43:40,819
what the model streamline under left you
see pictures of times on the right you

513
00:43:40,820 --> 00:43:45,610
see pictures are of currents different
cars so the more likely it was training

514
00:43:45,610 --> 00:43:47,950
on the different pixel colors that were
coming

515
00:43:47,950 --> 00:43:52,259
in if you see there's a lot more bright
colors on the 10 photos and a lot more

516
00:43:52,260 --> 00:44:00,000
dark photos in the car in the car photos
so if you give it a tank on a road or

517
00:44:00,000 --> 00:44:04,310
with a grey sky then it is very possible
that the model would have told you at

518
00:44:04,310 --> 00:44:08,940
the car so even though this model
performed very well the training set and

519
00:44:08,940 --> 00:44:13,720
that ended the test set it wasn't a good
model because it didn't capture the

520
00:44:13,720 --> 00:44:18,390
essence of what you were trying to do so

521
00:44:18,390 --> 00:44:25,100
lastly I want to attack the model so
what I'm about to come up there are two

522
00:44:25,100 --> 00:44:28,520
things I'm trying to do here the first
thing is to manipulate this learning

523
00:44:28,520 --> 00:44:33,550
system to permit the specific attack so
for example if I want to perform data

524
00:44:33,550 --> 00:44:38,600
infiltrated from a particular system and
I i know that the anomaly detection

525
00:44:38,600 --> 00:44:43,520
system is gonna catch me because usually
people don't just do a sequel of my

526
00:44:43,520 --> 00:44:49,030
database so I want to be able to change
has done that action system such that

527
00:44:49,030 --> 00:44:53,120
this will not be martin O'Malley so the
second thing is that the great

528
00:44:53,120 --> 00:44:57,670
performance after system and the
compromise its reliability you can do

529
00:44:57,670 --> 00:45:02,630
this by method I talked about earlier so
if the system generates lots of animals

530
00:45:02,630 --> 00:45:06,240
lot of false positives of the false
negatives and generates a thousand

531
00:45:06,240 --> 00:45:11,319
alerts and our venue operators going to
be ignoring all alerts from the system

532
00:45:11,320 --> 00:45:18,050
so that's a way to degrade performance
shop is is an important concept in this

533
00:45:18,050 --> 00:45:24,420
area so sharp is actually used the word
is English word used to describe the

534
00:45:24,420 --> 00:45:29,400
smoke comes out of the back of airplanes
and it's used to deflect calling this a

535
00:45:29,400 --> 00:45:34,310
lot to confuse homing missiles and this
is exactly what we're doing here were

536
00:45:34,310 --> 00:45:36,600
inserting shaft into the machine
learning model

537
00:45:36,600 --> 00:45:41,049
try to confuse the mother were tryna
make the model think that maybe this new

538
00:45:41,050 --> 00:45:46,400
point inserted is normal and when we
insert a large amount of Staff Sergeant

539
00:45:46,400 --> 00:45:51,900
another man of Sheriff then were able to
change the model so here we see we have

540
00:45:51,900 --> 00:45:54,190
a very naive decision model year

541
00:45:54,190 --> 00:46:00,230
decision boundary here but this is
amanda is defined by a point

542
00:46:00,230 --> 00:46:05,440
live out that the decision boundary then
it's an anomaly within the decision

543
00:46:05,440 --> 00:46:10,320
boundary then it's not an anomaly so
this is just a very simple distance and

544
00:46:10,320 --> 00:46:16,680
the distance of the blue circle to the
to the boundary is just for example

545
00:46:16,680 --> 00:46:22,990
understands aura or a kind of threshold
that you used to define what animals and

546
00:46:22,990 --> 00:46:26,959
whatnot numbers and social can
theoretically happy to shift the

547
00:46:26,960 --> 00:46:33,890
decision boundary insider direction that
benefits you another kind of stuff is

548
00:46:33,890 --> 00:46:37,520
when you have a lot of this in all
different directions and what you're

549
00:46:37,520 --> 00:46:41,210
basically trying to do for example
didn't come to use indeed US attacks is

550
00:46:41,210 --> 00:46:46,010
when you insert sheriff in all different
directions if you dress in search

551
00:46:46,010 --> 00:46:50,700
traffic that is unpredictable and is all
over the place then you can see that the

552
00:46:50,700 --> 00:46:55,589
decision boundary initially a small but
then it became be so popular the website

553
00:46:55,590 --> 00:46:59,859
it's hard for you to know if this is due
to organic traffic maybe you're just

554
00:46:59,859 --> 00:47:05,190
getting visitors from around the world
all of a sudden or or maybe your traffic

555
00:47:05,190 --> 00:47:09,400
maybe there's something wrong with your
time series generation so it's really

556
00:47:09,400 --> 00:47:13,410
don't know what the cause of this
problem and if you seeing shot like that

557
00:47:13,410 --> 00:47:19,009
then you're basically doing the internal
system so if you think about it isn't

558
00:47:19,010 --> 00:47:24,080
easy to detect when someone is injecting
a lot of traffic to your website exactly

559
00:47:24,080 --> 00:47:28,850
so that's why people come up with W
Fraga tech so the bombings analogy is

560
00:47:28,850 --> 00:47:34,029
when you put off route into cold water
and heated up slowly then it won't

561
00:47:34,030 --> 00:47:38,609
actually jump out I'm not not retrieved
it is true or not but they say he put a

562
00:47:38,609 --> 00:47:41,720
front into boiling water Elgin right out
because it will be able to know that

563
00:47:41,720 --> 00:47:46,730
it's it's in trouble though to avoid
detection

564
00:47:46,730 --> 00:47:52,119
similarly to this analogy you go slow
your incident inserting Scharf at a very

565
00:47:52,119 --> 00:47:57,200
high rate at once you insert it slowly
over time so for example you insert 10%

566
00:47:57,200 --> 00:48:03,000
of your total shock today and the next
week you answer 20% 30% so it looks like

567
00:48:03,000 --> 00:48:03,990
it's organic

568
00:48:03,990 --> 00:48:11,729
what's natural to confuse the operator
to defend against this level of Cisco

569
00:48:11,730 --> 00:48:12,510
quickly and

570
00:48:12,510 --> 00:48:18,370
talk about how such some some possible
solutions you may comment in a goal set

571
00:48:18,370 --> 00:48:22,839
a test set so before you run your system
before you deploy public publicly you

572
00:48:22,840 --> 00:48:30,400
can have a goal set that basically tell
you what's good and what's not based on

573
00:48:30,400 --> 00:48:36,480
a certain tests you have an after
running it for a month tested again and

574
00:48:36,480 --> 00:48:40,140
you see how it's changed over time and
if it seems too much maybe someone

575
00:48:40,140 --> 00:48:44,870
attacking you but how do you know it's
much less organic change the next thing

576
00:48:44,870 --> 00:48:48,460
that this hasn't been reached racial
detection what I mean by this

577
00:48:48,460 --> 00:48:54,760
so if someone's injecting Scharf a more
efficient way to do it is to inject

578
00:48:54,760 --> 00:48:59,260
Scharf close to a decision boundary
because they don't want it to be under

579
00:48:59,260 --> 00:49:03,320
sharp volume to detect detected as
anomalies they wanna go slow and

580
00:49:03,320 --> 00:49:08,610
gradually and so it can detect the ratio
of your traffic that is falling into

581
00:49:08,610 --> 00:49:12,200
this certain legislation boundary then
maybe you can detect when someone's

582
00:49:12,200 --> 00:49:20,089
gonna take you so far machine learning
be secure

583
00:49:20,090 --> 00:49:24,740
it's not easy to achieve for
unsupervised online learning because the

584
00:49:24,740 --> 00:49:29,160
learning conundrum the learning paradox
is that with no contextual knowledge it

585
00:49:29,160 --> 00:49:32,629
only uses statistics to make decisions
that are inherently strongly

586
00:49:32,630 --> 00:49:37,850
context-based you can make it much
harder for the adversary though you can

587
00:49:37,850 --> 00:49:40,920
slow them down and it gives you a chance
to detect when something is going wrong

588
00:49:40,920 --> 00:49:46,850
and then perform a review so there are
many defenses proposed for this there's

589
00:49:46,850 --> 00:49:52,470
antidote which uses the median instead
of the mean because obviously median is

590
00:49:52,470 --> 00:49:58,549
a much more robust secure way of finding
variance and PCA actually it's not

591
00:49:58,550 --> 00:50:04,810
designed for security it uses the mean
it maximizes the mean so you have to use

592
00:50:04,810 --> 00:50:08,880
the proper distribution as well not
everything is gosh in somethings are

593
00:50:08,880 --> 00:50:15,220
somethings are Laplacian so you wanna be
able to understand your data model well

594
00:50:15,220 --> 00:50:20,430
and then choose proper model that
represents it so this is what I

595
00:50:20,430 --> 00:50:24,569
mentioned earlier if you want to find a
proper distribution for a dataset and to

596
00:50:24,570 --> 00:50:25,600
really understand you're in

597
00:50:25,600 --> 00:50:30,900
and understand your threat model and
know what you're looking for

598
00:50:30,900 --> 00:50:36,560
going to this section I run my own
simulations of some real data when I did

599
00:50:36,560 --> 00:50:41,610
was I implemented some anomaly detection
systems based on papers out there so

600
00:50:41,610 --> 00:50:47,370
what's considered to be the latest and
the best results in in our research

601
00:50:47,370 --> 00:50:53,080
papers I just implemented these and so I
had finally in control over which part

602
00:50:53,080 --> 00:50:58,850
of the algorithm I was using image but I
was not then I had I i dont get some

603
00:50:58,850 --> 00:51:06,810
lunch dinner sets from various sources
so it might be a bit faint but all of

604
00:51:06,810 --> 00:51:12,160
this are dots are plotted and this is a
10% subset often put there that I chose

605
00:51:12,160 --> 00:51:16,310
from input apache server where blocks
the y axis is the production of the

606
00:51:16,310 --> 00:51:21,520
first principal component so not that I
use PCA and I chose only the first

607
00:51:21,520 --> 00:51:26,400
principal component so the first one of
them actually captured about 67 percent

608
00:51:26,400 --> 00:51:31,740
of the variance which is pretty good for
a real-life datasets and you can see

609
00:51:31,740 --> 00:51:38,669
there some kind of trend here this is
just a simple linear regression model

610
00:51:38,670 --> 00:51:46,010
train line and can see robust PCA and
naive PCA so naive PC users just very

611
00:51:46,010 --> 00:51:51,400
standard way our feature selection and
robust PCs uses methods there are for

612
00:51:51,400 --> 00:52:02,550
example users / in model and it uses my
absolute deviation and so I injected

613
00:52:02,550 --> 00:52:09,870
some staff here this is a simple simple
safe injection and general see if it

614
00:52:09,870 --> 00:52:11,040
what changed

615
00:52:11,040 --> 00:52:17,730
tramlines by the way during this stuff
is hard to go to trial and error because

616
00:52:17,730 --> 00:52:25,950
it doesn't I can't tell what and you get
a point where we're going to end up and

617
00:52:25,950 --> 00:52:29,680
basically have to insert needle points
and then have to see where it ends up

618
00:52:29,680 --> 00:52:31,500
and then change

619
00:52:31,500 --> 00:52:35,280
various parameters of the input they are
such that I can generate shot in a

620
00:52:35,280 --> 00:52:36,100
particular area

621
00:52:36,100 --> 00:52:41,680
and you'll see that it did move the
faint lines are where it was originally

622
00:52:41,680 --> 00:52:47,819
and dark lines are are where they are
now after distrust injection robust PCA

623
00:52:47,820 --> 00:52:53,040
generates a smaller shift and I UPC
generates a larger shift which is

624
00:52:53,040 --> 00:53:00,140
expected because users median and mean
and then I have a simple detection logic

625
00:53:00,140 --> 00:53:07,940
so if I see that more than 10% off the
data coming in is classified as an

626
00:53:07,940 --> 00:53:13,140
ominous tone are detected so this is
just some simple detection logic that

627
00:53:13,140 --> 00:53:16,680
you can use to detect when someone is
trying to poison your model and to

628
00:53:16,680 --> 00:53:22,250
defend against that I have some training
periods here so over training period

629
00:53:22,250 --> 00:53:28,830
2468 and 10 injected various various
levels of stuff and you seem different

630
00:53:28,830 --> 00:53:36,150
colors from green to red what point at
which trinkets and this is the other

631
00:53:36,150 --> 00:53:43,600
model when I'm getting so much that I'm
just confusing the model and to see if

632
00:53:43,600 --> 00:53:48,120
the notion of of normality and to expand
decision boundary

633
00:53:48,120 --> 00:53:56,500
it shifts quite a lot as well and this
is a graph that I want to go into some

634
00:53:56,500 --> 00:54:03,570
detail so the ideal place is when you
have very low false positive rate and

635
00:54:03,570 --> 00:54:10,520
very high true path so the idea cases a
ninety degree line from here to here and

636
00:54:10,520 --> 00:54:16,430
a random detector will give you with 50%
probability that something will just

637
00:54:16,430 --> 00:54:19,899
kill you with 50% probability but
something is an anomaly or not and it

638
00:54:19,900 --> 00:54:29,330
can be seen by the middle line here so
as you can see when the plots approach

639
00:54:29,330 --> 00:54:34,670
the ninety-degree line then it's a
better detector even a protester random

640
00:54:34,670 --> 00:54:39,750
detected and you know it's been
compromised so robust PCA with 30% shot

641
00:54:39,750 --> 00:54:42,440
which means that thirty percent of the
total traffic there is an election

642
00:54:42,440 --> 00:54:47,730
system has seen over the training period
is injected by adversaries

643
00:54:47,730 --> 00:54:55,300
then here 50% off its further weakened
but then the Texan was tough because

644
00:54:55,300 --> 00:54:59,570
when injecting 60% of traffic deaths

645
00:54:59,570 --> 00:55:04,340
operator sees overtraining period then
you can get its pretty easy for operated

646
00:55:04,340 --> 00:55:09,530
a fine and is being attacked so using a
bombing attack is pretty good compromise

647
00:55:09,530 --> 00:55:16,820
and you can see the blue line there is
the result of a bombing attack on our

648
00:55:16,820 --> 00:55:19,840
Casilla

649
00:55:19,840 --> 00:55:27,680
successes for robust PCR using buying
from injection we can see that there's a

650
00:55:27,680 --> 00:55:33,980
38% evasions access which in my opinion
is pretty good because if I would extend

651
00:55:33,980 --> 00:55:38,340
the training period and to extend attack
period then I can push this number is

652
00:55:38,340 --> 00:55:39,430
much higher

653
00:55:39,430 --> 00:55:45,460
I was using a ten-week training period
and about 200,000 200,000 lockset so

654
00:55:45,460 --> 00:55:52,700
that 50% sharp inject I got 38 percent
of its own success and if four in 10 4

655
00:55:52,700 --> 00:55:57,710
antenna attempt an attack on a web
server goes through then I'd say this

656
00:55:57,710 --> 00:56:03,650
anomaly detection system was a failure
so early detection systems today are not

657
00:56:03,650 --> 00:56:08,020
so good but they're still improving your
machine learning-based and immune

658
00:56:08,020 --> 00:56:13,240
systems are very vulnerable to
compromise so if you would use machine

659
00:56:13,240 --> 00:56:17,000
learning if you want to build an anomaly
detection system from scratch which a

660
00:56:17,000 --> 00:56:21,520
lot of people want to do then I don't
actually recommend you to start off by

661
00:56:21,520 --> 00:56:24,750
listing down what you're looking for
what kind of animals you're looking for

662
00:56:24,750 --> 00:56:29,300
and to weigh both we're both methods
static method in machine learning

663
00:56:29,300 --> 00:56:34,090
methods in a fine features and
thresholds using machine learning and

664
00:56:34,090 --> 00:56:39,109
then called that into static rule sets
and then you used to study group sets so

665
00:56:39,109 --> 00:56:43,350
you can get both benefits of machine
learning model and aspiring model and

666
00:56:43,350 --> 00:56:47,670
then you can retrain this features in
regional December features every so

667
00:56:47,670 --> 00:56:53,990
often to take into account the drift
they are traffic might be seeing so I

668
00:56:53,990 --> 00:56:58,220
want to do more tests on animals as the
detection systems that are separated so

669
00:56:58,220 --> 00:57:02,779
far been running tests on my own ideal
models based on research papers that

670
00:57:02,780 --> 00:57:06,330
have promised good results but there are
some open source solutions and there are

671
00:57:06,330 --> 00:57:09,990
a lot of vendors out there they are
offering machine learning machine

672
00:57:09,990 --> 00:57:16,939
learning anomaly detection and none of
them have varying levels of efficiency I

673
00:57:16,940 --> 00:57:19,720
want to try other defenses against
pointing techniques can also experiment

674
00:57:19,720 --> 00:57:27,118
on more resilient machine learning
models so lastly I mentioned this meetup

675
00:57:27,119 --> 00:57:31,930
group does this user group in in Silicon
Valley data mining for cybersecurity it

676
00:57:31,930 --> 00:57:35,368
it's been very successful be or have
over a thousand members of our after

677
00:57:35,369 --> 00:57:40,160
only six months and we have a meet up
every every month where big companies

678
00:57:40,160 --> 00:57:45,170
like Facebook Google LinkedIn and
Netflix half half hour came to present

679
00:57:45,170 --> 00:57:47,680
their security teams are talking about
how to use data mining and machine

680
00:57:47,680 --> 00:57:53,440
learning problems so I think this is
beneficial to all communities and you're

681
00:57:53,440 --> 00:57:59,100
interested in starting a meetup group
like this in Japan or chapter here feel

682
00:57:59,100 --> 00:58:02,480
free to come to me and we can talk about
it

683
00:58:02,480 --> 00:58:10,010
thank you

684
00:58:10,010 --> 00:58:30,200
guest at the moment they must get the
cash 1864 the presentation and you

685
00:58:30,200 --> 00:58:35,569
mention about the lack of an anomaly
dated part of the problem I don't lie

686
00:58:35,570 --> 00:58:44,970
detection but is it possible to ask such
a preparation pandered to provide such a

687
00:58:44,970 --> 00:58:49,350
normally datas says they have tons of it
for sure

688
00:58:49,350 --> 00:58:55,319
yeah so that that's a good point there
to their two minor issues that I see

689
00:58:55,320 --> 00:59:02,190
what would this so for one you have to
ensure that your the positive test cloud

690
00:59:02,190 --> 00:59:05,890
that positive training class is truly
positive and it's hard to ensure that

691
00:59:05,890 --> 00:59:09,250
because when I running a web server is
bound to be anomalies that that you're

692
00:59:09,250 --> 00:59:12,730
looking for that you haven't detected
and so if you include that into your

693
00:59:12,730 --> 00:59:20,200
public training set and even if you have
a very large volume of negative training

694
00:59:20,200 --> 00:59:25,750
examples then the model will still not
be ideal in a sec includes debt there

695
00:59:25,750 --> 00:59:31,090
are just too many by definition there
are too many examples of deviation from

696
00:59:31,090 --> 00:59:37,240
from normality so let's say you hire a
fencing team to to attack your model and

697
00:59:37,240 --> 00:59:43,270
they generate attacks in in some certain
way so the notion of normality is quite

698
00:59:43,270 --> 00:59:48,750
narrow but the notion of abnormality is
very wide so there's just so many ways

699
00:59:48,750 --> 00:59:54,030
that that traffic can be classified as
abnormal and it's hard to really train

700
00:59:54,030 --> 01:00:04,460
machinery model to capture all the
normality

701
01:00:04,460 --> 01:00:08,849
like a dumbass the idea that was a
mistake

