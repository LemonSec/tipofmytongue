1
00:00:00,330 --> 00:00:07,790
[Music]

2
00:00:07,790 --> 00:00:10,860
as Redmond tonight I spent a lot of time

3
00:00:10,860 --> 00:00:13,799
actually talking to DW but it tends not

4
00:00:13,799 --> 00:00:17,100
to be about cybersecurity but about the

5
00:00:17,100 --> 00:00:19,410
latest Trump tweet or the latest State

6
00:00:19,410 --> 00:00:21,330
of the Union address or whatever is

7
00:00:21,330 --> 00:00:23,820
happening in in Washington so that's the

8
00:00:23,820 --> 00:00:25,740
advantage of and disadvantage of being a

9
00:00:25,740 --> 00:00:27,750
generalist I know a little bit about

10
00:00:27,750 --> 00:00:29,310
everything just enough to make me

11
00:00:29,310 --> 00:00:32,369
dangerous but I want to take the the

12
00:00:32,369 --> 00:00:33,660
topic of the first panel which was

13
00:00:33,660 --> 00:00:35,520
really about people and Larry asked the

14
00:00:35,520 --> 00:00:36,930
question you know getting to the

15
00:00:36,930 --> 00:00:40,230
intrinsic value of y what is in symud

16
00:00:40,230 --> 00:00:42,210
Incident izing people to do things in

17
00:00:42,210 --> 00:00:43,649
cybersecurity what is motivating

18
00:00:43,649 --> 00:00:45,750
decisions and I think that that's going

19
00:00:45,750 --> 00:00:46,860
to be something that's going to inflect

20
00:00:46,860 --> 00:00:48,989
our conversation about the threat

21
00:00:48,989 --> 00:00:52,680
landscape in next gen technology the the

22
00:00:52,680 --> 00:00:54,180
title of this this has come up a couple

23
00:00:54,180 --> 00:00:56,370
times as paradigm shift it needed

24
00:00:56,370 --> 00:00:59,039
question mark I would say that the

25
00:00:59,039 --> 00:01:01,230
actual title should be paradigm shifted

26
00:01:01,230 --> 00:01:04,949
shift happening period but the question

27
00:01:04,949 --> 00:01:06,390
is is what are the values motivating

28
00:01:06,390 --> 00:01:08,490
that paradigm shift is it security I

29
00:01:08,490 --> 00:01:10,530
think we would all say no its

30
00:01:10,530 --> 00:01:13,439
convenience its efficiency its

31
00:01:13,439 --> 00:01:15,049
satisfaction its consumer satisfaction

32
00:01:15,049 --> 00:01:17,220
these are the things that are motivating

33
00:01:17,220 --> 00:01:20,189
the ships so we have a mismatch here so

34
00:01:20,189 --> 00:01:22,590
we're having this paradigm shift we're

35
00:01:22,590 --> 00:01:24,119
entering what I would call a deep

36
00:01:24,119 --> 00:01:26,700
digital age we're gonna have at the end

37
00:01:26,700 --> 00:01:28,619
of next year 20 billion IOT devices

38
00:01:28,619 --> 00:01:31,200
online we're having a change in the

39
00:01:31,200 --> 00:01:32,579
Internet infrastructure which is gonna

40
00:01:32,579 --> 00:01:36,570
make internet much more quick IOT AI

41
00:01:36,570 --> 00:01:39,180
machine learning blockchain quantum

42
00:01:39,180 --> 00:01:41,280
computing is all changing the way we

43
00:01:41,280 --> 00:01:42,500
need to think about the threat landscape

44
00:01:42,500 --> 00:01:45,869
but we still in the policy community in

45
00:01:45,869 --> 00:01:48,360
the corporate community and as users are

46
00:01:48,360 --> 00:01:50,820
still in the digital age and the digital

47
00:01:50,820 --> 00:01:52,590
age is about laptops

48
00:01:52,590 --> 00:01:55,079
it's about smartphones it's about social

49
00:01:55,079 --> 00:01:57,540
media so what we're gonna try to do in

50
00:01:57,540 --> 00:01:59,460
this session is reconcile the two no

51
00:01:59,460 --> 00:02:01,649
easy task but we have some really smart

52
00:02:01,649 --> 00:02:03,180
people who are gonna do that for us so

53
00:02:03,180 --> 00:02:04,890
I'm gonna call them up and as I do if

54
00:02:04,890 --> 00:02:06,420
you guys can come up take a seat and we

55
00:02:06,420 --> 00:02:07,680
can give them a hand of a round of

56
00:02:07,680 --> 00:02:11,489
applause first Reiner buh-bam guard

57
00:02:11,489 --> 00:02:15,050
rhyme Reiner come on up

58
00:02:16,930 --> 00:02:20,270
Reiner is the CEO of second net which he

59
00:02:20,270 --> 00:02:22,370
joined in 1997 which i think is the year

60
00:02:22,370 --> 00:02:25,190
was founded he's a physician by trade

61
00:02:25,190 --> 00:02:27,650
choose me a physicist by training and he

62
00:02:27,650 --> 00:02:29,120
is one of the members of the Nisa

63
00:02:29,120 --> 00:02:30,530
stakeholders group right now please take

64
00:02:30,530 --> 00:02:33,980
a seat here next we have Terry Halverson

65
00:02:33,980 --> 00:02:37,540
Terry come on up all right

66
00:02:37,540 --> 00:02:39,830
Terry is one of those guys who needs no

67
00:02:39,830 --> 00:02:42,050
introduction he has been at Samsung

68
00:02:42,050 --> 00:02:46,400
since 2017 where he is the CIO and EVP

69
00:02:46,400 --> 00:02:48,470
for the company's IT and mobile b2b

70
00:02:48,470 --> 00:02:52,490
group but he comes from a long career in

71
00:02:52,490 --> 00:02:54,680
the military the US military and at the

72
00:02:54,680 --> 00:02:57,530
DoD where he was at the end the chief

73
00:02:57,530 --> 00:02:59,600
information officer of the US military

74
00:02:59,600 --> 00:03:01,870
probably the world's largest platform

75
00:03:01,870 --> 00:03:05,120
next we have dr. su Jane Thompson sue

76
00:03:05,120 --> 00:03:07,570
come on up

77
00:03:10,060 --> 00:03:13,490
sujin is VP and partner for cyber and

78
00:03:13,490 --> 00:03:17,090
biometric and US public services at IBM

79
00:03:17,090 --> 00:03:18,740
and she's been working in the field for

80
00:03:18,740 --> 00:03:22,250
30 years she started when she was five

81
00:03:22,250 --> 00:03:25,040
years old ladies into the way 35 years

82
00:03:25,040 --> 00:03:29,300
old next we have good Burt sheriff Boone

83
00:03:29,300 --> 00:03:31,970
Burt come on up come back oh there he is

84
00:03:31,970 --> 00:03:34,490
he is a partner at McKinsey in Berlin

85
00:03:34,490 --> 00:03:41,030
yes where he leads McKinsey's or as one

86
00:03:41,030 --> 00:03:42,440
of the leads of McKenzie's European

87
00:03:42,440 --> 00:03:46,070
cybersecurity practice from 2014 to 2015

88
00:03:46,070 --> 00:03:49,070
he was at the German Ministry of Defence

89
00:03:49,070 --> 00:03:51,410
where he led efforts to create the

90
00:03:51,410 --> 00:03:53,270
cyberspace strategy and cyber

91
00:03:53,270 --> 00:03:56,660
information domain command and last but

92
00:03:56,660 --> 00:03:59,090
not least we have Norbert Gauss Norbert

93
00:03:59,090 --> 00:04:02,050
where are you come on up

94
00:04:04,379 --> 00:04:07,780
Norbert is the executive VP at Siemens

95
00:04:07,780 --> 00:04:10,660
corporate technology responsible for R&D

96
00:04:10,660 --> 00:04:13,599
for digitization and automation and he

97
00:04:13,599 --> 00:04:16,660
has been at Siemens since 1991 so he

98
00:04:16,660 --> 00:04:18,370
really knows the corporation inside and

99
00:04:18,370 --> 00:04:21,820
out so this is the panel also started at

100
00:04:21,820 --> 00:04:22,960
five years old ladies and gentlemen

101
00:04:22,960 --> 00:04:25,270
we've got another young in them all

102
00:04:25,270 --> 00:04:26,740
right so we have a very distinguished

103
00:04:26,740 --> 00:04:27,880
panel so we're going to get into all

104
00:04:27,880 --> 00:04:31,630
these technologies AI IOT blockchain etc

105
00:04:31,630 --> 00:04:33,040
but let's start with a hot topic right

106
00:04:33,040 --> 00:04:35,290
now if we talk about being at the

107
00:04:35,290 --> 00:04:37,450
present at the creation we're creating

108
00:04:37,450 --> 00:04:38,830
the systems that are going to govern us

109
00:04:38,830 --> 00:04:41,050
for the next generation and one of it of

110
00:04:41,050 --> 00:04:44,080
course is this Wozniak point the Wozniak

111
00:04:44,080 --> 00:04:46,990
question faster internet in the next six

112
00:04:46,990 --> 00:04:48,400
months a lot of governments in Europe

113
00:04:48,400 --> 00:04:49,750
they're going to make a decision about

114
00:04:49,750 --> 00:04:52,960
how they want to build their 5g networks

115
00:04:52,960 --> 00:04:54,669
and so I want to start with a question

116
00:04:54,669 --> 00:04:55,870
to Terry Terry

117
00:04:55,870 --> 00:05:00,310
what's at stake here oh yeah and please

118
00:05:00,310 --> 00:05:06,280
yeah use the mics lots of money okay who

119
00:05:06,280 --> 00:05:08,320
gets the money that's gonna be what's at

120
00:05:08,320 --> 00:05:09,340
stake

121
00:05:09,340 --> 00:05:11,740
I think there's been enough studies I

122
00:05:11,740 --> 00:05:14,050
tend agree there the groups that get to

123
00:05:14,050 --> 00:05:14,860
them what we're really talking about

124
00:05:14,860 --> 00:05:17,380
here is Feige yeah the groups to get to

125
00:05:17,380 --> 00:05:21,190
5g and the ecosystem so just to be a

126
00:05:21,190 --> 00:05:23,800
little lengthy 5g network is not by

127
00:05:23,800 --> 00:05:26,680
itself the revolution it's a convergence

128
00:05:26,680 --> 00:05:28,289
of a couple different things

129
00:05:28,289 --> 00:05:31,090
5g speeds and latency coupled with now

130
00:05:31,090 --> 00:05:34,360
the ability to store a massive amounts

131
00:05:34,360 --> 00:05:37,330
of data on the edge and then you throw

132
00:05:37,330 --> 00:05:40,240
in our last speakers topic of AI that's

133
00:05:40,240 --> 00:05:42,190
going to drive economies business

134
00:05:42,190 --> 00:05:45,099
changes we talk about paradigm shifts so

135
00:05:45,099 --> 00:05:47,950
for fun as soon as you get driverless

136
00:05:47,950 --> 00:05:49,990
cars autonomous we know that accident

137
00:05:49,990 --> 00:05:52,419
rates will go down what happens to the

138
00:05:52,419 --> 00:05:55,090
auto insurance industry you change I

139
00:05:55,090 --> 00:05:56,800
mean these are all things that are going

140
00:05:56,800 --> 00:05:59,889
to happen and the consumer part of this

141
00:05:59,889 --> 00:06:01,960
business is not going to be the first

142
00:06:01,960 --> 00:06:05,560
part that we talk about 5g well you out

143
00:06:05,560 --> 00:06:08,650
there is not first going to be driven by

144
00:06:08,650 --> 00:06:11,320
consumers 4G was fired she's going to be

145
00:06:11,320 --> 00:06:13,630
driven by governments and Industry okay

146
00:06:13,630 --> 00:06:15,990
so we've got 5g being driven by

147
00:06:15,990 --> 00:06:17,850
governments and Industry so how do we

148
00:06:17,850 --> 00:06:18,990
need to make sure to harden that

149
00:06:18,990 --> 00:06:21,390
security as we're building the basic

150
00:06:21,390 --> 00:06:22,680
infrastructure that's going to govern us

151
00:06:22,680 --> 00:06:25,230
for the next 15 years what are the

152
00:06:25,230 --> 00:06:27,240
concerns that are worrying policymakers

153
00:06:27,240 --> 00:06:29,580
well I think the is the last panel was

154
00:06:29,580 --> 00:06:31,740
really a good topic of that the concerns

155
00:06:31,740 --> 00:06:34,260
are no one really does want to pay for

156
00:06:34,260 --> 00:06:36,300
security yeah I mean maybe even in the

157
00:06:36,300 --> 00:06:37,950
panel we heard some remembers argue that

158
00:06:37,950 --> 00:06:39,090
they didn't have to pay for security I

159
00:06:39,090 --> 00:06:41,790
think was very wrong but they did we've

160
00:06:41,790 --> 00:06:43,410
got to figure out how we're going to

161
00:06:43,410 --> 00:06:46,620
explain that this is all going to cost

162
00:06:46,620 --> 00:06:49,650
some money and government is going to

163
00:06:49,650 --> 00:06:51,570
pay some industry is gonna pay some and

164
00:06:51,570 --> 00:06:52,980
unfortunately yes consumers are gonna

165
00:06:52,980 --> 00:06:56,190
pay some and how we build that case how

166
00:06:56,190 --> 00:06:58,200
we educate all of the players in that

167
00:06:58,200 --> 00:07:00,690
are going to be key and to date we

168
00:07:00,690 --> 00:07:03,120
frankly have done a pretty poor job on

169
00:07:03,120 --> 00:07:05,430
educating people about cyber security

170
00:07:05,430 --> 00:07:07,380
including the cyber security

171
00:07:07,380 --> 00:07:10,110
professionals so we used to apply when a

172
00:07:10,110 --> 00:07:11,790
military guys who used as an Intel

173
00:07:11,790 --> 00:07:12,750
officer you had to play a game called

174
00:07:12,750 --> 00:07:16,050
bet your bars I had to go brief in the

175
00:07:16,050 --> 00:07:17,520
intelligence business when I came in

176
00:07:17,520 --> 00:07:19,050
always the junior officer was the Intel

177
00:07:19,050 --> 00:07:20,820
guy because he was expendable you went

178
00:07:20,820 --> 00:07:22,530
in and you told your commander I think

179
00:07:22,530 --> 00:07:24,510
this is gonna happen and you had to tell

180
00:07:24,510 --> 00:07:26,670
him in terms of what he understood this

181
00:07:26,670 --> 00:07:29,310
many people might die this man my cyber

182
00:07:29,310 --> 00:07:30,720
security guys would come in and talk to

183
00:07:30,720 --> 00:07:33,360
me about 20 minutes of technical risk

184
00:07:33,360 --> 00:07:34,590
and I said well what's that mean to my

185
00:07:34,590 --> 00:07:36,300
mission we don't do your mission I don't

186
00:07:36,300 --> 00:07:39,420
know we got to start talking about this

187
00:07:39,420 --> 00:07:43,080
in terms of mission or money but just to

188
00:07:43,080 --> 00:07:44,820
use an example on 5g in the United

189
00:07:44,820 --> 00:07:46,320
States at least one of the vectors of

190
00:07:46,320 --> 00:07:48,750
vulnerability that's discussed is using

191
00:07:48,750 --> 00:07:51,150
parts from Huawei and that has been a

192
00:07:51,150 --> 00:07:53,340
case that at least for good or for evil

193
00:07:53,340 --> 00:07:56,280
has been successful to keep Huawei from

194
00:07:56,280 --> 00:07:58,980
being axe accessing these contracts with

195
00:07:58,980 --> 00:08:01,260
companies like Verizon and AT&T that

196
00:08:01,260 --> 00:08:04,410
debate is happening here now secretary

197
00:08:04,410 --> 00:08:06,060
Pompeo who will probably be here

198
00:08:06,060 --> 00:08:08,010
tomorrow is making a tour through

199
00:08:08,010 --> 00:08:10,170
Central Europe discussing this is he

200
00:08:10,170 --> 00:08:15,000
right to try to get Huawei excluded from

201
00:08:15,000 --> 00:08:17,340
being part of the what 5g network in

202
00:08:17,340 --> 00:08:18,930
Europe you know from a US government

203
00:08:18,930 --> 00:08:21,480
perspective I guess you'd have to say

204
00:08:21,480 --> 00:08:24,720
he's right that's the u.s. policy I

205
00:08:24,720 --> 00:08:26,160
think a better way to have this

206
00:08:26,160 --> 00:08:28,740
discussion though is about data

207
00:08:28,740 --> 00:08:29,830
sovereignty

208
00:08:29,830 --> 00:08:33,610
so I go out to see lots of international

209
00:08:33,610 --> 00:08:34,659
government partners

210
00:08:34,659 --> 00:08:36,880
I'll candidly admit half of them are

211
00:08:36,880 --> 00:08:39,130
just as worried about the u.s. spying on

212
00:08:39,130 --> 00:08:41,169
them as they are China yeah that's a

213
00:08:41,169 --> 00:08:43,360
fact so what we try to talk about at

214
00:08:43,360 --> 00:08:45,250
Samsung what I think the argument ought

215
00:08:45,250 --> 00:08:47,640
to be about is how do you protect

216
00:08:47,640 --> 00:08:51,070
government data sovereignty so we try to

217
00:08:51,070 --> 00:08:53,020
go in and tell our government customers

218
00:08:53,020 --> 00:08:55,589
hey we're gonna give you a system that

219
00:08:55,589 --> 00:08:59,890
is your system your key your protection

220
00:08:59,890 --> 00:09:03,510
and instead of maybe writing about

221
00:09:03,510 --> 00:09:06,700
individual companies or systems there

222
00:09:06,700 --> 00:09:09,070
needs to be a description of here are

223
00:09:09,070 --> 00:09:10,540
the things that if you're a government

224
00:09:10,540 --> 00:09:12,940
you want you want access to code the

225
00:09:12,940 --> 00:09:15,940
country companies ought to give to

226
00:09:15,940 --> 00:09:18,190
government customers and I'm gonna make

227
00:09:18,190 --> 00:09:19,390
sure I'm specifically talking about

228
00:09:19,390 --> 00:09:21,339
government customers here they've got to

229
00:09:21,339 --> 00:09:22,829
be willing to give more of their code

230
00:09:22,829 --> 00:09:25,209
you've got to be what that wasn't enough

231
00:09:25,209 --> 00:09:28,209
for the US right they also wanted to see

232
00:09:28,209 --> 00:09:30,670
but but you say it wasn't enough I will

233
00:09:30,670 --> 00:09:35,260
argue that some of the companies that

234
00:09:35,260 --> 00:09:37,329
are saying they give code and I had to

235
00:09:37,329 --> 00:09:38,829
see it what I did didn't actually give

236
00:09:38,829 --> 00:09:40,870
me my code they didn't give the code up

237
00:09:40,870 --> 00:09:42,640
giving the code up means you really are

238
00:09:42,640 --> 00:09:45,190
sharing it you've got to talk about

239
00:09:45,190 --> 00:09:47,980
supply chain and you've got to be

240
00:09:47,980 --> 00:09:49,779
willing to have your factories your

241
00:09:49,779 --> 00:09:52,300
spaces they ought to be able to be

242
00:09:52,300 --> 00:09:54,190
inspected on pretty sharp short notice

243
00:09:54,190 --> 00:09:57,850
by governments there's a list of things

244
00:09:57,850 --> 00:09:59,260
I think you would put out and if

245
00:09:59,260 --> 00:10:01,510
everybody plays by those maybe they

246
00:10:01,510 --> 00:10:03,339
should be allowing we're gonna get back

247
00:10:03,339 --> 00:10:05,350
to the supply chain point in a bit but I

248
00:10:05,350 --> 00:10:06,220
want to go to

249
00:10:06,220 --> 00:10:08,320
Reiner and ask about something that came

250
00:10:08,320 --> 00:10:11,200
up in the keynote we're talking about AI

251
00:10:11,200 --> 00:10:13,510
and cyber security can you give us an

252
00:10:13,510 --> 00:10:15,910
assessment of you know the good guys and

253
00:10:15,910 --> 00:10:19,270
the bad guys in AI are how is a I

254
00:10:19,270 --> 00:10:22,029
changing the threatmate matrix online

255
00:10:22,029 --> 00:10:24,670
what is the offensive systems that can

256
00:10:24,670 --> 00:10:28,600
be used in what are the defensive all

257
00:10:28,600 --> 00:10:31,779
technological things have mostly two

258
00:10:31,779 --> 00:10:35,680
sides one of the good side perhaps to

259
00:10:35,680 --> 00:10:39,270
protect more that means in the case of

260
00:10:39,270 --> 00:10:42,790
artifical intelligence more to better

261
00:10:42,790 --> 00:10:44,020
analyze

262
00:10:44,020 --> 00:10:46,930
perhaps network infrastructures -

263
00:10:46,930 --> 00:10:51,690
predictive information about perhaps

264
00:10:51,690 --> 00:10:54,670
vulnerabilities and so on perhaps also

265
00:10:54,670 --> 00:10:57,550
in the area of identification of persons

266
00:10:57,550 --> 00:11:00,520
you can use artifical intelligence very

267
00:11:00,520 --> 00:11:03,130
well but on the other hand I'm

268
00:11:03,130 --> 00:11:07,360
absolutely sure that attackers will use

269
00:11:07,360 --> 00:11:10,030
the same technology if it is available

270
00:11:10,030 --> 00:11:12,130
on the market and it will be available

271
00:11:12,130 --> 00:11:15,820
it will be open sources will be no

272
00:11:15,820 --> 00:11:18,210
access control for such technologies

273
00:11:18,210 --> 00:11:21,970
will be used for intelligent attackers

274
00:11:21,970 --> 00:11:25,030
to analyze the situation to find out who

275
00:11:25,030 --> 00:11:29,240
are the open doors and perhaps also to

276
00:11:29,240 --> 00:11:31,020
[Music]

277
00:11:31,020 --> 00:11:36,340
develop special Trojan horses and so on

278
00:11:36,340 --> 00:11:40,510
which have much more chances not too

279
00:11:40,510 --> 00:11:45,600
easy to detect and not and perhaps more

280
00:11:45,600 --> 00:11:48,700
destroying potentials for critical

281
00:11:48,700 --> 00:11:52,990
infrastructure so it means in

282
00:11:52,990 --> 00:11:57,090
combination with high-speed networks 5g

283
00:11:57,090 --> 00:11:59,640
artifical intelligence could be a risk

284
00:11:59,640 --> 00:12:03,100
also for let's say our infrastructure on

285
00:12:03,100 --> 00:12:06,100
the other hand if we use it really in an

286
00:12:06,100 --> 00:12:09,670
good behavior perhaps in the automotive

287
00:12:09,670 --> 00:12:11,530
sector and so we can protect our

288
00:12:11,530 --> 00:12:13,420
infrastructure with the same technology

289
00:12:13,420 --> 00:12:15,940
yeah Norbert we talked about a critical

290
00:12:15,940 --> 00:12:17,980
infrastructure and industrial systems

291
00:12:17,980 --> 00:12:20,350
this is Siemens bread-and-butter what do

292
00:12:20,350 --> 00:12:22,210
you think what's how is AI changing the

293
00:12:22,210 --> 00:12:25,840
landscape first thank you then before I

294
00:12:25,840 --> 00:12:28,450
start to talk about AI maybe I want to

295
00:12:28,450 --> 00:12:30,520
disagree with one statement it has been

296
00:12:30,520 --> 00:12:32,890
made a few times now in the on the

297
00:12:32,890 --> 00:12:35,440
industrial side I know definitely that

298
00:12:35,440 --> 00:12:37,480
it's not true that nobody wants to spend

299
00:12:37,480 --> 00:12:40,330
money on security we spent a ton of

300
00:12:40,330 --> 00:12:41,500
money on security

301
00:12:41,500 --> 00:12:44,320
now just in in corporate technology we

302
00:12:44,320 --> 00:12:46,810
have more than 200 researchers driving

303
00:12:46,810 --> 00:12:49,000
this we spent money on every product yes

304
00:12:49,000 --> 00:12:51,280
we are not perfect we have cross

305
00:12:51,280 --> 00:12:53,970
industry

306
00:12:53,970 --> 00:12:56,010
Alliance is filled up with our charter

307
00:12:56,010 --> 00:12:57,360
of trust and there's a lot of things

308
00:12:57,360 --> 00:13:00,060
going on in worldwide so the industry is

309
00:13:00,060 --> 00:13:01,320
not perfect when it comes to

310
00:13:01,320 --> 00:13:03,750
cybersecurity by far not perfect now but

311
00:13:03,750 --> 00:13:06,090
we are spending a lot of money in all

312
00:13:06,090 --> 00:13:08,760
these areas and artificial intelligence

313
00:13:08,760 --> 00:13:11,750
I think is definitely one of them now we

314
00:13:11,750 --> 00:13:14,820
we start already to use artificial

315
00:13:14,820 --> 00:13:16,890
intelligence in in many different ways

316
00:13:16,890 --> 00:13:19,980
across everything we do in industry also

317
00:13:19,980 --> 00:13:23,400
in cybersecurity one way we we have you

318
00:13:23,400 --> 00:13:24,960
know we're working on prototypes already

319
00:13:24,960 --> 00:13:29,040
is to more or less in in real time or in

320
00:13:29,040 --> 00:13:32,150
almost real time being able to detect

321
00:13:32,150 --> 00:13:35,910
malicious data now that can be inflicted

322
00:13:35,910 --> 00:13:39,720
in infected into industrial networks now

323
00:13:39,720 --> 00:13:41,760
many people have been trying this on

324
00:13:41,760 --> 00:13:43,470
classical communication networks for a

325
00:13:43,470 --> 00:13:45,510
long time industrial networks have a few

326
00:13:45,510 --> 00:13:47,940
specifics which we can use that make it

327
00:13:47,940 --> 00:13:49,290
much easier to do it in these

328
00:13:49,290 --> 00:13:50,880
environments you know their networks are

329
00:13:50,880 --> 00:13:53,490
much very strictly hierarchical data

330
00:13:53,490 --> 00:13:55,140
structures and data rates are much

331
00:13:55,140 --> 00:13:58,050
easier and slower so we are using this

332
00:13:58,050 --> 00:14:02,130
so detecting attacks much faster is one

333
00:14:02,130 --> 00:14:04,230
way where artificial intelligence can

334
00:14:04,230 --> 00:14:06,360
really help but it can also help and

335
00:14:06,360 --> 00:14:08,010
that will be probably the next step we

336
00:14:08,010 --> 00:14:10,980
see is not only detecting attacks but

337
00:14:10,980 --> 00:14:12,900
really understanding from the patterns

338
00:14:12,900 --> 00:14:16,890
we see what is the the anticipated or

339
00:14:16,890 --> 00:14:20,010
the the impact the attack tries to have

340
00:14:20,010 --> 00:14:23,160
to build a broader and more more

341
00:14:23,160 --> 00:14:26,040
comprehensive defense strategy

342
00:14:26,040 --> 00:14:29,190
and the next step we see will then

343
00:14:29,190 --> 00:14:31,470
really be and what we have been doing in

344
00:14:31,470 --> 00:14:33,270
in maintenance for example everybody is

345
00:14:33,270 --> 00:14:34,770
talking with AI about predictive

346
00:14:34,770 --> 00:14:36,930
maintenance looking at all the attack

347
00:14:36,930 --> 00:14:39,510
patterns I am convinced that in the end

348
00:14:39,510 --> 00:14:43,140
we will be able to predict from past and

349
00:14:43,140 --> 00:14:45,630
current attacks across a large number of

350
00:14:45,630 --> 00:14:49,500
also different systems predict future

351
00:14:49,500 --> 00:14:52,350
attack patterns and then pre-developed

352
00:14:52,350 --> 00:14:55,050
potential defense strategies so that

353
00:14:55,050 --> 00:14:57,450
this first mover advantage shrinks

354
00:14:57,450 --> 00:15:00,330
eventually might even become very very

355
00:15:00,330 --> 00:15:02,520
small you know yes that's a vision and

356
00:15:02,520 --> 00:15:04,890
it will take a while but artificial

357
00:15:04,890 --> 00:15:06,970
intelligence I think is here

358
00:15:06,970 --> 00:15:09,430
or one technology that might change the

359
00:15:09,430 --> 00:15:11,470
game of this first mover advantage and

360
00:15:11,470 --> 00:15:14,470
where the attacker always has the the

361
00:15:14,470 --> 00:15:16,930
advantage over the defender that's

362
00:15:16,930 --> 00:15:18,190
interesting so you're saying that this

363
00:15:18,190 --> 00:15:20,380
is leveling back in in favor of the

364
00:15:20,380 --> 00:15:23,380
defender a little bit that's what we

365
00:15:23,380 --> 00:15:25,120
would hope but I would already be happy

366
00:15:25,120 --> 00:15:27,970
if the disadvantage is going to go away

367
00:15:27,970 --> 00:15:30,610
you know if it's equal level I'd be fine

368
00:15:30,610 --> 00:15:33,280
already and and already there will be a

369
00:15:33,280 --> 00:15:35,140
lot of work and need a little bit of

370
00:15:35,140 --> 00:15:36,100
time

371
00:15:36,100 --> 00:15:39,970
good invert this idea of supply chains

372
00:15:39,970 --> 00:15:42,160
in the end the role of AI and protecting

373
00:15:42,160 --> 00:15:43,660
supply chains was mentioned how do you

374
00:15:43,660 --> 00:15:45,550
see changes in the cyber security

375
00:15:45,550 --> 00:15:47,560
landscape with these new technologies

376
00:15:47,560 --> 00:15:50,800
but also new logistics systems coming

377
00:15:50,800 --> 00:15:54,070
online yeah I'm happy to maybe edit the

378
00:15:54,070 --> 00:15:55,990
business perspective to this I'm just

379
00:15:55,990 --> 00:15:57,040
for the fun of it I probably have to

380
00:15:57,040 --> 00:15:58,480
disagree with the last statement only of

381
00:15:58,480 --> 00:16:00,040
what octagon said I agree with

382
00:16:00,040 --> 00:16:01,810
everything else but I actually think

383
00:16:01,810 --> 00:16:03,700
that the gap between defend and often is

384
00:16:03,700 --> 00:16:04,930
probably good unfortunately become

385
00:16:04,930 --> 00:16:06,370
bigger and I think the main reason is

386
00:16:06,370 --> 00:16:08,470
that and I think it was mentioned in the

387
00:16:08,470 --> 00:16:10,750
impulse statement the new technologies

388
00:16:10,750 --> 00:16:12,400
are basically just too attractive to

389
00:16:12,400 --> 00:16:14,500
resist for business just to give you one

390
00:16:14,500 --> 00:16:16,780
example we just looked at ai ai I think

391
00:16:16,780 --> 00:16:18,100
in the last one or two years we see it

392
00:16:18,100 --> 00:16:19,660
we talk about it a lot has really taken

393
00:16:19,660 --> 00:16:22,480
off and so we looked at McKinsey at the

394
00:16:22,480 --> 00:16:24,820
400 use cases that we could see being

395
00:16:24,820 --> 00:16:26,230
reasonably happening in the next year's

396
00:16:26,230 --> 00:16:28,150
and the value of that and the the big

397
00:16:28,150 --> 00:16:29,380
number that comes out of this is that

398
00:16:29,380 --> 00:16:32,290
until 2030 Europe alone could add 2.7

399
00:16:32,290 --> 00:16:34,300
trillion euros to its GDP by

400
00:16:34,300 --> 00:16:36,460
implementing AI it doesn't matter

401
00:16:36,460 --> 00:16:38,140
whether it's 2.7 or 2.5 trillion but

402
00:16:38,140 --> 00:16:39,910
it's a very big number and what that

403
00:16:39,910 --> 00:16:41,290
means if you're sitting inside a company

404
00:16:41,290 --> 00:16:43,240
and the companies that of course I also

405
00:16:43,240 --> 00:16:46,000
serve as that means that the business is

406
00:16:46,000 --> 00:16:47,050
firmly in the driver seat and is

407
00:16:47,050 --> 00:16:49,630
pursuing that opportunity all right and

408
00:16:49,630 --> 00:16:51,910
I think that is a major paradigm shift

409
00:16:51,910 --> 00:16:54,130
from the digitisation wave we've had in

410
00:16:54,130 --> 00:16:55,600
the last years because what it means now

411
00:16:55,600 --> 00:16:57,820
is the core business is being digitized

412
00:16:57,820 --> 00:16:59,560
not the periphery it's not corporate IT

413
00:16:59,560 --> 00:17:02,050
we're talking about so supporting things

414
00:17:02,050 --> 00:17:03,880
but it's actually the product the

415
00:17:03,880 --> 00:17:05,589
production it's the development

416
00:17:05,589 --> 00:17:07,869
processes and that's a huge paradigm

417
00:17:07,869 --> 00:17:09,010
shift I think we talked about about

418
00:17:09,010 --> 00:17:10,480
paradigm shifts today for good reason

419
00:17:10,480 --> 00:17:13,359
because that is actually now people in

420
00:17:13,359 --> 00:17:15,010
charge of this that if their foot firmly

421
00:17:15,010 --> 00:17:17,619
on the accelerator and the people

422
00:17:17,619 --> 00:17:20,010
currently in charge of IT security who

423
00:17:20,010 --> 00:17:21,030
when in power to do that I actually

424
00:17:21,030 --> 00:17:22,949
sitting in IT because that's what

425
00:17:22,949 --> 00:17:24,690
traditionally the challenge has been

426
00:17:24,690 --> 00:17:26,760
right that's changing now but it's

427
00:17:26,760 --> 00:17:29,070
changing this is why I would say it's a

428
00:17:29,070 --> 00:17:31,470
it's a it's a difficult challenge it's

429
00:17:31,470 --> 00:17:33,390
changing too slowly I'm still from what

430
00:17:33,390 --> 00:17:36,060
we see largely because the management

431
00:17:36,060 --> 00:17:37,500
system to deal with this complexity that

432
00:17:37,500 --> 00:17:39,270
we're adding is just taking time to

433
00:17:39,270 --> 00:17:40,500
implement all right so what you see now

434
00:17:40,500 --> 00:17:42,600
and as the last comment maybe what you

435
00:17:42,600 --> 00:17:46,020
see now in manufacturing and automotive

436
00:17:46,020 --> 00:17:47,520
and a lot of these industries is they're

437
00:17:47,520 --> 00:17:49,440
changing but this goes all the way down

438
00:17:49,440 --> 00:17:50,940
to the supply chain it goes into

439
00:17:50,940 --> 00:17:52,710
procurement it goes into development

440
00:17:52,710 --> 00:17:54,150
there's a lot of people needed you

441
00:17:54,150 --> 00:17:55,650
basically need entirely new management

442
00:17:55,650 --> 00:17:57,660
system I would say the best analogy we

443
00:17:57,660 --> 00:18:00,300
have is quality how we manage quality I

444
00:18:00,300 --> 00:18:01,800
mean you know you don't manage quality

445
00:18:01,800 --> 00:18:03,630
as an afterthought you don't look at the

446
00:18:03,630 --> 00:18:06,450
infamous German white mass after you've

447
00:18:06,450 --> 00:18:08,010
produced the car right but it's actually

448
00:18:08,010 --> 00:18:09,960
a paradigm that goes all the way to the

449
00:18:09,960 --> 00:18:11,190
very beginning when you design it I

450
00:18:11,190 --> 00:18:13,560
think that shift is I think I'm afraid

451
00:18:13,560 --> 00:18:15,390
gonna take time and at the same time the

452
00:18:15,390 --> 00:18:16,890
business is going to keep its foot on

453
00:18:16,890 --> 00:18:18,450
the accelerator yeah this is what

454
00:18:18,450 --> 00:18:19,950
essentially what we talk about when we

455
00:18:19,950 --> 00:18:21,180
say deep tech I mean you're really

456
00:18:21,180 --> 00:18:23,520
talking about just going into the entire

457
00:18:23,520 --> 00:18:26,640
system the organization management every

458
00:18:26,640 --> 00:18:29,460
aspect is going to be digitized you Jane

459
00:18:29,460 --> 00:18:31,350
you work a lot with the US government on

460
00:18:31,350 --> 00:18:33,120
this stuff are you seeing this kind of

461
00:18:33,120 --> 00:18:36,390
let's say organizational anthropology

462
00:18:36,390 --> 00:18:37,950
change in the US government and how it

463
00:18:37,950 --> 00:18:41,330
approaches things like logistics

464
00:18:41,330 --> 00:18:43,950
absolutely quite interesting if I may

465
00:18:43,950 --> 00:18:46,380
can I go back to we talk about AI here

466
00:18:46,380 --> 00:18:49,140
yeah hey I really is it's a must I mean

467
00:18:49,140 --> 00:18:51,480
everybody's using AI today to certain

468
00:18:51,480 --> 00:18:53,310
degrees and think about this it's not

469
00:18:53,310 --> 00:18:56,130
just us using AI the bad actors are

470
00:18:56,130 --> 00:18:58,830
using AI as well and think about it if

471
00:18:58,830 --> 00:19:00,690
you were the bad actors you got to think

472
00:19:00,690 --> 00:19:03,360
like a hackers what would they do they

473
00:19:03,360 --> 00:19:05,310
will attack your AI as opposed to your

474
00:19:05,310 --> 00:19:07,890
generic data so while we're talking

475
00:19:07,890 --> 00:19:09,840
about AI does not forget you had to

476
00:19:09,840 --> 00:19:12,690
protect AI as well first of all second

477
00:19:12,690 --> 00:19:14,880
of war here is that you know in the

478
00:19:14,880 --> 00:19:17,580
cyber secure we're leveraging AI to do

479
00:19:17,580 --> 00:19:19,710
the threat detection think about that

480
00:19:19,710 --> 00:19:21,690
detecting threat it's only step number

481
00:19:21,690 --> 00:19:24,540
one what do you do next is to be able to

482
00:19:24,540 --> 00:19:26,850
automate your process in a small way so

483
00:19:26,850 --> 00:19:29,700
you can t fear the attacks and lastly

484
00:19:29,700 --> 00:19:31,770
here is very importantly like my

485
00:19:31,770 --> 00:19:33,650
panelist was talking about yes

486
00:19:33,650 --> 00:19:35,390
how you to predict the train of your

487
00:19:35,390 --> 00:19:38,840
threat environment and now be into the

488
00:19:38,840 --> 00:19:40,310
next question here we talk about supply

489
00:19:40,310 --> 00:19:42,650
chain right supply chain is not just

490
00:19:42,650 --> 00:19:45,170
that first-tier suppliers we'll talk a

491
00:19:45,170 --> 00:19:47,900
second here cert here we also not just

492
00:19:47,900 --> 00:19:50,240
talk on the software that deliver to

493
00:19:50,240 --> 00:19:52,070
your shop you also talked about me know

494
00:19:52,070 --> 00:19:54,770
where they are spyware in bed in the

495
00:19:54,770 --> 00:19:57,080
Middle where they are spyware embedded

496
00:19:57,080 --> 00:19:59,960
in your heart in how we're even embedded

497
00:19:59,960 --> 00:20:02,570
in your second tier or certain service

498
00:20:02,570 --> 00:20:05,500
providers environments and so those

499
00:20:05,500 --> 00:20:08,000
attack can be coming from all different

500
00:20:08,000 --> 00:20:11,950
directions okay now how do we deploy AI

501
00:20:11,950 --> 00:20:14,780
we got a very complex problem here and

502
00:20:14,780 --> 00:20:16,460
this is why I think coming down to the

503
00:20:16,460 --> 00:20:19,070
bottom line here yes AI is not a linear

504
00:20:19,070 --> 00:20:21,470
problem it's a honey Nia problem and

505
00:20:21,470 --> 00:20:23,780
that's what cyber security today I still

506
00:20:23,780 --> 00:20:25,250
hearing a lot of folks talk about cyber

507
00:20:25,250 --> 00:20:26,990
security I always talk about what do a

508
00:20:26,990 --> 00:20:29,980
then B and then C in the serial thinking

509
00:20:29,980 --> 00:20:33,110
paradigm shift I think security meltdown

510
00:20:33,110 --> 00:20:34,730
I think two years ago I've been talking

511
00:20:34,730 --> 00:20:36,770
about there is a cyber perfect storm and

512
00:20:36,770 --> 00:20:38,780
we're in the middle of the cyber perfect

513
00:20:38,780 --> 00:20:42,500
storm because not only we have to deal

514
00:20:42,500 --> 00:20:44,960
with the legacy technology we also have

515
00:20:44,960 --> 00:20:47,660
innovation continue coming out Rochon

516
00:20:47,660 --> 00:20:51,200
Kyle quantum computing IOT and you name

517
00:20:51,200 --> 00:20:52,610
it tomorrow we're gonna get a new term

518
00:20:52,610 --> 00:20:54,890
here okay so you have emphasized the

519
00:20:54,890 --> 00:20:56,330
threat and you talked a little bit about

520
00:20:56,330 --> 00:20:59,060
data and data manipulation as being a

521
00:20:59,060 --> 00:21:01,250
vector a threat vector what about the

522
00:21:01,250 --> 00:21:04,580
use of source code is that a threat in

523
00:21:04,580 --> 00:21:06,170
the AI space what are the other threats

524
00:21:06,170 --> 00:21:07,550
what do you see is the most important

525
00:21:07,550 --> 00:21:09,410
threat as companies are deploying AI

526
00:21:09,410 --> 00:21:11,930
only talking of the AI space are we

527
00:21:11,930 --> 00:21:14,150
imagine you know not only just the

528
00:21:14,150 --> 00:21:16,400
source code but also as how you deploy

529
00:21:16,400 --> 00:21:19,430
that AI think about bias bias opinion I

530
00:21:19,430 --> 00:21:21,950
can actually manipulate your a a result

531
00:21:21,950 --> 00:21:24,770
by you know putting the bias opinion

532
00:21:24,770 --> 00:21:28,010
bias algorithm into your eye and this is

533
00:21:28,010 --> 00:21:30,230
why if you look at IBM I should have put

534
00:21:30,230 --> 00:21:32,530
you know gathering 1 million facial

535
00:21:32,530 --> 00:21:36,590
images to do what to bypass the bias on

536
00:21:36,590 --> 00:21:39,430
the skin color on the facial shape

537
00:21:39,430 --> 00:21:42,230
because the bias can actually manipulate

538
00:21:42,230 --> 00:21:44,630
I and really the loaded your AR

539
00:21:44,630 --> 00:21:45,770
accuracies

540
00:21:45,770 --> 00:21:47,630
and this is where what I'm kind of going

541
00:21:47,630 --> 00:21:50,450
back to security for AI it's something

542
00:21:50,450 --> 00:21:53,630
that we can take it lightly and this is

543
00:21:53,630 --> 00:21:56,420
where security first AI is one of the

544
00:21:56,420 --> 00:21:58,280
technology that we are deploying and we

545
00:21:58,280 --> 00:22:00,290
got to put security first and this is

546
00:22:00,290 --> 00:22:01,820
where a lot of folks talk about security

547
00:22:01,820 --> 00:22:03,950
by design think about it security by

548
00:22:03,950 --> 00:22:05,570
design only for the new system you gonna

549
00:22:05,570 --> 00:22:07,760
deploy how about the legacy environment

550
00:22:07,760 --> 00:22:09,590
because when you exercise ai you're

551
00:22:09,590 --> 00:22:11,780
gonna probably gonna inter inter

552
00:22:11,780 --> 00:22:13,760
integrate it well interoperable with

553
00:22:13,760 --> 00:22:16,460
your existing system and that's why how

554
00:22:16,460 --> 00:22:18,170
you create this holistic security

555
00:22:18,170 --> 00:22:20,240
postures so not only you can deploy AI

556
00:22:20,240 --> 00:22:23,120
this entire ecosystem is secure let me

557
00:22:23,120 --> 00:22:24,890
let me bring in Terry on this because

558
00:22:24,890 --> 00:22:27,530
what what Soojin just brought up is you

559
00:22:27,530 --> 00:22:28,640
know this isn't happening in a vacuum

560
00:22:28,640 --> 00:22:31,730
this is coming over legacy systems you

561
00:22:31,730 --> 00:22:33,020
have worked in the Pentagon you're

562
00:22:33,020 --> 00:22:35,059
working with Samsung huge institutions

563
00:22:35,059 --> 00:22:36,950
huge organizations obviously building

564
00:22:36,950 --> 00:22:38,900
their technologies on top of what exists

565
00:22:38,900 --> 00:22:40,280
how are you making sure that those

566
00:22:40,280 --> 00:22:42,880
pre-existing technologies are safe I

567
00:22:42,880 --> 00:22:45,440
don't know that you can actually make

568
00:22:45,440 --> 00:22:47,179
sure that they're all going to be safe

569
00:22:47,179 --> 00:22:49,910
what you can do though is make sure that

570
00:22:49,910 --> 00:22:52,670
as you're building your new systems you

571
00:22:52,670 --> 00:22:54,740
are at least recognizing that it is

572
00:22:54,740 --> 00:22:57,020
going to be playing in a legacy world

573
00:22:57,020 --> 00:23:00,200
first so you need to think about those

574
00:23:00,200 --> 00:23:01,850
vulnerabilities and to the extent you

575
00:23:01,850 --> 00:23:05,630
can try to either back fit or the the

576
00:23:05,630 --> 00:23:08,300
word I tend to use more now is rap there

577
00:23:08,300 --> 00:23:11,000
are ways today with that ability if you

578
00:23:11,000 --> 00:23:12,980
look at it homeless if you take AI if

579
00:23:12,980 --> 00:23:15,050
you take the faster speeds if you take

580
00:23:15,050 --> 00:23:17,900
the lower latency I can build from the

581
00:23:17,900 --> 00:23:19,640
beginning a system that can partially

582
00:23:19,640 --> 00:23:22,520
rap the legacy that's out there that

583
00:23:22,520 --> 00:23:25,790
will give us a higher degree of security

584
00:23:25,790 --> 00:23:28,100
than we have today but I think the other

585
00:23:28,100 --> 00:23:30,740
thing we need to first admit there is no

586
00:23:30,740 --> 00:23:33,320
perfect security yep the other thing I

587
00:23:33,320 --> 00:23:35,410
tell everybody built every system today

588
00:23:35,410 --> 00:23:38,690
knowing not if but at some point it's

589
00:23:38,690 --> 00:23:41,900
going to be corrupted so you heard the

590
00:23:41,900 --> 00:23:42,770
last panel talked a little about

591
00:23:42,770 --> 00:23:45,110
resiliency I do think you can do

592
00:23:45,110 --> 00:23:47,990
pre-planned resiliency too and I think

593
00:23:47,990 --> 00:23:49,760
we need to think hard about what does

594
00:23:49,760 --> 00:23:51,980
that mean you know I remember when I

595
00:23:51,980 --> 00:23:53,929
first started you first reaction you had

596
00:23:53,929 --> 00:23:56,780
to an attack was shut down the system

597
00:23:56,780 --> 00:23:58,490
that's pretty stupid because then you've

598
00:23:58,490 --> 00:23:59,660
they've won

599
00:23:59,660 --> 00:24:01,429
you've got to think about what you're

600
00:24:01,429 --> 00:24:03,350
gonna do how you how do you actually

601
00:24:03,350 --> 00:24:05,419
operate through the attack protect part

602
00:24:05,419 --> 00:24:07,940
of the systems that has probably been

603
00:24:07,940 --> 00:24:10,190
the weakest part of how we think about

604
00:24:10,190 --> 00:24:12,500
cyber security not thinking about you're

605
00:24:12,500 --> 00:24:13,730
going to get attacked what's the

606
00:24:13,730 --> 00:24:16,520
resiliency plan so Reiner you are

607
00:24:16,520 --> 00:24:18,650
working with the German government on

608
00:24:18,650 --> 00:24:20,330
these kind of questions how are you

609
00:24:20,330 --> 00:24:22,250
starting to bring in these next-gen

610
00:24:22,250 --> 00:24:24,080
technologies into their thinking about

611
00:24:24,080 --> 00:24:28,160
resiliency what we have seen in the last

612
00:24:28,160 --> 00:24:33,350
let's say 30 years as always that if

613
00:24:33,350 --> 00:24:37,309
there is any regulation it's could be

614
00:24:37,309 --> 00:24:41,059
absolutely helpful for let's say makes

615
00:24:41,059 --> 00:24:45,530
more trust business insights of products

616
00:24:45,530 --> 00:24:48,890
that's our experiences we have seen a

617
00:24:48,890 --> 00:24:51,380
lot of development especially in the

618
00:24:51,380 --> 00:24:55,400
high security area that it's absolutely

619
00:24:55,400 --> 00:24:58,490
helpful to have let's say product

620
00:24:58,490 --> 00:25:01,270
certification qualification and so on

621
00:25:01,270 --> 00:25:05,780
otherwise you have white West in the

622
00:25:05,780 --> 00:25:07,789
software development everything is

623
00:25:07,789 --> 00:25:10,659
possible and you have no control about

624
00:25:10,659 --> 00:25:14,330
such new devices new solutions new

625
00:25:14,330 --> 00:25:18,350
software and so and we have it also in

626
00:25:18,350 --> 00:25:21,110
other technology areas look at the

627
00:25:21,110 --> 00:25:24,070
automotive sector I tell the story

628
00:25:24,070 --> 00:25:30,200
always again for let's say in the 70s we

629
00:25:30,200 --> 00:25:32,780
had in Germany certain thousand dead

630
00:25:32,780 --> 00:25:36,260
people on the roads then there was a

631
00:25:36,260 --> 00:25:39,140
regulation to that all the manufacturer

632
00:25:39,140 --> 00:25:43,190
have to implement and security belt but

633
00:25:43,190 --> 00:25:47,510
nobody used it then the regulation came

634
00:25:47,510 --> 00:25:53,299
with this time saying the 10 dots mark

635
00:25:53,299 --> 00:25:57,980
if you don't use a belt immediately 90

636
00:25:57,980 --> 00:26:02,450
percent used it and now we have too much

637
00:26:02,450 --> 00:26:05,809
but only 3,000 dead people on the road

638
00:26:05,809 --> 00:26:08,900
so it shows security products could be

639
00:26:08,900 --> 00:26:11,900
helpful but a regulation of the

640
00:26:11,900 --> 00:26:13,220
government is

641
00:26:13,220 --> 00:26:15,530
not useful otherwise the people will not

642
00:26:15,530 --> 00:26:18,770
invest in that will not do it or will

643
00:26:18,770 --> 00:26:22,429
not use it because to use a belt is the

644
00:26:22,429 --> 00:26:25,580
same like switch on security products

645
00:26:25,580 --> 00:26:29,030
take a smart card take a software

646
00:26:29,030 --> 00:26:31,970
security on top make any other

647
00:26:31,970 --> 00:26:34,520
protection use it switch it on

648
00:26:34,520 --> 00:26:35,919
perhaps and make it a little bit slower

649
00:26:35,919 --> 00:26:38,480
so on otherwise you will not be

650
00:26:38,480 --> 00:26:42,200
successful therefore our way must be to

651
00:26:42,200 --> 00:26:46,250
have an security level which is useful

652
00:26:46,250 --> 00:26:50,179
and an high enough on one hand and it's

653
00:26:50,179 --> 00:26:52,659
there's a qualification behind and

654
00:26:52,659 --> 00:26:55,100
therefore all the artificial

655
00:26:55,100 --> 00:26:57,530
intelligence could be helpful to make

656
00:26:57,530 --> 00:27:00,049
such qualification processes faster so

657
00:27:00,049 --> 00:27:01,460
you're talking about regulation a

658
00:27:01,460 --> 00:27:04,490
regulatory centric model based on fines

659
00:27:04,490 --> 00:27:06,890
based on punitive fines as being a good

660
00:27:06,890 --> 00:27:10,100
incentive can we talk a little bit about

661
00:27:10,100 --> 00:27:12,140
the culture here in Europe how do you

662
00:27:12,140 --> 00:27:13,909
think that Europe is gonna create the

663
00:27:13,909 --> 00:27:16,549
incentive structure to a deploy these

664
00:27:16,549 --> 00:27:19,309
next gen technologies like AI but B

665
00:27:19,309 --> 00:27:21,919
create incentive structures perhaps

666
00:27:21,919 --> 00:27:23,330
through regulation that will make them

667
00:27:23,330 --> 00:27:25,250
safe and how do you see that when you

668
00:27:25,250 --> 00:27:26,659
compare it to places like the United

669
00:27:26,659 --> 00:27:28,400
States or Asia and maybe Thierry we can

670
00:27:28,400 --> 00:27:31,549
get you in on this as well well luckily

671
00:27:31,549 --> 00:27:33,289
we have other experts in the room like

672
00:27:33,289 --> 00:27:35,210
mr. Heimlich mr. Kuhn who will probably

673
00:27:35,210 --> 00:27:36,799
talk about this I'm very informed a

674
00:27:36,799 --> 00:27:38,690
government perspective but I think

675
00:27:38,690 --> 00:27:40,690
there's a couple of things that I think

676
00:27:40,690 --> 00:27:43,159
two things that I'd like to highlight I

677
00:27:43,159 --> 00:27:46,370
think the one thing is we do see that

678
00:27:46,370 --> 00:27:49,610
regulation has a strong impact on

679
00:27:49,610 --> 00:27:52,039
businesses obviously and actually does

680
00:27:52,039 --> 00:27:53,480
trigger quite some positive things I

681
00:27:53,480 --> 00:27:54,770
think we often talk about the negative

682
00:27:54,770 --> 00:27:56,570
things of regulation and we everyone has

683
00:27:56,570 --> 00:27:57,710
their philosophy about whether it's

684
00:27:57,710 --> 00:27:59,630
nudging or regulation you prefer more

685
00:27:59,630 --> 00:28:01,730
but the one thing you do see is that all

686
00:28:01,730 --> 00:28:03,559
those notches even if they don't come

687
00:28:03,559 --> 00:28:05,360
with fines or so even if it's just a

688
00:28:05,360 --> 00:28:07,039
sort of UN standard that gets

689
00:28:07,039 --> 00:28:09,380
implemented an ISO standard there are

690
00:28:09,380 --> 00:28:11,809
sort of ways of establishing practices

691
00:28:11,809 --> 00:28:13,820
in the industries that do create

692
00:28:13,820 --> 00:28:15,890
progress so I work with a lot of

693
00:28:15,890 --> 00:28:17,960
companies that very strongly built a

694
00:28:17,960 --> 00:28:19,669
sort of security management systems

695
00:28:19,669 --> 00:28:21,740
around some of those standards or some

696
00:28:21,740 --> 00:28:23,419
regulations that they think are coming

697
00:28:23,419 --> 00:28:25,100
down the line so I think there's people

698
00:28:25,100 --> 00:28:25,970
moving along

699
00:28:25,970 --> 00:28:27,650
and I think the European paradigm is

700
00:28:27,650 --> 00:28:29,840
probably slightly strong on that side

701
00:28:29,840 --> 00:28:31,130
especially the German I think IT

702
00:28:31,130 --> 00:28:32,570
security law is I think generally

703
00:28:32,570 --> 00:28:34,730
recognized as a very strong law but I

704
00:28:34,730 --> 00:28:36,530
think it's effective and working

705
00:28:36,530 --> 00:28:38,210
I think Terry's probably am a better

706
00:28:38,210 --> 00:28:39,410
place to talk about the u.s. approach

707
00:28:39,410 --> 00:28:41,390
but from a European perspective it

708
00:28:41,390 --> 00:28:42,710
strikes me as slightly more on the

709
00:28:42,710 --> 00:28:44,840
nudging side and the effect of that

710
00:28:44,840 --> 00:28:46,610
he'll know better I think there's a

711
00:28:46,610 --> 00:28:47,810
second thing though that we oftentimes

712
00:28:47,810 --> 00:28:49,490
forget that I would like to highlight is

713
00:28:49,490 --> 00:28:51,850
we also need to invest into the

714
00:28:51,850 --> 00:28:55,310
ecosystems and what I mean by that is I

715
00:28:55,310 --> 00:28:58,070
think you really in this very deep

716
00:28:58,070 --> 00:28:59,690
digital world that you've described if

717
00:28:59,690 --> 00:29:00,860
you really want to factor as a

718
00:29:00,860 --> 00:29:02,870
government you really will only factor

719
00:29:02,870 --> 00:29:05,900
if you also have an industry I think if

720
00:29:05,900 --> 00:29:07,910
you and I think we've historically had

721
00:29:07,910 --> 00:29:09,260
actually a strong industry in this I

722
00:29:09,260 --> 00:29:11,360
think we have representatives sitting

723
00:29:11,360 --> 00:29:13,760
here but just to give you a slight sort

724
00:29:13,760 --> 00:29:16,280
of like caution for warning caution for

725
00:29:16,280 --> 00:29:19,730
optimism the warning I think is I looked

726
00:29:19,730 --> 00:29:22,460
at patents of security products 10-15

727
00:29:22,460 --> 00:29:24,200
years ago and actually Germany was in

728
00:29:24,200 --> 00:29:25,640
number four actually quite strong and a

729
00:29:25,640 --> 00:29:28,850
lot of the Reuter related security

730
00:29:28,850 --> 00:29:30,530
products when we look at the current

731
00:29:30,530 --> 00:29:31,520
technologies that you described

732
00:29:31,520 --> 00:29:35,390
blockchain AI moving target defense sort

733
00:29:35,390 --> 00:29:39,350
of find here buzzword then we're nowhere

734
00:29:39,350 --> 00:29:41,360
near that right we are factor 10 20

735
00:29:41,360 --> 00:29:43,850
sometimes 50 behind the payments of many

736
00:29:43,850 --> 00:29:45,950
other countries even though and this is

737
00:29:45,950 --> 00:29:47,300
the positive side there is I think a lot

738
00:29:47,300 --> 00:29:50,150
of potential I think this is why this

739
00:29:50,150 --> 00:29:52,160
convention in this area is I think a

740
00:29:52,160 --> 00:29:52,880
very good idea

741
00:29:52,880 --> 00:29:54,920
because we have six million developers

742
00:29:54,920 --> 00:29:56,240
in Europe that's actually more than the

743
00:29:56,240 --> 00:29:58,250
United States so contrary to popular

744
00:29:58,250 --> 00:29:59,480
belief we actually have a lot of

745
00:29:59,480 --> 00:30:00,050
potential

746
00:30:00,050 --> 00:30:01,940
we just need to manage to cluster it

747
00:30:01,940 --> 00:30:04,010
bundle it more not fragmented and I

748
00:30:04,010 --> 00:30:06,620
think ecosystems like in this region and

749
00:30:06,620 --> 00:30:08,540
other places are things that governments

750
00:30:08,540 --> 00:30:09,830
can support that's a great point and

751
00:30:09,830 --> 00:30:10,700
we're going to get to some of those

752
00:30:10,700 --> 00:30:12,560
other buzzwords in a second but Terry I

753
00:30:12,560 --> 00:30:15,430
want to take this point about ecosystems

754
00:30:15,430 --> 00:30:17,120
Guttenberg mentioned the fact that

755
00:30:17,120 --> 00:30:18,770
ecosystems are so important in

756
00:30:18,770 --> 00:30:21,740
optimizing AI how does the ecosystem the

757
00:30:21,740 --> 00:30:25,580
government University private sector

758
00:30:25,580 --> 00:30:26,780
relationship when dealing with

759
00:30:26,780 --> 00:30:28,730
cybersecurity look in the United States

760
00:30:28,730 --> 00:30:30,170
when we're talking about these next-gen

761
00:30:30,170 --> 00:30:32,630
technologies so two things I want to

762
00:30:32,630 --> 00:30:34,310
correct what you said Rainer didn't say

763
00:30:34,310 --> 00:30:39,140
to build a rule-based or an incentive

764
00:30:39,140 --> 00:30:39,860
based system

765
00:30:39,860 --> 00:30:42,320
what he said was we made seatbelts

766
00:30:42,320 --> 00:30:45,559
easy-to-use people still didn't use them

767
00:30:45,559 --> 00:30:47,179
right and then we put on a little

768
00:30:47,179 --> 00:30:48,740
incentive by raising the pipe but it

769
00:30:48,740 --> 00:30:50,659
still had a negative incentive but it

770
00:30:50,659 --> 00:30:53,899
still had to be easy to use right so one

771
00:30:53,899 --> 00:30:55,370
of the things I think that we've got to

772
00:30:55,370 --> 00:30:57,769
think about a dis is all gotta be a

773
00:30:57,769 --> 00:31:01,370
related system totally 100% I mean if

774
00:31:01,370 --> 00:31:04,220
you make something really hard to use I

775
00:31:04,220 --> 00:31:06,260
mean some of you probably lived through

776
00:31:06,260 --> 00:31:09,710
this in governments industry make a

777
00:31:09,710 --> 00:31:13,130
one-time one of my systems had a 23

778
00:31:13,130 --> 00:31:17,389
character password required now I don't

779
00:31:17,389 --> 00:31:18,679
know about you but I have trouble

780
00:31:18,679 --> 00:31:21,289
remembering my name some warnings in the

781
00:31:21,289 --> 00:31:23,179
23 character and don't write it down

782
00:31:23,179 --> 00:31:25,309
don't store it anywhere great that's

783
00:31:25,309 --> 00:31:26,419
stupid

784
00:31:26,419 --> 00:31:28,340
people can't do that and they're not

785
00:31:28,340 --> 00:31:30,909
gonna do that so you do have to balance

786
00:31:30,909 --> 00:31:35,149
how do I make it easy how do I show both

787
00:31:35,149 --> 00:31:37,010
corporate and consumer and their

788
00:31:37,010 --> 00:31:39,139
different models that's the other

789
00:31:39,139 --> 00:31:40,340
difference I would tell you you're gonna

790
00:31:40,340 --> 00:31:42,529
have a model for a corporate structure

791
00:31:42,529 --> 00:31:44,630
here that will be different than the

792
00:31:44,630 --> 00:31:47,059
model for the consumer structure that's

793
00:31:47,059 --> 00:31:50,059
okay we do tend to think about these one

794
00:31:50,059 --> 00:31:52,100
models and everything has to fit there's

795
00:31:52,100 --> 00:31:54,490
going to be multiple models and

796
00:31:54,490 --> 00:31:57,279
everything has to fit in that multiple

797
00:31:57,279 --> 00:32:00,019
so in the US I can't give you some plan

798
00:32:00,019 --> 00:32:02,299
sort of how to how does that all work I

799
00:32:02,299 --> 00:32:05,570
think you're seeing the the u.s. start

800
00:32:05,570 --> 00:32:07,970
to teach cybersecurity

801
00:32:07,970 --> 00:32:10,220
where's personal we're doing that at an

802
00:32:10,220 --> 00:32:12,500
earlier age doing enough of that yet no

803
00:32:12,500 --> 00:32:16,220
but it's moving in direction to my

804
00:32:16,220 --> 00:32:18,169
friend from Siemens I do see more money

805
00:32:18,169 --> 00:32:20,960
being spent on security but what I would

806
00:32:20,960 --> 00:32:24,320
say is that lagged our knowledge of the

807
00:32:24,320 --> 00:32:26,809
security problems we started doing that

808
00:32:26,809 --> 00:32:29,330
at a point where I think we looked at it

809
00:32:29,330 --> 00:32:31,610
and said ok now potentially the security

810
00:32:31,610 --> 00:32:34,309
problem causes more costs more money

811
00:32:34,309 --> 00:32:37,399
than not doing the security problem and

812
00:32:37,399 --> 00:32:39,200
that's ok I mean that that's that's a

813
00:32:39,200 --> 00:32:42,139
risk equation and I do think you've got

814
00:32:42,139 --> 00:32:45,110
to understand the risk equations much

815
00:32:45,110 --> 00:32:46,820
better than we do today

816
00:32:46,820 --> 00:32:49,309
and again risk equation for a consumer

817
00:32:49,309 --> 00:32:51,320
is going to be a much different factor

818
00:32:51,320 --> 00:32:53,780
than a risk equation for a corporation

819
00:32:53,780 --> 00:32:57,650
or a risk equation for a government we

820
00:32:57,650 --> 00:32:59,360
taught one of the things I will say in

821
00:32:59,360 --> 00:33:00,950
the US so one of the incentives came up

822
00:33:00,950 --> 00:33:03,890
so well we should have maybe a security

823
00:33:03,890 --> 00:33:06,860
tax if I'm a government or I'm a

824
00:33:06,860 --> 00:33:08,930
government politician today in the u.s.

825
00:33:08,930 --> 00:33:11,480
I am probably not standing up and saying

826
00:33:11,480 --> 00:33:13,700
you know we're gonna have a consumer

827
00:33:13,700 --> 00:33:16,130
security tax because then I won't be in

828
00:33:16,130 --> 00:33:17,630
the government anymore and I won't have

829
00:33:17,630 --> 00:33:19,100
to worry about that problem anymore

830
00:33:19,100 --> 00:33:21,290
that might however work in Europe

831
00:33:21,290 --> 00:33:22,820
because it's very different different

832
00:33:22,820 --> 00:33:24,500
cultural benefit we're going to have to

833
00:33:24,500 --> 00:33:26,720
figure these out and it's hard because

834
00:33:26,720 --> 00:33:29,300
it's it's case-by-case what are the

835
00:33:29,300 --> 00:33:31,250
right balances and what's the right

836
00:33:31,250 --> 00:33:33,350
ecosystem that you're playing it

837
00:33:33,350 --> 00:33:35,870
first thing is define your ecosystem and

838
00:33:35,870 --> 00:33:37,460
that's not easy you've got to define the

839
00:33:37,460 --> 00:33:38,540
one you're playing in and that's not

840
00:33:38,540 --> 00:33:41,750
easy at all to do right sujay and take

841
00:33:41,750 --> 00:33:43,280
up maybe some of these issues because

842
00:33:43,280 --> 00:33:45,590
what Terry basically said is all these

843
00:33:45,590 --> 00:33:47,060
technologies that we're talking about

844
00:33:47,060 --> 00:33:48,980
are coming online simultaneously and

845
00:33:48,980 --> 00:33:50,810
they all kind of have to fit together in

846
00:33:50,810 --> 00:33:52,910
some way like a puzzle you can't pull

847
00:33:52,910 --> 00:33:54,470
them apart from each other so we've

848
00:33:54,470 --> 00:33:56,450
talked a lot about AI maybe you can talk

849
00:33:56,450 --> 00:33:57,800
a little bit about cloud computing

850
00:33:57,800 --> 00:33:59,210
because there has been a lot of

851
00:33:59,210 --> 00:34:00,560
developments if we were having this

852
00:34:00,560 --> 00:34:02,810
conversation probably thirty years ago

853
00:34:02,810 --> 00:34:04,460
this would have been primarily about

854
00:34:04,460 --> 00:34:06,800
cloud computing what has been the

855
00:34:06,800 --> 00:34:08,300
evolution in thinking about cloud

856
00:34:08,300 --> 00:34:10,879
security data storage security in the

857
00:34:10,879 --> 00:34:12,020
past couple of years and where do you

858
00:34:12,020 --> 00:34:17,330
see us going computing we all have this

859
00:34:17,330 --> 00:34:19,310
miracle thinking that we just have to

860
00:34:19,310 --> 00:34:21,290
care to pull over you ask that over the

861
00:34:21,290 --> 00:34:22,969
cloud and some kind of miracle will

862
00:34:22,969 --> 00:34:25,070
happen right three years after our

863
00:34:25,070 --> 00:34:27,139
lessons learned today it tells us that

864
00:34:27,139 --> 00:34:29,929
it's not as simple as it appears to be

865
00:34:29,929 --> 00:34:31,520
otherwise we'll be all enjoying the

866
00:34:31,520 --> 00:34:33,830
cloud today you know being a formal

867
00:34:33,830 --> 00:34:35,270
meteorologist I can tell you I probably

868
00:34:35,270 --> 00:34:38,899
know more than anyone else not the car

869
00:34:38,899 --> 00:34:41,000
that we are talking about today I think

870
00:34:41,000 --> 00:34:43,159
the convergence of technology today is

871
00:34:43,159 --> 00:34:44,629
you know make the cloud environment

872
00:34:44,629 --> 00:34:47,389
become a lot more complexed we're not

873
00:34:47,389 --> 00:34:48,710
just talking on private car we're not

874
00:34:48,710 --> 00:34:50,060
talking about communica at all

875
00:34:50,060 --> 00:34:52,070
multi-cloud we talk about the cloud that

876
00:34:52,070 --> 00:34:53,840
actually have certain type of standard

877
00:34:53,840 --> 00:34:56,600
built in in the u.s. we have Ferrum from

878
00:34:56,600 --> 00:34:58,460
the government side allowing us to be

879
00:34:58,460 --> 00:35:01,160
able to qualify giving some credential

880
00:35:01,160 --> 00:35:03,230
to host certain type of data right

881
00:35:03,230 --> 00:35:05,750
having said that the security firm

882
00:35:05,750 --> 00:35:07,280
before today

883
00:35:07,280 --> 00:35:10,340
I think steel is missing in terms of you

884
00:35:10,340 --> 00:35:12,980
know really had that exclusive focus on

885
00:35:12,980 --> 00:35:15,320
security so I can ask the folks in a

886
00:35:15,320 --> 00:35:17,390
room here when you are you know

887
00:35:17,390 --> 00:35:19,970
initiating your call migration project

888
00:35:19,970 --> 00:35:21,650
do you actually have a set-aside

889
00:35:21,650 --> 00:35:24,350
security budget just for that particular

890
00:35:24,350 --> 00:35:27,740
project I don't see anybody raise your

891
00:35:27,740 --> 00:35:31,070
hand here right very field seaman good

892
00:35:31,070 --> 00:35:33,290
for you so what I'm saying here is that

893
00:35:33,290 --> 00:35:37,250
you know there is a majority of the of

894
00:35:37,250 --> 00:35:39,470
the play here really creative on our

895
00:35:39,470 --> 00:35:41,810
ability and think about this the bad

896
00:35:41,810 --> 00:35:45,140
actor is just like us is opportunist we

897
00:35:45,140 --> 00:35:47,060
are looking opportunity to attack and

898
00:35:47,060 --> 00:35:49,190
just like the cryptocurrency today if

899
00:35:49,190 --> 00:35:51,260
you look at the critical mining machines

900
00:35:51,260 --> 00:35:54,590
is actually they are bad actor embedded

901
00:35:54,590 --> 00:35:56,960
inside behind the mining and really

902
00:35:56,960 --> 00:36:00,350
hijacking billions of dollars of money

903
00:36:00,350 --> 00:36:03,800
out of cryptocurrency environment that's

904
00:36:03,800 --> 00:36:06,020
opportunity and so we needs are smart

905
00:36:06,020 --> 00:36:08,510
people on the so-called good side if you

906
00:36:08,510 --> 00:36:10,640
would really need to think about when

907
00:36:10,640 --> 00:36:13,130
the emerging technology about to be

908
00:36:13,130 --> 00:36:15,860
deploy in a massive way where is

909
00:36:15,860 --> 00:36:18,260
security how do we secure the

910
00:36:18,260 --> 00:36:20,600
environment not just a new technology

911
00:36:20,600 --> 00:36:23,270
itself but also how you interoperable

912
00:36:23,270 --> 00:36:25,100
and integrate it with your existing

913
00:36:25,100 --> 00:36:27,170
system as an example you'll call

914
00:36:27,170 --> 00:36:29,600
migration have you have you thought

915
00:36:29,600 --> 00:36:33,020
about to evaluate before you migrate to

916
00:36:33,020 --> 00:36:35,240
the cloud your security posture and then

917
00:36:35,240 --> 00:36:38,630
evaluate the post migration and how do

918
00:36:38,630 --> 00:36:42,190
you maintain the defensive mechanism

919
00:36:42,190 --> 00:36:45,410
continuum of your ability to continue

920
00:36:45,410 --> 00:36:48,200
monitor and control your environment and

921
00:36:48,200 --> 00:36:50,180
still have the single pane of the glass

922
00:36:50,180 --> 00:36:52,520
you know where your threat environments

923
00:36:52,520 --> 00:36:54,380
are and this is where we talk about

924
00:36:54,380 --> 00:36:57,470
technology however does a Didache

925
00:36:57,470 --> 00:36:59,900
blockchain le the quantum computing

926
00:36:59,900 --> 00:37:02,930
added IOT and many other new and

927
00:37:02,930 --> 00:37:05,480
advanced AI algorithm if you would I

928
00:37:05,480 --> 00:37:08,330
would say this those technology can be

929
00:37:08,330 --> 00:37:10,520
used by the good people like us can also

930
00:37:10,520 --> 00:37:13,430
be used about by the bad people and also

931
00:37:13,430 --> 00:37:15,290
I don't categorize people as good and

932
00:37:15,290 --> 00:37:17,780
bad I think there's three groups good in

933
00:37:17,780 --> 00:37:21,050
our digital user bad digital users

934
00:37:21,050 --> 00:37:22,610
and then this me know people in the

935
00:37:22,610 --> 00:37:24,140
media have not decided which way they

936
00:37:24,140 --> 00:37:27,680
want to be and they change and those are

937
00:37:27,680 --> 00:37:31,280
the vulnerability is yet to be predicted

938
00:37:31,280 --> 00:37:33,320
and that's the hardest one they're the

939
00:37:33,320 --> 00:37:35,360
negligent users and that's probably the

940
00:37:35,360 --> 00:37:37,820
the biggest population Siemens is a

941
00:37:37,820 --> 00:37:40,310
company that is you know at they kind of

942
00:37:40,310 --> 00:37:42,890
at the vanguard of using some of this

943
00:37:42,890 --> 00:37:44,180
cloud stuff in some of the most

944
00:37:44,180 --> 00:37:46,670
sensitive technology sensitive

945
00:37:46,670 --> 00:37:49,220
industries can you talk a little bit

946
00:37:49,220 --> 00:37:52,460
about how you are a surgeon brought up

947
00:37:52,460 --> 00:37:54,800
this idea of integration but obviously

948
00:37:54,800 --> 00:37:55,880
there's a lot of vulnerabilities that

949
00:37:55,880 --> 00:37:57,470
come with integration how are you guys

950
00:37:57,470 --> 00:38:00,110
reconciling you know stopgap with with

951
00:38:00,110 --> 00:38:02,300
Cloud usage for things like updating

952
00:38:02,300 --> 00:38:05,000
software that's a loaded loaded question

953
00:38:05,000 --> 00:38:08,240
I mean the the scenario for us is really

954
00:38:08,240 --> 00:38:10,190
coming from the product side you know we

955
00:38:10,190 --> 00:38:14,120
deploy products across 170 or so

956
00:38:14,120 --> 00:38:16,910
countries so products we deploy they

957
00:38:16,910 --> 00:38:20,300
have a lifespan of 20 to 30 years so we

958
00:38:20,300 --> 00:38:22,490
have to not not only make all those

959
00:38:22,490 --> 00:38:24,500
products become connected and

960
00:38:24,500 --> 00:38:27,170
intelligent we also have to maintain a

961
00:38:27,170 --> 00:38:29,450
product lifecycle of not only hardware

962
00:38:29,450 --> 00:38:31,520
like in the past but also software for

963
00:38:31,520 --> 00:38:33,520
20 to 30 years you know unlike the

964
00:38:33,520 --> 00:38:35,720
smartphones we all carry none of them

965
00:38:35,720 --> 00:38:38,440
will be really supported by whoever did

966
00:38:38,440 --> 00:38:41,270
develop them and sold them in five years

967
00:38:41,270 --> 00:38:43,370
with security updates we have to do this

968
00:38:43,370 --> 00:38:45,170
in 15 and 20 years or the software and

969
00:38:45,170 --> 00:38:47,090
hardware stacks we develop and deploy

970
00:38:47,090 --> 00:38:49,940
deploy we have to do this in a way so

971
00:38:49,940 --> 00:38:51,530
that we are pretty certain that we can

972
00:38:51,530 --> 00:38:53,390
support a lifecycle with security

973
00:38:53,390 --> 00:38:54,440
updates in 20 years

974
00:38:54,440 --> 00:38:57,230
so this is this is where where things

975
00:38:57,230 --> 00:38:58,940
start already you know then then we have

976
00:38:58,940 --> 00:39:00,740
the connectivity we were talking about

977
00:39:00,740 --> 00:39:04,640
5g earlier a little bit yeah 5g is one

978
00:39:04,640 --> 00:39:07,370
of many network technologies we use to

979
00:39:07,370 --> 00:39:09,980
connect those devices with other devices

980
00:39:09,980 --> 00:39:12,920
for example to communicate organize them

981
00:39:12,920 --> 00:39:15,650
and among them before anything gets into

982
00:39:15,650 --> 00:39:18,170
the cloud to pre-process data to store

983
00:39:18,170 --> 00:39:20,390
data in the edge now and then the

984
00:39:20,390 --> 00:39:22,880
connectivity gets up into a cloud so

985
00:39:22,880 --> 00:39:24,710
this is also what we have to understand

986
00:39:24,710 --> 00:39:28,100
and with all the data in the cloud we

987
00:39:28,100 --> 00:39:31,160
have a ton of applications in software

988
00:39:31,160 --> 00:39:32,750
that covers the whole lifecycle from

989
00:39:32,750 --> 00:39:33,860
design to

990
00:39:33,860 --> 00:39:36,170
production - operation - service and

991
00:39:36,170 --> 00:39:38,600
maintenance and everything has to be has

992
00:39:38,600 --> 00:39:42,110
to be consistent over 20 years so this

993
00:39:42,110 --> 00:39:43,940
is the scenario which we have to cover

994
00:39:43,940 --> 00:39:47,390
and make secure now and when I said we

995
00:39:47,390 --> 00:39:49,520
spend a lot of money in Seon on security

996
00:39:49,520 --> 00:39:51,350
it's not because we want to be poster

997
00:39:51,350 --> 00:39:53,810
child or anything we don't yeah it's we

998
00:39:53,810 --> 00:39:56,870
have to because if this is not going to

999
00:39:56,870 --> 00:39:58,700
be secure and we are far from perfect

1000
00:39:58,700 --> 00:40:01,280
digitalization will not happen or will

1001
00:40:01,280 --> 00:40:04,850
fail we need to we also need to get

1002
00:40:04,850 --> 00:40:07,520
better in on that now and artificial

1003
00:40:07,520 --> 00:40:09,830
intelligence is not only on the

1004
00:40:09,830 --> 00:40:13,070
applications I really believe it will be

1005
00:40:13,070 --> 00:40:15,020
able if we do it right

1006
00:40:15,020 --> 00:40:17,930
to help us make things more secure if we

1007
00:40:17,930 --> 00:40:21,050
use artificial intelligence in cyber

1008
00:40:21,050 --> 00:40:23,390
security because what artificial

1009
00:40:23,390 --> 00:40:25,370
intelligence ultimately usually is doing

1010
00:40:25,370 --> 00:40:27,230
it's kind of automating the decision

1011
00:40:27,230 --> 00:40:29,840
process and detection process if we

1012
00:40:29,840 --> 00:40:32,570
bring this into cybersecurity then this

1013
00:40:32,570 --> 00:40:35,300
first mover advantage at least get mutt

1014
00:40:35,300 --> 00:40:38,150
gets much smaller yes they will also use

1015
00:40:38,150 --> 00:40:40,880
it but the first mover advantage will go

1016
00:40:40,880 --> 00:40:43,610
not completely away but maybe almost

1017
00:40:43,610 --> 00:40:46,790
away so it's it really is a is a broad

1018
00:40:46,790 --> 00:40:48,740
scope of technology and it's not enough

1019
00:40:48,740 --> 00:40:51,470
for us to only look at IO T or only look

1020
00:40:51,470 --> 00:40:53,660
at the digit win or only look at 5g or

1021
00:40:53,660 --> 00:40:57,020
only look at this and so on everything

1022
00:40:57,020 --> 00:40:58,490
is connected with everything you know

1023
00:40:58,490 --> 00:41:00,410
and I cannot even say that artificial

1024
00:41:00,410 --> 00:41:02,390
intelligence is more important in IOT or

1025
00:41:02,390 --> 00:41:04,490
less important than cyber security or

1026
00:41:04,490 --> 00:41:05,240
what-have-you

1027
00:41:05,240 --> 00:41:08,150
if one of these links is weak in my

1028
00:41:08,150 --> 00:41:11,000
world we are weak that is the perfect

1029
00:41:11,000 --> 00:41:12,920
point to wrap up on and to give us a

1030
00:41:12,920 --> 00:41:15,320
point for the next the next panel that

1031
00:41:15,320 --> 00:41:17,210
we have here where we'll discuss quantum

1032
00:41:17,210 --> 00:41:19,220
computing but I took away two big points

1033
00:41:19,220 --> 00:41:21,140
which one is all these things are really

1034
00:41:21,140 --> 00:41:22,760
connected we're talking a lot about a

1035
00:41:22,760 --> 00:41:24,170
lot of disparate technologies but

1036
00:41:24,170 --> 00:41:25,910
they're all interlocking you can't

1037
00:41:25,910 --> 00:41:27,620
really pull them disentangle them one

1038
00:41:27,620 --> 00:41:29,600
firm in another and to the intrinsic

1039
00:41:29,600 --> 00:41:31,220
value that we're really dealing with and

1040
00:41:31,220 --> 00:41:33,440
every person on this panel who's working

1041
00:41:33,440 --> 00:41:36,380
for major organizations has a burden to

1042
00:41:36,380 --> 00:41:38,660
bear is the burden of trust how do you

1043
00:41:38,660 --> 00:41:40,460
win trust how do you maintain trust and

1044
00:41:40,460 --> 00:41:41,990
that is becoming more difficult in this

1045
00:41:41,990 --> 00:41:44,000
in this deep digital age thank you so

1046
00:41:44,000 --> 00:41:46,070
much panelists and we will turn it over

1047
00:41:46,070 --> 00:41:48,610
to Brent

1048
00:41:48,910 --> 00:41:57,578
[Music]

