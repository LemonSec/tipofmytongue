1
00:00:04,960 --> 00:00:06,399
um well we're going to go ahead and turn

2
00:00:06,399 --> 00:00:08,400
it over to you both uh noah and john

3
00:00:08,400 --> 00:00:10,480
thank you both for uh for for joining us

4
00:00:10,480 --> 00:00:11,280
here

5
00:00:11,280 --> 00:00:13,040
both coming to us from straws freedberg

6
00:00:13,040 --> 00:00:14,960
as you see and uh with a very strong

7
00:00:14,960 --> 00:00:16,800
software development background and

8
00:00:16,800 --> 00:00:18,480
especially when we look to

9
00:00:18,480 --> 00:00:20,720
uh you know these really hard problems

10
00:00:20,720 --> 00:00:22,480
of addressing an exponentially

11
00:00:22,480 --> 00:00:24,240
increasing volume of data that comes

12
00:00:24,240 --> 00:00:25,920
across in our logs as well as many

13
00:00:25,920 --> 00:00:28,160
different formats uh sometimes the old

14
00:00:28,160 --> 00:00:30,720
standby is when it comes to the not

15
00:00:30,720 --> 00:00:32,640
necessarily just the tool grip but the

16
00:00:32,640 --> 00:00:34,399
approach that has become known as

17
00:00:34,399 --> 00:00:36,880
grepping um i'm eager to see what uh

18
00:00:36,880 --> 00:00:38,800
what all you're bringing to bring to the

19
00:00:38,800 --> 00:00:41,120
fight in the modern age of searching log

20
00:00:41,120 --> 00:00:42,160
so

21
00:00:42,160 --> 00:00:44,160
glad to have you on board with us and i

22
00:00:44,160 --> 00:00:46,480
will turn it over to you both

23
00:00:46,480 --> 00:00:48,559
all righty

24
00:00:48,559 --> 00:00:49,920
howdy everybody

25
00:00:49,920 --> 00:00:54,000
and uh uh welcome to uh grep and logs

26
00:00:54,000 --> 00:00:55,840
we've got a lot of get through so we're

27
00:00:55,840 --> 00:00:58,640
just gonna get started

28
00:00:58,640 --> 00:00:59,760
so

29
00:00:59,760 --> 00:01:01,280
just to go through our agenda real quick

30
00:01:01,280 --> 00:01:03,600
before we do introductions uh we're

31
00:01:03,600 --> 00:01:04,559
going to talk about some guiding

32
00:01:04,559 --> 00:01:05,760
principles

33
00:01:05,760 --> 00:01:07,920
we're going to talk about

34
00:01:07,920 --> 00:01:09,360
sort of these new fields of data

35
00:01:09,360 --> 00:01:11,520
engineering and data science we're going

36
00:01:11,520 --> 00:01:14,320
to go back in time to the 1970s and talk

37
00:01:14,320 --> 00:01:16,799
about core unix command line tools we're

38
00:01:16,799 --> 00:01:19,040
going to talk about how to approach

39
00:01:19,040 --> 00:01:22,400
structured data like csv and json

40
00:01:22,400 --> 00:01:24,560
they're going to talk about my favorite

41
00:01:24,560 --> 00:01:26,960
doc topic which is performance

42
00:01:26,960 --> 00:01:29,119
uh we'll talk about a tool that we have

43
00:01:29,119 --> 00:01:31,439
um that's open source called light grip

44
00:01:31,439 --> 00:01:34,000
and then we'll talk about how the cloud

45
00:01:34,000 --> 00:01:35,920
affects all of this

46
00:01:35,920 --> 00:01:37,680
so uh

47
00:01:37,680 --> 00:01:40,320
introductions my name is jon stewart uh

48
00:01:40,320 --> 00:01:42,560
as corey alteid once said not the one

49
00:01:42,560 --> 00:01:44,960
you miss the other one

50
00:01:44,960 --> 00:01:45,759
and

51
00:01:45,759 --> 00:01:47,280
i

52
00:01:47,280 --> 00:01:48,320
do

53
00:01:48,320 --> 00:01:50,079
forensic software development i've been

54
00:01:50,079 --> 00:01:52,640
doing it for 18 years now

55
00:01:52,640 --> 00:01:55,439
before at uh guidance software on encase

56
00:01:55,439 --> 00:01:57,119
and then i had my own company lightbox

57
00:01:57,119 --> 00:01:59,119
technologies strike straws fredberg

58
00:01:59,119 --> 00:02:01,840
acquired that in 2015 and i've been with

59
00:02:01,840 --> 00:02:03,840
straws for the past uh

60
00:02:03,840 --> 00:02:05,200
uh six years

61
00:02:05,200 --> 00:02:06,560
um noah

62
00:02:06,560 --> 00:02:08,479
my name is noah rubin nice to meet you

63
00:02:08,479 --> 00:02:11,599
all i'm newer on the scene than john i'm

64
00:02:11,599 --> 00:02:12,879
a director

65
00:02:12,879 --> 00:02:15,200
in our chicago lab i head up our chicago

66
00:02:15,200 --> 00:02:16,560
lab which is where john and i are

67
00:02:16,560 --> 00:02:18,319
standing right now

68
00:02:18,319 --> 00:02:20,560
and before coming to straws i was a

69
00:02:20,560 --> 00:02:22,400
software engineer and a data scientist

70
00:02:22,400 --> 00:02:24,640
at some startups and doing some natural

71
00:02:24,640 --> 00:02:27,200
language processing at a tech company in

72
00:02:27,200 --> 00:02:29,280
china i no longer have the computer i

73
00:02:29,280 --> 00:02:31,680
was using when i was in china

74
00:02:31,680 --> 00:02:34,959
and i'm i am very passionate about

75
00:02:34,959 --> 00:02:37,360
automating stuff because i'm lazy

76
00:02:37,360 --> 00:02:40,000
and scaling up how we do forensics

77
00:02:40,000 --> 00:02:42,879
and when noah's uh not doing client work

78
00:02:42,879 --> 00:02:43,840
which is

79
00:02:43,840 --> 00:02:44,720
you know

80
00:02:44,720 --> 00:02:46,800
for about three hours out of every week

81
00:02:46,800 --> 00:02:48,480
uh he likes to show me up by writing new

82
00:02:48,480 --> 00:02:50,560
tools himself so it really keeps the

83
00:02:50,560 --> 00:02:51,840
pressure on me

84
00:02:51,840 --> 00:02:52,720
um

85
00:02:52,720 --> 00:02:55,200
so our guiding principles uh really

86
00:02:55,200 --> 00:02:56,959
sorry like the core here

87
00:02:56,959 --> 00:02:59,840
is that you want to go back we want to

88
00:02:59,840 --> 00:03:02,480
use those old time

89
00:03:02,480 --> 00:03:04,879
unix command line tools for processing

90
00:03:04,879 --> 00:03:08,000
logs because that approach

91
00:03:08,000 --> 00:03:09,599
allows you

92
00:03:09,599 --> 00:03:11,599
uh to get a good combination of

93
00:03:11,599 --> 00:03:14,480
performance while still coping with all

94
00:03:14,480 --> 00:03:17,519
the variants that each set law a set of

95
00:03:17,519 --> 00:03:20,080
log data comes to you so you want tools

96
00:03:20,080 --> 00:03:21,360
that

97
00:03:21,360 --> 00:03:23,680
our command line can be scripted out

98
00:03:23,680 --> 00:03:25,920
so you can create complex pipelines you

99
00:03:25,920 --> 00:03:28,319
want simple tools that do a single task

100
00:03:28,319 --> 00:03:29,120
well

101
00:03:29,120 --> 00:03:30,879
and then you want to compose those tools

102
00:03:30,879 --> 00:03:32,239
by putting them together like they're

103
00:03:32,239 --> 00:03:34,480
legos

104
00:03:34,480 --> 00:03:36,239
you want to make sure that you're paying

105
00:03:36,239 --> 00:03:37,760
attention to performance so that you

106
00:03:37,760 --> 00:03:39,680
don't miss your deadlines so that you

107
00:03:39,680 --> 00:03:42,640
don't when you get a large log data set

108
00:03:42,640 --> 00:03:44,959
that you don't have your tools break on

109
00:03:44,959 --> 00:03:47,599
you and and miss your deadlines and have

110
00:03:47,599 --> 00:03:50,799
unhappy clients um and then finally uh

111
00:03:50,799 --> 00:03:52,000
and this is sort of like the new thing

112
00:03:52,000 --> 00:03:54,319
to talk about is that forensics is

113
00:03:54,319 --> 00:03:56,480
really sort of like uh data engineering

114
00:03:56,480 --> 00:03:58,319
and data science and we should approach

115
00:03:58,319 --> 00:04:00,560
it as such

116
00:04:00,560 --> 00:04:02,239
so i'm going to try and convince you

117
00:04:02,239 --> 00:04:04,159
that that is true um because i don't

118
00:04:04,159 --> 00:04:06,239
often hear people talk about data

119
00:04:06,239 --> 00:04:09,360
science and forensics together but i i

120
00:04:09,360 --> 00:04:11,200
promise you that they are the same thing

121
00:04:11,200 --> 00:04:13,280
forensics is in engineering and data

122
00:04:13,280 --> 00:04:16,238
science so to define some terms quickly

123
00:04:16,238 --> 00:04:18,880
what is data engineering it is shaping

124
00:04:18,880 --> 00:04:21,600
data into a usable state so that you can

125
00:04:21,600 --> 00:04:22,960
use it to do

126
00:04:22,960 --> 00:04:25,520
data science which is interpreting data

127
00:04:25,520 --> 00:04:28,240
to extract actionable insights and for

128
00:04:28,240 --> 00:04:30,960
anyone that works dfir if those don't

129
00:04:30,960 --> 00:04:32,720
sound like what you do every day then

130
00:04:32,720 --> 00:04:34,960
i'm not really sure what you're doing

131
00:04:34,960 --> 00:04:37,520
so that the high level process for data

132
00:04:37,520 --> 00:04:40,560
engineering and data science as follows

133
00:04:40,560 --> 00:04:42,720
you get some raw data set it could be a

134
00:04:42,720 --> 00:04:44,479
set of logs from a client could be from

135
00:04:44,479 --> 00:04:46,880
insert some internal tooling whatever it

136
00:04:46,880 --> 00:04:47,919
is

137
00:04:47,919 --> 00:04:50,639
uh ideally you define a set of questions

138
00:04:50,639 --> 00:04:52,000
you want to answer

139
00:04:52,000 --> 00:04:53,840
about those data

140
00:04:53,840 --> 00:04:55,919
and from the questions that you want to

141
00:04:55,919 --> 00:04:58,400
answer you can determine what are the

142
00:04:58,400 --> 00:05:00,720
necessary steps i need to do

143
00:05:00,720 --> 00:05:02,320
to answer those questions both from an

144
00:05:02,320 --> 00:05:04,400
engineering side and from an analysis

145
00:05:04,400 --> 00:05:05,199
side

146
00:05:05,199 --> 00:05:07,039
then you actually do the data

147
00:05:07,039 --> 00:05:09,120
engineering shape the data into the

148
00:05:09,120 --> 00:05:11,280
state that you need it

149
00:05:11,280 --> 00:05:13,280
and then you do your analysis and then

150
00:05:13,280 --> 00:05:15,039
rinse and repeat

151
00:05:15,039 --> 00:05:16,240
so

152
00:05:16,240 --> 00:05:18,880
here's a nice high level diagram of what

153
00:05:18,880 --> 00:05:21,680
this should look like in our opinion

154
00:05:21,680 --> 00:05:23,280
you get your data you define your

155
00:05:23,280 --> 00:05:24,560
question set

156
00:05:24,560 --> 00:05:25,919
engineer it

157
00:05:25,919 --> 00:05:28,160
really important step is generating

158
00:05:28,160 --> 00:05:30,160
intermediate data sets

159
00:05:30,160 --> 00:05:31,840
which i'll get into why that's important

160
00:05:31,840 --> 00:05:33,280
on the next slide

161
00:05:33,280 --> 00:05:36,320
um i what i like to do is do some triage

162
00:05:36,320 --> 00:05:38,080
analysis like i'm going to pull out my

163
00:05:38,080 --> 00:05:39,759
event logs process them quick and just

164
00:05:39,759 --> 00:05:41,919
see if i can get some quick wins you do

165
00:05:41,919 --> 00:05:43,759
your in-depth stuff communicate the

166
00:05:43,759 --> 00:05:45,600
stuff to the client or internal

167
00:05:45,600 --> 00:05:48,880
stakeholders rinse and repeat

168
00:05:48,880 --> 00:05:51,600
this is what i see a lot of people doing

169
00:05:51,600 --> 00:05:53,759
not just elsewhere but sometimes it

170
00:05:53,759 --> 00:05:55,120
straws too

171
00:05:55,120 --> 00:05:57,360
this is important for everybody is

172
00:05:57,360 --> 00:05:59,680
you'll get some raw data set some people

173
00:05:59,680 --> 00:06:03,280
will just grab for iocs and that's it

174
00:06:03,280 --> 00:06:05,039
just go straight to communicating that

175
00:06:05,039 --> 00:06:07,520
and that's it sometimes the client will

176
00:06:07,520 --> 00:06:09,440
say hey i have this one very specific

177
00:06:09,440 --> 00:06:12,240
question so you take your raw data you

178
00:06:12,240 --> 00:06:14,240
process it to answer that one very

179
00:06:14,240 --> 00:06:15,680
specific question

180
00:06:15,680 --> 00:06:17,520
and then council come back comes back

181
00:06:17,520 --> 00:06:19,039
it's like oh well actually i have this

182
00:06:19,039 --> 00:06:20,639
other kind of related question but it's

183
00:06:20,639 --> 00:06:22,800
a little bit different and you realize

184
00:06:22,800 --> 00:06:24,880
shoot i missed like a couple fields in

185
00:06:24,880 --> 00:06:26,720
my original processing that i actually

186
00:06:26,720 --> 00:06:28,560
needed for this other question so then

187
00:06:28,560 --> 00:06:30,160
you go back and you reprocess the data

188
00:06:30,160 --> 00:06:31,680
you gotta wait three hours for that to

189
00:06:31,680 --> 00:06:32,720
be done

190
00:06:32,720 --> 00:06:34,960
i've seen people just like

191
00:06:34,960 --> 00:06:37,280
popping open sublime text and manually

192
00:06:37,280 --> 00:06:38,880
reviewing json

193
00:06:38,880 --> 00:06:40,160
some people are like you know what i

194
00:06:40,160 --> 00:06:40,960
don't even want to deal with this

195
00:06:40,960 --> 00:06:42,240
command line stuff i'm just going to

196
00:06:42,240 --> 00:06:43,759
throw it in spelunker elk and do it

197
00:06:43,759 --> 00:06:44,800
there

198
00:06:44,800 --> 00:06:47,600
um it's it's a hodgepodge and so here

199
00:06:47,600 --> 00:06:50,000
are the key takeaways that

200
00:06:50,000 --> 00:06:52,240
i just want to impart on everybody one

201
00:06:52,240 --> 00:06:54,880
the quicker you do data engineering the

202
00:06:54,880 --> 00:06:56,240
quicker you can start doing your

203
00:06:56,240 --> 00:06:58,400
analysis

204
00:06:58,400 --> 00:07:00,560
another thing is define your question

205
00:07:00,560 --> 00:07:02,560
set to the best of your ability before

206
00:07:02,560 --> 00:07:03,599
you

207
00:07:03,599 --> 00:07:06,240
shape your data because then you don't

208
00:07:06,240 --> 00:07:09,199
have to spend hours reprocessing stuff

209
00:07:09,199 --> 00:07:11,199
never mutate the original data set

210
00:07:11,199 --> 00:07:13,039
always generate intermediate data sets

211
00:07:13,039 --> 00:07:15,680
from your transformations

212
00:07:15,680 --> 00:07:19,039
and it will save everybody a lot of time

213
00:07:19,039 --> 00:07:20,560
one final thing i want to talk about

214
00:07:20,560 --> 00:07:23,919
there uh noah i'm sorry to spark in on

215
00:07:23,919 --> 00:07:24,960
you uh

216
00:07:24,960 --> 00:07:28,160
contemporaneously but um the

217
00:07:28,160 --> 00:07:30,560
with data engineering and that approach

218
00:07:30,560 --> 00:07:32,720
while your log data might change from

219
00:07:32,720 --> 00:07:34,400
investigation to investigation to

220
00:07:34,400 --> 00:07:36,560
investigation

221
00:07:36,560 --> 00:07:39,360
the data engineering approach allows you

222
00:07:39,360 --> 00:07:41,680
to do some preparation beforehand when

223
00:07:41,680 --> 00:07:44,960
you're not in the thick of it and so by

224
00:07:44,960 --> 00:07:46,000
you know

225
00:07:46,000 --> 00:07:48,400
when you have a new uh case come in with

226
00:07:48,400 --> 00:07:50,479
new set of logs you can say let me adapt

227
00:07:50,479 --> 00:07:52,639
those logs to my existing process and

228
00:07:52,639 --> 00:07:54,240
then i have all this existing

229
00:07:54,240 --> 00:07:56,000
infrastructure that i've built up that

230
00:07:56,000 --> 00:07:57,919
lets me get through things quickly and

231
00:07:57,919 --> 00:07:59,840
it allows you to actually take a smarter

232
00:07:59,840 --> 00:08:02,639
approach um but not have to do it just

233
00:08:02,639 --> 00:08:05,440
in the moment of the investigation

234
00:08:05,440 --> 00:08:07,520
so let's talk about uh these core unix

235
00:08:07,520 --> 00:08:09,680
command line tools uh i'm guessing that

236
00:08:09,680 --> 00:08:11,680
there are two groups of people uh right

237
00:08:11,680 --> 00:08:13,599
now there are those who know these

238
00:08:13,599 --> 00:08:16,000
commit unix command line tools and

239
00:08:16,000 --> 00:08:17,680
they're they're really afraid that we're

240
00:08:17,680 --> 00:08:20,240
going to go into depth on them uh

241
00:08:20,240 --> 00:08:22,479
to to that group of people and say just

242
00:08:22,479 --> 00:08:24,479
be patient we will we will get through

243
00:08:24,479 --> 00:08:26,879
this very briefly

244
00:08:26,879 --> 00:08:29,039
and i'm imagining the second group

245
00:08:29,039 --> 00:08:30,560
of you

246
00:08:30,560 --> 00:08:32,080
don't have much experience at all with

247
00:08:32,080 --> 00:08:35,200
these command line tools and i don't

248
00:08:35,200 --> 00:08:36,799
want to go through those tools in depth

249
00:08:36,799 --> 00:08:39,519
we don't have the time right now but i

250
00:08:39,519 --> 00:08:40,479
really

251
00:08:40,479 --> 00:08:41,760
urge you

252
00:08:41,760 --> 00:08:44,640
to take a moment and and learn these

253
00:08:44,640 --> 00:08:47,680
tools and here's the set of tools that

254
00:08:47,680 --> 00:08:50,240
we think are uh these are your daily

255
00:08:50,240 --> 00:08:53,600
drivers that will allow you if

256
00:08:53,600 --> 00:08:56,000
if you learn these tools you will be

257
00:08:56,000 --> 00:08:57,920
extraordinarily productive

258
00:08:57,920 --> 00:09:01,120
it's only like less than a dozen right

259
00:09:01,120 --> 00:09:03,519
and and you'll be able to cope with

260
00:09:03,519 --> 00:09:05,519
anything that comes your way in a

261
00:09:05,519 --> 00:09:07,440
performant way

262
00:09:07,440 --> 00:09:09,120
some of them have to deal

263
00:09:09,120 --> 00:09:11,519
do with uh dealing with input and just

264
00:09:11,519 --> 00:09:13,279
transforming the input some of them

265
00:09:13,279 --> 00:09:15,360
actually have to do with with processing

266
00:09:15,360 --> 00:09:18,240
the data like graph and cut and sort and

267
00:09:18,240 --> 00:09:19,200
unique

268
00:09:19,200 --> 00:09:21,839
um sed is an interesting tool it's not

269
00:09:21,839 --> 00:09:23,519
so easy to use but you could there are

270
00:09:23,519 --> 00:09:25,519
lots of examples online that you can

271
00:09:25,519 --> 00:09:27,120
just copy and paste from that that's

272
00:09:27,120 --> 00:09:29,360
what i do and said really allows you to

273
00:09:29,360 --> 00:09:32,240
do um search and replace at scale just

274
00:09:32,240 --> 00:09:34,320
right on from the command line

275
00:09:34,320 --> 00:09:36,480
the final one i have to confess

276
00:09:36,480 --> 00:09:38,880
uh is awk and awk is its own programming

277
00:09:38,880 --> 00:09:41,519
language um and i don't know it very

278
00:09:41,519 --> 00:09:44,240
well i can't use it very well uh so

279
00:09:44,240 --> 00:09:45,760
that's something that i need to bone up

280
00:09:45,760 --> 00:09:49,040
on more myself but i am in awe of those

281
00:09:49,040 --> 00:09:52,240
who can use ark well if you know awk you

282
00:09:52,240 --> 00:09:54,240
can do anything in about five lines of

283
00:09:54,240 --> 00:09:57,040
code um and i think it might be the

284
00:09:57,040 --> 00:09:59,760
secret to help palmer and success maybe

285
00:09:59,760 --> 00:10:01,600
uh or at least one of them

286
00:10:01,600 --> 00:10:02,720
so

287
00:10:02,720 --> 00:10:04,320
uh we're going to

288
00:10:04,320 --> 00:10:06,800
skip on now to talk about

289
00:10:06,800 --> 00:10:08,720
processing structure data but we do have

290
00:10:08,720 --> 00:10:10,320
a lot of links in the slide deck and

291
00:10:10,320 --> 00:10:12,240
some other

292
00:10:12,240 --> 00:10:13,839
slides about

293
00:10:13,839 --> 00:10:17,360
all those core command line tools

294
00:10:17,360 --> 00:10:20,240
uh i will say we have one we have one um

295
00:10:20,240 --> 00:10:22,959
example of an aux script in the appendix

296
00:10:22,959 --> 00:10:26,160
slides where you can convert uh my sql

297
00:10:26,160 --> 00:10:28,720
output to sqlite compatible output and

298
00:10:28,720 --> 00:10:30,480
that has helped me in a number of cases

299
00:10:30,480 --> 00:10:32,160
it's a ridiculous aux script if you want

300
00:10:32,160 --> 00:10:34,160
to see like how far you can take it you

301
00:10:34,160 --> 00:10:35,760
take a look at that script

302
00:10:35,760 --> 00:10:37,040
okay so we're going to talk about two

303
00:10:37,040 --> 00:10:38,480
types of structured data that i'm pretty

304
00:10:38,480 --> 00:10:40,000
sure everybody here deals with from time

305
00:10:40,000 --> 00:10:43,120
to time one is csv data or more

306
00:10:43,120 --> 00:10:45,760
generally tabular data there's one

307
00:10:45,760 --> 00:10:47,519
specific toolkit that i really like to

308
00:10:47,519 --> 00:10:50,240
use it's called csv kit open source

309
00:10:50,240 --> 00:10:52,160
written in python available on github

310
00:10:52,160 --> 00:10:54,079
they have documentation as well on read

311
00:10:54,079 --> 00:10:55,440
the docs

312
00:10:55,440 --> 00:10:57,279
and it's a collection of command line

313
00:10:57,279 --> 00:10:58,399
tools

314
00:10:58,399 --> 00:11:00,959
they're more than just the ones that um

315
00:11:00,959 --> 00:11:02,399
mentioned here but these are like the

316
00:11:02,399 --> 00:11:04,079
core set of tools that i like to use

317
00:11:04,079 --> 00:11:07,760
from csv kids so one is csv clean that's

318
00:11:07,760 --> 00:11:09,440
a really quick way just to see hey does

319
00:11:09,440 --> 00:11:12,800
my tabular data uh is like a uniform set

320
00:11:12,800 --> 00:11:14,959
of columns for every row that's a that's

321
00:11:14,959 --> 00:11:16,880
a pretty basic one because sometimes you

322
00:11:16,880 --> 00:11:19,680
get stuff that's from clients that's

323
00:11:19,680 --> 00:11:21,600
messy super messy so you just want to

324
00:11:21,600 --> 00:11:23,680
see is it clean enough for me to start

325
00:11:23,680 --> 00:11:25,519
doing analysis or do i need to do some

326
00:11:25,519 --> 00:11:27,760
data engineering to clean it up

327
00:11:27,760 --> 00:11:29,760
csv format's a simple one if you get

328
00:11:29,760 --> 00:11:31,519
some that's like

329
00:11:31,519 --> 00:11:33,360
some random encoding

330
00:11:33,360 --> 00:11:36,320
csv format will encode it and use the

331
00:11:36,320 --> 00:11:37,680
proper

332
00:11:37,680 --> 00:11:40,240
field delimiters and quotations which is

333
00:11:40,240 --> 00:11:41,360
really nice

334
00:11:41,360 --> 00:11:44,399
csv look uh have you ever like catted a

335
00:11:44,399 --> 00:11:47,200
csv file to standard out and just tried

336
00:11:47,200 --> 00:11:49,120
to look through the rows it's very

337
00:11:49,120 --> 00:11:51,040
difficult and i find myself squinting at

338
00:11:51,040 --> 00:11:53,680
it if you look if you use csv look it

339
00:11:53,680 --> 00:11:55,600
just formats it really nicely and it's

340
00:11:55,600 --> 00:11:58,160
it's easier to look at csv stat if you

341
00:11:58,160 --> 00:12:00,320
want to get basic descriptive statistics

342
00:12:00,320 --> 00:12:02,880
for a set of columns in a csv super easy

343
00:12:02,880 --> 00:12:03,760
tool

344
00:12:03,760 --> 00:12:07,600
csv cut is a lot like the unix cut tool

345
00:12:07,600 --> 00:12:09,600
but it handles csv data better and there

346
00:12:09,600 --> 00:12:11,519
are some intricacies there that it just

347
00:12:11,519 --> 00:12:13,279
you don't have to deal with them

348
00:12:13,279 --> 00:12:15,760
csv join is a is a really useful tool if

349
00:12:15,760 --> 00:12:18,399
you have two csvs that have some sort of

350
00:12:18,399 --> 00:12:20,000
column that you can join them together

351
00:12:20,000 --> 00:12:20,720
with

352
00:12:20,720 --> 00:12:22,480
you just run csv join and you can get a

353
00:12:22,480 --> 00:12:25,040
joint data so super easy

354
00:12:25,040 --> 00:12:27,279
so we have some examples in here i'm not

355
00:12:27,279 --> 00:12:28,399
going to play this whole thing because

356
00:12:28,399 --> 00:12:30,560
it's too long but obviously you can take

357
00:12:30,560 --> 00:12:33,120
a look at the slides after so this shows

358
00:12:33,120 --> 00:12:35,360
a good workflow for using the tools i

359
00:12:35,360 --> 00:12:38,160
just talked about using csv clean to see

360
00:12:38,160 --> 00:12:39,600
if your data sets

361
00:12:39,600 --> 00:12:40,320
are

362
00:12:40,320 --> 00:12:43,360
clean and if they're not creating a copy

363
00:12:43,360 --> 00:12:45,519
at a cleaned version of them so that you

364
00:12:45,519 --> 00:12:48,720
can do your analysis you can use csv

365
00:12:48,720 --> 00:12:51,440
csv stat to look at the headers just do

366
00:12:51,440 --> 00:12:54,079
some exploratory analysis on it

367
00:12:54,079 --> 00:12:56,320
this is an example of using csv look you

368
00:12:56,320 --> 00:12:57,920
can see it's much easier to look at that

369
00:12:57,920 --> 00:13:00,480
than just raw csv data

370
00:13:00,480 --> 00:13:03,279
and then i use csv join to actually join

371
00:13:03,279 --> 00:13:06,399
two csvs together on an id column

372
00:13:06,399 --> 00:13:07,760
we have a repo which we'll talk about

373
00:13:07,760 --> 00:13:10,480
later but we have a repo associated with

374
00:13:10,480 --> 00:13:12,639
this talk up on the stratosphere github

375
00:13:12,639 --> 00:13:13,440
org

376
00:13:13,440 --> 00:13:16,240
all of the data sets and examples are up

377
00:13:16,240 --> 00:13:17,920
on that repo you can take a look at the

378
00:13:17,920 --> 00:13:19,600
map

379
00:13:19,600 --> 00:13:23,040
the next is json data i love that name

380
00:13:23,040 --> 00:13:24,880
it's absolutely fantastic

381
00:13:24,880 --> 00:13:28,160
json data we see all the time o365 bc

382
00:13:28,160 --> 00:13:30,720
type investigations we see them in aws

383
00:13:30,720 --> 00:13:32,639
with cloudtrail logs

384
00:13:32,639 --> 00:13:35,680
all the time and jq is probably the best

385
00:13:35,680 --> 00:13:37,920
tool for command line json processing

386
00:13:37,920 --> 00:13:40,399
it's written in c it's super fast and it

387
00:13:40,399 --> 00:13:41,760
can do pretty much anything you can

388
00:13:41,760 --> 00:13:44,880
think of with json input

389
00:13:44,880 --> 00:13:47,519
so this is an example of cloudtrail logs

390
00:13:47,519 --> 00:13:49,279
for those of you that don't use aws as

391
00:13:49,279 --> 00:13:52,320
much it's essentially the audit trail

392
00:13:52,320 --> 00:13:54,160
for

393
00:13:54,160 --> 00:13:57,360
admin level actions in aws and then it

394
00:13:57,360 --> 00:13:59,360
can also do some s3 access logging as

395
00:13:59,360 --> 00:14:00,399
well

396
00:14:00,399 --> 00:14:02,880
the format for cloudtrail logs is their

397
00:14:02,880 --> 00:14:06,639
gzipped json data and each log file

398
00:14:06,639 --> 00:14:09,120
contains one or more records so what we

399
00:14:09,120 --> 00:14:11,279
show in this example is you can unzip

400
00:14:11,279 --> 00:14:14,079
the logs do some exploratory analysis on

401
00:14:14,079 --> 00:14:17,040
what the json structure is all using jq

402
00:14:17,040 --> 00:14:19,040
all from the command line and then you

403
00:14:19,040 --> 00:14:21,440
can actually take each record generate

404
00:14:21,440 --> 00:14:24,160
some tabular data in csv because dealing

405
00:14:24,160 --> 00:14:26,639
with json can be a pain

406
00:14:26,639 --> 00:14:28,880
and then run some descriptive statistics

407
00:14:28,880 --> 00:14:31,839
on the csv data

408
00:14:32,320 --> 00:14:34,160
so my favorite topic

409
00:14:34,160 --> 00:14:36,800
performance

410
00:14:37,120 --> 00:14:38,560
all the tools that we've talked about so

411
00:14:38,560 --> 00:14:40,720
far will make you productive and being

412
00:14:40,720 --> 00:14:43,440
productive is the best performance win

413
00:14:43,440 --> 00:14:45,040
but there are some times when the client

414
00:14:45,040 --> 00:14:46,959
calls you up on friday afternoon and

415
00:14:46,959 --> 00:14:49,680
says i've got 20 terabytes of logs

416
00:14:49,680 --> 00:14:51,839
you've never seen before and i need your

417
00:14:51,839 --> 00:14:54,480
preliminary analysis done by

418
00:14:54,480 --> 00:14:56,560
monday morning and no one else will take

419
00:14:56,560 --> 00:14:58,399
the case but i know that you will

420
00:14:58,399 --> 00:14:59,519
right so

421
00:14:59,519 --> 00:15:02,480
those times you need the nos button to

422
00:15:02,480 --> 00:15:04,079
go really fast

423
00:15:04,079 --> 00:15:06,560
um so here are some tools that will help

424
00:15:06,560 --> 00:15:07,680
you do that

425
00:15:07,680 --> 00:15:09,519
the most important of which is gnu

426
00:15:09,519 --> 00:15:11,440
parallel we'll talk about that a little

427
00:15:11,440 --> 00:15:14,720
bit later in another slide um no i

428
00:15:14,720 --> 00:15:16,320
showed you all sorts of fantastic things

429
00:15:16,320 --> 00:15:19,040
can be done with csv kit csv kit is

430
00:15:19,040 --> 00:15:22,320
great you should learn it um but csv kit

431
00:15:22,320 --> 00:15:25,199
is written in python and python is slow

432
00:15:25,199 --> 00:15:28,000
so csv kit is not as fast as it could

433
00:15:28,000 --> 00:15:30,560
possibly be there's a new tool called

434
00:15:30,560 --> 00:15:33,519
xsb it's written in rust has a lot of

435
00:15:33,519 --> 00:15:35,199
functionality itself a little different

436
00:15:35,199 --> 00:15:36,959
than csv kit

437
00:15:36,959 --> 00:15:39,199
but programs that are written in rust

438
00:15:39,199 --> 00:15:40,480
are fast

439
00:15:40,480 --> 00:15:43,120
so that will help you get through

440
00:15:43,120 --> 00:15:45,360
loads and loads and loads of csv if

441
00:15:45,360 --> 00:15:46,720
that's if that's the kind of data that

442
00:15:46,720 --> 00:15:48,000
you're working with

443
00:15:48,000 --> 00:15:49,120
um

444
00:15:49,120 --> 00:15:51,040
for those of you who

445
00:15:51,040 --> 00:15:54,240
use python for scripting working with

446
00:15:54,240 --> 00:15:57,120
json data and python is amazing it's

447
00:15:57,120 --> 00:15:59,120
awesome it's so easy right you just like

448
00:15:59,120 --> 00:16:01,199
parse the string boom it's addict there

449
00:16:01,199 --> 00:16:04,160
you go you're off to the races in python

450
00:16:04,160 --> 00:16:06,560
problem is python is slow and so

451
00:16:06,560 --> 00:16:09,680
processing json in python is slow um

452
00:16:09,680 --> 00:16:12,959
there is a rust library for parsing json

453
00:16:12,959 --> 00:16:14,959
called or json

454
00:16:14,959 --> 00:16:17,199
and although it's written in rust

455
00:16:17,199 --> 00:16:20,320
it is designed for easy usage in python

456
00:16:20,320 --> 00:16:22,959
you can just pip install it and import

457
00:16:22,959 --> 00:16:25,279
it it is a drop-in replacement for the

458
00:16:25,279 --> 00:16:28,240
standard library module i've there is

459
00:16:28,240 --> 00:16:31,360
the uh absolute best thing to use

460
00:16:31,360 --> 00:16:33,440
because there's there are a few things

461
00:16:33,440 --> 00:16:34,880
in life that can make your python

462
00:16:34,880 --> 00:16:37,759
scripts five times faster but or jason

463
00:16:37,759 --> 00:16:40,399
can and it has for us and it's just been

464
00:16:40,399 --> 00:16:42,639
fantastic uh you should just always use

465
00:16:42,639 --> 00:16:44,639
it by default for everything

466
00:16:44,639 --> 00:16:46,800
um for those of you who've got a little

467
00:16:46,800 --> 00:16:49,519
more tech uh skill with with being able

468
00:16:49,519 --> 00:16:51,759
to use c or c plus plus and some

469
00:16:51,759 --> 00:16:54,560
compilers even if just a little bit

470
00:16:54,560 --> 00:16:56,720
and if you need to go really fast if you

471
00:16:56,720 --> 00:17:00,160
need that 900 horsepower charger um

472
00:17:00,160 --> 00:17:01,839
there's a tool there's a library called

473
00:17:01,839 --> 00:17:05,839
simd json cindy json uses the sse and

474
00:17:05,839 --> 00:17:09,839
avx vector instructions uh on intel cpus

475
00:17:09,839 --> 00:17:14,240
to highly accelerate uh json parsing and

476
00:17:14,240 --> 00:17:16,400
it is the fastest json parser out there

477
00:17:16,400 --> 00:17:18,160
it's not even close it's twice as fast

478
00:17:18,160 --> 00:17:20,000
as the next fastest

479
00:17:20,000 --> 00:17:22,880
and it can process gigabytes of json a

480
00:17:22,880 --> 00:17:24,000
minute

481
00:17:24,000 --> 00:17:25,599
so

482
00:17:25,599 --> 00:17:26,480
um

483
00:17:26,480 --> 00:17:28,240
some performance principles to keep in

484
00:17:28,240 --> 00:17:30,160
mind the most important one is

485
00:17:30,160 --> 00:17:32,960
procedural and that is just to start to

486
00:17:32,960 --> 00:17:34,640
benchmark

487
00:17:34,640 --> 00:17:37,039
however roughly whatever your throughput

488
00:17:37,039 --> 00:17:38,799
is on your pipeline as your processing

489
00:17:38,799 --> 00:17:40,960
data i find a lot of people don't do

490
00:17:40,960 --> 00:17:42,720
this and then they get close to the

491
00:17:42,720 --> 00:17:44,840
deadline of when they've got to make it

492
00:17:44,840 --> 00:17:47,280
deliverable and they don't know when

493
00:17:47,280 --> 00:17:48,480
it's going to be done and they start

494
00:17:48,480 --> 00:17:50,640
calling me and by that point there's

495
00:17:50,640 --> 00:17:52,000
nothing i can do

496
00:17:52,000 --> 00:17:53,120
if you

497
00:17:53,120 --> 00:17:56,960
uh are able to estimate what your rough

498
00:17:56,960 --> 00:17:59,120
throughput bandwidth is for your whole

499
00:17:59,120 --> 00:18:00,880
pri pipeline

500
00:18:00,880 --> 00:18:03,120
you'll be able to extrapolate from that

501
00:18:03,120 --> 00:18:04,400
to get a sense of when you'll be

502
00:18:04,400 --> 00:18:07,360
finished processing and if it's tight or

503
00:18:07,360 --> 00:18:09,440
if you're going to overshoot it

504
00:18:09,440 --> 00:18:11,120
you can do that you know if you've done

505
00:18:11,120 --> 00:18:12,960
that early on you can adjust your

506
00:18:12,960 --> 00:18:14,480
approach or

507
00:18:14,480 --> 00:18:16,320
set client expectations appropriately

508
00:18:16,320 --> 00:18:18,320
right you can batch things up into sets

509
00:18:18,320 --> 00:18:20,480
and get at least a few batches done as

510
00:18:20,480 --> 00:18:22,480
opposed to having being zero percent

511
00:18:22,480 --> 00:18:25,039
done when your your deadline comes

512
00:18:25,039 --> 00:18:27,440
um the second most important thing with

513
00:18:27,440 --> 00:18:29,520
performance these days is of course to

514
00:18:29,520 --> 00:18:31,919
make use of all the cores that you have

515
00:18:31,919 --> 00:18:33,840
on your system

516
00:18:33,840 --> 00:18:36,240
the new parallel is the key to doing

517
00:18:36,240 --> 00:18:39,919
that because it will launch

518
00:18:39,919 --> 00:18:42,960
a command for each set of uh file paths

519
00:18:42,960 --> 00:18:45,600
that it has as input and can you and it

520
00:18:45,600 --> 00:18:47,360
could saturate your cores we'll show you

521
00:18:47,360 --> 00:18:49,360
a demo of that in just a moment um

522
00:18:49,360 --> 00:18:51,520
finally when it comes to nvme or finally

523
00:18:51,520 --> 00:18:53,039
when it comes to io

524
00:18:53,039 --> 00:18:56,000
you got to use nvme there is no excuse

525
00:18:56,000 --> 00:19:00,400
in 2021 for not using nvme to its full

526
00:19:00,400 --> 00:19:03,440
it is absolutely the best money you can

527
00:19:03,440 --> 00:19:06,720
buy the cheapest money you can buy to

528
00:19:06,720 --> 00:19:09,840
accelerate your forensic processing nvme

529
00:19:09,840 --> 00:19:12,080
drives are just amazing don't bother

530
00:19:12,080 --> 00:19:14,640
around with spinning rust ever

531
00:19:14,640 --> 00:19:17,039
don't even bother with sata ssd drives

532
00:19:17,039 --> 00:19:19,360
because sata is actually the bottleneck

533
00:19:19,360 --> 00:19:21,280
the drive itself can go way fast it's

534
00:19:21,280 --> 00:19:24,320
the same hardware as in as this in nvme

535
00:19:24,320 --> 00:19:26,480
but the protocol is so slow it can't set

536
00:19:26,480 --> 00:19:28,799
it can't fully utilize the underlying

537
00:19:28,799 --> 00:19:29,760
hardware

538
00:19:29,760 --> 00:19:31,600
so use nvme

539
00:19:31,600 --> 00:19:33,919
finally if your processing is going to

540
00:19:33,919 --> 00:19:36,720
produce a lot of output

541
00:19:36,720 --> 00:19:38,799
you should consider using a second i o

542
00:19:38,799 --> 00:19:41,039
device that's dedicated for grabbing

543
00:19:41,039 --> 00:19:43,919
that output and that way you avoid

544
00:19:43,919 --> 00:19:46,559
mixed i o so you can read from one

545
00:19:46,559 --> 00:19:47,600
device

546
00:19:47,600 --> 00:19:49,679
right to the other they're both going to

547
00:19:49,679 --> 00:19:51,520
perform as fast as they possibly can

548
00:19:51,520 --> 00:19:52,559
then

549
00:19:52,559 --> 00:19:55,840
even if you put even nvme suffers under

550
00:19:55,840 --> 00:19:58,320
mixed i o

551
00:19:58,320 --> 00:20:00,960
okay so this is an example of the kind

552
00:20:00,960 --> 00:20:03,039
of throughput that you can get using gnu

553
00:20:03,039 --> 00:20:04,400
parallel

554
00:20:04,400 --> 00:20:06,000
so you can see on the bottom we have

555
00:20:06,000 --> 00:20:08,559
h-top running showing you how many cores

556
00:20:08,559 --> 00:20:10,080
we have eight logical cores on this

557
00:20:10,080 --> 00:20:11,039
machine

558
00:20:11,039 --> 00:20:12,640
it's not a great machine i'll just say

559
00:20:12,640 --> 00:20:14,400
that up front so that you can do much

560
00:20:14,400 --> 00:20:16,799
better in this example we have five and

561
00:20:16,799 --> 00:20:19,440
a half megs of compressed aws cloudtrail

562
00:20:19,440 --> 00:20:20,400
log data

563
00:20:20,400 --> 00:20:22,159
and what we've done is written a script

564
00:20:22,159 --> 00:20:24,720
that operates on a single file to unzip

565
00:20:24,720 --> 00:20:27,919
the file and then run jq on the json

566
00:20:27,919 --> 00:20:31,520
data to extract individual csv records

567
00:20:31,520 --> 00:20:34,400
so you can see me typing here is

568
00:20:34,400 --> 00:20:36,799
i'm going to use the find command

569
00:20:36,799 --> 00:20:39,520
to list all of the gz which are the

570
00:20:39,520 --> 00:20:42,080
gzipped cloudtrail logs

571
00:20:42,080 --> 00:20:44,400
and then i'm going to pipe them to gnu

572
00:20:44,400 --> 00:20:47,840
parallel i'm using the jobs

573
00:20:47,840 --> 00:20:49,360
command line switch

574
00:20:49,360 --> 00:20:51,840
to schedule essentially five files to be

575
00:20:51,840 --> 00:20:54,159
processed on each core and i'm gonna run

576
00:20:54,159 --> 00:20:56,080
the script on each one so you'll see in

577
00:20:56,080 --> 00:20:57,760
a second on the bottom

578
00:20:57,760 --> 00:21:01,200
our cpu utilization is going to spike

579
00:21:01,200 --> 00:21:04,000
once i hit enter

580
00:21:04,000 --> 00:21:06,000
now it spikes there we go right and

581
00:21:06,000 --> 00:21:08,159
that's the difference between using a

582
00:21:08,159 --> 00:21:10,240
single core to sequentially process all

583
00:21:10,240 --> 00:21:12,159
your files and actually using the

584
00:21:12,159 --> 00:21:14,480
computing power that you have

585
00:21:14,480 --> 00:21:16,480
at your fingertips yeah and eight cores

586
00:21:16,480 --> 00:21:18,720
is one thing right but uh you know there

587
00:21:18,720 --> 00:21:20,080
are a lot of tools out there that are

588
00:21:20,080 --> 00:21:21,679
eight cores but you probably have more

589
00:21:21,679 --> 00:21:23,039
than eight cores these days you might

590
00:21:23,039 --> 00:21:24,880
have 20 you might have 30 on the servers

591
00:21:24,880 --> 00:21:28,240
you might have 64 cores and so

592
00:21:28,240 --> 00:21:30,559
um there aren't any other tools out

593
00:21:30,559 --> 00:21:32,559
there other than this kind of approach

594
00:21:32,559 --> 00:21:34,480
that can really saturate

595
00:21:34,480 --> 00:21:36,960
64 cores at once and it's awesome when

596
00:21:36,960 --> 00:21:40,000
you can be able to do that

597
00:21:40,000 --> 00:21:42,240
so uh light grip

598
00:21:42,240 --> 00:21:44,559
light grip is a search tool

599
00:21:44,559 --> 00:21:47,440
i wrote i started writing it in

600
00:21:47,440 --> 00:21:49,919
uh it's over like 11 years ago now the

601
00:21:49,919 --> 00:21:52,880
spring of 2010. um

602
00:21:52,880 --> 00:21:54,320
had i known how long it was going to

603
00:21:54,320 --> 00:21:55,600
take me to finish i never would have

604
00:21:55,600 --> 00:21:58,640
started i got it uh kind of running it

605
00:21:58,640 --> 00:22:01,280
was really fast it was like three times

606
00:22:01,280 --> 00:22:03,600
faster in nk so i made some changes ten

607
00:22:03,600 --> 00:22:06,400
times faster in case uh and then i

608
00:22:06,400 --> 00:22:08,640
finally came up with like a really good

609
00:22:08,640 --> 00:22:09,919
set of

610
00:22:09,919 --> 00:22:12,960
uh qa tests and it took me two years to

611
00:22:12,960 --> 00:22:15,520
pass those q a tests because it's such a

612
00:22:15,520 --> 00:22:16,720
hard problem

613
00:22:16,720 --> 00:22:19,120
and that's why no one has really ever

614
00:22:19,120 --> 00:22:20,960
before solved the multi-pattern grip

615
00:22:20,960 --> 00:22:22,640
problem um

616
00:22:22,640 --> 00:22:24,799
so we released it as op we released it

617
00:22:24,799 --> 00:22:27,360
as a add-on product for encase back in

618
00:22:27,360 --> 00:22:30,960
2012. in 2013 uh

619
00:22:30,960 --> 00:22:32,799
partnership with simpson garfinkel then

620
00:22:32,799 --> 00:22:34,880
he was at the naval postgrad school at

621
00:22:34,880 --> 00:22:36,880
the time uh we open sourced it and

622
00:22:36,880 --> 00:22:39,840
integrated it into bulk extractor

623
00:22:39,840 --> 00:22:41,840
lightcraft features standard grupp

624
00:22:41,840 --> 00:22:45,200
syntax not its own frank and grep um and

625
00:22:45,200 --> 00:22:46,159
so

626
00:22:46,159 --> 00:22:48,320
and what really distinguishes light grub

627
00:22:48,320 --> 00:22:50,159
is its support for different encodings

628
00:22:50,159 --> 00:22:52,840
so utf-8 utf-16

629
00:22:52,840 --> 00:22:54,799
ascii abdict

630
00:22:54,799 --> 00:22:57,200
everything every encoding under the sun

631
00:22:57,200 --> 00:22:59,520
light grip can say can look at a pattern

632
00:22:59,520 --> 00:23:02,640
and identify it in whichever encodings

633
00:23:02,640 --> 00:23:04,640
you want to search for

634
00:23:04,640 --> 00:23:07,120
in the raw binary data so it doesn't try

635
00:23:07,120 --> 00:23:09,840
to decode the binary data it look it

636
00:23:09,840 --> 00:23:12,159
changes your keyword to recognize it in

637
00:23:12,159 --> 00:23:13,760
binary data

638
00:23:13,760 --> 00:23:16,320
um and light grip powers a lot of the

639
00:23:16,320 --> 00:23:19,280
forensic processing at straws free work

640
00:23:19,280 --> 00:23:20,159
so

641
00:23:20,159 --> 00:23:22,559
we are pushing out a new release of

642
00:23:22,559 --> 00:23:25,360
light grip uh it's probably gonna slip

643
00:23:25,360 --> 00:23:27,840
until tomorrow uh i gotta finish up some

644
00:23:27,840 --> 00:23:31,200
documentation changes on there um but uh

645
00:23:31,200 --> 00:23:32,080
uh

646
00:23:32,080 --> 00:23:33,840
it will come with a new command line

647
00:23:33,840 --> 00:23:36,240
tool now we've had it for a long time it

648
00:23:36,240 --> 00:23:39,200
still probably needs um you know

649
00:23:39,200 --> 00:23:41,840
needs your it really needs your feedback

650
00:23:41,840 --> 00:23:43,919
uh for me to take it to the next level

651
00:23:43,919 --> 00:23:46,640
but it'll allow you to search binary or

652
00:23:46,640 --> 00:23:49,360
uh or log data text data

653
00:23:49,360 --> 00:23:50,960
and we'll have a

654
00:23:50,960 --> 00:23:53,200
windows build available so that instead

655
00:23:53,200 --> 00:23:54,960
of just like having a library for c

656
00:23:54,960 --> 00:23:57,279
developers um you'll be able to use

657
00:23:57,279 --> 00:23:59,919
lightroom from the command line um we're

658
00:23:59,919 --> 00:24:02,159
also changing the open source license on

659
00:24:02,159 --> 00:24:02,880
it

660
00:24:02,880 --> 00:24:05,279
today uh it's been under the gnu public

661
00:24:05,279 --> 00:24:08,799
license uh and with this new release

662
00:24:08,799 --> 00:24:10,960
we're switching the license over to the

663
00:24:10,960 --> 00:24:13,360
apache 2 software license which is far

664
00:24:13,360 --> 00:24:15,840
more friendly to other businesses so

665
00:24:15,840 --> 00:24:17,520
hopefully we can get

666
00:24:17,520 --> 00:24:19,679
a greater adoption for light grab and

667
00:24:19,679 --> 00:24:21,679
see it in other tools and

668
00:24:21,679 --> 00:24:23,919
get get more usage of it from from lots

669
00:24:23,919 --> 00:24:25,039
of folks

670
00:24:25,039 --> 00:24:26,720
this release will also

671
00:24:26,720 --> 00:24:29,360
feature reduced memory usage on really

672
00:24:29,360 --> 00:24:32,080
complex pattern sets so you can check it

673
00:24:32,080 --> 00:24:34,320
out at our uh straws freebird github

674
00:24:34,320 --> 00:24:35,360
page

675
00:24:35,360 --> 00:24:36,480
um

676
00:24:36,480 --> 00:24:37,840
the thing that distinguishes a

677
00:24:37,840 --> 00:24:39,679
multi-pattern search

678
00:24:39,679 --> 00:24:41,760
the algorithms get complex but at the

679
00:24:41,760 --> 00:24:43,200
high level it's really easy to

680
00:24:43,200 --> 00:24:44,240
understand

681
00:24:44,240 --> 00:24:46,880
grep is a fantastic tool it's

682
00:24:46,880 --> 00:24:49,679
really fast it's hard to beat um

683
00:24:49,679 --> 00:24:51,840
but if you have 100 keywords and you've

684
00:24:51,840 --> 00:24:54,559
got to search a terabyte of log files

685
00:24:54,559 --> 00:24:56,480
you're going to have to research that

686
00:24:56,480 --> 00:24:59,200
log file 100 times

687
00:24:59,200 --> 00:25:00,400
light grab can take those hundred

688
00:25:00,400 --> 00:25:03,440
keywords bundle them up together in a

689
00:25:03,440 --> 00:25:05,840
principled way and take one pass through

690
00:25:05,840 --> 00:25:08,559
that uh one terabyte file now it's not a

691
00:25:08,559 --> 00:25:10,720
hundred times faster but it's still

692
00:25:10,720 --> 00:25:13,679
usually several times faster than doing

693
00:25:13,679 --> 00:25:15,760
a loop a hundred times over

694
00:25:15,760 --> 00:25:19,039
uh uh with grub and so that's that's

695
00:25:19,039 --> 00:25:21,520
the thing that inspired this talk is uh

696
00:25:21,520 --> 00:25:23,200
all the half game cases that we had to

697
00:25:23,200 --> 00:25:25,360
deal with um

698
00:25:25,360 --> 00:25:28,000
uh in march where we had like tons of

699
00:25:28,000 --> 00:25:31,440
iis and exchange logs and we had a lot

700
00:25:31,440 --> 00:25:34,640
of uh ioc search terms ip addresses and

701
00:25:34,640 --> 00:25:37,120
aspx web shelves and we just needed to

702
00:25:37,120 --> 00:25:40,720
triage exchange logs as fast as we could

703
00:25:40,720 --> 00:25:43,440
to um to figure out you know like was

704
00:25:43,440 --> 00:25:46,159
this exchange server uh uh hit with the

705
00:25:46,159 --> 00:25:48,159
web shell do we see anything that

706
00:25:48,159 --> 00:25:50,159
indicates that it was and so this little

707
00:25:50,159 --> 00:25:53,200
gif here shows us searching uh 339

708
00:25:53,200 --> 00:25:54,720
megabytes

709
00:25:54,720 --> 00:25:59,440
of uh iis exchange log data for 67 uh

710
00:25:59,440 --> 00:26:02,240
half name related uh terms and you can

711
00:26:02,240 --> 00:26:04,159
see it takes like four seconds or

712
00:26:04,159 --> 00:26:06,480
whatever

713
00:26:07,600 --> 00:26:10,240
okay so we've talked about some core

714
00:26:10,240 --> 00:26:12,240
command line tools and how to compose

715
00:26:12,240 --> 00:26:15,039
them together to do stuff fast now we're

716
00:26:15,039 --> 00:26:16,320
going to talk about taking those

717
00:26:16,320 --> 00:26:18,400
principles and applying them uh using

718
00:26:18,400 --> 00:26:20,559
cloud infrastructure and scaling up with

719
00:26:20,559 --> 00:26:22,880
aws

720
00:26:22,880 --> 00:26:25,679
so aws i think at this point has in the

721
00:26:25,679 --> 00:26:27,760
hundreds of services that they offer and

722
00:26:27,760 --> 00:26:29,120
they add new stuff all the time but

723
00:26:29,120 --> 00:26:30,640
we're going to talk about three very

724
00:26:30,640 --> 00:26:32,000
critical ones

725
00:26:32,000 --> 00:26:34,799
uh the s3 simple storage service

726
00:26:34,799 --> 00:26:36,960
sure most people have heard of it if not

727
00:26:36,960 --> 00:26:38,799
it's a it's an object store you can

728
00:26:38,799 --> 00:26:39,760
store

729
00:26:39,760 --> 00:26:42,240
stuff whatever you want as discrete

730
00:26:42,240 --> 00:26:44,480
objects kind of like a file system

731
00:26:44,480 --> 00:26:46,880
but it operates over http

732
00:26:46,880 --> 00:26:49,200
and it's cheap if you want to store a

733
00:26:49,200 --> 00:26:51,919
whole bunch of log data or images or

734
00:26:51,919 --> 00:26:55,200
whatever in s3 it's super cheap and

735
00:26:55,200 --> 00:26:57,120
you're you can be relatively sure that

736
00:26:57,120 --> 00:26:59,120
they're not going to lose your data

737
00:26:59,120 --> 00:27:02,080
the next one is aws lambda it's their

738
00:27:02,080 --> 00:27:04,320
serverless compute platform

739
00:27:04,320 --> 00:27:06,960
or in other words you run functions so

740
00:27:06,960 --> 00:27:09,120
each you can think about each lambda

741
00:27:09,120 --> 00:27:11,600
like a javascript function or a python

742
00:27:11,600 --> 00:27:14,080
function or rust or c plus plus you can

743
00:27:14,080 --> 00:27:16,640
run a whole bunch of different stuff

744
00:27:16,640 --> 00:27:18,399
um but it's really just at the end of

745
00:27:18,399 --> 00:27:20,320
the day a function and the nice thing

746
00:27:20,320 --> 00:27:22,559
about lambda is you can pair it with

747
00:27:22,559 --> 00:27:25,200
other services like s3 so you can

748
00:27:25,200 --> 00:27:27,039
trigger

749
00:27:27,039 --> 00:27:30,080
lambdas to run when other stuff happens

750
00:27:30,080 --> 00:27:34,000
in aws like uploading a file to s3 the

751
00:27:34,000 --> 00:27:36,000
last thing is the cloud development kit

752
00:27:36,000 --> 00:27:37,360
or cdk

753
00:27:37,360 --> 00:27:39,360
if anyone's familiar with hashicorp

754
00:27:39,360 --> 00:27:42,080
terraform it's kind of the same idea it

755
00:27:42,080 --> 00:27:44,559
allows you to write code to describe

756
00:27:44,559 --> 00:27:47,360
what you want to build in aws

757
00:27:47,360 --> 00:27:49,120
and then it will deploy your

758
00:27:49,120 --> 00:27:51,520
infrastructure for you

759
00:27:51,520 --> 00:27:53,279
and that integrates with aws cloud

760
00:27:53,279 --> 00:27:55,360
formation

761
00:27:55,360 --> 00:27:57,200
so if you remember the previous sketch

762
00:27:57,200 --> 00:27:59,360
that i made of showing like find to

763
00:27:59,360 --> 00:28:02,320
parallel to like lots of uh uh greps

764
00:28:02,320 --> 00:28:05,279
that are happening simultaneously um

765
00:28:05,279 --> 00:28:07,360
that sketch kind of illustrated

766
00:28:07,360 --> 00:28:09,679
that a traditional computer has

767
00:28:09,679 --> 00:28:11,279
bottlenecks we're bottlenecked on our

768
00:28:11,279 --> 00:28:12,399
input

769
00:28:12,399 --> 00:28:15,440
aws is essentially a new type of

770
00:28:15,440 --> 00:28:16,640
computer

771
00:28:16,640 --> 00:28:17,679
where

772
00:28:17,679 --> 00:28:21,360
we can put up lots of data into it right

773
00:28:21,360 --> 00:28:23,760
near infinite amount of data

774
00:28:23,760 --> 00:28:27,200
into s3 stored very cheaply extremely

775
00:28:27,200 --> 00:28:28,480
reliably

776
00:28:28,480 --> 00:28:29,279
and

777
00:28:29,279 --> 00:28:31,679
that data amazon takes care

778
00:28:31,679 --> 00:28:33,679
of striping that data across their

779
00:28:33,679 --> 00:28:35,840
entire data etc right so you're

780
00:28:35,840 --> 00:28:38,720
utilizing lots and lots and lots of io

781
00:28:38,720 --> 00:28:40,000
devices

782
00:28:40,000 --> 00:28:42,240
and lots and lots of storage servers

783
00:28:42,240 --> 00:28:44,960
at the same time then you can pair

784
00:28:44,960 --> 00:28:46,399
your lambdas

785
00:28:46,399 --> 00:28:48,880
with individual s3 objects to do your

786
00:28:48,880 --> 00:28:50,320
searches in parallel

787
00:28:50,320 --> 00:28:51,120
so

788
00:28:51,120 --> 00:28:53,039
while gnu parallel is great if you just

789
00:28:53,039 --> 00:28:55,200
have a single system and a hard drive uh

790
00:28:55,200 --> 00:28:56,880
hooked up to it with you know a couple

791
00:28:56,880 --> 00:28:58,399
of terabytes of logs

792
00:28:58,399 --> 00:29:00,960
if you have dozens of terabytes of logs

793
00:29:00,960 --> 00:29:02,640
thousands of files

794
00:29:02,640 --> 00:29:05,120
aws can scale to that without any

795
00:29:05,120 --> 00:29:06,799
bottlenecks and that's really kind of

796
00:29:06,799 --> 00:29:09,039
like the revolutionary part of what

797
00:29:09,039 --> 00:29:10,720
makes aws

798
00:29:10,720 --> 00:29:13,520
a new type of computer

799
00:29:13,520 --> 00:29:16,720
okay so i'm going to show an example of

800
00:29:16,720 --> 00:29:20,240
what using the aws cdk looks like

801
00:29:20,240 --> 00:29:21,679
there are some tools that you have to

802
00:29:21,679 --> 00:29:23,440
have installed which i'll talk about in

803
00:29:23,440 --> 00:29:25,200
a second

804
00:29:25,200 --> 00:29:28,399
we have a repo associated with this talk

805
00:29:28,399 --> 00:29:30,080
like i said before

806
00:29:30,080 --> 00:29:32,399
one of the things inside of that repo is

807
00:29:32,399 --> 00:29:35,520
a template cdk application so you don't

808
00:29:35,520 --> 00:29:37,440
have to do this from scratch we've given

809
00:29:37,440 --> 00:29:38,559
a template

810
00:29:38,559 --> 00:29:41,520
for you and we have an example lambda

811
00:29:41,520 --> 00:29:43,279
function written it was just shown in

812
00:29:43,279 --> 00:29:44,720
this example

813
00:29:44,720 --> 00:29:47,840
and all it really does is it prints out

814
00:29:47,840 --> 00:29:50,080
hey there's a new object in s3 so what

815
00:29:50,080 --> 00:29:52,559
we've done in the cdk ad is we have an

816
00:29:52,559 --> 00:29:56,159
s3 bucket and then this lambda

817
00:29:56,159 --> 00:29:58,000
and we've hooked them up so that every

818
00:29:58,000 --> 00:30:00,240
time a new object gets uploaded to the

819
00:30:00,240 --> 00:30:01,279
bucket

820
00:30:01,279 --> 00:30:03,760
you run the lambda and

821
00:30:03,760 --> 00:30:05,360
thinking about how you could use this

822
00:30:05,360 --> 00:30:07,279
that's great it you could use it however

823
00:30:07,279 --> 00:30:09,279
you want right every time uh someone

824
00:30:09,279 --> 00:30:11,200
uploads a file you could grab for

825
00:30:11,200 --> 00:30:13,679
keywords you could process the data do

826
00:30:13,679 --> 00:30:15,360
some data engineering and then throw it

827
00:30:15,360 --> 00:30:16,880
in elasticsearch

828
00:30:16,880 --> 00:30:18,880
um although i would i would encourage

829
00:30:18,880 --> 00:30:21,039
you to do some enrichment or more

830
00:30:21,039 --> 00:30:23,039
analysis before just throwing stuff in a

831
00:30:23,039 --> 00:30:24,320
search tool

832
00:30:24,320 --> 00:30:27,279
um you can do whatever you want and it's

833
00:30:27,279 --> 00:30:29,520
up there in the repo for you uh as you

834
00:30:29,520 --> 00:30:31,440
can see it's really just as easy as

835
00:30:31,440 --> 00:30:36,799
writing cdk deploy it deploys it

836
00:30:36,799 --> 00:30:40,159
so last like i said the repo link is is

837
00:30:40,159 --> 00:30:42,799
right here we have this template awk aws

838
00:30:42,799 --> 00:30:45,679
cdk app and a bunch of documentation on

839
00:30:45,679 --> 00:30:47,679
how to install the tools to use it and

840
00:30:47,679 --> 00:30:49,919
deploy it and then also how to change it

841
00:30:49,919 --> 00:30:51,600
so it's actually useful for you to

842
00:30:51,600 --> 00:30:53,039
process data

843
00:30:53,039 --> 00:30:54,960
we have a shell scripts with all of the

844
00:30:54,960 --> 00:30:57,440
commands from the examples and we have

845
00:30:57,440 --> 00:30:59,519
the data sets that

846
00:30:59,519 --> 00:31:01,279
we used in the examples

847
00:31:01,279 --> 00:31:04,720
we also have a docker file with the with

848
00:31:04,720 --> 00:31:06,880
a static build of light grep in it so

849
00:31:06,880 --> 00:31:08,799
you can play around with like rep we

850
00:31:08,799 --> 00:31:10,880
have all of the tools i'm pretty sure i

851
00:31:10,880 --> 00:31:13,519
got all of them uh in the docker file if

852
00:31:13,519 --> 00:31:15,679
i'm missing one i'm happy to add it

853
00:31:15,679 --> 00:31:17,200
so you can play around with all of these

854
00:31:17,200 --> 00:31:18,640
tools without having to install them

855
00:31:18,640 --> 00:31:19,760
yourself

856
00:31:19,760 --> 00:31:22,799
um and i also added some core data

857
00:31:22,799 --> 00:31:25,840
science libraries from python and from r

858
00:31:25,840 --> 00:31:29,840
if anyone are fans of those

859
00:31:29,840 --> 00:31:32,720
all right i think we have a uh a few

860
00:31:32,720 --> 00:31:34,240
questions that have come in

861
00:31:34,240 --> 00:31:35,200
um

862
00:31:35,200 --> 00:31:37,600
the first i will pose to uh noah here

863
00:31:37,600 --> 00:31:39,519
around the fly he hasn't seen it

864
00:31:39,519 --> 00:31:41,919
but that is can one pipe unix command

865
00:31:41,919 --> 00:31:44,240
output to csv kit

866
00:31:44,240 --> 00:31:46,240
so because oftentimes the spacing of

867
00:31:46,240 --> 00:31:48,399
stuff like that uh will will have

868
00:31:48,399 --> 00:31:49,919
problems

869
00:31:49,919 --> 00:31:53,440
yes csv kit all of the tools in cst kit

870
00:31:53,440 --> 00:31:56,640
are designed to read from standard in so

871
00:31:56,640 --> 00:32:00,240
if you use one unix tool to output some

872
00:32:00,240 --> 00:32:02,799
data you can pipe it directly to the csv

873
00:32:02,799 --> 00:32:04,399
kid tools and they'll read them by

874
00:32:04,399 --> 00:32:05,919
default you can also

875
00:32:05,919 --> 00:32:08,159
just read from a file with the csvk

876
00:32:08,159 --> 00:32:09,840
tools but they're really designed with

877
00:32:09,840 --> 00:32:10,559
the

878
00:32:10,559 --> 00:32:11,840
unix

879
00:32:11,840 --> 00:32:14,799
unix usage style in mind so yes

880
00:32:14,799 --> 00:32:17,120
okay and then two questions about light

881
00:32:17,120 --> 00:32:18,240
grip

882
00:32:18,240 --> 00:32:20,159
i'm going to answer the second one first

883
00:32:20,159 --> 00:32:21,919
um did they mention a flight grip will

884
00:32:21,919 --> 00:32:23,679
parse the information in compressed log

885
00:32:23,679 --> 00:32:26,480
files like secret the answer to that is

886
00:32:26,480 --> 00:32:27,440
no

887
00:32:27,440 --> 00:32:29,120
light grip just deals with the data as

888
00:32:29,120 --> 00:32:31,519
it is searches the data as it is um

889
00:32:31,519 --> 00:32:34,000
that's why the bulk extractor uh

890
00:32:34,000 --> 00:32:35,600
integration is a great thing i've been

891
00:32:35,600 --> 00:32:37,679
talking with simpson garfinkel about a

892
00:32:37,679 --> 00:32:40,399
bulk extractor 2.0 that's that's kind of

893
00:32:40,399 --> 00:32:42,640
coming later this year i think and uh

894
00:32:42,640 --> 00:32:44,720
it's able to deal with the actual

895
00:32:44,720 --> 00:32:47,519
extraction and present that data to

896
00:32:47,519 --> 00:32:49,519
bulk strength i will say there are

897
00:32:49,519 --> 00:32:50,960
command line tools to solve those

898
00:32:50,960 --> 00:32:52,799
problems so you don't necessarily need

899
00:32:52,799 --> 00:32:55,279
to write a new one you can use g unzip

900
00:32:55,279 --> 00:32:58,000
or gun zip if you use gun zip dash c it

901
00:32:58,000 --> 00:33:00,000
just pipes the standard out you can pipe

902
00:33:00,000 --> 00:33:02,480
it to light crep you can use zcat is

903
00:33:02,480 --> 00:33:04,720
another easy one so

904
00:33:04,720 --> 00:33:06,799
if you compare other existing tools to

905
00:33:06,799 --> 00:33:08,240
handle those problems

906
00:33:08,240 --> 00:33:09,919
so then the other one is how does leica

907
00:33:09,919 --> 00:33:12,320
compare to silver searcher uh that's a

908
00:33:12,320 --> 00:33:14,480
good question that i am

909
00:33:14,480 --> 00:33:16,960
not super able to answer right on the

910
00:33:16,960 --> 00:33:19,519
off the cuff but i'll do my best um my

911
00:33:19,519 --> 00:33:21,440
understanding is silver searcher it's a

912
00:33:21,440 --> 00:33:23,360
good search tool uh it's designed for

913
00:33:23,360 --> 00:33:26,480
searching code repositories and

914
00:33:26,480 --> 00:33:29,919
i believe it indexes the data first so

915
00:33:29,919 --> 00:33:33,279
it's creating a search index of of data

916
00:33:33,279 --> 00:33:35,200
usually based around text

917
00:33:35,200 --> 00:33:36,399
and

918
00:33:36,399 --> 00:33:38,559
then can answer you know queries very

919
00:33:38,559 --> 00:33:41,440
quickly based upon that search index uh

920
00:33:41,440 --> 00:33:42,960
litecrep is not

921
00:33:42,960 --> 00:33:45,120
a index search tool it is just a

922
00:33:45,120 --> 00:33:47,600
straightforward one bite at a time grep

923
00:33:47,600 --> 00:33:49,919
tool it just can do multi-pattern crap

924
00:33:49,919 --> 00:33:53,120
and has support for lots of encodings um

925
00:33:53,120 --> 00:33:55,519
so so lycra is really geared more

926
00:33:55,519 --> 00:33:58,080
towards doing um

927
00:33:58,080 --> 00:33:59,679
binary searching searching of binary

928
00:33:59,679 --> 00:34:01,440
data that's really like how it got its

929
00:34:01,440 --> 00:34:03,120
life was i need to go through

930
00:34:03,120 --> 00:34:04,640
unallocated space and find all the

931
00:34:04,640 --> 00:34:06,480
things all at once

932
00:34:06,480 --> 00:34:08,879
um but we have found it useful for for

933
00:34:08,879 --> 00:34:10,480
searching logs as well

934
00:34:10,480 --> 00:34:12,560
silver searcher great tool though that's

935
00:34:12,560 --> 00:34:15,040
uh that's a nice one to put in your your

936
00:34:15,040 --> 00:34:17,679
tool belt as

937
00:34:18,839 --> 00:34:21,760
well any other questions

938
00:34:21,760 --> 00:34:24,399
how are we doing

939
00:34:24,879 --> 00:34:25,839
that's it

940
00:34:25,839 --> 00:34:28,000
um there's there's one in here about a

941
00:34:28,000 --> 00:34:29,440
comparison to yara i don't know if

942
00:34:29,440 --> 00:34:31,520
that's enough of a difference from a lot

943
00:34:31,520 --> 00:34:33,359
of what you just talked about well oh

944
00:34:33,359 --> 00:34:36,639
yeah we're streaming yeah

945
00:34:36,960 --> 00:34:39,199
i mean so yara has like a ton of

946
00:34:39,199 --> 00:34:41,679
yara has a ton of functionality uh i've

947
00:34:41,679 --> 00:34:43,520
looked real hard at what they're doing

948
00:34:43,520 --> 00:34:44,320
and

949
00:34:44,320 --> 00:34:46,079
you know like okay should i just drop

950
00:34:46,079 --> 00:34:47,599
like up in the yard and stuff and it's

951
00:34:47,599 --> 00:34:50,480
just it's not possible i wish it were

952
00:34:50,480 --> 00:34:51,679
um

953
00:34:51,679 --> 00:34:52,879
but uh

954
00:34:52,879 --> 00:34:54,239
with all the other things that yara is

955
00:34:54,239 --> 00:34:56,639
doing with parsing pe files and

956
00:34:56,639 --> 00:34:58,880
uh the hashing and everything else is

957
00:34:58,880 --> 00:35:01,760
doing it's its own thing um but in terms

958
00:35:01,760 --> 00:35:04,400
of raw search performance like

959
00:35:04,400 --> 00:35:07,119
uh search term search term like grapple

960
00:35:07,119 --> 00:35:09,440
will beat yara

961
00:35:09,440 --> 00:35:10,800
yeah and you know

962
00:35:10,800 --> 00:35:12,240
i kind of figured that's the the

963
00:35:12,240 --> 00:35:13,520
direction that you would lean in that

964
00:35:13,520 --> 00:35:15,440
answer and i think that goes really well

965
00:35:15,440 --> 00:35:17,040
with some of the commentary that we had

966
00:35:17,040 --> 00:35:19,200
in the slack channel which was you know

967
00:35:19,200 --> 00:35:21,599
using a tool to do two things versus the

968
00:35:21,599 --> 00:35:23,359
individual tool and then chaining it

969
00:35:23,359 --> 00:35:25,440
together with pipes and such and there's

970
00:35:25,440 --> 00:35:26,880
a lot of benefit that you get out of

971
00:35:26,880 --> 00:35:28,800
either of those approaches but i agree

972
00:35:28,800 --> 00:35:30,560
like pick the tool that's best for the

973
00:35:30,560 --> 00:35:33,200
specific task you need instead of you

974
00:35:33,200 --> 00:35:33,920
know

975
00:35:33,920 --> 00:35:36,000
you're not going to use zeke to replace

976
00:35:36,000 --> 00:35:37,839
uh your ids like that's just not what

977
00:35:37,839 --> 00:35:39,119
it's designed for and kind of like you

978
00:35:39,119 --> 00:35:40,960
said with yara versus light crap i think

979
00:35:40,960 --> 00:35:44,960
that's another good analogy there

980
00:35:44,960 --> 00:35:47,760
uh yeah so the the link to the docker

981
00:35:47,760 --> 00:35:48,640
file

982
00:35:48,640 --> 00:35:52,160
and rip the repo in general um it's it's

983
00:35:52,160 --> 00:35:54,320
uh it's in the slide deck

984
00:35:54,320 --> 00:35:56,079
we can put it inside out man yeah we'll

985
00:35:56,079 --> 00:35:57,839
put it in the slot we'll put it in slack

986
00:35:57,839 --> 00:36:00,400
too and respond to uh questions in slack

987
00:36:00,400 --> 00:36:02,240
over the next couple days so please hit

988
00:36:02,240 --> 00:36:03,839
us up there for sure

989
00:36:03,839 --> 00:36:04,960
awesome

990
00:36:04,960 --> 00:36:06,640
the last thing that i want to say before

991
00:36:06,640 --> 00:36:09,200
before we go into lunch here is when

992
00:36:09,200 --> 00:36:12,079
these slides do become available in your

993
00:36:12,079 --> 00:36:14,640
portal accounts um

994
00:36:14,640 --> 00:36:16,240
we didn't even get to what i think might

995
00:36:16,240 --> 00:36:20,160
be the hidden gold nugget in uh in your

996
00:36:20,160 --> 00:36:22,320
slide deck which is your appendix holy

997
00:36:22,320 --> 00:36:25,280
cow this is like i i've sneak peeked

998
00:36:25,280 --> 00:36:27,760
through there and you've got a slide on

999
00:36:27,760 --> 00:36:29,040
each of the various tools that you

1000
00:36:29,040 --> 00:36:32,079
talked about um i am absolutely going to

1001
00:36:32,079 --> 00:36:33,760
be pointing to that for a lot of reasons

1002
00:36:33,760 --> 00:36:35,599
because that is a fantastic collection

1003
00:36:35,599 --> 00:36:38,000
of just the what's the tool what's it do

1004
00:36:38,000 --> 00:36:40,800
and and how do i make use of that so uh

1005
00:36:40,800 --> 00:36:42,240
just a little bit of a preview for

1006
00:36:42,240 --> 00:36:43,760
everybody when this does show up in your

1007
00:36:43,760 --> 00:36:45,680
account you are going to be uh very very

1008
00:36:45,680 --> 00:36:47,839
pleased to go back to the slides that we

1009
00:36:47,839 --> 00:36:49,359
didn't even you know have a chance to

1010
00:36:49,359 --> 00:36:50,880
see here so thank you for putting all

1011
00:36:50,880 --> 00:36:52,720
that information together

1012
00:36:52,720 --> 00:36:54,320
that was a necessity we realized it

1013
00:36:54,320 --> 00:36:56,560
would be a 90-minute talk if we covered

1014
00:36:56,560 --> 00:36:58,880
each one of those yeah

1015
00:36:58,880 --> 00:37:00,800
well you know it's going to be good

1016
00:37:00,800 --> 00:37:02,320
reference for those that want it and uh

1017
00:37:02,320 --> 00:37:03,839
something that we'll refer back to for a

1018
00:37:03,839 --> 00:37:07,320
long time i think

1019
00:37:09,839 --> 00:37:11,920
you

