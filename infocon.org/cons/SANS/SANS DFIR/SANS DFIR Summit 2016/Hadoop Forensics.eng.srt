1
00:00:00,600 --> 00:00:02,836
(soft music)

2
00:00:11,144 --> 00:00:13,813
(applause)

3
00:00:13,813 --> 00:00:15,181
- [Man] Thank you very much.

4
00:00:15,181 --> 00:00:18,118
- [Kevvie] Thanks a lot, Matt.

5
00:00:18,118 --> 00:00:20,552
And not all Canadians
are polite, by the way.

6
00:00:20,553 --> 00:00:22,188
I've met some real doozies.

7
00:00:22,188 --> 00:00:24,023
(audience laughing)

8
00:00:24,024 --> 00:00:27,127
So we're gonna talk today
about Hadoop forensics,

9
00:00:27,127 --> 00:00:29,628
and you can't talk
about the forensics

10
00:00:29,629 --> 00:00:32,165
without first talking
about breaches.

11
00:00:32,165 --> 00:00:33,299
And we know what the breach is.

12
00:00:33,299 --> 00:00:34,667
The source of breaches

13
00:00:34,667 --> 00:00:37,537
or the cause is good
old fashioned hacking

14
00:00:37,537 --> 00:00:39,071
more often than not.

15
00:00:39,072 --> 00:00:42,142
And when people break into
systems and they steal data,

16
00:00:42,142 --> 00:00:43,643
more often than not,

17
00:00:43,643 --> 00:00:46,746
that data is actually
stored within a database.

18
00:00:46,746 --> 00:00:48,581
Okay, so I've been
preaching the importance

19
00:00:48,581 --> 00:00:53,019
of database forensics now
for just over nine years,

20
00:00:53,019 --> 00:00:55,687
which makes me feel
really, really old.

21
00:00:55,688 --> 00:00:59,192
But at the same time,
the industry is changing.

22
00:00:59,192 --> 00:01:00,160
It's evolving.

23
00:01:00,160 --> 00:01:02,194
It's not just about databases.

24
00:01:02,195 --> 00:01:04,263
It's about data.

25
00:01:04,263 --> 00:01:09,169
A lot of organizations are
taking data out of databases

26
00:01:09,169 --> 00:01:11,471
and they're storing it
in big data technology.

27
00:01:12,972 --> 00:01:14,673
When you look at the
big data technology

28
00:01:14,674 --> 00:01:16,276
who's been deployed
more than any other

29
00:01:16,276 --> 00:01:19,878
technology in the world,
that's Apache Hadoop.

30
00:01:19,879 --> 00:01:21,748
So when we're called in
to do investigations,

31
00:01:21,748 --> 00:01:23,750
a lot of times the trail
leads to the databases.

32
00:01:23,750 --> 00:01:25,985
We could investigate
based on the stuff

33
00:01:25,985 --> 00:01:27,220
that's been published,

34
00:01:27,220 --> 00:01:29,122
so my own research,
the books, the tools.

35
00:01:29,122 --> 00:01:30,557
We can investigate.

36
00:01:30,557 --> 00:01:33,026
We can determine what the
hacker did within the database.

37
00:01:33,026 --> 00:01:34,527
But when you look
at modern databases,

38
00:01:34,527 --> 00:01:36,696
they have connectors
to Apache Hadoop,

39
00:01:36,696 --> 00:01:39,199
so the trail doesn't
stop at the database.

40
00:01:39,199 --> 00:01:41,434
It keeps going now to
big data technologies,

41
00:01:41,434 --> 00:01:44,704
and forensics hasn't made
that leap yet to Apache Hadoop

42
00:01:44,704 --> 00:01:46,773
and other big data technologies.

43
00:01:46,773 --> 00:01:49,075
So the goal I have
today for this talk

44
00:01:49,075 --> 00:01:50,577
is we're gonna take a look at

45
00:01:50,577 --> 00:01:54,013
and we're gonna get our feet
wet in Apache Hadoop forensics.

46
00:01:54,013 --> 00:01:56,749
And a pet peeve I
have are presentations

47
00:01:56,749 --> 00:01:58,217
that are too high-level.

48
00:01:58,218 --> 00:01:59,886
So you get the presentation
and it seems cool,

49
00:01:59,886 --> 00:02:01,420
but you get to actually do it,

50
00:02:01,421 --> 00:02:04,424
and you're not sure exactly
what to do and when and why.

51
00:02:04,424 --> 00:02:07,127
So in 25 minutes-ish,

52
00:02:07,127 --> 00:02:09,228
we're gonna step through
the actual commands

53
00:02:09,229 --> 00:02:10,497
that you can run to conduct

54
00:02:10,497 --> 00:02:13,199
a Hadoop forensic investigation.

55
00:02:13,199 --> 00:02:14,868
Why you should
run them, as well.

56
00:02:15,768 --> 00:02:17,003
So you're gonna leave today,

57
00:02:17,003 --> 00:02:18,370
you're gonna go
back to your jobs,

58
00:02:18,371 --> 00:02:20,340
your gonna get a call
regarding Hadoop investigation.

59
00:02:20,340 --> 00:02:22,609
You can just grab this
presentation, nothing else,

60
00:02:22,609 --> 00:02:24,711
and actually run through
an investigation.

61
00:02:24,711 --> 00:02:26,880
So we're gonna do all
that in 25 minutes

62
00:02:26,880 --> 00:02:30,517
or your money back.
(audience laughing)

63
00:02:30,517 --> 00:02:31,918
So before we jump into things,

64
00:02:31,918 --> 00:02:33,386
I'll tell you a
bit about myself.

65
00:02:33,386 --> 00:02:35,622
So I'm Kevvie Fowler,
and just a show of hands,

66
00:02:35,622 --> 00:02:37,891
how many of you think
Kevvie is my real name?

67
00:02:40,793 --> 00:02:42,195
It is my real name.

68
00:02:42,195 --> 00:02:43,796
I've got a tougher
question for you, though.

69
00:02:43,796 --> 00:02:46,632
What's the background or
the nationality of Kevvie?

70
00:02:46,633 --> 00:02:47,901
It's not a very common name.

71
00:02:47,901 --> 00:02:49,802
You can put Kevvie in
Google and I pop up.

72
00:02:49,802 --> 00:02:51,404
There's no one else.

73
00:02:51,404 --> 00:02:53,573
So what's the
background of Kevvie?

74
00:02:53,573 --> 00:02:54,774
Any brave souls?

75
00:02:57,410 --> 00:02:59,746
Canadian? (laughing)
(audience laughing)

76
00:02:59,746 --> 00:03:01,247
That's correct,

77
00:03:01,247 --> 00:03:03,383
but the background of the
name is actually Irish.

78
00:03:04,784 --> 00:03:06,486
So as you can probably
guess from looking at me,

79
00:03:06,486 --> 00:03:09,289
I don't have a ton of
Irish in my family tree.

80
00:03:09,289 --> 00:03:12,591
I just have an Irish name,
but please do call me Kevvie.

81
00:03:12,592 --> 00:03:14,360
I've met a bunch of people
today who have changed it

82
00:03:14,360 --> 00:03:17,730
to Kevin automatically,
so just call me Kevvie.

83
00:03:17,730 --> 00:03:20,600
I'm the National Cyber
Response Leader for KPMG,

84
00:03:20,600 --> 00:03:24,537
so that means I wear two hats
and I'm based out in Canada.

85
00:03:24,537 --> 00:03:26,206
The first hat is
helping organizations

86
00:03:26,206 --> 00:03:28,708
identify what information
is important to them,

87
00:03:28,708 --> 00:03:30,510
ensure they have right
safeguards in place

88
00:03:30,510 --> 00:03:33,012
to actually protect
that information.

89
00:03:33,012 --> 00:03:36,382
So we deliver services like
red teaming, blue teaming,

90
00:03:36,382 --> 00:03:39,953
incident response forensics,
governance reviews, et cetera.

91
00:03:41,387 --> 00:03:43,389
And then when a suspected
intrusion actually happens,

92
00:03:43,389 --> 00:03:45,058
the second hat is reactive.

93
00:03:45,058 --> 00:03:46,492
So I'll lead teams
that will go in

94
00:03:46,492 --> 00:03:48,828
and confirm or deny if
something actually happened

95
00:03:48,828 --> 00:03:49,996
within an environment.\\

96
00:03:49,996 --> 00:03:52,332
If something did
happen, we investigate

97
00:03:52,332 --> 00:03:54,033
and precisely scope
what happened.

98
00:03:54,033 --> 00:03:56,803
And a better job
you can do scoping\

99
00:03:56,803 --> 00:03:59,439
the lesser the impact
is to the organization.

100
00:03:59,439 --> 00:04:00,707
So I get both the proactive

101
00:04:00,707 --> 00:04:04,043
as well as the reactive
sides of cybersecurity.

102
00:04:04,043 --> 00:04:06,379
I'm author of "Data Breach
Preparation Response,"

103
00:04:06,379 --> 00:04:07,880
which was just
published last week,

104
00:04:07,880 --> 00:04:10,882
so I'm sure a few of you
have already bought a copy.

105
00:04:10,883 --> 00:04:12,018
Everyone's looking
the other way.

106
00:04:12,018 --> 00:04:13,453
No one's maintaining
eye contact.

107
00:04:13,453 --> 00:04:15,521
I'm good and sure
that hasn't happened.

108
00:04:15,521 --> 00:04:18,390
I'm also author of "SQL
Server Forensic Analysis,"

109
00:04:18,391 --> 00:04:19,926
and I'm contributing author,

110
00:04:19,926 --> 00:04:21,995
meaning I've written chapters
for a bunch of other security

111
00:04:21,995 --> 00:04:25,098
and forensic books at
the bottom of the slide.

112
00:04:25,098 --> 00:04:27,567
I'm also a sans-lethal
forensicator,

113
00:04:27,567 --> 00:04:30,436
and I speak a lot
at conferences.

114
00:04:30,436 --> 00:04:31,638
So enough about me.

115
00:04:31,638 --> 00:04:33,905
Let's just into Apache
Hadoop forensics.

116
00:04:35,074 --> 00:04:37,210
So when you look
at Apache Hadoop,

117
00:04:37,210 --> 00:04:38,710
has anyone here actually
asked the question,

118
00:04:38,711 --> 00:04:41,114
or has anyone here actually
worked on an investigation

119
00:04:41,114 --> 00:04:42,315
involving Apache Hadoop?

120
00:04:43,683 --> 00:04:45,051
No one yet.

121
00:04:45,051 --> 00:04:46,619
And I hope everyone's
gonna be honest.

122
00:04:46,619 --> 00:04:49,322
Does anyone here not
have the faintest idea

123
00:04:49,322 --> 00:04:50,623
what Apache Hadoop is?

124
00:04:52,625 --> 00:04:53,860
Even the AV guys know.

125
00:04:53,860 --> 00:04:56,062
They're like, no, I know
what Apache Hadoop is.

126
00:04:56,062 --> 00:04:57,597
(audience laughing)

127
00:04:57,597 --> 00:05:01,034
So Hadoop is basically just a
framework to store information

128
00:05:01,034 --> 00:05:03,102
and to analyze information.

129
00:05:03,102 --> 00:05:05,804
So you can go out there and
you can download Apache Hadoop,

130
00:05:05,805 --> 00:05:07,040
you're gonna install it,

131
00:05:07,040 --> 00:05:09,776
and what it is basically
is just a file system.

132
00:05:09,776 --> 00:05:13,079
So it's just an Hadoop
distributed file system.

133
00:05:13,079 --> 00:05:14,480
But it's modular.

134
00:05:14,480 --> 00:05:18,351
You can add a lot of components
on top of Apache Hadoop,

135
00:05:18,351 --> 00:05:20,186
so you can add database servers,

136
00:05:20,186 --> 00:05:22,621
and there's a variety of
different database platforms

137
00:05:22,622 --> 00:05:26,859
you can add on there, relational
databases, SQL, no-SQL.

138
00:05:26,859 --> 00:05:29,662
You can add advanced
search functionality,

139
00:05:29,662 --> 00:05:32,498
so you can add a bunch of
components onto Apache Hadoop.

140
00:05:32,498 --> 00:05:33,966
In the 25 bins we have today,

141
00:05:33,966 --> 00:05:36,369
we're not gonna cover how
to do an investigation

142
00:05:36,369 --> 00:05:38,538
across all of these componenets.

143
00:05:38,538 --> 00:05:40,707
We are gonna focus on the
core of Apache Hadoop,

144
00:05:40,707 --> 00:05:42,909
which is just HDFS.

145
00:05:42,909 --> 00:05:45,211
Okay, so that's just
the file system.

146
00:05:45,211 --> 00:05:47,213
And we're gonna look
at Hadoop version five,

147
00:05:47,213 --> 00:05:49,916
and Cladera 5.7
was what was used

148
00:05:49,916 --> 00:05:51,784
for the research in
this presentation,

149
00:05:51,784 --> 00:05:53,252
but other versions of Hadoop,

150
00:05:53,252 --> 00:05:55,855
the same principles, the same
commands, et cetera, work.

151
00:05:55,855 --> 00:05:57,857
Anything over really
Hadoop version one.

152
00:06:01,494 --> 00:06:04,297
So what does Apache
Hadoop actually look like?

153
00:06:04,297 --> 00:06:06,666
This is a scaled-down
representation

154
00:06:06,666 --> 00:06:10,503
of what an actual Apache Hadoop
implementation looks like.

155
00:06:10,503 --> 00:06:12,372
There are some other
components that aren't relevant

156
00:06:12,372 --> 00:06:14,474
based on what we're
gonna talk about today,

157
00:06:14,474 --> 00:06:17,377
so I've actually omitted
them from the slide.

158
00:06:17,377 --> 00:06:19,479
But I'm gonna start
on the right-hand side

159
00:06:19,479 --> 00:06:21,246
for you viewing out
there in the audience,

160
00:06:21,247 --> 00:06:24,650
and work my way to the
left-hand side of the screen.

161
00:06:24,650 --> 00:06:28,087
So every Apache
Hadoop implementation

162
00:06:28,087 --> 00:06:29,389
has a lot of data nodes,

163
00:06:30,523 --> 00:06:32,191
and these data nodes
do nothing more

164
00:06:32,191 --> 00:06:34,926
than just store information
and follow orders.

165
00:06:34,927 --> 00:06:36,863
So that's where the
actual data is stored,

166
00:06:36,863 --> 00:06:38,297
so you do an investigation,

167
00:06:38,297 --> 00:06:40,366
you want to suck information
out of Apache Hadoop,

168
00:06:40,366 --> 00:06:43,169
you're gonna grab
that information
from your data nodes.

169
00:06:44,237 --> 00:06:45,838
Then you have a job tracker,

170
00:06:47,006 --> 00:06:49,441
and what the job tracker is,

171
00:06:49,442 --> 00:06:52,545
it focuses on just mapping
and reducing information.

172
00:06:52,545 --> 00:06:54,614
So if you look at Apache Hadoop,

173
00:06:54,614 --> 00:06:57,316
their claim to fame is instead
of moving data to processing,

174
00:06:57,316 --> 00:07:00,286
it moves processing to data.

175
00:07:00,286 --> 00:07:01,788
So you folks here in the US

176
00:07:01,788 --> 00:07:03,589
are gonna be going through
an election fairly soon,

177
00:07:03,589 --> 00:07:04,891
and I'm gonna reserve comments

178
00:07:04,891 --> 00:07:07,894
about some of your candidates.

179
00:07:07,894 --> 00:07:09,395
But for an example,

180
00:07:09,395 --> 00:07:13,399
if I wanted to store a
file in Apache Hadoop

181
00:07:13,399 --> 00:07:16,169
that had a listing of all
the registered voters,

182
00:07:16,169 --> 00:07:18,270
number one, Apache Hadoop
works on replication,

183
00:07:18,271 --> 00:07:19,772
so I add one file,

184
00:07:19,772 --> 00:07:22,241
it automatically creates
a bunch of other replicas.

185
00:07:22,241 --> 00:07:23,676
It takes all those replicas

186
00:07:23,676 --> 00:07:25,844
and it spreads them
across all the data nodes.

187
00:07:27,280 --> 00:07:30,582
And then if I were to send a
query to Apache Hadoop saying,

188
00:07:30,583 --> 00:07:33,686
give me all registered
voters who have a first name

189
00:07:33,686 --> 00:07:35,621
that starts with A, B, or C,

190
00:07:35,621 --> 00:07:36,989
what the job
tracker is gonna do,

191
00:07:36,989 --> 00:07:40,159
it's gonna talk to
the first data node

192
00:07:40,159 --> 00:07:42,028
and say, you give
all registered voters

193
00:07:42,028 --> 00:07:44,063
who has the first name
that starts with A.

194
00:07:44,063 --> 00:07:45,298
It'll go to the other data node

195
00:07:45,298 --> 00:07:48,234
and say the same
thing, but with B.

196
00:07:48,234 --> 00:07:49,702
And the third data node,

197
00:07:49,702 --> 00:07:52,238
give me all the registerd
voters with a first name of C.

198
00:07:52,238 --> 00:07:55,141
So it maps the work out
to a bunch of data nodes,

199
00:07:55,141 --> 00:07:57,043
the data nodes do the work,

200
00:07:57,043 --> 00:07:59,512
and then they send the results
back to the job tracker

201
00:07:59,512 --> 00:08:01,814
and it reduces all of
those results into one

202
00:08:01,814 --> 00:08:04,584
and sends that back to
the person requesting it.

203
00:08:04,584 --> 00:08:07,353
So job tracker just works on
mapping and reducing work.

204
00:08:08,254 --> 00:08:10,289
Then we have the name node,

205
00:08:10,289 --> 00:08:13,125
and the name node is where
all of the metadata is.

206
00:08:13,125 --> 00:08:15,628
When you look at Apache
Hadoop implemenations,

207
00:08:15,628 --> 00:08:20,366
it's not uncommon to find
multi-pedabyt installations.

208
00:08:20,366 --> 00:08:23,603
When it comes down to forensics,
you can't image everything.

209
00:08:23,603 --> 00:08:27,106
You can't image
several pedabytes.

210
00:08:27,106 --> 00:08:29,075
So you really want to
focus on that metadata,

211
00:08:29,075 --> 00:08:32,278
and that's the server that all
that metadata is stored on,

212
00:08:32,278 --> 00:08:33,980
which is your name node server.

213
00:08:33,980 --> 00:08:36,883
We'll get into more details
on that in a little bit.

214
00:08:36,883 --> 00:08:38,985
Then you have your interfaces.

215
00:08:38,985 --> 00:08:41,621
So anyone who wants to
interact with Apache Hadoop,

216
00:08:41,621 --> 00:08:42,989
it has to come in
through an interface.

217
00:08:42,989 --> 00:08:45,658
You can do so by
map produce jobs.

218
00:08:45,658 --> 00:08:49,061
That's equivalent to query
in a traditional database.

219
00:08:50,429 --> 00:08:52,031
You have a command line

220
00:08:52,031 --> 00:08:54,834
so you can actually get in
there via command prompt.

221
00:08:54,834 --> 00:08:56,702
You can get in there
via the web as well,

222
00:08:56,702 --> 00:08:59,238
so every node
within Apache Hadoop

223
00:08:59,238 --> 00:09:01,173
actually has an
integrated web server.

224
00:09:01,173 --> 00:09:03,109
I won't talk about security
in this presentation,

225
00:09:03,109 --> 00:09:05,044
but they all have web
servers within them.

226
00:09:05,044 --> 00:09:07,179
You can actually navigate
from system to system,

227
00:09:07,179 --> 00:09:08,247
and they have an API.

228
00:09:09,649 --> 00:09:11,284
So all that information's
within Apache Hadoop,

229
00:09:11,284 --> 00:09:13,886
and that's how you
actually get information in

230
00:09:13,886 --> 00:09:14,887
and out of Hadoop.

231
00:09:16,222 --> 00:09:18,057
And in most Hadoop clusters
for authentication,

232
00:09:18,057 --> 00:09:20,059
they connect back
to an LDAP server,

233
00:09:20,059 --> 00:09:21,761
and they normally have
management systems

234
00:09:21,761 --> 00:09:24,363
that allow you to manage
hundreds and hundreds of nodes.

235
00:09:24,363 --> 00:09:25,932
The limitation
within Apache Hadoop

236
00:09:25,932 --> 00:09:29,468
for how many nodes you can
actually have is 4,000.

237
00:09:29,468 --> 00:09:30,703
And again, a lot
of these are multi,

238
00:09:30,703 --> 00:09:33,172
multi-pedabyte installations.

239
00:09:33,172 --> 00:09:34,874
So that's typically
what an Apache Hadoop

240
00:09:34,874 --> 00:09:36,075
installation looks like.

241
00:09:37,510 --> 00:09:39,478
So from an investigation
standpoint, that's great.

242
00:09:39,478 --> 00:09:40,746
You understand Apache Hadoop,

243
00:09:40,746 --> 00:09:43,983
why it's important to
manage investigations

244
00:09:43,983 --> 00:09:45,383
leading to Hadoop,

245
00:09:45,384 --> 00:09:47,053
but what can you actually
get out of Hadoop?

246
00:09:47,053 --> 00:09:48,854
And there's lots of
different artifacts,

247
00:09:48,854 --> 00:09:51,657
but we're gonna focus
on a handful today.

248
00:09:51,657 --> 00:09:53,526
We're gonna focus on
cluster properties.

249
00:09:53,526 --> 00:09:55,394
How to figure out what
you're actually dealing with

250
00:09:55,394 --> 00:09:58,297
when you're called in
for an investigation.

251
00:09:58,297 --> 00:10:00,800
Logging, how the cluster
is actually set up,

252
00:10:00,800 --> 00:10:03,002
because that will impact
your investigation.

253
00:10:04,203 --> 00:10:05,838
The metadata we talked about.

254
00:10:05,838 --> 00:10:07,673
We're not gonna image
pedabytes and pedabytes.

255
00:10:07,673 --> 00:10:09,742
We're gonna take
pedabytes of information.

256
00:10:09,742 --> 00:10:12,011
We're gonna just
look at the metadata

257
00:10:13,179 --> 00:10:15,414
and focus on just the
gigabytes of information

258
00:10:15,414 --> 00:10:18,484
that are relevant
to the investigation
we're investigating,

259
00:10:18,484 --> 00:10:20,886
and metadata's gonna
be how you do that.

260
00:10:20,886 --> 00:10:23,222
And Hadoop is not a database,

261
00:10:23,222 --> 00:10:25,524
but it does have
a transaction log.

262
00:10:25,524 --> 00:10:27,226
So we're gonna take a look
at the transaction log

263
00:10:27,226 --> 00:10:29,228
and see what information
we can get out of that,

264
00:10:29,228 --> 00:10:32,632
and also have to retrieve
files out of Apache Hadoop.

265
00:10:32,632 --> 00:10:34,433
Again, all of that
in 25 minutes.

266
00:10:35,868 --> 00:10:37,870
So we'll jump into
cluster properties,

267
00:10:37,870 --> 00:10:40,473
so if you get a phone call
saying there's an investigation,

268
00:10:40,473 --> 00:10:42,474
it's leading to Apache Hadoop,

269
00:10:42,475 --> 00:10:44,176
the first question you're
gonna ask yourself is

270
00:10:44,176 --> 00:10:45,678
what am I dealing with

271
00:10:45,678 --> 00:10:48,614
and how long am I gonna be
stuck here imaging this thing?

272
00:10:48,614 --> 00:10:50,082
And to answer those questions,

273
00:10:50,082 --> 00:10:52,617
you're gonna need to get
information about the cluster,

274
00:10:52,618 --> 00:10:55,721
and that comes in the form
of cluster properties.

275
00:10:55,721 --> 00:10:57,390
So some of the properties
you're gonna want to know are

276
00:10:57,390 --> 00:11:00,593
how many nodes are in this
Apache Hadoop installation,

277
00:11:00,593 --> 00:11:02,060
and how big is it?

278
00:11:02,061 --> 00:11:05,398
Are we dealing in gigabytes,
terabytes, or pedabytes?

279
00:11:05,398 --> 00:11:06,832
There's a few different ways

280
00:11:06,832 --> 00:11:09,001
you can get cluster properties
within Apache Hadoop.

281
00:11:09,001 --> 00:11:11,670
You can do it via the web,

282
00:11:11,671 --> 00:11:14,940
so via the different web
servers on the different nodes

283
00:11:14,940 --> 00:11:16,308
within the cluster,

284
00:11:16,308 --> 00:11:18,210
or you do that by
a command prompt.

285
00:11:18,210 --> 00:11:21,013
And just focusing on
the web for a second,

286
00:11:21,013 --> 00:11:22,181
you just type in the address

287
00:11:22,181 --> 00:11:23,115
that you're looking
to connect to,

288
00:11:23,115 --> 00:11:25,551
so the IP address and the port,

289
00:11:25,551 --> 00:11:29,188
and it will actually
dump information
about the actual nodes

290
00:11:29,188 --> 00:11:31,223
within that Apache
Hadoop installation.

291
00:11:31,223 --> 00:11:34,060
So I've highlighted a few
areas on the slide here.

292
00:11:34,060 --> 00:11:35,493
We have live nodes.

293
00:11:35,494 --> 00:11:38,764
How many nodes does this
cluster actually comprise of?

294
00:11:38,764 --> 00:11:41,267
And in the example I have,
it's actually running in a VM,

295
00:11:41,267 --> 00:11:42,868
so it's one live node.

296
00:11:42,868 --> 00:11:44,804
But the real juicy information

297
00:11:44,804 --> 00:11:47,273
comes down to the
name node storage,

298
00:11:47,273 --> 00:11:49,208
and that's image and edits.

299
00:11:49,208 --> 00:11:50,910
That gives you the actual path

300
00:11:50,910 --> 00:11:53,079
to all of that
metadata that you need,

301
00:11:53,079 --> 00:11:55,647
to actually reduce
an investigation,

302
00:11:55,648 --> 00:11:58,751
theoretically from pedabytes
to the relevant gigabytes

303
00:11:58,751 --> 00:12:00,619
of information that you need.

304
00:12:00,619 --> 00:12:02,154
So you want to make
sure you get that path

305
00:12:02,154 --> 00:12:04,323
and you write that down so
you know \where to get access

306
00:12:04,323 --> 00:12:06,257
to this information
in a little bit.

307
00:12:06,258 --> 00:12:08,027
And you can get the
same type of information

308
00:12:08,027 --> 00:12:11,330
via command line
via HDFS DFS Admin,

309
00:12:11,330 --> 00:12:13,699
which is the integrated
Hadoop utility that allows you

310
00:12:13,699 --> 00:12:16,234
to dump similar information
out via command prompt.

311
00:12:19,038 --> 00:12:20,272
So when it comes
down to logging,

312
00:12:20,272 --> 00:12:22,174
there's ton of
logs within Hadoop.

313
00:12:22,174 --> 00:12:23,776
We're not gonna cover
the tons of logs.

314
00:12:23,776 --> 00:12:26,579
I'm gonna generalize here,
just in the interest of time.

315
00:12:26,579 --> 00:12:29,281
So if you look at the
core-site.xml file,

316
00:12:29,281 --> 00:12:32,617
it contains general properties
about the entire cluster.

317
00:12:32,618 --> 00:12:34,086
Most relevant within that file

318
00:12:34,086 --> 00:12:37,223
comes down to the trash
management processes.

319
00:12:37,223 --> 00:12:39,324
So we need delete
files from Hadoop.

320
00:12:39,325 --> 00:12:40,826
Can you still recover them?

321
00:12:40,826 --> 00:12:42,528
And you can recover them
from an area within Hadoop

322
00:12:42,528 --> 00:12:43,863
known as Trash.

323
00:12:43,863 --> 00:12:45,131
And even if it's
moved from Trash,

324
00:12:45,131 --> 00:12:46,298
and it's moved to
another location,

325
00:12:46,298 --> 00:12:47,533
you can still recover it.

326
00:12:47,533 --> 00:12:49,034
And even when it's
deleted from there,

327
00:12:49,034 --> 00:12:51,036
you can still recover it
based on the next location.

328
00:12:51,036 --> 00:12:53,204
But all of that configuration
is within that file,

329
00:12:53,205 --> 00:12:55,441
so that will help you understand
what you're dealing with.

330
00:12:55,441 --> 00:12:59,245
HDFS-site.xml, that
contains information

331
00:12:59,245 --> 00:13:02,581
just about the
Hadoop file system,

332
00:13:02,581 --> 00:13:04,583
so that will give you the
path, the directories,

333
00:13:04,583 --> 00:13:05,818
how it's configured,

334
00:13:05,818 --> 00:13:07,953
how many replicas it's
storing, et cetera.

335
00:13:07,953 --> 00:13:11,557
So that'll provide
you information about
that file system.

336
00:13:11,557 --> 00:13:15,294
Then it comes down to map
reduce, the site.xml file.

337
00:13:15,294 --> 00:13:18,864
That will tell you how map
reduce is actually set up,

338
00:13:18,864 --> 00:13:20,900
and in that file,

339
00:13:20,900 --> 00:13:22,701
it'll actually list
a temporary directory

340
00:13:22,701 --> 00:13:24,336
that it uses for scratch,

341
00:13:24,336 --> 00:13:27,373
so when queries are
executed within Hadoop,

342
00:13:27,373 --> 00:13:30,476
it sometimes stores temporary
results within a folder.

343
00:13:30,476 --> 00:13:31,677
You can actually
go to that folder

344
00:13:31,677 --> 00:13:33,512
and get an idea of
what the PaaS commands

345
00:13:33,512 --> 00:13:36,781
were that were executed based
on that scratch location.

346
00:13:36,782 --> 00:13:39,518
And last but not least,
there are log4j properties,

347
00:13:39,518 --> 00:13:41,821
and that contains information
regarding the configuration

348
00:13:41,821 --> 00:13:44,256
of a lot of the daemons
running on the systems

349
00:13:44,256 --> 00:13:45,824
and the nodes within Hadoop.

350
00:13:47,393 --> 00:13:49,228
So those are the
key configuration
files that tell you

351
00:13:49,228 --> 00:13:50,829
how the cluster's
been configured,

352
00:13:50,830 --> 00:13:53,532
and that's gonna tie in
when we jump to analysis.

353
00:13:54,934 --> 00:13:57,603
So when looking at Hadoop,

354
00:13:57,603 --> 00:14:00,772
we talked about the
focus is just HDFS,

355
00:14:00,773 --> 00:14:02,875
which is a file system.

356
00:14:02,875 --> 00:14:04,310
And when you look
at a file system,

357
00:14:04,310 --> 00:14:06,545
Hadoop stores all the
information the file systems,

358
00:14:06,545 --> 00:14:09,481
so all files, all folders,

359
00:14:09,481 --> 00:14:11,450
and the structure of that,

360
00:14:11,450 --> 00:14:14,453
all within what's known
as an FS image file.

361
00:14:16,088 --> 00:14:19,824
And that FS image, again, stores
files, folders, et cetera,

362
00:14:19,825 --> 00:14:22,862
but it's not complete.

363
00:14:22,862 --> 00:14:25,698
Hadoop contains information
about the file system,

364
00:14:25,698 --> 00:14:27,933
but it also contains
a transaction log.

365
00:14:29,034 --> 00:14:31,503
By default, the
transaction log stores

366
00:14:31,503 --> 00:14:33,839
a million operations within it.

367
00:14:35,274 --> 00:14:38,878
So to get the accurate picture
of the layout of a cluster,

368
00:14:38,878 --> 00:14:41,447
you have to look at
both the FS image

369
00:14:41,447 --> 00:14:43,215
as well as the transaction log.

370
00:14:45,351 --> 00:14:46,886
From an investigation
standpoint,

371
00:14:46,886 --> 00:14:48,920
it's not fun to look
at those two files

372
00:14:48,921 --> 00:14:51,523
and try and manually
correlate what happens.

373
00:14:51,523 --> 00:14:54,627
So I might look at a folder
within my FS image named Kevvie,

374
00:14:54,627 --> 00:14:56,829
but there was a transaction
executed last week

375
00:14:56,829 --> 00:14:58,930
renaming that folder
from Kevvie to Fowler.

376
00:14:58,931 --> 00:15:01,767
I would have to manually map
that transaction to my image

377
00:15:01,767 --> 00:15:04,970
to figure out what the file
system actually looks like.

378
00:15:04,970 --> 00:15:07,573
So before you dump any
information out of FS image,

379
00:15:07,573 --> 00:15:09,775
I recommend you
run a checkpoint.

380
00:15:09,775 --> 00:15:12,511
And that checkpoint takes
that transaction log

381
00:15:12,511 --> 00:15:13,978
and that FS image.

382
00:15:13,979 --> 00:15:15,481
It brings it together

383
00:15:15,481 --> 00:15:18,617
and it comes up with an
up-to-date FS image file.

384
00:15:18,617 --> 00:15:22,087
So you want a checkpoint first
before you dump information.

385
00:15:22,087 --> 00:15:23,689
When it does that
checkpoint process,

386
00:15:23,689 --> 00:15:26,224
the current FS image
gets moved to archive

387
00:15:26,225 --> 00:15:27,626
and a new one is created,

388
00:15:27,626 --> 00:15:29,762
so if you look at the
screen capture below,

389
00:15:29,762 --> 00:15:31,296
that's just using File Manager,

390
00:15:31,297 --> 00:15:34,133
but that's looking at
the actual file system

391
00:15:34,133 --> 00:15:37,403
of the Linux file system with
a name node installed on it,

392
00:15:37,403 --> 00:15:39,872
and you see there's
several FS image files,

393
00:15:39,872 --> 00:15:43,474
and you notice an FS image
file with a number at the end.

394
00:15:43,475 --> 00:15:44,910
That number at
the end represents

395
00:15:44,910 --> 00:15:49,048
the last transaction to
modify that file system image.

396
00:15:49,048 --> 00:15:51,784
The file underneath
it with a dot MD5,

397
00:15:51,784 --> 00:15:54,386
that of course is
the MD5 file hash,

398
00:15:54,386 --> 00:15:56,755
and Hadoop uses that
for integrity matching.

399
00:15:58,357 --> 00:16:00,659
So you're gonna want to make
sure you run the checkpoint

400
00:16:00,659 --> 00:16:03,329
before extracting information
from your FS image.

401
00:16:04,196 --> 00:16:05,597
And that begs the question,

402
00:16:05,597 --> 00:16:08,667
how do you actually get
information out of the FS image?

403
00:16:08,667 --> 00:16:10,102
There's an integrated utility

404
00:16:10,102 --> 00:16:14,173
known as Offline Image Viewer,

405
00:16:14,173 --> 00:16:16,442
and that allows you to
actually extract information.

406
00:16:16,442 --> 00:16:18,410
You can put a delimiter
in there as well

407
00:16:18,410 --> 00:16:19,578
so you can export that to a file

408
00:16:19,578 --> 00:16:20,946
and you can incorporate that

409
00:16:20,946 --> 00:16:23,549
into whatever timeline
application you're using

410
00:16:23,549 --> 00:16:25,584
to factor that in to the
broader investigation.

411
00:16:25,584 --> 00:16:26,819
What's happening in the network?

412
00:16:26,819 --> 00:16:28,152
What's happening in
the operating system?

413
00:16:28,153 --> 00:16:31,290
And now what's happening
within Apache Hadoop?

414
00:16:31,290 --> 00:16:33,257
The timestamps
within Apache Hadoop,

415
00:16:33,258 --> 00:16:35,594
when you look at
last access times,

416
00:16:35,594 --> 00:16:37,963
those are accurate
up to one hour,

417
00:16:39,198 --> 00:16:40,665
which is a little odd
if you think about

418
00:16:40,666 --> 00:16:44,136
modern operating systems
such as Windows NTFS,

419
00:16:44,136 --> 00:16:45,838
those are instantaneous, right?

420
00:16:47,272 --> 00:16:48,140
Wrong.

421
00:16:48,140 --> 00:16:49,675
Trick question.

422
00:16:49,675 --> 00:16:52,411
Does anyone know what the
delay is for Windows NTFS

423
00:16:52,411 --> 00:16:55,446
last access time, by default?

424
00:16:55,447 --> 00:16:56,281
- [Man] An hour.

425
00:16:56,281 --> 00:16:57,116
- One hour.

426
00:16:57,116 --> 00:16:58,283
You got it.

427
00:16:58,283 --> 00:16:59,651
You can even disable
it altogether.

428
00:16:59,651 --> 00:17:01,219
A lot of people look at
the last access times,

429
00:17:01,220 --> 00:17:02,855
think that's guaranteed
to be accurate.

430
00:17:02,855 --> 00:17:06,025
It's accurate up to one hour,
just like Apache Hadoop.

431
00:17:06,025 --> 00:17:07,459
You could even turn off

432
00:17:07,459 --> 00:17:10,496
or disable the NTFS
last access times.

433
00:17:10,496 --> 00:17:13,532
You can do exactly the same
thing within Apache Hadoop.

434
00:17:13,531 --> 00:17:16,367
So HDFS is very similar to NTFS

435
00:17:16,367 --> 00:17:18,604
regarding how it
actually operates.

436
00:17:18,604 --> 00:17:20,005
But that's how to
extract information

437
00:17:20,005 --> 00:17:22,174
regarding the actual
files in the folders.

438
00:17:22,174 --> 00:17:23,575
So if you're doing
timeline analysis,

439
00:17:23,575 --> 00:17:24,977
you want to know what
files are accessed

440
00:17:24,977 --> 00:17:26,979
or modified within a
certain time frame,

441
00:17:26,979 --> 00:17:28,547
you want to dump your FS image

442
00:17:28,547 --> 00:17:29,982
after you've run
that checkpoint,

443
00:17:29,982 --> 00:17:31,649
you can figure out
who's been doing what

444
00:17:31,650 --> 00:17:33,352
within your Apache
Hadoop cluster.

445
00:17:35,120 --> 00:17:36,921
Second half of the picture,
which we've talked about,

446
00:17:36,922 --> 00:17:38,557
is a transaction log.

447
00:17:38,557 --> 00:17:42,428
That's actually called the
Edit log within Apache Hadoop,

448
00:17:42,428 --> 00:17:44,630
and similar to the FS image,

449
00:17:44,630 --> 00:17:46,165
whenever you run a checkpoint,

450
00:17:46,165 --> 00:17:50,034
the current edit log, which
is named Edits in Progress,

451
00:17:50,035 --> 00:17:51,837
gets moved to the archive,

452
00:17:51,837 --> 00:17:53,906
and you see there's three
archives above that file

453
00:17:53,906 --> 00:17:55,706
in the screen capture.

454
00:17:55,707 --> 00:17:58,077
If you look at the
top edits file,

455
00:17:58,077 --> 00:17:59,545
you notice there's two numbers.

456
00:17:59,545 --> 00:18:02,281
So it'll be Edits underscore
one number dash another number.

457
00:18:02,281 --> 00:18:04,983
Those numbers represent
the first transaction

458
00:18:04,983 --> 00:18:08,720
that actually modified
data on disk within Hadoop,

459
00:18:08,720 --> 00:18:11,123
and that last number
is the last transaction

460
00:18:11,123 --> 00:18:13,325
that modified information.

461
00:18:13,325 --> 00:18:15,260
And the interesting thing
is it actually tells you

462
00:18:15,260 --> 00:18:17,496
the range of transactions
within the file,

463
00:18:17,496 --> 00:18:21,467
so you can go back to a past
FS image or a past edit log,

464
00:18:21,467 --> 00:18:23,669
we'll say one week, two
weeks, or three weeks ago,

465
00:18:23,669 --> 00:18:25,104
and actually get an exact copy

466
00:18:25,104 --> 00:18:27,005
of the way the file
system looked like

467
00:18:27,005 --> 00:18:28,807
and what the transaction
log looked like

468
00:18:28,807 --> 00:18:31,510
in the past without
restoring from backup.

469
00:18:31,510 --> 00:18:32,644
So that's actually
pretty helpful

470
00:18:32,644 --> 00:18:34,480
from an investigation
standpoint.

471
00:18:39,985 --> 00:18:42,754
So when you look at the actual
information that's stored

472
00:18:42,754 --> 00:18:44,223
in the edit log...

473
00:18:44,223 --> 00:18:46,859
Again, Hadoop's edit
log is very similar

474
00:18:46,859 --> 00:18:50,896
to a transaction log,
so it stores operations,

475
00:18:50,896 --> 00:18:52,264
and there's lots of
different operations.

476
00:18:52,264 --> 00:18:54,500
I'm not gonna cover all
the operations today,

477
00:18:55,367 --> 00:18:56,835
and we'll cover a few,

478
00:18:56,835 --> 00:18:57,669
and not even all the ones
that are highlighted,

479
00:18:57,669 --> 00:18:59,571
but I'll cover OP_ADD.

480
00:18:59,571 --> 00:19:02,374
Anytime a file is added
to the Hadoop file system,

481
00:19:02,374 --> 00:19:05,410
it's logged in that edit log
with an OP_ADD operation.

482
00:19:05,410 --> 00:19:09,882
OP_RENAME_OLD is anytime
you rename or move a file,

483
00:19:09,882 --> 00:19:11,550
it'll be logged
as OP_RENAME_OLD.

484
00:19:11,550 --> 00:19:14,052
Whenever you delete a file
or make a new directory,

485
00:19:14,052 --> 00:19:17,054
it's logged in OP_DELETE
or OP_MAKE_DIRECTORY.

486
00:19:17,055 --> 00:19:19,124
Skipping over a few, OP_TIMES.

487
00:19:21,326 --> 00:19:23,962
Or anytime you modify
the timestamps of
a file or a folder,

488
00:19:23,962 --> 00:19:26,532
it'll be an OP_TIME operation.

489
00:19:26,532 --> 00:19:29,801
And OP_END_LOG segment
and OP_STARTUP_LOG_SEGMENT

490
00:19:29,801 --> 00:19:31,035
or your default.

491
00:19:31,036 --> 00:19:32,905
Any edit log will have
those to mark the start

492
00:19:32,905 --> 00:19:34,373
and the end of it.

493
00:19:34,373 --> 00:19:37,943
Those are just a few operations
within the transaction log.

494
00:19:37,943 --> 00:19:39,211
So that's well and good.

495
00:19:39,211 --> 00:19:40,712
You're called into
an investigation.

496
00:19:40,712 --> 00:19:42,848
You're looking at an edit log
that has a million entries.

497
00:19:42,848 --> 00:19:44,316
How do you figure out a summary

498
00:19:44,316 --> 00:19:46,218
of what's actually
happened within it?

499
00:19:47,386 --> 00:19:50,689
And to do that, you can
actually use the OEV,

500
00:19:50,689 --> 00:19:54,893
not to be confused with
Offline Image Viewer.

501
00:19:54,893 --> 00:19:57,329
This is Offline Edit Viewer,

502
00:19:57,329 --> 00:19:58,830
so you can actually
use this utility

503
00:19:58,830 --> 00:20:02,466
with the processor argument,

504
00:20:02,467 --> 00:20:05,070
and it will dump a summary
of all the transactions.

505
00:20:05,070 --> 00:20:07,873
How many files were added,
deleted, modified, renamed?

506
00:20:07,873 --> 00:20:10,409
All of that good
stuff is in there.

507
00:20:10,409 --> 00:20:11,810
So I recommend if
you're on a cluster,

508
00:20:11,810 --> 00:20:13,044
you wanna type that command

509
00:20:13,045 --> 00:20:14,646
and make sure you have
that summary, right?

510
00:20:14,646 --> 00:20:16,114
You want to change
your standard out.

511
00:20:16,114 --> 00:20:17,616
Make sure you have that summary

512
00:20:17,616 --> 00:20:20,285
and you understand the type of
information within a million

513
00:20:21,520 --> 00:20:23,822
of those transactions
in your transaction log.

514
00:20:25,557 --> 00:20:26,959
And despite the name Offline,

515
00:20:26,959 --> 00:20:29,094
you actually run it
against online files,

516
00:20:29,094 --> 00:20:30,295
which is kinda confusing,

517
00:20:30,295 --> 00:20:32,264
but that's the way
Hadoop does its thing.

518
00:20:34,967 --> 00:20:37,836
So after you understand the
summary of what's in there,

519
00:20:37,836 --> 00:20:40,339
you want to actually take
a look at the transactions

520
00:20:40,339 --> 00:20:41,907
in that transaction log,

521
00:20:41,907 --> 00:20:43,375
and just changing
your processor,

522
00:20:43,375 --> 00:20:45,844
so eliminating the
processor argument,

523
00:20:45,844 --> 00:20:50,115
you can actually dump
information in plain text

524
00:20:50,115 --> 00:20:52,451
out of your transaction log.

525
00:20:52,451 --> 00:20:55,954
And what type of format
does that look like?

526
00:20:57,456 --> 00:20:58,889
Xml, you got it.

527
00:20:58,890 --> 00:21:01,893
So it dumps information
out in xml format,

528
00:21:01,893 --> 00:21:04,630
and when you look
at Apache Hadoop,

529
00:21:04,630 --> 00:21:05,764
there are lots of operations.

530
00:21:05,764 --> 00:21:07,498
You add a file to Hadoop,

531
00:21:07,499 --> 00:21:10,035
there's a lot of
intercluster communication.

532
00:21:10,035 --> 00:21:10,969
So you add one file.

533
00:21:10,969 --> 00:21:12,371
There might be 10 operations

534
00:21:12,371 --> 00:21:14,940
that are added to that edit log.

535
00:21:14,940 --> 00:21:17,943
The very last operation
is an OP_CLOSE,

536
00:21:17,943 --> 00:21:19,177
and that's cumulative

537
00:21:19,177 --> 00:21:21,112
and contains all
of the information

538
00:21:21,113 --> 00:21:23,248
in all of the other operations.

539
00:21:23,248 --> 00:21:25,117
So whenever you're looking
at any transaction,

540
00:21:25,117 --> 00:21:28,153
you wanna look specifically
at the OP_CLOSE,

541
00:21:28,153 --> 00:21:29,421
and that will be cumulative,

542
00:21:29,421 --> 00:21:30,956
and that will give you
all your information.

543
00:21:30,956 --> 00:21:33,257
If you're looking at some
of the earlier operations

544
00:21:33,258 --> 00:21:34,926
within that transaction,

545
00:21:34,926 --> 00:21:36,395
you're only gonna get
partial information,

546
00:21:36,395 --> 00:21:38,330
so make sure you're
looking at the OP_CLOSE,

547
00:21:38,330 --> 00:21:40,932
and that's highlighted
there under operation.

548
00:21:40,932 --> 00:21:43,602
Then you have the actual
unique transaction ID

549
00:21:43,602 --> 00:21:44,803
associated with Hadoop.

550
00:21:46,238 --> 00:21:47,806
Then you have the file
that was actually created.

551
00:21:47,806 --> 00:21:50,642
This is an example of someone
creating a file within Hadoop,

552
00:21:50,642 --> 00:21:53,679
and you have your modify
and your access times.

553
00:21:53,679 --> 00:21:56,748
And what format do those
times seem to be within?

554
00:22:02,054 --> 00:22:03,255
What do you guys think?

555
00:22:03,255 --> 00:22:04,356
Based on my Linux
and forensics people?

556
00:22:05,757 --> 00:22:07,192
- Epoch?
- Epoch, you got it.

557
00:22:07,192 --> 00:22:10,796
Epoch, and Epoch could be
either seconds or milliseconds,

558
00:22:10,796 --> 00:22:12,397
and that's milliseconds,

559
00:22:12,397 --> 00:22:14,599
so that's the number
of milliseconds since
January 1, 1970.

560
00:22:16,001 --> 00:22:17,569
And it gives you
information about blocks

561
00:22:17,569 --> 00:22:19,404
and the file
permissions as well.

562
00:22:19,404 --> 00:22:20,472
But you can actually go through

563
00:22:20,472 --> 00:22:22,206
and map out what user did what.

564
00:22:22,207 --> 00:22:23,442
Who added a file?

565
00:22:23,442 --> 00:22:24,509
Who deleted a file?

566
00:22:24,509 --> 00:22:26,011
Who renamed a file?

567
00:22:26,011 --> 00:22:28,914
What the internal
Hadoop system was doing.

568
00:22:28,914 --> 00:22:30,248
So it's very, very good

569
00:22:30,248 --> 00:22:32,117
from a forensics and
security standpoint.

570
00:22:32,117 --> 00:22:33,851
A backdate what happened.

571
00:22:33,852 --> 00:22:36,321
And again, you get a
million of these operations

572
00:22:36,321 --> 00:22:37,289
within that edit log.

573
00:22:37,289 --> 00:22:39,358
It's not by size by default.

574
00:22:39,358 --> 00:22:41,393
It's by the number of
operations it stores,

575
00:22:41,393 --> 00:22:44,930
and you can get a million
out of this edit log.

576
00:22:49,634 --> 00:22:50,936
Okay, so you've done
the investigation,

577
00:22:50,936 --> 00:22:53,638
you understand what the
file system looks like,

578
00:22:53,638 --> 00:22:55,741
you understand the type of
operations that were there,

579
00:22:55,741 --> 00:22:59,277
and you've likely identified
some files that you care about.

580
00:22:59,277 --> 00:23:00,645
Maybe a hacker broke in.

581
00:23:00,645 --> 00:23:02,180
I can't use hacker.

582
00:23:02,180 --> 00:23:04,983
I'm used to speaking to board
members and audit committees,

583
00:23:04,983 --> 00:23:06,985
and I can't use cyber
attacker or the other terms,

584
00:23:06,985 --> 00:23:08,419
so I use hacker.

585
00:23:08,420 --> 00:23:11,022
So I'm not at those
types of conferences now.

586
00:23:11,022 --> 00:23:13,291
So a cyber criminal breaks
in and they store a file.

587
00:23:13,291 --> 00:23:16,293
It might be malware or something
else within Apache Hadoop.

588
00:23:16,294 --> 00:23:17,496
So you've got a file of interest

589
00:23:17,496 --> 00:23:19,297
that you want to
actually check out.

590
00:23:19,297 --> 00:23:21,199
So how do you actually get
the information out of Hadoop?

591
00:23:21,199 --> 00:23:25,504
There's a utility within
Hadoop, HDFS FS Check.

592
00:23:25,504 --> 00:23:27,205
That will allow you
to get information

593
00:23:27,205 --> 00:23:29,140
about a specific
file or directory,

594
00:23:29,141 --> 00:23:31,810
and that will tell you
where the file's located.

595
00:23:31,810 --> 00:23:33,043
Remember, every
time you add a file,

596
00:23:33,044 --> 00:23:35,647
Hadoop makes a bunch of
replicas of that file,

597
00:23:35,647 --> 00:23:38,216
so you can have
hundreds of data nodes,

598
00:23:38,216 --> 00:23:41,052
but the file's stored
on maybe three of them.

599
00:23:41,052 --> 00:23:43,621
So you want to make
sure you can get the IDs

600
00:23:43,622 --> 00:23:45,390
of the actual nodes
that contain the files

601
00:23:45,390 --> 00:23:46,625
and the block IDs.

602
00:23:46,625 --> 00:23:48,527
You can get all that
with this command,

603
00:23:48,527 --> 00:23:50,929
and that will dump the
information via command prompt.

604
00:23:50,929 --> 00:23:53,799
Once you get that, you
navigate to the actual server,

605
00:23:53,799 --> 00:23:55,834
and you're just in the
Linux operating systems,

606
00:23:55,834 --> 00:23:59,171
assuming you've installed
Hadoop on Linux,

607
00:23:59,171 --> 00:24:03,241
and you map to the
actual location that
contains the block,

608
00:24:03,241 --> 00:24:04,476
and you can get the block.

609
00:24:04,476 --> 00:24:06,645
You can just simply use
whatever imaging tool

610
00:24:06,645 --> 00:24:09,748
you're used to using,
DDD_CFOD, whatver,

611
00:24:10,615 --> 00:24:12,717
and just image that block.

612
00:24:12,717 --> 00:24:15,754
That allows you to extract
the file from Hadoop

613
00:24:15,754 --> 00:24:18,857
while preserving the timestamps.

614
00:24:18,857 --> 00:24:20,225
Hadoop has some native utilities

615
00:24:20,225 --> 00:24:22,427
that allow you to
take files out of HDFS

616
00:24:22,427 --> 00:24:24,995
and dump it out in the
local operating system,

617
00:24:24,996 --> 00:24:27,299
but it clobbers your timestamps.

618
00:24:27,299 --> 00:24:28,500
So this way would
allow you to do it

619
00:24:28,500 --> 00:24:30,602
while preserving your
actual timestamps.

620
00:24:31,970 --> 00:24:34,306
And despite Hadoop being
installed on top of a server,

621
00:24:34,306 --> 00:24:35,841
all the files are just blocks

622
00:24:35,841 --> 00:24:37,775
and just data text
files that are stored

623
00:24:37,776 --> 00:24:39,578
in the local file system.

624
00:24:39,578 --> 00:24:41,812
So you can go through
Hadoop to extract data,

625
00:24:41,813 --> 00:24:43,882
or you can go in through the
local file system and steal it.

626
00:24:43,882 --> 00:24:46,284
Hadoop won't even
know you were there,

627
00:24:46,284 --> 00:24:49,721
because this is the way
coming in via the back door.

628
00:24:49,721 --> 00:24:51,122
So when you look at Trash,

629
00:24:51,122 --> 00:24:52,691
when files are
deleted from Hadoop,

630
00:24:52,691 --> 00:24:54,693
they're not deleted
forever, of course.

631
00:24:54,693 --> 00:24:56,595
When you delete a file,

632
00:24:56,595 --> 00:24:59,631
Hadoop just moves it from
the original location

633
00:24:59,631 --> 00:25:01,566
to a trash directory.

634
00:25:01,566 --> 00:25:05,136
It stays in that trash directory
for a little bit of time.

635
00:25:05,136 --> 00:25:06,404
It's then marked for deletion

636
00:25:06,404 --> 00:25:07,839
and it's moved to
a new structure,

637
00:25:07,839 --> 00:25:11,877
which is transaction
ID, month, date,

638
00:25:11,877 --> 00:25:14,212
and it's stored there
until a process comes by

639
00:25:14,212 --> 00:25:16,047
and actually deletes that file.

640
00:25:17,782 --> 00:25:20,385
So during that process after
someone deletes a file,

641
00:25:20,385 --> 00:25:23,288
you can still go to the trash
directory and retrieve it,

642
00:25:23,288 --> 00:25:26,258
you can go to the other
location that I mentioned

643
00:25:26,258 --> 00:25:29,127
and retrieve it there while
it's scheduled for deletion.

644
00:25:29,127 --> 00:25:31,096
But even after it's
scheduled for deletion,

645
00:25:31,096 --> 00:25:32,230
and we'll say it gets deleted,

646
00:25:32,230 --> 00:25:33,798
do you think it's gone forever?

647
00:25:35,133 --> 00:25:36,001
50% chance.

648
00:25:36,001 --> 00:25:36,835
Who wants to guess?

649
00:25:36,835 --> 00:25:37,669
Who's a brave soul?

650
00:25:38,937 --> 00:25:40,504
- No?
- No, there you go.

651
00:25:40,505 --> 00:25:42,007
No, it's not,

652
00:25:42,007 --> 00:25:44,576
because it's just a file
in the local file system.

653
00:25:44,576 --> 00:25:45,944
When you have a file in Linux

654
00:25:45,944 --> 00:25:47,646
or a file in Windows
that gets deleted,

655
00:25:47,646 --> 00:25:48,914
you still recover it.

656
00:25:48,914 --> 00:25:51,316
It's no different
within Apache Hadoop.

657
00:25:51,316 --> 00:25:54,152
So there's multiple levels you
can actually recover a file

658
00:25:54,152 --> 00:25:56,488
from when coming
through Apache Hadoop.

659
00:25:56,488 --> 00:25:58,790
And keep in mind, there
are some caveats here.

660
00:25:59,958 --> 00:26:03,962
So deletions done by an
API don't go to trash.

661
00:26:05,130 --> 00:26:07,465
When a user might
be on command line,

662
00:26:07,465 --> 00:26:08,800
you can actually delete a file

663
00:26:08,800 --> 00:26:10,368
and specify not to go to trash,

664
00:26:10,368 --> 00:26:11,570
so there are a few
ways you can try

665
00:26:11,570 --> 00:26:12,971
and get around
the trash process,

666
00:26:12,971 --> 00:26:14,639
which isn't enabled by default,

667
00:26:14,639 --> 00:26:17,442
but Hadoop or Apache
recommends you enable it.

668
00:26:18,677 --> 00:26:20,211
And even it's not

669
00:26:20,211 --> 00:26:21,379
and it's deleted from the
local file system, again,

670
00:26:21,379 --> 00:26:22,647
you can still recover it

671
00:26:22,647 --> 00:26:25,082
using your standard
data recovery tools.

672
00:26:26,217 --> 00:26:27,686
And the last slide I
want to talk about today

673
00:26:27,686 --> 00:26:28,887
are threats to look out for.

674
00:26:28,887 --> 00:26:30,722
So we've talked
about Apache Hadoop

675
00:26:30,722 --> 00:26:32,490
and it being a file system.

676
00:26:32,490 --> 00:26:33,758
I told you earlier today
we're gonna focus on

677
00:26:33,758 --> 00:26:34,960
the just the file system.

678
00:26:34,960 --> 00:26:36,494
I sorta lied, because
I've got one slide

679
00:26:36,494 --> 00:26:39,496
talking about some of
the add-on applications.

680
00:26:40,365 --> 00:26:41,666
There's a lot of databases.

681
00:26:41,666 --> 00:26:43,168
I think there's three or
four different databases

682
00:26:43,168 --> 00:26:45,370
you can add onto Apache Hadoop,

683
00:26:45,370 --> 00:26:46,804
and a lot of organizations

684
00:26:46,805 --> 00:26:48,239
focus on the
traditional development,

685
00:26:48,239 --> 00:26:50,241
whether it's DotNet or Java or C

686
00:26:50,241 --> 00:26:52,143
or whatever language
they're developing in.

687
00:26:52,143 --> 00:26:53,612
They have software
development life cycles

688
00:26:53,612 --> 00:26:55,413
to help ensure they
got secure code,

689
00:26:56,481 --> 00:26:58,850
and the SDLC is working
great over here,

690
00:26:58,850 --> 00:27:01,151
but as soon as you taught
big data organizations

691
00:27:01,152 --> 00:27:04,856
spin up a big data
technology or platform,

692
00:27:04,856 --> 00:27:06,591
they bring in sometimes
some consultant,

693
00:27:06,591 --> 00:27:07,859
sometimes it's in-house people.

694
00:27:07,859 --> 00:27:09,761
They develop things and
they do their own thing.

695
00:27:09,761 --> 00:27:11,262
For whatever reason,

696
00:27:11,262 --> 00:27:15,100
the SDLC doesn't map to
big data implementations,

697
00:27:15,100 --> 00:27:16,501
so a lot of the vulnerabilities

698
00:27:16,501 --> 00:27:18,235
that you had with traditional
development languages

699
00:27:18,236 --> 00:27:20,672
back in the early
2000s, late 2000s,

700
00:27:20,672 --> 00:27:23,541
are still present
within Apache Hadoop.

701
00:27:23,541 --> 00:27:26,043
One of the databases
you can add on to Hadoop

702
00:27:26,044 --> 00:27:27,278
is called Hive.

703
00:27:27,278 --> 00:27:30,649
You interact with
it by HQL or HiveQL.

704
00:27:30,649 --> 00:27:32,016
And guess what?

705
00:27:32,017 --> 00:27:35,020
HiveQL is still susceptible
to SQL injection.

706
00:27:35,020 --> 00:27:37,389
So I'm not gonna get
into first-order SQL
injection attacks

707
00:27:37,389 --> 00:27:39,323
or second-order SQL
injection attacks,

708
00:27:39,324 --> 00:27:42,027
but simple, dynamic,
blind, stack queries.

709
00:27:42,027 --> 00:27:46,197
All that good stuff you can
still within the Hive by HQL.

710
00:27:46,197 --> 00:27:47,966
So for those guys in
the audience here,

711
00:27:47,966 --> 00:27:49,934
guys and gals who are red teams,

712
00:27:51,403 --> 00:27:53,972
when you look at your attack
paths or your attack trees,

713
00:27:53,972 --> 00:27:57,341
you can actually come into an
application via SQL injection.

714
00:27:57,342 --> 00:27:59,444
So web server to database.

715
00:27:59,444 --> 00:28:02,047
Modern databases have
connectors to Apache Hadoop's

716
00:28:02,047 --> 00:28:04,215
SQL server 2012 and higher,

717
00:28:04,215 --> 00:28:06,650
leveraged an HQL vulerability,

718
00:28:06,651 --> 00:28:08,286
extract information
out of Hadoop,

719
00:28:08,286 --> 00:28:09,521
return it to your database,

720
00:28:09,521 --> 00:28:11,856
and then return it
to your web browser

721
00:28:11,856 --> 00:28:14,125
to view halfwy across
the ocean somethere.

722
00:28:14,125 --> 00:28:15,493
So when you're looking
at attack path,

723
00:28:15,493 --> 00:28:17,294
don't count out Hadoop.

724
00:28:17,295 --> 00:28:19,431
It's connected to more things
than you might think it is.

725
00:28:19,431 --> 00:28:20,899
From a forensics standpoint,

726
00:28:20,899 --> 00:28:22,966
make sure your standard
operating procedures

727
00:28:22,967 --> 00:28:25,370
cover Apache Hadoop as well.

728
00:28:26,971 --> 00:28:28,473
So we covered Apache Hadoop,

729
00:28:28,473 --> 00:28:29,941
some of the commands to
run to actually figure out

730
00:28:29,941 --> 00:28:31,209
what you're dealing with,

731
00:28:31,209 --> 00:28:33,078
how to get information
about the file system,

732
00:28:33,078 --> 00:28:34,913
how to look at the
transaction log,

733
00:28:34,913 --> 00:28:36,081
and how to pull it togther

734
00:28:36,081 --> 00:28:39,417
and also a new
threat with HiveQL,

735
00:28:39,417 --> 00:28:40,918
and we talked about the Hive,

736
00:28:40,919 --> 00:28:42,587
but you have Impala,
you have Spark.

737
00:28:42,587 --> 00:28:46,157
There's lots of database-type
products within Apache Hadoop.

738
00:28:46,157 --> 00:28:47,292
I haven't done the
research there,

739
00:28:47,292 --> 00:28:48,827
but if I was a betting man,

740
00:28:48,827 --> 00:28:51,396
I'd go all in to say they
had the same weaknesses

741
00:28:51,396 --> 00:28:54,299
regarding HQL injection.

742
00:28:54,299 --> 00:28:55,800
So based on what
we covered today,

743
00:28:55,800 --> 00:29:00,271
are there any questions that
you have in Apache Hadoop?

744
00:29:01,106 --> 00:29:02,006
My Irish heritage?

745
00:29:02,006 --> 00:29:02,841
Whatever it is.

746
00:29:02,841 --> 00:29:04,074
Yes.

747
00:29:04,075 --> 00:29:05,376
With Hadoop, you can
install that on Linux,

748
00:29:05,376 --> 00:29:07,345
or you can install
it on Windows.

749
00:29:07,345 --> 00:29:09,180
So you can a standard
Linux server.

750
00:29:09,180 --> 00:29:10,982
It's gonna be your EXD 2, 3.

751
00:29:10,982 --> 00:29:13,051
If you're gonna install
that within Windows,

752
00:29:13,051 --> 00:29:14,351
that's gonna be NTFS.

753
00:29:14,352 --> 00:29:15,353
That's gonna be standard.

754
00:29:15,353 --> 00:29:16,888
You install Hadoop on top of it,

755
00:29:16,888 --> 00:29:20,591
and it still stores files
in that local file system,

756
00:29:20,592 --> 00:29:23,461
so it's no different from
a standard Linux server,

757
00:29:23,461 --> 00:29:26,097
no different from a
standard Windows server.

758
00:29:26,097 --> 00:29:28,533
So it's what we all know
about the same tools you have,

759
00:29:28,533 --> 00:29:31,101
the same techniques can all
be used to steal the blocks

760
00:29:31,102 --> 00:29:32,604
right out of Apache Hadoop,

761
00:29:32,604 --> 00:29:34,372
and it won't even
know you were there.

762
00:29:37,242 --> 00:29:38,976
- [Man] Time for
one more question.

763
00:29:38,977 --> 00:29:40,245
Anyone?
- One more question?

764
00:29:40,245 --> 00:29:41,780
Well, I can't be off this easy.

765
00:29:45,750 --> 00:29:47,085
- Alright, thank you very much.

766
00:29:47,085 --> 00:29:48,586
- That's it.

767
00:29:48,586 --> 00:29:49,554
Thank you, and everyone enjoy
the rest of your conference.

768
00:29:49,554 --> 00:29:53,892
(applause)
(soft music)

