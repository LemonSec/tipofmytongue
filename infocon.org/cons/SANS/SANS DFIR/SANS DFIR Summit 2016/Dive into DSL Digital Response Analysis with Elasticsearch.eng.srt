1
00:00:01,901 --> 00:00:05,572
(tones chime and intensify)

2
00:00:11,378 --> 00:00:14,381
(audience applauds)

3
00:00:15,415 --> 00:00:17,017
- Thank you.

4
00:00:17,017 --> 00:00:18,752
Like Phil said, we are with

5
00:00:18,752 --> 00:00:21,287
the KPMG Forensics
Team out of Chicago.

6
00:00:21,287 --> 00:00:22,822
And before we get started,

7
00:00:22,822 --> 00:00:26,226
I just want to mention one
of our other team members

8
00:00:26,226 --> 00:00:29,195
from KPMG Forensics
Team out of Chicago,

9
00:00:29,195 --> 00:00:33,767
Ken Johnson, who couldn't
be with us here today.

10
00:00:33,767 --> 00:00:37,003
If you guys were at the
memorial service yesterday,

11
00:00:37,003 --> 00:00:40,440
the site at the bottom is
a website that we set up

12
00:00:40,440 --> 00:00:42,842
where people can give
their condolences to him,

13
00:00:42,842 --> 00:00:45,345
and it's just
amazing, the response.

14
00:00:45,345 --> 00:00:47,180
The comments are on the side.

15
00:00:47,180 --> 00:00:49,182
You scroll down and
then you click for more

16
00:00:49,182 --> 00:00:51,117
and you can just keep
scrolling, and clicking,

17
00:00:51,117 --> 00:00:54,254
and scrolling, and
clicking; it goes forever.

18
00:00:54,254 --> 00:00:57,157
And I just want to thank
the DFIR community,

19
00:00:57,157 --> 00:00:59,092
and SANS for their
response to this.

20
00:00:59,092 --> 00:01:02,896
It really showed what
kind of a community we are

21
00:01:02,896 --> 00:01:05,165
from what they did.

22
00:01:05,165 --> 00:01:08,268
So, getting started with this.

23
00:01:08,268 --> 00:01:12,972
This presentation actually
began whenever I attended

24
00:01:12,972 --> 00:01:17,476
the Elastic{ON} world tour
conference in Chicago.

25
00:01:17,477 --> 00:01:20,780
So, Elastic{ON} is the main
Elasticsearch conference

26
00:01:20,780 --> 00:01:23,616
in San Francisco and
they did a world tour,

27
00:01:23,616 --> 00:01:27,153
hitting many major cities
in the US and Europe.

28
00:01:27,153 --> 00:01:29,155
I got to go to Chicago,

29
00:01:29,155 --> 00:01:31,958
the one in Chicago,
and that's where I met

30
00:01:31,958 --> 00:01:35,128
the developer of the Python
library for Elasticsearch.

31
00:01:35,128 --> 00:01:37,097
His name is Honza Kral.

32
00:01:37,097 --> 00:01:39,999
And I went up and talked
to him, and I said

33
00:01:39,999 --> 00:01:43,536
"Hey, I use your Python
library; it's awesome.

34
00:01:43,536 --> 00:01:45,371
I don't really have
any questions for you,

35
00:01:45,371 --> 00:01:47,173
but can you show
me something cool?"

36
00:01:47,173 --> 00:01:48,975
And he said, "Have you ever used

37
00:01:48,975 --> 00:01:51,344
the Elasticsearch DSL library."

38
00:01:51,344 --> 00:01:53,378
And I said "No, I've
never heard of it."

39
00:01:53,379 --> 00:01:55,181
So he showed it to me,
showed me all these

40
00:01:55,181 --> 00:01:58,651
cool features for it, and
I go, okay, that's cool,

41
00:01:58,651 --> 00:02:01,521
and checked out the other
stuff at Elastic{ON}

42
00:02:01,521 --> 00:02:04,090
with all the visualizations
and great presentations.

43
00:02:04,090 --> 00:02:06,226
Highly recommend, if you
guys are into Elasticsearch

44
00:02:06,226 --> 00:02:08,661
and they do that again,
definitely check it out.

45
00:02:08,661 --> 00:02:09,996
I kept this in the
back of my mind

46
00:02:09,996 --> 00:02:11,531
and we actually were
able to apply it

47
00:02:11,531 --> 00:02:12,465
to some of the
stuff we were doing

48
00:02:12,465 --> 00:02:15,001
in the forensic lab in Chicago.

49
00:02:16,202 --> 00:02:18,771
So, since this is just
a 20, 25 minute talk,

50
00:02:18,771 --> 00:02:20,739
we have a few assumptions.

51
00:02:20,740 --> 00:02:23,243
We assume that you have
Elasticsearch already deployed,

52
00:02:23,243 --> 00:02:25,678
and that you've ingested
forensic artifacts

53
00:02:25,678 --> 00:02:27,113
in Elasticsearch.

54
00:02:27,113 --> 00:02:30,316
If you have any issues
getting started with those,

55
00:02:30,316 --> 00:02:32,886
find me afterwards;
if you buy me a beer

56
00:02:32,886 --> 00:02:35,989
I would love to talk to
you about how to do those.

57
00:02:35,989 --> 00:02:39,125
We also assume that you have
the Python library installed.

58
00:02:39,125 --> 00:02:41,628
There's the command to do that,
just type in those letters,

59
00:02:41,628 --> 00:02:45,097
hit enter, and you're
well on your way.

60
00:02:45,098 --> 00:02:47,934
So, the Elasticsearch
DSL Library,

61
00:02:47,934 --> 00:02:49,469
we're going to be using Python,

62
00:02:49,469 --> 00:02:51,436
but the same
functionality is available

63
00:02:51,437 --> 00:02:53,406
in many other languages.

64
00:02:54,741 --> 00:02:58,044
There's objects in the
DSL library language.

65
00:02:58,044 --> 00:03:00,013
Today we're gonna be
dealing with queries,

66
00:03:00,013 --> 00:03:02,182
aggregations, and searches.

67
00:03:03,383 --> 00:03:05,952
The really cool thing
about the DSL language

68
00:03:05,952 --> 00:03:09,389
is the query objects;
they can be combined

69
00:03:09,389 --> 00:03:12,992
with boolean operators,
so an ampersand for and,

70
00:03:12,992 --> 00:03:15,695
a pipe for or, and
a tilde for not.

71
00:03:15,695 --> 00:03:17,230
And we'll go through
some examples

72
00:03:17,230 --> 00:03:20,233
of what we can do with that.

73
00:03:20,233 --> 00:03:22,302
These DSL objects, all of these

74
00:03:22,302 --> 00:03:24,771
can be deserialized
to a dictionary.

75
00:03:24,771 --> 00:03:27,339
If you guys aren't coders
and the word deserialized

76
00:03:27,340 --> 00:03:29,876
is kind of intimidating
to you, it's real simple.

77
00:03:29,876 --> 00:03:33,012
We'll show you an example
of what's going on.

78
00:03:33,012 --> 00:03:38,017
It supports every type of query
that Elasticsearch supports,

79
00:03:39,419 --> 00:03:40,954
but we're going to be
focusing on query strings.

80
00:03:40,954 --> 00:03:44,190
If you've ever used Kibana,
you've used a query string.

81
00:03:44,190 --> 00:03:46,759
And the other thing
about Elasticsearch DSL

82
00:03:46,759 --> 00:03:49,862
is if you do implement
it, don't be surprised

83
00:03:49,862 --> 00:03:52,765
if it works flawlessly
with Django,

84
00:03:52,765 --> 00:03:55,802
because Honza Kral, the
developer of this library,

85
00:03:55,802 --> 00:03:58,438
is also a core Django developer.

86
00:04:00,073 --> 00:04:04,911
So, let's say we want to make
a query on file execution.

87
00:04:04,911 --> 00:04:06,412
So this is how we would start:

88
00:04:06,412 --> 00:04:10,450
we import the library and
we import the Q object,

89
00:04:12,318 --> 00:04:17,090
and then here is how you
would build a query object

90
00:04:17,089 --> 00:04:19,424
and we're gonna use
the artifact prefetch.

91
00:04:19,425 --> 00:04:22,729
It's a great artifact to
see if the file executed.

92
00:04:22,729 --> 00:04:25,565
So you make your query
string right there,

93
00:04:25,565 --> 00:04:28,067
and then this is
how you deserialize

94
00:04:28,067 --> 00:04:30,203
a query object to a string.

95
00:04:30,203 --> 00:04:32,405
And then we can print it out,

96
00:04:32,405 --> 00:04:34,107
and this is what it looks like.

97
00:04:34,107 --> 00:04:37,043
This is what's called
the body string,

98
00:04:37,043 --> 00:04:40,146
and this is what is
sent to Elasticsearch

99
00:04:40,146 --> 00:04:42,649
in the HTTP REST API.

100
00:04:42,649 --> 00:04:45,318
And as you can see,
it's pretty basic.

101
00:04:45,318 --> 00:04:47,787
There's just a
couple of brackets

102
00:04:47,787 --> 00:04:50,390
and you've got your
query string right there.

103
00:04:50,390 --> 00:04:54,027
But this can get a lot
more complex, as we'll see.

104
00:04:54,027 --> 00:04:56,562
So, here's a second
version of that query.

105
00:04:56,562 --> 00:04:58,164
Let's say we want to add
on a little bit more,

106
00:04:58,164 --> 00:05:00,767
we wanna bring in
more artifacts.

107
00:05:00,767 --> 00:05:03,069
Let's say we have a
Windows 8 and up machine,

108
00:05:03,069 --> 00:05:05,738
we have Amcache available to us,

109
00:05:06,906 --> 00:05:09,108
so we can just create
another query object

110
00:05:09,108 --> 00:05:11,878
with the artifact type amcache,

111
00:05:11,878 --> 00:05:13,712
and then we can combine
those two together

112
00:05:13,713 --> 00:05:15,915
with a pipe for a boolean or.

113
00:05:15,915 --> 00:05:19,052
So we're gonna say,
artifact type prefetch

114
00:05:19,052 --> 00:05:22,655
or artifact type amcache,
and then that builds

115
00:05:22,655 --> 00:05:25,825
a new query object
with the two of those.

116
00:05:25,825 --> 00:05:29,062
Again, we deserialize it,
and this is what you get

117
00:05:29,062 --> 00:05:30,897
with your body string.

118
00:05:32,265 --> 00:05:35,268
Grew significantly; we now
have a should statement in here

119
00:05:35,268 --> 00:05:36,936
with two conditions,

120
00:05:37,837 --> 00:05:39,906
there is a boolean or in between

121
00:05:39,906 --> 00:05:44,711
the artifact type prefetch
and the artifact type amcache.

122
00:05:44,711 --> 00:05:49,549
So, let's take this another
level of complexity.

123
00:05:49,549 --> 00:05:52,585
We'll add to it,
so this same query

124
00:05:52,585 --> 00:05:56,289
but with Amcache it
tells you all DLLs

125
00:05:56,289 --> 00:05:57,724
that were loaded with Amcache,

126
00:05:57,724 --> 00:06:00,093
and I love that; I love
the verbosity of that,

127
00:06:00,093 --> 00:06:01,627
but let's say we want
to filter those out,

128
00:06:01,627 --> 00:06:05,298
for example, say we just
want the executables.

129
00:06:05,298 --> 00:06:09,469
We can build a query for
paths that end with .dll,

130
00:06:10,670 --> 00:06:12,405
and if you can see
it's kind of small

131
00:06:12,405 --> 00:06:14,907
at the beginning of the
query, there's a little tilde

132
00:06:14,907 --> 00:06:17,109
for the boolean not.

133
00:06:17,110 --> 00:06:21,280
So our query condition is
going to look like this,

134
00:06:22,515 --> 00:06:25,685
where q0 is the
artifact type prefetch

135
00:06:25,685 --> 00:06:28,020
or artifact type amcache and

136
00:06:29,956 --> 00:06:31,891
not path ending in dll.

137
00:06:34,694 --> 00:06:37,730
So we deserialize
this to a dictionary,

138
00:06:37,730 --> 00:06:39,932
and you see where
I'm going with this.

139
00:06:39,932 --> 00:06:43,336
This isn't very manageable
if someone asks you,

140
00:06:43,336 --> 00:06:45,605
"Can you take something out,
can you put something in?"

141
00:06:45,605 --> 00:06:48,741
Managing all these curly
brackets and braces

142
00:06:48,741 --> 00:06:51,978
would just be a nightmare;
it doesn't really scale.

143
00:06:51,978 --> 00:06:55,114
And so, actually whenever I
wrote my first complex query

144
00:06:55,114 --> 00:06:59,085
like this, it sparked
something in my head

145
00:06:59,085 --> 00:07:01,753
from back in my reverse
engineering days,

146
00:07:01,754 --> 00:07:04,757
and I took a look
at the complex query

147
00:07:04,757 --> 00:07:08,327
and if there's any reverse
engineers in the audience,

148
00:07:08,327 --> 00:07:11,731
this should sort of
resemble a YARA signature,

149
00:07:11,731 --> 00:07:14,033
like the YARA condition
at the bottom,

150
00:07:14,033 --> 00:07:16,803
and I was like, "Wow,
that's brilliant,"

151
00:07:16,803 --> 00:07:19,572
and that leads to
the next point,

152
00:07:20,940 --> 00:07:22,909
that whenever we're talking
about this in the lab,

153
00:07:22,909 --> 00:07:25,077
we're saying reverse engineers

154
00:07:25,077 --> 00:07:28,681
for static binary analysis,
they have YARA signatures,

155
00:07:28,681 --> 00:07:31,751
for IR we have
indicators of compromise,

156
00:07:31,751 --> 00:07:36,155
but what do we have to
identify things that are

157
00:07:36,155 --> 00:07:39,025
of significant forensics...

158
00:07:39,025 --> 00:07:41,626
of forensic significance?

159
00:07:41,627 --> 00:07:44,564
So we're wondering, what format
are we gonna put this in?

160
00:07:44,564 --> 00:07:47,834
And, messing around with
the Elasticsearch DSL,

161
00:07:47,834 --> 00:07:49,669
we're able to do this.

162
00:07:52,038 --> 00:07:55,041
So with the indicators
of compromise

163
00:07:56,476 --> 00:07:58,311
and the YARA signatures,
you have a certain amount

164
00:07:58,311 --> 00:08:00,179
of functionality
available to you

165
00:08:00,179 --> 00:08:02,315
with analysis you
can do with that.

166
00:08:02,315 --> 00:08:05,251
What do we have
available for forensics?

167
00:08:05,251 --> 00:08:09,188
Well, with Apache Lucene, we
can have boolean operators

168
00:08:09,188 --> 00:08:13,759
in the string itself, not
just combining the objects

169
00:08:13,759 --> 00:08:15,695
but actually in
the query string.

170
00:08:15,695 --> 00:08:18,097
Oldie but a goodie, wildcards.

171
00:08:18,097 --> 00:08:20,132
You guys know how
to use wildcards.

172
00:08:20,132 --> 00:08:23,603
A phrase query is a
string around quotes,

173
00:08:23,603 --> 00:08:28,074
it'll search that whole phrase,
and that's not very complex,

174
00:08:28,074 --> 00:08:30,543
but the really interesting
thing is the next one,

175
00:08:30,543 --> 00:08:32,512
proximity queries.

176
00:08:32,511 --> 00:08:35,780
You can actually search
for those individual words

177
00:08:35,780 --> 00:08:37,683
out of order with
the edit distance

178
00:08:37,683 --> 00:08:40,620
of whatever you
specify, one or greater.

179
00:08:40,620 --> 00:08:44,256
If the words are out of
order that will still match.

180
00:08:44,256 --> 00:08:46,459
Fuzzy queries are awesome.

181
00:08:46,459 --> 00:08:48,560
This example, you
know, you have those

182
00:08:48,561 --> 00:08:51,731
malicious iterations of SvcHost:

183
00:08:51,731 --> 00:08:55,001
ScvHost and SvcHost with a zero.

184
00:08:55,001 --> 00:08:59,137
This uses the Levenshtein
distance to find iterations

185
00:08:59,138 --> 00:09:02,341
of a string, so we
can use this to find

186
00:09:02,341 --> 00:09:05,077
those malicious
executable names.

187
00:09:06,445 --> 00:09:09,181
A range query is really
useful for event log,

188
00:09:09,181 --> 00:09:11,250
so that would be all
the user activity

189
00:09:11,250 --> 00:09:13,486
in the security event logs.

190
00:09:13,486 --> 00:09:15,221
Regular expression,
what would we do without

191
00:09:15,221 --> 00:09:16,522
regular expression?

192
00:09:16,522 --> 00:09:18,858
We would be
hard-coating file paths.

193
00:09:18,858 --> 00:09:20,560
You don't have to do that.

194
00:09:20,560 --> 00:09:22,328
There's an example
that will match

195
00:09:22,328 --> 00:09:25,731
both program files
and program files x86.

196
00:09:27,099 --> 00:09:29,302
As well as what we covered
in the last slides,

197
00:09:29,302 --> 00:09:31,537
that you can combine
these objects together

198
00:09:31,537 --> 00:09:35,875
and keep adding to them with
your different conditions.

199
00:09:35,875 --> 00:09:37,376
And then also, since we keep it

200
00:09:37,376 --> 00:09:40,579
in the query string format,
it can be used in Kibana.

201
00:09:40,580 --> 00:09:42,281
We can just add those
all up, convert it

202
00:09:42,281 --> 00:09:44,784
back to a query string,
pop it in Kibana,

203
00:09:44,784 --> 00:09:46,718
and then go from there.

204
00:09:47,853 --> 00:09:50,256
So now that we can make a query,

205
00:09:50,256 --> 00:09:52,358
Andrea's gonna show us
what we can do with that

206
00:09:52,358 --> 00:09:55,393
and do some really
cool analysis with it.

207
00:09:55,394 --> 00:09:56,495
- So, aggregations.

208
00:09:56,495 --> 00:09:58,531
Why do we use aggregations?

209
00:09:58,531 --> 00:10:02,635
Well, aggregations let us
do analytics on a search.

210
00:10:02,635 --> 00:10:04,837
They are performed
at search time,

211
00:10:04,837 --> 00:10:07,340
and the way they work is
just to build buckets,

212
00:10:07,340 --> 00:10:10,710
classifying all your data
entries in the last search.

213
00:10:10,710 --> 00:10:12,778
And there's so many types
of aggregations you can use.

214
00:10:12,778 --> 00:10:14,246
I'm gonna show
you some examples.

215
00:10:14,246 --> 00:10:16,382
If you have the
terms aggregations,

216
00:10:16,382 --> 00:10:19,050
those are based on a field,
and they take into account

217
00:10:19,051 --> 00:10:21,020
all the possible values
that that field can have

218
00:10:21,020 --> 00:10:22,655
within your data set.

219
00:10:22,655 --> 00:10:24,055
Then you have the
date histogram,

220
00:10:24,056 --> 00:10:27,493
to get a really nice timeline
overview of your case.

221
00:10:27,493 --> 00:10:31,464
Metrics, that's if you have
fields that contain numbers,

222
00:10:31,464 --> 00:10:34,200
you can calculate averages,
standard deviations,

223
00:10:34,200 --> 00:10:37,903
and so many other
mathematical operations.

224
00:10:37,903 --> 00:10:40,072
You have also the
geo bound ones,

225
00:10:40,072 --> 00:10:42,942
that is if you have latitude
and longitude coordinates

226
00:10:42,942 --> 00:10:44,877
within your data sets,
those are two values

227
00:10:44,877 --> 00:10:46,478
that you have ingested.

228
00:10:46,479 --> 00:10:48,514
You can do aggregations on those

229
00:10:48,514 --> 00:10:52,184
to see how many data
are related to a country

230
00:10:52,184 --> 00:10:54,620
or a region or a
city, stuff like that.

231
00:10:54,620 --> 00:10:57,123
There's so many more, and
with the newer version

232
00:10:57,123 --> 00:10:59,325
of Elasticsearch, they
have built even more types

233
00:10:59,325 --> 00:11:01,093
of aggregations, which is cool.

234
00:11:01,093 --> 00:11:03,562
But the coolest part is
that you can nest them.

235
00:11:03,562 --> 00:11:05,698
You can nest different
types of aggregations

236
00:11:05,698 --> 00:11:09,367
to open a really nice
set of data analytics.

237
00:11:11,337 --> 00:11:14,840
So, how do we build
these aggregations?

238
00:11:14,840 --> 00:11:16,375
First you create the query.

239
00:11:16,375 --> 00:11:18,244
Brian already told you how to,

240
00:11:18,244 --> 00:11:19,845
you just import the Q object

241
00:11:19,845 --> 00:11:22,815
and we here have
all the MFT entries.

242
00:11:22,815 --> 00:11:25,851
And then you create your
aggregation, the top level one.

243
00:11:25,851 --> 00:11:29,188
You import the aggregation
object from the Elasticsearch

244
00:11:29,188 --> 00:11:33,759
wonderful DSL library and here
we have a terms aggregations

245
00:11:33,759 --> 00:11:37,296
on the extension field
of the MFT entries.

246
00:11:37,296 --> 00:11:41,467
And then we add both of
them to the search object.

247
00:11:42,968 --> 00:11:45,905
So here's how you add the
query and the aggregation

248
00:11:45,905 --> 00:11:48,207
to the search, but when
you add the aggregation

249
00:11:48,207 --> 00:11:51,277
you actually have to specify
the name that you are giving

250
00:11:51,277 --> 00:11:53,045
to that top level aggregation.

251
00:11:53,045 --> 00:11:56,382
Here the one that makes the
most sense is extensions,

252
00:11:56,382 --> 00:11:59,151
since we are filtering
on the field extension.

253
00:11:59,151 --> 00:12:02,555
And finally, you just
execute your search,

254
00:12:03,989 --> 00:12:06,325
and you obtain the result
back from Elasticsearch.

255
00:12:06,325 --> 00:12:08,360
So, what we can
do with all this,

256
00:12:08,360 --> 00:12:10,596
you can ask all sorts
of forensic questions.

257
00:12:10,596 --> 00:12:12,565
Maybe you can ask, how
many different files

258
00:12:12,565 --> 00:12:15,000
are on the file system
for file extension,

259
00:12:15,000 --> 00:12:18,404
and to access that
result, that information,

260
00:12:18,404 --> 00:12:21,640
you just do a simple iteration
of the result object.

261
00:12:21,640 --> 00:12:23,509
Take into account that
you have to put there

262
00:12:23,509 --> 00:12:26,278
the name of the top
level aggregation,

263
00:12:26,278 --> 00:12:27,813
and then you can just
print the buckets

264
00:12:27,813 --> 00:12:29,782
and the count
inside each bucket.

265
00:12:29,782 --> 00:12:31,717
This is how I see it
working in my mind.

266
00:12:31,717 --> 00:12:34,920
You have all the MFT entries,
you find one MFT entry

267
00:12:34,920 --> 00:12:38,524
which associates to a
file with extension htm,

268
00:12:38,524 --> 00:12:40,493
you put it in that bucket.

269
00:12:40,493 --> 00:12:41,794
You find another one,
you put it in the bucket.

270
00:12:41,794 --> 00:12:43,395
You find another one, and so on.

271
00:12:43,395 --> 00:12:46,598
And you end up having all
the different file extensions

272
00:12:46,599 --> 00:12:49,001
with the counts
associated with it.

273
00:12:49,001 --> 00:12:51,370
But you can have even
more forensic value to it.

274
00:12:51,370 --> 00:12:54,006
You can nest these
aggregations and answer

275
00:12:54,006 --> 00:12:56,408
the next forensic
question: how many files

276
00:12:56,408 --> 00:13:00,346
are created on a daily basis
on the per file extension?

277
00:13:00,346 --> 00:13:03,682
So here we have the same
thing, all the MFT entries,

278
00:13:03,682 --> 00:13:06,385
and now we're gonna
use a date histogram

279
00:13:06,385 --> 00:13:10,189
to group the data entries
in Elasticsearch per day.

280
00:13:10,189 --> 00:13:12,591
And we add a child bucket,
and that's the same one

281
00:13:12,591 --> 00:13:14,927
we used before but now it's
no longer the parent one,

282
00:13:14,927 --> 00:13:18,197
it's the child, and it's
the fields on extensions.

283
00:13:18,197 --> 00:13:20,699
And we add our
query to the search,

284
00:13:20,699 --> 00:13:22,468
we add our aggregation
to the search,

285
00:13:22,468 --> 00:13:25,237
and now we add the name
for the parent one;

286
00:13:25,237 --> 00:13:26,671
It stays MFT.

287
00:13:26,672 --> 00:13:28,374
You only need that one.

288
00:13:28,374 --> 00:13:29,809
And when you execute the search,

289
00:13:29,809 --> 00:13:31,577
take back the result, and bam,

290
00:13:31,577 --> 00:13:35,614
all the files created per
day per file extension.

291
00:13:35,614 --> 00:13:39,418
You're probably wondering
what happened on January 1st.

292
00:13:39,418 --> 00:13:41,954
That's actually from a real
case that we were working on,

293
00:13:41,954 --> 00:13:45,391
and we had this coworker
working on this image

294
00:13:45,391 --> 00:13:49,862
and we saw this result,
all this 81, 82, 83 files

295
00:13:49,862 --> 00:13:51,896
being created on January 1st.

296
00:13:51,897 --> 00:13:53,766
What is that?

297
00:13:53,766 --> 00:13:55,434
Were you busy on January 1st?

298
00:13:55,434 --> 00:13:56,968
That's what we
asked our coworker,

299
00:13:56,969 --> 00:13:59,471
and it was like, no, that was
January 1st; it's a holiday.

300
00:13:59,471 --> 00:14:02,341
And, well, someone
apparently created an image

301
00:14:02,341 --> 00:14:06,045
on the system, on the
custodian's system.

302
00:14:06,045 --> 00:14:08,646
That's pretty interesting,
and he told us

303
00:14:08,647 --> 00:14:13,118
that another company came
to do forensics acquisitions

304
00:14:13,118 --> 00:14:15,421
of all the images that
were involved in the case,

305
00:14:15,421 --> 00:14:16,822
and apparently
they were not using

306
00:14:16,822 --> 00:14:19,558
any external device
to store the images,

307
00:14:19,558 --> 00:14:21,360
which was pretty mindblowing.

308
00:14:21,360 --> 00:14:24,063
- [Brian] Thanks for
overriding slack space for us,

309
00:14:24,063 --> 00:14:26,098
- Taking all the
evidence out, whatever.

310
00:14:26,098 --> 00:14:26,931
- [Brian] We appreciate it.

311
00:14:26,932 --> 00:14:28,567
- (laughs)

312
00:14:28,567 --> 00:14:30,436
So, let's move on,
let's go farther

313
00:14:30,436 --> 00:14:32,905
and answer another question.

314
00:14:32,905 --> 00:14:35,241
How many users logged
into the system?

315
00:14:35,241 --> 00:14:38,143
Which user accounts?
How they did login.

316
00:14:38,143 --> 00:14:41,747
So again, we move on
from the MFT records,

317
00:14:41,747 --> 00:14:43,415
we go to the Windows event logs,

318
00:14:43,415 --> 00:14:46,517
especially the 4624,
the login events.

319
00:14:46,518 --> 00:14:48,520
So we have all our
security events,

320
00:14:48,520 --> 00:14:51,523
and then we have
our date histogram

321
00:14:51,523 --> 00:14:55,027
with the days on which
those events occurred.

322
00:14:55,027 --> 00:14:57,029
We group them on usernames,

323
00:14:57,029 --> 00:14:59,064
and each username
is gonna log in

324
00:14:59,064 --> 00:15:00,966
using a different log on type.

325
00:15:00,966 --> 00:15:03,636
So this is how all the
data is gonna be bucketed

326
00:15:03,636 --> 00:15:05,204
in our aggregation.

327
00:15:06,538 --> 00:15:08,073
Again, we add our
aggregation to our search

328
00:15:08,073 --> 00:15:11,776
using the parent name
aggregation, execute it,

329
00:15:11,777 --> 00:15:13,579
and we have all the results.

330
00:15:13,579 --> 00:15:17,382
All the usernames that log
on to the machine per day,

331
00:15:17,383 --> 00:15:19,585
and which log on type they use;

332
00:15:19,585 --> 00:15:21,487
if they use different
ones it will show up there

333
00:15:21,487 --> 00:15:24,156
like on the 31st of March, 2004.

334
00:15:26,091 --> 00:15:28,894
Here is a really good equivalent

335
00:15:28,894 --> 00:15:33,299
between these aggregations on
Elasticsearch DSL and Kibana,

336
00:15:33,299 --> 00:15:35,867
because what it actually
is doing is the same thing:

337
00:15:35,868 --> 00:15:37,970
it's an aggregation per day.

338
00:15:37,970 --> 00:15:40,005
You see the time
stamp on the x axis,

339
00:15:40,005 --> 00:15:42,675
and then you split the
area into target usernames

340
00:15:42,675 --> 00:15:45,743
or target username use
and our log on types.

341
00:15:45,744 --> 00:15:47,246
So the different colors
that you see here

342
00:15:47,246 --> 00:15:49,615
are the different log
on types per username,

343
00:15:49,615 --> 00:15:51,417
and each chart is a
different username

344
00:15:51,417 --> 00:15:53,452
that was logging
into that machine.

345
00:15:53,452 --> 00:15:55,988
And here you can play with
the dates and everything.

346
00:15:55,988 --> 00:15:58,290
So really, we have a
lot of information,

347
00:15:58,290 --> 00:16:01,360
a lot of tools here, but
what does this all mean?

348
00:16:01,360 --> 00:16:05,230
- So, applying the Elasticsearch
DSL to what we're doing

349
00:16:05,230 --> 00:16:08,434
in the lab, we're able
to manage all of our

350
00:16:08,434 --> 00:16:10,935
forensic inquiries into groups

351
00:16:10,936 --> 00:16:15,074
based on our common tasks
like insider threat,

352
00:16:15,074 --> 00:16:19,243
incident response, and just
general forensic questions,

353
00:16:19,244 --> 00:16:22,281
and that led to a more
consistent analysis

354
00:16:22,281 --> 00:16:24,616
across all of our
forensic investigators,

355
00:16:24,616 --> 00:16:27,086
them having this
information available,

356
00:16:27,086 --> 00:16:29,188
as well as a more
rigorous analysis.

357
00:16:29,188 --> 00:16:30,689
I'll give you an example.

358
00:16:30,689 --> 00:16:33,858
You may look for user
activity in the event logs,

359
00:16:33,859 --> 00:16:36,028
but are you checking
for if the event logs

360
00:16:36,028 --> 00:16:37,963
are cleared every time?

361
00:16:37,963 --> 00:16:41,900
Do you know where to check
for that for an EVTX?

362
00:16:41,900 --> 00:16:44,002
Do you know where for EVT?

363
00:16:44,003 --> 00:16:46,772
You know, we don't
memorize every single thing

364
00:16:46,772 --> 00:16:48,374
we need to check
every single time,

365
00:16:48,374 --> 00:16:50,609
so have this all
down and available

366
00:16:50,609 --> 00:16:53,812
for the investigator to search.

367
00:16:53,812 --> 00:16:55,514
This is also extensible.

368
00:16:55,514 --> 00:16:58,017
Since we have these queries,
if a new artifact comes out

369
00:16:58,017 --> 00:17:01,253
for a new operating system,
or new research comes out,

370
00:17:01,253 --> 00:17:05,124
we just create another
query for that artifact,

371
00:17:05,124 --> 00:17:07,059
add it on to our
list of queries,

372
00:17:07,059 --> 00:17:09,762
and just keep building
on top of that.

373
00:17:09,762 --> 00:17:11,797
And since it's built
on Elasticsearch,

374
00:17:11,797 --> 00:17:15,100
it's scalable, we can
ingest many indices

375
00:17:15,099 --> 00:17:19,370
and then search across them
for big investigations,

376
00:17:20,739 --> 00:17:24,343
and also it's extremely
fast, so we get these answers

377
00:17:24,343 --> 00:17:26,178
in a very timely basis.

378
00:17:26,178 --> 00:17:29,681
So, thank you and are
there any questions?

379
00:17:32,251 --> 00:17:33,685
- [Audience Member] That
was definitely awesome,

380
00:17:33,685 --> 00:17:37,255
especially visualizing
how events are happening

381
00:17:37,256 --> 00:17:39,358
on a network or on a
system, and so you know,

382
00:17:39,358 --> 00:17:41,927
you were mentioning you
can input multiple indices,

383
00:17:41,927 --> 00:17:44,463
you can combine
multiple systems?

384
00:17:44,463 --> 00:17:47,466
You get all thirty systems
involved in an investigation

385
00:17:47,466 --> 00:17:49,101
and run a query and say,
"Show me all the times,

386
00:17:49,101 --> 00:17:52,471
you know, suspicious log on,
or a suspicious user logged on

387
00:17:52,471 --> 00:17:54,940
across all the systems,"
and kind of visualize

388
00:17:54,940 --> 00:17:56,308
a timeline like that,
is that something

389
00:17:56,308 --> 00:17:57,609
that you're able to do?

390
00:17:57,609 --> 00:17:59,144
- Yeah, so I should
clarify that, so yeah,

391
00:17:59,144 --> 00:18:03,515
you can at the index
level have that machine,

392
00:18:03,515 --> 00:18:06,685
and then there's an alias
functionality in Elasticsearch

393
00:18:06,685 --> 00:18:10,054
which is basically just a
pointer, so you can point to

394
00:18:10,055 --> 00:18:13,125
five, ten, fifteen,
whatever machines,

395
00:18:13,125 --> 00:18:16,161
search on that one, and
with Elasticsearch DSL

396
00:18:16,161 --> 00:18:18,697
the results that are returned,

397
00:18:18,697 --> 00:18:23,534
you can access the actual index
within the metadata of it,

398
00:18:24,837 --> 00:18:28,574
so, you are able to
determine individual machines

399
00:18:28,574 --> 00:18:30,075
from that timeline.

400
00:18:30,075 --> 00:18:33,345
- [Audience Member]
Awesome, thank you.

401
00:18:33,345 --> 00:18:35,747
- [Phil] Any other questions?

402
00:18:37,049 --> 00:18:38,617
Everybody's probably
downloading and getting ready

403
00:18:38,617 --> 00:18:40,252
to install Elasticsearch,
is probably

404
00:18:40,252 --> 00:18:42,120
what's really
happening right now.

405
00:18:42,121 --> 00:18:44,289
Well, excellent, well
thanks very much,

406
00:18:44,289 --> 00:18:45,591
very good talk.

407
00:18:45,591 --> 00:18:46,958
(audience applause)

408
00:18:46,959 --> 00:18:50,629
(tones chime and intensify)

