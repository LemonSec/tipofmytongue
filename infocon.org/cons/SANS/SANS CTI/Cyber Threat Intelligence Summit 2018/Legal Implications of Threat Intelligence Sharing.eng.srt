1
00:00:00,600 --> 00:00:03,036
(intense music)

2
00:00:10,043 --> 00:00:13,046
(audience applauding)

3
00:00:13,046 --> 00:00:14,681
- This is my first time at SANS,

4
00:00:14,681 --> 00:00:16,716
and I'm really excited to
be here, I have to admit

5
00:00:16,716 --> 00:00:20,387
I'm a little bit
intimidated by this crowd,

6
00:00:21,521 --> 00:00:22,922
that doesn't happen
to me too often.

7
00:00:22,922 --> 00:00:24,657
And of course I have to follow

8
00:00:24,657 --> 00:00:26,960
the most interesting man
in information security,

9
00:00:26,960 --> 00:00:28,428
which is...

10
00:00:28,428 --> 00:00:31,798
I gotta fire my agent
when I get back.

11
00:00:31,798 --> 00:00:34,900
Anyway, I spend my time, I
do a fair amount of speaking,

12
00:00:34,901 --> 00:00:37,737
I'll talk about what my
day job is in a minute,

13
00:00:37,737 --> 00:00:38,972
but I do a fair
amount of speaking,

14
00:00:38,972 --> 00:00:40,673
I speak at two types
of conferences.

15
00:00:40,673 --> 00:00:45,578
I speak at legal conferences,
where I try to help lawyers

16
00:00:45,578 --> 00:00:48,448
understand technology risks and

17
00:00:48,448 --> 00:00:51,584
how legal decisions
can have an impact on

18
00:00:51,584 --> 00:00:53,353
technology and how
cyber security,

19
00:00:53,353 --> 00:00:55,255
how lawyers need to be involved

20
00:00:55,255 --> 00:00:57,057
in assessing cyber
security risks,

21
00:00:57,057 --> 00:00:59,726
and then I speak at techno
conferences like this one,

22
00:00:59,726 --> 00:01:02,095
and yes, try to
help technologists

23
00:01:02,095 --> 00:01:04,431
and cyber security professionals
from breaking the law.

24
00:01:04,431 --> 00:01:08,401
Which actually can be
complicated from time to time.

25
00:01:08,401 --> 00:01:11,237
But really, more often,
and this is really

26
00:01:11,237 --> 00:01:14,274
my objective today, is
to get you to think about

27
00:01:15,175 --> 00:01:17,509
how the decisions that you make

28
00:01:17,510 --> 00:01:20,280
can potentially
create legal liability

29
00:01:20,280 --> 00:01:22,382
that you weren't
otherwise aware of,

30
00:01:22,382 --> 00:01:25,852
so that's really what I'm
gonna talk about here today.

31
00:01:27,253 --> 00:01:28,922
The other thing I'll point
out before I get goin'

32
00:01:28,922 --> 00:01:32,425
here is because I do
these two different kinds

33
00:01:32,425 --> 00:01:34,961
of presentations, has
anyone, first of all,

34
00:01:34,961 --> 00:01:36,629
is there any
lawyers in the room?

35
00:01:38,431 --> 00:01:42,000
All right, good, good, good.
(audience laughs)

36
00:01:42,001 --> 00:01:44,471
So if anyone, so you
probly have not been

37
00:01:44,471 --> 00:01:45,972
to a legal conference,

38
00:01:47,307 --> 00:01:50,076
or seen people who ordinarily
speak at these legal

39
00:01:50,076 --> 00:01:54,012
conferences, but lawyers love
to receive input in text form.

40
00:01:54,013 --> 00:01:57,717
So it's slide after slide
of dense bullet points.

41
00:01:57,717 --> 00:01:59,819
And obviously, technologists
do not like that,

42
00:01:59,819 --> 00:02:01,254
they like cartoons,
ya know, again,

43
00:02:01,254 --> 00:02:03,056
great that I get to
follow a guy that created

44
00:02:03,056 --> 00:02:05,991
his own cartoons for
his presentation.

45
00:02:05,992 --> 00:02:07,994
So a lot of times I
find myself in between,

46
00:02:07,994 --> 00:02:10,196
in fact, when I first
sent my deck to Rebecca,

47
00:02:10,196 --> 00:02:12,832
who helped me, she bounced
it back immediately, like,

48
00:02:12,832 --> 00:02:14,501
too many bullets, ya
know, what the heck,

49
00:02:14,501 --> 00:02:15,768
this crowd isn't
gonna go for that.

50
00:02:15,768 --> 00:02:17,170
So I tried to find
this middle ground,

51
00:02:17,170 --> 00:02:20,273
I did put some
pictures in, but go

52
00:02:20,273 --> 00:02:23,409
a little bit easy on
me in that regard.

53
00:02:23,409 --> 00:02:25,411
So another point I wanna
make before I jump in is

54
00:02:25,411 --> 00:02:28,748
that a lot of times,
or most of the time

55
00:02:28,748 --> 00:02:30,517
when I get up and
speak, I am very,

56
00:02:30,517 --> 00:02:33,453
very confident in
the voracity of,

57
00:02:34,687 --> 00:02:37,223
in sort of the strength
of the position

58
00:02:37,223 --> 00:02:39,993
that I'm about to take
in giving my talk.

59
00:02:39,993 --> 00:02:42,162
This is not one of
those situations.

60
00:02:43,329 --> 00:02:46,633
I actually think I may
not be 100% correct

61
00:02:46,633 --> 00:02:49,235
in the fears that I have
around information sharing.

62
00:02:51,504 --> 00:02:54,140
So the ideal outcome
of this talk is that

63
00:02:54,140 --> 00:02:57,277
yes, I make you guys
a little bit more wary

64
00:02:57,277 --> 00:03:00,146
about sharing, and
more aware of the risk,

65
00:03:00,146 --> 00:03:02,682
but also hoping, and I'll
try to get to the Q&A

66
00:03:02,682 --> 00:03:04,583
so we have some time
to get into discussion,

67
00:03:04,584 --> 00:03:05,818
I hope you can make
me feel a little

68
00:03:05,818 --> 00:03:08,488
more comfortable with
it, because I think

69
00:03:08,488 --> 00:03:11,558
I may have taken it
too far in my concern.

70
00:03:11,558 --> 00:03:14,727
But the point is, there's a lot
of uncertainty in this area.

71
00:03:14,727 --> 00:03:17,864
And I really want everyone
here to think about

72
00:03:17,864 --> 00:03:19,666
the potential
consequences of engaging

73
00:03:19,666 --> 00:03:22,234
in sharing threat intelligence.

74
00:03:23,570 --> 00:03:27,106
So this is really, this
talk is an expansion

75
00:03:27,106 --> 00:03:29,742
of sort of a cocktail
party rant that I developed

76
00:03:31,010 --> 00:03:33,780
probly over a year ago, ya know,

77
00:03:33,780 --> 00:03:35,548
when I used to talk
about how people

78
00:03:35,548 --> 00:03:37,784
are sharing willy nilly,
and there's the pressure

79
00:03:37,784 --> 00:03:40,887
coming from the government
to share when really it's,

80
00:03:40,887 --> 00:03:43,656
it's the government
just trying to get us

81
00:03:43,656 --> 00:03:46,226
to provide them with
indicators, there's really not

82
00:03:46,226 --> 00:03:48,761
that much of an intent
to share back to us,

83
00:03:48,761 --> 00:03:52,732
so I tried to expand
that into this talk.

84
00:03:52,732 --> 00:03:55,235
So let me talk
about, a little bit

85
00:03:55,235 --> 00:03:57,370
more about my background here.

86
00:03:58,605 --> 00:04:01,573
So my background,
I am an attorney,

87
00:04:01,574 --> 00:04:04,444
I serve as the Chief Privacy
Officer for my company,

88
00:04:04,444 --> 00:04:08,147
UnitedLex, so that's the
quasi-legal role that I have,

89
00:04:08,147 --> 00:04:10,416
and then I also
run the company's

90
00:04:10,416 --> 00:04:12,484
Cyber Risk Solutions practice.

91
00:04:12,485 --> 00:04:14,654
And so the bulk of my career

92
00:04:14,654 --> 00:04:16,889
since I left, officially,
the practice of law

93
00:04:16,889 --> 00:04:19,492
back in 2002, has
been working at

94
00:04:19,492 --> 00:04:22,962
the intersection of
law and technology.

95
00:04:22,962 --> 00:04:25,565
So I started my
career as a litigator,

96
00:04:27,033 --> 00:04:30,069
I got into the area
of privacy law,

97
00:04:30,069 --> 00:04:32,305
again, spent a lot of my time

98
00:04:32,305 --> 00:04:34,173
working in the
cybersecurity field,

99
00:04:34,173 --> 00:04:37,744
running forensics practices

100
00:04:37,744 --> 00:04:40,513
for the better part
of the last 15 years,

101
00:04:40,513 --> 00:04:44,616
and really, have had
a front row seat to

102
00:04:44,617 --> 00:04:47,920
dozens and dozens of
incidents and breaches

103
00:04:47,920 --> 00:04:49,522
that have occurred
over the years,

104
00:04:49,522 --> 00:04:51,623
which has made me a
world-class paranoiac.

105
00:04:53,226 --> 00:04:56,162
When people ask me, well, how
did you make the transition

106
00:04:56,162 --> 00:04:58,563
from being a lawyer into
the field of cybersecurity,

107
00:04:58,564 --> 00:05:01,301
information security,
I always point out,

108
00:05:01,301 --> 00:05:03,436
it's really not
that far of a leap,

109
00:05:03,436 --> 00:05:06,306
when you think about the
fundamentals of what,

110
00:05:06,306 --> 00:05:08,107
what a lawyer does and what
an information security

111
00:05:08,107 --> 00:05:10,843
professional does, we
all are paid to assume

112
00:05:10,843 --> 00:05:12,812
that calamity is right
around the corner, right,

113
00:05:12,812 --> 00:05:15,081
that you're gonna
step into the street

114
00:05:15,081 --> 00:05:16,983
and get hit by a bus, and okay,

115
00:05:16,983 --> 00:05:18,918
what do we need to do to
prevent that from happening.

116
00:05:18,918 --> 00:05:21,387
So the mindset is
actually very similar,

117
00:05:21,387 --> 00:05:26,125
we just have to learn new
terminology to a large extent.

118
00:05:28,795 --> 00:05:31,898
And also like
lawyers, information
security professionals

119
00:05:31,898 --> 00:05:34,967
tend to be there, they have
front row seats, when the stuff

120
00:05:34,967 --> 00:05:37,669
hits the fan, and you
sort of learn (laughs)

121
00:05:37,670 --> 00:05:41,207
negative reinforcement,
which is to a large extent

122
00:05:41,207 --> 00:05:44,377
the same thing that
happens with lawyers.

123
00:05:44,377 --> 00:05:47,647
I think another thing that
you need to know about me

124
00:05:47,647 --> 00:05:49,949
and where I'm coming
from on this is,

125
00:05:49,949 --> 00:05:52,218
I actually am not
much of a sharer.

126
00:05:53,853 --> 00:05:57,223
I am not on Face, not
only am I not on Facebook,

127
00:05:57,223 --> 00:06:00,626
I have trouble understanding
why anyone is on Facebook.

128
00:06:01,961 --> 00:06:05,331
And I'm not not on Facebook
because I'm worried

129
00:06:05,331 --> 00:06:07,834
that I'm gonna share something
that I don't wanna share,

130
00:06:07,834 --> 00:06:09,068
I have complete
control over that.

131
00:06:09,068 --> 00:06:10,770
You know why I'm
not on Facebook?

132
00:06:10,770 --> 00:06:12,003
'Cause I don't wanna
know what the rest of you

133
00:06:12,004 --> 00:06:13,673
wanna share with me.
(audience laughs)

134
00:06:13,673 --> 00:06:15,074
All right, so I just
don't understand this

135
00:06:15,074 --> 00:06:16,676
compulsion to share,
so I think that colors

136
00:06:16,676 --> 00:06:21,080
my attitude on
this topic as well.

137
00:06:21,080 --> 00:06:23,549
I'm here to talk about sharing,

138
00:06:25,818 --> 00:06:27,986
sharing, I will point
out at the outset here,

139
00:06:27,987 --> 00:06:30,790
does not come naturally
to human beings, right?

140
00:06:30,790 --> 00:06:33,993
Even when we're kids, we
have to be taught to share.

141
00:06:33,993 --> 00:06:36,429
I know with my own kids, it's
some of the first lessons

142
00:06:36,429 --> 00:06:38,931
you teach them, how
it's important to share,

143
00:06:38,931 --> 00:06:41,768
and it's nice to share,
and I certainly can see

144
00:06:41,768 --> 00:06:44,703
there are many, many benefits
to sharing in the world.

145
00:06:44,704 --> 00:06:48,207
To form a relatively
functional society,

146
00:06:48,207 --> 00:06:50,810
we can build alliances,
we can build trust

147
00:06:50,810 --> 00:06:54,814
with people through sharing,
so I totally get that.

148
00:06:54,814 --> 00:06:56,349
But I think we do
need to keep in mind

149
00:06:56,349 --> 00:06:58,584
that there could be a reason,
an evolutionary reason,

150
00:06:58,584 --> 00:07:00,486
why it doesn't come
naturally to us. (chuckles)

151
00:07:00,486 --> 00:07:03,055
All right, so we should be
wary, we should be careful

152
00:07:03,055 --> 00:07:05,792
about the circumstances
in which we share.

153
00:07:05,792 --> 00:07:08,628
So sharing is most
effective, in my opinion,

154
00:07:08,628 --> 00:07:10,897
when, especially when we're
talking about intelligence,

155
00:07:10,897 --> 00:07:13,666
when the two parties
sharing intelligence

156
00:07:13,666 --> 00:07:18,671
have relatively equal ability
to act on that intelligence.

157
00:07:19,806 --> 00:07:22,442
If you look at two
nation states sharing

158
00:07:22,442 --> 00:07:25,044
threat intelligence, or look
at the Five Eyes, right,

159
00:07:25,044 --> 00:07:28,714
the reason that that is
effective, for the most part,

160
00:07:28,714 --> 00:07:30,450
is that you know
that when you share

161
00:07:30,450 --> 00:07:33,753
a piece of intelligence
with that other party,

162
00:07:33,753 --> 00:07:35,188
they're gonna be able
to make some use of it,

163
00:07:35,188 --> 00:07:36,389
they're gonna
understand what it is,

164
00:07:36,389 --> 00:07:37,457
they're gonna be
able to analyze it,

165
00:07:37,457 --> 00:07:38,991
they're gonna be able to put it

166
00:07:38,991 --> 00:07:42,829
in the proper context for
themselves, and act on it.

167
00:07:42,829 --> 00:07:45,431
When you get a situation
where the parties

168
00:07:45,431 --> 00:07:47,800
sharing intelligence
with each other are not,

169
00:07:47,800 --> 00:07:50,603
do not have equal capability
to act on an intelligence,

170
00:07:50,603 --> 00:07:53,206
you can get some weird
things happening.

171
00:07:53,206 --> 00:07:55,908
Number one, you could just have
sort of ineffective sharing,

172
00:07:55,908 --> 00:07:57,743
that one of the
parties who's receiving

173
00:07:57,743 --> 00:07:59,312
threat intelligence
doesn't really know

174
00:07:59,312 --> 00:08:01,113
what to do with it, they
don't really understand

175
00:08:01,113 --> 00:08:03,749
what it is, they need
help with the context.

176
00:08:06,219 --> 00:08:08,320
And the other thing
is that the parties

177
00:08:08,321 --> 00:08:09,889
who have the greater capability

178
00:08:09,889 --> 00:08:13,459
tend to benefit the most
from that threat sharing.

179
00:08:16,696 --> 00:08:18,898
I'm not gonna talk, it's
hard to do this talk

180
00:08:18,898 --> 00:08:20,766
without talking at all about,

181
00:08:22,168 --> 00:08:24,871
about Cissa, or Ceesa, I'm
not sure how to pronounce it,

182
00:08:24,871 --> 00:08:27,340
Cybersecurity
Information Sharing Act.

183
00:08:27,340 --> 00:08:31,911
Everyone's familiar with this
act, I assume, in this room,

184
00:08:31,911 --> 00:08:34,080
if not, I can talk
to you afterward.

185
00:08:34,080 --> 00:08:37,350
How many of you participate
in information sharing

186
00:08:38,784 --> 00:08:42,487
under CISA, either through an
ISAC or directly in the AIS.

187
00:08:44,123 --> 00:08:46,492
Okay, that's actually
a smaller percentage

188
00:08:46,492 --> 00:08:49,395
than I thought of this crowd.

189
00:08:49,395 --> 00:08:50,997
How many of you
participate in other,

190
00:08:50,997 --> 00:08:55,200
more informal sharing
with other organizations.

191
00:08:55,201 --> 00:08:57,703
Okay, so I think between
those two, we're talkin' about

192
00:08:57,703 --> 00:09:00,973
the majority of the people
engage in some sort of

193
00:09:02,942 --> 00:09:03,943
information sharing.

194
00:09:06,245 --> 00:09:09,649
I wanna, what I
wanna focus on today

195
00:09:09,649 --> 00:09:11,884
is the ways that
you can actually

196
00:09:11,884 --> 00:09:15,555
be punished for
sharing information,

197
00:09:15,555 --> 00:09:18,156
that this information
can be used against you.

198
00:09:19,258 --> 00:09:20,893
There are three primary areas

199
00:09:20,893 --> 00:09:22,094
where you can get into trouble.

200
00:09:22,094 --> 00:09:24,230
One, you can share
too much information,

201
00:09:24,230 --> 00:09:26,699
and/or share that
information too broadly,

202
00:09:26,699 --> 00:09:28,867
with too wide of a group.

203
00:09:28,868 --> 00:09:32,038
Number two, you can fail to act

204
00:09:32,038 --> 00:09:34,707
on information that
is shared with you.

205
00:09:34,707 --> 00:09:38,778
And number three, in
devoting time and resources

206
00:09:38,778 --> 00:09:42,682
to information sharing,
you can neglect

207
00:09:42,682 --> 00:09:46,152
to pay attention to
security priorities,

208
00:09:46,152 --> 00:09:48,020
you know, that frankly
may be more effective

209
00:09:48,020 --> 00:09:49,555
if you put more time into them.

210
00:09:50,923 --> 00:09:54,126
So I would suggest
that those three areas

211
00:09:54,126 --> 00:09:56,529
can create legal risk
for your organization.

212
00:09:58,664 --> 00:10:01,033
I assume most of you are
familiar with the concept

213
00:10:01,033 --> 00:10:03,936
of negligence, the legal
concept of negligence.

214
00:10:03,936 --> 00:10:06,939
That you failed to do
what a reasonable person,

215
00:10:06,939 --> 00:10:10,175
or reasonable company,
given the same set of facts

216
00:10:10,176 --> 00:10:11,744
and the same situation,
would have done.

217
00:10:11,744 --> 00:10:13,579
If you've failed to
meet that standard,

218
00:10:13,579 --> 00:10:15,881
of sort of the
reasonable person,

219
00:10:15,881 --> 00:10:19,285
then you could be held
liable for your failure.

220
00:10:19,285 --> 00:10:22,488
And so that's what a lot of
civil litigation is based on,

221
00:10:22,488 --> 00:10:25,091
and frankly, what a lot of
regulatory actions are based on

222
00:10:25,091 --> 00:10:28,027
as well in this area,
is this question of,

223
00:10:28,027 --> 00:10:29,528
did you act appropriately,

224
00:10:29,528 --> 00:10:32,932
did you fail to
meet this standard.

225
00:10:32,932 --> 00:10:35,768
And I think information sharing,
for these three reasons,

226
00:10:35,768 --> 00:10:38,971
you can get into trouble
and create liability.

227
00:10:40,473 --> 00:10:43,008
Bottom line up front here,
and then I'll get into some,

228
00:10:43,009 --> 00:10:47,013
a couple of examples
and scenarios,

229
00:10:47,013 --> 00:10:48,547
so threat intelligence sharing

230
00:10:48,547 --> 00:10:50,249
can be very beneficial,
so I don't wanna come off

231
00:10:50,249 --> 00:10:52,551
as someone who's against
sharing altogether,

232
00:10:52,551 --> 00:10:55,554
but you've gotta think
before you share,

233
00:10:55,554 --> 00:10:58,156
you may be inadvertently
creating liability and risks.

234
00:10:58,157 --> 00:11:00,292
You've gotta think
about how, each time

235
00:11:00,292 --> 00:11:03,929
you share, how that
could be happening.

236
00:11:03,929 --> 00:11:06,098
Second, you've gotta have
a plan kfor how you intend

237
00:11:06,098 --> 00:11:08,633
to consume and orchestrate
threat intelligence

238
00:11:08,634 --> 00:11:11,604
that you're receiving, so
there's a risk in sharing,

239
00:11:11,604 --> 00:11:14,006
which tends to be more
obvious, there's also a risk

240
00:11:14,006 --> 00:11:15,641
just in consuming and receiving.

241
00:11:15,641 --> 00:11:17,810
I know I've talked
to a lot of companies

242
00:11:17,810 --> 00:11:19,045
that are participating
in an ISAC,

243
00:11:19,045 --> 00:11:22,448
or one of these sort
of industry-based

244
00:11:22,448 --> 00:11:24,516
information sharing
communities, and they say,

245
00:11:24,517 --> 00:11:26,419
oh, no risk for me,
I'm only consuming,

246
00:11:26,419 --> 00:11:28,319
I'm not sharing
anything outbound.

247
00:11:28,320 --> 00:11:29,822
What I'm gonna talk about
today is that doesn't

248
00:11:29,822 --> 00:11:31,757
necessarily eliminate your risk.

249
00:11:31,757 --> 00:11:34,859
You gotta have a plan
in place that allows you

250
00:11:34,860 --> 00:11:37,163
to orchestrate a response
to that intelligence.

251
00:11:37,163 --> 00:11:39,098
And then finally, this
is obvious to most of you

252
00:11:39,098 --> 00:11:41,199
who are in this profession,
the most valuable

253
00:11:41,200 --> 00:11:43,669
threat intelligence originates
from your own environment

254
00:11:43,669 --> 00:11:45,771
and frankly, stays
there, you don't need

255
00:11:45,771 --> 00:11:48,207
to share it for it to
be valuable to you,

256
00:11:48,207 --> 00:11:50,443
and frankly, a lot
of the intelligence

257
00:11:50,443 --> 00:11:52,044
may only be valuable to you.

258
00:11:53,479 --> 00:11:57,216
So what is CISA, I only am
gonna spend a minute on this,

259
00:11:57,216 --> 00:11:59,852
the Cybersecurity
Information Sharing Act,

260
00:11:59,852 --> 00:12:01,487
either the key to
combating hackers,

261
00:12:01,487 --> 00:12:03,189
or a dire threat to
Americans' privacy,

262
00:12:03,189 --> 00:12:05,824
just depends on who
you ask, I would have

263
00:12:05,825 --> 00:12:08,594
one more possibility
here, that it's just,

264
00:12:08,594 --> 00:12:12,064
could just be a colossal
waste of time and resources.

265
00:12:12,064 --> 00:12:15,834
Time will tell, I
certainly have my doubts,

266
00:12:15,835 --> 00:12:18,204
so what does CISA
do, real quickly,

267
00:12:18,204 --> 00:12:20,271
the first piece of
cybersecurity legislation

268
00:12:20,272 --> 00:12:23,709
that congress was able
to actually pass in this

269
00:12:23,709 --> 00:12:28,447
sort of new age of ever-present
cyber threats to all of us.

270
00:12:28,447 --> 00:12:31,650
Creates a sort of voluntary,
quote unquote voluntary,

271
00:12:31,650 --> 00:12:33,285
peer pressure based system.

272
00:12:34,453 --> 00:12:37,256
I get a little hot
under the collar

273
00:12:37,256 --> 00:12:40,226
when I hear some of the
government representatives

274
00:12:40,226 --> 00:12:42,795
talk about information
sharing and sort of

275
00:12:42,795 --> 00:12:45,598
appealing to our patriotic
duty, and stuff like that,

276
00:12:45,598 --> 00:12:50,368
without talking about some of
the risks in these programs.

277
00:12:50,369 --> 00:12:55,341
One of the reasons it took
so long to enact CISA is that

278
00:12:58,310 --> 00:12:59,779
corporations were
not comfortable

279
00:12:59,779 --> 00:13:01,947
sharing information
without having some level

280
00:13:01,947 --> 00:13:04,083
of liability protection,
so CISA has that.

281
00:13:05,317 --> 00:13:06,752
To a point,

282
00:13:08,721 --> 00:13:10,222
and I won't get
into too much detail

283
00:13:10,222 --> 00:13:12,458
on this now, I'm happy to
talk about it afterward,

284
00:13:12,458 --> 00:13:14,293
but in order to receive
the benefit of this

285
00:13:14,293 --> 00:13:17,096
liability protection, you
have to share information

286
00:13:17,096 --> 00:13:20,733
according to the dictates
and requirements of CISA.

287
00:13:20,733 --> 00:13:24,435
One of those requirements
is, you must eliminate

288
00:13:24,436 --> 00:13:27,573
all personally identifiable
information, PII,

289
00:13:27,573 --> 00:13:30,608
from the information
that you're sharing.

290
00:13:30,609 --> 00:13:33,913
The entire burden for
doing that is on you.

291
00:13:33,913 --> 00:13:36,615
And if you fail to
do that, you get

292
00:13:36,615 --> 00:13:38,884
no liability
protection under CISA.

293
00:13:40,286 --> 00:13:42,154
CISA also protects the
information that you share

294
00:13:42,154 --> 00:13:46,625
from being used as the basis
for an enforcement action

295
00:13:46,625 --> 00:13:49,428
by a government, state
or local regulator.

296
00:13:49,428 --> 00:13:51,931
So it does sort of usurp
any other state laws

297
00:13:51,931 --> 00:13:56,068
that might, might be
inconsistent with CISA,

298
00:13:56,068 --> 00:13:59,138
CISA controls, and so
CISA says this information

299
00:13:59,138 --> 00:14:03,008
cannot be used as the basis
for an enforcement action.

300
00:14:03,008 --> 00:14:04,243
Now the reason I'm emphasizing

301
00:14:04,243 --> 00:14:06,045
as the basis for an
enforcement action

302
00:14:06,045 --> 00:14:10,015
is it is not at all clear, and
when something is not clear,

303
00:14:11,183 --> 00:14:13,819
we as lawyers assume
the worst, right,

304
00:14:13,819 --> 00:14:15,788
assume that it
will be interpreted

305
00:14:15,788 --> 00:14:18,524
in the light least
favorable to you.

306
00:14:18,524 --> 00:14:20,392
So it cannot be
used as the basis

307
00:14:20,392 --> 00:14:23,596
for an enforcement action,
but could it be used

308
00:14:23,596 --> 00:14:26,699
if an enforcement
action were initiated

309
00:14:26,699 --> 00:14:28,334
because of some other
piece of evidence,

310
00:14:28,334 --> 00:14:30,636
or some other driver,
could that information

311
00:14:30,636 --> 00:14:34,240
be used as a piece of
an enforcement action.

312
00:14:34,240 --> 00:14:37,076
I'm not convinced that
that couldn't be done.

313
00:14:37,076 --> 00:14:39,812
So think about the fact that
the information you share,

314
00:14:39,812 --> 00:14:42,348
what I would suggest to
you is you gotta assume

315
00:14:42,348 --> 00:14:45,150
that the information
you share under CISA

316
00:14:45,150 --> 00:14:47,920
or with one of these ISACs,
or really with anybody,

317
00:14:47,920 --> 00:14:49,188
can and will be
used against you,

318
00:14:49,188 --> 00:14:51,423
think Miranda
Rights here, right.

319
00:14:51,423 --> 00:14:52,625
Now, there are a
lot of situations

320
00:14:52,625 --> 00:14:54,760
where you can ask
yourself that question

321
00:14:54,760 --> 00:14:56,428
and still be
comfortable sharing,

322
00:14:56,428 --> 00:14:58,697
there are other situations,
and we'll talk about this,

323
00:14:58,697 --> 00:15:00,733
where you might
not be comfortable.

324
00:15:02,301 --> 00:15:04,603
So that's really what CISA does,

325
00:15:06,272 --> 00:15:10,209
I like to sort of boil it
down into layman's terms,

326
00:15:10,209 --> 00:15:12,077
give me your threat
intel, you know,

327
00:15:12,077 --> 00:15:15,981
only if you feel like it,
and we'll totally protect it

328
00:15:15,981 --> 00:15:18,851
and we probably won't share
it with other agencies,

329
00:15:18,851 --> 00:15:20,853
although they say that
they will share it

330
00:15:20,853 --> 00:15:23,789
if it will help them
prosecute a crime,

331
00:15:23,789 --> 00:15:26,492
anti-terrorism, these
kinds of things.

332
00:15:26,492 --> 00:15:28,993
And if there's stuff in there
that makes you look bad,

333
00:15:28,994 --> 00:15:32,865
we're not gonna use it
against you, probably,

334
00:15:32,865 --> 00:15:35,334
but someone else might, right.

335
00:15:35,334 --> 00:15:38,670
This is information that
could be used against you,

336
00:15:38,671 --> 00:15:40,839
certainly doesn't prevent
you from being pursued

337
00:15:40,839 --> 00:15:42,708
or have this information
used against you

338
00:15:42,708 --> 00:15:44,642
by a regulator outside
the United States,

339
00:15:44,643 --> 00:15:48,247
or by civil litigants.

340
00:15:48,247 --> 00:15:51,416
So what does CISA
not do, it does not

341
00:15:51,417 --> 00:15:54,153
protect you from
liability if you fail

342
00:15:54,153 --> 00:15:56,155
to share intelligence
the right way,

343
00:15:56,155 --> 00:15:59,257
it does not protect you in
the event of a data breach,

344
00:15:59,258 --> 00:16:01,427
we're gonna talk
more about that,

345
00:16:01,427 --> 00:16:02,795
and it does not
protect you against

346
00:16:02,795 --> 00:16:04,997
legal action outside the U.S.

347
00:16:06,365 --> 00:16:09,368
So it cannot protect you
against civil legal action, so

348
00:16:09,368 --> 00:16:11,170
the information that you share

349
00:16:11,170 --> 00:16:13,439
is protected from
a FOIA request,

350
00:16:13,439 --> 00:16:15,841
so I can't launch a
fishing expedition and say,

351
00:16:15,841 --> 00:16:18,043
hey, I wanna see all
the threat intelligence

352
00:16:18,043 --> 00:16:21,313
that Company A has shared
through their ISAC,

353
00:16:21,313 --> 00:16:24,216
or under CISA, and
use that as a way

354
00:16:24,216 --> 00:16:26,719
to try to come up
with a way to sue them

355
00:16:26,719 --> 00:16:28,120
for some sort of negligence.

356
00:16:29,455 --> 00:16:33,993
But, if I know that a
company that has had a breach

357
00:16:33,993 --> 00:16:35,494
participates in
information sharing,

358
00:16:35,494 --> 00:16:37,396
and I'm gonna show
you an example of

359
00:16:38,831 --> 00:16:43,102
a document request that
was served a few years ago

360
00:16:43,102 --> 00:16:46,605
in an enforcement
action, and you'll see

361
00:16:46,605 --> 00:16:48,340
the kinds of questions
that are asked.

362
00:16:48,340 --> 00:16:50,309
One of the questions that
I would certainly ask

363
00:16:50,309 --> 00:16:53,945
if I were a civil, a
plaintiff suing a company

364
00:16:53,946 --> 00:16:56,815
post data breaches, hey,
what information sharing

365
00:16:56,815 --> 00:16:58,317
programs do you participate in.

366
00:16:58,317 --> 00:17:00,619
And not only that,
what information,

367
00:17:00,619 --> 00:17:03,088
I wanna see all of the
information that you shared

368
00:17:03,088 --> 00:17:07,393
with this organization
over the last five years.

369
00:17:07,393 --> 00:17:09,795
So that information
may be protected

370
00:17:09,795 --> 00:17:12,731
in that the government
isn't going to share it,

371
00:17:12,731 --> 00:17:14,933
but you still have a
record of the information

372
00:17:14,933 --> 00:17:16,167
that you shared that you would

373
00:17:16,167 --> 00:17:19,804
have to turn over
in a litigation.

374
00:17:21,973 --> 00:17:24,675
Again, the various torts,
as we talk about them

375
00:17:24,675 --> 00:17:28,079
in the legal sense, not
these delicious torts

376
00:17:28,079 --> 00:17:31,283
that I'm presenting
here, really are based

377
00:17:31,283 --> 00:17:32,985
on this, again,
reasonableness standard,

378
00:17:32,985 --> 00:17:35,853
can I establish that this
company was negligent

379
00:17:38,323 --> 00:17:41,093
in the way it responded
to a particular incident

380
00:17:41,093 --> 00:17:44,363
or indicator, that is
what the plaintiff's bar

381
00:17:44,363 --> 00:17:46,465
out there, all the lawyers
out there that are,

382
00:17:46,465 --> 00:17:48,233
that are suing, and
you've probly noticed,

383
00:17:48,233 --> 00:17:50,436
any significant
data breach today

384
00:17:50,436 --> 00:17:53,871
is followed by a
raft of lawsuits.

385
00:17:53,872 --> 00:17:56,075
There's a lot of plaintiff,
a lot of very smart,

386
00:17:56,075 --> 00:17:58,043
talented, creative
plaintiff lawyers,

387
00:17:58,043 --> 00:18:00,178
painful as it is for
me to say those words,

388
00:18:01,313 --> 00:18:03,048
that are comin' up
with all kinds of ways

389
00:18:03,048 --> 00:18:05,684
to come after companies
and make them look bad,

390
00:18:05,684 --> 00:18:08,153
and either extract
a sort of ransom

391
00:18:08,153 --> 00:18:11,890
settlement out of
them, or actually prove

392
00:18:11,890 --> 00:18:14,693
in a court of law that
they were negligent.

393
00:18:14,693 --> 00:18:17,062
So there's also
no protection for

394
00:18:17,062 --> 00:18:18,664
intelligence that you
share more casually,

395
00:18:18,664 --> 00:18:20,933
and I've seen this happen a lot,

396
00:18:20,933 --> 00:18:22,501
I've done a lot of
data breach work

397
00:18:22,501 --> 00:18:24,403
over the years as I mentioned,

398
00:18:24,403 --> 00:18:26,271
and one of the things
we very often see

399
00:18:26,271 --> 00:18:27,539
when we come in, we're talkin'

400
00:18:27,539 --> 00:18:29,174
to the information
security team,

401
00:18:29,174 --> 00:18:31,977
is that sometime early on,

402
00:18:31,977 --> 00:18:34,913
after the incident was detected
internally, before they knew

403
00:18:34,913 --> 00:18:37,749
it was a breach, we try
to avoid using that word

404
00:18:37,749 --> 00:18:41,587
until we know there's been
some kind of exposure.

405
00:18:42,988 --> 00:18:44,556
So early on, when
all they've got

406
00:18:44,556 --> 00:18:46,992
is somethin' weird, a
system actin' funny,

407
00:18:46,992 --> 00:18:49,695
a strange lookin'
file showing up,

408
00:18:49,695 --> 00:18:52,130
that there's a lot of
informal information sharing

409
00:18:52,131 --> 00:18:53,832
going on, so you have
your two or three people

410
00:18:53,832 --> 00:18:57,436
that you really trust, and
it can be a lonely job.

411
00:18:57,436 --> 00:18:58,669
Especially if you've
got a small team,

412
00:18:58,670 --> 00:19:01,373
or if you're by yourself
in your organization,

413
00:19:01,373 --> 00:19:03,509
it can be a lonely feeling,
you're constantly worried

414
00:19:03,509 --> 00:19:05,444
about missing something,
so you probly have

415
00:19:05,444 --> 00:19:07,846
a group of people that you
trust and you reach out to.

416
00:19:07,846 --> 00:19:10,315
We see that a lot in incident
response is that there's,

417
00:19:10,315 --> 00:19:13,652
usually what we'll see
is just an email saying,

418
00:19:13,652 --> 00:19:17,089
hey, Bob, you got time to talk,
I saw somethin' weird today.

419
00:19:18,257 --> 00:19:19,858
And you don't know what
that conversation was,

420
00:19:19,858 --> 00:19:21,960
so you go and talk to the, hey,

421
00:19:21,960 --> 00:19:24,663
what did you and Bob
talk about there?

422
00:19:24,663 --> 00:19:26,431
And what would Bob
say if Bob were asked

423
00:19:26,431 --> 00:19:28,467
what you guys talked
about that day.

424
00:19:28,467 --> 00:19:30,636
So you really need
to be careful,

425
00:19:30,636 --> 00:19:32,404
it's not the main
point of my talk today,

426
00:19:32,404 --> 00:19:35,574
but think about casual
information sharing

427
00:19:35,574 --> 00:19:39,211
as a situation that can
create risk just as much

428
00:19:39,211 --> 00:19:42,414
as this more formal
information sharing can do.

429
00:19:42,414 --> 00:19:46,585
So you really need to
recalibrate your risk meter,

430
00:19:46,585 --> 00:19:48,720
and rethink your
assumptions about sharing.

431
00:19:50,122 --> 00:19:53,492
A lot of people would take
these statements for granted.

432
00:19:53,492 --> 00:19:55,694
It can't hurt if we just receive
threat intelligence, right,

433
00:19:55,694 --> 00:19:58,397
well I already talked about
how it actually can hurt.

434
00:19:58,397 --> 00:20:00,331
Better to know about a
threat than not know.

435
00:20:00,332 --> 00:20:02,434
We're certainly past the
head-in-the-sand days,

436
00:20:02,434 --> 00:20:05,437
I get that, but again,
if you are aware

437
00:20:05,437 --> 00:20:08,506
of a potential threat
and you fail to act,

438
00:20:08,507 --> 00:20:10,976
you could definitely
be in a worse position

439
00:20:10,976 --> 00:20:12,311
from a liability perspective

440
00:20:12,311 --> 00:20:14,913
than if you hadn't known at all.

441
00:20:14,913 --> 00:20:16,348
So you've really gotta
make sure that you're

442
00:20:16,348 --> 00:20:18,317
in a position to
orchestrate these responses.

443
00:20:18,317 --> 00:20:20,185
I can't possibly stay on
top of emerging threats

444
00:20:20,185 --> 00:20:22,687
alone, I need help
and collaboration,

445
00:20:22,688 --> 00:20:25,924
that can certainly be
true for most people.

446
00:20:25,924 --> 00:20:27,693
It is a good idea to
seek a second opinion

447
00:20:27,693 --> 00:20:29,428
from a trusted friend
when I find something

448
00:20:29,428 --> 00:20:31,396
that I think might be
bad, just to confirm,

449
00:20:31,396 --> 00:20:34,332
before I raise the alarm
bell, I wanna make sure.

450
00:20:34,333 --> 00:20:36,401
You've really gotta be careful,

451
00:20:36,401 --> 00:20:39,538
because all of those
things I just talked about,

452
00:20:39,538 --> 00:20:41,907
this is I think the one,
one of the legal terms

453
00:20:41,907 --> 00:20:44,243
I'm gonna introduce today,
how many of you are familiar

454
00:20:44,243 --> 00:20:48,347
with the term discoverable
in a legal context.

455
00:20:48,347 --> 00:20:50,616
Okay, I won't ask why you're
familiar with that term,

456
00:20:50,616 --> 00:20:53,151
but it probably was not
a pleasant experience.

457
00:20:53,151 --> 00:20:55,187
What does it mean, what
does discoverable mean.

458
00:20:56,688 --> 00:20:59,358
- [Audience Member] (mumbles)
The opposition can request it.

459
00:20:59,358 --> 00:21:01,360
- The opposition can
request it, and is entitled

460
00:21:01,360 --> 00:21:05,129
to receive a copy of
it in a litigation.

461
00:21:05,130 --> 00:21:06,365
They file the case against you,

462
00:21:06,365 --> 00:21:08,500
you can serve a
discovery request,

463
00:21:10,869 --> 00:21:15,307
and anything that is
relevant to the claim

464
00:21:16,475 --> 00:21:19,477
must be produced by
the party who receives

465
00:21:19,478 --> 00:21:21,313
this discovery request,
unless it's protected

466
00:21:21,313 --> 00:21:24,316
by attorney-client privilege
or some other exemptions there.

467
00:21:25,717 --> 00:21:28,920
Everything that I just
showed on the last slide,

468
00:21:28,920 --> 00:21:31,723
all these things, this is
discoverable information.

469
00:21:33,292 --> 00:21:36,695
So the information that you
receive through your ISAC

470
00:21:36,695 --> 00:21:38,897
or any information
sharing mechanism,

471
00:21:38,897 --> 00:21:41,433
is potentially
discoverable information

472
00:21:41,433 --> 00:21:42,901
that can be used against you.

473
00:21:44,202 --> 00:21:45,537
Let's talk about
how that plays out,

474
00:21:45,537 --> 00:21:47,572
so what every regulator,
in my experience,

475
00:21:47,572 --> 00:21:49,041
what every regulator
or plaintiff

476
00:21:49,041 --> 00:21:51,976
wants to know after
a breach has occurred

477
00:21:52,878 --> 00:21:54,612
is the same set of stuff, right,

478
00:21:54,613 --> 00:21:57,582
obviously there's different
ways that you ask for it,

479
00:21:57,582 --> 00:22:00,519
and the answers are highly
varied, but they all

480
00:22:00,519 --> 00:22:03,755
wanna know how did it happen,
how did the breach happen.

481
00:22:03,755 --> 00:22:07,859
When did you know
about the breach,

482
00:22:07,859 --> 00:22:10,829
at what time did you first
know that you had an issue,

483
00:22:10,829 --> 00:22:12,898
and as we all know,
anybody who's done

484
00:22:12,898 --> 00:22:15,100
incident response
work knows that

485
00:22:15,100 --> 00:22:17,735
that is a very difficult
question to answer.

486
00:22:17,736 --> 00:22:18,904
When did I know what?

487
00:22:20,339 --> 00:22:22,607
When did I know there was an
actual, actually exfiltration

488
00:22:22,607 --> 00:22:24,743
of information, I can
give you that date,

489
00:22:24,743 --> 00:22:26,278
but when did I know
that there might be

490
00:22:26,278 --> 00:22:28,580
something funny going
on in my environment?

491
00:22:28,580 --> 00:22:29,814
(chuckles)

492
00:22:29,815 --> 00:22:32,017
I feel that way
almost every day,

493
00:22:32,017 --> 00:22:33,552
so how do you trace that back.

494
00:22:35,020 --> 00:22:39,458
How did you respond to this
information when you got it.

495
00:22:39,458 --> 00:22:43,362
What was exposed, and how do
you know what was exposed,

496
00:22:43,362 --> 00:22:44,963
that's one of the most
contentious issues

497
00:22:44,963 --> 00:22:48,700
that comes up when you're
working with a regulator

498
00:22:48,700 --> 00:22:51,470
post breach or you're
being sued by a plaintiff

499
00:22:51,470 --> 00:22:54,539
is, they will ask you,
well, what was exposed.

500
00:22:54,539 --> 00:22:57,642
It was personal information,
that's why you see

501
00:22:57,642 --> 00:23:01,012
these companies having to
do iterative disclosures,

502
00:23:01,012 --> 00:23:05,216
well, it was only 100,000
usernames and passwords,

503
00:23:05,217 --> 00:23:08,420
but the passwords were,
salted encrypted, whatever.

504
00:23:09,554 --> 00:23:10,889
And then, eh, a
couple weeks later,

505
00:23:10,889 --> 00:23:13,658
actually it was 500,000
usernames and passwords,

506
00:23:15,093 --> 00:23:17,562
and 300,000 of them actually
weren't salted encrypted,

507
00:23:17,562 --> 00:23:19,264
and you get this trickle effect.

508
00:23:19,264 --> 00:23:23,001
It is very difficult
in determining,

509
00:23:23,001 --> 00:23:25,103
determining that you were
exposed in the first place,

510
00:23:25,103 --> 00:23:27,506
that you had a breach,
can be challenging,

511
00:23:27,506 --> 00:23:29,574
a lot of times it's
pretty easy to determine

512
00:23:29,574 --> 00:23:31,643
yeah, we had an
issue, we were owned,

513
00:23:31,643 --> 00:23:34,813
somebody had access to these
servers for a period of time,

514
00:23:34,813 --> 00:23:36,715
but to tell me what they
actually did on those servers,

515
00:23:36,715 --> 00:23:39,117
and what information they
actually compromised,

516
00:23:39,117 --> 00:23:40,585
is a very difficult question.

517
00:23:42,020 --> 00:23:44,456
Were you on notice of the
risk, and what measures

518
00:23:44,456 --> 00:23:45,924
were in place to
prevent the breach.

519
00:23:45,924 --> 00:23:49,360
This is where the information
sharing starts to get,

520
00:23:50,595 --> 00:23:52,230
starts to get a
little scary, right.

521
00:23:52,230 --> 00:23:54,533
Were you on notice, well,
what does that mean.

522
00:23:55,867 --> 00:23:59,236
Does it mean that there
was a threat feed,

523
00:23:59,237 --> 00:24:00,906
whether it was an
email from a friend,

524
00:24:00,906 --> 00:24:05,911
or bulletin from an ISAC
sitting unread in my box,

525
00:24:07,112 --> 00:24:09,381
I just never got to it,
or I had this indicator

526
00:24:09,381 --> 00:24:11,883
but I fat-fingered
putting it into my,

527
00:24:13,318 --> 00:24:16,288
into one of my security
devices, and we missed it.

528
00:24:16,288 --> 00:24:18,490
When were we on notice,
this is something

529
00:24:18,490 --> 00:24:23,495
that gets argued a lot in
these data breach litigations.

530
00:24:24,696 --> 00:24:26,831
And then what people,
process and technology

531
00:24:26,832 --> 00:24:29,334
failures contributed
to the breach.

532
00:24:29,334 --> 00:24:30,936
These are the things the
regulators wanna know,

533
00:24:30,936 --> 00:24:33,038
these are things the
plaintiffs wanna know,

534
00:24:33,038 --> 00:24:35,407
these are things that
you need to think about

535
00:24:36,741 --> 00:24:38,176
frankly, long before
there's any sort of breach,

536
00:24:38,176 --> 00:24:40,412
but whenever you're
dealing with information,

537
00:24:40,412 --> 00:24:42,214
and certainly sharing
any information.

538
00:24:42,214 --> 00:24:45,517
I've taken an excerpt
from, this is an actual

539
00:24:45,517 --> 00:24:47,452
document request that was served

540
00:24:47,452 --> 00:24:49,221
by the Federal Trade
Commission on LabMD,

541
00:24:49,221 --> 00:24:52,057
anybody familiar
with this LabMD case?

542
00:24:52,057 --> 00:24:55,827
It's a really
interesting case study,

543
00:24:55,827 --> 00:24:58,230
I don't have time to get
into it, but if you're,

544
00:24:59,698 --> 00:25:02,701
look it up, look up LabMD if
you're interested in sort of

545
00:25:02,701 --> 00:25:06,171
how the government handles
these breach investigations.

546
00:25:06,171 --> 00:25:10,842
So I'm gonna point you to
request number six here.

547
00:25:10,842 --> 00:25:12,911
So this is the Federal
Trade Commission

548
00:25:12,911 --> 00:25:14,713
serving a document
request, looking for

549
00:25:14,713 --> 00:25:18,216
all communications,
all communications,

550
00:25:18,216 --> 00:25:19,918
between or among
the Company, LabMD,

551
00:25:19,918 --> 00:25:22,920
or any third party, right,
so any communications

552
00:25:22,921 --> 00:25:24,723
no matter who was a part
of these conversations,

553
00:25:24,723 --> 00:25:29,494
about any security incident
at any point in time.

554
00:25:31,563 --> 00:25:34,499
That is standard fare
today, you will see that

555
00:25:34,499 --> 00:25:37,335
in every document request
that is served today

556
00:25:37,335 --> 00:25:39,604
by a regulator or
private litigant,

557
00:25:39,604 --> 00:25:41,406
and chances are a
court's gonna say,

558
00:25:41,406 --> 00:25:44,075
yeah, that makes sense,
it's relevant to the claim,

559
00:25:45,176 --> 00:25:46,811
and you need to provide
this information,

560
00:25:46,811 --> 00:25:49,014
that is all discoverable
information.

561
00:25:49,014 --> 00:25:50,115
And number seven's
pretty scary too,

562
00:25:50,115 --> 00:25:51,750
all forensic reports or analyses

563
00:25:51,750 --> 00:25:53,552
relating to any
security incident.

564
00:25:54,986 --> 00:25:57,656
Some very sensitive information
that can be turned over,

565
00:25:57,656 --> 00:26:00,692
and a lot of the people
in this room are involved

566
00:26:00,692 --> 00:26:03,295
in creating whatever
gets turned over

567
00:26:03,295 --> 00:26:05,897
in response to those requests,

568
00:26:05,897 --> 00:26:07,666
so you gotta think about that.

569
00:26:07,666 --> 00:26:10,802
Am I comfortable having this,
everything that I create,

570
00:26:10,802 --> 00:26:14,773
all these communications that
I have about threats, shared.

571
00:26:14,773 --> 00:26:17,141
Okay, so I'm gonna go
through a scenario,

572
00:26:18,810 --> 00:26:22,379
and then wrap it up here
and take some questions,

573
00:26:22,380 --> 00:26:24,449
just to make this a
little bit more real.

574
00:26:25,817 --> 00:26:27,552
Okay, so your crack
information security team

575
00:26:27,552 --> 00:26:29,788
notices unusual
communication from a host

576
00:26:29,788 --> 00:26:31,890
in your environment
communicating out

577
00:26:31,890 --> 00:26:35,026
to an unknown IP,
not blacklisted,

578
00:26:35,026 --> 00:26:36,528
but unknown, and an
unusual time of day,

579
00:26:36,528 --> 00:26:38,697
something that sticks out,
right, something fairly,

580
00:26:38,697 --> 00:26:41,800
we should go check out
what the heck this was.

581
00:26:41,800 --> 00:26:43,568
You investigate, you
find what appears to be

582
00:26:43,568 --> 00:26:47,405
an exfiltration
folder on this host,

583
00:26:47,405 --> 00:26:51,309
you also find some
malware on that host,

584
00:26:52,444 --> 00:26:55,947
you act decisively
and quickly, your team

585
00:26:55,947 --> 00:26:58,649
is at its best, it
isolates the host,

586
00:26:59,918 --> 00:27:01,119
and prevents any
exfiltration, so the actual

587
00:27:01,119 --> 00:27:02,687
exfiltration folder's
there, but you can

588
00:27:02,687 --> 00:27:05,156
conclusively show,
because of the log data,

589
00:27:05,156 --> 00:27:07,926
that nothin' went back
out, you prevented

590
00:27:07,926 --> 00:27:10,662
any propagation, you
scanned across your network,

591
00:27:10,662 --> 00:27:13,565
you've contained the incident,

592
00:27:13,565 --> 00:27:17,602
you eradicate the
malware, it's a good day

593
00:27:17,602 --> 00:27:20,705
for the team, there was
only one host affected,

594
00:27:22,107 --> 00:27:25,610
you're feelin' good, right,
go out and have a beer,

595
00:27:25,610 --> 00:27:28,446
and hey, why not, we'll
help our friends, right,

596
00:27:28,446 --> 00:27:29,848
who might have experienced
the same thing,

597
00:27:29,848 --> 00:27:33,251
you share these indicators
through your ISAO,

598
00:27:33,251 --> 00:27:35,954
your Information Sharing
and Analysis Organization

599
00:27:35,954 --> 00:27:37,822
that you participate in.

600
00:27:37,822 --> 00:27:40,792
So let's carry this
through into Scenario Two.

601
00:27:44,062 --> 00:27:46,130
You're the one who
created these indicators,

602
00:27:46,131 --> 00:27:47,766
you've shared them
with your ISAO,

603
00:27:47,766 --> 00:27:49,234
you now need to make
sure that you're

604
00:27:49,234 --> 00:27:51,770
consuming them properly,
and that you're making sure

605
00:27:51,770 --> 00:27:52,871
that you're in a
position to prevent

606
00:27:52,871 --> 00:27:54,572
any recurrence of this attack.

607
00:27:54,572 --> 00:27:57,141
So you consume this information,

608
00:27:57,142 --> 00:27:59,544
ingest them into your SEM,
your other security devices

609
00:27:59,544 --> 00:28:03,381
as needed, but due to a
defective device feed,

610
00:28:03,381 --> 00:28:05,150
one of the correlation
rules that you set up

611
00:28:05,150 --> 00:28:07,052
in the wake of this
incident, fails,

612
00:28:07,052 --> 00:28:11,423
and fails to detect a
reoccurence of the same attack.

613
00:28:13,892 --> 00:28:17,128
And you're exposed,
but you're blind to it,

614
00:28:17,128 --> 00:28:18,562
you have no way to know.

615
00:28:18,563 --> 00:28:21,666
Four months later, a system
administrator notices

616
00:28:21,666 --> 00:28:23,501
what appears to be
a familiar looking

617
00:28:23,501 --> 00:28:25,970
exfiltration
directory on a server,

618
00:28:25,970 --> 00:28:29,541
investigation shows that
100 gigabytes of TAR files

619
00:28:29,541 --> 00:28:32,310
were exfiltrated over
the preceding months,

620
00:28:34,212 --> 00:28:36,047
and this information
included personal information

621
00:28:36,047 --> 00:28:38,983
for 100,000 people,
you have to go through

622
00:28:38,983 --> 00:28:41,619
a public notification,
so now the whole world

623
00:28:41,619 --> 00:28:43,655
knows that this happened,
you have to issue

624
00:28:43,655 --> 00:28:46,791
a statement on the breach,
which usually includes

625
00:28:46,791 --> 00:28:49,394
some level of suggestion
about how it happened

626
00:28:49,394 --> 00:28:52,897
and what occurred,
somebody out there

627
00:28:52,897 --> 00:28:56,367
on the internet connects
the dots and realizes,

628
00:28:56,367 --> 00:28:58,903
wait a minute, the
indicators necessary

629
00:28:58,903 --> 00:29:01,272
to detect and prevent this
breach were actually shared

630
00:29:01,272 --> 00:29:03,808
via your ISAO four
months earlier.

631
00:29:03,808 --> 00:29:05,510
Now, even if they don't
know that you are the one

632
00:29:05,510 --> 00:29:08,179
who shared it because
you can, presumably,

633
00:29:08,179 --> 00:29:10,715
obscure your identity, although
I question that as well,

634
00:29:10,715 --> 00:29:13,418
how well you can do that in an
information sharing context,

635
00:29:13,418 --> 00:29:15,219
but even if you're
able to, you still

636
00:29:15,220 --> 00:29:18,957
have to explain how you were
aware of this information,

637
00:29:18,957 --> 00:29:22,293
or your ISAO had notified
you about these indicators

638
00:29:22,293 --> 00:29:25,897
four months prior, and
if you're asked about it,

639
00:29:27,132 --> 00:29:29,934
again, you can't
lie, it's pretty easy

640
00:29:29,934 --> 00:29:31,168
for a regulator
to determine that

641
00:29:31,169 --> 00:29:34,172
yeah, actually, it
was you that shared,

642
00:29:34,172 --> 00:29:39,177
you had the exact same
incident four months prior,

643
00:29:40,345 --> 00:29:41,579
and you failed to take
appropriate action.

644
00:29:41,579 --> 00:29:43,081
How many in this
room would like to be

645
00:29:43,081 --> 00:29:45,550
in that situation, having to
explain how this happened.

646
00:29:46,651 --> 00:29:47,485
I hope nobody.

647
00:29:48,820 --> 00:29:51,289
So it's a bad situation
none of us wanna be in,

648
00:29:51,289 --> 00:29:54,526
and it all, and hopefully,
you're understanding

649
00:29:55,860 --> 00:29:57,728
that information
sharing, and the way

650
00:29:57,729 --> 00:29:59,531
you go about sharing information

651
00:29:59,531 --> 00:30:01,399
could actually
exacerbate a situation.

652
00:30:01,399 --> 00:30:04,202
Now am I saying if you
rewind, that they never

653
00:30:04,202 --> 00:30:06,704
shoulda shared those
indicators with their ISAO?

654
00:30:06,704 --> 00:30:08,438
How many people think
that they should just,

655
00:30:08,439 --> 00:30:10,842
shouldn't have shared
those indicators.

656
00:30:10,842 --> 00:30:13,044
Yeah, I don't think
that, I think they,

657
00:30:13,044 --> 00:30:14,312
what's the right thing to do,

658
00:30:14,312 --> 00:30:15,746
if you're participating
in one of these things,

659
00:30:15,747 --> 00:30:17,482
then yeah, that's a situation
where you should share.

660
00:30:17,482 --> 00:30:18,950
What they needed
to do is make sure

661
00:30:18,950 --> 00:30:22,420
that their own
mitigation was effective.

662
00:30:23,588 --> 00:30:25,056
So if you're going
to share, make sure

663
00:30:25,056 --> 00:30:27,892
that you are in a position
to, especially in a situation

664
00:30:27,892 --> 00:30:29,727
where you're sharing
information, you gotta make sure

665
00:30:29,727 --> 00:30:31,930
that you're effective
in your response.

666
00:30:31,930 --> 00:30:34,799
So how else can
sharing information
make you less secure.

667
00:30:36,267 --> 00:30:38,136
You all are more familiar
with these issues than I am,

668
00:30:38,136 --> 00:30:40,538
limited resources to
invest in security,

669
00:30:40,538 --> 00:30:44,042
and there can be wasted
time chasing false alarms,

670
00:30:44,042 --> 00:30:45,776
there's a lot of false
alarms that come in

671
00:30:45,777 --> 00:30:47,912
through these information
sharing channels.

672
00:30:49,347 --> 00:30:51,649
Either things that would never
have materialized for you,

673
00:30:51,649 --> 00:30:54,284
and there's no way to really
know that one way or the other,

674
00:30:54,285 --> 00:30:57,188
or just things that turn
out to be inaccurate.

675
00:30:57,188 --> 00:30:59,591
And the more, as the
volume and velocity

676
00:30:59,591 --> 00:31:01,959
of sharing increases,
which it is certain to do,

677
00:31:01,960 --> 00:31:05,363
that problem is gonna get worse,
not better, in my opinion.

678
00:31:06,831 --> 00:31:10,267
And that signal-to-noise
ratio over time deteriorating,

679
00:31:10,268 --> 00:31:12,670
I guess noise-to-signal
ratio, can lead to

680
00:31:12,670 --> 00:31:17,108
erosion of trust in these
threat intel sources.

681
00:31:17,108 --> 00:31:20,311
So the take-aways here,
I'm not saying don't share,

682
00:31:20,311 --> 00:31:22,213
but you've really gotta
proceed with caution.

683
00:31:22,213 --> 00:31:24,415
Make sure that you
have a mature program,

684
00:31:24,415 --> 00:31:25,884
particularly to make
use of the information

685
00:31:25,884 --> 00:31:28,953
that you're consuming,
before you start to share.

686
00:31:28,953 --> 00:31:32,757
Don't give in to peer
pressure or this sorta

687
00:31:32,757 --> 00:31:35,126
compulsion to validate
everything that you're thinking

688
00:31:35,126 --> 00:31:37,428
with a peer that's
outside your organization.

689
00:31:37,428 --> 00:31:38,930
That can be risky,
try to find someone

690
00:31:38,930 --> 00:31:41,933
inside your organization you
can talk to about this stuff.

691
00:31:41,933 --> 00:31:44,869
And then, do what you
do best, which is assume

692
00:31:44,869 --> 00:31:47,772
and evaluate the worst
possible (chuckles) outcome.

693
00:31:47,772 --> 00:31:49,607
And prepare for
it, and I call this

694
00:31:49,607 --> 00:31:51,576
practicing your
Congressional testimony.

695
00:31:53,077 --> 00:31:55,880
I'm guessing that a lot of
you do tabletop exercises,

696
00:31:55,880 --> 00:31:57,181
if you don't, you really should

697
00:31:57,181 --> 00:31:59,117
do tabletop exercise
on a regular basis,

698
00:31:59,117 --> 00:32:03,288
include that as part of
it, a mock deposition

699
00:32:03,288 --> 00:32:07,558
of you explaining some of
these difficult challenges,

700
00:32:07,558 --> 00:32:10,161
it is a sobering
exercise to go through.

701
00:32:10,161 --> 00:32:12,764
All right, outta time
here, but thanks everybody!

702
00:32:12,764 --> 00:32:15,733
(audience applauds)

703
00:32:15,733 --> 00:32:18,403
(intense music)

