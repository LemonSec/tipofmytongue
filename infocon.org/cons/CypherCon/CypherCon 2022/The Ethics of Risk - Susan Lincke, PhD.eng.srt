1
00:00:00,180 --> 00:00:03,399
[Music]

2
00:00:07,700 --> 00:00:10,700
thank you

3
00:00:20,990 --> 00:00:29,238
[Music]

4
00:00:37,160 --> 00:00:40,218
all right

5
00:00:47,280 --> 00:00:49,460
foreign

6
00:01:03,199 --> 00:01:07,380
model and the framework actually which

7
00:01:07,380 --> 00:01:10,439
is something like a a maturity model but

8
00:01:10,439 --> 00:01:13,380
the reason I really got into this was

9
00:01:13,380 --> 00:01:17,520
because I was I kept seeing my little

10
00:01:17,520 --> 00:01:19,920
statistic is the number of reached

11
00:01:19,920 --> 00:01:22,259
records per population in the United

12
00:01:22,259 --> 00:01:25,500
States and usually the number of reached

13
00:01:25,500 --> 00:01:30,659
records is about 360 million which is

14
00:01:30,659 --> 00:01:33,240
approximately equal to the population of

15
00:01:33,240 --> 00:01:36,060
the United States or higher than that or

16
00:01:36,060 --> 00:01:39,299
even double that so um so I kind of got

17
00:01:39,299 --> 00:01:42,420
this picture that security is not well

18
00:01:42,420 --> 00:01:44,939
funded in Industry

19
00:01:44,939 --> 00:01:48,000
um you know reaches our Ransom Crime

20
00:01:48,000 --> 00:01:51,360
Pays which is ransomware so if you think

21
00:01:51,360 --> 00:01:55,380
about risk usually you pay for risk up

22
00:01:55,380 --> 00:01:58,020
to the point of you know you pay for

23
00:01:58,020 --> 00:02:01,079
controls up to the point of of your risk

24
00:02:01,079 --> 00:02:02,460
uh

25
00:02:02,460 --> 00:02:04,079
and

26
00:02:04,079 --> 00:02:06,360
um and it affects effectively the world

27
00:02:06,360 --> 00:02:07,860
is not

28
00:02:07,860 --> 00:02:09,720
um if you look at the world itself it

29
00:02:09,720 --> 00:02:12,060
obviously is not paying enough for for

30
00:02:12,060 --> 00:02:15,000
security uh one of the other things you

31
00:02:15,000 --> 00:02:16,860
can think about is really that we've

32
00:02:16,860 --> 00:02:19,620
eradicated all sorts of diseases we've

33
00:02:19,620 --> 00:02:21,840
spent a lot of time you know eradicating

34
00:02:21,840 --> 00:02:24,480
and paying for diseases to go away but

35
00:02:24,480 --> 00:02:27,540
it that isn't true in security so the

36
00:02:27,540 --> 00:02:30,060
question is is um our company is you

37
00:02:30,060 --> 00:02:32,459
know only looking for profit is there a

38
00:02:32,459 --> 00:02:35,760
lack of interest is it an engineering

39
00:02:35,760 --> 00:02:38,160
problem and

40
00:02:38,160 --> 00:02:41,879
um uh and what's the what is ethical

41
00:02:41,879 --> 00:02:45,420
risk and you know how can we help to

42
00:02:45,420 --> 00:02:47,940
solve this problem from an ethical risk

43
00:02:47,940 --> 00:02:49,260
perspective

44
00:02:49,260 --> 00:02:52,800
so I I developed a model and it's really

45
00:02:52,800 --> 00:02:54,900
based on the next slide but let me

46
00:02:54,900 --> 00:02:56,940
introduce it anyway

47
00:02:56,940 --> 00:02:59,580
um the first one is risk immature it's

48
00:02:59,580 --> 00:03:01,680
it's those small businesses that have no

49
00:03:01,680 --> 00:03:02,940
clue that they need to be concerned

50
00:03:02,940 --> 00:03:04,560
about risk and then there's

51
00:03:04,560 --> 00:03:07,019
self-protection which a lot of companies

52
00:03:07,019 --> 00:03:10,500
are at which basically we protect our

53
00:03:10,500 --> 00:03:13,319
organization and then there's the next

54
00:03:13,319 --> 00:03:16,019
level up which is compliance which is we

55
00:03:16,019 --> 00:03:18,000
need to adhere to laws we're going to

56
00:03:18,000 --> 00:03:20,640
try to adhere to laws and then there's

57
00:03:20,640 --> 00:03:23,280
stakeholder concern which is you know

58
00:03:23,280 --> 00:03:25,920
being concerned with customers but also

59
00:03:25,920 --> 00:03:28,620
being concerned with our employee our

60
00:03:28,620 --> 00:03:32,099
employees our financiers our suppliers

61
00:03:32,099 --> 00:03:35,459
our community the people we do business

62
00:03:35,459 --> 00:03:39,060
with every day and then the the top is a

63
00:03:39,060 --> 00:03:42,299
societal concern and this is concern for

64
00:03:42,299 --> 00:03:44,340
the society and environment

65
00:03:44,340 --> 00:03:47,540
now this model really

66
00:03:47,540 --> 00:03:50,720
came out of her corresponds with

67
00:03:50,720 --> 00:03:53,599
Colbert's Kohlberg's

68
00:03:53,599 --> 00:03:56,280
levels of moral judgment and his six

69
00:03:56,280 --> 00:03:58,560
stages which in the bottom two and

70
00:03:58,560 --> 00:04:02,700
kohlberg is a psychologist and so he

71
00:04:02,700 --> 00:04:04,500
basically said that you know at the

72
00:04:04,500 --> 00:04:07,140
lowest stage we had the pre-conventional

73
00:04:07,140 --> 00:04:09,659
morality respect for power and

74
00:04:09,659 --> 00:04:13,439
Punishment and really the two stages

75
00:04:13,439 --> 00:04:15,239
that are part of that is obedience to

76
00:04:15,239 --> 00:04:17,760
Authority and we benefit each other

77
00:04:17,760 --> 00:04:20,639
Mutual deals so you can kind of think of

78
00:04:20,639 --> 00:04:25,620
that kid who screens in order to get the

79
00:04:25,620 --> 00:04:28,560
candy he wants right you know and we'll

80
00:04:28,560 --> 00:04:30,840
make a deal I'll stop screaming if you

81
00:04:30,840 --> 00:04:34,199
give me the candy that I want the next

82
00:04:34,199 --> 00:04:37,259
level up is conventional morality which

83
00:04:37,259 --> 00:04:40,560
is social values and rules and this is

84
00:04:40,560 --> 00:04:43,380
social approval good versus bad and we

85
00:04:43,380 --> 00:04:45,960
all know what's what Society improves

86
00:04:45,960 --> 00:04:48,240
and what they don't approve part of that

87
00:04:48,240 --> 00:04:50,280
is how we dress every day

88
00:04:50,280 --> 00:04:55,259
and the next is obey social legal and

89
00:04:55,259 --> 00:04:56,840
religious laws

90
00:04:56,840 --> 00:04:59,460
depending on what you

91
00:04:59,460 --> 00:05:00,479
um

92
00:05:00,479 --> 00:05:02,520
you've practiced basically and then

93
00:05:02,520 --> 00:05:04,560
there's the post-conventional justice

94
00:05:04,560 --> 00:05:07,020
and Welfare which is to benefit Society

95
00:05:07,020 --> 00:05:10,560
often trying to change laws and ethical

96
00:05:10,560 --> 00:05:13,860
principles over social norms now a

97
00:05:13,860 --> 00:05:16,620
Piaget is basically in the education

98
00:05:16,620 --> 00:05:19,940
world and he teaches

99
00:05:19,940 --> 00:05:24,000
he basically talks about how students

100
00:05:24,000 --> 00:05:28,020
children learn ethical values and the

101
00:05:28,020 --> 00:05:30,919
the what they go through is the premoral

102
00:05:30,919 --> 00:05:34,020
egocentrism respect for authority and

103
00:05:34,020 --> 00:05:38,000
rules the second level is hetero heteron

104
00:05:38,000 --> 00:05:40,620
heteronome whatever cooperation of

105
00:05:40,620 --> 00:05:42,780
mutual respect and then there's the

106
00:05:42,780 --> 00:05:45,780
autonomous level which is a reciprocity

107
00:05:45,780 --> 00:05:49,620
and equality at the business level we

108
00:05:49,620 --> 00:05:53,039
have Friedman and Freeman and I will

109
00:05:53,039 --> 00:05:55,500
talk about them as I go through the

110
00:05:55,500 --> 00:05:58,139
different levels so let's start off with

111
00:05:58,139 --> 00:06:01,380
risk and mature and risk you know risky

112
00:06:01,380 --> 00:06:03,419
mature are those small businesses that

113
00:06:03,419 --> 00:06:04,680
have no clue that they should be

114
00:06:04,680 --> 00:06:07,139
concerned about risk or security for

115
00:06:07,139 --> 00:06:11,340
that matter and the two steps here would

116
00:06:11,340 --> 00:06:13,560
be adopt and standardized risk process

117
00:06:13,560 --> 00:06:16,080
which uh you guys are all probably

118
00:06:16,080 --> 00:06:20,039
fairly familiar with at least have some

119
00:06:20,039 --> 00:06:21,780
understanding of but the other is create

120
00:06:21,780 --> 00:06:24,300
a culture of risk communication and this

121
00:06:24,300 --> 00:06:27,180
is really deals with ethics so let me go

122
00:06:27,180 --> 00:06:30,180
over what's involved with that the first

123
00:06:30,180 --> 00:06:33,419
is involve business management so

124
00:06:33,419 --> 00:06:36,840
basically business

125
00:06:36,840 --> 00:06:38,940
um you know the management will never

126
00:06:38,940 --> 00:06:40,259
buy into something that they don't

127
00:06:40,259 --> 00:06:42,419
understand very well and so it is

128
00:06:42,419 --> 00:06:45,600
important that they go through that they

129
00:06:45,600 --> 00:06:48,120
help to develop this the risk scenarios

130
00:06:48,120 --> 00:06:50,400
and risk scenarios or stories that

131
00:06:50,400 --> 00:06:52,680
describe how an actor might Implement a

132
00:06:52,680 --> 00:06:55,620
threat to an asset via an event with a

133
00:06:55,620 --> 00:06:56,819
time frame

134
00:06:56,819 --> 00:06:58,860
uh so

135
00:06:58,860 --> 00:07:01,380
um uh so it's really important just to

136
00:07:01,380 --> 00:07:05,100
uh to include them the next is creating

137
00:07:05,100 --> 00:07:06,960
a culture of communication and

138
00:07:06,960 --> 00:07:10,020
responsibility and this really uh hits

139
00:07:10,020 --> 00:07:12,840
from both sides one side is from the

140
00:07:12,840 --> 00:07:15,840
manager side and

141
00:07:15,840 --> 00:07:17,400
um uh and I don't know if you've ever

142
00:07:17,400 --> 00:07:19,080
been in a meeting and you've suggested

143
00:07:19,080 --> 00:07:22,440
something and uh the manager basically

144
00:07:22,440 --> 00:07:24,780
poo-poos your idea

145
00:07:24,780 --> 00:07:27,720
um then uh you know you're not likely to

146
00:07:27,720 --> 00:07:29,460
speak up again

147
00:07:29,460 --> 00:07:32,900
um on on that idea or even on others and

148
00:07:32,900 --> 00:07:35,360
effectively

149
00:07:35,360 --> 00:07:37,919
employees must feel comfortable asking

150
00:07:37,919 --> 00:07:40,979
for help reporting ethical issues and

151
00:07:40,979 --> 00:07:42,720
bad news to management without being

152
00:07:42,720 --> 00:07:46,139
dismissed and um and and basically

153
00:07:46,139 --> 00:07:48,180
business management understands that

154
00:07:48,180 --> 00:07:50,000
loyalty comes from ethical leadership

155
00:07:50,000 --> 00:07:52,860
and employee contribution to decision

156
00:07:52,860 --> 00:07:54,240
making

157
00:07:54,240 --> 00:07:57,599
uh so from the employee side employees

158
00:07:57,599 --> 00:08:00,240
also must communicate effectively with

159
00:08:00,240 --> 00:08:01,580
management

160
00:08:01,580 --> 00:08:03,259
and

161
00:08:03,259 --> 00:08:05,940
there's really two parts to this one

162
00:08:05,940 --> 00:08:09,660
part is to basically ask until their

163
00:08:09,660 --> 00:08:12,240
questions are answered which

164
00:08:12,240 --> 00:08:13,800
probably is going to get them in a

165
00:08:13,800 --> 00:08:15,840
little bit of trouble but effectively

166
00:08:15,840 --> 00:08:18,960
ask nicely you know well what happens if

167
00:08:18,960 --> 00:08:20,639
this happens until there is at least

168
00:08:20,639 --> 00:08:23,220
some form of answer coming back

169
00:08:23,220 --> 00:08:24,900
but the second thing that needs to

170
00:08:24,900 --> 00:08:28,620
happen is that um as technical people

171
00:08:28,620 --> 00:08:30,680
very often

172
00:08:30,680 --> 00:08:34,380
we we know that we have a technical

173
00:08:34,380 --> 00:08:36,958
vocabulary and it's hard to communicate

174
00:08:36,958 --> 00:08:39,539
that to management and so effectively we

175
00:08:39,539 --> 00:08:41,159
can say well you know that issue

176
00:08:41,159 --> 00:08:44,039
occurred because of a technical issue

177
00:08:44,039 --> 00:08:46,980
um and that excuses us but but on the

178
00:08:46,980 --> 00:08:48,420
other hand maybe we should be saying

179
00:08:48,420 --> 00:08:50,100
because we have an older firewall and if

180
00:08:50,100 --> 00:08:52,620
we had a newer one then

181
00:08:52,620 --> 00:08:55,080
um uh then perhaps this problem wouldn't

182
00:08:55,080 --> 00:08:58,380
have occurred so uh well not perhaps

183
00:08:58,380 --> 00:09:00,240
then this this problem would not have

184
00:09:00,240 --> 00:09:01,440
occurred because it would have caught

185
00:09:01,440 --> 00:09:02,880
this

186
00:09:02,880 --> 00:09:06,000
so it's um it's it's the the

187
00:09:06,000 --> 00:09:08,399
communication has to happen on both

188
00:09:08,399 --> 00:09:11,339
sides the next is to document and

189
00:09:11,339 --> 00:09:14,339
communicate risk findings and I think a

190
00:09:14,339 --> 00:09:17,580
lot of people in the security world uh

191
00:09:17,580 --> 00:09:20,100
can get frustrated because uh you know

192
00:09:20,100 --> 00:09:22,200
there's a security recommendations don't

193
00:09:22,200 --> 00:09:23,700
go anywhere

194
00:09:23,700 --> 00:09:25,740
um or don't go as far as they would like

195
00:09:25,740 --> 00:09:29,519
it to and um and effectively

196
00:09:29,519 --> 00:09:33,720
um uh if you I'm basically a c risk and

197
00:09:33,720 --> 00:09:37,800
a um you know isaka see risk and and

198
00:09:37,800 --> 00:09:39,540
isaka

199
00:09:39,540 --> 00:09:43,080
um certified Information Systems auditor

200
00:09:43,080 --> 00:09:45,839
and one of the things that they

201
00:09:45,839 --> 00:09:48,060
emphasize is okay there's

202
00:09:48,060 --> 00:09:48,740
um

203
00:09:48,740 --> 00:09:52,200
the raci which stands for responsible

204
00:09:52,200 --> 00:09:54,959
accountable consultant and informed and

205
00:09:54,959 --> 00:09:57,360
basically management is responsible for

206
00:09:57,360 --> 00:10:00,180
making decisions uh you aren't

207
00:10:00,180 --> 00:10:01,200
um you know if you're a security

208
00:10:01,200 --> 00:10:03,240
professional you aren't responsible for

209
00:10:03,240 --> 00:10:06,360
that so effectively your job is to

210
00:10:06,360 --> 00:10:08,760
inform them and then their job is to

211
00:10:08,760 --> 00:10:10,740
make the decision and you can't feel

212
00:10:10,740 --> 00:10:13,800
guilty about their decision so

213
00:10:13,800 --> 00:10:15,860
um so that is

214
00:10:15,860 --> 00:10:19,740
all you can really do is inform them and

215
00:10:19,740 --> 00:10:22,500
then you know document your

216
00:10:22,500 --> 00:10:25,200
recommendations and make sure that

217
00:10:25,200 --> 00:10:26,820
they're recommend you know that their

218
00:10:26,820 --> 00:10:29,220
decisions are documented next we're

219
00:10:29,220 --> 00:10:30,720
going to go on to the self-protection

220
00:10:30,720 --> 00:10:32,700
level and the sun protection level

221
00:10:32,700 --> 00:10:36,080
really comes out of Milton Friedman

222
00:10:36,080 --> 00:10:39,360
who wrote what does it mean to say that

223
00:10:39,360 --> 00:10:41,160
the corporate executive has a social

224
00:10:41,160 --> 00:10:43,920
responsibility in its capacity city as a

225
00:10:43,920 --> 00:10:46,500
businessman if this statement is not

226
00:10:46,500 --> 00:10:48,720
your rhetoric it must mean that he is to

227
00:10:48,720 --> 00:10:50,579
act in some way that is not in the

228
00:10:50,579 --> 00:10:53,700
interest of his employers for example

229
00:10:53,700 --> 00:10:55,560
that he is to refrain from increasing

230
00:10:55,560 --> 00:10:57,240
the price of a product in order to

231
00:10:57,240 --> 00:10:59,519
contribute to the social objective of

232
00:10:59,519 --> 00:11:01,500
preventing inflation even though a price

233
00:11:01,500 --> 00:11:04,200
increase would be the best interest of

234
00:11:04,200 --> 00:11:06,899
the corporation or that he used to make

235
00:11:06,899 --> 00:11:09,600
expenditures on reducing pollution

236
00:11:09,600 --> 00:11:11,700
beyond the amount that is in the best

237
00:11:11,700 --> 00:11:13,980
interest of the corporation or that is

238
00:11:13,980 --> 00:11:15,959
required by law in order to contribute

239
00:11:15,959 --> 00:11:18,300
to the social objective of improving the

240
00:11:18,300 --> 00:11:19,440
environment

241
00:11:19,440 --> 00:11:23,300
now we know that

242
00:11:23,760 --> 00:11:26,399
um a lot of organizations now are

243
00:11:26,399 --> 00:11:28,140
concerned about the environment they are

244
00:11:28,140 --> 00:11:30,180
spending money beyond what they need to

245
00:11:30,180 --> 00:11:33,779
for um uh for

246
00:11:33,779 --> 00:11:34,500
um

247
00:11:34,500 --> 00:11:38,399
too due to regulation so basically you

248
00:11:38,399 --> 00:11:42,060
know this is uh one popular ideology but

249
00:11:42,060 --> 00:11:45,120
a number of organizations go beyond it

250
00:11:45,120 --> 00:11:47,399
the self-protection level obviously you

251
00:11:47,399 --> 00:11:50,579
need to convince management of

252
00:11:50,579 --> 00:11:53,100
why they need to spend on security and

253
00:11:53,100 --> 00:11:57,240
so it's a good idea to to look at how

254
00:11:57,240 --> 00:11:59,760
it's going to protect them on average

255
00:11:59,760 --> 00:12:02,459
five percent of Revenue is lost annually

256
00:12:02,459 --> 00:12:07,200
due to fraud that is across decades and

257
00:12:07,200 --> 00:12:11,899
decades it is also across countries

258
00:12:11,899 --> 00:12:15,060
that it's you know at least that much so

259
00:12:15,060 --> 00:12:17,459
effectively

260
00:12:17,459 --> 00:12:19,140
um uh and there's some additional

261
00:12:19,140 --> 00:12:21,540
statistics there but the key practices

262
00:12:21,540 --> 00:12:24,600
here at the higher level are trained to

263
00:12:24,600 --> 00:12:26,820
evaluate fraud security business risk

264
00:12:26,820 --> 00:12:31,019
manage for organizational sustainability

265
00:12:31,019 --> 00:12:33,600
um and there I don't mean Environmental

266
00:12:33,600 --> 00:12:35,519
um it calculate quantitative risk

267
00:12:35,519 --> 00:12:38,040
analysis for the organization so let's

268
00:12:38,040 --> 00:12:40,519
look at some of the specifics under that

269
00:12:40,519 --> 00:12:43,680
so the first is provide an anonymous

270
00:12:43,680 --> 00:12:45,959
recording mechanism for ethical

271
00:12:45,959 --> 00:12:49,339
violations and basically again another

272
00:12:49,339 --> 00:12:52,800
statistic that shows up again and again

273
00:12:52,800 --> 00:12:56,220
year after year across continents is

274
00:12:56,220 --> 00:12:59,639
that the most popular way of finding

275
00:12:59,639 --> 00:13:02,519
fraud is to have some way of getting

276
00:13:02,519 --> 00:13:06,740
tips okay so uh so basically people

277
00:13:06,740 --> 00:13:09,019
including

278
00:13:09,019 --> 00:13:13,800
customers vendors and employees really

279
00:13:13,800 --> 00:13:16,320
want some Anonymous way of reporting

280
00:13:16,320 --> 00:13:19,320
tips either Anonymous or they can be in

281
00:13:19,320 --> 00:13:23,459
person so so this is the most popular

282
00:13:23,459 --> 00:13:26,779
way of detecting or you know

283
00:13:26,779 --> 00:13:29,279
potentially yeah well I'm detecting

284
00:13:29,279 --> 00:13:30,420
fraud

285
00:13:30,420 --> 00:13:33,440
the next is develop a code of ethics

286
00:13:33,440 --> 00:13:35,579
addressing organizational and

287
00:13:35,579 --> 00:13:38,399
sustainability one of the things that I

288
00:13:38,399 --> 00:13:40,680
think is really interesting is the C

289
00:13:40,680 --> 00:13:42,420
risk manual

290
00:13:42,420 --> 00:13:45,000
um see risks being

291
00:13:45,000 --> 00:13:46,920
um

292
00:13:46,920 --> 00:13:50,100
certification for risk and information

293
00:13:50,100 --> 00:13:53,160
assistance controls they explicitly

294
00:13:53,160 --> 00:13:55,320
recognize that management may choose to

295
00:13:55,320 --> 00:13:57,000
ignore regulation when the cost of

296
00:13:57,000 --> 00:14:01,399
compliance exceeds the law spines or or

297
00:14:01,399 --> 00:14:05,220
Consequences which basically means that

298
00:14:05,220 --> 00:14:05,820
um

299
00:14:05,820 --> 00:14:08,820
uh even though the sea risk manual does

300
00:14:08,820 --> 00:14:12,240
talk about how you should handle

301
00:14:12,240 --> 00:14:13,760
um

302
00:14:13,760 --> 00:14:17,519
regulation they recognize that evidently

303
00:14:17,519 --> 00:14:21,000
it is perfectly okay not to adhere to it

304
00:14:21,000 --> 00:14:23,880
um so unfortunately with no code of

305
00:14:23,880 --> 00:14:24,920
ethics

306
00:14:24,920 --> 00:14:27,540
basically each employee's home moral

307
00:14:27,540 --> 00:14:29,060
behavior

308
00:14:29,060 --> 00:14:31,620
is going to equal their business and

309
00:14:31,620 --> 00:14:33,600
World Behavior so at that point you need

310
00:14:33,600 --> 00:14:36,240
some kind of code of ethics now at the

311
00:14:36,240 --> 00:14:39,480
at this level okay which is you know the

312
00:14:39,480 --> 00:14:41,880
self-protection a lot of the code of

313
00:14:41,880 --> 00:14:44,180
ethics attempts to

314
00:14:44,180 --> 00:14:47,639
basically protect management so it'll

315
00:14:47,639 --> 00:14:49,019
say something like members should

316
00:14:49,019 --> 00:14:50,880
promote Management's understanding of

317
00:14:50,880 --> 00:14:52,740
information processing methods and

318
00:14:52,740 --> 00:14:55,380
procedures members owe trust to their

319
00:14:55,380 --> 00:14:57,060
employers to Garden the employees

320
00:14:57,060 --> 00:14:59,820
interest the employer's interests and to

321
00:14:59,820 --> 00:15:04,100
advise the employers wisely and honestly

322
00:15:04,199 --> 00:15:06,660
the the next is pricing insurance with

323
00:15:06,660 --> 00:15:09,060
discounts for controls

324
00:15:09,060 --> 00:15:11,579
um so basically uh you know obviously

325
00:15:11,579 --> 00:15:17,160
one risk mechanism uh to handle risk one

326
00:15:17,160 --> 00:15:20,040
treatment to handle risk is to use

327
00:15:20,040 --> 00:15:23,160
Insurance to purchase insurance and what

328
00:15:23,160 --> 00:15:25,079
does Insurance do well it protects you

329
00:15:25,079 --> 00:15:26,579
but it means that you don't have to

330
00:15:26,579 --> 00:15:29,279
implement any controls in other words it

331
00:15:29,279 --> 00:15:31,560
doesn't make the problem any any better

332
00:15:31,560 --> 00:15:35,399
at all okay so I'll just give an example

333
00:15:35,399 --> 00:15:38,040
of this because I was interested in this

334
00:15:38,040 --> 00:15:42,420
ethical risk concept I wrote a paper on

335
00:15:42,420 --> 00:15:46,019
um on school on school shootings and uh

336
00:15:46,019 --> 00:15:47,940
and basically looked at what is the

337
00:15:47,940 --> 00:15:50,399
probability that a school shooting is

338
00:15:50,399 --> 00:15:51,440
going to happen

339
00:15:51,440 --> 00:15:55,320
and then what is the impact if it does

340
00:15:55,320 --> 00:15:57,540
happen and believe it or not

341
00:15:57,540 --> 00:16:00,060
um and I and I use various ethical you

342
00:16:00,060 --> 00:16:02,519
know I kind of use this to say well

343
00:16:02,519 --> 00:16:04,680
let's look at you know if we find it

344
00:16:04,680 --> 00:16:07,639
this way or that way or some other way

345
00:16:07,639 --> 00:16:10,740
how how would it turn out well it turns

346
00:16:10,740 --> 00:16:14,160
out that even large universities really

347
00:16:14,160 --> 00:16:17,459
large universities should spend about at

348
00:16:17,459 --> 00:16:19,019
most three thousand dollars a year to

349
00:16:19,019 --> 00:16:22,260
protect uh protect students and and that

350
00:16:22,260 --> 00:16:24,720
includes the most ethical you know

351
00:16:24,720 --> 00:16:27,839
standards for risks so basically

352
00:16:27,839 --> 00:16:30,720
um with less it might be you know below

353
00:16:30,720 --> 00:16:31,860
a thousand

354
00:16:31,860 --> 00:16:34,260
um and uh fortunately I worked at

355
00:16:34,260 --> 00:16:36,500
University of Wisconsin Parkside where

356
00:16:36,500 --> 00:16:39,120
where we can lock the doors from the

357
00:16:39,120 --> 00:16:42,360
inside of each of the classrooms and

358
00:16:42,360 --> 00:16:43,860
they spend a heck of a lot more than

359
00:16:43,860 --> 00:16:45,420
three thousand dollars to accomplish

360
00:16:45,420 --> 00:16:47,940
that so I'm internally grateful that

361
00:16:47,940 --> 00:16:51,060
they didn't do a good risk quantitative

362
00:16:51,060 --> 00:16:53,100
risk for that

363
00:16:53,100 --> 00:16:54,360
and now we're going to go into

364
00:16:54,360 --> 00:16:56,459
compliance concern and compliance

365
00:16:56,459 --> 00:17:00,779
concern is is obviously concerned with

366
00:17:00,779 --> 00:17:03,839
with regulation and courts and that kind

367
00:17:03,839 --> 00:17:08,579
of thing so um Economist Ronald Coast is

368
00:17:08,579 --> 00:17:10,439
the person that I'm talking about here

369
00:17:10,439 --> 00:17:12,599
and one of the things that I thought was

370
00:17:12,599 --> 00:17:15,000
interesting about what he said is that

371
00:17:15,000 --> 00:17:16,740
there's really two sides to every story

372
00:17:16,740 --> 00:17:20,339
and this is true for regulation and for

373
00:17:20,339 --> 00:17:23,699
um for like lawsuits okay and that is is

374
00:17:23,699 --> 00:17:27,480
that uh one side wants to do business

375
00:17:27,480 --> 00:17:29,160
and the other side doesn't want to get

376
00:17:29,160 --> 00:17:33,780
hurt and one possibility is that if you

377
00:17:33,780 --> 00:17:38,880
take away this then then the person who

378
00:17:38,880 --> 00:17:40,200
doesn't want to get hurt doesn't get

379
00:17:40,200 --> 00:17:42,179
hurt but on the other hand you really

380
00:17:42,179 --> 00:17:45,720
hurt them the um the employer you know

381
00:17:45,720 --> 00:17:48,240
kind of deal and the other possibility

382
00:17:48,240 --> 00:17:51,660
is that you just help the employer

383
00:17:51,660 --> 00:17:55,220
um and and potentially really hurt the

384
00:17:55,220 --> 00:17:57,419
customers of the the people in the

385
00:17:57,419 --> 00:17:59,460
society and so there needs to be a

386
00:17:59,460 --> 00:18:01,220
balance between the two

387
00:18:01,220 --> 00:18:05,100
which I I thought was interesting in in

388
00:18:05,100 --> 00:18:07,260
him pointing that out

389
00:18:07,260 --> 00:18:10,080
now I think with compliance concern in

390
00:18:10,080 --> 00:18:12,539
order to raise your management to that

391
00:18:12,539 --> 00:18:15,059
level if they are not uh you kind of

392
00:18:15,059 --> 00:18:16,919
have to really talk dollar figures

393
00:18:16,919 --> 00:18:19,500
because dollar figures speak to

394
00:18:19,500 --> 00:18:22,200
management and so I put some figures

395
00:18:22,200 --> 00:18:25,080
together Capital One was fined 80

396
00:18:25,080 --> 00:18:26,179
million

397
00:18:26,179 --> 00:18:29,340
Wendy's was to find 50 million and this

398
00:18:29,340 --> 00:18:31,160
is regulatory okay

399
00:18:31,160 --> 00:18:35,220
Texas paid 3.2 million Texas hospital

400
00:18:35,220 --> 00:18:39,840
Target made 18.5 million and Google was

401
00:18:39,840 --> 00:18:44,880
buying 50 million euros with gdpr so if

402
00:18:44,880 --> 00:18:47,340
that doesn't get people's attention

403
00:18:47,340 --> 00:18:49,740
um I'm not sure what would but

404
00:18:49,740 --> 00:18:53,340
effectively key practices at the highest

405
00:18:53,340 --> 00:18:55,860
level include a train for compliance

406
00:18:55,860 --> 00:18:58,440
risk value legal endurance within

407
00:18:58,440 --> 00:19:01,580
management address regulation fully

408
00:19:01,580 --> 00:19:04,500
consider legal responsibility Beyond

409
00:19:04,500 --> 00:19:07,200
regulation and follow software standard

410
00:19:07,200 --> 00:19:09,480
units for Quality Security and safety

411
00:19:09,480 --> 00:19:12,200
and we'll go into some of those

412
00:19:12,200 --> 00:19:15,240
aspects so the first one is lead

413
00:19:15,240 --> 00:19:18,299
ethically in other words management has

414
00:19:18,299 --> 00:19:19,520
to

415
00:19:19,520 --> 00:19:23,039
exemplify what they want from their

416
00:19:23,039 --> 00:19:25,679
employee employees so management should

417
00:19:25,679 --> 00:19:28,679
exemplify train for award and promote

418
00:19:28,679 --> 00:19:30,740
good ethical behavior and correct bad

419
00:19:30,740 --> 00:19:33,299
behaviors both in person in the training

420
00:19:33,299 --> 00:19:34,320
room so

421
00:19:34,320 --> 00:19:38,640
and um I as you may notice I actually

422
00:19:38,640 --> 00:19:42,539
went a little bit into the business as

423
00:19:42,539 --> 00:19:44,539
well as the security

424
00:19:44,539 --> 00:19:47,280
stuff in this in this paper and then

425
00:19:47,280 --> 00:19:50,299
talk about threat but the fsgo is is

426
00:19:50,299 --> 00:19:53,700
regulation relating to business ethics

427
00:19:53,700 --> 00:19:55,740
and they require that a high level

428
00:19:55,740 --> 00:19:58,980
management position or person be

429
00:19:58,980 --> 00:20:01,320
assigned overall overall responsibility

430
00:20:01,320 --> 00:20:03,419
for compliance and ethics in an

431
00:20:03,419 --> 00:20:05,840
organization

432
00:20:05,840 --> 00:20:09,539
the next is design assign ethical risk

433
00:20:09,539 --> 00:20:12,600
accountability so basically

434
00:20:12,600 --> 00:20:15,240
um you know how how does this happen in

435
00:20:15,240 --> 00:20:17,700
an organization and let's

436
00:20:17,700 --> 00:20:19,620
um start with the executives they have

437
00:20:19,620 --> 00:20:22,140
to establish a culture of safety the

438
00:20:22,140 --> 00:20:24,480
project managers must ensure safety is

439
00:20:24,480 --> 00:20:26,700
given due consideration and then

440
00:20:26,700 --> 00:20:29,480
developers also have some responsibility

441
00:20:29,480 --> 00:20:32,400
they have to identify compliance or

442
00:20:32,400 --> 00:20:35,039
moral problems collect facts and

443
00:20:35,039 --> 00:20:37,919
evaluate alternative Solutions implement

444
00:20:37,919 --> 00:20:40,080
the solution and monitor results as

445
00:20:40,080 --> 00:20:43,740
feedback now users also are involved in

446
00:20:43,740 --> 00:20:45,840
this they are responsible for their

447
00:20:45,840 --> 00:20:48,600
actions and may need training for proper

448
00:20:48,600 --> 00:20:52,140
product operation

449
00:20:52,140 --> 00:20:55,380
okay the next is addressing regulation

450
00:20:55,380 --> 00:20:57,360
and

451
00:20:57,360 --> 00:21:00,780
um and there's various aspects to this

452
00:21:00,780 --> 00:21:03,480
um so the first thing is

453
00:21:03,480 --> 00:21:05,100
um and this is basically from a secret

454
00:21:05,100 --> 00:21:07,440
more than anything else uh developed by

455
00:21:07,440 --> 00:21:09,299
compliance oriented business impact

456
00:21:09,299 --> 00:21:12,240
analysis for privacy regulation and

457
00:21:12,240 --> 00:21:14,520
privacy impact analysis for example so

458
00:21:14,520 --> 00:21:17,100
you need to develop scenarios to

459
00:21:17,100 --> 00:21:19,140
demonstrate how the regulation affects

460
00:21:19,140 --> 00:21:20,880
the business the scenarios are the

461
00:21:20,880 --> 00:21:23,340
stories quantify the cost of

462
00:21:23,340 --> 00:21:25,740
non-adherent to regulation consider

463
00:21:25,740 --> 00:21:28,140
civil law and third-party liabilities

464
00:21:28,140 --> 00:21:30,980
such as contracts intellectual heart

465
00:21:30,980 --> 00:21:33,860
property and privacy

466
00:21:33,860 --> 00:21:37,039
and and to evaluate in international

467
00:21:37,039 --> 00:21:41,280
regulations as well since many companies

468
00:21:41,280 --> 00:21:43,580
operate in multiple countries

469
00:21:43,580 --> 00:21:45,240
[Music]

470
00:21:45,240 --> 00:21:49,200
at any time and and the next concern is

471
00:21:49,200 --> 00:21:51,960
Rule versus the intent of the law one of

472
00:21:51,960 --> 00:21:55,140
the things that Friedman will say later

473
00:21:55,140 --> 00:21:58,020
on in the presentation is that when

474
00:21:58,020 --> 00:22:00,539
regulation needs to be Rewritten that is

475
00:22:00,539 --> 00:22:02,840
a failure of management

476
00:22:02,840 --> 00:22:05,520
because obviously they didn't do it well

477
00:22:05,520 --> 00:22:07,740
enough so

478
00:22:07,740 --> 00:22:09,200
the next

479
00:22:09,200 --> 00:22:11,880
which really kind of goes into that

480
00:22:11,880 --> 00:22:14,520
lawsuit aspect is evaluated product

481
00:22:14,520 --> 00:22:16,320
liability

482
00:22:16,320 --> 00:22:19,980
and one of the major areas right now

483
00:22:19,980 --> 00:22:23,460
that are ripe for liability issues is

484
00:22:23,460 --> 00:22:26,700
artificial intelligence and artificial

485
00:22:26,700 --> 00:22:30,960
intelligence right now often trains on a

486
00:22:30,960 --> 00:22:34,340
biased manual systems and so effectively

487
00:22:34,340 --> 00:22:38,039
what they turn out is is biased

488
00:22:38,039 --> 00:22:40,860
decisions and those biased decisions are

489
00:22:40,860 --> 00:22:42,600
occurring in criminal justice Health

490
00:22:42,600 --> 00:22:45,059
Care employment and education and

491
00:22:45,059 --> 00:22:48,620
basically this perpetuates systemic harm

492
00:22:48,620 --> 00:22:51,480
causing long-term maltreatment that

493
00:22:51,480 --> 00:22:53,520
basically affects people's lives and

494
00:22:53,520 --> 00:22:56,580
deaths uh so

495
00:22:56,580 --> 00:22:59,220
um uh effectively

496
00:22:59,220 --> 00:23:00,419
um

497
00:23:00,419 --> 00:23:03,419
uh what what should happen from an

498
00:23:03,419 --> 00:23:05,100
engineering this perspective is to

499
00:23:05,100 --> 00:23:07,919
consider how the algorithm will likely

500
00:23:07,919 --> 00:23:11,700
impact special groups is it accurate and

501
00:23:11,700 --> 00:23:15,000
fair uh in other words I know in one

502
00:23:15,000 --> 00:23:18,059
study they they basically

503
00:23:18,059 --> 00:23:21,980
looked at the rate of recidivism

504
00:23:21,980 --> 00:23:27,310
for people who uh

505
00:23:27,310 --> 00:23:28,740
[Music]

506
00:23:28,740 --> 00:23:31,799
at the basement were by the judicial

507
00:23:31,799 --> 00:23:34,440
system and whether people should be able

508
00:23:34,440 --> 00:23:38,820
to be put on parole and they found out

509
00:23:38,820 --> 00:23:41,280
the number one factor was your race as

510
00:23:41,280 --> 00:23:43,620
to whether you got the um

511
00:23:43,620 --> 00:23:45,059
uh

512
00:23:45,059 --> 00:23:47,419
uh whether you've got the parole or not

513
00:23:47,419 --> 00:23:50,100
so consider how the algorithm will

514
00:23:50,100 --> 00:23:52,260
likely impact special groups is it fair

515
00:23:52,260 --> 00:23:54,780
accurate and fair observe use in

516
00:23:54,780 --> 00:23:57,120
implementation evaluating greater

517
00:23:57,120 --> 00:23:59,460
fairness on special groups so after you

518
00:23:59,460 --> 00:24:01,380
before you walk moment to look at the

519
00:24:01,380 --> 00:24:03,659
rate after you were implement it see how

520
00:24:03,659 --> 00:24:06,120
it's handled and then publicize how the

521
00:24:06,120 --> 00:24:08,039
algorithm Works in order to promote

522
00:24:08,039 --> 00:24:10,140
transparency

523
00:24:10,140 --> 00:24:13,380
manage projects responsibly those people

524
00:24:13,380 --> 00:24:15,740
who are in development know that

525
00:24:15,740 --> 00:24:18,919
sometimes the most

526
00:24:18,919 --> 00:24:22,700
serious concern is schedules schedules

527
00:24:22,700 --> 00:24:26,120
schedules and unfortunately

528
00:24:26,120 --> 00:24:30,299
this may lead to a series of lies and

529
00:24:30,299 --> 00:24:33,080
bad decisions

530
00:24:33,080 --> 00:24:35,760
with products being delivered late or

531
00:24:35,760 --> 00:24:38,760
missing features or products are lacking

532
00:24:38,760 --> 00:24:42,659
quality and or documentation so

533
00:24:42,659 --> 00:24:43,919
um

534
00:24:43,919 --> 00:24:47,100
so it is important to manage those

535
00:24:47,100 --> 00:24:49,340
projects responsibly in a way to do that

536
00:24:49,340 --> 00:24:53,299
is is realistic project management

537
00:24:53,299 --> 00:24:55,559
comprehensive risk management good

538
00:24:55,559 --> 00:24:57,360
communication quality software

539
00:24:57,360 --> 00:24:59,700
development standards a code of conduct

540
00:24:59,700 --> 00:25:02,880
and ethical training

541
00:25:02,880 --> 00:25:05,659
develop and follow soft law

542
00:25:05,659 --> 00:25:08,780
so in the legal

543
00:25:08,780 --> 00:25:13,380
situation basically if if you can show

544
00:25:13,380 --> 00:25:15,780
that you you know have adeared to the

545
00:25:15,780 --> 00:25:16,580
law

546
00:25:16,580 --> 00:25:21,179
and particularly its intent then

547
00:25:21,179 --> 00:25:22,740
um you know when bad things happen to

548
00:25:22,740 --> 00:25:26,159
you anyway you get off a lot easier if

549
00:25:26,159 --> 00:25:29,880
not entirely and in many places there is

550
00:25:29,880 --> 00:25:33,179
no law so one way that companies

551
00:25:33,179 --> 00:25:37,200
basically make sure that they um

552
00:25:37,200 --> 00:25:40,320
uh can protect themselves is to develop

553
00:25:40,320 --> 00:25:44,159
salt salt Rod now some of law is is not

554
00:25:44,159 --> 00:25:45,840
law that's

555
00:25:45,840 --> 00:25:48,000
um that's regulated by legislators but

556
00:25:48,000 --> 00:25:49,740
basically it's industry groups that get

557
00:25:49,740 --> 00:25:51,539
together that say this is what equality

558
00:25:51,539 --> 00:25:54,900
standard is and so there's a number of

559
00:25:54,900 --> 00:25:56,940
different soft laws that have been

560
00:25:56,940 --> 00:25:58,559
developed for this purpose a number of

561
00:25:58,559 --> 00:26:00,539
organizations

562
00:26:00,539 --> 00:26:03,720
um I'll participate in and they're

563
00:26:03,720 --> 00:26:07,460
listed they're listed here

564
00:26:09,539 --> 00:26:13,919
configure software for policy choice and

565
00:26:13,919 --> 00:26:16,500
um and and basically

566
00:26:16,500 --> 00:26:17,700
um

567
00:26:17,700 --> 00:26:20,299
when you're developing software

568
00:26:20,299 --> 00:26:22,980
very often you may have one customer in

569
00:26:22,980 --> 00:26:25,020
mind for who you're planning to sell

570
00:26:25,020 --> 00:26:28,140
this to but effectively

571
00:26:28,140 --> 00:26:29,940
um you know the worldwide

572
00:26:29,940 --> 00:26:31,159
[Music]

573
00:26:31,159 --> 00:26:33,600
implementation of this or shall we say

574
00:26:33,600 --> 00:26:36,360
being able to sell worldwide is very

575
00:26:36,360 --> 00:26:39,539
attractive but a lot of Nations have a

576
00:26:39,539 --> 00:26:41,279
lot of different regulations so if you

577
00:26:41,279 --> 00:26:43,740
can configure the software for the

578
00:26:43,740 --> 00:26:46,400
particular regulations it means that

579
00:26:46,400 --> 00:26:49,940
that basically you may be able to sell

580
00:26:49,940 --> 00:26:52,400
sell more

581
00:26:52,400 --> 00:26:55,980
which actually adheres to the regulation

582
00:26:55,980 --> 00:26:58,320
as well as what the companies are

583
00:26:58,320 --> 00:26:59,820
interested in so if you talk to

584
00:26:59,820 --> 00:27:01,080
different companies they may have

585
00:27:01,080 --> 00:27:03,600
different standards and and if you can

586
00:27:03,600 --> 00:27:05,760
config if you make those different

587
00:27:05,760 --> 00:27:08,880
options configurable then at that point

588
00:27:08,880 --> 00:27:11,100
uh there you know this is a good thing

589
00:27:11,100 --> 00:27:15,059
for for adherential regulation

590
00:27:15,059 --> 00:27:17,159
next we're going to go into stakeholder

591
00:27:17,159 --> 00:27:22,200
concern and and this is by Edward

592
00:27:22,200 --> 00:27:23,880
Freeman

593
00:27:23,880 --> 00:27:27,620
um and he basically

594
00:27:27,620 --> 00:27:31,380
said in one of his books so even if the

595
00:27:31,380 --> 00:27:33,120
ideologues who insist that the only

596
00:27:33,120 --> 00:27:36,240
legitimate purpose of a business is to

597
00:27:36,240 --> 00:27:38,880
maximize shareholder value or maximize

598
00:27:38,880 --> 00:27:41,760
profits the only way to do that is to

599
00:27:41,760 --> 00:27:43,679
create great project product products

600
00:27:43,679 --> 00:27:47,600
and service that customers want to buy

601
00:27:47,600 --> 00:27:51,659
he also said that deception erodes trust

602
00:27:51,659 --> 00:27:53,460
and trust is required for economic

603
00:27:53,460 --> 00:27:55,820
transactions business management

604
00:27:55,820 --> 00:27:58,140
responsibility for the effects of their

605
00:27:58,140 --> 00:28:00,539
of their actions including defending

606
00:28:00,539 --> 00:28:04,640
themselves to 60 Minutes of reporters

607
00:28:04,640 --> 00:28:08,460
what new regulation or when new

608
00:28:08,460 --> 00:28:10,980
regulation or litigation arises the

609
00:28:10,980 --> 00:28:12,539
implication is Business Management

610
00:28:12,539 --> 00:28:15,000
failure

611
00:28:15,000 --> 00:28:19,440
uh so if we um so basically the idea of

612
00:28:19,440 --> 00:28:23,340
stakeholder concern is uh is that we're

613
00:28:23,340 --> 00:28:25,320
concerned with

614
00:28:25,320 --> 00:28:28,080
um uh with our how our customers feel

615
00:28:28,080 --> 00:28:30,960
you know uh you know that we're that

616
00:28:30,960 --> 00:28:33,299
we're making our customers happy

617
00:28:33,299 --> 00:28:35,940
um our employees want to be loyal to us

618
00:28:35,940 --> 00:28:39,600
our vendors financiers

619
00:28:39,600 --> 00:28:41,760
um uh uh all

620
00:28:41,760 --> 00:28:44,400
um are interested in

621
00:28:44,400 --> 00:28:45,840
um you know and working we work well

622
00:28:45,840 --> 00:28:48,900
with them okay in a risk scenario I

623
00:28:48,900 --> 00:28:50,520
thought was really interesting was in

624
00:28:50,520 --> 00:28:53,330
the news and that is that

625
00:28:53,330 --> 00:28:53,760
[Music]

626
00:28:53,760 --> 00:28:54,440
um

627
00:28:54,440 --> 00:28:56,760
Russian hackers offered a

628
00:28:56,760 --> 00:28:59,340
russian-speaking Tesla employee one

629
00:28:59,340 --> 00:29:02,460
million dollar to install a ransomware

630
00:29:02,460 --> 00:29:06,179
attack and this Tesla employee turned it

631
00:29:06,179 --> 00:29:08,220
down and actually worked with law

632
00:29:08,220 --> 00:29:11,220
enforcement but certainly

633
00:29:11,220 --> 00:29:14,700
um you know if you're a corporation can

634
00:29:14,700 --> 00:29:17,520
you be be pretty well assured that all

635
00:29:17,520 --> 00:29:18,899
of your employees like you enough to

636
00:29:18,899 --> 00:29:22,399
turn down a one million dollar bribe

637
00:29:22,399 --> 00:29:24,720
I think that's a lot to ask for but

638
00:29:24,720 --> 00:29:27,720
still I think it raises the question

639
00:29:27,720 --> 00:29:30,000
so some of the key practices are learn

640
00:29:30,000 --> 00:29:32,039
the context of the business process and

641
00:29:32,039 --> 00:29:33,779
our product development and manage with

642
00:29:33,779 --> 00:29:36,240
the view toward all stakeholders discuss

643
00:29:36,240 --> 00:29:38,520
the qualitative impact of risk affecting

644
00:29:38,520 --> 00:29:41,220
all stakeholders stakeholders evaluate

645
00:29:41,220 --> 00:29:43,159
the impact of risk

646
00:29:43,159 --> 00:29:45,720
quantitatively inform communicate

647
00:29:45,720 --> 00:29:48,240
ethical risk to stakeholders and

648
00:29:48,240 --> 00:29:49,740
evaluate risk and supplement

649
00:29:49,740 --> 00:29:52,679
implementation for all stakeholders so

650
00:29:52,679 --> 00:29:55,200
let's go through some of those learn the

651
00:29:55,200 --> 00:29:57,600
context of the business process in our

652
00:29:57,600 --> 00:30:01,020
product development well very often

653
00:30:01,020 --> 00:30:04,260
uh if you if you talk to Engineers about

654
00:30:04,260 --> 00:30:07,140
what is a security concern

655
00:30:07,140 --> 00:30:09,779
um uh you know there's only a few things

656
00:30:09,779 --> 00:30:11,399
they will come up with which is

657
00:30:11,399 --> 00:30:14,700
basically malware firewalls

658
00:30:14,700 --> 00:30:16,679
um you know that kind of thing it's very

659
00:30:16,679 --> 00:30:18,840
limited they stay only within the

660
00:30:18,840 --> 00:30:21,179
security realm they don't look at the

661
00:30:21,179 --> 00:30:22,919
business realm at all so if you're

662
00:30:22,919 --> 00:30:24,360
dealing with health care they're not

663
00:30:24,360 --> 00:30:25,679
necessarily

664
00:30:25,679 --> 00:30:28,440
legal engineering for example that could

665
00:30:28,440 --> 00:30:31,679
happen within that business realm and um

666
00:30:31,679 --> 00:30:34,880
and so I I it's important for engineers

667
00:30:34,880 --> 00:30:37,740
to not just look you know to not just

668
00:30:37,740 --> 00:30:39,919
think okay it's it's only about

669
00:30:39,919 --> 00:30:42,679
firewalls and and

670
00:30:42,679 --> 00:30:45,539
intrusion detection and logs and that

671
00:30:45,539 --> 00:30:46,620
kind of thing

672
00:30:46,620 --> 00:30:49,320
so develop developers must understand

673
00:30:49,320 --> 00:30:51,179
the context legal and ethical

674
00:30:51,179 --> 00:30:53,580
requirements of the application they

675
00:30:53,580 --> 00:30:56,059
need to understand the complex social

676
00:30:56,059 --> 00:30:59,220
environment that includes their their

677
00:30:59,220 --> 00:31:01,500
clients their users the customers and

678
00:31:01,500 --> 00:31:04,140
the business and as well as

679
00:31:04,140 --> 00:31:06,899
understanding the things related to

680
00:31:06,899 --> 00:31:09,059
um to the software specialty in

681
00:31:09,059 --> 00:31:10,799
including security and safety

682
00:31:10,799 --> 00:31:13,020
implications which of course many

683
00:31:13,020 --> 00:31:15,600
developers don't even get that far

684
00:31:15,600 --> 00:31:18,059
so the next thing is as the world

685
00:31:18,059 --> 00:31:20,279
changes a different different ethical

686
00:31:20,279 --> 00:31:23,159
decisions may arise your um yearly so

687
00:31:23,159 --> 00:31:27,020
that becomes important as well

688
00:31:27,179 --> 00:31:30,419
the next issue is personalized risk and

689
00:31:30,419 --> 00:31:33,360
one of the things I was thinking about

690
00:31:33,360 --> 00:31:36,740
last night was

691
00:31:36,740 --> 00:31:41,460
was that most value of Life estimates

692
00:31:41,460 --> 00:31:44,820
often come out of what a person will

693
00:31:44,820 --> 00:31:47,820
earn through the rest of their through

694
00:31:47,820 --> 00:31:50,159
the rest of their life and so I said

695
00:31:50,159 --> 00:31:53,399
well okay let's let's just take

696
00:31:53,399 --> 00:31:56,340
um you know fifty thousand dollars which

697
00:31:56,340 --> 00:31:59,760
I think most people in this room if

698
00:31:59,760 --> 00:32:01,860
they're not making more than that then

699
00:32:01,860 --> 00:32:04,860
they will be very soon but effectively

700
00:32:04,860 --> 00:32:07,520
you know the average American probably

701
00:32:07,520 --> 00:32:09,919
might be a little closer to that range

702
00:32:09,919 --> 00:32:13,440
fifty thousand dollars 40 years is

703
00:32:13,440 --> 00:32:15,419
basically two million dollars so that

704
00:32:15,419 --> 00:32:19,559
puts the the value of Life at about two

705
00:32:19,559 --> 00:32:21,320
million dollars for the average American

706
00:32:21,320 --> 00:32:24,360
is there anyone in this room who would

707
00:32:24,360 --> 00:32:27,899
consider their life as being that value

708
00:32:27,899 --> 00:32:30,899
interested two million dollars okay

709
00:32:30,899 --> 00:32:33,720
um not at all I'm sure you know everyone

710
00:32:33,720 --> 00:32:35,760
is well about that and in fact actually

711
00:32:35,760 --> 00:32:39,559
when Studies have been done

712
00:32:39,559 --> 00:32:42,320
where you ask people to work on

713
00:32:42,320 --> 00:32:45,240
skyscapers for example how much are you

714
00:32:45,240 --> 00:32:47,580
willing to you know how much do you need

715
00:32:47,580 --> 00:32:51,600
to to earn in order to build on some of

716
00:32:51,600 --> 00:32:54,360
this on the some of the skyscrapers in

717
00:32:54,360 --> 00:32:56,279
the in the building of that

718
00:32:56,279 --> 00:32:57,380
um

719
00:32:57,380 --> 00:33:01,140
they've they can come to other estimates

720
00:33:01,140 --> 00:33:04,640
of what how people value their life and

721
00:33:04,640 --> 00:33:08,580
so the U.S and EPA has estimated life at

722
00:33:08,580 --> 00:33:14,039
7.4 million and a study in 1984 dollars

723
00:33:14,039 --> 00:33:16,039
was two to eight million

724
00:33:16,039 --> 00:33:19,980
with one I think at 12 million okay so

725
00:33:19,980 --> 00:33:22,100
so basically

726
00:33:22,100 --> 00:33:24,779
one issue needs to be that we need to

727
00:33:24,779 --> 00:33:28,080
Value life realistically in these risk

728
00:33:28,080 --> 00:33:28,799
um

729
00:33:28,799 --> 00:33:32,279
uh risk it you know

730
00:33:32,279 --> 00:33:37,620
risk analysis quite a data risk analysis

731
00:33:37,620 --> 00:33:39,720
um the solution is really to include

732
00:33:39,720 --> 00:33:41,279
employees and customers in Risk

733
00:33:41,279 --> 00:33:44,340
decisions and um one issue I'll be

734
00:33:44,340 --> 00:33:45,659
talking more about is the public

735
00:33:45,659 --> 00:33:49,200
tendency risk as a binary moral decision

736
00:33:49,200 --> 00:33:54,200
not as a probability so that should be

737
00:33:54,740 --> 00:33:57,240
something to be aware of

738
00:33:57,240 --> 00:34:00,120
evaluate trade-offs of concern so any

739
00:34:00,120 --> 00:34:02,880
any kind of decision including

740
00:34:02,880 --> 00:34:06,059
development or for that matter business

741
00:34:06,059 --> 00:34:08,460
business decision you have to make

742
00:34:08,460 --> 00:34:11,520
trade-offs between different values so

743
00:34:11,520 --> 00:34:15,659
on the right hand side we can see that

744
00:34:15,659 --> 00:34:17,820
particularly in development you might

745
00:34:17,820 --> 00:34:21,899
want to you have to get owner values and

746
00:34:21,899 --> 00:34:25,679
user values as well as then extract the

747
00:34:25,679 --> 00:34:27,239
process requirements and the product

748
00:34:27,239 --> 00:34:28,859
requirements

749
00:34:28,859 --> 00:34:31,739
for development on the left hand side we

750
00:34:31,739 --> 00:34:34,619
see a number of arrows and basically as

751
00:34:34,619 --> 00:34:37,379
we're working with the decision we can

752
00:34:37,379 --> 00:34:41,399
draw lines somewhere along uh this these

753
00:34:41,399 --> 00:34:44,339
arrows to say that yeah this should be

754
00:34:44,339 --> 00:34:47,040
you know this is this reliable but it

755
00:34:47,040 --> 00:34:49,679
needs to be more reliable that's this

756
00:34:49,679 --> 00:34:52,320
secure but it needs to be more secure

757
00:34:52,320 --> 00:34:54,300
for example and then you can start

758
00:34:54,300 --> 00:34:57,420
trading off some of those values

759
00:34:57,420 --> 00:35:01,619
consider risk beyond the expected broad

760
00:35:01,619 --> 00:35:03,900
framing consider considers worst case

761
00:35:03,900 --> 00:35:07,859
scenarios and should be permitted to be

762
00:35:07,859 --> 00:35:10,320
considered and the vision zero principle

763
00:35:10,320 --> 00:35:14,520
considers how to control for no risk of

764
00:35:14,520 --> 00:35:17,220
death or severe harm

765
00:35:17,220 --> 00:35:20,700
uh evaluate the outrage Factor

766
00:35:20,700 --> 00:35:22,400
um the Sandman

767
00:35:22,400 --> 00:35:26,099
worked with risk where he says you know

768
00:35:26,099 --> 00:35:30,380
normal risk is likelihood times impact

769
00:35:30,380 --> 00:35:34,260
but what if we act add in an outrage

770
00:35:34,260 --> 00:35:37,619
Factor okay so now risk is equal to the

771
00:35:37,619 --> 00:35:40,099
annual loss expectancy is equal to

772
00:35:40,099 --> 00:35:43,320
likelihood times implant impact plus

773
00:35:43,320 --> 00:35:47,640
outrage and outrage is the employee at

774
00:35:47,640 --> 00:35:49,640
is basically society's

775
00:35:49,640 --> 00:35:54,440
consideration of this and basically

776
00:35:54,440 --> 00:35:57,960
things that are not in control that

777
00:35:57,960 --> 00:36:00,900
affect human life and are morally

778
00:36:00,900 --> 00:36:03,180
problematic

779
00:36:03,180 --> 00:36:06,900
okay would tend to increase that value

780
00:36:06,900 --> 00:36:10,280
of the outrage in in the Public's

781
00:36:10,280 --> 00:36:13,260
understanding and this is why after

782
00:36:13,260 --> 00:36:14,940
Three Mile Island

783
00:36:14,940 --> 00:36:15,720
um

784
00:36:15,720 --> 00:36:20,940
uh nuclear facilities uh were now built

785
00:36:20,940 --> 00:36:22,440
so

786
00:36:22,440 --> 00:36:24,000
um the public tends to focus on the

787
00:36:24,000 --> 00:36:25,680
moral aspects of risk including what

788
00:36:25,680 --> 00:36:28,079
risks are deemed acceptable and why what

789
00:36:28,079 --> 00:36:30,500
will happen if the risk actually occurs

790
00:36:30,500 --> 00:36:33,780
and providing facts and statistics does

791
00:36:33,780 --> 00:36:36,480
not solve the ethical dilemmas nor does

792
00:36:36,480 --> 00:36:39,480
it lower the controversy and public

793
00:36:39,480 --> 00:36:41,820
controversy is in fact an indicator of

794
00:36:41,820 --> 00:36:45,060
an unresolved World moral issue and I

795
00:36:45,060 --> 00:36:46,680
think if you look at the newspapers

796
00:36:46,680 --> 00:36:49,800
today you'll see a number of moral

797
00:36:49,800 --> 00:36:53,940
issues that are problematic where we're

798
00:36:53,940 --> 00:36:56,940
not agreeing on things

799
00:36:56,940 --> 00:36:59,460
so it is

800
00:36:59,460 --> 00:37:01,560
um I I think you know actually in the

801
00:37:01,560 --> 00:37:03,240
engineering World

802
00:37:03,240 --> 00:37:06,260
um if if you look at you know

803
00:37:06,260 --> 00:37:09,300
I basically look at IEEE you know a lot

804
00:37:09,300 --> 00:37:11,880
of those papers and uh the ones related

805
00:37:11,880 --> 00:37:14,640
to electrical engineering

806
00:37:14,640 --> 00:37:17,579
um and and Engineering in general uh

807
00:37:17,579 --> 00:37:19,140
basically

808
00:37:19,140 --> 00:37:20,700
um they're concerned about safety and

809
00:37:20,700 --> 00:37:22,440
they have ways of really building that

810
00:37:22,440 --> 00:37:25,859
in and of course that ends up going into

811
00:37:25,859 --> 00:37:27,960
regulation and standards and so on and

812
00:37:27,960 --> 00:37:29,000
so forth

813
00:37:29,000 --> 00:37:34,200
but in in software a lot of times you

814
00:37:34,200 --> 00:37:36,660
know insecurity doesn't make it and

815
00:37:36,660 --> 00:37:39,240
um and so effectively

816
00:37:39,240 --> 00:37:42,439
um uh

817
00:37:42,480 --> 00:37:46,619
you know we we don't sometimes Security

818
00:37:46,619 --> 00:37:49,980
will go in last and uh and effectively

819
00:37:49,980 --> 00:37:52,920
it uh in in a number of software

820
00:37:52,920 --> 00:37:54,660
development I would assume

821
00:37:54,660 --> 00:37:57,839
um that that's still true uh but so

822
00:37:57,839 --> 00:38:00,380
effectively

823
00:38:00,980 --> 00:38:03,300
what engineering does is really

824
00:38:03,300 --> 00:38:05,220
describes the inherent risk of customers

825
00:38:05,220 --> 00:38:08,339
and sells them a safer product and

826
00:38:08,339 --> 00:38:10,380
um and we need to do more of that and

827
00:38:10,380 --> 00:38:12,660
and the only way really to do that is to

828
00:38:12,660 --> 00:38:16,200
put good statistics together to sell the

829
00:38:16,200 --> 00:38:18,359
security in the product

830
00:38:18,359 --> 00:38:21,300
um you know to the owner so they

831
00:38:21,300 --> 00:38:23,220
understand the implications of it much

832
00:38:23,220 --> 00:38:25,859
better if the customer chooses a less

833
00:38:25,859 --> 00:38:27,960
safe product the computer the company

834
00:38:27,960 --> 00:38:31,320
refuses to transact or contractual agree

835
00:38:31,320 --> 00:38:32,700
agreements should absolve the

836
00:38:32,700 --> 00:38:35,820
engineering company of responsibility so

837
00:38:35,820 --> 00:38:38,339
the known liability is explicitly

838
00:38:38,339 --> 00:38:40,560
accepted by the customer and if they had

839
00:38:40,560 --> 00:38:42,359
to sign that contract then I think that

840
00:38:42,359 --> 00:38:45,660
might be a different different issue

841
00:38:45,660 --> 00:38:50,700
address risk in software so

842
00:38:51,079 --> 00:38:54,200
one one process for doing this

843
00:38:54,200 --> 00:38:57,300
goes to the following stages identify

844
00:38:57,300 --> 00:38:59,460
the process project type and its direct

845
00:38:59,460 --> 00:39:02,460
and extended stakeholders two identify

846
00:39:02,460 --> 00:39:05,160
the task per phase of this upper

847
00:39:05,160 --> 00:39:06,660
development project

848
00:39:06,660 --> 00:39:09,240
three use structured questions to

849
00:39:09,240 --> 00:39:11,220
develop a map of associations between

850
00:39:11,220 --> 00:39:13,500
each task and its related stakeholders

851
00:39:13,500 --> 00:39:16,380
and four analyze risk discovered by the

852
00:39:16,380 --> 00:39:18,660
mapping in order to articulate the

853
00:39:18,660 --> 00:39:21,000
severity of the project risks and the

854
00:39:21,000 --> 00:39:23,880
risk treatment strategy

855
00:39:23,880 --> 00:39:26,220
uh some of the important metrics related

856
00:39:26,220 --> 00:39:29,940
to this is how much in advance you can

857
00:39:29,940 --> 00:39:31,320
see that a failure is going to happen

858
00:39:31,320 --> 00:39:35,420
that's really the idea of certainty

859
00:39:35,420 --> 00:39:38,460
how well the the risk is understood and

860
00:39:38,460 --> 00:39:42,780
known controllability is your ability to

861
00:39:42,780 --> 00:39:45,839
control harm so something happens but

862
00:39:45,839 --> 00:39:47,880
can you control it after the you know

863
00:39:47,880 --> 00:39:52,200
after the fact and then severity is is

864
00:39:52,200 --> 00:39:54,480
basically what we're used to which is

865
00:39:54,480 --> 00:39:57,960
considers the level of harm done the

866
00:39:57,960 --> 00:40:01,160
scale and its future impact

867
00:40:01,160 --> 00:40:04,500
now what engineering does if you look at

868
00:40:04,500 --> 00:40:06,060
for example

869
00:40:06,060 --> 00:40:08,839
um I think this actually comes from

870
00:40:08,839 --> 00:40:10,859
quite a few years ago they may have

871
00:40:10,859 --> 00:40:13,619
changed it but safer cars for example

872
00:40:13,619 --> 00:40:16,680
they first developed safety goals and

873
00:40:16,680 --> 00:40:20,720
then from the safety goals they defined

874
00:40:20,720 --> 00:40:24,119
functional safety requirements and from

875
00:40:24,119 --> 00:40:26,460
the functional safety requirements they

876
00:40:26,460 --> 00:40:29,820
develop technical safety requirements so

877
00:40:29,820 --> 00:40:32,940
basically the technicals you know the as

878
00:40:32,940 --> 00:40:35,640
an industry group they're putting

879
00:40:35,640 --> 00:40:37,980
together functional safety requirements

880
00:40:37,980 --> 00:40:40,079
which then when it goes to the

881
00:40:40,079 --> 00:40:42,240
individual company those become

882
00:40:42,240 --> 00:40:45,300
technical safety requirements and then

883
00:40:45,300 --> 00:40:47,160
of course there's always verification

884
00:40:47,160 --> 00:40:51,780
which ensures compliance of the design

885
00:40:51,780 --> 00:40:53,820
now let's look at the highest level

886
00:40:53,820 --> 00:40:58,020
which is a societal concern and here

887
00:40:58,020 --> 00:41:00,619
um we're looking at

888
00:41:00,619 --> 00:41:05,220
a philosophical theories okay so so

889
00:41:05,220 --> 00:41:08,099
basically it includes utilitarianism

890
00:41:08,099 --> 00:41:10,280
Theory

891
00:41:10,280 --> 00:41:13,859
which tries to it act and basically it

892
00:41:13,859 --> 00:41:16,380
tries to make the most people happy as

893
00:41:16,380 --> 00:41:17,720
possible

894
00:41:17,720 --> 00:41:20,940
deontological ethics which is similar to

895
00:41:20,940 --> 00:41:23,700
The Golden Rule if you want to others as

896
00:41:23,700 --> 00:41:25,859
you would have them doing to you and

897
00:41:25,859 --> 00:41:27,660
what's important there is the motive for

898
00:41:27,660 --> 00:41:29,599
your actions

899
00:41:29,599 --> 00:41:32,460
and then there's virtue ethics which is

900
00:41:32,460 --> 00:41:34,020
concerned about the character of an

901
00:41:34,020 --> 00:41:38,160
anti-entity and on avoiding avoiding

902
00:41:38,160 --> 00:41:41,760
bikes so virtue ethics can also be apply

903
00:41:41,760 --> 00:41:43,760
to the organizational level

904
00:41:43,760 --> 00:41:47,640
by improving in internal organizational

905
00:41:47,640 --> 00:41:50,160
qualities

906
00:41:50,160 --> 00:41:54,240
uh so if we think about

907
00:41:54,240 --> 00:41:58,079
um I think this the scenarios here

908
00:41:58,079 --> 00:42:00,380
um I actually

909
00:42:00,380 --> 00:42:05,940
first did a part of this presentation to

910
00:42:05,940 --> 00:42:09,900
um some businessmen and the year I did

911
00:42:09,900 --> 00:42:11,420
it which was about two years ago

912
00:42:11,420 --> 00:42:15,180
California had just gone through some

913
00:42:15,180 --> 00:42:19,079
horrendous fires where 12 000 lightning

914
00:42:19,079 --> 00:42:21,780
strikes in three weeks sparked almost

915
00:42:21,780 --> 00:42:25,619
two dozen major fires and

916
00:42:25,619 --> 00:42:28,140
um you know five million Acres Burns

917
00:42:28,140 --> 00:42:31,400
destroyed homes thousands fleeted so

918
00:42:31,400 --> 00:42:34,200
most recently I've also talked to people

919
00:42:34,200 --> 00:42:36,660
from someone from Puerto Rico who

920
00:42:36,660 --> 00:42:38,280
basically has moved from there because

921
00:42:38,280 --> 00:42:40,460
Hurricane Katrina

922
00:42:40,460 --> 00:42:45,119
which was a long time ago now they just

923
00:42:45,119 --> 00:42:48,180
have not recovered they they just

924
00:42:48,180 --> 00:42:49,800
um you know they have not recovered and

925
00:42:49,800 --> 00:42:52,880
she eventually gave up and moved away

926
00:42:52,880 --> 00:42:57,420
so the key practices train and think in

927
00:42:57,420 --> 00:43:01,020
ethics manage considering the societal

928
00:43:01,020 --> 00:43:03,900
impact of decisions discuss the societal

929
00:43:03,900 --> 00:43:06,420
impact of risk qualitatively and

930
00:43:06,420 --> 00:43:08,460
evaluate the impact of risk

931
00:43:08,460 --> 00:43:11,220
quantitatively and let's go through some

932
00:43:11,220 --> 00:43:13,680
of those train and think in ethics I

933
00:43:13,680 --> 00:43:16,200
really talked about those three ethical

934
00:43:16,200 --> 00:43:19,619
um you know philosophy shall we say and

935
00:43:19,619 --> 00:43:21,119
I think the one

936
00:43:21,119 --> 00:43:23,099
um that I'm going to expand on a little

937
00:43:23,099 --> 00:43:27,000
bit is the deontological and that is you

938
00:43:27,000 --> 00:43:28,800
know it says Universal application doing

939
00:43:28,800 --> 00:43:31,079
to others and really

940
00:43:31,079 --> 00:43:32,819
um uh

941
00:43:32,819 --> 00:43:36,480
what they say is can't any moral

942
00:43:36,480 --> 00:43:39,300
Behavior be done by everyone in the

943
00:43:39,300 --> 00:43:42,720
world and and effectively an example

944
00:43:42,720 --> 00:43:46,619
there is is lying does society work if

945
00:43:46,619 --> 00:43:49,200
if everyone lies and

946
00:43:49,200 --> 00:43:52,500
um we're actually going to find out the

947
00:43:52,500 --> 00:43:54,500
result of that

948
00:43:54,500 --> 00:43:57,599
unfortunately possibly very soon and

949
00:43:57,599 --> 00:43:58,980
we're seeing the effects of that right

950
00:43:58,980 --> 00:43:59,880
now

951
00:43:59,880 --> 00:44:04,079
so adopt a code of ethics that addresses

952
00:44:04,079 --> 00:44:06,359
societal concerns now we've talked about

953
00:44:06,359 --> 00:44:08,760
the code of ethics before in the

954
00:44:08,760 --> 00:44:10,920
self-protection and the self-protection

955
00:44:10,920 --> 00:44:14,339
is work loyally with management now I I

956
00:44:14,339 --> 00:44:18,839
didn't put actually this particular one

957
00:44:18,839 --> 00:44:21,480
is in each one of the levels but I want

958
00:44:21,480 --> 00:44:24,780
to contrast them here so self-protection

959
00:44:24,780 --> 00:44:27,800
work loyally with management

960
00:44:27,800 --> 00:44:31,980
compliance concern would potentially in

961
00:44:31,980 --> 00:44:35,460
addition to that add obey the laws and

962
00:44:35,460 --> 00:44:39,240
meet contracts carefully if you were at

963
00:44:39,240 --> 00:44:41,579
stakeholder concern you're going to say

964
00:44:41,579 --> 00:44:44,099
treat the customers vendors and

965
00:44:44,099 --> 00:44:47,640
employees with respect and if you're at

966
00:44:47,640 --> 00:44:51,119
the societal concern level which is a

967
00:44:51,119 --> 00:44:54,000
part of where IEEE and ACM are if you

968
00:44:54,000 --> 00:44:57,599
look at at their code of ethics this ACM

969
00:44:57,599 --> 00:44:59,579
says the public good is always the

970
00:44:59,579 --> 00:45:02,099
primary very consideration so you can

971
00:45:02,099 --> 00:45:04,980
see there that um

972
00:45:04,980 --> 00:45:09,119
you know ACM and IEEE actually are at

973
00:45:09,119 --> 00:45:11,460
the societal concern level

974
00:45:11,460 --> 00:45:14,280
discuss the societal impact of risk

975
00:45:14,280 --> 00:45:16,020
qualitatively

976
00:45:16,020 --> 00:45:18,480
um so again think outside the engineer

977
00:45:18,480 --> 00:45:19,819
role

978
00:45:19,819 --> 00:45:23,160
consider societal risk broadly in other

979
00:45:23,160 --> 00:45:25,260
words you know we tend to think only of

980
00:45:25,260 --> 00:45:28,440
the business we need to open that up to

981
00:45:28,440 --> 00:45:31,619
think more more broadly one way of doing

982
00:45:31,619 --> 00:45:33,660
that is to

983
00:45:33,660 --> 00:45:35,700
um you know is normally we calculate

984
00:45:35,700 --> 00:45:37,859
risk just actually I think that's coming

985
00:45:37,859 --> 00:45:39,300
up so maybe I shouldn't talk about it

986
00:45:39,300 --> 00:45:40,380
but

987
00:45:40,380 --> 00:45:40,980
um

988
00:45:40,980 --> 00:45:43,079
we look at risk only from the

989
00:45:43,079 --> 00:45:45,960
perspective of an organization right as

990
00:45:45,960 --> 00:45:48,359
people we look at risk from a personal

991
00:45:48,359 --> 00:45:50,880
perspective and I think if we think

992
00:45:50,880 --> 00:45:53,099
about whether we're buying

993
00:45:53,099 --> 00:45:56,040
um whether we are buying antivirus

994
00:45:56,040 --> 00:45:59,400
software or we are by buying

995
00:45:59,400 --> 00:46:02,420
um uh

996
00:46:02,760 --> 00:46:05,460
insurance for ourselves

997
00:46:05,460 --> 00:46:06,780
for

998
00:46:06,780 --> 00:46:08,780
um

999
00:46:11,700 --> 00:46:14,940
um for our identity identity theft okay

1000
00:46:14,940 --> 00:46:18,599
many of us are not um are not are not

1001
00:46:18,599 --> 00:46:21,359
ascending there I think okay so the last

1002
00:46:21,359 --> 00:46:23,640
one is avoid ignoring undesirable

1003
00:46:23,640 --> 00:46:26,280
decisions so here's the slide that

1004
00:46:26,280 --> 00:46:27,780
really kind of shows the personal level

1005
00:46:27,780 --> 00:46:30,359
of the organizational level and the

1006
00:46:30,359 --> 00:46:32,940
society level and

1007
00:46:32,940 --> 00:46:34,760
um and so

1008
00:46:34,760 --> 00:46:37,500
addressing all three and I think one of

1009
00:46:37,500 --> 00:46:39,480
the interests that I've got is to look

1010
00:46:39,480 --> 00:46:41,760
at it from a publication level at least

1011
00:46:41,760 --> 00:46:45,119
from from the society level

1012
00:46:45,119 --> 00:46:48,240
uh so this is the whole thing

1013
00:46:48,240 --> 00:46:50,760
um the conclusion for the benefits for

1014
00:46:50,760 --> 00:46:54,060
this is that the lower levels to the

1015
00:46:54,060 --> 00:46:55,380
compliance certainly there's more

1016
00:46:55,380 --> 00:46:59,220
stability there's fewer lawsuits there's

1017
00:46:59,220 --> 00:47:02,280
rare regulatory judgments and improved

1018
00:47:02,280 --> 00:47:04,680
Community reputation but if we look at

1019
00:47:04,680 --> 00:47:06,720
the higher levels which are not quite so

1020
00:47:06,720 --> 00:47:09,720
popular with business is that there's

1021
00:47:09,720 --> 00:47:11,880
new products potentially revolutionary

1022
00:47:11,880 --> 00:47:14,579
and um and I think we're seeing that

1023
00:47:14,579 --> 00:47:16,700
with a number of forward-thinking

1024
00:47:16,700 --> 00:47:19,440
organizations there's better customer

1025
00:47:19,440 --> 00:47:22,200
relationships better long-term employee

1026
00:47:22,200 --> 00:47:25,020
vendor relationships long-term Community

1027
00:47:25,020 --> 00:47:28,140
respect and a feeling of pride in

1028
00:47:28,140 --> 00:47:29,579
Goodwill

1029
00:47:29,579 --> 00:47:31,140
so

1030
00:47:31,140 --> 00:47:37,260
um one thing I'm asking is that if there

1031
00:47:37,260 --> 00:47:40,200
there is a survey and unfortunately I

1032
00:47:40,200 --> 00:47:44,240
there is okay so basically the the

1033
00:47:44,240 --> 00:47:46,859
www.thebottom one the word doc

1034
00:47:46,859 --> 00:47:51,780
um ethical risk that will bring you to

1035
00:47:51,780 --> 00:47:53,460
um kind of some questions that I have

1036
00:47:53,460 --> 00:47:55,140
formulated

1037
00:47:55,140 --> 00:47:57,599
um uh that can help you analyze where

1038
00:47:57,599 --> 00:47:59,339
you are one of the things that actually

1039
00:47:59,339 --> 00:48:02,579
let me go back to here there's there's

1040
00:48:02,579 --> 00:48:05,520
really four questionnaires and one of

1041
00:48:05,520 --> 00:48:08,339
them is for risk people another one is

1042
00:48:08,339 --> 00:48:10,079
for management another one's for

1043
00:48:10,079 --> 00:48:12,420
compliance or legal and the other ones

1044
00:48:12,420 --> 00:48:16,140
for development and Engineering so so

1045
00:48:16,140 --> 00:48:19,619
there are surveys there and uh you could

1046
00:48:19,619 --> 00:48:23,339
actually help me publish in this area if

1047
00:48:23,339 --> 00:48:26,520
you want to take one of those surveys

1048
00:48:26,520 --> 00:48:30,000
unfortunately I should have a link that

1049
00:48:30,000 --> 00:48:32,880
goes to a page where you can easily

1050
00:48:32,880 --> 00:48:34,980
click on this as opposed to copying down

1051
00:48:34,980 --> 00:48:37,380
all those alphabet but unfortunately I

1052
00:48:37,380 --> 00:48:39,420
don't have that so

1053
00:48:39,420 --> 00:48:42,780
um so basically if you want to pick one

1054
00:48:42,780 --> 00:48:44,839
if you're out in Industry

1055
00:48:44,839 --> 00:48:47,760
the best one that fits you and kind and

1056
00:48:47,760 --> 00:48:50,760
fill that and the one thing I would say

1057
00:48:50,760 --> 00:48:54,980
is that would help it is anonymous

1058
00:48:54,980 --> 00:48:57,300
and yes I have some people taking

1059
00:48:57,300 --> 00:48:59,579
pictures of it please do that would be

1060
00:48:59,579 --> 00:49:01,099
wonderful

1061
00:49:01,099 --> 00:49:03,300
and uh

1062
00:49:03,300 --> 00:49:07,619
and please do fill that in and that will

1063
00:49:07,619 --> 00:49:10,200
help me figure out where organizations

1064
00:49:10,200 --> 00:49:12,420
kind of are doing well where they may

1065
00:49:12,420 --> 00:49:15,440
not be doing quite so well

1066
00:49:15,440 --> 00:49:20,480
and yeah so are there any questions

1067
00:49:22,440 --> 00:49:23,579
yeah

1068
00:49:23,579 --> 00:49:26,640
so sometimes could it be to be focused

1069
00:49:26,640 --> 00:49:29,099
on the wrong area unless for the example

1070
00:49:29,099 --> 00:49:33,000
with the uh Three Mile Island which I

1071
00:49:33,000 --> 00:49:34,440
actually grew up in that area so I'm

1072
00:49:34,440 --> 00:49:36,300
very familiar with

1073
00:49:36,300 --> 00:49:39,300
instead of focusing on oh my gosh look

1074
00:49:39,300 --> 00:49:41,640
what happened at the new

1075
00:49:41,640 --> 00:49:43,380
plant could it be that we should have

1076
00:49:43,380 --> 00:49:45,180
focused on

1077
00:49:45,180 --> 00:49:47,339
Oh My Gosh Look What happens when the

1078
00:49:47,339 --> 00:49:50,960
YouTube Donuts because that's

1079
00:49:51,440 --> 00:49:55,020
uh accident and

1080
00:49:55,020 --> 00:49:57,480
maybe that's part of the problems we're

1081
00:49:57,480 --> 00:49:59,900
having with medical

1082
00:50:01,800 --> 00:50:04,020
that's a good question

1083
00:50:04,020 --> 00:50:05,880
yeah that's a good question I'm not so

1084
00:50:05,880 --> 00:50:08,119
sure I think a lot of people eat donuts

1085
00:50:08,119 --> 00:50:10,380
and I don't know if that all goes into

1086
00:50:10,380 --> 00:50:13,440
becomes a disaster so maybe there's

1087
00:50:13,440 --> 00:50:15,180
something about something else other

1088
00:50:15,180 --> 00:50:17,579
than the donuts you know well maybe what

1089
00:50:17,579 --> 00:50:19,800
they did is eating at the controls as

1090
00:50:19,800 --> 00:50:21,480
opposed

1091
00:50:21,480 --> 00:50:24,240
with sugar no the

1092
00:50:24,240 --> 00:50:26,460
Morning Light

1093
00:50:26,460 --> 00:50:28,380
provide a particular

1094
00:50:28,380 --> 00:50:31,339
problems

1095
00:50:31,520 --> 00:50:34,020
yeah no that's actually really what

1096
00:50:34,020 --> 00:50:37,520
happened is um

1097
00:50:37,859 --> 00:50:41,598
the council was here at his stomach

1098
00:50:42,240 --> 00:50:44,400
well you know what maybe then what

1099
00:50:44,400 --> 00:50:46,740
should be done is that they have a table

1100
00:50:46,740 --> 00:50:49,740
where they can put things down that

1101
00:50:49,740 --> 00:50:52,140
doesn't have controls on it or things

1102
00:50:52,140 --> 00:50:54,900
you know and then because unfortunately

1103
00:50:54,900 --> 00:50:56,700
people are always going to continue to

1104
00:50:56,700 --> 00:50:59,400
eat or do whatever particularly if

1105
00:50:59,400 --> 00:51:00,839
they're working overtime and that kind

1106
00:51:00,839 --> 00:51:03,480
of stuff so so in the engineering of it

1107
00:51:03,480 --> 00:51:05,640
it's also possible to address that but

1108
00:51:05,640 --> 00:51:07,980
but it's an interesting question yeah

1109
00:51:07,980 --> 00:51:10,800
other ideas other questions or

1110
00:51:10,800 --> 00:51:13,260
comments yeah is it possible to get some

1111
00:51:13,260 --> 00:51:14,700
of these slides interested in the one

1112
00:51:14,700 --> 00:51:18,059
where you've added your group down

1113
00:51:18,059 --> 00:51:19,440
the different four levels compared to

1114
00:51:19,440 --> 00:51:23,119
the other Frameworks and then also

1115
00:51:23,460 --> 00:51:25,200
better

1116
00:51:25,200 --> 00:51:26,940
um you know I I

1117
00:51:26,940 --> 00:51:30,980
um I right now I have

1118
00:51:30,980 --> 00:51:33,119
www.w.cs.uwp.edu staff linky ethical

1119
00:51:33,119 --> 00:51:35,160
risk okay now

1120
00:51:35,160 --> 00:51:36,839
um the last two days have been a

1121
00:51:36,839 --> 00:51:39,960
disaster for me my harness broke I was I

1122
00:51:39,960 --> 00:51:42,839
wanted to go to this conference

1123
00:51:42,839 --> 00:51:45,000
I'm not sure okay it's over five minutes

1124
00:51:45,000 --> 00:51:46,980
oh okay okay thank you

1125
00:51:46,980 --> 00:51:49,020
um so unfortunately I didn't get this

1126
00:51:49,020 --> 00:51:51,480
updated there but I will go home and get

1127
00:51:51,480 --> 00:51:54,619
this updated to

1128
00:51:54,619 --> 00:51:57,300
www.cs.uwke staff linky ethical risk I

1129
00:51:57,300 --> 00:51:59,640
will have it I will have it up there if

1130
00:51:59,640 --> 00:52:02,400
you want to get these slides you can you

1131
00:52:02,400 --> 00:52:04,559
can get them there it's just that

1132
00:52:04,559 --> 00:52:06,000
um unfortunately in the last two days

1133
00:52:06,000 --> 00:52:08,579
have been a disaster for me so

1134
00:52:08,579 --> 00:52:10,579
um

1135
00:52:10,980 --> 00:52:14,220
the survey links are not but I will put

1136
00:52:14,220 --> 00:52:15,720
them on there you know I mean maybe

1137
00:52:15,720 --> 00:52:17,579
that's what I should do I can probably

1138
00:52:17,579 --> 00:52:19,079
get them on there

1139
00:52:19,079 --> 00:52:20,700
um and and just make it easy for you

1140
00:52:20,700 --> 00:52:22,619
right now you can download some of those

1141
00:52:22,619 --> 00:52:24,119
questions but I think actually the

1142
00:52:24,119 --> 00:52:26,760
survey are updated from what is actually

1143
00:52:26,760 --> 00:52:29,040
even on on

1144
00:52:29,040 --> 00:52:30,359
uh

1145
00:52:30,359 --> 00:52:32,819
yeah so yeah

1146
00:52:32,819 --> 00:52:34,260
other questions I think there was one

1147
00:52:34,260 --> 00:52:35,880
over here go ahead yeah so I saw that

1148
00:52:35,880 --> 00:52:38,520
you had a hazard versus outrage how do

1149
00:52:38,520 --> 00:52:40,079
we

1150
00:52:40,079 --> 00:52:41,520
operation how do we how do you engage

1151
00:52:41,520 --> 00:52:43,440
that

1152
00:52:43,440 --> 00:52:45,540
they don't they don't really go into the

1153
00:52:45,540 --> 00:52:47,579
outrage they do have

1154
00:52:47,579 --> 00:52:49,740
um what he does to document are about

1155
00:52:49,740 --> 00:52:53,160
eight or nine different things that

1156
00:52:53,160 --> 00:52:55,440
cause outrage okay and I love I

1157
00:52:55,440 --> 00:52:56,819
mentioned three of them if it's

1158
00:52:56,819 --> 00:52:58,319
life-threatening

1159
00:52:58,319 --> 00:53:00,180
um if there's some moral issues and if

1160
00:53:00,180 --> 00:53:02,640
you can't control it okay

1161
00:53:02,640 --> 00:53:05,520
um then it it leaves it leads to outrage

1162
00:53:05,520 --> 00:53:07,440
but there's actually eight of them about

1163
00:53:07,440 --> 00:53:09,180
about seven or eight of them that salmon

1164
00:53:09,180 --> 00:53:10,319
talks about

1165
00:53:10,319 --> 00:53:12,960
yeah yeah there's something that might

1166
00:53:12,960 --> 00:53:14,700
help measure that so if you've heard of

1167
00:53:14,700 --> 00:53:16,020
Douglas Hubbard and how to measure

1168
00:53:16,020 --> 00:53:18,180
anything in cyber security risks

1169
00:53:18,180 --> 00:53:19,980
um he gives a great methodology on how

1170
00:53:19,980 --> 00:53:21,900
to become a good estimator for risk

1171
00:53:21,900 --> 00:53:23,760
things I think it could be applied to

1172
00:53:23,760 --> 00:53:25,980
outrage just as well as the other risk

1173
00:53:25,980 --> 00:53:28,200
elements what was her name uh it's uh

1174
00:53:28,200 --> 00:53:30,059
Douglas Hubbard how to measure anything

1175
00:53:30,059 --> 00:53:33,000
in cyber security risk

1176
00:53:33,000 --> 00:53:34,619
one of the things that's interesting in

1177
00:53:34,619 --> 00:53:37,380
fact I I probably would it might be

1178
00:53:37,380 --> 00:53:38,760
interesting for us to talk afterwards

1179
00:53:38,760 --> 00:53:40,079
because it sounds like you have some

1180
00:53:40,079 --> 00:53:42,300
additional insights here that would be

1181
00:53:42,300 --> 00:53:46,079
good for me to note yeah so

1182
00:53:46,079 --> 00:53:46,819
um

1183
00:53:46,819 --> 00:53:49,859
but if you really think of of security I

1184
00:53:49,859 --> 00:53:51,839
think one of the issues is very often

1185
00:53:51,839 --> 00:53:54,839
people's lives are not impacted you know

1186
00:53:54,839 --> 00:53:56,760
their life and death they're not life

1187
00:53:56,760 --> 00:53:58,800
and death situations which is why people

1188
00:53:58,800 --> 00:54:01,260
haven't taken it you know they haven't

1189
00:54:01,260 --> 00:54:04,140
taken it seriously but I think once it

1190
00:54:04,140 --> 00:54:06,420
starts getting into cars and you know I

1191
00:54:06,420 --> 00:54:08,760
mean it's it's really seriously in cars

1192
00:54:08,760 --> 00:54:10,319
where there's

1193
00:54:10,319 --> 00:54:12,119
um then then all of a sudden the life

1194
00:54:12,119 --> 00:54:13,680
and death will

1195
00:54:13,680 --> 00:54:16,559
it will become much more of an impact

1196
00:54:16,559 --> 00:54:18,780
yeah what do you think would be a better

1197
00:54:18,780 --> 00:54:20,640
approach when you're dealing with uh

1198
00:54:20,640 --> 00:54:22,740
morality is across like societal

1199
00:54:22,740 --> 00:54:24,660
morality across the world where there's

1200
00:54:24,660 --> 00:54:25,740
different

1201
00:54:25,740 --> 00:54:27,720
widely different variant views and if

1202
00:54:27,720 --> 00:54:29,460
you have an international business team

1203
00:54:29,460 --> 00:54:31,319
just go to one extreme or whatever the

1204
00:54:31,319 --> 00:54:32,819
most extreme is and you take an average

1205
00:54:32,819 --> 00:54:34,940
of it

1206
00:54:34,940 --> 00:54:38,160
okay so the question is is you know

1207
00:54:38,160 --> 00:54:40,200
because the world has flooded the

1208
00:54:40,200 --> 00:54:41,000
reviews

1209
00:54:41,000 --> 00:54:44,180
what you know where do you

1210
00:54:44,180 --> 00:54:48,420
where should you shoot for I think okay

1211
00:54:48,420 --> 00:54:51,059
and and maybe I'm not going to answer

1212
00:54:51,059 --> 00:54:53,460
that the way you're expecting but let me

1213
00:54:53,460 --> 00:54:55,680
try and then you can ask again

1214
00:54:55,680 --> 00:54:58,559
um I I think companies need to specify

1215
00:54:58,559 --> 00:55:01,260
what their goals are and then live by

1216
00:55:01,260 --> 00:55:04,079
those goals and raise the values you

1217
00:55:04,079 --> 00:55:06,359
know because it is true that in many

1218
00:55:06,359 --> 00:55:08,460
parts of the world

1219
00:55:08,460 --> 00:55:09,599
um

1220
00:55:09,599 --> 00:55:13,980
fraud is mostly you know friends you

1221
00:55:13,980 --> 00:55:15,839
know what I do with friends so it's it's

1222
00:55:15,839 --> 00:55:18,000
corruption more than anything else

1223
00:55:18,000 --> 00:55:20,040
whereas the United States it's more

1224
00:55:20,040 --> 00:55:23,400
asset and misappropriation and

1225
00:55:23,400 --> 00:55:26,460
um uh and so effectively

1226
00:55:26,460 --> 00:55:27,260
um

1227
00:55:27,260 --> 00:55:30,960
it's it it's it's really important to

1228
00:55:30,960 --> 00:55:32,940
put people in a place that can lead by

1229
00:55:32,940 --> 00:55:35,220
example

1230
00:55:35,220 --> 00:55:37,319
um and understand what the problems are

1231
00:55:37,319 --> 00:55:39,900
in most different societies

1232
00:55:39,900 --> 00:55:41,640
does that answer your question or did

1233
00:55:41,640 --> 00:55:43,520
you have a yeah

1234
00:55:43,520 --> 00:55:46,500
it's better to pick just the general

1235
00:55:46,500 --> 00:55:48,660
standpoint of whatever

1236
00:55:48,660 --> 00:55:51,660
your business decided okay

1237
00:55:51,660 --> 00:55:53,819
morality would be as opposed to try to

1238
00:55:53,819 --> 00:55:56,160
take a general Sam okay so I think there

1239
00:55:56,160 --> 00:55:59,460
is a there is another uh question a lot

1240
00:55:59,460 --> 00:56:01,140
of times management wherever management

1241
00:56:01,140 --> 00:56:03,420
is particularly top management that's

1242
00:56:03,420 --> 00:56:05,280
where the organization is right I mean

1243
00:56:05,280 --> 00:56:07,079
whatever level the plot management is at

1244
00:56:07,079 --> 00:56:09,300
that's where the organization is and so

1245
00:56:09,300 --> 00:56:10,800
what happens if you're at a higher level

1246
00:56:10,800 --> 00:56:14,700
than them you know and part of what I'm

1247
00:56:14,700 --> 00:56:17,520
talking about in this is how you can

1248
00:56:17,520 --> 00:56:20,400
move things at least a little bit you

1249
00:56:20,400 --> 00:56:22,200
know that you understand our

1250
00:56:22,200 --> 00:56:24,540
organizations at this level how do I

1251
00:56:24,540 --> 00:56:27,540
move it up a little bit uh

1252
00:56:27,540 --> 00:56:30,480
and and so some of it is communicating

1253
00:56:30,480 --> 00:56:32,940
so you can look at your particular you

1254
00:56:32,940 --> 00:56:34,980
know where my organization is and then

1255
00:56:34,980 --> 00:56:37,680
how I move it up is you know by

1256
00:56:37,680 --> 00:56:39,599
communicating better or

1257
00:56:39,599 --> 00:56:42,059
um or or you know or by not being as

1258
00:56:42,059 --> 00:56:43,800
concerned because it's management it's

1259
00:56:43,800 --> 00:56:47,579
prerogative at the very lowest level so

1260
00:56:47,579 --> 00:56:49,559
um or by potentially talking about

1261
00:56:49,559 --> 00:56:51,540
things you know I mean instead of

1262
00:56:51,540 --> 00:56:53,700
talking about societal values I mean

1263
00:56:53,700 --> 00:56:56,460
each one of these bubbles are good right

1264
00:56:56,460 --> 00:56:58,619
I mean you can't just go to societal

1265
00:56:58,619 --> 00:57:00,599
level you have to you know you have to

1266
00:57:00,599 --> 00:57:02,119
you have to go to

1267
00:57:02,119 --> 00:57:05,339
compliance first and then you go to you

1268
00:57:05,339 --> 00:57:07,500
know valuing your customers and then you

1269
00:57:07,500 --> 00:57:10,440
go to societal values potentially

1270
00:57:10,440 --> 00:57:12,720
um I mean it is possible to to move

1271
00:57:12,720 --> 00:57:14,880
people a little bit but really when it

1272
00:57:14,880 --> 00:57:17,760
comes right down to it all of these all

1273
00:57:17,760 --> 00:57:21,480
of these are important and um and so you

1274
00:57:21,480 --> 00:57:22,319
kind of have to see where your

1275
00:57:22,319 --> 00:57:26,960
organization is and very often the um

1276
00:57:28,380 --> 00:57:28,770
um

1277
00:57:28,770 --> 00:57:32,050
[Music]

1278
00:57:33,740 --> 00:57:36,180
the code of ethics will tell you exactly

1279
00:57:36,180 --> 00:57:38,280
where it is

1280
00:57:38,280 --> 00:57:40,500
he from the code of ethics exactly where

1281
00:57:40,500 --> 00:57:43,380
your organization is so okay I think our

1282
00:57:43,380 --> 00:57:46,319
time is up right no we haven't no

1283
00:57:46,319 --> 00:57:49,859
yeah it's it okay time to go okay thank

1284
00:57:49,859 --> 00:57:52,040
you

1285
00:58:23,640 --> 00:58:26,240
foreign

