1
00:00:00,000 --> 00:00:02,640
hey everyone

2
00:00:04,319 --> 00:00:07,600
i'm going to share my screen

3
00:00:07,600 --> 00:00:11,679
hi bamna hey

4
00:00:11,679 --> 00:00:13,599
so let me give you a quick intro and

5
00:00:13,599 --> 00:00:16,239
then i'll give it over to you sounds

6
00:00:16,239 --> 00:00:17,199
good

7
00:00:17,199 --> 00:00:20,320
okay so moving on to

8
00:00:20,320 --> 00:00:23,039
uh next session try your hand at machine

9
00:00:23,039 --> 00:00:24,480
learning for security

10
00:00:24,480 --> 00:00:27,039
we have vanassam soman here she's a

11
00:00:27,039 --> 00:00:27,680
security

12
00:00:27,680 --> 00:00:30,080
senior security research lead working at

13
00:00:30,080 --> 00:00:32,640
microsoft defender research team

14
00:00:32,640 --> 00:00:34,480
in her day job she develops machine

15
00:00:34,480 --> 00:00:36,320
learning models to classify malware in

16
00:00:36,320 --> 00:00:37,520
real time

17
00:00:37,520 --> 00:00:39,360
so in the past she worked in the field

18
00:00:39,360 --> 00:00:41,360
of threat intelligence and loves to play

19
00:00:41,360 --> 00:00:42,840
with natural language crossing

20
00:00:42,840 --> 00:00:44,320
algorithms

21
00:00:44,320 --> 00:00:46,160
she also holds a master's degree in

22
00:00:46,160 --> 00:00:47,440
computer security from

23
00:00:47,440 --> 00:00:49,920
georgia tech and is also a trainer for

24
00:00:49,920 --> 00:00:50,640
uh

25
00:00:50,640 --> 00:00:51,920
malware who was living with black

26
00:00:51,920 --> 00:00:54,320
foreign it's really

27
00:00:54,320 --> 00:00:56,879
awesome to have you here bangla so over

28
00:00:56,879 --> 00:00:58,719
to you now

29
00:00:58,719 --> 00:01:00,160
thank you so much for the wonderful

30
00:01:00,160 --> 00:01:02,239
introduction namisha and uh

31
00:01:02,239 --> 00:01:04,080
thank you to shakthikhan for inviting me

32
00:01:04,080 --> 00:01:06,960
over uh i was actually very interested

33
00:01:06,960 --> 00:01:09,200
in listening to the last presenter maher

34
00:01:09,200 --> 00:01:10,479
nosh and i was

35
00:01:10,479 --> 00:01:11,920
very like happy that she was talking

36
00:01:11,920 --> 00:01:13,840
about another application of machine

37
00:01:13,840 --> 00:01:15,280
learning insecurity

38
00:01:15,280 --> 00:01:16,880
because that is the domain that i love

39
00:01:16,880 --> 00:01:18,640
and i'm excited to share that with you

40
00:01:18,640 --> 00:01:21,040
as well

41
00:01:22,240 --> 00:01:26,240
all right so i think uh

42
00:01:26,240 --> 00:01:28,560
you all should be able to see my screen

43
00:01:28,560 --> 00:01:29,520
now

44
00:01:29,520 --> 00:01:32,000
so i want to start by talking about how

45
00:01:32,000 --> 00:01:32,880
i got here

46
00:01:32,880 --> 00:01:35,600
um the previous presenter shared yet

47
00:01:35,600 --> 00:01:37,840
another use case of machine learning and

48
00:01:37,840 --> 00:01:40,320
its application and security and i think

49
00:01:40,320 --> 00:01:42,000
i'm seeing that revolution happening

50
00:01:42,000 --> 00:01:45,040
in more and more areas of uh information

51
00:01:45,040 --> 00:01:46,000
security

52
00:01:46,000 --> 00:01:47,680
and the fundamental reason that's

53
00:01:47,680 --> 00:01:48,960
underlying this is that security

54
00:01:48,960 --> 00:01:50,799
expertise is scarce and machine learning

55
00:01:50,799 --> 00:01:52,640
is like a really simple and powerful

56
00:01:52,640 --> 00:01:53,360
tool

57
00:01:53,360 --> 00:01:56,479
to help defenders scale their resources

58
00:01:56,479 --> 00:01:58,000
and you know accomplish their goals

59
00:01:58,000 --> 00:01:59,600
whether it's protecting you know their

60
00:01:59,600 --> 00:02:00,399
customers

61
00:02:00,399 --> 00:02:04,880
or discovering better attacks etc

62
00:02:04,880 --> 00:02:06,479
you know there is a parallel between

63
00:02:06,479 --> 00:02:08,560
what is happening in um

64
00:02:08,560 --> 00:02:10,800
the security machine learning space with

65
00:02:10,800 --> 00:02:12,640
you know what happened a while back

66
00:02:12,640 --> 00:02:16,400
uh with automation so for example like

67
00:02:16,400 --> 00:02:18,239
about like five to ten years ago we saw

68
00:02:18,239 --> 00:02:19,680
this uh you know

69
00:02:19,680 --> 00:02:22,879
secops became a commonly used term

70
00:02:22,879 --> 00:02:25,120
and you know uh it was more and more

71
00:02:25,120 --> 00:02:26,400
accepted that

72
00:02:26,400 --> 00:02:28,000
some amount of automation and coding

73
00:02:28,000 --> 00:02:29,760
skills is really really useful for

74
00:02:29,760 --> 00:02:31,519
security researchers to have so that

75
00:02:31,519 --> 00:02:32,800
they can really magnify

76
00:02:32,800 --> 00:02:35,760
and multiply their impact so i think

77
00:02:35,760 --> 00:02:37,280
from there you know similar to that

78
00:02:37,280 --> 00:02:38,480
we're seeing this movement

79
00:02:38,480 --> 00:02:41,840
to ml cyclops where the knowledge of ml

80
00:02:41,840 --> 00:02:43,680
uh and its applications and security is

81
00:02:43,680 --> 00:02:45,920
becoming much more useful where you know

82
00:02:45,920 --> 00:02:48,160
all security researchers and engineers

83
00:02:48,160 --> 00:02:49,200
are kind of

84
00:02:49,200 --> 00:02:52,000
touching aspects of ml and this type of

85
00:02:52,000 --> 00:02:53,680
is kind of becomes another

86
00:02:53,680 --> 00:02:58,159
tool in your in your arsenal if you will

87
00:02:58,159 --> 00:02:59,760
so this is why i feel very uh

88
00:02:59,760 --> 00:03:01,360
passionately about this subject and i'm

89
00:03:01,360 --> 00:03:03,599
excited to be here today

90
00:03:03,599 --> 00:03:06,239
my agenda for this talk is i'll begin by

91
00:03:06,239 --> 00:03:07,200
giving like a

92
00:03:07,200 --> 00:03:09,680
a a bird's eye view of the various

93
00:03:09,680 --> 00:03:10,640
applications of

94
00:03:10,640 --> 00:03:12,959
machine learning and ai insecurity that

95
00:03:12,959 --> 00:03:14,000
i have seen

96
00:03:14,000 --> 00:03:16,640
in the industry i talk about the common

97
00:03:16,640 --> 00:03:19,440
building blocks of ml based solutions

98
00:03:19,440 --> 00:03:21,519
that are used and then i talk about the

99
00:03:21,519 --> 00:03:23,680
specific challenges of applying machine

100
00:03:23,680 --> 00:03:25,360
learning to security

101
00:03:25,360 --> 00:03:26,879
and then i'll conclude with a few

102
00:03:26,879 --> 00:03:28,959
different examples of different types of

103
00:03:28,959 --> 00:03:30,080
applications

104
00:03:30,080 --> 00:03:33,920
of machine learning and security um

105
00:03:33,920 --> 00:03:36,000
so the first uh so firstly i want to

106
00:03:36,000 --> 00:03:36,959
talk about these

107
00:03:36,959 --> 00:03:38,879
different categories in which you know i

108
00:03:38,879 --> 00:03:40,400
see applications of machine learning and

109
00:03:40,400 --> 00:03:42,080
security today this is not an exhaustive

110
00:03:42,080 --> 00:03:44,959
list you know just some common areas

111
00:03:44,959 --> 00:03:48,159
so um the most common

112
00:03:48,159 --> 00:03:50,080
uh type of machine learning application

113
00:03:50,080 --> 00:03:51,200
that you will see is

114
00:03:51,200 --> 00:03:53,760
classification or supervised ml so in

115
00:03:53,760 --> 00:03:55,519
this use case typically you have

116
00:03:55,519 --> 00:03:57,360
a label data set and then you train a

117
00:03:57,360 --> 00:03:58,959
classifier to identify

118
00:03:58,959 --> 00:04:01,120
similar things so maybe you have a set

119
00:04:01,120 --> 00:04:03,120
of files that are you know clean

120
00:04:03,120 --> 00:04:04,400
some of them are labeled clean some of

121
00:04:04,400 --> 00:04:06,239
them are labeled malware

122
00:04:06,239 --> 00:04:07,599
and then you know you want to create a

123
00:04:07,599 --> 00:04:09,439
classifier to distinguish between other

124
00:04:09,439 --> 00:04:10,959
clean and narrower files

125
00:04:10,959 --> 00:04:12,400
uh this type of machine learning is

126
00:04:12,400 --> 00:04:13,519
commonly used for you know other

127
00:04:13,519 --> 00:04:15,760
entities like files domains behaviors

128
00:04:15,760 --> 00:04:16,959
etc

129
00:04:16,959 --> 00:04:20,000
uh in the technical sense you will see a

130
00:04:20,000 --> 00:04:22,000
lot of linear models applied here

131
00:04:22,000 --> 00:04:24,080
like average perceptrons and also like

132
00:04:24,080 --> 00:04:25,759
there are some deep learning models

133
00:04:25,759 --> 00:04:27,280
applied here but

134
00:04:27,280 --> 00:04:29,520
there is some there is a limitation

135
00:04:29,520 --> 00:04:30,720
there in the sense that deep learning

136
00:04:30,720 --> 00:04:31,919
models will usually

137
00:04:31,919 --> 00:04:33,360
you know they will be heavier in terms

138
00:04:33,360 --> 00:04:34,720
of the compute they require and the

139
00:04:34,720 --> 00:04:36,240
training times etc so

140
00:04:36,240 --> 00:04:38,160
that kind of limits their use a little

141
00:04:38,160 --> 00:04:39,360
bit

142
00:04:39,360 --> 00:04:42,479
and there are others as well uh so since

143
00:04:42,479 --> 00:04:43,919
this is the most basic use case like

144
00:04:43,919 --> 00:04:45,600
throughout my presentation today i will

145
00:04:45,600 --> 00:04:47,520
use this example to illustrate

146
00:04:47,520 --> 00:04:50,000
different uh different you know concepts

147
00:04:50,000 --> 00:04:51,759
and challenges etc like the binary

148
00:04:51,759 --> 00:04:57,199
classification of files

149
00:04:57,199 --> 00:04:59,759
the second most commonly uh used use

150
00:04:59,759 --> 00:05:01,680
case is unsupervised ml

151
00:05:01,680 --> 00:05:03,680
so uh you know we saw this in the last

152
00:05:03,680 --> 00:05:05,840
talk where the author was talking about

153
00:05:05,840 --> 00:05:07,840
anomaly detection to detect

154
00:05:07,840 --> 00:05:09,440
attacks against iot system so that's

155
00:05:09,440 --> 00:05:10,960
that's an excellent example

156
00:05:10,960 --> 00:05:14,400
of using unsupervised ml um in general

157
00:05:14,400 --> 00:05:16,000
it is used in cases where you

158
00:05:16,000 --> 00:05:17,600
you're trying to detect some kind of new

159
00:05:17,600 --> 00:05:19,680
badness uh where you don't have a label

160
00:05:19,680 --> 00:05:20,639
data set

161
00:05:20,639 --> 00:05:23,840
um clustering is another example where

162
00:05:23,840 --> 00:05:26,639
unsupervised ml is used for example if

163
00:05:26,639 --> 00:05:27,360
you have

164
00:05:27,360 --> 00:05:30,240
multiple behaviors entities files etc

165
00:05:30,240 --> 00:05:31,840
that belong to a campaign

166
00:05:31,840 --> 00:05:34,880
clustering techniques can be used to

167
00:05:34,880 --> 00:05:36,479
kind of group them together and you know

168
00:05:36,479 --> 00:05:37,919
maybe present them together to a

169
00:05:37,919 --> 00:05:39,680
security analyst to look at

170
00:05:39,680 --> 00:05:42,880
for instance um uh in in the previous

171
00:05:42,880 --> 00:05:43,280
talk

172
00:05:43,280 --> 00:05:44,400
the clustering methods that were

173
00:05:44,400 --> 00:05:46,160
mentioned you know she specifically said

174
00:05:46,160 --> 00:05:47,919
that hey this is one thing that does not

175
00:05:47,919 --> 00:05:48,880
require

176
00:05:48,880 --> 00:05:51,039
uh the cluster number to be mentioned so

177
00:05:51,039 --> 00:05:52,400
that's like a class of methods that are

178
00:05:52,400 --> 00:05:53,440
non-parametric

179
00:05:53,440 --> 00:05:54,720
and that's really important because if

180
00:05:54,720 --> 00:05:56,319
you don't have good labels you don't

181
00:05:56,319 --> 00:05:58,080
have a good way of saying that

182
00:05:58,080 --> 00:05:59,680
uh you know what the number of clusters

183
00:05:59,680 --> 00:06:01,440
will be so uh

184
00:06:01,440 --> 00:06:03,199
you know that's kind of the type of a

185
00:06:03,199 --> 00:06:05,919
type of technique that's used commonly

186
00:06:05,919 --> 00:06:06,960
now i move to some

187
00:06:06,960 --> 00:06:08,880
uh of you know like more advanced use

188
00:06:08,880 --> 00:06:10,000
cases um

189
00:06:10,000 --> 00:06:11,919
for these uh types of applications you

190
00:06:11,919 --> 00:06:13,759
don't sometimes you see them in product

191
00:06:13,759 --> 00:06:15,520
but it's not very common you definitely

192
00:06:15,520 --> 00:06:17,520
see them in academia and in industry

193
00:06:17,520 --> 00:06:20,240
research labs

194
00:06:20,240 --> 00:06:23,440
um so uh adversarial ml is a big one uh

195
00:06:23,440 --> 00:06:25,199
there are several papers from academia

196
00:06:25,199 --> 00:06:25,600
around

197
00:06:25,600 --> 00:06:28,400
evading existing uh detection systems so

198
00:06:28,400 --> 00:06:30,319
they will build ml models that

199
00:06:30,319 --> 00:06:32,000
you know uh that are able to give an

200
00:06:32,000 --> 00:06:33,600
output that will help

201
00:06:33,600 --> 00:06:35,680
an attacker evade an existing system

202
00:06:35,680 --> 00:06:36,880
like you know there have been papers

203
00:06:36,880 --> 00:06:38,240
around like how to evade an av

204
00:06:38,240 --> 00:06:40,639
engine using adversarial ml and the

205
00:06:40,639 --> 00:06:41,919
converse of this domain

206
00:06:41,919 --> 00:06:44,720
is robustness how can we build better ml

207
00:06:44,720 --> 00:06:45,680
models that are

208
00:06:45,680 --> 00:06:48,080
robust against such adversarial attacks

209
00:06:48,080 --> 00:06:49,360
how can you protect against data

210
00:06:49,360 --> 00:06:50,880
poisoning or

211
00:06:50,880 --> 00:06:54,159
model inversion for instance

212
00:06:54,240 --> 00:06:56,240
another domain is interpretability so

213
00:06:56,240 --> 00:06:58,240
this is really critical because a lot of

214
00:06:58,240 --> 00:06:59,280
times customers

215
00:06:59,280 --> 00:07:01,840
don't directly trust the output of an ml

216
00:07:01,840 --> 00:07:02,400
model

217
00:07:02,400 --> 00:07:04,240
and they want to understand you know why

218
00:07:04,240 --> 00:07:05,759
something was called suspicious or

219
00:07:05,759 --> 00:07:06,560
malicious

220
00:07:06,560 --> 00:07:09,360
so this uh this set of techniques helps

221
00:07:09,360 --> 00:07:11,120
you reason over the outcomes and build

222
00:07:11,120 --> 00:07:12,960
some kind of explainability

223
00:07:12,960 --> 00:07:16,560
into the output of the ml model

224
00:07:16,560 --> 00:07:19,360
and um another domain that i want to

225
00:07:19,360 --> 00:07:21,360
talk about is reinforcement learning

226
00:07:21,360 --> 00:07:23,440
so again this is very research oriented

227
00:07:23,440 --> 00:07:24,400
and i haven't seen

228
00:07:24,400 --> 00:07:27,440
applications in product but there there

229
00:07:27,440 --> 00:07:28,319
is research around

230
00:07:28,319 --> 00:07:30,639
hey how can we provide personalized

231
00:07:30,639 --> 00:07:31,599
protection

232
00:07:31,599 --> 00:07:33,599
using maybe the contextual band problem

233
00:07:33,599 --> 00:07:35,840
or can we build an automated red team

234
00:07:35,840 --> 00:07:37,360
system using reinforcement learning or

235
00:07:37,360 --> 00:07:40,400
even an automated defense system

236
00:07:40,400 --> 00:07:43,440
so these are some categories in which ml

237
00:07:43,440 --> 00:07:45,680
is applied in security today

238
00:07:45,680 --> 00:07:48,720
um i want to open with an example

239
00:07:48,720 --> 00:07:52,000
of how and this is from my work where we

240
00:07:52,000 --> 00:07:54,879
uh we built a classical ml model to

241
00:07:54,879 --> 00:07:56,479
protect against an attack from the earth

242
00:07:56,479 --> 00:07:59,039
sniff botnet

243
00:07:59,039 --> 00:08:01,360
um so this was a small scale targeted

244
00:08:01,360 --> 00:08:03,039
attack and like the graph kind of shows

245
00:08:03,039 --> 00:08:04,639
you you know like over time

246
00:08:04,639 --> 00:08:07,599
um the the prevalence of the files that

247
00:08:07,599 --> 00:08:10,400
were associated with this campaign

248
00:08:10,400 --> 00:08:11,840
uh it was targeted towards small

249
00:08:11,840 --> 00:08:13,199
businesses and the attacker did

250
00:08:13,199 --> 00:08:14,000
something

251
00:08:14,000 --> 00:08:17,360
interesting where um they tailored the

252
00:08:17,360 --> 00:08:18,800
name of the document

253
00:08:18,800 --> 00:08:20,240
to the victim that they were sending it

254
00:08:20,240 --> 00:08:22,639
to for example the dolan care statement

255
00:08:22,639 --> 00:08:24,800
or doc it was a macro enabled document

256
00:08:24,800 --> 00:08:25,599
that was sent

257
00:08:25,599 --> 00:08:28,080
exclusively to victims in a specific

258
00:08:28,080 --> 00:08:29,039
city in

259
00:08:29,039 --> 00:08:32,240
missouri us uh where dolan care is

260
00:08:32,240 --> 00:08:34,000
actually an elderly care assisted living

261
00:08:34,000 --> 00:08:34,880
facility

262
00:08:34,880 --> 00:08:36,399
so they were kind of disguising

263
00:08:36,399 --> 00:08:38,640
themselves as a receipt or like an

264
00:08:38,640 --> 00:08:40,080
invoice from this particular

265
00:08:40,080 --> 00:08:41,519
company that's a real company in that

266
00:08:41,519 --> 00:08:43,279
area and

267
00:08:43,279 --> 00:08:44,880
obviously like the document itself you

268
00:08:44,880 --> 00:08:46,800
know it uh wanted the

269
00:08:46,800 --> 00:08:48,720
victim to enable the macros and then

270
00:08:48,720 --> 00:08:50,959
once you do this obfuscated powershell

271
00:08:50,959 --> 00:08:52,240
will get launched

272
00:08:52,240 --> 00:08:54,160
which will connect to a domain and

273
00:08:54,160 --> 00:08:56,240
download the payload which is uh the er

274
00:08:56,240 --> 00:08:58,080
sniff.net

275
00:08:58,080 --> 00:08:59,680
um and you know you can see that like

276
00:08:59,680 --> 00:09:01,920
the different in the different cities

277
00:09:01,920 --> 00:09:03,279
that they were present they used

278
00:09:03,279 --> 00:09:05,760
different uh lures that were specific to

279
00:09:05,760 --> 00:09:07,360
that city so for example in nebraska

280
00:09:07,360 --> 00:09:08,880
they use dms which is a landscaping

281
00:09:08,880 --> 00:09:10,160
company there

282
00:09:10,160 --> 00:09:12,080
uh in missouri they use dolan care and

283
00:09:12,080 --> 00:09:14,080
then in tennessee they use dockery which

284
00:09:14,080 --> 00:09:15,680
is like a flooring company there so they

285
00:09:15,680 --> 00:09:17,680
were customizing you know

286
00:09:17,680 --> 00:09:19,200
this type of attack that they already

287
00:09:19,200 --> 00:09:21,440
had

288
00:09:21,440 --> 00:09:24,560
um so here's how uh machine learning

289
00:09:24,560 --> 00:09:25,120
helped

290
00:09:25,120 --> 00:09:27,360
in protecting in this specific case so

291
00:09:27,360 --> 00:09:29,040
we had one set of models that were

292
00:09:29,040 --> 00:09:30,640
running on the client like these are

293
00:09:30,640 --> 00:09:31,040
very

294
00:09:31,040 --> 00:09:33,839
lightweight linear models uh that don't

295
00:09:33,839 --> 00:09:35,279
give a verdict but they kind of give

296
00:09:35,279 --> 00:09:36,240
like a suspicious

297
00:09:36,240 --> 00:09:38,000
or not suspicious verdict instead of

298
00:09:38,000 --> 00:09:40,240
malicious or benign so if that model

299
00:09:40,240 --> 00:09:42,240
says that uh the file is suspicious then

300
00:09:42,240 --> 00:09:43,600
it sends like a

301
00:09:43,600 --> 00:09:45,920
a larger section of metadata about the

302
00:09:45,920 --> 00:09:48,000
file itself up to the cloud

303
00:09:48,000 --> 00:09:49,519
and then the cloud will apply like a

304
00:09:49,519 --> 00:09:51,519
series of models to it so the two

305
00:09:51,519 --> 00:09:52,959
specific models that i want to talk

306
00:09:52,959 --> 00:09:53,360
about

307
00:09:53,360 --> 00:09:55,360
uh are the model that are specific to

308
00:09:55,360 --> 00:09:58,240
macro files and the one for non-pe files

309
00:09:58,240 --> 00:10:00,240
uh in this case both of these models

310
00:10:00,240 --> 00:10:02,000
said with a higher degree of probability

311
00:10:02,000 --> 00:10:03,519
that the files are malicious

312
00:10:03,519 --> 00:10:05,120
and then the cloud you know sends a

313
00:10:05,120 --> 00:10:06,640
verdict back to the client

314
00:10:06,640 --> 00:10:09,360
and the file gets quarantined so now i

315
00:10:09,360 --> 00:10:10,079
want to

316
00:10:10,079 --> 00:10:13,279
deep dive into uh the features and kind

317
00:10:13,279 --> 00:10:14,079
of understand

318
00:10:14,079 --> 00:10:16,640
how this model is able to decide that

319
00:10:16,640 --> 00:10:19,120
this file is malicious

320
00:10:19,120 --> 00:10:21,519
this model used three broad categories

321
00:10:21,519 --> 00:10:22,160
of features

322
00:10:22,160 --> 00:10:23,839
expert features fuzzy hash based

323
00:10:23,839 --> 00:10:25,920
features and contextual data

324
00:10:25,920 --> 00:10:27,680
so expert features you can think of them

325
00:10:27,680 --> 00:10:30,399
basically as researcher signatures but

326
00:10:30,399 --> 00:10:32,560
low fidelity researcher signatures so

327
00:10:32,560 --> 00:10:34,240
any one of these signatures

328
00:10:34,240 --> 00:10:35,920
will not really say that a file is

329
00:10:35,920 --> 00:10:37,839
malicious or not but it definitely says

330
00:10:37,839 --> 00:10:39,519
that you know the file is malicious

331
00:10:39,519 --> 00:10:42,320
sorry suspicious so an example of that

332
00:10:42,320 --> 00:10:42,800
is

333
00:10:42,800 --> 00:10:46,000
um maybe like the file has a certain

334
00:10:46,000 --> 00:10:47,760
you know import function or maybe it is

335
00:10:47,760 --> 00:10:49,600
work it is changing a certain registry

336
00:10:49,600 --> 00:10:50,000
key

337
00:10:50,000 --> 00:10:51,760
uh and you can see that through the

338
00:10:51,760 --> 00:10:54,000
strings so that is an expert feature and

339
00:10:54,000 --> 00:10:54,880
you know

340
00:10:54,880 --> 00:10:56,800
it might not always hold true that a

341
00:10:56,800 --> 00:10:58,240
file that is touching that particular

342
00:10:58,240 --> 00:11:00,000
registry key is malicious

343
00:11:00,000 --> 00:11:01,760
but it becomes a signal to the

344
00:11:01,760 --> 00:11:03,760
classifier and says that okay

345
00:11:03,760 --> 00:11:05,839
i need to kind of think more about this

346
00:11:05,839 --> 00:11:08,560
file so those are the expert features

347
00:11:08,560 --> 00:11:10,560
and then the second type of features is

348
00:11:10,560 --> 00:11:12,240
the fuzzy hash features

349
00:11:12,240 --> 00:11:13,839
which is basically you know the rolling

350
00:11:13,839 --> 00:11:16,480
hash of the content of the file itself

351
00:11:16,480 --> 00:11:18,399
and then the third type of features is

352
00:11:18,399 --> 00:11:20,160
the contextual data

353
00:11:20,160 --> 00:11:23,360
and so these features are um

354
00:11:23,360 --> 00:11:25,360
you know where was the file coming from

355
00:11:25,360 --> 00:11:27,040
you know which folder did the file land

356
00:11:27,040 --> 00:11:28,800
into what is the name of the file is it

357
00:11:28,800 --> 00:11:30,800
similar to other legitimate names and

358
00:11:30,800 --> 00:11:32,399
what's the distance between all of that

359
00:11:32,399 --> 00:11:33,680
so those are some of the contextual

360
00:11:33,680 --> 00:11:34,480
features

361
00:11:34,480 --> 00:11:37,440
now you can see here that um the expert

362
00:11:37,440 --> 00:11:38,000
features

363
00:11:38,000 --> 00:11:39,920
and the fuzzy hash features really

364
00:11:39,920 --> 00:11:41,839
contributed highly to saying that the

365
00:11:41,839 --> 00:11:43,040
file is malicious

366
00:11:43,040 --> 00:11:45,680
whereas the contextual data was not able

367
00:11:45,680 --> 00:11:46,399
to uh

368
00:11:46,399 --> 00:11:48,000
make that much contribution in the

369
00:11:48,000 --> 00:11:50,079
malicious verdict and that makes sense

370
00:11:50,079 --> 00:11:51,680
because we saw earlier that the

371
00:11:51,680 --> 00:11:52,639
attackers

372
00:11:52,639 --> 00:11:55,200
were modifying each file to kind of make

373
00:11:55,200 --> 00:11:56,560
it look kind of real

374
00:11:56,560 --> 00:11:58,320
so therefore the contextual features in

375
00:11:58,320 --> 00:12:00,720
this particular case were not powerful

376
00:12:00,720 --> 00:12:02,399
but as you can see that that's the power

377
00:12:02,399 --> 00:12:03,760
of the machine learning model you know

378
00:12:03,760 --> 00:12:05,040
since it is considering

379
00:12:05,040 --> 00:12:07,519
all of these signals together uh it's

380
00:12:07,519 --> 00:12:09,200
you're still able to detect the file as

381
00:12:09,200 --> 00:12:11,839
malicious

382
00:12:13,040 --> 00:12:14,959
so now i'm going to talk about the

383
00:12:14,959 --> 00:12:16,480
building blocks of

384
00:12:16,480 --> 00:12:19,360
a machine learning algorithm so these

385
00:12:19,360 --> 00:12:20,800
are like the basics that you need to

386
00:12:20,800 --> 00:12:22,000
cover if you want to build a machine

387
00:12:22,000 --> 00:12:23,120
learning algorithm

388
00:12:23,120 --> 00:12:25,600
the first thing that you need is data

389
00:12:25,600 --> 00:12:26,800
and then we'll talk about

390
00:12:26,800 --> 00:12:29,519
various features that can be leveraged

391
00:12:29,519 --> 00:12:29,920
uh

392
00:12:29,920 --> 00:12:31,920
we'll talk about how to fit a function

393
00:12:31,920 --> 00:12:33,760
and what that really means

394
00:12:33,760 --> 00:12:36,839
and then evaluating the efficacy of your

395
00:12:36,839 --> 00:12:39,680
model

396
00:12:39,680 --> 00:12:43,360
so let's talk about getting the data

397
00:12:43,360 --> 00:12:45,440
the most common type of data set that

398
00:12:45,440 --> 00:12:47,360
most people start out with when they are

399
00:12:47,360 --> 00:12:48,079
you know

400
00:12:48,079 --> 00:12:49,680
getting introduced to machine learning

401
00:12:49,680 --> 00:12:51,440
is the toy data sets

402
00:12:51,440 --> 00:12:53,760
these are usually packaged alongside you

403
00:12:53,760 --> 00:12:55,839
know python packages or

404
00:12:55,839 --> 00:12:59,440
um data libraries um so an iris data set

405
00:12:59,440 --> 00:13:01,120
is a there is a really good example you

406
00:13:01,120 --> 00:13:01,440
know

407
00:13:01,440 --> 00:13:03,600
so especially when you read blogs about

408
00:13:03,600 --> 00:13:05,680
different machine learning algorithms

409
00:13:05,680 --> 00:13:07,200
these are the data sets that most of

410
00:13:07,200 --> 00:13:09,519
them will be leveraging

411
00:13:09,519 --> 00:13:11,839
now for our security use case there are

412
00:13:11,839 --> 00:13:12,639
some really good

413
00:13:12,639 --> 00:13:14,800
open source data sets available so if

414
00:13:14,800 --> 00:13:16,639
you want to try your hand at machine

415
00:13:16,639 --> 00:13:18,000
learning for security

416
00:13:18,000 --> 00:13:19,680
these data sets are they have been

417
00:13:19,680 --> 00:13:21,200
released by companies they

418
00:13:21,200 --> 00:13:23,360
uh mimic the real world and in that

419
00:13:23,360 --> 00:13:25,839
sense are very useful for

420
00:13:25,839 --> 00:13:27,760
building realistic uh conclusions

421
00:13:27,760 --> 00:13:29,120
realistic algorithms

422
00:13:29,120 --> 00:13:32,000
uh for security so the first one here is

423
00:13:32,000 --> 00:13:33,760
the ember data set which was released a

424
00:13:33,760 --> 00:13:34,000
few

425
00:13:34,000 --> 00:13:37,200
years ago by end game it has about

426
00:13:37,200 --> 00:13:39,199
a million files and when it was released

427
00:13:39,199 --> 00:13:40,720
it was the most comprehensive

428
00:13:40,720 --> 00:13:44,000
malware file dataset

429
00:13:45,040 --> 00:13:47,120
the second data set is much newer it was

430
00:13:47,120 --> 00:13:48,560
only released i think

431
00:13:48,560 --> 00:13:51,360
uh in december last year or january this

432
00:13:51,360 --> 00:13:52,000
year

433
00:13:52,000 --> 00:13:53,920
uh so force and reversing labs came

434
00:13:53,920 --> 00:13:55,040
together to release

435
00:13:55,040 --> 00:13:57,199
a larger data set which has about 20

436
00:13:57,199 --> 00:13:58,320
million files

437
00:13:58,320 --> 00:14:00,320
and it also has some amount of

438
00:14:00,320 --> 00:14:02,560
behavioral information so it's uh it's

439
00:14:02,560 --> 00:14:04,000
pretty comprehensive and a lot of

440
00:14:04,000 --> 00:14:05,680
research teams have now moved to

441
00:14:05,680 --> 00:14:08,079
experimenting with this data set

442
00:14:08,079 --> 00:14:11,360
um microsoft also uh released a data set

443
00:14:11,360 --> 00:14:13,120
a while back on kaggle

444
00:14:13,120 --> 00:14:14,480
in there as part of their malware

445
00:14:14,480 --> 00:14:16,399
classification challenge but at this

446
00:14:16,399 --> 00:14:20,399
point that dataset is a bit outdated

447
00:14:20,399 --> 00:14:21,760
and then obviously there is you know

448
00:14:21,760 --> 00:14:24,079
real data which is hard to get

449
00:14:24,079 --> 00:14:26,720
but um short of getting actual product

450
00:14:26,720 --> 00:14:28,880
telemetry virustotal is a really good

451
00:14:28,880 --> 00:14:30,800
real world data set that has been useful

452
00:14:30,800 --> 00:14:32,959
in experiments in my experience

453
00:14:32,959 --> 00:14:35,279
um through the api you can get

454
00:14:35,279 --> 00:14:36,639
large-scale data

455
00:14:36,639 --> 00:14:38,800
it is a bit biased towards malware

456
00:14:38,800 --> 00:14:39,680
versus clean

457
00:14:39,680 --> 00:14:41,040
so that is something you'll have to

458
00:14:41,040 --> 00:14:43,760
account for but in general it's uh

459
00:14:43,760 --> 00:14:45,920
it's kind of like a good really source

460
00:14:45,920 --> 00:14:47,120
of data

461
00:14:47,120 --> 00:14:48,560
and then the last one obviously is

462
00:14:48,560 --> 00:14:50,160
product telemetry so

463
00:14:50,160 --> 00:14:53,120
this is this is not usually shared uh

464
00:14:53,120 --> 00:14:54,959
with outside audiences because of

465
00:14:54,959 --> 00:14:56,839
restrictions around privacy and

466
00:14:56,839 --> 00:14:58,240
compliance

467
00:14:58,240 --> 00:15:01,440
but the most realistic assessments and

468
00:15:01,440 --> 00:15:03,040
practical applications come by

469
00:15:03,040 --> 00:15:06,319
experimenting on this data

470
00:15:06,880 --> 00:15:09,519
cool now we're going to talk about

471
00:15:09,519 --> 00:15:10,800
feature extraction

472
00:15:10,800 --> 00:15:12,800
and i'm going to solicit some audience

473
00:15:12,800 --> 00:15:14,160
participation

474
00:15:14,160 --> 00:15:15,760
so i know that this is you know we

475
00:15:15,760 --> 00:15:17,279
started doing conferences

476
00:15:17,279 --> 00:15:19,360
virtually and but i still want to enable

477
00:15:19,360 --> 00:15:21,040
audience participation

478
00:15:21,040 --> 00:15:24,240
so um i want to ask

479
00:15:24,240 --> 00:15:26,560
all of you to go to mentee.com and enter

480
00:15:26,560 --> 00:15:28,480
this code and then i'll ask you some

481
00:15:28,480 --> 00:15:29,680
questions and then

482
00:15:29,680 --> 00:15:31,600
you know we'll make some word clouds

483
00:15:31,600 --> 00:15:33,519
with the answers that you give

484
00:15:33,519 --> 00:15:37,279
i'm gonna uh stop sharing my screen for

485
00:15:37,279 --> 00:15:41,839
a second

486
00:15:43,600 --> 00:15:45,759
and

487
00:15:49,120 --> 00:15:55,839
and go to mentee.com myself

488
00:16:01,279 --> 00:16:05,040
okay so here um

489
00:16:05,040 --> 00:16:06,639
what are some useful features that you

490
00:16:06,639 --> 00:16:09,120
can use to classify malware files

491
00:16:09,120 --> 00:16:11,759
so just uh go to that website enter that

492
00:16:11,759 --> 00:16:13,440
code and then you'll be able to enter

493
00:16:13,440 --> 00:16:16,000
outputs uh enter some words and you know

494
00:16:16,000 --> 00:16:17,440
this will automatically generate

495
00:16:17,440 --> 00:16:21,120
word clouds awesome yeah these are these

496
00:16:21,120 --> 00:16:22,160
are all good answers

497
00:16:22,160 --> 00:16:25,519
import functions pe headers

498
00:16:25,519 --> 00:16:27,440
yep signer hash file extension really

499
00:16:27,440 --> 00:16:29,759
good

500
00:16:32,320 --> 00:16:37,839
awesome really cool

501
00:16:39,680 --> 00:16:42,800
okay so

502
00:16:43,519 --> 00:16:46,720
let the i'll go find it

503
00:16:49,279 --> 00:16:53,360
nice yeah code spyware

504
00:16:57,040 --> 00:17:00,319
i love this thank you all for

505
00:17:02,839 --> 00:17:05,839
participating

506
00:17:10,959 --> 00:17:14,079
yeah linkers

507
00:17:14,160 --> 00:17:15,679
okay there are several several really

508
00:17:15,679 --> 00:17:18,480
good answers here

509
00:17:18,480 --> 00:17:20,720
i'll give it 10 more seconds and then

510
00:17:20,720 --> 00:17:33,440
i'll move to the next next question

511
00:17:33,440 --> 00:17:35,919
nice so yes it's really easy to come up

512
00:17:35,919 --> 00:17:37,440
with all of these features to classify

513
00:17:37,440 --> 00:17:38,720
malware files

514
00:17:38,720 --> 00:17:42,000
let's try one more

515
00:17:42,000 --> 00:17:43,440
what are some useful features to

516
00:17:43,440 --> 00:17:47,840
classify the malware domains

517
00:17:52,000 --> 00:17:53,760
let's think about a different set of

518
00:17:53,760 --> 00:17:55,440
features if you have

519
00:17:55,440 --> 00:17:57,039
if you want to classify domain names how

520
00:17:57,039 --> 00:17:59,360
could you do it yeah tld is a good one

521
00:17:59,360 --> 00:18:01,200
entropy excellent actually that's

522
00:18:01,200 --> 00:18:04,240
relevant for both files and domains

523
00:18:04,240 --> 00:18:04,640
um

524
00:18:04,640 --> 00:18:07,839
[Music]

525
00:18:11,039 --> 00:18:15,840
yeah length who is

526
00:18:16,480 --> 00:18:18,640
domain name registration yep that's

527
00:18:18,640 --> 00:18:19,760
really good

528
00:18:19,760 --> 00:18:29,679
name servers

529
00:18:29,679 --> 00:18:32,000
nice

530
00:18:33,120 --> 00:18:35,039
n grams yeah that's a really good one

531
00:18:35,039 --> 00:18:37,520
too organ for

532
00:18:37,520 --> 00:18:39,840
okay i'll give this uh 10 seconds and

533
00:18:39,840 --> 00:18:40,799
then i'll

534
00:18:40,799 --> 00:18:43,840
go back to the deck

535
00:18:51,120 --> 00:18:57,760
misspelled words also a good one

536
00:18:57,760 --> 00:19:01,520
dig url yeah yeah

537
00:19:01,520 --> 00:19:05,120
protocol http or https nice

538
00:19:05,120 --> 00:19:10,240
okay excellent

539
00:19:10,240 --> 00:19:13,679
i will go back to sharing

540
00:19:16,080 --> 00:19:17,600
my deck but thank you for the

541
00:19:17,600 --> 00:19:18,720
participation and thank you for the

542
00:19:18,720 --> 00:19:20,160
answers that's exactly

543
00:19:20,160 --> 00:19:25,840
uh what i was trying to get

544
00:19:28,480 --> 00:19:31,679
hey well now so i'm reminding you

545
00:19:31,679 --> 00:19:34,880
there's five more minutes

546
00:19:35,919 --> 00:19:41,200
oh didn't i have one hour

547
00:19:41,679 --> 00:19:45,280
oh sorry i said mine sorry my bed

548
00:19:45,280 --> 00:19:53,840
okay good

549
00:19:55,679 --> 00:19:58,400
all right so yeah we all had good

550
00:19:58,400 --> 00:19:59,039
answers

551
00:19:59,039 --> 00:20:00,480
but you know this is just an example of

552
00:20:00,480 --> 00:20:02,480
like hey there are a lot of features

553
00:20:02,480 --> 00:20:04,799
um that you can use uh for you know

554
00:20:04,799 --> 00:20:05,919
malware classification or domain

555
00:20:05,919 --> 00:20:08,000
classification and then some of you also

556
00:20:08,000 --> 00:20:10,000
pointed out that you know you can use

557
00:20:10,000 --> 00:20:11,360
you don't have to do manual feature

558
00:20:11,360 --> 00:20:12,799
extraction there are machine learning

559
00:20:12,799 --> 00:20:14,159
techniques that will do that for you

560
00:20:14,159 --> 00:20:15,520
automatically

561
00:20:15,520 --> 00:20:17,200
uh the downside of that is that you know

562
00:20:17,200 --> 00:20:18,880
you lose out on some explainability

563
00:20:18,880 --> 00:20:22,320
sometimes okay so let's talk about

564
00:20:22,320 --> 00:20:25,679
um curve fitting so machine learning

565
00:20:25,679 --> 00:20:28,080
uh you know sometimes it can appear a as

566
00:20:28,080 --> 00:20:29,840
a daunting new domain but let me tell

567
00:20:29,840 --> 00:20:31,760
you that it's just glorified comforting

568
00:20:31,760 --> 00:20:33,200
and if you've ever done a class in

569
00:20:33,200 --> 00:20:34,240
numerical analysis

570
00:20:34,240 --> 00:20:36,880
or linear algebra then you have you know

571
00:20:36,880 --> 00:20:38,000
you've done some curve fitting and

572
00:20:38,000 --> 00:20:40,000
that's essentially what machine learning

573
00:20:40,000 --> 00:20:40,480
is

574
00:20:40,480 --> 00:20:42,960
at its very core so this is a really

575
00:20:42,960 --> 00:20:44,480
simple simple example

576
00:20:44,480 --> 00:20:46,159
of you know how you would build a linear

577
00:20:46,159 --> 00:20:48,000
model so you have an

578
00:20:48,000 --> 00:20:50,960
input that is the x uh the x variable

579
00:20:50,960 --> 00:20:52,320
and then you have the output which is a

580
00:20:52,320 --> 00:20:53,679
y variable

581
00:20:53,679 --> 00:20:55,200
and you can see it plotted in the green

582
00:20:55,200 --> 00:20:57,280
dots and that's in this toy graph

583
00:20:57,280 --> 00:20:58,799
and then you can imagine that if you

584
00:20:58,799 --> 00:21:01,120
wanted to plot a linear function

585
00:21:01,120 --> 00:21:03,280
to fit the points then you you'd have to

586
00:21:03,280 --> 00:21:05,039
define you know the parameters of this

587
00:21:05,039 --> 00:21:06,720
line b0 and b1

588
00:21:06,720 --> 00:21:08,480
uh and then you would use numerical

589
00:21:08,480 --> 00:21:10,000
analysis techniques to find

590
00:21:10,000 --> 00:21:12,320
these parameters such that the error is

591
00:21:12,320 --> 00:21:13,200
minimized

592
00:21:13,200 --> 00:21:14,559
so that you know the distance between

593
00:21:14,559 --> 00:21:16,640
the points and the line itself because

594
00:21:16,640 --> 00:21:19,520
that is minimized so that is kind of you

595
00:21:19,520 --> 00:21:20,400
know

596
00:21:20,400 --> 00:21:21,679
essentially like this is how you would

597
00:21:21,679 --> 00:21:23,600
fit like a linear regression model

598
00:21:23,600 --> 00:21:25,679
um obviously like practical machine

599
00:21:25,679 --> 00:21:27,440
learning is a bit more complicated so i

600
00:21:27,440 --> 00:21:28,320
want to talk about

601
00:21:28,320 --> 00:21:29,919
just two different types of functions

602
00:21:29,919 --> 00:21:31,120
you know that would be used more

603
00:21:31,120 --> 00:21:31,919
commonly

604
00:21:31,919 --> 00:21:34,080
so the sigmoid function is you know very

605
00:21:34,080 --> 00:21:36,080
commonly used specifically for binary

606
00:21:36,080 --> 00:21:37,600
classification problems

607
00:21:37,600 --> 00:21:39,919
uh like the kind that i usually work on

608
00:21:39,919 --> 00:21:40,880
uh which is

609
00:21:40,880 --> 00:21:42,720
um the reason that it's used is it's

610
00:21:42,720 --> 00:21:44,000
easily differentiable

611
00:21:44,000 --> 00:21:45,760
and also that it gives an output that is

612
00:21:45,760 --> 00:21:47,600
zero or one so you know the binary

613
00:21:47,600 --> 00:21:48,080
problem

614
00:21:48,080 --> 00:21:50,559
uh is helped there and then the second

615
00:21:50,559 --> 00:21:51,440
one i talk about

616
00:21:51,440 --> 00:21:53,760
is the value function which also has a

617
00:21:53,760 --> 00:21:55,360
similar effect as a sigmoid function in

618
00:21:55,360 --> 00:21:56,720
the sense that it's binary

619
00:21:56,720 --> 00:21:58,640
but also the computation here becomes

620
00:21:58,640 --> 00:21:59,840
much more efficient

621
00:21:59,840 --> 00:22:01,600
so when you have a neural network which

622
00:22:01,600 --> 00:22:03,679
has multiple uh multiple nodes and you

623
00:22:03,679 --> 00:22:05,679
have to do similar like computations for

624
00:22:05,679 --> 00:22:07,760
or fitting at each and every node then

625
00:22:07,760 --> 00:22:09,039
the value function

626
00:22:09,039 --> 00:22:11,039
which optimizes or makes the computation

627
00:22:11,039 --> 00:22:12,640
more efficient it becomes

628
00:22:12,640 --> 00:22:15,520
much much more lighter and makes the

629
00:22:15,520 --> 00:22:19,200
training of the model much faster

630
00:22:20,559 --> 00:22:22,480
awesome so the next thing i want to talk

631
00:22:22,480 --> 00:22:25,280
about is evaluation

632
00:22:25,280 --> 00:22:27,760
evaluation is like really tricky

633
00:22:27,760 --> 00:22:28,799
compared for

634
00:22:28,799 --> 00:22:31,440
the security use cases compared to you

635
00:22:31,440 --> 00:22:33,440
know usual machine learning use cases

636
00:22:33,440 --> 00:22:34,640
the first question you want to think

637
00:22:34,640 --> 00:22:37,120
about is where is your ground truth

638
00:22:37,120 --> 00:22:39,120
uh and this kind of relates to the

639
00:22:39,120 --> 00:22:40,159
challenge of getting

640
00:22:40,159 --> 00:22:43,039
labeled data good label data in security

641
00:22:43,039 --> 00:22:43,919
usually you will

642
00:22:43,919 --> 00:22:46,799
you know you'll have noise labels um you

643
00:22:46,799 --> 00:22:47,840
have to decide

644
00:22:47,840 --> 00:22:49,120
how you're going to get it is it going

645
00:22:49,120 --> 00:22:51,679
to be through graders or is it going to

646
00:22:51,679 --> 00:22:52,559
be through

647
00:22:52,559 --> 00:22:54,640
you know some kind of data sets that i

648
00:22:54,640 --> 00:22:55,840
talked about so that's

649
00:22:55,840 --> 00:22:58,159
a question that you have to answer and

650
00:22:58,159 --> 00:22:59,200
then the next

651
00:22:59,200 --> 00:23:01,120
aspect to consider is that your data set

652
00:23:01,120 --> 00:23:03,039
will usually be very unbalanced

653
00:23:03,039 --> 00:23:08,000
um in the sense that um

654
00:23:08,000 --> 00:23:09,520
so for example you know if you're

655
00:23:09,520 --> 00:23:11,280
building a file classifier and you have

656
00:23:11,280 --> 00:23:13,840
10 000 files in your organization

657
00:23:13,840 --> 00:23:16,960
99 99 of those will be clean

658
00:23:16,960 --> 00:23:18,480
uh only one percent of those will be

659
00:23:18,480 --> 00:23:20,960
malware so how are you going to build a

660
00:23:20,960 --> 00:23:23,120
classifier so that it is able to know

661
00:23:23,120 --> 00:23:24,960
that most of the files are clean but

662
00:23:24,960 --> 00:23:26,880
also get enough signal

663
00:23:26,880 --> 00:23:28,640
uh so that you know it's able to say

664
00:23:28,640 --> 00:23:30,640
that okay some files are malware so that

665
00:23:30,640 --> 00:23:33,679
it can identify the malicious signal

666
00:23:33,679 --> 00:23:36,000
and this unbalanced problem also affects

667
00:23:36,000 --> 00:23:37,360
the metrics that you can use

668
00:23:37,360 --> 00:23:39,840
so for example if you just use accuracy

669
00:23:39,840 --> 00:23:41,440
for this type of a

670
00:23:41,440 --> 00:23:44,159
problem then you know because 99 out of

671
00:23:44,159 --> 00:23:45,520
100 files are already

672
00:23:45,520 --> 00:23:46,640
cleaned so if you have a naive

673
00:23:46,640 --> 00:23:48,320
classifier that just says everything is

674
00:23:48,320 --> 00:23:48,799
clean

675
00:23:48,799 --> 00:23:51,840
it would already have 99 accuracy and

676
00:23:51,840 --> 00:23:53,200
obviously this classifier is totally

677
00:23:53,200 --> 00:23:54,480
useless because it's not going to find

678
00:23:54,480 --> 00:23:56,000
any malware

679
00:23:56,000 --> 00:23:57,520
so which is why we use things like

680
00:23:57,520 --> 00:23:59,600
precision recall and fp rate

681
00:23:59,600 --> 00:24:01,520
um so let me tell you what precision

682
00:24:01,520 --> 00:24:03,039
means simply

683
00:24:03,039 --> 00:24:05,600
if your precision is say 0.7 then it

684
00:24:05,600 --> 00:24:08,080
means that for every 100 files that your

685
00:24:08,080 --> 00:24:09,120
classifier says

686
00:24:09,120 --> 00:24:12,320
is malware only 70 of them

687
00:24:12,320 --> 00:24:15,439
are actually nowhere

688
00:24:15,840 --> 00:24:19,120
then recall you know it can be defined

689
00:24:19,120 --> 00:24:21,840
like this so for every 100 files that

690
00:24:21,840 --> 00:24:23,360
are malware that are seen by your

691
00:24:23,360 --> 00:24:24,480
classifier

692
00:24:24,480 --> 00:24:27,200
so if your recall is 0.7 it means that

693
00:24:27,200 --> 00:24:28,159
only 70

694
00:24:28,159 --> 00:24:29,440
of those files will actually be

695
00:24:29,440 --> 00:24:31,520
identified by your classifier and the

696
00:24:31,520 --> 00:24:32,480
remaining

697
00:24:32,480 --> 00:24:35,039
30 will be you know there will be misses

698
00:24:35,039 --> 00:24:37,760
and your classifier will not detect them

699
00:24:37,760 --> 00:24:39,679
um another important metric that you

700
00:24:39,679 --> 00:24:41,360
want to look at is the false positive

701
00:24:41,360 --> 00:24:41,760
rate

702
00:24:41,760 --> 00:24:44,159
um so for example if your false positive

703
00:24:44,159 --> 00:24:45,279
rate is 0.3

704
00:24:45,279 --> 00:24:47,919
that means that for every 100 files that

705
00:24:47,919 --> 00:24:49,679
your you know classifier says

706
00:24:49,679 --> 00:24:52,559
our malware 30 of them are actually not

707
00:24:52,559 --> 00:24:53,360
malware

708
00:24:53,360 --> 00:24:56,320
so this is a very critical metric for

709
00:24:56,320 --> 00:24:57,440
machine learning

710
00:24:57,440 --> 00:24:59,679
uh applications that are deployed in

711
00:24:59,679 --> 00:25:01,120
production because

712
00:25:01,120 --> 00:25:02,640
organizations can be very sensitive

713
00:25:02,640 --> 00:25:03,840
because every time you have a false

714
00:25:03,840 --> 00:25:05,679
positive say you block a file as malware

715
00:25:05,679 --> 00:25:07,120
and it wasn't you know

716
00:25:07,120 --> 00:25:08,720
someone you know they downloaded a file

717
00:25:08,720 --> 00:25:09,919
and they can't use it or there was a

718
00:25:09,919 --> 00:25:11,200
business critical application that

719
00:25:11,200 --> 00:25:12,240
they're not going to be able to use

720
00:25:12,240 --> 00:25:13,039
anymore

721
00:25:13,039 --> 00:25:14,880
so that is considered very very

722
00:25:14,880 --> 00:25:17,120
disruptive

723
00:25:17,120 --> 00:25:20,080
um also we also use like f1 score and

724
00:25:20,080 --> 00:25:21,679
the pr curve can be used to plot

725
00:25:21,679 --> 00:25:23,200
performance characteristics and

726
00:25:23,200 --> 00:25:26,880
determine the probability threshold

727
00:25:27,679 --> 00:25:29,679
all right now i'm going to talk about uh

728
00:25:29,679 --> 00:25:31,600
some of the major challenges that we see

729
00:25:31,600 --> 00:25:34,799
in um applications of mln security

730
00:25:34,799 --> 00:25:37,760
so the first one is labels um the lack

731
00:25:37,760 --> 00:25:39,039
of or the noise in

732
00:25:39,039 --> 00:25:41,520
these uh the scale of the data and the

733
00:25:41,520 --> 00:25:43,360
features themselves

734
00:25:43,360 --> 00:25:46,159
uh measuring actual customer outcomes is

735
00:25:46,159 --> 00:25:47,840
also very tricky

736
00:25:47,840 --> 00:25:50,159
uh i'll talk more about all these

737
00:25:50,159 --> 00:25:51,440
including you know the changing

738
00:25:51,440 --> 00:25:53,679
landscape in the malware as well as

739
00:25:53,679 --> 00:25:57,200
the clean files adversarial evasion um

740
00:25:57,200 --> 00:25:58,960
and then false positives you know which

741
00:25:58,960 --> 00:26:00,559
i mentioned just now where

742
00:26:00,559 --> 00:26:02,080
those are like sometimes a bigger

743
00:26:02,080 --> 00:26:03,840
problem than false negatives for

744
00:26:03,840 --> 00:26:05,039
customers

745
00:26:05,039 --> 00:26:08,159
and then finally interpretability so

746
00:26:08,159 --> 00:26:10,240
let's start with labels um

747
00:26:10,240 --> 00:26:12,240
this is built from uh microsoft

748
00:26:12,240 --> 00:26:14,240
defenders telemetry so

749
00:26:14,240 --> 00:26:17,600
96 of the malware uh that we had on our

750
00:26:17,600 --> 00:26:18,320
systems was

751
00:26:18,320 --> 00:26:21,600
seen only once and never again so that's

752
00:26:21,600 --> 00:26:23,600
a huge huge percentage of files that are

753
00:26:23,600 --> 00:26:24,000
just

754
00:26:24,000 --> 00:26:25,600
seen only once in your system so it's

755
00:26:25,600 --> 00:26:28,080
really hard to get good labels in this

756
00:26:28,080 --> 00:26:29,520
type of environment

757
00:26:29,520 --> 00:26:31,440
so you can see that only three percent

758
00:26:31,440 --> 00:26:32,880
are seen two to ten times

759
00:26:32,880 --> 00:26:36,080
and only point zero one are seen on uh

760
00:26:36,080 --> 00:26:38,000
you know more than a thousand files and

761
00:26:38,000 --> 00:26:39,600
only a small percentage you know you can

762
00:26:39,600 --> 00:26:40,880
get good labels from

763
00:26:40,880 --> 00:26:44,320
from external data sources even so

764
00:26:44,320 --> 00:26:46,080
obviously if you want to train a machine

765
00:26:46,080 --> 00:26:48,000
learning classifier how are you going to

766
00:26:48,000 --> 00:26:48,720
give it

767
00:26:48,720 --> 00:26:50,559
adequate exposure to the diversity of

768
00:26:50,559 --> 00:26:53,760
malware features while at the same time

769
00:26:53,760 --> 00:26:56,640
you know keeping your labels uh less

770
00:26:56,640 --> 00:26:58,640
noisy and accurate

771
00:26:58,640 --> 00:27:01,520
this is a huge challenge for you know

772
00:27:01,520 --> 00:27:02,080
across

773
00:27:02,080 --> 00:27:04,640
the classification or supervised ml

774
00:27:04,640 --> 00:27:07,840
domain in security

775
00:27:08,559 --> 00:27:11,919
the second challenge is um scale

776
00:27:11,919 --> 00:27:14,559
so in uh for microsoft defender for

777
00:27:14,559 --> 00:27:16,000
instance you know when we are looking

778
00:27:16,000 --> 00:27:19,039
only at file encounters on the client

779
00:27:19,039 --> 00:27:21,360
side across the environment we have

780
00:27:21,360 --> 00:27:23,120
millions of files that are seen every

781
00:27:23,120 --> 00:27:23,760
day

782
00:27:23,760 --> 00:27:26,000
so at that stage the model has to be

783
00:27:26,000 --> 00:27:28,080
extremely lightweight and it has to be

784
00:27:28,080 --> 00:27:28,799
able to

785
00:27:28,799 --> 00:27:30,720
you have a small memory footprint so

786
00:27:30,720 --> 00:27:32,320
that you know it is not taking away

787
00:27:32,320 --> 00:27:34,159
memory from the client it has to have a

788
00:27:34,159 --> 00:27:35,679
small compute footprint so that it's not

789
00:27:35,679 --> 00:27:37,200
taking away processing power from the

790
00:27:37,200 --> 00:27:38,159
client itself

791
00:27:38,159 --> 00:27:39,919
and then the inference time has to be

792
00:27:39,919 --> 00:27:41,440
very fast because

793
00:27:41,440 --> 00:27:43,200
you know if if someone is using their

794
00:27:43,200 --> 00:27:44,480
computer and they're you know waiting

795
00:27:44,480 --> 00:27:45,039
for like

796
00:27:45,039 --> 00:27:46,880
two or three minutes to let to open

797
00:27:46,880 --> 00:27:49,279
their file while the av is checking it

798
00:27:49,279 --> 00:27:50,559
or something they're not going to like

799
00:27:50,559 --> 00:27:51,600
that experience

800
00:27:51,600 --> 00:27:54,240
so it has to like uh reply within

801
00:27:54,240 --> 00:27:55,760
milliseconds

802
00:27:55,760 --> 00:27:57,200
and the same holds true for any

803
00:27:57,200 --> 00:27:58,640
real-time model you know even whether

804
00:27:58,640 --> 00:28:00,080
it's at the client of the cloud you have

805
00:28:00,080 --> 00:28:00,640
to

806
00:28:00,640 --> 00:28:02,640
give your inference time is very very

807
00:28:02,640 --> 00:28:03,760
short for it to

808
00:28:03,760 --> 00:28:06,559
give a good customer experience um and

809
00:28:06,559 --> 00:28:07,600
then you know we have

810
00:28:07,600 --> 00:28:09,919
we can have like heavier models but

811
00:28:09,919 --> 00:28:11,120
usually we use them

812
00:28:11,120 --> 00:28:13,840
you know in a slightly offline setting

813
00:28:13,840 --> 00:28:14,799
where for example

814
00:28:14,799 --> 00:28:16,640
you know if a file is suspicious enough

815
00:28:16,640 --> 00:28:18,480
we'll you know collect the sample

816
00:28:18,480 --> 00:28:21,039
detonate it run a deep neural network on

817
00:28:21,039 --> 00:28:23,360
it and then you know send a verdict back

818
00:28:23,360 --> 00:28:25,200
um but usually that takes you know

819
00:28:25,200 --> 00:28:27,039
minutes uh or

820
00:28:27,039 --> 00:28:29,600
sometimes even longer so those models so

821
00:28:29,600 --> 00:28:31,200
that's kind of like how you want to

822
00:28:31,200 --> 00:28:32,880
think about the problem of scale at

823
00:28:32,880 --> 00:28:34,559
different stages

824
00:28:34,559 --> 00:28:36,000
of this defense and depth kind of

825
00:28:36,000 --> 00:28:38,559
pipeline

826
00:28:42,080 --> 00:28:44,240
um i want to talk a little bit about

827
00:28:44,240 --> 00:28:46,240
measuring actual customer outcomes

828
00:28:46,240 --> 00:28:48,240
so now let's say that you have built a

829
00:28:48,240 --> 00:28:49,760
model and then you've deployed it to the

830
00:28:49,760 --> 00:28:51,440
customer and now it's looking at all

831
00:28:51,440 --> 00:28:53,200
these new files and it's giving verdicts

832
00:28:53,200 --> 00:28:54,159
out on that

833
00:28:54,159 --> 00:28:55,840
how are you going to determine uh how

834
00:28:55,840 --> 00:28:57,200
it's doing because you know you might

835
00:28:57,200 --> 00:28:57,520
not

836
00:28:57,520 --> 00:28:59,039
see some of these files ever again you

837
00:28:59,039 --> 00:29:00,799
may not get access to any of these files

838
00:29:00,799 --> 00:29:02,000
because you know they're the customer's

839
00:29:02,000 --> 00:29:03,440
private property

840
00:29:03,440 --> 00:29:05,679
so uh one way to do it is uh you know

841
00:29:05,679 --> 00:29:07,120
manual grading so

842
00:29:07,120 --> 00:29:08,720
in this case obviously you have access

843
00:29:08,720 --> 00:29:10,320
to a certain set of files

844
00:29:10,320 --> 00:29:11,840
and you you know manually have some

845
00:29:11,840 --> 00:29:13,120
researchers look at and say that okay

846
00:29:13,120 --> 00:29:14,559
this verdict was right this was wrong

847
00:29:14,559 --> 00:29:16,240
etc

848
00:29:16,240 --> 00:29:18,320
um another way is to then automatically

849
00:29:18,320 --> 00:29:20,399
extrapolate from that you know manual

850
00:29:20,399 --> 00:29:22,000
grading so for example maybe you grade

851
00:29:22,000 --> 00:29:22,320
10

852
00:29:22,320 --> 00:29:24,240
instances and then you know you can say

853
00:29:24,240 --> 00:29:26,159
that okay these 50 are similar to these

854
00:29:26,159 --> 00:29:27,760
10 instances so i'll consider

855
00:29:27,760 --> 00:29:29,440
you know applying these grades to this

856
00:29:29,440 --> 00:29:31,440
extended data set

857
00:29:31,440 --> 00:29:33,120
and then you know customer submissions

858
00:29:33,120 --> 00:29:34,559
is always a good option where you know

859
00:29:34,559 --> 00:29:36,000
the customer will say

860
00:29:36,000 --> 00:29:38,559
hey you know this is an fp that you know

861
00:29:38,559 --> 00:29:40,480
you've detected wrongly or this is an fm

862
00:29:40,480 --> 00:29:41,919
that you didn't detect

863
00:29:41,919 --> 00:29:43,760
but obviously if you get to that point

864
00:29:43,760 --> 00:29:45,840
it's leading to a bad experience for the

865
00:29:45,840 --> 00:29:48,240
client

866
00:29:48,240 --> 00:29:50,640
um the changing landscape is very

867
00:29:50,640 --> 00:29:51,600
interesting

868
00:29:51,600 --> 00:29:53,520
in security we're often you know we

869
00:29:53,520 --> 00:29:55,200
often talk about how the malware

870
00:29:55,200 --> 00:29:57,039
landscape is changing so fast and you

871
00:29:57,039 --> 00:29:59,520
know we have to adapt that quickly

872
00:29:59,520 --> 00:30:00,960
but one thing that we don't think about

873
00:30:00,960 --> 00:30:02,480
is like the temporal drifting clean

874
00:30:02,480 --> 00:30:03,520
features

875
00:30:03,520 --> 00:30:04,880
and through our experiments we've

876
00:30:04,880 --> 00:30:07,279
noticed that the temporal drift in clean

877
00:30:07,279 --> 00:30:07,919
features is

878
00:30:07,919 --> 00:30:09,440
actually more than you see in malware

879
00:30:09,440 --> 00:30:11,600
features which causes um

880
00:30:11,600 --> 00:30:13,840
so if a model is allowed to stay stale

881
00:30:13,840 --> 00:30:15,039
for too long

882
00:30:15,039 --> 00:30:16,720
then it will keep having a higher and

883
00:30:16,720 --> 00:30:18,320
higher false positive rate because it's

884
00:30:18,320 --> 00:30:20,320
not able to

885
00:30:20,320 --> 00:30:22,159
judge the new clean features that are

886
00:30:22,159 --> 00:30:23,840
coming in and that totally makes sense

887
00:30:23,840 --> 00:30:25,200
because you know in this world there are

888
00:30:25,200 --> 00:30:26,240
probably more

889
00:30:26,240 --> 00:30:27,919
clean software developers than there are

890
00:30:27,919 --> 00:30:29,919
malware developers

891
00:30:29,919 --> 00:30:31,440
so that's another interesting thing to

892
00:30:31,440 --> 00:30:33,520
keep in mind

893
00:30:33,520 --> 00:30:35,840
so interpretability is a big one uh

894
00:30:35,840 --> 00:30:36,640
usually

895
00:30:36,640 --> 00:30:38,559
people will not trust the output a

896
00:30:38,559 --> 00:30:39,840
machine learning model

897
00:30:39,840 --> 00:30:42,000
if uh you know a supervisor unsupervised

898
00:30:42,000 --> 00:30:42,960
model says that hey

899
00:30:42,960 --> 00:30:45,200
something is bad they'll be like okay

900
00:30:45,200 --> 00:30:46,480
how bad is it

901
00:30:46,480 --> 00:30:48,720
uh is it like ransomware bad so i should

902
00:30:48,720 --> 00:30:50,000
unplug my computer or

903
00:30:50,000 --> 00:30:52,159
is it just like a browser pop-up so that

904
00:30:52,159 --> 00:30:53,760
you know i just i can ignore it for a

905
00:30:53,760 --> 00:30:54,720
few more days and

906
00:30:54,720 --> 00:30:56,720
do something more important and then you

907
00:30:56,720 --> 00:30:58,159
know they kind of want context around

908
00:30:58,159 --> 00:31:00,080
okay what attacker is this associated to

909
00:31:00,080 --> 00:31:01,600
what should i do what's the next thing

910
00:31:01,600 --> 00:31:03,200
that i would expect this

911
00:31:03,200 --> 00:31:06,240
type of attack uh to go so all of these

912
00:31:06,240 --> 00:31:06,960
things can be

913
00:31:06,960 --> 00:31:08,559
uh addressed using interpretive

914
00:31:08,559 --> 00:31:10,320
interpretability methods from machine

915
00:31:10,320 --> 00:31:12,639
learning

916
00:31:12,880 --> 00:31:14,960
um adversarial evasion is something i

917
00:31:14,960 --> 00:31:16,559
touched on earlier

918
00:31:16,559 --> 00:31:18,240
and this is a huge challenge as well now

919
00:31:18,240 --> 00:31:19,760
that more and more

920
00:31:19,760 --> 00:31:21,360
uh applications of machine learning and

921
00:31:21,360 --> 00:31:23,440
security are you know prevalent

922
00:31:23,440 --> 00:31:25,200
uh the attackers are catching up and

923
00:31:25,200 --> 00:31:27,440
they are exploring ml based methods

924
00:31:27,440 --> 00:31:29,039
to evade these models and these

925
00:31:29,039 --> 00:31:31,200
detectors but obviously there are like

926
00:31:31,200 --> 00:31:32,640
the traditional methods as well you know

927
00:31:32,640 --> 00:31:34,159
like more sophisticated packing or

928
00:31:34,159 --> 00:31:36,799
obfuscation

929
00:31:38,159 --> 00:31:40,640
so now that we've covered the challenges

930
00:31:40,640 --> 00:31:42,240
of applying machine learning and

931
00:31:42,240 --> 00:31:43,120
security

932
00:31:43,120 --> 00:31:44,960
i want to share some examples with you

933
00:31:44,960 --> 00:31:46,720
and go into the depths of how we

934
00:31:46,720 --> 00:31:49,679
do some of these things so i showed this

935
00:31:49,679 --> 00:31:50,799
um

936
00:31:50,799 --> 00:31:53,279
example to you earlier where we this

937
00:31:53,279 --> 00:31:54,399
classical linear

938
00:31:54,399 --> 00:31:57,360
model was used to um block the earth's

939
00:31:57,360 --> 00:31:58,799
import net

940
00:31:58,799 --> 00:32:01,519
so um here's how you know the process of

941
00:32:01,519 --> 00:32:02,640
building this model goes

942
00:32:02,640 --> 00:32:04,799
the first step is selecting samples uh

943
00:32:04,799 --> 00:32:07,039
we have millions of samples uh we want

944
00:32:07,039 --> 00:32:08,720
to select like a small subset of known

945
00:32:08,720 --> 00:32:10,240
clean and malware files

946
00:32:10,240 --> 00:32:11,919
and then you want to fine-tune the ratio

947
00:32:11,919 --> 00:32:14,640
of clean and malware

948
00:32:14,640 --> 00:32:16,799
to ensure that you know the model should

949
00:32:16,799 --> 00:32:17,679
understand that

950
00:32:17,679 --> 00:32:19,279
most of the universe will be clean but

951
00:32:19,279 --> 00:32:21,519
also has enough malware signal

952
00:32:21,519 --> 00:32:23,279
to you know discriminate between

953
00:32:23,279 --> 00:32:25,600
malicious and clean files

954
00:32:25,600 --> 00:32:27,519
we might also want to target sometimes

955
00:32:27,519 --> 00:32:29,519
specific file types you know like we had

956
00:32:29,519 --> 00:32:33,039
that macro specific model for instance

957
00:32:33,039 --> 00:32:34,559
then the next thing is feature selection

958
00:32:34,559 --> 00:32:36,559
so you all had excellent ideas on you

959
00:32:36,559 --> 00:32:38,080
know how feature selection can work

960
00:32:38,080 --> 00:32:39,840
uh in general you know features can be

961
00:32:39,840 --> 00:32:41,360
related to any of the machines

962
00:32:41,360 --> 00:32:42,559
attributes

963
00:32:42,559 --> 00:32:44,240
dynamic or contextual attributes of the

964
00:32:44,240 --> 00:32:46,480
files static features of the file

965
00:32:46,480 --> 00:32:49,039
and how we do it is that we extract

966
00:32:49,039 --> 00:32:50,159
millions of features

967
00:32:50,159 --> 00:32:53,279
uh you know for a set of data but then

968
00:32:53,279 --> 00:32:55,360
we have to do feature selection or some

969
00:32:55,360 --> 00:32:57,360
kind of reduction because

970
00:32:57,360 --> 00:32:59,440
at that high dimensionality the

971
00:32:59,440 --> 00:33:00,640
computation

972
00:33:00,640 --> 00:33:02,559
for the training of the model will not

973
00:33:02,559 --> 00:33:04,080
be feasible in a reasonable amount of

974
00:33:04,080 --> 00:33:04,720
time

975
00:33:04,720 --> 00:33:07,360
so we kind of do some meta-analysis to

976
00:33:07,360 --> 00:33:08,640
say which features are more important

977
00:33:08,640 --> 00:33:10,159
which features have more discriminative

978
00:33:10,159 --> 00:33:11,440
power and then we

979
00:33:11,440 --> 00:33:14,559
downsize to just those and then

980
00:33:14,559 --> 00:33:15,600
you know we do the kind of

981
00:33:15,600 --> 00:33:17,840
experimentation hyper parameter tuning

982
00:33:17,840 --> 00:33:18,399
that is

983
00:33:18,399 --> 00:33:20,480
a part of every machine learning model

984
00:33:20,480 --> 00:33:22,960
you try out different models you try out

985
00:33:22,960 --> 00:33:26,000
different ratios and then the one thing

986
00:33:26,000 --> 00:33:27,200
that we have to do again is

987
00:33:27,200 --> 00:33:29,519
tuning the probability threshold because

988
00:33:29,519 --> 00:33:30,799
we know that customers are really

989
00:33:30,799 --> 00:33:32,640
sensitive to false positive rates

990
00:33:32,640 --> 00:33:36,000
and um we have to maintain or we have to

991
00:33:36,000 --> 00:33:38,000
tune the operational characteristics of

992
00:33:38,000 --> 00:33:39,440
the classifier such that

993
00:33:39,440 --> 00:33:43,360
the false positive rate will remain low

994
00:33:43,360 --> 00:33:45,200
and then you know first we ship it in

995
00:33:45,200 --> 00:33:47,120
this quote-unquote experimental mode

996
00:33:47,120 --> 00:33:47,840
where

997
00:33:47,840 --> 00:33:49,760
it will kind of be looking at the

998
00:33:49,760 --> 00:33:51,120
traffic uh but not

999
00:33:51,120 --> 00:33:53,120
giving blocking verdicts and this is

1000
00:33:53,120 --> 00:33:54,720
when we kind of evaluate

1001
00:33:54,720 --> 00:33:56,399
uh the actual outcomes that the

1002
00:33:56,399 --> 00:33:58,080
classifier is able to provide on new

1003
00:33:58,080 --> 00:33:59,360
unseen traffic

1004
00:33:59,360 --> 00:34:01,279
and then if it is performing up to part

1005
00:34:01,279 --> 00:34:02,480
then it will be turned

1006
00:34:02,480 --> 00:34:07,840
to a blocking mode classifier

1007
00:34:08,879 --> 00:34:11,040
the second example i want to give is how

1008
00:34:11,040 --> 00:34:12,960
we use ensembl models for malware

1009
00:34:12,960 --> 00:34:14,000
classification

1010
00:34:14,000 --> 00:34:15,599
this was this was work done by my

1011
00:34:15,599 --> 00:34:18,159
colleagues jubil harley and randy

1012
00:34:18,159 --> 00:34:20,480
the idea of ensemble learning is that uh

1013
00:34:20,480 --> 00:34:22,239
to build a prediction model

1014
00:34:22,239 --> 00:34:24,000
instead of just using you know a set of

1015
00:34:24,000 --> 00:34:25,520
classifiers we can

1016
00:34:25,520 --> 00:34:27,440
bring these uh classifiers together you

1017
00:34:27,440 --> 00:34:28,560
know we use a set of simple base

1018
00:34:28,560 --> 00:34:30,239
classifiers and then we bring them

1019
00:34:30,239 --> 00:34:31,280
together

1020
00:34:31,280 --> 00:34:33,599
uh to this stacked approach so that the

1021
00:34:33,599 --> 00:34:34,800
machine learning

1022
00:34:34,800 --> 00:34:36,639
one machine learning model can decide

1023
00:34:36,639 --> 00:34:38,480
you know based on these signals which

1024
00:34:38,480 --> 00:34:39,760
are more accurate which are less

1025
00:34:39,760 --> 00:34:42,800
accurate and then give a final verdict

1026
00:34:42,800 --> 00:34:46,079
um this is how the architecture of a

1027
00:34:46,079 --> 00:34:47,760
an ensemble classifier could look like

1028
00:34:47,760 --> 00:34:49,280
so maybe you have a set of base

1029
00:34:49,280 --> 00:34:50,719
classifiers you know for example there

1030
00:34:50,719 --> 00:34:52,320
was a macro classifier there might be

1031
00:34:52,320 --> 00:34:54,320
one that's focused on behaviors

1032
00:34:54,320 --> 00:34:56,480
one focused on contextual features and

1033
00:34:56,480 --> 00:34:58,240
you have all these base classifiers

1034
00:34:58,240 --> 00:35:01,440
um that give their own prediction

1035
00:35:01,440 --> 00:35:03,119
in our case you know we were using their

1036
00:35:03,119 --> 00:35:04,720
prediction in terms of the probability

1037
00:35:04,720 --> 00:35:05,119
of

1038
00:35:05,119 --> 00:35:06,880
something being malware and then you

1039
00:35:06,880 --> 00:35:08,240
feed these predictions

1040
00:35:08,240 --> 00:35:11,440
as the data set into a new classifier

1041
00:35:11,440 --> 00:35:13,599
which then you know uh gives the final

1042
00:35:13,599 --> 00:35:15,440
verdict

1043
00:35:15,440 --> 00:35:17,280
there are several benefits to using an

1044
00:35:17,280 --> 00:35:19,119
ensemble model the first of which is

1045
00:35:19,119 --> 00:35:19,440
that

1046
00:35:19,440 --> 00:35:21,359
it filters out noisy signals

1047
00:35:21,359 --> 00:35:22,880
automatically from any

1048
00:35:22,880 --> 00:35:25,280
underperforming model so if one or two

1049
00:35:25,280 --> 00:35:26,560
models within that

1050
00:35:26,560 --> 00:35:30,560
set of base classifiers is noisy then uh

1051
00:35:30,560 --> 00:35:33,040
the stacked classifier will be able to

1052
00:35:33,040 --> 00:35:36,240
smooth out that noise

1053
00:35:36,240 --> 00:35:38,079
um and the second thing that's very

1054
00:35:38,079 --> 00:35:39,599
interesting that's an interesting side

1055
00:35:39,599 --> 00:35:40,880
effect is that it increases the

1056
00:35:40,880 --> 00:35:42,640
predictive power but also adds some

1057
00:35:42,640 --> 00:35:44,160
explainability so

1058
00:35:44,160 --> 00:35:46,079
for example if i have a stacked

1059
00:35:46,079 --> 00:35:47,520
classifier that has a number of base

1060
00:35:47,520 --> 00:35:48,320
classifiers

1061
00:35:48,320 --> 00:35:50,079
maybe one of them is for contextual

1062
00:35:50,079 --> 00:35:52,000
features maybe one of them is for

1063
00:35:52,000 --> 00:35:54,079
you know fuzzy hash based features so

1064
00:35:54,079 --> 00:35:55,920
then you know after i get the output

1065
00:35:55,920 --> 00:35:57,520
from the stack classifier i can easily

1066
00:35:57,520 --> 00:35:58,400
say

1067
00:35:58,400 --> 00:36:00,880
that okay this file was malicious but it

1068
00:36:00,880 --> 00:36:02,160
was malicious because

1069
00:36:02,160 --> 00:36:03,760
of you know it was in a suspicious

1070
00:36:03,760 --> 00:36:06,400
context or because the fuzzy hash was

1071
00:36:06,400 --> 00:36:07,920
looking bad and it was similar to bad

1072
00:36:07,920 --> 00:36:09,839
things whereas if i had

1073
00:36:09,839 --> 00:36:11,520
you know combined all of these features

1074
00:36:11,520 --> 00:36:13,520
directly into one classifier that

1075
00:36:13,520 --> 00:36:14,960
interpretability becomes a bit more

1076
00:36:14,960 --> 00:36:17,599
complicated

1077
00:36:17,599 --> 00:36:19,520
and then it adds resilience against

1078
00:36:19,520 --> 00:36:20,960
attacks and individual models so we

1079
00:36:20,960 --> 00:36:21,599
talked about

1080
00:36:21,599 --> 00:36:23,760
uh adversarial machine learning and you

1081
00:36:23,760 --> 00:36:26,000
know how attackers are exploring that

1082
00:36:26,000 --> 00:36:27,440
ensemble models because it's a

1083
00:36:27,440 --> 00:36:29,359
combination of multiple models it

1084
00:36:29,359 --> 00:36:32,720
adds some resilience as well

1085
00:36:33,680 --> 00:36:35,440
i'll give one more example and this is

1086
00:36:35,440 --> 00:36:36,720
very different from

1087
00:36:36,720 --> 00:36:39,200
the stuff that we've discussed so far

1088
00:36:39,200 --> 00:36:39,839
which is

1089
00:36:39,839 --> 00:36:41,359
an application of natural language

1090
00:36:41,359 --> 00:36:44,160
processing for threat intelligence

1091
00:36:44,160 --> 00:36:46,320
um so i want to introduce the concept of

1092
00:36:46,320 --> 00:36:48,079
named energy extraction here

1093
00:36:48,079 --> 00:36:50,240
so this is a blob of text and you know

1094
00:36:50,240 --> 00:36:51,599
if i feed this

1095
00:36:51,599 --> 00:36:54,079
into like a named extractor from azure

1096
00:36:54,079 --> 00:36:54,800
or google

1097
00:36:54,800 --> 00:36:57,440
um it can extract like entities like

1098
00:36:57,440 --> 00:36:57,839
this

1099
00:36:57,839 --> 00:37:00,400
which uh would be say you know sansa

1100
00:37:00,400 --> 00:37:01,760
stark is a person

1101
00:37:01,760 --> 00:37:04,079
eddard struck as a person winterfell is

1102
00:37:04,079 --> 00:37:05,440
you know an organization or a

1103
00:37:05,440 --> 00:37:06,960
geopolitical entity

1104
00:37:06,960 --> 00:37:08,560
so this is what named energy extraction

1105
00:37:08,560 --> 00:37:10,720
looks like you feed in a blog of text

1106
00:37:10,720 --> 00:37:12,240
and then you know it automatically

1107
00:37:12,240 --> 00:37:14,320
extracts the significant

1108
00:37:14,320 --> 00:37:16,800
entities that are present in that text

1109
00:37:16,800 --> 00:37:17,680
now if we look

1110
00:37:17,680 --> 00:37:21,040
at um a machine uh sorry a security

1111
00:37:21,040 --> 00:37:22,720
related text you know this is a

1112
00:37:22,720 --> 00:37:25,280
an apt white paper about a china based

1113
00:37:25,280 --> 00:37:26,160
actor

1114
00:37:26,160 --> 00:37:29,440
uh what happens here is that

1115
00:37:29,440 --> 00:37:30,800
the entities that i would like to

1116
00:37:30,800 --> 00:37:32,800
extract from this are you know

1117
00:37:32,800 --> 00:37:35,359
the names or the aliases of the actor

1118
00:37:35,359 --> 00:37:36,560
and the techniques that it's in the

1119
00:37:36,560 --> 00:37:38,320
mighty techniques that it's using

1120
00:37:38,320 --> 00:37:40,160
but obviously uh you know the

1121
00:37:40,160 --> 00:37:41,760
off-the-shelf uh

1122
00:37:41,760 --> 00:37:43,920
named energy extractors since they are

1123
00:37:43,920 --> 00:37:45,119
not uh tuned

1124
00:37:45,119 --> 00:37:47,920
to the to the cyber security domain

1125
00:37:47,920 --> 00:37:48,480
they're not

1126
00:37:48,480 --> 00:37:50,640
able to do this type of stuff so this

1127
00:37:50,640 --> 00:37:51,599
kind of example

1128
00:37:51,599 --> 00:37:53,920
is about training a named entity

1129
00:37:53,920 --> 00:37:54,640
extractor

1130
00:37:54,640 --> 00:37:57,200
custom uh applicable to the cyber

1131
00:37:57,200 --> 00:37:58,880
security domain

1132
00:37:58,880 --> 00:38:01,599
and um the reason we did that is because

1133
00:38:01,599 --> 00:38:03,200
we want to generate these security

1134
00:38:03,200 --> 00:38:04,960
knowledge graph type of structures

1135
00:38:04,960 --> 00:38:06,800
so for example you know if i can run

1136
00:38:06,800 --> 00:38:08,720
like a named entity extraction model

1137
00:38:08,720 --> 00:38:11,680
and build relationships across documents

1138
00:38:11,680 --> 00:38:12,480
i can

1139
00:38:12,480 --> 00:38:13,839
automatically build like a security

1140
00:38:13,839 --> 00:38:15,520
knowledge graph so this graph in

1141
00:38:15,520 --> 00:38:16,160
particular

1142
00:38:16,160 --> 00:38:18,640
shows the overlap in microtechniques

1143
00:38:18,640 --> 00:38:20,560
used by imotete which is a commodity

1144
00:38:20,560 --> 00:38:21,680
malware family

1145
00:38:21,680 --> 00:38:24,400
with some apts and you know so this kind

1146
00:38:24,400 --> 00:38:25,119
of

1147
00:38:25,119 --> 00:38:26,720
work is something that threat

1148
00:38:26,720 --> 00:38:29,280
intelligence analysts do a lot

1149
00:38:29,280 --> 00:38:31,280
so building machine learning techniques

1150
00:38:31,280 --> 00:38:32,960
to create the security knowledge graph

1151
00:38:32,960 --> 00:38:34,000
automatically

1152
00:38:34,000 --> 00:38:37,599
is potentially a very powerful thing

1153
00:38:38,079 --> 00:38:40,240
um to construct the training data of

1154
00:38:40,240 --> 00:38:41,440
this data set

1155
00:38:41,440 --> 00:38:43,440
because we're now in the nlp domain it's

1156
00:38:43,440 --> 00:38:45,440
a totally different technique

1157
00:38:45,440 --> 00:38:47,520
instead of having like clean or malware

1158
00:38:47,520 --> 00:38:49,680
labels we have this labeling technique

1159
00:38:49,680 --> 00:38:50,560
called iob

1160
00:38:50,560 --> 00:38:53,920
or inside outside beginning and

1161
00:38:53,920 --> 00:38:55,599
so this is an example of how that would

1162
00:38:55,599 --> 00:38:57,200
work so eddard stark is

1163
00:38:57,200 --> 00:38:59,040
the phrase that indicates the name of a

1164
00:38:59,040 --> 00:39:00,880
person the first

1165
00:39:00,880 --> 00:39:02,800
token in that phrase will be b dash

1166
00:39:02,800 --> 00:39:04,480
person which stands for beginning of

1167
00:39:04,480 --> 00:39:05,200
person

1168
00:39:05,200 --> 00:39:07,520
and the second or all subsequent tokens

1169
00:39:07,520 --> 00:39:09,359
in that phrase will be tagged with eye

1170
00:39:09,359 --> 00:39:10,640
dash person which is

1171
00:39:10,640 --> 00:39:13,920
inside a person and all the uh tokens

1172
00:39:13,920 --> 00:39:14,800
that are

1173
00:39:14,800 --> 00:39:17,119
not in any of the you know phrases that

1174
00:39:17,119 --> 00:39:18,560
we care about or entities that we care

1175
00:39:18,560 --> 00:39:20,240
about will be tagged with o

1176
00:39:20,240 --> 00:39:23,279
which means outside

1177
00:39:23,359 --> 00:39:25,760
so uh here is an example of a sentence

1178
00:39:25,760 --> 00:39:27,680
from the cyber security domain about the

1179
00:39:27,680 --> 00:39:29,119
numbered panda actor

1180
00:39:29,119 --> 00:39:31,520
and here is how it would get uh you know

1181
00:39:31,520 --> 00:39:32,160
labeled

1182
00:39:32,160 --> 00:39:35,280
in the iob context so numbered panda

1183
00:39:35,280 --> 00:39:38,560
is the bad actor phrase and numbered is

1184
00:39:38,560 --> 00:39:40,079
the beginning of the bad actor phrase

1185
00:39:40,079 --> 00:39:42,400
and panda is the inside and bad active

1186
00:39:42,400 --> 00:39:44,560
phrase and all the subsequent tokens are

1187
00:39:44,560 --> 00:39:45,680
you know outside

1188
00:39:45,680 --> 00:39:48,240
etc

1189
00:39:49,440 --> 00:39:51,839
so uh again in the future extraction

1190
00:39:51,839 --> 00:39:53,359
space we have a different set of

1191
00:39:53,359 --> 00:39:54,640
features here

1192
00:39:54,640 --> 00:39:57,040
so we use some traditional nlp features

1193
00:39:57,040 --> 00:39:58,800
like you know part of speech lemma

1194
00:39:58,800 --> 00:40:01,839
word type etc and then we also have some

1195
00:40:01,839 --> 00:40:03,520
unsupervised features you know which is

1196
00:40:03,520 --> 00:40:05,119
word embeddings

1197
00:40:05,119 --> 00:40:07,920
um in very simple terms word embeddings

1198
00:40:07,920 --> 00:40:08,319
are

1199
00:40:08,319 --> 00:40:10,560
vector representations of words uh in

1200
00:40:10,560 --> 00:40:12,079
text such that their

1201
00:40:12,079 --> 00:40:13,839
meaning and semantic relationships are

1202
00:40:13,839 --> 00:40:15,680
captured by the numerical vectors

1203
00:40:15,680 --> 00:40:17,359
so for example if two words mean the

1204
00:40:17,359 --> 00:40:19,040
same thing or are used in the same

1205
00:40:19,040 --> 00:40:20,720
context a lot

1206
00:40:20,720 --> 00:40:22,880
the cosine similarity of their vectors

1207
00:40:22,880 --> 00:40:24,160
will be high

1208
00:40:24,160 --> 00:40:26,319
so this particular world meetings model

1209
00:40:26,319 --> 00:40:28,480
was trained using word to work on like a

1210
00:40:28,480 --> 00:40:30,079
corpus of ti data

1211
00:40:30,079 --> 00:40:33,280
so think security blogs apt white papers

1212
00:40:33,280 --> 00:40:36,800
incident response reports etc

1213
00:40:36,800 --> 00:40:38,400
and this is an example of how well the

1214
00:40:38,400 --> 00:40:40,400
word embeddings worked in this context

1215
00:40:40,400 --> 00:40:41,760
so this is the visualization of the

1216
00:40:41,760 --> 00:40:43,440
embeddings in tensorboard

1217
00:40:43,440 --> 00:40:44,960
um and since that embeddings were

1218
00:40:44,960 --> 00:40:46,960
trained uh on documents related to the

1219
00:40:46,960 --> 00:40:48,160
cyber domain

1220
00:40:48,160 --> 00:40:49,920
the clusters of points that are formed

1221
00:40:49,920 --> 00:40:51,839
here are related to the context in which

1222
00:40:51,839 --> 00:40:53,119
the words appear

1223
00:40:53,119 --> 00:40:55,440
so for instance the closest four points

1224
00:40:55,440 --> 00:40:56,800
to apd 28

1225
00:40:56,800 --> 00:40:58,560
two of them are its aliases and two of

1226
00:40:58,560 --> 00:41:00,800
them are other campaigns related to it

1227
00:41:00,800 --> 00:41:03,599
by attribution

1228
00:41:04,880 --> 00:41:07,119
this was the architecture of uh this

1229
00:41:07,119 --> 00:41:08,240
solution where

1230
00:41:08,240 --> 00:41:10,640
you know we had a corpus of blogs white

1231
00:41:10,640 --> 00:41:11,839
papers the text

1232
00:41:11,839 --> 00:41:14,640
we had some simple html and pdf parsers

1233
00:41:14,640 --> 00:41:16,400
we did some minimal text processing

1234
00:41:16,400 --> 00:41:18,240
just to clean out um and then we

1235
00:41:18,240 --> 00:41:20,240
extracted the word embeddings

1236
00:41:20,240 --> 00:41:22,400
we split it into a training and test

1237
00:41:22,400 --> 00:41:23,520
data set

1238
00:41:23,520 --> 00:41:25,119
we did the custom feature extraction

1239
00:41:25,119 --> 00:41:26,160
which i told you about like the

1240
00:41:26,160 --> 00:41:27,280
traditional nlp

1241
00:41:27,280 --> 00:41:28,960
features and then added the word

1242
00:41:28,960 --> 00:41:30,720
embedding features as well

1243
00:41:30,720 --> 00:41:32,079
and then we experimented with two

1244
00:41:32,079 --> 00:41:34,079
different models here crf

1245
00:41:34,079 --> 00:41:37,200
and lstm crf is

1246
00:41:37,200 --> 00:41:39,839
more like a linear model that is applied

1247
00:41:39,839 --> 00:41:42,000
to a sequence problem and lstm is a

1248
00:41:42,000 --> 00:41:43,680
special type of neural network

1249
00:41:43,680 --> 00:41:45,599
that is also applicable for sequence

1250
00:41:45,599 --> 00:41:47,359
problems they're both

1251
00:41:47,359 --> 00:41:49,119
very widely used specifically for

1252
00:41:49,119 --> 00:41:52,560
language models

1253
00:41:52,960 --> 00:41:56,720
um this is how we evaluated the model

1254
00:41:56,720 --> 00:41:58,560
uh we've measured the positive precision

1255
00:41:58,560 --> 00:42:01,680
and recall and here you can see that

1256
00:42:01,680 --> 00:42:03,520
the positive precision was pretty high

1257
00:42:03,520 --> 00:42:06,079
across the board for all of the models

1258
00:42:06,079 --> 00:42:08,079
but the recall you know the lstm was

1259
00:42:08,079 --> 00:42:09,440
kind of doing better in terms of the

1260
00:42:09,440 --> 00:42:11,839
recall

1261
00:42:11,839 --> 00:42:14,000
but then you know we looked at these

1262
00:42:14,000 --> 00:42:16,000
metrics on specifically uh

1263
00:42:16,000 --> 00:42:18,160
unseen tokens so these are the tokens

1264
00:42:18,160 --> 00:42:19,839
where um

1265
00:42:19,839 --> 00:42:21,280
these tokens were not present in the

1266
00:42:21,280 --> 00:42:22,560
training data set at all so these are

1267
00:42:22,560 --> 00:42:24,560
just net new tokens and we want to see

1268
00:42:24,560 --> 00:42:26,640
how the model does in terms of

1269
00:42:26,640 --> 00:42:28,079
identifying these

1270
00:42:28,079 --> 00:42:30,560
using the context in which they appear

1271
00:42:30,560 --> 00:42:31,920
so here you can see that

1272
00:42:31,920 --> 00:42:34,079
the crf for the embeddings it does it

1273
00:42:34,079 --> 00:42:36,400
performs uh the best

1274
00:42:36,400 --> 00:42:39,680
um and it was the only model that had

1275
00:42:39,680 --> 00:42:44,800
any manner of positive recall

1276
00:42:44,800 --> 00:42:47,200
so uh hopefully i've given you like a

1277
00:42:47,200 --> 00:42:49,520
diversity of like examples and kind of

1278
00:42:49,520 --> 00:42:50,640
piqued your interest

1279
00:42:50,640 --> 00:42:53,760
uh if you want to learn more uh

1280
00:42:53,760 --> 00:42:55,359
the basic machine learning course on

1281
00:42:55,359 --> 00:42:57,359
coursera by andrew eng it's really the

1282
00:42:57,359 --> 00:42:58,160
best

1283
00:42:58,160 --> 00:43:00,960
he is not only like a great visionary of

1284
00:43:00,960 --> 00:43:04,079
the field but also a really good teacher

1285
00:43:04,079 --> 00:43:05,839
um the malware data science book by

1286
00:43:05,839 --> 00:43:07,680
joshua saxey again a really good

1287
00:43:07,680 --> 00:43:08,400
resource

1288
00:43:08,400 --> 00:43:10,079
but very specific to the malware

1289
00:43:10,079 --> 00:43:12,400
classification problem

1290
00:43:12,400 --> 00:43:15,520
kgo is a data science platform where

1291
00:43:15,520 --> 00:43:18,000
various organizations provide data sets

1292
00:43:18,000 --> 00:43:18,880
and you know kind of

1293
00:43:18,880 --> 00:43:20,560
define a data science problem that they

1294
00:43:20,560 --> 00:43:22,480
want to solve and then everyone

1295
00:43:22,480 --> 00:43:24,240
can try their hand at solving the

1296
00:43:24,240 --> 00:43:26,240
problem optimize their solution

1297
00:43:26,240 --> 00:43:27,680
and then usually a lot of people will

1298
00:43:27,680 --> 00:43:29,680
also share their solutions so it's

1299
00:43:29,680 --> 00:43:32,560
a great place to learn uh about applying

1300
00:43:32,560 --> 00:43:34,560
ml

1301
00:43:34,560 --> 00:43:36,480
and the last thing is canvas which is

1302
00:43:36,480 --> 00:43:37,760
the conference on applied machine

1303
00:43:37,760 --> 00:43:39,680
learning for information security um

1304
00:43:39,680 --> 00:43:41,520
it's a relatively new conference

1305
00:43:41,520 --> 00:43:45,440
um the the last edition was in 2019

1306
00:43:45,440 --> 00:43:46,960
they didn't do one last year because of

1307
00:43:46,960 --> 00:43:48,560
the pandemic but i think there's going

1308
00:43:48,560 --> 00:43:50,079
to be one more in

1309
00:43:50,079 --> 00:43:53,680
2021 so yeah these are some things that

1310
00:43:53,680 --> 00:43:54,960
you can use if you want to learn more

1311
00:43:54,960 --> 00:43:57,599
about the space

1312
00:43:57,920 --> 00:44:02,000
and with that i will leave some time for

1313
00:44:06,839 --> 00:44:09,839
questions

1314
00:44:30,880 --> 00:44:34,800
hey so um i don't see any questions

1315
00:44:34,800 --> 00:44:37,839
there's the question

1316
00:44:38,000 --> 00:44:41,520
we are using ml in security if ml

1317
00:44:41,520 --> 00:44:45,440
secure itself yeah so that's a great

1318
00:44:45,440 --> 00:44:46,079
question

1319
00:44:46,079 --> 00:44:48,160
uh like all systems you know ml is not

1320
00:44:48,160 --> 00:44:49,200
perfectly secure

1321
00:44:49,200 --> 00:44:50,640
and i think i talked about some of you

1322
00:44:50,640 --> 00:44:52,480
know the adversarial ml

1323
00:44:52,480 --> 00:44:54,720
and how different machine learning

1324
00:44:54,720 --> 00:44:56,720
techniques themselves are used to attack

1325
00:44:56,720 --> 00:44:58,319
other machine learning based detection

1326
00:44:58,319 --> 00:45:00,079
systems and there are several academic

1327
00:45:00,079 --> 00:45:01,359
papers on that

1328
00:45:01,359 --> 00:45:03,839
um you know but it's an evolving space

1329
00:45:03,839 --> 00:45:05,440
it's not secure but there is

1330
00:45:05,440 --> 00:45:08,480
a research around robustness on how to

1331
00:45:08,480 --> 00:45:09,440
protect against

1332
00:45:09,440 --> 00:45:11,520
different types of attacks for machine

1333
00:45:11,520 --> 00:45:12,480
learning

1334
00:45:12,480 --> 00:45:14,960
um so yeah it's again it's a powerful

1335
00:45:14,960 --> 00:45:18,960
tool but it's not perfect at all

1336
00:45:18,960 --> 00:45:23,280
and uh what are your thoughts on using

1337
00:45:23,280 --> 00:45:26,400
machine learning for fishing

1338
00:45:26,400 --> 00:45:29,440
attacks um fishing is actually

1339
00:45:29,440 --> 00:45:32,800
uh one of the pri one of the first

1340
00:45:32,800 --> 00:45:34,880
few applications uh where machine

1341
00:45:34,880 --> 00:45:36,560
learning was applied

1342
00:45:36,560 --> 00:45:38,720
uh so it's definitely very applicable

1343
00:45:38,720 --> 00:45:40,480
and you can use multiple different types

1344
00:45:40,480 --> 00:45:42,000
of techniques you know you can apply

1345
00:45:42,000 --> 00:45:43,520
machine learning to the

1346
00:45:43,520 --> 00:45:45,359
you know the context you know the full

1347
00:45:45,359 --> 00:45:46,880
email content uh

1348
00:45:46,880 --> 00:45:49,440
you know the arrival of the email there

1349
00:45:49,440 --> 00:45:50,240
is even

1350
00:45:50,240 --> 00:45:53,760
um research around applying machine

1351
00:45:53,760 --> 00:45:54,400
learning

1352
00:45:54,400 --> 00:45:57,119
to detect brand impersonation so you

1353
00:45:57,119 --> 00:45:58,160
know a lot of phishing

1354
00:45:58,160 --> 00:46:00,319
emails will try to impersonate like

1355
00:46:00,319 --> 00:46:01,839
legitimate brands to get

1356
00:46:01,839 --> 00:46:03,680
people to click or enter their password

1357
00:46:03,680 --> 00:46:04,880
etc

1358
00:46:04,880 --> 00:46:07,280
so yeah phishing has definitely a lot of

1359
00:46:07,280 --> 00:46:10,480
applications of machine learning

1360
00:46:11,200 --> 00:46:14,800
next question uh what are the algorithms

1361
00:46:14,800 --> 00:46:16,960
used for feature selection in malware

1362
00:46:16,960 --> 00:46:17,760
detection

1363
00:46:17,760 --> 00:46:21,359
example e headers data since the use

1364
00:46:21,359 --> 00:46:22,880
case is critical did it

1365
00:46:22,880 --> 00:46:26,720
often require manual intervention

1366
00:46:26,720 --> 00:46:28,720
so this is a really interesting question

1367
00:46:28,720 --> 00:46:30,319
and different people

1368
00:46:30,319 --> 00:46:32,160
uh have different approaches here i

1369
00:46:32,160 --> 00:46:34,000
think what has worked

1370
00:46:34,000 --> 00:46:36,319
very well so here's what will happen you

1371
00:46:36,319 --> 00:46:38,800
can usually build a deep learning model

1372
00:46:38,800 --> 00:46:40,800
that will automatically select the

1373
00:46:40,800 --> 00:46:42,560
features under the hood

1374
00:46:42,560 --> 00:46:45,359
and will produce really good performance

1375
00:46:45,359 --> 00:46:45,760
but

1376
00:46:45,760 --> 00:46:47,280
the problem with that is that deep

1377
00:46:47,280 --> 00:46:49,440
learning models tend to be heavy and

1378
00:46:49,440 --> 00:46:51,119
less efficient you know they require

1379
00:46:51,119 --> 00:46:52,880
higher compute and they have

1380
00:46:52,880 --> 00:46:56,079
uh they need more processing so uh

1381
00:46:56,079 --> 00:46:57,839
what happens is that usually what i have

1382
00:46:57,839 --> 00:46:59,839
seen work better to build lighter weight

1383
00:46:59,839 --> 00:47:01,359
models is to have like a security

1384
00:47:01,359 --> 00:47:03,280
researcher who is an expert in the field

1385
00:47:03,280 --> 00:47:04,480
and who can identify

1386
00:47:04,480 --> 00:47:06,800
features so keep in mind that by

1387
00:47:06,800 --> 00:47:08,319
features i don't mean like each and

1388
00:47:08,319 --> 00:47:09,920
every pe header that you know you have

1389
00:47:09,920 --> 00:47:10,400
to like

1390
00:47:10,400 --> 00:47:11,839
say whether it's a feature or not but

1391
00:47:11,839 --> 00:47:13,839
you can just say that okay i'm going to

1392
00:47:13,839 --> 00:47:15,200
look at this field

1393
00:47:15,200 --> 00:47:17,040
that contains all the pe headers as a

1394
00:47:17,040 --> 00:47:19,200
feature or i might i might extract like

1395
00:47:19,200 --> 00:47:20,160
the engrams

1396
00:47:20,160 --> 00:47:22,240
from this particular field and use that

1397
00:47:22,240 --> 00:47:23,680
as features because i think there is

1398
00:47:23,680 --> 00:47:25,520
interesting information here

1399
00:47:25,520 --> 00:47:28,720
so there are multiple ways to do it and

1400
00:47:28,720 --> 00:47:30,400
there is no right or wrong answer it

1401
00:47:30,400 --> 00:47:32,400
really depends on

1402
00:47:32,400 --> 00:47:37,599
your uh your use case essentially

1403
00:47:37,680 --> 00:47:40,319
next question three reasons why machine

1404
00:47:40,319 --> 00:47:41,760
learning security techniques

1405
00:47:41,760 --> 00:47:45,680
over other security techniques

1406
00:47:46,000 --> 00:47:49,680
why not learning security techniques

1407
00:47:50,839 --> 00:47:53,119
um maybe the question

1408
00:47:53,119 --> 00:47:54,960
uh i don't quite understand this

1409
00:47:54,960 --> 00:47:56,319
question

1410
00:47:56,319 --> 00:47:58,960
so machine learning's applications to

1411
00:47:58,960 --> 00:47:59,839
security

1412
00:47:59,839 --> 00:48:01,760
are i'm not saying that they are

1413
00:48:01,760 --> 00:48:03,200
necessarily

1414
00:48:03,200 --> 00:48:06,160
going to okay so this is how this is

1415
00:48:06,160 --> 00:48:07,599
what i understand the question like why

1416
00:48:07,599 --> 00:48:09,200
do you want to use machine learning

1417
00:48:09,200 --> 00:48:10,079
based

1418
00:48:10,079 --> 00:48:12,319
uh protections and defenses rather than

1419
00:48:12,319 --> 00:48:14,240
just using the traditional side

1420
00:48:14,240 --> 00:48:16,319
um the biggest reason is that you know

1421
00:48:16,319 --> 00:48:17,680
the scale of the problem

1422
00:48:17,680 --> 00:48:19,920
of defending you know the internet that

1423
00:48:19,920 --> 00:48:22,000
we have right now is very very large so

1424
00:48:22,000 --> 00:48:25,200
it's not always possible to um use

1425
00:48:25,200 --> 00:48:26,559
like the traditional methods so for

1426
00:48:26,559 --> 00:48:28,400
example at the speed that the malware

1427
00:48:28,400 --> 00:48:30,480
landscape is evolving like if i had a

1428
00:48:30,480 --> 00:48:32,400
huge team of security researchers who

1429
00:48:32,400 --> 00:48:33,440
were building

1430
00:48:33,440 --> 00:48:35,359
really good genetic signatures but it

1431
00:48:35,359 --> 00:48:37,040
would be really hard for them to keep up

1432
00:48:37,040 --> 00:48:38,559
with the skill that we have

1433
00:48:38,559 --> 00:48:41,200
um so that is the power of machine

1434
00:48:41,200 --> 00:48:42,800
learning like it can help defend your

1435
00:48:42,800 --> 00:48:43,520
skill

1436
00:48:43,520 --> 00:48:45,200
but obviously there is a downside you

1437
00:48:45,200 --> 00:48:46,880
know like these algorithms are not

1438
00:48:46,880 --> 00:48:50,240
perfect they suffer from bias

1439
00:48:50,240 --> 00:48:53,040
and they need you know a lot of work to

1440
00:48:53,040 --> 00:48:53,520
uh

1441
00:48:53,520 --> 00:48:55,520
maintain and also i think the most

1442
00:48:55,520 --> 00:48:56,960
critical thing is that in terms of

1443
00:48:56,960 --> 00:48:58,480
discovering new attacks

1444
00:48:58,480 --> 00:49:00,640
and in terms of like protecting against

1445
00:49:00,640 --> 00:49:02,800
newer attacks

1446
00:49:02,800 --> 00:49:04,880
securities like just traditional

1447
00:49:04,880 --> 00:49:06,640
security researchers and those kind of

1448
00:49:06,640 --> 00:49:07,520
techniques

1449
00:49:07,520 --> 00:49:09,839
have shown more efficacy but at the same

1450
00:49:09,839 --> 00:49:11,520
time you know there is a large section

1451
00:49:11,520 --> 00:49:13,119
where these machine learning techniques

1452
00:49:13,119 --> 00:49:15,200
are applicable and you know what happens

1453
00:49:15,200 --> 00:49:16,319
is that

1454
00:49:16,319 --> 00:49:18,319
applying these techniques can free up

1455
00:49:18,319 --> 00:49:20,400
your security research resources so that

1456
00:49:20,400 --> 00:49:22,000
they can focus on the

1457
00:49:22,000 --> 00:49:24,480
you know newer or higher criticality

1458
00:49:24,480 --> 00:49:26,880
problems

1459
00:49:29,440 --> 00:49:32,319
okay next question sometimes our data

1460
00:49:32,319 --> 00:49:33,599
sets need protection

1461
00:49:33,599 --> 00:49:36,240
and can't be known to everyone how to

1462
00:49:36,240 --> 00:49:38,160
ensure security of data sets through

1463
00:49:38,160 --> 00:49:41,280
ml uh that's

1464
00:49:41,280 --> 00:49:43,680
that's a really interesting question so

1465
00:49:43,680 --> 00:49:45,440
one way to interpret this question is

1466
00:49:45,440 --> 00:49:47,440
that like how can we

1467
00:49:47,440 --> 00:49:48,880
make sure that our machine learning

1468
00:49:48,880 --> 00:49:50,559
algorithms are not

1469
00:49:50,559 --> 00:49:52,400
um breaking the privacy of the

1470
00:49:52,400 --> 00:49:53,839
underlying data set

1471
00:49:53,839 --> 00:49:56,559
and that is a whole domain uh you know

1472
00:49:56,559 --> 00:49:56,960
under

1473
00:49:56,960 --> 00:50:00,240
um fairness and responsible ai where we

1474
00:50:00,240 --> 00:50:02,160
want to have privacy preserving ai and

1475
00:50:02,160 --> 00:50:03,119
ml

1476
00:50:03,119 --> 00:50:07,359
but um maybe maybe the question also

1477
00:50:07,359 --> 00:50:09,760
means that how do you defend

1478
00:50:09,760 --> 00:50:11,920
data that is stored in various servers

1479
00:50:11,920 --> 00:50:13,359
using machine learning

1480
00:50:13,359 --> 00:50:16,319
um or maybe how do you prevent data

1481
00:50:16,319 --> 00:50:18,240
exfiltration using machine learning

1482
00:50:18,240 --> 00:50:20,160
so there are definitely series of

1483
00:50:20,160 --> 00:50:22,240
techniques particularly using anomaly

1484
00:50:22,240 --> 00:50:23,280
detection

1485
00:50:23,280 --> 00:50:25,599
uh which look at the operations that are

1486
00:50:25,599 --> 00:50:27,520
taken on data sets so

1487
00:50:27,520 --> 00:50:29,680
for example i might have a sequence of

1488
00:50:29,680 --> 00:50:30,880
uh operations that were

1489
00:50:30,880 --> 00:50:33,520
taken on a data set and i might you know

1490
00:50:33,520 --> 00:50:35,359
build an anomaly detection on that

1491
00:50:35,359 --> 00:50:38,480
so for instance uh maybe i have uh

1492
00:50:38,480 --> 00:50:39,760
you know maybe i have a sharepoint

1493
00:50:39,760 --> 00:50:41,200
server where you know a certain team

1494
00:50:41,200 --> 00:50:43,119
accesses these documents regularly but

1495
00:50:43,119 --> 00:50:45,520
then one day i see a person from a just

1496
00:50:45,520 --> 00:50:47,040
a totally different team access that

1497
00:50:47,040 --> 00:50:47,920
data

1498
00:50:47,920 --> 00:50:50,240
and that event uh kind of triggers the

1499
00:50:50,240 --> 00:50:51,599
anomaly detection

1500
00:50:51,599 --> 00:50:55,839
so those kind of applications also exist

1501
00:50:57,440 --> 00:51:00,079
next question could you comment on the

1502
00:51:00,079 --> 00:51:00,960
quality of

1503
00:51:00,960 --> 00:51:03,680
open source data sets for ml in security

1504
00:51:03,680 --> 00:51:05,520
domain

1505
00:51:05,520 --> 00:51:07,440
yeah absolutely so that's a great

1506
00:51:07,440 --> 00:51:09,920
question because if we want researchers

1507
00:51:09,920 --> 00:51:11,119
and academia

1508
00:51:11,119 --> 00:51:13,200
to do good research for ml in the

1509
00:51:13,200 --> 00:51:14,880
security domain we want we need to

1510
00:51:14,880 --> 00:51:16,480
provide them with good open source data

1511
00:51:16,480 --> 00:51:17,119
sets

1512
00:51:17,119 --> 00:51:18,800
and in the beginning there were a lot of

1513
00:51:18,800 --> 00:51:20,640
restrictions around

1514
00:51:20,640 --> 00:51:22,960
privacy and compliance that prevented

1515
00:51:22,960 --> 00:51:24,960
companies from sharing this data

1516
00:51:24,960 --> 00:51:27,200
but recently some companies uh like so

1517
00:51:27,200 --> 00:51:28,400
forth they have made

1518
00:51:28,400 --> 00:51:31,119
huge strides in that in that regard so

1519
00:51:31,119 --> 00:51:33,119
the soil data set that i mentioned

1520
00:51:33,119 --> 00:51:34,720
it is a really high quality data set

1521
00:51:34,720 --> 00:51:36,319
that can be used uh

1522
00:51:36,319 --> 00:51:39,599
for you know training almost real life

1523
00:51:39,599 --> 00:51:41,040
machine learning models i haven't

1524
00:51:41,040 --> 00:51:43,359
experimented with myself but it seems

1525
00:51:43,359 --> 00:51:46,240
really comprehensive

1526
00:51:47,520 --> 00:51:50,000
next question in the beginning of the

1527
00:51:50,000 --> 00:51:50,559
talk

1528
00:51:50,559 --> 00:51:52,880
uh you mentioned contextual based

1529
00:51:52,880 --> 00:51:55,920
features can you elaborate on that

1530
00:51:55,920 --> 00:51:58,079
yeah so by contextual features i mean

1531
00:51:58,079 --> 00:51:59,839
you know looking at the context in which

1532
00:51:59,839 --> 00:52:00,720
a particular

1533
00:52:00,720 --> 00:52:03,280
file appears so was it downloaded from

1534
00:52:03,280 --> 00:52:04,800
the web was it downloaded from an

1535
00:52:04,800 --> 00:52:06,000
untrusted website

1536
00:52:06,000 --> 00:52:09,119
was it um you know is it which folder is

1537
00:52:09,119 --> 00:52:09,359
it

1538
00:52:09,359 --> 00:52:12,720
in what kind of uh name does it have so

1539
00:52:12,720 --> 00:52:14,079
all of these features that

1540
00:52:14,079 --> 00:52:16,079
have nothing to do with the content of

1541
00:52:16,079 --> 00:52:18,720
the file or you know what actions it

1542
00:52:18,720 --> 00:52:19,920
takes but it's just like

1543
00:52:19,920 --> 00:52:22,000
what is the context in which it appears

1544
00:52:22,000 --> 00:52:23,599
because for example if like

1545
00:52:23,599 --> 00:52:25,359
this file that you have just downloaded

1546
00:52:25,359 --> 00:52:28,240
from an untrusted website if it you know

1547
00:52:28,240 --> 00:52:29,119
goes into the

1548
00:52:29,119 --> 00:52:30,880
if it appears in the system folder with

1549
00:52:30,880 --> 00:52:32,400
like a really similar name to a

1550
00:52:32,400 --> 00:52:33,599
legitimate file

1551
00:52:33,599 --> 00:52:36,000
then that is inherently more suspicious

1552
00:52:36,000 --> 00:52:37,520
than you know something in the downloads

1553
00:52:37,520 --> 00:52:40,319
folder for instance

1554
00:52:41,119 --> 00:52:44,079
next question uh has microsoft explored

1555
00:52:44,079 --> 00:52:46,240
computer vision techniques for malware

1556
00:52:46,240 --> 00:52:49,040
bytes data

1557
00:52:49,040 --> 00:52:51,119
so there have been we have done some

1558
00:52:51,119 --> 00:52:52,400
experiments uh

1559
00:52:52,400 --> 00:52:55,280
using to apply computer vision to like

1560
00:52:55,280 --> 00:52:56,640
just the distribution

1561
00:52:56,640 --> 00:52:59,520
of malwarebytes i think i remember

1562
00:52:59,520 --> 00:53:00,319
seeing like

1563
00:53:00,319 --> 00:53:03,200
some open source blogs as well on it uh

1564
00:53:03,200 --> 00:53:05,119
but in general those techniques were not

1565
00:53:05,119 --> 00:53:07,440
found to be efficient for the production

1566
00:53:07,440 --> 00:53:09,119
context so they're not used

1567
00:53:09,119 --> 00:53:12,319
uh in production um because frankly for

1568
00:53:12,319 --> 00:53:14,240
malware classification

1569
00:53:14,240 --> 00:53:15,760
i mean why would i use like computer

1570
00:53:15,760 --> 00:53:17,359
vision if i can just get a

1571
00:53:17,359 --> 00:53:18,960
really similar performance using linear

1572
00:53:18,960 --> 00:53:20,400
models so

1573
00:53:20,400 --> 00:53:23,119
that's kind of been the driving force so

1574
00:53:23,119 --> 00:53:23,520
far

1575
00:53:23,520 --> 00:53:26,000
um but there are some there is some

1576
00:53:26,000 --> 00:53:29,520
research around that space

1577
00:53:29,520 --> 00:53:32,240
next question windows defender use

1578
00:53:32,240 --> 00:53:34,079
layered machine learning models can you

1579
00:53:34,079 --> 00:53:34,559
describe

1580
00:53:34,559 --> 00:53:38,240
that yeah of course so uh the layered

1581
00:53:38,240 --> 00:53:39,440
machine learning models

1582
00:53:39,440 --> 00:53:40,559
in defender you know i kind of shared

1583
00:53:40,559 --> 00:53:42,720
one slide on that but i what happens is

1584
00:53:42,720 --> 00:53:44,079
that there is one layer of machine

1585
00:53:44,079 --> 00:53:45,760
learning models that sit on the client

1586
00:53:45,760 --> 00:53:46,559
itself

1587
00:53:46,559 --> 00:53:48,400
uh these are typically very simple

1588
00:53:48,400 --> 00:53:50,880
models and they will usually not give a

1589
00:53:50,880 --> 00:53:52,319
malware or clean verdict they will only

1590
00:53:52,319 --> 00:53:54,400
say suspicious and not suspicious

1591
00:53:54,400 --> 00:53:55,920
and if something is found to be

1592
00:53:55,920 --> 00:53:58,319
suspicious uh it will like a certain

1593
00:53:58,319 --> 00:54:00,400
metadata package will be sent up to the

1594
00:54:00,400 --> 00:54:02,079
cloud and that is where like the

1595
00:54:02,079 --> 00:54:04,079
majority of our machine learning models

1596
00:54:04,079 --> 00:54:04,720
live

1597
00:54:04,720 --> 00:54:06,960
uh usually they are usually they are

1598
00:54:06,960 --> 00:54:07,839
linear uh

1599
00:54:07,839 --> 00:54:09,119
machine learning models average

1600
00:54:09,119 --> 00:54:11,760
perceptrons uh boosted trees et cetera

1601
00:54:11,760 --> 00:54:13,920
but there are some deep learning models

1602
00:54:13,920 --> 00:54:15,440
that we have recently deployed to the

1603
00:54:15,440 --> 00:54:16,480
cloud as well

1604
00:54:16,480 --> 00:54:18,559
so these models will look at you know

1605
00:54:18,559 --> 00:54:20,640
this metadata vector and we'll say

1606
00:54:20,640 --> 00:54:21,119
whether

1607
00:54:21,119 --> 00:54:23,839
uh you know file is now over or not and

1608
00:54:23,839 --> 00:54:25,440
so far these models are very

1609
00:54:25,440 --> 00:54:26,800
uh you know they'll give a quick

1610
00:54:26,800 --> 00:54:29,599
response they're highly efficient

1611
00:54:29,599 --> 00:54:32,079
um if even these models are not able to

1612
00:54:32,079 --> 00:54:33,599
give a good verdict and

1613
00:54:33,599 --> 00:54:35,119
they still you know put that put the

1614
00:54:35,119 --> 00:54:36,839
file in a certain bucket of

1615
00:54:36,839 --> 00:54:38,000
suspiciousness

1616
00:54:38,000 --> 00:54:40,000
then you know we'll request the sample

1617
00:54:40,000 --> 00:54:41,359
and then we have some

1618
00:54:41,359 --> 00:54:45,200
deeper some neural networks dnn based

1619
00:54:45,200 --> 00:54:47,839
that will look at the sample itself

1620
00:54:47,839 --> 00:54:49,200
analyze the static

1621
00:54:49,200 --> 00:54:51,280
features detonate it and use all of

1622
00:54:51,280 --> 00:54:52,799
these features to say whether the file

1623
00:54:52,799 --> 00:54:54,079
is malicious or not

1624
00:54:54,079 --> 00:54:55,760
so these are kind of the various layers

1625
00:54:55,760 --> 00:54:59,520
that we have in the defender land

1626
00:55:03,119 --> 00:55:05,520
there is a question in jack can a ml

1627
00:55:05,520 --> 00:55:06,799
model be reverse

1628
00:55:06,799 --> 00:55:11,040
engineered i think that's what they mean

1629
00:55:12,160 --> 00:55:15,359
um so i think it is possible to sort of

1630
00:55:15,359 --> 00:55:17,680
reverse engineer an ml model so

1631
00:55:17,680 --> 00:55:19,040
one of the things that i have played

1632
00:55:19,040 --> 00:55:20,720
around with is you know once

1633
00:55:20,720 --> 00:55:22,480
a machine learning model has given an

1634
00:55:22,480 --> 00:55:24,240
output or you know a series of output on

1635
00:55:24,240 --> 00:55:24,960
my uh

1636
00:55:24,960 --> 00:55:28,079
inference data set then i can you know

1637
00:55:28,079 --> 00:55:29,920
reverse engineer what are the features

1638
00:55:29,920 --> 00:55:31,119
that were important

1639
00:55:31,119 --> 00:55:32,720
in that context you know why did a

1640
00:55:32,720 --> 00:55:35,520
certain file get classified as malware

1641
00:55:35,520 --> 00:55:37,760
uh so and i can see which features

1642
00:55:37,760 --> 00:55:39,359
contributed to it being classified as

1643
00:55:39,359 --> 00:55:40,400
malware

1644
00:55:40,400 --> 00:55:43,520
um in that sense it is possible uh to

1645
00:55:43,520 --> 00:55:44,720
reverse engineer

1646
00:55:44,720 --> 00:55:47,920
some models but it's not always

1647
00:55:47,920 --> 00:55:49,280
meaningful because a lot of times

1648
00:55:49,280 --> 00:55:51,119
features are you know maybe like it's an

1649
00:55:51,119 --> 00:55:52,559
engram of a string or something so it

1650
00:55:52,559 --> 00:55:53,359
doesn't

1651
00:55:53,359 --> 00:55:55,440
um give you very much information but

1652
00:55:55,440 --> 00:55:58,880
technically it is possible to do that

1653
00:55:59,599 --> 00:56:03,440
uh maybe last question okay yeah

1654
00:56:03,440 --> 00:56:06,400
uh what can be used to reverse these

1655
00:56:06,400 --> 00:56:08,799
models

1656
00:56:08,880 --> 00:56:12,319
um so there are there are black box

1657
00:56:12,319 --> 00:56:13,760
techniques and then there are white box

1658
00:56:13,760 --> 00:56:14,960
techniques i think the stuff that i

1659
00:56:14,960 --> 00:56:16,079
talked about was the white box

1660
00:56:16,079 --> 00:56:17,119
techniques where

1661
00:56:17,119 --> 00:56:19,920
i have access to the model and you know

1662
00:56:19,920 --> 00:56:20,559
i can

1663
00:56:20,559 --> 00:56:22,799
inference it on a series of data set and

1664
00:56:22,799 --> 00:56:24,000
i can see you know

1665
00:56:24,000 --> 00:56:25,200
which are the features that are

1666
00:56:25,200 --> 00:56:27,760
important uh for the model and which are

1667
00:56:27,760 --> 00:56:28,880
the features that are contributing

1668
00:56:28,880 --> 00:56:30,079
maximum to the malicious

1669
00:56:30,079 --> 00:56:34,000
verdict etc um so

1670
00:56:34,000 --> 00:56:36,240
tools are just like you know coding

1671
00:56:36,240 --> 00:56:37,040
python

1672
00:56:37,040 --> 00:56:39,040
and just libraries so you don't need

1673
00:56:39,040 --> 00:56:40,400
like

1674
00:56:40,400 --> 00:56:45,839
a specific tool set here

1675
00:56:45,920 --> 00:56:48,960
last question what is the latest

1676
00:56:48,960 --> 00:56:50,480
machine learning applications for

1677
00:56:50,480 --> 00:56:53,040
security

1678
00:56:53,040 --> 00:56:54,720
oh wow this is a really fast moving

1679
00:56:54,720 --> 00:56:56,400
domain so i don't know quite what is

1680
00:56:56,400 --> 00:56:58,400
like the absolute latest thing

1681
00:56:58,400 --> 00:57:02,559
um i think the stuff that i have found

1682
00:57:02,559 --> 00:57:04,079
most exciting is applications of

1683
00:57:04,079 --> 00:57:06,240
reinforcement learning to security

1684
00:57:06,240 --> 00:57:07,599
again this has not been seen in the

1685
00:57:07,599 --> 00:57:10,480
production context yet but

1686
00:57:10,480 --> 00:57:12,480
one of my colleagues recently published

1687
00:57:12,480 --> 00:57:14,720
an uh an open source library

1688
00:57:14,720 --> 00:57:16,880
uh where you can have where you can

1689
00:57:16,880 --> 00:57:17,839
simulate uh

1690
00:57:17,839 --> 00:57:20,240
rl based uh attacks and defense models

1691
00:57:20,240 --> 00:57:21,520
for security

1692
00:57:21,520 --> 00:57:22,880
so that kind of stuff is very exciting

1693
00:57:22,880 --> 00:57:24,880
you know how can um

1694
00:57:24,880 --> 00:57:27,599
how can you leverage machine learning to

1695
00:57:27,599 --> 00:57:29,200
automatically attack systems and then in

1696
00:57:29,200 --> 00:57:30,880
the future automatically defend systems

1697
00:57:30,880 --> 00:57:41,839
as well

1698
00:57:43,520 --> 00:57:46,720
hey nimisha you're on mute

1699
00:57:47,119 --> 00:57:50,000
i'm sorry that's it for the q a i think

1700
00:57:50,000 --> 00:57:51,200
yeah you answered

1701
00:57:51,200 --> 00:57:53,680
all the questions so thank you for the

1702
00:57:53,680 --> 00:57:56,160
in information session donna

1703
00:57:56,160 --> 00:58:01,040
awesome thanks for having me

