1
00:00:00,030 --> 00:00:03,270
alright we'll go ahead and get started

2
00:00:03,270 --> 00:00:09,960
on to our final keynote on the day for

3
00:00:09,960 --> 00:00:11,280
the for the threat intelligence

4
00:00:11,280 --> 00:00:14,250
practitioner summit I'm very happy to

5
00:00:14,250 --> 00:00:17,090
have Andrea here to actually talk about

6
00:00:17,090 --> 00:00:21,180
the human side of all of this and this

7
00:00:21,180 --> 00:00:22,350
is an area that I'm particularly

8
00:00:22,350 --> 00:00:25,080
passionate about that cybersecurity is

9
00:00:25,080 --> 00:00:26,820
really not just a technical discipline

10
00:00:26,820 --> 00:00:29,400
but is also a very much a human

11
00:00:29,400 --> 00:00:31,260
discipline and so this is a very

12
00:00:31,260 --> 00:00:33,899
important topic and I'm glad that we can

13
00:00:33,899 --> 00:00:35,850
have a talk on this on this point so

14
00:00:35,850 --> 00:00:38,640
Andrea thanks Michael um thanks everyone

15
00:00:38,640 --> 00:00:40,350
for coming and virus bulletin for

16
00:00:40,350 --> 00:00:41,790
hosting this and for you guys for

17
00:00:41,790 --> 00:00:44,160
sponsoring this it's been great hearing

18
00:00:44,160 --> 00:00:45,570
so much about data sharing and

19
00:00:45,570 --> 00:00:47,309
collaboration actually over the last two

20
00:00:47,309 --> 00:00:50,309
days and so I think a lot of fits into a

21
00:00:50,309 --> 00:00:51,710
lot of themes I've been seeing across

22
00:00:51,710 --> 00:00:54,899
the two days of talks so far I'm just

23
00:00:54,899 --> 00:00:56,760
quick background I'm trained as an

24
00:00:56,760 --> 00:00:59,070
analyst social scientist worked in the

25
00:00:59,070 --> 00:01:00,030
Department of Defense doing a lot of

26
00:01:00,030 --> 00:01:03,149
threat Intel analysis to event a cyber

27
00:01:03,149 --> 00:01:04,438
security role now and even work to a

28
00:01:04,438 --> 00:01:06,600
variety of startups and so what you'll

29
00:01:06,600 --> 00:01:07,650
see throughout this talk is sort of an

30
00:01:07,650 --> 00:01:10,170
integration of various aspects of my

31
00:01:10,170 --> 00:01:11,790
career that blend the the social social

32
00:01:11,790 --> 00:01:13,950
element with a technical element and why

33
00:01:13,950 --> 00:01:15,570
I think secure sharing is so

34
00:01:15,570 --> 00:01:18,450
foundational to this important selection

35
00:01:18,450 --> 00:01:20,250
point we're actually at right now in the

36
00:01:20,250 --> 00:01:22,020
world system as democracies are on

37
00:01:22,020 --> 00:01:23,610
decline and authoritarian especially

38
00:01:23,610 --> 00:01:24,869
digital authoritarianism is on the rise

39
00:01:24,869 --> 00:01:26,820
secure sharing can provide a nice

40
00:01:26,820 --> 00:01:30,390
counterweight to some of those trends so

41
00:01:30,390 --> 00:01:32,549
what we'll be going over quite a bit

42
00:01:32,549 --> 00:01:33,930
first I want to talk about the status

43
00:01:33,930 --> 00:01:36,200
quo that's going on right now as far as

44
00:01:36,200 --> 00:01:38,280
the mountains of data that's out there

45
00:01:38,280 --> 00:01:39,630
but we still don't have access to the

46
00:01:39,630 --> 00:01:41,460
right data and talk about some of the

47
00:01:41,460 --> 00:01:43,920
reasons why that's the case they want to

48
00:01:43,920 --> 00:01:45,299
take a look at the so socio-technical

49
00:01:45,299 --> 00:01:48,060
look at some recommendations for how we

50
00:01:48,060 --> 00:01:49,439
can build up some the secure sharing and

51
00:01:49,439 --> 00:01:51,479
overcome the state of deserts issue and

52
00:01:51,479 --> 00:01:53,130
then I'll get into a tale of two futures

53
00:01:53,130 --> 00:01:56,790
one with sharing and one without and why

54
00:01:56,790 --> 00:01:57,990
that is going to be so essential as we

55
00:01:57,990 --> 00:02:00,229
look ahead at the future of the Internet

56
00:02:00,229 --> 00:02:03,240
so first once been a ton of time on this

57
00:02:03,240 --> 00:02:05,250
we're all well-versed in it but we know

58
00:02:05,250 --> 00:02:06,420
those bubble charts just keep getting

59
00:02:06,420 --> 00:02:08,610
bigger we keep seeing the Marriot and

60
00:02:08,610 --> 00:02:11,489
the Equifax and the new Yahoo it's fine

61
00:02:11,489 --> 00:02:13,950
I had this chart once a few weeks ago

62
00:02:13,950 --> 00:02:15,000
and literally the talk right after me

63
00:02:15,000 --> 00:02:16,349
had the big bubble charge with each

64
00:02:16,349 --> 00:02:19,110
other that the the breaches and so we

65
00:02:19,110 --> 00:02:20,220
know this and then and that's

66
00:02:20,220 --> 00:02:22,470
problematic obviously and it gets back

67
00:02:22,470 --> 00:02:25,410
to the proliferation anyone saw Salinas

68
00:02:25,410 --> 00:02:26,970
talk a little earlier today she talked

69
00:02:26,970 --> 00:02:28,560
about this as well in both the IT and

70
00:02:28,560 --> 00:02:30,810
the OT worlds as far as proliferation

71
00:02:30,810 --> 00:02:33,120
and it's a big issue both for targets

72
00:02:33,120 --> 00:02:34,590
but also for the tactic and tools and

73
00:02:34,590 --> 00:02:36,540
then the threat actors we're seeing this

74
00:02:36,540 --> 00:02:38,220
again obviously could be a whole talk in

75
00:02:38,220 --> 00:02:40,140
itself but there's a lot of diffusion of

76
00:02:40,140 --> 00:02:41,819
the capabilities from nation-states to a

77
00:02:41,819 --> 00:02:44,819
whole range of actors and along with the

78
00:02:44,819 --> 00:02:46,140
tools and tactics that they're doing and

79
00:02:46,140 --> 00:02:48,030
really where the point where seemingly

80
00:02:48,030 --> 00:02:51,480
no target is off-limits and so given

81
00:02:51,480 --> 00:02:53,549
that because we're constantly hearing

82
00:02:53,549 --> 00:02:56,400
about all these breaches and all the

83
00:02:56,400 --> 00:02:57,540
actors that are rising the proliferation

84
00:02:57,540 --> 00:02:59,310
of it because of all the headline

85
00:02:59,310 --> 00:03:01,080
fatigue we have in this area you know

86
00:03:01,080 --> 00:03:03,630
there's a sense of fatalism that has

87
00:03:03,630 --> 00:03:05,310
been permeating where our major

88
00:03:05,310 --> 00:03:06,870
paradigms right now in our industry are

89
00:03:06,870 --> 00:03:09,090
assuming breach privacy's dead humans

90
00:03:09,090 --> 00:03:10,620
are the weakest links and it basically

91
00:03:10,620 --> 00:03:13,230
you know leaves two leads to just a lack

92
00:03:13,230 --> 00:03:14,700
of faith and you know almost defeatism

93
00:03:14,700 --> 00:03:16,830
as we're trying to approach this really

94
00:03:16,830 --> 00:03:18,989
really important issue and so isn't to

95
00:03:18,989 --> 00:03:20,640
say that there aren't you know the data

96
00:03:20,640 --> 00:03:23,280
points don't provide some impetus for

97
00:03:23,280 --> 00:03:25,200
why this is why what some dominant

98
00:03:25,200 --> 00:03:26,910
paradigms are but that we need to move

99
00:03:26,910 --> 00:03:29,220
beyond it and actually aspire towards a

100
00:03:29,220 --> 00:03:30,810
different future than that and not just

101
00:03:30,810 --> 00:03:32,250
hit not just accept this as our current

102
00:03:32,250 --> 00:03:34,350
reality or the future reality need to

103
00:03:34,350 --> 00:03:37,500
push back on that but given our the

104
00:03:37,500 --> 00:03:39,510
reason why and why we're at this point

105
00:03:39,510 --> 00:03:42,810
what the repercussions of accepting that

106
00:03:42,810 --> 00:03:45,600
as as the status quo is that there's

107
00:03:45,600 --> 00:03:47,850
this data hoarding epidemic that's going

108
00:03:47,850 --> 00:03:51,030
on and so we're locking down data either

109
00:03:51,030 --> 00:03:53,700
for fear of it being stolen by the range

110
00:03:53,700 --> 00:03:55,470
of attackers that I talked about through

111
00:03:55,470 --> 00:03:57,209
compliance we can we see GDP are in a

112
00:03:57,209 --> 00:04:00,420
whole range of data protection policies

113
00:04:00,420 --> 00:04:02,340
that are really popping up I think

114
00:04:02,340 --> 00:04:04,500
Europe has been a big leader in that

115
00:04:04,500 --> 00:04:06,180
area we're seeing a brazil's following

116
00:04:06,180 --> 00:04:07,709
something similar the US has

117
00:04:07,709 --> 00:04:09,000
california's a lot coming up so it's

118
00:04:09,000 --> 00:04:11,250
both the attackers that are coming after

119
00:04:11,250 --> 00:04:13,410
the data and a compliance regimes that

120
00:04:13,410 --> 00:04:14,760
are leading organizations and

121
00:04:14,760 --> 00:04:16,500
communities to really just want to hoard

122
00:04:16,500 --> 00:04:17,010
the data

123
00:04:17,010 --> 00:04:19,649
keep it all themselves and not share it

124
00:04:19,649 --> 00:04:21,959
and the other aspect of the sharing is

125
00:04:21,959 --> 00:04:23,610
that because the third-party breaches

126
00:04:23,610 --> 00:04:25,229
that are going on that's another reason

127
00:04:25,229 --> 00:04:26,780
why people are

128
00:04:26,780 --> 00:04:28,190
concerned about sharing and whether it's

129
00:04:28,190 --> 00:04:30,020
intentional or not that those

130
00:04:30,020 --> 00:04:32,150
third-party data compromises that have

131
00:04:32,150 --> 00:04:35,000
really instilled a lack of trust amongst

132
00:04:35,000 --> 00:04:37,370
organizations and much businesses to

133
00:04:37,370 --> 00:04:39,700
share and collaborate with each other

134
00:04:39,700 --> 00:04:42,320
and so why does this matter

135
00:04:42,320 --> 00:04:44,090
as we are seeing these data styles

136
00:04:44,090 --> 00:04:46,280
through the hoarding that's going on for

137
00:04:46,280 --> 00:04:47,540
most organizations it hurts the bottom

138
00:04:47,540 --> 00:04:49,460
line and that this complain is simple

139
00:04:49,460 --> 00:04:51,020
its limits their ability to actually

140
00:04:51,020 --> 00:04:52,700
reap business value from a lot of data

141
00:04:52,700 --> 00:04:54,440
they have and we see that both in

142
00:04:54,440 --> 00:04:56,210
security with threat intelligence but

143
00:04:56,210 --> 00:04:57,410
also with other organizations across

144
00:04:57,410 --> 00:04:59,919
anything from health care to energy

145
00:04:59,919 --> 00:05:02,270
medical industry and so forth it's

146
00:05:02,270 --> 00:05:03,410
hurting the bottom line and it's hurting

147
00:05:03,410 --> 00:05:05,090
the ability to innovate

148
00:05:05,090 --> 00:05:06,890
it hurts security and we actually have

149
00:05:06,890 --> 00:05:09,110
had a really great panel to explain most

150
00:05:09,110 --> 00:05:10,100
of that so I don't need to go into it

151
00:05:10,100 --> 00:05:12,350
too much the benefits of collaborating

152
00:05:12,350 --> 00:05:14,750
and sharing are really beneficial for

153
00:05:14,750 --> 00:05:18,290
security protection for safety the

154
00:05:18,290 --> 00:05:19,910
absence of which which we see an awful

155
00:05:19,910 --> 00:05:21,860
lot of is actually really detrimental

156
00:05:21,860 --> 00:05:23,570
it's decreasing our visibility I'm gonna

157
00:05:23,570 --> 00:05:24,650
set actually a couple times you only see

158
00:05:24,650 --> 00:05:26,870
a part of the pie and so we're we had we

159
00:05:26,870 --> 00:05:28,760
lacked the situational awareness that

160
00:05:28,760 --> 00:05:31,430
goes on and then also fragments of

161
00:05:31,430 --> 00:05:33,080
community so instead of working together

162
00:05:33,080 --> 00:05:35,660
as a cohesive unit it leads to distrust

163
00:05:35,660 --> 00:05:37,669
in different islands of information

164
00:05:37,669 --> 00:05:40,100
going on within the community which

165
00:05:40,100 --> 00:05:41,750
basically means that is separate

166
00:05:41,750 --> 00:05:44,090
insights are being reaped across the

167
00:05:44,090 --> 00:05:46,100
area but there's no holistic information

168
00:05:46,100 --> 00:05:49,310
that we're looking help innovate and so

169
00:05:49,310 --> 00:05:50,360
those of us that are researches and

170
00:05:50,360 --> 00:05:52,580
analysts we don't have enough access to

171
00:05:52,580 --> 00:05:54,080
the right data that we need we know

172
00:05:54,080 --> 00:05:55,640
we're not getting a complete picture we

173
00:05:55,640 --> 00:05:56,720
were all we know we're only able to see

174
00:05:56,720 --> 00:05:59,180
the bits that we have access to and so

175
00:05:59,180 --> 00:06:00,950
that's why I think of as a data deserts

176
00:06:00,950 --> 00:06:02,720
despite the fact that you know we hear

177
00:06:02,720 --> 00:06:03,440
about the big data

178
00:06:03,440 --> 00:06:05,750
you know big data trends that are going

179
00:06:05,750 --> 00:06:07,100
on there's a day-to-day lose like

180
00:06:07,100 --> 00:06:10,280
whatever whatever kind of hair time you

181
00:06:10,280 --> 00:06:11,330
want you want to think about with that

182
00:06:11,330 --> 00:06:13,130
it's really lean to that scarcity and

183
00:06:13,130 --> 00:06:14,900
it's a data scarcity that's hurting us

184
00:06:14,900 --> 00:06:16,340
right now from innovating from

185
00:06:16,340 --> 00:06:17,300
collaborating and really with

186
00:06:17,300 --> 00:06:19,820
strengthening our defenses and the real

187
00:06:19,820 --> 00:06:20,810
world consequences and this was

188
00:06:20,810 --> 00:06:22,340
interesting I she's just two days ago in

189
00:06:22,340 --> 00:06:24,169
Politico they're talking about

190
00:06:24,169 --> 00:06:25,970
ransomware and how the unreported

191
00:06:25,970 --> 00:06:27,830
attacks are ransomware is actually

192
00:06:27,830 --> 00:06:30,020
leaving it the places where local law

193
00:06:30,020 --> 00:06:32,210
enforcement local governments

194
00:06:32,210 --> 00:06:33,410
cybersecurity firms

195
00:06:33,410 --> 00:06:34,880
basically you name it anyone who's

196
00:06:34,880 --> 00:06:35,990
concerned about ransomware which at this

197
00:06:35,990 --> 00:06:38,290
point was almost everyone

198
00:06:38,290 --> 00:06:39,370
they're not getting a full picture of

199
00:06:39,370 --> 00:06:41,200
the threat and so I think it's really

200
00:06:41,200 --> 00:06:43,210
interesting talking about that and we'll

201
00:06:43,210 --> 00:06:44,670
get into some of the reasons why that is

202
00:06:44,670 --> 00:06:46,450
when we get to some of the socio

203
00:06:46,450 --> 00:06:48,490
technical aspects of it but we're seeing

204
00:06:48,490 --> 00:06:50,950
real-world effects of organizations and

205
00:06:50,950 --> 00:06:52,090
governments not being able to defend

206
00:06:52,090 --> 00:06:52,990
themselves because they don't have a

207
00:06:52,990 --> 00:06:54,010
complete picture of actually what's

208
00:06:54,010 --> 00:06:55,780
going on and then ransomware is really a

209
00:06:55,780 --> 00:06:59,860
good example of that so just to kind of

210
00:06:59,860 --> 00:07:01,210
summarize you go to Steve estate you

211
00:07:01,210 --> 00:07:03,400
know there's a whole vast amount of you

212
00:07:03,400 --> 00:07:05,470
the marking material but not not a large

213
00:07:05,470 --> 00:07:07,510
amount of high-quality vetted data that

214
00:07:07,510 --> 00:07:08,890
we all need access to us researchers and

215
00:07:08,890 --> 00:07:11,050
analysts and so as researchers and

216
00:07:11,050 --> 00:07:12,340
analysts were always seeing a portion

217
00:07:12,340 --> 00:07:14,110
the pie and this is actually a really

218
00:07:14,110 --> 00:07:16,510
good Washington Post article by

219
00:07:16,510 --> 00:07:18,040
University of Tulsa professor Tyler

220
00:07:18,040 --> 00:07:19,750
Moore who wrote about cyber secure he's

221
00:07:19,750 --> 00:07:21,130
one of the biggest problems in cyber

222
00:07:21,130 --> 00:07:22,720
security is the lack of data sharing and

223
00:07:22,720 --> 00:07:23,800
a data scarcity problem that we're

224
00:07:23,800 --> 00:07:27,730
seeing right now so this gets to the

225
00:07:27,730 --> 00:07:29,020
data paradox we'll get to another

226
00:07:29,020 --> 00:07:30,610
paradox in a little bit what we're

227
00:07:30,610 --> 00:07:32,290
seeing on the one hand so much data is

228
00:07:32,290 --> 00:07:35,110
out there across though all of these

229
00:07:35,110 --> 00:07:37,330
that you want but researchers and

230
00:07:37,330 --> 00:07:38,800
analysts are not accessing the right

231
00:07:38,800 --> 00:07:40,960
data at the right time that they need to

232
00:07:40,960 --> 00:07:42,610
really provide strain and strengthen our

233
00:07:42,610 --> 00:07:46,120
defenses and so how can we fix this how

234
00:07:46,120 --> 00:07:49,150
can we create a system where sharing the

235
00:07:49,150 --> 00:07:51,040
data and we're innovating and we're

236
00:07:51,040 --> 00:07:52,570
delivering value to our customers and

237
00:07:52,570 --> 00:07:54,540
strengthen defenses for our customers

238
00:07:54,540 --> 00:07:58,000
and so again steals from Martin I think

239
00:07:58,000 --> 00:07:59,470
he just left if you allowed me to put

240
00:07:59,470 --> 00:08:02,230
this on there so we don't want my signs

241
00:08:02,230 --> 00:08:03,400
earlier he's talked about the users as a

242
00:08:03,400 --> 00:08:05,650
weakest link I don't think there was a

243
00:08:05,650 --> 00:08:07,750
funny pushback on that that well you

244
00:08:07,750 --> 00:08:08,890
know it's just we're not allowed to say

245
00:08:08,890 --> 00:08:10,660
that anymore it's not ever so loud to

246
00:08:10,660 --> 00:08:12,010
say anymore it's that we have to

247
00:08:12,010 --> 00:08:13,750
actually treat them as features not bugs

248
00:08:13,750 --> 00:08:15,190
in our system it's we actually need to

249
00:08:15,190 --> 00:08:16,270
integrate them and to as we're building

250
00:08:16,270 --> 00:08:18,340
solutions understanding humans are part

251
00:08:18,340 --> 00:08:19,960
of it not a bug and we can't take them

252
00:08:19,960 --> 00:08:22,210
out of it and so this actually led me to

253
00:08:22,210 --> 00:08:23,620
change the title of the talk because I

254
00:08:23,620 --> 00:08:25,090
thought that was just brilliant that he

255
00:08:25,090 --> 00:08:27,730
that he did that so I do I can to make

256
00:08:27,730 --> 00:08:28,750
sure it's of it from the next virus

257
00:08:28,750 --> 00:08:32,140
bulletins will Satish so how do we go

258
00:08:32,140 --> 00:08:32,830
about doing that

259
00:08:32,830 --> 00:08:34,390
how we build these systems and processes

260
00:08:34,390 --> 00:08:36,309
for search to secure sharing and so I'll

261
00:08:36,309 --> 00:08:38,679
walk through each of these incentives

262
00:08:38,679 --> 00:08:40,479
trust in usability and each of these

263
00:08:40,479 --> 00:08:43,419
kind of cross over a different balance

264
00:08:43,419 --> 00:08:45,310
in each one from a social systems to the

265
00:08:45,310 --> 00:08:46,360
technical systems and really the

266
00:08:46,360 --> 00:08:48,550
integration across the two so we'll

267
00:08:48,550 --> 00:08:50,730
start off with incentives

268
00:08:50,730 --> 00:08:52,709
and so the carrots and the sticks that's

269
00:08:52,709 --> 00:08:54,300
you do like intern intro to

270
00:08:54,300 --> 00:08:55,829
international relations you learn that

271
00:08:55,829 --> 00:08:57,660
senta's matter you can either a reward

272
00:08:57,660 --> 00:08:59,550
or you can punish that's basically a

273
00:08:59,550 --> 00:09:00,690
foundation for incentives and they

274
00:09:00,690 --> 00:09:02,519
matter as far as influencing and

275
00:09:02,519 --> 00:09:04,980
altering behavior but for way too long

276
00:09:04,980 --> 00:09:06,690
when we think that thought about data

277
00:09:06,690 --> 00:09:08,459
sharing within our industry it's been

278
00:09:08,459 --> 00:09:11,010
one way and such a the example I gave

279
00:09:11,010 --> 00:09:12,810
earlier about that's behind the

280
00:09:12,810 --> 00:09:13,860
government not necessarily getting all

281
00:09:13,860 --> 00:09:16,410
the ransomware data that they need part

282
00:09:16,410 --> 00:09:17,639
of it is that lack of trust a part of

283
00:09:17,639 --> 00:09:18,959
it's the belief that you're gonna give

284
00:09:18,959 --> 00:09:20,730
the data one way but never get any

285
00:09:20,730 --> 00:09:21,990
benefits out of it coming the other way

286
00:09:21,990 --> 00:09:23,820
and I've heard that over and over again

287
00:09:23,820 --> 00:09:26,160
from organizations who don't want to

288
00:09:26,160 --> 00:09:27,389
work with the government because they

289
00:09:27,389 --> 00:09:29,040
they don't see anything coming back in

290
00:09:29,040 --> 00:09:30,120
their favor they see the government

291
00:09:30,120 --> 00:09:31,649
supporting the data not providing that

292
00:09:31,649 --> 00:09:34,920
benefit in return and you knows

293
00:09:34,920 --> 00:09:36,600
inserting the panel beforehand really

294
00:09:36,600 --> 00:09:38,130
emphasized the research process of it

295
00:09:38,130 --> 00:09:40,529
which is great except gets into some of

296
00:09:40,529 --> 00:09:42,500
the stuff that we'll be talking about

297
00:09:42,500 --> 00:09:45,000
and so if you look at the research on

298
00:09:45,000 --> 00:09:46,709
actually on incentives and building

299
00:09:46,709 --> 00:09:49,470
trust in really building collaborative

300
00:09:49,470 --> 00:09:51,449
social networks you have to look at the

301
00:09:51,449 --> 00:09:52,620
density of the nother networks of

302
00:09:52,620 --> 00:09:53,970
structure size and directionality all

303
00:09:53,970 --> 00:09:55,319
these different aspects of the networks

304
00:09:55,319 --> 00:09:57,480
really impact the incentives of the

305
00:09:57,480 --> 00:09:58,649
members in the quality of information

306
00:09:58,649 --> 00:10:00,810
available to them and that last part is

307
00:10:00,810 --> 00:10:02,190
what I really want to focus on that

308
00:10:02,190 --> 00:10:04,139
based on the network and based on how

309
00:10:04,139 --> 00:10:05,550
the flitter the information flows are

310
00:10:05,550 --> 00:10:07,949
going impact significantly the amount of

311
00:10:07,949 --> 00:10:09,300
data that's available within those

312
00:10:09,300 --> 00:10:11,910
networks themselves and this is a really

313
00:10:11,910 --> 00:10:13,350
great book if anyone wants to read more

314
00:10:13,350 --> 00:10:14,940
about network theory and how it's

315
00:10:14,940 --> 00:10:17,699
impacting relations and security on the

316
00:10:17,699 --> 00:10:18,720
chessboard in the web's are really

317
00:10:18,720 --> 00:10:21,779
interesting book that hits upon that so

318
00:10:21,779 --> 00:10:22,769
we need to think about it for more of a

319
00:10:22,769 --> 00:10:23,790
network's perspective when we're

320
00:10:23,790 --> 00:10:25,769
thinking about data sharing within our

321
00:10:25,769 --> 00:10:27,810
industry and it must be so that rises

322
00:10:27,810 --> 00:10:29,819
all boats rise up it can't just be

323
00:10:29,819 --> 00:10:31,500
that's beneficial to one not beneficial

324
00:10:31,500 --> 00:10:33,630
to another that again going back to the

325
00:10:33,630 --> 00:10:35,130
incentives they have to be there to

326
00:10:35,130 --> 00:10:37,410
demonstrate there's reciprocity for it

327
00:10:37,410 --> 00:10:39,180
and that the mutual benefits are there

328
00:10:39,180 --> 00:10:42,029
for the secure sharing absent that kind

329
00:10:42,029 --> 00:10:43,079
of network and apps in that kind of

330
00:10:43,079 --> 00:10:46,529
directionality within it the individuals

331
00:10:46,529 --> 00:10:47,760
in organizations simply aren't going to

332
00:10:47,760 --> 00:10:52,649
be sharing with one another and so we

333
00:10:52,649 --> 00:10:54,060
want to focus on creating a network

334
00:10:54,060 --> 00:10:55,889
system where we've got all the fun that

335
00:10:55,889 --> 00:10:57,209
we talked about earlier we're very well

336
00:10:57,209 --> 00:10:58,410
known for that if you're uncertain in

337
00:10:58,410 --> 00:11:00,420
doubt but making it so the incentives

338
00:11:00,420 --> 00:11:02,100
are there so the fear of missing out

339
00:11:02,100 --> 00:11:04,000
actually overwhelms the flood

340
00:11:04,000 --> 00:11:05,830
is dominating the industry and I think

341
00:11:05,830 --> 00:11:07,210
we can get there and talk about there's

342
00:11:07,210 --> 00:11:08,560
some signs that we're getting there now

343
00:11:08,560 --> 00:11:10,900
but be great to get to that point where

344
00:11:10,900 --> 00:11:12,880
you have different groups are wanting to

345
00:11:12,880 --> 00:11:15,250
participate now and again you know that

346
00:11:15,250 --> 00:11:17,350
panel before talking about how RFPs now

347
00:11:17,350 --> 00:11:19,000
or tell are starting to talk about are

348
00:11:19,000 --> 00:11:20,980
you collaborating with other groups you

349
00:11:20,980 --> 00:11:22,210
know that that's those are the kind of

350
00:11:22,210 --> 00:11:23,740
incentive structures that need to be

351
00:11:23,740 --> 00:11:25,810
built to encourage various kinds of

352
00:11:25,810 --> 00:11:29,250
collaboration across our industry

353
00:11:30,450 --> 00:11:32,770
unfortunately and incentives alone

354
00:11:32,770 --> 00:11:33,970
aren't going to be the only thing that

355
00:11:33,970 --> 00:11:36,670
we need we also need trust so it's much

356
00:11:36,670 --> 00:11:38,170
of incentives that you have that are

357
00:11:38,170 --> 00:11:41,110
there and as great as they are it's a

358
00:11:41,110 --> 00:11:42,610
necessary condition but not a sufficient

359
00:11:42,610 --> 00:11:44,590
condition so we we need to have other

360
00:11:44,590 --> 00:11:47,950
aspects to it so Trust is really a core

361
00:11:47,950 --> 00:11:51,220
feature of this so we see what we see

362
00:11:51,220 --> 00:11:52,990
more so than not you can look at the

363
00:11:52,990 --> 00:11:55,690
status quo is one organization there's a

364
00:11:55,690 --> 00:11:56,980
huge amount mistrust of what an

365
00:11:56,980 --> 00:11:58,570
organization may do if you share it with

366
00:11:58,570 --> 00:11:59,980
another organization or another

367
00:11:59,980 --> 00:12:02,260
individual and again we see that in the

368
00:12:02,260 --> 00:12:04,180
area of you sharing with the government

369
00:12:04,180 --> 00:12:06,610
I'm not so that the argument that I hear

370
00:12:06,610 --> 00:12:08,230
so off is well look the government you

371
00:12:08,230 --> 00:12:09,640
can't even set this in the United States

372
00:12:09,640 --> 00:12:11,650
I'm not actually seen you know across

373
00:12:11,650 --> 00:12:12,220
the globe

374
00:12:12,220 --> 00:12:13,660
you can't even defend their own date I

375
00:12:13,660 --> 00:12:15,730
look at OPM look at the Joint Staff

376
00:12:15,730 --> 00:12:17,170
getting hacked look at state why and so

377
00:12:17,170 --> 00:12:18,640
they kind of can list off a bunch of

378
00:12:18,640 --> 00:12:21,190
these other breaches and compromises

379
00:12:21,190 --> 00:12:22,390
that have gone on and so why should we

380
00:12:22,390 --> 00:12:24,190
share with assist with with a government

381
00:12:24,190 --> 00:12:25,030
or why should we share with another

382
00:12:25,030 --> 00:12:26,710
organization if we're not sure that

383
00:12:26,710 --> 00:12:27,700
they're gonna be able to cure the data

384
00:12:27,700 --> 00:12:28,960
or who they're gonna give it to you and

385
00:12:28,960 --> 00:12:30,280
it's basically throwing it into a black

386
00:12:30,280 --> 00:12:32,650
box and that's not just a new private

387
00:12:32,650 --> 00:12:34,750
sector public sector it goes to karate

388
00:12:34,750 --> 00:12:36,190
within the government when I was there a

389
00:12:36,190 --> 00:12:38,880
huge issue for sharing and dictate and

390
00:12:38,880 --> 00:12:40,420
collaboration sharing collaboration

391
00:12:40,420 --> 00:12:42,580
going across organizations and so and

392
00:12:42,580 --> 00:12:44,410
this is just a human factor that has to

393
00:12:44,410 --> 00:12:46,839
be integrated along with it because if

394
00:12:46,839 --> 00:12:48,670
you have that mistrust there really are

395
00:12:48,670 --> 00:12:52,480
no incentives for the sharing and so

396
00:12:52,480 --> 00:12:53,440
when we're thinking about building any

397
00:12:53,440 --> 00:12:55,330
kind of an Associated economic system we

398
00:12:55,330 --> 00:12:56,800
got to look at the look at the trust and

399
00:12:56,800 --> 00:12:58,810
look at how that's foundational and what

400
00:12:58,810 --> 00:13:01,060
aspects can we do to build trust within

401
00:13:01,060 --> 00:13:03,670
within the networked environment that

402
00:13:03,670 --> 00:13:06,130
we're working in and so again go back to

403
00:13:06,130 --> 00:13:08,050
the network defects and so instead of

404
00:13:08,050 --> 00:13:09,910
looking at as a bilateral relationship

405
00:13:09,910 --> 00:13:12,520
with me a unidirectional trust requires

406
00:13:12,520 --> 00:13:16,930
human interaction and so we're generally

407
00:13:16,930 --> 00:13:17,860
an industry

408
00:13:17,860 --> 00:13:20,200
over the austere EO type we have a lot

409
00:13:20,200 --> 00:13:21,880
of introverts in it I'm one the

410
00:13:21,880 --> 00:13:24,040
introverted you know human interaction

411
00:13:24,040 --> 00:13:26,560
can be hard it can be exhausting but

412
00:13:26,560 --> 00:13:27,790
it's something that we need to be doing

413
00:13:27,790 --> 00:13:30,340
and so the reason why it's so important

414
00:13:30,340 --> 00:13:31,510
is because it doesn't build that

415
00:13:31,510 --> 00:13:32,890
reservoir of social capital and so

416
00:13:32,890 --> 00:13:35,740
social capital and in that a social

417
00:13:35,740 --> 00:13:38,380
science term but basically is looks at

418
00:13:38,380 --> 00:13:39,790
the interactions and relationships

419
00:13:39,790 --> 00:13:42,130
across a group of people and what that

420
00:13:42,130 --> 00:13:44,070
looks like and it's been proven that the

421
00:13:44,070 --> 00:13:46,480
more social capital you have has been

422
00:13:46,480 --> 00:13:48,550
linked to everything from greater loves

423
00:13:48,550 --> 00:13:50,320
a democracy greater innovation greater

424
00:13:50,320 --> 00:13:53,020
economic development all the benefits

425
00:13:53,020 --> 00:13:54,190
that we want to be seeing coming out of

426
00:13:54,190 --> 00:13:56,550
it and so the human interaction is

427
00:13:56,550 --> 00:13:58,660
essential and because it then will

428
00:13:58,660 --> 00:13:59,560
support the propensity to

429
00:13:59,560 --> 00:14:01,570
self-organizing the groups self-organize

430
00:14:01,570 --> 00:14:03,670
into associations and that's what we're

431
00:14:03,670 --> 00:14:06,340
starting to see and just looking at just

432
00:14:06,340 --> 00:14:07,870
like a one in doubt from what I've seen

433
00:14:07,870 --> 00:14:10,150
here for the human interaction component

434
00:14:10,150 --> 00:14:11,680
well my former colleagues

435
00:14:11,680 --> 00:14:13,360
Bobby Fowler gave a talk yesterday on

436
00:14:13,360 --> 00:14:14,830
machine learning and looking at

437
00:14:14,830 --> 00:14:16,810
identifying processed child anomalies

438
00:14:16,810 --> 00:14:19,480
and one of his slides at the very end

439
00:14:19,480 --> 00:14:21,310
was talking about the lack of data that

440
00:14:21,310 --> 00:14:22,600
he had that he was able to do this

441
00:14:22,600 --> 00:14:24,400
analysis who was pretty comfortable with

442
00:14:24,400 --> 00:14:25,600
it but still the end was a little too

443
00:14:25,600 --> 00:14:27,160
small so we'd love to have some more

444
00:14:27,160 --> 00:14:30,130
data by dinner yesterday night he had

445
00:14:30,130 --> 00:14:32,050
two different people come up to him each

446
00:14:32,050 --> 00:14:33,430
of which offering about terabytes of

447
00:14:33,430 --> 00:14:34,810
data for him to help improve his

448
00:14:34,810 --> 00:14:37,360
modeling and so the human interaction is

449
00:14:37,360 --> 00:14:39,040
important building up that trust though

450
00:14:39,040 --> 00:14:40,630
and understanding what was going on that

451
00:14:40,630 --> 00:14:42,070
helps by coming these conferences and

452
00:14:42,070 --> 00:14:43,390
seeing the research that's that's going

453
00:14:43,390 --> 00:14:47,050
there and I would argue that those two

454
00:14:47,050 --> 00:14:48,580
working at that level are the three of

455
00:14:48,580 --> 00:14:51,790
them I guess and the data exchange will

456
00:14:51,790 --> 00:14:54,100
probably get to the get to the desired

457
00:14:54,100 --> 00:14:56,020
end point of you know improving that

458
00:14:56,020 --> 00:14:57,400
machine learning model for greater

459
00:14:57,400 --> 00:15:00,790
detection much faster than had it been

460
00:15:00,790 --> 00:15:03,190
like a big corporate you know formal

461
00:15:03,190 --> 00:15:05,260
collaborative agreement and so I'm

462
00:15:05,260 --> 00:15:06,760
talking about data sharing it has to

463
00:15:06,760 --> 00:15:08,440
occur to all these different levels we

464
00:15:08,440 --> 00:15:09,820
need to have the formal organizations

465
00:15:09,820 --> 00:15:10,810
that are doing it but also that human

466
00:15:10,810 --> 00:15:12,220
element of the what person-to-person

467
00:15:12,220 --> 00:15:14,200
interactions they're going on which is

468
00:15:14,200 --> 00:15:15,430
why these kind of conferences are so

469
00:15:15,430 --> 00:15:16,630
important for those of us in this in

470
00:15:16,630 --> 00:15:18,070
this industry to really help move the

471
00:15:18,070 --> 00:15:19,690
needle and help strengthen our defenses

472
00:15:19,690 --> 00:15:20,980
but I thought that was just a great

473
00:15:20,980 --> 00:15:24,220
example of how one through the data

474
00:15:24,220 --> 00:15:25,660
silos that are going on

475
00:15:25,660 --> 00:15:26,920
but by coming to these kind of events

476
00:15:26,920 --> 00:15:29,110
and interacting with people we can

477
00:15:29,110 --> 00:15:30,130
actually start to remove some of those

478
00:15:30,130 --> 00:15:30,550
silos

479
00:15:30,550 --> 00:15:35,140
and collaborate so while trust on the

480
00:15:35,140 --> 00:15:37,690
human side is increasingly important and

481
00:15:37,690 --> 00:15:39,160
just really essential for collaboration

482
00:15:39,160 --> 00:15:40,750
as an industry we're moving toward zero

483
00:15:40,750 --> 00:15:43,870
trust and I like I don't think that's a

484
00:15:43,870 --> 00:15:44,950
bad thing I think it's really helping us

485
00:15:44,950 --> 00:15:46,390
move forward and helping strengthen our

486
00:15:46,390 --> 00:15:49,570
defense our defenses but it is if it's a

487
00:15:49,570 --> 00:15:51,400
notion of not trusting anyone we're

488
00:15:51,400 --> 00:15:53,920
denying by default on that can refer to

489
00:15:53,920 --> 00:15:55,930
users an identity don't trust who you're

490
00:15:55,930 --> 00:15:56,980
sending things to you devices

491
00:15:56,980 --> 00:15:59,200
applications data comes across the board

492
00:15:59,200 --> 00:16:01,600
and again I got other whole talks on

493
00:16:01,600 --> 00:16:03,580
this and we're overwhelmed with a lot of

494
00:16:03,580 --> 00:16:05,920
the zero trust literature I think it's

495
00:16:05,920 --> 00:16:07,690
important to highlight that one and this

496
00:16:07,690 --> 00:16:08,950
is really great for us as far as

497
00:16:08,950 --> 00:16:11,320
maintaining the security of the data but

498
00:16:11,320 --> 00:16:13,060
provides another paradox that we're in

499
00:16:13,060 --> 00:16:14,740
we're on the technical side we're

500
00:16:14,740 --> 00:16:16,600
basically building systems on based on

501
00:16:16,600 --> 00:16:19,150
the absence of trust when social systems

502
00:16:19,150 --> 00:16:20,770
we need to have much greater trust

503
00:16:20,770 --> 00:16:22,060
that's going on and we have an

504
00:16:22,060 --> 00:16:23,380
increasing reliance on this kind of

505
00:16:23,380 --> 00:16:25,780
trust and so again what can we do about

506
00:16:25,780 --> 00:16:26,950
that how can we move forward with that

507
00:16:26,950 --> 00:16:30,340
and I would argue that one key element

508
00:16:30,340 --> 00:16:33,090
could be focusing on the usability and

509
00:16:33,090 --> 00:16:36,150
may not seem super intuitive for that

510
00:16:36,150 --> 00:16:38,590
and one highlight it's not just riding

511
00:16:38,590 --> 00:16:40,300
the usability wave usability is being

512
00:16:40,300 --> 00:16:41,980
thrown around you know everywhere in

513
00:16:41,980 --> 00:16:44,170
security now which the only and I can as

514
00:16:44,170 --> 00:16:46,030
someone who's done what decent of work

515
00:16:46,030 --> 00:16:47,320
on human-computer interaction I'm

516
00:16:47,320 --> 00:16:49,510
thrilled to see security embracing a lot

517
00:16:49,510 --> 00:16:52,630
of this it's we're fairly far behind

518
00:16:52,630 --> 00:16:54,250
some other tech industries and then the

519
00:16:54,250 --> 00:16:57,820
focus on usability but um you can

520
00:16:57,820 --> 00:17:00,190
actually point out to the aspects of

521
00:17:00,190 --> 00:17:02,020
theme listen it and simplification

522
00:17:02,020 --> 00:17:03,670
empowerment workflow all those kind of

523
00:17:03,670 --> 00:17:05,440
things are really really important so

524
00:17:05,440 --> 00:17:07,180
while they're one hand they're marketing

525
00:17:07,180 --> 00:17:09,069
its buzzword bingo you know we've seen

526
00:17:09,069 --> 00:17:11,619
that everywhere it really isn't it's an

527
00:17:11,619 --> 00:17:12,910
essential component and can really help

528
00:17:12,910 --> 00:17:15,700
get us to a point where we can craft

529
00:17:15,700 --> 00:17:17,589
those incentives and have greater trust

530
00:17:17,589 --> 00:17:19,390
in the system and so how can that

531
00:17:19,390 --> 00:17:21,849
actually even happen so one more

532
00:17:21,849 --> 00:17:24,250
buzzword for of us demystifying

533
00:17:24,250 --> 00:17:26,859
usability for secure sharing so there's

534
00:17:26,859 --> 00:17:28,089
a couple elements of how I think

535
00:17:28,089 --> 00:17:29,860
usability could play a really strong

536
00:17:29,860 --> 00:17:32,290
role in helping secure sharing and there

537
00:17:32,290 --> 00:17:33,610
are technical elements to it and then

538
00:17:33,610 --> 00:17:35,620
there are also the social elements of it

539
00:17:35,620 --> 00:17:38,350
and so one is the customizable data

540
00:17:38,350 --> 00:17:40,120
protection and this is where you know

541
00:17:40,120 --> 00:17:41,320
what these technologies are already

542
00:17:41,320 --> 00:17:43,840
there and sort of the good analogy that

543
00:17:43,840 --> 00:17:44,420
I

544
00:17:44,420 --> 00:17:47,930
I always think of a fair amount a month

545
00:17:47,930 --> 00:17:49,780
or two a guy heard a woman talking about

546
00:17:49,780 --> 00:17:52,220
creating the green buildings that the

547
00:17:52,220 --> 00:17:53,810
all the technology is there for us to

548
00:17:53,810 --> 00:17:56,060
have really green buildings now but why

549
00:17:56,060 --> 00:17:57,590
aren't we doing it well part of it is

550
00:17:57,590 --> 00:17:58,760
the systems and processes are in place

551
00:17:58,760 --> 00:18:00,170
incentives aren't there the Trust isn't

552
00:18:00,170 --> 00:18:01,850
there there's so many reasons that are

553
00:18:01,850 --> 00:18:04,070
very similar to what I see here and so I

554
00:18:04,070 --> 00:18:05,690
think for secure sharing a lot of

555
00:18:05,690 --> 00:18:07,340
technology is there which doesn't mean

556
00:18:07,340 --> 00:18:09,740
we shouldn't still be innovating but on

557
00:18:09,740 --> 00:18:11,840
the backend we can figure out as far as

558
00:18:11,840 --> 00:18:13,160
the various controls who can have access

559
00:18:13,160 --> 00:18:15,590
keeping track of the data can track of

560
00:18:15,590 --> 00:18:17,900
the devices and so forth where we can

561
00:18:17,900 --> 00:18:20,030
really innovate a bunch more it's

562
00:18:20,030 --> 00:18:21,380
providing some those visual cues as well

563
00:18:21,380 --> 00:18:23,210
and those visual cues are really

564
00:18:23,210 --> 00:18:25,340
important one so it can make it much

565
00:18:25,340 --> 00:18:27,200
more accessible for a broader range of

566
00:18:27,200 --> 00:18:28,370
folks who want to be sharing data

567
00:18:28,370 --> 00:18:29,900
whether it's threatened tell threatened

568
00:18:29,900 --> 00:18:31,730
tell or any other kind of data so that's

569
00:18:31,730 --> 00:18:33,410
really important so the tech element can

570
00:18:33,410 --> 00:18:34,580
help provide that transparency

571
00:18:34,580 --> 00:18:36,320
especially on the front and in how it's

572
00:18:36,320 --> 00:18:38,930
designed but it's also gonna require

573
00:18:38,930 --> 00:18:41,600
that human element so for zero trust it

574
00:18:41,600 --> 00:18:42,800
only works as well is that the person

575
00:18:42,800 --> 00:18:44,900
you fry detonated management all those

576
00:18:44,900 --> 00:18:46,940
kind of aspects if if you're actually

577
00:18:46,940 --> 00:18:48,800
adhering to it so if you think about

578
00:18:48,800 --> 00:18:50,360
examples where the human element really

579
00:18:50,360 --> 00:18:52,040
is important is say if someone's working

580
00:18:52,040 --> 00:18:53,360
in your organization had access to a lot

581
00:18:53,360 --> 00:18:54,800
of data then they're no longer working

582
00:18:54,800 --> 00:18:56,450
with you are you actually elaborating

583
00:18:56,450 --> 00:18:57,860
the technology to revoke their access

584
00:18:57,860 --> 00:18:59,660
privileges very often we see that

585
00:18:59,660 --> 00:19:01,640
doesn't happen and they maintain access

586
00:19:01,640 --> 00:19:03,050
to that data even after they leave an

587
00:19:03,050 --> 00:19:04,790
organization and so again that's the

588
00:19:04,790 --> 00:19:06,560
techno light a cue so far I need to have

589
00:19:06,560 --> 00:19:08,380
the human element following up on it to

590
00:19:08,380 --> 00:19:10,550
ensure those the controls are still

591
00:19:10,550 --> 00:19:12,740
being actually used and are adhere to as

592
00:19:12,740 --> 00:19:16,100
the social system evolves when it comes

593
00:19:16,100 --> 00:19:18,050
to sharing communities we're seeing more

594
00:19:18,050 --> 00:19:19,280
and more unsecured enclaves I think

595
00:19:19,280 --> 00:19:20,480
that's interesting area to be looking at

596
00:19:20,480 --> 00:19:21,770
I mean it and just expanding the

597
00:19:21,770 --> 00:19:23,240
usability and the customization of the

598
00:19:23,240 --> 00:19:24,680
controls I think are really important

599
00:19:24,680 --> 00:19:26,570
and then the human element creating the

600
00:19:26,570 --> 00:19:28,610
organizations that are out there CTA is

601
00:19:28,610 --> 00:19:30,710
you know great one of several they're

602
00:19:30,710 --> 00:19:33,430
out there for encouraging the sharing

603
00:19:33,430 --> 00:19:35,390
integrating those together and really

604
00:19:35,390 --> 00:19:37,160
helping off and not optimize the

605
00:19:37,160 --> 00:19:38,150
accessibility of those sharing

606
00:19:38,150 --> 00:19:40,730
communities but as we're going through

607
00:19:40,730 --> 00:19:41,930
it all and I'm looking at these ability

608
00:19:41,930 --> 00:19:43,550
we're really focusing on what that

609
00:19:43,550 --> 00:19:44,960
impact is on security and privacy and I

610
00:19:44,960 --> 00:19:47,450
think that's really important too often

611
00:19:47,450 --> 00:19:48,710
we're here that you're securing privacy

612
00:19:48,710 --> 00:19:49,730
there must be some sort of trade-off of

613
00:19:49,730 --> 00:19:51,980
innovation and convenience and then like

614
00:19:51,980 --> 00:19:53,630
let's not accept that it doesn't have to

615
00:19:53,630 --> 00:19:55,250
be the way occasionally it may provide

616
00:19:55,250 --> 00:19:56,710
some friction but let's move to an area

617
00:19:56,710 --> 00:19:58,630
it's as frictionless as we as we can get

618
00:19:58,630 --> 00:20:00,970
and I think in that that should be one

619
00:20:00,970 --> 00:20:03,580
of our aspirations and again just going

620
00:20:03,580 --> 00:20:04,840
back to the notion of the trust in the

621
00:20:04,840 --> 00:20:06,190
sense it's really fostering trusted

622
00:20:06,190 --> 00:20:07,720
that's transparency being able to see

623
00:20:07,720 --> 00:20:09,340
the visual cues that you're securing the

624
00:20:09,340 --> 00:20:10,840
data or you're not securing the data who

625
00:20:10,840 --> 00:20:12,070
has access to it through some various a

626
00:20:12,070 --> 00:20:13,870
lot of logs and so forth there's a lot

627
00:20:13,870 --> 00:20:14,799
of interesting work that I think can be

628
00:20:14,799 --> 00:20:16,240
done here and that already is being done

629
00:20:16,240 --> 00:20:17,529
here which i think is what is the

630
00:20:17,529 --> 00:20:20,679
exciting thing about it but if you're

631
00:20:20,679 --> 00:20:21,700
out there in with your own community

632
00:20:21,700 --> 00:20:23,230
just one thing to keep in mind this is

633
00:20:23,230 --> 00:20:25,210
what we see a lot that if it's not

634
00:20:25,210 --> 00:20:26,919
usable no matter like the best laid

635
00:20:26,919 --> 00:20:28,809
plans are going to get circumvented if

636
00:20:28,809 --> 00:20:30,549
it doesn't fit within the workflow so

637
00:20:30,549 --> 00:20:31,750
make sure it works within the workflow

638
00:20:31,750 --> 00:20:33,190
those of us who are analysts we know

639
00:20:33,190 --> 00:20:34,860
that we've found ways to work around

640
00:20:34,860 --> 00:20:40,090
whatever kind of security defenses might

641
00:20:40,090 --> 00:20:42,580
might be in place for us and I've seen

642
00:20:42,580 --> 00:20:43,990
my team members have done that as well

643
00:20:43,990 --> 00:20:45,279
so making sure it integrates the

644
00:20:45,279 --> 00:20:47,020
workflow and helps them as opposed to

645
00:20:47,020 --> 00:20:49,000
being supposed to being seen as

646
00:20:49,000 --> 00:20:51,669
something needs to be circumvented and

647
00:20:51,669 --> 00:20:53,380
so usability and not just a buzzword

648
00:20:53,380 --> 00:20:55,360
although it's still it's very much one

649
00:20:55,360 --> 00:20:56,770
but it's something that can be a force

650
00:20:56,770 --> 00:20:59,200
multiplier it can build trust it can

651
00:20:59,200 --> 00:21:00,370
help create the the various incentives

652
00:21:00,370 --> 00:21:02,169
through creating the communities and it

653
00:21:02,169 --> 00:21:04,480
really be a core component of of

654
00:21:04,480 --> 00:21:08,799
creating a secure sharing system and so

655
00:21:08,799 --> 00:21:10,210
that would do you know a fair amount you

656
00:21:10,210 --> 00:21:11,860
looking at the status quo of where we

657
00:21:11,860 --> 00:21:15,789
are of some ways to really get beyond

658
00:21:15,789 --> 00:21:18,730
that and aspire towards a better both

659
00:21:18,730 --> 00:21:19,990
the social and technical system where

660
00:21:19,990 --> 00:21:21,880
we're doing secure sharing and what the

661
00:21:21,880 --> 00:21:23,590
benefits are and I think we've seen that

662
00:21:23,590 --> 00:21:26,230
again across the across the two days

663
00:21:26,230 --> 00:21:27,700
from the yesterday's keynote talking

664
00:21:27,700 --> 00:21:29,200
about playing together that's been a

665
00:21:29,200 --> 00:21:30,909
theme that I've seen throughout several

666
00:21:30,909 --> 00:21:32,320
the talks is really coming together as a

667
00:21:32,320 --> 00:21:34,990
community which is great because we need

668
00:21:34,990 --> 00:21:36,159
to come together as a community right

669
00:21:36,159 --> 00:21:38,890
now it's a really important time again

670
00:21:38,890 --> 00:21:39,549
just like putting my international

671
00:21:39,549 --> 00:21:42,490
relations hat back on there is that the

672
00:21:42,490 --> 00:21:43,960
broader spread digital authoritarianism

673
00:21:43,960 --> 00:21:45,370
that's actually where most of my

674
00:21:45,370 --> 00:21:47,140
research tends to focus these days and

675
00:21:47,140 --> 00:21:49,120
really looking at the everything from

676
00:21:49,120 --> 00:21:50,380
the privacy implications of surveillance

677
00:21:50,380 --> 00:21:53,440
state states and facial recognition the

678
00:21:53,440 --> 00:21:54,820
very state of regular regulations that

679
00:21:54,820 --> 00:21:56,529
are going on that require data

680
00:21:56,529 --> 00:21:58,240
localization and access to that data

681
00:21:58,240 --> 00:22:01,000
those kind of laws are popping up and so

682
00:22:01,000 --> 00:22:03,250
us as a community are we have both a

683
00:22:03,250 --> 00:22:05,320
front row seat and what's going on but

684
00:22:05,320 --> 00:22:06,610
we also should be major players and

685
00:22:06,610 --> 00:22:08,740
ensuring what the future of the internet

686
00:22:08,740 --> 00:22:10,179
is going to look like

687
00:22:10,179 --> 00:22:12,129
both within our own countries our own

688
00:22:12,129 --> 00:22:15,339
domains but also across the globe and so

689
00:22:15,339 --> 00:22:17,019
on the one hand we can you kind of sit

690
00:22:17,019 --> 00:22:19,979
back and look at the dystopian realities

691
00:22:19,979 --> 00:22:23,709
had a great talk yesterday on that the

692
00:22:23,709 --> 00:22:25,359
how the fiction is impacting our

693
00:22:25,359 --> 00:22:27,489
perspective of how we're gonna be going

694
00:22:27,489 --> 00:22:30,129
as a community and we can we can sit

695
00:22:30,129 --> 00:22:31,119
back in and take that and just

696
00:22:31,119 --> 00:22:33,249
acknowledge the FUD and just assume that

697
00:22:33,249 --> 00:22:34,389
the breaches are always gonna happen

698
00:22:34,389 --> 00:22:35,619
there's nothing we can do about it

699
00:22:35,619 --> 00:22:37,719
or could at least aspire towards a

700
00:22:37,719 --> 00:22:39,389
better future leveraging the technology

701
00:22:39,389 --> 00:22:42,339
large insurance eat to help ensure

702
00:22:42,339 --> 00:22:44,859
security and help ensure us a lot of the

703
00:22:44,859 --> 00:22:46,659
innovation and the collaboration that we

704
00:22:46,659 --> 00:22:48,399
know needs to happen to get towards that

705
00:22:48,399 --> 00:22:51,279
better point and the the good news is

706
00:22:51,279 --> 00:22:53,619
from my perspective is that all that fun

707
00:22:53,619 --> 00:22:54,729
that we've been seeing the suing the

708
00:22:54,729 --> 00:22:56,919
breach price is done so forth all those

709
00:22:56,919 --> 00:22:59,499
data silos and fear they're starting to

710
00:22:59,499 --> 00:23:02,199
be replaced they're being placed slowly

711
00:23:02,199 --> 00:23:03,279
and we're not there yet we can where I

712
00:23:03,279 --> 00:23:04,239
feel like we're at the very very

713
00:23:04,239 --> 00:23:06,189
beginning of that wave which is exciting

714
00:23:06,189 --> 00:23:07,389
as all of us here can actually help

715
00:23:07,389 --> 00:23:09,999
shape that wave or even being a really

716
00:23:09,999 --> 00:23:11,529
seeing where collaboration trust and

717
00:23:11,529 --> 00:23:13,299
sharing can take us as a community and

718
00:23:13,299 --> 00:23:14,649
actually help strengthen our defenses

719
00:23:14,649 --> 00:23:16,749
help strengthen the safety of our

720
00:23:16,749 --> 00:23:19,659
systems help ensure the democracies that

721
00:23:19,659 --> 00:23:22,749
that we want to help promote help

722
00:23:22,749 --> 00:23:25,119
provide greater innovation reinforce the

723
00:23:25,119 --> 00:23:27,129
role of privacy of the fundamental right

724
00:23:27,129 --> 00:23:28,959
so many different aspects as such

725
00:23:28,959 --> 00:23:30,819
far-reaching implications across society

726
00:23:30,819 --> 00:23:33,129
and we're seeing it's a hard business

727
00:23:33,129 --> 00:23:34,839
review just had an article last month

728
00:23:34,839 --> 00:23:36,279
and why are companies forming these

729
00:23:36,279 --> 00:23:40,389
alliances and either underlying in the

730
00:23:40,389 --> 00:23:42,879
bottom line of that was it benefits them

731
00:23:42,879 --> 00:23:44,589
right the end of the day you know

732
00:23:44,589 --> 00:23:45,849
organizations don't come together and

733
00:23:45,849 --> 00:23:47,559
collaborate if it doesn't benefit them

734
00:23:47,559 --> 00:23:49,119
and the energies gets into the incentive

735
00:23:49,119 --> 00:23:50,109
so the incentive structures are

736
00:23:50,109 --> 00:23:51,999
switching now where it isn't there in

737
00:23:51,999 --> 00:23:53,559
their benefit and so these are just

738
00:23:53,559 --> 00:23:57,239
several very of very different kinds of

739
00:23:57,239 --> 00:23:59,469
collaboration I think goes from

740
00:23:59,469 --> 00:24:01,209
everything from something global like

741
00:24:01,209 --> 00:24:04,059
the Paris call which has I think

742
00:24:04,059 --> 00:24:05,439
hundreds of organizations and countries

743
00:24:05,439 --> 00:24:07,479
at this point a macron introduced at

744
00:24:07,479 --> 00:24:09,279
last November basically calling for on

745
00:24:09,279 --> 00:24:10,809
the creation of norms and proper

746
00:24:10,809 --> 00:24:13,119
behavior in cyberspace talk about

747
00:24:13,119 --> 00:24:15,339
virustotal and the the data can go into

748
00:24:15,339 --> 00:24:17,229
there the in the increase of open

749
00:24:17,229 --> 00:24:18,549
frameworks that we're starting to see

750
00:24:18,549 --> 00:24:20,469
and we've seen several of them over the

751
00:24:20,469 --> 00:24:22,980
past few days those are great and other

752
00:24:22,980 --> 00:24:24,630
the steering systems from the ice axe to

753
00:24:24,630 --> 00:24:27,540
the CTA charter trust and they're said a

754
00:24:27,540 --> 00:24:28,890
lot out there they're starting to pop up

755
00:24:28,890 --> 00:24:31,830
that are really really essential and

756
00:24:31,830 --> 00:24:33,240
beneficial and I think that they're

757
00:24:33,240 --> 00:24:35,340
indicative of where we're hopefully

758
00:24:35,340 --> 00:24:37,260
heading as an industry as we're starting

759
00:24:37,260 --> 00:24:38,790
to realize how to build in the

760
00:24:38,790 --> 00:24:40,230
incentives how to build in the trust how

761
00:24:40,230 --> 00:24:42,090
to build and more of a usability across

762
00:24:42,090 --> 00:24:43,830
our community and so I think that's

763
00:24:43,830 --> 00:24:45,540
really exciting because I think at the

764
00:24:45,540 --> 00:24:46,830
end of the day our ability to do that

765
00:24:46,830 --> 00:24:48,840
our ability to collaborate and build

766
00:24:48,840 --> 00:24:51,480
these alliances to counter a lot of

767
00:24:51,480 --> 00:24:52,770
trend the negative trends are out there

768
00:24:52,770 --> 00:24:54,960
as far as losing surveillance having

769
00:24:54,960 --> 00:24:55,890
those bubble charts grow bigger and

770
00:24:55,890 --> 00:24:56,549
bigger I think it's gonna be

771
00:24:56,549 --> 00:24:58,710
foundational to start decreasing those

772
00:24:58,710 --> 00:25:00,600
public charts and to decreasing the

773
00:25:00,600 --> 00:25:02,100
number of targets that might be under

774
00:25:02,100 --> 00:25:03,960
under attack and so those of us that are

775
00:25:03,960 --> 00:25:06,120
here as we you know as we spend the next

776
00:25:06,120 --> 00:25:07,470
few days you're working together here

777
00:25:07,470 --> 00:25:08,809
and go back to our organizations

778
00:25:08,809 --> 00:25:11,130
hopefully you know leaving anything is

779
00:25:11,130 --> 00:25:12,000
just think about how these secure

780
00:25:12,000 --> 00:25:13,380
sharing systems are both beneficial to

781
00:25:13,380 --> 00:25:15,510
you for your organization for the

782
00:25:15,510 --> 00:25:17,040
customers they may have with the

783
00:25:17,040 --> 00:25:18,990
government but they're also really

784
00:25:18,990 --> 00:25:21,059
foundational to helping determine where

785
00:25:21,059 --> 00:25:22,380
we're going as an industry in the future

786
00:25:22,380 --> 00:25:24,299
the internet which is huge implications

787
00:25:24,299 --> 00:25:26,669
for I think democratization and

788
00:25:26,669 --> 00:25:29,400
fundamental rights across the globe and

789
00:25:29,400 --> 00:25:32,900
I'll end it on that thank you

