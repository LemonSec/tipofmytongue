1
00:00:00,240 --> 00:00:02,480
i'm alicia wandles i run the uh

2
00:00:02,480 --> 00:00:03,679
partnership for countering influence

3
00:00:03,679 --> 00:00:05,600
operations at the in carnegie endowment

4
00:00:05,600 --> 00:00:07,520
for international peace and i'm going to

5
00:00:07,520 --> 00:00:09,040
be sharing a little bit about what we've

6
00:00:09,040 --> 00:00:10,719
learned and looking at this topic over

7
00:00:10,719 --> 00:00:12,639
the last couple of years this is not

8
00:00:12,639 --> 00:00:14,080
going though

9
00:00:14,080 --> 00:00:15,920
so what i want to tell you is share some

10
00:00:15,920 --> 00:00:16,720
of the things that we've been

11
00:00:16,720 --> 00:00:18,480
researching um

12
00:00:18,480 --> 00:00:22,160
first thing is what's the problem

13
00:00:22,160 --> 00:00:23,920
democracies around the world have been

14
00:00:23,920 --> 00:00:25,359
struggling to

15
00:00:25,359 --> 00:00:28,720
address an immediate existential crisis

16
00:00:28,720 --> 00:00:30,560
in coronavirus

17
00:00:30,560 --> 00:00:31,279
um

18
00:00:31,279 --> 00:00:32,719
and there would be a lovely slide here

19
00:00:32,719 --> 00:00:34,800
if it goes yes what's the problem okay

20
00:00:34,800 --> 00:00:37,120
where we're going i'm gonna start so

21
00:00:37,120 --> 00:00:38,879
democracies have been struggling to

22
00:00:38,879 --> 00:00:40,719
respond to coronavirus which is an

23
00:00:40,719 --> 00:00:43,200
immediate existential crisis

24
00:00:43,200 --> 00:00:45,039
how do we expect to actually address

25
00:00:45,039 --> 00:00:47,680
climate change when we are having

26
00:00:47,680 --> 00:00:49,520
problems with public decision making and

27
00:00:49,520 --> 00:00:51,520
why is this the case

28
00:00:51,520 --> 00:00:53,120
mostly because we have a polluted

29
00:00:53,120 --> 00:00:55,039
information environment in which people

30
00:00:55,039 --> 00:00:57,840
can manipulate things and public opinion

31
00:00:57,840 --> 00:00:59,600
the information environment is the space

32
00:00:59,600 --> 00:01:01,920
where humans and machines produce and

33
00:01:01,920 --> 00:01:04,080
share information to make sense of the

34
00:01:04,080 --> 00:01:05,119
world

35
00:01:05,119 --> 00:01:06,400
information can include things like

36
00:01:06,400 --> 00:01:09,840
ideas songs codes images and it moves

37
00:01:09,840 --> 00:01:11,040
through the information environment

38
00:01:11,040 --> 00:01:12,560
through different channels that includes

39
00:01:12,560 --> 00:01:14,880
social media but also television radio

40
00:01:14,880 --> 00:01:17,759
print websites and in person

41
00:01:17,759 --> 00:01:19,360
and little is really known about how

42
00:01:19,360 --> 00:01:20,960
that information environment actually

43
00:01:20,960 --> 00:01:23,040
works as a system

44
00:01:23,040 --> 00:01:25,040
now savvy actors as most of us know will

45
00:01:25,040 --> 00:01:27,360
attempt to influence audiences and

46
00:01:27,360 --> 00:01:29,200
outcomes through things like influence

47
00:01:29,200 --> 00:01:30,720
operations

48
00:01:30,720 --> 00:01:32,560
and this tends to happen in a cyclical

49
00:01:32,560 --> 00:01:35,040
model with the ultimate aim of engaging

50
00:01:35,040 --> 00:01:37,119
audiences to become participants in that

51
00:01:37,119 --> 00:01:39,600
and spread the propaganda as well

52
00:01:39,600 --> 00:01:42,079
enter the pcio so we are an

53
00:01:42,079 --> 00:01:44,000
international multi-stakeholder

54
00:01:44,000 --> 00:01:46,000
community of about 31 partners of

55
00:01:46,000 --> 00:01:48,000
academics industry

56
00:01:48,000 --> 00:01:50,320
government and other donors and our aim

57
00:01:50,320 --> 00:01:51,840
is really to foster evidence-based

58
00:01:51,840 --> 00:01:53,520
policy making to counter threats within

59
00:01:53,520 --> 00:01:55,520
the information environment

60
00:01:55,520 --> 00:01:57,840
since 2020 when we launched in january

61
00:01:57,840 --> 00:01:59,439
we started out by conducting a bunch of

62
00:01:59,439 --> 00:02:01,280
meta studies to really understand who

63
00:02:01,280 --> 00:02:03,360
was doing what to counter influence

64
00:02:03,360 --> 00:02:05,200
operations and how much did we really

65
00:02:05,200 --> 00:02:06,799
understand about their effects and the

66
00:02:06,799 --> 00:02:09,199
efficacy of impact of interventions to

67
00:02:09,199 --> 00:02:11,440
them so this is very brief on what we

68
00:02:11,440 --> 00:02:13,680
learned first defining the problem is

69
00:02:13,680 --> 00:02:15,920
hard olga will appreciate this very much

70
00:02:15,920 --> 00:02:17,760
a lot of definitions are used mostly and

71
00:02:17,760 --> 00:02:19,920
consistently from field to field and

72
00:02:19,920 --> 00:02:22,400
even in the same field sometimes

73
00:02:22,400 --> 00:02:24,480
it's a big emerging field we identified

74
00:02:24,480 --> 00:02:27,120
460 initiatives last year

75
00:02:27,120 --> 00:02:28,959
majority are focusing on fact checking

76
00:02:28,959 --> 00:02:31,280
or supporting journalism or some form of

77
00:02:31,280 --> 00:02:33,760
research

78
00:02:34,720 --> 00:02:35,680
and

79
00:02:35,680 --> 00:02:36,640
okay

80
00:02:36,640 --> 00:02:39,519
the experts do fortunately agree mostly

81
00:02:39,519 --> 00:02:41,040
on next steps and that tends to come

82
00:02:41,040 --> 00:02:42,640
down to sharing more information

83
00:02:42,640 --> 00:02:44,560
regulating tech companies greater

84
00:02:44,560 --> 00:02:46,400
transparency

85
00:02:46,400 --> 00:02:48,879
and platforms are making more tweaks but

86
00:02:48,879 --> 00:02:50,640
these tweaks to the platforms tend to be

87
00:02:50,640 --> 00:02:52,480
focused on redirecting users to better

88
00:02:52,480 --> 00:02:55,840
information or content labeling

89
00:02:55,840 --> 00:02:56,959
governments have also seized the

90
00:02:56,959 --> 00:02:58,879
opportunity with covid yes and i've got

91
00:02:58,879 --> 00:03:01,760
five minutes uh to um to implement laws

92
00:03:01,760 --> 00:03:03,120
to counter aspects of influence

93
00:03:03,120 --> 00:03:05,760
operations so what are the gaps

94
00:03:05,760 --> 00:03:07,040
first up we talk about a whole of

95
00:03:07,040 --> 00:03:08,640
society solution but we don't really

96
00:03:08,640 --> 00:03:10,400
have the glue bringing it together there

97
00:03:10,400 --> 00:03:12,239
isn't an obvious field to speak of

98
00:03:12,239 --> 00:03:13,680
there's no key leading institute that i

99
00:03:13,680 --> 00:03:14,879
could recommend that somebody would go

100
00:03:14,879 --> 00:03:15,840
and learn about the information

101
00:03:15,840 --> 00:03:17,680
environment a lot of the convening is ad

102
00:03:17,680 --> 00:03:19,200
hoc and if you look at the different

103
00:03:19,200 --> 00:03:20,879
stakeholders who have to come together

104
00:03:20,879 --> 00:03:22,640
it's difficult they don't trust each

105
00:03:22,640 --> 00:03:25,599
other and they're not really talking

106
00:03:25,599 --> 00:03:26,879
just to give some example in the

107
00:03:26,879 --> 00:03:28,799
resources that are required we take an

108
00:03:28,799 --> 00:03:31,200
8c step to change and that involves

109
00:03:31,200 --> 00:03:32,640
building that community convening them

110
00:03:32,640 --> 00:03:34,159
and finding answers and moving that

111
00:03:34,159 --> 00:03:35,680
forward

112
00:03:35,680 --> 00:03:36,959
one of the reasons we don't have this

113
00:03:36,959 --> 00:03:38,720
clue is because operational funding is

114
00:03:38,720 --> 00:03:40,239
lacking and the community tends to agree

115
00:03:40,239 --> 00:03:41,840
with this a lot of it's project-based so

116
00:03:41,840 --> 00:03:43,519
we get a lot more case studies than

117
00:03:43,519 --> 00:03:45,200
anything else and there's a lack of

118
00:03:45,200 --> 00:03:46,799
diversity in the field the majority of

119
00:03:46,799 --> 00:03:50,080
it is in north america and europe

120
00:03:50,080 --> 00:03:51,680
most of the proposals that come out

121
00:03:51,680 --> 00:03:53,360
don't have the details for industry or

122
00:03:53,360 --> 00:03:55,200
government to implement so this is also

123
00:03:55,200 --> 00:03:56,879
problematic we don't really know what to

124
00:03:56,879 --> 00:03:57,760
do

125
00:03:57,760 --> 00:03:59,599
and little is known about the threat so

126
00:03:59,599 --> 00:04:01,120
we don't know much about the effects of

127
00:04:01,120 --> 00:04:03,200
influence operations mostly only in the

128
00:04:03,200 --> 00:04:05,040
context of mass media very little is

129
00:04:05,040 --> 00:04:07,280
known about the long-term effects using

130
00:04:07,280 --> 00:04:08,879
digital media and we don't know much

131
00:04:08,879 --> 00:04:11,040
about western populations we know even

132
00:04:11,040 --> 00:04:13,280
less about the efficacy of interventions

133
00:04:13,280 --> 00:04:14,720
eight percent of the interventions we

134
00:04:14,720 --> 00:04:16,320
looked at that companies disclosed only

135
00:04:16,320 --> 00:04:18,079
eight percent mentioned efficacy with no

136
00:04:18,079 --> 00:04:19,680
further details of whether it worked or

137
00:04:19,680 --> 00:04:20,639
not

138
00:04:20,639 --> 00:04:22,800
um that leaves massive gaps in things

139
00:04:22,800 --> 00:04:24,720
like what happens when we de-platform

140
00:04:24,720 --> 00:04:26,840
people what happens when we do content

141
00:04:26,840 --> 00:04:29,440
moderation why do we know so little

142
00:04:29,440 --> 00:04:30,960
down comes down to a lack of

143
00:04:30,960 --> 00:04:32,639
transparency reporting researchers just

144
00:04:32,639 --> 00:04:34,240
don't know what research is available so

145
00:04:34,240 --> 00:04:36,160
they don't even know what to ask for

146
00:04:36,160 --> 00:04:38,080
we don't have data sharing rules so

147
00:04:38,080 --> 00:04:39,440
nobody really knows who should have

148
00:04:39,440 --> 00:04:41,440
access to what data for what purposes

149
00:04:41,440 --> 00:04:43,360
and who gets to decide that and then on

150
00:04:43,360 --> 00:04:45,120
top of that even if we had those things

151
00:04:45,120 --> 00:04:47,440
worked out we don't have a mechanism for

152
00:04:47,440 --> 00:04:50,160
longer term research to facilitate

153
00:04:50,160 --> 00:04:51,919
knowing more about interventions that

154
00:04:51,919 --> 00:04:53,280
would protect the independence of

155
00:04:53,280 --> 00:04:55,680
researchers so what to do

156
00:04:55,680 --> 00:04:57,520
a networked problem needs a networked

157
00:04:57,520 --> 00:04:59,759
solution so every stakeholder really

158
00:04:59,759 --> 00:05:01,360
needs to come together and do things

159
00:05:01,360 --> 00:05:03,919
civil society should be convening to

160
00:05:03,919 --> 00:05:05,280
come up with some of these answers to

161
00:05:05,280 --> 00:05:06,960
determine who gets access to what data

162
00:05:06,960 --> 00:05:09,120
and on what terms academia should come

163
00:05:09,120 --> 00:05:10,560
together instead of being separate

164
00:05:10,560 --> 00:05:12,400
centers work on having bigger

165
00:05:12,400 --> 00:05:13,919
infrastructure to do

166
00:05:13,919 --> 00:05:16,320
bigger testing like on interventions

167
00:05:16,320 --> 00:05:17,520
government should be regulating

168
00:05:17,520 --> 00:05:19,520
transparency reporting and data sharing

169
00:05:19,520 --> 00:05:21,600
rules obviously civil society should be

170
00:05:21,600 --> 00:05:23,280
giving them some answers there but they

171
00:05:23,280 --> 00:05:24,800
should also look at funding something

172
00:05:24,800 --> 00:05:26,160
like a cern for the information

173
00:05:26,160 --> 00:05:27,800
environment something that is

174
00:05:27,800 --> 00:05:30,160
multi-stakeholder industry can fund

175
00:05:30,160 --> 00:05:31,840
donors can fund and researchers can go

176
00:05:31,840 --> 00:05:33,120
and learn more

177
00:05:33,120 --> 00:05:34,479
platforms don't have to wait for

178
00:05:34,479 --> 00:05:36,080
governments they can start to do greater

179
00:05:36,080 --> 00:05:38,400
transparency reporting today but they

180
00:05:38,400 --> 00:05:40,000
should also be continuing to work with

181
00:05:40,000 --> 00:05:42,000
researchers ideally on good faith

182
00:05:42,000 --> 00:05:43,759
maintaining their independence and

183
00:05:43,759 --> 00:05:46,080
finally donors can support that glue and

184
00:05:46,080 --> 00:05:48,080
that multi-stakeholder need that we have

185
00:05:48,080 --> 00:05:50,400
to have a whole of society solution

186
00:05:50,400 --> 00:05:52,720
and with that i will not have any more

187
00:05:52,720 --> 00:05:56,160
tech failures thank you

