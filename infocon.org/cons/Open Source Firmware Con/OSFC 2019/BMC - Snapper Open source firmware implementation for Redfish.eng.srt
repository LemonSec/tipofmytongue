1
00:00:03,190 --> 00:00:06,259
[Music]

2
00:00:07,670 --> 00:00:12,420
I'm Kevin from Lenovo so I'm going to

3
00:00:12,420 --> 00:00:15,360
talk about the open source graphic

4
00:00:15,360 --> 00:00:20,820
engine actually this is not something

5
00:00:20,820 --> 00:00:23,880
like how to implement the Rhenish

6
00:00:23,880 --> 00:00:27,689
specific schema but a kind of framework

7
00:00:27,689 --> 00:00:32,130
to help us develop our radish service

8
00:00:32,130 --> 00:00:36,980
quickly yeah we we had this idea about

9
00:00:36,980 --> 00:00:42,270
since the beginning of last year because

10
00:00:42,270 --> 00:00:44,309
you know the real fish is moving very

11
00:00:44,309 --> 00:00:48,530
fast we are consider the TMT of

12
00:00:48,530 --> 00:00:52,110
discussion and update on the radishes

13
00:00:52,110 --> 00:00:57,899
schema and radish spec so sometimes we

14
00:00:57,899 --> 00:01:01,079
have to change our design and also

15
00:01:01,079 --> 00:01:04,949
repeat to some repeat work because of

16
00:01:04,949 --> 00:01:08,970
there is a new education on the property

17
00:01:08,970 --> 00:01:12,890
because because there is new enum enum

18
00:01:12,890 --> 00:01:18,360
member or new annotation we have to some

19
00:01:18,360 --> 00:01:23,460
change to to adapt this new change but

20
00:01:23,460 --> 00:01:26,119
adapt this in a spec and schema update

21
00:01:26,119 --> 00:01:30,930
so we do want a more efficient engine to

22
00:01:30,930 --> 00:01:34,520
reduce the maintenance effort on the

23
00:01:34,520 --> 00:01:38,400
schemas and the red fish spec upgrading

24
00:01:38,400 --> 00:01:42,270
but focus more on the red fish schema

25
00:01:42,270 --> 00:01:50,700
itself okay what what we want to want

26
00:01:50,700 --> 00:01:54,899
from the red fish engine first it should

27
00:01:54,899 --> 00:02:00,119
be a small engine with with small engine

28
00:02:00,119 --> 00:02:03,570
with fuel memory footprint and it could

29
00:02:03,570 --> 00:02:08,699
be the library so it we want it to run

30
00:02:08,699 --> 00:02:13,770
on post p.m. C and the laptop means

31
00:02:13,770 --> 00:02:18,510
and we do want to have a measure to team

32
00:02:18,510 --> 00:02:23,040
the to tune the response time one of the

33
00:02:23,040 --> 00:02:26,130
most important Singh for redfish engine

34
00:02:26,130 --> 00:02:30,750
is we have to separate the redfish per

35
00:02:30,750 --> 00:02:34,830
taco stories from the redfish data model

36
00:02:34,830 --> 00:02:39,690
for example I'm focusing on the red

37
00:02:39,690 --> 00:02:43,890
storage controller is unnecessary for me

38
00:02:43,890 --> 00:02:46,950
to know the detail of the how to

39
00:02:46,950 --> 00:02:49,890
calculate the attack how to assign the

40
00:02:49,890 --> 00:02:53,190
privilege I need to focus more on the

41
00:02:53,190 --> 00:02:59,390
data model of raid volume drive right so

42
00:02:59,390 --> 00:03:03,120
you know the redfish spec has a long

43
00:03:03,120 --> 00:03:06,690
list of requirements we don't want to

44
00:03:06,690 --> 00:03:09,870
this one this becomes spreading or the

45
00:03:09,870 --> 00:03:14,600
skimmers but we do want put all of the

46
00:03:14,600 --> 00:03:18,090
protocol and service related modules

47
00:03:18,090 --> 00:03:23,160
inside the redfish engine and also we

48
00:03:23,160 --> 00:03:27,360
want to extract most common things in

49
00:03:27,360 --> 00:03:30,390
the real fish schema outside of the data

50
00:03:30,390 --> 00:03:36,870
model so based on which we can make make

51
00:03:36,870 --> 00:03:39,870
the most automation work for example we

52
00:03:39,870 --> 00:03:43,950
can also generate the crossfire which

53
00:03:43,950 --> 00:03:46,980
containing the static definition from

54
00:03:46,980 --> 00:03:54,450
the schema and we also want to do auto

55
00:03:54,450 --> 00:03:58,470
generation for our redfish response or

56
00:03:58,470 --> 00:04:03,240
the most common commonalities like the

57
00:04:03,240 --> 00:04:06,810
headers the meditator the OData the

58
00:04:06,810 --> 00:04:09,870
OData information the navigation

59
00:04:09,870 --> 00:04:15,930
property the links and so on and also

60
00:04:15,930 --> 00:04:21,079
you know the redfish cover a lot of area

61
00:04:21,079 --> 00:04:24,450
which means we need to get the data from

62
00:04:24,450 --> 00:04:27,630
all kinds of the data source so we doing

63
00:04:27,630 --> 00:04:31,800
neither unified method to represent

64
00:04:31,800 --> 00:04:36,690
their the diversity of data type and

65
00:04:36,690 --> 00:04:41,370
data source this can also make sure we

66
00:04:41,370 --> 00:04:44,700
have a common coding style when multiple

67
00:04:44,700 --> 00:04:48,780
people much more people working on the

68
00:04:48,780 --> 00:04:54,480
redfish schemas and finally about the

69
00:04:54,480 --> 00:04:58,620
performance as you know we cannot

70
00:04:58,620 --> 00:05:01,440
promise each schema has the same

71
00:05:01,440 --> 00:05:05,130
response time but we do want a measure

72
00:05:05,130 --> 00:05:10,860
to tune tune the response time most time

73
00:05:10,860 --> 00:05:18,000
it means the caching okay

74
00:05:18,000 --> 00:05:20,580
let's take a look at the snare

75
00:05:20,580 --> 00:05:24,330
architecture first basically it has

76
00:05:24,330 --> 00:05:29,130
three parts the first is there in green

77
00:05:29,130 --> 00:05:31,470
color is the web server and application

78
00:05:31,470 --> 00:05:34,500
server we are using the nginx

79
00:05:34,500 --> 00:05:37,470
and the Judy comb and the flask of

80
00:05:37,470 --> 00:05:39,840
framework which for which using which

81
00:05:39,840 --> 00:05:43,410
use their tablets Jie interface the I

82
00:05:43,410 --> 00:05:47,510
think this has a couple of benefits

83
00:05:47,510 --> 00:05:53,250
first we can move some work balance work

84
00:05:53,250 --> 00:05:57,870
to the nginx instead of and then invent

85
00:05:57,870 --> 00:06:01,980
it in the redfish engine and also if we

86
00:06:01,980 --> 00:06:05,880
have better choice on the web server or

87
00:06:05,880 --> 00:06:08,520
application server we can replace them

88
00:06:08,520 --> 00:06:12,990
without changing our engine code okay

89
00:06:12,990 --> 00:06:17,310
the second part is the engine as I just

90
00:06:17,310 --> 00:06:21,180
mentioned we put the most protocol and

91
00:06:21,180 --> 00:06:24,860
the slowest modules inside the engine

92
00:06:24,860 --> 00:06:28,080
including the request parsing the data

93
00:06:28,080 --> 00:06:33,740
model parsing and redfish related

94
00:06:33,740 --> 00:06:37,130
related service a service module like

95
00:06:37,130 --> 00:06:41,660
session token authentication privilege

96
00:06:41,660 --> 00:06:45,870
and we also exchange most commonalities

97
00:06:45,870 --> 00:06:50,009
from the relevant schema we put the base

98
00:06:50,009 --> 00:06:54,900
class in this engine for auto generation

99
00:06:54,900 --> 00:06:57,690
of the response we put our auto

100
00:06:57,690 --> 00:07:00,630
generated a classifier in this layer I

101
00:07:00,630 --> 00:07:03,889
will talk about it later

102
00:07:06,060 --> 00:07:11,160
the third part is the exact relative

103
00:07:11,160 --> 00:07:17,190
schema implementation we also put the

104
00:07:17,190 --> 00:07:20,039
data interface layer here to help

105
00:07:20,039 --> 00:07:24,410
developers to to work on their own

106
00:07:24,410 --> 00:07:31,410
schemers and we have their back-end data

107
00:07:31,410 --> 00:07:34,349
provision inside this base layer and

108
00:07:34,349 --> 00:07:40,199
also providing the caching we also have

109
00:07:40,199 --> 00:07:45,650
the standalone tour to generate the

110
00:07:45,650 --> 00:07:54,509
schema the class files from schemer ok

111
00:07:54,509 --> 00:07:59,610
next I will introduce several design

112
00:07:59,610 --> 00:08:03,570
point house never take the efficiency

113
00:08:03,570 --> 00:08:06,659
into account first is about the class

114
00:08:06,659 --> 00:08:10,199
hierarchy so at the bottom is the base

115
00:08:10,199 --> 00:08:14,699
class and resource class map map to the

116
00:08:14,699 --> 00:08:21,930
standard schema in this layer there are

117
00:08:21,930 --> 00:08:28,500
many methods to use to help assemble the

118
00:08:28,500 --> 00:08:35,240
response data without your menu work and

119
00:08:35,240 --> 00:08:39,870
we we put the most common and

120
00:08:39,870 --> 00:08:44,820
commonality things in in this base based

121
00:08:44,820 --> 00:08:50,100
classes and in the middle the middle of

122
00:08:50,100 --> 00:08:52,589
the middle block in the auto-generated

123
00:08:52,589 --> 00:08:55,230
cast there are

124
00:08:55,230 --> 00:08:59,430
from the schema files so this prey and

125
00:08:59,430 --> 00:09:03,840
bring a great benefit from schema

126
00:09:03,840 --> 00:09:08,790
upgrading for example if we are we are

127
00:09:08,790 --> 00:09:11,550
planning to use the upgrading upgrade to

128
00:09:11,550 --> 00:09:16,650
the schema 2018 country there are some

129
00:09:16,650 --> 00:09:19,890
new notations on the property there are

130
00:09:19,890 --> 00:09:24,480
a new privilege defined for the class or

131
00:09:24,480 --> 00:09:28,920
operation or there are some new enum

132
00:09:28,920 --> 00:09:34,590
enum members we just need to use the

133
00:09:34,590 --> 00:09:38,550
utility to generate all these class

134
00:09:38,550 --> 00:09:42,170
files from schema and replace them so

135
00:09:42,170 --> 00:09:45,980
the base class will contain all the

136
00:09:45,980 --> 00:09:49,260
privilege of validation the enum

137
00:09:49,260 --> 00:09:52,650
validation in the request body and even

138
00:09:52,650 --> 00:09:57,180
the annotation inside the schema schema

139
00:09:57,180 --> 00:09:59,900
class file so you don't need to need to

140
00:09:59,900 --> 00:10:03,510
change the implement implementation or

141
00:10:03,510 --> 00:10:07,530
the best class you just need to replace

142
00:10:07,530 --> 00:10:12,600
the auto generic the classifier above

143
00:10:12,600 --> 00:10:17,550
the classifier exactly what you need to

144
00:10:17,550 --> 00:10:21,720
focus on the schema class you just need

145
00:10:21,720 --> 00:10:25,320
to fear in the dynamic data you are

146
00:10:25,320 --> 00:10:28,620
interested in to implement in these

147
00:10:28,620 --> 00:10:33,330
files this class files and they are when

148
00:10:33,330 --> 00:10:36,290
to when when mapping to the

149
00:10:36,290 --> 00:10:39,990
auto-generate in the class for example

150
00:10:39,990 --> 00:10:43,860
the computer system you interact from

151
00:10:43,860 --> 00:10:47,450
the underscore P computer system and the

152
00:10:47,450 --> 00:10:51,330
chassis and the power and also on the

153
00:10:51,330 --> 00:10:56,760
top is the order redfish protocol and

154
00:10:56,760 --> 00:11:00,650
the service modules you can extend your

155
00:11:00,650 --> 00:11:04,380
modules based on the update of the

156
00:11:04,380 --> 00:11:09,100
redfish schema without change

157
00:11:09,100 --> 00:11:12,580
the engine code or schema class class

158
00:11:12,580 --> 00:11:21,160
fare okay next is about the resource

159
00:11:21,160 --> 00:11:26,890
tree basically result research you can

160
00:11:26,890 --> 00:11:29,580
view the redfish data module as a

161
00:11:29,580 --> 00:11:35,440
resource tree the concept in in redfish

162
00:11:35,440 --> 00:11:39,460
spec is the resource tree is actually a

163
00:11:39,460 --> 00:11:45,120
set of URI there is only one we all know

164
00:11:45,120 --> 00:11:49,710
URI that is their service service route

165
00:11:49,710 --> 00:11:54,400
but actually we cannot make sure all the

166
00:11:54,400 --> 00:11:57,070
rest the UI are consistent because it

167
00:11:57,070 --> 00:12:01,090
may change our hardware configuration

168
00:12:01,090 --> 00:12:12,640
for our designer yeah and sometimes the

169
00:12:12,640 --> 00:12:15,520
cross reference reference in the between

170
00:12:15,520 --> 00:12:18,550
the class resulting the reference ring

171
00:12:18,550 --> 00:12:23,440
so let's considering how the network

172
00:12:23,440 --> 00:12:27,760
adapter collection how does it know how

173
00:12:27,760 --> 00:12:32,380
many members under it and how does the

174
00:12:32,380 --> 00:12:35,710
network adapter knows the reference

175
00:12:35,710 --> 00:12:41,970
being to PCI device on the system schema

176
00:12:41,970 --> 00:12:45,310
apparently hard code is not a good idea

177
00:12:45,310 --> 00:12:49,860
so in snapper we introduced their

178
00:12:49,860 --> 00:12:55,180
accessor key you can make your own lower

179
00:12:55,180 --> 00:13:00,220
two to define the accessor key each URL

180
00:13:00,220 --> 00:13:04,360
is mapped to the accessor key here we

181
00:13:04,360 --> 00:13:07,470
defined a successor key with the

182
00:13:07,470 --> 00:13:12,310
instance ID and class name now

183
00:13:12,310 --> 00:13:16,440
you can map between the URI and

184
00:13:16,440 --> 00:13:19,030
obsessive heat

185
00:13:19,030 --> 00:13:22,000
the question is how do we get the

186
00:13:22,000 --> 00:13:22,750
topology

187
00:13:22,750 --> 00:13:30,100
of the UI are a necessity if you are

188
00:13:30,100 --> 00:13:32,470
using the data module passing you can

189
00:13:32,470 --> 00:13:35,800
certainly get part of the topology but

190
00:13:35,800 --> 00:13:40,630
is not enough you must implement the cat

191
00:13:40,630 --> 00:13:45,030
member ID access a key for each resource

192
00:13:45,030 --> 00:13:49,630
that means when the PCI device

193
00:13:49,630 --> 00:13:54,010
collection cards their PCI that you'd

194
00:13:54,010 --> 00:13:56,980
want to know the PCI device ID it were

195
00:13:56,980 --> 00:14:00,760
called or PCI device cat member access a

196
00:14:00,760 --> 00:14:03,670
key then this function will return the

197
00:14:03,670 --> 00:14:09,190
instance ID and the accessor key so that

198
00:14:09,190 --> 00:14:11,910
builds the relationship between the

199
00:14:11,910 --> 00:14:15,910
schema instance ID and accessor key and

200
00:14:15,910 --> 00:14:20,670
the URI so back to the previous question

201
00:14:20,670 --> 00:14:25,210
how do we know the how does the network

202
00:14:25,210 --> 00:14:28,960
adapter knows there is a link under the

203
00:14:28,960 --> 00:14:36,690
system schema is just called function

204
00:14:36,690 --> 00:14:40,720
members as a key for the PCI device

205
00:14:40,720 --> 00:14:44,350
class then PCI device class will return

206
00:14:44,350 --> 00:14:47,500
the accessor key and based on the

207
00:14:47,500 --> 00:14:50,950
information inside the Sasaki is the

208
00:14:50,950 --> 00:14:55,210
slot number on Baldwin and sorcery then

209
00:14:55,210 --> 00:14:59,400
we use their mapping API to kill the URI

210
00:14:59,400 --> 00:15:05,560
yes and please note if you are using the

211
00:15:05,560 --> 00:15:09,400
hard code of your eye the TM TF markup

212
00:15:09,400 --> 00:15:15,160
may change may move the PCI device from

213
00:15:15,160 --> 00:15:18,370
system to the chassis over time we don't

214
00:15:18,370 --> 00:15:23,560
know when he makes the change but if you

215
00:15:23,560 --> 00:15:27,730
are using the accessor key you only knew

216
00:15:27,730 --> 00:15:32,020
to to a small change for example if you

217
00:15:32,020 --> 00:15:36,579
are under the chassis you can determine

218
00:15:36,579 --> 00:15:39,579
they're determine you if you need to

219
00:15:39,579 --> 00:15:43,389
return the right mapping by the

220
00:15:43,389 --> 00:15:48,399
containing pass if the containing pass

221
00:15:48,399 --> 00:15:51,879
is under chassis then you return this

222
00:15:51,879 --> 00:15:55,720
psi device device table but if you are

223
00:15:55,720 --> 00:15:58,360
under the system you don't need to

224
00:15:58,360 --> 00:16:08,110
return any Macky okay next is about the

225
00:16:08,110 --> 00:16:12,360
unified data data in the face layer

226
00:16:12,360 --> 00:16:15,279
since the redfish schema cover many

227
00:16:15,279 --> 00:16:18,670
areas there are many data sauce and the

228
00:16:18,670 --> 00:16:21,939
tail type so internally we were used or

229
00:16:21,939 --> 00:16:26,529
binary JSON Pisan to represent the

230
00:16:26,529 --> 00:16:30,850
internal data structure and based on

231
00:16:30,850 --> 00:16:34,059
which we can provide a common API to set

232
00:16:34,059 --> 00:16:39,220
in the carrot ater this actually split

233
00:16:39,220 --> 00:16:43,299
the data provision to two layers one

234
00:16:43,299 --> 00:16:46,509
layer is just simply fill in the schema

235
00:16:46,509 --> 00:16:51,089
data and the other is for data driver

236
00:16:51,089 --> 00:16:54,069
which actually interact with the

237
00:16:54,069 --> 00:16:59,799
hardware interface and we can group this

238
00:16:59,799 --> 00:17:02,439
theater you know redfish request

239
00:17:02,439 --> 00:17:06,909
normally they read read or write a bunch

240
00:17:06,909 --> 00:17:10,959
of data so we can group the groups in

241
00:17:10,959 --> 00:17:13,630
digital high profile we can put the

242
00:17:13,630 --> 00:17:17,398
inventory data together we can pull the

243
00:17:17,398 --> 00:17:22,689
red wiki data together we can put the

244
00:17:22,689 --> 00:17:27,059
reddit or together by different profile

245
00:17:27,059 --> 00:17:31,720
so as you can see several schema can

246
00:17:31,720 --> 00:17:36,279
share the same data returned from this

247
00:17:36,279 --> 00:17:39,639
API so furniture this kind of data

248
00:17:39,639 --> 00:17:42,789
interface can centralize the data

249
00:17:42,789 --> 00:17:45,809
caching management

250
00:17:50,690 --> 00:17:54,200
this is an example of the interface API

251
00:17:54,200 --> 00:17:58,679
we provide the two api's calendar and

252
00:17:58,679 --> 00:18:02,100
senator the first example carried the

253
00:18:02,100 --> 00:18:06,350
data from profile computer system

254
00:18:06,350 --> 00:18:09,750
non-volatile data which means most of

255
00:18:09,750 --> 00:18:13,860
this data are consistent we don't need

256
00:18:13,860 --> 00:18:18,750
to retrieve them again and again and and

257
00:18:18,750 --> 00:18:20,940
the we can also called a function

258
00:18:20,940 --> 00:18:25,200
defining the XML with parameters and the

259
00:18:25,200 --> 00:18:30,419
set data by key value format yeah

260
00:18:30,419 --> 00:18:34,679
on the right side the XML define the

261
00:18:34,679 --> 00:18:39,750
profile in the computer system group you

262
00:18:39,750 --> 00:18:42,929
can see we support the variable and

263
00:18:42,929 --> 00:18:49,530
table and there are flags for the right

264
00:18:49,530 --> 00:18:54,840
potater and at the bottom is the usage

265
00:18:54,840 --> 00:18:59,820
of P so you can see this kind of vision

266
00:18:59,820 --> 00:19:02,700
usage is very straightforward and easy

267
00:19:02,700 --> 00:19:05,210
to use

268
00:19:08,720 --> 00:19:15,419
so next talk about the data caching in

269
00:19:15,419 --> 00:19:20,220
previous data interface by Nature it has

270
00:19:20,220 --> 00:19:24,080
the caching management capability so

271
00:19:24,080 --> 00:19:27,480
basically we in stepper we can have to

272
00:19:27,480 --> 00:19:33,440
cash layer the first is the json text

273
00:19:33,440 --> 00:19:39,980
json text caching is mapped UI 2 to the

274
00:19:39,980 --> 00:19:45,360
json response the most common scenario

275
00:19:45,360 --> 00:19:49,169
is the log entries for example you have

276
00:19:49,169 --> 00:19:54,540
five four thousand log entries this this

277
00:19:54,540 --> 00:19:58,610
kind of data will take a long time to do

278
00:19:58,610 --> 00:20:02,309
sterilization so if we catch this

279
00:20:02,309 --> 00:20:08,460
in memory or database laggin readies you

280
00:20:08,460 --> 00:20:12,379
can save a lot of civilization time that

281
00:20:12,379 --> 00:20:16,889
this kind of caching has a problem that

282
00:20:16,889 --> 00:20:21,679
is the Quran analogy is too big it

283
00:20:21,679 --> 00:20:25,889
cannot share between between class okay

284
00:20:25,889 --> 00:20:31,679
then we have the second layer air to air

285
00:20:31,679 --> 00:20:34,499
to caching based on piece on because B

286
00:20:34,499 --> 00:20:38,489
song is a binary you save lots of memory

287
00:20:38,489 --> 00:20:44,839
and we can group this data by profile

288
00:20:44,839 --> 00:20:50,869
here take an example we pull the storage

289
00:20:50,869 --> 00:20:54,869
inventory together in you know you know

290
00:20:54,869 --> 00:20:58,379
piece on object and we also put we PDF

291
00:20:58,379 --> 00:21:03,139
raw data in another piece on object and

292
00:21:03,139 --> 00:21:08,669
and also about the network adapter here

293
00:21:08,669 --> 00:21:11,759
is an example when you when the clan

294
00:21:11,759 --> 00:21:17,969
they used their car or postman to access

295
00:21:17,969 --> 00:21:22,889
their network collection the back end

296
00:21:22,889 --> 00:21:26,479
actually already have other network

297
00:21:26,479 --> 00:21:30,149
adapter information from from board to

298
00:21:30,149 --> 00:21:36,389
sort 1 2 3 so if we catch this data you

299
00:21:36,389 --> 00:21:38,909
don't need to access them again when

300
00:21:38,909 --> 00:21:42,570
next time user access the slot 1 network

301
00:21:42,570 --> 00:21:47,450
adapter so the most important thing for

302
00:21:47,450 --> 00:21:52,409
l2 cache is how we group this data how

303
00:21:52,409 --> 00:21:56,159
we determine the granularity of this

304
00:21:56,159 --> 00:21:59,570
business object

305
00:22:04,040 --> 00:22:10,110
yeah I I did a comparison between the

306
00:22:10,110 --> 00:22:13,830
snapper and BMC web actually I don't

307
00:22:13,830 --> 00:22:16,970
find many example to do the comparison

308
00:22:16,970 --> 00:22:22,380
so so please forgive me if I have missed

309
00:22:22,380 --> 00:22:29,100
understanding in this table here is a

310
00:22:29,100 --> 00:22:32,910
little unfair to compare BMC web with

311
00:22:32,910 --> 00:22:37,830
Napa because BMC app is actually to

312
00:22:37,830 --> 00:22:41,610
everything web server which support not

313
00:22:41,610 --> 00:22:45,680
only redfish but also the web GUI the

314
00:22:45,680 --> 00:22:51,870
console direction where snapper folks on

315
00:22:51,870 --> 00:22:54,990
the redfish Engine a lightweight

316
00:22:54,990 --> 00:22:58,650
lightweight engine it does not create

317
00:22:58,650 --> 00:23:02,750
the web server it can even replace the

318
00:23:02,750 --> 00:23:06,960
web server it connected to so I think

319
00:23:06,960 --> 00:23:10,080
it's possible that we can integrate this

320
00:23:10,080 --> 00:23:16,980
engine with BMC at BMC web and because

321
00:23:16,980 --> 00:23:20,100
it is using their index and the flask

322
00:23:20,100 --> 00:23:26,130
framework it can leverage many open

323
00:23:26,130 --> 00:23:31,410
source features like ssee misaki from

324
00:23:31,410 --> 00:23:35,840
the nginx and the nginx and flask

325
00:23:35,840 --> 00:23:40,920
modules and we also create the data

326
00:23:40,920 --> 00:23:44,900
interface layer to reduce the effort of

327
00:23:44,900 --> 00:23:53,780
schema developer about the performance I

328
00:23:53,780 --> 00:23:56,760
think the BMC web is using the booster

329
00:23:56,760 --> 00:24:01,860
tau I so it should be we should have

330
00:24:01,860 --> 00:24:06,450
high efficiency on performance snapper

331
00:24:06,450 --> 00:24:10,590
use the C extension module and we rely

332
00:24:10,590 --> 00:24:14,790
on the Python networking library G event

333
00:24:14,790 --> 00:24:17,640
for concurrent access

334
00:24:17,640 --> 00:24:20,580
because the junii comb is prefault

335
00:24:20,580 --> 00:24:24,960
the pre for the worker mode mmm is

336
00:24:24,960 --> 00:24:28,080
support they're much much more process

337
00:24:28,080 --> 00:24:32,190
module so but but we also have the

338
00:24:32,190 --> 00:24:35,040
capability to support multi thread and

339
00:24:35,040 --> 00:24:38,640
we have two levels of data caching to

340
00:24:38,640 --> 00:24:44,700
tune the performance about the protocol

341
00:24:44,700 --> 00:24:47,600
and stories

342
00:24:47,820 --> 00:24:53,250
I see the BMC web will rely on the web

343
00:24:53,250 --> 00:24:58,080
server cron support I also see there are

344
00:24:58,080 --> 00:25:02,130
some missing function defined in the

345
00:25:02,130 --> 00:25:05,460
redfish spec like attacked or auditory

346
00:25:05,460 --> 00:25:08,790
tank property and aquarium parameters

347
00:25:08,790 --> 00:25:12,290
and the task and the head had operation

348
00:25:12,290 --> 00:25:16,440
our snapper in our snapper implantation

349
00:25:16,440 --> 00:25:19,679
we are fully compliant with relevance

350
00:25:19,679 --> 00:25:27,630
back 1.5 and also if tmpfs redfish back

351
00:25:27,630 --> 00:25:30,540
have new requirement on the protocol

352
00:25:30,540 --> 00:25:33,990
service and we can easily add a new

353
00:25:33,990 --> 00:25:37,860
modules without changing the current

354
00:25:37,860 --> 00:25:45,780
schema code engine code or we should

355
00:25:45,780 --> 00:25:51,300
only change the engine code next is

356
00:25:51,300 --> 00:25:57,540
about the redfish data model i see the

357
00:25:57,540 --> 00:26:03,150
pmc web schema class is derived from the

358
00:26:03,150 --> 00:26:07,230
node which is covered with the chrome

359
00:26:07,230 --> 00:26:13,950
app instance for each schema it is using

360
00:26:13,950 --> 00:26:18,960
the static you are rotating which means

361
00:26:18,960 --> 00:26:21,960
the class is binding it with the hull

362
00:26:21,960 --> 00:26:27,600
code URI and all the schema

363
00:26:27,600 --> 00:26:31,220
implementation should be in there

364
00:26:31,220 --> 00:26:34,370
schema class an action class but for

365
00:26:34,370 --> 00:26:37,750
snapper we are running the runtime

366
00:26:37,750 --> 00:26:42,350
runtime band in between the URI and the

367
00:26:42,350 --> 00:26:45,520
real fishy resource because we have the

368
00:26:45,520 --> 00:26:50,750
data model passing the EDM passing so

369
00:26:50,750 --> 00:26:55,460
there is no hardcore UI and we are also

370
00:26:55,460 --> 00:27:02,290
trying to - most automation work - like

371
00:27:02,290 --> 00:27:06,800
we generate the most common data in the

372
00:27:06,800 --> 00:27:11,780
response for each schema about the red

373
00:27:11,780 --> 00:27:16,760
fish upgrading yeah I do see I to see

374
00:27:16,760 --> 00:27:19,370
the PMC web I have Auto generation for

375
00:27:19,370 --> 00:27:26,870
the magic message registry but when they

376
00:27:26,870 --> 00:27:29,420
I think when they they are going to

377
00:27:29,420 --> 00:27:31,790
upgrade the schema they need to change

378
00:27:31,790 --> 00:27:37,040
the code for the schema version UNAM

379
00:27:37,040 --> 00:27:43,790
dispatcher for snapper we generate all

380
00:27:43,790 --> 00:27:47,120
the registry including error message

381
00:27:47,120 --> 00:27:49,640
registry event reduced privilege

382
00:27:49,640 --> 00:27:52,820
registry from the skin of schema fair

383
00:27:52,820 --> 00:27:56,840
and for the base registry we just run

384
00:27:56,840 --> 00:27:59,870
time generated from the data we got from

385
00:27:59,870 --> 00:28:08,540
UEFI yeah and we also as I mentioned in

386
00:28:08,540 --> 00:28:12,879
in the class in

387
00:28:30,670 --> 00:28:33,260
as I just mentioned the efficient

388
00:28:33,260 --> 00:28:37,580
someone the class and hierarchy the

389
00:28:37,580 --> 00:28:40,250
snapper used the utility to generate all

390
00:28:40,250 --> 00:28:44,720
the schema header files from OData XML

391
00:28:44,720 --> 00:28:49,190
schema yeah for if you want to operate

392
00:28:49,190 --> 00:28:52,190
the schema bundle we just need a few

393
00:28:52,190 --> 00:29:02,980
minutes to replace the head of our next

394
00:29:02,980 --> 00:29:07,720
these are what we want to do first never

395
00:29:07,720 --> 00:29:13,550
in future first we need to move move the

396
00:29:13,550 --> 00:29:18,320
move from Python to 2003 since the

397
00:29:18,320 --> 00:29:21,560
patient to will not be maintained in

398
00:29:21,560 --> 00:29:29,420
2020 but the snapper actually has less

399
00:29:29,420 --> 00:29:31,670
dependency on Python you should not

400
00:29:31,670 --> 00:29:35,870
should not be a big work and also if we

401
00:29:35,870 --> 00:29:39,890
want a smaller footprint for the redfish

402
00:29:39,890 --> 00:29:44,180
engine mmm I think we can consider a

403
00:29:44,180 --> 00:29:47,150
better web server

404
00:29:47,150 --> 00:29:53,270
I don't entity you know the Python will

405
00:29:53,270 --> 00:29:58,940
consume memory more memory than C or C++

406
00:29:58,940 --> 00:30:04,100
code yeah and also for the outdated

407
00:30:04,100 --> 00:30:07,400
schema passing because currently the

408
00:30:07,400 --> 00:30:10,240
redfish schema bundle is very large so

409
00:30:10,240 --> 00:30:15,770
when we parsing the XML file you need

410
00:30:15,770 --> 00:30:18,530
more mount memory but we can disable

411
00:30:18,530 --> 00:30:23,440
some functions like we don't need the

412
00:30:23,440 --> 00:30:26,740
description long description notation

413
00:30:26,740 --> 00:30:32,020
that will save a lot of memory I think

414
00:30:35,670 --> 00:30:41,650
the the most saying I am interesting is

415
00:30:41,650 --> 00:30:46,750
the PRD m4r de this is the new

416
00:30:46,750 --> 00:30:50,590
specification from the MTF is just

417
00:30:50,590 --> 00:30:53,620
published couples a couple months ago

418
00:30:53,620 --> 00:30:57,700
but as you know this standard is very

419
00:30:57,700 --> 00:31:01,480
important to support the aisle

420
00:31:01,480 --> 00:31:05,700
management it actually provides the

421
00:31:05,700 --> 00:31:08,920
renovation management capability on IO

422
00:31:08,920 --> 00:31:12,940
device the PCI device so basically the

423
00:31:12,940 --> 00:31:18,250
BMC act as a proxy between the redfish

424
00:31:18,250 --> 00:31:23,290
client and i/o device so what what we

425
00:31:23,290 --> 00:31:27,190
need to do is to translate the redfish

426
00:31:27,190 --> 00:31:31,030
message and operation to clear their

427
00:31:31,030 --> 00:31:34,750
message operations two things we need to

428
00:31:34,750 --> 00:31:38,860
do the wine is the binary JSON encoding

429
00:31:38,860 --> 00:31:41,830
decoding because the option card does

430
00:31:41,830 --> 00:31:44,290
not accept JSON text

431
00:31:44,290 --> 00:31:49,690
they only accept binary so we we want

432
00:31:49,690 --> 00:31:52,000
the MTF to to have this kind of tour

433
00:31:52,000 --> 00:31:57,330
organ sauce and also we need the schema

434
00:31:57,330 --> 00:32:03,700
dictionary discovery and we also need

435
00:32:03,700 --> 00:32:06,640
the PR DM library to support the new PID

436
00:32:06,640 --> 00:32:14,470
M type for a de next we we want to add

437
00:32:14,470 --> 00:32:17,380
the bus interface apart you know data

438
00:32:17,380 --> 00:32:21,130
interface layer it can be used for a lot

439
00:32:21,130 --> 00:32:25,530
of data source like telemetry cache

440
00:32:25,530 --> 00:32:34,050
refreshing avenging currently we we have

441
00:32:34,050 --> 00:32:39,310
simulator opened on the github Lenovo

442
00:32:39,310 --> 00:32:44,640
repository is part of the snapper code

443
00:32:44,640 --> 00:32:45,910
it

444
00:32:45,910 --> 00:32:49,480
but it almost cover most the engine

445
00:32:49,480 --> 00:32:54,370
particle you can run on your laptop or

446
00:32:54,370 --> 00:32:58,060
if you want to run inside BMC yes you

447
00:32:58,060 --> 00:33:07,030
can change the make fair or to do it and

448
00:33:07,030 --> 00:33:11,170
I also want to have a quick introduction

449
00:33:11,170 --> 00:33:16,090
under few slides this is the auto

450
00:33:16,090 --> 00:33:21,090
generation class the left is the

451
00:33:21,090 --> 00:33:24,520
resource classifier the right top right

452
00:33:24,520 --> 00:33:28,540
is the registry top water bottom right

453
00:33:28,540 --> 00:33:32,260
is the privilege fair so for the schema

454
00:33:32,260 --> 00:33:36,630
fair you can see the it cleared most

455
00:33:36,630 --> 00:33:39,400
static information from the schema

456
00:33:39,400 --> 00:33:42,180
including the property and the property

457
00:33:42,180 --> 00:33:48,730
attribution and action information om OM

458
00:33:48,730 --> 00:33:56,100
property and also the the attribution

459
00:33:56,100 --> 00:33:59,740
under the annotation all kinds of

460
00:33:59,740 --> 00:34:08,530
annotation on the properties and on

461
00:34:08,530 --> 00:34:11,440
further registry you can directly use

462
00:34:11,440 --> 00:34:18,449
the function to add the message into the

463
00:34:18,449 --> 00:34:22,139
rad official response

464
00:34:25,730 --> 00:34:29,599
this is how snapper generator resource

465
00:34:29,599 --> 00:34:34,940
tree on the left is the recursion to

466
00:34:34,940 --> 00:34:39,918
build the topology is that from salutes

467
00:34:39,918 --> 00:34:43,480
and transfers are the properties and

468
00:34:43,480 --> 00:34:49,369
expand them and also called get member

469
00:34:49,369 --> 00:34:51,739
function CAD member ID access a key of

470
00:34:51,739 --> 00:34:56,299
the child node finally it generated the

471
00:34:56,299 --> 00:34:59,420
URI and assess the key table mapping

472
00:34:59,420 --> 00:35:04,700
table so take a look at the PCI device

473
00:35:04,700 --> 00:35:08,990
of computer system how do we determine

474
00:35:08,990 --> 00:35:12,230
how to reach that further value for this

475
00:35:12,230 --> 00:35:13,000
property

476
00:35:13,000 --> 00:35:19,099
it's just invoke the function for PCI

477
00:35:19,099 --> 00:35:25,030
device and the PCI device class will

478
00:35:25,030 --> 00:35:30,369
return the table of ID and access a key

479
00:35:30,369 --> 00:35:34,549
so based on the table based on the ID

480
00:35:34,549 --> 00:35:42,549
and accessor key you can search the URI

481
00:35:42,549 --> 00:35:47,020
by the information inside the access key

482
00:35:47,020 --> 00:35:51,819
then you call the data for PCI device

483
00:35:53,950 --> 00:36:00,020
this is our UI runtime binding we don't

484
00:36:00,020 --> 00:36:02,900
have called the URI but for each segment

485
00:36:02,900 --> 00:36:06,920
in the UI we will plan it to class like

486
00:36:06,920 --> 00:36:10,430
Reggie facially when we burn with the

487
00:36:10,430 --> 00:36:14,000
binding is south load and the chassis is

488
00:36:14,000 --> 00:36:18,890
so it's chessick collection and chassis

489
00:36:18,890 --> 00:36:23,900
ID 1 means chassis class and key segment

490
00:36:23,900 --> 00:36:30,190
1 and power 4 per class

491
00:36:32,350 --> 00:36:36,250
this is all yummy extension the family

492
00:36:36,250 --> 00:36:40,360
this is what the schema implementation

493
00:36:40,360 --> 00:36:46,320
looks like is for the error interface

494
00:36:46,320 --> 00:36:50,650
look from this handle gate function you

495
00:36:50,650 --> 00:36:54,040
just need to fill in a few dynamic

496
00:36:54,040 --> 00:36:58,230
dynamic properties probably dynamic data

497
00:36:58,230 --> 00:37:03,400
for the rest data in the response you

498
00:37:03,400 --> 00:37:08,260
don't need to care and right in the red

499
00:37:08,260 --> 00:37:17,080
side is the patch operation okay I think

500
00:37:17,080 --> 00:37:19,560
that's all

501
00:37:24,589 --> 00:37:27,920
any questions

502
00:37:38,450 --> 00:37:41,280
if I wanted to try out snapper is all

503
00:37:41,280 --> 00:37:44,849
the code out on Lenovo / snapper yeah

504
00:37:44,849 --> 00:37:47,810
that's correct

505
00:37:51,710 --> 00:37:54,690
so do you have any actual performance

506
00:37:54,690 --> 00:37:56,550
numbers comparing your implementation

507
00:37:56,550 --> 00:38:03,150
GBMC was CPU usage that kind of thing

508
00:38:03,150 --> 00:38:07,500
yeah I have since I implement the the

509
00:38:07,500 --> 00:38:10,710
lonoa implement the snapper work on

510
00:38:10,710 --> 00:38:15,150
stack including the engine and specific

511
00:38:15,150 --> 00:38:21,720
schema we implement the 27 to 33 so part

512
00:38:21,720 --> 00:38:24,740
of the data provider code are the no

513
00:38:24,740 --> 00:38:28,530
private code so this parameter R or

514
00:38:28,530 --> 00:38:34,320
based on the for code stack so if you

515
00:38:34,320 --> 00:38:38,760
look at the fair size the engines has in

516
00:38:38,760 --> 00:38:45,599
is about 400 400 kilobytes and the date

517
00:38:45,599 --> 00:38:48,960
of data interface is a large is about 1

518
00:38:48,960 --> 00:38:53,160
megabyte because we have the all

519
00:38:53,160 --> 00:38:57,630
underlying code including the database

520
00:38:57,630 --> 00:39:00,869
database access our camera access mcdv

521
00:39:00,869 --> 00:39:04,950
idea make sense they are all in there so

522
00:39:04,950 --> 00:39:09,420
this dating phase is is a large and we

523
00:39:09,420 --> 00:39:13,079
also have the snapper provider which

524
00:39:13,079 --> 00:39:16,319
implement the order dynamic data of

525
00:39:16,319 --> 00:39:29,880
skimmers it's three methods when you are

526
00:39:29,880 --> 00:39:33,569
using the snapper engine you need to

527
00:39:33,569 --> 00:39:37,950
replace there you need to implement your

528
00:39:37,950 --> 00:39:42,420
own provider and coding on your own data

529
00:39:42,420 --> 00:39:46,310
interface layer maybe you can use your

530
00:39:46,310 --> 00:39:49,560
user debug d-bus

531
00:39:49,560 --> 00:39:51,780
you can use your own database in the

532
00:39:51,780 --> 00:39:55,260
data interface functions and the

533
00:39:55,260 --> 00:40:00,300
profiles so that the data interface

534
00:40:00,300 --> 00:40:03,530
library in your system depends on your

535
00:40:03,530 --> 00:40:07,410
data Java implementation and also the

536
00:40:07,410 --> 00:40:11,880
test provider residens about the memory

537
00:40:11,880 --> 00:40:17,730
footprint the junior come I just

538
00:40:17,730 --> 00:40:21,000
mentioned the Python will consume large

539
00:40:21,000 --> 00:40:27,210
memory so much income is 10 Macbeth in

540
00:40:27,210 --> 00:40:30,750
the 10 map at 2 is Unicom is itself the

541
00:40:30,750 --> 00:40:35,370
other 8 Macbeth is flask frost module

542
00:40:35,370 --> 00:40:41,480
the web framework and currently with the

543
00:40:41,480 --> 00:40:47,220
radish schema bonded to 28 industry the

544
00:40:47,220 --> 00:40:51,660
passing result will occupy 8 Macbeth but

545
00:40:51,660 --> 00:40:55,860
if we reduce the unnecessary a notation

546
00:40:55,860 --> 00:41:00,000
can be smaller and the photos snapper

547
00:41:00,000 --> 00:41:04,560
engine and for provider because we have

548
00:41:04,560 --> 00:41:08,640
a for stack is a 16 Macbeth but if you

549
00:41:08,640 --> 00:41:11,730
want to check the rear side of a pure

550
00:41:11,730 --> 00:41:14,880
ratify a snapper engine you can use the

551
00:41:14,880 --> 00:41:21,870
simulator and remember stupid affair in

552
00:41:21,870 --> 00:41:24,630
current open source I haven't stupid

553
00:41:24,630 --> 00:41:47,870
affair yet this is the we are a yeah

554
00:41:51,160 --> 00:41:54,339
I walked in late sorry from repeating or

555
00:41:54,339 --> 00:41:55,299
asking questions you already answered

556
00:41:55,299 --> 00:41:58,720
this uh did you say this was a this used

557
00:41:58,720 --> 00:42:00,039
to be a private code base and now you're

558
00:42:00,039 --> 00:42:02,400
open sourcing it is that what happened

559
00:42:02,400 --> 00:42:07,989
the engine code he'd already opened but

560
00:42:07,989 --> 00:42:10,359
did it start off as a proprietary code

561
00:42:10,359 --> 00:42:13,769
base and now you're donating it or

562
00:42:13,769 --> 00:42:15,910
developing this code stack in the open

563
00:42:15,910 --> 00:42:19,410
since the beginning the only pro

564
00:42:19,410 --> 00:42:22,299
probably coda is our provider

565
00:42:22,299 --> 00:42:25,479
implantation the whole engine is already

566
00:42:25,479 --> 00:42:27,910
opened I understand I understand it's

567
00:42:27,910 --> 00:42:31,420
open now did it start open yeah we just

568
00:42:31,420 --> 00:42:34,599
open it a couple of weeks ago because

569
00:42:34,599 --> 00:42:39,700
you know we need to go through the open

570
00:42:39,700 --> 00:42:41,670
source really process of our company

571
00:42:41,670 --> 00:42:46,839
yeah so we open it about a couple of

572
00:42:46,839 --> 00:43:00,279
weeks ago right okay and how long how

573
00:43:00,279 --> 00:43:03,339
long have you as a been in-house can you

574
00:43:03,339 --> 00:43:07,779
say we have this idea since the

575
00:43:07,779 --> 00:43:10,660
beginning of last year okay was there

576
00:43:10,660 --> 00:43:11,739
like was there ever a point in you kind

577
00:43:11,739 --> 00:43:13,059
of your decision-making process where

578
00:43:13,059 --> 00:43:15,519
you're looking at BMC web and and you

579
00:43:15,519 --> 00:43:19,680
decided no we're gonna really do this

580
00:43:19,680 --> 00:43:23,829
not exactly since we are looking at EMC

581
00:43:23,829 --> 00:43:28,089
app because we have is actually

582
00:43:28,089 --> 00:43:33,489
happening in our x86 server we have some

583
00:43:33,489 --> 00:43:38,019
problem when the real fish when the TMT

584
00:43:38,019 --> 00:43:42,700
of Rhenish from forum upgrading the

585
00:43:42,700 --> 00:43:46,329
design just Bank the schema we have some

586
00:43:46,329 --> 00:43:51,549
pain point like the schema updates the

587
00:43:51,549 --> 00:43:55,269
property tribution UNAM sometimes we

588
00:43:55,269 --> 00:44:02,019
have to redesign our real fish or to

589
00:44:02,019 --> 00:44:05,049
some repeated work so we want

590
00:44:05,049 --> 00:44:09,009
such a more efficient engine to help

591
00:44:09,009 --> 00:44:13,659
accelerate our schema development yeah

592
00:44:13,659 --> 00:44:20,159
the comparison I just did this operation

593
00:44:20,159 --> 00:44:25,419
recently before that I don't I don't in

594
00:44:25,419 --> 00:44:27,789
last year I haven't look into the BMC

595
00:44:27,789 --> 00:44:31,479
web one more question so like there are

596
00:44:31,479 --> 00:44:33,459
a number of comparison points there it's

597
00:44:33,459 --> 00:44:36,309
a good chart like why would would it is

598
00:44:36,309 --> 00:44:40,630
it possible to get these things that are

599
00:44:40,630 --> 00:44:42,640
missing from BMC web into BMC what

600
00:44:42,640 --> 00:44:44,919
Brasilia kind of opinion that it's sort

601
00:44:44,919 --> 00:44:50,439
of a real ask I think the snapper can be

602
00:44:50,439 --> 00:44:54,279
compared at the library and his web

603
00:44:54,279 --> 00:44:58,059
server can either replace both so if we

604
00:44:58,059 --> 00:45:00,069
have a better choice of web server and

605
00:45:00,069 --> 00:45:02,769
application server we can replace the

606
00:45:02,769 --> 00:45:05,469
end ganks and Unicom yes I think it's

607
00:45:05,469 --> 00:45:07,689
possible to integrate the snapper with

608
00:45:07,689 --> 00:45:15,239
BMC web ok thank you

609
00:45:19,010 --> 00:45:26,510
your questions okay thank you

610
00:45:27,360 --> 00:45:29,660
[Applause]

611
00:45:29,660 --> 00:45:36,810
[Music]

