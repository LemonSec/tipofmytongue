1
00:00:10,559 --> 00:00:15,610
<font color="#CCCCCC">thank you Roman</font>

2
00:00:13,050 --> 00:00:18,530
[Applause]

3
00:00:15,610 --> 00:00:21,410
well good afternoon<font color="#E5E5E5"> everyone and thank</font>

4
00:00:18,530 --> 00:00:24,529
you<font color="#CCCCCC"> for sticking around late on the</font>

5
00:00:21,410 --> 00:00:26,000
second<font color="#E5E5E5"> day of the conference it's my</font>

6
00:00:24,529 --> 00:00:29,210
pleasure to talk to<font color="#CCCCCC"> you</font><font color="#E5E5E5"> about deep</font>

7
00:00:26,000 --> 00:00:32,900
learning<font color="#E5E5E5"> in the context of parallel</font>

8
00:00:29,210 --> 00:00:35,720
databases<font color="#CCCCCC"> all</font><font color="#E5E5E5"> of the software</font><font color="#CCCCCC"> that's</font>

9
00:00:32,900 --> 00:00:38,120
used<font color="#E5E5E5"> in this project</font><font color="#CCCCCC"> that we did</font><font color="#E5E5E5"> is open</font>

10
00:00:35,720 --> 00:00:41,570
<font color="#E5E5E5">source software</font><font color="#CCCCCC"> of some of which is</font>

11
00:00:38,120 --> 00:00:43,280
<font color="#E5E5E5">listed here</font><font color="#CCCCCC"> so in the spirit of FOSDEM</font>

12
00:00:41,570 --> 00:00:47,780
<font color="#E5E5E5">it's definitely</font><font color="#CCCCCC"> something that</font><font color="#E5E5E5"> you can</font>

13
00:00:43,280 --> 00:00:51,470
try at home so I<font color="#E5E5E5"> thought I'd start</font><font color="#CCCCCC"> by</font>

14
00:00:47,780 --> 00:00:54,080
giving a very<font color="#CCCCCC"> brief</font><font color="#E5E5E5"> introduction to deep</font>

15
00:00:51,470 --> 00:00:56,269
learning<font color="#E5E5E5"> perhaps for folks who are less</font>

16
00:00:54,080 --> 00:00:58,339
familiar<font color="#CCCCCC"> with this</font><font color="#E5E5E5"> topic</font><font color="#CCCCCC"> although you</font>

17
00:00:56,269 --> 00:01:02,600
may<font color="#E5E5E5"> have heard a lot about it in the</font>

18
00:00:58,339 --> 00:01:05,900
context of AI and the popular press so I

19
00:01:02,600 --> 00:01:08,780
kind<font color="#CCCCCC"> of</font><font color="#E5E5E5"> favor this Russian doll kind of</font>

20
00:01:05,900 --> 00:01:10,880
description of<font color="#CCCCCC"> the AI landscape</font><font color="#E5E5E5"> which</font>

21
00:01:08,780 --> 00:01:14,659
has artificial intelligence machine

22
00:01:10,880 --> 00:01:19,280
<font color="#CCCCCC">learning and deep learning embedded kind</font>

23
00:01:14,659 --> 00:01:22,310
of within<font color="#E5E5E5"> that so deep learning is a</font>

24
00:01:19,280 --> 00:01:26,000
kind of<font color="#CCCCCC"> machine learning it's inspired</font>

25
00:01:22,310 --> 00:01:30,009
by<font color="#CCCCCC"> biology of the brain and it uses a</font>

26
00:01:26,000 --> 00:01:30,009
class of<font color="#E5E5E5"> algorithms which</font><font color="#CCCCCC"> are called</font>

27
00:01:30,189 --> 00:01:36,648
which uses a class of<font color="#E5E5E5"> algorithms can you</font>

28
00:01:32,810 --> 00:01:41,500
hear me<font color="#E5E5E5"> at the top</font><font color="#CCCCCC"> ok no ok I'm</font><font color="#E5E5E5"> going to</font>

29
00:01:36,649 --> 00:01:45,939
shout louder<font color="#CCCCCC"> so deep learning</font><font color="#E5E5E5"> is</font>

30
00:01:41,500 --> 00:01:48,200
inspired<font color="#E5E5E5"> by biology</font><font color="#CCCCCC"> of the brain it is</font>

31
00:01:45,939 --> 00:01:51,758
<font color="#CCCCCC">uses a class of</font><font color="#E5E5E5"> algorithms called</font>

32
00:01:48,200 --> 00:01:51,759
<font color="#E5E5E5">artificial neural networks and</font>

33
00:01:52,539 --> 00:01:56,840
<font color="#E5E5E5">artificial neural networks</font><font color="#CCCCCC"> kind</font><font color="#E5E5E5"> of came</font>

34
00:01:55,310 --> 00:01:58,609
from<font color="#E5E5E5"> this brain biology from</font>

35
00:01:56,840 --> 00:02:00,259
neuroscience<font color="#E5E5E5"> but I don't think</font>

36
00:01:58,609 --> 00:02:03,199
neuroscience really drives the

37
00:02:00,259 --> 00:02:05,749
innovation<font color="#CCCCCC"> of artificial intelligence</font>

38
00:02:03,200 --> 00:02:07,520
networks<font color="#E5E5E5"> today and they've been</font>

39
00:02:05,749 --> 00:02:09,799
certainly growing<font color="#E5E5E5"> in size there's</font>

40
00:02:07,520 --> 00:02:13,069
networks with hundreds<font color="#E5E5E5"> of millions of</font>

41
00:02:09,800 --> 00:02:15,040
weights right<font color="#E5E5E5"> now but I read a couple</font><font color="#CCCCCC"> as</font>

42
00:02:13,069 --> 00:02:17,720
<font color="#E5E5E5">of like</font><font color="#CCCCCC"> a year</font><font color="#E5E5E5"> or two ago the largest</font>

43
00:02:15,040 --> 00:02:21,140
<font color="#E5E5E5">artificial neural networks were roughly</font>

44
00:02:17,720 --> 00:02:23,359
<font color="#E5E5E5">the size of the nervous</font><font color="#CCCCCC"> system of an</font>

45
00:02:21,140 --> 00:02:26,929
insect<font color="#CCCCCC"> so although</font><font color="#E5E5E5"> they're large in</font>

46
00:02:23,360 --> 00:02:27,390
computer terms in terms of<font color="#E5E5E5"> biology and</font>

47
00:02:26,930 --> 00:02:29,910
other

48
00:02:27,390 --> 00:02:32,940
organisms<font color="#E5E5E5"> and systems</font><font color="#CCCCCC"> they're not</font>

49
00:02:29,910 --> 00:02:36,240
<font color="#E5E5E5">actually that big these are some</font>

50
00:02:32,940 --> 00:02:39,660
<font color="#E5E5E5">examples of deep learning algorithms can</font>

51
00:02:36,240 --> 00:02:42,030
you hear me better<font color="#CCCCCC"> at the top</font><font color="#E5E5E5"> now yes</font>

52
00:02:39,660 --> 00:02:44,310
<font color="#E5E5E5">sort of okay</font>

53
00:02:42,030 --> 00:02:46,590
these are<font color="#E5E5E5"> examples of some deep learning</font>

54
00:02:44,310 --> 00:02:48,060
algorithms<font color="#E5E5E5"> for</font><font color="#CCCCCC"> example on the</font><font color="#E5E5E5"> left of</font>

55
00:02:46,590 --> 00:02:51,060
this<font color="#E5E5E5"> slide we</font><font color="#CCCCCC"> have a multi-layer</font>

56
00:02:48,060 --> 00:02:54,000
perceptron so this is a classic neural

57
00:02:51,060 --> 00:02:56,370
net<font color="#CCCCCC"> which</font><font color="#E5E5E5"> is fully connected</font><font color="#CCCCCC"> and</font><font color="#E5E5E5"> what</font>

58
00:02:54,000 --> 00:02:58,709
that means<font color="#E5E5E5"> is every neuron from one</font>

59
00:02:56,370 --> 00:03:02,070
layer is connected<font color="#E5E5E5"> to every other neuron</font>

60
00:02:58,709 --> 00:03:05,160
from<font color="#E5E5E5"> another layer</font><font color="#CCCCCC"> so these are ler</font>

61
00:03:02,070 --> 00:03:09,359
these are used for<font color="#CCCCCC"> I would say</font><font color="#E5E5E5"> kind of</font>

62
00:03:05,160 --> 00:03:13,489
simple<font color="#E5E5E5"> to moderately complex problems in</font>

63
00:03:09,360 --> 00:03:17,370
<font color="#CCCCCC">the</font><font color="#E5E5E5"> area of like machine translation and</font>

64
00:03:13,489 --> 00:03:19,769
<font color="#E5E5E5">fraud detection and such but there's</font>

65
00:03:17,370 --> 00:03:21,989
other more specialized networks for

66
00:03:19,769 --> 00:03:23,519
example recurrent neural networks in the

67
00:03:21,989 --> 00:03:25,980
middle here recurrent neural networks

68
00:03:23,519 --> 00:03:27,830
are used<font color="#E5E5E5"> on sequential data like we</font>

69
00:03:25,980 --> 00:03:31,530
<font color="#CCCCCC">heard in them in the machine translation</font>

70
00:03:27,830 --> 00:03:32,930
<font color="#E5E5E5">presentation</font><font color="#CCCCCC"> just a few moments ago I'm</font>

71
00:03:31,530 --> 00:03:36,290
going to talk a bit<font color="#E5E5E5"> more about</font>

72
00:03:32,930 --> 00:03:38,489
<font color="#CCCCCC">convolutional neural networks</font><font color="#E5E5E5"> so</font>

73
00:03:36,290 --> 00:03:41,820
convolutional neural networks<font color="#E5E5E5"> these are</font>

74
00:03:38,489 --> 00:03:43,170
very effective<font color="#CCCCCC"> for</font><font color="#E5E5E5"> computer vision so</font>

75
00:03:41,820 --> 00:03:46,890
you can see on the right side<font color="#CCCCCC"> of this</font>

76
00:03:43,170 --> 00:03:49,530
<font color="#CCCCCC">slide this is an image from an</font>

77
00:03:46,890 --> 00:03:51,869
autonomous vehicle<font color="#E5E5E5"> and what this vehicle</font>

78
00:03:49,530 --> 00:03:54,540
<font color="#CCCCCC">is able</font><font color="#E5E5E5"> to do using</font><font color="#CCCCCC"> these convolutional</font>

79
00:03:51,870 --> 00:03:58,350
neural networks is to<font color="#CCCCCC"> identify objects</font>

80
00:03:54,540 --> 00:04:00,870
in the scene<font color="#E5E5E5"> and to rank them to</font>

81
00:03:58,350 --> 00:04:02,609
<font color="#E5E5E5">classify them with a probability so</font>

82
00:04:00,870 --> 00:04:06,060
that's a vehicle<font color="#CCCCCC"> with a certain</font>

83
00:04:02,610 --> 00:04:08,160
probability<font color="#CCCCCC"> these</font><font color="#E5E5E5"> modern CNN's can</font>

84
00:04:06,060 --> 00:04:11,810
identify<font color="#E5E5E5"> hundreds of objects in the</font>

85
00:04:08,160 --> 00:04:16,560
scene of different types<font color="#E5E5E5"> stoplights</font>

86
00:04:11,810 --> 00:04:18,690
<font color="#E5E5E5">pedestrians people</font><font color="#CCCCCC"> on bicycles</font><font color="#E5E5E5"> etc and</font>

87
00:04:16,560 --> 00:04:21,630
<font color="#E5E5E5">they're really they work really well</font>

88
00:04:18,690 --> 00:04:25,200
<font color="#E5E5E5">because they use a lot fewer connections</font>

89
00:04:21,630 --> 00:04:27,659
<font color="#E5E5E5">between the networks by virtue of using</font>

90
00:04:25,200 --> 00:04:30,440
convolutions or<font color="#CCCCCC"> terms these filters that</font>

91
00:04:27,660 --> 00:04:32,640
move through<font color="#CCCCCC"> thee through</font><font color="#E5E5E5"> the image</font>

92
00:04:30,440 --> 00:04:35,310
<font color="#CCCCCC">another interesting thing</font><font color="#E5E5E5"> about these</font>

93
00:04:32,640 --> 00:04:36,960
networks is<font color="#E5E5E5"> that they have translational</font>

94
00:04:35,310 --> 00:04:40,200
invariance which<font color="#E5E5E5"> what</font><font color="#CCCCCC"> that means is that</font>

95
00:04:36,960 --> 00:04:42,479
it's hard to<font color="#E5E5E5"> fool them so if you move</font>

96
00:04:40,200 --> 00:04:45,150
image to a different<font color="#E5E5E5"> moving object to a</font>

97
00:04:42,480 --> 00:04:47,190
different place in<font color="#CCCCCC"> the image because the</font>

98
00:04:45,150 --> 00:04:49,289
convolution filter moves for that image

99
00:04:47,190 --> 00:04:53,219
even though different<font color="#E5E5E5"> pixels are laid</font><font color="#CCCCCC"> up</font>

100
00:04:49,290 --> 00:04:55,680
in<font color="#E5E5E5"> the image it can still sit say that's</font>

101
00:04:53,220 --> 00:04:57,330
a cat there or there that's a bicycle

102
00:04:55,680 --> 00:04:59,730
there are there even<font color="#CCCCCC"> though it's in a</font>

103
00:04:57,330 --> 00:05:01,979
<font color="#E5E5E5">different part of the image and here's a</font>

104
00:04:59,730 --> 00:05:05,460
<font color="#CCCCCC">couple</font><font color="#E5E5E5"> examples of classic networks that</font>

105
00:05:01,980 --> 00:05:10,200
that<font color="#E5E5E5"> people use we've heard about GPUs</font>

106
00:05:05,460 --> 00:05:12,210
this afternoon<font color="#E5E5E5"> neural</font><font color="#CCCCCC"> networks are often</font>

107
00:05:10,200 --> 00:05:15,060
<font color="#E5E5E5">they sort</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> devolve</font><font color="#CCCCCC"> to doing tons and</font>

108
00:05:12,210 --> 00:05:17,370
tons of matrix operations<font color="#E5E5E5"> and graphic</font>

109
00:05:15,060 --> 00:05:19,140
<font color="#E5E5E5">processing units are well suited to</font>

110
00:05:17,370 --> 00:05:20,940
<font color="#E5E5E5">these algorithms because they do tons</font>

111
00:05:19,140 --> 00:05:25,680
and tons of matrix operations in

112
00:05:20,940 --> 00:05:27,360
<font color="#CCCCCC">parallel and very quickly so this</font><font color="#E5E5E5"> is</font>

113
00:05:25,680 --> 00:05:29,880
what I<font color="#CCCCCC"> would say is like</font><font color="#E5E5E5"> standard</font>

114
00:05:27,360 --> 00:05:33,020
infrastructure for running deep neural

115
00:05:29,880 --> 00:05:38,250
networks<font color="#E5E5E5"> you have</font><font color="#CCCCCC"> often a single server</font>

116
00:05:33,020 --> 00:05:40,020
and one or more GPUs and this works<font color="#CCCCCC"> very</font>

117
00:05:38,250 --> 00:05:42,060
well<font color="#CCCCCC"> you can stream data through these</font>

118
00:05:40,020 --> 00:05:43,710
things and solve big problems<font color="#CCCCCC"> but sort</font>

119
00:05:42,060 --> 00:05:46,740
<font color="#CCCCCC">of keep this picture in mind as we talk</font>

120
00:05:43,710 --> 00:05:50,219
about<font color="#E5E5E5"> parallel databases so there's a</font>

121
00:05:46,740 --> 00:05:51,630
particular<font color="#E5E5E5"> parallel database that that I</font>

122
00:05:50,220 --> 00:05:55,530
work on it's an open source<font color="#E5E5E5"> project</font>

123
00:05:51,630 --> 00:05:59,330
<font color="#CCCCCC">called greenplum</font><font color="#E5E5E5"> database and it looks</font>

124
00:05:55,530 --> 00:06:02,039
<font color="#CCCCCC">like this</font><font color="#E5E5E5"> so a parallel database like</font>

125
00:05:59,330 --> 00:06:05,250
green plum and there are many of<font color="#E5E5E5"> them</font>

126
00:06:02,040 --> 00:06:08,190
out<font color="#E5E5E5"> there they're characterized by kind</font>

127
00:06:05,250 --> 00:06:10,170
of shared nothing architecture<font color="#CCCCCC"> so it's</font>

128
00:06:08,190 --> 00:06:13,260
sequel based you have sequel<font color="#E5E5E5"> that comes</font>

129
00:06:10,170 --> 00:06:15,630
<font color="#E5E5E5">in to the master node and</font><font color="#CCCCCC"> that sequel</font>

130
00:06:13,260 --> 00:06:19,050
workload<font color="#CCCCCC"> gets distributed to</font><font color="#E5E5E5"> the worker</font>

131
00:06:15,630 --> 00:06:22,500
nodes<font color="#E5E5E5"> and in green plum lingo the worker</font>

132
00:06:19,050 --> 00:06:25,080
nodes are<font color="#CCCCCC"> called segments</font><font color="#E5E5E5"> so you can</font>

133
00:06:22,500 --> 00:06:27,240
think<font color="#CCCCCC"> of these</font><font color="#E5E5E5"> as having post grass</font>

134
00:06:25,080 --> 00:06:29,909
databases running<font color="#E5E5E5"> on all of these worker</font>

135
00:06:27,240 --> 00:06:34,080
nodes<font color="#CCCCCC"> and having the workload</font>

136
00:06:29,910 --> 00:06:37,440
distributed to them<font color="#CCCCCC"> by virtue</font><font color="#E5E5E5"> of a</font>

137
00:06:34,080 --> 00:06:39,270
pretty<font color="#E5E5E5"> advanced query planner then those</font>

138
00:06:37,440 --> 00:06:42,630
results are brought back<font color="#CCCCCC"> to the master</font>

139
00:06:39,270 --> 00:06:45,090
<font color="#E5E5E5">and that's the result set so what we</font>

140
00:06:42,630 --> 00:06:47,159
would<font color="#CCCCCC"> like to</font><font color="#E5E5E5"> do is add deep learning to</font>

141
00:06:45,090 --> 00:06:49,919
that architecture<font color="#E5E5E5"> so this is the</font>

142
00:06:47,160 --> 00:06:53,280
approach that<font color="#E5E5E5"> we've we've taken we want</font>

143
00:06:49,920 --> 00:06:57,270
to be able<font color="#E5E5E5"> to attach worker nodes GP</font>

144
00:06:53,280 --> 00:06:58,859
<font color="#CCCCCC">Jews</font><font color="#E5E5E5"> to these segments</font><font color="#CCCCCC"> to take</font><font color="#E5E5E5"> advantage</font>

145
00:06:57,270 --> 00:07:01,770
of<font color="#E5E5E5"> all the</font><font color="#CCCCCC"> great things that GPUs can do</font>

146
00:06:58,860 --> 00:07:04,919
and we also want<font color="#E5E5E5"> to run deep learning</font>

147
00:07:01,770 --> 00:07:07,130
<font color="#E5E5E5">libraries on the</font><font color="#CCCCCC"> worker nodes</font><font color="#E5E5E5"> so where</font>

148
00:07:04,919 --> 00:07:11,430
we feel we can innovate<font color="#E5E5E5"> is not</font>

149
00:07:07,130 --> 00:07:13,260
necessarily<font color="#E5E5E5"> in you know changing those</font>

150
00:07:11,430 --> 00:07:14,580
libraries<font color="#CCCCCC"> themselves because there's so</font>

151
00:07:13,260 --> 00:07:16,500
much rapid innovation happening

152
00:07:14,580 --> 00:07:18,090
happening in those<font color="#E5E5E5"> libraries so we want</font>

153
00:07:16,500 --> 00:07:21,330
to run<font color="#E5E5E5"> those as is on the worker nodes</font>

154
00:07:18,090 --> 00:07:24,989
and marshal the data and return<font color="#CCCCCC"> the</font>

155
00:07:21,330 --> 00:07:28,010
results<font color="#CCCCCC"> using</font><font color="#E5E5E5"> Apache MATLAB which is an</font>

156
00:07:24,990 --> 00:07:32,340
open source in database learning project

157
00:07:28,010 --> 00:07:37,349
<font color="#E5E5E5">so if you sort of compare this somewhat</font>

158
00:07:32,340 --> 00:07:40,080
lonely architecture to this kind of

159
00:07:37,350 --> 00:07:41,780
<font color="#E5E5E5">distributed system then you kind of get</font>

160
00:07:40,080 --> 00:07:44,760
the idea<font color="#CCCCCC"> that yeah there's actually</font>

161
00:07:41,780 --> 00:07:47,760
possibility to do more computation in

162
00:07:44,760 --> 00:07:50,430
parallel and<font color="#E5E5E5"> be more efficient very</font>

163
00:07:47,760 --> 00:07:52,830
<font color="#E5E5E5">quickly</font><font color="#CCCCCC"> word</font><font color="#E5E5E5"> about</font><font color="#CCCCCC"> patchy</font><font color="#E5E5E5"> MATLAB is in</font>

164
00:07:50,430 --> 00:07:54,060
database machine<font color="#CCCCCC"> learning</font><font color="#E5E5E5"> the idea</font><font color="#CCCCCC"> is</font>

165
00:07:52,830 --> 00:07:56,430
that<font color="#E5E5E5"> you don't want to</font><font color="#CCCCCC"> move</font><font color="#E5E5E5"> that data</font>

166
00:07:54,060 --> 00:07:59,880
<font color="#CCCCCC">out of the database to</font><font color="#E5E5E5"> another place</font>

167
00:07:56,430 --> 00:08:03,540
<font color="#E5E5E5">operate on it and move it</font><font color="#CCCCCC"> back so what</font>

168
00:07:59,880 --> 00:08:05,669
<font color="#E5E5E5">Apache</font><font color="#CCCCCC"> Madlib does is it</font><font color="#E5E5E5"> brings machine</font>

169
00:08:03,540 --> 00:08:09,660
<font color="#E5E5E5">learning statistics computation to where</font>

170
00:08:05,669 --> 00:08:11,430
the<font color="#E5E5E5"> the data live so you might ask</font><font color="#CCCCCC"> okay</font>

171
00:08:09,660 --> 00:08:13,260
like<font color="#E5E5E5"> what is the kind of distributed</font>

172
00:08:11,430 --> 00:08:16,860
machine learning you can do in a in<font color="#E5E5E5"> a</font>

173
00:08:13,260 --> 00:08:18,659
cluster<font color="#CCCCCC"> and there's different options</font>

174
00:08:16,860 --> 00:08:22,169
<font color="#CCCCCC">there</font><font color="#E5E5E5"> different ways</font><font color="#CCCCCC"> that you can go</font>

175
00:08:18,660 --> 00:08:24,060
about about<font color="#E5E5E5"> doing it I'm going to talk</font>

176
00:08:22,169 --> 00:08:27,960
about<font color="#CCCCCC"> the first one primarily</font><font color="#E5E5E5"> in this</font>

177
00:08:24,060 --> 00:08:30,810
talk which<font color="#CCCCCC"> is running</font><font color="#E5E5E5"> a single big model</font>

178
00:08:27,960 --> 00:08:33,569
across<font color="#CCCCCC"> a</font><font color="#E5E5E5"> cluster so what that means is</font>

179
00:08:30,810 --> 00:08:36,150
you take<font color="#E5E5E5"> a very large data set you</font>

180
00:08:33,570 --> 00:08:38,430
distribute it to<font color="#E5E5E5"> your worker node so you</font>

181
00:08:36,150 --> 00:08:41,250
have different<font color="#E5E5E5"> shards of data on those</font>

182
00:08:38,429 --> 00:08:44,280
worker nodes<font color="#E5E5E5"> then you use a single model</font>

183
00:08:41,250 --> 00:08:45,990
architecture and then you run that in

184
00:08:44,280 --> 00:08:49,050
<font color="#E5E5E5">parallel and you build a single large</font>

185
00:08:45,990 --> 00:08:51,480
model so<font color="#E5E5E5"> that's the work</font><font color="#CCCCCC"> that we've done</font>

186
00:08:49,050 --> 00:08:54,479
<font color="#E5E5E5">primarily that I'm going to talk about</font>

187
00:08:51,480 --> 00:08:58,140
<font color="#E5E5E5">today</font><font color="#CCCCCC"> I did want to touch touch on one</font>

188
00:08:54,480 --> 00:09:00,690
other area which is hyper per<font color="#E5E5E5"> amore</font>

189
00:08:58,140 --> 00:09:01,890
tuning the thing one<font color="#E5E5E5"> interesting I've</font>

190
00:09:00,690 --> 00:09:05,080
<font color="#E5E5E5">learnt the thing I've learned about</font>

191
00:09:01,890 --> 00:09:07,240
<font color="#CCCCCC">working on neural nets is that</font>

192
00:09:05,080 --> 00:09:09,610
it's a bit of a black art<font color="#E5E5E5"> like even</font>

193
00:09:07,240 --> 00:09:11,230
experienced practitioners<font color="#E5E5E5"> can't say oh</font>

194
00:09:09,610 --> 00:09:13,570
this<font color="#E5E5E5"> mole architecture with these</font>

195
00:09:11,230 --> 00:09:17,500
<font color="#E5E5E5">parameters is going to work on this data</font>

196
00:09:13,570 --> 00:09:19,720
set<font color="#E5E5E5"> there's so much tuning involved so</font>

197
00:09:17,500 --> 00:09:21,850
another<font color="#CCCCCC"> option for</font><font color="#E5E5E5"> using the cluster is</font>

198
00:09:19,720 --> 00:09:25,180
to<font color="#CCCCCC"> replicate your data on</font><font color="#E5E5E5"> your different</font>

199
00:09:21,850 --> 00:09:27,400
worker nodes<font color="#CCCCCC"> so same data on</font><font color="#E5E5E5"> across like</font>

200
00:09:25,180 --> 00:09:29,380
on each worker node<font color="#E5E5E5"> same model</font>

201
00:09:27,400 --> 00:09:32,890
architecture by different parameters<font color="#E5E5E5"> and</font>

202
00:09:29,380 --> 00:09:37,570
then run<font color="#E5E5E5"> tens or hundreds of options</font><font color="#CCCCCC"> you</font>

203
00:09:32,890 --> 00:09:40,180
<font color="#E5E5E5">know of parameter versions and collect</font>

204
00:09:37,570 --> 00:09:41,560
the results<font color="#E5E5E5"> so that's another kind of</font>

205
00:09:40,180 --> 00:09:44,680
aspect of<font color="#E5E5E5"> this</font><font color="#CCCCCC"> project that</font><font color="#E5E5E5"> we're</font>

206
00:09:41,560 --> 00:09:48,189
<font color="#CCCCCC">working</font><font color="#E5E5E5"> on</font><font color="#CCCCCC"> let's talk about the</font><font color="#E5E5E5"> workflow</font>

207
00:09:44,680 --> 00:09:51,370
<font color="#E5E5E5">right now</font><font color="#CCCCCC"> so you take raw images</font><font color="#E5E5E5"> say</font>

208
00:09:48,190 --> 00:09:53,650
JPEG<font color="#E5E5E5"> image is and then for each pixel of</font>

209
00:09:51,370 --> 00:09:56,320
those<font color="#E5E5E5"> images</font><font color="#CCCCCC"> in the case of a</font><font color="#E5E5E5"> color</font>

210
00:09:53,650 --> 00:09:59,470
image it would<font color="#E5E5E5"> have an</font><font color="#CCCCCC"> R a G and a</font><font color="#E5E5E5"> B</font>

211
00:09:56,320 --> 00:10:01,600
value<font color="#E5E5E5"> you create a numpy array from</font>

212
00:09:59,470 --> 00:10:08,200
<font color="#E5E5E5">those so you can have your resolution</font>

213
00:10:01,600 --> 00:10:10,390
times<font color="#CCCCCC"> 3 in this stacked array what we do</font>

214
00:10:08,200 --> 00:10:12,340
is we actually<font color="#E5E5E5"> write it to CSV</font><font color="#CCCCCC"> and then</font>

215
00:10:10,390 --> 00:10:15,310
read<font color="#CCCCCC"> it in a parallel fashion into the</font>

216
00:10:12,340 --> 00:10:19,150
database<font color="#E5E5E5"> so there is kind of tooling</font>

217
00:10:15,310 --> 00:10:22,000
<font color="#CCCCCC">that we've developed that allows</font><font color="#E5E5E5"> you to</font>

218
00:10:19,150 --> 00:10:24,430
<font color="#E5E5E5">take these raw images and then stream</font>

219
00:10:22,000 --> 00:10:27,670
them into the<font color="#E5E5E5"> database</font><font color="#CCCCCC"> so once you have</font>

220
00:10:24,430 --> 00:10:29,770
them in<font color="#CCCCCC"> the database then that's when</font>

221
00:10:27,670 --> 00:10:32,459
kind of the deep learning<font color="#E5E5E5"> can can happen</font>

222
00:10:29,770 --> 00:10:38,470
so how does distribute a deep learning

223
00:10:32,460 --> 00:10:40,180
work the way<font color="#CCCCCC"> that it works is by</font>

224
00:10:38,470 --> 00:10:41,980
distributing the workload<font color="#E5E5E5"> to the</font>

225
00:10:40,180 --> 00:10:48,060
segments<font color="#E5E5E5"> and then collecting the results</font>

226
00:10:41,980 --> 00:10:50,710
<font color="#CCCCCC">back so for example step</font><font color="#E5E5E5"> one is the</font>

227
00:10:48,060 --> 00:10:52,959
model<font color="#CCCCCC"> that you've written</font><font color="#E5E5E5"> say a tensor</font>

228
00:10:50,710 --> 00:10:54,520
<font color="#E5E5E5">flow model chunks through the mini</font>

229
00:10:52,960 --> 00:10:57,310
batches of the data that<font color="#CCCCCC"> is on</font><font color="#E5E5E5"> that</font>

230
00:10:54,520 --> 00:11:00,130
<font color="#E5E5E5">worker node right so once they've</font>

231
00:10:57,310 --> 00:11:02,140
completed<font color="#CCCCCC"> that then they do what's</font>

232
00:11:00,130 --> 00:11:05,740
called<font color="#E5E5E5"> a merge</font><font color="#CCCCCC"> function so the merge</font>

233
00:11:02,140 --> 00:11:08,020
involves<font color="#E5E5E5"> collecting the weights and the</font>

234
00:11:05,740 --> 00:11:10,120
gradients from the model state

235
00:11:08,020 --> 00:11:13,870
effectively<font color="#E5E5E5"> from each of the worker</font>

236
00:11:10,120 --> 00:11:16,930
nodes<font color="#E5E5E5"> combining it together then for a</font>

237
00:11:13,870 --> 00:11:18,480
<font color="#E5E5E5">particular operation there may be a</font>

238
00:11:16,930 --> 00:11:22,979
final<font color="#CCCCCC"> function that's</font>

239
00:11:18,480 --> 00:11:25,320
and then you<font color="#CCCCCC"> iterate</font><font color="#E5E5E5"> so then you</font>

240
00:11:22,980 --> 00:11:28,260
rebroadcast your<font color="#E5E5E5"> weights you go through</font>

241
00:11:25,320 --> 00:11:29,449
another<font color="#E5E5E5"> epoch of your data and then</font><font color="#CCCCCC"> you</font>

242
00:11:28,260 --> 00:11:31,620
[Music]

243
00:11:29,449 --> 00:11:34,010
<font color="#CCCCCC">do</font><font color="#E5E5E5"> that either for</font><font color="#CCCCCC"> a fixed number of</font>

244
00:11:31,620 --> 00:11:36,690
iterations or until you you you converge

245
00:11:34,010 --> 00:11:43,410
<font color="#E5E5E5">so it's this idea of iterative model</font>

246
00:11:36,690 --> 00:11:45,600
execution for distributed systems<font color="#E5E5E5"> so the</font>

247
00:11:43,410 --> 00:11:48,389
question you<font color="#CCCCCC"> know that we've been</font>

248
00:11:45,600 --> 00:11:50,040
looking<font color="#CCCCCC"> at is what does that merge</font>

249
00:11:48,389 --> 00:11:52,740
function<font color="#E5E5E5"> look like like how do we</font>

250
00:11:50,040 --> 00:11:55,529
collect the this this<font color="#E5E5E5"> data in order to</font>

251
00:11:52,740 --> 00:11:57,180
<font color="#E5E5E5">do this distributed computation so it</font>

252
00:11:55,529 --> 00:11:59,910
turns out not surprisingly this<font color="#CCCCCC"> is an</font>

253
00:11:57,180 --> 00:12:03,060
open<font color="#E5E5E5"> area of research</font><font color="#CCCCCC"> there's a</font><font color="#E5E5E5"> paper</font>

254
00:11:59,910 --> 00:12:05,490
I've quoted here which<font color="#CCCCCC"> I think is</font><font color="#E5E5E5"> very</font>

255
00:12:03,060 --> 00:12:07,018
useful<font color="#E5E5E5"> that gives a survey of different</font>

256
00:12:05,490 --> 00:12:10,920
approaches to<font color="#E5E5E5"> take the distributed</font>

257
00:12:07,019 --> 00:12:13,470
learning<font color="#CCCCCC"> we tested three so far we</font>

258
00:12:10,920 --> 00:12:17,550
touched it simple averaging we tested an

259
00:12:13,470 --> 00:12:19,889
ensemble in method which is putting

260
00:12:17,550 --> 00:12:21,560
<font color="#E5E5E5">models together</font><font color="#CCCCCC"> and then we tested</font>

261
00:12:19,889 --> 00:12:25,170
something<font color="#E5E5E5"> called elastic averaging</font>

262
00:12:21,560 --> 00:12:28,138
<font color="#E5E5E5">stochastic gradient descent so I'd like</font>

263
00:12:25,170 --> 00:12:31,260
<font color="#E5E5E5">to share some results from the work</font><font color="#CCCCCC"> that</font>

264
00:12:28,139 --> 00:12:33,089
we've we've done<font color="#E5E5E5"> so far the</font>

265
00:12:31,260 --> 00:12:36,899
infrastructure<font color="#CCCCCC"> that we used we tested</font>

266
00:12:33,089 --> 00:12:42,389
primarily on Google cloud platform<font color="#CCCCCC"> we</font>

267
00:12:36,899 --> 00:12:44,550
used high memory<font color="#E5E5E5"> CPUs with 208 gig of</font>

268
00:12:42,389 --> 00:12:50,540
memory per per machine per<font color="#E5E5E5"> virtual</font>

269
00:12:44,550 --> 00:12:53,160
machine we use<font color="#CCCCCC"> P 100 GPUs and we tested</font>

270
00:12:50,540 --> 00:12:56,519
<font color="#E5E5E5">database cluster</font><font color="#CCCCCC"> sizes from</font><font color="#E5E5E5"> a single</font>

271
00:12:53,160 --> 00:13:01,439
node up to 20 nodes and we have one GPU

272
00:12:56,519 --> 00:13:03,360
per segment<font color="#E5E5E5"> we use two data sets</font><font color="#CCCCCC"> this is</font>

273
00:13:01,440 --> 00:13:04,829
<font color="#CCCCCC">a classic one that people</font><font color="#E5E5E5"> in machine</font>

274
00:13:03,360 --> 00:13:07,380
learning<font color="#E5E5E5"> use it's called</font><font color="#CCCCCC"> cypher 10</font>

275
00:13:04,829 --> 00:13:12,469
<font color="#CCCCCC">they're low resolution color pictures</font>

276
00:13:07,380 --> 00:13:14,910
they're 32 by 32 and<font color="#E5E5E5"> there's 60,000</font>

277
00:13:12,470 --> 00:13:16,949
<font color="#CCCCCC">there's a more challenging data set</font>

278
00:13:14,910 --> 00:13:17,610
which is a really<font color="#CCCCCC"> great data set</font><font color="#E5E5E5"> from</font>

279
00:13:16,949 --> 00:13:21,479
MIT

280
00:13:17,610 --> 00:13:24,510
it's called places<font color="#CCCCCC"> the version we use</font>

281
00:13:21,480 --> 00:13:26,120
<font color="#CCCCCC">has 1.8 million examples there's another</font>

282
00:13:24,510 --> 00:13:28,910
one with<font color="#E5E5E5"> 8 or 10 million</font>

283
00:13:26,120 --> 00:13:35,570
<font color="#E5E5E5">apples</font><font color="#CCCCCC"> their higher resolution 256 by</font>

284
00:13:28,910 --> 00:13:41,480
256 and they<font color="#E5E5E5"> comprise 365 places like</font>

285
00:13:35,570 --> 00:13:45,620
cafe library<font color="#CCCCCC"> Park kinds</font><font color="#E5E5E5"> of things so one</font>

286
00:13:41,480 --> 00:13:47,330
<font color="#CCCCCC">of the results that we've seen</font><font color="#E5E5E5"> I'm not</font>

287
00:13:45,620 --> 00:13:49,640
sure how readable<font color="#CCCCCC"> that is but I'll call</font>

288
00:13:47,330 --> 00:13:52,760
out the colors<font color="#E5E5E5"> so what we started with</font>

289
00:13:49,640 --> 00:13:56,529
is a<font color="#E5E5E5"> simple one layer convolutional</font>

290
00:13:52,760 --> 00:13:58,700
neural net<font color="#CCCCCC"> there's an architecture there</font>

291
00:13:56,529 --> 00:14:02,060
<font color="#CCCCCC">which you can look at later if you want</font>

292
00:13:58,700 --> 00:14:05,690
so we ran it<font color="#E5E5E5"> on a single</font><font color="#CCCCCC"> node 4 node</font>

293
00:14:02,060 --> 00:14:11,270
cluster<font color="#CCCCCC"> 8 and 16</font><font color="#E5E5E5"> so what this is</font>

294
00:14:05,690 --> 00:14:14,270
plotting is the accuracy on<font color="#CCCCCC"> the y-axis</font>

295
00:14:11,270 --> 00:14:18,020
<font color="#CCCCCC">and then number of iterations on the</font>

296
00:14:14,270 --> 00:14:20,660
<font color="#CCCCCC">x-axis so what it shows is that</font><font color="#E5E5E5"> this is</font>

297
00:14:18,020 --> 00:14:23,980
very<font color="#CCCCCC"> well-behaved</font><font color="#E5E5E5"> data</font><font color="#CCCCCC"> set so for one</font>

298
00:14:20,660 --> 00:14:27,829
node<font color="#E5E5E5"> it takes fewer iterations to reach</font>

299
00:14:23,980 --> 00:14:32,150
an accuracy of<font color="#E5E5E5"> you know 75 or 80 percent</font>

300
00:14:27,830 --> 00:14:34,730
<font color="#E5E5E5">that's the blue line</font><font color="#CCCCCC"> the Green</font><font color="#E5E5E5"> Line is</font><font color="#CCCCCC"> a</font>

301
00:14:32,150 --> 00:14:38,089
<font color="#CCCCCC">20 node cluster so in terms of number of</font>

302
00:14:34,730 --> 00:14:40,070
iterations<font color="#CCCCCC"> it takes longer for that to</font>

303
00:14:38,089 --> 00:14:43,040
converge which kind of makes<font color="#E5E5E5"> sense</font>

304
00:14:40,070 --> 00:14:46,550
<font color="#E5E5E5">because each worker node is only seeing</font>

305
00:14:43,040 --> 00:14:48,529
a portion of<font color="#E5E5E5"> all of the examples</font><font color="#CCCCCC"> and</font>

306
00:14:46,550 --> 00:14:50,660
there's only 50 thousands or<font color="#CCCCCC"> so there's</font>

307
00:14:48,529 --> 00:14:52,550
<font color="#E5E5E5">not that many so this was sort</font><font color="#CCCCCC"> of</font>

308
00:14:50,660 --> 00:14:54,709
expected the thing we're interested

309
00:14:52,550 --> 00:14:56,390
<font color="#CCCCCC">though is in his run time because the</font>

310
00:14:54,709 --> 00:14:58,670
reason we want to do a distributed

311
00:14:56,390 --> 00:15:00,529
system is that we want we want<font color="#E5E5E5"> our</font>

312
00:14:58,670 --> 00:15:02,740
models to train much faster<font color="#E5E5E5"> than if</font>

313
00:15:00,529 --> 00:15:08,600
you're<font color="#CCCCCC"> just using</font><font color="#E5E5E5"> a single single node</font>

314
00:15:02,740 --> 00:15:13,430
so this plot then shows run time on<font color="#E5E5E5"> the</font>

315
00:15:08,600 --> 00:15:18,550
<font color="#CCCCCC">y-axis and accuracy on the left</font><font color="#E5E5E5"> axis so</font>

316
00:15:13,430 --> 00:15:25,040
the way<font color="#CCCCCC"> to read it</font><font color="#E5E5E5"> is</font><font color="#CCCCCC"> for a this</font><font color="#E5E5E5"> is</font><font color="#CCCCCC"> 75%</font>

317
00:15:18,550 --> 00:15:28,579
<font color="#E5E5E5">accuracy so one node takes 40,000 or</font>

318
00:15:25,040 --> 00:15:30,829
4,000<font color="#E5E5E5"> can read</font><font color="#CCCCCC"> at</font><font color="#E5E5E5"> 4,000 seconds if you</font>

319
00:15:28,580 --> 00:15:34,790
run it on<font color="#E5E5E5"> the cluster then it</font><font color="#CCCCCC"> runs much</font>

320
00:15:30,830 --> 00:15:37,510
<font color="#E5E5E5">faster</font><font color="#CCCCCC"> however you'll note that</font><font color="#E5E5E5"> if you</font>

321
00:15:34,790 --> 00:15:39,569
run it<font color="#E5E5E5"> on four nodes or eight nodes or</font>

322
00:15:37,510 --> 00:15:41,880
16 nodes it

323
00:15:39,570 --> 00:15:44,610
run linearly faster<font color="#E5E5E5"> so you get to a</font>

324
00:15:41,880 --> 00:15:48,390
point where yes distributing that

325
00:15:44,610 --> 00:15:50,520
computation helps you but you know if

326
00:15:48,390 --> 00:15:52,470
you throw<font color="#E5E5E5"> more GPUs at it it doesn't</font>

327
00:15:50,520 --> 00:15:55,470
<font color="#CCCCCC">necessarily mean you're gonna get</font><font color="#E5E5E5"> faster</font>

328
00:15:52,470 --> 00:15:58,020
convergence<font color="#E5E5E5"> so let's look at a more</font>

329
00:15:55,470 --> 00:16:02,240
complicated<font color="#CCCCCC"> model this now is a</font>

330
00:15:58,020 --> 00:16:06,300
<font color="#CCCCCC">six-layer</font><font color="#E5E5E5"> neural net so it has six</font>

331
00:16:02,240 --> 00:16:08,250
layers<font color="#CCCCCC"> stacked together</font><font color="#E5E5E5"> and this is the</font>

332
00:16:06,300 --> 00:16:10,589
same thing<font color="#E5E5E5"> in terms of run time our</font>

333
00:16:08,250 --> 00:16:14,790
sorry in terms<font color="#CCCCCC"> of accuracy</font><font color="#E5E5E5"> on the left</font>

334
00:16:10,590 --> 00:16:17,700
side and iterations on the right side<font color="#E5E5E5"> so</font>

335
00:16:14,790 --> 00:16:19,500
this looking at<font color="#E5E5E5"> this model this shows me</font>

336
00:16:17,700 --> 00:16:21,960
<font color="#E5E5E5">that it doesn't</font><font color="#CCCCCC"> perhaps</font>

337
00:16:19,500 --> 00:16:24,830
generalize that well because you see

338
00:16:21,960 --> 00:16:27,810
your accuracy going down<font color="#E5E5E5"> after a</font><font color="#CCCCCC"> while</font>

339
00:16:24,830 --> 00:16:30,840
<font color="#CCCCCC">but one kind of interesting thing to me</font>

340
00:16:27,810 --> 00:16:33,180
<font color="#E5E5E5">is</font><font color="#CCCCCC"> that in this in this plot is that if</font>

341
00:16:30,840 --> 00:16:37,620
you run it<font color="#E5E5E5"> on one iteration if you rent</font>

342
00:16:33,180 --> 00:16:39,150
it<font color="#E5E5E5"> on just a single server</font><font color="#CCCCCC"> you see that</font>

343
00:16:37,620 --> 00:16:40,740
it doesn't generalize that well but if

344
00:16:39,150 --> 00:16:46,170
<font color="#E5E5E5">you run it on the cluster</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> doesn't</font>

345
00:16:40,740 --> 00:16:48,180
show<font color="#E5E5E5"> up as</font><font color="#CCCCCC"> much so if you cast</font><font color="#E5E5E5"> that plot</font>

346
00:16:46,170 --> 00:16:50,939
in terms<font color="#CCCCCC"> of time like we did on</font><font color="#E5E5E5"> the</font>

347
00:16:48,180 --> 00:16:52,469
<font color="#CCCCCC">other ones</font><font color="#E5E5E5"> then what you see is for this</font>

348
00:16:50,940 --> 00:16:57,060
<font color="#E5E5E5">particular data set and this model</font>

349
00:16:52,470 --> 00:17:00,180
architecture at least<font color="#E5E5E5"> if to achieve</font><font color="#CCCCCC"> 75</font>

350
00:16:57,060 --> 00:17:04,050
<font color="#CCCCCC">percent accuracy you know</font><font color="#E5E5E5"> one node takes</font>

351
00:17:00,180 --> 00:17:09,800
the the longest<font color="#CCCCCC"> takes</font><font color="#E5E5E5"> 750 seconds for</font>

352
00:17:04,050 --> 00:17:13,290
nodes<font color="#E5E5E5"> is you know 600 seconds</font><font color="#CCCCCC"> eight is</font>

353
00:17:09,800 --> 00:17:15,930
300 or so so it does<font color="#E5E5E5"> get faster so</font><font color="#CCCCCC"> here</font>

354
00:17:13,290 --> 00:17:17,339
you seem more a case for<font color="#E5E5E5"> this small</font>

355
00:17:15,930 --> 00:17:20,760
architecture that the bigger your

356
00:17:17,339 --> 00:17:22,050
cluster is it does seem<font color="#E5E5E5"> to run faster up</font>

357
00:17:20,760 --> 00:17:26,640
to<font color="#CCCCCC"> a</font><font color="#E5E5E5"> point you don't see a lot of</font>

358
00:17:22,050 --> 00:17:29,610
difference between<font color="#CCCCCC"> 8 and 16 nodes so</font>

359
00:17:26,640 --> 00:17:34,050
let's look at maybe one or two<font color="#E5E5E5"> more</font>

360
00:17:29,610 --> 00:17:37,350
<font color="#CCCCCC">examples</font><font color="#E5E5E5"> this is on the places data set</font>

361
00:17:34,050 --> 00:17:39,960
that<font color="#E5E5E5"> I talked about so that's a much</font>

362
00:17:37,350 --> 00:17:43,379
larger data set and then we're<font color="#CCCCCC"> using a</font>

363
00:17:39,960 --> 00:17:45,630
<font color="#CCCCCC">well known molecule vgg and this has a</font>

364
00:17:43,380 --> 00:17:51,010
hundred<font color="#E5E5E5"> and thirty million parameters</font><font color="#CCCCCC"> so</font>

365
00:17:45,630 --> 00:17:53,290
this is a pretty big model so

366
00:17:51,010 --> 00:17:55,180
as expected<font color="#E5E5E5"> we see that if you in terms</font>

367
00:17:53,290 --> 00:17:58,570
of<font color="#E5E5E5"> number of iterations it takes fewer</font>

368
00:17:55,180 --> 00:18:03,850
<font color="#E5E5E5">iterations to converge</font><font color="#CCCCCC"> we notice here on</font>

369
00:17:58,570 --> 00:18:05,350
20<font color="#E5E5E5"> a cluster of 20 we ran it up to 100</font>

370
00:18:03,850 --> 00:18:10,209
iterations and<font color="#E5E5E5"> it still hadn't got to</font>

371
00:18:05,350 --> 00:18:13,449
<font color="#CCCCCC">the</font><font color="#E5E5E5"> accuracy we we wanted</font><font color="#CCCCCC"> and if you</font>

372
00:18:10,210 --> 00:18:17,080
look at the time<font color="#E5E5E5"> and this</font><font color="#CCCCCC"> is really</font><font color="#E5E5E5"> less</font>

373
00:18:13,450 --> 00:18:19,240
convincing<font color="#E5E5E5"> what it's showing is</font><font color="#CCCCCC"> that if</font>

374
00:18:17,080 --> 00:18:22,240
you forget the<font color="#E5E5E5"> 20 node cluster for a</font>

375
00:18:19,240 --> 00:18:26,110
moment which is<font color="#E5E5E5"> the green one these are</font>

376
00:18:22,240 --> 00:18:27,490
<font color="#CCCCCC">1 5</font><font color="#E5E5E5"> and 10 nodes</font><font color="#CCCCCC"> actually and you don't</font>

377
00:18:26,110 --> 00:18:30,370
really<font color="#E5E5E5"> see any benefit from this</font>

378
00:18:27,490 --> 00:18:32,080
distributed computation so there's

379
00:18:30,370 --> 00:18:34,750
perhaps a number<font color="#E5E5E5"> of reasons why this</font>

380
00:18:32,080 --> 00:18:39,220
happens<font color="#E5E5E5"> it could be that this data set</font>

381
00:18:34,750 --> 00:18:41,440
has so many levels<font color="#CCCCCC"> in it</font><font color="#E5E5E5"> that like so</font>

382
00:18:39,220 --> 00:18:43,000
<font color="#E5E5E5">many different</font><font color="#CCCCCC"> classes that distributing</font>

383
00:18:41,440 --> 00:18:48,220
that<font color="#E5E5E5"> computation doesn't help you that</font>

384
00:18:43,000 --> 00:18:51,820
that much<font color="#CCCCCC"> last example I want to show is</font>

385
00:18:48,220 --> 00:18:53,890
on an ensemble<font color="#E5E5E5"> so what we did is</font><font color="#CCCCCC"> we took</font>

386
00:18:51,820 --> 00:18:56,110
this convolutional neural net we ran it

387
00:18:53,890 --> 00:18:58,720
<font color="#E5E5E5">on each of the worker nodes and instead</font>

388
00:18:56,110 --> 00:19:02,530
<font color="#CCCCCC">of</font><font color="#E5E5E5"> averaging the result of those we fed</font>

389
00:18:58,720 --> 00:19:05,740
<font color="#CCCCCC">that into another CNN which is a very</font>

390
00:19:02,530 --> 00:19:12,360
very simple<font color="#E5E5E5"> CNN to see what the effect</font>

391
00:19:05,740 --> 00:19:15,910
of<font color="#CCCCCC"> Ensemble models is so this is</font><font color="#E5E5E5"> the</font>

392
00:19:12,360 --> 00:19:19,810
result<font color="#E5E5E5"> and one interesting so the red</font>

393
00:19:15,910 --> 00:19:23,220
line<font color="#CCCCCC"> is if you just so this is</font><font color="#E5E5E5"> accuracy</font>

394
00:19:19,810 --> 00:19:26,590
on<font color="#E5E5E5"> the left side</font><font color="#CCCCCC"> number of iterations so</font>

395
00:19:23,220 --> 00:19:27,310
<font color="#CCCCCC">the red line</font><font color="#E5E5E5"> is if you just average the</font>

396
00:19:26,590 --> 00:19:29,139
worker nodes

397
00:19:27,310 --> 00:19:32,200
<font color="#CCCCCC">you know what</font><font color="#E5E5E5"> accuracy to get and you</font>

398
00:19:29,140 --> 00:19:34,180
see it's converging albeit slowly<font color="#E5E5E5"> if you</font>

399
00:19:32,200 --> 00:19:36,250
<font color="#CCCCCC">take the output</font><font color="#E5E5E5"> at a given point and</font>

400
00:19:34,180 --> 00:19:39,700
feed it through this<font color="#CCCCCC"> ensemble this</font>

401
00:19:36,250 --> 00:19:42,100
simpler CNN at the end<font color="#E5E5E5"> then your</font>

402
00:19:39,700 --> 00:19:45,640
<font color="#E5E5E5">accuracy actually jumps quite a lot by</font>

403
00:19:42,100 --> 00:19:47,110
two or<font color="#E5E5E5"> three times so</font><font color="#CCCCCC"> we only ran it for</font>

404
00:19:45,640 --> 00:19:50,770
40 iterations<font color="#E5E5E5"> because it was very</font>

405
00:19:47,110 --> 00:19:53,010
<font color="#E5E5E5">expensive</font><font color="#CCCCCC"> but at least we</font><font color="#E5E5E5"> got</font><font color="#CCCCCC"> to that we</font>

406
00:19:50,770 --> 00:19:56,020
got<font color="#CCCCCC"> a feeling that</font><font color="#E5E5E5"> this idea of</font>

407
00:19:53,010 --> 00:19:58,480
ensembles<font color="#E5E5E5"> in the distributed system may</font>

408
00:19:56,020 --> 00:20:01,510
be something that<font color="#E5E5E5"> is very</font><font color="#CCCCCC"> useful in</font><font color="#E5E5E5"> the</font>

409
00:19:58,480 --> 00:20:03,310
<font color="#E5E5E5">future so I'm gonna skip this</font><font color="#CCCCCC"> one</font>

410
00:20:01,510 --> 00:20:05,490
<font color="#E5E5E5">because I do want to talk about lessons</font>

411
00:20:03,310 --> 00:20:08,168
<font color="#CCCCCC">learned</font>

412
00:20:05,490 --> 00:20:11,499
both on the modeling<font color="#E5E5E5"> and on</font>

413
00:20:08,169 --> 00:20:15,369
infrastructure lessons<font color="#E5E5E5"> learned on the</font>

414
00:20:11,499 --> 00:20:17,379
modeling<font color="#CCCCCC"> I would say yes distributed</font>

415
00:20:15,369 --> 00:20:19,749
deep learning<font color="#E5E5E5"> can potentially run faster</font>

416
00:20:17,379 --> 00:20:24,399
than<font color="#CCCCCC"> a single</font><font color="#E5E5E5"> node so for a given</font>

417
00:20:19,749 --> 00:20:28,539
accuracy<font color="#E5E5E5"> you can</font><font color="#CCCCCC"> get benefits from</font><font color="#E5E5E5"> from</font>

418
00:20:24,399 --> 00:20:31,059
distributing that<font color="#E5E5E5"> computation our</font>

419
00:20:28,539 --> 00:20:34,299
experience in our experience we<font color="#E5E5E5"> found it</font>

420
00:20:31,059 --> 00:20:36,220
to be<font color="#CCCCCC"> pretty challenging</font><font color="#E5E5E5"> to do this it</font>

421
00:20:34,299 --> 00:20:37,840
was<font color="#CCCCCC"> really</font><font color="#E5E5E5"> interesting work but anybody</font>

422
00:20:36,220 --> 00:20:39,639
who's working<font color="#E5E5E5"> on distributed systems</font>

423
00:20:37,840 --> 00:20:43,059
know that things<font color="#E5E5E5"> don't seem to come</font>

424
00:20:39,639 --> 00:20:45,879
easily<font color="#CCCCCC"> the other thing is we were</font>

425
00:20:43,059 --> 00:20:48,820
<font color="#E5E5E5">working in the context of a distributed</font>

426
00:20:45,879 --> 00:20:50,619
<font color="#CCCCCC">database</font><font color="#E5E5E5"> this parallel database so there</font>

427
00:20:48,820 --> 00:20:52,539
<font color="#E5E5E5">are some things</font><font color="#CCCCCC"> that like we're just</font><font color="#E5E5E5"> on</font>

428
00:20:50,619 --> 00:20:54,879
a linux cluster we could do<font color="#E5E5E5"> differently</font>

429
00:20:52,539 --> 00:20:56,889
<font color="#E5E5E5">but since you need to work with</font><font color="#CCCCCC"> the</font>

430
00:20:54,879 --> 00:20:59,259
query processor<font color="#CCCCCC"> you need to work with</font>

431
00:20:56,889 --> 00:21:01,240
<font color="#CCCCCC">how the database</font><font color="#E5E5E5"> marshals data around</font>

432
00:20:59,259 --> 00:21:06,519
<font color="#E5E5E5">that can kind of handcuff you a little</font>

433
00:21:01,240 --> 00:21:09,149
bit<font color="#E5E5E5"> so that was when I know one of the</font>

434
00:21:06,519 --> 00:21:12,309
limitations that we we had to<font color="#E5E5E5"> deal with</font>

435
00:21:09,149 --> 00:21:15,998
<font color="#E5E5E5">on the infrastructure lessons learned</font><font color="#CCCCCC"> I</font>

436
00:21:12,309 --> 00:21:19,720
said my number<font color="#E5E5E5"> one thing</font><font color="#CCCCCC"> is beware of</font>

437
00:21:15,999 --> 00:21:21,669
the cost of GPUs on public cloud<font color="#CCCCCC"> when we</font>

438
00:21:19,720 --> 00:21:24,330
got we had some sticker shock when<font color="#CCCCCC"> we</font>

439
00:21:21,669 --> 00:21:29,110
<font color="#E5E5E5">got our bill</font><font color="#CCCCCC"> after the first</font><font color="#E5E5E5"> first month</font>

440
00:21:24,330 --> 00:21:32,740
<font color="#E5E5E5">the other thing is memory management can</font>

441
00:21:29,110 --> 00:21:34,809
be very finicky GPU initialization

442
00:21:32,740 --> 00:21:38,230
settings and freeing tensorflow memory

443
00:21:34,809 --> 00:21:45,100
there's a lot<font color="#E5E5E5"> of kind of</font><font color="#CCCCCC"> folklore there</font>

444
00:21:38,230 --> 00:21:46,990
<font color="#E5E5E5">and I put at the back</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> this deck</font><font color="#CCCCCC"> kind</font>

445
00:21:45,100 --> 00:21:50,129
<font color="#CCCCCC">of a more detailed list</font><font color="#E5E5E5"> so if anybody's</font>

446
00:21:46,990 --> 00:21:52,389
<font color="#CCCCCC">having issues around GPU memory and such</font>

447
00:21:50,129 --> 00:21:54,129
<font color="#E5E5E5">we found a bunch of blog</font><font color="#CCCCCC"> posts that were</font>

448
00:21:52,389 --> 00:21:55,539
<font color="#E5E5E5">helpful a lot of stuff we kind of had to</font>

449
00:21:54,129 --> 00:21:57,908
fail and learn<font color="#E5E5E5"> on</font>

450
00:21:55,539 --> 00:22:00,369
our own so there's a lot<font color="#CCCCCC"> of details here</font>

451
00:21:57,909 --> 00:22:03,850
<font color="#CCCCCC">on</font><font color="#E5E5E5"> if you're running into challenges</font>

452
00:22:00,369 --> 00:22:05,439
<font color="#CCCCCC">this isn't just database specific</font><font color="#E5E5E5"> you</font>

453
00:22:03,850 --> 00:22:10,769
know have a look<font color="#E5E5E5"> they're there you there</font>

454
00:22:05,440 --> 00:22:10,769
<font color="#CCCCCC">may be some may save you a bit of</font><font color="#E5E5E5"> time</font>

455
00:22:11,820 --> 00:22:17,470
<font color="#E5E5E5">yeah so maybe since we're getting</font><font color="#CCCCCC"> to the</font>

456
00:22:15,489 --> 00:22:20,919
<font color="#E5E5E5">end here</font><font color="#CCCCCC"> I'll talk about</font><font color="#E5E5E5"> the future work</font>

457
00:22:17,470 --> 00:22:23,019
<font color="#CCCCCC">that we're planning to</font><font color="#E5E5E5"> do so Apache</font>

458
00:22:20,919 --> 00:22:26,379
<font color="#E5E5E5">madly</font><font color="#CCCCCC"> passive release coming up in</font>

459
00:22:23,019 --> 00:22:28,419
probably<font color="#E5E5E5"> within the</font><font color="#CCCCCC"> next month</font><font color="#E5E5E5"> which is</font>

460
00:22:26,379 --> 00:22:31,658
<font color="#E5E5E5">the initial release of the distributed</font>

461
00:22:28,419 --> 00:22:33,220
deep learning<font color="#CCCCCC"> so we're currently</font><font color="#E5E5E5"> going</font>

462
00:22:31,659 --> 00:22:37,869
<font color="#E5E5E5">to support</font><font color="#CCCCCC"> care us with the</font><font color="#E5E5E5"> tensorflow</font>

463
00:22:33,220 --> 00:22:39,849
<font color="#CCCCCC">back-end with GPU support</font><font color="#E5E5E5"> and the</font><font color="#CCCCCC"> work</font>

464
00:22:37,869 --> 00:22:43,478
that<font color="#E5E5E5"> we have planned for the</font><font color="#CCCCCC"> remainder</font>

465
00:22:39,849 --> 00:22:45,809
<font color="#E5E5E5">of this year</font><font color="#CCCCCC"> this is very deep learning</font>

466
00:22:43,479 --> 00:22:49,059
<font color="#CCCCCC">focused so we want to look at more</font>

467
00:22:45,809 --> 00:22:52,899
distributed deep learning<font color="#E5E5E5"> methods we</font>

468
00:22:49,059 --> 00:22:55,440
want to implement<font color="#CCCCCC"> parallel hyper</font>

469
00:22:52,899 --> 00:23:00,008
parameter tuning that<font color="#CCCCCC"> I mentioned before</font>

470
00:22:55,440 --> 00:23:04,479
<font color="#E5E5E5">and we're also in the</font><font color="#CCCCCC"> process of adding</font>

471
00:23:00,009 --> 00:23:06,249
<font color="#CCCCCC">a model versioning repository so</font><font color="#E5E5E5"> that</font>

472
00:23:04,479 --> 00:23:09,070
will allow<font color="#CCCCCC"> you to run tons and tons of</font>

473
00:23:06,249 --> 00:23:12,159
<font color="#CCCCCC">models in parallel</font><font color="#E5E5E5"> collect your results</font>

474
00:23:09,070 --> 00:23:15,009
in one in<font color="#E5E5E5"> one table and then sort of</font>

475
00:23:12,159 --> 00:23:21,580
sort through them to pick your best best

476
00:23:15,009 --> 00:23:22,380
parameters that was it so thank you very

477
00:23:21,580 --> 00:23:32,749
<font color="#E5E5E5">much for you</font>

478
00:23:22,380 --> 00:23:32,749
[Applause]

479
00:23:37,409 --> 00:23:53,280
yes yeah<font color="#CCCCCC"> okay I think that's because</font>

480
00:23:48,549 --> 00:23:56,559
<font color="#E5E5E5">that model doesn't generalize well so</font>

481
00:23:53,280 --> 00:23:58,210
<font color="#CCCCCC">okay</font><font color="#E5E5E5"> so the question is why if</font><font color="#CCCCCC"> you rent</font>

482
00:23:56,559 --> 00:24:02,230
it<font color="#CCCCCC"> for more accurate for a larger</font>

483
00:23:58,210 --> 00:24:05,799
accuracy<font color="#CCCCCC"> does it does</font><font color="#E5E5E5"> if you run for</font>

484
00:24:02,230 --> 00:24:09,549
more<font color="#CCCCCC"> iterations as accuracy drop so this</font>

485
00:24:05,799 --> 00:24:14,230
can<font color="#CCCCCC"> happen for certain kind</font><font color="#E5E5E5"> of models</font>

486
00:24:09,549 --> 00:24:17,440
<font color="#CCCCCC">that</font><font color="#E5E5E5"> start to see increasing gradients</font>

487
00:24:14,230 --> 00:24:19,900
<font color="#CCCCCC">so accuracy goes down there's</font><font color="#E5E5E5"> there</font><font color="#CCCCCC"> is a</font>

488
00:24:17,440 --> 00:24:21,640
blog post here where we got this<font color="#E5E5E5"> we</font>

489
00:24:19,900 --> 00:24:24,130
didn't create this model<font color="#CCCCCC"> we got it from</font>

490
00:24:21,640 --> 00:24:25,299
this researcher so the results that<font color="#E5E5E5"> we</font>

491
00:24:24,130 --> 00:24:28,960
see here are very similar<font color="#E5E5E5"> to</font><font color="#CCCCCC"> that</font>

492
00:24:25,299 --> 00:24:31,240
<font color="#E5E5E5">person's results I think the bottom line</font>

493
00:24:28,960 --> 00:24:40,169
<font color="#E5E5E5">is it's not a very good</font><font color="#CCCCCC"> model for</font><font color="#E5E5E5"> that</font>

494
00:24:31,240 --> 00:24:40,169
data<font color="#E5E5E5"> side sorry</font>

495
00:24:42,520 --> 00:24:49,400
this is<font color="#E5E5E5"> a yeah I was showing I was using</font>

496
00:24:46,100 --> 00:24:51,740
<font color="#E5E5E5">this as an example to</font><font color="#CCCCCC"> show that if you</font>

497
00:24:49,400 --> 00:24:55,280
run if you run a certain<font color="#E5E5E5"> model</font>

498
00:24:51,740 --> 00:24:57,740
architecture<font color="#E5E5E5"> on a data set on a single</font>

499
00:24:55,280 --> 00:25:00,020
<font color="#E5E5E5">node and then you run the same thing in</font>

500
00:24:57,740 --> 00:25:02,299
a cluster<font color="#CCCCCC"> you actually can see different</font>

501
00:25:00,020 --> 00:25:06,309
convergence behavior<font color="#E5E5E5"> that was sort of</font>

502
00:25:02,299 --> 00:25:06,309
the<font color="#E5E5E5"> point of demonstrative showing this</font>

