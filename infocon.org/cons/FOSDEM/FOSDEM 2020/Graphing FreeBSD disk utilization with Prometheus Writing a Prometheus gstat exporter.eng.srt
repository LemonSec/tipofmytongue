1
00:00:08,320 --> 00:00:13,920
exporter

2
00:00:09,040 --> 00:00:15,759
so exactly thank you hello everybody

3
00:00:13,920 --> 00:00:17,680
we have plenty of time i submitted this

4
00:00:15,759 --> 00:00:18,320
as a lightning talk and was allocated an

5
00:00:17,680 --> 00:00:21,279
hour so

6
00:00:18,320 --> 00:00:23,198
um if you have questions or comments or

7
00:00:21,279 --> 00:00:26,240
anything please feel free to

8
00:00:23,199 --> 00:00:26,240
to interrupt me and

9
00:00:26,400 --> 00:00:32,238
does anybody use prometheus already

10
00:00:29,519 --> 00:00:34,800
or it's new for everybody a few people

11
00:00:32,238 --> 00:00:38,640
okay you've heard of it

12
00:00:34,800 --> 00:00:40,959
great we got a

13
00:00:38,640 --> 00:00:43,200
prometheus exporter in freebc 12. so

14
00:00:40,960 --> 00:00:46,879
somebody in the freebsd world at least

15
00:00:43,200 --> 00:00:52,000
knows and uses prometheus apart from me

16
00:00:46,879 --> 00:00:55,440
so this was a few months ago i

17
00:00:52,000 --> 00:00:57,840
i wrote an exporter to to make

18
00:00:55,440 --> 00:00:58,800
disk utilization metrics available to

19
00:00:57,840 --> 00:01:02,000
prometheus

20
00:00:58,800 --> 00:01:03,919
from freebsd and i was

21
00:01:02,000 --> 00:01:06,240
encouraged to to come here and talk

22
00:01:03,920 --> 00:01:09,200
about it i did a blog post about it and

23
00:01:06,240 --> 00:01:10,839
was encouraged to come here and tell you

24
00:01:09,200 --> 00:01:13,840
so

25
00:01:10,840 --> 00:01:15,119
uh since

26
00:01:13,840 --> 00:01:17,119
many people probably don't know

27
00:01:15,119 --> 00:01:17,920
prometheus i'll start out by talking a

28
00:01:17,119 --> 00:01:21,119
little bit about

29
00:01:17,920 --> 00:01:21,840
what it is and what it does and why it's

30
00:01:21,119 --> 00:01:24,799
awesome

31
00:01:21,840 --> 00:01:25,200
and then the concept of exporters and

32
00:01:24,799 --> 00:01:27,759
then

33
00:01:25,200 --> 00:01:30,960
we'll get to gstat and the actual

34
00:01:27,759 --> 00:01:30,960
exporter that i wrote for that

35
00:01:31,119 --> 00:01:36,400
i also have a graffana dashboard to

36
00:01:34,240 --> 00:01:38,479
actually visualize the data connected

37
00:01:36,400 --> 00:01:40,479
and if everything works and my internet

38
00:01:38,479 --> 00:01:43,119
connection keeps working then

39
00:01:40,479 --> 00:01:44,159
i have a live demo of some servers at my

40
00:01:43,119 --> 00:01:46,240
workplace

41
00:01:44,159 --> 00:01:49,680
using this so it's always nice to see

42
00:01:46,240 --> 00:01:49,679
some actual real data

43
00:01:50,000 --> 00:01:53,759
but i'm not making any promises because

44
00:01:52,000 --> 00:01:56,000
my connection has been flagging so let's

45
00:01:53,759 --> 00:02:00,240
see when we get to it live demos always

46
00:01:56,000 --> 00:02:03,280
as you know a bit tricky sometimes so

47
00:02:00,240 --> 00:02:03,920
my name is thomas rasmussen i'm called

48
00:02:03,280 --> 00:02:07,439
twigling

49
00:02:03,920 --> 00:02:11,120
on the internet i'm

50
00:02:07,439 --> 00:02:15,760
born and live in copenhagen in denmark

51
00:02:11,120 --> 00:02:15,760
and i do system architecture

52
00:02:16,720 --> 00:02:24,319
build and administer freebsd servers

53
00:02:21,120 --> 00:02:25,520
i do some programming as well and was

54
00:02:24,319 --> 00:02:27,200
recently laid off

55
00:02:25,520 --> 00:02:29,200
with three months with salary so i'm

56
00:02:27,200 --> 00:02:30,958
just going to fuss them and

57
00:02:29,200 --> 00:02:32,640
relaxing and enjoying myself at the

58
00:02:30,959 --> 00:02:34,640
moment and looking for something else to

59
00:02:32,640 --> 00:02:36,799
do

60
00:02:34,640 --> 00:02:38,958
i run a dns service called uncensored

61
00:02:36,800 --> 00:02:42,000
dns for 10 years now

62
00:02:38,959 --> 00:02:43,760
11 years which is before

63
00:02:42,000 --> 00:02:47,680
google dns and before all the other

64
00:02:43,760 --> 00:02:49,599
public recursive dns server services

65
00:02:47,680 --> 00:02:51,360
i started this as an alternative to the

66
00:02:49,599 --> 00:02:54,720
isp sensor

67
00:02:51,360 --> 00:02:56,800
often censored isp dns servers

68
00:02:54,720 --> 00:02:59,120
and it's been running since i also help

69
00:02:56,800 --> 00:03:02,480
organize bonheck which is an annual

70
00:02:59,120 --> 00:03:04,239
danish hacker camp i highly recommend

71
00:03:02,480 --> 00:03:06,000
going it is a great time it's a week

72
00:03:04,239 --> 00:03:09,440
with temps and laptops and hacking

73
00:03:06,000 --> 00:03:12,720
around with all kinds of fun things

74
00:03:09,440 --> 00:03:16,319
and i have a bunch of minor

75
00:03:12,720 --> 00:03:18,800
open source projects on github

76
00:03:16,319 --> 00:03:20,560
i use freebsd exclusively on servers

77
00:03:18,800 --> 00:03:23,760
i've used this since

78
00:03:20,560 --> 00:03:24,799
5.2.1 i think was my my first installed

79
00:03:23,760 --> 00:03:27,120
server

80
00:03:24,799 --> 00:03:28,959
and on my laptops i run something called

81
00:03:27,120 --> 00:03:31,760
cubes os which is an

82
00:03:28,959 --> 00:03:33,519
awesome operating system i wish it was

83
00:03:31,760 --> 00:03:36,879
freebsd based but it's a

84
00:03:33,519 --> 00:03:37,599
fedora and scent based it's the concept

85
00:03:36,879 --> 00:03:39,440
is you have

86
00:03:37,599 --> 00:03:41,200
different virtual machines for different

87
00:03:39,440 --> 00:03:42,079
contexts so i have one for work and one

88
00:03:41,200 --> 00:03:43,599
for

89
00:03:42,080 --> 00:03:46,959
playing with the bond hack stuff and

90
00:03:43,599 --> 00:03:49,280
they can really disposable browsers

91
00:03:46,959 --> 00:03:50,400
for for surfing so the vm just

92
00:03:49,280 --> 00:03:52,400
disappears so if you

93
00:03:50,400 --> 00:03:53,519
happen to click something bad then just

94
00:03:52,400 --> 00:03:58,400
nothing really happens

95
00:03:53,519 --> 00:03:58,400
so awesome concept so

96
00:03:58,720 --> 00:04:02,799
i'm here to talk about this g-stat

97
00:04:00,799 --> 00:04:04,080
exporter

98
00:04:02,799 --> 00:04:06,319
this is actually the first time i've

99
00:04:04,080 --> 00:04:07,040
been in the psd room there's never been

100
00:04:06,319 --> 00:04:09,280
room but

101
00:04:07,040 --> 00:04:11,679
one of the perks of being a speaker is

102
00:04:09,280 --> 00:04:14,799
that you have to let me in

103
00:04:11,680 --> 00:04:17,440
so um i wrote this tweet

104
00:04:14,799 --> 00:04:19,120
and someone picked up on it and uh and

105
00:04:17,440 --> 00:04:20,560
recommended i come to uh

106
00:04:19,120 --> 00:04:22,639
to fuss them and talk about it and here

107
00:04:20,560 --> 00:04:25,520
we are so

108
00:04:22,639 --> 00:04:27,520
prometheus and in their own words it's

109
00:04:25,520 --> 00:04:29,359
an open source monitoring system

110
00:04:27,520 --> 00:04:32,479
dimensional data model flexible query

111
00:04:29,360 --> 00:04:35,759
language efficient time series database

112
00:04:32,479 --> 00:04:37,520
and that is pretty much it it is very

113
00:04:35,759 --> 00:04:39,440
the whole ecosystem around it is very

114
00:04:37,520 --> 00:04:42,560
unixy in that that they do

115
00:04:39,440 --> 00:04:43,199
one thing and they do it well prometheus

116
00:04:42,560 --> 00:04:47,120
in itself

117
00:04:43,199 --> 00:04:48,880
is is a time series database

118
00:04:47,120 --> 00:04:50,639
it really doesn't do anything except

119
00:04:48,880 --> 00:04:51,680
collect metrics and stick them in a time

120
00:04:50,639 --> 00:04:53,680
series database

121
00:04:51,680 --> 00:04:56,160
and then have it has an api to make them

122
00:04:53,680 --> 00:04:58,880
available to graphene for example so you

123
00:04:56,160 --> 00:05:00,639
can pull the data back out

124
00:04:58,880 --> 00:05:02,080
it doesn't do alerting it has something

125
00:05:00,639 --> 00:05:02,800
that's something called alert manager

126
00:05:02,080 --> 00:05:04,479
which

127
00:05:02,800 --> 00:05:06,479
from the same team but it's a separate

128
00:05:04,479 --> 00:05:08,880
separate piece of software

129
00:05:06,479 --> 00:05:09,599
separate configuration file and stuff

130
00:05:08,880 --> 00:05:13,360
which handles

131
00:05:09,600 --> 00:05:15,199
alerting and alert dependencies

132
00:05:13,360 --> 00:05:17,120
and which teams should get what alert

133
00:05:15,199 --> 00:05:19,360
and patient duty support and all that

134
00:05:17,120 --> 00:05:19,360
stuff

135
00:05:20,240 --> 00:05:25,280
monitoring has often i think been many

136
00:05:22,960 --> 00:05:28,400
years been

137
00:05:25,280 --> 00:05:30,320
it's been a bit of a like there was a

138
00:05:28,400 --> 00:05:32,960
void left when nagi has kind of

139
00:05:30,320 --> 00:05:35,039
stopped being modern and i've been i've

140
00:05:32,960 --> 00:05:36,719
been using sapix and a few other things

141
00:05:35,039 --> 00:05:38,479
but nothing has really been

142
00:05:36,720 --> 00:05:40,800
it's like they try to do too much

143
00:05:38,479 --> 00:05:43,120
somehow it's a big job both to

144
00:05:40,800 --> 00:05:44,479
gather the data and visualize it and do

145
00:05:43,120 --> 00:05:46,080
alerting and everything and i haven't

146
00:05:44,479 --> 00:05:48,639
really been happy with anything

147
00:05:46,080 --> 00:05:50,880
and i think you know like the first time

148
00:05:48,639 --> 00:05:52,320
you tried zfs and thought this is what a

149
00:05:50,880 --> 00:05:53,840
file system should have been like all

150
00:05:52,320 --> 00:05:54,400
along where have you been all my life

151
00:05:53,840 --> 00:05:55,919
this is

152
00:05:54,400 --> 00:05:57,919
how i feel about monitoring with

153
00:05:55,919 --> 00:06:01,120
prometheus it is an excellent

154
00:05:57,919 --> 00:06:03,198
system so

155
00:06:01,120 --> 00:06:04,880
it is based on they call it dimensional

156
00:06:03,199 --> 00:06:07,120
time series data

157
00:06:04,880 --> 00:06:07,919
it means that your metrics can have

158
00:06:07,120 --> 00:06:10,160
labels

159
00:06:07,919 --> 00:06:12,400
they call it so if you have a metric

160
00:06:10,160 --> 00:06:15,759
called http requests

161
00:06:12,400 --> 00:06:17,840
total for example for a web server then

162
00:06:15,759 --> 00:06:21,680
and you get a number when you when you

163
00:06:17,840 --> 00:06:23,758
scrape the data so you had 100 requests

164
00:06:21,680 --> 00:06:25,600
then that data can be can have multiple

165
00:06:23,759 --> 00:06:26,240
dimensions and that can have labels to

166
00:06:25,600 --> 00:06:28,000
say

167
00:06:26,240 --> 00:06:29,840
what the path of the request was or what

168
00:06:28,000 --> 00:06:33,360
the http status code was

169
00:06:29,840 --> 00:06:36,000
so you can ask to get only the http

170
00:06:33,360 --> 00:06:37,440
404 requests for example and then make a

171
00:06:36,000 --> 00:06:40,560
graph of those on your

172
00:06:37,440 --> 00:06:42,479
in your graphana this is incredibly

173
00:06:40,560 --> 00:06:44,720
powerful it's actually a

174
00:06:42,479 --> 00:06:45,758
relatively simple concept but it

175
00:06:44,720 --> 00:06:49,759
enriches the data

176
00:06:45,759 --> 00:06:50,240
greatly prometheus and alert manager and

177
00:06:49,759 --> 00:06:51,680
all the

178
00:06:50,240 --> 00:06:55,280
accompanying software has all written

179
00:06:51,680 --> 00:06:58,160
and go it was

180
00:06:55,280 --> 00:06:58,719
started at the soundcloud some years ago

181
00:06:58,160 --> 00:07:03,120
it's been

182
00:06:58,720 --> 00:07:05,360
open source since 2015 and there's no

183
00:07:03,120 --> 00:07:06,560
company that owns it now it's a regular

184
00:07:05,360 --> 00:07:08,800
open source project with

185
00:07:06,560 --> 00:07:10,720
many contributors and a great community

186
00:07:08,800 --> 00:07:11,280
a great ise channel and stuff so if you

187
00:07:10,720 --> 00:07:14,960
have

188
00:07:11,280 --> 00:07:14,960
questions they're very happy to help

189
00:07:15,120 --> 00:07:19,759
so this is the the diagram of the

190
00:07:17,360 --> 00:07:21,759
ecosystem from their website

191
00:07:19,759 --> 00:07:23,120
um central to the whole thing is the pro

192
00:07:21,759 --> 00:07:27,440
media server it's a

193
00:07:23,120 --> 00:07:30,000
time series database and it stores

194
00:07:27,440 --> 00:07:31,759
the matrix of course on some sort of

195
00:07:30,000 --> 00:07:33,919
disk storage

196
00:07:31,759 --> 00:07:35,360
it's a pull model prometheus that's one

197
00:07:33,919 --> 00:07:37,280
of the things that

198
00:07:35,360 --> 00:07:38,880
some people have to get used to which

199
00:07:37,280 --> 00:07:41,039
means that prometheus connects

200
00:07:38,880 --> 00:07:43,759
to the stuff it's monitoring and pulls

201
00:07:41,039 --> 00:07:45,280
the metrics over http

202
00:07:43,759 --> 00:07:46,879
various ways around that if you

203
00:07:45,280 --> 00:07:47,840
absolutely cannot do it like that but

204
00:07:46,879 --> 00:07:50,319
but that is the

205
00:07:47,840 --> 00:07:51,679
idea for example the freebsd system

206
00:07:50,319 --> 00:07:54,319
exporter that came and

207
00:07:51,680 --> 00:07:55,280
arrived in freebies g12 is supposed to

208
00:07:54,319 --> 00:07:58,400
be run

209
00:07:55,280 --> 00:08:02,479
in and the inaudib

210
00:07:58,400 --> 00:08:02,479
connects to it and pulls out the metrics

211
00:08:03,039 --> 00:08:06,240
so and that that's what the the

212
00:08:04,879 --> 00:08:09,360
exporters make available

213
00:08:06,240 --> 00:08:12,560
they open an http endpoint and

214
00:08:09,360 --> 00:08:13,360
it's just a plain text key value list of

215
00:08:12,560 --> 00:08:16,319
lines i'll

216
00:08:13,360 --> 00:08:16,319
have examples later

217
00:08:16,879 --> 00:08:20,400
there's something called push gateway

218
00:08:18,319 --> 00:08:22,400
that if you have a job that runs for an

219
00:08:20,400 --> 00:08:23,280
hour and then has some metrics to submit

220
00:08:22,400 --> 00:08:25,840
at the end of that

221
00:08:23,280 --> 00:08:26,318
you can use push gateway for that it has

222
00:08:25,840 --> 00:08:28,960
a very

223
00:08:26,319 --> 00:08:30,000
um extensive service discovery system so

224
00:08:28,960 --> 00:08:33,519
if you are

225
00:08:30,000 --> 00:08:35,200
aws kubernetes user or you use one of

226
00:08:33,519 --> 00:08:36,959
the many many cloud services available

227
00:08:35,200 --> 00:08:39,919
there's probably a discovery service

228
00:08:36,958 --> 00:08:43,119
discovery thing available for it

229
00:08:39,919 --> 00:08:46,480
at work i i worked on a

230
00:08:43,120 --> 00:08:48,399
worked in an isp until i got laid off

231
00:08:46,480 --> 00:08:50,080
we used the file service discovery thing

232
00:08:48,399 --> 00:08:51,360
and just exported a list of customers

233
00:08:50,080 --> 00:08:53,440
and ip addresses and that was

234
00:08:51,360 --> 00:08:55,120
automatically ingested by prometheus so

235
00:08:53,440 --> 00:08:56,640
when we add a new customer in our

236
00:08:55,120 --> 00:08:58,399
central provisioning system it

237
00:08:56,640 --> 00:09:01,839
automatically shows up and

238
00:08:58,399 --> 00:09:05,680
starts pulling the data from it

239
00:09:01,839 --> 00:09:09,519
and to

240
00:09:05,680 --> 00:09:12,560
to talk to prometheus and and

241
00:09:09,519 --> 00:09:14,959
query the data get it out they

242
00:09:12,560 --> 00:09:16,880
invented something called prom cool it

243
00:09:14,959 --> 00:09:18,239
is

244
00:09:16,880 --> 00:09:20,720
what doesn't really have anything to do

245
00:09:18,240 --> 00:09:23,120
with sql it is a

246
00:09:20,720 --> 00:09:24,640
custom query language specifically for

247
00:09:23,120 --> 00:09:26,240
this and that will be examples of this

248
00:09:24,640 --> 00:09:29,600
later and it is a very powerful

249
00:09:26,240 --> 00:09:31,440
and easy to use language

250
00:09:29,600 --> 00:09:33,920
and like i said alert manager is a

251
00:09:31,440 --> 00:09:37,519
separate component so when

252
00:09:33,920 --> 00:09:38,880
when you add some

253
00:09:37,519 --> 00:09:41,360
threshold or something you want to be

254
00:09:38,880 --> 00:09:43,439
alerted on in prometheus

255
00:09:41,360 --> 00:09:44,800
it pushes an alert to alert manager an

256
00:09:43,440 --> 00:09:47,519
alert manager handles

257
00:09:44,800 --> 00:09:49,439
all the nitty-gritty with who's on

258
00:09:47,519 --> 00:09:50,880
vacation now and who has the duty and

259
00:09:49,440 --> 00:09:53,360
who does what

260
00:09:50,880 --> 00:09:56,000
so all that complexity is separate from

261
00:09:53,360 --> 00:09:56,000
prometheus

262
00:09:59,040 --> 00:10:04,399
okay so we generally speak

263
00:10:02,320 --> 00:10:06,800
of black box and white box monitoring

264
00:10:04,399 --> 00:10:10,079
black box monitoring is when you

265
00:10:06,800 --> 00:10:11,760
ping your whatever or just

266
00:10:10,079 --> 00:10:14,079
poke something from the outside check if

267
00:10:11,760 --> 00:10:15,519
it's listening on port 80 or

268
00:10:14,079 --> 00:10:17,199
whatever you might do white box

269
00:10:15,519 --> 00:10:19,600
monitoring is when the

270
00:10:17,200 --> 00:10:20,640
the thing being monitored is kind of in

271
00:10:19,600 --> 00:10:24,160
the game

272
00:10:20,640 --> 00:10:26,079
so so for example uh

273
00:10:24,160 --> 00:10:28,160
the freebsd systems alex powder is a

274
00:10:26,079 --> 00:10:30,880
good example or

275
00:10:28,160 --> 00:10:31,760
an exporter for yeah for gstat data for

276
00:10:30,880 --> 00:10:33,760
example

277
00:10:31,760 --> 00:10:36,000
means you're kind of inside the system

278
00:10:33,760 --> 00:10:38,880
exposing the metrics to the outside

279
00:10:36,000 --> 00:10:40,880
and that's a much better way to do it

280
00:10:38,880 --> 00:10:43,200
than blackbox monitoring

281
00:10:40,880 --> 00:10:44,320
it has support for that as well if if

282
00:10:43,200 --> 00:10:46,399
you need to

283
00:10:44,320 --> 00:10:48,079
like we do at work ping seven thousand

284
00:10:46,399 --> 00:10:49,519
customers every five seconds then

285
00:10:48,079 --> 00:10:51,279
you do that with uh something called

286
00:10:49,519 --> 00:10:53,519
black box exporter but

287
00:10:51,279 --> 00:10:54,480
but mostly and primarily it is white box

288
00:10:53,519 --> 00:10:58,160
monitoring it is

289
00:10:54,480 --> 00:10:58,160
supposed to be used uh

290
00:10:58,399 --> 00:11:02,079
with uh with the exporters built into

291
00:11:00,720 --> 00:11:05,839
whatever you are monitoring

292
00:11:02,079 --> 00:11:05,839
so if you have a system like our

293
00:11:06,000 --> 00:11:11,040
provisioning system at work it's easy to

294
00:11:08,959 --> 00:11:14,239
for example

295
00:11:11,040 --> 00:11:15,839
export metrics for the queue length for

296
00:11:14,240 --> 00:11:17,920
provisioning the customers

297
00:11:15,839 --> 00:11:19,040
the router needs to be the switch ports

298
00:11:17,920 --> 00:11:20,000
need to be configured and there's

299
00:11:19,040 --> 00:11:23,040
sometimes a queue

300
00:11:20,000 --> 00:11:24,160
if there's many changes at once and it's

301
00:11:23,040 --> 00:11:26,480
very easy when you're

302
00:11:24,160 --> 00:11:27,760
in the code to export this metric and

303
00:11:26,480 --> 00:11:30,880
just

304
00:11:27,760 --> 00:11:34,399
do a slash prometheus or slash matrix

305
00:11:30,880 --> 00:11:38,880
endpoint in your in your system and

306
00:11:34,399 --> 00:11:41,839
export it it's over http like i said

307
00:11:38,880 --> 00:11:43,360
and it is designed to to handle

308
00:11:41,839 --> 00:11:45,279
everything you can throw at it

309
00:11:43,360 --> 00:11:48,320
you don't shouldn't be picky you should

310
00:11:45,279 --> 00:11:50,320
instrument all parts of your system uh

311
00:11:48,320 --> 00:11:51,680
anything that might someday make sense

312
00:11:50,320 --> 00:11:53,040
and even if you think it doesn't because

313
00:11:51,680 --> 00:11:55,439
sometimes these

314
00:11:53,040 --> 00:11:56,719
subtle little changes in whatever can it

315
00:11:55,440 --> 00:11:58,160
turn out to be

316
00:11:56,720 --> 00:12:00,160
to be important and then it's very nice

317
00:11:58,160 --> 00:12:02,240
to have the history it is

318
00:12:00,160 --> 00:12:04,800
it is very efficient it is designed to

319
00:12:02,240 --> 00:12:07,440
handle hundreds of thousands of metrics

320
00:12:04,800 --> 00:12:11,839
and on a single

321
00:12:07,440 --> 00:12:14,639
single server commodity hardware

322
00:12:11,839 --> 00:12:16,079
it doesn't do it has a little bit of you

323
00:12:14,639 --> 00:12:18,240
can when you're exploring the data in

324
00:12:16,079 --> 00:12:20,399
the prometheus web interface it can draw

325
00:12:18,240 --> 00:12:21,519
a simple crude graph but it's not meant

326
00:12:20,399 --> 00:12:23,360
for graphing it's

327
00:12:21,519 --> 00:12:25,920
meant to be used with gravina or

328
00:12:23,360 --> 00:12:30,320
something else but i highly recommend

329
00:12:25,920 --> 00:12:31,760
it is excellent integrated into graphata

330
00:12:30,320 --> 00:12:33,760
there's client libraries in many

331
00:12:31,760 --> 00:12:35,360
languages the g-static port i wrote is

332
00:12:33,760 --> 00:12:36,480
written in python and you start out with

333
00:12:35,360 --> 00:12:37,760
these

334
00:12:36,480 --> 00:12:39,920
10 lines or something from the

335
00:12:37,760 --> 00:12:41,519
prometheus example client and you just

336
00:12:39,920 --> 00:12:43,439
add

337
00:12:41,519 --> 00:12:45,519
name define your metrics saying we have

338
00:12:43,440 --> 00:12:48,320
a in g stat we have a

339
00:12:45,519 --> 00:12:49,440
disk i o busy present message metric or

340
00:12:48,320 --> 00:12:50,959
whatever and

341
00:12:49,440 --> 00:12:54,079
and you know feed data into them and it

342
00:12:50,959 --> 00:12:56,959
takes care of listening on the on http

343
00:12:54,079 --> 00:12:58,959
and and serving the data up in the in

344
00:12:56,959 --> 00:13:00,319
the right order and it's uh

345
00:12:58,959 --> 00:13:02,959
it is easy to use and they have of

346
00:13:00,320 --> 00:13:05,519
course in go and many other languages

347
00:13:02,959 --> 00:13:06,000
client libraries so it is easy to get

348
00:13:05,519 --> 00:13:09,120
started

349
00:13:06,000 --> 00:13:09,120
riding your own exporter

350
00:13:09,839 --> 00:13:13,519
so it um it is like i said very

351
00:13:12,480 --> 00:13:15,200
efficient

352
00:13:13,519 --> 00:13:16,880
they say about three and a half pipes

353
00:13:15,200 --> 00:13:20,079
per data point

354
00:13:16,880 --> 00:13:21,040
and it is remarkably efficient when

355
00:13:20,079 --> 00:13:22,959
pulling in data

356
00:13:21,040 --> 00:13:25,199
we went to we replaced our cyborgs

357
00:13:22,959 --> 00:13:27,680
installation which was

358
00:13:25,200 --> 00:13:28,399
thrashing the discs using a postgres as

359
00:13:27,680 --> 00:13:31,760
a back-end

360
00:13:28,399 --> 00:13:34,880
i love postgres i don't love cervix but

361
00:13:31,760 --> 00:13:36,240
it was a decent setup but it was i mean

362
00:13:34,880 --> 00:13:39,600
we could barely

363
00:13:36,240 --> 00:13:41,680
handle polling them once per minute

364
00:13:39,600 --> 00:13:43,600
the five eight thousand customers or

365
00:13:41,680 --> 00:13:45,279
however much we had back then

366
00:13:43,600 --> 00:13:47,760
and when we switched to prometheus we're

367
00:13:45,279 --> 00:13:50,320
polling them once every five seconds

368
00:13:47,760 --> 00:13:52,079
we're doing additional metrics besides

369
00:13:50,320 --> 00:13:54,160
what we did with cybex and the server is

370
00:13:52,079 --> 00:13:56,479
bought now it's not doing nothing

371
00:13:54,160 --> 00:13:57,839
so we went from absolutely threshing the

372
00:13:56,480 --> 00:14:00,320
drives to

373
00:13:57,839 --> 00:14:02,000
the server being almost idle it is

374
00:14:00,320 --> 00:14:04,959
amazingly

375
00:14:02,000 --> 00:14:06,560
efficient if you go beyond what one

376
00:14:04,959 --> 00:14:08,800
server can handle

377
00:14:06,560 --> 00:14:10,160
it supports sharding and federation so

378
00:14:08,800 --> 00:14:11,599
you can have many prometheus servers

379
00:14:10,160 --> 00:14:13,439
working together

380
00:14:11,600 --> 00:14:14,720
and it does that very very well and the

381
00:14:13,440 --> 00:14:16,480
alert manager as well

382
00:14:14,720 --> 00:14:19,120
supports high availability stuff so you

383
00:14:16,480 --> 00:14:20,800
can send an alert to any number of alert

384
00:14:19,120 --> 00:14:21,120
manager instances and only one of them

385
00:14:20,800 --> 00:14:24,560
will

386
00:14:21,120 --> 00:14:28,160
actually alert you at

387
00:14:24,560 --> 00:14:30,638
work the prometheus

388
00:14:28,160 --> 00:14:33,519
installation i was managing this had

389
00:14:30,639 --> 00:14:35,440
about 750 000 time series

390
00:14:33,519 --> 00:14:38,160
and about 13 and a half thousand per

391
00:14:35,440 --> 00:14:39,920
second metrics put into the database and

392
00:14:38,160 --> 00:14:42,079
that's small in prometheus terms that's

393
00:14:39,920 --> 00:14:42,079
a

394
00:14:42,959 --> 00:14:47,760
not an issue at all you can do that on a

395
00:14:44,560 --> 00:14:50,800
laptop or on whatever

396
00:14:47,760 --> 00:14:51,279
it's a in a jail all my stuff runs in

397
00:14:50,800 --> 00:14:54,079
sales

398
00:14:51,279 --> 00:14:55,839
and you can easily have like one for

399
00:14:54,079 --> 00:14:57,599
each team it's not like

400
00:14:55,839 --> 00:14:59,199
managing an elk instance or something

401
00:14:57,600 --> 00:15:01,519
like that where you have to

402
00:14:59,199 --> 00:15:02,560
you can just wave up a new if you if you

403
00:15:01,519 --> 00:15:03,920
want one for

404
00:15:02,560 --> 00:15:05,279
the network team and one for the

405
00:15:03,920 --> 00:15:13,839
operations team or whatever you can

406
00:15:05,279 --> 00:15:13,839
easily do that um

407
00:15:15,519 --> 00:15:23,279
okay so the query language

408
00:15:18,639 --> 00:15:24,800
from ql uh i brought a few few examples

409
00:15:23,279 --> 00:15:27,839
and it's easy to look up

410
00:15:24,800 --> 00:15:29,758
it has

411
00:15:27,839 --> 00:15:31,279
a page or two and then your if you

412
00:15:29,759 --> 00:15:34,320
understand basic uh

413
00:15:31,279 --> 00:15:36,480
basic statistics and math then then it

414
00:15:34,320 --> 00:15:39,920
should be easy to use

415
00:15:36,480 --> 00:15:43,440
first things first http requests total

416
00:15:39,920 --> 00:15:47,360
is the example metric we're using here

417
00:15:43,440 --> 00:15:50,959
so that's a that's that was the

418
00:15:47,360 --> 00:15:50,959
internet connection that stopped working

419
00:15:52,839 --> 00:15:58,399
um so that just returns the metric http

420
00:15:56,240 --> 00:16:00,079
request total

421
00:15:58,399 --> 00:16:02,160
if for example i want the metric but

422
00:16:00,079 --> 00:16:05,040
only if the job label

423
00:16:02,160 --> 00:16:06,399
is api server and the handler is slave

424
00:16:05,040 --> 00:16:09,519
apis less comments

425
00:16:06,399 --> 00:16:12,639
then i put it in the curly brackets and

426
00:16:09,519 --> 00:16:14,639
it only returns

427
00:16:12,639 --> 00:16:16,639
the number of http requests that had

428
00:16:14,639 --> 00:16:21,040
those

429
00:16:16,639 --> 00:16:22,800
those labels you can also

430
00:16:21,040 --> 00:16:24,639
return return arrangements you get you

431
00:16:22,800 --> 00:16:25,839
get all the values for given for for

432
00:16:24,639 --> 00:16:28,639
example for a five minute

433
00:16:25,839 --> 00:16:30,639
interval uh if you need that uh you can

434
00:16:28,639 --> 00:16:34,240
use regular expressions for the labels

435
00:16:30,639 --> 00:16:36,720
and um and you can do

436
00:16:34,240 --> 00:16:38,720
fuzzy matching and stuff for the uh for

437
00:16:36,720 --> 00:16:40,560
the label values as well

438
00:16:38,720 --> 00:16:42,800
i use those labels a lot in the g-stat

439
00:16:40,560 --> 00:16:45,439
exporter to save stuff like the disks

440
00:16:42,800 --> 00:16:47,359
serial numbers and stuff we'll get to

441
00:16:45,440 --> 00:16:50,160
that later

442
00:16:47,360 --> 00:16:51,360
um a bit more advanced it has it has

443
00:16:50,160 --> 00:16:53,759
some keywords to do

444
00:16:51,360 --> 00:16:55,519
uh for example uh it can calculate uh

445
00:16:53,759 --> 00:16:59,279
the rate over five minutes

446
00:16:55,519 --> 00:17:02,399
uh for for this metric or even do

447
00:16:59,279 --> 00:17:05,439
a quantile uh 95th

448
00:17:02,399 --> 00:17:07,119
percentile is used a lot when

449
00:17:05,439 --> 00:17:08,640
when charging for bandwidth and hosting

450
00:17:07,119 --> 00:17:12,239
operations and stuff like that or

451
00:17:08,640 --> 00:17:16,959
wherever you need to to use it

452
00:17:12,240 --> 00:17:16,959
and it is it is very flexible

453
00:17:17,039 --> 00:17:20,240
it can also show uh the you can query

454
00:17:19,919 --> 00:17:22,000
the

455
00:17:20,240 --> 00:17:23,039
the alert the number of alerts firing in

456
00:17:22,000 --> 00:17:24,000
the system and when when the

457
00:17:23,039 --> 00:17:25,280
installation grows

458
00:17:24,000 --> 00:17:27,119
that's the sort of stuff you want on

459
00:17:25,280 --> 00:17:28,240
your front page of your graffana

460
00:17:27,119 --> 00:17:33,840
instance you can have

461
00:17:28,240 --> 00:17:33,840
zero or five alerts or whatever

462
00:17:34,559 --> 00:17:40,080
okay so

463
00:17:37,600 --> 00:17:41,039
prometheus connects to an exporter and

464
00:17:40,080 --> 00:17:44,960
gets

465
00:17:41,039 --> 00:17:49,679
fed metrics and ingests them and these

466
00:17:44,960 --> 00:17:52,960
metrics are exported by by exporters

467
00:17:49,679 --> 00:17:54,080
machine metrics like like the normal

468
00:17:52,960 --> 00:17:56,559
metrics like

469
00:17:54,080 --> 00:17:57,918
ram and and i o and network traffic and

470
00:17:56,559 --> 00:17:59,520
stuff like that

471
00:17:57,919 --> 00:18:01,919
are exported by something called node

472
00:17:59,520 --> 00:18:04,000
exporter which we have in ports

473
00:18:01,919 --> 00:18:05,840
and it works well but on freebsc it

474
00:18:04,000 --> 00:18:08,720
doesn't have a

475
00:18:05,840 --> 00:18:09,760
disk utilization metrics that has thus

476
00:18:08,720 --> 00:18:12,880
on linux

477
00:18:09,760 --> 00:18:14,640
of course but but not on previously

478
00:18:12,880 --> 00:18:17,280
which is why i started out because when

479
00:18:14,640 --> 00:18:18,880
you're troubleshooting stuff on a

480
00:18:17,280 --> 00:18:20,799
database server or something it's very

481
00:18:18,880 --> 00:18:23,520
nice to have metrics for for disk

482
00:18:20,799 --> 00:18:23,520
utilization

483
00:18:26,559 --> 00:18:31,760
there's exporters for every type of

484
00:18:29,760 --> 00:18:32,400
software you can almost imagine it it is

485
00:18:31,760 --> 00:18:35,440
uh there's

486
00:18:32,400 --> 00:18:37,520
hundreds of them uh

487
00:18:35,440 --> 00:18:39,039
each is allocated a port on this list

488
00:18:37,520 --> 00:18:39,520
you just go to the wiki and allocator

489
00:18:39,039 --> 00:18:43,120
port and

490
00:18:39,520 --> 00:18:43,600
for the gstat exporter i got port 92

491
00:18:43,120 --> 00:18:46,959
something

492
00:18:43,600 --> 00:18:48,399
something but but that's a good list and

493
00:18:46,960 --> 00:18:49,520
you can search for freebsd on it and

494
00:18:48,400 --> 00:18:51,280
find a few others that

495
00:18:49,520 --> 00:18:52,639
there's a jail exporter which is very

496
00:18:51,280 --> 00:18:56,960
nice that somebody made

497
00:18:52,640 --> 00:18:58,480
that uses rctl to get iops and memory

498
00:18:56,960 --> 00:19:01,440
use and stuff for

499
00:18:58,480 --> 00:19:02,080
for jails and then exports it in a in a

500
00:19:01,440 --> 00:19:03,679
way so

501
00:19:02,080 --> 00:19:05,760
prometheus can ingest it so you can get

502
00:19:03,679 --> 00:19:08,160
graphs to say how much

503
00:19:05,760 --> 00:19:09,440
ram each jail is using and how many iops

504
00:19:08,160 --> 00:19:10,640
and stuff that is very nice if you're

505
00:19:09,440 --> 00:19:14,320
using jails for

506
00:19:10,640 --> 00:19:14,320
yeah if you're using jails at all

507
00:19:19,280 --> 00:19:25,360
okay so you know i guess you all know

508
00:19:22,840 --> 00:19:28,480
g-step

509
00:19:25,360 --> 00:19:32,320
it looks like this it is a top

510
00:19:28,480 --> 00:19:36,799
style thing that shows

511
00:19:32,320 --> 00:19:39,360
the the geom devices and the

512
00:19:36,799 --> 00:19:40,160
read and write operations by default you

513
00:19:39,360 --> 00:19:43,439
can show

514
00:19:40,160 --> 00:19:46,880
deletes and other stuff as well um

515
00:19:43,440 --> 00:19:49,679
i actually have no wait i don't because

516
00:19:46,880 --> 00:19:49,679
i'm not online

517
00:19:50,720 --> 00:19:54,320
okay so this is g stat and these these

518
00:19:52,960 --> 00:19:55,520
are the numbers i was interested in

519
00:19:54,320 --> 00:20:00,960
getting into a graph

520
00:19:55,520 --> 00:20:00,960
and um uh that's why we're here today

521
00:20:01,039 --> 00:20:07,919
um there's a c

522
00:20:04,720 --> 00:20:10,559
flag for for g-step that makes it output

523
00:20:07,919 --> 00:20:11,520
in csv mode it's called in the man page

524
00:20:10,559 --> 00:20:14,559
instead of

525
00:20:11,520 --> 00:20:17,760
the top style display

526
00:20:14,559 --> 00:20:21,200
and unfortunately

527
00:20:17,760 --> 00:20:24,480
a bit of a complaint would be that it

528
00:20:21,200 --> 00:20:27,760
it it when you enable

529
00:20:24,480 --> 00:20:29,280
c it also enables endless mode so it

530
00:20:27,760 --> 00:20:33,360
keeps running you can't just make it

531
00:20:29,280 --> 00:20:36,559
output values once and exit so that's uh

532
00:20:33,360 --> 00:20:37,918
that should probably be added because

533
00:20:36,559 --> 00:20:41,678
usually in

534
00:20:37,919 --> 00:20:44,320
exporters when prometheus connects you

535
00:20:41,679 --> 00:20:44,799
scrape the data and then you return it

536
00:20:44,320 --> 00:20:47,120
but

537
00:20:44,799 --> 00:20:48,960
i've had to keep it running endlessly

538
00:20:47,120 --> 00:20:49,918
the g-stat and just streaming the output

539
00:20:48,960 --> 00:20:52,400
from it into the

540
00:20:49,919 --> 00:20:52,400
collector

541
00:20:53,039 --> 00:20:56,879
also if g-stat could somehow export the

542
00:20:56,240 --> 00:21:00,960
counters

543
00:20:56,880 --> 00:21:02,559
it bases the the top display on

544
00:21:00,960 --> 00:21:04,240
so these are you know the difference

545
00:21:02,559 --> 00:21:08,080
between counters and gauges

546
00:21:04,240 --> 00:21:10,480
uh and and we lose a bit of precision

547
00:21:08,080 --> 00:21:12,559
because i'm reading the g-step values as

548
00:21:10,480 --> 00:21:14,640
counters it shows me how much

549
00:21:12,559 --> 00:21:16,240
there's an update frequency in g-step so

550
00:21:14,640 --> 00:21:17,440
every five seconds or every one second

551
00:21:16,240 --> 00:21:20,159
or something

552
00:21:17,440 --> 00:21:21,520
and and stuff that happens in between

553
00:21:20,159 --> 00:21:22,799
let's say every five seconds

554
00:21:21,520 --> 00:21:24,480
is kind of lost and if you had the

555
00:21:22,799 --> 00:21:25,120
counters instead prometheus would be

556
00:21:24,480 --> 00:21:28,080
able to

557
00:21:25,120 --> 00:21:28,559
it doesn't matter if you read every five

558
00:21:28,080 --> 00:21:30,158
or

559
00:21:28,559 --> 00:21:32,158
ten seconds then you would get all the

560
00:21:30,159 --> 00:21:33,760
information and

561
00:21:32,159 --> 00:21:35,440
i'm not a c person but looking at the

562
00:21:33,760 --> 00:21:37,840
code it looks like it is counters

563
00:21:35,440 --> 00:21:39,520
when it comes out of the kernel and and

564
00:21:37,840 --> 00:21:41,760
somewhere it is

565
00:21:39,520 --> 00:21:45,840
converted to the to the human-friendly

566
00:21:41,760 --> 00:21:45,840
display that we see in z-stat

567
00:21:46,960 --> 00:21:51,679
okay so with the with with g-step we

568
00:21:51,039 --> 00:21:53,840
really have

569
00:21:51,679 --> 00:21:55,919
everything we need with the c-mode we

570
00:21:53,840 --> 00:21:59,039
have everything we need to write in a

571
00:21:55,919 --> 00:22:00,320
write-in exporter um

572
00:21:59,039 --> 00:22:02,559
like i said the client library

573
00:22:00,320 --> 00:22:03,840
prometheus client libraries is easy to

574
00:22:02,559 --> 00:22:07,360
get started with

575
00:22:03,840 --> 00:22:09,918
the g-stat exporter is 240 lines

576
00:22:07,360 --> 00:22:11,520
it sounds like more than this because

577
00:22:09,919 --> 00:22:13,919
it's formatted

578
00:22:11,520 --> 00:22:15,039
with these code beautifying tools and

579
00:22:13,919 --> 00:22:18,240
they are

580
00:22:15,039 --> 00:22:21,919
very generous with the new lines so

581
00:22:18,240 --> 00:22:25,200
um it's on github and yeah port 9248

582
00:22:21,919 --> 00:22:27,440
is uh it's a g static portal it's not a

583
00:22:25,200 --> 00:22:29,280
they don't add them to services files

584
00:22:27,440 --> 00:22:31,120
everywhere because they're so

585
00:22:29,280 --> 00:22:33,280
dynamic and there's so many of them but

586
00:22:31,120 --> 00:22:35,199
but they do have the uh

587
00:22:33,280 --> 00:22:36,879
the page on their website or the wiki on

588
00:22:35,200 --> 00:22:38,840
github to

589
00:22:36,880 --> 00:22:40,799
to keep track of which ports are used by

590
00:22:38,840 --> 00:22:44,480
what

591
00:22:40,799 --> 00:22:47,679
okay so running the exporter locally

592
00:22:44,480 --> 00:22:50,960
and and grabbing for

593
00:22:47,679 --> 00:22:51,440
da zero and grabbing for right this is

594
00:22:50,960 --> 00:22:55,280
the

595
00:22:51,440 --> 00:22:56,960
output for for the g-stat exporter you

596
00:22:55,280 --> 00:22:57,280
see it's just a plain text and this is

597
00:22:56,960 --> 00:23:00,080
just

598
00:22:57,280 --> 00:23:01,678
fetch outputting to uh to std out so

599
00:23:00,080 --> 00:23:02,720
it's a it's a plain sex response and

600
00:23:01,679 --> 00:23:05,919
it's just

601
00:23:02,720 --> 00:23:08,880
lines of metric name and then

602
00:23:05,919 --> 00:23:10,559
labels and then finally a space and then

603
00:23:08,880 --> 00:23:12,000
a number

604
00:23:10,559 --> 00:23:14,320
which is the actual value for that

605
00:23:12,000 --> 00:23:17,840
metric with that combination of labels

606
00:23:14,320 --> 00:23:17,840
for that specific time

607
00:23:19,120 --> 00:23:23,199
as you can see i've stuffed all the the

608
00:23:21,840 --> 00:23:26,240
info i can get from

609
00:23:23,200 --> 00:23:28,559
geom what's it called geom

610
00:23:26,240 --> 00:23:30,000
identifier or something there's a geom

611
00:23:28,559 --> 00:23:33,200
command that can return

612
00:23:30,000 --> 00:23:34,400
rpm and all these neat metrics so i can

613
00:23:33,200 --> 00:23:36,640
get

614
00:23:34,400 --> 00:23:38,799
yeah for example this exercise and rpm

615
00:23:36,640 --> 00:23:42,400
and the serial number for the disks and

616
00:23:38,799 --> 00:23:44,559
the size which is very nice because as

617
00:23:42,400 --> 00:23:47,279
we will see later in grafena it uh

618
00:23:44,559 --> 00:23:49,360
it makes it easy to to filter for

619
00:23:47,279 --> 00:23:51,440
example and show only the nvme drives

620
00:23:49,360 --> 00:23:53,439
only the spinning drives or

621
00:23:51,440 --> 00:23:55,760
only the 12 terabyte drives or whatever

622
00:23:53,440 --> 00:23:55,760
you need

623
00:24:00,159 --> 00:24:04,400
okay so as i said it doesn't visualize

624
00:24:03,840 --> 00:24:06,879
stuff but

625
00:24:04,400 --> 00:24:08,480
by itself but the the uh the integration

626
00:24:06,880 --> 00:24:11,840
superfan is really good

627
00:24:08,480 --> 00:24:13,600
it can uh once you add the prometheus

628
00:24:11,840 --> 00:24:15,439
instance as a data source in graffana

629
00:24:13,600 --> 00:24:16,639
when you start typing a metric name it

630
00:24:15,440 --> 00:24:21,039
autocompletes it and

631
00:24:16,640 --> 00:24:21,039
it it really works very very well

632
00:24:21,600 --> 00:24:25,279
and you can filter dashboard by label

633
00:24:23,840 --> 00:24:25,600
values and i'll show you what i mean by

634
00:24:25,279 --> 00:24:27,360
that

635
00:24:25,600 --> 00:24:28,879
later but that for example means that i

636
00:24:27,360 --> 00:24:31,600
can easily in graffana

637
00:24:28,880 --> 00:24:33,200
just select one disk if i'm if i'm

638
00:24:31,600 --> 00:24:37,199
suspecting that one disk is

639
00:24:33,200 --> 00:24:38,799
foggy or select only the 5400 ipm drives

640
00:24:37,200 --> 00:24:41,120
across all servers if i

641
00:24:38,799 --> 00:24:42,559
should for some reason have the need for

642
00:24:41,120 --> 00:24:45,439
that

643
00:24:42,559 --> 00:24:46,480
i published the dashboard at at the

644
00:24:45,440 --> 00:24:50,080
gravana

645
00:24:46,480 --> 00:24:52,159
dashboard publishing place and

646
00:24:50,080 --> 00:24:54,158
and if you don't know gravana i highly

647
00:24:52,159 --> 00:24:56,880
recommend it it is hope

648
00:24:54,159 --> 00:24:57,919
it is uh it's very easy to make it makes

649
00:24:56,880 --> 00:25:00,640
pretty graphs for you

650
00:24:57,919 --> 00:25:02,159
like you don't have to decide on colors

651
00:25:00,640 --> 00:25:04,400
and stuff it uses colors that look

652
00:25:02,159 --> 00:25:07,360
well together and uh and generally just

653
00:25:04,400 --> 00:25:09,600
works extremely well

654
00:25:07,360 --> 00:25:10,799
okay i'm gonna need to uh fix my

655
00:25:09,600 --> 00:25:12,480
internet connection so

656
00:25:10,799 --> 00:25:15,840
talk about yourself for 30 seconds while

657
00:25:12,480 --> 00:25:15,840
i get this

658
00:25:20,090 --> 00:25:23,149
[Music]

659
00:25:40,840 --> 00:25:43,840
okay

660
00:26:13,200 --> 00:26:25,840
just work for the love of sorry

661
00:26:49,279 --> 00:26:53,520
yes i would if i could it's a long and

662
00:26:52,880 --> 00:26:57,279
boring

663
00:26:53,520 --> 00:26:57,279
explanation why it doesn't work but

664
00:26:57,440 --> 00:27:02,400
and usually the phone just works but of

665
00:26:59,039 --> 00:27:02,400
course right now it doesn't

666
00:27:04,840 --> 00:27:07,840
is

667
00:27:10,559 --> 00:27:21,840
yeah thanks guys but it's not gonna work

668
00:27:40,080 --> 00:27:42,399
really

669
00:27:43,279 --> 00:27:49,840
this is great advertisement for linux

670
00:27:56,720 --> 00:27:59,360
it really isn't

671
00:28:00,320 --> 00:28:05,918
i'm glad i want that this might happen

672
00:28:03,200 --> 00:28:05,919
ahead of time

673
00:28:14,840 --> 00:28:17,840
yes

674
00:28:22,159 --> 00:28:27,440
okay so maybe we skip the live demo

675
00:28:25,840 --> 00:28:28,959
since it's not working i'm sorry about

676
00:28:27,440 --> 00:28:31,840
that

677
00:28:28,960 --> 00:28:31,840
um

678
00:28:32,080 --> 00:28:34,960
i'll talk a bit more and then try again

679
00:28:33,520 --> 00:28:36,399
in a few minutes when it's had time to

680
00:28:34,960 --> 00:28:40,000
recover

681
00:28:36,399 --> 00:28:41,439
uh i do have some uh

682
00:28:40,000 --> 00:28:43,279
some screenshots in the beginning that i

683
00:28:41,440 --> 00:28:46,960
can go back to but uh

684
00:28:43,279 --> 00:28:48,559
oh yeah thanks um it was a pretty quick

685
00:28:46,960 --> 00:28:52,320
project i did it in a day

686
00:28:48,559 --> 00:28:54,240
uh saturday in the morning investigating

687
00:28:52,320 --> 00:28:56,158
how i would do it and then i wrote the

688
00:28:54,240 --> 00:28:58,240
code during the day and in the evening i

689
00:28:56,159 --> 00:28:59,840
did the dashboard and

690
00:28:58,240 --> 00:29:01,600
then i wrote a blog post and posted it

691
00:28:59,840 --> 00:29:03,678
so it was that

692
00:29:01,600 --> 00:29:04,959
in the scope of the day you can you can

693
00:29:03,679 --> 00:29:05,600
write an exporter for something

694
00:29:04,960 --> 00:29:07,360
including

695
00:29:05,600 --> 00:29:09,918
doing a nice dashboard and making

696
00:29:07,360 --> 00:29:13,360
everything work

697
00:29:09,919 --> 00:29:15,679
if gsat gets support for for outputting

698
00:29:13,360 --> 00:29:18,158
just a line and the next hitting i'll

699
00:29:15,679 --> 00:29:20,000
probably change the way it calls g-step

700
00:29:18,159 --> 00:29:21,279
so it doesn't keep it running

701
00:29:20,000 --> 00:29:24,880
perpetually but just

702
00:29:21,279 --> 00:29:24,880
cause it when it is scraped

703
00:29:25,679 --> 00:29:33,039
and maybe good airflex to

704
00:29:29,760 --> 00:29:35,600
support if there's an effort

705
00:29:33,039 --> 00:29:36,480
gstat can show consumers as well as

706
00:29:35,600 --> 00:29:39,678
producers

707
00:29:36,480 --> 00:29:41,600
in the gm stack and

708
00:29:39,679 --> 00:29:44,559
and some people might need this for

709
00:29:41,600 --> 00:29:46,639
reasons i can't really

710
00:29:44,559 --> 00:29:47,100
the graph hana dashboard needs some fine

711
00:29:46,640 --> 00:29:48,240
tuning

712
00:29:47,100 --> 00:29:54,320
[Music]

713
00:29:48,240 --> 00:29:56,000
and if anybody wants to oh off

714
00:29:54,320 --> 00:29:58,799
if anybody want feels like helping with

715
00:29:56,000 --> 00:30:03,039
that then then you're very very welcome

716
00:29:58,799 --> 00:30:09,840
um i'm gonna get this one quick

717
00:30:03,039 --> 00:30:09,840
final attempt

718
00:30:22,880 --> 00:30:26,320
well i have it i have it open

719
00:30:24,880 --> 00:30:29,200
fortunately so we can show you

720
00:30:26,320 --> 00:30:30,320
some of the things i was talking about

721
00:30:29,200 --> 00:30:33,039
the labels

722
00:30:30,320 --> 00:30:33,039
i add to the

723
00:30:34,840 --> 00:30:40,639
hey the labels i added to the metrics

724
00:30:39,039 --> 00:30:42,480
can be used in graffana to make this

725
00:30:40,640 --> 00:30:44,799
nice

726
00:30:42,480 --> 00:30:46,399
little table in the top that shows this

727
00:30:44,799 --> 00:30:51,440
is uh

728
00:30:46,399 --> 00:30:54,959
in the top we have the

729
00:30:51,440 --> 00:30:56,880
the uh the filters that ravana take

730
00:30:54,960 --> 00:30:59,200
makes it possible to to filter by label

731
00:30:56,880 --> 00:31:01,600
values so this shows

732
00:30:59,200 --> 00:31:02,640
server names and this shows the

733
00:31:01,600 --> 00:31:07,519
different geoms

734
00:31:02,640 --> 00:31:07,519
and the different disk sizes we have

735
00:31:07,600 --> 00:31:11,519
and section sizes and rpm and stuff and

736
00:31:09,840 --> 00:31:13,678
this is where would for example

737
00:31:11,519 --> 00:31:15,279
i didn't know we still had a 5400 rpm

738
00:31:13,679 --> 00:31:18,000
drive but after i installed this

739
00:31:15,279 --> 00:31:19,279
then i got a bit better overview of the

740
00:31:18,000 --> 00:31:21,200
entire

741
00:31:19,279 --> 00:31:23,600
and it's a it's a nice way to see for

742
00:31:21,200 --> 00:31:25,039
example serial number if you know a disk

743
00:31:23,600 --> 00:31:26,480
is performing poorly you can easily see

744
00:31:25,039 --> 00:31:31,519
the serial number and ask your

745
00:31:26,480 --> 00:31:31,519
on site hands to to fix it

746
00:31:31,600 --> 00:31:34,959
another than that there's sections for

747
00:31:33,600 --> 00:31:37,360
each each of the

748
00:31:34,960 --> 00:31:39,200
metrics geom exports so there's a

749
00:31:37,360 --> 00:31:43,760
latency graph

750
00:31:39,200 --> 00:31:46,880
for read write and delete latency and

751
00:31:43,760 --> 00:31:48,559
there's a bandwidth section for of

752
00:31:46,880 --> 00:31:50,960
course it can't

753
00:31:48,559 --> 00:31:52,639
trust me there's bandwidth graphs behind

754
00:31:50,960 --> 00:31:55,440
if i was online

755
00:31:52,640 --> 00:31:57,600
and there's an eye up session section

756
00:31:55,440 --> 00:31:57,600
and

757
00:31:57,760 --> 00:32:01,840
i'm a queue in the cube that depth this

758
00:32:00,159 --> 00:32:03,600
is showing 30 days

759
00:32:01,840 --> 00:32:05,120
i've had this running for a few months

760
00:32:03,600 --> 00:32:08,320
since i wrote it and

761
00:32:05,120 --> 00:32:11,678
one of the fun things is that if you've

762
00:32:08,320 --> 00:32:13,120
if you've ran a very large set of s

763
00:32:11,679 --> 00:32:16,480
pools you probably know that

764
00:32:13,120 --> 00:32:20,239
the scrubbing can take almost forever

765
00:32:16,480 --> 00:32:21,440
to uh to complete which means that many

766
00:32:20,240 --> 00:32:23,440
uh

767
00:32:21,440 --> 00:32:24,640
people with loud pools end up doing like

768
00:32:23,440 --> 00:32:26,159
a nightly scrub

769
00:32:24,640 --> 00:32:27,919
that just poses it runs it through the

770
00:32:26,159 --> 00:32:29,039
night and poses it in the day in the

771
00:32:27,919 --> 00:32:30,720
working hours

772
00:32:29,039 --> 00:32:33,039
and that's a fun uh this is the pc

773
00:32:30,720 --> 00:32:34,960
present and the scrubs are these uh this

774
00:32:33,039 --> 00:32:35,519
is 30 days so you can see we scrub all

775
00:32:34,960 --> 00:32:37,679
weekend

776
00:32:35,519 --> 00:32:39,360
and then every night and then all

777
00:32:37,679 --> 00:32:43,760
weekend and every night

778
00:32:39,360 --> 00:32:46,080
um so it's a it's a fun pattern to

779
00:32:43,760 --> 00:32:47,760
have to observe and we found we found a

780
00:32:46,080 --> 00:32:49,199
bad disk in one of the servers using

781
00:32:47,760 --> 00:32:50,879
this

782
00:32:49,200 --> 00:32:52,960
i can't find the data now of course but

783
00:32:50,880 --> 00:32:52,960
uh

784
00:32:53,600 --> 00:32:56,799
it works really well to to get an

785
00:32:55,600 --> 00:33:00,320
overview of the

786
00:32:56,799 --> 00:33:00,320
of the disk utilization and

787
00:33:00,399 --> 00:33:06,799
that's actually that's all i wanted so

788
00:33:03,919 --> 00:33:06,799
so that's very nice

789
00:33:08,559 --> 00:33:14,720
any questions we have plenty of time yes

790
00:33:12,080 --> 00:33:15,678
uh my ocd screams at me that the media

791
00:33:14,720 --> 00:33:18,240
size should be just

792
00:33:15,679 --> 00:33:18,880
a number without the string yeah but

793
00:33:18,240 --> 00:33:22,080
it's uh

794
00:33:18,880 --> 00:33:23,360
exported directly from the uh

795
00:33:22,080 --> 00:33:26,720
[Music]

796
00:33:23,360 --> 00:33:29,120
the string that the geom info exports so

797
00:33:26,720 --> 00:33:30,960
i could cut it off and then tell

798
00:33:29,120 --> 00:33:31,600
graffana that it's a byte value and it

799
00:33:30,960 --> 00:33:34,640
would

800
00:33:31,600 --> 00:33:38,080
humanize it for me um you're right

801
00:33:34,640 --> 00:33:40,320
that would be that would be easy uh

802
00:33:38,080 --> 00:33:43,439
graffana knows

803
00:33:40,320 --> 00:33:45,200
like it knows units so this this graph

804
00:33:43,440 --> 00:33:46,799
i've told it that this is a number of

805
00:33:45,200 --> 00:33:50,159
seconds and then

806
00:33:46,799 --> 00:33:51,279
it shows auto scales the the y-axis to

807
00:33:50,159 --> 00:33:55,360
show that this is uh

808
00:33:51,279 --> 00:33:59,039
milliseconds of course not seconds

809
00:33:55,360 --> 00:34:00,719
fortunately so and it understands like

810
00:33:59,039 --> 00:34:00,900
megabytes per second and stuff like that

811
00:34:00,720 --> 00:34:02,159
so

812
00:34:00,900 --> 00:34:06,240
[Music]

813
00:34:02,159 --> 00:34:08,159
it is very lovely to work with

814
00:34:06,240 --> 00:34:09,918
yes have you had any trouble with

815
00:34:08,159 --> 00:34:11,679
prometheus or graphana because you're on

816
00:34:09,918 --> 00:34:12,719
previously instead of linux like things

817
00:34:11,679 --> 00:34:15,040
looking for systemd

818
00:34:12,719 --> 00:34:17,439
or stuff like that apart from it not

819
00:34:15,040 --> 00:34:21,199
exporting disk utilization

820
00:34:17,440 --> 00:34:22,639
no oh right the question was

821
00:34:21,199 --> 00:34:25,279
have i had any problems running a

822
00:34:22,639 --> 00:34:27,760
grafana or prometheus on freebsd

823
00:34:25,280 --> 00:34:28,879
because it's previously and not linux

824
00:34:27,760 --> 00:34:31,679
and no

825
00:34:28,879 --> 00:34:32,078
apart from the the node exporter that's

826
00:34:31,679 --> 00:34:34,320
the

827
00:34:32,079 --> 00:34:36,879
thing that exports exposes system

828
00:34:34,320 --> 00:34:39,040
metrics like memory and disk io and

829
00:34:36,879 --> 00:34:41,118
stuff like that apart from it not

830
00:34:39,040 --> 00:34:44,320
exporting disk io and not export

831
00:34:41,119 --> 00:34:47,040
exporting a set of s data then

832
00:34:44,320 --> 00:34:48,320
no everything works very well the ports

833
00:34:47,040 --> 00:34:51,440
work perfectly and

834
00:34:48,320 --> 00:34:53,679
they are updated timely and run run very

835
00:34:51,440 --> 00:34:53,679
well

836
00:34:54,159 --> 00:34:58,720
the latest version that the upcoming

837
00:34:56,960 --> 00:35:02,079
version of node exporter does have

838
00:34:58,720 --> 00:35:04,160
freebsd zfs support and

839
00:35:02,079 --> 00:35:06,160
i'm very much looking forward to that

840
00:35:04,160 --> 00:35:07,520
and combined with this then you can get

841
00:35:06,160 --> 00:35:11,440
a great

842
00:35:07,520 --> 00:35:11,440
impression of how your storage is doing

843
00:35:13,760 --> 00:35:21,119
yeah yes i

844
00:35:17,119 --> 00:35:24,240
have questions in a bit disjoint topics

845
00:35:21,119 --> 00:35:26,640
the first one is

846
00:35:24,240 --> 00:35:27,680
you were touching keywords like alerting

847
00:35:26,640 --> 00:35:30,240
and federation

848
00:35:27,680 --> 00:35:32,480
and this kind of stuff so my questions

849
00:35:30,240 --> 00:35:32,479
about

850
00:35:34,240 --> 00:35:38,959
these servers what you were scraping yes

851
00:35:36,880 --> 00:35:41,680
they were only scraping endpoints

852
00:35:38,960 --> 00:35:43,280
or they were scraped and processed at

853
00:35:41,680 --> 00:35:46,240
the same site

854
00:35:43,280 --> 00:35:47,040
or you had one central from ethio server

855
00:35:46,240 --> 00:35:49,680
and then

856
00:35:47,040 --> 00:35:50,800
scraping all the endpoints yes one

857
00:35:49,680 --> 00:35:52,879
central server that

858
00:35:50,800 --> 00:35:54,000
that connects out to you could you

859
00:35:52,880 --> 00:35:56,240
configure it

860
00:35:54,000 --> 00:35:57,200
tell it which targets to scrape and then

861
00:35:56,240 --> 00:36:00,479
it just sucks up

862
00:35:57,200 --> 00:36:01,359
all the metrics that that that are

863
00:36:00,480 --> 00:36:03,760
available at that

864
00:36:01,359 --> 00:36:03,759
endpoint

865
00:36:05,760 --> 00:36:12,240
the delay or the latency

866
00:36:08,800 --> 00:36:14,480
no i mean you would think but no it just

867
00:36:12,240 --> 00:36:16,160
really works very well um it's it's a

868
00:36:14,480 --> 00:36:18,800
bit of a radical

869
00:36:16,160 --> 00:36:21,118
both the the pull model and the the idea

870
00:36:18,800 --> 00:36:21,119
that it

871
00:36:22,160 --> 00:36:26,240
like that it does describe as it

872
00:36:24,160 --> 00:36:27,279
connects so if reviews doesn't connect

873
00:36:26,240 --> 00:36:30,399
for a while then

874
00:36:27,280 --> 00:36:33,359
there's no data collected but it really

875
00:36:30,400 --> 00:36:34,640
works very very well and and turns out

876
00:36:33,359 --> 00:36:35,920
that it doesn't matter too much as long

877
00:36:34,640 --> 00:36:36,799
as you use counters it wouldn't matter

878
00:36:35,920 --> 00:36:38,640
for g-stat

879
00:36:36,800 --> 00:36:39,839
exporter because you would lose the

880
00:36:38,640 --> 00:36:41,759
precision

881
00:36:39,839 --> 00:36:42,960
if you reboot your previous server then

882
00:36:41,760 --> 00:36:44,560
you would lose the five minutes or

883
00:36:42,960 --> 00:36:46,640
whatever it takes

884
00:36:44,560 --> 00:36:47,680
but if you have counters available then

885
00:36:46,640 --> 00:36:49,520
uh then you don't

886
00:36:47,680 --> 00:36:50,879
because it doesn't matter if if medius

887
00:36:49,520 --> 00:36:54,800
is gone for a while it'll

888
00:36:50,880 --> 00:36:57,920
figure out how to uh

889
00:36:54,800 --> 00:36:57,920
what's the word i'm looking for

890
00:36:58,640 --> 00:37:03,118
yeah exactly uh it'll it'll smooth it

891
00:37:00,640 --> 00:37:05,680
out uh yeah

892
00:37:03,119 --> 00:37:06,880
so and no it works very well and people

893
00:37:05,680 --> 00:37:10,078
are doing some

894
00:37:06,880 --> 00:37:12,960
like the big uh hosting operators um

895
00:37:10,079 --> 00:37:15,040
i do have some like they have 500

896
00:37:12,960 --> 00:37:17,280
prometheus servers doing

897
00:37:15,040 --> 00:37:19,520
uh millions and millions of data points

898
00:37:17,280 --> 00:37:20,560
per second some crazy big instances so

899
00:37:19,520 --> 00:37:22,720
for my users

900
00:37:20,560 --> 00:37:24,560
with 100 servers or something i know

901
00:37:22,720 --> 00:37:26,480
it's a it's always nice to

902
00:37:24,560 --> 00:37:28,720
to have to know that it can handle way

903
00:37:26,480 --> 00:37:30,720
more than you need

904
00:37:28,720 --> 00:37:32,720
but this also means that you had one

905
00:37:30,720 --> 00:37:33,919
central server so you had to maintain

906
00:37:32,720 --> 00:37:37,118
your alerts

907
00:37:33,920 --> 00:37:40,000
on one server right so these

908
00:37:37,119 --> 00:37:43,040
or the alert conditions had to be

909
00:37:40,000 --> 00:37:43,040
written on one server

910
00:37:44,800 --> 00:37:47,920
well the alert manager can is a separate

911
00:37:46,880 --> 00:37:49,599
jail i mean it could

912
00:37:47,920 --> 00:37:53,200
it could be a separate server if you

913
00:37:49,599 --> 00:37:53,200
want it yeah i mean

914
00:37:54,160 --> 00:37:59,839
yeah a different question yeah in

915
00:37:57,760 --> 00:38:01,680
the intro of your talk you mentioned

916
00:37:59,839 --> 00:38:04,880
that

917
00:38:01,680 --> 00:38:09,440
you take care of this born hack camp

918
00:38:04,880 --> 00:38:12,720
yes what's the topic is this

919
00:38:09,440 --> 00:38:16,079
yeah it is stuff like this

920
00:38:12,720 --> 00:38:19,279
it is infosec it is also hardware

921
00:38:16,079 --> 00:38:22,320
open source and really broad if you uh

922
00:38:19,280 --> 00:38:23,280
if you've been to the german ccc camp or

923
00:38:22,320 --> 00:38:24,880
the dutch

924
00:38:23,280 --> 00:38:26,480
big hacker camps it is very much

925
00:38:24,880 --> 00:38:29,520
inspired by those

926
00:38:26,480 --> 00:38:30,960
so it is kind of reminds a little bit of

927
00:38:29,520 --> 00:38:31,759
first time except that people live in

928
00:38:30,960 --> 00:38:36,400
tents and

929
00:38:31,760 --> 00:38:39,200
there's not as many people of course

930
00:38:36,400 --> 00:38:40,720
about 400 people i think this year and

931
00:38:39,200 --> 00:38:41,759
it's slowly growing it's the fifth year

932
00:38:40,720 --> 00:38:44,640
this time so

933
00:38:41,760 --> 00:38:46,560
and it's an annual event and it's

934
00:38:44,640 --> 00:38:48,720
getting traction it's a lovely little

935
00:38:46,560 --> 00:38:51,200
event

936
00:38:48,720 --> 00:38:51,200
so um

937
00:38:52,720 --> 00:38:55,439
any more questions

938
00:38:58,960 --> 00:39:02,800
and some yes you seem to have a good

939
00:39:01,599 --> 00:39:06,000
experience with promoters

940
00:39:02,800 --> 00:39:06,000
absolutely did you try

941
00:39:06,480 --> 00:39:14,000
this computing stack telegraph and

942
00:39:10,640 --> 00:39:17,040
i've used no not really i've

943
00:39:14,000 --> 00:39:19,760
i haven't used it enough to uh to i've

944
00:39:17,040 --> 00:39:21,040
not set it up i've only uh briefly been

945
00:39:19,760 --> 00:39:22,320
exposed to it when somebody else had

946
00:39:21,040 --> 00:39:25,279
configured it so i

947
00:39:22,320 --> 00:39:25,280
i don't know enough but

948
00:39:25,359 --> 00:39:28,400
you know how you can sometimes sense

949
00:39:26,800 --> 00:39:29,040
that something has momentum like a

950
00:39:28,400 --> 00:39:32,000
project

951
00:39:29,040 --> 00:39:33,680
like every i think everywhere i look

952
00:39:32,000 --> 00:39:35,440
prometheus exporters are popping up like

953
00:39:33,680 --> 00:39:38,720
natively inside

954
00:39:35,440 --> 00:39:41,040
whatever nginx or whatever suddenly has

955
00:39:38,720 --> 00:39:42,399
a way to explore and and that tells me

956
00:39:41,040 --> 00:39:44,720
that uh

957
00:39:42,400 --> 00:39:45,440
you know i'm not the only one who feels

958
00:39:44,720 --> 00:39:47,680
that this is the

959
00:39:45,440 --> 00:39:49,440
the right approach and uh and that's

960
00:39:47,680 --> 00:39:51,759
very nice because i think uh

961
00:39:49,440 --> 00:39:52,960
i think we have been i've been looking

962
00:39:51,760 --> 00:39:55,359
for something nice

963
00:39:52,960 --> 00:39:56,320
for monitoring for years and years i've

964
00:39:55,359 --> 00:39:59,119
never been really

965
00:39:56,320 --> 00:40:00,160
really happy even back when nagios was

966
00:39:59,119 --> 00:40:02,160
the thing to run

967
00:40:00,160 --> 00:40:03,598
i've never really been very happy with

968
00:40:02,160 --> 00:40:06,879
it i think everybody really

969
00:40:03,599 --> 00:40:07,200
kind of loved to hate it or yeah so i am

970
00:40:06,880 --> 00:40:09,359
so

971
00:40:07,200 --> 00:40:11,118
so happy that finally something and it

972
00:40:09,359 --> 00:40:12,720
also i mean the aesthetics are also very

973
00:40:11,119 --> 00:40:14,480
important because even though

974
00:40:12,720 --> 00:40:15,839
technically it's the same number as if

975
00:40:14,480 --> 00:40:16,960
it's an arctic graph or a pretty graph

976
00:40:15,839 --> 00:40:18,640
but it just

977
00:40:16,960 --> 00:40:21,040
still makes a difference it's nicer to

978
00:40:18,640 --> 00:40:21,040
work with

979
00:40:21,119 --> 00:40:27,760
i have one response to that

980
00:40:24,240 --> 00:40:28,959
uh so just looking at uh influx while

981
00:40:27,760 --> 00:40:30,960
it's

982
00:40:28,960 --> 00:40:32,079
nice to work with in at least some

983
00:40:30,960 --> 00:40:35,119
aspects

984
00:40:32,079 --> 00:40:36,240
uh probability use has much much more

985
00:40:35,119 --> 00:40:39,280
advanced math

986
00:40:36,240 --> 00:40:42,000
available like for example in influx

987
00:40:39,280 --> 00:40:42,800
it's not possible in a single query to

988
00:40:42,000 --> 00:40:46,160
get

989
00:40:42,800 --> 00:40:48,000
uh for example you get the return codes

990
00:40:46,160 --> 00:40:51,040
of http requests

991
00:40:48,000 --> 00:40:54,400
in influx it's not possible to get

992
00:40:51,040 --> 00:40:58,079
what proportion of requests were

993
00:40:54,400 --> 00:40:59,280
say errors it's possible in graphite

994
00:40:58,079 --> 00:41:04,640
it's possible in

995
00:40:59,280 --> 00:41:04,640
prometheus it's not possible in influx

996
00:41:04,839 --> 00:41:07,839
growth

997
00:41:10,560 --> 00:41:14,319
sure any more questions

998
00:41:16,240 --> 00:41:20,078
i have a quick question because i'm

999
00:41:18,319 --> 00:41:20,720
rarely surrounded by a lot of freebsd

1000
00:41:20,079 --> 00:41:23,200
people so

1001
00:41:20,720 --> 00:41:23,200
i just

1002
00:41:30,160 --> 00:41:33,680
oh damn i went offline oh and i can't

1003
00:41:32,720 --> 00:41:35,439
even show it

1004
00:41:33,680 --> 00:41:37,440
okay well the question though the

1005
00:41:35,440 --> 00:41:39,200
problem is you know uh the jail exporter

1006
00:41:37,440 --> 00:41:43,040
i was talking about

1007
00:41:39,200 --> 00:41:46,399
which measure which uses rctl to export

1008
00:41:43,040 --> 00:41:48,400
jail metrics and then then you can do

1009
00:41:46,400 --> 00:41:49,440
graphs for for example memory use inside

1010
00:41:48,400 --> 00:41:51,760
the tail

1011
00:41:49,440 --> 00:41:53,200
so i i use that of course because i use

1012
00:41:51,760 --> 00:41:55,760
jails for all my stuff

1013
00:41:53,200 --> 00:41:56,640
and it's very nice to see if the disk is

1014
00:41:55,760 --> 00:41:58,880
being trashed

1015
00:41:56,640 --> 00:42:00,480
you can easily see which jail is causing

1016
00:41:58,880 --> 00:42:02,480
this

1017
00:42:00,480 --> 00:42:04,880
postgres doing an auto vacuum or what's

1018
00:42:02,480 --> 00:42:04,880
going on

1019
00:42:04,960 --> 00:42:12,800
rctl counts shared memory

1020
00:42:08,640 --> 00:42:14,160
twice in jails or as many times i have

1021
00:42:12,800 --> 00:42:16,000
what i have in this tab that i can

1022
00:42:14,160 --> 00:42:17,680
switch over to because i'm offline

1023
00:42:16,000 --> 00:42:20,319
i have two jails on this server it has a

1024
00:42:17,680 --> 00:42:24,720
20 128 gigabytes of ram

1025
00:42:20,319 --> 00:42:24,720
i have two postgres jails and rctl

1026
00:42:25,599 --> 00:42:30,319
tells me that one jail is using about

1027
00:42:27,920 --> 00:42:30,800
130 gigs and the other jail is using 80

1028
00:42:30,319 --> 00:42:32,640
gigs

1029
00:42:30,800 --> 00:42:35,200
which is about 200 gigs and there's 40

1030
00:42:32,640 --> 00:42:37,440
more jails on it so clearly

1031
00:42:35,200 --> 00:42:40,000
something is is wrong and i've been able

1032
00:42:37,440 --> 00:42:42,720
to track down as much as

1033
00:42:40,000 --> 00:42:44,640
it is i don't know what the fix is but

1034
00:42:42,720 --> 00:42:47,759
it's absolutely misreporting

1035
00:42:44,640 --> 00:42:49,680
memory use in jails and since

1036
00:42:47,760 --> 00:42:51,119
if i have 100 postgres workers and each

1037
00:42:49,680 --> 00:42:54,000
of them is using

1038
00:42:51,119 --> 00:42:54,560
two gigabytes of ram then and most of it

1039
00:42:54,000 --> 00:42:57,119
is shared

1040
00:42:54,560 --> 00:42:58,880
what you want rctl to export is how much

1041
00:42:57,119 --> 00:42:59,520
memory would be freed if i shut the gel

1042
00:42:58,880 --> 00:43:01,599
down

1043
00:42:59,520 --> 00:43:03,280
i don't care if 100 workers is using two

1044
00:43:01,599 --> 00:43:05,680
gigabytes you shouldn't report 200

1045
00:43:03,280 --> 00:43:08,560
gigabytes memory use

1046
00:43:05,680 --> 00:43:10,160
does anybody know is this just a bug or

1047
00:43:08,560 --> 00:43:12,160
is this

1048
00:43:10,160 --> 00:43:13,598
not possible what i'm asking for or i

1049
00:43:12,160 --> 00:43:16,240
mean

1050
00:43:13,599 --> 00:43:17,280
between jails are you even able to tell

1051
00:43:16,240 --> 00:43:19,680
not between jails

1052
00:43:17,280 --> 00:43:22,160
but between 100 postgres workers in the

1053
00:43:19,680 --> 00:43:25,040
same jail for example

1054
00:43:22,160 --> 00:43:26,480
and then each of those they have access

1055
00:43:25,040 --> 00:43:28,240
to the same shared memory space and it's

1056
00:43:26,480 --> 00:43:30,960
been counted 100 times because

1057
00:43:28,240 --> 00:43:34,078
there's 100 workers so it tells me that

1058
00:43:30,960 --> 00:43:34,880
one jail is using 140 gigs of ram on 128

1059
00:43:34,079 --> 00:43:38,720
gig server

1060
00:43:34,880 --> 00:43:38,720
which clearly is not true

1061
00:43:39,520 --> 00:43:43,040
i wonder how it would look like if it

1062
00:43:41,520 --> 00:43:47,520
was running outside of jail

1063
00:43:43,040 --> 00:43:50,720
if it would be reported the same way uh

1064
00:43:47,520 --> 00:43:54,240
it would but we can

1065
00:43:50,720 --> 00:43:56,319
talk about it later after i just

1066
00:43:54,240 --> 00:43:58,319
it's been bugging me because uh i

1067
00:43:56,319 --> 00:44:02,240
finally got some nice gel graphs and

1068
00:43:58,319 --> 00:44:03,599
the memory usage is just way way off

1069
00:44:02,240 --> 00:44:05,839
i was like what the hell is wrong how

1070
00:44:03,599 --> 00:44:06,839
can it be using 300 gigabytes of ram

1071
00:44:05,839 --> 00:44:10,319
when it has

1072
00:44:06,839 --> 00:44:11,200
128 and our ctl exports the v memory use

1073
00:44:10,319 --> 00:44:13,680
as well and that's

1074
00:44:11,200 --> 00:44:16,240
of course a high number and higher than

1075
00:44:13,680 --> 00:44:17,118
the it's 1.2 terabytes of memory rv

1076
00:44:16,240 --> 00:44:20,078
memory i think

1077
00:44:17,119 --> 00:44:21,280
but but the memory use should be we

1078
00:44:20,079 --> 00:44:22,960
agreed that it should be

1079
00:44:21,280 --> 00:44:25,920
however much would be freed if the jail

1080
00:44:22,960 --> 00:44:25,920
was shut down right

1081
00:44:27,599 --> 00:44:32,319
otherwise what's the point of reporting

1082
00:44:29,200 --> 00:44:35,680
i mean you see my you see what i mean

1083
00:44:32,319 --> 00:44:37,920
okay i think our ray open pr

1084
00:44:35,680 --> 00:44:38,879
and write to a mailing list and see if i

1085
00:44:37,920 --> 00:44:40,880
can uh

1086
00:44:38,880 --> 00:44:42,480
i think i can catch somebody's attention

1087
00:44:40,880 --> 00:44:44,720
with that but the other one that

1088
00:44:42,480 --> 00:44:45,920
the jade exporter is really awesome and

1089
00:44:44,720 --> 00:44:48,839
stuff like iops

1090
00:44:45,920 --> 00:44:50,000
and that works very well so you can see

1091
00:44:48,839 --> 00:44:52,400
which

1092
00:44:50,000 --> 00:44:55,119
jail is trashing a disk or or using a

1093
00:44:52,400 --> 00:44:55,119
lot of cpu

1094
00:44:56,000 --> 00:45:01,920
i think that's it for me we're done

1095
00:44:59,440 --> 00:45:02,960
with a quarter quarter of an hour to

1096
00:45:01,920 --> 00:45:05,280
spare so

1097
00:45:02,960 --> 00:45:08,319
you can use that time to for

1098
00:45:05,280 --> 00:45:10,880
self-reflection and the self-improvement

1099
00:45:08,319 --> 00:45:14,400
and thanks for having me and i hope you

1100
00:45:10,880 --> 00:45:19,839
enjoyed foster and my talk and

1101
00:45:14,400 --> 00:45:19,839
yeah thank you

