1
00:00:05,520 --> 00:00:09,280
hi everyone

2
00:00:06,399 --> 00:00:10,879
i'm mamini ratzin bazafi and today i

3
00:00:09,280 --> 00:00:12,719
will talk about

4
00:00:10,880 --> 00:00:14,639
the design of high-performance

5
00:00:12,719 --> 00:00:16,480
multi-threading framework

6
00:00:14,639 --> 00:00:18,080
so the multi-threading framework is

7
00:00:16,480 --> 00:00:21,199
implemented in nim

8
00:00:18,080 --> 00:00:23,439
but everything i'm talking about is uh

9
00:00:21,199 --> 00:00:27,199
something that you can also do

10
00:00:23,439 --> 00:00:30,000
in c rest aida

11
00:00:27,199 --> 00:00:31,840
any kind of language and my hope is that

12
00:00:30,000 --> 00:00:33,200
after that talk you will be able to

13
00:00:31,840 --> 00:00:35,280
implement your own multi-threading

14
00:00:33,200 --> 00:00:38,880
framework in a weekend

15
00:00:35,280 --> 00:00:38,880
so let's see how we can do that

16
00:00:43,040 --> 00:00:48,640
so a bit uh of things about myself

17
00:00:46,079 --> 00:00:49,920
uh i've been using nim from uh for three

18
00:00:48,640 --> 00:00:53,120
years now

19
00:00:49,920 --> 00:00:55,199
and i'm a blockchain developer

20
00:00:53,120 --> 00:00:56,640
during the day and a high performance

21
00:00:55,199 --> 00:00:58,320
computing developer

22
00:00:56,640 --> 00:00:59,840
and also data scientists during the

23
00:00:58,320 --> 00:01:02,719
night

24
00:00:59,840 --> 00:01:04,559
you have my twitter and my github

25
00:01:02,719 --> 00:01:07,119
accounts

26
00:01:04,559 --> 00:01:09,119
so how did i start doing a

27
00:01:07,119 --> 00:01:11,439
multi-threading framework from scratch

28
00:01:09,119 --> 00:01:13,200
three years ago i discovered nim i

29
00:01:11,439 --> 00:01:14,559
wanted to do some high performance

30
00:01:13,200 --> 00:01:17,520
computing

31
00:01:14,560 --> 00:01:18,640
in nim and i had two threading models i

32
00:01:17,520 --> 00:01:22,960
could use

33
00:01:18,640 --> 00:01:26,000
one is openmp because nim compiles to c

34
00:01:22,960 --> 00:01:26,640
and you can add openmp annotation to

35
00:01:26,000 --> 00:01:28,560
them

36
00:01:26,640 --> 00:01:30,479
so that makes it very easy to do

37
00:01:28,560 --> 00:01:32,799
parallel for loops

38
00:01:30,479 --> 00:01:34,079
the second shooting model is a simple

39
00:01:32,799 --> 00:01:38,840
thread pool

40
00:01:34,079 --> 00:01:41,600
with just a spawn and a sink

41
00:01:38,840 --> 00:01:45,119
statement and

42
00:01:41,600 --> 00:01:46,240
uh so i used openmp uh for starter on my

43
00:01:45,119 --> 00:01:50,399
multi-trading run

44
00:01:46,240 --> 00:01:53,679
on my uh high performance computing

45
00:01:50,399 --> 00:01:56,799
and tensor library but a year ago

46
00:01:53,680 --> 00:01:58,960
i was dissatisfied with a

47
00:01:56,799 --> 00:01:59,750
lot of the internal including the

48
00:01:58,960 --> 00:02:01,119
threading

49
00:01:59,750 --> 00:02:04,159
[Music]

50
00:02:01,119 --> 00:02:06,719
so i started to re-implement everything

51
00:02:04,159 --> 00:02:06,719
from scratch

52
00:02:07,280 --> 00:02:14,560
and the goal today is as i said

53
00:02:10,878 --> 00:02:16,079
uh end up with a multifading runtime

54
00:02:14,560 --> 00:02:18,239
that you can implement in a weekend

55
00:02:16,080 --> 00:02:20,239
and to go there we need to first

56
00:02:18,239 --> 00:02:23,520
understand the design space

57
00:02:20,239 --> 00:02:26,239
second understand what are hardware and

58
00:02:23,520 --> 00:02:29,599
software multithreading

59
00:02:26,239 --> 00:02:32,080
so definitions use cases parallel api

60
00:02:29,599 --> 00:02:33,440
and the sources of overhead how to

61
00:02:32,080 --> 00:02:36,400
benchmark them

62
00:02:33,440 --> 00:02:38,480
and the design constraint that those

63
00:02:36,400 --> 00:02:41,599
bring

64
00:02:38,480 --> 00:02:43,920
so first thing understanding the design

65
00:02:41,599 --> 00:02:43,920
space

66
00:02:44,080 --> 00:02:48,640
one thing that you hear often is that

67
00:02:46,160 --> 00:02:51,200
concurrency is not parallelism

68
00:02:48,640 --> 00:02:52,720
so here you have a coffee machine over

69
00:02:51,200 --> 00:02:56,000
here two coffee machine

70
00:02:52,720 --> 00:02:59,200
concurrency it's the ability of uh

71
00:02:56,000 --> 00:03:02,879
the hardware no sorry of the os

72
00:02:59,200 --> 00:03:06,238
or the scheduler to switch between

73
00:03:02,879 --> 00:03:10,000
to interleave two threads of execution

74
00:03:06,239 --> 00:03:13,760
on one single resources

75
00:03:10,000 --> 00:03:16,480
in case of a parallel runtime

76
00:03:13,760 --> 00:03:19,840
you have two threads of execution and

77
00:03:16,480 --> 00:03:19,840
two resources

78
00:03:21,760 --> 00:03:28,159
another thing that you will see a lot

79
00:03:25,120 --> 00:03:31,280
is probably one one threading

80
00:03:28,159 --> 00:03:32,159
and one threading and then threading so

81
00:03:31,280 --> 00:03:35,680
this is about

82
00:03:32,159 --> 00:03:36,720
on the left the number of application

83
00:03:35,680 --> 00:03:39,280
thread

84
00:03:36,720 --> 00:03:41,120
and on the right of hardware free

85
00:03:39,280 --> 00:03:44,400
hardware thread

86
00:03:41,120 --> 00:03:44,799
and so this is something that is often

87
00:03:44,400 --> 00:03:47,680
seen

88
00:03:44,799 --> 00:03:49,120
at the os level so when you talk about a

89
00:03:47,680 --> 00:03:52,239
very old os

90
00:03:49,120 --> 00:03:53,519
they had sometimes n1 threading or one

91
00:03:52,239 --> 00:03:56,319
one threading

92
00:03:53,519 --> 00:03:57,120
but this is something that also happens

93
00:03:56,319 --> 00:04:00,560
at the

94
00:03:57,120 --> 00:04:04,080
language or the runtime level

95
00:04:00,560 --> 00:04:04,959
it's just a definitions and we won't see

96
00:04:04,080 --> 00:04:09,280
it again

97
00:04:04,959 --> 00:04:12,159
so the problem is

98
00:04:09,280 --> 00:04:13,680
for multiflora runtime how to schedule

99
00:04:12,159 --> 00:04:17,120
end tasks

100
00:04:13,680 --> 00:04:17,120
on n hardware threads

101
00:04:20,079 --> 00:04:27,360
and another thing on the design space is

102
00:04:23,520 --> 00:04:31,120
latency versus throughput so latency

103
00:04:27,360 --> 00:04:34,240
is for a single task

104
00:04:31,120 --> 00:04:37,280
if you have like m tasks and

105
00:04:34,240 --> 00:04:40,000
you have a single task maybe you want

106
00:04:37,280 --> 00:04:40,400
uh the single task to be fair like first

107
00:04:40,000 --> 00:04:42,800
in

108
00:04:40,400 --> 00:04:44,320
first out so this is what happened when

109
00:04:42,800 --> 00:04:46,320
you have multiple clients

110
00:04:44,320 --> 00:04:48,159
a single server and you are supposed to

111
00:04:46,320 --> 00:04:49,120
serve all your clients let's say a web

112
00:04:48,160 --> 00:04:52,400
page

113
00:04:49,120 --> 00:04:56,400
and you don't want

114
00:04:52,400 --> 00:04:57,120
to clients to spend an hour waiting

115
00:04:56,400 --> 00:05:00,799
until

116
00:04:57,120 --> 00:05:01,440
every everyone else is done or for video

117
00:05:00,800 --> 00:05:03,600
decoding

118
00:05:01,440 --> 00:05:05,759
if you have multiple frame and you need

119
00:05:03,600 --> 00:05:09,039
each frame to be

120
00:05:05,759 --> 00:05:11,759
processed one after the other you need

121
00:05:09,039 --> 00:05:12,960
some fairness so this is optimizing for

122
00:05:11,759 --> 00:05:14,479
latency

123
00:05:12,960 --> 00:05:17,120
the other thing is optimizing for

124
00:05:14,479 --> 00:05:19,758
throughput for example in scientific

125
00:05:17,120 --> 00:05:20,720
simulations you don't care about the

126
00:05:19,759 --> 00:05:23,120
single

127
00:05:20,720 --> 00:05:23,759
computation you want everything the

128
00:05:23,120 --> 00:05:25,919
whole

129
00:05:23,759 --> 00:05:26,880
work package to be done as fast as

130
00:05:25,919 --> 00:05:30,000
possible so

131
00:05:26,880 --> 00:05:30,400
even if the first task waits for one

132
00:05:30,000 --> 00:05:32,720
week

133
00:05:30,400 --> 00:05:35,758
as long as everything is done as fast as

134
00:05:32,720 --> 00:05:35,759
possible that's fine

135
00:05:37,360 --> 00:05:42,560
another design axis is cooperative

136
00:05:40,880 --> 00:05:44,880
versus preemptive

137
00:05:42,560 --> 00:05:46,400
cooperative multi-threading you probably

138
00:05:44,880 --> 00:05:48,960
heard about coroutines

139
00:05:46,400 --> 00:05:49,840
fibers green threads first class

140
00:05:48,960 --> 00:05:53,198
continuations

141
00:05:49,840 --> 00:05:54,960
in scheme for example

142
00:05:53,199 --> 00:05:57,120
the characteristic is that those are

143
00:05:54,960 --> 00:06:00,080
lightweight

144
00:05:57,120 --> 00:06:02,400
and you cannot use hardware threads for

145
00:06:00,080 --> 00:06:05,680
those

146
00:06:02,400 --> 00:06:09,679
second preemptive multi-threading also

147
00:06:05,680 --> 00:06:12,479
p threads used in openmp

148
00:06:09,680 --> 00:06:13,440
tbb or silk if you dive into multifading

149
00:06:12,479 --> 00:06:16,479
runtimes

150
00:06:13,440 --> 00:06:19,600
those are scheduled by the the os

151
00:06:16,479 --> 00:06:21,440
they have easier context switches and

152
00:06:19,600 --> 00:06:25,280
you need synchronization primitives

153
00:06:21,440 --> 00:06:27,120
because those are real threads let's say

154
00:06:25,280 --> 00:06:28,638
so synchronization primitives can be

155
00:06:27,120 --> 00:06:32,960
locks atomics

156
00:06:28,639 --> 00:06:34,639
and also things that are more

157
00:06:32,960 --> 00:06:37,039
less known like transactional memory or

158
00:06:34,639 --> 00:06:40,080
message passing

159
00:06:37,039 --> 00:06:41,039
and you have io tasks and cpu tasks so

160
00:06:40,080 --> 00:06:44,318
io tasks

161
00:06:41,039 --> 00:06:47,440
like uh you're waiting for

162
00:06:44,319 --> 00:06:51,199
network connections and

163
00:06:47,440 --> 00:06:54,319
or files and you create tasks

164
00:06:51,199 --> 00:06:56,720
so those are usually latency optimized

165
00:06:54,319 --> 00:06:58,000
and implemented via async await while

166
00:06:56,720 --> 00:07:01,120
cpu tasks

167
00:06:58,000 --> 00:07:04,160
they are throughput optimized and

168
00:07:01,120 --> 00:07:07,280
the terms that are used usually are

169
00:07:04,160 --> 00:07:10,400
spawn and sync so there is a

170
00:07:07,280 --> 00:07:12,159
parallel let's say between both api it

171
00:07:10,400 --> 00:07:12,960
shows that the internals are completely

172
00:07:12,160 --> 00:07:15,199
different

173
00:07:12,960 --> 00:07:16,159
and the requirements are different the

174
00:07:15,199 --> 00:07:19,120
skills

175
00:07:16,160 --> 00:07:22,400
for maintenance are different the os api

176
00:07:19,120 --> 00:07:22,400
are completely different as well

177
00:07:23,360 --> 00:07:27,120
so for the talk i will focus on cpu

178
00:07:26,240 --> 00:07:29,599
tasks

179
00:07:27,120 --> 00:07:30,960
optimized for throughput on preemptive

180
00:07:29,599 --> 00:07:32,800
scheduling so multiple hardware

181
00:07:30,960 --> 00:07:36,000
offerings

182
00:07:32,800 --> 00:07:37,039
so now we have a bit of definitions and

183
00:07:36,000 --> 00:07:39,039
let's see

184
00:07:37,039 --> 00:07:41,520
the different forms of multi-threading

185
00:07:39,039 --> 00:07:45,360
that exists

186
00:07:41,520 --> 00:07:48,719
at a hardware level we have four kinds

187
00:07:45,360 --> 00:07:49,680
there are many more but the four main

188
00:07:48,720 --> 00:07:52,960
kinds are

189
00:07:49,680 --> 00:07:56,319
ilp instruction level parallelism so

190
00:07:52,960 --> 00:08:01,039
a hardware let's say a cpu

191
00:07:56,319 --> 00:08:03,280
arm or x86 has multiple execution ports

192
00:08:01,039 --> 00:08:05,120
for example to do an addition you have

193
00:08:03,280 --> 00:08:07,758
two or three parts available

194
00:08:05,120 --> 00:08:08,800
they are called 0 5 6 something like

195
00:08:07,759 --> 00:08:12,160
this

196
00:08:08,800 --> 00:08:15,120
and you can schedule multiple

197
00:08:12,160 --> 00:08:15,840
additions in parallel that's done by the

198
00:08:15,120 --> 00:08:19,360
processor

199
00:08:15,840 --> 00:08:21,840
as long as one execution port is free

200
00:08:19,360 --> 00:08:22,960
themed single instruction multiple data

201
00:08:21,840 --> 00:08:27,119
so if you heard about

202
00:08:22,960 --> 00:08:28,400
sse avx or on arm neon for example

203
00:08:27,120 --> 00:08:30,639
those are also called vector

204
00:08:28,400 --> 00:08:33,120
instructions you have an addition and

205
00:08:30,639 --> 00:08:34,320
it works on four floating points at the

206
00:08:33,120 --> 00:08:37,839
same time

207
00:08:34,320 --> 00:08:41,919
sims single instruction multiple thread

208
00:08:37,839 --> 00:08:45,040
so those are gpus basically

209
00:08:41,919 --> 00:08:47,920
and on gpu you have 32

210
00:08:45,040 --> 00:08:48,480
thread for nvidia gpu volts are called a

211
00:08:47,920 --> 00:08:51,199
warp

212
00:08:48,480 --> 00:08:52,320
and they have to do the exact same

213
00:08:51,200 --> 00:08:55,839
instructions

214
00:08:52,320 --> 00:08:59,839
and for example if you do a if branch

215
00:08:55,839 --> 00:09:01,519
on a gpu it will execute both branches

216
00:08:59,839 --> 00:09:02,959
and the last one simultaneous

217
00:09:01,519 --> 00:09:05,680
multi-threading

218
00:09:02,959 --> 00:09:06,560
also called in intel speak hyper

219
00:09:05,680 --> 00:09:10,160
threading

220
00:09:06,560 --> 00:09:10,640
it's a a way to use all the execution

221
00:09:10,160 --> 00:09:13,680
parts

222
00:09:10,640 --> 00:09:16,000
by having uh logical threads uh

223
00:09:13,680 --> 00:09:18,000
sharing the same execution resources

224
00:09:16,000 --> 00:09:20,080
same memory bandwidth

225
00:09:18,000 --> 00:09:22,959
because it's usually quite hard to use

226
00:09:20,080 --> 00:09:26,640
all execution ports at the same time

227
00:09:22,959 --> 00:09:28,880
and it's not always two sibling calls

228
00:09:26,640 --> 00:09:32,160
that are using per trading if you use

229
00:09:28,880 --> 00:09:36,320
xeon phi it's a four

230
00:09:32,160 --> 00:09:38,800
sibling calls that you have now

231
00:09:36,320 --> 00:09:40,480
let's talk about the form of parallelism

232
00:09:38,800 --> 00:09:43,279
that you might want to implement

233
00:09:40,480 --> 00:09:45,120
or support in your runtime first one

234
00:09:43,279 --> 00:09:47,680
data parallelism

235
00:09:45,120 --> 00:09:49,680
easy it's just a parallel for loop if

236
00:09:47,680 --> 00:09:52,959
you use the openmp

237
00:09:49,680 --> 00:09:54,079
or if you used on c plus plus intel tbb

238
00:09:52,959 --> 00:09:58,880
parallel 4 or

239
00:09:54,080 --> 00:10:01,360
on rust rayon that's exactly that

240
00:09:58,880 --> 00:10:02,160
it's the same instruction multiple data

241
00:10:01,360 --> 00:10:04,720
use cases

242
00:10:02,160 --> 00:10:06,880
scientific computing you have vectors

243
00:10:04,720 --> 00:10:10,880
matrices you do a for loop on

244
00:10:06,880 --> 00:10:13,200
all data challenges

245
00:10:10,880 --> 00:10:15,360
uh how to support nested parallelism for

246
00:10:13,200 --> 00:10:19,279
example openmp doesn't really support

247
00:10:15,360 --> 00:10:22,720
nested parallelism and there are other

248
00:10:19,279 --> 00:10:25,519
load balancing challenge so it might

249
00:10:22,720 --> 00:10:27,680
seem surprising but splitting a loop is

250
00:10:25,519 --> 00:10:32,480
for multi-threading is actually complex

251
00:10:27,680 --> 00:10:35,519
because if you split before

252
00:10:32,480 --> 00:10:36,399
entering the loop maybe you might try to

253
00:10:35,519 --> 00:10:38,800
split

254
00:10:36,399 --> 00:10:40,640
a loop in 10 even though the loop is

255
00:10:38,800 --> 00:10:42,399
super small and you don't need to split

256
00:10:40,640 --> 00:10:45,839
it

257
00:10:42,399 --> 00:10:47,680
if you split well i won't enter in the

258
00:10:45,839 --> 00:10:51,519
detail because

259
00:10:47,680 --> 00:10:53,790
but you have free spreading strategies

260
00:10:51,519 --> 00:10:54,959
that you can research if you want to

261
00:10:53,790 --> 00:10:58,399
[Music]

262
00:10:54,959 --> 00:10:58,399
implement data parallelism

263
00:10:59,279 --> 00:11:05,680
the main one task parallelism so this

264
00:11:02,320 --> 00:11:08,160
is uh spawn and sync

265
00:11:05,680 --> 00:11:09,359
it's basically a function call that may

266
00:11:08,160 --> 00:11:12,560
or may not

267
00:11:09,360 --> 00:11:14,640
be executed on another hardware threads

268
00:11:12,560 --> 00:11:16,000
and the may or may not is managed by the

269
00:11:14,640 --> 00:11:19,680
scheduling runtime

270
00:11:16,000 --> 00:11:23,680
an example intel tbb or openmp tasks

271
00:11:19,680 --> 00:11:26,319
since openmp version 3.0

272
00:11:23,680 --> 00:11:27,599
use cases anywhere you want a parallel

273
00:11:26,320 --> 00:11:30,560
function

274
00:11:27,600 --> 00:11:30,959
for example a parallel tree algorithm

275
00:11:30,560 --> 00:11:33,279
like

276
00:11:30,959 --> 00:11:36,239
depth first or breadth first search

277
00:11:33,279 --> 00:11:39,279
divide and conquer algorithm

278
00:11:36,240 --> 00:11:43,680
and there are multiple challenges

279
00:11:39,279 --> 00:11:46,720
the api now most of

280
00:11:43,680 --> 00:11:49,920
the runtimes are using futures

281
00:11:46,720 --> 00:11:52,399
uh in weave and in neem i'm using flovar

282
00:11:49,920 --> 00:11:53,360
to distinguish from io tasks that are

283
00:11:52,399 --> 00:11:55,839
using futures

284
00:11:53,360 --> 00:11:57,920
and cpu tasks that are using flowva it's

285
00:11:55,839 --> 00:12:00,079
just a name

286
00:11:57,920 --> 00:12:01,439
other challenge synchronization

287
00:12:00,079 --> 00:12:04,959
scheduling overhead

288
00:12:01,440 --> 00:12:08,320
and memory management because

289
00:12:04,959 --> 00:12:09,119
you need to save tasks okay five minutes

290
00:12:08,320 --> 00:12:12,800
left

291
00:12:09,120 --> 00:12:16,079
let's go fast we have another

292
00:12:12,800 --> 00:12:17,760
kind of thing data flow parallelism

293
00:12:16,079 --> 00:12:20,000
four names pipeline graph stream

294
00:12:17,760 --> 00:12:22,079
data-driven parallelism

295
00:12:20,000 --> 00:12:23,600
the main thing is you can express

296
00:12:22,079 --> 00:12:27,839
dependencies between

297
00:12:23,600 --> 00:12:32,800
tasks like dependency independency

298
00:12:27,839 --> 00:12:36,800
outs and in-out clauses parallel api

299
00:12:32,800 --> 00:12:39,839
you have i think launch

300
00:12:36,800 --> 00:12:43,279
a thread that may be parallelized

301
00:12:39,839 --> 00:12:45,200
await a wait for a result so this is

302
00:12:43,279 --> 00:12:48,160
for io tasks and we can use uh

303
00:12:45,200 --> 00:12:48,160
sponsoring for

304
00:12:49,120 --> 00:12:53,760
multithreaded tasks data probabilism i

305
00:12:52,560 --> 00:12:55,359
talked about for loops

306
00:12:53,760 --> 00:12:58,480
that are flow parallelism there is no

307
00:12:55,360 --> 00:13:02,320
established api

308
00:12:58,480 --> 00:13:06,880
but you can use either declarative one

309
00:13:02,320 --> 00:13:08,800
where you ex create a flow graph

310
00:13:06,880 --> 00:13:09,600
explicitly before entering a parallel

311
00:13:08,800 --> 00:13:14,479
section

312
00:13:09,600 --> 00:13:19,519
or you can pass a handle like a promise

313
00:13:14,480 --> 00:13:21,200
to set a task ready or not

314
00:13:19,519 --> 00:13:24,240
so sources of overhead and

315
00:13:21,200 --> 00:13:27,440
implementation details

316
00:13:24,240 --> 00:13:30,480
you have scheduling overhead because

317
00:13:27,440 --> 00:13:31,839
switching between tasks is costly and

318
00:13:30,480 --> 00:13:35,360
switching between

319
00:13:31,839 --> 00:13:36,639
a channel when you need to create

320
00:13:35,360 --> 00:13:39,360
threads or destroy them

321
00:13:36,639 --> 00:13:41,680
is costly as well easy solution use a

322
00:13:39,360 --> 00:13:44,959
thread pull

323
00:13:41,680 --> 00:13:47,920
memory overhead you might

324
00:13:44,959 --> 00:13:50,399
if you use the fibonacci tasks it

325
00:13:47,920 --> 00:13:52,959
fibonacci 40 for example we create two

326
00:13:50,399 --> 00:13:54,480
at the power of 40 tasks meaning

327
00:13:52,959 --> 00:13:56,079
trillions of tasks

328
00:13:54,480 --> 00:13:57,600
and you need to deal with memory

329
00:13:56,079 --> 00:14:00,560
management

330
00:13:57,600 --> 00:14:02,800
when there is nothing to do so you will

331
00:14:00,560 --> 00:14:02,800
need

332
00:14:02,959 --> 00:14:08,160
some clever memory management with

333
00:14:05,839 --> 00:14:11,199
memory pools to deal with that

334
00:14:08,160 --> 00:14:13,120
and also on memory pools sometimes you

335
00:14:11,199 --> 00:14:15,120
have one thread that produces all the

336
00:14:13,120 --> 00:14:17,199
tasks and another thread that consumes

337
00:14:15,120 --> 00:14:18,320
everything and you cannot use caching in

338
00:14:17,199 --> 00:14:20,880
that case

339
00:14:18,320 --> 00:14:22,480
so you need to handle that as well i'm

340
00:14:20,880 --> 00:14:23,839
skipping on the cactus stack and

341
00:14:22,480 --> 00:14:26,880
segmented stacks

342
00:14:23,839 --> 00:14:30,240
but it's a complex research

343
00:14:26,880 --> 00:14:33,279
and a go and rust tried and failed

344
00:14:30,240 --> 00:14:33,920
and abandoned cactus stocks and the new

345
00:14:33,279 --> 00:14:37,279
gcc

346
00:14:33,920 --> 00:14:41,599
from three months ago also

347
00:14:37,279 --> 00:14:44,959
is stackless to avoid cactus task issues

348
00:14:41,600 --> 00:14:47,839
load balancing the meat of

349
00:14:44,959 --> 00:14:47,839
the talk let's say

350
00:14:48,320 --> 00:14:51,920
the issue with simple thread pool is

351
00:14:50,160 --> 00:14:52,880
that usually you have one global task

352
00:14:51,920 --> 00:14:55,439
fuel

353
00:14:52,880 --> 00:14:56,000
you dispatch a task to already thread

354
00:14:55,440 --> 00:14:59,440
but

355
00:14:56,000 --> 00:15:02,160
you have a contention issue because uh

356
00:14:59,440 --> 00:15:03,440
this task queue is if you have uh 10

357
00:15:02,160 --> 00:15:06,000
threads that are asking

358
00:15:03,440 --> 00:15:07,120
uh uh give me a task give me a test give

359
00:15:06,000 --> 00:15:10,839
me a task

360
00:15:07,120 --> 00:15:13,360
the task queue will be very

361
00:15:10,839 --> 00:15:15,360
busy and the best way to scale a

362
00:15:13,360 --> 00:15:17,839
parallel program is to share nothing

363
00:15:15,360 --> 00:15:20,079
this is andal's law and it tells you

364
00:15:17,839 --> 00:15:21,600
that if you have 95 percent of your

365
00:15:20,079 --> 00:15:23,519
program that is parallel

366
00:15:21,600 --> 00:15:25,600
the maximum speed up that you can get is

367
00:15:23,519 --> 00:15:28,720
only 20.

368
00:15:25,600 --> 00:15:31,279
so you need to avoid serial parts as

369
00:15:28,720 --> 00:15:34,160
much as possible

370
00:15:31,279 --> 00:15:35,360
and serial sources of sterilizations are

371
00:15:34,160 --> 00:15:37,839
the single task queue

372
00:15:35,360 --> 00:15:39,199
if you have a memory pool that is also

373
00:15:37,839 --> 00:15:41,199
global

374
00:15:39,199 --> 00:15:43,439
and you need to distribute everything on

375
00:15:41,199 --> 00:15:45,519
multiple threads

376
00:15:43,440 --> 00:15:48,000
so one way to do that is to have work

377
00:15:45,519 --> 00:15:51,279
stealing you have multiple worker

378
00:15:48,000 --> 00:15:54,639
one ip everyone has

379
00:15:51,279 --> 00:15:57,920
his own task queue it push and pop

380
00:15:54,639 --> 00:15:59,519
from one end of the queue and when you

381
00:15:57,920 --> 00:16:03,120
run out of tasks

382
00:15:59,519 --> 00:16:05,440
it gets from another worker

383
00:16:03,120 --> 00:16:06,720
a task this way the synchronization

384
00:16:05,440 --> 00:16:09,839
happens only if

385
00:16:06,720 --> 00:16:09,839
the queue is empty

386
00:16:10,800 --> 00:16:16,079
this is a summary of every

387
00:16:13,920 --> 00:16:17,519
uh relate things related to work

388
00:16:16,079 --> 00:16:19,920
stealing and there is

389
00:16:17,519 --> 00:16:20,959
mathematical proof of optimality

390
00:16:19,920 --> 00:16:24,000
asymptotical

391
00:16:20,959 --> 00:16:27,119
optimality which is why almost everyone

392
00:16:24,000 --> 00:16:30,079
except julia is using

393
00:16:27,120 --> 00:16:33,120
work stealing julia is using something

394
00:16:30,079 --> 00:16:36,479
called parallel depth first scheduling

395
00:16:33,120 --> 00:16:37,120
it's also proven optimal but it has a

396
00:16:36,480 --> 00:16:42,000
different

397
00:16:37,120 --> 00:16:43,759
performance profile and hovered profile

398
00:16:42,000 --> 00:16:45,839
it's still in development because it was

399
00:16:43,759 --> 00:16:50,160
released in september and

400
00:16:45,839 --> 00:16:52,160
it's uh you can look into it

401
00:16:50,160 --> 00:16:53,920
uh one thing that you should look into

402
00:16:52,160 --> 00:16:56,719
are memory models

403
00:16:53,920 --> 00:16:57,439
i've given the talk from help center

404
00:16:56,720 --> 00:17:00,480
inside

405
00:16:57,440 --> 00:17:01,600
uh it will if you want to use atomics uh

406
00:17:00,480 --> 00:17:05,599
relaxed atomics

407
00:17:01,600 --> 00:17:09,599
acquire release it's very important to

408
00:17:05,599 --> 00:17:09,599
watch it because it's written nowhere

409
00:17:11,439 --> 00:17:16,839
i'm skipping on load balancing you have

410
00:17:14,079 --> 00:17:19,359
a multiple strategy to

411
00:17:16,839 --> 00:17:22,958
uh share tasks

412
00:17:19,359 --> 00:17:26,319
still one still half adaptative

413
00:17:22,959 --> 00:17:29,679
but i don't have the time to go inside

414
00:17:26,319 --> 00:17:33,120
so the end work stealing in a runtime

415
00:17:29,679 --> 00:17:33,679
in a weekend you need a task data

416
00:17:33,120 --> 00:17:36,639
structure

417
00:17:33,679 --> 00:17:37,039
with a function pointer and a blob for

418
00:17:36,640 --> 00:17:39,360
task

419
00:17:37,039 --> 00:17:39,360
inputs

420
00:17:41,120 --> 00:17:46,799
or closure you need a start stop step

421
00:17:44,400 --> 00:17:49,280
for data parallelism if you want to

422
00:17:46,799 --> 00:17:51,520
express a for loop

423
00:17:49,280 --> 00:17:52,399
press next field for intrusive cues and

424
00:17:51,520 --> 00:17:55,918
dqs

425
00:17:52,400 --> 00:18:00,640
a future pointer to send the result back

426
00:17:55,919 --> 00:18:03,520
to the caller a work ceiling dq with a

427
00:18:00,640 --> 00:18:05,039
head tail filled and push first pop

428
00:18:03,520 --> 00:18:08,080
first and still last

429
00:18:05,039 --> 00:18:11,200
operations and for the api in it

430
00:18:08,080 --> 00:18:11,840
to create your thread pull exit to shut

431
00:18:11,200 --> 00:18:14,080
it on

432
00:18:11,840 --> 00:18:15,120
shut it down and spawn sponsoring to

433
00:18:14,080 --> 00:18:19,199
create tasks

434
00:18:15,120 --> 00:18:21,039
and retrieve their results

435
00:18:19,200 --> 00:18:23,440
some references those are also on the

436
00:18:21,039 --> 00:18:29,840
fosdem website

437
00:18:23,440 --> 00:18:29,840
and that's it so

438
00:18:40,840 --> 00:18:46,639
sorry yes

439
00:18:43,039 --> 00:18:47,440
so uh currently my uh run time is

440
00:18:46,640 --> 00:18:50,480
actually uh

441
00:18:47,440 --> 00:18:53,919
at least as fast or or faster than uh

442
00:18:50,480 --> 00:18:57,600
any other runtimes uh openmp tbb

443
00:18:53,919 --> 00:19:01,760
rayon hpx

444
00:18:57,600 --> 00:19:04,719
julia so really

445
00:19:01,760 --> 00:19:05,440
if you have a challenge on benchmark

446
00:19:04,720 --> 00:19:11,840
speed

447
00:19:05,440 --> 00:19:11,840
i'm ready to take it

