1
00:00:07,200 --> 00:00:11,040
all right i'm gonna go ahead and start

2
00:00:08,720 --> 00:00:12,000
my name is lily winfrey can people hear

3
00:00:11,040 --> 00:00:14,399
me

4
00:00:12,000 --> 00:00:15,440
okay i'm gonna try and be a little loud

5
00:00:14,400 --> 00:00:16,800
and i

6
00:00:15,440 --> 00:00:18,480
am going to be talking to you about

7
00:00:16,800 --> 00:00:19,439
frictionless data for reproducible

8
00:00:18,480 --> 00:00:21,199
research

9
00:00:19,439 --> 00:00:22,640
i'm a product manager at the open

10
00:00:21,199 --> 00:00:24,640
knowledge foundation

11
00:00:22,640 --> 00:00:27,039
and my background is in neuroscience

12
00:00:24,640 --> 00:00:28,000
research so i'm another neuroscientist

13
00:00:27,039 --> 00:00:30,240
today

14
00:00:28,000 --> 00:00:32,000
but now i do open data with a focus on

15
00:00:30,240 --> 00:00:34,640
open science

16
00:00:32,000 --> 00:00:35,840
and this bitly link here is the link to

17
00:00:34,640 --> 00:00:37,920
my slides

18
00:00:35,840 --> 00:00:40,079
there's a lot of content that has links

19
00:00:37,920 --> 00:00:41,120
in here and some content that i threw up

20
00:00:40,079 --> 00:00:43,440
that you can look at

21
00:00:41,120 --> 00:00:44,959
later after the talks done to practice

22
00:00:43,440 --> 00:00:47,920
and play on your own

23
00:00:44,960 --> 00:00:49,320
but to start i want to ask you how many

24
00:00:47,920 --> 00:00:50,559
of you have heard about the

25
00:00:49,320 --> 00:00:53,840
reproducibility

26
00:00:50,559 --> 00:00:56,399
crisis in research

27
00:00:53,840 --> 00:00:58,399
all right great cool we're going to be

28
00:00:56,399 --> 00:01:01,039
talking about that today and

29
00:00:58,399 --> 00:01:02,800
um so for those of you that don't know

30
00:01:01,039 --> 00:01:04,479
it's the idea that there are some

31
00:01:02,800 --> 00:01:06,320
experiments in science that aren't

32
00:01:04,479 --> 00:01:08,880
reproducible

33
00:01:06,320 --> 00:01:09,360
and i'm gonna show one recent example of

34
00:01:08,880 --> 00:01:14,080
this

35
00:01:09,360 --> 00:01:14,080
it's dr kate leskowski recently had to

36
00:01:14,320 --> 00:01:20,399
had to get her paper retracted

37
00:01:17,759 --> 00:01:22,240
because of a data issue that they found

38
00:01:20,400 --> 00:01:25,200
years after it was published

39
00:01:22,240 --> 00:01:26,320
and basically they couldn't understand

40
00:01:25,200 --> 00:01:29,040
some of the data

41
00:01:26,320 --> 00:01:30,079
and they couldn't understand how it was

42
00:01:29,040 --> 00:01:32,799
created

43
00:01:30,079 --> 00:01:34,479
and this is just a horrible feeling for

44
00:01:32,799 --> 00:01:35,840
any scientist to have to retract your

45
00:01:34,479 --> 00:01:38,000
paper

46
00:01:35,840 --> 00:01:39,680
so why does this happen why are

47
00:01:38,000 --> 00:01:41,280
experiments not reproducible

48
00:01:39,680 --> 00:01:43,360
there's a lot of debate about this right

49
00:01:41,280 --> 00:01:43,920
now but there are certain things that we

50
00:01:43,360 --> 00:01:46,399
know

51
00:01:43,920 --> 00:01:48,479
and for instance methods for doing

52
00:01:46,399 --> 00:01:49,759
experiments are often not published or

53
00:01:48,479 --> 00:01:50,798
they're not published openly or

54
00:01:49,759 --> 00:01:52,720
completely

55
00:01:50,799 --> 00:01:54,799
and the same for data especially raw

56
00:01:52,720 --> 00:01:56,320
data is often not published

57
00:01:54,799 --> 00:01:58,960
and so it can be really difficult to

58
00:01:56,320 --> 00:01:59,439
understand what happened from raw data

59
00:01:58,960 --> 00:02:02,079
to the

60
00:01:59,439 --> 00:02:04,000
analyzed and published results so today

61
00:02:02,079 --> 00:02:06,960
we're going to be talking a lot about

62
00:02:04,000 --> 00:02:08,080
these data management issues in research

63
00:02:06,960 --> 00:02:11,120
and the frictionless data

64
00:02:08,080 --> 00:02:14,720
project is focused on helping fix

65
00:02:11,120 --> 00:02:15,760
some of these data management issues

66
00:02:14,720 --> 00:02:17,520
first i'm going to tell you about the

67
00:02:15,760 --> 00:02:19,120
open knowledge foundation where i work

68
00:02:17,520 --> 00:02:20,879
and then i'll get into the technical

69
00:02:19,120 --> 00:02:23,200
background of frictionless data

70
00:02:20,879 --> 00:02:25,280
and then get into a use case where we've

71
00:02:23,200 --> 00:02:27,119
been working with researchers

72
00:02:25,280 --> 00:02:29,280
so the open knowledge foundation is a

73
00:02:27,120 --> 00:02:30,000
non-profit we've been around for 15

74
00:02:29,280 --> 00:02:32,000
years

75
00:02:30,000 --> 00:02:34,080
and we're focused on creating a fair

76
00:02:32,000 --> 00:02:35,760
free and open future

77
00:02:34,080 --> 00:02:37,200
this is a future where everyone has

78
00:02:35,760 --> 00:02:39,840
access to data

79
00:02:37,200 --> 00:02:40,480
and where people know how to use that

80
00:02:39,840 --> 00:02:44,400
data

81
00:02:40,480 --> 00:02:44,399
to drive social positive change

82
00:02:45,280 --> 00:02:49,599
and the project that i work on at okf is

83
00:02:47,840 --> 00:02:50,080
the frictionless data for reproducible

84
00:02:49,599 --> 00:02:53,280
research

85
00:02:50,080 --> 00:02:54,080
project this is where we are removing

86
00:02:53,280 --> 00:02:56,480
friction

87
00:02:54,080 --> 00:02:57,120
and research data to move from data to

88
00:02:56,480 --> 00:02:59,920
insight

89
00:02:57,120 --> 00:03:01,840
faster this is an open source project

90
00:02:59,920 --> 00:03:03,518
and we're very community focused

91
00:03:01,840 --> 00:03:05,280
and by that i mean that we really depend

92
00:03:03,519 --> 00:03:06,560
on our community to make this project

93
00:03:05,280 --> 00:03:08,400
successful

94
00:03:06,560 --> 00:03:10,480
so right here i have pictured my

95
00:03:08,400 --> 00:03:12,239
colleagues on this specific project

96
00:03:10,480 --> 00:03:14,399
but it's more than just four of us that

97
00:03:12,239 --> 00:03:16,319
do this work we really rely on our

98
00:03:14,400 --> 00:03:17,519
community to give us feedback and use

99
00:03:16,319 --> 00:03:19,599
our tools

100
00:03:17,519 --> 00:03:21,040
and after this talk i hope that i've

101
00:03:19,599 --> 00:03:23,599
convinced many of you to join our

102
00:03:21,040 --> 00:03:26,720
community

103
00:03:23,599 --> 00:03:28,319
okay so this project is overseen by the

104
00:03:26,720 --> 00:03:30,319
open knowledge foundation

105
00:03:28,319 --> 00:03:31,599
and is funded by the alfred p sloan

106
00:03:30,319 --> 00:03:33,119
foundation

107
00:03:31,599 --> 00:03:35,599
we have three main ways that we

108
00:03:33,120 --> 00:03:37,040
collaborate we have the fellows program

109
00:03:35,599 --> 00:03:39,040
where we're working with early career

110
00:03:37,040 --> 00:03:41,040
researchers to teach them about open

111
00:03:39,040 --> 00:03:43,200
science and data management

112
00:03:41,040 --> 00:03:44,720
and using the frictionless tools we have

113
00:03:43,200 --> 00:03:46,000
the tool fund where we're working with

114
00:03:44,720 --> 00:03:48,560
developers to develop

115
00:03:46,000 --> 00:03:49,760
new tooling for reproducible research

116
00:03:48,560 --> 00:03:51,920
based off of

117
00:03:49,760 --> 00:03:53,518
frictionless data and we're about to

118
00:03:51,920 --> 00:03:56,720
open up another round of this

119
00:03:53,519 --> 00:03:58,400
so stay tuned we give funding to people

120
00:03:56,720 --> 00:04:00,080
and then we also have the pilots which

121
00:03:58,400 --> 00:04:02,080
are collaborations they're very

122
00:04:00,080 --> 00:04:04,000
intensive one-on-one collaborations

123
00:04:02,080 --> 00:04:06,239
between us and our developers

124
00:04:04,000 --> 00:04:08,640
and researcher teams to help solve data

125
00:04:06,239 --> 00:04:10,560
workflow issues with those researchers

126
00:04:08,640 --> 00:04:12,238
and we're also actively looking for new

127
00:04:10,560 --> 00:04:15,599
pilot collaborations so if you're

128
00:04:12,239 --> 00:04:17,600
interested please come talk to me

129
00:04:15,599 --> 00:04:19,039
so i've said frictionless data a lot but

130
00:04:17,600 --> 00:04:20,560
what does that mean

131
00:04:19,040 --> 00:04:22,560
basically we're trying to remove

132
00:04:20,560 --> 00:04:24,320
frictions and working with data

133
00:04:22,560 --> 00:04:25,759
you can think of these as like the data

134
00:04:24,320 --> 00:04:28,400
cleaning steps

135
00:04:25,759 --> 00:04:29,600
and questions such as what's the license

136
00:04:28,400 --> 00:04:31,440
for this data

137
00:04:29,600 --> 00:04:33,360
what does this data value mean you know

138
00:04:31,440 --> 00:04:35,600
can i even use this data

139
00:04:33,360 --> 00:04:37,759
can i use data that was created in excel

140
00:04:35,600 --> 00:04:39,759
and then run it in my python code

141
00:04:37,759 --> 00:04:42,080
who created the data and things like

142
00:04:39,759 --> 00:04:43,840
checking the quality of the data

143
00:04:42,080 --> 00:04:45,280
so oftentimes these are thought of as

144
00:04:43,840 --> 00:04:46,799
like the boring parts

145
00:04:45,280 --> 00:04:49,039
that you have to do to data before you

146
00:04:46,800 --> 00:04:51,040
can analyze your data and get a result

147
00:04:49,040 --> 00:04:52,160
but these are very important anyone

148
00:04:51,040 --> 00:04:56,080
that's worked with data

149
00:04:52,160 --> 00:04:57,639
knows how important cleaning data is

150
00:04:56,080 --> 00:05:00,400
frictionless data is a set of

151
00:04:57,639 --> 00:05:02,160
specifications for data and metadata

152
00:05:00,400 --> 00:05:04,719
interoperability

153
00:05:02,160 --> 00:05:06,160
and a collection of open source software

154
00:05:04,720 --> 00:05:08,240
libraries

155
00:05:06,160 --> 00:05:10,880
it's also a range of best practices for

156
00:05:08,240 --> 00:05:12,160
data management and importantly it's

157
00:05:10,880 --> 00:05:14,400
platform agnostic

158
00:05:12,160 --> 00:05:16,080
meaning that it's very interoperable and

159
00:05:14,400 --> 00:05:19,198
specific or purposefully

160
00:05:16,080 --> 00:05:20,240
generalizable so the main question i

161
00:05:19,199 --> 00:05:22,800
want to talk about today

162
00:05:20,240 --> 00:05:23,680
is how can researchers and other data

163
00:05:22,800 --> 00:05:26,960
wranglers

164
00:05:23,680 --> 00:05:28,800
use frictionless data and to get into

165
00:05:26,960 --> 00:05:30,719
this question i'm going to talk about

166
00:05:28,800 --> 00:05:32,639
one of our pilot use cases that's

167
00:05:30,720 --> 00:05:35,039
ongoing right now

168
00:05:32,639 --> 00:05:36,479
it's with the bicodimo group which

169
00:05:35,039 --> 00:05:39,120
stands for

170
00:05:36,479 --> 00:05:41,758
biological and chemical oceanographic

171
00:05:39,120 --> 00:05:44,720
data management office

172
00:05:41,759 --> 00:05:46,240
and this group is funded by the nsf

173
00:05:44,720 --> 00:05:47,360
which is a major science funder in the

174
00:05:46,240 --> 00:05:50,240
us

175
00:05:47,360 --> 00:05:51,199
and basically anyone that does

176
00:05:50,240 --> 00:05:53,520
oceanographic

177
00:05:51,199 --> 00:05:54,479
research in the u.s that's funded by the

178
00:05:53,520 --> 00:05:57,520
nsf

179
00:05:54,479 --> 00:05:59,758
submits their data to beco demo and then

180
00:05:57,520 --> 00:06:01,280
bcodimo has data managers

181
00:05:59,759 --> 00:06:04,240
that go through and they clean all of

182
00:06:01,280 --> 00:06:06,799
that data and then they host it as well

183
00:06:04,240 --> 00:06:07,360
and so when it's clean and hosted other

184
00:06:06,800 --> 00:06:09,199
people

185
00:06:07,360 --> 00:06:11,360
can access that data including the

186
00:06:09,199 --> 00:06:14,160
public and other researchers that can

187
00:06:11,360 --> 00:06:15,840
then build upon that research

188
00:06:14,160 --> 00:06:17,520
and i want to mention the team that

189
00:06:15,840 --> 00:06:20,080
we're working with at bicodimo

190
00:06:17,520 --> 00:06:21,840
is amber york conrad schloer adam

191
00:06:20,080 --> 00:06:23,840
shepard and danny kinkade

192
00:06:21,840 --> 00:06:26,080
and i also shamelessly stole a bunch of

193
00:06:23,840 --> 00:06:26,960
these slides from amber york she gave a

194
00:06:26,080 --> 00:06:29,120
talk at csv

195
00:06:26,960 --> 00:06:30,318
conf and that's what this zenodo link

196
00:06:29,120 --> 00:06:34,000
here is is the

197
00:06:30,319 --> 00:06:35,600
link to the rest of her slides okay i

198
00:06:34,000 --> 00:06:37,360
love talking about beco demo because

199
00:06:35,600 --> 00:06:39,840
their data is messy

200
00:06:37,360 --> 00:06:41,199
as you can imagine they have data about

201
00:06:39,840 --> 00:06:43,758
everything in the ocean

202
00:06:41,199 --> 00:06:44,240
they have data on coral reefs they have

203
00:06:43,759 --> 00:06:46,880
data

204
00:06:44,240 --> 00:06:49,280
on ocean salinity they have data on

205
00:06:46,880 --> 00:06:50,800
jellyfish like it's very interesting

206
00:06:49,280 --> 00:06:53,520
data

207
00:06:50,800 --> 00:06:53,919
but it's also messy if you can see where

208
00:06:53,520 --> 00:06:56,000
i'm

209
00:06:53,919 --> 00:06:57,440
pointing here there's this column that's

210
00:06:56,000 --> 00:06:59,440
showing dates

211
00:06:57,440 --> 00:07:00,960
and it's a date range so there's two

212
00:06:59,440 --> 00:07:04,000
different data points

213
00:07:00,960 --> 00:07:05,599
in one one cell and there the dates are

214
00:07:04,000 --> 00:07:07,360
written how americans

215
00:07:05,599 --> 00:07:08,960
write dates which is how no one else

216
00:07:07,360 --> 00:07:12,720
writes a date

217
00:07:08,960 --> 00:07:12,719
so it's confusing

218
00:07:14,240 --> 00:07:19,680
and so um the data managers get this

219
00:07:17,919 --> 00:07:20,560
messy data and then they really have to

220
00:07:19,680 --> 00:07:22,880
wrangle it

221
00:07:20,560 --> 00:07:25,440
to make it more clean so other people

222
00:07:22,880 --> 00:07:25,440
can use it

223
00:07:25,599 --> 00:07:30,479
so we're working with the data managers

224
00:07:28,560 --> 00:07:32,000
using this program called data package

225
00:07:30,479 --> 00:07:32,880
pipelines that i'll tell you about in a

226
00:07:32,000 --> 00:07:35,759
minute

227
00:07:32,880 --> 00:07:37,520
and trying to help the data managers in

228
00:07:35,759 --> 00:07:40,080
their various tasks

229
00:07:37,520 --> 00:07:41,198
so for example the data managers need to

230
00:07:40,080 --> 00:07:43,440
do things like

231
00:07:41,199 --> 00:07:44,960
they add spatiotemporal context in

232
00:07:43,440 --> 00:07:47,599
standardized formats

233
00:07:44,960 --> 00:07:49,280
things like date and time or even time

234
00:07:47,599 --> 00:07:50,800
zones because this data is from around

235
00:07:49,280 --> 00:07:52,159
the world

236
00:07:50,800 --> 00:07:54,240
they record things like latitude and

237
00:07:52,160 --> 00:07:56,479
longitude and make it standard

238
00:07:54,240 --> 00:07:58,560
even the depth at which a measurement is

239
00:07:56,479 --> 00:08:00,159
taken under the ocean

240
00:07:58,560 --> 00:08:01,919
and then they also need to do things

241
00:08:00,160 --> 00:08:04,879
like correct quality issues

242
00:08:01,919 --> 00:08:06,639
fix incorrect inconsistent formatting

243
00:08:04,879 --> 00:08:09,520
corrupt data characters

244
00:08:06,639 --> 00:08:10,000
data gaps like is this value that says n

245
00:08:09,520 --> 00:08:12,159
a

246
00:08:10,000 --> 00:08:14,960
is that actually nothing or does it

247
00:08:12,160 --> 00:08:16,639
connote something to that researcher

248
00:08:14,960 --> 00:08:18,719
they also have to fix invalid species

249
00:08:16,639 --> 00:08:20,720
names and things like typos

250
00:08:18,720 --> 00:08:22,000
and i'm sure many of you that have dealt

251
00:08:20,720 --> 00:08:24,879
with messy data

252
00:08:22,000 --> 00:08:26,160
recognize many of these steps and

253
00:08:24,879 --> 00:08:29,520
basically what they're doing

254
00:08:26,160 --> 00:08:32,160
is reformatting the data for reusability

255
00:08:29,520 --> 00:08:32,159
by others

256
00:08:32,559 --> 00:08:35,760
so this collaboration that we have with

257
00:08:34,320 --> 00:08:38,159
beco demo

258
00:08:35,760 --> 00:08:40,000
is where we're going in and giving

259
00:08:38,159 --> 00:08:43,120
developer time to try and take

260
00:08:40,000 --> 00:08:45,839
their messy data turn it into clean data

261
00:08:43,120 --> 00:08:48,080
and then host it for others to use and

262
00:08:45,839 --> 00:08:50,080
we're trying to make this entire process

263
00:08:48,080 --> 00:08:52,480
reproducible so that other people can

264
00:08:50,080 --> 00:08:56,160
understand what we did to this data or

265
00:08:52,480 --> 00:08:56,160
what bikodimo did to the data

266
00:08:56,480 --> 00:09:00,399
okay so this is a great one of my

267
00:08:58,959 --> 00:09:02,239
favorite slides

268
00:09:00,399 --> 00:09:03,839
is that the researchers are out there

269
00:09:02,240 --> 00:09:04,959
they're working hard they're collecting

270
00:09:03,839 --> 00:09:06,720
this data

271
00:09:04,959 --> 00:09:08,800
and then we come over and we're like oh

272
00:09:06,720 --> 00:09:10,959
hey did you record the metadata

273
00:09:08,800 --> 00:09:11,920
like did you remember to do that and the

274
00:09:10,959 --> 00:09:14,399
answer is usually

275
00:09:11,920 --> 00:09:15,680
no so i want to talk a little bit more

276
00:09:14,399 --> 00:09:18,080
about metadata

277
00:09:15,680 --> 00:09:18,079
today

278
00:09:18,959 --> 00:09:22,959
and tell you also how the frictionless

279
00:09:20,800 --> 00:09:24,959
data tools are useful

280
00:09:22,959 --> 00:09:26,239
so first of all they can be used to keep

281
00:09:24,959 --> 00:09:27,599
track of your metadata

282
00:09:26,240 --> 00:09:28,880
we were just talking about metadata a

283
00:09:27,600 --> 00:09:30,160
little bit in the last talk but for

284
00:09:28,880 --> 00:09:32,480
those of you that don't know

285
00:09:30,160 --> 00:09:34,319
it's data about your data it's things

286
00:09:32,480 --> 00:09:38,080
like what's the license

287
00:09:34,320 --> 00:09:40,320
and like what are column names so

288
00:09:38,080 --> 00:09:42,320
using the frictionless data tooling such

289
00:09:40,320 --> 00:09:44,880
as this browser tool here

290
00:09:42,320 --> 00:09:46,480
and again i posted these slides so you

291
00:09:44,880 --> 00:09:48,240
can click on these links later and play

292
00:09:46,480 --> 00:09:50,800
around with them

293
00:09:48,240 --> 00:09:51,839
you in this browser tool you can take

294
00:09:50,800 --> 00:09:54,079
raw data

295
00:09:51,839 --> 00:09:55,360
and insert it and the tool will

296
00:09:54,080 --> 00:09:58,080
automatically

297
00:09:55,360 --> 00:09:59,040
create metadata for you that you can go

298
00:09:58,080 --> 00:10:01,519
in and edit

299
00:09:59,040 --> 00:10:02,719
and the metadata is in json so it's

300
00:10:01,519 --> 00:10:05,519
machine readable

301
00:10:02,720 --> 00:10:07,120
and it's interoperable and why is this

302
00:10:05,519 --> 00:10:09,600
important for scientists

303
00:10:07,120 --> 00:10:11,760
well in all data wranglers really it's

304
00:10:09,600 --> 00:10:12,640
important to keep track of your metadata

305
00:10:11,760 --> 00:10:14,959
so that you

306
00:10:12,640 --> 00:10:16,880
know what is in your data you know

307
00:10:14,959 --> 00:10:18,079
future you knows that and anyone else

308
00:10:16,880 --> 00:10:20,160
that wants to use your data

309
00:10:18,079 --> 00:10:22,160
can know that as well like for instance

310
00:10:20,160 --> 00:10:24,560
i know what sem means because

311
00:10:22,160 --> 00:10:25,839
i was a scientist and i did statistics

312
00:10:24,560 --> 00:10:28,959
but there's a good chance that someone

313
00:10:25,839 --> 00:10:30,959
else might not know what that means

314
00:10:28,959 --> 00:10:32,719
okay another thing that frictionless

315
00:10:30,959 --> 00:10:35,040
data can do is help you

316
00:10:32,720 --> 00:10:36,160
package your data and this is where you

317
00:10:35,040 --> 00:10:37,920
take raw data

318
00:10:36,160 --> 00:10:40,079
and your metadata and package it

319
00:10:37,920 --> 00:10:42,000
together and we like to think of this as

320
00:10:40,079 --> 00:10:44,079
a shipping container analogy

321
00:10:42,000 --> 00:10:46,560
where the container contains your raw

322
00:10:44,079 --> 00:10:48,560
data and your metadata optionally you

323
00:10:46,560 --> 00:10:49,040
can also include a schema about your

324
00:10:48,560 --> 00:10:51,920
data

325
00:10:49,040 --> 00:10:52,800
and this describes kind of like the big

326
00:10:51,920 --> 00:10:54,719
picture

327
00:10:52,800 --> 00:10:56,160
about your data it can include things

328
00:10:54,720 --> 00:10:58,320
like what

329
00:10:56,160 --> 00:10:59,920
type of data should be in a column and

330
00:10:58,320 --> 00:11:03,040
how many rows and columns

331
00:10:59,920 --> 00:11:04,719
your data set has all right so we have

332
00:11:03,040 --> 00:11:05,680
two different tools to work with data

333
00:11:04,720 --> 00:11:08,079
packages

334
00:11:05,680 --> 00:11:09,680
we have many software libraries and

335
00:11:08,079 --> 00:11:11,279
they're all open source again

336
00:11:09,680 --> 00:11:12,839
and then again we have this browser tool

337
00:11:11,279 --> 00:11:14,240
where you can actually create a data

338
00:11:12,839 --> 00:11:16,160
package

339
00:11:14,240 --> 00:11:17,519
and why is it important to package your

340
00:11:16,160 --> 00:11:20,160
data well

341
00:11:17,519 --> 00:11:21,040
package data is useful data when you

342
00:11:20,160 --> 00:11:22,880
have

343
00:11:21,040 --> 00:11:24,240
package data i'm going to use a lego

344
00:11:22,880 --> 00:11:25,680
analogy to talk about this

345
00:11:24,240 --> 00:11:28,000
i'm assuming many of you have played

346
00:11:25,680 --> 00:11:30,000
with legos before and one of the

347
00:11:28,000 --> 00:11:31,360
best things about legos in my opinion is

348
00:11:30,000 --> 00:11:33,920
that you can take different

349
00:11:31,360 --> 00:11:36,160
blocks from different sets and they

350
00:11:33,920 --> 00:11:37,680
automatically work together

351
00:11:36,160 --> 00:11:39,199
and it's the same idea with a data

352
00:11:37,680 --> 00:11:42,399
package it's in this nice

353
00:11:39,200 --> 00:11:44,720
standard package format and then you can

354
00:11:42,399 --> 00:11:48,079
use different tools and just plug and

355
00:11:44,720 --> 00:11:50,079
play it's very interoperable

356
00:11:48,079 --> 00:11:51,120
also package data can be easily

357
00:11:50,079 --> 00:11:53,439
published

358
00:11:51,120 --> 00:11:57,360
and you can publish his data on data

359
00:11:53,440 --> 00:11:58,959
repositories such as zenodo

360
00:11:57,360 --> 00:12:01,120
all right another thing you can do with

361
00:11:58,959 --> 00:12:02,239
frictionless data is create a schema to

362
00:12:01,120 --> 00:12:03,920
describe your data

363
00:12:02,240 --> 00:12:05,519
and then validate your data based on

364
00:12:03,920 --> 00:12:07,680
that schema

365
00:12:05,519 --> 00:12:08,959
and why is this important this is my

366
00:12:07,680 --> 00:12:12,000
favorite horror story

367
00:12:08,959 --> 00:12:15,040
about research data being invalid

368
00:12:12,000 --> 00:12:17,279
is that excel will actually take

369
00:12:15,040 --> 00:12:18,319
certain gene names and convert them to

370
00:12:17,279 --> 00:12:21,600
date

371
00:12:18,320 --> 00:12:24,959
without telling you it does it silently

372
00:12:21,600 --> 00:12:25,839
so there are genes like 7 that excel

373
00:12:24,959 --> 00:12:28,880
will convert

374
00:12:25,839 --> 00:12:31,519
to december 7th and then

375
00:12:28,880 --> 00:12:32,399
and then that data value is no longer

376
00:12:31,519 --> 00:12:33,920
useful for you

377
00:12:32,399 --> 00:12:35,760
and there are several papers that had to

378
00:12:33,920 --> 00:12:36,560
be pulled because the analysis was

379
00:12:35,760 --> 00:12:37,519
incorrect

380
00:12:36,560 --> 00:12:39,599
because this happened and the

381
00:12:37,519 --> 00:12:41,200
researchers didn't realize it

382
00:12:39,600 --> 00:12:43,040
so one way to know that this has

383
00:12:41,200 --> 00:12:44,320
happened to your data is to create a

384
00:12:43,040 --> 00:12:46,800
schema

385
00:12:44,320 --> 00:12:48,399
and here are the frictionless data tools

386
00:12:46,800 --> 00:12:51,120
that will help you with this

387
00:12:48,399 --> 00:12:52,639
schema would tell you you know column a

388
00:12:51,120 --> 00:12:55,040
is supposed to be strings

389
00:12:52,639 --> 00:12:55,760
and it will and so if you validate based

390
00:12:55,040 --> 00:12:58,240
on that

391
00:12:55,760 --> 00:12:59,600
and it detects oh there's a date format

392
00:12:58,240 --> 00:13:01,839
instead of a string there

393
00:12:59,600 --> 00:13:03,440
it will give you an error and so that's

394
00:13:01,839 --> 00:13:06,720
what i'm showing you here on the

395
00:13:03,440 --> 00:13:10,000
over here this is the good tables

396
00:13:06,720 --> 00:13:11,600
client and you can do good tables and it

397
00:13:10,000 --> 00:13:13,200
will validate your data

398
00:13:11,600 --> 00:13:15,839
and here it's showing you this is a

399
00:13:13,200 --> 00:13:17,920
valid data set there are zero errors

400
00:13:15,839 --> 00:13:19,839
but if it was invalid it would tell you

401
00:13:17,920 --> 00:13:21,760
exactly where those errors are and what

402
00:13:19,839 --> 00:13:24,560
the error is

403
00:13:21,760 --> 00:13:26,160
so we have try.good tables is a browser

404
00:13:24,560 --> 00:13:27,518
tool to look at this and we have good

405
00:13:26,160 --> 00:13:29,439
table software libraries

406
00:13:27,519 --> 00:13:32,639
and table schema software libraries that

407
00:13:29,440 --> 00:13:32,639
will help write schemas

408
00:13:32,720 --> 00:13:35,360
all right the final piece of

409
00:13:34,160 --> 00:13:36,800
frictionless data software i'm going to

410
00:13:35,360 --> 00:13:39,040
tell you about really quickly

411
00:13:36,800 --> 00:13:40,639
is the data package pipelines and this

412
00:13:39,040 --> 00:13:42,079
is what we're using with this pilot

413
00:13:40,639 --> 00:13:43,920
collaboration

414
00:13:42,079 --> 00:13:45,439
data package pipelines is a data

415
00:13:43,920 --> 00:13:48,800
processing pipeline

416
00:13:45,440 --> 00:13:51,040
software again open source it's a python

417
00:13:48,800 --> 00:13:54,719
framework for declarative processing of

418
00:13:51,040 --> 00:13:57,040
tabular data and so it has standardized

419
00:13:54,720 --> 00:13:57,519
data processing steps already built into

420
00:13:57,040 --> 00:14:00,480
it

421
00:13:57,519 --> 00:14:02,079
things like joins find and replace but

422
00:14:00,480 --> 00:14:05,360
in addition to that you can write

423
00:14:02,079 --> 00:14:08,079
custom processors in python

424
00:14:05,360 --> 00:14:08,560
for things that you know your specific

425
00:14:08,079 --> 00:14:12,000
data

426
00:14:08,560 --> 00:14:12,800
needs to happen a lot these pipelines

427
00:14:12,000 --> 00:14:16,079
are defined

428
00:14:12,800 --> 00:14:18,719
in a pipeline spec yaml file and this

429
00:14:16,079 --> 00:14:20,079
includes the specific processors that

430
00:14:18,720 --> 00:14:22,480
were done on your data

431
00:14:20,079 --> 00:14:24,239
and any execution parameters and having

432
00:14:22,480 --> 00:14:27,199
this information written down really

433
00:14:24,240 --> 00:14:30,639
helps with reproducibility

434
00:14:27,199 --> 00:14:31,199
fdd or dpp produces a single data

435
00:14:30,639 --> 00:14:34,720
package

436
00:14:31,199 --> 00:14:38,479
as its output okay

437
00:14:34,720 --> 00:14:41,120
finally we have all of these software

438
00:14:38,480 --> 00:14:43,120
on our website this is just a screenshot

439
00:14:41,120 --> 00:14:45,199
and i encourage you to go look at it we

440
00:14:43,120 --> 00:14:48,720
have python code is

441
00:14:45,199 --> 00:14:49,199
our main software library that we write

442
00:14:48,720 --> 00:14:52,480
in

443
00:14:49,199 --> 00:14:55,199
but we also have javascript ruby r etc

444
00:14:52,480 --> 00:14:57,040
we have a lot of languages

445
00:14:55,199 --> 00:14:58,639
all right so now i'm going to go back to

446
00:14:57,040 --> 00:15:00,639
our use case and show you how we're

447
00:14:58,639 --> 00:15:02,399
using data package pipelines

448
00:15:00,639 --> 00:15:03,920
i like to think of frictionless data is

449
00:15:02,399 --> 00:15:06,639
coming in and trying to help

450
00:15:03,920 --> 00:15:09,920
make this research data really useful

451
00:15:06,639 --> 00:15:09,920
and live up to its full

452
00:15:10,839 --> 00:15:14,000
potential

453
00:15:12,320 --> 00:15:15,680
all right so how does data package

454
00:15:14,000 --> 00:15:19,600
pipelines help

455
00:15:15,680 --> 00:15:21,359
the bcodemo users first of all it gives

456
00:15:19,600 --> 00:15:22,560
data managers a more immersive

457
00:15:21,360 --> 00:15:24,399
experience

458
00:15:22,560 --> 00:15:26,079
part of this pilot collaboration has

459
00:15:24,399 --> 00:15:29,360
included building a new ui

460
00:15:26,079 --> 00:15:31,120
for the data managers this has reduced

461
00:15:29,360 --> 00:15:32,720
data set processing time

462
00:15:31,120 --> 00:15:35,120
it's removed the barrier programmatic

463
00:15:32,720 --> 00:15:37,199
ability for these data managers

464
00:15:35,120 --> 00:15:38,639
and it's avoided having to hand write

465
00:15:37,199 --> 00:15:40,880
things like pipeline spec

466
00:15:38,639 --> 00:15:42,000
file or python scripts which you know

467
00:15:40,880 --> 00:15:45,360
reduces errors

468
00:15:42,000 --> 00:15:47,120
and is faster you can also add custom

469
00:15:45,360 --> 00:15:50,399
metadata to the pipeline

470
00:15:47,120 --> 00:15:52,399
the beco demo users have really rich

471
00:15:50,399 --> 00:15:54,399
metadata and so when we're working with

472
00:15:52,399 --> 00:15:56,320
the beco demo data managers they wanted

473
00:15:54,399 --> 00:15:58,959
to make sure that we were able to

474
00:15:56,320 --> 00:16:00,880
capture all of this metadata and keep

475
00:15:58,959 --> 00:16:03,359
track of it

476
00:16:00,880 --> 00:16:05,279
and importantly you can also add

477
00:16:03,360 --> 00:16:05,839
capabilities that were not already in

478
00:16:05,279 --> 00:16:08,320
the

479
00:16:05,839 --> 00:16:11,120
base data package pipelines by adding

480
00:16:08,320 --> 00:16:12,320
custom processors

481
00:16:11,120 --> 00:16:14,240
now i'm going to show you some of those

482
00:16:12,320 --> 00:16:17,120
custom processors that were added

483
00:16:14,240 --> 00:16:18,800
and also an example of what the bcodemo

484
00:16:17,120 --> 00:16:21,040
pipeline looks like

485
00:16:18,800 --> 00:16:22,079
so each one of these arrows is a

486
00:16:21,040 --> 00:16:24,399
different processor

487
00:16:22,079 --> 00:16:25,120
step they're things like load around the

488
00:16:24,399 --> 00:16:26,240
field

489
00:16:25,120 --> 00:16:29,040
and then we're going to look at an

490
00:16:26,240 --> 00:16:30,800
example of a find and replace

491
00:16:29,040 --> 00:16:33,120
here you can see in the notes this is

492
00:16:30,800 --> 00:16:35,920
fixing inconsistent time format

493
00:16:33,120 --> 00:16:37,519
some didn't have seconds and to do this

494
00:16:35,920 --> 00:16:38,000
we've highlighted the field which is

495
00:16:37,519 --> 00:16:40,399
time

496
00:16:38,000 --> 00:16:42,320
and then in entered in the find pattern

497
00:16:40,399 --> 00:16:44,880
then the replace pattern

498
00:16:42,320 --> 00:16:47,360
and this particular piece of the

499
00:16:44,880 --> 00:16:49,360
processing pipeline is now shown in this

500
00:16:47,360 --> 00:16:51,759
pipeline spec emo file

501
00:16:49,360 --> 00:16:53,120
and what you can see is that it is it's

502
00:16:51,759 --> 00:16:55,199
human readable

503
00:16:53,120 --> 00:16:57,199
so that the next person that uses this

504
00:16:55,199 --> 00:16:58,319
pipeline knows exactly what happened to

505
00:16:57,199 --> 00:17:00,880
the data

506
00:16:58,320 --> 00:17:03,519
and knows how to reproduce what happened

507
00:17:00,880 --> 00:17:03,519
to the data

508
00:17:03,600 --> 00:17:07,760
here's another example where we are

509
00:17:05,359 --> 00:17:09,678
going to show changing the date format

510
00:17:07,760 --> 00:17:12,799
using data package pipelines

511
00:17:09,679 --> 00:17:15,439
so here we have this date column again

512
00:17:12,799 --> 00:17:16,639
written how americans write dates not

513
00:17:15,439 --> 00:17:19,520
super useful

514
00:17:16,640 --> 00:17:20,880
and so the output from running this

515
00:17:19,520 --> 00:17:23,520
processor step

516
00:17:20,880 --> 00:17:24,079
is two columns one where we have the

517
00:17:23,520 --> 00:17:27,039
date

518
00:17:24,079 --> 00:17:29,120
in a nice iso standard format and then

519
00:17:27,039 --> 00:17:32,480
another column where we have the date

520
00:17:29,120 --> 00:17:34,719
still in the same way that the rich

521
00:17:32,480 --> 00:17:36,640
that the researcher originally put it in

522
00:17:34,720 --> 00:17:38,559
in case that connoted some important

523
00:17:36,640 --> 00:17:40,960
thing for the researcher but it's in a

524
00:17:38,559 --> 00:17:44,240
more standardized format

525
00:17:40,960 --> 00:17:45,840
and the output of this data package

526
00:17:44,240 --> 00:17:48,799
pipeline process

527
00:17:45,840 --> 00:17:49,918
is this pipeline spec emo file the raw

528
00:17:48,799 --> 00:17:52,320
data

529
00:17:49,919 --> 00:17:53,600
and then the metadata and this is all

530
00:17:52,320 --> 00:17:56,480
captured together

531
00:17:53,600 --> 00:17:57,840
so you can repeat the experiment and you

532
00:17:56,480 --> 00:18:00,640
can repeat the pipeline

533
00:17:57,840 --> 00:18:04,320
and then you could say host this data

534
00:18:00,640 --> 00:18:07,760
and metadata on the bcodemo site

535
00:18:04,320 --> 00:18:10,080
so to sum up this collaboration so far

536
00:18:07,760 --> 00:18:11,039
we were aiming to take bicodimo's messy

537
00:18:10,080 --> 00:18:13,840
data

538
00:18:11,039 --> 00:18:16,240
and then run it through the pipeline and

539
00:18:13,840 --> 00:18:19,600
get out the pipeline spec yaml file

540
00:18:16,240 --> 00:18:20,400
the data package metadata and the raw

541
00:18:19,600 --> 00:18:22,080
data

542
00:18:20,400 --> 00:18:23,679
that can then be used by other

543
00:18:22,080 --> 00:18:27,120
researchers or other

544
00:18:23,679 --> 00:18:29,360
data managers further down the road

545
00:18:27,120 --> 00:18:31,520
okay we just ended phase one of this

546
00:18:29,360 --> 00:18:34,080
collaboration or going into phase two

547
00:18:31,520 --> 00:18:36,160
so our next steps here are the release

548
00:18:34,080 --> 00:18:38,000
of the open source community version of

549
00:18:36,160 --> 00:18:40,000
this pipeline it's not quite done so

550
00:18:38,000 --> 00:18:42,080
it's not available yet

551
00:18:40,000 --> 00:18:44,480
and then also this will allow the public

552
00:18:42,080 --> 00:18:45,678
to re-run these pipelines or build upon

553
00:18:44,480 --> 00:18:48,240
them

554
00:18:45,679 --> 00:18:50,480
and then also we're adding in validation

555
00:18:48,240 --> 00:18:52,240
with the good tables library so we're

556
00:18:50,480 --> 00:18:54,720
going to be able to check that the data

557
00:18:52,240 --> 00:18:57,120
remains valid throughout this process

558
00:18:54,720 --> 00:18:59,360
and i can't say enough good things about

559
00:18:57,120 --> 00:19:01,439
the work that beco demo is doing

560
00:18:59,360 --> 00:19:03,199
it's super interesting research so here

561
00:19:01,440 --> 00:19:05,200
are links where you can find out more

562
00:19:03,200 --> 00:19:06,960
about them and i encourage you to

563
00:19:05,200 --> 00:19:10,080
check it out if you're interested in

564
00:19:06,960 --> 00:19:11,840
oceanography in any way

565
00:19:10,080 --> 00:19:13,439
all right now i'm getting into the

566
00:19:11,840 --> 00:19:15,120
slides that i'm not going to talk about

567
00:19:13,440 --> 00:19:16,640
but are up here just so that you can

568
00:19:15,120 --> 00:19:19,120
play with them later

569
00:19:16,640 --> 00:19:21,120
and if you're interested here's a good

570
00:19:19,120 --> 00:19:24,080
place to start looking into frictionless

571
00:19:21,120 --> 00:19:27,199
data it's our field guide

572
00:19:24,080 --> 00:19:27,840
and then we have these links up here i

573
00:19:27,200 --> 00:19:29,679
have

574
00:19:27,840 --> 00:19:30,879
links to play with our browser tools and

575
00:19:29,679 --> 00:19:33,280
some toy data

576
00:19:30,880 --> 00:19:34,400
or you can run your own data and good

577
00:19:33,280 --> 00:19:37,200
tables this is one that

578
00:19:34,400 --> 00:19:38,240
will validate we also have continuous

579
00:19:37,200 --> 00:19:40,160
validation

580
00:19:38,240 --> 00:19:42,640
so this is good tables integrated into

581
00:19:40,160 --> 00:19:43,280
github so every time you push your data

582
00:19:42,640 --> 00:19:45,840
it will

583
00:19:43,280 --> 00:19:47,440
validate your data automatically and

584
00:19:45,840 --> 00:19:48,799
this is an example where it's showing

585
00:19:47,440 --> 00:19:50,240
you all of these errors

586
00:19:48,799 --> 00:19:52,480
that were found the last time someone

587
00:19:50,240 --> 00:19:54,400
pushed data

588
00:19:52,480 --> 00:19:56,799
all right so with that i'm going to end

589
00:19:54,400 --> 00:19:58,799
but again here's the link to the slides

590
00:19:56,799 --> 00:20:01,200
and i want to end by asking you to join

591
00:19:58,799 --> 00:20:02,559
our community and if you use these tools

592
00:20:01,200 --> 00:20:04,559
please let me know

593
00:20:02,559 --> 00:20:06,399
we're always looking for feedback and

594
00:20:04,559 --> 00:20:09,280
for new use cases

595
00:20:06,400 --> 00:20:10,000
here's our github repository a link to

596
00:20:09,280 --> 00:20:12,080
our disqus

597
00:20:10,000 --> 00:20:13,360
form where you can ask questions about

598
00:20:12,080 --> 00:20:16,480
open science but also

599
00:20:13,360 --> 00:20:18,479
open anything and then our gitter

600
00:20:16,480 --> 00:20:20,880
chat where if you have a question a

601
00:20:18,480 --> 00:20:22,640
technical question post it in getter and

602
00:20:20,880 --> 00:20:23,600
we will answer or our community will

603
00:20:22,640 --> 00:20:25,200
answer

604
00:20:23,600 --> 00:20:27,918
have a youtube channel with a lot more

605
00:20:25,200 --> 00:20:32,320
tutorials and then our frictionless data

606
00:20:27,919 --> 00:20:32,320
field guide and with that i will take

607
00:20:32,840 --> 00:20:35,840
questions

608
00:20:46,840 --> 00:20:49,840
um

609
00:20:58,080 --> 00:21:03,840
do you have an experience with forcing

610
00:21:04,480 --> 00:21:09,039
i don't did you say you work in did you

611
00:21:07,600 --> 00:21:12,320
say you work in microscopy

612
00:21:09,039 --> 00:21:13,280
yes okay the question was he works in

613
00:21:12,320 --> 00:21:16,399
microscopy

614
00:21:13,280 --> 00:21:18,158
and often the file software

615
00:21:16,400 --> 00:21:20,400
is proprietary and so it's very

616
00:21:18,159 --> 00:21:22,640
difficult to get the metadata

617
00:21:20,400 --> 00:21:24,159
is that correct he was asking if i have

618
00:21:22,640 --> 00:21:27,679
experience trying to force

619
00:21:24,159 --> 00:21:32,320
companies to give up metadata i do not

620
00:21:27,679 --> 00:21:45,200
that's a great question yeah

621
00:21:32,320 --> 00:21:47,280
sorry sorry

622
00:21:45,200 --> 00:21:49,360
yeah so the question was do i care about

623
00:21:47,280 --> 00:21:52,639
the metadata of the metadata

624
00:21:49,360 --> 00:21:56,000
for example ontologies um

625
00:21:52,640 --> 00:21:58,720
we are purposefully general

626
00:21:56,000 --> 00:22:00,080
so that's a tiny bit too specific like i

627
00:21:58,720 --> 00:22:01,360
think the last few years saying they

628
00:22:00,080 --> 00:22:04,480
don't care what

629
00:22:01,360 --> 00:22:06,000
kind how you document your metadata we

630
00:22:04,480 --> 00:22:09,840
also kind of feel that way

631
00:22:06,000 --> 00:22:13,200
um i personally care about ontologies

632
00:22:09,840 --> 00:22:18,158
but we do not have any like

633
00:22:13,200 --> 00:22:18,159
standards that we really hold on to

634
00:22:18,400 --> 00:22:23,440
yeah um as a follow up on this question

635
00:22:21,679 --> 00:22:25,919
is like

636
00:22:23,440 --> 00:22:27,679
one one way to answer this in my opinion

637
00:22:25,919 --> 00:22:30,240
because i've used a little bit of this

638
00:22:27,679 --> 00:22:31,520
technology is that the distinctions

639
00:22:30,240 --> 00:22:34,799
between metadata

640
00:22:31,520 --> 00:22:36,320
and data is a blur one where for

641
00:22:34,799 --> 00:22:37,520
instance what you call the metadata on

642
00:22:36,320 --> 00:22:40,960
this metadata

643
00:22:37,520 --> 00:22:41,840
ontology can be added into your data

644
00:22:40,960 --> 00:22:45,120
basically

645
00:22:41,840 --> 00:22:48,720
as a resource that you can document

646
00:22:45,120 --> 00:22:50,959
in your metadata data package in a way

647
00:22:48,720 --> 00:22:51,760
so my question then following on this id

648
00:22:50,960 --> 00:22:54,640
is

649
00:22:51,760 --> 00:22:58,080
in the pipeline context you've showed a

650
00:22:54,640 --> 00:23:02,480
example of cleaning data

651
00:22:58,080 --> 00:23:05,520
so i have two questions about that first

652
00:23:02,480 --> 00:23:07,440
how do you in your experience

653
00:23:05,520 --> 00:23:09,039
put the correction into the mate into

654
00:23:07,440 --> 00:23:12,080
the pipeline are

655
00:23:09,039 --> 00:23:12,799
correcting the raw data when do you

656
00:23:12,080 --> 00:23:15,199
choose

657
00:23:12,799 --> 00:23:16,240
to do one on the other and the second

658
00:23:15,200 --> 00:23:18,080
one is

659
00:23:16,240 --> 00:23:20,080
do they have you experience using

660
00:23:18,080 --> 00:23:23,199
pipeline not to correct

661
00:23:20,080 --> 00:23:25,520
but to aggregate to create new secondary

662
00:23:23,200 --> 00:23:26,480
data from the roadway yeah great

663
00:23:25,520 --> 00:23:28,960
questions okay

664
00:23:26,480 --> 00:23:30,240
i'm gonna try and repeat um first of all

665
00:23:28,960 --> 00:23:33,440
a good comment

666
00:23:30,240 --> 00:23:34,640
that you can document things like if

667
00:23:33,440 --> 00:23:37,200
you're using a specif

668
00:23:34,640 --> 00:23:38,640
specific ontology in the metadata and

669
00:23:37,200 --> 00:23:43,840
then first question

670
00:23:38,640 --> 00:23:43,840
was so now i forgot

671
00:23:45,120 --> 00:23:50,719
itself sometimes oh yeah absolutely when

672
00:23:48,480 --> 00:23:52,559
when do you decide to put metadata into

673
00:23:50,720 --> 00:23:54,240
the raw data

674
00:23:52,559 --> 00:23:56,000
i think that's part of the data

675
00:23:54,240 --> 00:24:00,320
management and

676
00:23:56,000 --> 00:24:02,880
teaching researchers best practices

677
00:24:00,320 --> 00:24:04,559
and i think it's also personal it

678
00:24:02,880 --> 00:24:06,080
depends on the lab it depends on the

679
00:24:04,559 --> 00:24:08,320
experiment

680
00:24:06,080 --> 00:24:09,600
and i also don't think it matters a

681
00:24:08,320 --> 00:24:11,360
whole lot i think it's just important

682
00:24:09,600 --> 00:24:13,279
that it's documented at all

683
00:24:11,360 --> 00:24:14,559
and then the second question was have we

684
00:24:13,279 --> 00:24:18,720
used the pipeline

685
00:24:14,559 --> 00:24:22,000
to integrate data is that it um

686
00:24:18,720 --> 00:24:24,400
i have not used it and i don't know of a

687
00:24:22,000 --> 00:24:26,000
specific use case but it's possible

688
00:24:24,400 --> 00:24:29,039
and it could be that someone's done it

689
00:24:26,000 --> 00:24:29,039
and i just don't know about it

690
00:24:29,279 --> 00:24:32,799
you mentioned going from a tabular

691
00:24:30,880 --> 00:24:36,400
spreadsheet to a machine emittable json

692
00:24:32,799 --> 00:24:39,840
is that based on a standard it is

693
00:24:36,400 --> 00:24:41,600
and we have standards on

694
00:24:39,840 --> 00:24:43,760
i'm going to redirect you to our website

695
00:24:41,600 --> 00:24:46,080
to look at all of our standards

696
00:24:43,760 --> 00:24:47,360
we call them specifications because

697
00:24:46,080 --> 00:24:50,000
they're

698
00:24:47,360 --> 00:24:52,240
supposed to be more flexible than like a

699
00:24:50,000 --> 00:24:53,440
real standard but yeah

700
00:24:52,240 --> 00:24:58,159
i'm going to look over here and see i

701
00:24:53,440 --> 00:24:58,159
haven't okay yes

702
00:24:58,480 --> 00:25:00,640
so

703
00:25:05,039 --> 00:25:10,480
that's a great question um we were

704
00:25:08,159 --> 00:25:11,520
i think open refine had integrated data

705
00:25:10,480 --> 00:25:13,600
packages

706
00:25:11,520 --> 00:25:15,039
but then one of our software libraries

707
00:25:13,600 --> 00:25:18,559
they were using had

708
00:25:15,039 --> 00:25:20,799
a license that is

709
00:25:18,559 --> 00:25:22,240
not recognized by the osi because it has

710
00:25:20,799 --> 00:25:24,480
the stan the

711
00:25:22,240 --> 00:25:26,240
statement that it must be used for good

712
00:25:24,480 --> 00:25:29,840
so then open refine had to drop it

713
00:25:26,240 --> 00:25:32,240
so we're working right now to try and

714
00:25:29,840 --> 00:25:33,600
rewrite that library with an osi

715
00:25:32,240 --> 00:25:36,559
compliant

716
00:25:33,600 --> 00:25:39,600
license so that hopefully we can get

717
00:25:36,559 --> 00:25:39,600
that functionality back

718
00:25:41,120 --> 00:25:47,520
to know what yes fellow does

719
00:25:44,240 --> 00:25:51,760
very briefly because what they find in

720
00:25:47,520 --> 00:25:53,679
um with my colleagues is that sometimes

721
00:25:51,760 --> 00:25:55,360
i mean they don't know or they don't

722
00:25:53,679 --> 00:25:57,200
know enough or

723
00:25:55,360 --> 00:25:58,879
so they are not comfortable in using

724
00:25:57,200 --> 00:26:01,919
even very basic things

725
00:25:58,880 --> 00:26:02,880
like i don't know beautiful environments

726
00:26:01,919 --> 00:26:05,520
in python

727
00:26:02,880 --> 00:26:06,080
yeah so when they try to say okay i put

728
00:26:05,520 --> 00:26:08,480
my code

729
00:26:06,080 --> 00:26:10,320
in a new machine and i have to restore

730
00:26:08,480 --> 00:26:12,400
all the dependencies

731
00:26:10,320 --> 00:26:14,240
because they don't remember what they

732
00:26:12,400 --> 00:26:15,360
needed so they just run the code until

733
00:26:14,240 --> 00:26:18,559
they find some error and

734
00:26:15,360 --> 00:26:21,039
oh i need this and so on so i'm

735
00:26:18,559 --> 00:26:22,080
i'm finding myself that i explain these

736
00:26:21,039 --> 00:26:25,120
very basic

737
00:26:22,080 --> 00:26:27,120
things yeah many people and i i

738
00:26:25,120 --> 00:26:28,799
wanted to know if somebody has already

739
00:26:27,120 --> 00:26:32,639
done it and if i can find

740
00:26:28,799 --> 00:26:34,960
some slides yeah we the fellows are

741
00:26:32,640 --> 00:26:35,679
below that level programmatically

742
00:26:34,960 --> 00:26:37,840
usually they

743
00:26:35,679 --> 00:26:39,919
know like some are some python but we

744
00:26:37,840 --> 00:26:42,320
aren't teaching them about

745
00:26:39,919 --> 00:26:44,240
oh sorry i didn't repeat the question um

746
00:26:42,320 --> 00:26:47,439
we aren't teaching them about specific

747
00:26:44,240 --> 00:26:48,559
pro programmatic environments that was

748
00:26:47,440 --> 00:26:52,240
kind of the question

749
00:26:48,559 --> 00:26:55,678
uh yes but resources like that exist

750
00:26:52,240 --> 00:26:57,120
i i wonder if emmy knows

751
00:26:55,679 --> 00:26:59,120
and the answer to that she's talking

752
00:26:57,120 --> 00:27:01,918
later you can ask her

753
00:26:59,120 --> 00:27:02,479
we have to stop sorry out of time thank

754
00:27:01,919 --> 00:27:07,840
you

755
00:27:02,480 --> 00:27:07,840
thank you

756
00:27:11,600 --> 00:27:13,678
you

