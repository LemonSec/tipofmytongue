1
00:00:05,040 --> 00:00:10,400
and uh the last talk is going to be

2
00:00:07,440 --> 00:00:17,920
about how to build a smart reverse proxy

3
00:00:10,400 --> 00:00:17,920
and go so run applause

4
00:00:19,359 --> 00:00:23,840
okay thank you so we're going to build

5
00:00:22,400 --> 00:00:26,640
the smart reverse proxy

6
00:00:23,840 --> 00:00:27,039
and go there's a couple of words about

7
00:00:26,640 --> 00:00:30,160
me

8
00:00:27,039 --> 00:00:31,679
my name i am a backend is

9
00:00:30,160 --> 00:00:34,160
in the infrastructure department at

10
00:00:31,679 --> 00:00:35,839
gitlab and we are an old remote company

11
00:00:34,160 --> 00:00:38,559
so i am one of those faces

12
00:00:35,840 --> 00:00:39,440
back there so we gather once a year this

13
00:00:38,559 --> 00:00:44,399
is the last

14
00:00:39,440 --> 00:00:47,920
game we made in new orleans city so

15
00:00:44,399 --> 00:00:51,039
i'm going to tell you about us sorry

16
00:00:47,920 --> 00:00:53,120
so imagine the infrastructure department

17
00:00:51,039 --> 00:00:54,239
announcing that we are going to migrate

18
00:00:53,120 --> 00:00:56,239
our production

19
00:00:54,239 --> 00:00:57,279
from azure cloud to google cloud

20
00:00:56,239 --> 00:01:00,078
platform say

21
00:00:57,280 --> 00:01:01,280
wow this is really cool more or less at

22
00:01:00,079 --> 00:01:03,600
the same time

23
00:01:01,280 --> 00:01:05,280
with more or less the same deadline the

24
00:01:03,600 --> 00:01:06,960
distribution team announced

25
00:01:05,280 --> 00:01:08,560
that we are going to release cloud

26
00:01:06,960 --> 00:01:11,679
native charts installation

27
00:01:08,560 --> 00:01:13,760
for gitlab also this is really cool

28
00:01:11,680 --> 00:01:15,759
then you start thinking wow we could we

29
00:01:13,760 --> 00:01:18,880
ship features we keep

30
00:01:15,759 --> 00:01:20,159
delivering git lab while migrating all

31
00:01:18,880 --> 00:01:22,560
these things

32
00:01:20,159 --> 00:01:24,240
and then you start thinking about all

33
00:01:22,560 --> 00:01:26,080
the little technical depths that you

34
00:01:24,240 --> 00:01:27,520
have seen all the dirty tricks in the

35
00:01:26,080 --> 00:01:29,920
code basis

36
00:01:27,520 --> 00:01:30,640
i'm not really sure that this journey

37
00:01:29,920 --> 00:01:33,680
would be

38
00:01:30,640 --> 00:01:34,799
so fantastic but before we dig into the

39
00:01:33,680 --> 00:01:39,360
story

40
00:01:34,799 --> 00:01:42,640
i need to go back in time to mid 2015.

41
00:01:39,360 --> 00:01:45,920
so we are a reuben rails company why i'm

42
00:01:42,640 --> 00:01:49,520
talking here at golden belt conference

43
00:01:45,920 --> 00:01:53,600
so we had a problem we had a big problem

44
00:01:49,520 --> 00:01:54,240
with slow riggs nobody likes low records

45
00:01:53,600 --> 00:01:57,039
but

46
00:01:54,240 --> 00:01:58,560
our problem was for not really a

47
00:01:57,040 --> 00:02:01,680
performance

48
00:01:58,560 --> 00:02:04,320
of some requests but

49
00:02:01,680 --> 00:02:05,119
by design you are supposed to move data

50
00:02:04,320 --> 00:02:07,679
think about

51
00:02:05,119 --> 00:02:09,758
git operation if you want to clone the

52
00:02:07,680 --> 00:02:13,200
kernel ripple over https

53
00:02:09,758 --> 00:02:14,799
it takes time no matter how optimization

54
00:02:13,200 --> 00:02:16,319
you can put there there's a bandwidth

55
00:02:14,800 --> 00:02:19,360
and there's data that you have to move

56
00:02:16,319 --> 00:02:21,599
so it takes time back in those days the

57
00:02:19,360 --> 00:02:24,160
only solution we had for this was

58
00:02:21,599 --> 00:02:25,359
yeah you can clone over https but it

59
00:02:24,160 --> 00:02:30,319
better if you do it over

60
00:02:25,360 --> 00:02:32,560
ssh so one of the reason

61
00:02:30,319 --> 00:02:34,640
was because of this problem was that we

62
00:02:32,560 --> 00:02:37,519
have a technology stack

63
00:02:34,640 --> 00:02:38,079
that was based basically on a working

64
00:02:37,519 --> 00:02:40,480
daemon

65
00:02:38,080 --> 00:02:42,400
which was designed only for serving

66
00:02:40,480 --> 00:02:44,000
first client on low latency high

67
00:02:42,400 --> 00:02:47,200
bandwidth connection

68
00:02:44,000 --> 00:02:48,000
so this is a working daemon so you can

69
00:02:47,200 --> 00:02:49,839
imagine that

70
00:02:48,000 --> 00:02:53,040
you have a master process that loads

71
00:02:49,840 --> 00:02:55,760
your code then it works you create some

72
00:02:53,040 --> 00:02:56,959
workers and the master process handle

73
00:02:55,760 --> 00:02:59,280
incoming connection

74
00:02:56,959 --> 00:03:01,360
forwarding them to one of those process

75
00:02:59,280 --> 00:03:03,920
and if the process is waiting

76
00:03:01,360 --> 00:03:05,840
doing io you cannot serve any other

77
00:03:03,920 --> 00:03:08,879
request because it's it's not a

78
00:03:05,840 --> 00:03:10,319
multi-threaded application so you can

79
00:03:08,879 --> 00:03:12,319
imagine that if you are cloning

80
00:03:10,319 --> 00:03:14,480
something in this situation

81
00:03:12,319 --> 00:03:16,399
you just you're losing capacity over

82
00:03:14,480 --> 00:03:20,238
data

83
00:03:16,400 --> 00:03:23,360
when you transmit data so the basic idea

84
00:03:20,239 --> 00:03:23,760
is this one we had an hra proxy in front

85
00:03:23,360 --> 00:03:26,720
of

86
00:03:23,760 --> 00:03:28,480
git lab i removed database or the

87
00:03:26,720 --> 00:03:29,040
external dependency one just want you to

88
00:03:28,480 --> 00:03:32,159
focus

89
00:03:29,040 --> 00:03:32,959
on this so you have a web server which

90
00:03:32,159 --> 00:03:35,359
handles

91
00:03:32,959 --> 00:03:37,280
requests and apis and aha proxy in front

92
00:03:35,360 --> 00:03:39,840
of it

93
00:03:37,280 --> 00:03:41,280
so let's enter workers a smart reverse

94
00:03:39,840 --> 00:03:43,040
proxy

95
00:03:41,280 --> 00:03:45,440
so there are a lot of reverse proxy out

96
00:03:43,040 --> 00:03:48,159
there why we had to write a smart one

97
00:03:45,440 --> 00:03:51,359
and what does it mean

98
00:03:48,159 --> 00:03:53,439
the idea is that is this smart

99
00:03:51,360 --> 00:03:55,519
because it's not a general purpose

100
00:03:53,439 --> 00:03:56,319
reverse proxy but it really knows your

101
00:03:55,519 --> 00:03:59,760
workload

102
00:03:56,319 --> 00:04:01,920
and can help you where it's needed

103
00:03:59,760 --> 00:04:03,280
it was named workers to make fun of the

104
00:04:01,920 --> 00:04:05,679
magical unicorn

105
00:04:03,280 --> 00:04:07,360
and the idea was that you can have the

106
00:04:05,680 --> 00:04:09,519
magical animal but if you need to do the

107
00:04:07,360 --> 00:04:12,560
heavy lifting you need workers

108
00:04:09,519 --> 00:04:15,360
so let's start with a simple example how

109
00:04:12,560 --> 00:04:16,959
hard will be writing a reverse proxy in

110
00:04:15,360 --> 00:04:19,440
go

111
00:04:16,959 --> 00:04:22,960
this is a reverse proxy in go three line

112
00:04:19,440 --> 00:04:26,320
of code error checking and import

113
00:04:22,960 --> 00:04:29,198
so let's take a look at this

114
00:04:26,320 --> 00:04:30,639
first you need an url for your upstream

115
00:04:29,199 --> 00:04:33,440
server

116
00:04:30,639 --> 00:04:33,919
and yeah this is the thing that you need

117
00:04:33,440 --> 00:04:37,520
then

118
00:04:33,919 --> 00:04:39,758
you need um proxy from the http utils

119
00:04:37,520 --> 00:04:40,880
new single host reverse proxy you pass

120
00:04:39,759 --> 00:04:43,600
the url in

121
00:04:40,880 --> 00:04:46,000
and then listen and surf done you have a

122
00:04:43,600 --> 00:04:49,440
reverse proxy

123
00:04:46,000 --> 00:04:53,360
now we have reverse proxy

124
00:04:49,440 --> 00:04:56,719
how can we speed up as low request

125
00:04:53,360 --> 00:05:00,320
let's imagine that we have a slow

126
00:04:56,720 --> 00:05:04,160
end point which is on slash slow

127
00:05:00,320 --> 00:05:07,360
so this is the amount of code

128
00:05:04,160 --> 00:05:08,080
removing import that you need to rewrite

129
00:05:07,360 --> 00:05:11,360
the thing

130
00:05:08,080 --> 00:05:14,000
so let's go through the code

131
00:05:11,360 --> 00:05:15,919
i cheated a bit because in order to fit

132
00:05:14,000 --> 00:05:17,280
everything into one slide i had imported

133
00:05:15,919 --> 00:05:21,039
a package

134
00:05:17,280 --> 00:05:23,520
so this is gorilla max router it's

135
00:05:21,039 --> 00:05:25,199
you can do these things directly with

136
00:05:23,520 --> 00:05:27,280
the standard library but the idea here

137
00:05:25,199 --> 00:05:30,400
is that i want to

138
00:05:27,280 --> 00:05:31,359
easily declare a handler that handle a

139
00:05:30,400 --> 00:05:32,719
specific route

140
00:05:31,360 --> 00:05:35,280
so that's the reason why we have a

141
00:05:32,720 --> 00:05:38,240
router here so first thing

142
00:05:35,280 --> 00:05:39,440
you need a router then you need a

143
00:05:38,240 --> 00:05:41,280
middleware

144
00:05:39,440 --> 00:05:42,479
yeah because something that we figured

145
00:05:41,280 --> 00:05:45,359
out

146
00:05:42,479 --> 00:05:45,919
on our logging system is that if you put

147
00:05:45,360 --> 00:05:48,160
a

148
00:05:45,919 --> 00:05:50,320
reverse proxy in between all your log

149
00:05:48,160 --> 00:05:53,120
system will be filled with localhost

150
00:05:50,320 --> 00:05:54,880
incoming connection so you need to take

151
00:05:53,120 --> 00:05:57,280
care of the

152
00:05:54,880 --> 00:05:58,960
address and all the information about

153
00:05:57,280 --> 00:06:02,960
the external client so it's

154
00:05:58,960 --> 00:06:05,919
just three headers and you're done

155
00:06:02,960 --> 00:06:06,719
and then what you need is a handle

156
00:06:05,919 --> 00:06:10,080
function

157
00:06:06,720 --> 00:06:13,199
that can rewrite your

158
00:06:10,080 --> 00:06:15,680
slow end point in go

159
00:06:13,199 --> 00:06:16,880
so the basic idea here is that you don't

160
00:06:15,680 --> 00:06:20,240
rewrite

161
00:06:16,880 --> 00:06:22,880
your code base but you just pinpoint

162
00:06:20,240 --> 00:06:26,080
the pain that you have and you rewrite

163
00:06:22,880 --> 00:06:28,159
them in a more performant way

164
00:06:26,080 --> 00:06:29,280
then basically we go back to our old

165
00:06:28,160 --> 00:06:31,919
code we post

166
00:06:29,280 --> 00:06:34,159
the before the upstream url we create a

167
00:06:31,919 --> 00:06:37,198
new single host reverse proxy

168
00:06:34,160 --> 00:06:39,360
and we bind it to the

169
00:06:37,199 --> 00:06:41,120
to the router so that everything that

170
00:06:39,360 --> 00:06:44,000
doesn't match a specific route

171
00:06:41,120 --> 00:06:46,000
will go through the reverse proxy in our

172
00:06:44,000 --> 00:06:49,840
upstream

173
00:06:46,000 --> 00:06:49,840
so this is what we did

174
00:06:50,000 --> 00:06:56,720
it was 22nd of september 2015

175
00:06:53,599 --> 00:07:00,080
we released this idea

176
00:06:56,720 --> 00:07:03,599
where aj proxy was connected to

177
00:07:00,080 --> 00:07:05,440
workers and in case of a git operation

178
00:07:03,599 --> 00:07:07,360
so cloning and pooling

179
00:07:05,440 --> 00:07:09,039
we were doing authorization and

180
00:07:07,360 --> 00:07:10,720
authentication in the old ways we were

181
00:07:09,039 --> 00:07:13,840
forwarding the information

182
00:07:10,720 --> 00:07:17,199
to unicorn and the old raids code base

183
00:07:13,840 --> 00:07:19,039
but instead of handling the clone

184
00:07:17,199 --> 00:07:22,560
operation in rails

185
00:07:19,039 --> 00:07:23,599
we were just uh working um with the gold

186
00:07:22,560 --> 00:07:26,479
thing we're forking

187
00:07:23,599 --> 00:07:28,800
the git binary and forwarding all the

188
00:07:26,479 --> 00:07:30,639
the body of the requester so basically

189
00:07:28,800 --> 00:07:32,080
kind of a cgi you can imagine this is

190
00:07:30,639 --> 00:07:35,599
like a cgi but

191
00:07:32,080 --> 00:07:38,639
done in the reverse proxy instead of

192
00:07:35,599 --> 00:07:39,039
in the raids application over the time

193
00:07:38,639 --> 00:07:42,400
this

194
00:07:39,039 --> 00:07:43,360
evolved a bit so today we have a new

195
00:07:42,400 --> 00:07:46,638
component which is

196
00:07:43,360 --> 00:07:50,080
italy which is written in go is a grpc

197
00:07:46,639 --> 00:07:52,080
server and it handles the all the

198
00:07:50,080 --> 00:07:53,440
git code so if you want to interact with

199
00:07:52,080 --> 00:07:56,560
the repository

200
00:07:53,440 --> 00:07:58,400
you can do the glpc call and it's an

201
00:07:56,560 --> 00:08:00,479
external component

202
00:07:58,400 --> 00:08:01,679
so we were able to speed up git

203
00:08:00,479 --> 00:08:04,800
operation

204
00:08:01,680 --> 00:08:06,080
bye bye slow request a couple of months

205
00:08:04,800 --> 00:08:09,759
later

206
00:08:06,080 --> 00:08:13,120
we released the ci system of gitlab

207
00:08:09,759 --> 00:08:15,680
and we had we had another problem

208
00:08:13,120 --> 00:08:17,919
we had a big offender in the context of

209
00:08:15,680 --> 00:08:20,000
slow requests which was the ci runner

210
00:08:17,919 --> 00:08:23,198
attempting to upload artifacts

211
00:08:20,000 --> 00:08:24,879
so you can easily imagine that you can i

212
00:08:23,199 --> 00:08:25,680
think we had a limit of one gigabyte i'm

213
00:08:24,879 --> 00:08:28,960
not sure

214
00:08:25,680 --> 00:08:30,960
so we had this fleet of process

215
00:08:28,960 --> 00:08:32,718
that we're uploading artifacts

216
00:08:30,960 --> 00:08:35,519
constantly and

217
00:08:32,719 --> 00:08:36,240
i want to give you some number here i

218
00:08:35,519 --> 00:08:38,880
took the

219
00:08:36,240 --> 00:08:40,000
memory footprint of our production

220
00:08:38,880 --> 00:08:43,200
installation

221
00:08:40,000 --> 00:08:46,480
of gitlab and the unicorn process

222
00:08:43,200 --> 00:08:49,519
takes around 800 megabytes

223
00:08:46,480 --> 00:08:51,279
of ram workers

224
00:08:49,519 --> 00:08:52,560
70 megabyte so there's an order of

225
00:08:51,279 --> 00:08:53,839
magnitude in there

226
00:08:52,560 --> 00:08:55,839
so you can imagine where you want to

227
00:08:53,839 --> 00:08:57,040
spend your time in your machine if you

228
00:08:55,839 --> 00:09:00,080
are

229
00:08:57,040 --> 00:09:02,000
if you're under heavy load so we came up

230
00:09:00,080 --> 00:09:05,200
with this idea of body hijacking

231
00:09:02,000 --> 00:09:08,080
which is more or less described here

232
00:09:05,200 --> 00:09:08,480
so the idea is that you have an external

233
00:09:08,080 --> 00:09:11,600
client

234
00:09:08,480 --> 00:09:14,560
in our case is the ci runner

235
00:09:11,600 --> 00:09:15,120
and this client needs to upload some

236
00:09:14,560 --> 00:09:20,239
file

237
00:09:15,120 --> 00:09:22,240
okay so when the request goes to workers

238
00:09:20,240 --> 00:09:23,920
instead of forwarding it directly to

239
00:09:22,240 --> 00:09:26,640
rails

240
00:09:23,920 --> 00:09:27,360
which in that case will dump the file on

241
00:09:26,640 --> 00:09:29,519
disk

242
00:09:27,360 --> 00:09:31,120
and replace the file with a file handler

243
00:09:29,519 --> 00:09:31,839
in the hash of parameters of your

244
00:09:31,120 --> 00:09:35,920
request

245
00:09:31,839 --> 00:09:37,680
we will act before so we will parse the

246
00:09:35,920 --> 00:09:40,079
incoming records in workers

247
00:09:37,680 --> 00:09:41,120
and we save the incoming file to the

248
00:09:40,080 --> 00:09:42,640
disk

249
00:09:41,120 --> 00:09:44,959
because this is what will happen later

250
00:09:42,640 --> 00:09:46,880
in the process but we can do this in a

251
00:09:44,959 --> 00:09:48,640
performant way and multithreaded we'd go

252
00:09:46,880 --> 00:09:51,680
to things and everything

253
00:09:48,640 --> 00:09:53,600
then we strip out the the body from the

254
00:09:51,680 --> 00:09:54,079
incoming request and we replace it with

255
00:09:53,600 --> 00:09:57,360
some

256
00:09:54,080 --> 00:10:00,640
metadata that tells the upstream server

257
00:09:57,360 --> 00:10:01,760
where we put those files so we forward

258
00:10:00,640 --> 00:10:04,240
them to rails

259
00:10:01,760 --> 00:10:05,600
and we had a middleware in rails that

260
00:10:04,240 --> 00:10:07,120
was reading

261
00:10:05,600 --> 00:10:09,839
the new headers the new information with

262
00:10:07,120 --> 00:10:12,000
metadata and basically replacing

263
00:10:09,839 --> 00:10:13,120
the file again in the hf parameters so

264
00:10:12,000 --> 00:10:15,440
that as an engineer

265
00:10:13,120 --> 00:10:16,160
when you're just writing your controller

266
00:10:15,440 --> 00:10:18,320
code

267
00:10:16,160 --> 00:10:20,000
it's exactly the same as if the request

268
00:10:18,320 --> 00:10:21,279
was coming directly through the raids

269
00:10:20,000 --> 00:10:22,959
application or

270
00:10:21,279 --> 00:10:24,720
through workers because you still have a

271
00:10:22,959 --> 00:10:26,880
file handler there so it's completely

272
00:10:24,720 --> 00:10:30,000
transparent

273
00:10:26,880 --> 00:10:31,439
so this is what we did it was more or

274
00:10:30,000 --> 00:10:34,399
less two months later

275
00:10:31,440 --> 00:10:34,720
the other implementation and so we speed

276
00:10:34,399 --> 00:10:37,839
up

277
00:10:34,720 --> 00:10:41,120
all the uploads bye bye requests

278
00:10:37,839 --> 00:10:46,079
and it's time to go back to our story

279
00:10:41,120 --> 00:10:48,959
so we had to release cloud native charts

280
00:10:46,079 --> 00:10:49,519
we had a big problem now network file

281
00:10:48,959 --> 00:10:52,959
system

282
00:10:49,519 --> 00:10:56,320
nfs now let me explain you

283
00:10:52,959 --> 00:10:57,920
why this was a problem i collapsed

284
00:10:56,320 --> 00:10:59,360
everything back into the gitlab box

285
00:10:57,920 --> 00:11:01,519
because i'm going to add new

286
00:10:59,360 --> 00:11:02,399
stuff here so i don't want to confuse

287
00:11:01,519 --> 00:11:04,560
you with a lot of

288
00:11:02,399 --> 00:11:06,720
information so this is the same thing aj

289
00:11:04,560 --> 00:11:11,359
proxy and there you have workers and

290
00:11:06,720 --> 00:11:11,360
the rates application and everything so

291
00:11:11,600 --> 00:11:15,839
we had to do a synchronous operation so

292
00:11:14,320 --> 00:11:18,000
sidekick is a

293
00:11:15,839 --> 00:11:20,399
cue processor for ruby and rates

294
00:11:18,000 --> 00:11:23,760
application and we use radius as a hue

295
00:11:20,399 --> 00:11:25,760
so for instance we have support for

296
00:11:23,760 --> 00:11:27,760
object storage if something needs to be

297
00:11:25,760 --> 00:11:29,920
uploaded in object storage

298
00:11:27,760 --> 00:11:31,760
it you get it gets on a temporary

299
00:11:29,920 --> 00:11:34,719
location then you

300
00:11:31,760 --> 00:11:35,120
write the job on redis and sidekick fix

301
00:11:34,720 --> 00:11:39,040
it

302
00:11:35,120 --> 00:11:40,640
and move it to the object storage now

303
00:11:39,040 --> 00:11:42,160
if you think about this this works

304
00:11:40,640 --> 00:11:43,120
really well if you are on a single

305
00:11:42,160 --> 00:11:44,480
machine

306
00:11:43,120 --> 00:11:46,959
but as soon as you have an aj

307
00:11:44,480 --> 00:11:48,800
installation or if it's

308
00:11:46,959 --> 00:11:50,638
uh kubernetes installation where you

309
00:11:48,800 --> 00:11:52,240
have pods and so each one of these

310
00:11:50,639 --> 00:11:54,399
blocks is uh

311
00:11:52,240 --> 00:11:57,600
boundaries you have a big problem

312
00:11:54,399 --> 00:12:00,320
because you can't do this

313
00:11:57,600 --> 00:12:00,959
basically we were mounting the same nfs

314
00:12:00,320 --> 00:12:04,240
share

315
00:12:00,959 --> 00:12:06,800
on across all of our fleet

316
00:12:04,240 --> 00:12:07,680
so that regardless of the workers that

317
00:12:06,800 --> 00:12:10,560
was

318
00:12:07,680 --> 00:12:11,199
processing the incoming connection every

319
00:12:10,560 --> 00:12:13,920
machine

320
00:12:11,200 --> 00:12:14,320
in the sidekick fleet was able to read

321
00:12:13,920 --> 00:12:18,639
it

322
00:12:14,320 --> 00:12:20,240
and move it to the final destination so

323
00:12:18,639 --> 00:12:22,720
i want to give you some numbers also

324
00:12:20,240 --> 00:12:23,440
here because i was surprised when they

325
00:12:22,720 --> 00:12:26,079
told me

326
00:12:23,440 --> 00:12:26,880
so nfs is something that almost everyone

327
00:12:26,079 --> 00:12:29,680
knows about

328
00:12:26,880 --> 00:12:30,480
but very few knows about the

329
00:12:29,680 --> 00:12:32,399
requirements

330
00:12:30,480 --> 00:12:34,800
for running this thing in production

331
00:12:32,399 --> 00:12:37,200
with a very

332
00:12:34,800 --> 00:12:38,319
big storage and in an intensive

333
00:12:37,200 --> 00:12:40,240
operation

334
00:12:38,320 --> 00:12:42,399
so you can imagine that everything is

335
00:12:40,240 --> 00:12:44,800
constrained by the speed of the disk

336
00:12:42,399 --> 00:12:46,639
and the bandwidth that you have on the

337
00:12:44,800 --> 00:12:48,959
on the network

338
00:12:46,639 --> 00:12:50,959
so you want to have a lot of memory

339
00:12:48,959 --> 00:12:53,760
because the last thing you want is that

340
00:12:50,959 --> 00:12:56,880
swapping you don't want to

341
00:12:53,760 --> 00:12:57,279
content the memory swapping on disk with

342
00:12:56,880 --> 00:12:59,200
the

343
00:12:57,279 --> 00:13:01,920
i o on disk for writing your reading

344
00:12:59,200 --> 00:13:02,720
information so in our production we had

345
00:13:01,920 --> 00:13:05,279
an eight core

346
00:13:02,720 --> 00:13:07,279
machines with 50 gigabytes of ram just

347
00:13:05,279 --> 00:13:10,959
for running that box there

348
00:13:07,279 --> 00:13:12,880
it's expensive and it's a single point

349
00:13:10,959 --> 00:13:15,839
of failure

350
00:13:12,880 --> 00:13:16,560
and if you have to ship the cloud native

351
00:13:15,839 --> 00:13:19,600
installation

352
00:13:16,560 --> 00:13:20,160
on kubernetes you can't use this because

353
00:13:19,600 --> 00:13:23,839
kubernetes

354
00:13:20,160 --> 00:13:24,560
can handle nfs but it's not cloud native

355
00:13:23,839 --> 00:13:28,079
because

356
00:13:24,560 --> 00:13:31,279
it expects you to have an nfs outside

357
00:13:28,079 --> 00:13:32,800
of the cluster so we had to figure out a

358
00:13:31,279 --> 00:13:36,320
way for

359
00:13:32,800 --> 00:13:39,839
removing nfs from this graph

360
00:13:36,320 --> 00:13:40,720
so we came up with this idea maybe we

361
00:13:39,839 --> 00:13:43,120
should

362
00:13:40,720 --> 00:13:44,959
implement object storage directly in

363
00:13:43,120 --> 00:13:48,160
workers

364
00:13:44,959 --> 00:13:48,719
there's a side story here at this point

365
00:13:48,160 --> 00:13:50,719
in time

366
00:13:48,720 --> 00:13:52,000
object service was an enterprise feature

367
00:13:50,720 --> 00:13:55,279
so you need a license

368
00:13:52,000 --> 00:13:56,880
for this we decided that okay we we want

369
00:13:55,279 --> 00:13:58,560
to ship the open source version on

370
00:13:56,880 --> 00:14:01,120
kubernetes as well so this has to be

371
00:13:58,560 --> 00:14:03,439
backported in open source first

372
00:14:01,120 --> 00:14:05,760
so think about the timeline we were

373
00:14:03,440 --> 00:14:06,480
moving from another cloud provider we

374
00:14:05,760 --> 00:14:10,560
had to ship

375
00:14:06,480 --> 00:14:12,160
the the kubernetes native installation

376
00:14:10,560 --> 00:14:14,399
and we started realizing that we also

377
00:14:12,160 --> 00:14:17,040
had to backboard features make sure that

378
00:14:14,399 --> 00:14:20,000
that was working and built all these

379
00:14:17,040 --> 00:14:23,120
things together so

380
00:14:20,000 --> 00:14:26,160
first thing we we started with our own

381
00:14:23,120 --> 00:14:26,639
use case so we targeted only google

382
00:14:26,160 --> 00:14:29,439
cloud

383
00:14:26,639 --> 00:14:30,000
storage because we were moving there and

384
00:14:29,440 --> 00:14:33,040
we started

385
00:14:30,000 --> 00:14:37,680
with git lfs which was a very easy

386
00:14:33,040 --> 00:14:40,240
api to fix let's say so get lfs it's

387
00:14:37,680 --> 00:14:41,199
large file storage for git it's an api

388
00:14:40,240 --> 00:14:44,079
that you can

389
00:14:41,199 --> 00:14:45,199
add to your git storage and when you

390
00:14:44,079 --> 00:14:48,560
want to track

391
00:14:45,199 --> 00:14:50,319
uh say binary or a big file whatever it

392
00:14:48,560 --> 00:14:52,560
is you can ask

393
00:14:50,320 --> 00:14:53,680
lfs to track it directly in object

394
00:14:52,560 --> 00:14:56,719
storage so

395
00:14:53,680 --> 00:14:57,519
that when you commit it on it the file

396
00:14:56,720 --> 00:15:00,160
will be replaced

397
00:14:57,519 --> 00:15:01,279
with uh a pointer to a location on that

398
00:15:00,160 --> 00:15:03,199
storage and

399
00:15:01,279 --> 00:15:04,639
the git client will just handle the

400
00:15:03,199 --> 00:15:06,240
thing for you so when you clone and

401
00:15:04,639 --> 00:15:07,600
check out you get you download the file

402
00:15:06,240 --> 00:15:09,839
and you have it but it's not

403
00:15:07,600 --> 00:15:11,680
in the ripple not technically in the

404
00:15:09,839 --> 00:15:14,000
repo and this was easy

405
00:15:11,680 --> 00:15:14,959
because you have a very simple api that

406
00:15:14,000 --> 00:15:17,760
tells you please

407
00:15:14,959 --> 00:15:19,760
put this object there it gives you the

408
00:15:17,760 --> 00:15:20,240
size and the body of the request is just

409
00:15:19,760 --> 00:15:23,519
a file

410
00:15:20,240 --> 00:15:28,240
so very easy one

411
00:15:23,519 --> 00:15:31,279
now i have a background as a ruby reeds

412
00:15:28,240 --> 00:15:33,920
developer and the first thing that i

413
00:15:31,279 --> 00:15:34,320
realized looking at the i o package was

414
00:15:33,920 --> 00:15:37,279
uh

415
00:15:34,320 --> 00:15:37,920
i don't like it it's so i expected more

416
00:15:37,279 --> 00:15:40,959
feature

417
00:15:37,920 --> 00:15:42,000
i expect it to be more powerful then i

418
00:15:40,959 --> 00:15:44,719
started writing

419
00:15:42,000 --> 00:15:45,440
go code daily and say oh i really love

420
00:15:44,720 --> 00:15:47,759
it

421
00:15:45,440 --> 00:15:49,600
so the idea that i a reader and i of

422
00:15:47,759 --> 00:15:50,399
writer are so simple you can pipe them

423
00:15:49,600 --> 00:15:53,600
together

424
00:15:50,399 --> 00:15:55,040
and it's incredibly powerful without

425
00:15:53,600 --> 00:15:56,959
you don't need all those abstractions

426
00:15:55,040 --> 00:16:00,079
it's just everything it's a stream of

427
00:15:56,959 --> 00:16:03,758
bite you can read it or you can write it

428
00:16:00,079 --> 00:16:05,359
so this is it still fits in one slide

429
00:16:03,759 --> 00:16:07,600
maybe it's a bit hard to read

430
00:16:05,360 --> 00:16:09,839
but this is an android that gives you

431
00:16:07,600 --> 00:16:13,360
the idea how can you

432
00:16:09,839 --> 00:16:14,399
do body hijacking and directly storing

433
00:16:13,360 --> 00:16:18,160
the information

434
00:16:14,399 --> 00:16:20,320
on object storage while it is in transit

435
00:16:18,160 --> 00:16:22,160
so without buffering it without writing

436
00:16:20,320 --> 00:16:25,360
it on disk

437
00:16:22,160 --> 00:16:28,160
let's go through it so

438
00:16:25,360 --> 00:16:29,680
first thing we didn't want to move

439
00:16:28,160 --> 00:16:32,800
authorization logic

440
00:16:29,680 --> 00:16:34,719
to workers because the idea is that you

441
00:16:32,800 --> 00:16:36,560
need to write what you need to speed up

442
00:16:34,720 --> 00:16:39,839
the operation but we still have

443
00:16:36,560 --> 00:16:42,079
hundreds of engineers that work on ruby

444
00:16:39,839 --> 00:16:43,920
on rates daily so we just want to move

445
00:16:42,079 --> 00:16:45,279
to keep everything in the ruby rates

446
00:16:43,920 --> 00:16:48,319
code base so we

447
00:16:45,279 --> 00:16:50,000
made an api that basically

448
00:16:48,320 --> 00:16:51,759
received the request and with some

449
00:16:50,000 --> 00:16:54,000
information from the request

450
00:16:51,759 --> 00:16:55,440
check if you are authorized or not to

451
00:16:54,000 --> 00:16:59,839
upload that information

452
00:16:55,440 --> 00:17:03,440
and gives you back a pre-signed url

453
00:16:59,839 --> 00:17:06,799
so then in the contest

454
00:17:03,440 --> 00:17:09,679
of the of the handler in the go proxy

455
00:17:06,799 --> 00:17:11,918
you just create a new http request a put

456
00:17:09,679 --> 00:17:15,360
request on the signed url

457
00:17:11,919 --> 00:17:17,679
and you forward this work yeah

458
00:17:15,359 --> 00:17:18,399
and you forward the body of the incoming

459
00:17:17,679 --> 00:17:20,559
request

460
00:17:18,400 --> 00:17:22,000
wrap it in uh no closer but just a

461
00:17:20,559 --> 00:17:24,799
little

462
00:17:22,000 --> 00:17:25,599
so what happens here is and you set the

463
00:17:24,799 --> 00:17:28,799
content length

464
00:17:25,599 --> 00:17:29,520
from the from the request that is coming

465
00:17:28,799 --> 00:17:33,120
in

466
00:17:29,520 --> 00:17:35,679
and then you just too much

467
00:17:33,120 --> 00:17:38,479
okay it's not alive forgive me so the

468
00:17:35,679 --> 00:17:41,600
point is that then you run this request

469
00:17:38,480 --> 00:17:43,600
you basically are moving the

470
00:17:41,600 --> 00:17:45,439
body of the incoming request while you

471
00:17:43,600 --> 00:17:49,520
read it from your

472
00:17:45,440 --> 00:17:50,400
from the git client directly into s3 or

473
00:17:49,520 --> 00:17:52,400
google cloud

474
00:17:50,400 --> 00:17:54,160
storage or whatever mini io whatever

475
00:17:52,400 --> 00:17:56,080
you're using as an object storage

476
00:17:54,160 --> 00:17:57,360
so you don't buffer it and as soon as

477
00:17:56,080 --> 00:18:01,280
you read it it gets

478
00:17:57,360 --> 00:18:03,120
directly in the object storage

479
00:18:01,280 --> 00:18:04,879
once you're done and you check that

480
00:18:03,120 --> 00:18:07,760
nothing failed

481
00:18:04,880 --> 00:18:09,120
you copy the incoming request remove the

482
00:18:07,760 --> 00:18:10,799
body

483
00:18:09,120 --> 00:18:13,360
set the content length to zero because

484
00:18:10,799 --> 00:18:14,000
you removed it you had some metadata

485
00:18:13,360 --> 00:18:16,559
that you should

486
00:18:14,000 --> 00:18:17,200
definitely sign telling where you stored

487
00:18:16,559 --> 00:18:20,000
it

488
00:18:17,200 --> 00:18:21,039
and you forward the request like a real

489
00:18:20,000 --> 00:18:23,760
proxy

490
00:18:21,039 --> 00:18:25,440
so when the reapers reach the upstream

491
00:18:23,760 --> 00:18:29,200
so the rails application

492
00:18:25,440 --> 00:18:32,720
the file is already safely stored on s3

493
00:18:29,200 --> 00:18:35,840
on the object storage whatever it is so

494
00:18:32,720 --> 00:18:40,480
mission complete well

495
00:18:35,840 --> 00:18:43,600
not exactly as i said we had some

496
00:18:40,480 --> 00:18:44,559
dirty tricks and that we had to take

497
00:18:43,600 --> 00:18:47,760
care

498
00:18:44,559 --> 00:18:51,039
so we were lucky because

499
00:18:47,760 --> 00:18:54,080
google cloud storage is not exactly

500
00:18:51,039 --> 00:18:56,960
an s3 like implementation has a

501
00:18:54,080 --> 00:18:58,960
one difference that allowed us to shift

502
00:18:56,960 --> 00:19:01,760
this

503
00:18:58,960 --> 00:19:03,760
so google cloud storage is the only s3

504
00:19:01,760 --> 00:19:05,120
compatible implementation out there that

505
00:19:03,760 --> 00:19:07,760
allows you to stream

506
00:19:05,120 --> 00:19:09,918
unknown ranked records this is not

507
00:19:07,760 --> 00:19:11,280
compatible with the s3 api so minio will

508
00:19:09,919 --> 00:19:12,160
refuse it and all the other

509
00:19:11,280 --> 00:19:14,720
implementation

510
00:19:12,160 --> 00:19:17,120
they want to know upfront how much

511
00:19:14,720 --> 00:19:20,240
storage you need for that request

512
00:19:17,120 --> 00:19:22,080
so we had around 35 000 ci runners in

513
00:19:20,240 --> 00:19:25,120
the wild outside of our control

514
00:19:22,080 --> 00:19:28,080
that were sending artifacts without

515
00:19:25,120 --> 00:19:29,760
launch requests because they were

516
00:19:28,080 --> 00:19:32,879
compressing it

517
00:19:29,760 --> 00:19:34,799
on on transit directly on the upload

518
00:19:32,880 --> 00:19:38,000
record so we cannot have the

519
00:19:34,799 --> 00:19:41,039
the size without writing it directly

520
00:19:38,000 --> 00:19:44,799
on disk so this was a big problem

521
00:19:41,039 --> 00:19:46,160
so next iteration we went back to the

522
00:19:44,799 --> 00:19:49,360
drawing board and

523
00:19:46,160 --> 00:19:50,400
we started looking more deeply at the s3

524
00:19:49,360 --> 00:19:52,639
apis

525
00:19:50,400 --> 00:19:54,000
and we found out this thing the

526
00:19:52,640 --> 00:19:57,919
multi-part upload

527
00:19:54,000 --> 00:20:00,720
so divide and upload it was designed for

528
00:19:57,919 --> 00:20:01,280
another use case the idea here is that

529
00:20:00,720 --> 00:20:04,320
to

530
00:20:01,280 --> 00:20:07,440
increase performance to use

531
00:20:04,320 --> 00:20:09,840
more make a better use of your bandwidth

532
00:20:07,440 --> 00:20:10,960
you can split your original object in

533
00:20:09,840 --> 00:20:13,360
several parts

534
00:20:10,960 --> 00:20:15,120
upload them concurrently and then you

535
00:20:13,360 --> 00:20:17,039
have a final code that just

536
00:20:15,120 --> 00:20:19,039
finalized everything this is just one

537
00:20:17,039 --> 00:20:22,080
single object

538
00:20:19,039 --> 00:20:25,600
and then you have your final object

539
00:20:22,080 --> 00:20:25,918
now we decided to just implement this

540
00:20:25,600 --> 00:20:28,639
thing

541
00:20:25,919 --> 00:20:29,440
in our reverse proxy but we found out

542
00:20:28,640 --> 00:20:31,360
that

543
00:20:29,440 --> 00:20:33,840
all the libraries out there were

544
00:20:31,360 --> 00:20:36,240
designed for this kind of use case so

545
00:20:33,840 --> 00:20:36,879
either they expected to be able to seek

546
00:20:36,240 --> 00:20:40,000
the file

547
00:20:36,880 --> 00:20:43,600
on disk so that we can they could run

548
00:20:40,000 --> 00:20:48,080
multiple performance upload in parallel

549
00:20:43,600 --> 00:20:49,678
or they were optimizing for they were

550
00:20:48,080 --> 00:20:52,639
not taking care of memories so

551
00:20:49,679 --> 00:20:54,400
if they weren't able to re to gather the

552
00:20:52,640 --> 00:20:56,320
sides of the request like if an income

553
00:20:54,400 --> 00:21:00,000
you have an incoming body

554
00:20:56,320 --> 00:21:02,639
it was just okay the maximum amount

555
00:21:00,000 --> 00:21:04,080
that i can put is 600 megabytes so i

556
00:21:02,640 --> 00:21:07,039
would just start reading

557
00:21:04,080 --> 00:21:08,320
600 megabytes and then upload it and

558
00:21:07,039 --> 00:21:10,400
this was a problem because we had to

559
00:21:08,320 --> 00:21:13,840
keep memory usage under control

560
00:21:10,400 --> 00:21:16,240
because we had to take care of

561
00:21:13,840 --> 00:21:17,360
multiple concurrent upload from the

562
00:21:16,240 --> 00:21:20,320
outside

563
00:21:17,360 --> 00:21:21,600
and so we wanted to make this in a in a

564
00:21:20,320 --> 00:21:24,720
way we could control

565
00:21:21,600 --> 00:21:28,080
memory usage so we came up with this

566
00:21:24,720 --> 00:21:28,880
very simple idea which is whenever this

567
00:21:28,080 --> 00:21:31,918
comes in

568
00:21:28,880 --> 00:21:33,520
we create a temporary file we write up

569
00:21:31,919 --> 00:21:35,679
to 50 megabytes it's

570
00:21:33,520 --> 00:21:37,440
if the api controls that number but just

571
00:21:35,679 --> 00:21:41,039
to give you an idea

572
00:21:37,440 --> 00:21:43,120
so we write the first bytes to the disk

573
00:21:41,039 --> 00:21:45,200
then we upload that temporary file as a

574
00:21:43,120 --> 00:21:47,280
part of the multi-part upload

575
00:21:45,200 --> 00:21:49,120
we delete the file so do we keep also

576
00:21:47,280 --> 00:21:51,520
the disk usage under control

577
00:21:49,120 --> 00:21:52,479
and we check are we are we done no go

578
00:21:51,520 --> 00:21:55,760
back to the beginning

579
00:21:52,480 --> 00:21:56,559
right temp file write upload delete once

580
00:21:55,760 --> 00:21:59,200
we reach

581
00:21:56,559 --> 00:22:01,120
the end of the incoming stream so the

582
00:21:59,200 --> 00:22:03,200
but the request body

583
00:22:01,120 --> 00:22:05,840
we say okay we're done and we send the

584
00:22:03,200 --> 00:22:09,440
finalized call

585
00:22:05,840 --> 00:22:13,039
and that's it so we made it

586
00:22:09,440 --> 00:22:13,520
we were able to migrate live system from

587
00:22:13,039 --> 00:22:15,440
the

588
00:22:13,520 --> 00:22:16,799
from one cloud provider to the other one

589
00:22:15,440 --> 00:22:19,440
we were able in

590
00:22:16,799 --> 00:22:20,799
to release the first iteration of the

591
00:22:19,440 --> 00:22:23,200
cloud native installation

592
00:22:20,799 --> 00:22:25,760
then the second one had also support for

593
00:22:23,200 --> 00:22:28,960
menio and the other one

594
00:22:25,760 --> 00:22:30,640
so yeah i want to thank you

595
00:22:28,960 --> 00:22:32,080
for listening to me and i want to

596
00:22:30,640 --> 00:22:34,080
highlight some

597
00:22:32,080 --> 00:22:35,760
takeaways from this story which what we

598
00:22:34,080 --> 00:22:37,760
learned

599
00:22:35,760 --> 00:22:38,799
so the first thing is you can speed up a

600
00:22:37,760 --> 00:22:40,960
web application

601
00:22:38,799 --> 00:22:43,679
reading writing a reverse proxy in go no

602
00:22:40,960 --> 00:22:45,520
matter if you are a company that writes

603
00:22:43,679 --> 00:22:48,480
in another language you can sort

604
00:22:45,520 --> 00:22:50,799
incrementally it's an iterative approach

605
00:22:48,480 --> 00:22:53,039
which is a good thing because you can

606
00:22:50,799 --> 00:22:55,200
rewrite only these low end points as

607
00:22:53,039 --> 00:22:56,480
is not that kind of project when you say

608
00:22:55,200 --> 00:22:58,640
yeah we're going to rewrite

609
00:22:56,480 --> 00:23:00,000
the world code base because go is the

610
00:22:58,640 --> 00:23:03,520
way to do yeah it is

611
00:23:00,000 --> 00:23:04,880
but nobody no higher level management

612
00:23:03,520 --> 00:23:08,080
will ever accept you yeah let's

613
00:23:04,880 --> 00:23:10,640
write everything so you can start we're

614
00:23:08,080 --> 00:23:11,600
showing where you can improve things you

615
00:23:10,640 --> 00:23:14,080
can forward

616
00:23:11,600 --> 00:23:16,320
to another service if you need it which

617
00:23:14,080 --> 00:23:19,520
is a very good entry point

618
00:23:16,320 --> 00:23:21,760
for splitting a monolith into a

619
00:23:19,520 --> 00:23:24,080
microservice or just a service

620
00:23:21,760 --> 00:23:27,600
architecture

621
00:23:24,080 --> 00:23:30,080
and always always remember to sign

622
00:23:27,600 --> 00:23:32,719
modified requests if you expect to

623
00:23:30,080 --> 00:23:35,199
change something

624
00:23:32,720 --> 00:23:36,000
sign it so that the upstream should

625
00:23:35,200 --> 00:23:38,880
check

626
00:23:36,000 --> 00:23:39,919
that the signature knows that the thing

627
00:23:38,880 --> 00:23:42,880
that you brought in there

628
00:23:39,919 --> 00:23:45,919
are really coming from your me your

629
00:23:42,880 --> 00:23:48,400
reverse proxy and not from the outside

630
00:23:45,919 --> 00:23:49,360
workers search code is available at the

631
00:23:48,400 --> 00:23:51,840
url is there

632
00:23:49,360 --> 00:23:53,439
and it's released under mit license so

633
00:23:51,840 --> 00:23:55,439
all the examples that

634
00:23:53,440 --> 00:23:57,360
you have seen here are just small

635
00:23:55,440 --> 00:23:58,960
examples not extracted from the real

636
00:23:57,360 --> 00:24:02,559
code base just to

637
00:23:58,960 --> 00:24:04,000
show the key points and yeah but if you

638
00:24:02,559 --> 00:24:06,000
want to take a look at how we did it

639
00:24:04,000 --> 00:24:07,840
there are more complexity involved

640
00:24:06,000 --> 00:24:09,760
you can you're free to study the code

641
00:24:07,840 --> 00:24:18,080
contribute if you like and

642
00:24:09,760 --> 00:24:23,039
that's it thank you

643
00:24:18,080 --> 00:24:23,039
thank you any questions yes

644
00:24:26,000 --> 00:24:33,279
thank you for the talk uh how do

645
00:24:29,360 --> 00:24:36,879
you test proxy api calls

646
00:24:33,279 --> 00:24:37,760
okay how do we test the reverse api

647
00:24:36,880 --> 00:24:39,760
proxy

648
00:24:37,760 --> 00:24:41,760
as you're leaving please try to make

649
00:24:39,760 --> 00:24:44,960
less noise thank you

650
00:24:41,760 --> 00:24:47,760
we have several levels of testing so

651
00:24:44,960 --> 00:24:48,559
we have the unit testing and acceptance

652
00:24:47,760 --> 00:24:50,879
testing

653
00:24:48,559 --> 00:24:52,559
in both projects so they are tested in

654
00:24:50,880 --> 00:24:55,039
isolation

655
00:24:52,559 --> 00:24:56,480
so for every commit in the ci you run

656
00:24:55,039 --> 00:24:59,679
this kind of test

657
00:24:56,480 --> 00:25:02,080
then we have um the apps

658
00:24:59,679 --> 00:25:04,000
this is just our case the the raids

659
00:25:02,080 --> 00:25:07,039
application has a reference of the

660
00:25:04,000 --> 00:25:08,640
version of the upstream proxy of the

661
00:25:07,039 --> 00:25:11,440
proxy that is supposed to work

662
00:25:08,640 --> 00:25:13,679
with and when we bundle everything

663
00:25:11,440 --> 00:25:14,000
together we have some qa pipeline that

664
00:25:13,679 --> 00:25:16,720
just

665
00:25:14,000 --> 00:25:18,400
builds the entire system and they run

666
00:25:16,720 --> 00:25:23,039
some

667
00:25:18,400 --> 00:25:24,799
some use case end to end through all the

668
00:25:23,039 --> 00:25:26,559
great thank you very much we don't have

669
00:25:24,799 --> 00:25:29,030
time for more questions sorry but you

670
00:25:26,559 --> 00:25:34,670
can come and talk to him thank you

671
00:25:29,030 --> 00:25:34,670
[Applause]

