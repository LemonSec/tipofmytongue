1
00:00:05,120 --> 00:00:09,359
so we are very happy to have here

2
00:00:06,640 --> 00:00:09,359
jimmy angela

3
00:00:10,719 --> 00:00:16,000
thank you before we start

4
00:00:14,240 --> 00:00:18,080
how many of you have a background in

5
00:00:16,000 --> 00:00:21,359
mathematics

6
00:00:18,080 --> 00:00:24,240
so a few hands many hands and how many

7
00:00:21,359 --> 00:00:26,960
of you have a background in linguistics

8
00:00:24,240 --> 00:00:28,400
oh okay nice to see some people here

9
00:00:26,960 --> 00:00:30,880
with

10
00:00:28,400 --> 00:00:32,238
both kinds of skills um it's going to be

11
00:00:30,880 --> 00:00:33,760
boring for both of you because it

12
00:00:32,238 --> 00:00:35,440
doesn't have enough maths

13
00:00:33,760 --> 00:00:38,320
and it doesn't have enough linguistics

14
00:00:35,440 --> 00:00:38,320
but um

15
00:00:39,520 --> 00:00:44,399
it does have some samples so um the

16
00:00:42,719 --> 00:00:48,000
motivation for this talk

17
00:00:44,399 --> 00:00:51,199
is um i was speaking to some

18
00:00:48,000 --> 00:00:54,079
let's say colleagues customers

19
00:00:51,199 --> 00:00:54,800
community members and i was surprised to

20
00:00:54,079 --> 00:00:57,760
find out

21
00:00:54,800 --> 00:00:58,399
that these things aren't all that well

22
00:00:57,760 --> 00:01:01,519
known

23
00:00:58,399 --> 00:01:05,439
there are people that use postgres that

24
00:01:01,520 --> 00:01:08,400
actually use search engines

25
00:01:05,438 --> 00:01:09,439
uh to index their text like external

26
00:01:08,400 --> 00:01:11,280
search engines

27
00:01:09,439 --> 00:01:13,919
and it's all basically the same code

28
00:01:11,280 --> 00:01:15,920
base because it's always lucine

29
00:01:13,920 --> 00:01:18,159
and solar which is lucine and

30
00:01:15,920 --> 00:01:22,400
elasticsearch which is lucine

31
00:01:18,159 --> 00:01:24,240
and these things aren't databases

32
00:01:22,400 --> 00:01:25,520
they create indexes for your text but

33
00:01:24,240 --> 00:01:28,158
they're not databases

34
00:01:25,520 --> 00:01:29,600
your text is in your database so why do

35
00:01:28,159 --> 00:01:31,040
you have an extra thing

36
00:01:29,600 --> 00:01:33,039
when you can index your text and

37
00:01:31,040 --> 00:01:33,680
database and that's the motivation let's

38
00:01:33,040 --> 00:01:38,079
see

39
00:01:33,680 --> 00:01:41,680
how you can search for text in postgres

40
00:01:38,079 --> 00:01:44,798
now we're going to look at what full

41
00:01:41,680 --> 00:01:44,799
text search means

42
00:01:45,280 --> 00:01:48,880
we're going to look at operators and

43
00:01:46,960 --> 00:01:51,600
functions what

44
00:01:48,880 --> 00:01:52,880
dictionaries are in postgres a few

45
00:01:51,600 --> 00:01:57,199
examples

46
00:01:52,880 --> 00:01:59,839
um how to index text in postgres

47
00:01:57,200 --> 00:02:02,079
how to deal with text that isn't human

48
00:01:59,840 --> 00:02:06,079
language

49
00:02:02,079 --> 00:02:08,639
uh or natural text what collations are

50
00:02:06,079 --> 00:02:11,359
what other things you can search for

51
00:02:08,639 --> 00:02:15,839
that are not strictly text

52
00:02:11,360 --> 00:02:15,840
and how to keep this whole thing running

53
00:02:17,360 --> 00:02:21,360
now are you excited for full text search

54
00:02:19,840 --> 00:02:24,800
yeah

55
00:02:21,360 --> 00:02:25,760
you shouldn't be your your trust is

56
00:02:24,800 --> 00:02:28,480
misplaced

57
00:02:25,760 --> 00:02:30,399
so we do have things like this appearing

58
00:02:28,480 --> 00:02:32,399
in the presentation

59
00:02:30,400 --> 00:02:34,080
and they have been known to include to

60
00:02:32,400 --> 00:02:34,879
induce drowsiness and inappropriate

61
00:02:34,080 --> 00:02:36,959
sleep so

62
00:02:34,879 --> 00:02:39,440
if you feel like sleeping nobody will

63
00:02:36,959 --> 00:02:39,440
blame you

64
00:02:39,760 --> 00:02:44,720
let's look at what is text

65
00:02:44,959 --> 00:02:48,640
let's look at the representation of text

66
00:02:47,440 --> 00:02:52,239
and postgres

67
00:02:48,640 --> 00:02:56,079
we have the well-known car

68
00:02:52,239 --> 00:02:56,640
with a character limit that says you can

69
00:02:56,080 --> 00:02:59,519
only store

70
00:02:56,640 --> 00:03:02,159
this many characters this is padded so

71
00:02:59,519 --> 00:03:05,360
if you define a car of 10

72
00:03:02,159 --> 00:03:08,560
then and insert five characters

73
00:03:05,360 --> 00:03:10,480
in that field you will get five

74
00:03:08,560 --> 00:03:13,040
blank spaces at the end because it's

75
00:03:10,480 --> 00:03:13,840
padded these blank spaces do not count

76
00:03:13,040 --> 00:03:16,879
for anything

77
00:03:13,840 --> 00:03:20,159
they are ignored by postgres

78
00:03:16,879 --> 00:03:22,399
then you have varchar

79
00:03:20,159 --> 00:03:25,040
which is variable length but still has a

80
00:03:22,400 --> 00:03:28,239
fixed limit

81
00:03:25,040 --> 00:03:30,560
and then you have text

82
00:03:28,239 --> 00:03:32,239
and the confusingly named varchar which

83
00:03:30,560 --> 00:03:35,200
is the same thing

84
00:03:32,239 --> 00:03:36,159
and you should really use the word text

85
00:03:35,200 --> 00:03:37,920
i believe

86
00:03:36,159 --> 00:03:39,920
if you want to store an unlimited amount

87
00:03:37,920 --> 00:03:41,200
of text because it makes it clear that

88
00:03:39,920 --> 00:03:44,399
it is not varchar

89
00:03:41,200 --> 00:03:46,959
it's a different type of column so

90
00:03:44,400 --> 00:03:48,000
this is a character large object you can

91
00:03:46,959 --> 00:03:51,599
store

92
00:03:48,000 --> 00:03:55,519
a lot of text in there and in

93
00:03:51,599 --> 00:03:57,839
the variable length text in varchar

94
00:03:55,519 --> 00:03:58,959
we see that trailing spaces are

95
00:03:57,840 --> 00:04:02,000
significant

96
00:03:58,959 --> 00:04:05,280
if you leave a space at the end of your

97
00:04:02,000 --> 00:04:06,959
string then that counts for

98
00:04:05,280 --> 00:04:09,519
like expressions or for regular

99
00:04:06,959 --> 00:04:09,519
expressions

100
00:04:09,840 --> 00:04:15,040
how are they stored depends on the

101
00:04:13,360 --> 00:04:17,440
character set

102
00:04:15,040 --> 00:04:19,918
there are multi-byte encodings like

103
00:04:17,440 --> 00:04:22,400
utf-8 that will use one character for

104
00:04:19,918 --> 00:04:23,440
latin text or two characters for other

105
00:04:22,400 --> 00:04:27,359
languages

106
00:04:23,440 --> 00:04:29,759
or as many as the uh

107
00:04:27,360 --> 00:04:31,199
ad in the end uh with all these emojis

108
00:04:29,759 --> 00:04:34,880
and stuff that they keep adding

109
00:04:31,199 --> 00:04:38,080
um the storage is one byte plus

110
00:04:34,880 --> 00:04:42,639
the length of your string in bytes

111
00:04:38,080 --> 00:04:44,479
not characters so 126 bytes

112
00:04:42,639 --> 00:04:46,479
and if you go over that limit the

113
00:04:44,479 --> 00:04:50,159
storage changes to four

114
00:04:46,479 --> 00:04:52,719
bytes plus the length of your string

115
00:04:50,160 --> 00:04:52,720
in bytes

116
00:04:54,880 --> 00:04:59,680
but there is no guarantee that this is

117
00:04:57,919 --> 00:05:00,400
the amount of space that it will take up

118
00:04:59,680 --> 00:05:03,440
on disk

119
00:05:00,400 --> 00:05:07,120
because of compression and we also have

120
00:05:03,440 --> 00:05:08,719
toast or the oversized attribute

121
00:05:07,120 --> 00:05:10,960
the oversized attribute storage

122
00:05:08,720 --> 00:05:12,400
technique which puts it in separate

123
00:05:10,960 --> 00:05:15,520
tables

124
00:05:12,400 --> 00:05:15,919
if it goes over the maximum length of

125
00:05:15,520 --> 00:05:18,240
your

126
00:05:15,919 --> 00:05:18,240
row

127
00:05:19,520 --> 00:05:24,960
so what is text search

128
00:05:22,960 --> 00:05:26,320
we're talking here about information

129
00:05:24,960 --> 00:05:29,919
retrieval

130
00:05:26,320 --> 00:05:33,120
and more specifically tax retrieval

131
00:05:29,919 --> 00:05:35,919
this is a well-known domain

132
00:05:33,120 --> 00:05:36,960
and we mainly use it for searches on

133
00:05:35,919 --> 00:05:38,960
metadata

134
00:05:36,960 --> 00:05:40,638
which can be like descriptive

135
00:05:38,960 --> 00:05:43,919
bibliographic there can be

136
00:05:40,639 --> 00:05:47,120
tags tagging objects and are used

137
00:05:43,919 --> 00:05:49,280
for discovery and identification you

138
00:05:47,120 --> 00:05:51,680
are generally not when you're talking

139
00:05:49,280 --> 00:05:54,719
about text search and text

140
00:05:51,680 --> 00:05:58,400
matching you are not

141
00:05:54,720 --> 00:05:59,840
trying to uh identify let's say the web

142
00:05:58,400 --> 00:06:01,440
pages or the articles that you're

143
00:05:59,840 --> 00:06:03,919
interested in

144
00:06:01,440 --> 00:06:05,600
through their content you're looking at

145
00:06:03,919 --> 00:06:06,000
metadata fields and you're trying to

146
00:06:05,600 --> 00:06:10,160
match

147
00:06:06,000 --> 00:06:12,960
words out of the whole thing so matching

148
00:06:10,160 --> 00:06:13,680
substring search data extraction it's

149
00:06:12,960 --> 00:06:15,680
useful for

150
00:06:13,680 --> 00:06:17,199
you can clean your data you can use it

151
00:06:15,680 --> 00:06:20,319
for data mining

152
00:06:17,199 --> 00:06:24,639
and these are the supported

153
00:06:20,319 --> 00:06:28,720
operators in postgres for this sort of

154
00:06:24,639 --> 00:06:32,319
thing you can use like

155
00:06:28,720 --> 00:06:33,199
or double tilde and case insensitive

156
00:06:32,319 --> 00:06:38,000
like

157
00:06:33,199 --> 00:06:40,479
or double tilde star which

158
00:06:38,000 --> 00:06:42,160
is basically wild card expressions if

159
00:06:40,479 --> 00:06:43,199
you want to match something against a

160
00:06:42,160 --> 00:06:46,400
wild card

161
00:06:43,199 --> 00:06:47,840
with the percent sign or underscore

162
00:06:46,400 --> 00:06:50,080
percent sign for any number of

163
00:06:47,840 --> 00:06:53,119
characters underscore for

164
00:06:50,080 --> 00:06:56,400
uh just one character then that is the

165
00:06:53,120 --> 00:06:56,400
sql way to do it

166
00:06:56,880 --> 00:07:03,840
you also get regular expressions

167
00:07:00,240 --> 00:07:07,199
and the tilde until the star

168
00:07:03,840 --> 00:07:11,280
are posix compatible regular expressions

169
00:07:07,199 --> 00:07:13,280
so not quite perl but very standard

170
00:07:11,280 --> 00:07:15,039
and postgres implements them out of the

171
00:07:13,280 --> 00:07:18,638
box there's also the function

172
00:07:15,039 --> 00:07:19,919
regex match where you can give it a

173
00:07:18,639 --> 00:07:22,960
string of text

174
00:07:19,919 --> 00:07:26,240
and a pattern and it will return

175
00:07:22,960 --> 00:07:30,638
whatever matched the pattern from

176
00:07:26,240 --> 00:07:33,759
that original string that you gave it

177
00:07:30,639 --> 00:07:38,160
but are these things enough

178
00:07:33,759 --> 00:07:38,160
for searching for text

179
00:07:38,319 --> 00:07:41,919
you can't have with regular expressions

180
00:07:41,360 --> 00:07:45,039
and

181
00:07:41,919 --> 00:07:47,198
likes you cannot get ranking of results

182
00:07:45,039 --> 00:07:48,080
because nothing has meaning it's just

183
00:07:47,199 --> 00:07:50,720
characters

184
00:07:48,080 --> 00:07:51,758
it's not language you can't have

185
00:07:50,720 --> 00:07:54,240
relevance

186
00:07:51,759 --> 00:07:54,240
in this

187
00:07:54,879 --> 00:07:59,039
so no concept of language when you're

188
00:07:57,039 --> 00:08:00,240
using when you're matching with regular

189
00:07:59,039 --> 00:08:04,080
expressions

190
00:08:00,240 --> 00:08:06,400
therefore if you search for a ship

191
00:08:04,080 --> 00:08:08,479
and you search for shipping it will be

192
00:08:06,400 --> 00:08:10,840
two totally different things

193
00:08:08,479 --> 00:08:13,440
that will never be related they will not

194
00:08:10,840 --> 00:08:16,159
match

195
00:08:13,440 --> 00:08:16,800
and also they can't be indexed which is

196
00:08:16,160 --> 00:08:18,400
bad

197
00:08:16,800 --> 00:08:20,240
if you have a lot of text that you want

198
00:08:18,400 --> 00:08:23,919
to search for

199
00:08:20,240 --> 00:08:27,840
okay you can index these

200
00:08:23,919 --> 00:08:27,840
in some limited way

201
00:08:28,400 --> 00:08:32,399
and there's also similar too you better

202
00:08:30,479 --> 00:08:35,760
forget about that one

203
00:08:32,399 --> 00:08:38,958
it's a mismatch with some mix

204
00:08:35,760 --> 00:08:41,838
between posix regular expressions in sql

205
00:08:38,958 --> 00:08:44,399
that everyone is better off not using so

206
00:08:41,839 --> 00:08:47,600
just forget about it

207
00:08:44,399 --> 00:08:52,959
now we have inserted the word full

208
00:08:47,600 --> 00:08:52,959
what do we mean by full text search

209
00:08:53,200 --> 00:08:56,720
now we're going a step beyond from

210
00:08:54,800 --> 00:08:57,439
information retrieval and text retrieval

211
00:08:56,720 --> 00:09:00,800
we're going to

212
00:08:57,440 --> 00:09:03,360
document retrieval so based on

213
00:09:00,800 --> 00:09:04,160
our predicates or what we are searching

214
00:09:03,360 --> 00:09:07,200
for

215
00:09:04,160 --> 00:09:08,880
we want to return results that are

216
00:09:07,200 --> 00:09:12,000
documents

217
00:09:08,880 --> 00:09:13,760
articles web pages whatever contains

218
00:09:12,000 --> 00:09:16,800
text

219
00:09:13,760 --> 00:09:19,680
so full text searches

220
00:09:16,800 --> 00:09:20,800
a search of words or tokens as we call

221
00:09:19,680 --> 00:09:23,599
them

222
00:09:20,800 --> 00:09:26,560
in a database so the sum of all

223
00:09:23,600 --> 00:09:26,560
documents that you have

224
00:09:29,279 --> 00:09:35,120
if you don't have an index for this this

225
00:09:32,160 --> 00:09:38,719
will degenerate into a serial search

226
00:09:35,120 --> 00:09:41,519
so when you start searching it won't be

227
00:09:38,720 --> 00:09:43,440
faster than grep

228
00:09:41,519 --> 00:09:46,160
because it has to go through the entire

229
00:09:43,440 --> 00:09:50,480
document for every document

230
00:09:46,160 --> 00:09:52,480
so you do need an index for this

231
00:09:50,480 --> 00:09:55,040
because you don't want to scan through

232
00:09:52,480 --> 00:09:58,160
the entire document

233
00:09:55,040 --> 00:10:01,439
there are also techniques that

234
00:09:58,160 --> 00:10:03,120
let's say make it easier to search and

235
00:10:01,440 --> 00:10:05,920
reduce the amount of text

236
00:10:03,120 --> 00:10:08,640
based on natural language processing

237
00:10:05,920 --> 00:10:08,640
algorithms

238
00:10:09,760 --> 00:10:14,720
and we also have to talk about the

239
00:10:12,560 --> 00:10:17,439
famous trade-off

240
00:10:14,720 --> 00:10:18,560
of precision versus recall what is

241
00:10:17,440 --> 00:10:22,000
precision

242
00:10:18,560 --> 00:10:25,199
it's how accurate your text

243
00:10:22,000 --> 00:10:28,640
your search results are therefore

244
00:10:25,200 --> 00:10:30,320
how many false positives you're getting

245
00:10:28,640 --> 00:10:31,680
or how many false negatives you're

246
00:10:30,320 --> 00:10:34,399
getting

247
00:10:31,680 --> 00:10:36,000
when you're retrieving your result set

248
00:10:34,399 --> 00:10:39,440
from your query

249
00:10:36,000 --> 00:10:42,000
and recall is how many

250
00:10:39,440 --> 00:10:43,920
results you're getting back based on

251
00:10:42,000 --> 00:10:47,680
that query how restrictive

252
00:10:43,920 --> 00:10:49,599
is your query

253
00:10:47,680 --> 00:10:50,719
and these are influenced by things

254
00:10:49,600 --> 00:10:54,160
called stop words

255
00:10:50,720 --> 00:10:55,440
and also stemming and we'll look at

256
00:10:54,160 --> 00:10:58,160
those but first of all

257
00:10:55,440 --> 00:10:58,959
let's look at documents and tokens so a

258
00:10:58,160 --> 00:11:02,079
document is a

259
00:10:58,959 --> 00:11:06,000
chunk of text it's

260
00:11:02,079 --> 00:11:09,120
a field in a row in your database

261
00:11:06,000 --> 00:11:09,120
in a table of course

262
00:11:09,839 --> 00:11:12,880
any type of text is considered the

263
00:11:11,920 --> 00:11:17,040
document

264
00:11:12,880 --> 00:11:20,079
for this context today

265
00:11:17,040 --> 00:11:22,959
parsing of documents

266
00:11:20,079 --> 00:11:24,319
into classes of tokens is what needs to

267
00:11:22,959 --> 00:11:28,319
be done next

268
00:11:24,320 --> 00:11:30,720
so tokens can be strings

269
00:11:28,320 --> 00:11:31,839
they can be alphanumeric strings they

270
00:11:30,720 --> 00:11:34,959
can be numbers

271
00:11:31,839 --> 00:11:37,760
they can be any sort of character group

272
00:11:34,959 --> 00:11:39,040
that you define that is interesting to

273
00:11:37,760 --> 00:11:42,399
you for your own

274
00:11:39,040 --> 00:11:45,360
purposes so breaking up the text into

275
00:11:42,399 --> 00:11:48,240
tokens and classes of tokens is parsing

276
00:11:45,360 --> 00:11:48,880
and we're fortunate enough uh as a

277
00:11:48,240 --> 00:11:52,000
result

278
00:11:48,880 --> 00:11:54,880
of uh serious effort

279
00:11:52,000 --> 00:11:56,959
and time that has been spent we do have

280
00:11:54,880 --> 00:12:00,160
a parser that is very good

281
00:11:56,959 --> 00:12:01,839
that comes for free with postgres or

282
00:12:00,160 --> 00:12:03,199
if you have different requirements you

283
00:12:01,839 --> 00:12:07,040
can write your own parser

284
00:12:03,200 --> 00:12:07,040
as long as you can write it in c

285
00:12:08,839 --> 00:12:14,720
now in order to do anything

286
00:12:11,519 --> 00:12:17,760
useful with our data set

287
00:12:14,720 --> 00:12:18,720
we need to convert the tokens from these

288
00:12:17,760 --> 00:12:22,160
documents

289
00:12:18,720 --> 00:12:24,320
into lexemes

290
00:12:22,160 --> 00:12:26,240
therefore we have to perform what is

291
00:12:24,320 --> 00:12:29,839
called the normalization

292
00:12:26,240 --> 00:12:32,240
of our strings we have to turn them into

293
00:12:29,839 --> 00:12:35,279
some more usable form

294
00:12:32,240 --> 00:12:36,240
from their original form and what is a

295
00:12:35,279 --> 00:12:40,959
lexing

296
00:12:36,240 --> 00:12:44,639
it's an abstract lexical unit

297
00:12:40,959 --> 00:12:46,079
that is representing the set of related

298
00:12:44,639 --> 00:12:49,120
words

299
00:12:46,079 --> 00:12:52,239
so you can call it the word root

300
00:12:49,120 --> 00:12:55,519
but a word root only applies for

301
00:12:52,240 --> 00:12:59,200
human language so a lexine

302
00:12:55,519 --> 00:13:04,240
uh in the context of natural language

303
00:12:59,200 --> 00:13:08,240
is the word root therefore

304
00:13:04,240 --> 00:13:12,240
the lexeme search excuse me

305
00:13:08,240 --> 00:13:15,600
the lexeme search can stand for

306
00:13:12,240 --> 00:13:18,839
the words searched and searcher because

307
00:13:15,600 --> 00:13:21,360
you can reduce these words down to that

308
00:13:18,839 --> 00:13:24,560
lexi

309
00:13:21,360 --> 00:13:27,760
what are stop words that we mentioned

310
00:13:24,560 --> 00:13:28,800
stop words are very common words that

311
00:13:27,760 --> 00:13:31,360
appear in our text

312
00:13:28,800 --> 00:13:32,399
therefore they have no value like in

313
00:13:31,360 --> 00:13:35,839
english

314
00:13:32,399 --> 00:13:38,560
articles like z and a

315
00:13:35,839 --> 00:13:39,839
uh do not have meaning for our search

316
00:13:38,560 --> 00:13:44,000
most of the time

317
00:13:39,839 --> 00:13:46,480
so we filter them out and by filtering

318
00:13:44,000 --> 00:13:47,519
out stop words that increases the

319
00:13:46,480 --> 00:13:49,600
precision

320
00:13:47,519 --> 00:13:53,199
of our search right because we're

321
00:13:49,600 --> 00:13:53,199
getting more relevant results

322
00:13:54,399 --> 00:13:56,880
it does

323
00:13:59,279 --> 00:14:04,480
and how do we remove them we remove the

324
00:14:02,320 --> 00:14:08,639
stop words

325
00:14:04,480 --> 00:14:08,639
based on dictionaries

326
00:14:09,120 --> 00:14:16,320
some dictionaries have stop lists

327
00:14:12,959 --> 00:14:18,959
and these are words that

328
00:14:16,320 --> 00:14:20,720
are checked and removed and eliminated

329
00:14:18,959 --> 00:14:22,560
from

330
00:14:20,720 --> 00:14:24,000
the document before we can do anything

331
00:14:22,560 --> 00:14:27,518
with it

332
00:14:24,000 --> 00:14:29,360
but if you do remove stop words

333
00:14:27,519 --> 00:14:32,160
you have to consider one fact what

334
00:14:29,360 --> 00:14:33,680
happens to phrase search

335
00:14:32,160 --> 00:14:35,920
if you are looking for something that

336
00:14:33,680 --> 00:14:38,399
includes articles for instance or very

337
00:14:35,920 --> 00:14:41,599
common words

338
00:14:38,399 --> 00:14:42,839
as a phrase then you have to consider

339
00:14:41,600 --> 00:14:47,440
other

340
00:14:42,839 --> 00:14:50,959
things let's talk about stemming

341
00:14:47,440 --> 00:14:54,079
and stemming is reducing as we said the

342
00:14:50,959 --> 00:14:54,079
words to lexemes

343
00:14:54,880 --> 00:14:59,600
and it does increase the number of the

344
00:14:58,000 --> 00:15:03,600
results so it does increase

345
00:14:59,600 --> 00:15:07,279
recall and if we

346
00:15:03,600 --> 00:15:09,040
have um something that matches multiple

347
00:15:07,279 --> 00:15:11,120
words we're going to get more results it

348
00:15:09,040 --> 00:15:13,839
goes without saying

349
00:15:11,120 --> 00:15:14,720
so the algorithms that we can use for

350
00:15:13,839 --> 00:15:18,000
stemming

351
00:15:14,720 --> 00:15:19,279
are as we said normalization using

352
00:15:18,000 --> 00:15:21,440
dictionaries

353
00:15:19,279 --> 00:15:22,959
where we can find out what the word

354
00:15:21,440 --> 00:15:26,959
stems are

355
00:15:22,959 --> 00:15:28,560
we can strip prefixes and suffixes

356
00:15:26,959 --> 00:15:30,560
that we know exist in a specific

357
00:15:28,560 --> 00:15:32,638
language and

358
00:15:30,560 --> 00:15:33,758
break down the words to its constituent

359
00:15:32,639 --> 00:15:36,800
parts

360
00:15:33,759 --> 00:15:40,320
we can also uh

361
00:15:36,800 --> 00:15:43,599
use automatic production so

362
00:15:40,320 --> 00:15:46,720
maybe if you find the word stem by

363
00:15:43,600 --> 00:15:50,079
attempting to add ing to it

364
00:15:46,720 --> 00:15:54,959
you are creating a possible result by

365
00:15:50,079 --> 00:15:59,439
producing a new word based on the stem

366
00:15:54,959 --> 00:16:03,119
we have also limitization rules

367
00:15:59,440 --> 00:16:05,120
that define how words are stemmed in

368
00:16:03,120 --> 00:16:07,680
each language and we also have

369
00:16:05,120 --> 00:16:08,639
n-gram models which are probabilistic

370
00:16:07,680 --> 00:16:14,399
models

371
00:16:08,639 --> 00:16:14,399
that show how you can reduce words

372
00:16:15,920 --> 00:16:21,599
but we mentioned languages here

373
00:16:18,959 --> 00:16:22,079
and it is hard to do this across

374
00:16:21,600 --> 00:16:26,639
multiple

375
00:16:22,079 --> 00:16:26,638
languages at the same time

376
00:16:29,279 --> 00:16:33,199
so how is

377
00:16:30,790 --> 00:16:35,519
[Music]

378
00:16:33,199 --> 00:16:36,319
full text search represented in postgres

379
00:16:35,519 --> 00:16:40,320
we have

380
00:16:36,320 --> 00:16:43,360
a data type that's called ts vector

381
00:16:40,320 --> 00:16:47,040
which represents a document

382
00:16:43,360 --> 00:16:50,160
stripped down to its essential form

383
00:16:47,040 --> 00:16:52,480
it's a pre-processed document

384
00:16:50,160 --> 00:16:53,759
stored as a separate data type in

385
00:16:52,480 --> 00:16:57,040
postgres

386
00:16:53,759 --> 00:17:01,120
and then we have ts query which

387
00:16:57,040 --> 00:17:04,240
is our search query our predicate

388
00:17:01,120 --> 00:17:07,039
and it's normalized into lex seams so

389
00:17:04,240 --> 00:17:08,400
before you can use postgres's full text

390
00:17:07,039 --> 00:17:10,000
search features

391
00:17:08,400 --> 00:17:12,400
you have to convert everything to one of

392
00:17:10,000 --> 00:17:14,480
these types

393
00:17:12,400 --> 00:17:15,679
and these come with utility functions

394
00:17:14,480 --> 00:17:19,919
like two

395
00:17:15,679 --> 00:17:23,520
ts vector that turns

396
00:17:19,919 --> 00:17:24,799
your document into the ts vector form by

397
00:17:23,520 --> 00:17:28,879
normalizing it

398
00:17:24,799 --> 00:17:32,240
and it also we also have two ts query

399
00:17:28,880 --> 00:17:34,480
that expects specific ts query syntax

400
00:17:32,240 --> 00:17:36,400
that you have to know or you can just

401
00:17:34,480 --> 00:17:38,080
use plain to ts query

402
00:17:36,400 --> 00:17:39,440
if you have some text that you want to

403
00:17:38,080 --> 00:17:43,600
turn into a ps query

404
00:17:39,440 --> 00:17:46,960
and that ignores uh capitalization

405
00:17:43,600 --> 00:17:50,799
and punctuation points and all that

406
00:17:46,960 --> 00:17:53,600
if you want to find out how

407
00:17:50,799 --> 00:17:55,360
these things happen under the hood you

408
00:17:53,600 --> 00:17:57,600
can use ts debug

409
00:17:55,360 --> 00:17:58,399
and that will show you how everything is

410
00:17:57,600 --> 00:18:00,559
converted

411
00:17:58,400 --> 00:18:01,440
and what token type it's been converted

412
00:18:00,559 --> 00:18:05,120
into

413
00:18:01,440 --> 00:18:05,120
by the dictionary that you're using

414
00:18:06,000 --> 00:18:09,600
so the operator types that we have for

415
00:18:08,320 --> 00:18:13,039
full text search

416
00:18:09,600 --> 00:18:15,918
are the double at sign which

417
00:18:13,039 --> 00:18:16,720
means ts vector matches ts query on the

418
00:18:15,919 --> 00:18:18,880
other side

419
00:18:16,720 --> 00:18:21,280
and you can also turn it around and you

420
00:18:18,880 --> 00:18:24,080
can make it ts query matches ts vector

421
00:18:21,280 --> 00:18:25,760
it works both ways we have ts vector

422
00:18:24,080 --> 00:18:28,559
concatenation

423
00:18:25,760 --> 00:18:29,280
and for ts queries we have these

424
00:18:28,559 --> 00:18:32,559
operators

425
00:18:29,280 --> 00:18:33,840
and or and not that help you formulate

426
00:18:32,559 --> 00:18:36,799
your query

427
00:18:33,840 --> 00:18:38,080
we also have the followed by operator

428
00:18:36,799 --> 00:18:41,520
which means that

429
00:18:38,080 --> 00:18:42,000
ts query 1 must be followed by ts query

430
00:18:41,520 --> 00:18:44,879
2

431
00:18:42,000 --> 00:18:45,440
and that's a way to define order in your

432
00:18:44,880 --> 00:18:49,600
search

433
00:18:45,440 --> 00:18:53,039
word order we also have contains

434
00:18:49,600 --> 00:18:55,678
and contained in so

435
00:18:53,039 --> 00:18:58,240
we mentioned dictionaries multiple times

436
00:18:55,679 --> 00:19:01,840
what are dictionaries in postgres

437
00:18:58,240 --> 00:19:06,400
their programs they accept tokens as

438
00:19:01,840 --> 00:19:08,000
input and their output is the normalized

439
00:19:06,400 --> 00:19:09,760
text that we need

440
00:19:08,000 --> 00:19:11,919
they improve the search quality as we

441
00:19:09,760 --> 00:19:12,799
said by eliminating stop words and

442
00:19:11,919 --> 00:19:15,919
normalizing

443
00:19:12,799 --> 00:19:20,720
into lexemes they reduce

444
00:19:15,919 --> 00:19:24,320
the size of your ts vector which is good

445
00:19:20,720 --> 00:19:26,320
and you can use

446
00:19:24,320 --> 00:19:28,639
search dictionaries by create text

447
00:19:26,320 --> 00:19:30,879
search dictionary name

448
00:19:28,640 --> 00:19:31,919
you can choose one of the many available

449
00:19:30,880 --> 00:19:33,679
templates

450
00:19:31,919 --> 00:19:36,160
simple doesn't attempt to do anything

451
00:19:33,679 --> 00:19:37,600
smart it just treats it just separates

452
00:19:36,160 --> 00:19:40,799
words by

453
00:19:37,600 --> 00:19:41,840
white space which is useful if you want

454
00:19:40,799 --> 00:19:44,080
to match

455
00:19:41,840 --> 00:19:46,720
like proper names that are not words in

456
00:19:44,080 --> 00:19:46,720
any language

457
00:19:46,880 --> 00:19:51,440
and you can also define the set of stop

458
00:19:50,320 --> 00:19:55,360
words

459
00:19:51,440 --> 00:19:55,360
for your text search configuration

460
00:19:55,440 --> 00:20:01,840
you can also decide to change those

461
00:19:59,600 --> 00:20:04,000
in the order which is convenient for you

462
00:20:01,840 --> 00:20:07,918
so you can have it

463
00:20:04,000 --> 00:20:11,120
first perform the

464
00:20:07,919 --> 00:20:13,600
dictionary normalization using i spell

465
00:20:11,120 --> 00:20:15,280
and then the simple one and you can

466
00:20:13,600 --> 00:20:17,520
assign different weights

467
00:20:15,280 --> 00:20:18,840
to each one of those so you can tell

468
00:20:17,520 --> 00:20:21,520
apart

469
00:20:18,840 --> 00:20:23,360
the proper names let's say that were

470
00:20:21,520 --> 00:20:25,520
kept by simple

471
00:20:23,360 --> 00:20:27,360
but were eliminated by english i spell

472
00:20:25,520 --> 00:20:30,480
and you can assign different weights

473
00:20:27,360 --> 00:20:30,479
to them in your results

474
00:20:31,039 --> 00:20:36,400
and these come with your uh system

475
00:20:34,480 --> 00:20:37,600
because we can use the open source i

476
00:20:36,400 --> 00:20:40,240
spell my spell

477
00:20:37,600 --> 00:20:44,480
and han spell dictionaries for this uh

478
00:20:40,240 --> 00:20:44,480
on top of the included ones in postgres

479
00:20:44,559 --> 00:20:51,120
so let's do some

480
00:20:48,000 --> 00:20:54,400
text matching so select

481
00:20:51,120 --> 00:20:56,959
to ts vector a nice day for a car ride

482
00:20:54,400 --> 00:21:00,720
is our document

483
00:20:56,960 --> 00:21:04,720
and our query is i am riding

484
00:21:00,720 --> 00:21:07,760
does it match yes

485
00:21:04,720 --> 00:21:12,559
it matches according to our

486
00:21:07,760 --> 00:21:15,120
english dictionary rules but why

487
00:21:12,559 --> 00:21:16,399
i want to find out what two ts vector

488
00:21:15,120 --> 00:21:20,320
generated

489
00:21:16,400 --> 00:21:24,080
that made it match so 2ts vector

490
00:21:20,320 --> 00:21:26,960
generated these lex seams car day

491
00:21:24,080 --> 00:21:26,960
nice ride

492
00:21:27,760 --> 00:21:34,559
and 2ts query

493
00:21:31,200 --> 00:21:37,679
reduced everything down to right

494
00:21:34,559 --> 00:21:40,399
so yes it matches

495
00:21:37,679 --> 00:21:40,400
it's one of those

496
00:21:42,000 --> 00:21:48,720
let's see something that doesn't match

497
00:21:45,600 --> 00:21:52,080
so if i select to

498
00:21:48,720 --> 00:21:54,320
ts vector a nice day for a car ride and

499
00:21:52,080 --> 00:21:58,320
my ts query is i am riding a bike

500
00:21:54,320 --> 00:22:01,360
does that match no it doesn't match

501
00:21:58,320 --> 00:22:06,158
because my

502
00:22:01,360 --> 00:22:08,399
plane to ts query converted my query to

503
00:22:06,159 --> 00:22:10,799
it stripped it down to the two essential

504
00:22:08,400 --> 00:22:13,440
words ride and bike

505
00:22:10,799 --> 00:22:15,360
and bike isn't included so therefore the

506
00:22:13,440 --> 00:22:18,240
document isn't relevant to our search

507
00:22:15,360 --> 00:22:18,240
it's not returned

508
00:22:21,120 --> 00:22:24,840
another example we can look at for

509
00:22:23,520 --> 00:22:27,600
matching

510
00:22:24,840 --> 00:22:31,120
is starman and star

511
00:22:27,600 --> 00:22:33,120
they do not match because they don't

512
00:22:31,120 --> 00:22:36,399
according to our dictionary always they

513
00:22:33,120 --> 00:22:38,639
don't belong to the same word root

514
00:22:36,400 --> 00:22:40,000
but i can cheat and i can say i'm

515
00:22:38,640 --> 00:22:45,280
interested in all words

516
00:22:40,000 --> 00:22:45,280
that begin with star so i can say

517
00:22:46,720 --> 00:22:54,240
2ts query star

518
00:22:49,840 --> 00:22:57,760
followed by an asterisk

519
00:22:54,240 --> 00:22:58,240
and that matches because that makes it

520
00:22:57,760 --> 00:23:01,440
accept

521
00:22:58,240 --> 00:23:04,240
everything that begins with star

522
00:23:01,440 --> 00:23:06,000
now you will notice that i am not using

523
00:23:04,240 --> 00:23:08,880
two

524
00:23:06,000 --> 00:23:10,799
ts vector and 2ts query here because

525
00:23:08,880 --> 00:23:12,480
postgres does it automatically for you

526
00:23:10,799 --> 00:23:15,120
sometimes

527
00:23:12,480 --> 00:23:16,240
when it is able to it will cast these to

528
00:23:15,120 --> 00:23:19,039
the proper types

529
00:23:16,240 --> 00:23:19,039
automatically

530
00:23:21,120 --> 00:23:24,479
we also have the wonderful function web

531
00:23:23,039 --> 00:23:28,080
search to ts query

532
00:23:24,480 --> 00:23:33,200
that attempts a google style query

533
00:23:28,080 --> 00:23:36,399
and you can use things like and or

534
00:23:33,200 --> 00:23:36,960
and you can use like double quotation

535
00:23:36,400 --> 00:23:39,919
marks

536
00:23:36,960 --> 00:23:40,880
to mark phrases and you can also use

537
00:23:39,919 --> 00:23:43,440
minus

538
00:23:40,880 --> 00:23:44,240
signs to remove the results that you

539
00:23:43,440 --> 00:23:47,360
don't want

540
00:23:44,240 --> 00:23:51,919
from your like a web search engine so

541
00:23:47,360 --> 00:23:55,520
this makes our ts query become

542
00:23:51,919 --> 00:23:55,520
stray followed by cat

543
00:23:55,679 --> 00:23:59,840
and it must not have cat followed by

544
00:23:59,279 --> 00:24:01,279
shelter

545
00:23:59,840 --> 00:24:03,360
because i'm only interested in the stray

546
00:24:01,279 --> 00:24:06,960
cats but not cat shelters

547
00:24:03,360 --> 00:24:06,959
for the purposes of our search of course

548
00:24:07,520 --> 00:24:18,000
let's look at an example table of

549
00:24:12,000 --> 00:24:22,559
it's immediately followed by our

550
00:24:18,000 --> 00:24:22,559
immediately followed by if you want

551
00:24:23,039 --> 00:24:26,320
sorry the question was does it mean

552
00:24:25,039 --> 00:24:28,400
followed by or does it mean

553
00:24:26,320 --> 00:24:30,720
immediately after immediately followed

554
00:24:28,400 --> 00:24:31,679
by this operator means immediately

555
00:24:30,720 --> 00:24:34,799
followed by

556
00:24:31,679 --> 00:24:35,840
but if instead of the minus sign in the

557
00:24:34,799 --> 00:24:38,400
middle of the operator

558
00:24:35,840 --> 00:24:40,320
if you insert a two or a three that

559
00:24:38,400 --> 00:24:41,440
means two tokens away or three tokens

560
00:24:40,320 --> 00:24:43,120
away

561
00:24:41,440 --> 00:24:45,360
so you can define the distance that you

562
00:24:43,120 --> 00:24:48,799
want

563
00:24:45,360 --> 00:24:49,840
so let's use an example table of the

564
00:24:48,799 --> 00:24:53,600
contents

565
00:24:49,840 --> 00:24:57,360
of pgsql hackers the mailing list

566
00:24:53,600 --> 00:25:00,399
and the contents have um

567
00:24:57,360 --> 00:25:03,840
an idea parent id which

568
00:25:00,400 --> 00:25:06,960
email you replied to sent time

569
00:25:03,840 --> 00:25:08,320
timestamp what we're interested in here

570
00:25:06,960 --> 00:25:11,600
is the subject

571
00:25:08,320 --> 00:25:15,439
the author and the body of the email

572
00:25:11,600 --> 00:25:20,240
in plain text so

573
00:25:15,440 --> 00:25:20,240
how big is this this is 478 megabytes

574
00:25:27,200 --> 00:25:31,440
we also mentioned how relevant our

575
00:25:30,320 --> 00:25:35,120
results are

576
00:25:31,440 --> 00:25:38,880
and ranking them so for ranking we have

577
00:25:35,120 --> 00:25:42,158
ts rank and its

578
00:25:38,880 --> 00:25:43,919
cover density variant ts rank cd

579
00:25:42,159 --> 00:25:46,000
cover density just takes into account

580
00:25:43,919 --> 00:25:49,440
how close your terms are

581
00:25:46,000 --> 00:25:51,600
in the text and it bumps up

582
00:25:49,440 --> 00:25:53,840
the relevance of things that are closer

583
00:25:51,600 --> 00:25:57,120
together

584
00:25:53,840 --> 00:26:00,720
but anyway let's use ts rank for now

585
00:25:57,120 --> 00:26:03,918
so from our example table if i select

586
00:26:00,720 --> 00:26:07,919
the subject and the ts

587
00:26:03,919 --> 00:26:11,440
rank of our body

588
00:26:07,919 --> 00:26:12,640
converted to our email body converted to

589
00:26:11,440 --> 00:26:14,880
ts vector

590
00:26:12,640 --> 00:26:16,960
and you'll notice that i'm putting core

591
00:26:14,880 --> 00:26:19,919
less in here

592
00:26:16,960 --> 00:26:22,159
so that i can have meaningful results if

593
00:26:19,919 --> 00:26:25,840
i have a null

594
00:26:22,159 --> 00:26:25,840
text body in my email

595
00:26:26,400 --> 00:26:31,840
i am trying to match this to

596
00:26:30,000 --> 00:26:33,600
aggregate the word aggregate so i'm

597
00:26:31,840 --> 00:26:36,000
trying to find the

598
00:26:33,600 --> 00:26:36,959
top five subjects because i'm doing an

599
00:26:36,000 --> 00:26:39,200
order by

600
00:26:36,960 --> 00:26:40,720
descending limit five i'm trying to find

601
00:26:39,200 --> 00:26:43,440
the top five

602
00:26:40,720 --> 00:26:45,840
subjects of emails that contain the word

603
00:26:43,440 --> 00:26:45,840
aggregate

604
00:26:46,400 --> 00:26:54,080
and when i run it it produces a rank

605
00:26:50,799 --> 00:26:58,080
and it tells me that these were the most

606
00:26:54,080 --> 00:26:59,840
relevant subjects uh excuse me these

607
00:26:58,080 --> 00:27:02,639
were these were emails

608
00:26:59,840 --> 00:27:03,439
that in their body were most relevant to

609
00:27:02,640 --> 00:27:05,360
the word

610
00:27:03,440 --> 00:27:06,640
aggregate i know that's not a very

611
00:27:05,360 --> 00:27:09,520
useful search

612
00:27:06,640 --> 00:27:11,120
because it's going to show up a lot but

613
00:27:09,520 --> 00:27:14,720
um

614
00:27:11,120 --> 00:27:14,719
these are the subjects of the emails

615
00:27:15,120 --> 00:27:23,039
sorry 32 is uh the scaling of the uh

616
00:27:18,640 --> 00:27:26,399
result so you it's a bit mask

617
00:27:23,039 --> 00:27:29,520
that defines how the weights are applied

618
00:27:26,399 --> 00:27:30,799
to your ranking algorithm these are all

619
00:27:29,520 --> 00:27:32,960
documented as you know

620
00:27:30,799 --> 00:27:34,799
postgres or you may not know postgres

621
00:27:32,960 --> 00:27:36,960
has excellent documentation on the web

622
00:27:34,799 --> 00:27:40,320
and there are going to be links to that

623
00:27:36,960 --> 00:27:40,320
at the end of the presentation

624
00:27:42,080 --> 00:27:46,720
let's look at some statistics we have

625
00:27:44,559 --> 00:27:49,678
the function tstat

626
00:27:46,720 --> 00:27:51,760
that can be used for verifying that our

627
00:27:49,679 --> 00:27:55,039
configuration for text search

628
00:27:51,760 --> 00:27:59,039
and dictionaries and everything else

629
00:27:55,039 --> 00:28:01,200
works fine for our own purposes so

630
00:27:59,039 --> 00:28:03,039
if we select star from the function

631
00:28:01,200 --> 00:28:06,720
tstat

632
00:28:03,039 --> 00:28:09,919
and we pass it a ts vector

633
00:28:06,720 --> 00:28:09,919
because that's what it needs

634
00:28:10,960 --> 00:28:16,640
i will attempt to find the

635
00:28:14,320 --> 00:28:19,600
number of documents the number of words

636
00:28:16,640 --> 00:28:23,760
etc and it returns

637
00:28:19,600 --> 00:28:27,120
the uh according to my configuration

638
00:28:23,760 --> 00:28:30,158
the top words that it found

639
00:28:27,120 --> 00:28:33,600
in the in the documents so

640
00:28:30,159 --> 00:28:36,159
the most common word was use wrote

641
00:28:33,600 --> 00:28:38,000
would think patch and so on which is

642
00:28:36,159 --> 00:28:44,320
something that you might expect from

643
00:28:38,000 --> 00:28:47,760
the pgsql hackers mailing list

644
00:28:44,320 --> 00:28:51,918
now we mentioned indexing

645
00:28:47,760 --> 00:28:52,960
and we have to recognize that the normal

646
00:28:51,919 --> 00:28:56,320
default

647
00:28:52,960 --> 00:28:59,520
of b3 which is our index type in

648
00:28:56,320 --> 00:29:02,000
postgres or default

649
00:28:59,520 --> 00:29:03,440
isn't all that suitable for full text

650
00:29:02,000 --> 00:29:06,320
search

651
00:29:03,440 --> 00:29:07,440
you can use it for a limited sort of

652
00:29:06,320 --> 00:29:10,639
text matching

653
00:29:07,440 --> 00:29:13,919
and searching if you create

654
00:29:10,640 --> 00:29:18,480
the index specifying varchar

655
00:29:13,919 --> 00:29:18,480
pattern knobs then you can match

656
00:29:18,559 --> 00:29:22,879
left anchored text or if you reverse the

657
00:29:21,360 --> 00:29:25,520
string in the index

658
00:29:22,880 --> 00:29:26,399
then in the index expression then you

659
00:29:25,520 --> 00:29:30,559
can search

660
00:29:26,399 --> 00:29:30,559
from the end of the string forwards

661
00:29:31,120 --> 00:29:36,959
it is going to be less

662
00:29:34,320 --> 00:29:38,720
useful than full text search but it is

663
00:29:36,960 --> 00:29:40,720
one way of doing it

664
00:29:38,720 --> 00:29:43,039
for full text search we have the gin

665
00:29:40,720 --> 00:29:43,039
index

666
00:29:44,159 --> 00:29:47,840
which is an inverted index which means

667
00:29:46,480 --> 00:29:52,799
that

668
00:29:47,840 --> 00:29:52,799
each lexing has one entry in the index

669
00:29:53,440 --> 00:29:58,159
it is quite large and it's slow to

670
00:29:56,640 --> 00:30:01,440
update

671
00:29:58,159 --> 00:30:04,640
so it is better used on

672
00:30:01,440 --> 00:30:04,640
less dynamic data

673
00:30:04,880 --> 00:30:12,799
and you apply it on ts vector columns

674
00:30:08,960 --> 00:30:16,960
we also have gist the generalized

675
00:30:12,799 --> 00:30:19,360
search tree index that is lossy

676
00:30:16,960 --> 00:30:20,480
it is going to be smaller on the same

677
00:30:19,360 --> 00:30:23,279
data set

678
00:30:20,480 --> 00:30:24,640
but it is also going to be slower

679
00:30:23,279 --> 00:30:27,760
because as a lossy

680
00:30:24,640 --> 00:30:29,520
index it produces false positives and in

681
00:30:27,760 --> 00:30:31,600
order to eliminate those false positives

682
00:30:29,520 --> 00:30:33,918
postgres has to go back

683
00:30:31,600 --> 00:30:34,719
to the row and actually determine

684
00:30:33,919 --> 00:30:37,120
whether the row

685
00:30:34,720 --> 00:30:38,799
is a useful result or not so it is a bit

686
00:30:37,120 --> 00:30:42,000
slower

687
00:30:38,799 --> 00:30:45,200
it's so it's better suited for fewer

688
00:30:42,000 --> 00:30:48,559
unique items if you have

689
00:30:45,200 --> 00:30:49,760
let's say a limited number of things

690
00:30:48,559 --> 00:30:52,639
that you're searching for

691
00:30:49,760 --> 00:30:54,480
then just might be better index for you

692
00:30:52,640 --> 00:30:57,679
and you can apply it on ts vector

693
00:30:54,480 --> 00:30:57,679
or tf query columns

694
00:30:57,919 --> 00:31:03,519
now let's see what unindexed full text

695
00:31:01,440 --> 00:31:05,600
search looks like

696
00:31:03,519 --> 00:31:07,279
i will do an explain analyze and that

697
00:31:05,600 --> 00:31:09,199
will show me how my query is being

698
00:31:07,279 --> 00:31:11,919
executed

699
00:31:09,200 --> 00:31:13,679
and my query is going to be select count

700
00:31:11,919 --> 00:31:16,080
from mail messages

701
00:31:13,679 --> 00:31:17,360
so find me all the text messages all the

702
00:31:16,080 --> 00:31:22,720
emails

703
00:31:17,360 --> 00:31:22,719
where that contain the word aggregate

704
00:31:23,919 --> 00:31:29,440
when i run it it will do a parallel

705
00:31:28,159 --> 00:31:33,039
sequential scan

706
00:31:29,440 --> 00:31:35,600
because in recent versions of postgres

707
00:31:33,039 --> 00:31:36,240
you can have parallel scans of tables

708
00:31:35,600 --> 00:31:39,279
and

709
00:31:36,240 --> 00:31:42,000
specifically in version 12 you have

710
00:31:39,279 --> 00:31:44,559
the just in time compiler enabled which

711
00:31:42,000 --> 00:31:47,360
also speeds up things

712
00:31:44,559 --> 00:31:49,200
that's why i've highlighted it here the

713
00:31:47,360 --> 00:31:54,158
execution time is horrible

714
00:31:49,200 --> 00:31:57,600
it took around 26 27 seconds almost

715
00:31:54,159 --> 00:31:59,039
to look through these 400 megabytes 485

716
00:31:57,600 --> 00:32:02,559
megabytes of emails

717
00:31:59,039 --> 00:32:07,120
and determine the ones that contain

718
00:32:02,559 --> 00:32:08,799
aggregate indexed

719
00:32:07,120 --> 00:32:11,439
all we have to do is create an index and

720
00:32:08,799 --> 00:32:14,240
mail messages using gin

721
00:32:11,440 --> 00:32:16,559
that's the syntax for specifying the gin

722
00:32:14,240 --> 00:32:21,519
index type

723
00:32:16,559 --> 00:32:24,639
and then we convert our

724
00:32:21,519 --> 00:32:27,679
subject concatenated with

725
00:32:24,640 --> 00:32:29,120
our body you can do that i haven't done

726
00:32:27,679 --> 00:32:30,799
it in the examples that i'm actually

727
00:32:29,120 --> 00:32:33,360
running but that is a possibility

728
00:32:30,799 --> 00:32:34,720
you could want to index the subject as

729
00:32:33,360 --> 00:32:37,678
well as the body

730
00:32:34,720 --> 00:32:39,600
so because people will often use words

731
00:32:37,679 --> 00:32:41,039
in the subject that they don't repeat in

732
00:32:39,600 --> 00:32:42,799
the body of the text

733
00:32:41,039 --> 00:32:44,960
so that is a possibility you can index

734
00:32:42,799 --> 00:32:47,279
both of them as a ts vector

735
00:32:44,960 --> 00:32:49,519
using the english the english language

736
00:32:47,279 --> 00:32:52,799
rules

737
00:32:49,519 --> 00:32:56,159
but you can also in postgres 12

738
00:32:52,799 --> 00:32:58,799
you can use a generated column and

739
00:32:56,159 --> 00:33:00,799
because you can convert everything into

740
00:32:58,799 --> 00:33:04,158
a separate ts vector column

741
00:33:00,799 --> 00:33:06,720
instead of doing it on the fly like this

742
00:33:04,159 --> 00:33:08,240
you can alter the table and add a column

743
00:33:06,720 --> 00:33:11,360
of type ts

744
00:33:08,240 --> 00:33:11,919
of type t as vector and the new syntax

745
00:33:11,360 --> 00:33:16,000
is

746
00:33:11,919 --> 00:33:18,880
generated always as stored

747
00:33:16,000 --> 00:33:20,880
for now postgres only supports stored

748
00:33:18,880 --> 00:33:21,679
generated columns that means they stay

749
00:33:20,880 --> 00:33:24,880
on your disk

750
00:33:21,679 --> 00:33:26,000
they're not generated on the fly so as

751
00:33:24,880 --> 00:33:29,039
ts vector

752
00:33:26,000 --> 00:33:32,720
core less to get rid of nulls

753
00:33:29,039 --> 00:33:35,760
so coreless this with a blank space

754
00:33:32,720 --> 00:33:39,679
and the body and that will keep

755
00:33:35,760 --> 00:33:40,640
your vector column updated all the time

756
00:33:39,679 --> 00:33:42,799
even though

757
00:33:40,640 --> 00:33:44,399
you may be only updating the emails

758
00:33:42,799 --> 00:33:47,039
column

759
00:33:44,399 --> 00:33:47,600
sorry and then you just create an index

760
00:33:47,039 --> 00:33:53,200
on

761
00:33:47,600 --> 00:33:56,080
that generated column

762
00:33:53,200 --> 00:33:57,120
now let's look at gist does it help us

763
00:33:56,080 --> 00:33:58,559
for our search

764
00:33:57,120 --> 00:34:02,320
it's the exact same search we're

765
00:33:58,559 --> 00:34:02,320
searching for the word aggregate

766
00:34:03,360 --> 00:34:07,918
it did help we can see that and bitmap

767
00:34:06,320 --> 00:34:11,279
index scan was used

768
00:34:07,919 --> 00:34:13,918
and our search was much faster

769
00:34:11,280 --> 00:34:16,079
but you will notice that we have rows

770
00:34:13,918 --> 00:34:18,399
removed by index recheck

771
00:34:16,079 --> 00:34:20,480
so that means that it went it had to go

772
00:34:18,399 --> 00:34:23,199
back and figure out whether these

773
00:34:20,480 --> 00:34:25,760
results were useful or not

774
00:34:23,199 --> 00:34:27,040
so it is a bit slower than it could be

775
00:34:25,760 --> 00:34:29,280
it's not optimal

776
00:34:27,040 --> 00:34:31,359
for this sort of search the execution

777
00:34:29,280 --> 00:34:34,480
time was only 5.6

778
00:34:31,359 --> 00:34:40,239
seconds so about

779
00:34:34,480 --> 00:34:40,240
4.8 times faster than not using an index

780
00:34:40,960 --> 00:34:46,480
it's still slow because it's not it's

781
00:34:43,280 --> 00:34:48,480
because it's not the suitable search

782
00:34:46,480 --> 00:34:50,719
it's not a suitable index type for this

783
00:34:48,480 --> 00:34:53,918
search

784
00:34:50,719 --> 00:34:57,279
let's use gin instead

785
00:34:53,918 --> 00:34:57,279
so explain analyze again

786
00:34:57,599 --> 00:35:04,079
5.6 wow milliseconds

787
00:35:00,880 --> 00:35:06,079
not seconds here so

788
00:35:04,079 --> 00:35:07,599
we're seeing a significant difference

789
00:35:06,079 --> 00:35:10,640
here it uses

790
00:35:07,599 --> 00:35:15,280
a bitmap index scan of our

791
00:35:10,640 --> 00:35:18,480
index column and it's approximately

792
00:35:15,280 --> 00:35:19,200
4 700 times faster than not using an

793
00:35:18,480 --> 00:35:21,040
index

794
00:35:19,200 --> 00:35:22,799
so you know you're doing something right

795
00:35:21,040 --> 00:35:25,680
when you're getting things that are less

796
00:35:22,800 --> 00:35:25,680
than a millisecond

797
00:35:29,119 --> 00:35:37,520
um less than a second excuse me um so

798
00:35:34,320 --> 00:35:39,680
gin and just indexed operations we have

799
00:35:37,520 --> 00:35:43,200
these operators

800
00:35:39,680 --> 00:35:45,279
for ts vector gen indexes the

801
00:35:43,200 --> 00:35:47,200
double add sign operator so it indexes

802
00:35:45,280 --> 00:35:50,400
matching as we saw

803
00:35:47,200 --> 00:35:53,598
for json b or binary stored

804
00:35:50,400 --> 00:35:56,480
json in our database it

805
00:35:53,599 --> 00:35:58,640
supports the existence operators and

806
00:35:56,480 --> 00:36:01,520
also the contains operator

807
00:35:58,640 --> 00:36:02,400
so these are operations that will be

808
00:36:01,520 --> 00:36:04,800
very fast

809
00:36:02,400 --> 00:36:07,280
if you've indexed your json b column

810
00:36:04,800 --> 00:36:07,280
with gin

811
00:36:07,920 --> 00:36:14,560
and gist supports

812
00:36:11,200 --> 00:36:17,598
again the at sign

813
00:36:14,560 --> 00:36:19,440
for text matching but it also supports

814
00:36:17,599 --> 00:36:22,720
the containment operators

815
00:36:19,440 --> 00:36:26,640
for ts query whether something contains

816
00:36:22,720 --> 00:36:30,399
the other

817
00:36:26,640 --> 00:36:33,839
excuse me some super useful

818
00:36:30,400 --> 00:36:36,960
modules that come with postgres

819
00:36:33,839 --> 00:36:40,400
are pg trigram we'll look at that

820
00:36:36,960 --> 00:36:43,520
in the next few slides

821
00:36:40,400 --> 00:36:46,880
on the accent that removes accidents

822
00:36:43,520 --> 00:36:49,359
accents and diacritics from your text

823
00:36:46,880 --> 00:36:50,320
so that you can match a u with a numeral

824
00:36:49,359 --> 00:36:54,000
out

825
00:36:50,320 --> 00:36:56,880
to a u without a numeral out um

826
00:36:54,000 --> 00:36:58,640
if that's something that you need to do

827
00:36:56,880 --> 00:37:00,000
for instance many search engines do that

828
00:36:58,640 --> 00:37:03,440
they remove accents

829
00:37:00,000 --> 00:37:06,079
before returning results and fuzzy

830
00:37:03,440 --> 00:37:06,079
string match

831
00:37:06,400 --> 00:37:10,720
gives you useful metrics such as string

832
00:37:09,359 --> 00:37:13,119
similarity

833
00:37:10,720 --> 00:37:15,040
and it can return things like

834
00:37:13,119 --> 00:37:15,680
levenshtein distances between two

835
00:37:15,040 --> 00:37:18,960
strings

836
00:37:15,680 --> 00:37:21,040
how different the strings are

837
00:37:18,960 --> 00:37:23,520
it also supports sound x metaphone and

838
00:37:21,040 --> 00:37:25,200
double metaphone

839
00:37:23,520 --> 00:37:26,880
to be totally honest with you i haven't

840
00:37:25,200 --> 00:37:30,319
used it for that purpose

841
00:37:26,880 --> 00:37:31,359
so there is a warning on the postgres

842
00:37:30,320 --> 00:37:34,000
documentation

843
00:37:31,359 --> 00:37:36,880
that these soundx metaphone and double

844
00:37:34,000 --> 00:37:39,040
metaphor may not work well with utf-8

845
00:37:36,880 --> 00:37:39,040
so

846
00:37:39,920 --> 00:37:43,440
it works like this select name from

847
00:37:41,680 --> 00:37:47,680
users where levenstein

848
00:37:43,440 --> 00:37:50,880
the function stephen

849
00:37:47,680 --> 00:37:52,879
on the column name is less than two so

850
00:37:50,880 --> 00:37:55,599
if something is less than two characters

851
00:37:52,880 --> 00:37:57,680
different from stephen please return it

852
00:37:55,599 --> 00:38:00,079
so i'm essentially looking for steven

853
00:37:57,680 --> 00:38:00,078
with a v

854
00:38:02,400 --> 00:38:09,200
other index types that exist follow the

855
00:38:05,920 --> 00:38:09,839
drinks paradigm and there is a proof of

856
00:38:09,200 --> 00:38:13,200
concept

857
00:38:09,839 --> 00:38:15,599
index called vodka i do not know

858
00:38:13,200 --> 00:38:16,399
the state of its development right now

859
00:38:15,599 --> 00:38:19,760
but it

860
00:38:16,400 --> 00:38:22,800
offers advanced features

861
00:38:19,760 --> 00:38:23,200
and may i mention here that most of

862
00:38:22,800 --> 00:38:26,000
these

863
00:38:23,200 --> 00:38:27,520
text search functionality was

864
00:38:26,000 --> 00:38:30,839
implemented by our

865
00:38:27,520 --> 00:38:32,400
friends in russia community members that

866
00:38:30,839 --> 00:38:34,880
have

867
00:38:32,400 --> 00:38:36,000
contributed significant portions of code

868
00:38:34,880 --> 00:38:38,960
for indexing

869
00:38:36,000 --> 00:38:40,880
and full text searching and we also have

870
00:38:38,960 --> 00:38:43,760
rum

871
00:38:40,880 --> 00:38:45,280
rum is currently maintained and you can

872
00:38:43,760 --> 00:38:48,800
find it on

873
00:38:45,280 --> 00:38:51,920
that dodgy russian website

874
00:38:48,800 --> 00:38:52,800
and it stores positional information for

875
00:38:51,920 --> 00:38:55,839
lexiems

876
00:38:52,800 --> 00:38:56,640
which means that it can perform faster

877
00:38:55,839 --> 00:38:58,640
ranking

878
00:38:56,640 --> 00:39:00,078
because our indexes are not that useful

879
00:38:58,640 --> 00:39:02,879
for ranking

880
00:39:00,079 --> 00:39:04,960
and faster phrase search because you're

881
00:39:02,880 --> 00:39:07,359
looking for consecutive things

882
00:39:04,960 --> 00:39:10,160
it also stores distance between

883
00:39:07,359 --> 00:39:12,720
timestamps floats and money

884
00:39:10,160 --> 00:39:15,839
so these operations with this operator

885
00:39:12,720 --> 00:39:15,839
become very fast

886
00:39:15,920 --> 00:39:19,760
now one use case that i came across this

887
00:39:18,320 --> 00:39:22,960
year was

888
00:39:19,760 --> 00:39:26,240
a free text that was not natural text

889
00:39:22,960 --> 00:39:29,440
it was a text column that contained

890
00:39:26,240 --> 00:39:31,359
arbitrary strings

891
00:39:29,440 --> 00:39:33,599
non-human readable generally that

892
00:39:31,359 --> 00:39:36,640
contained keywords

893
00:39:33,599 --> 00:39:40,240
that needed to match

894
00:39:36,640 --> 00:39:42,240
something so basically the user was

895
00:39:40,240 --> 00:39:42,720
searching for strings that he did not

896
00:39:42,240 --> 00:39:45,759
know

897
00:39:42,720 --> 00:39:47,118
beforehand so

898
00:39:45,760 --> 00:39:49,040
you couldn't index based on those

899
00:39:47,119 --> 00:39:51,280
strings you had to search through the

900
00:39:49,040 --> 00:39:54,960
entire thing

901
00:39:51,280 --> 00:39:56,720
so such as keywords and device logs

902
00:39:54,960 --> 00:39:58,400
so dictionaries are not very helpful

903
00:39:56,720 --> 00:40:00,160
here because we're not talking about

904
00:39:58,400 --> 00:40:03,440
language

905
00:40:00,160 --> 00:40:07,520
and i created an arbitrary example

906
00:40:03,440 --> 00:40:11,119
uh with around 10 10 million rows

907
00:40:07,520 --> 00:40:14,000
of about 100 character iot device log

908
00:40:11,119 --> 00:40:15,200
entries fake of course

909
00:40:14,000 --> 00:40:17,119
some contain strings that are

910
00:40:15,200 --> 00:40:19,040
significant to the user as we said but

911
00:40:17,119 --> 00:40:23,119
the user doesn't know which strings

912
00:40:19,040 --> 00:40:23,119
until the last moment at runtime

913
00:40:23,440 --> 00:40:26,800
so we populate the table with random hex

914
00:40:26,240 --> 00:40:29,200
codes

915
00:40:26,800 --> 00:40:30,079
and we make one percent of these log

916
00:40:29,200 --> 00:40:33,040
entries uh

917
00:40:30,079 --> 00:40:33,520
contain a significant keyword and i

918
00:40:33,040 --> 00:40:35,520
chose

919
00:40:33,520 --> 00:40:37,359
keywords from etc dictionaries common

920
00:40:35,520 --> 00:40:39,759
words

921
00:40:37,359 --> 00:40:41,520
so they look like this they have

922
00:40:39,760 --> 00:40:45,520
something hidden in the middle

923
00:40:41,520 --> 00:40:45,520
of all the non-human readable stuff

924
00:40:46,240 --> 00:40:53,200
so select message from log entries

925
00:40:50,160 --> 00:40:55,759
they look like this barely readable

926
00:40:53,200 --> 00:40:59,598
barely useful for full text search and

927
00:40:55,760 --> 00:40:59,599
it's about 1.4 gigabytes

928
00:41:00,000 --> 00:41:03,599
our query that we're interested in

929
00:41:01,599 --> 00:41:05,200
speeding up is select star from log

930
00:41:03,599 --> 00:41:08,160
entries where message is

931
00:41:05,200 --> 00:41:08,879
like source so you can't you don't know

932
00:41:08,160 --> 00:41:11,040
anything

933
00:41:08,880 --> 00:41:12,720
you're interested in the word source but

934
00:41:11,040 --> 00:41:14,800
you don't know what precedes it and what

935
00:41:12,720 --> 00:41:18,240
follows it

936
00:41:14,800 --> 00:41:20,560
so how long will this take parallel

937
00:41:18,240 --> 00:41:25,839
sequential scan shows that it took

938
00:41:20,560 --> 00:41:25,839
9.6 seconds as we said too slow

939
00:41:29,680 --> 00:41:35,040
how do trigrams help us we mentioned

940
00:41:32,640 --> 00:41:38,720
that it's a probabilistic language model

941
00:41:35,040 --> 00:41:41,359
based on markov chains and

942
00:41:38,720 --> 00:41:42,480
three characters makes an engram a

943
00:41:41,359 --> 00:41:46,000
trigram

944
00:41:42,480 --> 00:41:48,960
so what it can show you is the

945
00:41:46,000 --> 00:41:51,599
similarity of alphanumeric text

946
00:41:48,960 --> 00:41:55,680
by counting the number for instance of

947
00:41:51,599 --> 00:41:55,680
shared trigrams between the two strings

948
00:41:56,800 --> 00:42:00,640
you use it by creating extension pg

949
00:41:58,880 --> 00:42:03,760
trigram

950
00:42:00,640 --> 00:42:06,720
by selecting show trigram

951
00:42:03,760 --> 00:42:07,200
you see what it converts your source

952
00:42:06,720 --> 00:42:09,279
word

953
00:42:07,200 --> 00:42:10,319
into and it splits it into these

954
00:42:09,280 --> 00:42:11,760
trigrams

955
00:42:10,319 --> 00:42:14,560
you'll notice that some of them are

956
00:42:11,760 --> 00:42:18,640
padded with white space

957
00:42:14,560 --> 00:42:21,920
create an index on log entries using gin

958
00:42:18,640 --> 00:42:23,680
and i specify gin trigram operations

959
00:42:21,920 --> 00:42:26,319
here

960
00:42:23,680 --> 00:42:26,319
did it help

961
00:42:27,680 --> 00:42:34,078
it's about 37 times 37 000 times faster

962
00:42:31,839 --> 00:42:35,279
than looking through the text so this

963
00:42:34,079 --> 00:42:38,640
index type

964
00:42:35,280 --> 00:42:42,160
helps you search for non-language

965
00:42:38,640 --> 00:42:43,839
text it also

966
00:42:42,160 --> 00:42:45,440
works with regular expressions so

967
00:42:43,839 --> 00:42:48,400
instead of like you could have used the

968
00:42:45,440 --> 00:42:52,400
regular expression here

969
00:42:48,400 --> 00:42:55,599
but this comes at a cost and the index

970
00:42:52,400 --> 00:42:57,760
is 1.6 megabytes

971
00:42:55,599 --> 00:42:59,359
gigabytes excuse me it's as big as the

972
00:42:57,760 --> 00:43:01,839
table

973
00:42:59,359 --> 00:43:02,720
but if you want rapid immediate

974
00:43:01,839 --> 00:43:05,920
responses

975
00:43:02,720 --> 00:43:08,480
that's the trade-off other tricks that

976
00:43:05,920 --> 00:43:11,680
trigram

977
00:43:08,480 --> 00:43:13,119
gives you is similarity of text as a

978
00:43:11,680 --> 00:43:18,000
number

979
00:43:13,119 --> 00:43:21,200
the distance between two bits of text

980
00:43:18,000 --> 00:43:23,760
a boolean that tells you if the text

981
00:43:21,200 --> 00:43:26,399
is more similar than your similarity

982
00:43:23,760 --> 00:43:28,480
threshold that you configure and

983
00:43:26,400 --> 00:43:31,280
it's supported by gin and the gist

984
00:43:28,480 --> 00:43:31,280
indexes as well

985
00:43:33,040 --> 00:43:38,079
postgres supports as we mentioned

986
00:43:36,160 --> 00:43:40,160
nearly every character set that is out

987
00:43:38,079 --> 00:43:40,720
there you can find out what your client

988
00:43:40,160 --> 00:43:45,040
is using

989
00:43:40,720 --> 00:43:48,240
by querying pg client encoding

990
00:43:45,040 --> 00:43:54,079
and you can convert strings

991
00:43:48,240 --> 00:43:55,759
between source and target encodings but

992
00:43:54,079 --> 00:43:58,560
you can also have your client

993
00:43:55,760 --> 00:43:59,200
do automatic character set conversion by

994
00:43:58,560 --> 00:44:02,480
setting

995
00:43:59,200 --> 00:44:04,799
your client encoding to uh whatever you

996
00:44:02,480 --> 00:44:06,480
want and that will attempt if it is

997
00:44:04,800 --> 00:44:07,760
possible to convert from the character

998
00:44:06,480 --> 00:44:09,760
set in the database

999
00:44:07,760 --> 00:44:12,400
to your client's character set it will

1000
00:44:09,760 --> 00:44:15,440
attempt to do it automatically

1001
00:44:12,400 --> 00:44:18,079
now we have to speak about collision

1002
00:44:15,440 --> 00:44:19,119
which is the sort order that is

1003
00:44:18,079 --> 00:44:21,280
different even with

1004
00:44:19,119 --> 00:44:22,640
the same alphabets it's different in

1005
00:44:21,280 --> 00:44:25,760
different languages

1006
00:44:22,640 --> 00:44:28,160
so you have you can

1007
00:44:25,760 --> 00:44:30,480
define the collation order and the

1008
00:44:28,160 --> 00:44:33,680
character classification

1009
00:44:30,480 --> 00:44:34,640
for a specific language per column by

1010
00:44:33,680 --> 00:44:37,759
creating it

1011
00:44:34,640 --> 00:44:41,040
at creation time by specifying it

1012
00:44:37,760 --> 00:44:42,319
at creation time or during an operation

1013
00:44:41,040 --> 00:44:44,480
so you can select

1014
00:44:42,319 --> 00:44:45,599
with a specific german language

1015
00:44:44,480 --> 00:44:48,720
collation

1016
00:44:45,599 --> 00:44:48,720
from a table

1017
00:44:49,040 --> 00:44:53,440
that means that you are not restricted

1018
00:44:51,040 --> 00:44:54,000
by whatever configuration your database

1019
00:44:53,440 --> 00:44:59,839
has

1020
00:44:54,000 --> 00:45:05,839
and excuse me lc collate and lcc type

1021
00:44:59,839 --> 00:45:05,839
and sorry

1022
00:45:08,960 --> 00:45:12,560
in postgres 12 we have non-deterministic

1023
00:45:11,280 --> 00:45:15,760
relations

1024
00:45:12,560 --> 00:45:16,480
that can also ignore accents when

1025
00:45:15,760 --> 00:45:20,560
sorting

1026
00:45:16,480 --> 00:45:20,560
text and that is a new feature

1027
00:45:21,520 --> 00:45:28,160
we mentioned uh other types of documents

1028
00:45:25,119 --> 00:45:28,160
such as json

1029
00:45:28,800 --> 00:45:32,480
it supports indexing it can be converted

1030
00:45:30,880 --> 00:45:34,800
into ts vector

1031
00:45:32,480 --> 00:45:36,960
you can use the function json b to ts

1032
00:45:34,800 --> 00:45:40,079
vector

1033
00:45:36,960 --> 00:45:41,839
and that will classify numeric key

1034
00:45:40,079 --> 00:45:45,440
strings and booleans differently

1035
00:45:41,839 --> 00:45:48,799
if you want to also in postgres 12

1036
00:45:45,440 --> 00:45:49,599
we have the sql standard sql json query

1037
00:45:48,800 --> 00:45:52,160
language

1038
00:45:49,599 --> 00:45:52,640
that you can use to perform searches on

1039
00:45:52,160 --> 00:45:56,078
uh

1040
00:45:52,640 --> 00:45:58,839
json and we also have js query

1041
00:45:56,079 --> 00:46:00,560
that supports that's a different query

1042
00:45:58,839 --> 00:46:05,119
language

1043
00:46:00,560 --> 00:46:06,960
finally make sure you vacuum analyze

1044
00:46:05,119 --> 00:46:08,560
so that your table statistics are up to

1045
00:46:06,960 --> 00:46:11,920
date

1046
00:46:08,560 --> 00:46:13,279
so that your gen entries are integrated

1047
00:46:11,920 --> 00:46:15,839
into the main index

1048
00:46:13,280 --> 00:46:16,319
because for update reasons it keeps a

1049
00:46:15,839 --> 00:46:18,880
list at

1050
00:46:16,319 --> 00:46:20,319
the end and only vacuum updates your gen

1051
00:46:18,880 --> 00:46:22,800
indexes

1052
00:46:20,319 --> 00:46:23,920
you can set your statistics you need to

1053
00:46:22,800 --> 00:46:26,079
keep them accurate

1054
00:46:23,920 --> 00:46:27,760
by number of distinct values if you know

1055
00:46:26,079 --> 00:46:30,240
them

1056
00:46:27,760 --> 00:46:31,119
by setting correlated columns that

1057
00:46:30,240 --> 00:46:33,598
influence

1058
00:46:31,119 --> 00:46:35,599
the results of your searches and you do

1059
00:46:33,599 --> 00:46:36,160
need to run explain analyze from time to

1060
00:46:35,599 --> 00:46:37,599
time

1061
00:46:36,160 --> 00:46:39,839
because you don't know that the query

1062
00:46:37,599 --> 00:46:43,440
that works now perfectly may work in a

1063
00:46:39,839 --> 00:46:45,680
year's time or not

1064
00:46:43,440 --> 00:46:47,760
and maintenance work man because we're

1065
00:46:45,680 --> 00:46:48,399
talking about huge index creation times

1066
00:46:47,760 --> 00:46:50,319
here

1067
00:46:48,400 --> 00:46:52,400
is very significant especially for gen

1068
00:46:50,319 --> 00:46:55,200
indexes

1069
00:46:52,400 --> 00:46:56,480
now one final thing is the curious case

1070
00:46:55,200 --> 00:46:59,439
of text name

1071
00:46:56,480 --> 00:47:00,960
i found in my uh in the code that was

1072
00:46:59,440 --> 00:47:03,520
that i was maintaining

1073
00:47:00,960 --> 00:47:07,520
i found create something like create

1074
00:47:03,520 --> 00:47:07,520
table user id serial text name

1075
00:47:07,599 --> 00:47:13,839
wait that is type name

1076
00:47:11,359 --> 00:47:14,720
and it works fine it appears to work

1077
00:47:13,839 --> 00:47:17,759
fine

1078
00:47:14,720 --> 00:47:21,680
but it looks like instead of typing in

1079
00:47:17,760 --> 00:47:24,880
name text so the column named name

1080
00:47:21,680 --> 00:47:28,558
of type text they typed column

1081
00:47:24,880 --> 00:47:30,640
named text of type name and

1082
00:47:28,559 --> 00:47:32,000
that is an internal type that will

1083
00:47:30,640 --> 00:47:35,040
actually store strings

1084
00:47:32,000 --> 00:47:36,800
but it will only store up to 64 bytes so

1085
00:47:35,040 --> 00:47:40,000
you'll have nasty surprises

1086
00:47:36,800 --> 00:47:41,520
if you use type name for your text so

1087
00:47:40,000 --> 00:47:53,839
don't be sleepy when you're creating

1088
00:47:41,520 --> 00:47:53,839
tables thank you very much

1089
00:48:02,480 --> 00:48:08,960
what is it the same

1090
00:48:06,319 --> 00:48:10,880
if you have 10 entries with convenience

1091
00:48:08,960 --> 00:48:12,559
characters or if you have a millionaires

1092
00:48:10,880 --> 00:48:13,440
which is just 10 characters in with

1093
00:48:12,559 --> 00:48:16,000
ratings

1094
00:48:13,440 --> 00:48:16,640
like are there any downsides to these

1095
00:48:16,000 --> 00:48:21,440
different

1096
00:48:16,640 --> 00:48:23,040
indexes so the question is um

1097
00:48:21,440 --> 00:48:25,040
whether there's a difference between

1098
00:48:23,040 --> 00:48:28,079
indexing uh

1099
00:48:25,040 --> 00:48:30,800
very large values and

1100
00:48:28,079 --> 00:48:32,000
or very many values with uh small

1101
00:48:30,800 --> 00:48:35,760
content right

1102
00:48:32,000 --> 00:48:39,200
so as we said just is better at

1103
00:48:35,760 --> 00:48:43,280
storing at indexing things that are

1104
00:48:39,200 --> 00:48:46,960
fewer but unique whereas

1105
00:48:43,280 --> 00:48:48,079
gin is better with large pieces of text

1106
00:48:46,960 --> 00:48:50,640
that are non-structured

1107
00:48:48,079 --> 00:48:50,640
or random

1108
00:48:51,760 --> 00:48:55,839
any questions how would you index the

1109
00:48:54,319 --> 00:48:57,759
documents

1110
00:48:55,839 --> 00:48:59,920
but you don't know which one sorry can

1111
00:48:57,760 --> 00:49:01,599
you repeat that

1112
00:48:59,920 --> 00:49:05,440
in a human language but you don't know

1113
00:49:01,599 --> 00:49:05,440
which one so they could be german

1114
00:49:05,520 --> 00:49:10,000
so you will uh is the question is how do

1115
00:49:08,240 --> 00:49:10,479
you index documents if you don't know

1116
00:49:10,000 --> 00:49:13,359
the

1117
00:49:10,480 --> 00:49:14,000
which human language they are they are

1118
00:49:13,359 --> 00:49:16,078
in

1119
00:49:14,000 --> 00:49:17,119
and the question is complex so you

1120
00:49:16,079 --> 00:49:19,680
either have to

1121
00:49:17,119 --> 00:49:21,119
use all dictionaries that you know in

1122
00:49:19,680 --> 00:49:24,078
order to be able to match

1123
00:49:21,119 --> 00:49:25,839
and then produce weights so you can

1124
00:49:24,079 --> 00:49:28,319
decide based on the ranking

1125
00:49:25,839 --> 00:49:30,720
which is the most probable language that

1126
00:49:28,319 --> 00:49:34,240
is being used in the text

1127
00:49:30,720 --> 00:49:34,799
okay sorry last question so we heavily

1128
00:49:34,240 --> 00:49:44,240
use

1129
00:49:34,800 --> 00:49:44,240
json b and also the full text search

