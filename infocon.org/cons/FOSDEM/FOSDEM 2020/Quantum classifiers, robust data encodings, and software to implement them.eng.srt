1
00:00:05,440 --> 00:00:08,800
okay everyone

2
00:00:06,319 --> 00:00:11,200
i'd like to introduce our next speaker

3
00:00:08,800 --> 00:00:11,200
ryan

4
00:00:12,639 --> 00:00:16,560
ryan is a phd student at michigan state

5
00:00:15,440 --> 00:00:19,520
university where he

6
00:00:16,560 --> 00:00:21,359
pursues a dual phd in physics sounds

7
00:00:19,520 --> 00:00:23,039
pretty stressful

8
00:00:21,359 --> 00:00:24,480
and he's interested in both the physics

9
00:00:23,039 --> 00:00:26,560
of computation

10
00:00:24,480 --> 00:00:28,880
and the computation of physics so it

11
00:00:26,560 --> 00:00:32,479
basically means like what can we learn

12
00:00:28,880 --> 00:00:33,920
from quantum physics about computation

13
00:00:32,479 --> 00:00:37,040
and also how can we use quantum

14
00:00:33,920 --> 00:00:38,960
computers to understand physics better

15
00:00:37,040 --> 00:00:41,760
and he's the creator and maintainer of

16
00:00:38,960 --> 00:00:43,440
the quantum open source project nisk ai

17
00:00:41,760 --> 00:00:45,920
and today he's talking about quantum

18
00:00:43,440 --> 00:00:48,000
classifiers robust data encodings and

19
00:00:45,920 --> 00:00:51,360
software to implement them

20
00:00:48,000 --> 00:00:51,360
so please welcome ryan

21
00:00:55,440 --> 00:01:00,959
can anyone hear me i think this is on

22
00:00:57,840 --> 00:01:03,039
okay uh yeah thank you thanks uh mark uh

23
00:01:00,960 --> 00:01:04,080
and thanks to the organizers uh for this

24
00:01:03,039 --> 00:01:06,400
great event

25
00:01:04,080 --> 00:01:08,320
uh so today i will mainly be talking

26
00:01:06,400 --> 00:01:10,479
about uh three things

27
00:01:08,320 --> 00:01:12,639
uh all related uh the first is our

28
00:01:10,479 --> 00:01:15,920
library in this gi

29
00:01:12,640 --> 00:01:17,759
and the second is

30
00:01:15,920 --> 00:01:19,040
some research that sort of spawned off

31
00:01:17,759 --> 00:01:22,000
of this

32
00:01:19,040 --> 00:01:23,280
about data encodings and robustness

33
00:01:22,000 --> 00:01:25,840
which i'll get into

34
00:01:23,280 --> 00:01:26,960
and then the third thing is is just how

35
00:01:25,840 --> 00:01:29,920
anyone contribute

36
00:01:26,960 --> 00:01:30,798
uh to niskay i if if you're interested

37
00:01:29,920 --> 00:01:34,560
um

38
00:01:30,799 --> 00:01:36,560
so here is our uh github niskaya

39
00:01:34,560 --> 00:01:37,920
we say is an open source framework for

40
00:01:36,560 --> 00:01:38,560
quantum machine learning in the near

41
00:01:37,920 --> 00:01:41,680
term

42
00:01:38,560 --> 00:01:42,079
uh sort of one of several now uh we have

43
00:01:41,680 --> 00:01:43,520
a

44
00:01:42,079 --> 00:01:46,000
little bit different approach than

45
00:01:43,520 --> 00:01:47,840
others uh well we're in python but we're

46
00:01:46,000 --> 00:01:50,079
targeted only for regetti's quantum

47
00:01:47,840 --> 00:01:51,680
cloud services

48
00:01:50,079 --> 00:01:53,759
instead of being sort of hardware and

49
00:01:51,680 --> 00:01:56,159
back end agnostic

50
00:01:53,759 --> 00:01:57,520
we're only writing for this back end in

51
00:01:56,159 --> 00:02:00,640
the near term it has

52
00:01:57,520 --> 00:02:02,560
uh very promising uh

53
00:02:00,640 --> 00:02:05,200
capabilities for for near-term quantum

54
00:02:02,560 --> 00:02:07,920
computation such as dedicated time

55
00:02:05,200 --> 00:02:08,399
active resets and all these nice things

56
00:02:07,920 --> 00:02:10,959
um

57
00:02:08,399 --> 00:02:11,599
so far in our library um we're still

58
00:02:10,959 --> 00:02:14,640
sort of

59
00:02:11,599 --> 00:02:17,679
in a uh alpha stage

60
00:02:14,640 --> 00:02:19,760
um uh we have several examples

61
00:02:17,680 --> 00:02:21,920
for quantum classifiers one of which i'm

62
00:02:19,760 --> 00:02:23,280
going to go through here in a moment

63
00:02:21,920 --> 00:02:25,119
and we're looking to add some kernel

64
00:02:23,280 --> 00:02:26,400
methods for for quantum machine learning

65
00:02:25,120 --> 00:02:29,599
in the near future

66
00:02:26,400 --> 00:02:32,400
and we hope to put

67
00:02:29,599 --> 00:02:33,839
get ourselves on pi pi uh at some point

68
00:02:32,400 --> 00:02:36,080
this year

69
00:02:33,840 --> 00:02:36,959
uh so a high level overview of our

70
00:02:36,080 --> 00:02:40,400
library

71
00:02:36,959 --> 00:02:43,440
um on the left we have uh tools for

72
00:02:40,400 --> 00:02:45,120
representing data so c data for

73
00:02:43,440 --> 00:02:46,480
classical data and queue data for

74
00:02:45,120 --> 00:02:49,120
quantum data

75
00:02:46,480 --> 00:02:50,560
classical data is you know what you

76
00:02:49,120 --> 00:02:53,040
normally have

77
00:02:50,560 --> 00:02:55,120
and quantum data you can think of as

78
00:02:53,040 --> 00:02:58,640
something like a hamiltonian

79
00:02:55,120 --> 00:03:01,680
describing maybe a molecule

80
00:02:58,640 --> 00:03:03,279
and then we have tools to uh design and

81
00:03:01,680 --> 00:03:06,000
put together

82
00:03:03,280 --> 00:03:06,879
a so-called quantum neural network so we

83
00:03:06,000 --> 00:03:09,120
have

84
00:03:06,879 --> 00:03:10,159
an encoder which is something i'll talk

85
00:03:09,120 --> 00:03:12,959
about

86
00:03:10,159 --> 00:03:16,560
a lot in part two this is how you map

87
00:03:12,959 --> 00:03:16,560
your input data into the circuits

88
00:03:16,640 --> 00:03:22,399
on satsas for evolving

89
00:03:20,159 --> 00:03:24,319
and then predictions and measurements uh

90
00:03:22,400 --> 00:03:25,680
for extracting sort of information from

91
00:03:24,319 --> 00:03:28,238
that circuits

92
00:03:25,680 --> 00:03:29,360
and then we have uh tools for defining

93
00:03:28,239 --> 00:03:31,760
loss

94
00:03:29,360 --> 00:03:33,599
and trainers for optimizing this the

95
00:03:31,760 --> 00:03:35,200
sort of normal thing you need in machine

96
00:03:33,599 --> 00:03:37,359
learning libraries

97
00:03:35,200 --> 00:03:39,440
and then things that help you run on the

98
00:03:37,360 --> 00:03:43,040
back end

99
00:03:39,440 --> 00:03:45,760
so now i'd like to go through briefly

100
00:03:43,040 --> 00:03:46,879
one of the examples that we have and the

101
00:03:45,760 --> 00:03:47,599
problem that we're going to be

102
00:03:46,879 --> 00:03:50,720
considering

103
00:03:47,599 --> 00:03:52,798
is a classification problem but

104
00:03:50,720 --> 00:03:53,840
in the sort of quantum world so this if

105
00:03:52,799 --> 00:03:56,319
you're not familiar

106
00:03:53,840 --> 00:03:58,319
we input a bunch of feature vectors and

107
00:03:56,319 --> 00:04:01,760
each one of them has a label

108
00:03:58,319 --> 00:04:04,480
and then we want to outputs um some

109
00:04:01,760 --> 00:04:05,679
sort of uh so to speak intelligent

110
00:04:04,480 --> 00:04:07,760
quantum machine

111
00:04:05,680 --> 00:04:09,040
uh which can correctly classify all

112
00:04:07,760 --> 00:04:10,879
these features hopefully

113
00:04:09,040 --> 00:04:12,239
and make predictions on new data that it

114
00:04:10,879 --> 00:04:14,640
hasn't seen yet

115
00:04:12,239 --> 00:04:16,239
so quantum is uh in blue here because

116
00:04:14,640 --> 00:04:16,798
it's the same problem in the classical

117
00:04:16,238 --> 00:04:18,798
world

118
00:04:16,798 --> 00:04:20,719
uh we're just using quantum resources to

119
00:04:18,798 --> 00:04:22,880
do it

120
00:04:20,720 --> 00:04:24,479
um so this is one example that we have

121
00:04:22,880 --> 00:04:25,919
here's the link if you want to look at

122
00:04:24,479 --> 00:04:27,680
it on the github and i'll just walk

123
00:04:25,919 --> 00:04:30,560
through the code briefly

124
00:04:27,680 --> 00:04:31,680
so the first thing we do is import and

125
00:04:30,560 --> 00:04:34,160
then start

126
00:04:31,680 --> 00:04:34,880
what we call an engine an engine is sort

127
00:04:34,160 --> 00:04:37,520
of a

128
00:04:34,880 --> 00:04:38,560
connection to righties qvm and quill

129
00:04:37,520 --> 00:04:40,840
compiler

130
00:04:38,560 --> 00:04:42,000
which you need to run locally on your

131
00:04:40,840 --> 00:04:44,400
computer

132
00:04:42,000 --> 00:04:45,919
and this image down here is a graphic

133
00:04:44,400 --> 00:04:48,320
that regetti has made

134
00:04:45,919 --> 00:04:49,280
to show these things um you need to do

135
00:04:48,320 --> 00:04:52,159
this locally

136
00:04:49,280 --> 00:04:53,840
and this is nice because you can do it

137
00:04:52,160 --> 00:04:58,639
uh within your script you don't have to

138
00:04:53,840 --> 00:04:58,638
start another terminal

139
00:04:58,800 --> 00:05:03,440
and and so on the next thing we do is

140
00:05:02,080 --> 00:05:05,280
we get some data so here we're just

141
00:05:03,440 --> 00:05:07,600
going to use random data which can be

142
00:05:05,280 --> 00:05:10,320
separated by a horizontal line

143
00:05:07,600 --> 00:05:11,039
so that's line five here and then if you

144
00:05:10,320 --> 00:05:13,919
want

145
00:05:11,039 --> 00:05:14,240
we have tools to visualize data so if

146
00:05:13,919 --> 00:05:16,000
you

147
00:05:14,240 --> 00:05:17,600
run this you'll see a plot that looks

148
00:05:16,000 --> 00:05:19,600
something like this

149
00:05:17,600 --> 00:05:21,840
so the top class colored green and the

150
00:05:19,600 --> 00:05:23,919
bottom class colored blue

151
00:05:21,840 --> 00:05:26,479
and this line is showing you the the

152
00:05:23,919 --> 00:05:28,400
decision boundary between the two

153
00:05:26,479 --> 00:05:29,840
um so here's the point where we get to

154
00:05:28,400 --> 00:05:32,960
the encoder

155
00:05:29,840 --> 00:05:34,239
uh the encoding that we use here is one

156
00:05:32,960 --> 00:05:35,440
example of several that we have in the

157
00:05:34,240 --> 00:05:37,919
library this is

158
00:05:35,440 --> 00:05:40,000
what we call this dense angle encoding

159
00:05:37,919 --> 00:05:44,159
so the dense angle encoding maps

160
00:05:40,000 --> 00:05:46,800
two features into a single qubit

161
00:05:44,160 --> 00:05:49,440
so here the features are x1 and x2 and

162
00:05:46,800 --> 00:05:53,199
they get mapped into the qubit

163
00:05:49,440 --> 00:05:54,880
written in the lower left uh as shown

164
00:05:53,199 --> 00:05:56,639
and then on the lower right is this

165
00:05:54,880 --> 00:05:58,080
graphical representation of what's

166
00:05:56,639 --> 00:06:00,639
happening so you're taking

167
00:05:58,080 --> 00:06:02,080
uh data that lies in the unit square

168
00:06:00,639 --> 00:06:02,880
this is classical data and then you're

169
00:06:02,080 --> 00:06:06,318
putting it

170
00:06:02,880 --> 00:06:09,039
on the surface of the bloch sphere on

171
00:06:06,319 --> 00:06:11,120
our qubit here

172
00:06:09,039 --> 00:06:12,880
the decision boundary you start with is

173
00:06:11,120 --> 00:06:14,319
the one that you see here

174
00:06:12,880 --> 00:06:17,120
and the goal of the classifier is to

175
00:06:14,319 --> 00:06:20,720
train this to

176
00:06:17,120 --> 00:06:23,440
correctly classify all the input data

177
00:06:20,720 --> 00:06:24,800
so that's what we do in a few more steps

178
00:06:23,440 --> 00:06:26,240
we first need to define what our

179
00:06:24,800 --> 00:06:28,720
onslaughts is

180
00:06:26,240 --> 00:06:30,479
so here on a single qubit all ancestors

181
00:06:28,720 --> 00:06:32,240
are essentially the same

182
00:06:30,479 --> 00:06:34,639
so we use a product downsides here which

183
00:06:32,240 --> 00:06:36,880
is parameterized by two angles

184
00:06:34,639 --> 00:06:37,919
and then we add a measurement to the

185
00:06:36,880 --> 00:06:39,199
circuit

186
00:06:37,919 --> 00:06:41,120
and the way that we get our

187
00:06:39,199 --> 00:06:43,600
classification is is by doing this

188
00:06:41,120 --> 00:06:44,960
measurement

189
00:06:43,600 --> 00:06:47,360
once we have these three things the

190
00:06:44,960 --> 00:06:50,080
encoder the onsites and the measurement

191
00:06:47,360 --> 00:06:51,759
we can form them into a quantum neural

192
00:06:50,080 --> 00:06:54,960
network

193
00:06:51,759 --> 00:06:57,199
and this is how you do that here we also

194
00:06:54,960 --> 00:07:00,560
provided a computer which here is

195
00:06:57,199 --> 00:07:02,720
one cubit qvm by righetti and

196
00:07:00,560 --> 00:07:03,919
something which we call a predictor

197
00:07:02,720 --> 00:07:07,280
which is how you map

198
00:07:03,919 --> 00:07:11,520
from the raw outputs of

199
00:07:07,280 --> 00:07:14,479
measurement data to a prediction

200
00:07:11,520 --> 00:07:15,120
and this is the the quantum circuits of

201
00:07:14,479 --> 00:07:18,000
what this

202
00:07:15,120 --> 00:07:19,759
code is representing now once we have

203
00:07:18,000 --> 00:07:21,440
this now we can train

204
00:07:19,759 --> 00:07:22,639
i'll just go through this quickly this

205
00:07:21,440 --> 00:07:24,560
is the standard thing in machine

206
00:07:22,639 --> 00:07:26,800
learning and then once you train

207
00:07:24,560 --> 00:07:28,319
you can get all the predictions and here

208
00:07:26,800 --> 00:07:30,479
in this case

209
00:07:28,319 --> 00:07:31,840
you are able to collect correctly

210
00:07:30,479 --> 00:07:35,758
classify

211
00:07:31,840 --> 00:07:37,599
all of this data um

212
00:07:35,759 --> 00:07:39,280
so that's that's one example that we

213
00:07:37,599 --> 00:07:40,719
have there are a few others

214
00:07:39,280 --> 00:07:42,960
there's just a few things that i want to

215
00:07:40,720 --> 00:07:45,680
note uh so this is an

216
00:07:42,960 --> 00:07:47,039
implementation uh with fewer than 20

217
00:07:45,680 --> 00:07:48,639
lines of code

218
00:07:47,039 --> 00:07:50,159
and most of them were sort of comments

219
00:07:48,639 --> 00:07:53,520
so even fewer

220
00:07:50,160 --> 00:07:55,520
um so that's nice um you could uh

221
00:07:53,520 --> 00:07:57,840
it's modular so you can play around with

222
00:07:55,520 --> 00:07:58,799
this and easily extend it to other

223
00:07:57,840 --> 00:08:01,440
models

224
00:07:58,800 --> 00:08:02,960
um you can add more qubits you can

225
00:08:01,440 --> 00:08:05,599
change what the encoding is

226
00:08:02,960 --> 00:08:07,280
uh you can change what the onsites is

227
00:08:05,599 --> 00:08:09,120
and you can do all these things

228
00:08:07,280 --> 00:08:10,638
and test the performance and see what

229
00:08:09,120 --> 00:08:14,080
sort of performance

230
00:08:10,639 --> 00:08:15,039
you're able to get and if you notice

231
00:08:14,080 --> 00:08:17,680
there are sort of

232
00:08:15,039 --> 00:08:19,520
very few references to how you're

233
00:08:17,680 --> 00:08:20,960
actually running the underlying circuits

234
00:08:19,520 --> 00:08:23,198
and programs

235
00:08:20,960 --> 00:08:24,000
we try to take care of that mostly all

236
00:08:23,199 --> 00:08:26,560
under the hood

237
00:08:24,000 --> 00:08:28,240
you need to tell at least right now what

238
00:08:26,560 --> 00:08:29,599
computer you want to run on

239
00:08:28,240 --> 00:08:32,560
but maybe that could even change in the

240
00:08:29,599 --> 00:08:35,200
future and so this is nice

241
00:08:32,559 --> 00:08:36,399
uh for people who may have more of a

242
00:08:35,200 --> 00:08:38,240
machine learning background than a

243
00:08:36,399 --> 00:08:40,159
quantum computing background

244
00:08:38,240 --> 00:08:42,799
and this could allow them to sort of get

245
00:08:40,159 --> 00:08:44,800
into the space

246
00:08:42,799 --> 00:08:45,839
okay so that's the first part uh the

247
00:08:44,800 --> 00:08:48,319
second part

248
00:08:45,839 --> 00:08:49,760
is now some recent research that i want

249
00:08:48,320 --> 00:08:52,800
to talk about

250
00:08:49,760 --> 00:08:54,640
this is currently unpublished

251
00:08:52,800 --> 00:08:57,040
i hope it will be published soon on the

252
00:08:54,640 --> 00:08:57,040
archive

253
00:08:57,279 --> 00:09:00,959
and this is work that i've been doing

254
00:08:58,720 --> 00:09:02,720
with brian coyle who is a phd student at

255
00:09:00,959 --> 00:09:04,239
the university of edinburgh

256
00:09:02,720 --> 00:09:05,760
and someone who i actually met at

257
00:09:04,240 --> 00:09:08,399
fosterm last year

258
00:09:05,760 --> 00:09:09,200
so he made contributions to niskayae and

259
00:09:08,399 --> 00:09:11,519
then we were

260
00:09:09,200 --> 00:09:14,560
working on this research project and he

261
00:09:11,519 --> 00:09:16,800
joined and had a lot of great ideas

262
00:09:14,560 --> 00:09:18,959
so what i want to do is sort of formally

263
00:09:16,800 --> 00:09:21,120
define at a mathematical level what's

264
00:09:18,959 --> 00:09:24,239
the quantum classifier

265
00:09:21,120 --> 00:09:25,839
model we're considering is so the first

266
00:09:24,240 --> 00:09:28,080
thing like i said is we start with some

267
00:09:25,839 --> 00:09:30,240
data points x which is our feature

268
00:09:28,080 --> 00:09:30,880
vector and we map that into a quantum

269
00:09:30,240 --> 00:09:33,600
state

270
00:09:30,880 --> 00:09:36,080
roosevelts and this is the encoding

271
00:09:33,600 --> 00:09:39,120
after this we evolve with some unitary

272
00:09:36,080 --> 00:09:40,160
and then we make a prediction so here on

273
00:09:39,120 --> 00:09:43,279
the lower

274
00:09:40,160 --> 00:09:44,880
right you see

275
00:09:43,279 --> 00:09:46,320
this example for one qubit that's what

276
00:09:44,880 --> 00:09:48,399
we saw and you can generalize this to

277
00:09:46,320 --> 00:09:51,279
more qubits at the top

278
00:09:48,399 --> 00:09:51,920
and the way we get the prediction is by

279
00:09:51,279 --> 00:09:54,720
uh

280
00:09:51,920 --> 00:09:55,599
doing a measurement of a single qubit

281
00:09:54,720 --> 00:09:57,760
and we

282
00:09:55,600 --> 00:09:59,040
do the classification as follows so if

283
00:09:57,760 --> 00:10:01,519
we measure the qubits

284
00:09:59,040 --> 00:10:02,640
zero more often than we measure at one

285
00:10:01,519 --> 00:10:05,760
we assign it to

286
00:10:02,640 --> 00:10:07,760
uh the class zero you could call it

287
00:10:05,760 --> 00:10:09,519
anything but call it class zero

288
00:10:07,760 --> 00:10:11,120
and we assign it to the other class call

289
00:10:09,519 --> 00:10:14,720
it class one

290
00:10:11,120 --> 00:10:18,240
if we measure one more often than zero

291
00:10:14,720 --> 00:10:20,959
okay so this is a sort of standard model

292
00:10:18,240 --> 00:10:22,320
considered in a lot of recent literature

293
00:10:20,959 --> 00:10:24,800
this is perhaps the first

294
00:10:22,320 --> 00:10:25,920
in this paper by adfari and hartman

295
00:10:24,800 --> 00:10:29,760
nevin

296
00:10:25,920 --> 00:10:32,399
from i forget the year but fairly recent

297
00:10:29,760 --> 00:10:33,200
and here they're doing the sort of same

298
00:10:32,399 --> 00:10:36,959
structure

299
00:10:33,200 --> 00:10:38,640
where it's this unitary circuit

300
00:10:36,959 --> 00:10:40,239
and then they measure a single qubit in

301
00:10:38,640 --> 00:10:41,680
order to get their prediction

302
00:10:40,240 --> 00:10:43,200
so there's these sort of three points

303
00:10:41,680 --> 00:10:43,839
that they go through again they encode

304
00:10:43,200 --> 00:10:45,440
the data

305
00:10:43,839 --> 00:10:48,480
evolve with the onsites and then measure

306
00:10:45,440 --> 00:10:50,160
qubit there's other work that considers

307
00:10:48,480 --> 00:10:51,920
very similar models

308
00:10:50,160 --> 00:10:53,439
and this is just to sort of motivate a

309
00:10:51,920 --> 00:10:54,399
little bit that what we're doing is

310
00:10:53,440 --> 00:10:57,760
sensible

311
00:10:54,399 --> 00:10:59,200
um so here again

312
00:10:57,760 --> 00:11:01,600
the quantum circuit is essentially the

313
00:10:59,200 --> 00:11:02,000
same and here in the lower right they

314
00:11:01,600 --> 00:11:05,200
write

315
00:11:02,000 --> 00:11:06,640
down their equation for getting the

316
00:11:05,200 --> 00:11:08,959
classification

317
00:11:06,640 --> 00:11:10,399
which is very similar to ours on the

318
00:11:08,959 --> 00:11:12,719
previous slide except they add

319
00:11:10,399 --> 00:11:14,480
a bias here as well and that's just

320
00:11:12,720 --> 00:11:16,079
something that you can do um and again

321
00:11:14,480 --> 00:11:18,320
it's these three points

322
00:11:16,079 --> 00:11:19,839
um this is the onslaughts that they

323
00:11:18,320 --> 00:11:22,720
consider in this paper

324
00:11:19,839 --> 00:11:24,959
um and in most papers uh a lot of people

325
00:11:22,720 --> 00:11:27,839
are focusing on the onsaatzes

326
00:11:24,959 --> 00:11:29,518
um and not so much the first points of

327
00:11:27,839 --> 00:11:32,000
the data encoding

328
00:11:29,519 --> 00:11:33,279
and that's what we're going to look at

329
00:11:32,000 --> 00:11:36,399
here

330
00:11:33,279 --> 00:11:38,399
so here's the three points again

331
00:11:36,399 --> 00:11:39,680
and and yep this is what i just said so

332
00:11:38,399 --> 00:11:42,399
so this is the one that we're going to

333
00:11:39,680 --> 00:11:44,239
focus on

334
00:11:42,399 --> 00:11:45,680
and the motivation of why this is

335
00:11:44,240 --> 00:11:47,760
important um

336
00:11:45,680 --> 00:11:49,439
this is not just in the quantum world in

337
00:11:47,760 --> 00:11:49,920
the classical world how you represent

338
00:11:49,440 --> 00:11:53,839
data

339
00:11:49,920 --> 00:11:55,120
is critical to the uh success of a

340
00:11:53,839 --> 00:11:57,519
machine learning model

341
00:11:55,120 --> 00:11:58,880
so on on the left here you're seeing the

342
00:11:57,519 --> 00:12:02,240
geocentric model

343
00:11:58,880 --> 00:12:03,920
of the solar system and the orbit

344
00:12:02,240 --> 00:12:06,000
of the planets in this case is very

345
00:12:03,920 --> 00:12:07,439
complicated and

346
00:12:06,000 --> 00:12:09,600
you can imagine it would be harder to

347
00:12:07,440 --> 00:12:11,519
learn than

348
00:12:09,600 --> 00:12:13,680
the heliocentric model on the right

349
00:12:11,519 --> 00:12:17,519
which everything is is just a nice

350
00:12:13,680 --> 00:12:21,680
ellipse essentially a circular orbit

351
00:12:17,519 --> 00:12:23,839
um so in the quantum world

352
00:12:21,680 --> 00:12:25,199
how we encode classical data is is going

353
00:12:23,839 --> 00:12:27,600
to be very important both for the

354
00:12:25,200 --> 00:12:29,120
learnability of the model that is

355
00:12:27,600 --> 00:12:31,680
what sorts of decision boundaries you

356
00:12:29,120 --> 00:12:33,040
can learn and also for robustness to

357
00:12:31,680 --> 00:12:34,880
noise

358
00:12:33,040 --> 00:12:36,319
and there's an interesting sort of

359
00:12:34,880 --> 00:12:37,200
qualitative thing that we see is that

360
00:12:36,320 --> 00:12:42,320
there's a trade-off

361
00:12:37,200 --> 00:12:44,480
between these two so this is a sort of

362
00:12:42,320 --> 00:12:45,920
cartoon picture of how i think about

363
00:12:44,480 --> 00:12:48,399
encoding

364
00:12:45,920 --> 00:12:49,040
classical data in the quantum states so

365
00:12:48,399 --> 00:12:51,440
you have

366
00:12:49,040 --> 00:12:52,079
hilbert space which which everyone likes

367
00:12:51,440 --> 00:12:54,320
to

368
00:12:52,079 --> 00:12:55,680
uh comment that it's you know very large

369
00:12:54,320 --> 00:12:57,440
there's this huge place where we can

370
00:12:55,680 --> 00:12:59,599
store all of our big data

371
00:12:57,440 --> 00:13:00,560
um but that's not really completely true

372
00:12:59,600 --> 00:13:02,800
there's a sort of

373
00:13:00,560 --> 00:13:04,800
uh long narrow pipe that we have to

374
00:13:02,800 --> 00:13:07,519
squeeze it through to get it there

375
00:13:04,800 --> 00:13:09,359
so a lot of work in quantum machine

376
00:13:07,519 --> 00:13:10,160
learning assumes sort of a full wave

377
00:13:09,360 --> 00:13:12,800
function

378
00:13:10,160 --> 00:13:13,920
representation of an input feature

379
00:13:12,800 --> 00:13:16,639
vector

380
00:13:13,920 --> 00:13:19,040
but this is known in general to take

381
00:13:16,639 --> 00:13:20,639
linear time in the number of features

382
00:13:19,040 --> 00:13:22,079
or exponential time in the number of

383
00:13:20,639 --> 00:13:23,519
qubits

384
00:13:22,079 --> 00:13:25,359
so this can be a very limiting

385
00:13:23,519 --> 00:13:27,200
assumption especially in near-term

386
00:13:25,360 --> 00:13:29,920
practical approaches

387
00:13:27,200 --> 00:13:33,279
another thing is qram which is sort of a

388
00:13:29,920 --> 00:13:35,760
memory model for quantum computers

389
00:13:33,279 --> 00:13:36,320
uh this is sort of interesting from an

390
00:13:35,760 --> 00:13:39,360
oracle

391
00:13:36,320 --> 00:13:40,880
and uh complexity uh standpoint uh but

392
00:13:39,360 --> 00:13:43,199
again in terms of practicality

393
00:13:40,880 --> 00:13:44,880
uh this this can't really be done uh

394
00:13:43,199 --> 00:13:47,599
right now and maybe not

395
00:13:44,880 --> 00:13:49,040
ever and the third thing which i

396
00:13:47,600 --> 00:13:49,600
mentioned briefly before was quantum

397
00:13:49,040 --> 00:13:51,120
data

398
00:13:49,600 --> 00:13:52,720
which doesn't have this encoding problem

399
00:13:51,120 --> 00:13:55,920
because it's sort of uh

400
00:13:52,720 --> 00:13:57,920
the underlying data has nice physical

401
00:13:55,920 --> 00:14:00,000
assumptions such as like locality and

402
00:13:57,920 --> 00:14:01,760
other things

403
00:14:00,000 --> 00:14:03,040
so so you may be able to get away there

404
00:14:01,760 --> 00:14:04,639
but if you want to do any quantum

405
00:14:03,040 --> 00:14:07,839
machine learning on classical data

406
00:14:04,639 --> 00:14:11,120
you're always going to have this problem

407
00:14:07,839 --> 00:14:13,839
and these encodings have been studied a

408
00:14:11,120 --> 00:14:16,160
little bit but not too extensively

409
00:14:13,839 --> 00:14:18,720
this is a table taken from the very nice

410
00:14:16,160 --> 00:14:20,240
book of maria and francesco

411
00:14:18,720 --> 00:14:22,959
and one example that they have here

412
00:14:20,240 --> 00:14:25,920
again is the full wave function encoding

413
00:14:22,959 --> 00:14:25,920
where you're encoding

414
00:14:26,639 --> 00:14:32,880
each feature into the amplitude

415
00:14:29,920 --> 00:14:33,439
and you have this logarithmic number of

416
00:14:32,880 --> 00:14:35,920
qubits

417
00:14:33,440 --> 00:14:37,680
but but exponential depth and in the

418
00:14:35,920 --> 00:14:38,800
table there's other encodings that they

419
00:14:37,680 --> 00:14:40,479
consider

420
00:14:38,800 --> 00:14:41,839
i won't go through them all but just to

421
00:14:40,480 --> 00:14:43,440
show you that

422
00:14:41,839 --> 00:14:46,320
this has been thought about a little bit

423
00:14:43,440 --> 00:14:50,959
but but not too much

424
00:14:46,320 --> 00:14:53,279
one nice recent paper defines this

425
00:14:50,959 --> 00:14:55,760
tensor product encoding where they're

426
00:14:53,279 --> 00:14:55,760
taking

427
00:14:56,000 --> 00:14:59,600
cosine and sine of an input feature

428
00:14:58,000 --> 00:15:00,720
vector and they're doing a tensor

429
00:14:59,600 --> 00:15:03,760
product encoding

430
00:15:00,720 --> 00:15:04,800
over all of the qubits so here if you

431
00:15:03,760 --> 00:15:07,040
had

432
00:15:04,800 --> 00:15:08,079
capital n features you would encode them

433
00:15:07,040 --> 00:15:11,760
into capital and

434
00:15:08,079 --> 00:15:13,680
qubits so it's one feature per qubits

435
00:15:11,760 --> 00:15:15,519
and and this is another type of encoding

436
00:15:13,680 --> 00:15:18,399
that that can be used uh

437
00:15:15,519 --> 00:15:19,760
in a quantum machine learning model um

438
00:15:18,399 --> 00:15:20,720
and when you think about it you can sort

439
00:15:19,760 --> 00:15:22,800
of get creative

440
00:15:20,720 --> 00:15:24,480
with with how you do this representation

441
00:15:22,800 --> 00:15:25,920
so this is the example of the dense

442
00:15:24,480 --> 00:15:28,399
angle encoding

443
00:15:25,920 --> 00:15:30,240
that we used in the example earlier so

444
00:15:28,399 --> 00:15:32,320
here we still have cosine and sine

445
00:15:30,240 --> 00:15:34,079
but you could also add this exponential

446
00:15:32,320 --> 00:15:36,720
term and you can pack sort of

447
00:15:34,079 --> 00:15:38,839
two features into one cubit so that's

448
00:15:36,720 --> 00:15:40,959
why we call this the the dense angle

449
00:15:38,839 --> 00:15:42,480
encoding

450
00:15:40,959 --> 00:15:44,560
and this is the same image that we saw

451
00:15:42,480 --> 00:15:46,320
earlier and if you really think about it

452
00:15:44,560 --> 00:15:49,680
you can generalize this to any sort of

453
00:15:46,320 --> 00:15:51,839
l2 normalizable functions

454
00:15:49,680 --> 00:15:53,040
so we don't have to use cosine or sine

455
00:15:51,839 --> 00:15:53,600
there's nothing really special about

456
00:15:53,040 --> 00:15:55,759
that

457
00:15:53,600 --> 00:15:58,160
other than it's the sort of one of the

458
00:15:55,759 --> 00:16:00,240
standard representations of the qubits

459
00:15:58,160 --> 00:16:02,719
but we could use any functional form

460
00:16:00,240 --> 00:16:04,639
here as long as it defines a valid

461
00:16:02,720 --> 00:16:06,800
quantum state

462
00:16:04,639 --> 00:16:08,000
and the important and interesting thing

463
00:16:06,800 --> 00:16:10,319
is what functions we

464
00:16:08,000 --> 00:16:11,600
use sort of directly determines what

465
00:16:10,320 --> 00:16:14,959
decision boundaries

466
00:16:11,600 --> 00:16:16,399
the model is capable of learning and for

467
00:16:14,959 --> 00:16:17,920
the model we're considering

468
00:16:16,399 --> 00:16:19,360
again you can figure out what the

469
00:16:17,920 --> 00:16:22,399
decision boundary is at least in

470
00:16:19,360 --> 00:16:24,480
principle by solving this equation

471
00:16:22,399 --> 00:16:25,839
and for the single cubic classifier it's

472
00:16:24,480 --> 00:16:27,839
it's not bad to do

473
00:16:25,839 --> 00:16:30,480
you can write it down in terms of the

474
00:16:27,839 --> 00:16:33,519
optimal matrix elements

475
00:16:30,480 --> 00:16:34,880
the sort of variables in this equation

476
00:16:33,519 --> 00:16:36,800
are important

477
00:16:34,880 --> 00:16:38,959
but what is important is that it depends

478
00:16:36,800 --> 00:16:41,279
on the underlying data encoding

479
00:16:38,959 --> 00:16:43,680
so row zero zero depends on the function

480
00:16:41,279 --> 00:16:45,839
f here and row zero one depends on f and

481
00:16:43,680 --> 00:16:45,839
g

482
00:16:46,079 --> 00:16:50,638
um and this is two examples of two

483
00:16:49,120 --> 00:16:52,480
different encodings

484
00:16:50,639 --> 00:16:53,920
and the type of decision boundaries that

485
00:16:52,480 --> 00:16:56,079
you can learn

486
00:16:53,920 --> 00:16:57,680
so i'm not trying to hypnotize you here

487
00:16:56,079 --> 00:17:00,239
but i'm just trying to

488
00:16:57,680 --> 00:17:01,599
to emphasize this point that uh on the

489
00:17:00,240 --> 00:17:02,560
left for the wave function coding you

490
00:17:01,600 --> 00:17:04,959
can see

491
00:17:02,560 --> 00:17:06,159
the the decision boundaries you're

492
00:17:04,959 --> 00:17:08,480
capable of learning

493
00:17:06,160 --> 00:17:09,360
are distinctly different from the ones

494
00:17:08,480 --> 00:17:10,640
on the right

495
00:17:09,359 --> 00:17:13,119
uh which is where you're using this

496
00:17:10,640 --> 00:17:16,319
dunce ankle encoding okay

497
00:17:13,119 --> 00:17:18,239
so that's uh one interesting thing about

498
00:17:16,319 --> 00:17:19,839
uh this this type of encoding how you

499
00:17:18,240 --> 00:17:22,880
represent your data

500
00:17:19,839 --> 00:17:25,438
um uh let me skip this

501
00:17:22,880 --> 00:17:26,959
this is just a side note the the other

502
00:17:25,439 --> 00:17:29,760
interesting points

503
00:17:26,959 --> 00:17:30,720
you can ask is for a given encoding how

504
00:17:29,760 --> 00:17:34,000
robust is

505
00:17:30,720 --> 00:17:35,200
that to noise to a particular noise

506
00:17:34,000 --> 00:17:37,200
model

507
00:17:35,200 --> 00:17:38,320
and this is the the main question that

508
00:17:37,200 --> 00:17:41,360
we're focusing on

509
00:17:38,320 --> 00:17:43,039
in our work so what do i mean by

510
00:17:41,360 --> 00:17:46,159
robustness

511
00:17:43,039 --> 00:17:47,679
so why hats of rosabex denote the

512
00:17:46,160 --> 00:17:49,360
predicted label by the quantum

513
00:17:47,679 --> 00:17:52,559
classifier

514
00:17:49,360 --> 00:17:54,000
where there is no noise and then for

515
00:17:52,559 --> 00:17:57,200
some noise channel

516
00:17:54,000 --> 00:17:59,200
script e we say that the classifier is

517
00:17:57,200 --> 00:18:01,919
robust to this noise channel

518
00:17:59,200 --> 00:18:03,600
if and only if the following holds so

519
00:18:01,919 --> 00:18:04,080
what this equation is saying is on the

520
00:18:03,600 --> 00:18:06,559
left

521
00:18:04,080 --> 00:18:08,399
is the prediction in the noisy case and

522
00:18:06,559 --> 00:18:10,240
we want that to match the prediction in

523
00:18:08,400 --> 00:18:12,960
the noiseless case

524
00:18:10,240 --> 00:18:14,880
okay so you don't need the measurement

525
00:18:12,960 --> 00:18:15,760
statistics or even the state to be the

526
00:18:14,880 --> 00:18:17,600
same

527
00:18:15,760 --> 00:18:18,879
all you really want for the purposes of

528
00:18:17,600 --> 00:18:22,399
classification

529
00:18:18,880 --> 00:18:26,000
is for the predictions to be the same

530
00:18:22,400 --> 00:18:26,559
so this is interesting for several

531
00:18:26,000 --> 00:18:30,480
reasons

532
00:18:26,559 --> 00:18:30,480
it's related to the idea of fixed points

533
00:18:34,160 --> 00:18:37,440
sorry i lost my mic it's related to the

534
00:18:35,760 --> 00:18:39,520
idea of fixed points

535
00:18:37,440 --> 00:18:42,000
of quantum channels but it's sort of a

536
00:18:39,520 --> 00:18:44,480
generalization of that

537
00:18:42,000 --> 00:18:46,320
if if you have a fixed point it's

538
00:18:44,480 --> 00:18:48,000
definitely a robust point

539
00:18:46,320 --> 00:18:50,639
but a robust point is definitely not a

540
00:18:48,000 --> 00:18:50,640
fixed point

541
00:18:51,039 --> 00:18:57,520
so i thought i would give two or maybe

542
00:18:54,799 --> 00:18:59,840
three slides on a very brief background

543
00:18:57,520 --> 00:19:02,160
about noise and quantum systems

544
00:18:59,840 --> 00:19:03,439
so i can go through this quickly but

545
00:19:02,160 --> 00:19:05,760
just to give you an idea

546
00:19:03,440 --> 00:19:07,280
so noise occurs because we can't

547
00:19:05,760 --> 00:19:09,200
completely isolate

548
00:19:07,280 --> 00:19:10,399
our quantum computers or our quantum

549
00:19:09,200 --> 00:19:12,480
systems they're always

550
00:19:10,400 --> 00:19:14,480
interacting with the environment so we

551
00:19:12,480 --> 00:19:15,200
have unitary evolution not just acting

552
00:19:14,480 --> 00:19:17,200
on

553
00:19:15,200 --> 00:19:18,400
the principle system that is our quantum

554
00:19:17,200 --> 00:19:20,400
computer say

555
00:19:18,400 --> 00:19:22,240
but also on environmental variables as

556
00:19:20,400 --> 00:19:25,440
well and the thing we measure

557
00:19:22,240 --> 00:19:27,120
is what you get after tracing out the

558
00:19:25,440 --> 00:19:30,400
environment

559
00:19:27,120 --> 00:19:32,559
and we often use this equivalent but

560
00:19:30,400 --> 00:19:34,080
maybe more convenient operator sum

561
00:19:32,559 --> 00:19:36,720
representation

562
00:19:34,080 --> 00:19:38,559
which is where you define these cross

563
00:19:36,720 --> 00:19:40,320
operators e sub k

564
00:19:38,559 --> 00:19:41,600
which satisfy this equation at the

565
00:19:40,320 --> 00:19:44,480
bottom

566
00:19:41,600 --> 00:19:46,639
and these define um a particular quantum

567
00:19:44,480 --> 00:19:49,200
channel

568
00:19:46,640 --> 00:19:50,320
and there's sort of uh quantum channels

569
00:19:49,200 --> 00:19:52,960
that are

570
00:19:50,320 --> 00:19:53,520
commonly studied uh just to give you an

571
00:19:52,960 --> 00:19:55,520
idea

572
00:19:53,520 --> 00:19:57,120
these are some which have uh nice

573
00:19:55,520 --> 00:19:57,918
graphical representations to get an

574
00:19:57,120 --> 00:20:00,559
intuition

575
00:19:57,919 --> 00:20:02,080
so on the left is depolarizing noise and

576
00:20:00,559 --> 00:20:02,960
you can think about this for a single

577
00:20:02,080 --> 00:20:05,120
qubits as

578
00:20:02,960 --> 00:20:07,039
uniformly contracting uh the bloch

579
00:20:05,120 --> 00:20:09,199
sphere inward

580
00:20:07,039 --> 00:20:11,760
so the radial points stay the same but

581
00:20:09,200 --> 00:20:14,240
the uh the radius itself is shrinking

582
00:20:11,760 --> 00:20:15,919
inwards towards the origin

583
00:20:14,240 --> 00:20:17,679
you also have dephasing noise the

584
00:20:15,919 --> 00:20:21,760
phasing is

585
00:20:17,679 --> 00:20:24,960
contracting towards the z-axis

586
00:20:21,760 --> 00:20:26,559
or the computational basis so this is

587
00:20:24,960 --> 00:20:29,760
sort of making a qubit

588
00:20:26,559 --> 00:20:31,280
a bit there's a

589
00:20:29,760 --> 00:20:34,158
polynoise which is a slight

590
00:20:31,280 --> 00:20:37,039
generalization of depolarizing noise

591
00:20:34,159 --> 00:20:39,440
where you can have different

592
00:20:37,039 --> 00:20:41,280
deformations along different axes

593
00:20:39,440 --> 00:20:42,559
so the x y and z axes of the block

594
00:20:41,280 --> 00:20:44,639
sphere

595
00:20:42,559 --> 00:20:46,559
and the one i want to mention because

596
00:20:44,640 --> 00:20:48,000
it's important for the result that we

597
00:20:46,559 --> 00:20:50,480
have later on

598
00:20:48,000 --> 00:20:52,080
is the amplitude damping channel which

599
00:20:50,480 --> 00:20:54,799
is

600
00:20:52,080 --> 00:20:57,439
sort of this flow towards the ground

601
00:20:54,799 --> 00:20:59,360
states of the qubit

602
00:20:57,440 --> 00:21:00,640
and that's represented in this picture

603
00:20:59,360 --> 00:21:03,520
here

604
00:21:00,640 --> 00:21:03,840
okay so that's a very quick crash course

605
00:21:03,520 --> 00:21:07,039
and

606
00:21:03,840 --> 00:21:09,918
um in noise channels um

607
00:21:07,039 --> 00:21:11,200
and you can consider two regimes maybe

608
00:21:09,919 --> 00:21:12,159
in the interest of time i'll skip this

609
00:21:11,200 --> 00:21:14,000
slide as well

610
00:21:12,159 --> 00:21:16,240
all i'm saying here is you can consider

611
00:21:14,000 --> 00:21:18,240
uh the regime in which

612
00:21:16,240 --> 00:21:19,520
noise only acts during the unitary

613
00:21:18,240 --> 00:21:21,440
evolution

614
00:21:19,520 --> 00:21:24,158
and you can imagine this is slightly

615
00:21:21,440 --> 00:21:25,919
easier to prove things in

616
00:21:24,159 --> 00:21:27,919
and that's what most of our results are

617
00:21:25,919 --> 00:21:28,400
in and the more realistic case is where

618
00:21:27,919 --> 00:21:31,520
noise

619
00:21:28,400 --> 00:21:32,880
also happens during the encoding as well

620
00:21:31,520 --> 00:21:36,158
so that's that's on the bottom of the

621
00:21:32,880 --> 00:21:38,799
slide that's all i'm saying here

622
00:21:36,159 --> 00:21:40,000
okay so what's uh what's interesting is

623
00:21:38,799 --> 00:21:42,240
we can show that

624
00:21:40,000 --> 00:21:43,600
this class of this classifier that we

625
00:21:42,240 --> 00:21:46,159
defined is quantum

626
00:21:43,600 --> 00:21:47,760
neural network classifier is robust to

627
00:21:46,159 --> 00:21:50,880
poly noise

628
00:21:47,760 --> 00:21:52,480
if this condition here is satisfied the

629
00:21:50,880 --> 00:21:54,159
probability of a bit flip and the

630
00:21:52,480 --> 00:21:57,440
probability of a bit phase flip

631
00:21:54,159 --> 00:21:59,600
is less than or equal to a half

632
00:21:57,440 --> 00:22:01,200
so i debated whether or not to go

633
00:21:59,600 --> 00:22:02,080
through the proof but it's actually very

634
00:22:01,200 --> 00:22:04,400
simple and

635
00:22:02,080 --> 00:22:06,480
and just to give you an idea of how to

636
00:22:04,400 --> 00:22:09,840
show this i thought i would

637
00:22:06,480 --> 00:22:11,919
um so again we define robustness as

638
00:22:09,840 --> 00:22:13,600
the classification in the noisy case

639
00:22:11,919 --> 00:22:15,440
being the same as the classification in

640
00:22:13,600 --> 00:22:17,280
the noise bliss case

641
00:22:15,440 --> 00:22:18,480
so on the on the left hand side of this

642
00:22:17,280 --> 00:22:21,039
equation and the proof

643
00:22:18,480 --> 00:22:22,720
we write down what the prediction is for

644
00:22:21,039 --> 00:22:24,879
the noisy case

645
00:22:22,720 --> 00:22:26,159
so here you have the trace of pi zero

646
00:22:24,880 --> 00:22:29,919
and then the evolved states

647
00:22:26,159 --> 00:22:32,080
put with the noise channel acting on it

648
00:22:29,919 --> 00:22:33,520
and you can use just simple properties

649
00:22:32,080 --> 00:22:36,639
of the trace

650
00:22:33,520 --> 00:22:40,000
linearity here to write this in terms of

651
00:22:36,640 --> 00:22:43,760
expectation in the noise list case

652
00:22:40,000 --> 00:22:46,559
so here it also looks like something

653
00:22:43,760 --> 00:22:49,520
that's expectation in the noiseless case

654
00:22:46,559 --> 00:22:52,240
and you can just do a substitution

655
00:22:49,520 --> 00:22:52,840
and massage things a little bit to have

656
00:22:52,240 --> 00:22:56,400
the

657
00:22:52,840 --> 00:22:57,439
noiseless classification appear so

658
00:22:56,400 --> 00:23:00,880
that's the trace

659
00:22:57,440 --> 00:23:02,960
of pi zero rho tilde x and now you just

660
00:23:00,880 --> 00:23:04,640
need to argue as follows

661
00:23:02,960 --> 00:23:06,320
so so this is the the guy that

662
00:23:04,640 --> 00:23:07,840
determines what our classification is in

663
00:23:06,320 --> 00:23:09,439
the noiseless case

664
00:23:07,840 --> 00:23:11,360
and if that's greater than or equal to a

665
00:23:09,440 --> 00:23:11,760
half then all you do is substitute into

666
00:23:11,360 --> 00:23:14,240
that

667
00:23:11,760 --> 00:23:15,919
uh that into this equation and you see

668
00:23:14,240 --> 00:23:17,760
that the noisy prediction is also

669
00:23:15,919 --> 00:23:19,440
greater than or equal to a half

670
00:23:17,760 --> 00:23:20,720
um so you have robustness here that's

671
00:23:19,440 --> 00:23:23,039
that's the definition that's how we

672
00:23:20,720 --> 00:23:24,960
define robustness

673
00:23:23,039 --> 00:23:27,120
and then the other case you can flip it

674
00:23:24,960 --> 00:23:30,000
around and if it's less than half you

675
00:23:27,120 --> 00:23:30,000
can show the same thing

676
00:23:31,440 --> 00:23:35,280
so that's one result and this is an

677
00:23:34,640 --> 00:23:38,240
example

678
00:23:35,280 --> 00:23:40,080
illustrating that so on the left uh you

679
00:23:38,240 --> 00:23:43,360
we have a poly channel

680
00:23:40,080 --> 00:23:44,559
in which px plus py is is less than a

681
00:23:43,360 --> 00:23:47,520
half

682
00:23:44,559 --> 00:23:48,559
and here the points are colored green if

683
00:23:47,520 --> 00:23:50,799
they are robust

684
00:23:48,559 --> 00:23:53,360
that is the noisy classification and

685
00:23:50,799 --> 00:23:56,320
noise list classifications agree

686
00:23:53,360 --> 00:23:57,520
and all the points are robust here as we

687
00:23:56,320 --> 00:23:59,918
would expect

688
00:23:57,520 --> 00:24:00,960
from the last results and on the right

689
00:23:59,919 --> 00:24:04,000
you're seeing an image

690
00:24:00,960 --> 00:24:06,880
where that condition

691
00:24:04,000 --> 00:24:07,200
is not satisfied so here that p x plus p

692
00:24:06,880 --> 00:24:09,600
y

693
00:24:07,200 --> 00:24:10,640
is is not less than or equal to a half

694
00:24:09,600 --> 00:24:13,520
um and we have

695
00:24:10,640 --> 00:24:14,320
uh points that are misclassified here so

696
00:24:13,520 --> 00:24:19,600
those are the

697
00:24:14,320 --> 00:24:21,360
red x's um i hope that's visible

698
00:24:19,600 --> 00:24:23,120
so it's interesting that not all points

699
00:24:21,360 --> 00:24:24,639
are misclassified only some of them are

700
00:24:23,120 --> 00:24:27,199
and i'll get to that

701
00:24:24,640 --> 00:24:29,520
in a few slides i'll skip these

702
00:24:27,200 --> 00:24:31,120
corollaries this is just saying

703
00:24:29,520 --> 00:24:34,400
you know you might ask what's special

704
00:24:31,120 --> 00:24:37,039
about px and py

705
00:24:34,400 --> 00:24:37,840
that's sort of an asymmetry the reason

706
00:24:37,039 --> 00:24:39,120
they're

707
00:24:37,840 --> 00:24:42,158
those are the ones in the condition is

708
00:24:39,120 --> 00:24:43,360
because we're measuring in the z basis

709
00:24:42,159 --> 00:24:44,720
so you could flip this around and

710
00:24:43,360 --> 00:24:46,719
measure in a different basis and then

711
00:24:44,720 --> 00:24:48,400
change around what the condition is

712
00:24:46,720 --> 00:24:50,559
and this might be useful if you have a

713
00:24:48,400 --> 00:24:53,679
noise model for a particular computer

714
00:24:50,559 --> 00:24:54,799
where one of pxpy or pz is high

715
00:24:53,679 --> 00:24:57,799
then you can measure in a different

716
00:24:54,799 --> 00:25:00,080
basis and get better classification

717
00:24:57,799 --> 00:25:02,158
results um

718
00:25:00,080 --> 00:25:04,720
you can also show we can show

719
00:25:02,159 --> 00:25:06,960
unconditional robustness due to phasing

720
00:25:04,720 --> 00:25:08,960
i won't go through the proof of this but

721
00:25:06,960 --> 00:25:10,880
just to mention it

722
00:25:08,960 --> 00:25:13,360
you can also show unconditional

723
00:25:10,880 --> 00:25:14,640
robustness to depolarizing noise

724
00:25:13,360 --> 00:25:16,639
and this is nice because you can have

725
00:25:14,640 --> 00:25:18,000
depolarizing noise acting at any point

726
00:25:16,640 --> 00:25:20,559
in the circuit

727
00:25:18,000 --> 00:25:21,520
so as far as this quantum classifier is

728
00:25:20,559 --> 00:25:23,678
concerned

729
00:25:21,520 --> 00:25:25,520
depolarizing noise is sort of a joke it

730
00:25:23,679 --> 00:25:29,279
doesn't really matter

731
00:25:25,520 --> 00:25:29,679
so that's nice i'll skip the proof for

732
00:25:29,279 --> 00:25:32,159
this

733
00:25:29,679 --> 00:25:33,919
the one that i want to highlight briefly

734
00:25:32,159 --> 00:25:36,159
in the remaining time

735
00:25:33,919 --> 00:25:37,520
is the case for the amplitude damping

736
00:25:36,159 --> 00:25:39,840
channel

737
00:25:37,520 --> 00:25:41,440
so this is where the underlying data

738
00:25:39,840 --> 00:25:43,678
encoding really shows up

739
00:25:41,440 --> 00:25:45,919
and matters a lot so the result that

740
00:25:43,679 --> 00:25:48,080
we're able to show here is robustness

741
00:25:45,919 --> 00:25:49,279
if and only if this equation in the

742
00:25:48,080 --> 00:25:51,840
middle of slide

743
00:25:49,279 --> 00:25:52,960
holds so on the right hand side the

744
00:25:51,840 --> 00:25:55,439
lowercase p

745
00:25:52,960 --> 00:25:57,440
is the strength of the noise channel and

746
00:25:55,440 --> 00:26:00,000
on the left hand side

747
00:25:57,440 --> 00:26:00,720
the condition involves the functions f

748
00:26:00,000 --> 00:26:03,440
and g

749
00:26:00,720 --> 00:26:05,679
which define the data encoding and the

750
00:26:03,440 --> 00:26:10,080
optimal unitary parameters

751
00:26:05,679 --> 00:26:13,039
or matrix elements u 1 0 u11

752
00:26:10,080 --> 00:26:13,039
and u 1 0.

753
00:26:13,279 --> 00:26:16,320
so that's the condition for robustness

754
00:26:15,760 --> 00:26:18,720
and

755
00:26:16,320 --> 00:26:20,879
you might ask if it's possible to find

756
00:26:18,720 --> 00:26:22,000
any functions f and g which satisfy this

757
00:26:20,880 --> 00:26:25,360
condition

758
00:26:22,000 --> 00:26:25,760
and the answer is uh yes um and the

759
00:26:25,360 --> 00:26:29,120
reason

760
00:26:25,760 --> 00:26:32,480
is related to uh fixed points and uh

761
00:26:29,120 --> 00:26:34,239
i'll explain that uh in in a minute

762
00:26:32,480 --> 00:26:35,760
uh but first i just want to give some

763
00:26:34,240 --> 00:26:38,720
illustrations of this

764
00:26:35,760 --> 00:26:40,879
um and again to highlight where the

765
00:26:38,720 --> 00:26:42,559
underlying encoding matters

766
00:26:40,880 --> 00:26:44,559
so first let's consider the wave

767
00:26:42,559 --> 00:26:45,678
function encoding so this is the

768
00:26:44,559 --> 00:26:48,240
standard one

769
00:26:45,679 --> 00:26:49,760
that a lot of people consider and here

770
00:26:48,240 --> 00:26:51,120
again i'm using the same notation the

771
00:26:49,760 --> 00:26:54,400
points are colored green

772
00:26:51,120 --> 00:26:56,799
if they are robust and they're red x's

773
00:26:54,400 --> 00:26:58,400
if they are not robust so here the

774
00:26:56,799 --> 00:27:01,440
strength of the channel is zero

775
00:26:58,400 --> 00:27:02,960
so this is the noise list case so every

776
00:27:01,440 --> 00:27:05,440
point is robust

777
00:27:02,960 --> 00:27:07,360
now we start increasing the noise so now

778
00:27:05,440 --> 00:27:09,120
the strength of the channel is 0.1

779
00:27:07,360 --> 00:27:10,719
and we start getting points that are

780
00:27:09,120 --> 00:27:13,279
misclassified

781
00:27:10,720 --> 00:27:14,960
so a certain set of points which are

782
00:27:13,279 --> 00:27:15,520
along the diagonal here if you can't see

783
00:27:14,960 --> 00:27:17,440
it

784
00:27:15,520 --> 00:27:19,600
are the ones being misclassified and as

785
00:27:17,440 --> 00:27:23,760
we increase the strength of the channel

786
00:27:19,600 --> 00:27:26,000
this set of not robust points grows

787
00:27:23,760 --> 00:27:28,320
and keeps growing and keeps growing

788
00:27:26,000 --> 00:27:32,159
until it saturates

789
00:27:28,320 --> 00:27:35,360
okay so one thing to note here is

790
00:27:32,159 --> 00:27:37,039
you still have points that are robust

791
00:27:35,360 --> 00:27:38,559
even though you have this noise channel

792
00:27:37,039 --> 00:27:41,840
acting

793
00:27:38,559 --> 00:27:43,360
so that's important now i want to show

794
00:27:41,840 --> 00:27:44,879
the same example but with a different

795
00:27:43,360 --> 00:27:47,360
data encoding

796
00:27:44,880 --> 00:27:49,600
this is now the dense angle encoding

797
00:27:47,360 --> 00:27:52,158
which we defined previously

798
00:27:49,600 --> 00:27:53,039
and here as we in increase the strength

799
00:27:52,159 --> 00:27:54,240
of the channel

800
00:27:53,039 --> 00:27:56,480
you can see that you still get

801
00:27:54,240 --> 00:28:00,320
misclassified points but it's a

802
00:27:56,480 --> 00:28:02,159
different set than what we saw with the

803
00:28:00,320 --> 00:28:04,879
previous encoding

804
00:28:02,159 --> 00:28:06,720
so now you have points that are getting

805
00:28:04,880 --> 00:28:08,720
misclassified here

806
00:28:06,720 --> 00:28:11,200
and as i keep increasing the strength of

807
00:28:08,720 --> 00:28:14,399
the channel until it saturates

808
00:28:11,200 --> 00:28:16,399
so again same behavior but the

809
00:28:14,399 --> 00:28:18,320
interesting and crucial thing is that

810
00:28:16,399 --> 00:28:21,439
this is a different set of points

811
00:28:18,320 --> 00:28:23,279
um so so the robustness of the model

812
00:28:21,440 --> 00:28:24,799
um the robustness of the classifier

813
00:28:23,279 --> 00:28:26,159
really depends on the underlying data

814
00:28:24,799 --> 00:28:29,120
and coding

815
00:28:26,159 --> 00:28:29,840
as well as the learnability so i'll

816
00:28:29,120 --> 00:28:32,399
mention this

817
00:28:29,840 --> 00:28:34,399
very briefly you can you can always

818
00:28:32,399 --> 00:28:36,719
achieve a robust data encoding

819
00:28:34,399 --> 00:28:37,439
um for any trace preserving quantum

820
00:28:36,720 --> 00:28:39,440
channel

821
00:28:37,440 --> 00:28:40,880
um and this is because any trace

822
00:28:39,440 --> 00:28:44,960
preserving quantum channel

823
00:28:40,880 --> 00:28:47,840
um always has at least one fixed point

824
00:28:44,960 --> 00:28:49,679
so if you're really such on robustness

825
00:28:47,840 --> 00:28:51,279
you can encode everything into that

826
00:28:49,679 --> 00:28:53,120
fixed point

827
00:28:51,279 --> 00:28:55,279
however this is where that sort of

828
00:28:53,120 --> 00:28:57,918
qualitative trade-off comes in

829
00:28:55,279 --> 00:28:59,760
from a machine learning perspective if

830
00:28:57,919 --> 00:29:00,720
you're encoding everything into the same

831
00:28:59,760 --> 00:29:02,240
points

832
00:29:00,720 --> 00:29:04,000
and trying to learn something about that

833
00:29:02,240 --> 00:29:05,679
data that's a very bad idea

834
00:29:04,000 --> 00:29:07,760
because you have you have no way to

835
00:29:05,679 --> 00:29:09,679
discern between the points

836
00:29:07,760 --> 00:29:11,760
from a quantum information perspective

837
00:29:09,679 --> 00:29:13,840
that's nice maybe because you know that

838
00:29:11,760 --> 00:29:15,760
they'll all at least be classified

839
00:29:13,840 --> 00:29:19,439
correctly

840
00:29:15,760 --> 00:29:21,360
correctly meaning they'll all be robust

841
00:29:19,440 --> 00:29:24,080
so that's an extreme case but it also

842
00:29:21,360 --> 00:29:25,918
holds for less extreme

843
00:29:24,080 --> 00:29:27,120
examples so there's this trade-off

844
00:29:25,919 --> 00:29:28,799
between sort of

845
00:29:27,120 --> 00:29:31,039
the more robustness you have and the

846
00:29:28,799 --> 00:29:35,200
less learnability you have

847
00:29:31,039 --> 00:29:38,158
and maybe vice versa this was a nice

848
00:29:35,200 --> 00:29:39,279
picture that's somewhat related to what

849
00:29:38,159 --> 00:29:41,679
i was just saying

850
00:29:39,279 --> 00:29:43,679
uh that i should have head up before i

851
00:29:41,679 --> 00:29:45,600
bored you with that explanation

852
00:29:43,679 --> 00:29:47,279
um but yeah so this is a nice picture

853
00:29:45,600 --> 00:29:50,719
this was made by brian so

854
00:29:47,279 --> 00:29:52,960
thanks brian um and

855
00:29:50,720 --> 00:29:54,960
uh this should have been skipped as you

856
00:29:52,960 --> 00:29:57,760
can see

857
00:29:54,960 --> 00:29:58,559
so uh just to conclude this the second

858
00:29:57,760 --> 00:30:00,879
uh part

859
00:29:58,559 --> 00:30:01,600
um and then then conclude the the whole

860
00:30:00,880 --> 00:30:04,159
talk

861
00:30:01,600 --> 00:30:05,918
in a few minutes um so encoding

862
00:30:04,159 --> 00:30:08,480
classical data is an important but

863
00:30:05,919 --> 00:30:11,440
understudy problem in the literature

864
00:30:08,480 --> 00:30:12,080
many speed ups and qml algorithms rely

865
00:30:11,440 --> 00:30:14,799
on this

866
00:30:12,080 --> 00:30:16,240
which which may not be plausible for

867
00:30:14,799 --> 00:30:18,080
practical purposes

868
00:30:16,240 --> 00:30:20,080
we saw that the data encoding directly

869
00:30:18,080 --> 00:30:21,439
determines the learnable decision

870
00:30:20,080 --> 00:30:24,559
boundaries

871
00:30:21,440 --> 00:30:26,399
and also determines the robustness

872
00:30:24,559 --> 00:30:28,158
so the classifier model itself

873
00:30:26,399 --> 00:30:30,000
independent of the encoding has some

874
00:30:28,159 --> 00:30:32,399
nice robustness

875
00:30:30,000 --> 00:30:34,840
and then for certain error models you

876
00:30:32,399 --> 00:30:38,719
can encode your data in such a way

877
00:30:34,840 --> 00:30:41,520
um that you can still ensure robustness

878
00:30:38,720 --> 00:30:42,960
while maybe perhaps maintaining

879
00:30:41,520 --> 00:30:44,240
learnability as well

880
00:30:42,960 --> 00:30:47,200
so that's the example we saw with the

881
00:30:44,240 --> 00:30:48,880
amplitude damping channel

882
00:30:47,200 --> 00:30:50,559
and again the last point a robust data

883
00:30:48,880 --> 00:30:53,600
encoding always exists

884
00:30:50,559 --> 00:30:55,039
just by virtue of fixed points but again

885
00:30:53,600 --> 00:30:58,240
likely at the expense of the

886
00:30:55,039 --> 00:30:58,240
learnability of the model

887
00:30:58,399 --> 00:31:01,760
okay so that's it now i just have one

888
00:31:00,399 --> 00:31:04,158
slide saying uh

889
00:31:01,760 --> 00:31:04,960
we we welcome your contributions so if

890
00:31:04,159 --> 00:31:07,519
anything i said

891
00:31:04,960 --> 00:31:09,200
sounded interesting at all i'd love to

892
00:31:07,519 --> 00:31:10,640
get in touch with you

893
00:31:09,200 --> 00:31:12,559
if you're more from a programming

894
00:31:10,640 --> 00:31:14,080
background we have a lot of programming

895
00:31:12,559 --> 00:31:16,080
that needs to be done

896
00:31:14,080 --> 00:31:17,279
if you're interested in research we'd

897
00:31:16,080 --> 00:31:20,480
love to hear your ideas

898
00:31:17,279 --> 00:31:23,279
we have ideas so this is

899
00:31:20,480 --> 00:31:24,559
our github you can connect with myself

900
00:31:23,279 --> 00:31:26,720
as well i'll be around for the rest of

901
00:31:24,559 --> 00:31:29,918
the day

902
00:31:26,720 --> 00:31:32,320
and yeah so i'd like to thank will zang

903
00:31:29,919 --> 00:31:34,720
the unitary fund and all of its sponsors

904
00:31:32,320 --> 00:31:35,918
for giving us support for niskayae i

905
00:31:34,720 --> 00:31:38,080
really want to thank

906
00:31:35,919 --> 00:31:39,840
these guys nick ezel youssef omula joe

907
00:31:38,080 --> 00:31:42,480
yasway and arkantiku

908
00:31:39,840 --> 00:31:43,678
for contributing a lot to niskaya

909
00:31:42,480 --> 00:31:46,240
upfront

910
00:31:43,679 --> 00:31:47,440
and others contributions that we got

911
00:31:46,240 --> 00:31:49,360
later down the line

912
00:31:47,440 --> 00:31:51,600
uh huge thanks to brian coyle who who

913
00:31:49,360 --> 00:31:53,918
started contributing to niskaya

914
00:31:51,600 --> 00:31:55,439
and then also worked with me on this uh

915
00:31:53,919 --> 00:31:57,360
this research part

916
00:31:55,440 --> 00:31:59,120
and again i met brian lester at foster

917
00:31:57,360 --> 00:32:01,519
so so that's nice

918
00:31:59,120 --> 00:32:03,439
and i'd also like to thank both nana liu

919
00:32:01,519 --> 00:32:07,120
and the nasa quail team

920
00:32:03,440 --> 00:32:10,240
for useful comments and

921
00:32:07,120 --> 00:32:11,760
on the robust data encodings work um and

922
00:32:10,240 --> 00:32:15,610
and thanks to you all for listening

923
00:32:11,760 --> 00:32:22,109
and i'd be happy to take any questions

924
00:32:15,610 --> 00:32:22,109
[Applause]

925
00:32:34,640 --> 00:32:36,720
you

