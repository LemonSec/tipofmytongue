1
00:00:05,279 --> 00:00:09,840
okay

2
00:00:06,640 --> 00:00:12,879
the floor is yours thank you

3
00:00:09,840 --> 00:00:14,960
so uh i come from the

4
00:00:12,880 --> 00:00:16,080
european center for medium range weather

5
00:00:14,960 --> 00:00:20,080
forecast

6
00:00:16,079 --> 00:00:23,198
and we have a pretty large

7
00:00:20,080 --> 00:00:25,919
archive of our forecast

8
00:00:23,199 --> 00:00:26,880
in fact is the largest in the world and

9
00:00:25,920 --> 00:00:29,840
so

10
00:00:26,880 --> 00:00:31,840
extracting data from there is a quite

11
00:00:29,840 --> 00:00:35,120
tricky

12
00:00:31,840 --> 00:00:38,480
what do we do we are a

13
00:00:35,120 --> 00:00:43,599
an international organization

14
00:00:38,480 --> 00:00:46,398
financed by 34 member states

15
00:00:43,600 --> 00:00:48,719
and we perform operational services

16
00:00:46,399 --> 00:00:48,719
namely

17
00:00:48,800 --> 00:00:53,360
weather forecast and we support the

18
00:00:51,760 --> 00:00:58,239
national weather services

19
00:00:53,360 --> 00:01:01,600
in the exploitation of our data

20
00:00:58,239 --> 00:01:03,760
we do research in this field and we also

21
00:01:01,600 --> 00:01:07,920
provide us some

22
00:01:03,760 --> 00:01:10,560
copernicus services so free access to

23
00:01:07,920 --> 00:01:11,119
some of our data and some data that we

24
00:01:10,560 --> 00:01:14,240
compute

25
00:01:11,119 --> 00:01:17,360
especially for that obviously

26
00:01:14,240 --> 00:01:18,399
this is not just computing we also have

27
00:01:17,360 --> 00:01:22,000
to acquire

28
00:01:18,400 --> 00:01:23,040
quite a lot of data in fact weather data

29
00:01:22,000 --> 00:01:26,159
for us are

30
00:01:23,040 --> 00:01:28,880
two main categories one is made

31
00:01:26,159 --> 00:01:30,720
by observation so we collect data from

32
00:01:28,880 --> 00:01:32,320
satellites from radar from weather

33
00:01:30,720 --> 00:01:35,439
station and

34
00:01:32,320 --> 00:01:38,639
whatever we can fetch and

35
00:01:35,439 --> 00:01:41,199
we archive those data in our system

36
00:01:38,640 --> 00:01:42,000
and then we use those observation to

37
00:01:41,200 --> 00:01:44,159
create a

38
00:01:42,000 --> 00:01:45,520
a an interpolation of the status of the

39
00:01:44,159 --> 00:01:48,560
atmosphere

40
00:01:45,520 --> 00:01:52,240
then from that interpolation

41
00:01:48,560 --> 00:01:54,960
we start our numerical models that

42
00:01:52,240 --> 00:01:55,439
are coupled models so we have a inertia

43
00:01:54,960 --> 00:01:58,798
model

44
00:01:55,439 --> 00:02:00,320
and a sea ice model a land model and an

45
00:01:58,799 --> 00:02:01,840
atmosphere one

46
00:02:00,320 --> 00:02:04,320
obviously we are more interested in the

47
00:02:01,840 --> 00:02:08,479
atmosphere

48
00:02:04,320 --> 00:02:11,840
this model is producing a

49
00:02:08,479 --> 00:02:14,400
simulated uh set of variables for each

50
00:02:11,840 --> 00:02:15,680
cell of the our grid

51
00:02:14,400 --> 00:02:17,760
[Music]

52
00:02:15,680 --> 00:02:19,520
obviously the variables are temperature

53
00:02:17,760 --> 00:02:23,840
pressure uh wind spin

54
00:02:19,520 --> 00:02:23,840
and direction humidity and so on

55
00:02:23,920 --> 00:02:30,079
we are interested in both the 2d fields

56
00:02:27,280 --> 00:02:30,959
mainly on the lens surface and three

57
00:02:30,080 --> 00:02:34,000
fields like

58
00:02:30,959 --> 00:02:34,000
the wall atmosphere

59
00:02:34,840 --> 00:02:40,959
ddd computational cost is a

60
00:02:38,000 --> 00:02:41,440
linear in the number of cells so we have

61
00:02:40,959 --> 00:02:44,160
to

62
00:02:41,440 --> 00:02:44,560
somehow optimize the data grid that we

63
00:02:44,160 --> 00:02:48,840
use

64
00:02:44,560 --> 00:02:51,440
to reduce the computational cost

65
00:02:48,840 --> 00:02:54,160
uh in fact okay

66
00:02:51,440 --> 00:02:55,040
now uh i will forget for a while the

67
00:02:54,160 --> 00:02:58,079
observation i

68
00:02:55,040 --> 00:03:01,120
i will focus on the uh model output

69
00:02:58,080 --> 00:03:02,959
is the part that we really uh are

70
00:03:01,120 --> 00:03:07,280
required to archive and assess

71
00:03:02,959 --> 00:03:10,480
quickly for our users

72
00:03:07,280 --> 00:03:11,440
again the grid instead of keeping a

73
00:03:10,480 --> 00:03:15,440
regular

74
00:03:11,440 --> 00:03:17,760
lat long grid we optimized a little bit

75
00:03:15,440 --> 00:03:19,680
the computational cost by using a grid

76
00:03:17,760 --> 00:03:20,000
that is decreasing the number of points

77
00:03:19,680 --> 00:03:23,519
as

78
00:03:20,000 --> 00:03:26,959
we approach the polls uh we call that

79
00:03:23,519 --> 00:03:27,840
greedy the reduced uh gaussian octahedra

80
00:03:26,959 --> 00:03:30,879
agreed

81
00:03:27,840 --> 00:03:32,080
why octahedral because it's essentially

82
00:03:30,879 --> 00:03:35,518
based on

83
00:03:32,080 --> 00:03:36,720
an octahedron that is uh just outside

84
00:03:35,519 --> 00:03:40,159
the earth

85
00:03:36,720 --> 00:03:42,480
and so we can somehow have

86
00:03:40,159 --> 00:03:44,560
the number of points that we use for

87
00:03:42,480 --> 00:03:48,399
describing the

88
00:03:44,560 --> 00:03:50,560
each layer of the atmosphere um

89
00:03:48,400 --> 00:03:52,480
obviously this is really helpful from

90
00:03:50,560 --> 00:03:55,439
the computational point of view

91
00:03:52,480 --> 00:03:55,920
while uh all the geometric computation

92
00:03:55,439 --> 00:03:58,480
are

93
00:03:55,920 --> 00:04:00,399
somehow affected by the fact that we

94
00:03:58,480 --> 00:04:04,159
have to interpolate from this

95
00:04:00,400 --> 00:04:07,360
grid to the one that our user wants

96
00:04:04,159 --> 00:04:10,319
another aspect is the uh z level

97
00:04:07,360 --> 00:04:12,400
we cannot use a constant z level uh when

98
00:04:10,319 --> 00:04:15,599
we have a mountain obviously we are not

99
00:04:12,400 --> 00:04:19,759
interested in the atmosphere uh under

100
00:04:15,599 --> 00:04:23,039
the ground so we have the

101
00:04:19,759 --> 00:04:25,040
layers that are uh following the

102
00:04:23,040 --> 00:04:26,080
shape of the earth so the digital

103
00:04:25,040 --> 00:04:28,639
elevation model

104
00:04:26,080 --> 00:04:29,758
up to a certain pressure and then we go

105
00:04:28,639 --> 00:04:33,120
uh

106
00:04:29,759 --> 00:04:36,400
on constant layers so even on the

107
00:04:33,120 --> 00:04:39,759
we have to interpolate in a somehow

108
00:04:36,400 --> 00:04:43,359
custom way having

109
00:04:39,759 --> 00:04:44,400
those constraints we design the system

110
00:04:43,360 --> 00:04:47,040
that is

111
00:04:44,400 --> 00:04:48,799
able to scale with the resolution so the

112
00:04:47,040 --> 00:04:52,240
idea is that

113
00:04:48,800 --> 00:04:53,040
we want to provide a higher resolution

114
00:04:52,240 --> 00:04:56,400
simulation

115
00:04:53,040 --> 00:04:57,280
for our user but obviously increasing

116
00:04:56,400 --> 00:05:00,320
the resolution

117
00:04:57,280 --> 00:05:02,380
resolution is quite costly doubling the

118
00:05:00,320 --> 00:05:04,719
resolution usually

119
00:05:02,380 --> 00:05:07,280
[Music]

120
00:05:04,720 --> 00:05:08,240
costing us eight times the computational

121
00:05:07,280 --> 00:05:10,638
power

122
00:05:08,240 --> 00:05:12,479
to for the dimension one because we have

123
00:05:10,639 --> 00:05:14,800
to reduce the time sp

124
00:05:12,479 --> 00:05:16,240
time step to get the accurate

125
00:05:14,800 --> 00:05:19,600
calculation so

126
00:05:16,240 --> 00:05:20,400
uh the the the impact on the computation

127
00:05:19,600 --> 00:05:23,520
and on the

128
00:05:20,400 --> 00:05:27,120
output file is pretty heavy uh

129
00:05:23,520 --> 00:05:30,960
right now we are running our system at

130
00:05:27,120 --> 00:05:34,479
nine kilometer uh per cell

131
00:05:30,960 --> 00:05:39,120
so essentially every cell is 81 square

132
00:05:34,479 --> 00:05:42,719
kilometers on the global simulation

133
00:05:39,120 --> 00:05:46,000
this generate layers that are 6.6

134
00:05:42,720 --> 00:05:46,400
million of points roughly and so each

135
00:05:46,000 --> 00:05:51,199
field

136
00:05:46,400 --> 00:05:55,840
is stored in about 50 megabytes

137
00:05:51,199 --> 00:05:55,840
the problem is that we store

138
00:05:55,960 --> 00:06:01,840
137 layer for each

139
00:05:58,800 --> 00:06:05,400
variables and we

140
00:06:01,840 --> 00:06:06,799
perform 51 different simulation

141
00:06:05,400 --> 00:06:09,919
[Music]

142
00:06:06,800 --> 00:06:12,479
every twice a day to be honest

143
00:06:09,919 --> 00:06:13,919
and so essentially we generate millions

144
00:06:12,479 --> 00:06:17,520
of field every day

145
00:06:13,919 --> 00:06:20,479
so our archive is increasing nearly

146
00:06:17,520 --> 00:06:23,440
200 terabytes per day so in five days we

147
00:06:20,479 --> 00:06:27,039
had a petabyte

148
00:06:23,440 --> 00:06:30,800
so a little bit more in detail

149
00:06:27,039 --> 00:06:33,280
we perform every day twice a day

150
00:06:30,800 --> 00:06:34,160
so with two different timing at the

151
00:06:33,280 --> 00:06:38,479
midnight and

152
00:06:34,160 --> 00:06:42,080
mid at noon a simulation at 9 kilometers

153
00:06:38,479 --> 00:06:45,280
then we perform 50 simulation with a

154
00:06:42,080 --> 00:06:48,400
an ensemble of 50 simulation at at

155
00:06:45,280 --> 00:06:49,039
18 kilometers resolution again twice a

156
00:06:48,400 --> 00:06:51,919
day

157
00:06:49,039 --> 00:06:54,159
then we have a lower resolution but

158
00:06:51,919 --> 00:06:56,960
extended in time

159
00:06:54,160 --> 00:06:57,360
simulation and then we have also quite a

160
00:06:56,960 --> 00:06:59,280
lot of

161
00:06:57,360 --> 00:07:02,080
research activity that is also

162
00:06:59,280 --> 00:07:03,840
generating new datasets

163
00:07:02,080 --> 00:07:05,599
this is the amount of data that we

164
00:07:03,840 --> 00:07:09,599
distribute and

165
00:07:05,599 --> 00:07:12,080
in the in the last few years the

166
00:07:09,599 --> 00:07:13,680
amount of data has been increasing

167
00:07:12,080 --> 00:07:16,800
almost exponentially

168
00:07:13,680 --> 00:07:20,000
and this is our forecast so

169
00:07:16,800 --> 00:07:22,240
right now our simulation

170
00:07:20,000 --> 00:07:23,680
that is performing in one hour is

171
00:07:22,240 --> 00:07:27,280
generating roughly

172
00:07:23,680 --> 00:07:30,160
70 terabytes of data

173
00:07:27,280 --> 00:07:30,880
so it means that we are right to disk

174
00:07:30,160 --> 00:07:34,400
roughly

175
00:07:30,880 --> 00:07:36,479
19 gigabytes per second and this is

176
00:07:34,400 --> 00:07:38,318
our major constraints our model can

177
00:07:36,479 --> 00:07:41,440
scale way better

178
00:07:38,319 --> 00:07:42,800
we theoretically could already produce

179
00:07:41,440 --> 00:07:45,919
this amount of data but

180
00:07:42,800 --> 00:07:49,280
simply we cannot store it the the

181
00:07:45,919 --> 00:07:50,400
io that we are using today that is a

182
00:07:49,280 --> 00:07:53,039
power alpha system

183
00:07:50,400 --> 00:07:54,960
with luster and so on is not able to

184
00:07:53,039 --> 00:07:56,960
cope with that

185
00:07:54,960 --> 00:07:58,560
and we are working to improve on that

186
00:07:56,960 --> 00:08:00,638
size but in any case

187
00:07:58,560 --> 00:08:02,000
we are committed to increase to five

188
00:08:00,639 --> 00:08:05,440
kilometer resolution

189
00:08:02,000 --> 00:08:08,400
by the uh 2025

190
00:08:05,440 --> 00:08:09,919
so we are still working on that our

191
00:08:08,400 --> 00:08:13,039
carpeting facilities

192
00:08:09,919 --> 00:08:15,919
we have a redundant system with two

193
00:08:13,039 --> 00:08:15,919
supercomputers

194
00:08:16,080 --> 00:08:21,359
we already signed for uh

195
00:08:19,360 --> 00:08:22,879
better ones that are going to be

196
00:08:21,360 --> 00:08:26,080
deployed this year

197
00:08:22,879 --> 00:08:28,560
but right now we are using those that

198
00:08:26,080 --> 00:08:32,640
are that are still uh

199
00:08:28,560 --> 00:08:36,399
in position 42 and 43 of the top 500 so

200
00:08:32,640 --> 00:08:39,199
not too bad um but again the the

201
00:08:36,399 --> 00:08:41,279
bottleneck there is the the last

202
00:08:39,200 --> 00:08:44,720
parallel phase system

203
00:08:41,279 --> 00:08:46,720
then we have also some resources some

204
00:08:44,720 --> 00:08:50,240
cloud resources for disseminating

205
00:08:46,720 --> 00:08:52,080
our data one is under the umbrella of

206
00:08:50,240 --> 00:08:52,399
the copernicus services and the other

207
00:08:52,080 --> 00:08:55,760
one

208
00:08:52,399 --> 00:08:57,760
is still experimental is an european

209
00:08:55,760 --> 00:09:00,720
weather cloud that is

210
00:08:57,760 --> 00:09:01,760
useful for our member state to exploit

211
00:09:00,720 --> 00:09:04,560
those data

212
00:09:01,760 --> 00:09:06,160
close to the data source so instead of

213
00:09:04,560 --> 00:09:08,399
fetching the data from us

214
00:09:06,160 --> 00:09:10,640
moving the data in their own facilities

215
00:09:08,399 --> 00:09:13,680
and performing the simulation there

216
00:09:10,640 --> 00:09:14,399
they can move the computation close to

217
00:09:13,680 --> 00:09:18,479
the data

218
00:09:14,399 --> 00:09:20,480
and hopefully reduce the overall latency

219
00:09:18,480 --> 00:09:22,320
and the most interesting part our

220
00:09:20,480 --> 00:09:25,519
archive right now we have a

221
00:09:22,320 --> 00:09:28,640
300 petabytes obviously we cannot

222
00:09:25,519 --> 00:09:31,920
keep everything on disks so

223
00:09:28,640 --> 00:09:36,319
we have a large tape archive but

224
00:09:31,920 --> 00:09:38,719
we have also some nice caching

225
00:09:36,320 --> 00:09:40,080
policies so essentially only four

226
00:09:38,720 --> 00:09:43,680
percent of the

227
00:09:40,080 --> 00:09:47,680
requests are written the um

228
00:09:43,680 --> 00:09:50,079
the tapes all the 96

229
00:09:47,680 --> 00:09:50,880
all the remaining 96 are either

230
00:09:50,080 --> 00:09:53,920
performed

231
00:09:50,880 --> 00:09:55,360
from a in object store that we designed

232
00:09:53,920 --> 00:09:59,599
or

233
00:09:55,360 --> 00:10:03,600
a are disk based cache

234
00:09:59,600 --> 00:10:04,240
again we are adding nearly 250 terabytes

235
00:10:03,600 --> 00:10:07,680
per day

236
00:10:04,240 --> 00:10:09,440
so four five days are an additional

237
00:10:07,680 --> 00:10:13,279
petabyte for us

238
00:10:09,440 --> 00:10:15,920
uh and also our archive we lit the

239
00:10:13,279 --> 00:10:18,959
capacity of our

240
00:10:15,920 --> 00:10:21,360
oracle tape libraries because the

241
00:10:18,959 --> 00:10:22,160
four type libraries that we have are

242
00:10:21,360 --> 00:10:26,000
going to

243
00:10:22,160 --> 00:10:29,680
store up to 370 petabytes so

244
00:10:26,000 --> 00:10:31,360
we have to to extend it uh somehow and

245
00:10:29,680 --> 00:10:33,040
in any case we are going to move all the

246
00:10:31,360 --> 00:10:36,160
arcadia in another

247
00:10:33,040 --> 00:10:37,519
competing center during this year so we

248
00:10:36,160 --> 00:10:40,640
have to manage carefully

249
00:10:37,519 --> 00:10:45,519
all those data okay uh

250
00:10:40,640 --> 00:10:49,120
quite a lot of data how our user request

251
00:10:45,519 --> 00:10:51,360
the bit they need they give us a

252
00:10:49,120 --> 00:10:53,920
user request we have a query language

253
00:10:51,360 --> 00:10:55,680
designer for that

254
00:10:53,920 --> 00:10:57,360
they can specify the number of levels

255
00:10:55,680 --> 00:10:59,519
they want the the

256
00:10:57,360 --> 00:11:01,680
sum of the parameter temperature

257
00:10:59,519 --> 00:11:04,640
humidity or whatever they need

258
00:11:01,680 --> 00:11:05,680
a range of dates because they may be

259
00:11:04,640 --> 00:11:08,800
interested in

260
00:11:05,680 --> 00:11:13,599
simulating the evolution over time

261
00:11:08,800 --> 00:11:13,599
and also the accuracy we store

262
00:11:13,680 --> 00:11:17,120
description of the atmosphere each hour

263
00:11:16,640 --> 00:11:20,319
uh

264
00:11:17,120 --> 00:11:22,399
so they can even decide to uh

265
00:11:20,320 --> 00:11:24,160
subsample in this case they are

266
00:11:22,399 --> 00:11:27,760
requiring 10 days

267
00:11:24,160 --> 00:11:30,240
of simulation uh with a file

268
00:11:27,760 --> 00:11:31,040
every three hours and also they can

269
00:11:30,240 --> 00:11:33,839
specify

270
00:11:31,040 --> 00:11:34,640
a regional domain usually our services

271
00:11:33,839 --> 00:11:36,640
are

272
00:11:34,640 --> 00:11:38,560
interested in downscaling on a special

273
00:11:36,640 --> 00:11:39,680
area so we provided the simulation

274
00:11:38,560 --> 00:11:43,040
worldwide

275
00:11:39,680 --> 00:11:47,359
and then they focus on

276
00:11:43,040 --> 00:11:51,120
their country their area

277
00:11:47,360 --> 00:11:54,320
okay we can easily split those

278
00:11:51,120 --> 00:11:54,720
requests in two parts one is related to

279
00:11:54,320 --> 00:11:58,639
the

280
00:11:54,720 --> 00:12:01,600
hypertube datas in the sense that

281
00:11:58,639 --> 00:12:02,399
we consider our data as filling in

282
00:12:01,600 --> 00:12:04,639
hypercube

283
00:12:02,399 --> 00:12:06,240
with the several dimensions that are the

284
00:12:04,639 --> 00:12:09,600
data the level the

285
00:12:06,240 --> 00:12:12,000
variables and so on and we have to index

286
00:12:09,600 --> 00:12:15,760
those data according to that

287
00:12:12,000 --> 00:12:18,720
and then a geometric query

288
00:12:15,760 --> 00:12:19,279
up to now a box but probably something

289
00:12:18,720 --> 00:12:22,639
uh

290
00:12:19,279 --> 00:12:26,160
more interesting will will be arriving

291
00:12:22,639 --> 00:12:27,120
pretty soon okay uh how can we cope with

292
00:12:26,160 --> 00:12:31,279
the hypercube

293
00:12:27,120 --> 00:12:33,360
data assets uh we have a uh

294
00:12:31,279 --> 00:12:35,200
domain specific object store that we

295
00:12:33,360 --> 00:12:39,760
call the field db

296
00:12:35,200 --> 00:12:42,959
fdb so each

297
00:12:39,760 --> 00:12:45,600
data bit in our fdb is a

298
00:12:42,959 --> 00:12:47,119
layer so is a field described in the

299
00:12:45,600 --> 00:12:50,839
atmosphere

300
00:12:47,120 --> 00:12:53,760
the model is writing directly to the fdb

301
00:12:50,839 --> 00:12:55,839
and uh this is also

302
00:12:53,760 --> 00:12:57,279
required to support the throughput that

303
00:12:55,839 --> 00:12:58,720
we need from our model

304
00:12:57,279 --> 00:13:00,320
in the sense that the parallel alpha

305
00:12:58,720 --> 00:13:03,040
system cannot guarantee

306
00:13:00,320 --> 00:13:03,760
uh the 19 gigabytes per second that we

307
00:13:03,040 --> 00:13:06,800
need

308
00:13:03,760 --> 00:13:10,720
so we have this cache that is

309
00:13:06,800 --> 00:13:13,040
supporting the io operations

310
00:13:10,720 --> 00:13:14,000
we are also adding several kind of

311
00:13:13,040 --> 00:13:17,040
different

312
00:13:14,000 --> 00:13:19,200
backend to our object store right now

313
00:13:17,040 --> 00:13:22,079
again in operation we have a posix

314
00:13:19,200 --> 00:13:22,959
file system and we have we are adding

315
00:13:22,079 --> 00:13:26,000
something

316
00:13:22,959 --> 00:13:29,439
really fancy using nvram

317
00:13:26,000 --> 00:13:32,480
and in that case we can reach

318
00:13:29,440 --> 00:13:33,279
hundreds of gigabytes per second but we

319
00:13:32,480 --> 00:13:36,880
are also

320
00:13:33,279 --> 00:13:41,600
uh considering some uh cloud

321
00:13:36,880 --> 00:13:45,120
friendly uh layer like a set

322
00:13:41,600 --> 00:13:45,600
seth is still not performing to the

323
00:13:45,120 --> 00:13:49,440
level

324
00:13:45,600 --> 00:13:52,800
we have from posix so is

325
00:13:49,440 --> 00:13:56,320
roughly four times slower so we are

326
00:13:52,800 --> 00:14:00,000
still working on that in any case the

327
00:13:56,320 --> 00:14:02,880
object store is supporting our hypercube

328
00:14:00,000 --> 00:14:03,600
queries so the ddd uh all those

329
00:14:02,880 --> 00:14:05,839
parameters

330
00:14:03,600 --> 00:14:06,800
are selected from an index and then we

331
00:14:05,839 --> 00:14:09,519
just

332
00:14:06,800 --> 00:14:10,399
uh eat the disk for uh loading the the

333
00:14:09,519 --> 00:14:14,000
field uh

334
00:14:10,399 --> 00:14:17,199
with a length and an after offset

335
00:14:14,000 --> 00:14:20,320
uh we get a nice properties

336
00:14:17,199 --> 00:14:23,359
like uh is

337
00:14:20,320 --> 00:14:28,320
fully acid uh the system

338
00:14:23,360 --> 00:14:28,320
all the data are fully flushed so we get

339
00:14:29,360 --> 00:14:35,600
some nice guarantee on ddd data

340
00:14:33,600 --> 00:14:38,560
[Music]

341
00:14:35,600 --> 00:14:38,560
that the data are

342
00:14:38,800 --> 00:14:42,839
stable even if we have a crash in the

343
00:14:41,279 --> 00:14:45,680
computing system

344
00:14:42,839 --> 00:14:48,959
moreover having

345
00:14:45,680 --> 00:14:51,839
a large computing environment

346
00:14:48,959 --> 00:14:53,680
we may have trouble like a node that is

347
00:14:51,839 --> 00:14:56,639
dropping or something like that so

348
00:14:53,680 --> 00:14:57,279
we may need to re-run a computation and

349
00:14:56,639 --> 00:15:00,480
we still

350
00:14:57,279 --> 00:15:01,439
want to uh guarantee that the data are

351
00:15:00,480 --> 00:15:05,440
fully accessible

352
00:15:01,440 --> 00:15:08,480
and reliable so uh we

353
00:15:05,440 --> 00:15:11,839
have a right ones policy

354
00:15:08,480 --> 00:15:12,560
so in case of a rerun that are not

355
00:15:11,839 --> 00:15:15,040
overwritten

356
00:15:12,560 --> 00:15:16,719
otherwise we can risk it to have two

357
00:15:15,040 --> 00:15:19,599
instances of the data that are

358
00:15:16,720 --> 00:15:21,360
not fully consistent with right in a

359
00:15:19,600 --> 00:15:25,600
newer location then we push

360
00:15:21,360 --> 00:15:29,920
the space that is no longer required

361
00:15:25,600 --> 00:15:29,920
when the new write has been completed

362
00:15:30,320 --> 00:15:34,320
okay the idea is that we produce the

363
00:15:33,759 --> 00:15:37,040
data

364
00:15:34,320 --> 00:15:38,160
we store on the parallel phase system

365
00:15:37,040 --> 00:15:42,480
and then we archive

366
00:15:38,160 --> 00:15:45,279
on the tapes with fdb

367
00:15:42,480 --> 00:15:47,120
all the process has been modified so the

368
00:15:45,279 --> 00:15:50,480
object store is taking care of

369
00:15:47,120 --> 00:15:52,079
all the i o and then is forwarding the

370
00:15:50,480 --> 00:15:52,800
data to the parallel facility and to the

371
00:15:52,079 --> 00:15:56,800
archive

372
00:15:52,800 --> 00:16:00,399
and eventually we will provide also a

373
00:15:56,800 --> 00:16:03,599
a cloud consumer that

374
00:16:00,399 --> 00:16:07,040
is a really interesting for

375
00:16:03,600 --> 00:16:10,959
assessing the data and

376
00:16:07,040 --> 00:16:10,959
performing computation close to them

377
00:16:11,360 --> 00:16:15,120
for the geometrical part again we have

378
00:16:14,399 --> 00:16:17,759
agreed that

379
00:16:15,120 --> 00:16:18,560
that is not user friendly in the sense

380
00:16:17,759 --> 00:16:21,600
that

381
00:16:18,560 --> 00:16:25,040
is not the

382
00:16:21,600 --> 00:16:28,320
regular lat lawn that our user needs

383
00:16:25,040 --> 00:16:32,160
so we have to interpolate uh to

384
00:16:28,320 --> 00:16:33,440
a regular one up to now we are

385
00:16:32,160 --> 00:16:36,719
interpolating the

386
00:16:33,440 --> 00:16:40,320
wall layer so the globe and

387
00:16:36,720 --> 00:16:42,399
just then we crop the the user selected

388
00:16:40,320 --> 00:16:46,160
domain

389
00:16:42,399 --> 00:16:46,160
we do that because

390
00:16:46,959 --> 00:16:52,719
using luster we cannot byte address

391
00:16:50,720 --> 00:16:53,920
the data so we have to read the wall

392
00:16:52,720 --> 00:16:58,000
field and

393
00:16:53,920 --> 00:17:01,199
compute on that uh we are all working on

394
00:16:58,000 --> 00:17:02,160
nicer data storage for byte

395
00:17:01,199 --> 00:17:05,359
addressability

396
00:17:02,160 --> 00:17:07,199
and so for being able to read just the

397
00:17:05,359 --> 00:17:08,559
subset of the data that are required for

398
00:17:07,199 --> 00:17:13,360
the interpolation so

399
00:17:08,559 --> 00:17:13,359
essentially the the portion of the

400
00:17:13,679 --> 00:17:16,880
gaussian grid that are required to

401
00:17:16,079 --> 00:17:21,198
interpolate

402
00:17:16,880 --> 00:17:24,640
just the subdomain this is still in work

403
00:17:21,199 --> 00:17:28,720
but hopefully it will see the

404
00:17:24,640 --> 00:17:28,720
daylight in a few months

405
00:17:29,600 --> 00:17:36,639
again what can we do

406
00:17:33,039 --> 00:17:41,440
for uh giving all the user access to

407
00:17:36,640 --> 00:17:44,559
uh our archive uh we are developing a a

408
00:17:41,440 --> 00:17:48,080
cloud as a service system

409
00:17:44,559 --> 00:17:51,280
for uh letting the user to use our

410
00:17:48,080 --> 00:17:53,678
query language and real assess the data

411
00:17:51,280 --> 00:17:54,559
the system is under development and is

412
00:17:53,679 --> 00:17:57,440
financed by

413
00:17:54,559 --> 00:17:58,240
a couple of european projects namely i'm

414
00:17:57,440 --> 00:18:01,600
paid by

415
00:17:58,240 --> 00:18:05,600
one of those that is lexis

416
00:18:01,600 --> 00:18:08,879
the idea is that the data are accessible

417
00:18:05,600 --> 00:18:12,959
even from a externally

418
00:18:08,880 --> 00:18:15,360
by simply eating a rest api

419
00:18:12,960 --> 00:18:16,880
all from the european weather cloud that

420
00:18:15,360 --> 00:18:19,918
we are hosting

421
00:18:16,880 --> 00:18:23,600
everything is a nice restful api

422
00:18:19,919 --> 00:18:26,240
with a both command

423
00:18:23,600 --> 00:18:27,199
line interface and the python client

424
00:18:26,240 --> 00:18:29,120
that is

425
00:18:27,200 --> 00:18:32,000
connecting and supporting all the

426
00:18:29,120 --> 00:18:35,520
queries that we offer

427
00:18:32,000 --> 00:18:38,400
we give through these polytope

428
00:18:35,520 --> 00:18:39,520
adder access to the archive and to the

429
00:18:38,400 --> 00:18:42,000
real-time data

430
00:18:39,520 --> 00:18:43,840
there is a licensing issue after now so

431
00:18:42,000 --> 00:18:47,679
essentially the data are not

432
00:18:43,840 --> 00:18:51,439
freely available we are

433
00:18:47,679 --> 00:18:54,400
selling the data for a living but

434
00:18:51,440 --> 00:18:56,160
we are committed thanks to an effort of

435
00:18:54,400 --> 00:19:01,120
our member state to release

436
00:18:56,160 --> 00:19:04,240
our data but it will takes a few years

437
00:19:01,120 --> 00:19:04,239
more details on the

438
00:19:04,799 --> 00:19:08,160
exploitation of those data from the

439
00:19:07,200 --> 00:19:12,960
cloud will be

440
00:19:08,160 --> 00:19:15,960
in this talk given by my colleague john

441
00:19:12,960 --> 00:19:22,420
yeah obviously for question if we have

442
00:19:15,960 --> 00:19:22,420
[Applause]

443
00:19:22,840 --> 00:19:25,840
time

444
00:19:29,440 --> 00:19:35,760
uh okay we have a an open source

445
00:19:32,799 --> 00:19:36,879
version of the model that is open ifs we

446
00:19:35,760 --> 00:19:40,480
have

447
00:19:36,880 --> 00:19:43,919
open answers for a search to the archive

448
00:19:40,480 --> 00:19:46,720
the only thing that is uh not

449
00:19:43,919 --> 00:19:47,760
accessible for free is the real time

450
00:19:46,720 --> 00:19:49,679
forecast

451
00:19:47,760 --> 00:19:51,840
in the sense that uh for a research

452
00:19:49,679 --> 00:19:53,039
usually having the data with a few days

453
00:19:51,840 --> 00:19:56,559
of delays

454
00:19:53,039 --> 00:19:59,760
is not an issue but you can easily

455
00:19:56,559 --> 00:20:01,678
ask for a research assets and

456
00:19:59,760 --> 00:20:05,039
play with the high resolution and

457
00:20:01,679 --> 00:20:08,240
whatever you want

458
00:20:05,039 --> 00:20:11,039
as and the project

459
00:20:08,240 --> 00:20:11,760
i'm working with we will have an open

460
00:20:11,039 --> 00:20:15,520
call

461
00:20:11,760 --> 00:20:18,400
for uh exploiting those data

462
00:20:15,520 --> 00:20:18,960
and so you can apply for the lexis open

463
00:20:18,400 --> 00:20:22,480
call

464
00:20:18,960 --> 00:20:25,039
and ask for us as even of the

465
00:20:22,480 --> 00:20:26,159
real-time data it will be time

466
00:20:25,039 --> 00:20:27,760
constrained

467
00:20:26,159 --> 00:20:29,200
for the lifetime of the project but

468
00:20:27,760 --> 00:20:31,679
still

469
00:20:29,200 --> 00:20:32,480
something relevant and useful for

470
00:20:31,679 --> 00:20:35,280
playing with

471
00:20:32,480 --> 00:20:38,159
the largest archive that uh apna is

472
00:20:35,280 --> 00:20:38,158
available on the world

473
00:20:39,760 --> 00:20:46,400
okay another one

474
00:20:42,960 --> 00:20:50,240
yes are you planning to

475
00:20:46,400 --> 00:20:54,400
make the data available in like regular

476
00:20:50,240 --> 00:20:58,480
uh variable by regular

477
00:20:54,400 --> 00:20:58,480
cloud okay

478
00:20:59,280 --> 00:21:06,960
sure sure the the

479
00:21:03,520 --> 00:21:10,879
possibility to get the data on a regular

480
00:21:06,960 --> 00:21:14,000
lat long grid is uh is already there

481
00:21:10,880 --> 00:21:16,960
in the sense that the interpolation

482
00:21:14,000 --> 00:21:18,320
in the is a parameter of the query

483
00:21:16,960 --> 00:21:23,360
language so you can

484
00:21:18,320 --> 00:21:27,280
select uh the desired resolution

485
00:21:23,360 --> 00:21:30,240
one grade one degree 0.23

486
00:21:27,280 --> 00:21:31,039
whatever you want up to 0.1 degrees up

487
00:21:30,240 --> 00:21:33,360
to now

488
00:21:31,039 --> 00:21:35,440
and essentially the interpolation is

489
00:21:33,360 --> 00:21:38,559
performed on the fly for you

490
00:21:35,440 --> 00:21:39,520
so you already can get the data on the

491
00:21:38,559 --> 00:21:41,918
grid that you

492
00:21:39,520 --> 00:21:42,799
like without the the need to implement

493
00:21:41,919 --> 00:21:44,799
it

494
00:21:42,799 --> 00:21:46,960
oh sorry so you can talk afterwards if

495
00:21:44,799 --> 00:21:56,480
you want

496
00:21:46,960 --> 00:21:56,480
thank you very much once again

