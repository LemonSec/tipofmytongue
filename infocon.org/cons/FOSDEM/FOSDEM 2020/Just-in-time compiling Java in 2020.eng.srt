1
00:00:05,680 --> 00:00:09,360
so hello everyone

2
00:00:06,879 --> 00:00:12,079
and welcome to my talk about the trust

3
00:00:09,360 --> 00:00:15,200
and time compilers in the open jdk

4
00:00:12,080 --> 00:00:17,039
i'm martin darr and i work for sap we

5
00:00:15,200 --> 00:00:21,039
are a small team

6
00:00:17,039 --> 00:00:24,160
i'm working on the jvm at sap in waldorf

7
00:00:21,039 --> 00:00:25,359
and this time we have three time three

8
00:00:24,160 --> 00:00:27,680
talks in a row

9
00:00:25,359 --> 00:00:28,880
so we will also see two of my colleagues

10
00:00:27,680 --> 00:00:30,640
later on

11
00:00:28,880 --> 00:00:33,040
but let's get started with with the

12
00:00:30,640 --> 00:00:35,920
agenda so i will talk about the

13
00:00:33,040 --> 00:00:38,000
just in time compilers which translates

14
00:00:35,920 --> 00:00:39,680
java bytecode into a machine code in the

15
00:00:38,000 --> 00:00:41,840
open jdk

16
00:00:39,680 --> 00:00:44,000
i will also talk about how different

17
00:00:41,840 --> 00:00:46,480
compilers work together

18
00:00:44,000 --> 00:00:47,039
and i also like i like to address

19
00:00:46,480 --> 00:00:50,640
resource

20
00:00:47,039 --> 00:00:52,160
usage as well so first of all how many

21
00:00:50,640 --> 00:00:55,360
compilers do we have

22
00:00:52,160 --> 00:00:57,760
in the open jdk we've already heard

23
00:00:55,360 --> 00:00:58,000
about two ones but how many do we have

24
00:00:57,760 --> 00:01:01,839
we

25
00:00:58,000 --> 00:01:04,799
have one more three ones

26
00:01:01,840 --> 00:01:06,799
so um the one we haven't heard today yet

27
00:01:04,799 --> 00:01:08,560
is the klein compiler which is also

28
00:01:06,799 --> 00:01:10,880
called c1

29
00:01:08,560 --> 00:01:13,439
it compiles pretty quickly but with the

30
00:01:10,880 --> 00:01:15,199
lower optimization level

31
00:01:13,439 --> 00:01:17,039
and then we have the server compiler

32
00:01:15,200 --> 00:01:19,840
also called c2

33
00:01:17,040 --> 00:01:21,680
we already had a talk about that one

34
00:01:19,840 --> 00:01:25,119
it's kind of the opposite opposite

35
00:01:21,680 --> 00:01:26,080
it compiles slowly but therefore with a

36
00:01:25,119 --> 00:01:29,040
high optimization

37
00:01:26,080 --> 00:01:30,720
level for example it has a lot of loop

38
00:01:29,040 --> 00:01:32,880
optimizations

39
00:01:30,720 --> 00:01:34,320
and we've also heard about the escape

40
00:01:32,880 --> 00:01:36,798
analysis

41
00:01:34,320 --> 00:01:39,279
and there are still people on working on

42
00:01:36,799 --> 00:01:41,520
improvements for that

43
00:01:39,280 --> 00:01:42,960
both compilers are available on a lot of

44
00:01:41,520 --> 00:01:45,600
platforms

45
00:01:42,960 --> 00:01:47,759
including powerpc and s390 which are

46
00:01:45,600 --> 00:01:49,679
supported by our team

47
00:01:47,759 --> 00:01:51,360
and these two compilers are used by

48
00:01:49,680 --> 00:01:55,119
default

49
00:01:51,360 --> 00:01:57,680
and we've also heard about gral which is

50
00:01:55,119 --> 00:01:59,759
rather new in the open 20k it is still

51
00:01:57,680 --> 00:02:01,920
experimental in the open tradek

52
00:01:59,759 --> 00:02:04,079
that means it is not used by default you

53
00:02:01,920 --> 00:02:06,320
need to switch it on

54
00:02:04,079 --> 00:02:07,520
if you want to use it and it is

55
00:02:06,320 --> 00:02:11,280
developed on

56
00:02:07,520 --> 00:02:14,239
github so updates get merged into

57
00:02:11,280 --> 00:02:14,239
the open jdk

58
00:02:14,560 --> 00:02:18,720
special to growl is that it is written

59
00:02:16,480 --> 00:02:22,160
in java that is a big difference to

60
00:02:18,720 --> 00:02:25,040
the other two compilers and

61
00:02:22,160 --> 00:02:27,520
that also does a lot of optimization it

62
00:02:25,040 --> 00:02:28,879
has a more sophisticated escape analysis

63
00:02:27,520 --> 00:02:30,800
for example

64
00:02:28,879 --> 00:02:32,640
andrew has already shown a few things

65
00:02:30,800 --> 00:02:36,080
about graal

66
00:02:32,640 --> 00:02:37,200
so thanks for that and this it is

67
00:02:36,080 --> 00:02:40,160
optimized for

68
00:02:37,200 --> 00:02:43,920
dynamic languages by the way growl

69
00:02:40,160 --> 00:02:46,239
compiler is also called qual vm compiler

70
00:02:43,920 --> 00:02:48,079
and i'd like to show a few things uh

71
00:02:46,239 --> 00:02:51,360
andrew has already mentioned

72
00:02:48,080 --> 00:02:54,400
i got one slide from oracle

73
00:02:51,360 --> 00:02:56,160
so thanks to oracle for providing it um

74
00:02:54,400 --> 00:02:57,680
there are three different use cases of

75
00:02:56,160 --> 00:03:00,480
growl vm

76
00:02:57,680 --> 00:03:01,680
and the growl compiler is always in the

77
00:03:00,480 --> 00:03:04,799
center of it

78
00:03:01,680 --> 00:03:07,360
together with the jvmci the java virtual

79
00:03:04,800 --> 00:03:10,319
machine compiler interface

80
00:03:07,360 --> 00:03:13,200
and the use case on the very left is the

81
00:03:10,319 --> 00:03:16,399
one which is available in the open jdk

82
00:03:13,200 --> 00:03:18,399
so you have a java application

83
00:03:16,400 --> 00:03:19,920
or java methods which get compiled by

84
00:03:18,400 --> 00:03:23,200
kraal and

85
00:03:19,920 --> 00:03:26,319
they run on the hotspot vm the java

86
00:03:23,200 --> 00:03:28,640
virtual machine of the open jdk

87
00:03:26,319 --> 00:03:31,119
so that path on the very left is

88
00:03:28,640 --> 00:03:34,000
supported by the open jdk

89
00:03:31,120 --> 00:03:35,040
um and in addition to that on the right

90
00:03:34,000 --> 00:03:37,360
hand side you can

91
00:03:35,040 --> 00:03:40,720
see the native image technology and we

92
00:03:37,360 --> 00:03:40,720
also already mentioned

93
00:03:41,200 --> 00:03:46,000
and everything gets compiled

94
00:03:44,840 --> 00:03:48,640
pre-compiled

95
00:03:46,000 --> 00:03:50,400
um and there's something in between

96
00:03:48,640 --> 00:03:54,000
where only the crowd compiler is

97
00:03:50,400 --> 00:03:56,480
pre-compiled into a shared library

98
00:03:54,000 --> 00:03:58,239
so that's the basic difference between

99
00:03:56,480 --> 00:04:00,159
this approach and this one

100
00:03:58,239 --> 00:04:03,920
here crawler compiler itself is

101
00:04:00,159 --> 00:04:03,920
pre-compiled in a shared library

102
00:04:04,319 --> 00:04:09,359
so back to the different compilers

103
00:04:09,680 --> 00:04:13,120
i'd like to compare performance a little

104
00:04:12,080 --> 00:04:15,200
bit

105
00:04:13,120 --> 00:04:16,399
by the way this is an old benchmark with

106
00:04:15,200 --> 00:04:19,199
an old jdk

107
00:04:16,399 --> 00:04:20,320
and an old garbage collector but don't

108
00:04:19,199 --> 00:04:22,320
care about numbers

109
00:04:20,320 --> 00:04:23,840
i think it's good to get an a first

110
00:04:22,320 --> 00:04:25,280
impression about

111
00:04:23,840 --> 00:04:27,198
the performance of the different

112
00:04:25,280 --> 00:04:29,919
compilers

113
00:04:27,199 --> 00:04:30,880
so at at the bottom you can see

114
00:04:29,919 --> 00:04:33,440
interpreter for

115
00:04:30,880 --> 00:04:36,240
uh for reference so that means we are

116
00:04:33,440 --> 00:04:39,280
not using any just in time compiler

117
00:04:36,240 --> 00:04:41,040
and you can get that by uh specifying

118
00:04:39,280 --> 00:04:44,239
the runtime option minus x

119
00:04:41,040 --> 00:04:47,199
int that stands for interpreter mode

120
00:04:44,240 --> 00:04:48,800
so you will all you will only use the

121
00:04:47,199 --> 00:04:49,600
interpreter no compile no cheat

122
00:04:48,800 --> 00:04:52,479
compilers at

123
00:04:49,600 --> 00:04:54,840
all and as you can see the performance

124
00:04:52,479 --> 00:04:58,479
is pretty poor

125
00:04:54,840 --> 00:05:00,239
um already much faster is is the c1 the

126
00:04:58,479 --> 00:05:02,719
client compiler

127
00:05:00,240 --> 00:05:03,680
you can select that for example by using

128
00:05:02,720 --> 00:05:07,120
this flag

129
00:05:03,680 --> 00:05:10,320
tier stop at level three um that might

130
00:05:07,120 --> 00:05:15,520
sound a little bit complicated

131
00:05:10,320 --> 00:05:15,520
and i have to have to note that

132
00:05:15,600 --> 00:05:19,680
level 3 means that c1 still performs

133
00:05:18,160 --> 00:05:22,720
profiling

134
00:05:19,680 --> 00:05:24,479
so you won't get the best performance

135
00:05:22,720 --> 00:05:27,199
out of c1 by that

136
00:05:24,479 --> 00:05:28,960
if you want to tune c1 you would select

137
00:05:27,199 --> 00:05:30,960
stop at level 1

138
00:05:28,960 --> 00:05:33,440
and then you would get c1 without

139
00:05:30,960 --> 00:05:35,280
profiling

140
00:05:33,440 --> 00:05:37,280
but in this case i want the better

141
00:05:35,280 --> 00:05:39,840
profiling information that's why i left

142
00:05:37,280 --> 00:05:39,840
it on

143
00:05:40,160 --> 00:05:46,720
if you want to use the c2 compiler only

144
00:05:42,960 --> 00:05:49,120
you can switch off theater compilation

145
00:05:46,720 --> 00:05:52,240
and then you get the blue line which is

146
00:05:49,120 --> 00:05:52,240
already much faster

147
00:05:52,400 --> 00:05:55,919
and the default configuration uses

148
00:05:54,160 --> 00:05:58,000
tiered compilation

149
00:05:55,919 --> 00:06:00,159
and you get the fastest startup and the

150
00:05:58,000 --> 00:06:02,560
best peak performance

151
00:06:00,160 --> 00:06:03,520
the best peak performance also because

152
00:06:02,560 --> 00:06:06,560
the profiling

153
00:06:03,520 --> 00:06:07,919
information is better but i'll explain

154
00:06:06,560 --> 00:06:10,720
the tiered compilation

155
00:06:07,919 --> 00:06:11,280
stuff later on more in more detail so

156
00:06:10,720 --> 00:06:12,880
you should

157
00:06:11,280 --> 00:06:14,719
be able to understand it better at the

158
00:06:12,880 --> 00:06:17,759
end

159
00:06:14,720 --> 00:06:18,319
but for those who hate this old stuff i

160
00:06:17,759 --> 00:06:21,199
have

161
00:06:18,319 --> 00:06:23,440
also a slide with the latest jdk so the

162
00:06:21,199 --> 00:06:27,199
same old benchmark with the latest

163
00:06:23,440 --> 00:06:28,960
jdk 15 and you can

164
00:06:27,199 --> 00:06:30,400
already see that the peak performance is

165
00:06:28,960 --> 00:06:34,719
better

166
00:06:30,400 --> 00:06:36,400
with c2 especially um and you can also

167
00:06:34,720 --> 00:06:38,400
see the green line which is new that is

168
00:06:36,400 --> 00:06:40,318
growl

169
00:06:38,400 --> 00:06:43,280
in order to use gral you need to use

170
00:06:40,319 --> 00:06:45,199
this switch use trayvmci compiler

171
00:06:43,280 --> 00:06:47,520
which is an experimental option so you

172
00:06:45,199 --> 00:06:51,759
need to unlock it in addition

173
00:06:47,520 --> 00:06:54,719
and growl is the default jvmci compiler

174
00:06:51,759 --> 00:06:56,639
so you will get so you will get growl by

175
00:06:54,720 --> 00:06:58,800
this flag

176
00:06:56,639 --> 00:07:00,319
so performance peak performance of growl

177
00:06:58,800 --> 00:07:02,400
is good

178
00:07:00,319 --> 00:07:03,759
um even for this very traditional

179
00:07:02,400 --> 00:07:06,638
workload

180
00:07:03,759 --> 00:07:07,440
growl performance is uh of course better

181
00:07:06,639 --> 00:07:10,639
for more

182
00:07:07,440 --> 00:07:16,400
modern workloads for example if you

183
00:07:10,639 --> 00:07:16,400
run scala that's what twitter does a lot

184
00:07:16,880 --> 00:07:20,800
and i should also mention that the open

185
00:07:18,960 --> 00:07:23,840
jdk only contains

186
00:07:20,800 --> 00:07:26,479
the community edition of gral

187
00:07:23,840 --> 00:07:27,679
there's also an enterprise an enterprise

188
00:07:26,479 --> 00:07:30,719
version available

189
00:07:27,680 --> 00:07:32,160
which contains more optimizations so you

190
00:07:30,720 --> 00:07:35,120
will get better performance with the

191
00:07:32,160 --> 00:07:35,120
enterprise edition

192
00:07:35,680 --> 00:07:39,360
and you can also see that the startup

193
00:07:38,319 --> 00:07:41,919
takes longer

194
00:07:39,360 --> 00:07:43,520
it takes a couple of seconds until here

195
00:07:41,919 --> 00:07:46,479
4.5 seconds

196
00:07:43,520 --> 00:07:48,159
roughly to get peak performance and

197
00:07:46,479 --> 00:07:51,199
that's due to the fact that

198
00:07:48,160 --> 00:07:53,280
growl itself is written in java so the

199
00:07:51,199 --> 00:07:54,879
crowd compiler itself gets interpreted

200
00:07:53,280 --> 00:07:57,758
at the beginning

201
00:07:54,879 --> 00:07:59,680
and then later on hot methods gets get

202
00:07:57,759 --> 00:08:01,680
compiled by c1

203
00:07:59,680 --> 00:08:03,759
and later on they get compiled by which

204
00:08:01,680 --> 00:08:05,840
compiler

205
00:08:03,759 --> 00:08:07,360
they crawl compile itself so crawl

206
00:08:05,840 --> 00:08:10,719
compiles itself

207
00:08:07,360 --> 00:08:12,720
and then takes a few seconds so this may

208
00:08:10,720 --> 00:08:14,319
be okay for large server applications

209
00:08:12,720 --> 00:08:15,440
where you can afford spending a few

210
00:08:14,319 --> 00:08:18,560
seconds

211
00:08:15,440 --> 00:08:18,879
but there's also a possibility to fix

212
00:08:18,560 --> 00:08:23,199
that

213
00:08:18,879 --> 00:08:27,440
if you need a quicker startup and

214
00:08:23,199 --> 00:08:31,039
that's available with with qual vm

215
00:08:27,440 --> 00:08:33,039
so the jvm um has a feature called

216
00:08:31,039 --> 00:08:35,199
printflex final

217
00:08:33,039 --> 00:08:37,199
and if you enable that you will see all

218
00:08:35,200 --> 00:08:41,039
flag configurations the vm

219
00:08:37,200 --> 00:08:44,640
sets for itself and you can also find

220
00:08:41,039 --> 00:08:46,160
use jvmci native life use nate jvmci

221
00:08:44,640 --> 00:08:48,160
native library

222
00:08:46,160 --> 00:08:49,519
and with gral vm that one is true by

223
00:08:48,160 --> 00:08:53,279
default

224
00:08:49,519 --> 00:08:56,080
and that means the jvm is using

225
00:08:53,279 --> 00:08:58,320
the pre-compiled shared library so the

226
00:08:56,080 --> 00:08:59,920
crawl compiler is already pre-compiled

227
00:08:58,320 --> 00:09:02,160
and you get a pretty good startup with

228
00:08:59,920 --> 00:09:02,160
that

229
00:09:03,760 --> 00:09:08,800
so next i've promised to explain tiered

230
00:09:06,160 --> 00:09:10,800
compilation a little bit

231
00:09:08,800 --> 00:09:12,319
so tied compilation is basically the

232
00:09:10,800 --> 00:09:14,800
answer to the question of

233
00:09:12,320 --> 00:09:16,640
uh how these different compilers work

234
00:09:14,800 --> 00:09:18,240
together

235
00:09:16,640 --> 00:09:20,720
as already mentioned at the beginning

236
00:09:18,240 --> 00:09:23,040
everything is starts at the interpreter

237
00:09:20,720 --> 00:09:24,560
which is tier zero

238
00:09:23,040 --> 00:09:27,920
and then we have three different tiers

239
00:09:24,560 --> 00:09:31,279
for the c1 compiler

240
00:09:27,920 --> 00:09:34,160
tier one is c1 without any profiling

241
00:09:31,279 --> 00:09:36,080
that is used only for trivial methods

242
00:09:34,160 --> 00:09:38,399
when the c1

243
00:09:36,080 --> 00:09:40,000
believes that it's not worth optimizing

244
00:09:38,399 --> 00:09:44,000
further so we will

245
00:09:40,000 --> 00:09:47,279
stick on on this trivial compilation

246
00:09:44,000 --> 00:09:50,480
um and then there's tier tier two c

247
00:09:47,279 --> 00:09:51,439
one uses reduced profiling and it does

248
00:09:50,480 --> 00:09:54,720
that when

249
00:09:51,440 --> 00:09:57,360
it thinks there's too much work to do so

250
00:09:54,720 --> 00:09:59,120
we just should make it quick and the

251
00:09:57,360 --> 00:10:02,880
default

252
00:09:59,120 --> 00:10:04,000
tier 4 c1 is tier 3 and you get the full

253
00:10:02,880 --> 00:10:08,880
profiling

254
00:10:04,000 --> 00:10:11,120
code compiled into the compiled method

255
00:10:08,880 --> 00:10:14,320
and then finally the tier 4 is for the

256
00:10:11,120 --> 00:10:17,440
highest optimization level

257
00:10:14,320 --> 00:10:19,040
and it uses c2 compiler by default in

258
00:10:17,440 --> 00:10:20,880
open 20k

259
00:10:19,040 --> 00:10:24,319
and you can replace it by crawl if you

260
00:10:20,880 --> 00:10:24,320
enable it explicitly

261
00:10:24,959 --> 00:10:31,680
you can also see the the

262
00:10:28,079 --> 00:10:33,599
tiers when you enable print compilation

263
00:10:31,680 --> 00:10:35,359
you can see which method gets compiled

264
00:10:33,600 --> 00:10:38,640
at which tier

265
00:10:35,360 --> 00:10:41,920
and typically most methods

266
00:10:38,640 --> 00:10:42,240
get started started at tier 3 then you

267
00:10:41,920 --> 00:10:46,000
get

268
00:10:42,240 --> 00:10:48,480
also tier 4 method compiled by c2 in

269
00:10:46,000 --> 00:10:50,240
this case

270
00:10:48,480 --> 00:10:53,440
but here here's also a picture to

271
00:10:50,240 --> 00:10:55,279
explain that a little bit more in detail

272
00:10:53,440 --> 00:10:58,079
everything starts in the interpreter as

273
00:10:55,279 --> 00:11:00,480
already mentioned at the beginning

274
00:10:58,079 --> 00:11:02,000
and the interpreter performs invocation

275
00:11:00,480 --> 00:11:04,079
counting

276
00:11:02,000 --> 00:11:05,120
and once the invocation counterfeit

277
00:11:04,079 --> 00:11:08,239
method

278
00:11:05,120 --> 00:11:10,880
reaches a certain level then a

279
00:11:08,240 --> 00:11:12,880
compile task gets generated in the c1

280
00:11:10,880 --> 00:11:14,880
compile queue

281
00:11:12,880 --> 00:11:17,360
i see one compiler that can pick it up

282
00:11:14,880 --> 00:11:20,880
and create a c1 compiled method

283
00:11:17,360 --> 00:11:24,160
which is a tier 3 method in this example

284
00:11:20,880 --> 00:11:26,640
and as already mentioned tier 3

285
00:11:24,160 --> 00:11:28,480
also does profiling which includes

286
00:11:26,640 --> 00:11:31,120
invocation counting

287
00:11:28,480 --> 00:11:32,880
so this compiled code still does

288
00:11:31,120 --> 00:11:36,640
invocation counting

289
00:11:32,880 --> 00:11:39,839
and once a compiled method reaches

290
00:11:36,640 --> 00:11:42,720
or a method reaches this level

291
00:11:39,839 --> 00:11:45,040
then a compiled task gets generated in

292
00:11:42,720 --> 00:11:48,000
the c2 compile queue

293
00:11:45,040 --> 00:11:49,439
and similar to c1 as the two compilers

294
00:11:48,000 --> 00:11:53,839
that can pick it up and

295
00:11:49,440 --> 00:11:53,839
create the fastest version of the method

296
00:11:54,079 --> 00:12:00,959
and this is how uh how it works for

297
00:11:57,200 --> 00:12:02,240
method invocations um but there may be

298
00:12:00,959 --> 00:12:04,239
long-running loops

299
00:12:02,240 --> 00:12:05,600
which without any method invocations and

300
00:12:04,240 --> 00:12:07,440
obviously

301
00:12:05,600 --> 00:12:08,720
um the invocation counting will not help

302
00:12:07,440 --> 00:12:11,360
in that case

303
00:12:08,720 --> 00:12:13,040
that's why there's also baggage counting

304
00:12:11,360 --> 00:12:15,839
which works similar so it's

305
00:12:13,040 --> 00:12:18,079
almost the same slide but here with back

306
00:12:15,839 --> 00:12:21,279
edge counters in

307
00:12:18,079 --> 00:12:23,439
instead of the invocation counters with

308
00:12:21,279 --> 00:12:26,320
different limits

309
00:12:23,440 --> 00:12:27,440
um and what happens here we um the

310
00:12:26,320 --> 00:12:30,560
compilers generate

311
00:12:27,440 --> 00:12:33,680
so-called osr methods which then

312
00:12:30,560 --> 00:12:37,518
stands for on stack replacement

313
00:12:33,680 --> 00:12:41,120
and um they are special methods they

314
00:12:37,519 --> 00:12:41,120
have an entry point for the loop

315
00:12:42,800 --> 00:12:47,040
and onstage replacement is called this

316
00:12:45,279 --> 00:12:49,279
way because

317
00:12:47,040 --> 00:12:51,120
an interpreted method gets removed from

318
00:12:49,279 --> 00:12:53,680
the stack frame

319
00:12:51,120 --> 00:12:54,880
and it gets replaced by a compiled stack

320
00:12:53,680 --> 00:12:56,160
frame

321
00:12:54,880 --> 00:12:58,800
that's why we call it unstack

322
00:12:56,160 --> 00:13:01,439
replacement

323
00:12:58,800 --> 00:13:03,199
so i've already talked about compiler

324
00:13:01,440 --> 00:13:06,160
threads

325
00:13:03,200 --> 00:13:07,760
how many compiler threads are we using

326
00:13:06,160 --> 00:13:09,920
well that depends on

327
00:13:07,760 --> 00:13:11,200
the machine we are running on in the

328
00:13:09,920 --> 00:13:15,360
office i have a

329
00:13:11,200 --> 00:13:18,399
40 cpu linux machine

330
00:13:15,360 --> 00:13:18,800
and when using printflex final i can see

331
00:13:18,399 --> 00:13:21,200
that

332
00:13:18,800 --> 00:13:22,560
the vm selects the i compiler crown to

333
00:13:21,200 --> 00:13:26,000
15

334
00:13:22,560 --> 00:13:29,199
that is computed by a fancy formula

335
00:13:26,000 --> 00:13:32,480
and it

336
00:13:29,200 --> 00:13:32,959
it one third of them are reserved for c1

337
00:13:32,480 --> 00:13:35,440
and the

338
00:13:32,959 --> 00:13:36,239
remaining 10 in this case are reserved

339
00:13:35,440 --> 00:13:40,720
for

340
00:13:36,240 --> 00:13:44,399
c2 threads and similar

341
00:13:40,720 --> 00:13:47,279
to compiler threads the the vm

342
00:13:44,399 --> 00:13:48,320
also decides on how many gc threads to

343
00:13:47,279 --> 00:13:52,000
use

344
00:13:48,320 --> 00:13:53,920
which is 28 on my machine and obviously

345
00:13:52,000 --> 00:13:55,440
these numbers are pretty high for simple

346
00:13:53,920 --> 00:13:58,000
workloads when you just do

347
00:13:55,440 --> 00:13:58,639
trivial things with your jvm you don't

348
00:13:58,000 --> 00:14:01,199
need

349
00:13:58,639 --> 00:14:03,760
so many threads we already heard this

350
00:14:01,199 --> 00:14:06,800
morning that flats are expensive

351
00:14:03,760 --> 00:14:09,360
so we usually don't want that

352
00:14:06,800 --> 00:14:11,359
and that's why we have implemented a new

353
00:14:09,360 --> 00:14:12,880
feature that was contributed by us it's

354
00:14:11,360 --> 00:14:14,480
called dynamic number of compiler

355
00:14:12,880 --> 00:14:17,600
threads

356
00:14:14,480 --> 00:14:18,800
we already shipped it with 20k11 so it's

357
00:14:17,600 --> 00:14:21,839
not brand new but

358
00:14:18,800 --> 00:14:24,160
it's the first time it is shown at a

359
00:14:21,839 --> 00:14:28,320
conference i believe

360
00:14:24,160 --> 00:14:31,519
um and what we do by this new feature

361
00:14:28,320 --> 00:14:35,839
um we interpret these numbers as

362
00:14:31,519 --> 00:14:39,600
maximum numbers so we start up to 15

363
00:14:35,839 --> 00:14:42,320
compiler threads um

364
00:14:39,600 --> 00:14:44,079
and i'll get back to to that later but

365
00:14:42,320 --> 00:14:48,160
we start one

366
00:14:44,079 --> 00:14:51,519
thread of each type at startup and

367
00:14:48,160 --> 00:14:54,160
additional threads only on demand

368
00:14:51,519 --> 00:14:54,800
there's a similar feature called dynamic

369
00:14:54,160 --> 00:14:57,600
number of

370
00:14:54,800 --> 00:14:59,839
tc threads which was already implemented

371
00:14:57,600 --> 00:15:01,440
by oracle we just switched switched it

372
00:14:59,839 --> 00:15:05,040
on by default with tradik

373
00:15:01,440 --> 00:15:10,079
11 and with that you get

374
00:15:05,040 --> 00:15:13,120
of course much much lower resource usage

375
00:15:10,079 --> 00:15:15,920
um it's still possible to switch these

376
00:15:13,120 --> 00:15:16,560
features off to get to get the old

377
00:15:15,920 --> 00:15:19,439
behavior

378
00:15:16,560 --> 00:15:20,479
so all compiler and gc threads get

379
00:15:19,440 --> 00:15:24,480
started

380
00:15:20,480 --> 00:15:25,680
at the vm startup um i have tuned all

381
00:15:24,480 --> 00:15:28,880
the memory settings

382
00:15:25,680 --> 00:15:30,479
to very low sizes so the jvm should

383
00:15:28,880 --> 00:15:33,199
actually

384
00:15:30,480 --> 00:15:36,160
uh not use a lot of memory but you can

385
00:15:33,199 --> 00:15:39,839
see virtual memory is pretty high here

386
00:15:36,160 --> 00:15:42,560
and that's because of the threads

387
00:15:39,839 --> 00:15:43,519
they reserve a lot of virtual memory or

388
00:15:42,560 --> 00:15:46,239
they occupy

389
00:15:43,519 --> 00:15:48,160
virtual memory on linux due to the

390
00:15:46,240 --> 00:15:52,000
chilepsy

391
00:15:48,160 --> 00:15:54,560
and if you don't switch off these new

392
00:15:52,000 --> 00:15:57,680
features you can see we get a much lower

393
00:15:54,560 --> 00:16:02,000
virtual memory usage it's from

394
00:15:57,680 --> 00:16:02,000
6 gigabyte to 1.5 down

395
00:16:02,880 --> 00:16:08,639
but it's not about not only about um

396
00:16:06,320 --> 00:16:09,680
not only about virtual memory we of

397
00:16:08,639 --> 00:16:12,800
course

398
00:16:09,680 --> 00:16:16,479
also save other resources

399
00:16:12,800 --> 00:16:19,279
but you can trace um

400
00:16:16,480 --> 00:16:19,839
compiler threads also by this flag it's

401
00:16:19,279 --> 00:16:22,160
a

402
00:16:19,839 --> 00:16:23,199
diagnostic flag so you need to switch it

403
00:16:22,160 --> 00:16:26,480
on to enable

404
00:16:23,199 --> 00:16:28,560
to unlock these options and

405
00:16:26,480 --> 00:16:30,320
as already mentioned you can see that

406
00:16:28,560 --> 00:16:33,119
the jvm

407
00:16:30,320 --> 00:16:34,880
starts initially one com one compiler

408
00:16:33,120 --> 00:16:38,160
thread of each type

409
00:16:34,880 --> 00:16:40,240
so which is one c2 and one c one thread

410
00:16:38,160 --> 00:16:42,480
and they get kept alive for the whole

411
00:16:40,240 --> 00:16:45,680
lifetime of the trayvm

412
00:16:42,480 --> 00:16:48,320
and uh the other threats only get

413
00:16:45,680 --> 00:16:50,000
added on demand that depends on the

414
00:16:48,320 --> 00:16:52,560
compile queue length

415
00:16:50,000 --> 00:16:54,880
and also on the available memory and

416
00:16:52,560 --> 00:16:57,279
code cache space which is available

417
00:16:54,880 --> 00:16:58,240
because we don't want to mess up things

418
00:16:57,279 --> 00:17:00,639
when the

419
00:16:58,240 --> 00:17:01,440
memory is already full we don't want to

420
00:17:00,639 --> 00:17:04,799
start any

421
00:17:01,440 --> 00:17:07,760
any further threads

422
00:17:04,799 --> 00:17:10,000
and once they these compiler compiler

423
00:17:07,760 --> 00:17:13,039
threads don't have any work left to do

424
00:17:10,000 --> 00:17:15,039
they will die after some time

425
00:17:13,039 --> 00:17:17,280
and they die in the reverse order they

426
00:17:15,039 --> 00:17:20,640
were generated so we don't have any gaps

427
00:17:17,280 --> 00:17:20,639
in the compiler list

428
00:17:20,799 --> 00:17:26,079
so that's the feature we are already

429
00:17:24,000 --> 00:17:29,200
using

430
00:17:26,079 --> 00:17:31,760
and when one

431
00:17:29,200 --> 00:17:34,080
remark on the memory usage of the

432
00:17:31,760 --> 00:17:36,559
compilers

433
00:17:34,080 --> 00:17:38,240
c1 and c2 compilers of course use native

434
00:17:36,559 --> 00:17:41,280
memory

435
00:17:38,240 --> 00:17:44,160
and in comparison to that the growl

436
00:17:41,280 --> 00:17:46,160
compiler uses java heap

437
00:17:44,160 --> 00:17:47,360
so that may be an issue because your

438
00:17:46,160 --> 00:17:50,400
java application

439
00:17:47,360 --> 00:17:53,918
uses the same heap and you may

440
00:17:50,400 --> 00:17:58,160
need to to select to configure larger

441
00:17:53,919 --> 00:18:00,480
heap with the xmx flag

442
00:17:58,160 --> 00:18:02,960
otherwise you may get out of memory

443
00:18:00,480 --> 00:18:02,960
issues

444
00:18:03,039 --> 00:18:09,679
and it is also solved by

445
00:18:06,240 --> 00:18:11,039
using this chat library because that

446
00:18:09,679 --> 00:18:14,160
uses an

447
00:18:11,039 --> 00:18:16,240
a separate heap which is part of the

448
00:18:14,160 --> 00:18:19,600
native image technology

449
00:18:16,240 --> 00:18:21,520
so it's not it doesn't use the

450
00:18:19,600 --> 00:18:24,480
the regular java heap which you want to

451
00:18:21,520 --> 00:18:27,679
use for your application

452
00:18:24,480 --> 00:18:30,400
so that's already it but i wanted to

453
00:18:27,679 --> 00:18:32,320
tell maybe a few remarks it is also

454
00:18:30,400 --> 00:18:35,120
possible to configure

455
00:18:32,320 --> 00:18:36,799
the compiler threads to to use lower

456
00:18:35,120 --> 00:18:40,479
memory for example you can

457
00:18:36,799 --> 00:18:42,879
tune in lining but of course that may

458
00:18:40,480 --> 00:18:45,760
have performance implications

459
00:18:42,880 --> 00:18:46,640
and it is also possible to set a note

460
00:18:45,760 --> 00:18:48,720
limit

461
00:18:46,640 --> 00:18:51,440
for the c1 compiler that will make it

462
00:18:48,720 --> 00:18:52,720
smaller or will limit the the memory it

463
00:18:51,440 --> 00:18:54,960
uses

464
00:18:52,720 --> 00:18:56,960
but of course that has always side

465
00:18:54,960 --> 00:19:00,000
effects so i

466
00:18:56,960 --> 00:19:03,039
wouldn't recommend that in general

467
00:19:00,000 --> 00:19:11,840
so i i'm sure we have time for

468
00:19:03,039 --> 00:19:11,840
questions left excellent any questions

469
00:19:13,840 --> 00:19:18,159
yeah we need a microphone

470
00:19:19,919 --> 00:19:22,799
that's still here

471
00:19:29,600 --> 00:19:34,240
i was just wondering what the compiler

472
00:19:32,799 --> 00:19:36,400
thread count and

473
00:19:34,240 --> 00:19:38,559
heap size or virtual memory size or

474
00:19:36,400 --> 00:19:41,520
whatever sizes look like when you force

475
00:19:38,559 --> 00:19:42,480
uh tier one when you only run with c1

476
00:19:41,520 --> 00:19:45,200
because

477
00:19:42,480 --> 00:19:46,400
i would assume it's fewer threads and

478
00:19:45,200 --> 00:19:49,039
less heat but i don't

479
00:19:46,400 --> 00:19:50,559
you didn't cover that the virtual memory

480
00:19:49,039 --> 00:19:53,679
issue is due to

481
00:19:50,559 --> 00:19:57,520
to the malloc arenas from tree lib c

482
00:19:53,679 --> 00:20:00,720
um and the first allocation already uh

483
00:19:57,520 --> 00:20:02,799
occupies a 128 megabyte block of

484
00:20:00,720 --> 00:20:04,960
of heap it's not really used it's only

485
00:20:02,799 --> 00:20:07,520
virtual memory so in most cases it's not

486
00:20:04,960 --> 00:20:09,600
really a problem

487
00:20:07,520 --> 00:20:11,679
but that is independent of which

488
00:20:09,600 --> 00:20:13,520
compiler it is or which spread it is it

489
00:20:11,679 --> 00:20:16,240
also happens with java

490
00:20:13,520 --> 00:20:17,200
threads or with any other thread and

491
00:20:16,240 --> 00:20:19,520
there's also an

492
00:20:17,200 --> 00:20:21,840
another way to fix that you can

493
00:20:19,520 --> 00:20:25,200
configure the tlipc

494
00:20:21,840 --> 00:20:29,520
um to use lower less

495
00:20:25,200 --> 00:20:31,600
malloc arenas there's a malloc arena max

496
00:20:29,520 --> 00:20:34,080
environment variable and you can limit

497
00:20:31,600 --> 00:20:36,959
the memory by by using that

498
00:20:34,080 --> 00:20:38,158
that they that may have impacts on on

499
00:20:36,960 --> 00:20:40,720
other performance

500
00:20:38,159 --> 00:20:41,440
things um because if you have many

501
00:20:40,720 --> 00:20:43,600
native

502
00:20:41,440 --> 00:20:45,280
threads which perform a lot of

503
00:20:43,600 --> 00:20:47,600
concurrent mallocs

504
00:20:45,280 --> 00:20:49,918
you may get issues with that but for the

505
00:20:47,600 --> 00:20:52,959
jvm itself it works pretty well

506
00:20:49,919 --> 00:20:55,280
we have tried that we have experimented

507
00:20:52,960 --> 00:20:57,520
with using only one malloc arena

508
00:20:55,280 --> 00:20:59,200
and the trade vm itself still works

509
00:20:57,520 --> 00:21:02,240
quite okay because

510
00:20:59,200 --> 00:21:03,120
it has its own memory management and we

511
00:21:02,240 --> 00:21:05,280
are not so

512
00:21:03,120 --> 00:21:06,320
not so much using many concurrent

513
00:21:05,280 --> 00:21:09,520
mallocs

514
00:21:06,320 --> 00:21:09,520
small concurrent mallocs

515
00:21:11,039 --> 00:21:17,840
good question actually thanks

516
00:21:18,960 --> 00:21:31,840
further questions

517
00:21:32,480 --> 00:21:37,600
so i have a question um for the for the

518
00:21:35,520 --> 00:21:39,520
server compiler and the client compiler

519
00:21:37,600 --> 00:21:40,879
um the code cache is managed by the

520
00:21:39,520 --> 00:21:43,520
sweeper

521
00:21:40,880 --> 00:21:46,400
it's the same mechanism implemented for

522
00:21:43,520 --> 00:21:49,120
the growl vm

523
00:21:46,400 --> 00:21:51,120
um the sweeper has a different as a

524
00:21:49,120 --> 00:21:51,678
separate flat so it's no longer a part

525
00:21:51,120 --> 00:21:55,280
of the

526
00:21:51,679 --> 00:21:57,919
compiler flats so i'm not aware of any

527
00:21:55,280 --> 00:21:58,879
relationship between growl growl

528
00:21:57,919 --> 00:22:01,679
compiler and

529
00:21:58,880 --> 00:22:04,640
under the sweeper maybe andrew has has a

530
00:22:01,679 --> 00:22:04,640
few thoughts about that

531
00:22:10,840 --> 00:22:13,840
um

532
00:22:25,039 --> 00:22:29,200
but i do know that the the there's this

533
00:22:28,000 --> 00:22:31,360
external uh there's

534
00:22:29,200 --> 00:22:32,480
there's a change made to like external

535
00:22:31,360 --> 00:22:34,719
uh

536
00:22:32,480 --> 00:22:36,159
code segments not in the initial

537
00:22:34,720 --> 00:22:37,840
original code cache

538
00:22:36,159 --> 00:22:39,840
and they're wrapped with a stub that

539
00:22:37,840 --> 00:22:41,039
points to them so growl is managing some

540
00:22:39,840 --> 00:22:42,399
of its own memory i think and i'm not

541
00:22:41,039 --> 00:22:43,440
sure how that gets reclaimed

542
00:22:42,400 --> 00:22:45,280
but growl does know about

543
00:22:43,440 --> 00:22:46,880
de-optimization events it may be also

544
00:22:45,280 --> 00:22:48,879
there's a way they can find out about

545
00:22:46,880 --> 00:22:49,919
the fact that something is is has been

546
00:22:48,880 --> 00:22:51,120
uh released

547
00:22:49,919 --> 00:22:53,039
and there's a release protocol i just

548
00:22:51,120 --> 00:22:56,399
don't know for sure

549
00:22:53,039 --> 00:22:58,400
okay thanks i'm not

550
00:22:56,400 --> 00:23:01,039
not really a growl expert i've worked a

551
00:22:58,400 --> 00:23:05,120
lot on c1 and c2 but not so much in goal

552
00:23:01,039 --> 00:23:06,960
so but related to the code cache

553
00:23:05,120 --> 00:23:09,039
um there was a significant change back

554
00:23:06,960 --> 00:23:12,000
in the past we only had

555
00:23:09,039 --> 00:23:12,879
the sweeper run by by the the compiler

556
00:23:12,000 --> 00:23:15,200
threads and

557
00:23:12,880 --> 00:23:23,360
in the meantime we have a dedicated

558
00:23:15,200 --> 00:23:26,080
sweep of that

559
00:23:23,360 --> 00:23:26,080
more questions

560
00:23:30,640 --> 00:23:37,840
so i think we're done yeah thanks

561
00:23:32,720 --> 00:23:37,840
everyone for your attention

562
00:23:44,240 --> 00:23:46,320
you

