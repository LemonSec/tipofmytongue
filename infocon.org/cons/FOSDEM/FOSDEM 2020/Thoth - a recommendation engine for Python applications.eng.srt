1
00:00:05,359 --> 00:00:08,960
okay everyone

2
00:00:06,240 --> 00:00:09,840
so please welcome friday who's gonna

3
00:00:08,960 --> 00:00:14,880
tell us about

4
00:00:09,840 --> 00:00:14,879
uh tots the recommendation engine for

5
00:00:14,920 --> 00:00:17,920
python

6
00:00:19,600 --> 00:00:22,640
okay so hello everyone welcome to this

7
00:00:21,840 --> 00:00:25,519
presentation

8
00:00:22,640 --> 00:00:27,599
about project tots about recommendation

9
00:00:25,519 --> 00:00:30,159
engine for python applications

10
00:00:27,599 --> 00:00:31,439
before i start let me introduce myself

11
00:00:30,160 --> 00:00:33,680
my name is fridolin

12
00:00:31,439 --> 00:00:35,840
i work at redhead i'm not a first time

13
00:00:33,680 --> 00:00:39,040
speaker here so you probably see me

14
00:00:35,840 --> 00:00:41,920
like two years back i

15
00:00:39,040 --> 00:00:43,680
had a talk about project celino right

16
00:00:41,920 --> 00:00:47,120
now i work on project tort

17
00:00:43,680 --> 00:00:49,440
i'm one of the developers of tot and

18
00:00:47,120 --> 00:00:50,320
i would like to introduce that project

19
00:00:49,440 --> 00:00:54,079
to you

20
00:00:50,320 --> 00:00:56,480
so what is tot and why i thought

21
00:00:54,079 --> 00:00:57,680
you probably know the pi pi the python

22
00:00:56,480 --> 00:01:00,398
package index

23
00:00:57,680 --> 00:01:01,280
that's index that hosts open source

24
00:01:00,399 --> 00:01:04,080
projects

25
00:01:01,280 --> 00:01:05,519
and when i wrote these slides uh i found

26
00:01:04,080 --> 00:01:08,640
out that there is something like

27
00:01:05,519 --> 00:01:10,159
200 000 projects available out there

28
00:01:08,640 --> 00:01:15,520
free to use

29
00:01:10,159 --> 00:01:17,360
and there are about 1.6 million releases

30
00:01:15,520 --> 00:01:19,600
uh that's quite a huge number i would

31
00:01:17,360 --> 00:01:21,840
say and this number

32
00:01:19,600 --> 00:01:23,119
grows each and every day especially with

33
00:01:21,840 --> 00:01:26,560
popularity that

34
00:01:23,119 --> 00:01:29,840
python ecosystem uh has

35
00:01:26,560 --> 00:01:32,560
so let's try to use some packages

36
00:01:29,840 --> 00:01:33,360
so let's create some artificial example

37
00:01:32,560 --> 00:01:35,280
and

38
00:01:33,360 --> 00:01:36,960
use some packages that are hosted on pi

39
00:01:35,280 --> 00:01:38,960
pi so let's say

40
00:01:36,960 --> 00:01:40,559
we would like to use tensorflow a

41
00:01:38,960 --> 00:01:43,679
machine learning library

42
00:01:40,560 --> 00:01:46,399
and also flask so we install these

43
00:01:43,680 --> 00:01:49,680
dependencies using for example pip

44
00:01:46,399 --> 00:01:51,680
and we write our application that uses

45
00:01:49,680 --> 00:01:54,399
these two libraries

46
00:01:51,680 --> 00:01:55,600
okay but if you take a look at this

47
00:01:54,399 --> 00:01:57,600
scenario it's not

48
00:01:55,600 --> 00:01:58,798
just your application with these two

49
00:01:57,600 --> 00:02:01,119
libraries

50
00:01:58,799 --> 00:02:02,079
uh by installing these libraries you

51
00:02:01,119 --> 00:02:04,719
also introduce

52
00:02:02,079 --> 00:02:05,360
most of the time transitive dependencies

53
00:02:04,719 --> 00:02:07,919
and

54
00:02:05,360 --> 00:02:08,479
uh you use some python interpreter that

55
00:02:07,920 --> 00:02:11,920
uses

56
00:02:08,479 --> 00:02:12,640
these packages this python interpreter

57
00:02:11,920 --> 00:02:14,238
then runs

58
00:02:12,640 --> 00:02:16,559
inside some operating system that

59
00:02:14,239 --> 00:02:20,480
provides some packages

60
00:02:16,560 --> 00:02:23,280
such as glipc that is in our case used

61
00:02:20,480 --> 00:02:24,560
by tensorflow and then there is that

62
00:02:23,280 --> 00:02:27,760
kernel space

63
00:02:24,560 --> 00:02:29,440
where operating system runs and provides

64
00:02:27,760 --> 00:02:31,840
some kernel modules

65
00:02:29,440 --> 00:02:34,000
and this all runs on some hardware you

66
00:02:31,840 --> 00:02:36,640
can imagine that if you have any issue

67
00:02:34,000 --> 00:02:39,120
in any layer of this stack then your

68
00:02:36,640 --> 00:02:42,000
application simply misbehaves or

69
00:02:39,120 --> 00:02:42,879
provides wrong outputs or simply doesn't

70
00:02:42,000 --> 00:02:46,080
even

71
00:02:42,879 --> 00:02:47,840
run or doesn't even start so let's take

72
00:02:46,080 --> 00:02:51,120
a look

73
00:02:47,840 --> 00:02:53,200
at this layer let's take a look at

74
00:02:51,120 --> 00:02:54,720
direct dependencies and transitive

75
00:02:53,200 --> 00:02:57,679
dependencies

76
00:02:54,720 --> 00:03:00,159
uh when i were when i was writing these

77
00:02:57,680 --> 00:03:03,599
slides i took a look about

78
00:03:00,159 --> 00:03:06,079
uh at packages

79
00:03:03,599 --> 00:03:07,599
and i found out that there are 33

80
00:03:06,080 --> 00:03:09,680
releases of flask

81
00:03:07,599 --> 00:03:11,760
and when you do pip install flask you

82
00:03:09,680 --> 00:03:13,840
introduce five additional packages such

83
00:03:11,760 --> 00:03:17,280
as click it's dangerous ginger

84
00:03:13,840 --> 00:03:18,480
and so on these all are also released in

85
00:03:17,280 --> 00:03:21,280
different versions

86
00:03:18,480 --> 00:03:23,280
and now let's suppose that we find all

87
00:03:21,280 --> 00:03:25,040
the versions of these libraries

88
00:03:23,280 --> 00:03:27,440
and we would like to know how many

89
00:03:25,040 --> 00:03:27,920
combinations are there to install flask

90
00:03:27,440 --> 00:03:31,040
with

91
00:03:27,920 --> 00:03:32,000
its uh dependencies and i got this huge

92
00:03:31,040 --> 00:03:35,440
number this is

93
00:03:32,000 --> 00:03:36,159
estimation uh so the actual number will

94
00:03:35,440 --> 00:03:38,319
vary

95
00:03:36,159 --> 00:03:39,359
uh based on actual resolution but you

96
00:03:38,319 --> 00:03:41,200
see that there is

97
00:03:39,360 --> 00:03:43,280
like something like 50 million

98
00:03:41,200 --> 00:03:46,159
possibilities how you can install

99
00:03:43,280 --> 00:03:47,519
a flask flask application with different

100
00:03:46,159 --> 00:03:50,239
versions of

101
00:03:47,519 --> 00:03:51,519
its dependencies i did the same for

102
00:03:50,239 --> 00:03:53,840
tensorflow

103
00:03:51,519 --> 00:03:55,760
and at the time i was writing these

104
00:03:53,840 --> 00:03:56,799
slides there were 85 releases of

105
00:03:55,760 --> 00:03:59,200
tensorflow

106
00:03:56,799 --> 00:04:00,319
and tensorflow is much bigger project

107
00:03:59,200 --> 00:04:03,920
than flask

108
00:04:00,319 --> 00:04:06,399
and it introduces something like 30 uh

109
00:04:03,920 --> 00:04:07,359
additional packages when i did uh

110
00:04:06,400 --> 00:04:09,680
estimation of

111
00:04:07,360 --> 00:04:11,760
on how many combinations are there to

112
00:04:09,680 --> 00:04:14,640
install a tensorflow

113
00:04:11,760 --> 00:04:17,680
stack i got this huge number so it was

114
00:04:14,640 --> 00:04:20,719
something like 10 to the power of 20.

115
00:04:17,680 --> 00:04:24,639
if we take a look at some

116
00:04:20,720 --> 00:04:27,840
ai approaches that are uh today

117
00:04:24,639 --> 00:04:30,400
you probably know alphago the famous go

118
00:04:27,840 --> 00:04:33,520
ai that beats every go player

119
00:04:30,400 --> 00:04:37,840
and it uses machine learning to

120
00:04:33,520 --> 00:04:41,919
to find best possible move in a game

121
00:04:37,840 --> 00:04:44,719
and alphago or or go

122
00:04:41,919 --> 00:04:48,159
game in general has something like 10 to

123
00:04:44,720 --> 00:04:52,000
the power of 172 combinations

124
00:04:48,160 --> 00:04:55,360
how you can place uh things on on board

125
00:04:52,000 --> 00:04:58,400
so it's much bigger a much bigger number

126
00:04:55,360 --> 00:05:01,039
uh compared to other number but

127
00:04:58,400 --> 00:05:02,400
in project thought we took similar

128
00:05:01,039 --> 00:05:06,240
approaches the

129
00:05:02,400 --> 00:05:09,840
ss alphago and we are trying to

130
00:05:06,240 --> 00:05:12,960
trying to solve uh developers uh

131
00:05:09,840 --> 00:05:15,359
issues with stacks so this was

132
00:05:12,960 --> 00:05:16,560
this layer so we had something like 10

133
00:05:15,360 --> 00:05:19,520
to the power of 20

134
00:05:16,560 --> 00:05:21,360
number of combinations but there is ma

135
00:05:19,520 --> 00:05:24,159
there is much more layers

136
00:05:21,360 --> 00:05:25,120
so if we take a look at this software

137
00:05:24,160 --> 00:05:27,440
layer

138
00:05:25,120 --> 00:05:28,800
there are different python interpreters

139
00:05:27,440 --> 00:05:29,759
in different versions that you can

140
00:05:28,800 --> 00:05:31,360
install

141
00:05:29,759 --> 00:05:33,039
then there are different operating

142
00:05:31,360 --> 00:05:34,479
systems in different versions that you

143
00:05:33,039 --> 00:05:36,960
can install

144
00:05:34,479 --> 00:05:39,599
then you have packages available on pi

145
00:05:36,960 --> 00:05:42,400
api such as flask or tensorflow

146
00:05:39,600 --> 00:05:43,840
or you can host your own packages on

147
00:05:42,400 --> 00:05:47,120
your own

148
00:05:43,840 --> 00:05:49,840
index and we do so in our team

149
00:05:47,120 --> 00:05:51,360
we optimize tensorflow by switching

150
00:05:49,840 --> 00:05:54,080
different compiler flex

151
00:05:51,360 --> 00:05:54,800
and we provide a community index that

152
00:05:54,080 --> 00:05:57,280
hosts

153
00:05:54,800 --> 00:05:58,560
tensorflow releases free to use so if

154
00:05:57,280 --> 00:06:01,119
you use tensorflow

155
00:05:58,560 --> 00:06:02,479
you can use these releases and gain some

156
00:06:01,120 --> 00:06:05,600
performance

157
00:06:02,479 --> 00:06:09,680
okay so now the question is

158
00:06:05,600 --> 00:06:13,280
what should i use out of these

159
00:06:09,680 --> 00:06:16,240
options that i have now let's simplify

160
00:06:13,280 --> 00:06:17,440
our other use case and let's say that we

161
00:06:16,240 --> 00:06:20,240
have

162
00:06:17,440 --> 00:06:22,080
some two libraries one is called simply

163
00:06:20,240 --> 00:06:23,360
and the other one is another lip so it's

164
00:06:22,080 --> 00:06:26,080
similar case as

165
00:06:23,360 --> 00:06:26,880
flask and tensorflow but in this case

166
00:06:26,080 --> 00:06:28,560
simply

167
00:06:26,880 --> 00:06:30,479
will not introduce any transitive

168
00:06:28,560 --> 00:06:32,880
dependencies to our stack

169
00:06:30,479 --> 00:06:34,000
and another leap will not introduce any

170
00:06:32,880 --> 00:06:36,479
any additional

171
00:06:34,000 --> 00:06:37,199
libraries to understand in other words

172
00:06:36,479 --> 00:06:40,159
we install

173
00:06:37,199 --> 00:06:42,080
just these two libraries when we run our

174
00:06:40,160 --> 00:06:44,720
python application

175
00:06:42,080 --> 00:06:45,120
now we would like to find some function

176
00:06:44,720 --> 00:06:48,479
that

177
00:06:45,120 --> 00:06:51,520
describes how good our software stack is

178
00:06:48,479 --> 00:06:53,599
so we would like to install simplip and

179
00:06:51,520 --> 00:06:56,240
another leap in different versions

180
00:06:53,599 --> 00:06:58,479
and evaluate how good the given software

181
00:06:56,240 --> 00:07:01,680
stack is

182
00:06:58,479 --> 00:07:04,400
we can do so and we can create

183
00:07:01,680 --> 00:07:06,319
such function and we install different

184
00:07:04,400 --> 00:07:07,198
versions of simply different versions of

185
00:07:06,319 --> 00:07:10,000
another leap

186
00:07:07,199 --> 00:07:11,840
and then we have some overall score this

187
00:07:10,000 --> 00:07:14,880
function is discrete so we have

188
00:07:11,840 --> 00:07:16,799
discrete values and let's try to

189
00:07:14,880 --> 00:07:19,840
interpolate these values

190
00:07:16,800 --> 00:07:22,800
what we get we get a surface and

191
00:07:19,840 --> 00:07:24,159
uh we see that in some cases our

192
00:07:22,800 --> 00:07:28,080
application

193
00:07:24,160 --> 00:07:30,560
is scored better in some cases uh burst

194
00:07:28,080 --> 00:07:32,560
uh what this score can mean it can be

195
00:07:30,560 --> 00:07:33,919
what mean whatever you can think of it

196
00:07:32,560 --> 00:07:36,639
can be for example number of

197
00:07:33,919 --> 00:07:39,039
vulnerabilities present in your software

198
00:07:36,639 --> 00:07:41,199
it can be also performance of your

199
00:07:39,039 --> 00:07:42,880
machine learning model

200
00:07:41,199 --> 00:07:44,960
but it can be also a combination of

201
00:07:42,880 --> 00:07:48,960
these two or any other

202
00:07:44,960 --> 00:07:50,719
metric you could think about

203
00:07:48,960 --> 00:07:52,318
what we are looking for we are looking

204
00:07:50,720 --> 00:07:54,800
for these spikes

205
00:07:52,319 --> 00:07:55,599
like values that are very high high

206
00:07:54,800 --> 00:07:58,800
score

207
00:07:55,599 --> 00:08:01,199
and we assume that the scoring function

208
00:07:58,800 --> 00:08:04,319
returns higher values for software

209
00:08:01,199 --> 00:08:07,680
uh that is good for our use case

210
00:08:04,319 --> 00:08:10,319
so uh that's what we are looking for

211
00:08:07,680 --> 00:08:11,440
and that's basically taught in thoughts

212
00:08:10,319 --> 00:08:14,560
we aggregate some

213
00:08:11,440 --> 00:08:17,360
knowledge about packages such as

214
00:08:14,560 --> 00:08:18,560
if the application builds correctly if

215
00:08:17,360 --> 00:08:20,400
it runs correctly

216
00:08:18,560 --> 00:08:23,120
if it runs correctly on different python

217
00:08:20,400 --> 00:08:25,758
versions in different operating systems

218
00:08:23,120 --> 00:08:27,199
uh if it behaves correctly what's the

219
00:08:25,759 --> 00:08:31,280
overall performance

220
00:08:27,199 --> 00:08:33,599
and then there is written and a resolver

221
00:08:31,280 --> 00:08:34,880
that takes into account these uh these

222
00:08:33,599 --> 00:08:37,360
observations

223
00:08:34,880 --> 00:08:39,120
and can resolve software stacks that are

224
00:08:37,360 --> 00:08:40,080
high performing so if you remember

225
00:08:39,120 --> 00:08:43,760
that's a slide

226
00:08:40,080 --> 00:08:48,160
with a function uh these spikes

227
00:08:43,760 --> 00:08:51,519
are in the wall state space

228
00:08:48,160 --> 00:08:52,719
so we say that our that latest versions

229
00:08:51,519 --> 00:08:56,000
are not always

230
00:08:52,720 --> 00:08:58,240
greatest choices and that's basically

231
00:08:56,000 --> 00:09:00,399
the idea behind dots

232
00:08:58,240 --> 00:09:01,839
thought is a bigger project it runs in

233
00:09:00,399 --> 00:09:05,040
openshift there are

234
00:09:01,839 --> 00:09:05,920
components and different integration

235
00:09:05,040 --> 00:09:08,000
points

236
00:09:05,920 --> 00:09:08,959
but the most interesting component for

237
00:09:08,000 --> 00:09:12,399
this slide

238
00:09:08,959 --> 00:09:16,000
for this talk is advisor that's

239
00:09:12,399 --> 00:09:18,080
a component inside sort and it's

240
00:09:16,000 --> 00:09:19,200
basically the implementation of python

241
00:09:18,080 --> 00:09:22,320
resolver

242
00:09:19,200 --> 00:09:24,640
so uh thought is a

243
00:09:22,320 --> 00:09:26,720
service uh that provides endpoints that

244
00:09:24,640 --> 00:09:29,839
you can consume data from

245
00:09:26,720 --> 00:09:32,240
and it's pure server-side resolution

246
00:09:29,839 --> 00:09:33,839
we had multiple iterations on

247
00:09:32,240 --> 00:09:36,640
implementation

248
00:09:33,839 --> 00:09:38,399
the very first implementation was pure

249
00:09:36,640 --> 00:09:41,120
python implementation

250
00:09:38,399 --> 00:09:42,480
and uh it loaded the world dependency

251
00:09:41,120 --> 00:09:44,480
graph into memory

252
00:09:42,480 --> 00:09:45,760
and then we had representation of these

253
00:09:44,480 --> 00:09:48,720
packages

254
00:09:45,760 --> 00:09:50,240
that was basically an ari graph and we

255
00:09:48,720 --> 00:09:52,720
did transactional operations

256
00:09:50,240 --> 00:09:54,880
so we said these remove these three

257
00:09:52,720 --> 00:09:57,200
packages from dependency graph

258
00:09:54,880 --> 00:09:57,920
and that transaction air proceeded or

259
00:09:57,200 --> 00:10:00,240
not

260
00:09:57,920 --> 00:10:02,640
it was also possible to score uh these

261
00:10:00,240 --> 00:10:04,560
packages and the dependency graph was

262
00:10:02,640 --> 00:10:07,600
adjusted in a way that

263
00:10:04,560 --> 00:10:10,800
resolution led to to

264
00:10:07,600 --> 00:10:13,519
higher performing stacks sooner

265
00:10:10,800 --> 00:10:14,240
this implementation was however memory

266
00:10:13,519 --> 00:10:16,640
quite

267
00:10:14,240 --> 00:10:18,079
hungry so the main issue was memory

268
00:10:16,640 --> 00:10:21,519
consumption and

269
00:10:18,079 --> 00:10:24,319
as you know objects in python

270
00:10:21,519 --> 00:10:25,839
carry some additional metadata such as

271
00:10:24,320 --> 00:10:29,600
reference counts

272
00:10:25,839 --> 00:10:32,240
and for example for a tensorflow stack

273
00:10:29,600 --> 00:10:33,200
we required something like 32 gigabytes

274
00:10:32,240 --> 00:10:36,800
just to resolve

275
00:10:33,200 --> 00:10:39,760
a software stack so we abandoned

276
00:10:36,800 --> 00:10:40,319
this uh solution and we rewritten the

277
00:10:39,760 --> 00:10:43,839
wall

278
00:10:40,320 --> 00:10:47,040
part into like the core part into c

279
00:10:43,839 --> 00:10:48,640
c plus plus so we designed a protocol

280
00:10:47,040 --> 00:10:51,279
that effectively serialized the wall

281
00:10:48,640 --> 00:10:54,399
dependency graph and we were able to

282
00:10:51,279 --> 00:10:55,680
uh keep nodes of dependency graph in

283
00:10:54,399 --> 00:10:59,120
something like 24

284
00:10:55,680 --> 00:11:02,640
bytes and we gained

285
00:10:59,120 --> 00:11:05,040
also uh memory consumption improvements

286
00:11:02,640 --> 00:11:05,760
however we reached another bottleneck

287
00:11:05,040 --> 00:11:08,719
and then was

288
00:11:05,760 --> 00:11:09,439
a number of queries that were hit to uh

289
00:11:08,720 --> 00:11:12,480
to our

290
00:11:09,440 --> 00:11:14,560
knowledge base or to our database just

291
00:11:12,480 --> 00:11:17,120
to obtain the dependency graph

292
00:11:14,560 --> 00:11:18,319
so there were something like two and a

293
00:11:17,120 --> 00:11:20,959
half thousand

294
00:11:18,320 --> 00:11:21,680
queries to to our database just to

295
00:11:20,959 --> 00:11:24,239
obtain

296
00:11:21,680 --> 00:11:25,760
uh tensorflow's dependency graph and

297
00:11:24,240 --> 00:11:29,360
then there were subsequent

298
00:11:25,760 --> 00:11:32,000
queries uh to score the dependency graph

299
00:11:29,360 --> 00:11:33,200
and that required a lot of pressure on

300
00:11:32,000 --> 00:11:36,560
databases so

301
00:11:33,200 --> 00:11:40,160
we change database two times

302
00:11:36,560 --> 00:11:43,760
so right now we run thirds database and

303
00:11:40,160 --> 00:11:47,199
later on we also decided to abandon this

304
00:11:43,760 --> 00:11:50,399
solution because of these queries that's

305
00:11:47,200 --> 00:11:51,600
quite a huge number so we looked for

306
00:11:50,399 --> 00:11:54,160
some solutions

307
00:11:51,600 --> 00:11:55,200
that we could apply from theoretical

308
00:11:54,160 --> 00:11:58,079
informatics

309
00:11:55,200 --> 00:11:58,800
and we did operations research so we

310
00:11:58,079 --> 00:12:02,239
implemented

311
00:11:58,800 --> 00:12:04,399
different types of of uh resolving

312
00:12:02,240 --> 00:12:05,600
for example hill climbing or simulated

313
00:12:04,399 --> 00:12:08,959
annealing

314
00:12:05,600 --> 00:12:10,240
and this rethinking of the wall resolve

315
00:12:08,959 --> 00:12:13,680
roulette into

316
00:12:10,240 --> 00:12:17,040
split the wall resolver into two parts

317
00:12:13,680 --> 00:12:20,560
one was resolver that can lazily

318
00:12:17,040 --> 00:12:23,040
uh resolve software stack based on

319
00:12:20,560 --> 00:12:24,319
based on python uh ecosystem

320
00:12:23,040 --> 00:12:26,639
specification

321
00:12:24,320 --> 00:12:28,079
and then there is predictor that guides

322
00:12:26,639 --> 00:12:30,959
this resolver

323
00:12:28,079 --> 00:12:32,160
uh on which packages in which versions

324
00:12:30,959 --> 00:12:34,880
should be resolved in

325
00:12:32,160 --> 00:12:37,040
order to come up with some software

326
00:12:34,880 --> 00:12:40,079
stack

327
00:12:37,040 --> 00:12:41,120
this uh implementation worked for us but

328
00:12:40,079 --> 00:12:44,880
if you think about

329
00:12:41,120 --> 00:12:47,440
it um uh we were basically randomly

330
00:12:44,880 --> 00:12:49,600
sampling this wall state space of uh

331
00:12:47,440 --> 00:12:52,800
software stacks that can be resolved

332
00:12:49,600 --> 00:12:56,079
so we randomly picked some scored

333
00:12:52,800 --> 00:12:59,760
it and evaluated the score that's

334
00:12:56,079 --> 00:13:01,359
not nice so we try to find another

335
00:12:59,760 --> 00:13:03,600
solutions

336
00:13:01,360 --> 00:13:05,360
we know that there is this function that

337
00:13:03,600 --> 00:13:08,320
describes packages

338
00:13:05,360 --> 00:13:10,399
but what about finding gradients so if

339
00:13:08,320 --> 00:13:12,639
we are able to find the gradient

340
00:13:10,399 --> 00:13:13,680
of this function then the gradient can

341
00:13:12,639 --> 00:13:16,079
lead us to

342
00:13:13,680 --> 00:13:17,439
these spikes and we can find higher

343
00:13:16,079 --> 00:13:21,040
score

344
00:13:17,440 --> 00:13:22,959
scores software stacks much faster

345
00:13:21,040 --> 00:13:24,399
and that was basically the next approach

346
00:13:22,959 --> 00:13:28,560
we took

347
00:13:24,399 --> 00:13:28,560
so the most interesting

348
00:13:28,639 --> 00:13:32,639
paper that we evaluated was neural

349
00:13:30,880 --> 00:13:34,320
combinatorial optimization with

350
00:13:32,639 --> 00:13:37,200
reinforcement learning

351
00:13:34,320 --> 00:13:38,480
published by google brain and we really

352
00:13:37,200 --> 00:13:42,240
try to learn that

353
00:13:38,480 --> 00:13:44,800
gradient unfortunately

354
00:13:42,240 --> 00:13:46,560
you if you think about thought as a

355
00:13:44,800 --> 00:13:47,599
service that should be responsible to

356
00:13:46,560 --> 00:13:49,599
users

357
00:13:47,600 --> 00:13:51,600
you don't want to learn some some

358
00:13:49,600 --> 00:13:54,320
gradients and you don't want to

359
00:13:51,600 --> 00:13:55,680
spend time on learning some neural

360
00:13:54,320 --> 00:13:57,440
network

361
00:13:55,680 --> 00:13:59,040
and and provide inputs to neural

362
00:13:57,440 --> 00:14:00,320
networks so you don't want to spend two

363
00:13:59,040 --> 00:14:03,519
hours to learn

364
00:14:00,320 --> 00:14:04,320
to learn neural network and another 30

365
00:14:03,519 --> 00:14:06,800
minutes to

366
00:14:04,320 --> 00:14:08,399
to query your database so we also

367
00:14:06,800 --> 00:14:11,439
abandoned this solution

368
00:14:08,399 --> 00:14:12,800
and uh right now we use gradient-free

369
00:14:11,440 --> 00:14:14,800
methods

370
00:14:12,800 --> 00:14:16,719
so we implemented temporal difference

371
00:14:14,800 --> 00:14:19,040
and monte carlo research

372
00:14:16,720 --> 00:14:20,240
where monte carlo research looks the

373
00:14:19,040 --> 00:14:23,439
most promising

374
00:14:20,240 --> 00:14:23,920
way how to resolve uh graphs so how does

375
00:14:23,440 --> 00:14:27,199
it work

376
00:14:23,920 --> 00:14:29,519
we basically try to sample that's

377
00:14:27,199 --> 00:14:30,959
that state space that function of all

378
00:14:29,519 --> 00:14:34,560
possible uh

379
00:14:30,959 --> 00:14:37,839
australian uh resolved software stacks

380
00:14:34,560 --> 00:14:40,638
and we learn policy how to how to find

381
00:14:37,839 --> 00:14:42,160
the best software stack based on

382
00:14:40,639 --> 00:14:45,199
predictor and based on

383
00:14:42,160 --> 00:14:47,600
scoring mechanism in the resolver

384
00:14:45,199 --> 00:14:48,800
the resolver itself is a reconfiguration

385
00:14:47,600 --> 00:14:52,560
pipeline so

386
00:14:48,800 --> 00:14:53,199
uh you can write different pipeline

387
00:14:52,560 --> 00:14:55,199
units

388
00:14:53,199 --> 00:14:56,479
i think there are five pipeline units in

389
00:14:55,199 --> 00:14:59,120
total and

390
00:14:56,480 --> 00:15:00,320
this pipeline units are dynamically

391
00:14:59,120 --> 00:15:03,360
constructed on each

392
00:15:00,320 --> 00:15:06,560
request and they score

393
00:15:03,360 --> 00:15:10,000
uh actual resolver steps

394
00:15:06,560 --> 00:15:11,920
so uh this way we can for example

395
00:15:10,000 --> 00:15:14,399
uh plug a new pipeline you need if you

396
00:15:11,920 --> 00:15:17,279
are using convolutional neural network

397
00:15:14,399 --> 00:15:19,600
uh that is uh we can plug a pipeline

398
00:15:17,279 --> 00:15:22,000
unit that is specified just to score

399
00:15:19,600 --> 00:15:23,199
uh convolutional neural uh network

400
00:15:22,000 --> 00:15:26,399
layers

401
00:15:23,199 --> 00:15:28,479
um then a special component

402
00:15:26,399 --> 00:15:29,920
in our deployment is called dependency

403
00:15:28,480 --> 00:15:32,959
monkey and

404
00:15:29,920 --> 00:15:34,319
this component is able to gather

405
00:15:32,959 --> 00:15:36,079
observations for us

406
00:15:34,320 --> 00:15:37,440
such as if the application runs

407
00:15:36,079 --> 00:15:39,920
correctly uh

408
00:15:37,440 --> 00:15:41,199
if it behaves correctly what is the

409
00:15:39,920 --> 00:15:43,439
performance

410
00:15:41,199 --> 00:15:44,719
and this dependency monkey can sample

411
00:15:43,440 --> 00:15:47,920
that uh state space

412
00:15:44,720 --> 00:15:50,880
of all the possible uh stacks

413
00:15:47,920 --> 00:15:51,439
and uh resolve software stacks that we

414
00:15:50,880 --> 00:15:54,480
don't

415
00:15:51,440 --> 00:15:55,120
have any observations about and submit

416
00:15:54,480 --> 00:15:58,160
them to

417
00:15:55,120 --> 00:16:00,399
to our service that evaluates how good

418
00:15:58,160 --> 00:16:04,399
the given software stack is

419
00:16:00,399 --> 00:16:04,399
so this is how we are

420
00:16:07,759 --> 00:16:13,440
advisor in our stage environment so

421
00:16:11,040 --> 00:16:15,040
we have thought deployed at red hat and

422
00:16:13,440 --> 00:16:18,399
i asked to resolve

423
00:16:15,040 --> 00:16:21,920
a tensorflow stack

424
00:16:18,399 --> 00:16:23,519
tensorflow stack together with flask uh

425
00:16:21,920 --> 00:16:25,199
if you if you can see the

426
00:16:23,519 --> 00:16:26,800
resolution took something like eight

427
00:16:25,199 --> 00:16:29,279
seconds uh

428
00:16:26,800 --> 00:16:30,959
i asked to resolve latest software stack

429
00:16:29,279 --> 00:16:32,320
when i asked to resolve the best

430
00:16:30,959 --> 00:16:35,119
possible software stack

431
00:16:32,320 --> 00:16:36,000
the resolution took something like two

432
00:16:35,120 --> 00:16:39,120
and

433
00:16:36,000 --> 00:16:42,320
2.7 minutes so almost three minutes and

434
00:16:39,120 --> 00:16:45,759
we were able to score a half million of

435
00:16:42,320 --> 00:16:49,440
uh software stacks if you compare it to

436
00:16:45,759 --> 00:16:52,800
uh p pen uh the packaging

437
00:16:49,440 --> 00:16:54,880
tool that is recommended by pipa

438
00:16:52,800 --> 00:16:56,399
when i tried to install tensorflow and

439
00:16:54,880 --> 00:16:59,839
flask and

440
00:16:56,399 --> 00:17:01,839
ask to resolve its latest version the

441
00:16:59,839 --> 00:17:03,440
resolution took something like one

442
00:17:01,839 --> 00:17:06,880
minute and i had

443
00:17:03,440 --> 00:17:10,000
turned on pip cache so uh

444
00:17:06,880 --> 00:17:11,199
so it it was using cached artifacts from

445
00:17:10,000 --> 00:17:13,760
pipa

446
00:17:11,199 --> 00:17:14,480
so we are really benefiting of that

447
00:17:13,760 --> 00:17:16,720
offline

448
00:17:14,480 --> 00:17:18,079
resolution that we have inside our

449
00:17:16,720 --> 00:17:20,240
resolver where we

450
00:17:18,079 --> 00:17:22,000
do not contact directly by pa but we

451
00:17:20,240 --> 00:17:25,439
have pre-computed data

452
00:17:22,000 --> 00:17:29,120
and we can resolve software sticks

453
00:17:25,439 --> 00:17:31,600
quite fast there are also other parts of

454
00:17:29,120 --> 00:17:32,399
thoughts that i haven't spoke about for

455
00:17:31,600 --> 00:17:36,159
example we use

456
00:17:32,400 --> 00:17:38,480
bots to automate updates of dependencies

457
00:17:36,160 --> 00:17:41,919
new releases of bots components

458
00:17:38,480 --> 00:17:42,480
of thoughts components you can find also

459
00:17:41,919 --> 00:17:45,760
our

460
00:17:42,480 --> 00:17:49,280
tensorflow index that hosts these

461
00:17:45,760 --> 00:17:52,799
tensorflow artifacts that are optimized

462
00:17:49,280 --> 00:17:53,360
for performance and we have integration

463
00:17:52,799 --> 00:17:55,760
with

464
00:17:53,360 --> 00:17:56,959
different tools such as openshift or

465
00:17:55,760 --> 00:18:00,000
jupyter notebooks

466
00:17:56,960 --> 00:18:00,320
so if you are a data scientist and you

467
00:18:00,000 --> 00:18:02,799
use

468
00:18:00,320 --> 00:18:04,960
jupiter notebooks you can enable our

469
00:18:02,799 --> 00:18:07,280
extension to jupiter notebooks and

470
00:18:04,960 --> 00:18:08,160
it will talk to taut and tot will

471
00:18:07,280 --> 00:18:10,480
resolve

472
00:18:08,160 --> 00:18:12,640
software stacks for you then there is

473
00:18:10,480 --> 00:18:12,960
command line interface that is similar

474
00:18:12,640 --> 00:18:15,840
to

475
00:18:12,960 --> 00:18:15,840
pip or pen

476
00:18:16,000 --> 00:18:21,360
if you have a github

477
00:18:19,200 --> 00:18:22,720
repository and you use python

478
00:18:21,360 --> 00:18:25,840
applications

479
00:18:22,720 --> 00:18:29,039
we soon will release a developer

480
00:18:25,840 --> 00:18:31,678
developer preview for

481
00:18:29,039 --> 00:18:32,879
our application that is called caberget

482
00:18:31,679 --> 00:18:34,880
and you can

483
00:18:32,880 --> 00:18:35,919
install it you can enable it for your

484
00:18:34,880 --> 00:18:37,679
repository

485
00:18:35,919 --> 00:18:40,000
and your dependencies will be

486
00:18:37,679 --> 00:18:42,480
automatically managed by

487
00:18:40,000 --> 00:18:44,480
thoughts so if you want to give tot a

488
00:18:42,480 --> 00:18:47,840
try feel free to install

489
00:18:44,480 --> 00:18:50,720
this application and

490
00:18:47,840 --> 00:18:52,879
we can make thoughts better so it's a

491
00:18:50,720 --> 00:18:54,960
community project

492
00:18:52,880 --> 00:18:57,440
you can find information about thoughts

493
00:18:54,960 --> 00:18:58,160
on these sites we have twitter so feel

494
00:18:57,440 --> 00:19:02,080
free to follow

495
00:18:58,160 --> 00:19:04,720
us on station twitter handle

496
00:19:02,080 --> 00:19:05,439
we are open source so you can find us on

497
00:19:04,720 --> 00:19:08,000
github

498
00:19:05,440 --> 00:19:09,200
under tod station organization and if

499
00:19:08,000 --> 00:19:12,559
you are interested in

500
00:19:09,200 --> 00:19:13,679
bots there is link to both and if you

501
00:19:12,559 --> 00:19:17,200
have any feedback

502
00:19:13,679 --> 00:19:20,080
for a bot you can submit it to

503
00:19:17,200 --> 00:19:21,200
our feedback form so this way i would

504
00:19:20,080 --> 00:19:25,840
like to thank you

505
00:19:21,200 --> 00:19:25,840
and that's it

506
00:19:26,600 --> 00:19:30,600
[Applause]

507
00:19:30,960 --> 00:19:34,880
we have plenty of time for questions

508
00:19:36,840 --> 00:19:39,840
unless

509
00:19:49,039 --> 00:19:53,200
so the question was whether this is just

510
00:19:51,840 --> 00:19:56,000
for pipe pi

511
00:19:53,200 --> 00:19:58,000
or if this solution will work also for

512
00:19:56,000 --> 00:20:02,640
your local packages

513
00:19:58,000 --> 00:20:05,600
uh in tot we can register python index

514
00:20:02,640 --> 00:20:06,000
if it follows a pep specification of

515
00:20:05,600 --> 00:20:09,199
python

516
00:20:06,000 --> 00:20:10,159
index and if it is publicly hosted we

517
00:20:09,200 --> 00:20:13,280
are able to

518
00:20:10,159 --> 00:20:21,840
analyze these packages and

519
00:20:13,280 --> 00:20:21,840
resolve and use them for resolver so

520
00:20:26,080 --> 00:20:29,918
when computing the number of possible

521
00:20:28,960 --> 00:20:32,080
configuration

522
00:20:29,919 --> 00:20:34,559
did you take into account the actual

523
00:20:32,080 --> 00:20:35,520
version requirements or it was just

524
00:20:34,559 --> 00:20:38,000
based on the

525
00:20:35,520 --> 00:20:39,280
name of each package uh sorry can you

526
00:20:38,000 --> 00:20:42,559
can you repeat

527
00:20:39,280 --> 00:20:46,399
is that better now so when computing the

528
00:20:42,559 --> 00:20:48,799
the number of possible configuration

529
00:20:46,400 --> 00:20:50,000
did you take into account the actual

530
00:20:48,799 --> 00:20:52,879
version requirements

531
00:20:50,000 --> 00:20:54,880
or it was just based on the name of each

532
00:20:52,880 --> 00:20:57,120
package

533
00:20:54,880 --> 00:20:58,720
uh it was based on actual version

534
00:20:57,120 --> 00:21:01,520
specification

535
00:20:58,720 --> 00:21:02,799
so some packages were really discarded

536
00:21:01,520 --> 00:21:04,158
like older versions were really

537
00:21:02,799 --> 00:21:08,240
discarded by

538
00:21:04,159 --> 00:21:08,240
by these combinations

539
00:21:12,640 --> 00:21:16,480
how trustworthy is the the output uh of

540
00:21:15,600 --> 00:21:19,120
thought say

541
00:21:16,480 --> 00:21:21,280
it tells you that uh the best uh score

542
00:21:19,120 --> 00:21:24,320
for your software stack uh is using

543
00:21:21,280 --> 00:21:27,840
uh a pretty old version of a

544
00:21:24,320 --> 00:21:30,720
simple lib or another lib and you may

545
00:21:27,840 --> 00:21:31,520
be missing some features that you need

546
00:21:30,720 --> 00:21:32,799
but

547
00:21:31,520 --> 00:21:34,240
thought is telling you that your

548
00:21:32,799 --> 00:21:35,360
software stack is going to have a better

549
00:21:34,240 --> 00:21:38,640
score

550
00:21:35,360 --> 00:21:40,000
so how are you to handle that so if you

551
00:21:38,640 --> 00:21:44,159
use some features

552
00:21:40,000 --> 00:21:45,280
of libraries that are released like in

553
00:21:44,159 --> 00:21:47,440
recent versions

554
00:21:45,280 --> 00:21:49,678
uh it should be stated in your

555
00:21:47,440 --> 00:21:52,240
requirements file or in your uh

556
00:21:49,679 --> 00:21:53,919
pip file uh because you really rely on

557
00:21:52,240 --> 00:21:59,840
that uh newer version

558
00:21:53,919 --> 00:21:59,840
uh that has these features

559
00:22:03,760 --> 00:22:07,840
uh can we expect this to come to other

560
00:22:06,159 --> 00:22:12,720
languages as well

561
00:22:07,840 --> 00:22:15,280
like golang or c anything so right now

562
00:22:12,720 --> 00:22:17,200
we are focused on python

563
00:22:15,280 --> 00:22:19,360
maybe follow us on twitter we will post

564
00:22:17,200 --> 00:22:27,840
updates

565
00:22:19,360 --> 00:22:27,840
say yes so yes

566
00:22:29,600 --> 00:22:36,240
if i have a very custom score function

567
00:22:32,960 --> 00:22:39,280
i can provide it and you recompute

568
00:22:36,240 --> 00:22:39,679
it for all the package i need and i mean

569
00:22:39,280 --> 00:22:43,600
it's

570
00:22:39,679 --> 00:22:47,120
it sounds very expensive

571
00:22:43,600 --> 00:22:50,240
in terms of recomputing of what

572
00:22:47,120 --> 00:22:53,199
if i have a custom score function

573
00:22:50,240 --> 00:22:54,960
can i provide my home score function or

574
00:22:53,200 --> 00:22:57,039
yeah

575
00:22:54,960 --> 00:22:59,280
you can so there are these pipeline

576
00:22:57,039 --> 00:23:02,480
units that you can implement

577
00:22:59,280 --> 00:23:05,200
and in that case you can

578
00:23:02,480 --> 00:23:06,400
basically create your your your pipeline

579
00:23:05,200 --> 00:23:09,600
you need and say

580
00:23:06,400 --> 00:23:10,400
uh this is score that i would like to

581
00:23:09,600 --> 00:23:12,719
expect

582
00:23:10,400 --> 00:23:15,280
and then the pipeline unis is plugged to

583
00:23:12,720 --> 00:23:15,280
resolver

584
00:23:19,440 --> 00:23:35,360
well thank you for doing okay thank you

585
00:23:21,760 --> 00:23:35,360
very much

