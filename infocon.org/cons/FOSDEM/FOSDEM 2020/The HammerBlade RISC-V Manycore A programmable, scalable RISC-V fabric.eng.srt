1
00:00:04,660 --> 00:00:09,040
I'm excited to present here I'm Michael

2
00:00:07,570 --> 00:00:11,350
Thao and professor at University of

3
00:00:09,040 --> 00:00:14,049
Washington also known as u dubs

4
00:00:11,350 --> 00:00:15,700
sometimes and this is max Ruttenberg

5
00:00:14,049 --> 00:00:17,820
who's one of my PhD students we'll be

6
00:00:15,700 --> 00:00:20,530
presenting the second half of the talk

7
00:00:17,820 --> 00:00:24,580
so the talk is entitled the hammer Glade

8
00:00:20,530 --> 00:00:26,849
risk by Maker and it's essentially a

9
00:00:24,580 --> 00:00:29,019
programmable scalable respite fabric

10
00:00:26,849 --> 00:00:30,789
that you know in some senses is

11
00:00:29,019 --> 00:00:34,900
equivalent to a GPGPU but it's a

12
00:00:30,789 --> 00:00:38,800
completely open source so for motivation

13
00:00:34,900 --> 00:00:40,960
you know especially in this room you

14
00:00:38,800 --> 00:00:43,149
know we're experiencing this open source

15
00:00:40,960 --> 00:00:45,579
Renaissance right we got open source is

16
00:00:43,149 --> 00:00:48,370
A's open source CAD tools open source

17
00:00:45,579 --> 00:00:50,530
processors open source libraries in RTL

18
00:00:48,370 --> 00:00:51,910
and so I think you know we're going to

19
00:00:50,530 --> 00:00:53,680
see enormous transformation in the

20
00:00:51,910 --> 00:00:55,959
industry because of all this open source

21
00:00:53,680 --> 00:00:58,180
stuff in hardware that we're doing and

22
00:00:55,960 --> 00:01:00,760
then at the same time we have all of

23
00:00:58,180 --> 00:01:02,769
these new application domains that are

24
00:01:00,760 --> 00:01:05,830
enabled essentially by Moore's Law

25
00:01:02,769 --> 00:01:08,890
winding down right so and the key to

26
00:01:05,830 --> 00:01:11,979
technologies really for enabling these

27
00:01:08,890 --> 00:01:14,560
domains are the development of new dsls

28
00:01:11,979 --> 00:01:16,479
domain-specific languages that help us

29
00:01:14,560 --> 00:01:20,110
make the specification of pelo

30
00:01:16,479 --> 00:01:21,759
compilation or make the process of

31
00:01:20,110 --> 00:01:23,009
compiling some application and getting

32
00:01:21,759 --> 00:01:25,390
it to run in parallel in some way

33
00:01:23,009 --> 00:01:29,590
feasible and then also the availability

34
00:01:25,390 --> 00:01:30,670
of new parallel computers and those are

35
00:01:29,590 --> 00:01:32,470
the things they're going to get us the

36
00:01:30,670 --> 00:01:33,579
energy efficiency that we need in order

37
00:01:32,470 --> 00:01:35,560
to get that you know the high

38
00:01:33,579 --> 00:01:38,679
performance within a given thermal

39
00:01:35,560 --> 00:01:40,689
envelope so the hammer blade many-core

40
00:01:38,680 --> 00:01:43,240
is kind of seeking to be the base class

41
00:01:40,689 --> 00:01:44,559
of these parallel computer bricks that

42
00:01:43,240 --> 00:01:47,320
are needed for these new application

43
00:01:44,560 --> 00:01:50,070
domains so what is a hammer glade mini

44
00:01:47,320 --> 00:01:53,020
course so it's a highly programmable

45
00:01:50,070 --> 00:01:55,750
highly energy-efficient spatial fabric

46
00:01:53,020 --> 00:01:57,699
for sort of mixed sparse dense compute

47
00:01:55,750 --> 00:02:00,759
so the idea is this is trying to address

48
00:01:57,700 --> 00:02:03,219
not just codes that would run reasonably

49
00:02:00,759 --> 00:02:06,280
well in a traditional GPU but also new

50
00:02:03,219 --> 00:02:07,929
codes like graph codes that are much

51
00:02:06,280 --> 00:02:10,810
less well behaved in a much more

52
00:02:07,930 --> 00:02:13,120
challenging to get some parallel

53
00:02:10,810 --> 00:02:14,980
speed-up and at the very heart of the

54
00:02:13,120 --> 00:02:17,140
many cores we have a super high

55
00:02:14,980 --> 00:02:18,690
efficiency compute tile it's a one

56
00:02:17,140 --> 00:02:22,720
instruction per cycle overs five

57
00:02:18,690 --> 00:02:24,460
and then each tile has an instruction

58
00:02:22,720 --> 00:02:26,710
cache and a local data scratch pad and

59
00:02:24,460 --> 00:02:27,820
you can adjust the size of this but

60
00:02:26,710 --> 00:02:29,140
generally you want it to be small

61
00:02:27,820 --> 00:02:30,549
because the smaller it is the more

62
00:02:29,140 --> 00:02:31,690
course you can fit on the chip right so

63
00:02:30,550 --> 00:02:33,610
there's a trade-off there and we'll show

64
00:02:31,690 --> 00:02:35,820
you how the architecture actually allows

65
00:02:33,610 --> 00:02:38,440
you to flexibly change that trade-off

66
00:02:35,820 --> 00:02:40,329
each core also has an FPU and then a

67
00:02:38,440 --> 00:02:42,700
little router to talk to the other

68
00:02:40,330 --> 00:02:45,700
course and then it's very scalable you

69
00:02:42,700 --> 00:02:47,679
just stamp out as many course as you

70
00:02:45,700 --> 00:02:49,089
want if you have you know a silicon area

71
00:02:47,680 --> 00:02:54,330
you just stamp them out until you fill

72
00:02:49,090 --> 00:02:58,030
up the area so so we have very good

73
00:02:54,330 --> 00:03:01,630
efficiency for the mini course so this

74
00:02:58,030 --> 00:03:03,670
is a picture of a floorplan and tsmc 16

75
00:03:01,630 --> 00:03:06,070
nanometer and then this is actually a

76
00:03:03,670 --> 00:03:09,220
die photo from the actual chip so this

77
00:03:06,070 --> 00:03:11,400
was a chip we presented at the risk 5

78
00:03:09,220 --> 00:03:15,790
summit the fourth one I think it was in

79
00:03:11,400 --> 00:03:19,080
2017 and what's amazing about this core

80
00:03:15,790 --> 00:03:22,570
is that even with super small

81
00:03:19,080 --> 00:03:24,790
instruction memory and data memory the

82
00:03:22,570 --> 00:03:26,620
memories themselves occupy 64 percent of

83
00:03:24,790 --> 00:03:28,900
the tile area so it's a kind of existing

84
00:03:26,620 --> 00:03:30,430
proof that yeah you pick try to go in

85
00:03:28,900 --> 00:03:31,960
like refactor this core and try to

86
00:03:30,430 --> 00:03:33,640
squeeze it down a little more but the

87
00:03:31,960 --> 00:03:37,000
maximum improvement you could possibly

88
00:03:33,640 --> 00:03:40,029
get would be about 30 36 percent so it's

89
00:03:37,000 --> 00:03:42,970
a so small in fact that you can fit 40

90
00:03:40,030 --> 00:03:45,190
of these per millimeter and 16 nanometer

91
00:03:42,970 --> 00:03:49,990
and at 7 nanometer you be able to fit

92
00:03:45,190 --> 00:03:51,160
120 of these per millimeter so zooming

93
00:03:49,990 --> 00:03:52,570
out you know we have this array of

94
00:03:51,160 --> 00:03:54,850
course but we need so we need to also

95
00:03:52,570 --> 00:03:59,560
integrate them into a pillow memory

96
00:03:54,850 --> 00:04:01,960
system and so the architectural model is

97
00:03:59,560 --> 00:04:04,030
you have the C of cores and then at the

98
00:04:01,960 --> 00:04:06,220
edge you have these l2 Victim caches

99
00:04:04,030 --> 00:04:07,450
that are then connected to many parallel

100
00:04:06,220 --> 00:04:08,020
memory channels so it could be for

101
00:04:07,450 --> 00:04:12,429
example

102
00:04:08,020 --> 00:04:14,170
HBM to which you know in in GPU systems

103
00:04:12,430 --> 00:04:17,250
you can have 64 parallel memory channels

104
00:04:14,170 --> 00:04:20,229
these days and these caches are also

105
00:04:17,250 --> 00:04:23,860
adaptive and can essentially change

106
00:04:20,230 --> 00:04:25,540
their favor on-the-fly as the as you

107
00:04:23,860 --> 00:04:27,940
learn more about the workload in the

108
00:04:25,540 --> 00:04:30,760
input data set

109
00:04:27,940 --> 00:04:33,280
now it's pretty easy to create a see of

110
00:04:30,760 --> 00:04:35,380
course so the real special sauce is how

111
00:04:33,280 --> 00:04:37,479
you weave these things together so the

112
00:04:35,380 --> 00:04:39,370
interesting thing about how this is done

113
00:04:37,480 --> 00:04:41,920
in our system is that every single

114
00:04:39,370 --> 00:04:43,930
memory location in every single local

115
00:04:41,920 --> 00:04:45,490
course scratchpad every single memory

116
00:04:43,930 --> 00:04:47,140
location in the l2 is every single

117
00:04:45,490 --> 00:04:48,940
memory location in the drn's it's all

118
00:04:47,140 --> 00:04:50,320
addressable by all the course so that

119
00:04:48,940 --> 00:04:51,730
means they can very easily collaborate

120
00:04:50,320 --> 00:04:53,560
just by doing loads and store

121
00:04:51,730 --> 00:04:56,860
instructions which will automatically

122
00:04:53,560 --> 00:05:00,580
get routed over to the whichever cord is

123
00:04:56,860 --> 00:05:02,890
or cache it is that owns that particular

124
00:05:00,580 --> 00:05:05,950
location and so a core can actually

125
00:05:02,890 --> 00:05:07,810
issue you know many repeated loads and

126
00:05:05,950 --> 00:05:09,039
stores those things will go out in the

127
00:05:07,810 --> 00:05:11,650
network in parallel and then come back

128
00:05:09,040 --> 00:05:14,470
potentially out of order and the core

129
00:05:11,650 --> 00:05:17,020
can continue executing instructions it's

130
00:05:14,470 --> 00:05:20,620
only when it actually tries to use the

131
00:05:17,020 --> 00:05:22,030
register that was the result of the load

132
00:05:20,620 --> 00:05:25,690
instruction that it might actually stall

133
00:05:22,030 --> 00:05:29,140
waiting for the data so one of the

134
00:05:25,690 --> 00:05:31,960
concepts that we have in trying to map

135
00:05:29,140 --> 00:05:33,520
software to the many cores then we have

136
00:05:31,960 --> 00:05:37,000
this big array of tiles and now we want

137
00:05:33,520 --> 00:05:38,620
to sort of group them together for two

138
00:05:37,000 --> 00:05:40,150
reasons so one is we may want to execute

139
00:05:38,620 --> 00:05:41,980
you know many different programs at the

140
00:05:40,150 --> 00:05:43,900
same time on their many core so we have

141
00:05:41,980 --> 00:05:46,000
this concept of tile group and here we

142
00:05:43,900 --> 00:05:48,400
show a tile group which is a 4 by 4 tile

143
00:05:46,000 --> 00:05:52,660
group where I allocated you know these

144
00:05:48,400 --> 00:05:56,169
16 tiles on the mini core and the many

145
00:05:52,660 --> 00:05:58,090
core essentially can the computation

146
00:05:56,169 --> 00:05:59,919
that's running on the mini core inside

147
00:05:58,090 --> 00:06:02,380
that's how a group can share all the

148
00:05:59,919 --> 00:06:05,250
collective memories of those tiles so

149
00:06:02,380 --> 00:06:07,690
this is a way for you to manage not only

150
00:06:05,250 --> 00:06:09,310
how much parallelism you have by having

151
00:06:07,690 --> 00:06:11,590
more fewer course but also how much

152
00:06:09,310 --> 00:06:13,780
working memory that you have so if you

153
00:06:11,590 --> 00:06:15,310
have a big working set then you can use

154
00:06:13,780 --> 00:06:17,409
a larger group of cores in order to

155
00:06:15,310 --> 00:06:19,540
increase the amount of local memory that

156
00:06:17,410 --> 00:06:21,010
you have and of course because those

157
00:06:19,540 --> 00:06:23,470
cores are located near each other the

158
00:06:21,010 --> 00:06:25,840
latency is very low in order to access

159
00:06:23,470 --> 00:06:31,510
the data whereas you know if you go off

160
00:06:25,840 --> 00:06:33,219
chip the latency would be higher so in

161
00:06:31,510 --> 00:06:35,500
order to program this we have of course

162
00:06:33,220 --> 00:06:37,180
we have dsls which are more user facing

163
00:06:35,500 --> 00:06:39,100
but in terms of writing you know very

164
00:06:37,180 --> 00:06:39,850
high performance library code for those

165
00:06:39,100 --> 00:06:41,650
dsls

166
00:06:39,850 --> 00:06:44,500
we have something we call cooter light

167
00:06:41,650 --> 00:06:45,698
which is very analogous to CUDA and the

168
00:06:44,500 --> 00:06:47,650
idea is that you know there's a big

169
00:06:45,699 --> 00:06:50,650
developer base for CUDA that's familiar

170
00:06:47,650 --> 00:06:52,750
with us you know programming constructs

171
00:06:50,650 --> 00:06:54,849
and we would like to essentially take

172
00:06:52,750 --> 00:06:56,110
the the knowledge that people have

173
00:06:54,850 --> 00:06:58,630
embedded in CUDA code and then

174
00:06:56,110 --> 00:07:02,380
relatively easily move that knowledge

175
00:06:58,630 --> 00:07:05,440
over to executing on the many core and

176
00:07:02,380 --> 00:07:07,449
so this is an example comparison of a

177
00:07:05,440 --> 00:07:08,979
snippet of device that wrote on CUDA and

178
00:07:07,449 --> 00:07:10,660
the equivalent code included light and

179
00:07:08,979 --> 00:07:14,620
it's you know there's there's it's

180
00:07:10,660 --> 00:07:16,060
fairly one two what in terms of being

181
00:07:14,620 --> 00:07:18,010
able to move code over there there's

182
00:07:16,060 --> 00:07:19,360
certainly some differences in terms of

183
00:07:18,010 --> 00:07:20,919
optimization and what would be

184
00:07:19,360 --> 00:07:22,780
preferable in one architecture the other

185
00:07:20,919 --> 00:07:26,909
but in terms of getting something up and

186
00:07:22,780 --> 00:07:29,198
running it's fairly straightforward so

187
00:07:26,910 --> 00:07:31,780
of course you have the many core and

188
00:07:29,199 --> 00:07:34,090
these cores are highly specialized for a

189
00:07:31,780 --> 00:07:36,250
dense compute but we would like to put a

190
00:07:34,090 --> 00:07:38,500
control processor of some form so

191
00:07:36,250 --> 00:07:41,440
there's two flavors of this in our

192
00:07:38,500 --> 00:07:45,099
system so the first is we support doing

193
00:07:41,440 --> 00:07:46,719
PCIe attached acceleration so in this

194
00:07:45,099 --> 00:07:49,449
case you know we have a chip it's a mini

195
00:07:46,720 --> 00:07:51,070
courtship it's on a PCIe board and then

196
00:07:49,449 --> 00:07:54,729
we put we connect it to a Xeon server

197
00:07:51,070 --> 00:07:57,460
and we have this up and running on f1 so

198
00:07:54,729 --> 00:07:59,590
in f1 s case there's an FPGA board which

199
00:07:57,460 --> 00:08:02,138
is simulating our mini core and then we

200
00:07:59,590 --> 00:08:04,359
have our you know host code running on

201
00:08:02,139 --> 00:08:06,490
the Xeon so you have x86 code talking to

202
00:08:04,360 --> 00:08:08,199
the menu court together in the longer

203
00:08:06,490 --> 00:08:10,750
term our goal is to actually have a

204
00:08:08,199 --> 00:08:13,180
black parrot integrated on the same SOC

205
00:08:10,750 --> 00:08:15,400
as the many core so for example you

206
00:08:13,180 --> 00:08:17,349
could imagine right now we would run PI

207
00:08:15,400 --> 00:08:20,320
torch on the Xeon and it offloads calls

208
00:08:17,349 --> 00:08:23,110
to the PCIe board but eventually you

209
00:08:20,320 --> 00:08:25,710
know when PI torch runs on verse 5 we

210
00:08:23,110 --> 00:08:28,900
would be running that on a black parent

211
00:08:25,710 --> 00:08:30,789
so I'd like to mention that we've

212
00:08:28,900 --> 00:08:32,978
actually been through many iterations so

213
00:08:30,789 --> 00:08:35,919
we have a very agile methodology where

214
00:08:32,979 --> 00:08:38,080
we do tape outs we build software on the

215
00:08:35,919 --> 00:08:39,669
devices we get experience than we

216
00:08:38,080 --> 00:08:42,700
develop the next generation so we're

217
00:08:39,669 --> 00:08:44,740
actually entering our fifth generation

218
00:08:42,700 --> 00:08:48,310
an iteration of this so the fifth

219
00:08:44,740 --> 00:08:50,410
silicon iteration of the mini core and

220
00:08:48,310 --> 00:08:52,869
so we started out in 180 nanometer and

221
00:08:50,410 --> 00:08:55,510
then we did two chips in 16 nanometer

222
00:08:52,870 --> 00:08:57,250
that had a 511

223
00:08:55,510 --> 00:08:59,200
miss five-course so it broke the world

224
00:08:57,250 --> 00:09:02,020
record for risk 5 performance actually

225
00:08:59,200 --> 00:09:04,990
probably still holds it and also for

226
00:09:02,020 --> 00:09:08,380
core mark for any is a and the latest

227
00:09:04,990 --> 00:09:10,180
system a hammer one is where we've

228
00:09:08,380 --> 00:09:12,580
really been focusing on programmability

229
00:09:10,180 --> 00:09:16,359
improvements to grow the user base and

230
00:09:12,580 --> 00:09:21,000
also a floating-point as support so I'd

231
00:09:16,360 --> 00:09:25,350
like to mention that in addition to the

232
00:09:21,000 --> 00:09:28,210
the many core itself my group has

233
00:09:25,350 --> 00:09:30,070
website B jump at org we have a bunch of

234
00:09:28,210 --> 00:09:32,050
stuff that might be of interest to you

235
00:09:30,070 --> 00:09:33,790
if you're developing open source

236
00:09:32,050 --> 00:09:36,880
hardware so one of them is something we

237
00:09:33,790 --> 00:09:39,490
call bechamel STL dan I mentioned this

238
00:09:36,880 --> 00:09:41,020
in the last talk so this is a library

239
00:09:39,490 --> 00:09:42,790
very high quality implementations of

240
00:09:41,020 --> 00:09:44,470
almost every hardware primitive that you

241
00:09:42,790 --> 00:09:46,660
can think of you know what routers you

242
00:09:44,470 --> 00:09:49,210
want arbiters you want caches you want

243
00:09:46,660 --> 00:09:51,010
networks you want high-speed links that

244
00:09:49,210 --> 00:09:52,540
go off your chip all of these things you

245
00:09:51,010 --> 00:09:53,830
know we've developed over you know the

246
00:09:52,540 --> 00:09:56,560
last ten years in our group and we've

247
00:09:53,830 --> 00:09:58,300
really iterated on their quality and

248
00:09:56,560 --> 00:10:01,270
also results and it's all been taped out

249
00:09:58,300 --> 00:10:04,359
validated we also have open source

250
00:10:01,270 --> 00:10:06,610
motherboards so you can design your ASIC

251
00:10:04,360 --> 00:10:08,320
to a particular interface standard that

252
00:10:06,610 --> 00:10:10,420
we specified and then you can just take

253
00:10:08,320 --> 00:10:12,190
your race they can plug it into a socket

254
00:10:10,420 --> 00:10:13,360
and we'll just work with this board so

255
00:10:12,190 --> 00:10:15,550
you don't have to go and design a board

256
00:10:13,360 --> 00:10:19,090
in order to I use the silicon that you

257
00:10:15,550 --> 00:10:21,219
developed we also have open source BGA

258
00:10:19,090 --> 00:10:23,560
packages these are much higher kin out

259
00:10:21,220 --> 00:10:25,990
and you can get off the shelf so if you

260
00:10:23,560 --> 00:10:27,280
design your chip to this interface

261
00:10:25,990 --> 00:10:29,590
standard that when you specify this

262
00:10:27,280 --> 00:10:31,900
Asics akut standard then you can get

263
00:10:29,590 --> 00:10:33,880
much more higher performance bandwidth

264
00:10:31,900 --> 00:10:38,530
out of your piece of silicon than you

265
00:10:33,880 --> 00:10:40,450
would on your own so I'm going to wrap

266
00:10:38,530 --> 00:10:42,550
up the hardware part I wanted to also

267
00:10:40,450 --> 00:10:45,160
mention that you know we have developed

268
00:10:42,550 --> 00:10:47,490
a methodology for taking cores out and

269
00:10:45,160 --> 00:10:50,410
replacing them with accelerators in this

270
00:10:47,490 --> 00:10:52,330
fabric and in fact our collaborators at

271
00:10:50,410 --> 00:10:54,579
Cornell have been developing several

272
00:10:52,330 --> 00:10:57,700
accelerators and have you know done

273
00:10:54,580 --> 00:10:59,620
proof of concept already with having you

274
00:10:57,700 --> 00:11:02,440
know hybrid many core accelerator

275
00:10:59,620 --> 00:11:04,990
systems so now I'm going to turn it over

276
00:11:02,440 --> 00:11:08,760
to max he's going to talk about some of

277
00:11:04,990 --> 00:11:08,760
our hammer hammer blade software stacks

278
00:11:28,709 --> 00:11:34,209
thank you

279
00:11:30,600 --> 00:11:35,829
so yeah so I'm gonna hi I'm max I'm

280
00:11:34,209 --> 00:11:39,729
gonna give an overview of our software

281
00:11:35,829 --> 00:11:41,640
stack so one of the primary goals of

282
00:11:39,730 --> 00:11:44,260
hammer blade is programmability and

283
00:11:41,640 --> 00:11:45,930
portability of code that already exists

284
00:11:44,260 --> 00:11:48,610
that's written higher-level frameworks

285
00:11:45,930 --> 00:11:51,370
so CUDA light which is our low-level

286
00:11:48,610 --> 00:11:54,550
programming API is the building block

287
00:11:51,370 --> 00:11:56,230
for these higher-level frameworks our

288
00:11:54,550 --> 00:11:58,750
collaborators at Cornell are working on

289
00:11:56,230 --> 00:12:01,959
pi torch our back-end for pi torch for

290
00:11:58,750 --> 00:12:05,410
hammer Glade we at u-dub are also

291
00:12:01,959 --> 00:12:10,180
working on D GL which is a Python

292
00:12:05,410 --> 00:12:11,680
library for targeting a crap structured

293
00:12:10,180 --> 00:12:15,459
data but using machine learning

294
00:12:11,680 --> 00:12:18,040
techniques and T VM which is a machine

295
00:12:15,459 --> 00:12:20,279
learning ir and we're also targeting

296
00:12:18,040 --> 00:12:27,730
graphic which is a DSL for

297
00:12:20,279 --> 00:12:30,630
high-performance graph analytics so just

298
00:12:27,730 --> 00:12:33,640
a quick introduction to graph it its

299
00:12:30,630 --> 00:12:36,370
main feature is that it decouples the

300
00:12:33,640 --> 00:12:38,920
the semantics of the program from the

301
00:12:36,370 --> 00:12:42,010
optimizations that are applied this is

302
00:12:38,920 --> 00:12:44,589
important because with graph algorithms

303
00:12:42,010 --> 00:12:46,329
there's a whole the field of

304
00:12:44,589 --> 00:12:47,860
optimizations is pretty wide and they

305
00:12:46,329 --> 00:12:49,959
don't always work on every single input

306
00:12:47,860 --> 00:12:53,680
sometimes the optimizations that work

307
00:12:49,959 --> 00:12:57,130
are highly reliant on the input the two

308
00:12:53,680 --> 00:12:59,109
main types are edge sets vertex sets and

309
00:12:57,130 --> 00:13:02,079
then there's a scheduling which which

310
00:12:59,110 --> 00:13:05,160
you can see down here which says what

311
00:13:02,079 --> 00:13:05,160
optimizations to apply

312
00:13:07,750 --> 00:13:13,420
so we ported this to hammer blade and

313
00:13:11,500 --> 00:13:15,069
here's sort of an example so the same

314
00:13:13,420 --> 00:13:19,870
snippet of code I actually took it from

315
00:13:15,069 --> 00:13:24,250
BFS basically the frontier is just being

316
00:13:19,870 --> 00:13:27,069
updated by traversing the existing

317
00:13:24,250 --> 00:13:29,290
frontier applying an update function and

318
00:13:27,069 --> 00:13:32,319
then creating a new frontier from the

319
00:13:29,290 --> 00:13:34,750
notes that were updated and seeing the

320
00:13:32,319 --> 00:13:37,029
scheduling language that we've said that

321
00:13:34,750 --> 00:13:39,699
we've add that we've said that we want

322
00:13:37,029 --> 00:13:43,480
the dents pull direction and that we

323
00:13:39,699 --> 00:13:45,040
want to generate kemberly code down here

324
00:13:43,480 --> 00:13:47,230
you can see the host side that is

325
00:13:45,040 --> 00:13:52,089
generated so this runs on the x86 or

326
00:13:47,230 --> 00:13:54,490
black Barrett you can see that it's just

327
00:13:52,089 --> 00:14:01,149
the outer loop but we're offloading the

328
00:13:54,490 --> 00:14:04,029
work here's the generated risk 5 code

329
00:14:01,149 --> 00:14:06,939
it's C++ code the work is self assigned

330
00:14:04,029 --> 00:14:10,779
we have a local range function and then

331
00:14:06,939 --> 00:14:12,459
we are doing a parallel dance update and

332
00:14:10,779 --> 00:14:20,170
then finally when all the work is done

333
00:14:12,459 --> 00:14:22,420
we do a tile group sync so graphs are

334
00:14:20,170 --> 00:14:25,209
kind of challenging because they are

335
00:14:22,420 --> 00:14:27,579
typically memory intensive so taking

336
00:14:25,209 --> 00:14:30,069
full advantage of all your cores can be

337
00:14:27,579 --> 00:14:32,609
tricky you might add spending they might

338
00:14:30,069 --> 00:14:36,579
spend a lot of time waiting for memory

339
00:14:32,610 --> 00:14:38,319
so one way you might be able to fix this

340
00:14:36,579 --> 00:14:39,790
address this problem on hammer blade is

341
00:14:38,319 --> 00:14:42,250
doing something similar to what we might

342
00:14:39,790 --> 00:14:46,120
do on GPUs week and Tylar memory

343
00:14:42,250 --> 00:14:48,339
accesses so in this case where the

344
00:14:46,120 --> 00:14:52,600
proposed plan would be to take vertex

345
00:14:48,339 --> 00:14:57,519
data and block and keep an axis in a

346
00:14:52,600 --> 00:15:00,399
block faction yeah fashion so we would

347
00:14:57,519 --> 00:15:02,050
pull it in on one block for a group of

348
00:15:00,399 --> 00:15:05,680
tiles and they would pull it into their

349
00:15:02,050 --> 00:15:09,040
local memory and then they could just do

350
00:15:05,680 --> 00:15:11,258
sparse updates from the from the edges

351
00:15:09,040 --> 00:15:13,959
so they can access the edges you would

352
00:15:11,259 --> 00:15:16,059
keep them you would keep the edges

353
00:15:13,959 --> 00:15:17,709
partitioned across DRAM channels this

354
00:15:16,059 --> 00:15:19,920
would maximize your message transfer

355
00:15:17,709 --> 00:15:19,920
rate

356
00:15:24,420 --> 00:15:29,439
and then you could restrict your updates

357
00:15:27,130 --> 00:15:32,140
to a given range so this would keep your

358
00:15:29,440 --> 00:15:35,260
lookout this would keep you from

359
00:15:32,140 --> 00:15:36,640
breaking your cache and it will reduce

360
00:15:35,260 --> 00:15:39,340
the amount of time you spend waiting

361
00:15:36,640 --> 00:15:44,590
around for memory it will maximize the

362
00:15:39,340 --> 00:15:46,810
parallelism of your course so now I've

363
00:15:44,590 --> 00:15:50,920
given you a overview of mapping an

364
00:15:46,810 --> 00:15:52,479
application Tamarack blade I want to I'm

365
00:15:50,920 --> 00:15:57,910
bored of you all with how you can get

366
00:15:52,480 --> 00:16:01,540
involved so our main simulating

367
00:15:57,910 --> 00:16:05,530
environment is C C++ co-simulation

368
00:16:01,540 --> 00:16:08,469
environment allows you to simulate your

369
00:16:05,530 --> 00:16:12,040
entire stack the host software mini core

370
00:16:08,470 --> 00:16:16,180
software we use synopsis as our RTL

371
00:16:12,040 --> 00:16:18,280
simulator I should note that most of

372
00:16:16,180 --> 00:16:20,849
code in our the RTL on our group is

373
00:16:18,280 --> 00:16:23,350
there later friendly so it would be a

374
00:16:20,850 --> 00:16:27,760
straightforward and solid contribution

375
00:16:23,350 --> 00:16:30,100
to get there later working the codes all

376
00:16:27,760 --> 00:16:33,189
up in github you cloned this repository

377
00:16:30,100 --> 00:16:35,530
called Blade Runner you get the related

378
00:16:33,190 --> 00:16:37,120
sub projects and then there are

379
00:16:35,530 --> 00:16:39,910
instructions and the readme about to get

380
00:16:37,120 --> 00:16:41,110
up and running if you have the tools in

381
00:16:39,910 --> 00:16:46,209
the environment it's fairly

382
00:16:41,110 --> 00:16:50,200
straightforward we also support

383
00:16:46,210 --> 00:16:51,940
deployment on AWS you need them the bada

384
00:16:50,200 --> 00:16:56,140
tools you need to design links Vlado

385
00:16:51,940 --> 00:17:01,030
compiler but it's all part of the same

386
00:16:56,140 --> 00:17:03,189
project and we have instruct detailed

387
00:17:01,030 --> 00:17:05,139
instructions in the room yeah about how

388
00:17:03,190 --> 00:17:06,550
you can build the fpga image hiking

389
00:17:05,140 --> 00:17:08,050
building machine image and you can get

390
00:17:06,550 --> 00:17:10,359
up and running and machine image will

391
00:17:08,050 --> 00:17:12,280
have everything they'll have runtime

392
00:17:10,359 --> 00:17:16,889
libraries development tools for risk 5

393
00:17:12,280 --> 00:17:16,889
the fpga bit image all of it

394
00:17:18,720 --> 00:17:25,390
and lastly I'd like to suggest some

395
00:17:23,230 --> 00:17:27,040
directions you could take to help us

396
00:17:25,390 --> 00:17:31,540
contribute to the Mittman so a hammer

397
00:17:27,040 --> 00:17:33,730
blade software stack all of our software

398
00:17:31,540 --> 00:17:35,379
frameworks that we've reported our works

399
00:17:33,730 --> 00:17:38,410
in progress everything down to CUDA

400
00:17:35,380 --> 00:17:40,120
light but there's some other really

401
00:17:38,410 --> 00:17:42,370
interesting directions we could take

402
00:17:40,120 --> 00:17:46,500
that we haven't explored yet so halide

403
00:17:42,370 --> 00:17:49,000
is for image processing we haven't

404
00:17:46,500 --> 00:17:51,990
looked at that yet but we're pretty

405
00:17:49,000 --> 00:17:55,510
confident that would be a good fit

406
00:17:51,990 --> 00:17:59,380
tensorflow right now we have PI torch

407
00:17:55,510 --> 00:18:03,820
but tensorflow would also be excellent F

408
00:17:59,380 --> 00:18:07,690
of T W is a library for doing

409
00:18:03,820 --> 00:18:10,000
FFTs on GPUs there's also ku F of T for

410
00:18:07,690 --> 00:18:13,740
CUDA we would really love one for hammer

411
00:18:10,000 --> 00:18:15,790
blame any core as well on top of that

412
00:18:13,740 --> 00:18:17,290
Cornell has been working on building

413
00:18:15,790 --> 00:18:19,870
their own accelerators and gnash and

414
00:18:17,290 --> 00:18:22,030
adding them to our mess and it would be

415
00:18:19,870 --> 00:18:25,229
great if you could add for people to add

416
00:18:22,030 --> 00:18:32,080
their own accelerators and to expand our

417
00:18:25,230 --> 00:18:34,900
application domains so we have a full

418
00:18:32,080 --> 00:18:36,850
stack team and they work very very hard

419
00:18:34,900 --> 00:18:41,500
on this project and I'd like to thank

420
00:18:36,850 --> 00:18:43,659
them before including and on behalf of

421
00:18:41,500 --> 00:18:45,700
the hammer blade team we sleep all we

422
00:18:43,660 --> 00:18:47,490
really hope you contribute and thank you

423
00:18:45,700 --> 00:18:55,109
very much

424
00:18:47,490 --> 00:18:55,109
[Applause]

425
00:18:59,710 --> 00:19:13,990
oh so the question was how how many of

426
00:19:09,670 --> 00:19:15,640
these can you fit on the FPGA so so we

427
00:19:13,990 --> 00:19:17,410
we haven't done too much with smaller

428
00:19:15,640 --> 00:19:20,770
DJ's but we can fit a hundred and ninety

429
00:19:17,410 --> 00:19:22,270
two cores on a big FPGA so a reasonable

430
00:19:20,770 --> 00:19:24,540
number of cores and fit on a small at

431
00:19:22,270 --> 00:19:24,540
PGA

