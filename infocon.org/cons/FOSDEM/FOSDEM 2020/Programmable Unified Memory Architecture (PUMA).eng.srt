1
00:00:04,560 --> 00:00:10,559
so let's continue I'm very happy to

2
00:00:08,100 --> 00:00:12,420
announce state who is working with Intel

3
00:00:10,559 --> 00:00:16,939
and he's talking about programmable

4
00:00:12,420 --> 00:00:20,970
Unified's in memory architecture Puma

5
00:00:16,940 --> 00:00:24,180
okay thank you I'm afraid there's going

6
00:00:20,970 --> 00:00:25,710
to be another Hardware talk but I'll

7
00:00:24,180 --> 00:00:28,020
keep it simple and hope that you

8
00:00:25,710 --> 00:00:30,390
appreciate the fact that the hardware is

9
00:00:28,020 --> 00:00:35,160
also evaluate evolving and that we're

10
00:00:30,390 --> 00:00:39,839
looking at more efficient processors for

11
00:00:35,160 --> 00:00:41,699
graph processing so as you all know

12
00:00:39,839 --> 00:00:43,320
graph processing is getting bigger and

13
00:00:41,699 --> 00:00:45,659
bigger and graphs are getting bigger and

14
00:00:43,320 --> 00:00:50,190
bigger and you want to process more data

15
00:00:45,659 --> 00:00:51,960
so we set ourselves a a so we found that

16
00:00:50,190 --> 00:00:54,748
that we need to increase the efficiency

17
00:00:51,960 --> 00:00:58,649
of graph processing versus existing

18
00:00:54,749 --> 00:01:02,339
architectures such as CPUs and GPUs so

19
00:00:58,649 --> 00:01:05,580
to that we propose the programmable

20
00:01:02,339 --> 00:01:10,369
unified memory architecture greevey ated

21
00:01:05,580 --> 00:01:13,020
as Puma and in this talk I want to

22
00:01:10,369 --> 00:01:14,700
explain to you why graph processing is

23
00:01:13,020 --> 00:01:17,158
actually challenging on the existing

24
00:01:14,700 --> 00:01:20,039
architectures what makes Puma fit for

25
00:01:17,159 --> 00:01:23,880
graph processing some high-level details

26
00:01:20,039 --> 00:01:25,759
about Puma and how it performs and after

27
00:01:23,880 --> 00:01:28,890
this presentation you'll probably get a

28
00:01:25,759 --> 00:01:30,539
question can we buy Puma unfortunately

29
00:01:28,890 --> 00:01:35,670
not yet because it's still on the

30
00:01:30,539 --> 00:01:37,049
development okay as you all know Intel

31
00:01:35,670 --> 00:01:42,149
is the market leader of high performance

32
00:01:37,049 --> 00:01:43,799
processors and these processors we've

33
00:01:42,149 --> 00:01:45,479
implemented a lot of things for regular

34
00:01:43,799 --> 00:01:48,509
applications that works very well such

35
00:01:45,479 --> 00:01:50,630
as branch prediction ranges and more in

36
00:01:48,509 --> 00:01:53,490
general regular applications are very

37
00:01:50,630 --> 00:01:55,408
predictable so we use that to predict

38
00:01:53,490 --> 00:01:58,829
branches ahead such that we can float

39
00:01:55,409 --> 00:02:00,719
instructions more in a continuous way we

40
00:01:58,829 --> 00:02:03,109
have caches that assumes that if you

41
00:02:00,719 --> 00:02:05,999
access some data you will access it

42
00:02:03,109 --> 00:02:08,579
again in the near future or its

43
00:02:05,999 --> 00:02:09,990
neighboring data we have vector

44
00:02:08,580 --> 00:02:12,330
operations that perform the same

45
00:02:09,990 --> 00:02:14,129
operation of neighboring data and all

46
00:02:12,330 --> 00:02:16,440
that works good for regular applications

47
00:02:14,129 --> 00:02:18,149
but graph applications as the previous

48
00:02:16,440 --> 00:02:24,629
speaker also explained are

49
00:02:18,150 --> 00:02:28,129
that nice for these architectures for

50
00:02:24,629 --> 00:02:30,599
example many of the graph applicator

51
00:02:28,129 --> 00:02:33,989
grab applicators have many branches that

52
00:02:30,599 --> 00:02:35,970
are data dependent so the actual outcome

53
00:02:33,989 --> 00:02:38,819
of the bench depends on the data that is

54
00:02:35,970 --> 00:02:39,599
in the graph and which is of course not

55
00:02:38,819 --> 00:02:44,000
predictable

56
00:02:39,599 --> 00:02:44,000
so branch predictors don't work loud

57
00:02:44,390 --> 00:02:49,858
data is also accessed in a scatter tray

58
00:02:47,129 --> 00:02:52,260
so that was also introduced by previous

59
00:02:49,859 --> 00:02:55,260
speakers so you access the neighbors and

60
00:02:52,260 --> 00:02:57,328
the neighbors are not the next notes in

61
00:02:55,260 --> 00:02:59,909
your list of notes they are scattered

62
00:02:57,329 --> 00:03:01,739
all over the place so caches don't wear

63
00:02:59,909 --> 00:03:03,929
out because you don't use the a

64
00:03:01,739 --> 00:03:06,540
neighboring data of them to reduce the

65
00:03:03,930 --> 00:03:09,209
same data and the same for vector

66
00:03:06,540 --> 00:03:13,769
operations so we have we see very low

67
00:03:09,209 --> 00:03:16,889
performance on regular CPUs for graph

68
00:03:13,769 --> 00:03:20,000
applications many people have proposed

69
00:03:16,889 --> 00:03:23,760
to use GPUs they are actually better and

70
00:03:20,000 --> 00:03:26,129
general because of higher bandwidth and

71
00:03:23,760 --> 00:03:29,399
more threads but they actually suffer

72
00:03:26,129 --> 00:03:31,918
from the same problems if branches

73
00:03:29,400 --> 00:03:34,500
diverge so go to another direction then

74
00:03:31,919 --> 00:03:37,500
you cannot the parallelism cannot be

75
00:03:34,500 --> 00:03:40,169
fully exploited scattered memory

76
00:03:37,500 --> 00:03:41,129
accesses prevents to use manual

77
00:03:40,169 --> 00:03:43,260
coalescing

78
00:03:41,129 --> 00:03:45,720
meaning that all of the official things

79
00:03:43,260 --> 00:03:48,418
you cannot use in GPUs too and we also

80
00:03:45,720 --> 00:03:50,220
saw before in previous presentations

81
00:03:48,419 --> 00:03:54,720
there is a problem of memory capacity

82
00:03:50,220 --> 00:03:57,209
and scaling out so what are potential

83
00:03:54,720 --> 00:03:59,099
solutions there have been proposals of

84
00:03:57,209 --> 00:04:02,459
growth accelerated so that specific

85
00:03:59,099 --> 00:04:04,048
chips made for graph applications that

86
00:04:02,459 --> 00:04:06,659
have a some functionality for example

87
00:04:04,049 --> 00:04:09,239
that implement sparse linear linear

88
00:04:06,659 --> 00:04:12,599
algebra algorithms or Verte eccentrics

89
00:04:09,239 --> 00:04:14,940
algorithms the problem there is that you

90
00:04:12,599 --> 00:04:17,339
fixed with that functionality so if your

91
00:04:14,940 --> 00:04:20,789
application your algorithm works best

92
00:04:17,339 --> 00:04:23,840
for within the sparse linear algebra but

93
00:04:20,789 --> 00:04:26,580
your accelerator is a vertex centric

94
00:04:23,840 --> 00:04:29,029
operation accelerator then you need to

95
00:04:26,580 --> 00:04:31,979
transfer your application and

96
00:04:29,029 --> 00:04:33,870
potentially and you also need a host

97
00:04:31,980 --> 00:04:36,030
so a CPU that that controls this

98
00:04:33,870 --> 00:04:39,480
accelerator and there is the data

99
00:04:36,030 --> 00:04:41,400
transfer between both another solution

100
00:04:39,480 --> 00:04:43,830
is to have a general instruction set

101
00:04:41,400 --> 00:04:45,299
processor like the normal CPUs but

102
00:04:43,830 --> 00:04:48,419
that's been optimized for graph

103
00:04:45,300 --> 00:04:49,830
allocations which is then more flexible

104
00:04:48,420 --> 00:04:52,140
because you have an instruction set you

105
00:04:49,830 --> 00:04:53,969
can implement other algorithms - it's

106
00:04:52,140 --> 00:04:56,610
self-contained you don't need a host /

107
00:04:53,970 --> 00:05:01,080
sake and that's actually what our

108
00:04:56,610 --> 00:05:05,340
approach was in Pune okay I've talked

109
00:05:01,080 --> 00:05:08,849
about the challenges that graph

110
00:05:05,340 --> 00:05:11,849
applications pose so how did we solve

111
00:05:08,850 --> 00:05:14,550
that in Puma most of the graph

112
00:05:11,850 --> 00:05:17,100
applications are very memory bound so

113
00:05:14,550 --> 00:05:19,530
most of the dam if you have a vengeful

114
00:05:17,100 --> 00:05:21,780
score as the uncle for example it's just

115
00:05:19,530 --> 00:05:24,630
stuck by the slow memory it's mating

116
00:05:21,780 --> 00:05:29,729
forever so you can't cannot use all of

117
00:05:24,630 --> 00:05:32,370
its speed so instead in Puma we have

118
00:05:29,730 --> 00:05:34,680
much lighter course much slower cost but

119
00:05:32,370 --> 00:05:36,360
we have many of them so they still wait

120
00:05:34,680 --> 00:05:39,950
for memory but because we have many of

121
00:05:36,360 --> 00:05:42,680
them the total throughput is higher I

122
00:05:39,950 --> 00:05:45,719
said that before caching was a problem

123
00:05:42,680 --> 00:05:47,640
so the problem is that if you know data

124
00:05:45,720 --> 00:05:50,340
you only need that one element that you

125
00:05:47,640 --> 00:05:51,930
notice that you load not the full cache

126
00:05:50,340 --> 00:05:54,710
length so in a conventional architecture

127
00:05:51,930 --> 00:05:57,660
you know the full cache rank from memory

128
00:05:54,710 --> 00:05:59,880
through the memory bus into the cache

129
00:05:57,660 --> 00:06:01,530
and then to the core but as you see it

130
00:05:59,880 --> 00:06:03,290
the blue only the blue points are used

131
00:06:01,530 --> 00:06:06,830
it's very inefficient use of the cache

132
00:06:03,290 --> 00:06:11,790
capacity and of the memory bandwidth so

133
00:06:06,830 --> 00:06:15,840
it means you waste a lot of things a lot

134
00:06:11,790 --> 00:06:17,730
of potential for Puma we have lot of

135
00:06:15,840 --> 00:06:20,190
course but we also optimize the memory

136
00:06:17,730 --> 00:06:23,070
accesses such that you can access only a

137
00:06:20,190 --> 00:06:26,430
single element we bypass caches because

138
00:06:23,070 --> 00:06:30,300
they are not efficient to get more chip

139
00:06:26,430 --> 00:06:34,680
area for course and other stuff and get

140
00:06:30,300 --> 00:06:38,220
more fluent flu flow of memory

141
00:06:34,680 --> 00:06:39,600
operations another thing is the size of

142
00:06:38,220 --> 00:06:41,640
the graph so if you have a very large

143
00:06:39,600 --> 00:06:43,969
graph you need to partition it and put

144
00:06:41,640 --> 00:06:46,159
it into multiple nodes

145
00:06:43,969 --> 00:06:49,308
multiple compute nodes multiple servers

146
00:06:46,159 --> 00:06:52,789
but then when the graph of algorithm

147
00:06:49,309 --> 00:06:55,339
wants to access data on another node it

148
00:06:52,789 --> 00:06:57,289
needs to go to the whole communication

149
00:06:55,339 --> 00:06:59,839
stack which takes a lot of time and

150
00:06:57,289 --> 00:07:02,748
unfortunately graph algorithms are not

151
00:06:59,839 --> 00:07:04,819
very predictable in their locality so it

152
00:07:02,749 --> 00:07:07,999
often happens of course that we need to

153
00:07:04,819 --> 00:07:11,110
go off note giving a large performance

154
00:07:07,999 --> 00:07:13,699
penalty so for Puma we have this

155
00:07:11,110 --> 00:07:15,019
hardware disability check memory so

156
00:07:13,699 --> 00:07:17,929
there is a shared memory across the

157
00:07:15,019 --> 00:07:20,209
whole system you don't need to think

158
00:07:17,929 --> 00:07:21,589
about communication

159
00:07:20,209 --> 00:07:25,759
the network is high-bandwidth

160
00:07:21,589 --> 00:07:28,360
low-latency just to reduce the latency

161
00:07:25,759 --> 00:07:33,379
or the performance impact of remote

162
00:07:28,360 --> 00:07:35,659
accesses another thing that we saw that

163
00:07:33,379 --> 00:07:37,599
is that there are very a lot of common

164
00:07:35,659 --> 00:07:40,449
patterns in graph applications and we

165
00:07:37,599 --> 00:07:43,669
designed some offload engines that

166
00:07:40,449 --> 00:07:46,699
efficiently execute these patterns for

167
00:07:43,669 --> 00:07:49,639
example Atomics are very much very much

168
00:07:46,699 --> 00:07:52,519
used in graph algorithms but for a cord

169
00:07:49,639 --> 00:07:54,139
that's a very intensive operation so you

170
00:07:52,519 --> 00:07:56,269
have to look data you have to load the

171
00:07:54,139 --> 00:07:58,849
data update it and write it back and

172
00:07:56,269 --> 00:08:01,849
unlock the data so the core is often

173
00:07:58,849 --> 00:08:03,979
stuck a lot a long time performing this

174
00:08:01,849 --> 00:08:07,339
Atomics so and Puma we have this offload

175
00:08:03,979 --> 00:08:09,679
engine that performs the Atomics the

176
00:08:07,339 --> 00:08:12,009
release the core from performing these

177
00:08:09,679 --> 00:08:14,748
economics so just the poor the core

178
00:08:12,009 --> 00:08:17,179
issues an atomic instruction but leaves

179
00:08:14,749 --> 00:08:19,489
the execution over to the offload engine

180
00:08:17,179 --> 00:08:21,489
and the co can continue in executing

181
00:08:19,489 --> 00:08:23,508
other instructions in the background or

182
00:08:21,489 --> 00:08:25,609
so the operation is done in the

183
00:08:23,509 --> 00:08:27,919
background and the offload engine looks

184
00:08:25,610 --> 00:08:30,709
where the data is located and performs

185
00:08:27,919 --> 00:08:32,509
the update locally similarly a gather

186
00:08:30,709 --> 00:08:36,589
operations for example if you want to

187
00:08:32,509 --> 00:08:38,808
gather the some carrots tricks the

188
00:08:36,589 --> 00:08:42,680
characteristics of the labors of a

189
00:08:38,808 --> 00:08:45,670
vertex the normal operation is that you

190
00:08:42,679 --> 00:08:48,258
load index of the worst first neighbor

191
00:08:45,670 --> 00:08:50,000
then you load the data of the neighbor

192
00:08:48,259 --> 00:08:53,449
and store it somewhere so that's a very

193
00:08:50,000 --> 00:08:55,189
intensive process for the course so we

194
00:08:53,449 --> 00:08:57,829
have this DMA

195
00:08:55,189 --> 00:08:59,930
gather offload engine so the core again

196
00:08:57,830 --> 00:09:02,060
just excuse this DMA gather instruction

197
00:08:59,930 --> 00:09:03,650
and then continuous executing and in the

198
00:09:02,060 --> 00:09:06,199
background the offload and he performs

199
00:09:03,650 --> 00:09:08,360
the necessary memory accesses without

200
00:09:06,200 --> 00:09:12,410
actually needing to move the data all

201
00:09:08,360 --> 00:09:14,990
the data to the core itself other

202
00:09:12,410 --> 00:09:19,699
operations are man copy barriers queues

203
00:09:14,990 --> 00:09:22,250
and and so forth so going to into a

204
00:09:19,700 --> 00:09:25,430
little bit more architectural detail of

205
00:09:22,250 --> 00:09:28,160
a single kuma core so that's a schematic

206
00:09:25,430 --> 00:09:30,079
overview of a single puma core it

207
00:09:28,160 --> 00:09:32,510
consists of multiple pipelines so each

208
00:09:30,080 --> 00:09:35,480
of these are pipelines each pipeline

209
00:09:32,510 --> 00:09:39,590
supports multiple threads why like the

210
00:09:35,480 --> 00:09:41,120
previous presenters says it said most of

211
00:09:39,590 --> 00:09:43,070
the time we are waiting for memory

212
00:09:41,120 --> 00:09:44,720
operations so if we are meeting for

213
00:09:43,070 --> 00:09:46,730
memory operations we just switch to

214
00:09:44,720 --> 00:09:49,250
another threat if this one is also

215
00:09:46,730 --> 00:09:51,410
waiting switch to yet another type and

216
00:09:49,250 --> 00:09:54,530
so on just to hide all of these memory

217
00:09:51,410 --> 00:09:56,209
latencies we have limited caches you

218
00:09:54,530 --> 00:09:59,870
have next option Casa data cache which

219
00:09:56,210 --> 00:10:05,080
are very small it's for a very local

220
00:09:59,870 --> 00:10:05,080
data such as the stack of your threat

221
00:10:05,950 --> 00:10:11,870
the instructions executed by discourse

222
00:10:09,950 --> 00:10:14,150
are a novel instruction set that we

223
00:10:11,870 --> 00:10:16,210
designed it's based on reduced

224
00:10:14,150 --> 00:10:20,720
instruction set so simple instructions

225
00:10:16,210 --> 00:10:23,270
and we added specific instructions that

226
00:10:20,720 --> 00:10:25,910
are you can then use by graph operations

227
00:10:23,270 --> 00:10:32,270
such as a single instruction indirect

228
00:10:25,910 --> 00:10:34,550
load because we have limited caching we

229
00:10:32,270 --> 00:10:36,460
don't want to go to main memory all of

230
00:10:34,550 --> 00:10:38,780
the time for the data that's been been

231
00:10:36,460 --> 00:10:42,020
consumed and produced locally so we have

232
00:10:38,780 --> 00:10:43,880
a manually accessible scratch pad so the

233
00:10:42,020 --> 00:10:46,670
programmer can decide whether to cache

234
00:10:43,880 --> 00:10:49,490
its data to use a scratch pad or to go

235
00:10:46,670 --> 00:10:51,020
to main memory it's part of the global

236
00:10:49,490 --> 00:10:55,190
address space so other costs can also

237
00:10:51,020 --> 00:10:57,439
access the scratch pad of this core I

238
00:10:55,190 --> 00:10:58,910
talked about offload engines so they

239
00:10:57,440 --> 00:11:01,760
perform these operations in the

240
00:10:58,910 --> 00:11:03,949
background we have a memory controller

241
00:11:01,760 --> 00:11:06,890
that's been optimized for these small

242
00:11:03,950 --> 00:11:08,420
accesses he died accesses and we have a

243
00:11:06,890 --> 00:11:11,900
network interface to connect to other

244
00:11:08,420 --> 00:11:13,790
course also of course using this aid

245
00:11:11,900 --> 00:11:17,380
packets because most of the data is

246
00:11:13,790 --> 00:11:21,770
trans is used in the small granules

247
00:11:17,380 --> 00:11:25,670
granularities okay the full permit

248
00:11:21,770 --> 00:11:28,430
system as we envision it as its

249
00:11:25,670 --> 00:11:30,620
hierarchically built we have multiple of

250
00:11:28,430 --> 00:11:32,750
discourse it format I'll multiples of

251
00:11:30,620 --> 00:11:37,760
these starts form a note multiples of

252
00:11:32,750 --> 00:11:39,860
the these notes form a system and that

253
00:11:37,760 --> 00:11:41,780
means that a since you have already that

254
00:11:39,860 --> 00:11:44,330
many threats on a chord you have that

255
00:11:41,780 --> 00:11:47,870
many course on a tower and so on that

256
00:11:44,330 --> 00:11:51,070
the full system can easily consists of

257
00:11:47,870 --> 00:11:54,140
millions of Hardware context for threats

258
00:11:51,070 --> 00:11:56,050
again the global address space and the

259
00:11:54,140 --> 00:11:58,580
memory is shared across the full system

260
00:11:56,050 --> 00:12:01,040
so you can access any data from any

261
00:11:58,580 --> 00:12:02,990
point in the system

262
00:12:01,040 --> 00:12:05,240
of course this requires a very high

263
00:12:02,990 --> 00:12:07,790
bandwidth low latency network so we use

264
00:12:05,240 --> 00:12:10,040
hyper X network topology this is the

265
00:12:07,790 --> 00:12:14,030
textbook rest representation of this so

266
00:12:10,040 --> 00:12:16,490
you have a hierarchical design where

267
00:12:14,030 --> 00:12:18,650
within each level our everything is

268
00:12:16,490 --> 00:12:20,600
fully connected and then on the next

269
00:12:18,650 --> 00:12:25,400
level there are connections between the

270
00:12:20,600 --> 00:12:27,530
levels furthermore we plan to have

271
00:12:25,400 --> 00:12:30,020
optical connections between the thousand

272
00:12:27,530 --> 00:12:33,770
three notes again to increase bandwidth

273
00:12:30,020 --> 00:12:36,910
and latency more interesting for you

274
00:12:33,770 --> 00:12:39,319
maybe it's how do we program this beast

275
00:12:36,910 --> 00:12:41,750
that's still work-in-progress so we're

276
00:12:39,320 --> 00:12:44,800
also working and at a software

277
00:12:41,750 --> 00:12:47,410
infrastructure for this architecture

278
00:12:44,800 --> 00:12:50,750
initially where we did our first

279
00:12:47,410 --> 00:12:54,709
experiments which is a simple SPMD based

280
00:12:50,750 --> 00:12:56,840
parallelism using c and we use in

281
00:12:54,710 --> 00:13:01,280
forensics special intrinsics for the

282
00:12:56,840 --> 00:13:06,290
puma specific instructions and our vm

283
00:13:01,280 --> 00:13:08,900
for building a compiler in the meantime

284
00:13:06,290 --> 00:13:10,370
there has been increasing support for

285
00:13:08,900 --> 00:13:14,630
c++ p threats

286
00:13:10,370 --> 00:13:17,240
open and p and some tasking and we're

287
00:13:14,630 --> 00:13:18,790
also looking at implementing common

288
00:13:17,240 --> 00:13:23,740
graph library Scala programming

289
00:13:18,790 --> 00:13:23,740
languages and a Python front-end

290
00:13:23,870 --> 00:13:31,230
okay now importantly does pull-up

291
00:13:28,610 --> 00:13:35,570
fulfill its promises namely that it's

292
00:13:31,230 --> 00:13:37,079
more efficient to general to do graph

293
00:13:35,570 --> 00:13:39,600
analysis on it

294
00:13:37,079 --> 00:13:42,029
of course the chip is not available yet

295
00:13:39,600 --> 00:13:44,630
and neither its own so we are also

296
00:13:42,029 --> 00:13:48,779
working at FPGA model but that's not

297
00:13:44,630 --> 00:13:50,850
ready yet so basically our results that

298
00:13:48,779 --> 00:13:54,509
I will show up based on simulation where

299
00:13:50,850 --> 00:13:56,130
you take a puma binary you have a

300
00:13:54,509 --> 00:13:57,990
functional simulator that decodes the

301
00:13:56,130 --> 00:14:00,660
instructions of the binary that performs

302
00:13:57,990 --> 00:14:03,149
the actual simulated operations and I

303
00:14:00,660 --> 00:14:05,610
have the timing simulation that gives us

304
00:14:03,149 --> 00:14:07,560
that models all of the hardware

305
00:14:05,610 --> 00:14:10,079
structures such as the course the memory

306
00:14:07,560 --> 00:14:12,810
scratchpad Network and so on and that

307
00:14:10,079 --> 00:14:18,689
gives us a performance number and also

308
00:14:12,810 --> 00:14:22,349
interesting for developers a profile of

309
00:14:18,690 --> 00:14:25,829
how the execution looks like where cores

310
00:14:22,350 --> 00:14:27,569
are idle that's the thing part or used

311
00:14:25,829 --> 00:14:30,449
or waiting for memory that's the

312
00:14:27,569 --> 00:14:33,120
light-blue part such that the developers

313
00:14:30,449 --> 00:14:34,920
can optimize their fight the bottlenecks

314
00:14:33,120 --> 00:14:37,199
and of team optimize their application

315
00:14:34,920 --> 00:14:40,099
we also have an analytical model which i

316
00:14:37,199 --> 00:14:43,380
won't go into detail because simulation

317
00:14:40,100 --> 00:14:45,839
is a very intensive process so we can

318
00:14:43,380 --> 00:14:49,170
simulate up to a few tiles but after

319
00:14:45,839 --> 00:14:51,269
that it becomes quickly infeasible and

320
00:14:49,170 --> 00:14:53,579
then therefore you use this analytical

321
00:14:51,269 --> 00:14:57,569
model which we then validate using

322
00:14:53,579 --> 00:15:01,380
simulation on a smaller course then we

323
00:14:57,569 --> 00:15:04,880
took a problem of kernels graph colors

324
00:15:01,380 --> 00:15:07,560
and applications we run them on a

325
00:15:04,880 --> 00:15:10,649
high-end in told Xeon server with four

326
00:15:07,560 --> 00:15:13,560
sockets we optimize them if needed and

327
00:15:10,649 --> 00:15:16,529
we also also ported and optimize these

328
00:15:13,560 --> 00:15:20,670
applications for Puma and the initial

329
00:15:16,529 --> 00:15:23,089
estimates show that this high-end

330
00:15:20,670 --> 00:15:26,250
zealand server with four circuits were

331
00:15:23,089 --> 00:15:29,449
approximately consumes the same power as

332
00:15:26,250 --> 00:15:33,649
a puma note well consumed in the future

333
00:15:29,449 --> 00:15:36,680
so comparing their performances also a

334
00:15:33,649 --> 00:15:39,840
energy efficiency

335
00:15:36,680 --> 00:15:41,969
we also did some multi-node on Zeon but

336
00:15:39,840 --> 00:15:45,320
that didn't work well because here's it

337
00:15:41,970 --> 00:15:48,450
you have this communication overhead

338
00:15:45,320 --> 00:15:50,400
that the results that the result was

339
00:15:48,450 --> 00:15:52,890
that we saw no speed-up for most

340
00:15:50,400 --> 00:15:55,829
applications or even slowdowns we

341
00:15:52,890 --> 00:16:00,150
projected to 16 to my notes and as I

342
00:15:55,830 --> 00:16:03,090
will show shortly this skills much

343
00:16:00,150 --> 00:16:04,500
better so here is an overview of some of

344
00:16:03,090 --> 00:16:07,590
the applications the performance results

345
00:16:04,500 --> 00:16:10,230
so it shows the speed up of a single

346
00:16:07,590 --> 00:16:13,500
Puma north versus a high incidence

347
00:16:10,230 --> 00:16:15,720
server and you see that there is always

348
00:16:13,500 --> 00:16:18,930
PETA there is a lot but there is a large

349
00:16:15,720 --> 00:16:21,360
range so it goes from T times to 200

350
00:16:18,930 --> 00:16:24,150
more than 200 times faster than Z own

351
00:16:21,360 --> 00:16:27,060
that's because some applications are

352
00:16:24,150 --> 00:16:29,340
more complete intensive did so they do

353
00:16:27,060 --> 00:16:31,410
more compute and of course the actors

354
00:16:29,340 --> 00:16:33,600
even can use all of its resources to

355
00:16:31,410 --> 00:16:36,000
efficiently do that on the other hand if

356
00:16:33,600 --> 00:16:39,360
it's purely memory bandwidth bound such

357
00:16:36,000 --> 00:16:43,200
as random walks then we see a very huge

358
00:16:39,360 --> 00:16:45,990
performance increase and the 16 order of

359
00:16:43,200 --> 00:16:48,830
injections also show that it scales much

360
00:16:45,990 --> 00:16:51,810
better than the others

361
00:16:48,830 --> 00:16:54,870
okay that was base get so I showed you

362
00:16:51,810 --> 00:16:57,930
that Puma is a programmable instruction

363
00:16:54,870 --> 00:17:00,180
set processor for graph applications it

364
00:16:57,930 --> 00:17:04,169
contains many features which I discussed

365
00:17:00,180 --> 00:17:05,938
and we show that it's true simulation

366
00:17:04,170 --> 00:17:09,120
and modeling that it's one to two orders

367
00:17:05,939 --> 00:17:12,120
of magnitude faster than a equal powers

368
00:17:09,119 --> 00:17:15,560
ian and it skills well to multi node and

369
00:17:12,119 --> 00:17:15,560
it's still a development of course

370
00:17:17,140 --> 00:17:25,479
[Applause]

371
00:17:32,360 --> 00:17:37,740
think this will be affordable as rock

372
00:17:36,330 --> 00:17:40,049
station or would you say no this is

373
00:17:37,740 --> 00:17:43,950
cloud installation that is somewhere in

374
00:17:40,049 --> 00:17:47,549
the service center so the question is

375
00:17:43,950 --> 00:17:50,370
whether this is a as a workstation PC or

376
00:17:47,549 --> 00:17:54,260
or desktop based thing or a no it would

377
00:17:50,370 --> 00:18:17,010
be more in a data center pick

378
00:17:54,260 --> 00:18:21,150
supercomputer setup so the question is

379
00:18:17,010 --> 00:18:25,950
if if we plan to have some lets it host

380
00:18:21,150 --> 00:18:27,900
notes or yeah we're still working on

381
00:18:25,950 --> 00:18:33,210
that so there is an there is an option

382
00:18:27,900 --> 00:18:35,880
to have potentially x86 nodes that do

383
00:18:33,210 --> 00:18:37,740
more of the other stuff that this is not

384
00:18:35,880 --> 00:18:40,040
efficient for but it's still under

385
00:18:37,740 --> 00:18:40,040
development

386
00:18:59,660 --> 00:19:37,890
it's not in the same it's an all

387
00:19:31,890 --> 00:19:40,890
connection but what's the okay so the

388
00:19:37,890 --> 00:19:44,550
question is is is data that's further in

389
00:19:40,890 --> 00:19:46,710
memory will it be slow were to exit yes

390
00:19:44,550 --> 00:19:50,070
of course because it's a very large

391
00:19:46,710 --> 00:19:51,360
system its distributed across the system

392
00:19:50,070 --> 00:19:54,330
will be distributed across multiple

393
00:19:51,360 --> 00:19:56,550
notes multiple racks so there will be a

394
00:19:54,330 --> 00:19:58,409
larger penalty but because of this all

395
00:19:56,550 --> 00:20:00,840
of this memory latency hiding techniques

396
00:19:58,410 --> 00:20:02,670
and these offload engines that perform

397
00:20:00,840 --> 00:20:05,310
it very efficiently you won't see as

398
00:20:02,670 --> 00:20:08,000
much as you see an conventional multi

399
00:20:05,310 --> 00:20:08,000
node set up

400
00:20:18,620 --> 00:20:25,850
[Applause]

