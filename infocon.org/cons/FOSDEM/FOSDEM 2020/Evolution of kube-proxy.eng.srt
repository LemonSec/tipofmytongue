1
00:00:17,359 --> 00:00:20,000
okay next up we've got lauren bernard

2
00:00:18,960 --> 00:00:21,600
who's going to be talking to us about

3
00:00:20,000 --> 00:00:31,840
the evolution

4
00:00:21,600 --> 00:00:31,840
evolution of cubeproxy

5
00:00:33,760 --> 00:00:45,839
i can tell if the mic is live

6
00:00:57,440 --> 00:01:00,480
is it green it's not green

7
00:01:00,960 --> 00:01:04,799
okay should be good now okay oh that's

8
00:01:02,960 --> 00:01:07,439
better yay

9
00:01:04,799 --> 00:01:08,159
so as i was saying i work at datadog and

10
00:01:07,439 --> 00:01:10,320
we're sas

11
00:01:08,159 --> 00:01:11,920
monitoring company and i put a few

12
00:01:10,320 --> 00:01:12,798
figures on the slide but what matters

13
00:01:11,920 --> 00:01:15,520
most

14
00:01:12,799 --> 00:01:16,640
is we run pretty large infrastructure

15
00:01:15,520 --> 00:01:19,679
and we run a lot of

16
00:01:16,640 --> 00:01:20,159
kubernetes clusters some of them um very

17
00:01:19,680 --> 00:01:23,920
big

18
00:01:20,159 --> 00:01:24,240
up to 3000 nodes and of course as we

19
00:01:23,920 --> 00:01:27,040
want

20
00:01:24,240 --> 00:01:29,119
large clusters we've had a few issues

21
00:01:27,040 --> 00:01:31,360
with kubernetes and some of them

22
00:01:29,119 --> 00:01:32,320
related to accessing services at this

23
00:01:31,360 --> 00:01:35,040
scale

24
00:01:32,320 --> 00:01:35,758
and that's why i got into uh i got very

25
00:01:35,040 --> 00:01:37,840
involved

26
00:01:35,759 --> 00:01:40,880
with whiskey proxy and in particular

27
00:01:37,840 --> 00:01:44,000
with ipvs mode for focus proxy

28
00:01:40,880 --> 00:01:46,399
so just to get started uh some very

29
00:01:44,000 --> 00:01:48,079
quick background on cooperation itself

30
00:01:46,399 --> 00:01:50,079
so q proxy is a component that's

31
00:01:48,079 --> 00:01:51,679
responsible for

32
00:01:50,079 --> 00:01:53,439
managing the service abstraction in

33
00:01:51,680 --> 00:01:54,560
kubernetes so it's a component that runs

34
00:01:53,439 --> 00:01:56,960
on every node

35
00:01:54,560 --> 00:01:58,000
and it's going to enable access to

36
00:01:56,960 --> 00:02:01,839
cluster ip

37
00:01:58,000 --> 00:02:04,159
which is a virtual ip from this node

38
00:02:01,840 --> 00:02:05,840
um and so basically when you try to

39
00:02:04,159 --> 00:02:06,799
access the service you try to access the

40
00:02:05,840 --> 00:02:08,239
special ip

41
00:02:06,799 --> 00:02:12,640
and could practice responsible for

42
00:02:08,239 --> 00:02:12,640
transforming this ip into a put back end

43
00:02:13,280 --> 00:02:16,959
if we look at what a deployment and

44
00:02:14,800 --> 00:02:18,239
service look like

45
00:02:16,959 --> 00:02:20,000
here on this example you have a

46
00:02:18,239 --> 00:02:21,920
deployment of three pods

47
00:02:20,000 --> 00:02:24,160
and and what matters for for these

48
00:02:21,920 --> 00:02:26,559
deployments is it has a label

49
00:02:24,160 --> 00:02:28,079
which is the app is called eco and then

50
00:02:26,560 --> 00:02:28,959
you have a matching service on the right

51
00:02:28,080 --> 00:02:31,680
hand side

52
00:02:28,959 --> 00:02:34,080
which is uh selecting parts from this

53
00:02:31,680 --> 00:02:36,480
deployment by using the selector saying

54
00:02:34,080 --> 00:02:39,280
select all the pods uh with label app

55
00:02:36,480 --> 00:02:39,280
equal echoes

56
00:02:39,440 --> 00:02:43,359
and when you create all these subjects

57
00:02:41,120 --> 00:02:44,800
you're going to to have all this created

58
00:02:43,360 --> 00:02:46,959
so first you're going to create the

59
00:02:44,800 --> 00:02:48,319
deployment the deployment is going to

60
00:02:46,959 --> 00:02:49,360
create a replica set which is

61
00:02:48,319 --> 00:02:52,160
responsible

62
00:02:49,360 --> 00:02:53,599
for creating all the pods so in our case

63
00:02:52,160 --> 00:02:55,519
we have we have three pods

64
00:02:53,599 --> 00:02:57,040
and the replica set is going to manage

65
00:02:55,519 --> 00:02:58,959
this these reports

66
00:02:57,040 --> 00:03:00,239
so as i was saying before all these pods

67
00:02:58,959 --> 00:03:02,319
have a label

68
00:03:00,239 --> 00:03:04,000
and the service is matching the same

69
00:03:02,319 --> 00:03:06,958
label so it's going to consider all this

70
00:03:04,000 --> 00:03:09,120
bud as back-end for for the service

71
00:03:06,959 --> 00:03:11,360
there's actually an additional component

72
00:03:09,120 --> 00:03:13,519
which is called an endpoint

73
00:03:11,360 --> 00:03:15,280
and this component is completely managed

74
00:03:13,519 --> 00:03:17,360
by kubuntu by the control plane

75
00:03:15,280 --> 00:03:19,200
you will almost never see it because you

76
00:03:17,360 --> 00:03:21,280
don't interact with it

77
00:03:19,200 --> 00:03:22,238
and this endpoint is actually an

78
00:03:21,280 --> 00:03:25,280
optimization

79
00:03:22,239 --> 00:03:27,040
so that q proxy on all the nodes doesn't

80
00:03:25,280 --> 00:03:28,640
have to watch all the pod objects which

81
00:03:27,040 --> 00:03:31,760
can be pretty big

82
00:03:28,640 --> 00:03:33,279
a pod object is easily easily a thousand

83
00:03:31,760 --> 00:03:35,280
bytes so quite often

84
00:03:33,280 --> 00:03:36,879
bigger and if of course if you have a

85
00:03:35,280 --> 00:03:37,760
large amount of pods and general amounts

86
00:03:36,879 --> 00:03:39,599
of service

87
00:03:37,760 --> 00:03:41,359
if you need to add in memory the

88
00:03:39,599 --> 00:03:42,159
description of all the pods on all the

89
00:03:41,360 --> 00:03:45,040
nodes

90
00:03:42,159 --> 00:03:46,560
it's going to be pretty big so the

91
00:03:45,040 --> 00:03:47,760
endpoint object is actually a very

92
00:03:46,560 --> 00:03:50,959
simplified view

93
00:03:47,760 --> 00:03:52,879
of all the ips backing a given service

94
00:03:50,959 --> 00:03:55,519
so in my example here you can see that

95
00:03:52,879 --> 00:03:57,840
the endpoint is a very simple structure

96
00:03:55,519 --> 00:04:00,720
with the ip of all the pod backing

97
00:03:57,840 --> 00:04:02,959
backing the service

98
00:04:00,720 --> 00:04:04,959
there's an additional concept which is

99
00:04:02,959 --> 00:04:06,640
called readiness

100
00:04:04,959 --> 00:04:08,080
in kubernetes you can associate a

101
00:04:06,640 --> 00:04:10,640
readiness probe

102
00:04:08,080 --> 00:04:11,840
with a deployment and in this case a

103
00:04:10,640 --> 00:04:14,559
produce won't be

104
00:04:11,840 --> 00:04:15,680
ready unless it satisfies this readiness

105
00:04:14,560 --> 00:04:17,440
probe

106
00:04:15,680 --> 00:04:20,079
and the way it works is you can for

107
00:04:17,440 --> 00:04:23,120
instance specify an http probe

108
00:04:20,079 --> 00:04:25,520
and connect to a path and verify that

109
00:04:23,120 --> 00:04:28,000
it's healthy

110
00:04:25,520 --> 00:04:29,520
and the way it's used is basically if if

111
00:04:28,000 --> 00:04:32,320
a pod is not ready because

112
00:04:29,520 --> 00:04:33,840
it can't serve traffic it won't be added

113
00:04:32,320 --> 00:04:35,120
as an endpoint so when an application

114
00:04:33,840 --> 00:04:38,320
try to connect to the service

115
00:04:35,120 --> 00:04:38,320
it won't be routed to that but

116
00:04:39,199 --> 00:04:43,520
so if i get back to the example i was

117
00:04:40,880 --> 00:04:44,320
giving before with my echo service with

118
00:04:43,520 --> 00:04:46,639
tripods

119
00:04:44,320 --> 00:04:48,639
if i connect in another pod and connect

120
00:04:46,639 --> 00:04:51,919
to a virtual ip

121
00:04:48,639 --> 00:04:53,840
vip into in 10.200 here this is the ip

122
00:04:51,919 --> 00:04:55,359
of the service

123
00:04:53,840 --> 00:04:56,960
i'm actually going to be connected to

124
00:04:55,360 --> 00:04:59,040
the pod and as you can see on this slide

125
00:04:56,960 --> 00:05:01,198
the pod is just giving us an answer

126
00:04:59,040 --> 00:05:02,639
their own source ip and you can see that

127
00:05:01,199 --> 00:05:04,320
i'm actually getting to three different

128
00:05:02,639 --> 00:05:05,680
pods which are the three parts backing

129
00:05:04,320 --> 00:05:07,520
the service

130
00:05:05,680 --> 00:05:09,039
of course if one of the pod was not

131
00:05:07,520 --> 00:05:11,280
ready i wouldn't be right

132
00:05:09,039 --> 00:05:13,120
there and so if of the tripod only two

133
00:05:11,280 --> 00:05:14,559
were ready i would see only two ips

134
00:05:13,120 --> 00:05:16,400
there

135
00:05:14,560 --> 00:05:18,080
so this is the very quick introduction

136
00:05:16,400 --> 00:05:21,440
on on what

137
00:05:18,080 --> 00:05:24,159
uh q proxy what your proxy does so how

138
00:05:21,440 --> 00:05:27,120
does this all work

139
00:05:24,160 --> 00:05:28,639
so in order for a per to be added to an

140
00:05:27,120 --> 00:05:30,320
end point it needs to be ready it needs

141
00:05:28,639 --> 00:05:31,520
to be running on a node and it needs to

142
00:05:30,320 --> 00:05:33,759
be ready

143
00:05:31,520 --> 00:05:35,359
and the component that's responsible for

144
00:05:33,759 --> 00:05:35,840
giving this information to the control

145
00:05:35,360 --> 00:05:38,000
plane

146
00:05:35,840 --> 00:05:39,919
is the cubelet the cubelet is the main

147
00:05:38,000 --> 00:05:40,800
component running on each kubernetes

148
00:05:39,919 --> 00:05:43,359
node

149
00:05:40,800 --> 00:05:44,400
and quiz is responsible for starting pod

150
00:05:43,360 --> 00:05:47,039
by interacting

151
00:05:44,400 --> 00:05:48,239
with the container runtime and also

152
00:05:47,039 --> 00:05:50,719
performing the health check

153
00:05:48,240 --> 00:05:52,400
if there's a health check for the pod

154
00:05:50,720 --> 00:05:54,400
and the qubit will

155
00:05:52,400 --> 00:05:55,919
all the time give information on on the

156
00:05:54,400 --> 00:05:58,080
pod that are running on the host

157
00:05:55,919 --> 00:05:59,280
and update the status is the pod running

158
00:05:58,080 --> 00:06:01,758
is the pod ready

159
00:05:59,280 --> 00:06:03,119
all this information and this

160
00:06:01,759 --> 00:06:04,720
information is going to be reported to

161
00:06:03,120 --> 00:06:06,479
the api server and starting in

162
00:06:04,720 --> 00:06:09,120
cd which is the data store for

163
00:06:06,479 --> 00:06:09,120
kubernetes

164
00:06:09,759 --> 00:06:14,400
and this is now translated to endpoint

165
00:06:12,720 --> 00:06:15,840
by a very specific controller in

166
00:06:14,400 --> 00:06:18,479
kubernetes which is called

167
00:06:15,840 --> 00:06:20,159
the endpoint controller and the role of

168
00:06:18,479 --> 00:06:21,840
this controller is just to maintain the

169
00:06:20,160 --> 00:06:23,759
endpoint objects

170
00:06:21,840 --> 00:06:25,679
so the endpoint controller is going to

171
00:06:23,759 --> 00:06:28,000
watch for all the services

172
00:06:25,680 --> 00:06:29,840
and all the pods and updates the

173
00:06:28,000 --> 00:06:31,680
endpoint object for all the services

174
00:06:29,840 --> 00:06:35,840
based on all the pod matching a given

175
00:06:31,680 --> 00:06:35,840
label that are ready of course

176
00:06:37,199 --> 00:06:40,319
now that we have this endpoint object

177
00:06:38,880 --> 00:06:42,319
that is synchronized

178
00:06:40,319 --> 00:06:44,319
by the endpoint controller if we want to

179
00:06:42,319 --> 00:06:45,680
access a service from from a node from a

180
00:06:44,319 --> 00:06:48,880
container

181
00:06:45,680 --> 00:06:51,199
this is where q proxy enters into play

182
00:06:48,880 --> 00:06:52,000
so q proxy is responsible for watching

183
00:06:51,199 --> 00:06:53,840
services

184
00:06:52,000 --> 00:06:56,479
and the end point associated with each

185
00:06:53,840 --> 00:06:58,400
service and to configure something that

186
00:06:56,479 --> 00:07:00,000
i call a proctor and you're going to see

187
00:06:58,400 --> 00:07:03,840
why i call it a proxy because you have

188
00:07:00,000 --> 00:07:03,840
different ways to do that

189
00:07:04,160 --> 00:07:07,199
and when the client is going to connect

190
00:07:06,080 --> 00:07:09,919
to the service

191
00:07:07,199 --> 00:07:11,599
using the virtual ip the proxy is

192
00:07:09,919 --> 00:07:12,639
responsible for sending traffic to

193
00:07:11,599 --> 00:07:14,560
actual pods

194
00:07:12,639 --> 00:07:16,000
okay so i'm a client i'm trying to talk

195
00:07:14,560 --> 00:07:17,680
to the echo service i mentioned

196
00:07:16,000 --> 00:07:19,520
earlier i'm sending traffic to the

197
00:07:17,680 --> 00:07:21,840
virtual ip and the proxy is going to

198
00:07:19,520 --> 00:07:25,440
write me either to pod one or but be

199
00:07:21,840 --> 00:07:25,440
on port two in my in my example

200
00:07:26,000 --> 00:07:29,440
so there are different implementation of

201
00:07:28,240 --> 00:07:32,720
the proxy

202
00:07:29,440 --> 00:07:34,160
um the initial implementation was was a

203
00:07:32,720 --> 00:07:37,520
user space implementation

204
00:07:34,160 --> 00:07:38,080
so you can see this as an hg proxy for

205
00:07:37,520 --> 00:07:40,400
instance

206
00:07:38,080 --> 00:07:42,840
so you would have a local proxy running

207
00:07:40,400 --> 00:07:44,159
and all traffic would be running through

208
00:07:42,840 --> 00:07:45,919
it

209
00:07:44,160 --> 00:07:48,000
the actual implementation looks like

210
00:07:45,919 --> 00:07:49,919
this so

211
00:07:48,000 --> 00:07:51,120
when you have a client it's going to be

212
00:07:49,919 --> 00:07:54,240
rewrited and

213
00:07:51,120 --> 00:07:55,919
when it sends traffic to to a service ip

214
00:07:54,240 --> 00:07:59,039
the traffic is going to be rewrited to

215
00:07:55,919 --> 00:08:00,400
cubox itself okay so you have a mapping

216
00:07:59,039 --> 00:08:03,759
table rules that's

217
00:08:00,400 --> 00:08:05,758
going to redirect traffic to your proxy

218
00:08:03,759 --> 00:08:07,120
and the way it works is for every

219
00:08:05,759 --> 00:08:08,879
service in the cluster

220
00:08:07,120 --> 00:08:10,639
to proxy is locally going to find an

221
00:08:08,879 --> 00:08:12,960
available port

222
00:08:10,639 --> 00:08:14,479
bind it and create an epitable rule

223
00:08:12,960 --> 00:08:14,960
that's going to redirect traffic for the

224
00:08:14,479 --> 00:08:17,440
service

225
00:08:14,960 --> 00:08:19,280
directly to your proxy and then to proxy

226
00:08:17,440 --> 00:08:20,479
itself it's going to do the actual load

227
00:08:19,280 --> 00:08:23,039
balancing to pod

228
00:08:20,479 --> 00:08:25,520
and connect to pod one and point to in

229
00:08:23,039 --> 00:08:27,759
my example

230
00:08:25,520 --> 00:08:30,400
so the way it works if you look at ip

231
00:08:27,759 --> 00:08:32,000
tables on the on the instance themselves

232
00:08:30,400 --> 00:08:34,319
you can see that in the pre-routing

233
00:08:32,000 --> 00:08:36,719
chain there's this rule capturing

234
00:08:34,320 --> 00:08:37,440
everything okay and sending traffic to

235
00:08:36,719 --> 00:08:40,399
um

236
00:08:37,440 --> 00:08:40,959
the portal's container chain and in that

237
00:08:40,399 --> 00:08:44,320
chain

238
00:08:40,958 --> 00:08:46,319
you have one rule for each service

239
00:08:44,320 --> 00:08:47,360
in the cluster and this was actually

240
00:08:46,320 --> 00:08:50,640
very simple

241
00:08:47,360 --> 00:08:51,519
um if traffic if the destination the

242
00:08:50,640 --> 00:08:55,600
traffic

243
00:08:51,519 --> 00:08:57,839
is the service ip vp on the slide

244
00:08:55,600 --> 00:08:58,640
to the service port then traffic is

245
00:08:57,839 --> 00:09:01,279
redirected

246
00:08:58,640 --> 00:09:02,319
to the ip of the node which qracks in

247
00:09:01,279 --> 00:09:04,399
this is binding

248
00:09:02,320 --> 00:09:06,800
on a specific port that's been

249
00:09:04,399 --> 00:09:09,839
attributed locally by coproxy amongst

250
00:09:06,800 --> 00:09:09,839
available ports

251
00:09:10,000 --> 00:09:14,160
so this works fine but as you can

252
00:09:12,240 --> 00:09:15,760
imagine this is not very performance

253
00:09:14,160 --> 00:09:17,600
because every time you send traffic

254
00:09:15,760 --> 00:09:19,839
traffic is going to be sent from

255
00:09:17,600 --> 00:09:21,760
kernel lane to user land to the proxy

256
00:09:19,839 --> 00:09:24,640
and then back on the interface

257
00:09:21,760 --> 00:09:25,920
so it's not great also if you do that

258
00:09:24,640 --> 00:09:26,640
there's no way to actually keep the

259
00:09:25,920 --> 00:09:28,399
source ip

260
00:09:26,640 --> 00:09:30,160
if you're accessing the service from the

261
00:09:28,399 --> 00:09:32,000
container

262
00:09:30,160 --> 00:09:34,240
the ip you're going to see at the

263
00:09:32,000 --> 00:09:36,240
destination is going to be the node ip

264
00:09:34,240 --> 00:09:37,440
of your proxy itself because of course

265
00:09:36,240 --> 00:09:39,600
your proxy is going to initiate the

266
00:09:37,440 --> 00:09:41,680
connection to the back ends

267
00:09:39,600 --> 00:09:43,279
so this is not recommended anymore i'm

268
00:09:41,680 --> 00:09:44,719
pretty sure if some of you are running

269
00:09:43,279 --> 00:09:45,680
kubernetes today you're not running in

270
00:09:44,720 --> 00:09:49,680
bad mode

271
00:09:45,680 --> 00:09:51,120
um it's almost it's it's kind of

272
00:09:49,680 --> 00:09:52,959
deprecated but it's definitely not

273
00:09:51,120 --> 00:09:54,880
recommended anymore and the current

274
00:09:52,959 --> 00:09:56,239
default implementation since kubernetes

275
00:09:54,880 --> 00:09:58,320
1.2

276
00:09:56,240 --> 00:10:01,680
is iptables which i'm going to talk

277
00:09:58,320 --> 00:10:01,680
about just right now

278
00:10:03,040 --> 00:10:08,000
um so the first the first mode was user

279
00:10:06,079 --> 00:10:10,160
space and the second is iptables and

280
00:10:08,000 --> 00:10:11,519
this one is the default and once again

281
00:10:10,160 --> 00:10:12,079
if you're running to proxy it's very

282
00:10:11,519 --> 00:10:13,920
likely

283
00:10:12,079 --> 00:10:15,839
this is the mode you're currently using

284
00:10:13,920 --> 00:10:18,560
and it's the one that's used in most

285
00:10:15,839 --> 00:10:20,959
managed offerings if you look at gke or

286
00:10:18,560 --> 00:10:24,399
eks for instance on gcp and aws

287
00:10:20,959 --> 00:10:24,399
this is the mod they they use

288
00:10:24,800 --> 00:10:29,199
so in epitables mode we still use ip

289
00:10:27,839 --> 00:10:31,440
tables for redirection

290
00:10:29,200 --> 00:10:32,240
but we're not redirecting traffic took

291
00:10:31,440 --> 00:10:34,800
your proxy

292
00:10:32,240 --> 00:10:37,360
we're directly using ip tables to

293
00:10:34,800 --> 00:10:39,279
redirect traffic to to back-end pods

294
00:10:37,360 --> 00:10:40,560
so if you can you can turn this example

295
00:10:39,279 --> 00:10:42,880
but we're

296
00:10:40,560 --> 00:10:43,680
sending traffic to a virtual ip and this

297
00:10:42,880 --> 00:10:47,120
virtual ip

298
00:10:43,680 --> 00:10:48,959
is denoted to a pod ip okay and

299
00:10:47,120 --> 00:10:51,040
when the traffic comes back you hit the

300
00:10:48,959 --> 00:10:52,959
contract and you're reverse nutty to to

301
00:10:51,040 --> 00:10:55,199
the body itself

302
00:10:52,959 --> 00:10:57,359
so it looks pretty simple when we see it

303
00:10:55,200 --> 00:11:01,600
that way it's actually

304
00:10:57,360 --> 00:11:04,880
a bit complicated so the design

305
00:11:01,600 --> 00:11:05,760
is is it this one so basically same as

306
00:11:04,880 --> 00:11:08,320
before

307
00:11:05,760 --> 00:11:10,000
uh i q proxy will hook into the

308
00:11:08,320 --> 00:11:10,320
pre-routing chain and the output chain

309
00:11:10,000 --> 00:11:12,560
for

310
00:11:10,320 --> 00:11:13,760
local traffic and everything will be

311
00:11:12,560 --> 00:11:17,040
sent to a chain called

312
00:11:13,760 --> 00:11:18,880
cube services and that send

313
00:11:17,040 --> 00:11:20,880
that chain will do the same type of

314
00:11:18,880 --> 00:11:24,000
matching as we had before

315
00:11:20,880 --> 00:11:25,200
so it will match the cluster ip and the

316
00:11:24,000 --> 00:11:27,680
service port

317
00:11:25,200 --> 00:11:28,640
and send traffic to a chain for the

318
00:11:27,680 --> 00:11:30,399
service itself

319
00:11:28,640 --> 00:11:32,640
so what's important here is you have a

320
00:11:30,399 --> 00:11:34,000
new chain for every service of course if

321
00:11:32,640 --> 00:11:35,519
you have a lot of services

322
00:11:34,000 --> 00:11:37,200
you're going to have a lot of chains and

323
00:11:35,519 --> 00:11:40,399
your rp table configuration

324
00:11:37,200 --> 00:11:40,399
is going to be kind of a mess

325
00:11:40,560 --> 00:11:44,239
in that chain you have one rule for each

326
00:11:43,440 --> 00:11:46,000
back end

327
00:11:44,240 --> 00:11:47,600
and this is where things start to be a

328
00:11:46,000 --> 00:11:50,800
bit hacky

329
00:11:47,600 --> 00:11:53,120
as you can see here qproxy is using the

330
00:11:50,800 --> 00:11:55,680
statistic iptables model

331
00:11:53,120 --> 00:11:56,399
to to randomly send traffic to the back

332
00:11:55,680 --> 00:11:59,519
ends

333
00:11:56,399 --> 00:12:01,839
and the way it does it is well there's

334
00:11:59,519 --> 00:12:05,040
a probability for the rule to apply and

335
00:12:01,839 --> 00:12:06,160
if the rule apply you route it to a pod

336
00:12:05,040 --> 00:12:08,240
as you can see when you read it for the

337
00:12:06,160 --> 00:12:10,160
first time this is not very intuitive

338
00:12:08,240 --> 00:12:12,240
because instead of saying

339
00:12:10,160 --> 00:12:13,360
uh one third one side one third which

340
00:12:12,240 --> 00:12:14,079
you would expect if you have three

341
00:12:13,360 --> 00:12:15,680
backhands

342
00:12:14,079 --> 00:12:17,199
it's actually one third for the first

343
00:12:15,680 --> 00:12:18,560
one because there's once

344
00:12:17,200 --> 00:12:20,560
one chance out of three the rule is

345
00:12:18,560 --> 00:12:22,719
going to apply but then

346
00:12:20,560 --> 00:12:23,599
if you if the rule if the rule is not

347
00:12:22,720 --> 00:12:25,360
matched

348
00:12:23,600 --> 00:12:26,720
uh you should only have two back ends

349
00:12:25,360 --> 00:12:29,760
and so that way

350
00:12:26,720 --> 00:12:32,880
50 for the second one

351
00:12:29,760 --> 00:12:34,800
and finally um you have a chain for each

352
00:12:32,880 --> 00:12:37,120
endpoint for a service

353
00:12:34,800 --> 00:12:38,719
and this chain is just responsible for

354
00:12:37,120 --> 00:12:40,959
doing d-nut itself

355
00:12:38,720 --> 00:12:42,880
so modifying the destination ip to use

356
00:12:40,959 --> 00:12:45,680
the pod ip

357
00:12:42,880 --> 00:12:47,200
and it's also doing something that's a

358
00:12:45,680 --> 00:12:50,319
bit surprising when you look at it

359
00:12:47,200 --> 00:12:53,600
the first time for traffic that is sent

360
00:12:50,320 --> 00:12:56,480
back to the pod uh where

361
00:12:53,600 --> 00:12:57,120
it was coming from so happen traffic we

362
00:12:56,480 --> 00:12:59,200
need to use

363
00:12:57,120 --> 00:13:00,560
s-nuts so let's let's look into that

364
00:12:59,200 --> 00:13:03,760
because this is a bit uh

365
00:13:00,560 --> 00:13:06,079
complicated so imagine i'm

366
00:13:03,760 --> 00:13:08,240
in pod one and i want to access a

367
00:13:06,079 --> 00:13:12,399
service that's backed by pod one

368
00:13:08,240 --> 00:13:15,440
okay and other pods if traffic if i just

369
00:13:12,399 --> 00:13:17,600
do dna the after dinner the traffic

370
00:13:15,440 --> 00:13:18,880
is going to have the same source ip and

371
00:13:17,600 --> 00:13:22,560
destination ip

372
00:13:18,880 --> 00:13:25,200
and of course this will not work great

373
00:13:22,560 --> 00:13:25,680
and so in that case uh what iptable does

374
00:13:25,200 --> 00:13:28,560
is

375
00:13:25,680 --> 00:13:29,680
it's doing sourcenading to the host ip

376
00:13:28,560 --> 00:13:35,839
so traffic can be

377
00:13:29,680 --> 00:13:35,839
reverse netted back to the destination

378
00:13:36,079 --> 00:13:39,120
another thing you can do with kubernetes

379
00:13:38,480 --> 00:13:41,120
services

380
00:13:39,120 --> 00:13:42,480
people tend not to use it much and i

381
00:13:41,120 --> 00:13:44,240
think it's a good idea because it's a

382
00:13:42,480 --> 00:13:47,440
bit complicated too

383
00:13:44,240 --> 00:13:49,519
is to use affinity so what you

384
00:13:47,440 --> 00:13:51,600
what you can't want for the application

385
00:13:49,519 --> 00:13:52,480
is for all traffic from the same source

386
00:13:51,600 --> 00:13:54,880
ip

387
00:13:52,480 --> 00:13:56,320
to get to the same back end um because

388
00:13:54,880 --> 00:13:59,040
you have some sort of

389
00:13:56,320 --> 00:14:01,519
affinity constraints and as you can

390
00:13:59,040 --> 00:14:02,000
imagine doing persistency with ip tables

391
00:14:01,519 --> 00:14:05,360
is not

392
00:14:02,000 --> 00:14:07,360
easy so the way this is done is using

393
00:14:05,360 --> 00:14:08,079
the recent module which usually people

394
00:14:07,360 --> 00:14:11,120
use for

395
00:14:08,079 --> 00:14:14,239
security to avoid uh port scanning

396
00:14:11,120 --> 00:14:16,160
or dealers and in that case what happens

397
00:14:14,240 --> 00:14:19,360
is when you did add traffic

398
00:14:16,160 --> 00:14:19,920
to a pod ip you also insert this source

399
00:14:19,360 --> 00:14:23,440
ip

400
00:14:19,920 --> 00:14:24,079
into a specific set a specific recent

401
00:14:23,440 --> 00:14:26,800
set

402
00:14:24,079 --> 00:14:28,719
with the same name for for the end point

403
00:14:26,800 --> 00:14:30,719
and then the next time

404
00:14:28,720 --> 00:14:32,079
you hit the service chain okay you

405
00:14:30,720 --> 00:14:33,839
remember this chain where you had all

406
00:14:32,079 --> 00:14:35,599
the load balancing rules

407
00:14:33,839 --> 00:14:37,120
in addition to the load balancing rules

408
00:14:35,600 --> 00:14:39,120
at the beginning of the chain

409
00:14:37,120 --> 00:14:41,199
you have these rules there that are

410
00:14:39,120 --> 00:14:43,199
checking if the source ip

411
00:14:41,199 --> 00:14:45,199
is matching against the set that exists

412
00:14:43,199 --> 00:14:46,880
and if it does traffic is directly going

413
00:14:45,199 --> 00:14:48,959
to be sent to the matching endpoint

414
00:14:46,880 --> 00:14:50,800
without using without going through load

415
00:14:48,959 --> 00:14:54,079
balancing rules

416
00:14:50,800 --> 00:14:55,199
so this works but this also feels quite

417
00:14:54,079 --> 00:14:58,319
i mean a bit hacky

418
00:14:55,199 --> 00:15:00,639
and and it's very difficult

419
00:14:58,320 --> 00:15:02,800
to to expire connections and do do

420
00:15:00,639 --> 00:15:06,399
discuss things

421
00:15:02,800 --> 00:15:08,639
so in terms of limitations uh well using

422
00:15:06,399 --> 00:15:10,480
ip tables to do load balancing is kind

423
00:15:08,639 --> 00:15:13,360
of a surprising idea

424
00:15:10,480 --> 00:15:13,920
it works super surprisingly well though

425
00:15:13,360 --> 00:15:15,519
uh

426
00:15:13,920 --> 00:15:17,279
and it's been running for i mean many

427
00:15:15,519 --> 00:15:18,720
most people are running committed this

428
00:15:17,279 --> 00:15:22,000
way

429
00:15:18,720 --> 00:15:22,880
it's very hard to debug though very

430
00:15:22,000 --> 00:15:26,240
quickly you have

431
00:15:22,880 --> 00:15:27,600
tens of thousands of ip table it's very

432
00:15:26,240 --> 00:15:30,560
hard to understand

433
00:15:27,600 --> 00:15:31,120
and to be honest the last time i i was

434
00:15:30,560 --> 00:15:34,079
running

435
00:15:31,120 --> 00:15:35,600
um in ip table mode on a midsize cluster

436
00:15:34,079 --> 00:15:38,160
of about a thousand nodes

437
00:15:35,600 --> 00:15:39,360
we had i think 50 000 iptable rules and

438
00:15:38,160 --> 00:15:40,000
when you want to debug it understand

439
00:15:39,360 --> 00:15:43,360
what's happening

440
00:15:40,000 --> 00:15:44,959
it's it's pretty tricky worse uh

441
00:15:43,360 --> 00:15:47,279
there's actually a huge performance

442
00:15:44,959 --> 00:15:49,040
impact both on the data plane and the

443
00:15:47,279 --> 00:15:52,079
control plane

444
00:15:49,040 --> 00:15:54,240
so here i'm i'm just quoting from a very

445
00:15:52,079 --> 00:15:57,680
interesting talk

446
00:15:54,240 --> 00:15:59,600
from cupcake in 2017 and

447
00:15:57,680 --> 00:16:01,758
this is the impact of happy tables on

448
00:15:59,600 --> 00:16:03,920
riding performances

449
00:16:01,759 --> 00:16:04,959
so what happens is of course if you have

450
00:16:03,920 --> 00:16:06,639
a lot of rules

451
00:16:04,959 --> 00:16:08,719
to traverse before getting to your

452
00:16:06,639 --> 00:16:10,959
endpoint this means

453
00:16:08,720 --> 00:16:12,320
uh it's going to take some time to go

454
00:16:10,959 --> 00:16:13,518
through all the all the rules in the

455
00:16:12,320 --> 00:16:16,079
chains

456
00:16:13,519 --> 00:16:17,120
and as you have more more back-ends and

457
00:16:16,079 --> 00:16:19,519
more services

458
00:16:17,120 --> 00:16:20,480
this can actually take quite some time

459
00:16:19,519 --> 00:16:23,440
you can see invest

460
00:16:20,480 --> 00:16:24,320
in this extreme example here with 50 000

461
00:16:23,440 --> 00:16:27,440
services

462
00:16:24,320 --> 00:16:30,079
that at one point it's taking like

463
00:16:27,440 --> 00:16:30,959
about seven milliseconds just to get

464
00:16:30,079 --> 00:16:32,479
through the chain

465
00:16:30,959 --> 00:16:34,160
so you want to establish a connection

466
00:16:32,480 --> 00:16:34,800
and just going through the epitable

467
00:16:34,160 --> 00:16:38,240
chains

468
00:16:34,800 --> 00:16:38,240
is going to take you a few milliseconds

469
00:16:39,279 --> 00:16:44,399
and the worst part is actually note the

470
00:16:42,079 --> 00:16:46,719
data plane it's the control plane

471
00:16:44,399 --> 00:16:48,240
because the way to proxy works when used

472
00:16:46,720 --> 00:16:50,480
in ip table mode

473
00:16:48,240 --> 00:16:51,839
is every time there's a change in

474
00:16:50,480 --> 00:16:53,759
endpoint of services

475
00:16:51,839 --> 00:16:55,040
it's going to recompute the full set of

476
00:16:53,759 --> 00:16:56,639
iptable rules

477
00:16:55,040 --> 00:16:58,480
and as i was saying before this can be

478
00:16:56,639 --> 00:17:01,120
tens of thousands of rules

479
00:16:58,480 --> 00:17:02,480
and do a single atomic reload of all

480
00:17:01,120 --> 00:17:04,959
these rules

481
00:17:02,480 --> 00:17:06,240
so it's fine on mid-size clusters and or

482
00:17:04,959 --> 00:17:08,959
small size cluster

483
00:17:06,240 --> 00:17:10,240
but on large clusters this very easily

484
00:17:08,959 --> 00:17:13,120
takes a few minutes

485
00:17:10,240 --> 00:17:14,319
um so if you if you look into issues in

486
00:17:13,119 --> 00:17:16,000
on the comments uh

487
00:17:14,319 --> 00:17:18,079
github you're gonna see that many people

488
00:17:16,000 --> 00:17:20,480
are actually getting out errors

489
00:17:18,079 --> 00:17:21,678
because by default currency time times

490
00:17:20,480 --> 00:17:24,559
out after five minutes

491
00:17:21,679 --> 00:17:26,559
when trying to read all rules but it's

492
00:17:24,559 --> 00:17:28,240
very easy to get to to go far above five

493
00:17:26,559 --> 00:17:30,799
minutes so you can see in this example

494
00:17:28,240 --> 00:17:32,480
that with five thousand services uh it

495
00:17:30,799 --> 00:17:34,799
takes more than 10 minutes

496
00:17:32,480 --> 00:17:37,039
and if you reach like 20 000 services to

497
00:17:34,799 --> 00:17:39,600
proxy becomes completely unusable

498
00:17:37,039 --> 00:17:41,280
uh in this example here it took them

499
00:17:39,600 --> 00:17:42,320
five hours to just reload one set of

500
00:17:41,280 --> 00:17:43,840
rules

501
00:17:42,320 --> 00:17:46,159
as you can imagine on last clusters

502
00:17:43,840 --> 00:17:48,080
endpoint tend to move quite a lot and so

503
00:17:46,160 --> 00:17:48,480
you would want thing to be like updated

504
00:17:48,080 --> 00:17:50,639
in the

505
00:17:48,480 --> 00:17:53,919
in a matter of a few seconds so five

506
00:17:50,640 --> 00:17:53,919
hours is definitely too long

507
00:17:54,400 --> 00:18:00,799
and this gets us to ipvs

508
00:17:57,600 --> 00:18:03,840
so the idea of ipvs is well ipvs

509
00:18:00,799 --> 00:18:05,840
is a load balancer built into the kernel

510
00:18:03,840 --> 00:18:06,959
and so of course it's designed for load

511
00:18:05,840 --> 00:18:09,439
balancing

512
00:18:06,960 --> 00:18:10,559
and the way it works is actually pretty

513
00:18:09,440 --> 00:18:13,600
pretty logical

514
00:18:10,559 --> 00:18:17,360
for each service you have a virtual

515
00:18:13,600 --> 00:18:19,840
server in ipvs backed by real servers

516
00:18:17,360 --> 00:18:21,520
and when you initiate a connection your

517
00:18:19,840 --> 00:18:22,080
your application is just going to send

518
00:18:21,520 --> 00:18:24,720
traffic

519
00:18:22,080 --> 00:18:26,080
to uh to ipvs ipva is going to select

520
00:18:24,720 --> 00:18:26,559
the backends and traffic is going to be

521
00:18:26,080 --> 00:18:29,039
routed

522
00:18:26,559 --> 00:18:30,960
to these back-ends and i'm just

523
00:18:29,039 --> 00:18:32,000
mentioning the tvs contract because as

524
00:18:30,960 --> 00:18:34,160
you'll see later

525
00:18:32,000 --> 00:18:36,559
we've had some issues with the ipvs

526
00:18:34,160 --> 00:18:39,520
contract

527
00:18:36,559 --> 00:18:40,480
when a pod is deleted it's removed from

528
00:18:39,520 --> 00:18:42,799
the real service

529
00:18:40,480 --> 00:18:44,480
so you could you can see here that

530
00:18:42,799 --> 00:18:47,200
back-end x is not

531
00:18:44,480 --> 00:18:49,200
no longer a server for service s and

532
00:18:47,200 --> 00:18:50,640
what happens in that case in the kernel

533
00:18:49,200 --> 00:18:52,400
is trafficking to this back-end is going

534
00:18:50,640 --> 00:18:55,120
to be to be dropped

535
00:18:52,400 --> 00:18:56,960
so this is not ideal and the way we

536
00:18:55,120 --> 00:19:00,000
actually address it with ipvs

537
00:18:56,960 --> 00:19:02,240
is we use uh the cctl i put on the top

538
00:19:00,000 --> 00:19:03,039
which is going to make sure that any new

539
00:19:02,240 --> 00:19:04,640
packets

540
00:19:03,039 --> 00:19:06,559
to the access of this connection is

541
00:19:04,640 --> 00:19:08,880
going to be to to clean up

542
00:19:06,559 --> 00:19:09,760
the ipvs contract and trigger a reset to

543
00:19:08,880 --> 00:19:12,880
notify

544
00:19:09,760 --> 00:19:13,360
both back-ends so that works pretty

545
00:19:12,880 --> 00:19:15,919
great

546
00:19:13,360 --> 00:19:17,199
but it's still not ideal this means we

547
00:19:15,919 --> 00:19:19,280
don't have any kind of graceful

548
00:19:17,200 --> 00:19:22,080
termination

549
00:19:19,280 --> 00:19:24,639
so imagine you're currently connected to

550
00:19:22,080 --> 00:19:27,678
an http server and downloading data

551
00:19:24,640 --> 00:19:29,039
if uh if the pod is moved to terminating

552
00:19:27,679 --> 00:19:30,720
states

553
00:19:29,039 --> 00:19:32,080
usually what you would expect is for the

554
00:19:30,720 --> 00:19:34,559
communication to terminate

555
00:19:32,080 --> 00:19:36,399
and things to continue fine afterwards

556
00:19:34,559 --> 00:19:38,160
but in this case what you proxy was

557
00:19:36,400 --> 00:19:40,720
doing in the first ipv's implementation

558
00:19:38,160 --> 00:19:41,760
was just remove the reservoir so the

559
00:19:40,720 --> 00:19:44,799
connection would be

560
00:19:41,760 --> 00:19:46,640
abruptly abruptly cut this wasn't an

561
00:19:44,799 --> 00:19:48,879
issue with happy tables because even if

562
00:19:46,640 --> 00:19:50,080
you remove the ip table's rules

563
00:19:48,880 --> 00:19:52,080
your entry was still hitting the

564
00:19:50,080 --> 00:19:55,280
contract so traffic was still flowing

565
00:19:52,080 --> 00:19:56,159
fine after removing the back ends so

566
00:19:55,280 --> 00:19:59,600
addressing this

567
00:19:56,160 --> 00:20:01,360
took some time but we we had an

568
00:19:59,600 --> 00:20:02,000
implementation of graceful termination

569
00:20:01,360 --> 00:20:05,120
in in

570
00:20:02,000 --> 00:20:07,360
kubernetes 1.12 and and the way this

571
00:20:05,120 --> 00:20:10,479
works if you're familiar with ipvs is

572
00:20:07,360 --> 00:20:13,840
pretty pretty logical so what we do is

573
00:20:10,480 --> 00:20:15,760
when a pod is set to terminating mode

574
00:20:13,840 --> 00:20:17,280
uh we update the weight of the real

575
00:20:15,760 --> 00:20:18,879
server to zero

576
00:20:17,280 --> 00:20:20,559
so no new connection will be established

577
00:20:18,880 --> 00:20:22,080
with this back end but by established

578
00:20:20,559 --> 00:20:25,440
connection are still going to work

579
00:20:22,080 --> 00:20:25,439
so this works a lot better

580
00:20:26,000 --> 00:20:30,480
for garbage collection it's actually

581
00:20:28,240 --> 00:20:32,960
easy the way it works is

582
00:20:30,480 --> 00:20:34,000
we have a thread that's running every

583
00:20:32,960 --> 00:20:36,400
every few seconds every

584
00:20:34,000 --> 00:20:38,240
minute i think that's watching that

585
00:20:36,400 --> 00:20:38,880
looking for all the back ends for a

586
00:20:38,240 --> 00:20:40,880
given

587
00:20:38,880 --> 00:20:42,960
um all the connections for a given back

588
00:20:40,880 --> 00:20:46,159
end and when this gets to zero

589
00:20:42,960 --> 00:20:48,080
uh the back end is removed that's that's

590
00:20:46,159 --> 00:20:50,720
perfectly fine because it's removed

591
00:20:48,080 --> 00:20:51,918
as spots go down this kind of thin and

592
00:20:50,720 --> 00:20:56,320
the connection is

593
00:20:51,919 --> 00:20:58,799
is removed there's one um

594
00:20:56,320 --> 00:21:00,240
one small issue um which is what happens

595
00:20:58,799 --> 00:21:02,720
if

596
00:21:00,240 --> 00:21:04,240
the back end is actually crashing and

597
00:21:02,720 --> 00:21:05,360
you don't get a fin and the connection

598
00:21:04,240 --> 00:21:08,080
is not

599
00:21:05,360 --> 00:21:09,760
properly shut down and in that case you

600
00:21:08,080 --> 00:21:11,600
have this typical contract issue that's

601
00:21:09,760 --> 00:21:13,600
not specific to ipvs

602
00:21:11,600 --> 00:21:15,840
which is well the application is not

603
00:21:13,600 --> 00:21:17,840
going to notice that the backend is gone

604
00:21:15,840 --> 00:21:18,879
until either the entry expire in the in

605
00:21:17,840 --> 00:21:22,158
the contract

606
00:21:18,880 --> 00:21:24,240
or it's it retries sending packets until

607
00:21:22,159 --> 00:21:25,440
it detects that the connection is still

608
00:21:24,240 --> 00:21:29,280
which by default

609
00:21:25,440 --> 00:21:32,559
uh takes 15 minutes on on mostly

610
00:21:29,280 --> 00:21:34,080
most linux installations um and

611
00:21:32,559 --> 00:21:36,320
if you want if you encounter this kind

612
00:21:34,080 --> 00:21:38,960
of issues what what we recommend

613
00:21:36,320 --> 00:21:39,918
in the uh i p in the proxy pvs community

614
00:21:38,960 --> 00:21:43,360
is to lower

615
00:21:39,919 --> 00:21:44,720
tcp tries to uh to to actually detect

616
00:21:43,360 --> 00:21:47,360
that the connection is failed much

617
00:21:44,720 --> 00:21:47,360
much earlier

618
00:21:48,240 --> 00:21:52,000
ipps connection tracking has been a bit

619
00:21:51,120 --> 00:21:54,879
complicated

620
00:21:52,000 --> 00:21:56,000
uh as i mean i'm sure most of you know

621
00:21:54,880 --> 00:21:57,600
all the issues you can have with the

622
00:21:56,000 --> 00:21:59,600
contract in linux

623
00:21:57,600 --> 00:22:02,559
ipvs has its own connection tracking

624
00:21:59,600 --> 00:22:03,039
system and it's in terms of granularity

625
00:22:02,559 --> 00:22:04,720
it's

626
00:22:03,039 --> 00:22:06,320
much less granular than the standard

627
00:22:04,720 --> 00:22:07,600
contract and you can see that the

628
00:22:06,320 --> 00:22:10,559
default timeout has

629
00:22:07,600 --> 00:22:11,360
are pretty high and the main issue we've

630
00:22:10,559 --> 00:22:13,678
seen with this

631
00:22:11,360 --> 00:22:16,000
is actually the udp timeout which is set

632
00:22:13,679 --> 00:22:18,720
by default to five minutes

633
00:22:16,000 --> 00:22:20,559
and it's this was very bad especially

634
00:22:18,720 --> 00:22:23,440
for dns traffic

635
00:22:20,559 --> 00:22:24,720
because as you can imagine any dns query

636
00:22:23,440 --> 00:22:25,440
will actually get an entry in the

637
00:22:24,720 --> 00:22:28,400
contract

638
00:22:25,440 --> 00:22:30,080
and it would be kept for five minutes so

639
00:22:28,400 --> 00:22:30,720
that's one of the reasons we decided to

640
00:22:30,080 --> 00:22:32,879
disable

641
00:22:30,720 --> 00:22:34,159
graceful stagnation for udp and only do

642
00:22:32,880 --> 00:22:37,120
it for tcp

643
00:22:34,159 --> 00:22:37,679
so for udp if as soon as the backend is

644
00:22:37,120 --> 00:22:40,879
removed

645
00:22:37,679 --> 00:22:42,400
we just remove the real server it's not

646
00:22:40,880 --> 00:22:46,000
perfect but it's much better

647
00:22:42,400 --> 00:22:46,000
and it solves a lot of issues

648
00:22:47,120 --> 00:22:51,120
one of the things we've added very

649
00:22:48,799 --> 00:22:53,280
recently to the ipvs implementation

650
00:22:51,120 --> 00:22:54,639
is an easy way to set timeout in your

651
00:22:53,280 --> 00:22:56,240
configuration

652
00:22:54,640 --> 00:22:57,679
and we're probably going to change the

653
00:22:56,240 --> 00:23:00,720
default timeout soon

654
00:22:57,679 --> 00:23:02,159
but we've been very careful

655
00:23:00,720 --> 00:23:03,760
with doing that because we don't want to

656
00:23:02,159 --> 00:23:05,760
break existing installation

657
00:23:03,760 --> 00:23:07,840
one of the main thing we plan to do is

658
00:23:05,760 --> 00:23:09,200
probably to change the default udp timer

659
00:23:07,840 --> 00:23:11,840
to 30 seconds

660
00:23:09,200 --> 00:23:13,039
because the largest use case by far in

661
00:23:11,840 --> 00:23:16,158
kubernetes environments

662
00:23:13,039 --> 00:23:18,879
of udp connections is dns traffic

663
00:23:16,159 --> 00:23:21,360
and 30 second timeout is much more than

664
00:23:18,880 --> 00:23:24,000
more than enough

665
00:23:21,360 --> 00:23:25,439
um ideally what we'd want is be able to

666
00:23:24,000 --> 00:23:27,520
set the weight to zero

667
00:23:25,440 --> 00:23:29,200
when the pod enter when a backend pod

668
00:23:27,520 --> 00:23:30,879
enters terminating states

669
00:23:29,200 --> 00:23:33,440
and just remove it when the pod is

670
00:23:30,880 --> 00:23:37,840
deleted but the current endpoint api

671
00:23:33,440 --> 00:23:37,840
doesn't allow this

672
00:23:38,799 --> 00:23:42,960
so very quickly in terms of status for

673
00:23:41,200 --> 00:23:45,760
ipvs

674
00:23:42,960 --> 00:23:47,440
it works pretty well at large scale

675
00:23:45,760 --> 00:23:49,679
we've been running ipvs on a very large

676
00:23:47,440 --> 00:23:52,640
cluster for some time and it's been

677
00:23:49,679 --> 00:23:54,240
very okay for us we didn't encounter any

678
00:23:52,640 --> 00:23:55,600
of the scale issues i was mentioning

679
00:23:54,240 --> 00:23:58,000
before

680
00:23:55,600 --> 00:23:59,918
be careful though it's not 100 feature

681
00:23:58,000 --> 00:24:00,400
parity with iptables implementation

682
00:23:59,919 --> 00:24:03,600
which

683
00:24:00,400 --> 00:24:05,200
which is still the the reference one

684
00:24:03,600 --> 00:24:07,120
and it took us some times to tune all

685
00:24:05,200 --> 00:24:09,440
the ipvs parameters to make it

686
00:24:07,120 --> 00:24:11,678
to make it better but we're getting

687
00:24:09,440 --> 00:24:11,679
there

688
00:24:12,000 --> 00:24:15,279
a few things i wanted to mention that

689
00:24:13,679 --> 00:24:17,279
are common challenges

690
00:24:15,279 --> 00:24:19,840
for your proxy regardless of the

691
00:24:17,279 --> 00:24:19,840
implementation

692
00:24:20,559 --> 00:24:25,120
so the first one is the scalability of

693
00:24:23,600 --> 00:24:27,199
the control plane

694
00:24:25,120 --> 00:24:29,120
so you remember before that the

695
00:24:27,200 --> 00:24:30,640
interaction of q proxy with the

696
00:24:29,120 --> 00:24:32,399
rest of the cluster is using the

697
00:24:30,640 --> 00:24:34,240
endpoint object

698
00:24:32,400 --> 00:24:36,240
and the thing is every time there's an

699
00:24:34,240 --> 00:24:39,600
update to a service a new pod

700
00:24:36,240 --> 00:24:40,559
a pod changing uh becoming ready or not

701
00:24:39,600 --> 00:24:42,080
ready

702
00:24:40,559 --> 00:24:43,840
then the full endpoint object is

703
00:24:42,080 --> 00:24:45,678
recomputed and this

704
00:24:43,840 --> 00:24:47,279
full endpoint object is sent to all

705
00:24:45,679 --> 00:24:49,840
queue proxies

706
00:24:47,279 --> 00:24:50,880
so it's fine if your endpoint object is

707
00:24:49,840 --> 00:24:53,439
small enough

708
00:24:50,880 --> 00:24:54,880
but as your endpoint starts to get big

709
00:24:53,440 --> 00:24:57,279
if you have for instance in my example

710
00:24:54,880 --> 00:24:59,200
2000 backends

711
00:24:57,279 --> 00:25:01,200
this can lead to a lot of traffic so in

712
00:24:59,200 --> 00:25:04,880
my example here imagine you have

713
00:25:01,200 --> 00:25:07,200
a service with 2000 back-ends

714
00:25:04,880 --> 00:25:08,240
each node will receive 200 kilobytes of

715
00:25:07,200 --> 00:25:10,559
of traffic for

716
00:25:08,240 --> 00:25:11,360
any given update okay so that's quite a

717
00:25:10,559 --> 00:25:13,360
lot

718
00:25:11,360 --> 00:25:14,959
and of course the api server needs to

719
00:25:13,360 --> 00:25:18,240
send this information to

720
00:25:14,960 --> 00:25:20,640
all the nodes in the cluster which means

721
00:25:18,240 --> 00:25:22,000
well in that example about a gigabyte of

722
00:25:20,640 --> 00:25:25,679
traffic so that's

723
00:25:22,000 --> 00:25:27,520
quite a lot but worst if you're using

724
00:25:25,679 --> 00:25:28,960
if you're doing a rolling update which

725
00:25:27,520 --> 00:25:30,080
means you're going to update all your

726
00:25:28,960 --> 00:25:32,720
pods in the

727
00:25:30,080 --> 00:25:33,840
in your 2000 back-end service it's mean

728
00:25:32,720 --> 00:25:36,000
one by one

729
00:25:33,840 --> 00:25:37,360
then going to be deleted and replaced

730
00:25:36,000 --> 00:25:39,039
which means at minimum

731
00:25:37,360 --> 00:25:42,000
you're going to do what i was saying

732
00:25:39,039 --> 00:25:44,080
before 2000 time 4000 back-ends

733
00:25:42,000 --> 00:25:45,600
which is going to which which means

734
00:25:44,080 --> 00:25:46,799
you're going to send two terabytes of

735
00:25:45,600 --> 00:25:49,760
traffic and this is

736
00:25:46,799 --> 00:25:49,760
of course quite a lot

737
00:25:50,080 --> 00:25:55,918
so this has been addressed very recently

738
00:25:52,799 --> 00:25:56,400
in kubernetes by using endpoint slices

739
00:25:55,919 --> 00:25:59,440
so

740
00:25:56,400 --> 00:26:00,960
the idea is that instead of using a

741
00:25:59,440 --> 00:26:01,919
single endpoint object for large

742
00:26:00,960 --> 00:26:04,240
services

743
00:26:01,919 --> 00:26:05,679
an endpoint can actually be backed by

744
00:26:04,240 --> 00:26:08,240
multiple slices

745
00:26:05,679 --> 00:26:09,200
and the maximum size of slice is 100

746
00:26:08,240 --> 00:26:11,840
endpoints

747
00:26:09,200 --> 00:26:12,720
and so you only need to synchronize the

748
00:26:11,840 --> 00:26:14,959
slice

749
00:26:12,720 --> 00:26:17,279
where a main point change happens which

750
00:26:14,960 --> 00:26:19,520
means it's much more efficient

751
00:26:17,279 --> 00:26:22,080
so this is still in beta but this has

752
00:26:19,520 --> 00:26:25,520
been available since 117 so it's still

753
00:26:22,080 --> 00:26:28,158
pretty recent

754
00:26:25,520 --> 00:26:29,039
another very common issue uh we've seen

755
00:26:28,159 --> 00:26:31,120
with um

756
00:26:29,039 --> 00:26:32,240
q prox implementation in is the size of

757
00:26:31,120 --> 00:26:34,320
the contract

758
00:26:32,240 --> 00:26:35,279
uh all the current previous

759
00:26:34,320 --> 00:26:37,439
implementation oh

760
00:26:35,279 --> 00:26:38,880
sorry q plus implementation rely on the

761
00:26:37,440 --> 00:26:40,559
contract which means

762
00:26:38,880 --> 00:26:42,400
if you have services that get a lot of

763
00:26:40,559 --> 00:26:44,240
traffic you create a lot of connection

764
00:26:42,400 --> 00:26:47,279
entry in the contract

765
00:26:44,240 --> 00:26:49,200
it's especially bad for dns usually

766
00:26:47,279 --> 00:26:50,720
in kubernetes you have a service that's

767
00:26:49,200 --> 00:26:53,120
supplying dns to the cluster

768
00:26:50,720 --> 00:26:54,720
and it's backed by pods and of course

769
00:26:53,120 --> 00:26:58,399
for each query you then

770
00:26:54,720 --> 00:27:00,240
create entries in the contract and

771
00:26:58,400 --> 00:27:01,679
and if you run kubernetes at large scale

772
00:27:00,240 --> 00:27:03,200
what's going to happen is you're going

773
00:27:01,679 --> 00:27:06,400
to fill the contract and drop

774
00:27:03,200 --> 00:27:08,159
your dns queries which is not great

775
00:27:06,400 --> 00:27:10,480
so there are different ways to to

776
00:27:08,159 --> 00:27:13,120
address that a very common uh

777
00:27:10,480 --> 00:27:15,760
way that's becoming a standard is to use

778
00:27:13,120 --> 00:27:17,520
a node local dns cache on every node

779
00:27:15,760 --> 00:27:19,120
okay so when you do queries you would

780
00:27:17,520 --> 00:27:22,080
first hit the local cache

781
00:27:19,120 --> 00:27:23,360
and option queries we use tcp so this is

782
00:27:22,080 --> 00:27:26,720
much more efficient and this

783
00:27:23,360 --> 00:27:27,520
is what's being used in most set in most

784
00:27:26,720 --> 00:27:30,880
new setups

785
00:27:27,520 --> 00:27:31,918
today another thing is we had a very

786
00:27:30,880 --> 00:27:35,120
good

787
00:27:31,919 --> 00:27:38,159
very good surprise with canal 5.0

788
00:27:35,120 --> 00:27:38,559
in this example here you can see that we

789
00:27:38,159 --> 00:27:41,679
have

790
00:27:38,559 --> 00:27:44,080
a set of container backing

791
00:27:41,679 --> 00:27:45,200
the dns service and we were very

792
00:27:44,080 --> 00:27:46,879
surprised that the

793
00:27:45,200 --> 00:27:49,360
number of entries in the contract was

794
00:27:46,880 --> 00:27:50,159
very different we had like a bimodal

795
00:27:49,360 --> 00:27:51,918
distribution of

796
00:27:50,159 --> 00:27:53,679
number of entries in the contract for

797
00:27:51,919 --> 00:27:57,039
these nodes providing

798
00:27:53,679 --> 00:27:59,039
um gns and we discovered that

799
00:27:57,039 --> 00:28:00,799
on five auto kernel the number of

800
00:27:59,039 --> 00:28:01,360
entries in the contract was much lower

801
00:28:00,799 --> 00:28:04,158
than on

802
00:28:01,360 --> 00:28:05,199
datum 415 and the reason is these two

803
00:28:04,159 --> 00:28:08,159
commits here

804
00:28:05,200 --> 00:28:09,039
that optimize the way the kernel does uh

805
00:28:08,159 --> 00:28:12,240
contracting of

806
00:28:09,039 --> 00:28:14,000
udp in in in canada five

807
00:28:12,240 --> 00:28:15,200
and this is much better as you can see

808
00:28:14,000 --> 00:28:16,080
like the number of entries in the

809
00:28:15,200 --> 00:28:18,320
contract was divided

810
00:28:16,080 --> 00:28:19,360
by two thanks to these two comments so

811
00:28:18,320 --> 00:28:22,960
it was very good

812
00:28:19,360 --> 00:28:26,080
great news i'm almost done

813
00:28:22,960 --> 00:28:26,960
i just wanted to mention uh very recent

814
00:28:26,080 --> 00:28:29,600
features

815
00:28:26,960 --> 00:28:30,480
of qproxy the first one is a dual stack

816
00:28:29,600 --> 00:28:32,158
support

817
00:28:30,480 --> 00:28:34,000
so kubernetes has a whole is supporting

818
00:28:32,159 --> 00:28:36,559
ipv4 and ipv6

819
00:28:34,000 --> 00:28:38,399
uh in alpha since 2016 and of course

820
00:28:36,559 --> 00:28:40,559
coupe proxy is doing it too

821
00:28:38,399 --> 00:28:41,678
and the most recent change and the most

822
00:28:40,559 --> 00:28:44,240
recent feature

823
00:28:41,679 --> 00:28:46,000
is the support for topology wherein

824
00:28:44,240 --> 00:28:49,039
which will allow you to

825
00:28:46,000 --> 00:28:49,440
pre to connect in preference to local

826
00:28:49,039 --> 00:28:51,600
pods

827
00:28:49,440 --> 00:28:53,919
local meaning on the same node or in the

828
00:28:51,600 --> 00:28:55,600
same zone same data center

829
00:28:53,919 --> 00:28:58,240
and not load balance to all the other

830
00:28:55,600 --> 00:29:01,439
parts so this is still in alpha but

831
00:28:58,240 --> 00:29:04,320
pretty promising and

832
00:29:01,440 --> 00:29:04,960
well in conclusion um q proxy is working

833
00:29:04,320 --> 00:29:08,158
pretty well

834
00:29:04,960 --> 00:29:09,760
at very significant scale

835
00:29:08,159 --> 00:29:11,200
there's still a lot of effort in that

836
00:29:09,760 --> 00:29:13,360
place because as

837
00:29:11,200 --> 00:29:15,039
i mean this talk was all about the all

838
00:29:13,360 --> 00:29:16,479
the problems we're trying to start with

839
00:29:15,039 --> 00:29:18,960
with your proxy

840
00:29:16,480 --> 00:29:21,039
right we're getting there the main issue

841
00:29:18,960 --> 00:29:23,200
is iptables and ipvs

842
00:29:21,039 --> 00:29:24,480
are not great match for the service

843
00:29:23,200 --> 00:29:26,320
abstraction because

844
00:29:24,480 --> 00:29:27,760
they were not designed to do clan silent

845
00:29:26,320 --> 00:29:29,039
balancing i mean they work

846
00:29:27,760 --> 00:29:31,840
but they were definitely not designed to

847
00:29:29,039 --> 00:29:33,360
do that so we can make them work but it

848
00:29:31,840 --> 00:29:34,480
feels hacky and we've had a lot of

849
00:29:33,360 --> 00:29:37,360
issues

850
00:29:34,480 --> 00:29:38,960
so a very amazing alternative in terms

851
00:29:37,360 --> 00:29:42,240
of implementation

852
00:29:38,960 --> 00:29:44,159
is ebps based on balancing and i i

853
00:29:42,240 --> 00:29:46,000
think some of you uh were there earlier

854
00:29:44,159 --> 00:29:49,120
to see the talk by daniel bachmann

855
00:29:46,000 --> 00:29:51,279
on how they do they implement the

856
00:29:49,120 --> 00:29:54,158
service attraction in psyllium

857
00:29:51,279 --> 00:29:54,720
and this is very promising because well

858
00:29:54,159 --> 00:29:57,279
this is

859
00:29:54,720 --> 00:29:58,080
epf base was designed from the ground up

860
00:29:57,279 --> 00:30:01,039
to work

861
00:29:58,080 --> 00:30:02,000
with kubernetes and it's very efficient

862
00:30:01,039 --> 00:30:03,919
and actually

863
00:30:02,000 --> 00:30:05,840
in datadog we're actually moving to this

864
00:30:03,919 --> 00:30:09,679
implementation instead of your proxy in

865
00:30:05,840 --> 00:30:12,559
most of our clusters

866
00:30:09,679 --> 00:30:14,880
and that's it thank you very much and

867
00:30:12,559 --> 00:30:14,879
thank you

868
00:30:18,310 --> 00:30:20,879
[Applause]

869
00:30:19,600 --> 00:30:22,639
and we're unfortunately out of time

870
00:30:20,880 --> 00:30:24,240
times questions would have to be

871
00:30:22,640 --> 00:30:31,919
outside the room i stay around if you

872
00:30:24,240 --> 00:30:31,919
have questions

