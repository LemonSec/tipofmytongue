1
00:00:08,639 --> 00:00:11,599
so

2
00:00:09,920 --> 00:00:13,200
it's already been said like this talk is

3
00:00:11,599 --> 00:00:16,800
about sharding graphs and how to

4
00:00:13,200 --> 00:00:18,720
query them and because this

5
00:00:16,800 --> 00:00:20,560
uh this talk is only for 20 minutes we

6
00:00:18,720 --> 00:00:22,000
can like we can evaluate all the

7
00:00:20,560 --> 00:00:24,400
all the approaches all the possible

8
00:00:22,000 --> 00:00:26,080
approaches so we are going to focus on

9
00:00:24,400 --> 00:00:28,080
one that we sort of like found very

10
00:00:26,080 --> 00:00:29,359
interesting and feel a very good results

11
00:00:28,080 --> 00:00:32,479
and we implemented that

12
00:00:29,359 --> 00:00:36,719
and that's basically uh splitting uh

13
00:00:32,479 --> 00:00:38,398
your graph your graph database into uh

14
00:00:36,719 --> 00:00:39,680
into these general subgraphs that

15
00:00:38,399 --> 00:00:41,040
basically have no any physical

16
00:00:39,680 --> 00:00:43,280
connections between them

17
00:00:41,040 --> 00:00:45,360
and then introducing basically very

18
00:00:43,280 --> 00:00:47,280
language extensions that will

19
00:00:45,360 --> 00:00:51,360
enable you to easily work with those

20
00:00:47,280 --> 00:00:53,520
with those graphs as if it was one graph

21
00:00:51,360 --> 00:00:54,879
and to make it more interesting

22
00:00:53,520 --> 00:00:57,520
basically we are going to

23
00:00:54,879 --> 00:00:58,800
show this concept on an example and the

24
00:00:57,520 --> 00:01:02,000
example is going to be the

25
00:00:58,800 --> 00:01:02,559
ldbc social network benchmark so

26
00:01:02,000 --> 00:01:05,040
basically

27
00:01:02,559 --> 00:01:07,039
uh ld social network benchmark is a

28
00:01:05,040 --> 00:01:10,000
benchmark that emulates a

29
00:01:07,040 --> 00:01:10,400
social network and it defines the data

30
00:01:10,000 --> 00:01:13,680
set

31
00:01:10,400 --> 00:01:16,640
uh data model and query that are

32
00:01:13,680 --> 00:01:17,040
basically part of that of that benchmark

33
00:01:16,640 --> 00:01:19,280
and

34
00:01:17,040 --> 00:01:20,240
what is interesting about is that the

35
00:01:19,280 --> 00:01:22,000
data set is

36
00:01:20,240 --> 00:01:23,679
the graph is very strongly connected

37
00:01:22,000 --> 00:01:26,080
basically we have a colleague who

38
00:01:23,680 --> 00:01:26,799
is sort of an in-house expert on lbc

39
00:01:26,080 --> 00:01:28,798
benchmark

40
00:01:26,799 --> 00:01:30,320
and when we when we told him like we

41
00:01:28,799 --> 00:01:32,159
want to basically do a

42
00:01:30,320 --> 00:01:33,758
shorted version of dbc benchmark he was

43
00:01:32,159 --> 00:01:34,720
like you are crazy that that can't

44
00:01:33,759 --> 00:01:38,159
perform well

45
00:01:34,720 --> 00:01:40,560
and we sort of prove him wrong so first

46
00:01:38,159 --> 00:01:42,079
like uh before we go into the sharding

47
00:01:40,560 --> 00:01:42,880
let's show the introduction to the lbc

48
00:01:42,079 --> 00:01:45,439
model so we are

49
00:01:42,880 --> 00:01:46,079
in the picture it's a little bit blurry

50
00:01:45,439 --> 00:01:48,320
uh

51
00:01:46,079 --> 00:01:49,600
so basically it's a it's a it's a it's a

52
00:01:48,320 --> 00:01:51,520
social network so

53
00:01:49,600 --> 00:01:52,798
uh you have people and they have some

54
00:01:51,520 --> 00:01:54,000
relationship they basically have a

55
00:01:52,799 --> 00:01:56,320
relationship between them that's

56
00:01:54,000 --> 00:01:57,280
basically the core of every social

57
00:01:56,320 --> 00:02:00,398
network

58
00:01:57,280 --> 00:02:03,280
then uh the main domain uh

59
00:02:00,399 --> 00:02:04,240
workload that people do is basically

60
00:02:03,280 --> 00:02:08,160
post messages

61
00:02:04,240 --> 00:02:10,878
in forums so

62
00:02:08,160 --> 00:02:11,280
you you have you have forums date which

63
00:02:10,878 --> 00:02:14,560
has

64
00:02:11,280 --> 00:02:16,480
which have members and owner and also

65
00:02:14,560 --> 00:02:19,280
basically the main workload is that

66
00:02:16,480 --> 00:02:19,840
uh people put uh posts in those forums

67
00:02:19,280 --> 00:02:22,560
and

68
00:02:19,840 --> 00:02:24,000
uh and also they can comment on the post

69
00:02:22,560 --> 00:02:26,720
and comment on

70
00:02:24,000 --> 00:02:27,280
comments and of course each post and

71
00:02:26,720 --> 00:02:29,840
comment

72
00:02:27,280 --> 00:02:30,319
has an author or they all also can be

73
00:02:29,840 --> 00:02:32,959
liked

74
00:02:30,319 --> 00:02:32,958
by a person

75
00:02:33,360 --> 00:02:40,560
and uh uh there are uh

76
00:02:37,200 --> 00:02:41,440
each uh each forum uh each forum uh post

77
00:02:40,560 --> 00:02:43,120
or uh

78
00:02:41,440 --> 00:02:44,640
comment uh have tag basically

79
00:02:43,120 --> 00:02:48,080
representing its content

80
00:02:44,640 --> 00:02:49,119
uh tag have uh a class uh hierarchy

81
00:02:48,080 --> 00:02:52,400
basically

82
00:02:49,120 --> 00:02:54,239
in hierarchy and people can express

83
00:02:52,400 --> 00:02:55,440
interest in some topics basically saying

84
00:02:54,239 --> 00:02:59,120
i'm interested in

85
00:02:55,440 --> 00:02:59,120
topics marked by those texts

86
00:02:59,760 --> 00:03:04,000
and also people live in cities cities

87
00:03:03,040 --> 00:03:06,879
allocated in

88
00:03:04,000 --> 00:03:10,000
countries and each message meaning post

89
00:03:06,879 --> 00:03:12,079
and comment have also

90
00:03:10,000 --> 00:03:13,440
basically country where they created

91
00:03:12,080 --> 00:03:16,080
from

92
00:03:13,440 --> 00:03:16,079
linked to them

93
00:03:16,959 --> 00:03:21,920
and because like sharding as we learned

94
00:03:20,159 --> 00:03:23,519
is not just like about the data it's

95
00:03:21,920 --> 00:03:24,079
also like about the workload that you

96
00:03:23,519 --> 00:03:25,760
want to

97
00:03:24,080 --> 00:03:27,519
perform so basically you have to when

98
00:03:25,760 --> 00:03:30,640
you want to charge something efficiently

99
00:03:27,519 --> 00:03:31,120
you should look both at your uh at the

100
00:03:30,640 --> 00:03:32,480
data

101
00:03:31,120 --> 00:03:34,239
and also basically what you want to do

102
00:03:32,480 --> 00:03:35,440
with the data so just like example how a

103
00:03:34,239 --> 00:03:38,560
typical ldbc

104
00:03:35,440 --> 00:03:41,280
uh query looks like uh first

105
00:03:38,560 --> 00:03:43,360
in this example it's a query 9 from the

106
00:03:41,280 --> 00:03:46,000
interactive complex category

107
00:03:43,360 --> 00:03:46,400
and given a start person you are looking

108
00:03:46,000 --> 00:03:48,720
for

109
00:03:46,400 --> 00:03:50,319
messages created by friends of that

110
00:03:48,720 --> 00:03:52,080
person or friends of friends

111
00:03:50,319 --> 00:03:54,000
that were and the messages have some

112
00:03:52,080 --> 00:03:57,360
filtering criteria they have to be

113
00:03:54,000 --> 00:03:58,720
uh created before a given date so

114
00:03:57,360 --> 00:04:00,720
basically this query cannot transfer

115
00:03:58,720 --> 00:04:02,640
transfers quite uh

116
00:04:00,720 --> 00:04:04,959
traverses a quite big portion of that

117
00:04:02,640 --> 00:04:04,958
graph

118
00:04:05,120 --> 00:04:11,280
uh yeah so uh the interesting part

119
00:04:09,040 --> 00:04:12,840
so how we just for inspiration how we

120
00:04:11,280 --> 00:04:16,000
decided to

121
00:04:12,840 --> 00:04:17,440
chart shardis model uh

122
00:04:16,000 --> 00:04:19,040
what is interesting about the approach

123
00:04:17,440 --> 00:04:20,079
we took to sharding we basically like

124
00:04:19,040 --> 00:04:22,320
evaluated many

125
00:04:20,079 --> 00:04:24,000
many approaches and this is sort of like

126
00:04:22,320 --> 00:04:26,159
the model we came up with

127
00:04:24,000 --> 00:04:29,040
uh what is interesting about is is that

128
00:04:26,160 --> 00:04:32,479
is asymmetric basically that means that

129
00:04:29,040 --> 00:04:34,639
not all the shards hold uh hold

130
00:04:32,479 --> 00:04:37,039
basically have the same data model data

131
00:04:34,639 --> 00:04:39,199
set we have uh

132
00:04:37,040 --> 00:04:40,639
always one person chart that basically

133
00:04:39,199 --> 00:04:42,479
contains all the

134
00:04:40,639 --> 00:04:44,720
information about the people and

135
00:04:42,479 --> 00:04:47,360
basically the relationship between them

136
00:04:44,720 --> 00:04:48,880
and then we have something that the rest

137
00:04:47,360 --> 00:04:51,840
of the charts are what we called

138
00:04:48,880 --> 00:04:53,040
forum charts basically we distribute

139
00:04:51,840 --> 00:04:56,159
distribute all the

140
00:04:53,040 --> 00:04:59,199
all the forums on the remaining uh

141
00:04:56,160 --> 00:05:03,360
this is a big advantage because the

142
00:04:59,199 --> 00:05:05,440
forums are sort of a forest so

143
00:05:03,360 --> 00:05:06,560
from craft perspective and they don't

144
00:05:05,440 --> 00:05:09,199
have any

145
00:05:06,560 --> 00:05:09,759
interesting relationships between them

146
00:05:09,199 --> 00:05:11,600
so

147
00:05:09,759 --> 00:05:13,440
this is this can be done kind of easily

148
00:05:11,600 --> 00:05:16,800
for some kind using some kind of

149
00:05:13,440 --> 00:05:20,000
easy sharding functions for since modulo

150
00:05:16,800 --> 00:05:22,160
forum id what's also interesting about

151
00:05:20,000 --> 00:05:24,560
this model is that

152
00:05:22,160 --> 00:05:26,800
basically person also sort of is

153
00:05:24,560 --> 00:05:28,960
represented on those forum charts

154
00:05:26,800 --> 00:05:30,720
but they are sort of like not full

155
00:05:28,960 --> 00:05:34,080
representation of the person

156
00:05:30,720 --> 00:05:36,160
it's basically just a node with

157
00:05:34,080 --> 00:05:37,280
person id you can sort of like see it as

158
00:05:36,160 --> 00:05:40,080
a refer

159
00:05:37,280 --> 00:05:41,359
like a proxy note or a reference to that

160
00:05:40,080 --> 00:05:43,120
to that person charlotte actually

161
00:05:41,360 --> 00:05:44,400
contains like the full information about

162
00:05:43,120 --> 00:05:47,680
the given person

163
00:05:44,400 --> 00:05:50,560
and also like for efficiency some of the

164
00:05:47,680 --> 00:05:52,080
data is are replicated like across all

165
00:05:50,560 --> 00:05:54,639
shards which person's the

166
00:05:52,080 --> 00:05:55,359
uh the location structure and the tag

167
00:05:54,639 --> 00:05:57,199
structure

168
00:05:55,360 --> 00:05:58,800
which isn't such a big problem because

169
00:05:57,199 --> 00:05:59,440
the data is a very small part of the

170
00:05:58,800 --> 00:06:01,440
data set

171
00:05:59,440 --> 00:06:02,960
basically like the from the data volume

172
00:06:01,440 --> 00:06:05,120
point of view the

173
00:06:02,960 --> 00:06:08,318
biggest part are the messages and the

174
00:06:05,120 --> 00:06:08,319
forums and that structure

175
00:06:09,440 --> 00:06:14,719
and now when we have the data model

176
00:06:13,039 --> 00:06:21,840
the interesting part like how we can

177
00:06:14,720 --> 00:06:21,840
easily query that and work with that

178
00:06:27,520 --> 00:06:31,039
right so assuming we have our data now

179
00:06:30,560 --> 00:06:36,080
split

180
00:06:31,039 --> 00:06:38,479
over uh some set of disjoint sub graphs

181
00:06:36,080 --> 00:06:40,400
how could we use cipher to to query

182
00:06:38,479 --> 00:06:43,440
across those

183
00:06:40,400 --> 00:06:46,560
so we introduced two new

184
00:06:43,440 --> 00:06:51,599
constructs to cipher

185
00:06:46,560 --> 00:06:51,599
which are use and call subquery

186
00:06:52,400 --> 00:06:59,039
and use quite simply dictates

187
00:06:55,440 --> 00:07:00,960
what graph what subgraph a certain query

188
00:06:59,039 --> 00:07:04,800
part should go to

189
00:07:00,960 --> 00:07:07,280
so the match here will go to the

190
00:07:04,800 --> 00:07:09,599
graph a graph and match only from that

191
00:07:07,280 --> 00:07:09,599
graph

192
00:07:10,560 --> 00:07:17,360
and the use is allowed

193
00:07:14,880 --> 00:07:18,800
for each sort of query part so in a

194
00:07:17,360 --> 00:07:22,560
union for instance you can

195
00:07:18,800 --> 00:07:22,560
uh select two different graphs

196
00:07:24,319 --> 00:07:28,319
and the other construct is the call

197
00:07:26,560 --> 00:07:31,440
subgra subquery

198
00:07:28,319 --> 00:07:34,240
uh clause which

199
00:07:31,440 --> 00:07:35,520
is very similar to calling a procedure

200
00:07:34,240 --> 00:07:39,360
in cipher

201
00:07:35,520 --> 00:07:43,359
uh except it's an inline cipher query

202
00:07:39,360 --> 00:07:47,199
that is the body of the of the call

203
00:07:43,360 --> 00:07:51,039
and like any cipher

204
00:07:47,199 --> 00:07:53,440
clause this call block the subquery gets

205
00:07:51,039 --> 00:07:56,878
executed once per incoming row

206
00:07:53,440 --> 00:08:00,319
so in this case it's gonna get executed

207
00:07:56,879 --> 00:08:02,720
three times each for once for each

208
00:08:00,319 --> 00:08:05,039
value of x as we unwind this list of

209
00:08:02,720 --> 00:08:08,080
three values

210
00:08:05,039 --> 00:08:12,159
and the return

211
00:08:08,080 --> 00:08:15,359
values the return columns of the

212
00:08:12,160 --> 00:08:18,319
of the subquery are then exposed and

213
00:08:15,360 --> 00:08:19,199
available as new variables in the outer

214
00:08:18,319 --> 00:08:22,479
scope

215
00:08:19,199 --> 00:08:25,440
so here we return the number of movies

216
00:08:22,479 --> 00:08:27,440
and then we have access to the number of

217
00:08:25,440 --> 00:08:32,000
movies

218
00:08:27,440 --> 00:08:34,159
variable outside of the subquery

219
00:08:32,000 --> 00:08:35,360
and these can then be combined in

220
00:08:34,159 --> 00:08:38,319
interesting ways

221
00:08:35,360 --> 00:08:40,000
so we can have the use clause of course

222
00:08:38,320 --> 00:08:43,039
to dictate on what

223
00:08:40,000 --> 00:08:45,279
on what subgraph the uh the sub query

224
00:08:43,039 --> 00:08:48,480
should execute against

225
00:08:45,279 --> 00:08:50,399
so in this case we would just go to some

226
00:08:48,480 --> 00:08:52,560
graph a for the duration of this

227
00:08:50,399 --> 00:08:53,839
subquery and then we return to the outer

228
00:08:52,560 --> 00:08:57,839
context

229
00:08:53,839 --> 00:08:57,839
once it's finished executing

230
00:08:59,839 --> 00:09:04,320
and we also have support for correlative

231
00:09:03,440 --> 00:09:07,360
subqueries

232
00:09:04,320 --> 00:09:09,440
meaning that the subquery can

233
00:09:07,360 --> 00:09:11,040
access variables coming from the outer

234
00:09:09,440 --> 00:09:12,959
scope

235
00:09:11,040 --> 00:09:14,880
and we have opted for an explicit

236
00:09:12,959 --> 00:09:18,160
approach here where you need to

237
00:09:14,880 --> 00:09:20,839
specify each parameter that you want to

238
00:09:18,160 --> 00:09:23,439
each variable that you want to access

239
00:09:20,839 --> 00:09:28,240
inside so that looks like this with

240
00:09:23,440 --> 00:09:28,240
x imports the x from the outer scope

241
00:09:29,360 --> 00:09:36,720
and the most powerful

242
00:09:33,360 --> 00:09:41,680
use of call and use together is the

243
00:09:36,720 --> 00:09:41,680
dynamic use lookup so

244
00:09:41,839 --> 00:09:44,880
where you can go to a different sub

245
00:09:44,080 --> 00:09:47,519
graph

246
00:09:44,880 --> 00:09:49,360
depending on data that's coming in so

247
00:09:47,519 --> 00:09:52,080
this is again

248
00:09:49,360 --> 00:09:52,959
sort of a correlated subquery because

249
00:09:52,080 --> 00:09:55,279
you sort of

250
00:09:52,959 --> 00:09:58,560
choose what graph to execute against

251
00:09:55,279 --> 00:10:02,800
based on the value of this graph id

252
00:09:58,560 --> 00:10:04,959
variable so each execution of the

253
00:10:02,800 --> 00:10:07,760
of the inner of the subquery goes to a

254
00:10:04,959 --> 00:10:07,760
different sub graph

255
00:10:07,920 --> 00:10:10,959
right so let's look at this in the

256
00:10:10,079 --> 00:10:14,079
context of

257
00:10:10,959 --> 00:10:16,000
uh one of the ldbc queries how would the

258
00:10:14,079 --> 00:10:17,439
cipher actually look to implement one of

259
00:10:16,000 --> 00:10:20,560
these so

260
00:10:17,440 --> 00:10:24,560
this is interactive complex number six

261
00:10:20,560 --> 00:10:28,399
from the ldbc queries and

262
00:10:24,560 --> 00:10:29,839
it reads like this i've expressed it in

263
00:10:28,399 --> 00:10:32,959
some sort of

264
00:10:29,839 --> 00:10:34,720
pseudocode here so we're given a person

265
00:10:32,959 --> 00:10:37,599
and we're given a tag

266
00:10:34,720 --> 00:10:39,279
we want to find friends and friends of

267
00:10:37,600 --> 00:10:41,200
friends of this person

268
00:10:39,279 --> 00:10:44,480
then we're going to we want to find the

269
00:10:41,200 --> 00:10:48,240
posts made by these friends

270
00:10:44,480 --> 00:10:50,560
that have this tag tag and then

271
00:10:48,240 --> 00:10:51,760
the final result is basically all other

272
00:10:50,560 --> 00:10:55,518
tags of

273
00:10:51,760 --> 00:10:58,959
this set of posts that we have found

274
00:10:55,519 --> 00:10:58,959
okay so how would that look

275
00:10:59,760 --> 00:11:03,439
so we would start with the subquery that

276
00:11:01,839 --> 00:11:06,560
goes to the person chart

277
00:11:03,440 --> 00:11:08,880
uh remember we have the person's uh

278
00:11:06,560 --> 00:11:09,680
network in one chart and then we have

279
00:11:08,880 --> 00:11:13,040
sort of different

280
00:11:09,680 --> 00:11:16,560
forum shards containing the posts and

281
00:11:13,040 --> 00:11:17,040
uh the bulk of the data so we go here we

282
00:11:16,560 --> 00:11:20,479
match

283
00:11:17,040 --> 00:11:23,920
uh persons and

284
00:11:20,480 --> 00:11:26,000
friends of friends uh and then we return

285
00:11:23,920 --> 00:11:27,199
uh just a collection of the friend ids

286
00:11:26,000 --> 00:11:29,360
so now we've

287
00:11:27,200 --> 00:11:32,000
taken care of the friend of friends part

288
00:11:29,360 --> 00:11:32,000
of this query

289
00:11:32,079 --> 00:11:39,279
and then we continue by

290
00:11:36,320 --> 00:11:42,240
going to each of the shards each of the

291
00:11:39,279 --> 00:11:45,279
remaining shards the forum shards

292
00:11:42,240 --> 00:11:46,399
we import this friend ids to each of

293
00:11:45,279 --> 00:11:49,360
them

294
00:11:46,399 --> 00:11:50,399
and we basically reboot the query at

295
00:11:49,360 --> 00:11:54,639
this point

296
00:11:50,399 --> 00:11:57,760
by matching for friends

297
00:11:54,639 --> 00:12:01,760
where the id is in this friend ids

298
00:11:57,760 --> 00:12:04,880
collection so it's sort of a manual uh

299
00:12:01,760 --> 00:12:10,160
passing of data through

300
00:12:04,880 --> 00:12:10,560
to the different parts of the query this

301
00:12:10,160 --> 00:12:14,000
is

302
00:12:10,560 --> 00:12:16,000
um all right and

303
00:12:14,000 --> 00:12:17,200
at the end we do a final global

304
00:12:16,000 --> 00:12:19,920
aggregation

305
00:12:17,200 --> 00:12:21,440
so what we actually need to return is

306
00:12:19,920 --> 00:12:23,519
the

307
00:12:21,440 --> 00:12:25,920
tag name for each of these tags that we

308
00:12:23,519 --> 00:12:29,839
found and then we

309
00:12:25,920 --> 00:12:31,199
aggregate on the we aggregate the post

310
00:12:29,839 --> 00:12:33,680
counts for all of those

311
00:12:31,200 --> 00:12:35,680
so we did a local aggregation first on

312
00:12:33,680 --> 00:12:38,000
each of the shards

313
00:12:35,680 --> 00:12:40,079
and now we do another global aggregation

314
00:12:38,000 --> 00:12:44,639
of those

315
00:12:40,079 --> 00:12:44,638
already sort of summed number of posts

316
00:12:44,720 --> 00:12:48,079
and then finally we limit everything to

317
00:12:47,040 --> 00:12:53,199
20 because that's

318
00:12:48,079 --> 00:12:57,199
what the query expresses

319
00:12:53,200 --> 00:13:00,880
and using this sharding scheme

320
00:12:57,200 --> 00:13:03,839
most of the queries in the ldbc

321
00:13:00,880 --> 00:13:05,040
interactive complex set of queries can

322
00:13:03,839 --> 00:13:07,440
be expressed in

323
00:13:05,040 --> 00:13:08,800
very similar ways this is another

324
00:13:07,440 --> 00:13:12,160
example

325
00:13:08,800 --> 00:13:14,319
where the the first part is quite

326
00:13:12,160 --> 00:13:16,639
similar we again go to the person chart

327
00:13:14,320 --> 00:13:19,279
and find friends of friends

328
00:13:16,639 --> 00:13:20,079
that's a common pattern in these queries

329
00:13:19,279 --> 00:13:23,519
and we then go

330
00:13:20,079 --> 00:13:24,800
to uh the forum charts but this time

331
00:13:23,519 --> 00:13:29,040
performing some other

332
00:13:24,800 --> 00:13:32,560
uh matching and some other

333
00:13:29,040 --> 00:13:32,560
kind of aggregation

334
00:13:32,880 --> 00:13:39,920
all right uh

335
00:13:36,160 --> 00:13:42,800
that's basically all that we have uh

336
00:13:39,920 --> 00:13:43,760
for further reading uh we have a blog

337
00:13:42,800 --> 00:13:47,199
post out

338
00:13:43,760 --> 00:13:52,160
uh about our implementation of this

339
00:13:47,199 --> 00:13:54,160
inside of neo4j the language constructs

340
00:13:52,160 --> 00:13:55,600
are released under the opencypher

341
00:13:54,160 --> 00:13:58,079
project

342
00:13:55,600 --> 00:14:01,760
and those are open sourced the

343
00:13:58,079 --> 00:14:04,959
implementation the engine behind

344
00:14:01,760 --> 00:14:05,680
this sharding scheme is not yet open

345
00:14:04,959 --> 00:14:07,359
sourced

346
00:14:05,680 --> 00:14:10,880
we're hoping that parts of it are going

347
00:14:07,360 --> 00:14:10,880
to get open sourced in the future

348
00:14:12,639 --> 00:14:16,720
right and michael has asked me to say

349
00:14:16,160 --> 00:14:20,000
that

350
00:14:16,720 --> 00:14:23,920
neo4j is currently hiring engineers

351
00:14:20,000 --> 00:14:23,920
in all in all positions

352
00:14:25,440 --> 00:14:31,839
right thank you

353
00:14:42,560 --> 00:14:49,760
for the user to just fire your old query

354
00:14:47,120 --> 00:14:51,040
and the query processor knows how the

355
00:14:49,760 --> 00:14:54,639
graph is started

356
00:14:51,040 --> 00:14:58,719
and then compiles the

357
00:14:54,639 --> 00:15:01,760
simple user query in the extensive

358
00:14:58,720 --> 00:15:04,480
right so the question is couldn't this

359
00:15:01,760 --> 00:15:05,519
uh directing what shard to go to in the

360
00:15:04,480 --> 00:15:09,120
queries be

361
00:15:05,519 --> 00:15:12,079
solved automatically by uh by the system

362
00:15:09,120 --> 00:15:13,199
and yes it could and we're sort of

363
00:15:12,079 --> 00:15:16,319
exploring

364
00:15:13,199 --> 00:15:19,040
things like that but neo4j

365
00:15:16,320 --> 00:15:20,800
is currently schema-less and we're

366
00:15:19,040 --> 00:15:21,839
looking at solutions of introducing

367
00:15:20,800 --> 00:15:24,800
schemas

368
00:15:21,839 --> 00:15:25,680
that would dictate where data lives in

369
00:15:24,800 --> 00:15:28,240
shards

370
00:15:25,680 --> 00:15:31,279
and going that route to eliminate these

371
00:15:28,240 --> 00:15:41,199
sort of annotations of where to find

372
00:15:31,279 --> 00:15:42,280
your data

373
00:15:41,199 --> 00:15:45,199
i'm glad you asked

374
00:15:42,280 --> 00:15:49,040
[Laughter]

375
00:15:45,199 --> 00:15:49,519
uh right so we saw uh what else can you

376
00:15:49,040 --> 00:15:53,120
do with

377
00:15:49,519 --> 00:15:57,759
uh with the fabric uh with this sort of

378
00:15:53,120 --> 00:15:59,920
up so other use cases of course is like

379
00:15:57,759 --> 00:16:01,199
data federation where you might have a

380
00:15:59,920 --> 00:16:05,839
bunch of

381
00:16:01,199 --> 00:16:08,719
already separate databases and

382
00:16:05,839 --> 00:16:10,160
you want to run some sort of analytical

383
00:16:08,720 --> 00:16:13,519
queries

384
00:16:10,160 --> 00:16:15,519
across all of your data across your

385
00:16:13,519 --> 00:16:16,959
let's say you have a microservice

386
00:16:15,519 --> 00:16:19,040
architecture where you have a bunch of

387
00:16:16,959 --> 00:16:22,800
different

388
00:16:19,040 --> 00:16:26,319
neo4j stores that are not connected

389
00:16:22,800 --> 00:16:30,479
you might run queries like this to

390
00:16:26,320 --> 00:16:30,480
to aggregate data across all of them

391
00:16:35,040 --> 00:16:37,839
more questions

392
00:16:38,639 --> 00:16:51,839
good thanks

393
00:16:52,240 --> 00:16:54,320
you

