1
00:00:10,639 --> 00:00:14,719
unmuted

2
00:00:11,280 --> 00:00:15,838
okay so hi everyone i'm joshua mack this

3
00:00:14,719 --> 00:00:17,439
is nirmal

4
00:00:15,839 --> 00:00:19,199
and we're talking about automating

5
00:00:17,440 --> 00:00:22,400
programming and development

6
00:00:19,199 --> 00:00:25,840
of heterogeneous socs with llvm tools

7
00:00:22,400 --> 00:00:27,439
um so to give okay give a quick

8
00:00:25,840 --> 00:00:30,480
background on who we are

9
00:00:27,439 --> 00:00:32,480
um we're a group of collaboration

10
00:00:30,480 --> 00:00:35,520
between arizona state university

11
00:00:32,479 --> 00:00:36,559
university of arizona michigan carnegie

12
00:00:35,520 --> 00:00:38,000
mellon and two

13
00:00:36,559 --> 00:00:40,480
industry partners arm and general

14
00:00:38,000 --> 00:00:42,960
dynamics and the overall

15
00:00:40,480 --> 00:00:44,559
um and the the people specifically

16
00:00:42,960 --> 00:00:47,360
behind the work in this presentation

17
00:00:44,559 --> 00:00:48,879
other than myself and normal are listed

18
00:00:47,360 --> 00:00:50,800
up here because without

19
00:00:48,879 --> 00:00:52,399
the work of the team together we

20
00:00:50,800 --> 00:00:54,640
wouldn't actually have results to show

21
00:00:52,399 --> 00:00:57,120
today

22
00:00:54,640 --> 00:00:59,440
and so some background on what this

23
00:00:57,120 --> 00:01:01,280
collaboration of ours is trying to do

24
00:00:59,440 --> 00:01:04,158
is uh historically you know there's

25
00:01:01,280 --> 00:01:06,479
always a performance gap between

26
00:01:04,159 --> 00:01:08,240
um just general purpose cpus and asics

27
00:01:06,479 --> 00:01:09,760
and you can kind of span the

28
00:01:08,240 --> 00:01:10,960
the gap there with heterogeneity

29
00:01:09,760 --> 00:01:13,040
different kinds of heterogeneous

30
00:01:10,960 --> 00:01:14,640
platforms and ultimately

31
00:01:13,040 --> 00:01:16,799
all our collaboration is trying to do is

32
00:01:14,640 --> 00:01:19,759
kind of close that gap a bit

33
00:01:16,799 --> 00:01:21,040
um so we want to build something that is

34
00:01:19,759 --> 00:01:24,080
still pretty general

35
00:01:21,040 --> 00:01:24,799
like still usable easily by programmers

36
00:01:24,080 --> 00:01:26,798
but can

37
00:01:24,799 --> 00:01:29,040
help approach kind of the asic

38
00:01:26,799 --> 00:01:30,720
performance trend line

39
00:01:29,040 --> 00:01:33,680
and the way we want to do that is by

40
00:01:30,720 --> 00:01:36,158
building a heterogeneous chip

41
00:01:33,680 --> 00:01:38,159
this heterogeneous chip will have you

42
00:01:36,159 --> 00:01:41,200
know a variety of different accelerators

43
00:01:38,159 --> 00:01:43,200
standard traditional cpu cores

44
00:01:41,200 --> 00:01:45,200
and what we want to do with this chip

45
00:01:43,200 --> 00:01:46,079
specifically is we don't want it to try

46
00:01:45,200 --> 00:01:48,320
and be

47
00:01:46,079 --> 00:01:50,399
you know let's uh let's compute all the

48
00:01:48,320 --> 00:01:52,720
things we want it to be focused and

49
00:01:50,399 --> 00:01:54,000
have kind of a purpose to it so we're

50
00:01:52,720 --> 00:01:56,240
not trying to do a general purpose

51
00:01:54,000 --> 00:01:57,119
heterogeneous chip we're trying to take

52
00:01:56,240 --> 00:01:59,119
uh

53
00:01:57,119 --> 00:02:00,399
kind of a domain of applications like

54
00:01:59,119 --> 00:02:03,360
signal processing

55
00:02:00,399 --> 00:02:05,280
and see if by kind of restricting the

56
00:02:03,360 --> 00:02:07,040
set of applications you're

57
00:02:05,280 --> 00:02:08,640
using you can come up with some new

58
00:02:07,040 --> 00:02:12,080
clever optimizations

59
00:02:08,639 --> 00:02:15,839
on how you then build for that chip

60
00:02:12,080 --> 00:02:16,640
um and so where the real emphasis kind

61
00:02:15,840 --> 00:02:18,400
of is

62
00:02:16,640 --> 00:02:20,319
in this project is we want to focus on

63
00:02:18,400 --> 00:02:22,319
making the tooling as easy to use as

64
00:02:20,319 --> 00:02:24,720
possible because

65
00:02:22,319 --> 00:02:26,560
from a custom hardware point of view you

66
00:02:24,720 --> 00:02:28,879
can pretty easily build something that

67
00:02:26,560 --> 00:02:31,120
no one wants to use

68
00:02:28,879 --> 00:02:32,720
and so the real selling point is going

69
00:02:31,120 --> 00:02:34,319
to be not that you can build a chip but

70
00:02:32,720 --> 00:02:36,800
you can get

71
00:02:34,319 --> 00:02:38,799
a workflow that makes developers want to

72
00:02:36,800 --> 00:02:40,640
use it as well

73
00:02:38,800 --> 00:02:42,400
and ultimately what we would want out of

74
00:02:40,640 --> 00:02:43,200
this collaboration isn't just a single

75
00:02:42,400 --> 00:02:44,720
chip either

76
00:02:43,200 --> 00:02:47,518
we want some kind of repeatable

77
00:02:44,720 --> 00:02:50,080
methodology for given a new domain

78
00:02:47,519 --> 00:02:51,840
how do you go about finding what

79
00:02:50,080 --> 00:02:53,599
accelerators to include

80
00:02:51,840 --> 00:02:55,360
and building a tool chain that will let

81
00:02:53,599 --> 00:02:58,238
you target that

82
00:02:55,360 --> 00:02:59,920
um and so kind of traditionally you can

83
00:02:58,239 --> 00:03:01,599
think of you know computing as like a

84
00:02:59,920 --> 00:03:03,040
three stack

85
00:03:01,599 --> 00:03:04,720
cake where you have the hardware on the

86
00:03:03,040 --> 00:03:06,319
bottom and then there's some resource

87
00:03:04,720 --> 00:03:08,480
management like an operating

88
00:03:06,319 --> 00:03:10,000
system in between that provides nice

89
00:03:08,480 --> 00:03:11,440
interfaces for

90
00:03:10,000 --> 00:03:13,519
uh developer apps and then the

91
00:03:11,440 --> 00:03:16,000
applications sit on top and when you

92
00:03:13,519 --> 00:03:18,000
try to apply this to a dsoc uh you kind

93
00:03:16,000 --> 00:03:19,519
of have some questions that come up so

94
00:03:18,000 --> 00:03:21,599
in the hardware side you know what

95
00:03:19,519 --> 00:03:23,519
accelerators do you want to include it's

96
00:03:21,599 --> 00:03:24,959
a bigger question now than just how many

97
00:03:23,519 --> 00:03:26,159
cores do you want to have

98
00:03:24,959 --> 00:03:28,239
or you know what frequency are they

99
00:03:26,159 --> 00:03:31,760
going to run out what fundamental

100
00:03:28,239 --> 00:03:33,200
accelerators what pieces of your domain

101
00:03:31,760 --> 00:03:34,159
are worth accelerating in the first

102
00:03:33,200 --> 00:03:35,760
place

103
00:03:34,159 --> 00:03:37,040
um how do you launch work onto

104
00:03:35,760 --> 00:03:38,560
accelerators in some kind of

105
00:03:37,040 --> 00:03:41,040
standardized way

106
00:03:38,560 --> 00:03:42,959
because you know that this chip is going

107
00:03:41,040 --> 00:03:44,640
to need to

108
00:03:42,959 --> 00:03:46,000
handle you know a variety of

109
00:03:44,640 --> 00:03:48,079
accelerators and

110
00:03:46,000 --> 00:03:49,040
we want the interfaces for that to be

111
00:03:48,080 --> 00:03:53,040
simple

112
00:03:49,040 --> 00:03:55,040
um what exactly are we scheduling so

113
00:03:53,040 --> 00:03:56,879
with a lot of heterogeneous programming

114
00:03:55,040 --> 00:03:58,640
you might say we've optimized this for

115
00:03:56,879 --> 00:04:00,319
the gpu

116
00:03:58,640 --> 00:04:01,679
and what that does is just statically at

117
00:04:00,319 --> 00:04:04,159
compile time

118
00:04:01,680 --> 00:04:05,439
uh links in support for the gpu and it

119
00:04:04,159 --> 00:04:06,640
always runs there every time you're on

120
00:04:05,439 --> 00:04:09,680
the binary

121
00:04:06,640 --> 00:04:11,518
um but in a world where everything

122
00:04:09,680 --> 00:04:13,920
every application is heterogeneous and

123
00:04:11,519 --> 00:04:17,358
all using the resources of your chip

124
00:04:13,920 --> 00:04:19,358
you might want a smarter approach where

125
00:04:17,358 --> 00:04:20,959
you have more of the flexibility that

126
00:04:19,358 --> 00:04:21,599
you see in cpu scheduling that you can

127
00:04:20,959 --> 00:04:23,759
fall back

128
00:04:21,600 --> 00:04:25,360
on to different implementations on

129
00:04:23,759 --> 00:04:28,560
different devices

130
00:04:25,360 --> 00:04:30,160
depending on the system workload um

131
00:04:28,560 --> 00:04:31,759
and then another question that you get

132
00:04:30,160 --> 00:04:33,600
with this is how do you integrate new

133
00:04:31,759 --> 00:04:37,199
applications to this framework

134
00:04:33,600 --> 00:04:38,880
uh because as we said the the usability

135
00:04:37,199 --> 00:04:41,120
is incredibly important

136
00:04:38,880 --> 00:04:42,719
and along this same lines as usability

137
00:04:41,120 --> 00:04:44,240
worth mentioning is how do you debug

138
00:04:42,720 --> 00:04:46,320
accelerators there's no

139
00:04:44,240 --> 00:04:48,240
stack there's no instruction pointer

140
00:04:46,320 --> 00:04:50,320
there's no registers necessarily

141
00:04:48,240 --> 00:04:51,360
in a generic accelerator and so how do

142
00:04:50,320 --> 00:04:54,639
you provide

143
00:04:51,360 --> 00:04:56,560
interfaces that make that easy in this

144
00:04:54,639 --> 00:04:56,880
talk we're really only going to focus on

145
00:04:56,560 --> 00:05:00,160
just

146
00:04:56,880 --> 00:05:01,919
how do you integrate new applications um

147
00:05:00,160 --> 00:05:03,600
and but it's worth noting that we have

148
00:05:01,919 --> 00:05:05,680
people thinking about all of the

149
00:05:03,600 --> 00:05:07,919
questions on the previous slide

150
00:05:05,680 --> 00:05:09,759
um and so for the flow that we're

151
00:05:07,919 --> 00:05:13,039
presenting today

152
00:05:09,759 --> 00:05:14,320
we have a prototype tool chain that uses

153
00:05:13,039 --> 00:05:18,159
dynamic

154
00:05:14,320 --> 00:05:21,120
tracing to collect an application trace

155
00:05:18,160 --> 00:05:24,479
and then uses that to try and recognize

156
00:05:21,120 --> 00:05:26,720
relevant kernels in your code and

157
00:05:24,479 --> 00:05:28,719
perform some additional analysis that

158
00:05:26,720 --> 00:05:30,240
allows us to retarget them for different

159
00:05:28,720 --> 00:05:33,840
accelerators in the system

160
00:05:30,240 --> 00:05:36,000
without the user uh needing to intervene

161
00:05:33,840 --> 00:05:37,919
and so to step through the step uh the

162
00:05:36,000 --> 00:05:39,680
process here

163
00:05:37,919 --> 00:05:41,599
we start with there's an open source

164
00:05:39,680 --> 00:05:43,360
tool chain called trace atlas that has

165
00:05:41,600 --> 00:05:47,039
been worked on heavily by

166
00:05:43,360 --> 00:05:47,840
richard uhri at asu and what trace atlas

167
00:05:47,039 --> 00:05:51,680
does

168
00:05:47,840 --> 00:05:54,400
is it is a whole tool chain for

169
00:05:51,680 --> 00:05:56,240
collecting runtime based dynamic

170
00:05:54,400 --> 00:05:59,758
application traces

171
00:05:56,240 --> 00:06:02,479
from llvm code and so the process

172
00:05:59,759 --> 00:06:03,440
of how it works is you there's some

173
00:06:02,479 --> 00:06:06,719
libraries

174
00:06:03,440 --> 00:06:08,719
for implementing the tracing methods

175
00:06:06,720 --> 00:06:11,360
and you inject those tracing calls

176
00:06:08,720 --> 00:06:13,199
through an llvm opt pass

177
00:06:11,360 --> 00:06:15,280
you then compile the binary and run it

178
00:06:13,199 --> 00:06:16,400
and that produces a complete application

179
00:06:15,280 --> 00:06:19,599
trace

180
00:06:16,400 --> 00:06:21,359
and then um because of some dynamic

181
00:06:19,600 --> 00:06:22,560
trace compression that uses z-lib that

182
00:06:21,360 --> 00:06:24,240
we've implemented

183
00:06:22,560 --> 00:06:26,240
along with some other clever techniques

184
00:06:24,240 --> 00:06:28,560
on choosing what and what not to trace

185
00:06:26,240 --> 00:06:29,440
uh we're able to actually make this

186
00:06:28,560 --> 00:06:31,840
usable for

187
00:06:29,440 --> 00:06:34,319
a very wide range of applications and

188
00:06:31,840 --> 00:06:35,919
not just kind of trivial

189
00:06:34,319 --> 00:06:37,680
small examples without running out of

190
00:06:35,919 --> 00:06:39,599
disk space

191
00:06:37,680 --> 00:06:40,880
so like i said built on zlib and

192
00:06:39,600 --> 00:06:43,600
compared to state-of-the-art

193
00:06:40,880 --> 00:06:46,000
it's between two and a thousand x

194
00:06:43,600 --> 00:06:49,360
reduction and time to collect the trace

195
00:06:46,000 --> 00:06:50,960
and 2x reduction and on disk trace size

196
00:06:49,360 --> 00:06:52,960
um and so once you have the trace

197
00:06:50,960 --> 00:06:55,599
collected then you can analyze

198
00:06:52,960 --> 00:06:56,400
that and see not just um you know

199
00:06:55,599 --> 00:06:58,560
statically

200
00:06:56,400 --> 00:07:01,039
what the program's behavior could be but

201
00:06:58,560 --> 00:07:04,000
um actually what the behavior was

202
00:07:01,039 --> 00:07:06,240
and so uh to do that we use this concept

203
00:07:04,000 --> 00:07:07,919
of kernels which are basically just

204
00:07:06,240 --> 00:07:09,919
groups of basic blocks that are highly

205
00:07:07,919 --> 00:07:13,120
correlated and they recur very

206
00:07:09,919 --> 00:07:15,280
frequently throughout the program

207
00:07:13,120 --> 00:07:17,360
and there's this notion of kernel

208
00:07:15,280 --> 00:07:19,359
affinity which is the basically

209
00:07:17,360 --> 00:07:20,479
the transition probability between any

210
00:07:19,360 --> 00:07:23,599
two uh

211
00:07:20,479 --> 00:07:25,360
kernels in your original source program

212
00:07:23,599 --> 00:07:26,639
um and what we can do with that is we

213
00:07:25,360 --> 00:07:29,199
can because we have this

214
00:07:26,639 --> 00:07:30,400
application trace we can collect or we

215
00:07:29,199 --> 00:07:32,160
can determine

216
00:07:30,400 --> 00:07:34,479
um empirically what all of the

217
00:07:32,160 --> 00:07:35,680
affinities were in our given application

218
00:07:34,479 --> 00:07:39,199
execution

219
00:07:35,680 --> 00:07:41,199
and then we can uh cluster basically all

220
00:07:39,199 --> 00:07:44,560
of the related basic blocks together

221
00:07:41,199 --> 00:07:46,000
and then so this provides um a series of

222
00:07:44,560 --> 00:07:48,560
collections of basic blocks

223
00:07:46,000 --> 00:07:49,199
where you end up with say these blocks

224
00:07:48,560 --> 00:07:52,319
here

225
00:07:49,199 --> 00:07:53,520
recurring uh very frequently in the

226
00:07:52,319 --> 00:07:55,039
source program so

227
00:07:53,520 --> 00:07:57,440
those are related to each other and then

228
00:07:55,039 --> 00:07:58,800
these blocks recur very frequently and

229
00:07:57,440 --> 00:08:02,879
then the other blocks

230
00:07:58,800 --> 00:08:05,120
uh are not uh considered kernels

231
00:08:02,879 --> 00:08:06,960
um and what we can do from there is say

232
00:08:05,120 --> 00:08:08,639
if the original source program

233
00:08:06,960 --> 00:08:10,000
had looked something like this where

234
00:08:08,639 --> 00:08:10,639
these were the two kernels that were

235
00:08:10,000 --> 00:08:14,479
labeled

236
00:08:10,639 --> 00:08:17,039
before uh we can think of this as a

237
00:08:14,479 --> 00:08:17,520
directed acyclic graph representation

238
00:08:17,039 --> 00:08:19,919
where

239
00:08:17,520 --> 00:08:22,000
the transitions between each kernel or

240
00:08:19,919 --> 00:08:25,758
non-kernel boundary kind of gets grouped

241
00:08:22,000 --> 00:08:27,120
together into a node of llvm basic

242
00:08:25,759 --> 00:08:29,759
blocks

243
00:08:27,120 --> 00:08:30,319
um and so what we can do on top of that

244
00:08:29,759 --> 00:08:31,840
is

245
00:08:30,319 --> 00:08:33,760
because we know that the kernels are

246
00:08:31,840 --> 00:08:35,919
important sections of the code we can

247
00:08:33,760 --> 00:08:37,439
add additional analysis on that and if

248
00:08:35,919 --> 00:08:39,760
we can detect that

249
00:08:37,440 --> 00:08:40,959
say this kernel was a fast fourier

250
00:08:39,760 --> 00:08:44,240
transform

251
00:08:40,958 --> 00:08:47,040
then based on our knowledge of the

252
00:08:44,240 --> 00:08:48,640
dsoc that we're targeting we can add in

253
00:08:47,040 --> 00:08:50,800
support for other

254
00:08:48,640 --> 00:08:53,439
platform invocations automatically

255
00:08:50,800 --> 00:08:56,479
without the user having to

256
00:08:53,440 --> 00:08:58,160
define what those are and similarly if

257
00:08:56,480 --> 00:09:00,800
this is a forward error correction

258
00:08:58,160 --> 00:09:04,319
we can do a similar process and the

259
00:09:00,800 --> 00:09:07,359
result of that is that you have this

260
00:09:04,320 --> 00:09:09,120
acyclic graph where the actual

261
00:09:07,360 --> 00:09:11,920
platform that you're dispatching on

262
00:09:09,120 --> 00:09:14,640
isn't determined at compile time and so

263
00:09:11,920 --> 00:09:16,399
you build in support for any possible

264
00:09:14,640 --> 00:09:18,080
platform for each node

265
00:09:16,399 --> 00:09:20,320
into the binary and that gives you kind

266
00:09:18,080 --> 00:09:23,040
of a fat binary structure

267
00:09:20,320 --> 00:09:24,720
um and together with the dag metadata

268
00:09:23,040 --> 00:09:28,319
that is what we kind of

269
00:09:24,720 --> 00:09:31,519
output to be considered an application

270
00:09:28,320 --> 00:09:34,720
um and then to run this application

271
00:09:31,519 --> 00:09:36,560
uh you know we could try and target

272
00:09:34,720 --> 00:09:39,120
linux directly but it was more

273
00:09:36,560 --> 00:09:43,199
advantageous for us getting started to

274
00:09:39,120 --> 00:09:45,839
use a a user space based application

275
00:09:43,200 --> 00:09:46,399
runtime and so the way this ends up

276
00:09:45,839 --> 00:09:50,720
working

277
00:09:46,399 --> 00:09:53,440
is uh we essentially allocate p threads

278
00:09:50,720 --> 00:09:54,080
to account as schedulable resources in

279
00:09:53,440 --> 00:09:57,519
our

280
00:09:54,080 --> 00:09:59,680
uh dsoc system we manage a

281
00:09:57,519 --> 00:10:01,200
ready queue of tasks so you inject some

282
00:09:59,680 --> 00:10:03,920
applications and then you can

283
00:10:01,200 --> 00:10:06,240
because of the graph structure you can

284
00:10:03,920 --> 00:10:08,079
tell all the dependencies

285
00:10:06,240 --> 00:10:11,200
and then we are able to implement custom

286
00:10:08,079 --> 00:10:12,959
resource management techniques

287
00:10:11,200 --> 00:10:14,480
that involve different heterogeneous

288
00:10:12,959 --> 00:10:16,479
scheduling heuristics that

289
00:10:14,480 --> 00:10:17,519
wouldn't be present in just a standard

290
00:10:16,480 --> 00:10:21,120
linux based

291
00:10:17,519 --> 00:10:22,720
journal scheduler and so the question is

292
00:10:21,120 --> 00:10:24,880
why did we do this in user space

293
00:10:22,720 --> 00:10:26,480
because it allows us to iterate a lot

294
00:10:24,880 --> 00:10:29,040
faster while we're still

295
00:10:26,480 --> 00:10:30,800
in this early pre-silicon development

296
00:10:29,040 --> 00:10:34,399
phase

297
00:10:30,800 --> 00:10:36,479
so it lowers turnaround time and it uh

298
00:10:34,399 --> 00:10:38,480
allows for easy adding of accelerators

299
00:10:36,480 --> 00:10:40,640
because you can just

300
00:10:38,480 --> 00:10:42,480
directly use you know memory mapped

301
00:10:40,640 --> 00:10:45,120
registers or whatever interface

302
00:10:42,480 --> 00:10:46,720
that you need to to access whatever is

303
00:10:45,120 --> 00:10:48,800
in your design

304
00:10:46,720 --> 00:10:50,000
and really it just allows it allows us

305
00:10:48,800 --> 00:10:52,399
to uh

306
00:10:50,000 --> 00:10:54,320
co-evolve both all the software the

307
00:10:52,399 --> 00:10:57,040
application the developers are writing

308
00:10:54,320 --> 00:10:59,680
along with the hardware designs all in

309
00:10:57,040 --> 00:11:02,240
kind of one easy environment

310
00:10:59,680 --> 00:11:03,439
and so with that together we end up with

311
00:11:02,240 --> 00:11:05,920
this overall flow

312
00:11:03,440 --> 00:11:07,839
that captures where the left-hand side

313
00:11:05,920 --> 00:11:11,839
is the dynamic tracing analysis

314
00:11:07,839 --> 00:11:14,560
and the right-hand side is the run time

315
00:11:11,839 --> 00:11:16,320
and to give an idea of before we had the

316
00:11:14,560 --> 00:11:19,199
compilation process

317
00:11:16,320 --> 00:11:20,560
uh going we had some handwritten

318
00:11:19,200 --> 00:11:22,480
applications kind of

319
00:11:20,560 --> 00:11:24,560
going what do we want to ascend what do

320
00:11:22,480 --> 00:11:27,360
we want applications to look like

321
00:11:24,560 --> 00:11:28,800
and so this is a one of the test apps

322
00:11:27,360 --> 00:11:32,240
that we had written

323
00:11:28,800 --> 00:11:35,120
where essentially the source code on the

324
00:11:32,240 --> 00:11:37,040
left here doesn't actually contain

325
00:11:35,120 --> 00:11:38,320
any of the original knowledge about what

326
00:11:37,040 --> 00:11:40,640
the

327
00:11:38,320 --> 00:11:42,560
sequence of calls needed to be to

328
00:11:40,640 --> 00:11:46,160
recreate this original application

329
00:11:42,560 --> 00:11:48,239
it's just each section of the code

330
00:11:46,160 --> 00:11:49,600
has been segmented out into basically a

331
00:11:48,240 --> 00:11:51,920
stateless function

332
00:11:49,600 --> 00:11:52,880
with all of the arguments and state

333
00:11:51,920 --> 00:11:56,399
passed in

334
00:11:52,880 --> 00:11:57,920
as memory references um and so what we

335
00:11:56,399 --> 00:11:59,839
can do with this is we can have

336
00:11:57,920 --> 00:12:01,920
you know the different nodes in the dag

337
00:11:59,839 --> 00:12:03,200
and then if a particular node supports

338
00:12:01,920 --> 00:12:04,800
multiple platforms

339
00:12:03,200 --> 00:12:07,120
we can have multiple different functions

340
00:12:04,800 --> 00:12:08,800
that that can dispatch to

341
00:12:07,120 --> 00:12:10,639
and then this couples with just a json

342
00:12:08,800 --> 00:12:13,040
based dag representation

343
00:12:10,639 --> 00:12:14,639
that includes information about

344
00:12:13,040 --> 00:12:16,319
essentially all the memory requirements

345
00:12:14,639 --> 00:12:19,360
of the application what do variables

346
00:12:16,320 --> 00:12:20,880
need to be allocated as what

347
00:12:19,360 --> 00:12:23,360
values do they need to be initialized

348
00:12:20,880 --> 00:12:26,160
with along with the dependency structure

349
00:12:23,360 --> 00:12:28,320
and which arguments each kernel requires

350
00:12:26,160 --> 00:12:30,000
and so with that together we're able to

351
00:12:28,320 --> 00:12:32,160
run that through our system but

352
00:12:30,000 --> 00:12:33,360
ultimately we don't want to rewrite

353
00:12:32,160 --> 00:12:35,680
everything to be

354
00:12:33,360 --> 00:12:36,880
you know like that we want to just take

355
00:12:35,680 --> 00:12:39,680
a simple

356
00:12:36,880 --> 00:12:41,439
c code that you know if on is used as

357
00:12:39,680 --> 00:12:43,519
the example here only because it's

358
00:12:41,440 --> 00:12:45,040
small enough to fit on a slide and we

359
00:12:43,519 --> 00:12:47,279
want to turn that into

360
00:12:45,040 --> 00:12:48,560
something that can run in this system

361
00:12:47,279 --> 00:12:50,320
and so we're going to work through

362
00:12:48,560 --> 00:12:51,599
an example with this and for reference

363
00:12:50,320 --> 00:12:53,519
this is the output

364
00:12:51,600 --> 00:12:54,959
of what this gives if you just compile

365
00:12:53,519 --> 00:12:57,600
and run it

366
00:12:54,959 --> 00:12:59,279
um so stepping through the high level

367
00:12:57,600 --> 00:13:01,200
process from earlier

368
00:12:59,279 --> 00:13:04,320
we start by just compiling to

369
00:13:01,200 --> 00:13:05,920
intermediate llb mir

370
00:13:04,320 --> 00:13:08,160
and once we do that we renumber the

371
00:13:05,920 --> 00:13:09,439
basic blocks with a very simple opt pass

372
00:13:08,160 --> 00:13:13,040
that basically just

373
00:13:09,440 --> 00:13:16,800
allows us to um coordinate

374
00:13:13,040 --> 00:13:17,439
uh which basic blocks belong to which

375
00:13:16,800 --> 00:13:20,479
kernels

376
00:13:17,440 --> 00:13:22,320
after we've done our analysis phase

377
00:13:20,480 --> 00:13:24,160
so we see here that these basic blocks

378
00:13:22,320 --> 00:13:26,560
are labeled

379
00:13:24,160 --> 00:13:27,279
we instrument with the dynamic tracing

380
00:13:26,560 --> 00:13:30,479
calls

381
00:13:27,279 --> 00:13:32,560
so we can see here that we dump we do

382
00:13:30,480 --> 00:13:36,560
things like dump ids of basic blocks on

383
00:13:32,560 --> 00:13:36,560
basic block entry dump loads and stores

384
00:13:36,639 --> 00:13:43,199
and then that gets linked in

385
00:13:40,240 --> 00:13:44,560
in the back end so we compile the tracer

386
00:13:43,199 --> 00:13:47,839
binary

387
00:13:44,560 --> 00:13:49,518
we run that binary and running that

388
00:13:47,839 --> 00:13:51,040
binary produces the output trace of the

389
00:13:49,519 --> 00:13:52,959
application

390
00:13:51,040 --> 00:13:55,279
and what we can do then is use trace

391
00:13:52,959 --> 00:13:57,920
atlas's kernel detection mechanisms

392
00:13:55,279 --> 00:13:58,800
to detect which basic blocks in the

393
00:13:57,920 --> 00:14:01,120
original code

394
00:13:58,800 --> 00:14:03,199
are considered kernels by their

395
00:14:01,120 --> 00:14:05,040
definition as well as what the

396
00:14:03,199 --> 00:14:07,680
producer consumer relationships were

397
00:14:05,040 --> 00:14:08,639
between those various kernels so we can

398
00:14:07,680 --> 00:14:10,719
see here that

399
00:14:08,639 --> 00:14:12,399
we have two kernels basic blocks one two

400
00:14:10,720 --> 00:14:15,040
three and five six seven

401
00:14:12,399 --> 00:14:16,959
and then kernel one consumes from kernel

402
00:14:15,040 --> 00:14:19,599
zero

403
00:14:16,959 --> 00:14:20,719
um and so together with all of this

404
00:14:19,600 --> 00:14:24,079
information

405
00:14:20,720 --> 00:14:27,120
we're able to take the

406
00:14:24,079 --> 00:14:28,160
original llvmir that's been unchanged

407
00:14:27,120 --> 00:14:29,760
from the user

408
00:14:28,160 --> 00:14:31,600
and we're and we can use this to

409
00:14:29,760 --> 00:14:33,199
refactor the application to essentially

410
00:14:31,600 --> 00:14:35,279
outline each of the sections

411
00:14:33,199 --> 00:14:37,279
that need to be outlined and create a

412
00:14:35,279 --> 00:14:40,639
dag-based application

413
00:14:37,279 --> 00:14:43,360
um so in this case the uh

414
00:14:40,639 --> 00:14:44,639
the kernels are you know one two three

415
00:14:43,360 --> 00:14:46,000
and five six seven and then the

416
00:14:44,639 --> 00:14:48,720
non-kernel blocks

417
00:14:46,000 --> 00:14:50,079
in between correspond to these sections

418
00:14:48,720 --> 00:14:52,959
of the original code

419
00:14:50,079 --> 00:14:54,399
where these two nodes were essentially

420
00:14:52,959 --> 00:14:55,680
clustered as kernels because they were

421
00:14:54,399 --> 00:14:58,079
considered hot enough

422
00:14:55,680 --> 00:14:58,959
but say this for loop here wasn't and so

423
00:14:58,079 --> 00:15:02,160
it was grouped

424
00:14:58,959 --> 00:15:05,199
with all of the rest of this here

425
00:15:02,160 --> 00:15:08,399
um at the same time we are able to

426
00:15:05,199 --> 00:15:10,800
analyze the memory requirements

427
00:15:08,399 --> 00:15:12,480
in the application so we essentially

428
00:15:10,800 --> 00:15:14,399
build kind of a symbol table

429
00:15:12,480 --> 00:15:15,839
uh where we just determine okay what how

430
00:15:14,399 --> 00:15:18,480
big does each variable need to be

431
00:15:15,839 --> 00:15:19,680
if it's initialized with anything within

432
00:15:18,480 --> 00:15:21,120
a reasonable search

433
00:15:19,680 --> 00:15:22,719
distance of where the allocation

434
00:15:21,120 --> 00:15:25,839
happened can we

435
00:15:22,720 --> 00:15:28,079
uh resolve that to be constant um if

436
00:15:25,839 --> 00:15:31,680
it's a pointer can we try and resolve

437
00:15:28,079 --> 00:15:35,359
any malik calls as constant essentially

438
00:15:31,680 --> 00:15:37,920
um and from there we use uh we outline

439
00:15:35,360 --> 00:15:38,959
each of those sections of the code in

440
00:15:37,920 --> 00:15:42,240
two different

441
00:15:38,959 --> 00:15:43,920
nodes and the so the change dell vmir

442
00:15:42,240 --> 00:15:46,000
essentially looks like this where

443
00:15:43,920 --> 00:15:49,199
nothing happens in between

444
00:15:46,000 --> 00:15:52,639
um each of the node calls and

445
00:15:49,199 --> 00:15:53,439
then we can generate the uh the json

446
00:15:52,639 --> 00:15:55,920
based dag

447
00:15:53,440 --> 00:15:56,880
that calls those same nodes in the same

448
00:15:55,920 --> 00:15:58,399
sequence

449
00:15:56,880 --> 00:16:00,320
along with allocating all of the

450
00:15:58,399 --> 00:16:03,120
variables that are necessary

451
00:16:00,320 --> 00:16:04,560
and so with these two together we're

452
00:16:03,120 --> 00:16:07,360
able to compile

453
00:16:04,560 --> 00:16:09,199
this new llvm into a shared object and

454
00:16:07,360 --> 00:16:10,320
then with that shared object coupled

455
00:16:09,199 --> 00:16:13,279
with the json

456
00:16:10,320 --> 00:16:14,880
we can hand that off to our runtime and

457
00:16:13,279 --> 00:16:18,320
run it through

458
00:16:14,880 --> 00:16:19,600
the flow so just to validate that all of

459
00:16:18,320 --> 00:16:21,120
this stuff we've done has actually

460
00:16:19,600 --> 00:16:23,040
preserved the functionality

461
00:16:21,120 --> 00:16:25,440
we'll run five instances of this app

462
00:16:23,040 --> 00:16:29,279
here and we'll note that the output here

463
00:16:25,440 --> 00:16:31,600
matches the output from before

464
00:16:29,279 --> 00:16:33,120
um and so with that i'll hand it over to

465
00:16:31,600 --> 00:16:34,959
normal to explain

466
00:16:33,120 --> 00:16:37,519
how we then use this for more advanced

467
00:16:34,959 --> 00:16:40,239
applications

468
00:16:37,519 --> 00:16:40,240
thank you josh

469
00:16:41,519 --> 00:16:45,759
so as a part of this project we have

470
00:16:43,680 --> 00:16:46,399
created a user space scheduling

471
00:16:45,759 --> 00:16:48,480
framework

472
00:16:46,399 --> 00:16:50,079
to rapidly evaluate different solutions

473
00:16:48,480 --> 00:16:54,240
that we are coming up for the target

474
00:16:50,079 --> 00:16:56,079
dsoc uh so this framework is designed to

475
00:16:54,240 --> 00:16:58,000
run in the user space which makes it

476
00:16:56,079 --> 00:17:00,239
portable across different virtual and

477
00:16:58,000 --> 00:17:03,120
hardware platforms

478
00:17:00,240 --> 00:17:05,199
uh so for today's discussion i'm going

479
00:17:03,120 --> 00:17:07,599
to use

480
00:17:05,199 --> 00:17:08,880
this particular dsog data flow as a

481
00:17:07,599 --> 00:17:10,879
target dsoc

482
00:17:08,880 --> 00:17:12,559
and this particular target dsog is

483
00:17:10,880 --> 00:17:15,760
composed of a quad-core arm

484
00:17:12,559 --> 00:17:17,359
processor and two fft accelerators this

485
00:17:15,760 --> 00:17:19,119
fft accelerators

486
00:17:17,359 --> 00:17:20,958
communicates with main memory memory

487
00:17:19,119 --> 00:17:23,119
using dmips

488
00:17:20,959 --> 00:17:24,720
and this ips are mainly used for bulk

489
00:17:23,119 --> 00:17:27,678
data transfer

490
00:17:24,720 --> 00:17:30,000
and we have this arm processor which is

491
00:17:27,679 --> 00:17:34,240
used which uses memory map interface

492
00:17:30,000 --> 00:17:37,120
to configure our accelerators and ips

493
00:17:34,240 --> 00:17:38,160
so we use our framework to emulate this

494
00:17:37,120 --> 00:17:40,719
target dsoc

495
00:17:38,160 --> 00:17:42,320
on top of a real uh hardware platform

496
00:17:40,720 --> 00:17:45,840
that is vc102

497
00:17:42,320 --> 00:17:46,879
and also we demonstrate its portability

498
00:17:45,840 --> 00:17:48,959
by running the same

499
00:17:46,880 --> 00:17:50,960
platform on top of xilinx scheme which

500
00:17:48,960 --> 00:17:54,080
is a virtual platform

501
00:17:50,960 --> 00:17:57,440
so the zc102 board it consists of an

502
00:17:54,080 --> 00:18:00,399
uh zinc mp soc and it has

503
00:17:57,440 --> 00:18:02,400
an on-chip quad-core arm processor and

504
00:18:00,400 --> 00:18:05,039
also on-chip programmable logic

505
00:18:02,400 --> 00:18:07,760
we use the programmable fabric of the

506
00:18:05,039 --> 00:18:08,879
zinc soc to implement our accelerators

507
00:18:07,760 --> 00:18:11,280
and ip

508
00:18:08,880 --> 00:18:13,200
and we use the arm processor as is to

509
00:18:11,280 --> 00:18:15,760
replicate the emulation

510
00:18:13,200 --> 00:18:18,400
of this particular target d sock so

511
00:18:15,760 --> 00:18:20,320
benefit of using zc102 is that

512
00:18:18,400 --> 00:18:21,919
it helps us in getting more realistic

513
00:18:20,320 --> 00:18:23,280
performance estimate compared to a

514
00:18:21,919 --> 00:18:25,120
virtual platform

515
00:18:23,280 --> 00:18:27,120
and it also helps us in performing full

516
00:18:25,120 --> 00:18:29,039
system functional validation especially

517
00:18:27,120 --> 00:18:32,080
validating the implementations of our

518
00:18:29,039 --> 00:18:34,080
accelerators and ips uh this

519
00:18:32,080 --> 00:18:36,159
we use xilinx camo as our virtual

520
00:18:34,080 --> 00:18:38,960
platform so this is basically uh

521
00:18:36,160 --> 00:18:39,520
provided by xilinx and what it does is

522
00:18:38,960 --> 00:18:43,440
that it

523
00:18:39,520 --> 00:18:47,039
uh it basically emulates an arm v8 isa

524
00:18:43,440 --> 00:18:48,559
on top of x86 isa it runs as an indipro

525
00:18:47,039 --> 00:18:50,799
independent process in the host

526
00:18:48,559 --> 00:18:52,879
operating system and we implement our

527
00:18:50,799 --> 00:18:55,918
accelerators and ipn system c which are

528
00:18:52,880 --> 00:18:57,840
also running as an independent process

529
00:18:55,919 --> 00:18:59,600
in the host operating system these two

530
00:18:57,840 --> 00:19:01,840
processes communicate with each other

531
00:18:59,600 --> 00:19:04,080
using inter process communication

532
00:19:01,840 --> 00:19:05,760
or benefit of using xilinx scheme on top

533
00:19:04,080 --> 00:19:07,600
of real hardware is that you don't

534
00:19:05,760 --> 00:19:08,720
really need a hardware it can be used by

535
00:19:07,600 --> 00:19:10,480
multiple

536
00:19:08,720 --> 00:19:12,559
developers application developers in

537
00:19:10,480 --> 00:19:14,400
parallel

538
00:19:12,559 --> 00:19:17,360
so basically it isolates the necessity

539
00:19:14,400 --> 00:19:17,360
of real hardware

540
00:19:18,480 --> 00:19:21,919
uh so next i will uh talk about how we

541
00:19:21,120 --> 00:19:24,399
use

542
00:19:21,919 --> 00:19:25,919
our framework to validate the tool chain

543
00:19:24,400 --> 00:19:28,880
that josh introduced

544
00:19:25,919 --> 00:19:29,360
before me uh so we have the we take the

545
00:19:28,880 --> 00:19:32,480
sample

546
00:19:29,360 --> 00:19:32,879
uh radar correlator code and we this is

547
00:19:32,480 --> 00:19:34,640
this

548
00:19:32,880 --> 00:19:36,000
traditional flow of compilation and

549
00:19:34,640 --> 00:19:39,039
execution of a standard c

550
00:19:36,000 --> 00:19:40,559
code or we compile it and we create we

551
00:19:39,039 --> 00:19:43,120
generate its output which is

552
00:19:40,559 --> 00:19:43,840
the lag value and then we send this

553
00:19:43,120 --> 00:19:46,479
particular c

554
00:19:43,840 --> 00:19:48,240
code through the tool chain uh which is

555
00:19:46,480 --> 00:19:49,039
trace atlas that is being used for

556
00:19:48,240 --> 00:19:52,160
kernel

557
00:19:49,039 --> 00:19:54,000
detection the fat binary and code

558
00:19:52,160 --> 00:19:56,000
refactoring which basically takes the

559
00:19:54,000 --> 00:19:58,480
kernel identified by trace atlas

560
00:19:56,000 --> 00:19:59,440
and it creates the shared object for the

561
00:19:58,480 --> 00:20:00,799
binary and

562
00:19:59,440 --> 00:20:02,720
the dag representation of the

563
00:20:00,799 --> 00:20:04,879
application and then

564
00:20:02,720 --> 00:20:06,720
we send this particular files through

565
00:20:04,880 --> 00:20:08,480
our user space framework

566
00:20:06,720 --> 00:20:09,760
and then we compare the output of

567
00:20:08,480 --> 00:20:12,159
monolithic c

568
00:20:09,760 --> 00:20:14,320
code with the output generated by our

569
00:20:12,159 --> 00:20:15,760
user user space framework and which if

570
00:20:14,320 --> 00:20:17,039
they are equal we assume that okay the

571
00:20:15,760 --> 00:20:18,080
application integration has been

572
00:20:17,039 --> 00:20:22,000
successful

573
00:20:18,080 --> 00:20:24,960
with our user space framework uh

574
00:20:22,000 --> 00:20:26,559
so once we complete the integration of

575
00:20:24,960 --> 00:20:27,280
application with the framework we want

576
00:20:26,559 --> 00:20:29,678
to

577
00:20:27,280 --> 00:20:30,559
see how how we can use this particular

578
00:20:29,679 --> 00:20:33,120
framework

579
00:20:30,559 --> 00:20:34,879
and the applications to uh do the search

580
00:20:33,120 --> 00:20:35,840
design space exploration of the target

581
00:20:34,880 --> 00:20:37,919
dsoc

582
00:20:35,840 --> 00:20:39,280
so in order to do that uh what we do is

583
00:20:37,919 --> 00:20:41,520
that we create

584
00:20:39,280 --> 00:20:42,399
we use the real hardware platform and we

585
00:20:41,520 --> 00:20:44,158
create real

586
00:20:42,400 --> 00:20:45,520
uh workload using more realistic

587
00:20:44,159 --> 00:20:48,799
benchmarks from

588
00:20:45,520 --> 00:20:49,918
wi-fi and pulse doppler domain or radar

589
00:20:48,799 --> 00:20:51,600
domain

590
00:20:49,919 --> 00:20:53,520
and our emulation framework basically

591
00:20:51,600 --> 00:20:54,959
supports two operation mode which one is

592
00:20:53,520 --> 00:20:55,918
validation and the other one is

593
00:20:54,960 --> 00:20:57,760
performance

594
00:20:55,919 --> 00:20:59,440
in validation mode basically we can

595
00:20:57,760 --> 00:21:01,360
inject multiple instances of

596
00:20:59,440 --> 00:21:02,720
applications simultaneously

597
00:21:01,360 --> 00:21:04,479
whereas in performance mode the

598
00:21:02,720 --> 00:21:06,080
applications are injected in time

599
00:21:04,480 --> 00:21:08,960
separated manner

600
00:21:06,080 --> 00:21:10,960
the time separation can be periodic or

601
00:21:08,960 --> 00:21:13,440
random we also have a feature where

602
00:21:10,960 --> 00:21:16,799
a user can provide an input trace file

603
00:21:13,440 --> 00:21:18,640
to create the workload

604
00:21:16,799 --> 00:21:20,559
for our target dsoc we assume our

605
00:21:18,640 --> 00:21:24,159
resource pool is composed of three cpu

606
00:21:20,559 --> 00:21:24,158
cores and two accelerators

607
00:21:24,320 --> 00:21:30,000
also in this slide we use our

608
00:21:27,840 --> 00:21:31,600
platform and our tool chain to do design

609
00:21:30,000 --> 00:21:33,360
space exploration

610
00:21:31,600 --> 00:21:37,439
for different configurations and

611
00:21:33,360 --> 00:21:39,678
heuristics so on the left hand side

612
00:21:37,440 --> 00:21:42,000
what we have is we run our emulation

613
00:21:39,679 --> 00:21:45,600
space framework in validation mode

614
00:21:42,000 --> 00:21:47,039
and on the x axis we we uh iterate over

615
00:21:45,600 --> 00:21:49,520
different configurations

616
00:21:47,039 --> 00:21:50,960
by uh by changing the core count and

617
00:21:49,520 --> 00:21:53,440
accelerator count

618
00:21:50,960 --> 00:21:54,480
or the workload is created by injecting

619
00:21:53,440 --> 00:21:56,880
one instances of

620
00:21:54,480 --> 00:21:57,760
each application that we that i showed

621
00:21:56,880 --> 00:22:00,080
earlier

622
00:21:57,760 --> 00:22:01,120
and on the y-axis we have the execution

623
00:22:00,080 --> 00:22:03,840
time of the

624
00:22:01,120 --> 00:22:05,280
given workload so we basically we do the

625
00:22:03,840 --> 00:22:07,678
design space exploration

626
00:22:05,280 --> 00:22:09,120
of which configuration will suit based

627
00:22:07,679 --> 00:22:11,440
best for our target

628
00:22:09,120 --> 00:22:13,918
applications and we then narrow down on

629
00:22:11,440 --> 00:22:16,400
the on this particular configurations

630
00:22:13,919 --> 00:22:17,440
to uh depending on what the performance

631
00:22:16,400 --> 00:22:19,280
requirement and

632
00:22:17,440 --> 00:22:20,960
energy requirement we can select any of

633
00:22:19,280 --> 00:22:21,440
this config configuration for doing

634
00:22:20,960 --> 00:22:24,480
further

635
00:22:21,440 --> 00:22:26,000
analysis so for this particular plot we

636
00:22:24,480 --> 00:22:28,559
have selected a configuration which

637
00:22:26,000 --> 00:22:29,919
is composed of two cpu cores and two fft

638
00:22:28,559 --> 00:22:33,120
accelerators

639
00:22:29,919 --> 00:22:34,799
so this particular analysis has been

640
00:22:33,120 --> 00:22:36,799
done in the performance mode

641
00:22:34,799 --> 00:22:38,879
where applications are injected in

642
00:22:36,799 --> 00:22:41,039
periodic manner for 100 milliseconds

643
00:22:38,880 --> 00:22:42,000
and on the y-axis we have the execution

644
00:22:41,039 --> 00:22:44,879
time of the given

645
00:22:42,000 --> 00:22:46,240
of the generated workload trace uh and

646
00:22:44,880 --> 00:22:47,520
in this particular plot we are

647
00:22:46,240 --> 00:22:49,280
basically evaluating different

648
00:22:47,520 --> 00:22:51,760
scheduling heuristics

649
00:22:49,280 --> 00:22:52,799
so at the end of the uh this slide what

650
00:22:51,760 --> 00:22:56,000
i want to say is that

651
00:22:52,799 --> 00:22:58,720
we have a complete software stack uh

652
00:22:56,000 --> 00:23:00,000
and a scheduler framework which can be

653
00:22:58,720 --> 00:23:03,120
used for

654
00:23:00,000 --> 00:23:04,320
uh performing dsoc design space

655
00:23:03,120 --> 00:23:09,840
exploration during the

656
00:23:04,320 --> 00:23:09,840
initial phase of dsoc development

657
00:23:10,080 --> 00:23:13,520
other than that i would also like to say

658
00:23:12,000 --> 00:23:14,640
that in this particular project we have

659
00:23:13,520 --> 00:23:17,039
been trying to develop

660
00:23:14,640 --> 00:23:18,480
a ecosystem of tools for the early early

661
00:23:17,039 --> 00:23:19,760
development of dsoc

662
00:23:18,480 --> 00:23:22,480
and one of the tool that we have

663
00:23:19,760 --> 00:23:24,320
developed is a domain specific ds3

664
00:23:22,480 --> 00:23:27,440
simulation it's a discrete

665
00:23:24,320 --> 00:23:29,360
event simulation tool and it can be used

666
00:23:27,440 --> 00:23:32,000
for evaluating different scheduling

667
00:23:29,360 --> 00:23:33,918
algorithms power management policies and

668
00:23:32,000 --> 00:23:36,080
design space exploration for energy

669
00:23:33,919 --> 00:23:38,080
performance and area trade-off so

670
00:23:36,080 --> 00:23:38,399
benefit of this particular tool is that

671
00:23:38,080 --> 00:23:40,320
in

672
00:23:38,400 --> 00:23:42,320
uh during the early phase when you don't

673
00:23:40,320 --> 00:23:43,039
have applications ready for the target

674
00:23:42,320 --> 00:23:44,720
desock

675
00:23:43,039 --> 00:23:46,720
or even the accelerators are not being

676
00:23:44,720 --> 00:23:49,520
implemented for the target dsoc this

677
00:23:46,720 --> 00:23:52,000
tool can be used for doing all those

678
00:23:49,520 --> 00:23:54,720
system level design decisions

679
00:23:52,000 --> 00:23:56,640
as soon as possible and depending on the

680
00:23:54,720 --> 00:23:58,559
outcome of this we can target

681
00:23:56,640 --> 00:24:00,320
we can target uh the required

682
00:23:58,559 --> 00:24:04,080
accelerators for pre

683
00:24:00,320 --> 00:24:06,639
creating emulation platform uh

684
00:24:04,080 --> 00:24:18,639
okay then i will hand it back to josh to

685
00:24:06,640 --> 00:24:20,400
go over the demo i'm not going to trust

686
00:24:18,640 --> 00:24:23,039
the inline video to even play there

687
00:24:20,400 --> 00:24:24,799
so um what we're going to see in this

688
00:24:23,039 --> 00:24:27,200
demo is stepping through

689
00:24:24,799 --> 00:24:28,799
the full compilation flow with the radar

690
00:24:27,200 --> 00:24:29,840
correlator example that we had mentioned

691
00:24:28,799 --> 00:24:31,200
earlier

692
00:24:29,840 --> 00:24:33,199
so what we're going to do first to just

693
00:24:31,200 --> 00:24:34,799
go through the application and just

694
00:24:33,200 --> 00:24:37,200
verify that it is a

695
00:24:34,799 --> 00:24:39,360
very basic standard c application so

696
00:24:37,200 --> 00:24:42,960
there's some file io up at the top

697
00:24:39,360 --> 00:24:45,840
um there's some

698
00:24:42,960 --> 00:24:48,480
dft calculations there's some uh

699
00:24:45,840 --> 00:24:50,399
pairwise multiplication there's an idft

700
00:24:48,480 --> 00:24:52,320
there's a maximum for loop and then you

701
00:24:50,400 --> 00:24:56,080
print out the value at the end

702
00:24:52,320 --> 00:24:58,559
uh no pragmas know anything

703
00:24:56,080 --> 00:25:00,480
and then what we do is we pass it

704
00:24:58,559 --> 00:25:01,918
through the compilation flow

705
00:25:00,480 --> 00:25:03,840
uh what it's doing here is it's

706
00:25:01,919 --> 00:25:06,400
collecting the execution trace

707
00:25:03,840 --> 00:25:07,279
and uh what this also happens to give us

708
00:25:06,400 --> 00:25:10,159
is some

709
00:25:07,279 --> 00:25:11,120
uh reference points for the dft-1 and

710
00:25:10,159 --> 00:25:14,159
the dft

711
00:25:11,120 --> 00:25:18,080
ii execution as well as well as the

712
00:25:14,159 --> 00:25:19,520
uh output value of 0.2516

713
00:25:18,080 --> 00:25:21,520
and then what it's doing now is it's

714
00:25:19,520 --> 00:25:25,600
going through the kernel detection phase

715
00:25:21,520 --> 00:25:25,600
um and analyzing the trace

716
00:25:26,640 --> 00:25:29,919
as well as now extracting the producer

717
00:25:28,720 --> 00:25:31,440
consumer relationships

718
00:25:29,919 --> 00:25:33,600
there's a little bit of time dilation

719
00:25:31,440 --> 00:25:37,039
here um to save

720
00:25:33,600 --> 00:25:39,600
on uh time in the presentation but

721
00:25:37,039 --> 00:25:41,600
yeah now what it's done after that is it

722
00:25:39,600 --> 00:25:42,559
went and it refactored the application

723
00:25:41,600 --> 00:25:47,120
into

724
00:25:42,559 --> 00:25:47,120
the blocks of non-kernel and kernel code

725
00:25:48,480 --> 00:25:51,760
and as we can see you know it just

726
00:25:50,000 --> 00:25:53,520
alternates kernel non-kernel

727
00:25:51,760 --> 00:25:56,158
and then what it did was it analyzed

728
00:25:53,520 --> 00:25:58,158
each node in the graph to see if it can

729
00:25:56,159 --> 00:26:00,640
swap in an optimized implementation

730
00:25:58,159 --> 00:26:02,080
so in this case node seven and nine were

731
00:26:00,640 --> 00:26:05,360
detected as

732
00:26:02,080 --> 00:26:06,240
dft uh kernels and so we were able to

733
00:26:05,360 --> 00:26:09,120
swap in

734
00:26:06,240 --> 00:26:10,880
um an fft accelerator invocation that we

735
00:26:09,120 --> 00:26:12,399
can use instead

736
00:26:10,880 --> 00:26:14,240
and so what we're doing here is we're

737
00:26:12,400 --> 00:26:16,799
copying the output

738
00:26:14,240 --> 00:26:17,679
shared object in json to our hardware

739
00:26:16,799 --> 00:26:21,840
platform

740
00:26:17,679 --> 00:26:23,760
and then we'd run it and show that the

741
00:26:21,840 --> 00:26:26,639
modified application is now able to

742
00:26:23,760 --> 00:26:29,039
dispatch onto our hardware accelerator

743
00:26:26,640 --> 00:26:31,279
without the user intervening and we can

744
00:26:29,039 --> 00:26:33,360
see that those two dft kernels are much

745
00:26:31,279 --> 00:26:34,640
faster

746
00:26:33,360 --> 00:26:38,879
and just to show that this kind of

747
00:26:34,640 --> 00:26:41,360
scales we do 10 back to back

748
00:26:38,880 --> 00:26:42,880
and similarly they are able to dispatch

749
00:26:41,360 --> 00:26:44,639
successfully they don't

750
00:26:42,880 --> 00:26:46,159
step on each other's toes with

751
00:26:44,640 --> 00:26:47,840
multi-threading

752
00:26:46,159 --> 00:26:50,720
and all of the outputs individually

753
00:26:47,840 --> 00:26:50,720
still remain correct

754
00:26:51,520 --> 00:26:55,520
and so just to illustrate this with a

755
00:26:54,159 --> 00:26:58,720
diagram

756
00:26:55,520 --> 00:27:00,480
uh we generate a gantt chart that shows

757
00:26:58,720 --> 00:27:01,600
the activity on each core as well as

758
00:27:00,480 --> 00:27:05,039
accelerators

759
00:27:01,600 --> 00:27:05,760
and we can see that um the fft

760
00:27:05,039 --> 00:27:07,679
accelerator

761
00:27:05,760 --> 00:27:08,879
sees some activity as well as the

762
00:27:07,679 --> 00:27:11,600
existing standard

763
00:27:08,880 --> 00:27:11,600
cpu code

764
00:27:12,000 --> 00:27:15,279
um and so yeah that's the end of the

765
00:27:14,799 --> 00:27:18,960
demo

766
00:27:15,279 --> 00:27:18,960
here uh

767
00:27:19,440 --> 00:27:23,440
this back to it yeah and so i think just

768
00:27:22,080 --> 00:27:26,320
the takeaway here is that

769
00:27:23,440 --> 00:27:27,679
this was uh no human in the loop um

770
00:27:26,320 --> 00:27:30,639
standalone

771
00:27:27,679 --> 00:27:32,240
you know integration from c code to

772
00:27:30,640 --> 00:27:34,720
running on an accelerator

773
00:27:32,240 --> 00:27:35,760
uh and while it's definitely in an early

774
00:27:34,720 --> 00:27:38,399
phase um

775
00:27:35,760 --> 00:27:38,879
we're excited to see where it can go uh

776
00:27:38,399 --> 00:27:41,678
so

777
00:27:38,880 --> 00:27:42,960
in conclusions um we're pretty happy

778
00:27:41,679 --> 00:27:45,679
with what we've accomplished

779
00:27:42,960 --> 00:27:47,520
so far uh having any kind of vertically

780
00:27:45,679 --> 00:27:51,200
integrated software and hardware stack

781
00:27:47,520 --> 00:27:54,639
is a bit of a challenging task

782
00:27:51,200 --> 00:27:56,480
but for next the upcoming releases we do

783
00:27:54,640 --> 00:27:58,720
hope to have more mature

784
00:27:56,480 --> 00:28:00,480
uh system c and or rtl accelerators

785
00:27:58,720 --> 00:28:02,799
available in the

786
00:28:00,480 --> 00:28:04,080
github repository for everyone to mess

787
00:28:02,799 --> 00:28:08,000
with the current version

788
00:28:04,080 --> 00:28:10,480
now is only cpu support um and then

789
00:28:08,000 --> 00:28:12,000
uh also improve our integration of our

790
00:28:10,480 --> 00:28:15,200
compiler tool chain with

791
00:28:12,000 --> 00:28:18,960
a richer and richer application

792
00:28:15,200 --> 00:28:18,960
and with that we'll take any questions

793
00:28:19,350 --> 00:28:26,240
[Applause]

794
00:28:24,240 --> 00:28:27,520
so the uh the determination of the

795
00:28:26,240 --> 00:28:30,880
kernels is that to

796
00:28:27,520 --> 00:28:30,879
remove cycles from the

797
00:28:31,279 --> 00:28:35,600
uh so the question is to um for

798
00:28:33,679 --> 00:28:38,880
determining kernels is that to remove

799
00:28:35,600 --> 00:28:42,320
cycles from the graph uh so

800
00:28:38,880 --> 00:28:45,120
are you um questioning kind of the like

801
00:28:42,320 --> 00:28:46,960
is is the determining the boundaries of

802
00:28:45,120 --> 00:28:49,039
the start and end of some recursive

803
00:28:46,960 --> 00:28:50,000
process so that the graph doesn't cycle

804
00:28:49,039 --> 00:28:54,559
back around

805
00:28:50,000 --> 00:28:57,679
or uh yes and what happens if your

806
00:28:54,559 --> 00:29:01,120
kernel becomes parallel as well

807
00:28:57,679 --> 00:29:01,679
so the the goal is actually to have kind

808
00:29:01,120 --> 00:29:04,320
of

809
00:29:01,679 --> 00:29:05,200
parallelization of kernels because with

810
00:29:04,320 --> 00:29:07,360
um

811
00:29:05,200 --> 00:29:08,640
i guess to the second part there with

812
00:29:07,360 --> 00:29:09,840
this uh

813
00:29:08,640 --> 00:29:12,080
idea of the producer consumer

814
00:29:09,840 --> 00:29:14,240
relationships that the dag might be

815
00:29:12,080 --> 00:29:15,678
a lot uh more complicated than just a

816
00:29:14,240 --> 00:29:17,919
simple linear chain

817
00:29:15,679 --> 00:29:19,360
and the hope is that by having this

818
00:29:17,919 --> 00:29:21,679
knowledge of producer consumer

819
00:29:19,360 --> 00:29:23,279
you can say oh this kernel never

820
00:29:21,679 --> 00:29:24,320
consumes or produces anything that this

821
00:29:23,279 --> 00:29:26,240
kernel ever needs

822
00:29:24,320 --> 00:29:29,279
they both consume from some common

823
00:29:26,240 --> 00:29:32,480
ancestor and so we can schedule them

824
00:29:29,279 --> 00:29:34,080
uh simultaneously um but to your first

825
00:29:32,480 --> 00:29:35,520
question i guess the clustering of the

826
00:29:34,080 --> 00:29:37,840
kernels is

827
00:29:35,520 --> 00:29:38,639
uh essentially more so that we can

828
00:29:37,840 --> 00:29:41,279
identify

829
00:29:38,640 --> 00:29:42,880
what areas of the code are important

830
00:29:41,279 --> 00:29:45,360
we're not

831
00:29:42,880 --> 00:29:46,720
necessarily trying to eliminate cycles

832
00:29:45,360 --> 00:29:49,918
entirely

833
00:29:46,720 --> 00:29:51,279
but yeah we just know that this area of

834
00:29:49,919 --> 00:29:54,159
the code is important and

835
00:29:51,279 --> 00:29:56,480
warrants tip for further analysis

836
00:29:54,159 --> 00:30:00,080
difficult applications

837
00:29:56,480 --> 00:30:03,200
the new radio stuff yeah i'm sure

838
00:30:00,080 --> 00:30:05,600
there's someone working

839
00:30:03,200 --> 00:30:05,600
on that

840
00:30:17,279 --> 00:30:20,320
oh how do we calculate the affinity

841
00:30:18,880 --> 00:30:23,520
values okay um

842
00:30:20,320 --> 00:30:25,520
i'd actually defer you to uh the trace

843
00:30:23,520 --> 00:30:26,320
atlas paper here up on archive it goes

844
00:30:25,520 --> 00:30:29,440
into

845
00:30:26,320 --> 00:30:33,200
uh excruciating detail about all of the

846
00:30:29,440 --> 00:30:36,720
uh calculations behind the process

847
00:30:33,200 --> 00:30:36,720
um but

848
00:30:46,480 --> 00:30:50,240
so the question is how do you actually

849
00:30:48,240 --> 00:30:51,840
identify the kernel and at this stage

850
00:30:50,240 --> 00:30:53,919
the answer is yeah we're manually

851
00:30:51,840 --> 00:30:55,760
identifying them

852
00:30:53,919 --> 00:30:57,679
the hope is that there are some other

853
00:30:55,760 --> 00:31:01,279
people working on better

854
00:30:57,679 --> 00:31:03,360
more generalizable ways to do kernel

855
00:31:01,279 --> 00:31:05,120
recognition and part of the

856
00:31:03,360 --> 00:31:06,959
hypothesis is kind of maybe by

857
00:31:05,120 --> 00:31:08,158
restricting to it as a domain of

858
00:31:06,960 --> 00:31:10,480
applications rather than

859
00:31:08,159 --> 00:31:12,799
all applications you know there are

860
00:31:10,480 --> 00:31:19,840
recurring ways that people code things

861
00:31:12,799 --> 00:31:19,840
within a particular area

862
00:31:34,000 --> 00:31:38,880
yeah so david brooks's group at harvard

863
00:31:37,600 --> 00:31:39,840
has also been doing something very

864
00:31:38,880 --> 00:31:41,519
similar

865
00:31:39,840 --> 00:31:44,840
they're using the dynamic tracers but

866
00:31:41,519 --> 00:31:47,840
they're doing pattern matching on the

867
00:31:44,840 --> 00:31:47,840
tracers

868
00:31:56,640 --> 00:32:00,000
okay and the the comment here was that

869
00:31:58,799 --> 00:32:02,480
uh david brooks's group

870
00:32:00,000 --> 00:32:04,880
at harvard is taking a similar approach

871
00:32:02,480 --> 00:32:06,240
to kernel detection and probabilistic

872
00:32:04,880 --> 00:32:09,039
pattern matching

873
00:32:06,240 --> 00:32:09,039
of kernels

874
00:32:11,440 --> 00:32:15,200
so assuming you don't have a library of

875
00:32:13,519 --> 00:32:18,480
enhanced tuned optimized

876
00:32:15,200 --> 00:32:20,000
kernels i'm

877
00:32:18,480 --> 00:32:21,919
assuming there also exists some

878
00:32:20,000 --> 00:32:26,000
solutions where you start from

879
00:32:21,919 --> 00:32:28,880
almost z code and it lowers to

880
00:32:26,000 --> 00:32:30,880
a programmable accelerator maybe you can

881
00:32:28,880 --> 00:32:31,919
look at an fpga as a programmable

882
00:32:30,880 --> 00:32:35,200
accelerator

883
00:32:31,919 --> 00:32:37,120
would that still be a useful option to

884
00:32:35,200 --> 00:32:39,360
go to or would you just lose too much

885
00:32:37,120 --> 00:32:40,239
efficiency because the tools aren't as

886
00:32:39,360 --> 00:32:44,000
good as

887
00:32:40,240 --> 00:32:46,720
the hand-tubed so the question is

888
00:32:44,000 --> 00:32:47,519
um assuming you don't have a library of

889
00:32:46,720 --> 00:32:50,720
existing

890
00:32:47,519 --> 00:32:51,919
optimized kernel implementations uh and

891
00:32:50,720 --> 00:32:53,440
you still wanted to target an

892
00:32:51,919 --> 00:32:55,760
accelerator automatically is there any

893
00:32:53,440 --> 00:32:58,080
kind of process where you could just

894
00:32:55,760 --> 00:32:58,879
gradually essentially generate an

895
00:32:58,080 --> 00:33:01,918
accelerator

896
00:32:58,880 --> 00:33:03,440
that's applicable for a given kernel

897
00:33:01,919 --> 00:33:05,200
i don't know that any work has been done

898
00:33:03,440 --> 00:33:06,640
in that but i would assume that some of

899
00:33:05,200 --> 00:33:09,760
the hls tool chains

900
00:33:06,640 --> 00:33:11,200
would have a role to fill in there and i

901
00:33:09,760 --> 00:33:12,559
think it would probably be pretty

902
00:33:11,200 --> 00:33:15,519
interesting to see

903
00:33:12,559 --> 00:33:15,519
what could come out of

904
00:33:23,679 --> 00:33:39,840
no that questions then thank the

905
00:33:25,200 --> 00:33:39,840
speakers again

906
00:33:40,159 --> 00:33:42,240
you

