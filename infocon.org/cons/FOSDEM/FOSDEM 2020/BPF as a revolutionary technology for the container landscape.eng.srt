1
00:00:06,000 --> 00:00:09,679
okay

2
00:00:07,120 --> 00:00:11,599
our next talk is going to be about bpf

3
00:00:09,679 --> 00:00:13,280
as a revolutionary technology for the

4
00:00:11,599 --> 00:00:15,759
container landscape by daniel bogman

5
00:00:13,280 --> 00:00:18,799
he's one of the bpf kernel maintainers

6
00:00:15,759 --> 00:00:20,400
and get started yeah thanks a lot for

7
00:00:18,800 --> 00:00:21,600
the introduction and it's really great

8
00:00:20,400 --> 00:00:23,359
to see so many

9
00:00:21,600 --> 00:00:25,039
people here that the room is fully

10
00:00:23,359 --> 00:00:27,439
backed

11
00:00:25,039 --> 00:00:29,439
so first off i want to start with the

12
00:00:27,439 --> 00:00:30,960
container landscape

13
00:00:29,439 --> 00:00:33,280
it's i guess obvious to everyone the

14
00:00:30,960 --> 00:00:36,320
containers exist for a long time

15
00:00:33,280 --> 00:00:39,040
but like how do the production

16
00:00:36,320 --> 00:00:40,239
deployments look like and i found a nice

17
00:00:39,040 --> 00:00:41,760
usage report from

18
00:00:40,239 --> 00:00:43,279
cystic folks where they looked into

19
00:00:41,760 --> 00:00:47,280
various aspects

20
00:00:43,280 --> 00:00:49,680
i i picked out three points from there

21
00:00:47,280 --> 00:00:50,640
um one is the that i found very

22
00:00:49,680 --> 00:00:53,600
interesting and

23
00:00:50,640 --> 00:00:54,640
um as a trend and one is the uh lifetime

24
00:00:53,600 --> 00:00:55,920
of a container

25
00:00:54,640 --> 00:00:58,640
and we can see here that there's a

26
00:00:55,920 --> 00:01:01,760
continuously decreasing lifetime

27
00:00:58,640 --> 00:01:03,680
many of the containers seem to run as

28
00:01:01,760 --> 00:01:06,880
you can see here on the chart for

29
00:01:03,680 --> 00:01:09,040
10 seconds or less and

30
00:01:06,880 --> 00:01:10,000
that basically doubled over the last

31
00:01:09,040 --> 00:01:13,680
year

32
00:01:10,000 --> 00:01:15,840
and that trend seems to be increasing um

33
00:01:13,680 --> 00:01:16,720
the other thing is how many containers

34
00:01:15,840 --> 00:01:19,119
are running

35
00:01:16,720 --> 00:01:19,759
on i hope you can see the numbers good

36
00:01:19,119 --> 00:01:22,240
here

37
00:01:19,759 --> 00:01:24,000
um how many containers are running on a

38
00:01:22,240 --> 00:01:25,920
on one single node

39
00:01:24,000 --> 00:01:27,920
and that number also has doubled from

40
00:01:25,920 --> 00:01:31,360
the last year and now it seems to be

41
00:01:27,920 --> 00:01:33,040
30 as a median um

42
00:01:31,360 --> 00:01:34,799
and the last one i guess it's pretty

43
00:01:33,040 --> 00:01:36,400
obvious but what is the main

44
00:01:34,799 --> 00:01:38,400
orchestrator if you

45
00:01:36,400 --> 00:01:40,640
uh deploy containers and that's

46
00:01:38,400 --> 00:01:43,040
kubernetes so in including

47
00:01:40,640 --> 00:01:45,280
if you include openshift and ranger here

48
00:01:43,040 --> 00:01:48,479
as well then it's 89

49
00:01:45,280 --> 00:01:49,280
so that's quite vast majority and as a

50
00:01:48,479 --> 00:01:51,520
common

51
00:01:49,280 --> 00:01:52,960
denominator a common base platform for

52
00:01:51,520 --> 00:01:53,679
all of this is of course the linux

53
00:01:52,960 --> 00:01:55,759
kernel

54
00:01:53,680 --> 00:01:57,680
and it has to provide all the building

55
00:01:55,759 --> 00:01:59,439
blocks that we know

56
00:01:57,680 --> 00:02:00,799
for the isolation in terms of name

57
00:01:59,439 --> 00:02:02,960
spaces so you have

58
00:02:00,799 --> 00:02:04,560
various namespaces network namespaces

59
00:02:02,960 --> 00:02:05,919
recently time name space code merged

60
00:02:04,560 --> 00:02:08,080
into the kernel

61
00:02:05,920 --> 00:02:09,920
and also for resource management which

62
00:02:08,080 --> 00:02:11,440
is done through the c groups

63
00:02:09,919 --> 00:02:13,599
um it has to provide the network

64
00:02:11,440 --> 00:02:15,599
connectivity and in case of the network

65
00:02:13,599 --> 00:02:18,640
namespaces it's either to reef devices

66
00:02:15,599 --> 00:02:21,440
ipv line devices or various others

67
00:02:18,640 --> 00:02:22,079
and security policies that you can

68
00:02:21,440 --> 00:02:24,079
implement

69
00:02:22,080 --> 00:02:25,680
and that can be done in terms of linux

70
00:02:24,080 --> 00:02:28,080
security modules or

71
00:02:25,680 --> 00:02:28,800
firewall policies and so on and so forth

72
00:02:28,080 --> 00:02:32,959
and all of this

73
00:02:28,800 --> 00:02:35,920
also has to withstand all the um

74
00:02:32,959 --> 00:02:36,959
increasing scalability needs and the

75
00:02:35,920 --> 00:02:40,559
higher and higher

76
00:02:36,959 --> 00:02:43,680
churn frequencies of containers while

77
00:02:40,560 --> 00:02:45,760
also coping with the subsystems

78
00:02:43,680 --> 00:02:47,360
that exist for a very long time in the

79
00:02:45,760 --> 00:02:49,920
kernel for 10 years or more and that

80
00:02:47,360 --> 00:02:52,800
have been designed back then

81
00:02:49,920 --> 00:02:54,399
and of course the newest column has the

82
00:02:52,800 --> 00:02:57,040
paradigm of never break

83
00:02:54,400 --> 00:02:58,080
user space so all of this has to still

84
00:02:57,040 --> 00:03:01,120
keep working

85
00:02:58,080 --> 00:03:03,680
while you optimize all of that and

86
00:03:01,120 --> 00:03:04,720
just to pick out the two examples in

87
00:03:03,680 --> 00:03:08,080
networking

88
00:03:04,720 --> 00:03:10,959
so there's tc and iptables net filter

89
00:03:08,080 --> 00:03:12,159
they both have been designed long ago

90
00:03:10,959 --> 00:03:14,720
and

91
00:03:12,159 --> 00:03:16,239
they're both generally extensible but at

92
00:03:14,720 --> 00:03:18,640
the same time

93
00:03:16,239 --> 00:03:19,680
their framework is inflexible for

94
00:03:18,640 --> 00:03:23,200
today's needs

95
00:03:19,680 --> 00:03:25,519
because in case of tc and

96
00:03:23,200 --> 00:03:27,679
iptables like their the whole processing

97
00:03:25,519 --> 00:03:29,200
pipeline is part of the api contract

98
00:03:27,680 --> 00:03:31,840
with user space

99
00:03:29,200 --> 00:03:33,040
for example tc has classifiers and

100
00:03:31,840 --> 00:03:35,760
actions

101
00:03:33,040 --> 00:03:36,400
um you can load them and you really have

102
00:03:35,760 --> 00:03:39,760
to

103
00:03:36,400 --> 00:03:41,440
uh specify how to package this process

104
00:03:39,760 --> 00:03:43,599
if it doesn't hit this classifier then

105
00:03:41,440 --> 00:03:45,280
you create another classifier instance

106
00:03:43,599 --> 00:03:46,720
and if it matches this then you have to

107
00:03:45,280 --> 00:03:50,799
go through this action and

108
00:03:46,720 --> 00:03:52,799
similarly with iptables you basically um

109
00:03:50,799 --> 00:03:54,799
in the worst case end up traversing like

110
00:03:52,799 --> 00:03:58,080
a linear list of rules

111
00:03:54,799 --> 00:03:59,439
that you have and yeah so if it's

112
00:03:58,080 --> 00:04:01,599
getting really complex

113
00:03:59,439 --> 00:04:03,439
then basically it slows down your fast

114
00:04:01,599 --> 00:04:04,720
path because it's sitting right in the

115
00:04:03,439 --> 00:04:07,519
middle of it

116
00:04:04,720 --> 00:04:08,400
and system software today basically has

117
00:04:07,519 --> 00:04:10,080
to

118
00:04:08,400 --> 00:04:11,840
support the wide range of kernels

119
00:04:10,080 --> 00:04:14,560
because people in production are

120
00:04:11,840 --> 00:04:16,639
running old kernels or the latest

121
00:04:14,560 --> 00:04:18,000
kernels ideally

122
00:04:16,639 --> 00:04:20,320
we would love that everybody is running

123
00:04:18,000 --> 00:04:22,320
the latest stable but that's like

124
00:04:20,320 --> 00:04:23,919
not always the case and then basically

125
00:04:22,320 --> 00:04:27,360
you end up

126
00:04:23,919 --> 00:04:29,120
baking policy in like deeply into your

127
00:04:27,360 --> 00:04:30,560
code base and there's significant effort

128
00:04:29,120 --> 00:04:32,320
if you want to rewrite it

129
00:04:30,560 --> 00:04:34,000
um just a random pick and loop network

130
00:04:32,320 --> 00:04:36,560
it basically encodes

131
00:04:34,000 --> 00:04:38,320
um the arguments for later on shelling

132
00:04:36,560 --> 00:04:41,280
out to ap tables

133
00:04:38,320 --> 00:04:43,040
and that's not really great and i found

134
00:04:41,280 --> 00:04:46,159
this x uh kcd

135
00:04:43,040 --> 00:04:47,360
uh where it says here to itself to the

136
00:04:46,160 --> 00:04:48,720
future self

137
00:04:47,360 --> 00:04:51,280
if you want to fix it you have to

138
00:04:48,720 --> 00:04:54,960
rewrite it and it's kind of creepy

139
00:04:51,280 --> 00:04:56,799
but yeah not only uh lib network but

140
00:04:54,960 --> 00:04:59,680
also kubernetes relies a lot

141
00:04:56,800 --> 00:05:01,680
on iptables specifically for its service

142
00:04:59,680 --> 00:05:05,840
implementation

143
00:05:01,680 --> 00:05:09,759
through cue proxy and there are also

144
00:05:05,840 --> 00:05:12,239
issues in terms in terms of scalability

145
00:05:09,759 --> 00:05:14,240
so some of them are for example low and

146
00:05:12,240 --> 00:05:16,240
unpredictable packet latency

147
00:05:14,240 --> 00:05:18,320
because if you have many different

148
00:05:16,240 --> 00:05:21,120
services

149
00:05:18,320 --> 00:05:22,960
and the way that ip doubles is basically

150
00:05:21,120 --> 00:05:24,720
programmed and traversed

151
00:05:22,960 --> 00:05:26,159
you end up either hitting the first rule

152
00:05:24,720 --> 00:05:28,320
which is really the best case or you

153
00:05:26,160 --> 00:05:30,240
have to traverse all the different rules

154
00:05:28,320 --> 00:05:32,240
and then you end up at the last one and

155
00:05:30,240 --> 00:05:36,639
that's not really

156
00:05:32,240 --> 00:05:38,080
a nice outcome and

157
00:05:36,639 --> 00:05:40,400
there are slow update times because you

158
00:05:38,080 --> 00:05:42,560
have to replace the entire blob of rules

159
00:05:40,400 --> 00:05:43,440
and this can really take a long a long

160
00:05:42,560 --> 00:05:46,400
time if

161
00:05:43,440 --> 00:05:47,600
the kernel has to pause and make sense

162
00:05:46,400 --> 00:05:50,479
of it and

163
00:05:47,600 --> 00:05:51,520
basically install the rules into the

164
00:05:50,479 --> 00:05:53,359
data path

165
00:05:51,520 --> 00:05:54,719
there are also reliability issues i will

166
00:05:53,360 --> 00:05:56,160
get to that in a in a bit

167
00:05:54,720 --> 00:05:58,240
and in flexibility because what i

168
00:05:56,160 --> 00:06:00,319
mentioned in terms of the contract with

169
00:05:58,240 --> 00:06:02,400
user space

170
00:06:00,319 --> 00:06:03,360
um in terms of performance even when you

171
00:06:02,400 --> 00:06:06,159
do like

172
00:06:03,360 --> 00:06:07,280
basic measurements um from host to host

173
00:06:06,160 --> 00:06:09,600
you will see that

174
00:06:07,280 --> 00:06:10,638
in a lot of cases even with just very

175
00:06:09,600 --> 00:06:13,440
few rules

176
00:06:10,639 --> 00:06:14,960
um in the kernel if you run birth top to

177
00:06:13,440 --> 00:06:15,600
see what are the heaviest hitters in

178
00:06:14,960 --> 00:06:17,840
terms of

179
00:06:15,600 --> 00:06:18,880
cpu cycles that are consumed that is

180
00:06:17,840 --> 00:06:21,840
often case

181
00:06:18,880 --> 00:06:21,840
ip tables

182
00:06:22,080 --> 00:06:25,520
and there are also other issues so

183
00:06:24,639 --> 00:06:27,919
probably

184
00:06:25,520 --> 00:06:28,639
many of you that run kubernetes in

185
00:06:27,919 --> 00:06:30,960
production

186
00:06:28,639 --> 00:06:32,800
might have hit this bug where you've got

187
00:06:30,960 --> 00:06:34,880
basically dns delays of

188
00:06:32,800 --> 00:06:37,039
five seconds or more because of a

189
00:06:34,880 --> 00:06:39,919
connection tracking race

190
00:06:37,039 --> 00:06:41,440
my colleague martinez he submitted

191
00:06:39,919 --> 00:06:43,758
actually a patch

192
00:06:41,440 --> 00:06:44,880
in august 2018 but the patch could only

193
00:06:43,759 --> 00:06:47,840
merge like

194
00:06:44,880 --> 00:06:49,360
around half a year later and he even did

195
00:06:47,840 --> 00:06:51,440
some research like the first occurrence

196
00:06:49,360 --> 00:06:53,440
in the bug was actually in 2010

197
00:06:51,440 --> 00:06:54,960
uh that people were hitting it um it's

198
00:06:53,440 --> 00:06:56,880
fixed now which is really good

199
00:06:54,960 --> 00:06:58,638
but like until it really goes down all

200
00:06:56,880 --> 00:07:00,960
the way downstream to

201
00:06:58,639 --> 00:07:02,240
users in production it takes a lot of

202
00:07:00,960 --> 00:07:04,479
time actually

203
00:07:02,240 --> 00:07:06,240
there also some other issues along the

204
00:07:04,479 --> 00:07:08,560
way uh for example i

205
00:07:06,240 --> 00:07:10,240
copied here the documentation from

206
00:07:08,560 --> 00:07:12,240
kubernetes for cube admin

207
00:07:10,240 --> 00:07:14,720
so now iptables is being replaced with

208
00:07:12,240 --> 00:07:17,199
nf tables but they're basically

209
00:07:14,720 --> 00:07:19,280
um issues with the don't break uses base

210
00:07:17,199 --> 00:07:20,400
part where they recommend that you

211
00:07:19,280 --> 00:07:23,840
should

212
00:07:20,400 --> 00:07:25,520
um yeah basically run the old

213
00:07:23,840 --> 00:07:27,599
iptables binary and don't upgrade

214
00:07:25,520 --> 00:07:31,359
because rules are getting installed

215
00:07:27,599 --> 00:07:33,039
twice which breaks cube proxy

216
00:07:31,360 --> 00:07:35,360
in terms of debug ability if you only

217
00:07:33,039 --> 00:07:37,680
have a few rules that's good

218
00:07:35,360 --> 00:07:39,680
i think that's fairly easy to follow you

219
00:07:37,680 --> 00:07:42,800
can also

220
00:07:39,680 --> 00:07:44,639
see like the counters if you write ip

221
00:07:42,800 --> 00:07:46,800
double safe dash c

222
00:07:44,639 --> 00:07:47,759
so it's easy to debug but if you have a

223
00:07:46,800 --> 00:07:51,039
lot of rules

224
00:07:47,759 --> 00:07:54,080
for example here in the left side uh

225
00:07:51,039 --> 00:07:55,199
i we deployed basically 100 services

226
00:07:54,080 --> 00:07:57,199
only

227
00:07:55,199 --> 00:07:59,039
all with the same back end but that

228
00:07:57,199 --> 00:07:59,840
already gets complex to follow like

229
00:07:59,039 --> 00:08:01,919
where my

230
00:07:59,840 --> 00:08:03,359
packet is dropped right and the other

231
00:08:01,919 --> 00:08:06,639
thing is also that

232
00:08:03,360 --> 00:08:09,199
um gproxy periodically resets the

233
00:08:06,639 --> 00:08:11,599
counters in ip tables so yeah good luck

234
00:08:09,199 --> 00:08:15,440
with the backing it's

235
00:08:11,599 --> 00:08:18,479
um to show you the basic packet flow

236
00:08:15,440 --> 00:08:19,919
um basically you have an skb at the

237
00:08:18,479 --> 00:08:22,400
beginning it hits the tc

238
00:08:19,919 --> 00:08:23,440
ingress path and then later on all the

239
00:08:22,400 --> 00:08:26,080
yellow boxes

240
00:08:23,440 --> 00:08:27,599
are net filter related you do a flip

241
00:08:26,080 --> 00:08:29,520
lookup and then if you want to

242
00:08:27,599 --> 00:08:32,080
push the packet back out the kernel you

243
00:08:29,520 --> 00:08:35,760
hit the tc egress path and then

244
00:08:32,080 --> 00:08:37,360
you send it to the device and

245
00:08:35,760 --> 00:08:40,240
just to show you an example how cute

246
00:08:37,360 --> 00:08:42,080
proxy installs the rules with iptable so

247
00:08:40,240 --> 00:08:44,320
that you have a clearer picture of it

248
00:08:42,080 --> 00:08:45,839
if you for example create a server's

249
00:08:44,320 --> 00:08:46,720
engine x in this case which has a

250
00:08:45,839 --> 00:08:49,839
virtual

251
00:08:46,720 --> 00:08:50,560
cluster ip of 3g3 and it has two

252
00:08:49,839 --> 00:08:52,880
endpoints

253
00:08:50,560 --> 00:08:54,719
then basically how it looks like in ip

254
00:08:52,880 --> 00:08:56,480
tables

255
00:08:54,720 --> 00:08:59,519
that's the way how cute proxy installs

256
00:08:56,480 --> 00:09:00,880
it it basically first creates a

257
00:08:59,519 --> 00:09:03,279
rule in the net table for the

258
00:09:00,880 --> 00:09:03,920
pre-routing where you redirect all the

259
00:09:03,279 --> 00:09:06,800
new

260
00:09:03,920 --> 00:09:08,479
traffic to the cube services chain and

261
00:09:06,800 --> 00:09:09,279
then basically there you you will check

262
00:09:08,480 --> 00:09:12,080
for the

263
00:09:09,279 --> 00:09:14,640
virtual ip and if that's happening and

264
00:09:12,080 --> 00:09:17,040
if that matches you basically go to the

265
00:09:14,640 --> 00:09:18,080
cube service engine x and there you have

266
00:09:17,040 --> 00:09:21,199
like a random

267
00:09:18,080 --> 00:09:23,279
selector with the probability of two

268
00:09:21,200 --> 00:09:25,360
like because you have two back ends and

269
00:09:23,279 --> 00:09:27,279
then you either select

270
00:09:25,360 --> 00:09:28,480
the first one or the second and then

271
00:09:27,279 --> 00:09:31,040
basically packet

272
00:09:28,480 --> 00:09:32,240
goes on right and if you have many many

273
00:09:31,040 --> 00:09:34,079
rules

274
00:09:32,240 --> 00:09:36,480
you basically have to first match on all

275
00:09:34,080 --> 00:09:39,839
of the service ips before you

276
00:09:36,480 --> 00:09:41,440
end up at the right one but you can

277
00:09:39,839 --> 00:09:42,160
actually optimize this path by only

278
00:09:41,440 --> 00:09:45,680
going from

279
00:09:42,160 --> 00:09:46,640
dc ingress to tc egress with the help of

280
00:09:45,680 --> 00:09:48,880
ebpf

281
00:09:46,640 --> 00:09:50,560
i first mentioned in the beginning that

282
00:09:48,880 --> 00:09:53,760
um

283
00:09:50,560 --> 00:09:56,800
dc also has issues in terms of

284
00:09:53,760 --> 00:10:01,439
with exposing the processing pipeline

285
00:09:56,800 --> 00:10:04,000
but the module that we wrote for ebbf

286
00:10:01,440 --> 00:10:05,839
basically bypasses this because you can

287
00:10:04,000 --> 00:10:08,000
do all of it

288
00:10:05,839 --> 00:10:09,839
in ebpf and then directly return a

289
00:10:08,000 --> 00:10:12,000
verdict without having to process

290
00:10:09,839 --> 00:10:13,760
many many dc rules there so it's

291
00:10:12,000 --> 00:10:15,600
actually efficient

292
00:10:13,760 --> 00:10:16,959
and i would say it's probably the only

293
00:10:15,600 --> 00:10:19,200
efficient

294
00:10:16,959 --> 00:10:20,719
module that is there in the tc software

295
00:10:19,200 --> 00:10:22,720
path

296
00:10:20,720 --> 00:10:24,560
um how does it work basically so you

297
00:10:22,720 --> 00:10:27,120
have a piece of code it's similar

298
00:10:24,560 --> 00:10:27,920
it's c like syntax you can compile it

299
00:10:27,120 --> 00:10:31,680
with lvm

300
00:10:27,920 --> 00:10:33,839
lvm has a bpf backend and

301
00:10:31,680 --> 00:10:35,920
that generates an object file there's a

302
00:10:33,839 --> 00:10:37,680
bpf loader in user space for example

303
00:10:35,920 --> 00:10:39,680
ipro 2 or

304
00:10:37,680 --> 00:10:41,439
there's libpf which is maintained in the

305
00:10:39,680 --> 00:10:43,920
linux kernel tree as well

306
00:10:41,440 --> 00:10:46,000
and it basically creates the maps it

307
00:10:43,920 --> 00:10:48,880
links the maps into the bpf program

308
00:10:46,000 --> 00:10:49,200
and in the end it calls a syscall for

309
00:10:48,880 --> 00:10:51,439
the

310
00:10:49,200 --> 00:10:52,320
for loading the program then in the

311
00:10:51,440 --> 00:10:54,079
kernel it

312
00:10:52,320 --> 00:10:55,600
goes through a verifier verifier make

313
00:10:54,079 --> 00:10:58,880
sure that everything is

314
00:10:55,600 --> 00:11:00,800
safe and sound and then basically

315
00:10:58,880 --> 00:11:02,000
it hits the just-in-time compiler where

316
00:11:00,800 --> 00:11:04,399
it translates

317
00:11:02,000 --> 00:11:06,480
um bpf instructions into native cpu

318
00:11:04,399 --> 00:11:08,720
instructions and basically all

319
00:11:06,480 --> 00:11:10,000
major uh kernel architectures they have

320
00:11:08,720 --> 00:11:11,920
support for jit

321
00:11:10,000 --> 00:11:14,640
these days and then it creates native

322
00:11:11,920 --> 00:11:15,599
code and basically your user space agent

323
00:11:14,640 --> 00:11:18,399
that you have

324
00:11:15,600 --> 00:11:20,560
can access bpf maps to update state at

325
00:11:18,399 --> 00:11:21,760
runtime or to fetch state from the ppf

326
00:11:20,560 --> 00:11:25,279
program if it wrote

327
00:11:21,760 --> 00:11:27,360
into maps and again like if traffic is

328
00:11:25,279 --> 00:11:29,600
coming then it hits the bpf code and it

329
00:11:27,360 --> 00:11:32,320
can either redirect it to another device

330
00:11:29,600 --> 00:11:33,680
or basically can drop the packet that's

331
00:11:32,320 --> 00:11:35,920
also possible

332
00:11:33,680 --> 00:11:38,319
and an agent itself can also like

333
00:11:35,920 --> 00:11:41,519
periodically update ppf programs

334
00:11:38,320 --> 00:11:42,320
so that's um yeah that's that's the

335
00:11:41,519 --> 00:11:45,120
basic

336
00:11:42,320 --> 00:11:47,120
uh workflow there and so yeah so why is

337
00:11:45,120 --> 00:11:49,040
bpf a radical shift in

338
00:11:47,120 --> 00:11:51,200
um because you can get full

339
00:11:49,040 --> 00:11:51,920
programmability right so it allows you

340
00:11:51,200 --> 00:11:54,480
to

341
00:11:51,920 --> 00:11:55,599
uh it allows the user to tinker and to

342
00:11:54,480 --> 00:11:57,839
change the kernel

343
00:11:55,600 --> 00:11:59,600
data path in any way it wants but still

344
00:11:57,839 --> 00:12:02,000
with the safety belt on it's not like

345
00:11:59,600 --> 00:12:04,639
kernel modules where you can potentially

346
00:12:02,000 --> 00:12:06,160
crash the kernel so i've been talking a

347
00:12:04,639 --> 00:12:07,440
lot about networking and this whole talk

348
00:12:06,160 --> 00:12:09,920
is actually about

349
00:12:07,440 --> 00:12:11,760
container networking here but it's also

350
00:12:09,920 --> 00:12:13,439
uh there are also different areas like

351
00:12:11,760 --> 00:12:14,240
tracing and security tracing you

352
00:12:13,440 --> 00:12:16,079
probably

353
00:12:14,240 --> 00:12:17,519
heard a lot from bren greg where he's

354
00:12:16,079 --> 00:12:19,680
doing great work there

355
00:12:17,519 --> 00:12:21,600
and security there are recently

356
00:12:19,680 --> 00:12:24,079
proposals for linux security modules to

357
00:12:21,600 --> 00:12:25,600
make it programmable with bpf as well

358
00:12:24,079 --> 00:12:27,839
in networking it allows you to fully

359
00:12:25,600 --> 00:12:28,480
define the forwarding pipeline which is

360
00:12:27,839 --> 00:12:31,200
great

361
00:12:28,480 --> 00:12:32,720
and the thing is like ppf has a stable

362
00:12:31,200 --> 00:12:34,480
api guarantee so it's pretty much

363
00:12:32,720 --> 00:12:36,800
similar to syscalls

364
00:12:34,480 --> 00:12:39,360
where you don't break it once it works

365
00:12:36,800 --> 00:12:40,880
and you in it the verifier passes

366
00:12:39,360 --> 00:12:43,279
and the other thing is you get native

367
00:12:40,880 --> 00:12:46,639
speed so similar to kernel modules there

368
00:12:43,279 --> 00:12:48,320
um you can update your bpf programs live

369
00:12:46,639 --> 00:12:50,079
at runtime without having to reboot

370
00:12:48,320 --> 00:12:52,320
anything without having to

371
00:12:50,079 --> 00:12:54,079
even tear down services so it works

372
00:12:52,320 --> 00:12:56,160
without service disruption

373
00:12:54,079 --> 00:12:58,719
and it's really designed for performance

374
00:12:56,160 --> 00:13:02,160
in production use cases right

375
00:12:58,720 --> 00:13:04,639
bpf is basically developed mainly by

376
00:13:02,160 --> 00:13:05,439
facebook and some folks from from our

377
00:13:04,639 --> 00:13:08,079
site

378
00:13:05,440 --> 00:13:10,079
google red hat cloudflare many many

379
00:13:08,079 --> 00:13:11,439
people and it's really great

380
00:13:10,079 --> 00:13:14,479
that there are so many contributors

381
00:13:11,440 --> 00:13:17,200
there's really a long tale of 280

382
00:13:14,480 --> 00:13:18,560
um seven contributors in the kernel

383
00:13:17,200 --> 00:13:21,120
since the beginning

384
00:13:18,560 --> 00:13:22,800
and they're also large-scale users of it

385
00:13:21,120 --> 00:13:23,760
so facebook is running it in production

386
00:13:22,800 --> 00:13:25,279
and all of their

387
00:13:23,760 --> 00:13:27,600
infrastructure whenever you hit

388
00:13:25,279 --> 00:13:29,680
facebook.com you will go to bpf

389
00:13:27,600 --> 00:13:31,200
um netflix are using it in a large scale

390
00:13:29,680 --> 00:13:33,040
for tracing

391
00:13:31,200 --> 00:13:35,200
google is using it in terms of traffic

392
00:13:33,040 --> 00:13:37,279
shaping to optimize their tcp stack and

393
00:13:35,200 --> 00:13:39,360
many others cloudfare

394
00:13:37,279 --> 00:13:41,360
folks they're using it for their load

395
00:13:39,360 --> 00:13:44,880
balancer and ddos protection

396
00:13:41,360 --> 00:13:45,199
in production and it's even available on

397
00:13:44,880 --> 00:13:47,360
rel

398
00:13:45,199 --> 00:13:48,719
kernels which means something because it

399
00:13:47,360 --> 00:13:50,000
probably means now it's mainstream

400
00:13:48,720 --> 00:13:51,680
because people are back putting it to

401
00:13:50,000 --> 00:13:54,880
rail which is a good thing

402
00:13:51,680 --> 00:13:56,638
and yeah and even like the old iptables

403
00:13:54,880 --> 00:14:00,079
maintainer rusty russell he admits that

404
00:13:56,639 --> 00:14:02,079
yeah beatables used to be good enough

405
00:14:00,079 --> 00:14:03,439
but these days there's it needs a

406
00:14:02,079 --> 00:14:06,800
radical shift

407
00:14:03,440 --> 00:14:10,079
um and that's basically bpf right so

408
00:14:06,800 --> 00:14:12,800
how does this all link back to

409
00:14:10,079 --> 00:14:14,399
kubernetes networking um so basically

410
00:14:12,800 --> 00:14:15,279
there's sodium which is an open source

411
00:14:14,399 --> 00:14:18,880
project

412
00:14:15,279 --> 00:14:21,680
it's a cni for kubernetes

413
00:14:18,880 --> 00:14:23,120
and it implements its full data path in

414
00:14:21,680 --> 00:14:24,560
vpf

415
00:14:23,120 --> 00:14:26,160
it supports the kubernetes service

416
00:14:24,560 --> 00:14:28,560
implementation which is the focus of

417
00:14:26,160 --> 00:14:31,760
this talk but there's also many other

418
00:14:28,560 --> 00:14:34,079
things there uh network policies

419
00:14:31,760 --> 00:14:34,959
uh multi-cluster um accessibility

420
00:14:34,079 --> 00:14:36,479
encryption

421
00:14:34,959 --> 00:14:38,160
and so on and so forth so you can look

422
00:14:36,480 --> 00:14:41,600
it up it's all open source

423
00:14:38,160 --> 00:14:43,680
on github um yeah so how did we get to

424
00:14:41,600 --> 00:14:45,040
the buff and psyllium of replacing two

425
00:14:43,680 --> 00:14:46,800
proxy with bpf

426
00:14:45,040 --> 00:14:49,279
where you can basically then end up

427
00:14:46,800 --> 00:14:52,880
deleting the cube proxy daemon set

428
00:14:49,279 --> 00:14:54,880
and don't have uh you don't even need

429
00:14:52,880 --> 00:14:56,839
your long list of iptable rules anymore

430
00:14:54,880 --> 00:14:59,519
then so basically

431
00:14:56,839 --> 00:15:01,199
to give you an overview of what cube

432
00:14:59,519 --> 00:15:02,480
proxy does you basically have different

433
00:15:01,199 --> 00:15:04,959
services

434
00:15:02,480 --> 00:15:06,079
one is called cluster ip cluster ip

435
00:15:04,959 --> 00:15:09,359
basically allows you

436
00:15:06,079 --> 00:15:11,120
an in cluster access of a virtual ip

437
00:15:09,360 --> 00:15:12,880
where basically you can either access

438
00:15:11,120 --> 00:15:14,079
this from a part or from the host name

439
00:15:12,880 --> 00:15:15,839
space

440
00:15:14,079 --> 00:15:17,680
there are services which are called node

441
00:15:15,839 --> 00:15:20,880
port which allow the access

442
00:15:17,680 --> 00:15:21,920
from outside um but also at the same

443
00:15:20,880 --> 00:15:24,079
time from inside

444
00:15:21,920 --> 00:15:25,040
you can you can view it as an onion so

445
00:15:24,079 --> 00:15:27,359
for example

446
00:15:25,040 --> 00:15:28,959
the noteboard also makes use of cluster

447
00:15:27,360 --> 00:15:33,199
ip

448
00:15:28,959 --> 00:15:36,399
and yeah there's also external ip

449
00:15:33,199 --> 00:15:38,240
which means you have an access to an

450
00:15:36,399 --> 00:15:40,399
external ip you can basically override

451
00:15:38,240 --> 00:15:43,440
it with your own back ends

452
00:15:40,399 --> 00:15:46,639
it also again works on top of

453
00:15:43,440 --> 00:15:49,759
node port and there's last but not least

454
00:15:46,639 --> 00:15:51,600
a load balancer service which is usually

455
00:15:49,759 --> 00:15:53,600
when you have an external cloud provider

456
00:15:51,600 --> 00:15:55,519
and then it redirects buckets to your

457
00:15:53,600 --> 00:15:57,040
either external ipo and output services

458
00:15:55,519 --> 00:15:57,920
so you can access it from the outside

459
00:15:57,040 --> 00:16:01,519
world

460
00:15:57,920 --> 00:16:05,599
right and in psyllium basically

461
00:16:01,519 --> 00:16:07,680
in prior versions we implemented

462
00:16:05,600 --> 00:16:09,839
the whole thing in bpf the following way

463
00:16:07,680 --> 00:16:11,279
like the cluster ip if you have part two

464
00:16:09,839 --> 00:16:13,839
part connectivity

465
00:16:11,279 --> 00:16:16,639
if you basically connect to a virtual ip

466
00:16:13,839 --> 00:16:19,279
like going back to the 333 example from

467
00:16:16,639 --> 00:16:20,240
previous slides with the engine x

468
00:16:19,279 --> 00:16:22,000
service

469
00:16:20,240 --> 00:16:24,079
you basically have your own part here

470
00:16:22,000 --> 00:16:25,920
which has an it's all network namespace

471
00:16:24,079 --> 00:16:27,120
you connect to it through reef devices

472
00:16:25,920 --> 00:16:30,160
and basically

473
00:16:27,120 --> 00:16:32,480
in in on this side you're hitting the

474
00:16:30,160 --> 00:16:34,240
tc ingress path in the host namespace

475
00:16:32,480 --> 00:16:37,440
and basically there bpf is

476
00:16:34,240 --> 00:16:39,199
attached and what it does inside bpf

477
00:16:37,440 --> 00:16:41,040
in the bpf program is basically a

478
00:16:39,199 --> 00:16:43,599
service map lookup

479
00:16:41,040 --> 00:16:45,519
it checks whether the uh whether there's

480
00:16:43,600 --> 00:16:46,399
an endpoint for that address and then it

481
00:16:45,519 --> 00:16:50,480
does the

482
00:16:46,399 --> 00:16:52,480
dnat on the back end in bpf it creates a

483
00:16:50,480 --> 00:16:53,759
connection tracking entry

484
00:16:52,480 --> 00:16:55,120
so that it can make sure that the

485
00:16:53,759 --> 00:16:56,480
replies are coming back in and are

486
00:16:55,120 --> 00:16:58,000
translated correctly

487
00:16:56,480 --> 00:16:59,759
and here you can see like the content

488
00:16:58,000 --> 00:17:03,519
basically of a service map

489
00:16:59,759 --> 00:17:05,360
it has different backends encoded here

490
00:17:03,519 --> 00:17:06,799
and for the connection tracker we can

491
00:17:05,359 --> 00:17:08,240
basically make sure that whenever

492
00:17:06,799 --> 00:17:09,918
replies come back in that there's

493
00:17:08,240 --> 00:17:11,599
connection tracking entry it will hit it

494
00:17:09,919 --> 00:17:13,679
it will do the reverse net and go back

495
00:17:11,599 --> 00:17:17,599
to the original client

496
00:17:13,679 --> 00:17:19,919
so yeah that's like the basic path um

497
00:17:17,599 --> 00:17:21,678
and how is this all plumped down from

498
00:17:19,919 --> 00:17:23,520
kubernetes to sodium basically

499
00:17:21,679 --> 00:17:25,280
sodium listens to events from the cube

500
00:17:23,520 --> 00:17:28,400
api server

501
00:17:25,280 --> 00:17:30,559
and there so whenever uh

502
00:17:28,400 --> 00:17:32,480
there are service updates it listens to

503
00:17:30,559 --> 00:17:34,160
it for example here and then

504
00:17:32,480 --> 00:17:36,400
back end updates and then it basically

505
00:17:34,160 --> 00:17:38,480
pushes this down into this ppf service

506
00:17:36,400 --> 00:17:42,400
map that is sitting in the data path

507
00:17:38,480 --> 00:17:45,200
right so recently we

508
00:17:42,400 --> 00:17:46,799
reworked the cluster ip access with

509
00:17:45,200 --> 00:17:49,039
something even more efficient

510
00:17:46,799 --> 00:17:52,639
so the linux kernel has support for

511
00:17:49,039 --> 00:17:54,559
attaching bpf programs to c group v2

512
00:17:52,640 --> 00:17:56,720
there are different ways you can attach

513
00:17:54,559 --> 00:17:58,399
to it so just to walk you through it so

514
00:17:56,720 --> 00:18:00,640
if a client again connects to

515
00:17:58,400 --> 00:18:02,960
an engine x service then basically we

516
00:18:00,640 --> 00:18:06,000
already attached to the connect syscall

517
00:18:02,960 --> 00:18:08,559
and we can see that

518
00:18:06,000 --> 00:18:10,320
the client is trying to access one of

519
00:18:08,559 --> 00:18:11,360
the servers ips we can already do the

520
00:18:10,320 --> 00:18:13,840
map lookup

521
00:18:11,360 --> 00:18:15,199
right from there and then we can

522
00:18:13,840 --> 00:18:16,879
basically rewrite

523
00:18:15,200 --> 00:18:19,120
the destination address to one of the

524
00:18:16,880 --> 00:18:20,400
back ends and that's much more efficient

525
00:18:19,120 --> 00:18:22,959
because you don't have to do

526
00:18:20,400 --> 00:18:24,559
actual dna you don't have to mangle

527
00:18:22,960 --> 00:18:27,039
packets in the data path

528
00:18:24,559 --> 00:18:29,760
you can just rewrite the address that

529
00:18:27,039 --> 00:18:32,160
was used to pass down for the connect

530
00:18:29,760 --> 00:18:33,520
system call so basically the kernel

531
00:18:32,160 --> 00:18:35,280
there so basically

532
00:18:33,520 --> 00:18:37,360
it looks like as if the application

533
00:18:35,280 --> 00:18:40,559
directly connects to the back end

534
00:18:37,360 --> 00:18:41,918
but it's still like the kernel um

535
00:18:40,559 --> 00:18:43,918
makes it think that it's actually

536
00:18:41,919 --> 00:18:45,919
connecting to the service ap

537
00:18:43,919 --> 00:18:47,679
right and the same for connected udp so

538
00:18:45,919 --> 00:18:49,039
there's connected udp and unconnected

539
00:18:47,679 --> 00:18:50,559
udp in the kernel

540
00:18:49,039 --> 00:18:52,320
and you can do the same there from the

541
00:18:50,559 --> 00:18:54,399
connect hook

542
00:18:52,320 --> 00:18:56,000
and if you have unconnected udp where

543
00:18:54,400 --> 00:18:58,080
you only use send message and receive

544
00:18:56,000 --> 00:18:59,039
message you basically don't have any

545
00:18:58,080 --> 00:19:00,879
state

546
00:18:59,039 --> 00:19:02,320
and there you need like a small

547
00:19:00,880 --> 00:19:04,559
translation table

548
00:19:02,320 --> 00:19:06,559
for those two hooks where you can then

549
00:19:04,559 --> 00:19:08,399
uh if the packet goes up

550
00:19:06,559 --> 00:19:10,720
like back to the application to receive

551
00:19:08,400 --> 00:19:12,880
message where you can then rewrite the

552
00:19:10,720 --> 00:19:14,240
back-end ip with the service with the

553
00:19:12,880 --> 00:19:16,960
virtual service ip

554
00:19:14,240 --> 00:19:18,480
right yeah so i think that's really

555
00:19:16,960 --> 00:19:21,600
really powerful

556
00:19:18,480 --> 00:19:23,919
feature in saves overhead

557
00:19:21,600 --> 00:19:25,520
then the case of noteboard services so

558
00:19:23,919 --> 00:19:26,320
basically here we have this external

559
00:19:25,520 --> 00:19:29,200
client

560
00:19:26,320 --> 00:19:30,080
it tries to connect to a service on the

561
00:19:29,200 --> 00:19:32,880
nodes ip

562
00:19:30,080 --> 00:19:36,799
within the node port range and the

563
00:19:32,880 --> 00:19:36,799
packet arrives on the physical device

564
00:19:37,039 --> 00:19:41,679
uh ppf program which is sitting there it

565
00:19:39,360 --> 00:19:44,479
does the service lookup and it

566
00:19:41,679 --> 00:19:45,760
is automatically then doing a um dnat to

567
00:19:44,480 --> 00:19:47,919
one of the back ends

568
00:19:45,760 --> 00:19:49,120
it checks whether it's so psyllium has

569
00:19:47,919 --> 00:19:50,640
the knowledge whether the back end is

570
00:19:49,120 --> 00:19:52,399
local or remote and it checks

571
00:19:50,640 --> 00:19:54,080
when it's local then it just redirects

572
00:19:52,400 --> 00:19:56,080
it into the parts

573
00:19:54,080 --> 00:19:58,159
network namespace so that's the trivial

574
00:19:56,080 --> 00:19:59,439
case where the backend is basically

575
00:19:58,160 --> 00:20:01,280
local to the node

576
00:19:59,440 --> 00:20:02,960
if you have the more complicated stuff

577
00:20:01,280 --> 00:20:06,320
where it's remote

578
00:20:02,960 --> 00:20:10,240
again then you will connect to

579
00:20:06,320 --> 00:20:12,240
the same nodes ip address where there's

580
00:20:10,240 --> 00:20:15,120
the ppf program running

581
00:20:12,240 --> 00:20:16,559
and then it determines how so the back

582
00:20:15,120 --> 00:20:19,678
end is not local

583
00:20:16,559 --> 00:20:20,080
i have to go i have to go to the remote

584
00:20:19,679 --> 00:20:22,960
node

585
00:20:20,080 --> 00:20:24,559
and basically it's doing a first the dna

586
00:20:22,960 --> 00:20:26,159
and then it's also doing an s net and

587
00:20:24,559 --> 00:20:28,320
that s net is also done

588
00:20:26,159 --> 00:20:29,600
in bpf so we basically wrote an ad

589
00:20:28,320 --> 00:20:31,840
engine in bpf

590
00:20:29,600 --> 00:20:33,199
it's doing the translation and when the

591
00:20:31,840 --> 00:20:35,600
reply comes back

592
00:20:33,200 --> 00:20:37,120
it has to do all of the reverse s-net

593
00:20:35,600 --> 00:20:39,199
and d-net and pushing the packet back

594
00:20:37,120 --> 00:20:41,439
out and that basically works

595
00:20:39,200 --> 00:20:42,799
we have it in in our soleum code base

596
00:20:41,440 --> 00:20:46,000
merged and

597
00:20:42,799 --> 00:20:49,520
it can go over direct routing or also

598
00:20:46,000 --> 00:20:51,520
over a tunnel mesh in psyllium

599
00:20:49,520 --> 00:20:53,360
yeah so all done in bpf then there are

600
00:20:51,520 --> 00:20:55,600
some something that's called

601
00:20:53,360 --> 00:20:56,559
external traffic policy local in

602
00:20:55,600 --> 00:20:59,520
kubernetes

603
00:20:56,559 --> 00:21:00,559
where you basically say um that it can

604
00:20:59,520 --> 00:21:02,240
only connect to

605
00:21:00,559 --> 00:21:04,158
those back ends that are actually local

606
00:21:02,240 --> 00:21:04,799
to the node so you don't take this extra

607
00:21:04,159 --> 00:21:06,320
hop

608
00:21:04,799 --> 00:21:08,240
and basically the packet would be

609
00:21:06,320 --> 00:21:09,760
dropped in this case if it tries to

610
00:21:08,240 --> 00:21:11,520
connect there

611
00:21:09,760 --> 00:21:13,440
and something that we merged just

612
00:21:11,520 --> 00:21:14,240
recently in cilium is also direct server

613
00:21:13,440 --> 00:21:16,480
return

614
00:21:14,240 --> 00:21:18,240
so there we basically save the overhead

615
00:21:16,480 --> 00:21:20,960
of of doing the asnet

616
00:21:18,240 --> 00:21:22,640
and whenever a service again tries to

617
00:21:20,960 --> 00:21:23,600
connect to a backend that is remote to

618
00:21:22,640 --> 00:21:27,200
the node

619
00:21:23,600 --> 00:21:29,439
basically we encode the um

620
00:21:27,200 --> 00:21:32,640
the service ip so that basically the

621
00:21:29,440 --> 00:21:35,679
back end then in bpf can rewrite

622
00:21:32,640 --> 00:21:37,440
uh the packet with the source as the

623
00:21:35,679 --> 00:21:39,919
original service id and can reply

624
00:21:37,440 --> 00:21:42,159
directly to the to the client without

625
00:21:39,919 --> 00:21:43,440
uh going back to the to the other node

626
00:21:42,159 --> 00:21:46,000
for this extra hub

627
00:21:43,440 --> 00:21:46,720
so that's more efficient even and that

628
00:21:46,000 --> 00:21:48,880
all happens

629
00:21:46,720 --> 00:21:51,039
inside bpf without any of the other

630
00:21:48,880 --> 00:21:54,000
dependencies needed

631
00:21:51,039 --> 00:21:55,840
so we did some performance benchmarking

632
00:21:54,000 --> 00:21:57,919
on aws

633
00:21:55,840 --> 00:21:58,879
with sio renetworking and compared

634
00:21:57,919 --> 00:22:01,280
basically

635
00:21:58,880 --> 00:22:02,320
uh the whole thing um to ib tables and

636
00:22:01,280 --> 00:22:05,840
ipvs

637
00:22:02,320 --> 00:22:06,559
and um we were running net births uh tcp

638
00:22:05,840 --> 00:22:09,678
crr

639
00:22:06,559 --> 00:22:12,399
gcp crr is basically a net perth test um

640
00:22:09,679 --> 00:22:14,559
where your your you're establishing the

641
00:22:12,400 --> 00:22:15,840
tcp handshake you're exchanging one byte

642
00:22:14,559 --> 00:22:17,039
of information then you're already

643
00:22:15,840 --> 00:22:19,520
tearing down

644
00:22:17,039 --> 00:22:20,320
um the tcp connection and it's doing

645
00:22:19,520 --> 00:22:22,879
this as many

646
00:22:20,320 --> 00:22:24,960
times as you can and we were basically

647
00:22:22,880 --> 00:22:27,600
running this against the notepad service

648
00:22:24,960 --> 00:22:28,799
and as you can see here with the default

649
00:22:27,600 --> 00:22:31,360
baked in

650
00:22:28,799 --> 00:22:32,080
q proxy implementation you're really

651
00:22:31,360 --> 00:22:37,840
hitting the

652
00:22:32,080 --> 00:22:40,240
limits if you have 2700 services

653
00:22:37,840 --> 00:22:40,959
that really is noticeable in terms of

654
00:22:40,240 --> 00:22:42,720
overhead

655
00:22:40,960 --> 00:22:44,159
because you have to traverse all the

656
00:22:42,720 --> 00:22:47,520
lists and ip tables

657
00:22:44,159 --> 00:22:49,760
um yeah and

658
00:22:47,520 --> 00:22:51,440
it's it's it's being done for all the

659
00:22:49,760 --> 00:22:54,799
new connections right

660
00:22:51,440 --> 00:22:57,919
and yeah so

661
00:22:54,799 --> 00:22:58,559
the second uh benchmark is basically the

662
00:22:57,919 --> 00:23:01,919
tcp

663
00:22:58,559 --> 00:23:02,639
rr which means that it's creating a tcp

664
00:23:01,919 --> 00:23:05,360
connection

665
00:23:02,640 --> 00:23:07,840
and then it's only is sending one byte

666
00:23:05,360 --> 00:23:09,520
of data as a ping-pong

667
00:23:07,840 --> 00:23:11,520
and then tearing it down so you don't

668
00:23:09,520 --> 00:23:13,120
have the new connection

669
00:23:11,520 --> 00:23:15,280
every time so you can see here that

670
00:23:13,120 --> 00:23:18,879
iptables is actually

671
00:23:15,280 --> 00:23:20,799
performing not as bad as before

672
00:23:18,880 --> 00:23:22,480
but bpf is still the clear winner and in

673
00:23:20,799 --> 00:23:24,320
the dsr implementation where you save

674
00:23:22,480 --> 00:23:27,360
this extra hub

675
00:23:24,320 --> 00:23:28,639
gives you the lowest latency

676
00:23:27,360 --> 00:23:30,719
the other thing that we are currently

677
00:23:28,640 --> 00:23:31,679
working on right now is basically to

678
00:23:30,720 --> 00:23:34,159
move this

679
00:23:31,679 --> 00:23:35,840
um extra hop that we had where the

680
00:23:34,159 --> 00:23:38,400
backend is remote to denote

681
00:23:35,840 --> 00:23:39,840
into the xtp layer xdp layer is

682
00:23:38,400 --> 00:23:42,159
basically

683
00:23:39,840 --> 00:23:44,240
another hook for bpf in the system where

684
00:23:42,159 --> 00:23:45,600
you can where you can run bpf programs

685
00:23:44,240 --> 00:23:47,440
right at the driver layer

686
00:23:45,600 --> 00:23:49,279
without even having the overhead of

687
00:23:47,440 --> 00:23:51,360
allocating a socket buffer in the linux

688
00:23:49,279 --> 00:23:51,760
kernel without even having to go through

689
00:23:51,360 --> 00:23:54,879
the

690
00:23:51,760 --> 00:23:56,960
g0 engine and higher layers

691
00:23:54,880 --> 00:23:58,559
so it's it's been running at much

692
00:23:56,960 --> 00:24:01,840
earlier hook than just the tc

693
00:23:58,559 --> 00:24:03,360
ingress and we can already dare if we

694
00:24:01,840 --> 00:24:05,360
determine that the packet at the back

695
00:24:03,360 --> 00:24:07,199
end is remote we can already there push

696
00:24:05,360 --> 00:24:09,360
it back out the driver again without

697
00:24:07,200 --> 00:24:11,120
having to go into higher layers

698
00:24:09,360 --> 00:24:12,559
and all the three cloud providers now

699
00:24:11,120 --> 00:24:14,158
they have finally support

700
00:24:12,559 --> 00:24:15,760
so the last thing that got merged was

701
00:24:14,159 --> 00:24:17,679
from what

702
00:24:15,760 --> 00:24:19,520
into the kernel was the ena driver that

703
00:24:17,679 --> 00:24:22,480
it got xtp support so you can run it

704
00:24:19,520 --> 00:24:25,200
also in aws with siu v networking

705
00:24:22,480 --> 00:24:26,159
and this chart here is basically from a

706
00:24:25,200 --> 00:24:29,200
presentation

707
00:24:26,159 --> 00:24:30,720
from netdefcon from facebook folks so

708
00:24:29,200 --> 00:24:33,279
they're used

709
00:24:30,720 --> 00:24:34,480
for a long time ipvs for their main l4

710
00:24:33,279 --> 00:24:37,520
load balancing

711
00:24:34,480 --> 00:24:40,400
and basically they switched all their

712
00:24:37,520 --> 00:24:41,360
uh front and load balancer to bpf and

713
00:24:40,400 --> 00:24:44,240
xdp

714
00:24:41,360 --> 00:24:45,760
in production and they see much better

715
00:24:44,240 --> 00:24:46,960
throughput so they don't publish

716
00:24:45,760 --> 00:24:49,279
concrete numbers

717
00:24:46,960 --> 00:24:50,559
but they said it has a performance gain

718
00:24:49,279 --> 00:24:53,120
for 10x and more

719
00:24:50,559 --> 00:24:54,960
which is quite impressive so yeah so

720
00:24:53,120 --> 00:24:57,279
that's for the

721
00:24:54,960 --> 00:24:58,000
psyllium 1 8 release we have scheduled

722
00:24:57,279 --> 00:25:02,400
that

723
00:24:58,000 --> 00:25:05,440
to pour it into xdp and

724
00:25:02,400 --> 00:25:06,000
basically to recap with with the with

725
00:25:05,440 --> 00:25:08,640
the whole

726
00:25:06,000 --> 00:25:10,880
um data path and bpf you get a much

727
00:25:08,640 --> 00:25:12,960
better performance and lower latency

728
00:25:10,880 --> 00:25:14,000
you can have faster service updates

729
00:25:12,960 --> 00:25:18,080
because you don't have to go

730
00:25:14,000 --> 00:25:18,720
to um to then to the netlink and you

731
00:25:18,080 --> 00:25:22,000
don't have to

732
00:25:18,720 --> 00:25:22,000
replace the entire blob

733
00:25:22,159 --> 00:25:26,880
and you just do like a bpf system call

734
00:25:24,559 --> 00:25:28,960
for updating dpf maps

735
00:25:26,880 --> 00:25:30,559
it's much more reliable you have less

736
00:25:28,960 --> 00:25:32,400
lines of code overall

737
00:25:30,559 --> 00:25:34,320
and you don't need to wait for a new

738
00:25:32,400 --> 00:25:35,840
kernel if you have a fix that you

739
00:25:34,320 --> 00:25:37,918
desperately need in production you can

740
00:25:35,840 --> 00:25:40,559
just patch it live because

741
00:25:37,919 --> 00:25:41,360
you can just change the bpf program on

742
00:25:40,559 --> 00:25:43,279
the fly

743
00:25:41,360 --> 00:25:44,879
you get much better visibility i didn't

744
00:25:43,279 --> 00:25:47,200
really talk about that

745
00:25:44,880 --> 00:25:48,799
in this talk here but there's something

746
00:25:47,200 --> 00:25:50,720
called the perfect buffer output where

747
00:25:48,799 --> 00:25:54,480
you can basically

748
00:25:50,720 --> 00:25:55,840
export custom data um to a perf to a

749
00:25:54,480 --> 00:25:56,400
high performance per ring buffer and

750
00:25:55,840 --> 00:25:59,199
then

751
00:25:56,400 --> 00:25:59,919
collect all of this in user space and

752
00:25:59,200 --> 00:26:02,799
it's not

753
00:25:59,919 --> 00:26:04,559
it's not subject to any um kernel you

754
00:26:02,799 --> 00:26:06,799
api restrictions or something you can

755
00:26:04,559 --> 00:26:09,039
really define your custom structs and

756
00:26:06,799 --> 00:26:10,639
see that you want to push down there

757
00:26:09,039 --> 00:26:13,200
and then we can also correlate traffic

758
00:26:10,640 --> 00:26:14,640
with container traffic for example

759
00:26:13,200 --> 00:26:16,960
uh you don't have to shell out to

760
00:26:14,640 --> 00:26:20,000
iptables and you're able to

761
00:26:16,960 --> 00:26:21,840
much more customize your data path you

762
00:26:20,000 --> 00:26:23,120
can change basically the behavior on the

763
00:26:21,840 --> 00:26:24,720
fly which is great

764
00:26:23,120 --> 00:26:26,639
and it's fully integrated with the rest

765
00:26:24,720 --> 00:26:27,200
of psyllium so this aspect only covered

766
00:26:26,640 --> 00:26:28,960
the

767
00:26:27,200 --> 00:26:30,640
load balancing integration but there's

768
00:26:28,960 --> 00:26:33,840
also policy and much more

769
00:26:30,640 --> 00:26:37,360
in psyllium so yeah with that said

770
00:26:33,840 --> 00:26:41,360
um if you want to try out the cube proxy

771
00:26:37,360 --> 00:26:44,080
free how to it's just like

772
00:26:41,360 --> 00:26:45,279
three or four comments to run it there's

773
00:26:44,080 --> 00:26:47,600
a cube admin

774
00:26:45,279 --> 00:26:50,559
integration where you can deploy cube

775
00:26:47,600 --> 00:26:52,320
admin without q proxy

776
00:26:50,559 --> 00:26:54,320
and then you can basically run solium

777
00:26:52,320 --> 00:26:55,279
and enable it with the cube proxy free

778
00:26:54,320 --> 00:26:57,760
feature

779
00:26:55,279 --> 00:26:59,760
uh the code is all on github if you want

780
00:26:57,760 --> 00:27:01,120
to check it out and there's also a slack

781
00:26:59,760 --> 00:27:07,840
community so

782
00:27:01,120 --> 00:27:07,840
yeah thanks for that are there any

783
00:27:09,840 --> 00:27:15,918
do we have questions are there any

784
00:27:13,279 --> 00:27:15,919
questions there

785
00:27:16,720 --> 00:27:23,440
oh yeah

786
00:27:21,120 --> 00:27:24,399
sorry uh privilege mode so does it

787
00:27:23,440 --> 00:27:27,440
require privilege

788
00:27:24,399 --> 00:27:28,559
mode so if it's working like on a from

789
00:27:27,440 --> 00:27:30,080
behind so you

790
00:27:28,559 --> 00:27:31,840
 the question is whether it

791
00:27:30,080 --> 00:27:34,960
requires privilege mode or

792
00:27:31,840 --> 00:27:37,279
yeah yeah so most of the ppf features

793
00:27:34,960 --> 00:27:41,039
they are in privileged mode so basically

794
00:27:37,279 --> 00:27:43,200
the psyllium part that you deploy runs

795
00:27:41,039 --> 00:27:45,200
basically on a host namespace as well

796
00:27:43,200 --> 00:27:46,799
and it has to install all the bpf and

797
00:27:45,200 --> 00:27:47,760
manage all the ppf but that's basically

798
00:27:46,799 --> 00:27:50,399
privileged

799
00:27:47,760 --> 00:27:51,039
we disable the unprivileged ppf by

800
00:27:50,399 --> 00:27:56,000
default

801
00:27:51,039 --> 00:27:56,000
so yeah other questions

802
00:28:03,200 --> 00:28:12,080
uh it's uh the first time that uh

803
00:28:08,399 --> 00:28:12,399
i'm i'm seeing a bpf if i understood

804
00:28:12,080 --> 00:28:16,240
well

805
00:28:12,399 --> 00:28:18,959
it's all in user space and i'm concerned

806
00:28:16,240 --> 00:28:20,000
about security because you never talk

807
00:28:18,960 --> 00:28:23,440
about security

808
00:28:20,000 --> 00:28:26,399
is it possible for a

809
00:28:23,440 --> 00:28:28,399
a container to tamper the rules or

810
00:28:26,399 --> 00:28:30,320
something similar

811
00:28:28,399 --> 00:28:31,840
so basically all the bpf programs that

812
00:28:30,320 --> 00:28:33,279
are loaded into the kernel they have to

813
00:28:31,840 --> 00:28:35,600
pass the verifier

814
00:28:33,279 --> 00:28:37,600
and the verifier does um strict security

815
00:28:35,600 --> 00:28:39,279
checks it checks for example

816
00:28:37,600 --> 00:28:40,799
whether all the types are saying whether

817
00:28:39,279 --> 00:28:42,559
you don't do out of bound access whether

818
00:28:40,799 --> 00:28:43,120
you so basically all the stuff that you

819
00:28:42,559 --> 00:28:45,200
cannot

820
00:28:43,120 --> 00:28:46,639
crash or destabilize the kernel and it

821
00:28:45,200 --> 00:28:49,120
even goes that far

822
00:28:46,640 --> 00:28:51,200
that you you've probably heard of all

823
00:28:49,120 --> 00:28:52,320
the spectre stuff it even goes that far

824
00:28:51,200 --> 00:28:56,000
to mitigate

825
00:28:52,320 --> 00:28:58,559
uh potential spectre issues um

826
00:28:56,000 --> 00:29:00,240
in by rewriting parts of the bpf program

827
00:28:58,559 --> 00:29:01,440
that are loaded but still to keep the

828
00:29:00,240 --> 00:29:03,200
logic the same

829
00:29:01,440 --> 00:29:05,039
so it's really doing a lot of work there

830
00:29:03,200 --> 00:29:08,640
to um

831
00:29:05,039 --> 00:29:08,640
make sure everything is sound safe

832
00:29:09,520 --> 00:29:13,840
other questions

833
00:29:15,760 --> 00:29:18,879
so while you're walking if you want some

834
00:29:17,279 --> 00:29:21,200
stickers there are also some ppf

835
00:29:18,880 --> 00:29:21,200
stickers

836
00:29:24,640 --> 00:29:30,000
have you done any performance comparison

837
00:29:26,799 --> 00:29:32,720
with the the good proxy free project

838
00:29:30,000 --> 00:29:34,320
when you have a small number of services

839
00:29:32,720 --> 00:29:35,039
but the number of cluster nodes

840
00:29:34,320 --> 00:29:38,720
increases

841
00:29:35,039 --> 00:29:40,000
like is there a performance gain there

842
00:29:38,720 --> 00:29:41,840
so when you have a small number of

843
00:29:40,000 --> 00:29:42,880
servers but the number of cluster nodes

844
00:29:41,840 --> 00:29:44,399
increases

845
00:29:42,880 --> 00:29:46,960
so there's still a performance gain

846
00:29:44,399 --> 00:29:48,879
because the um ip table so comes at a

847
00:29:46,960 --> 00:29:50,159
much later point in the networking stack

848
00:29:48,880 --> 00:29:52,720
compared to the tc

849
00:29:50,159 --> 00:29:54,399
ingress so dc inquest is basically run

850
00:29:52,720 --> 00:29:56,880
right after

851
00:29:54,399 --> 00:29:57,760
geo is executed so it's still earlier

852
00:29:56,880 --> 00:29:59,520
and also

853
00:29:57,760 --> 00:30:01,679
all the bpf stuff is just in time

854
00:29:59,520 --> 00:30:03,279
compiled it tries to void

855
00:30:01,679 --> 00:30:05,200
all the indirect calls as much as

856
00:30:03,279 --> 00:30:07,120
possible

857
00:30:05,200 --> 00:30:08,480
as opposed to iptables so there's it's

858
00:30:07,120 --> 00:30:11,840
still a better performance and there's

859
00:30:08,480 --> 00:30:13,360
still less overhead in the fast path

860
00:30:11,840 --> 00:30:23,840
okay we're out of time thank you very

861
00:30:13,360 --> 00:30:23,840
much thank you

862
00:30:24,880 --> 00:30:26,960
you

