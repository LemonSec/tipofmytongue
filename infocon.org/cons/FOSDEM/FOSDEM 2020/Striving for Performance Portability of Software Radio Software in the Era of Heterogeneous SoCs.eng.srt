1
00:00:05,440 --> 00:00:08,960
the next talk

2
00:00:06,319 --> 00:00:09,360
is uh jeffrey vetter is going to talk to

3
00:00:08,960 --> 00:00:12,799
us

4
00:00:09,360 --> 00:00:16,160
about making sdrs more portable in the

5
00:00:12,799 --> 00:00:18,640
area of heterogeneous stocks

6
00:00:16,160 --> 00:00:20,320
good afternoon everyone uh i hope you

7
00:00:18,640 --> 00:00:21,840
had a chance to go out and grab some

8
00:00:20,320 --> 00:00:24,080
lunch

9
00:00:21,840 --> 00:00:25,198
it's a it's an interesting experience

10
00:00:24,080 --> 00:00:27,359
here

11
00:00:25,199 --> 00:00:28,640
my colleague say young lee is with me

12
00:00:27,359 --> 00:00:30,800
and he's going to help me give

13
00:00:28,640 --> 00:00:32,239
a part of the talk so you've already

14
00:00:30,800 --> 00:00:32,719
heard several of these things this

15
00:00:32,238 --> 00:00:35,440
morning

16
00:00:32,719 --> 00:00:37,520
where where people are porting sdr to

17
00:00:35,440 --> 00:00:39,680
different heterogeneous architectures

18
00:00:37,520 --> 00:00:40,800
and i would argue that's that's not just

19
00:00:39,680 --> 00:00:42,559
a

20
00:00:40,800 --> 00:00:45,199
an sdr phenomenon we're going to see

21
00:00:42,559 --> 00:00:46,640
that everywhere at doe for example

22
00:00:45,200 --> 00:00:48,640
all of our next supercomputers are going

23
00:00:46,640 --> 00:00:51,440
to be gpu based and so

24
00:00:48,640 --> 00:00:52,399
we have lots of users struggling with

25
00:00:51,440 --> 00:00:55,440
how to

26
00:00:52,399 --> 00:00:56,840
port to these these architectures so

27
00:00:55,440 --> 00:01:00,000
we're part of a

28
00:00:56,840 --> 00:01:01,280
dsoc program and we're

29
00:01:00,000 --> 00:01:04,640
investigating the performance

30
00:01:01,280 --> 00:01:06,320
portability sdr we've done a lot of

31
00:01:04,640 --> 00:01:08,080
profiling and trying to understand

32
00:01:06,320 --> 00:01:09,679
target architectures and this really

33
00:01:08,080 --> 00:01:11,840
isn't different from what we do with our

34
00:01:09,680 --> 00:01:12,560
doe applications it's different software

35
00:01:11,840 --> 00:01:14,640
of course

36
00:01:12,560 --> 00:01:16,640
but we sit down we don't know

37
00:01:14,640 --> 00:01:17,280
astrophysics we don't know materials

38
00:01:16,640 --> 00:01:19,439
design

39
00:01:17,280 --> 00:01:20,560
we sit down with those people and try to

40
00:01:19,439 --> 00:01:22,798
learn how to

41
00:01:20,560 --> 00:01:24,479
to make their applications perform well

42
00:01:22,799 --> 00:01:26,960
on these architectures and so

43
00:01:24,479 --> 00:01:29,039
that's what we're doing here we're

44
00:01:26,960 --> 00:01:31,119
trying to use open programming models to

45
00:01:29,040 --> 00:01:32,320
do this

46
00:01:31,119 --> 00:01:34,640
one of the things that's really

47
00:01:32,320 --> 00:01:35,600
important is future proofing your

48
00:01:34,640 --> 00:01:37,520
application

49
00:01:35,600 --> 00:01:39,039
so you don't have to worry about new

50
00:01:37,520 --> 00:01:42,240
architectures

51
00:01:39,040 --> 00:01:43,360
needing to recode your your application

52
00:01:42,240 --> 00:01:45,920
because of that

53
00:01:43,360 --> 00:01:48,240
and then finally intelligent runtime

54
00:01:45,920 --> 00:01:49,840
systems we've heard a lot about earlier

55
00:01:48,240 --> 00:01:51,679
we think that's going to play an

56
00:01:49,840 --> 00:01:54,079
increasingly important role

57
00:01:51,680 --> 00:01:56,159
in addition to code generation and the

58
00:01:54,079 --> 00:01:58,798
performance

59
00:01:56,159 --> 00:02:01,119
of the applications and the goal that

60
00:01:58,799 --> 00:02:04,719
we're looking at is being able to take

61
00:02:01,119 --> 00:02:06,240
you know untouched the same application

62
00:02:04,719 --> 00:02:08,478
and running on a

63
00:02:06,240 --> 00:02:10,560
qualcomm snapdragon and run it on the

64
00:02:08,479 --> 00:02:12,080
largest supercomputer in the world

65
00:02:10,560 --> 00:02:14,239
and and just see if we can meet that

66
00:02:12,080 --> 00:02:17,360
goal now it may not may not happen but

67
00:02:14,239 --> 00:02:21,360
that's the challenge we're aiming for

68
00:02:17,360 --> 00:02:24,959
so just a short note about

69
00:02:21,360 --> 00:02:27,040
specialization so um i love this chart

70
00:02:24,959 --> 00:02:29,520
by ray kurzwal that talks about the

71
00:02:27,040 --> 00:02:32,319
growth of computing since uh

72
00:02:29,520 --> 00:02:33,519
1900 and it really shows you where we

73
00:02:32,319 --> 00:02:35,518
are today we're

74
00:02:33,519 --> 00:02:37,519
nearing the end there where we've had

75
00:02:35,519 --> 00:02:40,400
some outstanding gains from

76
00:02:37,519 --> 00:02:41,840
cmos scaling and other things now we're

77
00:02:40,400 --> 00:02:42,640
really getting into this transition

78
00:02:41,840 --> 00:02:45,280
period

79
00:02:42,640 --> 00:02:46,559
and i label it the sixth wave i think

80
00:02:45,280 --> 00:02:48,319
there's a lot of things are going to

81
00:02:46,560 --> 00:02:49,280
happen and we're already seeing a lot of

82
00:02:48,319 --> 00:02:51,200
this

83
00:02:49,280 --> 00:02:53,120
the first one is people are really

84
00:02:51,200 --> 00:02:55,440
specializing their

85
00:02:53,120 --> 00:02:57,840
applications and optimizing their code

86
00:02:55,440 --> 00:02:59,200
for the architectures we have

87
00:02:57,840 --> 00:03:01,360
the other one that i'm not going to talk

88
00:02:59,200 --> 00:03:02,399
about is emerging technologies you know

89
00:03:01,360 --> 00:03:03,599
have

90
00:03:02,400 --> 00:03:05,440
you seen all the excitement about

91
00:03:03,599 --> 00:03:07,920
quantum computing and neuromorphic

92
00:03:05,440 --> 00:03:10,239
computing and all these other things

93
00:03:07,920 --> 00:03:13,200
that's what what people are looking for

94
00:03:10,239 --> 00:03:15,040
really the next computational paradigm

95
00:03:13,200 --> 00:03:16,879
today i'm going to really focus on

96
00:03:15,040 --> 00:03:18,239
architectural specialization i think

97
00:03:16,879 --> 00:03:21,280
this is where we'll be

98
00:03:18,239 --> 00:03:22,080
for for perhaps 10 years or more where

99
00:03:21,280 --> 00:03:24,319
we take

100
00:03:22,080 --> 00:03:25,920
the the cmos which is an incredible

101
00:03:24,319 --> 00:03:28,560
technology

102
00:03:25,920 --> 00:03:29,359
and just specialize it for workloads and

103
00:03:28,560 --> 00:03:32,239
so that's

104
00:03:29,360 --> 00:03:33,680
that's the specialization part and it's

105
00:03:32,239 --> 00:03:35,440
really important that you get the

106
00:03:33,680 --> 00:03:37,840
specialization right

107
00:03:35,440 --> 00:03:39,680
now consequence of that is that the

108
00:03:37,840 --> 00:03:41,840
architecture start to yield

109
00:03:39,680 --> 00:03:42,959
very complex programming models and this

110
00:03:41,840 --> 00:03:44,480
schematic i use

111
00:03:42,959 --> 00:03:47,440
for talking about high performance

112
00:03:44,480 --> 00:03:50,000
computing in the sense that

113
00:03:47,440 --> 00:03:50,640
you know even if you are in astrophysics

114
00:03:50,000 --> 00:03:53,760
or

115
00:03:50,640 --> 00:03:54,640
material design or climate modeling your

116
00:03:53,760 --> 00:03:56,319
application

117
00:03:54,640 --> 00:03:59,279
these days has to be made up of this

118
00:03:56,319 --> 00:04:02,399
stack where you have maybe mpi

119
00:03:59,280 --> 00:04:05,040
some threading model on the smp and then

120
00:04:02,400 --> 00:04:05,680
if you're using an accelerator uh open

121
00:04:05,040 --> 00:04:08,640
acc

122
00:04:05,680 --> 00:04:10,959
cuda opencl something like that and it

123
00:04:08,640 --> 00:04:12,798
really varies across platforms

124
00:04:10,959 --> 00:04:14,560
cuda's not available everywhere of

125
00:04:12,799 --> 00:04:16,880
course it's proprietary

126
00:04:14,560 --> 00:04:19,040
other things like opencl may or may not

127
00:04:16,880 --> 00:04:20,880
be available

128
00:04:19,040 --> 00:04:22,560
and so when you look at it in that

129
00:04:20,880 --> 00:04:24,960
context really

130
00:04:22,560 --> 00:04:27,680
and this is this is generally for the

131
00:04:24,960 --> 00:04:29,520
dsoc program is first

132
00:04:27,680 --> 00:04:31,520
how do you design that architecture i

133
00:04:29,520 --> 00:04:34,320
show up on your doorstep and say i have

134
00:04:31,520 --> 00:04:36,639
10 billion transistors they're yours

135
00:04:34,320 --> 00:04:38,719
what do you want on your chip

136
00:04:36,639 --> 00:04:40,720
how do you analyze your workload and so

137
00:04:38,720 --> 00:04:42,400
part of our project focuses on looking

138
00:04:40,720 --> 00:04:44,720
at sdr to try to

139
00:04:42,400 --> 00:04:45,599
understand what what those features

140
00:04:44,720 --> 00:04:48,639
would be

141
00:04:45,600 --> 00:04:50,560
the second one is programmability

142
00:04:48,639 --> 00:04:52,880
you know how you know is it really

143
00:04:50,560 --> 00:04:54,880
possible to design an application with

144
00:04:52,880 --> 00:04:56,080
one language and programming model

145
00:04:54,880 --> 00:04:58,159
that will run across all these

146
00:04:56,080 --> 00:05:00,960
architectures and you get reasonable

147
00:04:58,160 --> 00:05:03,520
performance out of them

148
00:05:00,960 --> 00:05:04,000
that's you know a question but it's

149
00:05:03,520 --> 00:05:05,919
going to

150
00:05:04,000 --> 00:05:08,400
impact all these teams writing

151
00:05:05,919 --> 00:05:10,560
applications and software

152
00:05:08,400 --> 00:05:12,000
they only have so much time to spend on

153
00:05:10,560 --> 00:05:16,160
science and

154
00:05:12,000 --> 00:05:20,160
importing software and so this was

155
00:05:16,160 --> 00:05:21,600
um the the darpa program uh and it has

156
00:05:20,160 --> 00:05:22,800
you know three different areas some of

157
00:05:21,600 --> 00:05:24,320
them i've already mentioned and i'll

158
00:05:22,800 --> 00:05:26,320
talk more about in a moment

159
00:05:24,320 --> 00:05:27,840
but the important part here is you

160
00:05:26,320 --> 00:05:31,199
really do assume

161
00:05:27,840 --> 00:05:32,880
that you're going to have you know socs

162
00:05:31,199 --> 00:05:35,199
and they're going to be very diverse

163
00:05:32,880 --> 00:05:36,639
i'm talking way more than an armed cpu

164
00:05:35,199 --> 00:05:38,639
and an armed gpu

165
00:05:36,639 --> 00:05:40,000
they may have inference engines they may

166
00:05:38,639 --> 00:05:42,000
have fpgas

167
00:05:40,000 --> 00:05:43,199
and other things on them and and you've

168
00:05:42,000 --> 00:05:44,880
already heard about some of the

169
00:05:43,199 --> 00:05:45,199
complexity dealing with those when you

170
00:05:44,880 --> 00:05:48,560
just

171
00:05:45,199 --> 00:05:50,560
just have one and so

172
00:05:48,560 --> 00:05:53,280
the overall structure of our project

173
00:05:50,560 --> 00:05:55,360
really started out like this

174
00:05:53,280 --> 00:05:56,880
you know we were looking at applications

175
00:05:55,360 --> 00:06:00,000
we're using a

176
00:05:56,880 --> 00:06:02,960
ontology framework to try to capture

177
00:06:00,000 --> 00:06:05,520
the differences and the properties of

178
00:06:02,960 --> 00:06:07,198
things in sdr

179
00:06:05,520 --> 00:06:08,799
then the programming system is what

180
00:06:07,199 --> 00:06:10,560
seong's going to talk about this is

181
00:06:08,800 --> 00:06:11,840
really static analysis and code

182
00:06:10,560 --> 00:06:14,800
generation

183
00:06:11,840 --> 00:06:17,198
so if you're going to run you know your

184
00:06:14,800 --> 00:06:18,400
sdr module on an fpga you just heard a

185
00:06:17,199 --> 00:06:20,720
great talk about

186
00:06:18,400 --> 00:06:23,198
what all goes into that right is there

187
00:06:20,720 --> 00:06:25,840
any way we can simplify that

188
00:06:23,199 --> 00:06:26,639
and then next you've got the runtime

189
00:06:25,840 --> 00:06:28,400
system

190
00:06:26,639 --> 00:06:30,400
and and you know there's a big question

191
00:06:28,400 --> 00:06:30,880
there you know will lennox evolve to

192
00:06:30,400 --> 00:06:33,440
handle

193
00:06:30,880 --> 00:06:35,440
all these heterogeneous processors and

194
00:06:33,440 --> 00:06:37,440
memories that are coming its way

195
00:06:35,440 --> 00:06:38,719
or will that be handed off to another

196
00:06:37,440 --> 00:06:40,800
runtime system

197
00:06:38,720 --> 00:06:42,160
and we've developed a prototype runtime

198
00:06:40,800 --> 00:06:42,800
system that i'll tell you a little bit

199
00:06:42,160 --> 00:06:45,680
about

200
00:06:42,800 --> 00:06:46,639
that's trying to explore that space and

201
00:06:45,680 --> 00:06:50,319
then finally

202
00:06:46,639 --> 00:06:51,280
is the the hardware and and several of

203
00:06:50,319 --> 00:06:54,479
the teams

204
00:06:51,280 --> 00:06:57,039
in dsoc are actually building hardware

205
00:06:54,479 --> 00:06:58,960
we're not but we're reaching out and

206
00:06:57,039 --> 00:07:01,199
making use of some of the more complex

207
00:06:58,960 --> 00:07:04,239
systems out there

208
00:07:01,199 --> 00:07:05,680
um and this is just a kind of a timeline

209
00:07:04,240 --> 00:07:09,039
of how we see this

210
00:07:05,680 --> 00:07:12,240
manifesting itself over time um

211
00:07:09,039 --> 00:07:14,400
i'll skip that due to time so

212
00:07:12,240 --> 00:07:16,160
let's just talk a little bit about the

213
00:07:14,400 --> 00:07:19,280
range of hardware

214
00:07:16,160 --> 00:07:20,560
that we are thinking about so so as i

215
00:07:19,280 --> 00:07:23,679
mentioned earlier we

216
00:07:20,560 --> 00:07:26,400
we run on summit right now and this

217
00:07:23,680 --> 00:07:28,000
schematic you see it's a 200 petaflop

218
00:07:26,400 --> 00:07:31,120
system at oak ridge

219
00:07:28,000 --> 00:07:34,400
it's got 27

220
00:07:31,120 --> 00:07:35,919
000 voltage gpus in it and and they're

221
00:07:34,400 --> 00:07:38,638
all configured in this

222
00:07:35,919 --> 00:07:38,960
node schematic you see here where you've

223
00:07:38,639 --> 00:07:42,319
got

224
00:07:38,960 --> 00:07:44,400
six gpus and two power nines

225
00:07:42,319 --> 00:07:46,479
you know getting all those programming

226
00:07:44,400 --> 00:07:49,280
models that i was talking about to work

227
00:07:46,479 --> 00:07:50,318
properly and efficiently on that is very

228
00:07:49,280 --> 00:07:51,840
complex

229
00:07:50,319 --> 00:07:53,919
and then you add the fact that you've

230
00:07:51,840 --> 00:07:55,919
got thousands of other nodes like this

231
00:07:53,919 --> 00:07:57,840
that are communicating at the same time

232
00:07:55,919 --> 00:08:01,198
and potentially doing i o

233
00:07:57,840 --> 00:08:03,119
and it's very dynamic uh a very dynamic

234
00:08:01,199 --> 00:08:05,440
system

235
00:08:03,120 --> 00:08:06,960
we just heard a nice talk about fpgas

236
00:08:05,440 --> 00:08:10,160
and being able to use them

237
00:08:06,960 --> 00:08:11,359
for for signal processing we're looking

238
00:08:10,160 --> 00:08:12,960
at those

239
00:08:11,360 --> 00:08:14,960
and then more recently we've been

240
00:08:12,960 --> 00:08:18,159
looking at nvidia's

241
00:08:14,960 --> 00:08:20,239
jetson and so this is uh again it's an

242
00:08:18,160 --> 00:08:21,120
soc but it's very much an aggressive

243
00:08:20,240 --> 00:08:25,360
design

244
00:08:21,120 --> 00:08:28,720
it has arm cores it has a voltage gpus

245
00:08:25,360 --> 00:08:31,039
smaller than what you get on a v100

246
00:08:28,720 --> 00:08:31,919
it's got a deep learning accelerator and

247
00:08:31,039 --> 00:08:35,199
and some other

248
00:08:31,919 --> 00:08:37,199
things like vision processors and and

249
00:08:35,200 --> 00:08:38,800
vision processing accelerators

250
00:08:37,200 --> 00:08:40,560
you know that's when you get right down

251
00:08:38,799 --> 00:08:42,240
to it that's what we think the future is

252
00:08:40,559 --> 00:08:44,800
going to look like we're going to have

253
00:08:42,240 --> 00:08:46,240
you know thousands of skus of different

254
00:08:44,800 --> 00:08:48,160
types of processors

255
00:08:46,240 --> 00:08:49,920
and you're just going to buy the one

256
00:08:48,160 --> 00:08:53,279
that fits your

257
00:08:49,920 --> 00:08:54,000
your niche and then of course there's

258
00:08:53,279 --> 00:08:55,600
snapdragon

259
00:08:54,000 --> 00:08:57,440
this is the one that we're focusing on

260
00:08:55,600 --> 00:08:59,600
this year and and it

261
00:08:57,440 --> 00:09:01,120
isn't is not as well known but more

262
00:08:59,600 --> 00:09:03,120
popular than

263
00:09:01,120 --> 00:09:04,959
than the xavier that i just mentioned

264
00:09:03,120 --> 00:09:06,080
because they're they're cell phone chips

265
00:09:04,959 --> 00:09:08,880
right

266
00:09:06,080 --> 00:09:10,160
they're very complex they've got arm big

267
00:09:08,880 --> 00:09:13,040
and little cores

268
00:09:10,160 --> 00:09:16,240
they have vector accelerators they've

269
00:09:13,040 --> 00:09:18,240
had a tensor accelerator like a

270
00:09:16,240 --> 00:09:20,080
training engine and then different

271
00:09:18,240 --> 00:09:21,360
codecs and things like that in addition

272
00:09:20,080 --> 00:09:24,560
to all the i o for co

273
00:09:21,360 --> 00:09:28,720
connectivity to 5g and so on so

274
00:09:24,560 --> 00:09:31,839
it's it's a very growing space and it

275
00:09:28,720 --> 00:09:33,440
seems to be accelerating

276
00:09:31,839 --> 00:09:35,519
so just a word about some of the

277
00:09:33,440 --> 00:09:37,360
application studies that we're doing

278
00:09:35,519 --> 00:09:39,200
and we have some tools that we'll make

279
00:09:37,360 --> 00:09:41,120
available if you're interested in

280
00:09:39,200 --> 00:09:44,480
looking at those

281
00:09:41,120 --> 00:09:47,680
so when we put together apps

282
00:09:44,480 --> 00:09:50,080
we're you know we're not experts in sdr

283
00:09:47,680 --> 00:09:51,439
we're learning a lot and so we wanted to

284
00:09:50,080 --> 00:09:53,600
go out and get some

285
00:09:51,440 --> 00:09:55,040
workflows that would allow us to to

286
00:09:53,600 --> 00:09:58,000
understand

287
00:09:55,040 --> 00:09:59,680
how to profile and how to analyze these

288
00:09:58,000 --> 00:10:01,200
we started with the wi-fi one

289
00:09:59,680 --> 00:10:02,399
and it's in part because we really

290
00:10:01,200 --> 00:10:04,079
understand how to measure the

291
00:10:02,399 --> 00:10:06,800
performance of that

292
00:10:04,079 --> 00:10:08,239
on a on a application level you know if

293
00:10:06,800 --> 00:10:10,079
you do a file transfer and you're

294
00:10:08,240 --> 00:10:12,240
running it across this flow

295
00:10:10,079 --> 00:10:13,199
you can get some real feedback as to

296
00:10:12,240 --> 00:10:16,560
error rates

297
00:10:13,200 --> 00:10:18,640
as well as throughput and so

298
00:10:16,560 --> 00:10:19,599
that's uh that's something that we've

299
00:10:18,640 --> 00:10:22,720
been studying

300
00:10:19,600 --> 00:10:24,640
um we've also built tools and

301
00:10:22,720 --> 00:10:26,560
and these tools that really focus on the

302
00:10:24,640 --> 00:10:28,480
ontologies and the performance analysis

303
00:10:26,560 --> 00:10:30,319
that i mentioned earlier

304
00:10:28,480 --> 00:10:33,279
the you know the first few we're really

305
00:10:30,320 --> 00:10:35,040
looking at all the flows in the system

306
00:10:33,279 --> 00:10:36,640
and the later ones are more for

307
00:10:35,040 --> 00:10:38,719
profiling that so we built some

308
00:10:36,640 --> 00:10:40,959
frameworks that let you

309
00:10:38,720 --> 00:10:43,200
put these flows up run them for say 30

310
00:10:40,959 --> 00:10:44,800
seconds and then do a profile

311
00:10:43,200 --> 00:10:47,760
of where you're spending your time in

312
00:10:44,800 --> 00:10:49,680
the flows

313
00:10:47,760 --> 00:10:52,160
this is an example of of where some of

314
00:10:49,680 --> 00:10:54,239
the time is going

315
00:10:52,160 --> 00:10:55,439
and you know it's it's pretty much a

316
00:10:54,240 --> 00:10:57,279
normal profile

317
00:10:55,440 --> 00:10:59,120
the you know one of the questions is you

318
00:10:57,279 --> 00:11:00,000
know are we getting representative flow

319
00:10:59,120 --> 00:11:04,000
graphs and

320
00:11:00,000 --> 00:11:06,959
you know is 30 seconds really sufficient

321
00:11:04,000 --> 00:11:07,360
the cooler looking analysis is this

322
00:11:06,959 --> 00:11:09,359
block

323
00:11:07,360 --> 00:11:11,600
proximity analysis and so what we did

324
00:11:09,360 --> 00:11:12,880
was look at sea grant and some of the

325
00:11:11,600 --> 00:11:15,600
other

326
00:11:12,880 --> 00:11:17,120
repos we had access to and what you see

327
00:11:15,600 --> 00:11:19,120
in this graph over here is really a

328
00:11:17,120 --> 00:11:20,880
networking diagram where the nodes

329
00:11:19,120 --> 00:11:22,560
represent basically how many times

330
00:11:20,880 --> 00:11:25,600
they're used and the

331
00:11:22,560 --> 00:11:28,239
and the weights on the edges represent

332
00:11:25,600 --> 00:11:29,120
connections between those and workflows

333
00:11:28,240 --> 00:11:31,120
so

334
00:11:29,120 --> 00:11:32,480
and this is useful for example if you're

335
00:11:31,120 --> 00:11:35,279
trying to combine

336
00:11:32,480 --> 00:11:37,279
operators that you can put on a on an

337
00:11:35,279 --> 00:11:38,800
soc that's coming down the line or maybe

338
00:11:37,279 --> 00:11:43,839
optimize

339
00:11:38,800 --> 00:11:43,839
by creating a new module for example

340
00:11:44,240 --> 00:11:47,839
and that was i let me see if i got the

341
00:11:46,640 --> 00:11:49,760
uh yeah so

342
00:11:47,839 --> 00:11:51,920
all of these tools are available down

343
00:11:49,760 --> 00:11:55,360
here on github

344
00:11:51,920 --> 00:11:57,680
we'd love feedback on them so

345
00:11:55,360 --> 00:11:59,600
next working our way down uh the

346
00:11:57,680 --> 00:12:01,359
programming systems is where we've been

347
00:11:59,600 --> 00:12:02,320
spending a lot of time recently as well

348
00:12:01,360 --> 00:12:04,399
as the runtime

349
00:12:02,320 --> 00:12:11,839
system so i'm going to let sayong tell

350
00:12:04,399 --> 00:12:11,839
you about our programming strategy

351
00:12:13,600 --> 00:12:17,519
yeah i'll talk about the pruning system

352
00:12:15,360 --> 00:12:19,600
that we used to pull the gradient of

353
00:12:17,519 --> 00:12:21,279
loss to the heterogeneous devices this

354
00:12:19,600 --> 00:12:22,639
gives the overview of the system as you

355
00:12:21,279 --> 00:12:24,800
can see we use

356
00:12:22,639 --> 00:12:27,040
two compilers the base one one is the

357
00:12:24,800 --> 00:12:29,120
the open arg and here the llbm openoc is

358
00:12:27,040 --> 00:12:31,040
the homegrown compiler developed by us

359
00:12:29,120 --> 00:12:33,680
and we use the multiple programming

360
00:12:31,040 --> 00:12:37,040
models like uh the openmp openhc

361
00:12:33,680 --> 00:12:39,279
iris and heap cuda opencl so

362
00:12:37,040 --> 00:12:40,480
by looking at this figure it may not be

363
00:12:39,279 --> 00:12:41,839
to understand what's going on in the

364
00:12:40,480 --> 00:12:44,160
flowing system so let's look at the

365
00:12:41,839 --> 00:12:46,560
concrete example

366
00:12:44,160 --> 00:12:48,560
so this shows how we pulled your

367
00:12:46,560 --> 00:12:50,000
coordinate blocks to the headline device

368
00:12:48,560 --> 00:12:52,239
using our framework

369
00:12:50,000 --> 00:12:53,760
so basically what we assume is that user

370
00:12:52,240 --> 00:12:55,440
can write a program

371
00:12:53,760 --> 00:12:57,760
using the high-level programs like

372
00:12:55,440 --> 00:12:58,320
openmp opencc in this example suppose

373
00:12:57,760 --> 00:12:59,760
that

374
00:12:58,320 --> 00:13:02,880
user writes a community block using

375
00:12:59,760 --> 00:13:05,200
openhcg then our compiler performed

376
00:13:02,880 --> 00:13:06,720
sources translation and generated output

377
00:13:05,200 --> 00:13:07,680
program depending on the target adapter

378
00:13:06,720 --> 00:13:09,440
that means that

379
00:13:07,680 --> 00:13:11,199
if you want to run your own connected

380
00:13:09,440 --> 00:13:13,519
plug on the nvidia gpu then

381
00:13:11,200 --> 00:13:15,680
our compiler generate the output task

382
00:13:13,519 --> 00:13:17,920
and it will run on the nvidia gpu and

383
00:13:15,680 --> 00:13:20,239
if you want to run your the blocks on

384
00:13:17,920 --> 00:13:20,560
the for example arm or other geometry

385
00:13:20,240 --> 00:13:22,959
like

386
00:13:20,560 --> 00:13:24,800
cpu then our pro compiler will

387
00:13:22,959 --> 00:13:26,079
automatically generate output openmp

388
00:13:24,800 --> 00:13:27,439
program and running on the dojo

389
00:13:26,079 --> 00:13:29,599
architecture likewise

390
00:13:27,440 --> 00:13:31,279
depending on the target architecture our

391
00:13:29,600 --> 00:13:31,839
compiler automatically generate output

392
00:13:31,279 --> 00:13:33,920
kernels

393
00:13:31,839 --> 00:13:35,920
and one good thing about our framework

394
00:13:33,920 --> 00:13:38,079
is that no matter how different type of

395
00:13:35,920 --> 00:13:39,760
output models are used we use a common

396
00:13:38,079 --> 00:13:41,439
runtime called iris that's

397
00:13:39,760 --> 00:13:42,880
another runtime system that we developed

398
00:13:41,440 --> 00:13:44,800
by ourselves so

399
00:13:42,880 --> 00:13:46,079
one benefit of having common runtime

400
00:13:44,800 --> 00:13:48,880
interfaces that

401
00:13:46,079 --> 00:13:50,239
you can write application where one task

402
00:13:48,880 --> 00:13:52,560
is running on the gpu using

403
00:13:50,240 --> 00:13:53,839
cuda and another test running on the cpu

404
00:13:52,560 --> 00:13:55,599
using openmp and

405
00:13:53,839 --> 00:13:57,279
yet another task running on the fpga

406
00:13:55,600 --> 00:13:58,800
using openshift something like that so

407
00:13:57,279 --> 00:14:00,639
the kind of true intimidating of the

408
00:13:58,800 --> 00:14:02,800
multiple different output models are

409
00:14:00,639 --> 00:14:05,279
possible by using our framework

410
00:14:02,800 --> 00:14:06,000
and we also support several other new

411
00:14:05,279 --> 00:14:08,480
one like a

412
00:14:06,000 --> 00:14:09,040
ship another tpu frame model developed

413
00:14:08,480 --> 00:14:11,760
by the

414
00:14:09,040 --> 00:14:13,680
the amd we also support like another new

415
00:14:11,760 --> 00:14:17,439
one called the sql but i will not talk

416
00:14:13,680 --> 00:14:19,120
detail about these new phone models

417
00:14:17,440 --> 00:14:21,199
and for example if you want to put your

418
00:14:19,120 --> 00:14:25,120
coordinator blocks on the jbl like

419
00:14:21,199 --> 00:14:29,279
soc then we can use the the cuda for the

420
00:14:25,120 --> 00:14:31,279
jbl gpu and open it for the jb cpu

421
00:14:29,279 --> 00:14:32,959
and this shows the the code structure of

422
00:14:31,279 --> 00:14:34,480
the openness block as you know the

423
00:14:32,959 --> 00:14:37,439
original opening

424
00:14:34,480 --> 00:14:38,000
gr blocks are written as a c plus plus

425
00:14:37,440 --> 00:14:40,240
class

426
00:14:38,000 --> 00:14:42,560
and we follow the same structure that

427
00:14:40,240 --> 00:14:45,839
means that if you want to write your own

428
00:14:42,560 --> 00:14:48,000
openhcc gel block then you have to

429
00:14:45,839 --> 00:14:49,360
write this type of class but one common

430
00:14:48,000 --> 00:14:52,160
requirement is that

431
00:14:49,360 --> 00:14:54,800
every opennessy block should inherit the

432
00:14:52,160 --> 00:14:55,439
common the parent class called the glhcc

433
00:14:54,800 --> 00:14:57,279
basic

434
00:14:55,440 --> 00:14:59,040
block what it does is that it

435
00:14:57,279 --> 00:14:59,680
initialized the data structure used by

436
00:14:59,040 --> 00:15:02,319
the open

437
00:14:59,680 --> 00:15:02,959
openness runtime and it also assigned a

438
00:15:02,320 --> 00:15:05,839
unique

439
00:15:02,959 --> 00:15:06,479
logical thread id to each block instance

440
00:15:05,839 --> 00:15:08,639
actually

441
00:15:06,480 --> 00:15:10,560
the reason why we need the kind of the

442
00:15:08,639 --> 00:15:12,560
parent class is because the

443
00:15:10,560 --> 00:15:13,680
multi-threading issue as you know that

444
00:15:12,560 --> 00:15:16,719
is learned in the in

445
00:15:13,680 --> 00:15:18,160
the morning session original kinetic

446
00:15:16,720 --> 00:15:20,560
framework provide multi-stream that

447
00:15:18,160 --> 00:15:22,079
means that multiple gr blocks can

448
00:15:20,560 --> 00:15:22,638
execute by the multiple different

449
00:15:22,079 --> 00:15:24,800
threats

450
00:15:22,639 --> 00:15:26,000
but the problem is that our openness

451
00:15:24,800 --> 00:15:28,240
runtime also supports

452
00:15:26,000 --> 00:15:29,920
multi-threading but the problem is that

453
00:15:28,240 --> 00:15:32,160
original coordinator block

454
00:15:29,920 --> 00:15:33,759
the framework used uh the c plus plus

455
00:15:32,160 --> 00:15:35,040
the boost threading library based

456
00:15:33,759 --> 00:15:37,759
multi-threading while

457
00:15:35,040 --> 00:15:38,880
openac runtime is the the open based

458
00:15:37,759 --> 00:15:41,120
voltage threading so

459
00:15:38,880 --> 00:15:43,199
two systems does not know each other so

460
00:15:41,120 --> 00:15:44,079
two to handle the multi-selected safety

461
00:15:43,199 --> 00:15:46,079
issue by

462
00:15:44,079 --> 00:15:47,359
integrating two different programming so

463
00:15:46,079 --> 00:15:49,040
what we did is that

464
00:15:47,360 --> 00:15:51,680
we introduced the concept of the logical

465
00:15:49,040 --> 00:15:54,000
thread by using the logical thread id

466
00:15:51,680 --> 00:15:55,199
that opens runtime node what kind of the

467
00:15:54,000 --> 00:15:57,600
thread local data

468
00:15:55,199 --> 00:15:59,439
structure should be used so it's a low

469
00:15:57,600 --> 00:16:00,560
level detail about how to enforce the

470
00:15:59,440 --> 00:16:02,560
stress safety

471
00:16:00,560 --> 00:16:04,079
when we integrate multiple systems but i

472
00:16:02,560 --> 00:16:07,199
don't explain all the details but

473
00:16:04,079 --> 00:16:09,599
anyway in this open hcc block

474
00:16:07,199 --> 00:16:11,359
we provide two implementation one is the

475
00:16:09,600 --> 00:16:12,720
reference cp implementation which is

476
00:16:11,360 --> 00:16:13,759
exactly the same as the original

477
00:16:12,720 --> 00:16:16,000
kundalini block

478
00:16:13,759 --> 00:16:17,440
and openness implementation that's the

479
00:16:16,000 --> 00:16:18,320
openness version of the reference

480
00:16:17,440 --> 00:16:20,320
provision

481
00:16:18,320 --> 00:16:21,360
if you write the basic openness

482
00:16:20,320 --> 00:16:23,199
implementation

483
00:16:21,360 --> 00:16:25,680
it will perform the three type of task

484
00:16:23,199 --> 00:16:27,439
first at the beginning of the invocation

485
00:16:25,680 --> 00:16:29,359
you should copy the data from the host

486
00:16:27,440 --> 00:16:31,120
memory to device memory and

487
00:16:29,360 --> 00:16:33,040
launch the device kernel and at the end

488
00:16:31,120 --> 00:16:37,199
of the execution we have to copy the

489
00:16:33,040 --> 00:16:39,360
result back to the host memory

490
00:16:37,199 --> 00:16:40,959
this shows the example translation of

491
00:16:39,360 --> 00:16:42,720
the openness block as you can see in the

492
00:16:40,959 --> 00:16:45,920
left it shows the simple openness

493
00:16:42,720 --> 00:16:48,399
block for the the gl log module

494
00:16:45,920 --> 00:16:50,160
what we did here is that in addition in

495
00:16:48,399 --> 00:16:51,759
on top of the existing cp implementation

496
00:16:50,160 --> 00:16:52,959
we just add one line of the openness

497
00:16:51,759 --> 00:16:54,800
iterative then

498
00:16:52,959 --> 00:16:56,319
our compiler automatically generates the

499
00:16:54,800 --> 00:16:57,439
output host program and the kernel

500
00:16:56,320 --> 00:17:01,120
program depending on the

501
00:16:57,440 --> 00:17:02,399
target architecture and this shows the

502
00:17:01,120 --> 00:17:05,439
example workflow

503
00:17:02,399 --> 00:17:08,240
where we used our own open htr block

504
00:17:05,439 --> 00:17:10,000
so as you can see here type workflow use

505
00:17:08,240 --> 00:17:12,480
our gr openhash block

506
00:17:10,000 --> 00:17:14,480
on the other hand param workflow use the

507
00:17:12,480 --> 00:17:16,240
original reference gl blocks

508
00:17:14,480 --> 00:17:17,760
so the only difference is that both

509
00:17:16,240 --> 00:17:20,000
vocabulary the same thing

510
00:17:17,760 --> 00:17:23,520
but one use the openness block the other

511
00:17:20,000 --> 00:17:26,480
is the reference cpu implementation

512
00:17:23,520 --> 00:17:27,679
and this shows the the basic management

513
00:17:26,480 --> 00:17:30,400
scheme for the openness

514
00:17:27,679 --> 00:17:33,039
enabled gr workflow so because the

515
00:17:30,400 --> 00:17:33,039
openstack button not

516
00:17:44,960 --> 00:17:48,400
after finishing available device corner

517
00:17:46,559 --> 00:17:50,240
execution then we have to write data

518
00:17:48,400 --> 00:17:52,080
back to host but in this case there are

519
00:17:50,240 --> 00:17:54,400
some inefficiency for example

520
00:17:52,080 --> 00:17:55,360
if you look at the openness of blog on

521
00:17:54,400 --> 00:17:57,679
blog tool

522
00:17:55,360 --> 00:18:00,000
actually if you look at the green

523
00:17:57,679 --> 00:18:00,720
colored data we have three copies and

524
00:18:00,000 --> 00:18:02,960
there are some

525
00:18:00,720 --> 00:18:04,160
data transfers between host and gpu but

526
00:18:02,960 --> 00:18:07,200
we don't have to do that

527
00:18:04,160 --> 00:18:09,600
because the second the openness block

528
00:18:07,200 --> 00:18:10,480
also runs on the same device indicated

529
00:18:09,600 --> 00:18:12,000
we don't have to do

530
00:18:10,480 --> 00:18:13,600
unnecessary data transfer between

531
00:18:12,000 --> 00:18:16,720
hosting the gpu so

532
00:18:13,600 --> 00:18:18,559
what we did is that if we know that both

533
00:18:16,720 --> 00:18:20,320
producer blocks and consumer blocks

534
00:18:18,559 --> 00:18:20,799
running on the same device then we can

535
00:18:20,320 --> 00:18:22,399
reduce

536
00:18:20,799 --> 00:18:25,039
unnecessary memory transfer between host

537
00:18:22,400 --> 00:18:25,039
and the device

538
00:18:26,640 --> 00:18:30,559
and this shows the example output of the

539
00:18:28,480 --> 00:18:32,240
the sample workflow

540
00:18:30,559 --> 00:18:34,240
and let's look at the performance

541
00:18:32,240 --> 00:18:35,919
comparison what we did here is that

542
00:18:34,240 --> 00:18:38,640
we compared the performance of the

543
00:18:35,919 --> 00:18:40,400
openhcg block based workflow versus the

544
00:18:38,640 --> 00:18:41,360
original cpu implementation based

545
00:18:40,400 --> 00:18:44,400
workflow

546
00:18:41,360 --> 00:18:46,479
then in this figure red

547
00:18:44,400 --> 00:18:47,840
the block means that each openness is

548
00:18:46,480 --> 00:18:49,600
blocked and

549
00:18:47,840 --> 00:18:51,039
green blank means it's reference cp

550
00:18:49,600 --> 00:18:52,799
implementation so so if

551
00:18:51,039 --> 00:18:54,559
the both blocks label the same thing

552
00:18:52,799 --> 00:18:57,200
that means that it does the same

553
00:18:54,559 --> 00:18:59,039
algorithm but one is implemented openhcc

554
00:18:57,200 --> 00:19:02,000
the other implemented reference cpu

555
00:18:59,039 --> 00:19:02,400
and here what we did is that we run the

556
00:19:02,000 --> 00:19:04,640
both

557
00:19:02,400 --> 00:19:05,679
openc workflow and the representative

558
00:19:04,640 --> 00:19:09,520
workflow on the

559
00:19:05,679 --> 00:19:11,600
same cpu as i said before

560
00:19:09,520 --> 00:19:12,879
two implementation reference cpu

561
00:19:11,600 --> 00:19:14,559
implementation and openness

562
00:19:12,880 --> 00:19:17,200
implementation so if we

563
00:19:14,559 --> 00:19:19,039
target the cpu then even openhcc blocks

564
00:19:17,200 --> 00:19:21,280
will just use the simple the original

565
00:19:19,039 --> 00:19:22,240
represent cp implementation so in this

566
00:19:21,280 --> 00:19:24,720
case we have to

567
00:19:22,240 --> 00:19:27,360
see that both open hd block and the

568
00:19:24,720 --> 00:19:29,200
reference cpu block should run equally

569
00:19:27,360 --> 00:19:31,600
but as you can see in the for example

570
00:19:29,200 --> 00:19:33,520
d2k do you want to d2k

571
00:19:31,600 --> 00:19:34,959
our openness blocks a little bit

572
00:19:33,520 --> 00:19:36,320
performed better than the reference cpu

573
00:19:34,960 --> 00:19:38,559
block because when we implement the

574
00:19:36,320 --> 00:19:39,120
electron cpu version inside the openness

575
00:19:38,559 --> 00:19:40,720
block

576
00:19:39,120 --> 00:19:42,159
we applied very simple caching

577
00:19:40,720 --> 00:19:45,039
optimization that causes some

578
00:19:42,160 --> 00:19:45,039
performance difference

579
00:19:45,120 --> 00:19:48,639
let's look at the next case in this case

580
00:19:47,360 --> 00:19:51,039
what we did is that

581
00:19:48,640 --> 00:19:53,120
we uploaded the openness block again on

582
00:19:51,039 --> 00:19:54,879
the cpu but using the opening at the

583
00:19:53,120 --> 00:19:56,719
back in the parameter that means that

584
00:19:54,880 --> 00:19:58,960
instead of using the reference cpu

585
00:19:56,720 --> 00:20:00,400
implementation openness block use the

586
00:19:58,960 --> 00:20:03,520
openmp implementation of

587
00:20:00,400 --> 00:20:06,240
that block so here we can see

588
00:20:03,520 --> 00:20:07,520
in some blocks openhcc plus performs

589
00:20:06,240 --> 00:20:09,520
better than the reference cpu

590
00:20:07,520 --> 00:20:13,200
information but in other blocks like

591
00:20:09,520 --> 00:20:14,559
the b and c still reference cpu version

592
00:20:13,200 --> 00:20:17,360
performs better on the opening

593
00:20:14,559 --> 00:20:19,120
block why because some of the original

594
00:20:17,360 --> 00:20:20,879
grenade implementation was

595
00:20:19,120 --> 00:20:22,399
characterized using the fork library

596
00:20:20,880 --> 00:20:24,880
that means here what we compared

597
00:20:22,400 --> 00:20:26,799
that the the version parallelized in

598
00:20:24,880 --> 00:20:28,960
openmp multi-threading versus the

599
00:20:26,799 --> 00:20:31,280
version factorized in the vocal library

600
00:20:28,960 --> 00:20:32,240
so let me show that depending on the

601
00:20:31,280 --> 00:20:33,600
characteristic

602
00:20:32,240 --> 00:20:35,280
sometimes it's better to use the

603
00:20:33,600 --> 00:20:35,918
vectorization and sometimes it's better

604
00:20:35,280 --> 00:20:38,879
to use

605
00:20:35,919 --> 00:20:38,880
the multi threading

606
00:20:38,960 --> 00:20:43,200
and next what we did here is that we

607
00:20:41,600 --> 00:20:45,600
offload the openness blocks

608
00:20:43,200 --> 00:20:46,720
gpu but without any optimization about

609
00:20:45,600 --> 00:20:48,639
the memory transfer

610
00:20:46,720 --> 00:20:50,880
as you can see here even though we

611
00:20:48,640 --> 00:20:52,799
offload the computation to the gpu

612
00:20:50,880 --> 00:20:54,880
because of the extra over for the memory

613
00:20:52,799 --> 00:20:57,760
transfer between hosts and the device

614
00:20:54,880 --> 00:20:58,799
in most cases gpu performed worse than

615
00:20:57,760 --> 00:20:59,919
the cp version

616
00:20:58,799 --> 00:21:02,639
because of the memory change for

617
00:20:59,919 --> 00:21:04,400
overhead but when we applied our memory

618
00:21:02,640 --> 00:21:04,880
transfer optimization we could remove

619
00:21:04,400 --> 00:21:06,799
some

620
00:21:04,880 --> 00:21:09,520
unnecessary memory transfer so in the

621
00:21:06,799 --> 00:21:11,200
case in most cases openness

622
00:21:09,520 --> 00:21:13,360
block performed better than cpu block

623
00:21:11,200 --> 00:21:16,640
but one there is one exception

624
00:21:13,360 --> 00:21:18,158
block a in this case still openness

625
00:21:16,640 --> 00:21:19,919
and the openness performance works in

626
00:21:18,159 --> 00:21:20,720
the reference cpu it is because the

627
00:21:19,919 --> 00:21:22,799
original

628
00:21:20,720 --> 00:21:24,640
cpu implementation was factorized during

629
00:21:22,799 --> 00:21:27,840
the folk library that means that

630
00:21:24,640 --> 00:21:29,039
still we compare against vectorization

631
00:21:27,840 --> 00:21:31,039
on cpu versus

632
00:21:29,039 --> 00:21:32,799
paralyzed on gpu so depending on the

633
00:21:31,039 --> 00:21:34,480
computation it may be still

634
00:21:32,799 --> 00:21:36,720
better to run on the cpu in the

635
00:21:34,480 --> 00:21:38,880
vectorization but in most other cases

636
00:21:36,720 --> 00:21:39,919
if you we can upload the computation to

637
00:21:38,880 --> 00:21:41,679
the gpu and

638
00:21:39,919 --> 00:21:43,520
optimize the unnecessary memory transfer

639
00:21:41,679 --> 00:21:46,000
then we can be the problems of the

640
00:21:43,520 --> 00:21:47,520
original reference cpu

641
00:21:46,000 --> 00:21:49,200
and this is a little bit more

642
00:21:47,520 --> 00:21:52,000
compressible i'll skip that

643
00:21:49,200 --> 00:21:52,480
and so what we can learn from here is

644
00:21:52,000 --> 00:21:54,799
that

645
00:21:52,480 --> 00:21:57,039
once you create your blocks using the

646
00:21:54,799 --> 00:21:58,559
openhcg and create a workflow using

647
00:21:57,039 --> 00:22:00,559
openhc blocks then

648
00:21:58,559 --> 00:22:02,240
you can create one workflow that can run

649
00:22:00,559 --> 00:22:03,520
on the multiple different type of device

650
00:22:02,240 --> 00:22:06,159
together that means that

651
00:22:03,520 --> 00:22:08,240
part of the your blocks run on the gpu

652
00:22:06,159 --> 00:22:09,840
and part of your block run on the cpu

653
00:22:08,240 --> 00:22:11,520
another part of blocked on the fpga the

654
00:22:09,840 --> 00:22:13,520
kind of true mixing of the

655
00:22:11,520 --> 00:22:16,559
the heterogeneous devices possible if

656
00:22:13,520 --> 00:22:16,559
you use this framework

657
00:22:16,720 --> 00:22:20,960
thank you see young so uh just one thing

658
00:22:19,360 --> 00:22:24,479
i want to say about that diagram

659
00:22:20,960 --> 00:22:27,280
um we modified the new radio so that we

660
00:22:24,480 --> 00:22:30,080
could add some attributes

661
00:22:27,280 --> 00:22:31,440
to the blocks that actually let you say

662
00:22:30,080 --> 00:22:34,320
where to co-locate

663
00:22:31,440 --> 00:22:36,320
the the blocks ideally that would happen

664
00:22:34,320 --> 00:22:37,918
automatically somewhere along the way

665
00:22:36,320 --> 00:22:40,080
so there'd be a phase where you could

666
00:22:37,919 --> 00:22:42,240
you could basically unify some of those

667
00:22:40,080 --> 00:22:44,399
blocks into a super block and then put

668
00:22:42,240 --> 00:22:47,120
them on an fpga for example

669
00:22:44,400 --> 00:22:48,480
and so we're running a little short on

670
00:22:47,120 --> 00:22:51,039
time i want to

671
00:22:48,480 --> 00:22:52,480
tell you about iris so iris is our

672
00:22:51,039 --> 00:22:55,200
runtime system

673
00:22:52,480 --> 00:22:56,960
and so um we've looked around there's a

674
00:22:55,200 --> 00:22:59,679
lot of runtime systems out there

675
00:22:56,960 --> 00:23:01,360
we we didn't find one that ran on all

676
00:22:59,679 --> 00:23:03,440
the platforms with all the different

677
00:23:01,360 --> 00:23:05,918
programming models we wanted to run

678
00:23:03,440 --> 00:23:06,960
for the reason seyong mentioned you know

679
00:23:05,919 --> 00:23:08,720
some

680
00:23:06,960 --> 00:23:10,159
nodes just don't support certain

681
00:23:08,720 --> 00:23:12,320
programming models well

682
00:23:10,159 --> 00:23:14,480
and so in some cases openmp is a great

683
00:23:12,320 --> 00:23:17,678
i'm sorry opencl is a great choice

684
00:23:14,480 --> 00:23:20,000
on things like xavier opencl is not not

685
00:23:17,679 --> 00:23:22,720
there so you have to use kudo

686
00:23:20,000 --> 00:23:23,360
and openmp and so we created this to

687
00:23:22,720 --> 00:23:25,760
help both

688
00:23:23,360 --> 00:23:26,879
orchestrate the data movement and launch

689
00:23:25,760 --> 00:23:30,000
launch and manage

690
00:23:26,880 --> 00:23:33,440
tasks across a diverse set of system

691
00:23:30,000 --> 00:23:34,159
uh architectures um we have several

692
00:23:33,440 --> 00:23:36,640
different

693
00:23:34,159 --> 00:23:37,840
models here uh there we have a memory

694
00:23:36,640 --> 00:23:40,320
model

695
00:23:37,840 --> 00:23:41,918
that also will track with a directory

696
00:23:40,320 --> 00:23:43,039
where your data is in these different

697
00:23:41,919 --> 00:23:45,919
devices

698
00:23:43,039 --> 00:23:49,279
and inject data movement into the

699
00:23:45,919 --> 00:23:51,520
execution of the tasks if necessary

700
00:23:49,279 --> 00:23:52,320
and it really looks it looks like a dag

701
00:23:51,520 --> 00:23:53,760
so

702
00:23:52,320 --> 00:23:55,678
you know everything in computer science

703
00:23:53,760 --> 00:23:56,799
at some point seems to get back to dags

704
00:23:55,679 --> 00:23:58,640
but uh

705
00:23:56,799 --> 00:24:00,240
that that seems to be working for us

706
00:23:58,640 --> 00:24:02,400
we're not talking about millions of

707
00:24:00,240 --> 00:24:04,159
nodes we're talking about dozens or

708
00:24:02,400 --> 00:24:06,480
maybe a few hundred uh

709
00:24:04,159 --> 00:24:08,080
nodes in our dag and then and then

710
00:24:06,480 --> 00:24:09,279
sayong talked about the programming

711
00:24:08,080 --> 00:24:10,879
models

712
00:24:09,279 --> 00:24:12,400
here's an example of what we're running

713
00:24:10,880 --> 00:24:14,240
across today so

714
00:24:12,400 --> 00:24:16,840
with iris you can take it and run it on

715
00:24:14,240 --> 00:24:20,559
this node for example that has

716
00:24:16,840 --> 00:24:22,639
xeon voltas i'm sorry pascals and

717
00:24:20,559 --> 00:24:24,080
stratix 10 in it you can run it on a

718
00:24:22,640 --> 00:24:26,159
summit node

719
00:24:24,080 --> 00:24:29,120
using the requisite programming model on

720
00:24:26,159 --> 00:24:32,320
each chip and then it runs on

721
00:24:29,120 --> 00:24:33,918
radeon xavier and snapdragon and they

722
00:24:32,320 --> 00:24:36,639
list out we list out what

723
00:24:33,919 --> 00:24:38,080
what software stack we use to do that i

724
00:24:36,640 --> 00:24:40,000
don't have a demo i didn't think i had

725
00:24:38,080 --> 00:24:41,039
time for it but here's snapshots of it

726
00:24:40,000 --> 00:24:42,960
booting on

727
00:24:41,039 --> 00:24:45,360
all of those platforms and you can get a

728
00:24:42,960 --> 00:24:47,360
feel for for how it does that

729
00:24:45,360 --> 00:24:50,080
um here it's you know identifying that

730
00:24:47,360 --> 00:24:51,918
it has the cuda platform available

731
00:24:50,080 --> 00:24:53,840
and loading the shared object for that

732
00:24:51,919 --> 00:24:57,440
and skipping things like hip

733
00:24:53,840 --> 00:24:57,439
and uh and opencl

734
00:24:57,600 --> 00:25:01,039
so the task scheduling is interesting

735
00:24:59,520 --> 00:25:03,600
many of you already know about task

736
00:25:01,039 --> 00:25:05,600
scheduling so

737
00:25:03,600 --> 00:25:07,039
the thing i want to mention here is that

738
00:25:05,600 --> 00:25:09,120
you know we're actually getting

739
00:25:07,039 --> 00:25:10,960
the fun part now we've got this nice

740
00:25:09,120 --> 00:25:12,840
framework that has many mechanisms in it

741
00:25:10,960 --> 00:25:15,279
to let us schedule things

742
00:25:12,840 --> 00:25:17,360
but we want to start exploring the

743
00:25:15,279 --> 00:25:19,039
device selection policies now

744
00:25:17,360 --> 00:25:21,039
you could do a simple thing like have a

745
00:25:19,039 --> 00:25:24,158
hint a pragma that says

746
00:25:21,039 --> 00:25:25,840
run all of these tasks on a gpu but we

747
00:25:24,159 --> 00:25:28,400
could also look at things like the

748
00:25:25,840 --> 00:25:30,240
profile based or the ontology based or

749
00:25:28,400 --> 00:25:31,440
or one of the original ideas we looked

750
00:25:30,240 --> 00:25:33,679
at was

751
00:25:31,440 --> 00:25:35,600
using a performance model to give you a

752
00:25:33,679 --> 00:25:36,880
rough indication of where a task should

753
00:25:35,600 --> 00:25:39,360
be performed

754
00:25:36,880 --> 00:25:40,799
and so and then and then once you have

755
00:25:39,360 --> 00:25:43,360
that the task scheduler

756
00:25:40,799 --> 00:25:45,039
executes that on the ready queue every

757
00:25:43,360 --> 00:25:47,520
time

758
00:25:45,039 --> 00:25:48,799
i've got a simple example here of a

759
00:25:47,520 --> 00:25:50,639
saxby

760
00:25:48,799 --> 00:25:52,000
and this is running on the xavier using

761
00:25:50,640 --> 00:25:55,760
the gpus and the

762
00:25:52,000 --> 00:25:56,640
arm cpus basically what we want to do is

763
00:25:55,760 --> 00:26:00,158
run

764
00:25:56,640 --> 00:26:00,880
sax beyond the the the a times x on the

765
00:26:00,159 --> 00:26:04,080
gpu

766
00:26:00,880 --> 00:26:05,840
and the y on the cpu and so one of the

767
00:26:04,080 --> 00:26:08,639
things that i mentioned was that we we

768
00:26:05,840 --> 00:26:10,399
do have dependencies

769
00:26:08,640 --> 00:26:12,159
i'm running short so i'm going to speed

770
00:26:10,400 --> 00:26:14,000
through this there's a c

771
00:26:12,159 --> 00:26:16,080
plus plus interface to iris and there's

772
00:26:14,000 --> 00:26:17,600
a python interface tyrus the python

773
00:26:16,080 --> 00:26:20,480
interface is right here

774
00:26:17,600 --> 00:26:21,039
it's a lot easier to explain you create

775
00:26:20,480 --> 00:26:24,880
you know

776
00:26:21,039 --> 00:26:27,919
data regions with iris then you you can

777
00:26:24,880 --> 00:26:30,000
launch these tasks and these tasks

778
00:26:27,919 --> 00:26:31,360
point to code for example right there's

779
00:26:30,000 --> 00:26:34,240
the cuda code

780
00:26:31,360 --> 00:26:35,760
that gets launched when iris submits

781
00:26:34,240 --> 00:26:37,200
this task and it's called up for

782
00:26:35,760 --> 00:26:40,000
execution

783
00:26:37,200 --> 00:26:41,279
and you have to do the transfers and an

784
00:26:40,000 --> 00:26:43,679
identification of

785
00:26:41,279 --> 00:26:46,000
you know how big the data is there's

786
00:26:43,679 --> 00:26:48,960
also the openmp version

787
00:26:46,000 --> 00:26:50,960
we have to execute a little pre and post

788
00:26:48,960 --> 00:26:53,200
execution code for these

789
00:26:50,960 --> 00:26:55,760
but we think we can automate that with

790
00:26:53,200 --> 00:26:58,640
things like open art

791
00:26:55,760 --> 00:26:59,679
and then here's the execution uh the

792
00:26:58,640 --> 00:27:03,679
memory management

793
00:26:59,679 --> 00:27:05,279
so in this diagram it's not it doesn't

794
00:27:03,679 --> 00:27:06,720
really stand out very much but

795
00:27:05,279 --> 00:27:09,039
basically what happens is when you

796
00:27:06,720 --> 00:27:11,840
identify these read write dependencies

797
00:27:09,039 --> 00:27:13,520
on the different data regions you're

798
00:27:11,840 --> 00:27:16,959
basically telling iris

799
00:27:13,520 --> 00:27:18,480
look there's a potential inconsistency

800
00:27:16,960 --> 00:27:21,120
there and so you need to

801
00:27:18,480 --> 00:27:21,679
to manage that so it has a directory and

802
00:27:21,120 --> 00:27:24,719
it

803
00:27:21,679 --> 00:27:27,600
it knows to go in and move the data

804
00:27:24,720 --> 00:27:30,080
from one device to another based on the

805
00:27:27,600 --> 00:27:32,959
task being completed

806
00:27:30,080 --> 00:27:34,000
all right i'll jump forward let me uh so

807
00:27:32,960 --> 00:27:35,760
the other thing was just

808
00:27:34,000 --> 00:27:37,440
that we're looking at efficiency we're

809
00:27:35,760 --> 00:27:39,840
trying to get get this

810
00:27:37,440 --> 00:27:42,000
execute where we can do almost 90 000

811
00:27:39,840 --> 00:27:44,000
tasks per second on both the cpu and the

812
00:27:42,000 --> 00:27:46,000
gpu

813
00:27:44,000 --> 00:27:48,559
it's online so feel free to take a look

814
00:27:46,000 --> 00:27:50,399
at it and let us know what you think

815
00:27:48,559 --> 00:27:51,918
and with that i'd like to thank you know

816
00:27:50,399 --> 00:27:55,199
fostom and sdr

817
00:27:51,919 --> 00:27:56,640
and darpa and doe for for funding and

818
00:27:55,200 --> 00:28:03,840
inspiring our work

819
00:27:56,640 --> 00:28:03,840
thank you

820
00:28:04,880 --> 00:28:06,960
you

