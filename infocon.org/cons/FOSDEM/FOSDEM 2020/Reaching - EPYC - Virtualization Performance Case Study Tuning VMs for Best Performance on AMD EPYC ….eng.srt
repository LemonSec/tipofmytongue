1
00:00:05,279 --> 00:00:10,000
right

2
00:00:06,000 --> 00:00:10,480
hi everyone and um i am dario uh thanks

3
00:00:10,000 --> 00:00:13,599
for

4
00:00:10,480 --> 00:00:15,280
staying until now until the very last

5
00:00:13,599 --> 00:00:17,039
session of the day at least in this dev

6
00:00:15,280 --> 00:00:20,560
room and

7
00:00:17,039 --> 00:00:22,480
coming seeing this talk about

8
00:00:20,560 --> 00:00:24,000
maximizing the performance of a virtual

9
00:00:22,480 --> 00:00:25,519
machine or rather of

10
00:00:24,000 --> 00:00:27,160
workloads running inside virtual

11
00:00:25,519 --> 00:00:30,560
machines which

12
00:00:27,160 --> 00:00:33,120
oftentimes is simplified as

13
00:00:30,560 --> 00:00:33,760
i mean i do the vcpu pinning and then

14
00:00:33,120 --> 00:00:37,199
i'm done

15
00:00:33,760 --> 00:00:38,960
i'm not i well it's not

16
00:00:37,200 --> 00:00:40,719
for that's the book of it actually but

17
00:00:38,960 --> 00:00:43,680
there are a

18
00:00:40,719 --> 00:00:44,399
couple of more things at least in my

19
00:00:43,680 --> 00:00:47,760
opinion

20
00:00:44,399 --> 00:00:49,360
and let's start having a very quick look

21
00:00:47,760 --> 00:00:52,800
at these

22
00:00:49,360 --> 00:00:53,280
class of processors of cpus which are

23
00:00:52,800 --> 00:00:56,718
the

24
00:00:53,280 --> 00:00:58,800
cpus from amd the so-called

25
00:00:56,719 --> 00:01:00,399
epic or epic 2 because it's the second

26
00:00:58,800 --> 00:01:03,799
generation of

27
00:01:00,399 --> 00:01:06,799
these architecture family

28
00:01:03,799 --> 00:01:10,320
702 series

29
00:01:06,799 --> 00:01:13,520
which is which are multi-chip

30
00:01:10,320 --> 00:01:16,720
modules composed of nine dyes

31
00:01:13,520 --> 00:01:20,158
that's how they call it the

32
00:01:16,720 --> 00:01:22,798
these things one of these dyes

33
00:01:20,159 --> 00:01:24,400
is dedicated to io and of chip

34
00:01:22,799 --> 00:01:26,320
communications

35
00:01:24,400 --> 00:01:27,759
in every socket i mean there are nine of

36
00:01:26,320 --> 00:01:29,679
these dyes in every socket and one is

37
00:01:27,759 --> 00:01:33,040
dedicated to ios the iodi

38
00:01:29,680 --> 00:01:35,600
and the other eight uh ones

39
00:01:33,040 --> 00:01:36,799
are the actual compute dies then there

40
00:01:35,600 --> 00:01:40,320
is the concept of

41
00:01:36,799 --> 00:01:44,320
core complex ccx which is basically

42
00:01:40,320 --> 00:01:47,039
a set a combination of four cores

43
00:01:44,320 --> 00:01:47,600
which it which also means eight threads

44
00:01:47,040 --> 00:01:50,399
because

45
00:01:47,600 --> 00:01:51,280
these processors have hyper threading

46
00:01:50,399 --> 00:01:54,399
and uh

47
00:01:51,280 --> 00:01:56,799
we will look into this a little bit uh

48
00:01:54,399 --> 00:01:57,600
more detail later each car complex has

49
00:01:56,799 --> 00:02:00,000
its own

50
00:01:57,600 --> 00:02:01,360
uh l one two l three uh cache uh

51
00:02:00,000 --> 00:02:04,159
hierarchy

52
00:02:01,360 --> 00:02:06,079
uh but yeah uh we'll see about this

53
00:02:04,159 --> 00:02:07,680
later let's also introduce the concept

54
00:02:06,079 --> 00:02:10,799
of core complex

55
00:02:07,680 --> 00:02:13,920
die or ccd which is basically two

56
00:02:10,800 --> 00:02:17,200
cc axes and so eight cores

57
00:02:13,920 --> 00:02:19,440
16 threads each ccd this is the

58
00:02:17,200 --> 00:02:20,640
important part this is the important

59
00:02:19,440 --> 00:02:23,840
thing about ccds

60
00:02:20,640 --> 00:02:26,079
each ccds ccd has

61
00:02:23,840 --> 00:02:26,959
dedicated infinity fabric it's the name

62
00:02:26,080 --> 00:02:30,319
of the technology

63
00:02:26,959 --> 00:02:33,920
link to the iodi right and so

64
00:02:30,319 --> 00:02:37,679
these are processors that can have up to

65
00:02:33,920 --> 00:02:38,640
64 cores which means 128 threads and you

66
00:02:37,680 --> 00:02:42,160
can have them

67
00:02:38,640 --> 00:02:45,200
in two socket arrangements so

68
00:02:42,160 --> 00:02:47,280
uh yeah and each socket has eight memory

69
00:02:45,200 --> 00:02:49,280
channel

70
00:02:47,280 --> 00:02:50,640
for two towards the memory so yeah these

71
00:02:49,280 --> 00:02:51,599
are links if you download these lights

72
00:02:50,640 --> 00:02:54,238
and

73
00:02:51,599 --> 00:02:55,760
navigate them to uh fetch more

74
00:02:54,239 --> 00:02:58,000
information but you can easily

75
00:02:55,760 --> 00:02:59,760
find a lot more information on google so

76
00:02:58,000 --> 00:03:01,840
i said that this talk is going about

77
00:02:59,760 --> 00:03:03,518
it's going to be about tuning

78
00:03:01,840 --> 00:03:04,879
virtualization and workload learning in

79
00:03:03,519 --> 00:03:07,680
some workloads at least

80
00:03:04,879 --> 00:03:08,319
running inside virtual machines we will

81
00:03:07,680 --> 00:03:11,280
i will

82
00:03:08,319 --> 00:03:11,920
uh say a few things most of which are

83
00:03:11,280 --> 00:03:15,040
going to be

84
00:03:11,920 --> 00:03:18,238
general enough but we will

85
00:03:15,040 --> 00:03:20,480
use a case study throughout the talk and

86
00:03:18,239 --> 00:03:20,480
so

87
00:03:20,879 --> 00:03:27,280
i will speak

88
00:03:24,159 --> 00:03:27,280
about these

89
00:03:27,360 --> 00:03:31,200
efforts this work that we did together

90
00:03:29,920 --> 00:03:35,679
as

91
00:03:31,200 --> 00:03:39,119
suse with amd as a partner

92
00:03:35,680 --> 00:03:42,319
on coming up with a set of tuning advice

93
00:03:39,120 --> 00:03:43,120
uh for optimizing the performance of one

94
00:03:42,319 --> 00:03:46,720
of our

95
00:03:43,120 --> 00:03:49,280
suse products the linux enterprise

96
00:03:46,720 --> 00:03:51,200
server 15 sp1 which is pretty much the

97
00:03:49,280 --> 00:03:56,159
same

98
00:03:51,200 --> 00:03:56,159
as opensuse elip 15.1

99
00:03:56,720 --> 00:04:00,480
on this class of amd processors so i

100
00:03:59,439 --> 00:04:02,720
will

101
00:04:00,480 --> 00:04:04,159
use these as a case study that's

102
00:04:02,720 --> 00:04:07,359
important to

103
00:04:04,159 --> 00:04:08,959
to to to say and to remember right so

104
00:04:07,360 --> 00:04:11,840
this is um

105
00:04:08,959 --> 00:04:13,680
another way to look at that a one

106
00:04:11,840 --> 00:04:15,439
specific instance of

107
00:04:13,680 --> 00:04:16,799
the series of processors that i

108
00:04:15,439 --> 00:04:19,839
introduced before

109
00:04:16,798 --> 00:04:23,198
the 7742 whatever

110
00:04:19,839 --> 00:04:25,119
it's a big one uh it's one of

111
00:04:23,199 --> 00:04:27,040
it's it's the biggest setup the one that

112
00:04:25,120 --> 00:04:31,040
i said that has

113
00:04:27,040 --> 00:04:34,080
64 cores uh 100 which means 128 threads

114
00:04:31,040 --> 00:04:36,240
and uh comes into sockets

115
00:04:34,080 --> 00:04:38,080
and this is the one that we used for the

116
00:04:36,240 --> 00:04:42,000
this guide and the one that i'm

117
00:04:38,080 --> 00:04:45,520
going to to refer to for this talk uh

118
00:04:42,000 --> 00:04:49,360
close up on one um

119
00:04:45,520 --> 00:04:52,960
ccx uh this is what i was seeing so

120
00:04:49,360 --> 00:04:56,160
each ccx has

121
00:04:52,960 --> 00:04:58,960
its own l3 cache and of course each

122
00:04:56,160 --> 00:05:01,759
core also has dedicated l1 and n2 as

123
00:04:58,960 --> 00:05:04,880
usual but the fact that there is

124
00:05:01,759 --> 00:05:07,440
an l3 cache per ccx and

125
00:05:04,880 --> 00:05:08,960
for example not pneuma node it's a

126
00:05:07,440 --> 00:05:11,280
little bit well it's not weird

127
00:05:08,960 --> 00:05:13,039
a but it's something specific about this

128
00:05:11,280 --> 00:05:13,520
architecture something which is at least

129
00:05:13,039 --> 00:05:16,560
very

130
00:05:13,520 --> 00:05:18,880
different than many other

131
00:05:16,560 --> 00:05:19,759
architectures that you find around at

132
00:05:18,880 --> 00:05:22,880
least in the

133
00:05:19,759 --> 00:05:26,000
x86 world and

134
00:05:22,880 --> 00:05:28,719
yeah tuning the performance basically

135
00:05:26,000 --> 00:05:29,759
mean uh if you really want to achieve to

136
00:05:28,720 --> 00:05:32,320
try to achieve

137
00:05:29,759 --> 00:05:32,800
performance inside vms which with the

138
00:05:32,320 --> 00:05:35,199
that

139
00:05:32,800 --> 00:05:36,240
matches the one that you would get on

140
00:05:35,199 --> 00:05:38,160
bare metal

141
00:05:36,240 --> 00:05:40,000
then it also means static partitioning

142
00:05:38,160 --> 00:05:43,199
you cannot avoid doing

143
00:05:40,000 --> 00:05:45,280
uh at least some of some of that

144
00:05:43,199 --> 00:05:47,120
and that it makes that still make sense

145
00:05:45,280 --> 00:05:47,679
to speak about virtualization then if we

146
00:05:47,120 --> 00:05:51,039
have to

147
00:05:47,680 --> 00:05:53,120
partition resources statically well uh

148
00:05:51,039 --> 00:05:55,120
yes according to me at least especially

149
00:05:53,120 --> 00:05:55,759
on such a large platform because you can

150
00:05:55,120 --> 00:05:58,560
still

151
00:05:55,759 --> 00:05:59,360
use it first for a server consolidation

152
00:05:58,560 --> 00:06:03,280
because it's so

153
00:05:59,360 --> 00:06:05,280
huge that you can put a lot of vms on it

154
00:06:03,280 --> 00:06:06,799
and then you have the argument about

155
00:06:05,280 --> 00:06:08,400
flexibility and

156
00:06:06,800 --> 00:06:09,840
high availability and other stuff so

157
00:06:08,400 --> 00:06:11,440
what resources are we talking about

158
00:06:09,840 --> 00:06:15,359
partitioning well all the

159
00:06:11,440 --> 00:06:19,120
relevant resources cpu memory and io

160
00:06:15,360 --> 00:06:22,880
this talk will focus

161
00:06:19,120 --> 00:06:26,000
focus focusing on cpu and memory

162
00:06:22,880 --> 00:06:28,960
i o uh leave it for another another one

163
00:06:26,000 --> 00:06:30,240
uh so the first kind of partition is for

164
00:06:28,960 --> 00:06:33,520
partitioning sorry

165
00:06:30,240 --> 00:06:35,039
is going to be between aust and guest or

166
00:06:33,520 --> 00:06:36,960
guests

167
00:06:35,039 --> 00:06:39,120
meaning that you want to you most of the

168
00:06:36,960 --> 00:06:40,239
time want to leave some of the resources

169
00:06:39,120 --> 00:06:43,039
uh

170
00:06:40,240 --> 00:06:43,520
namely some cpus and some memory to the

171
00:06:43,039 --> 00:06:46,479
host

172
00:06:43,520 --> 00:06:47,680
because you have to connect with ssh or

173
00:06:46,479 --> 00:06:50,719
whatever to the host

174
00:06:47,680 --> 00:06:54,000
to do monitoring or management

175
00:06:50,720 --> 00:06:55,919
uh you uh and then they and then

176
00:06:54,000 --> 00:06:57,680
oftentimes um depending on the

177
00:06:55,919 --> 00:07:00,799
configuration but i mean

178
00:06:57,680 --> 00:07:04,479
most of the time uh just us

179
00:07:00,800 --> 00:07:06,639
to carry out some activity on the alf or

180
00:07:04,479 --> 00:07:07,840
to help let's say like let's say like

181
00:07:06,639 --> 00:07:11,520
that the

182
00:07:07,840 --> 00:07:14,880
vms for for example for doing io

183
00:07:11,520 --> 00:07:17,280
running the qm your threads whatever

184
00:07:14,880 --> 00:07:17,919
recommendation really depends on what

185
00:07:17,280 --> 00:07:21,440
are the

186
00:07:17,919 --> 00:07:22,719
your actual goals um one good rule of

187
00:07:21,440 --> 00:07:26,080
thumb is to leave

188
00:07:22,720 --> 00:07:27,120
at least one core per socket to host

189
00:07:26,080 --> 00:07:30,159
activities

190
00:07:27,120 --> 00:07:32,160
also on this particular architecture

191
00:07:30,160 --> 00:07:33,680
it will be better if you would if you

192
00:07:32,160 --> 00:07:37,440
manage not to break

193
00:07:33,680 --> 00:07:40,720
uh for example what we said before uh

194
00:07:37,440 --> 00:07:44,000
to be the ccx because of

195
00:07:40,720 --> 00:07:44,879
otherwise you will have the vms or some

196
00:07:44,000 --> 00:07:49,039
of the mems

197
00:07:44,879 --> 00:07:50,879
and the hosts sharing l3 hashes which

198
00:07:49,039 --> 00:07:52,719
it's generally not something that you

199
00:07:50,879 --> 00:07:54,720
want for good performances

200
00:07:52,720 --> 00:07:56,560
if possible you would also try to not

201
00:07:54,720 --> 00:07:57,599
break you should also try to not break a

202
00:07:56,560 --> 00:08:00,160
ccd but then

203
00:07:57,599 --> 00:08:00,639
that would mean leaving eight core 16

204
00:08:00,160 --> 00:08:02,879
threads

205
00:08:00,639 --> 00:08:03,919
for the for the host which you may or

206
00:08:02,879 --> 00:08:07,840
may not

207
00:08:03,919 --> 00:08:10,400
uh want wanting to to want to do

208
00:08:07,840 --> 00:08:11,840
and that arms really how much memory to

209
00:08:10,400 --> 00:08:12,479
leave to the host it really depends

210
00:08:11,840 --> 00:08:16,000
let's say

211
00:08:12,479 --> 00:08:19,440
50 gigabytes and be done with it

212
00:08:16,000 --> 00:08:21,360
so another thing huge pages so whether

213
00:08:19,440 --> 00:08:22,000
or not use huge pages and how to use

214
00:08:21,360 --> 00:08:24,000
them

215
00:08:22,000 --> 00:08:25,280
typically and this is one of the general

216
00:08:24,000 --> 00:08:27,120
thing this is

217
00:08:25,280 --> 00:08:29,039
really general about virtualization not

218
00:08:27,120 --> 00:08:30,319
on not really specific about this

219
00:08:29,039 --> 00:08:32,479
platform

220
00:08:30,319 --> 00:08:34,159
if you if possible you always want to

221
00:08:32,479 --> 00:08:35,839
use huge pages for the virtual machine

222
00:08:34,159 --> 00:08:38,640
but you don't want to use

223
00:08:35,839 --> 00:08:40,159
the transf them in the transparent huge

224
00:08:38,640 --> 00:08:41,598
pages way let's say you want to

225
00:08:40,159 --> 00:08:44,560
pre-allocate the huge pages

226
00:08:41,599 --> 00:08:46,959
at boot time of the host and then use

227
00:08:44,560 --> 00:08:50,079
them for uh

228
00:08:46,959 --> 00:08:53,599
as the backing of the memory of the vms

229
00:08:50,080 --> 00:08:55,279
and and you don't want to have

230
00:08:53,600 --> 00:08:56,640
automatic number balancing at the east

231
00:08:55,279 --> 00:08:58,320
level because you have

232
00:08:56,640 --> 00:09:00,160
uh you you are doing you're going to to

233
00:08:58,320 --> 00:09:01,680
do static partitioning anyway

234
00:09:00,160 --> 00:09:03,519
in the guest it depends it depends on

235
00:09:01,680 --> 00:09:06,000
the worker that you run on the guest

236
00:09:03,519 --> 00:09:08,080
it's not different than tuning a

237
00:09:06,000 --> 00:09:09,440
workload on bare method from this point

238
00:09:08,080 --> 00:09:10,800
of view once you have tuned in there's

239
00:09:09,440 --> 00:09:12,480
then inside the vm

240
00:09:10,800 --> 00:09:15,599
you just treat the problem like you

241
00:09:12,480 --> 00:09:18,640
would do it on a bare metal machine

242
00:09:15,600 --> 00:09:20,959
similar to the vm that you are

243
00:09:18,640 --> 00:09:22,480
focusing on and one word about power

244
00:09:20,959 --> 00:09:25,920
management at the lowest

245
00:09:22,480 --> 00:09:29,120
level of course uh again it depends um

246
00:09:25,920 --> 00:09:31,519
in general it's good to uh do at least

247
00:09:29,120 --> 00:09:33,920
some benchmarks

248
00:09:31,519 --> 00:09:35,600
limiting for example the deep sleep

249
00:09:33,920 --> 00:09:37,760
states and using uh

250
00:09:35,600 --> 00:09:39,279
like performance as a cpu fragment

251
00:09:37,760 --> 00:09:42,560
because it will help you

252
00:09:39,279 --> 00:09:45,600
get a first uh set of results which

253
00:09:42,560 --> 00:09:48,959
are consistent and that and

254
00:09:45,600 --> 00:09:51,279
that don't vary too much then

255
00:09:48,959 --> 00:09:53,359
it depends whether this is uh okay for

256
00:09:51,279 --> 00:09:56,560
you and for your

257
00:09:53,360 --> 00:09:59,760
actual goals to uh keep these

258
00:09:56,560 --> 00:10:01,279
settings or if saving a little bit more

259
00:09:59,760 --> 00:10:03,279
of power is important

260
00:10:01,279 --> 00:10:05,839
and if it is you have to reassess the

261
00:10:03,279 --> 00:10:08,800
tuning and rerun in the benchmarks

262
00:10:05,839 --> 00:10:10,880
and so on and so forth with the proper

263
00:10:08,800 --> 00:10:12,959
power management configuration

264
00:10:10,880 --> 00:10:13,920
that you that you want to have let's say

265
00:10:12,959 --> 00:10:17,359
in production

266
00:10:13,920 --> 00:10:20,959
then as i said pinning the vcpus

267
00:10:17,360 --> 00:10:23,680
we want to do that and we do that

268
00:10:20,959 --> 00:10:24,560
in for example libvert with i mean like

269
00:10:23,680 --> 00:10:28,800
this

270
00:10:24,560 --> 00:10:31,279
and you want to if possible

271
00:10:28,800 --> 00:10:33,680
and i was already touching on this

272
00:10:31,279 --> 00:10:36,880
before if possible you want to pin the

273
00:10:33,680 --> 00:10:40,079
vcpus of the vms in such a way that

274
00:10:36,880 --> 00:10:43,360
you pinned to the ccds

275
00:10:40,079 --> 00:10:46,160
because in such a way you won't have

276
00:10:43,360 --> 00:10:46,640
two different vms which will have to

277
00:10:46,160 --> 00:10:49,040
share

278
00:10:46,640 --> 00:10:50,079
the boundaries of the infinity fabric

279
00:10:49,040 --> 00:10:53,339
link

280
00:10:50,079 --> 00:10:54,719
from the ccd to the iodide

281
00:10:53,340 --> 00:10:56,880
[Music]

282
00:10:54,720 --> 00:10:58,560
this means that if you do that you will

283
00:10:56,880 --> 00:11:00,959
be able to

284
00:10:58,560 --> 00:11:01,920
you will be able to configure like that

285
00:11:00,959 --> 00:11:04,880
up to either

286
00:11:01,920 --> 00:11:06,240
14 or 16 vms it depends on how many cpus

287
00:11:04,880 --> 00:11:08,480
you leave to the host

288
00:11:06,240 --> 00:11:09,519
on an fx2 platform like the one i showed

289
00:11:08,480 --> 00:11:12,959
at the beginning

290
00:11:09,519 --> 00:11:15,200
and if it's not possible to pin

291
00:11:12,959 --> 00:11:16,880
the ccd level then you may consider

292
00:11:15,200 --> 00:11:18,880
pinning at the ccx level

293
00:11:16,880 --> 00:11:20,000
because again yeah you that then the vm

294
00:11:18,880 --> 00:11:22,800
will will share

295
00:11:20,000 --> 00:11:23,839
the boundaries of the infinite fabric

296
00:11:22,800 --> 00:11:25,439
link to the iodi

297
00:11:23,839 --> 00:11:27,120
but at least they don't share the l3

298
00:11:25,440 --> 00:11:29,519
hashes

299
00:11:27,120 --> 00:11:31,519
and at worst at least be into course and

300
00:11:29,519 --> 00:11:34,880
don't make vms shares uh

301
00:11:31,519 --> 00:11:36,959
cores and uh uh executes on

302
00:11:34,880 --> 00:11:38,959
on sibling cipher thread and also share

303
00:11:36,959 --> 00:11:41,920
l1 and then to caches unless

304
00:11:38,959 --> 00:11:42,399
you really want to ask for big troubles

305
00:11:41,920 --> 00:11:45,680
uh

306
00:11:42,399 --> 00:11:48,720
memory placement similar to vcpus but

307
00:11:45,680 --> 00:11:50,319
simpler even simpler probably because if

308
00:11:48,720 --> 00:11:53,680
the vm

309
00:11:50,320 --> 00:11:57,600
that you want to use is big enough

310
00:11:53,680 --> 00:12:00,719
uh to um span to take

311
00:11:57,600 --> 00:12:02,639
both the pneuma nodes then you put half

312
00:12:00,720 --> 00:12:04,160
of the memory of the vm in one mode

313
00:12:02,639 --> 00:12:05,839
and the other alpha on the other and

314
00:12:04,160 --> 00:12:08,000
then you also uh

315
00:12:05,839 --> 00:12:09,600
i guess yeah i have this in the next

316
00:12:08,000 --> 00:12:13,839
right

317
00:12:09,600 --> 00:12:15,519
so let's yeah sorry and then uh in the

318
00:12:13,839 --> 00:12:17,839
in the other case uh which is when the

319
00:12:15,519 --> 00:12:19,760
vm is not large enough

320
00:12:17,839 --> 00:12:21,360
to spawn both the new node and it's uh

321
00:12:19,760 --> 00:12:23,519
it fits in

322
00:12:21,360 --> 00:12:24,720
uh just one of the one odds then you put

323
00:12:23,519 --> 00:12:27,440
all its memory in that

324
00:12:24,720 --> 00:12:28,240
mode as simple as that and then

325
00:12:27,440 --> 00:12:29,519
enlightenment

326
00:12:28,240 --> 00:12:31,440
that's what i wanted to try to say

327
00:12:29,519 --> 00:12:34,079
before uh if

328
00:12:31,440 --> 00:12:35,760
the vm spans both of the neumann odd

329
00:12:34,079 --> 00:12:38,160
then you have yes put it the

330
00:12:35,760 --> 00:12:39,439
half of this memory put sorry one half

331
00:12:38,160 --> 00:12:40,800
of this memory and one node and the

332
00:12:39,440 --> 00:12:44,000
other alpha on the other

333
00:12:40,800 --> 00:12:46,479
but you also have to provide to the vm

334
00:12:44,000 --> 00:12:49,040
a suitable and meaningful virtual

335
00:12:46,480 --> 00:12:49,040
topology

336
00:12:49,360 --> 00:12:57,120
virtual new methodology uh if it doesn't

337
00:12:53,120 --> 00:13:00,000
uh you're fine you just enforce that

338
00:12:57,120 --> 00:13:01,760
the uh the memory stays on one one node

339
00:13:00,000 --> 00:13:05,040
but you still have to provide

340
00:13:01,760 --> 00:13:06,000
in both cases a meaningful uh cpu

341
00:13:05,040 --> 00:13:08,800
topology

342
00:13:06,000 --> 00:13:09,600
so v12 sockets threads cores stuff like

343
00:13:08,800 --> 00:13:13,519
that

344
00:13:09,600 --> 00:13:16,240
and also um good let's say cpu model

345
00:13:13,519 --> 00:13:16,880
uh what does it mean good we will see in

346
00:13:16,240 --> 00:13:20,240
a few

347
00:13:16,880 --> 00:13:22,160
few slides yeah then security

348
00:13:20,240 --> 00:13:23,440
secure encrypted virtualization amd and

349
00:13:22,160 --> 00:13:25,920
this series processor

350
00:13:23,440 --> 00:13:27,680
also provides a feature which basically

351
00:13:25,920 --> 00:13:29,920
allows you to encrypt the memory of the

352
00:13:27,680 --> 00:13:32,239
virtual machines

353
00:13:29,920 --> 00:13:34,399
and it's transparent to the vms it's

354
00:13:32,240 --> 00:13:36,240
very efficient it's very it's very cool

355
00:13:34,399 --> 00:13:38,560
there are instructions to set it up i'm

356
00:13:36,240 --> 00:13:41,040
not going to cover this in details

357
00:13:38,560 --> 00:13:42,160
uh and security so the hardware uh

358
00:13:41,040 --> 00:13:44,560
vulnerabilities

359
00:13:42,160 --> 00:13:45,519
which are well known these days uh the

360
00:13:44,560 --> 00:13:47,518
good thing about

361
00:13:45,519 --> 00:13:48,880
this processor is that amd processor in

362
00:13:47,519 --> 00:13:51,360
general and these in particular

363
00:13:48,880 --> 00:13:51,920
are only one only let's say vulnerable

364
00:13:51,360 --> 00:13:53,920
to

365
00:13:51,920 --> 00:13:56,240
a subset of them and in particular the

366
00:13:53,920 --> 00:13:59,040
nastier one for virtualization

367
00:13:56,240 --> 00:14:00,639
this is not vulnerable so we are happy

368
00:13:59,040 --> 00:14:02,880
about that

369
00:14:00,639 --> 00:14:04,720
benchmarks the benchmarks that i run uh

370
00:14:02,880 --> 00:14:06,240
i said i wanted to focus on cpu and

371
00:14:04,720 --> 00:14:08,480
memory so i will

372
00:14:06,240 --> 00:14:09,760
show results of running stream which is

373
00:14:08,480 --> 00:14:12,639
a memory benchmark

374
00:14:09,760 --> 00:14:13,040
and i will show what i will show right

375
00:14:12,639 --> 00:14:15,519
now

376
00:14:13,040 --> 00:14:16,560
is going to be the results of running

377
00:14:15,519 --> 00:14:19,839
stream on

378
00:14:16,560 --> 00:14:20,880
bare metal and then inside one or more

379
00:14:19,839 --> 00:14:24,560
vms

380
00:14:20,880 --> 00:14:27,760
and so so that we can compare results

381
00:14:24,560 --> 00:14:27,760
and you can configure stream

382
00:14:28,000 --> 00:14:33,440
in our case we use the openmp

383
00:14:31,040 --> 00:14:35,599
for parallelization of the stream jobs

384
00:14:33,440 --> 00:14:39,360
and so we used a different number of

385
00:14:35,600 --> 00:14:41,279
threads which was either 16 or 32

386
00:14:39,360 --> 00:14:42,880
on bayer method in general the real time

387
00:14:41,279 --> 00:14:45,360
again is to use

388
00:14:42,880 --> 00:14:46,800
as many threads as there are memory

389
00:14:45,360 --> 00:14:47,600
channel but this is not an information

390
00:14:46,800 --> 00:14:51,040
that is

391
00:14:47,600 --> 00:14:52,320
easily available via software that you

392
00:14:51,040 --> 00:14:55,360
can easily

393
00:14:52,320 --> 00:14:56,959
figure out by software and so you can

394
00:14:55,360 --> 00:14:58,399
kind of approximate it by using one

395
00:14:56,959 --> 00:15:00,000
thread per lnc

396
00:14:58,399 --> 00:15:01,760
and this is applied this applies to both

397
00:15:00,000 --> 00:15:03,600
the host and

398
00:15:01,760 --> 00:15:05,439
the virtual machine so what do i have

399
00:15:03,600 --> 00:15:08,639
here here i have

400
00:15:05,440 --> 00:15:10,639
uh in the

401
00:15:08,639 --> 00:15:12,800
purple bars the results of running

402
00:15:10,639 --> 00:15:16,720
string on bare metal

403
00:15:12,800 --> 00:15:18,479
and then in green

404
00:15:16,720 --> 00:15:21,199
stream run inside the vm without any

405
00:15:18,480 --> 00:15:23,600
kind of tuning so the performance

406
00:15:21,199 --> 00:15:24,319
don't match and you see it very well

407
00:15:23,600 --> 00:15:26,000
then i

408
00:15:24,320 --> 00:15:27,600
applied a little bit of tuning so the vm

409
00:15:26,000 --> 00:15:29,440
had a virtual topology but uh

410
00:15:27,600 --> 00:15:31,360
he wasn't doing any pinning of cpu

411
00:15:29,440 --> 00:15:33,040
memory and so again in the blue

412
00:15:31,360 --> 00:15:35,279
in the light blue bar and again the

413
00:15:33,040 --> 00:15:39,120
performance uh don't match

414
00:15:35,279 --> 00:15:40,160
and then uh magic you apply the tuning

415
00:15:39,120 --> 00:15:44,240
that i described

416
00:15:40,160 --> 00:15:48,000
and you see in the last

417
00:15:44,240 --> 00:15:51,839
bar that now the performance of

418
00:15:48,000 --> 00:15:55,600
bayer method and inside vms

419
00:15:51,839 --> 00:15:58,240
it's at just one vm basically

420
00:15:55,600 --> 00:15:59,040
matches so that's what we wanted and

421
00:15:58,240 --> 00:16:02,240
this is for

422
00:15:59,040 --> 00:16:05,920
this is when running stream uh just

423
00:16:02,240 --> 00:16:09,440
in single thread mode the same

424
00:16:05,920 --> 00:16:12,479
when using 30 threads for

425
00:16:09,440 --> 00:16:13,759
stream as as you can see the there is we

426
00:16:12,480 --> 00:16:15,839
we are able to reach very good

427
00:16:13,759 --> 00:16:16,240
performance because inside the vm we

428
00:16:15,839 --> 00:16:18,800
achieve

429
00:16:16,240 --> 00:16:21,199
pretty much the same level as um on the

430
00:16:18,800 --> 00:16:21,199
host

431
00:16:21,360 --> 00:16:24,880
here i use two vms instead of one so

432
00:16:23,759 --> 00:16:28,320
there are a lot of

433
00:16:24,880 --> 00:16:29,279
uh elements of the plot you would want

434
00:16:28,320 --> 00:16:32,560
to focus

435
00:16:29,279 --> 00:16:36,959
on again the first one is bare metal

436
00:16:32,560 --> 00:16:40,800
the red and black ones are

437
00:16:36,959 --> 00:16:42,800
i'm now using two vms and are the um

438
00:16:40,800 --> 00:16:45,040
score the results the the performance

439
00:16:42,800 --> 00:16:48,479
that you get from stream when run

440
00:16:45,040 --> 00:16:51,439
inside these two vms so it's okay

441
00:16:48,480 --> 00:16:52,959
that it's lower it's it's is less than

442
00:16:51,440 --> 00:16:54,320
bare metal because now you have

443
00:16:52,959 --> 00:16:56,719
partition and the

444
00:16:54,320 --> 00:16:58,079
uh the cpu in two basically and you are

445
00:16:56,720 --> 00:17:00,720
assigned

446
00:16:58,079 --> 00:17:01,439
each part to a different two to one vm

447
00:17:00,720 --> 00:17:05,039
and

448
00:17:01,440 --> 00:17:06,959
the important part the nice part is that

449
00:17:05,039 --> 00:17:08,640
as you can see the performance of the

450
00:17:06,959 --> 00:17:09,679
huey m is quite consistent because they

451
00:17:08,640 --> 00:17:12,480
are

452
00:17:09,679 --> 00:17:13,039
basically performing the same in all the

453
00:17:12,480 --> 00:17:15,199
three

454
00:17:13,039 --> 00:17:16,480
three the four stream operations and

455
00:17:15,199 --> 00:17:19,199
then this last bar

456
00:17:16,480 --> 00:17:20,079
is the basically the sum of these two uh

457
00:17:19,199 --> 00:17:23,360
which again

458
00:17:20,079 --> 00:17:26,480
is uh pretty much matches bare metal

459
00:17:23,359 --> 00:17:29,520
and so we are happy again

460
00:17:26,480 --> 00:17:31,200
now um i mentioned

461
00:17:29,520 --> 00:17:33,200
secure encrypted virtualization what's

462
00:17:31,200 --> 00:17:34,880
the i said that the memory of the vm is

463
00:17:33,200 --> 00:17:37,840
encrypted what's the overhead

464
00:17:34,880 --> 00:17:39,600
that comes with that uh as a matter of

465
00:17:37,840 --> 00:17:40,320
fact at least for this benchmark is very

466
00:17:39,600 --> 00:17:43,760
very slow

467
00:17:40,320 --> 00:17:45,600
uh on papers you find that

468
00:17:43,760 --> 00:17:48,400
it's it always stays within three

469
00:17:45,600 --> 00:17:50,399
percent in these cases

470
00:17:48,400 --> 00:17:52,880
at least as far as i can measure the it

471
00:17:50,400 --> 00:17:55,679
stayed within one percent

472
00:17:52,880 --> 00:17:56,799
and uh yeah another benchmark this time

473
00:17:55,679 --> 00:18:00,559
this is called

474
00:17:56,799 --> 00:18:01,760
uh nas pb it's a very cpu intensive

475
00:18:00,559 --> 00:18:04,960
benchmark this time

476
00:18:01,760 --> 00:18:07,280
which is what i said memory and cpu also

477
00:18:04,960 --> 00:18:10,640
uses a parallelization framework it's

478
00:18:07,280 --> 00:18:14,559
open mpi this time not openmp and

479
00:18:10,640 --> 00:18:17,440
this time lower is better before i

480
00:18:14,559 --> 00:18:19,760
probably forgot to say higher was better

481
00:18:17,440 --> 00:18:23,200
and uh yeah the same the same stuff

482
00:18:19,760 --> 00:18:26,640
uh basically first bar bare metal

483
00:18:23,200 --> 00:18:28,559
last bar vm with tuning and we want them

484
00:18:26,640 --> 00:18:31,520
to match and to be very similar

485
00:18:28,559 --> 00:18:32,720
and that's actually the case uh

486
00:18:31,520 --> 00:18:36,480
returning applied

487
00:18:32,720 --> 00:18:40,160
in all the various variants of

488
00:18:36,480 --> 00:18:43,520
the naspb benchmark and again i

489
00:18:40,160 --> 00:18:47,039
also uh benchmarked um

490
00:18:43,520 --> 00:18:47,440
with encrypted visualization enabled or

491
00:18:47,039 --> 00:18:50,640
not

492
00:18:47,440 --> 00:18:51,360
or disabled with this cpu intensive

493
00:18:50,640 --> 00:18:53,280
benchmark

494
00:18:51,360 --> 00:18:56,080
and again less than one percent

495
00:18:53,280 --> 00:18:59,120
performance impact which is very good

496
00:18:56,080 --> 00:19:02,399
now the cpu model in theory uh the

497
00:18:59,120 --> 00:19:05,120
uh this is this is qmu that builds

498
00:19:02,400 --> 00:19:06,240
uh virtua cpu for the virtual machine

499
00:19:05,120 --> 00:19:09,199
what flags

500
00:19:06,240 --> 00:19:10,160
uh how it presents it uh what kind of

501
00:19:09,200 --> 00:19:11,840
cpu

502
00:19:10,160 --> 00:19:14,559
does can you present to your virtual

503
00:19:11,840 --> 00:19:17,280
machine uh interior if you want

504
00:19:14,559 --> 00:19:18,639
to achieve best possible performance

505
00:19:17,280 --> 00:19:20,399
you'll find in various pieces of

506
00:19:18,640 --> 00:19:21,600
documentation that you should use this

507
00:19:20,400 --> 00:19:24,640
thing so host

508
00:19:21,600 --> 00:19:26,000
pass through but it depends on for

509
00:19:24,640 --> 00:19:29,360
example the version of

510
00:19:26,000 --> 00:19:29,919
your software in in this case qmualibird

511
00:19:29,360 --> 00:19:33,280
as we said

512
00:19:29,919 --> 00:19:35,280
this effort was about doing this tuning

513
00:19:33,280 --> 00:19:37,039
on

514
00:19:35,280 --> 00:19:39,120
these particular distributions

515
00:19:37,039 --> 00:19:40,720
distribution sorry

516
00:19:39,120 --> 00:19:42,320
and as a matter of fact the distribution

517
00:19:40,720 --> 00:19:46,320
went out before

518
00:19:42,320 --> 00:19:49,280
the epic platform series of process

519
00:19:46,320 --> 00:19:50,559
processor was available and so if you

520
00:19:49,280 --> 00:19:52,240
use those pass-through

521
00:19:50,559 --> 00:19:54,639
it turns out that in this particular

522
00:19:52,240 --> 00:19:58,640
case it doesn't do a good job

523
00:19:54,640 --> 00:19:59,679
and the detail is here of why it's here

524
00:19:58,640 --> 00:20:02,880
because the threads

525
00:19:59,679 --> 00:20:05,280
basically are not exposed correctly so

526
00:20:02,880 --> 00:20:06,720
this is as a matter of fact there is a

527
00:20:05,280 --> 00:20:10,480
cpu model called epic

528
00:20:06,720 --> 00:20:11,840
which is there because it's the previous

529
00:20:10,480 --> 00:20:15,200
it is the one that represents the

530
00:20:11,840 --> 00:20:18,639
previous generation of epic processors

531
00:20:15,200 --> 00:20:22,240
and if you use if you use that one uh

532
00:20:18,640 --> 00:20:24,000
yeah except i yeah except i i faced

533
00:20:22,240 --> 00:20:25,200
the basic same thing but that's uh

534
00:20:24,000 --> 00:20:26,640
that's the type well let's call it like

535
00:20:25,200 --> 00:20:29,200
that this would have been two

536
00:20:26,640 --> 00:20:30,640
and uh it uses uh it it provides the vm

537
00:20:29,200 --> 00:20:34,320
a better sorry for that

538
00:20:30,640 --> 00:20:36,880
a better um virtual topology and

539
00:20:34,320 --> 00:20:37,678
and in fact this is uh what happens if

540
00:20:36,880 --> 00:20:40,240
you use

541
00:20:37,679 --> 00:20:41,919
us pass through it's this one so it's

542
00:20:40,240 --> 00:20:44,080
again lower is better so

543
00:20:41,919 --> 00:20:45,280
it's tuning applied but using ghost

544
00:20:44,080 --> 00:20:46,639
pastor as a cp model

545
00:20:45,280 --> 00:20:48,399
very very bad because we want it to be

546
00:20:46,640 --> 00:20:50,880
here using

547
00:20:48,400 --> 00:20:51,679
epic it's here of course if you if you

548
00:20:50,880 --> 00:20:54,240
go uh

549
00:20:51,679 --> 00:20:55,760
if you use a more updated distribution a

550
00:20:54,240 --> 00:20:58,080
new version of uh

551
00:20:55,760 --> 00:20:59,200
opensuse li or whatever other

552
00:20:58,080 --> 00:21:01,439
distribution

553
00:20:59,200 --> 00:21:02,320
or just the code from upstream you will

554
00:21:01,440 --> 00:21:04,799
find

555
00:21:02,320 --> 00:21:05,360
the epic 2 cpu model there and you can

556
00:21:04,799 --> 00:21:08,400
use it

557
00:21:05,360 --> 00:21:10,959
but this was uh this last they did

558
00:21:08,400 --> 00:21:13,520
i i put this part in here because i

559
00:21:10,960 --> 00:21:13,520
wanted to

560
00:21:13,600 --> 00:21:16,639
stress the fact that yes there are no

561
00:21:15,360 --> 00:21:19,760
disturbing advices but

562
00:21:16,640 --> 00:21:20,799
you really should always double check

563
00:21:19,760 --> 00:21:23,039
because those pass through was the

564
00:21:20,799 --> 00:21:25,600
natural choice and it wasn't

565
00:21:23,039 --> 00:21:27,360
performing well uh now i have other

566
00:21:25,600 --> 00:21:30,158
stream benchmarks but i

567
00:21:27,360 --> 00:21:31,039
rather try to leave some time for

568
00:21:30,159 --> 00:21:35,840
questions

569
00:21:31,039 --> 00:21:39,679
then yeah let's see so

570
00:21:35,840 --> 00:21:39,678
yeah uh basically

571
00:21:39,760 --> 00:21:43,039
the conclusions are that

572
00:21:41,830 --> 00:21:45,120
[Music]

573
00:21:43,039 --> 00:21:46,879
achieving very good performance even

574
00:21:45,120 --> 00:21:48,639
performance that actually matches the

575
00:21:46,880 --> 00:21:52,480
one of those inside

576
00:21:48,640 --> 00:21:55,360
either one or more vms is possible

577
00:21:52,480 --> 00:21:56,559
at least for a certain at least for

578
00:21:55,360 --> 00:21:59,039
certain workloads

579
00:21:56,559 --> 00:22:00,399
and it happens mostly via via resource

580
00:21:59,039 --> 00:22:03,039
partitioning

581
00:22:00,400 --> 00:22:04,159
if you use kvm qmu libvirt in that

582
00:22:03,039 --> 00:22:05,840
particular product

583
00:22:04,159 --> 00:22:08,400
even better if you use them from

584
00:22:05,840 --> 00:22:10,639
upstream you have all the tools of the

585
00:22:08,400 --> 00:22:13,280
capabilities to achieve this

586
00:22:10,640 --> 00:22:13,679
very good result partitioning we at suze

587
00:22:13,280 --> 00:22:15,760
also

588
00:22:13,679 --> 00:22:17,760
support xen and you can do pretty much

589
00:22:15,760 --> 00:22:21,120
the same exam although you will lack

590
00:22:17,760 --> 00:22:22,960
uh the performance won't be uh as good

591
00:22:21,120 --> 00:22:24,320
as this because still lacks the

592
00:22:22,960 --> 00:22:26,400
capability of exposing properly the

593
00:22:24,320 --> 00:22:28,080
vital topology of the to the guest

594
00:22:26,400 --> 00:22:31,760
avitos project the guests

595
00:22:28,080 --> 00:22:34,240
and uh and the epic two platform tunes

596
00:22:31,760 --> 00:22:35,520
turned out to be uh quite a good

597
00:22:34,240 --> 00:22:39,440
platform from this

598
00:22:35,520 --> 00:22:42,000
point of view um because they offer

599
00:22:39,440 --> 00:22:43,440
great scalability uh offer great offer

600
00:22:42,000 --> 00:22:46,159
memory encryption with uh

601
00:22:43,440 --> 00:22:47,440
exceptionally low overhead as we see and

602
00:22:46,159 --> 00:22:49,280
because they are only affected by a

603
00:22:47,440 --> 00:22:52,559
subset of the

604
00:22:49,280 --> 00:22:55,039
vulnerability um flows

605
00:22:52,559 --> 00:22:56,720
related to speculative execution so with

606
00:22:55,039 --> 00:22:58,240
that yeah again in this last you will

607
00:22:56,720 --> 00:22:58,960
find a little bit more information about

608
00:22:58,240 --> 00:23:01,360
myself

609
00:22:58,960 --> 00:23:02,000
and uh while taking a question let me as

610
00:23:01,360 --> 00:23:05,120
we did

611
00:23:02,000 --> 00:23:07,200
this morning uh say one

612
00:23:05,120 --> 00:23:08,639
more time farewell to my very good

613
00:23:07,200 --> 00:23:10,720
friend last course with this picture

614
00:23:08,640 --> 00:23:22,880
they cannot force them a few years ago

615
00:23:10,720 --> 00:23:26,320
and yeah but really questions

616
00:23:22,880 --> 00:23:27,520
yeah i see three hands um

617
00:23:26,320 --> 00:23:30,080
[Music]

618
00:23:27,520 --> 00:23:32,639
i guess um so were any of those

619
00:23:30,080 --> 00:23:36,960
benchmarks

620
00:23:32,640 --> 00:23:36,960
regard vns that span across

621
00:23:41,760 --> 00:23:45,120
yeah i will always forget about that the

622
00:23:44,000 --> 00:23:50,240
question

623
00:23:45,120 --> 00:23:53,039
the question is whether the benchmarks

624
00:23:50,240 --> 00:23:53,440
any of the benchmarks that i show were

625
00:23:53,039 --> 00:23:55,679
run

626
00:23:53,440 --> 00:23:56,960
in a scenario where avm was spawning

627
00:23:55,679 --> 00:24:00,640
multiple pneuma nodes

628
00:23:56,960 --> 00:24:03,760
so when i showed

629
00:24:00,640 --> 00:24:06,880
these results

630
00:24:03,760 --> 00:24:10,000
the one these ones for

631
00:24:06,880 --> 00:24:14,080
no these ones one vm okay

632
00:24:10,000 --> 00:24:16,159
if you use one vm one very big vm

633
00:24:14,080 --> 00:24:17,520
then yeah i have another and one slide

634
00:24:16,159 --> 00:24:21,120
that i didn't show but uh

635
00:24:17,520 --> 00:24:23,520
let's use it for that this was uh

636
00:24:21,120 --> 00:24:25,678
the vm that was using that benchmark so

637
00:24:23,520 --> 00:24:27,200
it was spanning both of the new nodes it

638
00:24:25,679 --> 00:24:30,880
basically had all the

639
00:24:27,200 --> 00:24:32,480
uh pretty much as many vcp users that

640
00:24:30,880 --> 00:24:34,400
are pcpu's with the exception of the

641
00:24:32,480 --> 00:24:36,080
ones that i decided to leave to the host

642
00:24:34,400 --> 00:24:37,440
but this was spanning both the pneuma

643
00:24:36,080 --> 00:24:42,399
nodes and so it had

644
00:24:37,440 --> 00:24:42,400
a virtual topology exposed to it

645
00:24:44,960 --> 00:24:48,880
the question now is about what was the

646
00:24:46,960 --> 00:24:51,440
huge page size choosing it was one

647
00:24:48,880 --> 00:24:51,440
gigabyte

648
00:24:51,679 --> 00:25:01,840
and the other questions let's go there

649
00:25:08,400 --> 00:25:11,919
there was an advantage to staying within

650
00:25:10,480 --> 00:25:14,320
the ccd

651
00:25:11,919 --> 00:25:15,760
as opposed to as it got proposed to

652
00:25:14,320 --> 00:25:17,600
going outside the ccd

653
00:25:15,760 --> 00:25:19,279
did you manage to characterize that you

654
00:25:17,600 --> 00:25:20,799
have supporting numbers in any sense to

655
00:25:19,279 --> 00:25:24,720
support that association

656
00:25:20,799 --> 00:25:28,080
so this is uh the question was about

657
00:25:24,720 --> 00:25:28,960
since i said that if possible it's

658
00:25:28,080 --> 00:25:30,879
better to

659
00:25:28,960 --> 00:25:32,559
configure a vm so that it stays inside

660
00:25:30,880 --> 00:25:34,559
the ccd instead of ccx

661
00:25:32,559 --> 00:25:36,399
if it goes out outside stuff like that

662
00:25:34,559 --> 00:25:39,678
whether i have numbers for that

663
00:25:36,400 --> 00:25:42,720
not yet again this was uh in these

664
00:25:39,679 --> 00:25:46,720
slides that i decided to

665
00:25:42,720 --> 00:25:48,799
skip but if you see we are con this is

666
00:25:46,720 --> 00:25:51,520
an ongoing effort we are

667
00:25:48,799 --> 00:25:53,120
running uh more benchmark continuing

668
00:25:51,520 --> 00:25:56,400
doing our evaluation

669
00:25:53,120 --> 00:25:56,399
and so i have

670
00:25:56,799 --> 00:26:02,080
i'm finished but going over

671
00:25:59,279 --> 00:26:02,080
investigation

672
00:26:02,640 --> 00:26:10,159
with multiple vms in cases

673
00:26:06,480 --> 00:26:12,400
where i actually fulfilled my own

674
00:26:10,159 --> 00:26:14,240
recommendation and so i don't split ccds

675
00:26:12,400 --> 00:26:17,919
and stuff but also in cases where

676
00:26:14,240 --> 00:26:24,320
i uh violates them and i put vms

677
00:26:17,919 --> 00:26:28,960
across ccds just as a hint

678
00:26:24,320 --> 00:26:33,279
when you start this for example

679
00:26:28,960 --> 00:26:36,320
is a case where 6vm were used

680
00:26:33,279 --> 00:26:38,720
and you see that the absolute

681
00:26:36,320 --> 00:26:40,240
level of the performance is the correct

682
00:26:38,720 --> 00:26:43,360
one if you do the math

683
00:26:40,240 --> 00:26:45,520
is the this is fine but

684
00:26:43,360 --> 00:26:47,439
the performance is also actually quite

685
00:26:45,520 --> 00:26:47,840
consistent in this case this case this

686
00:26:47,440 --> 00:26:50,320
case

687
00:26:47,840 --> 00:26:51,439
but you see some strange behavior here

688
00:26:50,320 --> 00:26:53,600
and that's what

689
00:26:51,440 --> 00:26:55,520
uh typically again this is an ongoing

690
00:26:53,600 --> 00:26:56,959
investigation so this is just a little

691
00:26:55,520 --> 00:26:58,158
bit of speculation but what we are

692
00:26:56,960 --> 00:27:00,720
seeing is that

693
00:26:58,159 --> 00:27:02,960
when you start uh not respecting this

694
00:27:00,720 --> 00:27:06,240
recommendation and and putting vms

695
00:27:02,960 --> 00:27:08,080
in pinning vms in such a way that share

696
00:27:06,240 --> 00:27:10,080
too much resources then what happens is

697
00:27:08,080 --> 00:27:13,840
that you have this

698
00:27:10,080 --> 00:27:17,279
not so much consistent behavior

699
00:27:13,840 --> 00:27:20,399
uh in the results i have

700
00:27:17,279 --> 00:27:22,480
other classes here here is a there is

701
00:27:20,399 --> 00:27:23,760
here it is another example where the

702
00:27:22,480 --> 00:27:27,760
recommendations were not

703
00:27:23,760 --> 00:27:29,520
really uh respected and

704
00:27:27,760 --> 00:27:31,200
you have the performances which are not

705
00:27:29,520 --> 00:27:34,559
equally uh

706
00:27:31,200 --> 00:27:34,880
exactly the same in all vms yeah there

707
00:27:34,559 --> 00:27:36,080
was

708
00:27:34,880 --> 00:27:38,399
other questions but i think we are out

709
00:27:36,080 --> 00:27:42,158
of time we are so

710
00:27:38,399 --> 00:27:42,158
uh i'm happy to answer i mean

711
00:27:42,960 --> 00:27:47,039
i mean i can it's not recorded but if

712
00:27:45,919 --> 00:27:51,039
you want them

713
00:27:47,039 --> 00:27:52,399
uh i i mean i'm fine go ahead

714
00:27:51,039 --> 00:27:54,158
sure first of all thank you for your

715
00:27:52,399 --> 00:27:56,239
talk i very much enjoyed it thank you

716
00:27:54,159 --> 00:27:57,919
um i was wondering have the these

717
00:27:56,240 --> 00:27:59,760
optimizations and human things been

718
00:27:57,919 --> 00:28:00,559
implemented in for example openstack

719
00:27:59,760 --> 00:28:02,720
already

720
00:28:00,559 --> 00:28:04,799
like is there a plan to do that or

721
00:28:02,720 --> 00:28:07,440
something uh

722
00:28:04,799 --> 00:28:09,039
the i have the quest well i guess i

723
00:28:07,440 --> 00:28:10,399
repeat it as well the question was about

724
00:28:09,039 --> 00:28:12,080
whether this implementation these

725
00:28:10,399 --> 00:28:13,760
optimization are implemented in

726
00:28:12,080 --> 00:28:15,439
openstack or similar software

727
00:28:13,760 --> 00:28:17,120
i have no idea i have never played with

728
00:28:15,440 --> 00:28:20,000
openstack and i don't plan to win the

729
00:28:17,120 --> 00:28:24,479
foreseeable future to be honest

730
00:28:20,000 --> 00:28:24,480
i am aware of very few

731
00:28:24,760 --> 00:28:28,720
[Music]

732
00:28:26,080 --> 00:28:29,600
very few efforts and very few

733
00:28:28,720 --> 00:28:31,600
capabilities

734
00:28:29,600 --> 00:28:32,639
similar to the one that you are saying

735
00:28:31,600 --> 00:28:35,199
so doing

736
00:28:32,640 --> 00:28:36,559
uh resource partitioning and

737
00:28:35,200 --> 00:28:39,360
optimization

738
00:28:36,559 --> 00:28:40,879
at this level automatically either in

739
00:28:39,360 --> 00:28:42,639
openstack or in many other software

740
00:28:40,880 --> 00:28:46,320
there are solutions

741
00:28:42,640 --> 00:28:47,039
but uh achieving this level of details

742
00:28:46,320 --> 00:28:51,120
in the tuning

743
00:28:47,039 --> 00:28:52,799
it's quite hard because after all that

744
00:28:51,120 --> 00:28:54,639
because of reasons i mean i don't know

745
00:28:52,799 --> 00:28:56,879
myself but you have then

746
00:28:54,640 --> 00:28:58,640
it's a matter of interface that you uh

747
00:28:56,880 --> 00:29:03,039
present to the user

748
00:28:58,640 --> 00:29:05,520
for letting him or her able to

749
00:29:03,039 --> 00:29:07,679
achieve this after all it turns out to

750
00:29:05,520 --> 00:29:07,679
be

751
00:29:07,919 --> 00:29:10,960
rather similar to the xml itself because

752
00:29:09,840 --> 00:29:13,199
it's a very detailed

753
00:29:10,960 --> 00:29:14,080
level of so i'm not saying it's not

754
00:29:13,200 --> 00:29:18,080
possible i

755
00:29:14,080 --> 00:29:19,840
would uh really hope that uh

756
00:29:18,080 --> 00:29:21,279
the situation was better but i'm not

757
00:29:19,840 --> 00:29:22,639
aware of any

758
00:29:21,279 --> 00:29:24,840
anything that reaches this level of

759
00:29:22,640 --> 00:29:27,820
details yeah good

760
00:29:24,840 --> 00:29:30,949
um

761
00:29:27,820 --> 00:29:30,950
[Music]

762
00:29:37,360 --> 00:29:41,840
on the right side where you have uneven

763
00:29:48,000 --> 00:29:55,840
i was trying this once

764
00:30:00,880 --> 00:30:04,640
so that for example you have the

765
00:30:02,640 --> 00:30:08,960
operating system on those which

766
00:30:04,640 --> 00:30:08,960
try to swap instantly instead

767
00:30:09,600 --> 00:30:12,320
yes there are

768
00:30:13,039 --> 00:30:18,480
i haven't monitored that uh that part

769
00:30:16,320 --> 00:30:18,480
but

770
00:30:18,559 --> 00:30:22,080
the fact is that at least according to

771
00:30:21,279 --> 00:30:23,520
me and to my

772
00:30:22,080 --> 00:30:26,879
experience in running similar

773
00:30:23,520 --> 00:30:30,240
evaluations in uh uh

774
00:30:26,880 --> 00:30:33,520
even in other platforms the

775
00:30:30,240 --> 00:30:35,840
consistencies of results like these

776
00:30:33,520 --> 00:30:37,679
uh is something which is quite good and

777
00:30:35,840 --> 00:30:40,959
that you don't find very often

778
00:30:37,679 --> 00:30:44,720
but as apparently as soon as you uh

779
00:30:40,960 --> 00:30:48,480
like mix things in a not necessarily

780
00:30:44,720 --> 00:30:51,520
super ideal way then these uh very nice

781
00:30:48,480 --> 00:30:54,960
properties to start to like

782
00:30:51,520 --> 00:30:58,480
fade away uh so

783
00:30:54,960 --> 00:31:00,320
yeah i haven't checked that um whether

784
00:30:58,480 --> 00:31:02,000
what you said also was happening in

785
00:31:00,320 --> 00:31:03,360
these cases but uh yeah

786
00:31:02,000 --> 00:31:07,039
[Music]

787
00:31:03,360 --> 00:31:09,840
i have i have a scenario with

788
00:31:07,039 --> 00:31:11,120
30 vms where i am violating the

789
00:31:09,840 --> 00:31:14,240
recommendations

790
00:31:11,120 --> 00:31:16,840
by using too many uh basically

791
00:31:14,240 --> 00:31:18,399
threads for stream if you count all of

792
00:31:16,840 --> 00:31:20,879
them uh

793
00:31:18,399 --> 00:31:22,000
running inside all the vms and if you

794
00:31:20,880 --> 00:31:23,840
look at the actual

795
00:31:22,000 --> 00:31:25,120
throughput that you achieve that's

796
00:31:23,840 --> 00:31:27,918
actually quite good

797
00:31:25,120 --> 00:31:29,120
but it's so unbalanced if you some uh if

798
00:31:27,919 --> 00:31:32,159
you sum all of them up it's

799
00:31:29,120 --> 00:31:33,279
it matches or even overcome the one that

800
00:31:32,159 --> 00:31:43,919
you achieved on the host

801
00:31:33,279 --> 00:31:43,919
but then it's all like that up and downs

