1
00:00:07,040 --> 00:00:11,120
all right

2
00:00:07,919 --> 00:00:11,759
so next up we have frank um talking

3
00:00:11,120 --> 00:00:15,678
about

4
00:00:11,759 --> 00:00:17,199
model selection for deep learning

5
00:00:15,679 --> 00:00:19,330
neural networks he's going to tell us

6
00:00:17,199 --> 00:00:22,460
more about it thank you

7
00:00:19,330 --> 00:00:22,459
[Applause]

8
00:00:23,840 --> 00:00:27,680
thank you very much can you hear me okay

9
00:00:26,720 --> 00:00:29,519
yeah

10
00:00:27,680 --> 00:00:30,880
great well it's really a pleasure to be

11
00:00:29,519 --> 00:00:34,000
here at uh

12
00:00:30,880 --> 00:00:35,920
fosdem again uh to talk about deep

13
00:00:34,000 --> 00:00:38,239
neural networks

14
00:00:35,920 --> 00:00:39,520
um this project the software in this

15
00:00:38,239 --> 00:00:42,239
project

16
00:00:39,520 --> 00:00:43,280
was written by my colleagues at pivotal

17
00:00:42,239 --> 00:00:46,160
now vmware

18
00:00:43,280 --> 00:00:47,280
among other folks but including the kill

19
00:00:46,160 --> 00:00:50,640
ekta

20
00:00:47,280 --> 00:00:52,239
orhan and domino and we also work with

21
00:00:50,640 --> 00:00:52,960
the university of california at san

22
00:00:52,239 --> 00:00:56,839
diego

23
00:00:52,960 --> 00:00:58,160
professor kumar and his phd student yoo

24
00:00:56,840 --> 00:01:01,280
hao

25
00:00:58,160 --> 00:01:03,440
so thanks to those folks up front

26
00:01:01,280 --> 00:01:04,960
all of the software in this project is

27
00:01:03,440 --> 00:01:07,439
free and open source there's no

28
00:01:04,959 --> 00:01:10,080
commercial or

29
00:01:07,439 --> 00:01:12,399
closed source software here at all in

30
00:01:10,080 --> 00:01:14,320
the spirit of fosdem

31
00:01:12,400 --> 00:01:15,840
this is the agenda what i'd like to talk

32
00:01:14,320 --> 00:01:18,399
about today

33
00:01:15,840 --> 00:01:19,439
first talk about how one could train

34
00:01:18,400 --> 00:01:23,920
deep neural nets

35
00:01:19,439 --> 00:01:25,758
in parallel secondly to introduce you

36
00:01:23,920 --> 00:01:28,000
to something called model hopper

37
00:01:25,759 --> 00:01:30,159
parallel parallelization

38
00:01:28,000 --> 00:01:31,200
which is based on the work that uc san

39
00:01:30,159 --> 00:01:34,159
diego

40
00:01:31,200 --> 00:01:36,079
did in this area talk about an

41
00:01:34,159 --> 00:01:38,960
implementation of model hopper on a

42
00:01:36,079 --> 00:01:40,639
massively parallel processing database

43
00:01:38,960 --> 00:01:42,000
and then give a couple of examples one

44
00:01:40,640 --> 00:01:44,640
with grid search

45
00:01:42,000 --> 00:01:48,880
and one with automated machine learning

46
00:01:44,640 --> 00:01:48,880
using a method called uh hyperband

47
00:01:49,360 --> 00:01:53,520
so let's get started we've all heard

48
00:01:51,200 --> 00:01:55,680
about deep learning and deep nets

49
00:01:53,520 --> 00:01:57,600
in the popular press in the specialty

50
00:01:55,680 --> 00:01:59,920
press

51
00:01:57,600 --> 00:02:02,158
it's a kind of machine learning which is

52
00:01:59,920 --> 00:02:02,960
originally inspired by biology of the

53
00:02:02,159 --> 00:02:04,880
brain

54
00:02:02,960 --> 00:02:07,360
although these days it has less to do

55
00:02:04,880 --> 00:02:10,560
with with neuroscience

56
00:02:07,360 --> 00:02:11,920
and it is characterized by artificial

57
00:02:10,560 --> 00:02:14,319
neural networks

58
00:02:11,920 --> 00:02:16,000
which are networks that have multiple

59
00:02:14,319 --> 00:02:18,720
layers they have an input layer

60
00:02:16,000 --> 00:02:20,640
for example an image an output layer

61
00:02:18,720 --> 00:02:22,480
which could be a classification

62
00:02:20,640 --> 00:02:24,399
and in between they have many

63
00:02:22,480 --> 00:02:26,319
potentially

64
00:02:24,400 --> 00:02:27,599
many hidden layers with different

65
00:02:26,319 --> 00:02:29,519
components

66
00:02:27,599 --> 00:02:30,720
and an example is this convolutional

67
00:02:29,520 --> 00:02:33,519
neural network

68
00:02:30,720 --> 00:02:35,440
which is often you kind of network which

69
00:02:33,519 --> 00:02:37,920
is often used in image processing

70
00:02:35,440 --> 00:02:40,800
applications

71
00:02:37,920 --> 00:02:43,040
the problem is there are many problems

72
00:02:40,800 --> 00:02:46,640
but one of them is that training

73
00:02:43,040 --> 00:02:48,799
neural nets is very very hard

74
00:02:46,640 --> 00:02:50,000
the accuracy of a model is a function of

75
00:02:48,800 --> 00:02:52,560
the architecture that you

76
00:02:50,000 --> 00:02:54,480
choose like the kind the number of in

77
00:02:52,560 --> 00:02:55,360
hidden layers what the components are of

78
00:02:54,480 --> 00:02:57,440
those

79
00:02:55,360 --> 00:02:58,800
depends on what the hyper parameters or

80
00:02:57,440 --> 00:03:00,800
the parent you know the

81
00:02:58,800 --> 00:03:03,280
the the parameters for those models are

82
00:03:00,800 --> 00:03:06,800
depends on the data set

83
00:03:03,280 --> 00:03:10,080
so um it's kind of the

84
00:03:06,800 --> 00:03:12,319
um hidden secret in data science that

85
00:03:10,080 --> 00:03:14,080
you have to do a lot of trial and error

86
00:03:12,319 --> 00:03:15,839
in order to come up with a model which

87
00:03:14,080 --> 00:03:17,760
is effective

88
00:03:15,840 --> 00:03:19,760
so just as an example let's say that we

89
00:03:17,760 --> 00:03:21,280
have four model architectures that we're

90
00:03:19,760 --> 00:03:23,760
interested in

91
00:03:21,280 --> 00:03:25,519
we have different batch sizes four

92
00:03:23,760 --> 00:03:26,560
different batch sizes we're interested

93
00:03:25,519 --> 00:03:28,319
in

94
00:03:26,560 --> 00:03:31,200
we want to try four different learning

95
00:03:28,319 --> 00:03:31,760
rates and we want to try four different

96
00:03:31,200 --> 00:03:34,958
methods

97
00:03:31,760 --> 00:03:37,599
of regularization or smoothing well

98
00:03:34,959 --> 00:03:38,799
already we're up to 256 different

99
00:03:37,599 --> 00:03:40,319
combinations

100
00:03:38,799 --> 00:03:41,840
so if you're training all of these

101
00:03:40,319 --> 00:03:44,238
models one at a time

102
00:03:41,840 --> 00:03:46,640
it's going to be extremely expensive

103
00:03:44,239 --> 00:03:48,319
it's going to take a long time

104
00:03:46,640 --> 00:03:49,839
so there's definitely definitely a need

105
00:03:48,319 --> 00:03:52,480
for for speed here

106
00:03:49,840 --> 00:03:54,560
which means work in parallel and working

107
00:03:52,480 --> 00:03:58,560
on parallel means we're talking about

108
00:03:54,560 --> 00:03:58,560
distributed compute systems

109
00:03:58,959 --> 00:04:04,159
so um how could we you know what are the

110
00:04:02,400 --> 00:04:08,080
different ways in which we could train

111
00:04:04,159 --> 00:04:09,679
neural nets in in parallel and

112
00:04:08,080 --> 00:04:11,519
in order to understand the various

113
00:04:09,680 --> 00:04:14,159
options let's just briefly remind

114
00:04:11,519 --> 00:04:16,560
ourselves what gradient descent is

115
00:04:14,159 --> 00:04:17,519
it's the most common optimizer you know

116
00:04:16,560 --> 00:04:19,199
variations of this

117
00:04:17,519 --> 00:04:21,040
most common optimizer which is used in

118
00:04:19,199 --> 00:04:23,199
deep learning

119
00:04:21,040 --> 00:04:24,240
so we want to minimize some loss

120
00:04:23,199 --> 00:04:27,120
function

121
00:04:24,240 --> 00:04:28,080
so we have a cost function on the y-axis

122
00:04:27,120 --> 00:04:31,040
we have a variable

123
00:04:28,080 --> 00:04:32,719
on the x-axis which is perhaps a weight

124
00:04:31,040 --> 00:04:35,919
for one of the

125
00:04:32,720 --> 00:04:38,080
hidden layer in one of the hidden layers

126
00:04:35,919 --> 00:04:40,240
we have an initial value which is that

127
00:04:38,080 --> 00:04:42,880
dot on the right-hand side

128
00:04:40,240 --> 00:04:44,000
we calculate the slope at that point or

129
00:04:42,880 --> 00:04:45,600
the gradient

130
00:04:44,000 --> 00:04:47,680
and then we move in the negative

131
00:04:45,600 --> 00:04:49,840
direction of that gradient in order to

132
00:04:47,680 --> 00:04:52,800
find the bottom of the curve

133
00:04:49,840 --> 00:04:54,638
and the size of the steps that we take

134
00:04:52,800 --> 00:04:56,160
is a function of what's called the

135
00:04:54,639 --> 00:04:57,680
learning rate

136
00:04:56,160 --> 00:05:00,320
so hopefully we get to the bottom of

137
00:04:57,680 --> 00:05:03,039
this curve our model has converged

138
00:05:00,320 --> 00:05:04,320
and it gives us a reasonable a

139
00:05:03,039 --> 00:05:08,560
reasonable

140
00:05:04,320 --> 00:05:12,080
answer thing is we often don't

141
00:05:08,560 --> 00:05:15,120
operate on single weights at a time

142
00:05:12,080 --> 00:05:17,198
we we operate on tons of weights and we

143
00:05:15,120 --> 00:05:20,560
do things in what are called

144
00:05:17,199 --> 00:05:22,720
batches or mini batches

145
00:05:20,560 --> 00:05:23,600
so a brief kind of description of mini

146
00:05:22,720 --> 00:05:26,080
batch

147
00:05:23,600 --> 00:05:26,880
stochastic gradient descent it's much

148
00:05:26,080 --> 00:05:30,000
faster than

149
00:05:26,880 --> 00:05:31,440
working on one one at a time so in our

150
00:05:30,000 --> 00:05:33,919
data set we have

151
00:05:31,440 --> 00:05:34,880
an x we have two features we have x1 and

152
00:05:33,919 --> 00:05:37,280
x2

153
00:05:34,880 --> 00:05:38,960
we have a y value which is our label so

154
00:05:37,280 --> 00:05:42,320
we're talking about a supervised

155
00:05:38,960 --> 00:05:43,520
learning problem and we have a model

156
00:05:42,320 --> 00:05:45,360
that we've initialized

157
00:05:43,520 --> 00:05:47,198
think of the dot on that curve on the

158
00:05:45,360 --> 00:05:50,960
previous slide

159
00:05:47,199 --> 00:05:54,720
we passed that model over this

160
00:05:50,960 --> 00:05:57,039
mini batch subset of the data and

161
00:05:54,720 --> 00:05:58,160
we get a new model well how do we get

162
00:05:57,039 --> 00:06:00,800
that model

163
00:05:58,160 --> 00:06:03,840
what we do is we calculate the gradient

164
00:06:00,800 --> 00:06:06,319
which is the slope

165
00:06:03,840 --> 00:06:08,080
and we use eta which is our learning

166
00:06:06,319 --> 00:06:09,919
rate it's negative because we want to

167
00:06:08,080 --> 00:06:12,639
move in the negative direction

168
00:06:09,919 --> 00:06:14,080
so we're moving down the curve so we do

169
00:06:12,639 --> 00:06:16,880
that for our updated mod

170
00:06:14,080 --> 00:06:18,960
we get an updated model then we take

171
00:06:16,880 --> 00:06:21,039
that model we pass it over the next mini

172
00:06:18,960 --> 00:06:23,359
batch we get a new

173
00:06:21,039 --> 00:06:25,199
model we pass that over the next mini

174
00:06:23,360 --> 00:06:27,120
batch we get a new model

175
00:06:25,199 --> 00:06:28,560
and we continue until we've gone through

176
00:06:27,120 --> 00:06:30,639
all of the data

177
00:06:28,560 --> 00:06:31,600
which is called one iteration or one

178
00:06:30,639 --> 00:06:34,880
epoch

179
00:06:31,600 --> 00:06:36,880
some people call that and we do this in

180
00:06:34,880 --> 00:06:39,280
a sequential manner

181
00:06:36,880 --> 00:06:40,000
it doesn't it you can visit the data in

182
00:06:39,280 --> 00:06:41,758
any order

183
00:06:40,000 --> 00:06:43,039
but you need to sequentially visit all

184
00:06:41,759 --> 00:06:43,840
of the data because that's very

185
00:06:43,039 --> 00:06:46,719
important for

186
00:06:43,840 --> 00:06:48,400
for convergence but you notice we did

187
00:06:46,720 --> 00:06:51,440
this on a single machine

188
00:06:48,400 --> 00:06:52,638
so like how would we do this on multiple

189
00:06:51,440 --> 00:06:56,000
machines how would we

190
00:06:52,639 --> 00:06:57,840
run this in in parallel

191
00:06:56,000 --> 00:07:00,080
and there's different ways to do this

192
00:06:57,840 --> 00:07:02,799
there's task parallelism

193
00:07:00,080 --> 00:07:05,520
and data parallelism so let's look at

194
00:07:02,800 --> 00:07:07,919
those briefly one at a time

195
00:07:05,520 --> 00:07:09,758
task parallelism now we want to train

196
00:07:07,919 --> 00:07:11,520
multiple models at a time

197
00:07:09,759 --> 00:07:12,800
because we've got a big cluster to work

198
00:07:11,520 --> 00:07:15,440
on

199
00:07:12,800 --> 00:07:16,880
so what we do is we have a bunch of

200
00:07:15,440 --> 00:07:20,000
models we're interested in

201
00:07:16,880 --> 00:07:23,520
say those 256 we looked at

202
00:07:20,000 --> 00:07:25,680
we replicate our data on each machine

203
00:07:23,520 --> 00:07:26,880
so you copy your data to each into each

204
00:07:25,680 --> 00:07:29,680
machine

205
00:07:26,880 --> 00:07:32,159
and once you've done that then here we

206
00:07:29,680 --> 00:07:35,360
have our data copied on each machine

207
00:07:32,160 --> 00:07:36,160
we move one model to that machine and

208
00:07:35,360 --> 00:07:37,680
then

209
00:07:36,160 --> 00:07:39,919
we do the steps we showed on the

210
00:07:37,680 --> 00:07:43,280
previous slide we train that for

211
00:07:39,919 --> 00:07:46,479
multiple iterations and we get a model

212
00:07:43,280 --> 00:07:48,878
and that's fine um the problem with that

213
00:07:46,479 --> 00:07:50,080
is that it's very inefficient in terms

214
00:07:48,879 --> 00:07:51,919
of memory and storage

215
00:07:50,080 --> 00:07:53,280
because deep learning data sets are

216
00:07:51,919 --> 00:07:55,198
extremely large

217
00:07:53,280 --> 00:07:56,479
so if you're copying them around it

218
00:07:55,199 --> 00:07:58,400
takes a long time

219
00:07:56,479 --> 00:08:00,000
they may also not fit in memory on a

220
00:07:58,400 --> 00:08:01,758
single machine or they may not even fit

221
00:08:00,000 --> 00:08:03,840
on the disk

222
00:08:01,759 --> 00:08:04,879
so we'd like to look at other options

223
00:08:03,840 --> 00:08:08,318
besides

224
00:08:04,879 --> 00:08:08,879
task parallelism oh you could put in on

225
00:08:08,319 --> 00:08:12,160
a

226
00:08:08,879 --> 00:08:13,919
common file system as well but

227
00:08:12,160 --> 00:08:15,759
then you end up with issues around

228
00:08:13,919 --> 00:08:17,758
network bandwidth as you're moving data

229
00:08:15,759 --> 00:08:21,599
from your file system

230
00:08:17,759 --> 00:08:25,440
to your workers so let's talk about

231
00:08:21,599 --> 00:08:27,120
data parallelism data parallels

232
00:08:25,440 --> 00:08:29,120
parallelism is a bit different now we

233
00:08:27,120 --> 00:08:30,800
want to train we're going to train one

234
00:08:29,120 --> 00:08:32,159
model at a time

235
00:08:30,800 --> 00:08:35,959
however we're going to do a little bit

236
00:08:32,159 --> 00:08:39,760
differently so we've got our

237
00:08:35,958 --> 00:08:41,679
256 models that we want to train

238
00:08:39,760 --> 00:08:43,039
instead of replicating our data we

239
00:08:41,679 --> 00:08:45,439
partition our data

240
00:08:43,039 --> 00:08:47,600
some people call those shards so you put

241
00:08:45,440 --> 00:08:50,320
some of your data on one machine

242
00:08:47,600 --> 00:08:52,720
some of your data on another machine etc

243
00:08:50,320 --> 00:08:55,920
until you've sharded your data

244
00:08:52,720 --> 00:08:58,320
and then um

245
00:08:55,920 --> 00:08:59,680
what you want to do is figure out how

246
00:08:58,320 --> 00:09:02,160
you can train a model

247
00:08:59,680 --> 00:09:03,680
when your data is split up like that and

248
00:09:02,160 --> 00:09:05,680
there's different methods to

249
00:09:03,680 --> 00:09:07,040
to do that we heard about one earlier

250
00:09:05,680 --> 00:09:10,319
this morning

251
00:09:07,040 --> 00:09:13,279
with with spark so

252
00:09:10,320 --> 00:09:13,760
how do we do that so we have a model we

253
00:09:13,279 --> 00:09:15,120
put the

254
00:09:13,760 --> 00:09:17,519
we're going to train one at a time and

255
00:09:15,120 --> 00:09:20,560
put the rest of them in the queue

256
00:09:17,519 --> 00:09:22,240
and we broadcast that model to each of

257
00:09:20,560 --> 00:09:25,359
our worker machines

258
00:09:22,240 --> 00:09:27,600
right then we train

259
00:09:25,360 --> 00:09:28,800
that model on the data that is on that

260
00:09:27,600 --> 00:09:30,880
machine

261
00:09:28,800 --> 00:09:33,279
and you could train it on all of the

262
00:09:30,880 --> 00:09:35,920
data on that machine the whole partition

263
00:09:33,279 --> 00:09:36,800
or you could train it on a batch of it

264
00:09:35,920 --> 00:09:37,920
part of it

265
00:09:36,800 --> 00:09:39,839
and you get different kind of

266
00:09:37,920 --> 00:09:42,640
characteristics depending on how you

267
00:09:39,839 --> 00:09:43,839
you do that once you have that model

268
00:09:42,640 --> 00:09:45,920
trained

269
00:09:43,839 --> 00:09:47,760
then you send the updates back to some

270
00:09:45,920 --> 00:09:50,560
kind of masternode

271
00:09:47,760 --> 00:09:50,880
which updates a global model then where

272
00:09:50,560 --> 00:09:54,319
you

273
00:09:50,880 --> 00:09:58,080
you've done one iteration

274
00:09:54,320 --> 00:09:59,600
and then you rebroadcast and repeat that

275
00:09:58,080 --> 00:10:01,040
and once you've trained that model the

276
00:09:59,600 --> 00:10:02,720
next one in the queue comes in and you

277
00:10:01,040 --> 00:10:06,319
train that one

278
00:10:02,720 --> 00:10:09,760
so that's data parallelism um

279
00:10:06,320 --> 00:10:11,440
if you update your model just once

280
00:10:09,760 --> 00:10:13,360
after you've gone through all of your

281
00:10:11,440 --> 00:10:16,000
data in the partition

282
00:10:13,360 --> 00:10:17,839
you get um what's called bulk

283
00:10:16,000 --> 00:10:18,880
synchronous parallelism or model

284
00:10:17,839 --> 00:10:21,760
averaging

285
00:10:18,880 --> 00:10:22,480
and this can be very fast however it

286
00:10:21,760 --> 00:10:26,319
suffers from

287
00:10:22,480 --> 00:10:28,160
often poor convergence

288
00:10:26,320 --> 00:10:30,240
the other thing you could do is you

289
00:10:28,160 --> 00:10:31,839
could update once per mini batch

290
00:10:30,240 --> 00:10:35,200
so instead of going through all of your

291
00:10:31,839 --> 00:10:38,000
data you go through a mini batch

292
00:10:35,200 --> 00:10:38,720
then you send the updates to the master

293
00:10:38,000 --> 00:10:40,959
and

294
00:10:38,720 --> 00:10:43,279
depending how you do that you could do

295
00:10:40,959 --> 00:10:45,760
it in a synchronous way

296
00:10:43,279 --> 00:10:46,880
you could do it in an asynchronous way

297
00:10:45,760 --> 00:10:50,560
whenever

298
00:10:46,880 --> 00:10:52,320
um a worker is done it sends results up

299
00:10:50,560 --> 00:10:53,839
or you could do it in a kind of

300
00:10:52,320 --> 00:10:57,040
decentralized way

301
00:10:53,839 --> 00:10:59,519
so if you took the orange master

302
00:10:57,040 --> 00:11:00,560
out of the picture here then you end up

303
00:10:59,519 --> 00:11:03,440
with

304
00:11:00,560 --> 00:11:04,399
um a kind of mpi all reduce approach

305
00:11:03,440 --> 00:11:06,640
right

306
00:11:04,399 --> 00:11:08,160
and these have different pros and cons

307
00:11:06,640 --> 00:11:10,160
some of them work well they do suffer

308
00:11:08,160 --> 00:11:15,120
from a high communication

309
00:11:10,160 --> 00:11:17,519
cost so um

310
00:11:15,120 --> 00:11:19,040
what could we do to get the boasts of

311
00:11:17,519 --> 00:11:21,360
best of both worlds what

312
00:11:19,040 --> 00:11:22,560
could we do to get goodness from task

313
00:11:21,360 --> 00:11:26,160
parallelism

314
00:11:22,560 --> 00:11:28,079
goodness from data parallelism and

315
00:11:26,160 --> 00:11:29,760
the approach we've taken is something

316
00:11:28,079 --> 00:11:32,560
called model hopper

317
00:11:29,760 --> 00:11:34,319
parallelism and model hopper parallelism

318
00:11:32,560 --> 00:11:37,439
is based on work which is done

319
00:11:34,320 --> 00:11:41,279
at san diego which we've implemented

320
00:11:37,440 --> 00:11:45,200
in this distributed database

321
00:11:41,279 --> 00:11:48,640
so what is model hopper parallelism

322
00:11:45,200 --> 00:11:51,200
um the way you start this

323
00:11:48,640 --> 00:11:52,560
problem out is it's the same as the data

324
00:11:51,200 --> 00:11:53,760
parallel approach

325
00:11:52,560 --> 00:11:55,920
so what you want to do is you don't want

326
00:11:53,760 --> 00:11:56,800
to copy your data to each of your worker

327
00:11:55,920 --> 00:11:59,839
machines

328
00:11:56,800 --> 00:12:01,760
you want to partition it or shard it

329
00:11:59,839 --> 00:12:04,480
so that's the way you start in the same

330
00:12:01,760 --> 00:12:06,639
way that you do with data parallel

331
00:12:04,480 --> 00:12:08,320
so you've got data on these different

332
00:12:06,639 --> 00:12:12,240
machines

333
00:12:08,320 --> 00:12:13,920
then you put send a model

334
00:12:12,240 --> 00:12:15,440
to each of the an initial model to each

335
00:12:13,920 --> 00:12:17,360
of those machines

336
00:12:15,440 --> 00:12:18,800
and we have 256 so we're going to do

337
00:12:17,360 --> 00:12:22,000
this in a kind of round-robin

338
00:12:18,800 --> 00:12:24,800
uh way you train the model

339
00:12:22,000 --> 00:12:25,600
on that entire partition but then

340
00:12:24,800 --> 00:12:29,040
instead of

341
00:12:25,600 --> 00:12:31,360
going back to the master to do an update

342
00:12:29,040 --> 00:12:33,680
what you do is you move the model state

343
00:12:31,360 --> 00:12:36,480
over to another worker

344
00:12:33,680 --> 00:12:37,680
so we've moved the model state from the

345
00:12:36,480 --> 00:12:40,399
blue one from

346
00:12:37,680 --> 00:12:42,479
the the left side of the screen here to

347
00:12:40,399 --> 00:12:44,800
the center of the screen

348
00:12:42,480 --> 00:12:46,240
and the data is locked in place because

349
00:12:44,800 --> 00:12:47,599
it's too expensive to move you're just

350
00:12:46,240 --> 00:12:50,639
moving the model state

351
00:12:47,600 --> 00:12:51,600
you're moving the model weights then on

352
00:12:50,639 --> 00:12:55,600
the

353
00:12:51,600 --> 00:12:58,240
you do another iteration of training

354
00:12:55,600 --> 00:12:59,279
so you train the the data you train them

355
00:12:58,240 --> 00:13:02,160
all on the

356
00:12:59,279 --> 00:13:04,000
data on the new partition and then you

357
00:13:02,160 --> 00:13:06,000
do the same thing again

358
00:13:04,000 --> 00:13:08,240
so the mall that was in the middle the

359
00:13:06,000 --> 00:13:10,000
green one has now moved to the far right

360
00:13:08,240 --> 00:13:13,279
side of the screen

361
00:13:10,000 --> 00:13:16,399
and you train there and

362
00:13:13,279 --> 00:13:18,639
once you've done that over all of the uh

363
00:13:16,399 --> 00:13:20,079
workers then you've actually completed

364
00:13:18,639 --> 00:13:23,680
one iteration of your

365
00:13:20,079 --> 00:13:26,800
your data so what we're doing is

366
00:13:23,680 --> 00:13:29,040
locking the data in place no data motion

367
00:13:26,800 --> 00:13:31,040
but moving model state which is much

368
00:13:29,040 --> 00:13:35,040
smaller

369
00:13:31,040 --> 00:13:37,519
so how does that work well it performs

370
00:13:35,040 --> 00:13:38,319
um it performs pretty well this is from

371
00:13:37,519 --> 00:13:42,000
the

372
00:13:38,320 --> 00:13:42,720
the paper um the the curve that we're

373
00:13:42,000 --> 00:13:45,600
looking here

374
00:13:42,720 --> 00:13:46,000
is like before a loss curve on the x on

375
00:13:45,600 --> 00:13:49,360
the y

376
00:13:46,000 --> 00:13:51,760
axis and epoch so

377
00:13:49,360 --> 00:13:53,279
number of passes over the data on the

378
00:13:51,760 --> 00:13:56,000
x-axis

379
00:13:53,279 --> 00:13:57,519
and the model hopper is one of the

380
00:13:56,000 --> 00:14:00,240
bottom curves so it has very good

381
00:13:57,519 --> 00:14:03,600
convergence performance

382
00:14:00,240 --> 00:14:05,760
and if you look at runtime and gpu

383
00:14:03,600 --> 00:14:06,880
utilization compared to other methods

384
00:14:05,760 --> 00:14:09,199
it's pretty good

385
00:14:06,880 --> 00:14:10,320
so it has high gpu utilization and as a

386
00:14:09,199 --> 00:14:14,079
result it has

387
00:14:10,320 --> 00:14:14,079
you know very good uh runtime

388
00:14:14,320 --> 00:14:17,600
so we want to take this now and

389
00:14:16,720 --> 00:14:20,720
implement

390
00:14:17,600 --> 00:14:21,600
implement it on a massively parallel

391
00:14:20,720 --> 00:14:24,320
processing

392
00:14:21,600 --> 00:14:25,760
database so there's an open source

393
00:14:24,320 --> 00:14:28,560
project out there

394
00:14:25,760 --> 00:14:29,120
called green pump database there are a

395
00:14:28,560 --> 00:14:30,800
bunch of

396
00:14:29,120 --> 00:14:32,480
other massively parallel processing

397
00:14:30,800 --> 00:14:34,079
databases out there

398
00:14:32,480 --> 00:14:37,279
most of them are commercial this is to

399
00:14:34,079 --> 00:14:40,000
my knowledge the only open source one

400
00:14:37,279 --> 00:14:40,800
and this is what the architecture looks

401
00:14:40,000 --> 00:14:44,800
like

402
00:14:40,800 --> 00:14:47,120
and as expected you have a master node

403
00:14:44,800 --> 00:14:48,000
you have a bunch of workers you can

404
00:14:47,120 --> 00:14:51,120
think of these

405
00:14:48,000 --> 00:14:52,560
as many many postgres databases running

406
00:14:51,120 --> 00:14:55,760
in parallel

407
00:14:52,560 --> 00:14:59,040
and these are on the these host machines

408
00:14:55,760 --> 00:15:01,199
so in the host machines you've installed

409
00:14:59,040 --> 00:15:03,920
keras and tensorflow which are the

410
00:15:01,199 --> 00:15:06,240
libraries we currently support

411
00:15:03,920 --> 00:15:07,760
we also install apache madlib there

412
00:15:06,240 --> 00:15:09,519
which is an in-database machine learning

413
00:15:07,760 --> 00:15:11,680
library

414
00:15:09,519 --> 00:15:12,959
and the one thing i want you to note is

415
00:15:11,680 --> 00:15:15,680
that

416
00:15:12,959 --> 00:15:17,279
we for cost control reasons if you have

417
00:15:15,680 --> 00:15:19,359
like a really big database you might not

418
00:15:17,279 --> 00:15:22,079
have gpus on all of those machines

419
00:15:19,360 --> 00:15:23,360
you may only have gpus on certain of the

420
00:15:22,079 --> 00:15:26,079
hosts

421
00:15:23,360 --> 00:15:28,160
so what we need to be careful of is that

422
00:15:26,079 --> 00:15:28,880
we distribute data only to the host

423
00:15:28,160 --> 00:15:32,319
where there's

424
00:15:28,880 --> 00:15:33,839
gpus and we do when we do our model

425
00:15:32,320 --> 00:15:36,560
hopping like i showed before

426
00:15:33,839 --> 00:15:37,440
moving model state from from machine to

427
00:15:36,560 --> 00:15:38,880
machine

428
00:15:37,440 --> 00:15:42,320
we need to make sure that we only move

429
00:15:38,880 --> 00:15:42,320
it to machines that have gpus

430
00:15:43,360 --> 00:15:48,720
this is what the api looks like

431
00:15:46,800 --> 00:15:50,479
i won't go through the details of it but

432
00:15:48,720 --> 00:15:52,720
there's an initial there's an api

433
00:15:50,480 --> 00:15:55,680
for defining the modes that you care

434
00:15:52,720 --> 00:15:57,279
about so those 256 models that we want

435
00:15:55,680 --> 00:15:59,839
to run

436
00:15:57,279 --> 00:16:01,040
you define what those are and those can

437
00:15:59,839 --> 00:16:04,160
be architectures

438
00:16:01,040 --> 00:16:05,759
or hyper parameters and then

439
00:16:04,160 --> 00:16:07,759
you want to train those models you want

440
00:16:05,759 --> 00:16:11,600
to train them change them from purple

441
00:16:07,759 --> 00:16:13,199
to orange and there's a function that

442
00:16:11,600 --> 00:16:14,720
is used to do that you specify the

443
00:16:13,199 --> 00:16:15,920
number of iterations whether you want to

444
00:16:14,720 --> 00:16:19,839
use gpus or

445
00:16:15,920 --> 00:16:22,160
or not right so you

446
00:16:19,839 --> 00:16:24,959
create your models you want to run and

447
00:16:22,160 --> 00:16:27,120
then you train them

448
00:16:24,959 --> 00:16:28,000
so i'd like to go through a couple of

449
00:16:27,120 --> 00:16:30,480
examples

450
00:16:28,000 --> 00:16:30,480
of the

451
00:16:31,360 --> 00:16:34,480
of the results of this so we're going to

452
00:16:33,279 --> 00:16:37,519
use something called

453
00:16:34,480 --> 00:16:40,079
cipher which is a kind of well-known

454
00:16:37,519 --> 00:16:42,959
image processing um

455
00:16:40,079 --> 00:16:44,638
image data set and we're running on

456
00:16:42,959 --> 00:16:46,000
google cloud platform

457
00:16:44,639 --> 00:16:48,000
the approach that we're going to do to

458
00:16:46,000 --> 00:16:49,199
training is to initially cast the net

459
00:16:48,000 --> 00:16:51,279
wide

460
00:16:49,199 --> 00:16:52,719
and then once we've casted the net wide

461
00:16:51,279 --> 00:16:54,320
we're going to look at certain areas

462
00:16:52,720 --> 00:16:54,800
that are of interest and kind of zoom in

463
00:16:54,320 --> 00:16:57,680
on

464
00:16:54,800 --> 00:16:59,439
on those so here's the model

465
00:16:57,680 --> 00:17:02,160
configurations that we want to

466
00:16:59,440 --> 00:17:03,360
run we have three model architectures we

467
00:17:02,160 --> 00:17:05,280
want to run

468
00:17:03,360 --> 00:17:07,679
we have a bunch of optimizers with

469
00:17:05,280 --> 00:17:10,639
different parameters and we want to try

470
00:17:07,679 --> 00:17:13,760
four batch sizes and if we do that that

471
00:17:10,640 --> 00:17:16,400
ends up with 96 configurations

472
00:17:13,760 --> 00:17:17,679
so we train those on this parallel

473
00:17:16,400 --> 00:17:20,959
database

474
00:17:17,679 --> 00:17:24,319
and we get 96 different

475
00:17:20,959 --> 00:17:25,520
curves right so we have accuracy on the

476
00:17:24,319 --> 00:17:29,199
left side

477
00:17:25,520 --> 00:17:29,199
and the loss on the right side

478
00:17:29,440 --> 00:17:33,679
so we might say oh why don't we just use

479
00:17:32,160 --> 00:17:35,520
the most accurate one

480
00:17:33,679 --> 00:17:37,360
this is the most accurate curve it's

481
00:17:35,520 --> 00:17:38,799
about 82 percent

482
00:17:37,360 --> 00:17:42,159
but there's a danger in doing that

483
00:17:38,799 --> 00:17:44,160
because it may be overfitting the data

484
00:17:42,160 --> 00:17:46,960
so the top curve here on the left hand

485
00:17:44,160 --> 00:17:49,520
side is actually the training data

486
00:17:46,960 --> 00:17:50,240
the bottom curve is on the validation

487
00:17:49,520 --> 00:17:52,720
data

488
00:17:50,240 --> 00:17:54,640
and you can see you have a big gap there

489
00:17:52,720 --> 00:17:55,440
so that's dangerous because it means

490
00:17:54,640 --> 00:17:59,280
it's not your

491
00:17:55,440 --> 00:18:01,440
your model is not generalizing well

492
00:17:59,280 --> 00:18:02,960
so what you can do is to do your next

493
00:18:01,440 --> 00:18:06,400
iteration

494
00:18:02,960 --> 00:18:07,919
is to plot the variance plot those

495
00:18:06,400 --> 00:18:10,880
differences

496
00:18:07,919 --> 00:18:12,720
there and then pick the ones that have

497
00:18:10,880 --> 00:18:16,080
the smallest difference

498
00:18:12,720 --> 00:18:18,720
right this is one technique so uh

499
00:18:16,080 --> 00:18:20,720
on the left-hand side we have the deltas

500
00:18:18,720 --> 00:18:24,160
the bottom is all the mall support

501
00:18:20,720 --> 00:18:26,080
uh the um the tuples that we ran

502
00:18:24,160 --> 00:18:27,440
and then we're gonna pick you know the

503
00:18:26,080 --> 00:18:29,199
good ones

504
00:18:27,440 --> 00:18:30,480
here's an example of one that overfit

505
00:18:29,200 --> 00:18:33,919
less right

506
00:18:30,480 --> 00:18:35,919
so we

507
00:18:33,919 --> 00:18:37,120
refine the model configurations that we

508
00:18:35,919 --> 00:18:39,760
want to run

509
00:18:37,120 --> 00:18:40,959
based on kind of the good ones we saw

510
00:18:39,760 --> 00:18:42,879
and then

511
00:18:40,960 --> 00:18:44,799
it's a smaller number here we're going

512
00:18:42,880 --> 00:18:48,559
to run just 12 of them

513
00:18:44,799 --> 00:18:49,520
and we run those 12 and we can pick a

514
00:18:48,559 --> 00:18:51,200
good one from

515
00:18:49,520 --> 00:18:52,559
from there because these are ones that

516
00:18:51,200 --> 00:18:55,919
that overfit less

517
00:18:52,559 --> 00:18:57,760
right so here's a sample good result and

518
00:18:55,919 --> 00:18:59,760
then once we have a sample good result

519
00:18:57,760 --> 00:19:02,000
it's now in the order of 80 percent

520
00:18:59,760 --> 00:19:04,799
the modest amount of overfitting we can

521
00:19:02,000 --> 00:19:10,080
do inference on it which is prediction

522
00:19:04,799 --> 00:19:13,360
and what does this look like a dog

523
00:19:10,080 --> 00:19:15,600
yeah the model says it's a dog too okay

524
00:19:13,360 --> 00:19:18,159
so i'm going to finish up quickly

525
00:19:15,600 --> 00:19:20,559
talking about automating that

526
00:19:18,160 --> 00:19:23,360
so you can add automated machine

527
00:19:20,559 --> 00:19:25,120
learning on top of this

528
00:19:23,360 --> 00:19:26,959
one example that we've done here is with

529
00:19:25,120 --> 00:19:28,559
hyperband and hyper

530
00:19:26,960 --> 00:19:30,000
band uses the idea of what's called

531
00:19:28,559 --> 00:19:31,840
success of having

532
00:19:30,000 --> 00:19:33,600
so it's automating some of the steps

533
00:19:31,840 --> 00:19:35,199
that we did before

534
00:19:33,600 --> 00:19:37,199
um so we don't have to do as much

535
00:19:35,200 --> 00:19:40,240
analysis ourselves we start with a big

536
00:19:37,200 --> 00:19:42,880
set of models we train them for a while

537
00:19:40,240 --> 00:19:44,240
you get a subset of models from there

538
00:19:42,880 --> 00:19:46,960
you train again

539
00:19:44,240 --> 00:19:47,760
you have survivors there that you

540
00:19:46,960 --> 00:19:49,520
continue

541
00:19:47,760 --> 00:19:51,120
with

542
00:19:49,520 --> 00:19:53,280
[Music]

543
00:19:51,120 --> 00:19:55,840
so the way hyperband works is you

544
00:19:53,280 --> 00:19:58,639
specify a couple of parameters including

545
00:19:55,840 --> 00:19:59,600
the resources you want to use and how

546
00:19:58,640 --> 00:20:02,799
you want to

547
00:19:59,600 --> 00:20:05,520
have your data and once you do that

548
00:20:02,799 --> 00:20:07,039
it creates a schedule for you don't have

549
00:20:05,520 --> 00:20:08,639
to worry too too much about how the

550
00:20:07,039 --> 00:20:10,080
schedule is created there's a whole

551
00:20:08,640 --> 00:20:12,960
theory behind it

552
00:20:10,080 --> 00:20:14,879
but if you train this if you run

553
00:20:12,960 --> 00:20:20,480
according to this schedule

554
00:20:14,880 --> 00:20:20,480
for example you start with picking 81

555
00:20:21,039 --> 00:20:24,158
different models you train it for one

556
00:20:22,720 --> 00:20:27,039
iteration

557
00:20:24,159 --> 00:20:28,159
then you pick the best 27 you train it

558
00:20:27,039 --> 00:20:30,400
for three more

559
00:20:28,159 --> 00:20:30,730
and you proceed through these brackets

560
00:20:30,400 --> 00:20:31,919
then

561
00:20:30,730 --> 00:20:33,919
[Music]

562
00:20:31,919 --> 00:20:35,600
this is a way of automating the model

563
00:20:33,919 --> 00:20:38,240
selection

564
00:20:35,600 --> 00:20:39,280
approach so we do that here again we

565
00:20:38,240 --> 00:20:40,799
start with a bunch of model

566
00:20:39,280 --> 00:20:42,480
configurations

567
00:20:40,799 --> 00:20:45,039
now we're not doing grid search we

568
00:20:42,480 --> 00:20:46,480
actually set ranges for things because

569
00:20:45,039 --> 00:20:50,158
we need to generate

570
00:20:46,480 --> 00:20:51,600
generate different combinations um

571
00:20:50,159 --> 00:20:53,679
and then we run it according to the

572
00:20:51,600 --> 00:20:55,918
schedule and the same thing happens

573
00:20:53,679 --> 00:20:58,559
right you get a lot of information

574
00:20:55,919 --> 00:21:00,559
you could stop here and say i'm gonna

575
00:20:58,559 --> 00:21:02,559
i'm gonna just go through one path of my

576
00:21:00,559 --> 00:21:06,720
automated machine learning

577
00:21:02,559 --> 00:21:08,720
you could then do another set of

578
00:21:06,720 --> 00:21:10,480
of the same thing which is what i did

579
00:21:08,720 --> 00:21:11,520
here so it's the same steps we did

580
00:21:10,480 --> 00:21:14,240
before

581
00:21:11,520 --> 00:21:14,799
is we looked at ones that were very good

582
00:21:14,240 --> 00:21:16,880
we

583
00:21:14,799 --> 00:21:18,559
kept those we threw out the ones that

584
00:21:16,880 --> 00:21:20,960
are not very good

585
00:21:18,559 --> 00:21:22,158
and then we do another round and you

586
00:21:20,960 --> 00:21:24,159
might run that for

587
00:21:22,159 --> 00:21:25,760
much longer for example because you have

588
00:21:24,159 --> 00:21:30,000
your your better

589
00:21:25,760 --> 00:21:33,039
models so once you do that um

590
00:21:30,000 --> 00:21:35,679
you re-run and you get your next

591
00:21:33,039 --> 00:21:37,360
you get better models and here they're

592
00:21:35,679 --> 00:21:40,960
you can see they're in the order of you

593
00:21:37,360 --> 00:21:40,959
know 80 percent or so

594
00:21:41,039 --> 00:21:44,158
here's an example of a model from here

595
00:21:43,600 --> 00:21:46,240
with

596
00:21:44,159 --> 00:21:47,440
yeah close to 80 percent accuracy but

597
00:21:46,240 --> 00:21:50,480
very little uh

598
00:21:47,440 --> 00:21:50,480
very little over thing

599
00:21:51,840 --> 00:21:58,720
so to wrap up model hopper

600
00:21:56,240 --> 00:21:59,679
is a interesting kind of approach for

601
00:21:58,720 --> 00:22:03,600
doing

602
00:21:59,679 --> 00:22:05,520
distributed training of deep nets

603
00:22:03,600 --> 00:22:07,039
you can implement it in a massively

604
00:22:05,520 --> 00:22:08,240
parallel processing database like

605
00:22:07,039 --> 00:22:11,039
greenplum it works pretty

606
00:22:08,240 --> 00:22:12,960
pretty well it scales very well you can

607
00:22:11,039 --> 00:22:14,240
add automated machine learning methods

608
00:22:12,960 --> 00:22:16,240
on top of that

609
00:22:14,240 --> 00:22:17,520
so that you have to do less work

610
00:22:16,240 --> 00:22:20,880
yourself

611
00:22:17,520 --> 00:22:23,918
in terms of selecting good models

612
00:22:20,880 --> 00:22:25,360
and for future work a short list of

613
00:22:23,919 --> 00:22:29,360
things we're working on

614
00:22:25,360 --> 00:22:31,439
we want to improve gpu efficiency

615
00:22:29,360 --> 00:22:33,280
because we're working in the database

616
00:22:31,440 --> 00:22:35,440
we're not quite as efficient as the

617
00:22:33,280 --> 00:22:37,280
results that show in the paper

618
00:22:35,440 --> 00:22:38,640
we want to add more automated machine

619
00:22:37,280 --> 00:22:40,720
learning methods

620
00:22:38,640 --> 00:22:44,080
and then add pi torch support in

621
00:22:40,720 --> 00:22:45,760
addition to tensorflow

622
00:22:44,080 --> 00:22:48,240
there are some references including some

623
00:22:45,760 --> 00:22:55,840
jupiter workbooks to get started

624
00:22:48,240 --> 00:22:55,840
thank you for your attention

625
00:22:57,280 --> 00:23:02,320
thank you frank we have uh three minutes

626
00:22:59,600 --> 00:23:09,840
for questions please stay seated

627
00:23:02,320 --> 00:23:09,840
any questions

628
00:23:13,679 --> 00:23:19,840
hi thanks for tonight nice talk um

629
00:23:17,760 --> 00:23:21,919
with the partitions of mini batches you

630
00:23:19,840 --> 00:23:23,439
see that it's very important the data is

631
00:23:21,919 --> 00:23:26,080
processed sequentially

632
00:23:23,440 --> 00:23:27,760
but then model hopper distributes the

633
00:23:26,080 --> 00:23:30,320
partition

634
00:23:27,760 --> 00:23:31,280
in the different worker nodes but only

635
00:23:30,320 --> 00:23:33,678
one of the models

636
00:23:31,280 --> 00:23:35,440
is going to start from the beginning and

637
00:23:33,679 --> 00:23:38,559
follow the sequential read

638
00:23:35,440 --> 00:23:42,000
is that an issue or is the partitioning

639
00:23:38,559 --> 00:23:44,240
model hopper different to the mini batch

640
00:23:42,000 --> 00:23:45,520
um it's logically efficient to

641
00:23:44,240 --> 00:23:48,240
sequentially

642
00:23:45,520 --> 00:23:49,679
going through all of the data so the

643
00:23:48,240 --> 00:23:51,679
question was

644
00:23:49,679 --> 00:23:53,039
is there a is there an issue with

645
00:23:51,679 --> 00:23:55,440
visiting the data

646
00:23:53,039 --> 00:23:56,240
in the right order or incomplete with

647
00:23:55,440 --> 00:23:58,480
model hopper

648
00:23:56,240 --> 00:23:59,600
i think that was the question yeah and

649
00:23:58,480 --> 00:24:01,840
the answer is that

650
00:23:59,600 --> 00:24:03,360
it's logically the same as as

651
00:24:01,840 --> 00:24:05,199
sequentially going through all of the

652
00:24:03,360 --> 00:24:08,158
data so when you hop from

653
00:24:05,200 --> 00:24:10,720
from machine to machine you're visiting

654
00:24:08,159 --> 00:24:12,320
a new set of data on that machine

655
00:24:10,720 --> 00:24:15,279
right so you when you finish one

656
00:24:12,320 --> 00:24:16,320
iteration each model has seen all of the

657
00:24:15,279 --> 00:24:20,159
data

658
00:24:16,320 --> 00:24:22,799
the thing of of gradient descent is that

659
00:24:20,159 --> 00:24:23,440
it's uh robust with respect to visit

660
00:24:22,799 --> 00:24:25,200
order

661
00:24:23,440 --> 00:24:27,200
so you don't need to visit the data in

662
00:24:25,200 --> 00:24:29,440
the same order each time

663
00:24:27,200 --> 00:24:31,200
um in fact it's randomized initially up

664
00:24:29,440 --> 00:24:34,799
front before it's distributed

665
00:24:31,200 --> 00:24:36,000
but you do need to visit all of the data

666
00:24:34,799 --> 00:24:38,639
because that's very important for

667
00:24:36,000 --> 00:24:41,120
convergence okay thank you

668
00:24:38,640 --> 00:24:41,120
you're welcome

669
00:24:44,640 --> 00:24:51,279
one more quick question maybe but

670
00:24:48,000 --> 00:24:51,279
we're almost running out of time

671
00:24:51,440 --> 00:25:01,840
all right then let's let's rap about

672
00:24:52,960 --> 00:25:01,840
wrap it up thank you frank

673
00:25:11,440 --> 00:25:13,520
you

