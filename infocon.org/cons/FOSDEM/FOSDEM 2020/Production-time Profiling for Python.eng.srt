1
00:00:11,440 --> 00:00:13,920
right

2
00:00:14,240 --> 00:00:18,080
next talk please welcome julian who's

3
00:00:16,239 --> 00:00:31,839
going to tell us about profiling in

4
00:00:18,080 --> 00:00:31,839
python thank you

5
00:00:40,879 --> 00:00:46,160
oh okay so sorry so today i'm gonna talk

6
00:00:44,640 --> 00:00:48,160
about uh production

7
00:00:46,160 --> 00:00:49,760
uh profiling for python so if you don't

8
00:00:48,160 --> 00:00:51,839
know me i'm julian

9
00:00:49,760 --> 00:00:53,280
i worked at deladoc for almost a year

10
00:00:51,840 --> 00:00:55,199
now and i've been doing

11
00:00:53,280 --> 00:00:58,320
free software stuff for like 20 years

12
00:00:55,199 --> 00:00:59,839
now um uh

13
00:00:58,320 --> 00:01:01,600
this is a boring statement let's say

14
00:00:59,840 --> 00:01:03,359
that anything that i say today might

15
00:01:01,600 --> 00:01:04,000
change in the future but i have to say

16
00:01:03,359 --> 00:01:06,960
that to you

17
00:01:04,000 --> 00:01:09,520
so um so first i'm going to talk about

18
00:01:06,960 --> 00:01:12,639
what profiling is and what you need it

19
00:01:09,520 --> 00:01:15,759
uh then i'll talk about uh how we build

20
00:01:12,640 --> 00:01:17,439
a dialogue a profiler for production

21
00:01:15,759 --> 00:01:17,920
system that we use internally that we're

22
00:01:17,439 --> 00:01:21,439
going to

23
00:01:17,920 --> 00:01:23,119
provide to our customers in the future

24
00:01:21,439 --> 00:01:24,559
so the first thing to understand is what

25
00:01:23,119 --> 00:01:27,040
profiling is and what

26
00:01:24,560 --> 00:01:27,759
when you need it uh if you never use

27
00:01:27,040 --> 00:01:30,320
profiling

28
00:01:27,759 --> 00:01:31,280
here's the simple answer that it tries

29
00:01:30,320 --> 00:01:32,798
to to solve

30
00:01:31,280 --> 00:01:34,880
as a sort of question so to solve it's

31
00:01:32,799 --> 00:01:37,280
like if i were to ask you

32
00:01:34,880 --> 00:01:39,280
in a program which function or which

33
00:01:37,280 --> 00:01:40,960
line of code is going to be the most

34
00:01:39,280 --> 00:01:42,240
costly the most

35
00:01:40,960 --> 00:01:44,079
time you're going to spend in your

36
00:01:42,240 --> 00:01:45,600
program running and doing that function

37
00:01:44,079 --> 00:01:47,520
of that line of god

38
00:01:45,600 --> 00:01:49,759
it will be probably pretty hard for you

39
00:01:47,520 --> 00:01:52,479
to guess or to to give me an answer

40
00:01:49,759 --> 00:01:53,680
an accurate answer it's really hard to

41
00:01:52,479 --> 00:01:56,399
know when you run

42
00:01:53,680 --> 00:01:56,960
programs in production which endpoint is

43
00:01:56,399 --> 00:02:00,240
going to

44
00:01:56,960 --> 00:02:02,399
spend a lot of time on the cpu

45
00:02:00,240 --> 00:02:04,479
the one line that is going to look at

46
00:02:02,399 --> 00:02:06,159
the most memory in your programs

47
00:02:04,479 --> 00:02:07,600
you might guess but the more complex

48
00:02:06,159 --> 00:02:08,879
your application is going to be

49
00:02:07,600 --> 00:02:10,959
the more complicated it's going to

50
00:02:08,878 --> 00:02:12,799
answer that question

51
00:02:10,959 --> 00:02:15,040
um so the goal of profiling is to

52
00:02:12,800 --> 00:02:17,840
actually get inside your program and

53
00:02:15,040 --> 00:02:19,599
get all of this data and extract it and

54
00:02:17,840 --> 00:02:21,280
be able to say okay this is a stack

55
00:02:19,599 --> 00:02:22,399
trace what i see most of them this is a

56
00:02:21,280 --> 00:02:24,400
stack trace so what i

57
00:02:22,400 --> 00:02:26,319
talk about stack trace is like all of

58
00:02:24,400 --> 00:02:28,800
the functions that are being called

59
00:02:26,319 --> 00:02:30,319
one after the other and being able to

60
00:02:28,800 --> 00:02:31,760
okay this is

61
00:02:30,319 --> 00:02:33,359
what my program is doing and this is

62
00:02:31,760 --> 00:02:34,000
where i need to optimize it to make it

63
00:02:33,360 --> 00:02:35,599
faster

64
00:02:34,000 --> 00:02:37,280
because this is the end goal is to make

65
00:02:35,599 --> 00:02:38,399
to be able to see what your program is

66
00:02:37,280 --> 00:02:40,640
doing and to make

67
00:02:38,400 --> 00:02:43,680
and to make better decisions about where

68
00:02:40,640 --> 00:02:45,200
to spend your time to optimize it

69
00:02:43,680 --> 00:02:46,959
uh this is typically the kind of output

70
00:02:45,200 --> 00:02:48,720
that you will see if you do profiling on

71
00:02:46,959 --> 00:02:50,480
a python program or any kind of program

72
00:02:48,720 --> 00:02:52,640
for that matter

73
00:02:50,480 --> 00:02:54,480
where you see stack traces and when the

74
00:02:52,640 --> 00:02:56,079
width of the rectangle is like the term

75
00:02:54,480 --> 00:03:00,000
spent on cpu or

76
00:02:56,080 --> 00:03:00,000
allocating memory or anything like that

77
00:03:00,080 --> 00:03:04,000
there are two types of profiling the

78
00:03:02,480 --> 00:03:06,560
first one is deterministic profiling

79
00:03:04,000 --> 00:03:08,239
so if you ever use c profile the python

80
00:03:06,560 --> 00:03:10,800
profiler which is built in

81
00:03:08,239 --> 00:03:11,680
since like a long time in python this is

82
00:03:10,800 --> 00:03:14,879
what it does

83
00:03:11,680 --> 00:03:16,159
um and there's another type

84
00:03:14,879 --> 00:03:17,440
of profiling which is what we're going

85
00:03:16,159 --> 00:03:18,000
to talk about today is like the

86
00:03:17,440 --> 00:03:20,000
statistical

87
00:03:18,000 --> 00:03:21,840
profiling where you do things in a

88
00:03:20,000 --> 00:03:24,720
different way which has its own

89
00:03:21,840 --> 00:03:25,360
perks so deterministic profiling if you

90
00:03:24,720 --> 00:03:26,799
never use c

91
00:03:25,360 --> 00:03:29,040
profile or if you don't know it works

92
00:03:26,799 --> 00:03:30,879
it's pretty easy it actually hooks

93
00:03:29,040 --> 00:03:32,798
itself inside python it says to

94
00:03:30,879 --> 00:03:34,159
see python each time you're going to

95
00:03:32,799 --> 00:03:36,319
call and execute a function

96
00:03:34,159 --> 00:03:37,519
i want you to tell me and i will like

97
00:03:36,319 --> 00:03:39,440
not down the time

98
00:03:37,519 --> 00:03:41,120
it is and when you finish to execute the

99
00:03:39,440 --> 00:03:42,640
function you just call me again and i

100
00:03:41,120 --> 00:03:43,840
will note down the time and then i will

101
00:03:42,640 --> 00:03:45,440
compute the difference between the two

102
00:03:43,840 --> 00:03:47,599
times and i will know that which

103
00:03:45,440 --> 00:03:49,200
this function has run for that many

104
00:03:47,599 --> 00:03:50,879
seconds

105
00:03:49,200 --> 00:03:52,480
so that's pretty cool it's pretty easy

106
00:03:50,879 --> 00:03:55,920
to understand but

107
00:03:52,480 --> 00:03:58,798
it has a few issues which is like

108
00:03:55,920 --> 00:04:00,159
you only know the whole times that means

109
00:03:58,799 --> 00:04:02,799
you know

110
00:04:00,159 --> 00:04:04,798
how much sequence your function has

111
00:04:02,799 --> 00:04:07,040
taken but you don't know what it did

112
00:04:04,799 --> 00:04:08,640
like it was it waiting for io was it

113
00:04:07,040 --> 00:04:10,400
doing cpu stack what is it allocating

114
00:04:08,640 --> 00:04:12,159
memory do you have nuclear you just

115
00:04:10,400 --> 00:04:14,319
know that it's been two seconds exiting

116
00:04:12,159 --> 00:04:16,719
the function

117
00:04:14,319 --> 00:04:18,159
the way it works is that it looks itself

118
00:04:16,720 --> 00:04:19,680
on functions which

119
00:04:18,160 --> 00:04:21,199
is the granularity that you will get

120
00:04:19,680 --> 00:04:23,440
from the output so

121
00:04:21,199 --> 00:04:25,280
if your program is just one giant

122
00:04:23,440 --> 00:04:27,199
function we will have nothing

123
00:04:25,280 --> 00:04:29,280
interesting in terms of information and

124
00:04:27,199 --> 00:04:30,639
if your program is fast enough function

125
00:04:29,280 --> 00:04:32,638
or very small function calling each

126
00:04:30,639 --> 00:04:35,120
other it's going to

127
00:04:32,639 --> 00:04:35,840
have so much overhead that your program

128
00:04:35,120 --> 00:04:38,479
is going to be

129
00:04:35,840 --> 00:04:40,159
10 times or 20 times slower when it runs

130
00:04:38,479 --> 00:04:41,840
so it's pretty hard to use c profile in

131
00:04:40,160 --> 00:04:42,880
production system for that reason

132
00:04:41,840 --> 00:04:44,799
because the most

133
00:04:42,880 --> 00:04:46,560
functions you have in your program the

134
00:04:44,800 --> 00:04:49,280
slower it's going to be

135
00:04:46,560 --> 00:04:50,320
um c profile in particular has this one

136
00:04:49,280 --> 00:04:52,080
problem where

137
00:04:50,320 --> 00:04:53,440
in our case it was an issue where it

138
00:04:52,080 --> 00:04:55,599
exports its data

139
00:04:53,440 --> 00:04:56,880
also in a custom file format which is

140
00:04:55,600 --> 00:04:57,919
something like pcl if i remember

141
00:04:56,880 --> 00:05:00,479
correctly

142
00:04:57,919 --> 00:05:02,000
which is not something that is really

143
00:05:00,479 --> 00:05:03,919
easy to use in any other programming

144
00:05:02,000 --> 00:05:06,000
language

145
00:05:03,919 --> 00:05:07,359
so for all of that those reasons it's

146
00:05:06,000 --> 00:05:09,120
impossible to use something like

147
00:05:07,360 --> 00:05:11,199
deterministic profiling and c profile

148
00:05:09,120 --> 00:05:13,520
in production system you can't afford to

149
00:05:11,199 --> 00:05:14,639
have your web server to run 20 times

150
00:05:13,520 --> 00:05:16,799
slower just because you're doing

151
00:05:14,639 --> 00:05:18,960
profiling

152
00:05:16,800 --> 00:05:20,479
the other way of doing profiling is

153
00:05:18,960 --> 00:05:21,680
different is like you're going to run

154
00:05:20,479 --> 00:05:23,280
your program normally you're not going

155
00:05:21,680 --> 00:05:26,000
to change anything

156
00:05:23,280 --> 00:05:27,359
but every now and then like every 10

157
00:05:26,000 --> 00:05:28,479
millisecond or so you're going to

158
00:05:27,360 --> 00:05:30,639
interrupt your program

159
00:05:28,479 --> 00:05:32,560
and to look at what it's doing right now

160
00:05:30,639 --> 00:05:33,199
so what is going to tell you if you do

161
00:05:32,560 --> 00:05:34,720
that

162
00:05:33,199 --> 00:05:36,400
often enough you're going to have

163
00:05:34,720 --> 00:05:38,400
statistically a good representation of

164
00:05:36,400 --> 00:05:40,320
what your program is doing usually

165
00:05:38,400 --> 00:05:42,560
so it's not like the absolute truth

166
00:05:40,320 --> 00:05:43,520
contrary to contrary to something like c

167
00:05:42,560 --> 00:05:45,360
profile which

168
00:05:43,520 --> 00:05:46,799
really meters every call you're just

169
00:05:45,360 --> 00:05:49,199
going to in that case

170
00:05:46,800 --> 00:05:50,800
statistically over a few hours or a few

171
00:05:49,199 --> 00:05:52,880
days of your program running

172
00:05:50,800 --> 00:05:54,560
going to have a number of samples that's

173
00:05:52,880 --> 00:05:56,719
going to show you

174
00:05:54,560 --> 00:05:58,160
um which number of times you see each

175
00:05:56,720 --> 00:05:59,919
stack trace and you're going to have the

176
00:05:58,160 --> 00:06:03,759
kind of output that i showed you earlier

177
00:05:59,919 --> 00:06:05,919
which is roughly statistically accurate

178
00:06:03,759 --> 00:06:07,280
the upside of doing that is that you can

179
00:06:05,919 --> 00:06:08,560
actually measure a lot of different

180
00:06:07,280 --> 00:06:10,400
things like the wall time but also the

181
00:06:08,560 --> 00:06:12,880
cpu charm which tells you

182
00:06:10,400 --> 00:06:14,799
uh if you spend time just doing i o or

183
00:06:12,880 --> 00:06:16,880
actually using the cpu and doing things

184
00:06:14,800 --> 00:06:19,120
like computing results

185
00:06:16,880 --> 00:06:20,159
it has a very low overhead the way it

186
00:06:19,120 --> 00:06:22,000
works is that it

187
00:06:20,160 --> 00:06:24,080
leaves your program doing what it does

188
00:06:22,000 --> 00:06:25,919
usually and does not try to

189
00:06:24,080 --> 00:06:27,359
rub the functions or interrupt them or

190
00:06:25,919 --> 00:06:29,039
whatever it just

191
00:06:27,360 --> 00:06:30,560
know and then execute something in the

192
00:06:29,039 --> 00:06:31,199
background saying okay what is it doing

193
00:06:30,560 --> 00:06:34,240
right now

194
00:06:31,199 --> 00:06:35,919
and that's it uh you were able uh thanks

195
00:06:34,240 --> 00:06:37,919
to to python that is to go

196
00:06:35,919 --> 00:06:39,198
down to the line of code that is being

197
00:06:37,919 --> 00:06:41,280
executed so you're not just

198
00:06:39,199 --> 00:06:44,400
talking about functions but you're able

199
00:06:41,280 --> 00:06:46,239
to see which line of code in particular

200
00:06:44,400 --> 00:06:47,919
is going to be executed in some

201
00:06:46,240 --> 00:06:50,319
languages not python unfortunately you

202
00:06:47,919 --> 00:06:51,039
can go down to the opcode so the precise

203
00:06:50,319 --> 00:06:52,960
like

204
00:06:51,039 --> 00:06:54,400
operation in the line that's not

205
00:06:52,960 --> 00:06:56,479
possible in python yet

206
00:06:54,400 --> 00:06:57,440
unless you split your code a lot and you

207
00:06:56,479 --> 00:06:58,800
can have like

208
00:06:57,440 --> 00:07:00,560
other things like i'll talk about that

209
00:06:58,800 --> 00:07:02,319
later um see

210
00:07:00,560 --> 00:07:03,919
the report against the exception running

211
00:07:02,319 --> 00:07:04,960
in your program that are being raised

212
00:07:03,919 --> 00:07:06,639
and you're able to

213
00:07:04,960 --> 00:07:08,560
see them and to see the control panel

214
00:07:06,639 --> 00:07:11,039
program um

215
00:07:08,560 --> 00:07:11,680
so what i'm going to talk after is what

216
00:07:11,039 --> 00:07:13,919
we did

217
00:07:11,680 --> 00:07:15,680
at datalog implementing this uh before

218
00:07:13,919 --> 00:07:18,400
doing that i spent a lot of time

219
00:07:15,680 --> 00:07:20,880
uh checking the state of the art all the

220
00:07:18,400 --> 00:07:22,799
dozens of python profiles out there

221
00:07:20,880 --> 00:07:24,080
none of them does that a lot of them

222
00:07:22,800 --> 00:07:26,560
actually focus on

223
00:07:24,080 --> 00:07:27,840
their pooch doing fantasy flame chart or

224
00:07:26,560 --> 00:07:30,160
things like that

225
00:07:27,840 --> 00:07:31,679
which is not something i had to solve i

226
00:07:30,160 --> 00:07:33,680
was only interested into getting the

227
00:07:31,680 --> 00:07:34,479
data and exporting them into a proper

228
00:07:33,680 --> 00:07:36,960
data format

229
00:07:34,479 --> 00:07:38,719
so what i'm going to talk about just now

230
00:07:36,960 --> 00:07:40,479
so the first thing i had to say to the

231
00:07:38,720 --> 00:07:41,840
provider to solve was like how do you

232
00:07:40,479 --> 00:07:45,599
get the cpu

233
00:07:41,840 --> 00:07:46,239
and world time information so the way i

234
00:07:45,599 --> 00:07:48,800
saw that

235
00:07:46,240 --> 00:07:50,960
is in a thread so you run a part the

236
00:07:48,800 --> 00:07:53,199
parallel thread in your python program

237
00:07:50,960 --> 00:07:55,359
that slips most of the time let's say

238
00:07:53,199 --> 00:07:58,080
every 10 milliseconds you wake up

239
00:07:55,360 --> 00:07:59,440
and you're going to use the cis dot

240
00:07:58,080 --> 00:08:00,960
current friend which is like a

241
00:07:59,440 --> 00:08:02,560
private function that has been there

242
00:08:00,960 --> 00:08:04,159
forever and that everybody used so it's

243
00:08:02,560 --> 00:08:07,360
not really private

244
00:08:04,160 --> 00:08:09,440
but what this function returns is a list

245
00:08:07,360 --> 00:08:11,360
a dictionary where the key is a thread

246
00:08:09,440 --> 00:08:14,400
and the value is a

247
00:08:11,360 --> 00:08:16,080
stack trace basically so if you do every

248
00:08:14,400 --> 00:08:18,080
10 milliseconds

249
00:08:16,080 --> 00:08:20,400
regularly you're able to each time see

250
00:08:18,080 --> 00:08:21,840
which stack is being executed

251
00:08:20,400 --> 00:08:24,159
on each thread and you're able to

252
00:08:21,840 --> 00:08:25,758
construct a model where you know

253
00:08:24,160 --> 00:08:28,000
like in real time what your program is

254
00:08:25,759 --> 00:08:30,639
doing every 10 milliseconds

255
00:08:28,000 --> 00:08:31,759
uh when you do that you can actually use

256
00:08:30,639 --> 00:08:35,120
uh so that's only

257
00:08:31,759 --> 00:08:36,959
linux specific for this part a function

258
00:08:35,120 --> 00:08:39,279
that is um

259
00:08:36,958 --> 00:08:41,199
thread get cpu clock id which basically

260
00:08:39,279 --> 00:08:44,480
written you a special clock

261
00:08:41,200 --> 00:08:47,600
and in line which measures the cpu time

262
00:08:44,480 --> 00:08:49,040
but each its thread is consuming so

263
00:08:47,600 --> 00:08:50,880
every 10 milliseconds you will be able

264
00:08:49,040 --> 00:08:52,800
to get a stack trace and

265
00:08:50,880 --> 00:08:54,720
actually associate that stack trace with

266
00:08:52,800 --> 00:08:56,800
a number of nanoseconds or milliseconds

267
00:08:54,720 --> 00:08:58,560
of cpu that has been used by the thread

268
00:08:56,800 --> 00:09:00,399
um so at the end you get a pretty good

269
00:08:58,560 --> 00:09:01,040
representation on each of your threads

270
00:09:00,399 --> 00:09:04,320
what they

271
00:09:01,040 --> 00:09:06,079
would do and how much cpu they used uh

272
00:09:04,320 --> 00:09:08,000
you can actually also get the wall charm

273
00:09:06,080 --> 00:09:10,399
because you can use a regular

274
00:09:08,000 --> 00:09:11,920
clock uh so you get this kind of output

275
00:09:10,399 --> 00:09:14,000
where you have the regular so this is an

276
00:09:11,920 --> 00:09:15,760
example of a python program we're like

277
00:09:14,000 --> 00:09:17,600
uh being profiled for one minute with

278
00:09:15,760 --> 00:09:19,200
four threads that's why you may not see

279
00:09:17,600 --> 00:09:19,839
but there's like a four minutes on the

280
00:09:19,200 --> 00:09:22,080
top root

281
00:09:19,839 --> 00:09:23,519
uh span on the top there's like four

282
00:09:22,080 --> 00:09:24,000
minutes of wall time in total because

283
00:09:23,519 --> 00:09:25,839
you got

284
00:09:24,000 --> 00:09:27,040
four threads running for one minute and

285
00:09:25,839 --> 00:09:28,720
you have the detail of

286
00:09:27,040 --> 00:09:30,560
which function has been called in which

287
00:09:28,720 --> 00:09:32,560
thread and if you have the same data

288
00:09:30,560 --> 00:09:34,160
for the cpu time which is like way less

289
00:09:32,560 --> 00:09:36,479
there's only like three seconds or

290
00:09:34,160 --> 00:09:38,399
five seconds cpu time being used and you

291
00:09:36,480 --> 00:09:40,000
see the output looks like a bit the same

292
00:09:38,399 --> 00:09:41,680
you see the same stack traces

293
00:09:40,000 --> 00:09:43,920
but you see that a lot of the functions

294
00:09:41,680 --> 00:09:46,399
are like weight functions that are just

295
00:09:43,920 --> 00:09:47,920
doing nothing so if you were using

296
00:09:46,399 --> 00:09:49,519
something like c profile you would see a

297
00:09:47,920 --> 00:09:50,479
lot of time being spread in the weight

298
00:09:49,519 --> 00:09:52,000
function

299
00:09:50,480 --> 00:09:53,519
which is not really interesting because

300
00:09:52,000 --> 00:09:55,760
it's just a weight function

301
00:09:53,519 --> 00:09:57,519
where if you take a look at the cpu time

302
00:09:55,760 --> 00:09:58,560
using the data you got for the thread

303
00:09:57,519 --> 00:10:00,800
gcpu

304
00:09:58,560 --> 00:10:02,399
clock id you actually get the number of

305
00:10:00,800 --> 00:10:03,920
nanoseconds being used on the cpu and

306
00:10:02,399 --> 00:10:04,560
you can see here for example the weight

307
00:10:03,920 --> 00:10:06,319
function

308
00:10:04,560 --> 00:10:08,560
we don't see them in the in the charts

309
00:10:06,320 --> 00:10:08,560
below

310
00:10:09,279 --> 00:10:12,480
so otherwise as i was saying it's a way

311
00:10:11,680 --> 00:10:14,560
better

312
00:10:12,480 --> 00:10:17,120
way to do profiling in production system

313
00:10:14,560 --> 00:10:20,560
because it has a very low overhead so

314
00:10:17,120 --> 00:10:23,600
for example on my laptop if i

315
00:10:20,560 --> 00:10:24,959
if i am to a profile a python program

316
00:10:23,600 --> 00:10:27,440
with 10 threads

317
00:10:24,959 --> 00:10:30,000
which are which have each one executing

318
00:10:27,440 --> 00:10:32,800
30 function long stack traces

319
00:10:30,000 --> 00:10:33,519
i can pull and generate 100 samples per

320
00:10:32,800 --> 00:10:36,640
second

321
00:10:33,519 --> 00:10:40,399
for only one percent of cpu so it's

322
00:10:36,640 --> 00:10:42,640
i mean it's already a pretty good um

323
00:10:40,399 --> 00:10:44,560
thing to get and it's it's actually can

324
00:10:42,640 --> 00:10:45,839
get a bit cheaper because the way it

325
00:10:44,560 --> 00:10:47,279
works you sleep every like 10

326
00:10:45,839 --> 00:10:48,880
milliseconds but you can actually

327
00:10:47,279 --> 00:10:50,399
increase that amount of sleeping so if

328
00:10:48,880 --> 00:10:52,000
you want to have less impact of your

329
00:10:50,399 --> 00:10:55,040
performance you can dynamically

330
00:10:52,000 --> 00:10:55,600
adapt uh your sleeping time like i was

331
00:10:55,040 --> 00:10:58,000
saying you

332
00:10:55,600 --> 00:10:59,279
are able to get so you need to do a bit

333
00:10:58,000 --> 00:11:01,279
of c for that

334
00:10:59,279 --> 00:11:03,439
but because it's not exposed in python

335
00:11:01,279 --> 00:11:04,800
but you actually able to get the

336
00:11:03,440 --> 00:11:08,000
profiling

337
00:11:04,800 --> 00:11:10,719
exception information so when you

338
00:11:08,000 --> 00:11:12,000
use a block like try accept and it goes

339
00:11:10,720 --> 00:11:13,920
back to a few functions

340
00:11:12,000 --> 00:11:15,040
before being called you can you can see

341
00:11:13,920 --> 00:11:18,000
that in python it's

342
00:11:15,040 --> 00:11:19,680
equivalent of this dot x info you can

343
00:11:18,000 --> 00:11:21,120
catch that and see the control for your

344
00:11:19,680 --> 00:11:22,880
after program

345
00:11:21,120 --> 00:11:24,959
and you can limit the resources i was i

346
00:11:22,880 --> 00:11:28,000
was saying you can basically sleep

347
00:11:24,959 --> 00:11:29,518
uh more often if you need

348
00:11:28,000 --> 00:11:31,040
right now the only downside that we

349
00:11:29,519 --> 00:11:33,680
don't do c profiling so

350
00:11:31,040 --> 00:11:35,439
if you have psiphon module or c module

351
00:11:33,680 --> 00:11:38,239
uh you are not able to see any

352
00:11:35,440 --> 00:11:39,839
information below the python stack

353
00:11:38,240 --> 00:11:42,720
uh that's something i like to do to

354
00:11:39,839 --> 00:11:44,959
solve it uh in an extra release

355
00:11:42,720 --> 00:11:47,360
um we also do memory profiling so for

356
00:11:44,959 --> 00:11:49,839
memory profiling it's way easier because

357
00:11:47,360 --> 00:11:51,440
uh i would say most of the work has been

358
00:11:49,839 --> 00:11:54,399
done already

359
00:11:51,440 --> 00:11:55,440
in python so there's a python module

360
00:11:54,399 --> 00:11:57,920
since

361
00:11:55,440 --> 00:11:58,959
3.4 which is called trace malloc so what

362
00:11:57,920 --> 00:12:01,599
tracemat does

363
00:11:58,959 --> 00:12:02,239
is that it replaces the malloc function

364
00:12:01,600 --> 00:12:03,920
used by

365
00:12:02,240 --> 00:12:06,079
python to allocate the memory and all

366
00:12:03,920 --> 00:12:08,800
the objects that you use in python

367
00:12:06,079 --> 00:12:10,399
by its own wrapper which allocates the

368
00:12:08,800 --> 00:12:13,439
memory but also

369
00:12:10,399 --> 00:12:15,680
make a trace of the allocation and first

370
00:12:13,440 --> 00:12:18,800
metallic is able to export to you

371
00:12:15,680 --> 00:12:21,920
the information about which line of code

372
00:12:18,800 --> 00:12:23,359
uh allocated a memory and like how many

373
00:12:21,920 --> 00:12:26,479
megabytes and let times

374
00:12:23,360 --> 00:12:28,560
it has been called so it's pretty easy

375
00:12:26,480 --> 00:12:30,240
to get this kind of information without

376
00:12:28,560 --> 00:12:32,399
doing too much work

377
00:12:30,240 --> 00:12:35,040
um the problem with trace malloc is that

378
00:12:32,399 --> 00:12:36,560
it's it wraps and it counts every object

379
00:12:35,040 --> 00:12:38,719
location that you do which makes your

380
00:12:36,560 --> 00:12:39,920
program very slow again

381
00:12:38,720 --> 00:12:42,000
so that's not something you can do on

382
00:12:39,920 --> 00:12:43,360
production system if you have to to

383
00:12:42,000 --> 00:12:45,680
if you have a program but there's a lot

384
00:12:43,360 --> 00:12:47,120
of location and gets four times or ten

385
00:12:45,680 --> 00:12:48,719
times slower just because you are

386
00:12:47,120 --> 00:12:49,279
tracing in the allocation it's a problem

387
00:12:48,720 --> 00:12:52,399
so

388
00:12:49,279 --> 00:12:54,399
the trick here is to also do sampling

389
00:12:52,399 --> 00:12:55,600
and doing statistical aggregation of the

390
00:12:54,399 --> 00:12:57,200
results so

391
00:12:55,600 --> 00:12:58,959
the algorithm is pretty simple like you

392
00:12:57,200 --> 00:13:01,040
sleep most of the time

393
00:12:58,959 --> 00:13:02,880
and once in a while you just wake up

394
00:13:01,040 --> 00:13:04,480
activate or deactivate trace malloc to

395
00:13:02,880 --> 00:13:05,519
get the samples or not of memory

396
00:13:04,480 --> 00:13:07,279
location

397
00:13:05,519 --> 00:13:09,360
if you do that like five percent of the

398
00:13:07,279 --> 00:13:11,680
time it's usually okay you only get five

399
00:13:09,360 --> 00:13:14,720
percent of the allocation being tracked

400
00:13:11,680 --> 00:13:16,719
by um trace malloc but over time if you

401
00:13:14,720 --> 00:13:18,240
do out for a few minutes a few hours

402
00:13:16,720 --> 00:13:21,839
you get a pretty good presentation of

403
00:13:18,240 --> 00:13:23,120
your patterns in your application

404
00:13:21,839 --> 00:13:24,880
so it gives you two results as i was

405
00:13:23,120 --> 00:13:26,639
saying the number of allocations that

406
00:13:24,880 --> 00:13:29,839
each of your function does

407
00:13:26,639 --> 00:13:31,040
and the size so the two charts looks a

408
00:13:29,839 --> 00:13:33,440
bit of the same

409
00:13:31,040 --> 00:13:35,199
but i mean the width is going to change

410
00:13:33,440 --> 00:13:37,680
based on if the allocation is going

411
00:13:35,200 --> 00:13:38,639
to allocate bigger objects or not but

412
00:13:37,680 --> 00:13:40,000
this is the kind of process that you

413
00:13:38,639 --> 00:13:42,000
might get at the end with uh

414
00:13:40,000 --> 00:13:43,199
stack traces and the number of megabytes

415
00:13:42,000 --> 00:13:45,519
that you're going to have

416
00:13:43,199 --> 00:13:47,599
are located so pretty easy again to

417
00:13:45,519 --> 00:13:48,880
debug your program to see

418
00:13:47,600 --> 00:13:51,199
what's costly in terms of memory so

419
00:13:48,880 --> 00:13:51,920
that's only like um the allocation that

420
00:13:51,199 --> 00:13:54,639
you do

421
00:13:51,920 --> 00:13:56,079
not the size of the memory you use but

422
00:13:54,639 --> 00:13:56,639
only the number of times you allocate

423
00:13:56,079 --> 00:13:58,638
memory

424
00:13:56,639 --> 00:14:02,079
it doesn't track like when you create or

425
00:13:58,639 --> 00:14:04,800
if you don't use the object anymore

426
00:14:02,079 --> 00:14:05,760
um so right now the limitation of trace

427
00:14:04,800 --> 00:14:07,279
melodic like one thing is that the

428
00:14:05,760 --> 00:14:10,399
overhead it's really slow

429
00:14:07,279 --> 00:14:10,959
really really slow uh i'm not sure right

430
00:14:10,399 --> 00:14:13,199
now

431
00:14:10,959 --> 00:14:15,119
how fast you you can i mean if you can

432
00:14:13,199 --> 00:14:16,880
improve it to make it faster

433
00:14:15,120 --> 00:14:19,199
there's no frame information so the way

434
00:14:16,880 --> 00:14:20,560
it tracks things it's pretty simple you

435
00:14:19,199 --> 00:14:22,079
don't have any contact information about

436
00:14:20,560 --> 00:14:24,800
which way did their location

437
00:14:22,079 --> 00:14:26,560
so you can't map that to any threats and

438
00:14:24,800 --> 00:14:29,680
it's also limited in the

439
00:14:26,560 --> 00:14:31,599
information reports for the cpu

440
00:14:29,680 --> 00:14:33,040
and wall time before it's pretty easy to

441
00:14:31,600 --> 00:14:35,839
get the function name because it's

442
00:14:33,040 --> 00:14:37,279
reported in the stack uh in that case

443
00:14:35,839 --> 00:14:39,360
trace metallic just gives you

444
00:14:37,279 --> 00:14:41,199
the line number and the file so you

445
00:14:39,360 --> 00:14:44,160
don't know which function or which class

446
00:14:41,199 --> 00:14:45,599
you are actually trying to allocate even

447
00:14:44,160 --> 00:14:48,480
which type of

448
00:14:45,600 --> 00:14:49,920
thing you are locating it is pretty um

449
00:14:48,480 --> 00:14:50,880
light in that regard so that's something

450
00:14:49,920 --> 00:14:54,399
i'd like i'd love to

451
00:14:50,880 --> 00:14:56,839
improve in the future another thing

452
00:14:54,399 --> 00:14:58,480
where we do a bit of profiling is

453
00:14:56,839 --> 00:14:59,920
threading

454
00:14:58,480 --> 00:15:01,279
so usually when you do multi-threading

455
00:14:59,920 --> 00:15:02,959
in python so a lot of people don't do

456
00:15:01,279 --> 00:15:03,600
multi-threading because of the global

457
00:15:02,959 --> 00:15:06,719
lock and

458
00:15:03,600 --> 00:15:07,680
how it makes things slow etc but you

459
00:15:06,720 --> 00:15:12,000
still need some time

460
00:15:07,680 --> 00:15:14,160
to have threads running um

461
00:15:12,000 --> 00:15:15,920
you do um application with a lot of

462
00:15:14,160 --> 00:15:17,680
threads you need to synchronize over

463
00:15:15,920 --> 00:15:19,040
something and usually you use a lock

464
00:15:17,680 --> 00:15:21,040
which is like the most basic

465
00:15:19,040 --> 00:15:22,880
uh object you can get in the threading

466
00:15:21,040 --> 00:15:25,439
world in python

467
00:15:22,880 --> 00:15:28,079
and that lock when you have a lot of

468
00:15:25,440 --> 00:15:29,839
thread strain to acquire it

469
00:15:28,079 --> 00:15:31,758
uh you might have contention and it

470
00:15:29,839 --> 00:15:33,920
might make your program slow

471
00:15:31,759 --> 00:15:35,040
uh there are there's nothing in python

472
00:15:33,920 --> 00:15:36,719
to to

473
00:15:35,040 --> 00:15:38,800
basically to give you one kind of

474
00:15:36,720 --> 00:15:40,240
information about is my thread blocking

475
00:15:38,800 --> 00:15:40,719
because it's trying to acquire a lock or

476
00:15:40,240 --> 00:15:43,360
not

477
00:15:40,720 --> 00:15:44,560
uh is it waiting for too long is it

478
00:15:43,360 --> 00:15:46,320
watching forever

479
00:15:44,560 --> 00:15:48,079
wait what is it what is my contention

480
00:15:46,320 --> 00:15:48,639
issue here yeah it's pretty hard to know

481
00:15:48,079 --> 00:15:51,599
so

482
00:15:48,639 --> 00:15:52,160
the way we solve that is we basically

483
00:15:51,600 --> 00:15:54,240
wrap the

484
00:15:52,160 --> 00:15:55,199
threading.log instances so each time you

485
00:15:54,240 --> 00:15:56,959
create a new lock

486
00:15:55,199 --> 00:15:58,959
you actually get a lock which is not a

487
00:15:56,959 --> 00:16:00,160
real lock i mean it's a real lock but

488
00:15:58,959 --> 00:16:02,239
under the hood we

489
00:16:00,160 --> 00:16:03,680
intercept it to see when your plank is

490
00:16:02,240 --> 00:16:04,959
going to acquire it and when it's going

491
00:16:03,680 --> 00:16:07,120
to release it

492
00:16:04,959 --> 00:16:08,239
that makes it possible to actually get

493
00:16:07,120 --> 00:16:11,440
the information about

494
00:16:08,240 --> 00:16:13,360
how many how much time they spent um

495
00:16:11,440 --> 00:16:15,360
trying to acquire the lock or how much

496
00:16:13,360 --> 00:16:16,800
time they spent acquiring the lock like

497
00:16:15,360 --> 00:16:18,079
being the lord being held by one of the

498
00:16:16,800 --> 00:16:19,439
threats

499
00:16:18,079 --> 00:16:21,599
so you get a lot of information for

500
00:16:19,440 --> 00:16:22,639
example you get the total lock aquarium

501
00:16:21,600 --> 00:16:24,800
wait time

502
00:16:22,639 --> 00:16:26,959
so this is the number of milliseconds

503
00:16:24,800 --> 00:16:29,359
usually not that long

504
00:16:26,959 --> 00:16:31,359
but your program is trying to wait it's

505
00:16:29,360 --> 00:16:33,040
going to wait to acquire that lock

506
00:16:31,360 --> 00:16:35,519
so with this kind of information you're

507
00:16:33,040 --> 00:16:36,160
able to see that you may need more luck

508
00:16:35,519 --> 00:16:38,079
or to

509
00:16:36,160 --> 00:16:40,319
um just put your program different parts

510
00:16:38,079 --> 00:16:41,040
to not use only one lock with several

511
00:16:40,320 --> 00:16:43,199
locks etc

512
00:16:41,040 --> 00:16:44,959
you can get all of this information to

513
00:16:43,199 --> 00:16:45,439
be able to redesign your program which

514
00:16:44,959 --> 00:16:46,880
is

515
00:16:45,440 --> 00:16:49,199
not always easy and possible but

516
00:16:46,880 --> 00:16:50,800
sometimes it is and you get all of that

517
00:16:49,199 --> 00:16:53,839
with the different threads that try to

518
00:16:50,800 --> 00:16:53,839
acquire the different clocks

519
00:16:54,240 --> 00:16:59,680
um so once we build all that

520
00:16:57,600 --> 00:17:00,639
we had to export all the data to use

521
00:16:59,680 --> 00:17:04,480
them

522
00:17:00,639 --> 00:17:06,480
uh which is a its own challenge

523
00:17:04,480 --> 00:17:09,039
so like i was saying c profile has its

524
00:17:06,480 --> 00:17:12,400
own custom format not really interesting

525
00:17:09,039 --> 00:17:12,400
to not useful for us

526
00:17:12,880 --> 00:17:16,720
there is one format which is pretty used

527
00:17:14,959 --> 00:17:18,799
in the world which is a calgarian format

528
00:17:16,720 --> 00:17:20,720
so it comes from the background program

529
00:17:18,799 --> 00:17:24,000
which just profiling for c

530
00:17:20,720 --> 00:17:25,760
um and it has in on text-based uh format

531
00:17:24,000 --> 00:17:28,960
that you can use to export data

532
00:17:25,760 --> 00:17:31,840
the problem is that it's it's text

533
00:17:28,960 --> 00:17:33,440
basically so it's not really like it's

534
00:17:31,840 --> 00:17:37,360
easy to consume but it's not

535
00:17:33,440 --> 00:17:40,000
really efficient in any way and

536
00:17:37,360 --> 00:17:41,600
so that's another good solution for us

537
00:17:40,000 --> 00:17:42,320
like i was saying a lot of python

538
00:17:41,600 --> 00:17:45,360
profiler

539
00:17:42,320 --> 00:17:47,760
then directly export html so

540
00:17:45,360 --> 00:17:49,918
that never gave us any clue on how to

541
00:17:47,760 --> 00:17:52,240
export to something that you can use for

542
00:17:49,919 --> 00:17:53,120
any other program uh until we're from

543
00:17:52,240 --> 00:17:56,559
tproff

544
00:17:53,120 --> 00:17:57,678
so if you don't know pprof it's a lot of

545
00:17:56,559 --> 00:18:00,000
things

546
00:17:57,679 --> 00:18:02,000
the name of the of the profiler that is

547
00:18:00,000 --> 00:18:03,840
in the go programming language

548
00:18:02,000 --> 00:18:05,760
uh it's also the name of the tool they

549
00:18:03,840 --> 00:18:07,678
use to display the data and it has the

550
00:18:05,760 --> 00:18:09,440
name of the format so it's like the same

551
00:18:07,679 --> 00:18:11,919
word for three different things

552
00:18:09,440 --> 00:18:14,000
um so the p prop format is pretty cool

553
00:18:11,919 --> 00:18:15,760
because that's what you can use for if

554
00:18:14,000 --> 00:18:17,840
you do go and go profiling you can

555
00:18:15,760 --> 00:18:20,080
export all of your data in prof format

556
00:18:17,840 --> 00:18:22,000
so it's a format based on protobuf so it

557
00:18:20,080 --> 00:18:23,840
makes it pretty easy to

558
00:18:22,000 --> 00:18:25,760
understand there's a schema with all the

559
00:18:23,840 --> 00:18:27,600
fields documented etc

560
00:18:25,760 --> 00:18:30,240
it's easy to use and protobuf is easy to

561
00:18:27,600 --> 00:18:32,879
use in python too

562
00:18:30,240 --> 00:18:34,080
it's very fast to to serialize so it

563
00:18:32,880 --> 00:18:36,320
would be a shame to be

564
00:18:34,080 --> 00:18:38,080
a i mean to try to be fast gathering all

565
00:18:36,320 --> 00:18:38,639
the data about your python program and

566
00:18:38,080 --> 00:18:41,039
then

567
00:18:38,640 --> 00:18:42,720
being very slow about exporting it so so

568
00:18:41,039 --> 00:18:44,559
that's that's not a problem

569
00:18:42,720 --> 00:18:46,480
the way the people format works is not

570
00:18:44,559 --> 00:18:48,399
going to export everything that you

571
00:18:46,480 --> 00:18:49,120
gather like every sample with every

572
00:18:48,400 --> 00:18:52,400
timestamp

573
00:18:49,120 --> 00:18:53,918
etc it's going to aggregate all of that

574
00:18:52,400 --> 00:18:55,919
so you're going to compute the sum the

575
00:18:53,919 --> 00:18:57,280
average the minimum whatever you need

576
00:18:55,919 --> 00:18:58,240
and export all of that with the stack

577
00:18:57,280 --> 00:19:00,480
traces in your

578
00:18:58,240 --> 00:19:01,919
format at the end and it's pretty space

579
00:19:00,480 --> 00:19:03,840
efficient so there's a pretty simple

580
00:19:01,919 --> 00:19:05,120
model of a string pool where

581
00:19:03,840 --> 00:19:06,639
all your string because you have a lot

582
00:19:05,120 --> 00:19:07,840
of repetition usually like all the

583
00:19:06,640 --> 00:19:10,000
functions you're going to see

584
00:19:07,840 --> 00:19:12,159
are always the same the spectres are

585
00:19:10,000 --> 00:19:14,480
going to be most of the time the same so

586
00:19:12,160 --> 00:19:15,919
all of that is pretty well uh i'd say

587
00:19:14,480 --> 00:19:19,360
organized in the schema

588
00:19:15,919 --> 00:19:21,840
and it's just zipped by default so

589
00:19:19,360 --> 00:19:23,120
pretty fast to generate pretty slow i

590
00:19:21,840 --> 00:19:24,480
probably software they're pretty small

591
00:19:23,120 --> 00:19:28,159
to store

592
00:19:24,480 --> 00:19:31,200
for example on average we use

593
00:19:28,160 --> 00:19:32,960
20 kilobytes of storage to store

594
00:19:31,200 --> 00:19:35,200
one minute of profiling for one python

595
00:19:32,960 --> 00:19:39,440
process so pretty easy to store

596
00:19:35,200 --> 00:19:42,000
even for a few hours or a few days

597
00:19:39,440 --> 00:19:43,919
so prof is also the name of the tool

598
00:19:42,000 --> 00:19:46,240
that you can install pretty easily uh

599
00:19:43,919 --> 00:19:47,440
if you know a bit of a go but it's not

600
00:19:46,240 --> 00:19:49,919
that hard so

601
00:19:47,440 --> 00:19:51,120
it's able to generate this kind of flame

602
00:19:49,919 --> 00:19:53,360
chart

603
00:19:51,120 --> 00:19:55,439
which is on top left where you see all

604
00:19:53,360 --> 00:19:57,120
the stack traces and the width is going

605
00:19:55,440 --> 00:19:58,559
to be either

606
00:19:57,120 --> 00:20:01,280
the size of the location that you do in

607
00:19:58,559 --> 00:20:02,399
memory the time you spend on cpu etc

608
00:20:01,280 --> 00:20:04,960
depending on the status that you

609
00:20:02,400 --> 00:20:07,760
gathered or you can also use

610
00:20:04,960 --> 00:20:09,600
this kind of uh chat on the r with each

611
00:20:07,760 --> 00:20:11,280
rectangle is going to be one function

612
00:20:09,600 --> 00:20:12,719
and everything is going to be connecting

613
00:20:11,280 --> 00:20:15,039
by the number of times they called

614
00:20:12,720 --> 00:20:16,720
each other etc so it's pretty hard to

615
00:20:15,039 --> 00:20:17,200
read when you're not used to it but if

616
00:20:16,720 --> 00:20:18,880
you

617
00:20:17,200 --> 00:20:21,440
i promise if you spend like a few

618
00:20:18,880 --> 00:20:21,919
minutes on it and prof is pretty easy to

619
00:20:21,440 --> 00:20:25,760
gather

620
00:20:21,919 --> 00:20:28,159
and to understand so it's easy to use

621
00:20:25,760 --> 00:20:32,000
it you can open an ep profile with it

622
00:20:28,159 --> 00:20:35,120
being go or python or anything

623
00:20:32,000 --> 00:20:36,480
so what we did uh a data log is uh as an

624
00:20:35,120 --> 00:20:38,479
open source library

625
00:20:36,480 --> 00:20:39,840
which is not yet public just because

626
00:20:38,480 --> 00:20:41,039
like for them is too early but i'm

627
00:20:39,840 --> 00:20:42,799
working on it

628
00:20:41,039 --> 00:20:44,640
uh it's going to be integrated into you

629
00:20:42,799 --> 00:20:47,679
know into our tracer

630
00:20:44,640 --> 00:20:49,600
uh library which is already open source

631
00:20:47,679 --> 00:20:51,440
so the profiling part is going to be

632
00:20:49,600 --> 00:20:52,959
integrated into that and if you want to

633
00:20:51,440 --> 00:20:54,799
use it like you're not using datalog

634
00:20:52,960 --> 00:20:55,679
it's fine because it also supports

635
00:20:54,799 --> 00:20:58,960
exporting the

636
00:20:55,679 --> 00:21:01,679
data to a prof file files

637
00:20:58,960 --> 00:21:02,400
even s so you'll be able to use it

638
00:21:01,679 --> 00:21:04,159
directly

639
00:21:02,400 --> 00:21:05,760
and to export all the design p prop and

640
00:21:04,159 --> 00:21:10,080
use the tool uh

641
00:21:05,760 --> 00:21:11,760
i show uh just uh before

642
00:21:10,080 --> 00:21:14,080
uh so as i said it's not released yet so

643
00:21:11,760 --> 00:21:16,158
if you want to know what literally is uh

644
00:21:14,080 --> 00:21:17,520
you can follow me on twitter or anything

645
00:21:16,159 --> 00:21:18,559
and i'll be able to announce it should

646
00:21:17,520 --> 00:21:31,840
be in a few weeks

647
00:21:18,559 --> 00:21:31,840
no promise but i hope so and that's it

648
00:21:38,720 --> 00:21:45,840
we have four means for questions

649
00:21:56,840 --> 00:21:59,840
okay

650
00:22:02,559 --> 00:22:04,639
you

