1
00:00:07,040 --> 00:00:11,920
hello everyone

2
00:00:08,639 --> 00:00:14,719
hi um

3
00:00:11,920 --> 00:00:15,280
i'm emma foley this is christoph kapka

4
00:00:14,719 --> 00:00:17,840
and

5
00:00:15,280 --> 00:00:19,359
we're going to talk to you today about

6
00:00:17,840 --> 00:00:20,080
how to tell what's really going on in

7
00:00:19,359 --> 00:00:23,359
unife

8
00:00:20,080 --> 00:00:24,479
infrastructure why you need to do it and

9
00:00:23,359 --> 00:00:28,560
what you can do

10
00:00:24,480 --> 00:00:32,480
um what you can do about it if you

11
00:00:28,560 --> 00:00:32,479
can't see what's going on uh

12
00:00:33,120 --> 00:00:38,800
legal requirements have been met um

13
00:00:36,960 --> 00:00:40,239
first i'm going to do an introduction

14
00:00:38,800 --> 00:00:43,440
and talk briefly about

15
00:00:40,239 --> 00:00:45,440
barometer and barometers and kristoff is

16
00:00:43,440 --> 00:00:47,280
going to talk about collecti

17
00:00:45,440 --> 00:00:48,800
and i will talk about barometer again

18
00:00:47,280 --> 00:00:49,760
and how the two projects relate to each

19
00:00:48,800 --> 00:00:51,519
other

20
00:00:49,760 --> 00:00:52,960
and then christophe will talk about

21
00:00:51,520 --> 00:00:55,760
potential use cases

22
00:00:52,960 --> 00:00:56,399
and i will switch over to plans upcoming

23
00:00:55,760 --> 00:00:59,680
features

24
00:00:56,399 --> 00:01:01,920
and open the floor to questions

25
00:00:59,680 --> 00:01:03,440
so uh why do i need to know what's going

26
00:01:01,920 --> 00:01:06,960
on in my infrastructure

27
00:01:03,440 --> 00:01:10,479
well as telcos and enterprises move

28
00:01:06,960 --> 00:01:14,158
towards a cloud-based it infrastructure

29
00:01:10,479 --> 00:01:16,640
they start moving their workloads to

30
00:01:14,159 --> 00:01:17,280
from fixed function network appliances

31
00:01:16,640 --> 00:01:19,200
to

32
00:01:17,280 --> 00:01:20,400
commodity hardware in order to reduce

33
00:01:19,200 --> 00:01:24,640
costs but

34
00:01:20,400 --> 00:01:26,320
as they move to general purpose hardware

35
00:01:24,640 --> 00:01:27,920
they become more and more reliant on the

36
00:01:26,320 --> 00:01:28,559
data center and they become more

37
00:01:27,920 --> 00:01:30,880
vulnerable

38
00:01:28,560 --> 00:01:32,159
to the costs associated with data center

39
00:01:30,880 --> 00:01:34,079
downtime

40
00:01:32,159 --> 00:01:36,000
those costs are not just financial

41
00:01:34,079 --> 00:01:39,199
although even a minute of

42
00:01:36,000 --> 00:01:42,399
data center downtime is very very costly

43
00:01:39,200 --> 00:01:44,799
the cost also comes

44
00:01:42,399 --> 00:01:45,920
in terms of additional complexity

45
00:01:44,799 --> 00:01:49,840
required

46
00:01:45,920 --> 00:01:49,840
and service availability

47
00:01:50,479 --> 00:01:53,600
so as they move from fixed function

48
00:01:52,320 --> 00:01:56,880
network appliances

49
00:01:53,600 --> 00:02:00,399
to the an nfv environment the

50
00:01:56,880 --> 00:02:02,240
tooling required to actually maintain

51
00:02:00,399 --> 00:02:03,520
hosts and orchestrate this becomes more

52
00:02:02,240 --> 00:02:05,920
complex

53
00:02:03,520 --> 00:02:06,960
at the same time the requirements the

54
00:02:05,920 --> 00:02:10,318
customers have

55
00:02:06,960 --> 00:02:14,079
for maintaining service insurance qos

56
00:02:10,318 --> 00:02:16,640
and same levels of availability

57
00:02:14,080 --> 00:02:17,200
they remain constant they need to be

58
00:02:16,640 --> 00:02:20,640
meshed

59
00:02:17,200 --> 00:02:20,640
or exceeded

60
00:02:21,360 --> 00:02:27,200
this requires more more complex tooling

61
00:02:25,360 --> 00:02:28,720
and more metrics to be available in the

62
00:02:27,200 --> 00:02:31,200
environment

63
00:02:28,720 --> 00:02:33,120
so that's additional complexity for

64
00:02:31,200 --> 00:02:36,238
deploying

65
00:02:33,120 --> 00:02:37,519
additional hardship when actually

66
00:02:36,239 --> 00:02:40,160
maintaining the

67
00:02:37,519 --> 00:02:42,080
level of performance required and then

68
00:02:40,160 --> 00:02:45,519
even more additional complexity

69
00:02:42,080 --> 00:02:48,400
in monitoring what you have going on now

70
00:02:45,519 --> 00:02:48,400
because it is

71
00:02:49,519 --> 00:02:52,640
uh it is vital to monitor the systems

72
00:02:51,840 --> 00:02:55,519
because

73
00:02:52,640 --> 00:02:57,040
um there are many different things that

74
00:02:55,519 --> 00:02:57,760
can have effect have an effect on

75
00:02:57,040 --> 00:02:59,840
performance

76
00:02:57,760 --> 00:03:01,920
and many different things as you move up

77
00:02:59,840 --> 00:03:04,800
in complexity that can actually cause

78
00:03:01,920 --> 00:03:07,200
downtime and you move from not only

79
00:03:04,800 --> 00:03:09,280
having to monitor the platform itself

80
00:03:07,200 --> 00:03:10,958
but also having to monitor the

81
00:03:09,280 --> 00:03:11,519
applications running on top of it

82
00:03:10,959 --> 00:03:13,360
because

83
00:03:11,519 --> 00:03:14,879
you don't want something like ovs or

84
00:03:13,360 --> 00:03:17,440
dpdk or

85
00:03:14,879 --> 00:03:18,159
openstack kubernetes to go down because

86
00:03:17,440 --> 00:03:20,879
that would be

87
00:03:18,159 --> 00:03:20,879
disastrous

88
00:03:21,360 --> 00:03:25,120
this is where barometer comes in and

89
00:03:24,319 --> 00:03:27,119
first off

90
00:03:25,120 --> 00:03:28,400
a barometer as in the scientific

91
00:03:27,120 --> 00:03:30,400
instrument

92
00:03:28,400 --> 00:03:31,840
is a device for measuring atmospheric

93
00:03:30,400 --> 00:03:33,840
pressure

94
00:03:31,840 --> 00:03:35,599
it is usually used for short-term

95
00:03:33,840 --> 00:03:40,560
weather forecasts

96
00:03:35,599 --> 00:03:42,798
and another use that

97
00:03:40,560 --> 00:03:44,239
that many people aren't aware of is that

98
00:03:42,799 --> 00:03:46,799
it can be used to measure

99
00:03:44,239 --> 00:03:48,239
altitude or height above sea level now

100
00:03:46,799 --> 00:03:50,560
when

101
00:03:48,239 --> 00:03:52,239
when scientists were designing a

102
00:03:50,560 --> 00:03:53,599
barometer they probably didn't expect

103
00:03:52,239 --> 00:03:56,319
this

104
00:03:53,599 --> 00:03:56,720
to actually be a use of it and the same

105
00:03:56,319 --> 00:03:58,958
way

106
00:03:56,720 --> 00:04:00,560
when the barometer project was created

107
00:03:58,959 --> 00:04:02,080
there were a lot of use cases that have

108
00:04:00,560 --> 00:04:04,319
since emerged that we did not foresee at

109
00:04:02,080 --> 00:04:07,840
the time

110
00:04:04,319 --> 00:04:09,760
so barometer itself is paradivo pnfe

111
00:04:07,840 --> 00:04:12,319
and i'll explain briefly what that is

112
00:04:09,760 --> 00:04:15,679
because it

113
00:04:12,319 --> 00:04:18,880
barometer's relationship to opnfv

114
00:04:15,680 --> 00:04:19,918
dictates the activities that the project

115
00:04:18,880 --> 00:04:23,759
actually

116
00:04:19,918 --> 00:04:25,440
undertakes so opnfe

117
00:04:23,759 --> 00:04:27,520
is the open platform for network

118
00:04:25,440 --> 00:04:28,000
function virtualization it's a linux

119
00:04:27,520 --> 00:04:31,680
foundation

120
00:04:28,000 --> 00:04:36,000
networking project and it tries to

121
00:04:31,680 --> 00:04:38,720
ease the adoption of nfe it does this by

122
00:04:36,000 --> 00:04:39,520
developing more nfv friendly features in

123
00:04:38,720 --> 00:04:42,880
upstream

124
00:04:39,520 --> 00:04:45,440
projects and then providing tooling

125
00:04:42,880 --> 00:04:47,840
to deploy test and integrate these same

126
00:04:45,440 --> 00:04:47,840
features

127
00:04:48,560 --> 00:04:51,680
so that is what barometer does barometer

128
00:04:51,280 --> 00:04:54,000
is

129
00:04:51,680 --> 00:04:56,400
concerned with collecting metrics to

130
00:04:54,000 --> 00:04:59,120
help you monitor the nfv infrastructure

131
00:04:56,400 --> 00:05:00,000
and exposing these metrics to higher

132
00:04:59,120 --> 00:05:02,080
level

133
00:05:00,000 --> 00:05:03,759
fault management systems that can

134
00:05:02,080 --> 00:05:07,280
actually

135
00:05:03,759 --> 00:05:08,160
introspect and analyze and automate the

136
00:05:07,280 --> 00:05:11,758
management

137
00:05:08,160 --> 00:05:13,360
and fault detection in your data center

138
00:05:11,759 --> 00:05:15,039
so like i said barometer does testing

139
00:05:13,360 --> 00:05:16,639
integration deployment and upstream

140
00:05:15,039 --> 00:05:19,840
development

141
00:05:16,639 --> 00:05:21,520
on a metrics collection

142
00:05:19,840 --> 00:05:23,119
and that's what christoph is going to

143
00:05:21,520 --> 00:05:24,960
talk to you about

144
00:05:23,120 --> 00:05:27,199
the upstream projects which barometer

145
00:05:24,960 --> 00:05:30,080
actually does contribute to

146
00:05:27,199 --> 00:05:33,280
and i will try very briefly to explain

147
00:05:30,080 --> 00:05:33,280
what that project is

148
00:05:34,639 --> 00:05:41,840
and kristoff will actually

149
00:05:38,000 --> 00:05:44,960
give you some more useful information

150
00:05:41,840 --> 00:05:49,520
okay yes collect

151
00:05:44,960 --> 00:06:07,840
is pretty major piece of software

152
00:05:49,520 --> 00:06:07,840
uh this kind of veteran in there

153
00:06:13,759 --> 00:06:19,440
okay perfect

154
00:06:16,880 --> 00:06:20,479
so yeah collectd is pretty major piece

155
00:06:19,440 --> 00:06:22,560
of software

156
00:06:20,479 --> 00:06:23,758
it is kind of veteran in the deployments

157
00:06:22,560 --> 00:06:26,880
across the industry

158
00:06:23,759 --> 00:06:30,479
very well deployed

159
00:06:26,880 --> 00:06:34,080
it is there for about 16 years

160
00:06:30,479 --> 00:06:36,400
during those years collectd was

161
00:06:34,080 --> 00:06:37,919
continuously evolving and adapting to

162
00:06:36,400 --> 00:06:40,638
industry needs

163
00:06:37,919 --> 00:06:43,680
um it is written in c especially core

164
00:06:40,639 --> 00:06:45,600
demon it doesn't have any dependencies

165
00:06:43,680 --> 00:06:47,759
and is built with small footprint in

166
00:06:45,600 --> 00:06:50,080
mind

167
00:06:47,759 --> 00:06:52,479
is open sourced mostly mit some older

168
00:06:50,080 --> 00:06:56,000
plugins are still gpl

169
00:06:52,479 --> 00:06:58,560
um as this doesn't have any dependencies

170
00:06:56,000 --> 00:07:00,080
it is platform independent around on

171
00:06:58,560 --> 00:07:03,120
most of the available

172
00:07:00,080 --> 00:07:06,880
operating systems that are there um

173
00:07:03,120 --> 00:07:10,080
just providing you ability to collect

174
00:07:06,880 --> 00:07:11,759
uh multiple metrics and events uh

175
00:07:10,080 --> 00:07:14,880
included in the correct d repository

176
00:07:11,759 --> 00:07:17,440
there is over 140 of them

177
00:07:14,880 --> 00:07:19,520
of various types some of them are

178
00:07:17,440 --> 00:07:21,520
reading the telemetry from various

179
00:07:19,520 --> 00:07:23,039
pieces either from applications or from

180
00:07:21,520 --> 00:07:26,639
the platform hardware

181
00:07:23,039 --> 00:07:30,400
or many other places it's also

182
00:07:26,639 --> 00:07:33,599
able to write this telemetry to multiple

183
00:07:30,400 --> 00:07:36,000
ways to norbond either to the file

184
00:07:33,599 --> 00:07:38,319
of the csv or to some time series

185
00:07:36,000 --> 00:07:40,960
databases like in fluxdb

186
00:07:38,319 --> 00:07:41,919
or any other and there are also binding

187
00:07:40,960 --> 00:07:44,318
plugins if

188
00:07:41,919 --> 00:07:46,159
in case if the plugins that are out

189
00:07:44,319 --> 00:07:48,560
there are not enough for you

190
00:07:46,160 --> 00:07:49,680
you can write some python scripts or

191
00:07:48,560 --> 00:07:52,000
java applications

192
00:07:49,680 --> 00:07:54,080
and feed them into the collect decor

193
00:07:52,000 --> 00:07:56,800
daemon to dispatch those applications

194
00:07:54,080 --> 00:07:58,960
for further integration with your

195
00:07:56,800 --> 00:08:02,000
analytics stack

196
00:07:58,960 --> 00:08:04,560
there's also modules for

197
00:08:02,000 --> 00:08:06,319
logging handling notifications

198
00:08:04,560 --> 00:08:10,160
aggregation thresholding

199
00:08:06,319 --> 00:08:12,400
filtering metrics interesting

200
00:08:10,160 --> 00:08:13,280
plugin is the network because it is able

201
00:08:12,400 --> 00:08:15,758
to

202
00:08:13,280 --> 00:08:19,599
read and write the data over the network

203
00:08:15,759 --> 00:08:22,960
with the collectd specific protocol

204
00:08:19,599 --> 00:08:25,039
so collectd can be

205
00:08:22,960 --> 00:08:26,479
treated as a client that is producing

206
00:08:25,039 --> 00:08:28,639
the data but also

207
00:08:26,479 --> 00:08:29,758
as a server that is receiving them do

208
00:08:28,639 --> 00:08:32,560
something with them for example

209
00:08:29,759 --> 00:08:36,479
aggregating and forward them

210
00:08:32,559 --> 00:08:38,799
something far somewhere further so

211
00:08:36,479 --> 00:08:40,000
we know that collectd provides us

212
00:08:38,799 --> 00:08:43,359
ability to collect

213
00:08:40,000 --> 00:08:45,760
the metrics but which are

214
00:08:43,360 --> 00:08:46,959
kind of interested for you why would you

215
00:08:45,760 --> 00:08:50,480
like to choose the collect

216
00:08:46,959 --> 00:08:54,160
actually there are existing standard

217
00:08:50,480 --> 00:08:57,360
organization bodies like edc or c entity

218
00:08:54,160 --> 00:09:00,480
that are working on the documenting

219
00:08:57,360 --> 00:09:01,760
uh specifications that are listing out

220
00:09:00,480 --> 00:09:04,320
the set of

221
00:09:01,760 --> 00:09:05,920
metrics that you are and capabilities

222
00:09:04,320 --> 00:09:09,200
that you are particularly interested in

223
00:09:05,920 --> 00:09:12,399
the nav architecture

224
00:09:09,200 --> 00:09:15,680
today we will focus mostly on the nfvi

225
00:09:12,399 --> 00:09:17,200
so the platform telemetry and part of

226
00:09:15,680 --> 00:09:19,359
the traffic telemetry

227
00:09:17,200 --> 00:09:20,640
but there are also possibilities about

228
00:09:19,360 --> 00:09:22,560
to scrape the

229
00:09:20,640 --> 00:09:24,160
application telemetry directly from the

230
00:09:22,560 --> 00:09:25,839
vnfs

231
00:09:24,160 --> 00:09:27,519
with some of the plugins like for

232
00:09:25,839 --> 00:09:30,560
example dpdk telemetry

233
00:09:27,519 --> 00:09:31,519
to push all this data to some telemetry

234
00:09:30,560 --> 00:09:34,719
databases for

235
00:09:31,519 --> 00:09:38,000
analytics engine and closing the

236
00:09:34,720 --> 00:09:40,080
loop with the providing feedback to

237
00:09:38,000 --> 00:09:43,120
back to the manual systems to make

238
00:09:40,080 --> 00:09:46,080
decisions about corrective actions

239
00:09:43,120 --> 00:09:49,120
so what's more available in the collect

240
00:09:46,080 --> 00:09:51,279
d to monitor the navi

241
00:09:49,120 --> 00:09:52,560
there are plugins like mce lock pci

242
00:09:51,279 --> 00:09:55,040
errors or log parser

243
00:09:52,560 --> 00:09:56,560
that i that are able to provide you

244
00:09:55,040 --> 00:09:58,160
specific counters about

245
00:09:56,560 --> 00:09:59,760
for example memory errors that are

246
00:09:58,160 --> 00:10:02,079
happening uh

247
00:09:59,760 --> 00:10:04,800
on your dreams uh through the

248
00:10:02,079 --> 00:10:08,079
intellectual technology or rus features

249
00:10:04,800 --> 00:10:09,760
which are basically features built for

250
00:10:08,079 --> 00:10:11,359
reliability availability and service

251
00:10:09,760 --> 00:10:14,640
ability which are

252
00:10:11,360 --> 00:10:16,720
helping your platform to

253
00:10:14,640 --> 00:10:18,640
serve you longer even if there are any

254
00:10:16,720 --> 00:10:20,720
failures occurring

255
00:10:18,640 --> 00:10:22,720
the intel research director technology

256
00:10:20,720 --> 00:10:25,920
allows you to monitor

257
00:10:22,720 --> 00:10:29,279
per process id or per core

258
00:10:25,920 --> 00:10:29,920
your cash utilization of the last level

259
00:10:29,279 --> 00:10:33,920
cache

260
00:10:29,920 --> 00:10:35,040
or memory bandwidth vert plugin provides

261
00:10:33,920 --> 00:10:37,279
you the

262
00:10:35,040 --> 00:10:38,319
insights into the live view domains so

263
00:10:37,279 --> 00:10:42,240
they compute

264
00:10:38,320 --> 00:10:45,360
storage or networking inside the vms

265
00:10:42,240 --> 00:10:48,079
there are integration for obs and dptk

266
00:10:45,360 --> 00:10:49,920
that allows you to see what is happening

267
00:10:48,079 --> 00:10:51,680
on your network with

268
00:10:49,920 --> 00:10:53,519
some packet processing counters

269
00:10:51,680 --> 00:10:56,640
including errors

270
00:10:53,519 --> 00:10:57,760
and and drop rates that are occurring

271
00:10:56,640 --> 00:10:59,519
there

272
00:10:57,760 --> 00:11:01,439
there are python based plugins that

273
00:10:59,519 --> 00:11:03,760
allows you to write this telemetry to

274
00:11:01,440 --> 00:11:06,160
the openstack for consumption

275
00:11:03,760 --> 00:11:07,920
you can also push this data to the kafka

276
00:11:06,160 --> 00:11:10,959
to amqp

277
00:11:07,920 --> 00:11:13,120
on to prometus or

278
00:11:10,959 --> 00:11:14,399
for example to the vnf event stream

279
00:11:13,120 --> 00:11:18,000
which is a project

280
00:11:14,399 --> 00:11:20,320
in the on up you can

281
00:11:18,000 --> 00:11:23,200
monitor the health of the storage or

282
00:11:20,320 --> 00:11:23,200
power consumption

283
00:11:23,279 --> 00:11:27,519
something closer to the platform in case

284
00:11:25,600 --> 00:11:31,200
you are for example selling

285
00:11:27,519 --> 00:11:32,720
the resources of the cloud to someone

286
00:11:31,200 --> 00:11:34,240
you may be interested in the out-of-band

287
00:11:32,720 --> 00:11:37,360
telemetry

288
00:11:34,240 --> 00:11:40,640
which is provided via a redfish or

289
00:11:37,360 --> 00:11:43,760
a apmi and there are also

290
00:11:40,640 --> 00:11:45,839
pmu counters that may be interested for

291
00:11:43,760 --> 00:11:48,959
you

292
00:11:45,839 --> 00:11:51,680
which are monitoring the

293
00:11:48,959 --> 00:11:53,760
low level counter center processor which

294
00:11:51,680 --> 00:11:57,680
may be useful in some cases

295
00:11:53,760 --> 00:11:59,600
like branch misses mispredictions or

296
00:11:57,680 --> 00:12:04,479
cache misses

297
00:11:59,600 --> 00:12:04,480
so now let's get back for a moment to

298
00:12:08,839 --> 00:12:12,720
barometer

299
00:12:10,160 --> 00:12:14,160
um you may have noticed that i like

300
00:12:12,720 --> 00:12:16,480
asking questions

301
00:12:14,160 --> 00:12:17,760
um so how does barometer relate to

302
00:12:16,480 --> 00:12:21,040
collect d

303
00:12:17,760 --> 00:12:22,639
um well uh collect e helps us to collect

304
00:12:21,040 --> 00:12:24,959
metrics and that's the core of what we

305
00:12:22,639 --> 00:12:26,240
want to do because

306
00:12:24,959 --> 00:12:28,160
no matter what you're going to do with

307
00:12:26,240 --> 00:12:30,240
those metrics no matter how you want to

308
00:12:28,160 --> 00:12:31,600
manage your nfv environment you still

309
00:12:30,240 --> 00:12:34,639
need those metrics to be

310
00:12:31,600 --> 00:12:36,800
available and easy to access

311
00:12:34,639 --> 00:12:39,279
in whatever format you want in whatever

312
00:12:36,800 --> 00:12:43,680
higher level uh management drop

313
00:12:39,279 --> 00:12:45,839
or automation that you use

314
00:12:43,680 --> 00:12:46,800
so collecti helps us collect the metrics

315
00:12:45,839 --> 00:12:48,560
and

316
00:12:46,800 --> 00:12:50,000
if it wasn't if this project didn't

317
00:12:48,560 --> 00:12:51,199
exist basically we'd have a lot more

318
00:12:50,000 --> 00:12:54,160
work to do in

319
00:12:51,200 --> 00:12:56,480
barometer so it's only fair that we try

320
00:12:54,160 --> 00:12:58,880
to give back to the collective community

321
00:12:56,480 --> 00:13:00,399
and we do this not only by upstreaming

322
00:12:58,880 --> 00:13:03,680
our own features

323
00:13:00,399 --> 00:13:04,399
but also by helping the community in

324
00:13:03,680 --> 00:13:08,638
general

325
00:13:04,399 --> 00:13:12,079
in general onboard new contributors

326
00:13:08,639 --> 00:13:15,440
review pull requests and

327
00:13:12,079 --> 00:13:18,160
also barometer itself provides a load of

328
00:13:15,440 --> 00:13:19,440
testing and deployment tooling which

329
00:13:18,160 --> 00:13:23,920
feed back into the

330
00:13:19,440 --> 00:13:27,600
upstream collecti ci and provide

331
00:13:23,920 --> 00:13:29,040
validation information to developers on

332
00:13:27,600 --> 00:13:32,480
their pull requests

333
00:13:29,040 --> 00:13:34,719
and also assists

334
00:13:32,480 --> 00:13:36,000
release time to actually validate the

335
00:13:34,720 --> 00:13:37,920
collect e

336
00:13:36,000 --> 00:13:39,760
releases and make sure everything is

337
00:13:37,920 --> 00:13:42,319
working

338
00:13:39,760 --> 00:13:43,680
uh so uh if i want to play around with

339
00:13:42,320 --> 00:13:46,240
barometer or collect d

340
00:13:43,680 --> 00:13:47,599
and take advantage of all these new nfe

341
00:13:46,240 --> 00:13:50,720
features there

342
00:13:47,600 --> 00:13:52,800
what can we do barometer

343
00:13:50,720 --> 00:13:54,240
takes care of some deployment tooling as

344
00:13:52,800 --> 00:13:57,760
well that makes it easier

345
00:13:54,240 --> 00:13:59,839
to install and

346
00:13:57,760 --> 00:14:01,439
integrate collecti into whatever system

347
00:13:59,839 --> 00:14:03,760
you have

348
00:14:01,440 --> 00:14:04,720
you could also install collectee from a

349
00:14:03,760 --> 00:14:08,880
package manager

350
00:14:04,720 --> 00:14:10,959
and and configure it yourself

351
00:14:08,880 --> 00:14:13,920
but this gets a little bit tedious after

352
00:14:10,959 --> 00:14:15,839
one or two servers

353
00:14:13,920 --> 00:14:17,120
so what we've done in barometer is we've

354
00:14:15,839 --> 00:14:19,600
containerized

355
00:14:17,120 --> 00:14:21,040
um we've containerized collect d and

356
00:14:19,600 --> 00:14:22,320
we've written a bunch of ansible

357
00:14:21,040 --> 00:14:25,599
playbooks to

358
00:14:22,320 --> 00:14:26,880
automatically configure uh

359
00:14:25,600 --> 00:14:29,199
all the plugins that we think are

360
00:14:26,880 --> 00:14:31,040
relevant but you can also put in your

361
00:14:29,199 --> 00:14:33,279
own

362
00:14:31,040 --> 00:14:35,120
so this one click installer will let you

363
00:14:33,279 --> 00:14:37,760
install collect d as is

364
00:14:35,120 --> 00:14:38,800
or install alongside influx cb and

365
00:14:37,760 --> 00:14:42,480
grafana

366
00:14:38,800 --> 00:14:44,319
or alongside prometheus and

367
00:14:42,480 --> 00:14:46,079
this is a few examples of how the

368
00:14:44,320 --> 00:14:48,000
metrics could actually be consumed and i

369
00:14:46,079 --> 00:14:48,880
will talk about some of the pros and

370
00:14:48,000 --> 00:14:53,199
cons of

371
00:14:48,880 --> 00:14:56,720
these uh reference deployments

372
00:14:53,199 --> 00:15:00,160
so first up is influx cp in grafana

373
00:14:56,720 --> 00:15:00,160
this is a

374
00:15:00,480 --> 00:15:07,040
very simple architecture uh you

375
00:15:04,000 --> 00:15:10,000
dispatch the metrics from collective

376
00:15:07,040 --> 00:15:10,719
from collectd via its network plugin and

377
00:15:10,000 --> 00:15:12,720
these are

378
00:15:10,720 --> 00:15:14,000
sent to the time series database in

379
00:15:12,720 --> 00:15:16,000
fluxdb

380
00:15:14,000 --> 00:15:17,440
from here you can grab the metrics for

381
00:15:16,000 --> 00:15:19,360
whatever offline analysis you want to do

382
00:15:17,440 --> 00:15:19,760
hook it into any existing tooling you

383
00:15:19,360 --> 00:15:22,560
have

384
00:15:19,760 --> 00:15:23,920
that talks to influx or create your own

385
00:15:22,560 --> 00:15:25,680
tooling around that and pull those

386
00:15:23,920 --> 00:15:28,319
metrics

387
00:15:25,680 --> 00:15:29,599
or very simply you can write out of the

388
00:15:28,320 --> 00:15:32,320
box

389
00:15:29,600 --> 00:15:33,120
get some nice graphs with grfana and if

390
00:15:32,320 --> 00:15:36,480
you're running

391
00:15:33,120 --> 00:15:39,279
grafana 4.0 or above you get some basic

392
00:15:36,480 --> 00:15:39,279
alerting as well

393
00:15:39,759 --> 00:15:44,079
so prometheus is very popular especially

394
00:15:42,639 --> 00:15:47,440
when you're talking about

395
00:15:44,079 --> 00:15:49,040
um kubernetes and cloud native

396
00:15:47,440 --> 00:15:51,920
infrastructures

397
00:15:49,040 --> 00:15:52,800
uh but there is a side problem when you

398
00:15:51,920 --> 00:15:54,959
try to deploy

399
00:15:52,800 --> 00:15:56,319
prometheus with collecti in that

400
00:15:54,959 --> 00:15:58,079
collecti is a push model

401
00:15:56,320 --> 00:15:59,759
for metrics and prometheus has a pull

402
00:15:58,079 --> 00:16:01,758
model so

403
00:15:59,759 --> 00:16:02,880
collecti as it doesn't have any in-built

404
00:16:01,759 --> 00:16:04,240
storage has to

405
00:16:02,880 --> 00:16:06,399
put those metrics somewhere until

406
00:16:04,240 --> 00:16:07,920
prometheus pulls them

407
00:16:06,399 --> 00:16:09,839
so there's two plugins that do this

408
00:16:07,920 --> 00:16:12,479
there is the right promises plugin

409
00:16:09,839 --> 00:16:13,920
and there is a collect exporter both of

410
00:16:12,480 --> 00:16:15,920
them work in the same way in that they

411
00:16:13,920 --> 00:16:19,199
create a smaller web server

412
00:16:15,920 --> 00:16:21,040
which hosts the metrics um until

413
00:16:19,199 --> 00:16:22,160
prometheus prometheus comes along and

414
00:16:21,040 --> 00:16:25,839
scrapes that

415
00:16:22,160 --> 00:16:26,560
remote end point as your infrastructure

416
00:16:25,839 --> 00:16:30,079
scales

417
00:16:26,560 --> 00:16:33,680
this becomes a little bit problematic

418
00:16:30,079 --> 00:16:36,638
because well prometheus is scraping a

419
00:16:33,680 --> 00:16:39,920
whole bunch of remote endpoints and that

420
00:16:36,639 --> 00:16:41,839
takes a non-zero amount of time

421
00:16:39,920 --> 00:16:43,360
eventually what happens is the time it

422
00:16:41,839 --> 00:16:44,320
takes for prometheus to scrape the

423
00:16:43,360 --> 00:16:47,360
metrics

424
00:16:44,320 --> 00:16:48,399
from all the hosts in that time

425
00:16:47,360 --> 00:16:50,959
collective will have created more

426
00:16:48,399 --> 00:16:52,240
metrics and those will overwrite the

427
00:16:50,959 --> 00:16:55,518
existing ones

428
00:16:52,240 --> 00:16:56,079
so you end up with larger infrastructure

429
00:16:55,519 --> 00:16:59,759
missing

430
00:16:56,079 --> 00:17:04,000
data and another issue with this

431
00:16:59,759 --> 00:17:07,599
is that the timestamp recorded by

432
00:17:04,000 --> 00:17:10,000
prometheus is the actual scrape time

433
00:17:07,599 --> 00:17:12,079
and this may not be the same as the

434
00:17:10,000 --> 00:17:14,000
collection time for the metric

435
00:17:12,079 --> 00:17:15,438
so if it's a small deployment you can

436
00:17:14,000 --> 00:17:16,480
probably get over that because it's a

437
00:17:15,439 --> 00:17:20,319
small variation

438
00:17:16,480 --> 00:17:21,839
but as you scale up

439
00:17:20,319 --> 00:17:23,438
as you scale up the differences become

440
00:17:21,839 --> 00:17:25,039
more and more profound

441
00:17:23,439 --> 00:17:27,280
and this actually limits the rate at

442
00:17:25,039 --> 00:17:30,799
which you can collect metrics so there's

443
00:17:27,280 --> 00:17:30,799
a lot of trade-offs that have to be made

444
00:17:31,039 --> 00:17:35,360
um i think that's

445
00:17:34,160 --> 00:17:37,919
i'm pretty sure it's something else to

446
00:17:35,360 --> 00:17:40,879
say about that um

447
00:17:37,919 --> 00:17:42,160
but i'll figure that out um so uh the

448
00:17:40,880 --> 00:17:45,360
issues here would be

449
00:17:42,160 --> 00:17:46,880
the the metric selection time is not

450
00:17:45,360 --> 00:17:50,399
being preserved

451
00:17:46,880 --> 00:17:53,840
and the the latency involved uh means

452
00:17:50,400 --> 00:17:53,840
you have to trade off as you scale up

453
00:17:54,400 --> 00:17:58,720
and i thought i remembered for a second

454
00:17:57,440 --> 00:18:01,120
what else was wrong oh

455
00:17:58,720 --> 00:18:02,640
yeah normally when this happens you

456
00:18:01,120 --> 00:18:03,918
would just deploy more instances of your

457
00:18:02,640 --> 00:18:06,880
application

458
00:18:03,919 --> 00:18:07,520
but prometheus explicitly operates in a

459
00:18:06,880 --> 00:18:09,840
single

460
00:18:07,520 --> 00:18:11,360
server mode so there's always only one

461
00:18:09,840 --> 00:18:15,439
prometheus instance

462
00:18:11,360 --> 00:18:17,760
if you want high availability you

463
00:18:15,440 --> 00:18:18,880
you deploy two or more prometheus

464
00:18:17,760 --> 00:18:22,080
instances

465
00:18:18,880 --> 00:18:22,559
but you can't share the data between

466
00:18:22,080 --> 00:18:24,159
them

467
00:18:22,559 --> 00:18:25,678
each prometheus instance will be

468
00:18:24,160 --> 00:18:26,480
scraping all the endpoints and that

469
00:18:25,679 --> 00:18:29,600
doesn't really

470
00:18:26,480 --> 00:18:30,960
solve the problem of latency so

471
00:18:29,600 --> 00:18:33,280
that's where the service telemetry

472
00:18:30,960 --> 00:18:35,280
framework comes in uh this is pretty new

473
00:18:33,280 --> 00:18:36,639
in that up until wednesday it was called

474
00:18:35,280 --> 00:18:39,840
saf

475
00:18:36,640 --> 00:18:41,520
but we had to change the name

476
00:18:39,840 --> 00:18:43,120
so in this case you still have collectee

477
00:18:41,520 --> 00:18:45,520
running

478
00:18:43,120 --> 00:18:46,479
you still have these same plugins

479
00:18:45,520 --> 00:18:50,160
configured

480
00:18:46,480 --> 00:18:51,520
but instead of exposing a a local scrape

481
00:18:50,160 --> 00:18:54,160
endpoint

482
00:18:51,520 --> 00:18:56,480
remote scrape endpoint for prometheus

483
00:18:54,160 --> 00:18:59,200
all the metrics are dispatched over amq

484
00:18:56,480 --> 00:19:01,600
and then receiving the other side in the

485
00:18:59,200 --> 00:19:03,360
stf application which is hosted at the

486
00:19:01,600 --> 00:19:06,879
moment on openshift

487
00:19:03,360 --> 00:19:08,240
um the metrics then are pulled off the

488
00:19:06,880 --> 00:19:11,919
amq bus

489
00:19:08,240 --> 00:19:14,080
by a application called smart gateway

490
00:19:11,919 --> 00:19:16,240
which exposes the metrics on a local

491
00:19:14,080 --> 00:19:19,678
scrape endpoint to prometheus

492
00:19:16,240 --> 00:19:21,840
and smartgateway also takes care of

493
00:19:19,679 --> 00:19:24,160
the issue with the right time versus the

494
00:19:21,840 --> 00:19:26,399
scrape time

495
00:19:24,160 --> 00:19:28,000
so then the metrics are available in

496
00:19:26,400 --> 00:19:28,799
prometheus the same way as they were

497
00:19:28,000 --> 00:19:32,559
before

498
00:19:28,799 --> 00:19:34,559
and this also takes into account events

499
00:19:32,559 --> 00:19:36,639
and those are available through

500
00:19:34,559 --> 00:19:39,440
elasticsearch

501
00:19:36,640 --> 00:19:40,400
this looks complicated uh it actually

502
00:19:39,440 --> 00:19:42,880
ends up not being

503
00:19:40,400 --> 00:19:44,799
very complicated because all the

504
00:19:42,880 --> 00:19:48,000
orchestration for that is taken care of

505
00:19:44,799 --> 00:19:49,200
uh by um the service assurance

506
00:19:48,000 --> 00:19:51,360
orchestrator

507
00:19:49,200 --> 00:19:53,840
which actually deploys all of these um

508
00:19:51,360 --> 00:19:53,840
for you

509
00:19:54,400 --> 00:20:01,039
so with that um

510
00:19:58,480 --> 00:20:03,360
we have the metrics available and you

511
00:20:01,039 --> 00:20:05,600
can make them available to whatever

512
00:20:03,360 --> 00:20:06,959
other system you want with maybe a

513
00:20:05,600 --> 00:20:08,000
little bit of effort maybe a lot of

514
00:20:06,960 --> 00:20:11,520
effort but there's a lot of reference

515
00:20:08,000 --> 00:20:14,640
implementations and a lot of choices

516
00:20:11,520 --> 00:20:17,840
i like to use the stf

517
00:20:14,640 --> 00:20:18,559
acronym here so you can use these

518
00:20:17,840 --> 00:20:20,959
metrics

519
00:20:18,559 --> 00:20:22,240
to stop your system from being stressed

520
00:20:20,960 --> 00:20:25,600
to failure

521
00:20:22,240 --> 00:20:27,840
or you can use them to see the future

522
00:20:25,600 --> 00:20:31,039
or kristoff can tell you some ways that

523
00:20:27,840 --> 00:20:31,039
we're actually using them

524
00:20:32,159 --> 00:20:35,440
okay so let's start to the first one

525
00:20:34,640 --> 00:20:38,320
that was

526
00:20:35,440 --> 00:20:39,760
actually being used pretty recently in

527
00:20:38,320 --> 00:20:42,879
november

528
00:20:39,760 --> 00:20:46,840
uh so during the kubecon in

529
00:20:42,880 --> 00:20:48,400
in san diego they have deployed to start

530
00:20:46,840 --> 00:20:51,360
again

531
00:20:48,400 --> 00:20:52,720
is it not working it should be working

532
00:20:51,360 --> 00:20:55,840
can you not hear

533
00:20:52,720 --> 00:20:59,039
you can hear me okay perfect

534
00:20:55,840 --> 00:21:02,799
so yeah they have deployed full

535
00:20:59,039 --> 00:21:06,080
open source 5g network they have made

536
00:21:02,799 --> 00:21:08,240
a call from one city to another

537
00:21:06,080 --> 00:21:09,678
and as part of this virtual center

538
00:21:08,240 --> 00:21:12,320
office there

539
00:21:09,679 --> 00:21:13,679
for the monitoring the actual barometer

540
00:21:12,320 --> 00:21:16,080
have been used

541
00:21:13,679 --> 00:21:16,960
so here we can see a grafana dashboard

542
00:21:16,080 --> 00:21:21,120
that shows

543
00:21:16,960 --> 00:21:24,880
us us some statistics from the system

544
00:21:21,120 --> 00:21:24,879
so pretty cool big stuff

545
00:21:24,960 --> 00:21:28,799
now let's get to something simpler this

546
00:21:27,280 --> 00:21:32,639
is actually just a

547
00:21:28,799 --> 00:21:35,918
proof of concept here

548
00:21:32,640 --> 00:21:37,120
on the one server we have two vbnj

549
00:21:35,919 --> 00:21:39,760
instances

550
00:21:37,120 --> 00:21:42,158
that are running in hot standby mode so

551
00:21:39,760 --> 00:21:45,039
one is actively processing

552
00:21:42,159 --> 00:21:47,360
the traffic and the other one is waiting

553
00:21:45,039 --> 00:21:49,600
in orange standby mode

554
00:21:47,360 --> 00:21:50,399
they are deployed on two separate numa

555
00:21:49,600 --> 00:21:53,678
nodes

556
00:21:50,400 --> 00:21:57,440
so they have different memory

557
00:21:53,679 --> 00:22:00,480
regions um and

558
00:21:57,440 --> 00:22:03,600
the resiliency part here is that if the

559
00:22:00,480 --> 00:22:06,320
one point on active

560
00:22:03,600 --> 00:22:07,280
vbng instance is memory is getting

561
00:22:06,320 --> 00:22:09,678
corrupted

562
00:22:07,280 --> 00:22:11,440
during that time it's just getting

563
00:22:09,679 --> 00:22:13,760
degradated

564
00:22:11,440 --> 00:22:15,440
the telemetry from there is scraped from

565
00:22:13,760 --> 00:22:18,480
the mmc lock plugin

566
00:22:15,440 --> 00:22:20,000
it is dispatched to the prometeus and if

567
00:22:18,480 --> 00:22:23,520
the increase of the

568
00:22:20,000 --> 00:22:26,240
corrected memory errors is

569
00:22:23,520 --> 00:22:26,879
increasing too fast because usually

570
00:22:26,240 --> 00:22:29,919
memory

571
00:22:26,880 --> 00:22:32,799
is starting to send some

572
00:22:29,919 --> 00:22:34,559
uh generate some corrected memory errors

573
00:22:32,799 --> 00:22:37,520
but small amount in the time

574
00:22:34,559 --> 00:22:38,840
but as they are appearing more and more

575
00:22:37,520 --> 00:22:41,760
it is

576
00:22:38,840 --> 00:22:43,840
more probable that we'll hit the

577
00:22:41,760 --> 00:22:46,879
uncorrected memory errors that could

578
00:22:43,840 --> 00:22:48,158
crush our platform so before that

579
00:22:46,880 --> 00:22:50,400
actually happened

580
00:22:48,159 --> 00:22:51,679
we may find out the increase of the

581
00:22:50,400 --> 00:22:54,880
happening of the correct

582
00:22:51,679 --> 00:22:57,760
errors and do something with it so in

583
00:22:54,880 --> 00:23:00,400
this proof of concept we are just uh

584
00:22:57,760 --> 00:23:01,039
triggering the remediation action which

585
00:23:00,400 --> 00:23:02,960
moves

586
00:23:01,039 --> 00:23:04,640
uh the traffic from one vibrancy

587
00:23:02,960 --> 00:23:08,480
instance to another

588
00:23:04,640 --> 00:23:10,720
just to simulate the high viability

589
00:23:08,480 --> 00:23:12,320
so this is that was one of the first

590
00:23:10,720 --> 00:23:13,039
proof of concept to show that it is

591
00:23:12,320 --> 00:23:14,879
possible

592
00:23:13,039 --> 00:23:16,158
based on monitoring the platform

593
00:23:14,880 --> 00:23:19,840
telemetry

594
00:23:16,159 --> 00:23:21,919
uh prevent the outage time or

595
00:23:19,840 --> 00:23:22,879
shorten as much as possible and they

596
00:23:21,919 --> 00:23:26,159
serve its

597
00:23:22,880 --> 00:23:28,480
interruptions but there are more than

598
00:23:26,159 --> 00:23:29,919
just memory being corrupted

599
00:23:28,480 --> 00:23:32,240
you can also watch for example

600
00:23:29,919 --> 00:23:35,760
temperature headroom

601
00:23:32,240 --> 00:23:37,360
to prevent any cpu throttling or you can

602
00:23:35,760 --> 00:23:39,360
watch for the

603
00:23:37,360 --> 00:23:40,559
last level cache occupancy or memory

604
00:23:39,360 --> 00:23:44,320
bandwidth to prevent

605
00:23:40,559 --> 00:23:47,760
any noisy neighbor impacting

606
00:23:44,320 --> 00:23:50,320
or affecting your workloads you can

607
00:23:47,760 --> 00:23:51,840
combine all of those metrics into some

608
00:23:50,320 --> 00:23:54,480
similar indicators

609
00:23:51,840 --> 00:23:57,360
about half half of your platform or a

610
00:23:54,480 --> 00:23:57,360
computed node

611
00:23:58,159 --> 00:24:01,840
which leads us to the second proof of

612
00:24:01,039 --> 00:24:05,919
concept

613
00:24:01,840 --> 00:24:07,678
that was doing actually that so we have

614
00:24:05,919 --> 00:24:09,200
two compute nodes managed by the

615
00:24:07,679 --> 00:24:12,640
kubernetes

616
00:24:09,200 --> 00:24:14,080
we are scraping the rdt pmu ipmi and

617
00:24:12,640 --> 00:24:17,440
transfer technology

618
00:24:14,080 --> 00:24:19,439
metrics we are pushing this to the kafka

619
00:24:17,440 --> 00:24:21,279
stack for streaming analytics which is

620
00:24:19,440 --> 00:24:23,760
we are calculating this hostile

621
00:24:21,279 --> 00:24:26,400
indicator and it is providing this

622
00:24:23,760 --> 00:24:28,320
information to the parameters

623
00:24:26,400 --> 00:24:29,840
now let's take a look at this new

624
00:24:28,320 --> 00:24:33,200
component there that we are

625
00:24:29,840 --> 00:24:35,840
seeing the telemetry our scheduler

626
00:24:33,200 --> 00:24:38,799
this is extension to the default

627
00:24:35,840 --> 00:24:42,240
kubernetes scheduler that is making it

628
00:24:38,799 --> 00:24:45,760
aware about the telemetry to help it

629
00:24:42,240 --> 00:24:49,520
with the scheduling decisions

630
00:24:45,760 --> 00:24:51,120
so you can feed their policies

631
00:24:49,520 --> 00:24:52,960
which are monitoring the particular

632
00:24:51,120 --> 00:24:55,360
metrics and you can say

633
00:24:52,960 --> 00:24:57,760
if the platform for example is healthy

634
00:24:55,360 --> 00:25:00,799
you can deploy there something new

635
00:24:57,760 --> 00:25:04,080
if there are some minor issues or the

636
00:25:00,799 --> 00:25:07,520
resources are getting uh

637
00:25:04,080 --> 00:25:08,799
being saturated then you can just keep

638
00:25:07,520 --> 00:25:11,039
what's already there but

639
00:25:08,799 --> 00:25:13,360
do not schedule anything new or even if

640
00:25:11,039 --> 00:25:15,679
there are any critical issues you can

641
00:25:13,360 --> 00:25:18,559
evacuate everything and reschedule on

642
00:25:15,679 --> 00:25:18,559
more healthy notes

643
00:25:18,880 --> 00:25:25,600
so by monitoring this matrix

644
00:25:22,880 --> 00:25:26,159
you can perform those actions and

645
00:25:25,600 --> 00:25:29,120
prevent

646
00:25:26,159 --> 00:25:29,600
the and do some service healing and

647
00:25:29,120 --> 00:25:33,520
platform

648
00:25:29,600 --> 00:25:36,080
resiliency um

649
00:25:33,520 --> 00:25:36,720
then i can just quickly tell briefly

650
00:25:36,080 --> 00:25:39,279
about the

651
00:25:36,720 --> 00:25:41,520
power saving demo uh on all of the

652
00:25:39,279 --> 00:25:44,400
slides at the bottom there is a link

653
00:25:41,520 --> 00:25:45,200
to where you can find more in detailed

654
00:25:44,400 --> 00:25:48,799
information

655
00:25:45,200 --> 00:25:50,919
about those demos so here we have a

656
00:25:48,799 --> 00:25:52,879
kubernetes cluster that was running the

657
00:25:50,919 --> 00:25:54,720
vcmtspots

658
00:25:52,880 --> 00:25:56,240
that were using pool mode drivers so

659
00:25:54,720 --> 00:25:59,760
they were eating

660
00:25:56,240 --> 00:26:01,279
100 cpu all of the time the

661
00:25:59,760 --> 00:26:03,360
platform telemetry has been pushed to

662
00:26:01,279 --> 00:26:06,400
the influx db and then it was

663
00:26:03,360 --> 00:26:07,279
being monitored by analytics engine that

664
00:26:06,400 --> 00:26:09,440
was previously

665
00:26:07,279 --> 00:26:10,880
trained to find the correlation between

666
00:26:09,440 --> 00:26:13,919
platform telemetry

667
00:26:10,880 --> 00:26:18,159
cpu core frequencies for performance

668
00:26:13,919 --> 00:26:21,600
and packet drop rates

669
00:26:18,159 --> 00:26:23,919
and let's see the results

670
00:26:21,600 --> 00:26:26,719
the red line here is the actual traffic

671
00:26:23,919 --> 00:26:30,080
pattern from one of the operators

672
00:26:26,720 --> 00:26:31,440
from peak to peak it's 24 hour period

673
00:26:30,080 --> 00:26:33,678
time

674
00:26:31,440 --> 00:26:34,480
on the top and the blue line shows us

675
00:26:33,679 --> 00:26:36,480
the

676
00:26:34,480 --> 00:26:38,240
power consumption on the top we are

677
00:26:36,480 --> 00:26:41,440
using the default

678
00:26:38,240 --> 00:26:43,279
linux power governance

679
00:26:41,440 --> 00:26:45,520
performance settings so it is keeping

680
00:26:43,279 --> 00:26:47,120
all of the cars always on the turbo

681
00:26:45,520 --> 00:26:49,200
in the middle we are seeing the on

682
00:26:47,120 --> 00:26:51,279
demand but due to the 100 cpu

683
00:26:49,200 --> 00:26:53,919
utilization it is also

684
00:26:51,279 --> 00:26:55,679
keeping a very high power consumption

685
00:26:53,919 --> 00:26:58,880
and at the bottom we can see

686
00:26:55,679 --> 00:27:02,799
possible saving of the

687
00:26:58,880 --> 00:27:05,039
energy due to the lower course frequency

688
00:27:02,799 --> 00:27:06,960
being managed by this analytics

689
00:27:05,039 --> 00:27:10,080
engineering

690
00:27:06,960 --> 00:27:10,480
and as we don't have much time i will

691
00:27:10,080 --> 00:27:14,000
just

692
00:27:10,480 --> 00:27:16,960
keep that in summary there's

693
00:27:14,000 --> 00:27:17,840
much positive changes that you can do by

694
00:27:16,960 --> 00:27:20,159
monitoring

695
00:27:17,840 --> 00:27:22,879
the platform telemetry going through the

696
00:27:20,159 --> 00:27:26,000
service heating energy optimization

697
00:27:22,880 --> 00:27:28,720
quality of your service

698
00:27:26,000 --> 00:27:30,559
there's also possibility with the intel

699
00:27:28,720 --> 00:27:33,279
threat detection to find the

700
00:27:30,559 --> 00:27:35,039
threats if someone is not trying to

701
00:27:33,279 --> 00:27:37,200
attack your platform which are based on

702
00:27:35,039 --> 00:27:41,200
the pmu metrics for example

703
00:27:37,200 --> 00:27:43,360
uh but also there are als opnv projects

704
00:27:41,200 --> 00:27:45,760
uh that are utilizing barometer and

705
00:27:43,360 --> 00:27:48,559
collecting for example vsphere

706
00:27:45,760 --> 00:27:49,679
bottlenecks and yardstick that are using

707
00:27:48,559 --> 00:27:53,360
them in the testing

708
00:27:49,679 --> 00:27:53,919
phases and as use cases are still

709
00:27:53,360 --> 00:27:56,240
growing

710
00:27:53,919 --> 00:27:56,960
the software still needs to adapt and

711
00:27:56,240 --> 00:28:00,080
evolve to

712
00:27:56,960 --> 00:28:00,880
melt them what leads us to the next

713
00:28:00,080 --> 00:28:02,480
plants

714
00:28:00,880 --> 00:28:04,960
for example in the collect d and in

715
00:28:02,480 --> 00:28:06,880
barometer

716
00:28:04,960 --> 00:28:08,320
so we don't have much time left i'll

717
00:28:06,880 --> 00:28:11,200
race through this

718
00:28:08,320 --> 00:28:11,678
um up next barometer in the next six

719
00:28:11,200 --> 00:28:14,080
months

720
00:28:11,679 --> 00:28:16,000
we hope to help contribute to the

721
00:28:14,080 --> 00:28:19,760
collect e511 release particularly

722
00:28:16,000 --> 00:28:22,559
for a new dbdk telemetry plug-in which

723
00:28:19,760 --> 00:28:25,919
will use a new telemetry api and dpdk

724
00:28:22,559 --> 00:28:29,200
and to proceed the existing dptk stats

725
00:28:25,919 --> 00:28:31,360
plugins that are available we want to

726
00:28:29,200 --> 00:28:32,960
get the capabilities plug-in

727
00:28:31,360 --> 00:28:34,399
merge and that provides some static

728
00:28:32,960 --> 00:28:37,520
system information

729
00:28:34,399 --> 00:28:38,719
uh redfish plug-in and uh md events

730
00:28:37,520 --> 00:28:42,240
plug-ins are also

731
00:28:38,720 --> 00:28:45,279
in flight as well as a bunch of

732
00:28:42,240 --> 00:28:48,640
bug fixes uh we are hoping to get

733
00:28:45,279 --> 00:28:50,480
uh to do more work on our collect dci to

734
00:28:48,640 --> 00:28:54,399
actually run more validation tests

735
00:28:50,480 --> 00:28:57,120
and help to verify uh collect the

736
00:28:54,399 --> 00:28:58,320
uh patches and releases in an automated

737
00:28:57,120 --> 00:29:01,840
fashion

738
00:28:58,320 --> 00:29:05,279
uh always documentation updates

739
00:29:01,840 --> 00:29:07,199
and there are a bunch of uh metrics

740
00:29:05,279 --> 00:29:08,640
uh requests and collaboration requests

741
00:29:07,200 --> 00:29:11,840
from vs perth

742
00:29:08,640 --> 00:29:12,640
from the mana manual api working group

743
00:29:11,840 --> 00:29:16,320
and

744
00:29:12,640 --> 00:29:20,320
from the cntt group which is

745
00:29:16,320 --> 00:29:22,559
the common nfe testing task force

746
00:29:20,320 --> 00:29:24,000
so they want to provide a bunch of new

747
00:29:22,559 --> 00:29:27,279
reference implementations

748
00:29:24,000 --> 00:29:28,480
and uh unify efforts across a bunch of

749
00:29:27,279 --> 00:29:31,840
different projects

750
00:29:28,480 --> 00:29:35,919
in the linux foundation and outside

751
00:29:31,840 --> 00:29:38,399
uh so um if you want to get in touch

752
00:29:35,919 --> 00:29:40,640
um we have a weekly brahma meeting

753
00:29:38,399 --> 00:29:44,000
tuesdays at 5 pm utc

754
00:29:40,640 --> 00:29:47,679
and bi-weekly collect e-meetings

755
00:29:44,000 --> 00:29:48,080
mondays 3 p.m information about both of

756
00:29:47,679 --> 00:29:49,760
these

757
00:29:48,080 --> 00:29:51,360
is available in the mailing list

758
00:29:49,760 --> 00:29:54,720
archives

759
00:29:51,360 --> 00:29:56,559
and you can get in touch by

760
00:29:54,720 --> 00:29:59,279
contacting the relevant mailing lists

761
00:29:56,559 --> 00:30:01,440
for both projects

762
00:29:59,279 --> 00:30:02,320
and if you want to try out uh some of

763
00:30:01,440 --> 00:30:05,120
what you've seen

764
00:30:02,320 --> 00:30:07,678
uh the best place to go is github for

765
00:30:05,120 --> 00:30:09,918
barometer and collect the source code

766
00:30:07,679 --> 00:30:11,840
as well as the service telemetry

767
00:30:09,919 --> 00:30:13,679
framework

768
00:30:11,840 --> 00:30:15,120
to get started with documentation on

769
00:30:13,679 --> 00:30:17,279
collectd their wiki is pretty

770
00:30:15,120 --> 00:30:18,719
comprehensive and lays out

771
00:30:17,279 --> 00:30:20,880
all the configuration instructions for

772
00:30:18,720 --> 00:30:22,720
each individual plugin

773
00:30:20,880 --> 00:30:24,000
if you want to dive into plug-in

774
00:30:22,720 --> 00:30:27,679
development in

775
00:30:24,000 --> 00:30:30,159
collecti we put together a

776
00:30:27,679 --> 00:30:31,760
plug-in development guide in barometer

777
00:30:30,159 --> 00:30:34,080
which is focused solely on getting your

778
00:30:31,760 --> 00:30:37,039
first plugin up and running

779
00:30:34,080 --> 00:30:38,480
and if you want to get involved by

780
00:30:37,039 --> 00:30:41,840
contributing

781
00:30:38,480 --> 00:30:42,640
features or share your own requests or

782
00:30:41,840 --> 00:30:46,000
requirements

783
00:30:42,640 --> 00:30:49,360
uh that information is on the

784
00:30:46,000 --> 00:30:51,840
opn fee wiki on barometer's page

785
00:30:49,360 --> 00:30:53,439
all of these links will be up on the

786
00:30:51,840 --> 00:30:57,760
schedule later

787
00:30:53,440 --> 00:30:59,360
so you can find them there

788
00:30:57,760 --> 00:31:01,279
if you want to contribute to collect d

789
00:30:59,360 --> 00:31:03,600
there is a a bunch of different things

790
00:31:01,279 --> 00:31:07,039
you can do

791
00:31:03,600 --> 00:31:10,320
down to starting with simple testing or

792
00:31:07,039 --> 00:31:15,600
you can contribute changes both features

793
00:31:10,320 --> 00:31:18,158
and bug fixes or you can

794
00:31:15,600 --> 00:31:18,959
provide code reviews and more

795
00:31:18,159 --> 00:31:22,159
information

796
00:31:18,960 --> 00:31:22,880
on that link and if you just want more

797
00:31:22,159 --> 00:31:25,360
information

798
00:31:22,880 --> 00:31:28,159
you are welcome to comment on pull

799
00:31:25,360 --> 00:31:31,760
requests asking for clarification

800
00:31:28,159 --> 00:31:34,240
or catch us on irc

801
00:31:31,760 --> 00:31:34,240
collect d

802
00:31:35,919 --> 00:31:39,120
okay and if you still want to get

803
00:31:38,559 --> 00:31:40,399
involved

804
00:31:39,120 --> 00:31:42,239
there's actually collecting meetup

805
00:31:40,399 --> 00:31:44,158
happening later this month in munich

806
00:31:42,240 --> 00:31:46,720
this is the second

807
00:31:44,159 --> 00:31:48,080
second one and we're going to be

808
00:31:46,720 --> 00:31:51,919
discussing

809
00:31:48,080 --> 00:31:54,240
things like new features um

810
00:31:51,919 --> 00:31:55,760
testing strategies upstream processes

811
00:31:54,240 --> 00:31:59,200
release processes

812
00:31:55,760 --> 00:32:00,000
and discussing um architecture and

813
00:31:59,200 --> 00:32:03,200
requirements

814
00:32:00,000 --> 00:32:06,159
for collect e 6.0 which would be the

815
00:32:03,200 --> 00:32:06,960
uh next major release of it and would

816
00:32:06,159 --> 00:32:08,720
represent

817
00:32:06,960 --> 00:32:10,480
a lot of efforts to actually make

818
00:32:08,720 --> 00:32:14,000
collecti more cloudy

819
00:32:10,480 --> 00:32:16,240
so things like an api for

820
00:32:14,000 --> 00:32:18,159
submitting and querying metrics and for

821
00:32:16,240 --> 00:32:19,279
dynamic reconfiguration of collective

822
00:32:18,159 --> 00:32:23,120
because at the moment

823
00:32:19,279 --> 00:32:27,760
it's pretty static in its configuration

824
00:32:23,120 --> 00:32:30,799
and also features like adding

825
00:32:27,760 --> 00:32:34,080
labels to metrics so that they'll be

826
00:32:30,799 --> 00:32:36,559
a little bit closer in functionality to

827
00:32:34,080 --> 00:32:38,158
the to other collectors that are

828
00:32:36,559 --> 00:32:41,120
available

829
00:32:38,159 --> 00:32:42,720
um information on that schedule on ether

830
00:32:41,120 --> 00:32:46,399
pad and

831
00:32:42,720 --> 00:32:49,519
meet up information on the mailing list

832
00:32:46,399 --> 00:32:53,039
and before i finish i would like to

833
00:32:49,519 --> 00:32:53,039
note that it was not just us

834
00:32:53,440 --> 00:32:56,799
helping with this work usually when you

835
00:32:55,840 --> 00:32:58,959
get someone up

836
00:32:56,799 --> 00:33:00,240
presenting it's easy to to forget

837
00:32:58,960 --> 00:33:01,840
there's actually a lot of people

838
00:33:00,240 --> 00:33:04,320
also contributing to the projects as

839
00:33:01,840 --> 00:33:06,158
well so i'd like to thank these people

840
00:33:04,320 --> 00:33:10,559
that helped with

841
00:33:06,159 --> 00:33:12,159
various demos development and

842
00:33:10,559 --> 00:33:14,720
those requirements and driving the

843
00:33:12,159 --> 00:33:14,720
projects

844
00:33:14,880 --> 00:33:27,039
and does anybody have any questions

845
00:33:24,399 --> 00:33:27,918
so i i had calls to look at collectd

846
00:33:27,039 --> 00:33:30,720
recently

847
00:33:27,919 --> 00:33:32,399
and the whole ev every time you add a

848
00:33:30,720 --> 00:33:35,039
new data source

849
00:33:32,399 --> 00:33:37,120
add a plug-in it seems to somewhat

850
00:33:35,039 --> 00:33:39,519
limited scalability

851
00:33:37,120 --> 00:33:40,799
you're kind of bound to waiting for a

852
00:33:39,519 --> 00:33:44,000
new collective

853
00:33:40,799 --> 00:33:45,760
release to add a new plugin to get that

854
00:33:44,000 --> 00:33:47,840
new piece of functionality are any plans

855
00:33:45,760 --> 00:33:50,080
to make that more dynamic

856
00:33:47,840 --> 00:33:52,320
yeah that's part of the discussion for

857
00:33:50,080 --> 00:33:54,080
6.0 as it might require major

858
00:33:52,320 --> 00:33:56,399
rear architecture of the collective

859
00:33:54,080 --> 00:33:56,879
internals so there are plans to make it

860
00:33:56,399 --> 00:33:59,840
more

861
00:33:56,880 --> 00:34:02,559
dynamically reloadable as part of this

862
00:33:59,840 --> 00:34:02,559
codification

863
00:34:03,519 --> 00:34:09,359
question right now yeah

864
00:34:06,720 --> 00:34:11,119
the question actually was uh that there

865
00:34:09,359 --> 00:34:12,000
is an issue with the calligraphy that if

866
00:34:11,119 --> 00:34:13,359
you add the

867
00:34:12,000 --> 00:34:15,040
want to change the configuration you

868
00:34:13,359 --> 00:34:18,239
have to restart it

869
00:34:15,040 --> 00:34:18,639
and are there any plans to change it so

870
00:34:18,239 --> 00:34:21,839
yeah

871
00:34:18,639 --> 00:34:21,839
the answer is

872
00:34:36,480 --> 00:34:44,399
there were so many metrics that

873
00:34:39,679 --> 00:34:47,599
were written down the hard drive so it

874
00:34:44,399 --> 00:34:50,480
took a lot of so

875
00:34:47,599 --> 00:34:51,040
now time presenting an approach to check

876
00:34:50,480 --> 00:34:54,879
extra

877
00:34:51,040 --> 00:34:58,400
layers on top of pulling so

878
00:34:54,879 --> 00:34:59,040
like this is do you have any benchmarks

879
00:34:58,400 --> 00:35:02,480
or how

880
00:34:59,040 --> 00:35:07,119
does it scale or what's your opinion on

881
00:35:02,480 --> 00:35:07,119
how to size this system for operating

882
00:35:07,200 --> 00:35:11,279
so the question was that

883
00:35:12,000 --> 00:35:16,000
so the question was that collectively

884
00:35:14,800 --> 00:35:19,040
produced a lot of metrics

885
00:35:16,000 --> 00:35:20,720
this takes up a lot of disk space um how

886
00:35:19,040 --> 00:35:23,920
does this actually scale

887
00:35:20,720 --> 00:35:25,598
and the

888
00:35:23,920 --> 00:35:27,440
what we've presented here shows

889
00:35:25,599 --> 00:35:28,160
additional layers of complexity and do

890
00:35:27,440 --> 00:35:31,599
we have any

891
00:35:28,160 --> 00:35:33,598
uh benchmarks

892
00:35:31,599 --> 00:35:35,520
benchmarks for scaling or guides for

893
00:35:33,599 --> 00:35:40,079
scaling

894
00:35:35,520 --> 00:35:43,839
we occasionally run benchmarks

895
00:35:40,079 --> 00:35:47,520
in terms of storage

896
00:35:43,839 --> 00:35:50,480
mostly the metrics are dispatched to um

897
00:35:47,520 --> 00:35:52,320
to remote locations and things like

898
00:35:50,480 --> 00:35:52,880
prometheus aren't designed for long-term

899
00:35:52,320 --> 00:35:55,280
storage

900
00:35:52,880 --> 00:35:56,320
of the metrics so typically they will be

901
00:35:55,280 --> 00:36:00,640
aggregated

902
00:35:56,320 --> 00:36:02,960
and an archive to reduce

903
00:36:00,640 --> 00:36:05,759
um reduce the amount of metrics you have

904
00:36:02,960 --> 00:36:05,760
to actually store

905
00:36:06,079 --> 00:36:10,160
and unfortunately the time's up if

906
00:36:08,320 --> 00:36:12,560
anyone has any more questions

907
00:36:10,160 --> 00:36:22,240
feel free to come up afterwards thank

908
00:36:12,560 --> 00:36:22,240
you thank you

