1
00:00:05,839 --> 00:00:08,559
so i will talk about decentralized

2
00:00:07,279 --> 00:00:12,000
management of

3
00:00:08,559 --> 00:00:12,959
digital objects for science and although

4
00:00:12,000 --> 00:00:15,839
i'm giving the talk

5
00:00:12,960 --> 00:00:16,160
what i will present is not my own work

6
00:00:15,839 --> 00:00:18,960
or

7
00:00:16,160 --> 00:00:19,520
not solely my own work but you'll see

8
00:00:18,960 --> 00:00:21,520
the

9
00:00:19,520 --> 00:00:23,359
the essentially the works of five core

10
00:00:21,520 --> 00:00:26,400
developers and the dedicated

11
00:00:23,359 --> 00:00:27,760
documentation project team and

12
00:00:26,400 --> 00:00:29,759
given that we've heard how important

13
00:00:27,760 --> 00:00:32,480
funding and networking is uh

14
00:00:29,760 --> 00:00:33,360
i'm happy to say that we're funded and

15
00:00:32,479 --> 00:00:36,078
networked

16
00:00:33,360 --> 00:00:36,800
so that should give you an indication of

17
00:00:36,079 --> 00:00:40,320
the

18
00:00:36,800 --> 00:00:42,399
likelihood of survival for the

19
00:00:40,320 --> 00:00:44,079
immediate future so i'll be talking

20
00:00:42,399 --> 00:00:45,680
about a tool called dataled

21
00:00:44,079 --> 00:00:48,239
and it's for joint management of data

22
00:00:45,680 --> 00:00:49,840
code through their entire

23
00:00:48,239 --> 00:00:52,718
life cycle so that includes version

24
00:00:49,840 --> 00:00:54,160
control getting data from a to b

25
00:00:52,719 --> 00:00:56,480
it's a free and open source software

26
00:00:54,160 --> 00:00:58,239
written in python mit licensed

27
00:00:56,480 --> 00:00:59,519
so it has a python api and a command

28
00:00:58,239 --> 00:01:02,320
line interface

29
00:00:59,520 --> 00:01:03,840
and and so on and instead of you know

30
00:01:02,320 --> 00:01:05,760
giving a lengthy explanation

31
00:01:03,840 --> 00:01:07,119
of of you know what this really means

32
00:01:05,760 --> 00:01:08,640
and what the details are

33
00:01:07,119 --> 00:01:10,720
uh given that this is a developer

34
00:01:08,640 --> 00:01:14,560
meeting i anticipate the question

35
00:01:10,720 --> 00:01:18,158
uh sounds like it why not use git

36
00:01:14,560 --> 00:01:20,240
and i can tell you that a data data set

37
00:01:18,159 --> 00:01:22,159
which is the core data structure that

38
00:01:20,240 --> 00:01:25,039
the datalite software uses to

39
00:01:22,159 --> 00:01:26,240
handle everything is actually a git

40
00:01:25,040 --> 00:01:28,479
repository

41
00:01:26,240 --> 00:01:30,000
so everything you're seeing uh

42
00:01:28,479 --> 00:01:32,479
feature-wise sits on top of

43
00:01:30,000 --> 00:01:33,680
a very mature let's call it

44
00:01:32,479 --> 00:01:36,960
industry-grade

45
00:01:33,680 --> 00:01:37,759
uh base of tooling there are a few

46
00:01:36,960 --> 00:01:39,520
principles

47
00:01:37,759 --> 00:01:41,680
uh that we follow in the development of

48
00:01:39,520 --> 00:01:42,560
dataled that try to make sure that we

49
00:01:41,680 --> 00:01:45,040
don't ruin

50
00:01:42,560 --> 00:01:47,280
the the features and and possibilities

51
00:01:45,040 --> 00:01:49,360
that that the tool like it gives us

52
00:01:47,280 --> 00:01:50,960
so for example uh dale ed only

53
00:01:49,360 --> 00:01:51,679
recognizes two entities in the world

54
00:01:50,960 --> 00:01:54,640
ones are

55
00:01:51,680 --> 00:01:56,640
you know files we all know them and data

56
00:01:54,640 --> 00:01:58,640
sets which are collections of files

57
00:01:56,640 --> 00:02:00,159
and there's there's no other you know

58
00:01:58,640 --> 00:02:02,799
domain specific

59
00:02:00,159 --> 00:02:03,680
uh specialization in there we try to

60
00:02:02,799 --> 00:02:06,560
minimize

61
00:02:03,680 --> 00:02:07,280
uh custom procedures and and routines

62
00:02:06,560 --> 00:02:08,878
that

63
00:02:07,280 --> 00:02:11,038
are necessary so if git provides this

64
00:02:08,878 --> 00:02:13,280
feature to implement

65
00:02:11,038 --> 00:02:14,238
a solution for a given problem then we

66
00:02:13,280 --> 00:02:16,959
will do that

67
00:02:14,239 --> 00:02:18,160
with the aim that if data led being and

68
00:02:16,959 --> 00:02:20,480
somewhat academic

69
00:02:18,160 --> 00:02:21,680
development project happens to die that

70
00:02:20,480 --> 00:02:24,319
its users will not

71
00:02:21,680 --> 00:02:25,520
unnecessarily suffer from that death but

72
00:02:24,319 --> 00:02:27,839
they can continue

73
00:02:25,520 --> 00:02:28,959
uh with you know the the mature base

74
00:02:27,840 --> 00:02:31,040
still being intact

75
00:02:28,959 --> 00:02:32,000
and i think we can agree that if git

76
00:02:31,040 --> 00:02:34,079
will vanish

77
00:02:32,000 --> 00:02:36,640
something will will be developed that

78
00:02:34,080 --> 00:02:39,120
allows allows us to transition to it

79
00:02:36,640 --> 00:02:40,079
and we also try very hard to not uh

80
00:02:39,120 --> 00:02:41,840
compromise the

81
00:02:40,080 --> 00:02:43,760
complete decentralization that git

82
00:02:41,840 --> 00:02:47,360
allows us so no mandatory

83
00:02:43,760 --> 00:02:48,720
services and

84
00:02:47,360 --> 00:02:50,160
if you've worked with git and you try to

85
00:02:48,720 --> 00:02:52,080
put lots of data into it you'll know

86
00:02:50,160 --> 00:02:54,400
that git doesn't handle large files

87
00:02:52,080 --> 00:02:55,599
very well a quick question to the room

88
00:02:54,400 --> 00:02:58,720
who knows about

89
00:02:55,599 --> 00:03:00,079
or who's heard about git nx so quite a

90
00:02:58,720 --> 00:03:01,840
few people

91
00:03:00,080 --> 00:03:03,120
gitanex is the tool that we use in

92
00:03:01,840 --> 00:03:04,560
dataled in order to

93
00:03:03,120 --> 00:03:06,080
manage large files so instead of

94
00:03:04,560 --> 00:03:07,280
managing putting the content into a

95
00:03:06,080 --> 00:03:08,959
version control we only put

96
00:03:07,280 --> 00:03:10,560
information about its location and

97
00:03:08,959 --> 00:03:14,000
identity into

98
00:03:10,560 --> 00:03:17,360
into kit which makes a lot of things

99
00:03:14,000 --> 00:03:19,599
much more easy and gideon ex for those

100
00:03:17,360 --> 00:03:20,560
who don't know it but no uh get large

101
00:03:19,599 --> 00:03:23,839
file uh

102
00:03:20,560 --> 00:03:25,519
support is the from our perspective and

103
00:03:23,840 --> 00:03:27,200
given the principles that i mentioned

104
00:03:25,519 --> 00:03:28,879
the superior alternative because it's a

105
00:03:27,200 --> 00:03:29,359
completely decentralized solution so

106
00:03:28,879 --> 00:03:31,599
every

107
00:03:29,360 --> 00:03:33,200
everything that you can do with git for

108
00:03:31,599 --> 00:03:35,679
code you can do with gitanex

109
00:03:33,200 --> 00:03:36,480
plus git for large files which is very

110
00:03:35,680 --> 00:03:39,519
nice

111
00:03:36,480 --> 00:03:43,119
and the purpose of dataled here is

112
00:03:39,519 --> 00:03:46,239
to make well for those who know git nx

113
00:03:43,120 --> 00:03:49,760
the the actual use inside it uh

114
00:03:46,239 --> 00:03:53,519
to uh transparent to to a certain degree

115
00:03:49,760 --> 00:03:56,560
so uh when i just said that we are

116
00:03:53,519 --> 00:03:58,000
basically using git and gideon x and

117
00:03:56,560 --> 00:03:59,920
everything that we want to do can be

118
00:03:58,000 --> 00:04:01,439
done with them why is there need for

119
00:03:59,920 --> 00:04:02,000
dedicated software that sits on top of

120
00:04:01,439 --> 00:04:03,599
it

121
00:04:02,000 --> 00:04:04,720
and there's a there's a single most

122
00:04:03,599 --> 00:04:06,480
important reason there are other reasons

123
00:04:04,720 --> 00:04:08,720
but one is most important

124
00:04:06,480 --> 00:04:10,238
and that is a single repository is is

125
00:04:08,720 --> 00:04:10,959
typically not enough for many of the

126
00:04:10,239 --> 00:04:13,920
workflows

127
00:04:10,959 --> 00:04:15,280
that we need particularly in science and

128
00:04:13,920 --> 00:04:16,798
there are technical reasons for that for

129
00:04:15,280 --> 00:04:18,560
example if you try to put a million

130
00:04:16,798 --> 00:04:20,000
files into git repository or

131
00:04:18,560 --> 00:04:22,400
try to do a hundred thousand commits you

132
00:04:20,000 --> 00:04:25,680
will quickly see the end of the system

133
00:04:22,400 --> 00:04:27,440
and the uh there are other

134
00:04:25,680 --> 00:04:29,120
application based issues for example if

135
00:04:27,440 --> 00:04:31,600
you have different uh

136
00:04:29,120 --> 00:04:33,040
bits of information that are targeting

137
00:04:31,600 --> 00:04:34,560
different audiences with different

138
00:04:33,040 --> 00:04:36,400
access permissions and so on

139
00:04:34,560 --> 00:04:38,800
it gets very complicated even on a

140
00:04:36,400 --> 00:04:40,799
technical level to make that happen

141
00:04:38,800 --> 00:04:42,080
if you're if you're stuck with a single

142
00:04:40,800 --> 00:04:43,919
uh repository

143
00:04:42,080 --> 00:04:45,199
and there are other reasons i could go

144
00:04:43,919 --> 00:04:48,400
on about this

145
00:04:45,199 --> 00:04:50,400
quite a while so in science

146
00:04:48,400 --> 00:04:53,758
we essentially only have what i would

147
00:04:50,400 --> 00:04:56,799
call modular data management

148
00:04:53,759 --> 00:04:58,479
issues in particular in in collaborative

149
00:04:56,800 --> 00:05:01,759
environments so if we look at

150
00:04:58,479 --> 00:05:03,120
a a typical science of workflow then

151
00:05:01,759 --> 00:05:05,919
we're talking about

152
00:05:03,120 --> 00:05:08,479
the combination of individual pieces

153
00:05:05,919 --> 00:05:10,320
that have been developed in the past

154
00:05:08,479 --> 00:05:12,240
into something that produces novel

155
00:05:10,320 --> 00:05:13,280
results which will then be published in

156
00:05:12,240 --> 00:05:14,720
in one way or another

157
00:05:13,280 --> 00:05:16,559
right so in more abstract terms we're

158
00:05:14,720 --> 00:05:17,360
talking about aggregation of works

159
00:05:16,560 --> 00:05:20,479
across

160
00:05:17,360 --> 00:05:21,280
time and also different uh collaborators

161
00:05:20,479 --> 00:05:24,320
or con

162
00:05:21,280 --> 00:05:26,479
contributors so mapping these onto

163
00:05:24,320 --> 00:05:27,520
uh dedicated repositories feels kind of

164
00:05:26,479 --> 00:05:29,680
natural right so you

165
00:05:27,520 --> 00:05:30,880
develop a library you have a source code

166
00:05:29,680 --> 00:05:33,680
repository for it you

167
00:05:30,880 --> 00:05:35,600
have a data catalog you have a you know

168
00:05:33,680 --> 00:05:38,960
some sort of data set for that

169
00:05:35,600 --> 00:05:40,639
right so these modular environments are

170
00:05:38,960 --> 00:05:43,440
kind of natural to the thing

171
00:05:40,639 --> 00:05:44,000
that we do now what i want to show you

172
00:05:43,440 --> 00:05:47,199
now

173
00:05:44,000 --> 00:05:47,919
is a lot of code that just runs through

174
00:05:47,199 --> 00:05:51,120
and just

175
00:05:47,919 --> 00:05:53,359
gives you a more or less visceral

176
00:05:51,120 --> 00:05:55,520
demonstration how how that feels like if

177
00:05:53,360 --> 00:05:57,199
you do it with git and guidance natively

178
00:05:55,520 --> 00:05:58,960
so just click the play button it's not

179
00:05:57,199 --> 00:06:00,720
necessary that you you know read all the

180
00:05:58,960 --> 00:06:01,680
lines what's happening here is basically

181
00:06:00,720 --> 00:06:04,800
assembling

182
00:06:01,680 --> 00:06:06,880
in-git usage the picture that we've just

183
00:06:04,800 --> 00:06:08,240
seen so there's a student that created a

184
00:06:06,880 --> 00:06:10,319
repository

185
00:06:08,240 --> 00:06:12,160
for code there's another student that

186
00:06:10,319 --> 00:06:13,120
collected some data put it into another

187
00:06:12,160 --> 00:06:15,919
repository

188
00:06:13,120 --> 00:06:17,440
and now there's a postdoc that creates

189
00:06:15,919 --> 00:06:20,719
another repository

190
00:06:17,440 --> 00:06:22,400
that uses git sub module mechanism to

191
00:06:20,720 --> 00:06:25,520
combine these

192
00:06:22,400 --> 00:06:26,560
repositories into a third repository

193
00:06:25,520 --> 00:06:28,719
that then will

194
00:06:26,560 --> 00:06:29,680
track analysis results that have been

195
00:06:28,720 --> 00:06:32,720
derived

196
00:06:29,680 --> 00:06:35,360
from the combination of

197
00:06:32,720 --> 00:06:36,000
the the the data or the application of

198
00:06:35,360 --> 00:06:39,280
some novel

199
00:06:36,000 --> 00:06:40,800
analysis algorithm uh to these data

200
00:06:39,280 --> 00:06:42,638
so that could be a situation that has

201
00:06:40,800 --> 00:06:45,199
happened in the past it's the

202
00:06:42,639 --> 00:06:46,080
situation in in some lab and now the pi

203
00:06:45,199 --> 00:06:49,919
comes

204
00:06:46,080 --> 00:06:51,758
after a while and write up a paper about

205
00:06:49,919 --> 00:06:54,318
this so how does it look like

206
00:06:51,759 --> 00:06:55,199
if we have another git repository that

207
00:06:54,319 --> 00:06:56,960
tries to

208
00:06:55,199 --> 00:06:58,319
you know contain the manuscript which

209
00:06:56,960 --> 00:07:00,080
then you know somewhat

210
00:06:58,319 --> 00:07:01,599
you know comprehensively tracks all the

211
00:07:00,080 --> 00:07:03,039
inputs so how does it look like so we

212
00:07:01,599 --> 00:07:06,479
start with the repository

213
00:07:03,039 --> 00:07:10,000
gets created uh and then we put in

214
00:07:06,479 --> 00:07:13,280
the entire study repository as a

215
00:07:10,000 --> 00:07:15,039
git nx infused repository and the pi

216
00:07:13,280 --> 00:07:16,799
needs to remember that this one needs to

217
00:07:15,039 --> 00:07:18,400
be initialized so he needs to look at

218
00:07:16,800 --> 00:07:21,039
you know what the branches are that are

219
00:07:18,400 --> 00:07:21,520
available and then we need to emit that

220
00:07:21,039 --> 00:07:24,400
one

221
00:07:21,520 --> 00:07:27,280
and of course at this point we are ready

222
00:07:24,400 --> 00:07:28,479
so we can use gita again to assemble the

223
00:07:27,280 --> 00:07:30,239
entire work tree which

224
00:07:28,479 --> 00:07:32,400
clones all the repositories we have a

225
00:07:30,240 --> 00:07:34,080
nice project-based directory

226
00:07:32,400 --> 00:07:35,520
that contains the data the code

227
00:07:34,080 --> 00:07:37,680
everything that's necessary

228
00:07:35,520 --> 00:07:38,639
really nice for reproducibility

229
00:07:37,680 --> 00:07:40,880
excellent

230
00:07:38,639 --> 00:07:42,080
but the pi finds a code a bug in the

231
00:07:40,880 --> 00:07:44,879
code it happens

232
00:07:42,080 --> 00:07:46,800
right so tries to apply the fix to the

233
00:07:44,879 --> 00:07:47,840
code repository because the postdoc is

234
00:07:46,800 --> 00:07:50,479
not there to do it

235
00:07:47,840 --> 00:07:52,000
uh for the pi so it it goes right in

236
00:07:50,479 --> 00:07:53,680
there so it's a decentralized system

237
00:07:52,000 --> 00:07:55,039
right should be no problem to just do it

238
00:07:53,680 --> 00:07:57,280
in any random clone

239
00:07:55,039 --> 00:07:59,680
uh and then tries to just get at the

240
00:07:57,280 --> 00:08:03,359
code and then the problems start in this

241
00:07:59,680 --> 00:08:06,720
in this scenario so git does not

242
00:08:03,360 --> 00:08:08,639
really well or makes the usage of those

243
00:08:06,720 --> 00:08:09,759
sub module mechanisms really easy so we

244
00:08:08,639 --> 00:08:11,039
have to remember

245
00:08:09,759 --> 00:08:12,720
if we want to commit something to the

246
00:08:11,039 --> 00:08:14,318
code repository then we have to remember

247
00:08:12,720 --> 00:08:15,360
where's the code repository go into the

248
00:08:14,319 --> 00:08:17,680
code repository

249
00:08:15,360 --> 00:08:19,280
and then commit it there but given that

250
00:08:17,680 --> 00:08:22,879
it's a

251
00:08:19,280 --> 00:08:23,679
sub module we will discover that git

252
00:08:22,879 --> 00:08:25,120
uses

253
00:08:23,680 --> 00:08:26,800
what's called detached heads so if you

254
00:08:25,120 --> 00:08:28,319
can commit something to that we'll later

255
00:08:26,800 --> 00:08:28,800
on discover that we cannot publish it

256
00:08:28,319 --> 00:08:30,720
back

257
00:08:28,800 --> 00:08:32,880
to the original repository it all

258
00:08:30,720 --> 00:08:34,320
becomes a nightmare and in the end we

259
00:08:32,880 --> 00:08:35,838
have to go up the hierarchy

260
00:08:34,320 --> 00:08:38,959
and commit all the changes in order to

261
00:08:35,839 --> 00:08:40,800
have a an orderly clean repository

262
00:08:38,958 --> 00:08:42,079
at the very end and of course this can

263
00:08:40,799 --> 00:08:44,240
all be done

264
00:08:42,080 --> 00:08:45,440
but it's very complicated and tends to

265
00:08:44,240 --> 00:08:47,680
be used as an argument

266
00:08:45,440 --> 00:08:48,640
why you we wouldn't want to use that

267
00:08:47,680 --> 00:08:51,040
kind of machinery

268
00:08:48,640 --> 00:08:52,240
in typical science workflows and then

269
00:08:51,040 --> 00:08:54,880
there are little

270
00:08:52,240 --> 00:08:55,279
bits and pieces we can we can use git nx

271
00:08:54,880 --> 00:08:56,720
to

272
00:08:55,279 --> 00:08:58,720
to obtain the data that's what it's

273
00:08:56,720 --> 00:09:00,959
written for but again get nx focused on

274
00:08:58,720 --> 00:09:02,640
a single repository so it won't it won't

275
00:09:00,959 --> 00:09:04,399
give us the data we need unless we go

276
00:09:02,640 --> 00:09:05,680
back to the repository that actually

277
00:09:04,399 --> 00:09:07,920
contains the data

278
00:09:05,680 --> 00:09:08,959
and then discover it you know it knows

279
00:09:07,920 --> 00:09:10,560
where how to get it from

280
00:09:08,959 --> 00:09:12,239
fulfills the file handle and so on we

281
00:09:10,560 --> 00:09:12,880
have data all the situation that we

282
00:09:12,240 --> 00:09:14,640
wanted

283
00:09:12,880 --> 00:09:15,920
everything is good and at the end of the

284
00:09:14,640 --> 00:09:17,839
project the problems

285
00:09:15,920 --> 00:09:19,199
are still not done because we can't just

286
00:09:17,839 --> 00:09:21,279
use a system call

287
00:09:19,200 --> 00:09:23,200
to remove the project because gitanex

288
00:09:21,279 --> 00:09:25,360
uses special tricks and pieces to

289
00:09:23,200 --> 00:09:27,760
protect us from data loss which then

290
00:09:25,360 --> 00:09:29,360
turn out to be

291
00:09:27,760 --> 00:09:31,760
involve required knowledge that we need

292
00:09:29,360 --> 00:09:33,680
to give it back permissions and so on so

293
00:09:31,760 --> 00:09:35,360
and you can imagine how that would feel

294
00:09:33,680 --> 00:09:37,040
like for a person who doesn't do that

295
00:09:35,360 --> 00:09:38,320
full time right so all the information

296
00:09:37,040 --> 00:09:39,519
is lost all the time needs to be

297
00:09:38,320 --> 00:09:41,760
rediscovered all the time

298
00:09:39,519 --> 00:09:42,800
it's a big hassle and and causes

299
00:09:41,760 --> 00:09:45,120
complications

300
00:09:42,800 --> 00:09:46,079
so for the rest of the the demonstration

301
00:09:45,120 --> 00:09:49,200
we'll just do

302
00:09:46,080 --> 00:09:51,040
the pi part again and use datalat so you

303
00:09:49,200 --> 00:09:53,120
can you can see how it's different

304
00:09:51,040 --> 00:09:54,800
we use the exact same pieces as input

305
00:09:53,120 --> 00:09:55,120
that were produced by the students and

306
00:09:54,800 --> 00:09:57,439
the

307
00:09:55,120 --> 00:09:59,600
the postdoc to provide the the building

308
00:09:57,440 --> 00:10:02,320
blocks for the paper

309
00:09:59,600 --> 00:10:03,440
but we'll now use just uh um daylight

310
00:10:02,320 --> 00:10:05,839
commands so we can

311
00:10:03,440 --> 00:10:07,760
create a data set which is create a git

312
00:10:05,839 --> 00:10:09,680
repository simply by using create

313
00:10:07,760 --> 00:10:11,360
that's all you need to remember we can

314
00:10:09,680 --> 00:10:13,279
clone any other data set

315
00:10:11,360 --> 00:10:14,480
at any place in any other data set and

316
00:10:13,279 --> 00:10:16,240
make it a sub data set

317
00:10:14,480 --> 00:10:17,839
that uses the submodule mechanism we

318
00:10:16,240 --> 00:10:18,959
don't see it yet all the setup that is

319
00:10:17,839 --> 00:10:22,000
necessary is done

320
00:10:18,959 --> 00:10:26,160
inside we can

321
00:10:22,000 --> 00:10:29,279
ask for any piece in this super data set

322
00:10:26,160 --> 00:10:30,480
simply using the get command that will

323
00:10:29,279 --> 00:10:32,480
automatically obtain

324
00:10:30,480 --> 00:10:34,800
all the intermediate repositories that

325
00:10:32,480 --> 00:10:37,839
might be necessary in order to fulfill

326
00:10:34,800 --> 00:10:38,640
a a request to a particular data file or

327
00:10:37,839 --> 00:10:40,399
code file

328
00:10:38,640 --> 00:10:41,839
all happens automatically there is no

329
00:10:40,399 --> 00:10:44,480
need for specific

330
00:10:41,839 --> 00:10:45,200
re-initialization and the great thing is

331
00:10:44,480 --> 00:10:46,800
that

332
00:10:45,200 --> 00:10:48,480
if we actually work with these

333
00:10:46,800 --> 00:10:51,359
hierarchies

334
00:10:48,480 --> 00:10:53,360
dataled will feel will make this

335
00:10:51,360 --> 00:10:55,200
hierarchy of nested repositories

336
00:10:53,360 --> 00:10:57,440
actually feel like a single monorepo

337
00:10:55,200 --> 00:10:58,399
so we can do things like status requests

338
00:10:57,440 --> 00:11:01,120
and it will tell us

339
00:10:58,399 --> 00:11:03,040
not only that the top-level sub-data set

340
00:11:01,120 --> 00:11:05,040
or sub-module is modified but gives

341
00:11:03,040 --> 00:11:06,319
an actual indication what is the

342
00:11:05,040 --> 00:11:08,560
situation

343
00:11:06,320 --> 00:11:09,920
all the way down and because it can do

344
00:11:08,560 --> 00:11:12,319
that it can also

345
00:11:09,920 --> 00:11:12,959
apply uh modifications really quickly so

346
00:11:12,320 --> 00:11:14,959
we can

347
00:11:12,959 --> 00:11:16,560
we can just say save me this entire

348
00:11:14,959 --> 00:11:18,000
thing and it will make sure

349
00:11:16,560 --> 00:11:19,680
that the actual change is committed to

350
00:11:18,000 --> 00:11:20,240
the repository that contains the change

351
00:11:19,680 --> 00:11:22,880
and all the

352
00:11:20,240 --> 00:11:25,120
repositories upstairs know that there

353
00:11:22,880 --> 00:11:27,200
was a change that was caused by this

354
00:11:25,120 --> 00:11:28,480
thing and applies the commit messages

355
00:11:27,200 --> 00:11:29,279
all the way up so we have a clean

356
00:11:28,480 --> 00:11:32,880
hierarchy

357
00:11:29,279 --> 00:11:36,079
uh at the at the very top so that's

358
00:11:32,880 --> 00:11:38,320
my idea of an of a demonstration um

359
00:11:36,079 --> 00:11:40,640
how you know why data led is

360
00:11:38,320 --> 00:11:43,200
particularly suited more suited than

361
00:11:40,640 --> 00:11:44,720
um than git and guidance on their own

362
00:11:43,200 --> 00:11:49,040
for these modular

363
00:11:44,720 --> 00:11:50,959
workflows so the situation

364
00:11:49,040 --> 00:11:52,880
if we if we continue this line of

365
00:11:50,959 --> 00:11:56,560
thought is that we can essentially map

366
00:11:52,880 --> 00:11:57,200
all those modular uh problems where we

367
00:11:56,560 --> 00:12:00,239
have

368
00:11:57,200 --> 00:12:00,959
data that comes from you know different

369
00:12:00,240 --> 00:12:02,720
entities

370
00:12:00,959 --> 00:12:04,160
is maintained at a different pace

371
00:12:02,720 --> 00:12:06,240
evolves at a different pace

372
00:12:04,160 --> 00:12:07,920
has different access permissions etc

373
00:12:06,240 --> 00:12:09,519
that we still want to combine to produce

374
00:12:07,920 --> 00:12:11,360
scientific results in a very

375
00:12:09,519 --> 00:12:12,800
you know identifiable and accountable

376
00:12:11,360 --> 00:12:16,560
way we can map

377
00:12:12,800 --> 00:12:19,680
onto these uh technological uh pieces

378
00:12:16,560 --> 00:12:21,119
and you can use the same kind of idea in

379
00:12:19,680 --> 00:12:21,920
many contexts right so it doesn't have

380
00:12:21,120 --> 00:12:24,720
to be science

381
00:12:21,920 --> 00:12:26,160
it could also be for example uh the

382
00:12:24,720 --> 00:12:27,760
scenario of a continuous integration

383
00:12:26,160 --> 00:12:28,399
system where you have a repository of

384
00:12:27,760 --> 00:12:29,839
data

385
00:12:28,399 --> 00:12:31,760
maybe that was the output of a

386
00:12:29,839 --> 00:12:33,200
scientific study right but you want to

387
00:12:31,760 --> 00:12:33,920
use it in context of continuous

388
00:12:33,200 --> 00:12:36,240
integration

389
00:12:33,920 --> 00:12:36,959
so you use it as a dependency to a

390
00:12:36,240 --> 00:12:39,839
software

391
00:12:36,959 --> 00:12:41,599
package and the the sub data set doesn't

392
00:12:39,839 --> 00:12:42,560
need to be polluted with the idea that

393
00:12:41,600 --> 00:12:44,320
it's now

394
00:12:42,560 --> 00:12:46,079
being used inside a continuous

395
00:12:44,320 --> 00:12:47,279
integration system so you can flexibly

396
00:12:46,079 --> 00:12:51,279
reuse components

397
00:12:47,279 --> 00:12:54,560
of data or code in whatever

398
00:12:51,279 --> 00:12:57,600
system you're you're trying to build and

399
00:12:54,560 --> 00:12:58,079
oops and the uh the point i want to make

400
00:12:57,600 --> 00:13:01,040
is

401
00:12:58,079 --> 00:13:02,560
that this model actually scales quite

402
00:13:01,040 --> 00:13:04,719
far so if you go to

403
00:13:02,560 --> 00:13:07,439
uh this page on github you can actually

404
00:13:04,720 --> 00:13:09,600
uh data let clone a repository

405
00:13:07,440 --> 00:13:11,120
that tracks 50 million files and 80

406
00:13:09,600 --> 00:13:13,279
terabytes of data

407
00:13:11,120 --> 00:13:14,560
where the actual sub data sets four and

408
00:13:13,279 --> 00:13:16,560
a half thousand of them

409
00:13:14,560 --> 00:13:19,279
don't even live on github nor does the

410
00:13:16,560 --> 00:13:20,239
data and for data led it just feels like

411
00:13:19,279 --> 00:13:22,240
clone this thing

412
00:13:20,240 --> 00:13:23,920
get me that file and it figures all out

413
00:13:22,240 --> 00:13:24,720
internally given you have access

414
00:13:23,920 --> 00:13:27,199
permissions

415
00:13:24,720 --> 00:13:28,160
and the readme tells you how to get that

416
00:13:27,200 --> 00:13:31,839
so it's

417
00:13:28,160 --> 00:13:35,920
it it can extend uh quite far into the

418
00:13:31,839 --> 00:13:39,360
um into the you know complexity that we

419
00:13:35,920 --> 00:13:42,880
often face in in scientific scenarios

420
00:13:39,360 --> 00:13:45,360
um another key bit that is missing in

421
00:13:42,880 --> 00:13:46,320
in most cases uh most version control

422
00:13:45,360 --> 00:13:49,120
systems

423
00:13:46,320 --> 00:13:50,560
is that we typically don't know what the

424
00:13:49,120 --> 00:13:54,079
cause for a change was

425
00:13:50,560 --> 00:13:57,680
right so we if we if we modify a file

426
00:13:54,079 --> 00:14:01,279
uh in uh in some code base then we

427
00:13:57,680 --> 00:14:03,120
have to be uh disciplined and

428
00:14:01,279 --> 00:14:04,959
amended with an appropriate commit

429
00:14:03,120 --> 00:14:06,639
message in order to you know transmit

430
00:14:04,959 --> 00:14:08,160
the message of what the cause for that

431
00:14:06,639 --> 00:14:10,800
change was to the past

432
00:14:08,160 --> 00:14:12,480
right if we run any sort of tools in

433
00:14:10,800 --> 00:14:14,399
scientific data processing

434
00:14:12,480 --> 00:14:16,160
that step is typically not captured

435
00:14:14,399 --> 00:14:17,440
right we have input data then some

436
00:14:16,160 --> 00:14:19,199
script is applied

437
00:14:17,440 --> 00:14:21,040
with some parameters to produce output

438
00:14:19,199 --> 00:14:23,439
data but not necessarily

439
00:14:21,040 --> 00:14:25,040
is the is that parameterization or how

440
00:14:23,440 --> 00:14:27,760
exactly the script was called

441
00:14:25,040 --> 00:14:29,199
uh uh recorded which is a huge problem

442
00:14:27,760 --> 00:14:31,600
for reproducibility of

443
00:14:29,199 --> 00:14:33,359
of of scientific results right because

444
00:14:31,600 --> 00:14:35,600
at the point where you want to reproduce

445
00:14:33,360 --> 00:14:36,639
the person who's done it is usually no

446
00:14:35,600 --> 00:14:38,959
longer accessible

447
00:14:36,639 --> 00:14:40,240
and everything is a big question mark so

448
00:14:38,959 --> 00:14:42,880
in datalit thank you

449
00:14:40,240 --> 00:14:44,079
in daylight we can uh we can use the

450
00:14:42,880 --> 00:14:47,920
machinery that i've just

451
00:14:44,079 --> 00:14:50,959
shown you that it can basically analyze

452
00:14:47,920 --> 00:14:53,439
and and and and commit

453
00:14:50,959 --> 00:14:54,959
arbitrarily complex trees of you know

454
00:14:53,440 --> 00:14:58,399
connected or nested

455
00:14:54,959 --> 00:15:00,319
modular units to very simply

456
00:14:58,399 --> 00:15:02,079
capture pretty much arbitrary

457
00:15:00,320 --> 00:15:04,880
modifications of any

458
00:15:02,079 --> 00:15:06,560
uh data set we only think we need to do

459
00:15:04,880 --> 00:15:06,959
and datalet provides a tool called run

460
00:15:06,560 --> 00:15:09,839
is

461
00:15:06,959 --> 00:15:11,599
we just wrap the execution of an

462
00:15:09,839 --> 00:15:13,680
arbitrary command line

463
00:15:11,600 --> 00:15:15,519
with dataled and datalite does nothing

464
00:15:13,680 --> 00:15:16,079
but checking that a working tree is

465
00:15:15,519 --> 00:15:19,440
clean

466
00:15:16,079 --> 00:15:21,599
however complex runs the tool

467
00:15:19,440 --> 00:15:24,000
checks what the modifications are and

468
00:15:21,600 --> 00:15:24,480
then makes a commit message that amends

469
00:15:24,000 --> 00:15:26,720
that

470
00:15:24,480 --> 00:15:28,079
recorded modification with information

471
00:15:26,720 --> 00:15:30,000
about what tool was run

472
00:15:28,079 --> 00:15:31,279
in which way and puts it in a commit

473
00:15:30,000 --> 00:15:34,000
message like we would do

474
00:15:31,279 --> 00:15:36,079
with the manual edit uh in a codebase

475
00:15:34,000 --> 00:15:37,519
and now i hear some of you saying yeah

476
00:15:36,079 --> 00:15:39,758
but you know if you

477
00:15:37,519 --> 00:15:41,120
can compute it on one system uh that

478
00:15:39,759 --> 00:15:42,720
doesn't mean you can compute in another

479
00:15:41,120 --> 00:15:43,519
system so lots of information is still

480
00:15:42,720 --> 00:15:45,120
missing

481
00:15:43,519 --> 00:15:47,920
but uh we all know that there's

482
00:15:45,120 --> 00:15:50,399
technology like uh containers right so

483
00:15:47,920 --> 00:15:51,360
lots of uh you know scientific

484
00:15:50,399 --> 00:15:52,720
institutions

485
00:15:51,360 --> 00:15:54,639
are capable of running things like

486
00:15:52,720 --> 00:15:56,160
singularity right a singularity

487
00:15:54,639 --> 00:15:58,160
container is just a file

488
00:15:56,160 --> 00:15:59,839
this system is built for tracking files

489
00:15:58,160 --> 00:16:01,040
of arbitrary size right so you just put

490
00:15:59,839 --> 00:16:03,120
your container in there

491
00:16:01,040 --> 00:16:04,079
it's tracked like any other piece that

492
00:16:03,120 --> 00:16:06,000
you use

493
00:16:04,079 --> 00:16:07,199
and daylight provides a specific

494
00:16:06,000 --> 00:16:10,160
extension to make use of those

495
00:16:07,199 --> 00:16:12,319
containers so you run the same execution

496
00:16:10,160 --> 00:16:13,600
and data performs the exact same thing

497
00:16:12,320 --> 00:16:16,240
make sure it's clean

498
00:16:13,600 --> 00:16:17,279
apply it record the change and then the

499
00:16:16,240 --> 00:16:18,639
commit message

500
00:16:17,279 --> 00:16:20,240
but the execution actually runs in the

501
00:16:18,639 --> 00:16:20,959
container that the data set also

502
00:16:20,240 --> 00:16:23,199
contains

503
00:16:20,959 --> 00:16:24,880
so you pretty much have achieved a

504
00:16:23,199 --> 00:16:26,399
comprehensive provenance tracking

505
00:16:24,880 --> 00:16:28,320
although you don't know still don't know

506
00:16:26,399 --> 00:16:29,839
what exactly happened inside that

507
00:16:28,320 --> 00:16:31,600
execution but you know which execution

508
00:16:29,839 --> 00:16:34,240
it was and you could do forensics

509
00:16:31,600 --> 00:16:36,399
uh later on which is much more than what

510
00:16:34,240 --> 00:16:38,880
normally is able to do

511
00:16:36,399 --> 00:16:40,000
um and the last point i want to make in

512
00:16:38,880 --> 00:16:42,160
the in in

513
00:16:40,000 --> 00:16:43,759
the remaining minutes is uh that we've

514
00:16:42,160 --> 00:16:46,319
heard about the necessity

515
00:16:43,759 --> 00:16:47,440
for metadata uh repeatedly and

516
00:16:46,320 --> 00:16:49,040
especially if we're going

517
00:16:47,440 --> 00:16:50,800
you know to heterogeneous data

518
00:16:49,040 --> 00:16:53,920
collections large data collections

519
00:16:50,800 --> 00:16:54,959
stuff that we cannot you know humanely

520
00:16:53,920 --> 00:16:57,519
process i would

521
00:16:54,959 --> 00:16:59,359
i would say it's it's an issue how we

522
00:16:57,519 --> 00:17:01,680
deal with metadata and metadata

523
00:16:59,360 --> 00:17:03,279
uh how we describe data is an active

524
00:17:01,680 --> 00:17:06,000
field of research for decades

525
00:17:03,279 --> 00:17:07,280
and has the problem of you know not

526
00:17:06,000 --> 00:17:10,079
really delivering

527
00:17:07,280 --> 00:17:10,559
the great results that justify all the

528
00:17:10,079 --> 00:17:12,559
effort

529
00:17:10,559 --> 00:17:13,678
that went into it and that's because

530
00:17:12,559 --> 00:17:15,760
these standards change

531
00:17:13,679 --> 00:17:17,039
all the time and we need to somehow

532
00:17:15,760 --> 00:17:18,400
automate these

533
00:17:17,039 --> 00:17:20,160
these issues in order to be able to

534
00:17:18,400 --> 00:17:22,959
track uh the

535
00:17:20,160 --> 00:17:24,319
developments that are done on the

536
00:17:22,959 --> 00:17:27,679
metadata description

537
00:17:24,319 --> 00:17:30,399
uh research and apply them to the

538
00:17:27,679 --> 00:17:31,760
to the vast amount of data that uh that

539
00:17:30,400 --> 00:17:34,240
is being processed and

540
00:17:31,760 --> 00:17:35,440
and generated so in datalit just to

541
00:17:34,240 --> 00:17:38,960
register the thought

542
00:17:35,440 --> 00:17:42,000
there is this idea that metadata

543
00:17:38,960 --> 00:17:44,320
is programmatically extracted from data

544
00:17:42,000 --> 00:17:46,160
and multiple representations of the same

545
00:17:44,320 --> 00:17:47,520
information can live in parallel next to

546
00:17:46,160 --> 00:17:49,440
each other so we can

547
00:17:47,520 --> 00:17:52,080
use a metadata standard that exists

548
00:17:49,440 --> 00:17:53,919
today and transition to a new one

549
00:17:52,080 --> 00:17:55,600
once it's available and have that

550
00:17:53,919 --> 00:17:56,240
transition be machine aided so it's

551
00:17:55,600 --> 00:17:59,280
quickly

552
00:17:56,240 --> 00:18:01,840
it can be done with justifiable

553
00:17:59,280 --> 00:18:03,678
effort and data's purpose here is not to

554
00:18:01,840 --> 00:18:06,159
be a comprehensive description engine

555
00:18:03,679 --> 00:18:07,120
the description is the duty of the

556
00:18:06,160 --> 00:18:09,440
specific

557
00:18:07,120 --> 00:18:10,719
domains of application that know how

558
00:18:09,440 --> 00:18:12,720
data needs to be described

559
00:18:10,720 --> 00:18:14,400
the purpose of datalite here is that you

560
00:18:12,720 --> 00:18:16,320
can provide a little script

561
00:18:14,400 --> 00:18:18,160
that tells data that here is the

562
00:18:16,320 --> 00:18:20,240
metadata that i

563
00:18:18,160 --> 00:18:21,360
you know learned about this data set and

564
00:18:20,240 --> 00:18:24,799
it takes care of

565
00:18:21,360 --> 00:18:26,879
uh handling it of storing it in a in

566
00:18:24,799 --> 00:18:28,240
a standardized format also being able to

567
00:18:26,880 --> 00:18:30,400
detach it from a data set

568
00:18:28,240 --> 00:18:31,840
in order to be able to put it into

569
00:18:30,400 --> 00:18:34,559
databases to enable

570
00:18:31,840 --> 00:18:35,199
uh queries and search and and and and so

571
00:18:34,559 --> 00:18:39,120
on

572
00:18:35,200 --> 00:18:41,280
so to summarize dayled is a tool

573
00:18:39,120 --> 00:18:42,639
that makes the combination of kit and

574
00:18:41,280 --> 00:18:43,360
gitanox for decentralized data

575
00:18:42,640 --> 00:18:45,520
management

576
00:18:43,360 --> 00:18:46,399
much more convenient and simpler for

577
00:18:45,520 --> 00:18:48,720
many

578
00:18:46,400 --> 00:18:50,000
scientific workflows where we're not

579
00:18:48,720 --> 00:18:52,080
necessarily being interested

580
00:18:50,000 --> 00:18:53,039
in making every scientist a software

581
00:18:52,080 --> 00:18:56,480
developer

582
00:18:53,039 --> 00:18:58,640
and cannot afford it and uh it has

583
00:18:56,480 --> 00:18:59,679
it aids uh providence capture and

584
00:18:58,640 --> 00:19:02,559
discoverability

585
00:18:59,679 --> 00:19:04,000
through uh support of metadata and there

586
00:19:02,559 --> 00:19:06,799
is it can do much more

587
00:19:04,000 --> 00:19:08,559
than i was able to uh to present today

588
00:19:06,799 --> 00:19:10,879
and if you want to find out more

589
00:19:08,559 --> 00:19:11,600
you can go to handbook.datalet.org and

590
00:19:10,880 --> 00:19:13,600
get a

591
00:19:11,600 --> 00:19:16,080
fairly comprehensive view of what it can

592
00:19:13,600 --> 00:19:17,840
do both from the basics of you know why

593
00:19:16,080 --> 00:19:20,399
is version control important for

594
00:19:17,840 --> 00:19:22,320
for science and for other applications

595
00:19:20,400 --> 00:19:25,520
but also for specialized

596
00:19:22,320 --> 00:19:27,678
use cases that shed light on

597
00:19:25,520 --> 00:19:29,679
problems like how to construct a

598
00:19:27,679 --> 00:19:32,400
scalable data store or how to

599
00:19:29,679 --> 00:19:33,039
write a reproducible paper for for signs

600
00:19:32,400 --> 00:19:35,360
using

601
00:19:33,039 --> 00:19:37,360
this type of tooling and with that uh i

602
00:19:35,360 --> 00:19:39,918
thank you and we're also hiring

603
00:19:37,360 --> 00:19:42,080
not just in not just in germany but also

604
00:19:39,919 --> 00:19:53,840
in the us so whatever you prefer

605
00:19:42,080 --> 00:19:53,840
get in touch please thank you

606
00:20:03,039 --> 00:20:06,960
that was not a question but i would like

607
00:20:04,400 --> 00:20:09,280
to repeat that

608
00:20:06,960 --> 00:20:11,120
he has talked to uh to the core devs for

609
00:20:09,280 --> 00:20:13,360
for years but until the talk here he

610
00:20:11,120 --> 00:20:13,360
never

611
00:20:19,280 --> 00:20:22,320
uh two understood of provenance and one

612
00:20:21,280 --> 00:20:25,520
i two questions

613
00:20:22,320 --> 00:20:26,080
um do you support the w3c prop json

614
00:20:25,520 --> 00:20:28,158
format

615
00:20:26,080 --> 00:20:30,879
and do you support something like

616
00:20:28,159 --> 00:20:34,080
research object crate or any sort of

617
00:20:30,880 --> 00:20:34,080
uh standard to

618
00:20:36,320 --> 00:20:39,360
right so so question one was whether we

619
00:20:38,880 --> 00:20:42,799
are

620
00:20:39,360 --> 00:20:43,840
uh employing wc3 uh prof standard for

621
00:20:42,799 --> 00:20:47,600
providence capture

622
00:20:43,840 --> 00:20:50,080
and uh the the two stage answer

623
00:20:47,600 --> 00:20:50,799
one is datalite doesn't care what we

624
00:20:50,080 --> 00:20:53,280
support

625
00:20:50,799 --> 00:20:53,918
it just accepts you know a structured

626
00:20:53,280 --> 00:20:55,678
report

627
00:20:53,919 --> 00:20:57,039
of whatever structure and if that's

628
00:20:55,679 --> 00:20:59,200
crappy then

629
00:20:57,039 --> 00:21:01,520
it will be crappy but it will be managed

630
00:20:59,200 --> 00:21:04,640
if it's good like wc3 prof

631
00:21:01,520 --> 00:21:07,760
which datalite's own metadata extractors

632
00:21:04,640 --> 00:21:08,559
internally use yes then it will be more

633
00:21:07,760 --> 00:21:10,799
usable

634
00:21:08,559 --> 00:21:12,960
but there is no requirement that the

635
00:21:10,799 --> 00:21:15,360
metadata your first attempt at

636
00:21:12,960 --> 00:21:16,240
metadata needs to be perfect or

637
00:21:15,360 --> 00:21:18,959
standardized

638
00:21:16,240 --> 00:21:19,919
or signed off by some entity you can do

639
00:21:18,960 --> 00:21:22,480
whatever you want

640
00:21:19,919 --> 00:21:23,600
if you think that's useful and the

641
00:21:22,480 --> 00:21:26,320
second question was

642
00:21:23,600 --> 00:21:27,840
whether we push containers using some

643
00:21:26,320 --> 00:21:32,480
standardized mechanism

644
00:21:27,840 --> 00:21:32,480
to some kind of archive or catalog

645
00:21:33,120 --> 00:21:37,120
yeah there's there's there's nothing in

646
00:21:35,360 --> 00:21:39,600
dataled that does that but

647
00:21:37,120 --> 00:21:40,320
but for us again data knows files and

648
00:21:39,600 --> 00:21:43,360
data sets

649
00:21:40,320 --> 00:21:45,280
so container as a file uh it you can

650
00:21:43,360 --> 00:21:47,120
data that has an extension mechanism

651
00:21:45,280 --> 00:21:48,879
where it would be very simple there's a

652
00:21:47,120 --> 00:21:49,520
template you can clone from from github

653
00:21:48,880 --> 00:21:51,919
where you could

654
00:21:49,520 --> 00:21:53,760
you could add sub commands to daylight

655
00:21:51,919 --> 00:21:56,240
that for example would say okay

656
00:21:53,760 --> 00:21:58,000
list me all the containers uh that it

657
00:21:56,240 --> 00:22:00,080
knows in this data set and then

658
00:21:58,000 --> 00:22:01,679
you know give them a you know unique

659
00:22:00,080 --> 00:22:03,039
identifier and push them somewhere

660
00:22:01,679 --> 00:22:05,919
but there's no built-in support in

661
00:22:03,039 --> 00:22:05,919
datalite core for that

662
00:22:06,400 --> 00:22:10,400
this is a very fundamental question the

663
00:22:08,799 --> 00:22:13,600
computer scientists i work with

664
00:22:10,400 --> 00:22:17,039
are very reluctant to put data

665
00:22:13,600 --> 00:22:20,559
under git because

666
00:22:17,039 --> 00:22:22,080
source code version control is based on

667
00:22:20,559 --> 00:22:25,360
graph theory

668
00:22:22,080 --> 00:22:28,080
and revision histories

669
00:22:25,360 --> 00:22:30,639
for data would be better supported by

670
00:22:28,080 --> 00:22:30,639
category

671
00:22:32,880 --> 00:22:40,799
for actually doing proper control

672
00:22:37,120 --> 00:22:41,600
right how so the the question was

673
00:22:40,799 --> 00:22:44,559
whether

674
00:22:41,600 --> 00:22:44,959
uh i paraphrase it uh if you excuse me

675
00:22:44,559 --> 00:22:47,120
the

676
00:22:44,960 --> 00:22:49,039
the question was whether git's way of

677
00:22:47,120 --> 00:22:52,158
identifying information

678
00:22:49,039 --> 00:22:54,320
is sufficient for scientific data and

679
00:22:52,159 --> 00:22:56,320
whether we should switch to something

680
00:22:54,320 --> 00:22:57,600
that is based on a different theory is

681
00:22:56,320 --> 00:23:00,480
that fair and

682
00:22:57,600 --> 00:23:01,678
and so the um i can tell you i've never

683
00:23:00,480 --> 00:23:04,720
thought about that

684
00:23:01,679 --> 00:23:05,840
because uh the this whole uh project is

685
00:23:04,720 --> 00:23:08,080
about practicality

686
00:23:05,840 --> 00:23:09,678
right so we are sitting on top of git

687
00:23:08,080 --> 00:23:10,639
not because we like its theoretical

688
00:23:09,679 --> 00:23:12,320
implications

689
00:23:10,640 --> 00:23:15,440
but because it's a widely adopted

690
00:23:12,320 --> 00:23:18,240
platform that we still have to discover

691
00:23:15,440 --> 00:23:19,440
the the limitations for the work that

692
00:23:18,240 --> 00:23:21,440
that we've been doing

693
00:23:19,440 --> 00:23:22,720
right and likewise for guidance right it

694
00:23:21,440 --> 00:23:24,400
interfaces with the storage

695
00:23:22,720 --> 00:23:24,960
infrastructure that the planet has right

696
00:23:24,400 --> 00:23:27,520
now

697
00:23:24,960 --> 00:23:29,360
right so so that's why we're doing it uh

698
00:23:27,520 --> 00:23:30,960
if this turns out to be a problem

699
00:23:29,360 --> 00:23:32,639
i'm sure it turns out to be a problem

700
00:23:30,960 --> 00:23:34,240
not within the scope of dataled

701
00:23:32,640 --> 00:23:35,840
but within the scope of you know

702
00:23:34,240 --> 00:23:36,960
tracking information or identifying

703
00:23:35,840 --> 00:23:38,320
information in general

704
00:23:36,960 --> 00:23:40,080
and then i'm sure we'll jump on the

705
00:23:38,320 --> 00:23:44,559
train wherever it goes

706
00:23:40,080 --> 00:23:44,559
when it goes thank you

707
00:23:46,320 --> 00:23:51,360
i know i have uh

708
00:23:50,000 --> 00:23:53,760
had them in the past the problem of

709
00:23:51,360 --> 00:23:54,799
sharing really big data sets like in the

710
00:23:53,760 --> 00:23:58,879
terabytes

711
00:23:54,799 --> 00:24:01,039
and i didn't use it because i i

712
00:23:58,880 --> 00:24:02,159
couldn't manage it and i discovered this

713
00:24:01,039 --> 00:24:05,200
project which is called

714
00:24:02,159 --> 00:24:08,000
that and he is

715
00:24:05,200 --> 00:24:08,799
it is like the main idea is kind of

716
00:24:08,000 --> 00:24:11,919
similar

717
00:24:08,799 --> 00:24:14,320
because you can uh version a data set

718
00:24:11,919 --> 00:24:14,960
in a very small way to give and the nice

719
00:24:14,320 --> 00:24:17,439
thing is

720
00:24:14,960 --> 00:24:19,279
um that the file gets distributed and

721
00:24:17,440 --> 00:24:20,559
appears to peer fraction yes so my

722
00:24:19,279 --> 00:24:24,159
question was

723
00:24:20,559 --> 00:24:26,000
how does that manage the transfer

724
00:24:24,159 --> 00:24:28,720
of the file when i chrome the 80

725
00:24:26,000 --> 00:24:31,279
terabyte exposes if i have the space

726
00:24:28,720 --> 00:24:32,080
put it somewhere how does it work yeah

727
00:24:31,279 --> 00:24:35,520
transfer

728
00:24:32,080 --> 00:24:37,760
and can you comment uh on the

729
00:24:35,520 --> 00:24:38,799
this update that project is that the

730
00:24:37,760 --> 00:24:41,520
technology that

731
00:24:38,799 --> 00:24:42,320
you could integrate in that lab right so

732
00:24:41,520 --> 00:24:45,200
um

733
00:24:42,320 --> 00:24:46,080
in in data like we don't even we don't

734
00:24:45,200 --> 00:24:49,120
make that decision

735
00:24:46,080 --> 00:24:51,678
right in datalit it's gitanex that

736
00:24:49,120 --> 00:24:53,120
is responsible for for data transport so

737
00:24:51,679 --> 00:24:55,279
guidance for example

738
00:24:53,120 --> 00:24:57,199
can use torrents for data transport you

739
00:24:55,279 --> 00:24:59,039
can use ipfs for data transport

740
00:24:57,200 --> 00:25:00,799
it can use all kinds of stuff right if

741
00:24:59,039 --> 00:25:04,240
your data is on amazon's

742
00:25:00,799 --> 00:25:06,080
big hard drive you don't care right

743
00:25:04,240 --> 00:25:08,720
and it can use that too right so we

744
00:25:06,080 --> 00:25:10,960
we're not you're not making a decision

745
00:25:08,720 --> 00:25:12,559
for a particular single technology

746
00:25:10,960 --> 00:25:14,000
because in our experience

747
00:25:12,559 --> 00:25:16,000
there is no single technology that

748
00:25:14,000 --> 00:25:18,559
serves all the use cases right

749
00:25:16,000 --> 00:25:19,120
and and that's why that is gitanex's

750
00:25:18,559 --> 00:25:21,678
domain

751
00:25:19,120 --> 00:25:23,520
and it does that really well so in in

752
00:25:21,679 --> 00:25:23,919
that is they make different choices and

753
00:25:23,520 --> 00:25:26,240
they

754
00:25:23,919 --> 00:25:27,440
they have a more you know stringent you

755
00:25:26,240 --> 00:25:29,840
know set of technologies

756
00:25:27,440 --> 00:25:30,880
that they choose for reasons that are

757
00:25:29,840 --> 00:25:34,320
valid and

758
00:25:30,880 --> 00:25:37,679
and there's no it's not that you know

759
00:25:34,320 --> 00:25:38,799
we we can you can do similar things in

760
00:25:37,679 --> 00:25:41,760
daylight plus it's

761
00:25:38,799 --> 00:25:42,320
uh it's it's foundation there's no no

762
00:25:41,760 --> 00:25:46,158
you know

763
00:25:42,320 --> 00:25:46,158
direct conflict of any sorts

764
00:25:47,679 --> 00:25:57,840
thank you

765
00:25:58,320 --> 00:26:00,399
you

