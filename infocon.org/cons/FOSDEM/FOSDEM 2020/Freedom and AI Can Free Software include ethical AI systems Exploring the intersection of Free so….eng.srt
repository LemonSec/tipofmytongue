1
00:00:05,200 --> 00:00:08,720
more or less

2
00:00:06,080 --> 00:00:09,679
our movement here about free software

3
00:00:08,720 --> 00:00:11,360
and

4
00:00:09,679 --> 00:00:12,960
the community behind the free software

5
00:00:11,360 --> 00:00:16,160
and the ethics and and

6
00:00:12,960 --> 00:00:19,039
what we have created here

7
00:00:16,160 --> 00:00:20,560
but this will then be shifted to the

8
00:00:19,039 --> 00:00:23,600
more or less upcoming ai

9
00:00:20,560 --> 00:00:25,680
data mining whatever world

10
00:00:23,600 --> 00:00:27,199
and i think it's a very fascinating

11
00:00:25,680 --> 00:00:30,880
topic how to

12
00:00:27,199 --> 00:00:32,719
keep our ethics our themes uh

13
00:00:30,880 --> 00:00:34,160
alive in this world because it's very

14
00:00:32,719 --> 00:00:36,960
very necessary

15
00:00:34,160 --> 00:00:38,078
we have all these um google and other

16
00:00:36,960 --> 00:00:40,719
big companies uh

17
00:00:38,079 --> 00:00:41,840
yeah trying to to get the hold on our

18
00:00:40,719 --> 00:00:43,600
data stuff

19
00:00:41,840 --> 00:00:45,360
so this will be presented by justin

20
00:00:43,600 --> 00:00:48,239
flory and mike nolan

21
00:00:45,360 --> 00:00:48,640
they are from our mike is from rochester

22
00:00:48,239 --> 00:00:51,839
or

23
00:00:48,640 --> 00:00:53,440
florida okay sorry and they're from the

24
00:00:51,840 --> 00:00:55,840
institute of technology

25
00:00:53,440 --> 00:00:56,879
libra corps and unicef office of

26
00:00:55,840 --> 00:00:59,520
innovation

27
00:00:56,879 --> 00:01:00,730
please give him the big welcome and

28
00:00:59,520 --> 00:01:06,960
enjoy the talk

29
00:01:00,730 --> 00:01:10,560
[Applause]

30
00:01:06,960 --> 00:01:13,280
so in 2018 victoria krakovna

31
00:01:10,560 --> 00:01:14,799
at google's deepmind ai lab asked her

32
00:01:13,280 --> 00:01:16,320
colleagues at google

33
00:01:14,799 --> 00:01:18,159
to bring her some examples of

34
00:01:16,320 --> 00:01:21,439
misbehaving ai

35
00:01:18,159 --> 00:01:23,360
so how does ai never lose a game of

36
00:01:21,439 --> 00:01:25,919
tetris

37
00:01:23,360 --> 00:01:28,159
you pause the game how does a

38
00:01:25,920 --> 00:01:31,040
self-driving car that's supposed to keep

39
00:01:28,159 --> 00:01:33,439
itself face fast and safe keep itself

40
00:01:31,040 --> 00:01:36,079
fast and safe

41
00:01:33,439 --> 00:01:37,279
spins in a circle in the same spot in

42
00:01:36,079 --> 00:01:39,520
the same article

43
00:01:37,280 --> 00:01:41,280
there was an artificial life simulation

44
00:01:39,520 --> 00:01:43,280
to simulate evolution

45
00:01:41,280 --> 00:01:46,399
that ended up creating this species that

46
00:01:43,280 --> 00:01:48,240
had this sedentary lazy lifestyle

47
00:01:46,399 --> 00:01:49,920
and it would mate to produce new

48
00:01:48,240 --> 00:01:51,679
children that it would then eat for

49
00:01:49,920 --> 00:01:53,920
resources and energy

50
00:01:51,680 --> 00:01:54,720
and then made to have more edible

51
00:01:53,920 --> 00:01:57,920
children

52
00:01:54,720 --> 00:01:58,798
so some of these are a little funny or

53
00:01:57,920 --> 00:02:01,600
odd but

54
00:01:58,799 --> 00:02:03,040
this is increasingly becoming our world

55
00:02:01,600 --> 00:02:05,679
not eating children

56
00:02:03,040 --> 00:02:06,640
but major social networks are serving

57
00:02:05,680 --> 00:02:08,959
you ads for

58
00:02:06,640 --> 00:02:10,160
cat food after you post about getting a

59
00:02:08,959 --> 00:02:13,200
cat

60
00:02:10,160 --> 00:02:14,959
major u.s retailers know that you are

61
00:02:13,200 --> 00:02:16,640
pregnant before you've even told your

62
00:02:14,959 --> 00:02:19,840
family or friends

63
00:02:16,640 --> 00:02:20,879
so what happens when these stories turn

64
00:02:19,840 --> 00:02:23,280
into

65
00:02:20,879 --> 00:02:25,679
a major job matching site knows that

66
00:02:23,280 --> 00:02:28,560
you're afraid to ask for a raise

67
00:02:25,680 --> 00:02:31,200
and a major online retailer knows that

68
00:02:28,560 --> 00:02:32,560
you're an impulse buyer

69
00:02:31,200 --> 00:02:35,040
the rest of the world is starting to

70
00:02:32,560 --> 00:02:38,080
take notice too last week

71
00:02:35,040 --> 00:02:40,239
january 20th the ceo of google made a

72
00:02:38,080 --> 00:02:42,319
call for regulation for the governments

73
00:02:40,239 --> 00:02:45,680
of the u.s and europe to start

74
00:02:42,319 --> 00:02:47,599
coming up with ways to regulate ai

75
00:02:45,680 --> 00:02:48,800
so this brings us to the perfect

76
00:02:47,599 --> 00:02:50,079
opportunity to talk about good

77
00:02:48,800 --> 00:02:52,160
goodheart's law

78
00:02:50,080 --> 00:02:53,120
when a measure becomes a target it

79
00:02:52,160 --> 00:02:58,000
ceases to be

80
00:02:53,120 --> 00:03:01,519
a good measure we who work in technology

81
00:02:58,000 --> 00:03:06,080
are too often a witness to a system

82
00:03:01,519 --> 00:03:08,560
that does not serve why

83
00:03:06,080 --> 00:03:10,319
actually is a pretty easy answer and it

84
00:03:08,560 --> 00:03:13,680
explains a lot

85
00:03:10,319 --> 00:03:16,879
technology was designed with technology

86
00:03:13,680 --> 00:03:20,879
at the center not people

87
00:03:16,879 --> 00:03:24,319
which is to say it was badly designed

88
00:03:20,879 --> 00:03:25,599
i got you and so this is the problem

89
00:03:24,319 --> 00:03:28,640
that we want to talk about

90
00:03:25,599 --> 00:03:29,839
during this talk today right there is

91
00:03:28,640 --> 00:03:32,000
this increasing

92
00:03:29,840 --> 00:03:33,680
diversion between what we feel like

93
00:03:32,000 --> 00:03:37,280
technology should supposed to

94
00:03:33,680 --> 00:03:38,159
should be doing and what it's actually

95
00:03:37,280 --> 00:03:40,319
doing

96
00:03:38,159 --> 00:03:42,720
and more importantly there aren't really

97
00:03:40,319 --> 00:03:46,480
any clear actual steps that we have seen

98
00:03:42,720 --> 00:03:48,799
to mitigate the the the

99
00:03:46,480 --> 00:03:51,119
risks of this technology infringing on

100
00:03:48,799 --> 00:03:52,879
our own human rights

101
00:03:51,120 --> 00:03:55,280
and so specifically we want to break

102
00:03:52,879 --> 00:03:56,720
this down into three main things

103
00:03:55,280 --> 00:03:58,799
we want to think about what sort of

104
00:03:56,720 --> 00:04:01,120
freedoms do we actually want to protect

105
00:03:58,799 --> 00:04:03,519
specifically from ai systems

106
00:04:01,120 --> 00:04:04,560
and then we want to think about a couple

107
00:04:03,519 --> 00:04:07,360
different ways

108
00:04:04,560 --> 00:04:09,280
that we as creators of these ai systems

109
00:04:07,360 --> 00:04:11,360
can actually build in ways that to make

110
00:04:09,280 --> 00:04:13,280
sure that it's respecting these rights

111
00:04:11,360 --> 00:04:15,760
and then lastly and perhaps most

112
00:04:13,280 --> 00:04:17,759
importantly we want to think of a few

113
00:04:15,760 --> 00:04:20,238
different ways that we can organize

114
00:04:17,759 --> 00:04:23,199
and ensure that everyone's rights will

115
00:04:20,238 --> 00:04:26,400
be respected by this

116
00:04:23,199 --> 00:04:29,120
so let's start from the beginning

117
00:04:26,400 --> 00:04:31,679
what freedoms do we want to protect

118
00:04:29,120 --> 00:04:35,120
before we can look at today

119
00:04:31,680 --> 00:04:39,040
let's go back to 1983.

120
00:04:35,120 --> 00:04:40,880
so 1983 september the gnu project was

121
00:04:39,040 --> 00:04:42,800
founded by richard stallman

122
00:04:40,880 --> 00:04:45,199
it was a software project but it was

123
00:04:42,800 --> 00:04:47,199
also it was more than just software

124
00:04:45,199 --> 00:04:49,280
it came it came with a set of goals and

125
00:04:47,199 --> 00:04:52,000
a vision to give computer users

126
00:04:49,280 --> 00:04:54,840
freedom and control in their use of

127
00:04:52,000 --> 00:04:57,759
computers and computing devices

128
00:04:54,840 --> 00:04:59,599
how it would be done by collaboratively

129
00:04:57,759 --> 00:05:00,000
developing and publishing software that

130
00:04:59,600 --> 00:05:02,400
gives

131
00:05:00,000 --> 00:05:03,440
everyone the rights to freely run it any

132
00:05:02,400 --> 00:05:05,520
way you want

133
00:05:03,440 --> 00:05:06,719
to copy and distribute it share it with

134
00:05:05,520 --> 00:05:09,280
other people

135
00:05:06,720 --> 00:05:10,479
to study it and see how it works and to

136
00:05:09,280 --> 00:05:11,919
modify it

137
00:05:10,479 --> 00:05:14,479
many of you probably know these as the

138
00:05:11,919 --> 00:05:17,359
four freedoms of free software

139
00:05:14,479 --> 00:05:17,680
so the gnu project was always more than

140
00:05:17,360 --> 00:05:20,320
just

141
00:05:17,680 --> 00:05:23,440
software it came with a set of values

142
00:05:20,320 --> 00:05:26,400
and ethics that the project believed in

143
00:05:23,440 --> 00:05:28,800
copyleft becomes this copyright hack to

144
00:05:26,400 --> 00:05:31,359
protect these essential rights

145
00:05:28,800 --> 00:05:33,639
but who will actually protect and

146
00:05:31,360 --> 00:05:36,639
enforce these rights

147
00:05:33,639 --> 00:05:39,039
1985 october the free software

148
00:05:36,639 --> 00:05:41,759
foundation is is created to support and

149
00:05:39,039 --> 00:05:43,440
sustain the free software movement

150
00:05:41,759 --> 00:05:45,120
the values of the gnu project were

151
00:05:43,440 --> 00:05:47,120
important and valuable

152
00:05:45,120 --> 00:05:48,240
but it wasn't enough to just leave them

153
00:05:47,120 --> 00:05:51,440
on their own

154
00:05:48,240 --> 00:05:53,199
at first the fsf focused on employing

155
00:05:51,440 --> 00:05:53,919
software developers to work on the

156
00:05:53,199 --> 00:05:56,240
software

157
00:05:53,919 --> 00:05:57,758
for the gnu project and free software

158
00:05:56,240 --> 00:05:59,919
later they transition

159
00:05:57,759 --> 00:06:02,479
to legal and structural issues to

160
00:05:59,919 --> 00:06:04,719
support the free software community

161
00:06:02,479 --> 00:06:06,159
so it's one thing to have your values

162
00:06:04,720 --> 00:06:08,000
and ethics out there

163
00:06:06,160 --> 00:06:10,560
but they need to be protected and

164
00:06:08,000 --> 00:06:12,400
respected by the rest of the world

165
00:06:10,560 --> 00:06:14,319
so the free software foundation

166
00:06:12,400 --> 00:06:16,560
represents the sustainability

167
00:06:14,319 --> 00:06:18,639
of protecting these rights and beliefs

168
00:06:16,560 --> 00:06:19,759
that have been put forth by the gnu

169
00:06:18,639 --> 00:06:22,080
project

170
00:06:19,759 --> 00:06:22,800
so this helps to sustain those rights

171
00:06:22,080 --> 00:06:25,120
but

172
00:06:22,800 --> 00:06:27,360
how does a foundation like the fsf

173
00:06:25,120 --> 00:06:29,199
actually enforce these rights

174
00:06:27,360 --> 00:06:33,199
four years later we see the first

175
00:06:29,199 --> 00:06:35,840
version of the gpl license in 1989

176
00:06:33,199 --> 00:06:37,680
for the gnu project stewarded by the

177
00:06:35,840 --> 00:06:40,960
free software foundation

178
00:06:37,680 --> 00:06:43,520
the gpl was unique because it put power

179
00:06:40,960 --> 00:06:44,719
in the hands of people and activists

180
00:06:43,520 --> 00:06:47,280
like us

181
00:06:44,720 --> 00:06:49,280
about how we decide the rules of how

182
00:06:47,280 --> 00:06:52,960
people use our software

183
00:06:49,280 --> 00:06:54,880
so copyleft is now put into legal policy

184
00:06:52,960 --> 00:06:56,000
and it's the way that software

185
00:06:54,880 --> 00:06:58,080
developers

186
00:06:56,000 --> 00:07:00,960
place the free software values at the

187
00:06:58,080 --> 00:07:03,680
core of their code and their projects

188
00:07:00,960 --> 00:07:04,080
so although the enforcement of copyleft

189
00:07:03,680 --> 00:07:06,960
has

190
00:07:04,080 --> 00:07:07,758
its fair share of issues this was still

191
00:07:06,960 --> 00:07:10,318
the teeth

192
00:07:07,759 --> 00:07:11,919
in actually being able to enforce these

193
00:07:10,319 --> 00:07:15,039
values and beliefs that

194
00:07:11,919 --> 00:07:16,400
were put forth previously and copyright

195
00:07:15,039 --> 00:07:18,800
wasn't something that we were really

196
00:07:16,400 --> 00:07:20,239
thinking about before the 1980s when we

197
00:07:18,800 --> 00:07:22,880
shared software

198
00:07:20,240 --> 00:07:25,280
so in that sense copyleft becomes this

199
00:07:22,880 --> 00:07:27,680
radical invention and software with the

200
00:07:25,280 --> 00:07:30,400
proliferation of the gpl

201
00:07:27,680 --> 00:07:33,360
especially two years later in prominent

202
00:07:30,400 --> 00:07:36,000
projects like the linux kernel

203
00:07:33,360 --> 00:07:37,840
okay so is the past actually relevant

204
00:07:36,000 --> 00:07:40,319
today

205
00:07:37,840 --> 00:07:42,560
let's reflect on how the free software

206
00:07:40,319 --> 00:07:43,680
movement responded to these societal

207
00:07:42,560 --> 00:07:46,479
issues

208
00:07:43,680 --> 00:07:48,319
so free software was a response to the

209
00:07:46,479 --> 00:07:51,520
changing ecosystem

210
00:07:48,319 --> 00:07:53,520
of computing in the 1970s software

211
00:07:51,520 --> 00:07:55,440
became more valued because there was a

212
00:07:53,520 --> 00:07:58,318
standardization on hardware

213
00:07:55,440 --> 00:07:59,759
so often there were so many different

214
00:07:58,319 --> 00:08:01,199
architectures to work with so the

215
00:07:59,759 --> 00:08:02,400
software you're building well you'd have

216
00:08:01,199 --> 00:08:04,000
to do it on all these different

217
00:08:02,400 --> 00:08:05,840
architectures if people actually wanted

218
00:08:04,000 --> 00:08:08,960
to use it practically

219
00:08:05,840 --> 00:08:11,119
so now there became fewer architectures

220
00:08:08,960 --> 00:08:14,560
we started to standardize on things like

221
00:08:11,120 --> 00:08:18,960
86 64 and 32-bit architectures

222
00:08:14,560 --> 00:08:22,319
so now software's value had increased

223
00:08:18,960 --> 00:08:24,560
it became a commodity the four freedoms

224
00:08:22,319 --> 00:08:26,879
were de facto the nature of how

225
00:08:24,560 --> 00:08:28,240
software was distributed and shared

226
00:08:26,879 --> 00:08:30,400
before that time

227
00:08:28,240 --> 00:08:31,680
but after commodification this was no

228
00:08:30,400 --> 00:08:33,360
longer true

229
00:08:31,680 --> 00:08:35,440
the four freedoms were rooted in a

230
00:08:33,360 --> 00:08:36,479
belief that there are essential rights

231
00:08:35,440 --> 00:08:38,560
that belong to

232
00:08:36,479 --> 00:08:39,599
all users of computers and computer

233
00:08:38,559 --> 00:08:41,838
systems

234
00:08:39,599 --> 00:08:45,279
stallman observed this change at the mit

235
00:08:41,839 --> 00:08:48,240
media labs in the 1970s and early 1980s

236
00:08:45,279 --> 00:08:50,399
which motivated him and also many others

237
00:08:48,240 --> 00:08:52,320
to stand up for software freedom by

238
00:08:50,399 --> 00:08:55,200
asserting these rights

239
00:08:52,320 --> 00:08:57,279
so to respond to this commodification

240
00:08:55,200 --> 00:08:58,000
free software took a freedom-based

241
00:08:57,279 --> 00:09:00,399
approach

242
00:08:58,000 --> 00:09:02,880
to establishing the four freedoms across

243
00:09:00,399 --> 00:09:06,399
the 1980s that we just looked at

244
00:09:02,880 --> 00:09:08,959
so looking back almost 40 years ago

245
00:09:06,399 --> 00:09:11,120
is it possible for us to extend and make

246
00:09:08,959 --> 00:09:14,399
the past relevant again today

247
00:09:11,120 --> 00:09:19,680
in our ever-changing world

248
00:09:14,399 --> 00:09:22,000
well first how has the world changed

249
00:09:19,680 --> 00:09:24,160
so the history of free software overlaps

250
00:09:22,000 --> 00:09:26,240
with what is happening right now

251
00:09:24,160 --> 00:09:28,399
we're combining software and data to

252
00:09:26,240 --> 00:09:30,560
determine literal human futures

253
00:09:28,399 --> 00:09:31,519
what you and i are going to do next what

254
00:09:30,560 --> 00:09:34,560
we want to buy

255
00:09:31,519 --> 00:09:35,760
next what our next way to spend our

256
00:09:34,560 --> 00:09:38,239
money will be

257
00:09:35,760 --> 00:09:39,120
so another way to put it what are human

258
00:09:38,240 --> 00:09:43,040
futures

259
00:09:39,120 --> 00:09:46,720
the combination of data and software

260
00:09:43,040 --> 00:09:48,480
ai in a sense so how are human futures

261
00:09:46,720 --> 00:09:50,320
becoming a commodity

262
00:09:48,480 --> 00:09:51,839
you know before software was this thing

263
00:09:50,320 --> 00:09:55,040
that we we sold

264
00:09:51,839 --> 00:09:57,120
it had inherent value behind it and

265
00:09:55,040 --> 00:09:58,160
now that our futures are becoming this

266
00:09:57,120 --> 00:10:00,240
new commodity

267
00:09:58,160 --> 00:10:02,240
it comes from that ability to predict

268
00:10:00,240 --> 00:10:04,320
what you are going to do next and that's

269
00:10:02,240 --> 00:10:05,920
what drives so much of the profit behind

270
00:10:04,320 --> 00:10:07,760
these business models

271
00:10:05,920 --> 00:10:09,279
but we have to remember that data is

272
00:10:07,760 --> 00:10:11,439
only one piece

273
00:10:09,279 --> 00:10:13,360
of this really big puzzle it is the

274
00:10:11,440 --> 00:10:14,560
enabling force to determining your

275
00:10:13,360 --> 00:10:16,480
future

276
00:10:14,560 --> 00:10:18,160
so in today's world we have these

277
00:10:16,480 --> 00:10:19,839
third-party organizations that are

278
00:10:18,160 --> 00:10:21,839
increasingly collecting data on a

279
00:10:19,839 --> 00:10:24,000
massive centralized scale

280
00:10:21,839 --> 00:10:25,519
and your data is what enables companies

281
00:10:24,000 --> 00:10:27,279
to sell your future

282
00:10:25,519 --> 00:10:29,440
so one way to think about it is data

283
00:10:27,279 --> 00:10:32,399
isn't gold but it's more like

284
00:10:29,440 --> 00:10:34,800
oil you consume the data and you sell

285
00:10:32,399 --> 00:10:38,399
the output

286
00:10:34,800 --> 00:10:41,359
so mike where are we today well

287
00:10:38,399 --> 00:10:43,040
you know we noticed that data is a huge

288
00:10:41,360 --> 00:10:44,959
factor in this new equation of

289
00:10:43,040 --> 00:10:46,480
these issues that we're realizing all

290
00:10:44,959 --> 00:10:47,279
around us and we've responded

291
00:10:46,480 --> 00:10:49,040
accordingly

292
00:10:47,279 --> 00:10:50,720
i'm sure many of you here in this room

293
00:10:49,040 --> 00:10:51,519
are very familiar with the privacy

294
00:10:50,720 --> 00:10:54,480
movement

295
00:10:51,519 --> 00:10:55,600
and what all many organizations are

296
00:10:54,480 --> 00:10:57,760
doing to try to

297
00:10:55,600 --> 00:10:59,680
allow us to take hold of our data and

298
00:10:57,760 --> 00:11:00,560
take back control of what happens with

299
00:10:59,680 --> 00:11:04,079
it

300
00:11:00,560 --> 00:11:05,279
even within our regulatory bodies gdpr

301
00:11:04,079 --> 00:11:07,199
here in the eu

302
00:11:05,279 --> 00:11:09,680
has been a massive movement in trying to

303
00:11:07,200 --> 00:11:11,600
enable its citizens to be able to take

304
00:11:09,680 --> 00:11:13,680
back that power as well

305
00:11:11,600 --> 00:11:15,440
and it's natural and acceptable to want

306
00:11:13,680 --> 00:11:17,439
to protect data privacy

307
00:11:15,440 --> 00:11:19,360
but privacy is only a single part of

308
00:11:17,440 --> 00:11:21,279
this greater equation of buying and

309
00:11:19,360 --> 00:11:24,480
selling human futures

310
00:11:21,279 --> 00:11:26,560
and more so what the current data

311
00:11:24,480 --> 00:11:28,240
data privacy movement has focused on is

312
00:11:26,560 --> 00:11:30,079
empowering individuals

313
00:11:28,240 --> 00:11:31,760
but this doesn't necessarily protect us

314
00:11:30,079 --> 00:11:33,599
from societal effects

315
00:11:31,760 --> 00:11:35,279
i'm sure maybe a few of you here have

316
00:11:33,600 --> 00:11:37,680
heard about predicting

317
00:11:35,279 --> 00:11:39,680
predictive policing algorithms where

318
00:11:37,680 --> 00:11:41,519
we'll use data from

319
00:11:39,680 --> 00:11:43,760
you know crimes committed around a city

320
00:11:41,519 --> 00:11:45,120
to determine what neighborhood should

321
00:11:43,760 --> 00:11:46,800
our police patrol

322
00:11:45,120 --> 00:11:48,800
and what neighborhoods will be more

323
00:11:46,800 --> 00:11:50,959
likely to have arrests happen in them

324
00:11:48,800 --> 00:11:52,319
even our court systems are now using

325
00:11:50,959 --> 00:11:53,839
data to determine

326
00:11:52,320 --> 00:11:56,000
what sort of punishment a person should

327
00:11:53,839 --> 00:11:58,320
get based upon past crimes committed by

328
00:11:56,000 --> 00:12:00,639
other people so this is beyond just our

329
00:11:58,320 --> 00:12:02,560
personal data in our personal agency

330
00:12:00,639 --> 00:12:04,320
there are parts of our society that are

331
00:12:02,560 --> 00:12:05,199
being that are employing the use of

332
00:12:04,320 --> 00:12:08,320
other data

333
00:12:05,200 --> 00:12:10,000
beyond ours which will affect us

334
00:12:08,320 --> 00:12:11,440
and so while the data privacy movement

335
00:12:10,000 --> 00:12:13,279
has been a key factor

336
00:12:11,440 --> 00:12:14,720
in helping combat the effects of

337
00:12:13,279 --> 00:12:16,959
surveillance capitalism

338
00:12:14,720 --> 00:12:18,560
we've noticed that there's still gaps

339
00:12:16,959 --> 00:12:19,040
and we notice that they we need to

340
00:12:18,560 --> 00:12:21,439
approach

341
00:12:19,040 --> 00:12:23,120
ai not just in pieces but rather as a

342
00:12:21,440 --> 00:12:27,120
whole

343
00:12:23,120 --> 00:12:29,279
and so we have um we

344
00:12:27,120 --> 00:12:31,600
some organizations recognize this

345
00:12:29,279 --> 00:12:34,000
challenge and have started to address it

346
00:12:31,600 --> 00:12:36,000
uh the popularity of working groups such

347
00:12:34,000 --> 00:12:37,519
as the ai now institute or the

348
00:12:36,000 --> 00:12:40,320
partnership on ai

349
00:12:37,519 --> 00:12:42,079
have begun working on this as of a few

350
00:12:40,320 --> 00:12:44,240
years ago

351
00:12:42,079 --> 00:12:45,680
but these what we've seen is that these

352
00:12:44,240 --> 00:12:47,760
groups are not showing to be very

353
00:12:45,680 --> 00:12:50,000
effective on moving forward with ways of

354
00:12:47,760 --> 00:12:52,160
ensuring that people are productively

355
00:12:50,000 --> 00:12:54,880
effectively protected from ai

356
00:12:52,160 --> 00:12:56,160
systems they write reports with many

357
00:12:54,880 --> 00:12:58,480
suggestions but

358
00:12:56,160 --> 00:13:00,240
there is a big emphasis on this whole

359
00:12:58,480 --> 00:13:02,560
light self-regulation

360
00:13:00,240 --> 00:13:03,839
we'll give you a few tips but we aren't

361
00:13:02,560 --> 00:13:05,599
here to make sure that you follow

362
00:13:03,839 --> 00:13:08,880
through with them

363
00:13:05,600 --> 00:13:10,959
we're just passing around some ideas so

364
00:13:08,880 --> 00:13:12,160
if we're in this major societal shift

365
00:13:10,959 --> 00:13:14,319
from software

366
00:13:12,160 --> 00:13:15,279
as a commodity to human futures as a

367
00:13:14,320 --> 00:13:18,399
commodity

368
00:13:15,279 --> 00:13:19,760
where do we go from here well

369
00:13:18,399 --> 00:13:21,600
the first thing that happened during the

370
00:13:19,760 --> 00:13:24,480
free software movement was

371
00:13:21,600 --> 00:13:26,480
a definition of rights or freedoms we

372
00:13:24,480 --> 00:13:28,160
noticed that we are presenting

373
00:13:26,480 --> 00:13:30,399
so now what we're doing is we're going

374
00:13:28,160 --> 00:13:32,480
to present three possible freedoms

375
00:13:30,399 --> 00:13:35,440
with regards to ai systems that we can

376
00:13:32,480 --> 00:13:37,440
think of going into this new decade

377
00:13:35,440 --> 00:13:38,959
now we are presenting the truth or the

378
00:13:37,440 --> 00:13:41,120
answer we're just

379
00:13:38,959 --> 00:13:42,719
presenting experiences from our own

380
00:13:41,120 --> 00:13:44,160
careers in the research that we have

381
00:13:42,720 --> 00:13:47,199
done up to here

382
00:13:44,160 --> 00:13:49,199
obviously we we know that we're just two

383
00:13:47,199 --> 00:13:50,079
people from two very specific

384
00:13:49,199 --> 00:13:52,880
backgrounds

385
00:13:50,079 --> 00:13:54,319
and we look forward to input from

386
00:13:52,880 --> 00:13:57,519
everyone in this group

387
00:13:54,320 --> 00:14:01,040
patches are always welcome so

388
00:13:57,519 --> 00:14:03,040
what are these freedoms well

389
00:14:01,040 --> 00:14:04,880
first we know that we're entitled to

390
00:14:03,040 --> 00:14:05,760
know and understand how decisions are

391
00:14:04,880 --> 00:14:08,000
made

392
00:14:05,760 --> 00:14:09,199
so you should have the freedom to audit

393
00:14:08,000 --> 00:14:11,120
and or understand

394
00:14:09,199 --> 00:14:12,240
how these automated decisions are made

395
00:14:11,120 --> 00:14:14,399
right

396
00:14:12,240 --> 00:14:15,680
i mean imagine a teacher gives you a bad

397
00:14:14,399 --> 00:14:17,839
grade on an essay

398
00:14:15,680 --> 00:14:20,719
right naturally you're curious to know

399
00:14:17,839 --> 00:14:22,639
why i mean i know i was

400
00:14:20,720 --> 00:14:23,760
so why wouldn't you want to understand

401
00:14:22,639 --> 00:14:26,880
why a decision made

402
00:14:23,760 --> 00:14:30,319
made how it affects you

403
00:14:26,880 --> 00:14:32,079
so really this may seem familiar because

404
00:14:30,320 --> 00:14:33,360
it really came from the freedom to study

405
00:14:32,079 --> 00:14:35,199
the source

406
00:14:33,360 --> 00:14:36,560
this is really where the freedom to

407
00:14:35,199 --> 00:14:38,560
understand how decisions

408
00:14:36,560 --> 00:14:40,160
come from like this is the source of

409
00:14:38,560 --> 00:14:42,239
that in our opinion

410
00:14:40,160 --> 00:14:43,279
so by show of hands who here knows what

411
00:14:42,240 --> 00:14:46,399
linus's law

412
00:14:43,279 --> 00:14:49,120
is so a few

413
00:14:46,399 --> 00:14:50,880
so linus's law is with enough eyeballs

414
00:14:49,120 --> 00:14:53,600
all bugs are shallow

415
00:14:50,880 --> 00:14:55,760
if you open up a code base or a system

416
00:14:53,600 --> 00:14:57,760
to enough people to look at it

417
00:14:55,760 --> 00:14:59,439
any problems with it will be more likely

418
00:14:57,760 --> 00:15:01,279
to be found

419
00:14:59,440 --> 00:15:03,279
and experts across different backgrounds

420
00:15:01,279 --> 00:15:04,720
and fields should be able to research

421
00:15:03,279 --> 00:15:08,800
and understand how these

422
00:15:04,720 --> 00:15:08,800
huge ai systems are affecting us

423
00:15:09,360 --> 00:15:13,680
and but the thing that makes this very

424
00:15:12,079 --> 00:15:14,319
different from one of the initial four

425
00:15:13,680 --> 00:15:15,920
freedoms

426
00:15:14,320 --> 00:15:18,560
is that this goes beyond just reading

427
00:15:15,920 --> 00:15:20,160
the source code ai systems are not just

428
00:15:18,560 --> 00:15:22,800
source code anymore

429
00:15:20,160 --> 00:15:24,480
it's the source code the training data

430
00:15:22,800 --> 00:15:26,639
which was used to train the model

431
00:15:24,480 --> 00:15:28,240
but it's also considerations that the

432
00:15:26,639 --> 00:15:29,920
developers took when developing

433
00:15:28,240 --> 00:15:31,759
and what the team is made of the

434
00:15:29,920 --> 00:15:33,680
underlying research within the context

435
00:15:31,759 --> 00:15:35,839
in which the system will operate

436
00:15:33,680 --> 00:15:37,359
and so we have to understand that our

437
00:15:35,839 --> 00:15:41,839
freedom to audit

438
00:15:37,360 --> 00:15:41,839
goes beyond just reading code

439
00:15:42,000 --> 00:15:47,680
so for example has anyone here heard of

440
00:15:45,759 --> 00:15:49,440
this trolley problem

441
00:15:47,680 --> 00:15:50,800
this has gotten very popular from

442
00:15:49,440 --> 00:15:52,800
self-driving cars

443
00:15:50,800 --> 00:15:55,839
because they're already programmed to

444
00:15:52,800 --> 00:15:57,920
make decisions involving human life

445
00:15:55,839 --> 00:15:59,839
and so how are decisions made when human

446
00:15:57,920 --> 00:16:01,599
life is at risk

447
00:15:59,839 --> 00:16:04,079
well we have the right to know that

448
00:16:01,600 --> 00:16:05,279
right and this technology directly

449
00:16:04,079 --> 00:16:09,040
impacts us

450
00:16:05,279 --> 00:16:12,000
as humans and our friends and family

451
00:16:09,040 --> 00:16:12,959
so a study asked people if a car had to

452
00:16:12,000 --> 00:16:15,279
choose between

453
00:16:12,959 --> 00:16:16,880
running into a cheap running into a tree

454
00:16:15,279 --> 00:16:19,600
or running into a child

455
00:16:16,880 --> 00:16:21,519
which choice should the car make so i

456
00:16:19,600 --> 00:16:23,519
hope you'll all be relieved to know that

457
00:16:21,519 --> 00:16:26,160
over 90 percent of respondents said it

458
00:16:23,519 --> 00:16:28,800
should run into the tree

459
00:16:26,160 --> 00:16:29,360
however the next question said would you

460
00:16:28,800 --> 00:16:31,839
choose

461
00:16:29,360 --> 00:16:33,600
to would you choose to ride in that car

462
00:16:31,839 --> 00:16:36,320
that would run into the tree

463
00:16:33,600 --> 00:16:36,959
and vast majority of the respondents

464
00:16:36,320 --> 00:16:39,040
said no

465
00:16:36,959 --> 00:16:42,079
i don't want to run into a tree i want

466
00:16:39,040 --> 00:16:42,079
to save myself

467
00:16:42,720 --> 00:16:47,360
so would it be ethical for a car to take

468
00:16:46,320 --> 00:16:51,120
into

469
00:16:47,360 --> 00:16:53,680
your age your race gender social status

470
00:16:51,120 --> 00:16:55,440
when deciding whether you get to live if

471
00:16:53,680 --> 00:16:57,279
a self-driving car could access

472
00:16:55,440 --> 00:16:59,600
personal information such as criminal

473
00:16:57,279 --> 00:17:01,120
history or known friends

474
00:16:59,600 --> 00:17:03,680
would it be ethical to use that

475
00:17:01,120 --> 00:17:05,520
information when making these decisions

476
00:17:03,680 --> 00:17:07,839
would it even be moral for someone to

477
00:17:05,520 --> 00:17:08,879
make a car which favored the safety of

478
00:17:07,839 --> 00:17:11,438
passengers

479
00:17:08,880 --> 00:17:13,120
within the car above all else no matter

480
00:17:11,439 --> 00:17:15,600
what

481
00:17:13,119 --> 00:17:17,520
well maybe we're just asking too much of

482
00:17:15,599 --> 00:17:19,839
ai systems to make these decisions for

483
00:17:17,520 --> 00:17:19,839
us

484
00:17:21,280 --> 00:17:25,199
so our second freedom that i want to

485
00:17:23,439 --> 00:17:27,280
talk about is

486
00:17:25,199 --> 00:17:28,640
these systems we know are capable of

487
00:17:27,280 --> 00:17:31,520
harm we just

488
00:17:28,640 --> 00:17:33,280
demonstrated it in some way or another

489
00:17:31,520 --> 00:17:35,440
and we deserve the guarantee of

490
00:17:33,280 --> 00:17:36,399
liability when these systems do create

491
00:17:35,440 --> 00:17:38,000
harm

492
00:17:36,400 --> 00:17:40,000
so you should also have the freedom to

493
00:17:38,000 --> 00:17:42,480
deliver and expect accountability

494
00:17:40,000 --> 00:17:43,120
and responsibility by those who design

495
00:17:42,480 --> 00:17:45,120
and deploy

496
00:17:43,120 --> 00:17:46,959
automated decision-making systems that

497
00:17:45,120 --> 00:17:48,959
affect you

498
00:17:46,960 --> 00:17:50,880
when machines make decisions for us

499
00:17:48,960 --> 00:17:53,840
based on how we program them

500
00:17:50,880 --> 00:17:55,520
who is accountable for these decisions

501
00:17:53,840 --> 00:17:58,639
is it the machines

502
00:17:55,520 --> 00:18:00,400
is it the creators is it us

503
00:17:58,640 --> 00:18:02,559
well clearly we feel that it's the

504
00:18:00,400 --> 00:18:04,400
creators and the organizations which are

505
00:18:02,559 --> 00:18:05,918
profiting from it

506
00:18:04,400 --> 00:18:09,520
those who create these systems

507
00:18:05,919 --> 00:18:12,000
oftentimes do to profit themselves

508
00:18:09,520 --> 00:18:13,440
and we as those effective deserve to be

509
00:18:12,000 --> 00:18:16,799
put over their profit

510
00:18:13,440 --> 00:18:19,360
this is our livelihoods in our lives

511
00:18:16,799 --> 00:18:20,080
so i want to give an example for all of

512
00:18:19,360 --> 00:18:22,719
you

513
00:18:20,080 --> 00:18:23,760
how could social media be possibly ever

514
00:18:22,720 --> 00:18:26,880
connected to

515
00:18:23,760 --> 00:18:28,320
genocidal campaigns

516
00:18:26,880 --> 00:18:30,640
some of you may be familiar with the

517
00:18:28,320 --> 00:18:33,200
rohingya genocide in myanmar

518
00:18:30,640 --> 00:18:34,880
and so what was the role of facebook in

519
00:18:33,200 --> 00:18:37,600
all of this

520
00:18:34,880 --> 00:18:38,559
the facebook news feed engages uh the

521
00:18:37,600 --> 00:18:41,439
facebook news

522
00:18:38,559 --> 00:18:44,000
news feed optimizes on engaging content

523
00:18:41,440 --> 00:18:45,520
but what is engaging content

524
00:18:44,000 --> 00:18:47,520
you know what makes people click on

525
00:18:45,520 --> 00:18:49,440
links

526
00:18:47,520 --> 00:18:50,879
well numerous studies have shown that

527
00:18:49,440 --> 00:18:53,039
optimizing for engagement

528
00:18:50,880 --> 00:18:55,520
increases recommendations for extremist

529
00:18:53,039 --> 00:18:58,320
and alarming content

530
00:18:55,520 --> 00:18:59,200
researchers knew of these issues and

531
00:18:58,320 --> 00:19:01,200
called them out

532
00:18:59,200 --> 00:19:03,679
years before the rohingya genocide

533
00:19:01,200 --> 00:19:07,039
happened in myanmar

534
00:19:03,679 --> 00:19:09,120
so is facebook responsible for fake news

535
00:19:07,039 --> 00:19:11,039
propped up by religious and military

536
00:19:09,120 --> 00:19:13,600
leaders that contributed to an ethnic

537
00:19:11,039 --> 00:19:15,600
cleansing of the rohingya people

538
00:19:13,600 --> 00:19:17,520
they didn't obviously facebook didn't

539
00:19:15,600 --> 00:19:18,879
explicitly think about causing genocide

540
00:19:17,520 --> 00:19:21,520
when building this feature

541
00:19:18,880 --> 00:19:23,360
but it was a contributing factor in some

542
00:19:21,520 --> 00:19:24,160
way or another they did know about it

543
00:19:23,360 --> 00:19:27,760
and many

544
00:19:24,160 --> 00:19:30,720
significant researchers told them

545
00:19:27,760 --> 00:19:32,640
so who is to blame here is it facebook

546
00:19:30,720 --> 00:19:34,480
is it nobody is it just a bad thing that

547
00:19:32,640 --> 00:19:36,640
happened

548
00:19:34,480 --> 00:19:38,799
i don't know but what we do know is that

549
00:19:36,640 --> 00:19:41,840
profits were placed over people

550
00:19:38,799 --> 00:19:41,840
here specifically

551
00:19:42,160 --> 00:19:46,640
so this brings us into our third freedom

552
00:19:44,640 --> 00:19:49,679
so no decision making system

553
00:19:46,640 --> 00:19:50,960
is ever perfect we are always missing

554
00:19:49,679 --> 00:19:53,280
some data

555
00:19:50,960 --> 00:19:54,880
so you should also have the freedom to

556
00:19:53,280 --> 00:19:57,840
appeal a decision that

557
00:19:54,880 --> 00:19:59,440
affects you so maybe have you ever told

558
00:19:57,840 --> 00:20:01,520
a story to someone to

559
00:19:59,440 --> 00:20:04,080
try to help them see where you're coming

560
00:20:01,520 --> 00:20:06,639
from or to empathize with your situation

561
00:20:04,080 --> 00:20:08,799
or maybe you have to explain a fact

562
00:20:06,640 --> 00:20:10,559
about yourself in a background check or

563
00:20:08,799 --> 00:20:12,639
something that doesn't really represent

564
00:20:10,559 --> 00:20:14,320
who you are maybe you've heard the

565
00:20:12,640 --> 00:20:15,440
phrase you know walk in someone else's

566
00:20:14,320 --> 00:20:18,399
shoes

567
00:20:15,440 --> 00:20:19,360
so our ability to do this is what

568
00:20:18,400 --> 00:20:22,880
connects us

569
00:20:19,360 --> 00:20:24,000
as humans and often helps us avert

570
00:20:22,880 --> 00:20:26,320
disaster

571
00:20:24,000 --> 00:20:28,559
more often than you think there are

572
00:20:26,320 --> 00:20:32,559
always hidden stories that are not

573
00:20:28,559 --> 00:20:35,039
captured by a set of data points

574
00:20:32,559 --> 00:20:36,000
so we should always have the opportunity

575
00:20:35,039 --> 00:20:38,158
to break through

576
00:20:36,000 --> 00:20:39,200
automated systems that influence an

577
00:20:38,159 --> 00:20:41,760
organization

578
00:20:39,200 --> 00:20:44,559
and the people behind these systems to

579
00:20:41,760 --> 00:20:47,440
always use our humanity

580
00:20:44,559 --> 00:20:50,960
so by a show of hands how many people

581
00:20:47,440 --> 00:20:54,320
here have a university degree

582
00:20:50,960 --> 00:20:55,840
awesome i don't so many of you whether

583
00:20:54,320 --> 00:20:57,840
you have one or whether you don't you've

584
00:20:55,840 --> 00:21:00,158
applied for jobs at some point

585
00:20:57,840 --> 00:21:03,678
and you when you apply you submit your

586
00:21:00,159 --> 00:21:05,440
cv or your resume to the application

587
00:21:03,679 --> 00:21:08,000
we already know that automated tools are

588
00:21:05,440 --> 00:21:09,679
used to review cvs and job searching

589
00:21:08,000 --> 00:21:13,200
applications

590
00:21:09,679 --> 00:21:16,320
but what is the point of the interview

591
00:21:13,200 --> 00:21:18,480
with a real person interviews are a

592
00:21:16,320 --> 00:21:20,720
chance to tell our own hidden stories

593
00:21:18,480 --> 00:21:24,720
and explain the gaps between

594
00:21:20,720 --> 00:21:27,120
what's on our cv and what isn't

595
00:21:24,720 --> 00:21:27,760
it gives us a chance to build empathy

596
00:21:27,120 --> 00:21:31,760
between

597
00:21:27,760 --> 00:21:34,080
us and who we want to be our employer

598
00:21:31,760 --> 00:21:34,799
so where else do we see some examples of

599
00:21:34,080 --> 00:21:37,199
this

600
00:21:34,799 --> 00:21:38,960
in most court systems you have appellate

601
00:21:37,200 --> 00:21:40,960
courts where if you think that the

602
00:21:38,960 --> 00:21:41,520
decision was made where it was an unfair

603
00:21:40,960 --> 00:21:44,640
trial

604
00:21:41,520 --> 00:21:47,440
or you had a bias judge there's a system

605
00:21:44,640 --> 00:21:50,000
in place for you to appeal that decision

606
00:21:47,440 --> 00:21:51,120
same for rejection for loans and credit

607
00:21:50,000 --> 00:21:52,799
offers

608
00:21:51,120 --> 00:21:55,439
if you believe that a decision was made

609
00:21:52,799 --> 00:21:57,440
unfairly against you on bad data

610
00:21:55,440 --> 00:21:59,120
you can appeal to an impartial an

611
00:21:57,440 --> 00:22:03,200
impartial third party

612
00:21:59,120 --> 00:22:05,760
there is a system in place to appeal so

613
00:22:03,200 --> 00:22:08,000
to wrap up what is the idea behind this

614
00:22:05,760 --> 00:22:11,039
freedom

615
00:22:08,000 --> 00:22:13,200
we must not erase the opportunity for

616
00:22:11,039 --> 00:22:14,799
human connection and empathy when these

617
00:22:13,200 --> 00:22:18,559
decisions are made

618
00:22:14,799 --> 00:22:18,559
even with automated systems

619
00:22:19,600 --> 00:22:22,639
so i hope all of you are beginning to

620
00:22:21,280 --> 00:22:26,080
see a trend here

621
00:22:22,640 --> 00:22:29,440
in a theme that we're trying to build

622
00:22:26,080 --> 00:22:30,559
so we know what freedoms that we feel

623
00:22:29,440 --> 00:22:33,360
like we're entitled to

624
00:22:30,559 --> 00:22:35,120
or at least a subset of them but how do

625
00:22:33,360 --> 00:22:36,959
we as creators of these systems even

626
00:22:35,120 --> 00:22:39,039
respect those

627
00:22:36,960 --> 00:22:40,880
well just as the free software movement

628
00:22:39,039 --> 00:22:42,559
did before us

629
00:22:40,880 --> 00:22:44,159
there are certain ways that we can

630
00:22:42,559 --> 00:22:45,918
establish creating those freedoms

631
00:22:44,159 --> 00:22:47,679
certain rules and suggestions

632
00:22:45,919 --> 00:22:50,640
suggestions and guidelines that we can

633
00:22:47,679 --> 00:22:52,400
give to designers and developers and

634
00:22:50,640 --> 00:22:55,120
data scientists and people who create

635
00:22:52,400 --> 00:22:58,640
these systems and so we have a couple of

636
00:22:55,120 --> 00:22:58,639
recommendations that we've thought of

637
00:22:59,039 --> 00:23:04,158
so we like to claim that models are

638
00:23:02,000 --> 00:23:05,360
foolproof and avoid bias and that we

639
00:23:04,159 --> 00:23:07,679
really thought about it and we got the

640
00:23:05,360 --> 00:23:09,678
right data set and everything's perfect

641
00:23:07,679 --> 00:23:11,280
but how do we prove this when your

642
00:23:09,679 --> 00:23:12,400
training data set is completely

643
00:23:11,280 --> 00:23:14,559
proprietary

644
00:23:12,400 --> 00:23:16,640
or with the fact that we know that any

645
00:23:14,559 --> 00:23:19,840
data set is really a subset of

646
00:23:16,640 --> 00:23:20,640
real data right how can we verify these

647
00:23:19,840 --> 00:23:22,959
claims of

648
00:23:20,640 --> 00:23:23,679
amazing accuracy made by creators of ai

649
00:23:22,960 --> 00:23:26,159
systems

650
00:23:23,679 --> 00:23:27,679
if we can't actually reproduce what what

651
00:23:26,159 --> 00:23:30,159
is happening

652
00:23:27,679 --> 00:23:31,440
and so our point here is reproducibility

653
00:23:30,159 --> 00:23:34,080
is about that freedom

654
00:23:31,440 --> 00:23:36,080
to audit right and it connects back to

655
00:23:34,080 --> 00:23:38,080
that first freedom that we talked about

656
00:23:36,080 --> 00:23:39,520
where we should be able to look in and

657
00:23:38,080 --> 00:23:41,600
understand how these decision making

658
00:23:39,520 --> 00:23:44,639
systems are have

659
00:23:41,600 --> 00:23:45,840
and so in general we have three concrete

660
00:23:44,640 --> 00:23:48,240
steps that we think

661
00:23:45,840 --> 00:23:49,279
can help ensure that a model is

662
00:23:48,240 --> 00:23:51,760
reproducible

663
00:23:49,279 --> 00:23:52,559
you really need three main things as we

664
00:23:51,760 --> 00:23:54,720
said earlier

665
00:23:52,559 --> 00:23:55,760
you need access to the training data to

666
00:23:54,720 --> 00:23:57,840
train the model

667
00:23:55,760 --> 00:23:59,520
you need access to the source code so

668
00:23:57,840 --> 00:24:01,199
you can have understand what model

669
00:23:59,520 --> 00:24:02,240
you're using in the corresponding

670
00:24:01,200 --> 00:24:03,600
pipelines

671
00:24:02,240 --> 00:24:05,679
and then you also need proper

672
00:24:03,600 --> 00:24:07,918
documentation describing the model

673
00:24:05,679 --> 00:24:09,600
in fact i think that this is actually

674
00:24:07,919 --> 00:24:11,520
the most critical piece

675
00:24:09,600 --> 00:24:14,080
so by show of hands who here has worked

676
00:24:11,520 --> 00:24:15,520
with open source

677
00:24:14,080 --> 00:24:17,520
awesome i think i came to the right

678
00:24:15,520 --> 00:24:20,400
conference

679
00:24:17,520 --> 00:24:22,158
so sorry but keep your hands up if

680
00:24:20,400 --> 00:24:23,520
documentation was useful for you to

681
00:24:22,159 --> 00:24:26,799
understand how

682
00:24:23,520 --> 00:24:26,799
the open source project work

683
00:24:27,120 --> 00:24:32,080
good documentation is an equalizer

684
00:24:30,320 --> 00:24:33,520
users have the right to reasonable

685
00:24:32,080 --> 00:24:35,360
documentation detailing the

686
00:24:33,520 --> 00:24:36,720
functionality of the ai system that they

687
00:24:35,360 --> 00:24:38,879
are subjected to

688
00:24:36,720 --> 00:24:40,400
now i know this is general language but

689
00:24:38,880 --> 00:24:42,880
there are key steps that we can move

690
00:24:40,400 --> 00:24:44,799
this forward

691
00:24:42,880 --> 00:24:46,080
even ai now the institute that we

692
00:24:44,799 --> 00:24:49,279
mentioned earlier

693
00:24:46,080 --> 00:24:50,720
they have they have published a standard

694
00:24:49,279 --> 00:24:53,520
documentation format

695
00:24:50,720 --> 00:24:54,400
that at least gives maybe the minimum

696
00:24:53,520 --> 00:24:56,480
needed

697
00:24:54,400 --> 00:24:58,159
information as to how the model was

698
00:24:56,480 --> 00:24:58,640
created things that they took into

699
00:24:58,159 --> 00:25:00,240
account

700
00:24:58,640 --> 00:25:02,000
what data that they're actually using

701
00:25:00,240 --> 00:25:04,000
and what they're trying to do with it

702
00:25:02,000 --> 00:25:05,600
and even this can be a very very

703
00:25:04,000 --> 00:25:06,320
powerful thing in equalizing the

704
00:25:05,600 --> 00:25:08,719
knowledge

705
00:25:06,320 --> 00:25:10,399
among the creators and the users of the

706
00:25:08,720 --> 00:25:13,120
software

707
00:25:10,400 --> 00:25:14,720
now i know this isn't perfect but we can

708
00:25:13,120 --> 00:25:17,439
design these requirements in an

709
00:25:14,720 --> 00:25:17,440
actual way

710
00:25:18,480 --> 00:25:22,960
so for our second suggestion it's

711
00:25:20,960 --> 00:25:24,880
difficult to create liability upon

712
00:25:22,960 --> 00:25:27,600
yourself as a creator of something

713
00:25:24,880 --> 00:25:28,559
right you know there are some options

714
00:25:27,600 --> 00:25:31,678
that we thought of

715
00:25:28,559 --> 00:25:34,320
like uh the b corporation certification

716
00:25:31,679 --> 00:25:36,960
can help you can sign on to to help

717
00:25:34,320 --> 00:25:38,879
create liability upon your organization

718
00:25:36,960 --> 00:25:40,960
but we don't really have anything like

719
00:25:38,880 --> 00:25:42,720
this for the ai freedoms that we're

720
00:25:40,960 --> 00:25:44,799
talking about today

721
00:25:42,720 --> 00:25:46,240
so how can we be responsible in

722
00:25:44,799 --> 00:25:48,000
designing our systems to be

723
00:25:46,240 --> 00:25:50,080
accountable for these adverse side

724
00:25:48,000 --> 00:25:53,360
effects that we're trying to prevent

725
00:25:50,080 --> 00:25:55,520
well let's design responsibly

726
00:25:53,360 --> 00:25:56,639
one option that really gave us a lot of

727
00:25:55,520 --> 00:26:00,080
inspiration

728
00:25:56,640 --> 00:26:03,200
it comes from gdpr and the

729
00:26:00,080 --> 00:26:04,240
the the requirement of performing impact

730
00:26:03,200 --> 00:26:06,799
assessments

731
00:26:04,240 --> 00:26:08,480
so gdpr requires what's called a data

732
00:26:06,799 --> 00:26:10,000
protection impact assessment

733
00:26:08,480 --> 00:26:12,159
anytime you collect significant amounts

734
00:26:10,000 --> 00:26:13,279
of user data and so as part of this

735
00:26:12,159 --> 00:26:15,520
impact assessment

736
00:26:13,279 --> 00:26:16,720
you consider you do a risk assessment so

737
00:26:15,520 --> 00:26:17,600
you have to consider things that might

738
00:26:16,720 --> 00:26:19,840
go wrong

739
00:26:17,600 --> 00:26:21,520
and you document mitigations that you

740
00:26:19,840 --> 00:26:22,240
have in place to help try to prevent

741
00:26:21,520 --> 00:26:24,960
that

742
00:26:22,240 --> 00:26:27,120
it's not perfect but what it does do is

743
00:26:24,960 --> 00:26:27,600
it creates a small amount of liability

744
00:26:27,120 --> 00:26:29,840
and

745
00:26:27,600 --> 00:26:31,520
more importantly a paper trail on the

746
00:26:29,840 --> 00:26:32,158
creator for having considered these side

747
00:26:31,520 --> 00:26:34,320
effects

748
00:26:32,159 --> 00:26:35,600
and what sort of considerations they

749
00:26:34,320 --> 00:26:37,678
made

750
00:26:35,600 --> 00:26:39,918
most importantly it forces creators to

751
00:26:37,679 --> 00:26:40,559
consider ethical repercussions as part

752
00:26:39,919 --> 00:26:43,039
of the design

753
00:26:40,559 --> 00:26:43,039
process

754
00:26:43,679 --> 00:26:47,440
so our third suggestion for how we can

755
00:26:46,159 --> 00:26:49,120
protect these freedoms

756
00:26:47,440 --> 00:26:50,960
is going back to the appealing

757
00:26:49,120 --> 00:26:54,320
mechanisms mentioned earlier

758
00:26:50,960 --> 00:26:56,400
so we have these systems are already

759
00:26:54,320 --> 00:26:58,240
you know there's no secret sauce or new

760
00:26:56,400 --> 00:26:59,760
ideas that we're pitching here

761
00:26:58,240 --> 00:27:02,080
you know so let's start with the things

762
00:26:59,760 --> 00:27:05,600
that we already have as a model

763
00:27:02,080 --> 00:27:07,918
so appealing mechanisms what are they

764
00:27:05,600 --> 00:27:08,799
human-controlled methods to appeal a

765
00:27:07,919 --> 00:27:11,279
decision

766
00:27:08,799 --> 00:27:12,639
and correct mistakes made by automated

767
00:27:11,279 --> 00:27:14,720
systems

768
00:27:12,640 --> 00:27:15,840
common among other services we already

769
00:27:14,720 --> 00:27:18,880
have

770
00:27:15,840 --> 00:27:20,639
the courts bank loans we talked about

771
00:27:18,880 --> 00:27:22,640
these so

772
00:27:20,640 --> 00:27:25,279
even if an automated decision-making

773
00:27:22,640 --> 00:27:27,200
system does have input on a decision

774
00:27:25,279 --> 00:27:30,720
appealing mechanisms must be

775
00:27:27,200 --> 00:27:32,480
human-centered and human-controlled

776
00:27:30,720 --> 00:27:34,399
so appealing mechanisms should be

777
00:27:32,480 --> 00:27:38,559
designed to create empathy

778
00:27:34,399 --> 00:27:39,760
between humans so okay like that's nice

779
00:27:38,559 --> 00:27:41,279
and all but like what would that look

780
00:27:39,760 --> 00:27:44,080
like for ai right

781
00:27:41,279 --> 00:27:45,520
so an example of manual override is an

782
00:27:44,080 --> 00:27:48,399
excellent example of this

783
00:27:45,520 --> 00:27:50,320
in the tesla autopilot cars the moment

784
00:27:48,399 --> 00:27:51,360
that you as the driver place your hand

785
00:27:50,320 --> 00:27:53,840
on the wheel

786
00:27:51,360 --> 00:27:56,080
your feet on the gas pedal or the brake

787
00:27:53,840 --> 00:27:58,158
you immediately override

788
00:27:56,080 --> 00:28:00,480
any automated decisions that the car

789
00:27:58,159 --> 00:28:03,840
autopilot would make for you

790
00:28:00,480 --> 00:28:06,480
in a sense you appeal the decisions

791
00:28:03,840 --> 00:28:08,000
that the car would make for you so

792
00:28:06,480 --> 00:28:09,760
that's a good example

793
00:28:08,000 --> 00:28:11,279
but what happens when appealing

794
00:28:09,760 --> 00:28:14,720
mechanisms are

795
00:28:11,279 --> 00:28:15,440
poorly designed many of you are probably

796
00:28:14,720 --> 00:28:19,279
familiar

797
00:28:15,440 --> 00:28:21,360
with the boeing 737 max and the mcas

798
00:28:19,279 --> 00:28:23,520
system and the results that it had in

799
00:28:21,360 --> 00:28:26,959
many of their planes crashing

800
00:28:23,520 --> 00:28:27,918
so our appealing systems must place user

801
00:28:26,960 --> 00:28:30,960
experience

802
00:28:27,919 --> 00:28:33,279
at the forefront appealing mechanisms

803
00:28:30,960 --> 00:28:34,880
need to be designed with accessibility

804
00:28:33,279 --> 00:28:38,159
and ease of use

805
00:28:34,880 --> 00:28:42,320
or else the right to appeal is not

806
00:28:38,159 --> 00:28:42,320
evenly distributed to all people

807
00:28:42,960 --> 00:28:46,559
so okay so free software came from

808
00:28:45,679 --> 00:28:48,960
wanting to build

809
00:28:46,559 --> 00:28:50,399
software with our values and freedoms at

810
00:28:48,960 --> 00:28:52,559
the core

811
00:28:50,399 --> 00:28:53,600
can we reapply that to the changing

812
00:28:52,559 --> 00:28:56,080
world that's

813
00:28:53,600 --> 00:28:58,158
increasingly being driven by data and

814
00:28:56,080 --> 00:29:01,360
human futures

815
00:28:58,159 --> 00:29:03,679
we want to adhere to these freedoms but

816
00:29:01,360 --> 00:29:05,678
we have to agree on what they are we

817
00:29:03,679 --> 00:29:07,679
need to collectively define

818
00:29:05,679 --> 00:29:09,679
something that we can collectively

819
00:29:07,679 --> 00:29:12,320
organize around

820
00:29:09,679 --> 00:29:13,520
so looking back how did free software do

821
00:29:12,320 --> 00:29:16,720
it

822
00:29:13,520 --> 00:29:18,559
the four freedoms came before the fsf

823
00:29:16,720 --> 00:29:20,320
which came before the software freedom

824
00:29:18,559 --> 00:29:21,360
movement was really a thing that we were

825
00:29:20,320 --> 00:29:22,639
talking about

826
00:29:21,360 --> 00:29:24,719
before we were even here at this

827
00:29:22,640 --> 00:29:26,799
conference so

828
00:29:24,720 --> 00:29:29,919
we need to find more ways to work

829
00:29:26,799 --> 00:29:31,918
together and standardize our freedoms

830
00:29:29,919 --> 00:29:33,039
while we can talk about the freedoms all

831
00:29:31,919 --> 00:29:34,880
we want

832
00:29:33,039 --> 00:29:37,120
first we need a common structure to

833
00:29:34,880 --> 00:29:39,520
define what we want

834
00:29:37,120 --> 00:29:41,279
and then agree upon that so you want to

835
00:29:39,520 --> 00:29:45,039
build a movement

836
00:29:41,279 --> 00:29:47,840
define a standard and as it happens

837
00:29:45,039 --> 00:29:49,200
we free software people are a little

838
00:29:47,840 --> 00:29:51,918
pretty good at that

839
00:29:49,200 --> 00:29:53,120
so standards and protocols are kind of

840
00:29:51,919 --> 00:29:55,679
our thing

841
00:29:53,120 --> 00:29:56,559
while the open web it's not perfect

842
00:29:55,679 --> 00:29:59,840
right

843
00:29:56,559 --> 00:30:01,840
it has its flaws but we have created a

844
00:29:59,840 --> 00:30:04,080
globally represented standard

845
00:30:01,840 --> 00:30:06,240
and many standards that are mostly

846
00:30:04,080 --> 00:30:06,720
respected by organizations around the

847
00:30:06,240 --> 00:30:09,039
world

848
00:30:06,720 --> 00:30:11,360
and they're in use every day some

849
00:30:09,039 --> 00:30:13,039
examples the world wide web consortium

850
00:30:11,360 --> 00:30:15,439
the w3c

851
00:30:13,039 --> 00:30:16,240
the web hypertext application technology

852
00:30:15,440 --> 00:30:20,159
working group

853
00:30:16,240 --> 00:30:22,960
of mouthful what wg the ieee

854
00:30:20,159 --> 00:30:26,240
the internet engineering task force

855
00:30:22,960 --> 00:30:28,000
rfc's request for comments

856
00:30:26,240 --> 00:30:30,399
so there's a there is a need to take

857
00:30:28,000 --> 00:30:31,360
this into account that we do have forums

858
00:30:30,399 --> 00:30:34,799
in place

859
00:30:31,360 --> 00:30:38,080
to do this type of work already

860
00:30:34,799 --> 00:30:40,799
and some of the power is in our hands

861
00:30:38,080 --> 00:30:42,559
as designers and makers of our shared

862
00:30:40,799 --> 00:30:44,639
digital world

863
00:30:42,559 --> 00:30:47,200
what we presented are some of the ways

864
00:30:44,640 --> 00:30:48,960
we believe engineers of all backgrounds

865
00:30:47,200 --> 00:30:50,559
building automated decision-making

866
00:30:48,960 --> 00:30:54,559
systems can remain

867
00:30:50,559 --> 00:30:54,559
conscious about these freedoms

868
00:30:55,039 --> 00:30:58,960
so these methods and ideas they may work

869
00:30:58,320 --> 00:31:00,559
for us

870
00:30:58,960 --> 00:31:02,399
as creators right because we have

871
00:31:00,559 --> 00:31:04,799
control over what create

872
00:31:02,399 --> 00:31:06,000
but how do we scale this issue from the

873
00:31:04,799 --> 00:31:08,320
individual level of us

874
00:31:06,000 --> 00:31:11,679
as creators to the societal level making

875
00:31:08,320 --> 00:31:14,240
sure that everyone is protected

876
00:31:11,679 --> 00:31:15,120
well looking throughout history society

877
00:31:14,240 --> 00:31:17,120
has had

878
00:31:15,120 --> 00:31:18,799
many different tools that can help

879
00:31:17,120 --> 00:31:21,918
organize movements

880
00:31:18,799 --> 00:31:23,120
enforce their demands and we all have

881
00:31:21,919 --> 00:31:26,240
our own skills

882
00:31:23,120 --> 00:31:29,518
and positions in place in society that

883
00:31:26,240 --> 00:31:31,279
where we can make an impact so

884
00:31:29,519 --> 00:31:33,200
we need to understand how different

885
00:31:31,279 --> 00:31:35,519
tools can create different kinds of

886
00:31:33,200 --> 00:31:38,640
impact that we wish to see

887
00:31:35,519 --> 00:31:41,039
and no matter how you can trade bu

888
00:31:38,640 --> 00:31:41,760
it's always up to you to join in as i

889
00:31:41,039 --> 00:31:44,879
said earlier

890
00:31:41,760 --> 00:31:44,879
patches are always welcome

891
00:31:45,200 --> 00:31:49,360
but how do we ensure the freedoms are

892
00:31:47,600 --> 00:31:50,879
respected by the rest of society

893
00:31:49,360 --> 00:31:52,719
because we know if we don't protect

894
00:31:50,880 --> 00:31:54,880
these freedoms we can't ever truly be

895
00:31:52,720 --> 00:31:57,600
sure that they'll be respected

896
00:31:54,880 --> 00:31:58,960
and so in our research we have found

897
00:31:57,600 --> 00:32:01,120
four main methods of

898
00:31:58,960 --> 00:32:02,320
either enforcing or at very least

899
00:32:01,120 --> 00:32:04,239
incentivizing

900
00:32:02,320 --> 00:32:07,439
others to also respect these same

901
00:32:04,240 --> 00:32:10,159
freedoms that we're trying to protect

902
00:32:07,440 --> 00:32:11,519
and so the first one is maybe an obvious

903
00:32:10,159 --> 00:32:13,519
one to many of you

904
00:32:11,519 --> 00:32:15,760
when researching the history of free

905
00:32:13,519 --> 00:32:17,600
software we naturally gravitate

906
00:32:15,760 --> 00:32:19,200
gravitated towards using licensing as a

907
00:32:17,600 --> 00:32:21,760
model because

908
00:32:19,200 --> 00:32:23,760
that's what they did because ai systems

909
00:32:21,760 --> 00:32:25,279
extend simply beyond the model

910
00:32:23,760 --> 00:32:27,600
but also the data and even the

911
00:32:25,279 --> 00:32:28,799
documentation this began to get a little

912
00:32:27,600 --> 00:32:32,399
tricky

913
00:32:28,799 --> 00:32:35,519
and so neither i nor justin are lawyers

914
00:32:32,399 --> 00:32:37,840
believe it or not but we suspect that

915
00:32:35,519 --> 00:32:39,840
licensing could be a strategy that one

916
00:32:37,840 --> 00:32:42,000
could use to ensure derivatives are

917
00:32:39,840 --> 00:32:44,480
developed into sustainable and ethical

918
00:32:42,000 --> 00:32:45,840
fashion by developing some sort of

919
00:32:44,480 --> 00:32:48,640
comprehensive package

920
00:32:45,840 --> 00:32:49,678
package license solution for a code and

921
00:32:48,640 --> 00:32:51,600
data sets

922
00:32:49,679 --> 00:32:53,279
so no matter what part you're touching

923
00:32:51,600 --> 00:32:54,799
whether you're a contributor of data or

924
00:32:53,279 --> 00:32:57,360
a contributor of code

925
00:32:54,799 --> 00:32:59,200
licensing could be a way of virally

926
00:32:57,360 --> 00:33:00,000
infecting all other parts that have to

927
00:32:59,200 --> 00:33:03,120
use it

928
00:33:00,000 --> 00:33:05,360
and enforcing these rights now

929
00:33:03,120 --> 00:33:06,959
obviously this has a few pros and cons

930
00:33:05,360 --> 00:33:09,039
the biggest con that we

931
00:33:06,960 --> 00:33:10,000
immediately saw was you know this is

932
00:33:09,039 --> 00:33:12,559
contributing to the

933
00:33:10,000 --> 00:33:14,159
issue of license proliferation it's yet

934
00:33:12,559 --> 00:33:15,840
another software license out there that

935
00:33:14,159 --> 00:33:18,880
hasn't been tested in court

936
00:33:15,840 --> 00:33:19,600
right and you know it the more licenses

937
00:33:18,880 --> 00:33:22,559
we have

938
00:33:19,600 --> 00:33:24,000
the less sure people are within the our

939
00:33:22,559 --> 00:33:26,559
ability to actually enforce

940
00:33:24,000 --> 00:33:28,000
them it creates this massive legal and

941
00:33:26,559 --> 00:33:29,918
litigatory overhead

942
00:33:28,000 --> 00:33:31,519
and it places strain on like the

943
00:33:29,919 --> 00:33:33,519
original creator of the software to

944
00:33:31,519 --> 00:33:36,559
actually go out and try to enforce that

945
00:33:33,519 --> 00:33:38,640
as the holder of the copyright however

946
00:33:36,559 --> 00:33:40,399
it does shift power into activist hands

947
00:33:38,640 --> 00:33:41,360
it gives us an immediate tool that we

948
00:33:40,399 --> 00:33:43,678
can use now

949
00:33:41,360 --> 00:33:46,479
to begin saying us as creators or

950
00:33:43,679 --> 00:33:48,720
contributors to these ai systems

951
00:33:46,480 --> 00:33:51,519
we want it to work this way and we don't

952
00:33:48,720 --> 00:33:53,200
have to wait for anyone to decide that

953
00:33:51,519 --> 00:33:54,880
and so we have seen a number of

954
00:33:53,200 --> 00:33:56,880
interesting innovations in the area

955
00:33:54,880 --> 00:33:59,760
specifically around data licensing

956
00:33:56,880 --> 00:34:00,159
which has tested out the waters a little

957
00:33:59,760 --> 00:34:02,480
bit

958
00:34:00,159 --> 00:34:04,320
not comprehensively but maybe shown a

959
00:34:02,480 --> 00:34:07,360
weight possible way forward

960
00:34:04,320 --> 00:34:09,759
open street maps uh a few years ago

961
00:34:07,360 --> 00:34:11,520
changed their licensing model to uh

962
00:34:09,760 --> 00:34:12,560
almost adhere to more of a copyleft

963
00:34:11,520 --> 00:34:15,679
strategy

964
00:34:12,560 --> 00:34:16,960
allowing them to gain power in the

965
00:34:15,679 --> 00:34:18,079
creation of their own data set and

966
00:34:16,960 --> 00:34:18,960
allowing people who want to make

967
00:34:18,079 --> 00:34:22,399
derivatives

968
00:34:18,960 --> 00:34:23,119
also contribute back the cdla took into

969
00:34:22,399 --> 00:34:24,879
account

970
00:34:23,119 --> 00:34:26,480
people's personally identifiable

971
00:34:24,879 --> 00:34:30,078
information and ensuring that it doesn't

972
00:34:26,480 --> 00:34:30,079
get released in public data sets

973
00:34:31,839 --> 00:34:35,759
the next step that we really thought

974
00:34:33,359 --> 00:34:38,639
about was regulation

975
00:34:35,760 --> 00:34:40,480
you know we were inspired by gdpr and

976
00:34:38,639 --> 00:34:42,320
the strong arm of the government is an

977
00:34:40,480 --> 00:34:44,960
enticing tool to want to use

978
00:34:42,320 --> 00:34:47,200
right however the road to creating

979
00:34:44,960 --> 00:34:49,839
ethical ai regulation is long

980
00:34:47,199 --> 00:34:51,199
we all know about the issue of politics

981
00:34:49,839 --> 00:34:53,359
and how slow it is and

982
00:34:51,199 --> 00:34:54,638
to conv convey the right information to

983
00:34:53,359 --> 00:34:58,480
regulators

984
00:34:54,639 --> 00:35:01,040
and we want all of our rights here now

985
00:34:58,480 --> 00:35:02,960
but while regulation is slow to create

986
00:35:01,040 --> 00:35:04,400
ai ethics is something that regulators

987
00:35:02,960 --> 00:35:06,480
are already interested in

988
00:35:04,400 --> 00:35:08,000
and the time is now to try to impart our

989
00:35:06,480 --> 00:35:11,920
needs upon them

990
00:35:08,000 --> 00:35:13,200
loud and clear but this also has a few

991
00:35:11,920 --> 00:35:16,079
pros and cons right

992
00:35:13,200 --> 00:35:16,799
it's powerful but it's slow and it's

993
00:35:16,079 --> 00:35:18,960
also

994
00:35:16,800 --> 00:35:20,320
political and unstable regulation can

995
00:35:18,960 --> 00:35:23,599
get repealed

996
00:35:20,320 --> 00:35:25,040
and it's geographic so maybe it's

997
00:35:23,599 --> 00:35:26,240
somewhere else but not within your

998
00:35:25,040 --> 00:35:28,560
country

999
00:35:26,240 --> 00:35:30,399
and lastly the issue of regulatory

1000
00:35:28,560 --> 00:35:32,560
capture we live in a world with

1001
00:35:30,400 --> 00:35:34,320
lots of very very strong wealthy and

1002
00:35:32,560 --> 00:35:36,000
powerful technology companies

1003
00:35:34,320 --> 00:35:38,720
that have strong opinions as to what

1004
00:35:36,000 --> 00:35:41,760
they should be able to do

1005
00:35:38,720 --> 00:35:43,279
but looking back even the ceo of google

1006
00:35:41,760 --> 00:35:44,880
has suggested that

1007
00:35:43,280 --> 00:35:47,920
countries begin at least thinking about

1008
00:35:44,880 --> 00:35:47,920
creating regulation

1009
00:35:48,480 --> 00:35:53,200
so this brings us to our third our third

1010
00:35:50,640 --> 00:35:55,440
idea of how to help protect our freedoms

1011
00:35:53,200 --> 00:35:56,640
so the difference of knowledge between

1012
00:35:55,440 --> 00:35:59,280
consumers

1013
00:35:56,640 --> 00:36:00,640
and producers is one thing that allows

1014
00:35:59,280 --> 00:36:04,480
for exploitation

1015
00:36:00,640 --> 00:36:06,799
by producers so one way we see a way

1016
00:36:04,480 --> 00:36:09,359
forward is having a third party

1017
00:36:06,800 --> 00:36:11,119
independent certification body which is

1018
00:36:09,359 --> 00:36:12,240
kind of a classic solution to

1019
00:36:11,119 --> 00:36:15,440
simplifying

1020
00:36:12,240 --> 00:36:17,680
really complex information for consumers

1021
00:36:15,440 --> 00:36:19,599
to help them make the right and smart

1022
00:36:17,680 --> 00:36:22,480
decisions for their own life

1023
00:36:19,599 --> 00:36:24,400
so a certifying authority could allow

1024
00:36:22,480 --> 00:36:26,320
creators of an ai system

1025
00:36:24,400 --> 00:36:29,040
which respects our freedoms to more

1026
00:36:26,320 --> 00:36:31,280
easily earn consumer trust

1027
00:36:29,040 --> 00:36:34,160
there are some pros and cons again on

1028
00:36:31,280 --> 00:36:35,920
the one hand this does empower consumers

1029
00:36:34,160 --> 00:36:37,440
i don't have to have a three-hour

1030
00:36:35,920 --> 00:36:40,000
conversation with my mom

1031
00:36:37,440 --> 00:36:42,000
explaining ethical ai when she's going

1032
00:36:40,000 --> 00:36:44,079
to download an app on her phone

1033
00:36:42,000 --> 00:36:46,079
to help her make the right decision

1034
00:36:44,079 --> 00:36:48,640
right this gives people who are

1035
00:36:46,079 --> 00:36:50,000
not in this bubble of our world to

1036
00:36:48,640 --> 00:36:51,759
understand

1037
00:36:50,000 --> 00:36:53,839
how they can make the right and smart

1038
00:36:51,760 --> 00:36:54,720
decisions and have there be some kind of

1039
00:36:53,839 --> 00:36:58,160
authority

1040
00:36:54,720 --> 00:36:58,959
to support that and while it does

1041
00:36:58,160 --> 00:37:01,359
empower

1042
00:36:58,960 --> 00:37:03,040
it doesn't explicitly protect it is kind

1043
00:37:01,359 --> 00:37:05,920
of an opt-in model

1044
00:37:03,040 --> 00:37:07,200
but this does empower competition if

1045
00:37:05,920 --> 00:37:09,520
you're going into market

1046
00:37:07,200 --> 00:37:10,480
and your competitor is pushing out that

1047
00:37:09,520 --> 00:37:13,359
message for

1048
00:37:10,480 --> 00:37:14,800
an ethical or more respects your rights

1049
00:37:13,359 --> 00:37:16,480
product it is something that puts a

1050
00:37:14,800 --> 00:37:18,880
little bit of pressure on you as a

1051
00:37:16,480 --> 00:37:21,119
competitor in that space

1052
00:37:18,880 --> 00:37:22,000
but one thing to really emphasize here

1053
00:37:21,119 --> 00:37:24,320
is that it must be

1054
00:37:22,000 --> 00:37:26,400
run by a third party you can't have a

1055
00:37:24,320 --> 00:37:27,280
single company or organization rolling

1056
00:37:26,400 --> 00:37:29,520
in saying

1057
00:37:27,280 --> 00:37:31,680
we know how to do ai the best way here's

1058
00:37:29,520 --> 00:37:34,560
the way to do it let's go

1059
00:37:31,680 --> 00:37:36,319
no you have to have an intersectional

1060
00:37:34,560 --> 00:37:38,640
building of different people from

1061
00:37:36,320 --> 00:37:40,560
different backgrounds and perspectives

1062
00:37:38,640 --> 00:37:43,118
small and humanitarian corporate

1063
00:37:40,560 --> 00:37:45,520
backgrounds so it's not just one voice

1064
00:37:43,119 --> 00:37:48,000
overpowering the conversation

1065
00:37:45,520 --> 00:37:48,960
so going back to some examples we did

1066
00:37:48,000 --> 00:37:51,200
briefly mention

1067
00:37:48,960 --> 00:37:52,880
b corporations earlier this is a

1068
00:37:51,200 --> 00:37:54,720
certifying authority at least in the

1069
00:37:52,880 --> 00:37:57,040
united states as far as i know

1070
00:37:54,720 --> 00:37:59,439
to help ensure corporations have a

1071
00:37:57,040 --> 00:38:01,759
positive impact on their workers

1072
00:37:59,440 --> 00:38:02,960
community customers or even the

1073
00:38:01,760 --> 00:38:05,040
environment

1074
00:38:02,960 --> 00:38:06,720
another example from the u.s is we have

1075
00:38:05,040 --> 00:38:08,960
the department of agriculture

1076
00:38:06,720 --> 00:38:10,720
and organic food the department of

1077
00:38:08,960 --> 00:38:12,960
agriculture in the united states

1078
00:38:10,720 --> 00:38:14,959
acts as a certifying authority to ensure

1079
00:38:12,960 --> 00:38:17,200
that food producers don't use

1080
00:38:14,960 --> 00:38:18,240
chemicals and growth hormones and other

1081
00:38:17,200 --> 00:38:20,399
substances

1082
00:38:18,240 --> 00:38:22,319
in growing their food so you as the

1083
00:38:20,400 --> 00:38:24,800
consumer can go and

1084
00:38:22,320 --> 00:38:26,240
buy food that respects your that your

1085
00:38:24,800 --> 00:38:27,920
preferences

1086
00:38:26,240 --> 00:38:29,520
another example is actually one that's

1087
00:38:27,920 --> 00:38:30,560
really right here and personal to

1088
00:38:29,520 --> 00:38:32,400
probably all of us

1089
00:38:30,560 --> 00:38:34,720
is the free software foundation's

1090
00:38:32,400 --> 00:38:36,320
respect your freedom certification

1091
00:38:34,720 --> 00:38:38,560
we've seen that with examples of things

1092
00:38:36,320 --> 00:38:42,800
like the lulzbot 3d printers

1093
00:38:38,560 --> 00:38:44,240
and also the librim laptops from purism

1094
00:38:42,800 --> 00:38:46,480
so this brings us to our fourth and

1095
00:38:44,240 --> 00:38:48,879
final idea of how we can enforce

1096
00:38:46,480 --> 00:38:51,040
these freedoms and the fourth one is

1097
00:38:48,880 --> 00:38:54,480
lastly what has historically been

1098
00:38:51,040 --> 00:38:57,920
one of our strongest tools as workers

1099
00:38:54,480 --> 00:39:00,160
is to organize labor organization is a

1100
00:38:57,920 --> 00:39:01,520
powerful tool that workers can use to

1101
00:39:00,160 --> 00:39:05,040
ensure their freedoms

1102
00:39:01,520 --> 00:39:06,960
are respected so maybe this

1103
00:39:05,040 --> 00:39:08,320
maybe this is personal to you it's

1104
00:39:06,960 --> 00:39:11,119
personal to me but

1105
00:39:08,320 --> 00:39:12,640
more tech workers are noticing that

1106
00:39:11,119 --> 00:39:14,240
there are negative impacts to the

1107
00:39:12,640 --> 00:39:16,078
systems that we're building

1108
00:39:14,240 --> 00:39:17,439
the facebook example from earlier with

1109
00:39:16,079 --> 00:39:19,520
myanmar

1110
00:39:17,440 --> 00:39:21,760
a labor union's ability to affect a

1111
00:39:19,520 --> 00:39:23,280
workforce's employment contract can

1112
00:39:21,760 --> 00:39:25,520
incentivize your company

1113
00:39:23,280 --> 00:39:27,200
to take notice and respect the wishes of

1114
00:39:25,520 --> 00:39:29,839
their employees

1115
00:39:27,200 --> 00:39:31,439
a union can transition some power from

1116
00:39:29,839 --> 00:39:34,960
the shareholders hands

1117
00:39:31,440 --> 00:39:37,040
into the hands of the creators me you

1118
00:39:34,960 --> 00:39:39,119
who are actually creating the real value

1119
00:39:37,040 --> 00:39:42,079
of these systems

1120
00:39:39,119 --> 00:39:44,880
so what are some examples of that in

1121
00:39:42,079 --> 00:39:47,280
this 21st century world we're living in

1122
00:39:44,880 --> 00:39:48,640
the amazon employees for climate justice

1123
00:39:47,280 --> 00:39:52,000
are organizing against

1124
00:39:48,640 --> 00:39:54,160
amazon's inaction towards climate change

1125
00:39:52,000 --> 00:39:55,680
github employees are protesting and

1126
00:39:54,160 --> 00:39:57,759
threatening to strike

1127
00:39:55,680 --> 00:39:59,200
over their company's deal with america's

1128
00:39:57,760 --> 00:40:01,760
violently and xenophobic

1129
00:39:59,200 --> 00:40:03,520
immigration enforcement department

1130
00:40:01,760 --> 00:40:05,359
google has already threatened their

1131
00:40:03,520 --> 00:40:06,480
employees by trying to unionize that

1132
00:40:05,359 --> 00:40:08,560
they have been

1133
00:40:06,480 --> 00:40:11,040
acted google has been actively deploying

1134
00:40:08,560 --> 00:40:13,839
expensive counter measures to sow doubt

1135
00:40:11,040 --> 00:40:17,839
in the power of labor organization

1136
00:40:13,839 --> 00:40:18,880
so all this said the power of labor

1137
00:40:17,839 --> 00:40:20,640
organization

1138
00:40:18,880 --> 00:40:22,319
is being respected by these

1139
00:40:20,640 --> 00:40:25,598
organizations

1140
00:40:22,319 --> 00:40:27,359
so much that they're scared google and

1141
00:40:25,599 --> 00:40:30,240
amazon have already started to fire

1142
00:40:27,359 --> 00:40:32,319
employees over these issues they want to

1143
00:40:30,240 --> 00:40:34,240
downplay and put it down before it makes

1144
00:40:32,319 --> 00:40:35,279
them change what kind of organization

1145
00:40:34,240 --> 00:40:37,118
they are

1146
00:40:35,280 --> 00:40:38,880
and possibly bring a shift to the

1147
00:40:37,119 --> 00:40:42,079
organizational and

1148
00:40:38,880 --> 00:40:44,640
culture and values there as scary as

1149
00:40:42,079 --> 00:40:47,760
this can be for us

1150
00:40:44,640 --> 00:40:47,759
it's more so for them

1151
00:40:47,920 --> 00:40:51,440
so to summarize everything we've talked

1152
00:40:49,839 --> 00:40:54,160
about here today

1153
00:40:51,440 --> 00:40:55,520
the stakes have never been higher these

1154
00:40:54,160 --> 00:40:58,160
systems are being built

1155
00:40:55,520 --> 00:40:59,119
not next year not next month not

1156
00:40:58,160 --> 00:41:02,160
tomorrow

1157
00:40:59,119 --> 00:41:02,960
now these problems are not going to go

1158
00:41:02,160 --> 00:41:05,440
away

1159
00:41:02,960 --> 00:41:06,000
they're only going to continue and grow

1160
00:41:05,440 --> 00:41:08,319
but we know

1161
00:41:06,000 --> 00:41:09,599
we can imagine a better world because we

1162
00:41:08,319 --> 00:41:12,880
have to

1163
00:41:09,599 --> 00:41:14,880
the alternative is unacceptable

1164
00:41:12,880 --> 00:41:17,040
so the history of the free software

1165
00:41:14,880 --> 00:41:20,160
movement left us clues

1166
00:41:17,040 --> 00:41:22,240
to learn about how to build a social

1167
00:41:20,160 --> 00:41:24,960
movement to address these problematic

1168
00:41:22,240 --> 00:41:27,439
patterns in our digital society

1169
00:41:24,960 --> 00:41:29,440
let's take these cues and start

1170
00:41:27,440 --> 00:41:31,119
demanding for our freedoms to be

1171
00:41:29,440 --> 00:41:33,040
respected

1172
00:41:31,119 --> 00:41:35,040
just like those who came before us

1173
00:41:33,040 --> 00:41:37,599
nearly 40 years ago

1174
00:41:35,040 --> 00:41:39,359
now i really wish that i could come out

1175
00:41:37,599 --> 00:41:41,440
of this and say here's an organization

1176
00:41:39,359 --> 00:41:44,799
that you can go to and support

1177
00:41:41,440 --> 00:41:48,079
and this will be the way forward

1178
00:41:44,800 --> 00:41:50,800
but i don't what i do have

1179
00:41:48,079 --> 00:41:52,240
is all of the people in this room and

1180
00:41:50,800 --> 00:41:54,000
around the world

1181
00:41:52,240 --> 00:41:55,919
through perseverance through

1182
00:41:54,000 --> 00:41:59,839
perseverance we can begin

1183
00:41:55,920 --> 00:42:01,920
championing this issue in our own way

1184
00:41:59,839 --> 00:42:04,160
for those who agree with us and want to

1185
00:42:01,920 --> 00:42:07,599
commit to standing up for our freedoms

1186
00:42:04,160 --> 00:42:11,399
we do have one last action for you

1187
00:42:07,599 --> 00:42:14,000
pledge we have drafted a pledge at

1188
00:42:11,400 --> 00:42:14,720
freerfuture.world and we want you to

1189
00:42:14,000 --> 00:42:16,160
pledge

1190
00:42:14,720 --> 00:42:18,000
we want you to pledge that you will not

1191
00:42:16,160 --> 00:42:20,560
participate in the creation of software

1192
00:42:18,000 --> 00:42:22,400
which infringes on our freedoms

1193
00:42:20,560 --> 00:42:23,680
we want you to pledge that instead we'll

1194
00:42:22,400 --> 00:42:26,800
build a better world

1195
00:42:23,680 --> 00:42:28,640
for all of us not just a few and we want

1196
00:42:26,800 --> 00:42:31,440
you to push your organizations to do the

1197
00:42:28,640 --> 00:42:33,680
right thing because

1198
00:42:31,440 --> 00:42:35,280
we cannot accept having our future

1199
00:42:33,680 --> 00:42:37,440
bought and sold for other people's

1200
00:42:35,280 --> 00:42:39,760
profit

1201
00:42:37,440 --> 00:42:41,520
and i mean if there's anything we want

1202
00:42:39,760 --> 00:42:44,800
you to take away from this talk

1203
00:42:41,520 --> 00:42:47,440
it's this me justin

1204
00:42:44,800 --> 00:42:48,400
you everyone in this room watching it

1205
00:42:47,440 --> 00:42:50,160
online

1206
00:42:48,400 --> 00:42:52,160
we do have some power to make this

1207
00:42:50,160 --> 00:42:54,160
change so

1208
00:42:52,160 --> 00:42:55,200
let's build a future that we own

1209
00:42:54,160 --> 00:42:59,839
together

1210
00:42:55,200 --> 00:42:59,839
for everyone thank you

1211
00:43:08,400 --> 00:43:12,640
thank you justin flory and mike nolan we

1212
00:43:11,200 --> 00:43:14,640
have a few minutes

1213
00:43:12,640 --> 00:43:17,200
very few minutes so are there any

1214
00:43:14,640 --> 00:43:17,200
questions

1215
00:43:24,839 --> 00:43:27,839
okay

1216
00:43:28,000 --> 00:43:33,280
hello um thank you for a wonderful

1217
00:43:31,119 --> 00:43:34,800
presentation i would like to know uh

1218
00:43:33,280 --> 00:43:37,200
what do you what do you think

1219
00:43:34,800 --> 00:43:39,680
whether uh the big companies that are

1220
00:43:37,200 --> 00:43:42,399
creating huge models for example

1221
00:43:39,680 --> 00:43:43,279
google speech models or and so on

1222
00:43:42,400 --> 00:43:45,200
similar ones

1223
00:43:43,280 --> 00:43:47,280
that cost a lot of electricity to

1224
00:43:45,200 --> 00:43:50,399
produce should they

1225
00:43:47,280 --> 00:43:53,440
in your opinion open source the data or

1226
00:43:50,400 --> 00:43:54,160
open source only the models and whether

1227
00:43:53,440 --> 00:43:57,760
it's

1228
00:43:54,160 --> 00:43:58,240
beneficial because if somebody else

1229
00:43:57,760 --> 00:44:00,720
would

1230
00:43:58,240 --> 00:44:01,759
would recreate those models they would

1231
00:44:00,720 --> 00:44:04,160
also spend

1232
00:44:01,760 --> 00:44:06,960
those same amounts of energy so what do

1233
00:44:04,160 --> 00:44:10,160
you think about this

1234
00:44:06,960 --> 00:44:12,400
yeah so uh your question was a bit

1235
00:44:10,160 --> 00:44:14,240
quiet but i think you're sort of asking

1236
00:44:12,400 --> 00:44:15,680
whether these organizations building

1237
00:44:14,240 --> 00:44:18,078
extremely large

1238
00:44:15,680 --> 00:44:22,560
and kind of powerful models should be

1239
00:44:18,079 --> 00:44:25,280
required to open source their software

1240
00:44:22,560 --> 00:44:27,759
and data too so this is a tough question

1241
00:44:25,280 --> 00:44:29,680
to answer because it's it is so broad

1242
00:44:27,760 --> 00:44:31,760
right and in some circumstances there

1243
00:44:29,680 --> 00:44:33,359
are justifiable concerns for

1244
00:44:31,760 --> 00:44:34,960
saying we shouldn't open up this data

1245
00:44:33,359 --> 00:44:37,040
set and

1246
00:44:34,960 --> 00:44:39,200
while i don't want to give it like a

1247
00:44:37,040 --> 00:44:41,520
broad answer to that same question

1248
00:44:39,200 --> 00:44:43,118
i do think we should consider if you

1249
00:44:41,520 --> 00:44:44,560
can't open up a data set or if it

1250
00:44:43,119 --> 00:44:48,160
contains pii

1251
00:44:44,560 --> 00:44:51,040
or if it's a risk to people is it worth

1252
00:44:48,160 --> 00:44:51,440
creating like a publicly used model that

1253
00:44:51,040 --> 00:44:54,079
is

1254
00:44:51,440 --> 00:44:54,960
creating actions upon that data set a

1255
00:44:54,079 --> 00:44:57,839
lot of our

1256
00:44:54,960 --> 00:44:59,119
the implications of ai software stems

1257
00:44:57,839 --> 00:45:00,960
from us ignoring

1258
00:44:59,119 --> 00:45:03,200
the implications of other things so we

1259
00:45:00,960 --> 00:45:04,240
just kind of put it away into a closet

1260
00:45:03,200 --> 00:45:06,399
i know that's not like the most

1261
00:45:04,240 --> 00:45:09,279
satisfying answer but i i hope it

1262
00:45:06,400 --> 00:45:10,800
kind of addresses it i would say as so

1263
00:45:09,280 --> 00:45:14,400
many people are moving up

1264
00:45:10,800 --> 00:45:16,160
out and in we probably should

1265
00:45:14,400 --> 00:45:18,480
let you ask your questions further

1266
00:45:16,160 --> 00:45:20,078
questions in person so you're probably

1267
00:45:18,480 --> 00:45:22,560
available for further first

1268
00:45:20,079 --> 00:45:25,440
questions so thank you again for the

1269
00:45:22,560 --> 00:45:25,440
good talk and

1270
00:45:25,460 --> 00:45:34,369
[Applause]

1271
00:45:43,680 --> 00:45:49,839
that's very cool i didn't know that this

1272
00:45:46,839 --> 00:45:49,839
is

1273
00:46:00,319 --> 00:46:02,400
you

