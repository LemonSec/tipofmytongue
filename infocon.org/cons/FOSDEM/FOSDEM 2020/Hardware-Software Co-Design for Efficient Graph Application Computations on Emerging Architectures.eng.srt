1
00:00:07,180 --> 00:00:11,570
[Applause]

2
00:00:08,889 --> 00:00:13,399
good afternoon everyone I'm a ninja

3
00:00:11,570 --> 00:00:15,410
Manoah and I'm today I'm going to be

4
00:00:13,400 --> 00:00:18,050
presenting on behalf of the decades team

5
00:00:15,410 --> 00:00:19,610
our work on hardware/software co.design

6
00:00:18,050 --> 00:00:23,079
for efficient graph application

7
00:00:19,610 --> 00:00:25,579
computations on emerging architectures

8
00:00:23,079 --> 00:00:28,369
I'm gonna give a brief overview of what

9
00:00:25,579 --> 00:00:30,140
the decades project is so decades is

10
00:00:28,369 --> 00:00:32,329
part of the DARPA software-defined

11
00:00:30,140 --> 00:00:34,760
hardware program which aims to design

12
00:00:32,329 --> 00:00:36,530
runtime reconfigurable hardware that can

13
00:00:34,760 --> 00:00:38,510
accelerate a variety of data intensive

14
00:00:36,530 --> 00:00:40,010
software applications in the broad

15
00:00:38,510 --> 00:00:42,980
domains of machine learning and graphic

16
00:00:40,010 --> 00:00:44,839
analytics so the decades approach is to

17
00:00:42,980 --> 00:00:46,898
design a heterogeneous tile based chip

18
00:00:44,839 --> 00:00:48,920
that is a combination of core

19
00:00:46,899 --> 00:00:50,780
accelerator and intelligent storage

20
00:00:48,920 --> 00:00:53,210
tiles as you can see on this image on

21
00:00:50,780 --> 00:00:55,999
the right and this is a collaborative

22
00:00:53,210 --> 00:00:58,519
effort between researchers at Princeton

23
00:00:55,999 --> 00:01:00,559
and Columbia University so all of our

24
00:00:58,519 --> 00:01:06,020
tools are or will be in the very near

25
00:01:00,559 --> 00:01:07,700
future open source at this link below so

26
00:01:06,020 --> 00:01:09,289
many machine learning and data science

27
00:01:07,700 --> 00:01:11,840
applications need to process large

28
00:01:09,290 --> 00:01:14,530
amounts of dense data in the for example

29
00:01:11,840 --> 00:01:16,580
images composed of many pixels

30
00:01:14,530 --> 00:01:18,620
fortunately huge strides have been made

31
00:01:16,580 --> 00:01:21,130
in processing this type these types of

32
00:01:18,620 --> 00:01:23,480
data like neural network accelerators

33
00:01:21,130 --> 00:01:25,970
meanwhile graphs can efficiently

34
00:01:23,480 --> 00:01:28,100
represent big data although their data

35
00:01:25,970 --> 00:01:30,310
layouts are often sparse and so they

36
00:01:28,100 --> 00:01:32,899
require different computing paradigms

37
00:01:30,310 --> 00:01:35,000
due to the ubiquity of graph databases

38
00:01:32,900 --> 00:01:36,500
and data structures graph applications

39
00:01:35,000 --> 00:01:39,050
are at the heart of many big data

40
00:01:36,500 --> 00:01:42,080
analytics as you all know for example

41
00:01:39,050 --> 00:01:43,520
recommendation systems here's an example

42
00:01:42,080 --> 00:01:46,490
of Twitter's use of a recommendation

43
00:01:43,520 --> 00:01:48,289
system so if a user goes to positives

44
00:01:46,490 --> 00:01:53,899
Twitter page they will be recommended

45
00:01:48,290 --> 00:01:57,410
other free and open source software so

46
00:01:53,900 --> 00:01:59,470
in order to process big data modern

47
00:01:57,410 --> 00:02:01,550
technology trends have employed

48
00:01:59,470 --> 00:02:03,470
specialized hardware which has led to

49
00:02:01,550 --> 00:02:06,009
accelerator oriented heterogeneity and

50
00:02:03,470 --> 00:02:09,019
parallelism as you can see in the graph

51
00:02:06,010 --> 00:02:11,360
the purple black and orange lines show

52
00:02:09,019 --> 00:02:14,330
the trends with these performance over

53
00:02:11,360 --> 00:02:16,219
time and these have significantly

54
00:02:14,330 --> 00:02:18,170
benefited compute bound workloads but as

55
00:02:16,219 --> 00:02:19,608
you can see by the Green Line there's a

56
00:02:18,170 --> 00:02:20,899
gap between processor and memory

57
00:02:19,609 --> 00:02:22,760
performance

58
00:02:20,900 --> 00:02:25,459
and in the context of amtel's law as

59
00:02:22,760 --> 00:02:27,470
computers growing faster their access

60
00:02:25,459 --> 00:02:30,980
the relative of memory access time is

61
00:02:27,470 --> 00:02:33,579
only growing slower unfortunately many

62
00:02:30,980 --> 00:02:36,530
graphs applications are memory bound and

63
00:02:33,579 --> 00:02:38,780
furthermore these graphic locations need

64
00:02:36,530 --> 00:02:40,549
to process data sets that are massive

65
00:02:38,780 --> 00:02:41,900
and continuing to grow exponentially so

66
00:02:40,549 --> 00:02:45,200
like the Twitter network contains

67
00:02:41,900 --> 00:02:47,689
millions of nodes and the ability to

68
00:02:45,200 --> 00:02:49,220
process these networks hasn't kept up so

69
00:02:47,689 --> 00:02:50,959
we need efficient graph processing

70
00:02:49,220 --> 00:02:55,879
techniques that can keep up with our

71
00:02:50,959 --> 00:02:57,920
modern data sets so in order to design

72
00:02:55,879 --> 00:02:59,780
efficient graph processing techniques we

73
00:02:57,920 --> 00:03:01,608
need to understand their bottlenecks and

74
00:02:59,780 --> 00:03:03,170
because many graph applications are

75
00:03:01,609 --> 00:03:07,159
memory bound we look at their data

76
00:03:03,170 --> 00:03:08,958
access patterns so as you saw in the

77
00:03:07,159 --> 00:03:10,638
last presentation we were introduced to

78
00:03:08,959 --> 00:03:12,739
the idea of a frontier so we look at

79
00:03:10,639 --> 00:03:14,930
many graph applications that are from

80
00:03:12,739 --> 00:03:16,879
iterative and frontier based and this

81
00:03:14,930 --> 00:03:18,739
includes the widespread breadth-first

82
00:03:16,879 --> 00:03:21,649
search single source shortest paths and

83
00:03:18,739 --> 00:03:24,560
PageRank algorithms so what does it mean

84
00:03:21,650 --> 00:03:27,379
to be iterative frontier based well like

85
00:03:24,560 --> 00:03:28,910
we saw we have a frontier of node we

86
00:03:27,379 --> 00:03:31,340
have multiple iterations to traverse the

87
00:03:28,910 --> 00:03:33,168
graph and within each iteration of the

88
00:03:31,340 --> 00:03:35,030
graph our iteration of the algorithm we

89
00:03:33,169 --> 00:03:38,239
have a frontier of nodes that contains

90
00:03:35,030 --> 00:03:40,310
the IDS we want to process and then we

91
00:03:38,239 --> 00:03:43,400
also have this flat array node Val's

92
00:03:40,310 --> 00:03:45,079
which stores the per node properties so

93
00:03:43,400 --> 00:03:46,549
depending on the objective of the

94
00:03:45,079 --> 00:03:48,859
algorithm we store a different type of

95
00:03:46,549 --> 00:03:50,479
data for each node so in breadth-first

96
00:03:48,859 --> 00:03:53,629
search this would be the number of hops

97
00:03:50,479 --> 00:03:55,699
away from our given source node and then

98
00:03:53,629 --> 00:03:57,530
on the right we have the kernel template

99
00:03:55,699 --> 00:03:59,419
for these iterative reference to your

100
00:03:57,530 --> 00:04:01,609
base graft applications so I'm going to

101
00:03:59,419 --> 00:04:02,840
walk through this template in the

102
00:04:01,609 --> 00:04:05,900
context of the breadth-first search

103
00:04:02,840 --> 00:04:08,959
algorithm so we're starting with our

104
00:04:05,900 --> 00:04:11,299
root node of 0 and so for every node in

105
00:04:08,959 --> 00:04:13,790
our frontier will you do a processing of

106
00:04:11,299 --> 00:04:15,709
that node and then we look at all of

107
00:04:13,790 --> 00:04:19,339
those note all of that nodes neighbors

108
00:04:15,709 --> 00:04:22,039
and so in this case we well we do an

109
00:04:19,339 --> 00:04:24,560
update of update neighbor function on

110
00:04:22,039 --> 00:04:26,150
that neighbor and the exact details of

111
00:04:24,560 --> 00:04:27,710
this function depend on the objective of

112
00:04:26,150 --> 00:04:30,020
the algorithm in the case of

113
00:04:27,710 --> 00:04:32,150
breadth-first search we do a load to the

114
00:04:30,020 --> 00:04:34,400
node Val's array in order to determine

115
00:04:32,150 --> 00:04:36,409
if the node has been visited and

116
00:04:34,400 --> 00:04:38,810
if it has not been visited then we need

117
00:04:36,410 --> 00:04:41,650
to store the number of hops away that

118
00:04:38,810 --> 00:04:44,720
note is from our given source node and

119
00:04:41,650 --> 00:04:46,609
so because these updates depend on the

120
00:04:44,720 --> 00:04:49,850
location of these neighbors these

121
00:04:46,610 --> 00:04:52,490
updates need an indirect memory access

122
00:04:49,850 --> 00:04:54,770
and this leads to as you can see in this

123
00:04:52,490 --> 00:04:59,210
flat array this leads to irregular

124
00:04:54,770 --> 00:05:01,310
accesses within this array so this is

125
00:04:59,210 --> 00:05:05,299
going to be the key thing to think about

126
00:05:01,310 --> 00:05:07,490
going forward and then looking at the

127
00:05:05,300 --> 00:05:09,169
notes that have not been visited we add

128
00:05:07,490 --> 00:05:12,110
those to the frontier for the next

129
00:05:09,169 --> 00:05:15,320
iteration of the algorithm and then this

130
00:05:12,110 --> 00:05:20,750
process continues until we regeneration

131
00:05:15,320 --> 00:05:22,820
where our frontier is empty so why are

132
00:05:20,750 --> 00:05:25,100
irregular memory accesses problematic

133
00:05:22,820 --> 00:05:27,710
well modern memory hierarchies are

134
00:05:25,100 --> 00:05:30,260
composed of multiple caches and caches

135
00:05:27,710 --> 00:05:32,210
are designed to store frequently

136
00:05:30,260 --> 00:05:34,370
accessed data that is stored in

137
00:05:32,210 --> 00:05:36,440
contiguous blocks so when your memory

138
00:05:34,370 --> 00:05:39,530
accesses are irregular caches are not

139
00:05:36,440 --> 00:05:42,050
amenable to these accesses you can see

140
00:05:39,530 --> 00:05:45,440
this is highlighted by this sample

141
00:05:42,050 --> 00:05:47,330
memory hierarchy below as we have to

142
00:05:45,440 --> 00:05:48,919
traverse the memory hierarchy and as we

143
00:05:47,330 --> 00:05:50,750
missed in each level of the cache we

144
00:05:48,919 --> 00:05:53,510
eventually go off chip to the main

145
00:05:50,750 --> 00:05:55,729
memory and so if you recall in the

146
00:05:53,510 --> 00:05:57,889
previous kernel-based in the kernel

147
00:05:55,729 --> 00:05:59,120
template the update neighbors function

148
00:05:57,889 --> 00:06:01,729
that performed the irregular memory

149
00:05:59,120 --> 00:06:03,979
accesses was inside a nested loop and so

150
00:06:01,729 --> 00:06:05,719
it occurred very frequently and so we

151
00:06:03,979 --> 00:06:09,229
define a regular memory accesses that

152
00:06:05,720 --> 00:06:12,680
occur frequently as llamas this is our

153
00:06:09,229 --> 00:06:16,130
acronym for them and so to quantify why

154
00:06:12,680 --> 00:06:17,900
llamas are problematic we look at five

155
00:06:16,130 --> 00:06:20,659
different graph and sparse applications

156
00:06:17,900 --> 00:06:23,388
and break down their run times into

157
00:06:20,660 --> 00:06:24,979
compute versus memory so all of the

158
00:06:23,389 --> 00:06:27,500
compute is highlighted by the orange

159
00:06:24,979 --> 00:06:30,320
bars and the memory accesses are broken

160
00:06:27,500 --> 00:06:32,840
into the llamas versus our non llamas in

161
00:06:30,320 --> 00:06:34,460
the yellow and as you can see the llamas

162
00:06:32,840 --> 00:06:38,960
are dominating the run time for all of

163
00:06:34,460 --> 00:06:41,599
these applications this graph below

164
00:06:38,960 --> 00:06:44,570
shows specifically the llamas last level

165
00:06:41,599 --> 00:06:46,039
cache miss rate so if you look at all of

166
00:06:44,570 --> 00:06:47,689
these five different applications you

167
00:06:46,039 --> 00:06:51,169
can see that the last level cache

168
00:06:47,689 --> 00:06:53,719
rate is 0.5 or above which means that 50

169
00:06:51,169 --> 00:06:55,878
more than 50% of the time these llamas

170
00:06:53,719 --> 00:06:59,239
are performing an expensive long lengthy

171
00:06:55,879 --> 00:07:00,829
memory access to main memory so because

172
00:06:59,239 --> 00:07:02,959
llamas have a disproportionately large

173
00:07:00,829 --> 00:07:04,729
impact on the performance of these graph

174
00:07:02,959 --> 00:07:08,629
applications our work seeks to

175
00:07:04,729 --> 00:07:11,659
specifically address them and thus we

176
00:07:08,629 --> 00:07:14,749
introduced our approach fast llamas so

177
00:07:11,659 --> 00:07:17,209
fast llamas is another acronym short for

178
00:07:14,749 --> 00:07:18,829
full stack approach and specialization

179
00:07:17,209 --> 00:07:22,039
techniques for hiding long latency

180
00:07:18,829 --> 00:07:23,869
memory accesses so this at a high level

181
00:07:22,039 --> 00:07:26,479
is a data supply approach that

182
00:07:23,869 --> 00:07:28,959
efficiently Maps graph applications on

183
00:07:26,479 --> 00:07:31,579
two pairs of producer and consumer cores

184
00:07:28,959 --> 00:07:33,919
and we have a programming model that can

185
00:07:31,579 --> 00:07:35,899
allow for more explicit mapping of these

186
00:07:33,919 --> 00:07:37,219
applications as well as specialized

187
00:07:35,899 --> 00:07:39,499
hardware support that can a

188
00:07:37,219 --> 00:07:42,369
synchronously issue irregular memory

189
00:07:39,499 --> 00:07:44,479
accesses and we do get results of

190
00:07:42,369 --> 00:07:48,289
impressive speed ups from this and I

191
00:07:44,479 --> 00:07:50,149
will show those at the end first I'm

192
00:07:48,289 --> 00:07:54,349
going to give a brief overview of the

193
00:07:50,149 --> 00:07:56,959
decoupling technique so decoupling is

194
00:07:54,349 --> 00:07:58,849
the TEC is a technique where a program

195
00:07:56,959 --> 00:08:01,009
is statically divided into two

196
00:07:58,849 --> 00:08:03,019
independent instruction streams one of

197
00:08:01,009 --> 00:08:05,300
these streams is mapped onto a producer

198
00:08:03,019 --> 00:08:08,029
core and this core is responsible for

199
00:08:05,300 --> 00:08:10,759
all memory accesses and the necessary

200
00:08:08,029 --> 00:08:12,889
address computation for these accesses

201
00:08:10,759 --> 00:08:14,839
and then the consumer core is

202
00:08:12,889 --> 00:08:17,629
responsible for all the valued

203
00:08:14,839 --> 00:08:20,089
computation so these cores run

204
00:08:17,629 --> 00:08:21,739
independently and in parallel and so

205
00:08:20,089 --> 00:08:24,860
this creates a form of heterogeneous

206
00:08:21,739 --> 00:08:26,929
parallelism it's to Ellis trait the

207
00:08:24,860 --> 00:08:29,389
contrast these two execution timelines

208
00:08:26,929 --> 00:08:31,758
below show on the Left homogeneous

209
00:08:29,389 --> 00:08:35,419
parallelism where the two threads the

210
00:08:31,759 --> 00:08:37,399
top and the bottom rows are performing

211
00:08:35,419 --> 00:08:39,049
the same types of computation and memory

212
00:08:37,399 --> 00:08:41,990
access whereas when you have

213
00:08:39,049 --> 00:08:43,818
heterogeneous parallelism one thread is

214
00:08:41,990 --> 00:08:45,889
responsible for the memory accesses this

215
00:08:43,818 --> 00:08:47,180
is the producer core and then the other

216
00:08:45,889 --> 00:08:50,569
thread is doing the cup of the

217
00:08:47,180 --> 00:08:52,550
computation so homogeneous parallelism

218
00:08:50,569 --> 00:08:54,709
is great when you have a very compute

219
00:08:52,550 --> 00:08:56,870
bound application whereas when you have

220
00:08:54,709 --> 00:09:00,060
a memory bound application heterogeneous

221
00:08:56,870 --> 00:09:02,580
parallelism comes into play

222
00:09:00,060 --> 00:09:05,099
so that main idea of decoupling is to

223
00:09:02,580 --> 00:09:07,890
tolerate memory latency and this is done

224
00:09:05,100 --> 00:09:09,660
by having the producer issue requests to

225
00:09:07,890 --> 00:09:12,390
the memory hierarchies and retrieving

226
00:09:09,660 --> 00:09:15,209
the data before the consumer needs it

227
00:09:12,390 --> 00:09:17,640
and so these cores utilize a specialized

228
00:09:15,210 --> 00:09:20,070
hardware the communication queue in

229
00:09:17,640 --> 00:09:22,439
order to have the producer store the

230
00:09:20,070 --> 00:09:27,480
data before the consumer needs to eat

231
00:09:22,440 --> 00:09:28,740
this data and this timeline on the

232
00:09:27,480 --> 00:09:31,140
bottom right that I pointed to earlier

233
00:09:28,740 --> 00:09:34,350
just illustrates how this can tolerate

234
00:09:31,140 --> 00:09:36,090
memory latency so there's a warm-up

235
00:09:34,350 --> 00:09:38,220
period where these two cores start

236
00:09:36,090 --> 00:09:40,050
running at the same time so the producer

237
00:09:38,220 --> 00:09:42,210
needs to gather its run ahead but it

238
00:09:40,050 --> 00:09:45,120
does this very quickly and once it's

239
00:09:42,210 --> 00:09:47,300
done this then once the producer can

240
00:09:45,120 --> 00:09:50,240
asynchronously issue memory accesses and

241
00:09:47,300 --> 00:09:53,310
the consumers then never has to stall

242
00:09:50,240 --> 00:09:55,080
whereas typically these long lean these

243
00:09:53,310 --> 00:09:56,449
memory accesses would be long latency

244
00:09:55,080 --> 00:10:01,440
and the consumer would have to wait

245
00:09:56,450 --> 00:10:03,450
waiting for this data because decoupling

246
00:10:01,440 --> 00:10:05,310
creates two different instruction

247
00:10:03,450 --> 00:10:07,560
streams that are independent but the

248
00:10:05,310 --> 00:10:10,229
original dependencies in the program are

249
00:10:07,560 --> 00:10:12,180
now changed and remapped so that

250
00:10:10,230 --> 00:10:15,030
dependencies are only respective to the

251
00:10:12,180 --> 00:10:17,189
individual slices so there might be a

252
00:10:15,030 --> 00:10:19,170
dependency on the producers memory

253
00:10:17,190 --> 00:10:21,390
access but now this could be mapped onto

254
00:10:19,170 --> 00:10:23,579
the consumer and so when this does

255
00:10:21,390 --> 00:10:26,699
happen we take advantage of this with

256
00:10:23,580 --> 00:10:29,670
asynchronous accesses so asynchronous

257
00:10:26,700 --> 00:10:32,040
accesses are memory accesses where the

258
00:10:29,670 --> 00:10:34,709
data is not later used by the producer

259
00:10:32,040 --> 00:10:36,810
so this is where the producer can hand

260
00:10:34,710 --> 00:10:38,850
it off to the consumer and move on to

261
00:10:36,810 --> 00:10:41,939
issue its other memory it's later memory

262
00:10:38,850 --> 00:10:43,740
accesses and so as a result it doesn't

263
00:10:41,940 --> 00:10:46,560
have to occupy its Hardware structures

264
00:10:43,740 --> 00:10:49,050
or its pipeline resources and this is

265
00:10:46,560 --> 00:10:50,760
illustrated on the right where we have

266
00:10:49,050 --> 00:10:53,880
two different execution timelines the

267
00:10:50,760 --> 00:10:55,890
top one shows the scenario where there

268
00:10:53,880 --> 00:10:58,590
are no asynchronous memory accesses so

269
00:10:55,890 --> 00:11:00,840
as you can see that each memory access

270
00:10:58,590 --> 00:11:03,000
the producer needs to issue depends on

271
00:11:00,840 --> 00:11:04,830
the previous one and this leads to

272
00:11:03,000 --> 00:11:07,830
frequent stalling both on the producer

273
00:11:04,830 --> 00:11:10,440
and the consumer whereas when we have

274
00:11:07,830 --> 00:11:12,840
asynchronous memory accesses the

275
00:11:10,440 --> 00:11:13,920
producer can issue a request and move on

276
00:11:12,840 --> 00:11:15,690
to its next one with

277
00:11:13,920 --> 00:11:19,939
having to wait for the previous one to

278
00:11:15,690 --> 00:11:22,800
finish and following sorry

279
00:11:19,940 --> 00:11:26,690
following this warmup period the

280
00:11:22,800 --> 00:11:29,189
consumer never has to stall as a result

281
00:11:26,690 --> 00:11:30,690
so now I'm going to talk about how fast

282
00:11:29,190 --> 00:11:36,420
Mama's leverages this decoupling

283
00:11:30,690 --> 00:11:39,570
technique to tolerate latency so to

284
00:11:36,420 --> 00:11:41,120
illustrate or to provide a contrast this

285
00:11:39,570 --> 00:11:44,310
is the original

286
00:11:41,120 --> 00:11:45,300
kernel for the iterative graph it are

287
00:11:44,310 --> 00:11:48,750
different to your based graph

288
00:11:45,300 --> 00:11:51,420
applications this is broken down into

289
00:11:48,750 --> 00:11:53,880
three high-level functions the process

290
00:11:51,420 --> 00:11:56,670
node which is highlighted by the orange

291
00:11:53,880 --> 00:11:59,339
boxes and then we have the update

292
00:11:56,670 --> 00:12:01,680
neighbors which is our llama this is

293
00:11:59,339 --> 00:12:04,260
highlighted by the red boxes and then

294
00:12:01,680 --> 00:12:06,779
the conditional a conditional addition

295
00:12:04,260 --> 00:12:09,870
of nodes to the frontier is highlighted

296
00:12:06,779 --> 00:12:13,589
by the blue boxes and so when we execute

297
00:12:09,870 --> 00:12:16,620
this template on an inorder core we can

298
00:12:13,589 --> 00:12:21,540
see that the llamas are dominating the

299
00:12:16,620 --> 00:12:23,910
runtime but fast llamas decouples this

300
00:12:21,540 --> 00:12:26,519
program so that the process node

301
00:12:23,910 --> 00:12:28,529
function it's mapped onto the producer

302
00:12:26,519 --> 00:12:31,170
so in this execution timeline on the

303
00:12:28,529 --> 00:12:33,600
lower left we have the producer is the

304
00:12:31,170 --> 00:12:37,110
top row and the consumer is the bottom

305
00:12:33,600 --> 00:12:38,850
row and then the middle row this wide

306
00:12:37,110 --> 00:12:40,800
row shows what's happening

307
00:12:38,850 --> 00:12:43,649
asynchronously in the memory hierarchy

308
00:12:40,800 --> 00:12:45,209
so this isn't mapped onto a core the

309
00:12:43,649 --> 00:12:48,390
producer and the consumer are the two

310
00:12:45,209 --> 00:12:50,099
cores running in parallel and so the

311
00:12:48,390 --> 00:12:54,180
producer does the note processing and

312
00:12:50,100 --> 00:12:55,980
then it can issue llamas this is shown

313
00:12:54,180 --> 00:12:58,229
by the small boxes with the indet

314
00:12:55,980 --> 00:13:01,110
written on them so it can issue a

315
00:12:58,230 --> 00:13:03,029
regular memory access and then continue

316
00:13:01,110 --> 00:13:04,980
to issue on its next one and these are

317
00:13:03,029 --> 00:13:07,199
not time consuming operations and then

318
00:13:04,980 --> 00:13:09,360
the llamas are issued or running

319
00:13:07,199 --> 00:13:11,880
asynchronously in the memory hierarchy

320
00:13:09,360 --> 00:13:14,550
and then when their data comes back the

321
00:13:11,880 --> 00:13:17,490
consumer can eat the data and continue

322
00:13:14,550 --> 00:13:19,829
on with its respective functions so

323
00:13:17,490 --> 00:13:21,329
there's a warm-up period again where the

324
00:13:19,829 --> 00:13:23,729
producer needs to gain its initial run

325
00:13:21,329 --> 00:13:24,689
ahead but then it could continue from

326
00:13:23,730 --> 00:13:26,819
there

327
00:13:24,690 --> 00:13:29,940
and the llamas are asynchronously issued

328
00:13:26,819 --> 00:13:32,069
after this warmup period and as a result

329
00:13:29,940 --> 00:13:34,800
the consumer is never stalling waiting

330
00:13:32,069 --> 00:13:40,349
for these llamas and fast llamas is able

331
00:13:34,800 --> 00:13:42,990
to tolerate memory latency so this is

332
00:13:40,350 --> 00:13:44,790
this is a relatively detailed hardware

333
00:13:42,990 --> 00:13:47,279
diagram I'm not going to talk about all

334
00:13:44,790 --> 00:13:49,230
of the individual parts of it but I'm

335
00:13:47,279 --> 00:13:51,870
gonna go over the main additions that

336
00:13:49,230 --> 00:13:56,069
fast llamas uses to support this in

337
00:13:51,870 --> 00:13:58,110
hardware so we have an specialised

338
00:13:56,069 --> 00:14:00,628
buffer called the asynchronous access

339
00:13:58,110 --> 00:14:03,329
buffer and so this is when this is used

340
00:14:00,629 --> 00:14:05,730
when the producer issues a memory access

341
00:14:03,329 --> 00:14:08,969
and then this asynchronous access buffer

342
00:14:05,730 --> 00:14:11,910
can store the addresses of the inflight

343
00:14:08,970 --> 00:14:13,050
memory requests and then when the data

344
00:14:11,910 --> 00:14:15,990
comes back from the memory hierarchy

345
00:14:13,050 --> 00:14:18,660
then these this data is matched with its

346
00:14:15,990 --> 00:14:20,310
corresponding address and then the data

347
00:14:18,660 --> 00:14:22,019
is passed to the communication queue

348
00:14:20,310 --> 00:14:25,459
between the producer and consumer cores

349
00:14:22,019 --> 00:14:28,110
and then the consumer can use this data

350
00:14:25,459 --> 00:14:31,619
so when we have an asynchronous memory

351
00:14:28,110 --> 00:14:33,389
access its issued by the producer sent

352
00:14:31,620 --> 00:14:35,279
to the memory hierarchy its address is

353
00:14:33,389 --> 00:14:38,399
tracked like I mentioned and then when

354
00:14:35,279 --> 00:14:40,949
the data comes back sometimes the data

355
00:14:38,399 --> 00:14:43,439
might be modified and this can be sent

356
00:14:40,949 --> 00:14:48,240
directly to the memory hierarchy or onto

357
00:14:43,439 --> 00:14:51,060
the queue the communication queue so now

358
00:14:48,240 --> 00:14:54,600
I'm gonna show some results of this

359
00:14:51,060 --> 00:14:56,609
approach we looked at five different

360
00:14:54,600 --> 00:14:58,949
graph in sparse applications these are

361
00:14:56,610 --> 00:15:00,990
the five that I mentioned before with

362
00:14:58,949 --> 00:15:03,300
the llama graph so we have two

363
00:15:00,990 --> 00:15:05,490
applications on top element-wise farce

364
00:15:03,300 --> 00:15:07,829
dense is a matrix multiplication between

365
00:15:05,490 --> 00:15:09,930
a sparse matrix and a dense matrix and

366
00:15:07,829 --> 00:15:11,880
then we have bipartite graph projections

367
00:15:09,930 --> 00:15:13,800
which are which is an algorithm that

368
00:15:11,880 --> 00:15:16,019
operates on a bipartite graph and it

369
00:15:13,800 --> 00:15:18,258
relates nodes in one graph based off of

370
00:15:16,019 --> 00:15:21,000
their common neighbors and the other one

371
00:15:18,259 --> 00:15:23,939
and then we have a vertex programmable

372
00:15:21,000 --> 00:15:25,980
graph processing algorithm so we use

373
00:15:23,939 --> 00:15:27,209
three of the most widespread algorithms

374
00:15:25,980 --> 00:15:30,269
breadth first search single source

375
00:15:27,209 --> 00:15:32,399
shortest paths and PageRank and the

376
00:15:30,269 --> 00:15:34,500
difference between these algorithms and

377
00:15:32,399 --> 00:15:36,689
the two above are that these algorithms

378
00:15:34,500 --> 00:15:38,760
require an explicit annotation by the

379
00:15:36,689 --> 00:15:41,010
programmer so our programming

380
00:15:38,760 --> 00:15:43,980
support sack of annotation that allows

381
00:15:41,010 --> 00:15:47,580
the programmer to explicitly guide

382
00:15:43,980 --> 00:15:49,920
mapping so it can tell the compiler that

383
00:15:47,580 --> 00:15:53,820
performs our decoupling to map and

384
00:15:49,920 --> 00:15:57,420
memory access onto the consumer and then

385
00:15:53,820 --> 00:15:59,100
the top two applications do not they can

386
00:15:57,420 --> 00:16:04,439
automatically be sliced with our

387
00:15:59,100 --> 00:16:06,870
compiler so going back to the decades

388
00:16:04,440 --> 00:16:08,970
Hardware we have the notion of quartiles

389
00:16:06,870 --> 00:16:12,060
and so these quartiles can be

390
00:16:08,970 --> 00:16:13,760
reconfigured so we can have them as two

391
00:16:12,060 --> 00:16:16,469
parallel quartiles that run

392
00:16:13,760 --> 00:16:18,630
simultaneously or we could have a fast

393
00:16:16,470 --> 00:16:21,720
llamas pair which is a producer quartile

394
00:16:18,630 --> 00:16:24,770
and a consumer quartile and so we

395
00:16:21,720 --> 00:16:27,600
evaluate both of these configurations

396
00:16:24,770 --> 00:16:29,790
when we compare these two we could see

397
00:16:27,600 --> 00:16:32,730
that highlighted by the blue and the

398
00:16:29,790 --> 00:16:36,060
yellow bars in this graph which this

399
00:16:32,730 --> 00:16:38,280
graph shows the geo mean of each of

400
00:16:36,060 --> 00:16:39,900
these five applications so we run we ran

401
00:16:38,280 --> 00:16:42,150
these applications on multiple different

402
00:16:39,900 --> 00:16:43,980
types of data sets a combination of real

403
00:16:42,150 --> 00:16:46,770
and synthetic networks but we're just

404
00:16:43,980 --> 00:16:49,080
showing their geo means here and looking

405
00:16:46,770 --> 00:16:50,520
at the geo means we can see that fast

406
00:16:49,080 --> 00:16:54,200
llamas outperform traditional

407
00:16:50,520 --> 00:16:56,610
parallelism by up to two categories and

408
00:16:54,200 --> 00:16:58,680
then because graph applications are

409
00:16:56,610 --> 00:17:00,090
memory bound we look at an inorder core

410
00:16:58,680 --> 00:17:02,280
with the perfect cache because this

411
00:17:00,090 --> 00:17:05,069
provides a performance idealization as

412
00:17:02,280 --> 00:17:07,949
if every memory access had only latency

413
00:17:05,069 --> 00:17:09,659
of one cycle and so we can see that

414
00:17:07,949 --> 00:17:13,199
looking across the board at the orange

415
00:17:09,660 --> 00:17:15,720
and yellow bars fast I was able to

416
00:17:13,199 --> 00:17:19,020
achieve up to 96.2%

417
00:17:15,720 --> 00:17:21,120
of perfect cache performance and then

418
00:17:19,020 --> 00:17:23,459
looking at fast llamas compared to our

419
00:17:21,119 --> 00:17:26,399
baseline performance which is that of a

420
00:17:23,459 --> 00:17:29,370
single inorder quartile we see up to a

421
00:17:26,400 --> 00:17:31,110
5.3 2x performance improvement here but

422
00:17:29,370 --> 00:17:33,479
when we looked at individual application

423
00:17:31,110 --> 00:17:39,540
input combinations we saw up to an eight

424
00:17:33,480 --> 00:17:42,000
point six exact speedo so this work was

425
00:17:39,540 --> 00:17:45,149
supported by DARPA as I ventured before

426
00:17:42,000 --> 00:17:46,500
and so in conclusion fast llamas is

427
00:17:45,150 --> 00:17:48,630
Hardware soft it's a hardware software

428
00:17:46,500 --> 00:17:51,559
co.design approach that tolerates

429
00:17:48,630 --> 00:17:53,779
latency on graph applications with

430
00:17:51,559 --> 00:17:56,029
it's programming model its compiler that

431
00:17:53,779 --> 00:17:58,730
can perform the automatic slicing into

432
00:17:56,029 --> 00:18:00,230
producer-consumer pairs and this the

433
00:17:58,730 --> 00:18:03,769
specialized hardware support for

434
00:18:00,230 --> 00:18:06,559
asynchronous memory accesses are team

435
00:18:03,769 --> 00:18:08,149
members so decades is a large effort

436
00:18:06,559 --> 00:18:10,730
between prison and Columbia so our key

437
00:18:08,149 --> 00:18:13,699
members are listed here and then you can

438
00:18:10,730 --> 00:18:15,830
access our applications our compiler and

439
00:18:13,700 --> 00:18:18,529
the simulator we use to get these

440
00:18:15,830 --> 00:18:22,129
performance results at these links below

441
00:18:18,529 --> 00:18:24,019
and this is also being implemented to be

442
00:18:22,129 --> 00:18:25,580
designed on our chip so the RTL is in

443
00:18:24,019 --> 00:18:50,330
progress but that will be available soon

444
00:18:25,580 --> 00:18:52,850
as well so the question was can this

445
00:18:50,330 --> 00:18:54,860
architecture be or mitigate latency in

446
00:18:52,850 --> 00:18:58,969
Deptford surging strongly connected

447
00:18:54,860 --> 00:19:00,769
components so this will will see the

448
00:18:58,970 --> 00:19:03,200
most impressive speed ups when you have

449
00:19:00,769 --> 00:19:04,820
these llamas these like long links

450
00:19:03,200 --> 00:19:08,269
memory axises dong dominating the

451
00:19:04,820 --> 00:19:10,100
performance this could be I guess it

452
00:19:08,269 --> 00:19:11,629
depends on the implementation of the

453
00:19:10,100 --> 00:19:13,399
algorithm but we study the most work

454
00:19:11,629 --> 00:19:16,279
efficient ones where the long least

455
00:19:13,399 --> 00:19:18,529
memory accesses are exposed so I think

456
00:19:16,279 --> 00:19:20,149
in depth-first search the long lean

457
00:19:18,529 --> 00:19:26,259
scenarios are not as much of a problem

458
00:19:20,149 --> 00:19:26,258
there but it could work

459
00:19:50,130 --> 00:19:54,740
and then my question is about how you

460
00:20:06,100 --> 00:20:11,879
so I think you're asking about cases

461
00:20:08,140 --> 00:20:11,880
where the consumer needs

462
00:20:31,299 --> 00:20:35,249
you you because of the memory

463
00:20:43,200 --> 00:20:49,050
oh so you are you asking if there's like

464
00:20:47,220 --> 00:20:54,000
a limited window which the decoupling

465
00:20:49,050 --> 00:20:56,040
can attack okay so the when we do the

466
00:20:54,000 --> 00:20:59,880
decoupling the compiler so the program

467
00:20:56,040 --> 00:21:02,870
is sliced so the compiler producer has

468
00:20:59,880 --> 00:21:06,930
it sees its list of a memory accesses to

469
00:21:02,870 --> 00:21:08,489
issue one an instance is where the

470
00:21:06,930 --> 00:21:10,770
programming model actually comes in so

471
00:21:08,490 --> 00:21:13,350
we have an annotation in our programming

472
00:21:10,770 --> 00:21:15,510
model that can map or tell the compiler

473
00:21:13,350 --> 00:21:18,300
just to put certain memory accesses on

474
00:21:15,510 --> 00:21:21,120
the consumer and so in that case we

475
00:21:18,300 --> 00:21:23,040
would leverage that annotation so that

476
00:21:21,120 --> 00:21:24,629
the consumer could just do these memory

477
00:21:23,040 --> 00:21:27,139
accesses and not have to wait for the

478
00:21:24,630 --> 00:21:27,140
producer

