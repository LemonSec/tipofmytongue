1
00:00:05,270 --> 00:00:12,809
new ways of looking at things that we in

2
00:00:09,510 --> 00:00:14,520
the performance team love so daddy okay

3
00:00:12,809 --> 00:00:16,230
thank you thanks very much thanks for

4
00:00:14,520 --> 00:00:18,359
having me here it's very I'm really

5
00:00:16,230 --> 00:00:20,039
thrilled so what I'm gonna do is that

6
00:00:18,359 --> 00:00:22,140
I'm gonna bring basically two viewpoints

7
00:00:20,039 --> 00:00:24,689
I've been working on academia for the

8
00:00:22,140 --> 00:00:28,109
last 15 years or so and the last year I

9
00:00:24,689 --> 00:00:30,150
moved to what way so I basically have in

10
00:00:28,109 --> 00:00:32,040
the street view points with a university

11
00:00:30,150 --> 00:00:34,170
mindset and so today what they're gonna

12
00:00:32,040 --> 00:00:36,570
do is that I'm gonna talk about metrics

13
00:00:34,170 --> 00:00:38,100
and models for web and there's a little

14
00:00:36,570 --> 00:00:40,110
longer subtitle that I'm not going to

15
00:00:38,100 --> 00:00:42,270
comment here we're gonna see throughout

16
00:00:40,110 --> 00:00:44,430
things of all so of course this work

17
00:00:42,270 --> 00:00:46,490
wouldn't be not possible with number of

18
00:00:44,430 --> 00:00:48,390
people if they're in alphabetical order

19
00:00:46,490 --> 00:00:49,650
two are actually in the room one is

20
00:00:48,390 --> 00:00:51,030
Geelong to boot from Wikimedia

21
00:00:49,650 --> 00:00:54,810
Foundation the other is Flavia from

22
00:00:51,030 --> 00:00:57,270
Telecom Polytech so thanks with to them

23
00:00:54,810 --> 00:01:00,060
we also can discuss more interesting

24
00:00:57,270 --> 00:01:02,400
thing now so just to set about what

25
00:01:00,060 --> 00:01:03,990
we're focusing on I mean we're I'm not a

26
00:01:02,400 --> 00:01:05,548
web developer so I'm gonna have a

27
00:01:03,990 --> 00:01:07,500
completely different focus and right now

28
00:01:05,549 --> 00:01:09,659
I'm working on equipment vendors so I

29
00:01:07,500 --> 00:01:12,420
will have a very much lower Larry focus

30
00:01:09,659 --> 00:01:16,469
so no matter what you're working on if

31
00:01:12,420 --> 00:01:18,060
you're a browser maker if you're a CSP

32
00:01:16,469 --> 00:01:19,259
content service provider if you're an

33
00:01:18,060 --> 00:01:20,969
internet service provider or an

34
00:01:19,259 --> 00:01:22,499
equipment vendor what you care about is

35
00:01:20,969 --> 00:01:24,689
that the users are happy right

36
00:01:22,499 --> 00:01:26,999
so offer quality of experience is a

37
00:01:24,689 --> 00:01:30,240
common goal and of course if you

38
00:01:26,999 --> 00:01:31,619
something doesn't go bad so if something

39
00:01:30,240 --> 00:01:34,560
goes bad you want to be able to detect

40
00:01:31,619 --> 00:01:36,659
it fast if possible if you want just to

41
00:01:34,560 --> 00:01:39,960
be able to forecast before things go bad

42
00:01:36,659 --> 00:01:41,280
and if you're good at forecasting you

43
00:01:39,960 --> 00:01:43,169
could also try to prevent things from

44
00:01:41,280 --> 00:01:45,539
going bad you know that for your user

45
00:01:43,170 --> 00:01:46,950
not to churn so detecting quality of the

46
00:01:45,539 --> 00:01:49,619
gradation quality of experience the

47
00:01:46,950 --> 00:01:51,420
gradation is important now how do you

48
00:01:49,619 --> 00:01:52,920
detect quality of experience and all you

49
00:01:51,420 --> 00:01:55,229
define it well typically would need to

50
00:01:52,920 --> 00:01:56,819
have a good idea of how the user if they

51
00:01:55,229 --> 00:01:59,549
are happy or not and then try to

52
00:01:56,819 --> 00:02:01,619
correlate some of the telemetry so like

53
00:01:59,549 --> 00:02:02,789
for instance the boomerang is collecting

54
00:02:01,619 --> 00:02:05,069
a lot of telemetry and we try to

55
00:02:02,789 --> 00:02:06,929
correlate that with the user quality of

56
00:02:05,069 --> 00:02:08,369
experience now if you're taking the

57
00:02:06,929 --> 00:02:10,289
point of view of equipment vendor of

58
00:02:08,369 --> 00:02:11,670
Internet service provider well you're

59
00:02:10,288 --> 00:02:13,649
gonna have a little bit harder time

60
00:02:11,670 --> 00:02:15,119
because you're not in the browser so you

61
00:02:13,650 --> 00:02:17,030
don't have all the rich telemetry and

62
00:02:15,120 --> 00:02:18,650
encryption is really

63
00:02:17,030 --> 00:02:20,360
it's really gonna be painful because

64
00:02:18,650 --> 00:02:22,580
you're not only gonna see stream

65
00:02:20,360 --> 00:02:24,410
unencrypted traffic silly we want to do

66
00:02:22,580 --> 00:02:26,330
something because otherwise your user

67
00:02:24,410 --> 00:02:27,440
will turn if the user will turn the

68
00:02:26,330 --> 00:02:30,560
equipment vendor will not be able to

69
00:02:27,440 --> 00:02:34,579
sell equipment and so there's a loss of

70
00:02:30,560 --> 00:02:36,800
money as well so it's important to get a

71
00:02:34,580 --> 00:02:38,410
hand on what's the quality of experience

72
00:02:36,800 --> 00:02:40,400
and use the quality of experience is

73
00:02:38,410 --> 00:02:42,620
basically affected by a lot of things

74
00:02:40,400 --> 00:02:46,040
including for instance the context or

75
00:02:42,620 --> 00:02:48,620
where is the user work other places if

76
00:02:46,040 --> 00:02:51,470
it's a pessimistic guy or if it's an old

77
00:02:48,620 --> 00:02:54,620
lady probably not have an accessory the

78
00:02:51,470 --> 00:02:56,810
same perception of delay and there so of

79
00:02:54,620 --> 00:02:59,450
course system influence factor if you

80
00:02:56,810 --> 00:03:01,610
are dancing in the building ground - or

81
00:02:59,450 --> 00:03:03,260
- - level floor problem your signal is

82
00:03:01,610 --> 00:03:05,750
not very good so we have slow

83
00:03:03,260 --> 00:03:08,630
performance so in order to factor all

84
00:03:05,750 --> 00:03:09,890
those five be an engineer of course

85
00:03:08,630 --> 00:03:11,989
you're gonna ask the user but you're

86
00:03:09,890 --> 00:03:14,089
gonna try to infer these things from

87
00:03:11,989 --> 00:03:16,100
looking from the system perspective so

88
00:03:14,090 --> 00:03:17,720
system perspective starts from the lower

89
00:03:16,100 --> 00:03:20,150
layer the network so over there you will

90
00:03:17,720 --> 00:03:22,090
able to measure some quality of service

91
00:03:20,150 --> 00:03:24,680
indication these will in turn affect

92
00:03:22,090 --> 00:03:26,810
application performance application

93
00:03:24,680 --> 00:03:29,440
metrics application QoS metrics like the

94
00:03:26,810 --> 00:03:32,810
one that boomerang was reporting or

95
00:03:29,440 --> 00:03:35,209
other like webpagetest are reporting to

96
00:03:32,810 --> 00:03:36,860
you some telemetry and from that we have

97
00:03:35,209 --> 00:03:38,660
an influence on the way in which the

98
00:03:36,860 --> 00:03:42,350
user experience in the browsing behavior

99
00:03:38,660 --> 00:03:43,790
and so what you're going to do is that

100
00:03:42,350 --> 00:03:45,829
you're going to be able to measure some

101
00:03:43,790 --> 00:03:47,359
of these metrics like from an end-to-end

102
00:03:45,829 --> 00:03:48,890
view point what is the latency what is

103
00:03:47,360 --> 00:03:50,870
the bandwidth which is a packet loss or

104
00:03:48,890 --> 00:03:53,149
a point-to-point what is the Wi-Fi

105
00:03:50,870 --> 00:03:54,470
quality of course it doesn't make sense

106
00:03:53,150 --> 00:03:55,880
if you look at the true put of a single

107
00:03:54,470 --> 00:03:56,690
connection because you want to put them

108
00:03:55,880 --> 00:03:58,940
all together

109
00:03:56,690 --> 00:04:01,070
in order to be able to tell meaningful

110
00:03:58,940 --> 00:04:02,450
metric from a session viewpoint session

111
00:04:01,070 --> 00:04:04,670
means for instance if you're looking at

112
00:04:02,450 --> 00:04:06,798
a web application is going to be page

113
00:04:04,670 --> 00:04:09,018
real-time or speed index that we're

114
00:04:06,799 --> 00:04:11,450
going to see later there are also

115
00:04:09,019 --> 00:04:13,459
session metrics that are correlating

116
00:04:11,450 --> 00:04:15,078
measureable multiple sessions so for

117
00:04:13,459 --> 00:04:16,820
instance engagement so measuring if

118
00:04:15,079 --> 00:04:19,400
you're staying on a website for long

119
00:04:16,820 --> 00:04:21,099
means that you typically would are happy

120
00:04:19,399 --> 00:04:23,510
with the quality of experience a serving

121
00:04:21,100 --> 00:04:25,940
and of course you can go and readily ask

122
00:04:23,510 --> 00:04:27,590
the user how he feels about the service

123
00:04:25,940 --> 00:04:29,800
you are giving him so that they can ask

124
00:04:27,590 --> 00:04:31,659
many user all around the room

125
00:04:29,800 --> 00:04:32,919
you get five stars and then you do the

126
00:04:31,659 --> 00:04:35,289
averages you've been your penis score

127
00:04:32,919 --> 00:04:36,698
you can also ask different thing and of

128
00:04:35,289 --> 00:04:38,289
course if you know about the device type

129
00:04:36,699 --> 00:04:40,030
if you have a cheap phone or if you have

130
00:04:38,289 --> 00:04:41,318
a high-end phone maybe your expectation

131
00:04:40,030 --> 00:04:45,280
are different maybe the phones are also

132
00:04:41,319 --> 00:04:46,659
rendering differently so all of that of

133
00:04:45,280 --> 00:04:48,340
course is very complex so today we're

134
00:04:46,659 --> 00:04:49,509
going to focus on a subset of eight

135
00:04:48,340 --> 00:04:52,119
particular we're going to look at the

136
00:04:49,509 --> 00:04:54,280
web that's the web dev room so we're

137
00:04:52,120 --> 00:04:55,930
gonna look into performance materialized

138
00:04:54,280 --> 00:04:58,059
page real time speed index try to see

139
00:04:55,930 --> 00:05:00,629
how this correlate with the min opinion

140
00:04:58,060 --> 00:05:03,340
score your user other user feedback and

141
00:05:00,629 --> 00:05:05,259
of course we're also gonna adopt the

142
00:05:03,340 --> 00:05:07,479
viewpoints of the lower layer carriers

143
00:05:05,259 --> 00:05:10,030
where they are only been able to measure

144
00:05:07,479 --> 00:05:11,620
some weak signals they don't see

145
00:05:10,030 --> 00:05:13,900
anything about the middle layer because

146
00:05:11,620 --> 00:05:16,360
squeak HTTP or whatever other kind of

147
00:05:13,900 --> 00:05:19,090
encryption and so they either want to

148
00:05:16,360 --> 00:05:20,979
try to from the network us you learn

149
00:05:19,090 --> 00:05:23,318
something about the application QoS or

150
00:05:20,979 --> 00:05:26,349
make a big step and go to the quality of

151
00:05:23,319 --> 00:05:27,580
experience of the user so that's

152
00:05:26,349 --> 00:05:30,509
basically the agenda for today so

153
00:05:27,580 --> 00:05:33,520
they're gonna delve into four different

154
00:05:30,509 --> 00:05:35,830
aspects data collection so the modeling

155
00:05:33,520 --> 00:05:37,990
parts the metric part and then again

156
00:05:35,830 --> 00:05:42,758
some method that allows you to go from

157
00:05:37,990 --> 00:05:44,080
row to top up so we have a putt if

158
00:05:42,759 --> 00:05:46,479
you're from the net to you need to start

159
00:05:44,080 --> 00:05:49,900
with your method learning something

160
00:05:46,479 --> 00:05:52,419
which is metrics about the browser can

161
00:05:49,900 --> 00:05:53,318
easily measure you need to learn the

162
00:05:52,419 --> 00:05:54,969
metrics that are useful

163
00:05:53,319 --> 00:05:56,800
so for doing that you need to couple two

164
00:05:54,969 --> 00:05:58,509
things you need to couple measurement

165
00:05:56,800 --> 00:06:00,639
involving the user asking user whether

166
00:05:58,509 --> 00:06:02,919
they help you or not and building models

167
00:06:00,639 --> 00:06:04,930
that based on your metric are hopefully

168
00:06:02,919 --> 00:06:07,900
able to extract the information from

169
00:06:04,930 --> 00:06:09,129
automatically collected one so in the

170
00:06:07,900 --> 00:06:10,150
agenda today we're going to work there

171
00:06:09,129 --> 00:06:13,210
are these top-down so we're going to

172
00:06:10,150 --> 00:06:14,739
start with with the data collection so

173
00:06:13,210 --> 00:06:16,750
that a collection typically what you do

174
00:06:14,740 --> 00:06:18,840
is that you build up some a crowd

175
00:06:16,750 --> 00:06:22,029
sourcing campaign they have a huge cost

176
00:06:18,840 --> 00:06:23,679
and there are no perfect campaign in the

177
00:06:22,029 --> 00:06:24,879
last years we have been doing three type

178
00:06:23,680 --> 00:06:27,610
of different things we've been asking

179
00:06:24,879 --> 00:06:29,789
user what is the mini opinion score so

180
00:06:27,610 --> 00:06:32,590
write your experience from one to five

181
00:06:29,789 --> 00:06:34,150
we have also been asking user when do

182
00:06:32,590 --> 00:06:35,888
you think that the page was finished or

183
00:06:34,150 --> 00:06:38,560
what is your user perceived page load

184
00:06:35,889 --> 00:06:40,570
time or seeing two pages at the same

185
00:06:38,560 --> 00:06:42,729
time which page did you think it finish

186
00:06:40,570 --> 00:06:43,810
at first so to get a little bit an idea

187
00:06:42,729 --> 00:06:46,409
of how the user

188
00:06:43,810 --> 00:06:49,210
even the web and finally with Wikipedia

189
00:06:46,410 --> 00:06:50,950
live in collaboration with Wikipedia we

190
00:06:49,210 --> 00:06:53,739
started asking the user whether they are

191
00:06:50,950 --> 00:06:56,380
satisfied with experience they have

192
00:06:53,740 --> 00:06:59,919
while browsing Wikipedia so of course

193
00:06:56,380 --> 00:07:01,840
there's no perfect solution in the first

194
00:06:59,919 --> 00:07:05,469
data set we were doing lab experiments

195
00:07:01,840 --> 00:07:07,590
this means that we were having few panel

196
00:07:05,470 --> 00:07:10,540
of people that were typically volunteer

197
00:07:07,590 --> 00:07:12,489
close 150 250 people recruiting

198
00:07:10,540 --> 00:07:13,930
universities so you have very specific

199
00:07:12,490 --> 00:07:17,620
class of population it will definitely

200
00:07:13,930 --> 00:07:19,030
not fit the grandma's behavior we the

201
00:07:17,620 --> 00:07:20,830
good side is that we were using real

202
00:07:19,030 --> 00:07:22,960
servers real protocols we were able to

203
00:07:20,830 --> 00:07:25,659
control the conditions but the number of

204
00:07:22,960 --> 00:07:27,849
web pages of course is not as completely

205
00:07:25,660 --> 00:07:28,900
representative of the Internet so then

206
00:07:27,850 --> 00:07:31,060
you can do something else

207
00:07:28,900 --> 00:07:32,739
stepping up by moving into crowdsourcing

208
00:07:31,060 --> 00:07:34,240
so you have for instance Amazon medical

209
00:07:32,740 --> 00:07:36,520
work mechanical turk so you can leverage

210
00:07:34,240 --> 00:07:39,070
a large pool of people but over there

211
00:07:36,520 --> 00:07:40,599
you need to you cannot let them access a

212
00:07:39,070 --> 00:07:42,490
web server so you will typically put

213
00:07:40,600 --> 00:07:45,190
videos of the web page rendering process

214
00:07:42,490 --> 00:07:47,560
so these not really exactly like Rosie

215
00:07:45,190 --> 00:07:48,850
you reach a larger audience but this

216
00:07:47,560 --> 00:07:51,490
what you're also interested in getting

217
00:07:48,850 --> 00:07:53,800
paid for for the task is so you need to

218
00:07:51,490 --> 00:07:55,830
filter out a lot of people that are just

219
00:07:53,800 --> 00:07:59,020
there to make money

220
00:07:55,830 --> 00:07:59,979
so let's film we did with Wikipedia is

221
00:07:59,020 --> 00:08:02,200
very interesting because we have

222
00:07:59,979 --> 00:08:05,140
actually we are polling the user so

223
00:08:02,200 --> 00:08:08,050
there's 1 billion pages visit monthly

224
00:08:05,140 --> 00:08:10,240
roughly and a tiny fraction of that is

225
00:08:08,050 --> 00:08:11,590
going to be polled for performance

226
00:08:10,240 --> 00:08:13,840
metric and a tiny fraction of that is

227
00:08:11,590 --> 00:08:15,580
going to also be pulled for binary

228
00:08:13,840 --> 00:08:16,960
feedback it's slightly more than minor

229
00:08:15,580 --> 00:08:18,609
feedback about whether they were happy

230
00:08:16,960 --> 00:08:20,650
or not so it is good because you're

231
00:08:18,610 --> 00:08:22,690
gonna pull users that are in the real

232
00:08:20,650 --> 00:08:24,669
service from the service they like the

233
00:08:22,690 --> 00:08:26,050
service they use typically the downside

234
00:08:24,669 --> 00:08:28,120
is that you have a huge Atarot Jannetty

235
00:08:26,050 --> 00:08:31,270
remember on top of my head that we were

236
00:08:28,120 --> 00:08:32,950
polling on 65,000 people they had they

237
00:08:31,270 --> 00:08:35,110
were looking at 42,000 different

238
00:08:32,950 --> 00:08:40,270
wikipedia pages 3,000

239
00:08:35,110 --> 00:08:42,310
networks of 250 devices and 45 browsers

240
00:08:40,270 --> 00:08:43,929
so there's a lot of heterogeneity and so

241
00:08:42,309 --> 00:08:46,750
building a single model is not

242
00:08:43,929 --> 00:08:48,729
necessarily trivial when I'm putting the

243
00:08:46,750 --> 00:08:51,370
icon there is that the data set are

244
00:08:48,730 --> 00:08:52,870
available so if peoples are interested

245
00:08:51,370 --> 00:08:55,570
there are people that are doing research

246
00:08:52,870 --> 00:08:57,670
on that like we're saying before sharing

247
00:08:55,570 --> 00:09:01,570
tools is important sharing performance

248
00:08:57,670 --> 00:09:02,979
in evaluation is important sharing the

249
00:09:01,570 --> 00:09:04,540
data is even more important because it

250
00:09:02,980 --> 00:09:06,250
allows you to replicate and see whether

251
00:09:04,540 --> 00:09:09,099
the performance that are reported or -

252
00:09:06,250 --> 00:09:12,340
or not so now that you've got the data

253
00:09:09,100 --> 00:09:15,250
ok cool what we do well basically we're

254
00:09:12,340 --> 00:09:18,070
gonna have a way to go from the data so

255
00:09:15,250 --> 00:09:19,810
our Y to find some function that based

256
00:09:18,070 --> 00:09:23,680
on some of the things that we are able

257
00:09:19,810 --> 00:09:25,989
to measure like our incognita Y plug

258
00:09:23,680 --> 00:09:27,910
into a formula F is going to be able to

259
00:09:25,990 --> 00:09:31,840
tell us what is magically if you want

260
00:09:27,910 --> 00:09:33,490
the user performance so here by each

261
00:09:31,840 --> 00:09:35,410
typically people use a single scalar

262
00:09:33,490 --> 00:09:36,970
metric Janell the page load time the

263
00:09:35,410 --> 00:09:38,980
function has been predetermined by an

264
00:09:36,970 --> 00:09:41,440
expert and there are typically two

265
00:09:38,980 --> 00:09:42,850
approaches that are being used one is a

266
00:09:41,440 --> 00:09:46,210
queueing support disease we are using

267
00:09:42,850 --> 00:09:48,460
our exponential model and here with a

268
00:09:46,210 --> 00:09:50,200
logarithmic model which is said to the

269
00:09:48,460 --> 00:09:52,960
weber Fechner law which is applicable

270
00:09:50,200 --> 00:09:55,000
heavier model that tells that the human

271
00:09:52,960 --> 00:09:57,730
response for stimulus is locally Tomica

272
00:09:55,000 --> 00:09:59,410
related and this is for instance used by

273
00:09:57,730 --> 00:10:00,670
a standard so what you do we do a lot of

274
00:09:59,410 --> 00:10:02,079
measurement all the points here are

275
00:10:00,670 --> 00:10:04,060
different answer from the different user

276
00:10:02,080 --> 00:10:06,030
and then you do a fitting and here the

277
00:10:04,060 --> 00:10:09,069
fitting you we can be happy with that

278
00:10:06,030 --> 00:10:11,020
now there are limits because typically

279
00:10:09,070 --> 00:10:13,810
there are a lot of metrics although

280
00:10:11,020 --> 00:10:15,460
telemetry that is made from browsers and

281
00:10:13,810 --> 00:10:17,260
so here we are only using a single

282
00:10:15,460 --> 00:10:19,780
metric so you can go one step further

283
00:10:17,260 --> 00:10:21,730
and instead of picking a single metric

284
00:10:19,780 --> 00:10:23,829
that you like and a single function that

285
00:10:21,730 --> 00:10:25,770
you like and although the fitting seems

286
00:10:23,830 --> 00:10:28,480
nice you could do something which is

287
00:10:25,770 --> 00:10:31,060
much learning driven so basically having

288
00:10:28,480 --> 00:10:32,770
a factor of input features and having an

289
00:10:31,060 --> 00:10:34,510
automated way to select what is the

290
00:10:32,770 --> 00:10:36,910
optimal fitting of the function by

291
00:10:34,510 --> 00:10:38,439
minimizing some error so here the trick

292
00:10:36,910 --> 00:10:39,819
is that whenever you select a very

293
00:10:38,440 --> 00:10:41,680
specific machine learning algorithm

294
00:10:39,820 --> 00:10:43,210
you're implicitly selecting which are

295
00:10:41,680 --> 00:10:46,270
the type of function that you will go a

296
00:10:43,210 --> 00:10:47,860
little bit to learn and here you see

297
00:10:46,270 --> 00:10:51,010
that you have a slight gain with respect

298
00:10:47,860 --> 00:10:53,140
to the typical models that you have here

299
00:10:51,010 --> 00:10:55,090
by considering more metrics of course

300
00:10:53,140 --> 00:10:56,620
there are different models that are

301
00:10:55,090 --> 00:10:58,750
available we're not going to delve in

302
00:10:56,620 --> 00:10:59,950
the detail of that just to say that for

303
00:10:58,750 --> 00:11:01,630
me there's still some room for

304
00:10:59,950 --> 00:11:03,940
improvement from going to the future

305
00:11:01,630 --> 00:11:05,620
that we have to the user experience but

306
00:11:03,940 --> 00:11:08,860
still you have a good and quite high

307
00:11:05,620 --> 00:11:10,540
correlation so this brings us to the

308
00:11:08,860 --> 00:11:11,260
metric what are the metrics that we can

309
00:11:10,540 --> 00:11:13,839
work on so

310
00:11:11,260 --> 00:11:15,640
in order to be quite clear about

311
00:11:13,840 --> 00:11:17,830
everything I have a very small animation

312
00:11:15,640 --> 00:11:20,020
about how is the web page loading

313
00:11:17,830 --> 00:11:22,570
process after you go and click on a link

314
00:11:20,020 --> 00:11:24,370
so we start something that you're gonna

315
00:11:22,570 --> 00:11:25,660
start downloading and at some point you

316
00:11:24,370 --> 00:11:27,820
will have an event that is gonna be

317
00:11:25,660 --> 00:11:29,380
fired by the browser document object

318
00:11:27,820 --> 00:11:30,870
model so at this point you know the

319
00:11:29,380 --> 00:11:32,950
structural page and you can start

320
00:11:30,870 --> 00:11:34,860
putting things around so we have a

321
00:11:32,950 --> 00:11:38,200
visual progress of the page that

322
00:11:34,860 --> 00:11:40,000
increases from zero to upward then you

323
00:11:38,200 --> 00:11:41,860
keep it downloading more things until at

324
00:11:40,000 --> 00:11:44,380
some point which is called typically

325
00:11:41,860 --> 00:11:46,240
above default all the portion of the all

326
00:11:44,380 --> 00:11:48,100
the visible portion of the page has been

327
00:11:46,240 --> 00:11:50,770
downloaded and shown to the screen

328
00:11:48,100 --> 00:11:52,780
that's called the ATF and your visual

329
00:11:50,770 --> 00:11:54,699
progress is increasing and you can

330
00:11:52,780 --> 00:11:56,829
represent here your visual progress is a

331
00:11:54,700 --> 00:11:58,750
function X of T that is growing from 0

332
00:11:56,830 --> 00:12:00,340
to 1 where 1 is basically everything

333
00:11:58,750 --> 00:12:02,980
that needed to be rendered for the page

334
00:12:00,340 --> 00:12:04,990
to be visually complete is finished so X

335
00:12:02,980 --> 00:12:06,490
of T of course you can also do something

336
00:12:04,990 --> 00:12:08,440
a little bit more fancy so basically

337
00:12:06,490 --> 00:12:11,530
here the integral of the residual of

338
00:12:08,440 --> 00:12:14,380
this function is the area the gray

339
00:12:11,530 --> 00:12:15,670
shaded area above the core and this gray

340
00:12:14,380 --> 00:12:18,130
shaded there above the curve is what

341
00:12:15,670 --> 00:12:19,630
Google defined the speed index so we're

342
00:12:18,130 --> 00:12:21,490
gonna come into that in a moment and

343
00:12:19,630 --> 00:12:22,840
then of course I mean you can keep

344
00:12:21,490 --> 00:12:24,580
downloading more content that it's not

345
00:12:22,840 --> 00:12:26,290
necessarily available and immediately

346
00:12:24,580 --> 00:12:28,300
visible but it's gonna be available when

347
00:12:26,290 --> 00:12:29,740
you scroll and that's when all the

348
00:12:28,300 --> 00:12:32,410
content is loaded is typically the page

349
00:12:29,740 --> 00:12:34,630
load time so now we have two type of

350
00:12:32,410 --> 00:12:35,980
metrics so one are the time instant

351
00:12:34,630 --> 00:12:37,930
metrics we have for instance ten to the

352
00:12:35,980 --> 00:12:39,610
first byte Dom 10 to the first paint

353
00:12:37,930 --> 00:12:42,339
about default Pedro x is a very specific

354
00:12:39,610 --> 00:12:43,450
time which are important to somebody and

355
00:12:42,340 --> 00:12:45,790
then you have something else which is

356
00:12:43,450 --> 00:12:48,760
the integral form of it which is

357
00:12:45,790 --> 00:12:50,709
basically looking at all the air above

358
00:12:48,760 --> 00:12:52,480
the curve so why this thing intuitively

359
00:12:50,710 --> 00:12:55,090
is important imagine that you have two

360
00:12:52,480 --> 00:12:56,680
realization of two pages that are

361
00:12:55,090 --> 00:12:58,990
exactly the same page real time so they

362
00:12:56,680 --> 00:13:01,329
finish exactly the same time but this

363
00:12:58,990 --> 00:13:03,280
one shows out for the content very fast

364
00:13:01,330 --> 00:13:06,430
and this one shows out of the content

365
00:13:03,280 --> 00:13:08,140
almost much more later right so in which

366
00:13:06,430 --> 00:13:10,630
of the two you would be happier in this

367
00:13:08,140 --> 00:13:12,430
one so whenever the area above the curve

368
00:13:10,630 --> 00:13:15,580
is smaller then it's better and it's

369
00:13:12,430 --> 00:13:17,439
faster so one additional comment is that

370
00:13:15,580 --> 00:13:19,180
given that you are integrating something

371
00:13:17,440 --> 00:13:21,400
that is a dimensional I mean integrating

372
00:13:19,180 --> 00:13:24,189
over time also the air above the curve

373
00:13:21,400 --> 00:13:25,089
is a time in dimension so physically if

374
00:13:24,190 --> 00:13:26,860
you are engineers

375
00:13:25,089 --> 00:13:28,540
would think that over time is a time

376
00:13:26,860 --> 00:13:31,360
unit of measure and you can think it is

377
00:13:28,540 --> 00:13:34,449
a virtual time that is explicit ating

378
00:13:31,360 --> 00:13:36,430
alphas was the the rendering process now

379
00:13:34,449 --> 00:13:38,109
you can define a family of metric like

380
00:13:36,430 --> 00:13:40,209
this and depending on what you put is X

381
00:13:38,110 --> 00:13:42,279
of T you're gonna have the speed index

382
00:13:40,209 --> 00:13:44,319
if you're looking for instance at the

383
00:13:42,279 --> 00:13:46,660
difference in the east gram that were

384
00:13:44,319 --> 00:13:48,430
shown for the colors on the page you

385
00:13:46,660 --> 00:13:50,050
have room speed index that is measuring

386
00:13:48,430 --> 00:13:51,699
the areas that each of the different

387
00:13:50,050 --> 00:13:53,559
object that are drawn on the page are

388
00:13:51,699 --> 00:13:55,449
gonna put and they're going to compare

389
00:13:53,559 --> 00:13:58,389
with the amount of rectangular should

390
00:13:55,449 --> 00:14:00,639
have been drawn at the end you can look

391
00:13:58,389 --> 00:14:02,620
at as the same PSSI perceptual speed

392
00:14:00,639 --> 00:14:04,269
index using sec metric which are much

393
00:14:02,620 --> 00:14:07,149
more advanced so all of that is very

394
00:14:04,269 --> 00:14:08,649
good because is visual progress but

395
00:14:07,149 --> 00:14:10,029
there are downsides so for instance you

396
00:14:08,649 --> 00:14:11,709
can only measure them in browsers and

397
00:14:10,029 --> 00:14:13,209
some of them are actually processing

398
00:14:11,709 --> 00:14:15,339
intensive so if you need to do a seam if

399
00:14:13,209 --> 00:14:16,779
there's a lot of computation in to do so

400
00:14:15,339 --> 00:14:19,329
some years ago we were proposing to do

401
00:14:16,779 --> 00:14:21,699
as a proxy of this mod mansard metric

402
00:14:19,329 --> 00:14:23,709
they were very simple inputs like object

403
00:14:21,699 --> 00:14:25,359
index or bytes just looking at the bytes

404
00:14:23,709 --> 00:14:28,779
that are coming you would get a pretty

405
00:14:25,360 --> 00:14:30,490
decent idea of what is coming to your

406
00:14:28,779 --> 00:14:32,379
browser if it's coming fast or not we're

407
00:14:30,490 --> 00:14:35,379
gonna see a little bit later if it's

408
00:14:32,379 --> 00:14:37,660
work or not good side is that you can do

409
00:14:35,379 --> 00:14:39,879
it in layer frame the network is correct

410
00:14:37,660 --> 00:14:41,649
with speed index doesn't necessarily is

411
00:14:39,879 --> 00:14:44,769
good for creative experience so that's a

412
00:14:41,649 --> 00:14:46,839
question that you need to address and

413
00:14:44,769 --> 00:14:50,050
I'm not going to go into these kind of

414
00:14:46,839 --> 00:14:51,339
details but you can also have effect in

415
00:14:50,050 --> 00:14:53,229
for instance the cutoff or the integral

416
00:14:51,339 --> 00:14:54,459
in order to optimize some of those

417
00:14:53,230 --> 00:14:56,800
metrics but I'm not going to go into

418
00:14:54,459 --> 00:14:58,449
this little so now if you are in the

419
00:14:56,800 --> 00:14:59,769
browser or if you're a content service

420
00:14:58,449 --> 00:15:01,508
provider what you have is that you are a

421
00:14:59,769 --> 00:15:03,309
pretty good picture of everything that

422
00:15:01,509 --> 00:15:05,589
is happening you per domain the vision

423
00:15:03,309 --> 00:15:07,569
of all the different objects also the

424
00:15:05,589 --> 00:15:09,550
type if there are images or not CSS

425
00:15:07,569 --> 00:15:11,679
whatever and you can reconstruct this

426
00:15:09,550 --> 00:15:15,609
picture with quite

427
00:15:11,679 --> 00:15:17,769
accuracy now if you are in in the dark

428
00:15:15,610 --> 00:15:19,720
so if you are an HP 400 people vendor

429
00:15:17,769 --> 00:15:21,519
what you will see is basically a series

430
00:15:19,720 --> 00:15:22,689
of packets coming from different flows

431
00:15:21,519 --> 00:15:24,220
and the only thing that you're gonna

432
00:15:22,689 --> 00:15:26,170
read is that okay these are this is a

433
00:15:24,220 --> 00:15:28,929
packet this is a packet full packet size

434
00:15:26,170 --> 00:15:31,019
until and it's a smaller one so what do

435
00:15:28,929 --> 00:15:34,569
you make out of it in order to

436
00:15:31,019 --> 00:15:36,309
extrapolate from this so again I'm not

437
00:15:34,569 --> 00:15:37,079
going to go into a lot of details I'll

438
00:15:36,309 --> 00:15:39,120
let

439
00:15:37,080 --> 00:15:41,520
rather gonna show you why this thing

440
00:15:39,120 --> 00:15:42,750
could work but basically the idea is if

441
00:15:41,520 --> 00:15:46,949
you are familiar with machine learning

442
00:15:42,750 --> 00:15:48,600
you need to perform some amount some

443
00:15:46,950 --> 00:15:50,490
really simple amount of signal

444
00:15:48,600 --> 00:15:52,470
processing in order to make your input

445
00:15:50,490 --> 00:15:54,180
to be homogeneous we're using supervised

446
00:15:52,470 --> 00:15:55,740
technique so supervised technique means

447
00:15:54,180 --> 00:15:59,520
that we need to have exactly the same

448
00:15:55,740 --> 00:16:01,260
input and then different models that we

449
00:15:59,520 --> 00:16:02,640
are using extreme gradient boosting

450
00:16:01,260 --> 00:16:04,590
which is an example method based on

451
00:16:02,640 --> 00:16:06,060
trays or 1d convolutional neural network

452
00:16:04,590 --> 00:16:08,460
what we do is that we present them with

453
00:16:06,060 --> 00:16:11,099
lot of samples and with a look this

454
00:16:08,460 --> 00:16:14,130
sample and we also explain them for

455
00:16:11,100 --> 00:16:15,840
instance had this above default value we

456
00:16:14,130 --> 00:16:17,340
build another model provided the same

457
00:16:15,840 --> 00:16:19,650
example and providing what is the page

458
00:16:17,340 --> 00:16:21,030
load time or the speed index of any

459
00:16:19,650 --> 00:16:23,100
metric that you're interested and we

460
00:16:21,030 --> 00:16:25,350
provide many samples to train a model

461
00:16:23,100 --> 00:16:28,380
and we test it over previously unseen

462
00:16:25,350 --> 00:16:31,620
cases to give an intuition why this

463
00:16:28,380 --> 00:16:33,090
should work so here we have the web page

464
00:16:31,620 --> 00:16:35,220
rendering so this is basically the user

465
00:16:33,090 --> 00:16:36,990
here is what we see in the browser where

466
00:16:35,220 --> 00:16:38,520
every burst is going to be one object

467
00:16:36,990 --> 00:16:40,770
and we have one color per different

468
00:16:38,520 --> 00:16:43,800
domain actually we're presenting only

469
00:16:40,770 --> 00:16:45,000
the top three domains and the others we

470
00:16:43,800 --> 00:16:46,680
are using the same color that was the

471
00:16:45,000 --> 00:16:49,080
pitch would be really really to color it

472
00:16:46,680 --> 00:16:51,329
and here it is what you see from the

473
00:16:49,080 --> 00:16:52,380
network so we're gonna have one packet

474
00:16:51,330 --> 00:16:54,300
with a bit more we're aggregating

475
00:16:52,380 --> 00:16:56,400
packets in 10 milliseconds and then

476
00:16:54,300 --> 00:16:59,069
you're gonna see one color per IP server

477
00:16:56,400 --> 00:17:00,900
so when I'm starting if I click on the

478
00:16:59,070 --> 00:17:04,170
right place you see that okay now this

479
00:17:00,900 --> 00:17:06,060
is a Chinese web page so it starts late

480
00:17:04,170 --> 00:17:07,320
at some point you see those finger

481
00:17:06,060 --> 00:17:08,760
progra saying here there was a big

482
00:17:07,319 --> 00:17:11,129
object this big object has been a lot of

483
00:17:08,760 --> 00:17:12,599
multiple packets same thing here the

484
00:17:11,130 --> 00:17:14,699
green packets correspond to this big

485
00:17:12,599 --> 00:17:16,829
object and you see that these course are

486
00:17:14,699 --> 00:17:18,420
slightly different but you see that

487
00:17:16,829 --> 00:17:22,470
there is some similarity right they are

488
00:17:18,420 --> 00:17:24,750
not completely different and indeed if

489
00:17:22,470 --> 00:17:26,790
you systematically perform this

490
00:17:24,750 --> 00:17:28,020
experiment this was just one example to

491
00:17:26,790 --> 00:17:30,450
show you how these things look like in

492
00:17:28,020 --> 00:17:32,070
for real then you can go and make an

493
00:17:30,450 --> 00:17:33,810
experiment where you monitoring the

494
00:17:32,070 --> 00:17:35,939
network so you taking the real encrypted

495
00:17:33,810 --> 00:17:37,290
traffic you are monitoring the browser

496
00:17:35,940 --> 00:17:38,910
so at the ground truth so you have the

497
00:17:37,290 --> 00:17:41,610
above default whatever metric you're

498
00:17:38,910 --> 00:17:45,210
interested in and you can repeat this

499
00:17:41,610 --> 00:17:47,280
process and try to see extrapolates own

500
00:17:45,210 --> 00:17:49,640
accuracy number so here is the only

501
00:17:47,280 --> 00:17:51,330
accuracy picture that I'm gonna show

502
00:17:49,640 --> 00:17:52,950
this is report

503
00:17:51,330 --> 00:17:54,269
in the absolute error in milliseconds I

504
00:17:52,950 --> 00:17:56,309
could use the median and this is the

505
00:17:54,269 --> 00:17:58,110
25th percentile and this is the 75

506
00:17:56,309 --> 00:17:59,820
percentile so and this is basically in

507
00:17:58,110 --> 00:18:00,899
the 75 per case your error is going to

508
00:17:59,820 --> 00:18:02,250
be much lower than this and in the

509
00:18:00,899 --> 00:18:04,379
median case is going to be this one and

510
00:18:02,250 --> 00:18:07,320
you can see here we have two different

511
00:18:04,380 --> 00:18:09,600
approach one is we even without machine

512
00:18:07,320 --> 00:18:11,250
learning are not going to explain why

513
00:18:09,600 --> 00:18:13,289
the colors before in the picture had a

514
00:18:11,250 --> 00:18:14,580
mathematical interpretation but I didn't

515
00:18:13,289 --> 00:18:17,190
want to bring it up today it's not a

516
00:18:14,580 --> 00:18:19,049
point but with an algorithm a ston that

517
00:18:17,190 --> 00:18:22,019
we can have already something that it's

518
00:18:19,049 --> 00:18:24,720
going to learn only a single function

519
00:18:22,019 --> 00:18:26,100
which is the byte index and we can

520
00:18:24,720 --> 00:18:27,480
approximate the white index learn from

521
00:18:26,100 --> 00:18:29,879
the network with the approximate with

522
00:18:27,480 --> 00:18:32,760
the application byte index that we learn

523
00:18:29,880 --> 00:18:35,370
in the browser and that one has a six

524
00:18:32,760 --> 00:18:36,629
percent error on top of that this

525
00:18:35,370 --> 00:18:39,239
without machine learning is a very

526
00:18:36,630 --> 00:18:40,529
simple online algorithm on top of that

527
00:18:39,240 --> 00:18:42,450
you can add machine learning and you can

528
00:18:40,529 --> 00:18:44,669
compensate for for deserters so you can

529
00:18:42,450 --> 00:18:46,769
reach a lower error and then you can

530
00:18:44,669 --> 00:18:48,389
learn generalize to any metric so we're

531
00:18:46,769 --> 00:18:50,190
learning the below the page load time

532
00:18:48,389 --> 00:18:52,229
the ability index the speed index or

533
00:18:50,190 --> 00:18:53,250
room speed index the Dom if you if

534
00:18:52,230 --> 00:18:56,309
you're interested in learning the Dom

535
00:18:53,250 --> 00:19:00,029
with this kind of errors so we did tests

536
00:18:56,309 --> 00:19:01,559
with orange on number of pages that we

537
00:19:00,029 --> 00:19:03,630
were never seen before in number

538
00:19:01,559 --> 00:19:05,490
networks were not seen before and these

539
00:19:03,630 --> 00:19:07,440
are the accuracy estimated indeed in

540
00:19:05,490 --> 00:19:09,990
those setting so it's a pretty good

541
00:19:07,440 --> 00:19:11,190
portable and ok not to make an

542
00:19:09,990 --> 00:19:12,870
advertisement but given that the

543
00:19:11,190 --> 00:19:15,269
algorithm works we are supporting it

544
00:19:12,870 --> 00:19:17,340
into our work products now there was one

545
00:19:15,269 --> 00:19:19,500
catch they didn't talk in this work due

546
00:19:17,340 --> 00:19:22,709
to lack of time is that we are also able

547
00:19:19,500 --> 00:19:24,840
to end a multi session so if I go to

548
00:19:22,710 --> 00:19:28,669
back here we see that there are a lot of

549
00:19:24,840 --> 00:19:28,668
packets coming from a lot of different

550
00:19:28,909 --> 00:19:33,179
flows but you need before to be able to

551
00:19:31,679 --> 00:19:35,519
isolate the flows that are going to go

552
00:19:33,179 --> 00:19:36,990
to the same session so this is something

553
00:19:35,519 --> 00:19:38,309
that you need in order to be able to

554
00:19:36,990 --> 00:19:39,870
apply your machine learning technique

555
00:19:38,309 --> 00:19:42,750
and it is also something that is done

556
00:19:39,870 --> 00:19:46,168
but we just didn't talk in for lack of

557
00:19:42,750 --> 00:19:47,970
time now so basically after okay now is

558
00:19:46,169 --> 00:19:51,210
where we stand so where we could go to

559
00:19:47,970 --> 00:19:52,860
go further so I'm going to just took

560
00:19:51,210 --> 00:19:54,779
about three three couple of three

561
00:19:52,860 --> 00:19:56,760
ideas so for people that are familiar

562
00:19:54,779 --> 00:19:59,159
with machine learning unfortunately in

563
00:19:56,760 --> 00:20:01,020
the web QE domain we're still at expert

564
00:19:59,159 --> 00:20:02,549
driven feature engineers so basically we

565
00:20:01,020 --> 00:20:04,590
have somebody that is defining speed

566
00:20:02,549 --> 00:20:06,480
index and why should be

567
00:20:04,590 --> 00:20:08,668
PDX seams are very natural and very

568
00:20:06,480 --> 00:20:11,850
bright idea but we have no clue whether

569
00:20:08,669 --> 00:20:14,429
it is really a proxy for quality of

570
00:20:11,850 --> 00:20:15,779
experience so a better approach I'm not

571
00:20:14,429 --> 00:20:18,390
saying more explainable so it's less

572
00:20:15,779 --> 00:20:20,970
intuitive would take raw input ro

573
00:20:18,390 --> 00:20:23,429
sensory data from the user and try to do

574
00:20:20,970 --> 00:20:24,990
what to learn the features by the

575
00:20:23,429 --> 00:20:27,270
learning process learning process gonna

576
00:20:24,990 --> 00:20:29,039
in the neural network through an atom

577
00:20:27,270 --> 00:20:30,389
throw back propagation is going to

578
00:20:29,039 --> 00:20:32,809
create some features that are the most

579
00:20:30,390 --> 00:20:35,580
relevant in order to find and explain

580
00:20:32,809 --> 00:20:38,549
what the why the user voted a given

581
00:20:35,580 --> 00:20:40,590
score okay so that's definitely not

582
00:20:38,549 --> 00:20:42,480
interpretable is more versatile the

583
00:20:40,590 --> 00:20:45,928
downside it is requires a lot of sample

584
00:20:42,480 --> 00:20:47,399
okay so here what we did was taking

585
00:20:45,929 --> 00:20:49,620
packets and learning any of this

586
00:20:47,399 --> 00:20:51,870
function similarly we could use these

587
00:20:49,620 --> 00:20:53,850
inputs and trying to learn functions

588
00:20:51,870 --> 00:20:55,500
which are user happiness course getting

589
00:20:53,850 --> 00:20:57,990
the data is difficult because you would

590
00:20:55,500 --> 00:20:59,730
want to be as less intrusive as possible

591
00:20:57,990 --> 00:21:00,809
so if you need to put sensors like this

592
00:20:59,730 --> 00:21:03,210
maybe you're affecting the user

593
00:21:00,809 --> 00:21:05,340
experience and other things that may be

594
00:21:03,210 --> 00:21:07,529
okay you can leverage so I know people

595
00:21:05,340 --> 00:21:08,699
that are working on happiness

596
00:21:07,529 --> 00:21:11,340
recognition through photo recognition

597
00:21:08,700 --> 00:21:12,929
but over there if you're happy it may be

598
00:21:11,340 --> 00:21:14,340
for the content of the message that you

599
00:21:12,929 --> 00:21:16,710
receive or the page that you're visiting

600
00:21:14,340 --> 00:21:18,840
and not whether the experience you're

601
00:21:16,710 --> 00:21:21,450
loading that page was was happy what was

602
00:21:18,840 --> 00:21:24,330
good so it's quite difficult to get the

603
00:21:21,450 --> 00:21:25,500
sex story part working second thing is

604
00:21:24,330 --> 00:21:27,449
that I was speaking about single model

605
00:21:25,500 --> 00:21:30,570
and actually we need single models

606
00:21:27,450 --> 00:21:32,640
because they are easy deployment but of

607
00:21:30,570 --> 00:21:35,730
course wide web is really really large

608
00:21:32,640 --> 00:21:37,200
so for instance Wikipedia as not it's

609
00:21:35,730 --> 00:21:39,690
not image intensive and you will have

610
00:21:37,200 --> 00:21:41,580
other websites that are mostly done by

611
00:21:39,690 --> 00:21:44,340
images or video so all can a single

612
00:21:41,580 --> 00:21:46,439
model fits all so of course you increase

613
00:21:44,340 --> 00:21:48,928
the accuracy you should go purple mod

614
00:21:46,440 --> 00:21:50,399
purple page here just an example picture

615
00:21:48,929 --> 00:21:53,100
where you have black line is one average

616
00:21:50,399 --> 00:21:54,570
model and these are all the points

617
00:21:53,100 --> 00:21:56,070
you're getting and of course if you have

618
00:21:54,570 --> 00:21:58,049
many per page models they're gonna be

619
00:21:56,070 --> 00:21:59,730
they're gonna have a better fit now the

620
00:21:58,049 --> 00:22:01,440
problem is that inherently this process

621
00:21:59,730 --> 00:22:03,990
is not scalable so how to make it

622
00:22:01,440 --> 00:22:05,909
scalable well by prioritizing things

623
00:22:03,990 --> 00:22:08,100
that are more important for instance if

624
00:22:05,909 --> 00:22:10,740
you have top 100 webpage you can build

625
00:22:08,100 --> 00:22:12,658
reliably model for the top 100 pages

626
00:22:10,740 --> 00:22:15,149
that are more frequently visited by by

627
00:22:12,659 --> 00:22:17,420
people then you can have a second

628
00:22:15,149 --> 00:22:19,699
approach in which we cluster

629
00:22:17,420 --> 00:22:21,670
top 1 million webpages for instance here

630
00:22:19,700 --> 00:22:24,920
you see number of clusters out of which

631
00:22:21,670 --> 00:22:26,060
24 pages were extracted and inside each

632
00:22:24,920 --> 00:22:28,160
each of these classes there are

633
00:22:26,060 --> 00:22:29,600
thousands of pages this cluster have

634
00:22:28,160 --> 00:22:31,190
similarity in terms of the number of

635
00:22:29,600 --> 00:22:33,169
domains the number of objects and at the

636
00:22:31,190 --> 00:22:34,820
size of the page so there are higher

637
00:22:33,170 --> 00:22:36,860
chances that if you build models that

638
00:22:34,820 --> 00:22:38,480
are accurate for pages in this class

639
00:22:36,860 --> 00:22:40,520
then you're going to also be able to

640
00:22:38,480 --> 00:22:42,710
cover more accurately the top 1 million

641
00:22:40,520 --> 00:22:44,360
and then of course okay for the rest to

642
00:22:42,710 --> 00:22:46,310
top 1 billion pages you're gonna use a

643
00:22:44,360 --> 00:22:48,469
single average model and pray it will

644
00:22:46,310 --> 00:22:50,840
work but at least you're gonna already

645
00:22:48,470 --> 00:22:53,050
have in a better operational point in

646
00:22:50,840 --> 00:22:55,580
the accuracy vs. scalability trade-off

647
00:22:53,050 --> 00:22:57,379
then finally comment which is a

648
00:22:55,580 --> 00:22:59,120
community comment if you are working in

649
00:22:57,380 --> 00:23:01,790
this space the first thing you need is

650
00:22:59,120 --> 00:23:04,879
data so keep collecting and sharing data

651
00:23:01,790 --> 00:23:07,970
is very important so I'm very happy that

652
00:23:04,880 --> 00:23:10,040
finally we with working with Wikimedia

653
00:23:07,970 --> 00:23:12,130
we were able to release data set in a

654
00:23:10,040 --> 00:23:14,180
properly anonymized form that was

655
00:23:12,130 --> 00:23:16,220
protecting successfully the privacy of

656
00:23:14,180 --> 00:23:17,900
people and also letting people doing the

657
00:23:16,220 --> 00:23:20,870
research not to build models better than

658
00:23:17,900 --> 00:23:21,800
the one we built so we you need to take

659
00:23:20,870 --> 00:23:23,209
into account when you go to the

660
00:23:21,800 --> 00:23:25,190
supermarket you already find this

661
00:23:23,210 --> 00:23:26,690
machine right and asking you are you

662
00:23:25,190 --> 00:23:28,370
happy or not and you click on it and you

663
00:23:26,690 --> 00:23:30,590
don't think even about it when you're

664
00:23:28,370 --> 00:23:32,060
calling over Skype or Facebook at the

665
00:23:30,590 --> 00:23:35,510
end of a call there's something that is

666
00:23:32,060 --> 00:23:37,070
calling as can you rate your call also

667
00:23:35,510 --> 00:23:38,870
my phone started asking me did you find

668
00:23:37,070 --> 00:23:40,790
the suggestion useful so to a binary

669
00:23:38,870 --> 00:23:42,919
feedback from him and this is them from

670
00:23:40,790 --> 00:23:45,230
Wikipedia so what you would you gain for

671
00:23:42,920 --> 00:23:48,650
keeping this date steady data collection

672
00:23:45,230 --> 00:23:50,840
is two things one you have until your

673
00:23:48,650 --> 00:23:52,730
model will not be good you really have

674
00:23:50,840 --> 00:23:54,590
some information from the user so you

675
00:23:52,730 --> 00:23:57,050
already know if something happens that

676
00:23:54,590 --> 00:23:58,760
is users of your service that are

677
00:23:57,050 --> 00:24:00,620
telling you directly and you don't need

678
00:23:58,760 --> 00:24:01,910
to go over twitter and try to understand

679
00:24:00,620 --> 00:24:03,979
if the user are complaining about your

680
00:24:01,910 --> 00:24:05,360
service through other channel second

681
00:24:03,980 --> 00:24:08,150
this continuous stream of data it's

682
00:24:05,360 --> 00:24:09,620
going to also be able to make your model

683
00:24:08,150 --> 00:24:12,620
better or if there's anything that

684
00:24:09,620 --> 00:24:14,000
changes next protocol so we had the HTTP

685
00:24:12,620 --> 00:24:17,149
to now it's going to be HTTP three

686
00:24:14,000 --> 00:24:18,410
sooner sooner over HTTP over quick maybe

687
00:24:17,150 --> 00:24:20,660
your model need to be retrained so you

688
00:24:18,410 --> 00:24:22,730
will need to have this kind of data and

689
00:24:20,660 --> 00:24:24,230
if the producer population is large

690
00:24:22,730 --> 00:24:26,810
enough they're also limited on cited

691
00:24:24,230 --> 00:24:29,720
only it is a risk of annoying users if

692
00:24:26,810 --> 00:24:32,510
you are having a small

693
00:24:29,720 --> 00:24:34,250
if you'll ever expand small planets so

694
00:24:32,510 --> 00:24:37,129
without so this is basically a talk that

695
00:24:34,250 --> 00:24:39,770
is based on these resources here I put

696
00:24:37,130 --> 00:24:41,900
the different papers they put also the

697
00:24:39,770 --> 00:24:44,180
icons for the different data set some of

698
00:24:41,900 --> 00:24:46,790
the implementation that we release and

699
00:24:44,180 --> 00:24:49,040
everything is accessible from from here

700
00:24:46,790 --> 00:24:53,600
in this page there are things that are

701
00:24:49,040 --> 00:24:56,120
not out yet so more will come so for

702
00:24:53,600 --> 00:24:57,919
with all this I figure I'm done so I

703
00:24:56,120 --> 00:24:59,209
would like to thank you for listening so

704
00:24:57,920 --> 00:25:00,620
far if you have any question please go

705
00:24:59,210 --> 00:25:09,900
ahead

706
00:25:00,620 --> 00:25:09,899
[Applause]

707
00:25:17,360 --> 00:25:40,428
if you if you shout they can also repeat

708
00:25:19,460 --> 00:25:43,940
the question yeah okay so there are

709
00:25:40,429 --> 00:25:45,320
studies also so the question is what if

710
00:25:43,940 --> 00:25:47,419
unable to break the key so what you're

711
00:25:45,320 --> 00:25:50,529
doing things about the encrypted stuff

712
00:25:47,419 --> 00:25:53,179
where you can break the encryption so

713
00:25:50,529 --> 00:25:55,039
government guys so there are two answers

714
00:25:53,179 --> 00:25:56,629
so if you are having the if you are able

715
00:25:55,039 --> 00:25:59,090
to decrypt probably you're not

716
00:25:56,629 --> 00:26:01,699
interested in web performance so you're

717
00:25:59,090 --> 00:26:04,369
breaking this you know the to look at

718
00:26:01,700 --> 00:26:05,960
the different information secondly what

719
00:26:04,369 --> 00:26:07,399
study are usenix

720
00:26:05,960 --> 00:26:11,239
telling you what is the fraction of

721
00:26:07,399 --> 00:26:12,939
India you got proxies for instance in in

722
00:26:11,239 --> 00:26:14,779
some of the institutions you have

723
00:26:12,940 --> 00:26:17,210
approximate allocating and you're

724
00:26:14,779 --> 00:26:19,340
accepting the key and in a PC that is

725
00:26:17,210 --> 00:26:21,559
managed by your organization indeed you

726
00:26:19,340 --> 00:26:23,418
have proxies so from which it is not

727
00:26:21,559 --> 00:26:25,850
necessarily useful now in GDP are these

728
00:26:23,419 --> 00:26:27,710
pretty serious now so definitely if you

729
00:26:25,850 --> 00:26:29,359
are one way there is going to be twice

730
00:26:27,710 --> 00:26:32,809
as more concern as if you're a regular

731
00:26:29,359 --> 00:26:34,658
vendor right now in this moment and so

732
00:26:32,809 --> 00:26:36,889
of course I mean having that your

733
00:26:34,659 --> 00:26:37,879
devices are totally not interested in

734
00:26:36,889 --> 00:26:39,649
looking in the payload because they

735
00:26:37,879 --> 00:26:41,570
don't need it okay it's much more

736
00:26:39,649 --> 00:26:43,070
important so here basically what we are

737
00:26:41,570 --> 00:26:45,408
doing is that we are leveraging very

738
00:26:43,070 --> 00:26:46,820
weak signals that are intrinsically in

739
00:26:45,409 --> 00:26:49,999
the timing information that comes in

740
00:26:46,820 --> 00:26:53,389
packets much like I mean Debussy was

741
00:26:49,999 --> 00:26:55,340
saying that the music is the silence

742
00:26:53,389 --> 00:26:57,590
between the notes right so here in

743
00:26:55,340 --> 00:26:58,939
somehow we're waiting the information

744
00:26:57,590 --> 00:27:00,168
that we see even if you are not

745
00:26:58,940 --> 00:27:01,879
listening to the notes not looking at

746
00:27:00,169 --> 00:27:04,700
the content to try to reconstruct the

747
00:27:01,879 --> 00:27:07,219
signals the finger was showing today was

748
00:27:04,700 --> 00:27:10,779
not for the government was more for in

749
00:27:07,220 --> 00:27:10,779
the internet service provider and

750
00:27:10,899 --> 00:27:15,469
equipment vendor but if you go up to the

751
00:27:13,369 --> 00:27:17,658
Chrome browser for instance they still

752
00:27:15,470 --> 00:27:20,330
the missing link is between the layer

753
00:27:17,659 --> 00:27:23,600
seven the layer eight the user so how do

754
00:27:20,330 --> 00:27:25,759
you can you ensure that means it would

755
00:27:23,600 --> 00:27:28,399
be a talking animal is asian of timing

756
00:27:25,759 --> 00:27:29,990
AP is so you want to normalize something

757
00:27:28,399 --> 00:27:32,270
that is relevant for the user

758
00:27:29,990 --> 00:27:34,960
so and this is the part where Indy

759
00:27:32,270 --> 00:27:37,370
defeat from going normalizing that from

760
00:27:34,960 --> 00:27:39,110
level seven point of view and if he is

761
00:27:37,370 --> 00:27:43,059
relevant we can learn it also from layer

762
00:27:39,110 --> 00:27:43,059
free without breaking an encryption key

763
00:27:44,260 --> 00:27:56,600
know if it clarifies or yeah okay so

764
00:27:54,890 --> 00:27:58,370
that's a very yeah so that's a very good

765
00:27:56,600 --> 00:27:59,870
question so actually it's seasonality so

766
00:27:58,370 --> 00:28:01,939
basically things that are non session

767
00:27:59,870 --> 00:28:04,189
alert I'm and particular seasonality

768
00:28:01,940 --> 00:28:07,670
means there's periodicity is something

769
00:28:04,190 --> 00:28:10,610
that we look out we extensively look out

770
00:28:07,670 --> 00:28:12,380
in the that in datasets for instance

771
00:28:10,610 --> 00:28:14,449
with Wikipedia we have measurement of

772
00:28:12,380 --> 00:28:16,490
months worth of studies so we were

773
00:28:14,450 --> 00:28:18,559
expecting to find a night effect weekend

774
00:28:16,490 --> 00:28:19,940
with taste effect we didn't find any

775
00:28:18,559 --> 00:28:22,010
about the happiness of the user was

776
00:28:19,940 --> 00:28:25,700
amazing like stationary during the

777
00:28:22,010 --> 00:28:28,340
period so this is documentary in WWI /

778
00:28:25,700 --> 00:28:30,040
and we also extended that see there I

779
00:28:28,340 --> 00:28:32,629
don't know what there's no seasonality

780
00:28:30,040 --> 00:28:50,059
we were expecting it the data now is

781
00:28:32,630 --> 00:28:53,570
available so yeah in do this much okay

782
00:28:50,059 --> 00:28:56,540
so that's basically I went very very

783
00:28:53,570 --> 00:28:59,720
fast yeah I'm gonna repeat the question

784
00:28:56,540 --> 00:29:02,240
when I come to here so basically the

785
00:28:59,720 --> 00:29:04,550
question is okay what is the magic how

786
00:29:02,240 --> 00:29:06,410
can you learn from the the pockets and

787
00:29:04,550 --> 00:29:08,600
actually we're not learning directly

788
00:29:06,410 --> 00:29:10,910
from the pockets because every webpage

789
00:29:08,600 --> 00:29:12,800
is a different number of pockets and our

790
00:29:10,910 --> 00:29:15,290
supervised method which are regression

791
00:29:12,800 --> 00:29:16,700
met the need to have a fixed at input so

792
00:29:15,290 --> 00:29:18,050
what we're doing is basically that we

793
00:29:16,700 --> 00:29:21,590
are shopping the time into regular

794
00:29:18,050 --> 00:29:24,620
interval of times and what happens is

795
00:29:21,590 --> 00:29:26,389
that basically you are sampling

796
00:29:24,620 --> 00:29:28,669
periodically signal you're sampling it

797
00:29:26,390 --> 00:29:31,370
periodically this signal when you are

798
00:29:28,670 --> 00:29:33,230
every so often every delta T you are

799
00:29:31,370 --> 00:29:35,120
looking at the pocket will calm put an

800
00:29:33,230 --> 00:29:38,929
integral there and you basically

801
00:29:35,120 --> 00:29:41,090
sampling this curve here right so this

802
00:29:38,929 --> 00:29:43,840
is the way which we get the input which

803
00:29:41,090 --> 00:29:46,449
is by just cumulating over small period

804
00:29:43,840 --> 00:29:48,668
time time arrivals pockets belong to the

805
00:29:46,450 --> 00:29:50,740
same session and that words make the

806
00:29:48,669 --> 00:29:51,909
team so there's the basical signal

807
00:29:50,740 --> 00:29:54,520
processing level amount to feature

808
00:29:51,909 --> 00:29:56,770
engineering to normalize your input to

809
00:29:54,520 --> 00:30:02,230
be able to fade it to a neural network

810
00:29:56,770 --> 00:30:06,158
and okay four minutes left so I was too

811
00:30:02,230 --> 00:30:09,610
fast so you can sorry guys you have a

812
00:30:06,159 --> 00:30:11,710
question okay cool I can ask directly in

813
00:30:09,610 --> 00:30:13,240
a microphone do you have an estimate of

814
00:30:11,710 --> 00:30:14,650
how many data points we collected on

815
00:30:13,240 --> 00:30:17,200
Wikipedia and like a typical week during

816
00:30:14,650 --> 00:30:18,640
the study how many data points we

817
00:30:17,200 --> 00:30:21,010
collected in a typical week during this

818
00:30:18,640 --> 00:30:24,279
study on Wikipedia you ain't even a week

819
00:30:21,010 --> 00:30:26,200
so on now I know that basically finger

820
00:30:24,279 --> 00:30:33,190
change a bit so I think I have some make

821
00:30:26,200 --> 00:30:34,870
up slides Oh too many backup slides so

822
00:30:33,190 --> 00:30:38,289
this is okay about the stationarity you

823
00:30:34,870 --> 00:30:40,529
get your picture there which is here so

824
00:30:38,289 --> 00:30:42,970
I know that we were collecting the

825
00:30:40,529 --> 00:30:47,169
62,000 data points during the first

826
00:30:42,970 --> 00:30:49,840
period which was basically first test

827
00:30:47,169 --> 00:30:52,059
case in which we were if I remember

828
00:30:49,840 --> 00:30:53,918
correctly web performance timing our

829
00:30:52,059 --> 00:30:56,230
trigger at once every 10,000 page visit

830
00:30:53,919 --> 00:30:59,190
in Wikipedia and out of those we were

831
00:30:56,230 --> 00:31:01,419
sampling one over 1,000 at the beginning

832
00:30:59,190 --> 00:31:03,279
at the end of that we step up a little

833
00:31:01,419 --> 00:31:05,620
bit so you you step up a little bit the

834
00:31:03,279 --> 00:31:09,399
sampling but this is basically over this

835
00:31:05,620 --> 00:31:11,289
time period of time so hidden is the

836
00:31:09,399 --> 00:31:13,989
fact that we basically issued the sample

837
00:31:11,289 --> 00:31:17,830
the query to 1.4 million people and only

838
00:31:13,990 --> 00:31:20,679
62k replied so because people are they

839
00:31:17,830 --> 00:31:24,158
can willingly or not accept to click on

840
00:31:20,679 --> 00:31:26,230
those or not so the numbers per week I

841
00:31:24,159 --> 00:31:28,510
don't have them in my mind because you

842
00:31:26,230 --> 00:31:31,360
are mostly focusing on can we get a

843
00:31:28,510 --> 00:31:33,760
breakdown of all the users are happy and

844
00:31:31,360 --> 00:31:36,250
in this case in Wikipedia 85 percent of

845
00:31:33,760 --> 00:31:38,770
the users are consistently happy with no

846
00:31:36,250 --> 00:31:42,870
seasonality and no correlation with some

847
00:31:38,770 --> 00:31:42,870
events ok

848
00:32:01,300 --> 00:32:09,350
okay so we can go back this is through

849
00:32:05,930 --> 00:32:11,450
slide 1 which is so you're here you want

850
00:32:09,350 --> 00:32:12,800
to know if things break or not right so

851
00:32:11,450 --> 00:32:14,210
if you are measure and indeed from the

852
00:32:12,800 --> 00:32:15,649
browser it's because you are in the

853
00:32:14,210 --> 00:32:18,350
browser you because you're a service

854
00:32:15,650 --> 00:32:21,890
provider now what what we do as a

855
00:32:18,350 --> 00:32:24,350
business is basically selling boxes to

856
00:32:21,890 --> 00:32:26,600
operators in operators what they do is

857
00:32:24,350 --> 00:32:29,030
that they sell pipe capacity to their

858
00:32:26,600 --> 00:32:30,350
customer which are the user and from

859
00:32:29,030 --> 00:32:33,410
time to time they have problems because

860
00:32:30,350 --> 00:32:34,850
the service doesn't work and the people

861
00:32:33,410 --> 00:32:36,710
will complain to the SP but actually

862
00:32:34,850 --> 00:32:39,050
it's not the SP the problem may be the

863
00:32:36,710 --> 00:32:41,150
content service provider may be the DNS

864
00:32:39,050 --> 00:32:42,580
may be BGP so over there basically

865
00:32:41,150 --> 00:32:45,830
there's a need for troubleshooting tools

866
00:32:42,580 --> 00:32:49,040
in order to be able to tell oh yeah

867
00:32:45,830 --> 00:32:50,870
it's our problem so it's our net - blade

868
00:32:49,040 --> 00:32:52,879
is down so we're gonna fix it or loot

869
00:32:50,870 --> 00:32:55,280
guys everything that we have on our side

870
00:32:52,880 --> 00:32:57,500
is ok but there are lots of problems on

871
00:32:55,280 --> 00:32:59,930
that web sites everywhere in order to be

872
00:32:57,500 --> 00:33:01,520
able to say so you need to know what is

873
00:32:59,930 --> 00:33:02,990
the typical page load time of your user

874
00:33:01,520 --> 00:33:05,660
or the technique whether this is

875
00:33:02,990 --> 00:33:07,340
changing so this is why indeed before I

876
00:33:05,660 --> 00:33:10,400
was working more on the if you want

877
00:33:07,340 --> 00:33:12,260
layer 7 what kinds of aspects and there

878
00:33:10,400 --> 00:33:14,360
the question was ok we have this PD

879
00:33:12,260 --> 00:33:16,520
index ok we have the above default but

880
00:33:14,360 --> 00:33:18,379
nobody tried to compare whether this was

881
00:33:16,520 --> 00:33:20,600
really relevant for the user so this is

882
00:33:18,380 --> 00:33:23,000
where we started involving users and now

883
00:33:20,600 --> 00:33:25,340
this bit about ok and then I'm working

884
00:33:23,000 --> 00:33:27,140
for in our equipment vendors so am i

885
00:33:25,340 --> 00:33:28,730
able to do the same things but from a

886
00:33:27,140 --> 00:33:30,440
more challenging viewpoint which is

887
00:33:28,730 --> 00:33:34,400
starting from a completely encrypted

888
00:33:30,440 --> 00:33:38,360
traffic just for I mean it is research

889
00:33:34,400 --> 00:33:40,520
so it's fun but then given that I'm no

890
00:33:38,360 --> 00:33:42,139
longer in University and I'm in wall

891
00:33:40,520 --> 00:33:43,580
with this business model behind because

892
00:33:42,140 --> 00:33:45,170
basically if you are able to detect

893
00:33:43,580 --> 00:33:46,970
whether there is a problem then you can

894
00:33:45,170 --> 00:33:48,350
fix it and then it will not have user

895
00:33:46,970 --> 00:33:50,690
churn and so you're not losing money

896
00:33:48,350 --> 00:33:52,550
right so the same thing from the content

897
00:33:50,690 --> 00:33:56,120
provider why they optimizing because

898
00:33:52,550 --> 00:33:57,649
there are ads except on Wikipedia so

899
00:33:56,120 --> 00:33:59,600
there are in Wikipedia there's no nation

900
00:33:57,650 --> 00:34:01,760
but if if you are Google

901
00:33:59,600 --> 00:34:04,490
or being Ephraim Michaels Facebook you

902
00:34:01,760 --> 00:34:06,260
you're showing us and you're this is the

903
00:34:04,490 --> 00:34:08,418
way you get money so if your webpage is

904
00:34:06,260 --> 00:34:10,668
law they were studied by Google by being

905
00:34:08,418 --> 00:34:12,440
they were showing that for any amount of

906
00:34:10,668 --> 00:34:14,089
milliseconds you ads from hundreds

907
00:34:12,440 --> 00:34:16,340
millisecond you have a loss in the

908
00:34:14,090 --> 00:34:18,320
number of people that are gonna go to

909
00:34:16,340 --> 00:34:20,960
the server click on the ads and so you

910
00:34:18,320 --> 00:34:24,470
have a losses of revenue and if you

911
00:34:20,960 --> 00:34:27,080
multiply two percent loss by 1.2 billion

912
00:34:24,469 --> 00:34:30,678
people on visiting that's that's big

913
00:34:27,080 --> 00:34:36,679
numbers so same thing but from encrypted

914
00:34:30,679 --> 00:34:38,780
pipe from the network guys few points ok

915
00:34:36,679 --> 00:34:39,889
so thanks thanks a lot

916
00:34:38,780 --> 00:34:48,379
for

917
00:34:39,889 --> 00:34:48,379
[Applause]

