1
00:00:06,240 --> 00:00:17,790
okay so good afternoon thank you for

2
00:00:14,530 --> 00:00:20,680
joining the session

3
00:00:17,790 --> 00:00:24,430
my name is Alberto and today we are

4
00:00:20,680 --> 00:00:28,390
going to talk about this unsupervised

5
00:00:24,430 --> 00:00:30,730
the free capped a catcher is the novel

6
00:00:28,390 --> 00:00:32,430
only the animal by in this case is best

7
00:00:30,730 --> 00:00:34,450
software how we call that in the

8
00:00:32,430 --> 00:00:39,250
translation language translation

9
00:00:34,450 --> 00:00:42,490
industry the software to translate I'm

10
00:00:39,250 --> 00:00:44,710
36 years older I started working more

11
00:00:42,490 --> 00:00:46,060
than 10 years ago I've been at the

12
00:00:44,710 --> 00:00:48,550
University I'm mastering computer

13
00:00:46,060 --> 00:00:52,450
science and I'm working most of my

14
00:00:48,550 --> 00:00:55,239
career in startups do today I look after

15
00:00:52,450 --> 00:00:57,579
as I sorry in develops a business unit

16
00:00:55,240 --> 00:01:00,540
of the company who sent me here source

17
00:00:57,579 --> 00:01:04,750
Anza and also I'm a machine learning

18
00:01:00,540 --> 00:01:07,030
major for the unit my forte is in

19
00:01:04,750 --> 00:01:09,640
kubernetes monitoring us continuous

20
00:01:07,030 --> 00:01:12,479
integration cloud hadoop so big data

21
00:01:09,640 --> 00:01:15,430
python everything which is around

22
00:01:12,479 --> 00:01:18,220
Jupiter notebook scikit-learn fanciful

23
00:01:15,430 --> 00:01:21,790
at night I attended in the spare time

24
00:01:18,220 --> 00:01:27,789
they have to model concerts and there's

25
00:01:21,790 --> 00:01:29,470
something here this diva one is just

26
00:01:27,790 --> 00:01:33,630
really annoying

27
00:01:29,470 --> 00:01:33,630
ok it won't go away

28
00:01:40,620 --> 00:01:46,770
great so we like to thank all so much

29
00:01:43,380 --> 00:01:49,229
the company was sending me here source

30
00:01:46,770 --> 00:01:50,970
sensor we are basically a consulting

31
00:01:49,230 --> 00:01:52,680
company completely devoted to open

32
00:01:50,970 --> 00:01:55,200
source software we've been existing

33
00:01:52,680 --> 00:01:57,660
since 20 years and we have a virus

34
00:01:55,200 --> 00:02:02,400
branches in Italy also one in London

35
00:01:57,660 --> 00:02:05,429
some so no more in Europe and we are we

36
00:02:02,400 --> 00:02:08,399
are delp's people we are coders we are

37
00:02:05,430 --> 00:02:09,660
data scientists and so we take after 360

38
00:02:08,399 --> 00:02:12,330
degrees everything we can

39
00:02:09,660 --> 00:02:14,400
so today the outline is as its follows

40
00:02:12,330 --> 00:02:17,460
so we're starting with the need for a

41
00:02:14,400 --> 00:02:20,400
standard tool chain for translating and

42
00:02:17,460 --> 00:02:23,220
localizing as we say software project

43
00:02:20,400 --> 00:02:25,050
then I will talk about the catch-all I'm

44
00:02:23,220 --> 00:02:27,800
bringing here today which is mate cat

45
00:02:25,050 --> 00:02:30,060
and what's the problem with today's

46
00:02:27,800 --> 00:02:33,410
machine translation which is the most

47
00:02:30,060 --> 00:02:36,810
fundamental supporting system for

48
00:02:33,410 --> 00:02:38,579
translation pipeline and what is the

49
00:02:36,810 --> 00:02:47,400
solution which is coming to the rescue

50
00:02:38,580 --> 00:02:49,440
and what we have here practically so so

51
00:02:47,400 --> 00:02:51,120
why do we need a standard tool chain

52
00:02:49,440 --> 00:02:55,609
although there are a lot of solutions

53
00:02:51,120 --> 00:02:58,620
out there to translate documents and

54
00:02:55,610 --> 00:03:01,350
software most importantly it feels that

55
00:02:58,620 --> 00:03:04,470
too many times we have too many ways to

56
00:03:01,350 --> 00:03:08,519
skin at the very same cat and this is

57
00:03:04,470 --> 00:03:10,739
because a lot of people are not into a

58
00:03:08,520 --> 00:03:13,440
translation industry so who doesn't

59
00:03:10,739 --> 00:03:15,270
understand reinvents from old from

60
00:03:13,440 --> 00:03:18,650
scratch and these always very dangerous

61
00:03:15,270 --> 00:03:21,720
because it ferments the efforts and so

62
00:03:18,650 --> 00:03:24,930
this software industry and particularly

63
00:03:21,720 --> 00:03:27,209
the false industry needs to adapt more

64
00:03:24,930 --> 00:03:29,970
to the industrial way of doing things

65
00:03:27,209 --> 00:03:31,980
which is the only one viable one and

66
00:03:29,970 --> 00:03:33,840
this because the industry actors which

67
00:03:31,980 --> 00:03:37,049
are the language service providers the

68
00:03:33,840 --> 00:03:39,600
LSP is a long set onto a consolidated

69
00:03:37,050 --> 00:03:42,030
set of processes of technologies and

70
00:03:39,600 --> 00:03:44,220
file formats there is all even a

71
00:03:42,030 --> 00:03:47,180
consortium which is called oasis which

72
00:03:44,220 --> 00:03:51,299
standardizes the file formats and the

73
00:03:47,180 --> 00:03:54,900
interchange technologies there allows to

74
00:03:51,299 --> 00:03:57,390
build the viable industry

75
00:03:54,900 --> 00:03:59,489
this man in this show is about exposing

76
00:03:57,390 --> 00:04:01,409
with battle-tested technologies to the

77
00:03:59,489 --> 00:04:03,569
community so we can return him back soon

78
00:04:01,409 --> 00:04:06,359
to what we love which is hacking and not

79
00:04:03,569 --> 00:04:09,599
coming up with new ways of translating a

80
00:04:06,360 --> 00:04:11,459
software or translating a document so

81
00:04:09,599 --> 00:04:13,768
the standard tool change for translation

82
00:04:11,459 --> 00:04:16,139
for translation company or translate a

83
00:04:13,769 --> 00:04:18,030
an open-source project who wants to be

84
00:04:16,139 --> 00:04:20,400
translated starts with a cat a

85
00:04:18,029 --> 00:04:24,150
computer-aided translation tool which is

86
00:04:20,399 --> 00:04:26,880
an editor that parses a bilingual file a

87
00:04:24,150 --> 00:04:29,969
bilingual file is a kind of an envelope

88
00:04:26,880 --> 00:04:32,100
container which represents our files

89
00:04:29,970 --> 00:04:35,400
which which is in the process of being

90
00:04:32,100 --> 00:04:35,970
translated so you put in this file in

91
00:04:35,400 --> 00:04:38,880
this envelope

92
00:04:35,970 --> 00:04:41,190
everything is extracted and you can

93
00:04:38,880 --> 00:04:44,340
manipulate the strings which is the text

94
00:04:41,190 --> 00:04:47,669
the buttons text labels the menu voices

95
00:04:44,340 --> 00:04:49,198
but even a simple docx document you can

96
00:04:47,669 --> 00:04:53,698
manipulate these strings into other

97
00:04:49,199 --> 00:04:56,190
languages and among other things it has

98
00:04:53,699 --> 00:04:59,639
to provide the true main cavities which

99
00:04:56,190 --> 00:05:01,710
is the tagger editing capability so you

100
00:04:59,639 --> 00:05:04,169
can manipulate the untranslatable

101
00:05:01,710 --> 00:05:07,979
entities like markup if you think about

102
00:05:04,169 --> 00:05:10,799
it a lot of the content in a well

103
00:05:07,979 --> 00:05:12,960
formatted file is the formatting which

104
00:05:10,800 --> 00:05:14,789
has nothing it doesn't need to be

105
00:05:12,960 --> 00:05:16,530
translated that you must not translate

106
00:05:14,789 --> 00:05:19,409
that you must preserve it otherwise you

107
00:05:16,530 --> 00:05:21,059
you're going to break the markup and so

108
00:05:19,410 --> 00:05:23,849
you have to manipulate that's the thing

109
00:05:21,060 --> 00:05:25,889
living that untouched and also number

110
00:05:23,849 --> 00:05:29,130
two format preservation so you must

111
00:05:25,889 --> 00:05:32,280
convert from the original file to a

112
00:05:29,130 --> 00:05:34,080
bilingual system then you have to

113
00:05:32,280 --> 00:05:36,750
translate all the strings and then you

114
00:05:34,080 --> 00:05:38,940
can pack the strings back in the origin

115
00:05:36,750 --> 00:05:40,889
file format preserving the formatting

116
00:05:38,940 --> 00:05:44,219
you don't want to break the file just

117
00:05:40,889 --> 00:05:46,260
because you change the strings another

118
00:05:44,220 --> 00:05:48,419
really fundamental component is the

119
00:05:46,260 --> 00:05:50,520
presence of a translation memory you can

120
00:05:48,419 --> 00:05:53,430
think a translation memory as a database

121
00:05:50,520 --> 00:05:56,070
of past translations that you can

122
00:05:53,430 --> 00:05:58,710
recycle or adapt for the incoming

123
00:05:56,070 --> 00:06:01,590
document which is coming here and the

124
00:05:58,710 --> 00:06:04,700
idea is that once you translate

125
00:06:01,590 --> 00:06:07,859
something you don't throw away all the

126
00:06:04,700 --> 00:06:09,719
singular translations but you keep them

127
00:06:07,860 --> 00:06:11,460
the next time you're gonna make a second

128
00:06:09,719 --> 00:06:14,099
revision for example of dishwashing

129
00:06:11,460 --> 00:06:16,888
machine manual you just want to use the

130
00:06:14,099 --> 00:06:19,319
very same sentences and just translate a

131
00:06:16,889 --> 00:06:21,060
new ones it's a matter of style guide

132
00:06:19,319 --> 00:06:25,740
it's a matter of preserving the

133
00:06:21,060 --> 00:06:26,550
experience like control panel from

134
00:06:25,740 --> 00:06:29,550
windows

135
00:06:26,550 --> 00:06:31,979
Italy is panel to control you cannot

136
00:06:29,550 --> 00:06:35,009
change that sentence to something else

137
00:06:31,979 --> 00:06:38,460
otherwise me all the window users who

138
00:06:35,009 --> 00:06:41,219
are customized the kind of sentence

139
00:06:38,460 --> 00:06:43,109
appearing somewhere in the menu will be

140
00:06:41,219 --> 00:06:48,300
confused if you change to somebody else

141
00:06:43,110 --> 00:06:50,939
like panel oh I don't know page control

142
00:06:48,300 --> 00:06:52,860
and for the control so you must preserve

143
00:06:50,939 --> 00:06:54,960
this style you must preserve everything

144
00:06:52,860 --> 00:06:56,819
that is part of the experience of the

145
00:06:54,960 --> 00:07:00,628
software and translation memory serves

146
00:06:56,819 --> 00:07:02,009
this purpose and last but not least

147
00:07:00,629 --> 00:07:03,960
component you need a machine translation

148
00:07:02,009 --> 00:07:06,270
which is a server that provides

149
00:07:03,960 --> 00:07:09,599
translations on the fly which is Google

150
00:07:06,270 --> 00:07:11,818
Translate for example so some dominant

151
00:07:09,599 --> 00:07:13,759
standards which are worth knowing is the

152
00:07:11,819 --> 00:07:15,569
ax life which is the exchange

153
00:07:13,759 --> 00:07:19,710
localization

154
00:07:15,569 --> 00:07:22,440
file format which is an xml-based file

155
00:07:19,710 --> 00:07:25,620
envelope that separates the strings from

156
00:07:22,440 --> 00:07:28,680
the markup so you to the doc X into X

157
00:07:25,620 --> 00:07:31,139
left envelope then you have like The

158
00:07:28,680 --> 00:07:33,210
Blob the binary of the original file who

159
00:07:31,139 --> 00:07:34,979
has been processed by removing all the

160
00:07:33,210 --> 00:07:37,529
strings and substituting that with

161
00:07:34,979 --> 00:07:40,469
placeholders then the placeholders are

162
00:07:37,529 --> 00:07:42,509
into a map that map's towards the

163
00:07:40,469 --> 00:07:44,759
strings you have them for the original

164
00:07:42,509 --> 00:07:47,789
language you are coming from but then

165
00:07:44,759 --> 00:07:50,550
you can add the new keys for the other

166
00:07:47,789 --> 00:07:52,289
languages you want to go into and so

167
00:07:50,550 --> 00:07:54,330
whenever you want to translate a

168
00:07:52,289 --> 00:07:56,250
document for example in English you just

169
00:07:54,330 --> 00:08:00,779
add the keys for the English that map

170
00:07:56,250 --> 00:08:02,969
then you pack telling the software to

171
00:08:00,779 --> 00:08:04,830
read only the keys in that language code

172
00:08:02,969 --> 00:08:07,439
and packs back the translated strings

173
00:08:04,830 --> 00:08:10,080
into the original document so replace

174
00:08:07,439 --> 00:08:12,479
the placeholders with the new strings

175
00:08:10,080 --> 00:08:14,550
all the document is saved and what you

176
00:08:12,479 --> 00:08:17,370
end up with is a file format with

177
00:08:14,550 --> 00:08:18,719
different language which feels like kind

178
00:08:17,370 --> 00:08:21,230
of magical up the first time you see it

179
00:08:18,719 --> 00:08:24,290
I'm gonna show it here

180
00:08:21,230 --> 00:08:25,940
TMX which is an XML exchange format for

181
00:08:24,290 --> 00:08:27,410
translation memories you want to export

182
00:08:25,940 --> 00:08:29,390
your translation memory and give it to

183
00:08:27,410 --> 00:08:32,090
another translator or to another member

184
00:08:29,390 --> 00:08:34,970
of the community you use a TM X which is

185
00:08:32,090 --> 00:08:37,220
a standard file format and the last

186
00:08:34,970 --> 00:08:40,190
format worth talking about is the PIO

187
00:08:37,220 --> 00:08:42,560
which is a the property property the

188
00:08:40,190 --> 00:08:46,190
specialized file format for the get text

189
00:08:42,559 --> 00:08:49,040
library which is a good new tool and is

190
00:08:46,190 --> 00:08:52,220
the one that you put the strings into

191
00:08:49,040 --> 00:08:54,560
and the software is expected to find the

192
00:08:52,220 --> 00:08:57,170
strings in that format so any anything

193
00:08:54,560 --> 00:08:59,479
that compiles with the get text library

194
00:08:57,170 --> 00:09:01,250
is able to be localized in this way when

195
00:08:59,480 --> 00:09:04,850
you start localizing a project you look

196
00:09:01,250 --> 00:09:07,400
for the get text peo files how not to do

197
00:09:04,850 --> 00:09:09,470
it for example iOS and Android

198
00:09:07,400 --> 00:09:11,000
there are long-standing offenders

199
00:09:09,470 --> 00:09:13,390
because they came up with their own

200
00:09:11,000 --> 00:09:15,620
proprietary file formats which is the

201
00:09:13,390 --> 00:09:19,730
localizable.strings and the strings.xml

202
00:09:15,620 --> 00:09:24,200
and there are tools to convert this

203
00:09:19,730 --> 00:09:25,850
stuff to the PIO format so it's under

204
00:09:24,200 --> 00:09:29,180
workflow is that you go you come with an

205
00:09:25,850 --> 00:09:31,790
original file you put it into a CAD tool

206
00:09:29,180 --> 00:09:33,199
or a cat server and that generates the

207
00:09:31,790 --> 00:09:34,969
next leaf container which is the

208
00:09:33,200 --> 00:09:36,890
envelope of the original file then you

209
00:09:34,970 --> 00:09:38,930
query the translation memory or a

210
00:09:36,890 --> 00:09:42,319
machine translation and you feel the

211
00:09:38,930 --> 00:09:44,390
content in it at any time you can export

212
00:09:42,320 --> 00:09:46,490
the container to an excel file and sign

213
00:09:44,390 --> 00:09:48,890
to an independent translator to have it

214
00:09:46,490 --> 00:09:49,880
translated this particularly important

215
00:09:48,890 --> 00:09:51,439
because you don't want to send the

216
00:09:49,880 --> 00:09:54,050
original file you don't want to develop

217
00:09:51,440 --> 00:09:56,930
the translator to gain access to the

218
00:09:54,050 --> 00:09:59,120
original 5 which may be reserved which

219
00:09:56,930 --> 00:10:02,479
isn't translating to also the file may

220
00:09:59,120 --> 00:10:05,510
be so big they want to split the load we

221
00:10:02,480 --> 00:10:08,360
among 10 translators so you have a 10

222
00:10:05,510 --> 00:10:12,080
small excel file with the 10 parallel

223
00:10:08,360 --> 00:10:14,210
pieces of stuff anytime you can export

224
00:10:12,080 --> 00:10:16,160
translation memory in a TMX or the

225
00:10:14,210 --> 00:10:18,700
machine translation in a modify you want

226
00:10:16,160 --> 00:10:21,860
deploy somewhere else and at any point

227
00:10:18,700 --> 00:10:24,830
you can take the extract container with

228
00:10:21,860 --> 00:10:27,500
the strings that bin translator until

229
00:10:24,830 --> 00:10:29,000
that point repackage back them into a

230
00:10:27,500 --> 00:10:31,160
translate file which is the thing you

231
00:10:29,000 --> 00:10:32,900
want to go back to your customer or to

232
00:10:31,160 --> 00:10:34,260
the community if you're working in a

233
00:10:32,900 --> 00:10:37,600
project

234
00:10:34,260 --> 00:10:40,450
so today we are showing the our kettle

235
00:10:37,600 --> 00:10:41,340
of choice is mate cat mate cat is an

236
00:10:40,450 --> 00:10:43,030
enterprise-grade

237
00:10:41,340 --> 00:10:46,750
completely free and open-source

238
00:10:43,030 --> 00:10:49,449
web-based the cat tool and it has been

239
00:10:46,750 --> 00:10:51,670
funded by European community in the 7th

240
00:10:49,450 --> 00:10:53,920
Framework Programme it costs the 3

241
00:10:51,670 --> 00:10:57,189
million euros ax and 2 years and half to

242
00:10:53,920 --> 00:10:58,990
develop it started with the four people

243
00:10:57,190 --> 00:11:03,220
team including me for the very first

244
00:10:58,990 --> 00:11:05,590
release it's currently its open software

245
00:11:03,220 --> 00:11:07,930
so you find it on github and it's

246
00:11:05,590 --> 00:11:10,000
evolved in the oppor and operated as a

247
00:11:07,930 --> 00:11:13,630
service by translated which is the

248
00:11:10,000 --> 00:11:16,360
company that took the tender to develop

249
00:11:13,630 --> 00:11:20,830
this technology and so now I'm showing

250
00:11:16,360 --> 00:11:22,870
you this so today I'm bringing you here

251
00:11:20,830 --> 00:11:24,340
there is the hosted version but the

252
00:11:22,870 --> 00:11:26,500
hosted version is not really interesting

253
00:11:24,340 --> 00:11:28,540
so today we are deploying that on our

254
00:11:26,500 --> 00:11:30,250
laptop in real time and this is the

255
00:11:28,540 --> 00:11:31,360
first thing I am bringing here which is

256
00:11:30,250 --> 00:11:35,560
a docker midgut

257
00:11:31,360 --> 00:11:37,360
and it is actually Arya packaging a

258
00:11:35,560 --> 00:11:39,400
proper packaging of this tool as a

259
00:11:37,360 --> 00:11:43,240
docker container so it's easy to deploy

260
00:11:39,400 --> 00:11:47,560
that so it's really easy you just have

261
00:11:43,240 --> 00:11:50,190
to clone and issue these really small

262
00:11:47,560 --> 00:11:53,469
commands so in this case now we are

263
00:11:50,190 --> 00:11:58,150
whoops let's start bringing up the my

264
00:11:53,470 --> 00:12:02,100
sequel container let's then bring up

265
00:11:58,150 --> 00:12:05,520
let's initialize the Massacre container

266
00:12:02,100 --> 00:12:08,530
testing if the container is connected

267
00:12:05,520 --> 00:12:14,079
and there we go ok

268
00:12:08,530 --> 00:12:17,339
everything is up to so my keyboard just

269
00:12:14,080 --> 00:12:21,340
switch to English thanks to them effect

270
00:12:17,340 --> 00:12:26,890
and then we bring me up the rest of the

271
00:12:21,340 --> 00:12:28,690
tool I started with some settings but

272
00:12:26,890 --> 00:12:31,000
you are free to customize everything I

273
00:12:28,690 --> 00:12:32,590
try to move everything from the build

274
00:12:31,000 --> 00:12:35,830
time after run time so you can just

275
00:12:32,590 --> 00:12:36,680
change your end time variable and to

276
00:12:35,830 --> 00:12:39,649
change

277
00:12:36,680 --> 00:12:41,859
accordingly the only thing which is

278
00:12:39,649 --> 00:12:46,339
really nasty it can be improved here

279
00:12:41,860 --> 00:12:47,930
actually is that it it expects you to it

280
00:12:46,339 --> 00:12:50,120
runs only on Chrome but that is by

281
00:12:47,930 --> 00:12:53,209
design okay but it expects you to have

282
00:12:50,120 --> 00:12:56,750
the dev Metacom for a matter of cookies

283
00:12:53,209 --> 00:12:58,579
and that were to find the API domains

284
00:12:56,750 --> 00:13:01,100
that these are Dwyer din a configuration

285
00:12:58,580 --> 00:13:02,660
file and Yuki I will make sure you can

286
00:13:01,100 --> 00:13:05,029
change your runtime in a later version

287
00:13:02,660 --> 00:13:09,529
but for now you just add the dev meet

288
00:13:05,029 --> 00:13:11,089
Capcom into your local lost because your

289
00:13:09,529 --> 00:13:15,140
PC must know where it is

290
00:13:11,089 --> 00:13:18,709
and is the catch-all interface so here

291
00:13:15,140 --> 00:13:20,569
you can upload anything and I'm studying

292
00:13:18,709 --> 00:13:23,359
with this docker and kubernetes

293
00:13:20,570 --> 00:13:29,570
presentation which is a file in english

294
00:13:23,360 --> 00:13:31,610
about docker and cuban Andes and let's

295
00:13:29,570 --> 00:13:34,399
just load the know that it's a oops

296
00:13:31,610 --> 00:13:38,890
sorry this is my fault because I forgot

297
00:13:34,399 --> 00:13:43,089
one thing to change the permissions

298
00:13:38,890 --> 00:13:43,089
otherwise they are complaining

299
00:13:47,779 --> 00:13:52,519
so the file has been uploaded now it's

300
00:13:50,120 --> 00:13:55,339
being converted to this intermediate

301
00:13:52,519 --> 00:13:58,480
presentation these supports over 72 file

302
00:13:55,339 --> 00:14:05,750
formats so all the offices stuff strings

303
00:13:58,480 --> 00:14:07,220
CAD is a really big array of formats it

304
00:14:05,750 --> 00:14:11,689
does the conversion for you and packs

305
00:14:07,220 --> 00:14:16,129
back so it's really nice now we start

306
00:14:11,689 --> 00:14:19,040
analyzing and of course I you can sign

307
00:14:16,129 --> 00:14:22,930
in with the Google account I put my

308
00:14:19,040 --> 00:14:25,610
develop my development keys in here and

309
00:14:22,930 --> 00:14:29,628
now it's a complaining that these up is

310
00:14:25,610 --> 00:14:29,930
not verified the urban environment don't

311
00:14:29,629 --> 00:14:32,750
worry

312
00:14:29,930 --> 00:14:35,719
a nice feature that this tool has is

313
00:14:32,750 --> 00:14:38,389
that it can scavenger for you your Docs

314
00:14:35,720 --> 00:14:40,160
from Google Drive and have them

315
00:14:38,389 --> 00:14:42,620
translated and re-upload it back to your

316
00:14:40,160 --> 00:14:45,379
drive which is really handy now it's

317
00:14:42,620 --> 00:14:48,560
analyzing all the all the words in order

318
00:14:45,379 --> 00:14:50,329
to show to find for duplicate the

319
00:14:48,560 --> 00:14:54,018
content so you don't have to translate

320
00:14:50,329 --> 00:14:56,329
something twice and for translate to for

321
00:14:54,019 --> 00:15:02,240
matches into a translation memory which

322
00:14:56,329 --> 00:15:04,128
I add and we just open the workbench so

323
00:15:02,240 --> 00:15:06,290
here you can see I've partially

324
00:15:04,129 --> 00:15:08,660
translated some stuff so it's locked I

325
00:15:06,290 --> 00:15:10,819
cannot touch it until i unlock the

326
00:15:08,660 --> 00:15:13,610
string but never mind because there are

327
00:15:10,819 --> 00:15:16,399
other strings here so for example I

328
00:15:13,610 --> 00:15:18,350
click here and I get a suggestion from a

329
00:15:16,399 --> 00:15:20,930
machine translation server which is in

330
00:15:18,350 --> 00:15:26,240
this case is going translate so we can

331
00:15:20,930 --> 00:15:27,829
just accept the translation or not oh so

332
00:15:26,240 --> 00:15:31,819
the new containers we don't translate

333
00:15:27,829 --> 00:15:33,550
the containers in Italian then we

334
00:15:31,819 --> 00:15:36,949
translate and it goes to the next one

335
00:15:33,550 --> 00:15:41,870
see here we have the tags these tags are

336
00:15:36,949 --> 00:15:43,459
the formatting of this PowerPoint so the

337
00:15:41,870 --> 00:15:46,040
problem is that this string has been

338
00:15:43,459 --> 00:15:47,809
form as been is in the middle of a tag

339
00:15:46,040 --> 00:15:52,699
which maybe is bolded or maybe the

340
00:15:47,809 --> 00:15:54,199
italic and I have to make sure that it's

341
00:15:52,699 --> 00:15:56,479
the same so for example spring you

342
00:15:54,199 --> 00:15:59,839
deploy your spring application spring

343
00:15:56,480 --> 00:16:02,800
these cases Beatriz SP area but is

344
00:15:59,839 --> 00:16:02,800
absolutely wrong

345
00:16:09,070 --> 00:16:18,410
and in this case spring application are

346
00:16:15,110 --> 00:16:20,690
in between the two different tags to

347
00:16:18,410 --> 00:16:26,000
nest the tags so what I'm doing here is

348
00:16:20,690 --> 00:16:33,550
I'm living there as it is so it's trying

349
00:16:26,000 --> 00:16:38,029
to guess all by itself come on okay

350
00:16:33,550 --> 00:16:40,790
accept it and I can go over and over

351
00:16:38,029 --> 00:16:44,089
over until all the strings have been

352
00:16:40,790 --> 00:16:47,349
translated there nice place to store

353
00:16:44,089 --> 00:16:49,970
your images okay fine

354
00:16:47,350 --> 00:16:53,380
anytime I can download the preview of

355
00:16:49,970 --> 00:16:56,600
how is it going and it's just

356
00:16:53,380 --> 00:17:00,050
downloading the file and I have an open

357
00:16:56,600 --> 00:17:02,750
it you can see has been translated

358
00:17:00,050 --> 00:17:04,669
Italian why all the formatting has been

359
00:17:02,750 --> 00:17:13,959
preserved which is pretty much

360
00:17:04,670 --> 00:17:13,959
impressive and there we go

361
00:17:16,490 --> 00:17:23,910
so a critic of all is good but for the

362
00:17:21,540 --> 00:17:26,129
most of my time here being relying on

363
00:17:23,910 --> 00:17:28,260
machine translation server to suggest me

364
00:17:26,130 --> 00:17:30,180
the correct sentences this because I

365
00:17:28,260 --> 00:17:32,250
just I had to accept or edit the

366
00:17:30,180 --> 00:17:35,370
suggestions this gives me a big

367
00:17:32,250 --> 00:17:37,080
opportunity boost but no matter how much

368
00:17:35,370 --> 00:17:40,050
data we translate we never have enough

369
00:17:37,080 --> 00:17:42,000
memories to reuse for the new project

370
00:17:40,050 --> 00:17:44,820
which is coming so machine translation

371
00:17:42,000 --> 00:17:48,930
system is here to actually fill the gaps

372
00:17:44,820 --> 00:17:51,149
which are always more than you have

373
00:17:48,930 --> 00:17:52,890
machine translation are machine learning

374
00:17:51,150 --> 00:17:55,410
systems they are trained at over

375
00:17:52,890 --> 00:17:59,010
datasets which are named parallel corpus

376
00:17:55,410 --> 00:18:02,250
or corpora parallel corpora serve as a

377
00:17:59,010 --> 00:18:04,710
be directional label data set objection

378
00:18:02,250 --> 00:18:07,170
a parallel corpora is a very very very

379
00:18:04,710 --> 00:18:09,060
long list of sentences in one language

380
00:18:07,170 --> 00:18:11,700
for example English and then a

381
00:18:09,060 --> 00:18:13,679
corresponding list of those sentences in

382
00:18:11,700 --> 00:18:15,930
the foreign language I'm trying to

383
00:18:13,680 --> 00:18:18,060
translate for example Italian so I have

384
00:18:15,930 --> 00:18:21,140
1 million centers in English and 1

385
00:18:18,060 --> 00:18:24,629
million same sentences translated in

386
00:18:21,140 --> 00:18:26,940
Italian and it serves as a big new

387
00:18:24,630 --> 00:18:29,610
actual label at the data set because I

388
00:18:26,940 --> 00:18:31,560
show an English sentence and then I show

389
00:18:29,610 --> 00:18:33,449
the corresponding Italian sentence but I

390
00:18:31,560 --> 00:18:36,000
can do in the opposite direction I want

391
00:18:33,450 --> 00:18:39,420
to start from Italian to English I start

392
00:18:36,000 --> 00:18:41,880
with the Italian string and I show as a

393
00:18:39,420 --> 00:18:47,550
label what what you should come up with

394
00:18:41,880 --> 00:18:50,520
the English string the problem with this

395
00:18:47,550 --> 00:18:52,409
thing is that you have to feed this

396
00:18:50,520 --> 00:18:55,139
machine learning server you have to come

397
00:18:52,410 --> 00:18:58,530
up with a lot of data and this has been

398
00:18:55,140 --> 00:19:01,320
particularly made worse by the advantage

399
00:18:58,530 --> 00:19:03,810
of the narrow technologies the neural

400
00:19:01,320 --> 00:19:07,590
machine translation since it's a neural

401
00:19:03,810 --> 00:19:11,790
network it requires a lot of data which

402
00:19:07,590 --> 00:19:15,270
in the hundreds of millions of are I

403
00:19:11,790 --> 00:19:17,220
need sentences ok a lot a lot a lot of

404
00:19:15,270 --> 00:19:19,950
stuff so since the technology is pretty

405
00:19:17,220 --> 00:19:21,810
much the same for all the players the

406
00:19:19,950 --> 00:19:25,890
winner is the one who has more data

407
00:19:21,810 --> 00:19:28,809
which namely is Google always that is

408
00:19:25,890 --> 00:19:30,220
the only really see

409
00:19:28,809 --> 00:19:32,139
use provider machine translation

410
00:19:30,220 --> 00:19:35,080
technology all the others are niche

411
00:19:32,139 --> 00:19:37,658
players will pretend to be very good but

412
00:19:35,080 --> 00:19:40,629
Google Translate actually is the

413
00:19:37,659 --> 00:19:43,929
top-notch player they start first they

414
00:19:40,629 --> 00:19:46,629
crawled a lot of data they align add a

415
00:19:43,929 --> 00:19:48,999
lot of data automatically or with manual

416
00:19:46,629 --> 00:19:53,769
effort to bootstrap then they really

417
00:19:48,999 --> 00:19:58,690
came up with the billions of words so

418
00:19:53,769 --> 00:20:00,999
the problem is that we we have to find

419
00:19:58,690 --> 00:20:03,429
this data there are efforts to procure a

420
00:20:00,999 --> 00:20:06,580
parallel data for free which is the

421
00:20:03,429 --> 00:20:08,679
opposite or spyler corpus it's a

422
00:20:06,580 --> 00:20:11,110
collection an open collection of

423
00:20:08,679 --> 00:20:13,690
parallel corpora which is grow every day

424
00:20:11,110 --> 00:20:15,610
for example they crawl pages in

425
00:20:13,690 --> 00:20:18,249
different languages and align them or

426
00:20:15,610 --> 00:20:21,158
for example they take books and they

427
00:20:18,249 --> 00:20:22,360
align the sentences the even though they

428
00:20:21,159 --> 00:20:24,909
are not exact the same translation

429
00:20:22,360 --> 00:20:28,059
because the literal translation is not

430
00:20:24,909 --> 00:20:30,009
really the same for example you know a

431
00:20:28,059 --> 00:20:34,470
book which is really really really easy

432
00:20:30,009 --> 00:20:38,559
to align the Bible the Bible is as

433
00:20:34,470 --> 00:20:41,619
notation for all the verses and all the

434
00:20:38,559 --> 00:20:44,918
different chapters is the most aligned a

435
00:20:41,619 --> 00:20:46,720
book or book in the world and it has all

436
00:20:44,919 --> 00:20:51,759
the languages in the world so although

437
00:20:46,720 --> 00:20:54,330
it's a way of writing is a little old it

438
00:20:51,759 --> 00:20:57,429
works very very well

439
00:20:54,330 --> 00:21:00,369
but the problem is that we have enough

440
00:20:57,429 --> 00:21:02,110
data to come up with a decent transition

441
00:21:00,369 --> 00:21:04,360
system because we can just use this

442
00:21:02,110 --> 00:21:06,129
stuff there are millions and tens of

443
00:21:04,360 --> 00:21:07,869
millions of sentences in this open

444
00:21:06,129 --> 00:21:09,189
corpora but just for FIGS

445
00:21:07,869 --> 00:21:12,129
which is French Italian German and

446
00:21:09,190 --> 00:21:15,909
Spanish if you have another language for

447
00:21:12,129 --> 00:21:18,459
example Norwegian or Sudanese you have

448
00:21:15,909 --> 00:21:20,230
no way of coming up with a decent

449
00:21:18,460 --> 00:21:23,259
machine translation system you just have

450
00:21:20,230 --> 00:21:27,879
not enough resources not enough parallel

451
00:21:23,259 --> 00:21:31,029
resources so we are stuck at that point

452
00:21:27,879 --> 00:21:33,820
because the technology we have is a very

453
00:21:31,029 --> 00:21:36,610
good to come up with the translations

454
00:21:33,820 --> 00:21:39,309
but only for a selected amount of

455
00:21:36,610 --> 00:21:41,409
languages and since those languages are

456
00:21:39,309 --> 00:21:43,990
those who can

457
00:21:41,410 --> 00:21:47,860
benefit from the presence of a

458
00:21:43,990 --> 00:21:50,860
translation industry there more and more

459
00:21:47,860 --> 00:21:53,560
resources are created in those languages

460
00:21:50,860 --> 00:21:56,889
because more and more documents and the

461
00:21:53,560 --> 00:21:58,899
interactions are produced and and so the

462
00:21:56,890 --> 00:22:03,220
more and more data they had it's a

463
00:21:58,900 --> 00:22:06,580
virtuous circle and the same can doesn't

464
00:22:03,220 --> 00:22:10,710
happen actually for the region Italian

465
00:22:06,580 --> 00:22:10,710
region for example or Japanese

466
00:22:10,950 --> 00:22:16,650
a bike because there's no interaction

467
00:22:14,700 --> 00:22:18,690
with the Japanese culture and black

468
00:22:16,650 --> 00:22:21,630
culture so you don't have aniline at the

469
00:22:18,690 --> 00:22:23,690
data set so you say okay what if we have

470
00:22:21,630 --> 00:22:27,090
the Japanese English and the English

471
00:22:23,690 --> 00:22:28,620
Hebrew we can do like a people we

472
00:22:27,090 --> 00:22:31,500
translate from Japanese to English and

473
00:22:28,620 --> 00:22:34,709
then from English to a bureau yes you

474
00:22:31,500 --> 00:22:38,429
and you lose a lot of fidelity because

475
00:22:34,710 --> 00:22:40,380
it's like we have Italy we have a game I

476
00:22:38,429 --> 00:22:43,049
don't know how you call that broad is

477
00:22:40,380 --> 00:22:45,419
the cordless phone in which you say a

478
00:22:43,049 --> 00:22:46,830
sentence into an air to somebody else

479
00:22:45,419 --> 00:22:48,870
and then the other has to tell to

480
00:22:46,830 --> 00:22:51,629
another in a ring of people and then at

481
00:22:48,870 --> 00:22:53,699
the end that you have to just do you

482
00:22:51,630 --> 00:22:56,130
have fun knowing what is the sentence

483
00:22:53,700 --> 00:22:57,510
that has arrived doing a pivoting in

484
00:22:56,130 --> 00:22:59,640
machine translation is really the same

485
00:22:57,510 --> 00:23:01,770
thing so you're just taking the output

486
00:22:59,640 --> 00:23:05,159
of a machine system and feeding it into

487
00:23:01,770 --> 00:23:07,080
the another machine which will add a lot

488
00:23:05,159 --> 00:23:09,330
of distortion and so you will not we

489
00:23:07,080 --> 00:23:13,168
will not be very happy because your

490
00:23:09,330 --> 00:23:15,178
translation will suck so let's just

491
00:23:13,169 --> 00:23:19,650
maybe a maybe since we are in a dead end

492
00:23:15,179 --> 00:23:20,909
we may take a step back the dead end

493
00:23:19,650 --> 00:23:24,289
comes from the fact that we have a

494
00:23:20,909 --> 00:23:27,120
supervised system the needs labeled data

495
00:23:24,289 --> 00:23:28,950
what do we just go with unsupervised

496
00:23:27,120 --> 00:23:32,219
training unsupervised training is a

497
00:23:28,950 --> 00:23:34,380
particular kind of a class of machine

498
00:23:32,220 --> 00:23:38,340
learning system which is not concerned

499
00:23:34,380 --> 00:23:40,470
with finding correlations between data

500
00:23:38,340 --> 00:23:44,280
and the label but finding a hidden

501
00:23:40,470 --> 00:23:47,820
structure into a corpus which is not as

502
00:23:44,280 --> 00:23:49,770
nullable so in this case we don't need

503
00:23:47,820 --> 00:23:51,418
the parallel examples to learn language

504
00:23:49,770 --> 00:23:55,850
it's not the way humans actually learn

505
00:23:51,419 --> 00:23:58,679
language so we learn Italian when we are

506
00:23:55,850 --> 00:24:01,168
we are our mother tongue language and

507
00:23:58,679 --> 00:24:03,539
then we learn another language but

508
00:24:01,169 --> 00:24:06,299
separated from Italian it's not that

509
00:24:03,539 --> 00:24:09,780
they just relentlessly show us examples

510
00:24:06,299 --> 00:24:11,940
of all the lore that we can come up with

511
00:24:09,780 --> 00:24:13,770
in our mother tongue language and they

512
00:24:11,940 --> 00:24:16,620
just throw it out the English

513
00:24:13,770 --> 00:24:18,658
translations for me I'm Italian until I

514
00:24:16,620 --> 00:24:22,379
learned English I just learned the

515
00:24:18,659 --> 00:24:23,670
English separately and then I started

516
00:24:22,380 --> 00:24:25,710
doing my mapping

517
00:24:23,670 --> 00:24:27,270
- you - languages and we can do actually

518
00:24:25,710 --> 00:24:29,720
the same how about learning two

519
00:24:27,270 --> 00:24:32,639
languages and then try to map between

520
00:24:29,720 --> 00:24:35,700
concepts which is easier because it's

521
00:24:32,640 --> 00:24:39,230
easy for me just to get a very very vast

522
00:24:35,700 --> 00:24:42,870
corpus of just Italian just a Norwegian

523
00:24:39,230 --> 00:24:44,910
learn those independently learn a model

524
00:24:42,870 --> 00:24:47,820
of how those languages are structured

525
00:24:44,910 --> 00:24:50,700
and then try to put some links some

526
00:24:47,820 --> 00:24:53,399
bridges in between when I master the two

527
00:24:50,700 --> 00:24:54,960
languages the point is that we are

528
00:24:53,400 --> 00:24:58,020
dealing with a machine so in order to

529
00:24:54,960 --> 00:25:00,060
map within between languages a computer

530
00:24:58,020 --> 00:25:02,820
needs to build a representation of that

531
00:25:00,060 --> 00:25:06,780
language and how can that be

532
00:25:02,820 --> 00:25:08,850
accomplished actually we may use

533
00:25:06,780 --> 00:25:12,629
language models language models are

534
00:25:08,850 --> 00:25:14,790
technology that allows a machine to come

535
00:25:12,630 --> 00:25:18,510
up with a structure our hidden structure

536
00:25:14,790 --> 00:25:21,649
because we're just showing it data

537
00:25:18,510 --> 00:25:24,990
samples and not teaching the rules and

538
00:25:21,650 --> 00:25:27,600
builds a model how a word the relates to

539
00:25:24,990 --> 00:25:31,020
another and what embedding is I think

540
00:25:27,600 --> 00:25:32,820
it's a technique that map's every word

541
00:25:31,020 --> 00:25:35,820
into a vector space

542
00:25:32,820 --> 00:25:38,189
remember geometry at school you have all

543
00:25:35,820 --> 00:25:41,040
these shapes in 3d environments which

544
00:25:38,190 --> 00:25:45,420
are vectors so these are an array of

545
00:25:41,040 --> 00:25:47,600
numbers and we can when the numbers are

546
00:25:45,420 --> 00:25:50,610
close to vectors are close in space

547
00:25:47,600 --> 00:25:55,860
words with near meanings will have near

548
00:25:50,610 --> 00:26:00,659
vectors in that space so for example I

549
00:25:55,860 --> 00:26:03,899
don't know Paris and Rome will be near

550
00:26:00,660 --> 00:26:06,540
in the space of the capitals because

551
00:26:03,900 --> 00:26:08,820
Paris will be in Rome would be very far

552
00:26:06,540 --> 00:26:12,300
from Milan for example because Milan is

553
00:26:08,820 --> 00:26:15,720
not a capital city or anything so Paris

554
00:26:12,300 --> 00:26:17,879
Rome Berlin would be very near and Milan

555
00:26:15,720 --> 00:26:20,610
maybe will be near Frankfurt among the

556
00:26:17,880 --> 00:26:26,000
Spay the dimension of all the cities

557
00:26:20,610 --> 00:26:29,310
were not capital to anything boy and the

558
00:26:26,000 --> 00:26:32,280
man will be near because they will be on

559
00:26:29,310 --> 00:26:34,580
the dimension of the type of sex you

560
00:26:32,280 --> 00:26:36,590
will be a dimensional just too

561
00:26:34,580 --> 00:26:39,230
two numbers actually like a binary

562
00:26:36,590 --> 00:26:41,990
dimension but anyway we have these very

563
00:26:39,230 --> 00:26:44,960
big vectors of 300 dimensions and we

564
00:26:41,990 --> 00:26:47,300
come up with a way to map from the world

565
00:26:44,960 --> 00:26:49,040
to disagree stuff and we can do even

566
00:26:47,300 --> 00:26:51,680
crazy things in theses vectors and

567
00:26:49,040 --> 00:26:53,659
numbers they actually allow us to do

568
00:26:51,680 --> 00:26:56,030
computations like you take the vector of

569
00:26:53,660 --> 00:26:59,720
Paris you subtract the vector of France

570
00:26:56,030 --> 00:27:02,600
and there are numbers you don't add the

571
00:26:59,720 --> 00:27:04,790
vector Italy and what you get is a

572
00:27:02,600 --> 00:27:08,689
vector which is almost the same as the

573
00:27:04,790 --> 00:27:11,300
vector of Rome and this means that if

574
00:27:08,690 --> 00:27:14,630
you we analyze enough sentences in one

575
00:27:11,300 --> 00:27:17,360
language we start to develop a very

576
00:27:14,630 --> 00:27:19,580
structured model or how that language

577
00:27:17,360 --> 00:27:25,360
works internally which is a language

578
00:27:19,580 --> 00:27:30,470
model our language model we need so we

579
00:27:25,360 --> 00:27:32,899
could induce a parallel corpus between

580
00:27:30,470 --> 00:27:36,320
the two independent languages by just

581
00:27:32,900 --> 00:27:38,690
mapping concepts between spaces in Latin

582
00:27:36,320 --> 00:27:40,340
spaces and we can bootstrap with a

583
00:27:38,690 --> 00:27:43,190
vocabulary for example we have

584
00:27:40,340 --> 00:27:46,189
vocabulary for Italian and Norwegian and

585
00:27:43,190 --> 00:27:49,220
so we know which some words translate

586
00:27:46,190 --> 00:27:51,140
into another and you can stop you can

587
00:27:49,220 --> 00:27:53,090
just bootstrap from them and then you

588
00:27:51,140 --> 00:27:56,230
can start mapping similar concepts or

589
00:27:53,090 --> 00:28:00,949
you can just use unique entities and

590
00:27:56,230 --> 00:28:03,640
frequencies so the the article is the

591
00:28:00,950 --> 00:28:06,290
very same frequency as the Italian

592
00:28:03,640 --> 00:28:10,070
corresponding article which is ill so

593
00:28:06,290 --> 00:28:12,500
you can hum as the very same frequency I

594
00:28:10,070 --> 00:28:16,490
can expect in a corpus as Casa Italian

595
00:28:12,500 --> 00:28:19,100
so we can just use this as eristic to

596
00:28:16,490 --> 00:28:24,650
guide our mapping in this very big Latin

597
00:28:19,100 --> 00:28:26,800
space so a very legit question is one

598
00:28:24,650 --> 00:28:29,450
the result be really really really noisy

599
00:28:26,800 --> 00:28:32,570
because you will end up with a lot a lot

600
00:28:29,450 --> 00:28:35,480
a lot of false positives yes but we

601
00:28:32,570 --> 00:28:40,970
could use statistics to compute means

602
00:28:35,480 --> 00:28:44,600
and among how many times house seems to

603
00:28:40,970 --> 00:28:48,170
align with dog or to casa

604
00:28:44,600 --> 00:28:51,949
and inferred the true positive throwing

605
00:28:48,170 --> 00:28:53,270
away all the false positives and here

606
00:28:51,950 --> 00:28:54,980
comes the phrase base machine

607
00:28:53,270 --> 00:28:56,990
translation which is which is the old

608
00:28:54,980 --> 00:28:59,060
technology there was a completely scrap

609
00:28:56,990 --> 00:29:01,250
the way when we came up with neural

610
00:28:59,060 --> 00:29:04,909
machine translation the first base

611
00:29:01,250 --> 00:29:07,490
machine translation had revolved around

612
00:29:04,910 --> 00:29:12,470
the concept that coal currencies of

613
00:29:07,490 --> 00:29:14,600
words are significantly our statistical

614
00:29:12,470 --> 00:29:18,020
significance sorry for example you see

615
00:29:14,600 --> 00:29:21,770
this character here what do you think is

616
00:29:18,020 --> 00:29:23,450
the translation of that character shrimp

617
00:29:21,770 --> 00:29:27,470
because it's the only character there of

618
00:29:23,450 --> 00:29:30,740
course always so you can assume that is

619
00:29:27,470 --> 00:29:37,010
the transition of shrimp and this is

620
00:29:30,740 --> 00:29:39,500
somewhat Berkeley if you happen to meet

621
00:29:37,010 --> 00:29:41,379
broccoli again in a world and you know

622
00:29:39,500 --> 00:29:44,300
the sentence and you just see this one

623
00:29:41,380 --> 00:29:46,940
maybe this is broccoli and these are a

624
00:29:44,300 --> 00:29:50,270
particle like something some connective

625
00:29:46,940 --> 00:29:53,090
that it's used it's a peculiar to the

626
00:29:50,270 --> 00:29:55,910
Chinese language and this is the way we

627
00:29:53,090 --> 00:29:58,340
used to train the old Google Translate

628
00:29:55,910 --> 00:30:00,020
now where Translate is narrow by in the

629
00:29:58,340 --> 00:30:02,629
old times it was a based on this

630
00:30:00,020 --> 00:30:04,190
technology which is a toolkit an

631
00:30:02,630 --> 00:30:07,300
open-source toolkit of course which is

632
00:30:04,190 --> 00:30:09,650
called Moses and has been developed by

633
00:30:07,300 --> 00:30:10,870
University of Toronto University of

634
00:30:09,650 --> 00:30:14,420
Edinburgh

635
00:30:10,870 --> 00:30:18,500
so the co-occurrences counts between

636
00:30:14,420 --> 00:30:21,860
words and sequence of words because I've

637
00:30:18,500 --> 00:30:24,910
been at school school can align with

638
00:30:21,860 --> 00:30:27,139
squalor in Italian but also I have been

639
00:30:24,910 --> 00:30:30,500
it's a sentence days alway always

640
00:30:27,140 --> 00:30:32,210
translates in sono stato in Italian so I

641
00:30:30,500 --> 00:30:34,850
can always count on only the

642
00:30:32,210 --> 00:30:37,130
co-occurrences of words school squad ah

643
00:30:34,850 --> 00:30:39,439
but also I've been so instead Oh

644
00:30:37,130 --> 00:30:41,990
and so I count the phrases and the

645
00:30:39,440 --> 00:30:44,600
phrases I treat them as a unit of

646
00:30:41,990 --> 00:30:48,050
translation and that is subject to call

647
00:30:44,600 --> 00:30:49,699
currencies counting too so I can use

648
00:30:48,050 --> 00:30:52,550
those who to calculate the translation

649
00:30:49,700 --> 00:30:55,030
probabilities a probability is just the

650
00:30:52,550 --> 00:30:59,149
idea that the seven times out of ten

651
00:30:55,030 --> 00:31:02,000
Casas is observed to koku with the house

652
00:30:59,150 --> 00:31:03,500
and that's a probability so I can just

653
00:31:02,000 --> 00:31:06,080
come up with the probabilities and

654
00:31:03,500 --> 00:31:09,980
choose the most probable sentence among

655
00:31:06,080 --> 00:31:11,960
all those who get to be aligned this

656
00:31:09,980 --> 00:31:13,700
probabilities are always I'm notated in

657
00:31:11,960 --> 00:31:15,830
a database a special database which is

658
00:31:13,700 --> 00:31:18,080
called phrase table creating a phrase

659
00:31:15,830 --> 00:31:21,500
table is the most expensive operation

660
00:31:18,080 --> 00:31:22,970
office-based machine translation so the

661
00:31:21,500 --> 00:31:24,800
most of China is based on three

662
00:31:22,970 --> 00:31:26,930
technologies which is the language model

663
00:31:24,800 --> 00:31:28,310
yes Telemark NLM is the language model

664
00:31:26,930 --> 00:31:30,740
calculates the probabilities of a

665
00:31:28,310 --> 00:31:32,960
sentence being meaningful then we have

666
00:31:30,740 --> 00:31:35,180
the aligner which is Gita and later was

667
00:31:32,960 --> 00:31:36,860
superseded by fast align and Moses which

668
00:31:35,180 --> 00:31:39,320
decodes the incoming message

669
00:31:36,860 --> 00:31:41,120
Moses projects the input sentence over

670
00:31:39,320 --> 00:31:43,100
phrase table to retrieve translation

671
00:31:41,120 --> 00:31:45,350
options such as among all the different

672
00:31:43,100 --> 00:31:47,750
options guided by the language model as

673
00:31:45,350 --> 00:31:50,649
a heuristic and that stops by itself

674
00:31:47,750 --> 00:31:54,020
when all input sentence has been covered

675
00:31:50,650 --> 00:31:56,840
and today we are presenting the second

676
00:31:54,020 --> 00:31:59,690
technologies which is Manos s mores as

677
00:31:56,840 --> 00:32:02,959
either stems from the paper from these

678
00:31:59,690 --> 00:32:05,420
three scientists and it's basically a

679
00:32:02,960 --> 00:32:07,760
toolkit to create a phrase tables from

680
00:32:05,420 --> 00:32:11,600
two more linguae datasets through word

681
00:32:07,760 --> 00:32:14,930
embedding then it creates a Moses model

682
00:32:11,600 --> 00:32:16,820
with this noisy thing does some fine

683
00:32:14,930 --> 00:32:21,440
tuning in the iteratively augment the

684
00:32:16,820 --> 00:32:23,810
data set by translating itself and so

685
00:32:21,440 --> 00:32:26,300
you can start with 1 million sentences

686
00:32:23,810 --> 00:32:28,520
and you rapidly go to 10 million

687
00:32:26,300 --> 00:32:31,280
sentences because you try different

688
00:32:28,520 --> 00:32:33,800
combinations between sentences that you

689
00:32:31,280 --> 00:32:37,190
have started with it's noisy

690
00:32:33,800 --> 00:32:40,490
sure but the sheer amount of data do you

691
00:32:37,190 --> 00:32:44,330
come up with ends up averaging out the

692
00:32:40,490 --> 00:32:48,620
noise in the long run and this is what I

693
00:32:44,330 --> 00:32:52,820
brought today so the second demo that

694
00:32:48,620 --> 00:32:55,070
I'm showing you today is Moses my going

695
00:32:52,820 --> 00:32:58,570
to be my small contribution has been a

696
00:32:55,070 --> 00:33:01,100
packaging this really really wild

697
00:32:58,570 --> 00:33:03,560
research prototype into a docker

698
00:33:01,100 --> 00:33:07,939
formatter so it's easy to deploy it's

699
00:33:03,560 --> 00:33:11,009
easy to use you have the

700
00:33:07,940 --> 00:33:14,789
the pipeline to train so you just take

701
00:33:11,009 --> 00:33:16,889
your two big text files with all Italian

702
00:33:14,789 --> 00:33:19,440
sentences and all Norwegian sentences

703
00:33:16,889 --> 00:33:20,610
for example and this sentence must not

704
00:33:19,440 --> 00:33:22,559
be related to each other

705
00:33:20,610 --> 00:33:25,320
you just take Italian Wikipedia and

706
00:33:22,559 --> 00:33:27,658
Norwegian Wikipedia or all the news to

707
00:33:25,320 --> 00:33:29,850
set up for the region for example and

708
00:33:27,659 --> 00:33:31,289
you stash in two separate files they

709
00:33:29,850 --> 00:33:33,959
don't have to be parallel they have to

710
00:33:31,289 --> 00:33:38,759
be monolingual then you launch the

711
00:33:33,960 --> 00:33:40,289
training passes a week and when the

712
00:33:38,759 --> 00:33:42,360
process is finished you will have a

713
00:33:40,289 --> 00:33:44,190
several gigabytes of files inside this

714
00:33:42,360 --> 00:33:46,979
train directory which is the models the

715
00:33:44,190 --> 00:33:49,980
phrase tables and you cannot shoot this

716
00:33:46,980 --> 00:33:51,480
translation server with the full with

717
00:33:49,980 --> 00:33:53,220
the following syntax you have to point

718
00:33:51,480 --> 00:33:55,919
it into the director of the model and

719
00:33:53,220 --> 00:33:58,139
you can now query your server with the

720
00:33:55,919 --> 00:33:59,369
following so you have this API which is

721
00:33:58,139 --> 00:34:01,498
another thing that I've built

722
00:33:59,369 --> 00:34:05,820
because Moses has no API server let's

723
00:34:01,499 --> 00:34:09,060
say so I built a flask based Python HTTP

724
00:34:05,820 --> 00:34:13,199
API please note that you have the verb

725
00:34:09,060 --> 00:34:16,619
here the query which is over in this

726
00:34:13,199 --> 00:34:19,859
case and the source language

727
00:34:16,619 --> 00:34:22,349
did I say Norwegian Swedish yes this

728
00:34:19,859 --> 00:34:25,230
because Moses doesn't have a Norwegian

729
00:34:22,349 --> 00:34:27,569
tokenizer to analyze the sentence but

730
00:34:25,230 --> 00:34:31,109
Swedish is really close so I used the

731
00:34:27,569 --> 00:34:33,810
Swedish tokenizer sorry but and the

732
00:34:31,109 --> 00:34:36,779
target in Italian and since this server

733
00:34:33,810 --> 00:34:42,839
is actually online I can also provide

734
00:34:36,780 --> 00:34:45,119
the working demo here it's really really

735
00:34:42,839 --> 00:34:48,089
really really slow because in this first

736
00:34:45,119 --> 00:34:50,369
version it tries to load the model in

737
00:34:48,089 --> 00:34:52,619
memory each time it's rather than the

738
00:34:50,369 --> 00:34:54,480
memory thrown away you take another

739
00:34:52,619 --> 00:34:57,089
sentence either loaded in memory at a

740
00:34:54,480 --> 00:35:00,720
time it will burn is just a show of

741
00:34:57,089 --> 00:35:07,078
walvis looping and these are JSON output

742
00:35:00,720 --> 00:35:11,399
though you have here so now I'm in order

743
00:35:07,079 --> 00:35:17,569
to show you how this thing work I have

744
00:35:11,400 --> 00:35:17,569
to go here and then

745
00:35:19,310 --> 00:35:28,380
select Swedish then I go here I try to

746
00:35:26,700 --> 00:35:31,319
disable the lookups because I'm

747
00:35:28,380 --> 00:35:35,730
purposely fully using a brown language I

748
00:35:31,320 --> 00:35:38,460
have to fix this someday so I don't get

749
00:35:35,730 --> 00:35:41,070
suggestions for the Swedish and then I

750
00:35:38,460 --> 00:35:44,600
add them Tianjin I chooses the provider

751
00:35:41,070 --> 00:35:47,970
Moses because I try to imitate the Moses

752
00:35:44,600 --> 00:35:51,930
HTTP / with the API I try to just

753
00:35:47,970 --> 00:35:57,299
implement it I add the whoops and rename

754
00:35:51,930 --> 00:36:01,560
which is mongooses okay and is the

755
00:35:57,300 --> 00:36:06,120
server I keep you just type a dummy you

756
00:36:01,560 --> 00:36:13,200
don't need it okay now we have to look

757
00:36:06,120 --> 00:36:17,100
for a file which I have somewhere and

758
00:36:13,200 --> 00:36:26,759
actually forgot where I stored it which

759
00:36:17,100 --> 00:36:29,370
is embarrassing and I think I what files

760
00:36:26,760 --> 00:36:32,460
okay I know I have a region file right

761
00:36:29,370 --> 00:36:36,390
now to show you but the idea is that you

762
00:36:32,460 --> 00:36:39,170
will end up with a very very very slow

763
00:36:36,390 --> 00:36:43,109
server that will actually provide

764
00:36:39,170 --> 00:36:45,960
translations to you not in the Google

765
00:36:43,110 --> 00:36:49,440
Translate speed very wise lower fashion

766
00:36:45,960 --> 00:36:51,990
and the translation are not even perfect

767
00:36:49,440 --> 00:36:54,840
but are good for a starting point to

768
00:36:51,990 --> 00:36:56,459
start translating your software and the

769
00:36:54,840 --> 00:36:59,400
advantage of these is that you don't

770
00:36:56,460 --> 00:37:02,130
even if you are Norwegian or Sudanese so

771
00:36:59,400 --> 00:37:06,360
you have a very very small presence

772
00:37:02,130 --> 00:37:10,530
online in terms of amount of data

773
00:37:06,360 --> 00:37:12,630
produced and parallel data produced with

774
00:37:10,530 --> 00:37:17,790
this kind of technology you can just

775
00:37:12,630 --> 00:37:22,490
throw it throw any enough data from

776
00:37:17,790 --> 00:37:26,880
books from articles from cooking recipes

777
00:37:22,490 --> 00:37:29,700
from subtitles of movies and that you

778
00:37:26,880 --> 00:37:30,520
have them a lot of if you go in Abu so

779
00:37:29,700 --> 00:37:32,649
the parallel

780
00:37:30,520 --> 00:37:35,140
corpora and you take all only the

781
00:37:32,650 --> 00:37:40,260
monolingual part that you are interest

782
00:37:35,140 --> 00:37:45,819
into then you might be able to actually

783
00:37:40,260 --> 00:37:48,220
build a very huge model proof is that

784
00:37:45,820 --> 00:37:50,380
I've launched the trade the the the

785
00:37:48,220 --> 00:37:52,390
model that I show you today is just a

786
00:37:50,380 --> 00:37:54,700
test model with few data the serious

787
00:37:52,390 --> 00:37:56,650
model for Italian Norwegian has been

788
00:37:54,700 --> 00:38:00,819
running for five days and hasn't

789
00:37:56,650 --> 00:38:03,730
finished training yet so I cannot really

790
00:38:00,820 --> 00:38:06,520
wait to you because it's just training

791
00:38:03,730 --> 00:38:08,380
on a 50 core machine has been running

792
00:38:06,520 --> 00:38:11,410
from for days and 5000 machine it's not

793
00:38:08,380 --> 00:38:13,720
looping it's just that it it does a

794
00:38:11,410 --> 00:38:17,980
really really longer refinement process

795
00:38:13,720 --> 00:38:21,270
so it induces this first table takes 10

796
00:38:17,980 --> 00:38:26,170
hours build the first model 20 minutes

797
00:38:21,270 --> 00:38:28,750
then starts doing some fine-tuning some

798
00:38:26,170 --> 00:38:33,280
parameter fine-tuning and this takes up

799
00:38:28,750 --> 00:38:37,150
approximately another hour or so then

800
00:38:33,280 --> 00:38:40,750
goes to the monolingual corpus translate

801
00:38:37,150 --> 00:38:43,210
it with the model it has built and the

802
00:38:40,750 --> 00:38:45,640
generates two parallel corpora because

803
00:38:43,210 --> 00:38:47,080
it goes to Norwegian applies the Italian

804
00:38:45,640 --> 00:38:48,730
origin model and generate a

805
00:38:47,080 --> 00:38:50,799
corresponding Italian translation and

806
00:38:48,730 --> 00:38:52,570
that's the same for the other and then

807
00:38:50,800 --> 00:38:55,780
now you have two very big parallel

808
00:38:52,570 --> 00:38:58,090
corpora that are used to augment the

809
00:38:55,780 --> 00:39:00,130
rough model that you started with it's

810
00:38:58,090 --> 00:39:02,410
like a vida be training you bootstrap

811
00:39:00,130 --> 00:39:05,700
with a rough model and use yourself to

812
00:39:02,410 --> 00:39:08,830
improve your own training so if you're

813
00:39:05,700 --> 00:39:11,379
supplied like me seven hundred millions

814
00:39:08,830 --> 00:39:13,240
of words this model takes a long time to

815
00:39:11,380 --> 00:39:15,610
translate seven hundred millions of

816
00:39:13,240 --> 00:39:18,120
stuff then you do another fine-tuning

817
00:39:15,610 --> 00:39:21,310
and then you do that again iterative ten

818
00:39:18,120 --> 00:39:24,160
tuning iterations four three iterations

819
00:39:21,310 --> 00:39:27,340
of back translating all your corpus from

820
00:39:24,160 --> 00:39:30,430
scratch takes a ridiculous amount of

821
00:39:27,340 --> 00:39:32,110
time so I hope to actually post some

822
00:39:30,430 --> 00:39:37,839
updates or how the experience went

823
00:39:32,110 --> 00:39:40,660
actually so in conclusion today what we

824
00:39:37,840 --> 00:39:43,090
add here so we have a docker packages

825
00:39:40,660 --> 00:39:43,850
version of the Moniz's which is ready to

826
00:39:43,090 --> 00:39:46,670
use to journey

827
00:39:43,850 --> 00:39:48,740
the training set we have an HTTP API

828
00:39:46,670 --> 00:39:50,630
server to query the model obtained this

829
00:39:48,740 --> 00:39:52,819
way and it's all available in this

830
00:39:50,630 --> 00:39:55,010
repository and then we have a packages

831
00:39:52,820 --> 00:39:57,320
version of maitika tool which is clewd

832
00:39:55,010 --> 00:40:01,130
also my single server and acti MQ

833
00:39:57,320 --> 00:40:03,110
instance virus demons to perform the

834
00:40:01,130 --> 00:40:05,990
analysis and the translation in the

835
00:40:03,110 --> 00:40:09,650
background and Apache to web app with a

836
00:40:05,990 --> 00:40:12,649
PHP a very Mongoose PHP Roboto

837
00:40:09,650 --> 00:40:14,960
rounds in docker composer but I aim to

838
00:40:12,650 --> 00:40:17,810
support you Benares so you can just

839
00:40:14,960 --> 00:40:21,410
deploy that anywhere and you can start

840
00:40:17,810 --> 00:40:26,440
your own very small LSP

841
00:40:21,410 --> 00:40:29,240
at home for your open projects kudos to

842
00:40:26,440 --> 00:40:31,520
the for Knights which are those were

843
00:40:29,240 --> 00:40:33,799
perpetrated in the cover of my

844
00:40:31,520 --> 00:40:35,120
presentation which is v co informant in

845
00:40:33,800 --> 00:40:37,100
the phrase base machine translation

846
00:40:35,120 --> 00:40:38,960
thomas michael off for inventing work to

847
00:40:37,100 --> 00:40:41,180
act which is the the mapping and

848
00:40:38,960 --> 00:40:42,940
language model with those vectors at

849
00:40:41,180 --> 00:40:45,290
ampush gate for inventing pi torch

850
00:40:42,940 --> 00:40:47,120
without which we will be never be able

851
00:40:45,290 --> 00:40:49,310
to actually train anything Niro and

852
00:40:47,120 --> 00:40:50,930
Michael are station for putting all

853
00:40:49,310 --> 00:40:53,990
together and be the author of the paper

854
00:40:50,930 --> 00:40:56,109
I took inspiration from thank you very

855
00:40:53,990 --> 00:40:56,109
much

856
00:41:00,130 --> 00:41:10,940
so are there any questions is the faces

857
00:41:07,160 --> 00:41:15,580
of the people the cover is sergeant

858
00:41:10,940 --> 00:41:15,580
pepper's Lonely Hearts Club Band yes

859
00:41:18,410 --> 00:41:21,520
[Music]

860
00:41:26,270 --> 00:41:29,670
[Music]

861
00:41:30,059 --> 00:41:40,930
yes yes yes so your colleague is asking

862
00:41:37,150 --> 00:41:43,809
when you translate a sentence which has

863
00:41:40,930 --> 00:41:47,440
some markup into another language who

864
00:41:43,809 --> 00:41:50,559
happened to remix the the words in a

865
00:41:47,440 --> 00:41:52,930
different order the tag formatting is

866
00:41:50,559 --> 00:41:55,540
not preserved because for example you

867
00:41:52,930 --> 00:41:57,910
may end up with a sentence between tags

868
00:41:55,540 --> 00:42:00,640
which is split into different sentences

869
00:41:57,910 --> 00:42:05,078
with something for in-between then you

870
00:42:00,640 --> 00:42:09,910
should duplicate the tags so this

871
00:42:05,079 --> 00:42:12,819
problem has been solved by Christian

872
00:42:09,910 --> 00:42:15,098
Bakr which is a Google brain scientist

873
00:42:12,819 --> 00:42:18,970
and working in the University of

874
00:42:15,099 --> 00:42:22,780
Edinburgh and he basically employed the

875
00:42:18,970 --> 00:42:25,959
trick so he factorized all the tagging

876
00:42:22,780 --> 00:42:29,319
if you have a sentence five words are

877
00:42:25,960 --> 00:42:32,829
tagged you put tags in each separate

878
00:42:29,319 --> 00:42:35,440
five words so you multiply the tags so

879
00:42:32,829 --> 00:42:38,079
that each single word is between is

880
00:42:35,440 --> 00:42:41,859
bolded independently then you translate

881
00:42:38,079 --> 00:42:44,230
you let those be remixed and then if you

882
00:42:41,859 --> 00:42:46,119
happen to have in the target language a

883
00:42:44,230 --> 00:42:48,970
sentence which to tax you can collapse

884
00:42:46,119 --> 00:42:50,079
them into words between one tag you

885
00:42:48,970 --> 00:42:53,020
solve them that way

886
00:42:50,079 --> 00:42:55,359
this technology that I have developed

887
00:42:53,020 --> 00:42:57,640
the doesn't feature the tracker which is

888
00:42:55,359 --> 00:43:01,109
a one of the first things I'm going to

889
00:42:57,640 --> 00:43:04,089
add actually because it's a very naive

890
00:43:01,109 --> 00:43:07,299
trick but it works very well actually

891
00:43:04,089 --> 00:43:09,700
and so I will scavenge that from the

892
00:43:07,299 --> 00:43:13,299
mid-cap project because the meta project

893
00:43:09,700 --> 00:43:16,598
this mate cat means machine translation

894
00:43:13,299 --> 00:43:18,040
unanswered the CAD tool and it shipped

895
00:43:16,599 --> 00:43:21,990
the original project when it was a

896
00:43:18,040 --> 00:43:21,990
research project with an array of

897
00:43:22,699 --> 00:43:28,640
little stuff like this who actually

898
00:43:25,150 --> 00:43:32,509
improve it a lot the the quality of the

899
00:43:28,640 --> 00:43:34,249
the engine I will try to go back in the

900
00:43:32,509 --> 00:43:36,619
old rapper when it was a research

901
00:43:34,249 --> 00:43:38,419
project and pulled out those back in now

902
00:43:36,619 --> 00:43:41,390
I've developed from scratch sure because

903
00:43:38,420 --> 00:43:59,569
it was just much easier to for the

904
00:43:41,390 --> 00:44:02,209
purpose of this conference okay if I saw

905
00:43:59,569 --> 00:44:04,640
your colleague asked you don't need the

906
00:44:02,209 --> 00:44:07,219
parallel corpus to Train monus's but

907
00:44:04,640 --> 00:44:09,259
what if you happen to use a to bowling

908
00:44:07,219 --> 00:44:13,039
word corpora which is in fact two

909
00:44:09,259 --> 00:44:18,489
parallel corpora will be easier or

910
00:44:13,039 --> 00:44:21,859
faster not faster by will be less noisy

911
00:44:18,489 --> 00:44:25,789
less noisy because you will end up with

912
00:44:21,859 --> 00:44:28,038
the good quality mappings which usually

913
00:44:25,789 --> 00:44:30,949
they end up for being very noisy when

914
00:44:28,039 --> 00:44:33,589
you use more lingual corpora because you

915
00:44:30,949 --> 00:44:36,469
end up with a lot of the false positives

916
00:44:33,589 --> 00:44:39,380
if you happen to use a parallel corpus

917
00:44:36,469 --> 00:44:42,709
those mappings end up for be high Kwaito

918
00:44:39,380 --> 00:44:44,839
mappings so your training is actually of

919
00:44:42,709 --> 00:44:47,390
a better quality you end up with a

920
00:44:44,839 --> 00:44:48,578
better model with less noise and so you

921
00:44:47,390 --> 00:44:51,650
have to spend less time doing

922
00:44:48,579 --> 00:44:54,349
fine-tuning a back translation and you

923
00:44:51,650 --> 00:44:57,049
don't have to create this really really

924
00:44:54,349 --> 00:45:00,259
really huge amount of data only for the

925
00:44:57,049 --> 00:45:02,779
sake of a Monte Carlo sampling that

926
00:45:00,259 --> 00:45:05,799
averages out the noise leaving the good

927
00:45:02,779 --> 00:45:05,799
stuff for in this place

928
00:45:21,210 --> 00:45:35,250
I am I think I didn't I understand the

929
00:45:24,960 --> 00:45:37,950
question yeah so all of these files

930
00:45:35,250 --> 00:45:43,670
which is the RTD officer the document

931
00:45:37,950 --> 00:45:46,769
the Excel PowerPoint also web pages also

932
00:45:43,670 --> 00:45:49,019
scans files it's true only if you put an

933
00:45:46,769 --> 00:45:51,509
advance at the filter which is a

934
00:45:49,019 --> 00:45:55,078
probability of translated and it does

935
00:45:51,510 --> 00:45:58,140
all OCR or PDF you can also just

936
00:45:55,079 --> 00:46:02,519
translate directly a TM x or x live and

937
00:45:58,140 --> 00:46:04,410
also desktop publishing stuff and vice

938
00:46:02,519 --> 00:46:09,359
localization formats like the properties

939
00:46:04,410 --> 00:46:11,279
and the ways the strings so all this

940
00:46:09,359 --> 00:46:14,940
thing whenever you put something in an

941
00:46:11,279 --> 00:46:17,519
ex-slave is generated behind the scenes

942
00:46:14,940 --> 00:46:20,460
which is the envelope container all the

943
00:46:17,519 --> 00:46:22,680
strings get pulled out and they are

944
00:46:20,460 --> 00:46:25,140
inserted in a temporary location when

945
00:46:22,680 --> 00:46:28,740
you translate you replace those strings

946
00:46:25,140 --> 00:46:31,140
and then when you do download or preview

947
00:46:28,740 --> 00:46:34,769
those strings get packet back into the X

948
00:46:31,140 --> 00:46:38,000
left the blob of the file is just a

949
00:46:34,769 --> 00:46:41,549
matter of replacing placeholders and

950
00:46:38,000 --> 00:46:45,569
packing all the Byner him and you end up

951
00:46:41,549 --> 00:46:51,509
with a PowerPoint or a docx file in the

952
00:46:45,569 --> 00:46:54,210
end at the other file format

953
00:46:51,509 --> 00:46:55,680
okay so okay you're the real question of

954
00:46:54,210 --> 00:46:57,569
your colleague was there was the process

955
00:46:55,680 --> 00:47:00,058
of adding another file format you

956
00:46:57,569 --> 00:47:02,759
basically have to implement a new class

957
00:47:00,059 --> 00:47:05,869
behind which is the class for that file

958
00:47:02,759 --> 00:47:12,960
format the idea is that the interface

959
00:47:05,869 --> 00:47:15,359
aspects here's a blob give me the Dex

960
00:47:12,960 --> 00:47:20,160
lift strings so you just have to

961
00:47:15,359 --> 00:47:22,710
implement that kind of process it might

962
00:47:20,160 --> 00:47:26,069
be really easy for example if it's you

963
00:47:22,710 --> 00:47:28,650
know llamo or JSON because you just have

964
00:47:26,069 --> 00:47:32,430
to come you can come up with a template

965
00:47:28,650 --> 00:47:33,780
ax left and you convert a JSON into an

966
00:47:32,430 --> 00:47:36,390
XML

967
00:47:33,780 --> 00:47:39,030
in the format it's life so you can just

968
00:47:36,390 --> 00:47:41,129
take the strings and you paste those in

969
00:47:39,030 --> 00:47:44,880
this kind of format if it's a more

970
00:47:41,130 --> 00:47:46,350
advanced of the format and I I'm coming

971
00:47:44,880 --> 00:47:49,050
up with the most difficult format out

972
00:47:46,350 --> 00:47:50,640
there which is the AutoCAD aldicott is a

973
00:47:49,050 --> 00:47:53,330
total nightmare because it's

974
00:47:50,640 --> 00:47:58,170
undocumented it's a heavily binary and

975
00:47:53,330 --> 00:48:00,810
the strings are basically shredded

976
00:47:58,170 --> 00:48:03,320
everywhere in the thing then you have to

977
00:48:00,810 --> 00:48:06,810
come up with something that is able to

978
00:48:03,320 --> 00:48:09,030
read the strings place a placeholder in

979
00:48:06,810 --> 00:48:11,730
the original file and you pack these

980
00:48:09,030 --> 00:48:14,490
interacts left and then you constrict

981
00:48:11,730 --> 00:48:16,350
the sentences out there I can show you

982
00:48:14,490 --> 00:48:27,649
acts live because it's not really the

983
00:48:16,350 --> 00:48:27,650
field once you see it here we are media

984
00:48:29,480 --> 00:48:39,390
so as you see here an excellent file you

985
00:48:34,590 --> 00:48:42,060
have issue have a very big blob which is

986
00:48:39,390 --> 00:48:44,190
the binary basis 64 encoded version of

987
00:48:42,060 --> 00:48:48,029
the file with the placeholders already

988
00:48:44,190 --> 00:48:50,280
put in between then you have the units

989
00:48:48,030 --> 00:48:53,160
which is a translation unit a sentence

990
00:48:50,280 --> 00:48:56,510
which has always a segment with a source

991
00:48:53,160 --> 00:48:58,770
a target so for example you eat two

992
00:48:56,510 --> 00:49:00,990
segments an application to my boy

993
00:48:58,770 --> 00:49:04,170
temporary success document and here you

994
00:49:00,990 --> 00:49:07,680
have the corresponding target this file

995
00:49:04,170 --> 00:49:12,090
is all from English to Japanese because

996
00:49:07,680 --> 00:49:14,069
of this Heather ear and then you go down

997
00:49:12,090 --> 00:49:16,290
and you keep having units a segment

998
00:49:14,070 --> 00:49:19,530
source and target unit segment source

999
00:49:16,290 --> 00:49:21,840
and target this is the very basic unit

1000
00:49:19,530 --> 00:49:25,430
of work so if to come up with another

1001
00:49:21,840 --> 00:49:28,680
way of producing annex life from a

1002
00:49:25,430 --> 00:49:31,830
profit i-5 format you have to come up

1003
00:49:28,680 --> 00:49:33,330
with a way to construe this XML once you

1004
00:49:31,830 --> 00:49:36,720
have the sentences is really easy you

1005
00:49:33,330 --> 00:49:39,900
can compose this example it's easy the

1006
00:49:36,720 --> 00:49:42,899
very difficult thing is that starting

1007
00:49:39,900 --> 00:49:45,960
from this stuff you just have to find a

1008
00:49:42,900 --> 00:49:47,350
way to take out strings put a

1009
00:49:45,960 --> 00:49:50,980
placeholder

1010
00:49:47,350 --> 00:49:54,730
with the ID of the unit you have here

1011
00:49:50,980 --> 00:49:58,300
and then you have to base64 encode and

1012
00:49:54,730 --> 00:50:04,090
stash it here let me just show you a

1013
00:49:58,300 --> 00:50:09,030
real one because since we are in I can

1014
00:50:04,090 --> 00:50:14,850
show you this a real example of

1015
00:50:09,030 --> 00:50:23,070
something we am here nope

1016
00:50:14,850 --> 00:50:29,860
these are PowerPoint loading loading

1017
00:50:23,070 --> 00:50:32,110
more loading okay so you can see the

1018
00:50:29,860 --> 00:50:34,330
translation units with these the X left

1019
00:50:32,110 --> 00:50:37,600
for version 1.0 the one I show you on

1020
00:50:34,330 --> 00:50:45,000
Wikipedia was the 2.0 sorry for the

1021
00:50:37,600 --> 00:50:48,839
mismatch sure the is my GPU is crunching

1022
00:50:45,000 --> 00:50:48,840
how big was this file

1023
00:50:55,030 --> 00:51:08,059
in the meantime I'm just okay set

1024
00:51:02,990 --> 00:51:10,549
wrapped okay here you can see this is

1025
00:51:08,059 --> 00:51:13,250
the base64 encoded the file the

1026
00:51:10,550 --> 00:51:16,609
PowerPoint file encoded as a big 64 if I

1027
00:51:13,250 --> 00:51:18,130
had take this one and do the code I can

1028
00:51:16,609 --> 00:51:31,069
see that the binary

1029
00:51:18,130 --> 00:51:34,940
pptx you can see the aether here at the

1030
00:51:31,069 --> 00:51:37,300
wonder-percent CPU okay and here you

1031
00:51:34,940 --> 00:51:40,099
have all the different translation units

1032
00:51:37,300 --> 00:51:45,380
when you have the source

1033
00:51:40,099 --> 00:51:47,210
doc you bananas and with the segment

1034
00:51:45,380 --> 00:51:50,510
source and the target document Cuban

1035
00:51:47,210 --> 00:51:51,680
eyes has been basically generated as a

1036
00:51:50,510 --> 00:51:53,809
copy of this one

1037
00:51:51,680 --> 00:51:55,910
so whenever when you translate the your

1038
00:51:53,809 --> 00:51:58,040
tree right here you write on to the

1039
00:51:55,910 --> 00:52:00,770
database and when you export you pack

1040
00:51:58,040 --> 00:52:03,319
all the original strings in the place of

1041
00:52:00,770 --> 00:52:06,020
this one you can see when the tags a

1042
00:52:03,319 --> 00:52:11,839
year the G tags they were showing inside

1043
00:52:06,020 --> 00:52:16,280
the ater boxer these are very nasty file

1044
00:52:11,839 --> 00:52:21,410
format with a very heavy documented

1045
00:52:16,280 --> 00:52:24,670
standard you just use this as a working

1046
00:52:21,410 --> 00:52:27,410
table or as a workbench and then you

1047
00:52:24,670 --> 00:52:30,200
when you have all the strings base is 60

1048
00:52:27,410 --> 00:52:32,029
good day code this one subdued of the

1049
00:52:30,200 --> 00:52:35,720
placeholder which pointed to a single

1050
00:52:32,030 --> 00:52:39,319
unit save that the blob as a file and

1051
00:52:35,720 --> 00:52:41,500
you have a fully translated file format

1052
00:52:39,319 --> 00:52:45,349
output this is the most complicated

1053
00:52:41,500 --> 00:52:48,220
process in extending this capital but it

1054
00:52:45,349 --> 00:52:49,910
has been done for each of these formats

1055
00:52:48,220 --> 00:52:54,609
any other questions

1056
00:52:49,910 --> 00:52:54,609
sure okay

1057
00:52:58,749 --> 00:53:07,299
so I chose Italian Norwegian because I

1058
00:53:02,960 --> 00:53:10,249
knew their there were not a lot of

1059
00:53:07,299 --> 00:53:12,559
resources some just some millions of

1060
00:53:10,249 --> 00:53:18,459
words for example in English which is

1061
00:53:12,559 --> 00:53:22,219
huge is 80 millions of sentences then

1062
00:53:18,460 --> 00:53:25,819
times 10 for each word you go 810

1063
00:53:22,219 --> 00:53:28,999
millions of sentences Italian Norwegian

1064
00:53:25,819 --> 00:53:30,319
has 12 millions of sentences 6 millions

1065
00:53:28,999 --> 00:53:35,180
and then because I was listening to

1066
00:53:30,319 --> 00:53:37,009
invert actually that works it actually

1067
00:53:35,180 --> 00:53:39,618
serves them they're always the one which

1068
00:53:37,009 --> 00:53:41,269
doesn't work it's Sardinian I try to

1069
00:53:39,619 --> 00:53:43,130
translate in Serbia but with ten

1070
00:53:41,269 --> 00:53:44,508
thousand sentences it's impossible to

1071
00:53:43,130 --> 00:53:47,180
run with a mapping with ten turns

1072
00:53:44,509 --> 00:53:49,160
something this you need some millions of

1073
00:53:47,180 --> 00:53:51,890
words by it's easy because you just need

1074
00:53:49,160 --> 00:53:55,910
to crawl sardinia websites

1075
00:53:51,890 --> 00:53:57,200
Sardinian books and stuff it's not

1076
00:53:55,910 --> 00:53:59,509
difficult to find the monolingual

1077
00:53:57,200 --> 00:54:01,578
corpora the real challenge is to find

1078
00:53:59,509 --> 00:54:03,349
the parallel corpora but this technology

1079
00:54:01,579 --> 00:54:05,989
turns the problem into scavenging

1080
00:54:03,349 --> 00:54:07,999
modeling work apparently which is the

1081
00:54:05,989 --> 00:54:10,009
more content you can just scroll in just

1082
00:54:07,999 --> 00:54:11,238
one single language totally unrelated

1083
00:54:10,009 --> 00:54:19,789
with the other language you want to

1084
00:54:11,239 --> 00:54:27,289
Carolla it's just an easier task yeah

1085
00:54:19,789 --> 00:54:31,729
google has it sure like google books for

1086
00:54:27,289 --> 00:54:35,329
example they have the crawler then align

1087
00:54:31,729 --> 00:54:37,788
it the 1 billion words invites languages

1088
00:54:35,329 --> 00:54:42,160
with good books and ice the top source

1089
00:54:37,789 --> 00:54:45,529
of the transition quality the when you

1090
00:54:42,160 --> 00:54:48,410
actually are on google maps and you bark

1091
00:54:45,529 --> 00:54:50,359
and a random address inside the google

1092
00:54:48,410 --> 00:54:54,109
maps and understands you that's because

1093
00:54:50,359 --> 00:54:58,069
it has a 1 trillion word the language

1094
00:54:54,109 --> 00:55:01,098
model built it's just so much data that

1095
00:54:58,069 --> 00:55:05,839
it oh it's always able to understand you

1096
00:55:01,099 --> 00:55:07,610
because he's always seen something so

1097
00:55:05,839 --> 00:55:09,580
you're on the region works the

1098
00:55:07,610 --> 00:55:13,640
I wanted to show you is still training

1099
00:55:09,580 --> 00:55:16,700
sorry about that but the idea is that it

1100
00:55:13,640 --> 00:55:18,319
takes always weeks of training but in

1101
00:55:16,700 --> 00:55:21,680
the end you will end up with a model

1102
00:55:18,320 --> 00:55:24,290
trained on a lot of data Moses since the

1103
00:55:21,680 --> 00:55:27,589
most is the last chain of the the ring

1104
00:55:24,290 --> 00:55:30,529
of the chain is not the best translation

1105
00:55:27,590 --> 00:55:32,510
software server out there B has been

1106
00:55:30,530 --> 00:55:34,220
superseded by the narrow machine

1107
00:55:32,510 --> 00:55:39,020
translation which is much more fluent

1108
00:55:34,220 --> 00:55:44,149
but since Moses is a phrase based the

1109
00:55:39,020 --> 00:55:45,759
counts are made as discrete counts so

1110
00:55:44,150 --> 00:55:49,970
it's always able to come up with the

1111
00:55:45,760 --> 00:55:54,140
probabilities even in absence of huge

1112
00:55:49,970 --> 00:55:56,149
data examples it's a it's a discrete

1113
00:55:54,140 --> 00:55:58,310
system opposed to a continues in the

1114
00:55:56,150 --> 00:56:01,430
mathematical sense system for the neural

1115
00:55:58,310 --> 00:56:05,060
networks generation quality is lower but

1116
00:56:01,430 --> 00:56:06,740
it is a score of 26 in a blue scale

1117
00:56:05,060 --> 00:56:09,590
which is the scale for translation

1118
00:56:06,740 --> 00:56:11,839
quality Google with an arrow achieves

1119
00:56:09,590 --> 00:56:14,360
the 40 points which is outstanding a

1120
00:56:11,840 --> 00:56:18,440
26-hour capers translating star drafting

1121
00:56:14,360 --> 00:56:19,430
I think our time's up thank you so much

1122
00:56:18,440 --> 00:56:28,890
for attending here

1123
00:56:19,430 --> 00:56:28,890
[Applause]

