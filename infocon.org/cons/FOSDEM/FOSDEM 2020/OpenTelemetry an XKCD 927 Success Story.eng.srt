1
00:00:04,960 --> 00:00:07,120
but before that i just wanted to let you

2
00:00:06,480 --> 00:00:10,240
guys know that

3
00:00:07,120 --> 00:00:13,519
you can see these uh papers that

4
00:00:10,240 --> 00:00:16,800
uh we have here uh taped to the

5
00:00:13,519 --> 00:00:18,160
to the door and the about an after party

6
00:00:16,800 --> 00:00:19,920
that we'll be having

7
00:00:18,160 --> 00:00:21,520
uh i don't know if you can fit this many

8
00:00:19,920 --> 00:00:23,840
people but uh

9
00:00:21,520 --> 00:00:25,680
definitely we can uh all get together

10
00:00:23,840 --> 00:00:26,880
and go out for for some years afterwards

11
00:00:25,680 --> 00:00:30,320
so yeah that will be

12
00:00:26,880 --> 00:00:32,558
sort of the location around around there

13
00:00:30,320 --> 00:00:35,120
yeah and we'll now start with the the

14
00:00:32,558 --> 00:00:38,640
talk uh by greg mefford

15
00:00:35,120 --> 00:00:40,718
so greg has been a a member of the

16
00:00:38,640 --> 00:00:42,399
sort of beam community specifically more

17
00:00:40,719 --> 00:00:45,039
involved with elixir

18
00:00:42,399 --> 00:00:46,480
with nerves uh he's a member of the eef

19
00:00:45,039 --> 00:00:48,320
observability group

20
00:00:46,480 --> 00:00:50,319
uh and he's also been heavily involved

21
00:00:48,320 --> 00:00:51,600
with the effort of distributed tracing

22
00:00:50,320 --> 00:00:53,360
within the community

23
00:00:51,600 --> 00:00:54,719
and today he's gonna present about open

24
00:00:53,360 --> 00:00:59,840
telemetry so

25
00:00:54,719 --> 00:00:59,840
yeah give it up for greg

26
00:01:02,160 --> 00:01:07,280
all right thank you so before i dive

27
00:01:05,119 --> 00:01:08,720
into specifically what open telemetry is

28
00:01:07,280 --> 00:01:10,159
and how it's a success story

29
00:01:08,720 --> 00:01:12,158
i want to talk about the problem that

30
00:01:10,159 --> 00:01:14,240
open telemetry tries to solve which is

31
00:01:12,159 --> 00:01:16,400
observability

32
00:01:14,240 --> 00:01:17,679
so you may have seen one of these

33
00:01:16,400 --> 00:01:18,960
instructions before where

34
00:01:17,680 --> 00:01:20,159
you have to push the button and then you

35
00:01:18,960 --> 00:01:22,080
receive the bacon when you're in the

36
00:01:20,159 --> 00:01:24,080
restroom

37
00:01:22,080 --> 00:01:25,439
so if you if you push the button and you

38
00:01:24,080 --> 00:01:26,640
don't receive bacon

39
00:01:25,439 --> 00:01:29,039
uh you're going to be heavily

40
00:01:26,640 --> 00:01:30,720
disappointed right so let's start with

41
00:01:29,040 --> 00:01:32,720
let's science this situation what what

42
00:01:30,720 --> 00:01:34,479
can we observe about the situation

43
00:01:32,720 --> 00:01:35,920
first of all no bacon this is very

44
00:01:34,479 --> 00:01:39,520
disappointing

45
00:01:35,920 --> 00:01:40,000
one out of five stars so there is warm

46
00:01:39,520 --> 00:01:42,320
air coming

47
00:01:40,000 --> 00:01:43,360
out though um is this supposed to cook

48
00:01:42,320 --> 00:01:46,398
the bacon i

49
00:01:43,360 --> 00:01:48,640
seems like that's not really safe

50
00:01:46,399 --> 00:01:50,000
there's also lots of noise so this

51
00:01:48,640 --> 00:01:50,479
machine's making a lot of noise is it

52
00:01:50,000 --> 00:01:52,320
broken

53
00:01:50,479 --> 00:01:53,840
or is it supposed to do that i don't i

54
00:01:52,320 --> 00:01:54,880
don't really know how this thing works

55
00:01:53,840 --> 00:01:57,119
um

56
00:01:54,880 --> 00:02:00,399
okay the noise stopped definitely broken

57
00:01:57,119 --> 00:02:00,399
uh no no bacon came out

58
00:02:00,640 --> 00:02:04,560
so if i engage my silver lining machine

59
00:02:02,799 --> 00:02:06,000
i can say well at least it responded to

60
00:02:04,560 --> 00:02:07,840
inputs right i pressed the button

61
00:02:06,000 --> 00:02:09,119
something happened uh there was a lot of

62
00:02:07,840 --> 00:02:11,280
correlation there so probably it's

63
00:02:09,119 --> 00:02:13,520
because i pressed the button

64
00:02:11,280 --> 00:02:14,400
um so when we talk about observability

65
00:02:13,520 --> 00:02:15,840
often

66
00:02:14,400 --> 00:02:17,520
your vendors will tell you that the

67
00:02:15,840 --> 00:02:20,640
pillars of observability are

68
00:02:17,520 --> 00:02:22,400
logs metrics and distributed traces i

69
00:02:20,640 --> 00:02:24,799
actually like to think about it more as

70
00:02:22,400 --> 00:02:26,160
different aspects on a like a cube of

71
00:02:24,800 --> 00:02:27,200
looking into your system from different

72
00:02:26,160 --> 00:02:29,440
dimensions

73
00:02:27,200 --> 00:02:30,958
so like the faces on the cube the

74
00:02:29,440 --> 00:02:33,120
aspects of observability

75
00:02:30,959 --> 00:02:35,440
are related on one axis and and

76
00:02:33,120 --> 00:02:37,200
different on another axis

77
00:02:35,440 --> 00:02:38,640
so if we consider distributed tracing

78
00:02:37,200 --> 00:02:40,000
for example

79
00:02:38,640 --> 00:02:42,559
you can learn a lot about what your

80
00:02:40,000 --> 00:02:43,920
system is doing in particular requests

81
00:02:42,560 --> 00:02:46,560
you can see if there was some kind of an

82
00:02:43,920 --> 00:02:48,958
alert or an error that was raised

83
00:02:46,560 --> 00:02:50,000
maybe how long it took and then if you

84
00:02:48,959 --> 00:02:52,400
aggregate those over

85
00:02:50,000 --> 00:02:54,160
time for different requests you can

86
00:02:52,400 --> 00:02:57,440
maybe learn a lot about the metrics

87
00:02:54,160 --> 00:02:58,959
um at a period of time

88
00:02:57,440 --> 00:03:00,319
and then if you also have logs then

89
00:02:58,959 --> 00:03:01,680
maybe you can learn something from an

90
00:03:00,319 --> 00:03:04,879
auditing perspective

91
00:03:01,680 --> 00:03:07,519
like you know bacon jam detected

92
00:03:04,879 --> 00:03:07,518
something like that

93
00:03:08,080 --> 00:03:10,800
and each aspect tells kind of a

94
00:03:09,360 --> 00:03:13,760
different story about your about what

95
00:03:10,800 --> 00:03:16,159
your system's doing inside

96
00:03:13,760 --> 00:03:18,480
so traces tell a story they tell a story

97
00:03:16,159 --> 00:03:20,319
about particular requests

98
00:03:18,480 --> 00:03:22,480
they're usually categorized with

99
00:03:20,319 --> 00:03:24,798
metadata

100
00:03:22,480 --> 00:03:26,000
tag metadata and also data about what

101
00:03:24,799 --> 00:03:28,080
happened in that request

102
00:03:26,000 --> 00:03:29,200
up to the user they're usually

103
00:03:28,080 --> 00:03:30,560
distributed over

104
00:03:29,200 --> 00:03:31,679
all of your different services so you

105
00:03:30,560 --> 00:03:33,440
can see what happened in a micro

106
00:03:31,680 --> 00:03:34,879
services architecture or monolithic

107
00:03:33,440 --> 00:03:36,799
architecture

108
00:03:34,879 --> 00:03:38,720
or maybe even across different vendor

109
00:03:36,799 --> 00:03:40,239
middlewares

110
00:03:38,720 --> 00:03:41,760
they're usually sampled because your

111
00:03:40,239 --> 00:03:42,879
vendor usually charges you based on how

112
00:03:41,760 --> 00:03:44,159
many you're sending so you want to

113
00:03:42,879 --> 00:03:46,560
sample those down to some

114
00:03:44,159 --> 00:03:48,000
useful percentage not like millions of

115
00:03:46,560 --> 00:03:49,440
traces per hour

116
00:03:48,000 --> 00:03:51,760
because you'll never look at all those

117
00:03:49,440 --> 00:03:51,760
anyway

118
00:03:52,239 --> 00:03:55,280
metrics tell you a different story so

119
00:03:54,080 --> 00:03:56,000
they're usually about types of

120
00:03:55,280 --> 00:03:57,760
operations

121
00:03:56,000 --> 00:03:59,439
like how many of something happened in a

122
00:03:57,760 --> 00:04:02,560
certain given period of time

123
00:03:59,439 --> 00:04:04,319
whether they were successful or errors

124
00:04:02,560 --> 00:04:05,840
and they're usually aggregated by a

125
00:04:04,319 --> 00:04:09,439
period of time and some

126
00:04:05,840 --> 00:04:11,519
some tag about the metric

127
00:04:09,439 --> 00:04:12,480
logs tell yet another story they're

128
00:04:11,519 --> 00:04:14,560
usually

129
00:04:12,480 --> 00:04:15,679
in very fine detail exactly something

130
00:04:14,560 --> 00:04:18,079
that happened

131
00:04:15,680 --> 00:04:19,120
at a at a particular point in time and

132
00:04:18,079 --> 00:04:20,639
hopefully if you're lucky they're

133
00:04:19,120 --> 00:04:21,120
structured so that you can search them

134
00:04:20,639 --> 00:04:24,960
and

135
00:04:21,120 --> 00:04:26,320
aggregate over different indices

136
00:04:24,960 --> 00:04:28,080
and usually they're collected from all

137
00:04:26,320 --> 00:04:30,719
your services into some central

138
00:04:28,080 --> 00:04:33,758
you know usually vendor hosted platform

139
00:04:30,720 --> 00:04:33,759
and you pay a lot of money for that

140
00:04:34,160 --> 00:04:37,840
yeah so i want to talk a little bit

141
00:04:36,080 --> 00:04:39,599
about distributed tracing theory

142
00:04:37,840 --> 00:04:41,440
because that seems like when i'm talking

143
00:04:39,600 --> 00:04:42,400
to developers that seems like the thing

144
00:04:41,440 --> 00:04:45,759
that people are less

145
00:04:42,400 --> 00:04:46,080
familiar with already and it all starts

146
00:04:45,759 --> 00:04:48,160
with

147
00:04:46,080 --> 00:04:50,080
the core concept of distributed tracing

148
00:04:48,160 --> 00:04:52,160
as a span

149
00:04:50,080 --> 00:04:54,080
a span has a name and it has a start

150
00:04:52,160 --> 00:04:55,440
time and an end time

151
00:04:54,080 --> 00:04:57,520
and then also it can have various

152
00:04:55,440 --> 00:04:58,719
attributes

153
00:04:57,520 --> 00:05:01,039
where there are some semantic

154
00:04:58,720 --> 00:05:02,639
conventions for example for http client

155
00:05:01,039 --> 00:05:03,919
server interactions and databases and

156
00:05:02,639 --> 00:05:05,360
things like that

157
00:05:03,919 --> 00:05:07,599
but in general it can be anything you

158
00:05:05,360 --> 00:05:07,600
want

159
00:05:07,680 --> 00:05:10,800
each span knows about what its parent

160
00:05:09,520 --> 00:05:12,639
span is so that you can have this

161
00:05:10,800 --> 00:05:16,000
hierarchy of traces

162
00:05:12,639 --> 00:05:17,199
and you can draw this waterfall graph or

163
00:05:16,000 --> 00:05:20,479
flame chart

164
00:05:17,199 --> 00:05:21,919
depending on what you want to call it

165
00:05:20,479 --> 00:05:23,840
and then all of those spans know that

166
00:05:21,919 --> 00:05:25,758
they belong to the same trace so there's

167
00:05:23,840 --> 00:05:27,679
a kind of a trace id and a parent span

168
00:05:25,759 --> 00:05:29,600
id

169
00:05:27,680 --> 00:05:31,759
the top level span in the trace doesn't

170
00:05:29,600 --> 00:05:33,280
have a parent so that's the root span

171
00:05:31,759 --> 00:05:34,960
and that's kind of how the collector

172
00:05:33,280 --> 00:05:38,400
knows that this is

173
00:05:34,960 --> 00:05:40,479
the top level of the trace

174
00:05:38,400 --> 00:05:42,159
we can also model in these waterfall

175
00:05:40,479 --> 00:05:45,120
charts parallelism

176
00:05:42,160 --> 00:05:46,160
so in this case we have two http client

177
00:05:45,120 --> 00:05:48,800
gets

178
00:05:46,160 --> 00:05:49,360
happening in parallel on the left there

179
00:05:48,800 --> 00:05:50,800
and then

180
00:05:49,360 --> 00:05:52,320
the results from both of those things

181
00:05:50,800 --> 00:05:53,360
are aggregated into this handle

182
00:05:52,320 --> 00:05:55,199
responses

183
00:05:53,360 --> 00:05:56,880
section of the processing and you might

184
00:05:55,199 --> 00:05:57,520
do some kind of a database call in there

185
00:05:56,880 --> 00:06:00,800
etc

186
00:05:57,520 --> 00:06:01,758
something else where distributed tracing

187
00:06:00,800 --> 00:06:04,160
gets really interesting

188
00:06:01,759 --> 00:06:05,759
is that inside of each of those parallel

189
00:06:04,160 --> 00:06:06,880
requests we can see what the downstream

190
00:06:05,759 --> 00:06:09,520
surface was doing

191
00:06:06,880 --> 00:06:10,240
so in this case we have uh the top level

192
00:06:09,520 --> 00:06:12,240
trace

193
00:06:10,240 --> 00:06:13,840
is from a service that's colored yellow

194
00:06:12,240 --> 00:06:15,600
and then we're making calls down to a

195
00:06:13,840 --> 00:06:17,440
green service that's written in

196
00:06:15,600 --> 00:06:18,880
elixir in phoenix and a red surface

197
00:06:17,440 --> 00:06:21,039
that's written in ruby and rails

198
00:06:18,880 --> 00:06:22,080
so you can kind of see what each of them

199
00:06:21,039 --> 00:06:25,599
are doing inside even though they're

200
00:06:22,080 --> 00:06:25,599
different technologies different servers

201
00:06:25,840 --> 00:06:30,080
and then when we bring it all together

202
00:06:27,280 --> 00:06:31,840
you can see overall the entire trace

203
00:06:30,080 --> 00:06:33,758
we had a phoenix server that was calling

204
00:06:31,840 --> 00:06:35,520
into both of these things

205
00:06:33,759 --> 00:06:37,840
and they're color coded by where the

206
00:06:35,520 --> 00:06:40,639
work happened

207
00:06:37,840 --> 00:06:41,840
so the way that works is that the the

208
00:06:40,639 --> 00:06:43,680
top level service

209
00:06:41,840 --> 00:06:45,758
sends a span context to the downstream

210
00:06:43,680 --> 00:06:47,199
services

211
00:06:45,759 --> 00:06:48,960
and that can tell the downstream service

212
00:06:47,199 --> 00:06:49,440
whether this trace is being sampled or

213
00:06:48,960 --> 00:06:52,880
not

214
00:06:49,440 --> 00:06:52,880
and then some other state metadata

215
00:06:53,840 --> 00:06:58,638
this used to be a vendor specific free

216
00:06:56,400 --> 00:07:01,520
for all of incompatible headers

217
00:06:58,639 --> 00:07:02,720
which was awesome and now there is a w3c

218
00:07:01,520 --> 00:07:04,560
standard

219
00:07:02,720 --> 00:07:07,520
called context propagation which is used

220
00:07:04,560 --> 00:07:07,520
by open telemetry

221
00:07:07,840 --> 00:07:11,359
one of the really neat things about

222
00:07:08,960 --> 00:07:12,960
distributed tracing is that the upstream

223
00:07:11,360 --> 00:07:14,800
service doesn't really need to care what

224
00:07:12,960 --> 00:07:16,960
the downstream service is doing

225
00:07:14,800 --> 00:07:18,639
it just needs to send that trace context

226
00:07:16,960 --> 00:07:20,159
or spam context

227
00:07:18,639 --> 00:07:21,199
and similarly the downstream services

228
00:07:20,160 --> 00:07:22,479
don't need to care about what the

229
00:07:21,199 --> 00:07:23,759
upstream service is doing

230
00:07:22,479 --> 00:07:26,080
they don't have to return all of their

231
00:07:23,759 --> 00:07:29,360
spam data back or anything like that

232
00:07:26,080 --> 00:07:31,198
they just need to pass their piece of

233
00:07:29,360 --> 00:07:34,080
the trace up to a central collector

234
00:07:31,199 --> 00:07:35,840
and then the collector knows that this

235
00:07:34,080 --> 00:07:37,199
this span is a subspan of that span so

236
00:07:35,840 --> 00:07:38,159
it can stitch everything back together

237
00:07:37,199 --> 00:07:41,280
into a

238
00:07:38,160 --> 00:07:41,280
cohesive trace

239
00:07:41,440 --> 00:07:44,719
another job of the collector is to

240
00:07:42,880 --> 00:07:46,879
decide which span which traces it's

241
00:07:44,720 --> 00:07:48,560
going to keep

242
00:07:46,879 --> 00:07:50,400
we can do that pretty simply with

243
00:07:48,560 --> 00:07:52,960
probabilistic head sampling

244
00:07:50,400 --> 00:07:55,198
so the idea there is that for each trace

245
00:07:52,960 --> 00:07:56,719
that started at the very beginning

246
00:07:55,199 --> 00:07:58,720
that is it's not part of it's not a

247
00:07:56,720 --> 00:08:01,440
child of an existing trace

248
00:07:58,720 --> 00:08:03,360
um you can decide to just flip a coin or

249
00:08:01,440 --> 00:08:04,960
keep like 0.1 percent of traces or

250
00:08:03,360 --> 00:08:06,720
whatever you want to do

251
00:08:04,960 --> 00:08:08,000
you'll notice the top one there we

252
00:08:06,720 --> 00:08:08,879
decided early on that we weren't going

253
00:08:08,000 --> 00:08:10,800
to sample it

254
00:08:08,879 --> 00:08:12,000
so we only have sort of a skeleton trace

255
00:08:10,800 --> 00:08:13,520
of each transition

256
00:08:12,000 --> 00:08:14,879
between services there we don't have to

257
00:08:13,520 --> 00:08:15,840
keep quite as much information because

258
00:08:14,879 --> 00:08:17,280
we know we're going to throw it away

259
00:08:15,840 --> 00:08:18,479
anyway

260
00:08:17,280 --> 00:08:19,840
and then for the other ones at the

261
00:08:18,479 --> 00:08:20,878
beginning we said we're going to keep

262
00:08:19,840 --> 00:08:22,400
both of these

263
00:08:20,879 --> 00:08:25,919
but then it's up to the collector to

264
00:08:22,400 --> 00:08:27,679
decide how many to actually keep

265
00:08:25,919 --> 00:08:29,440
based on your rate limiting or how much

266
00:08:27,680 --> 00:08:32,399
you want to pay your vendor

267
00:08:29,440 --> 00:08:34,320
you know whatever you want to do there's

268
00:08:32,399 --> 00:08:34,880
also such a thing as tail sampling so in

269
00:08:34,320 --> 00:08:36,399
this case

270
00:08:34,880 --> 00:08:37,679
we can decide that we only want to keep

271
00:08:36,399 --> 00:08:39,279
the ones that have an error thrown

272
00:08:37,679 --> 00:08:42,399
during the process

273
00:08:39,279 --> 00:08:42,399
but it could be really anything

274
00:08:42,559 --> 00:08:45,920
so i want to talk really briefly about

275
00:08:44,000 --> 00:08:48,320
some built-in beam tracing

276
00:08:45,920 --> 00:08:49,599
functionality that exists in the beam i

277
00:08:48,320 --> 00:08:51,200
want to note that this is not used for

278
00:08:49,600 --> 00:08:53,040
distributed tracing and i'll talk about

279
00:08:51,200 --> 00:08:55,760
why in a second

280
00:08:53,040 --> 00:08:57,279
but this is uh erlang's trace pattern uh

281
00:08:55,760 --> 00:08:58,880
functionality

282
00:08:57,279 --> 00:09:00,880
and then there's also a library called

283
00:08:58,880 --> 00:09:02,160
recon trace that's really useful for

284
00:09:00,880 --> 00:09:05,279
interacting with that in

285
00:09:02,160 --> 00:09:07,680
a more ergonomic way

286
00:09:05,279 --> 00:09:10,080
so the way that works in general um this

287
00:09:07,680 --> 00:09:12,239
is more like a sketch of how it works

288
00:09:10,080 --> 00:09:13,600
because it's kind of beam internals but

289
00:09:12,240 --> 00:09:14,800
essentially you tell the beam

290
00:09:13,600 --> 00:09:16,480
these are some trace patterns that i

291
00:09:14,800 --> 00:09:18,560
want to watch for so anytime a function

292
00:09:16,480 --> 00:09:20,080
gets called that matches these things

293
00:09:18,560 --> 00:09:22,880
i want you to set up a tracer process

294
00:09:20,080 --> 00:09:24,399
that will tell me that that happened

295
00:09:22,880 --> 00:09:26,080
there's there's a star on the tracer

296
00:09:24,399 --> 00:09:28,080
process here because there can only be

297
00:09:26,080 --> 00:09:29,760
one of those per beam

298
00:09:28,080 --> 00:09:32,480
but then anytime your application

299
00:09:29,760 --> 00:09:34,480
processes make a matching function call

300
00:09:32,480 --> 00:09:36,160
they will sort of magically send a

301
00:09:34,480 --> 00:09:37,920
message to that tracer process and then

302
00:09:36,160 --> 00:09:39,199
it will send your interactive shell

303
00:09:37,920 --> 00:09:42,640
process

304
00:09:39,200 --> 00:09:44,240
a message about those so some really

305
00:09:42,640 --> 00:09:45,760
nice features about this is that it's

306
00:09:44,240 --> 00:09:47,600
it's production safe as well as

307
00:09:45,760 --> 00:09:49,200
development

308
00:09:47,600 --> 00:09:50,880
it's very useful for interactive

309
00:09:49,200 --> 00:09:52,320
troubleshooting and debugging of your

310
00:09:50,880 --> 00:09:54,480
system

311
00:09:52,320 --> 00:09:56,880
some downsides you can only have one per

312
00:09:54,480 --> 00:09:59,360
beam and it's only for local

313
00:09:56,880 --> 00:10:02,000
tracing so you can't trace across

314
00:09:59,360 --> 00:10:02,000
distribution

315
00:10:02,959 --> 00:10:06,560
so the reason the reasons we wouldn't

316
00:10:04,640 --> 00:10:08,079
use that for distributed tracing

317
00:10:06,560 --> 00:10:09,279
is not not because of the local only

318
00:10:08,079 --> 00:10:09,839
part because we could run it on all the

319
00:10:09,279 --> 00:10:11,279
nodes

320
00:10:09,839 --> 00:10:12,720
it's mainly because there's only one per

321
00:10:11,279 --> 00:10:13,279
beam and it's designed for interactive

322
00:10:12,720 --> 00:10:15,120
use

323
00:10:13,279 --> 00:10:16,800
so if you're using that resource in like

324
00:10:15,120 --> 00:10:20,160
a tracing library then people can't use

325
00:10:16,800 --> 00:10:20,160
it for interactive troubleshooting

326
00:10:20,720 --> 00:10:23,920
okay so i want to talk a little bit

327
00:10:21,920 --> 00:10:26,079
about observability super

328
00:10:23,920 --> 00:10:27,680
superpowers that you get by having this

329
00:10:26,079 --> 00:10:29,199
distributed tracing functionality in

330
00:10:27,680 --> 00:10:30,239
addition to your kind of standard logs

331
00:10:29,200 --> 00:10:32,880
and metrics that people are more

332
00:10:30,240 --> 00:10:34,720
familiar with

333
00:10:32,880 --> 00:10:38,079
so the first one of those is the single

334
00:10:34,720 --> 00:10:40,640
request flame graph idea

335
00:10:38,079 --> 00:10:42,160
so here's an example from datadog apm

336
00:10:40,640 --> 00:10:45,120
this is from an example project that i

337
00:10:42,160 --> 00:10:47,199
put together for the spandex library

338
00:10:45,120 --> 00:10:48,160
but in this in this trace you can see a

339
00:10:47,200 --> 00:10:50,000
summary

340
00:10:48,160 --> 00:10:51,680
of um three different services being

341
00:10:50,000 --> 00:10:54,800
involved the first being

342
00:10:51,680 --> 00:10:56,560
a uh a plug-based gateway um so this is

343
00:10:54,800 --> 00:10:59,760
a cowboy and plug-based server

344
00:10:56,560 --> 00:11:00,880
and then it calls into a elixir phoenix

345
00:10:59,760 --> 00:11:02,160
back-end service

346
00:11:00,880 --> 00:11:04,240
and then the blue at the bottom is

347
00:11:02,160 --> 00:11:05,760
making a database call so in this case

348
00:11:04,240 --> 00:11:06,560
we've modeled the database as a separate

349
00:11:05,760 --> 00:11:08,319
service

350
00:11:06,560 --> 00:11:09,359
since kind of in a distributed system

351
00:11:08,320 --> 00:11:10,560
sense it's different than the

352
00:11:09,360 --> 00:11:13,760
application server

353
00:11:10,560 --> 00:11:15,199
and it lets you see a glance that

354
00:11:13,760 --> 00:11:18,240
percent of this time was spent in the

355
00:11:15,200 --> 00:11:18,240
database layer

356
00:11:20,480 --> 00:11:23,920
so kind of related to that being able to

357
00:11:23,440 --> 00:11:26,000
see

358
00:11:23,920 --> 00:11:28,160
what happened in a particular request we

359
00:11:26,000 --> 00:11:29,920
get stack traces in context

360
00:11:28,160 --> 00:11:31,439
so this is an example from an actual

361
00:11:29,920 --> 00:11:32,560
production system where there was an

362
00:11:31,440 --> 00:11:34,000
exception here

363
00:11:32,560 --> 00:11:36,719
you can barely see it but right here

364
00:11:34,000 --> 00:11:38,880
there's a red box around that

365
00:11:36,720 --> 00:11:40,079
that part of the trace but you can see

366
00:11:38,880 --> 00:11:42,480
at the top level up there

367
00:11:40,079 --> 00:11:44,319
there's a 200 response being returned so

368
00:11:42,480 --> 00:11:45,200
we threw an exception but we handled it

369
00:11:44,320 --> 00:11:47,680
and

370
00:11:45,200 --> 00:11:49,360
we returned to 200. so in that case if

371
00:11:47,680 --> 00:11:50,560
you were just looking at the exceptions

372
00:11:49,360 --> 00:11:51,680
in your logs you might see

373
00:11:50,560 --> 00:11:52,719
you know a whole bunch of crashes or

374
00:11:51,680 --> 00:11:53,839
something and you might be worried that

375
00:11:52,720 --> 00:11:55,760
there's a big problem

376
00:11:53,839 --> 00:11:57,519
but actually we've recovered and it

377
00:11:55,760 --> 00:11:58,800
wasn't a big deal

378
00:11:57,519 --> 00:12:00,800
so that gives you a lot of context

379
00:11:58,800 --> 00:12:01,920
around how you got to that the point of

380
00:12:00,800 --> 00:12:03,839
that error

381
00:12:01,920 --> 00:12:06,160
and also whether it was a big problem

382
00:12:03,839 --> 00:12:08,480
for you at the top level

383
00:12:06,160 --> 00:12:09,279
so if we click into that error span

384
00:12:08,480 --> 00:12:10,720
there

385
00:12:09,279 --> 00:12:12,800
you can see some more information about

386
00:12:10,720 --> 00:12:14,480
what the what the error was and i

387
00:12:12,800 --> 00:12:16,079
uh redacted out the stack trace because

388
00:12:14,480 --> 00:12:17,760
it's from a real system but you would

389
00:12:16,079 --> 00:12:19,199
see a stack trace there as well

390
00:12:17,760 --> 00:12:20,560
so that gives you a lot of context

391
00:12:19,200 --> 00:12:22,000
around clicking through each of those

392
00:12:20,560 --> 00:12:23,279
bands and saying how did i end up at

393
00:12:22,000 --> 00:12:25,120
this point

394
00:12:23,279 --> 00:12:27,439
what life choices have led me to this

395
00:12:25,120 --> 00:12:27,440
place

396
00:12:28,720 --> 00:12:32,079
so another great thing that you can see

397
00:12:30,560 --> 00:12:33,518
really quickly when you deploy

398
00:12:32,079 --> 00:12:35,920
distributed tracing is

399
00:12:33,519 --> 00:12:37,600
n plus one queries because we all have

400
00:12:35,920 --> 00:12:39,040
these things

401
00:12:37,600 --> 00:12:41,120
these ones are pretend because i made

402
00:12:39,040 --> 00:12:44,399
them in my example app i would never

403
00:12:41,120 --> 00:12:45,839
write an n plus one query in production

404
00:12:44,399 --> 00:12:47,120
but down at the bottom there you can see

405
00:12:45,839 --> 00:12:47,760
that there are quite a few database

406
00:12:47,120 --> 00:12:50,160
calls

407
00:12:47,760 --> 00:12:51,200
probably like 1500 database calls being

408
00:12:50,160 --> 00:12:52,639
made

409
00:12:51,200 --> 00:12:53,920
and if you zoom in there you can see

410
00:12:52,639 --> 00:12:54,880
that there's one being made from the

411
00:12:53,920 --> 00:12:57,040
controller

412
00:12:54,880 --> 00:13:00,320
and then from the view there's uh you

413
00:12:57,040 --> 00:13:02,079
know 1400 999

414
00:13:00,320 --> 00:13:03,519
so you can see that there's probably an

415
00:13:02,079 --> 00:13:05,120
n plus one situation going on there and

416
00:13:03,519 --> 00:13:06,480
also queries from the view

417
00:13:05,120 --> 00:13:07,920
so these are kind of like structural

418
00:13:06,480 --> 00:13:09,120
things that when you deploy distributed

419
00:13:07,920 --> 00:13:10,800
tracing even if you didn't write the

420
00:13:09,120 --> 00:13:13,040
code you can say that doesn't quite look

421
00:13:10,800 --> 00:13:13,040
right

422
00:13:13,680 --> 00:13:16,399
another really great thing you can see

423
00:13:14,959 --> 00:13:19,279
kind of visually and structurally is

424
00:13:16,399 --> 00:13:21,440
calls to the same service downstream

425
00:13:19,279 --> 00:13:22,639
so this is another another real trace

426
00:13:21,440 --> 00:13:23,680
where

427
00:13:22,639 --> 00:13:25,519
you can see that there's a bunch of

428
00:13:23,680 --> 00:13:26,880
things happening in parallel here but

429
00:13:25,519 --> 00:13:27,839
you can also see that three of these are

430
00:13:26,880 --> 00:13:30,560
going to the same

431
00:13:27,839 --> 00:13:32,480
service based on the color the light

432
00:13:30,560 --> 00:13:34,638
blue surface there

433
00:13:32,480 --> 00:13:36,160
so there might be a chance that it would

434
00:13:34,639 --> 00:13:37,040
be more efficient to batch those up into

435
00:13:36,160 --> 00:13:38,319
one call

436
00:13:37,040 --> 00:13:41,599
and get all the data back at once

437
00:13:38,320 --> 00:13:42,959
instead of making three separate calls

438
00:13:41,600 --> 00:13:44,480
and kind of the reverse of that

439
00:13:42,959 --> 00:13:46,399
sometimes you can actually get lower

440
00:13:44,480 --> 00:13:48,000
latency by doing things in parallel

441
00:13:46,399 --> 00:13:49,920
instead of matching them up

442
00:13:48,000 --> 00:13:51,440
so an example here is if you if you need

443
00:13:49,920 --> 00:13:53,040
to make a call to a downstream service

444
00:13:51,440 --> 00:13:53,680
and then use the results from that

445
00:13:53,040 --> 00:13:55,760
payload

446
00:13:53,680 --> 00:13:57,040
to make a different downstream call you

447
00:13:55,760 --> 00:13:58,560
might be able to

448
00:13:57,040 --> 00:14:00,240
reduce your overall latency by just

449
00:13:58,560 --> 00:14:01,359
doing a quick request to get the ids

450
00:14:00,240 --> 00:14:02,880
from that first call

451
00:14:01,360 --> 00:14:04,079
and then make the original requests and

452
00:14:02,880 --> 00:14:06,800
then in parallel make the other

453
00:14:04,079 --> 00:14:06,800
downstream call

454
00:14:08,560 --> 00:14:11,920
so another really great thing you can

455
00:14:10,399 --> 00:14:13,440
get out of these graphs is

456
00:14:11,920 --> 00:14:15,199
a feel for how much network latency

457
00:14:13,440 --> 00:14:16,800
you're dealing with

458
00:14:15,199 --> 00:14:18,639
when you look at this trace you can see

459
00:14:16,800 --> 00:14:20,560
that the the sort of teal there is an

460
00:14:18,639 --> 00:14:22,000
external request.get so this is like a

461
00:14:20,560 --> 00:14:23,199
client module that we've written

462
00:14:22,000 --> 00:14:24,560
and then you can see the downstream

463
00:14:23,199 --> 00:14:25,359
processing from the service that's

464
00:14:24,560 --> 00:14:27,518
getting called

465
00:14:25,360 --> 00:14:28,880
and there's a there's the these gaps in

466
00:14:27,519 --> 00:14:30,399
time here where you can't really account

467
00:14:28,880 --> 00:14:31,920
for what happened between

468
00:14:30,399 --> 00:14:33,920
making the call and the downstream

469
00:14:31,920 --> 00:14:35,519
service receiving the call

470
00:14:33,920 --> 00:14:36,719
so there's obviously a lot of other

471
00:14:35,519 --> 00:14:37,920
things at play here this isn't all

472
00:14:36,720 --> 00:14:39,920
network latency

473
00:14:37,920 --> 00:14:41,040
but if you had a lot of network latency

474
00:14:39,920 --> 00:14:42,399
then that's where it would show up and

475
00:14:41,040 --> 00:14:45,519
it would it would be

476
00:14:42,399 --> 00:14:46,560
something you could look out for i think

477
00:14:45,519 --> 00:14:47,839
the most important

478
00:14:46,560 --> 00:14:49,518
uh superpower that you get from

479
00:14:47,839 --> 00:14:51,199
distributed tracing is this culture

480
00:14:49,519 --> 00:14:53,040
shift

481
00:14:51,199 --> 00:14:54,560
where now whenever we're troubleshooting

482
00:14:53,040 --> 00:14:55,360
a problem someone will link to a trace

483
00:14:54,560 --> 00:14:57,119
in apm

484
00:14:55,360 --> 00:14:58,399
and there's all this context that comes

485
00:14:57,120 --> 00:14:59,680
along with that like how does the code

486
00:14:58,399 --> 00:15:03,040
work what's going wrong

487
00:14:59,680 --> 00:15:04,719
what did i expect what did i not see

488
00:15:03,040 --> 00:15:06,079
all these things kind of come with a

489
00:15:04,720 --> 00:15:07,199
really simple link into a tool that

490
00:15:06,079 --> 00:15:08,880
gives you all that information all at

491
00:15:07,199 --> 00:15:10,800
once

492
00:15:08,880 --> 00:15:12,000
it's really changed the way i work at a

493
00:15:10,800 --> 00:15:13,359
company called bleacher report it's

494
00:15:12,000 --> 00:15:16,560
really changed the way that we

495
00:15:13,360 --> 00:15:16,560
troubleshoot things in production

496
00:15:17,440 --> 00:15:21,600
um so with all the superpowers obviously

497
00:15:19,120 --> 00:15:22,560
there comes some some pitfalls

498
00:15:21,600 --> 00:15:24,000
some things that you'll probably have to

499
00:15:22,560 --> 00:15:25,439
figure out as you're deploying this this

500
00:15:24,000 --> 00:15:27,680
technology

501
00:15:25,440 --> 00:15:29,199
the first of which being sampling so if

502
00:15:27,680 --> 00:15:30,719
you're if you're

503
00:15:29,199 --> 00:15:32,639
not using sampling then you're probably

504
00:15:30,720 --> 00:15:34,480
going to send a lot of spans to your

505
00:15:32,639 --> 00:15:36,639
vendor or to your open source thing and

506
00:15:34,480 --> 00:15:38,079
either it will be your problem or will

507
00:15:36,639 --> 00:15:39,839
be someone else's problem when the bill

508
00:15:38,079 --> 00:15:40,959
comes

509
00:15:39,839 --> 00:15:44,399
so you should think about how many

510
00:15:40,959 --> 00:15:46,000
traces you actually need to have

511
00:15:44,399 --> 00:15:47,680
incomplete traces happen when you don't

512
00:15:46,000 --> 00:15:48,399
have tracing implemented on one of your

513
00:15:47,680 --> 00:15:49,758
services

514
00:15:48,399 --> 00:15:51,680
and that means any downstream calls

515
00:15:49,759 --> 00:15:53,120
don't get attached to the upstream calls

516
00:15:51,680 --> 00:15:54,560
so the the distributed part of

517
00:15:53,120 --> 00:15:55,519
distributed tracing doesn't happen and

518
00:15:54,560 --> 00:15:58,479
then

519
00:15:55,519 --> 00:16:00,079
it's not as useful another thing to be

520
00:15:58,480 --> 00:16:02,000
aware of is clock skew

521
00:16:00,079 --> 00:16:03,519
so in this case the downstream service

522
00:16:02,000 --> 00:16:04,079
anticipated that we were going to call

523
00:16:03,519 --> 00:16:05,839
it

524
00:16:04,079 --> 00:16:07,040
um and then it started working on the

525
00:16:05,839 --> 00:16:08,720
request ahead of time to save us a

526
00:16:07,040 --> 00:16:11,519
little bit of time

527
00:16:08,720 --> 00:16:13,040
um yeah so that didn't really happen uh

528
00:16:11,519 --> 00:16:14,480
the idea here is that the servers don't

529
00:16:13,040 --> 00:16:16,079
have the same clock right even if you're

530
00:16:14,480 --> 00:16:17,279
using ntp the clocks are not going to be

531
00:16:16,079 --> 00:16:18,880
exactly in sync

532
00:16:17,279 --> 00:16:20,160
all the time there's not a whole lot you

533
00:16:18,880 --> 00:16:23,199
can do about it other than just know

534
00:16:20,160 --> 00:16:26,079
that it's a thing

535
00:16:23,199 --> 00:16:28,319
so real quickly open telemetry and the

536
00:16:26,079 --> 00:16:31,680
history of open census and open tracing

537
00:16:28,320 --> 00:16:34,959
so open tracing was a cncf

538
00:16:31,680 --> 00:16:36,000
project and this is from their website

539
00:16:34,959 --> 00:16:38,399
so it's not a thing that you can

540
00:16:36,000 --> 00:16:40,079
download and it's not really a standard

541
00:16:38,399 --> 00:16:41,519
it's more of an api spec and it's

542
00:16:40,079 --> 00:16:43,040
various implementations

543
00:16:41,519 --> 00:16:44,480
so basically like people knew that this

544
00:16:43,040 --> 00:16:45,920
was a thing that they should build they

545
00:16:44,480 --> 00:16:47,680
built some things

546
00:16:45,920 --> 00:16:50,560
there's like some some guidelines and

547
00:16:47,680 --> 00:16:53,040
pirate code around how it should work

548
00:16:50,560 --> 00:16:54,560
this is where the spandex library falls

549
00:16:53,040 --> 00:16:56,639
and there's also

550
00:16:54,560 --> 00:16:58,160
an otter library in erlang and x-ray

551
00:16:56,639 --> 00:17:02,079
wraps around otter and

552
00:16:58,160 --> 00:17:04,399
elixir really interesting the

553
00:17:02,079 --> 00:17:05,520
open tracing standard is supported by

554
00:17:04,400 --> 00:17:07,679
nginx so your

555
00:17:05,520 --> 00:17:09,599
your proxy can actually participate in

556
00:17:07,679 --> 00:17:11,280
your spans which is pretty awesome

557
00:17:09,599 --> 00:17:13,760
i kind of wish that more vin vendor

558
00:17:11,280 --> 00:17:15,678
middlewares did that kind of thing

559
00:17:13,760 --> 00:17:18,160
there's also open census so this is a

560
00:17:15,679 --> 00:17:19,760
competing standard with open tracing

561
00:17:18,160 --> 00:17:22,000
it is a single set of libraries that you

562
00:17:19,760 --> 00:17:22,720
can download and it also handles metrics

563
00:17:22,000 --> 00:17:24,720
and

564
00:17:22,720 --> 00:17:26,799
traces and they were planning on having

565
00:17:24,720 --> 00:17:28,079
logs in the future

566
00:17:26,799 --> 00:17:29,918
open census has a whole bunch of

567
00:17:28,079 --> 00:17:32,720
different deployment options

568
00:17:29,919 --> 00:17:34,080
so you can have nothing in your app or

569
00:17:32,720 --> 00:17:35,600
you can have something in your app that

570
00:17:34,080 --> 00:17:37,360
talks to an external collector or you

571
00:17:35,600 --> 00:17:38,719
can deploy it as an agent in your app or

572
00:17:37,360 --> 00:17:41,360
a sidecar container

573
00:17:38,720 --> 00:17:43,039
whatever you want to do and then those

574
00:17:41,360 --> 00:17:44,399
open senses collectors support

575
00:17:43,039 --> 00:17:46,400
the tail based sampling thing that i was

576
00:17:44,400 --> 00:17:48,000
talking about you can also chain them

577
00:17:46,400 --> 00:17:48,960
and output to multiple different vendors

578
00:17:48,000 --> 00:17:51,840
or open source

579
00:17:48,960 --> 00:17:53,120
internal platforms but all that just to

580
00:17:51,840 --> 00:17:54,879
say that these two projects

581
00:17:53,120 --> 00:17:58,559
merged into a new project called open

582
00:17:54,880 --> 00:18:00,320
telemetry which is also a cncf project

583
00:17:58,559 --> 00:18:02,000
so yeah that's the success story of now

584
00:18:00,320 --> 00:18:03,600
there's less standards than there were

585
00:18:02,000 --> 00:18:06,799
to begin with even though

586
00:18:03,600 --> 00:18:09,120
temporarily there's more standards um

587
00:18:06,799 --> 00:18:10,799
so from the open telemetry website this

588
00:18:09,120 --> 00:18:12,479
is again a single set of libraries and

589
00:18:10,799 --> 00:18:14,160
tools that you can download and use

590
00:18:12,480 --> 00:18:16,000
there's official clients for each light

591
00:18:14,160 --> 00:18:18,400
for each language

592
00:18:16,000 --> 00:18:20,000
it's currently supporting metrics and

593
00:18:18,400 --> 00:18:22,640
traces and logs in the future just like

594
00:18:20,000 --> 00:18:22,640
open census

595
00:18:23,120 --> 00:18:27,120
on their website they have sort of a

596
00:18:24,480 --> 00:18:28,240
status status tracker that says

597
00:18:27,120 --> 00:18:30,559
you know how far along each of the

598
00:18:28,240 --> 00:18:32,160
implementations are um

599
00:18:30,559 --> 00:18:34,320
it's not up to date so the erlang one

600
00:18:32,160 --> 00:18:36,080
actually should be at 0.2

601
00:18:34,320 --> 00:18:37,678
at this point but the more exciting

602
00:18:36,080 --> 00:18:39,678
thing is that erlang's on there at all

603
00:18:37,679 --> 00:18:41,120
because it seems like erlang usually is

604
00:18:39,679 --> 00:18:42,960
not listed on official support for

605
00:18:41,120 --> 00:18:46,399
anything

606
00:18:42,960 --> 00:18:48,400
so i'm excited that it's there i want to

607
00:18:46,400 --> 00:18:51,039
briefly mention that i contribute to the

608
00:18:48,400 --> 00:18:52,799
spandex project along with zach daniel

609
00:18:51,039 --> 00:18:54,720
we have easy integrations with phoenix

610
00:18:52,799 --> 00:18:58,080
plug and ecto so that's kind of

611
00:18:54,720 --> 00:19:00,240
where my background comes from here

612
00:18:58,080 --> 00:19:01,760
it implements the open tracing standard

613
00:19:00,240 --> 00:19:03,120
but it only supports elixir really

614
00:19:01,760 --> 00:19:04,799
because it uses a lot of macros and it

615
00:19:03,120 --> 00:19:06,639
only supports data dog apm

616
00:19:04,799 --> 00:19:08,240
so maybe people here have used it and

617
00:19:06,640 --> 00:19:10,320
enjoy that but

618
00:19:08,240 --> 00:19:11,679
it has some limitations to it and then

619
00:19:10,320 --> 00:19:14,000
open census as i mentioned

620
00:19:11,679 --> 00:19:14,880
uh obviously implements open census uh

621
00:19:14,000 --> 00:19:17,360
it's written in

622
00:19:14,880 --> 00:19:18,559
erlang and elixir support and it has a

623
00:19:17,360 --> 00:19:20,080
bunch of back ends

624
00:19:18,559 --> 00:19:22,240
but basically these two projects are

625
00:19:20,080 --> 00:19:25,678
being merged just like the uh

626
00:19:22,240 --> 00:19:27,120
the industry projects and we have an

627
00:19:25,679 --> 00:19:30,640
open telemetry beam

628
00:19:27,120 --> 00:19:32,879
uh githu github.org and we're working on

629
00:19:30,640 --> 00:19:36,559
the official libraries in the opengl

630
00:19:32,880 --> 00:19:38,160
open telemetry org so

631
00:19:36,559 --> 00:19:40,639
i want to talk a little about about the

632
00:19:38,160 --> 00:19:43,600
erlang telemetry uh library

633
00:19:40,640 --> 00:19:44,880
because um on the observability working

634
00:19:43,600 --> 00:19:46,240
group we were talking about should we

635
00:19:44,880 --> 00:19:47,919
tell people that they should directly

636
00:19:46,240 --> 00:19:49,200
use this new open telemetry thing or

637
00:19:47,919 --> 00:19:51,440
should they use telometry

638
00:19:49,200 --> 00:19:52,960
or and and luckily we were at least able

639
00:19:51,440 --> 00:19:54,000
to steer people away from calling this

640
00:19:52,960 --> 00:19:56,799
new thing otp

641
00:19:54,000 --> 00:19:58,320
like open tracing platform because that

642
00:19:56,799 --> 00:20:01,440
would have been really bad

643
00:19:58,320 --> 00:20:03,918
um so but we've decided that we're

644
00:20:01,440 --> 00:20:05,520
hoping that this telemetry uh

645
00:20:03,919 --> 00:20:07,520
system that's currently for metrics and

646
00:20:05,520 --> 00:20:09,520
events we can also use for distributed

647
00:20:07,520 --> 00:20:10,879
tracing with a very simple api

648
00:20:09,520 --> 00:20:12,559
and then down the road maybe you'll need

649
00:20:10,880 --> 00:20:13,760
to directly use open telemetry but

650
00:20:12,559 --> 00:20:16,240
hopefully you can get by with just

651
00:20:13,760 --> 00:20:17,919
telemetry

652
00:20:16,240 --> 00:20:20,159
the main selling points for telemetry

653
00:20:17,919 --> 00:20:21,440
are that it's simple it's standard and

654
00:20:20,159 --> 00:20:26,320
it's pretty safe

655
00:20:21,440 --> 00:20:29,360
i mean it's as safe as a dependency gets

656
00:20:26,320 --> 00:20:31,520
so the way that it works is that in a

657
00:20:29,360 --> 00:20:33,600
library you would register

658
00:20:31,520 --> 00:20:35,360
or you would just kind of fire the event

659
00:20:33,600 --> 00:20:36,799
you don't need to register ahead of time

660
00:20:35,360 --> 00:20:40,240
and then in a receiver you can say i

661
00:20:36,799 --> 00:20:41,840
would like to receive events like that

662
00:20:40,240 --> 00:20:43,520
yeah so those get called synchronously

663
00:20:41,840 --> 00:20:44,720
which is useful for distributed tracing

664
00:20:43,520 --> 00:20:46,158
because you want to be able to catch

665
00:20:44,720 --> 00:20:47,919
those start and stop events when they

666
00:20:46,159 --> 00:20:49,840
happen

667
00:20:47,919 --> 00:20:51,200
you can also quickly get time series

668
00:20:49,840 --> 00:20:53,120
metrics out of things because

669
00:20:51,200 --> 00:20:55,360
they're just embedded straight in there

670
00:20:53,120 --> 00:20:58,719
so it's simple

671
00:20:55,360 --> 00:21:00,158
and the way that we keep it safe is that

672
00:20:58,720 --> 00:21:01,760
you register different handlers and if

673
00:21:00,159 --> 00:21:02,880
one of them crashes it gets removed from

674
00:21:01,760 --> 00:21:03,760
the table and it doesn't get called

675
00:21:02,880 --> 00:21:05,919
again

676
00:21:03,760 --> 00:21:08,080
so one strikeout you're out so it's

677
00:21:05,919 --> 00:21:09,520
relatively safe

678
00:21:08,080 --> 00:21:12,879
and it's pretty standard it's already

679
00:21:09,520 --> 00:21:16,000
included in plug phoenix and ecto

680
00:21:12,880 --> 00:21:17,440
so yep takeaways is if you're

681
00:21:16,000 --> 00:21:18,799
if you're a library author you should

682
00:21:17,440 --> 00:21:19,760
instrument it with telemetry if you're

683
00:21:18,799 --> 00:21:21,200
an app

684
00:21:19,760 --> 00:21:22,960
developer you should integrate with

685
00:21:21,200 --> 00:21:25,760
telemetry and you should learn more

686
00:21:22,960 --> 00:21:31,840
about open telemetry as it matures

687
00:21:25,760 --> 00:21:31,840
that's all i got

688
00:21:38,640 --> 00:21:40,720
you

