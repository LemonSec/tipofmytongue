1
00:00:06,560 --> 00:00:11,599
so

2
00:00:07,680 --> 00:00:15,040
hello everybody so we are

3
00:00:11,599 --> 00:00:16,800
ann lutz and bruno thomas we are coming

4
00:00:15,040 --> 00:00:19,599
from the icij

5
00:00:16,800 --> 00:00:21,760
it's the international consortium of

6
00:00:19,600 --> 00:00:26,160
investigative journalists

7
00:00:21,760 --> 00:00:29,519
so it's um it's the organization

8
00:00:26,160 --> 00:00:32,960
that released panama papers that

9
00:00:29,519 --> 00:00:36,320
we just saw before so it's

10
00:00:32,960 --> 00:00:40,399
more about journalism than science so

11
00:00:36,320 --> 00:00:44,160
thank you for admitting it admitting us

12
00:00:40,399 --> 00:00:47,360
and so we

13
00:00:44,160 --> 00:00:51,038
we are going to see um

14
00:00:47,360 --> 00:00:54,480
how we could use uh an index machine

15
00:00:51,039 --> 00:00:58,559
like elasticsearch or solar to help

16
00:00:54,480 --> 00:01:02,000
journalists to find um facts and stories

17
00:00:58,559 --> 00:01:05,518
among really big document corpuses

18
00:01:02,000 --> 00:01:08,560
or leagues we call them leagues

19
00:01:05,519 --> 00:01:12,640
in in journalism

20
00:01:08,560 --> 00:01:15,759
world so we are going to see

21
00:01:12,640 --> 00:01:18,880
um how we started from

22
00:01:15,759 --> 00:01:22,240
from legacy components

23
00:01:18,880 --> 00:01:25,360
and and how

24
00:01:22,240 --> 00:01:26,000
we we had some needs for the journalists

25
00:01:25,360 --> 00:01:29,680
and how we

26
00:01:26,000 --> 00:01:32,720
use that brics to build um

27
00:01:29,680 --> 00:01:35,920
another component on a project

28
00:01:32,720 --> 00:01:37,679
called datashare and and

29
00:01:35,920 --> 00:01:40,000
we are going to see the challenges and

30
00:01:37,680 --> 00:01:42,240
uh how we tackle

31
00:01:40,000 --> 00:01:42,240
them

32
00:01:43,200 --> 00:01:49,360
so first of all um just just um

33
00:01:46,799 --> 00:01:50,479
a little parenthesis legacy software is

34
00:01:49,360 --> 00:01:53,119
so is

35
00:01:50,479 --> 00:01:55,600
very often connected negatively it

36
00:01:53,119 --> 00:01:57,600
seemed like a drag that you're

37
00:01:55,600 --> 00:01:59,280
that you're pulling and that that is

38
00:01:57,600 --> 00:02:02,479
lowering the

39
00:01:59,280 --> 00:02:03,759
the the productivity of software

40
00:02:02,479 --> 00:02:06,960
development

41
00:02:03,759 --> 00:02:09,280
here is uh more at the first

42
00:02:06,960 --> 00:02:11,280
sense of the word is something that is

43
00:02:09,280 --> 00:02:15,120
transferred to someone

44
00:02:11,280 --> 00:02:18,959
so it's like an asset

45
00:02:15,120 --> 00:02:21,840
and the asset has some

46
00:02:18,959 --> 00:02:22,560
drawbacks and we are going to see how we

47
00:02:21,840 --> 00:02:25,040
use those

48
00:02:22,560 --> 00:02:26,000
assets and and remove the drawbacks to

49
00:02:25,040 --> 00:02:28,879
uh

50
00:02:26,000 --> 00:02:31,760
to to address the new needs of the of

51
00:02:28,879 --> 00:02:31,760
the journalists

52
00:02:33,120 --> 00:02:40,560
the second thing about legacy is uh

53
00:02:36,959 --> 00:02:41,519
is uh is it so it's capturing knowledge

54
00:02:40,560 --> 00:02:43,599
so uh

55
00:02:41,519 --> 00:02:45,200
we used the this component because there

56
00:02:43,599 --> 00:02:48,799
were knowledge about

57
00:02:45,200 --> 00:02:52,480
natural language processing and uh also

58
00:02:48,800 --> 00:02:55,360
what has been done before about the

59
00:02:52,480 --> 00:02:56,720
the main the huge leaks like panama

60
00:02:55,360 --> 00:02:59,920
papers

61
00:02:56,720 --> 00:03:02,000
and and the the other thing is that

62
00:02:59,920 --> 00:03:02,958
everyone is producing legacy at some

63
00:03:02,000 --> 00:03:06,239
point

64
00:03:02,959 --> 00:03:08,959
so in fact uh really

65
00:03:06,239 --> 00:03:09,360
fast after a few months of development

66
00:03:08,959 --> 00:03:13,280
we

67
00:03:09,360 --> 00:03:16,840
often finding that the the code base

68
00:03:13,280 --> 00:03:19,920
that we built few months before

69
00:03:16,840 --> 00:03:22,800
is is already legacy

70
00:03:19,920 --> 00:03:24,480
so the first break was um extract it was

71
00:03:22,800 --> 00:03:28,159
a common line tool

72
00:03:24,480 --> 00:03:31,359
and it was used on the panama papers and

73
00:03:28,159 --> 00:03:33,040
it was uh 11 million documents and it

74
00:03:31,360 --> 00:03:36,159
was 2.3

75
00:03:33,040 --> 00:03:39,359
tierra terabytes it was used also

76
00:03:36,159 --> 00:03:40,720
on swiss leaks and luxembourg leaks and

77
00:03:39,360 --> 00:03:44,640
it was about

78
00:03:40,720 --> 00:03:48,640
10k yeah 10 7 000

79
00:03:44,640 --> 00:03:52,238
lines of code it was uh java

80
00:03:48,640 --> 00:03:55,439
and uh the you can see the

81
00:03:52,239 --> 00:03:56,480
the the workflow here so basically it

82
00:03:55,439 --> 00:03:58,720
was uh

83
00:03:56,480 --> 00:04:00,238
it was scanning the the file system so

84
00:03:58,720 --> 00:04:03,519
we have we

85
00:04:00,239 --> 00:04:05,280
received some some leaks on a usb key or

86
00:04:03,519 --> 00:04:08,879
a hard drive or whatever

87
00:04:05,280 --> 00:04:11,360
and then it's uh it's scanning the

88
00:04:08,879 --> 00:04:12,319
the the file system and it's putting all

89
00:04:11,360 --> 00:04:15,760
the parts in a

90
00:04:12,319 --> 00:04:17,839
in a queue and a shared queue

91
00:04:15,760 --> 00:04:19,918
then there are extractors that are based

92
00:04:17,839 --> 00:04:23,119
on apache tica

93
00:04:19,918 --> 00:04:25,359
that are extracting the texts and and

94
00:04:23,120 --> 00:04:28,639
feeding a report map to

95
00:04:25,360 --> 00:04:31,199
to be allowed to to see afterwards which

96
00:04:28,639 --> 00:04:33,759
file has not been extracted correctly

97
00:04:31,199 --> 00:04:37,040
and that is sending the text to a spewer

98
00:04:33,759 --> 00:04:40,960
that is uh outputting the text into

99
00:04:37,040 --> 00:04:44,240
several outputs like a rest service or

100
00:04:40,960 --> 00:04:47,919
file or std out

101
00:04:44,240 --> 00:04:50,160
we we used uh with extracts

102
00:04:47,919 --> 00:04:52,400
most of the time it was solar that was

103
00:04:50,160 --> 00:04:54,560
the index was used at the time

104
00:04:52,400 --> 00:04:56,638
and that was one one of the the problems

105
00:04:54,560 --> 00:04:59,440
that we were facing and we will talk

106
00:04:56,639 --> 00:04:59,440
about this later

107
00:05:00,320 --> 00:05:05,120
the then when the all the the files were

108
00:05:04,000 --> 00:05:08,400
indexed into

109
00:05:05,120 --> 00:05:09,039
solar then there was um another open

110
00:05:08,400 --> 00:05:11,599
source

111
00:05:09,039 --> 00:05:12,719
um component that we used was called

112
00:05:11,600 --> 00:05:16,960
blacklight

113
00:05:12,720 --> 00:05:20,000
and it's a ruby on rails um software

114
00:05:16,960 --> 00:05:20,880
and then this this um blacklight is a

115
00:05:20,000 --> 00:05:24,000
kind of

116
00:05:20,880 --> 00:05:28,479
a framework that that can that allow

117
00:05:24,000 --> 00:05:31,280
to build um uh websites to browse

118
00:05:28,479 --> 00:05:32,800
some big corpuses of data like uh and

119
00:05:31,280 --> 00:05:36,159
it's used for example by

120
00:05:32,800 --> 00:05:36,720
libraries or universities and so we used

121
00:05:36,160 --> 00:05:39,680
it

122
00:05:36,720 --> 00:05:41,199
uh for um for the journalists to uh to

123
00:05:39,680 --> 00:05:44,880
help them to use

124
00:05:41,199 --> 00:05:47,840
solar and without uh having to know

125
00:05:44,880 --> 00:05:49,280
actually the complexity of uh of of the

126
00:05:47,840 --> 00:05:53,919
the indexer

127
00:05:49,280 --> 00:05:53,919
uh languages and uh and um

128
00:05:54,800 --> 00:05:57,280
inputs

129
00:05:58,080 --> 00:06:03,520
um yeah so so it was basically

130
00:06:01,440 --> 00:06:04,880
uh quite simple it's a front application

131
00:06:03,520 --> 00:06:07,198
under on the back end in

132
00:06:04,880 --> 00:06:08,880
ruby or rails and then the the back-end

133
00:06:07,199 --> 00:06:12,080
index

134
00:06:08,880 --> 00:06:15,440
then there was this um

135
00:06:12,080 --> 00:06:17,198
it was a repository it was already

136
00:06:15,440 --> 00:06:20,400
called data share but it was not

137
00:06:17,199 --> 00:06:21,120
in production control it was not the

138
00:06:20,400 --> 00:06:23,599
same that

139
00:06:21,120 --> 00:06:24,319
um the knowledge center that was used by

140
00:06:23,600 --> 00:06:26,800
a lot of

141
00:06:24,319 --> 00:06:28,319
journalists that i share was a proof of

142
00:06:26,800 --> 00:06:30,639
concept and was doing

143
00:06:28,319 --> 00:06:32,400
quite the same at the beginning it was

144
00:06:30,639 --> 00:06:35,600
uh passing and uh

145
00:06:32,400 --> 00:06:38,159
and extracting with tikka also the the

146
00:06:35,600 --> 00:06:39,919
the new thing was the five i don't know

147
00:06:38,160 --> 00:06:43,039
if if you can read but

148
00:06:39,919 --> 00:06:43,840
the five um natural language processing

149
00:06:43,039 --> 00:06:47,280
pipelines

150
00:06:43,840 --> 00:06:48,318
um that were used to uh to extract

151
00:06:47,280 --> 00:06:51,280
naming entities

152
00:06:48,319 --> 00:06:51,840
from from the text and help journalism

153
00:06:51,280 --> 00:06:55,198
to

154
00:06:51,840 --> 00:06:58,479
to to have a grasp of the of the leaks

155
00:06:55,199 --> 00:07:02,080
quite uh fast and that was

156
00:06:58,479 --> 00:07:05,520
an indexing uh these name entities

157
00:07:02,080 --> 00:07:06,800
into uh into the index so the the two

158
00:07:05,520 --> 00:07:10,080
new things here were

159
00:07:06,800 --> 00:07:13,440
the the natural language processing and

160
00:07:10,080 --> 00:07:16,880
uh it was using elasticsearch index

161
00:07:13,440 --> 00:07:18,560
and so we are we we had uh quite

162
00:07:16,880 --> 00:07:19,759
overlapping between datashare and

163
00:07:18,560 --> 00:07:23,120
extract

164
00:07:19,759 --> 00:07:26,880
and uh and we wanted to uh to um

165
00:07:23,120 --> 00:07:29,440
to do approximately the same as

166
00:07:26,880 --> 00:07:30,400
the knowledge center but with uh

167
00:07:29,440 --> 00:07:33,360
improved

168
00:07:30,400 --> 00:07:33,359
user experience

169
00:07:33,520 --> 00:07:36,639
and uh yeah that's it that was the the

170
00:07:35,840 --> 00:07:40,318
three main

171
00:07:36,639 --> 00:07:43,599
bricks and all the open source world

172
00:07:40,319 --> 00:07:47,440
that was uh bringing us some some

173
00:07:43,599 --> 00:07:48,878
other tools and we can now explain how

174
00:07:47,440 --> 00:07:58,330
we

175
00:07:48,879 --> 00:08:01,440
we build data share with this

176
00:07:58,330 --> 00:08:01,440
[Applause]

177
00:08:05,680 --> 00:08:13,280
so yes so in a in a sum up so that the

178
00:08:09,599 --> 00:08:17,039
that the issues that we had to fix so

179
00:08:13,280 --> 00:08:19,520
first in extract the

180
00:08:17,039 --> 00:08:21,919
the package was pretty big the packaging

181
00:08:19,520 --> 00:08:24,159
itself was pretty big so we decided to

182
00:08:21,919 --> 00:08:27,758
split it into two parts

183
00:08:24,160 --> 00:08:28,319
one was only the the the java libs or

184
00:08:27,759 --> 00:08:31,680
the

185
00:08:28,319 --> 00:08:34,560
jar that could be included

186
00:08:31,680 --> 00:08:35,200
in that will be included in our new tool

187
00:08:34,559 --> 00:08:37,439
and the

188
00:08:35,200 --> 00:08:38,399
the other one part was the command line

189
00:08:37,440 --> 00:08:40,399
interface to

190
00:08:38,399 --> 00:08:41,679
to use it through your terminal but

191
00:08:40,399 --> 00:08:46,000
without any

192
00:08:41,679 --> 00:08:50,239
any any ghee and

193
00:08:46,000 --> 00:08:52,720
we with the we face is also the

194
00:08:50,240 --> 00:08:53,680
a problem of scalability with the solar

195
00:08:52,720 --> 00:08:56,720
index

196
00:08:53,680 --> 00:08:58,319
because we had too many uh too many

197
00:08:56,720 --> 00:09:01,839
worlds too many

198
00:08:58,320 --> 00:09:04,640
documents in it so we wanted to

199
00:09:01,839 --> 00:09:06,080
to switch or to migrate from solar to

200
00:09:04,640 --> 00:09:09,120
elasticsearch

201
00:09:06,080 --> 00:09:11,920
to avoid this problem scalability uh

202
00:09:09,120 --> 00:09:13,279
just as a reminder uh yes just like

203
00:09:11,920 --> 00:09:17,120
bruno said

204
00:09:13,279 --> 00:09:21,200
the the panel purpose was uh how many

205
00:09:17,120 --> 00:09:23,040
so 11 millions of documents and

206
00:09:21,200 --> 00:09:26,000
our last leak uh i don't know if you

207
00:09:23,040 --> 00:09:30,079
heard about the luandalex um

208
00:09:26,000 --> 00:09:34,880
two weeks ago about angola and it was a

209
00:09:30,080 --> 00:09:37,120
more than 700 documents so that's pretty

210
00:09:34,880 --> 00:09:37,120
seven

211
00:09:37,920 --> 00:09:43,599
forget a few zeros uh documents

212
00:09:41,040 --> 00:09:45,199
so yes that's that's uh that's part of

213
00:09:43,600 --> 00:09:48,720
the problem

214
00:09:45,200 --> 00:09:51,760
and um we about

215
00:09:48,720 --> 00:09:53,680
uh about black light we had

216
00:09:51,760 --> 00:09:55,360
so it's in ruby on rails i have nothing

217
00:09:53,680 --> 00:09:58,079
about it but it was pretty

218
00:09:55,360 --> 00:09:59,440
pretty difficult to modify the interface

219
00:09:58,080 --> 00:10:03,120
and i will come back on it

220
00:09:59,440 --> 00:10:05,279
uh later but the the web interface the

221
00:10:03,120 --> 00:10:07,040
user experience is pretty important for

222
00:10:05,279 --> 00:10:10,399
us because

223
00:10:07,040 --> 00:10:12,959
journalists but probably researchers

224
00:10:10,399 --> 00:10:14,160
are not tech savvy at all so everything

225
00:10:12,959 --> 00:10:17,199
should be well

226
00:10:14,160 --> 00:10:18,319
well seen to to make their life as

227
00:10:17,200 --> 00:10:22,560
easiest as possible

228
00:10:18,320 --> 00:10:25,600
and our life easy too

229
00:10:22,560 --> 00:10:28,719
and another problem was

230
00:10:25,600 --> 00:10:31,040
maintainability so about the the bricks

231
00:10:28,720 --> 00:10:33,519
that we had but also

232
00:10:31,040 --> 00:10:34,319
also about the the work that the

233
00:10:33,519 --> 00:10:36,079
journalists

234
00:10:34,320 --> 00:10:37,760
uh wanted to do because they already

235
00:10:36,079 --> 00:10:40,880
have a tool that

236
00:10:37,760 --> 00:10:42,720
in production the the the one about

237
00:10:40,880 --> 00:10:45,439
black light

238
00:10:42,720 --> 00:10:46,000
that was working so we had to develop a

239
00:10:45,440 --> 00:10:48,399
new tool

240
00:10:46,000 --> 00:10:49,600
but without uh without any breaks so

241
00:10:48,399 --> 00:10:53,120
they needed the same

242
00:10:49,600 --> 00:10:56,000
the same functionalities without any um

243
00:10:53,120 --> 00:10:56,959
any end of service and with exactly the

244
00:10:56,000 --> 00:11:00,320
same documents

245
00:10:56,959 --> 00:11:03,199
and so that was part of the

246
00:11:00,320 --> 00:11:03,839
of the of the of the issues that we had

247
00:11:03,200 --> 00:11:08,560
and

248
00:11:03,839 --> 00:11:11,920
and then the challenges so um

249
00:11:08,560 --> 00:11:13,518
as i said before about the the huge

250
00:11:11,920 --> 00:11:15,839
amount of documents that

251
00:11:13,519 --> 00:11:17,040
that usually get at the beginning of the

252
00:11:15,839 --> 00:11:19,279
of the of a

253
00:11:17,040 --> 00:11:20,319
of a league there they get all those

254
00:11:19,279 --> 00:11:22,079
documents and the

255
00:11:20,320 --> 00:11:23,839
the first question is okay where should

256
00:11:22,079 --> 00:11:27,040
i begin so

257
00:11:23,839 --> 00:11:29,920
the the let's say new challenge of

258
00:11:27,040 --> 00:11:30,880
of data share would be to give them some

259
00:11:29,920 --> 00:11:34,479
ideas

260
00:11:30,880 --> 00:11:37,920
uh to to how could they start and

261
00:11:34,480 --> 00:11:40,880
how is the have a just have a

262
00:11:37,920 --> 00:11:42,319
global overview about this leak really

263
00:11:40,880 --> 00:11:47,680
fast

264
00:11:42,320 --> 00:11:47,680
and to answer some questions just like

265
00:11:47,920 --> 00:11:50,959
just like easy questions like how many

266
00:11:49,600 --> 00:11:54,079
documents but

267
00:11:50,959 --> 00:11:55,599
also how how big is the document

268
00:11:54,079 --> 00:11:59,439
speaking about

269
00:11:55,600 --> 00:12:01,680
geographical location or people

270
00:11:59,440 --> 00:12:02,959
included in those documents so just like

271
00:12:01,680 --> 00:12:06,239
a global overview

272
00:12:02,959 --> 00:12:06,719
so what's happening in these leaks so

273
00:12:06,240 --> 00:12:09,519
that's

274
00:12:06,720 --> 00:12:10,959
challenging that challenges that

275
00:12:09,519 --> 00:12:13,600
journalists encounter

276
00:12:10,959 --> 00:12:15,518
journalists encounter really often but

277
00:12:13,600 --> 00:12:17,440
i'm pretty sure that researchers have

278
00:12:15,519 --> 00:12:19,120
have quite the same tool so that's

279
00:12:17,440 --> 00:12:20,880
probably one of the reasons why we are

280
00:12:19,120 --> 00:12:22,800
here

281
00:12:20,880 --> 00:12:24,160
and to answer all these these

282
00:12:22,800 --> 00:12:27,199
challenging and

283
00:12:24,160 --> 00:12:30,319
faces all these issues we

284
00:12:27,200 --> 00:12:33,440
we developed data share here is

285
00:12:30,320 --> 00:12:36,959
um the the the new schema of the

286
00:12:33,440 --> 00:12:38,079
of the tool as you can see part of the

287
00:12:36,959 --> 00:12:41,439
workflow is really

288
00:12:38,079 --> 00:12:41,760
uh is really the same that than before

289
00:12:41,440 --> 00:12:45,200
but

290
00:12:41,760 --> 00:12:48,959
we so we we succeeded in

291
00:12:45,200 --> 00:12:51,440
putting the experienced

292
00:12:48,959 --> 00:12:52,560
bricks that we had just like extract in

293
00:12:51,440 --> 00:12:55,040
the in the new

294
00:12:52,560 --> 00:12:57,279
in the new tool of data share so there

295
00:12:55,040 --> 00:12:59,120
are there is extract here

296
00:12:57,279 --> 00:13:01,040
uh at the really beginning of the

297
00:12:59,120 --> 00:13:01,920
process at the scanning and indexing

298
00:13:01,040 --> 00:13:04,160
part

299
00:13:01,920 --> 00:13:05,439
and all the data extracted by extract

300
00:13:04,160 --> 00:13:08,560
just like the well named

301
00:13:05,440 --> 00:13:11,839
uh is will be uh will be uh

302
00:13:08,560 --> 00:13:14,479
pushed in into a elastic search index

303
00:13:11,839 --> 00:13:16,880
so that's a new one and we we needed to

304
00:13:14,480 --> 00:13:19,440
yes migrate the data from solar to

305
00:13:16,880 --> 00:13:20,320
elasticsearch because it would have

306
00:13:19,440 --> 00:13:23,440
taken

307
00:13:20,320 --> 00:13:27,040
months to re-index all the older leaks

308
00:13:23,440 --> 00:13:28,000
otherwise and you can see here that we

309
00:13:27,040 --> 00:13:30,240
have the

310
00:13:28,000 --> 00:13:31,120
nlp so at the at the really bottom

311
00:13:30,240 --> 00:13:34,720
that's the nlp

312
00:13:31,120 --> 00:13:37,360
pipeline so we we just uh

313
00:13:34,720 --> 00:13:38,240
get the five nlp pipelines that we had

314
00:13:37,360 --> 00:13:42,160
and developed an

315
00:13:38,240 --> 00:13:43,440
a common api to to uh to be able to

316
00:13:42,160 --> 00:13:46,319
interact with with

317
00:13:43,440 --> 00:13:48,079
it and to so i don't know if i mentioned

318
00:13:46,320 --> 00:13:51,199
that before probably not but the nlp

319
00:13:48,079 --> 00:13:52,800
pipelines is uh just a way to to collect

320
00:13:51,199 --> 00:13:56,000
unnamed entities in the

321
00:13:52,800 --> 00:13:59,279
in the document that we extracted before

322
00:13:56,000 --> 00:14:00,000
so yes that's it that's that's the new

323
00:13:59,279 --> 00:14:03,120
data share

324
00:14:00,000 --> 00:14:07,199
and i think no it's time for demo

325
00:14:03,120 --> 00:14:10,320
let's cross fingers yeah it works

326
00:14:07,199 --> 00:14:13,519
so here is our demo demo website

327
00:14:10,320 --> 00:14:15,360
about data share

328
00:14:13,519 --> 00:14:16,560
probably i should mention that it's an

329
00:14:15,360 --> 00:14:18,160
open source tool so

330
00:14:16,560 --> 00:14:20,239
everything is on github and if you need

331
00:14:18,160 --> 00:14:23,120
to if you need to use it

332
00:14:20,240 --> 00:14:24,639
on your own corpus of document it

333
00:14:23,120 --> 00:14:27,199
doesn't have to be big but

334
00:14:24,639 --> 00:14:28,639
if you just have document that you need

335
00:14:27,199 --> 00:14:31,040
to analyze

336
00:14:28,639 --> 00:14:31,920
it's free for you so here is an example

337
00:14:31,040 --> 00:14:35,360
uh on the

338
00:14:31,920 --> 00:14:39,839
lux slicks which is a a leaks that

339
00:14:35,360 --> 00:14:41,760
we revealed a few years ago

340
00:14:39,839 --> 00:14:42,880
so here's the list of the projects so

341
00:14:41,760 --> 00:14:46,079
you may have

342
00:14:42,880 --> 00:14:47,439
a numerous projects here or several

343
00:14:46,079 --> 00:14:50,800
projects here

344
00:14:47,440 --> 00:14:52,320
separated and if i click on look slicks

345
00:14:50,800 --> 00:14:55,839
i have first

346
00:14:52,320 --> 00:14:57,839
the number of documents in the leaks

347
00:14:55,839 --> 00:14:59,519
so that's pretty important and here is

348
00:14:57,839 --> 00:15:02,720
the list of documents

349
00:14:59,519 --> 00:15:04,720
so that that the first step and here on

350
00:15:02,720 --> 00:15:07,760
the left you have the facets

351
00:15:04,720 --> 00:15:11,040
so we built some facets on the

352
00:15:07,760 --> 00:15:14,480
on the elasticsearch and

353
00:15:11,040 --> 00:15:17,519
those facets are the response so

354
00:15:14,480 --> 00:15:18,160
as a response about the global overview

355
00:15:17,519 --> 00:15:20,959
of

356
00:15:18,160 --> 00:15:23,199
of the leak and of the document so by

357
00:15:20,959 --> 00:15:24,399
example here i can click on file types

358
00:15:23,199 --> 00:15:27,120
and i will see

359
00:15:24,399 --> 00:15:29,040
how many so the different types of

360
00:15:27,120 --> 00:15:31,040
document that i have in my corpus

361
00:15:29,040 --> 00:15:32,959
and how many of documents of each type

362
00:15:31,040 --> 00:15:33,759
so here in this corporate specifically i

363
00:15:32,959 --> 00:15:37,439
do have only

364
00:15:33,759 --> 00:15:38,720
pdf so so yes i have thousands of pdfs

365
00:15:37,440 --> 00:15:41,040
in that corpus

366
00:15:38,720 --> 00:15:42,399
and i can do exactly the same for the

367
00:15:41,040 --> 00:15:44,319
languages so i

368
00:15:42,399 --> 00:15:47,199
know in one click i can discover that i

369
00:15:44,320 --> 00:15:49,120
have some documents in in english french

370
00:15:47,199 --> 00:15:50,160
german and italian so it gave a first

371
00:15:49,120 --> 00:15:52,800
overview of

372
00:15:50,160 --> 00:15:53,279
the yes the languages of the documents

373
00:15:52,800 --> 00:15:56,800
in in

374
00:15:53,279 --> 00:15:59,600
in that corpus and

375
00:15:56,800 --> 00:16:01,199
and about the nlp pipelines here are the

376
00:15:59,600 --> 00:16:05,360
results so

377
00:16:01,199 --> 00:16:09,359
i i so all the name entities extracted

378
00:16:05,360 --> 00:16:12,639
sorry that are from oh thank you

379
00:16:09,360 --> 00:16:15,279
that's about people localization and

380
00:16:12,639 --> 00:16:16,000
organizations so here if i click i can

381
00:16:15,279 --> 00:16:19,279
see that

382
00:16:16,000 --> 00:16:19,920
in my so first in my more than thousands

383
00:16:19,279 --> 00:16:23,519
document

384
00:16:19,920 --> 00:16:26,959
i have i will have a 40 47

385
00:16:23,519 --> 00:16:30,399
000 different people mentioned in it

386
00:16:26,959 --> 00:16:33,439
and one of the main mentions is call

387
00:16:30,399 --> 00:16:37,519
mr cole which as you all know is the

388
00:16:33,440 --> 00:16:40,399
main shareholder and stakeholder of the

389
00:16:37,519 --> 00:16:42,800
of the lux leaks but we did a new new it

390
00:16:40,399 --> 00:16:45,600
before data share and before before

391
00:16:42,800 --> 00:16:47,439
reading the documents so that's that's a

392
00:16:45,600 --> 00:16:50,240
good um

393
00:16:47,440 --> 00:16:52,160
ad for for the corpus and i can do

394
00:16:50,240 --> 00:16:54,880
exactly the same with the locations

395
00:16:52,160 --> 00:16:55,759
and here i will see luxembourg us uk

396
00:16:54,880 --> 00:16:59,199
which are the

397
00:16:55,759 --> 00:17:03,199
the main countries uh um

398
00:16:59,199 --> 00:17:06,399
mentioned in the in that corpus

399
00:17:03,199 --> 00:17:07,199
so that's data share and that's the way

400
00:17:06,400 --> 00:17:10,160
we developed

401
00:17:07,199 --> 00:17:11,760
uh we developed it so that was the past

402
00:17:10,160 --> 00:17:15,199
and the present of data share

403
00:17:11,760 --> 00:17:17,839
and about the future we have lots of uh

404
00:17:15,199 --> 00:17:18,480
we would like to develop lots of things

405
00:17:17,839 --> 00:17:20,958
just like

406
00:17:18,480 --> 00:17:22,000
adding adding comments on documents

407
00:17:20,959 --> 00:17:23,760
because

408
00:17:22,000 --> 00:17:25,199
yeah i have also have access to

409
00:17:23,760 --> 00:17:27,919
documents so i would

410
00:17:25,199 --> 00:17:30,840
we would like to journalist to be able

411
00:17:27,919 --> 00:17:34,559
to put comments directly on on documents

412
00:17:30,840 --> 00:17:35,918
and creating some some dashboard on each

413
00:17:34,559 --> 00:17:39,840
project to have a

414
00:17:35,919 --> 00:17:41,760
really quick overview of

415
00:17:39,840 --> 00:17:43,039
i don't know how many uh how many

416
00:17:41,760 --> 00:17:45,679
documents on which tip

417
00:17:43,039 --> 00:17:47,039
on which type sorry and about the

418
00:17:45,679 --> 00:17:47,679
creation day just like a simple

419
00:17:47,039 --> 00:17:52,080
histogram

420
00:17:47,679 --> 00:17:52,080
on the creation depth date and um

421
00:17:52,320 --> 00:17:57,039
we would like to re we developed the

422
00:17:55,200 --> 00:17:58,720
installers because no it's based on

423
00:17:57,039 --> 00:17:59,200
docker and yeah we have some doubt about

424
00:17:58,720 --> 00:18:01,520
the

425
00:17:59,200 --> 00:18:02,559
about about it because we have lots of

426
00:18:01,520 --> 00:18:05,918
difficulties to

427
00:18:02,559 --> 00:18:09,678
to to with uh with docker

428
00:18:05,919 --> 00:18:11,520
and um and yeah the the point is that

429
00:18:09,679 --> 00:18:12,960
journalists i i said it before but

430
00:18:11,520 --> 00:18:15,200
that's really important for us

431
00:18:12,960 --> 00:18:17,360
that journalists are really not tech

432
00:18:15,200 --> 00:18:19,840
savvy at all so

433
00:18:17,360 --> 00:18:20,559
one of the import most important point

434
00:18:19,840 --> 00:18:24,080
is to

435
00:18:20,559 --> 00:18:26,720
to have a well-developed

436
00:18:24,080 --> 00:18:28,960
well-think well-stock sorry interface to

437
00:18:26,720 --> 00:18:31,120
to and easy to manipulate and easy to

438
00:18:28,960 --> 00:18:34,000
navigate and dig into the corpus

439
00:18:31,120 --> 00:18:35,120
and and easy to install on on machines

440
00:18:34,000 --> 00:18:38,640
and on computers for

441
00:18:35,120 --> 00:18:41,120
for everybody uh yeah and we do have

442
00:18:38,640 --> 00:18:42,240
uh yes and we wanted to develop a

443
00:18:41,120 --> 00:18:45,199
plug-in

444
00:18:42,240 --> 00:18:46,640
plugin architecture to to let developers

445
00:18:45,200 --> 00:18:49,919
to be able to

446
00:18:46,640 --> 00:18:53,600
to develop their own own plugins

447
00:18:49,919 --> 00:18:56,000
so so that's it and

448
00:18:53,600 --> 00:18:56,959
i hope you you will find a usage for

449
00:18:56,000 --> 00:19:11,840
that tool too

450
00:18:56,960 --> 00:19:11,840
and feel free to open issues

451
00:19:18,080 --> 00:19:25,520
which one the edward snowden corpus no

452
00:19:24,160 --> 00:19:27,679
because we hadn't

453
00:19:25,520 --> 00:19:28,720
i don't know uh i don't think so oh

454
00:19:27,679 --> 00:19:32,160
sorry yes

455
00:19:28,720 --> 00:19:32,720
so if the if this tool has been has been

456
00:19:32,160 --> 00:19:35,600
used

457
00:19:32,720 --> 00:19:37,039
uh on the document released by edward

458
00:19:35,600 --> 00:19:40,159
snowden

459
00:19:37,039 --> 00:19:42,160
so uh i don't think so because the

460
00:19:40,160 --> 00:19:45,039
so first the tool is quite new we

461
00:19:42,160 --> 00:19:47,520
released the beta version one year ago

462
00:19:45,039 --> 00:19:49,440
and uh i don't know if those documents

463
00:19:47,520 --> 00:19:53,840
are public or not

464
00:19:49,440 --> 00:19:53,840
so maybe maybe that's the reason

465
00:19:57,520 --> 00:20:07,840
not as as far as as we know now

466
00:20:01,200 --> 00:20:07,840
they they are not using it they should

467
00:20:08,159 --> 00:20:10,480
yes

468
00:20:15,360 --> 00:20:19,039
uh we heard about we've heard about it

469
00:20:17,760 --> 00:20:22,480
sure because it's

470
00:20:19,039 --> 00:20:25,679
it does quite the same than us i think

471
00:20:22,480 --> 00:20:25,679
but not as good as us

472
00:20:28,880 --> 00:20:32,720
sure sure there are lots of tools that

473
00:20:31,440 --> 00:20:35,280
that

474
00:20:32,720 --> 00:20:37,440
so it's not a new need because it has

475
00:20:35,280 --> 00:20:39,918
been the same need for years and years

476
00:20:37,440 --> 00:20:41,120
so open semantic search is doing more or

477
00:20:39,919 --> 00:20:44,799
less the same

478
00:20:41,120 --> 00:20:47,600
and there is a the one from occ aleph

479
00:20:44,799 --> 00:20:48,799
alef is another tool that that do more

480
00:20:47,600 --> 00:21:03,280
or less the same but

481
00:20:48,799 --> 00:21:06,480
i think that our is prettiest

482
00:21:03,280 --> 00:21:17,840
the so the question was uh the

483
00:21:06,480 --> 00:21:17,840
language the programming language used

484
00:21:28,799 --> 00:21:31,918
so the question was uh how many

485
00:21:31,039 --> 00:21:34,559
languages are

486
00:21:31,919 --> 00:21:37,840
supported by the nlp pipelines how do

487
00:21:34,559 --> 00:21:37,840
you figure out which one it is

488
00:21:38,799 --> 00:21:43,200
yeah it's it's a question of the of the

489
00:21:40,960 --> 00:21:46,159
chicken and the egg

490
00:21:43,200 --> 00:21:46,960
because yeah when we when we are we are

491
00:21:46,159 --> 00:21:50,640
detecting

492
00:21:46,960 --> 00:21:53,280
the language with uh um a small uh

493
00:21:50,640 --> 00:21:54,480
framework that is called a language uh

494
00:21:53,280 --> 00:21:57,520
detector

495
00:21:54,480 --> 00:22:00,480
and um and the problem is that

496
00:21:57,520 --> 00:22:02,480
when we are doing ocr then we have to

497
00:22:00,480 --> 00:22:05,600
provide the language and we have to

498
00:22:02,480 --> 00:22:08,559
to to make run ocr to to know

499
00:22:05,600 --> 00:22:10,240
uh what language is the text so for for

500
00:22:08,559 --> 00:22:13,280
the moment

501
00:22:10,240 --> 00:22:16,960
the ocr we are using tesseract and uh

502
00:22:13,280 --> 00:22:19,600
is uh it's um supporting uh

503
00:22:16,960 --> 00:22:20,559
i don't know i think it's 30 or more

504
00:22:19,600 --> 00:22:23,760
language

505
00:22:20,559 --> 00:22:26,559
languages but the

506
00:22:23,760 --> 00:22:29,520
so we are extracting texts from a lot of

507
00:22:26,559 --> 00:22:32,480
languages but

508
00:22:29,520 --> 00:22:33,039
the extraction of the name that it is uh

509
00:22:32,480 --> 00:22:37,039
are

510
00:22:33,039 --> 00:22:40,480
run on only five languages or six now

511
00:22:37,039 --> 00:22:45,360
i think that there is the chinese and uh

512
00:22:40,480 --> 00:22:47,679
yeah english german spanish and

513
00:22:45,360 --> 00:22:47,678
french

514
00:22:52,320 --> 00:22:55,760
then the question could be also like do

515
00:22:54,480 --> 00:22:58,320
you have features

516
00:22:55,760 --> 00:22:59,280
uh to let journalists actually declare

517
00:22:58,320 --> 00:23:01,120
that they are

518
00:22:59,280 --> 00:23:04,158
suspect that the the name entity or

519
00:23:01,120 --> 00:23:07,520
cognition algorithm failed

520
00:23:04,159 --> 00:23:11,039
or or the ucr so

521
00:23:07,520 --> 00:23:13,760
now not not uh the question was um

522
00:23:11,039 --> 00:23:14,640
is there some feature to to let

523
00:23:13,760 --> 00:23:16,960
journalists

524
00:23:14,640 --> 00:23:18,320
know that uh the extraction of name

525
00:23:16,960 --> 00:23:21,679
entity failed

526
00:23:18,320 --> 00:23:22,000
and no it's um it's a it's a feature

527
00:23:21,679 --> 00:23:24,159
that

528
00:23:22,000 --> 00:23:25,120
we that we are going to develop is how

529
00:23:24,159 --> 00:23:27,280
we are

530
00:23:25,120 --> 00:23:29,280
concentrating all the the name that it

531
00:23:27,280 --> 00:23:32,320
is for example knowing

532
00:23:29,280 --> 00:23:35,520
also that donald trump is the same as

533
00:23:32,320 --> 00:23:38,480
d trump and it's uh yeah

534
00:23:35,520 --> 00:23:41,760
it's uh like um open refine uh could

535
00:23:38,480 --> 00:23:41,760
could help us for example

536
00:23:42,559 --> 00:23:46,960
another question yeah yeah what kind of

537
00:23:45,440 --> 00:23:47,600
hardware do you actually need to like

538
00:23:46,960 --> 00:23:49,440
run

539
00:23:47,600 --> 00:23:52,240
through this whole process like a

540
00:23:49,440 --> 00:23:54,880
desktop computer or

541
00:23:52,240 --> 00:23:56,720
so there are we are we are using what

542
00:23:54,880 --> 00:23:59,840
what we are trying to do is

543
00:23:56,720 --> 00:24:02,960
what i call uh total scalability

544
00:23:59,840 --> 00:24:04,240
is um we we have built um because we

545
00:24:02,960 --> 00:24:06,559
have partners that are

546
00:24:04,240 --> 00:24:07,520
in in countries that are where there are

547
00:24:06,559 --> 00:24:11,360
not a lot of

548
00:24:07,520 --> 00:24:14,799
resources like africa for example and um

549
00:24:11,360 --> 00:24:16,799
and so we we made um a version with the

550
00:24:14,799 --> 00:24:18,240
embedded elasticsearch that could run on

551
00:24:16,799 --> 00:24:21,440
a raspberry pi

552
00:24:18,240 --> 00:24:23,600
but uh at the same time we want it to be

553
00:24:21,440 --> 00:24:26,720
able and we are running the

554
00:24:23,600 --> 00:24:29,439
the the the extraction of

555
00:24:26,720 --> 00:24:30,159
bid leaks like panama papers on cloud

556
00:24:29,440 --> 00:24:33,600
computing

557
00:24:30,159 --> 00:24:35,760
on on 30 or

558
00:24:33,600 --> 00:24:47,840
i think the most yeah 30 nodes at the

559
00:24:35,760 --> 00:24:47,840
same time

560
00:24:58,960 --> 00:25:05,600
of named entities from the documents

561
00:25:02,159 --> 00:25:09,520
no no no so so

562
00:25:05,600 --> 00:25:12,480
one of the sorry yes

563
00:25:09,520 --> 00:25:14,639
so the question was from a document can

564
00:25:12,480 --> 00:25:16,159
i extract the the

565
00:25:14,640 --> 00:25:19,440
how do you say that corpus of

566
00:25:16,159 --> 00:25:21,919
co-occurrence of sorry

567
00:25:19,440 --> 00:25:23,600
citations no network of name-densities

568
00:25:21,919 --> 00:25:26,000
in documents so

569
00:25:23,600 --> 00:25:28,240
so the simplest answer is no and the

570
00:25:26,000 --> 00:25:31,360
longer answer is

571
00:25:28,240 --> 00:25:33,360
we plan first to be able to export named

572
00:25:31,360 --> 00:25:36,399
entities as csv

573
00:25:33,360 --> 00:25:40,000
so that would be the first step

574
00:25:36,400 --> 00:25:40,000
and after that you you will do your own

575
00:25:40,840 --> 00:25:51,230
tricks

576
00:25:43,120 --> 00:25:51,229
[Applause]

577
00:25:52,440 --> 00:25:55,440
you

