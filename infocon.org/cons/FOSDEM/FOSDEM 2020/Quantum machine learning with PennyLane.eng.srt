1
00:00:15,040 --> 00:00:19,840
bad

2
00:00:15,839 --> 00:00:21,359
yeah yeah i was the same yesterday i got

3
00:00:19,840 --> 00:00:24,400
back to my hotel at

4
00:00:21,359 --> 00:00:31,840
three and i just slept

5
00:00:24,400 --> 00:00:31,840
yeah hey

6
00:00:39,550 --> 00:00:42,629
[Music]

7
00:01:01,390 --> 00:01:05,840
[Music]

8
00:01:03,039 --> 00:01:05,840
okay everyone

9
00:01:07,040 --> 00:01:12,159
i'd like to introduce josh josh holds a

10
00:01:10,159 --> 00:01:12,960
phd in quantum information from the

11
00:01:12,159 --> 00:01:15,040
university of

12
00:01:12,960 --> 00:01:17,280
western australia and is now working for

13
00:01:15,040 --> 00:01:19,759
the quantum full stack startup

14
00:01:17,280 --> 00:01:21,040
xanadu in toronto where he's part of the

15
00:01:19,759 --> 00:01:23,920
quantum machine learning

16
00:01:21,040 --> 00:01:25,920
software team and quantum open source

17
00:01:23,920 --> 00:01:27,680
project that he's actively working on

18
00:01:25,920 --> 00:01:28,479
includes strawberry fields and penny

19
00:01:27,680 --> 00:01:30,000
lane

20
00:01:28,479 --> 00:01:32,079
and today he's going to talk about

21
00:01:30,000 --> 00:01:40,479
quantum machine learning with penny lane

22
00:01:32,079 --> 00:01:43,360
so please welcome josh

23
00:01:40,479 --> 00:01:43,679
hi everyone uh so as mark mentioned i'm

24
00:01:43,360 --> 00:01:45,840
a

25
00:01:43,680 --> 00:01:47,360
my name's josh i'm a quantum software

26
00:01:45,840 --> 00:01:49,040
developer at xanadu

27
00:01:47,360 --> 00:01:50,479
and at xander we're a full stack quantum

28
00:01:49,040 --> 00:01:52,720
confusing company so we're working on

29
00:01:50,479 --> 00:01:56,240
both the continuous variable hardware

30
00:01:52,720 --> 00:01:59,600
as well as open source quantum software

31
00:01:56,240 --> 00:02:01,119
development so as mentioned one of our

32
00:01:59,600 --> 00:02:02,479
software platforms are strawberry fields

33
00:02:01,119 --> 00:02:04,479
this is for

34
00:02:02,479 --> 00:02:06,399
uh integration and submissing jobs to

35
00:02:04,479 --> 00:02:07,800
our quantum hardware

36
00:02:06,399 --> 00:02:10,160
i encourage you to check it out

37
00:02:07,800 --> 00:02:11,359
storyfields.ai for the purposes of this

38
00:02:10,160 --> 00:02:12,959
talk i'll be focusing on one of our

39
00:02:11,360 --> 00:02:15,040
other libraries pennylane

40
00:02:12,959 --> 00:02:16,239
which is a open source quantum machine

41
00:02:15,040 --> 00:02:18,480
learning framework

42
00:02:16,239 --> 00:02:19,840
so um for this talk so i gave this a

43
00:02:18,480 --> 00:02:22,879
similar talk last year

44
00:02:19,840 --> 00:02:25,920
introducing penny lane this year i'll

45
00:02:22,879 --> 00:02:27,359
be taking going back a bit discussing

46
00:02:25,920 --> 00:02:28,799
where the ideas for penny lane came from

47
00:02:27,360 --> 00:02:31,040
and also moving forward a year so

48
00:02:28,800 --> 00:02:33,120
discussing what's changed in penny lane

49
00:02:31,040 --> 00:02:37,519
over the last uh year since i last

50
00:02:33,120 --> 00:02:40,800
talked at foster

51
00:02:37,519 --> 00:02:42,480
so starting off um penny lane was

52
00:02:40,800 --> 00:02:46,560
conceived about

53
00:02:42,480 --> 00:02:49,679
july 2018. we were noticing that

54
00:02:46,560 --> 00:02:50,000
um quantum machine learning has become

55
00:02:49,680 --> 00:02:52,239
quite

56
00:02:50,000 --> 00:02:53,599
a buzzword the number of papers in

57
00:02:52,239 --> 00:02:55,120
quantum machinery in the archive had

58
00:02:53,599 --> 00:02:58,159
started to increase significantly and

59
00:02:55,120 --> 00:02:59,440
as we were in this noisy era of quantum

60
00:02:58,159 --> 00:03:00,959
computation where we have these

61
00:02:59,440 --> 00:03:02,400
small-scale devices

62
00:03:00,959 --> 00:03:04,480
people were wondering what could we do

63
00:03:02,400 --> 00:03:07,120
to combine machine learning with quantum

64
00:03:04,480 --> 00:03:08,720
computing and what can we get out of it

65
00:03:07,120 --> 00:03:11,040
so we sat down and we asked ourselves

66
00:03:08,720 --> 00:03:12,159
why was deep learning so successful over

67
00:03:11,040 --> 00:03:14,159
the last

68
00:03:12,159 --> 00:03:15,599
decade or so and it basically came down

69
00:03:14,159 --> 00:03:18,480
to three main points

70
00:03:15,599 --> 00:03:20,319
hardware advancements so we'd seen gpus

71
00:03:18,480 --> 00:03:23,359
have become immensely important in

72
00:03:20,319 --> 00:03:26,159
machine learning the ability to scale up

73
00:03:23,360 --> 00:03:27,599
these thousands of small computations

74
00:03:26,159 --> 00:03:29,200
that people have been using in graphics

75
00:03:27,599 --> 00:03:31,200
for decades

76
00:03:29,200 --> 00:03:33,040
for machine learning caused a seismic

77
00:03:31,200 --> 00:03:35,040
shift in what we could do

78
00:03:33,040 --> 00:03:36,079
and uh this is in addition to tensor

79
00:03:35,040 --> 00:03:38,560
processing units

80
00:03:36,080 --> 00:03:40,000
so tpus as well have uh sort of

81
00:03:38,560 --> 00:03:41,519
revolutionized what we can do in machine

82
00:03:40,000 --> 00:03:43,440
learning

83
00:03:41,519 --> 00:03:44,840
in addition the abundance of workhorse

84
00:03:43,440 --> 00:03:47,840
algorithms so things like back

85
00:03:44,840 --> 00:03:51,599
propagation uh gradient descent

86
00:03:47,840 --> 00:03:54,879
uh we were able to suddenly train

87
00:03:51,599 --> 00:03:56,560
massive neural networks using uh

88
00:03:54,879 --> 00:03:57,920
exact differentiation techniques that

89
00:03:56,560 --> 00:03:59,680
were much more stable than what we were

90
00:03:57,920 --> 00:04:00,798
doing previously

91
00:03:59,680 --> 00:04:02,400
and finally and this is probably a

92
00:04:00,799 --> 00:04:03,920
really large one is specialized

93
00:04:02,400 --> 00:04:07,360
user-friendly software

94
00:04:03,920 --> 00:04:08,798
so things like tensorflow pytorch jacks

95
00:04:07,360 --> 00:04:10,239
these machine learning frameworks that

96
00:04:08,799 --> 00:04:11,200
people most of you in the room probably

97
00:04:10,239 --> 00:04:13,680
familiar with

98
00:04:11,200 --> 00:04:14,480
these have done a huge amount to

99
00:04:13,680 --> 00:04:18,478
increase

100
00:04:14,480 --> 00:04:21,759
uh the accessibility of machine learning

101
00:04:18,478 --> 00:04:24,800
um yeah so this is kind of just

102
00:04:21,759 --> 00:04:26,160
uh leaning more into that so you can see

103
00:04:24,800 --> 00:04:27,440
that the number of projects using

104
00:04:26,160 --> 00:04:30,720
tensorflow have increased

105
00:04:27,440 --> 00:04:34,639
almost exponentially over the last uh

106
00:04:30,720 --> 00:04:36,080
10 years or so and

107
00:04:34,639 --> 00:04:37,360
as the industry has matured these

108
00:04:36,080 --> 00:04:38,159
software frameworks becoming more

109
00:04:37,360 --> 00:04:39,759
accessible

110
00:04:38,160 --> 00:04:41,199
more easy to use and there's more choice

111
00:04:39,759 --> 00:04:44,080
as well

112
00:04:41,199 --> 00:04:46,080
so tensorflow pi torch jacks the

113
00:04:44,080 --> 00:04:50,400
question is

114
00:04:46,080 --> 00:04:52,240
can we train quantum circuits so

115
00:04:50,400 --> 00:04:53,198
it was a question people had and people

116
00:04:52,240 --> 00:04:54,880
started playing around with in the

117
00:04:53,199 --> 00:04:56,639
quantum computing field

118
00:04:54,880 --> 00:04:57,919
it turns out quantum computing is also

119
00:04:56,639 --> 00:05:01,120
differentiable

120
00:04:57,919 --> 00:05:02,080
so you have a small quantum circuit

121
00:05:01,120 --> 00:05:04,080
which is controlled by

122
00:05:02,080 --> 00:05:05,440
quantum gates these quantum gates take

123
00:05:04,080 --> 00:05:06,960
classical parameters

124
00:05:05,440 --> 00:05:08,479
you perform a quantum computation on the

125
00:05:06,960 --> 00:05:10,400
hardware and then you perform a quantum

126
00:05:08,479 --> 00:05:12,080
measurement at the end and from these

127
00:05:10,400 --> 00:05:14,638
quantum measurements you can compute

128
00:05:12,080 --> 00:05:16,240
uh measurement statistics so you can

129
00:05:14,639 --> 00:05:18,720
measure certain quantum observables and

130
00:05:16,240 --> 00:05:20,400
take expectation values or variances

131
00:05:18,720 --> 00:05:22,000
and these things are deterministic if

132
00:05:20,400 --> 00:05:24,638
you put in the same parameters to this

133
00:05:22,000 --> 00:05:26,320
quantum circuit you get the same output

134
00:05:24,639 --> 00:05:28,240
so it turns out that these quantum

135
00:05:26,320 --> 00:05:29,759
circuits are differentiable

136
00:05:28,240 --> 00:05:31,520
the outputs depend smoothly on gate

137
00:05:29,759 --> 00:05:33,039
parameters

138
00:05:31,520 --> 00:05:38,320
can we practically train quantum

139
00:05:33,039 --> 00:05:40,719
circuits though

140
00:05:38,320 --> 00:05:41,759
it turns out we can and quantum machine

141
00:05:40,720 --> 00:05:43,520
learning methods

142
00:05:41,759 --> 00:05:45,280
called variational quantum algorithms

143
00:05:43,520 --> 00:05:48,560
have become one of the main near-term

144
00:05:45,280 --> 00:05:52,559
quantum algorithms for near-term devices

145
00:05:48,560 --> 00:05:54,639
so this happened around 2011 2012 2013

146
00:05:52,560 --> 00:05:55,680
um idea started forming in the quantum

147
00:05:54,639 --> 00:05:57,199
community things like

148
00:05:55,680 --> 00:05:59,199
the variational quantum eigensolver

149
00:05:57,199 --> 00:06:00,720
which was a really neat idea at the time

150
00:05:59,199 --> 00:06:02,400
saying that these quant variational

151
00:06:00,720 --> 00:06:04,080
quantum devices they give us access to

152
00:06:02,400 --> 00:06:05,440
this enormous state space that we can't

153
00:06:04,080 --> 00:06:06,880
normally access with classical

154
00:06:05,440 --> 00:06:09,199
techniques

155
00:06:06,880 --> 00:06:11,039
can we use can we train the classical

156
00:06:09,199 --> 00:06:14,400
parameters of these gates

157
00:06:11,039 --> 00:06:16,240
to generate a state that outputs a

158
00:06:14,400 --> 00:06:18,638
measurement statistic that we are

159
00:06:16,240 --> 00:06:20,880
looking for that we want to find

160
00:06:18,639 --> 00:06:22,160
so this is kind of uh this variational

161
00:06:20,880 --> 00:06:23,440
quantum algorithm is described on the

162
00:06:22,160 --> 00:06:25,520
right hand side here

163
00:06:23,440 --> 00:06:27,199
so we have a quantum sub routine this is

164
00:06:25,520 --> 00:06:29,039
a quantum black box we just have quantum

165
00:06:27,199 --> 00:06:31,440
gates on our quantum hardware

166
00:06:29,039 --> 00:06:32,960
we prepare a quantum state and we have

167
00:06:31,440 --> 00:06:35,520
some circuit parameters that drive these

168
00:06:32,960 --> 00:06:37,840
quantum gates

169
00:06:35,520 --> 00:06:39,520
um we run the quantum circuit we get a

170
00:06:37,840 --> 00:06:41,119
measurement output

171
00:06:39,520 --> 00:06:43,039
and then we perform some classical

172
00:06:41,120 --> 00:06:45,120
processing and classical optimization

173
00:06:43,039 --> 00:06:46,159
so we use techniques such as uh

174
00:06:45,120 --> 00:06:49,599
optimization

175
00:06:46,160 --> 00:06:50,960
and um gradient free optimization to

176
00:06:49,599 --> 00:06:54,240
update these parameters via some

177
00:06:50,960 --> 00:06:54,239
classical optimization loop

178
00:06:54,319 --> 00:06:57,919
so this was first uh these two

179
00:06:56,160 --> 00:06:59,680
algorithms mentioned up here vqe and

180
00:06:57,919 --> 00:07:00,479
qaoa these were sort of the first two

181
00:06:59,680 --> 00:07:02,319
major

182
00:07:00,479 --> 00:07:04,960
variational quantum algorithms but since

183
00:07:02,319 --> 00:07:06,639
then the field's really grown

184
00:07:04,960 --> 00:07:09,359
and there have been natural extensions

185
00:07:06,639 --> 00:07:10,880
to other circumstances that we've seen

186
00:07:09,360 --> 00:07:13,039
from standard machine learning so things

187
00:07:10,880 --> 00:07:14,800
like uh quantum classifiers can we

188
00:07:13,039 --> 00:07:16,560
distinguish different quantum states

189
00:07:14,800 --> 00:07:18,400
by using the sort of variational quantum

190
00:07:16,560 --> 00:07:20,160
algorithm

191
00:07:18,400 --> 00:07:21,758
nowadays there are many many proposals

192
00:07:20,160 --> 00:07:23,039
just because of how well the scales to

193
00:07:21,759 --> 00:07:24,560
noisy devices

194
00:07:23,039 --> 00:07:26,159
we don't have fault tolerance yet in

195
00:07:24,560 --> 00:07:29,199
quantum computing

196
00:07:26,160 --> 00:07:31,199
um that these algorithms seem to work so

197
00:07:29,199 --> 00:07:33,599
far very very well on the noisy devices

198
00:07:31,199 --> 00:07:33,599
that we have

199
00:07:34,160 --> 00:07:37,759
so what we want to do is we want to be

200
00:07:35,759 --> 00:07:39,360
able to train uh

201
00:07:37,759 --> 00:07:40,639
quantum variational quantum algorithms

202
00:07:39,360 --> 00:07:41,759
but we want to do them using some of the

203
00:07:40,639 --> 00:07:43,039
workhorse algorithms that have been

204
00:07:41,759 --> 00:07:46,479
developed over the last 10

205
00:07:43,039 --> 00:07:48,240
20 30 years of machine learning

206
00:07:46,479 --> 00:07:50,318
so one technique we have doing this is

207
00:07:48,240 --> 00:07:53,520
called the parameter shift method

208
00:07:50,319 --> 00:07:56,319
so initially we were using gradient free

209
00:07:53,520 --> 00:07:58,318
methods to train quantum circuits

210
00:07:56,319 --> 00:07:59,840
this shifted a bit and we started using

211
00:07:58,319 --> 00:08:01,680
something called the parameter shift

212
00:07:59,840 --> 00:08:02,799
method so this is an analytic method of

213
00:08:01,680 --> 00:08:05,039
computing

214
00:08:02,800 --> 00:08:06,479
gradients coming out of quantum devices

215
00:08:05,039 --> 00:08:08,318
so the main insight here is we're able

216
00:08:06,479 --> 00:08:10,000
to use the same quantum hardware

217
00:08:08,319 --> 00:08:12,560
to evaluate its own gradients and this

218
00:08:10,000 --> 00:08:13,759
is a significant insight

219
00:08:12,560 --> 00:08:15,280
because if we weren't able to do this

220
00:08:13,759 --> 00:08:16,960
we'd have to classically simulate the

221
00:08:15,280 --> 00:08:18,400
circuit to get the gradients

222
00:08:16,960 --> 00:08:21,599
and that's intractable as the size of

223
00:08:18,400 --> 00:08:23,039
the quantum devices we have increases

224
00:08:21,599 --> 00:08:24,400
you'd no longer be able to classically

225
00:08:23,039 --> 00:08:26,080
compute the gradient to perform this

226
00:08:24,400 --> 00:08:27,520
classical optimization loop so you're

227
00:08:26,080 --> 00:08:30,560
constrained to gradient free

228
00:08:27,520 --> 00:08:30,560
optimization techniques

229
00:08:31,120 --> 00:08:36,240
so this is just a

230
00:08:34,159 --> 00:08:38,000
very short diagram that sort of

231
00:08:36,240 --> 00:08:40,399
demonstrates this parameter shift

232
00:08:38,000 --> 00:08:42,719
method so on the left we have our

233
00:08:40,399 --> 00:08:43,919
variational quantum circuit

234
00:08:42,719 --> 00:08:45,279
it's an arbitrary circuit we have a

235
00:08:43,919 --> 00:08:46,480
couple of gates running on the quantum

236
00:08:45,279 --> 00:08:47,200
device and we have our input gate

237
00:08:46,480 --> 00:08:50,160
parameters

238
00:08:47,200 --> 00:08:51,839
and our output measurement statistics

239
00:08:50,160 --> 00:08:53,120
all the parameter shift rule says is

240
00:08:51,839 --> 00:08:54,880
that if we

241
00:08:53,120 --> 00:08:56,399
take the exact same quantum circuit that

242
00:08:54,880 --> 00:08:58,800
we want to train

243
00:08:56,399 --> 00:08:59,920
we shift the parameters forward by some

244
00:08:58,800 --> 00:09:01,680
value s

245
00:08:59,920 --> 00:09:03,120
backward by some value s and take the

246
00:09:01,680 --> 00:09:04,560
difference uh

247
00:09:03,120 --> 00:09:06,959
the quantum device will actually output

248
00:09:04,560 --> 00:09:10,079
the gradient of the quantum expectation

249
00:09:06,959 --> 00:09:12,560
value at that parameter value

250
00:09:10,080 --> 00:09:14,640
and the crucial thing here is that this

251
00:09:12,560 --> 00:09:17,680
shift is macroscopic

252
00:09:14,640 --> 00:09:19,120
so it's not uh finite differences we're

253
00:09:17,680 --> 00:09:19,839
not doing numerical differentiation

254
00:09:19,120 --> 00:09:22,080
we're doing

255
00:09:19,839 --> 00:09:24,320
analytic differentiation and we're able

256
00:09:22,080 --> 00:09:28,560
to get the output of the

257
00:09:24,320 --> 00:09:30,560
gradient with noisy devices even when

258
00:09:28,560 --> 00:09:33,359
even outside regimes where we might be

259
00:09:30,560 --> 00:09:33,359
drowned by noise

260
00:09:33,920 --> 00:09:37,360
so the quantum variational algorithms

261
00:09:36,080 --> 00:09:38,959
we've had so far have been

262
00:09:37,360 --> 00:09:41,279
quite simple in scope they've been

263
00:09:38,959 --> 00:09:42,800
mostly maybe one or a couple of quantum

264
00:09:41,279 --> 00:09:44,240
devices

265
00:09:42,800 --> 00:09:45,760
and we're getting the output of that

266
00:09:44,240 --> 00:09:47,600
quantum device and we've got that

267
00:09:45,760 --> 00:09:50,800
classical optimization loop directly

268
00:09:47,600 --> 00:09:54,800
training this quantum circuit

269
00:09:50,800 --> 00:09:55,439
so back in july 2018 or so we were

270
00:09:54,800 --> 00:09:57,439
thinking

271
00:09:55,440 --> 00:09:58,720
what if we want to do more than that

272
00:09:57,440 --> 00:10:00,000
whatever we want to do something like

273
00:09:58,720 --> 00:10:01,519
this

274
00:10:00,000 --> 00:10:03,519
so we have an arbitrary computational

275
00:10:01,519 --> 00:10:06,000
graph

276
00:10:03,519 --> 00:10:07,279
on this diagram here the green uh boxes

277
00:10:06,000 --> 00:10:08,720
the green nodes those are quantum

278
00:10:07,279 --> 00:10:10,079
circuits

279
00:10:08,720 --> 00:10:12,480
the yellow nodes are just arbitrary

280
00:10:10,079 --> 00:10:14,160
classical processing

281
00:10:12,480 --> 00:10:16,399
and we want it to be as arbitrary as we

282
00:10:14,160 --> 00:10:16,399
want

283
00:10:17,279 --> 00:10:20,959
why can't we construct something like

284
00:10:18,800 --> 00:10:21,680
this say we have a machine learning

285
00:10:20,959 --> 00:10:23,040
model

286
00:10:21,680 --> 00:10:24,079
involving lots of classical layers we

287
00:10:23,040 --> 00:10:25,279
want to replace a couple of them with

288
00:10:24,079 --> 00:10:27,199
quantum layers

289
00:10:25,279 --> 00:10:28,640
we want to do something like that in

290
00:10:27,200 --> 00:10:32,160
open source quantum software and we want

291
00:10:28,640 --> 00:10:34,319
to see what happens

292
00:10:32,160 --> 00:10:35,839
so that's kind of where we were we knew

293
00:10:34,320 --> 00:10:37,600
what we wanted to do we wanted an open

294
00:10:35,839 --> 00:10:39,600
source framework

295
00:10:37,600 --> 00:10:41,120
that could interface with the accessible

296
00:10:39,600 --> 00:10:43,760
machine learning framework such as pi

297
00:10:41,120 --> 00:10:45,600
torch tensorflow jacks we wanted to

298
00:10:43,760 --> 00:10:47,120
drive real quantum hardware devices

299
00:10:45,600 --> 00:10:48,079
using these techniques for analytic

300
00:10:47,120 --> 00:10:51,279
gradient

301
00:10:48,079 --> 00:10:52,399
computation on real devices and we

302
00:10:51,279 --> 00:10:55,120
wanted it to be

303
00:10:52,399 --> 00:10:55,600
accessible intuitive and allow you to

304
00:10:55,120 --> 00:10:57,360
perform

305
00:10:55,600 --> 00:10:59,360
arbitrary computations mixing classical

306
00:10:57,360 --> 00:11:01,040
and quantum however you want

307
00:10:59,360 --> 00:11:02,640
so we knew what we wanted to do we

308
00:11:01,040 --> 00:11:03,279
weren't really sure what it should look

309
00:11:02,640 --> 00:11:05,920
like

310
00:11:03,279 --> 00:11:07,040
so i for this presentation i actually

311
00:11:05,920 --> 00:11:09,040
scrolled

312
00:11:07,040 --> 00:11:10,480
up quite high into the deep dark depths

313
00:11:09,040 --> 00:11:12,000
of my slack history

314
00:11:10,480 --> 00:11:14,079
to see what we were discussing when it

315
00:11:12,000 --> 00:11:15,279
came to the ui of pennywing

316
00:11:14,079 --> 00:11:16,560
so i've got a couple of the messages

317
00:11:15,279 --> 00:11:17,920
here before i actually show you what the

318
00:11:16,560 --> 00:11:19,680
ui looks like

319
00:11:17,920 --> 00:11:21,519
this is the very first one the code

320
00:11:19,680 --> 00:11:22,880
should look as much as like numpy as

321
00:11:21,519 --> 00:11:24,720
possible

322
00:11:22,880 --> 00:11:25,839
um this was a very popular statement in

323
00:11:24,720 --> 00:11:28,079
our slack channel when we were

324
00:11:25,839 --> 00:11:31,360
discussing the architecture of pennylane

325
00:11:28,079 --> 00:11:31,359
so there was lots of agreement there

326
00:11:31,680 --> 00:11:35,680
we kind of got a bit distracted though

327
00:11:34,320 --> 00:11:37,040
we started looking at context managers

328
00:11:35,680 --> 00:11:39,839
we were like maybe we want to train

329
00:11:37,040 --> 00:11:42,319
things using context managers in python

330
00:11:39,839 --> 00:11:43,760
we spent i'm a bit surprised looking at

331
00:11:42,320 --> 00:11:45,839
slack history but we spent about two to

332
00:11:43,760 --> 00:11:47,200
three days trying to make this work

333
00:11:45,839 --> 00:11:48,560
before realizing it's absolutely

334
00:11:47,200 --> 00:11:50,959
atrocious idea people don't want to

335
00:11:48,560 --> 00:11:53,680
train things using context managers

336
00:11:50,959 --> 00:11:54,399
but hey a quantum node is kind of like a

337
00:11:53,680 --> 00:11:57,279
function

338
00:11:54,399 --> 00:11:57,519
you have input classical parameters you

339
00:11:57,279 --> 00:12:01,040
have

340
00:11:57,519 --> 00:12:02,480
output classical data that's come from

341
00:12:01,040 --> 00:12:04,880
taking measurement statistics of your

342
00:12:02,480 --> 00:12:06,320
quantum device

343
00:12:04,880 --> 00:12:08,800
you want to define it once and then you

344
00:12:06,320 --> 00:12:11,760
want to evaluate it multiple times

345
00:12:08,800 --> 00:12:11,760
to be able to train them

346
00:12:12,000 --> 00:12:16,079
so we start thinking hey quantum devices

347
00:12:15,120 --> 00:12:17,680
in this framework should just be

348
00:12:16,079 --> 00:12:19,040
functions that you call

349
00:12:17,680 --> 00:12:21,359
and compose with your classical

350
00:12:19,040 --> 00:12:22,160
functions so that's kind of where we

351
00:12:21,360 --> 00:12:24,560
ended up

352
00:12:22,160 --> 00:12:25,760
so that's our prototype on the left of

353
00:12:24,560 --> 00:12:26,800
what uh this

354
00:12:25,760 --> 00:12:29,040
quantum machine learning framework

355
00:12:26,800 --> 00:12:32,000
should look like

356
00:12:29,040 --> 00:12:32,959
and that's what we ended up with so in

357
00:12:32,000 --> 00:12:35,200
penny lane

358
00:12:32,959 --> 00:12:36,800
all quantum circuits are functions

359
00:12:35,200 --> 00:12:38,720
within these functions you define your

360
00:12:36,800 --> 00:12:40,240
quantum operations

361
00:12:38,720 --> 00:12:42,240
the function state classical parameters

362
00:12:40,240 --> 00:12:45,200
and the output

363
00:12:42,240 --> 00:12:45,200
measurement statistics

364
00:12:47,600 --> 00:12:52,240
so this is just a quick overview of what

365
00:12:50,880 --> 00:12:54,959
a quantum machine learning task looks

366
00:12:52,240 --> 00:12:54,959
like in pennyway

367
00:12:56,639 --> 00:13:00,800
the main uh things we want to do with

368
00:12:59,279 --> 00:13:02,720
pennylane i've mentioned them slightly

369
00:13:00,800 --> 00:13:05,199
before but just to reiterate it's

370
00:13:02,720 --> 00:13:06,639
open sourced and developed openly so not

371
00:13:05,200 --> 00:13:09,680
only is the code available

372
00:13:06,639 --> 00:13:10,720
on github but we also develop it openly

373
00:13:09,680 --> 00:13:12,479
on github

374
00:13:10,720 --> 00:13:14,399
so i think that's quite important and

375
00:13:12,480 --> 00:13:16,320
that anyone who wants to contribute can

376
00:13:14,399 --> 00:13:18,000
look at our github issues look at our

377
00:13:16,320 --> 00:13:19,519
pull requests and see the roadmap for

378
00:13:18,000 --> 00:13:20,880
where we want to take penny lane

379
00:13:19,519 --> 00:13:22,800
and this goes back all the way to the

380
00:13:20,880 --> 00:13:24,320
very beginning as well if you go back

381
00:13:22,800 --> 00:13:24,880
through the history of the guitar repo

382
00:13:24,320 --> 00:13:27,760
you'll see

383
00:13:24,880 --> 00:13:29,200
all those ideas in terms of the ui and

384
00:13:27,760 --> 00:13:32,079
what we want penny lane to do

385
00:13:29,200 --> 00:13:33,760
as pull requests and issues so we want

386
00:13:32,079 --> 00:13:35,680
pendulum to be functional

387
00:13:33,760 --> 00:13:37,519
pennyway is functional it has a python

388
00:13:35,680 --> 00:13:39,120
interface it's designed to integrate

389
00:13:37,519 --> 00:13:40,560
seamlessly with the machine learning

390
00:13:39,120 --> 00:13:44,560
frameworks that we have right now so

391
00:13:40,560 --> 00:13:44,560
numpy pi torch and tensorflow

392
00:13:45,120 --> 00:13:47,680
you can train the quantum computer the

393
00:13:46,560 --> 00:13:48,880
same way as you can with a neural

394
00:13:47,680 --> 00:13:51,199
network

395
00:13:48,880 --> 00:13:53,199
and crucially it's designed to scale as

396
00:13:51,199 --> 00:13:56,639
quantum computers grow in power

397
00:13:53,199 --> 00:13:59,920
so in penny lane all

398
00:13:56,639 --> 00:14:01,519
quantum functions are executable on the

399
00:13:59,920 --> 00:14:02,000
quantum hardware we have available so we

400
00:14:01,519 --> 00:14:04,560
have quantum

401
00:14:02,000 --> 00:14:06,880
hardware available over the cloud from

402
00:14:04,560 --> 00:14:08,560
rigetti from ibm

403
00:14:06,880 --> 00:14:10,480
there are plugins available to connect

404
00:14:08,560 --> 00:14:12,079
penny lane to that

405
00:14:10,480 --> 00:14:14,160
and by just changing a couple of lines

406
00:14:12,079 --> 00:14:17,839
you can switch what devices you're

407
00:14:14,160 --> 00:14:17,839
running your quantum functions on

408
00:14:18,720 --> 00:14:21,760
so the main abstraction layer in

409
00:14:20,399 --> 00:14:23,519
pennylane is i've discussed this

410
00:14:21,760 --> 00:14:25,920
previously by quantum node

411
00:14:23,519 --> 00:14:27,839
so the quantum node is what encapsulates

412
00:14:25,920 --> 00:14:31,599
your quantum computation

413
00:14:27,839 --> 00:14:33,360
and that's the diagram there on the left

414
00:14:31,600 --> 00:14:35,279
so you have your quantum circuit input

415
00:14:33,360 --> 00:14:37,519
parameters input state preparation

416
00:14:35,279 --> 00:14:39,120
output expectation values or variances

417
00:14:37,519 --> 00:14:41,040
or samples or probabilities these are

418
00:14:39,120 --> 00:14:44,079
all things you can train on analytically

419
00:14:41,040 --> 00:14:46,160
in pennywing so the

420
00:14:44,079 --> 00:14:47,199
goal is that a computation is end-to-end

421
00:14:46,160 --> 00:14:48,719
differentiable

422
00:14:47,199 --> 00:14:50,240
we want you to be able to use uh

423
00:14:48,720 --> 00:14:51,279
workhorse algorithms like gradient

424
00:14:50,240 --> 00:14:52,959
descent

425
00:14:51,279 --> 00:14:54,480
across your quantum classical hybrid

426
00:14:52,959 --> 00:14:56,880
model

427
00:14:54,480 --> 00:14:58,079
so all quantum nodes in penguin will

428
00:14:56,880 --> 00:14:59,760
always be

429
00:14:58,079 --> 00:15:01,199
differentiable end to end you can

430
00:14:59,760 --> 00:15:03,199
combine it with your other models

431
00:15:01,199 --> 00:15:05,120
in machine classical machine learning

432
00:15:03,199 --> 00:15:05,519
and the entire computational graph will

433
00:15:05,120 --> 00:15:12,160
be

434
00:15:05,519 --> 00:15:15,839
enter and differentiable

435
00:15:12,160 --> 00:15:16,639
so how do we do this so with pennylane

436
00:15:15,839 --> 00:15:18,639
we support

437
00:15:16,639 --> 00:15:21,120
uh three main interfaces we support

438
00:15:18,639 --> 00:15:23,040
numpy pytorch and tensorflow

439
00:15:21,120 --> 00:15:24,800
and the way this works is we simply we

440
00:15:23,040 --> 00:15:27,519
accept the tensor objects from

441
00:15:24,800 --> 00:15:29,120
these three frameworks so numpy if

442
00:15:27,519 --> 00:15:30,000
you're using autograd and arraybox the

443
00:15:29,120 --> 00:15:32,560
other two

444
00:15:30,000 --> 00:15:34,399
a torch sensor or a tensorflow tensor

445
00:15:32,560 --> 00:15:35,119
when it comes time to evaluate the

446
00:15:34,399 --> 00:15:37,040
quantum

447
00:15:35,120 --> 00:15:38,399
portion of your computation what we do

448
00:15:37,040 --> 00:15:40,079
is we just unwrap and extract the

449
00:15:38,399 --> 00:15:44,000
numerical value

450
00:15:40,079 --> 00:15:44,638
we evaluate it we use our formulas and

451
00:15:44,000 --> 00:15:47,040
internal

452
00:15:44,639 --> 00:15:48,800
architecture to evaluate the gradient

453
00:15:47,040 --> 00:15:50,399
convert it back to a tensor object and

454
00:15:48,800 --> 00:15:53,120
as well pass on the gradient information

455
00:15:50,399 --> 00:15:54,560
to the machine learning framework

456
00:15:53,120 --> 00:15:56,560
so this is how we're able to make the

457
00:15:54,560 --> 00:15:58,000
entire thing end-to-end differentiable

458
00:15:56,560 --> 00:16:00,079
even though we're using a mixture of

459
00:15:58,000 --> 00:16:02,480
quantum and classical hardware

460
00:16:00,079 --> 00:16:03,680
so you can almost think of the uh

461
00:16:02,480 --> 00:16:06,160
quantum hardware

462
00:16:03,680 --> 00:16:09,839
as accelerators in a sense similar to

463
00:16:06,160 --> 00:16:09,839
gpus and tpus

464
00:16:12,720 --> 00:16:16,320
pennylane is framework and hardware

465
00:16:14,160 --> 00:16:18,160
agnostic so i mentioned

466
00:16:16,320 --> 00:16:19,680
previously that one of our framework one

467
00:16:18,160 --> 00:16:21,120
of our hardware frameworks is strawberry

468
00:16:19,680 --> 00:16:22,638
field this is what we use to connect to

469
00:16:21,120 --> 00:16:24,320
our photonic hardware

470
00:16:22,639 --> 00:16:25,920
that we're developing in our photonic

471
00:16:24,320 --> 00:16:28,639
labs

472
00:16:25,920 --> 00:16:29,759
in addition uh we want to be able to mix

473
00:16:28,639 --> 00:16:31,279
and match so

474
00:16:29,759 --> 00:16:33,279
as we're in the noisy era there are

475
00:16:31,279 --> 00:16:34,639
advantages to our photonic technology

476
00:16:33,279 --> 00:16:36,399
there are also disadvantages to our

477
00:16:34,639 --> 00:16:38,560
photos technology

478
00:16:36,399 --> 00:16:40,000
uh this is similar to superconducting

479
00:16:38,560 --> 00:16:41,518
qubits this is similar to d-waves

480
00:16:40,000 --> 00:16:44,720
quantum annealer

481
00:16:41,519 --> 00:16:46,160
all these noisy uh devices have their

482
00:16:44,720 --> 00:16:47,759
pros and cons they have applications

483
00:16:46,160 --> 00:16:49,120
they're best suited for and applications

484
00:16:47,759 --> 00:16:50,399
that maybe are better in a different

485
00:16:49,120 --> 00:16:52,720
architecture

486
00:16:50,399 --> 00:16:54,079
so we want to be able to combine these

487
00:16:52,720 --> 00:16:57,199
different uh frameworks

488
00:16:54,079 --> 00:16:58,638
these different quantum devices um

489
00:16:57,199 --> 00:17:00,800
so you can take any of these there are

490
00:16:58,639 --> 00:17:02,560
plugins available for kiskit forest

491
00:17:00,800 --> 00:17:04,959
uh circ and strawberry fields and

492
00:17:02,560 --> 00:17:08,958
combine them with either tensorflow

493
00:17:04,959 --> 00:17:08,959
pure numpy for scikit-learn or pytorch

494
00:17:09,919 --> 00:17:14,400
the cool thing is you can even uh in one

495
00:17:12,880 --> 00:17:15,199
computation have devices from different

496
00:17:14,400 --> 00:17:18,720
providers

497
00:17:15,199 --> 00:17:20,079
so you can have a for instance

498
00:17:18,720 --> 00:17:21,839
this circuit down here on the bottom

499
00:17:20,079 --> 00:17:23,198
left that's a photonic circuit it's

500
00:17:21,839 --> 00:17:24,720
going into a

501
00:17:23,199 --> 00:17:27,760
classical processing node doing the

502
00:17:24,720 --> 00:17:27,760
cosine and square root

503
00:17:29,200 --> 00:17:33,039
and that's going up to a qubit circuit

504
00:17:31,120 --> 00:17:34,799
so you can take a provider from righty

505
00:17:33,039 --> 00:17:37,200
uh take a device from right to your ibm

506
00:17:34,799 --> 00:17:40,799
combine this with a photonic device

507
00:17:37,200 --> 00:17:40,799
train over the entire architecture

508
00:17:41,360 --> 00:17:44,719
so for those familiar with machine

509
00:17:43,200 --> 00:17:45,360
learning but maybe not quantum machine

510
00:17:44,720 --> 00:17:46,799
learning

511
00:17:45,360 --> 00:17:49,039
here's a very quick example of what you

512
00:17:46,799 --> 00:17:51,918
can do so we have a quantum circuit

513
00:17:49,039 --> 00:17:53,280
we're rotating

514
00:17:51,919 --> 00:17:55,679
we have one qubit in this quantum

515
00:17:53,280 --> 00:17:58,960
circuit we're rotating the

516
00:17:55,679 --> 00:18:00,960
qubit around the x-axis

517
00:17:58,960 --> 00:18:02,480
controlled by classical parameter doing

518
00:18:00,960 --> 00:18:03,520
the same thing around the y-axis

519
00:18:02,480 --> 00:18:05,679
controlled by a nascar

520
00:18:03,520 --> 00:18:06,960
another classical parameter and just

521
00:18:05,679 --> 00:18:09,600
measuring where this

522
00:18:06,960 --> 00:18:10,400
uh the state the cuber ends up in

523
00:18:09,600 --> 00:18:13,600
measuring the z

524
00:18:10,400 --> 00:18:15,440
the polyz expectation value

525
00:18:13,600 --> 00:18:17,918
so this is pretty simple in penny lane

526
00:18:15,440 --> 00:18:19,600
you just uh

527
00:18:17,919 --> 00:18:21,440
create your device so the device is just

528
00:18:19,600 --> 00:18:24,000
an abstraction for the qpu that you want

529
00:18:21,440 --> 00:18:27,280
to run and train your computation on

530
00:18:24,000 --> 00:18:30,720
in this case i'm using a qp

531
00:18:27,280 --> 00:18:32,240
device from rougetti first you define

532
00:18:30,720 --> 00:18:34,480
your quantum function so this

533
00:18:32,240 --> 00:18:35,840
uh this is a keynote it corresponds

534
00:18:34,480 --> 00:18:36,960
exactly to that quantum circuit in the

535
00:18:35,840 --> 00:18:38,959
previous slide

536
00:18:36,960 --> 00:18:41,039
i've got my two rotations i'm returning

537
00:18:38,960 --> 00:18:44,240
expectation value of the polyz

538
00:18:41,039 --> 00:18:46,160
measurement and the last part is just

539
00:18:44,240 --> 00:18:47,360
pure classical optimization exactly like

540
00:18:46,160 --> 00:18:49,039
you'd see in the pi torch or a

541
00:18:47,360 --> 00:18:50,479
tensorflow

542
00:18:49,039 --> 00:18:52,080
machine learning example in this case

543
00:18:50,480 --> 00:18:54,640
i'm just using pi torch

544
00:18:52,080 --> 00:18:56,480
to optimize so i'm defining my cost

545
00:18:54,640 --> 00:18:58,960
function which takes

546
00:18:56,480 --> 00:19:00,320
in the uh the q node and i'm just

547
00:18:58,960 --> 00:19:03,120
optimizing that to reduce

548
00:19:00,320 --> 00:19:05,280
the expectation value of that keynote so

549
00:19:03,120 --> 00:19:07,918
the key thing i guess is that

550
00:19:05,280 --> 00:19:09,200
the pennylane keynote is it makes

551
00:19:07,919 --> 00:19:10,720
pytorch sensor

552
00:19:09,200 --> 00:19:12,240
quantum aware in this example you can

553
00:19:10,720 --> 00:19:13,840
take a q node

554
00:19:12,240 --> 00:19:15,280
and use it within pi torsion from pi

555
00:19:13,840 --> 00:19:16,559
torch's point of view or tensorflow's

556
00:19:15,280 --> 00:19:18,080
point of view

557
00:19:16,559 --> 00:19:20,399
this function is a classical function

558
00:19:18,080 --> 00:19:23,120
that's end-to-end differentiable

559
00:19:20,400 --> 00:19:24,000
so these examples are available on our

560
00:19:23,120 --> 00:19:26,879
website

561
00:19:24,000 --> 00:19:28,559
as a tutorials just on the right hand

562
00:19:26,880 --> 00:19:30,480
side is the animation of this

563
00:19:28,559 --> 00:19:31,678
short script actually running on the qpu

564
00:19:30,480 --> 00:19:34,640
so we're training

565
00:19:31,679 --> 00:19:34,640
the qubit to

566
00:19:35,440 --> 00:19:38,559
to enter the state that's defined by the

567
00:19:37,039 --> 00:19:40,799
cross on the block sphere

568
00:19:38,559 --> 00:19:43,039
and you can see the qubit being trained

569
00:19:40,799 --> 00:19:45,918
and approaching

570
00:19:43,039 --> 00:19:45,919
the required state

571
00:19:46,240 --> 00:19:50,640
so over the last year we've worked hard

572
00:19:48,080 --> 00:19:53,600
on making pennylane more accessible

573
00:19:50,640 --> 00:19:54,640
so we want it to be a higher level

574
00:19:53,600 --> 00:19:55,360
abstraction a lot of the other

575
00:19:54,640 --> 00:19:58,559
frameworks

576
00:19:55,360 --> 00:20:00,559
are they're object oriented they're low

577
00:19:58,559 --> 00:20:03,600
level and they're very hardware focused

578
00:20:00,559 --> 00:20:05,678
so you can access things that may be

579
00:20:03,600 --> 00:20:07,120
specific to a particular hardware

580
00:20:05,679 --> 00:20:08,720
and things that we want to do sometimes

581
00:20:07,120 --> 00:20:10,158
so sometimes

582
00:20:08,720 --> 00:20:11,520
we need to think about the underlying

583
00:20:10,159 --> 00:20:12,240
hardware that we're running our quantum

584
00:20:11,520 --> 00:20:15,679
programming

585
00:20:12,240 --> 00:20:17,840
on other times we don't we want to think

586
00:20:15,679 --> 00:20:20,640
in a higher level abstraction

587
00:20:17,840 --> 00:20:21,918
we want to maybe do something assuming

588
00:20:20,640 --> 00:20:24,159
that

589
00:20:21,919 --> 00:20:26,000
the hardware will do the best it can

590
00:20:24,159 --> 00:20:27,120
based on the underlying framework

591
00:20:26,000 --> 00:20:28,720
so that's kind of where we're

592
00:20:27,120 --> 00:20:31,199
positioning penny lane it's sort of the

593
00:20:28,720 --> 00:20:31,200
numpy

594
00:20:31,520 --> 00:20:37,280
interface wrapping the pack underneath

595
00:20:35,760 --> 00:20:38,879
so a couple of things we've added to the

596
00:20:37,280 --> 00:20:40,480
library of the last year to make it more

597
00:20:38,880 --> 00:20:41,600
accessible to provide an easier way of

598
00:20:40,480 --> 00:20:42,880
constructing your quantum machine

599
00:20:41,600 --> 00:20:44,719
learning models

600
00:20:42,880 --> 00:20:46,320
a built-in library of circuit and stats

601
00:20:44,720 --> 00:20:48,000
from the qml literature

602
00:20:46,320 --> 00:20:49,918
so it's kind of an unanswered question

603
00:20:48,000 --> 00:20:52,240
about what quantum circuits

604
00:20:49,919 --> 00:20:53,600
and sats are easiest to train and best

605
00:20:52,240 --> 00:20:55,360
for particular

606
00:20:53,600 --> 00:20:56,799
machine learning models and quantum

607
00:20:55,360 --> 00:20:59,520
algorithms

608
00:20:56,799 --> 00:21:01,280
um papers are continuously coming out

609
00:20:59,520 --> 00:21:02,320
with new ansats that provide benefits

610
00:21:01,280 --> 00:21:04,559
for different

611
00:21:02,320 --> 00:21:06,080
different use cases so these are easily

612
00:21:04,559 --> 00:21:08,399
accessible in pennylane

613
00:21:06,080 --> 00:21:10,080
we have a qml.templates library and you

614
00:21:08,400 --> 00:21:12,320
can just include these in your quantum

615
00:21:10,080 --> 00:21:12,320
node

616
00:21:14,000 --> 00:21:17,120
we heard feedback from a lot of users

617
00:21:15,760 --> 00:21:19,440
that said that they had

618
00:21:17,120 --> 00:21:21,840
codes already using kiss kit or already

619
00:21:19,440 --> 00:21:23,679
using pi core that they wanted to

620
00:21:21,840 --> 00:21:25,360
um interface with their machine learning

621
00:21:23,679 --> 00:21:26,799
models but

622
00:21:25,360 --> 00:21:28,639
they didn't want to have to rewrite it

623
00:21:26,799 --> 00:21:30,480
using penny lane's interface

624
00:21:28,640 --> 00:21:31,840
so we added the ability to import from

625
00:21:30,480 --> 00:21:33,760
these quantum libraries

626
00:21:31,840 --> 00:21:36,559
so you can import your code from kiskits

627
00:21:33,760 --> 00:21:38,799
from chasm from pyquil from quill

628
00:21:36,559 --> 00:21:40,399
um it runs the same circuit it gets

629
00:21:38,799 --> 00:21:42,080
converted into a q node and it just

630
00:21:40,400 --> 00:21:44,640
becomes end to end differentiable

631
00:21:42,080 --> 00:21:46,080
so you can take your existing code and

632
00:21:44,640 --> 00:21:49,039
then interface it with pi torch or

633
00:21:46,080 --> 00:21:49,039
tensorflow straight away

634
00:21:50,400 --> 00:21:54,400
so automatically adds uh analytic

635
00:21:52,320 --> 00:21:55,600
differentiation to existing high quality

636
00:21:54,400 --> 00:21:57,919
frameworks

637
00:21:55,600 --> 00:21:59,280
cool thing is you can take these

638
00:21:57,919 --> 00:22:01,600
programs loaded from

639
00:21:59,280 --> 00:22:03,039
rigetti forest convert it into qnode and

640
00:22:01,600 --> 00:22:05,520
then deploy it another quantum hardware

641
00:22:03,039 --> 00:22:05,520
if you'd like

642
00:22:06,000 --> 00:22:09,120
we're also working on adding

643
00:22:07,600 --> 00:22:10,719
optimization that's

644
00:22:09,120 --> 00:22:12,639
maybe better suited for a quantum

645
00:22:10,720 --> 00:22:15,919
landscape so things like

646
00:22:12,640 --> 00:22:18,960
uh the quantum natural gradient

647
00:22:15,919 --> 00:22:20,080
so taking into account what the cost

648
00:22:18,960 --> 00:22:22,720
landscape of your

649
00:22:20,080 --> 00:22:23,280
variational circuit looks like in order

650
00:22:22,720 --> 00:22:25,600
to

651
00:22:23,280 --> 00:22:26,960
ensure better convergence so this is

652
00:22:25,600 --> 00:22:28,719
internal research that we've been

653
00:22:26,960 --> 00:22:31,919
working on at xanadu

654
00:22:28,720 --> 00:22:33,600
this is just a outline of what this

655
00:22:31,919 --> 00:22:34,880
algorithm looks like it's a kind of a

656
00:22:33,600 --> 00:22:36,559
quantum equivalent to the natural

657
00:22:34,880 --> 00:22:38,880
gradient if you've heard of that

658
00:22:36,559 --> 00:22:40,158
which takes into account classical

659
00:22:38,880 --> 00:22:41,520
euclidean space when you're doing

660
00:22:40,159 --> 00:22:42,960
gradient descent

661
00:22:41,520 --> 00:22:44,559
quantum natural gradient takes into

662
00:22:42,960 --> 00:22:46,000
account the quantum geometry when doing

663
00:22:44,559 --> 00:22:48,158
gradient descent

664
00:22:46,000 --> 00:22:49,600
and in general can lead to much better

665
00:22:48,159 --> 00:22:52,159
convergence

666
00:22:49,600 --> 00:22:53,520
with variational algorithms so this is

667
00:22:52,159 --> 00:22:56,559
just a plot comparing

668
00:22:53,520 --> 00:22:59,440
for a couple of circuits uh

669
00:22:56,559 --> 00:23:00,960
gradient descent in blue versus atom in

670
00:22:59,440 --> 00:23:01,520
green and then quantum natural gradient

671
00:23:00,960 --> 00:23:05,200
descent

672
00:23:01,520 --> 00:23:05,200
in red and black for different

673
00:23:06,840 --> 00:23:10,959
approximations

674
00:23:08,880 --> 00:23:13,280
um we've also heard feedback that people

675
00:23:10,960 --> 00:23:15,200
want their simulations to be faster

676
00:23:13,280 --> 00:23:17,200
so with penny lane we took the approach

677
00:23:15,200 --> 00:23:19,520
that it was hardware first

678
00:23:17,200 --> 00:23:22,240
if you defined a q node it should always

679
00:23:19,520 --> 00:23:24,240
be executable on hardware

680
00:23:22,240 --> 00:23:25,840
but there are shortcuts we can take with

681
00:23:24,240 --> 00:23:28,000
classical simulators one of those is if

682
00:23:25,840 --> 00:23:30,240
we write the simulator using tensorflow

683
00:23:28,000 --> 00:23:31,919
or pi torch for example we can do

684
00:23:30,240 --> 00:23:32,400
classical back propagation techniques

685
00:23:31,919 --> 00:23:35,200
that

686
00:23:32,400 --> 00:23:36,000
scale better in certain regimes so penny

687
00:23:35,200 --> 00:23:39,120
lane now includes

688
00:23:36,000 --> 00:23:40,000
a tencent network simulator uh built

689
00:23:39,120 --> 00:23:43,039
using google's

690
00:23:40,000 --> 00:23:44,720
tensor network software and tensorflow

691
00:23:43,039 --> 00:23:46,400
uh so this uses classical back

692
00:23:44,720 --> 00:23:47,760
propagation inside the keynote

693
00:23:46,400 --> 00:23:49,360
to compute the gradient which if you

694
00:23:47,760 --> 00:23:50,960
have a lot of parameters can be faster

695
00:23:49,360 --> 00:23:53,760
so enables

696
00:23:50,960 --> 00:23:55,440
routines where maybe you're training

697
00:23:53,760 --> 00:23:56,400
your quantum circuit on a simulator you

698
00:23:55,440 --> 00:23:58,400
get your

699
00:23:56,400 --> 00:24:04,880
trained parameters and then you deploy

700
00:23:58,400 --> 00:24:06,960
using test data on the quantum device

701
00:24:04,880 --> 00:24:08,480
we also have a new quantum chemistry

702
00:24:06,960 --> 00:24:09,600
plug-in so

703
00:24:08,480 --> 00:24:12,320
right at the beginning of the talk i

704
00:24:09,600 --> 00:24:14,399
mentioned that one of the

705
00:24:12,320 --> 00:24:16,240
first variational quantum algorithms to

706
00:24:14,400 --> 00:24:19,840
really create this

707
00:24:16,240 --> 00:24:21,360
qml second 2.0 wave was the vqe

708
00:24:19,840 --> 00:24:23,039
algorithm the variation of quantum eigen

709
00:24:21,360 --> 00:24:24,799
solver

710
00:24:23,039 --> 00:24:27,440
so this algorithm allows us to use

711
00:24:24,799 --> 00:24:30,799
quantum computers to

712
00:24:27,440 --> 00:24:33,679
probe the electronic molecular

713
00:24:30,799 --> 00:24:35,279
energies of various molecules so penny

714
00:24:33,679 --> 00:24:37,600
lane now interfaces with another

715
00:24:35,279 --> 00:24:40,640
open source quantum library open fermion

716
00:24:37,600 --> 00:24:43,840
by the quantum team at google

717
00:24:40,640 --> 00:24:45,679
so using open fermion you can

718
00:24:43,840 --> 00:24:46,879
convert any electronic molecular

719
00:24:45,679 --> 00:24:49,679
structure

720
00:24:46,880 --> 00:24:51,120
to a quantum set of quantum observables

721
00:24:49,679 --> 00:24:51,840
and these are now impossible in penny

722
00:24:51,120 --> 00:24:55,600
lane

723
00:24:51,840 --> 00:24:55,600
for training on quantum hardware

724
00:24:57,760 --> 00:25:00,960
so the other thing we really want to do

725
00:24:59,120 --> 00:25:02,399
is we i mentioned at the beginning we

726
00:25:00,960 --> 00:25:04,159
want to make it accessible and part of

727
00:25:02,400 --> 00:25:05,360
that is the ui and what frameworks we

728
00:25:04,159 --> 00:25:07,200
interface with

729
00:25:05,360 --> 00:25:08,639
another part of it is having good

730
00:25:07,200 --> 00:25:12,080
documentation

731
00:25:08,640 --> 00:25:13,760
uh stable api and a

732
00:25:12,080 --> 00:25:15,840
collection of tutorials for people to

733
00:25:13,760 --> 00:25:18,320
explore how to use penny lane

734
00:25:15,840 --> 00:25:19,439
so if you go to pennylane.ai qml please

735
00:25:18,320 --> 00:25:21,120
check it out

736
00:25:19,440 --> 00:25:23,120
where we have a constantly growing

737
00:25:21,120 --> 00:25:24,320
collection of quantum machine learning

738
00:25:23,120 --> 00:25:25,760
tutorials

739
00:25:24,320 --> 00:25:27,279
using pennylane across a mixture of

740
00:25:25,760 --> 00:25:28,320
different hardware devices hardware

741
00:25:27,279 --> 00:25:30,960
frameworks

742
00:25:28,320 --> 00:25:32,080
and also machine learning frameworks and

743
00:25:30,960 --> 00:25:33,520
these are things from

744
00:25:32,080 --> 00:25:35,840
just looking at variational quantum

745
00:25:33,520 --> 00:25:38,639
eigensolver looking at

746
00:25:35,840 --> 00:25:40,639
quantum state preparation to things such

747
00:25:38,640 --> 00:25:42,240
as quantum transfer learning

748
00:25:40,640 --> 00:25:43,919
doubly stochastic gradient descent in

749
00:25:42,240 --> 00:25:47,039
quantum systems

750
00:25:43,919 --> 00:25:47,840
quantum structure learning etc um so

751
00:25:47,039 --> 00:25:50,240
please have a look

752
00:25:47,840 --> 00:25:52,080
this not only is penny lane completely

753
00:25:50,240 --> 00:25:54,880
open source and developed in the open

754
00:25:52,080 --> 00:25:57,439
but the pennylane.ami dot ai slash qml

755
00:25:54,880 --> 00:25:59,919
website is open source and on github

756
00:25:57,440 --> 00:26:00,559
so any of these tutorials that are

757
00:25:59,919 --> 00:26:03,039
available there

758
00:26:00,559 --> 00:26:04,799
can be downloaded can be edited with

759
00:26:03,039 --> 00:26:06,960
pull requests and we also accept pull

760
00:26:04,799 --> 00:26:08,960
requests for new tutorials

761
00:26:06,960 --> 00:26:10,400
um we also have a twitter account

762
00:26:08,960 --> 00:26:13,600
pennylane ai

763
00:26:10,400 --> 00:26:15,120
so please follow and we'll we try every

764
00:26:13,600 --> 00:26:17,840
week or every two weeks to

765
00:26:15,120 --> 00:26:18,399
tweet out new qml results we see as well

766
00:26:17,840 --> 00:26:22,000
as

767
00:26:18,400 --> 00:26:22,000
new tutorials that are available using

768
00:26:22,840 --> 00:26:26,879
pennylane

769
00:26:24,720 --> 00:26:28,799
so we wanted to see where we could go

770
00:26:26,880 --> 00:26:30,559
with penny lane once we had this base ui

771
00:26:28,799 --> 00:26:33,600
this base abstraction

772
00:26:30,559 --> 00:26:35,678
we noticed that the core abstraction

773
00:26:33,600 --> 00:26:37,360
penny lane is this q node concept

774
00:26:35,679 --> 00:26:38,799
so a quantum device corresponds to a

775
00:26:37,360 --> 00:26:40,240
quantum function in python

776
00:26:38,799 --> 00:26:43,039
that is an end to end differentiable and

777
00:26:40,240 --> 00:26:45,200
can be used with any framework

778
00:26:43,039 --> 00:26:46,960
but we started to notice that it wasn't

779
00:26:45,200 --> 00:26:48,080
enough for us as internal researchers as

780
00:26:46,960 --> 00:26:51,360
xaml do

781
00:26:48,080 --> 00:26:52,559
we needed the ability to process these q

782
00:26:51,360 --> 00:26:54,479
nodes

783
00:26:52,559 --> 00:26:56,799
at a higher level abstraction we we

784
00:26:54,480 --> 00:26:59,919
found that we would have

785
00:26:56,799 --> 00:27:01,279
groups of q nodes and we wanted to work

786
00:26:59,919 --> 00:27:02,080
with these groups of q nodes without

787
00:27:01,279 --> 00:27:04,799
thinking about

788
00:27:02,080 --> 00:27:06,399
the low level quantum circuit within

789
00:27:04,799 --> 00:27:07,440
each q node

790
00:27:06,400 --> 00:27:09,440
and this is something that happened when

791
00:27:07,440 --> 00:27:12,080
we were thinking about vqe

792
00:27:09,440 --> 00:27:13,279
essentially the vqe module was two

793
00:27:12,080 --> 00:27:15,279
things it was

794
00:27:13,279 --> 00:27:16,640
a mapping operation we were taking a

795
00:27:15,279 --> 00:27:19,120
quantum circuit

796
00:27:16,640 --> 00:27:20,640
and mapping it across different return

797
00:27:19,120 --> 00:27:22,479
values different expectation values we

798
00:27:20,640 --> 00:27:25,600
wanted to measure

799
00:27:22,480 --> 00:27:27,200
and then a reduction operation once we

800
00:27:25,600 --> 00:27:28,080
evaluated all these different quantum

801
00:27:27,200 --> 00:27:30,080
nodes

802
00:27:28,080 --> 00:27:31,678
we wanted to just take the dot product

803
00:27:30,080 --> 00:27:33,520
between these quantum nodes

804
00:27:31,679 --> 00:27:36,320
and coefficients that defined the

805
00:27:33,520 --> 00:27:38,960
problem we were solving

806
00:27:36,320 --> 00:27:40,480
so we wanted a better abstraction for

807
00:27:38,960 --> 00:27:42,559
working with groups of keynotes that was

808
00:27:40,480 --> 00:27:44,000
algorithm independent

809
00:27:42,559 --> 00:27:46,240
we also noticed that we were getting to

810
00:27:44,000 --> 00:27:46,799
the point where multiple qp's were

811
00:27:46,240 --> 00:27:49,440
available

812
00:27:46,799 --> 00:27:50,639
right now on the cloud so if you're

813
00:27:49,440 --> 00:27:53,360
using ibm

814
00:27:50,640 --> 00:27:55,279
there are i think five or six ibm

815
00:27:53,360 --> 00:27:57,120
devices available

816
00:27:55,279 --> 00:27:58,399
uh for the public so if you sign up to

817
00:27:57,120 --> 00:28:00,559
their accounts

818
00:27:58,399 --> 00:28:02,219
um you have access to multiple devices

819
00:28:00,559 --> 00:28:03,360
right now

820
00:28:02,220 --> 00:28:05,520
[Music]

821
00:28:03,360 --> 00:28:06,879
the same thing is available on rigati

822
00:28:05,520 --> 00:28:10,639
forest there are i think

823
00:28:06,880 --> 00:28:14,880
two chips now available on the qcs

824
00:28:10,640 --> 00:28:16,399
aspen 4 and aspen 7. so

825
00:28:14,880 --> 00:28:18,159
maybe a year ago we were getting excited

826
00:28:16,399 --> 00:28:19,918
by having access to one device

827
00:28:18,159 --> 00:28:22,399
and that was fine now we have access to

828
00:28:19,919 --> 00:28:24,640
a couple of devices

829
00:28:22,399 --> 00:28:25,840
and it can be frustrating at some times

830
00:28:24,640 --> 00:28:27,679
now that we know we can have more than

831
00:28:25,840 --> 00:28:30,879
one we want a thousand

832
00:28:27,679 --> 00:28:31,200
so we started thinking what can we do

833
00:28:30,880 --> 00:28:33,039
with

834
00:28:31,200 --> 00:28:35,360
rather than have a single big qpu which

835
00:28:33,039 --> 00:28:37,520
is what a lot of hardware providers are

836
00:28:35,360 --> 00:28:39,199
working really hard to get towards what

837
00:28:37,520 --> 00:28:42,480
could we do in the interim if we had a

838
00:28:39,200 --> 00:28:42,480
thousand small qp's

839
00:28:42,880 --> 00:28:47,360
so we recently introduced the concept of

840
00:28:44,799 --> 00:28:49,120
a keynote collection

841
00:28:47,360 --> 00:28:50,799
so a qno collection is a group of

842
00:28:49,120 --> 00:28:54,080
keynotes that are evaluated

843
00:28:50,799 --> 00:28:56,840
they can be evaluated independently um

844
00:28:54,080 --> 00:28:59,120
so they can evaluate it uh at one go

845
00:28:56,840 --> 00:29:01,199
asynchronously

846
00:28:59,120 --> 00:29:02,320
at the same time we want it to be an

847
00:29:01,200 --> 00:29:03,679
abstraction that you don't have to think

848
00:29:02,320 --> 00:29:05,439
about too much

849
00:29:03,679 --> 00:29:07,039
and a common way for creating keynote

850
00:29:05,440 --> 00:29:09,200
collections and various

851
00:29:07,039 --> 00:29:10,720
quantum algorithms is you map a and sats

852
00:29:09,200 --> 00:29:12,640
or template

853
00:29:10,720 --> 00:29:14,159
over measurements so we wanted to extend

854
00:29:12,640 --> 00:29:17,200
that as well not only can you map

855
00:29:14,159 --> 00:29:19,039
a and sats or a quantum template over

856
00:29:17,200 --> 00:29:21,200
measurements but you can also map it

857
00:29:19,039 --> 00:29:23,440
over devices

858
00:29:21,200 --> 00:29:25,679
so i have a small example of what this

859
00:29:23,440 --> 00:29:28,080
looks like here

860
00:29:25,679 --> 00:29:29,039
so here i have a quantum circuit and

861
00:29:28,080 --> 00:29:32,960
sats

862
00:29:29,039 --> 00:29:34,879
um it just does two x-axis rotations

863
00:29:32,960 --> 00:29:36,640
uh both of these parameterized and then

864
00:29:34,880 --> 00:29:38,320
it does a c naught which is a quantum

865
00:29:36,640 --> 00:29:40,480
entangling gate

866
00:29:38,320 --> 00:29:41,840
then i have a list of observables i want

867
00:29:40,480 --> 00:29:43,440
to measure

868
00:29:41,840 --> 00:29:45,600
so these are just two qubit tensor

869
00:29:43,440 --> 00:29:48,320
observables i want to measure polyx

870
00:29:45,600 --> 00:29:50,799
cross polyzed and i also want to measure

871
00:29:48,320 --> 00:29:52,158
polyz cross polyx

872
00:29:50,799 --> 00:29:54,000
and then say i have access to two

873
00:29:52,159 --> 00:29:57,039
devices so in this example here i'm just

874
00:29:54,000 --> 00:29:59,360
using the rigetti forest qvm simulator

875
00:29:57,039 --> 00:30:01,520
uh with two devices that are currently

876
00:29:59,360 --> 00:30:04,240
available on the simulator aspen 4 and

877
00:30:01,520 --> 00:30:04,240
aspen 7.

878
00:30:04,559 --> 00:30:08,720
so previously you would create two q

879
00:30:06,880 --> 00:30:09,039
nodes you would be forced to evaluate

880
00:30:08,720 --> 00:30:10,880
them

881
00:30:09,039 --> 00:30:13,279
sequentially as you normally are in

882
00:30:10,880 --> 00:30:13,279
python

883
00:30:15,039 --> 00:30:18,480
we introduced a way to do this that

884
00:30:17,039 --> 00:30:18,960
makes it slightly simpler so you can

885
00:30:18,480 --> 00:30:21,919
treat

886
00:30:18,960 --> 00:30:23,600
you can use this new qml.map operation

887
00:30:21,919 --> 00:30:24,399
to map your ansats over both your

888
00:30:23,600 --> 00:30:27,439
observable list

889
00:30:24,399 --> 00:30:29,199
and your list of devices uh say what you

890
00:30:27,440 --> 00:30:30,640
want your measurement to be

891
00:30:29,200 --> 00:30:32,159
and the output of that is not just a

892
00:30:30,640 --> 00:30:33,840
single queue node but a collection or

893
00:30:32,159 --> 00:30:35,120
group of keynotes

894
00:30:33,840 --> 00:30:36,799
and these group of queue nodes are

895
00:30:35,120 --> 00:30:37,918
treated as one single object in penny

896
00:30:36,799 --> 00:30:40,000
lane

897
00:30:37,919 --> 00:30:42,399
so in this example here you have some

898
00:30:40,000 --> 00:30:43,679
parameters you pass it to your q nodes

899
00:30:42,399 --> 00:30:46,080
both of the quantum devices are

900
00:30:43,679 --> 00:30:47,279
evaluated at the same time

901
00:30:46,080 --> 00:30:50,799
and you get your results back as a

902
00:30:47,279 --> 00:30:52,159
single numpy array

903
00:30:50,799 --> 00:30:54,000
and the really cool thing about this is

904
00:30:52,159 --> 00:30:55,760
because we have access to these devices

905
00:30:54,000 --> 00:30:58,799
that we can access

906
00:30:55,760 --> 00:31:00,960
at the same time um

907
00:30:58,799 --> 00:31:02,240
it's embarrassingly paralyzable so we've

908
00:31:00,960 --> 00:31:05,440
added this asynchronous

909
00:31:02,240 --> 00:31:07,440
dispatch support to penny lane so this

910
00:31:05,440 --> 00:31:08,640
is experimental it's only recently been

911
00:31:07,440 --> 00:31:10,480
added

912
00:31:08,640 --> 00:31:12,960
uh but if you call your q nodes normally

913
00:31:10,480 --> 00:31:14,559
in this case it takes about five seconds

914
00:31:12,960 --> 00:31:16,399
if i pass the parallel equals true

915
00:31:14,559 --> 00:31:17,440
argument pending lanes dispatching both

916
00:31:16,399 --> 00:31:19,760
of these quantum circuits to be

917
00:31:17,440 --> 00:31:21,600
evaluated at the same time

918
00:31:19,760 --> 00:31:24,000
you get your results back in almost half

919
00:31:21,600 --> 00:31:24,000
the time

920
00:31:25,519 --> 00:31:28,960
so we added this ability to create qnode

921
00:31:27,519 --> 00:31:30,720
collections but we also want them to be

922
00:31:28,960 --> 00:31:33,679
easy to use

923
00:31:30,720 --> 00:31:34,559
so we introduced some high-level uh

924
00:31:33,679 --> 00:31:36,960
quantum function

925
00:31:34,559 --> 00:31:38,320
uh high-level processing functions they

926
00:31:36,960 --> 00:31:40,240
allow you to compose your kuno

927
00:31:38,320 --> 00:31:42,559
collections

928
00:31:40,240 --> 00:31:43,600
with other uh classical processing

929
00:31:42,559 --> 00:31:44,320
functions from your machine learning

930
00:31:43,600 --> 00:31:49,678
libraries

931
00:31:44,320 --> 00:31:51,279
so things like qml.sum qml.qml.apply

932
00:31:49,679 --> 00:31:52,720
so in this very simple three line

933
00:31:51,279 --> 00:31:54,880
example here

934
00:31:52,720 --> 00:31:56,480
i'm creating a so i'm using pi torch in

935
00:31:54,880 --> 00:31:57,279
this case creating a tensor of

936
00:31:56,480 --> 00:32:00,159
observables

937
00:31:57,279 --> 00:32:01,440
uh tensor of coefficients rather and

938
00:32:00,159 --> 00:32:02,799
then you can see in the second line i've

939
00:32:01,440 --> 00:32:04,320
got my q nodes

940
00:32:02,799 --> 00:32:05,679
and i'm just composing it with a whole

941
00:32:04,320 --> 00:32:07,200
bunch of different classical processing

942
00:32:05,679 --> 00:32:08,799
functions

943
00:32:07,200 --> 00:32:12,240
to manipulate the output of these q

944
00:32:08,799 --> 00:32:14,240
nodes and

945
00:32:12,240 --> 00:32:15,600
with the ability to manipulate qno

946
00:32:14,240 --> 00:32:17,200
collections in this way we've gone from

947
00:32:15,600 --> 00:32:19,199
something that might take

948
00:32:17,200 --> 00:32:20,640
uh 10 or 15 lines in python something

949
00:32:19,200 --> 00:32:21,519
that only becomes a single one line or a

950
00:32:20,640 --> 00:32:23,360
python

951
00:32:21,519 --> 00:32:25,120
so in this particular case i'm applying

952
00:32:23,360 --> 00:32:27,600
the torch sine function

953
00:32:25,120 --> 00:32:29,518
to my q nodes and then performing the

954
00:32:27,600 --> 00:32:30,799
dot products between

955
00:32:29,519 --> 00:32:33,360
the sign of those q nodes and the

956
00:32:30,799 --> 00:32:35,120
coefficients i just defined

957
00:32:33,360 --> 00:32:36,959
uh all this happens lazily in penny lane

958
00:32:35,120 --> 00:32:38,320
so the quantum evaluation only ever

959
00:32:36,960 --> 00:32:40,640
happens when you actually

960
00:32:38,320 --> 00:32:41,678
evaluate your qno collection your cost

961
00:32:40,640 --> 00:32:43,519
function

962
00:32:41,679 --> 00:32:45,679
so the third line here when i'm calling

963
00:32:43,519 --> 00:32:47,200
my new cost function with the parameters

964
00:32:45,679 --> 00:32:49,360
that's when the dispatching happens and

965
00:32:47,200 --> 00:32:51,039
penny elaine starts dispatching the

966
00:32:49,360 --> 00:32:54,840
quantum computations

967
00:32:51,039 --> 00:32:59,519
to the uh external quantum devices

968
00:32:54,840 --> 00:32:59,519
and performs the classical processing

969
00:33:02,080 --> 00:33:07,199
so i hope this talk has kind of given uh

970
00:33:05,840 --> 00:33:09,120
you guys an idea of our vision for

971
00:33:07,200 --> 00:33:11,120
quantum machine learning at xanadu

972
00:33:09,120 --> 00:33:12,239
we want uh quantum machine learning to

973
00:33:11,120 --> 00:33:15,279
be intuitive

974
00:33:12,240 --> 00:33:17,120
accessible um be easily explored and we

975
00:33:15,279 --> 00:33:18,799
want anyone to have the ability to run

976
00:33:17,120 --> 00:33:20,879
quantum machine learning algorithms

977
00:33:18,799 --> 00:33:22,840
whether it's on simulators or actually

978
00:33:20,880 --> 00:33:24,559
on hardware that's available in the

979
00:33:22,840 --> 00:33:26,158
cloud

980
00:33:24,559 --> 00:33:27,918
we want quantum machinery models to be

981
00:33:26,159 --> 00:33:30,320
widely widely shared

982
00:33:27,919 --> 00:33:31,519
and re-implemented by others so that's

983
00:33:30,320 --> 00:33:33,918
sort of the

984
00:33:31,519 --> 00:33:36,320
impetus behind the pennylane.ai qml

985
00:33:33,919 --> 00:33:36,320
collection

986
00:33:36,559 --> 00:33:40,399
circuits should be reusable and beddings

987
00:33:38,559 --> 00:33:41,840
pre post processing should be easy we

988
00:33:40,399 --> 00:33:44,639
want that to be something that you don't

989
00:33:41,840 --> 00:33:44,639
have to think about

990
00:33:44,960 --> 00:33:48,399
we want the documentation to be able to

991
00:33:47,200 --> 00:33:49,600
to be designed so that you can find

992
00:33:48,399 --> 00:33:50,479
these things easily and that maybe you

993
00:33:49,600 --> 00:33:51,600
don't even have to look at the

994
00:33:50,480 --> 00:33:53,600
documentation

995
00:33:51,600 --> 00:33:55,360
rather than searching throughout what

996
00:33:53,600 --> 00:33:58,399
methods are available

997
00:33:55,360 --> 00:33:59,840
uh you just find a function in paneling

998
00:33:58,399 --> 00:34:00,399
that applies that thing for you and you

999
00:33:59,840 --> 00:34:04,320
can do

1000
00:34:00,399 --> 00:34:04,320
function compositioning on your quantum

1001
00:34:04,840 --> 00:34:09,040
devices

1002
00:34:06,720 --> 00:34:10,159
um penny lane is open source development

1003
00:34:09,040 --> 00:34:13,599
is open source

1004
00:34:10,159 --> 00:34:15,599
and uh i think as uh mark mentioned

1005
00:34:13,599 --> 00:34:17,200
uh thomas mentioned in his first talk a

1006
00:34:15,599 --> 00:34:18,000
lot of the physicists working on quantum

1007
00:34:17,199 --> 00:34:19,759
open software

1008
00:34:18,000 --> 00:34:21,760
at the moment a i love the developers

1009
00:34:19,760 --> 00:34:25,040
art physicists i'm a physicist

1010
00:34:21,760 --> 00:34:28,000
i lost my team of physicists um

1011
00:34:25,040 --> 00:34:29,679
and sometimes it helps to get feedback

1012
00:34:28,000 --> 00:34:30,639
and contributions from people outside of

1013
00:34:29,679 --> 00:34:32,240
physics

1014
00:34:30,639 --> 00:34:34,399
so something we'd love is for people in

1015
00:34:32,239 --> 00:34:35,279
this room to have a look at our open

1016
00:34:34,399 --> 00:34:36,399
source code

1017
00:34:35,280 --> 00:34:38,399
have a look at the issues that are

1018
00:34:36,399 --> 00:34:40,638
opened and see if there's anything they

1019
00:34:38,399 --> 00:34:42,319
feel they can contribute because maybe

1020
00:34:40,639 --> 00:34:45,040
from someone outside of physics you guys

1021
00:34:42,320 --> 00:34:46,399
can add features or

1022
00:34:45,040 --> 00:34:49,839
implement things in a way that we would

1023
00:34:46,399 --> 00:34:49,839
never have thought of doing ourselves

1024
00:34:50,960 --> 00:34:55,599
so thank you got some links at the top

1025
00:34:53,599 --> 00:35:03,839
so this is our twitter link and our

1026
00:34:55,599 --> 00:35:03,839
qml tutorials page

1027
00:35:09,839 --> 00:35:11,920
you

