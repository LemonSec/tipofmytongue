1
00:00:08,960 --> 00:00:12,639
okay if everyone wants to get their

2
00:00:10,400 --> 00:00:16,960
seats i'm going to get started

3
00:00:12,639 --> 00:00:19,119
um so i'm here to talk about

4
00:00:16,960 --> 00:00:20,960
a project in our in our group at

5
00:00:19,119 --> 00:00:23,198
illinois called hpvm

6
00:00:20,960 --> 00:00:24,160
heterogeneous parallel virtual machine

7
00:00:23,199 --> 00:00:27,359
this is

8
00:00:24,160 --> 00:00:28,160
essentially meant to be a parallel

9
00:00:27,359 --> 00:00:31,039
compiler

10
00:00:28,160 --> 00:00:32,159
infrastructure that's built on top of

11
00:00:31,039 --> 00:00:35,120
llbm

12
00:00:32,159 --> 00:00:37,760
and the goal here is to try and get both

13
00:00:35,120 --> 00:00:39,199
performance and portability for

14
00:00:37,760 --> 00:00:41,120
code applications running on

15
00:00:39,200 --> 00:00:42,800
heterogeneous parallel systems

16
00:00:41,120 --> 00:00:44,718
first i should say by the way that i

17
00:00:42,800 --> 00:00:45,599
have not written almost any of the code

18
00:00:44,719 --> 00:00:48,000
in this

19
00:00:45,600 --> 00:00:49,039
project this was done all by my phd

20
00:00:48,000 --> 00:00:51,440
students

21
00:00:49,039 --> 00:00:53,039
unfortunately the lead students could

22
00:00:51,440 --> 00:00:54,640
not be here one of them is just applied

23
00:00:53,039 --> 00:00:56,320
for a green card and so he can't leave

24
00:00:54,640 --> 00:00:57,920
the country and the other one is some

25
00:00:56,320 --> 00:00:59,600
health issues so she couldn't leave the

26
00:00:57,920 --> 00:01:01,520
self i am

27
00:00:59,600 --> 00:01:02,719
holding the fort on there on their

28
00:01:01,520 --> 00:01:05,439
behalf but

29
00:01:02,719 --> 00:01:06,080
uh maria kotsifaku and hashem sherry for

30
00:01:05,438 --> 00:01:08,880
the two main

31
00:01:06,080 --> 00:01:09,119
leads on this uh prakal shrivastava jay

32
00:01:08,880 --> 00:01:11,439
and

33
00:01:09,119 --> 00:01:12,960
and the others listed here and we also

34
00:01:11,439 --> 00:01:14,559
have two other faculty members sarita

35
00:01:12,960 --> 00:01:17,919
advai and sasham salovich

36
00:01:14,560 --> 00:01:19,840
involved in this uh in this work

37
00:01:17,920 --> 00:01:22,080
so i don't think i have to tell you that

38
00:01:19,840 --> 00:01:22,479
heterogeneous socs are pretty ubiquitous

39
00:01:22,080 --> 00:01:24,640
and

40
00:01:22,479 --> 00:01:26,720
moreover it's not just the ubiquitous

41
00:01:24,640 --> 00:01:28,400
night right now but very important

42
00:01:26,720 --> 00:01:30,798
applications and increasingly important

43
00:01:28,400 --> 00:01:33,040
applications are going to be possible

44
00:01:30,799 --> 00:01:33,680
only because of these kinds of socs

45
00:01:33,040 --> 00:01:35,920
because you

46
00:01:33,680 --> 00:01:37,840
you really need dramatic performance

47
00:01:35,920 --> 00:01:39,840
improvements and energy

48
00:01:37,840 --> 00:01:41,920
efficiency to be able to make these

49
00:01:39,840 --> 00:01:43,759
kinds of applications possible

50
00:01:41,920 --> 00:01:46,799
and in order to make those possible you

51
00:01:43,759 --> 00:01:48,880
need socs that can be programmed

52
00:01:46,799 --> 00:01:50,479
as easily as possible and in fact the

53
00:01:48,880 --> 00:01:52,960
previous talk gave some

54
00:01:50,479 --> 00:01:54,399
good motivation for why you need both

55
00:01:52,960 --> 00:01:56,798
the performance efficiency

56
00:01:54,399 --> 00:01:58,399
energy efficiency and also the

57
00:01:56,799 --> 00:02:01,280
programmability

58
00:01:58,399 --> 00:02:02,079
um and these are some example domains of

59
00:02:01,280 --> 00:02:03,759
these

60
00:02:02,079 --> 00:02:05,520
kinds of problems we are actually

61
00:02:03,759 --> 00:02:06,719
working with an autonomous car

62
00:02:05,520 --> 00:02:08,318
application that i'll say a couple of

63
00:02:06,719 --> 00:02:09,919
words about and we're starting to work

64
00:02:08,318 --> 00:02:12,480
with a mobile robot

65
00:02:09,919 --> 00:02:13,359
for agriculture that's being developed

66
00:02:12,480 --> 00:02:16,959
at illinois

67
00:02:13,360 --> 00:02:20,000
also as example edge applications

68
00:02:16,959 --> 00:02:23,120
but in a project led by ibm

69
00:02:20,000 --> 00:02:25,760
we are using a um

70
00:02:23,120 --> 00:02:26,640
a model application of an autonomous

71
00:02:25,760 --> 00:02:28,959
vehicle

72
00:02:26,640 --> 00:02:31,200
that has multiple different kinds of

73
00:02:28,959 --> 00:02:31,599
application components a neural network

74
00:02:31,200 --> 00:02:36,079
for

75
00:02:31,599 --> 00:02:40,238
for image processing um an fft

76
00:02:36,080 --> 00:02:42,000
a viterbi decoder some control logic

77
00:02:40,239 --> 00:02:44,000
as a model application of an autonomous

78
00:02:42,000 --> 00:02:46,000
vehicle and

79
00:02:44,000 --> 00:02:47,200
the goal of the project is to be able to

80
00:02:46,000 --> 00:02:49,760
do full stack

81
00:02:47,200 --> 00:02:52,238
development of both the hardware design

82
00:02:49,760 --> 00:02:55,440
from the application domain code

83
00:02:52,239 --> 00:02:57,040
and a programming stack for that and

84
00:02:55,440 --> 00:02:59,680
there's actually several talks

85
00:02:57,040 --> 00:03:01,760
happening today and tomorrow at fosdem

86
00:02:59,680 --> 00:03:04,239
about different parts of this in the

87
00:03:01,760 --> 00:03:05,280
risc 5 room and in the software defined

88
00:03:04,239 --> 00:03:08,560
radio room

89
00:03:05,280 --> 00:03:11,040
and my talk is in this room here and

90
00:03:08,560 --> 00:03:13,280
our role in this project is to use hpvm

91
00:03:11,040 --> 00:03:15,920
for programmability so we're basically

92
00:03:13,280 --> 00:03:16,879
in some sense looking at the compiler

93
00:03:15,920 --> 00:03:19,440
infrastructure

94
00:03:16,879 --> 00:03:21,280
on the left and the development

95
00:03:19,440 --> 00:03:24,319
environment and programming languages

96
00:03:21,280 --> 00:03:28,400
and how to implement them easily on a or

97
00:03:24,319 --> 00:03:30,238
on a fairly uh custom soc design with a

98
00:03:28,400 --> 00:03:31,920
with a wide range of different

99
00:03:30,239 --> 00:03:35,360
accelerators

100
00:03:31,920 --> 00:03:37,359
and host cpu

101
00:03:35,360 --> 00:03:39,519
so just to motivate that i use a

102
00:03:37,360 --> 00:03:41,760
slightly different soc as an example but

103
00:03:39,519 --> 00:03:43,840
but the underlying problem that we're

104
00:03:41,760 --> 00:03:45,200
trying to tackle is that on a single soc

105
00:03:43,840 --> 00:03:46,480
first you have a whole number of

106
00:03:45,200 --> 00:03:48,159
different hardware

107
00:03:46,480 --> 00:03:50,480
instruction sets which you want to be

108
00:03:48,159 --> 00:03:52,159
able to target but that's not the only

109
00:03:50,480 --> 00:03:53,760
problem you also have a number of

110
00:03:52,159 --> 00:03:56,079
different parallelism models so there's

111
00:03:53,760 --> 00:03:59,679
different kinds of parallelism happening

112
00:03:56,080 --> 00:04:02,000
in the different components of the soc

113
00:03:59,680 --> 00:04:03,760
and even worse you have incompatible

114
00:04:02,000 --> 00:04:04,560
memory systems you typically have so you

115
00:04:03,760 --> 00:04:06,720
may have a

116
00:04:04,560 --> 00:04:07,680
share a cache coherent hierarchy and one

117
00:04:06,720 --> 00:04:10,959
you may have

118
00:04:07,680 --> 00:04:12,000
only a local scratch pad kind of memory

119
00:04:10,959 --> 00:04:14,400
in another

120
00:04:12,000 --> 00:04:16,000
and you might have a dma or some other

121
00:04:14,400 --> 00:04:17,519
data movement mechanism between the

122
00:04:16,000 --> 00:04:19,040
different accelerators

123
00:04:17,519 --> 00:04:21,120
and you want to be able to take an

124
00:04:19,040 --> 00:04:22,960
application and run on

125
00:04:21,120 --> 00:04:24,720
an soc like this but what can make it

126
00:04:22,960 --> 00:04:26,080
even worse is that different socs have

127
00:04:24,720 --> 00:04:27,600
different combinations

128
00:04:26,080 --> 00:04:29,199
of this hardware and that makes the

129
00:04:27,600 --> 00:04:31,680
portability problem

130
00:04:29,199 --> 00:04:32,639
far worse now different domains have

131
00:04:31,680 --> 00:04:34,000
different

132
00:04:32,639 --> 00:04:36,400
some domains have the portability

133
00:04:34,000 --> 00:04:37,120
problem some don't um mobile phones have

134
00:04:36,400 --> 00:04:38,560
it to a very

135
00:04:37,120 --> 00:04:40,080
large extent because you want to run an

136
00:04:38,560 --> 00:04:40,720
app on many different kinds of mobile

137
00:04:40,080 --> 00:04:42,800
phones

138
00:04:40,720 --> 00:04:44,160
think about android for example how many

139
00:04:42,800 --> 00:04:46,320
different socs

140
00:04:44,160 --> 00:04:47,919
power different android phones um

141
00:04:46,320 --> 00:04:49,840
otherwise socs don't necessarily have

142
00:04:47,919 --> 00:04:52,000
this problem if you're really custom

143
00:04:49,840 --> 00:04:53,119
compiling the stack for a particular soc

144
00:04:52,000 --> 00:04:55,199
design but

145
00:04:53,120 --> 00:04:56,720
you do always have this driver problem

146
00:04:55,199 --> 00:04:58,320
of heterogeneity

147
00:04:56,720 --> 00:05:00,560
and so we believe that the key to

148
00:04:58,320 --> 00:05:03,919
achieving performance and portability

149
00:05:00,560 --> 00:05:06,400
at the same time on this kind of uh

150
00:05:03,919 --> 00:05:08,159
system is to have well-designed

151
00:05:06,400 --> 00:05:10,638
abstractions for the underlying

152
00:05:08,160 --> 00:05:13,199
heterogeneous system for the underlying

153
00:05:10,639 --> 00:05:14,080
parallel hardware and to be able to use

154
00:05:13,199 --> 00:05:16,400
that to

155
00:05:14,080 --> 00:05:17,199
develop both compiler infrastructure and

156
00:05:16,400 --> 00:05:19,039
tooling

157
00:05:17,199 --> 00:05:20,400
around the uh to develop the whole

158
00:05:19,039 --> 00:05:24,880
software stack around those

159
00:05:20,400 --> 00:05:26,080
abstractions um so before i go into the

160
00:05:24,880 --> 00:05:27,440
abstractions that we use

161
00:05:26,080 --> 00:05:29,120
i just want to say a couple of words

162
00:05:27,440 --> 00:05:32,400
about the current state of

163
00:05:29,120 --> 00:05:32,800
um what you have with llvm and this is

164
00:05:32,400 --> 00:05:35,120
not

165
00:05:32,800 --> 00:05:37,280
a comprehensive slide but it's i think

166
00:05:35,120 --> 00:05:38,400
fairly representative of what choices

167
00:05:37,280 --> 00:05:41,119
are available today

168
00:05:38,400 --> 00:05:41,840
if you want to build parallel compilers

169
00:05:41,120 --> 00:05:45,680
with

170
00:05:41,840 --> 00:05:47,039
llvm based systems and so llvm itself of

171
00:05:45,680 --> 00:05:48,880
course as you all know is primarily

172
00:05:47,039 --> 00:05:51,039
targeted at vector parallelism so short

173
00:05:48,880 --> 00:05:53,919
sim d kind of thing sse avx things like

174
00:05:51,039 --> 00:05:55,599
that poly does polyhedral transforms and

175
00:05:53,919 --> 00:05:57,599
some scheduling

176
00:05:55,600 --> 00:05:59,520
taper is a project of mit that targets

177
00:05:57,600 --> 00:06:00,960
hydrogen and shared memory systems

178
00:05:59,520 --> 00:06:03,198
there's compilers for languages like

179
00:06:00,960 --> 00:06:04,880
openmp and opencl and cuda

180
00:06:03,199 --> 00:06:06,240
each of those are pretty specific to the

181
00:06:04,880 --> 00:06:07,919
particular language and they don't

182
00:06:06,240 --> 00:06:09,280
really try to generalize

183
00:06:07,919 --> 00:06:11,440
in terms of the languages that they

184
00:06:09,280 --> 00:06:15,039
support there's other projects like leg

185
00:06:11,440 --> 00:06:18,319
up for fpgas tensorflow for tpu and gpu

186
00:06:15,039 --> 00:06:20,000
i think mlir is certainly the most

187
00:06:18,319 --> 00:06:22,639
recent and most well-known perhaps

188
00:06:20,000 --> 00:06:23,600
addition to this list they support

189
00:06:22,639 --> 00:06:25,680
tensors

190
00:06:23,600 --> 00:06:28,080
especially very well and have strong

191
00:06:25,680 --> 00:06:29,759
support for polyhedral transforms for

192
00:06:28,080 --> 00:06:31,599
a high dimensional or actually any

193
00:06:29,759 --> 00:06:32,880
dimensional kinds of tensors

194
00:06:31,600 --> 00:06:35,120
i would argue that none of these are

195
00:06:32,880 --> 00:06:37,120
really attempting to capture a diverse

196
00:06:35,120 --> 00:06:39,120
range of heterogeneous parallelism

197
00:06:37,120 --> 00:06:42,960
which is what we think you need to build

198
00:06:39,120 --> 00:06:45,039
a flexible compiler infrastructure

199
00:06:42,960 --> 00:06:46,799
and so that's what the goal of our work

200
00:06:45,039 --> 00:06:48,800
has been in hpvm

201
00:06:46,800 --> 00:06:51,360
it's to develop a common parallel

202
00:06:48,800 --> 00:06:52,720
abstraction of this kind of parallelism

203
00:06:51,360 --> 00:06:55,440
of the diverse range of parallelism

204
00:06:52,720 --> 00:06:58,160
that's available and then use that

205
00:06:55,440 --> 00:07:00,719
to develop the programming environment

206
00:06:58,160 --> 00:07:02,720
and so that includes in our case

207
00:07:00,720 --> 00:07:03,840
a compiler ir which is an extension of

208
00:07:02,720 --> 00:07:05,759
llvm

209
00:07:03,840 --> 00:07:07,679
a virtual instruction set which is so

210
00:07:05,759 --> 00:07:08,800
just like llvm itself is the word both

211
00:07:07,680 --> 00:07:10,960
the virtual instruction set under

212
00:07:08,800 --> 00:07:13,360
compiler i you can actually ship code

213
00:07:10,960 --> 00:07:14,239
as llvm as apple does for watch and

214
00:07:13,360 --> 00:07:16,720
apple tv

215
00:07:14,240 --> 00:07:17,840
and apple tv and iphone and so on just

216
00:07:16,720 --> 00:07:19,440
like that you can ship

217
00:07:17,840 --> 00:07:21,198
code in this virtual instruction set

218
00:07:19,440 --> 00:07:24,319
form as hp vm code

219
00:07:21,199 --> 00:07:26,000
in a way to achieve portability and

220
00:07:24,319 --> 00:07:27,280
third we can also use this for runtime

221
00:07:26,000 --> 00:07:31,280
scheduling

222
00:07:27,280 --> 00:07:34,479
in the system

223
00:07:31,280 --> 00:07:35,758
so this is a high level view of what an

224
00:07:34,479 --> 00:07:38,000
hpvm

225
00:07:35,759 --> 00:07:39,039
infrastructure looks like the idea is

226
00:07:38,000 --> 00:07:41,120
that you have

227
00:07:39,039 --> 00:07:42,159
front ends for potentially a variety of

228
00:07:41,120 --> 00:07:43,680
languages

229
00:07:42,160 --> 00:07:46,720
right now we have front ends for

230
00:07:43,680 --> 00:07:48,879
essentially an extension of c with hpvm

231
00:07:46,720 --> 00:07:50,080
uh intrinsics and then the front end for

232
00:07:48,879 --> 00:07:53,919
keras for

233
00:07:50,080 --> 00:07:56,400
uh neural networks and these translate

234
00:07:53,919 --> 00:07:57,440
into the hpvm virtual instruction set or

235
00:07:56,400 --> 00:07:59,679
ir

236
00:07:57,440 --> 00:08:02,319
and then you have a variety of back end

237
00:07:59,680 --> 00:08:05,599
translators that translate hpvm

238
00:08:02,319 --> 00:08:08,639
ir into different hardware targets

239
00:08:05,599 --> 00:08:10,479
and this is schematic we don't support

240
00:08:08,639 --> 00:08:12,000
all of these yet but we do support a

241
00:08:10,479 --> 00:08:14,400
variety and you'll see which

242
00:08:12,000 --> 00:08:16,400
i'll come to those in a minute but the

243
00:08:14,400 --> 00:08:19,440
point now is that we can use

244
00:08:16,400 --> 00:08:21,198
this hpvm representation for achieving

245
00:08:19,440 --> 00:08:23,599
portable object code by

246
00:08:21,199 --> 00:08:24,960
using it as a virtual instruction set we

247
00:08:23,599 --> 00:08:27,280
can use it for doing

248
00:08:24,960 --> 00:08:29,520
for building retargetable compiler

249
00:08:27,280 --> 00:08:34,159
infrastructures so that you can

250
00:08:29,520 --> 00:08:36,478
both do optimization and code generation

251
00:08:34,159 --> 00:08:38,799
in a common compiler infrastructure for

252
00:08:36,479 --> 00:08:41,680
a variety of heterogeneous hardware

253
00:08:38,799 --> 00:08:43,679
and use it for runtime scheduling so

254
00:08:41,679 --> 00:08:44,959
that's the high level picture of what

255
00:08:43,679 --> 00:08:47,439
we're trying to do

256
00:08:44,959 --> 00:08:50,079
and the abstraction that we use that

257
00:08:47,440 --> 00:08:52,320
we've developed in the hpvm project

258
00:08:50,080 --> 00:08:53,519
looks like this it's essentially a data

259
00:08:52,320 --> 00:08:55,440
flow graph

260
00:08:53,519 --> 00:08:57,360
but with side effects so it's not pure

261
00:08:55,440 --> 00:08:59,120
data flow and that's important because

262
00:08:57,360 --> 00:09:00,800
many accelerators today actually support

263
00:08:59,120 --> 00:09:02,560
some form of shared memory

264
00:09:00,800 --> 00:09:04,240
so even gpus are starting to do that

265
00:09:02,560 --> 00:09:05,359
other accelerators are likely to do that

266
00:09:04,240 --> 00:09:07,680
in the future

267
00:09:05,360 --> 00:09:09,360
and shared memory is very important for

268
00:09:07,680 --> 00:09:10,640
achieving good performance on these

269
00:09:09,360 --> 00:09:13,120
systems

270
00:09:10,640 --> 00:09:15,360
but a single node in this data flow

271
00:09:13,120 --> 00:09:18,560
graph is essentially llvm code so it can

272
00:09:15,360 --> 00:09:20,240
be a mixture of scalar and vector code

273
00:09:18,560 --> 00:09:22,479
and so each node is represented as an

274
00:09:20,240 --> 00:09:24,959
lrvm function which can call other lrvm

275
00:09:22,480 --> 00:09:26,640
functions if you need to

276
00:09:24,959 --> 00:09:28,640
and otherwise it's a standard data flow

277
00:09:26,640 --> 00:09:29,760
graph except for one additional wrinkle

278
00:09:28,640 --> 00:09:31,600
which is that

279
00:09:29,760 --> 00:09:33,120
we make this graph hierarchical so a

280
00:09:31,600 --> 00:09:35,120
single node itself

281
00:09:33,120 --> 00:09:38,240
can be an entire data flow graph of its

282
00:09:35,120 --> 00:09:39,920
own so that you get a form of

283
00:09:38,240 --> 00:09:42,240
parallelism hierarchy and the reason for

284
00:09:39,920 --> 00:09:43,680
this is because it's very common to have

285
00:09:42,240 --> 00:09:45,600
multiple levels of parallelism in

286
00:09:43,680 --> 00:09:46,959
heterogeneous systems so you might have

287
00:09:45,600 --> 00:09:49,360
coarse grained parallelism across

288
00:09:46,959 --> 00:09:51,359
multiple different processing elements

289
00:09:49,360 --> 00:09:52,720
different gpus or different accelerators

290
00:09:51,360 --> 00:09:54,720
and the host processor

291
00:09:52,720 --> 00:09:56,000
but you also have extensive parallelism

292
00:09:54,720 --> 00:09:58,640
within a single

293
00:09:56,000 --> 00:10:00,000
processing element like a gpu or a fft

294
00:09:58,640 --> 00:10:02,160
accelerator or something else and

295
00:10:00,000 --> 00:10:03,519
and that gets captured nicely with this

296
00:10:02,160 --> 00:10:05,760
hierarchical graph

297
00:10:03,519 --> 00:10:07,680
and so the idea now is that the nodes

298
00:10:05,760 --> 00:10:09,200
here essentially represent either coarse

299
00:10:07,680 --> 00:10:10,479
grain or fine grained computational

300
00:10:09,200 --> 00:10:13,279
tasks

301
00:10:10,480 --> 00:10:15,200
graph edges represent logical data

302
00:10:13,279 --> 00:10:16,800
transfer data movement from a source

303
00:10:15,200 --> 00:10:19,279
node to a sync node

304
00:10:16,800 --> 00:10:20,719
it's logical in the sense that if two

305
00:10:19,279 --> 00:10:23,279
diff two nodes uh

306
00:10:20,720 --> 00:10:24,640
get mapped to the same device you don't

307
00:10:23,279 --> 00:10:25,360
actually have to do the physical data

308
00:10:24,640 --> 00:10:26,800
movement

309
00:10:25,360 --> 00:10:29,519
but logically there's like a copy

310
00:10:26,800 --> 00:10:31,279
happening in that case anyway

311
00:10:29,519 --> 00:10:33,200
and then you also have loads and stores

312
00:10:31,279 --> 00:10:34,720
which do implicit communication or

313
00:10:33,200 --> 00:10:37,040
implicit data movement between the

314
00:10:34,720 --> 00:10:39,600
different

315
00:10:37,040 --> 00:10:41,360
nodes and it's hierarchical as i just

316
00:10:39,600 --> 00:10:43,839
said

317
00:10:41,360 --> 00:10:46,320
one more important aspect of hpvm is

318
00:10:43,839 --> 00:10:48,240
that a single node is sort of like a gpu

319
00:10:46,320 --> 00:10:50,800
kernel just like a gpu kernel

320
00:10:48,240 --> 00:10:51,519
gets instantiated into a grid of threads

321
00:10:50,800 --> 00:10:53,199
which might be

322
00:10:51,519 --> 00:10:55,680
a one-dimensional two-dimensional and so

323
00:10:53,200 --> 00:10:58,800
on similarly an hpvm

324
00:10:55,680 --> 00:11:01,680
data flow graph node can be given an

325
00:10:58,800 --> 00:11:02,479
index or a sort of a vector of indices

326
00:11:01,680 --> 00:11:04,319
and we

327
00:11:02,480 --> 00:11:05,920
instantiate it into a set of parallel

328
00:11:04,320 --> 00:11:07,040
instances that will execute in parallel

329
00:11:05,920 --> 00:11:08,719
at runtime

330
00:11:07,040 --> 00:11:10,719
and so for example if you're given one

331
00:11:08,720 --> 00:11:12,480
index one and you end and you have n

332
00:11:10,720 --> 00:11:15,440
parallel instances or threads

333
00:11:12,480 --> 00:11:17,760
executing this uh the same code at

334
00:11:15,440 --> 00:11:20,240
runtime and we require that these

335
00:11:17,760 --> 00:11:21,439
instances be independent of each other

336
00:11:20,240 --> 00:11:23,440
um that's ex

337
00:11:21,440 --> 00:11:24,959
sort of expected or we rely on the

338
00:11:23,440 --> 00:11:28,560
program or the front end

339
00:11:24,959 --> 00:11:30,399
to ensure that's true and we support one

340
00:11:28,560 --> 00:11:30,959
two and three dimensional grids for

341
00:11:30,399 --> 00:11:32,560
these and

342
00:11:30,959 --> 00:11:34,719
conceptually there's no limitation but

343
00:11:32,560 --> 00:11:37,199
that's what the current system

344
00:11:34,720 --> 00:11:37,200
supports

345
00:11:37,839 --> 00:11:42,959
and so this is an example of a

346
00:11:40,959 --> 00:11:44,800
of a black and white edged uh sorry

347
00:11:42,959 --> 00:11:45,518
grayscale edge detection pipeline that

348
00:11:44,800 --> 00:11:48,719
we've

349
00:11:45,519 --> 00:11:50,320
uh developed in hpvm and it kind of

350
00:11:48,720 --> 00:11:51,360
shows the hierarchical graph structure

351
00:11:50,320 --> 00:11:54,240
so you have

352
00:11:51,360 --> 00:11:54,959
a pipeline you have pipeline task

353
00:11:54,240 --> 00:11:55,920
parallelism

354
00:11:54,959 --> 00:11:57,680
where you're doing different

355
00:11:55,920 --> 00:11:59,120
computations in each of the nodes of the

356
00:11:57,680 --> 00:12:01,920
graph

357
00:11:59,120 --> 00:12:03,519
you have medium grain data parallelism

358
00:12:01,920 --> 00:12:04,959
within the pipeline stages you have

359
00:12:03,519 --> 00:12:06,720
fine-grained data parallelism within

360
00:12:04,959 --> 00:12:09,199
each individual pipeline stage

361
00:12:06,720 --> 00:12:09,760
you have a graph hierarchy so that for

362
00:12:09,200 --> 00:12:11,519
example

363
00:12:09,760 --> 00:12:13,839
inside in node that there are zero

364
00:12:11,519 --> 00:12:16,399
crossings you have multiple different

365
00:12:13,839 --> 00:12:17,440
um potentially multiple different hpvm

366
00:12:16,399 --> 00:12:19,120
kernels there

367
00:12:17,440 --> 00:12:22,800
which the compiler can choose to fuse

368
00:12:19,120 --> 00:12:22,800
into a single kernel if it wants to

369
00:12:23,360 --> 00:12:26,480
um so there are details in this image

370
00:12:25,360 --> 00:12:27,680
which i won't go through if you're

371
00:12:26,480 --> 00:12:28,880
interested i'm more than happy to talk

372
00:12:27,680 --> 00:12:30,638
about it offline

373
00:12:28,880 --> 00:12:32,320
but the point here actually the takeaway

374
00:12:30,639 --> 00:12:32,880
point is that we can represent multiple

375
00:12:32,320 --> 00:12:35,920
different

376
00:12:32,880 --> 00:12:37,680
kinds of parallelism in a single

377
00:12:35,920 --> 00:12:38,959
parallel representation and that's

378
00:12:37,680 --> 00:12:40,560
important because

379
00:12:38,959 --> 00:12:41,839
you have a variety of different kinds of

380
00:12:40,560 --> 00:12:44,239
parallelism in these heterogeneous

381
00:12:41,839 --> 00:12:45,839
systems

382
00:12:44,240 --> 00:12:47,440
so the way this is actually implemented

383
00:12:45,839 --> 00:12:51,360
in practice is by using

384
00:12:47,440 --> 00:12:52,800
llvm intrinsic functions

385
00:12:51,360 --> 00:12:54,880
i'm sure all of you are pretty familiar

386
00:12:52,800 --> 00:12:56,880
at this point with llvm intrinsics but

387
00:12:54,880 --> 00:12:59,120
it's an easy way to add new

388
00:12:56,880 --> 00:12:59,920
operations to llvm without having to

389
00:12:59,120 --> 00:13:01,360
change

390
00:12:59,920 --> 00:13:03,760
a large number of passes in the

391
00:13:01,360 --> 00:13:06,880
infrastructure and so we have

392
00:13:03,760 --> 00:13:09,439
ins intrinsics for for declaring gra

393
00:13:06,880 --> 00:13:10,480
the graph structure so create node 1d 2d

394
00:13:09,440 --> 00:13:13,600
in 3d

395
00:13:10,480 --> 00:13:15,920
or create edge also bind input is sort

396
00:13:13,600 --> 00:13:19,120
of a special case where

397
00:13:15,920 --> 00:13:21,120
when you have a parent node with a graph

398
00:13:19,120 --> 00:13:22,800
inside it you need to connect the inputs

399
00:13:21,120 --> 00:13:23,200
of the parent node to the inputs of the

400
00:13:22,800 --> 00:13:25,359
child

401
00:13:23,200 --> 00:13:26,959
graph and those are just bindings as

402
00:13:25,360 --> 00:13:28,639
opposed to data flow edges

403
00:13:26,959 --> 00:13:31,199
and so we have a different intrinsic to

404
00:13:28,639 --> 00:13:34,240
declare those bindings

405
00:13:31,200 --> 00:13:35,839
and then intrinsics to query the current

406
00:13:34,240 --> 00:13:37,519
graph so you can ask for your current

407
00:13:35,839 --> 00:13:38,880
node id or the number of instances of

408
00:13:37,519 --> 00:13:42,000
your current node or the

409
00:13:38,880 --> 00:13:44,160
id of your parent node in order to for

410
00:13:42,000 --> 00:13:47,760
example partition a computation

411
00:13:44,160 --> 00:13:50,160
uh or or index into an apparel array in

412
00:13:47,760 --> 00:13:52,000
the threads that are executing in a node

413
00:13:50,160 --> 00:13:53,920
and then intrinsics for doing memory

414
00:13:52,000 --> 00:13:57,600
allocation and synchronization

415
00:13:53,920 --> 00:13:59,279
um like doing hpv malloc which will

416
00:13:57,600 --> 00:14:02,480
allocate memory on

417
00:13:59,279 --> 00:14:04,079
the appropriate um well so this is an

418
00:14:02,480 --> 00:14:05,120
abstraction of where memory will be

419
00:14:04,079 --> 00:14:10,239
allocated

420
00:14:05,120 --> 00:14:12,720
and it will be copied where it is needed

421
00:14:10,240 --> 00:14:15,920
and we also have intrinsics for doing

422
00:14:12,720 --> 00:14:18,160
atomic exchange atomic add and barrier

423
00:14:15,920 --> 00:14:19,839
this is not necessarily a complete set

424
00:14:18,160 --> 00:14:21,360
but these are enough to do a number of

425
00:14:19,839 --> 00:14:24,480
different applications

426
00:14:21,360 --> 00:14:26,720
that we have implemented we also have

427
00:14:24,480 --> 00:14:28,399
some additional intrinsics to interact

428
00:14:26,720 --> 00:14:30,480
with a host processor

429
00:14:28,399 --> 00:14:31,519
that can launch an execution on a

430
00:14:30,480 --> 00:14:35,920
particular

431
00:14:31,519 --> 00:14:40,160
set of targets in a particular

432
00:14:35,920 --> 00:14:43,279
heterogeneous system and so hpvm launch

433
00:14:40,160 --> 00:14:45,760
launches the execution of a single graph

434
00:14:43,279 --> 00:14:46,800
and that's an asynchronous operation so

435
00:14:45,760 --> 00:14:49,279
the graph will start

436
00:14:46,800 --> 00:14:50,240
executing while the host processor

437
00:14:49,279 --> 00:14:52,880
continues

438
00:14:50,240 --> 00:14:54,720
concurrently with that graph and a

439
00:14:52,880 --> 00:14:56,079
program is made up of multiple graphs so

440
00:14:54,720 --> 00:14:57,920
in fact in theory

441
00:14:56,079 --> 00:15:00,479
the host could go on and launch

442
00:14:57,920 --> 00:15:03,920
additional graphs at the same time

443
00:15:00,480 --> 00:15:05,920
and then hpvm weight stop blocks for

444
00:15:03,920 --> 00:15:07,920
completion of a particular

445
00:15:05,920 --> 00:15:09,279
graph and then for streaming

446
00:15:07,920 --> 00:15:10,560
applications we have two additional

447
00:15:09,279 --> 00:15:11,760
intrinsics push and pop which

448
00:15:10,560 --> 00:15:15,040
essentially push

449
00:15:11,760 --> 00:15:17,199
data elements into a graph for

450
00:15:15,040 --> 00:15:19,120
processing for example a stream of

451
00:15:17,199 --> 00:15:23,439
images or something like that and pop

452
00:15:19,120 --> 00:15:23,440
retrieves results back from that

453
00:15:24,320 --> 00:15:28,800
so i'll just quickly show you a

454
00:15:26,959 --> 00:15:31,119
high-level view of the optimization and

455
00:15:28,800 --> 00:15:33,439
code generation pipeline

456
00:15:31,120 --> 00:15:34,320
sorry uh infrastructure that that we

457
00:15:33,440 --> 00:15:36,000
support

458
00:15:34,320 --> 00:15:37,920
so we assume that there's some front-end

459
00:15:36,000 --> 00:15:40,399
that's generating the hpvm

460
00:15:37,920 --> 00:15:41,599
code representation which is llvm

461
00:15:40,399 --> 00:15:43,519
extended with intrinsics

462
00:15:41,600 --> 00:15:44,959
and i'll come back to though that issue

463
00:15:43,519 --> 00:15:48,320
in a moment and

464
00:15:44,959 --> 00:15:51,758
conceptually that can be done and then

465
00:15:48,320 --> 00:15:53,279
shipped to a server or something else

466
00:15:51,759 --> 00:15:54,800
that's aware of where the code is going

467
00:15:53,279 --> 00:15:57,040
to run so that

468
00:15:54,800 --> 00:15:58,399
either at the user site or at a server

469
00:15:57,040 --> 00:16:00,079
that is aware of where the code is

470
00:15:58,399 --> 00:16:01,440
running you could then do

471
00:16:00,079 --> 00:16:04,319
the final optimization and code

472
00:16:01,440 --> 00:16:06,000
generation in a target dependent manner

473
00:16:04,320 --> 00:16:07,759
you don't have to do that but that's

474
00:16:06,000 --> 00:16:09,759
that's one way to get object code

475
00:16:07,759 --> 00:16:13,279
portability

476
00:16:09,759 --> 00:16:15,600
and now we have a build dfg

477
00:16:13,279 --> 00:16:18,560
pass which is a module pass in llvm that

478
00:16:15,600 --> 00:16:20,959
takes the llvm intrinsics and generates

479
00:16:18,560 --> 00:16:23,439
a an explicit graph data structure

480
00:16:20,959 --> 00:16:25,920
representation of the hpv mir

481
00:16:23,440 --> 00:16:27,839
so the nodes and edges representing the

482
00:16:25,920 --> 00:16:30,079
hierarchical dataflow graph

483
00:16:27,839 --> 00:16:31,920
and then that data flow graph is given

484
00:16:30,079 --> 00:16:33,920
to a graph optimizer which does

485
00:16:31,920 --> 00:16:35,279
transformations like node fusion and

486
00:16:33,920 --> 00:16:37,680
tiling and other things

487
00:16:35,279 --> 00:16:39,279
that you can do as essentially high

488
00:16:37,680 --> 00:16:42,560
level graph transforms

489
00:16:39,279 --> 00:16:44,079
on the data flow graph itself and then

490
00:16:42,560 --> 00:16:45,359
we do code generation and code

491
00:16:44,079 --> 00:16:48,560
generation

492
00:16:45,360 --> 00:16:50,800
has works by going bottom up

493
00:16:48,560 --> 00:16:52,880
on the graph hierarchy so what i mean by

494
00:16:50,800 --> 00:16:55,599
that is we start at the leaf

495
00:16:52,880 --> 00:16:58,160
graphs the the smallest graphs or lower

496
00:16:55,600 --> 00:17:00,160
most graphs i guess in the hierarchy

497
00:16:58,160 --> 00:17:02,079
where the computations are and move up

498
00:17:00,160 --> 00:17:05,520
the hierarchy through the parents

499
00:17:02,079 --> 00:17:07,599
and every node can be translated for one

500
00:17:05,520 --> 00:17:09,760
or more target elements

501
00:17:07,599 --> 00:17:11,359
so one of the key there are two key

502
00:17:09,760 --> 00:17:12,559
features about the code generation here

503
00:17:11,359 --> 00:17:15,678
one of them is that

504
00:17:12,559 --> 00:17:18,000
any node in the in an hpvm program

505
00:17:15,679 --> 00:17:19,199
can be translated for any target

506
00:17:18,000 --> 00:17:22,880
processing element

507
00:17:19,199 --> 00:17:24,319
in the heterogeneous system in practice

508
00:17:22,880 --> 00:17:26,400
it may be the case that you get

509
00:17:24,319 --> 00:17:27,438
very bad performance if you translate it

510
00:17:26,400 --> 00:17:30,080
for a system

511
00:17:27,439 --> 00:17:32,320
for a node where the application code or

512
00:17:30,080 --> 00:17:33,760
the algorithm is not well suited but in

513
00:17:32,320 --> 00:17:35,120
principle that translation is always

514
00:17:33,760 --> 00:17:36,799
possible

515
00:17:35,120 --> 00:17:38,639
and practice you can get multiple

516
00:17:36,799 --> 00:17:41,120
different targets that do well

517
00:17:38,640 --> 00:17:43,200
with us with the same node the second

518
00:17:41,120 --> 00:17:44,879
feature of this is that we do our best

519
00:17:43,200 --> 00:17:46,960
to be able to reuse

520
00:17:44,880 --> 00:17:48,400
rendered vendor developed back ends

521
00:17:46,960 --> 00:17:50,000
because vendors

522
00:17:48,400 --> 00:17:51,520
have often put tremendous amount of

523
00:17:50,000 --> 00:17:53,120
effort into optimizing

524
00:17:51,520 --> 00:17:55,600
code generation for their target

525
00:17:53,120 --> 00:17:57,360
hardware and so being able to reuse that

526
00:17:55,600 --> 00:18:01,120
will save a tremendous both

527
00:17:57,360 --> 00:18:02,879
a lot of investment and also get

528
00:18:01,120 --> 00:18:05,039
very likely much better performance than

529
00:18:02,880 --> 00:18:06,240
what either we could do or a new project

530
00:18:05,039 --> 00:18:09,360
team could do

531
00:18:06,240 --> 00:18:13,200
and so in particular we can uh

532
00:18:09,360 --> 00:18:15,918
we support code generation for

533
00:18:13,200 --> 00:18:18,480
a host processor and uh to date we've

534
00:18:15,919 --> 00:18:19,600
only supported x864 but we could easily

535
00:18:18,480 --> 00:18:23,120
do other

536
00:18:19,600 --> 00:18:26,840
um hosts like risc-5 and arm

537
00:18:23,120 --> 00:18:28,080
and amd that are already supported in

538
00:18:26,840 --> 00:18:32,959
llvm

539
00:18:28,080 --> 00:18:34,000
we use intel's sphere back end for avx

540
00:18:32,960 --> 00:18:36,080
to do vector

541
00:18:34,000 --> 00:18:37,440
uh inst vector code generation and

542
00:18:36,080 --> 00:18:40,080
they've really tuned

543
00:18:37,440 --> 00:18:41,600
um spear for doing very good vector code

544
00:18:40,080 --> 00:18:44,639
generation

545
00:18:41,600 --> 00:18:48,559
we use the nvptx backend to do

546
00:18:44,640 --> 00:18:51,600
a code generation 2 ptx for nvidia gpus

547
00:18:48,559 --> 00:18:52,879
um that is actually an older version of

548
00:18:51,600 --> 00:18:54,000
the infrastructure i'll come back to

549
00:18:52,880 --> 00:18:55,919
that in a moment

550
00:18:54,000 --> 00:18:57,679
this is what we use for our experiments

551
00:18:55,919 --> 00:18:59,520
and the numbers i'm going to show you

552
00:18:57,679 --> 00:19:02,559
the open source release has used a newer

553
00:18:59,520 --> 00:19:05,280
version that uses opencl instead

554
00:19:02,559 --> 00:19:06,799
um and then we also have a back end for

555
00:19:05,280 --> 00:19:08,879
altera fpgas

556
00:19:06,799 --> 00:19:10,799
which uses altera's tool chain called

557
00:19:08,880 --> 00:19:13,919
aoc which is an

558
00:19:10,799 --> 00:19:14,879
hls tool for programming fpgas using

559
00:19:13,919 --> 00:19:17,840
opencl

560
00:19:14,880 --> 00:19:19,039
so we translate hpvm back to opencl in

561
00:19:17,840 --> 00:19:22,879
order to go through

562
00:19:19,039 --> 00:19:26,160
to hp to fpgas

563
00:19:22,880 --> 00:19:28,480
and one more feature that this enables

564
00:19:26,160 --> 00:19:31,280
so as i said every node can be mapped to

565
00:19:28,480 --> 00:19:33,520
any target processing element in the

566
00:19:31,280 --> 00:19:35,120
underlying heterogeneous system so if

567
00:19:33,520 --> 00:19:35,360
you think about this conceptually if you

568
00:19:35,120 --> 00:19:38,159
have

569
00:19:35,360 --> 00:19:40,399
n graph nodes and you have k different

570
00:19:38,160 --> 00:19:42,240
processing elements in the target system

571
00:19:40,400 --> 00:19:43,600
that gives you k to the power of n

572
00:19:42,240 --> 00:19:46,799
possible mappings

573
00:19:43,600 --> 00:19:48,879
just static mappings so possible um

574
00:19:46,799 --> 00:19:50,000
different combinations of code that you

575
00:19:48,880 --> 00:19:53,360
could generate from

576
00:19:50,000 --> 00:19:54,799
from a single hp vm program um and i'll

577
00:19:53,360 --> 00:19:56,799
show you an example of the different

578
00:19:54,799 --> 00:20:00,000
performance impacts this can have

579
00:19:56,799 --> 00:20:01,200
for the uh the graph pipeline the edge

580
00:20:00,000 --> 00:20:02,240
processing pipeline i was talking about

581
00:20:01,200 --> 00:20:05,280
earlier

582
00:20:02,240 --> 00:20:07,039
this also enables dynamic scheduling

583
00:20:05,280 --> 00:20:08,720
that can be much more flexible than if

584
00:20:07,039 --> 00:20:09,760
you didn't have this kind of flexible

585
00:20:08,720 --> 00:20:12,640
mapping

586
00:20:09,760 --> 00:20:14,080
so in particular we can modify the

587
00:20:12,640 --> 00:20:17,039
mappings of

588
00:20:14,080 --> 00:20:17,678
graph nodes to processing elements at

589
00:20:17,039 --> 00:20:19,679
runtime

590
00:20:17,679 --> 00:20:21,280
there's some restrictions on what we can

591
00:20:19,679 --> 00:20:22,960
map and when we can map

592
00:20:21,280 --> 00:20:24,799
but essentially when you start executing

593
00:20:22,960 --> 00:20:26,400
a node you can choose where to

594
00:20:24,799 --> 00:20:28,080
execute it and the next time you start

595
00:20:26,400 --> 00:20:29,600
you can execute it somewhere else

596
00:20:28,080 --> 00:20:32,720
which is the dynamic feature that you

597
00:20:29,600 --> 00:20:32,719
can that you can support

598
00:20:32,880 --> 00:20:36,000
so i'm going to present some performance

599
00:20:35,280 --> 00:20:38,960
results

600
00:20:36,000 --> 00:20:41,039
um i'm not going to do everything in

601
00:20:38,960 --> 00:20:44,159
detail just for lack of time

602
00:20:41,039 --> 00:20:44,799
but the uh the target system we used is

603
00:20:44,159 --> 00:20:48,640
a mult

604
00:20:44,799 --> 00:20:52,000
is a system where the multi-core host

605
00:20:48,640 --> 00:20:55,440
avx vector instructions on the host

606
00:20:52,000 --> 00:20:58,320
and an nvidia gtx 680 gpu

607
00:20:55,440 --> 00:21:00,400
with 15 36 cores and 2 gigabyte of ram

608
00:20:58,320 --> 00:21:00,720
and the first experiment we did was to

609
00:21:00,400 --> 00:21:04,240
see

610
00:21:00,720 --> 00:21:06,480
what kind of performance impact you get

611
00:21:04,240 --> 00:21:08,480
for if you compare hand written or hand

612
00:21:06,480 --> 00:21:11,280
tuned i should say hand tuned

613
00:21:08,480 --> 00:21:13,760
um opencl code that's been tuned

614
00:21:11,280 --> 00:21:16,480
separately for gpu versus c

615
00:21:13,760 --> 00:21:18,400
versus vector versus if you take the

616
00:21:16,480 --> 00:21:19,440
same code and run it on both gpu and

617
00:21:18,400 --> 00:21:22,480
vector

618
00:21:19,440 --> 00:21:23,200
hardware and uh the benchmarks we used

619
00:21:22,480 --> 00:21:24,480
for that

620
00:21:23,200 --> 00:21:26,480
are a bunch of benchmarks from the

621
00:21:24,480 --> 00:21:28,799
parboil suite which are

622
00:21:26,480 --> 00:21:30,400
multiple versions we use the opencl

623
00:21:28,799 --> 00:21:32,240
versions there and

624
00:21:30,400 --> 00:21:33,679
they have been hand tuned for both gpu

625
00:21:32,240 --> 00:21:36,640
and avx

626
00:21:33,679 --> 00:21:39,520
and this just lists which version we use

627
00:21:36,640 --> 00:21:39,520
for each one of them

628
00:21:39,919 --> 00:21:46,400
so for that experiment this graph

629
00:21:43,440 --> 00:21:47,120
shows the normalized execution time

630
00:21:46,400 --> 00:21:50,159
comparing

631
00:21:47,120 --> 00:21:52,879
hpvm to

632
00:21:50,159 --> 00:21:53,760
the hand coded baseline which is the

633
00:21:52,880 --> 00:21:57,440
right hand bar

634
00:21:53,760 --> 00:22:00,320
for each benchmark so so each bar here

635
00:21:57,440 --> 00:22:02,559
1.0 shows the performance of the opencl

636
00:22:00,320 --> 00:22:04,720
baseline code

637
00:22:02,559 --> 00:22:05,678
and that's the right hand bar the left

638
00:22:04,720 --> 00:22:08,880
hand bar is the

639
00:22:05,679 --> 00:22:10,799
is the hp vm bar and

640
00:22:08,880 --> 00:22:12,799
in this case because it's normalizing

641
00:22:10,799 --> 00:22:15,520
execution time lower is better

642
00:22:12,799 --> 00:22:17,120
so the the hope is that hpvm doesn't

643
00:22:15,520 --> 00:22:19,440
introduce a penalty

644
00:22:17,120 --> 00:22:20,719
for being hardware agnostic right this

645
00:22:19,440 --> 00:22:22,559
is the best we can do

646
00:22:20,720 --> 00:22:24,159
or in fact in theory the best we should

647
00:22:22,559 --> 00:22:27,280
be able to do is to match the

648
00:22:24,159 --> 00:22:27,840
hand coded hardware uh in practice we

649
00:22:27,280 --> 00:22:31,678
come

650
00:22:27,840 --> 00:22:35,678
very close in most cases um i think the

651
00:22:31,679 --> 00:22:39,679
biggest discrepancies are in sgm and bfs

652
00:22:35,679 --> 00:22:41,120
um where there's an extra longer kernel

653
00:22:39,679 --> 00:22:42,960
a copy time

654
00:22:41,120 --> 00:22:45,199
that was not being optimized by the

655
00:22:42,960 --> 00:22:50,400
compiler and that led to about a 22

656
00:22:45,200 --> 00:22:53,600
slowdown in uh bfs and similarly in sgm

657
00:22:50,400 --> 00:22:56,559
um and so

658
00:22:53,600 --> 00:22:58,080
the bottom line is that on gpus we are

659
00:22:56,559 --> 00:23:03,039
reasonably competitive with

660
00:22:58,080 --> 00:23:05,360
hand-coded uh opencl code for the gpu

661
00:23:03,039 --> 00:23:06,720
uh on av access stories almost as good

662
00:23:05,360 --> 00:23:08,639
although there's a performance penalty

663
00:23:06,720 --> 00:23:10,320
in lbn that's somewhat worse

664
00:23:08,640 --> 00:23:11,840
but again the point here is that we're

665
00:23:10,320 --> 00:23:13,039
taking the same code that i showed you

666
00:23:11,840 --> 00:23:14,879
in the previous graph

667
00:23:13,039 --> 00:23:16,240
compiled for gpus here we're compiling

668
00:23:14,880 --> 00:23:18,559
it for avx

669
00:23:16,240 --> 00:23:19,280
but comparing it against the hand tuned

670
00:23:18,559 --> 00:23:22,639
code for

671
00:23:19,280 --> 00:23:24,158
avx and in most cases we are hpvm is

672
00:23:22,640 --> 00:23:27,600
competitive with

673
00:23:24,159 --> 00:23:28,960
um the hand-tuned opencl except in the

674
00:23:27,600 --> 00:23:33,199
case of lbm

675
00:23:28,960 --> 00:23:33,200
and in the case of lbm um

676
00:23:33,440 --> 00:23:38,240
we checked the instructions that are

677
00:23:37,280 --> 00:23:41,279
being generated

678
00:23:38,240 --> 00:23:43,520
by our back end and they match very

679
00:23:41,279 --> 00:23:46,799
closely with the instructions generated

680
00:23:43,520 --> 00:23:48,320
for the hand tuned opencl

681
00:23:46,799 --> 00:23:50,320
but there was something with the driver

682
00:23:48,320 --> 00:23:52,720
that was causing the performance

683
00:23:50,320 --> 00:23:53,918
penalty which we have not been able to

684
00:23:52,720 --> 00:23:57,120
track down yet

685
00:23:53,919 --> 00:23:58,000
um so i'm not making excuses but i don't

686
00:23:57,120 --> 00:24:00,320
think this is

687
00:23:58,000 --> 00:24:01,440
a fundamental issue in the interest or

688
00:24:00,320 --> 00:24:04,879
in this in the

689
00:24:01,440 --> 00:24:06,559
approach or the abstraction itself

690
00:24:04,880 --> 00:24:08,320
the second experiment we did is to look

691
00:24:06,559 --> 00:24:10,320
at the benefit you can potentially get

692
00:24:08,320 --> 00:24:12,639
with static scheduling

693
00:24:10,320 --> 00:24:14,080
if you remember we said that if you have

694
00:24:12,640 --> 00:24:16,799
if you have

695
00:24:14,080 --> 00:24:17,199
k hardware targets and n nodes you can

696
00:24:16,799 --> 00:24:20,240
get

697
00:24:17,200 --> 00:24:21,120
n to the power of k possible mappings of

698
00:24:20,240 --> 00:24:22,640
the code

699
00:24:21,120 --> 00:24:25,120
and so in this case we have three

700
00:24:22,640 --> 00:24:28,320
targets right cpu gpu and vector

701
00:24:25,120 --> 00:24:30,479
and six pipeline stages where if in that

702
00:24:28,320 --> 00:24:31,918
edge detection pipeline we

703
00:24:30,480 --> 00:24:33,679
ignored the hierarchy and basically

704
00:24:31,919 --> 00:24:34,880
collapsed each top level node into a

705
00:24:33,679 --> 00:24:36,720
single node

706
00:24:34,880 --> 00:24:39,360
um so we have six pipeline stages and

707
00:24:36,720 --> 00:24:41,360
that gives us 729 possible mappings from

708
00:24:39,360 --> 00:24:44,959
one pipeline code

709
00:24:41,360 --> 00:24:47,120
and we just picked say we picked

710
00:24:44,960 --> 00:24:49,360
uh one two three seven so it's been a

711
00:24:47,120 --> 00:24:51,600
while seven arbitrary

712
00:24:49,360 --> 00:24:53,279
combinations of these mappings where

713
00:24:51,600 --> 00:24:56,480
what i mean by this is

714
00:24:53,279 --> 00:24:59,360
um s is for a mapping to just the cpu

715
00:24:56,480 --> 00:25:01,039
uh g is mapping to the gpu and v is

716
00:24:59,360 --> 00:25:03,918
mapping to the cpu with

717
00:25:01,039 --> 00:25:05,200
vector instructions for each node in

718
00:25:03,919 --> 00:25:09,520
this pipeline

719
00:25:05,200 --> 00:25:13,360
and so the sequence um let's say

720
00:25:09,520 --> 00:25:16,158
let's say ggs ggs means

721
00:25:13,360 --> 00:25:17,520
s g g s are the mappings for this

722
00:25:16,159 --> 00:25:19,200
particular case

723
00:25:17,520 --> 00:25:21,120
and what we're showing here is the

724
00:25:19,200 --> 00:25:22,640
performance you get in frames per second

725
00:25:21,120 --> 00:25:24,719
that's the y-axis

726
00:25:22,640 --> 00:25:26,559
so higher is better for different

727
00:25:24,720 --> 00:25:28,240
mappings of this pipeline and this

728
00:25:26,559 --> 00:25:29,678
all of these mappings come from the same

729
00:25:28,240 --> 00:25:31,679
code so

730
00:25:29,679 --> 00:25:33,120
the point here is not that some mappings

731
00:25:31,679 --> 00:25:33,679
do much better than others although that

732
00:25:33,120 --> 00:25:36,399
is

733
00:25:33,679 --> 00:25:38,559
sort of the uh corollary another query

734
00:25:36,400 --> 00:25:40,000
sorry the the predecessor to the point

735
00:25:38,559 --> 00:25:41,918
the point here is that you can have

736
00:25:40,000 --> 00:25:43,360
dramatically different performance with

737
00:25:41,919 --> 00:25:45,279
different mappings

738
00:25:43,360 --> 00:25:48,240
but because you can get all of these

739
00:25:45,279 --> 00:25:50,799
mappings from the same hpvm program

740
00:25:48,240 --> 00:25:52,159
um an optimize an optimizer like an auto

741
00:25:50,799 --> 00:25:55,440
tuner or something else

742
00:25:52,159 --> 00:25:56,960
can optimize the code to try and get as

743
00:25:55,440 --> 00:25:58,720
good performance as possible

744
00:25:56,960 --> 00:26:01,200
so that flexibility for scheduling can

745
00:25:58,720 --> 00:26:04,080
be really powerful

746
00:26:01,200 --> 00:26:05,919
um i don't have time to show you the

747
00:26:04,080 --> 00:26:08,240
dynamic scheduling results but i just

748
00:26:05,919 --> 00:26:10,320
briefly summarize

749
00:26:08,240 --> 00:26:11,760
the results that the main thing that we

750
00:26:10,320 --> 00:26:14,879
showed there is that

751
00:26:11,760 --> 00:26:17,440
if you have a pipeline like this running

752
00:26:14,880 --> 00:26:19,520
on a combination of cpu and gpu

753
00:26:17,440 --> 00:26:20,720
and you interrupt the execution on the

754
00:26:19,520 --> 00:26:22,320
gpu

755
00:26:20,720 --> 00:26:24,240
the application if you didn't have the

756
00:26:22,320 --> 00:26:25,760
dynamic scheduling capability

757
00:26:24,240 --> 00:26:27,279
the performance would completely drop

758
00:26:25,760 --> 00:26:29,039
off a cliff you would basically not be

759
00:26:27,279 --> 00:26:29,760
able to make almost any forward progress

760
00:26:29,039 --> 00:26:31,679
at all

761
00:26:29,760 --> 00:26:33,919
but when you have the dynamic scheduling

762
00:26:31,679 --> 00:26:36,960
capability you can gracefully degrade

763
00:26:33,919 --> 00:26:38,000
and map the same some of those nodes

764
00:26:36,960 --> 00:26:40,240
that are mapped to the gpu

765
00:26:38,000 --> 00:26:41,600
instead you can map them to the cpu and

766
00:26:40,240 --> 00:26:42,480
execute there using the vector

767
00:26:41,600 --> 00:26:46,240
instructions

768
00:26:42,480 --> 00:26:46,240
to get at least reasonable performance

769
00:26:46,320 --> 00:26:52,879
um so just to summarize hpvm

770
00:26:49,679 --> 00:26:54,720
is able to get performance that's that's

771
00:26:52,880 --> 00:26:56,320
reasonably comparable to hand tuned

772
00:26:54,720 --> 00:26:57,520
codes for the different heterogeneous

773
00:26:56,320 --> 00:27:00,559
hardware targets

774
00:26:57,520 --> 00:27:02,639
from a single hpvm program

775
00:27:00,559 --> 00:27:04,320
and the flexibility of static scheduling

776
00:27:02,640 --> 00:27:07,760
gives you

777
00:27:04,320 --> 00:27:10,639
a lot of freedom to be able to optimize

778
00:27:07,760 --> 00:27:12,158
for performance and similarly this

779
00:27:10,640 --> 00:27:15,039
lecture dynamic scheduling

780
00:27:12,159 --> 00:27:16,559
can also enable you to tolerate dynamic

781
00:27:15,039 --> 00:27:18,720
changes in workload

782
00:27:16,559 --> 00:27:20,000
or for example deadlines if you work in

783
00:27:18,720 --> 00:27:21,679
a real-time system or something

784
00:27:20,000 --> 00:27:23,840
like that and one of the things we're

785
00:27:21,679 --> 00:27:26,240
doing in the project with ibm

786
00:27:23,840 --> 00:27:27,760
is or i shouldn't say doing yet but

787
00:27:26,240 --> 00:27:29,760
we're going to do soon

788
00:27:27,760 --> 00:27:30,960
is ibm has been developing a scheduler

789
00:27:29,760 --> 00:27:33,760
called stomp

790
00:27:30,960 --> 00:27:35,039
for real-time scheduling of an

791
00:27:33,760 --> 00:27:36,879
application running on this kind of

792
00:27:35,039 --> 00:27:38,720
domain specific soc

793
00:27:36,880 --> 00:27:39,919
and what we are planning to do is to

794
00:27:38,720 --> 00:27:42,640
integrate the

795
00:27:39,919 --> 00:27:44,399
hpvm dynamics scheduler with stomp in

796
00:27:42,640 --> 00:27:46,000
order to take advantage of this kind of

797
00:27:44,399 --> 00:27:49,520
flexibility of being able to move

798
00:27:46,000 --> 00:27:53,840
computations from one node to another

799
00:27:49,520 --> 00:27:56,158
so we've released up hpvm open source

800
00:27:53,840 --> 00:27:56,879
um which includes the ir and a verify

801
00:27:56,159 --> 00:27:59,679
pass

802
00:27:56,880 --> 00:28:01,120
this is hpvm is an extension of llvm as

803
00:27:59,679 --> 00:28:04,559
i said this is

804
00:28:01,120 --> 00:28:06,639
right now using llvm9 um we have a front

805
00:28:04,559 --> 00:28:08,080
end that lowers from hpvmc the c

806
00:28:06,640 --> 00:28:10,320
extension of hpvm

807
00:28:08,080 --> 00:28:12,240
to the dataflow graphs and the open

808
00:28:10,320 --> 00:28:13,279
source release only includes the nvidia

809
00:28:12,240 --> 00:28:17,120
gpu

810
00:28:13,279 --> 00:28:20,799
and intel or amd hosts um

811
00:28:17,120 --> 00:28:22,879
we do plan to release avx

812
00:28:20,799 --> 00:28:24,559
but there's some limitation with the

813
00:28:22,880 --> 00:28:27,200
spear drivers with the openc

814
00:28:24,559 --> 00:28:28,720
intel opencl drivers that we weren't

815
00:28:27,200 --> 00:28:31,039
able to test really enough

816
00:28:28,720 --> 00:28:32,960
to be able to release yet and hopefully

817
00:28:31,039 --> 00:28:34,720
soon in the future the fpga back-end as

818
00:28:32,960 --> 00:28:37,520
well

819
00:28:34,720 --> 00:28:38,640
and there's also a documentation for the

820
00:28:37,520 --> 00:28:41,679
ir

821
00:28:38,640 --> 00:28:44,799
for the c programming interface for this

822
00:28:41,679 --> 00:28:46,159
and an installation guide and a much

823
00:28:44,799 --> 00:28:47,279
better test infrastructure than what

824
00:28:46,159 --> 00:28:50,559
we've been using

825
00:28:47,279 --> 00:28:52,080
so far internally that

826
00:28:50,559 --> 00:28:54,559
includes both regression tests and unit

827
00:28:52,080 --> 00:28:56,480
tests but also the parboil benchmarks

828
00:28:54,559 --> 00:29:00,559
the edge detection pipeline

829
00:28:56,480 --> 00:29:03,440
and uh an image sorry a camera pipeline

830
00:29:00,559 --> 00:29:04,799
for uh image processing that we've been

831
00:29:03,440 --> 00:29:07,760
using in the

832
00:29:04,799 --> 00:29:10,639
in the ibm project as a sort of a pipe

833
00:29:07,760 --> 00:29:10,640
cleaner application

834
00:29:10,720 --> 00:29:15,039
so all of those come with the release

835
00:29:12,720 --> 00:29:17,279
and the main changes we've made to

836
00:29:15,039 --> 00:29:19,600
to the code for this release are one of

837
00:29:17,279 --> 00:29:21,919
them was to port to llvm9 we were on a

838
00:29:19,600 --> 00:29:23,520
much older version for quite a long time

839
00:29:21,919 --> 00:29:25,760
and that's really the bulk of the work

840
00:29:23,520 --> 00:29:29,039
was that to make that

841
00:29:25,760 --> 00:29:31,440
more up-to-date we've also

842
00:29:29,039 --> 00:29:33,120
developed a new back-end from llvm to

843
00:29:31,440 --> 00:29:36,080
opencl

844
00:29:33,120 --> 00:29:37,520
based on the so long ago if you all some

845
00:29:36,080 --> 00:29:39,199
of you might remember there used to be a

846
00:29:37,520 --> 00:29:40,879
c back-end for lvm

847
00:29:39,200 --> 00:29:43,600
which had been abandoned for a while and

848
00:29:40,880 --> 00:29:45,039
turned out julia the julia project mit

849
00:29:43,600 --> 00:29:46,639
had revived that

850
00:29:45,039 --> 00:29:48,399
and it's much better now it's in much

851
00:29:46,640 --> 00:29:51,360
better shape it's more robust

852
00:29:48,399 --> 00:29:52,239
we used the julia c back end extended it

853
00:29:51,360 --> 00:29:55,918
to

854
00:29:52,240 --> 00:29:59,440
emit opencl as the back end

855
00:29:55,919 --> 00:30:02,559
and uh we use that to compile to ptx

856
00:29:59,440 --> 00:30:03,600
now instead of using the nvptx backend

857
00:30:02,559 --> 00:30:05,279
from before

858
00:30:03,600 --> 00:30:08,000
because we found a significant

859
00:30:05,279 --> 00:30:11,520
incompatibility between the ptx back end

860
00:30:08,000 --> 00:30:15,520
and the current uh nvidia drivers

861
00:30:11,520 --> 00:30:17,760
um and a better testing framework like i

862
00:30:15,520 --> 00:30:20,240
mentioned

863
00:30:17,760 --> 00:30:21,360
so the main piece of work we're doing

864
00:30:20,240 --> 00:30:22,640
right now well there's a there's a

865
00:30:21,360 --> 00:30:24,320
couple of things we're doing right now

866
00:30:22,640 --> 00:30:27,520
and then a couple of things that we are

867
00:30:24,320 --> 00:30:30,480
hoping to start on very soon um

868
00:30:27,520 --> 00:30:32,000
the hp vm to fpga code generation work

869
00:30:30,480 --> 00:30:34,159
our goal there

870
00:30:32,000 --> 00:30:35,360
is so first this is a bit of a research

871
00:30:34,159 --> 00:30:36,960
pro it's more than a bit it does a

872
00:30:35,360 --> 00:30:38,959
research project at this point

873
00:30:36,960 --> 00:30:41,360
but the goal there is to enable hardware

874
00:30:38,960 --> 00:30:42,960
agnostic programming of fpgas

875
00:30:41,360 --> 00:30:44,719
and you know traditionally you all know

876
00:30:42,960 --> 00:30:45,679
fpgas has been used widely for embedded

877
00:30:44,720 --> 00:30:46,960
systems they've been used for

878
00:30:45,679 --> 00:30:49,520
prototyping

879
00:30:46,960 --> 00:30:51,440
hardware designs but in all those cases

880
00:30:49,520 --> 00:30:53,520
typically you have a hardware designer

881
00:30:51,440 --> 00:30:55,520
on the team who understands hardware

882
00:30:53,520 --> 00:30:57,679
and can tune the hardware design really

883
00:30:55,520 --> 00:30:59,279
well things are starting to change in

884
00:30:57,679 --> 00:31:01,440
the fpga world because you're starting

885
00:30:59,279 --> 00:31:05,120
to get fpgas available in

886
00:31:01,440 --> 00:31:09,760
places like the in aws and also in

887
00:31:05,120 --> 00:31:11,600
azure where our open software teams can

888
00:31:09,760 --> 00:31:13,200
try to use fpgas to accelerate their

889
00:31:11,600 --> 00:31:14,799
applications but many software teams

890
00:31:13,200 --> 00:31:17,039
today don't have that kind of

891
00:31:14,799 --> 00:31:19,279
hardware expertise in-house and so

892
00:31:17,039 --> 00:31:21,519
there's much greater need these days

893
00:31:19,279 --> 00:31:22,559
for ha for doing more hardware agnostic

894
00:31:21,519 --> 00:31:24,799
kind of programming

895
00:31:22,559 --> 00:31:25,600
and even today's hls tools don't come

896
00:31:24,799 --> 00:31:27,840
close to that

897
00:31:25,600 --> 00:31:30,879
even though they're much simpler to use

898
00:31:27,840 --> 00:31:33,039
than verilog of vhdl

899
00:31:30,880 --> 00:31:34,640
you still have a lot of pragmas and a

900
00:31:33,039 --> 00:31:35,760
lot of hardware knowledge that you have

901
00:31:34,640 --> 00:31:38,559
to bring to bear

902
00:31:35,760 --> 00:31:39,919
in order to tune in and optimize code

903
00:31:38,559 --> 00:31:41,760
for an fpga

904
00:31:39,919 --> 00:31:43,519
so the goal in this project is to be

905
00:31:41,760 --> 00:31:45,120
able to use compiler transformations and

906
00:31:43,519 --> 00:31:48,240
optimizations

907
00:31:45,120 --> 00:31:49,840
starting with hpvm as the abstraction of

908
00:31:48,240 --> 00:31:51,919
parallelism

909
00:31:49,840 --> 00:31:54,000
in order to generate much better code

910
00:31:51,919 --> 00:31:55,440
for the fpga now we are not likely to

911
00:31:54,000 --> 00:31:57,120
match rtl

912
00:31:55,440 --> 00:31:59,679
but if we can come with a factor of two

913
00:31:57,120 --> 00:32:02,559
or a factor of four of hand coded rtl

914
00:31:59,679 --> 00:32:04,320
with almost no hand hardware knowledge

915
00:32:02,559 --> 00:32:06,879
we think that's going to be pretty

916
00:32:04,320 --> 00:32:08,480
uh useful for many teams so that's sort

917
00:32:06,880 --> 00:32:10,720
of the goal that we're aiming for

918
00:32:08,480 --> 00:32:12,080
if the compiler people among you will

919
00:32:10,720 --> 00:32:13,840
probably remember that

920
00:32:12,080 --> 00:32:15,120
john backus used to say if he could come

921
00:32:13,840 --> 00:32:17,360
within a factor of two

922
00:32:15,120 --> 00:32:19,120
of handwritten assembly our fortran

923
00:32:17,360 --> 00:32:20,479
compiler will be a success

924
00:32:19,120 --> 00:32:22,158
how many of you know that quote from

925
00:32:20,480 --> 00:32:25,200
john backus

926
00:32:22,159 --> 00:32:26,480
okay very good so this was in the 50s

927
00:32:25,200 --> 00:32:28,320
that they were trying to prove that you

928
00:32:26,480 --> 00:32:30,159
could write code and open in

929
00:32:28,320 --> 00:32:32,158
human written programming languages and

930
00:32:30,159 --> 00:32:34,080
actually compete against assembly code

931
00:32:32,159 --> 00:32:36,320
so this is a similar kind of excuse we

932
00:32:34,080 --> 00:32:38,000
have we want to come in in some factor

933
00:32:36,320 --> 00:32:40,480
the second major goal we have right now

934
00:32:38,000 --> 00:32:41,840
is approximate computing which is

935
00:32:40,480 --> 00:32:43,919
the idea that for many of the

936
00:32:41,840 --> 00:32:45,678
applications at the edge

937
00:32:43,919 --> 00:32:47,679
where energy efficiency is important is

938
00:32:45,679 --> 00:32:48,799
important you also have

939
00:32:47,679 --> 00:32:51,120
quite a significant amount of

940
00:32:48,799 --> 00:32:54,240
flexibility for

941
00:32:51,120 --> 00:32:56,479
reducing accuracy in order to tolerate

942
00:32:54,240 --> 00:32:58,559
um to in order to get better energy

943
00:32:56,480 --> 00:33:00,720
efficiency and better performance

944
00:32:58,559 --> 00:33:02,399
and that's true in many application

945
00:33:00,720 --> 00:33:02,880
application areas like machine learning

946
00:33:02,399 --> 00:33:05,760
and

947
00:33:02,880 --> 00:33:06,640
image processing and a lot of other ones

948
00:33:05,760 --> 00:33:09,279
and

949
00:33:06,640 --> 00:33:10,880
um the problem in practice is that there

950
00:33:09,279 --> 00:33:13,360
are lots of potential

951
00:33:10,880 --> 00:33:15,679
approximation techniques or what we call

952
00:33:13,360 --> 00:33:17,600
accuracy aware optimizations

953
00:33:15,679 --> 00:33:19,440
but using them requires understanding

954
00:33:17,600 --> 00:33:20,959
them in detail and then using the

955
00:33:19,440 --> 00:33:22,480
when there are multiple ones combining

956
00:33:20,960 --> 00:33:23,760
them is very very hard

957
00:33:22,480 --> 00:33:26,480
so what we're trying to do is to

958
00:33:23,760 --> 00:33:29,360
automate that process to a large extent

959
00:33:26,480 --> 00:33:31,679
to make it actually accessible to uh to

960
00:33:29,360 --> 00:33:33,120
ordinary programmers

961
00:33:31,679 --> 00:33:35,519
and that's been the major focus of the

962
00:33:33,120 --> 00:33:37,840
work with the ibm project

963
00:33:35,519 --> 00:33:39,440
we are also starting to develop a new

964
00:33:37,840 --> 00:33:42,080
dsl front end

965
00:33:39,440 --> 00:33:44,399
um in order to achieve interoperability

966
00:33:42,080 --> 00:33:47,918
between dsl for edge applications

967
00:33:44,399 --> 00:33:50,479
and another step we've been looking at

968
00:33:47,919 --> 00:33:52,559
fairly carefully is what benefits they

969
00:33:50,480 --> 00:33:55,039
might be to integrate with mlir because

970
00:33:52,559 --> 00:33:57,039
mlir has some significant advantages

971
00:33:55,039 --> 00:33:58,240
for the kind of work that we're doing

972
00:33:57,039 --> 00:33:59,440
and i'll just say a couple of words

973
00:33:58,240 --> 00:34:02,000
about that although i know i have only a

974
00:33:59,440 --> 00:34:04,159
few minutes left

975
00:34:02,000 --> 00:34:05,360
so for those of you who are not familiar

976
00:34:04,159 --> 00:34:07,600
with mli are this

977
00:34:05,360 --> 00:34:08,560
mlr stands for multi-level ir it's a

978
00:34:07,600 --> 00:34:10,799
framework

979
00:34:08,560 --> 00:34:11,839
essentially for allowing or defining

980
00:34:10,800 --> 00:34:14,800
multiple

981
00:34:11,839 --> 00:34:16,320
different compiler irs allowing them to

982
00:34:14,800 --> 00:34:18,399
inter-operate

983
00:34:16,320 --> 00:34:21,119
and making it much easier to implement

984
00:34:18,399 --> 00:34:23,839
multiple irs like this

985
00:34:21,119 --> 00:34:25,359
and the emphasis in mlir has been for

986
00:34:23,839 --> 00:34:26,320
machine learning or tensor based

987
00:34:25,359 --> 00:34:28,239
applications

988
00:34:26,320 --> 00:34:30,879
so there's a heavy focus on high

989
00:34:28,239 --> 00:34:32,799
dimensional tensors and on polyhedral

990
00:34:30,879 --> 00:34:34,799
transformations on multi-dimensional

991
00:34:32,800 --> 00:34:36,560
loops and so on

992
00:34:34,800 --> 00:34:38,000
um and in order to get this

993
00:34:36,560 --> 00:34:39,759
interoperability

994
00:34:38,000 --> 00:34:41,520
mlr defines different dialects and

995
00:34:39,760 --> 00:34:42,879
there's quite a significant number of

996
00:34:41,520 --> 00:34:44,719
dialects at this point

997
00:34:42,879 --> 00:34:46,319
so examples are things like the affine

998
00:34:44,719 --> 00:34:49,439
dialect for

999
00:34:46,320 --> 00:34:51,440
affine loop transforms and and arrays

1000
00:34:49,440 --> 00:34:52,879
of the llvm dialect which is used for

1001
00:34:51,440 --> 00:34:54,480
for code generation

1002
00:34:52,879 --> 00:34:56,319
linear algebra dialect for high

1003
00:34:54,480 --> 00:34:59,280
performance computing programs gpu

1004
00:34:56,320 --> 00:35:00,720
direct for compiling to ptx and so on

1005
00:34:59,280 --> 00:35:02,640
and one thing we're looking at is

1006
00:35:00,720 --> 00:35:06,799
whether it would make sense to make

1007
00:35:02,640 --> 00:35:08,480
hpvm be another dialect within mlir

1008
00:35:06,800 --> 00:35:10,000
i think the benefit for hpvm would be

1009
00:35:08,480 --> 00:35:12,720
that we would be able to get access to a

1010
00:35:10,000 --> 00:35:15,520
polyhedral framework which we don't have

1011
00:35:12,720 --> 00:35:17,118
directly right now and we might be able

1012
00:35:15,520 --> 00:35:18,560
to get new front ends in the future

1013
00:35:17,119 --> 00:35:20,079
although i don't think there's enough

1014
00:35:18,560 --> 00:35:22,078
there's anything really available today

1015
00:35:20,079 --> 00:35:23,440
that's open source um because the

1016
00:35:22,079 --> 00:35:24,400
tensorflow one i don't think is open

1017
00:35:23,440 --> 00:35:27,760
source

1018
00:35:24,400 --> 00:35:30,640
um mlr may benefit also from

1019
00:35:27,760 --> 00:35:32,240
back ends that they don't have for

1020
00:35:30,640 --> 00:35:33,359
example i don't think there's an fpga

1021
00:35:32,240 --> 00:35:35,200
back-end today

1022
00:35:33,359 --> 00:35:37,279
and we're also starting to develop a

1023
00:35:35,200 --> 00:35:41,520
neural network back-end for

1024
00:35:37,280 --> 00:35:43,440
intel's myriad npu

1025
00:35:41,520 --> 00:35:45,200
and potentially also the accuracy of our

1026
00:35:43,440 --> 00:35:48,400
optimizations that i was mentioning

1027
00:35:45,200 --> 00:35:50,960
a moment ago okay

1028
00:35:48,400 --> 00:35:52,079
so that's all i had to say really the

1029
00:35:50,960 --> 00:35:55,760
project

1030
00:35:52,079 --> 00:35:58,240
aims to get to to develop compilers

1031
00:35:55,760 --> 00:36:00,000
for achieving both programmability and

1032
00:35:58,240 --> 00:36:00,879
performance for heterogeneous parallel

1033
00:36:00,000 --> 00:36:02,720
systems

1034
00:36:00,880 --> 00:36:04,480
and in particular to make it easier to

1035
00:36:02,720 --> 00:36:05,359
build compilers for a variety of

1036
00:36:04,480 --> 00:36:08,320
different

1037
00:36:05,359 --> 00:36:09,759
programming languages including domain

1038
00:36:08,320 --> 00:36:10,960
specific languages and more general

1039
00:36:09,760 --> 00:36:12,480
purpose languages

1040
00:36:10,960 --> 00:36:14,560
and target them to these kinds of

1041
00:36:12,480 --> 00:36:16,800
heterogeneous systems

1042
00:36:14,560 --> 00:36:18,320
and in particular we can use a common

1043
00:36:16,800 --> 00:36:19,920
representation to do both

1044
00:36:18,320 --> 00:36:22,480
to do three different things a virtual

1045
00:36:19,920 --> 00:36:25,359
isa a compiler internal representation

1046
00:36:22,480 --> 00:36:26,000
and a runtime scheduler and that's sort

1047
00:36:25,359 --> 00:36:29,040
of the philosophy

1048
00:36:26,000 --> 00:36:41,839
behind the project and with that i'll

1049
00:36:29,040 --> 00:36:41,839
stop and take questions

1050
00:36:42,880 --> 00:36:47,680
so from talking to my colleagues in the

1051
00:36:46,079 --> 00:36:51,040
gpu department they say

1052
00:36:47,680 --> 00:36:52,319
there's if you program gp gpu using

1053
00:36:51,040 --> 00:36:53,759
opencl or cuda

1054
00:36:52,320 --> 00:36:56,240
there's basically no performance

1055
00:36:53,760 --> 00:36:58,880
portability you have to know

1056
00:36:56,240 --> 00:37:00,479
the micro architecture your accelerator

1057
00:36:58,880 --> 00:37:02,240
that's true the most performance out

1058
00:37:00,480 --> 00:37:04,000
and i think in your presentation i'm not

1059
00:37:02,240 --> 00:37:07,839
entirely sure if i saw anything

1060
00:37:04,000 --> 00:37:09,119
about um once you decide to map onto a

1061
00:37:07,839 --> 00:37:11,759
particular accelerator

1062
00:37:09,119 --> 00:37:14,079
i'm assuming the ir still looks as

1063
00:37:11,760 --> 00:37:16,720
pretty generic ir

1064
00:37:14,079 --> 00:37:18,800
right right right so this is to map to

1065
00:37:16,720 --> 00:37:20,240
the specific micro architecture

1066
00:37:18,800 --> 00:37:22,000
right away that's out of scope for this

1067
00:37:20,240 --> 00:37:25,359
research no so actually that

1068
00:37:22,000 --> 00:37:29,040
is the reason for this where did i

1069
00:37:25,359 --> 00:37:32,560
there we go so this particular step

1070
00:37:29,040 --> 00:37:33,279
where the ir starts out being hardware

1071
00:37:32,560 --> 00:37:34,720
agnostic

1072
00:37:33,280 --> 00:37:36,240
because the front end is lowering it to

1073
00:37:34,720 --> 00:37:38,000
an hpvm representation that's not

1074
00:37:36,240 --> 00:37:40,000
necessarily specific to a hardware

1075
00:37:38,000 --> 00:37:42,560
but to do any optimizations and code

1076
00:37:40,000 --> 00:37:45,040
generation you really have to be

1077
00:37:42,560 --> 00:37:47,520
cognizant of the hardware and so once

1078
00:37:45,040 --> 00:37:48,720
you are targeting a particular gpu for

1079
00:37:47,520 --> 00:37:51,839
example

1080
00:37:48,720 --> 00:37:53,919
this is very very gpu specific right

1081
00:37:51,839 --> 00:37:55,520
and so this is no longer agnostic at all

1082
00:37:53,920 --> 00:37:56,800
of the particular gpu and so you're

1083
00:37:55,520 --> 00:37:57,440
absolutely right you really have to be

1084
00:37:56,800 --> 00:37:59,200
tuned

1085
00:37:57,440 --> 00:38:00,720
to the particular gpu in order to do

1086
00:37:59,200 --> 00:38:01,439
that i think we had a question here and

1087
00:38:00,720 --> 00:38:05,279
then one here

1088
00:38:01,440 --> 00:38:05,280
yeah transfer

1089
00:38:09,040 --> 00:38:12,640
i should talk to you offline i would

1090
00:38:10,560 --> 00:38:13,920
love to talk to you offline we do have a

1091
00:38:12,640 --> 00:38:19,839
quantum computing person

1092
00:38:13,920 --> 00:38:19,839
and yeah this would be interesting

1093
00:38:28,480 --> 00:38:32,400
that's a good question so uh the

1094
00:38:30,560 --> 00:38:34,799
question is what is the benefit of using

1095
00:38:32,400 --> 00:38:37,040
llvm here as opposed to building

1096
00:38:34,800 --> 00:38:38,079
the infrastructure from scratch right i

1097
00:38:37,040 --> 00:38:39,279
think that's the question

1098
00:38:38,079 --> 00:38:41,359
so there's a couple of important

1099
00:38:39,280 --> 00:38:43,680
benefits actually one of them is that

1100
00:38:41,359 --> 00:38:45,520
there's a lot of investment in in back

1101
00:38:43,680 --> 00:38:48,160
ends for individual

1102
00:38:45,520 --> 00:38:49,359
or hardware targets in an infrastructure

1103
00:38:48,160 --> 00:38:51,680
like llvm

1104
00:38:49,359 --> 00:38:52,960
right gcc is the same there's a lot of

1105
00:38:51,680 --> 00:38:54,240
different hardware back-ends

1106
00:38:52,960 --> 00:38:56,480
and one of the important things we want

1107
00:38:54,240 --> 00:38:58,078
to do is to reuse these back-ends

1108
00:38:56,480 --> 00:39:00,079
and so we built our own infrastructure

1109
00:38:58,079 --> 00:39:02,480
we won't be able to do that

1110
00:39:00,079 --> 00:39:04,079
so that's one thing um the second is

1111
00:39:02,480 --> 00:39:05,440
that there are a lot of llvm front

1112
00:39:04,079 --> 00:39:07,359
engines so for example in the

1113
00:39:05,440 --> 00:39:08,720
for the c extension we're basically

1114
00:39:07,359 --> 00:39:12,880
using clang

1115
00:39:08,720 --> 00:39:16,480
and we're using llvm passes to translate

1116
00:39:12,880 --> 00:39:18,960
h c with hpv extensions into the hpvm ir

1117
00:39:16,480 --> 00:39:20,320
internally and that's what build dfg

1118
00:39:18,960 --> 00:39:22,800
does here

1119
00:39:20,320 --> 00:39:25,200
and so the front ends also become much

1120
00:39:22,800 --> 00:39:25,200
easier

1121
00:39:26,839 --> 00:39:29,839
yep

1122
00:39:32,640 --> 00:39:37,520
so the hpvm representation captures that

1123
00:39:35,760 --> 00:39:39,440
explicitly either explicitly or

1124
00:39:37,520 --> 00:39:41,119
implicitly depending on whether it's a

1125
00:39:39,440 --> 00:39:42,800
logical copy or whether it's just shared

1126
00:39:41,119 --> 00:39:46,240
memory um

1127
00:39:42,800 --> 00:39:48,560
so we do account for it and for the gpu

1128
00:39:46,240 --> 00:39:50,879
for example we actually can do

1129
00:39:48,560 --> 00:39:52,480
uh tiling for scratch pad in order to

1130
00:39:50,880 --> 00:39:55,680
optimize for

1131
00:39:52,480 --> 00:39:57,599
local memory versus global memory

1132
00:39:55,680 --> 00:39:59,118
so in that sense yes we do account for

1133
00:39:57,599 --> 00:40:02,000
it i think that

1134
00:39:59,119 --> 00:40:03,359
there's quite a significant additional

1135
00:40:02,000 --> 00:40:05,520
piece of work we can do

1136
00:40:03,359 --> 00:40:06,799
to have a better model for the overall

1137
00:40:05,520 --> 00:40:09,200
memory hierarchy

1138
00:40:06,800 --> 00:40:10,160
of the whole system so right now there's

1139
00:40:09,200 --> 00:40:13,359
not really a good

1140
00:40:10,160 --> 00:40:15,520
target memory model um

1141
00:40:13,359 --> 00:40:17,440
so we have logical memory copies and we

1142
00:40:15,520 --> 00:40:18,960
have shared memory loads and stores

1143
00:40:17,440 --> 00:40:20,720
but the architecture is not really

1144
00:40:18,960 --> 00:40:22,319
modeled better than that there are other

1145
00:40:20,720 --> 00:40:24,879
projects that do a much better job like

1146
00:40:22,319 --> 00:40:26,160
the legion project and so i think doing

1147
00:40:24,880 --> 00:40:27,520
something that has a better memory

1148
00:40:26,160 --> 00:40:29,279
target could do a better job of

1149
00:40:27,520 --> 00:40:31,680
optimization across the whole

1150
00:40:29,280 --> 00:40:31,680
program

1151
00:40:36,079 --> 00:40:42,880
okay i'm happy to take questions

1152
00:40:40,079 --> 00:40:44,560
so i have a follow-up first question

1153
00:40:42,880 --> 00:40:48,000
actually a question about this slide

1154
00:40:44,560 --> 00:40:49,759
so for example i have written the

1155
00:40:48,000 --> 00:40:52,240
developer

1156
00:40:49,760 --> 00:40:53,119
matrix multiplication uh three nested

1157
00:40:52,240 --> 00:40:54,799
loops

1158
00:40:53,119 --> 00:40:57,280
it's in the source program so i guess it

1159
00:40:54,800 --> 00:40:58,160
is compiled into some lvm code which

1160
00:40:57,280 --> 00:41:00,800
simulates

1161
00:40:58,160 --> 00:41:02,078
or implements this nested loop so to

1162
00:41:00,800 --> 00:41:04,480
understand correctly that

1163
00:41:02,079 --> 00:41:06,640
once i compile this program further into

1164
00:41:04,480 --> 00:41:09,040
gpu code

1165
00:41:06,640 --> 00:41:11,520
some clever gpu optimizer has to infer

1166
00:41:09,040 --> 00:41:13,920
that this was the matrix multiplication

1167
00:41:11,520 --> 00:41:15,520
and pick a very fast and efficient

1168
00:41:13,920 --> 00:41:17,280
kernel to do this mathematics

1169
00:41:15,520 --> 00:41:20,960
multiplication

1170
00:41:17,280 --> 00:41:22,240
so that's not what we do in hpvm

1171
00:41:20,960 --> 00:41:23,280
or in the sense that we're not trying to

1172
00:41:22,240 --> 00:41:25,118
recognize that it's a matrix

1173
00:41:23,280 --> 00:41:26,720
multiplication kernel

1174
00:41:25,119 --> 00:41:28,720
and which means that we may not be able

1175
00:41:26,720 --> 00:41:29,759
to do as well as targeting let's say a

1176
00:41:28,720 --> 00:41:32,640
tuned

1177
00:41:29,760 --> 00:41:33,760
um if there's a qdnn operation or

1178
00:41:32,640 --> 00:41:34,879
something else to do matrix

1179
00:41:33,760 --> 00:41:37,440
multiplication that's

1180
00:41:34,880 --> 00:41:39,599
hand tuned for a particular gpu we won't

1181
00:41:37,440 --> 00:41:42,160
be able to directly target that

1182
00:41:39,599 --> 00:41:43,920
um instead we would just do standard

1183
00:41:42,160 --> 00:41:45,040
compiler optimizations like tiling and

1184
00:41:43,920 --> 00:41:47,280
other optimizations

1185
00:41:45,040 --> 00:41:48,160
to get as good performance as possible

1186
00:41:47,280 --> 00:41:51,119
right

1187
00:41:48,160 --> 00:41:52,000
in practice so what we've been doing for

1188
00:41:51,119 --> 00:41:53,680
keras and

1189
00:41:52,000 --> 00:41:55,599
in particular what we think is the right

1190
00:41:53,680 --> 00:41:57,759
way to do this for any particular

1191
00:41:55,599 --> 00:42:01,200
important domain like tensors

1192
00:41:57,760 --> 00:42:02,800
is we extend hpvm further with tensor

1193
00:42:01,200 --> 00:42:05,598
intrinsics so we've added tensor

1194
00:42:02,800 --> 00:42:07,359
intrinsics to the hpvm representation

1195
00:42:05,599 --> 00:42:09,680
and tensor intrinsics now are a higher

1196
00:42:07,359 --> 00:42:11,839
level piece of information that we can

1197
00:42:09,680 --> 00:42:13,598
target to something like qdn

1198
00:42:11,839 --> 00:42:15,839
or a hardware accelerator or something

1199
00:42:13,599 --> 00:42:16,400
else without having to reverse engineer

1200
00:42:15,839 --> 00:42:20,078
what

1201
00:42:16,400 --> 00:42:23,119
the the kernel is another way to do it

1202
00:42:20,079 --> 00:42:26,960
instead of adding intrinsics in this way

1203
00:42:23,119 --> 00:42:28,800
is that another team on our project

1204
00:42:26,960 --> 00:42:30,400
the harvard folks david brooks and his

1205
00:42:28,800 --> 00:42:32,880
people are doing

1206
00:42:30,400 --> 00:42:34,480
some work to use tracing to

1207
00:42:32,880 --> 00:42:37,119
automatically detect by

1208
00:42:34,480 --> 00:42:38,079
specific cons or extract a dynamic trace

1209
00:42:37,119 --> 00:42:39,599
of execution

1210
00:42:38,079 --> 00:42:42,000
of basic blocks and do some pattern

1211
00:42:39,599 --> 00:42:42,560
matching probabilistically to match that

1212
00:42:42,000 --> 00:42:44,319
to

1213
00:42:42,560 --> 00:42:45,759
similar traces for a particular

1214
00:42:44,319 --> 00:42:47,440
computational kernel

1215
00:42:45,760 --> 00:42:48,800
so it might be an fft or a matrix

1216
00:42:47,440 --> 00:42:50,480
multiply or something else

1217
00:42:48,800 --> 00:42:52,560
and that tells you that this is a matrix

1218
00:42:50,480 --> 00:42:55,040
multiply and so now we can

1219
00:42:52,560 --> 00:42:55,759
directly target an accelerator in that

1220
00:42:55,040 --> 00:42:57,599
way

1221
00:42:55,760 --> 00:42:59,280
and so there we're going to basically

1222
00:42:57,599 --> 00:43:01,040
take input from their tool

1223
00:42:59,280 --> 00:43:02,400
to figure out what the accelerator is to

1224
00:43:01,040 --> 00:43:04,720
be targeting for

1225
00:43:02,400 --> 00:43:06,160
so that's the integration with how with

1226
00:43:04,720 --> 00:43:08,560
harvard that we're doing in the ibm

1227
00:43:06,160 --> 00:43:08,560
project

1228
00:43:10,839 --> 00:43:16,960
um

1229
00:43:12,960 --> 00:43:20,480
um are you open to let's say manual

1230
00:43:16,960 --> 00:43:23,599
um override for specific architectures

1231
00:43:20,480 --> 00:43:26,560
i suppose uh you the programmer is

1232
00:43:23,599 --> 00:43:28,319
it knows okay this piece is a bottleneck

1233
00:43:26,560 --> 00:43:30,960
or maybe he knows already hey

1234
00:43:28,319 --> 00:43:32,319
if i run it on a cuda you might use a

1235
00:43:30,960 --> 00:43:34,800
native clip on there

1236
00:43:32,319 --> 00:43:35,599
uh yeah that you can make specifications

1237
00:43:34,800 --> 00:43:39,760
for or

1238
00:43:35,599 --> 00:43:42,720
specific so yeah so you know

1239
00:43:39,760 --> 00:43:43,440
this was a these were powerpoint slides

1240
00:43:42,720 --> 00:43:47,759
in

1241
00:43:43,440 --> 00:43:49,520
we don't have a intelligent scheduler

1242
00:43:47,760 --> 00:43:51,200
right all the only numbers i showed you

1243
00:43:49,520 --> 00:43:53,520
was the potential that you could

1244
00:43:51,200 --> 00:43:55,279
build a very sophisticated scheduler in

1245
00:43:53,520 --> 00:43:56,400
practice what we do is to actually

1246
00:43:55,280 --> 00:43:59,280
do attributes just like you're

1247
00:43:56,400 --> 00:44:02,640
describing to say map this

1248
00:43:59,280 --> 00:44:04,240
node dfg node to this hardware device or

1249
00:44:02,640 --> 00:44:06,480
this set of hardware devices

1250
00:44:04,240 --> 00:44:08,160
as a set of choices and then we use that

1251
00:44:06,480 --> 00:44:10,160
to guide the compiler and so in practice

1252
00:44:08,160 --> 00:44:12,078
yes you can certainly put in attributes

1253
00:44:10,160 --> 00:44:13,598
to say which nodes should be mapped to

1254
00:44:12,079 --> 00:44:15,680
which hardware differs

1255
00:44:13,599 --> 00:44:17,359
to accelerate it could you also hint

1256
00:44:15,680 --> 00:44:21,919
okay stop doing this

1257
00:44:17,359 --> 00:44:23,440
in c but use this qr library uh

1258
00:44:21,920 --> 00:44:24,800
function yeah yeah yeah sure you can

1259
00:44:23,440 --> 00:44:26,240
basically make those without to be on

1260
00:44:24,800 --> 00:44:28,160
sincerity

1261
00:44:26,240 --> 00:44:29,919
yeah sorry i didn't see you earlier back

1262
00:44:28,160 --> 00:44:31,920
there

1263
00:44:29,920 --> 00:44:34,960
i was giving a hard time about it i know

1264
00:44:31,920 --> 00:44:36,720
i noticed go ahead

1265
00:44:34,960 --> 00:44:38,720
made a claim that um fundamentally i

1266
00:44:36,720 --> 00:44:40,959
think i agree right that all

1267
00:44:38,720 --> 00:44:41,839
parallel applications can be represented

1268
00:44:40,960 --> 00:44:44,640
as a

1269
00:44:41,839 --> 00:44:45,359
hierarchical data for that right so i

1270
00:44:44,640 --> 00:44:48,000
didn't say

1271
00:44:45,359 --> 00:44:48,880
all right at least i hope i didn't i

1272
00:44:48,000 --> 00:44:51,200
think that

1273
00:44:48,880 --> 00:44:51,920
what i okay so what i intended to say is

1274
00:44:51,200 --> 00:44:55,040
that

1275
00:44:51,920 --> 00:44:55,680
parallel applications that will that are

1276
00:44:55,040 --> 00:44:57,920
suitable

1277
00:44:55,680 --> 00:44:59,680
for heterogeneous parallel systems so

1278
00:44:57,920 --> 00:45:00,400
accelerators in hydrogenous parallel

1279
00:44:59,680 --> 00:45:03,200
systems

1280
00:45:00,400 --> 00:45:05,359
tend to be data parallel or some

1281
00:45:03,200 --> 00:45:08,078
combination of data parallel and

1282
00:45:05,359 --> 00:45:10,160
by streaming a pipeline right these are

1283
00:45:08,079 --> 00:45:12,000
not arbitrary threaded concurrent

1284
00:45:10,160 --> 00:45:14,000
parallel computations

1285
00:45:12,000 --> 00:45:15,680
and in particular i think there are

1286
00:45:14,000 --> 00:45:18,240
classes of parallelism that are not well

1287
00:45:15,680 --> 00:45:20,960
suited for hpvm or vice versa

1288
00:45:18,240 --> 00:45:21,839
and there are lots of multi-threaded

1289
00:45:20,960 --> 00:45:23,599
parallel

1290
00:45:21,839 --> 00:45:25,200
computations that i wouldn't want to try

1291
00:45:23,599 --> 00:45:33,839
and compile with this

1292
00:45:25,200 --> 00:45:33,839
so that brings you to my actual question

1293
00:45:38,079 --> 00:45:43,280
so how would you evaluate your claim

1294
00:45:40,160 --> 00:45:43,279
yeah so it's a good question

1295
00:45:43,440 --> 00:45:47,839
yeah so um

1296
00:45:48,640 --> 00:45:52,000
i don't think there's an ideal way to do

1297
00:45:50,160 --> 00:45:53,680
it because this is not

1298
00:45:52,000 --> 00:45:55,280
there's an infinite set of possible

1299
00:45:53,680 --> 00:45:58,319
parallel um

1300
00:45:55,280 --> 00:45:59,119
hardware designs um and heterogeneous

1301
00:45:58,319 --> 00:46:01,200
systems

1302
00:45:59,119 --> 00:46:03,040
i think that this will only happen by a

1303
00:46:01,200 --> 00:46:05,439
accumulation of evidence

1304
00:46:03,040 --> 00:46:06,880
and in practice what we hope to be able

1305
00:46:05,440 --> 00:46:09,920
to show is that

1306
00:46:06,880 --> 00:46:11,920
as you compile to systems with a few

1307
00:46:09,920 --> 00:46:14,319
different combinations of accelerators

1308
00:46:11,920 --> 00:46:16,400
we're able to get very good performance

1309
00:46:14,319 --> 00:46:18,880
or some definition of very good

1310
00:46:16,400 --> 00:46:20,319
and we're basically talking about some

1311
00:46:18,880 --> 00:46:22,560
coming to some factors

1312
00:46:20,319 --> 00:46:23,359
within some factor of hand tuned code

1313
00:46:22,560 --> 00:46:27,200
right

1314
00:46:23,359 --> 00:46:28,960
and that's i think the best practical

1315
00:46:27,200 --> 00:46:30,879
way to be able to make that

1316
00:46:28,960 --> 00:46:32,960
or to to provide evidence to justify

1317
00:46:30,880 --> 00:46:35,280
that claim i'll be honest with you

1318
00:46:32,960 --> 00:46:36,960
in practice what we found is that you

1319
00:46:35,280 --> 00:46:37,760
start doing this research and we did

1320
00:46:36,960 --> 00:46:38,960
that

1321
00:46:37,760 --> 00:46:41,359
and there's many interesting

1322
00:46:38,960 --> 00:46:42,960
applications of it and the students are

1323
00:46:41,359 --> 00:46:44,160
moving on and so we're now

1324
00:46:42,960 --> 00:46:46,480
moving to a phase where we're not trying

1325
00:46:44,160 --> 00:46:47,520
to prove this anymore we are now trying

1326
00:46:46,480 --> 00:46:49,760
to use this

1327
00:46:47,520 --> 00:46:51,440
for interesting and practical projects

1328
00:46:49,760 --> 00:46:52,319
so in the ibm project for example we're

1329
00:46:51,440 --> 00:46:54,480
trying to use this

1330
00:46:52,319 --> 00:46:56,640
as a way to design socs or make it

1331
00:46:54,480 --> 00:46:58,960
easier to design and program socs

1332
00:46:56,640 --> 00:47:01,200
and so i think there are many different

1333
00:46:58,960 --> 00:47:02,560
uh applications or goals you can have

1334
00:47:01,200 --> 00:47:03,200
and the approximate computing work we're

1335
00:47:02,560 --> 00:47:05,520
doing

1336
00:47:03,200 --> 00:47:07,200
is a way to make edge applications much

1337
00:47:05,520 --> 00:47:11,200
more energy efficient

1338
00:47:07,200 --> 00:47:14,160
so this mobile robot for for ag robots

1339
00:47:11,200 --> 00:47:17,118
or autonomous vehicles energy efficiency

1340
00:47:14,160 --> 00:47:17,118
is a major goal there

1341
00:47:18,319 --> 00:47:21,440
anyone who wants to leave an opportunity

1342
00:47:20,240 --> 00:47:23,598
to leave yes

1343
00:47:21,440 --> 00:47:25,040
okay and i'm happy to stay and keep

1344
00:47:23,599 --> 00:47:26,160
answering answering questions or talk

1345
00:47:25,040 --> 00:47:27,920
but

1346
00:47:26,160 --> 00:47:30,078
the announcement is people are welcome

1347
00:47:27,920 --> 00:47:33,359
to leave if they would like to

1348
00:47:30,079 --> 00:47:34,800
also i was going to ask

1349
00:47:33,359 --> 00:47:36,558
only because this makes a good last

1350
00:47:34,800 --> 00:47:38,559
question sure

1351
00:47:36,559 --> 00:47:40,000
you you like raised a red flag right at

1352
00:47:38,559 --> 00:47:41,119
the very end saying that you're going to

1353
00:47:40,000 --> 00:47:43,680
solve the

1354
00:47:41,119 --> 00:47:48,400
high-level language to fpga problem

1355
00:47:43,680 --> 00:47:48,399
which i've been burned by so many times

1356
00:47:53,359 --> 00:47:56,720
well i think the reason you're soaking

1357
00:47:54,640 --> 00:47:57,520
cynicism honestly i have talked to lots

1358
00:47:56,720 --> 00:47:59,118
of fpga pro

1359
00:47:57,520 --> 00:48:01,359
not a lot i've talked to some fpga

1360
00:47:59,119 --> 00:48:04,000
programmers and experts

1361
00:48:01,359 --> 00:48:05,520
is because what i usually find at least

1362
00:48:04,000 --> 00:48:07,440
when talking to them and i'm

1363
00:48:05,520 --> 00:48:08,720
assuming it's the same with you is that

1364
00:48:07,440 --> 00:48:10,000
they are coming from a background of

1365
00:48:08,720 --> 00:48:11,919
having hand tuned

1366
00:48:10,000 --> 00:48:14,079
or comparing mentally against some

1367
00:48:11,920 --> 00:48:16,559
hand-tuned baseline

1368
00:48:14,079 --> 00:48:18,079
and we are not trying to achieve that we

1369
00:48:16,559 --> 00:48:20,480
are not claiming that we will compete

1370
00:48:18,079 --> 00:48:22,000
with rtl

1371
00:48:20,480 --> 00:48:23,839
when you give these things quite

1372
00:48:22,000 --> 00:48:27,440
problems it looks beautiful

1373
00:48:23,839 --> 00:48:27,440
you give it like an intermediate problem

1374
00:48:30,160 --> 00:48:34,078
so you're saying that basically there's

1375
00:48:31,599 --> 00:48:36,000
no it'll be so far off any reasonable

1376
00:48:34,079 --> 00:48:39,200
performance that it won't be useful

1377
00:48:36,000 --> 00:48:42,480
yeah so that's

1378
00:48:39,200 --> 00:48:42,480
reasonably well and then

1379
00:48:42,880 --> 00:48:46,400
yeah so honestly i don't have the answer

1380
00:48:44,880 --> 00:48:49,040
to that because we are too early in that

1381
00:48:46,400 --> 00:48:50,319
process we are only doing so we what

1382
00:48:49,040 --> 00:48:52,640
we've done so far for example is that

1383
00:48:50,319 --> 00:48:56,000
image processing it's a camera pipeline

1384
00:48:52,640 --> 00:48:59,118
uh we've compiled it to altera it runs

1385
00:48:56,000 --> 00:49:00,240
reasonably fast and it um we've been

1386
00:48:59,119 --> 00:49:02,240
able to get the

1387
00:49:00,240 --> 00:49:03,839
for example the what is the initiation

1388
00:49:02,240 --> 00:49:04,558
interval was very bad when we started

1389
00:49:03,839 --> 00:49:06,078
with and

1390
00:49:04,559 --> 00:49:08,640
with some compiler optimizations we were

1391
00:49:06,079 --> 00:49:12,079
able to get it down to a well pipelined

1392
00:49:08,640 --> 00:49:13,200
single cycle uh pipeline um and that

1393
00:49:12,079 --> 00:49:16,800
fits in that

1394
00:49:13,200 --> 00:49:18,399
altera chip um is that

1395
00:49:16,800 --> 00:49:20,240
enough i don't think so i think more

1396
00:49:18,400 --> 00:49:22,640
complex applications we'll have to

1397
00:49:20,240 --> 00:49:23,839
the jury's definitely out and i think in

1398
00:49:22,640 --> 00:49:25,200
practice it's going to take many

1399
00:49:23,839 --> 00:49:26,240
significantly more compiler

1400
00:49:25,200 --> 00:49:29,040
transformations

1401
00:49:26,240 --> 00:49:30,399
in order to make this possible but until

1402
00:49:29,040 --> 00:49:31,599
we can do that and then see what it

1403
00:49:30,400 --> 00:49:34,640
works out like

1404
00:49:31,599 --> 00:49:37,200
i don't know but yeah this is

1405
00:49:34,640 --> 00:49:39,279
literally a research goal and a you know

1406
00:49:37,200 --> 00:49:43,598
an aim so far

1407
00:49:39,280 --> 00:49:46,640
yep yeah if you want to leave

1408
00:49:43,599 --> 00:49:46,640
prefacing the question

1409
00:49:55,200 --> 00:49:59,040
is every node independent you said

1410
00:50:05,200 --> 00:50:09,598
right so um one of the things which i

1411
00:50:08,720 --> 00:50:12,640
did not show

1412
00:50:09,599 --> 00:50:14,480
in the intrinsics is that we use a

1413
00:50:12,640 --> 00:50:16,799
runtime memory tracker

1414
00:50:14,480 --> 00:50:18,400
to track the current location of every

1415
00:50:16,800 --> 00:50:21,280
allocated memory object

1416
00:50:18,400 --> 00:50:23,359
so hpvm malloc allocates a memory object

1417
00:50:21,280 --> 00:50:26,079
and for example

1418
00:50:23,359 --> 00:50:26,720
if you try if you compute on one device

1419
00:50:26,079 --> 00:50:28,160
and move

1420
00:50:26,720 --> 00:50:30,078
and then you need to access on another

1421
00:50:28,160 --> 00:50:30,558
device we know that it's on the first

1422
00:50:30,079 --> 00:50:32,160
one

1423
00:50:30,559 --> 00:50:33,680
if you compute on one device and this

1424
00:50:32,160 --> 00:50:36,000
next

1425
00:50:33,680 --> 00:50:37,598
node dfg node computes on the same

1426
00:50:36,000 --> 00:50:39,359
device we know that they're both on the

1427
00:50:37,599 --> 00:50:40,319
same device the runtime tracker keeps

1428
00:50:39,359 --> 00:50:41,759
track of that

1429
00:50:40,319 --> 00:50:43,839
so the runtime memory tracker it's not

1430
00:50:41,760 --> 00:50:45,359
very complicated but it's essential to

1431
00:50:43,839 --> 00:50:48,000
be able to do exactly the optimization

1432
00:50:45,359 --> 00:50:48,000
you're talking about

1433
00:50:48,960 --> 00:50:57,669
okay yes thank you

1434
00:50:54,150 --> 00:50:57,669
[Applause]

1435
00:51:00,720 --> 00:51:02,799
you

