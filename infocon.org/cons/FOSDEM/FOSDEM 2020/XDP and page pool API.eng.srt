1
00:00:05,200 --> 00:00:08,000
hi

2
00:00:06,000 --> 00:00:09,440
we're going to present uh xdp and the

3
00:00:08,000 --> 00:00:10,800
page pool allocator and how you can

4
00:00:09,440 --> 00:00:12,719
easily add the driver

5
00:00:10,800 --> 00:00:14,480
or convent convert an existing linux

6
00:00:12,719 --> 00:00:17,600
driver into using xdp

7
00:00:14,480 --> 00:00:19,760
by using an internal api uh the reason

8
00:00:17,600 --> 00:00:21,359
we decided to talk is that i'm amelia

9
00:00:19,760 --> 00:00:22,960
i'm the technical lead for linaro heads

10
00:00:21,359 --> 00:00:24,640
and fog networking department and i'm

11
00:00:22,960 --> 00:00:26,640
serving as a co-maintainer for the page

12
00:00:24,640 --> 00:00:29,199
pool api at the moment

13
00:00:26,640 --> 00:00:30,560
uh i've added xdp support on a kernel

14
00:00:29,199 --> 00:00:32,320
driver

15
00:00:30,560 --> 00:00:33,920
and lorenzo is a software engineer for

16
00:00:32,320 --> 00:00:34,559
red hat he's maintaining a wireless

17
00:00:33,920 --> 00:00:38,000
driver it's

18
00:00:34,559 --> 00:00:39,919
mt76 right and he's added xdp support on

19
00:00:38,000 --> 00:00:40,480
the espresso bin board which uses the

20
00:00:39,920 --> 00:00:44,160
marvel

21
00:00:40,480 --> 00:00:48,319
mvneta driver so uh

22
00:00:44,160 --> 00:00:50,480
do you know what xtp is anyone does

23
00:00:48,320 --> 00:00:51,440
all right good let's let's go a bit

24
00:00:50,480 --> 00:00:53,599
faster on this one

25
00:00:51,440 --> 00:00:55,199
uh it's a it's a software offload path

26
00:00:53,600 --> 00:00:58,320
for the kernel

27
00:00:55,199 --> 00:00:59,839
uh we on the driver level we add some

28
00:00:58,320 --> 00:01:01,920
hooks on the rx path

29
00:00:59,840 --> 00:01:03,280
and by using the page pull api for the

30
00:01:01,920 --> 00:01:05,040
memory allocation

31
00:01:03,280 --> 00:01:06,400
we don't have to keep reallocating and

32
00:01:05,040 --> 00:01:07,840
freeing memory when we process the

33
00:01:06,400 --> 00:01:10,240
packets we just have to sync to the

34
00:01:07,840 --> 00:01:11,920
correct direction the dma direction

35
00:01:10,240 --> 00:01:13,520
for the cpu and the network interface to

36
00:01:11,920 --> 00:01:15,920
pick up the packets

37
00:01:13,520 --> 00:01:18,158
uh it was initially designed as an to

38
00:01:15,920 --> 00:01:19,759
operate on layer 2 and layer 3

39
00:01:18,159 --> 00:01:21,759
while the linux kernel operates on layer

40
00:01:19,759 --> 00:01:25,759
2 to say layer 7 but it's mostly

41
00:01:21,759 --> 00:01:28,080
optimized around layer 4 around layer 4.

42
00:01:25,759 --> 00:01:30,240
there's two reasons we get uh better

43
00:01:28,080 --> 00:01:31,520
performance on xdp the first one is that

44
00:01:30,240 --> 00:01:33,039
we

45
00:01:31,520 --> 00:01:35,280
on most of the cases we managed to

46
00:01:33,040 --> 00:01:37,840
recycle the memory we're using

47
00:01:35,280 --> 00:01:40,240
and we skip all the kernel paths that we

48
00:01:37,840 --> 00:01:43,439
don't really want like ip tables or

49
00:01:40,240 --> 00:01:46,399
the tc hook or the

50
00:01:43,439 --> 00:01:47,839
root root lookups and stuff like that uh

51
00:01:46,399 --> 00:01:50,560
it's important to keep in mind that this

52
00:01:47,840 --> 00:01:52,240
is not a kernel bypass

53
00:01:50,560 --> 00:01:54,079
one of the functionalities is a kernel

54
00:01:52,240 --> 00:01:55,759
bypass and you can dump packets directly

55
00:01:54,079 --> 00:01:57,679
to user space but it's

56
00:01:55,759 --> 00:01:59,280
it's a kernel it's an internal fast path

57
00:01:57,680 --> 00:02:02,560
and we'll we'll elaborate on this

58
00:01:59,280 --> 00:02:04,560
a bit more uh it it uses ex

59
00:02:02,560 --> 00:02:06,479
existing kernel apis and existing kernel

60
00:02:04,560 --> 00:02:09,520
functionality and you can program the

61
00:02:06,479 --> 00:02:10,800
number of packets and the the the type

62
00:02:09,520 --> 00:02:12,879
of the packets you want to process

63
00:02:10,800 --> 00:02:15,360
through bpf

64
00:02:12,879 --> 00:02:16,160
uh it's currently being used by facebook

65
00:02:15,360 --> 00:02:20,400
and cloudfare

66
00:02:16,160 --> 00:02:22,160
on load balancers on ddos protection

67
00:02:20,400 --> 00:02:23,840
so this is pretty much how the

68
00:02:22,160 --> 00:02:25,760
architecture looks like uh

69
00:02:23,840 --> 00:02:27,280
the the xdp you see on the driver is

70
00:02:25,760 --> 00:02:28,160
actually the bpf program doing the

71
00:02:27,280 --> 00:02:30,560
decision

72
00:02:28,160 --> 00:02:32,560
making so if it's an xdp pass which is

73
00:02:30,560 --> 00:02:34,080
one of the actions you have on xdp

74
00:02:32,560 --> 00:02:36,400
you send back the packet to the linux

75
00:02:34,080 --> 00:02:38,080
networking stuff it's if it's a tx and

76
00:02:36,400 --> 00:02:39,519
xdp tx

77
00:02:38,080 --> 00:02:41,200
what you do is that you send the packet

78
00:02:39,519 --> 00:02:42,879
back out of the interface it came from

79
00:02:41,200 --> 00:02:44,480
by changing the header or the source and

80
00:02:42,879 --> 00:02:46,319
mac address or

81
00:02:44,480 --> 00:02:47,518
the ip or anything you want to change on

82
00:02:46,319 --> 00:02:49,599
the packet

83
00:02:47,519 --> 00:02:51,599
the xdp redirect currently sends the

84
00:02:49,599 --> 00:02:52,399
packet over to user space or a remote

85
00:02:51,599 --> 00:02:55,119
cpu

86
00:02:52,400 --> 00:02:57,840
or anything you decide on that one and

87
00:02:55,120 --> 00:02:59,360
there's the ndo xdpx mid which you can

88
00:02:57,840 --> 00:03:01,360
pick up the packet the moment it arrives

89
00:02:59,360 --> 00:03:02,720
on your network interface and offload it

90
00:03:01,360 --> 00:03:04,400
to a different network card

91
00:03:02,720 --> 00:03:06,560
without having to go through the kernel

92
00:03:04,400 --> 00:03:08,159
network stack

93
00:03:06,560 --> 00:03:09,599
now the reason we created the page pull

94
00:03:08,159 --> 00:03:11,599
api is that the

95
00:03:09,599 --> 00:03:13,760
memory model for the whole approach is a

96
00:03:11,599 --> 00:03:16,399
bit weird

97
00:03:13,760 --> 00:03:18,079
we require packets to be in contiguous

98
00:03:16,400 --> 00:03:20,239
physical memory and this is not a

99
00:03:18,080 --> 00:03:21,360
requirement from us it's it comes from

100
00:03:20,239 --> 00:03:23,280
the bpf

101
00:03:21,360 --> 00:03:26,239
direct access for validating the packet

102
00:03:23,280 --> 00:03:26,239
and the correctness it

103
00:03:26,319 --> 00:03:29,359
so you can and you can't have one packet

104
00:03:29,040 --> 00:03:31,920
uh

105
00:03:29,360 --> 00:03:33,920
split across multiple physical pages at

106
00:03:31,920 --> 00:03:36,559
the moment so you're limited to

107
00:03:33,920 --> 00:03:37,280
uh non-jumbo frames and we don't we

108
00:03:36,560 --> 00:03:39,280
don't mean

109
00:03:37,280 --> 00:03:40,640
one thousand five thousand by byte

110
00:03:39,280 --> 00:03:42,640
packets it's just

111
00:03:40,640 --> 00:03:45,440
anything below a page size can be

112
00:03:42,640 --> 00:03:47,920
accommodated in xtp frame

113
00:03:45,440 --> 00:03:49,040
uh so the problem with that is that you

114
00:03:47,920 --> 00:03:52,319
cannot allocate

115
00:03:49,040 --> 00:03:53,679
uh the memory you want with whatever we

116
00:03:52,319 --> 00:03:56,640
have in the kernel

117
00:03:53,680 --> 00:03:58,080
uh of uh like functions like napier log

118
00:03:56,640 --> 00:03:59,679
frag which allocates fragments for your

119
00:03:58,080 --> 00:04:00,000
data and it's faster because we cast

120
00:03:59,680 --> 00:04:02,560
things

121
00:04:00,000 --> 00:04:04,319
in there you you really have to allocate

122
00:04:02,560 --> 00:04:05,840
the page you have to account for the

123
00:04:04,319 --> 00:04:07,359
headroom and the tail size we need on

124
00:04:05,840 --> 00:04:09,360
bpf

125
00:04:07,360 --> 00:04:11,439
uh and for the whatever you need on the

126
00:04:09,360 --> 00:04:12,799
skb now the

127
00:04:11,439 --> 00:04:14,319
what we discussed is that the buffers

128
00:04:12,799 --> 00:04:15,599
must be recycled in order to get the

129
00:04:14,319 --> 00:04:18,880
speed

130
00:04:15,599 --> 00:04:21,039
so the paid pool allocator we have it's

131
00:04:18,880 --> 00:04:22,880
optimized for one packet per page

132
00:04:21,040 --> 00:04:24,160
there we have use cases of people

133
00:04:22,880 --> 00:04:26,080
splitting the page and fitting it

134
00:04:24,160 --> 00:04:28,800
multiple packets in it

135
00:04:26,080 --> 00:04:30,479
but you you can't recycle based on the

136
00:04:28,800 --> 00:04:31,919
paintball allocator recycling functions

137
00:04:30,479 --> 00:04:32,880
you have to recycle on your own on that

138
00:04:31,919 --> 00:04:35,758
case

139
00:04:32,880 --> 00:04:36,400
uh on the native packet recycling we do

140
00:04:35,759 --> 00:04:38,320
it

141
00:04:36,400 --> 00:04:39,919
in the napi context mostly so this is

142
00:04:38,320 --> 00:04:41,360
really fast because you don't have extra

143
00:04:39,919 --> 00:04:43,840
any extra locking overhead you're

144
00:04:41,360 --> 00:04:46,960
already protected by the nappy context

145
00:04:43,840 --> 00:04:48,560
and the api also offers dma managing

146
00:04:46,960 --> 00:04:50,159
capabilities that means that it can map

147
00:04:48,560 --> 00:04:51,919
your buffers it can sync your buffers

148
00:04:50,160 --> 00:04:55,680
correctly and there's improvements from

149
00:04:51,919 --> 00:04:58,080
lorenzo that speed up this even more

150
00:04:55,680 --> 00:04:58,720
now this is not all perfect if you if

151
00:04:58,080 --> 00:05:01,840
you switch

152
00:04:58,720 --> 00:05:03,120
from uh napi a locus kb that the candle

153
00:05:01,840 --> 00:05:05,039
is doing to an xtp

154
00:05:03,120 --> 00:05:06,479
your network stack your normal network

155
00:05:05,039 --> 00:05:08,320
stack in the kernel is going to slow

156
00:05:06,479 --> 00:05:09,840
down because allocating a page

157
00:05:08,320 --> 00:05:11,360
compared to allocating fragments it's

158
00:05:09,840 --> 00:05:13,198
substantially slower

159
00:05:11,360 --> 00:05:14,960
but if you use if you use it for xdp

160
00:05:13,199 --> 00:05:17,840
then you get all the native

161
00:05:14,960 --> 00:05:19,680
uh performance improvements we have by

162
00:05:17,840 --> 00:05:21,599
with recycling packets

163
00:05:19,680 --> 00:05:23,360
the memory footprint is bigger because

164
00:05:21,600 --> 00:05:25,199
instead of allocating the

165
00:05:23,360 --> 00:05:27,280
the the amount of size you want for the

166
00:05:25,199 --> 00:05:28,880
packet you allocate the page and you fit

167
00:05:27,280 --> 00:05:30,719
the packet wherever you want in that

168
00:05:28,880 --> 00:05:33,520
page

169
00:05:30,720 --> 00:05:34,720
and we do have some off three patches uh

170
00:05:33,520 --> 00:05:36,639
to

171
00:05:34,720 --> 00:05:38,880
get some performance back from that

172
00:05:36,639 --> 00:05:41,280
penalty so

173
00:05:38,880 --> 00:05:43,360
we we have patches and we managed to

174
00:05:41,280 --> 00:05:45,440
recycle buffers even if they

175
00:05:43,360 --> 00:05:47,440
address the normal network stack so if

176
00:05:45,440 --> 00:05:50,160
it's an skb we eventually recycle that

177
00:05:47,440 --> 00:05:50,160
buffer as well

178
00:05:50,840 --> 00:05:56,000
so

179
00:05:52,720 --> 00:05:58,479
so actually uh

180
00:05:56,000 --> 00:05:59,360
elias has gone through some xp

181
00:05:58,479 --> 00:06:01,440
requirement

182
00:05:59,360 --> 00:06:02,479
and some more general information about

183
00:06:01,440 --> 00:06:05,280
xtp

184
00:06:02,479 --> 00:06:05,599
and i will give some more details about

185
00:06:05,280 --> 00:06:08,799
how

186
00:06:05,600 --> 00:06:11,360
implement xdp in an internet driver

187
00:06:08,800 --> 00:06:12,400
actually i use the mvneta marvel one

188
00:06:11,360 --> 00:06:15,520
gigabit driver

189
00:06:12,400 --> 00:06:16,960
as reference since for example the intel

190
00:06:15,520 --> 00:06:18,479
or the

191
00:06:16,960 --> 00:06:22,719
marvel implementation melanox

192
00:06:18,479 --> 00:06:25,360
implementation is are much more complex

193
00:06:22,720 --> 00:06:27,919
we need to take into account that in

194
00:06:25,360 --> 00:06:30,880
order to be accepted in the linux kernel

195
00:06:27,919 --> 00:06:32,000
our driver needs to implement all the

196
00:06:30,880 --> 00:06:36,159
possible

197
00:06:32,000 --> 00:06:40,400
xtp verdict that are xdp drop hdptx

198
00:06:36,160 --> 00:06:40,400
http pass and xtp redirect

199
00:06:41,680 --> 00:06:46,319
here i reported some other specification

200
00:06:44,560 --> 00:06:49,360
of the marvel espresso bin

201
00:06:46,319 --> 00:06:50,960
that is the development board i used to

202
00:06:49,360 --> 00:06:55,199
add xtp support

203
00:06:50,960 --> 00:06:58,000
to the mvneta driver and we can see that

204
00:06:55,199 --> 00:06:59,039
the marvel espresso bin runs a cortex

205
00:06:58,000 --> 00:07:02,000
a53

206
00:06:59,039 --> 00:07:03,599
and for networking we have two gigabit

207
00:07:02,000 --> 00:07:06,639
internet lan port

208
00:07:03,599 --> 00:07:10,000
one ethernet one port all of them

209
00:07:06,639 --> 00:07:14,000
connected together through an ethernet

210
00:07:10,000 --> 00:07:14,000
internet dsa hardware switch

211
00:07:15,440 --> 00:07:21,199
this diagram outline the

212
00:07:18,720 --> 00:07:22,639
life cycle of a buffer using a page pool

213
00:07:21,199 --> 00:07:25,199
allocator

214
00:07:22,639 --> 00:07:27,120
and we can see that it is the page pool

215
00:07:25,199 --> 00:07:29,919
allocator is usually created

216
00:07:27,120 --> 00:07:31,120
opening an interface since it is

217
00:07:29,919 --> 00:07:34,240
actually

218
00:07:31,120 --> 00:07:37,199
associated to a given

219
00:07:34,240 --> 00:07:40,080
risk risk skew in order to avoid the

220
00:07:37,199 --> 00:07:43,039
locking penalties

221
00:07:40,080 --> 00:07:44,080
from here we can see that it's possible

222
00:07:43,039 --> 00:07:47,599
to rely on

223
00:07:44,080 --> 00:07:51,359
to rely on the page pool api in order to

224
00:07:47,599 --> 00:07:54,639
uh for dma mapping and image thinking

225
00:07:51,360 --> 00:07:56,400
using using deskflex actually

226
00:07:54,639 --> 00:07:58,960
and what is important to notice in this

227
00:07:56,400 --> 00:08:02,560
slide is that when the nappy poll runs

228
00:07:58,960 --> 00:08:05,758
it actually run any dpf program that is

229
00:08:02,560 --> 00:08:09,120
attached to our network interface

230
00:08:05,759 --> 00:08:12,879
and this dbpf program ebpf program

231
00:08:09,120 --> 00:08:15,759
will return any dpf an xdp verdict

232
00:08:12,879 --> 00:08:16,800
let's say a result then the buffer will

233
00:08:15,759 --> 00:08:20,080
be recycled

234
00:08:16,800 --> 00:08:22,639
according to the to this result actually

235
00:08:20,080 --> 00:08:23,680
the pitch pull allocator will have two

236
00:08:22,639 --> 00:08:27,360
caches

237
00:08:23,680 --> 00:08:30,560
one in interrupt caches that is used

238
00:08:27,360 --> 00:08:32,240
this one actually and the that is used

239
00:08:30,560 --> 00:08:34,000
for when the driver is running in

240
00:08:32,240 --> 00:08:35,599
interrupt context and we have a single

241
00:08:34,000 --> 00:08:38,159
reference to the buffer

242
00:08:35,599 --> 00:08:40,959
or a pointer in cache that is used when

243
00:08:38,159 --> 00:08:44,000
we have a single reference to the buffer

244
00:08:40,958 --> 00:08:46,319
whenever our driver needs to refill the

245
00:08:44,000 --> 00:08:47,839
dma engine with the new buffer running

246
00:08:46,320 --> 00:08:51,120
for example in this case

247
00:08:47,839 --> 00:08:52,240
the net rx refill we can access to this

248
00:08:51,120 --> 00:08:56,320
caches instead of

249
00:08:52,240 --> 00:08:56,320
going through this lower page allocator

250
00:08:58,320 --> 00:09:03,279
here i reported the marv the member neta

251
00:09:01,680 --> 00:09:05,920
xdp architecture

252
00:09:03,279 --> 00:09:08,000
and you can see that whenever the mevo

253
00:09:05,920 --> 00:09:10,160
netapp runs it allocates a next

254
00:09:08,000 --> 00:09:13,440
unique sdp buffer that is the

255
00:09:10,160 --> 00:09:16,640
counterpart of an skb for http

256
00:09:13,440 --> 00:09:19,440
and the neteronxdp will actually run the

257
00:09:16,640 --> 00:09:20,640
ebpf program in the ebpf sandbox on our

258
00:09:19,440 --> 00:09:23,680
xtp buffer

259
00:09:20,640 --> 00:09:27,839
and will retire one of these xtp verdict

260
00:09:23,680 --> 00:09:30,399
xdb pass xtp drop tx on red or redirect

261
00:09:27,839 --> 00:09:32,959
and the buffer will be managed accorded

262
00:09:30,399 --> 00:09:36,160
accordingly

263
00:09:32,959 --> 00:09:39,279
it's important to notice here that

264
00:09:36,160 --> 00:09:40,800
the xtp buffer destructix dp buff is

265
00:09:39,279 --> 00:09:45,120
allocated on the stack

266
00:09:40,800 --> 00:09:50,399
and not through a kmm cache as is done

267
00:09:45,120 --> 00:09:53,279
for for a classic skb

268
00:09:50,399 --> 00:09:54,000
now let's go through each possible xdp

269
00:09:53,279 --> 00:09:57,839
verdict

270
00:09:54,000 --> 00:10:00,720
and let's consider xdp drop xdp drop

271
00:09:57,839 --> 00:10:02,399
is returned by our ebpf program when it

272
00:10:00,720 --> 00:10:05,279
wants to drop the packet

273
00:10:02,399 --> 00:10:07,839
as fast as he can and the typical use

274
00:10:05,279 --> 00:10:09,519
case for xtp drop is an anti-ddos

275
00:10:07,839 --> 00:10:12,240
application

276
00:10:09,519 --> 00:10:14,160
we can see here that whenever the packet

277
00:10:12,240 --> 00:10:18,640
return xdp drop

278
00:10:14,160 --> 00:10:20,160
the packet will be recycled in the in

279
00:10:18,640 --> 00:10:22,800
interrupt cache

280
00:10:20,160 --> 00:10:23,760
using page pool recycled redirect and

281
00:10:22,800 --> 00:10:27,120
moreover here

282
00:10:23,760 --> 00:10:29,519
i reported the

283
00:10:27,120 --> 00:10:32,160
comparison between a simple program that

284
00:10:29,519 --> 00:10:34,399
just run xdp drop

285
00:10:32,160 --> 00:10:35,600
and the same functionality implemented

286
00:10:34,399 --> 00:10:37,760
with tc

287
00:10:35,600 --> 00:10:39,519
with a dc filter ntc action and we can

288
00:10:37,760 --> 00:10:42,720
see that with tc

289
00:10:39,519 --> 00:10:45,519
with the xtpp with xdp we can

290
00:10:42,720 --> 00:10:46,959
almost reach 600 kilopackets per second

291
00:10:45,519 --> 00:10:51,279
drop it while with

292
00:10:46,959 --> 00:10:54,160
dc we can just roughly drop 180 kilo

293
00:10:51,279 --> 00:10:54,160
packets per second

294
00:10:54,560 --> 00:11:01,119
here we see the xt pt x how is the

295
00:10:57,920 --> 00:11:03,040
xdpdx works in the mvneta driver and

296
00:11:01,120 --> 00:11:04,880
xtptx is used to

297
00:11:03,040 --> 00:11:07,599
transmit the packet back to the

298
00:11:04,880 --> 00:11:10,959
interface where we receive the packet

299
00:11:07,600 --> 00:11:12,800
we can see that now typical application

300
00:11:10,959 --> 00:11:14,000
for example is a load balancer in this

301
00:11:12,800 --> 00:11:16,560
case we can see

302
00:11:14,000 --> 00:11:18,160
that now that running the memonetics

303
00:11:16,560 --> 00:11:21,839
depicts mid-back

304
00:11:18,160 --> 00:11:25,120
it's not the i mean the m1x

305
00:11:21,839 --> 00:11:28,240
xdp xmid back will reinsert the packet

306
00:11:25,120 --> 00:11:28,720
in the in the hardware the m8x ring and

307
00:11:28,240 --> 00:11:30,959
is not

308
00:11:28,720 --> 00:11:32,160
important in this case to the energy map

309
00:11:30,959 --> 00:11:34,959
the buffer since it

310
00:11:32,160 --> 00:11:35,680
has been already mapped by the page pool

311
00:11:34,959 --> 00:11:39,040
api

312
00:11:35,680 --> 00:11:40,079
we just need to flash the cpu caches in

313
00:11:39,040 --> 00:11:43,839
this case because

314
00:11:40,079 --> 00:11:43,839
the device is not current

315
00:11:44,560 --> 00:11:47,839
here we have the xdp redirect that is

316
00:11:46,720 --> 00:11:50,639
xdp redirect

317
00:11:47,839 --> 00:11:52,240
is used to transmit the packet to for

318
00:11:50,639 --> 00:11:55,519
example a remote interface

319
00:11:52,240 --> 00:11:58,560
or av or sorry

320
00:11:55,519 --> 00:12:02,000
remote cpu or or

321
00:11:58,560 --> 00:12:02,959
to even a socket using for example af

322
00:12:02,000 --> 00:12:06,320
xdp

323
00:12:02,959 --> 00:12:08,638
and the typical use case is like

324
00:12:06,320 --> 00:12:10,399
for example layer 2 forwarding it's

325
00:12:08,639 --> 00:12:12,160
important to notice here that

326
00:12:10,399 --> 00:12:14,240
in order to redirect to a diff to a

327
00:12:12,160 --> 00:12:16,160
remote interface for example

328
00:12:14,240 --> 00:12:17,360
uh the remote the device should

329
00:12:16,160 --> 00:12:20,319
implement the

330
00:12:17,360 --> 00:12:22,240
ndo redirect exmit function pointer and

331
00:12:20,320 --> 00:12:24,320
here we have the implementation done for

332
00:12:22,240 --> 00:12:26,720
the mevuneta

333
00:12:24,320 --> 00:12:28,399
we notice here that in this case is

334
00:12:26,720 --> 00:12:29,120
necessary to remove the mirror map the

335
00:12:28,399 --> 00:12:31,920
buffer

336
00:12:29,120 --> 00:12:32,240
since it is being received actually by

337
00:12:31,920 --> 00:12:35,599
from

338
00:12:32,240 --> 00:12:35,600
remote from remote device

339
00:12:35,680 --> 00:12:40,319
last verdict is xdp pass xdp pass is

340
00:12:39,440 --> 00:12:42,560
used to

341
00:12:40,320 --> 00:12:43,920
send the packet to the standard linux

342
00:12:42,560 --> 00:12:46,638
networking stack

343
00:12:43,920 --> 00:12:48,399
and in the net implementation we can see

344
00:12:46,639 --> 00:12:51,680
that we can rely on the

345
00:12:48,399 --> 00:12:52,959
build skb so there is no need to

346
00:12:51,680 --> 00:12:56,079
reallocate

347
00:12:52,959 --> 00:12:57,040
the buff the buffer for the payload of

348
00:12:56,079 --> 00:12:59,680
the packet

349
00:12:57,040 --> 00:13:01,439
but we need to take into account when we

350
00:12:59,680 --> 00:13:03,760
allocate the packet using the

351
00:13:01,440 --> 00:13:05,680
page pool api that we need to take into

352
00:13:03,760 --> 00:13:09,760
account even the size of the

353
00:13:05,680 --> 00:13:11,040
skb shared info what we notice moreover

354
00:13:09,760 --> 00:13:13,279
in this slide is that

355
00:13:11,040 --> 00:13:15,279
uh in this particular case we are not

356
00:13:13,279 --> 00:13:18,560
able to recycle the buffer yet

357
00:13:15,279 --> 00:13:19,600
since when whenever we need to refill

358
00:13:18,560 --> 00:13:22,479
the dimension

359
00:13:19,600 --> 00:13:24,800
with new buffers we need to go through

360
00:13:22,480 --> 00:13:27,440
the standard page allocator but

361
00:13:24,800 --> 00:13:29,920
as you just said this feature is under

362
00:13:27,440 --> 00:13:33,120
developing

363
00:13:29,920 --> 00:13:35,839
so in conclusion we saw

364
00:13:33,120 --> 00:13:37,279
some xdp requirements and some basic

365
00:13:35,839 --> 00:13:41,040
information about ccdp

366
00:13:37,279 --> 00:13:44,240
like xdp memory model we saw some basic

367
00:13:41,040 --> 00:13:45,040
about the page pool allocator and how to

368
00:13:44,240 --> 00:13:49,199
implement

369
00:13:45,040 --> 00:13:52,079
each xdp verdict we using this api

370
00:13:49,199 --> 00:13:53,760
and we saw the mvnet implementation as

371
00:13:52,079 --> 00:13:56,638
reference

372
00:13:53,760 --> 00:13:58,959
future works are definitely the adding

373
00:13:56,639 --> 00:14:02,560
support for skb recycling for the

374
00:13:58,959 --> 00:14:05,599
http pass functionality for http

375
00:14:02,560 --> 00:14:08,959
verdict and for example regarding

376
00:14:05,600 --> 00:14:11,680
we need to add support for the

377
00:14:08,959 --> 00:14:14,160
xdp support for the order buffer manager

378
00:14:11,680 --> 00:14:17,359
that is available on some device like

379
00:14:14,160 --> 00:14:19,199
solid drum clear fog native support for

380
00:14:17,360 --> 00:14:21,279
a fxdp

381
00:14:19,199 --> 00:14:22,560
and some interesting bits that are

382
00:14:21,279 --> 00:14:26,560
currently

383
00:14:22,560 --> 00:14:26,560
on the xtp roadmap

384
00:14:28,839 --> 00:14:31,839
questions

385
00:14:34,320 --> 00:14:42,240
please so for me for him

386
00:14:38,480 --> 00:14:42,240
so i was wondering is for me for

387
00:14:43,040 --> 00:14:52,399
of my him is relating to af hdb

388
00:14:48,240 --> 00:14:55,440
with af xtp the user space needs to

389
00:14:52,399 --> 00:14:58,160
allocate the memory yes and one of the

390
00:14:55,440 --> 00:14:58,160
restrictions

391
00:15:06,560 --> 00:15:10,079
no that's that's magnus i don't know if

392
00:15:08,240 --> 00:15:13,279
he's in the room uh yeah he's

393
00:15:10,079 --> 00:15:14,800
back there uh

394
00:15:13,279 --> 00:15:16,800
i can repeat the question one of the

395
00:15:14,800 --> 00:15:18,319
restrictions with af xdp is that we you

396
00:15:16,800 --> 00:15:20,639
couldn't use huge pages

397
00:15:18,320 --> 00:15:24,839
when you needed to map memory from the

398
00:15:20,639 --> 00:15:27,839
user space right

399
00:15:24,839 --> 00:15:27,839
okay

400
00:15:37,120 --> 00:15:40,160
the answer is that you can do it but

401
00:15:38,480 --> 00:15:55,839
it's not internally optimized

402
00:15:40,160 --> 00:15:55,839
at the moment for af xdp

403
00:15:58,959 --> 00:16:02,000
yes i've recently been experimenting

404
00:16:00,800 --> 00:16:05,920
with the xcp

405
00:16:02,000 --> 00:16:08,079
on bh interfaces which is slow

406
00:16:05,920 --> 00:16:09,759
um i'm wondering if there's like any

407
00:16:08,079 --> 00:16:10,239
hope by any chance that it could become

408
00:16:09,759 --> 00:16:12,480
fast

409
00:16:10,240 --> 00:16:15,759
in the future which interfaces uh the

410
00:16:12,480 --> 00:16:20,000
virtual virtual initiative

411
00:16:15,759 --> 00:16:20,000
interfaces vh

412
00:16:20,320 --> 00:16:23,839
this depends on your on the card you're

413
00:16:22,720 --> 00:16:25,839
on

414
00:16:23,839 --> 00:16:28,639
no i think it's software software

415
00:16:25,839 --> 00:16:28,639
software implementation

416
00:16:29,519 --> 00:16:32,240
i've never tried it actually i don't

417
00:16:30,959 --> 00:16:36,160
have any intention of working with it at

418
00:16:32,240 --> 00:16:50,560
the moment

419
00:16:36,160 --> 00:16:50,560
thank you thank you

