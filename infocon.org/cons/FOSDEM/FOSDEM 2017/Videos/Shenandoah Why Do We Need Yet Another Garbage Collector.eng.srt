1
00:00:04,620 --> 00:00:07,890
hi everybody I'm Christine flood and I'm

2
00:00:07,890 --> 00:00:10,410
here to talk to you about why we really

3
00:00:10,410 --> 00:00:13,639
do need one more garbage collector and

4
00:00:13,639 --> 00:00:16,500
OpenJDK and I can't get this there so

5
00:00:16,500 --> 00:00:18,210
I'm gonna hold on to it

6
00:00:18,210 --> 00:00:21,060
Shenandoah it's been going on for almost

7
00:00:21,060 --> 00:00:23,640
four years and there's been a lot of

8
00:00:23,640 --> 00:00:25,380
people contributing out to it over time

9
00:00:25,380 --> 00:00:27,930
so I like to give credit where credit is

10
00:00:27,930 --> 00:00:28,919
due what all these people are

11
00:00:28,919 --> 00:00:31,529
contributed along the way to getting it

12
00:00:31,529 --> 00:00:35,219
to the point where it is now and many of

13
00:00:35,219 --> 00:00:36,899
them are up on the stage this is Roman

14
00:00:36,899 --> 00:00:44,309
Alexei and Roland what don't give me a

15
00:00:44,309 --> 00:00:48,570
hard time okay so let's get started I've

16
00:00:48,570 --> 00:00:51,239
never been to Austin before so I gave

17
00:00:51,239 --> 00:00:53,940
I'm giving sort of all of the intro

18
00:00:53,940 --> 00:00:55,800
material to work up to where I want to

19
00:00:55,800 --> 00:00:58,469
be so what is a garbage collector and a

20
00:00:58,469 --> 00:00:59,969
garbage collector is sort of like an

21
00:00:59,969 --> 00:01:02,729
omniscient housekeeper right you bought

22
00:01:02,729 --> 00:01:04,170
these cookies and they weren't they

23
00:01:04,170 --> 00:01:05,280
weren't good they weren't bad you're

24
00:01:05,280 --> 00:01:06,450
never gonna eat them you leave them in

25
00:01:06,450 --> 00:01:08,310
your pantry and you've got this bread

26
00:01:08,310 --> 00:01:09,990
and it's expired what you didn't notice

27
00:01:09,990 --> 00:01:11,940
and it's in your pantry right the

28
00:01:11,940 --> 00:01:13,950
garbage collector can go through and see

29
00:01:13,950 --> 00:01:15,360
all those things that you're never gonna

30
00:01:15,360 --> 00:01:17,400
eat and get rid of them and take the

31
00:01:17,400 --> 00:01:18,660
things that you do have and organize

32
00:01:18,660 --> 00:01:20,940
them for you so running with managed

33
00:01:20,940 --> 00:01:22,890
memory really leaves you in a much

34
00:01:22,890 --> 00:01:28,620
better situation okay so I'm gonna jump

35
00:01:28,620 --> 00:01:31,530
into the current OpenJDK garbage

36
00:01:31,530 --> 00:01:33,870
collectors I think that they all have a

37
00:01:33,870 --> 00:01:36,060
place I know that there's sort of a

38
00:01:36,060 --> 00:01:37,560
support nightmare with these but I

39
00:01:37,560 --> 00:01:40,110
wanted to say that in certain situations

40
00:01:40,110 --> 00:01:42,600
I can see using all of them so for

41
00:01:42,600 --> 00:01:44,600
example if I was running in a

42
00:01:44,600 --> 00:01:47,580
containerized environment and I had 50

43
00:01:47,580 --> 00:01:50,160
jaebeum's running on a server I might

44
00:01:50,160 --> 00:01:53,400
want the smallest footprint the single

45
00:01:53,400 --> 00:01:55,260
threaded garbage collector that has no

46
00:01:55,260 --> 00:01:58,500
sort of wastage due to the algorithm and

47
00:01:58,500 --> 00:02:01,470
that would be serial GC if I was running

48
00:02:01,470 --> 00:02:03,900
something like a weather simulator and I

49
00:02:03,900 --> 00:02:06,690
needed the results fast and throughput

50
00:02:06,690 --> 00:02:09,090
was what it was all about I would want

51
00:02:09,090 --> 00:02:12,030
to use parallel GC if I wanted to

52
00:02:12,030 --> 00:02:14,189
minimize pause times with the current

53
00:02:14,189 --> 00:02:16,910
openjdk garbage collectors

54
00:02:16,910 --> 00:02:20,270
CMS does a young generation collection

55
00:02:20,270 --> 00:02:21,920
while the world stopped but it never

56
00:02:21,920 --> 00:02:24,440
does old generation work while the world

57
00:02:24,440 --> 00:02:26,150
is stopped so that can give you a

58
00:02:26,150 --> 00:02:28,519
minimal franc minimal pause times but it

59
00:02:28,519 --> 00:02:31,280
has some problems because it's a marking

60
00:02:31,280 --> 00:02:32,690
sleep old generation you can get

61
00:02:32,690 --> 00:02:34,940
fragmentation and your performance can

62
00:02:34,940 --> 00:02:37,340
degrade over time and if you want

63
00:02:37,340 --> 00:02:40,099
managed pastimes with compaction you

64
00:02:40,099 --> 00:02:42,739
want g one g one is as works very hard

65
00:02:42,739 --> 00:02:45,019
to be able to manage your pause times

66
00:02:45,019 --> 00:02:46,730
into whatever a particular window you

67
00:02:46,730 --> 00:02:49,790
need but for some folks these aren't

68
00:02:49,790 --> 00:02:52,340
enough right they have a heap that's too

69
00:02:52,340 --> 00:02:54,799
large and positon constraints that are

70
00:02:54,799 --> 00:02:59,150
too small and those are the folks that

71
00:02:59,150 --> 00:03:02,420
we we aim Shenandoah at people that want

72
00:03:02,420 --> 00:03:04,640
to run 100 gigabyte heaps or 200

73
00:03:04,640 --> 00:03:06,739
gigabyte heaps but they still have to

74
00:03:06,739 --> 00:03:09,680
respond to things in ten milliseconds so

75
00:03:09,680 --> 00:03:11,900
they if they know that a request gets

76
00:03:11,900 --> 00:03:15,170
stalled because of a GC for a minute

77
00:03:15,170 --> 00:03:17,510
then they've violated all kinds of

78
00:03:17,510 --> 00:03:22,790
quality of service guarantees so I'm

79
00:03:22,790 --> 00:03:23,930
going to cut this one at the bud

80
00:03:23,930 --> 00:03:25,609
everybody asks me why is this called

81
00:03:25,609 --> 00:03:31,670
Shenandoah it's because the airport pgc

82
00:03:31,670 --> 00:03:33,769
Airport which is cool that's the airport

83
00:03:33,769 --> 00:03:35,060
designation is right outside of

84
00:03:35,060 --> 00:03:38,319
Shenandoah National Park that's all

85
00:03:38,319 --> 00:03:41,420
there's no big secret it was just a cool

86
00:03:41,420 --> 00:03:43,130
little in joke and now you guys are all

87
00:03:43,130 --> 00:03:44,739
in on it

88
00:03:44,739 --> 00:03:48,769
alright so traditional collectors like

89
00:03:48,769 --> 00:03:51,590
the serial collector parallel pärnu are

90
00:03:51,590 --> 00:03:53,660
all separated into generations you have

91
00:03:53,660 --> 00:03:55,760
eaten you have survivor space you have

92
00:03:55,760 --> 00:03:58,010
old seis you have a card table over here

93
00:03:58,010 --> 00:03:59,959
and what this does is summarizes old to

94
00:03:59,959 --> 00:04:02,359
young pointers so you can collect just

95
00:04:02,359 --> 00:04:08,180
this part of the heap shenandoah doesn't

96
00:04:08,180 --> 00:04:11,480
use that shenandoah is a region based

97
00:04:11,480 --> 00:04:13,910
garbage collector and so is g1 what we

98
00:04:13,910 --> 00:04:15,739
do is we break up the heap into regions

99
00:04:15,739 --> 00:04:17,450
in any of these regions can be either

100
00:04:17,450 --> 00:04:21,228
old or young depending on you know that

101
00:04:21,228 --> 00:04:23,840
point in time so we'll start allocating

102
00:04:23,840 --> 00:04:25,789
in a region and will allocate in another

103
00:04:25,789 --> 00:04:27,830
region and another region maybe we'll

104
00:04:27,830 --> 00:04:29,400
have a big object that's bigger than

105
00:04:29,400 --> 00:04:33,210
than a single region and then over time

106
00:04:33,210 --> 00:04:34,889
those objects get tuned into garbage

107
00:04:34,889 --> 00:04:37,680
will pick the garbage east regions which

108
00:04:37,680 --> 00:04:39,870
are these two and will compact them over

109
00:04:39,870 --> 00:04:44,250
there and then we'll have free regions

110
00:04:44,250 --> 00:04:47,160
for you to allocate into so it's it's a

111
00:04:47,160 --> 00:04:49,290
different if what you learned in school

112
00:04:49,290 --> 00:04:51,000
was this traditional semi space isn't

113
00:04:51,000 --> 00:04:53,160
slightly different but not that

114
00:04:53,160 --> 00:04:57,630
different okay so stop the world

115
00:04:57,630 --> 00:04:59,460
compaction when I talked about

116
00:04:59,460 --> 00:05:01,260
compacting you know of those garbage

117
00:05:01,260 --> 00:05:02,460
collectors doing stopped the world

118
00:05:02,460 --> 00:05:04,110
compaction what they do is the Java

119
00:05:04,110 --> 00:05:06,389
threads run for a while they stop and

120
00:05:06,389 --> 00:05:08,220
then the GC threads do the compaction

121
00:05:08,220 --> 00:05:10,350
and then they stop and the Java threads

122
00:05:10,350 --> 00:05:14,669
start up again now shenandoah is a

123
00:05:14,669 --> 00:05:19,160
different beast right shenandoah has

124
00:05:19,160 --> 00:05:21,270
your running Java threads for a while

125
00:05:21,270 --> 00:05:24,720
you stop you do a quick scan of the the

126
00:05:24,720 --> 00:05:26,880
thread stacks and then you start a

127
00:05:26,880 --> 00:05:28,229
concurrent mark but you start the Java

128
00:05:28,229 --> 00:05:28,800
threads again

129
00:05:28,800 --> 00:05:31,350
you do a quick stop and scan of the

130
00:05:31,350 --> 00:05:33,479
thread stack and you do a final mark and

131
00:05:33,479 --> 00:05:35,070
then you do concurrent evacuation to all

132
00:05:35,070 --> 00:05:37,050
the Java threads are running so we have

133
00:05:37,050 --> 00:05:40,380
two very very short pauses that are just

134
00:05:40,380 --> 00:05:43,020
as long as it takes to scan basically

135
00:05:43,020 --> 00:05:48,450
your thread stacks so the concurrent

136
00:05:48,450 --> 00:05:50,210
marking part of that is a soft problem

137
00:05:50,210 --> 00:05:54,150
CMS g1 and and shenandoah all use

138
00:05:54,150 --> 00:05:55,620
basically the same snapshot at the

139
00:05:55,620 --> 00:05:58,770
beginning algorithm all that means is

140
00:05:58,770 --> 00:06:01,110
that if you update a pointer it used to

141
00:06:01,110 --> 00:06:03,000
point a bar and now points to Baz if

142
00:06:03,000 --> 00:06:04,770
you're in the middle of a concurrent

143
00:06:04,770 --> 00:06:06,690
marking cycle you have to be sure the

144
00:06:06,690 --> 00:06:08,699
bar stays alive because it was already

145
00:06:08,699 --> 00:06:09,900
it was alive at the beginning of

146
00:06:09,900 --> 00:06:11,910
concurrent marking so all it is is a

147
00:06:11,910 --> 00:06:15,120
barrier that adds bar to the things to

148
00:06:15,120 --> 00:06:20,940
be marked concurrent compaction that's

149
00:06:20,940 --> 00:06:23,190
what shenandoah does that's new that's

150
00:06:23,190 --> 00:06:25,380
what's cool is that we are able to

151
00:06:25,380 --> 00:06:27,510
compact the objects while the java

152
00:06:27,510 --> 00:06:30,840
threads are running so why is concurrent

153
00:06:30,840 --> 00:06:33,630
compaction so exciting it's it's just

154
00:06:33,630 --> 00:06:35,460
it's complicated right if you have

155
00:06:35,460 --> 00:06:37,500
several Java threads running that are

156
00:06:37,500 --> 00:06:39,900
all setting fields in an object and the

157
00:06:39,900 --> 00:06:41,729
GC thread is going to copy the object

158
00:06:41,729 --> 00:06:43,380
you can tell that there's all

159
00:06:43,380 --> 00:06:45,030
kinds of bad things that could happen

160
00:06:45,030 --> 00:06:47,040
right somebody could write to it and the

161
00:06:47,040 --> 00:06:49,740
GC thread could copy it or and we lose

162
00:06:49,740 --> 00:06:52,770
the update or these guys are writing two

163
00:06:52,770 --> 00:06:57,000
different copies so what you really want

164
00:06:57,000 --> 00:06:59,940
to have happen is you want your object

165
00:06:59,940 --> 00:07:01,650
you know all these threads to be

166
00:07:01,650 --> 00:07:03,480
pointing to foo and then flip a switch

167
00:07:03,480 --> 00:07:05,010
and have them all be pointing to foo

168
00:07:05,010 --> 00:07:07,710
Prime and that way you don't lose

169
00:07:07,710 --> 00:07:09,990
anything and everything is good but we

170
00:07:09,990 --> 00:07:11,730
can't really do this right even if we

171
00:07:11,730 --> 00:07:13,170
had some sort of fancy smancy

172
00:07:13,170 --> 00:07:16,320
transactional memory just the time it

173
00:07:16,320 --> 00:07:17,850
would take to find all the threads that

174
00:07:17,850 --> 00:07:19,740
are referencing foo and by the way it's

175
00:07:19,740 --> 00:07:21,210
not just threads it could be some

176
00:07:21,210 --> 00:07:23,760
objects very deep in the heap that are

177
00:07:23,760 --> 00:07:26,130
pointing to foo as well would all have

178
00:07:26,130 --> 00:07:27,780
to get updated at the same time and we

179
00:07:27,780 --> 00:07:31,140
can't really do that so the secret sauce

180
00:07:31,140 --> 00:07:33,030
in shenandoah which isn't really secret

181
00:07:33,030 --> 00:07:35,700
sauce is that we've added an indirection

182
00:07:35,700 --> 00:07:38,340
pointer to every object now this is the

183
00:07:38,340 --> 00:07:39,930
scariest part about shenandoah I was

184
00:07:39,930 --> 00:07:40,890
afraid that we weren't going to get

185
00:07:40,890 --> 00:07:44,190
accepted because this pointer you know

186
00:07:44,190 --> 00:07:45,780
we've increased the size of every object

187
00:07:45,780 --> 00:07:48,210
but the power it gives you is worth it

188
00:07:48,210 --> 00:07:49,710
right because you have these C threads

189
00:07:49,710 --> 00:07:52,170
that are porting to foo the garbage

190
00:07:52,170 --> 00:07:54,150
collector comes along and makes a

191
00:07:54,150 --> 00:07:56,430
speculative copy of foo and it does a

192
00:07:56,430 --> 00:07:58,260
casts to change the indirection pointer

193
00:07:58,260 --> 00:08:00,510
to point to food prime so anytime

194
00:08:00,510 --> 00:08:04,140
anybody accesses this object before the

195
00:08:04,140 --> 00:08:06,360
caz they get the old copy and after the

196
00:08:06,360 --> 00:08:08,370
cast they get the new copy it doesn't

197
00:08:08,370 --> 00:08:09,360
matter whether they're there in another

198
00:08:09,360 --> 00:08:11,460
thread stack or they're somewhere deep

199
00:08:11,460 --> 00:08:13,290
in the heap they're always going to have

200
00:08:13,290 --> 00:08:21,240
the current copy of foo so what this

201
00:08:21,240 --> 00:08:24,240
means is that all object accesses have

202
00:08:24,240 --> 00:08:27,030
to go through a read barrier now this is

203
00:08:27,030 --> 00:08:28,440
the other Bugaboo about shenandoah

204
00:08:28,440 --> 00:08:30,510
because the common wisdom back in the

205
00:08:30,510 --> 00:08:33,030
list days was that three barriers reads

206
00:08:33,030 --> 00:08:35,159
are seven times more prominent than

207
00:08:35,159 --> 00:08:37,559
writes and so you can't afford a ribéry

208
00:08:37,559 --> 00:08:39,690
err but the reality is that our read

209
00:08:39,690 --> 00:08:42,719
barrier is extremely cheap we have one

210
00:08:42,719 --> 00:08:44,550
hardware instruction right before here

211
00:08:44,550 --> 00:08:47,310
if you want to read a filled-in foo you

212
00:08:47,310 --> 00:08:49,680
have to find the location of foo add the

213
00:08:49,680 --> 00:08:52,200
field offset and read that location here

214
00:08:52,200 --> 00:08:54,270
we have to read the location minus eight

215
00:08:54,270 --> 00:08:56,110
resolve that and then read

216
00:08:56,110 --> 00:09:02,170
from there so without shenandoah on

217
00:09:02,170 --> 00:09:05,920
Intel a get field is a single move

218
00:09:05,920 --> 00:09:09,550
and with Shenandoah on Intel we move the

219
00:09:09,550 --> 00:09:11,019
the contents of the forwarding pointer

220
00:09:11,019 --> 00:09:13,120
into the register and then we do the

221
00:09:13,120 --> 00:09:15,579
read of the value so it really is one

222
00:09:15,579 --> 00:09:17,260
single machine disruption for a read

223
00:09:17,260 --> 00:09:20,290
barrier and as I will show you later on

224
00:09:20,290 --> 00:09:25,180
it is not cost prohibitive there's still

225
00:09:25,180 --> 00:09:26,740
a race condition if you're paying

226
00:09:26,740 --> 00:09:29,079
attention right because we have one

227
00:09:29,079 --> 00:09:30,940
thread that resolves a forwarding

228
00:09:30,940 --> 00:09:34,029
pointer and then stalls out and writes

229
00:09:34,029 --> 00:09:36,430
to foo and meanwhile another thread

230
00:09:36,430 --> 00:09:39,610
copies Phil and this is a problem so we

231
00:09:39,610 --> 00:09:40,990
also have to have a more complicated

232
00:09:40,990 --> 00:09:43,720
write barrier we have a write barrier

233
00:09:43,720 --> 00:09:46,959
that copies the object first before

234
00:09:46,959 --> 00:09:51,519
you're allowed to write to it so here

235
00:09:51,519 --> 00:09:53,260
you can see I've got a B and C over

236
00:09:53,260 --> 00:09:55,870
there if I want to write to D I have to

237
00:09:55,870 --> 00:09:57,760
first copy it over to two space and then

238
00:09:57,760 --> 00:09:59,649
write to it there

239
00:09:59,649 --> 00:10:02,740
this isn't that painful right there are

240
00:10:02,740 --> 00:10:04,600
other collectors out there that are

241
00:10:04,600 --> 00:10:06,760
concurrent collectors and they have to

242
00:10:06,760 --> 00:10:08,769
copy things on raid we don't have to

243
00:10:08,769 --> 00:10:10,329
copy things over you only copy them on

244
00:10:10,329 --> 00:10:12,220
writes and right so let's frequence and

245
00:10:12,220 --> 00:10:16,570
reads all right and so our write

246
00:10:16,570 --> 00:10:19,420
barriers are pretty quick these are

247
00:10:19,420 --> 00:10:21,130
ordered in this order for a complicated

248
00:10:21,130 --> 00:10:24,279
reason I won't go into but we check the

249
00:10:24,279 --> 00:10:25,630
back in progress thread which is

250
00:10:25,630 --> 00:10:28,060
thread-local and we read the forwarding

251
00:10:28,060 --> 00:10:30,220
pointer and if there isn't a vaccum

252
00:10:30,220 --> 00:10:31,899
progress we can just skip to the store

253
00:10:31,899 --> 00:10:34,300
so our write barriers are fairly quick

254
00:10:34,300 --> 00:10:35,860
in the case where you aren't actually

255
00:10:35,860 --> 00:10:38,320
evacuating objects and if you are

256
00:10:38,320 --> 00:10:40,839
they're there they just copy the object

257
00:10:40,839 --> 00:10:44,140
they're fairly quick as well so what are

258
00:10:44,140 --> 00:10:46,899
these barriers cost they're not really

259
00:10:46,899 --> 00:10:48,220
as much as you might think

260
00:10:48,220 --> 00:10:52,720
we have phases in situ where well these

261
00:10:52,720 --> 00:10:53,860
guys are going to correct me I'm not a

262
00:10:53,860 --> 00:10:56,589
c2 expert but we can do elimination of

263
00:10:56,589 --> 00:10:58,209
barriers on new objects and null

264
00:10:58,209 --> 00:11:01,000
pointers we can do repair eliminations

265
00:11:01,000 --> 00:11:05,140
on final fields and we can see - already

266
00:11:05,140 --> 00:11:07,480
does barrier hoisting so if you have a

267
00:11:07,480 --> 00:11:09,360
read barrier and

268
00:11:09,360 --> 00:11:11,070
it could sometimes voice it out of the

269
00:11:11,070 --> 00:11:15,240
loop to make it less expensive so I did

270
00:11:15,240 --> 00:11:18,330
a little experiment this is from a paper

271
00:11:18,330 --> 00:11:20,700
I did last year where I ran some of the

272
00:11:20,700 --> 00:11:23,760
de capo benchmarks and I gave them an

273
00:11:23,760 --> 00:11:25,770
incredibly large heap I gave them a

274
00:11:25,770 --> 00:11:26,940
large enough heap that they never

275
00:11:26,940 --> 00:11:30,120
actually had to do any GC work and these

276
00:11:30,120 --> 00:11:31,500
were the three benchmarks that were able

277
00:11:31,500 --> 00:11:33,630
to run in that environment and you can

278
00:11:33,630 --> 00:11:37,590
see that comparing shenandoah to g1 the

279
00:11:37,590 --> 00:11:40,440
overhead of shenandoah was between two

280
00:11:40,440 --> 00:11:41,700
point one and five point six percent

281
00:11:41,700 --> 00:11:44,340
this was a year ago we have some numbers

282
00:11:44,340 --> 00:11:48,050
now where we're doing even better

283
00:11:48,300 --> 00:11:50,010
are there any other gotchas to be

284
00:11:50,010 --> 00:11:54,120
concerned with in Shenandoah and you

285
00:11:54,120 --> 00:11:55,680
need to think about things like pointer

286
00:11:55,680 --> 00:11:59,940
comparisons right because if I have an a

287
00:11:59,940 --> 00:12:02,010
here and it's pointing to a prime if

288
00:12:02,010 --> 00:12:04,320
somebody has a pointer to the old object

289
00:12:04,320 --> 00:12:05,370
that somebody has a pointer to the new

290
00:12:05,370 --> 00:12:09,630
object you can sometimes get something

291
00:12:09,630 --> 00:12:10,800
saying they're not equal when they are

292
00:12:10,800 --> 00:12:13,470
so we compare the pointers directly and

293
00:12:13,470 --> 00:12:15,870
if they're not equal then we execute the

294
00:12:15,870 --> 00:12:17,310
right barrier on both of them

295
00:12:17,310 --> 00:12:19,380
and then we can compare them both into

296
00:12:19,380 --> 00:12:20,760
space and make sure that they're correct

297
00:12:20,760 --> 00:12:22,740
and there's there's complications like

298
00:12:22,740 --> 00:12:27,450
this for things like calves as well I'm

299
00:12:27,450 --> 00:12:35,670
sorry all right volatiles everybody was

300
00:12:35,670 --> 00:12:37,830
all concerned about volatiles volatiles

301
00:12:37,830 --> 00:12:39,690
just create a new state in new memory

302
00:12:39,690 --> 00:12:42,090
State and we don't do any optimizations

303
00:12:42,090 --> 00:12:47,040
across them and it's fine I'm gonna be

304
00:12:47,040 --> 00:12:48,300
really quick because we're gonna do a

305
00:12:48,300 --> 00:12:50,160
demos laughs towards this is

306
00:12:50,160 --> 00:12:51,990
elasticsearch

307
00:12:51,990 --> 00:12:54,180
this is just I wrote an elasticsearch

308
00:12:54,180 --> 00:12:56,310
benchmark because I wanted to see and

309
00:12:56,310 --> 00:12:57,510
the only thing that's really interesting

310
00:12:57,510 --> 00:13:00,300
here is you can see that this was also

311
00:13:00,300 --> 00:13:03,900
again from 2016 we were slightly slower

312
00:13:03,900 --> 00:13:06,270
in terms of runtime but if you look at

313
00:13:06,270 --> 00:13:08,100
our total pause time in milliseconds

314
00:13:08,100 --> 00:13:11,700
compared to the other collectors we ROCK

315
00:13:11,700 --> 00:13:14,010
if you look at our max pause times and

316
00:13:14,010 --> 00:13:16,260
our average plus times we also ROC if

317
00:13:16,260 --> 00:13:18,420
your main consideration is response time

318
00:13:18,420 --> 00:13:19,680
and you don't want

319
00:13:19,680 --> 00:13:21,570
you can't afford to pay for pauses

320
00:13:21,570 --> 00:13:25,410
Shenandoah is huge alright I also ran a

321
00:13:25,410 --> 00:13:27,960
very famous warehouse benchmark just

322
00:13:27,960 --> 00:13:32,160
recently and we actually because of some

323
00:13:32,160 --> 00:13:33,540
excellent work that these guys have been

324
00:13:33,540 --> 00:13:35,250
doing lately and optimizing our

325
00:13:35,250 --> 00:13:37,709
concurrent marking we are now winning in

326
00:13:37,709 --> 00:13:39,810
both in terms of Max J ops and critical

327
00:13:39,810 --> 00:13:40,350
jabs

328
00:13:40,350 --> 00:13:43,110
so we beat g1 in this particular

329
00:13:43,110 --> 00:13:44,850
situation in both throughput and in

330
00:13:44,850 --> 00:13:47,490
response time which is a huge win for us

331
00:13:47,490 --> 00:13:49,910
in terms of our getting better

332
00:13:49,910 --> 00:13:54,149
performance alright so we ship in fedora

333
00:13:54,149 --> 00:13:58,500
if you run Fedora 25 it's just - xx : +

334
00:13:58,500 --> 00:14:01,050
u Shenandoah GC and you can try it out

335
00:14:01,050 --> 00:14:04,290
and Java 8 we're scheduled to ship in

336
00:14:04,290 --> 00:14:06,209
the next release of rel I can't promise

337
00:14:06,209 --> 00:14:08,760
that that's going to happen but that's

338
00:14:08,760 --> 00:14:11,610
the current schedule and we've spent the

339
00:14:11,610 --> 00:14:13,350
last year working on product ization and

340
00:14:13,350 --> 00:14:15,600
optimizations we are much more stable

341
00:14:15,600 --> 00:14:16,740
and we're getting much better

342
00:14:16,740 --> 00:14:20,610
performance and if you have any

343
00:14:20,610 --> 00:14:22,800
questions or if you want you know - just

344
00:14:22,800 --> 00:14:25,830
send me email and I don't know ask where

345
00:14:25,830 --> 00:14:27,150
we got this Jenna no sweat shirts go

346
00:14:27,150 --> 00:14:29,850
ahead this is me and I'm gonna turn it

347
00:14:29,850 --> 00:14:34,760
over to Alexei who's gonna do the demo

348
00:14:41,240 --> 00:14:46,220
so obviously I did run my own benchmarks

349
00:14:46,220 --> 00:14:52,440
let's see come on nobody will believe

350
00:14:52,440 --> 00:14:56,400
you if you didn't run spec jbb on your

351
00:14:56,400 --> 00:15:02,450
product right ok let's do this come on

352
00:15:02,450 --> 00:15:05,279
can you see it yes you can

353
00:15:05,279 --> 00:15:11,040
so shortly before New Year Roman and I

354
00:15:11,040 --> 00:15:13,709
were having fun and actually written

355
00:15:13,709 --> 00:15:16,890
some visualizer - which we will try to

356
00:15:16,890 --> 00:15:18,300
understand how shannond or worse

357
00:15:18,300 --> 00:15:20,730
internally is just some eye candy it

358
00:15:20,730 --> 00:15:22,920
turns out to be much more useful not

359
00:15:22,920 --> 00:15:24,779
only for talks but only to see how the

360
00:15:24,779 --> 00:15:27,000
collector actually works so let's run

361
00:15:27,000 --> 00:15:31,709
spec gbb anders under this visualizer

362
00:15:31,709 --> 00:15:33,080
and shenandoah is

363
00:15:33,080 --> 00:15:36,110
so what you see here as Christine said

364
00:15:36,110 --> 00:15:38,870
it is the original eyes collector right

365
00:15:38,870 --> 00:15:40,399
so this is what you see here is actually

366
00:15:40,399 --> 00:15:43,610
the the Java heap the squares there are

367
00:15:43,610 --> 00:15:45,339
regions they are color coded to

368
00:15:45,339 --> 00:15:48,140
distinguish whether they are recently

369
00:15:48,140 --> 00:15:50,720
allocated how much leaf life data is

370
00:15:50,720 --> 00:15:53,839
there etc etc right now especially ramps

371
00:15:53,839 --> 00:15:58,579
up so it doesn't do any huge allocations

372
00:15:58,579 --> 00:16:01,670
at all it just tries to do tries to

373
00:16:01,670 --> 00:16:03,310
figure out what's going on there and

374
00:16:03,310 --> 00:16:05,870
there is little graph on the top which

375
00:16:05,870 --> 00:16:08,959
is kind of the time graph of these

376
00:16:08,959 --> 00:16:10,970
parameters there so right now nothing

377
00:16:10,970 --> 00:16:14,870
happens we just do the hip work and then

378
00:16:14,870 --> 00:16:16,610
something happens then we realize that

379
00:16:16,610 --> 00:16:18,440
we have to start the concurrent cycle

380
00:16:18,440 --> 00:16:21,709
and we end the thing there on the top in

381
00:16:21,709 --> 00:16:23,930
this yellow wish in the puke yellow

382
00:16:23,930 --> 00:16:26,450
color there it is actually the

383
00:16:26,450 --> 00:16:30,740
concurrent mark phase right and be a red

384
00:16:30,740 --> 00:16:33,260
kind of board they are following it it

385
00:16:33,260 --> 00:16:35,089
is actually the concurrent records just

386
00:16:35,089 --> 00:16:37,520
to get you the feel about the size of

387
00:16:37,520 --> 00:16:40,760
these phases in your regular young GC

388
00:16:40,760 --> 00:16:42,470
workload which mostly allocates which

389
00:16:42,470 --> 00:16:46,010
doesn't retain much etc etc but what you

390
00:16:46,010 --> 00:16:49,040
can also see there is that grayish line

391
00:16:49,040 --> 00:16:51,829
which is the used heap actually grows

392
00:16:51,829 --> 00:16:53,990
through the concurrent mark phase thus

393
00:16:53,990 --> 00:16:55,670
saying that the allocate are still

394
00:16:55,670 --> 00:16:58,610
working while the collector marks if you

395
00:16:58,610 --> 00:17:00,260
look closer at that graph you can

396
00:17:00,260 --> 00:17:02,839
actually also see that during evac that

397
00:17:02,839 --> 00:17:04,220
also happens but this is not the

398
00:17:04,220 --> 00:17:07,010
workload at which we will like to to

399
00:17:07,010 --> 00:17:09,829
show the evac for that we have another

400
00:17:09,829 --> 00:17:12,140
workload and this workload is really

401
00:17:12,140 --> 00:17:14,689
really simple I call it the rate Fragger

402
00:17:14,689 --> 00:17:18,500
you just say I have the array and I at

403
00:17:18,500 --> 00:17:20,750
random index I will just store the the

404
00:17:20,750 --> 00:17:23,270
binary the by tracer

405
00:17:23,270 --> 00:17:25,669
this workload defies generational

406
00:17:25,669 --> 00:17:27,650
hypothesis because the oldest project

407
00:17:27,650 --> 00:17:31,070
all objects died there by the way this

408
00:17:31,070 --> 00:17:32,890
is the characteristic which will be

409
00:17:32,890 --> 00:17:36,050
endemic for any in memory last least

410
00:17:36,050 --> 00:17:39,919
recently used memory cache so this is

411
00:17:39,919 --> 00:17:41,570
the simple workload that we can use

412
00:17:41,570 --> 00:17:46,200
there and if we run Shenandoah with it

413
00:17:46,200 --> 00:17:49,070
a few interesting things will happen

414
00:17:49,070 --> 00:17:51,419
first of all it has much higher

415
00:17:51,419 --> 00:17:56,100
allocation rate and as it goes you can

416
00:17:56,100 --> 00:18:00,179
see clearly that the oldest elements in

417
00:18:00,179 --> 00:18:02,700
the array gets rewritten and so regions

418
00:18:02,700 --> 00:18:05,730
that we're having lots of life objects

419
00:18:05,730 --> 00:18:07,769
will have less than last life project up

420
00:18:07,769 --> 00:18:09,840
to the point you have to deal with them

421
00:18:09,840 --> 00:18:12,000
somehow and Shannon though that means

422
00:18:12,000 --> 00:18:14,820
that you will intensify them as the

423
00:18:14,820 --> 00:18:17,039
regions with the most garbage add them

424
00:18:17,039 --> 00:18:19,440
to collection set evacuate all the life

425
00:18:19,440 --> 00:18:21,600
objects from there and move on if you

426
00:18:21,600 --> 00:18:23,429
have the generational collectors that

427
00:18:23,429 --> 00:18:25,230
kind of for quote will just fragment

428
00:18:25,230 --> 00:18:27,720
your old generation and you will face

429
00:18:27,720 --> 00:18:31,649
either the next pause like in j1 or the

430
00:18:31,649 --> 00:18:34,980
full GC you like in parallel etc so this

431
00:18:34,980 --> 00:18:37,380
workload can tells you that we can

432
00:18:37,380 --> 00:18:39,750
survive this otherwise heavily

433
00:18:39,750 --> 00:18:41,610
fragmenting workload with that defy

434
00:18:41,610 --> 00:18:44,340
generational hypothesis arguably most of

435
00:18:44,340 --> 00:18:46,350
the data that people now store on heap

436
00:18:46,350 --> 00:18:48,960
are actually caches so this is your

437
00:18:48,960 --> 00:18:52,380
go-to scenario but you can clearly see

438
00:18:52,380 --> 00:18:56,010
how much those phases take but we have

439
00:18:56,010 --> 00:19:00,269
the GC logs there and if you look there

440
00:19:00,269 --> 00:19:01,980
you can actually see what is the

441
00:19:01,980 --> 00:19:04,740
concurrent chignon doji cycle so this is

442
00:19:04,740 --> 00:19:09,149
the cycle number 55 and it has the poles

443
00:19:09,149 --> 00:19:10,980
in it mark which takes less than a

444
00:19:10,980 --> 00:19:13,470
millisecond which we clearly which we

445
00:19:13,470 --> 00:19:15,809
very fast scan the route set then has

446
00:19:15,809 --> 00:19:17,880
concurrent mark any any other concurrent

447
00:19:17,880 --> 00:19:22,409
GC does that it takes 160 milliseconds

448
00:19:22,409 --> 00:19:24,090
then you have a very short final mark

449
00:19:24,090 --> 00:19:26,789
phase one point five milliseconds and

450
00:19:26,789 --> 00:19:28,980
those are the only pauses that are ever

451
00:19:28,980 --> 00:19:31,110
observed by the imitator and this is

452
00:19:31,110 --> 00:19:32,880
where the Shenandoah beef is you have

453
00:19:32,880 --> 00:19:34,889
the concurrent evacuation which takes

454
00:19:34,889 --> 00:19:37,409
200 milliseconds there any other

455
00:19:37,409 --> 00:19:39,679
collector in OpenJDK will have to stop

456
00:19:39,679 --> 00:19:42,389
to copy the subject so what the number

457
00:19:42,389 --> 00:19:44,580
that you are looking at there is your

458
00:19:44,580 --> 00:19:48,510
post time if even though it's concurrent

459
00:19:48,510 --> 00:19:52,380
so you don't experience this as much so

460
00:19:52,380 --> 00:19:54,899
they will have time for out of the holes

461
00:19:54,899 --> 00:19:57,229
all right

462
00:19:57,380 --> 00:20:01,610
so so one of the things about this is

463
00:20:01,610 --> 00:20:04,630
that since you know that the most

464
00:20:04,630 --> 00:20:11,240
difficulty connecting with out much ill

465
00:20:11,240 --> 00:20:13,610
effect right the the application was

466
00:20:13,610 --> 00:20:15,590
still run for any other collector that

467
00:20:15,590 --> 00:20:18,200
would be insane right if you only get

468
00:20:18,200 --> 00:20:20,810
fear on GC back-to-back that means you

469
00:20:20,810 --> 00:20:23,090
always stopping the application for a

470
00:20:23,090 --> 00:20:24,950
long time it has nothing to do there

471
00:20:24,950 --> 00:20:27,080
right it's no work to do there for

472
00:20:27,080 --> 00:20:29,420
Shenandoah we have a different

473
00:20:29,420 --> 00:20:32,240
heuristics that drive collections and

474
00:20:32,240 --> 00:20:34,610
one of them that is very useful for

475
00:20:34,610 --> 00:20:37,280
testing is the is the heuristics that

476
00:20:37,280 --> 00:20:39,260
actually runs GC back back to back and

477
00:20:39,260 --> 00:20:41,210
you can clearly see that even though the

478
00:20:41,210 --> 00:20:43,040
collector is actually working very hard

479
00:20:43,040 --> 00:20:45,380
so the concurrent mark and concurrent in

480
00:20:45,380 --> 00:20:47,690
a back phases go back to back without

481
00:20:47,690 --> 00:20:49,910
stopping to breath they come the

482
00:20:49,910 --> 00:20:51,800
allocate are still working the

483
00:20:51,800 --> 00:20:53,420
application is still working even though

484
00:20:53,420 --> 00:20:57,320
what you do is back-to-back GCS so this

485
00:20:57,320 --> 00:20:59,180
is how the good concurrent collector

486
00:20:59,180 --> 00:21:01,160
should behave right this in the worst

487
00:21:01,160 --> 00:21:04,370
case when you have so much garbage in

488
00:21:04,370 --> 00:21:06,530
the heap that you have to run GC back to

489
00:21:06,530 --> 00:21:08,780
back your application still is able to

490
00:21:08,780 --> 00:21:18,310
run okay and the barriers right

491
00:21:21,280 --> 00:21:23,960
Christine told about the cost of the

492
00:21:23,960 --> 00:21:27,560
barriers so let's run some application

493
00:21:27,560 --> 00:21:29,300
obviously if you want to run throughput

494
00:21:29,300 --> 00:21:29,960
application

495
00:21:29,960 --> 00:21:33,560
let's run spec GBM right look at the

496
00:21:33,560 --> 00:21:40,040
classic thingy there so if we run if

497
00:21:40,040 --> 00:21:43,790
we're on a single workload XML

498
00:21:43,790 --> 00:21:45,710
validation on the with the parallel

499
00:21:45,710 --> 00:21:47,990
collector if you want to estimate the

500
00:21:47,990 --> 00:21:49,550
the impact of barriers we want the

501
00:21:49,550 --> 00:21:51,950
workload that does not retain much and

502
00:21:51,950 --> 00:21:53,870
the XML validation does not retain much

503
00:21:53,870 --> 00:21:57,560
is it delicates and allocates not a lot

504
00:21:57,560 --> 00:21:59,510
so this is there are actually very very

505
00:21:59,510 --> 00:22:01,310
short so the performance of this

506
00:22:01,310 --> 00:22:05,210
workload is roughly it correlates with

507
00:22:05,210 --> 00:22:06,680
the overhead of the barriers with the

508
00:22:06,680 --> 00:22:08,090
parallel collector you have only a

509
00:22:08,090 --> 00:22:11,120
simplest simple intelligent

510
00:22:11,120 --> 00:22:13,370
right barrier on the pointer stores so

511
00:22:13,370 --> 00:22:15,650
it's rather efficient there and you have

512
00:22:15,650 --> 00:22:19,419
the workload that runs around hung 400

513
00:22:19,419 --> 00:22:24,020
opps per minute right so if I run with

514
00:22:24,020 --> 00:22:29,150
j-1 what would you say the performance

515
00:22:29,150 --> 00:22:31,280
would be what would you expect from j1

516
00:22:31,280 --> 00:22:36,590
with this workload who is 400 after so

517
00:22:36,590 --> 00:22:49,210
Forex performance hit 200 300 400 500 oh

518
00:22:49,210 --> 00:22:53,900
come on you're not fun okay so with j-1

519
00:22:53,900 --> 00:22:57,500
if you run this you will actually see

520
00:22:57,500 --> 00:23:00,100
that because it has to do more

521
00:23:00,100 --> 00:23:03,470
heavyweight right barriers there it

522
00:23:03,470 --> 00:23:05,870
would actually much run slower this is

523
00:23:05,870 --> 00:23:08,150
the warm-up so we didn't we didn't care

524
00:23:08,150 --> 00:23:11,780
about this yet but it will sustain the

525
00:23:11,780 --> 00:23:14,630
throughput of 320 hours per minute and

526
00:23:14,630 --> 00:23:17,059
this is tells you something important

527
00:23:17,059 --> 00:23:19,280
and fundamental about GCS if your

528
00:23:19,280 --> 00:23:21,350
application does not care about pauses

529
00:23:21,350 --> 00:23:23,360
then well-engineered throughput

530
00:23:23,360 --> 00:23:25,520
collector will beat you every time

531
00:23:25,520 --> 00:23:28,789
so only if you care about the post time

532
00:23:28,789 --> 00:23:31,370
it makes sense to run the concurrent

533
00:23:31,370 --> 00:23:34,700
collector regardless of whether it's

534
00:23:34,700 --> 00:23:37,010
fully concurrent or partially concurrent

535
00:23:37,010 --> 00:23:42,620
etc at weekend so it's 330 up ups per

536
00:23:42,620 --> 00:23:44,870
minute so how about shenandoah so who is

537
00:23:44,870 --> 00:23:46,700
who is saying that Shalonda would be

538
00:23:46,700 --> 00:23:51,650
slower than g1 of course it would be

539
00:23:51,650 --> 00:23:57,490
whose for faster of course yes

540
00:23:58,470 --> 00:24:01,140
so for Shannon though you have the right

541
00:24:01,140 --> 00:24:03,120
barriers for every right not only for

542
00:24:03,120 --> 00:24:04,890
the Pointer rights and you have the Reid

543
00:24:04,890 --> 00:24:09,300
barriers right so you can think that if

544
00:24:09,300 --> 00:24:11,190
Reid barriers are very expensive then

545
00:24:11,190 --> 00:24:14,070
you will see an enormous performance

546
00:24:14,070 --> 00:24:17,550
impact right but it turns out it's not

547
00:24:17,550 --> 00:24:19,590
really that bad

548
00:24:19,590 --> 00:24:24,000
so j1 made 330 operations per minute and

549
00:24:24,000 --> 00:24:29,640
we are doing 320 I think operations per

550
00:24:29,640 --> 00:24:31,230
minute there so the cost of Reid

551
00:24:31,230 --> 00:24:32,550
barriers when you read barriers are

552
00:24:32,550 --> 00:24:35,400
really really basic in implement

553
00:24:35,400 --> 00:24:37,260
implementation wise it's really not that

554
00:24:37,260 --> 00:24:41,280
high all right but again if your

555
00:24:41,280 --> 00:24:45,150
workload is purely throughput you don't

556
00:24:45,150 --> 00:24:47,120
care about post times you can't afford

557
00:24:47,120 --> 00:24:49,800
larger GC pauses then you shouldn't

558
00:24:49,800 --> 00:24:51,330
really bother you should just run with

559
00:24:51,330 --> 00:24:53,510
parallel old and be happy about it

560
00:24:53,510 --> 00:24:58,170
okay yes you want to talk about the

561
00:24:58,170 --> 00:25:01,440
pauses yeah

562
00:25:01,440 --> 00:25:05,100
questions cool

563
00:25:05,100 --> 00:25:09,330
oh no not of not not as oh not as though

564
00:25:09,330 --> 00:25:26,190
as always banter expressions so there

565
00:25:26,190 --> 00:25:28,830
was a talk about the new pointer you

566
00:25:28,830 --> 00:25:31,260
create when you delete the old object

567
00:25:31,260 --> 00:25:35,670
and the my question is about a VM or a

568
00:25:35,670 --> 00:25:39,720
process running about here or so how do

569
00:25:39,720 --> 00:25:43,470
you think this iterations over creating

570
00:25:43,470 --> 00:25:47,040
new pointers what would we be the impact

571
00:25:47,040 --> 00:25:52,320
on the objects would it is this sold or

572
00:25:52,320 --> 00:25:55,710
visitors so you're asking what happens

573
00:25:55,710 --> 00:25:57,600
with the with the previous copy of the

574
00:25:57,600 --> 00:26:01,310
object so no if we create a new object

575
00:26:01,310 --> 00:26:05,750
to kind of solve the problem that

576
00:26:05,750 --> 00:26:08,970
threats or any other objects could not

577
00:26:08,970 --> 00:26:10,020
find the

578
00:26:10,020 --> 00:26:13,170
new object we create a new pointer if I

579
00:26:13,170 --> 00:26:19,380
understood correctly exactly so this is

580
00:26:19,380 --> 00:26:22,560
one iteration on one garbage collection

581
00:26:22,560 --> 00:26:28,650
if the process or whatever it goes for

582
00:26:28,650 --> 00:26:31,980
about a year I think creating a new

583
00:26:31,980 --> 00:26:35,250
object so after each a garbage

584
00:26:35,250 --> 00:26:38,960
collection you would create a new point

585
00:26:52,460 --> 00:26:55,170
you go through the next phase where you

586
00:26:55,170 --> 00:26:57,540
do this working and you make sure all

587
00:26:57,540 --> 00:26:59,430
your pointers now point to the to space

588
00:26:59,430 --> 00:27:02,070
copy and then that previous version can

589
00:27:02,070 --> 00:27:09,060
be a garbage collected okay okay thank

590
00:27:09,060 --> 00:27:11,149
you

591
00:27:11,450 --> 00:27:17,240
so do support both c1 and c2 yes

592
00:27:25,640 --> 00:27:29,640
[Applause]

