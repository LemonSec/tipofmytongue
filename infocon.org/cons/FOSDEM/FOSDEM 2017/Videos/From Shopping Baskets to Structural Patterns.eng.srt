1
00:00:04,610 --> 00:00:09,710
yeah hello everyone thank you so hello

2
00:00:09,710 --> 00:00:12,469
I'm Andrea from University of Leipzig

3
00:00:12,469 --> 00:00:14,080
and today I'm going to talk about

4
00:00:14,080 --> 00:00:17,060
frequent subgraph mining on Apache flink

5
00:00:17,060 --> 00:00:20,480
which I just hide behind the title of

6
00:00:20,480 --> 00:00:22,340
from shopping baskets to structural

7
00:00:22,340 --> 00:00:24,470
patterns because it's kind of related to

8
00:00:24,470 --> 00:00:26,990
something some of you might know from

9
00:00:26,990 --> 00:00:29,000
the term of a shopping basket and edit

10
00:00:29,000 --> 00:00:32,090
analysis or Association rules so first

11
00:00:32,090 --> 00:00:34,790
of all my question who view regulatory

12
00:00:34,790 --> 00:00:39,519
works with graphs and with Taita mining

13
00:00:39,519 --> 00:00:42,440
or reciter Mining yeah and and cross

14
00:00:42,440 --> 00:00:46,400
anti-de mining only alright so there so

15
00:00:46,400 --> 00:00:47,900
alright so I slowed down a little bit

16
00:00:47,900 --> 00:00:51,549
and yet I try Q to make the talk not too

17
00:00:51,549 --> 00:00:54,619
much scientific and rather give you a

18
00:00:54,619 --> 00:00:57,229
problem introduction and challenges in

19
00:00:57,229 --> 00:00:58,940
the implementation on Apache fling core

20
00:00:58,940 --> 00:01:00,760
and distribute the data flows in general

21
00:01:00,760 --> 00:01:04,010
so the contents are as follows

22
00:01:04,010 --> 00:01:06,229
first I introduced the problem and

23
00:01:06,229 --> 00:01:08,690
explained details about the current

24
00:01:08,690 --> 00:01:11,450
state of this of these algorithms then I

25
00:01:11,450 --> 00:01:14,690
proposed a solution which we found on

26
00:01:14,690 --> 00:01:16,340
top of a patch of link in the context of

27
00:01:16,340 --> 00:01:18,350
the group framework not a question who

28
00:01:18,350 --> 00:01:22,430
was attending the talk before so all

29
00:01:22,430 --> 00:01:24,470
right quite a lot so all right so um

30
00:01:24,470 --> 00:01:27,620
yeah it's it's in the context of an open

31
00:01:27,620 --> 00:01:29,180
source framework for distributed graph

32
00:01:29,180 --> 00:01:32,030
analytics called khru loop and finally I

33
00:01:32,030 --> 00:01:33,380
talked a little bit about about some

34
00:01:33,380 --> 00:01:35,180
optimizations of our solution if I find

35
00:01:35,180 --> 00:01:39,680
the time and of course conclude so part

36
00:01:39,680 --> 00:01:40,580
one is the problem

37
00:01:40,580 --> 00:01:44,810
so the general beta mining class of data

38
00:01:44,810 --> 00:01:46,520
mining problems is called a frequent

39
00:01:46,520 --> 00:01:48,950
pattern mining so and we are talking

40
00:01:48,950 --> 00:01:50,780
about the transactional setting which

41
00:01:50,780 --> 00:01:53,240
means we have a collection of things for

42
00:01:53,240 --> 00:01:55,340
example shopping baskets click streams

43
00:01:55,340 --> 00:01:57,620
XML documents chemical compounds

44
00:01:57,620 --> 00:02:00,110
whatever it's just depends on the data

45
00:02:00,110 --> 00:02:01,490
structure but we have a collection of

46
00:02:01,490 --> 00:02:03,770
let's say data sets which might have

47
00:02:03,770 --> 00:02:06,049
different structures and infrequent

48
00:02:06,049 --> 00:02:07,520
pattern mining we're interested in the

49
00:02:07,520 --> 00:02:09,318
patterns that for example occur in 80

50
00:02:09,318 --> 00:02:12,379
percent of these things and the frequent

51
00:02:12,379 --> 00:02:14,390
pattern mining can be categorized into

52
00:02:14,390 --> 00:02:16,280
the basic ones the frequent itemsets

53
00:02:16,280 --> 00:02:18,470
mining for example shopping baskets

54
00:02:18,470 --> 00:02:20,990
where we just interested in which things

55
00:02:20,990 --> 00:02:24,650
frequently co-occur together then we can

56
00:02:24,650 --> 00:02:29,210
if we add a constraint that seek mmm the

57
00:02:29,210 --> 00:02:30,950
order of items matters then we have

58
00:02:30,950 --> 00:02:32,600
frequent sequence mining for example a

59
00:02:32,600 --> 00:02:34,280
clickstream we're interested in which

60
00:02:34,280 --> 00:02:37,370
paths on a website go uses regulary or

61
00:02:37,370 --> 00:02:39,440
something like that then we can again

62
00:02:39,440 --> 00:02:41,690
extend it if we also consider branching

63
00:02:41,690 --> 00:02:44,420
of path then we are at frequent subtree

64
00:02:44,420 --> 00:02:47,150
mining which is a little bit more

65
00:02:47,150 --> 00:02:51,400
difficult in sequence mining but still

66
00:02:52,000 --> 00:02:55,040
very closely related to these things and

67
00:02:55,040 --> 00:02:56,870
then at the end if we are also

68
00:02:56,870 --> 00:02:58,910
interested in all the relationships so

69
00:02:58,910 --> 00:03:01,940
trees might have may have closing cycles

70
00:03:01,940 --> 00:03:04,100
and stuff like that then we have the

71
00:03:04,100 --> 00:03:06,410
problem of frequent subgraph mining and

72
00:03:06,410 --> 00:03:10,459
an example are chemical compounds so

73
00:03:10,459 --> 00:03:13,310
let's first go back to the simple

74
00:03:13,310 --> 00:03:16,280
variation of this problem some of you

75
00:03:16,280 --> 00:03:18,080
might know Association rules that this

76
00:03:18,080 --> 00:03:20,300
is something like people who brought and

77
00:03:20,300 --> 00:03:22,760
brought bread and butter often bought

78
00:03:22,760 --> 00:03:25,070
cheese and wine so we can use this for

79
00:03:25,070 --> 00:03:27,260
recommendations that's what some web

80
00:03:27,260 --> 00:03:30,230
shops do I guess for example if you

81
00:03:30,230 --> 00:03:32,420
already added two items to your shopping

82
00:03:32,420 --> 00:03:35,330
basket and Amazon for example then one

83
00:03:35,330 --> 00:03:37,970
way to recommend you other things is

84
00:03:37,970 --> 00:03:40,640
using Association rules which are based

85
00:03:40,640 --> 00:03:43,040
on the results of frequent pattern

86
00:03:43,040 --> 00:03:47,120
mining on past data sets in the frequent

87
00:03:47,120 --> 00:03:49,549
subgraph mining we can use it for

88
00:03:49,549 --> 00:03:51,920
similar things for example if we know

89
00:03:51,920 --> 00:03:53,900
that a substructure often occurs in

90
00:03:53,900 --> 00:03:56,810
legal stimulants and we have an unknown

91
00:03:56,810 --> 00:03:59,750
chemical substance which contains a sub

92
00:03:59,750 --> 00:04:01,610
structure which is known to be often

93
00:04:01,610 --> 00:04:04,760
part of illegal stimulants we can have a

94
00:04:04,760 --> 00:04:06,200
look on this substance if it's maybe

95
00:04:06,200 --> 00:04:08,390
it's a candidate for a new illegal

96
00:04:08,390 --> 00:04:11,239
stimulant I don't know if this example

97
00:04:11,239 --> 00:04:13,220
makes sense but I hope you get the idea

98
00:04:13,220 --> 00:04:18,798
so the problem definition just to

99
00:04:18,798 --> 00:04:21,500
conclude the introduction is our input

100
00:04:21,500 --> 00:04:23,570
is a graph collection so our collection

101
00:04:23,570 --> 00:04:26,330
of things in this case our graphs and we

102
00:04:26,330 --> 00:04:27,860
have a min support rash hold which

103
00:04:27,860 --> 00:04:30,349
specifies the minimum percentage of

104
00:04:30,349 --> 00:04:32,129
items of

105
00:04:32,129 --> 00:04:34,770
such a things which have to contain a

106
00:04:34,770 --> 00:04:36,599
pattern to be considered to be frequent

107
00:04:36,599 --> 00:04:40,080
and the output is the complete set of

108
00:04:40,080 --> 00:04:42,899
frequent subgraph patterns which are

109
00:04:42,899 --> 00:04:45,479
supported above the given threshold so

110
00:04:45,479 --> 00:04:50,550
them in support and to pray wait just

111
00:04:50,550 --> 00:04:55,800
give you a toy example on the bottom I

112
00:04:55,800 --> 00:04:58,379
hope it's with bull for most of you guys

113
00:04:58,379 --> 00:05:01,919
know so on the bottom and just and just

114
00:05:01,919 --> 00:05:03,769
believe me there there are three graphs

115
00:05:03,769 --> 00:05:08,719
this is often called a graph database

116
00:05:08,719 --> 00:05:21,089
and yeah Fairview I mean it's smaller

117
00:05:21,089 --> 00:05:25,369
but now more people enjoy the fun on

118
00:05:25,369 --> 00:05:27,449
seeing the graphs so this is the input

119
00:05:27,449 --> 00:05:30,509
it's a graph collection so we see the

120
00:05:30,509 --> 00:05:33,089
first I need to tell you in our problem

121
00:05:33,089 --> 00:05:34,649
scenario in the context of groups we are

122
00:05:34,649 --> 00:05:36,929
talking about directed multigraphs which

123
00:05:36,929 --> 00:05:40,139
is a difference to typical solutions so

124
00:05:40,139 --> 00:05:41,969
which means our edges have a direction

125
00:05:41,969 --> 00:05:43,559
like also in graph databases like near

126
00:05:43,559 --> 00:05:47,129
future and our we support multiple edges

127
00:05:47,129 --> 00:05:49,829
between any pair of vertices which is

128
00:05:49,829 --> 00:05:51,749
which changed the problem a little bit

129
00:05:51,749 --> 00:05:54,800
from the algorithmic point of view and

130
00:05:54,800 --> 00:05:57,479
yeah then we have now with three graphs

131
00:05:57,479 --> 00:06:01,079
and we're interested in sub graphs and

132
00:06:01,079 --> 00:06:03,029
which we can describe by patterns so a

133
00:06:03,029 --> 00:06:05,639
sub graph just to not confuse you a sub

134
00:06:05,639 --> 00:06:08,729
graph is a part of the graph and we're

135
00:06:08,729 --> 00:06:10,619
interested in such sub graphs which

136
00:06:10,619 --> 00:06:12,839
occur in at least 50 percent so in two

137
00:06:12,839 --> 00:06:17,849
of three and typical algorithms use the

138
00:06:17,849 --> 00:06:19,319
abstraction of a frequent pattern

139
00:06:19,319 --> 00:06:22,289
lettuce so which contains different

140
00:06:22,289 --> 00:06:24,659
levels in the first level there are all

141
00:06:24,659 --> 00:06:28,919
0 H sub graphs which means just vertices

142
00:06:28,919 --> 00:06:31,649
and we see we have a vertex a here and a

143
00:06:31,649 --> 00:06:33,209
or curious in this graph in this graph

144
00:06:33,209 --> 00:06:35,309
and in that one this is we have a

145
00:06:35,309 --> 00:06:38,699
frequency of 3 the same for B frequency

146
00:06:38,699 --> 00:06:41,159
of 3 we have C frequency of 1 because it

147
00:06:41,159 --> 00:06:42,959
only contains in this one and that we

148
00:06:42,959 --> 00:06:45,260
can say alright it's not a

149
00:06:45,260 --> 00:06:47,390
the threshold of 50% which would be two

150
00:06:47,390 --> 00:06:50,150
and that's we can prune it so now we

151
00:06:50,150 --> 00:06:53,420
continue and grow children from that we

152
00:06:53,420 --> 00:06:58,010
have for example a ap a BB or this loop

153
00:06:58,010 --> 00:07:01,910
pattern and they have their children of

154
00:07:01,910 --> 00:07:04,790
the 0h patterns and we still see okay

155
00:07:04,790 --> 00:07:07,100
this one has a frequency of three one

156
00:07:07,100 --> 00:07:09,200
and two one is below the threshold

157
00:07:09,200 --> 00:07:12,260
that's what we can prune it and finally

158
00:07:12,260 --> 00:07:14,240
if that's the largest frequent subgraph

159
00:07:14,240 --> 00:07:16,910
contained we have this 2h sub graph

160
00:07:16,910 --> 00:07:19,850
which is contained in two graphs at k

161
00:07:19,850 --> 00:07:22,160
tune this is T actually does the desired

162
00:07:22,160 --> 00:07:23,900
result but it's not and we do not

163
00:07:23,900 --> 00:07:25,550
implement it as a data structure it's

164
00:07:25,550 --> 00:07:29,240
just the theoretical model behind these

165
00:07:29,240 --> 00:07:32,210
algorithms and from the data structure

166
00:07:32,210 --> 00:07:33,710
point of view of course we have graphs

167
00:07:33,710 --> 00:07:35,090
and we have patterns which we need to

168
00:07:35,090 --> 00:07:36,950
store somehow but the most important

169
00:07:36,950 --> 00:07:39,770
thing are embeddings it's very similar

170
00:07:39,770 --> 00:07:42,230
to the previous talk with the pattern

171
00:07:42,230 --> 00:07:44,510
matching so we need embeddings which

172
00:07:44,510 --> 00:07:49,250
mapped the patterns to two actual sub

173
00:07:49,250 --> 00:07:51,530
graphs and in this case we can we see

174
00:07:51,530 --> 00:07:54,650
that due to Otto morphisms so a graph

175
00:07:54,650 --> 00:07:56,690
might contain a pattern multiple times

176
00:07:56,690 --> 00:07:58,970
which means this pattern is contained in

177
00:07:58,970 --> 00:08:01,790
this graph two times so once it's the

178
00:08:01,790 --> 00:08:04,640
Green Line so this vertex is this vertex

179
00:08:04,640 --> 00:08:06,320
and this vertex is this one and because

180
00:08:06,320 --> 00:08:08,780
this this vertices cannot be

181
00:08:08,780 --> 00:08:11,540
distinguished by their pattern we create

182
00:08:11,540 --> 00:08:14,390
a second embedding because of that so

183
00:08:14,390 --> 00:08:16,340
this is everything you need to know

184
00:08:16,340 --> 00:08:18,980
before I start explaining the algorithms

185
00:08:18,980 --> 00:08:21,770
so we have graphs which are the input

186
00:08:21,770 --> 00:08:24,170
elements we have sub graphs which are

187
00:08:24,170 --> 00:08:26,420
parts of the input elements we have

188
00:08:26,420 --> 00:08:29,510
patterns which are notes and our pattern

189
00:08:29,510 --> 00:08:31,850
letters which are isomorphic to sub

190
00:08:31,850 --> 00:08:33,710
graphs so isomorphic means there is a

191
00:08:33,710 --> 00:08:35,750
one-to-one mapping between vertices and

192
00:08:35,750 --> 00:08:38,450
edges and this is the tricky point of

193
00:08:38,450 --> 00:08:39,979
these algorithms the same as pattern

194
00:08:39,979 --> 00:08:42,530
matching and we have embeddings which

195
00:08:42,530 --> 00:08:44,540
are mappings between patterns and sub

196
00:08:44,540 --> 00:08:47,840
graphs and the challenges in

197
00:08:47,840 --> 00:08:49,730
implementing such algorithms especially

198
00:08:49,730 --> 00:08:51,200
in the context of shared-nothing

199
00:08:51,200 --> 00:08:54,350
clusters is that we need to meet the

200
00:08:54,350 --> 00:08:56,180
dataflow programming model in this case

201
00:08:56,180 --> 00:08:58,490
of Apache flink but this also applies to

202
00:08:58,490 --> 00:08:59,150
a Patras

203
00:08:59,150 --> 00:09:02,540
park or actually even Map Reduce and

204
00:09:02,540 --> 00:09:04,910
what we need to find is an efficient

205
00:09:04,910 --> 00:09:07,880
algorithm which does not rely on shared

206
00:09:07,880 --> 00:09:15,980
memory and I hope don't want to become

207
00:09:15,980 --> 00:09:18,650
too scientific now so I just explain

208
00:09:18,650 --> 00:09:20,990
your algorithm efficiency the problem of

209
00:09:20,990 --> 00:09:22,820
these algorithms is they contain the so

210
00:09:22,820 --> 00:09:24,770
called sub graph isomorphism problem

211
00:09:24,770 --> 00:09:26,210
which is np-complete

212
00:09:26,210 --> 00:09:29,270
we need to enumerate all isomorphisms

213
00:09:29,270 --> 00:09:31,880
between graphs actually where this very

214
00:09:31,880 --> 00:09:34,370
expensive and we cannot avoid this

215
00:09:34,370 --> 00:09:36,230
completely if we want to have the

216
00:09:36,230 --> 00:09:38,330
complete set of frequent sub graphs it's

217
00:09:38,330 --> 00:09:40,310
not possible to avoid this problem but

218
00:09:40,310 --> 00:09:42,140
what efficient algorithms can do to

219
00:09:42,140 --> 00:09:44,860
minimize the isomorphism testing effort

220
00:09:44,860 --> 00:09:48,230
so let me explain you the most of this

221
00:09:48,230 --> 00:09:50,840
work has been done in the early 2000s

222
00:09:50,840 --> 00:09:54,490
and but not in the context of

223
00:09:54,490 --> 00:09:56,630
distributed data flow systems but for

224
00:09:56,630 --> 00:09:58,940
single machines so there were a priori

225
00:09:58,940 --> 00:10:01,910
approaches these are based on a so

226
00:10:01,910 --> 00:10:03,200
called breadth-first search in the

227
00:10:03,200 --> 00:10:06,410
lettuce BFS which means we process first

228
00:10:06,410 --> 00:10:09,680
all frequent sub graphs of level 1 then

229
00:10:09,680 --> 00:10:11,900
generate candidates do pattern matching

230
00:10:11,900 --> 00:10:15,890
in the graph database find out frequent

231
00:10:15,890 --> 00:10:18,400
ones and join the frequent ones to two

232
00:10:18,400 --> 00:10:20,810
children and then we again do the

233
00:10:20,810 --> 00:10:22,850
pattern matching and repeat this until

234
00:10:22,850 --> 00:10:25,060
there are no frequent sub graphs left

235
00:10:25,060 --> 00:10:27,260
the good thing about this these

236
00:10:27,260 --> 00:10:29,330
algorithms is that they only need one

237
00:10:29,330 --> 00:10:31,640
iteration per level and it's allows a

238
00:10:31,640 --> 00:10:34,430
very effective frequency pruning but the

239
00:10:34,430 --> 00:10:36,560
problems of these algorithms are they

240
00:10:36,560 --> 00:10:37,550
contain the sub graph isomorphism

241
00:10:37,550 --> 00:10:40,130
testing in a pattern matching they

242
00:10:40,130 --> 00:10:41,540
contain additionally sub graph

243
00:10:41,540 --> 00:10:43,070
isomorphism testing in the candidate

244
00:10:43,070 --> 00:10:45,500
generation so when you have to frequent

245
00:10:45,500 --> 00:10:46,820
patterns you want to generate the child

246
00:10:46,820 --> 00:10:50,300
you need to enumerate all sub graphs of

247
00:10:50,300 --> 00:10:52,310
a certain level to generate children

248
00:10:52,310 --> 00:10:53,870
which is also very expensive

249
00:10:53,870 --> 00:10:57,140
you may generate candidates that don't

250
00:10:57,140 --> 00:10:59,480
even occur in a database because there

251
00:10:59,480 --> 00:11:01,430
are just candidates there's no guarantee

252
00:11:01,430 --> 00:11:04,550
for them to to a cure and the candidate

253
00:11:04,550 --> 00:11:07,010
generation itself is hard to paralyzed

254
00:11:07,010 --> 00:11:08,960
without shared memory because you need

255
00:11:08,960 --> 00:11:11,180
actually you need to build the cross

256
00:11:11,180 --> 00:11:13,029
product of all

257
00:11:13,029 --> 00:11:15,439
combination it's yeah just a lot of

258
00:11:15,439 --> 00:11:18,229
combinations this is why the more

259
00:11:18,229 --> 00:11:20,269
efficient a single machine approaches

260
00:11:20,269 --> 00:11:22,189
are based on a depth-first search in a

261
00:11:22,189 --> 00:11:24,499
lattice which means this is the BFS and

262
00:11:24,499 --> 00:11:28,009
this is the DFS they need they go from

263
00:11:28,009 --> 00:11:29,929
let's say from this pattern this is

264
00:11:29,929 --> 00:11:32,059
let's say the root pattern to to this

265
00:11:32,059 --> 00:11:34,519
pattern then go to this and the until no

266
00:11:34,519 --> 00:11:36,349
more frequent patterns are found then

267
00:11:36,349 --> 00:11:39,369
they jump back to a frequent parent and

268
00:11:39,369 --> 00:11:41,779
and so on and so on and the idea behind

269
00:11:41,779 --> 00:11:43,729
these algorithms is that we can skip

270
00:11:43,729 --> 00:11:47,199
some of the links in a lattice due to

271
00:11:47,199 --> 00:11:49,489
specific tricks which I will not explain

272
00:11:49,489 --> 00:11:51,439
now because this then really takes a lot

273
00:11:51,439 --> 00:11:53,149
of time but it's possible with these

274
00:11:53,149 --> 00:11:56,079
algorithms to avoid checking all the

275
00:11:56,079 --> 00:11:59,569
combinations so and the idea is we have

276
00:11:59,569 --> 00:12:01,339
an edge wise extension of frequent

277
00:12:01,339 --> 00:12:04,729
patterns and the growth rules can reduce

278
00:12:04,729 --> 00:12:07,069
the search to retrieve for example

279
00:12:07,069 --> 00:12:08,809
there's a G span algorithm as some very

280
00:12:08,809 --> 00:12:11,659
popular example of of this category the

281
00:12:11,659 --> 00:12:13,099
problem is where the item is amorphous

282
00:12:13,099 --> 00:12:16,279
and still occurs is that sometimes we we

283
00:12:16,279 --> 00:12:18,679
don't want to visit for example this

284
00:12:18,679 --> 00:12:22,159
edge and or link in the lattice but we

285
00:12:22,159 --> 00:12:24,859
accidentally a wizard it this this is

286
00:12:24,859 --> 00:12:28,369
why we need to we need to check the

287
00:12:28,369 --> 00:12:29,959
graph representation if it's really a

288
00:12:29,959 --> 00:12:33,349
valid if it's really on this tree that

289
00:12:33,349 --> 00:12:36,559
we wanted to find so we look at these

290
00:12:36,559 --> 00:12:38,720
algorithms okay it automatically finds

291
00:12:38,720 --> 00:12:40,939
canonical labels which means we do not

292
00:12:40,939 --> 00:12:44,179
we can compare them graph patterns by a

293
00:12:44,179 --> 00:12:47,119
simple string comparison the isomorphism

294
00:12:47,119 --> 00:12:49,069
testing is reduced to a minimum we just

295
00:12:49,069 --> 00:12:51,819
need to validate these labels which is

296
00:12:51,819 --> 00:12:54,559
the end we only checked patterns that

297
00:12:54,559 --> 00:12:57,619
actually exist so we solved a lot of the

298
00:12:57,619 --> 00:13:00,169
problems of the a priori algorithms but

299
00:13:00,169 --> 00:13:02,119
a problem is we need a large number of

300
00:13:02,119 --> 00:13:04,489
iterations for that and it can lead in a

301
00:13:04,489 --> 00:13:06,470
shared nothing cluster rich with many

302
00:13:06,470 --> 00:13:10,849
many many many workers it can lead to

303
00:13:10,849 --> 00:13:13,309
unbalanced parallelization because we

304
00:13:13,309 --> 00:13:15,979
can maybe paralyze a single paths but

305
00:13:15,979 --> 00:13:18,829
not a whole level the more idea of the

306
00:13:18,829 --> 00:13:20,899
new data flow systems is to bring the

307
00:13:20,899 --> 00:13:22,939
computation to the data which means

308
00:13:22,939 --> 00:13:25,459
processing all the for example all the

309
00:13:25,459 --> 00:13:26,240
embeddings

310
00:13:26,240 --> 00:13:28,009
of the same edge count at the same time

311
00:13:28,009 --> 00:13:30,769
and this is not easily possible with

312
00:13:30,769 --> 00:13:35,569
this DFS approach come back to our toy

313
00:13:35,569 --> 00:13:38,689
example we see this is why I use dotted

314
00:13:38,689 --> 00:13:39,920
lines for some of the links in the

315
00:13:39,920 --> 00:13:42,230
lattice so the a priori approaches would

316
00:13:42,230 --> 00:13:44,980
go all the links or there are even some

317
00:13:44,980 --> 00:13:47,629
optimizations which maybe skip some but

318
00:13:47,629 --> 00:13:49,730
still this is the basic difference and

319
00:13:49,730 --> 00:13:51,829
in a pattern growth one we can just

320
00:13:51,829 --> 00:13:56,720
leave out the dotted ones okay so now

321
00:13:56,720 --> 00:13:59,089
this is everything you need to know

322
00:13:59,089 --> 00:14:01,040
about frequent subgraph mining and the

323
00:14:01,040 --> 00:14:03,949
research in the past and now we we are

324
00:14:03,949 --> 00:14:05,839
facing these problems in the context of

325
00:14:05,839 --> 00:14:07,369
distributed data flow systems on

326
00:14:07,369 --> 00:14:12,499
shared-nothing clusters and the solution

327
00:14:12,499 --> 00:14:15,019
to this problem is something we call a

328
00:14:15,019 --> 00:14:17,329
level wise pattern growth which means we

329
00:14:17,329 --> 00:14:20,119
just do a parallel depth-first search in

330
00:14:20,119 --> 00:14:22,220
the lattice twins we process the lattice

331
00:14:22,220 --> 00:14:27,170
level wise but but still skip the links

332
00:14:27,170 --> 00:14:29,230
we do not want to visit and still avoid

333
00:14:29,230 --> 00:14:32,480
the isomorphism problem due to change at

334
00:14:32,480 --> 00:14:33,980
candidate generation for example and

335
00:14:33,980 --> 00:14:37,339
this means we only need kmx iterations

336
00:14:37,339 --> 00:14:38,809
which is the maximum edge count of the

337
00:14:38,809 --> 00:14:40,879
largest frequent pattern and can skip

338
00:14:40,879 --> 00:14:42,529
these links and we still minimize

339
00:14:42,529 --> 00:14:45,790
isomorphism testing and this approach

340
00:14:45,790 --> 00:14:48,829
fits the data flow programming model

341
00:14:48,829 --> 00:14:52,639
very well I will show you the using some

342
00:14:52,639 --> 00:14:54,170
pseudo code how we can implement this on

343
00:14:54,170 --> 00:15:01,040
a patch of link so for all who don't

344
00:15:01,040 --> 00:15:03,170
know a patch of link the basic

345
00:15:03,170 --> 00:15:04,790
attraction is you have data sets and

346
00:15:04,790 --> 00:15:07,490
then you have transformations on these

347
00:15:07,490 --> 00:15:10,519
data sets which where an high order

348
00:15:10,519 --> 00:15:13,639
function is a parameter and it's similar

349
00:15:13,639 --> 00:15:16,069
to lambda expressions but for data sets

350
00:15:16,069 --> 00:15:17,660
and the idea is that the function is

351
00:15:17,660 --> 00:15:20,569
executed for each element of a data set

352
00:15:20,569 --> 00:15:22,850
or for group of elements for example so

353
00:15:22,850 --> 00:15:25,429
but there's no they completely run

354
00:15:25,429 --> 00:15:27,829
concurrently are executed concurrently

355
00:15:27,829 --> 00:15:32,660
and all things which require global

356
00:15:32,660 --> 00:15:36,499
knowledge exchange I need to artificial

357
00:15:36,499 --> 00:15:38,560
artificial

358
00:15:38,560 --> 00:15:41,529
I need to create a careers in the data

359
00:15:41,529 --> 00:15:44,290
flow to exchange global knowledge so

360
00:15:44,290 --> 00:15:46,090
that's the the problem in the

361
00:15:46,090 --> 00:15:49,240
shared-nothing context and what you

362
00:15:49,240 --> 00:15:51,250
would do in a usual programming language

363
00:15:51,250 --> 00:15:53,260
okay you know these constraints but the

364
00:15:53,260 --> 00:15:54,700
native programming would be there would

365
00:15:54,700 --> 00:15:57,190
be a data set of graphs there would be a

366
00:15:57,190 --> 00:15:59,110
data set of all frequent patterns and

367
00:15:59,110 --> 00:16:01,510
there would be a data set of kedge

368
00:16:01,510 --> 00:16:03,100
frequent patterns that's all all the

369
00:16:03,100 --> 00:16:05,320
data sets we need and then we have a

370
00:16:05,320 --> 00:16:08,140
loop which terminates until we do not

371
00:16:08,140 --> 00:16:10,089
find any K edge frequent parents anymore

372
00:16:10,089 --> 00:16:12,580
and at the end we're interested in all

373
00:16:12,580 --> 00:16:14,380
frequent patterns that's what our

374
00:16:14,380 --> 00:16:17,560
algorithm should do what can we do for

375
00:16:17,560 --> 00:16:20,950
example we do a pattern growth in the in

376
00:16:20,950 --> 00:16:23,260
the beginning which means in which we

377
00:16:23,260 --> 00:16:24,910
use something called broadcasting and

378
00:16:24,910 --> 00:16:27,640
think it's like a very efficient cross

379
00:16:27,640 --> 00:16:29,589
product in a distributed data flow we

380
00:16:29,589 --> 00:16:34,720
just sent all K last iterations frequent

381
00:16:34,720 --> 00:16:37,300
patterns to all machines and then we can

382
00:16:37,300 --> 00:16:39,310
access them in the pattern growth step

383
00:16:39,310 --> 00:16:42,370
and initially it's empty so this is the

384
00:16:42,370 --> 00:16:46,570
dummy route in the lattice we apply the

385
00:16:46,570 --> 00:16:48,550
pattern growth using a map function

386
00:16:48,550 --> 00:16:50,350
which means we update the graphs because

387
00:16:50,350 --> 00:16:53,500
we not only the graph dataset does not

388
00:16:53,500 --> 00:16:56,320
only contain grass it also contains a

389
00:16:56,320 --> 00:17:00,270
map in between patterns and embeddings

390
00:17:00,270 --> 00:17:03,640
so which means in the pattern grow step

391
00:17:03,640 --> 00:17:06,670
we just check the map all the keys which

392
00:17:06,670 --> 00:17:08,980
are also contained in the frequent

393
00:17:08,980 --> 00:17:11,619
pattern set we need to grow and then the

394
00:17:11,619 --> 00:17:13,810
next step we report the patterns so

395
00:17:13,810 --> 00:17:16,390
after we grew patterns we report them

396
00:17:16,390 --> 00:17:19,990
using flat Maps flat map is is a like

397
00:17:19,990 --> 00:17:21,939
map function but can be an arbitrary

398
00:17:21,939 --> 00:17:24,910
cardinality output so it could output 0

399
00:17:24,910 --> 00:17:28,630
or many patterns then we it now it

400
00:17:28,630 --> 00:17:30,520
becomes a little relational we group by

401
00:17:30,520 --> 00:17:32,290
patterns so it's just the key or it's

402
00:17:32,290 --> 00:17:33,910
very similar to the reduce function of

403
00:17:33,910 --> 00:17:36,460
Map Reduce we just group by key we sum

404
00:17:36,460 --> 00:17:38,530
the frequency we filter out frequent

405
00:17:38,530 --> 00:17:40,360
ones and we additionally need to filter

406
00:17:40,360 --> 00:17:43,570
Bell at once because sometimes there are

407
00:17:43,570 --> 00:17:45,940
some false positives and we found out in

408
00:17:45,940 --> 00:17:47,770
our evaluations that it's far more

409
00:17:47,770 --> 00:17:51,790
efficient to group and filter

410
00:17:51,790 --> 00:17:54,790
invalid patterns instead of validating

411
00:17:54,790 --> 00:17:56,440
them in this step because in this step

412
00:17:56,440 --> 00:17:58,720
we have to validate for each graph and

413
00:17:58,720 --> 00:18:00,670
each pattern and here we only have to

414
00:18:00,670 --> 00:18:04,390
validate for each pattern in the whole

415
00:18:04,390 --> 00:18:06,940
system which means imagine we have an

416
00:18:06,940 --> 00:18:08,590
input size of like ten millions of

417
00:18:08,590 --> 00:18:10,870
graphs we do not repeat the same

418
00:18:10,870 --> 00:18:13,030
operations ten million times but do it

419
00:18:13,030 --> 00:18:15,400
here and this is in comparison to the

420
00:18:15,400 --> 00:18:17,830
complexity of counting a large data set

421
00:18:17,830 --> 00:18:21,660
it's it's basically free

422
00:18:21,660 --> 00:18:25,210
yeah and finally we add the current

423
00:18:25,210 --> 00:18:27,310
rounds frequent patterns to the all

424
00:18:27,310 --> 00:18:29,380
frequent patterns data set using a Union

425
00:18:29,380 --> 00:18:32,410
operator and that's that's how natively

426
00:18:32,410 --> 00:18:36,760
everyone would program it but a problem

427
00:18:36,760 --> 00:18:37,660
is that

428
00:18:37,660 --> 00:18:39,580
Flinx iteration it's a dataflow

429
00:18:39,580 --> 00:18:41,380
iteration it's not like a typical while

430
00:18:41,380 --> 00:18:44,890
loop it's something related to the batch

431
00:18:44,890 --> 00:18:47,770
API of Apache flink it allows only

432
00:18:47,770 --> 00:18:50,290
exactly one data set to be passed along

433
00:18:50,290 --> 00:18:54,070
iterations and problem is here we have

434
00:18:54,070 --> 00:18:56,500
actually we have three data sets which

435
00:18:56,500 --> 00:18:58,480
which we manipulate during our

436
00:18:58,480 --> 00:19:01,000
iterations so in but there is a

437
00:19:01,000 --> 00:19:02,770
constraint that from iteration body we

438
00:19:02,770 --> 00:19:04,630
cannot access these data sets so this

439
00:19:04,630 --> 00:19:08,050
doesn't work so the first one the fpk

440
00:19:08,050 --> 00:19:09,490
there's a quite simple solution to that

441
00:19:09,490 --> 00:19:13,050
we just pull it out of the iteration and

442
00:19:13,050 --> 00:19:15,850
repeat us a little bit and just

443
00:19:15,850 --> 00:19:18,460
instantiate the set of KH frequent

444
00:19:18,460 --> 00:19:20,830
patterns inside the loop so we do not we

445
00:19:20,830 --> 00:19:24,190
can avoid the external access but this

446
00:19:24,190 --> 00:19:27,370
does not work for the for this Union

447
00:19:27,370 --> 00:19:29,380
operation and our data set collecting

448
00:19:29,380 --> 00:19:30,940
all the frequent patterns of all

449
00:19:30,940 --> 00:19:34,150
iterations this is where we use them

450
00:19:34,150 --> 00:19:37,120
very hacky approach we just at the

451
00:19:37,120 --> 00:19:39,460
beginning of the data flow we added to

452
00:19:39,460 --> 00:19:43,600
the graph data set a new empty graph

453
00:19:43,600 --> 00:19:46,870
element which is um which is actually

454
00:19:46,870 --> 00:19:48,580
and it's an empty graph with an empty

455
00:19:48,580 --> 00:19:51,400
map and because it's an empty graph and

456
00:19:51,400 --> 00:19:52,960
we ensure that it's the only empty graph

457
00:19:52,960 --> 00:19:55,030
in the whole data set we can identify it

458
00:19:55,030 --> 00:19:57,430
and then we extend this map function not

459
00:19:57,430 --> 00:19:59,560
only we do not only apply pattern growth

460
00:19:59,560 --> 00:20:02,230
but pattern growth or store so we just

461
00:20:02,230 --> 00:20:04,930
check if the graph is empty we can

462
00:20:04,930 --> 00:20:07,030
use its pattern embeddings map to store

463
00:20:07,030 --> 00:20:11,320
frequent patterns so because we can

464
00:20:11,320 --> 00:20:12,880
reuse we don't need any additional data

465
00:20:12,880 --> 00:20:14,920
structure we could just used in the data

466
00:20:14,920 --> 00:20:16,870
structure we already have and at the end

467
00:20:16,870 --> 00:20:19,240
we filter out this single dummy graph

468
00:20:19,240 --> 00:20:20,770
which was collecting all the frequent

469
00:20:20,770 --> 00:20:23,160
patterns and use a flat map which

470
00:20:23,160 --> 00:20:26,320
creates a new pattern element in the

471
00:20:26,320 --> 00:20:29,700
output dataset including its frequency

472
00:20:29,700 --> 00:20:33,670
frequency we also can encode in the

473
00:20:33,670 --> 00:20:37,090
mappings and have the same result so

474
00:20:37,090 --> 00:20:39,280
this really works there was a solution

475
00:20:39,280 --> 00:20:44,230
to our problem okay so this was the the

476
00:20:44,230 --> 00:20:46,720
basic idea how we hope you understand

477
00:20:46,720 --> 00:20:48,820
the problem and how we implemented this

478
00:20:48,820 --> 00:20:50,470
on a distributed data flow system like

479
00:20:50,470 --> 00:20:53,350
Apache flink and now we did some

480
00:20:53,350 --> 00:20:55,420
optimizations to this problem so

481
00:20:55,420 --> 00:20:56,830
actually there are four of them and

482
00:20:56,830 --> 00:20:58,990
because two of them are very difficult

483
00:20:58,990 --> 00:21:01,540
to explain without becoming too

484
00:21:01,540 --> 00:21:04,570
scientific we just skip them so one is

485
00:21:04,570 --> 00:21:06,730
it's even more efficient to do the

486
00:21:06,730 --> 00:21:08,380
validation step in the combined function

487
00:21:08,380 --> 00:21:11,560
because it matters to have fewer tuples

488
00:21:11,560 --> 00:21:13,420
before you network traffic this course

489
00:21:13,420 --> 00:21:16,080
is for everyone's programming with

490
00:21:16,080 --> 00:21:18,400
distributed data flow systems it seems

491
00:21:18,400 --> 00:21:20,650
obvious and we use a merge-join

492
00:21:20,650 --> 00:21:22,840
instead of a cross - during the pattern

493
00:21:22,840 --> 00:21:24,850
growth process but this needs a lot of

494
00:21:24,850 --> 00:21:26,860
explanation now but what we also do we

495
00:21:26,860 --> 00:21:29,230
do a pre-processing where we use

496
00:21:29,230 --> 00:21:30,700
dictionary and code a dictionary

497
00:21:30,700 --> 00:21:32,710
encoding and label pruning to reduce the

498
00:21:32,710 --> 00:21:35,950
input size and the most efficient thing

499
00:21:35,950 --> 00:21:38,890
is we do a pattern encoding and with a

500
00:21:38,890 --> 00:21:41,040
fast and effective compression technique

501
00:21:41,040 --> 00:21:42,940
so the pre-processing

502
00:21:42,940 --> 00:21:46,060
idea is basically we keep only vertices

503
00:21:46,060 --> 00:21:50,200
with frequent labels then if we if we

504
00:21:50,200 --> 00:21:52,180
remove them we also remove some edges

505
00:21:52,180 --> 00:21:54,160
and then we consider only the remaining

506
00:21:54,160 --> 00:21:57,720
edges which are not which never occur in

507
00:21:57,720 --> 00:22:00,880
Congo with frequent labels frequent

508
00:22:00,880 --> 00:22:03,010
vertices and also count their

509
00:22:03,010 --> 00:22:05,830
frequencies and we can also drop all the

510
00:22:05,830 --> 00:22:08,770
edges with infrequent labels and based

511
00:22:08,770 --> 00:22:10,960
in in these steps we have the

512
00:22:10,960 --> 00:22:12,820
frequencies and all the labels so we can

513
00:22:12,820 --> 00:22:14,290
just reduce them to create dictionaries

514
00:22:14,290 --> 00:22:16,870
between strings and integers and because

515
00:22:16,870 --> 00:22:18,730
the frequent sub graph mining

516
00:22:18,730 --> 00:22:20,799
maintains a lot of comparison equality

517
00:22:20,799 --> 00:22:23,410
hash code building whatever that's it's

518
00:22:23,410 --> 00:22:25,210
it's much more efficient to use integer

519
00:22:25,210 --> 00:22:28,660
labels and set of strings and the

520
00:22:28,660 --> 00:22:30,790
workflow is then as follows we first

521
00:22:30,790 --> 00:22:32,710
encode graphs we process the actual

522
00:22:32,710 --> 00:22:34,179
mining and we decode the patterns

523
00:22:34,179 --> 00:22:37,390
because in many scenarios we we do not

524
00:22:37,390 --> 00:22:39,940
want patterns consisting of numbers we

525
00:22:39,940 --> 00:22:42,070
just we really want our patterns like

526
00:22:42,070 --> 00:22:46,390
RDF patterns for example so just to

527
00:22:46,390 --> 00:22:49,780
explain this approach we see here

528
00:22:49,780 --> 00:22:51,460
there's a vertex seal which contains

529
00:22:51,460 --> 00:22:53,350
it's only contained in a single graph

530
00:22:53,350 --> 00:22:55,809
and we see here is a edge labeled B

531
00:22:55,809 --> 00:22:59,140
which if we only consider the edge label

532
00:22:59,140 --> 00:23:00,970
it's it's still frequent because it

533
00:23:00,970 --> 00:23:02,799
contains in here and here but if we

534
00:23:02,799 --> 00:23:06,460
first remove this C guy then this p1 to

535
00:23:06,460 --> 00:23:10,540
preserve consistency with gout U and B

536
00:23:10,540 --> 00:23:12,940
is not frequent anymore and if we do

537
00:23:12,940 --> 00:23:14,919
this in a pre-processing these patterns

538
00:23:14,919 --> 00:23:20,049
will never be discovered and the second

539
00:23:20,049 --> 00:23:23,770
optimization technique is the pattern

540
00:23:23,770 --> 00:23:25,480
encoding and compression so we do not

541
00:23:25,480 --> 00:23:27,730
store the graphs in a typical way as you

542
00:23:27,730 --> 00:23:31,570
natively would program graphs using Java

543
00:23:31,570 --> 00:23:33,400
objects and so on because we found out

544
00:23:33,400 --> 00:23:36,340
that avoiding Java objects is one of the

545
00:23:36,340 --> 00:23:38,679
most efficient tuning techniques you can

546
00:23:38,679 --> 00:23:43,510
do in a patch of link this is at least

547
00:23:43,510 --> 00:23:46,450
for this scenario so we use multiplexed

548
00:23:46,450 --> 00:23:49,299
integer arrays to represent patterns and

549
00:23:49,299 --> 00:23:51,100
graphs and embeddings we represent

550
00:23:51,100 --> 00:23:52,890
everything in multiplex integer arrays

551
00:23:52,890 --> 00:23:56,110
which require K times so the edge count

552
00:23:56,110 --> 00:23:58,870
we need six positions in the array for

553
00:23:58,870 --> 00:24:01,840
each edge and we know that we only have

554
00:24:01,840 --> 00:24:03,669
positive values and we have very low

555
00:24:03,669 --> 00:24:06,820
upper bounds of the values so the upper

556
00:24:06,820 --> 00:24:09,100
bound is each count or the size of the

557
00:24:09,100 --> 00:24:11,559
label dictionaries and typically we do

558
00:24:11,559 --> 00:24:13,929
not exceed four to eight bits for this

559
00:24:13,929 --> 00:24:15,580
case I say just typically of course you

560
00:24:15,580 --> 00:24:17,860
can create datasets where it's different

561
00:24:17,860 --> 00:24:19,950
but just say in the most use cases

562
00:24:19,950 --> 00:24:23,770
that's enough this is much less than 32

563
00:24:23,770 --> 00:24:25,870
bits which is usually required for a

564
00:24:25,870 --> 00:24:27,820
single integer so we can use an integer

565
00:24:27,820 --> 00:24:30,130
compression and we can roughly in our

566
00:24:30,130 --> 00:24:31,860
data sets

567
00:24:31,860 --> 00:24:35,370
encode a complete edge from six to one

568
00:24:35,370 --> 00:24:38,730
integers in this case which and we use

569
00:24:38,730 --> 00:24:42,179
simple 16 from this nice open source

570
00:24:42,179 --> 00:24:46,350
package for this and the effect of this

571
00:24:46,350 --> 00:24:48,660
is we decrease network traffic because

572
00:24:48,660 --> 00:24:50,160
the patterns are traveled over the

573
00:24:50,160 --> 00:24:52,110
network are smaller we have a smaller

574
00:24:52,110 --> 00:24:54,000
grouping key in the group by operation

575
00:24:54,000 --> 00:24:57,270
which means this is notably faster and

576
00:24:57,270 --> 00:24:59,700
also the map access when we access the

577
00:24:59,700 --> 00:25:02,370
embeddings of our patterns that we have

578
00:25:02,370 --> 00:25:04,440
much smaller map keys so also this is

579
00:25:04,440 --> 00:25:07,970
more efficient then additionally we

580
00:25:07,970 --> 00:25:10,200
experimented with graph and embedding

581
00:25:10,200 --> 00:25:12,870
compression to mainly to decrease memory

582
00:25:12,870 --> 00:25:16,140
usage and to allow fastest a realization

583
00:25:16,140 --> 00:25:17,880
between transformations and this is also

584
00:25:17,880 --> 00:25:19,320
something we have learned this really

585
00:25:19,320 --> 00:25:22,500
matters in a patch of link and the

586
00:25:22,500 --> 00:25:24,179
decompression we execute only on demand

587
00:25:24,179 --> 00:25:25,980
for example and beddings

588
00:25:25,980 --> 00:25:29,820
of infrequent patterns will never be

589
00:25:29,820 --> 00:25:31,320
decompressed and so on though there are

590
00:25:31,320 --> 00:25:35,220
a lot of detailed optimizations yeah

591
00:25:35,220 --> 00:25:38,549
then we did some optimized evaluations

592
00:25:38,549 --> 00:25:42,299
so we used I do not talk about absolute

593
00:25:42,299 --> 00:25:43,679
run times now because it doesn't make

594
00:25:43,679 --> 00:25:45,240
sense if you do not have any background

595
00:25:45,240 --> 00:25:46,799
knowledge about the datasets and was it

596
00:25:46,799 --> 00:25:48,990
mean but what we did here in these

597
00:25:48,990 --> 00:25:50,850
experiments we have two datasets one is

598
00:25:50,850 --> 00:25:53,669
synthetic 2 we have 1 million grafts 90

599
00:25:53,669 --> 00:25:55,380
percent support but it gives already a

600
00:25:55,380 --> 00:25:56,970
lot of patterns this dataset and

601
00:25:56,970 --> 00:26:00,480
contains directed multi graphs and the

602
00:26:00,480 --> 00:26:03,210
classical scenario which is used by all

603
00:26:03,210 --> 00:26:05,580
other work in the field of frequent sub

604
00:26:05,580 --> 00:26:07,230
graph mining so molecular data set with

605
00:26:07,230 --> 00:26:09,480
undirected simple graphs with 1 million

606
00:26:09,480 --> 00:26:11,549
graphs and 10 percent support so it has

607
00:26:11,549 --> 00:26:13,950
a quite low support and it said it's 1

608
00:26:13,950 --> 00:26:17,010
million molecules it's mean with other

609
00:26:17,010 --> 00:26:18,540
types of data you can have big data

610
00:26:18,540 --> 00:26:21,750
problems with much larger input sizes

611
00:26:21,750 --> 00:26:23,790
but here the problem is the are the

612
00:26:23,790 --> 00:26:26,610
intermediate results and computing

613
00:26:26,610 --> 00:26:29,460
complexity so we see we used this data

614
00:26:29,460 --> 00:26:32,520
set and scale up from 6 to 96 threats

615
00:26:32,520 --> 00:26:35,730
which is roughly the same as 1 to 16

616
00:26:35,730 --> 00:26:38,549
nodes in a cluster machines and we see

617
00:26:38,549 --> 00:26:41,040
that especially on this synthetic data

618
00:26:41,040 --> 00:26:43,530
set for which we we optimized our

619
00:26:43,530 --> 00:26:46,140
argumentum for directed multigraphs

620
00:26:46,140 --> 00:26:48,860
it's shows we're quite good scale-up and

621
00:26:48,860 --> 00:26:51,630
for the even for the molecular database

622
00:26:51,630 --> 00:26:54,300
its it's not bad it's not linear and

623
00:26:54,300 --> 00:26:58,980
it's a but still quite good and then we

624
00:26:58,980 --> 00:27:02,240
also evaluated the impact of our

625
00:27:02,240 --> 00:27:05,190
optimizations so depending on the data

626
00:27:05,190 --> 00:27:07,560
set in this data set again we created

627
00:27:07,560 --> 00:27:09,780
many many infrequent labels that this

628
00:27:09,780 --> 00:27:12,360
technique is very efficient too of

629
00:27:12,360 --> 00:27:14,550
course but there are maybe there are

630
00:27:14,550 --> 00:27:16,800
real-world data sets such like RDF for

631
00:27:16,800 --> 00:27:18,620
example which have these characteristics

632
00:27:18,620 --> 00:27:20,820
which we consider at the design of this

633
00:27:20,820 --> 00:27:23,180
synthetic data set so we see that

634
00:27:23,180 --> 00:27:26,670
overall scale abilities all parallelism

635
00:27:26,670 --> 00:27:28,530
levels so from six to ninety six

636
00:27:28,530 --> 00:27:30,900
parallel slots we see that the

637
00:27:30,900 --> 00:27:33,090
dictionary encoding in this case can

638
00:27:33,090 --> 00:27:37,530
slow down the total run time by up to

639
00:27:37,530 --> 00:27:40,260
400% then we just avoided pattern

640
00:27:40,260 --> 00:27:42,210
compression so we only use the integer

641
00:27:42,210 --> 00:27:45,840
representation which is already like ten

642
00:27:45,840 --> 00:27:48,180
times faster than using strings

643
00:27:48,180 --> 00:27:51,810
this goes brings is a notable effect

644
00:27:51,810 --> 00:27:54,600
especially on the molecular database the

645
00:27:54,600 --> 00:27:57,210
embedding compression still has a

646
00:27:57,210 --> 00:28:00,060
notable impact we probably due to the

647
00:28:00,060 --> 00:28:02,910
serialization time but graph compression

648
00:28:02,910 --> 00:28:06,270
it's it's a little tuning effect but

649
00:28:06,270 --> 00:28:09,080
there it's not too much all right

650
00:28:09,080 --> 00:28:12,200
so conclusion what we did is the first

651
00:28:12,200 --> 00:28:15,030
approach to graph collection frequent

652
00:28:15,030 --> 00:28:17,010
sub graph mining using an in-memory data

653
00:28:17,010 --> 00:28:18,810
flow system they are MapReduce

654
00:28:18,810 --> 00:28:21,060
approaches but in this kind of system by

655
00:28:21,060 --> 00:28:23,150
best of our knowledge we were first and

656
00:28:23,150 --> 00:28:25,650
we support direct with multi graphs from

657
00:28:25,650 --> 00:28:27,390
arbitrary string labeled inputs or you

658
00:28:27,390 --> 00:28:29,340
can just enter a string labeled input

659
00:28:29,340 --> 00:28:31,050
file and you get string labeled a

660
00:28:31,050 --> 00:28:33,330
patterns we do not need to do any

661
00:28:33,330 --> 00:28:37,260
external pre-processing it works but

662
00:28:37,260 --> 00:28:39,800
using flink requires some workarounds as

663
00:28:39,800 --> 00:28:42,840
is you have seen and we think it's a

664
00:28:42,840 --> 00:28:44,250
much better choice than MapReduce

665
00:28:44,250 --> 00:28:46,530
because in each iteration you have to

666
00:28:46,530 --> 00:28:48,420
update three data sets and this is some

667
00:28:48,420 --> 00:28:50,790
this is also from MapReduce you also

668
00:28:50,790 --> 00:28:52,890
need workarounds to solve these problems

669
00:28:52,890 --> 00:28:56,070
but these MapReduce workarounds are by

670
00:28:56,070 --> 00:28:57,990
far more expensive than the workarounds

671
00:28:57,990 --> 00:28:59,070
you need for the

672
00:28:59,070 --> 00:29:02,789
laughs link based approach and the

673
00:29:02,789 --> 00:29:04,649
availability of the it's part of the

674
00:29:04,649 --> 00:29:06,950
group framework this distributed

675
00:29:06,950 --> 00:29:10,620
framework for cross processing it's it

676
00:29:10,620 --> 00:29:13,529
fits its algorithm API the current

677
00:29:13,529 --> 00:29:15,750
version you will find in the master

678
00:29:15,750 --> 00:29:17,519
branch it's not optimized it's still

679
00:29:17,519 --> 00:29:19,590
using Java objects and strings and stuff

680
00:29:19,590 --> 00:29:21,690
like that so it's not very efficient and

681
00:29:21,690 --> 00:29:23,250
the optimized version is still a messy

682
00:29:23,250 --> 00:29:25,590
research prototype but will be merged to

683
00:29:25,590 --> 00:29:49,110
the mrsa soon so thank you know we have

684
00:29:49,110 --> 00:29:50,730
the problem is the availability of such

685
00:29:50,730 --> 00:29:52,529
data sets so we have a data generator

686
00:29:52,529 --> 00:29:56,029
which creates this type of theta but

687
00:29:56,029 --> 00:29:58,769
this is why we use the synthetic data

688
00:29:58,769 --> 00:30:01,970
set which contains loops parallel edges

689
00:30:01,970 --> 00:30:09,500
some some many autumn or physics yeah

690
00:30:11,830 --> 00:30:14,899
[Music]

691
00:30:16,399 --> 00:30:23,490
yeah yeah but yeah molecule data of

692
00:30:23,490 --> 00:30:24,870
course I have there are tons of

693
00:30:24,870 --> 00:30:27,419
molecular data sets but the thing is

694
00:30:27,419 --> 00:30:30,090
that they do not in molecule datasets

695
00:30:30,090 --> 00:30:33,049
you have only two types of edges it's a

696
00:30:33,049 --> 00:30:36,269
single a single bound and double bound

697
00:30:36,269 --> 00:30:38,669
you have a very limited set of vertex

698
00:30:38,669 --> 00:30:43,950
labels and it's very they're a very low

699
00:30:43,950 --> 00:30:45,720
number of specific patterns that are

700
00:30:45,720 --> 00:30:48,360
cure like you have at some cycles so

701
00:30:48,360 --> 00:30:50,460
molecule of course you can tune the

702
00:30:50,460 --> 00:30:52,919
algorithm to molecule databases but it's

703
00:30:52,919 --> 00:30:55,350
we wanted to make a general approach

704
00:30:55,350 --> 00:30:59,220
fitting all the data sets and because we

705
00:30:59,220 --> 00:31:02,149
have no data set

