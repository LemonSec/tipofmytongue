1
00:00:37,150 --> 00:00:39,769
good evening everyone can you hear me

2
00:00:39,769 --> 00:00:43,160
it's okay can you hear me

3
00:00:43,160 --> 00:00:45,650
I appreciate that indeed that you stay

4
00:00:45,650 --> 00:00:47,269
tuned to the light presentation and

5
00:00:47,269 --> 00:00:48,680
higher do realize that I'm the last

6
00:00:48,680 --> 00:00:50,379
person standing between you and

7
00:00:50,379 --> 00:00:53,089
reception I guess beer so now try to

8
00:00:53,089 --> 00:00:57,080
keep it short I work at people time is

9
00:00:57,080 --> 00:00:58,820
near there there were a focus on big

10
00:00:58,820 --> 00:01:02,119
data distributed systems I'm privileged

11
00:01:02,119 --> 00:01:04,099
to spend enough time on working on open

12
00:01:04,099 --> 00:01:05,810
source project I am a geometer and peace

13
00:01:05,810 --> 00:01:09,340
PMC member for a particular project

14
00:01:09,340 --> 00:01:13,100
today I'm going to talk about two

15
00:01:13,100 --> 00:01:15,200
interesting trends one is has happened

16
00:01:15,200 --> 00:01:17,299
in the last decade the last 10 years

17
00:01:17,299 --> 00:01:18,890
this is the no Seco in big data the

18
00:01:18,890 --> 00:01:21,409
other one is the implementation of sicko

19
00:01:21,409 --> 00:01:23,689
interfaces for various notes no secure

20
00:01:23,689 --> 00:01:25,910
data systems which seem to be happening

21
00:01:25,910 --> 00:01:28,700
it's a big team in the last year in year

22
00:01:28,700 --> 00:01:31,580
and a half and this is a quote I found

23
00:01:31,580 --> 00:01:34,930
from Martin for work from 2012 where he

24
00:01:34,930 --> 00:01:38,390
predicts that if it's ever if ever

25
00:01:38,390 --> 00:01:40,040
sequel gets implemented on top of

26
00:01:40,040 --> 00:01:41,480
establishing no sequel this at least

27
00:01:41,480 --> 00:01:43,909
would generate plenty of arguments and I

28
00:01:43,909 --> 00:01:46,040
believe this talk is one of those going

29
00:01:46,040 --> 00:01:48,470
to be one of those arguments so a little

30
00:01:48,470 --> 00:01:51,860
bit of context the big data and no

31
00:01:51,860 --> 00:01:56,240
secure and non-secure big bank we've

32
00:01:56,240 --> 00:01:59,420
been so this is a landscape of Big Data

33
00:01:59,420 --> 00:02:01,760
technologies for the last year you see

34
00:02:01,760 --> 00:02:03,500
actually from according to some sources

35
00:02:03,500 --> 00:02:05,870
we have over 350 commercially supported

36
00:02:05,870 --> 00:02:07,640
mochiko and big data systems out there

37
00:02:07,640 --> 00:02:10,539
so what actually explains this explosion

38
00:02:10,539 --> 00:02:12,470
considering that for decades we've been

39
00:02:12,470 --> 00:02:14,420
happy or some people have been happy

40
00:02:14,420 --> 00:02:17,659
with relational databases it's difficult

41
00:02:17,659 --> 00:02:20,420
to judge by it by by the name of those

42
00:02:20,420 --> 00:02:22,340
projects no sequel and actually the

43
00:02:22,340 --> 00:02:25,549
strange no sequel big data they really

44
00:02:25,549 --> 00:02:27,769
don't write any prescriptive definitions

45
00:02:27,769 --> 00:02:29,510
but if you think of them more like a

46
00:02:29,510 --> 00:02:31,849
movement and try to understand what are

47
00:02:31,849 --> 00:02:33,630
the driving forces behind them

48
00:02:33,630 --> 00:02:35,850
I think this helps to position them and

49
00:02:35,850 --> 00:02:38,040
to understand why they're so vibrant and

50
00:02:38,040 --> 00:02:41,640
what makes them makes them think tick on

51
00:02:41,640 --> 00:02:44,190
the first place I think there's some

52
00:02:44,190 --> 00:02:45,570
concession about this one of the main

53
00:02:45,570 --> 00:02:47,400
driving forces behind those technologies

54
00:02:47,400 --> 00:02:48,660
and the boom of those technologies is

55
00:02:48,660 --> 00:02:51,060
the boom of Internet itself the fact

56
00:02:51,060 --> 00:02:53,670
that explosion of what mobile

57
00:02:53,670 --> 00:02:55,200
technologies Internet of Things

58
00:02:55,200 --> 00:02:58,500
practically any device now even smaller

59
00:02:58,500 --> 00:03:00,000
one is the source of information and

60
00:03:00,000 --> 00:03:02,250
generate information and the this in

61
00:03:02,250 --> 00:03:06,650
turn generates a cause three main

62
00:03:06,650 --> 00:03:08,970
challenges this is how to handle the

63
00:03:08,970 --> 00:03:10,770
volume of data the velocity of data and

64
00:03:10,770 --> 00:03:13,770
the variety of all these data sources so

65
00:03:13,770 --> 00:03:17,670
and Internet is the cause for these

66
00:03:17,670 --> 00:03:19,350
technologies or to fulfill the demand

67
00:03:19,350 --> 00:03:20,790
for those challenges the other hand it

68
00:03:20,790 --> 00:03:22,320
is actually the solution as well in

69
00:03:22,320 --> 00:03:23,910
order to handle to scale to provide

70
00:03:23,910 --> 00:03:26,580
technologies that can handle and address

71
00:03:26,580 --> 00:03:28,770
those challenges you are going to use

72
00:03:28,770 --> 00:03:30,780
again distributed systems and in turn

73
00:03:30,780 --> 00:03:32,910
using distributed system for dos for

74
00:03:32,910 --> 00:03:35,070
resolving those challenges means that

75
00:03:35,070 --> 00:03:37,940
some of the already presumed

76
00:03:37,940 --> 00:03:40,230
technologies and a guarantee is that

77
00:03:40,230 --> 00:03:42,570
relational databases provide like asset

78
00:03:42,570 --> 00:03:45,060
or - two-phase commit would be

79
00:03:45,060 --> 00:03:46,770
challenged and has to be addressed in

80
00:03:46,770 --> 00:03:48,840
different way it's known for example

81
00:03:48,840 --> 00:03:51,390
that the two-phase commit would Hank in

82
00:03:51,390 --> 00:03:53,490
- in certain cases in failure

83
00:03:53,490 --> 00:03:56,510
distributed system so all new class of

84
00:03:56,510 --> 00:04:00,950
approaches not for example the

85
00:04:00,950 --> 00:04:04,140
consistency and availability in case of

86
00:04:04,140 --> 00:04:05,640
partitioning of distributed system this

87
00:04:05,640 --> 00:04:09,120
is the cap or theorem is are meant to do

88
00:04:09,120 --> 00:04:11,010
and this is part of this tributed system

89
00:04:11,010 --> 00:04:13,110
and many of the technologies that that

90
00:04:13,110 --> 00:04:17,100
that implements this Paxos on the other

91
00:04:17,100 --> 00:04:20,190
hand and as I mentioned the two-phase

92
00:04:20,190 --> 00:04:22,890
commit is not really reliable approach -

93
00:04:22,890 --> 00:04:25,140
to ensure consistency within the

94
00:04:25,140 --> 00:04:27,660
distributed environment so a class of

95
00:04:27,660 --> 00:04:29,970
consist consensus based quorum based

96
00:04:29,970 --> 00:04:32,520
systems like taxes heavy merchants and

97
00:04:32,520 --> 00:04:34,230
many of the NAS ecosystems actually are

98
00:04:34,230 --> 00:04:35,630
based on those type of technologies

99
00:04:35,630 --> 00:04:39,780
another not that popular but I think

100
00:04:39,780 --> 00:04:41,640
very important driving forces the

101
00:04:41,640 --> 00:04:43,140
so-called

102
00:04:43,140 --> 00:04:44,490
relational or object-relational

103
00:04:44,490 --> 00:04:46,550
impedance mismatch

104
00:04:46,550 --> 00:04:48,470
which a relational database I'm not sure

105
00:04:48,470 --> 00:04:50,210
many of you maybe have done some

106
00:04:50,210 --> 00:04:51,800
application development they know that

107
00:04:51,800 --> 00:04:54,470
for some class of applications you just

108
00:04:54,470 --> 00:04:56,479
need to persist your application state

109
00:04:56,479 --> 00:04:59,330
which mark is it very often is a object

110
00:04:59,330 --> 00:05:01,880
based state into relational database for

111
00:05:01,880 --> 00:05:03,319
this you would need some sort of ORM

112
00:05:03,319 --> 00:05:05,569
technologies which is unnecessary for

113
00:05:05,569 --> 00:05:07,580
many use cases as I mentioned and this

114
00:05:07,580 --> 00:05:10,849
gave birth to technologies like or

115
00:05:10,849 --> 00:05:12,949
document based datastore this actually

116
00:05:12,949 --> 00:05:14,599
is a huge group of technologies out

117
00:05:14,599 --> 00:05:15,130
there

118
00:05:15,130 --> 00:05:18,139
driven by by this mismatch furthermore

119
00:05:18,139 --> 00:05:20,389
different type of stores and demands

120
00:05:20,389 --> 00:05:22,250
like graph based databases or full-text

121
00:05:22,250 --> 00:05:24,759
search where the traditional

122
00:05:24,759 --> 00:05:26,930
representation of database models in

123
00:05:26,930 --> 00:05:29,449
strict very often the raw base the

124
00:05:29,449 --> 00:05:31,610
relational model is not very very

125
00:05:31,610 --> 00:05:35,000
appropriate to to do it and there many

126
00:05:35,000 --> 00:05:36,560
other factors but I think those three

127
00:05:36,560 --> 00:05:39,380
kind of are powerful enough to to

128
00:05:39,380 --> 00:05:41,210
explain why there is such a search of

129
00:05:41,210 --> 00:05:43,250
technologies out there last one I want

130
00:05:43,250 --> 00:05:45,500
to talk is the cloud computing race and

131
00:05:45,500 --> 00:05:48,919
wife itself the possibilities to program

132
00:05:48,919 --> 00:05:50,539
the infrastructure actually talk to make

133
00:05:50,539 --> 00:05:51,800
this infrastructure to have

134
00:05:51,800 --> 00:05:54,530
infrastructure on demand is a main

135
00:05:54,530 --> 00:05:57,590
driver for eliminating the operational

136
00:05:57,590 --> 00:05:59,570
complexity and the cost and there is

137
00:05:59,570 --> 00:06:01,370
another side effect of this and this

138
00:06:01,370 --> 00:06:04,009
market textural there is this sub sub

139
00:06:04,009 --> 00:06:06,229
sub movement I would say which is shift

140
00:06:06,229 --> 00:06:07,969
from integration to application type of

141
00:06:07,969 --> 00:06:09,770
databases this is very popular into the

142
00:06:09,770 --> 00:06:13,250
micro services type of data applications

143
00:06:13,250 --> 00:06:15,169
so the idea is that if you have your

144
00:06:15,169 --> 00:06:16,610
application instead of having a single

145
00:06:16,610 --> 00:06:19,520
data store as the application state for

146
00:06:19,520 --> 00:06:22,280
your distributed application you rather

147
00:06:22,280 --> 00:06:24,530
have a dedicated application store for

148
00:06:24,530 --> 00:06:27,139
each application and have a well defined

149
00:06:27,139 --> 00:06:29,750
protocol application level means between

150
00:06:29,750 --> 00:06:32,210
this application so the database is not

151
00:06:32,210 --> 00:06:36,229
shared among them I think I hope those

152
00:06:36,229 --> 00:06:39,680
those those forces explain the reason

153
00:06:39,680 --> 00:06:40,969
why there is such a multitude of

154
00:06:40,969 --> 00:06:42,949
technologies out there and I don't want

155
00:06:42,949 --> 00:06:46,639
to justify this orto to dive further the

156
00:06:46,639 --> 00:06:48,979
point is that there are out there there

157
00:06:48,979 --> 00:06:52,009
lot and as I said over 450 commercially

158
00:06:52,009 --> 00:06:54,250
supported one and one of the interesting

159
00:06:54,250 --> 00:06:58,400
consequences of this is that almost any

160
00:06:58,400 --> 00:06:59,990
organization

161
00:06:59,990 --> 00:07:01,190
that would have a list here of those

162
00:07:01,190 --> 00:07:05,570
technologies deployed in in their data

163
00:07:05,570 --> 00:07:07,810
store data in the infrastructure so

164
00:07:07,810 --> 00:07:10,370
interesting question race how they're

165
00:07:10,370 --> 00:07:12,020
going to integrate those technologies

166
00:07:12,020 --> 00:07:14,360
this multitude of database that would

167
00:07:14,360 --> 00:07:16,310
usually don't have at least few data

168
00:07:16,310 --> 00:07:18,500
storage technologies dedicated a

169
00:07:18,500 --> 00:07:21,290
particularly good for particular type of

170
00:07:21,290 --> 00:07:26,420
use cases and it was discussed today

171
00:07:26,420 --> 00:07:29,270
I've observed I have observed so far too

172
00:07:29,270 --> 00:07:32,270
many trains that trying to provide this

173
00:07:32,270 --> 00:07:34,640
type of integration and by the way the

174
00:07:34,640 --> 00:07:36,530
integration of technologies is very big

175
00:07:36,530 --> 00:07:39,500
deal to do usually did standard ETL

176
00:07:39,500 --> 00:07:41,960
technologies and systems are trying to

177
00:07:41,960 --> 00:07:43,640
cope with this that's not I'm going to

178
00:07:43,640 --> 00:07:45,620
talk today I'm really trying to talk

179
00:07:45,620 --> 00:07:47,720
about how organization can provide a

180
00:07:47,720 --> 00:07:50,600
single holistic view over the data that

181
00:07:50,600 --> 00:07:52,220
is spread across different data stores

182
00:07:52,220 --> 00:07:54,620
which might be useful for certain use

183
00:07:54,620 --> 00:07:56,450
cases very often those are the

184
00:07:56,450 --> 00:07:58,720
analytical or maybe some data science

185
00:07:58,720 --> 00:08:01,610
type of use cases to train their data

186
00:08:01,610 --> 00:08:05,690
set so two main trends are emerging that

187
00:08:05,690 --> 00:08:09,200
are aiming to - in my opinion - to

188
00:08:09,200 --> 00:08:11,480
converging to provide more unified view

189
00:08:11,480 --> 00:08:12,980
on the data system and data processing

190
00:08:12,980 --> 00:08:16,220
system one is more functional based its

191
00:08:16,220 --> 00:08:18,020
unified programming model and you can

192
00:08:18,020 --> 00:08:19,820
today it was mentioned there was a very

193
00:08:19,820 --> 00:08:21,080
nice presentation comparing the

194
00:08:21,080 --> 00:08:24,200
interfaces of spark and fling you have

195
00:08:24,200 --> 00:08:25,370
noticed that they're very close and

196
00:08:25,370 --> 00:08:27,650
actually they're not the only two that

197
00:08:27,650 --> 00:08:28,400
are very similar

198
00:08:28,400 --> 00:08:31,190
apex Apache currents cascading Apache

199
00:08:31,190 --> 00:08:33,380
beam Deo actually inspired by a common

200
00:08:33,380 --> 00:08:36,020
one paper bought from 2010 I think is

201
00:08:36,020 --> 00:08:40,220
from Java Google paper as a type of API

202
00:08:40,220 --> 00:08:43,700
and there is a trend now at least that

203
00:08:43,700 --> 00:08:45,980
spark link and epics are implementing

204
00:08:45,980 --> 00:08:48,650
and converging at certain level under

205
00:08:48,650 --> 00:08:52,010
Apache beam in the project so this is

206
00:08:52,010 --> 00:08:53,960
and this is example snippet of how

207
00:08:53,960 --> 00:08:56,780
Apache beam looks as a notation the

208
00:08:56,780 --> 00:08:58,520
second trend and that's what I'm going

209
00:08:58,520 --> 00:09:01,400
to focus now today is apparently a lot

210
00:09:01,400 --> 00:09:02,990
of the Gnostic vendors and big data

211
00:09:02,990 --> 00:09:05,990
vendors are starting to implement sequel

212
00:09:05,990 --> 00:09:08,540
interfaces for their data stores or some

213
00:09:08,540 --> 00:09:10,400
sort sequel like interfaces for for

214
00:09:10,400 --> 00:09:12,830
diabetics data stores and

215
00:09:12,830 --> 00:09:14,480
some statistics from the last couple of

216
00:09:14,480 --> 00:09:17,510
years or Hadoop so shows that apparently

217
00:09:17,510 --> 00:09:19,730
majority of the tasks that are run on

218
00:09:19,730 --> 00:09:22,010
Hadoop nowadays are either high base or

219
00:09:22,010 --> 00:09:24,950
sequel some sort of seek on Hadoop type

220
00:09:24,950 --> 00:09:27,589
of solutions out there also for spark

221
00:09:27,589 --> 00:09:29,260
and there is a report from last year

222
00:09:29,260 --> 00:09:31,730
states that the most used production

223
00:09:31,730 --> 00:09:33,620
component is the spark sequel within

224
00:09:33,620 --> 00:09:36,320
their system so there is this and I

225
00:09:36,320 --> 00:09:38,990
stated Google F one paper actually has a

226
00:09:38,990 --> 00:09:41,120
quote there that any data system has to

227
00:09:41,120 --> 00:09:42,950
provide sequel interface and I found

228
00:09:42,950 --> 00:09:44,209
this particularly important because a

229
00:09:44,209 --> 00:09:46,519
lot of the Big Data technologies are now

230
00:09:46,519 --> 00:09:48,950
in the open space are influencing from

231
00:09:48,950 --> 00:09:50,810
the Google papers so there is this shift

232
00:09:50,810 --> 00:09:52,700
and ideas and it's interesting movement

233
00:09:52,700 --> 00:09:55,190
to and trend to observe apparently a lot

234
00:09:55,190 --> 00:09:58,940
of company I think it it's worthy to to

235
00:09:58,940 --> 00:10:01,040
try to reason what are the reason what

236
00:10:01,040 --> 00:10:02,660
what is the main driver for this

237
00:10:02,660 --> 00:10:05,990
movement for this converging it seems

238
00:10:05,990 --> 00:10:07,940
like the desire there is a lot of tools

239
00:10:07,940 --> 00:10:09,890
that know how toxic well out there with

240
00:10:09,890 --> 00:10:11,300
within the organization so it seems like

241
00:10:11,300 --> 00:10:13,459
Sukhoi pretty easy way to integrate with

242
00:10:13,459 --> 00:10:15,170
with with those existing tools of this

243
00:10:15,170 --> 00:10:17,630
more like legacy legacy reason secondary

244
00:10:17,630 --> 00:10:21,079
and I think this is a more important one

245
00:10:21,079 --> 00:10:25,450
is the relational model that stays under

246
00:10:25,450 --> 00:10:28,490
usually sequel engines and this is the

247
00:10:28,490 --> 00:10:31,250
the hot pits that we can talk about we

248
00:10:31,250 --> 00:10:33,020
should talk about today and I think this

249
00:10:33,020 --> 00:10:34,430
is important slide although it doesn't

250
00:10:34,430 --> 00:10:38,899
seems very very bright I can argue that

251
00:10:38,899 --> 00:10:40,790
practically any useful data system out

252
00:10:40,790 --> 00:10:42,740
there in one another form provides

253
00:10:42,740 --> 00:10:45,500
implements the set or back semantics so

254
00:10:45,500 --> 00:10:48,230
operators like projection filtering if

255
00:10:48,230 --> 00:10:49,670
the system is more advanced that some

256
00:10:49,670 --> 00:10:52,490
sort of join or group by would be

257
00:10:52,490 --> 00:10:54,410
present so in order to to provide some

258
00:10:54,410 --> 00:10:56,209
usefulness for the users the system have

259
00:10:56,209 --> 00:10:58,070
to implement explicitly or implicitly

260
00:10:58,070 --> 00:11:00,380
dissipate those operators so having this

261
00:11:00,380 --> 00:11:02,870
and acknowledging that this exists and

262
00:11:02,870 --> 00:11:04,579
very often in order to implement a

263
00:11:04,579 --> 00:11:07,570
pipeline or or query execution

264
00:11:07,570 --> 00:11:09,649
statement you have to chain multiple

265
00:11:09,649 --> 00:11:11,990
operators like this and when you start

266
00:11:11,990 --> 00:11:13,519
to play with this concept you realize

267
00:11:13,519 --> 00:11:14,810
it's practically this is the same

268
00:11:14,810 --> 00:11:17,540
relational algebra concept that are very

269
00:11:17,540 --> 00:11:19,940
common in the relational space and there

270
00:11:19,940 --> 00:11:21,440
is a tools that are very good in

271
00:11:21,440 --> 00:11:23,570
optimizing subtype of change and those

272
00:11:23,570 --> 00:11:25,120
are the planners and

273
00:11:25,120 --> 00:11:26,559
indeed the relational expression

274
00:11:26,559 --> 00:11:28,990
optimizers are very desirable feature

275
00:11:28,990 --> 00:11:30,519
for many of the technologies Big Data

276
00:11:30,519 --> 00:11:31,779
technologies simply they're very

277
00:11:31,779 --> 00:11:40,059
difficult to implement now and that's at

278
00:11:40,059 --> 00:11:41,410
least that word very difficult to

279
00:11:41,410 --> 00:11:43,990
implement until recently now there's at

280
00:11:43,990 --> 00:11:45,699
least couple of open-source technologies

281
00:11:45,699 --> 00:11:47,920
out there that provide some some some

282
00:11:47,920 --> 00:11:51,309
some help and are useful to to consider

283
00:11:51,309 --> 00:11:54,579
and try to use and leverage in order to

284
00:11:54,579 --> 00:11:56,920
provide this type of relational

285
00:11:56,920 --> 00:11:59,019
expressions relational cumulation within

286
00:11:59,019 --> 00:12:04,689
the existing big data systems I'm

287
00:12:04,689 --> 00:12:07,019
proposing here I mean this is again just

288
00:12:07,019 --> 00:12:09,550
simple subset of it's possible way how

289
00:12:09,550 --> 00:12:11,050
you can integrate how organization are

290
00:12:11,050 --> 00:12:13,240
I'm dealing with a lot of customers out

291
00:12:13,240 --> 00:12:14,769
there so I have some first-hand

292
00:12:14,769 --> 00:12:16,930
experience with some big players in the

293
00:12:16,930 --> 00:12:19,420
field and see and experience how they

294
00:12:19,420 --> 00:12:23,110
preserve and Pacey them the usefulness

295
00:12:23,110 --> 00:12:24,249
of the data how they're trying to

296
00:12:24,249 --> 00:12:26,439
integrate that their data and it seems

297
00:12:26,439 --> 00:12:28,839
like the most common approaches this one

298
00:12:28,839 --> 00:12:31,480
so this any standard for the writer

299
00:12:31,480 --> 00:12:33,790
database system approach so you pick one

300
00:12:33,790 --> 00:12:35,649
database that allows you to implement

301
00:12:35,649 --> 00:12:38,019
the connectors to external databases or

302
00:12:38,019 --> 00:12:41,050
the external data systems no sequel and

303
00:12:41,050 --> 00:12:42,879
then you can provide kind of single

304
00:12:42,879 --> 00:12:45,189
review on your system just via external

305
00:12:45,189 --> 00:12:50,139
tables which representation sorry which

306
00:12:50,139 --> 00:12:52,480
representation of the external logical

307
00:12:52,480 --> 00:12:55,269
data systems in this case I have

308
00:12:55,269 --> 00:12:57,429
experienced with Patrick Hope which is

309
00:12:57,429 --> 00:12:59,559
yet another to derive from the green

310
00:12:59,559 --> 00:13:01,899
plan that we discussed today again which

311
00:13:01,899 --> 00:13:04,029
was derived from Postgres it's MPP

312
00:13:04,029 --> 00:13:06,399
shared nothing solution which has very

313
00:13:06,399 --> 00:13:07,620
powered actually share very similar

314
00:13:07,620 --> 00:13:10,410
optimizer or its orc optimizer inside

315
00:13:10,410 --> 00:13:13,600
right what's more important provides a

316
00:13:13,600 --> 00:13:16,240
pxf framework this is a Java extension

317
00:13:16,240 --> 00:13:18,279
framework which allows you to actually

318
00:13:18,279 --> 00:13:20,379
implement a plug-ins for external system

319
00:13:20,379 --> 00:13:23,230
so this is I call it an 1/n model

320
00:13:23,230 --> 00:13:24,939
because the organization would use a

321
00:13:24,939 --> 00:13:27,670
single sequel usually post grad G DBC

322
00:13:27,670 --> 00:13:29,470
connector or DB seat to talk with one

323
00:13:29,470 --> 00:13:31,929
MPP database and where the extension

324
00:13:31,929 --> 00:13:33,519
mechanism would be able to see some

325
00:13:33,519 --> 00:13:36,639
portions of them and the Gnostic code

326
00:13:36,639 --> 00:13:39,110
system itself outside

327
00:13:39,110 --> 00:13:41,779
second approach is far more interesting

328
00:13:41,779 --> 00:13:43,339
in my opinion because it provide more

329
00:13:43,339 --> 00:13:46,190
autonomous autonomous e4 for them nós

330
00:13:46,190 --> 00:13:48,320
ecosystem themselves is to implement a

331
00:13:48,320 --> 00:13:49,700
Sequoia doctor for each system in

332
00:13:49,700 --> 00:13:51,740
isolation and for this purpose there is

333
00:13:51,740 --> 00:13:53,660
a frame or very powerful framework out

334
00:13:53,660 --> 00:13:56,570
there called Apache calcite and as you

335
00:13:56,570 --> 00:13:58,940
can see in this case each of the NOS

336
00:13:58,940 --> 00:14:00,860
ecosystem would actually have its own

337
00:14:00,860 --> 00:14:03,200
sequel representation interface its own

338
00:14:03,200 --> 00:14:05,089
optimization the advantage of this is

339
00:14:05,089 --> 00:14:06,230
that you might be able to tuned

340
00:14:06,230 --> 00:14:09,500
optimizers 4cq optimizers and the

341
00:14:09,500 --> 00:14:12,380
relational algebra expression optimizers

342
00:14:12,380 --> 00:14:15,170
better to the particulars Pacific's of

343
00:14:15,170 --> 00:14:19,459
the nas ecosystem and there is an

344
00:14:19,459 --> 00:14:21,470
interesting gyre ticket the Jireh issued

345
00:14:21,470 --> 00:14:23,209
that recently popped up which is

346
00:14:23,209 --> 00:14:25,339
exploring the possibilities to bridge

347
00:14:25,339 --> 00:14:30,350
those two approaches just one slide

348
00:14:30,350 --> 00:14:32,000
about the first the federated database

349
00:14:32,000 --> 00:14:34,070
approach how it looks like so on the

350
00:14:34,070 --> 00:14:35,810
sequel standpoint you're creating a

351
00:14:35,810 --> 00:14:37,579
table that looks like this standard type

352
00:14:37,579 --> 00:14:39,829
the interesting bit is that here you're

353
00:14:39,829 --> 00:14:41,660
providing location to your nas record

354
00:14:41,660 --> 00:14:43,730
data system where you want to wrap and

355
00:14:43,730 --> 00:14:45,620
you have to implement three classes

356
00:14:45,620 --> 00:14:47,269
which is the fragmented accessor and

357
00:14:47,269 --> 00:14:50,260
there is over the the purpose of the

358
00:14:50,260 --> 00:14:53,449
experimenter is that if the dating the

359
00:14:53,449 --> 00:14:55,010
Gnostic or data store allows you to

360
00:14:55,010 --> 00:14:57,199
partition the data in streams that you

361
00:14:57,199 --> 00:14:59,120
can process in parallel the row of the

362
00:14:59,120 --> 00:15:00,829
fermentor is indeed to establish this

363
00:15:00,829 --> 00:15:03,380
partition for each of the streams or

364
00:15:03,380 --> 00:15:05,390
separate streams in parallel the

365
00:15:05,390 --> 00:15:06,949
accessor actually breaks them into a

366
00:15:06,949 --> 00:15:08,930
collection of rows key value rows and

367
00:15:08,930 --> 00:15:11,209
for each of the this value rows there is

368
00:15:11,209 --> 00:15:12,829
over the last component you have to

369
00:15:12,829 --> 00:15:16,250
implement would convert it into a column

370
00:15:16,250 --> 00:15:19,010
a column list which would match this

371
00:15:19,010 --> 00:15:21,380
interfaces there is much more internals

372
00:15:21,380 --> 00:15:23,750
you can pass analytics and stuff to to

373
00:15:23,750 --> 00:15:25,940
configure the statistics in order to

374
00:15:25,940 --> 00:15:28,130
optimize the help the optimizer to

375
00:15:28,130 --> 00:15:30,050
adjust according this particular data

376
00:15:30,050 --> 00:15:32,600
store and this is very powerful approach

377
00:15:32,600 --> 00:15:35,120
if you for example already have Hadoop

378
00:15:35,120 --> 00:15:35,930
and Hokie

379
00:15:35,930 --> 00:15:38,570
like system in your infrastructure and

380
00:15:38,570 --> 00:15:40,459
you can just implement this such type of

381
00:15:40,459 --> 00:15:42,350
plugins and wrap and provide holistic

382
00:15:42,350 --> 00:15:46,449
view on your back-end system

383
00:15:47,500 --> 00:15:51,140
the second approach for direct one is to

384
00:15:51,140 --> 00:15:53,000
implement a sequel interface or

385
00:15:53,000 --> 00:15:55,640
you're an leverage tikka optimizer

386
00:15:55,640 --> 00:15:58,310
around your nasi code database and for

387
00:15:58,310 --> 00:16:00,370
this the Apache frame magic outside

388
00:16:00,370 --> 00:16:04,010
framework provides you query parser this

389
00:16:04,010 --> 00:16:05,360
is sequel query parts the validator

390
00:16:05,360 --> 00:16:07,580
optimizer I think this is the most

391
00:16:07,580 --> 00:16:10,040
important bit here as the bones you get

392
00:16:10,040 --> 00:16:11,750
the G DBC driver which you can talk with

393
00:16:11,750 --> 00:16:13,880
the system and one very important to

394
00:16:13,880 --> 00:16:15,680
design decision about the calcite is to

395
00:16:15,680 --> 00:16:17,360
stay out of the business of how data is

396
00:16:17,360 --> 00:16:19,580
stored and processed which in turn makes

397
00:16:19,580 --> 00:16:22,490
is very useful to implement and grab

398
00:16:22,490 --> 00:16:24,650
almost any existing data store out there

399
00:16:24,650 --> 00:16:26,960
I think this is by design and it's very

400
00:16:26,960 --> 00:16:29,120
powerful decision and if you take a look

401
00:16:29,120 --> 00:16:31,040
about the various technologies out there

402
00:16:31,040 --> 00:16:33,050
that use in one another way Apache

403
00:16:33,050 --> 00:16:34,460
calcite you would see that most of the

404
00:16:34,460 --> 00:16:38,630
big play already using it inside what I

405
00:16:38,630 --> 00:16:40,430
did I'm working on a patchy geode

406
00:16:40,430 --> 00:16:42,950
adapter the Apache geo TC memory data

407
00:16:42,950 --> 00:16:45,170
grid yet another key value store

408
00:16:45,170 --> 00:16:47,270
distributed hash and I'm going to use in

409
00:16:47,270 --> 00:16:48,830
some of the example just as a reference

410
00:16:48,830 --> 00:16:51,260
to illustrate how it looks like so this

411
00:16:51,260 --> 00:16:53,300
assuming that you decide to implement a

412
00:16:53,300 --> 00:16:55,490
sequel adapter using a particle size

413
00:16:55,490 --> 00:16:58,640
drone back-end data store no sequel data

414
00:16:58,640 --> 00:17:00,560
store their cup of decision you have to

415
00:17:00,560 --> 00:17:03,350
make and they're very important

416
00:17:03,350 --> 00:17:05,869
regarding the from one side to how much

417
00:17:05,869 --> 00:17:07,760
sequel completed you are going to expose

418
00:17:07,760 --> 00:17:09,859
and compliant with the sequel standard

419
00:17:09,859 --> 00:17:13,040
and other hand how much you're going to

420
00:17:13,040 --> 00:17:15,079
to leverage the power of the nice

421
00:17:15,079 --> 00:17:17,209
agnostic or not no sequel system you

422
00:17:17,209 --> 00:17:20,030
have so the first thing is you have to

423
00:17:20,030 --> 00:17:22,069
decide how we're going to convert your

424
00:17:22,069 --> 00:17:24,170
data type from the existing nas

425
00:17:24,170 --> 00:17:25,730
ecosystem let's say this is key value

426
00:17:25,730 --> 00:17:27,740
store or it could be even like some sort

427
00:17:27,740 --> 00:17:29,630
of graph representation into a tabular

428
00:17:29,630 --> 00:17:31,910
format that is expect bike outside and

429
00:17:31,910 --> 00:17:33,770
cow side has the standard metadata

430
00:17:33,770 --> 00:17:35,660
expected is the catalog schema which is

431
00:17:35,660 --> 00:17:37,460
collection of tables table which is

432
00:17:37,460 --> 00:17:39,980
collection of rows and row is just a

433
00:17:39,980 --> 00:17:42,260
list of columns represent to the

434
00:17:42,260 --> 00:17:45,380
relational data type so this is

435
00:17:45,380 --> 00:17:47,270
important decision because let's say

436
00:17:47,270 --> 00:17:49,250
that you want to express as some sort of

437
00:17:49,250 --> 00:17:51,110
a JSON or Java object which has

438
00:17:51,110 --> 00:17:53,510
hierarchy and you have to flatter in

439
00:17:53,510 --> 00:17:55,610
some tabular format you have to decide

440
00:17:55,610 --> 00:17:56,930
whether you we are going to spend

441
00:17:56,930 --> 00:17:59,000
computation and a lot of civilization

442
00:17:59,000 --> 00:18:00,890
and to achieve this or you just can't

443
00:18:00,890 --> 00:18:02,900
afford to implement only top-level

444
00:18:02,900 --> 00:18:04,610
fields or some smartness in stuff so

445
00:18:04,610 --> 00:18:06,680
it's up to you to decide what is the

446
00:18:06,680 --> 00:18:08,570
it's a trade-off so how much you're

447
00:18:08,570 --> 00:18:11,240
going to expose from your model as a as

448
00:18:11,240 --> 00:18:13,100
an opposite of the performance that

449
00:18:13,100 --> 00:18:15,920
you're going to gain or lose and the

450
00:18:15,920 --> 00:18:17,270
second more important thing that this is

451
00:18:17,270 --> 00:18:18,830
general principle for any data

452
00:18:18,830 --> 00:18:21,230
distributed data system is move the

453
00:18:21,230 --> 00:18:22,670
computation next to the data not other

454
00:18:22,670 --> 00:18:26,570
way around particular that means that if

455
00:18:26,570 --> 00:18:29,780
you in case of in the context of the

456
00:18:29,780 --> 00:18:32,150
sequel query you would like to run the

457
00:18:32,150 --> 00:18:34,730
executions of this query next to the

458
00:18:34,730 --> 00:18:37,400
node to where data is stored rather than

459
00:18:37,400 --> 00:18:38,810
actually moving the data to some central

460
00:18:38,810 --> 00:18:40,490
node where this processing is happening

461
00:18:40,490 --> 00:18:44,000
and then moving it back and forth in the

462
00:18:44,000 --> 00:18:45,740
context of Apache prowl site you have

463
00:18:45,740 --> 00:18:47,930
two approaches to to achieve this the

464
00:18:47,930 --> 00:18:50,420
first one is simple I call it simple it

465
00:18:50,420 --> 00:18:52,550
just allows you to to implement a very

466
00:18:52,550 --> 00:18:54,470
simple simple inter type of interface

467
00:18:54,470 --> 00:18:58,220
with ability to push down the predicates

468
00:18:58,220 --> 00:19:00,010
predicates

469
00:19:00,010 --> 00:19:01,880
operators relation imperative like

470
00:19:01,880 --> 00:19:04,280
filters and projections that means that

471
00:19:04,280 --> 00:19:06,800
if you have select some fields where

472
00:19:06,800 --> 00:19:09,500
from some table where something it makes

473
00:19:09,500 --> 00:19:11,990
any sense this fuse and there were

474
00:19:11,990 --> 00:19:13,700
clouds at the conditions to be pushed

475
00:19:13,700 --> 00:19:16,010
down to your not equal solution and you

476
00:19:16,010 --> 00:19:18,290
pre-filter and reprocess and return back

477
00:19:18,290 --> 00:19:20,240
only the amount of data that is

478
00:19:20,240 --> 00:19:24,280
necessary for okay for for then for the

479
00:19:24,280 --> 00:19:27,890
system to process I'm going to hurry up

480
00:19:27,890 --> 00:19:31,490
here's this is an example how it looks

481
00:19:31,490 --> 00:19:33,620
the simple scenario you're connecting to

482
00:19:33,620 --> 00:19:36,230
an orifice visa what is JDBC adopt this

483
00:19:36,230 --> 00:19:39,560
is the logical side JDBC protocol so

484
00:19:39,560 --> 00:19:42,860
when you connect to JDBC by the particle

485
00:19:42,860 --> 00:19:44,510
size gdb seed driver to your back-end

486
00:19:44,510 --> 00:19:46,940
system you have to provide a model in

487
00:19:46,940 --> 00:19:49,100
JSON format the only thing that model

488
00:19:49,100 --> 00:19:50,270
contains is your entry point

489
00:19:50,270 --> 00:19:53,020
implementation of the schema factory

490
00:19:53,020 --> 00:19:57,050
with some appearance that are relevant

491
00:19:57,050 --> 00:19:59,840
for your back-end systems the role of

492
00:19:59,840 --> 00:20:02,240
this schema is usually one liner of

493
00:20:02,240 --> 00:20:03,950
implementation is to create a schema

494
00:20:03,950 --> 00:20:06,380
based on those operators that appearance

495
00:20:06,380 --> 00:20:08,030
that you have provided the schema in

496
00:20:08,030 --> 00:20:10,100
turn depends of the mappings that you

497
00:20:10,100 --> 00:20:12,230
have decided to implement for your

498
00:20:12,230 --> 00:20:13,910
back-end system Nestico system and the

499
00:20:13,910 --> 00:20:15,410
relational staff would create a list of

500
00:20:15,410 --> 00:20:18,560
tables and the important things is that

501
00:20:18,560 --> 00:20:20,460
you have to implement a column types

502
00:20:20,460 --> 00:20:24,049
in these tables then when a query comes

503
00:20:24,049 --> 00:20:27,450
it would be passed to the scan parameter

504
00:20:27,450 --> 00:20:30,119
so that from this book order usually

505
00:20:30,119 --> 00:20:32,100
what means that it was going to collect

506
00:20:32,100 --> 00:20:35,279
try to extract this data set from your

507
00:20:35,279 --> 00:20:38,369
logical database and convert it in the

508
00:20:38,369 --> 00:20:40,200
convert method convert it into type that

509
00:20:40,200 --> 00:20:43,190
is compliant with the table definitions

510
00:20:43,190 --> 00:20:45,690
that you hear is that in this simple

511
00:20:45,690 --> 00:20:47,309
implementation you could get all date so

512
00:20:47,309 --> 00:20:48,809
there is not any moving of the

513
00:20:48,809 --> 00:20:50,399
computation code into the data

514
00:20:50,399 --> 00:20:54,330
everything goes to the central processor

515
00:20:54,330 --> 00:20:56,369
and get process they're led to version

516
00:20:56,369 --> 00:20:58,529
that you can optimize this computation

517
00:20:58,529 --> 00:21:00,779
it's called that instead of scannable

518
00:21:00,779 --> 00:21:02,669
table you can experiment filter

519
00:21:02,669 --> 00:21:04,139
scannable table projected future

520
00:21:04,139 --> 00:21:06,960
scannable table which allows you to push

521
00:21:06,960 --> 00:21:08,759
down the filters and projectors but

522
00:21:08,759 --> 00:21:10,649
that's everything that you can do as an

523
00:21:10,649 --> 00:21:12,119
optimization if you have a joint

524
00:21:12,119 --> 00:21:13,649
operators or group by operators

525
00:21:13,649 --> 00:21:15,600
everything would happen central place on

526
00:21:15,600 --> 00:21:18,570
the client side and the second approach

527
00:21:18,570 --> 00:21:20,909
that the calcite allow provides you with

528
00:21:20,909 --> 00:21:23,580
is to implement your own relational

529
00:21:23,580 --> 00:21:26,820
rules and in relation operators that

530
00:21:26,820 --> 00:21:28,259
would allow you to improvise

531
00:21:28,259 --> 00:21:30,749
implementation much closer to the to the

532
00:21:30,749 --> 00:21:33,269
native noshiko system in this case this

533
00:21:33,269 --> 00:21:36,830
is a apache geode and this is very fast

534
00:21:36,830 --> 00:21:40,470
going so how it looks like gdb see this

535
00:21:40,470 --> 00:21:42,090
is everything Hebrew is the standard

536
00:21:42,090 --> 00:21:44,460
Apache calcite framework this is the

537
00:21:44,460 --> 00:21:45,720
beads that you have to implement in

538
00:21:45,720 --> 00:21:48,179
order to to provide the adapter and this

539
00:21:48,179 --> 00:21:50,009
is code generated by the outside so

540
00:21:50,009 --> 00:21:52,700
sequel query comes it is passed to the

541
00:21:52,700 --> 00:21:55,110
parcel sequence converted into a

542
00:21:55,110 --> 00:21:58,200
relational expression and 3 it goes

543
00:21:58,200 --> 00:22:00,600
through the planner which performs some

544
00:22:00,600 --> 00:22:02,340
optimizations actually while performing

545
00:22:02,340 --> 00:22:05,249
these optimizations optimize tree is

546
00:22:05,249 --> 00:22:07,860
passed to the innumerable component

547
00:22:07,860 --> 00:22:09,600
which is responsible to convert this

548
00:22:09,600 --> 00:22:12,119
logical plan into physical plan and this

549
00:22:12,119 --> 00:22:13,830
is already the tricky part in this

550
00:22:13,830 --> 00:22:15,600
process actually if you have implemented

551
00:22:15,600 --> 00:22:17,759
your own rules and operators the

552
00:22:17,759 --> 00:22:19,200
enumerable components would consult

553
00:22:19,200 --> 00:22:22,009
those and would provide a wealth I word

554
00:22:22,009 --> 00:22:23,879
implementation actually chooses the

555
00:22:23,879 --> 00:22:25,619
expression tree this is concept from

556
00:22:25,619 --> 00:22:27,570
League for link 4j I think this is

557
00:22:27,570 --> 00:22:29,119
something that come from from from

558
00:22:29,119 --> 00:22:31,799
Microsoft and this interpreter actually

559
00:22:31,799 --> 00:22:33,359
generate Java code that's optimized for

560
00:22:33,359 --> 00:22:34,320
this implement a

561
00:22:34,320 --> 00:22:36,960
compiled and the JDBC query is

562
00:22:36,960 --> 00:22:40,320
executed so that's the whole 400

563
00:22:40,320 --> 00:22:43,200
complete line and I'm going to skip this

564
00:22:43,200 --> 00:22:45,240
those are the internals of the if you're

565
00:22:45,240 --> 00:22:47,060
going to implement your own rules and

566
00:22:47,060 --> 00:22:49,590
operators in the outside you would have

567
00:22:49,590 --> 00:22:52,200
to get acquainted with this and this is

568
00:22:52,200 --> 00:22:53,820
the generic pattern if you see any of

569
00:22:53,820 --> 00:22:55,500
the implementations of the calcite

570
00:22:55,500 --> 00:22:57,510
adapter out there more or less

571
00:22:57,510 --> 00:22:59,640
implemented for these these steps here

572
00:22:59,640 --> 00:23:02,760
and just I would finish with the example

573
00:23:02,760 --> 00:23:04,410
let's say that you have this relation of

574
00:23:04,410 --> 00:23:07,040
expressions this is the relation of

575
00:23:07,040 --> 00:23:09,300
operators that you would see now we have

576
00:23:09,300 --> 00:23:12,090
a join of two tables filter and project

577
00:23:12,090 --> 00:23:15,900
two of the fields so the optimizer throw

578
00:23:15,900 --> 00:23:17,970
would be to convert and to push some of

579
00:23:17,970 --> 00:23:19,680
the operators closer to the date so you

580
00:23:19,680 --> 00:23:22,320
see the project's so to reduce the

581
00:23:22,320 --> 00:23:24,660
amount of data that moves a port so this

582
00:23:24,660 --> 00:23:26,550
is usually the logical part of the

583
00:23:26,550 --> 00:23:29,340
parcel of the optimizer and this is the

584
00:23:29,340 --> 00:23:32,750
real example with the Apache geode

585
00:23:32,750 --> 00:23:35,730
adapter that I work on if you have this

586
00:23:35,730 --> 00:23:37,590
query this is the logical plan you see

587
00:23:37,590 --> 00:23:40,110
that you have no any optimization the

588
00:23:40,110 --> 00:23:41,550
scan on the tables

589
00:23:41,550 --> 00:23:45,060
then join perform then filter on the

590
00:23:45,060 --> 00:23:48,060
this see one of the field and projection

591
00:23:48,060 --> 00:23:51,630
extract two of the fields if you use the

592
00:23:51,630 --> 00:23:54,360
simple scannable approach which doesn't

593
00:23:54,360 --> 00:23:56,040
implement any rules you would have some

594
00:23:56,040 --> 00:23:57,810
advantage so that the planner would

595
00:23:57,810 --> 00:24:00,150
already reorder the sword or this this

596
00:24:00,150 --> 00:24:01,650
operator in a way that would mean they'd

597
00:24:01,650 --> 00:24:04,110
be executed much more efficiently but

598
00:24:04,110 --> 00:24:05,640
still most of the computation would

599
00:24:05,640 --> 00:24:07,260
happen on the client side where the did

600
00:24:07,260 --> 00:24:09,900
the visual dock the sequel part is

601
00:24:09,900 --> 00:24:12,540
performed if you move one step and you

602
00:24:12,540 --> 00:24:14,250
implement your own rules and have

603
00:24:14,250 --> 00:24:15,840
something that's much more tailored to

604
00:24:15,840 --> 00:24:18,510
your nest nautical data system in this

605
00:24:18,510 --> 00:24:22,230
case I have implemented geo project

606
00:24:22,230 --> 00:24:25,230
filter but there's also group by you see

607
00:24:25,230 --> 00:24:26,940
that actually it uses those operators

608
00:24:26,940 --> 00:24:28,860
and it leverages practically a lot of

609
00:24:28,860 --> 00:24:30,780
dos operators now executed on the nos

610
00:24:30,780 --> 00:24:33,840
ecosystem itself still you see that

611
00:24:33,840 --> 00:24:35,460
joint is not implemented there I'm

612
00:24:35,460 --> 00:24:37,950
working on this that means that now this

613
00:24:37,950 --> 00:24:40,470
query practice could be converted into

614
00:24:40,470 --> 00:24:42,870
two sub queries run on the nas ecosystem

615
00:24:42,870 --> 00:24:44,610
and the result would be returned and be

616
00:24:44,610 --> 00:24:46,200
performed on the client side but this is

617
00:24:46,200 --> 00:24:47,580
something that you can

618
00:24:47,580 --> 00:24:51,390
progress on typical gdb see how we are

619
00:24:51,390 --> 00:24:54,120
going to use this boom stand Java

620
00:24:54,120 --> 00:24:58,560
standpoint and I'm frightened over time

621
00:24:58,560 --> 00:25:01,260
so I'm sorry for this try to to run for

622
00:25:01,260 --> 00:25:01,480
it

623
00:25:01,480 --> 00:25:07,690
[Applause]

