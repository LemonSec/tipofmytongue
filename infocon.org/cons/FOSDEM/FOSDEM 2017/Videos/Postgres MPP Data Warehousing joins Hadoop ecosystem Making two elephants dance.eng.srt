1
00:00:00,000 --> 00:00:02,029
Oh

2
00:00:04,580 --> 00:00:07,400
hello everyone my name is Roman sharpish

3
00:00:07,400 --> 00:00:09,800
Nick and I work at pivotal I also happen

4
00:00:09,800 --> 00:00:12,440
to be a co organizer of this dev room

5
00:00:12,440 --> 00:00:14,990
which by the way was my sneaky plan to

6
00:00:14,990 --> 00:00:17,210
get the stock accepted because I really

7
00:00:17,210 --> 00:00:19,160
think that today I will be talking about

8
00:00:19,160 --> 00:00:21,970
something that's quite interesting

9
00:00:21,970 --> 00:00:24,410
before I do that let me introduce myself

10
00:00:24,410 --> 00:00:26,960
a little bit more my involvement with

11
00:00:26,960 --> 00:00:29,480
Big Data goes all the way back to the

12
00:00:29,480 --> 00:00:32,270
original Hadoop team at Yahoo which I

13
00:00:32,270 --> 00:00:35,899
joined in 2010 and since then I managed

14
00:00:35,899 --> 00:00:38,269
to work at Cloudera pivotal basically

15
00:00:38,269 --> 00:00:41,690
companies that do data do Hadoop and

16
00:00:41,690 --> 00:00:43,070
with Hadoop it was interesting because

17
00:00:43,070 --> 00:00:45,620
when it first appeared everybody felt

18
00:00:45,620 --> 00:00:47,559
like it would totally replace

19
00:00:47,559 --> 00:00:50,179
traditional data warehouse systems and

20
00:00:50,179 --> 00:00:52,609
for a good reason because Hadoop had a

21
00:00:52,609 --> 00:00:53,809
lot going for it

22
00:00:53,809 --> 00:00:57,350
it was open source it was scalable it

23
00:00:57,350 --> 00:01:00,289
was developer friendly it was basically

24
00:01:00,289 --> 00:01:02,600
a really good system and for some time

25
00:01:02,600 --> 00:01:06,200
it really seemed like it was going to

26
00:01:06,200 --> 00:01:09,380
happen but recently an interesting trend

27
00:01:09,380 --> 00:01:11,960
started to appear more and more

28
00:01:11,960 --> 00:01:14,210
enterprises started using Hadoop in

29
00:01:14,210 --> 00:01:16,640
addition to data warehouse systems not

30
00:01:16,640 --> 00:01:18,890
replacing them but basically augmenting

31
00:01:18,890 --> 00:01:22,760
them with Hadoop and at first this could

32
00:01:22,760 --> 00:01:25,280
look like a very strange architecture

33
00:01:25,280 --> 00:01:27,350
because why would you have two different

34
00:01:27,350 --> 00:01:29,810
systems you know kind of doing similar

35
00:01:29,810 --> 00:01:32,450
things why not replace it with Jessica

36
00:01:32,450 --> 00:01:34,430
dupe and have the promised enterprise

37
00:01:34,430 --> 00:01:37,160
data Lake so today we will be talking

38
00:01:37,160 --> 00:01:39,230
about why those architectures actually

39
00:01:39,230 --> 00:01:42,080
make sense and what drives those types

40
00:01:42,080 --> 00:01:45,500
of architectures all right all right ok

41
00:01:45,500 --> 00:01:49,850
awesome so yes so basically the first

42
00:01:49,850 --> 00:01:51,830
time I was actually starting to see this

43
00:01:51,830 --> 00:01:53,120
type of architectures that they were

44
00:01:53,120 --> 00:01:55,160
building I want like huh what what are

45
00:01:55,160 --> 00:01:56,630
you doing so this is actually the slide

46
00:01:56,630 --> 00:01:58,370
that was publicly presented so I can

47
00:01:58,370 --> 00:02:02,000
steal it without writing my own that was

48
00:02:02,000 --> 00:02:03,890
at the IBM conference and the guy was

49
00:02:03,890 --> 00:02:06,590
from Seagate so not even the customer I

50
00:02:06,590 --> 00:02:08,030
was dealing with and he was explaining

51
00:02:08,030 --> 00:02:10,780
how Seagate is dealing with sort of the

52
00:02:10,780 --> 00:02:12,950
Internet of Things because you know they

53
00:02:12,950 --> 00:02:14,630
basically manufacture a lot of hard

54
00:02:14,630 --> 00:02:15,680
drives and they want to collect

55
00:02:15,680 --> 00:02:17,220
information from the

56
00:02:17,220 --> 00:02:19,020
drives so they basically had a whole

57
00:02:19,020 --> 00:02:20,430
bunch of Hadoop clusters kind of an

58
00:02:20,430 --> 00:02:22,400
impoverished periphery you know

59
00:02:22,400 --> 00:02:24,330
aggregating all of the information but

60
00:02:24,330 --> 00:02:25,500
then all the time they actually had a

61
00:02:25,500 --> 00:02:27,090
traditional enterprise data warehousing

62
00:02:27,090 --> 00:02:29,550
right in the middle kind of actually

63
00:02:29,550 --> 00:02:31,410
taking care of the important data point

64
00:02:31,410 --> 00:02:33,090
and I was like what why not

65
00:02:33,090 --> 00:02:35,250
Hadoop and then we had a couple of more

66
00:02:35,250 --> 00:02:37,290
customers like that at P Hotel and the

67
00:02:37,290 --> 00:02:39,930
architecture that emerge throughout the

68
00:02:39,930 --> 00:02:42,710
group of customers was basically this

69
00:02:42,710 --> 00:02:46,440
and I would love if during the Q&A you

70
00:02:46,440 --> 00:02:48,180
could challenge me on this and maybe you

71
00:02:48,180 --> 00:02:50,070
can tell me how how wrong it is but

72
00:02:50,070 --> 00:02:51,209
that's actually what I'm seeing in the

73
00:02:51,209 --> 00:02:54,450
field so almost all of these guys who I

74
00:02:54,450 --> 00:02:55,800
will be talking about their building

75
00:02:55,800 --> 00:02:58,110
back-end architectures to essential

76
00:02:58,110 --> 00:03:00,030
support data driven applications and

77
00:03:00,030 --> 00:03:01,830
what I mean by that is that they

78
00:03:01,830 --> 00:03:03,510
basically have to optimize the

79
00:03:03,510 --> 00:03:04,770
relationship that they have with their

80
00:03:04,770 --> 00:03:06,780
customers and the devices or whatever it

81
00:03:06,780 --> 00:03:08,400
is that they're managing so it's

82
00:03:08,400 --> 00:03:10,910
typically like a bunch of either web or

83
00:03:10,910 --> 00:03:13,500
iut traffic coming into a data center

84
00:03:13,500 --> 00:03:15,240
and sometimes even multiple data centers

85
00:03:15,240 --> 00:03:17,910
and that's on the Left right and on the

86
00:03:17,910 --> 00:03:19,290
right you basically have a bunch of

87
00:03:19,290 --> 00:03:21,330
users you know using the application

88
00:03:21,330 --> 00:03:23,489
from mobile and desktop technologies and

89
00:03:23,489 --> 00:03:25,110
whatnot so the question then becomes

90
00:03:25,110 --> 00:03:27,350
what do you build in a data center as a

91
00:03:27,350 --> 00:03:30,510
complete rm2 ant architecture as opposed

92
00:03:30,510 --> 00:03:32,100
to just building a data science piece or

93
00:03:32,100 --> 00:03:34,410
you know a data management piece right

94
00:03:34,410 --> 00:03:36,840
and what we typically see is that and

95
00:03:36,840 --> 00:03:38,010
again you know some of it I'm just

96
00:03:38,010 --> 00:03:39,690
throwing names here you can use

97
00:03:39,690 --> 00:03:41,310
different projects within the Apache

98
00:03:41,310 --> 00:03:42,750
ecosystem but the name gives you a

99
00:03:42,750 --> 00:03:45,120
flavor for what typically gets used so

100
00:03:45,120 --> 00:03:46,410
typically like all of these events you

101
00:03:46,410 --> 00:03:48,630
know get represented as Kafka queues so

102
00:03:48,630 --> 00:03:49,860
the saying goes there if you know

103
00:03:49,860 --> 00:03:51,330
whatever it is on the outside of the

104
00:03:51,330 --> 00:03:53,730
datacenter must look like an Kafka to

105
00:03:53,730 --> 00:03:55,470
you you know inside of a data center and

106
00:03:55,470 --> 00:03:58,049
then you basically have some kind of an

107
00:03:58,049 --> 00:04:00,870
ETL 2.0 and what I mean by that is that

108
00:04:00,870 --> 00:04:02,730
it's not ETL that basically takes data

109
00:04:02,730 --> 00:04:04,410
from one existing place you know does

110
00:04:04,410 --> 00:04:06,030
something to it and put it into a

111
00:04:06,030 --> 00:04:07,680
different place it's actually ETL that

112
00:04:07,680 --> 00:04:09,989
does it on the data that's in flight so

113
00:04:09,989 --> 00:04:11,850
you know you basically have a bunch of

114
00:04:11,850 --> 00:04:14,940
ETL functions that are running on some

115
00:04:14,940 --> 00:04:16,640
kind of substrate so knife I the

116
00:04:16,640 --> 00:04:18,600
presentation right before me was a good

117
00:04:18,600 --> 00:04:21,478
example of something like that pivotal

118
00:04:21,478 --> 00:04:25,500
has the spring cloud data flow that can

119
00:04:25,500 --> 00:04:26,430
do some of that too

120
00:04:26,430 --> 00:04:29,849
but basically what gets what gets done

121
00:04:29,849 --> 00:04:30,750
here

122
00:04:30,750 --> 00:04:33,360
that you essentially split the traffic

123
00:04:33,360 --> 00:04:35,370
that's coming into your data center into

124
00:04:35,370 --> 00:04:37,320
multiple different things so first of

125
00:04:37,320 --> 00:04:38,880
all you basically get some of that

126
00:04:38,880 --> 00:04:40,980
traffic right straight into HDFS and you

127
00:04:40,980 --> 00:04:42,330
just dump it there and you leave it

128
00:04:42,330 --> 00:04:44,100
there and something will happen to it

129
00:04:44,100 --> 00:04:46,230
later on then some of that traffic is

130
00:04:46,230 --> 00:04:48,540
actually getting into the in-memory data

131
00:04:48,540 --> 00:04:50,340
grid and that's the thing that actually

132
00:04:50,340 --> 00:04:52,020
connects your application to the data

133
00:04:52,020 --> 00:04:53,550
that's in flight right now you know

134
00:04:53,550 --> 00:04:56,850
whatever this sensor is producing right

135
00:04:56,850 --> 00:04:58,380
now you will actually gather through

136
00:04:58,380 --> 00:05:00,630
that thing but then and that's actually

137
00:05:00,630 --> 00:05:02,370
interesting then you actually get some

138
00:05:02,370 --> 00:05:03,950
of the data into a more of a traditional

139
00:05:03,950 --> 00:05:06,360
MPP type of a solution and the good news

140
00:05:06,360 --> 00:05:07,890
is that you actually have an open source

141
00:05:07,890 --> 00:05:08,760
one and greenplum

142
00:05:08,760 --> 00:05:13,170
today and the split of all three here is

143
00:05:13,170 --> 00:05:16,380
interesting because what happens then is

144
00:05:16,380 --> 00:05:18,870
within all of these systems you

145
00:05:18,870 --> 00:05:20,610
basically have people who are typically

146
00:05:20,610 --> 00:05:22,350
you know either data scientist of data

147
00:05:22,350 --> 00:05:24,030
engineers who essentially maintain data

148
00:05:24,030 --> 00:05:25,680
models based on the raw data that gets

149
00:05:25,680 --> 00:05:28,800
into all of these systems right and you

150
00:05:28,800 --> 00:05:31,740
only get as much usability out of the

151
00:05:31,740 --> 00:05:34,020
data is the kind of data model

152
00:05:34,020 --> 00:05:35,370
interesting data models that you can

153
00:05:35,370 --> 00:05:37,080
then feed back into the application you

154
00:05:37,080 --> 00:05:38,220
get and these are the guys who

155
00:05:38,220 --> 00:05:40,200
maintaining it but even those guys

156
00:05:40,200 --> 00:05:42,240
basically sort of a lot of times at

157
00:05:42,240 --> 00:05:45,150
least in the customers that I see kind

158
00:05:45,150 --> 00:05:47,070
of push those data map models back into

159
00:05:47,070 --> 00:05:49,080
this middle layer which is the

160
00:05:49,080 --> 00:05:52,050
traditional MPP so the question is why

161
00:05:52,050 --> 00:05:56,550
and I think before we can answer that

162
00:05:56,550 --> 00:05:58,110
question again let me repeat the fact

163
00:05:58,110 --> 00:05:59,520
that what we're building here is not

164
00:05:59,520 --> 00:06:01,890
just a data science piece we're not just

165
00:06:01,890 --> 00:06:03,810
doing it for let's say you know life

166
00:06:03,810 --> 00:06:05,760
science is trying to analyze human DNA

167
00:06:05,760 --> 00:06:07,530
or something right we're building an EM

168
00:06:07,530 --> 00:06:09,540
to end architecture for essentially a

169
00:06:09,540 --> 00:06:12,990
modern way application you know the way

170
00:06:12,990 --> 00:06:15,600
you get it from uber or from Netflix or

171
00:06:15,600 --> 00:06:17,670
from Airbnb and for all of these guys

172
00:06:17,670 --> 00:06:18,630
you know the data that they're

173
00:06:18,630 --> 00:06:20,850
collecting must manifest itself in the

174
00:06:20,850 --> 00:06:22,620
application that's the end game for all

175
00:06:22,620 --> 00:06:25,590
of them right you know in uber it's

176
00:06:25,590 --> 00:06:27,990
basically you know tracking and sort of

177
00:06:27,990 --> 00:06:30,690
predicting and doing this search pricing

178
00:06:30,690 --> 00:06:32,790
you know Netflix must recommend the new

179
00:06:32,790 --> 00:06:35,370
movie for you and all of these guys

180
00:06:35,370 --> 00:06:37,200
again building this entrant architecture

181
00:06:37,200 --> 00:06:39,510
so can we just do it with Hadoop well

182
00:06:39,510 --> 00:06:40,830
again like I'm saying hopefully during

183
00:06:40,830 --> 00:06:43,290
the Q&A you will try to reconvince me

184
00:06:43,290 --> 00:06:44,190
that maybe we can

185
00:06:44,190 --> 00:06:47,580
but here's what I've seen so far in

186
00:06:47,580 --> 00:06:49,380
again talking talking to the customer

187
00:06:49,380 --> 00:06:50,610
because it's a pretty pragmatic

188
00:06:50,610 --> 00:06:52,470
presentation that I'm trying to do here

189
00:06:52,470 --> 00:06:55,320
so first answer is like yes absolutely

190
00:06:55,320 --> 00:06:56,820
but then you know the customer turns

191
00:06:56,820 --> 00:06:59,610
around and asks you but what do you mean

192
00:06:59,610 --> 00:07:01,200
by Hadoop what this could do these days

193
00:07:01,200 --> 00:07:02,880
really because if you're just talking

194
00:07:02,880 --> 00:07:04,830
about you know some kind of a scale out

195
00:07:04,830 --> 00:07:07,560
storage you know file system like HDFS

196
00:07:07,560 --> 00:07:08,790
yeah that's awesome you know we can

197
00:07:08,790 --> 00:07:10,890
totally use HDFS maybe you're also

198
00:07:10,890 --> 00:07:12,960
talking about scheduling frameworks

199
00:07:12,960 --> 00:07:15,540
something like Yaron that's fine but

200
00:07:15,540 --> 00:07:17,550
then there are all these other questions

201
00:07:17,550 --> 00:07:22,440
right a lot of the customers that are

202
00:07:22,440 --> 00:07:23,850
building those types of applications

203
00:07:23,850 --> 00:07:24,870
right now there are very traditional

204
00:07:24,870 --> 00:07:27,330
enterprises so they do a lot of what's

205
00:07:27,330 --> 00:07:28,980
known as bi you know business handle

206
00:07:28,980 --> 00:07:31,380
intelligence and bi is typically done

207
00:07:31,380 --> 00:07:32,940
through the tooling that actually

208
00:07:32,940 --> 00:07:34,620
expects a sequel database on the other

209
00:07:34,620 --> 00:07:37,410
end so the most you know traditional

210
00:07:37,410 --> 00:07:38,880
example of the tooling like that is a

211
00:07:38,880 --> 00:07:40,260
tableau so you know very

212
00:07:40,260 --> 00:07:42,090
well-established or the few I for doing

213
00:07:42,090 --> 00:07:44,220
bi and there is absolutely nothing that

214
00:07:44,220 --> 00:07:45,990
you can like hook up tableau right away

215
00:07:45,990 --> 00:07:48,120
to in Hadoop ecosystem you can kind of

216
00:07:48,120 --> 00:07:50,000
have to build additional bits and pieces

217
00:07:50,000 --> 00:07:53,220
into that that one is actually also

218
00:07:53,220 --> 00:07:54,810
interesting because a lot of them are

219
00:07:54,810 --> 00:07:57,030
telling me like this is awesome that all

220
00:07:57,030 --> 00:07:59,160
of the Big Data community is talking

221
00:07:59,160 --> 00:08:01,050
about machine learning in Scala and on

222
00:08:01,050 --> 00:08:04,410
SPARC and that's basically great for the

223
00:08:04,410 --> 00:08:07,110
five people that know how to do it and

224
00:08:07,110 --> 00:08:09,810
all the rest of us who are still stuck

225
00:08:09,810 --> 00:08:11,610
with sequel and you know stored

226
00:08:11,610 --> 00:08:13,650
procedures we actually want it to be

227
00:08:13,650 --> 00:08:16,110
given to us we want to contribute to the

228
00:08:16,110 --> 00:08:17,940
machine learning as much as the guys who

229
00:08:17,940 --> 00:08:20,970
actually do Scala and Java and whatnot

230
00:08:20,970 --> 00:08:23,070
so I actually tend to call it democracy

231
00:08:23,070 --> 00:08:25,620
democracy is bi and machine learning if

232
00:08:25,620 --> 00:08:27,690
you can open up it to the people who

233
00:08:27,690 --> 00:08:31,230
have the old-school skill sets then the

234
00:08:31,230 --> 00:08:33,450
the first thing that you're doing you're

235
00:08:33,450 --> 00:08:35,219
basically getting in getting them into

236
00:08:35,219 --> 00:08:37,650
the game right you cannot really put

237
00:08:37,650 --> 00:08:39,030
that barrier in front of them and say

238
00:08:39,030 --> 00:08:40,770
like you gotta learn scale you know go

239
00:08:40,770 --> 00:08:42,630
learn spark you know to be productive

240
00:08:42,630 --> 00:08:44,520
you kind of have to invite them over and

241
00:08:44,520 --> 00:08:45,780
you know extend the hand and say like

242
00:08:45,780 --> 00:08:47,160
yeah you can be productive you know and

243
00:08:47,160 --> 00:08:49,380
I'll show you how and maybe gradually

244
00:08:49,380 --> 00:08:51,120
you know you will build build out your

245
00:08:51,120 --> 00:08:54,240
skills have and the other two like once

246
00:08:54,240 --> 00:08:55,740
you do that the other to them becomes

247
00:08:55,740 --> 00:08:57,360
you know the question is like oh so now

248
00:08:57,360 --> 00:08:57,810
I

249
00:08:57,810 --> 00:08:59,850
not just five guys within my company who

250
00:08:59,850 --> 00:09:01,560
know how to do spark in Scala I have

251
00:09:01,560 --> 00:09:03,420
hundreds of people who actually are very

252
00:09:03,420 --> 00:09:05,639
comfortable doing bi but what happens if

253
00:09:05,639 --> 00:09:07,470
I open up that system to all of these

254
00:09:07,470 --> 00:09:09,569
guys to kinda like hammer on at once can

255
00:09:09,569 --> 00:09:12,839
it actually handle the load and the last

256
00:09:12,839 --> 00:09:13,980
one is again if you're building an

257
00:09:13,980 --> 00:09:15,149
application you kind of have to have

258
00:09:15,149 --> 00:09:16,649
something that you would be able to hook

259
00:09:16,649 --> 00:09:18,870
up in a traditional in memory scale out

260
00:09:18,870 --> 00:09:23,370
you know layer and a sequel database is

261
00:09:23,370 --> 00:09:25,230
typically a good option again you can do

262
00:09:25,230 --> 00:09:26,939
HBase or something like that but then

263
00:09:26,939 --> 00:09:28,410
you're sort of assembling bits and

264
00:09:28,410 --> 00:09:30,749
pieces in a very ad hoc manner instead

265
00:09:30,749 --> 00:09:32,670
of just having a central piece of your

266
00:09:32,670 --> 00:09:34,620
architecture that can do a lot of it can

267
00:09:34,620 --> 00:09:36,839
answer a lot of these questions as

268
00:09:36,839 --> 00:09:39,120
opposed to you know trying to do just

269
00:09:39,120 --> 00:09:42,240
one thing like many of the Hadoop

270
00:09:42,240 --> 00:09:45,540
ecosystem projects do so to summarize

271
00:09:45,540 --> 00:09:49,050
what I was trying to say I guess after I

272
00:09:49,050 --> 00:09:51,449
worked with who do four well I guess it

273
00:09:51,449 --> 00:09:56,100
would be ten years by now I can really

274
00:09:56,100 --> 00:09:58,680
vouch for these so coop is really great

275
00:09:58,680 --> 00:10:00,689
for hadouken hadouken system is really

276
00:10:00,689 --> 00:10:02,850
great for elastic storage capacity

277
00:10:02,850 --> 00:10:07,079
I mean HDFS is pretty well debugged and

278
00:10:07,079 --> 00:10:08,790
awesome at this point you know it has

279
00:10:08,790 --> 00:10:10,259
all the bells and whistles like high

280
00:10:10,259 --> 00:10:12,149
availability you can configure it you

281
00:10:12,149 --> 00:10:13,889
know to be pretty fault tolerant so if

282
00:10:13,889 --> 00:10:15,629
you want to just dump data you know to

283
00:10:15,629 --> 00:10:17,490
some place very quickly remember that

284
00:10:17,490 --> 00:10:19,529
you know bottom arrow on my slide you

285
00:10:19,529 --> 00:10:20,910
know just like the data comes in and you

286
00:10:20,910 --> 00:10:22,319
need to just store it for later

287
00:10:22,319 --> 00:10:24,449
processing HDFS is great it's great for

288
00:10:24,449 --> 00:10:26,459
lending data it's great for discovery is

289
00:10:26,459 --> 00:10:28,050
great for trying to figure out what's

290
00:10:28,050 --> 00:10:30,059
inside of your data and that is actually

291
00:10:30,059 --> 00:10:31,589
a job for a couple of people in the

292
00:10:31,589 --> 00:10:33,240
enterprise and here's why because all of

293
00:10:33,240 --> 00:10:36,689
these enterprises are really bound by

294
00:10:36,689 --> 00:10:38,910
regulations and just internal you know

295
00:10:38,910 --> 00:10:42,629
rules of what data can be exposed and

296
00:10:42,629 --> 00:10:45,480
how and if all you get is just raw

297
00:10:45,480 --> 00:10:48,240
streams of data that is coming in you

298
00:10:48,240 --> 00:10:50,490
typically want to reduce the number of

299
00:10:50,490 --> 00:10:52,050
people who can actually explore that

300
00:10:52,050 --> 00:10:54,269
data you want that exploration process

301
00:10:54,269 --> 00:10:56,069
to happen within a very limited group of

302
00:10:56,069 --> 00:10:59,040
people so that later on you know you can

303
00:10:59,040 --> 00:11:01,079
actually open it up but you can open up

304
00:11:01,079 --> 00:11:03,149
in a meaningful way so maybe you will be

305
00:11:03,149 --> 00:11:04,740
masking some of the data fields maybe

306
00:11:04,740 --> 00:11:05,939
you will be transforming it into

307
00:11:05,939 --> 00:11:07,920
something else maybe you will be putting

308
00:11:07,920 --> 00:11:10,199
a particular Eckles in place you know to

309
00:11:10,199 --> 00:11:11,570
actually make it more excessive

310
00:11:11,570 --> 00:11:13,730
but the initial exploration you know the

311
00:11:13,730 --> 00:11:16,040
fact that all of a sudden the log files

312
00:11:16,040 --> 00:11:17,270
that are streaming through your

313
00:11:17,270 --> 00:11:18,980
application contain you know personally

314
00:11:18,980 --> 00:11:20,930
identifiable information that cannot be

315
00:11:20,930 --> 00:11:21,910
overlooked

316
00:11:21,910 --> 00:11:24,140
and you know if you want to do

317
00:11:24,140 --> 00:11:25,820
sophisticated machine learning a

318
00:11:25,820 --> 00:11:27,890
duplicate system is great so

319
00:11:27,890 --> 00:11:30,260
complementary today to that traditional

320
00:11:30,260 --> 00:11:32,390
MPP which stands from massively parallel

321
00:11:32,390 --> 00:11:35,060
processing if you wondering what what

322
00:11:35,060 --> 00:11:37,700
MPP is it's actually really great for

323
00:11:37,700 --> 00:11:39,410
scheming right which is basically once

324
00:11:39,410 --> 00:11:41,090
you know what the data is look like you

325
00:11:41,090 --> 00:11:42,710
know what the data it is that you're

326
00:11:42,710 --> 00:11:44,660
dealing with you actually want it to be

327
00:11:44,660 --> 00:11:46,430
available to as many people as possible

328
00:11:46,430 --> 00:11:48,560
and then you create all these views and

329
00:11:48,560 --> 00:11:50,630
tables you know that it's very

330
00:11:50,630 --> 00:11:53,150
traditional bi person would expect but

331
00:11:53,150 --> 00:11:55,310
then you are making the data known you

332
00:11:55,310 --> 00:11:58,280
actually have an API between you who is

333
00:11:58,280 --> 00:11:59,870
maintaining the data infrastructure and

334
00:11:59,870 --> 00:12:01,310
the people who will be analyzing and

335
00:12:01,310 --> 00:12:03,140
creating the data model that is a very

336
00:12:03,140 --> 00:12:04,820
well understood API it's a bunch of

337
00:12:04,820 --> 00:12:07,630
views sometimes a bunch of tables

338
00:12:07,630 --> 00:12:09,830
transactionality sometimes comes into

339
00:12:09,830 --> 00:12:11,150
picture because it's easier to do

340
00:12:11,150 --> 00:12:13,280
transactions on MPP and to some

341
00:12:13,280 --> 00:12:15,980
customers is actually meaningful it's

342
00:12:15,980 --> 00:12:19,610
interesting and the last one for those

343
00:12:19,610 --> 00:12:21,580
of you who cannot actually see it I'll

344
00:12:21,580 --> 00:12:25,550
read the bottom one so that democratized

345
00:12:25,550 --> 00:12:28,670
machine learning you can all of a sudden

346
00:12:28,670 --> 00:12:30,680
use are and you can all of a sudden use

347
00:12:30,680 --> 00:12:32,150
your traditional sort of sequel like

348
00:12:32,150 --> 00:12:35,750
tools which I will cover in a bit so

349
00:12:35,750 --> 00:12:38,300
what do you do it will there's maybe a

350
00:12:38,300 --> 00:12:40,820
few choices in the open source you know

351
00:12:40,820 --> 00:12:42,920
the one that I will be getting you

352
00:12:42,920 --> 00:12:44,210
through today is called green plum

353
00:12:44,210 --> 00:12:46,400
database so green plum database is

354
00:12:46,400 --> 00:12:47,960
actually a proud member of the progress

355
00:12:47,960 --> 00:12:51,770
family of relational databases it's kind

356
00:12:51,770 --> 00:12:53,450
of like a long timeline but a couple of

357
00:12:53,450 --> 00:12:55,010
points I wanted to pull out you know

358
00:12:55,010 --> 00:12:56,690
politics actually got created a long

359
00:12:56,690 --> 00:12:58,460
time ago like really a long time ago in

360
00:12:58,460 --> 00:13:01,880
86 so green plum database got created

361
00:13:01,880 --> 00:13:03,380
based on progress eight you know it

362
00:13:03,380 --> 00:13:05,960
around 2005 interestingly enough there

363
00:13:05,960 --> 00:13:07,490
was exactly the year when Hadoop got

364
00:13:07,490 --> 00:13:09,440
created now who do got created around

365
00:13:09,440 --> 00:13:13,010
2005 as a sub project of match 2006 is

366
00:13:13,010 --> 00:13:16,100
when sort of Yahoo got really interested

367
00:13:16,100 --> 00:13:18,110
in it and the rest is history but the

368
00:13:18,110 --> 00:13:20,840
point that I wanted to make is that we

369
00:13:20,840 --> 00:13:22,520
open source we implement database in

370
00:13:22,520 --> 00:13:24,750
2015 and we

371
00:13:24,750 --> 00:13:28,140
even rebased it on Postgres 8.4 so it's

372
00:13:28,140 --> 00:13:29,700
still not possible sign and we're

373
00:13:29,700 --> 00:13:31,200
working on that but at least it's close

374
00:13:31,200 --> 00:13:33,000
enough to really sort of the kind of

375
00:13:33,000 --> 00:13:34,350
posters that you would expect

376
00:13:34,350 --> 00:13:36,960
so very quickly Green clamp database is

377
00:13:36,960 --> 00:13:40,680
a MPP project you know it's something

378
00:13:40,680 --> 00:13:43,320
that has been in development for more

379
00:13:43,320 --> 00:13:44,910
than ten years if you want to

380
00:13:44,910 --> 00:13:47,660
participate your best option is

381
00:13:47,660 --> 00:13:50,940
www.drinklab.org G you know all of the

382
00:13:50,940 --> 00:13:54,750
community aspects are on that website if

383
00:13:54,750 --> 00:13:56,610
you're wondering what green plum is and

384
00:13:56,610 --> 00:13:59,220
what it does think of it as a Charlotte

385
00:13:59,220 --> 00:14:01,230
Postgres so you basically have a bunch

386
00:14:01,230 --> 00:14:03,150
of what we call segments and these

387
00:14:03,150 --> 00:14:04,800
segments are essentially individual

388
00:14:04,800 --> 00:14:07,200
posters databases you also have a master

389
00:14:07,200 --> 00:14:09,000
course right here that is you know

390
00:14:09,000 --> 00:14:11,220
coordinating things like query planning

391
00:14:11,220 --> 00:14:13,230
and you know what data needs to get

392
00:14:13,230 --> 00:14:15,450
where but basically the way to think

393
00:14:15,450 --> 00:14:17,010
like the mental model is very sort of

394
00:14:17,010 --> 00:14:19,290
simplified mental model is it's a

395
00:14:19,290 --> 00:14:21,930
traditional progress database but every

396
00:14:21,930 --> 00:14:24,270
table that you create has a column that

397
00:14:24,270 --> 00:14:27,030
you need to designate and that column is

398
00:14:27,030 --> 00:14:28,920
used for sharding so basically whatever

399
00:14:28,920 --> 00:14:32,839
the hash of the value in that column

400
00:14:32,839 --> 00:14:35,730
depending on the value of that hash your

401
00:14:35,730 --> 00:14:37,770
data will end up in different segments

402
00:14:37,770 --> 00:14:42,150
on different hosts you still get if you

403
00:14:42,150 --> 00:14:44,610
as though it is a database that's just

404
00:14:44,610 --> 00:14:46,080
available on a single system it's a

405
00:14:46,080 --> 00:14:47,880
traditional process database so anything

406
00:14:47,880 --> 00:14:49,430
that works with partners would work with

407
00:14:49,430 --> 00:14:52,290
greenplum you can just connect you know

408
00:14:52,290 --> 00:14:53,850
your traditional PC pool command and

409
00:14:53,850 --> 00:14:55,320
connect to a master and then you know

410
00:14:55,320 --> 00:14:56,790
you're querying the database is just

411
00:14:56,790 --> 00:15:00,200
like you would do with pause-press

412
00:15:00,200 --> 00:15:02,730
it's a shared nothing architecture so

413
00:15:02,730 --> 00:15:04,500
basically like I said the master just

414
00:15:04,500 --> 00:15:08,040
you know does coordination and a lot of

415
00:15:08,040 --> 00:15:10,280
times we sort of do the high speed

416
00:15:10,280 --> 00:15:13,589
interconnects for pipeline so you know

417
00:15:13,589 --> 00:15:16,470
the the guys talk you know talk through

418
00:15:16,470 --> 00:15:19,470
this interconnect greenplum is a topic

419
00:15:19,470 --> 00:15:21,720
software solution so if you've seen

420
00:15:21,720 --> 00:15:23,580
green plum in the card where incarnation

421
00:15:23,580 --> 00:15:25,440
you know you might have seen some of the

422
00:15:25,440 --> 00:15:27,000
networking that was specifically

423
00:15:27,000 --> 00:15:29,370
optimized for it it helps that you don't

424
00:15:29,370 --> 00:15:31,560
have to do it so you can just launch

425
00:15:31,560 --> 00:15:33,990
green plum on Amazon or Azure and you're

426
00:15:33,990 --> 00:15:35,550
in your own data center that's that's

427
00:15:35,550 --> 00:15:36,930
just fine

428
00:15:36,930 --> 00:15:38,550
so a couple couple of the points that I

429
00:15:38,550 --> 00:15:41,040
wanted to make is that green plum

430
00:15:41,040 --> 00:15:43,770
actually has a concept that is now in

431
00:15:43,770 --> 00:15:46,650
Patras known as foreign dater rappers FD

432
00:15:46,650 --> 00:15:50,160
W's in green plum it's a different

433
00:15:50,160 --> 00:15:51,420
slightly different one because when

434
00:15:51,420 --> 00:15:53,700
green plum got it partners didn't

435
00:15:53,700 --> 00:15:56,580
actually have FGM w's but green plum can

436
00:15:56,580 --> 00:15:58,320
connect to a lot of data sources right

437
00:15:58,320 --> 00:16:00,480
so green pumpkin actually represent as

438
00:16:00,480 --> 00:16:02,010
an external table something that's one

439
00:16:02,010 --> 00:16:05,880
is three or in you know files on your

440
00:16:05,880 --> 00:16:08,339
file system or even in HDFS itself and

441
00:16:08,339 --> 00:16:10,200
that is exactly how we're connecting it

442
00:16:10,200 --> 00:16:11,370
to the Hadoop ecosystem so that

443
00:16:11,370 --> 00:16:14,610
connection to the HDFS is the Hadoop

444
00:16:14,610 --> 00:16:17,310
ecosystem connection interesting cool

445
00:16:17,310 --> 00:16:19,920
things that are sort of part of green

446
00:16:19,920 --> 00:16:23,130
plum Orca is the first query optimizer

447
00:16:23,130 --> 00:16:24,750
you know that is basically planning

448
00:16:24,750 --> 00:16:26,760
queries knowing that it does it for an

449
00:16:26,760 --> 00:16:29,370
MPP type of a system and it's optimizing

450
00:16:29,370 --> 00:16:32,790
the query to run for long it's actually

451
00:16:32,790 --> 00:16:34,980
a pretty sophisticated piece of software

452
00:16:34,980 --> 00:16:37,529
based on a scientific research that's

453
00:16:37,529 --> 00:16:40,770
been done for at least five years it's a

454
00:16:40,770 --> 00:16:42,270
standalone component so it can be

455
00:16:42,270 --> 00:16:45,360
plugged into any kind of database it's

456
00:16:45,360 --> 00:16:48,150
easier to plug it into well conceptually

457
00:16:48,150 --> 00:16:49,529
at least it's easier to plug it into

458
00:16:49,529 --> 00:16:51,240
Postgres you know family of databases

459
00:16:51,240 --> 00:16:52,860
but you can actually plug it into any

460
00:16:52,860 --> 00:16:54,900
database because it's implemented as a

461
00:16:54,900 --> 00:16:58,050
separate standalone service that does

462
00:16:58,050 --> 00:17:00,870
the query planning for you speaking of

463
00:17:00,870 --> 00:17:03,050
the democratized machine learning so

464
00:17:03,050 --> 00:17:06,300
greenplum actually comes with the

465
00:17:06,300 --> 00:17:08,040
machine learning library called Madlib

466
00:17:08,040 --> 00:17:10,260
and it's an interesting one because it

467
00:17:10,260 --> 00:17:11,849
does machine learning as though you are

468
00:17:11,849 --> 00:17:15,420
doing queries on tables via traditional

469
00:17:15,420 --> 00:17:19,050
sequel so for example if we want to do a

470
00:17:19,050 --> 00:17:21,119
linear regression and basically just

471
00:17:21,119 --> 00:17:23,369
train our model instead of writing any

472
00:17:23,369 --> 00:17:25,949
kind of code we would just invoke one of

473
00:17:25,949 --> 00:17:28,020
the pre-canned functions that madly but

474
00:17:28,020 --> 00:17:30,000
the library gives us now the invocation

475
00:17:30,000 --> 00:17:31,770
would look like a select statement you

476
00:17:31,770 --> 00:17:33,179
know that's the training of the model so

477
00:17:33,179 --> 00:17:35,370
we're basically passing tables here and

478
00:17:35,370 --> 00:17:38,100
what we get is we get the output that is

479
00:17:38,100 --> 00:17:40,260
the model that is trained and of course

480
00:17:40,260 --> 00:17:41,760
you know the way we would then use that

481
00:17:41,760 --> 00:17:43,380
model is again through the same Select

482
00:17:43,380 --> 00:17:45,570
you know similar select statement so all

483
00:17:45,570 --> 00:17:47,460
we're doing is we're essentially calling

484
00:17:47,460 --> 00:17:50,000
out functions that are being

485
00:17:50,000 --> 00:17:52,220
to us through the sequel interface by

486
00:17:52,220 --> 00:17:54,740
the Madlib that got installed on our

487
00:17:54,740 --> 00:17:57,230
cluster so again the flow between Madlib

488
00:17:57,230 --> 00:17:58,910
and the rest of the system is pretty

489
00:17:58,910 --> 00:18:01,160
simple actually it's kind of like you

490
00:18:01,160 --> 00:18:03,380
know the MapReduce without exposing to

491
00:18:03,380 --> 00:18:05,480
you the MapReduce part right so you

492
00:18:05,480 --> 00:18:06,860
essentially have a client which could be

493
00:18:06,860 --> 00:18:08,420
like I said anything you know could be

494
00:18:08,420 --> 00:18:09,140
Jupiter

495
00:18:09,140 --> 00:18:10,460
you know Apaches Apple II and if you're

496
00:18:10,460 --> 00:18:13,310
doing machine learning P sequel works

497
00:18:13,310 --> 00:18:14,780
just fine so you can run all over this

498
00:18:14,780 --> 00:18:17,120
examples in piece equal so the sequel

499
00:18:17,120 --> 00:18:19,970
client talks to the database server and

500
00:18:19,970 --> 00:18:22,130
then the interesting stuff begins on the

501
00:18:22,130 --> 00:18:24,620
master so you basically end up with a

502
00:18:24,620 --> 00:18:26,690
stored procedure that gets called on the

503
00:18:26,690 --> 00:18:29,060
master whole bunch of the same stored

504
00:18:29,060 --> 00:18:30,710
procedures get called on the segments

505
00:18:30,710 --> 00:18:32,210
you know then the aggregation happens if

506
00:18:32,210 --> 00:18:34,010
it needs to happen so it's kind of like

507
00:18:34,010 --> 00:18:36,500
fanning out and then a grinning there is

508
00:18:36,500 --> 00:18:41,990
a way for Madlib to also do coalescing

509
00:18:41,990 --> 00:18:43,700
of the data and basically around

510
00:18:43,700 --> 00:18:46,130
algorithm that need to converge so if

511
00:18:46,130 --> 00:18:47,690
the convergence needs to happen that the

512
00:18:47,690 --> 00:18:49,130
convergence would be happening through

513
00:18:49,130 --> 00:18:50,810
the master so that's the only bottleneck

514
00:18:50,810 --> 00:18:52,130
that you would have because you know the

515
00:18:52,130 --> 00:18:53,540
segment's will not be able to talk to

516
00:18:53,540 --> 00:18:55,820
each other but the convergence still

517
00:18:55,820 --> 00:18:57,320
could happen so if you're converging to

518
00:18:57,320 --> 00:19:00,710
a reasonably small value or small in

519
00:19:00,710 --> 00:19:03,410
size right in how you represent it then

520
00:19:03,410 --> 00:19:04,670
it works pretty well and that's how

521
00:19:04,670 --> 00:19:06,680
actually we do some of the graph

522
00:19:06,680 --> 00:19:12,140
algorithms within Madlib Madlib is

523
00:19:12,140 --> 00:19:13,280
interesting because it's not just

524
00:19:13,280 --> 00:19:13,730
greenplum

525
00:19:13,730 --> 00:19:15,800
we actually have people from the

526
00:19:15,800 --> 00:19:17,540
progress community who are using MATLAB

527
00:19:17,540 --> 00:19:20,780
to learn to do machine learning with in

528
00:19:20,780 --> 00:19:25,370
the progress installation itself so even

529
00:19:25,370 --> 00:19:26,930
if you're running a single node progress

530
00:19:26,930 --> 00:19:28,190
and you have a whole bunch of data

531
00:19:28,190 --> 00:19:29,990
locked into that single node progress

532
00:19:29,990 --> 00:19:31,970
you want to run you know simple linear

533
00:19:31,970 --> 00:19:33,740
regression or you want to run you know

534
00:19:33,740 --> 00:19:35,270
some graph algorithms that we're

535
00:19:35,270 --> 00:19:37,460
developing right now you can actually do

536
00:19:37,460 --> 00:19:41,120
it with MATLAB and just again for you to

537
00:19:41,120 --> 00:19:43,040
understand what's going on the pieces

538
00:19:43,040 --> 00:19:44,960
that you will be dealing with and

539
00:19:44,960 --> 00:19:47,360
basically at the very bottom you have a

540
00:19:47,360 --> 00:19:49,970
C API that progress exposes to anything

541
00:19:49,970 --> 00:19:52,400
that happens to be a function then you

542
00:19:52,400 --> 00:19:54,020
have a whole bunch of low-level

543
00:19:54,020 --> 00:19:55,400
abstractions you know that just

544
00:19:55,400 --> 00:19:57,170
mechanics of how Madlib itself is

545
00:19:57,170 --> 00:19:59,000
implemented and then a whole bunch of

546
00:19:59,000 --> 00:20:01,040
traditional machine learning functions

547
00:20:01,040 --> 00:20:02,840
that been implemented you know on top of

548
00:20:02,840 --> 00:20:03,830
these abstractions

549
00:20:03,830 --> 00:20:05,330
but essentially the idea is simple you

550
00:20:05,330 --> 00:20:06,740
know the poker segment gives you the

551
00:20:06,740 --> 00:20:09,980
data in form of tuples right you know

552
00:20:09,980 --> 00:20:11,450
think of it as an array of data that you

553
00:20:11,450 --> 00:20:13,160
can read through and then these

554
00:20:13,160 --> 00:20:14,840
functions basically do all of the work

555
00:20:14,840 --> 00:20:16,910
and if there's anything that needs to

556
00:20:16,910 --> 00:20:19,100
happen between different functions sort

557
00:20:19,100 --> 00:20:21,320
of running on different segments then

558
00:20:21,320 --> 00:20:22,550
that would happen through the

559
00:20:22,550 --> 00:20:25,130
abstractions that are provided here here

560
00:20:25,130 --> 00:20:26,840
you know you basically have a whole

561
00:20:26,840 --> 00:20:28,460
bunch of libraries that we've written

562
00:20:28,460 --> 00:20:30,200
for Madlib so python is one of them

563
00:20:30,200 --> 00:20:33,350
there's a integration between MATLAB and

564
00:20:33,350 --> 00:20:35,030
Python so you can actually do a lot of

565
00:20:35,030 --> 00:20:36,800
traditional machine learning you know

566
00:20:36,800 --> 00:20:38,210
talking to the back end which is a

567
00:20:38,210 --> 00:20:40,490
cluster another example of this layer is

568
00:20:40,490 --> 00:20:42,470
R so a lot of times we would actually

569
00:20:42,470 --> 00:20:44,630
have people who would call Madlib

570
00:20:44,630 --> 00:20:46,700
functions through the our interface

571
00:20:46,700 --> 00:20:48,290
because we have that integration

572
00:20:48,290 --> 00:20:50,480
provided to them where a Madlib function

573
00:20:50,480 --> 00:20:52,010
would actually look like a R function

574
00:20:52,010 --> 00:20:54,380
now of course again anything at this

575
00:20:54,380 --> 00:20:56,720
level you don't want a lot of data to

576
00:20:56,720 --> 00:20:58,550
being transferred back and forth so you

577
00:20:58,550 --> 00:21:00,530
basically just programming a cluster and

578
00:21:00,530 --> 00:21:03,350
you're telling the cluster what to do so

579
00:21:03,350 --> 00:21:05,180
that's Madlib it's pretty useful piece

580
00:21:05,180 --> 00:21:06,440
you know standalone piece it's actually

581
00:21:06,440 --> 00:21:08,570
an app a trapeze right now so it got

582
00:21:08,570 --> 00:21:09,800
transferred into the Apache Software

583
00:21:09,800 --> 00:21:12,500
Foundation it's an incubating project if

584
00:21:12,500 --> 00:21:13,910
you're interested you know we're more

585
00:21:13,910 --> 00:21:16,220
than welcome or you as a contributor you

586
00:21:16,220 --> 00:21:17,540
know there's quite a lot of interesting

587
00:21:17,540 --> 00:21:18,770
stuff to do

588
00:21:18,770 --> 00:21:21,290
greenplum itself is not an Apache

589
00:21:21,290 --> 00:21:22,940
project that's under the Apache License

590
00:21:22,940 --> 00:21:27,880
but it's a standalone project go to

591
00:21:31,180 --> 00:21:50,660
www.howtogetthejobideserve.com and then

592
00:21:50,660 --> 00:21:52,280
Debian was the first distribution that

593
00:21:52,280 --> 00:21:54,080
kind of like put it all together and

594
00:21:54,080 --> 00:21:55,190
then a whole bunch of secondary

595
00:21:55,190 --> 00:21:57,050
distributions you know got created based

596
00:21:57,050 --> 00:21:58,910
on Debian so Big Top is trying to do

597
00:21:58,910 --> 00:22:01,700
that with big data so you know we have

598
00:22:01,700 --> 00:22:03,470
Hadoop ecosystem but not just Hadoop

599
00:22:03,470 --> 00:22:05,210
ecosystem now we also have you know

600
00:22:05,210 --> 00:22:07,280
things like our green palm which I was

601
00:22:07,280 --> 00:22:10,040
talking about today there is big top and

602
00:22:10,040 --> 00:22:11,840
then a whole bunch of distributions that

603
00:22:11,840 --> 00:22:13,790
use big top to create products that they

604
00:22:13,790 --> 00:22:16,649
give to their customers

605
00:22:16,649 --> 00:22:20,200
so far we've laid the groundwork in the

606
00:22:20,200 --> 00:22:22,149
big-top community to integrate greenplum

607
00:22:22,149 --> 00:22:24,970
into the Hadoop ecosystem to allow you

608
00:22:24,970 --> 00:22:26,649
the kind of architectures I was talking

609
00:22:26,649 --> 00:22:29,590
about today so the basic functionality

610
00:22:29,590 --> 00:22:31,690
packaging deployment and docker

611
00:22:31,690 --> 00:22:33,759
orchestration is there so you can

612
00:22:33,759 --> 00:22:35,889
actually give the RPM and DB n packages

613
00:22:35,889 --> 00:22:36,999
for greenplum

614
00:22:36,999 --> 00:22:38,799
you know from Big Top community you can

615
00:22:38,799 --> 00:22:40,570
deploy them on your cluster using puppet

616
00:22:40,570 --> 00:22:43,570
code that is provided there are docker

617
00:22:43,570 --> 00:22:45,039
containers you know for that stuff as

618
00:22:45,039 --> 00:22:46,690
well there's basic integration with

619
00:22:46,690 --> 00:22:49,139
MATLAB so christian who is sitting here

620
00:22:49,139 --> 00:22:52,629
did a demo i think it lost for them

621
00:22:52,629 --> 00:22:54,519
maybe or some other conference where we

622
00:22:54,519 --> 00:22:56,169
basically demonstrated how you can use

623
00:22:56,169 --> 00:22:59,049
mad libs through apache Zeppelin which

624
00:22:59,049 --> 00:23:00,940
is a really nice tool similar to Jupiter

625
00:23:00,940 --> 00:23:02,470
you know from the Python community

626
00:23:02,470 --> 00:23:04,929
that's very well known in the Apache Big

627
00:23:04,929 --> 00:23:06,369
Data ecosystem you know if you're doing

628
00:23:06,369 --> 00:23:07,720
machine learning and you're trying to do

629
00:23:07,720 --> 00:23:10,269
this you know notebooks right for data

630
00:23:10,269 --> 00:23:13,149
scientists pretty useful tool we're

631
00:23:13,149 --> 00:23:14,619
actually interested in you know some of

632
00:23:14,619 --> 00:23:16,179
the juju deployment throw the basic

633
00:23:16,179 --> 00:23:17,739
rudimentary capabilities are there but

634
00:23:17,739 --> 00:23:18,970
if you're interested you know talk to me

635
00:23:18,970 --> 00:23:21,669
after this presentation and the HDFS

636
00:23:21,669 --> 00:23:24,039
integration is there you can get data in

637
00:23:24,039 --> 00:23:26,109
and out of HDFS as external tables but

638
00:23:26,109 --> 00:23:27,249
it's slow and we're trying to optimize

639
00:23:27,249 --> 00:23:29,289
it now so again talk to me after the

640
00:23:29,289 --> 00:23:32,619
presentation here's the stuff that were

641
00:23:32,619 --> 00:23:35,879
interested in doing and if you want

642
00:23:35,879 --> 00:23:38,100
additional items on this list

643
00:23:38,100 --> 00:23:41,320
be BRB be our guest I mean it's

644
00:23:41,320 --> 00:23:43,059
basically as much as you know you could

645
00:23:43,059 --> 00:23:46,809
you could possibly think of I'm

646
00:23:46,809 --> 00:23:51,309
particularly interested in this one and

647
00:23:51,309 --> 00:23:55,179
this this one so pause verse 9 is one of

648
00:23:55,179 --> 00:23:57,220
the big deals that we have to kind of

649
00:23:57,220 --> 00:23:58,299
like accomplish I don't think will

650
00:23:58,299 --> 00:23:59,679
accomplish it the same way we did it

651
00:23:59,679 --> 00:24:02,200
with progress eight which you know Heike

652
00:24:02,200 --> 00:24:03,759
for those of you who know him you know

653
00:24:03,759 --> 00:24:06,159
one of the sort of core progress guys

654
00:24:06,159 --> 00:24:08,739
kind of like just did this huge rebase

655
00:24:08,739 --> 00:24:10,389
of the entire code based on progress aid

656
00:24:10,389 --> 00:24:12,519
progress 9 is too big of a chance for us

657
00:24:12,519 --> 00:24:14,950
to bite that way so we'll probably just

658
00:24:14,950 --> 00:24:16,989
back port features from progress 9 you

659
00:24:16,989 --> 00:24:19,299
know feature at the time and that's

660
00:24:19,299 --> 00:24:20,830
actually where your feedback would be

661
00:24:20,830 --> 00:24:23,379
extremely useful because we need to know

662
00:24:23,379 --> 00:24:24,909
which features to prioritize you know

663
00:24:24,909 --> 00:24:26,470
for backporting so for example we know

664
00:24:26,470 --> 00:24:28,239
that binary JSON is super interesting

665
00:24:28,239 --> 00:24:30,160
right and the only reason we know is

666
00:24:30,160 --> 00:24:31,330
because you know people talked to us and

667
00:24:31,330 --> 00:24:36,340
told us so and this one is where I think

668
00:24:36,340 --> 00:24:38,410
the real interesting integration between

669
00:24:38,410 --> 00:24:40,000
Hadoop ecosystem and greenplum will

670
00:24:40,000 --> 00:24:43,570
begin so we want to make it a full sort

671
00:24:43,570 --> 00:24:45,430
of fledged member of the Cadoo PK system

672
00:24:45,430 --> 00:24:47,200
so all of the tools that exist within

673
00:24:47,200 --> 00:24:49,240
that ecosystem can actually benefit from

674
00:24:49,240 --> 00:24:52,870
green plum and vice versa so that's it

675
00:24:52,870 --> 00:24:54,580
that's that's all I have you know let's

676
00:24:54,580 --> 00:24:56,500
try to build it together if it sounds

677
00:24:56,500 --> 00:24:58,660
interesting at all to you and I'll leave

678
00:24:58,660 --> 00:25:01,090
you with this quote and open up for

679
00:25:01,090 --> 00:25:14,920
questions could you speak up I cannot

680
00:25:14,920 --> 00:25:19,420
really so that's that's what I'm saying

681
00:25:19,420 --> 00:25:23,500
we basically have to rely on that ETL

682
00:25:23,500 --> 00:25:25,120
that was in the middle so remember I was

683
00:25:25,120 --> 00:25:30,970
showing you the architecture we do but

684
00:25:30,970 --> 00:25:32,410
in a different way so the question is

685
00:25:32,410 --> 00:25:37,300
how do we deal with unstructured data so

686
00:25:37,300 --> 00:25:38,980
these are the architectures that I'm

687
00:25:38,980 --> 00:25:40,300
seeing so the way you deal with

688
00:25:40,300 --> 00:25:43,000
unstructured data is either here so your

689
00:25:43,000 --> 00:25:46,060
ETL basically puts some structure on

690
00:25:46,060 --> 00:25:49,180
that data right and that is when you

691
00:25:49,180 --> 00:25:51,310
know how to do it so you know that

692
00:25:51,310 --> 00:25:53,290
certain you know fields can be extracted

693
00:25:53,290 --> 00:25:55,690
in flight right away so then you extract

694
00:25:55,690 --> 00:25:57,550
those fields and you put it in directly

695
00:25:57,550 --> 00:25:59,260
into green plant database and you know

696
00:25:59,260 --> 00:26:02,080
in form of tables that's fine another

697
00:26:02,080 --> 00:26:03,910
way is you have your unstructured data

698
00:26:03,910 --> 00:26:07,090
at the HDFS level and these guys you

699
00:26:07,090 --> 00:26:08,290
know they're basically building

700
00:26:08,290 --> 00:26:11,080
exploratory data models right the data

701
00:26:11,080 --> 00:26:14,260
models that are constantly being tweaked

702
00:26:14,260 --> 00:26:16,930
but then they export it and make them

703
00:26:16,930 --> 00:26:19,120
available to a bigger audience within

704
00:26:19,120 --> 00:26:21,010
the enterprise by essentially again

705
00:26:21,010 --> 00:26:23,530
syncing up those data models as tables

706
00:26:23,530 --> 00:26:26,560
at the level of greenplum database so

707
00:26:26,560 --> 00:26:28,000
that then you can actually pick them up

708
00:26:28,000 --> 00:26:29,650
you know through MATLAB and you know all

709
00:26:29,650 --> 00:26:31,030
these other tools so that's how we deal

710
00:26:31,030 --> 00:26:33,780
with instruction later

711
00:26:35,490 --> 00:26:37,750
right and pxf you know one of the

712
00:26:37,750 --> 00:26:39,910
features that I was talking about

713
00:26:39,910 --> 00:26:41,680
integration wise is again in the

714
00:26:41,680 --> 00:26:43,450
direction of helping us deal with the

715
00:26:43,450 --> 00:26:45,940
unstructured data as quickly as possible

716
00:26:45,940 --> 00:26:52,690
so to speak yes right so there is a

717
00:26:52,690 --> 00:26:54,880
support so again I obviously couldn't go

718
00:26:54,880 --> 00:26:58,120
into great details about the

719
00:26:58,120 --> 00:27:00,430
architecture of the green plum but the

720
00:27:00,430 --> 00:27:02,050
question is do we support failover yes

721
00:27:02,050 --> 00:27:05,140
we do so you can have basically two

722
00:27:05,140 --> 00:27:06,370
masters you know in different

723
00:27:06,370 --> 00:27:12,160
configurations and yes you would

724
00:27:12,160 --> 00:27:14,050
basically have to note it's it's similar

725
00:27:14,050 --> 00:27:17,680
to how HDFS does AJ if you know that you

726
00:27:17,680 --> 00:27:22,270
know no it isn't right correct

727
00:27:22,270 --> 00:27:23,470
so again like I'm saying it's kind of

728
00:27:23,470 --> 00:27:25,390
like with HDFS where you have two named

729
00:27:25,390 --> 00:27:27,940
nodes you know that's the same approach

730
00:27:27,940 --> 00:27:36,220
yes so green plum is similar to situs DB

731
00:27:36,220 --> 00:27:39,490
so green plum and Silas take slightly

732
00:27:39,490 --> 00:27:43,060
different approaches to how to deal with

733
00:27:43,060 --> 00:27:46,140
progress and you get different sort of

734
00:27:46,140 --> 00:27:48,760
design constraints based on that so

735
00:27:48,760 --> 00:27:51,420
scientists made a decision to be

736
00:27:51,420 --> 00:27:54,220
essentially a plugin into a Postgres

737
00:27:54,220 --> 00:27:56,830
which makes it super easy to instantiate

738
00:27:56,830 --> 00:27:58,960
the situs cluster you basically just

739
00:27:58,960 --> 00:28:01,030
enable a plug-in on a bunch of progress

740
00:28:01,030 --> 00:28:02,440
nodes and you know you get a situs

741
00:28:02,440 --> 00:28:06,310
cluster but then on the flip side you're

742
00:28:06,310 --> 00:28:08,320
being constrained by whatever it is that

743
00:28:08,320 --> 00:28:10,330
is given to you by Postgres so for

744
00:28:10,330 --> 00:28:12,250
example green plum actually invests a

745
00:28:12,250 --> 00:28:14,860
lot in optimizing the inter interconnect

746
00:28:14,860 --> 00:28:17,380
ciders doesn't really have an ability to

747
00:28:17,380 --> 00:28:19,480
do those types of optimizations so it is

748
00:28:19,480 --> 00:28:21,700
similar but performance wise weight

749
00:28:21,700 --> 00:28:23,710
typically faster on this sort of the

750
00:28:23,710 --> 00:28:25,810
benchmarks that I've seen it doesn't

751
00:28:25,810 --> 00:28:27,190
mean that we're always faster because

752
00:28:27,190 --> 00:28:30,460
again depends on the workload but we do

753
00:28:30,460 --> 00:28:33,940
get more ability to optimize because we

754
00:28:33,940 --> 00:28:35,890
don't depend on Postgres as much as itis

755
00:28:35,890 --> 00:28:37,330
does so that would be the quickest way

756
00:28:37,330 --> 00:28:39,689
to answer

757
00:28:44,250 --> 00:28:53,089
[Applause]

