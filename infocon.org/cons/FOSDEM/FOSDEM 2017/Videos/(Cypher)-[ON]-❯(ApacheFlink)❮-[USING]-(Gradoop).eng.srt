1
00:00:23,200 --> 00:00:26,340
[Music]

2
00:04:01,300 --> 00:04:03,770
those are the craft dataflow systems

3
00:04:03,770 --> 00:04:05,870
select for example it's also craft

4
00:04:05,870 --> 00:04:07,520
possessing system but on top of a data

5
00:04:07,520 --> 00:04:09,830
flow system like Apache spark or Apache

6
00:04:09,830 --> 00:04:12,140
fling craft X for example on spark jelly

7
00:04:12,140 --> 00:04:15,290
on fling and now you see there's a yeah

8
00:04:15,290 --> 00:04:17,540
a free area in the right upper corner

9
00:04:17,540 --> 00:04:19,250
and this is where we position our

10
00:04:19,250 --> 00:04:20,988
research project project which is called

11
00:04:20,988 --> 00:04:24,020
Club and where we want to combine the

12
00:04:24,020 --> 00:04:26,030
the expressiveness of craft databases

13
00:04:26,030 --> 00:04:28,280
not as much as graph their betters but a

14
00:04:28,280 --> 00:04:30,440
little bit less but also the scalability

15
00:04:30,440 --> 00:04:31,850
so that's kind of the goal of the

16
00:04:31,850 --> 00:04:34,010
project and I will give you a short

17
00:04:34,010 --> 00:04:35,660
overview about the project before we

18
00:04:35,660 --> 00:04:37,900
dive into the site for implementation

19
00:04:37,900 --> 00:04:41,030
so we built on the shot nothing cluster

20
00:04:41,030 --> 00:04:43,130
like I already said so we use Hadoop and

21
00:04:43,130 --> 00:04:45,290
HDFS it's our data source data sink so

22
00:04:45,290 --> 00:04:47,930
that's where we store our crafts we use

23
00:04:47,930 --> 00:04:48,680
Apache flink

24
00:04:48,680 --> 00:04:50,900
as a distributed operator execution

25
00:04:50,900 --> 00:04:52,370
layer so this is where we implement

26
00:04:52,370 --> 00:04:54,740
actually our craft operators and cry

27
00:04:54,740 --> 00:04:56,360
doop itself is just a layer on top of

28
00:04:56,360 --> 00:04:59,060
Apache flink which uses the batch API of

29
00:04:59,060 --> 00:05:00,770
Apache fling or the stream API I will

30
00:05:00,770 --> 00:05:03,410
talk about this maybe later and Hadoop

31
00:05:03,410 --> 00:05:04,640
what it provides to you is an

32
00:05:04,640 --> 00:05:06,620
abstraction of property crafts on top of

33
00:05:06,620 --> 00:05:08,930
this data flow system so this is a data

34
00:05:08,930 --> 00:05:10,370
model that we implement I will talk

35
00:05:10,370 --> 00:05:13,880
about this on the next slides and we

36
00:05:13,880 --> 00:05:15,380
provide a set of graph data flow

37
00:05:15,380 --> 00:05:16,789
operator so one of them is for example

38
00:05:16,789 --> 00:05:18,830
pattern matching which we will talk

39
00:05:18,830 --> 00:05:22,220
about a detail today so but for those

40
00:05:22,220 --> 00:05:24,169
who don't know what fling is on the

41
00:05:24,169 --> 00:05:25,669
website I quote it's an open source

42
00:05:25,669 --> 00:05:27,530
streaming data flow engine and it

43
00:05:27,530 --> 00:05:28,990
provides to you data distribution

44
00:05:28,990 --> 00:05:30,919
communication and fault tolerance so all

45
00:05:30,919 --> 00:05:32,840
the hard parts of parallel and this is

46
00:05:32,840 --> 00:05:34,340
purely programming are handled by the

47
00:05:34,340 --> 00:05:36,140
system it does this really nice

48
00:05:36,140 --> 00:05:37,700
especially for the streaming use case

49
00:05:37,700 --> 00:05:39,590
but it also has a batch component which

50
00:05:39,590 --> 00:05:42,770
we are currently building on so what are

51
00:05:42,770 --> 00:05:44,360
the basic abstractions that you have in

52
00:05:44,360 --> 00:05:46,310
Apache flink so one is the data set a

53
00:05:46,310 --> 00:05:48,169
data set is just a disrepute collection

54
00:05:48,169 --> 00:05:50,659
of data objects like pojos so normally

55
00:05:50,659 --> 00:05:52,789
Java objects or tablets things like that

56
00:05:52,789 --> 00:05:54,660
and then you have transformed a

57
00:05:54,660 --> 00:05:56,640
and transformations are just operations

58
00:05:56,640 --> 00:05:58,440
on data set to produce another data set

59
00:05:58,440 --> 00:06:01,050
like for example map or its use so for

60
00:06:01,050 --> 00:06:02,250
example we have a data set here a

61
00:06:02,250 --> 00:06:04,140
transformation produces a new data set

62
00:06:04,140 --> 00:06:06,000
an effective flow program is just a

63
00:06:06,000 --> 00:06:07,950
composition of those transformations and

64
00:06:07,950 --> 00:06:10,050
this is the way how you program in these

65
00:06:10,050 --> 00:06:11,760
kind of systems so you have to you have

66
00:06:11,760 --> 00:06:14,190
to you get to translate your problem

67
00:06:14,190 --> 00:06:16,380
into a dataflow program using those

68
00:06:16,380 --> 00:06:17,910
abstractions that the dataflow framer

69
00:06:17,910 --> 00:06:20,490
provides to you so what flink does for

70
00:06:20,490 --> 00:06:22,830
you is the paralyzation so it scales out

71
00:06:22,830 --> 00:06:24,960
the computation and the data to multiple

72
00:06:24,960 --> 00:06:26,730
machines computes it and gives you in

73
00:06:26,730 --> 00:06:29,400
result so what are those abstractions I

74
00:06:29,400 --> 00:06:30,900
guess it's a bit small here

75
00:06:30,900 --> 00:06:33,120
it's just categorized in so this is

76
00:06:33,120 --> 00:06:34,980
fleeing data set transformations unirii

77
00:06:34,980 --> 00:06:37,080
so which means one data set is input and

78
00:06:37,080 --> 00:06:39,210
binary two data sets and you have the

79
00:06:39,210 --> 00:06:41,880
common suspects like map and reduce and

80
00:06:41,880 --> 00:06:44,370
these SQL like things like filter crew

81
00:06:44,370 --> 00:06:46,740
buy blah blah blah and we have the

82
00:06:46,740 --> 00:06:49,320
binary stuff like join for example for a

83
00:06:49,320 --> 00:06:51,420
union coke group and there are special

84
00:06:51,420 --> 00:06:53,640
operators for iterations so like Delta

85
00:06:53,640 --> 00:06:56,160
iteration or by iteration which allow

86
00:06:56,160 --> 00:06:58,410
you to express your iterative algorithms

87
00:06:58,410 --> 00:06:59,820
and most graph algorithms and machine

88
00:06:59,820 --> 00:07:02,190
learning algorithms are iterative to

89
00:07:02,190 --> 00:07:04,550
Express on this framework we also use it

90
00:07:04,550 --> 00:07:06,480
Sokka do on the other hand like I

91
00:07:06,480 --> 00:07:08,550
already said is an abstraction on top of

92
00:07:08,550 --> 00:07:11,430
Apache Fling which provides you not the

93
00:07:11,430 --> 00:07:13,800
data set API you have it underneath but

94
00:07:13,800 --> 00:07:15,600
it provides you already a property craft

95
00:07:15,600 --> 00:07:18,090
abstraction on top of link plus several

96
00:07:18,090 --> 00:07:22,290
operators so the basic data model that

97
00:07:22,290 --> 00:07:25,800
we that we have is a property craft so

98
00:07:25,800 --> 00:07:27,510
we have notices and directed edges and

99
00:07:27,510 --> 00:07:29,220
we have this concept of so-called

100
00:07:29,220 --> 00:07:31,110
logical crafts so we have not only one

101
00:07:31,110 --> 00:07:33,000
craft but you can have multiple crafts

102
00:07:33,000 --> 00:07:34,530
inside your data base which can also

103
00:07:34,530 --> 00:07:36,600
overlap and the concept here is that

104
00:07:36,600 --> 00:07:38,190
these crafts are also first-class

105
00:07:38,190 --> 00:07:40,230
citizens like borders and edges and they

106
00:07:40,230 --> 00:07:42,390
can be input and output of operators of

107
00:07:42,390 --> 00:07:45,660
transformations so this is why they have

108
00:07:45,660 --> 00:07:47,910
also identifier so uniquely identified

109
00:07:47,910 --> 00:07:50,370
in the system you have type labels

110
00:07:50,370 --> 00:07:51,630
maybe you can't read it but it's like

111
00:07:51,630 --> 00:07:53,760
this is an orc this is a clan this is an

112
00:07:53,760 --> 00:07:55,650
area they live in its Mordor so it has

113
00:07:55,650 --> 00:07:58,080
attributes and here we have to hop it so

114
00:07:58,080 --> 00:08:00,150
we have attributes everywhere and labels

115
00:08:00,150 --> 00:08:02,880
and you can you can all you can just

116
00:08:02,880 --> 00:08:04,229
load this into the system the

117
00:08:04,229 --> 00:08:05,400
abstractions already there you don't

118
00:08:05,400 --> 00:08:06,960
have to care about how this is mapped to

119
00:08:06,960 --> 00:08:08,470
the underlying data set API

120
00:08:08,470 --> 00:08:10,600
so this is what kadu provides you from

121
00:08:10,600 --> 00:08:12,670
the data model point of view and it's

122
00:08:12,670 --> 00:08:14,170
called the extended property craft model

123
00:08:14,170 --> 00:08:15,820
because of these logical crafts that you

124
00:08:15,820 --> 00:08:17,320
have additionally to the regular

125
00:08:17,320 --> 00:08:19,240
property craft model like a new você for

126
00:08:19,240 --> 00:08:22,060
example so in transformations that we

127
00:08:22,060 --> 00:08:25,090
provide are not only not are not based

128
00:08:25,090 --> 00:08:27,010
on data sets but on logical crafts as

129
00:08:27,010 --> 00:08:29,050
input so a single craft or a collection

130
00:08:29,050 --> 00:08:31,090
of crafts as input we also have you know

131
00:08:31,090 --> 00:08:33,520
en binary operators so for example what

132
00:08:33,520 --> 00:08:35,020
we have here is pattern matching which

133
00:08:35,020 --> 00:08:37,330
we have a focus on today and other

134
00:08:37,330 --> 00:08:39,070
things like grouping it's an oil AP

135
00:08:39,070 --> 00:08:41,590
aircraft or a P operator subcraft for

136
00:08:41,590 --> 00:08:44,080
example binary operations for example to

137
00:08:44,080 --> 00:08:46,540
combine two crafts to a new craft or to

138
00:08:46,540 --> 00:08:48,340
compute the overlap between two crafts

139
00:08:48,340 --> 00:08:52,750
or the exclusion also craft collection

140
00:08:52,750 --> 00:08:54,700
operators they are kind of relational so

141
00:08:54,700 --> 00:08:56,680
you have a collection of a large

142
00:08:56,680 --> 00:08:58,180
collection of crafts and you want to for

143
00:08:58,180 --> 00:09:00,100
example filter them so pi predicates

144
00:09:00,100 --> 00:09:02,680
like selection in relational algebra or

145
00:09:02,680 --> 00:09:04,720
also pattern matching on each of these

146
00:09:04,720 --> 00:09:06,910
crafts and they're also these

147
00:09:06,910 --> 00:09:09,160
theoretical operators like union of

148
00:09:09,160 --> 00:09:10,750
collections or intersection difference

149
00:09:10,750 --> 00:09:14,080
and so on so I wanna give you just two

150
00:09:14,080 --> 00:09:16,120
examples how these craft data flows look

151
00:09:16,120 --> 00:09:18,460
like using the subcraft operator so the

152
00:09:18,460 --> 00:09:20,980
input here is a logical graph we have

153
00:09:20,980 --> 00:09:22,839
different kinds of labeled so the colors

154
00:09:22,839 --> 00:09:24,730
are the labels on these loaded Snatchers

155
00:09:24,730 --> 00:09:27,130
and what you do as a programmer you just

156
00:09:27,130 --> 00:09:28,720
say okay I have malaria graph I load it

157
00:09:28,720 --> 00:09:31,270
from HDFS it's just a method here but

158
00:09:31,270 --> 00:09:33,430
there's a bit more happening and you say

159
00:09:33,430 --> 00:09:35,650
something ok my craft got subcraft and

160
00:09:35,650 --> 00:09:37,180
then you give two predicates so it's two

161
00:09:37,180 --> 00:09:38,830
lambdas which are just filtering

162
00:09:38,830 --> 00:09:41,050
functions for billets and edges so what

163
00:09:41,050 --> 00:09:43,600
we say here is a vertex is in the output

164
00:09:43,600 --> 00:09:45,640
graph it has a green label and an edges

165
00:09:45,640 --> 00:09:47,170
in the output craft if it has an orange

166
00:09:47,170 --> 00:09:49,300
label and of course both source and

167
00:09:49,300 --> 00:09:50,860
target vertex of the edge have to be

168
00:09:50,860 --> 00:09:55,240
will fill this vertex predicate so in

169
00:09:55,240 --> 00:09:56,500
this example you reside would look like

170
00:09:56,500 --> 00:09:56,950
this

171
00:09:56,950 --> 00:09:59,770
we have only those notices that are that

172
00:09:59,770 --> 00:10:01,240
fulfill the predicates and the edges

173
00:10:01,240 --> 00:10:02,920
between them that fulfill the predicates

174
00:10:02,920 --> 00:10:04,810
so on the output is again a lot sugar

175
00:10:04,810 --> 00:10:06,670
Kraft which again can be input for a

176
00:10:06,670 --> 00:10:08,470
number operator so this is the data flow

177
00:10:08,470 --> 00:10:10,480
on the Kraft level that's what era would

178
00:10:10,480 --> 00:10:12,430
gives you and what it also gives you a

179
00:10:12,430 --> 00:10:13,900
speller matching so you have again a

180
00:10:13,900 --> 00:10:16,030
logical Kraft is input then you define a

181
00:10:16,030 --> 00:10:17,620
pattern so for example we are looking

182
00:10:17,620 --> 00:10:20,800
for sub crafts that start from a

183
00:10:20,800 --> 00:10:21,889
Greenville takes with

184
00:10:21,889 --> 00:10:24,199
an orange edge to an orange vortex and

185
00:10:24,199 --> 00:10:25,819
then you have just the craft here you

186
00:10:25,819 --> 00:10:27,709
say Mitch you define a site for paedon

187
00:10:27,709 --> 00:10:30,049
it's just green orange orange and then

188
00:10:30,049 --> 00:10:31,970
the result is a craft collection in that

189
00:10:31,970 --> 00:10:34,220
case which contains all the SAP crafts

190
00:10:34,220 --> 00:10:35,899
that match this pattern and the sub

191
00:10:35,899 --> 00:10:37,489
crafts are of course our class of this

192
00:10:37,489 --> 00:10:39,619
input craft so this is from a high level

193
00:10:39,619 --> 00:10:42,439
point of view how Trudeau works and now

194
00:10:42,439 --> 00:10:44,389
we will focus on really a petal matching

195
00:10:44,389 --> 00:10:46,069
so how we how we do this because it's

196
00:10:46,069 --> 00:10:47,299
just an example right now we have

197
00:10:47,299 --> 00:10:49,429
multiple operators but this is a very

198
00:10:49,429 --> 00:10:52,069
interesting one so I will give you as an

199
00:10:52,069 --> 00:10:53,779
overview and then max will give you more

200
00:10:53,779 --> 00:10:56,899
details about the implementation so the

201
00:10:56,899 --> 00:10:59,389
steps are really similar to every

202
00:10:59,389 --> 00:11:01,309
clarity processing system so you have

203
00:11:01,309 --> 00:11:02,689
two queries this is the query from the

204
00:11:02,689 --> 00:11:05,839
beginning we pass the query at first so

205
00:11:05,839 --> 00:11:08,029
we have written our own art la crema we

206
00:11:08,029 --> 00:11:10,669
could switch to to open cipher the

207
00:11:10,669 --> 00:11:12,049
problem here is that open cipher doesn't

208
00:11:12,049 --> 00:11:13,369
support the extended property craft

209
00:11:13,369 --> 00:11:14,899
model but it's just the implementation

210
00:11:14,899 --> 00:11:17,389
detail so then we translate this into

211
00:11:17,389 --> 00:11:20,029
object representation of this query we

212
00:11:20,029 --> 00:11:21,350
translate the predicates to a

213
00:11:21,350 --> 00:11:23,029
contractive normal form because it's

214
00:11:23,029 --> 00:11:24,649
easier to evaluate them in that case

215
00:11:24,649 --> 00:11:27,589
then we use statistics about for example

216
00:11:27,589 --> 00:11:29,089
how many red labels are in the search

217
00:11:29,089 --> 00:11:31,699
graph how many blue etches on the search

218
00:11:31,699 --> 00:11:33,410
craft there are more statistics and then

219
00:11:33,410 --> 00:11:36,169
we use this we build a Coast based query

220
00:11:36,169 --> 00:11:38,449
optimizer that takes the career this

221
00:11:38,449 --> 00:11:39,919
representation the statistics and

222
00:11:39,919 --> 00:11:43,189
produces a query plan and then using

223
00:11:43,189 --> 00:11:45,949
this very plan every note represents a

224
00:11:45,949 --> 00:11:48,739
physical operator which met again to

225
00:11:48,739 --> 00:11:51,529
flink transformations and the output if

226
00:11:51,529 --> 00:11:53,569
you execute this is then the collection

227
00:11:53,569 --> 00:11:55,970
of found patterns in the graph and now

228
00:11:55,970 --> 00:11:57,619
max will tell you a bit more about

229
00:11:57,619 --> 00:12:00,110
what's happening after this part so we

230
00:12:00,110 --> 00:12:01,999
now have generated this plan and then we

231
00:12:01,999 --> 00:12:04,600
execute this

232
00:12:17,450 --> 00:12:21,260
yes so as Martin said after we have

233
00:12:21,260 --> 00:12:24,790
parsed and plant post and planting Emery

234
00:12:24,790 --> 00:12:28,130
we end up with something like this this

235
00:12:28,130 --> 00:12:31,040
is the crabby plan for this particular

236
00:12:31,040 --> 00:12:33,710
query and just to give you a short

237
00:12:33,710 --> 00:12:36,140
overview of what this actually means so

238
00:12:36,140 --> 00:12:37,970
this is actually a tree maybe it doesn't

239
00:12:37,970 --> 00:12:40,180
look like this in this kind of

240
00:12:40,180 --> 00:12:45,500
visualization but to execute it we

241
00:12:45,500 --> 00:12:47,600
basically start with the leaf node so

242
00:12:47,600 --> 00:12:50,410
for example if we want to find all

243
00:12:50,410 --> 00:12:53,270
relationships that match this sub

244
00:12:53,270 --> 00:12:55,040
pattern so we want to find all the

245
00:12:55,040 --> 00:12:57,350
relationships that have the type hate

246
00:12:57,350 --> 00:13:01,970
and so what we basically do is we filter

247
00:13:01,970 --> 00:13:06,830
and project the the collection of all

248
00:13:06,830 --> 00:13:09,340
relationships we have in our graph and

249
00:13:09,340 --> 00:13:12,800
after we've done this for all atomic

250
00:13:12,800 --> 00:13:16,970
patterns we start kind of building up

251
00:13:16,970 --> 00:13:18,920
larger patterns and we do this by

252
00:13:18,920 --> 00:13:22,850
joining so for example to get this sub

253
00:13:22,850 --> 00:13:26,930
pattern we join all nodes with the label

254
00:13:26,930 --> 00:13:32,450
or with a brief label or together with

255
00:13:32,450 --> 00:13:35,810
all relationships with type hates and so

256
00:13:35,810 --> 00:13:39,230
we continuously build up larger patterns

257
00:13:39,230 --> 00:13:43,730
by joining together smaller patterns and

258
00:13:43,730 --> 00:13:45,620
yet this is for example this is the last

259
00:13:45,620 --> 00:13:47,330
operator in this plant we do a filter

260
00:13:47,330 --> 00:13:51,650
step where we filter out all non-valid

261
00:13:51,650 --> 00:13:53,960
results according to a given predicate

262
00:13:53,960 --> 00:13:57,670
which is in this case making sure that

263
00:13:57,670 --> 00:14:00,230
port 1 and or two aren't the same and

264
00:14:00,230 --> 00:14:02,540
clan 1 and plenty aren't the same so we

265
00:14:02,540 --> 00:14:05,510
throw away all embeddings that there are

266
00:14:05,510 --> 00:14:08,060
all results that don't hold for this

267
00:14:08,060 --> 00:14:11,360
pattern I slightly special operator is

268
00:14:11,360 --> 00:14:13,310
the expanding operator this is a

269
00:14:13,310 --> 00:14:15,920
traversal operator so in this operator

270
00:14:15,920 --> 00:14:19,280
you don't only expand one hop but you

271
00:14:19,280 --> 00:14:22,010
can expand multiple hops using one

272
00:14:22,010 --> 00:14:25,010
operator this is the star thing is C or

273
00:14:25,010 --> 00:14:27,230
might not see over here so in this case

274
00:14:27,230 --> 00:14:30,230
we specify want to step at least or do

275
00:14:30,230 --> 00:14:31,100
at least one hop

276
00:14:31,100 --> 00:14:35,630
but at most 10 yes and just to walk you

277
00:14:35,630 --> 00:14:38,480
through some ixora swards internally and

278
00:14:38,480 --> 00:14:44,530
how these maps took link so oh yeah big

279
00:14:44,530 --> 00:14:48,290
every of these operators produces

280
00:14:48,290 --> 00:14:50,210
results intermediate results or the end

281
00:14:50,210 --> 00:14:52,610
result of course and so what we did is

282
00:14:52,610 --> 00:14:54,920
we implement the two main we have two

283
00:14:54,920 --> 00:14:57,140
main data structures for once this is

284
00:14:57,140 --> 00:15:00,710
the embedding the embedding stories one

285
00:15:00,710 --> 00:15:03,790
particular intermediate result or the

286
00:15:03,790 --> 00:15:07,880
end result so it has two types of three

287
00:15:07,880 --> 00:15:10,640
types of entries one is the list of ID

288
00:15:10,640 --> 00:15:13,210
entries this stores the mapping of

289
00:15:13,210 --> 00:15:17,980
actual nodes or vertices in our graph to

290
00:15:17,980 --> 00:15:21,440
note so edges in the query graph then we

291
00:15:21,440 --> 00:15:23,630
have a list of properties so every node

292
00:15:23,630 --> 00:15:25,280
as you know in the extended property

293
00:15:25,280 --> 00:15:27,980
graph model every node an edge and even

294
00:15:27,980 --> 00:15:30,080
graphs but that's not relevant here have

295
00:15:30,080 --> 00:15:33,380
properties like names or like Frodo has

296
00:15:33,380 --> 00:15:35,060
a name or like every hobbit has a name

297
00:15:35,060 --> 00:15:37,100
and we store this in a separate list and

298
00:15:37,100 --> 00:15:39,350
then we have paths this is just an

299
00:15:39,350 --> 00:15:41,660
implementation detail to make sure that

300
00:15:41,660 --> 00:15:43,730
if we do this variable length expand

301
00:15:43,730 --> 00:15:46,760
that we always end up with embeddings of

302
00:15:46,760 --> 00:15:48,950
the same size or with the same amount of

303
00:15:48,950 --> 00:15:51,560
entries but it's not too important for

304
00:15:51,560 --> 00:15:56,690
now to say these embeddings they're kind

305
00:15:56,690 --> 00:15:58,730
of dumb they don't know they only store

306
00:15:58,730 --> 00:16:00,110
things but they don't know what they

307
00:16:00,110 --> 00:16:02,570
actually store and to make sure we have

308
00:16:02,570 --> 00:16:06,710
a mapping we actually store this

309
00:16:06,710 --> 00:16:08,420
knowledge as well we have also embedding

310
00:16:08,420 --> 00:16:13,130
metadata the reason we split these

311
00:16:13,130 --> 00:16:17,090
things is that since we are in a

312
00:16:17,090 --> 00:16:18,680
distributed environment we eventually

313
00:16:18,680 --> 00:16:20,810
have to send data over the network

314
00:16:20,810 --> 00:16:23,090
this happens usually when we do a join

315
00:16:23,090 --> 00:16:26,600
and that's quite expensive compared to

316
00:16:26,600 --> 00:16:30,050
just accessing RAM or something like

317
00:16:30,050 --> 00:16:32,090
this and so we have to make sure that

318
00:16:32,090 --> 00:16:34,100
our data structures we actually send our

319
00:16:34,100 --> 00:16:37,190
network are as small as possible and

320
00:16:37,190 --> 00:16:40,010
since these metadata information is the

321
00:16:40,010 --> 00:16:42,230
same for every intermediate result for

322
00:16:42,230 --> 00:16:44,390
every step we can just have one metadata

323
00:16:44,390 --> 00:16:45,800
object

324
00:16:45,800 --> 00:16:51,500
for every note in the carry plan and

325
00:16:51,500 --> 00:16:52,880
this basically stores the mapping of

326
00:16:52,880 --> 00:16:55,220
which were available in our travel graph

327
00:16:55,220 --> 00:16:57,560
maps to which entry in the embedding and

328
00:16:57,560 --> 00:17:03,770
the same goes for properties so to have

329
00:17:03,770 --> 00:17:05,300
some examples how this actually works

330
00:17:05,300 --> 00:17:07,369
under the hood so we have three layers

331
00:17:07,369 --> 00:17:09,559
here this is like our example there this

332
00:17:09,559 --> 00:17:11,540
is the actually data we have our orts

333
00:17:11,540 --> 00:17:14,030
then we have a more or less relational

334
00:17:14,030 --> 00:17:18,589
view so kind of what we do on internally

335
00:17:18,589 --> 00:17:23,329
is we map things to relations and then

336
00:17:23,329 --> 00:17:26,959
we you have the the same thing as the

337
00:17:26,959 --> 00:17:29,929
flink view or how our physical operators

338
00:17:29,929 --> 00:17:34,510
in the end map to flink operators so

339
00:17:34,510 --> 00:17:37,100
this is an example for a leaf node so we

340
00:17:37,100 --> 00:17:40,940
want to find all nodes that are a hobbit

341
00:17:40,940 --> 00:17:42,350
and have the name Frodo Baggins

342
00:17:42,350 --> 00:17:46,940
so we scan through all our through all

343
00:17:46,940 --> 00:17:50,360
our nodes in the graph filter out these

344
00:17:50,360 --> 00:17:54,050
know these nodes which have the

345
00:17:54,050 --> 00:17:55,640
according label and the according name

346
00:17:55,640 --> 00:17:59,330
and what we end up is with a couple of

347
00:17:59,330 --> 00:18:02,780
or maybe just one node and all its

348
00:18:02,780 --> 00:18:05,630
properties and this can in a relational

349
00:18:05,630 --> 00:18:10,370
view this would be a table with the ID

350
00:18:10,370 --> 00:18:12,500
and the properties of every hobbit named

351
00:18:12,500 --> 00:18:14,410
frodo baggins

352
00:18:14,410 --> 00:18:17,660
SI follow up step we do a project step

353
00:18:17,660 --> 00:18:20,210
because we want to again save some data

354
00:18:20,210 --> 00:18:23,360
size we throw away all properties we

355
00:18:23,360 --> 00:18:25,850
don't need anymore because why why would

356
00:18:25,850 --> 00:18:27,440
we carry them if you never access them

357
00:18:27,440 --> 00:18:30,290
again in this case we are not really

358
00:18:30,290 --> 00:18:32,900
interested in in any property of Frodo

359
00:18:32,900 --> 00:18:36,140
so we throw out everything in flink this

360
00:18:36,140 --> 00:18:39,670
maps to a flat net function flat map

361
00:18:39,670 --> 00:18:42,710
allows us to combine these two steps

362
00:18:42,710 --> 00:18:46,040
into one this is some internal thing of

363
00:18:46,040 --> 00:18:48,140
Fling why why that's better instead we

364
00:18:48,140 --> 00:18:53,350
could also do a a map or a filter and a

365
00:18:53,350 --> 00:18:57,870
follow up map step but this words

366
00:18:57,870 --> 00:19:00,830
cost us more would need more time so

367
00:19:00,830 --> 00:19:04,800
flatmap is basically you have input UF

368
00:19:04,800 --> 00:19:07,500
as input you have one vertex or in this

369
00:19:07,500 --> 00:19:10,980
case a vertex and you can output 1 or 0

370
00:19:10,980 --> 00:19:13,020
or even more but now that's not the case

371
00:19:13,020 --> 00:19:16,140
for us you can output a variable amount

372
00:19:16,140 --> 00:19:23,370
of result results basically so this is

373
00:19:23,370 --> 00:19:26,580
leaf node operator that project and

374
00:19:26,580 --> 00:19:29,910
filter embeddings or vertices this is

375
00:19:29,910 --> 00:19:31,440
always the start operator and a great

376
00:19:31,440 --> 00:19:34,559
plan then you have the maybe most

377
00:19:34,559 --> 00:19:37,230
important operator it's a join so this

378
00:19:37,230 --> 00:19:41,850
is to join the sub patterns to larger

379
00:19:41,850 --> 00:19:44,070
patterns so for example you have this

380
00:19:44,070 --> 00:19:45,950
pattern you have the org which is

381
00:19:45,950 --> 00:19:48,300
related to a clan and you have an ORAC

382
00:19:48,300 --> 00:19:50,370
which aids another org and you basically

383
00:19:50,370 --> 00:19:52,410
want to join this together to become a

384
00:19:52,410 --> 00:19:55,830
larger pattern in relation to view this

385
00:19:55,830 --> 00:19:59,940
is the same as for relational databases

386
00:19:59,940 --> 00:20:02,610
so you have two tables basically in our

387
00:20:02,610 --> 00:20:04,590
case it's two intermediate results and

388
00:20:04,590 --> 00:20:09,750
you join them together we can only just

389
00:20:09,750 --> 00:20:11,040
join them because we have some

390
00:20:11,040 --> 00:20:12,840
constraints this is due to how data

391
00:20:12,840 --> 00:20:17,250
graph databases work there you have to

392
00:20:17,250 --> 00:20:19,740
basically check that you don't assign

393
00:20:19,740 --> 00:20:21,780
the same node or the same relationship

394
00:20:21,780 --> 00:20:23,850
to different nodes in your career graph

395
00:20:23,850 --> 00:20:26,460
so you don't want to be do this org and

396
00:20:26,460 --> 00:20:29,490
this org to be the same or you don't

397
00:20:29,490 --> 00:20:30,960
want this relationship and this

398
00:20:30,960 --> 00:20:32,730
relationship in this case they would

399
00:20:32,730 --> 00:20:34,110
never be the same but in other cases

400
00:20:34,110 --> 00:20:36,270
they could be the same you don't want to

401
00:20:36,270 --> 00:20:39,510
traverse the same relationship twice at

402
00:20:39,510 --> 00:20:41,670
least depending on your settings but

403
00:20:41,670 --> 00:20:43,679
it's out of scope for this talk maybe

404
00:20:43,679 --> 00:20:50,490
and so we take those two sub graphs and

405
00:20:50,490 --> 00:20:52,740
join them together and get one larger

406
00:20:52,740 --> 00:20:56,460
graph you can see that in the example we

407
00:20:56,460 --> 00:20:58,890
don't store this note twice because why

408
00:20:58,890 --> 00:21:01,200
would we would be a waste of space

409
00:21:01,200 --> 00:21:04,559
and again this maps to this time a flat

410
00:21:04,559 --> 00:21:07,920
joins on the flat map because as I just

411
00:21:07,920 --> 00:21:09,960
said some some intermediate results

412
00:21:09,960 --> 00:21:11,620
wouldn't be valid on the

413
00:21:11,620 --> 00:21:15,360
the assumption of isomorphism or this

414
00:21:15,360 --> 00:21:22,709
relationship or no to uniqueness so

415
00:21:23,309 --> 00:21:26,650
maybe the most interesting operator is

416
00:21:26,650 --> 00:21:29,170
the reliable and expand this is

417
00:21:29,170 --> 00:21:33,070
basically for in our example this is

418
00:21:33,070 --> 00:21:34,960
would be we have an org and we want to

419
00:21:34,960 --> 00:21:38,380
find Frodo Baggins and in one to ten

420
00:21:38,380 --> 00:21:41,140
steps and for this what we do is we use

421
00:21:41,140 --> 00:21:44,140
a iterative expand under the hood this

422
00:21:44,140 --> 00:21:47,020
is basically a just a iterative join

423
00:21:47,020 --> 00:21:50,170
with some here you can do some

424
00:21:50,170 --> 00:21:53,530
optimizations which hopefully help you

425
00:21:53,530 --> 00:21:57,940
making it faster so what we do is we

426
00:21:57,940 --> 00:21:59,410
have to let we have the left side which

427
00:21:59,410 --> 00:22:01,330
is only the orc and then we have a set

428
00:22:01,330 --> 00:22:06,340
of nodes relationships and their and

429
00:22:06,340 --> 00:22:13,570
their notes and then we kind of join

430
00:22:13,570 --> 00:22:16,710
them iterate iterate iteratively and

431
00:22:16,710 --> 00:22:20,170
yield every possible result of course if

432
00:22:20,170 --> 00:22:22,690
it's longer than one mob and shorter

433
00:22:22,690 --> 00:22:24,730
than ten hops so it's not that we only

434
00:22:24,730 --> 00:22:27,280
yield the largest result but we yield

435
00:22:27,280 --> 00:22:32,110
all intermediate results as well this is

436
00:22:32,110 --> 00:22:33,970
what Martin mentioned before we here we

437
00:22:33,970 --> 00:22:35,740
use the bollocky to raishin this is kind

438
00:22:35,740 --> 00:22:38,710
of a cool thing for your streaming

439
00:22:38,710 --> 00:22:40,750
processing systems spark for example

440
00:22:40,750 --> 00:22:46,170
hasn't doesn't have a iterative operator

441
00:22:46,170 --> 00:22:51,010
flinged us and so we use this iteration

442
00:22:51,010 --> 00:22:54,250
operator to iterate over the graph and

443
00:22:54,250 --> 00:22:57,490
produce our results and that's the last

444
00:22:57,490 --> 00:23:02,550
step we'll do filtering so we had this

445
00:23:02,550 --> 00:23:06,010
we had this predicate that what one and

446
00:23:06,010 --> 00:23:07,870
talked to are not allowed to be the same

447
00:23:07,870 --> 00:23:12,870
so obviously we have to filter out this

448
00:23:12,900 --> 00:23:17,020
result and that's quite straightforward

449
00:23:17,020 --> 00:23:20,800
and this maps to the filter operator

450
00:23:20,800 --> 00:23:22,750
which is already present in flink so

451
00:23:22,750 --> 00:23:25,770
that's quite straightforward

452
00:23:25,990 --> 00:23:29,410
that's basically the inter the operators

453
00:23:29,410 --> 00:23:33,220
we have implemented so far which allow

454
00:23:33,220 --> 00:23:36,280
us to run most Sai factories right now

455
00:23:36,280 --> 00:23:38,650
we what we don't have at the moment is a

456
00:23:38,650 --> 00:23:40,420
return statement this is kind of work in

457
00:23:40,420 --> 00:23:43,059
progress but we will eventually have it

458
00:23:43,059 --> 00:23:49,390
and there is some caveats on how to map

459
00:23:49,390 --> 00:23:53,920
the the projected results to a graph

460
00:23:53,920 --> 00:23:55,840
again that's why we don't have it right

461
00:23:55,840 --> 00:23:57,610
now and what we also don't have is

462
00:23:57,610 --> 00:23:59,620
aggregation but you can do aggregation

463
00:23:59,620 --> 00:24:03,990
by just adding I mean module itself has

464
00:24:03,990 --> 00:24:06,070
operators for aggregation so what you

465
00:24:06,070 --> 00:24:07,690
could do is you do the pattern matching

466
00:24:07,690 --> 00:24:10,000
without aggregation and do the

467
00:24:10,000 --> 00:24:14,530
aggregation as a follow-up so but most

468
00:24:14,530 --> 00:24:16,679
of the rest of the ciphers standard we

469
00:24:16,679 --> 00:24:24,059
support right now so now that we have

470
00:24:24,059 --> 00:24:27,220
planned our plan or planned our query

471
00:24:27,220 --> 00:24:30,910
this is what we end up with for flink so

472
00:24:30,910 --> 00:24:32,530
this is the a visualization of what

473
00:24:32,530 --> 00:24:35,410
flink generates so we have our we have

474
00:24:35,410 --> 00:24:37,630
our travel plan then we translate it to

475
00:24:37,630 --> 00:24:39,670
a fling query plan and this is for this

476
00:24:39,670 --> 00:24:41,410
particular query this would be the Flint

477
00:24:41,410 --> 00:24:43,270
query plan you don't see much here

478
00:24:43,270 --> 00:24:44,950
that's completely ok we just wanted to

479
00:24:44,950 --> 00:24:46,960
show you that it's quite a lot of steps

480
00:24:46,960 --> 00:24:49,540
you have to take in order to produce the

481
00:24:49,540 --> 00:24:52,840
result and this is just a more detailed

482
00:24:52,840 --> 00:24:56,440
view so yeah if you have basically your

483
00:24:56,440 --> 00:25:00,940
data source then you do a set of filter

484
00:25:00,940 --> 00:25:02,860
and projects so you get all you all the

485
00:25:02,860 --> 00:25:04,570
vertices and edges you want to work with

486
00:25:04,570 --> 00:25:07,679
and then you join them over and over and

487
00:25:07,679 --> 00:25:12,160
in the end you end up with one data set

488
00:25:12,160 --> 00:25:15,190
which contains your results and then you

489
00:25:15,190 --> 00:25:17,380
can for example write them into HDFS or

490
00:25:17,380 --> 00:25:21,990
wherever you want to store them so yeah

491
00:25:21,990 --> 00:25:24,820
what we want to do what we would like to

492
00:25:24,820 --> 00:25:29,500
do is that we want to integrate the

493
00:25:29,500 --> 00:25:32,710
cipher cipher technology complete

494
00:25:32,710 --> 00:25:36,340
compatibility toolkit this is a set of

495
00:25:36,340 --> 00:25:37,690
cucumber tests

496
00:25:37,690 --> 00:25:39,700
allow you if you implement them they

497
00:25:39,700 --> 00:25:41,769
tell you if you have implemented open

498
00:25:41,769 --> 00:25:43,049
cypher correctly

499
00:25:43,049 --> 00:25:47,080
hopefully we haven't really done any

500
00:25:47,080 --> 00:25:49,659
like broader checks now if what we

501
00:25:49,659 --> 00:25:52,000
produce is correct so this would be

502
00:25:52,000 --> 00:25:55,899
perfect we want to do benchmarking we

503
00:25:55,899 --> 00:25:59,250
have some preliminary preliminary

504
00:25:59,250 --> 00:26:04,029
evaluations but we should go into more

505
00:26:04,029 --> 00:26:05,889
detail for this and for this we want to

506
00:26:05,889 --> 00:26:09,250
use the L DBC social network benchmark

507
00:26:09,250 --> 00:26:10,539
this is a benchmark specifically

508
00:26:10,539 --> 00:26:13,320
designed for graph databases or graph

509
00:26:13,320 --> 00:26:17,409
systems and we can do a couple of

510
00:26:17,409 --> 00:26:19,120
optimizations so right now we use a

511
00:26:19,120 --> 00:26:21,610
greedy planner we could use a dynamic

512
00:26:21,610 --> 00:26:23,919
programming planner hopefully yielding

513
00:26:23,919 --> 00:26:24,220
better

514
00:26:24,220 --> 00:26:28,139
Travie plans and faster execution times

515
00:26:28,139 --> 00:26:31,090
we want to improve our cost models so in

516
00:26:31,090 --> 00:26:33,580
order to be able to produce a plan you

517
00:26:33,580 --> 00:26:35,799
have to estimate how costly each step in

518
00:26:35,799 --> 00:26:37,840
the plan would be right now we have a

519
00:26:37,840 --> 00:26:41,110
very basic very simple model based on

520
00:26:41,110 --> 00:26:43,269
probabilities but this doesn't scale at

521
00:26:43,269 --> 00:26:46,480
leap here not really if you have this

522
00:26:46,480 --> 00:26:50,529
amount of joints and yeah you can do

523
00:26:50,529 --> 00:26:52,750
reuse of intermediate steps so as you

524
00:26:52,750 --> 00:26:55,629
see we for every like or ik we do we

525
00:26:55,629 --> 00:26:57,279
have two orcs in our pattern right now

526
00:26:57,279 --> 00:26:59,860
for every order we would filter the list

527
00:26:59,860 --> 00:27:02,590
of all nodes in our graph but actually

528
00:27:02,590 --> 00:27:04,809
that's the same I mean the since we only

529
00:27:04,809 --> 00:27:08,679
filter for the enable org we yield the

530
00:27:08,679 --> 00:27:10,179
same result on both sides so we could

531
00:27:10,179 --> 00:27:12,610
reuse it basically what we don't do yet

532
00:27:12,610 --> 00:27:16,299
and another problem in the distributed

533
00:27:16,299 --> 00:27:18,879
environment is partitioning so if you

534
00:27:18,879 --> 00:27:22,269
have vertices which are with a high out

535
00:27:22,269 --> 00:27:24,370
degree or in degree so in a social

536
00:27:24,370 --> 00:27:25,929
network for example people with many

537
00:27:25,929 --> 00:27:30,039
many friends they increase the computing

538
00:27:30,039 --> 00:27:33,250
time on the particular node which is in

539
00:27:33,250 --> 00:27:35,529
charge for you for handling this node

540
00:27:35,529 --> 00:27:37,860
they increase the computing time

541
00:27:37,860 --> 00:27:40,659
unevenly to every other node in the

542
00:27:40,659 --> 00:27:42,429
cluster for example and so we have data

543
00:27:42,429 --> 00:27:46,119
skew which means that some workers take

544
00:27:46,119 --> 00:27:48,639
way more time to process a resort to

545
00:27:48,639 --> 00:27:50,890
produce a result and other workers

546
00:27:50,890 --> 00:27:53,620
and it's a hard thing to partition the

547
00:27:53,620 --> 00:27:55,990
notes over the cluster evenly so that

548
00:27:55,990 --> 00:28:01,590
you end up with a even computing time

549
00:28:01,590 --> 00:28:04,870
and yeah of course you want to support

550
00:28:04,870 --> 00:28:06,340
the rest of the cypher features and

551
00:28:06,340 --> 00:28:09,190
maybe even introduce more insightful

552
00:28:09,190 --> 00:28:10,930
features like regular path queries or

553
00:28:10,930 --> 00:28:15,250
support for this extended property graph

554
00:28:15,250 --> 00:28:18,600
model and here's where you can come into

555
00:28:18,600 --> 00:28:21,400
the big picture because we'd like to you

556
00:28:21,400 --> 00:28:23,020
if you are interested in this and if you

557
00:28:23,020 --> 00:28:25,630
think that's cool then it's open source

558
00:28:25,630 --> 00:28:30,010
so you can work on this with us together

559
00:28:30,010 --> 00:28:34,660
and yeah just contribute if you if you

560
00:28:34,660 --> 00:28:36,010
like to and if you think that's the cool

561
00:28:36,010 --> 00:28:39,490
thing yes thank you

562
00:28:39,490 --> 00:28:43,539
[Applause]

