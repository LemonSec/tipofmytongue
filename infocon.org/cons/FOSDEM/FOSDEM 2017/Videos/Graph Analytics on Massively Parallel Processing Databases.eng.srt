1
00:00:25,290 --> 00:00:29,280
okay welcome back everyone I'm very

2
00:00:29,280 --> 00:00:36,570
happy to welcome Frank here about graph

3
00:00:36,570 --> 00:00:40,860
analytics on scalable databases so

4
00:00:40,860 --> 00:00:48,989
welcome Frank and thank you very much

5
00:00:48,989 --> 00:00:51,090
it's a real pleasure to be here and

6
00:00:51,090 --> 00:00:53,370
thank you everybody for coming at the

7
00:00:53,370 --> 00:00:56,370
end of a long okay can you hear me in

8
00:00:56,370 --> 00:01:00,300
the back yeah okay okay I'll try to

9
00:01:00,300 --> 00:01:04,050
speak up I'll try to shout right so

10
00:01:04,050 --> 00:01:06,450
thank you very much it's really my

11
00:01:06,450 --> 00:01:08,400
pleasure to talk to you about graph

12
00:01:08,400 --> 00:01:10,140
analytics on massively parallel

13
00:01:10,140 --> 00:01:13,650
processing databases this talk is in the

14
00:01:13,650 --> 00:01:16,470
context of Apache Madlib which is an

15
00:01:16,470 --> 00:01:18,659
open-source project in the Apache

16
00:01:18,659 --> 00:01:21,450
Software Foundation but I hope that kind

17
00:01:21,450 --> 00:01:23,010
of some of the ideas are more general

18
00:01:23,010 --> 00:01:26,150
around running graph algorithms on

19
00:01:26,150 --> 00:01:32,010
parallel databases so I've been working

20
00:01:32,010 --> 00:01:33,870
primarily in the area of machine

21
00:01:33,870 --> 00:01:37,140
learning on parallel databases for a

22
00:01:37,140 --> 00:01:39,690
while graph is something kind of that's

23
00:01:39,690 --> 00:01:44,300
relatively new for us at least on

24
00:01:44,300 --> 00:01:46,830
products like greenplum database and

25
00:01:46,830 --> 00:01:49,430
apache hawk which are both open source

26
00:01:49,430 --> 00:01:54,750
MPP databases Apache hawk is similar to

27
00:01:54,750 --> 00:01:57,270
greenplum it's an MP Pio on sequel it's

28
00:01:57,270 --> 00:02:02,940
an MPP on Hadoop rather project so graph

29
00:02:02,940 --> 00:02:04,890
analytics is becoming very important

30
00:02:04,890 --> 00:02:08,008
part of the enterprise I you know a lot

31
00:02:08,008 --> 00:02:10,050
of academic work but in the last 10 to

32
00:02:10,050 --> 00:02:12,090
15 years has seem like a huge surge of

33
00:02:12,090 --> 00:02:15,659
interest in the enterprise and as it

34
00:02:15,659 --> 00:02:17,069
started to become more popular in the

35
00:02:17,069 --> 00:02:18,920
enterprise

36
00:02:18,920 --> 00:02:21,510
you know folks have looked at using

37
00:02:21,510 --> 00:02:23,519
specialized graph engines like like

38
00:02:23,519 --> 00:02:25,440
neo4j we've heard about some of them

39
00:02:25,440 --> 00:02:28,200
today which are great products the thing

40
00:02:28,200 --> 00:02:29,700
is a lot of the companies that we work

41
00:02:29,700 --> 00:02:33,260
with sequel is their primary workload

42
00:02:33,260 --> 00:02:37,230
they have expertise in sequel they have

43
00:02:37,230 --> 00:02:39,180
investments in sequel

44
00:02:39,180 --> 00:02:41,459
that's where their data is so they

45
00:02:41,459 --> 00:02:43,230
started asking us you know like what can

46
00:02:43,230 --> 00:02:46,590
you do on these database with respect

47
00:02:46,590 --> 00:02:49,500
with respect to graph because graph is

48
00:02:49,500 --> 00:02:51,659
such a common kind of representation for

49
00:02:51,659 --> 00:02:55,409
real real world problems so that's sort

50
00:02:55,409 --> 00:02:59,180
of the focus of of the talk today I

51
00:02:59,180 --> 00:03:03,319
thought I would start with a slide from

52
00:03:03,319 --> 00:03:06,329
DB engines that looks at database engine

53
00:03:06,329 --> 00:03:08,959
popularity and you can see the usual

54
00:03:08,959 --> 00:03:12,590
suspects here in the you know the top 10

55
00:03:12,590 --> 00:03:17,129
and the first graph database neo4j is

56
00:03:17,129 --> 00:03:19,319
around 21st position and there's about 4

57
00:03:19,319 --> 00:03:21,540
in the top 200 so that's not to say that

58
00:03:21,540 --> 00:03:23,370
these are not used and are not really

59
00:03:23,370 --> 00:03:25,680
great products but when it comes to the

60
00:03:25,680 --> 00:03:29,010
enterprise they're less present with

61
00:03:29,010 --> 00:03:30,870
respect to the trends of these databases

62
00:03:30,870 --> 00:03:32,480
over the last 4 years

63
00:03:32,480 --> 00:03:35,939
this is neo4j at the top this is Titan

64
00:03:35,939 --> 00:03:39,480
this is giraffe and this is graph DB so

65
00:03:39,480 --> 00:03:42,569
they're certainly growing this is a log

66
00:03:42,569 --> 00:03:45,540
scale so it looks like they're flatter

67
00:03:45,540 --> 00:03:47,069
than they are they're actually growing

68
00:03:47,069 --> 00:03:49,620
significantly although maybe flattened a

69
00:03:49,620 --> 00:03:54,230
little bit in the last year or so

70
00:03:55,250 --> 00:04:00,090
so graphs as you know represent entities

71
00:04:00,090 --> 00:04:01,829
in their relationships they can help us

72
00:04:01,829 --> 00:04:05,400
understand connections and they can be

73
00:04:05,400 --> 00:04:08,010
very small and simple like this group of

74
00:04:08,010 --> 00:04:11,849
friends here however most real-world

75
00:04:11,849 --> 00:04:15,329
databases are real world world most real

76
00:04:15,329 --> 00:04:18,630
world graph problems are actually very

77
00:04:18,630 --> 00:04:20,909
large and we've heard about some of

78
00:04:20,909 --> 00:04:24,030
those today in the previous talks which

79
00:04:24,030 --> 00:04:25,979
is very interesting this is an example

80
00:04:25,979 --> 00:04:30,539
of a visualization from LinkedIn in

81
00:04:30,539 --> 00:04:33,240
which is showing the connections for you

82
00:04:33,240 --> 00:04:37,500
know a particular person but these kinds

83
00:04:37,500 --> 00:04:39,449
of graphs exist in computer networks

84
00:04:39,449 --> 00:04:42,300
social networks internet transportation

85
00:04:42,300 --> 00:04:46,070
network so the question is can we have a

86
00:04:46,070 --> 00:04:48,570
computing platform that can reason over

87
00:04:48,570 --> 00:04:52,169
these large graphs efficiently and then

88
00:04:52,169 --> 00:04:53,009
do analytical

89
00:04:53,009 --> 00:04:58,830
operations at scale so the idea about

90
00:04:58,830 --> 00:05:01,259
you know massively parallel processing

91
00:05:01,259 --> 00:05:03,960
databases and using graphs on them well

92
00:05:03,960 --> 00:05:05,969
I mean does that make sense should we

93
00:05:05,969 --> 00:05:11,009
even you know go down this Avenue so

94
00:05:11,009 --> 00:05:12,809
here's a couple points that may lead us

95
00:05:12,809 --> 00:05:14,219
to believe that it might not be a bad

96
00:05:14,219 --> 00:05:16,259
idea to investigate first of all these

97
00:05:16,259 --> 00:05:18,389
databases are really built for scale so

98
00:05:18,389 --> 00:05:21,059
they've been devised from the ground up

99
00:05:21,059 --> 00:05:26,309
for very large datasets greenplum

100
00:05:26,309 --> 00:05:28,559
database has been around for you know 10

101
00:05:28,559 --> 00:05:31,349
plus years and it has really thousands

102
00:05:31,349 --> 00:05:33,300
of you know person years of investment

103
00:05:33,300 --> 00:05:36,270
in designing in kind of handling these

104
00:05:36,270 --> 00:05:42,059
large data sets one of one of the things

105
00:05:42,059 --> 00:05:44,189
that I've observed and speaking with our

106
00:05:44,189 --> 00:05:45,689
customers and speaking with the data

107
00:05:45,689 --> 00:05:48,089
science folks that I work with is that

108
00:05:48,089 --> 00:05:50,490
many enterprise use cases combine graph

109
00:05:50,490 --> 00:05:53,539
analytics with other kinds of techniques

110
00:05:53,539 --> 00:05:56,039
right so there's the context-specific

111
00:05:56,039 --> 00:05:59,430
part from graph but also the non graph

112
00:05:59,430 --> 00:06:03,419
methods that together combine to give

113
00:06:03,419 --> 00:06:07,229
the kinds of practical practical kind of

114
00:06:07,229 --> 00:06:09,779
end-to-end solutions to problems so

115
00:06:09,779 --> 00:06:13,349
that's another consideration I mentioned

116
00:06:13,349 --> 00:06:15,719
sequel earlier sequel is a most common

117
00:06:15,719 --> 00:06:17,819
workload in the enterprise it's used

118
00:06:17,819 --> 00:06:21,419
very widely data scientists know at

119
00:06:21,419 --> 00:06:24,389
along with Python and our analysts know

120
00:06:24,389 --> 00:06:27,240
it for sure and there's a also a whole

121
00:06:27,240 --> 00:06:32,009
ecosystem of business intelligence tools

122
00:06:32,009 --> 00:06:34,529
associated with sequel so many

123
00:06:34,529 --> 00:06:36,209
enterprises have invested heavily in

124
00:06:36,209 --> 00:06:37,979
that they're running tableau for

125
00:06:37,979 --> 00:06:41,399
visualization many other sort of BI

126
00:06:41,399 --> 00:06:45,569
products another issue is around data

127
00:06:45,569 --> 00:06:48,120
locality so there's a cost for

128
00:06:48,120 --> 00:06:51,509
replicating moving transforming data to

129
00:06:51,509 --> 00:06:53,459
external systems so when we're talking

130
00:06:53,459 --> 00:06:55,349
about very large graphs and very large

131
00:06:55,349 --> 00:06:58,110
sets of data the cost of moving really

132
00:06:58,110 --> 00:07:01,620
is a consideration and the other thing

133
00:07:01,620 --> 00:07:03,810
which is I guess really particularly

134
00:07:03,810 --> 00:07:05,780
important in in business

135
00:07:05,780 --> 00:07:11,800
is policy so there's there's a cost to

136
00:07:11,800 --> 00:07:15,260
procuring separate database engines you

137
00:07:15,260 --> 00:07:17,419
know running them in production doing

138
00:07:17,419 --> 00:07:19,790
all the oversight and training that are

139
00:07:19,790 --> 00:07:22,130
required for these database engines so

140
00:07:22,130 --> 00:07:23,840
what it means is that you need to

141
00:07:23,840 --> 00:07:25,940
convince the chief information officer

142
00:07:25,940 --> 00:07:28,460
that if you really want to use neo4j or

143
00:07:28,460 --> 00:07:30,860
you want to use titan or another graph

144
00:07:30,860 --> 00:07:33,080
database that needs to get put into

145
00:07:33,080 --> 00:07:33,800
production

146
00:07:33,800 --> 00:07:36,430
so there's a whole overhead associated

147
00:07:36,430 --> 00:07:39,290
with that especially the customers that

148
00:07:39,290 --> 00:07:41,180
we work with in financial services in

149
00:07:41,180 --> 00:07:43,280
government and healthcare they have very

150
00:07:43,280 --> 00:07:46,370
stringent recommendations around this so

151
00:07:46,370 --> 00:07:49,280
we've said it will be a good idea to do

152
00:07:49,280 --> 00:07:50,810
this but you know can you actually

153
00:07:50,810 --> 00:07:54,560
efficiently perform analytic processing

154
00:07:54,560 --> 00:07:59,270
on relational data in an MPP database so

155
00:07:59,270 --> 00:08:02,300
here I put the answer is yes it's not a

156
00:08:02,300 --> 00:08:04,850
blanket yes because I think there's a

157
00:08:04,850 --> 00:08:07,220
there's a class of graph algorithms that

158
00:08:07,220 --> 00:08:11,180
work effectively in MPP land there are

159
00:08:11,180 --> 00:08:13,430
others that don't and I will talk about

160
00:08:13,430 --> 00:08:18,560
that shortly but with apache Madlib

161
00:08:18,560 --> 00:08:20,990
running on pachi hawk or green plum

162
00:08:20,990 --> 00:08:23,270
database we've been able to solve for

163
00:08:23,270 --> 00:08:26,240
some pretty interesting kinds of use

164
00:08:26,240 --> 00:08:28,700
cases for our customers and I'll shortly

165
00:08:28,700 --> 00:08:31,100
give an example on cyber security that

166
00:08:31,100 --> 00:08:35,990
we've worked on recently so I want to

167
00:08:35,990 --> 00:08:40,250
say a word about Apache MATLAB it's an

168
00:08:40,250 --> 00:08:42,260
open source machine learning library

169
00:08:42,260 --> 00:08:45,950
it runs in database and what that means

170
00:08:45,950 --> 00:08:48,860
is that it's bringing the compute to the

171
00:08:48,860 --> 00:08:51,620
data so you don't need to move the data

172
00:08:51,620 --> 00:08:54,170
out operate on it and then move it back

173
00:08:54,170 --> 00:08:58,570
into the database and really the main

174
00:08:58,570 --> 00:09:03,190
kind of value around Apache MATLAB is

175
00:09:03,190 --> 00:09:06,140
scalability and performance if you have

176
00:09:06,140 --> 00:09:08,030
smaller data that fits in memory on a

177
00:09:08,030 --> 00:09:10,430
single node then there's a lot of other

178
00:09:10,430 --> 00:09:13,550
options for for machine learning and for

179
00:09:13,550 --> 00:09:16,550
graph as well right you can use R you

180
00:09:16,550 --> 00:09:18,530
can use a lot of the Python libraries

181
00:09:18,530 --> 00:09:19,430
there's camera

182
00:09:19,430 --> 00:09:22,760
Shoal products as well in this area like

183
00:09:22,760 --> 00:09:27,020
SAS so Madlib is really focused on scale

184
00:09:27,020 --> 00:09:33,500
and performance at scale it came out of

185
00:09:33,500 --> 00:09:35,330
work that was done initially at UC

186
00:09:35,330 --> 00:09:39,740
Berkeley in 2011 a very smart gentleman

187
00:09:39,740 --> 00:09:41,390
by the name of Professor Joe Heller

188
00:09:41,390 --> 00:09:43,730
Steen worked on this along with folks

189
00:09:43,730 --> 00:09:47,959
from what was EMC then EMC greenplum

190
00:09:47,959 --> 00:09:50,770
but since that time it's been

191
00:09:50,770 --> 00:09:53,589
collaboration with Stanford University

192
00:09:53,589 --> 00:09:56,000
University of wisconsin-madison and

193
00:09:56,000 --> 00:09:57,170
University of Florida have also

194
00:09:57,170 --> 00:09:59,000
contributed and in fact some of these

195
00:09:59,000 --> 00:10:02,630
ideas about university of about running

196
00:10:02,630 --> 00:10:04,700
graph in relational databases some of

197
00:10:04,700 --> 00:10:06,440
that academic work was done at

198
00:10:06,440 --> 00:10:08,360
university of wisconsin-madison so you

199
00:10:08,360 --> 00:10:10,720
can look up some papers there by

200
00:10:10,720 --> 00:10:18,910
professor Jignesh patel so whoops

201
00:10:27,830 --> 00:10:37,170
it looks like there it goes okay so that

202
00:10:37,170 --> 00:10:39,899
looks been around for about five years

203
00:10:39,899 --> 00:10:44,570
or so it's a pretty pretty expressive

204
00:10:44,570 --> 00:10:46,560
library

205
00:10:46,560 --> 00:10:48,750
it's got 35 to 40 functions and you can

206
00:10:48,750 --> 00:10:51,450
see here the release that's coming out

207
00:10:51,450 --> 00:10:53,339
in a couple of weeks has the first

208
00:10:53,339 --> 00:10:54,990
implementation of graph single source

209
00:10:54,990 --> 00:10:57,149
shortest path we've prototyped a bunch

210
00:10:57,149 --> 00:10:58,649
of other algorithms which are going to

211
00:10:58,649 --> 00:11:02,820
go in and a subsequent release but you

212
00:11:02,820 --> 00:11:04,470
know what we're doing is we're

213
00:11:04,470 --> 00:11:07,529
interested in adding this certain class

214
00:11:07,529 --> 00:11:09,300
of graph algorithms in MATLAB the way

215
00:11:09,300 --> 00:11:11,850
that look works is that its sequel based

216
00:11:11,850 --> 00:11:13,620
so it's kind of a declarative function

217
00:11:13,620 --> 00:11:16,080
call within a select statement this is

218
00:11:16,080 --> 00:11:18,209
just an example for training a linear

219
00:11:18,209 --> 00:11:20,430
regression model right so you give it an

220
00:11:20,430 --> 00:11:23,279
input table an output table representing

221
00:11:23,279 --> 00:11:25,620
your model here you want to predict

222
00:11:25,620 --> 00:11:29,370
house prices give in property tax number

223
00:11:29,370 --> 00:11:32,360
of bathrooms size of the of the property

224
00:11:32,360 --> 00:11:35,100
and you want to build that model by

225
00:11:35,100 --> 00:11:37,050
number of bedrooms right so there's this

226
00:11:37,050 --> 00:11:39,149
idea in machine learning about training

227
00:11:39,149 --> 00:11:42,899
and then prediction right so you train

228
00:11:42,899 --> 00:11:46,140
you build this model then when you get

229
00:11:46,140 --> 00:11:48,420
new data in that you want to predict

230
00:11:48,420 --> 00:11:51,089
then you have an Associated prediction

231
00:11:51,089 --> 00:11:52,800
statement for that right so this is kind

232
00:11:52,800 --> 00:11:57,480
of the idea behind the kind of the way

233
00:11:57,480 --> 00:11:59,820
Madlib is set up with respect to sequel

234
00:11:59,820 --> 00:12:01,770
so how does it work in the database so

235
00:12:01,770 --> 00:12:04,970
greenplum database is a shared-nothing

236
00:12:04,970 --> 00:12:09,300
parallel system right so you have sequel

237
00:12:09,300 --> 00:12:12,980
comes in to a master there is a query

238
00:12:12,980 --> 00:12:16,230
optimizer which figures those how to

239
00:12:16,230 --> 00:12:19,140
parse that sequel and distribute it to

240
00:12:19,140 --> 00:12:20,820
the worker knows the worker nodes are

241
00:12:20,820 --> 00:12:23,130
called segments segments is where all

242
00:12:23,130 --> 00:12:25,589
the computation happens and it's where

243
00:12:25,589 --> 00:12:27,690
the query processing happens where the

244
00:12:27,690 --> 00:12:29,399
data resides as well when you load data

245
00:12:29,399 --> 00:12:31,190
you low it loaded into the worker knows

246
00:12:31,190 --> 00:12:35,490
so this is not just green plum but other

247
00:12:35,490 --> 00:12:37,260
MPP databases out there

248
00:12:37,260 --> 00:12:39,300
look something like this so the way

249
00:12:39,300 --> 00:12:42,300
Madlib looks is that you can think of it

250
00:12:42,300 --> 00:12:44,310
as a kind of layer on top of greenplum

251
00:12:44,310 --> 00:12:48,660
database it accepts equal R as well as

252
00:12:48,660 --> 00:12:51,180
Python it does input validation and

253
00:12:51,180 --> 00:12:53,940
pre-processing here and then it actually

254
00:12:53,940 --> 00:12:57,480
runs machine learning algorithms in

255
00:12:57,480 --> 00:13:00,450
database as stored objects right so you

256
00:13:00,450 --> 00:13:02,520
can think of it as a layer on top of

257
00:13:02,520 --> 00:13:04,620
greenplum but also compiled with the

258
00:13:04,620 --> 00:13:07,950
database right okay so how do we do

259
00:13:07,950 --> 00:13:11,880
graph then in this this world so here is

260
00:13:11,880 --> 00:13:16,160
our old friend a directed graph we have

261
00:13:16,160 --> 00:13:21,690
vertices or nodes we have edges and the

262
00:13:21,690 --> 00:13:23,400
edges in this little example have

263
00:13:23,400 --> 00:13:25,500
weights so the edge weights can be

264
00:13:25,500 --> 00:13:27,530
positive they could be negative as well

265
00:13:27,530 --> 00:13:30,120
so the way we represent this in MATLAB

266
00:13:30,120 --> 00:13:33,690
is with a vertex table and an edge table

267
00:13:33,690 --> 00:13:35,880
similar to what we just heard with this

268
00:13:35,880 --> 00:13:39,780
ap presentation earlier so in the vertex

269
00:13:39,780 --> 00:13:42,840
table you have the vertex number could

270
00:13:42,840 --> 00:13:44,700
be a so a bunch of parameters associated

271
00:13:44,700 --> 00:13:50,120
with that in the edge table we have the

272
00:13:50,120 --> 00:13:53,750
source yeah so we have a source vertex

273
00:13:53,750 --> 00:13:57,720
destination vertex edge weight and then

274
00:13:57,720 --> 00:13:59,130
there could be a bunch of other edge

275
00:13:59,130 --> 00:14:04,590
parameters as well so the single source

276
00:14:04,590 --> 00:14:06,690
shortest path I kind of walk through how

277
00:14:06,690 --> 00:14:08,610
that was built some of the mechanics of

278
00:14:08,610 --> 00:14:11,040
how that was implemented just to remind

279
00:14:11,040 --> 00:14:13,920
you of what this is given a graph in a

280
00:14:13,920 --> 00:14:16,020
source vertex so the single source

281
00:14:16,020 --> 00:14:18,930
vertex you find the path to every single

282
00:14:18,930 --> 00:14:22,830
vertex in the graph such that the sum of

283
00:14:22,830 --> 00:14:25,860
the weights of the edges is minimized

284
00:14:25,860 --> 00:14:28,950
right so here there's two paths or

285
00:14:28,950 --> 00:14:31,620
multiple paths to get from A to F but

286
00:14:31,620 --> 00:14:35,580
the shortest path is to go a c/e D and F

287
00:14:35,580 --> 00:14:38,550
because that gives you this the shortest

288
00:14:38,550 --> 00:14:40,680
aggregate edge weight there's another

289
00:14:40,680 --> 00:14:43,890
path here through B but that's not

290
00:14:43,890 --> 00:14:45,300
included because that would have been a

291
00:14:45,300 --> 00:14:48,600
longer path so although this is a really

292
00:14:48,600 --> 00:14:50,750
simple algorithm

293
00:14:50,750 --> 00:14:52,820
it's actually very useful can be used

294
00:14:52,820 --> 00:14:55,430
for many things like vehicle routing and

295
00:14:55,430 --> 00:14:59,210
navigation degrees of separation in a

296
00:14:59,210 --> 00:15:01,780
social network also meant in

297
00:15:01,780 --> 00:15:03,890
telecommunications minimum passed away

298
00:15:03,890 --> 00:15:08,120
in a network plant facility layout VLSI

299
00:15:08,120 --> 00:15:11,920
design so many many different kinds of

300
00:15:11,920 --> 00:15:14,360
applications of what is actually a

301
00:15:14,360 --> 00:15:18,620
pretty simple pretty simple algorithm so

302
00:15:18,620 --> 00:15:21,260
the implementation that we did in matlab

303
00:15:21,260 --> 00:15:24,770
is this is a performance curve so we use

304
00:15:24,770 --> 00:15:26,030
something called a bellman-ford

305
00:15:26,030 --> 00:15:29,240
algorithm bellman-ford algorithm allows

306
00:15:29,240 --> 00:15:33,340
you to supports negative edge weights

307
00:15:33,340 --> 00:15:37,360
but not negative cycles and it runs an

308
00:15:37,360 --> 00:15:40,820
order of number of vertices times the

309
00:15:40,820 --> 00:15:44,020
number of edges in the worst case

310
00:15:44,020 --> 00:15:48,080
normally it's much less than that but

311
00:15:48,080 --> 00:15:49,820
that's sort of the kind of worst-case

312
00:15:49,820 --> 00:15:53,410
configuration of a graph so this is

313
00:15:53,410 --> 00:15:56,120
performance that we testing that we ran

314
00:15:56,120 --> 00:15:58,040
first of all it's running on a fairly

315
00:15:58,040 --> 00:16:01,640
small cluster it has a single master and

316
00:16:01,640 --> 00:16:05,510
it has 4 worker nodes and on those those

317
00:16:05,510 --> 00:16:07,970
worker nodes they're multi-core nodes so

318
00:16:07,970 --> 00:16:12,260
there there's multiple segment instances

319
00:16:12,260 --> 00:16:14,000
that are running on those nodes so you

320
00:16:14,000 --> 00:16:16,400
could say that the 4 node cluster but

321
00:16:16,400 --> 00:16:20,360
there's actually 24 worker nodes so this

322
00:16:20,360 --> 00:16:24,710
is number of vertices here so up to this

323
00:16:24,710 --> 00:16:28,250
is going up to 500,000 vertices this is

324
00:16:28,250 --> 00:16:32,120
the run time this is about 45 seconds so

325
00:16:32,120 --> 00:16:35,260
this was fixed 100 edges per vertex

326
00:16:35,260 --> 00:16:37,970
running in a relatively recent release

327
00:16:37,970 --> 00:16:41,330
of green plum for 310 so that at the end

328
00:16:41,330 --> 00:16:43,430
there corresponds to 50 million edges so

329
00:16:43,430 --> 00:16:47,330
it's it's not like a huge graph but

330
00:16:47,330 --> 00:16:49,130
fairly significant and it's showing like

331
00:16:49,130 --> 00:16:54,050
linear or sublinear run time increase

332
00:16:54,050 --> 00:16:57,170
with number of vertices right so in the

333
00:16:57,170 --> 00:16:58,820
worst case this would be a second order

334
00:16:58,820 --> 00:17:01,880
but because we fixed 100 edges per

335
00:17:01,880 --> 00:17:03,680
vertex that shows up more

336
00:17:03,680 --> 00:17:09,859
as linear right so this looks kind of

337
00:17:09,859 --> 00:17:11,390
faded probably can't see from the back

338
00:17:11,390 --> 00:17:13,520
but the way that you call it is like in

339
00:17:13,520 --> 00:17:14,929
a select statement as well right so

340
00:17:14,929 --> 00:17:16,819
there's a declarative function kind of a

341
00:17:16,819 --> 00:17:20,990
built-in function you call graph SSSP

342
00:17:20,990 --> 00:17:25,099
you give it a vertex table you give it a

343
00:17:25,099 --> 00:17:28,130
column where your vertex IDs are you

344
00:17:28,130 --> 00:17:31,820
give it a you have an edge table you

345
00:17:31,820 --> 00:17:35,510
provide the source vertex and it will

346
00:17:35,510 --> 00:17:39,200
generate the shortest path and then from

347
00:17:39,200 --> 00:17:40,790
there if you want to retrieve a

348
00:17:40,790 --> 00:17:42,559
particular path and there's a function

349
00:17:42,559 --> 00:17:47,900
to retrieve the path so I wanted to talk

350
00:17:47,900 --> 00:17:49,370
a little bit about implementation

351
00:17:49,370 --> 00:17:51,140
considerations which maybe go beyond

352
00:17:51,140 --> 00:17:54,110
just the single source shortest path and

353
00:17:54,110 --> 00:17:56,330
which you know go beyond greenplum and

354
00:17:56,330 --> 00:17:58,670
and viola this is around him around

355
00:17:58,670 --> 00:18:03,440
graph on MPP so I think with respect to

356
00:18:03,440 --> 00:18:08,059
relationships considerations on like

357
00:18:08,059 --> 00:18:09,800
which algorithms to implement it how to

358
00:18:09,800 --> 00:18:11,809
go about implementing those algorithms

359
00:18:11,809 --> 00:18:13,550
relationships are not a first-class

360
00:18:13,550 --> 00:18:17,090
citizen in a relational database at

361
00:18:17,090 --> 00:18:19,520
least from the graph perspective unlike

362
00:18:19,520 --> 00:18:23,059
for example in neo4j where you know the

363
00:18:23,059 --> 00:18:24,530
graph structure looks very different

364
00:18:24,530 --> 00:18:27,590
than the edge and vertex table so it

365
00:18:27,590 --> 00:18:30,590
means that joint operations are intense

366
00:18:30,590 --> 00:18:32,809
of right they're expensive from the

367
00:18:32,809 --> 00:18:35,600
perspective of compute in memory so you

368
00:18:35,600 --> 00:18:36,980
really want to minimize those for

369
00:18:36,980 --> 00:18:41,020
example yeah so if you're doing multiple

370
00:18:41,020 --> 00:18:44,929
relationship kinds of joins then you

371
00:18:44,929 --> 00:18:46,910
need to think about whether the MPP

372
00:18:46,910 --> 00:18:48,980
database is really the right place to do

373
00:18:48,980 --> 00:18:53,780
that table scans is another

374
00:18:53,780 --> 00:18:57,260
consideration so we use bellman-ford in

375
00:18:57,260 --> 00:18:59,240
that single source shortest path if we

376
00:18:59,240 --> 00:19:01,010
would use Dijkstra Dijkstra is another

377
00:19:01,010 --> 00:19:02,980
kind of implementation of shortest path

378
00:19:02,980 --> 00:19:06,380
which is more depth-first versus

379
00:19:06,380 --> 00:19:11,179
breadth-first it's much slower so you

380
00:19:11,179 --> 00:19:12,890
want to reduce the number of table scans

381
00:19:12,890 --> 00:19:16,930
so depth search kind of algorithms

382
00:19:16,930 --> 00:19:21,290
are expensive so you want to try to do

383
00:19:21,290 --> 00:19:23,330
breadth-first sorts of sorts of search

384
00:19:23,330 --> 00:19:26,000
as well as greedy kinds of algorithm

385
00:19:26,000 --> 00:19:30,710
algorithms that don't take advantage of

386
00:19:30,710 --> 00:19:32,090
query optimization are going to be a

387
00:19:32,090 --> 00:19:34,040
problem C query optimization is your

388
00:19:34,040 --> 00:19:35,630
friend because a lot of time has been

389
00:19:35,630 --> 00:19:40,100
spent on thinking about how to run

390
00:19:40,100 --> 00:19:42,290
distributed queries very effectively in

391
00:19:42,290 --> 00:19:47,330
MPP architectures right so if you are

392
00:19:47,330 --> 00:19:50,120
specifying that you want to scan a table

393
00:19:50,120 --> 00:19:52,880
and look for vertices or edges in a

394
00:19:52,880 --> 00:19:54,980
certain order then it's going to be much

395
00:19:54,980 --> 00:19:56,510
more expensive than letting the query

396
00:19:56,510 --> 00:20:00,309
optimizer do that for you

397
00:20:02,020 --> 00:20:04,670
implementation considerations right so

398
00:20:04,670 --> 00:20:07,429
on database limits like databases have

399
00:20:07,429 --> 00:20:09,490
some unusual things about them both

400
00:20:09,490 --> 00:20:12,500
greenplum and apache hawk are derived

401
00:20:12,500 --> 00:20:15,110
from Postgres so they were forked from

402
00:20:15,110 --> 00:20:18,380
Postgres so we inherit some of the

403
00:20:18,380 --> 00:20:21,200
limits that Postgres has for example the

404
00:20:21,200 --> 00:20:23,929
maximum working set size maximum set of

405
00:20:23,929 --> 00:20:27,679
a cell in Postgres is 1 gigabyte so if

406
00:20:27,679 --> 00:20:29,360
you're doing iterative algorithms that

407
00:20:29,360 --> 00:20:31,610
you are using you're you know you're

408
00:20:31,610 --> 00:20:35,600
storing parts of your graph or aspects

409
00:20:35,600 --> 00:20:37,580
of your graph then you need to think

410
00:20:37,580 --> 00:20:43,210
about about you know these limitations

411
00:20:43,210 --> 00:20:46,880
so before I get to our cybersecurity

412
00:20:46,880 --> 00:20:49,220
example I wanted to say a word about

413
00:20:49,220 --> 00:20:54,500
roadmap so these are the kinds of things

414
00:20:54,500 --> 00:20:56,510
that we have we've actually prototyped

415
00:20:56,510 --> 00:20:59,920
most of these already and then we'll be

416
00:20:59,920 --> 00:21:02,120
contributing them to Apache mod look

417
00:21:02,120 --> 00:21:05,030
pretty soon so next on the list is all

418
00:21:05,030 --> 00:21:07,190
pairs shortest path if you remember we

419
00:21:07,190 --> 00:21:08,990
did single source shortest path is pick

420
00:21:08,990 --> 00:21:12,050
one source find path to everywhere else

421
00:21:12,050 --> 00:21:15,410
in the database in the graph all pairs

422
00:21:15,410 --> 00:21:17,480
is like what is the shortest path

423
00:21:17,480 --> 00:21:20,350
between all nodes in the database so

424
00:21:20,350 --> 00:21:22,760
there is an algorithm floyd-warshall

425
00:21:22,760 --> 00:21:25,130
that runs in order of number of vertices

426
00:21:25,130 --> 00:21:28,760
cubed which we're looking at the reason

427
00:21:28,760 --> 00:21:30,260
this is interesting is because

428
00:21:30,260 --> 00:21:32,210
once you have all pair shortest paths

429
00:21:32,210 --> 00:21:33,230
you can get some really interesting

430
00:21:33,230 --> 00:21:37,700
measures like betweenness so between

431
00:21:37,700 --> 00:21:40,280
this is a way between the centrality is

432
00:21:40,280 --> 00:21:43,549
a way of finding out influencers in a

433
00:21:43,549 --> 00:21:46,370
network because that's where the

434
00:21:46,370 --> 00:21:48,410
shortest path the most of the shortest

435
00:21:48,410 --> 00:21:52,820
paths run through there's also things

436
00:21:52,820 --> 00:21:54,799
like graph diameter graph diameter is

437
00:21:54,799 --> 00:21:57,049
very interesting because what it allows

438
00:21:57,049 --> 00:21:59,630
you to do is show this the reach within

439
00:21:59,630 --> 00:22:01,130
a database which is really important in

440
00:22:01,130 --> 00:22:02,750
the cyber security example I'm going to

441
00:22:02,750 --> 00:22:06,860
show you page rank is kind of well-known

442
00:22:06,860 --> 00:22:09,130
one to identify importance of vertices

443
00:22:09,130 --> 00:22:11,540
connected components for clustering

444
00:22:11,540 --> 00:22:13,400
common components both strongly and

445
00:22:13,400 --> 00:22:17,090
weakly so these are are pretty good

446
00:22:17,090 --> 00:22:18,890
candidates because of breadth-first as

447
00:22:18,890 --> 00:22:20,270
well

448
00:22:20,270 --> 00:22:21,950
graph caught is an important one that

449
00:22:21,950 --> 00:22:24,290
we've been looking at the reason that's

450
00:22:24,290 --> 00:22:26,240
useful is that sometimes your data your

451
00:22:26,240 --> 00:22:28,400
graph is just too massive so you need to

452
00:22:28,400 --> 00:22:30,559
cut it in a kind of sensible way and

453
00:22:30,559 --> 00:22:33,380
then operate on sub graphs but that one

454
00:22:33,380 --> 00:22:37,280
is harder to do okay so I'd like to

455
00:22:37,280 --> 00:22:39,230
finish with a cybersecurity example that

456
00:22:39,230 --> 00:22:40,700
we've been working on in lateral

457
00:22:40,700 --> 00:22:42,830
movement detection which hopefully can

458
00:22:42,830 --> 00:22:44,120
kind of illustrate some of the points

459
00:22:44,120 --> 00:22:49,419
I've been making so the idea behind this

460
00:22:49,419 --> 00:22:54,860
example is that perimeter defenses is

461
00:22:54,860 --> 00:22:57,559
inadequate which is to say you can't

462
00:22:57,559 --> 00:23:00,679
base your network security on keeping

463
00:23:00,679 --> 00:23:02,710
people out you have to assume that that

464
00:23:02,710 --> 00:23:06,740
people will get into your network and

465
00:23:06,740 --> 00:23:08,570
you want to identify them in that case

466
00:23:08,570 --> 00:23:12,110
right so threats can come from within so

467
00:23:12,110 --> 00:23:14,900
here is sort of a path in which this is

468
00:23:14,900 --> 00:23:16,640
known to happen it's called the advanced

469
00:23:16,640 --> 00:23:19,130
persistent threat and this is the chain

470
00:23:19,130 --> 00:23:21,260
so there's a phishing attack or a

471
00:23:21,260 --> 00:23:24,320
zero-day attack in which say a user's

472
00:23:24,320 --> 00:23:31,429
machine is compromised then a third

473
00:23:31,429 --> 00:23:33,290
party machine will then come in the back

474
00:23:33,290 --> 00:23:35,740
door via this compromised machine and

475
00:23:35,740 --> 00:23:38,540
then there's a lateral movement that

476
00:23:38,540 --> 00:23:42,260
happens so once the bad actor is in the

477
00:23:42,260 --> 00:23:42,809
network

478
00:23:42,809 --> 00:23:45,419
they may expand their footprint expand

479
00:23:45,419 --> 00:23:48,389
their graph diameter to elevate

480
00:23:48,389 --> 00:23:52,039
importance look at financial data that

481
00:23:52,039 --> 00:23:55,909
may be available whatever hunt around

482
00:23:55,909 --> 00:23:59,129
then gather data from these servers and

483
00:23:59,129 --> 00:24:01,200
exfiltrate so this is a kind of a common

484
00:24:01,200 --> 00:24:03,330
path so I want to talk about this

485
00:24:03,330 --> 00:24:06,120
lateral movement idea so lateral

486
00:24:06,120 --> 00:24:08,159
movement though it's a data science

487
00:24:08,159 --> 00:24:10,230
problem in fact a lot of cyber security

488
00:24:10,230 --> 00:24:12,149
problems in our data science problems of

489
00:24:12,149 --> 00:24:15,210
which graph plays a part you want to

490
00:24:15,210 --> 00:24:17,519
look at users and user behavior models

491
00:24:17,519 --> 00:24:19,259
and you also want to look at networks

492
00:24:19,259 --> 00:24:22,110
and centers right so what users are

493
00:24:22,110 --> 00:24:25,529
touching which which machine which

494
00:24:25,529 --> 00:24:30,059
machines so the way that lateral

495
00:24:30,059 --> 00:24:33,779
movement detection works is it's as I

496
00:24:33,779 --> 00:24:35,610
kind of described before a multimodal

497
00:24:35,610 --> 00:24:39,840
problem so you have logs coming in from

498
00:24:39,840 --> 00:24:41,610
Active Directory and other sources

499
00:24:41,610 --> 00:24:44,580
server information lands in the data

500
00:24:44,580 --> 00:24:46,440
store somewhere so this could be an

501
00:24:46,440 --> 00:24:49,710
object store or it could be on greenplum

502
00:24:49,710 --> 00:24:52,009
database or external table somewhere

503
00:24:52,009 --> 00:24:55,440
could be Hadoop file system then within

504
00:24:55,440 --> 00:24:56,999
greenplum there's actually multiple

505
00:24:56,999 --> 00:25:00,240
models that are run I'm gonna show you a

506
00:25:00,240 --> 00:25:02,340
couple of those but I just listed them

507
00:25:02,340 --> 00:25:04,769
out here they were they the data

508
00:25:04,769 --> 00:25:06,929
scientists that we work with they run

509
00:25:06,929 --> 00:25:09,299
regression models they run clustering

510
00:25:09,299 --> 00:25:11,399
models they run recommendation models

511
00:25:11,399 --> 00:25:14,369
user behavior models and they run graph

512
00:25:14,369 --> 00:25:17,460
models and what they do is produce a net

513
00:25:17,460 --> 00:25:22,169
score by user on a daily or weekly basis

514
00:25:22,169 --> 00:25:24,539
and that net score is kind of an

515
00:25:24,539 --> 00:25:28,080
aggregation of all of the activity from

516
00:25:28,080 --> 00:25:31,139
that particular user this example there

517
00:25:31,139 --> 00:25:34,679
was 10,000 users and 60,000 servers so

518
00:25:34,679 --> 00:25:38,940
not huge but you know pretty big so

519
00:25:38,940 --> 00:25:40,409
here's an example of a regression model

520
00:25:40,409 --> 00:25:41,730
I know you can't see this at the back

521
00:25:41,730 --> 00:25:44,730
but this is week and this is the number

522
00:25:44,730 --> 00:25:48,779
of servers that are touched right so

523
00:25:48,779 --> 00:25:51,119
there's a big spike that happens here so

524
00:25:51,119 --> 00:25:53,070
if you run a simple linear regression

525
00:25:53,070 --> 00:25:55,450
and look at changes in slope over time

526
00:25:55,450 --> 00:25:57,580
that's one indicator of a change of

527
00:25:57,580 --> 00:26:02,650
behavior user behavior model is another

528
00:26:02,650 --> 00:26:07,270
one user behavior models they use in

529
00:26:07,270 --> 00:26:10,180
this particular case machine learning

530
00:26:10,180 --> 00:26:12,100
algorithm called principal components

531
00:26:12,100 --> 00:26:15,280
but in the heat map you can see here

532
00:26:15,280 --> 00:26:18,280
this is week as well and this is server

533
00:26:18,280 --> 00:26:21,040
it could be server group so the heat map

534
00:26:21,040 --> 00:26:24,130
is showing that this user is using

535
00:26:24,130 --> 00:26:26,620
server one quite a lot and they're using

536
00:26:26,620 --> 00:26:29,140
server 9 quite a lot but then they hit a

537
00:26:29,140 --> 00:26:31,240
point here at which they're using many

538
00:26:31,240 --> 00:26:33,580
servers so they begin walking into a lot

539
00:26:33,580 --> 00:26:35,710
of servers so this machine learning

540
00:26:35,710 --> 00:26:38,080
model which is building up this

541
00:26:38,080 --> 00:26:40,450
behavioral model will then score that

542
00:26:40,450 --> 00:26:43,180
accordingly I won't go over the others

543
00:26:43,180 --> 00:26:44,680
but then you have a graph model as well

544
00:26:44,680 --> 00:26:47,920
so the graph model is built on a weekly

545
00:26:47,920 --> 00:26:52,210
basis by user it uses this historical

546
00:26:52,210 --> 00:26:56,050
windows to build the graph it looks at

547
00:26:56,050 --> 00:26:58,570
which machines does a user login to

548
00:26:58,570 --> 00:27:00,850
which machines do they log in from how

549
00:27:00,850 --> 00:27:03,280
often etcetera so in the typical

550
00:27:03,280 --> 00:27:06,190
behavior they log in from say their home

551
00:27:06,190 --> 00:27:09,040
machine to this machine and they touch

552
00:27:09,040 --> 00:27:13,000
these servers in the anomalous behavior

553
00:27:13,000 --> 00:27:16,060
example they log in from this machine

554
00:27:16,060 --> 00:27:18,190
but they also go directly to other

555
00:27:18,190 --> 00:27:21,610
machines they go to this machine they go

556
00:27:21,610 --> 00:27:24,160
to a machine that they've never been on

557
00:27:24,160 --> 00:27:26,860
before which has a database with

558
00:27:26,860 --> 00:27:29,200
financial information it's a different

559
00:27:29,200 --> 00:27:31,540
kind of footprint and the reason this is

560
00:27:31,540 --> 00:27:33,940
really useful for lateral movement is

561
00:27:33,940 --> 00:27:37,180
that graphs are sensitive to right

562
00:27:37,180 --> 00:27:39,690
direction right so they have Direction

563
00:27:39,690 --> 00:27:42,910
order is important frequency is

564
00:27:42,910 --> 00:27:45,730
important as well right because as you

565
00:27:45,730 --> 00:27:48,910
have increased frequency your edge

566
00:27:48,910 --> 00:27:53,860
weight goes up that expands the the

567
00:27:53,860 --> 00:27:56,920
graph diameter right so that the kinds

568
00:27:56,920 --> 00:27:58,690
of graph measures that are important to

569
00:27:58,690 --> 00:28:03,010
look at here are things like central

570
00:28:03,010 --> 00:28:04,960
centrality measures like this between

571
00:28:04,960 --> 00:28:07,350
the centrality that increases

572
00:28:07,350 --> 00:28:09,370
unexpectedly

573
00:28:09,370 --> 00:28:12,040
and also how graph diameter changes over

574
00:28:12,040 --> 00:28:16,060
time that's really important thing by

575
00:28:16,060 --> 00:28:18,940
user to see if there's any kind of you

576
00:28:18,940 --> 00:28:23,950
know basic behavior change so then all

577
00:28:23,950 --> 00:28:26,470
of these models can be combined and then

578
00:28:26,470 --> 00:28:35,740
given a score by by user so the Apache

579
00:28:35,740 --> 00:28:38,160
MATLAB we're on our fourth release

580
00:28:38,160 --> 00:28:40,510
that's moving towards top level status

581
00:28:40,510 --> 00:28:43,330
anybody is welcome to to join us on this

582
00:28:43,330 --> 00:28:45,640
project so the thought I want to leave

583
00:28:45,640 --> 00:28:48,340
you with is MPP databases can be

584
00:28:48,340 --> 00:28:50,500
effective for solving a certain class of

585
00:28:50,500 --> 00:28:55,650
graph analytics problems at scale and

586
00:28:55,650 --> 00:28:58,170
thank you for your time

587
00:28:58,170 --> 00:29:06,609
[Applause]

588
00:29:17,450 --> 00:29:19,510
you

589
00:29:31,330 --> 00:29:35,030
yeah pushes it down I didn't go into the

590
00:29:35,030 --> 00:29:38,180
detail of it but the way the flow works

591
00:29:38,180 --> 00:29:42,140
is if you're on sort of a client here

592
00:29:42,140 --> 00:29:44,870
the sequel then is sent to the database

593
00:29:44,870 --> 00:29:48,050
server and then the compute actually

594
00:29:48,050 --> 00:29:50,660
happens via stored procedures on the

595
00:29:50,660 --> 00:29:54,080
worker nodes so yeah the heavy lifting

596
00:29:54,080 --> 00:29:57,080
is done where the data is on the worker

597
00:29:57,080 --> 00:29:59,290
nodes

