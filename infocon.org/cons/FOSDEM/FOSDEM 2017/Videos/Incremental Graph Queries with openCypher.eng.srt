1
00:00:04,530 --> 00:00:08,670
I was here because he's done ugly cool a

2
00:00:08,670 --> 00:00:11,370
lot of cool stuff around craft varying a

3
00:00:11,370 --> 00:00:14,430
complex model and large models and he

4
00:00:14,430 --> 00:00:17,820
will talk today about incremental crop

5
00:00:17,820 --> 00:00:21,890
growing so let's welcome him and we're

6
00:00:21,890 --> 00:00:30,990
up for recording so welcome everyone my

7
00:00:30,990 --> 00:00:32,308
name is Gabbar Suresh

8
00:00:32,308 --> 00:00:34,350
I'm a PhD student at the Hungarian

9
00:00:34,350 --> 00:00:36,539
Budapest University of Technology and

10
00:00:36,539 --> 00:00:38,640
economics and I also worked as a

11
00:00:38,640 --> 00:00:41,070
visiting researcher in Canada at McGill

12
00:00:41,070 --> 00:00:43,409
University my talk today will be about

13
00:00:43,409 --> 00:00:45,390
incremental graph queries with open

14
00:00:45,390 --> 00:00:48,300
cipher so let's dissect this title and

15
00:00:48,300 --> 00:00:50,309
first talk about incremental graph

16
00:00:50,309 --> 00:00:52,019
queries why would we want to use

17
00:00:52,019 --> 00:00:54,780
incremental graph queries well let's

18
00:00:54,780 --> 00:00:57,120
imagine a simple wave a network where

19
00:00:57,120 --> 00:00:59,309
the Rev a segments and trains are

20
00:00:59,309 --> 00:01:01,469
traveling all along drive a network

21
00:01:01,469 --> 00:01:04,170
obviously this is a safety critical

22
00:01:04,170 --> 00:01:06,390
system so if there is some malfunction

23
00:01:06,390 --> 00:01:09,299
or a design error in the system serious

24
00:01:09,299 --> 00:01:11,820
damage in property can occur or life can

25
00:01:11,820 --> 00:01:13,710
be threatened so it's absolutely

26
00:01:13,710 --> 00:01:16,009
important that we get this right and we

27
00:01:16,009 --> 00:01:19,200
closely monitor this system 24 hours a

28
00:01:19,200 --> 00:01:22,649
day seven 24 hours a day and seven days

29
00:01:22,649 --> 00:01:26,670
a week so we define constraints that

30
00:01:26,670 --> 00:01:28,860
must be kept during the running of the

31
00:01:28,860 --> 00:01:33,359
system for example if we have trains too

32
00:01:33,359 --> 00:01:35,490
close to each other we might issue the

33
00:01:35,490 --> 00:01:37,979
dangerous proximity and say that one of

34
00:01:37,979 --> 00:01:40,399
the trains must stop as they might crash

35
00:01:40,399 --> 00:01:43,439
also in this system there might be

36
00:01:43,439 --> 00:01:47,549
switches that navigate the trains and

37
00:01:47,549 --> 00:01:49,409
there is an interesting constraint in

38
00:01:49,409 --> 00:01:52,350
railway networks is that if the switch

39
00:01:52,350 --> 00:01:54,929
is set to a diverging position then the

40
00:01:54,929 --> 00:01:56,549
train cannot travel from the other

41
00:01:56,549 --> 00:01:58,679
direction because it might damage the

42
00:01:58,679 --> 00:02:00,749
switch this is called trailing the

43
00:02:00,749 --> 00:02:02,609
switch this is also something that we

44
00:02:02,609 --> 00:02:04,490
want to avoid because it damages

45
00:02:04,490 --> 00:02:08,788
valuable harder okay so how do we order

46
00:02:08,788 --> 00:02:10,919
this as a graph it's not that difficult

47
00:02:10,919 --> 00:02:14,040
we just say that each segment will be a

48
00:02:14,040 --> 00:02:16,050
node in the graph the transferor B nodes

49
00:02:16,050 --> 00:02:18,160
in the graph and then the switch

50
00:02:18,160 --> 00:02:20,440
we'll be a node in the graph storing its

51
00:02:20,440 --> 00:02:23,740
position as divergent so we had some

52
00:02:23,740 --> 00:02:26,320
edges now we have the tree the trains

53
00:02:26,320 --> 00:02:29,110
our own segments the segments are stored

54
00:02:29,110 --> 00:02:32,200
pretty much as a linked list and then we

55
00:02:32,200 --> 00:02:34,330
have the switch with the appropriate

56
00:02:34,330 --> 00:02:37,570
directions so if you want to do the

57
00:02:37,570 --> 00:02:39,640
proximity detection constraint we say

58
00:02:39,640 --> 00:02:43,300
that if the trains are on adjacent

59
00:02:43,300 --> 00:02:45,850
segments or they have less than two

60
00:02:45,850 --> 00:02:48,040
segments between them then we might

61
00:02:48,040 --> 00:02:51,460
issue a warning so as a graph pattern

62
00:02:51,460 --> 00:02:54,130
this might look like this we have a

63
00:02:54,130 --> 00:02:57,850
train segment and then there is a single

64
00:02:57,850 --> 00:03:00,100
segment or on or two segments between

65
00:03:00,100 --> 00:03:04,600
the segments that the trains are on this

66
00:03:04,600 --> 00:03:07,000
is pretty much a simple cipher query you

67
00:03:07,000 --> 00:03:10,630
can just take the graph pattern and

68
00:03:10,630 --> 00:03:13,330
describe it in the usual cipher syntax

69
00:03:13,330 --> 00:03:16,600
if you want in your 4j it generates a

70
00:03:16,600 --> 00:03:19,180
query plan like this and it executes and

71
00:03:19,180 --> 00:03:22,900
returns the values so for trying the

72
00:03:22,900 --> 00:03:25,570
switch we have another constraint it

73
00:03:25,570 --> 00:03:27,400
looks like this we have a trade which is

74
00:03:27,400 --> 00:03:31,090
on a segment which comes straight from a

75
00:03:31,090 --> 00:03:32,920
switch but the switch is set to a

76
00:03:32,920 --> 00:03:35,920
diverging position so this is something

77
00:03:35,920 --> 00:03:37,840
that we would like to avoid and then we

78
00:03:37,840 --> 00:03:42,190
would like to warn the users neo4j is

79
00:03:42,190 --> 00:03:43,810
also capable of running this pretty

80
00:03:43,810 --> 00:03:47,230
efficiently however in such a system we

81
00:03:47,230 --> 00:03:48,610
need to evaluate such queries

82
00:03:48,610 --> 00:03:50,770
continuously so we need to run these

83
00:03:50,770 --> 00:03:52,900
queries over and over and over again and

84
00:03:52,900 --> 00:03:55,180
this is basically the idea of

85
00:03:55,180 --> 00:03:57,640
incremental graph queries we restore a

86
00:03:57,640 --> 00:03:59,890
set of so called standing queries in the

87
00:03:59,890 --> 00:04:02,440
system and we continuously evaluate

88
00:04:02,440 --> 00:04:04,930
these queries upon each change in the

89
00:04:04,930 --> 00:04:08,560
graph there is quite simple algorithm

90
00:04:08,560 --> 00:04:10,600
which is called the rata algorithm it's

91
00:04:10,600 --> 00:04:14,380
actually comes from 1974 and was

92
00:04:14,380 --> 00:04:16,899
originally designed for rule-based

93
00:04:16,899 --> 00:04:19,358
expert systems the main idea of the

94
00:04:19,358 --> 00:04:21,459
rattle algorithm is that it indexes the

95
00:04:21,459 --> 00:04:24,100
graph and carries the interim results in

96
00:04:24,100 --> 00:04:26,500
a network Retta actually means net in

97
00:04:26,500 --> 00:04:28,510
latin so this defines a propagation

98
00:04:28,510 --> 00:04:31,630
Network for the query and it stores the

99
00:04:31,630 --> 00:04:32,290
reside

100
00:04:32,290 --> 00:04:35,320
so that it can only maintain the results

101
00:04:35,320 --> 00:04:38,170
upon changes so how does this look like

102
00:04:38,170 --> 00:04:40,810
in practice for the second constraint

103
00:04:40,810 --> 00:04:42,790
trailing the switch we have the graph

104
00:04:42,790 --> 00:04:44,920
pattern and this is our return at work

105
00:04:44,920 --> 00:04:47,160
as you can see you have your usual

106
00:04:47,160 --> 00:04:50,050
relational algebra operators like joins

107
00:04:50,050 --> 00:04:53,500
selections and projections so if we take

108
00:04:53,500 --> 00:04:56,890
the graph then first what we do is index

109
00:04:56,890 --> 00:05:00,040
the graph for the appropriate edges so

110
00:05:00,040 --> 00:05:03,100
first together the own edges to the

111
00:05:03,100 --> 00:05:05,980
indexer on the left then we gather the

112
00:05:05,980 --> 00:05:08,020
straight edges to the index on the right

113
00:05:08,020 --> 00:05:11,920
and now we start to calculate the

114
00:05:11,920 --> 00:05:15,070
results for the graph pattern so what we

115
00:05:15,070 --> 00:05:19,180
do is join the nodes that are connected

116
00:05:19,180 --> 00:05:23,910
so we have the Train on the segment and

117
00:05:23,910 --> 00:05:27,160
next to a switch we do a filtering

118
00:05:27,160 --> 00:05:30,400
operation which enforces that the switch

119
00:05:30,400 --> 00:05:33,130
is the diverging position and then in

120
00:05:33,130 --> 00:05:36,700
the end we project and we only need the

121
00:05:36,700 --> 00:05:39,370
number of the train and the switch that

122
00:05:39,370 --> 00:05:43,690
is next to the segment so what's the

123
00:05:43,690 --> 00:05:46,120
advantage of this approach well if the

124
00:05:46,120 --> 00:05:47,830
train moves slightly for example it

125
00:05:47,830 --> 00:05:50,920
moves from E to segment D then we only

126
00:05:50,920 --> 00:05:53,380
have to recalculate a small portion of

127
00:05:53,380 --> 00:05:55,660
the network in this case we propagate

128
00:05:55,660 --> 00:05:58,630
the change through the own indexer and

129
00:05:58,630 --> 00:06:02,440
then there are no more matches in the

130
00:06:02,440 --> 00:06:04,990
join operator so we have no more issues

131
00:06:04,990 --> 00:06:07,300
because the trailing the switch

132
00:06:07,300 --> 00:06:09,720
constraint is now not violated

133
00:06:09,720 --> 00:06:14,080
so to summarize usually graph databases

134
00:06:14,080 --> 00:06:16,150
and relational databases use batch

135
00:06:16,150 --> 00:06:18,370
queries which are more request driven

136
00:06:18,370 --> 00:06:20,440
there is a client and the client issues

137
00:06:20,440 --> 00:06:23,800
a query the database calculates the

138
00:06:23,800 --> 00:06:27,190
query results and it returns the results

139
00:06:27,190 --> 00:06:29,710
the results are obtained on demand so

140
00:06:29,710 --> 00:06:31,420
the client has to specifically ask the

141
00:06:31,420 --> 00:06:34,360
database for the query results in

142
00:06:34,360 --> 00:06:36,310
contrast incremental queries are more

143
00:06:36,310 --> 00:06:39,040
even driven so the client registers set

144
00:06:39,040 --> 00:06:41,200
of periods in advance and then for each

145
00:06:41,200 --> 00:06:42,850
change in the graph the results are

146
00:06:42,850 --> 00:06:44,770
maintained and this is a continuous loop

147
00:06:44,770 --> 00:06:45,409
the

148
00:06:45,409 --> 00:06:47,629
query results are always available for

149
00:06:47,629 --> 00:06:52,580
the client so in the past there have

150
00:06:52,580 --> 00:06:54,649
been many implementations for

151
00:06:54,649 --> 00:06:56,629
incremental query engines one of the

152
00:06:56,629 --> 00:06:58,069
most well-known is the Klipsch

153
00:06:58,069 --> 00:07:00,679
rule-based expert systems which was

154
00:07:00,679 --> 00:07:01,610
developed at NASA

155
00:07:01,610 --> 00:07:05,989
there is juice Retta based engine which

156
00:07:05,989 --> 00:07:08,749
is part of the JBoss stack and is

157
00:07:08,749 --> 00:07:11,749
developed by Red Hat and our University

158
00:07:11,749 --> 00:07:14,749
developed the via track Larry framework

159
00:07:14,749 --> 00:07:17,209
which works on the Eclipse modeling

160
00:07:17,209 --> 00:07:19,789
framework and is quite widely used in

161
00:07:19,789 --> 00:07:22,489
the modeling community we actually have

162
00:07:22,489 --> 00:07:24,800
a spin-off company called inquiry labs

163
00:07:24,800 --> 00:07:27,589
which continues to develop viatra

164
00:07:27,589 --> 00:07:31,149
and provide support so these are the

165
00:07:31,149 --> 00:07:33,919
academic and industrial tools and we

166
00:07:33,919 --> 00:07:37,129
have some very prototypical tools like

167
00:07:37,129 --> 00:07:39,679
instance which works rdf and is

168
00:07:39,679 --> 00:07:42,199
developed by otto University we have a

169
00:07:42,199 --> 00:07:45,139
scholar based solution called I 3ql

170
00:07:45,139 --> 00:07:46,929
developed by the Technical University

171
00:07:46,929 --> 00:07:50,990
Darmstadt and we have in query D which

172
00:07:50,990 --> 00:07:53,629
was my early PhD project in my

173
00:07:53,629 --> 00:07:58,009
university so what we don't have is a

174
00:07:58,009 --> 00:08:00,559
property graph based implementation so

175
00:08:00,559 --> 00:08:02,479
as far as we know there are no

176
00:08:02,479 --> 00:08:04,519
incremental query engines for property

177
00:08:04,519 --> 00:08:06,490
graphs yet but this is about to change

178
00:08:06,490 --> 00:08:10,669
so first we need a good cry language and

179
00:08:10,669 --> 00:08:12,469
a good ecosystem that we can integrate

180
00:08:12,469 --> 00:08:16,819
with so what is open cipher as you

181
00:08:16,819 --> 00:08:19,039
probably know the cipher Q language aims

182
00:08:19,039 --> 00:08:22,369
to be the sequel for graph databases so

183
00:08:22,369 --> 00:08:25,399
this means that new technology tries to

184
00:08:25,399 --> 00:08:29,300
provide a standard for its query

185
00:08:29,300 --> 00:08:31,429
language this comes with the obvious

186
00:08:31,429 --> 00:08:33,679
advantages that users will not be afraid

187
00:08:33,679 --> 00:08:36,799
of getting logged in with a specific

188
00:08:36,799 --> 00:08:40,370
vendor instead they can rely on the GAR

189
00:08:40,370 --> 00:08:42,620
certification and the reference

190
00:08:42,620 --> 00:08:45,800
implementation and also the open cipher

191
00:08:45,800 --> 00:08:48,259
kit provides a so-called technology

192
00:08:48,259 --> 00:08:50,720
compatibility kit which allows the

193
00:08:50,720 --> 00:08:53,420
developers to test their own open cipher

194
00:08:53,420 --> 00:08:55,939
compliant engines if they will comply

195
00:08:55,939 --> 00:08:58,519
with the rules of the open cipher

196
00:08:58,519 --> 00:08:59,120
language

197
00:08:59,120 --> 00:09:03,499
so open cypher actually defines a

198
00:09:03,499 --> 00:09:05,569
smaller subset of the cypher clear

199
00:09:05,569 --> 00:09:07,999
language but it still allows you to do

200
00:09:07,999 --> 00:09:10,160
many interesting things you can do

201
00:09:10,160 --> 00:09:12,499
almost all of the pattern matching stuff

202
00:09:12,499 --> 00:09:15,649
that you can do in normal neo4j cipher

203
00:09:15,649 --> 00:09:18,050
you can do your filtering operations

204
00:09:18,050 --> 00:09:19,850
there are collections like lists and

205
00:09:19,850 --> 00:09:22,639
maps there are operators for data

206
00:09:22,639 --> 00:09:25,490
manipulation and also more advanced

207
00:09:25,490 --> 00:09:30,129
constructs like variable length path so

208
00:09:30,129 --> 00:09:32,779
some features are excluded from the

209
00:09:32,779 --> 00:09:34,759
standard these are called the legacy

210
00:09:34,759 --> 00:09:36,790
constructs these are mostly

211
00:09:36,790 --> 00:09:39,589
implementation specific stuff like load

212
00:09:39,589 --> 00:09:42,829
CSV index constraints there are regular

213
00:09:42,829 --> 00:09:44,180
expression which are notoriously

214
00:09:44,180 --> 00:09:46,519
difficult to get right between different

215
00:09:46,519 --> 00:09:50,059
programming languages some of the more

216
00:09:50,059 --> 00:09:51,800
sophisticated list functions for example

217
00:09:51,800 --> 00:09:54,199
reduce are removed because they these

218
00:09:54,199 --> 00:09:55,999
are very powerful and you can almost

219
00:09:55,999 --> 00:09:59,230
program with reduce in your queries and

220
00:09:59,230 --> 00:10:02,329
there are a lot of predicate functions

221
00:10:02,329 --> 00:10:04,699
removed functions that calculate the

222
00:10:04,699 --> 00:10:07,220
shortest path case expressions and the

223
00:10:07,220 --> 00:10:09,889
IDS and interestingly and somewhat

224
00:10:09,889 --> 00:10:12,470
accidentally some of the legacy

225
00:10:12,470 --> 00:10:14,839
constructs are also very difficult to

226
00:10:14,839 --> 00:10:17,179
handle internally so in our prototype

227
00:10:17,179 --> 00:10:20,509
implementation we do not focus on the

228
00:10:20,509 --> 00:10:24,259
legacy construct but try to implement a

229
00:10:24,259 --> 00:10:26,240
meaningful set of the standard

230
00:10:26,240 --> 00:10:30,490
constructs in open cipher so in order to

231
00:10:30,490 --> 00:10:33,439
implement our increment or query engine

232
00:10:33,439 --> 00:10:36,170
we first mapped the open cipher query

233
00:10:36,170 --> 00:10:39,339
language to relational algebra this is

234
00:10:39,339 --> 00:10:41,720
quite a long mapping we had to define

235
00:10:41,720 --> 00:10:43,009
some custom relational algebra

236
00:10:43,009 --> 00:10:45,350
co-operators and we gathered our

237
00:10:45,350 --> 00:10:48,379
findings to a technical report and we

238
00:10:48,379 --> 00:10:49,939
published this technical report online

239
00:10:49,939 --> 00:10:53,269
it's built continuously on every change

240
00:10:53,269 --> 00:10:55,189
in our system so basically what you see

241
00:10:55,189 --> 00:10:57,800
in a technical report is the latest

242
00:10:57,800 --> 00:11:01,759
version as compiled by our parser and

243
00:11:01,759 --> 00:11:07,160
our our query engine so we gather a lot

244
00:11:07,160 --> 00:11:09,079
of theories from neo4j use cases for

245
00:11:09,079 --> 00:11:11,480
example tutorials our units use cases

246
00:11:11,480 --> 00:11:12,529
blow

247
00:11:12,529 --> 00:11:16,850
github and we translated these queries

248
00:11:16,850 --> 00:11:21,319
to standard batch search plans defined

249
00:11:21,319 --> 00:11:23,360
by relational algebra and then we

250
00:11:23,360 --> 00:11:26,120
translated the search plans to increment

251
00:11:26,120 --> 00:11:27,949
our search plans also defined in

252
00:11:27,949 --> 00:11:31,490
relational algebra some of the findings

253
00:11:31,490 --> 00:11:35,120
that we have is that some constructs

254
00:11:35,120 --> 00:11:37,459
even in standard open cipher are very

255
00:11:37,459 --> 00:11:38,870
difficult to get right

256
00:11:38,870 --> 00:11:41,689
incremental e especially lists because

257
00:11:41,689 --> 00:11:43,759
you have to treat the list with the

258
00:11:43,759 --> 00:11:45,980
appropriate granularity and that's not

259
00:11:45,980 --> 00:11:48,800
easy if you have lists that are nested

260
00:11:48,800 --> 00:11:50,600
into each other and then you can unwind

261
00:11:50,600 --> 00:11:52,069
these lists and that you can collect

262
00:11:52,069 --> 00:11:54,170
these lists back together this is

263
00:11:54,170 --> 00:11:56,329
something that's not easy to get right

264
00:11:56,329 --> 00:12:00,259
also standard rata implementations use

265
00:12:00,259 --> 00:12:03,379
set semantics in contrast open cipher

266
00:12:03,379 --> 00:12:04,939
uses back semantics and you can also

267
00:12:04,939 --> 00:12:08,660
order your results and select the top

268
00:12:08,660 --> 00:12:12,110
and/or skip some results in the result

269
00:12:12,110 --> 00:12:14,449
set and as I mentioned shortest path and

270
00:12:14,449 --> 00:12:16,850
all shortest paths are quite difficult

271
00:12:16,850 --> 00:12:18,529
to get right incrementally there are

272
00:12:18,529 --> 00:12:21,019
some papers in the topic so in theory it

273
00:12:21,019 --> 00:12:23,959
can be done but you have quite complex

274
00:12:23,959 --> 00:12:26,059
algorithms that do not always perform

275
00:12:26,059 --> 00:12:26,839
well in practice

276
00:12:26,839 --> 00:12:31,730
so these challenging features in cipher

277
00:12:31,730 --> 00:12:34,670
are those that are not traditionally

278
00:12:34,670 --> 00:12:37,069
handled by our two implementations we

279
00:12:37,069 --> 00:12:40,009
are looking into implementing the

280
00:12:40,009 --> 00:12:42,350
standard construct and see what we can

281
00:12:42,350 --> 00:12:47,300
do about the legacy constructs so by now

282
00:12:47,300 --> 00:12:49,910
we have two which we call in graph and

283
00:12:49,910 --> 00:12:51,939
incremental in memory graph query engine

284
00:12:51,939 --> 00:12:54,559
basically in graph works in a

285
00:12:54,559 --> 00:12:56,629
client-server architecture the client

286
00:12:56,629 --> 00:12:59,300
registers set of theories and then it

287
00:12:59,300 --> 00:13:01,399
continuously update update the graph

288
00:13:01,399 --> 00:13:04,040
through another Cleary's and the client

289
00:13:04,040 --> 00:13:06,139
can either retrieve the query results or

290
00:13:06,139 --> 00:13:08,389
it can subscribe and get change

291
00:13:08,389 --> 00:13:12,110
certifications for new results in the

292
00:13:12,110 --> 00:13:15,589
query set so how does in graph look

293
00:13:15,589 --> 00:13:17,629
internally well it pretty much looks

294
00:13:17,629 --> 00:13:19,579
like any other graph database or graph

295
00:13:19,579 --> 00:13:21,550
query engine it takes the square

296
00:13:21,550 --> 00:13:25,040
specification and builds a query syntax

297
00:13:25,040 --> 00:13:26,180
tree now

298
00:13:26,180 --> 00:13:28,100
as I mentioned the open cypher

299
00:13:28,100 --> 00:13:30,500
initiative provides the grammar for

300
00:13:30,500 --> 00:13:33,770
parts in the crease but it provides an

301
00:13:33,770 --> 00:13:35,690
antler based grammar and while

302
00:13:35,690 --> 00:13:37,970
experimenting we found that the antler

303
00:13:37,970 --> 00:13:41,029
grammar is a bit too low-level so we

304
00:13:41,029 --> 00:13:43,220
kept looking and eventually found

305
00:13:43,220 --> 00:13:45,920
something called the slicer software

306
00:13:45,920 --> 00:13:48,470
architecture workbench which implemented

307
00:13:48,470 --> 00:13:51,649
the open cypher language in Ex text now

308
00:13:51,649 --> 00:13:54,110
X text is a necklace project based on

309
00:13:54,110 --> 00:13:57,050
top of antler and it provides a really

310
00:13:57,050 --> 00:14:00,490
nice and high-level way for accessing

311
00:14:00,490 --> 00:14:03,560
the parse tree that's built from the

312
00:14:03,560 --> 00:14:08,149
query specification so we use slices xx

313
00:14:08,149 --> 00:14:10,399
language and then we build the

314
00:14:10,399 --> 00:14:13,220
relational algebra model through a code

315
00:14:13,220 --> 00:14:16,100
implemented in extent which is as you

316
00:14:16,100 --> 00:14:18,080
have probably guessed technology that's

317
00:14:18,080 --> 00:14:20,810
built on top of X text so this is a

318
00:14:20,810 --> 00:14:23,630
technology that allows us to very

319
00:14:23,630 --> 00:14:26,480
efficiently implement the algebra

320
00:14:26,480 --> 00:14:29,420
builder and Vivier this on top of

321
00:14:29,420 --> 00:14:33,020
Eclipse models so the parser takes this

322
00:14:33,020 --> 00:14:36,080
input and then it builds a simple search

323
00:14:36,080 --> 00:14:38,120
plan which is basically a relational

324
00:14:38,120 --> 00:14:41,300
algebraic expression so now what we have

325
00:14:41,300 --> 00:14:46,279
to do is somehow transform this to Retta

326
00:14:46,279 --> 00:14:47,990
network which allows incremental

327
00:14:47,990 --> 00:14:51,380
computation and deploy it so first we

328
00:14:51,380 --> 00:14:53,839
transform it and add some optimizations

329
00:14:53,839 --> 00:14:57,020
interestingly for the transformation and

330
00:14:57,020 --> 00:14:59,420
the optimization we use viatra which on

331
00:14:59,420 --> 00:15:01,370
its own is an incremental graph query

332
00:15:01,370 --> 00:15:03,470
engine so we use an incremental graph

333
00:15:03,470 --> 00:15:05,510
for engine to drive another increment to

334
00:15:05,510 --> 00:15:09,079
a graph query engine in the end we get a

335
00:15:09,079 --> 00:15:13,279
bit more complex network which is data

336
00:15:13,279 --> 00:15:16,430
network and we use our query deployer

337
00:15:16,430 --> 00:15:20,450
which uses asynchronous technologies and

338
00:15:20,450 --> 00:15:22,279
functional programming like Scala and

339
00:15:22,279 --> 00:15:26,420
the rocket rocket so why do we use these

340
00:15:26,420 --> 00:15:28,970
technologies well if we revisit our

341
00:15:28,970 --> 00:15:31,459
slide on the trading the switch

342
00:15:31,459 --> 00:15:34,790
constraint we find that the retinoids

343
00:15:34,790 --> 00:15:37,070
that perform the computation can be

344
00:15:37,070 --> 00:15:39,550
thought as actors in the act

345
00:15:39,550 --> 00:15:42,250
programming paradigm and actually this

346
00:15:42,250 --> 00:15:45,480
model lends itself very well to the

347
00:15:45,480 --> 00:15:48,160
actor programming paradigm because you

348
00:15:48,160 --> 00:15:50,500
can think of the propagation of the

349
00:15:50,500 --> 00:15:52,510
tuples in the network as asynchronous

350
00:15:52,510 --> 00:15:56,470
messages and this works pretty well in

351
00:15:56,470 --> 00:15:59,170
in practice we did a lot of experiment

352
00:15:59,170 --> 00:16:01,300
with parallel implementations and

353
00:16:01,300 --> 00:16:03,700
distributed implementations and they

354
00:16:03,700 --> 00:16:06,370
scale quite well but they are pretty

355
00:16:06,370 --> 00:16:08,560
difficult to set up so currently in

356
00:16:08,560 --> 00:16:12,370
graph is a single node tool but it in

357
00:16:12,370 --> 00:16:14,980
theory can work on multiple computers as

358
00:16:14,980 --> 00:16:19,329
well so one of the challenge that was

359
00:16:19,329 --> 00:16:21,399
pretty difficult is to get properties

360
00:16:21,399 --> 00:16:24,730
right because traditionally Reta uses

361
00:16:24,730 --> 00:16:27,579
simple tuples you have primitives like

362
00:16:27,579 --> 00:16:31,630
IDs string strings boolean's floats and

363
00:16:31,630 --> 00:16:32,380
so on

364
00:16:32,380 --> 00:16:35,440
in contrast here we have something like

365
00:16:35,440 --> 00:16:37,390
t dot number which is a primitive and

366
00:16:37,390 --> 00:16:39,850
then we have SW which is a switch which

367
00:16:39,850 --> 00:16:44,079
is a whole node in the graph so what we

368
00:16:44,079 --> 00:16:47,800
do is we build return at work first and

369
00:16:47,800 --> 00:16:51,850
then run a three-step inferencing first

370
00:16:51,850 --> 00:16:57,450
we say that we infer the schema of the

371
00:16:57,450 --> 00:17:03,010
operators so we start with the schema of

372
00:17:03,010 --> 00:17:07,679
the indexers and then build up the tree

373
00:17:07,679 --> 00:17:10,839
second we start writing down the tree

374
00:17:10,839 --> 00:17:13,449
and gather something called the extra

375
00:17:13,449 --> 00:17:15,280
attributes that will be required for

376
00:17:15,280 --> 00:17:18,069
computation or for returning the clear

377
00:17:18,069 --> 00:17:22,000
results so we gather the numbers and

378
00:17:22,000 --> 00:17:25,150
then we see that we need for the

379
00:17:25,150 --> 00:17:28,059
filtering the position of the switch so

380
00:17:28,059 --> 00:17:30,190
we also note that we need the number in

381
00:17:30,190 --> 00:17:33,040
the switch position we propagate this to

382
00:17:33,040 --> 00:17:35,650
the bottom we notice that we have to

383
00:17:35,650 --> 00:17:38,410
propagate the number to the right and

384
00:17:38,410 --> 00:17:40,600
then the number to the left and then the

385
00:17:40,600 --> 00:17:44,350
position to draw it so like so and then

386
00:17:44,350 --> 00:17:46,750
finally we traverse the tree from the

387
00:17:46,750 --> 00:17:50,140
bottom to the top and infer the schemas

388
00:17:50,140 --> 00:17:51,340
again

389
00:17:51,340 --> 00:17:54,700
and then we have the schemas and this

390
00:17:54,700 --> 00:17:57,010
comes with the advantage that we don't

391
00:17:57,010 --> 00:18:00,550
have to unnecessarily propagate the

392
00:18:00,550 --> 00:18:03,280
whole nodes and whole relationships up

393
00:18:03,280 --> 00:18:06,520
this tree and this saves a lot of memory

394
00:18:06,520 --> 00:18:10,810
in practice so let's revisit the remodel

395
00:18:10,810 --> 00:18:14,380
as I said in practice we want all of

396
00:18:14,380 --> 00:18:17,200
these queries to be evaluated 24 hours a

397
00:18:17,200 --> 00:18:19,870
day seven days a week so continuously

398
00:18:19,870 --> 00:18:21,010
and all queries

399
00:18:21,010 --> 00:18:23,980
must be evaluated fortunately this is

400
00:18:23,980 --> 00:18:26,020
quite easy with the Rotter network

401
00:18:26,020 --> 00:18:29,530
because we can use the return at work to

402
00:18:29,530 --> 00:18:32,800
check multiple queries so as you noticed

403
00:18:32,800 --> 00:18:34,780
on the right we have the trailing the

404
00:18:34,780 --> 00:18:36,730
switch constraint and on the left we

405
00:18:36,730 --> 00:18:39,730
have a new constraint which takes care

406
00:18:39,730 --> 00:18:42,820
of close proximity so it checks if the

407
00:18:42,820 --> 00:18:46,240
trains are in too close proximity to

408
00:18:46,240 --> 00:18:50,200
each other and as you can notice we have

409
00:18:50,200 --> 00:18:52,360
a feature called node reuse which means

410
00:18:52,360 --> 00:18:55,690
that if we have a node that is used by

411
00:18:55,690 --> 00:18:58,180
multiple parts of the data network then

412
00:18:58,180 --> 00:19:01,390
it can be reused and this saves a lot of

413
00:19:01,390 --> 00:19:03,150
memory if you have a lot of constraints

414
00:19:03,150 --> 00:19:07,900
so the figure also shows that the return

415
00:19:07,900 --> 00:19:10,180
at work is basically a dag directed

416
00:19:10,180 --> 00:19:13,240
acyclic graph and that allows it to

417
00:19:13,240 --> 00:19:16,150
reach a fixed point pretty quickly upon

418
00:19:16,150 --> 00:19:19,240
evaluation so how does this return

419
00:19:19,240 --> 00:19:21,580
Network work in plaque practice we have

420
00:19:21,580 --> 00:19:23,250
another animated slide for that

421
00:19:23,250 --> 00:19:26,590
basically we have a bit more indexers

422
00:19:26,590 --> 00:19:31,720
and then we evaluate the projections we

423
00:19:31,720 --> 00:19:33,970
calculate the join we now find that

424
00:19:33,970 --> 00:19:37,810
train 1 & 2 are too close together so we

425
00:19:37,810 --> 00:19:39,850
issue a warning to the user that 1 & 2

426
00:19:39,850 --> 00:19:42,760
are too close together 1 is on a and 2

427
00:19:42,760 --> 00:19:47,050
is on D so some time passes and our

428
00:19:47,050 --> 00:19:50,170
train moves from segment D to segment E

429
00:19:50,170 --> 00:19:52,180
and this shows the advantage of the

430
00:19:52,180 --> 00:19:53,740
rottenness work because now we only have

431
00:19:53,740 --> 00:19:56,320
to propagate the changes to the indexer

432
00:19:56,320 --> 00:20:01,630
we change the 2d data to weep and then

433
00:20:01,630 --> 00:20:04,570
we have no results on the dangerous

434
00:20:04,570 --> 00:20:05,170
proximate

435
00:20:05,170 --> 00:20:08,430
results but we will have some results in

436
00:20:08,430 --> 00:20:10,660
trailing the switch because this train

437
00:20:10,660 --> 00:20:13,270
is about to hit the switch that's set to

438
00:20:13,270 --> 00:20:16,120
a divergent position and obviously as I

439
00:20:16,120 --> 00:20:18,220
said this is a maneuver that's illegal

440
00:20:18,220 --> 00:20:23,170
array systems so I would like to

441
00:20:23,170 --> 00:20:26,530
highlight some use cases we have a model

442
00:20:26,530 --> 00:20:28,420
validation framework called the Train

443
00:20:28,420 --> 00:20:30,550
benchmark which is also built around

444
00:20:30,550 --> 00:20:33,670
railway network but instead of doing

445
00:20:33,670 --> 00:20:36,820
runtime checks we do checks during the

446
00:20:36,820 --> 00:20:37,750
design of it

447
00:20:37,750 --> 00:20:40,810
we have a static analysis framework for

448
00:20:40,810 --> 00:20:42,310
JavaScript source code that I will talk

449
00:20:42,310 --> 00:20:44,730
about in a minute and also I think that

450
00:20:44,730 --> 00:20:46,750
incremental queries can be beneficial

451
00:20:46,750 --> 00:20:49,060
for all use cases very have standing

452
00:20:49,060 --> 00:20:51,670
queries on a large and continuously

453
00:20:51,670 --> 00:20:54,370
changing graph so if you have a fraud

454
00:20:54,370 --> 00:20:56,070
graph and you would like to detect

455
00:20:56,070 --> 00:21:00,250
fraudulent drinks or users that are

456
00:21:00,250 --> 00:21:02,230
doing fraudulent behavior then

457
00:21:02,230 --> 00:21:04,600
incremental graph queries can be very

458
00:21:04,600 --> 00:21:06,760
useful also if you have a huge IT

459
00:21:06,760 --> 00:21:09,010
infrastructure that can also be modeled

460
00:21:09,010 --> 00:21:11,440
as a graph and obviously it's changing

461
00:21:11,440 --> 00:21:13,540
very quickly and you would like to have

462
00:21:13,540 --> 00:21:17,110
a lot of warnings in case something goes

463
00:21:17,110 --> 00:21:18,390
wrong

464
00:21:18,390 --> 00:21:22,510
so the first use case the terrain

465
00:21:22,510 --> 00:21:25,650
benchmark braveiy validation is

466
00:21:25,650 --> 00:21:28,420
basically a benchmark paper with the

467
00:21:28,420 --> 00:21:31,030
implementation available on github we

468
00:21:31,030 --> 00:21:34,140
implemented a set of scalable

469
00:21:34,140 --> 00:21:36,850
generators we can generate graphs in the

470
00:21:36,850 --> 00:21:38,920
Eclipse modeling framework property

471
00:21:38,920 --> 00:21:42,400
graphs the semantics verbs RDF format

472
00:21:42,400 --> 00:21:45,330
and sequel for relational databases and

473
00:21:45,330 --> 00:21:47,850
the benchmark defines a set of

474
00:21:47,850 --> 00:21:50,770
validation queries that must be enforced

475
00:21:50,770 --> 00:21:53,350
during the design of the river network

476
00:21:53,350 --> 00:21:55,510
we implemented this for more than a

477
00:21:55,510 --> 00:21:57,850
dozen tools and implemented automated

478
00:21:57,850 --> 00:22:00,490
visualization and reporting stuff so if

479
00:22:00,490 --> 00:22:02,380
you have graph query engine that you

480
00:22:02,380 --> 00:22:04,720
would like to use on large and

481
00:22:04,720 --> 00:22:06,970
continuously changing graphs it's worth

482
00:22:06,970 --> 00:22:09,310
giving a try to the Train benchmark

483
00:22:09,310 --> 00:22:13,030
system another use case is the static

484
00:22:13,030 --> 00:22:16,960
analysis of JavaScript applications this

485
00:22:16,960 --> 00:22:18,520
is a very simple example

486
00:22:18,520 --> 00:22:22,240
you have a division by zero in your

487
00:22:22,240 --> 00:22:24,940
source code and you can detect this by

488
00:22:24,940 --> 00:22:28,600
building a syntax tree and doing pattern

489
00:22:28,600 --> 00:22:31,420
matching on the syntax tree so basically

490
00:22:31,420 --> 00:22:33,760
this cipher query allows you to catch

491
00:22:33,760 --> 00:22:36,490
division by zeros in your source code

492
00:22:36,490 --> 00:22:38,770
now obviously this is a very simple

493
00:22:38,770 --> 00:22:42,790
example so in practice you have much

494
00:22:42,790 --> 00:22:44,680
more power when you have your whole

495
00:22:44,680 --> 00:22:47,470
repository represented as a graph you

496
00:22:47,470 --> 00:22:49,900
can do stuff like that code detection in

497
00:22:49,900 --> 00:22:51,940
your whole repository you can do type

498
00:22:51,940 --> 00:22:53,620
inference saying if you don't use

499
00:22:53,620 --> 00:22:56,740
typescript or any other languages that

500
00:22:56,740 --> 00:22:59,650
allow static typing for JavaScript and

501
00:22:59,650 --> 00:23:02,260
then you can use these for generating

502
00:23:02,260 --> 00:23:06,670
unit tests for your code so until now we

503
00:23:06,670 --> 00:23:09,010
formalize most of the standard open

504
00:23:09,010 --> 00:23:12,580
cipher language and gathered our

505
00:23:12,580 --> 00:23:14,440
findings to a technical report and

506
00:23:14,440 --> 00:23:18,100
implemented research prototype for the

507
00:23:18,100 --> 00:23:21,730
Raven model validation queries in this

508
00:23:21,730 --> 00:23:25,270
quarter we plan to use the TCK tests

509
00:23:25,270 --> 00:23:28,270
which are provided with open cipher this

510
00:23:28,270 --> 00:23:30,130
will allow us to increase the coverage

511
00:23:30,130 --> 00:23:32,980
of our mapping and probably will also

512
00:23:32,980 --> 00:23:35,470
reveal a lot of bugs in our parser and

513
00:23:35,470 --> 00:23:38,380
in our query engine and once we are done

514
00:23:38,380 --> 00:23:42,700
with that we will implement a more

515
00:23:42,700 --> 00:23:45,390
sophisticated optimizer using

516
00:23:45,390 --> 00:23:48,040
technologies from the Apache stack

517
00:23:48,040 --> 00:23:50,230
probably spark catalyst or the calcite

518
00:23:50,230 --> 00:23:52,960
system and we plan to publish our

519
00:23:52,960 --> 00:23:55,570
benchmark results in the top tire

520
00:23:55,570 --> 00:23:57,060
academic conference

521
00:23:57,060 --> 00:24:00,820
so to summarize I presented some open

522
00:24:00,820 --> 00:24:03,070
source projects the engraft project

523
00:24:03,070 --> 00:24:05,710
which allows you to evaluate incremental

524
00:24:05,710 --> 00:24:09,490
graph queries in open cipher there is a

525
00:24:09,490 --> 00:24:11,350
train - Marc system that allows you to

526
00:24:11,350 --> 00:24:15,280
benchmark such queries on realistic

527
00:24:15,280 --> 00:24:17,860
large data sets and then we also have a

528
00:24:17,860 --> 00:24:19,750
very interesting demonstrator called the

529
00:24:19,750 --> 00:24:21,790
most free projects which is the

530
00:24:21,790 --> 00:24:24,070
model-based demonstrator for smart and

531
00:24:24,070 --> 00:24:27,790
safe systems this is a demonstrator that

532
00:24:27,790 --> 00:24:29,680
revolves around the idea of a

533
00:24:29,680 --> 00:24:32,170
cyber-physical system and it uses a

534
00:24:32,170 --> 00:24:32,500
model

535
00:24:32,500 --> 00:24:35,800
and a lot of real-time computation to

536
00:24:35,800 --> 00:24:37,780
make sure that the mother of a reveille

537
00:24:37,780 --> 00:24:40,570
works safely these are all available and

538
00:24:40,570 --> 00:24:43,030
github that continues built and licensed

539
00:24:43,030 --> 00:24:45,760
under the eclipse public license thank

540
00:24:45,760 --> 00:24:47,420
you for your attention

541
00:24:47,420 --> 00:24:55,339
[Applause]

542
00:24:56,790 --> 00:25:22,290
okay yeah so the question was that do we

543
00:25:22,290 --> 00:25:25,180
use a cost-based optimizer do can we

544
00:25:25,180 --> 00:25:27,460
change the query plan upon execution or

545
00:25:27,460 --> 00:25:30,820
do we some statistics and this is a very

546
00:25:30,820 --> 00:25:34,390
good question and the Reta algorithm has

547
00:25:34,390 --> 00:25:38,320
some very good properties with this

548
00:25:38,320 --> 00:25:41,230
respect because once you have your query

549
00:25:41,230 --> 00:25:43,870
you can determine which graph nodes and

550
00:25:43,870 --> 00:25:45,460
graph edges you will need for

551
00:25:45,460 --> 00:25:47,650
computation and you can build only the

552
00:25:47,650 --> 00:25:49,330
lower level you can build the indexers

553
00:25:49,330 --> 00:25:53,380
from your current model and you can use

554
00:25:53,380 --> 00:25:55,390
these as an input for your cost-based

555
00:25:55,390 --> 00:25:58,090
optimizer and as I mentioned we plan to

556
00:25:58,090 --> 00:26:00,220
use the SPARC catalyst framework I

557
00:26:00,220 --> 00:26:02,050
started implementing a cost-based

558
00:26:02,050 --> 00:26:04,990
optimizer on my own and it turned out to

559
00:26:04,990 --> 00:26:08,470
be very very extremely difficult so we

560
00:26:08,470 --> 00:26:10,240
thought it might be a good idea to use

561
00:26:10,240 --> 00:26:13,240
sparks or the calcite frameworks

562
00:26:13,240 --> 00:26:16,180
optimizer because obviously these are

563
00:26:16,180 --> 00:26:19,780
more sophisticated and regarding the

564
00:26:19,780 --> 00:26:22,330
dynamic reconfiguration that's something

565
00:26:22,330 --> 00:26:24,700
that you can theoretically do with the

566
00:26:24,700 --> 00:26:26,650
right algorithm we had some experiment

567
00:26:26,650 --> 00:26:30,640
with that we have a system that does a

568
00:26:30,640 --> 00:26:33,310
design space exploration which is

569
00:26:33,310 --> 00:26:36,460
basically a search but it allows you to

570
00:26:36,460 --> 00:26:38,530
dynamically reconfigure your system so

571
00:26:38,530 --> 00:26:41,410
design space exploration we'll look for

572
00:26:41,410 --> 00:26:43,210
a better solution and then it will also

573
00:26:43,210 --> 00:26:46,330
give you the steps that you need to take

574
00:26:46,330 --> 00:26:48,190
transform your system to another

575
00:26:48,190 --> 00:26:51,340
representation this is something that is

576
00:26:51,340 --> 00:26:52,960
very interesting from an academic

577
00:26:52,960 --> 00:26:55,840
perspective but it's super difficult to

578
00:26:55,840 --> 00:26:58,540
get right in the real system so this is

579
00:26:58,540 --> 00:27:01,680
something that works just in theory yet

580
00:27:01,680 --> 00:27:16,990
okay next one yes there is a lot of

581
00:27:16,990 --> 00:27:21,520
truth to it and basically this is very

582
00:27:21,520 --> 00:27:22,930
useful if you want to maintain

583
00:27:22,930 --> 00:27:26,530
incremental views so if you have as I

584
00:27:26,530 --> 00:27:28,540
said at the beginning the main

585
00:27:28,540 --> 00:27:32,230
difference between the so called batch

586
00:27:32,230 --> 00:27:34,150
and incremental queries is that one is

587
00:27:34,150 --> 00:27:36,580
more pool based and the other one is

588
00:27:36,580 --> 00:27:39,070
push based this is a very nice fit for

589
00:27:39,070 --> 00:27:47,920
reactive applications I think yes yes so

590
00:27:47,920 --> 00:27:51,270
basically you have actors and obviously

591
00:27:51,270 --> 00:27:53,590
unfortunately the actor model allows you

592
00:27:53,590 --> 00:27:57,190
to store mutable state in the actor so

593
00:27:57,190 --> 00:28:00,250
this state is mutable but between the

594
00:28:00,250 --> 00:28:02,680
actors all the messages are immutable so

595
00:28:02,680 --> 00:28:04,630
you do immutable message-passing and

596
00:28:04,630 --> 00:28:33,790
this allows you to okay so the question

597
00:28:33,790 --> 00:28:37,900
is that we if we took advantage that the

598
00:28:37,900 --> 00:28:39,790
rave a network is static and only the

599
00:28:39,790 --> 00:28:43,050
trains move around the rave a network so

600
00:28:43,050 --> 00:28:45,610
basically this didn't really take

601
00:28:45,610 --> 00:28:48,900
advantage of that you can quite easily

602
00:28:48,900 --> 00:28:52,960
do it with the whole pretty much fully

603
00:28:52,960 --> 00:28:55,990
dynamic model but there are a lot of use

604
00:28:55,990 --> 00:29:00,290
cases that look like

605
00:29:00,290 --> 00:29:02,059
like the following you have a big set of

606
00:29:02,059 --> 00:29:04,669
static data you have a smart city and

607
00:29:04,669 --> 00:29:05,690
you have the buildings and the

608
00:29:05,690 --> 00:29:06,950
infrastructure and they don't really

609
00:29:06,950 --> 00:29:08,840
move anywhere and then you have the

610
00:29:08,840 --> 00:29:11,240
dynamic stuff like vehicles like weather

611
00:29:11,240 --> 00:29:12,679
people moving around

612
00:29:12,679 --> 00:29:16,549
Retta is I think quite a good fit for

613
00:29:16,549 --> 00:29:18,380
this but this is not a requirement this

614
00:29:18,380 --> 00:29:20,960
is just something that works really

615
00:29:20,960 --> 00:29:26,530
values we disable them I have a question

616
00:29:26,530 --> 00:29:28,640
one thing that I could imagine it's

617
00:29:28,640 --> 00:29:31,220
quite difficult to do is aggregation so

618
00:29:31,220 --> 00:29:33,290
if you create continuations because you

619
00:29:33,290 --> 00:29:35,299
have to kind of be able to track back

620
00:29:35,299 --> 00:29:36,740
where the aggregations originated from

621
00:29:36,740 --> 00:29:43,910
maybe if you're lossy yeah actually

622
00:29:43,910 --> 00:29:45,799
actually this is listed in the

623
00:29:45,799 --> 00:29:51,740
challenges in the open site for stuff

624
00:29:51,740 --> 00:29:53,000
somewhere between the lines

625
00:29:53,000 --> 00:29:55,160
so basically aggregation yeah this

626
00:29:55,160 --> 00:29:58,700
collection and lists and basically even

627
00:29:58,700 --> 00:30:01,070
if you want to maintain something like a

628
00:30:01,070 --> 00:30:03,410
minimum or a maximum incrementally you

629
00:30:03,410 --> 00:30:05,929
have to store all of the possible

630
00:30:05,929 --> 00:30:07,880
candidates so all of the interim results

631
00:30:07,880 --> 00:30:10,929
and then you can you can serve this

632
00:30:10,929 --> 00:30:12,650
obviously you can store it to the heap

633
00:30:12,650 --> 00:30:16,510
or in any efficient data structure but

634
00:30:16,510 --> 00:30:18,590
aggregations are quite difficult and

635
00:30:18,590 --> 00:30:20,390
they are usually not supported by rotary

636
00:30:20,390 --> 00:30:24,100
engines okay

