1
00:00:11,180 --> 00:00:18,510
to introduce a putz in my fight all

2
00:00:18,510 --> 00:00:20,310
right thank you thank you for to FOSDEM

3
00:00:20,310 --> 00:00:21,810
for having me here I apologize for my

4
00:00:21,810 --> 00:00:23,490
appearance I'm from LA and I forgot that

5
00:00:23,490 --> 00:00:26,040
it rained anywhere else in the world so

6
00:00:26,040 --> 00:00:28,260
yeah I am Angela presto I work at Horton

7
00:00:28,260 --> 00:00:30,120
works on a project management committee

8
00:00:30,120 --> 00:00:32,729
member and a committer on Apache knife I

9
00:00:32,729 --> 00:00:36,000
and the sub-project Apache minify and

10
00:00:36,000 --> 00:00:37,469
we're gonna talk about intelligently

11
00:00:37,469 --> 00:00:41,399
collecting data at the edge so I don't

12
00:00:41,399 --> 00:00:43,019
know if you can quite read this its

13
00:00:43,019 --> 00:00:45,719
introduction both to the application and

14
00:00:45,719 --> 00:00:49,050
to myself I have been working on

15
00:00:49,050 --> 00:00:51,089
security topics for about 13 years

16
00:00:51,089 --> 00:00:54,289
I joined the knife I open-source team

17
00:00:54,289 --> 00:00:58,440
got here as I said from LA and that's a

18
00:00:58,440 --> 00:01:01,829
little bit about me so we'll talk very

19
00:01:01,829 --> 00:01:04,080
briefly about what dataflow is and what

20
00:01:04,080 --> 00:01:05,760
the challenges involved in collecting

21
00:01:05,760 --> 00:01:08,400
the data are I apologize this is a

22
00:01:08,400 --> 00:01:10,259
fairly long presentation that's been

23
00:01:10,259 --> 00:01:11,700
compressed down to combine multiple

24
00:01:11,700 --> 00:01:13,649
things I will stick around afterwards

25
00:01:13,649 --> 00:01:15,030
for any questions but I'm gonna go

26
00:01:15,030 --> 00:01:19,290
through a little bit quickly today it's

27
00:01:19,290 --> 00:01:21,539
very simple right data flow is getting

28
00:01:21,539 --> 00:01:23,520
data from one plate one place one point

29
00:01:23,520 --> 00:01:26,549
to another we have producers so that

30
00:01:26,549 --> 00:01:28,979
could be computers log files it could be

31
00:01:28,979 --> 00:01:32,270
devices it could be user interaction

32
00:01:32,270 --> 00:01:34,530
send it usually over the Internet and

33
00:01:34,530 --> 00:01:37,049
then somebody or something needs to make

34
00:01:37,049 --> 00:01:40,820
sense of that data but it's hard and

35
00:01:40,820 --> 00:01:43,500
it's not the first time this problem has

36
00:01:43,500 --> 00:01:45,810
tried to be solved and many people have

37
00:01:45,810 --> 00:01:47,969
tried to do it in many different ways so

38
00:01:47,969 --> 00:01:50,609
the classic example of 14 to 15

39
00:01:50,609 --> 00:01:54,750
standards why is moving data effectively

40
00:01:54,750 --> 00:01:56,700
hard because moving data is not that

41
00:01:56,700 --> 00:01:59,429
hard right you can just put a modem

42
00:01:59,429 --> 00:02:01,799
somewhere and you have data flow but

43
00:02:01,799 --> 00:02:03,719
doing it correctly doing it effectively

44
00:02:03,719 --> 00:02:05,369
doing it in a way that makes sense for

45
00:02:05,369 --> 00:02:06,960
the consumers and producers of that data

46
00:02:06,960 --> 00:02:09,840
is very difficult like I said we have

47
00:02:09,840 --> 00:02:11,880
standards we have different formats we

48
00:02:11,880 --> 00:02:13,890
have exactly once delivery right which

49
00:02:13,890 --> 00:02:15,060
if anybody has worked with data before

50
00:02:15,060 --> 00:02:17,099
is a really difficult problem we have

51
00:02:17,099 --> 00:02:18,090
the protocol

52
00:02:18,090 --> 00:02:19,200
the veracity and validity of the

53
00:02:19,200 --> 00:02:22,530
information ensuring security over

54
00:02:22,530 --> 00:02:24,390
comings of security which as a security

55
00:02:24,390 --> 00:02:25,950
person I object to this completely

56
00:02:25,950 --> 00:02:28,650
accurate statement compliance schemas

57
00:02:28,650 --> 00:02:32,720
etc etc and again exactly once delivery

58
00:02:32,720 --> 00:02:35,700
so we're gonna connect a to B to C and

59
00:02:35,700 --> 00:02:36,629
it's very simple and straightforward

60
00:02:36,629 --> 00:02:38,879
we're gonna read log files off of the

61
00:02:38,879 --> 00:02:40,560
system we're gonna ingest them into

62
00:02:40,560 --> 00:02:43,260
sequel we're going to collect all that

63
00:02:43,260 --> 00:02:44,910
data from a bunch of different instances

64
00:02:44,910 --> 00:02:46,980
put it into a data Lake and run all of

65
00:02:46,980 --> 00:02:49,379
our queries and machine learning and

66
00:02:49,379 --> 00:02:51,450
modeling and everything on that data big

67
00:02:51,450 --> 00:02:54,930
data well this is easy right you write a

68
00:02:54,930 --> 00:02:56,310
bash script to put onto every computer

69
00:02:56,310 --> 00:02:58,500
you have a little bit of Python and you

70
00:02:58,500 --> 00:03:01,459
can script it together and there you go

71
00:03:01,459 --> 00:03:04,799
let's look at a model of courier service

72
00:03:04,799 --> 00:03:06,390
so I guess DHL is the mid 1 over here

73
00:03:06,390 --> 00:03:09,090
right we've got mobile devices everybody

74
00:03:09,090 --> 00:03:10,349
has a scanner that they're scanning the

75
00:03:10,349 --> 00:03:12,209
packages doesn't deliver them there are

76
00:03:12,209 --> 00:03:14,310
registers in the store where we're

77
00:03:14,310 --> 00:03:17,129
accepting and delivery parcels

78
00:03:17,129 --> 00:03:19,680
we have trucks we have delivers and all

79
00:03:19,680 --> 00:03:21,209
this goes to a gateway server that's in

80
00:03:21,209 --> 00:03:22,859
each store and it goes up to a

81
00:03:22,859 --> 00:03:24,209
distribution center where we have a

82
00:03:24,209 --> 00:03:27,180
cluster and we even have a data center

83
00:03:27,180 --> 00:03:29,190
right professional data center and we

84
00:03:29,190 --> 00:03:30,720
can do all of our machine learning and

85
00:03:30,720 --> 00:03:35,880
Big Data there so great I could get all

86
00:03:35,880 --> 00:03:38,190
this data now what I'm going to do with

87
00:03:38,190 --> 00:03:40,260
it well I'm gonna throw it into Kafka

88
00:03:40,260 --> 00:03:42,299
and use that to send it to storm and

89
00:03:42,299 --> 00:03:43,980
spark and flink and apex and anything

90
00:03:43,980 --> 00:03:46,280
else that's on Apaches homepage today

91
00:03:46,280 --> 00:03:48,690
you you can send it to any other

92
00:03:48,690 --> 00:03:50,340
consumer right I mean somebody is gonna

93
00:03:50,340 --> 00:03:51,750
want as soon as you start collecting

94
00:03:51,750 --> 00:03:53,430
data somebody's gonna want it ok that's

95
00:03:53,430 --> 00:03:56,220
a universal truth usually they want the

96
00:03:56,220 --> 00:03:57,810
data you don't have yet that's even

97
00:03:57,810 --> 00:04:01,620
harder so now let's scale this up

98
00:04:01,620 --> 00:04:02,940
because we're going to be an

99
00:04:02,940 --> 00:04:05,220
international courrier service and we

100
00:04:05,220 --> 00:04:08,459
have people and computers on every

101
00:04:08,459 --> 00:04:09,870
different continent every different

102
00:04:09,870 --> 00:04:11,940
corner of the globe raise your hand if

103
00:04:11,940 --> 00:04:13,169
you want to maintain the Python scripts

104
00:04:13,169 --> 00:04:17,760
for that for the rest of your life so

105
00:04:17,760 --> 00:04:19,320
let's talk about Apache knife I I've

106
00:04:19,320 --> 00:04:20,910
definitely stated some problems let's

107
00:04:20,910 --> 00:04:24,210
try and solve some knife I is an open

108
00:04:24,210 --> 00:04:26,130
source project part of the Apache

109
00:04:26,130 --> 00:04:29,250
Software Foundation it provides a ton of

110
00:04:29,250 --> 00:04:30,830
features that

111
00:04:30,830 --> 00:04:32,120
or we're gonna be very important for

112
00:04:32,120 --> 00:04:34,220
your data flow and data collection data

113
00:04:34,220 --> 00:04:38,479
buffering is huge now if I comes from an

114
00:04:38,479 --> 00:04:41,780
area where the importance of data can't

115
00:04:41,780 --> 00:04:44,870
be overstated and protecting that data

116
00:04:44,870 --> 00:04:47,449
but also not losing that data is really

117
00:04:47,449 --> 00:04:49,580
really important one of the features

118
00:04:49,580 --> 00:04:51,560
that I probably won't get into in too

119
00:04:51,560 --> 00:04:53,569
much detail but backpressure allows you

120
00:04:53,569 --> 00:04:56,419
to provide custom configurations so that

121
00:04:56,419 --> 00:04:58,849
if some piece of your chain starts

122
00:04:58,849 --> 00:05:00,740
slowing down it can actually send

123
00:05:00,740 --> 00:05:02,870
information to the predecessors and

124
00:05:02,870 --> 00:05:04,759
inform them of that information and

125
00:05:04,759 --> 00:05:07,370
allow them to either prioritize the data

126
00:05:07,370 --> 00:05:08,900
that you need and stop sending

127
00:05:08,900 --> 00:05:10,009
everything that's just first-in

128
00:05:10,009 --> 00:05:10,430
first-out

129
00:05:10,430 --> 00:05:13,250
it can start sending it to a different

130
00:05:13,250 --> 00:05:15,979
flow and backing it up putting it to

131
00:05:15,979 --> 00:05:17,780
buffer all kinds of things you can do

132
00:05:17,780 --> 00:05:19,340
with it that's really valuable

133
00:05:19,340 --> 00:05:21,470
like I said prioritize queuing we have

134
00:05:21,470 --> 00:05:22,819
quality of service that can be

135
00:05:22,819 --> 00:05:24,800
customized based on the flow data

136
00:05:24,800 --> 00:05:26,270
provenance is something that I will get

137
00:05:26,270 --> 00:05:28,610
into a little bit more detail we support

138
00:05:28,610 --> 00:05:31,699
push and pull models and it's a visual

139
00:05:31,699 --> 00:05:34,039
tool it's it's not something where you

140
00:05:34,039 --> 00:05:35,780
have to have a masters in order to start

141
00:05:35,780 --> 00:05:37,250
the thing up it's something that anybody

142
00:05:37,250 --> 00:05:39,889
with domain knowledge can use which

143
00:05:39,889 --> 00:05:42,409
really makes it easy to get the right

144
00:05:42,409 --> 00:05:44,509
people involved right because you're not

145
00:05:44,509 --> 00:05:46,310
always gonna be able to say everybody

146
00:05:46,310 --> 00:05:48,289
who has valuable insight to this problem

147
00:05:48,289 --> 00:05:50,240
is able to run the terminal and do

148
00:05:50,240 --> 00:05:52,009
everything there right so we're trying

149
00:05:52,009 --> 00:05:53,150
to open it up and make sure that you get

150
00:05:53,150 --> 00:05:54,380
the right people involved in your

151
00:05:54,380 --> 00:05:59,050
process not by originated with

152
00:05:59,050 --> 00:06:00,979
consequent flow based programming

153
00:06:00,979 --> 00:06:04,699
so there is some vocabulary basically a

154
00:06:04,699 --> 00:06:06,620
glossary here you'll hear me talk about

155
00:06:06,620 --> 00:06:08,900
flow files that's basically your atomic

156
00:06:08,900 --> 00:06:10,279
unit of data okay that's the thing

157
00:06:10,279 --> 00:06:12,139
that's moving through your system flow

158
00:06:12,139 --> 00:06:14,029
file processor is some component some

159
00:06:14,029 --> 00:06:16,580
black box it doesn't matter what the

160
00:06:16,580 --> 00:06:18,169
internal implementation is it's

161
00:06:18,169 --> 00:06:20,180
something that you can connect data in

162
00:06:20,180 --> 00:06:24,050
to get data out of and operate obviously

163
00:06:24,050 --> 00:06:25,090
in order to do that you need connections

164
00:06:25,090 --> 00:06:27,770
flow controller is essentially a

165
00:06:27,770 --> 00:06:30,620
scheduler right it's the the overarching

166
00:06:30,620 --> 00:06:32,210
system that's running all of this and

167
00:06:32,210 --> 00:06:33,650
orchestrating it and then a process

168
00:06:33,650 --> 00:06:35,930
group is basically just a collection of

169
00:06:35,930 --> 00:06:38,419
these processors and connections

170
00:06:38,419 --> 00:06:41,830
logically grouped together

171
00:06:42,379 --> 00:06:45,029
so knife I use completely data agnostic

172
00:06:45,029 --> 00:06:47,009
we really don't care what you're using

173
00:06:47,009 --> 00:06:49,319
it for but it was designed understanding

174
00:06:49,319 --> 00:06:51,089
that users have to care about the

175
00:06:51,089 --> 00:06:54,149
specifics and it allows you to do a lot

176
00:06:54,149 --> 00:06:56,369
of transformation or manipulation of the

177
00:06:56,369 --> 00:07:01,649
formats and protocols within the tool so

178
00:07:01,649 --> 00:07:04,619
a really good example to understand a

179
00:07:04,619 --> 00:07:07,229
flow file is to think of it using the

180
00:07:07,229 --> 00:07:10,649
analogy of an HTTP data all right so

181
00:07:10,649 --> 00:07:13,080
this is an HTTP response you can see

182
00:07:13,080 --> 00:07:14,369
that there's a header and then there's

183
00:07:14,369 --> 00:07:17,159
some content below a flow file is very

184
00:07:17,159 --> 00:07:20,789
similar we have attributes which is key

185
00:07:20,789 --> 00:07:23,039
value mapping and then we have the

186
00:07:23,039 --> 00:07:25,439
binary content of the flow file and the

187
00:07:25,439 --> 00:07:26,729
reason that this is important to be

188
00:07:26,729 --> 00:07:28,769
separated logically is that the

189
00:07:28,769 --> 00:07:31,139
attributes are maintained in memory

190
00:07:31,139 --> 00:07:33,959
they're accessible all the time they're

191
00:07:33,959 --> 00:07:35,819
usually pretty small right they're used

192
00:07:35,819 --> 00:07:36,959
for routing they're used for

193
00:07:36,959 --> 00:07:39,659
classification tagging access control

194
00:07:39,659 --> 00:07:41,969
the content could be anything from a

195
00:07:41,969 --> 00:07:44,699
couple bites of text to ten gigs of

196
00:07:44,699 --> 00:07:47,759
video okay when you're routing data you

197
00:07:47,759 --> 00:07:48,899
really don't want to be moving all of

198
00:07:48,899 --> 00:07:50,399
that through the heap constantly right

199
00:07:50,399 --> 00:07:52,289
so what we do is we've split this into

200
00:07:52,289 --> 00:07:54,959
two different storage capacities one is

201
00:07:54,959 --> 00:07:56,610
what we call the flow flow file

202
00:07:56,610 --> 00:07:59,039
repository which stores all of these

203
00:07:59,039 --> 00:08:01,379
flow files with their attributes but it

204
00:08:01,379 --> 00:08:04,319
then has similar to a pointer write a

205
00:08:04,319 --> 00:08:06,749
reference pointer it has a claim to

206
00:08:06,749 --> 00:08:08,669
content that's in a content repository

207
00:08:08,669 --> 00:08:11,339
so what we do is everything is dealt

208
00:08:11,339 --> 00:08:13,860
with through streaming interfaces the

209
00:08:13,860 --> 00:08:16,559
data the content is read into a content

210
00:08:16,559 --> 00:08:18,479
repository it's referenced from there

211
00:08:18,479 --> 00:08:19,919
but it means that it's not always on the

212
00:08:19,919 --> 00:08:21,929
heap whereas the attributes are always

213
00:08:21,929 --> 00:08:24,449
available for operation this allows it

214
00:08:24,449 --> 00:08:26,519
to process a ton of data very quickly

215
00:08:26,519 --> 00:08:29,369
and respecting the resources that you

216
00:08:29,369 --> 00:08:30,509
have available I'm not gonna pretend

217
00:08:30,509 --> 00:08:32,549
like you don't need to have resources in

218
00:08:32,549 --> 00:08:34,078
order to do this if you're operating on

219
00:08:34,078 --> 00:08:35,669
that scale of data but it makes it

220
00:08:35,669 --> 00:08:37,919
easier and it's not unnecessarily

221
00:08:37,919 --> 00:08:38,909
wasting the resources that you have

222
00:08:38,909 --> 00:08:42,568
available let's talk about the user

223
00:08:42,568 --> 00:08:45,360
interface we want less I mean I like

224
00:08:45,360 --> 00:08:47,699
this but in general we want less of this

225
00:08:47,699 --> 00:08:51,149
and more of this okay and let's talk

226
00:08:51,149 --> 00:08:52,709
about the your interface I'm actually

227
00:08:52,709 --> 00:08:55,860
gonna go up to the board a little bit

228
00:08:55,860 --> 00:08:58,050
so here we have what's called the

229
00:08:58,050 --> 00:08:59,850
navigation palette and then the

230
00:08:59,850 --> 00:09:02,310
operation palette down here across the

231
00:09:02,310 --> 00:09:04,470
top this this header these are

232
00:09:04,470 --> 00:09:06,450
components which you can drag on to the

233
00:09:06,450 --> 00:09:09,870
flow the canvas on the far right the

234
00:09:09,870 --> 00:09:11,340
hamburger menu I guess we're still

235
00:09:11,340 --> 00:09:13,890
Conniff that allows you to do get into

236
00:09:13,890 --> 00:09:15,690
some of the system maintenance and

237
00:09:15,690 --> 00:09:18,120
operations and then you can see on the

238
00:09:18,120 --> 00:09:20,250
graph itself your components connections

239
00:09:20,250 --> 00:09:30,020
etc this is mostly to illustrate that

240
00:09:30,020 --> 00:09:32,310
people tend to think of data processing

241
00:09:32,310 --> 00:09:35,280
as happening just in some core data

242
00:09:35,280 --> 00:09:37,080
center somewhere right you have a team

243
00:09:37,080 --> 00:09:39,150
that only cares about doing something

244
00:09:39,150 --> 00:09:40,470
with the data they really don't care

245
00:09:40,470 --> 00:09:42,180
about how the data got well they care if

246
00:09:42,180 --> 00:09:43,140
it doesn't actually get there but they

247
00:09:43,140 --> 00:09:44,370
don't care about how you're collecting

248
00:09:44,370 --> 00:09:46,760
the data where it's coming from

249
00:09:46,760 --> 00:09:50,040
on the other hand you have people and

250
00:09:50,040 --> 00:09:51,420
resources out in the field or at the

251
00:09:51,420 --> 00:09:53,190
edge which are doing the data collection

252
00:09:53,190 --> 00:09:56,430
right and not only that there's usually

253
00:09:56,430 --> 00:09:57,720
some other value they're providing or

254
00:09:57,720 --> 00:10:00,000
all the tool wouldn't be there one of

255
00:10:00,000 --> 00:10:01,920
the things that knife eye allows is for

256
00:10:01,920 --> 00:10:05,130
bi-directional data flow what I mean by

257
00:10:05,130 --> 00:10:07,620
that is we're extracting or exfilling

258
00:10:07,620 --> 00:10:09,960
some kind of important data from the

259
00:10:09,960 --> 00:10:11,370
edge and bring it back to a central

260
00:10:11,370 --> 00:10:13,620
resource but we also need to be able to

261
00:10:13,620 --> 00:10:16,740
send commands communication out to the

262
00:10:16,740 --> 00:10:19,530
edge in order to update the flow

263
00:10:19,530 --> 00:10:22,500
prioritize what we're getting ask

264
00:10:22,500 --> 00:10:24,510
somebody to resend something right so

265
00:10:24,510 --> 00:10:26,520
with this bi-directional data what we

266
00:10:26,520 --> 00:10:28,380
call the data plane and then the command

267
00:10:28,380 --> 00:10:31,260
plane or control plane it allows for us

268
00:10:31,260 --> 00:10:33,540
to improve the data collection

269
00:10:33,540 --> 00:10:36,030
make sure it's robust and stable as we

270
00:10:36,030 --> 00:10:41,070
move on we have over 180 processors that

271
00:10:41,070 --> 00:10:42,660
are available as part of the default

272
00:10:42,660 --> 00:10:45,930
knife I installation everything from

273
00:10:45,930 --> 00:10:48,870
sequel and sequel like data stores to

274
00:10:48,870 --> 00:10:51,720
Big Data to all the different edge

275
00:10:51,720 --> 00:10:53,730
formats and protocols that you might

276
00:10:53,730 --> 00:10:54,090
encounter

277
00:10:54,090 --> 00:10:59,370
so listen TCP log readers just file

278
00:10:59,370 --> 00:11:03,780
extractors kafka database integration

279
00:11:03,780 --> 00:11:07,130
all kinds of stuff there

280
00:11:07,599 --> 00:11:11,679
so HTTP email basically if you can think

281
00:11:11,679 --> 00:11:12,999
of it if we haven't written the

282
00:11:12,999 --> 00:11:14,889
processor for it already we can probably

283
00:11:14,889 --> 00:11:16,809
knock one out unless it's the new

284
00:11:16,809 --> 00:11:17,889
version of Kafka which will take some

285
00:11:17,889 --> 00:11:18,219
time

286
00:11:18,219 --> 00:11:22,119
uh-huh we can do a ton of different

287
00:11:22,119 --> 00:11:24,569
operations to that data so usually

288
00:11:24,569 --> 00:11:27,119
manipulating data transforming data

289
00:11:27,119 --> 00:11:29,979
requires some custom knowledge of that

290
00:11:29,979 --> 00:11:31,929
format right I want to go from XML to

291
00:11:31,929 --> 00:11:33,459
JSON well I understand X and I'll have

292
00:11:33,459 --> 00:11:35,259
to understand JSON I may have to

293
00:11:35,259 --> 00:11:36,579
understand different schemas that are

294
00:11:36,579 --> 00:11:40,419
involved here you drag a box on to the

295
00:11:40,419 --> 00:11:43,719
canvas and you have XML to JSON like I'm

296
00:11:43,719 --> 00:11:46,799
it really is that easy

297
00:11:46,799 --> 00:11:49,599
so now let's talk about minify and and I

298
00:11:49,599 --> 00:11:51,369
am blazing through this so I apologize

299
00:11:51,369 --> 00:11:54,539
if anything's getting a little confusing

300
00:11:54,539 --> 00:11:57,339
minify is gonna let us get out to the

301
00:11:57,339 --> 00:12:00,369
very edge so everybody in this room has

302
00:12:00,369 --> 00:12:03,099
a computer that's capable of the

303
00:12:03,099 --> 00:12:04,449
calculations needed to put a man on the

304
00:12:04,449 --> 00:12:07,959
moon 50 years ago right that kind of

305
00:12:07,959 --> 00:12:10,799
data collection data processing is

306
00:12:10,799 --> 00:12:13,389
unprecedented and will only continue to

307
00:12:13,389 --> 00:12:17,579
grow right data lives in the data center

308
00:12:17,579 --> 00:12:20,379
where knife I has has brought it in

309
00:12:20,379 --> 00:12:22,149
transformed it and provided it to

310
00:12:22,149 --> 00:12:24,729
whatever follow-on system exists but the

311
00:12:24,729 --> 00:12:26,979
data doesn't start there and so the

312
00:12:26,979 --> 00:12:29,139
closer we can get to the creation of the

313
00:12:29,139 --> 00:12:32,529
data the better quality will get the

314
00:12:32,529 --> 00:12:34,919
better control over that data will have

315
00:12:34,919 --> 00:12:37,329
we will be able to prioritize it we'll

316
00:12:37,329 --> 00:12:39,129
be able to secure it will get

317
00:12:39,129 --> 00:12:42,189
granularity of provenance of history in

318
00:12:42,189 --> 00:12:44,229
a way that we've never been able to

319
00:12:44,229 --> 00:12:50,919
before why not just put knife I out

320
00:12:50,919 --> 00:12:51,219
there

321
00:12:51,219 --> 00:12:56,409
well knife is big 726 megabytes for last

322
00:12:56,409 --> 00:12:59,499
release okay it's not a tiny thing it is

323
00:12:59,499 --> 00:13:01,149
respectful of the hardware like I said

324
00:13:01,149 --> 00:13:05,469
you can run it on a Raspberry Pi but you

325
00:13:05,469 --> 00:13:07,449
would like to run it on the heaviest

326
00:13:07,449 --> 00:13:09,789
machine you can find right we have a

327
00:13:09,789 --> 00:13:11,709
friend who has a machine running right

328
00:13:11,709 --> 00:13:17,169
now with 768 gigabytes of RAM and it

329
00:13:17,169 --> 00:13:18,300
uses it

330
00:13:18,300 --> 00:13:21,250
but what we'd also like to do is have

331
00:13:21,250 --> 00:13:23,490
minify where we can put that out on to

332
00:13:23,490 --> 00:13:25,840
client liberate library on somebody's

333
00:13:25,840 --> 00:13:30,880
iOS or Android device on every IOT chip

334
00:13:30,880 --> 00:13:33,400
with six legs that you can stick on a

335
00:13:33,400 --> 00:13:34,930
wall somewhere all right

336
00:13:34,930 --> 00:13:41,260
connected car knife eye 726 mix minified

337
00:13:41,260 --> 00:13:47,230
Java is 45 mix okay minify c++ is 700

338
00:13:47,230 --> 00:13:50,470
kilobytes so it's a 1,000 times

339
00:13:50,470 --> 00:13:57,580
improvement on space requirement so one

340
00:13:57,580 --> 00:13:59,650
of the things that we we did all go into

341
00:13:59,650 --> 00:14:01,420
an example here of putting minify on a

342
00:14:01,420 --> 00:14:04,090
connected car this is a project we did

343
00:14:04,090 --> 00:14:07,900
with qualcomm it allows us to tag data

344
00:14:07,900 --> 00:14:14,140
immediately so this is a model of the

345
00:14:14,140 --> 00:14:17,170
network inside of a car and I think it's

346
00:14:17,170 --> 00:14:18,640
still a little new to a lot of people

347
00:14:18,640 --> 00:14:21,520
that your car has computers in it now

348
00:14:21,520 --> 00:14:25,000
and really can't run without them unless

349
00:14:25,000 --> 00:14:29,260
you have a 50 year old car you have the

350
00:14:29,260 --> 00:14:31,720
can bus which is your main network

351
00:14:31,720 --> 00:14:33,940
within the car there's usually Ethernet

352
00:14:33,940 --> 00:14:35,500
onboard there's also like an

353
00:14:35,500 --> 00:14:38,170
interconnect Network and then there's

354
00:14:38,170 --> 00:14:39,640
whatever else has yet to be invented

355
00:14:39,640 --> 00:14:41,860
right Tesla is certainly not stopping on

356
00:14:41,860 --> 00:14:44,140
their cars every car manufacturer is

357
00:14:44,140 --> 00:14:47,920
moving in this direction minify living

358
00:14:47,920 --> 00:14:49,990
in the car I mean literally on that chip

359
00:14:49,990 --> 00:14:53,200
in the central computer of the car can

360
00:14:53,200 --> 00:14:54,280
ingest

361
00:14:54,280 --> 00:14:55,600
all of the data that's flowing across

362
00:14:55,600 --> 00:14:57,760
these different networks so everything

363
00:14:57,760 --> 00:15:00,640
from speed data to break measurements to

364
00:15:00,640 --> 00:15:05,320
GPS process all of that tag it

365
00:15:05,320 --> 00:15:07,750
prioritize it maybe filter stuff out

366
00:15:07,750 --> 00:15:10,540
based on geographic location if you're

367
00:15:10,540 --> 00:15:12,250
operating in China you're not allowed to

368
00:15:12,250 --> 00:15:13,720
send any of that information to a

369
00:15:13,720 --> 00:15:15,730
computer that lives outside of China so

370
00:15:15,730 --> 00:15:19,120
you can't just say oh well Ford has a

371
00:15:19,120 --> 00:15:21,460
data center for the entire world we'll

372
00:15:21,460 --> 00:15:22,780
get all the information in we'll do some

373
00:15:22,780 --> 00:15:24,070
machine learning and modelling there and

374
00:15:24,070 --> 00:15:26,320
then we'll send out our learnings now

375
00:15:26,320 --> 00:15:28,450
you have to have one I think maybe even

376
00:15:28,450 --> 00:15:29,950
now one for Europe one for the u.s. one

377
00:15:29,950 --> 00:15:31,390
for China one for Antarctica whatever

378
00:15:31,390 --> 00:15:32,110
you want to do

379
00:15:32,110 --> 00:15:34,000
so men if I can start routing that

380
00:15:34,000 --> 00:15:35,530
information encrypting that information

381
00:15:35,530 --> 00:15:37,720
prioritizing or filtering it while it's

382
00:15:37,720 --> 00:15:40,570
still on the car taking data off of a

383
00:15:40,570 --> 00:15:43,150
car is very expensive if you are near a

384
00:15:43,150 --> 00:15:46,120
Wi-Fi hotspot the car will prioritize

385
00:15:46,120 --> 00:15:48,760
that and send more data over Wi-Fi but

386
00:15:48,760 --> 00:15:50,140
when you're driving on a highway and

387
00:15:50,140 --> 00:15:51,910
there's somebody around it uses an LTE

388
00:15:51,910 --> 00:15:53,230
modem that's in the car and that's

389
00:15:53,230 --> 00:15:54,850
extremely expensive so you really don't

390
00:15:54,850 --> 00:15:56,440
want to send a bunch of wasted data a

391
00:15:56,440 --> 00:15:57,760
bunch of unnecessary data or

392
00:15:57,760 --> 00:15:59,590
uncompressed data over that if you don't

393
00:15:59,590 --> 00:16:03,640
have to so here's a little diagram the

394
00:16:03,640 --> 00:16:06,220
map on the right and the boxes there are

395
00:16:06,220 --> 00:16:09,280
actually showing the ratio of the data

396
00:16:09,280 --> 00:16:11,170
that's getting exfilled live via LTE

397
00:16:11,170 --> 00:16:14,950
versus Wi-Fi on the left you see the

398
00:16:14,950 --> 00:16:17,350
data flow from knife I and that is

399
00:16:17,350 --> 00:16:19,290
ingesting data off of those networks

400
00:16:19,290 --> 00:16:21,190
processing it filtering it and then

401
00:16:21,190 --> 00:16:23,590
sending it via the radios that are in

402
00:16:23,590 --> 00:16:27,880
the car one of the next things that

403
00:16:27,880 --> 00:16:29,230
we're really focused on developing is

404
00:16:29,230 --> 00:16:32,200
the flow versioning right as you develop

405
00:16:32,200 --> 00:16:34,180
your data flow all of this we'd like to

406
00:16:34,180 --> 00:16:35,470
use the analogy you're not building

407
00:16:35,470 --> 00:16:37,930
pipes you're a farmer digging irrigation

408
00:16:37,930 --> 00:16:39,580
ditches right that water is always

409
00:16:39,580 --> 00:16:41,290
flowing that river is not stopping

410
00:16:41,290 --> 00:16:42,610
because oh I need to update this

411
00:16:42,610 --> 00:16:45,370
processor so what you can do is continue

412
00:16:45,370 --> 00:16:47,320
a data flow build something new

413
00:16:47,320 --> 00:16:49,450
alongside of it make sure that works on

414
00:16:49,450 --> 00:16:50,710
the same data that's coming through

415
00:16:50,710 --> 00:16:52,540
without stopping anything any follow-on

416
00:16:52,540 --> 00:16:54,490
system and then when you have a new flow

417
00:16:54,490 --> 00:16:56,020
that's better you kill the old flow

418
00:16:56,020 --> 00:16:58,030
you've never stopped moving the data

419
00:16:58,030 --> 00:17:00,280
from the source to the the destination

420
00:17:00,280 --> 00:17:01,870
but you've improved your system as

421
00:17:01,870 --> 00:17:05,199
you're doing that unfortunately you

422
00:17:05,199 --> 00:17:06,670
can't prove that and then six weeks

423
00:17:06,670 --> 00:17:08,319
later you find out oh I needed something

424
00:17:08,319 --> 00:17:09,550
from the other flow that I didn't forgot

425
00:17:09,550 --> 00:17:11,319
to check well how do you go back to that

426
00:17:11,319 --> 00:17:12,910
old one so that's where flow versioning

427
00:17:12,910 --> 00:17:14,920
whether it's on the six week time frame

428
00:17:14,920 --> 00:17:17,530
or six seconds right I'm reading from

429
00:17:17,530 --> 00:17:19,599
IOT sources and I need to change the

430
00:17:19,599 --> 00:17:20,770
priority based on the available

431
00:17:20,770 --> 00:17:23,470
bandwidth I might want everything if I

432
00:17:23,470 --> 00:17:26,290
have the bandwidth I might want only top

433
00:17:26,290 --> 00:17:27,910
priority data when the bandwidth starts

434
00:17:27,910 --> 00:17:31,060
to get spotty so I can have my might

435
00:17:31,060 --> 00:17:32,920
control my command control system knife

436
00:17:32,920 --> 00:17:36,190
I write these rules into my flow and

437
00:17:36,190 --> 00:17:38,590
then send them back to minify and not

438
00:17:38,590 --> 00:17:41,230
just one instance event if I all 6.2

439
00:17:41,230 --> 00:17:42,730
million instances of minify they're

440
00:17:42,730 --> 00:17:44,470
running on that flow and that's all

441
00:17:44,470 --> 00:17:45,970
going to be taken care of

442
00:17:45,970 --> 00:17:49,059
seemlessly think I am getting close okay

443
00:17:49,059 --> 00:17:51,940
there's plenty more anybody who's

444
00:17:51,940 --> 00:17:53,259
interested is welcome to come talk to me

445
00:17:53,259 --> 00:17:55,299
I'll hang out outside and thank you very

446
00:17:55,299 --> 00:17:56,290
much for having me

447
00:17:56,290 --> 00:18:03,059
[Applause]

448
00:18:03,059 --> 00:18:11,019
sorry yeah question is it possible to

449
00:18:11,019 --> 00:18:12,580
make cues that are consumer controlled

450
00:18:12,580 --> 00:18:14,500
in knife I I don't understand the

451
00:18:14,500 --> 00:18:18,070
question fully but my answer is yes yes

452
00:18:18,070 --> 00:18:20,409
almost definitely it depends on what

453
00:18:20,409 --> 00:18:22,480
specific consumer format you're talking

454
00:18:22,480 --> 00:18:35,320
about but yes yes knife I will yes it's

455
00:18:35,320 --> 00:18:47,620
the opposite of Kafka yes yes yes yeah

456
00:18:47,620 --> 00:18:49,389
the question was about exactly once

457
00:18:49,389 --> 00:18:53,139
delivery knife I uses what's called our

458
00:18:53,139 --> 00:18:56,169
right ahead log to track all of the data

459
00:18:56,169 --> 00:18:58,919
that's flowing through the system and so

460
00:18:58,919 --> 00:19:01,450
it will guarantee exactly what's

461
00:19:01,450 --> 00:19:04,990
delivery here so that that's actually

462
00:19:04,990 --> 00:19:07,990
this copy-on-write the data is not

463
00:19:07,990 --> 00:19:09,879
manipulated in place the data exists

464
00:19:09,879 --> 00:19:11,919
permanently as long as you have storage

465
00:19:11,919 --> 00:19:13,870
to hold it when it gets sent to a

466
00:19:13,870 --> 00:19:15,669
follow-on system it receives a

467
00:19:15,669 --> 00:19:18,610
confirmation it's a two-phase commit or

468
00:19:18,610 --> 00:19:20,169
signal that the information was received

469
00:19:20,169 --> 00:19:22,840
if it wasn't not if I can replay that

470
00:19:22,840 --> 00:19:26,889
information to the following system yes

471
00:19:26,889 --> 00:19:29,158
and back

472
00:19:35,440 --> 00:19:37,460
I'm sorry can you repeat that a little

473
00:19:37,460 --> 00:19:42,920
louder please is it possible to make

474
00:19:42,920 --> 00:19:44,930
data transformations on the fly yes

475
00:19:44,930 --> 00:19:46,700
that's that's the whole point of knife I

476
00:19:46,700 --> 00:19:48,320
I mean that one of the whole points and

477
00:19:48,320 --> 00:19:51,380
I find so acts like format XML to JSON

478
00:19:51,380 --> 00:19:52,430
yes

479
00:19:52,430 --> 00:19:55,130
CSV to parsing records into different

480
00:19:55,130 --> 00:19:57,340
atomic units yes absolutely

481
00:19:57,340 --> 00:20:06,140
yes the question was performance versus

482
00:20:06,140 --> 00:20:08,000
Kafka or other systems that's a whole

483
00:20:08,000 --> 00:20:09,440
nother talk I mean that we could talk

484
00:20:09,440 --> 00:20:10,910
about that for an hour I'll take it

485
00:20:10,910 --> 00:20:12,260
offline with you if you want absolutely

486
00:20:12,260 --> 00:20:19,790
yes system requirements for minify and

487
00:20:19,790 --> 00:20:22,760
embedded devices the ability to run C

488
00:20:22,760 --> 00:20:28,130
and 700k minified bundles it's a

489
00:20:28,130 --> 00:20:30,350
completely new implementation of the

490
00:20:30,350 --> 00:20:32,030
same system so we have it in both Java

491
00:20:32,030 --> 00:20:35,570
on the JVM and a C implementation bare

492
00:20:35,570 --> 00:20:41,180
metal so Ram I believe it's four

493
00:20:41,180 --> 00:20:46,270
megabytes of RAM yes

494
00:20:52,530 --> 00:20:54,570
the question was schema list versus

495
00:20:54,570 --> 00:20:56,610
schema formats yeah we have plenty of

496
00:20:56,610 --> 00:20:58,620
processors for Avro we can immediately

497
00:20:58,620 --> 00:21:01,020
there's one literally called infer Avro

498
00:21:01,020 --> 00:21:03,480
schema so you bring in arbitrary data

499
00:21:03,480 --> 00:21:05,430
and it will basically build out the

500
00:21:05,430 --> 00:21:07,110
schema from that that Avro and

501
00:21:07,110 --> 00:21:08,070
information

502
00:21:08,070 --> 00:21:24,690
yes sure the question was two-phase

503
00:21:24,690 --> 00:21:28,490
commit right ahead log cluster scaling

504
00:21:28,490 --> 00:21:35,220
okay yeah it scales extremely well it is

505
00:21:35,220 --> 00:21:37,800
built to be a clustered system so

506
00:21:37,800 --> 00:21:40,230
there's an entire cluster coordinator it

507
00:21:40,230 --> 00:21:42,000
uses embedded zookeeper if another

508
00:21:42,000 --> 00:21:44,310
instances and available resource

509
00:21:44,310 --> 00:21:45,540
management in allocation is still

510
00:21:45,540 --> 00:21:46,590
something that we're continuing to work

511
00:21:46,590 --> 00:21:48,870
on and integrating with like Muzo sir

512
00:21:48,870 --> 00:21:51,180
yarn or some other resources manager for

513
00:21:51,180 --> 00:21:54,360
that but knife I will encapsulate the

514
00:21:54,360 --> 00:21:55,590
resource management it has cluster

515
00:21:55,590 --> 00:21:56,730
coordinator heartbeat and all that kind

516
00:21:56,730 --> 00:21:58,980
of stuff as well two-phase commit that's

517
00:21:58,980 --> 00:22:01,200
for the following systems to acknowledge

518
00:22:01,200 --> 00:22:03,360
they've received information the right

519
00:22:03,360 --> 00:22:05,220
ahead log implementation is so that

520
00:22:05,220 --> 00:22:06,810
we're again the copy-on-write so that

521
00:22:06,810 --> 00:22:08,700
it's not manipulated data in place you

522
00:22:08,700 --> 00:22:10,290
don't lose the the record of what that

523
00:22:10,290 --> 00:22:15,950
data was was there another one okay

524
00:22:22,120 --> 00:22:30,989
[Applause]

525
00:22:37,550 --> 00:22:39,750
yeah sorry the question is autonomous

526
00:22:39,750 --> 00:22:41,070
cars is that appropriate for this

527
00:22:41,070 --> 00:22:43,680
I mean morally or just like technically

528
00:22:43,680 --> 00:22:47,870
but technically absolutely yes

