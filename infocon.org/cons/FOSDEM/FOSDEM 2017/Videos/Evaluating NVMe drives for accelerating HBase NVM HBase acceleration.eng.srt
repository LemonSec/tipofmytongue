1
00:00:22,300 --> 00:00:25,339
hi good morning Nicholas Posey from the

2
00:00:25,339 --> 00:00:27,109
Barcelona supercomputing Center and

3
00:00:27,109 --> 00:00:29,509
today I'm going to talk and show some

4
00:00:29,509 --> 00:00:32,210
benchmark results both on the system

5
00:00:32,210 --> 00:00:34,460
level so that the base level and also on

6
00:00:34,460 --> 00:00:36,350
the application side on the application

7
00:00:36,350 --> 00:00:39,110
side applications use cases HBase and

8
00:00:39,110 --> 00:00:42,380
the drives were testing our mb m e mb m

9
00:00:42,380 --> 00:00:45,770
e dr number littell memory express are

10
00:00:45,770 --> 00:00:50,930
basically SSDs on a PCIe bus so that's

11
00:00:50,930 --> 00:00:52,730
what this talk is about this work is a

12
00:00:52,730 --> 00:00:55,370
collaboration between Barcelona

13
00:00:55,370 --> 00:00:58,490
supercomputing and Rackspace u.s. in

14
00:00:58,490 --> 00:01:01,820
this half academic and half industrial

15
00:01:01,820 --> 00:01:03,770
this is the first part of our research

16
00:01:03,770 --> 00:01:07,240
project so we're welcoming feedback and

17
00:01:07,240 --> 00:01:09,560
contributions to to the results you'll

18
00:01:09,560 --> 00:01:11,990
see ok so the outline I'm going to first

19
00:01:11,990 --> 00:01:15,500
introduce BSC and at the illogic why

20
00:01:15,500 --> 00:01:17,630
we're doing this a bit of the motivation

21
00:01:17,630 --> 00:01:20,240
and then we're going to get into system

22
00:01:20,240 --> 00:01:24,170
level benchmark of the nbme devices just

23
00:01:24,170 --> 00:01:26,300
to set expectations of the maximum

24
00:01:26,300 --> 00:01:27,890
performance we can get from the drives

25
00:01:27,890 --> 00:01:29,870
and then we're going to get into the

26
00:01:29,870 --> 00:01:31,580
core of the presentations which are the

27
00:01:31,580 --> 00:01:35,780
HBase benchmarks separated in two parts

28
00:01:35,780 --> 00:01:39,560
one is a read-only workload where we get

29
00:01:39,560 --> 00:01:41,540
the most of the benefits of the

30
00:01:41,540 --> 00:01:43,909
benchmarks and then a mixed workload

31
00:01:43,909 --> 00:01:46,340
where we're actually deleting updating

32
00:01:46,340 --> 00:01:49,880
inserting data into the stuff okay so

33
00:01:49,880 --> 00:01:51,890
the Barcelona supercomputing Center is

34
00:01:51,890 --> 00:01:53,870
the Spanish national supercomputing

35
00:01:53,870 --> 00:01:56,890
facility we have we host the marenostrum

36
00:01:56,890 --> 00:01:59,900
supercomputer this year through the

37
00:01:59,900 --> 00:02:02,030
european commission and spanish göran

38
00:02:02,030 --> 00:02:03,560
we're getting the new version we're in

39
00:02:03,560 --> 00:02:05,720
Austin for were you interested in

40
00:02:05,720 --> 00:02:08,899
supercomputing HPC follow the news will

41
00:02:08,899 --> 00:02:10,489
basically basing the Technical

42
00:02:10,489 --> 00:02:13,629
University of Catalonia in Barcelona and

43
00:02:13,629 --> 00:02:17,900
so most of the people in this end

44
00:02:17,900 --> 00:02:22,909
are either professors students or PhDs

45
00:02:22,909 --> 00:02:28,579
and so we have a very academic truck and

46
00:02:28,579 --> 00:02:31,040
we also have partnerships with industry

47
00:02:31,040 --> 00:02:33,170
players the reason I'm presenting these

48
00:02:33,170 --> 00:02:35,239
resources we've been working in the

49
00:02:35,239 --> 00:02:37,340
Hadoop big data ecosystem since around

50
00:02:37,340 --> 00:02:40,640
2008 first on schedulers and making sure

51
00:02:40,640 --> 00:02:42,890
that a concurrency of finish in time

52
00:02:42,890 --> 00:02:44,450
and for the past year we have been

53
00:02:44,450 --> 00:02:47,359
embarked in a benchmarking project for

54
00:02:47,359 --> 00:02:49,700
Hadoop ecosystem applications

55
00:02:49,700 --> 00:02:53,920
specifically in cloud and HPC

56
00:02:53,920 --> 00:02:58,389
architectures so this a logic project is

57
00:02:58,389 --> 00:03:01,370
the idea is an open data and open source

58
00:03:01,370 --> 00:03:03,909
project and the idea is to automate

59
00:03:03,909 --> 00:03:05,510
characterization of new hardware

60
00:03:05,510 --> 00:03:07,819
deployment an octave an optimized

61
00:03:07,819 --> 00:03:09,650
software configuration and throughout

62
00:03:09,650 --> 00:03:11,150
these three years we have built a

63
00:03:11,150 --> 00:03:13,370
benchmarking platform you can download

64
00:03:13,370 --> 00:03:15,849
and play with it basically does the

65
00:03:15,849 --> 00:03:17,840
provisioning of the clusters either

66
00:03:17,840 --> 00:03:20,989
on-premise or clouds it's setup the

67
00:03:20,989 --> 00:03:22,730
application you need to benchmark for

68
00:03:22,730 --> 00:03:24,829
let's say in this case HBase on Hadoop

69
00:03:24,829 --> 00:03:28,010
and it can run the different tests and

70
00:03:28,010 --> 00:03:29,359
change the configuration between

71
00:03:29,359 --> 00:03:33,290
different test runs and after we run the

72
00:03:33,290 --> 00:03:35,389
benchmarks we collect the results in an

73
00:03:35,389 --> 00:03:37,790
online repository this is available

74
00:03:37,790 --> 00:03:40,190
online so you can browse the results of

75
00:03:40,190 --> 00:03:42,139
the benchmarks and on top of that we're

76
00:03:42,139 --> 00:03:43,930
doing some analytics or we're doing a

77
00:03:43,930 --> 00:03:46,459
performance metric analytics and

78
00:03:46,459 --> 00:03:50,329
high-level metadata learning and also

79
00:03:50,329 --> 00:03:53,599
prediction models on try to find out and

80
00:03:53,599 --> 00:03:56,030
automate how to improve the performance

81
00:03:56,030 --> 00:03:59,750
of the systems we collaborate a lot with

82
00:03:59,750 --> 00:04:02,690
the industry in academia and this work

83
00:04:02,690 --> 00:04:04,699
in particular Rackspace is present and

84
00:04:04,699 --> 00:04:08,329
we got some support from Intel to make

85
00:04:08,329 --> 00:04:09,949
sure that their drives are configured

86
00:04:09,949 --> 00:04:13,190
properly ok so a bit of the motivations

87
00:04:13,190 --> 00:04:14,979
for the results I'm going to show you

88
00:04:14,979 --> 00:04:17,930
here I have some results the actual

89
00:04:17,930 --> 00:04:20,810
numbers are not important but we got a

90
00:04:20,810 --> 00:04:23,449
new cluster with nbme devices that are

91
00:04:23,449 --> 00:04:26,419
supposed to be to very fast and so we

92
00:04:26,419 --> 00:04:28,550
said ok let's run pterosaur terasort is

93
00:04:28,550 --> 00:04:31,700
like the default benchmark for

94
00:04:31,700 --> 00:04:34,220
Hadoop applications and let's run it and

95
00:04:34,220 --> 00:04:37,550
this first bar is lower is better so

96
00:04:37,550 --> 00:04:39,710
this is running time in seconds so this

97
00:04:39,710 --> 00:04:43,280
first bar shows the terasort sorting one

98
00:04:43,280 --> 00:04:46,040
terabyte of data in Hadoop or only on

99
00:04:46,040 --> 00:04:49,010
the nbme drives and we got this number

100
00:04:49,010 --> 00:04:51,770
and then we tried a combination of we

101
00:04:51,770 --> 00:04:54,020
had a j but on this cluster are going to

102
00:04:54,020 --> 00:04:57,710
present later and which I let's use the

103
00:04:57,710 --> 00:05:01,040
J but then disk plus in in MB me drive

104
00:05:01,040 --> 00:05:03,560
we got this number very similar to the

105
00:05:03,560 --> 00:05:05,990
first one of course we have more D's

106
00:05:05,990 --> 00:05:08,330
here what this could happen but then we

107
00:05:08,330 --> 00:05:10,640
we said okay let's only use the J but

108
00:05:10,640 --> 00:05:13,400
and we had the third bar and then let's

109
00:05:13,400 --> 00:05:15,620
say only let's only use five discs of

110
00:05:15,620 --> 00:05:19,280
the j-bot and we had less than ten

111
00:05:19,280 --> 00:05:21,890
percent performance different with only

112
00:05:21,890 --> 00:05:23,930
using the MEMS devices

113
00:05:23,930 --> 00:05:25,760
so we're saying what's going on here

114
00:05:25,760 --> 00:05:28,010
we've been running this code for years

115
00:05:28,010 --> 00:05:30,230
we know how do we set up correctly this

116
00:05:30,230 --> 00:05:33,440
are set up correctly why don't why

117
00:05:33,440 --> 00:05:35,360
aren't we getting a good performance

118
00:05:35,360 --> 00:05:37,520
increase from these drives when we

119
00:05:37,520 --> 00:05:41,510
switch from rotational drive to SSDs and

120
00:05:41,510 --> 00:05:44,690
years back we saw up to 3x so 300%

121
00:05:44,690 --> 00:05:47,960
performance increase so initially we

122
00:05:47,960 --> 00:05:51,650
wanted to see the benefits of using mb

123
00:05:51,650 --> 00:05:54,290
and me drives so our motivation start to

124
00:05:54,290 --> 00:05:56,150
explore use cases for big data where

125
00:05:56,150 --> 00:05:58,900
this this type of drives make sense

126
00:05:58,900 --> 00:06:01,430
we've had this poor initial result so we

127
00:06:01,430 --> 00:06:03,590
contacted Intel and they supplied us a

128
00:06:03,590 --> 00:06:05,450
HBase use case that we tried to

129
00:06:05,450 --> 00:06:08,090
replicate and we also wanted to measure

130
00:06:08,090 --> 00:06:10,820
the possibilities of these drives and

131
00:06:10,820 --> 00:06:15,160
actually get extender platform into

132
00:06:15,160 --> 00:06:17,720
benchmarking the devices not only the

133
00:06:17,720 --> 00:06:20,540
big data clusters and we find out that

134
00:06:20,540 --> 00:06:24,880
it is a challenge to produce big data

135
00:06:24,880 --> 00:06:26,840
application level benchmark that

136
00:06:26,840 --> 00:06:29,810
actually stresses a hardware completely

137
00:06:29,810 --> 00:06:32,150
so the reason for the marginal gains

138
00:06:32,150 --> 00:06:34,640
here is that we this is a very high-end

139
00:06:34,640 --> 00:06:36,740
hardware and maybe the workload is very

140
00:06:36,740 --> 00:06:39,410
small or that the J but is really fast

141
00:06:39,410 --> 00:06:41,900
so let's let's look at that into detail

142
00:06:41,900 --> 00:06:43,120
so the cluster

143
00:06:43,120 --> 00:06:44,660
specification

144
00:06:44,660 --> 00:06:49,570
ah you don't see on the screen so let's

145
00:06:49,570 --> 00:06:53,060
maybe like this okay let's use this

146
00:06:53,060 --> 00:06:56,120
format then so the cluster specification

147
00:06:56,120 --> 00:06:58,760
we have five notes one is the master for

148
00:06:58,760 --> 00:07:03,890
working notes sent os7 128gb soft run

149
00:07:03,890 --> 00:07:07,540
per note the master has like rate and

150
00:07:07,540 --> 00:07:11,840
disk for the data the Oasis is different

151
00:07:11,840 --> 00:07:14,090
than gigabit network uh not the working

152
00:07:14,090 --> 00:07:17,960
notes we have the nbme drives 1.6

153
00:07:17,960 --> 00:07:21,260
terabyte of storage and a jaybo - a

154
00:07:21,260 --> 00:07:29,270
stable of 10-15 rpm Seagate disk the in

155
00:07:29,270 --> 00:07:30,860
the slice you will find later the

156
00:07:30,860 --> 00:07:32,840
reference if you want to see them so the

157
00:07:32,840 --> 00:07:37,270
nbme drives were testing I stay Intel p3

158
00:07:37,270 --> 00:07:40,700
6:08 it's not the newest from this year

159
00:07:40,700 --> 00:07:44,710
but it's from a year back or half back

160
00:07:44,710 --> 00:07:49,580
it promises 5 gigabytes of read both

161
00:07:49,580 --> 00:07:53,030
random and unsecured reads throughput

162
00:07:53,030 --> 00:07:58,730
and a write bandwidth of two gigabytes

163
00:07:58,730 --> 00:08:02,540
per second so five to two Gs per second

164
00:08:02,540 --> 00:08:05,900
and the least the price is around 10k

165
00:08:05,900 --> 00:08:09,200
u.s. dollars but I searched last week

166
00:08:09,200 --> 00:08:12,200
when doing the slides we also have a

167
00:08:12,200 --> 00:08:15,020
second and be a me device this is an

168
00:08:15,020 --> 00:08:17,990
older generation it's an LSI Nitro warp

169
00:08:17,990 --> 00:08:21,380
drive is from 2012 the current market

170
00:08:21,380 --> 00:08:24,890
price for this one is 4k per unit but it

171
00:08:24,890 --> 00:08:26,840
was when it was released it was around

172
00:08:26,840 --> 00:08:31,000
12 K these discs were just using for

173
00:08:31,000 --> 00:08:33,710
verification and validation this is in

174
00:08:33,710 --> 00:08:36,770
another cluster and what's different of

175
00:08:36,770 --> 00:08:39,260
the drives is the first one is PCI III

176
00:08:39,260 --> 00:08:41,390
with eight lanes and the other one is

177
00:08:41,390 --> 00:08:45,560
PCIe - just an older generation okay

178
00:08:45,560 --> 00:08:48,040
let's start with the FIO benchmark if is

179
00:08:48,040 --> 00:08:51,080
flexible i/o benchmarks in my opinion is

180
00:08:51,080 --> 00:08:53,510
one of the best benchmarks to measure

181
00:08:53,510 --> 00:08:55,970
this performance can talk more about

182
00:08:55,970 --> 00:08:57,480
later that and

183
00:08:57,480 --> 00:09:01,019
what we want to do with FAO is first we

184
00:09:01,019 --> 00:09:03,120
want to assert the vendor specs so you

185
00:09:03,120 --> 00:09:05,579
get you buy a disk or you're going to

186
00:09:05,579 --> 00:09:06,959
buy a disk and you get these numbers

187
00:09:06,959 --> 00:09:08,699
from the vendor of what the dis is

188
00:09:08,699 --> 00:09:10,949
supposed to do we want to verify if we

189
00:09:10,949 --> 00:09:12,870
can really achieve these numbers or or

190
00:09:12,870 --> 00:09:15,930
those are only on ideal conditions the

191
00:09:15,930 --> 00:09:18,269
second part is we want to make sure that

192
00:09:18,269 --> 00:09:19,889
we have the hardware set up correctly

193
00:09:19,889 --> 00:09:22,260
for example on this Intel drives we had

194
00:09:22,260 --> 00:09:24,269
to update the firmware to actually get

195
00:09:24,269 --> 00:09:26,610
them performing well or to the to the

196
00:09:26,610 --> 00:09:28,399
most capacity so this is something

197
00:09:28,399 --> 00:09:30,300
important and we also want to set

198
00:09:30,300 --> 00:09:33,029
performance expectations so like let's

199
00:09:33,029 --> 00:09:34,920
look at some results of maximum

200
00:09:34,920 --> 00:09:36,959
bandwidth so this is megabytes per

201
00:09:36,959 --> 00:09:38,820
second that we can achieve in a cluster

202
00:09:38,820 --> 00:09:40,740
so let me guide you through the results

203
00:09:40,740 --> 00:09:44,250
all of the data will be in the slide so

204
00:09:44,250 --> 00:09:46,079
I'm just going to highlight certain

205
00:09:46,079 --> 00:09:49,410
things so each color here is a different

206
00:09:49,410 --> 00:09:52,310
benchmark we start with random read

207
00:09:52,310 --> 00:09:57,360
random right is the orange random right

208
00:09:57,360 --> 00:10:00,180
now this is sequential read is a gray

209
00:10:00,180 --> 00:10:04,110
and run sequential right if the yellow

210
00:10:04,110 --> 00:10:07,410
so the boasts the highest bar here this

211
00:10:07,410 --> 00:10:08,519
is higher is better

212
00:10:08,519 --> 00:10:10,440
this is throughput megabytes per second

213
00:10:10,440 --> 00:10:15,120
and both this Intel drives get close to

214
00:10:15,120 --> 00:10:19,110
5 gigabytes per second bandwidth the

215
00:10:19,110 --> 00:10:22,350
here the random is a bit lower the specs

216
00:10:22,350 --> 00:10:24,329
are the same but we got very close

217
00:10:24,329 --> 00:10:28,110
numbers to the specs and for the right

218
00:10:28,110 --> 00:10:31,019
we get that 2 gigabytes promising in the

219
00:10:31,019 --> 00:10:33,480
Specht with a particular configuration

220
00:10:33,480 --> 00:10:36,149
of FIO so this is with a configuration

221
00:10:36,149 --> 00:10:39,300
that we got the best results is what I'm

222
00:10:39,300 --> 00:10:41,250
showing just to compare that is then

223
00:10:41,250 --> 00:10:45,839
second here we have the SAS j-bot 15 rpm

224
00:10:45,839 --> 00:10:48,149
rotational drive here's the ten disk

225
00:10:48,149 --> 00:10:50,430
combined one thing that we can see is

226
00:10:50,430 --> 00:10:52,949
that the random reads and writes are

227
00:10:52,949 --> 00:10:56,339
much lower than V and B any devices but

228
00:10:56,339 --> 00:10:59,010
the 10 disk together has almost 2

229
00:10:59,010 --> 00:11:02,910
gigabytes gigabit gigabytes throughput

230
00:11:02,910 --> 00:11:05,640
bandwidth this will be the result with

231
00:11:05,640 --> 00:11:07,560
only one dish which gets the numbers

232
00:11:07,560 --> 00:11:09,540
that you expect from a single

233
00:11:09,540 --> 00:11:13,010
subscribe below 200 megabytes per second

234
00:11:13,010 --> 00:11:17,130
read and write so sequential is quite

235
00:11:17,130 --> 00:11:20,480
good on the J but so here's ours or

236
00:11:20,480 --> 00:11:25,740
other PCI Express Drive that actually

237
00:11:25,740 --> 00:11:28,050
comes with two disks so this is the

238
00:11:28,050 --> 00:11:29,880
resource we will get with only one disk

239
00:11:29,880 --> 00:11:31,740
and pretty much we get double the

240
00:11:31,740 --> 00:11:34,140
results with a second while using that

241
00:11:34,140 --> 00:11:37,260
to this together we see that the maximum

242
00:11:37,260 --> 00:11:39,779
bandwidth on this one is around a four

243
00:11:39,779 --> 00:11:43,320
gigabytes per second and the random

244
00:11:43,320 --> 00:11:46,440
writes are a bit below two gigabit per

245
00:11:46,440 --> 00:11:48,209
second but not so different from the

246
00:11:48,209 --> 00:11:52,470
newer generation Intel drives we got so

247
00:11:52,470 --> 00:11:56,820
about latency we did some tests also to

248
00:11:56,820 --> 00:12:00,240
see the latency of these devices all of

249
00:12:00,240 --> 00:12:03,140
the mb m ii devices has a very good

250
00:12:03,140 --> 00:12:06,060
latency so here lower is better it's

251
00:12:06,060 --> 00:12:10,649
below 400 microseconds both for reads

252
00:12:10,649 --> 00:12:13,620
and writes and the j both as we expected

253
00:12:13,620 --> 00:12:17,579
on the satellite we get a higher latency

254
00:12:17,579 --> 00:12:21,269
when we do random reads and bright but

255
00:12:21,269 --> 00:12:24,240
when we do sequential the latency is

256
00:12:24,240 --> 00:12:28,079
quite low and even below 300

257
00:12:28,079 --> 00:12:31,470
microseconds in in some cases this is

258
00:12:31,470 --> 00:12:38,279
with a particular FIO test with 64 K in

259
00:12:38,279 --> 00:12:42,000
request size and one in IO death FIO has

260
00:12:42,000 --> 00:12:44,339
a lot of configuration options at the

261
00:12:44,339 --> 00:12:47,790
end slides I will give the summary of

262
00:12:47,790 --> 00:12:50,820
them and yeah there are some notes on

263
00:12:50,820 --> 00:12:53,730
the slides for later so let's get into

264
00:12:53,730 --> 00:12:58,170
the application so HBase HBase is the

265
00:12:58,170 --> 00:13:02,519
big data big database performance equal

266
00:13:02,519 --> 00:13:05,160
it's built on top of Hadoop you can

267
00:13:05,160 --> 00:13:06,899
actually put it into any file system but

268
00:13:06,899 --> 00:13:11,100
usually people put it on top of HDFS for

269
00:13:11,100 --> 00:13:14,880
storage and safekeeping and it has good

270
00:13:14,880 --> 00:13:17,790
properties close to real time and low

271
00:13:17,790 --> 00:13:20,990
latency for random access and this is

272
00:13:20,990 --> 00:13:22,920
surprising for

273
00:13:22,920 --> 00:13:25,319
Hadoop ecosystem type of project and

274
00:13:25,319 --> 00:13:28,279
it's being used a lot in production and

275
00:13:28,279 --> 00:13:31,319
usually HBase is sort of a building

276
00:13:31,319 --> 00:13:34,309
block for other big data projects so

277
00:13:34,309 --> 00:13:36,929
some other projects that have a sequel

278
00:13:36,929 --> 00:13:38,609
interface it actually store the data

279
00:13:38,609 --> 00:13:41,579
through HBase so let's see how the HBase

280
00:13:41,579 --> 00:13:44,339
model works every time you do a write

281
00:13:44,339 --> 00:13:47,189
will be a put in the system put a value

282
00:13:47,189 --> 00:13:51,529
in the system this data is written

283
00:13:51,529 --> 00:13:54,089
directly to disk into the right ahead

284
00:13:54,089 --> 00:13:56,910
log for safekeeping but it's also put on

285
00:13:56,910 --> 00:14:00,059
the Java heap space into a monster so

286
00:14:00,059 --> 00:14:02,819
you can we can serve it directly from

287
00:14:02,819 --> 00:14:04,049
ground very fast

288
00:14:04,049 --> 00:14:07,139
this gives HBase low latency every once

289
00:14:07,139 --> 00:14:09,419
in a while the the buffers are flushed

290
00:14:09,419 --> 00:14:12,809
to HDFS disk to the edge files and if

291
00:14:12,809 --> 00:14:16,199
it's not in the mem as far data is

292
00:14:16,199 --> 00:14:20,519
written read read from HDFS it goes into

293
00:14:20,519 --> 00:14:23,160
the block Kiche a block a block is the

294
00:14:23,160 --> 00:14:25,559
unit of a storage in HBase of different

295
00:14:25,559 --> 00:14:28,309
types of storage and the blockage is I

296
00:14:28,309 --> 00:14:31,619
call it a level one type of cash with

297
00:14:31,619 --> 00:14:36,209
LRU least recently used algorithm to a

298
00:14:36,209 --> 00:14:41,549
big least frequently use blocks so in

299
00:14:41,549 --> 00:14:44,399
latest versions of HBase I have added

300
00:14:44,399 --> 00:14:46,019
something called the bucket catch a

301
00:14:46,019 --> 00:14:48,779
bucket cut sale you can think of a level

302
00:14:48,779 --> 00:14:52,350
two cache that can be off heap so you

303
00:14:52,350 --> 00:14:56,059
can have HBase controlling a larger

304
00:14:56,059 --> 00:14:59,730
piece of memory for for its cache and

305
00:14:59,730 --> 00:15:03,119
it's fixed size use when you set it up

306
00:15:03,119 --> 00:15:05,459
you say what size you want it you can

307
00:15:05,459 --> 00:15:08,129
set it up on heat actually on your Java

308
00:15:08,129 --> 00:15:11,399
heap space but we don't recommend that

309
00:15:11,399 --> 00:15:13,350
option we only found marginal

310
00:15:13,350 --> 00:15:15,299
improvements and you will be competing

311
00:15:15,299 --> 00:15:18,419
with already with a blockage you can set

312
00:15:18,419 --> 00:15:22,019
it up off heap so a different demo

313
00:15:22,019 --> 00:15:23,939
interface and different Java heap space

314
00:15:23,939 --> 00:15:27,779
through Java niño and the interesting

315
00:15:27,779 --> 00:15:29,699
part is you can set it also to any file

316
00:15:29,699 --> 00:15:31,439
on the file system in this case we put

317
00:15:31,439 --> 00:15:35,009
it in the nbme drive and also into a ram

318
00:15:35,009 --> 00:15:36,100
disk for

319
00:15:36,100 --> 00:15:38,230
testing so here you have the schematic

320
00:15:38,230 --> 00:15:40,240
is the here's the Java heap space and

321
00:15:40,240 --> 00:15:42,819
then you have off hip the level two

322
00:15:42,819 --> 00:15:45,519
cache for each face so we performed

323
00:15:45,519 --> 00:15:47,230
several experiments let me summarize

324
00:15:47,230 --> 00:15:51,720
them so first we have a baseline HBase

325
00:15:51,720 --> 00:15:55,930
1.24 we solve any special tuning and

326
00:15:55,930 --> 00:15:58,779
without bucket cache the second is let's

327
00:15:58,779 --> 00:16:02,259
put the bucket cache on the off hip but

328
00:16:02,259 --> 00:16:06,160
managed by by Java let's try that also

329
00:16:06,160 --> 00:16:09,310
into a run disk let's put the bucket

330
00:16:09,310 --> 00:16:11,410
cache into around this and let's put the

331
00:16:11,410 --> 00:16:16,000
bucket cash into an mm nvme file hosted

332
00:16:16,000 --> 00:16:18,160
on the on the drive the difference here

333
00:16:18,160 --> 00:16:20,319
is that of course we cannot allocate all

334
00:16:20,319 --> 00:16:23,829
of our run for for caching so we use 32

335
00:16:23,829 --> 00:16:27,630
GBS of ram for the ram note the run

336
00:16:27,630 --> 00:16:30,550
configurations and to cut we have 50 TVs

337
00:16:30,550 --> 00:16:34,089
were working on the nbme divisor so nvme

338
00:16:34,089 --> 00:16:36,670
has a larger cache size this is as

339
00:16:36,670 --> 00:16:39,940
expected as we have a larger drive done

340
00:16:39,940 --> 00:16:43,750
without memory so the experiments that

341
00:16:43,750 --> 00:16:46,089
I'm going to show are most of them are

342
00:16:46,089 --> 00:16:48,490
read-only the first group of them are

343
00:16:48,490 --> 00:16:51,160
let's try I'd use case where we are only

344
00:16:51,160 --> 00:16:54,100
reading data and then a mixed type of

345
00:16:54,100 --> 00:16:57,550
workload the benchmark is YC SB I will

346
00:16:57,550 --> 00:16:59,709
get into the benchmark a bit later and

347
00:16:59,709 --> 00:17:03,880
the payload is generating around 250

348
00:17:03,880 --> 00:17:06,909
million records which accommodate to 2

349
00:17:06,909 --> 00:17:09,789
terabytes of raw HDFS storage we're

350
00:17:09,789 --> 00:17:12,339
using replication of 1 here so this is

351
00:17:12,339 --> 00:17:15,390
the actual size that is stored in disk

352
00:17:15,390 --> 00:17:17,819
so let's get into the read-only

353
00:17:17,819 --> 00:17:21,130
benchmarks they get so doing I get from

354
00:17:21,130 --> 00:17:25,390
HBase we're using 500 thread to read the

355
00:17:25,390 --> 00:17:28,659
data in in the benchmark okay so let me

356
00:17:28,659 --> 00:17:30,549
guide you through the group of results

357
00:17:30,549 --> 00:17:32,620
each of this group of bars is a

358
00:17:32,620 --> 00:17:34,240
different this configuration from the

359
00:17:34,240 --> 00:17:36,610
for we have and each color is a

360
00:17:36,610 --> 00:17:39,220
different run a sequential run so this

361
00:17:39,220 --> 00:17:40,809
is will be the first run for the

362
00:17:40,809 --> 00:17:43,750
baseline second one and third one one

363
00:17:43,750 --> 00:17:45,610
thing that we can see is that first run

364
00:17:45,610 --> 00:17:49,070
is always a bit slower than the rest of

365
00:17:49,070 --> 00:17:51,500
runs so this means the cash was called

366
00:17:51,500 --> 00:17:54,050
there was no cars in there but we can

367
00:17:54,050 --> 00:17:56,000
see that the second and third runs they

368
00:17:56,000 --> 00:17:57,800
have the same times here higher is

369
00:17:57,800 --> 00:18:00,110
better this throughput in operations per

370
00:18:00,110 --> 00:18:03,470
second and this yellow line you see here

371
00:18:03,470 --> 00:18:06,010
is latency and latency lower the better

372
00:18:06,010 --> 00:18:09,920
we can see that bucket cash off heap and

373
00:18:09,920 --> 00:18:12,440
run these are very similar results so

374
00:18:12,440 --> 00:18:14,930
either if you put it managed by Java or

375
00:18:14,930 --> 00:18:17,690
you just mount into a file system at ten

376
00:18:17,690 --> 00:18:19,490
PFS file systems you get very similar

377
00:18:19,490 --> 00:18:22,820
results and the bucket cache can speed

378
00:18:22,820 --> 00:18:26,120
up a bit more the results in at the end

379
00:18:26,120 --> 00:18:28,760
the bucket cache gets to X the

380
00:18:28,760 --> 00:18:31,160
performance benefit from the baseline

381
00:18:31,160 --> 00:18:35,230
and fifty percent more than the RAM disk

382
00:18:35,230 --> 00:18:39,560
strategies so let's look at how this

383
00:18:39,560 --> 00:18:46,160
looks on the on the server level on the

384
00:18:46,160 --> 00:18:47,780
performance metrics so this is the

385
00:18:47,780 --> 00:18:51,230
average CPU for the baseline execution

386
00:18:51,230 --> 00:18:54,710
it's vertical bar here is showing a

387
00:18:54,710 --> 00:18:56,930
different one so this is the first one

388
00:18:56,930 --> 00:18:59,000
second and third consider it's pretty

389
00:18:59,000 --> 00:19:01,880
stable except that in the beginning here

390
00:19:01,880 --> 00:19:05,570
this red part this is white i/o the

391
00:19:05,570 --> 00:19:09,140
graph here in the bottom its measures

392
00:19:09,140 --> 00:19:11,810
the read performance from the disk and

393
00:19:11,810 --> 00:19:15,590
we see here a huge orange Pike that gets

394
00:19:15,590 --> 00:19:19,670
to two gigabytes per second and what

395
00:19:19,670 --> 00:19:21,500
it's basically happening is that at the

396
00:19:21,500 --> 00:19:24,260
beginning of the run files are being

397
00:19:24,260 --> 00:19:27,170
read from disk we have the weight IO

398
00:19:27,170 --> 00:19:29,810
but then memory is being filled in

399
00:19:29,810 --> 00:19:32,420
either Java heap space this blue space

400
00:19:32,420 --> 00:19:36,260
or the OS buffer cache so basically it's

401
00:19:36,260 --> 00:19:38,150
only reading from the disk on the first

402
00:19:38,150 --> 00:19:40,790
seconds of the execution and then is

403
00:19:40,790 --> 00:19:43,730
caching everything either on hip or on

404
00:19:43,730 --> 00:19:47,540
the on the OS or the OS is doing that

405
00:19:47,540 --> 00:19:47,930
for us

406
00:19:47,930 --> 00:19:50,510
network is quite constant network is not

407
00:19:50,510 --> 00:19:55,160
a bottleneck for any of these tests so

408
00:19:55,160 --> 00:19:57,260
we were saying okay so we're not

409
00:19:57,260 --> 00:19:59,090
actually measuring too much the disk

410
00:19:59,090 --> 00:20:01,190
performance everything is being cached

411
00:20:01,190 --> 00:20:02,710
in

412
00:20:02,710 --> 00:20:04,570
so there are a couple of strategies that

413
00:20:04,570 --> 00:20:07,540
you can follow let's look at the first

414
00:20:07,540 --> 00:20:11,200
let's look at the other examples on the

415
00:20:11,200 --> 00:20:14,860
off hip there's a right there's a read

416
00:20:14,860 --> 00:20:18,370
throughout all of the execution this is

417
00:20:18,370 --> 00:20:21,490
for the temporary temper phase one disk

418
00:20:21,490 --> 00:20:23,770
and this will be for the bucket cash in

419
00:20:23,770 --> 00:20:25,750
the bucket cash example that is twice as

420
00:20:25,750 --> 00:20:28,180
fast you can see that actually the cash

421
00:20:28,180 --> 00:20:31,270
is being filled in us as we execute the

422
00:20:31,270 --> 00:20:33,100
first one point one thing is that we

423
00:20:33,100 --> 00:20:35,650
cannot with this matrix tools we cannot

424
00:20:35,650 --> 00:20:38,650
see the how memory is being written or

425
00:20:38,650 --> 00:20:40,690
accessed by the tools and this is one of

426
00:20:40,690 --> 00:20:42,760
the improvements we need to make for all

427
00:20:42,760 --> 00:20:45,910
tools but what is happening is that the

428
00:20:45,910 --> 00:20:47,410
OS buffer cache is being really

429
00:20:47,410 --> 00:20:49,450
effective on these clusters and we're

430
00:20:49,450 --> 00:20:52,180
really stressing the hardware so the

431
00:20:52,180 --> 00:20:54,540
challenge is to actually how we can

432
00:20:54,540 --> 00:20:57,640
benchmark the drives outside of the the

433
00:20:57,640 --> 00:20:59,770
buffer cache one thing is to when a

434
00:20:59,770 --> 00:21:01,510
strategy will be to build a very large

435
00:21:01,510 --> 00:21:04,060
workload and we did some tests of

436
00:21:04,060 --> 00:21:06,520
building terabytes of data but they take

437
00:21:06,520 --> 00:21:09,670
days to run we cannot spend days just

438
00:21:09,670 --> 00:21:12,040
waiting to see if something finished and

439
00:21:12,040 --> 00:21:15,070
then days to process all of the data

440
00:21:15,070 --> 00:21:17,910
generated so the purchase we took were

441
00:21:17,910 --> 00:21:20,710
limiting the available RAM in the notes

442
00:21:20,710 --> 00:21:23,920
we lower the run available to the OS to

443
00:21:23,920 --> 00:21:27,430
32 GBS instead of 128 and the second

444
00:21:27,430 --> 00:21:29,620
experiment is actually to drop the buff

445
00:21:29,620 --> 00:21:32,440
the OS buffer cache a every 10 seconds

446
00:21:32,440 --> 00:21:36,310
so what happens here if we limit the

447
00:21:36,310 --> 00:21:38,620
memory simulating that the cluster has

448
00:21:38,620 --> 00:21:43,750
lower resources baseline on the baseline

449
00:21:43,750 --> 00:21:46,710
and the run strategies they have a

450
00:21:46,710 --> 00:21:50,380
pretty similar time while on the bucket

451
00:21:50,380 --> 00:21:52,420
cache where we have this external memory

452
00:21:52,420 --> 00:21:57,160
that is not in run we get up to X 8 X so

453
00:21:57,160 --> 00:21:58,420
a hundred percent performance

454
00:21:58,420 --> 00:22:01,860
improvement if the denotes had less

455
00:22:01,860 --> 00:22:06,820
capacitor capacity in the so if we look

456
00:22:06,820 --> 00:22:09,610
at the CPU charge for the four different

457
00:22:09,610 --> 00:22:11,800
strategies we see the red here is

458
00:22:11,800 --> 00:22:14,400
weightier so it's the bottleneck is

459
00:22:14,400 --> 00:22:18,270
reading from disks but on the bucket

460
00:22:18,270 --> 00:22:21,570
cuts example as things are not so slow

461
00:22:21,570 --> 00:22:23,490
as reading from the j-bot and we have

462
00:22:23,490 --> 00:22:27,090
the date already in LRU cache it it gets

463
00:22:27,090 --> 00:22:30,630
to be a times faster that this wid

464
00:22:30,630 --> 00:22:33,990
through put is around 2.5 gigabytes per

465
00:22:33,990 --> 00:22:36,570
second stable for all of the three

466
00:22:36,570 --> 00:22:39,840
strategies except for the MBA me where

467
00:22:39,840 --> 00:22:42,510
we get thirty eight gigabytes per second

468
00:22:42,510 --> 00:22:44,700
throughput for the whole cluster this is

469
00:22:44,700 --> 00:22:47,580
a Grenada throughout the cluster so the

470
00:22:47,580 --> 00:22:49,410
the rid throughput is actually quite

471
00:22:49,410 --> 00:22:52,380
high on the second example where we drop

472
00:22:52,380 --> 00:22:54,960
the buffer cache we do get some

473
00:22:54,960 --> 00:22:57,150
improvements by using the RAM disk and

474
00:22:57,150 --> 00:23:00,150
of heap and run so we're able to measure

475
00:23:00,150 --> 00:23:03,420
this but the improvement is marginal

476
00:23:03,420 --> 00:23:07,679
it's less than 1 X 50% and in this case

477
00:23:07,679 --> 00:23:10,890
a bucket cache has a performance

478
00:23:10,890 --> 00:23:14,820
improvement of a 9 X of course this

479
00:23:14,820 --> 00:23:16,559
running times the total running times

480
00:23:16,559 --> 00:23:19,290
are longer than on the first experiments

481
00:23:19,290 --> 00:23:21,300
where we had all of the run but this is

482
00:23:21,300 --> 00:23:23,340
a way that has been useful to actually

483
00:23:23,340 --> 00:23:26,760
see what will be the the capacity and

484
00:23:26,760 --> 00:23:28,590
what working can we expect from the

485
00:23:28,590 --> 00:23:32,700
drives ok the next set of experiments we

486
00:23:32,700 --> 00:23:35,300
did he are related to running the whole

487
00:23:35,300 --> 00:23:42,210
benchmark ycs B has several use cases we

488
00:23:42,210 --> 00:23:43,950
were using workload C in the first

489
00:23:43,950 --> 00:23:46,790
example that is read-only but there's a

490
00:23:46,790 --> 00:23:51,120
read update workload mostly read

491
00:23:51,120 --> 00:23:54,540
workload update workload and read

492
00:23:54,540 --> 00:23:57,600
modified type of write workloads seeing

493
00:23:57,600 --> 00:24:01,620
these workloads add new data to the to

494
00:24:01,620 --> 00:24:03,690
the workload we cannot run them

495
00:24:03,690 --> 00:24:05,670
repeatedly sequentially as we did before

496
00:24:05,670 --> 00:24:07,860
because we're actually increasing the

497
00:24:07,860 --> 00:24:10,380
the size of the storage so they

498
00:24:10,380 --> 00:24:12,080
recommend them to run them from

499
00:24:12,080 --> 00:24:15,420
Guadalupe A to F we're skipping were log

500
00:24:15,420 --> 00:24:18,660
e because it's quite slow as it does so

501
00:24:18,660 --> 00:24:22,710
my scans and we couldn't finish in time

502
00:24:22,710 --> 00:24:25,920
if not if we would have run it so very

503
00:24:25,920 --> 00:24:27,810
quickly on this results when we were run

504
00:24:27,810 --> 00:24:29,460
the mix were close we have three

505
00:24:29,460 --> 00:24:33,270
strategies baseline run and bucket casts

506
00:24:33,270 --> 00:24:36,480
its line is a different it's very so

507
00:24:36,480 --> 00:24:39,300
different benchmark and just quickly

508
00:24:39,300 --> 00:24:41,790
summarizing on the speed up on the data

509
00:24:41,790 --> 00:24:44,220
generation there is no speed up by using

510
00:24:44,220 --> 00:24:46,920
a class is right only and in some

511
00:24:46,920 --> 00:24:50,610
benchmarks we get more speed up but the

512
00:24:50,610 --> 00:24:54,050
most we get is around 2x

513
00:24:54,050 --> 00:24:58,230
no in this case no sorry it's less than

514
00:24:58,230 --> 00:25:01,770
1 X it's around 50% improvement by using

515
00:25:01,770 --> 00:25:07,530
the mbm it drives on this scenario also

516
00:25:07,530 --> 00:25:10,530
with the run strategies so we're

517
00:25:10,530 --> 00:25:12,390
actually not getting even a hundred

518
00:25:12,390 --> 00:25:16,200
percent improvement if we're doing like

519
00:25:16,200 --> 00:25:18,660
a more production style workload where

520
00:25:18,660 --> 00:25:21,750
we're also adding data and updates so on

521
00:25:21,750 --> 00:25:24,300
this set of experiments we also limited

522
00:25:24,300 --> 00:25:28,680
the run to 32gb spur node and the

523
00:25:28,680 --> 00:25:31,470
speed-up was hired as expected but it

524
00:25:31,470 --> 00:25:35,610
was around 87% for the nbme case this is

525
00:25:35,610 --> 00:25:37,170
the average of all of the different

526
00:25:37,170 --> 00:25:40,260
workloads and we got an improvement of

527
00:25:40,260 --> 00:25:46,380
around 87% so how does this look on CPU

528
00:25:46,380 --> 00:25:48,900
this will be baseline all of the

529
00:25:48,900 --> 00:25:52,160
information and this case will be the

530
00:25:52,160 --> 00:25:57,260
the nbme weight is the main bottleneck

531
00:25:57,260 --> 00:26:01,050
if we look at the disk chart we see

532
00:26:01,050 --> 00:26:03,450
what's going on here data is being

533
00:26:03,450 --> 00:26:05,820
generated this is a blue part and then

534
00:26:05,820 --> 00:26:09,570
all the orange area is being read from

535
00:26:09,570 --> 00:26:11,160
the j-bot or

536
00:26:11,160 --> 00:26:13,740
yeah pretty much from the j-bot and the

537
00:26:13,740 --> 00:26:16,500
rest is in the OS buffer cache on the

538
00:26:16,500 --> 00:26:19,410
case of the mb i mean we do have more

539
00:26:19,410 --> 00:26:22,170
rights during the execution and here's

540
00:26:22,170 --> 00:26:25,020
the updates on some of the workload but

541
00:26:25,020 --> 00:26:28,290
basically we get a more constant with

542
00:26:28,290 --> 00:26:30,360
throughput and with higher peaks of

543
00:26:30,360 --> 00:26:33,150
around 25 DBS per second so actually the

544
00:26:33,150 --> 00:26:34,770
drives are being used or being used

545
00:26:34,770 --> 00:26:35,730
quite effectively

546
00:26:35,730 --> 00:26:39,000
but the OS buffer cache is really

547
00:26:39,000 --> 00:26:42,450
helpful for HBase on

548
00:26:42,450 --> 00:26:45,700
hdfs in general here's just some charge

549
00:26:45,700 --> 00:26:47,530
with the network and memory just to keep

550
00:26:47,530 --> 00:26:49,480
on the slide someone wants to check

551
00:26:49,480 --> 00:26:52,870
later the network is not the bottleneck

552
00:26:52,870 --> 00:26:56,050
and it's highly used when generating the

553
00:26:56,050 --> 00:27:01,810
data actually ok yes ok let's get to the

554
00:27:01,810 --> 00:27:03,790
summary and conclusions of this work

555
00:27:03,790 --> 00:27:06,640
so with packet cachet we've been able to

556
00:27:06,640 --> 00:27:08,740
increase the performance of HBase

557
00:27:08,740 --> 00:27:11,200
especially putting the files into an M

558
00:27:11,200 --> 00:27:13,360
and B Emmy Drive however it was

559
00:27:13,360 --> 00:27:16,270
surprising that the speed-up was around

560
00:27:16,270 --> 00:27:19,210
2x for the base cases an area where we

561
00:27:19,210 --> 00:27:21,220
have the cache already warm and we're

562
00:27:21,220 --> 00:27:24,100
doing all a read-only type of workload

563
00:27:24,100 --> 00:27:27,210
if we're doing a general workload with

564
00:27:27,210 --> 00:27:32,230
50% to 100% improvement which is not

565
00:27:32,230 --> 00:27:33,970
that much of course if you can double

566
00:27:33,970 --> 00:27:35,620
the capacity of your cluster just by

567
00:27:35,620 --> 00:27:39,340
adding some hard work storage then that

568
00:27:39,340 --> 00:27:42,730
might be interesting but we see that as

569
00:27:42,730 --> 00:27:45,580
you limit more as you stress more the

570
00:27:45,580 --> 00:27:47,440
resources on your hard work the many

571
00:27:47,440 --> 00:27:50,320
feeds are higher we wouldn't recommend

572
00:27:50,320 --> 00:27:53,680
to to run a bucket cache or unhip we

573
00:27:53,680 --> 00:27:55,600
would recommend that to do it always off

574
00:27:55,600 --> 00:27:58,270
hip either a ram disk or or in a

575
00:27:58,270 --> 00:28:00,040
different mount point it can also be an

576
00:28:00,040 --> 00:28:03,670
SSD drive regular SAS or SATA doesn't

577
00:28:03,670 --> 00:28:06,640
have to be nbme to get some benefits

578
00:28:06,640 --> 00:28:09,580
some of the things we have learned that

579
00:28:09,580 --> 00:28:13,570
is testing high end hardware on the

580
00:28:13,570 --> 00:28:16,540
application level is not so easy you

581
00:28:16,540 --> 00:28:18,610
need to generate very big workloads or

582
00:28:18,610 --> 00:28:21,640
try to limit the resources to find out

583
00:28:21,640 --> 00:28:24,310
artificially how how much benefit you

584
00:28:24,310 --> 00:28:26,740
can get but you still need to do a per

585
00:28:26,740 --> 00:28:28,960
device per node and per cluster system

586
00:28:28,960 --> 00:28:32,860
level or micro level benchmarks the OS

587
00:28:32,860 --> 00:28:35,710
buffer cache is quite effective so if

588
00:28:35,710 --> 00:28:37,780
you have you add more RAM viewer you're

589
00:28:37,780 --> 00:28:41,500
actually speeding up your applications L

590
00:28:41,500 --> 00:28:44,620
an l2 cache with LRU benefits at some

591
00:28:44,620 --> 00:28:46,870
items are more popular than others why

592
00:28:46,870 --> 00:28:50,260
CSP does long tail distribution Cyprian

593
00:28:50,260 --> 00:28:52,990
type of distribution so their items are

594
00:28:52,990 --> 00:28:53,830
not normally

595
00:28:53,830 --> 00:28:58,380
you can get more speed up there but the

596
00:28:58,380 --> 00:29:00,909
techniques we use to either drop the

597
00:29:00,909 --> 00:29:03,429
cars or limit the RAM are effective to

598
00:29:03,429 --> 00:29:07,120
test with with this hardware to conclude

599
00:29:07,120 --> 00:29:09,490
I would say that nbme devices are fast

600
00:29:09,490 --> 00:29:12,970
they have very low latency but they are

601
00:29:12,970 --> 00:29:15,659
more expensive than than putting a J but

602
00:29:15,659 --> 00:29:19,210
big data applications are designed to

603
00:29:19,210 --> 00:29:21,850
work with sequential reads and write

604
00:29:21,850 --> 00:29:25,269
so until we actually write applications

605
00:29:25,269 --> 00:29:27,580
different and update the code to do

606
00:29:27,580 --> 00:29:31,210
random reads or even to byte address or

607
00:29:31,210 --> 00:29:34,840
tweet a MB any devices as if it was run

608
00:29:34,840 --> 00:29:38,500
not a blog devices as a disk then we

609
00:29:38,500 --> 00:29:41,080
will be using the full benefits of this

610
00:29:41,080 --> 00:29:44,139
write other than with use cases of use

611
00:29:44,139 --> 00:29:46,720
using touches right now for in the big

612
00:29:46,720 --> 00:29:48,700
data ecosystem you need to rely on

613
00:29:48,700 --> 00:29:51,940
external tools or some research projects

614
00:29:51,940 --> 00:29:53,700
to actually speed up and tired

615
00:29:53,700 --> 00:29:56,080
everything and instead of using it as a

616
00:29:56,080 --> 00:29:59,019
cache included in the in distorted file

617
00:29:59,019 --> 00:30:01,899
system so yes this is the first time of

618
00:30:01,899 --> 00:30:03,789
the first part of our work and we

619
00:30:03,789 --> 00:30:05,620
welcome any feedback that you may have

620
00:30:05,620 --> 00:30:08,260
and that will be old I will leave some

621
00:30:08,260 --> 00:30:10,299
reference in the slides and thank you

622
00:30:10,299 --> 00:30:12,779
for your attention

623
00:30:12,779 --> 00:30:19,980
ok I'll take some questions yes

624
00:30:34,509 --> 00:30:38,179
yes we use this the question was we use

625
00:30:38,179 --> 00:30:40,309
the stress tool to limit the run so it's

626
00:30:40,309 --> 00:30:42,139
basically a program that fills in the

627
00:30:42,139 --> 00:30:44,809
run to the amount that you tell it we

628
00:30:44,809 --> 00:30:47,480
only use it to fill the run so no CPU

629
00:30:47,480 --> 00:30:50,299
usage there's another option suggested

630
00:30:50,299 --> 00:30:52,429
that is to actually put in the boot

631
00:30:52,429 --> 00:30:55,129
kernel parameters the maximum RAM this

632
00:30:55,129 --> 00:30:58,669
would have been preferable but we didn't

633
00:30:58,669 --> 00:31:02,090
have the cluster on site and so we don't

634
00:31:02,090 --> 00:31:04,100
own the hardware and I didn't want to be

635
00:31:04,100 --> 00:31:05,529
recompile

636
00:31:05,529 --> 00:31:09,019
generating the would the inter amount

637
00:31:09,019 --> 00:31:18,590
and FS MBA or like our DMA okay so the

638
00:31:18,590 --> 00:31:21,590
question is if we looked into MB me over

639
00:31:21,590 --> 00:31:26,509
fabric so using direct memory our DMA

640
00:31:26,509 --> 00:31:32,059
over InfiniBand or or Roxy no I have

641
00:31:32,059 --> 00:31:35,269
taught and work with some of the papers

642
00:31:35,269 --> 00:31:37,970
Ohio State University professor panda if

643
00:31:37,970 --> 00:31:39,769
you look you have professor panda look

644
00:31:39,769 --> 00:31:42,590
for that they have some test worse

645
00:31:42,590 --> 00:31:45,289
experimental results where they do byte

646
00:31:45,289 --> 00:31:48,200
addressable remote memory access to any

647
00:31:48,200 --> 00:31:50,629
devices they show interesting numbers

648
00:31:50,629 --> 00:31:53,929
but the use case we're looking for is

649
00:31:53,929 --> 00:31:56,809
more for production that we want to

650
00:31:56,809 --> 00:32:00,529
apply soon and for that you need to

651
00:32:00,529 --> 00:32:02,690
modify application run maybe not

652
00:32:02,690 --> 00:32:07,820
production ready code that's the back

653
00:32:07,820 --> 00:32:10,059
face

654
00:32:17,440 --> 00:32:21,710
well there's less storage in RAM we're

655
00:32:21,710 --> 00:32:24,740
using 32 GBS of RAM for that and that is

656
00:32:24,740 --> 00:32:27,020
competing with the RF buffer castle you

657
00:32:27,020 --> 00:32:28,790
have less buffer cars and less memory

658
00:32:28,790 --> 00:32:31,400
for the Java heap space and in the other

659
00:32:31,400 --> 00:32:33,740
we have a per node one point six

660
00:32:33,740 --> 00:32:36,020
terabytes of storage we're using 250 was

661
00:32:36,020 --> 00:32:39,129
enough for experiments

662
00:32:47,060 --> 00:32:49,929
yes

663
00:33:14,520 --> 00:33:18,600
yes the question was we use XFS on the

664
00:33:18,600 --> 00:33:19,620
MBA me drive

665
00:33:19,620 --> 00:33:22,950
actually it's mounted as a rate software

666
00:33:22,950 --> 00:33:25,860
rate because the drive comes with two

667
00:33:25,860 --> 00:33:28,050
disks so we're losing some performance

668
00:33:28,050 --> 00:33:31,560
already there on the software rate yes

669
00:33:31,560 --> 00:33:35,490
there are other research I would call

670
00:33:35,490 --> 00:33:37,650
them research or newer projects that

671
00:33:37,650 --> 00:33:43,140
improve a standard file system file

672
00:33:43,140 --> 00:33:45,720
system this I think I named one here

673
00:33:45,720 --> 00:33:49,530
there's one called SSD FS which is a

674
00:33:49,530 --> 00:33:52,920
file system slot for flash drives they

675
00:33:52,920 --> 00:33:57,630
also save your drives just so that the

676
00:33:57,630 --> 00:34:00,000
the bytes don't burn out so easily so

677
00:34:00,000 --> 00:34:02,820
yeah that that's interesting but that

678
00:34:02,820 --> 00:34:05,220
six I will call it experimental for

679
00:34:05,220 --> 00:34:07,250
production this case though

680
00:34:07,250 --> 00:34:11,610
another question okay we can keep

681
00:34:11,610 --> 00:34:13,620
talking later if you have more questions

682
00:34:13,620 --> 00:34:17,449
so thanks all for your attention

683
00:34:18,320 --> 00:34:18,679
[Applause]

684
00:34:18,679 --> 00:34:21,879
[Music]

