1
00:00:00,060 --> 00:00:02,719
welcome

2
00:00:04,660 --> 00:00:08,080
back everybody for the next talk which

3
00:00:08,080 --> 00:00:10,059
is Jimmy on glucose from the University

4
00:00:10,059 --> 00:00:13,240
of Edinburgh about bridging bringing

5
00:00:13,240 --> 00:00:15,270
sorry the Semantic Web closer to reality

6
00:00:15,270 --> 00:00:18,910
PostgreSQL as RDF graph database thank

7
00:00:18,910 --> 00:00:21,850
you so I tried to break the words

8
00:00:21,850 --> 00:00:25,330
longest title record but I think there

9
00:00:25,330 --> 00:00:27,550
are people that beat me to it so I added

10
00:00:27,550 --> 00:00:30,430
an extra bit or how to export your data

11
00:00:30,430 --> 00:00:35,760
for to someone who's expecting RDF so

12
00:00:36,120 --> 00:00:39,879
what is the Semantic Web and what is RDF

13
00:00:39,879 --> 00:00:42,309
how many of you inside here know what

14
00:00:42,309 --> 00:00:47,019
RDF is oh okay that's that's great

15
00:00:47,019 --> 00:00:48,760
because everyone at the graph dev room

16
00:00:48,760 --> 00:00:56,489
knew what RDF was so it's a way to

17
00:00:56,670 --> 00:00:59,499
overcome the limitations of HTML and

18
00:00:59,499 --> 00:01:04,569
make the web machine readable so the way

19
00:01:04,569 --> 00:01:06,909
to do it some people think is to add

20
00:01:06,909 --> 00:01:09,130
metadata to everything and one way to

21
00:01:09,130 --> 00:01:12,729
add metadata to everything is RDF which

22
00:01:12,729 --> 00:01:16,570
is a data model that lets you turn

23
00:01:16,570 --> 00:01:21,100
everything into a graph actually a multi

24
00:01:21,100 --> 00:01:23,619
graph because multiple objects can have

25
00:01:23,619 --> 00:01:26,350
multiple things pointing to it to them

26
00:01:26,350 --> 00:01:30,399
and it's it's also strictly speaking in

27
00:01:30,399 --> 00:01:31,960
graph theory it's a labelled and

28
00:01:31,960 --> 00:01:34,840
directed graph which means that not only

29
00:01:34,840 --> 00:01:37,090
objects have properties but those

30
00:01:37,090 --> 00:01:38,920
properties usually have directions as

31
00:01:38,920 --> 00:01:44,640
well so it's person a authored book a

32
00:01:44,640 --> 00:01:48,159
this is a direction whereas you can't go

33
00:01:48,159 --> 00:01:50,590
back the other way using the property

34
00:01:50,590 --> 00:01:53,679
authored because the book that obviously

35
00:01:53,679 --> 00:01:58,659
hasn't authored the person so instead of

36
00:01:58,659 --> 00:02:01,780
the entity attribute value model that

37
00:02:01,780 --> 00:02:05,020
were used to we've given them fancy

38
00:02:05,020 --> 00:02:07,569
names and now they're called subject

39
00:02:07,569 --> 00:02:10,990
predicate object in RDF the subject is

40
00:02:10,990 --> 00:02:13,930
the thing that we're talking about the

41
00:02:13,930 --> 00:02:15,140
predicate

42
00:02:15,140 --> 00:02:18,620
is the property or action taken by the

43
00:02:18,620 --> 00:02:21,470
subject and the object is the recipient

44
00:02:21,470 --> 00:02:26,990
of that property or action so as these

45
00:02:26,990 --> 00:02:29,030
are three things we call them RDF

46
00:02:29,030 --> 00:02:34,100
triples and triples usually have your

47
00:02:34,100 --> 00:02:36,560
eyes encoded in them that let you

48
00:02:36,560 --> 00:02:41,030
instantly access the reference thing

49
00:02:41,030 --> 00:02:45,800
so in this silly example we have an

50
00:02:45,800 --> 00:02:48,680
example org person which is called mark

51
00:02:48,680 --> 00:02:55,180
twain who is who has the predicate

52
00:02:55,180 --> 00:03:00,650
example org relation author to link to

53
00:03:00,650 --> 00:03:04,640
an example org book in a more real-world

54
00:03:04,640 --> 00:03:08,840
example of the styles that we are going

55
00:03:08,840 --> 00:03:14,120
to look at in a minute it's an ad nicey

56
00:03:14,120 --> 00:03:16,550
UK I work at a Dean at the University of

57
00:03:16,550 --> 00:03:19,820
Edinburgh and we do lots of software

58
00:03:19,820 --> 00:03:22,250
development and bibliographic stuff

59
00:03:22,250 --> 00:03:27,470
mostly as well as some geospatial stuff

60
00:03:27,470 --> 00:03:29,810
but here we're just going to talk about

61
00:03:29,810 --> 00:03:35,290
text so we have in a Dean I see UK item

62
00:03:35,290 --> 00:03:42,320
that has DC is double in core it's a way

63
00:03:42,320 --> 00:03:46,190
to encode metadata for things like

64
00:03:46,190 --> 00:03:48,860
records and books and journals and

65
00:03:48,860 --> 00:03:52,070
articles and it has a title and the

66
00:03:52,070 --> 00:03:53,930
title here we can see that it's a

67
00:03:53,930 --> 00:03:57,049
literal it doesn't have to be a URI you

68
00:03:57,049 --> 00:03:59,600
can encode a literal in Navi F because

69
00:03:59,600 --> 00:04:04,519
how else can you give the full breadth

70
00:04:04,519 --> 00:04:08,110
of information that you need to share so

71
00:04:08,110 --> 00:04:11,299
what we have our namespaces that allow

72
00:04:11,299 --> 00:04:12,799
us to make this whole thing a bit

73
00:04:12,799 --> 00:04:16,789
shorter so we can bind namespaces for

74
00:04:16,789 --> 00:04:20,988
example DC elements which are all the

75
00:04:20,988 --> 00:04:23,660
things in dublin core that can be

76
00:04:23,660 --> 00:04:26,780
encoded in rdf and we can bind them to

77
00:04:26,780 --> 00:04:28,400
the prefix DC

78
00:04:28,400 --> 00:04:32,840
so we can just say dc:title this instead

79
00:04:32,840 --> 00:04:36,790
of writing the whole URL for everything

80
00:04:36,790 --> 00:04:40,010
now to store our triples we need triple

81
00:04:40,010 --> 00:04:43,820
stores and triple stores are what offer

82
00:04:43,820 --> 00:04:46,910
persistence to our graph so it's a way

83
00:04:46,910 --> 00:04:49,790
basically to put it on disk and index it

84
00:04:49,790 --> 00:04:55,130
and query it so for this purpose we went

85
00:04:55,130 --> 00:04:59,630
down route that not many people use we

86
00:04:59,630 --> 00:05:01,940
used RDF Lib which is a library in

87
00:05:01,940 --> 00:05:06,050
Python and we used an extension which is

88
00:05:06,050 --> 00:05:09,010
called rdf:li SQL alchemy

89
00:05:09,010 --> 00:05:11,300
unfortunately there used to be an

90
00:05:11,300 --> 00:05:13,639
extension that connected RDF live

91
00:05:13,639 --> 00:05:15,830
directly to Postgres but it's now being

92
00:05:15,830 --> 00:05:17,990
abandoned and the author says please use

93
00:05:17,990 --> 00:05:21,949
SQL alchemy instead so it's pretty clear

94
00:05:21,949 --> 00:05:23,900
by now that we decided to use Postgres

95
00:05:23,900 --> 00:05:28,039
to store all these triples and how can

96
00:05:28,039 --> 00:05:31,130
you do that because Postgres is a

97
00:05:31,130 --> 00:05:33,229
relational database how can you turn it

98
00:05:33,229 --> 00:05:35,599
into a graph and we're gonna let RDF

99
00:05:35,599 --> 00:05:39,500
live do this for us also

100
00:05:39,500 --> 00:05:43,190
excuse me RDF Lib supports querying of

101
00:05:43,190 --> 00:05:46,460
this database once you created the query

102
00:05:46,460 --> 00:05:48,650
language which is called sparkle now

103
00:05:48,650 --> 00:05:50,900
sparkle isn't very pretty to look at but

104
00:05:50,900 --> 00:05:53,479
once you get your head around the

105
00:05:53,479 --> 00:05:56,060
strange syntax which should appear

106
00:05:56,060 --> 00:05:57,500
strange to everyone in here who knows

107
00:05:57,500 --> 00:06:01,639
SQL once you get past that it's actually

108
00:06:01,639 --> 00:06:03,889
as functional as SQL now the bad thing

109
00:06:03,889 --> 00:06:06,849
is that there aren't many Sparkle

110
00:06:06,849 --> 00:06:09,710
databases that perform well but we're

111
00:06:09,710 --> 00:06:12,889
gonna look into that in a minute so

112
00:06:12,889 --> 00:06:14,870
let's look at the original data that we

113
00:06:14,870 --> 00:06:19,060
had that we wanted to encode into

114
00:06:19,060 --> 00:06:23,900
triples so originally we had some JSON

115
00:06:23,900 --> 00:06:26,470
records stored as Jason B and Postgres

116
00:06:26,470 --> 00:06:31,160
and our main identifier for those

117
00:06:31,160 --> 00:06:33,740
records because we didn't have serial

118
00:06:33,740 --> 00:06:36,979
IDs or any other columns we just had one

119
00:06:36,979 --> 00:06:39,480
big column which was called data

120
00:06:39,480 --> 00:06:41,610
and this is pretty much what it

121
00:06:41,610 --> 00:06:43,290
contained

122
00:06:43,290 --> 00:06:47,610
our identifier is the DOI which is the

123
00:06:47,610 --> 00:06:50,220
digital object identifier and it's a

124
00:06:50,220 --> 00:06:52,890
pretty much standard way for academics

125
00:06:52,890 --> 00:06:57,810
to and other people to identify digital

126
00:06:57,810 --> 00:07:00,450
objects whether those objects are a file

127
00:07:00,450 --> 00:07:08,280
or an article or a book or a data set so

128
00:07:08,280 --> 00:07:10,080
it's it's basically a pointer that

129
00:07:10,080 --> 00:07:13,380
redirects you to that object so it can

130
00:07:13,380 --> 00:07:18,540
be used as a URL as we can see here but

131
00:07:18,540 --> 00:07:21,390
this URL only points at the resolver

132
00:07:21,390 --> 00:07:25,770
which is DX dy dot org so you just ask

133
00:07:25,770 --> 00:07:29,310
the resolver to give you a redirect to

134
00:07:29,310 --> 00:07:36,030
the actual object and we also see that

135
00:07:36,030 --> 00:07:41,360
we have other metadata like title and

136
00:07:41,360 --> 00:07:44,610
created that date index date and so on

137
00:07:44,610 --> 00:07:47,400
I haven't the space on-screen to put on

138
00:07:47,400 --> 00:07:51,360
everything so let's start a little bit

139
00:07:51,360 --> 00:07:53,340
to look at the Python code that allows

140
00:07:53,340 --> 00:07:57,090
you to make a simple graph on disk

141
00:07:57,090 --> 00:08:01,380
inside Postgres so of course you have to

142
00:08:01,380 --> 00:08:04,830
import psycho PG 2 which is the let's

143
00:08:04,830 --> 00:08:07,050
say best Postgres connector right now

144
00:08:07,050 --> 00:08:10,500
unfortunately it's not pure python so if

145
00:08:10,500 --> 00:08:12,120
you want a pure python solution then you

146
00:08:12,120 --> 00:08:14,100
have to go with PG 8,000 or something

147
00:08:14,100 --> 00:08:16,860
like that and unfortunately those don't

148
00:08:16,860 --> 00:08:20,010
perform as well why would you like to do

149
00:08:20,010 --> 00:08:24,210
that because usually you don't have the

150
00:08:24,210 --> 00:08:27,000
Postgres libraries compiled for weird

151
00:08:27,000 --> 00:08:30,620
systems like embedded systems and arm

152
00:08:30,620 --> 00:08:33,280
machines

153
00:08:33,280 --> 00:08:36,219
but anyway let's go on in our Wintle

154
00:08:36,219 --> 00:08:39,570
world and assume that everyone can use

155
00:08:39,570 --> 00:08:46,480
x86 libraries so we import a few things

156
00:08:46,480 --> 00:08:48,730
from RDF Lib that let us create the

157
00:08:48,730 --> 00:08:51,430
graph in memory and we also import the

158
00:08:51,430 --> 00:08:53,950
extension which is called RDF SQL

159
00:08:53,950 --> 00:08:57,100
alchemy which lets us transfer this

160
00:08:57,100 --> 00:09:03,310
thing to Postgres excuse me now we also

161
00:09:03,310 --> 00:09:07,000
have two namespace declarations we

162
00:09:07,000 --> 00:09:09,400
create two namespace objects and one of

163
00:09:09,400 --> 00:09:12,340
them is our own it's private I mean even

164
00:09:12,340 --> 00:09:15,190
if you try to access this URL you won't

165
00:09:15,190 --> 00:09:16,660
be able to get anything because it

166
00:09:16,660 --> 00:09:18,130
doesn't exist it's an arbitrary

167
00:09:18,130 --> 00:09:21,340
namespace that we use to describe the

168
00:09:21,340 --> 00:09:26,290
objects inside our database and prism is

169
00:09:26,290 --> 00:09:28,060
actually a fairly standard namespace

170
00:09:28,060 --> 00:09:31,900
that gives you some bibliographic types

171
00:09:31,900 --> 00:09:34,360
of metadata you can use to encode and

172
00:09:34,360 --> 00:09:39,310
RDF so we register the plugins we give

173
00:09:39,310 --> 00:09:44,560
it a database URI and we say that please

174
00:09:44,560 --> 00:09:47,530
use post Pierce please use psycho PG my

175
00:09:47,530 --> 00:09:49,990
user my password at a local host and a

176
00:09:49,990 --> 00:09:53,350
database that we created called RDF

177
00:09:53,350 --> 00:09:55,839
graph which we're going to put our data

178
00:09:55,839 --> 00:10:01,690
in and we also have to give this graph a

179
00:10:01,690 --> 00:10:04,060
name so that we can find it in the

180
00:10:04,060 --> 00:10:06,610
database so this is the context that

181
00:10:06,610 --> 00:10:08,650
we're giving it and we're giving it in

182
00:10:08,650 --> 00:10:11,440
the Adina namespace so we're going to be

183
00:10:11,440 --> 00:10:12,910
creating a graph called

184
00:10:12,910 --> 00:10:18,550
Adina slash or the F graph so we told

185
00:10:18,550 --> 00:10:21,070
the plug-in to open the data store that

186
00:10:21,070 --> 00:10:24,400
we described using SQL alchemy and also

187
00:10:24,400 --> 00:10:27,790
put everything under Adina or DF graph

188
00:10:27,790 --> 00:10:32,380
which is our context and then we just do

189
00:10:32,380 --> 00:10:36,430
a graph open create equals true and that

190
00:10:36,430 --> 00:10:38,970
creates the tables in the database

191
00:10:38,970 --> 00:10:42,220
unfortunately it doesn't until you run a

192
00:10:42,220 --> 00:10:44,470
commit or you add something to the

193
00:10:44,470 --> 00:10:45,790
database so at this stage it doesn't

194
00:10:45,790 --> 00:10:46,180
have

195
00:10:46,180 --> 00:10:49,360
tables even though we've run we created

196
00:10:49,360 --> 00:10:52,089
true so we do a couple of binds to write

197
00:10:52,089 --> 00:10:54,399
two things in the database we bind the

198
00:10:54,399 --> 00:10:57,910
namespaces to the abbreviations Edina

199
00:10:57,910 --> 00:11:00,370
and prism so we can easily use them

200
00:11:00,370 --> 00:11:06,070
later on and we start to create our

201
00:11:06,070 --> 00:11:10,480
triples so we make an empty list of

202
00:11:10,480 --> 00:11:13,330
triples and we start to populated with

203
00:11:13,330 --> 00:11:16,360
things so we create an Edina item which

204
00:11:16,360 --> 00:11:19,510
is just an object in the Edina namespace

205
00:11:19,510 --> 00:11:26,529
and we give it a URL a URI which is item

206
00:11:26,529 --> 00:11:29,800
slash and number which we're going to

207
00:11:29,800 --> 00:11:32,200
use as our identifier for our own

208
00:11:32,200 --> 00:11:35,560
database and I'm just giving it one for

209
00:11:35,560 --> 00:11:43,390
now so we create a triple and we

210
00:11:43,390 --> 00:11:46,350
actually create this as a Python tuple

211
00:11:46,350 --> 00:11:50,650
so we give it an item that we have

212
00:11:50,650 --> 00:11:54,660
created we the predicate is the type so

213
00:11:54,660 --> 00:11:59,350
it has an RDF type of Edina item we're

214
00:11:59,350 --> 00:12:01,900
actually telling the object what it is

215
00:12:01,900 --> 00:12:05,950
and we also give it some properties so

216
00:12:05,950 --> 00:12:08,230
we create another triple for the same

217
00:12:08,230 --> 00:12:12,270
item and we tell it that it's DOI is

218
00:12:12,270 --> 00:12:15,400
this string here now you may wonder why

219
00:12:15,400 --> 00:12:19,589
this is not a URI and it's a literal and

220
00:12:20,310 --> 00:12:22,720
unfortunately the answer is that many

221
00:12:22,720 --> 00:12:26,410
valid do is result in invalid URLs that

222
00:12:26,410 --> 00:12:29,230
are resolved correctly so that's a

223
00:12:29,230 --> 00:12:33,339
problem your parser will reject this not

224
00:12:33,339 --> 00:12:35,230
not the specific one your parcel will

225
00:12:35,230 --> 00:12:38,700
reject the DOI as an in in and an

226
00:12:38,700 --> 00:12:43,570
invalid URI but there is over will work

227
00:12:43,570 --> 00:12:47,940
for some strange reason so

228
00:12:47,970 --> 00:12:50,010
the safest way is just to encoded as

229
00:12:50,010 --> 00:12:54,840
literal and here is what allows us to do

230
00:12:54,840 --> 00:12:57,450
this in a great scale because we have

231
00:12:57,450 --> 00:13:01,200
many records to talk about and this is

232
00:13:01,200 --> 00:13:04,830
the function called ad n and AD then

233
00:13:04,830 --> 00:13:09,210
goes through your list of triples in a

234
00:13:09,210 --> 00:13:13,110
nice pythonic loop here and adds

235
00:13:13,110 --> 00:13:15,750
everything does a list operation on this

236
00:13:15,750 --> 00:13:18,510
list and adds everything to our database

237
00:13:18,510 --> 00:13:23,880
which is called gdb here why don't you

238
00:13:23,880 --> 00:13:25,670
add a triple at the time and you prefer

239
00:13:25,670 --> 00:13:28,050
to do it as a batch

240
00:13:28,050 --> 00:13:30,960
it's much faster to do it with batching

241
00:13:30,960 --> 00:13:33,270
than just adding a triple adding a

242
00:13:33,270 --> 00:13:37,200
triple and then our last step to export

243
00:13:37,200 --> 00:13:39,540
our data and give it to someone else in

244
00:13:39,540 --> 00:13:44,880
RDF format is just a dot serialize we do

245
00:13:44,880 --> 00:13:47,100
a serialize on the whole graph the whole

246
00:13:47,100 --> 00:13:49,650
graph is exported in the turtle format

247
00:13:49,650 --> 00:13:53,040
and we just give it to the person who's

248
00:13:53,040 --> 00:13:55,740
waiting for it here comes the big moment

249
00:13:55,740 --> 00:13:59,520
we've only created two triples how does

250
00:13:59,520 --> 00:14:03,960
this thing scale and what is big data

251
00:14:03,960 --> 00:14:07,350
big data is when you have lots of data I

252
00:14:07,350 --> 00:14:09,360
mean if there's nothing to it it's just

253
00:14:09,360 --> 00:14:11,370
taking the same principles that you

254
00:14:11,370 --> 00:14:13,830
applies to your ten or ten thousand

255
00:14:13,830 --> 00:14:15,960
records and make it scale to ten billion

256
00:14:15,960 --> 00:14:19,950
records so how do you supersize this

257
00:14:19,950 --> 00:14:22,950
thing first of all you have to loop over

258
00:14:22,950 --> 00:14:25,140
the original database contents in our

259
00:14:25,140 --> 00:14:28,020
JSON B table you have to create the

260
00:14:28,020 --> 00:14:31,560
triples in RDF libbing Python and then

261
00:14:31,560 --> 00:14:33,540
you need to find an efficient way to add

262
00:14:33,540 --> 00:14:36,120
them to the graph because this whole

263
00:14:36,120 --> 00:14:41,700
interface between rdf:li SQL alchemy and

264
00:14:41,700 --> 00:14:44,910
Postgres has some significant overhead

265
00:14:44,910 --> 00:14:47,430
and then you need to find a way to

266
00:14:47,430 --> 00:14:49,830
serialize the graph efficiently how do

267
00:14:49,830 --> 00:14:52,260
you create an RDF output do you create

268
00:14:52,260 --> 00:14:54,330
the whole thing in memory do you have

269
00:14:54,330 --> 00:14:56,850
three terabytes of memory I don't think

270
00:14:56,850 --> 00:14:58,639
so

271
00:14:58,639 --> 00:15:03,690
but why do big data without Java there's

272
00:15:03,690 --> 00:15:07,410
a question that I've heard also why do

273
00:15:07,410 --> 00:15:09,180
you do graphs without Java because

274
00:15:09,180 --> 00:15:11,880
everything is so standard we can use

275
00:15:11,880 --> 00:15:14,040
gremlin and blueprints and tinker puppet

276
00:15:14,040 --> 00:15:21,269
Genna all those things and the reason is

277
00:15:21,269 --> 00:15:26,970
that all those things are frameworks and

278
00:15:26,970 --> 00:15:29,370
they add significant processing

279
00:15:29,370 --> 00:15:31,649
overheads and these are things that you

280
00:15:31,649 --> 00:15:34,320
cannot do a scale on your own desktop

281
00:15:34,320 --> 00:15:39,060
hardware so why don't you use an

282
00:15:39,060 --> 00:15:40,860
existing triple store database

283
00:15:40,860 --> 00:15:44,430
it was another question because I'm a

284
00:15:44,430 --> 00:15:49,230
Postgres KY is woman is the answer V and

285
00:15:49,230 --> 00:15:51,959
because we know and trust Postgres not

286
00:15:51,959 --> 00:15:54,050
to lose our data and not to crash

287
00:15:54,050 --> 00:15:57,899
because it's been working reliably for

288
00:15:57,899 --> 00:16:02,490
20 years or so and I also heard the

289
00:16:02,490 --> 00:16:05,310
question that why didn't you use

290
00:16:05,310 --> 00:16:08,190
so-and-so database because our database

291
00:16:08,190 --> 00:16:10,680
is the only graph DB that does so-and-so

292
00:16:10,680 --> 00:16:14,190
but those people don't tell you that if

293
00:16:14,190 --> 00:16:16,709
they run this on very very expensive

294
00:16:16,709 --> 00:16:19,410
servers with very expensive SSDs and

295
00:16:19,410 --> 00:16:23,190
very expensive sticks of RAM so this

296
00:16:23,190 --> 00:16:27,630
argument is invalid we can just run it

297
00:16:27,630 --> 00:16:30,990
in a few lines of code this whole

298
00:16:30,990 --> 00:16:34,649
project was less than maybe 200 lines of

299
00:16:34,649 --> 00:16:37,170
code 200 lines of code is looping

300
00:16:37,170 --> 00:16:40,410
through a graph in Java without doing

301
00:16:40,410 --> 00:16:42,180
any of the other things that we're going

302
00:16:42,180 --> 00:16:46,589
to do and we also have to take in mind

303
00:16:46,589 --> 00:16:51,389
that this is an optimized method I just

304
00:16:51,389 --> 00:16:53,639
used whatever components I could find

305
00:16:53,639 --> 00:16:57,720
and has to hack a few libraries to get

306
00:16:57,720 --> 00:17:00,959
them to work reliably with grades data

307
00:17:00,959 --> 00:17:04,949
set there is potential for improvement

308
00:17:04,949 --> 00:17:07,470
this is all open source

309
00:17:07,470 --> 00:17:10,180
most people will try to push you towards

310
00:17:10,180 --> 00:17:13,630
and close source database if you want to

311
00:17:13,630 --> 00:17:17,290
do big data and analytics and graph

312
00:17:17,290 --> 00:17:23,800
operations and so on so let's look at a

313
00:17:23,800 --> 00:17:26,619
little bit of familiar code for everyone

314
00:17:26,619 --> 00:17:29,200
who for anyone who's used Postgres under

315
00:17:29,200 --> 00:17:33,430
Python and we'll see that we open a

316
00:17:33,430 --> 00:17:39,810
connection and we create a cursor and

317
00:17:39,810 --> 00:17:43,480
then we create a sequence that we're

318
00:17:43,480 --> 00:17:46,240
going to use to name our objects we need

319
00:17:46,240 --> 00:17:49,330
some integers for our object identifier

320
00:17:49,330 --> 00:17:51,820
in the Idina namespace so as we're

321
00:17:51,820 --> 00:17:53,800
reading the database we're creating

322
00:17:53,800 --> 00:17:58,990
objects in the graph database and in

323
00:17:58,990 --> 00:18:02,310
order to read efficiently the database

324
00:18:02,310 --> 00:18:05,560
we have to use an named cursor in

325
00:18:05,560 --> 00:18:08,730
Postgres because if you don't use a

326
00:18:08,730 --> 00:18:12,640
server-side cursor in this code what

327
00:18:12,640 --> 00:18:14,200
python is going to try to do well

328
00:18:14,200 --> 00:18:16,690
actually psycho PG is load the whole

329
00:18:16,690 --> 00:18:19,150
database in RAM and that is something

330
00:18:19,150 --> 00:18:20,730
that you do not want

331
00:18:20,730 --> 00:18:23,470
so we create a cursor on the server-side

332
00:18:23,470 --> 00:18:28,480
we give it some large batch sizes to

333
00:18:28,480 --> 00:18:31,540
loop through so we're given iteration

334
00:18:31,540 --> 00:18:35,650
size of 50,000 Jason records and an

335
00:18:35,650 --> 00:18:38,620
array size that we're going to use to

336
00:18:38,620 --> 00:18:41,530
fetch the data inside Python of 10,000

337
00:18:41,530 --> 00:18:44,380
and then we execute just a select a

338
00:18:44,380 --> 00:18:47,110
single column from the table that we

339
00:18:47,110 --> 00:18:52,780
have and the the loop is very simple I

340
00:18:52,780 --> 00:18:57,040
haven't included the batching code which

341
00:18:57,040 --> 00:18:59,950
is trivial but all you do is loop every

342
00:18:59,950 --> 00:19:04,120
10,000 records and for each record you

343
00:19:04,120 --> 00:19:06,610
find the attributes that you are

344
00:19:06,610 --> 00:19:09,970
interested in in the JSON data and if

345
00:19:09,970 --> 00:19:14,970
they exist then you create a new Adina

346
00:19:14,970 --> 00:19:17,050
item

347
00:19:17,050 --> 00:19:21,490
and similarly you create the triples

348
00:19:21,490 --> 00:19:25,270
that refer to this object as we saw and

349
00:19:25,270 --> 00:19:30,250
two slides before so what is our first

350
00:19:30,250 --> 00:19:33,670
challenge because this seems very simple

351
00:19:33,670 --> 00:19:36,730
up to now the first challenge is rdf

352
00:19:36,730 --> 00:19:39,160
lube SQL alchemy itself I'm I don't want

353
00:19:39,160 --> 00:19:41,500
to slag it off but it wasn't written

354
00:19:41,500 --> 00:19:44,680
with big data in mind it's just a few

355
00:19:44,680 --> 00:19:47,650
lines of code that let you connect RDF

356
00:19:47,650 --> 00:19:53,230
lab with an on disk datastore so the

357
00:19:53,230 --> 00:19:56,500
version that I started with was auto

358
00:19:56,500 --> 00:19:59,080
committing and that's quite bad because

359
00:19:59,080 --> 00:20:03,970
it was trying to insert a statement when

360
00:20:03,970 --> 00:20:06,760
you added a triple it issued an insert

361
00:20:06,760 --> 00:20:09,130
into and for the next triple it issued

362
00:20:09,130 --> 00:20:11,710
another insert and in between it

363
00:20:11,710 --> 00:20:13,900
committed as well but that has been

364
00:20:13,900 --> 00:20:15,820
fixed fortunately now it doesn't auto

365
00:20:15,820 --> 00:20:18,820
commit anymore but it still tries to for

366
00:20:18,820 --> 00:20:20,890
ten thousand records do ten thousand

367
00:20:20,890 --> 00:20:24,400
inserts so I change the code a little

368
00:20:24,400 --> 00:20:27,250
bit and now it does this thing which we

369
00:20:27,250 --> 00:20:29,590
all know and love from Postgres insert

370
00:20:29,590 --> 00:20:35,230
into table values so and so and so it

371
00:20:35,230 --> 00:20:38,680
also creates lots of indexes and those

372
00:20:38,680 --> 00:20:41,140
should be dropped if you want your

373
00:20:41,140 --> 00:20:43,030
inserts to be optimal because you're

374
00:20:43,030 --> 00:20:45,100
going to be inserting sorry I haven't

375
00:20:45,100 --> 00:20:47,590
talked about the data size the do is

376
00:20:47,590 --> 00:20:50,440
that we're talking about are all the

377
00:20:50,440 --> 00:20:53,590
duis that have ever been issued so we're

378
00:20:53,590 --> 00:20:55,240
talking about a hundred million do eyes

379
00:20:55,240 --> 00:20:59,830
and the metadata that come with each DOI

380
00:20:59,830 --> 00:21:03,550
so the final database size was around

381
00:21:03,550 --> 00:21:09,150
3.5 billion triples so those indexes

382
00:21:09,150 --> 00:21:12,010
must be dropped if you want to insert

383
00:21:12,010 --> 00:21:16,210
efficiently our second challenge is if

384
00:21:16,210 --> 00:21:18,760
this thing stops we have to restart our

385
00:21:18,760 --> 00:21:20,920
select and we have to restart inserting

386
00:21:20,920 --> 00:21:22,600
so we have to get rid of our graph

387
00:21:22,600 --> 00:21:24,520
database with because we don't know if

388
00:21:24,520 --> 00:21:26,560
we're going to insert the same things

389
00:21:26,560 --> 00:21:29,590
twice so in order to deduplicate and be

390
00:21:29,590 --> 00:21:31,299
able to continue

391
00:21:31,299 --> 00:21:33,700
we just use an offset from our select

392
00:21:33,700 --> 00:21:36,669
from our original select we do a select

393
00:21:36,669 --> 00:21:39,640
data from my table offset the number of

394
00:21:39,640 --> 00:21:43,029
records that we know we've processed and

395
00:21:43,029 --> 00:21:47,679
we also do some deduplication in order

396
00:21:47,679 --> 00:21:53,500
to not insert the same object twice so

397
00:21:53,500 --> 00:21:56,620
if we have a publisher which is

398
00:21:56,620 --> 00:21:59,470
University of Edinburgh then we want all

399
00:21:59,470 --> 00:22:01,059
records that were published by

400
00:22:01,059 --> 00:22:02,799
University of Edinburgh to point to that

401
00:22:02,799 --> 00:22:05,470
specific object we want to point it to

402
00:22:05,470 --> 00:22:08,230
direct the Edina record for University

403
00:22:08,230 --> 00:22:11,380
of Edinburgh so one way to do it is you

404
00:22:11,380 --> 00:22:14,799
cash it in Ram it's just text so it

405
00:22:14,799 --> 00:22:17,980
doesn't take up lots of RAM so the way

406
00:22:17,980 --> 00:22:22,090
to do it is you write a sparkle query

407
00:22:22,090 --> 00:22:25,510
which you prepare for execution and all

408
00:22:25,510 --> 00:22:28,980
the sparkle query does is it finds

409
00:22:28,980 --> 00:22:31,419
organisations in the forth namespace

410
00:22:31,419 --> 00:22:33,820
full phrase friend-of-a-friend it's a

411
00:22:33,820 --> 00:22:35,980
namespace that lets you identify persons

412
00:22:35,980 --> 00:22:39,010
and organizations and online accounts of

413
00:22:39,010 --> 00:22:43,320
these persons and so on and you just

414
00:22:43,320 --> 00:22:46,690
extract the label so the title of each

415
00:22:46,690 --> 00:22:50,190
organisation and you put it all in a

416
00:22:50,190 --> 00:22:54,220
nice Python dictionary that you can

417
00:22:54,220 --> 00:22:57,309
refer to so you know what your serial

418
00:22:57,309 --> 00:23:00,100
what your object identifier is that

419
00:23:00,100 --> 00:23:01,960
you've already inserted in the database

420
00:23:01,960 --> 00:23:05,230
so before you insert before you resume

421
00:23:05,230 --> 00:23:07,480
inserting you select all the things that

422
00:23:07,480 --> 00:23:10,269
you've inserted in the database that you

423
00:23:10,269 --> 00:23:12,760
know are going to be replicated such as

424
00:23:12,760 --> 00:23:18,659
persons and organizations publishers and

425
00:23:18,659 --> 00:23:22,269
you put them in a cache that you can go

426
00:23:22,269 --> 00:23:25,750
through so if you find a key in your

427
00:23:25,750 --> 00:23:28,210
JSON data which is publisher you just

428
00:23:28,210 --> 00:23:32,950
say publisher found you go through the

429
00:23:32,950 --> 00:23:35,110
cache and you see if you have

430
00:23:35,110 --> 00:23:37,450
encountered this publisher before if you

431
00:23:37,450 --> 00:23:40,809
have then you use that data identifier

432
00:23:40,809 --> 00:23:43,630
for the same publisher if you don't you

433
00:23:43,630 --> 00:23:44,140
create

434
00:23:44,140 --> 00:23:48,180
a new record and insert this publisher

435
00:23:48,180 --> 00:23:51,250
so what is the third challenge the third

436
00:23:51,250 --> 00:23:55,960
challenge again has to do with RDF lib

437
00:23:55,960 --> 00:23:58,900
SQL alchemy which tries to select the

438
00:23:58,900 --> 00:24:00,910
whole graph in memory when you're trying

439
00:24:00,910 --> 00:24:05,500
to do this so you hack the code a little

440
00:24:05,500 --> 00:24:09,250
bit and you find some SQL alchemy code

441
00:24:09,250 --> 00:24:13,450
which specifies execution options so for

442
00:24:13,450 --> 00:24:18,700
this query you just run the option

443
00:24:18,700 --> 00:24:21,400
stream results equals true and all this

444
00:24:21,400 --> 00:24:23,500
does is creates a server-side cursor in

445
00:24:23,500 --> 00:24:24,040
Postgres

446
00:24:24,040 --> 00:24:27,040
and streams their results to Python

447
00:24:27,040 --> 00:24:28,720
instead of trying to load them all

448
00:24:28,720 --> 00:24:33,130
inside pythons memory and we also added

449
00:24:33,130 --> 00:24:37,090
a batching loop which every thousand

450
00:24:37,090 --> 00:24:38,280
records this is totally arbitrary

451
00:24:38,280 --> 00:24:41,080
thousand worked best for my desktop

452
00:24:41,080 --> 00:24:44,830
machine it yields and so it continues

453
00:24:44,830 --> 00:24:49,600
the loop also rdf:li bascule alchemy has

454
00:24:49,600 --> 00:24:52,090
the tendency to cache all the literals

455
00:24:52,090 --> 00:24:54,880
that it's found up to that moment in RAM

456
00:24:54,880 --> 00:24:58,210
and that code was deleted because it's

457
00:24:58,210 --> 00:24:59,920
totally inefficient when you're trying

458
00:24:59,920 --> 00:25:04,080
to do massive batch operations like this

459
00:25:04,080 --> 00:25:06,850
so the next step once you have inserted

460
00:25:06,850 --> 00:25:09,010
everything inside your database and you

461
00:25:09,010 --> 00:25:12,010
have a fully fledged graph database that

462
00:25:12,010 --> 00:25:14,710
lives inside of Postgres is you want to

463
00:25:14,710 --> 00:25:19,000
serialize it and serializing is a CPU

464
00:25:19,000 --> 00:25:21,280
intensive operation because rdf Libba

465
00:25:21,280 --> 00:25:23,860
tries to transform one type of data into

466
00:25:23,860 --> 00:25:27,580
another so we did it using multi

467
00:25:27,580 --> 00:25:30,010
processing which is a Python module that

468
00:25:30,010 --> 00:25:33,880
lets you fork additional processes so we

469
00:25:33,880 --> 00:25:35,800
create a few processes that are the

470
00:25:35,800 --> 00:25:38,470
workers we have a writer process that

471
00:25:38,470 --> 00:25:42,550
takes ownership of standard output so

472
00:25:42,550 --> 00:25:45,040
that we can write directly to standard

473
00:25:45,040 --> 00:25:47,100
output and pipe it into other things and

474
00:25:47,100 --> 00:25:50,740
we create queues that we send batches of

475
00:25:50,740 --> 00:25:56,270
data to so we can process them in small

476
00:25:56,270 --> 00:25:59,220
Forked processes that don't have a large

477
00:25:59,220 --> 00:26:02,370
memory footprint so we send a hundred

478
00:26:02,370 --> 00:26:04,590
records at a time to the worker process

479
00:26:04,590 --> 00:26:08,250
it finishes then dies because python is

480
00:26:08,250 --> 00:26:11,820
not good at freeing up your memory so we

481
00:26:11,820 --> 00:26:13,590
actually kill the process we spawn a new

482
00:26:13,590 --> 00:26:16,560
one and generally it's much better to

483
00:26:16,560 --> 00:26:19,470
deal with the context switching of all

484
00:26:19,470 --> 00:26:23,760
this thing than having Python processes

485
00:26:23,760 --> 00:26:27,050
balloon and never give their memory back

486
00:26:27,050 --> 00:26:30,300
so all you do is you run the Python

487
00:26:30,300 --> 00:26:35,700
script that serializes the data and you

488
00:26:35,700 --> 00:26:40,110
pass it through split you give it some

489
00:26:40,110 --> 00:26:42,810
numeric identifier for the output files

490
00:26:42,810 --> 00:26:44,970
and you split the files every four

491
00:26:44,970 --> 00:26:47,670
gigabytes just so you don't have any

492
00:26:47,670 --> 00:26:51,030
huge files to load later on and those

493
00:26:51,030 --> 00:26:54,300
rdf files will be in n-triples format

494
00:26:54,300 --> 00:26:57,420
because n-triples is better than turtle

495
00:26:57,420 --> 00:26:59,070
which we used in the initial example

496
00:26:59,070 --> 00:27:02,870
because turtle actually tries to

497
00:27:02,870 --> 00:27:06,060
condense all your data for a specific

498
00:27:06,060 --> 00:27:08,790
object inside of that underneath that

499
00:27:08,790 --> 00:27:10,980
object and it also tries to sort

500
00:27:10,980 --> 00:27:13,800
everything so there is no efficient way

501
00:27:13,800 --> 00:27:16,680
to serialize in turtle because if the

502
00:27:16,680 --> 00:27:18,360
whole database has to fit in memory to

503
00:27:18,360 --> 00:27:20,460
do that so we just output in the

504
00:27:20,460 --> 00:27:21,900
simplest possible format which is

505
00:27:21,900 --> 00:27:25,830
intervals and we also compress it on the

506
00:27:25,830 --> 00:27:29,090
fly this caused an unexpected problem

507
00:27:29,090 --> 00:27:33,210
that we'll go into but it generally

508
00:27:33,210 --> 00:27:36,330
worked fine I mean seem to utilize most

509
00:27:36,330 --> 00:27:40,340
of our CPUs with little overhead and

510
00:27:40,340 --> 00:27:43,740
this is just a desktop machine it had 16

511
00:27:43,740 --> 00:27:46,800
gigs of RAM and the memory footprint was

512
00:27:46,800 --> 00:27:50,010
400 megabytes so you can do it on pretty

513
00:27:50,010 --> 00:27:52,520
much anything

514
00:27:53,429 --> 00:27:56,559
so here's our fifth challenge is the

515
00:27:56,559 --> 00:27:59,020
serialization we were creating n-triples

516
00:27:59,020 --> 00:28:03,029
at a rate with all our four CPUs that

517
00:28:03,029 --> 00:28:06,220
could not be written to disk fast enough

518
00:28:06,220 --> 00:28:09,669
so it was in essence an undetected

519
00:28:09,669 --> 00:28:11,470
memory leak the process kept getting

520
00:28:11,470 --> 00:28:13,720
bigger and bigger but it was actually

521
00:28:13,720 --> 00:28:16,899
not fast enough the disk was not fast

522
00:28:16,899 --> 00:28:20,140
enough to absorb all those triples so we

523
00:28:20,140 --> 00:28:24,610
fixed that by using gzip which is much

524
00:28:24,610 --> 00:28:30,970
lighter than B sub-2 and we also I mean

525
00:28:30,970 --> 00:28:34,690
our CPU wise and we also change the code

526
00:28:34,690 --> 00:28:39,510
to empty those queues of batches and

527
00:28:39,510 --> 00:28:43,500
wait for the standard outlet to flush

528
00:28:43,500 --> 00:28:46,210
before we continue with the next batch

529
00:28:46,210 --> 00:28:51,669
so that slowed things down about 5% but

530
00:28:51,669 --> 00:28:52,929
when you're out the things that's not

531
00:28:52,929 --> 00:28:55,090
that much data it doesn't matter matter

532
00:28:55,090 --> 00:28:58,059
I mean five percent is acceptable so

533
00:28:58,059 --> 00:29:01,600
this is what the Postgres tables look

534
00:29:01,600 --> 00:29:05,020
like when you've filled the database so

535
00:29:05,020 --> 00:29:07,510
it sqlalchemy

536
00:29:07,510 --> 00:29:11,500
creates those ugly names and puts all of

537
00:29:11,500 --> 00:29:15,250
your records in side tables so literal

538
00:29:15,250 --> 00:29:18,159
statements go into this table and your

539
00:29:18,159 --> 00:29:20,980
eyes go into this table namespace binds

540
00:29:20,980 --> 00:29:22,990
go into this table so you can imagine

541
00:29:22,990 --> 00:29:24,850
that we have tables that have billions

542
00:29:24,850 --> 00:29:28,179
of records in them and Postgres handles

543
00:29:28,179 --> 00:29:31,059
them like a champ I mean no problems at

544
00:29:31,059 --> 00:29:35,799
all the only problem is that the index

545
00:29:35,799 --> 00:29:39,370
is created for those things where B 3

546
00:29:39,370 --> 00:29:45,039
indexes which were very large so let's

547
00:29:45,039 --> 00:29:49,690
look at one record now and a record has

548
00:29:49,690 --> 00:29:56,110
our table ID it has a subject which is

549
00:29:56,110 --> 00:29:59,649
our Dino object a predicate that says

550
00:29:59,649 --> 00:30:02,009
this

551
00:30:02,009 --> 00:30:05,649
this object was associated with this

552
00:30:05,649 --> 00:30:11,859
agent which is a person so this is all

553
00:30:11,859 --> 00:30:12,879
it needs

554
00:30:12,879 --> 00:30:16,599
you don't need complicated objects in

555
00:30:16,599 --> 00:30:19,629
Java you don't need em SEC realizations

556
00:30:19,629 --> 00:30:23,079
from in to binary we're just using text

557
00:30:23,079 --> 00:30:29,440
here more problems please make sure you

558
00:30:29,440 --> 00:30:30,789
don't enter literals

559
00:30:30,789 --> 00:30:35,099
in a field that is marked as URI because

560
00:30:35,099 --> 00:30:37,359
Python will complain when you try to

561
00:30:37,359 --> 00:30:42,009
serialize a URI that is not a URI you're

562
00:30:42,009 --> 00:30:42,820
gonna have problems

563
00:30:42,820 --> 00:30:46,059
fortunately our data was relatively well

564
00:30:46,059 --> 00:30:50,649
behaved so we only had about 88 records

565
00:30:50,649 --> 00:30:53,979
out of 100 million but that had two

566
00:30:53,979 --> 00:30:57,820
invalid do eyes unfortunately you can't

567
00:30:57,820 --> 00:31:00,509
convince those people to change their

568
00:31:00,509 --> 00:31:03,759
resolvers because they do tend to

569
00:31:03,759 --> 00:31:09,459
resolve those invalid you arise so some

570
00:31:09,459 --> 00:31:13,649
of those things failed when you try to

571
00:31:13,649 --> 00:31:18,999
URL encode them so you're always going

572
00:31:18,999 --> 00:31:23,019
to have a few bad apples also we

573
00:31:23,019 --> 00:31:25,690
mentioned the indexing issue your

574
00:31:25,690 --> 00:31:28,509
indexes for data set of this size are

575
00:31:28,509 --> 00:31:33,820
going to be huge so we need to find a

576
00:31:33,820 --> 00:31:36,549
way to change this rdf:li sqlalchemy

577
00:31:36,549 --> 00:31:39,549
code to better utilize full-text search

578
00:31:39,549 --> 00:31:43,899
related indexes like gin because b3

579
00:31:43,899 --> 00:31:46,749
doesn't really scale for this sort of

580
00:31:46,749 --> 00:31:49,029
thing when you're trying to search for

581
00:31:49,029 --> 00:31:52,479
particular strings or expressions also

582
00:31:52,479 --> 00:31:55,059
one other problem we faced was that we

583
00:31:55,059 --> 00:32:01,559
had some content fields which contained

584
00:32:01,619 --> 00:32:05,469
base64 encoded bytes and some of them

585
00:32:05,469 --> 00:32:08,619
were bigger than 10 megabytes RDF live

586
00:32:08,619 --> 00:32:10,899
does not handle this well it tries to

587
00:32:10,899 --> 00:32:13,299
convert everything into text and copies

588
00:32:13,299 --> 00:32:14,600
it multiple times

589
00:32:14,600 --> 00:32:16,669
so you end up with a memory footprint

590
00:32:16,669 --> 00:32:19,580
for a 10 megabyte file you end up with a

591
00:32:19,580 --> 00:32:23,380
memory footprint of sorry 10 megabyte

592
00:32:23,380 --> 00:32:27,350
record your memory footprint goes to

593
00:32:27,350 --> 00:32:31,759
maybe 2 or 3 gigabytes so it doesn't

594
00:32:31,759 --> 00:32:33,529
really scale please make sure you don't

595
00:32:33,529 --> 00:32:36,620
have unnecessarily long records or if

596
00:32:36,620 --> 00:32:40,130
you do link them from outside the graph

597
00:32:40,130 --> 00:32:44,299
database you also need to drop the

598
00:32:44,299 --> 00:32:48,039
indexes before you create which makes

599
00:32:48,039 --> 00:32:50,450
things more complicated if you have to

600
00:32:50,450 --> 00:32:52,759
restart because in order to select all

601
00:32:52,759 --> 00:32:55,039
your publishers and persons you have to

602
00:32:55,039 --> 00:32:57,139
have an index because your operation

603
00:32:57,139 --> 00:32:59,000
will never finish if you don't so you

604
00:32:59,000 --> 00:33:01,159
need to recreate the indexes and spend a

605
00:33:01,159 --> 00:33:05,419
couple of hours waiting so PG dump is

606
00:33:05,419 --> 00:33:07,129
your friend when all the indexes have

607
00:33:07,129 --> 00:33:08,960
been created just do a PG dump which

608
00:33:08,960 --> 00:33:10,730
gives you all the drop and create

609
00:33:10,730 --> 00:33:13,750
statements so you can do it at will and

610
00:33:13,750 --> 00:33:15,769
please make sure you have enough

611
00:33:15,769 --> 00:33:18,080
maintenance work memory because if you

612
00:33:18,080 --> 00:33:23,570
try to create 100 gigabytes index with

613
00:33:23,570 --> 00:33:26,269
64 megabytes of maintenance work memory

614
00:33:26,269 --> 00:33:33,230
that will not go well so this is what PG

615
00:33:33,230 --> 00:33:37,100
dump gives you and it's just a huge

616
00:33:37,100 --> 00:33:40,399
number of indexes and primary keys that

617
00:33:40,399 --> 00:33:44,870
are created so you don't need all of

618
00:33:44,870 --> 00:33:49,009
them but find out which ones you need in

619
00:33:49,009 --> 00:33:50,539
order to do your selects if you want to

620
00:33:50,539 --> 00:33:55,909
restart thank you very much our Idina

621
00:33:55,909 --> 00:33:58,909
developer blog is a lab sadena c UK

622
00:33:58,909 --> 00:34:01,730
where we have started putting up

623
00:34:01,730 --> 00:34:03,710
interesting snippets of code and things

624
00:34:03,710 --> 00:34:06,799
we're working on and the hack will go up

625
00:34:06,799 --> 00:34:09,319
on github on rdf:li of SQL alchemy

626
00:34:09,319 --> 00:34:12,230
unfortunately that's a commit I haven't

627
00:34:12,230 --> 00:34:15,770
pushed to github yet but I'll do it very

628
00:34:15,770 --> 00:34:18,800
soon so some people have also asked me

629
00:34:18,800 --> 00:34:21,199
for this huge data set to play with I'll

630
00:34:21,199 --> 00:34:23,569
check out what we can share from the

631
00:34:23,569 --> 00:34:25,520
University and I'll be sure to share

632
00:34:25,520 --> 00:34:26,668
what I can with

633
00:34:26,668 --> 00:34:35,159
so thank you very much thanks Jimmy and

634
00:34:35,159 --> 00:34:37,129
we have time for questions so please

635
00:34:37,129 --> 00:34:40,489
wait for the microphone

636
00:34:46,869 --> 00:34:50,510
yeah so what kind of schema do you use

637
00:34:50,510 --> 00:34:53,059
in Postgres to store these triples and

638
00:34:53,059 --> 00:34:57,079
then eat all these layers of RDF this

639
00:34:57,079 --> 00:35:00,109
thing here I didn't choose it

640
00:35:00,109 --> 00:35:03,349
it's what RDF live sqlalchemy tries to

641
00:35:03,349 --> 00:35:08,839
create so all it does is it creates very

642
00:35:08,839 --> 00:35:10,549
simple tables with subject predicate

643
00:35:10,549 --> 00:35:13,790
object and so on and fills those tables

644
00:35:13,790 --> 00:35:19,010
with all our triples so it's only five

645
00:35:19,010 --> 00:35:21,710
tables and two of them are the really

646
00:35:21,710 --> 00:35:23,750
large ones one of the reasons that you

647
00:35:23,750 --> 00:35:25,040
are eyes and one of the contains of

648
00:35:25,040 --> 00:35:27,470
strings okay so just the triples

649
00:35:27,470 --> 00:35:31,069
themselves basically directly and the

650
00:35:31,069 --> 00:35:34,520
SPARQL i-i don't really know it very

651
00:35:34,520 --> 00:35:36,680
well but i think it would support

652
00:35:36,680 --> 00:35:39,260
recursive queries or what would in SQL

653
00:35:39,260 --> 00:35:44,930
be recursive queries does does it and if

654
00:35:44,930 --> 00:35:47,750
so is such a query map to the database

655
00:35:47,750 --> 00:35:50,630
or does the software need to load

656
00:35:50,630 --> 00:35:53,030
everything in memory to let me try to do

657
00:35:53,030 --> 00:35:56,869
very advanced sparkle queries but I'm

658
00:35:56,869 --> 00:35:58,609
sure that rdf lip supports the full

659
00:35:58,609 --> 00:36:02,780
sparkle query set so these tend to work

660
00:36:02,780 --> 00:36:05,000
as fast as your SQL query would work

661
00:36:05,000 --> 00:36:08,049
inside Postgres so we can use this

662
00:36:08,049 --> 00:36:11,000
that's another good example is we can

663
00:36:11,000 --> 00:36:14,450
use it to serve an API directly from our

664
00:36:14,450 --> 00:36:16,940
Postgres that serves sparkle queries

665
00:36:16,940 --> 00:36:18,859
with the performance that we expect from

666
00:36:18,859 --> 00:36:21,980
Postgres and not some java thing which

667
00:36:21,980 --> 00:36:24,079
will have a huge memory footprint and

668
00:36:24,079 --> 00:36:26,329
need tens or hundreds of servers to

669
00:36:26,329 --> 00:36:30,200
reply do you know if sparkle is as

670
00:36:30,200 --> 00:36:34,460
expensive as SQL or more expressive it

671
00:36:34,460 --> 00:36:36,170
doesn't have all the features that we're

672
00:36:36,170 --> 00:36:38,869
used to in advanced SQL but you can do

673
00:36:38,869 --> 00:36:40,540
many things and you can also do

674
00:36:40,540 --> 00:36:42,650
multi-dimensional queries which now but

675
00:36:42,650 --> 00:36:44,030
I meant the other way around can any

676
00:36:44,030 --> 00:36:46,579
sparkle query be translated to an SQL

677
00:36:46,579 --> 00:36:51,500
query you know not directly Maps it Maps

678
00:36:51,500 --> 00:36:54,290
the things that it needs to select from

679
00:36:54,290 --> 00:36:57,410
our relational database into relational

680
00:36:57,410 --> 00:36:58,740
queries

681
00:36:58,740 --> 00:37:01,680
and that's done by rdf:li of SQL alchemy

682
00:37:01,680 --> 00:37:05,790
so the better we hack SQL rdf:li

683
00:37:05,790 --> 00:37:08,010
vascular Alchemy's code the better the

684
00:37:08,010 --> 00:37:11,240
queries will be okay thank you

685
00:37:11,240 --> 00:37:22,950
are there more questions thank you all

686
00:37:22,950 --> 00:37:24,869
right thank you for the talk I'm still

687
00:37:24,869 --> 00:37:26,790
I'm so I mean very interesting I'm sort

688
00:37:26,790 --> 00:37:28,410
of slight I'm looking to get that slide

689
00:37:28,410 --> 00:37:29,550
I'm trying to work out the relationship

690
00:37:29,550 --> 00:37:34,619
between the graph database and I see

691
00:37:34,619 --> 00:37:37,440
there's a subject predicate object then

692
00:37:37,440 --> 00:37:40,140
I see three other fields I can I can you

693
00:37:40,140 --> 00:37:42,240
know there's an ID that's fine then

694
00:37:42,240 --> 00:37:45,630
you've got a context and a term so the

695
00:37:45,630 --> 00:37:48,869
context tells us that this data belongs

696
00:37:48,869 --> 00:37:52,530
to our named graph which is called RDF

697
00:37:52,530 --> 00:37:56,490
graph and term comp is some term

698
00:37:56,490 --> 00:37:58,440
combination things that I haven't used

699
00:37:58,440 --> 00:38:00,990
them don't know how to use and is it

700
00:38:00,990 --> 00:38:03,270
always it's always zero in our data for

701
00:38:03,270 --> 00:38:05,340
some reason that seems like a lot of

702
00:38:05,340 --> 00:38:10,100
wasted space on on a large data set yes

703
00:38:10,100 --> 00:38:12,780
the other question I had I thought you

704
00:38:12,780 --> 00:38:14,190
mentioned this earlier and he's touched

705
00:38:14,190 --> 00:38:16,109
on my last question to he's you know

706
00:38:16,109 --> 00:38:18,570
you're gonna have these these you can

707
00:38:18,570 --> 00:38:21,090
have your sparking queries and they're

708
00:38:21,090 --> 00:38:22,920
gonna get mapped to you know database

709
00:38:22,920 --> 00:38:29,490
operations do you next thing for that

710
00:38:29,490 --> 00:38:30,930
how much time do you spend thinking

711
00:38:30,930 --> 00:38:31,950
about where you need to put your

712
00:38:31,950 --> 00:38:36,150
Postgres indexes I'm tuning it you

713
00:38:36,150 --> 00:38:39,570
basically have all those indexes to

714
00:38:39,570 --> 00:38:43,220
consider so you don't need all of them

715
00:38:43,220 --> 00:38:46,109
you just the best way would be to run

716
00:38:46,109 --> 00:38:50,400
your query and just analyze it and see

717
00:38:50,400 --> 00:38:52,730
what tables and what columns is hitting

718
00:38:52,730 --> 00:38:58,310
the index has come prefilled with yes

719
00:38:59,930 --> 00:39:03,960
you can of course we can but it all

720
00:39:03,960 --> 00:39:07,710
depends on what your what rdf:li bascule

721
00:39:07,710 --> 00:39:09,359
alchemy is trying to select from your

722
00:39:09,359 --> 00:39:11,609
database so it all depends on the

723
00:39:11,609 --> 00:39:14,220
generated query so you have to enable

724
00:39:14,220 --> 00:39:17,510
logging to find that statement and

725
00:39:17,510 --> 00:39:20,580
analyze it to create your indexes but

726
00:39:20,580 --> 00:39:21,509
you haven't you

727
00:39:21,509 --> 00:39:26,069
I didn't really have a use for indexes

728
00:39:26,069 --> 00:39:29,399
in this project until I mean it will

729
00:39:29,399 --> 00:39:31,769
need indexes if it's going to be

730
00:39:31,769 --> 00:39:34,079
connected to an API which will serve

731
00:39:34,079 --> 00:39:36,029
things directly from the database that

732
00:39:36,029 --> 00:39:37,859
the but the only use case I had for the

733
00:39:37,859 --> 00:39:41,159
index is up to now was to restart the

734
00:39:41,159 --> 00:39:53,329
insert operation any other questions

735
00:39:54,559 --> 00:40:03,869
if not then let's thank Jimmy again the

736
00:40:03,869 --> 00:40:06,149
next talk will be about the evolution of

737
00:40:06,149 --> 00:40:08,219
fault tolerance in Postgres at 3 o'clock

738
00:40:08,219 --> 00:40:12,979
so they have a 20 minute break

739
00:40:28,600 --> 00:40:30,660
you

