1
00:00:00,439 --> 00:00:03,439
hello

2
00:00:04,590 --> 00:00:09,389
I am mark Callahan I work on small data

3
00:00:09,389 --> 00:00:14,250
or transaction processing at Facebook

4
00:00:14,250 --> 00:00:17,310
we're jealous at the pipe that big data

5
00:00:17,310 --> 00:00:18,869
gets that we're trying to bring small

6
00:00:18,869 --> 00:00:20,090
back

7
00:00:20,090 --> 00:00:24,119
my focus is storage efficiency we have a

8
00:00:24,119 --> 00:00:28,529
lot of data at work so a few of us focus

9
00:00:28,529 --> 00:00:30,779
on reducing the amount of storage

10
00:00:30,779 --> 00:00:33,980
hardware we need to store that data I

11
00:00:33,980 --> 00:00:37,800
also named the project my rocks for the

12
00:00:37,800 --> 00:00:40,680
the engine so I'm very happy about my

13
00:00:40,680 --> 00:00:43,980
contribution to the effort my rocks is

14
00:00:43,980 --> 00:00:46,770
so rocks TB is a log structured merge

15
00:00:46,770 --> 00:00:50,570
tree it started out as a fork of leveldb

16
00:00:50,570 --> 00:00:53,730
it's changed so much at this point that

17
00:00:53,730 --> 00:00:56,280
I don't well it used to be a fork it's

18
00:00:56,280 --> 00:00:59,670
its own project at this point my rocks

19
00:00:59,670 --> 00:01:02,850
is a MySQL storage engine so we

20
00:01:02,850 --> 00:01:05,430
implement the storage engine API for

21
00:01:05,430 --> 00:01:08,640
MySQL many people have tried to do this

22
00:01:08,640 --> 00:01:11,850
it is hard to implement the storage

23
00:01:11,850 --> 00:01:14,759
engine API we are doing it it runs in

24
00:01:14,759 --> 00:01:18,479
production today a few months ago we

25
00:01:18,479 --> 00:01:22,470
announced that it's running on about 5%

26
00:01:22,470 --> 00:01:26,580
of the database servers in one of the

27
00:01:26,580 --> 00:01:29,550
data centers we've gone significantly

28
00:01:29,550 --> 00:01:31,410
beyond that we just haven't been

29
00:01:31,410 --> 00:01:34,979
specific about how far how much progress

30
00:01:34,979 --> 00:01:37,050
we have made so this is something that's

31
00:01:37,050 --> 00:01:41,310
running in production it's it's real it

32
00:01:41,310 --> 00:01:44,610
will eventually be usable for people

33
00:01:44,610 --> 00:01:46,860
beyond Facebook Sergey petunias here I

34
00:01:46,860 --> 00:01:48,780
like to always acknowledge his

35
00:01:48,780 --> 00:01:51,599
contribution he is a significant

36
00:01:51,599 --> 00:01:54,149
contributor to my rocks we would not be

37
00:01:54,149 --> 00:01:56,099
here without his effort so I appreciate

38
00:01:56,099 --> 00:01:58,160
that

39
00:01:58,430 --> 00:02:01,890
the brief editorial I just wanted to say

40
00:02:01,890 --> 00:02:04,920
for a few years there I think doing

41
00:02:04,920 --> 00:02:07,440
web-scale MySQL I got a lot of advice

42
00:02:07,440 --> 00:02:09,929
from people about what I should be doing

43
00:02:09,929 --> 00:02:12,540
or what I should be using

44
00:02:12,540 --> 00:02:14,100
maybe I don't get that much advice

45
00:02:14,100 --> 00:02:15,660
anymore

46
00:02:15,660 --> 00:02:18,720
it there were some dark years and there

47
00:02:18,720 --> 00:02:21,180
were some issues with the product that

48
00:02:21,180 --> 00:02:23,880
kind of desert earned the reputation but

49
00:02:23,880 --> 00:02:26,190
we're in a very different point right

50
00:02:26,190 --> 00:02:28,739
now so if just look at features there

51
00:02:28,739 --> 00:02:31,709
were several talks today about group

52
00:02:31,709 --> 00:02:34,980
replication synchronous replication in

53
00:02:34,980 --> 00:02:38,390
MySQL and it's it's a really big deal

54
00:02:38,390 --> 00:02:41,400
not losing commits if you look at

55
00:02:41,400 --> 00:02:43,380
performance in my sequel five seven and

56
00:02:43,380 --> 00:02:44,970
my sequel eight with what they've done

57
00:02:44,970 --> 00:02:48,480
for nodb it's a really big deal terms of

58
00:02:48,480 --> 00:02:51,480
usage I gave a community talk a few

59
00:02:51,480 --> 00:02:52,950
years back and at the end of the talk

60
00:02:52,950 --> 00:02:54,959
someone said well you're just using

61
00:02:54,959 --> 00:03:00,060
MySQL because it's legacy it was an

62
00:03:00,060 --> 00:03:02,640
interesting opinion we are competing for

63
00:03:02,640 --> 00:03:06,320
and winning new workloads at Facebook

64
00:03:06,320 --> 00:03:08,670
I'm gonna be vague because I haven't

65
00:03:08,670 --> 00:03:10,620
been clear to talk about all of these

66
00:03:10,620 --> 00:03:12,810
workloads but one that was disclosed in

67
00:03:12,810 --> 00:03:16,290
public was we put in ODB on top of HBase

68
00:03:16,290 --> 00:03:19,860
for messaging and the reason was to

69
00:03:19,860 --> 00:03:22,350
improve quality of service we are

70
00:03:22,350 --> 00:03:24,200
competing for additional workload so

71
00:03:24,200 --> 00:03:26,850
we're in a good place

72
00:03:26,850 --> 00:03:29,640
externally if you look at what the cloud

73
00:03:29,640 --> 00:03:31,890
providers are doing Amazon with Aurora

74
00:03:31,890 --> 00:03:35,910
and RDS it's a highly competitive

75
00:03:35,910 --> 00:03:39,660
product on the cloud and once you add or

76
00:03:39,660 --> 00:03:41,959
once you have highly available storage

77
00:03:41,959 --> 00:03:45,600
it's it's much easier to scale and

78
00:03:45,600 --> 00:03:48,540
operate MySQL and then finally if you

79
00:03:48,540 --> 00:03:51,030
look at the community and just my proxy

80
00:03:51,030 --> 00:03:54,450
for one of my proxies for success of the

81
00:03:54,450 --> 00:03:56,519
community is if you look at DB engines

82
00:03:56,519 --> 00:04:00,269
rankings MySQL is likely to become the

83
00:04:00,269 --> 00:04:01,829
number one database this year which

84
00:04:01,829 --> 00:04:04,709
means it will pass Oracle it's ahead of

85
00:04:04,709 --> 00:04:07,920
sequel server so I'm just I think we're

86
00:04:07,920 --> 00:04:09,580
in a really good place

87
00:04:09,580 --> 00:04:13,630
I get I get two questions frequently or

88
00:04:13,630 --> 00:04:15,610
I'm trying to answer two questions in

89
00:04:15,610 --> 00:04:18,370
terms of marketing why my rocks why are

90
00:04:18,370 --> 00:04:20,890
we doing it or why might you want to use

91
00:04:20,890 --> 00:04:24,880
it and and when to use it and for why my

92
00:04:24,880 --> 00:04:27,070
focus is storage efficiency so that I

93
00:04:27,070 --> 00:04:29,830
can go technical or less technical but

94
00:04:29,830 --> 00:04:32,380
really it's the best space efficiency

95
00:04:32,380 --> 00:04:34,630
meaning we use the least amount of space

96
00:04:34,630 --> 00:04:37,120
and it's not just an implementation

97
00:04:37,120 --> 00:04:42,940
thing the algorithm that we use is an

98
00:04:42,940 --> 00:04:44,920
ala log structured merge tree with

99
00:04:44,920 --> 00:04:49,360
leveled compaction we're likely to in

100
00:04:49,360 --> 00:04:50,890
the worst case we're going to use about

101
00:04:50,890 --> 00:04:55,930
10% more storage than optimal and I'm

102
00:04:55,930 --> 00:04:58,060
not aware of another database algorithm

103
00:04:58,060 --> 00:05:04,120
that can beat that 10% overhead the B

104
00:05:04,120 --> 00:05:06,400
tree you're likely to have about a 50%

105
00:05:06,400 --> 00:05:11,230
overhead better right efficiency by

106
00:05:11,230 --> 00:05:12,940
better write efficiency I mean if

107
00:05:12,940 --> 00:05:16,750
forgiven workload how much data are you

108
00:05:16,750 --> 00:05:19,210
writing to storage per transaction so if

109
00:05:19,210 --> 00:05:21,850
you run i/o staffed while running a

110
00:05:21,850 --> 00:05:25,419
benchmark we are with rocks DB and my

111
00:05:25,419 --> 00:05:28,840
rocks we write less back to storage per

112
00:05:28,840 --> 00:05:32,020
transaction good read efficiency and

113
00:05:32,020 --> 00:05:34,330
good is the vague term when the project

114
00:05:34,330 --> 00:05:37,720
started we tried to set some goals you

115
00:05:37,720 --> 00:05:40,330
know at what point do we think my rocks

116
00:05:40,330 --> 00:05:43,930
would be usable and so I I tried to be

117
00:05:43,930 --> 00:05:46,540
vague but I I guessed you know we want

118
00:05:46,540 --> 00:05:48,850
to compress 2x better than compressed

119
00:05:48,850 --> 00:05:49,419
nodb

120
00:05:49,419 --> 00:05:53,140
and we want to do that without being too

121
00:05:53,140 --> 00:05:55,660
much slower so we accepted that we might

122
00:05:55,660 --> 00:05:58,060
be slower in terms of response time for

123
00:05:58,060 --> 00:06:02,070
queries we just wanted to be good enough

124
00:06:02,070 --> 00:06:08,100
and the last point for why my rocks is

125
00:06:08,100 --> 00:06:12,540
it has benefits for both SSD NAND flash

126
00:06:12,540 --> 00:06:16,900
and disk the benefit the benefit for SSD

127
00:06:16,900 --> 00:06:19,390
is that compression is better so you

128
00:06:19,390 --> 00:06:21,820
will use less SS

129
00:06:21,820 --> 00:06:23,800
which can be a big deal if you're

130
00:06:23,800 --> 00:06:26,650
purchasing the SSD better right

131
00:06:26,650 --> 00:06:28,960
efficiency compared to something like

132
00:06:28,960 --> 00:06:32,470
you know DB means that endurance on the

133
00:06:32,470 --> 00:06:36,010
SSD is going to be less of an issue for

134
00:06:36,010 --> 00:06:40,030
a disc the using less disk capacity is

135
00:06:40,030 --> 00:06:42,430
not always a big deal if you're doing

136
00:06:42,430 --> 00:06:44,710
transaction process processing on top of

137
00:06:44,710 --> 00:06:47,170
a disk array you're you might not be

138
00:06:47,170 --> 00:06:48,970
using all of the disks so you might have

139
00:06:48,970 --> 00:06:52,660
spared capacity on the disk the benefit

140
00:06:52,660 --> 00:06:55,780
for discs is that better write

141
00:06:55,780 --> 00:07:00,070
efficiency means we use use less write

142
00:07:00,070 --> 00:07:05,350
capacity from the disk so we save more

143
00:07:05,350 --> 00:07:07,780
of the i/o capacity for reads so we can

144
00:07:07,780 --> 00:07:10,060
do more queries per second because more

145
00:07:10,060 --> 00:07:12,870
we're more efficient on the right side

146
00:07:12,870 --> 00:07:15,970
from the user database workload we

147
00:07:15,970 --> 00:07:18,640
actually compress use about half the

148
00:07:18,640 --> 00:07:20,620
space compared to compressed in OB Bay

149
00:07:20,620 --> 00:07:22,780
and about one-quarter of the space

150
00:07:22,780 --> 00:07:25,030
compared to uncompressed in ODB so we

151
00:07:25,030 --> 00:07:27,280
met the initial requirement for the

152
00:07:27,280 --> 00:07:29,730
project which was 2x better compression

153
00:07:29,730 --> 00:07:33,670
than compressed in ODB the last point

154
00:07:33,670 --> 00:07:36,370
was a surprise the right rate to storage

155
00:07:36,370 --> 00:07:38,530
with my rocks for this production

156
00:07:38,530 --> 00:07:41,410
workload is about 10 percent of the

157
00:07:41,410 --> 00:07:44,380
right rate to storage within ODB so

158
00:07:44,380 --> 00:07:46,360
we're just you know nodb is writing 10

159
00:07:46,360 --> 00:07:48,850
times as much in terms of kilobytes per

160
00:07:48,850 --> 00:07:51,220
second or megabytes per second to the

161
00:07:51,220 --> 00:07:55,510
SSD so the SSD endurance is much more of

162
00:07:55,510 --> 00:07:56,160
an issue

163
00:07:56,160 --> 00:07:59,320
our switching to my rocks makes that

164
00:07:59,320 --> 00:08:01,930
less of an issue so for when to consider

165
00:08:01,930 --> 00:08:05,560
my rocks I've been trying to slowly

166
00:08:05,560 --> 00:08:07,870
expanding the marketing claims I'm

167
00:08:07,870 --> 00:08:10,480
making from my rocks number one is if

168
00:08:10,480 --> 00:08:12,340
you're using a no debate and the

169
00:08:12,340 --> 00:08:14,830
databases database is larger than memory

170
00:08:14,830 --> 00:08:18,540
my goal is for my rocks to be

171
00:08:18,540 --> 00:08:20,260
competitive with nodb

172
00:08:20,260 --> 00:08:22,570
and I'm going to say it's a goal at this

173
00:08:22,570 --> 00:08:25,510
point there are performance problems in

174
00:08:25,510 --> 00:08:28,420
my rocks I do a lot of performance

175
00:08:28,420 --> 00:08:31,330
testing I also respond

176
00:08:31,330 --> 00:08:34,210
I think Valyria is probably here most

177
00:08:34,210 --> 00:08:35,740
recently Valerie reported a

178
00:08:35,740 --> 00:08:37,750
performance problem which I cannot

179
00:08:37,750 --> 00:08:41,729
reproduce which is funny because I

180
00:09:06,399 --> 00:09:09,170
okay so I initially made a joke when

181
00:09:09,170 --> 00:09:11,089
Valerie mentioned the problem that I

182
00:09:11,089 --> 00:09:14,029
said ah I can't reproduce just because

183
00:09:14,029 --> 00:09:15,889
it's the worst thing you can do in

184
00:09:15,889 --> 00:09:17,600
support is initially just tell the

185
00:09:17,600 --> 00:09:19,220
customer I can't reproduce it's not a

186
00:09:19,220 --> 00:09:20,389
bug

187
00:09:20,389 --> 00:09:23,449
but I actually can't reproduce it but so

188
00:09:23,449 --> 00:09:25,459
we have performance problems I know of

189
00:09:25,459 --> 00:09:26,870
some of them if the more that I know

190
00:09:26,870 --> 00:09:30,290
about the more we can fix but we're

191
00:09:30,290 --> 00:09:33,620
trying to target similar query latency

192
00:09:33,620 --> 00:09:38,029
as in ODB house or better so the goals

193
00:09:38,029 --> 00:09:43,160
are expanding progress so Yoshinori

194
00:09:43,160 --> 00:09:44,690
speaks after me which means he can

195
00:09:44,690 --> 00:09:46,040
correct all the mistakes I make

196
00:09:46,040 --> 00:09:48,350
Yoshinori is definitely more of an

197
00:09:48,350 --> 00:09:52,100
expert on this topic but from a

198
00:09:52,100 --> 00:09:53,899
marketing perspective if efficient

199
00:09:53,899 --> 00:09:55,940
performance is how I'd like to describe

200
00:09:55,940 --> 00:09:57,940
my rocks so we want storage efficiency

201
00:09:57,940 --> 00:10:03,199
with good enough performance and we got

202
00:10:03,199 --> 00:10:05,600
that we deployed it in production for

203
00:10:05,600 --> 00:10:07,430
the user database workload which is one

204
00:10:07,430 --> 00:10:10,670
is our most important my sequel workload

205
00:10:10,670 --> 00:10:15,130
that we we run and the deployment is

206
00:10:15,130 --> 00:10:20,329
continuing it's moving fast with

207
00:10:20,329 --> 00:10:24,380
correctness we started ports to Percona

208
00:10:24,380 --> 00:10:26,360
server and marija DB server this is a

209
00:10:26,360 --> 00:10:28,519
big deal to me I want people to use my

210
00:10:28,519 --> 00:10:30,860
rocks not just Facebook people but

211
00:10:30,860 --> 00:10:33,410
anywhere and for that to happen it needs

212
00:10:33,410 --> 00:10:35,389
to be in a proper distribution with

213
00:10:35,389 --> 00:10:39,110
support and so that's possible now with

214
00:10:39,110 --> 00:10:42,079
getting my rocks into Percona server and

215
00:10:42,079 --> 00:10:46,939
marija DB server for 2017 documentation

216
00:10:46,939 --> 00:10:49,069
and not even better documentation I it

217
00:10:49,069 --> 00:10:50,750
might be safe to say we need

218
00:10:50,750 --> 00:10:52,850
documentation

219
00:10:52,850 --> 00:10:56,959
I frequently get reminded of things I

220
00:10:56,959 --> 00:11:00,290
forgot about in part because a lot of

221
00:11:00,290 --> 00:11:02,059
the details are on internal discussion

222
00:11:02,059 --> 00:11:04,819
groups we need more production

223
00:11:04,819 --> 00:11:06,799
deployments and we are competing for

224
00:11:06,799 --> 00:11:09,109
them internally I know of at least one

225
00:11:09,109 --> 00:11:10,939
other company that I won't name in

226
00:11:10,939 --> 00:11:14,059
public but who is potentially using my

227
00:11:14,059 --> 00:11:16,669
rocks today they have a lot of my sequel

228
00:11:16,669 --> 00:11:19,569
talent though to make that happen

229
00:11:19,569 --> 00:11:22,459
hopefully it will be usable I can't

230
00:11:22,459 --> 00:11:25,279
claim it will be a ga for Murray DB or

231
00:11:25,279 --> 00:11:26,869
Percona server that's someone else's

232
00:11:26,869 --> 00:11:29,989
decision but it will be released in in

233
00:11:29,989 --> 00:11:32,720
some form factor that can be used and

234
00:11:32,720 --> 00:11:34,839
then we have a lot of performance

235
00:11:34,839 --> 00:11:38,600
improvements I have more details I need

236
00:11:38,600 --> 00:11:40,459
to publish in the performance

237
00:11:40,459 --> 00:11:42,679
comparisons and then the last point is

238
00:11:42,679 --> 00:11:45,709
features we want to expose more of the

239
00:11:45,709 --> 00:11:48,439
right optimizations that we have in my

240
00:11:48,439 --> 00:11:51,529
raw in rocks DB expose them via SQL and

241
00:11:51,529 --> 00:11:54,799
an example is time to live so you can

242
00:11:54,799 --> 00:11:57,559
have data aged out without having to

243
00:11:57,559 --> 00:11:59,809
explicitly delete it and that's likely

244
00:11:59,809 --> 00:12:03,339
to be the first rocks DB optimization we

245
00:12:03,339 --> 00:12:08,419
expose in SQL so efficiency I've made

246
00:12:08,419 --> 00:12:10,809
some strong claims I've done a lot of

247
00:12:10,809 --> 00:12:14,299
performance results that I publish I try

248
00:12:14,299 --> 00:12:15,860
to explain the results that I share I

249
00:12:15,860 --> 00:12:18,169
don't always do it sometimes I publish

250
00:12:18,169 --> 00:12:20,449
and then people ask me and I have to go

251
00:12:20,449 --> 00:12:22,879
back and revisit but a high at a high

252
00:12:22,879 --> 00:12:25,339
level you know in terms of space read

253
00:12:25,339 --> 00:12:28,069
and write efficiency why is space

254
00:12:28,069 --> 00:12:29,989
efficiency for a log structured merge

255
00:12:29,989 --> 00:12:32,509
tree why is it better than a B tree and

256
00:12:32,509 --> 00:12:35,299
the first is fragmentation the leaf

257
00:12:35,299 --> 00:12:39,470
nodes of a B tree will be one-half to

258
00:12:39,470 --> 00:12:41,959
two-thirds full subject to a random

259
00:12:41,959 --> 00:12:45,619
ordered sequence of updates so if

260
00:12:45,619 --> 00:12:47,779
they're 1/2 to 2/3 full you're wasting

261
00:12:47,779 --> 00:12:50,869
1/3 to 1/2 of your space in the buffer

262
00:12:50,869 --> 00:12:54,949
pool in memory and on disk with the log

263
00:12:54,949 --> 00:12:57,259
structured merge tree the space

264
00:12:57,259 --> 00:13:00,319
overheads about 10% rather than 1/3 to

265
00:13:00,319 --> 00:13:01,599
1/2

266
00:13:01,599 --> 00:13:03,909
fixed page size if you use compression

267
00:13:03,909 --> 00:13:06,909
within ODB your page size is fixed on on

268
00:13:06,909 --> 00:13:09,129
disk so if you have 2x compression

269
00:13:09,129 --> 00:13:13,389
configured 16 K page in memory 8 K page

270
00:13:13,389 --> 00:13:16,959
on disk compress 16 K down to 5 you

271
00:13:16,959 --> 00:13:19,779
still have to use 8 K on disk so it's

272
00:13:19,779 --> 00:13:23,019
another source of wasted compression per

273
00:13:23,019 --> 00:13:26,529
row metadata within ODB 13 bytes with

274
00:13:26,529 --> 00:13:29,679
rocks TB it's 6 or 7 bytes and we for

275
00:13:29,679 --> 00:13:32,589
most of the data for 90% of the data we

276
00:13:32,589 --> 00:13:34,689
are usually compressing that down to 0

277
00:13:34,689 --> 00:13:37,329
bytes so for small rows it's a really

278
00:13:37,329 --> 00:13:40,869
big deal key prefix encoding is applied

279
00:13:40,869 --> 00:13:43,509
in memory for uncompressed blocks and on

280
00:13:43,509 --> 00:13:48,189
disk with rocks TV so keys or indexes

281
00:13:48,189 --> 00:13:53,649
take up less space with rocks TV why is

282
00:13:53,649 --> 00:13:55,509
right efficiency better well if if

283
00:13:55,509 --> 00:13:57,339
you're using more space with a b-tree

284
00:13:57,339 --> 00:14:01,209
you have more pages to write back so

285
00:14:01,209 --> 00:14:04,659
that's point number one point number 2

286
00:14:04,659 --> 00:14:07,839
we tend to operate databases configured

287
00:14:07,839 --> 00:14:10,269
so that the working set is does not fit

288
00:14:10,269 --> 00:14:12,189
in RAM and the reason is if you have

289
00:14:12,189 --> 00:14:15,129
fast storage we have really good NAND

290
00:14:15,129 --> 00:14:17,979
flash if you have fast storage and you

291
00:14:17,979 --> 00:14:19,539
have your working set cached and RAM

292
00:14:19,539 --> 00:14:22,869
you're either using storage that's too

293
00:14:22,869 --> 00:14:23,499
fast

294
00:14:23,499 --> 00:14:28,539
or you have too much RAM and it's not

295
00:14:28,539 --> 00:14:30,729
always true but since we have fast

296
00:14:30,729 --> 00:14:32,379
storage we want to use it we want to do

297
00:14:32,379 --> 00:14:35,589
reads to it we shrink we try to use as

298
00:14:35,589 --> 00:14:38,259
little RAM as possible the worst case

299
00:14:38,259 --> 00:14:40,659
for a b-tree in in this kind of

300
00:14:40,659 --> 00:14:42,519
configuration is you're writing back

301
00:14:42,519 --> 00:14:45,519
pages with only one modified row on the

302
00:14:45,519 --> 00:14:47,619
page and in that case the right

303
00:14:47,619 --> 00:14:49,539
amplification is the size of the page

304
00:14:49,539 --> 00:14:55,300
over the size of the row that problem is

305
00:14:55,300 --> 00:14:57,880
with a log structured merge tree we're

306
00:14:57,880 --> 00:14:59,860
only writing modified rows there are no

307
00:14:59,860 --> 00:15:03,190
pages on the right path and then finally

308
00:15:03,190 --> 00:15:05,170
the the double right buffer within ODB

309
00:15:05,170 --> 00:15:08,620
doubles the right rate to storage for

310
00:15:08,620 --> 00:15:12,370
read efficiency we have more data in

311
00:15:12,370 --> 00:15:14,950
cache key prefix encoding no

312
00:15:14,950 --> 00:15:16,930
fragmentation so the cache hit rate is

313
00:15:16,930 --> 00:15:19,720
better so reads are faster a bloom

314
00:15:19,720 --> 00:15:22,960
filter is especially effective if you

315
00:15:22,960 --> 00:15:24,960
have a workload that's occasionally

316
00:15:24,960 --> 00:15:28,590
trying to read keys that don't exist

317
00:15:28,590 --> 00:15:31,210
with the B tree you might have to read a

318
00:15:31,210 --> 00:15:32,980
leaf page from storage with the bloom

319
00:15:32,980 --> 00:15:37,570
filter we avoid that finally we spend

320
00:15:37,570 --> 00:15:39,310
less on writes so we have more i/o

321
00:15:39,310 --> 00:15:41,980
capacity to spend on reads and then the

322
00:15:41,980 --> 00:15:44,020
last point is this thing called read

323
00:15:44,020 --> 00:15:46,720
free index maintenance maintenance for

324
00:15:46,720 --> 00:15:49,030
non-unique secondary indexes we don't

325
00:15:49,030 --> 00:15:51,850
have there's no leaf page to read n

326
00:15:51,850 --> 00:15:54,670
ODB's read-modify-write for secondary

327
00:15:54,670 --> 00:15:58,330
index maintenance my rocks rocks DB is

328
00:15:58,330 --> 00:16:04,210
right only I feel like I have a personal

329
00:16:04,210 --> 00:16:07,060
brand at stake prior to my rocks I

330
00:16:07,060 --> 00:16:08,710
didn't really have a product I was

331
00:16:08,710 --> 00:16:10,600
always on the using side of a product

332
00:16:10,600 --> 00:16:12,370
and and so it was easy to write about

333
00:16:12,370 --> 00:16:14,500
problems that I wanted to get fixed by

334
00:16:14,500 --> 00:16:16,930
market bugs so now I'm on a team that

335
00:16:16,930 --> 00:16:19,660
has a product called my rocks so the

336
00:16:19,660 --> 00:16:22,480
question is am I being honest and it's a

337
00:16:22,480 --> 00:16:26,800
challenge it's not as easy to be open

338
00:16:26,800 --> 00:16:28,300
about problems when you're on the

339
00:16:28,300 --> 00:16:32,440
product side so I'm working on it but

340
00:16:32,440 --> 00:16:34,750
there are problems rocks TB is too hard

341
00:16:34,750 --> 00:16:39,070
to tune the the workaround for now is

342
00:16:39,070 --> 00:16:41,830
we're improving the defaults at this

343
00:16:41,830 --> 00:16:43,360
point I claim there's two options you

344
00:16:43,360 --> 00:16:46,090
need to set the block cache size and the

345
00:16:46,090 --> 00:16:48,630
number of threads to use for compaction

346
00:16:48,630 --> 00:16:52,090
we can't today we we're not good at

347
00:16:52,090 --> 00:16:55,120
guessing that based on hardware capacity

348
00:16:55,120 --> 00:16:57,360
if you set those two options you'll get

349
00:16:57,360 --> 00:16:59,680
good enough performance and I'm going to

350
00:16:59,680 --> 00:17:02,290
skip the detail because I'm talking too

351
00:17:02,290 --> 00:17:05,709
long about some slides we had someone

352
00:17:05,709 --> 00:17:07,959
report a problem with sis bench an

353
00:17:07,959 --> 00:17:12,520
external user or early evaluator oh Lt P

354
00:17:12,520 --> 00:17:16,240
Lua does potentially long range scans

355
00:17:16,240 --> 00:17:20,109
under concurrency those were slow we

356
00:17:20,109 --> 00:17:23,109
doubled the QP s we fixed the

357
00:17:23,109 --> 00:17:25,560
performance problem doubled performance

358
00:17:25,560 --> 00:17:28,860
it was we borrowed some code from nodb

359
00:17:28,860 --> 00:17:32,110
the code was a little bit broken

360
00:17:32,110 --> 00:17:34,420
you know DB didn't notice because they

361
00:17:34,420 --> 00:17:36,040
weren't using the feature we were using

362
00:17:36,040 --> 00:17:38,140
for a performance counter that was

363
00:17:38,140 --> 00:17:40,600
supposed to be shorted so that different

364
00:17:40,600 --> 00:17:43,840
threads would all not compete on the

365
00:17:43,840 --> 00:17:46,650
same memory location to do the updates

366
00:17:46,650 --> 00:17:49,120
the brokenness meant all threads were

367
00:17:49,120 --> 00:17:52,570
updating array element zero memory

368
00:17:52,570 --> 00:17:57,040
system stalls fixing that doubled range

369
00:17:57,040 --> 00:18:00,580
scan throughput group commit if you turn

370
00:18:00,580 --> 00:18:02,590
on bin log crash safety which is a good

371
00:18:02,590 --> 00:18:05,560
thing to have on we lose 5 to 10 to 20%

372
00:18:05,560 --> 00:18:07,710
of throughput depending on your workload

373
00:18:07,710 --> 00:18:11,410
we're this is in discussion so we have

374
00:18:11,410 --> 00:18:15,190
plans to fix it this year it's just

375
00:18:15,190 --> 00:18:18,070
there's no code available yet large

376
00:18:18,070 --> 00:18:19,920
transactions all

377
00:18:19,920 --> 00:18:22,120
uncommitted changes are buffered in

378
00:18:22,120 --> 00:18:24,880
memory by my wrongs if you try to insert

379
00:18:24,880 --> 00:18:29,260
one billion rows in one transaction by

380
00:18:29,260 --> 00:18:31,630
default you're going to need enough

381
00:18:31,630 --> 00:18:35,050
memory to really double buffer that

382
00:18:35,050 --> 00:18:39,910
transaction temporarily so we have this

383
00:18:39,910 --> 00:18:42,130
commit early option where internally we

384
00:18:42,130 --> 00:18:44,950
can commit before you call commit so for

385
00:18:44,950 --> 00:18:48,250
bulk load that that helps today we have

386
00:18:48,250 --> 00:18:49,690
a limit on the maximum number of

387
00:18:49,690 --> 00:18:51,820
modified rows per transaction we're

388
00:18:51,820 --> 00:18:53,830
switching that to a limit on the maximum

389
00:18:53,830 --> 00:18:56,800
amount of memory per transaction and

390
00:18:56,800 --> 00:18:58,330
then we have design discussions in

391
00:18:58,330 --> 00:19:01,540
progress to better to be better at large

392
00:19:01,540 --> 00:19:03,970
transactions and this is one of the

393
00:19:03,970 --> 00:19:06,250
challenges rocks DB is meant for small

394
00:19:06,250 --> 00:19:07,370
transactions

395
00:19:07,370 --> 00:19:09,140
you're building a generic database

396
00:19:09,140 --> 00:19:11,210
engine you need to support a variety of

397
00:19:11,210 --> 00:19:13,880
workloads and so this is one source of

398
00:19:13,880 --> 00:19:15,320
diversity we need to get better at

399
00:19:15,320 --> 00:19:22,670
handling so the wet when I report

400
00:19:22,670 --> 00:19:23,870
performance

401
00:19:23,870 --> 00:19:26,420
I don't just try I want to explain

402
00:19:26,420 --> 00:19:28,040
performance so the first number is

403
00:19:28,040 --> 00:19:31,460
throughput transactions per second my

404
00:19:31,460 --> 00:19:35,540
sequel 5.6 significantly better than

405
00:19:35,540 --> 00:19:38,630
nodb with and without compression with

406
00:19:38,630 --> 00:19:39,860
my rocks and I was a little bit

407
00:19:39,860 --> 00:19:42,140
surprised by that but it's to me it's

408
00:19:42,140 --> 00:19:45,530
not a big deal the second column is Iost

409
00:19:45,530 --> 00:19:49,640
at how many random upper had random

410
00:19:49,640 --> 00:19:52,160
beads from storage per transaction it's

411
00:19:52,160 --> 00:19:53,870
about the same they're all about doing

412
00:19:53,870 --> 00:19:56,600
one one read per transaction third

413
00:19:56,600 --> 00:19:57,710
columns a big deal

414
00:19:57,710 --> 00:19:59,450
how many kilobytes are written back to

415
00:19:59,450 --> 00:20:03,080
storage per transaction in ODB is 15 to

416
00:20:03,080 --> 00:20:05,330
20 times worse for reasons I previously

417
00:20:05,330 --> 00:20:07,580
described and I've seen this it's not

418
00:20:07,580 --> 00:20:09,410
always 15 to 20 it depends on the

419
00:20:09,410 --> 00:20:11,660
workload but in this case it was pretty

420
00:20:11,660 --> 00:20:16,880
bad CPU per transaction similar database

421
00:20:16,880 --> 00:20:19,400
size per transaction my rocks is about

422
00:20:19,400 --> 00:20:22,880
half the size of compressed in ODB and

423
00:20:22,880 --> 00:20:24,680
about 1/4 the size of uncompressed in

424
00:20:24,680 --> 00:20:28,400
ODB last column is just a one way of

425
00:20:28,400 --> 00:20:30,110
looking at quality of service what's the

426
00:20:30,110 --> 00:20:32,870
99th percentile response time for a

427
00:20:32,870 --> 00:20:35,930
frequent transaction my rocks was lower

428
00:20:35,930 --> 00:20:38,690
here is better my rocks one millisecond

429
00:20:38,690 --> 00:20:42,170
nodb was 6 milliseconds so when I try to

430
00:20:42,170 --> 00:20:44,090
explain performance I use this kind of

431
00:20:44,090 --> 00:20:47,120
graph or table so I show throughput

432
00:20:47,120 --> 00:20:49,580
quality of service harbour efficiency

433
00:20:49,580 --> 00:20:51,710
and in this case my rocks did a lot

434
00:20:51,710 --> 00:20:56,540
better last number and then I'm done

435
00:20:56,540 --> 00:20:58,520
the value of right efficiency two

436
00:20:58,520 --> 00:21:01,910
results showing why my rocks is doing

437
00:21:01,910 --> 00:21:05,750
better thanks to better write efficiency

438
00:21:05,750 --> 00:21:08,120
doing less on writes saves more for

439
00:21:08,120 --> 00:21:10,310
reads first result is the insert

440
00:21:10,310 --> 00:21:13,370
benchmark for in-memory database so

441
00:21:13,370 --> 00:21:17,180
there's no reads from storage nodb 5 7

442
00:21:17,180 --> 00:21:20,180
certainly is faster than 5 6 but the

443
00:21:20,180 --> 00:21:21,230
interesting results

444
00:21:21,230 --> 00:21:23,630
that when you go from fast SSD to slow

445
00:21:23,630 --> 00:21:26,660
SSD nodb loses about half its throughput

446
00:21:26,660 --> 00:21:29,540
my rocks loses much less than half so I

447
00:21:29,540 --> 00:21:31,250
was I was happy with that result

448
00:21:31,250 --> 00:21:33,290
prior to that I have something from link

449
00:21:33,290 --> 00:21:36,320
bench similar hardware but using going

450
00:21:36,320 --> 00:21:41,270
from fast SSD to slow SSD to disk the

451
00:21:41,270 --> 00:21:43,190
interesting result here is that nodb is

452
00:21:43,190 --> 00:21:45,020
losing much more throughput as the

453
00:21:45,020 --> 00:21:47,480
storage gets slower than my Roxas so

454
00:21:47,480 --> 00:21:49,760
again better ride efficiency allows

455
00:21:49,760 --> 00:21:52,549
better read read performance that's all

456
00:21:52,549 --> 00:21:54,790
I have

457
00:21:55,180 --> 00:22:02,749
[Applause]

