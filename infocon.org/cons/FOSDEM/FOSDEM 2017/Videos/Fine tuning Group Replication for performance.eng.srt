1
00:00:00,030 --> 00:00:02,480
so

2
00:00:04,620 --> 00:00:06,990
my name is Ruth Oliveira I've been

3
00:00:06,990 --> 00:00:10,200
working for with group application and

4
00:00:10,200 --> 00:00:12,090
optimizing replication and on

5
00:00:12,090 --> 00:00:15,299
performance basically for the last times

6
00:00:15,299 --> 00:00:18,779
so what I'll be doing today is

7
00:00:18,779 --> 00:00:23,400
presenting some ways that you can some

8
00:00:23,400 --> 00:00:24,869
options and some ways that you can

9
00:00:24,869 --> 00:00:26,279
optimize group application for

10
00:00:26,279 --> 00:00:30,509
performance so I'll skip this you have

11
00:00:30,509 --> 00:00:36,620
seen that and so I'll try to explain

12
00:00:36,620 --> 00:00:39,360
better than installing benchmarks and

13
00:00:39,360 --> 00:00:41,130
everything and we all know that

14
00:00:41,130 --> 00:00:43,620
benchmarks have a purpose if many

15
00:00:43,620 --> 00:00:46,740
purpose but actually for a production

16
00:00:46,740 --> 00:00:48,870
user they mean very little because they

17
00:00:48,870 --> 00:00:51,120
need to run it with their workload and

18
00:00:51,120 --> 00:00:54,570
there are just metrics that you can use

19
00:00:54,570 --> 00:00:56,220
to try to understand what's happening

20
00:00:56,220 --> 00:00:58,890
but the real benchmark is the workload

21
00:00:58,890 --> 00:01:01,410
that we need so I'll instead of that

22
00:01:01,410 --> 00:01:03,480
I'll try to explain how it works behind

23
00:01:03,480 --> 00:01:06,869
the scenes in some things that impact

24
00:01:06,869 --> 00:01:09,810
performance so that then each person can

25
00:01:09,810 --> 00:01:13,049
strike what the workload is is doing ok

26
00:01:13,049 --> 00:01:17,009
so let me start with this anatomy of

27
00:01:17,009 --> 00:01:19,439
group application so in group

28
00:01:19,439 --> 00:01:24,270
application we have basically two big

29
00:01:24,270 --> 00:01:26,520
parts so we have the transaction and

30
00:01:26,520 --> 00:01:28,619
after a transaction is prepared so it

31
00:01:28,619 --> 00:01:32,429
executes and it's ready to on a node and

32
00:01:32,429 --> 00:01:35,490
it's ready to be propagated we group

33
00:01:35,490 --> 00:01:37,560
application enters at this point where

34
00:01:37,560 --> 00:01:40,380
it's ready to commit and then it gathers

35
00:01:40,380 --> 00:01:43,679
the right set information then it does

36
00:01:43,679 --> 00:01:47,069
what I call is a throttling point so I'm

37
00:01:47,069 --> 00:01:48,929
going directly to this details because I

38
00:01:48,929 --> 00:01:51,719
think people here prefer this lower

39
00:01:51,719 --> 00:01:54,299
level thing okay so then we go to the

40
00:01:54,299 --> 00:01:57,749
details of delays if needed and then we

41
00:01:57,749 --> 00:01:59,969
send the message out to ordering on

42
00:01:59,969 --> 00:02:02,310
group application as offering was so on

43
00:02:02,310 --> 00:02:03,070
okay

44
00:02:03,070 --> 00:02:05,890
then at this point the thread just Waits

45
00:02:05,890 --> 00:02:08,169
it's a switch there for something to

46
00:02:08,169 --> 00:02:10,630
happen and that something is going is

47
00:02:10,630 --> 00:02:12,160
the transactions going to the network

48
00:02:12,160 --> 00:02:15,520
being ordered in a in agreement with all

49
00:02:15,520 --> 00:02:17,680
nodes and then getting back to the

50
00:02:17,680 --> 00:02:20,920
certification on that same note and this

51
00:02:20,920 --> 00:02:23,290
is what's happening here ok so then once

52
00:02:23,290 --> 00:02:25,630
the certification result is in then the

53
00:02:25,630 --> 00:02:28,300
threat can continue and at that point

54
00:02:28,300 --> 00:02:30,790
the certification is either positive or

55
00:02:30,790 --> 00:02:33,040
negative or so so it's there is either a

56
00:02:33,040 --> 00:02:35,200
conflict or there's no conflict and we

57
00:02:35,200 --> 00:02:37,209
can go on and then we decide to commit

58
00:02:37,209 --> 00:02:40,540
or not this is basically the the main

59
00:02:40,540 --> 00:02:44,980
entry point for group replication which

60
00:02:44,980 --> 00:02:47,320
also means that if someone traces the

61
00:02:47,320 --> 00:02:49,480
what's the application doing or

62
00:02:49,480 --> 00:02:51,910
something why is group in Oslo this is

63
00:02:51,910 --> 00:02:54,010
not where you'll be seeing it much okay

64
00:02:54,010 --> 00:02:56,290
because it will just send it to the

65
00:02:56,290 --> 00:02:57,670
network and then wait for something to

66
00:02:57,670 --> 00:03:00,570
happen outside

67
00:03:01,290 --> 00:03:05,040
the second part is this one where indeed

68
00:03:05,040 --> 00:03:09,930
we have a loop that handles the the data

69
00:03:09,930 --> 00:03:12,420
that gets in in from the network orders

70
00:03:12,420 --> 00:03:15,750
and at that point we have the we receive

71
00:03:15,750 --> 00:03:17,819
the transactions get them order and we

72
00:03:17,819 --> 00:03:20,010
get a transaction one by one

73
00:03:20,010 --> 00:03:23,670
and then decide to certify and then see

74
00:03:23,670 --> 00:03:26,790
according to the states in each node the

75
00:03:26,790 --> 00:03:29,430
rated executed in each node and the

76
00:03:29,430 --> 00:03:30,810
information that comes on the right set

77
00:03:30,810 --> 00:03:32,849
from the network we decided if they are

78
00:03:32,849 --> 00:03:34,950
they are compatible that we can proceed

79
00:03:34,950 --> 00:03:36,629
with that transaction or if you have to

80
00:03:36,629 --> 00:03:40,379
abort this is deterministic because

81
00:03:40,379 --> 00:03:42,120
everything all the nodes received a

82
00:03:42,120 --> 00:03:43,680
transaction in the same order so the

83
00:03:43,680 --> 00:03:45,599
decision is the same on all nodes this

84
00:03:45,599 --> 00:03:47,970
is what makes the multi master system

85
00:03:47,970 --> 00:03:51,659
work ok but there's a here a split which

86
00:03:51,659 --> 00:03:54,060
is if the transaction is local we don't

87
00:03:54,060 --> 00:03:56,040
have to do anything else then ok saying

88
00:03:56,040 --> 00:04:00,209
fine commits and and go on but if the

89
00:04:00,209 --> 00:04:02,940
transaction is not local we need in the

90
00:04:02,940 --> 00:04:04,530
certification it's positive we need to

91
00:04:04,530 --> 00:04:07,680
store the transaction to be applied we

92
00:04:07,680 --> 00:04:08,760
don't execute the transaction

93
00:04:08,760 --> 00:04:12,540
immediately we just reuse what's already

94
00:04:12,540 --> 00:04:14,549
available with a synchronous application

95
00:04:14,549 --> 00:04:16,500
which is a slave a player we put it in a

96
00:04:16,500 --> 00:04:18,750
relay log and then eventually it will

97
00:04:18,750 --> 00:04:24,060
apply and this is the two main areas of

98
00:04:24,060 --> 00:04:27,720
group application which means also that

99
00:04:27,720 --> 00:04:31,820
we have these factors affecting or

100
00:04:31,820 --> 00:04:35,270
our performance the most okay so of

101
00:04:35,270 --> 00:04:37,310
course the obvious ones network benefit

102
00:04:37,310 --> 00:04:42,110
latest slower networks will be will be

103
00:04:42,110 --> 00:04:44,330
harder to deal but particularly latency

104
00:04:44,330 --> 00:04:46,580
very high latency will make us drop

105
00:04:46,580 --> 00:04:49,130
performance but this happens to everyone

106
00:04:49,130 --> 00:04:51,170
so that's fine you just have to adapt to

107
00:04:51,170 --> 00:04:53,180
that and then the certification

108
00:04:53,180 --> 00:04:55,610
throughput also is an important factor

109
00:04:55,610 --> 00:04:58,940
because the the certification is a

110
00:04:58,940 --> 00:05:01,640
sequential process we do it in a single

111
00:05:01,640 --> 00:05:04,010
thread that decides what happens to the

112
00:05:04,010 --> 00:05:07,940
to the transaction it's not a light it

113
00:05:07,940 --> 00:05:10,640
can do many transactions but we have to

114
00:05:10,640 --> 00:05:12,680
be careful with that because it may take

115
00:05:12,680 --> 00:05:14,900
longer and then we start delaying the

116
00:05:14,900 --> 00:05:19,010
acknowledgments because of that and then

117
00:05:19,010 --> 00:05:21,890
of course the the remote applier once

118
00:05:21,890 --> 00:05:23,630
they get to the relay log they depend on

119
00:05:23,630 --> 00:05:25,970
the same thing that replication depends

120
00:05:25,970 --> 00:05:27,410
which is the through potential supplier

121
00:05:27,410 --> 00:05:29,030
and the number of threads on slave

122
00:05:29,030 --> 00:05:31,960
acquire that can actually perform the

123
00:05:31,960 --> 00:05:36,980
remote apply okay so let's see what this

124
00:05:36,980 --> 00:05:37,430
means

125
00:05:37,430 --> 00:05:40,250
this means that these points here are

126
00:05:40,250 --> 00:05:42,320
the main connection condition board but

127
00:05:42,320 --> 00:05:44,180
not that one is not that much it's a

128
00:05:44,180 --> 00:05:46,220
small point where we gather the right

129
00:05:46,220 --> 00:05:48,200
set information it takes a bit we hash

130
00:05:48,200 --> 00:05:51,200
it but it's very light not something

131
00:05:51,200 --> 00:05:54,200
very significant but then we have this

132
00:05:54,200 --> 00:05:56,780
process we have to send the message out

133
00:05:56,780 --> 00:05:59,720
we have to reach the agreement and then

134
00:05:59,720 --> 00:06:02,720
wait on a certification on that on that

135
00:06:02,720 --> 00:06:05,330
side okay

136
00:06:05,330 --> 00:06:09,930
so this all is the main contention

137
00:06:09,930 --> 00:06:12,150
points in the certifier we have to make

138
00:06:12,150 --> 00:06:14,810
sure that the certification itself is

139
00:06:14,810 --> 00:06:16,890
able to keep up with the pace we have

140
00:06:16,890 --> 00:06:20,100
and we have to make sure that the

141
00:06:20,100 --> 00:06:23,220
transactions get really log at the rate

142
00:06:23,220 --> 00:06:25,320
that is enough to not delay the system

143
00:06:25,320 --> 00:06:28,080
this is can also be an issue and again

144
00:06:28,080 --> 00:06:30,830
the apply

145
00:06:32,759 --> 00:06:36,719
so we have a few options have a few

146
00:06:36,719 --> 00:06:40,710
options here and that allow us to

147
00:06:40,710 --> 00:06:45,779
control this these things okay

148
00:06:45,779 --> 00:06:48,449
some are not very controllable but for

149
00:06:48,449 --> 00:06:50,999
instance if we have high latency of

150
00:06:50,999 --> 00:06:53,009
course we can put more transactions and

151
00:06:53,009 --> 00:06:55,289
we need to extend more be able to put

152
00:06:55,289 --> 00:06:56,669
monstrous acts on this system and then

153
00:06:56,669 --> 00:06:59,789
they eventually arrived and we hide the

154
00:06:59,789 --> 00:07:02,430
late we we keep a high throughput at the

155
00:07:02,430 --> 00:07:03,659
cost of increasing to latency on

156
00:07:03,659 --> 00:07:06,029
transactions but at the point where we

157
00:07:06,029 --> 00:07:09,960
intercept the the transaction and we

158
00:07:09,960 --> 00:07:11,550
have the transaction prepared all locks

159
00:07:11,550 --> 00:07:13,710
are there but then it's very lightweight

160
00:07:13,710 --> 00:07:17,159
so increasing this latency you don't

161
00:07:17,159 --> 00:07:19,620
contend between threads a lot so it's

162
00:07:19,620 --> 00:07:21,319
fine if you increase the transactions

163
00:07:21,319 --> 00:07:23,339
parallel transaction it will behave

164
00:07:23,339 --> 00:07:27,870
rather well okay but then if we can do

165
00:07:27,870 --> 00:07:30,389
something which one one is okay let's

166
00:07:30,389 --> 00:07:33,120
compress the bandwidth we have a slower

167
00:07:33,120 --> 00:07:35,729
link let's compress it and just send it

168
00:07:35,729 --> 00:07:37,680
and take advantage of the fact that

169
00:07:37,680 --> 00:07:39,689
compression compared to the network can

170
00:07:39,689 --> 00:07:44,249
be at higher rates of compression so we

171
00:07:44,249 --> 00:07:46,860
can mostly use the CPU to compensate the

172
00:07:46,860 --> 00:07:49,979
network limitations we can also reduce

173
00:07:49,979 --> 00:07:52,169
of course the bin lock itself and use

174
00:07:52,169 --> 00:07:55,469
the minimum so that the rows that we

175
00:07:55,469 --> 00:08:00,749
sent our are smaller and then and then

176
00:08:00,749 --> 00:08:04,229
this is this one which I should maybe I

177
00:08:04,229 --> 00:08:05,309
shouldn't put this here okay because

178
00:08:05,309 --> 00:08:08,249
this this is something for very low

179
00:08:08,249 --> 00:08:10,229
level if you want to turn in some

180
00:08:10,229 --> 00:08:13,409
situations what happens is that and we

181
00:08:13,409 --> 00:08:14,969
found this to be effective in some

182
00:08:14,969 --> 00:08:16,919
situations so it's here because it may

183
00:08:16,919 --> 00:08:19,139
be useful but this is very particular

184
00:08:19,139 --> 00:08:23,520
which is if you have bursty purse these

185
00:08:23,520 --> 00:08:25,770
situations where you have sent something

186
00:08:25,770 --> 00:08:27,059
then you wait then you send and soon

187
00:08:27,059 --> 00:08:29,189
wait sometimes the thread that's

188
00:08:29,189 --> 00:08:30,990
receiving the threads will go to sleep

189
00:08:30,990 --> 00:08:33,240
scheduling out this right that makes the

190
00:08:33,240 --> 00:08:35,969
reception and delays the entering of

191
00:08:35,969 --> 00:08:37,139
same thread again

192
00:08:37,139 --> 00:08:39,779
so if we can avoid this for just a bit

193
00:08:39,779 --> 00:08:43,140
and sometimes we see a big throw because

194
00:08:43,140 --> 00:08:45,540
it never sleeps and this depends on

195
00:08:45,540 --> 00:08:48,870
so fine don't don't worry too much about

196
00:08:48,870 --> 00:08:50,730
this but it might be handy at some

197
00:08:50,730 --> 00:09:00,690
situations okay so um the certifier

198
00:09:00,690 --> 00:09:03,120
throughput

199
00:09:03,120 --> 00:09:06,029
yes we have to certify and I said we

200
00:09:06,029 --> 00:09:08,690
have to write it to the relay log and

201
00:09:08,690 --> 00:09:12,330
this is then sequential we certify and

202
00:09:12,330 --> 00:09:15,540
put the transactions there actually we

203
00:09:15,540 --> 00:09:16,920
try the solution where we did this in

204
00:09:16,920 --> 00:09:18,540
parallel but the benefit is not very

205
00:09:18,540 --> 00:09:20,070
good because in the end you'll be

206
00:09:20,070 --> 00:09:22,710
limited by the throughput of the writing

207
00:09:22,710 --> 00:09:25,770
to disk okay so in that either you are

208
00:09:25,770 --> 00:09:27,600
able to enter that throughput or you

209
00:09:27,600 --> 00:09:29,070
improve just a bit but then you will

210
00:09:29,070 --> 00:09:31,440
still be limited by that throughput on

211
00:09:31,440 --> 00:09:36,630
countries so take care of where you put

212
00:09:36,630 --> 00:09:39,000
your relay logs and if there are indeed

213
00:09:39,000 --> 00:09:40,920
the system is indeed capable of handling

214
00:09:40,920 --> 00:09:45,240
the the throughput you want otherwise if

215
00:09:45,240 --> 00:09:47,550
you may have noticed that certificate is

216
00:09:47,550 --> 00:09:49,350
the latest certification compared to

217
00:09:49,350 --> 00:09:53,400
other nodes and that that can be quite a

218
00:09:53,400 --> 00:09:56,310
delay if you don't take care there's

219
00:09:56,310 --> 00:09:58,260
also another issue that may happen when

220
00:09:58,260 --> 00:10:00,570
have multi master nodes multi will be

221
00:10:00,570 --> 00:10:04,700
right with multiple masters which is

222
00:10:05,779 --> 00:10:08,810
if we try to send the same kids use

223
00:10:08,810 --> 00:10:12,410
the same the to use digits sequentially

224
00:10:12,410 --> 00:10:15,170
in two different nodes we try to get

225
00:10:15,170 --> 00:10:16,519
four more not an answer to a terminal

226
00:10:16,519 --> 00:10:19,699
what this makes is the JT executed sets

227
00:10:19,699 --> 00:10:22,939
will be very large because they they are

228
00:10:22,939 --> 00:10:25,790
not able to compact the digit is sets it

229
00:10:25,790 --> 00:10:27,620
compact very well if they're continuous

230
00:10:27,620 --> 00:10:30,740
in fact two nodes and they are trying to

231
00:10:30,740 --> 00:10:32,660
send the same thing they will be

232
00:10:32,660 --> 00:10:35,269
building larger treated sets and this is

233
00:10:35,269 --> 00:10:36,860
important for the certification process

234
00:10:36,860 --> 00:10:41,269
okay so if you lose multi master please

235
00:10:41,269 --> 00:10:43,220
do not put this to one otherwise the

236
00:10:43,220 --> 00:10:44,980
performance will really drop as the

237
00:10:44,980 --> 00:10:47,089
certification itself and certification

238
00:10:47,089 --> 00:10:48,949
info will grow more than you need

239
00:10:48,949 --> 00:10:52,100
actually the the consequence of this

240
00:10:52,100 --> 00:10:55,399
which is not the problem I think but it

241
00:10:55,399 --> 00:10:57,649
may work in some situations is that the

242
00:10:57,649 --> 00:11:00,529
GPS will not be contiguous so you have

243
00:11:00,529 --> 00:11:02,000
the jitters from one node it will have

244
00:11:02,000 --> 00:11:03,980
an interval that it is from an element

245
00:11:03,980 --> 00:11:05,930
will have another interval then when

246
00:11:05,930 --> 00:11:07,939
that those intervals are exhausted

247
00:11:07,939 --> 00:11:10,370
they'll get another interval so you have

248
00:11:10,370 --> 00:11:13,220
G leads in blocks when you write the

249
00:11:13,220 --> 00:11:15,620
multi-master okay but this is something

250
00:11:15,620 --> 00:11:18,319
that by default it's already 1 million I

251
00:11:18,319 --> 00:11:21,079
think so but please don't put this to

252
00:11:21,079 --> 00:11:26,059
work and then we have the applier

253
00:11:26,059 --> 00:11:28,010
acquire sighs okay

254
00:11:28,010 --> 00:11:30,920
and the the player side of course we

255
00:11:30,920 --> 00:11:33,140
have to if want to use the parallel

256
00:11:33,140 --> 00:11:36,040
applier we have to use the logical clock

257
00:11:36,040 --> 00:11:40,700
scheme in schedule and we have to have

258
00:11:40,700 --> 00:11:42,500
enough threads to handle the workload

259
00:11:42,500 --> 00:11:45,220
fine it will depend on the system where

260
00:11:45,220 --> 00:11:50,150
where your anything.this and if possible

261
00:11:50,150 --> 00:11:53,900
use more than less but after a point it

262
00:11:53,900 --> 00:11:55,250
doesn't just doesn't pay to have more

263
00:11:55,250 --> 00:11:57,770
threads and that point may be between 8

264
00:11:57,770 --> 00:12:00,110
and 16 and depending if it's really

265
00:12:00,110 --> 00:12:05,450
intensive 24 32 but then we start having

266
00:12:05,450 --> 00:12:07,430
condition under which distribution also

267
00:12:07,430 --> 00:12:12,260
so one good thing about your application

268
00:12:12,260 --> 00:12:16,190
in the wire is that we take advantage of

269
00:12:16,190 --> 00:12:17,870
the information that suits for

270
00:12:17,870 --> 00:12:21,140
certification to improve the parallelism

271
00:12:21,140 --> 00:12:26,210
on the slave what this means is that we

272
00:12:26,210 --> 00:12:28,700
are already decided which transactions

273
00:12:28,700 --> 00:12:30,470
are compatible not between notes we know

274
00:12:30,470 --> 00:12:31,880
that from the certification so we also

275
00:12:31,880 --> 00:12:35,090
know which rows in the transaction are

276
00:12:35,090 --> 00:12:37,550
comparable between themselves so we use

277
00:12:37,550 --> 00:12:38,930
in group application use that

278
00:12:38,930 --> 00:12:41,120
information to schedule the transactions

279
00:12:41,120 --> 00:12:44,210
on this level okay and what test this

280
00:12:44,210 --> 00:12:47,410
means is that

281
00:12:47,410 --> 00:12:51,010
we can ramp up much faster on the slave

282
00:12:51,010 --> 00:12:52,990
a player using the right set information

283
00:12:52,990 --> 00:12:57,190
then using the final binary group commit

284
00:12:57,190 --> 00:13:01,060
okay a signature application which uses

285
00:13:01,060 --> 00:13:03,550
is that it mostly groups the transaction

286
00:13:03,550 --> 00:13:04,900
that are in the same group commit it

287
00:13:04,900 --> 00:13:06,700
says okay if they all commit it together

288
00:13:06,700 --> 00:13:08,170
it's because they are repelled on the

289
00:13:08,170 --> 00:13:09,700
server and we use that information to

290
00:13:09,700 --> 00:13:12,880
run it in parallel slaves but if we have

291
00:13:12,880 --> 00:13:16,300
a very fast storage very very fast

292
00:13:16,300 --> 00:13:20,410
search and only a few threads the group

293
00:13:20,410 --> 00:13:22,240
commit without having some delays

294
00:13:22,240 --> 00:13:24,550
inserted will be very small

295
00:13:24,550 --> 00:13:25,930
we're only a few transactions will

296
00:13:25,930 --> 00:13:28,330
commit together and in that case we'll

297
00:13:28,330 --> 00:13:30,640
have little parallelism on the other

298
00:13:30,640 --> 00:13:33,550
side on the slave a player if we use

299
00:13:33,550 --> 00:13:35,410
right side right side information here

300
00:13:35,410 --> 00:13:37,750
from loop application we can take

301
00:13:37,750 --> 00:13:40,750
advantage of the this fact that we don't

302
00:13:40,750 --> 00:13:42,730
need the group they need to decide what

303
00:13:42,730 --> 00:13:44,200
needs to be parallel or not

304
00:13:44,200 --> 00:13:48,130
so even with only a few threads we can

305
00:13:48,130 --> 00:13:50,560
get already a lot of the throughput of

306
00:13:50,560 --> 00:13:52,810
the slave a player okay this is one of

307
00:13:52,810 --> 00:13:54,220
the benefits and it's something that

308
00:13:54,220 --> 00:13:57,040
probably reduces lag in situation where

309
00:13:57,040 --> 00:13:58,510
we have lag right now with a single

310
00:13:58,510 --> 00:14:01,140
segregation

311
00:14:01,310 --> 00:14:06,998
so what another thing that's

312
00:14:07,660 --> 00:14:11,050
that we decided that of course the lair

313
00:14:11,050 --> 00:14:14,380
also has it but here it's important for

314
00:14:14,380 --> 00:14:18,280
us to keep the notes mostly and I mean

315
00:14:18,280 --> 00:14:22,320
no sleep close but not exactly in sync

316
00:14:22,320 --> 00:14:27,640
because at some point we need if you let

317
00:14:27,640 --> 00:14:29,740
the master go at full speed in some

318
00:14:29,740 --> 00:14:31,450
situations there's no way the slave can

319
00:14:31,450 --> 00:14:34,120
can keep up and in if we have a group

320
00:14:34,120 --> 00:14:36,940
where most where more than when members

321
00:14:36,940 --> 00:14:38,740
try to write it's very difficult for

322
00:14:38,740 --> 00:14:40,840
them to write effectively if they are

323
00:14:40,840 --> 00:14:44,440
not close but for us so we are to

324
00:14:44,440 --> 00:14:46,350
introduce flow control for this also but

325
00:14:46,350 --> 00:14:48,550
there's also some situations where we

326
00:14:48,550 --> 00:14:50,050
want to be able to manage the cluster

327
00:14:50,050 --> 00:14:53,080
without like for instance one of the

328
00:14:53,080 --> 00:14:54,760
important ones is to add a new member to

329
00:14:54,760 --> 00:14:58,510
a cluster that's writing fast so we have

330
00:14:58,510 --> 00:15:00,760
a lot of right to work well there and we

331
00:15:00,760 --> 00:15:03,940
want to be able to get in question but

332
00:15:03,940 --> 00:15:05,320
the work of a note that gets in the

333
00:15:05,320 --> 00:15:07,900
cluster is much larger than the note

334
00:15:07,900 --> 00:15:09,370
that's just running because it because

335
00:15:09,370 --> 00:15:11,680
it has to store what's coming in from

336
00:15:11,680 --> 00:15:13,840
the nether in also applying what's on

337
00:15:13,840 --> 00:15:19,570
the on the queue already so for this we

338
00:15:19,570 --> 00:15:21,460
introduce this this flow control which

339
00:15:21,460 --> 00:15:25,300
is a one thing that's works a bit

340
00:15:25,300 --> 00:15:29,950
differently okay I mean I'll skip the

341
00:15:29,950 --> 00:15:31,810
performant things

342
00:15:31,810 --> 00:15:37,060
graphs okay so the on this side we

343
00:15:37,060 --> 00:15:39,100
wanted to say okay it's the writer that

344
00:15:39,100 --> 00:15:42,279
decides we instead of doing like a lot

345
00:15:42,279 --> 00:15:44,350
of the cinemas or something like that to

346
00:15:44,350 --> 00:15:45,910
the survey simulated or something like

347
00:15:45,910 --> 00:15:46,120
that

348
00:15:46,120 --> 00:15:50,830
no we just sent its node sense to the to

349
00:15:50,830 --> 00:15:55,480
the server one message per second saying

350
00:15:55,480 --> 00:15:58,420
okay this is my cue this is my applier

351
00:15:58,420 --> 00:16:00,460
cue this is my certifier cue this is the

352
00:16:00,460 --> 00:16:02,920
number of transactions I try to execute

353
00:16:02,920 --> 00:16:06,089
in the last period this is the number of

354
00:16:06,089 --> 00:16:08,050
transactions I have successfully applied

355
00:16:08,050 --> 00:16:11,740
and so on and then everyone that is

356
00:16:11,740 --> 00:16:13,690
listening on network knows the state of

357
00:16:13,690 --> 00:16:16,540
all members on the web network and with

358
00:16:16,540 --> 00:16:19,060
that whenever a member wants to write it

359
00:16:19,060 --> 00:16:21,160
cannot write more than the state of that

360
00:16:21,160 --> 00:16:23,589
the system can withstand so if we notice

361
00:16:23,589 --> 00:16:25,630
that the slowest member on the last

362
00:16:25,630 --> 00:16:29,020
period that has a grow as a cue growing

363
00:16:29,020 --> 00:16:32,260
beyond a threshold that we set if that Q

364
00:16:32,260 --> 00:16:34,270
is larger then we decided it was the

365
00:16:34,270 --> 00:16:36,580
maximum then that note is away then we

366
00:16:36,580 --> 00:16:39,280
check how much it was able to run and

367
00:16:39,280 --> 00:16:42,970
then with that we simply okay so it was

368
00:16:42,970 --> 00:16:44,380
able to it send 1,000

369
00:16:44,380 --> 00:16:49,720
let's take 10% for allowing it to get

370
00:16:49,720 --> 00:16:52,600
the the all transactions and also try to

371
00:16:52,600 --> 00:16:55,570
keep it around this 1,000 and then next

372
00:16:55,570 --> 00:16:57,970
second we do exactly the same we try to

373
00:16:57,970 --> 00:17:00,130
always see if the states change and then

374
00:17:00,130 --> 00:17:02,080
we ramp up if everything is cleared and

375
00:17:02,080 --> 00:17:04,270
now we're very fast then we ramp up and

376
00:17:04,270 --> 00:17:06,939
then again you're always do this one per

377
00:17:06,939 --> 00:17:10,329
second so it's a whenever a writer wants

378
00:17:10,329 --> 00:17:12,520
to write it will check the quota that's

379
00:17:12,520 --> 00:17:14,859
available for him as a writer and that

380
00:17:14,859 --> 00:17:16,929
that's it so you won't see any flow

381
00:17:16,929 --> 00:17:19,390
control messages other than this status

382
00:17:19,390 --> 00:17:21,359
message growing around

383
00:17:21,359 --> 00:17:26,020
okay but okay flow control also

384
00:17:26,020 --> 00:17:28,240
introduces some changes but actually the

385
00:17:28,240 --> 00:17:31,210
flow disabling flow control there's

386
00:17:31,210 --> 00:17:33,130
actually in terms of throughput does not

387
00:17:33,130 --> 00:17:35,919
decrease much it decreases much if you

388
00:17:35,919 --> 00:17:40,360
put slow low threshold if you put very

389
00:17:40,360 --> 00:17:42,520
low threshold this is the is designed to

390
00:17:42,520 --> 00:17:45,840
hand to have threshold that's

391
00:17:45,840 --> 00:17:47,770
significantly larger than the number of

392
00:17:47,770 --> 00:17:49,480
transactions you can run in a second for

393
00:17:49,480 --> 00:17:51,910
instance so it should be larger than if

394
00:17:51,910 --> 00:17:53,890
you handle ten thousand transactions it

395
00:17:53,890 --> 00:17:55,360
should be larger than that otherwise it

396
00:17:55,360 --> 00:18:05,039
will be doing more than anything okay so

397
00:18:05,039 --> 00:18:11,260
that's it when it finished okay so as I

398
00:18:11,260 --> 00:18:13,950
said before this is just an artificial

399
00:18:13,950 --> 00:18:17,380
benchmark not to demean suspense which I

400
00:18:17,380 --> 00:18:20,440
like a lot it was great for us in

401
00:18:20,440 --> 00:18:22,600
development so but let me just show what

402
00:18:22,600 --> 00:18:25,330
you can expect in this configuration and

403
00:18:25,330 --> 00:18:27,520
with this compared to a synchronous

404
00:18:27,520 --> 00:18:30,400
replication okay of course you have a

405
00:18:30,400 --> 00:18:31,809
loss compared to a synchronous or

406
00:18:31,809 --> 00:18:33,580
clarification we start where a

407
00:18:33,580 --> 00:18:35,320
synchronous replication would just

408
00:18:35,320 --> 00:18:38,049
commit and we start our work there and

409
00:18:38,049 --> 00:18:41,289
of course we have higher latency those

410
00:18:41,289 --> 00:18:41,830
are good odds

411
00:18:41,830 --> 00:18:45,010
but in triangles there and but the thing

412
00:18:45,010 --> 00:18:48,220
is well it's we believe it this is

413
00:18:48,220 --> 00:18:50,590
reasonable we are of course trying to

414
00:18:50,590 --> 00:18:53,260
improve on this and this is we have

415
00:18:53,260 --> 00:18:56,730
overhead but this is something we're

416
00:18:56,730 --> 00:18:59,200
it's great for us that we reach already

417
00:18:59,200 --> 00:19:00,760
this point Matt we want to make it

418
00:19:00,760 --> 00:19:03,520
closer and of course this is only forces

419
00:19:03,520 --> 00:19:05,440
bench let's see how it behaves on all

420
00:19:05,440 --> 00:19:09,309
the real users and workloads okay so but

421
00:19:09,309 --> 00:19:11,470
at least is good for us to be able to

422
00:19:11,470 --> 00:19:13,770
reach this point

423
00:19:13,770 --> 00:19:16,480
there's also the issue of multi-master

424
00:19:16,480 --> 00:19:19,360
so of course we do not recommend it

425
00:19:19,360 --> 00:19:23,529
immediately but you can and using it

426
00:19:23,529 --> 00:19:26,500
there's some news about the scaling with

427
00:19:26,500 --> 00:19:28,809
the number of writers so there's a bit

428
00:19:28,809 --> 00:19:32,169
of scaling here but there's there's no

429
00:19:32,169 --> 00:19:34,120
big scaling from using multiple writers

430
00:19:34,120 --> 00:19:35,679
when you have a system that's fast

431
00:19:35,679 --> 00:19:37,539
enough of course if you have many reads

432
00:19:37,539 --> 00:19:41,830
were only a few rights and you yes of

433
00:19:41,830 --> 00:19:43,690
course you are exploring the read

434
00:19:43,690 --> 00:19:45,279
capacity need to note and the right in

435
00:19:45,279 --> 00:19:46,809
the then the read capacity will be

436
00:19:46,809 --> 00:19:50,950
enough to have some gain but what that's

437
00:19:50,950 --> 00:19:53,440
it it's something you need to study if

438
00:19:53,440 --> 00:19:55,870
it's benefits or you or not or if you

439
00:19:55,870 --> 00:19:58,149
can use it and then we also see the

440
00:19:58,149 --> 00:20:00,970
growth from three to nine members which

441
00:20:00,970 --> 00:20:04,270
is something which is also good there's

442
00:20:04,270 --> 00:20:06,429
some effect there on set five and seven

443
00:20:06,429 --> 00:20:09,580
members but that's that's okay so

444
00:20:09,580 --> 00:20:12,039
there's no big drop and there's no big

445
00:20:12,039 --> 00:20:14,220
drop because of what Ronnie said the

446
00:20:14,220 --> 00:20:16,840
decks come later then GCS layer that's

447
00:20:16,840 --> 00:20:19,390
that's booked below as capacity to

448
00:20:19,390 --> 00:20:21,940
handle this so even growing to nine

449
00:20:21,940 --> 00:20:24,419
members we still are able to handle this

450
00:20:24,419 --> 00:20:31,870
well and this is okay

451
00:20:31,870 --> 00:20:34,350
and then there's also another thing that

452
00:20:34,350 --> 00:20:37,360
Kenny was saying that you should not use

453
00:20:37,360 --> 00:20:41,020
whens no please with just what we wanted

454
00:20:41,020 --> 00:20:43,929
to say was that it's not as optimized

455
00:20:43,929 --> 00:20:47,380
for once as we can do and we were

456
00:20:47,380 --> 00:20:49,990
working on that but you can use it in

457
00:20:49,990 --> 00:20:52,690
once perfectly that's no issue there but

458
00:20:52,690 --> 00:20:55,330
of course what you are doing if you put

459
00:20:55,330 --> 00:20:59,140
this one node 8 7 with some delay in a3

460
00:20:59,140 --> 00:21:01,120
group of course you'll delay but then

461
00:21:01,120 --> 00:21:05,919
you compensate that by moving the line

462
00:21:05,919 --> 00:21:08,799
to the right so you will reach a

463
00:21:08,799 --> 00:21:10,779
throughput that's higher but with more

464
00:21:10,779 --> 00:21:14,409
threads and the same with the 250

465
00:21:14,409 --> 00:21:16,480
milliseconds and that also grows a bit

466
00:21:16,480 --> 00:21:20,669
more this way so

467
00:21:21,169 --> 00:21:25,139
and this is also over time the

468
00:21:25,139 --> 00:21:27,389
throughput but this is and this is not

469
00:21:27,389 --> 00:21:29,849
sustained throughput as before this is

470
00:21:29,849 --> 00:21:31,409
peak throughput this is the resultant

471
00:21:31,409 --> 00:21:34,349
suspension and we see there are small

472
00:21:34,349 --> 00:21:37,169
dips there it's Renee already complained

473
00:21:37,169 --> 00:21:39,689
about and put the work there this is the

474
00:21:39,689 --> 00:21:42,119
the garbage collection that's sometimes

475
00:21:42,119 --> 00:21:44,219
enters and lowers slightly the

476
00:21:44,219 --> 00:21:47,699
throughput there okay also improve on

477
00:21:47,699 --> 00:21:50,549
that well and that's it

478
00:21:50,549 --> 00:21:57,839
okay so right now we are waiting so also

479
00:21:57,839 --> 00:22:00,509
for some feedback and trying to

480
00:22:00,509 --> 00:22:04,549
understand if all the workloads

481
00:22:19,900 --> 00:22:24,430
okay so the thing is and I can explain

482
00:22:24,430 --> 00:22:27,880
as the the pipeline that you use on the

483
00:22:27,880 --> 00:22:31,690
communication system as as a number of

484
00:22:31,690 --> 00:22:33,309
slots and those slots are configured so

485
00:22:33,309 --> 00:22:35,440
having a very large latency will

486
00:22:35,440 --> 00:22:37,000
decrease your throughput it will work

487
00:22:37,000 --> 00:22:39,160
perfectly but then you have to know if

488
00:22:39,160 --> 00:22:41,640
you can put enough threads in parallel

489
00:22:41,640 --> 00:22:44,140
so if you couldn't withstand all of that

490
00:22:44,140 --> 00:22:48,280
like this there so fine but you need to

491
00:22:48,280 --> 00:22:49,660
know if your application can really

492
00:22:49,660 --> 00:22:54,690
withstand such a large number of threads

493
00:23:02,100 --> 00:23:06,080
no the transaction one transaction sorry

494
00:23:06,080 --> 00:23:11,130
in terms of execution where now if you

495
00:23:11,130 --> 00:23:13,620
put enough if you use more transactions

496
00:23:13,620 --> 00:23:15,450
you have more parallelism honestly on

497
00:23:15,450 --> 00:23:16,140
the server

498
00:23:16,140 --> 00:23:21,980
yes sir yeah no no no no

499
00:23:22,430 --> 00:23:26,860
okay any more questions Ellen

500
00:23:28,430 --> 00:23:33,290
okay so you're you're saying that we are

501
00:23:33,290 --> 00:23:36,770
using paksas I mean she's right so if I

502
00:23:36,770 --> 00:23:37,630
recall correctly

503
00:23:37,630 --> 00:23:42,080
paxos wait for majority right so in your

504
00:23:42,080 --> 00:23:44,240
graph when there was one note of three

505
00:23:44,240 --> 00:23:47,450
that was it's just latency was it

506
00:23:47,450 --> 00:23:49,220
because you were writing to it or it is

507
00:23:49,220 --> 00:23:51,380
because even when little you write on

508
00:23:51,380 --> 00:23:53,300
only one note where the majority is fast

509
00:23:53,300 --> 00:23:58,630
enough it slows down or not so and

510
00:23:58,630 --> 00:24:01,760
that's the things were actually working

511
00:24:01,760 --> 00:24:08,870
on but the the main issue is that we

512
00:24:08,870 --> 00:24:10,850
have to write to the network is so for

513
00:24:10,850 --> 00:24:12,800
one transaction that's what happens is

514
00:24:12,800 --> 00:24:14,600
we reach majority if we have two notes

515
00:24:14,600 --> 00:24:16,220
locally and one remote for one

516
00:24:16,220 --> 00:24:19,310
transaction we execute immediately

517
00:24:19,310 --> 00:24:21,710
there's no more delay there but the

518
00:24:21,710 --> 00:24:23,540
thing is this works for one transaction

519
00:24:23,540 --> 00:24:26,780
when you have many then at some point

520
00:24:26,780 --> 00:24:32,060
which is the Verizon which is then it

521
00:24:32,060 --> 00:24:34,340
will start delaying it will also start

522
00:24:34,340 --> 00:24:34,730
delaying

523
00:24:34,730 --> 00:24:36,440
if we immediate is send large

524
00:24:36,440 --> 00:24:38,600
transactions so if you send small

525
00:24:38,600 --> 00:24:40,580
transactions and you send only a few

526
00:24:40,580 --> 00:24:42,440
threads it should be able to have very

527
00:24:42,440 --> 00:24:44,930
low latency but then when you have more

528
00:24:44,930 --> 00:24:47,270
than it will have that added latency

529
00:24:47,270 --> 00:24:51,560
okay so yes it should be inspectors in

530
00:24:51,560 --> 00:24:53,600
because we is that we should be able to

531
00:24:53,600 --> 00:24:56,030
immediately return because we have two

532
00:24:56,030 --> 00:24:58,280
notes that are low latency but right now

533
00:24:58,280 --> 00:25:01,400
we can do that only as a one case

534
00:25:01,400 --> 00:25:03,530
instead of in in all situations that's

535
00:25:03,530 --> 00:25:06,580
what we need to fix

536
00:25:06,590 --> 00:25:08,540
okay

537
00:25:08,540 --> 00:25:14,720
[Applause]

