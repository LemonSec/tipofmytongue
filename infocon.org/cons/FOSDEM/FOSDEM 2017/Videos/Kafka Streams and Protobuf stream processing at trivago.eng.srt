1
00:00:04,720 --> 00:00:06,880
so a very quick introduction into a

2
00:00:06,880 --> 00:00:09,670
Kafka then some concepts about Kafka

3
00:00:09,670 --> 00:00:12,190
streams introduction on Google protocol

4
00:00:12,190 --> 00:00:15,129
buffers and what kind of stream

5
00:00:15,129 --> 00:00:18,390
processing you can do with that

6
00:00:19,740 --> 00:00:22,900
right-click over your Apache Kafka is a

7
00:00:22,900 --> 00:00:24,880
message queue system nowadays it's

8
00:00:24,880 --> 00:00:26,890
evolved into a distributed streaming

9
00:00:26,890 --> 00:00:30,250
platform so you can put messages since I

10
00:00:30,250 --> 00:00:33,070
refuse their queues are called topic in

11
00:00:33,070 --> 00:00:37,570
the sorry yeah all right so cough kites

12
00:00:37,570 --> 00:00:39,670
are distributed streaming platform you

13
00:00:39,670 --> 00:00:42,699
have message queues several you which

14
00:00:42,699 --> 00:00:44,350
are called topics each topic is

15
00:00:44,350 --> 00:00:46,960
partitioned and replicated so it allows

16
00:00:46,960 --> 00:00:49,629
you to scale out by putting more servers

17
00:00:49,629 --> 00:00:51,069
in your cluster and putting more

18
00:00:51,069 --> 00:00:53,920
partitions in your topic and each topic

19
00:00:53,920 --> 00:00:56,499
is each partition is usually replicated

20
00:00:56,499 --> 00:00:59,100
so if a machine goes down you have the

21
00:00:59,100 --> 00:01:01,539
data also somewhere else on a different

22
00:01:01,539 --> 00:01:06,250
machine and one important important part

23
00:01:06,250 --> 00:01:08,080
especially when it comes to Kafka

24
00:01:08,080 --> 00:01:09,400
streams is all of the consumer group

25
00:01:09,400 --> 00:01:13,840
logic so you can have several consumers

26
00:01:13,840 --> 00:01:16,330
forming a consumer group and reading

27
00:01:16,330 --> 00:01:19,630
from a particular message queue and this

28
00:01:19,630 --> 00:01:22,390
allows you also to scale the reading

29
00:01:22,390 --> 00:01:25,270
process and it works also quite similar

30
00:01:25,270 --> 00:01:28,230
so in this case we have two different

31
00:01:28,230 --> 00:01:31,270
consumer groups which are doing

32
00:01:31,270 --> 00:01:33,910
different things each but within each

33
00:01:33,910 --> 00:01:36,870
group you have several threats and

34
00:01:36,870 --> 00:01:39,040
basically they coordinate with each

35
00:01:39,040 --> 00:01:40,960
other to distribute the different

36
00:01:40,960 --> 00:01:43,690
partitions among each other so if one of

37
00:01:43,690 --> 00:01:45,190
your consumers and that group goes down

38
00:01:45,190 --> 00:01:48,310
the remaining the take over the work

39
00:01:48,310 --> 00:01:51,340
that that that consumer was doing and if

40
00:01:51,340 --> 00:01:53,500
you add more consumers than again the

41
00:01:53,500 --> 00:01:55,480
consumers will coordinate with each

42
00:01:55,480 --> 00:01:57,580
other to distribute the workload again

43
00:01:57,580 --> 00:02:00,420
which allows you also to easily scale

44
00:02:00,420 --> 00:02:03,940
the reading for the consuming from your

45
00:02:03,940 --> 00:02:09,550
cough adjuster all right so what is

46
00:02:09,550 --> 00:02:11,739
Kafka streams so they have been a lot of

47
00:02:11,739 --> 00:02:13,690
stream processing frameworks out there

48
00:02:13,690 --> 00:02:16,240
and sometimes you might wonder why is

49
00:02:16,240 --> 00:02:17,470
there need for another

50
00:02:17,470 --> 00:02:20,950
why is Marius Kafka streams different so

51
00:02:20,950 --> 00:02:23,140
one very important feature is it's a

52
00:02:23,140 --> 00:02:24,820
really small library it's not a huge

53
00:02:24,820 --> 00:02:27,670
framework that like you get this Park it

54
00:02:27,670 --> 00:02:29,470
doesn't have any further external

55
00:02:29,470 --> 00:02:33,070
dependencies so then you write a stream

56
00:02:33,070 --> 00:02:34,540
processing up in Kafka streams you'll

57
00:02:34,540 --> 00:02:37,150
just put a dependency on Kafka streams

58
00:02:37,150 --> 00:02:41,290
in your cradle and that's it and in the

59
00:02:41,290 --> 00:02:42,880
end what we get is a very simple small

60
00:02:42,880 --> 00:02:45,130
java application usually less than 10

61
00:02:45,130 --> 00:02:48,340
megabytes and you can run that one

62
00:02:48,340 --> 00:02:50,440
everywhere so you can put it on yarn on

63
00:02:50,440 --> 00:02:54,190
me source docker when it is all that you

64
00:02:54,190 --> 00:02:56,080
can also just copy the char on a server

65
00:02:56,080 --> 00:02:59,140
somewhere and started there so it's

66
00:02:59,140 --> 00:03:00,610
pretty agnostic on how you want to

67
00:03:00,610 --> 00:03:03,370
deploy it by most other stream

68
00:03:03,370 --> 00:03:05,290
processing frameworks they really have

69
00:03:05,290 --> 00:03:09,040
very explicit SOI requirements on where

70
00:03:09,040 --> 00:03:13,840
you deploy them they the Kafka streams

71
00:03:13,840 --> 00:03:16,630
basically uses this Kafka consumer probe

72
00:03:16,630 --> 00:03:19,150
logic that I introduced for scaling and

73
00:03:19,150 --> 00:03:21,370
fault tolerance so other stream

74
00:03:21,370 --> 00:03:23,170
processing frameworks had to really

75
00:03:23,170 --> 00:03:25,840
invest a lot of work to get elastic

76
00:03:25,840 --> 00:03:29,380
scaling and all that working and Kafka

77
00:03:29,380 --> 00:03:31,780
streams basically got it too for free by

78
00:03:31,780 --> 00:03:36,400
using the consumer logic and that

79
00:03:36,400 --> 00:03:38,110
consumer logic with Kafka has been

80
00:03:38,110 --> 00:03:40,600
already tested in battle-hardened for

81
00:03:40,600 --> 00:03:42,670
many years by now so it's also something

82
00:03:42,670 --> 00:03:45,930
that by now has proven to be working and

83
00:03:45,930 --> 00:03:49,299
it's due to its very small footprint

84
00:03:49,299 --> 00:03:50,830
it's ideal for a micro service

85
00:03:50,830 --> 00:03:53,170
architecture so you can have your

86
00:03:53,170 --> 00:03:56,200
different services or run Kafka streams

87
00:03:56,200 --> 00:03:58,720
applications that basically pull the

88
00:03:58,720 --> 00:04:01,450
transformations that read from the Kafka

89
00:04:01,450 --> 00:04:03,519
cluster and then expose that information

90
00:04:03,519 --> 00:04:06,370
back as a Kafka cluster as a topic in

91
00:04:06,370 --> 00:04:10,720
the Kafka cluster and Kafka streams has

92
00:04:10,720 --> 00:04:14,080
two different API so both in Java at the

93
00:04:14,080 --> 00:04:16,779
moment high level DSL which is very

94
00:04:16,779 --> 00:04:19,060
similar what you would get in spark with

95
00:04:19,060 --> 00:04:22,750
those filtered so in map functions that

96
00:04:22,750 --> 00:04:25,720
you can use and a low level processor

97
00:04:25,720 --> 00:04:27,820
API which is more explicit and at the

98
00:04:27,820 --> 00:04:29,950
moment also a bit more powerful it's

99
00:04:29,950 --> 00:04:31,180
similar to

100
00:04:31,180 --> 00:04:38,229
you get in storm in Samsa it also has a

101
00:04:38,229 --> 00:04:40,840
the concept of tables and streams that

102
00:04:40,840 --> 00:04:45,520
are separate so streams are basically

103
00:04:45,520 --> 00:04:47,889
infinite event blocks or you have all

104
00:04:47,889 --> 00:04:49,509
these messages coming in and the

105
00:04:49,509 --> 00:04:51,030
assumption is that it goes on forever

106
00:04:51,030 --> 00:04:54,639
and tables are finite with updates

107
00:04:54,639 --> 00:04:58,229
deletes inserts on their primary key and

108
00:04:58,229 --> 00:05:01,479
both can be represented as message

109
00:05:01,479 --> 00:05:04,479
queues as kafka topics by just taking

110
00:05:04,479 --> 00:05:07,539
the change look of a table so we have

111
00:05:07,539 --> 00:05:10,840
that year if a table and each action

112
00:05:10,840 --> 00:05:13,660
that modifies its content is a change

113
00:05:13,660 --> 00:05:18,220
log entry in a stream and this allows

114
00:05:18,220 --> 00:05:21,940
you to basically have stuff for example

115
00:05:21,940 --> 00:05:24,580
static look-up tables that you join with

116
00:05:24,580 --> 00:05:27,940
incoming data streams so one classic

117
00:05:27,940 --> 00:05:29,770
example is that you have customer events

118
00:05:29,770 --> 00:05:31,180
coming in and you want to look up

119
00:05:31,180 --> 00:05:35,460
customer information and enrich that

120
00:05:37,409 --> 00:05:39,880
kafka streams of course also allows

121
00:05:39,880 --> 00:05:44,740
stateful processing and it stores its

122
00:05:44,740 --> 00:05:46,780
state locally in a rocks DP by default

123
00:05:46,780 --> 00:05:48,639
you can also plug in any other DP if you

124
00:05:48,639 --> 00:05:55,990
look like and Oh doesn't skip yeah all

125
00:05:55,990 --> 00:05:57,909
right so you have a usually I rocks DP

126
00:05:57,909 --> 00:06:01,389
as a local state and this state with

127
00:06:01,389 --> 00:06:03,520
rocks DP is a key value store it's again

128
00:06:03,520 --> 00:06:06,039
some kind of a table which as we saw

129
00:06:06,039 --> 00:06:08,770
before can be seen as a change look and

130
00:06:08,770 --> 00:06:11,199
change look it's a stream so the way

131
00:06:11,199 --> 00:06:17,409
this state store is is made available is

132
00:06:17,409 --> 00:06:19,659
that it's written back to Kafka as a

133
00:06:19,659 --> 00:06:21,460
topic as a change book topic for your

134
00:06:21,460 --> 00:06:23,560
state store and this is the way that

135
00:06:23,560 --> 00:06:27,250
Kafka streams handers rebalancing and

136
00:06:27,250 --> 00:06:30,190
for tolerance because any other instance

137
00:06:30,190 --> 00:06:31,900
of your application can just read the

138
00:06:31,900 --> 00:06:36,210
store of your with the state of your

139
00:06:36,210 --> 00:06:38,680
streaming application from that change

140
00:06:38,680 --> 00:06:43,360
look and recover it so instead of these

141
00:06:43,360 --> 00:06:45,280
checkpointing or lineage that we get

142
00:06:45,280 --> 00:06:47,290
another streaming application you really

143
00:06:47,290 --> 00:06:49,960
can recover the full state of your

144
00:06:49,960 --> 00:06:52,000
application in case it goes down or in

145
00:06:52,000 --> 00:06:54,550
case you want to launch additional

146
00:06:54,550 --> 00:06:57,610
instances of it so this allows you to

147
00:06:57,610 --> 00:07:01,840
really scale your streaming applications

148
00:07:01,840 --> 00:07:04,270
up very seamlessly you just launch

149
00:07:04,270 --> 00:07:06,160
another application on a different

150
00:07:06,160 --> 00:07:07,510
computer or different server for example

151
00:07:07,510 --> 00:07:10,450
it connects to the consumer group it

152
00:07:10,450 --> 00:07:12,130
reads from the changelog recovery

153
00:07:12,130 --> 00:07:16,650
ripples the local state and takes over

154
00:07:16,650 --> 00:07:18,730
nowadays in the new version you can

155
00:07:18,730 --> 00:07:22,770
actually even vary the local state so

156
00:07:22,770 --> 00:07:24,760
depending on your use case you don't

157
00:07:24,760 --> 00:07:27,190
even need to worry okay now I my

158
00:07:27,190 --> 00:07:29,140
streaming application rights to the data

159
00:07:29,140 --> 00:07:31,060
back to Kafka and then from Kafka I need

160
00:07:31,060 --> 00:07:32,800
to write it to us some other kind of

161
00:07:32,800 --> 00:07:34,990
database and from that database like my

162
00:07:34,990 --> 00:07:37,930
application can finally read it no this

163
00:07:37,930 --> 00:07:40,930
way you can actually query the state of

164
00:07:40,930 --> 00:07:43,450
your streaming application directly you

165
00:07:43,450 --> 00:07:45,400
don't need additional infrastructure in

166
00:07:45,400 --> 00:07:48,580
between which again allows you to build

167
00:07:48,580 --> 00:07:51,340
very lightweight applications that don't

168
00:07:51,340 --> 00:07:56,280
need a lot of additional infrastructure

169
00:07:59,360 --> 00:08:02,330
a lot of tools and components are on the

170
00:08:02,330 --> 00:08:04,580
confluent platform and cough can make

171
00:08:04,580 --> 00:08:06,590
the implicit assumption that you're

172
00:08:06,590 --> 00:08:10,069
primarily using overall so as to it's

173
00:08:10,069 --> 00:08:12,229
like Kafka Connect which is responsible

174
00:08:12,229 --> 00:08:15,650
for connecting Kafka to other systems

175
00:08:15,650 --> 00:08:18,909
like relational databases or a tube and

176
00:08:18,909 --> 00:08:22,430
they usually work with our own messages

177
00:08:22,430 --> 00:08:24,710
same with a schema registry which at the

178
00:08:24,710 --> 00:08:28,069
moment only works for avro messages so

179
00:08:28,069 --> 00:08:29,780
overall usually is the natural choice if

180
00:08:29,780 --> 00:08:32,599
you start using Kafka it doesn't require

181
00:08:32,599 --> 00:08:35,000
a lot of effort to implement since you

182
00:08:35,000 --> 00:08:37,669
already get everything it's also a great

183
00:08:37,669 --> 00:08:43,279
choice in solution but one downside is

184
00:08:43,279 --> 00:08:44,899
that you always need to have the correct

185
00:08:44,899 --> 00:08:47,420
schema the message was written in in

186
00:08:47,420 --> 00:08:50,120
order to be able to read that message

187
00:08:50,120 --> 00:08:53,750
reliably and if you don't have the

188
00:08:53,750 --> 00:08:55,339
schema available to you for some reason

189
00:08:55,339 --> 00:08:58,459
then all your data is just white garbage

190
00:08:58,459 --> 00:09:00,140
that you can't really do anything with

191
00:09:00,140 --> 00:09:05,930
it and this is where Google protocol

192
00:09:05,930 --> 00:09:11,420
buffers come in it's very similar to

193
00:09:11,420 --> 00:09:13,160
Avro in a lot of features it's a binary

194
00:09:13,160 --> 00:09:16,250
message it has a defined schema it has a

195
00:09:16,250 --> 00:09:18,620
support for lots of languages to read

196
00:09:18,620 --> 00:09:22,760
and write photo buff messages but a big

197
00:09:22,760 --> 00:09:24,470
difference is that you can also read

198
00:09:24,470 --> 00:09:26,050
these polar buff messages with a

199
00:09:26,050 --> 00:09:28,370
different version of the schema or in

200
00:09:28,370 --> 00:09:30,680
this no schema at all you can get quite

201
00:09:30,680 --> 00:09:35,209
some information out of it so for as an

202
00:09:35,209 --> 00:09:37,250
example if you're one of the tutorial

203
00:09:37,250 --> 00:09:41,769
protobuf messages which is a person and

204
00:09:41,769 --> 00:09:44,660
this person message has several fields a

205
00:09:44,660 --> 00:09:49,220
name and ID as required fields and an

206
00:09:49,220 --> 00:09:52,670
email as a string and then phone numbers

207
00:09:52,670 --> 00:09:54,120
as

208
00:09:54,120 --> 00:09:56,249
an embedded message so you can also have

209
00:09:56,249 --> 00:09:58,649
basically hierarchies of protobuf

210
00:09:58,649 --> 00:10:01,769
messages and each field basically

211
00:10:01,769 --> 00:10:05,970
consists of four parts in dignify if

212
00:10:05,970 --> 00:10:09,180
it's required optional or repeated the

213
00:10:09,180 --> 00:10:12,480
type like string or integer field name

214
00:10:12,480 --> 00:10:21,059
and then a field ID so this is one

215
00:10:21,059 --> 00:10:24,480
example person John Doe you can see here

216
00:10:24,480 --> 00:10:26,220
with full schema we get all the

217
00:10:26,220 --> 00:10:28,199
information we have the name the ID

218
00:10:28,199 --> 00:10:32,220
email and phone number and as a binary

219
00:10:32,220 --> 00:10:34,889
message it looks like this so you can

220
00:10:34,889 --> 00:10:36,959
still see all the strings in that there

221
00:10:36,959 --> 00:10:39,990
but everything else is just fights that

222
00:10:39,990 --> 00:10:44,399
you can't as a human really read but now

223
00:10:44,399 --> 00:10:46,439
we can also read this message back with

224
00:10:46,439 --> 00:10:48,629
a different schema version so let's say

225
00:10:48,629 --> 00:10:50,220
we have an old schema that doesn't know

226
00:10:50,220 --> 00:10:53,519
about the email field yet we still get

227
00:10:53,519 --> 00:10:55,829
all the fields that we know about the

228
00:10:55,829 --> 00:10:58,589
name the ID and the phone but also

229
00:10:58,589 --> 00:11:00,990
additionally we get this field that we

230
00:11:00,990 --> 00:11:03,899
don't know anything about but the email

231
00:11:03,899 --> 00:11:06,480
and they are we just get a field ID and

232
00:11:06,480 --> 00:11:10,199
not the field name but we can still even

233
00:11:10,199 --> 00:11:12,540
just by looking at it kind of guess that

234
00:11:12,540 --> 00:11:16,019
this is supposed to be an email so even

235
00:11:16,019 --> 00:11:18,240
though we we haven't even heard of this

236
00:11:18,240 --> 00:11:20,279
field before we can still get the

237
00:11:20,279 --> 00:11:22,249
information out of there and read it and

238
00:11:22,249 --> 00:11:25,620
if we try to read the message here

239
00:11:25,620 --> 00:11:28,350
without any schema at all this is what

240
00:11:28,350 --> 00:11:30,149
we're gonna get we still have all the

241
00:11:30,149 --> 00:11:33,540
fields in the same order we just lose

242
00:11:33,540 --> 00:11:35,339
the field names instead have only the

243
00:11:35,339 --> 00:11:39,839
IDS of the field so even with no knowing

244
00:11:39,839 --> 00:11:41,999
nothing at all about the message we can

245
00:11:41,999 --> 00:11:44,600
still recover quite a lot of information

246
00:11:44,600 --> 00:11:47,910
this is basically the output of proto CD

247
00:11:47,910 --> 00:11:50,670
code raw you can put any protobuf

248
00:11:50,670 --> 00:11:52,410
message in there and it will tell you

249
00:11:52,410 --> 00:11:54,329
what fields are in there and this is a

250
00:11:54,329 --> 00:11:56,040
kind of thing that would be very hard to

251
00:11:56,040 --> 00:11:59,300
do with overall

252
00:12:00,130 --> 00:12:03,650
what another very cool feature our

253
00:12:03,650 --> 00:12:06,140
unknown fields so these are the fields

254
00:12:06,140 --> 00:12:08,350
that we don't know anything about and

255
00:12:08,350 --> 00:12:10,640
not only can you read them as I showed

256
00:12:10,640 --> 00:12:12,110
you with your schema you can also pass

257
00:12:12,110 --> 00:12:16,040
them on so if you read a message and you

258
00:12:16,040 --> 00:12:17,930
write it a crate and you want from this

259
00:12:17,930 --> 00:12:20,240
one then all the unknown fields that you

260
00:12:20,240 --> 00:12:22,610
can find in your original message will

261
00:12:22,610 --> 00:12:24,560
be copied over to the new one which

262
00:12:24,560 --> 00:12:26,720
allows you to basically to say things

263
00:12:26,720 --> 00:12:28,940
like decorations or extracting common

264
00:12:28,940 --> 00:12:31,700
fields or envelopes without needing to

265
00:12:31,700 --> 00:12:33,260
know there's a schema information of

266
00:12:33,260 --> 00:12:34,850
your original message you just put that

267
00:12:34,850 --> 00:12:37,340
message inside another message we will

268
00:12:37,340 --> 00:12:39,740
take over all the fields and send it on

269
00:12:39,740 --> 00:12:42,820
and you would not have to worry about

270
00:12:42,820 --> 00:12:45,620
breaking or destroying their original

271
00:12:45,620 --> 00:12:51,230
content it will still be fully there so

272
00:12:51,230 --> 00:12:54,290
now how can you use those two components

273
00:12:54,290 --> 00:12:56,300
together for a stream processing

274
00:12:56,300 --> 00:12:59,600
architecture so generally you still have

275
00:12:59,600 --> 00:13:02,390
all the usual message producers like

276
00:13:02,390 --> 00:13:05,620
logs databases api's external sources

277
00:13:05,620 --> 00:13:10,510
they all write their information in

278
00:13:10,510 --> 00:13:15,230
protobuf to your Kafka cluster where you

279
00:13:15,230 --> 00:13:17,900
have streaming processors that enrich

280
00:13:17,900 --> 00:13:21,500
showing filter aggregate all the usual

281
00:13:21,500 --> 00:13:23,810
stuff with your topics and create new

282
00:13:23,810 --> 00:13:26,450
topics with the output and that one can

283
00:13:26,450 --> 00:13:29,390
go to your various consumers for example

284
00:13:29,390 --> 00:13:32,240
databases reports dashboards other

285
00:13:32,240 --> 00:13:35,450
applications machine learning or storing

286
00:13:35,450 --> 00:13:39,530
it in Hadoop for example and you can

287
00:13:39,530 --> 00:13:43,220
exchange schema versions through for

288
00:13:43,220 --> 00:13:45,110
example a git repository or some other

289
00:13:45,110 --> 00:13:47,210
central place where everyone can

290
00:13:47,210 --> 00:13:52,990
retrieve these definitions and the

291
00:13:52,990 --> 00:13:54,920
developers engineers that write the

292
00:13:54,920 --> 00:13:57,650
message they write the input data they

293
00:13:57,650 --> 00:14:00,170
provide their protobuf definitions the

294
00:14:00,170 --> 00:14:01,610
engineers writing the streaming

295
00:14:01,610 --> 00:14:04,630
processors they also provide the

296
00:14:04,630 --> 00:14:07,160
definitions for their output while

297
00:14:07,160 --> 00:14:09,380
reading the one from the engineers and

298
00:14:09,380 --> 00:14:12,800
the consumers they just need to read the

299
00:14:12,800 --> 00:14:14,899
schema definitions to be able to parse

300
00:14:14,899 --> 00:14:18,769
everything correctly so how would that

301
00:14:18,769 --> 00:14:22,370
look as a likeness in an example here we

302
00:14:22,370 --> 00:14:26,769
have three different systems that emit

303
00:14:26,769 --> 00:14:30,560
events to Kafka so for example a web

304
00:14:30,560 --> 00:14:34,120
service that writes block events to your

305
00:14:34,120 --> 00:14:37,490
first stream so log events from your

306
00:14:37,490 --> 00:14:39,890
website for example in there we have our

307
00:14:39,890 --> 00:14:42,170
time stem field a session ID and the

308
00:14:42,170 --> 00:14:45,290
type like okay you looked at a product

309
00:14:45,290 --> 00:14:47,329
or a customer put a product in their

310
00:14:47,329 --> 00:14:49,490
shopping cart and then we have the

311
00:14:49,490 --> 00:14:52,300
option of use customer ID and Product ID

312
00:14:52,300 --> 00:14:55,610
then from your customer database you get

313
00:14:55,610 --> 00:14:58,970
these customer messages which again have

314
00:14:58,970 --> 00:15:02,810
a customer ID name and an email and your

315
00:15:02,810 --> 00:15:05,690
product database provides the product

316
00:15:05,690 --> 00:15:10,130
messages and with a product ID a name

317
00:15:10,130 --> 00:15:15,260
and a price these then you have a

318
00:15:15,260 --> 00:15:16,640
streaming application joining these

319
00:15:16,640 --> 00:15:19,040
three topics and emitting a new message

320
00:15:19,040 --> 00:15:22,430
which is a very simple message my rich

321
00:15:22,430 --> 00:15:24,350
event message basically has three fields

322
00:15:24,350 --> 00:15:27,110
one event field one customer field and

323
00:15:27,110 --> 00:15:30,350
one product field so I actually don't

324
00:15:30,350 --> 00:15:33,350
need to read to write a lot of schema

325
00:15:33,350 --> 00:15:35,120
definitions because I can just import

326
00:15:35,120 --> 00:15:39,230
the previous ones there and the output

327
00:15:39,230 --> 00:15:41,270
of this deeming application is again a

328
00:15:41,270 --> 00:15:44,600
stream in my Kafka cluster of these rich

329
00:15:44,600 --> 00:15:48,110
events and then in that I can for

330
00:15:48,110 --> 00:15:51,410
example plug in another application that

331
00:15:51,410 --> 00:15:53,450
does something like session association

332
00:15:53,450 --> 00:15:56,360
so it takes the session ID out of there

333
00:15:56,360 --> 00:16:00,620
and goes by them and my message my

334
00:16:00,620 --> 00:16:02,570
session message is also again very

335
00:16:02,570 --> 00:16:07,730
simple session message just consists of

336
00:16:07,730 --> 00:16:12,140
repeated which event messages so makes

337
00:16:12,140 --> 00:16:14,329
it very easy to define these things of

338
00:16:14,329 --> 00:16:15,800
course at this step I can also do

339
00:16:15,800 --> 00:16:17,990
further occasions like include fields

340
00:16:17,990 --> 00:16:20,240
for counting how many different products

341
00:16:20,240 --> 00:16:22,310
the user has looked at or how many

342
00:16:22,310 --> 00:16:24,560
interactions they did with our website

343
00:16:24,560 --> 00:16:25,790
and so on

344
00:16:25,790 --> 00:16:28,870
it's basically a very simple version and

345
00:16:28,870 --> 00:16:31,370
that one is then something that you can

346
00:16:31,370 --> 00:16:33,560
for example read by machine learning

347
00:16:33,560 --> 00:16:35,510
algorithms for personalization or

348
00:16:35,510 --> 00:16:38,030
recommendation engines you can put it in

349
00:16:38,030 --> 00:16:41,540
reports or you can let your analysts

350
00:16:41,540 --> 00:16:44,300
read it and do analyst and - there are

351
00:16:44,300 --> 00:16:52,580
talk reports on it now going now going

352
00:16:52,580 --> 00:16:55,190
back to this chart so this git

353
00:16:55,190 --> 00:16:57,260
repository or whatever you're using I

354
00:16:57,260 --> 00:16:59,150
mean it's basically just like they are

355
00:16:59,150 --> 00:17:01,430
Oh schema registry that comes with a

356
00:17:01,430 --> 00:17:03,260
confluent platform of Kafka so why is

357
00:17:03,260 --> 00:17:05,569
this one what's the advantage of this

358
00:17:05,569 --> 00:17:09,170
one well it can always happen that not

359
00:17:09,170 --> 00:17:11,690
all the product definitions there are

360
00:17:11,690 --> 00:17:14,170
actually up-to-date for example the

361
00:17:14,170 --> 00:17:16,940
developer could have forgotten to commit

362
00:17:16,940 --> 00:17:19,940
their message or the host of your kid

363
00:17:19,940 --> 00:17:22,160
repository accidentally deleted their

364
00:17:22,160 --> 00:17:24,770
production database and is now trying to

365
00:17:24,770 --> 00:17:27,140
recover up back up so it could always

366
00:17:27,140 --> 00:17:30,110
happen that this one doesn't quite work

367
00:17:30,110 --> 00:17:32,930
and someone doesn't have all the schema

368
00:17:32,930 --> 00:17:36,110
versions that they need so what happens

369
00:17:36,110 --> 00:17:39,170
in that case let's for example assume

370
00:17:39,170 --> 00:17:42,350
that in your product our database

371
00:17:42,350 --> 00:17:45,140
someone at some new field for the color

372
00:17:45,140 --> 00:17:47,420
of your product and starts writing that

373
00:17:47,420 --> 00:17:53,030
for the Kafka product stream now your

374
00:17:53,030 --> 00:17:55,010
three main application is starting to or

375
00:17:55,010 --> 00:17:56,690
is processing these events and putting

376
00:17:56,690 --> 00:18:00,200
them in the rich event message and all

377
00:18:00,200 --> 00:18:02,690
that happens is now that the product

378
00:18:02,690 --> 00:18:05,150
contains an unknown field which is the

379
00:18:05,150 --> 00:18:07,370
color but everything else will just keep

380
00:18:07,370 --> 00:18:11,590
on working and

381
00:18:12,900 --> 00:18:16,230
and even further down it causes no

382
00:18:16,230 --> 00:18:17,160
trouble at all

383
00:18:17,160 --> 00:18:19,380
my session will still contain these

384
00:18:19,380 --> 00:18:20,580
future events and these future events

385
00:18:20,580 --> 00:18:23,130
all will contain a product with unknown

386
00:18:23,130 --> 00:18:27,750
fields and even at the end the

387
00:18:27,750 --> 00:18:29,250
applications can still read these

388
00:18:29,250 --> 00:18:34,080
messages and if the VI are a list or a

389
00:18:34,080 --> 00:18:39,240
data scientists at the end looks at the

390
00:18:39,240 --> 00:18:41,550
messages he can find that unknown field

391
00:18:41,550 --> 00:18:43,500
it can look at the value and might even

392
00:18:43,500 --> 00:18:46,830
be able to find a determine that it's a

393
00:18:46,830 --> 00:18:49,410
supposed to be color maybe he was even

394
00:18:49,410 --> 00:18:50,670
the one that wrote the task to the

395
00:18:50,670 --> 00:18:52,230
engineer in the first place to put the

396
00:18:52,230 --> 00:18:54,720
field in there so the whole pipeline

397
00:18:54,720 --> 00:19:00,350
basically still works the same way and

398
00:19:00,350 --> 00:19:02,880
similar if it's just a need a field like

399
00:19:02,880 --> 00:19:07,680
here the price for a product off there's

400
00:19:07,680 --> 00:19:09,210
no difference between a field that

401
00:19:09,210 --> 00:19:10,980
simply doesn't exist in an empty field

402
00:19:10,980 --> 00:19:13,740
with a null value they both are simply

403
00:19:13,740 --> 00:19:16,530
not included in the protobuf message so

404
00:19:16,530 --> 00:19:18,990
you're going to have now is product that

405
00:19:18,990 --> 00:19:22,530
contains a null price and since your

406
00:19:22,530 --> 00:19:25,350
price since the very beginning was

407
00:19:25,350 --> 00:19:29,120
marked as an optional hopefully the

408
00:19:29,120 --> 00:19:30,840
engineers writing the stream

409
00:19:30,840 --> 00:19:33,840
applications kind of put in logic to

410
00:19:33,840 --> 00:19:36,360
handle null prices already so your

411
00:19:36,360 --> 00:19:38,640
streaming applications will continue

412
00:19:38,640 --> 00:19:43,920
running uninterrupted so in general what

413
00:19:43,920 --> 00:19:46,500
does this allow you so means that the

414
00:19:46,500 --> 00:19:48,570
different teams that are responsible for

415
00:19:48,570 --> 00:19:50,610
data for producing the data for

416
00:19:50,610 --> 00:19:52,590
processing the data for consuming the

417
00:19:52,590 --> 00:19:55,290
data they can all move at their own

418
00:19:55,290 --> 00:19:57,660
speed there's no real strict alignment

419
00:19:57,660 --> 00:19:59,610
for releases necessary so if you have a

420
00:19:59,610 --> 00:20:02,190
traditional OLTP system that feeds into

421
00:20:02,190 --> 00:20:03,810
a data warehouse and you want to do

422
00:20:03,810 --> 00:20:06,990
schema changes across the system with

423
00:20:06,990 --> 00:20:09,000
several for example my Secret Service

424
00:20:09,000 --> 00:20:11,820
involved it's very very painful because

425
00:20:11,820 --> 00:20:14,790
you need to make sure that each of the

426
00:20:14,790 --> 00:20:18,000
queries takes that into account but here

427
00:20:18,000 --> 00:20:20,250
we can really do a handoff and data

428
00:20:20,250 --> 00:20:23,430
engineering the the people in omit in

429
00:20:23,430 --> 00:20:24,990
the middle writing the screening process

430
00:20:24,990 --> 00:20:26,850
so they don't need to do anything

431
00:20:26,850 --> 00:20:29,040
they just rewrite that read the tape

432
00:20:29,040 --> 00:20:31,610
later and write it to the output and

433
00:20:31,610 --> 00:20:36,660
everyone can basically upgrade to new to

434
00:20:36,660 --> 00:20:39,990
new version schema versions at their own

435
00:20:39,990 --> 00:20:44,070
speed and only the actual producers and

436
00:20:44,070 --> 00:20:46,730
consumers need to align on including new

437
00:20:46,730 --> 00:20:49,740
information in your messages and the

438
00:20:49,740 --> 00:20:51,660
pipeline in between just forwards them

439
00:20:51,660 --> 00:20:57,690
without needing to know about it of

440
00:20:57,690 --> 00:21:00,780
course not everything is great about

441
00:21:00,780 --> 00:21:02,700
protobuf there are also some downsides

442
00:21:02,700 --> 00:21:06,650
compared to other also there's no

443
00:21:13,470 --> 00:21:16,450
like you kinda need to hand

444
00:21:16,450 --> 00:21:19,390
after your scheming be very considerate

445
00:21:19,390 --> 00:21:22,030
about it so you cannot reuse your field

446
00:21:22,030 --> 00:21:25,750
IDs that's the important thing and you

447
00:21:25,750 --> 00:21:28,990
need to stick to them and you need to

448
00:21:28,990 --> 00:21:30,730
also think about what you may mark as

449
00:21:30,730 --> 00:21:32,470
required and optional because once

450
00:21:32,470 --> 00:21:35,080
something is required you basically can

451
00:21:35,080 --> 00:21:37,870
never change it there's also less

452
00:21:37,870 --> 00:21:39,910
implementations for it around Kafka and

453
00:21:39,910 --> 00:21:42,670
Hadoop ecosystem and less people using

454
00:21:42,670 --> 00:21:44,970
it so you will find out that you will

455
00:21:44,970 --> 00:21:47,830
often have to write a serialized sati

456
00:21:47,830 --> 00:21:49,900
serialize for yourself there forever or

457
00:21:49,900 --> 00:21:52,450
you basically find enough

458
00:21:52,450 --> 00:21:56,170
implementations of that a big issue is

459
00:21:56,170 --> 00:21:59,200
that Google kind of wants to remove

460
00:21:59,200 --> 00:22:00,790
these unknown fields and protobuf free

461
00:22:00,790 --> 00:22:04,150
so very long discussion now going on in

462
00:22:04,150 --> 00:22:09,070
the related issue on github and one more

463
00:22:09,070 --> 00:22:11,230
thing is that protobuf messages usually

464
00:22:11,230 --> 00:22:14,530
are one or two bytes not sure and after

465
00:22:14,530 --> 00:22:17,380
all messages nowadays I don't think

466
00:22:17,380 --> 00:22:19,000
that's a really big deal but for some

467
00:22:19,000 --> 00:22:25,180
people that is important factor alright

468
00:22:25,180 --> 00:22:27,040
thank you I hope you learned something

469
00:22:27,040 --> 00:22:29,440
about kafka streams and pro buff and

470
00:22:29,440 --> 00:22:31,660
maybe have some inspiration yourself and

471
00:22:31,660 --> 00:22:35,460
we have any comments or questions

472
00:22:37,660 --> 00:22:54,790
I mean if you have so the question was

473
00:22:54,790 --> 00:22:57,040
if I only have two servers in my chakra

474
00:22:57,040 --> 00:22:58,840
cluster and one of them goes down how

475
00:22:58,840 --> 00:23:01,270
can I make sure that I don't lose any

476
00:23:01,270 --> 00:23:03,280
data and things still keep on running

477
00:23:03,280 --> 00:23:05,980
but if you set your topics to all have a

478
00:23:05,980 --> 00:23:08,650
replication factor of two it means each

479
00:23:08,650 --> 00:23:10,680
of your partitions everything should be

480
00:23:10,680 --> 00:23:13,620
existing on both servers so in that case

481
00:23:13,620 --> 00:23:16,360
basically they would be identical they

482
00:23:16,360 --> 00:23:18,460
both would have the full data set and if

483
00:23:18,460 --> 00:23:19,810
one of them goes down the other one

484
00:23:19,810 --> 00:23:22,360
still has everything and the producers

485
00:23:22,360 --> 00:23:25,210
would then all go to that one running

486
00:23:25,210 --> 00:23:28,030
path Katla server and write everything

487
00:23:28,030 --> 00:23:30,550
there and the consumers would also start

488
00:23:30,550 --> 00:23:33,550
on your reading from that one did that

489
00:23:33,550 --> 00:23:36,060
answer the question

490
00:23:39,510 --> 00:23:42,450
yep

491
00:23:42,450 --> 00:23:48,590
I mean usually the producer sends out

492
00:23:48,590 --> 00:23:51,360
the Kafka cluster sends out a signal

493
00:23:51,360 --> 00:23:54,210
back to the producer that it received

494
00:23:54,210 --> 00:23:55,740
the message and processed it and you can

495
00:23:55,740 --> 00:23:56,370
also define

496
00:23:56,370 --> 00:23:58,470
vended us that so we can say only

497
00:23:58,470 --> 00:24:00,950
acknowledge a message if it has been

498
00:24:00,950 --> 00:24:03,450
replicated on both machines so you can

499
00:24:03,450 --> 00:24:06,600
make sure that nothing is going missing

500
00:24:06,600 --> 00:24:08,670
let's actually a talk by Queen Shakira

501
00:24:08,670 --> 00:24:11,640
about when messages absolutely have to

502
00:24:11,640 --> 00:24:13,500
be there if you want to have a very

503
00:24:13,500 --> 00:24:15,060
strong guarantee that your messages are

504
00:24:15,060 --> 00:24:17,220
there recommend looking that up it's

505
00:24:17,220 --> 00:24:23,480
very helpful in that topic yeah

