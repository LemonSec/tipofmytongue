1
00:00:04,930 --> 00:00:09,440
hello everyone I'm Martin I'm a solution

2
00:00:09,440 --> 00:00:11,330
architect at Cloudera which makes that I

3
00:00:11,330 --> 00:00:13,429
help customers with Flynn Club there is

4
00:00:13,429 --> 00:00:17,360
one of the spark sorry which is one of

5
00:00:17,360 --> 00:00:19,250
the cloud there is one of the vendors

6
00:00:19,250 --> 00:00:21,619
that you can get out there to support

7
00:00:21,619 --> 00:00:24,410
you and spark and I'm PMC member of

8
00:00:24,410 --> 00:00:26,689
Apache Fling so I have quite some

9
00:00:26,689 --> 00:00:28,160
experience with both of these systems

10
00:00:28,160 --> 00:00:32,448
and then I'm going to walk you through a

11
00:00:32,448 --> 00:00:35,420
couple of use cases in this talk mostly

12
00:00:35,420 --> 00:00:37,940
focusing on the API not necessarily the

13
00:00:37,940 --> 00:00:39,680
the performance a little bit on the

14
00:00:39,680 --> 00:00:42,199
architecture of course I'm ready to play

15
00:00:42,199 --> 00:00:44,659
questions on those as well but basically

16
00:00:44,659 --> 00:00:46,400
what I would like to give you as a

17
00:00:46,400 --> 00:00:49,699
takeaway message is these systems are

18
00:00:49,699 --> 00:00:52,400
evolving a lot in terms of the api's and

19
00:00:52,400 --> 00:00:56,750
you can go much further with such a

20
00:00:56,750 --> 00:00:58,519
system than you would imagine

21
00:00:58,519 --> 00:01:01,010
in terms of use cases so the setup that

22
00:01:01,010 --> 00:01:03,229
we are going to use for this is the soul

23
00:01:03,229 --> 00:01:05,720
is this big fat story example she's

24
00:01:05,720 --> 00:01:08,299
nothing really fancy we will have a

25
00:01:08,299 --> 00:01:10,280
couple of pet stores so the model that

26
00:01:10,280 --> 00:01:12,560
we will see is we have a map with a

27
00:01:12,560 --> 00:01:15,140
couple of stores and we have customers

28
00:01:15,140 --> 00:01:17,150
shopping around in these stores

29
00:01:17,150 --> 00:01:19,610
generating transactions and we will take

30
00:01:19,610 --> 00:01:21,890
it from there

31
00:01:21,890 --> 00:01:25,130
so we have already best word count today

32
00:01:25,130 --> 00:01:27,350
a couple of times I will continue with

33
00:01:27,350 --> 00:01:29,030
that I guess that's that's something

34
00:01:29,030 --> 00:01:31,100
that everyone has to do to a certain

35
00:01:31,100 --> 00:01:34,820
level then I will elaborate a little bit

36
00:01:34,820 --> 00:01:37,070
on the model and then we will see how to

37
00:01:37,070 --> 00:01:40,100
do a little bit of data generation with

38
00:01:40,100 --> 00:01:43,310
the spark and flink also how to use

39
00:01:43,310 --> 00:01:46,610
their sequel api's or the api's that are

40
00:01:46,610 --> 00:01:49,370
very close to our data frames if you are

41
00:01:49,370 --> 00:01:51,290
familiar with that then we will do a

42
00:01:51,290 --> 00:01:53,180
little bit of machine learning and then

43
00:01:53,180 --> 00:01:55,460
prediction in streaming already near

44
00:01:55,460 --> 00:01:58,310
real-time so it's going to be a little

45
00:01:58,310 --> 00:02:00,409
too much code because I also had to cut

46
00:02:00,409 --> 00:02:02,360
a little bit of the slides from the

47
00:02:02,360 --> 00:02:05,450
theoretical part of course so what I

48
00:02:05,450 --> 00:02:09,348
suggest to try to do together is I would

49
00:02:09,348 --> 00:02:11,209
like to tell you the story and the code

50
00:02:11,209 --> 00:02:14,060
is going to be there and having the

51
00:02:14,060 --> 00:02:16,790
story and not necessarily understanding

52
00:02:16,790 --> 00:02:18,380
the code a hundred percent that

53
00:02:18,380 --> 00:02:20,090
that's completely what we are shooting

54
00:02:20,090 --> 00:02:23,930
for today so infamous word count example

55
00:02:23,930 --> 00:02:26,450
who have already seen a word count in

56
00:02:26,450 --> 00:02:29,360
their life okay good enough but that

57
00:02:29,360 --> 00:02:31,040
that's why I put it up there because

58
00:02:31,040 --> 00:02:33,680
it's it's just half of the people in the

59
00:02:33,680 --> 00:02:34,040
room

60
00:02:34,040 --> 00:02:36,470
so basically word count is the hello

61
00:02:36,470 --> 00:02:38,780
world of big data and that's exactly

62
00:02:38,780 --> 00:02:40,940
what it is it gives you the first

63
00:02:40,940 --> 00:02:43,850
impression it has certain merits and we

64
00:02:43,850 --> 00:02:46,580
should give those to it basically it yes

65
00:02:46,580 --> 00:02:49,160
unfortunately it may be a little low if

66
00:02:49,160 --> 00:02:51,110
you are sitting in the background but it

67
00:02:51,110 --> 00:02:53,240
gives you the idea that the MapReduce

68
00:02:53,240 --> 00:02:56,510
paradigm does the following for you even

69
00:02:56,510 --> 00:02:58,820
though you will you have a huge chunk of

70
00:02:58,820 --> 00:03:01,490
data and even though you will never have

71
00:03:01,490 --> 00:03:04,010
a global view of it on any single

72
00:03:04,010 --> 00:03:05,840
machine because it doesn't fit into one

73
00:03:05,840 --> 00:03:08,960
single machine you can still produce a

74
00:03:08,960 --> 00:03:11,960
global global account count for a given

75
00:03:11,960 --> 00:03:15,380
key in it so it it has this power and

76
00:03:15,380 --> 00:03:18,080
word count exactly demonstrates that but

77
00:03:18,080 --> 00:03:19,790
it doesn't give you much more people

78
00:03:19,790 --> 00:03:22,910
sometimes use this also for and that's

79
00:03:22,910 --> 00:03:26,030
what I'm trying to get into benchmarking

80
00:03:26,030 --> 00:03:28,160
a big data application okay I have

81
00:03:28,160 --> 00:03:30,140
installed spark now let's learn a word

82
00:03:30,140 --> 00:03:32,360
count and see whether it's worse okay or

83
00:03:32,360 --> 00:03:33,010
not

84
00:03:33,010 --> 00:03:36,020
well it will benchmark a little bit on

85
00:03:36,020 --> 00:03:38,120
on the shuffling face so at least you

86
00:03:38,120 --> 00:03:40,100
have you have this network communication

87
00:03:40,100 --> 00:03:42,320
maybe the disk i/o in the beginning and

88
00:03:42,320 --> 00:03:44,720
in in the end but that's basically it

89
00:03:44,720 --> 00:03:47,330
what happens if you want to do data

90
00:03:47,330 --> 00:03:49,310
streaming or machine learning or graph

91
00:03:49,310 --> 00:03:51,680
processing is it going to be enough for

92
00:03:51,680 --> 00:03:56,150
you well most likely not and that's very

93
00:03:56,150 --> 00:03:59,240
start bashing word count of course so

94
00:03:59,240 --> 00:04:01,940
then the other cool stuff that basically

95
00:04:01,940 --> 00:04:04,880
every one who is talking today has to do

96
00:04:04,880 --> 00:04:07,400
is give a shout out to Big Top because

97
00:04:07,400 --> 00:04:09,950
Big Top is cool so I have to do that as

98
00:04:09,950 --> 00:04:10,340
well

99
00:04:10,340 --> 00:04:13,760
the this project in general is part of

100
00:04:13,760 --> 00:04:16,459
Apogee big top you can have a look at

101
00:04:16,459 --> 00:04:19,608
the code there and references to the big

102
00:04:19,608 --> 00:04:23,240
bad store model basically big big top

103
00:04:23,240 --> 00:04:30,320
not only gives you this ability to to

104
00:04:30,320 --> 00:04:32,150
install your big data systems

105
00:04:32,150 --> 00:04:35,330
with with the puppet scripts and the

106
00:04:35,330 --> 00:04:37,910
juju charms and all these cool stuff but

107
00:04:37,910 --> 00:04:39,950
also when it's already installed you

108
00:04:39,950 --> 00:04:42,080
might be able to benchmark it I would

109
00:04:42,080 --> 00:04:43,970
definitely say that this is not the most

110
00:04:43,970 --> 00:04:46,130
major part of Victor but it's a new

111
00:04:46,130 --> 00:04:48,290
initiative very contributions are very

112
00:04:48,290 --> 00:04:51,410
welcome well in terms of the model that

113
00:04:51,410 --> 00:04:55,220
we are using I already mentioned that

114
00:04:55,220 --> 00:04:57,140
this is this big pet store model there's

115
00:04:57,140 --> 00:04:59,210
a scientific paper out on and basically

116
00:04:59,210 --> 00:05:01,610
the idea that you need to have here is

117
00:05:01,610 --> 00:05:06,410
that it can generate data in a big data

118
00:05:06,410 --> 00:05:10,160
setting that even if it's small scare or

119
00:05:10,160 --> 00:05:12,800
big scale it's sort of still relevant on

120
00:05:12,800 --> 00:05:15,470
on testing your big data applications so

121
00:05:15,470 --> 00:05:18,100
for that reason it might be nice for you

122
00:05:18,100 --> 00:05:21,080
it's it spaced on the distributions why

123
00:05:21,080 --> 00:05:23,600
it might be relevant okay so the two

124
00:05:23,600 --> 00:05:24,950
systems that we are going to deal with

125
00:05:24,950 --> 00:05:28,670
today and mostly their API is spark and

126
00:05:28,670 --> 00:05:32,060
flink - data processing systems and I

127
00:05:32,060 --> 00:05:34,210
would like to just highlight one

128
00:05:34,210 --> 00:05:36,590
fundamental difference between the two

129
00:05:36,590 --> 00:05:38,630
in terms of the architecture and the

130
00:05:38,630 --> 00:05:41,260
other parts are just matching each other

131
00:05:41,260 --> 00:05:44,060
when you look at the spark runtime the

132
00:05:44,060 --> 00:05:46,550
basic runtime of Apache spark is the so

133
00:05:46,550 --> 00:05:49,070
called RDD Raziel and distributed data

134
00:05:49,070 --> 00:05:52,010
set api that's the batch API so it

135
00:05:52,010 --> 00:05:53,990
suggests that you already have your data

136
00:05:53,990 --> 00:05:56,240
sitting on your local file system or

137
00:05:56,240 --> 00:06:00,110
HDFS or hs3 wherever and it's there and

138
00:06:00,110 --> 00:06:02,510
it's a finite data set so that's what

139
00:06:02,510 --> 00:06:05,030
spark runs on natively and then they

140
00:06:05,030 --> 00:06:07,940
build streaming on top of it the main

141
00:06:07,940 --> 00:06:10,700
difference in flink is that in flink

142
00:06:10,700 --> 00:06:13,190
data in the data stream api the

143
00:06:13,190 --> 00:06:15,140
streaming one and the data set api the

144
00:06:15,140 --> 00:06:17,630
batch one those are on the same level so

145
00:06:17,630 --> 00:06:19,970
if you want to have this finite data set

146
00:06:19,970 --> 00:06:22,400
or if you want to have something that is

147
00:06:22,400 --> 00:06:24,250
near real-time maybe giving answers

148
00:06:24,250 --> 00:06:27,800
under a second that's that sort of both

149
00:06:27,800 --> 00:06:29,780
so native implementation the streaming

150
00:06:29,780 --> 00:06:31,790
doesn't get translated on top of a batch

151
00:06:31,790 --> 00:06:35,830
job and that has different implications

152
00:06:35,830 --> 00:06:38,720
because of dibs it's much easier in

153
00:06:38,720 --> 00:06:41,090
SPARC to integrate within a streaming

154
00:06:41,090 --> 00:06:42,740
and a batch job because a streaming job

155
00:06:42,740 --> 00:06:45,190
is just a sequence of batch jobs

156
00:06:45,190 --> 00:06:47,380
but it might be live eating in a couple

157
00:06:47,380 --> 00:06:49,480
of cases so it's it it includes the

158
00:06:49,480 --> 00:06:53,080
trade of and we can define the mapping

159
00:06:53,080 --> 00:06:55,300
that we are going to go through today so

160
00:06:55,300 --> 00:06:57,820
we have the batch API smashed up we'll

161
00:06:57,820 --> 00:06:59,800
have a look at the streaming ones the

162
00:06:59,800 --> 00:07:02,800
machine learning ones and also the the

163
00:07:02,800 --> 00:07:08,140
sequel slash table ones I this talk is a

164
00:07:08,140 --> 00:07:09,880
little too short to also include the

165
00:07:09,880 --> 00:07:12,220
graph ones but maybe next time for Vysya

166
00:07:12,220 --> 00:07:16,690
I have to do that as well so then let's

167
00:07:16,690 --> 00:07:22,120
let's go to the coding part run out one

168
00:07:22,120 --> 00:07:25,660
of the authors of set paper he also

169
00:07:25,660 --> 00:07:29,560
coded the whole generation process in

170
00:07:29,560 --> 00:07:31,900
java classes of course both of these

171
00:07:31,900 --> 00:07:35,620
systems do have Java and Scala api's I'm

172
00:07:35,620 --> 00:07:37,360
going to stick with the Scala one

173
00:07:37,360 --> 00:07:39,430
because it's just easier to read in a

174
00:07:39,430 --> 00:07:42,880
slide who considers themselves Scala

175
00:07:42,880 --> 00:07:46,480
developer okay a couple

176
00:07:46,480 --> 00:07:51,790
Java developer good enough maybe Python

177
00:07:51,790 --> 00:07:54,850
that also else good okay I'm safe thank

178
00:07:54,850 --> 00:07:59,500
you very much so just a little bit of

179
00:07:59,500 --> 00:08:01,510
basically we have a way to generate

180
00:08:01,510 --> 00:08:03,850
these classes on a single thread that

181
00:08:03,850 --> 00:08:06,520
row not has already provided for us but

182
00:08:06,520 --> 00:08:08,260
I would like to use these distributed

183
00:08:08,260 --> 00:08:08,770
systems

184
00:08:08,770 --> 00:08:10,600
I would like to generate a whole bunch

185
00:08:10,600 --> 00:08:12,580
of this data in a distributed fashion

186
00:08:12,580 --> 00:08:15,190
that's basically my point and of course

187
00:08:15,190 --> 00:08:18,030
it's pretty easy to to do that the Veda

188
00:08:18,030 --> 00:08:21,640
we are doing it here is for this data

189
00:08:21,640 --> 00:08:23,500
generation to work we need to generate

190
00:08:23,500 --> 00:08:26,620
the so-called ground truth which is our

191
00:08:26,620 --> 00:08:28,750
customer IDs and the name of the

192
00:08:28,750 --> 00:08:30,730
customers basic customer information

193
00:08:30,730 --> 00:08:33,070
also basic information on the stores and

194
00:08:33,070 --> 00:08:36,159
on our product category so in this case

195
00:08:36,159 --> 00:08:38,110
product categories is like a doggy leash

196
00:08:38,110 --> 00:08:40,839
or cat food or whatever you would buy in

197
00:08:40,839 --> 00:08:43,929
a pet store and then in SPARC

198
00:08:43,929 --> 00:08:46,570
you have basically two main ways of

199
00:08:46,570 --> 00:08:49,390
passing data to your distributed

200
00:08:49,390 --> 00:08:52,210
operators remember these will get this

201
00:08:52,210 --> 00:08:53,920
used in Anna clusters maybe you will

202
00:08:53,920 --> 00:08:56,440
have ten instances running and

203
00:08:56,440 --> 00:08:58,420
generating your data

204
00:08:58,420 --> 00:09:02,850
one is of course to paralyze some

205
00:09:02,850 --> 00:09:05,290
collection that you already have or read

206
00:09:05,290 --> 00:09:07,270
from a file so that's the standard data

207
00:09:07,270 --> 00:09:09,370
set that is going to flow through your

208
00:09:09,370 --> 00:09:11,560
pipeline but that's also an alternative

209
00:09:11,560 --> 00:09:14,470
if you have just a smaller data set that

210
00:09:14,470 --> 00:09:16,540
you would like to propagate to all of

211
00:09:16,540 --> 00:09:19,720
your nodes and for example the stories

212
00:09:19,720 --> 00:09:22,210
is going to be such a data set in our

213
00:09:22,210 --> 00:09:25,330
example you can use sparks broadcast

214
00:09:25,330 --> 00:09:29,590
variable to do such such a thing then of

215
00:09:29,590 --> 00:09:32,890
the spark has this so-called functional

216
00:09:32,890 --> 00:09:36,310
API we already seen a lot of functional

217
00:09:36,310 --> 00:09:39,790
api's today in the display dev room and

218
00:09:39,790 --> 00:09:42,820
one of the main features there is you

219
00:09:42,820 --> 00:09:45,550
can you can use these functions for

220
00:09:45,550 --> 00:09:47,320
example a map function or reduce

221
00:09:47,320 --> 00:09:51,670
function or a flat map function and pass

222
00:09:51,670 --> 00:09:54,340
your user define functional if vivid in

223
00:09:54,340 --> 00:09:56,620
that you see one example here I just

224
00:09:56,620 --> 00:09:59,620
adjusted the time when we started

225
00:09:59,620 --> 00:10:03,370
generating the process that's that's

226
00:10:03,370 --> 00:10:05,860
basically the whole structure is how you

227
00:10:05,860 --> 00:10:08,580
would code the data generation in SPARC

228
00:10:08,580 --> 00:10:13,690
like let's switch to fling I also

229
00:10:13,690 --> 00:10:16,300
generate the grounds response changes in

230
00:10:16,300 --> 00:10:19,120
SPARC we called the basis of building

231
00:10:19,120 --> 00:10:21,520
this jog graph SPARC context now we call

232
00:10:21,520 --> 00:10:23,650
it execution environment of course

233
00:10:23,650 --> 00:10:26,520
relying on Ronald is the same and now

234
00:10:26,520 --> 00:10:29,620
it's slightly different the way how we

235
00:10:29,620 --> 00:10:31,960
they use extra information on our

236
00:10:31,960 --> 00:10:34,690
functions in Flint we use the Rick's

237
00:10:34,690 --> 00:10:36,970
trig function interface in SPARC we use

238
00:10:36,970 --> 00:10:39,490
another solution for that it's slightly

239
00:10:39,490 --> 00:10:41,920
different how we use the broadcast

240
00:10:41,920 --> 00:10:44,620
variables but as you see the basics

241
00:10:44,620 --> 00:10:47,410
having a mapping is is on the same level

242
00:10:47,410 --> 00:10:52,180
it's mapped out the same and that this

243
00:10:52,180 --> 00:10:53,800
is just a summary of what I've already

244
00:10:53,800 --> 00:10:58,000
said so we have already written our JSON

245
00:10:58,000 --> 00:11:00,100
output file so it this would be written

246
00:11:00,100 --> 00:11:03,190
in Jason just the sake of having able to

247
00:11:03,190 --> 00:11:06,010
look at it and let's do a little bit of

248
00:11:06,010 --> 00:11:08,290
ETL on top of the data that we already

249
00:11:08,290 --> 00:11:11,050
have so of course this is usually

250
00:11:11,050 --> 00:11:12,100
variable

251
00:11:12,100 --> 00:11:14,829
with such a process because some someone

252
00:11:14,829 --> 00:11:18,940
has already provided for you so how can

253
00:11:18,940 --> 00:11:22,920
we be will let's have two goals for this

254
00:11:22,920 --> 00:11:26,980
first first of all we we would like to

255
00:11:26,980 --> 00:11:29,380
feed this data into a recommender system

256
00:11:29,380 --> 00:11:31,810
eventually so we will we would need a

257
00:11:31,810 --> 00:11:33,880
customer at product pairs where a

258
00:11:33,880 --> 00:11:35,920
customer has purchased the given product

259
00:11:35,920 --> 00:11:37,600
that but the format that we have in the

260
00:11:37,600 --> 00:11:40,269
JSON SS that this given customer

261
00:11:40,269 --> 00:11:43,540
generated a transaction that has these

262
00:11:43,540 --> 00:11:46,560
ten products in it so we would like to

263
00:11:46,560 --> 00:11:50,279
get into this puffle format instant

264
00:11:50,279 --> 00:11:52,750
first thing first we don't really have

265
00:11:52,750 --> 00:11:56,139
an ID with our products first yet we

266
00:11:56,139 --> 00:11:57,519
only have that for the product

267
00:11:57,519 --> 00:12:00,100
categories so we could select the

268
00:12:00,100 --> 00:12:02,290
distinct of those and zip them with the

269
00:12:02,290 --> 00:12:04,449
unique ID these are all spark functions

270
00:12:04,449 --> 00:12:07,660
already included you can access it from

271
00:12:07,660 --> 00:12:13,060
the standard course part and then you

272
00:12:13,060 --> 00:12:15,910
joined this information together with

273
00:12:15,910 --> 00:12:18,370
the customers who already have an ID and

274
00:12:18,370 --> 00:12:21,670
then you have two IDs here this is just

275
00:12:21,670 --> 00:12:23,980
some basic mapping and the join and a

276
00:12:23,980 --> 00:12:25,750
dis thing and what's really interesting

277
00:12:25,750 --> 00:12:28,420
these are this is the batch API if I

278
00:12:28,420 --> 00:12:31,480
switch to flank practically not nothing

279
00:12:31,480 --> 00:12:33,910
changes the only part that this is

280
00:12:33,910 --> 00:12:35,889
exactly the same the only part that

281
00:12:35,889 --> 00:12:39,040
changes here is the way spark and fling

282
00:12:39,040 --> 00:12:42,189
like to speak about their day join and

283
00:12:42,189 --> 00:12:45,100
basically the signature is of the join

284
00:12:45,100 --> 00:12:49,689
but basically it's the same and the way

285
00:12:49,689 --> 00:12:52,720
they handle keys so if you could almost

286
00:12:52,720 --> 00:12:55,509
recompile it with which blink instead of

287
00:12:55,509 --> 00:12:57,759
spark and the other way you learn that

288
00:12:57,759 --> 00:13:00,490
so this keys is is the notion of the

289
00:13:00,490 --> 00:13:06,579
pyro RDD okay some fun next step we have

290
00:13:06,579 --> 00:13:10,079
already heard this too today from many

291
00:13:10,079 --> 00:13:12,939
speakers that it's not necessarily

292
00:13:12,939 --> 00:13:15,250
enough to provide the Java and Scala API

293
00:13:15,250 --> 00:13:19,029
because other people who have very good

294
00:13:19,029 --> 00:13:21,130
previous systems they may or may be used

295
00:13:21,130 --> 00:13:24,010
to sequel or R or Python so why not

296
00:13:24,010 --> 00:13:25,730
provide an interface for

297
00:13:25,730 --> 00:13:29,209
as well and both systems give you signal

298
00:13:29,209 --> 00:13:33,620
interfaces of course they usually they

299
00:13:33,620 --> 00:13:36,260
have the support standard that's a

300
00:13:36,260 --> 00:13:39,350
little older so I think SPARC currently

301
00:13:39,350 --> 00:13:43,130
supports with spark to the sequel mm

302
00:13:43,130 --> 00:13:46,130
sander the flink is on the sequel 92

303
00:13:46,130 --> 00:13:49,639
standard currently so antsy well in both

304
00:13:49,639 --> 00:13:51,860
cases but they are catching up and

305
00:13:51,860 --> 00:13:53,720
making sure you that you can also have

306
00:13:53,720 --> 00:13:56,380
complex queries queries on top of these

307
00:13:56,380 --> 00:13:59,360
so for example here in this query I

308
00:13:59,360 --> 00:14:00,980
would like to have a look at the data

309
00:14:00,980 --> 00:14:03,709
and select the stores that have the most

310
00:14:03,709 --> 00:14:06,440
transactions and you can achieve that

311
00:14:06,440 --> 00:14:10,639
with 2 sequel statement but one of the

312
00:14:10,639 --> 00:14:13,940
downsides of heaviness using sequel is

313
00:14:13,940 --> 00:14:16,370
you sort of lose the type of information

314
00:14:16,370 --> 00:14:19,880
that you have in your data because when

315
00:14:19,880 --> 00:14:23,329
you look you just type this declarative

316
00:14:23,329 --> 00:14:26,389
query as a string and you lose the

317
00:14:26,389 --> 00:14:29,389
information that actually this ID was a

318
00:14:29,389 --> 00:14:32,329
string or or this was an int and this

319
00:14:32,329 --> 00:14:34,970
name was a string so if you also want to

320
00:14:34,970 --> 00:14:37,880
have that then you can use the table or

321
00:14:37,880 --> 00:14:41,779
or data frames api's we also heard today

322
00:14:41,779 --> 00:14:44,930
that it's it's very difficult to write a

323
00:14:44,930 --> 00:14:49,029
UDF in hive for example and there

324
00:14:49,029 --> 00:14:52,459
sparkin flink I think give you very nice

325
00:14:52,459 --> 00:14:56,389
solutions in terms of writing UDF for

326
00:14:56,389 --> 00:14:58,339
example I would like to register this

327
00:14:58,339 --> 00:15:00,139
month's function which is a scalar

328
00:15:00,139 --> 00:15:03,560
function so that I can use it in my

329
00:15:03,560 --> 00:15:06,199
sequel statement this is literally this

330
00:15:06,199 --> 00:15:08,269
much code you write the scalar function

331
00:15:08,269 --> 00:15:10,880
you call register and then here you can

332
00:15:10,880 --> 00:15:13,130
use it so I think it's it's super

333
00:15:13,130 --> 00:15:15,800
convenient it it gets propagated to your

334
00:15:15,800 --> 00:15:18,980
working nodes the NOI already refers to

335
00:15:18,980 --> 00:15:23,060
the table API soon I the sequel I showed

336
00:15:23,060 --> 00:15:25,339
you in spark now I'm going to show the

337
00:15:25,339 --> 00:15:27,800
table or data frames API in flink both

338
00:15:27,800 --> 00:15:29,630
have actually both we just don't have

339
00:15:29,630 --> 00:15:32,360
enough time to visit everything so this

340
00:15:32,360 --> 00:15:34,850
is very pretty close to sequel it's

341
00:15:34,850 --> 00:15:37,850
still Scala it feels like sequel so the

342
00:15:37,850 --> 00:15:39,530
statements that you write is

343
00:15:39,530 --> 00:15:42,050
tables of group buy and select and join

344
00:15:42,050 --> 00:15:45,050
and very but you are still in Scala and

345
00:15:45,050 --> 00:15:47,090
you still have the type information and

346
00:15:47,090 --> 00:15:49,280
it's and if you want to go back to

347
00:15:49,280 --> 00:15:52,160
standard flink or standard it's

348
00:15:52,160 --> 00:15:55,730
very easier and here I complete the same

349
00:15:55,730 --> 00:16:00,610
let's select the most important stories

350
00:16:01,090 --> 00:16:05,990
ok and other topic is finally we

351
00:16:05,990 --> 00:16:08,510
realized than this customer and and

352
00:16:08,510 --> 00:16:10,850
product rares we would like to feed it

353
00:16:10,850 --> 00:16:12,560
to a machine learning system and if you

354
00:16:12,560 --> 00:16:15,140
are familiar with like it learned and

355
00:16:15,140 --> 00:16:17,810
the API should look pretty similar for

356
00:16:17,810 --> 00:16:22,700
you basically they have this estimator

357
00:16:22,700 --> 00:16:25,760
predictor API that's that's very

358
00:16:25,760 --> 00:16:28,640
familiar but from scikit-learn you so

359
00:16:28,640 --> 00:16:30,980
you have a model you call fit on it and

360
00:16:30,980 --> 00:16:33,920
then you can predict with the testing

361
00:16:33,920 --> 00:16:37,010
data set so that's that's what we would

362
00:16:37,010 --> 00:16:39,550
we would be able to call here the

363
00:16:39,550 --> 00:16:42,110
recommender implementation that we are

364
00:16:42,110 --> 00:16:45,110
using here is called ALS but there are

365
00:16:45,110 --> 00:16:47,270
also other solutions to to solve this

366
00:16:47,270 --> 00:16:50,180
problem instead of predicting it right

367
00:16:50,180 --> 00:16:52,940
here I'm just saving the model and we

368
00:16:52,940 --> 00:16:54,680
will use it later in in a streaming

369
00:16:54,680 --> 00:16:57,260
topology so right now with this much

370
00:16:57,260 --> 00:16:59,150
code and this is actually how much code

371
00:16:59,150 --> 00:17:02,000
you have to write in in ml lib it will

372
00:17:02,000 --> 00:17:04,339
be very similar in string canal you can

373
00:17:04,339 --> 00:17:06,650
train a machine learning model and save

374
00:17:06,650 --> 00:17:10,130
it and then we can reuse it later in

375
00:17:10,130 --> 00:17:13,699
flink it would be this much code this is

376
00:17:13,699 --> 00:17:15,829
really how much you would have to write

377
00:17:15,829 --> 00:17:18,280
there okay

378
00:17:18,280 --> 00:17:21,530
in terms of contouring the ML API is the

379
00:17:21,530 --> 00:17:24,380
spark on is definitely more mature and

380
00:17:24,380 --> 00:17:27,109
what API level the well they are almost

381
00:17:27,109 --> 00:17:30,170
as close as the batch API sorry so I

382
00:17:30,170 --> 00:17:33,380
think in given given that the fling

383
00:17:33,380 --> 00:17:35,330
phone catches up with algorithms it will

384
00:17:35,330 --> 00:17:37,070
be really easy to to switch between

385
00:17:37,070 --> 00:17:40,460
those and then let's play a little bit

386
00:17:40,460 --> 00:17:42,890
with streaming so we have worried they

387
00:17:42,890 --> 00:17:44,600
have a machine learning model and the

388
00:17:44,600 --> 00:17:47,929
tricky part there is usually when you

389
00:17:47,929 --> 00:17:50,570
have a recommender system model then the

390
00:17:50,570 --> 00:17:53,030
problem that you are trying to solve is

391
00:17:53,030 --> 00:17:56,030
I have this user he have watched five

392
00:17:56,030 --> 00:17:58,160
videos on my website and I would like to

393
00:17:58,160 --> 00:18:00,290
give recommendations so for three more

394
00:18:00,290 --> 00:18:01,970
well if you don't give you those

395
00:18:01,970 --> 00:18:04,550
recommendations in approximate with 200

396
00:18:04,550 --> 00:18:06,650
milliseconds the user is gone so they

397
00:18:06,650 --> 00:18:08,750
are worthless that's why we need

398
00:18:08,750 --> 00:18:13,130
streaming or near real-time processing

399
00:18:13,130 --> 00:18:16,250
and as I mentioned in spike it's really

400
00:18:16,250 --> 00:18:18,710
easy to accomplish such a thing because

401
00:18:18,710 --> 00:18:21,410
we just load the model that we have

402
00:18:21,410 --> 00:18:23,780
already prepared in batch and then we

403
00:18:23,780 --> 00:18:26,510
will we have a query which is coming

404
00:18:26,510 --> 00:18:29,750
from a socket I am just typing in user

405
00:18:29,750 --> 00:18:33,080
IDs and then I would like to to do the

406
00:18:33,080 --> 00:18:35,680
prediction I go back to my batch

407
00:18:35,680 --> 00:18:38,120
pipeline and use the prediction there

408
00:18:38,120 --> 00:18:39,980
this is something that is currently not

409
00:18:39,980 --> 00:18:42,050
available in flink and it's super

410
00:18:42,050 --> 00:18:45,620
convenient to code in spike of course

411
00:18:45,620 --> 00:18:47,420
you have other trade-offs in inspired

412
00:18:47,420 --> 00:18:51,440
and just to give you a sense of how to

413
00:18:51,440 --> 00:18:54,380
switch to a Java API this is actually

414
00:18:54,380 --> 00:18:56,090
coded in Java of course you don't see

415
00:18:56,090 --> 00:18:57,740
much difference the only difference is

416
00:18:57,740 --> 00:19:01,280
the semicolon here because almost

417
00:19:01,280 --> 00:19:03,890
already looks like Scala but in flink I

418
00:19:03,890 --> 00:19:06,770
decided and the code is available on

419
00:19:06,770 --> 00:19:09,650
github I decided to code the whole

420
00:19:09,650 --> 00:19:12,940
prediction instead of relying on the

421
00:19:12,940 --> 00:19:15,950
implementation in flink ml I coded it in

422
00:19:15,950 --> 00:19:17,780
in streaming which is because it's very

423
00:19:17,780 --> 00:19:20,300
straightforward this just multiplying a

424
00:19:20,300 --> 00:19:22,070
matrix with the vector and then

425
00:19:22,070 --> 00:19:26,900
selecting the top K yeah in terms of

426
00:19:26,900 --> 00:19:29,330
differences I think this is the part

427
00:19:29,330 --> 00:19:31,160
where spark and flink are the most

428
00:19:31,160 --> 00:19:33,590
difference because of the choice of

429
00:19:33,590 --> 00:19:36,410
architecture fling bus definitely

430
00:19:36,410 --> 00:19:37,880
barrier when it comes to stateful

431
00:19:37,880 --> 00:19:40,820
processing and timeliness what spark is

432
00:19:40,820 --> 00:19:44,960
improving in that department with with

433
00:19:44,960 --> 00:19:47,840
the new structure streaming API so as a

434
00:19:47,840 --> 00:19:51,170
summary you should definitely go beyond

435
00:19:51,170 --> 00:19:53,960
word count with the big bad story the

436
00:19:53,960 --> 00:19:56,060
batch concepts of spark and slink are

437
00:19:56,060 --> 00:19:58,400
very close but they have the differences

438
00:19:58,400 --> 00:20:02,180
in streaming you have seen a lot of use

439
00:20:02,180 --> 00:20:04,610
cases with under five hundred five

440
00:20:04,610 --> 00:20:06,220
hundred lines of code and it's

441
00:20:06,220 --> 00:20:07,960
available on github and you are very

442
00:20:07,960 --> 00:20:10,419
welcome to check it out and of course an

443
00:20:10,419 --> 00:20:14,409
Apache pet project is always fun big

444
00:20:14,409 --> 00:20:16,809
thanks to the guys who have helped me to

445
00:20:16,809 --> 00:20:19,510
accomplish the project itself and thank

446
00:20:19,510 --> 00:21:01,570
you very much yes so basic and yes so

447
00:21:01,570 --> 00:21:05,350
the question was how does spark and

448
00:21:05,350 --> 00:21:07,450
fling come together in Apache beam and

449
00:21:07,450 --> 00:21:12,070
and what's the future of Apache being in

450
00:21:12,070 --> 00:21:14,470
my understanding I think Apogee beam is

451
00:21:14,470 --> 00:21:16,480
a very interesting project in the sense

452
00:21:16,480 --> 00:21:20,440
that the Google guys who published the

453
00:21:20,440 --> 00:21:24,700
data flow paper which was the driving

454
00:21:24,700 --> 00:21:28,809
factor behind beam really got the

455
00:21:28,809 --> 00:21:30,970
timeliness of streaming application

456
00:21:30,970 --> 00:21:35,610
rights so their model there is it really

457
00:21:37,169 --> 00:21:39,850
goes into details in how to handle

458
00:21:39,850 --> 00:21:41,710
timeliness and late events and and

459
00:21:41,710 --> 00:21:44,230
that's one of the main powers of data

460
00:21:44,230 --> 00:21:47,919
flow and I think that if you look at the

461
00:21:47,919 --> 00:21:49,900
the spark and fling Krannert

462
00:21:49,900 --> 00:21:52,720
implementations currently none of this

463
00:21:52,720 --> 00:21:55,780
the system's can support all of the

464
00:21:55,780 --> 00:21:57,880
features of data flow but flink is much

465
00:21:57,880 --> 00:22:02,650
closer currently and the basic reason

466
00:22:02,650 --> 00:22:05,950
why spark is currently not supporting

467
00:22:05,950 --> 00:22:08,350
all of these streaming features because

468
00:22:08,350 --> 00:22:10,809
in spark because of this architectural

469
00:22:10,809 --> 00:22:13,000
choice for trance was very easy to

470
00:22:13,000 --> 00:22:14,649
accomplish because it was done batch

471
00:22:14,649 --> 00:22:17,919
fortress it gives it's done already but

472
00:22:17,919 --> 00:22:21,789
when you have this mini-batches so then

473
00:22:21,789 --> 00:22:23,559
it's really you don't really have a way

474
00:22:23,559 --> 00:22:25,390
to communicate between two mini-batch

475
00:22:25,390 --> 00:22:29,020
that's that's the main issue so that was

476
00:22:29,020 --> 00:22:31,510
one of the driving forces by a spark is

477
00:22:31,510 --> 00:22:33,640
coming out with a new streaming API this

478
00:22:33,640 --> 00:22:37,320
structure streaming to make up for a

479
00:22:37,320 --> 00:22:40,720
couple of these semantic differences

480
00:22:40,720 --> 00:22:43,600
between the watt spark streaming and and

481
00:22:43,600 --> 00:22:45,940
fling core data flow so it's definitely

482
00:22:45,940 --> 00:22:49,960
getting there but spark has to improve

483
00:22:49,960 --> 00:23:00,490
in terms of semantics yes please I think

484
00:23:00,490 --> 00:23:03,549
natalie is is a little bit in a

485
00:23:03,549 --> 00:23:05,950
different domain but I would like to

486
00:23:05,950 --> 00:23:13,299
learn more about med live as well thank

487
00:23:13,299 --> 00:23:14,540
you very much

488
00:23:14,540 --> 00:23:22,629
[Applause]

