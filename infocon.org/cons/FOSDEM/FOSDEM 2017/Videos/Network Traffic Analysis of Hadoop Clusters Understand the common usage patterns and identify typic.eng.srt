1
00:00:00,030 --> 00:00:02,719
about today

2
00:00:04,630 --> 00:00:06,870
have you looked at something practical

3
00:00:06,870 --> 00:00:11,620
which is kind of and I think if you were

4
00:00:11,620 --> 00:00:14,920
there for Frank's talk network traffic

5
00:00:14,920 --> 00:00:16,600
analytics is something that's quite

6
00:00:16,600 --> 00:00:18,220
important getting more importantly more

7
00:00:18,220 --> 00:00:20,350
like cybertek's weekend and stuff like

8
00:00:20,350 --> 00:00:20,830
that

9
00:00:20,830 --> 00:00:23,530
intrusions and so I welcome Martin and

10
00:00:23,530 --> 00:00:27,250
Mirko from Korea to talk about that how

11
00:00:27,250 --> 00:00:28,800
you would do that on the Hadoop clusters

12
00:00:28,800 --> 00:00:36,760
thank you thanks everyone thanks for

13
00:00:36,760 --> 00:00:39,370
staying for the last talk of this day

14
00:00:39,370 --> 00:00:42,999
room that mean yeah I think it's on it's

15
00:00:42,999 --> 00:00:47,620
green okay so my colleague is Mira and

16
00:00:47,620 --> 00:00:50,139
I'm Martin and we both work for one of

17
00:00:50,139 --> 00:00:52,479
the Hadoop lenders namely cloud area and

18
00:00:52,479 --> 00:00:54,969
we both work with customers solution

19
00:00:54,969 --> 00:00:57,129
architect basically means that we go out

20
00:00:57,129 --> 00:00:58,960
on site with the customer and try to

21
00:00:58,960 --> 00:01:01,659
help them to learn Hadoop to adapt

22
00:01:01,659 --> 00:01:03,429
Hadoop and bring more and more we're

23
00:01:03,429 --> 00:01:06,159
close to Hadoop and it's a very general

24
00:01:06,159 --> 00:01:08,740
question that we run into customers

25
00:01:08,740 --> 00:01:12,939
trying to have asked for help to

26
00:01:12,939 --> 00:01:14,920
understand the system that what does it

27
00:01:14,920 --> 00:01:17,770
mean that they are used to being one

28
00:01:17,770 --> 00:01:19,930
maybe the monolithic application are

29
00:01:19,930 --> 00:01:22,899
used to having a database system and now

30
00:01:22,899 --> 00:01:25,210
this is this brave new world of Hadoop

31
00:01:25,210 --> 00:01:26,829
technologies where everything is

32
00:01:26,829 --> 00:01:29,140
distributed and your data is floating

33
00:01:29,140 --> 00:01:32,049
around in the cloud and how do I make

34
00:01:32,049 --> 00:01:35,020
sense of that so one of the ways to to

35
00:01:35,020 --> 00:01:37,509
make sense of it is actually concentrate

36
00:01:37,509 --> 00:01:39,789
on the bottlenecks and network traffic

37
00:01:39,789 --> 00:01:41,920
is very usually one of the battle necks

38
00:01:41,920 --> 00:01:46,649
of a Hadoop processing system so the we

39
00:01:46,649 --> 00:01:49,390
came up with a couple of ideas of how to

40
00:01:49,390 --> 00:01:52,210
visualize this for clients and also to

41
00:01:52,210 --> 00:01:55,210
make sure that we ourselves understand

42
00:01:55,210 --> 00:01:58,359
this very close parent so I will give

43
00:01:58,359 --> 00:01:59,979
you a little bit of the motivation of

44
00:01:59,979 --> 00:02:03,369
life how Hadoop works and what we would

45
00:02:03,369 --> 00:02:05,859
like to visualize here then we go into

46
00:02:05,859 --> 00:02:08,710
the packet capturing and how we capture

47
00:02:08,710 --> 00:02:12,040
the data we do a little bit of analytics

48
00:02:12,040 --> 00:02:15,130
with the cloud area Hadoop stack the CDH

49
00:02:15,130 --> 00:02:18,450
for short and the Gaffey to

50
00:02:18,450 --> 00:02:20,819
which is an Open Graph visualization

51
00:02:20,819 --> 00:02:25,080
platform and produced but and figure

52
00:02:25,080 --> 00:02:29,400
that you see on the side so let's have a

53
00:02:29,400 --> 00:02:32,400
look at the the network load of a Hadoop

54
00:02:32,400 --> 00:02:35,940
cluster because we have hundreds and in

55
00:02:35,940 --> 00:02:38,250
a couple of use cases thousands of

56
00:02:38,250 --> 00:02:42,150
machine in one single cluster simple

57
00:02:42,150 --> 00:02:45,030
operations change when you put a file

58
00:02:45,030 --> 00:02:47,790
onto a file system that most of the time

59
00:02:47,790 --> 00:02:50,250
because it's a distributed file system

60
00:02:50,250 --> 00:02:53,400
DFS for sure short includes some network

61
00:02:53,400 --> 00:02:55,920
operations usually these files can be so

62
00:02:55,920 --> 00:02:57,959
large that they are bigger than a hard

63
00:02:57,959 --> 00:03:00,690
disk in a single machine also you want

64
00:03:00,690 --> 00:03:03,480
replication for for turns so just

65
00:03:03,480 --> 00:03:05,220
putting something to the file system is

66
00:03:05,220 --> 00:03:07,200
a network operation of course getting

67
00:03:07,200 --> 00:03:09,450
something from the file system is also a

68
00:03:09,450 --> 00:03:11,670
network operation then you have to make

69
00:03:11,670 --> 00:03:13,829
sure that there is some service

70
00:03:13,829 --> 00:03:15,930
discovery parts of the system know about

71
00:03:15,930 --> 00:03:17,579
each other so you need some heart

72
00:03:17,579 --> 00:03:20,519
beating going on in the system as such I

73
00:03:20,519 --> 00:03:23,700
would say a passive noise going on still

74
00:03:23,700 --> 00:03:25,920
network traffic is being generated and

75
00:03:25,920 --> 00:03:28,200
of course data is not just sitting

76
00:03:28,200 --> 00:03:30,180
around in a cluster you actually do some

77
00:03:30,180 --> 00:03:32,430
stuff on the data transactions analytics

78
00:03:32,430 --> 00:03:35,519
you name it there again generating

79
00:03:35,519 --> 00:03:40,019
traffic and one of the not necessarily

80
00:03:40,019 --> 00:03:41,609
nice features that we discovered by

81
00:03:41,609 --> 00:03:43,680
trying to explain this to customers if

82
00:03:43,680 --> 00:03:45,510
you look at the standard how do

83
00:03:45,510 --> 00:03:48,870
monitoring tools today the metrics that

84
00:03:48,870 --> 00:03:51,180
you can see for for network utilization

85
00:03:51,180 --> 00:03:53,519
they are aggregated on the host level

86
00:03:53,519 --> 00:03:55,350
and a lot of times that doesn't give you

87
00:03:55,350 --> 00:03:58,230
the capability to to look at individual

88
00:03:58,230 --> 00:04:02,160
services so instead of I'm using 80% of

89
00:04:02,160 --> 00:04:04,170
the bandwidth maybe I'm gonna say that

90
00:04:04,170 --> 00:04:07,590
ok my DFS is using 20% my heartbeats are

91
00:04:07,590 --> 00:04:10,319
using 1% and my processing jobs of these

92
00:04:10,319 --> 00:04:13,319
3 workflows are using an additional 50%

93
00:04:13,319 --> 00:04:15,930
maybe that's what I would like to see in

94
00:04:15,930 --> 00:04:18,478
a in a setup like this and currently

95
00:04:18,478 --> 00:04:20,910
that's not what you necessarily get from

96
00:04:20,910 --> 00:04:24,570
these systems also in this diagram today

97
00:04:24,570 --> 00:04:27,570
you have heard about cyber security and

98
00:04:27,570 --> 00:04:31,830
there's an Apache project that we are

99
00:04:31,830 --> 00:04:34,020
familiar with called a purchase pot

100
00:04:34,020 --> 00:04:36,599
which is mostly pushed by Intel and

101
00:04:36,599 --> 00:04:38,789
Cloudera but it's open source assist

102
00:04:38,789 --> 00:04:41,039
it's an Apogee project for intrusion

103
00:04:41,039 --> 00:04:44,069
detection and we'll mention a little bit

104
00:04:44,069 --> 00:04:47,159
how it different from differ from their

105
00:04:47,159 --> 00:04:50,729
approach so let's move to the data that

106
00:04:50,729 --> 00:04:53,550
we are using we used standard packet

107
00:04:53,550 --> 00:04:57,360
capture data so this is the the library

108
00:04:57,360 --> 00:04:59,430
it's most implemented for UNIX and

109
00:04:59,430 --> 00:05:01,110
Windows systems that you are familiar

110
00:05:01,110 --> 00:05:04,199
with if you use the tools built on top

111
00:05:04,199 --> 00:05:07,949
of a TCP dump and map Wireshark snort

112
00:05:07,949 --> 00:05:11,159
you name it they all use this this basic

113
00:05:11,159 --> 00:05:14,669
implementation we used a Python hook for

114
00:05:14,669 --> 00:05:18,419
this but it's the same data but a couple

115
00:05:18,419 --> 00:05:22,229
of gotchas in in the data capturing of

116
00:05:22,229 --> 00:05:24,870
course we what we don't want to create

117
00:05:24,870 --> 00:05:27,030
additional noise by capturing the data

118
00:05:27,030 --> 00:05:30,389
so we capture it on the local machines

119
00:05:30,389 --> 00:05:33,180
and write it to local file system and we

120
00:05:33,180 --> 00:05:35,340
only gather it after the capturing has

121
00:05:35,340 --> 00:05:39,479
finished and we throw away the data in

122
00:05:39,479 --> 00:05:41,669
the packet itself we are only capturing

123
00:05:41,669 --> 00:05:43,770
the structure of the graph it's mostly

124
00:05:43,770 --> 00:05:47,639
for clustering and for visualization not

125
00:05:47,639 --> 00:05:50,310
necessarily learning from the data

126
00:05:50,310 --> 00:05:53,849
inside the packet itself on the contrary

127
00:05:53,849 --> 00:05:56,190
but the opposite spot guys focus on in

128
00:05:56,190 --> 00:05:59,520
terms of cybersecurity they would like

129
00:05:59,520 --> 00:06:03,719
to learn topic analysis on on the data

130
00:06:03,719 --> 00:06:06,750
itself inside the packets one of the

131
00:06:06,750 --> 00:06:11,219
most important approaches in include

132
00:06:11,219 --> 00:06:14,520
this and they would like to build a

133
00:06:14,520 --> 00:06:18,240
cybersecurity platform for Hadoop using

134
00:06:18,240 --> 00:06:21,120
Hadoop technologies instead we could

135
00:06:21,120 --> 00:06:22,560
focus on the clustering and the

136
00:06:22,560 --> 00:06:26,099
visualization as I've mentioned so we

137
00:06:26,099 --> 00:06:27,360
had to come up with a couple of

138
00:06:27,360 --> 00:06:33,270
scenarios and we examined five scenarios

139
00:06:33,270 --> 00:06:36,330
in detail but here we only present three

140
00:06:36,330 --> 00:06:38,400
of those because those three were more

141
00:06:38,400 --> 00:06:41,610
important in terms of results so we had

142
00:06:41,610 --> 00:06:43,740
a heavy batch word code called

143
00:06:43,740 --> 00:06:45,480
pterosaurs very

144
00:06:45,480 --> 00:06:47,850
need input from the distributed file

145
00:06:47,850 --> 00:06:52,080
system and we do a big distributed sort

146
00:06:52,080 --> 00:06:55,320
on the keys in this input and would like

147
00:06:55,320 --> 00:06:57,870
to eventually write the output to the

148
00:06:57,870 --> 00:07:01,110
distributed file system again we also

149
00:07:01,110 --> 00:07:03,750
captured the idle cluster so have an

150
00:07:03,750 --> 00:07:05,460
idea of this background noise and

151
00:07:05,460 --> 00:07:08,450
heartbeats and then we had a more

152
00:07:08,450 --> 00:07:12,870
interesting approach when we reached out

153
00:07:12,870 --> 00:07:15,660
to the Twitter streaming API with spark

154
00:07:15,660 --> 00:07:17,490
streaming and collected data from the

155
00:07:17,490 --> 00:07:19,710
outside and we wanted to see how that

156
00:07:19,710 --> 00:07:22,410
appears in a network like this and also

157
00:07:22,410 --> 00:07:25,260
we wrote to HDFS we could have written

158
00:07:25,260 --> 00:07:27,840
to Kafka or any other source of your

159
00:07:27,840 --> 00:07:32,820
choice so the vague currently it works

160
00:07:32,820 --> 00:07:35,370
it's it's semi automatic we call like

161
00:07:35,370 --> 00:07:38,850
data in the obvious format for which you

162
00:07:38,850 --> 00:07:40,620
had the Opera screamer in two slides

163
00:07:40,620 --> 00:07:46,260
before with this pika-pi script we

164
00:07:46,260 --> 00:07:50,720
transform the events into networks using

165
00:07:50,720 --> 00:07:53,730
hive which is a sequel API on top of

166
00:07:53,730 --> 00:07:55,890
Hadoop now we have migrating these

167
00:07:55,890 --> 00:07:58,350
workloads to spark because then it's

168
00:07:58,350 --> 00:08:00,990
easier to register UDF's and and makes

169
00:08:00,990 --> 00:08:02,190
our life easier

170
00:08:02,190 --> 00:08:04,860
but currently it's in hive and also we

171
00:08:04,860 --> 00:08:08,310
use Givi this open graph visualization

172
00:08:08,310 --> 00:08:12,230
platform to do the visualization layer

173
00:08:12,230 --> 00:08:15,540
so let's have some initial results to

174
00:08:15,540 --> 00:08:17,310
give you some idea of what we have

175
00:08:17,310 --> 00:08:22,380
managed to capture both the sides of the

176
00:08:22,380 --> 00:08:26,300
nodes and the bits of the edges

177
00:08:26,300 --> 00:08:29,550
correspond to the workload and in the

178
00:08:29,550 --> 00:08:31,860
tire sort benchmark what we we have seen

179
00:08:31,860 --> 00:08:36,240
is the five nodes of our cluster often

180
00:08:36,240 --> 00:08:37,919
we're of course very prominent because

181
00:08:37,919 --> 00:08:39,630
we were reading data from the

182
00:08:39,630 --> 00:08:41,820
distributed file system and the sorting

183
00:08:41,820 --> 00:08:45,480
also involves a big shuffle in between

184
00:08:45,480 --> 00:08:46,860
the nodes so of course that

185
00:08:46,860 --> 00:08:49,350
communication had to be really prominent

186
00:08:49,350 --> 00:08:52,710
but some other nodes were also included

187
00:08:52,710 --> 00:08:55,170
with just a little bit of traffic and of

188
00:08:55,170 --> 00:08:57,030
course it's it's important to examine

189
00:08:57,030 --> 00:08:58,529
those

190
00:08:58,529 --> 00:09:01,829
in the other case we run the Twitter

191
00:09:01,829 --> 00:09:04,469
collection on just one single node and

192
00:09:04,469 --> 00:09:07,019
of course that's that's the one of in

193
00:09:07,019 --> 00:09:09,139
the center of this large email and

194
00:09:09,139 --> 00:09:12,029
that's the one that is of course also

195
00:09:12,029 --> 00:09:14,159
connected to the other four nodes that

196
00:09:14,159 --> 00:09:15,589
are in the cluster

197
00:09:15,589 --> 00:09:18,089
however it's connected to a lot of

198
00:09:18,089 --> 00:09:20,819
unknown new hosts that we haven't seen

199
00:09:20,819 --> 00:09:22,709
in the cluster yeah those are the ones

200
00:09:22,709 --> 00:09:25,769
that are actually sending the Twitter

201
00:09:25,769 --> 00:09:29,309
data to our cluster feed that I would

202
00:09:29,309 --> 00:09:32,429
like to pass the microphone to Mirko he

203
00:09:32,429 --> 00:09:36,559
is going to go deeper with the analysis

204
00:09:49,669 --> 00:09:52,439
so after looking into this very

205
00:09:52,439 --> 00:09:55,079
high-level initial results he could see

206
00:09:55,079 --> 00:09:58,039
what we expected we see two very typical

207
00:09:58,039 --> 00:10:00,599
different topologies of the

208
00:10:00,599 --> 00:10:01,949
communication networks you see

209
00:10:01,949 --> 00:10:04,529
communication networks here on a host

210
00:10:04,529 --> 00:10:06,449
level which is pretty interesting as a

211
00:10:06,449 --> 00:10:08,429
starting point but what can we learn

212
00:10:08,429 --> 00:10:10,919
from here not much yet okay there's a

213
00:10:10,919 --> 00:10:12,839
central node because one node speaks to

214
00:10:12,839 --> 00:10:14,819
the outside world and delivers data as

215
00:10:14,819 --> 00:10:17,099
an ingestion tool we have an internal

216
00:10:17,099 --> 00:10:19,289
operation running on five notes in

217
00:10:19,289 --> 00:10:21,149
parallel okay that's cool that's not

218
00:10:21,149 --> 00:10:23,849
something which helps us a lot we have

219
00:10:23,849 --> 00:10:26,939
to look deeper we have to increase the

220
00:10:26,939 --> 00:10:29,909
resolution we must look into more

221
00:10:29,909 --> 00:10:30,509
teachers

222
00:10:30,509 --> 00:10:33,089
this means we at thought information not

223
00:10:33,089 --> 00:10:35,399
only host information as we did in the

224
00:10:35,399 --> 00:10:38,009
very first analysis results next thing

225
00:10:38,009 --> 00:10:40,919
we add a timestamp we track the time

226
00:10:40,919 --> 00:10:43,799
dependent graph not as a snapshot for

227
00:10:43,799 --> 00:10:45,899
the whole period we look into little

228
00:10:45,899 --> 00:10:47,819
time slices and make this whole thing

229
00:10:47,819 --> 00:10:50,039
more dynamic and this allows us to do

230
00:10:50,039 --> 00:10:52,470
another thing this allows us to connect

231
00:10:52,470 --> 00:10:55,439
to a time domain this means we can

232
00:10:55,439 --> 00:10:57,779
switch between graph analytics and time

233
00:10:57,779 --> 00:10:59,999
series analytics and in order to show

234
00:10:59,999 --> 00:11:02,909
something useful Givi will be our friend

235
00:11:02,909 --> 00:11:06,119
if he allows us to collect data with

236
00:11:06,119 --> 00:11:08,449
time resolution and give he plots and

237
00:11:08,449 --> 00:11:11,039
visualizes graphs with time resolution

238
00:11:11,039 --> 00:11:12,209
you will see in Excel

239
00:11:12,209 --> 00:11:15,360
later I mentioned to increase the

240
00:11:15,360 --> 00:11:17,309
resolution we did it here we added more

241
00:11:17,309 --> 00:11:19,679
details to the picture what can we see

242
00:11:19,679 --> 00:11:22,559
not much in a moment we see some classes

243
00:11:22,559 --> 00:11:25,410
some clouds of populace with the same

244
00:11:25,410 --> 00:11:27,269
color everything which is in the same

245
00:11:27,269 --> 00:11:30,839
color means here's a port available on a

246
00:11:30,839 --> 00:11:33,660
particular host by at this port some

247
00:11:33,660 --> 00:11:35,610
communication happened during our

248
00:11:35,610 --> 00:11:38,069
experiments so via reconstruct first

249
00:11:38,069 --> 00:11:40,860
esthetic graph which only shows which

250
00:11:40,860 --> 00:11:42,689
ports and hosts have been involved

251
00:11:42,689 --> 00:11:45,449
during a certain data capture period

252
00:11:45,449 --> 00:11:48,660
that's the first start and now we must

253
00:11:48,660 --> 00:11:50,759
prove the data a little bit do a better

254
00:11:50,759 --> 00:11:53,579
layout and do some statistics first

255
00:11:53,579 --> 00:11:55,679
thing in this cluster or in this graph

256
00:11:55,679 --> 00:11:59,279
now we have 1.5 thousand notes about

257
00:11:59,279 --> 00:12:02,459
three thousand edges and I realized that

258
00:12:02,459 --> 00:12:05,490
a Fiat behaved not well anymore if this

259
00:12:05,490 --> 00:12:07,800
number goes up to hundred thousand but

260
00:12:07,800 --> 00:12:09,749
capturing half an hour or an hour of

261
00:12:09,749 --> 00:12:13,319
data will explode the staff so we went

262
00:12:13,319 --> 00:12:16,230
to spark and use spark to us for

263
00:12:16,230 --> 00:12:19,199
analytics but finally we do whole

264
00:12:19,199 --> 00:12:21,809
visualization in gave him our whole

265
00:12:21,809 --> 00:12:23,790
visualization means we start with a

266
00:12:23,790 --> 00:12:25,980
static network which represents our

267
00:12:25,980 --> 00:12:28,350
infrastructure the colors here encode at

268
00:12:28,350 --> 00:12:31,410
the moment the hosts same color means

269
00:12:31,410 --> 00:12:36,569
ports on the same host the blick a black

270
00:12:36,569 --> 00:12:38,670
lines are our real communication

271
00:12:38,670 --> 00:12:40,980
channels whenever a package traveled

272
00:12:40,980 --> 00:12:43,920
from one port to another it gives us

273
00:12:43,920 --> 00:12:46,170
such a trace okay this looks good for

274
00:12:46,170 --> 00:12:48,929
the moment but we can do more we can now

275
00:12:48,929 --> 00:12:51,480
study the topology of this network this

276
00:12:51,480 --> 00:12:53,579
communication network changes over time

277
00:12:53,579 --> 00:12:56,189
so topological properties change over

278
00:12:56,189 --> 00:12:58,889
time very interesting so different notes

279
00:12:58,889 --> 00:13:00,990
become important depending on what

280
00:13:00,990 --> 00:13:03,389
measure we apply we can now tune the

281
00:13:03,389 --> 00:13:06,360
things like we need this is still a

282
00:13:06,360 --> 00:13:08,339
field where research has not delivered

283
00:13:08,339 --> 00:13:11,160
final answers yet we've give here and if

284
00:13:11,160 --> 00:13:13,529
spark we are able to calculate different

285
00:13:13,529 --> 00:13:15,509
topological properties the

286
00:13:15,509 --> 00:13:17,029
interpretation is still an open problem

287
00:13:17,029 --> 00:13:20,129
we don't go deeper here our goal is to

288
00:13:20,129 --> 00:13:23,309
track the whole system in the more

289
00:13:23,309 --> 00:13:25,139
specific way our initial question was

290
00:13:25,139 --> 00:13:25,939
how

291
00:13:25,939 --> 00:13:28,339
of the network traffic is related to

292
00:13:28,339 --> 00:13:31,369
HDFS how much is related to spark

293
00:13:31,369 --> 00:13:34,069
shuffle and salt or shuffle services and

294
00:13:34,069 --> 00:13:36,529
all these different questions arise if

295
00:13:36,529 --> 00:13:38,479
you want to tune the system so this

296
00:13:38,479 --> 00:13:40,279
topology does not answer this questions

297
00:13:40,279 --> 00:13:43,819
but we can see what most are very

298
00:13:43,819 --> 00:13:45,499
important and what are the dominant

299
00:13:45,499 --> 00:13:47,539
ports okay that's an interesting aspect

300
00:13:47,539 --> 00:13:50,359
but let's go away let's make this whole

301
00:13:50,359 --> 00:13:53,209
dynamic thing visible here on the left

302
00:13:53,209 --> 00:13:56,359
hand side you see dynamically changing

303
00:13:56,359 --> 00:13:59,299
connectivity links between hosts ok

304
00:13:59,299 --> 00:14:01,609
that's what we have seen on the right

305
00:14:01,609 --> 00:14:03,229
hand side you see a different

306
00:14:03,229 --> 00:14:05,779
representation of the same data it is

307
00:14:05,779 --> 00:14:08,239
now not grouped by host anymore

308
00:14:08,239 --> 00:14:12,470
such a color spanning area here such a

309
00:14:12,470 --> 00:14:14,899
cluster in the same color is now not a

310
00:14:14,899 --> 00:14:17,359
host it's a subsystem it's for example

311
00:14:17,359 --> 00:14:19,729
HBase or it could be a zookeeper

312
00:14:19,729 --> 00:14:22,189
or it could be net reduce or whatever

313
00:14:22,189 --> 00:14:24,379
depending on what software you deploy on

314
00:14:24,379 --> 00:14:26,359
your head to cluster you may end up with

315
00:14:26,359 --> 00:14:29,119
a very different set up of such clusters

316
00:14:29,119 --> 00:14:32,329
and you don't usually not know what it

317
00:14:32,329 --> 00:14:34,789
is up front the correct data without

318
00:14:34,789 --> 00:14:37,339
knowing that such clusters even exist

319
00:14:37,339 --> 00:14:39,799
our analysis procedure highlights this

320
00:14:39,799 --> 00:14:42,169
and finds them out automatically so this

321
00:14:42,169 --> 00:14:43,849
is how we do it we go from a host

322
00:14:43,849 --> 00:14:48,319
centric representation to a layer or

323
00:14:48,319 --> 00:14:51,499
subsystem centric representation we turn

324
00:14:51,499 --> 00:14:52,249
things around

325
00:14:52,249 --> 00:14:54,229
this means we track the graph over time

326
00:14:54,229 --> 00:14:57,829
now we apply such an component or

327
00:14:57,829 --> 00:15:00,049
cluster detection algorithm and this

328
00:15:00,049 --> 00:15:01,720
cluster detection algorithm identifies

329
00:15:01,720 --> 00:15:05,329
such usually isolated clusters because

330
00:15:05,329 --> 00:15:08,179
these subsystems are usually well

331
00:15:08,179 --> 00:15:10,879
isolated in a hadoop system depending on

332
00:15:10,879 --> 00:15:12,919
your real environment this could be

333
00:15:12,919 --> 00:15:14,329
different that clusters could be

334
00:15:14,329 --> 00:15:16,629
interconnected in our case they are not

335
00:15:16,629 --> 00:15:19,849
so finally if we have such clusters

336
00:15:19,849 --> 00:15:23,209
where the different ports can be on very

337
00:15:23,209 --> 00:15:25,669
different hosts it makes sense to

338
00:15:25,669 --> 00:15:28,579
aggregate along these clusters not along

339
00:15:28,579 --> 00:15:30,979
a host anymore we aggregate now per

340
00:15:30,979 --> 00:15:33,679
subsystem and get some time series and

341
00:15:33,679 --> 00:15:36,079
with this time series we can say how

342
00:15:36,079 --> 00:15:39,540
much is there a correlation between

343
00:15:39,540 --> 00:15:42,720
HDFS and HBase are MapReduce and HDFS

344
00:15:42,720 --> 00:15:44,639
and so on this means if we change the

345
00:15:44,639 --> 00:15:46,860
perspective and aggregate along this new

346
00:15:46,860 --> 00:15:49,410
dimensions we end up with time series

347
00:15:49,410 --> 00:15:54,449
and we can look deeper into the behavior

348
00:15:54,449 --> 00:15:56,819
a final remark here this component

349
00:15:56,819 --> 00:15:59,399
centric view is very helpful it allows

350
00:15:59,399 --> 00:16:01,740
us to look again into the topology but

351
00:16:01,740 --> 00:16:04,019
be careful absolute numbers and data

352
00:16:04,019 --> 00:16:07,920
graph are really dangerous this numbers

353
00:16:07,920 --> 00:16:10,920
here this high degree or this high this

354
00:16:10,920 --> 00:16:13,800
huge circle which is used here and here

355
00:16:13,800 --> 00:16:15,959
they are totally misleading you cannot

356
00:16:15,959 --> 00:16:17,730
conclude from this picture that these

357
00:16:17,730 --> 00:16:20,009
both are the most important ones this is

358
00:16:20,009 --> 00:16:22,110
an artist in artificial effect of the

359
00:16:22,110 --> 00:16:24,269
analysis procedure just as a warning be

360
00:16:24,269 --> 00:16:26,459
careful with this always think about the

361
00:16:26,459 --> 00:16:28,949
right normalization usually you must

362
00:16:28,949 --> 00:16:30,690
look deeper into your data to get this

363
00:16:30,690 --> 00:16:33,980
but such pictures help you in order to

364
00:16:33,980 --> 00:16:37,560
get central and D central stuff by using

365
00:16:37,560 --> 00:16:39,540
a different layout algorithm it depends

366
00:16:39,540 --> 00:16:41,639
all on your layout algorithm how these

367
00:16:41,639 --> 00:16:43,470
pictures look like and if you want to

368
00:16:43,470 --> 00:16:44,880
cheat you just have to change the

369
00:16:44,880 --> 00:16:47,370
algorithm for layouting depending on the

370
00:16:47,370 --> 00:16:49,500
layout you can make the graph speaking

371
00:16:49,500 --> 00:16:51,720
that's all it's powerful but also

372
00:16:51,720 --> 00:16:53,970
dangerous you must be careful here again

373
00:16:53,970 --> 00:16:56,610
you switch the domain we go from x here

374
00:16:56,610 --> 00:16:59,220
we go from graph to x 0s what we see

375
00:16:59,220 --> 00:17:01,529
here is the overall activity during the

376
00:17:01,529 --> 00:17:04,079
MapReduce job and during the Twitter and

377
00:17:04,079 --> 00:17:07,980
just job both jobs they're running and

378
00:17:07,980 --> 00:17:10,409
you see what we do inside the cluster

379
00:17:10,409 --> 00:17:13,470
has not much heavy traffic but during

380
00:17:13,470 --> 00:17:15,599
data ingestion we record a lot of

381
00:17:15,599 --> 00:17:17,599
activity but we don't know which

382
00:17:17,599 --> 00:17:19,859
subsystem is really causing this

383
00:17:19,859 --> 00:17:23,130
activity this is why via group the stuff

384
00:17:23,130 --> 00:17:26,369
by subsystem and we come up with one

385
00:17:26,369 --> 00:17:28,140
time series which represents the number

386
00:17:28,140 --> 00:17:30,990
of packets per time interval for each

387
00:17:30,990 --> 00:17:33,780
individual subsystem starting with H DSS

388
00:17:33,780 --> 00:17:36,059
with HDFS where the name node is here

389
00:17:36,059 --> 00:17:38,610
just as an example used EF the node

390
00:17:38,610 --> 00:17:40,950
manager which organizes the workload in

391
00:17:40,950 --> 00:17:43,890
the cluster in red and blue black and

392
00:17:43,890 --> 00:17:47,299
yellow are communications related to the

393
00:17:47,299 --> 00:17:50,429
job management system so we can clearly

394
00:17:50,429 --> 00:17:53,220
separate this time series and see

395
00:17:53,220 --> 00:17:56,820
some effects for Hadoop behavior

396
00:17:56,820 --> 00:18:00,030
analytics you can figure out which kind

397
00:18:00,030 --> 00:18:03,630
of application has a lot of internal

398
00:18:03,630 --> 00:18:05,970
traffic and which not and if you look in

399
00:18:05,970 --> 00:18:08,370
this picture here we see an average the

400
00:18:08,370 --> 00:18:09,810
activity on the name node is much

401
00:18:09,810 --> 00:18:10,560
smaller

402
00:18:10,560 --> 00:18:13,110
we have no blue yellow or black one here

403
00:18:13,110 --> 00:18:15,480
this means on this layer there is no

404
00:18:15,480 --> 00:18:17,670
activity going on but what a surprise

405
00:18:17,670 --> 00:18:20,100
this job is of the same type like this

406
00:18:20,100 --> 00:18:23,070
jobs we can see here so here a result of

407
00:18:23,070 --> 00:18:26,820
system tuning in the old days the F lot

408
00:18:26,820 --> 00:18:29,340
of internal traffic after tuning the

409
00:18:29,340 --> 00:18:31,590
system in a specific way we could reduce

410
00:18:31,590 --> 00:18:34,260
this intermediate traffic and we could

411
00:18:34,260 --> 00:18:37,350
avoid the bottleneck that's one result

412
00:18:37,350 --> 00:18:39,930
of using this method so we are the very

413
00:18:39,930 --> 00:18:42,420
first experiments we have to at the

414
00:18:42,420 --> 00:18:44,580
moment we have the idle plaster where

415
00:18:44,580 --> 00:18:46,710
only the background noise is visible we

416
00:18:46,710 --> 00:18:48,900
have heavy ingest activity and we can

417
00:18:48,900 --> 00:18:51,450
clearly see our method allows us to

418
00:18:51,450 --> 00:18:54,360
segregate this ingestion activity from

419
00:18:54,360 --> 00:18:57,390
all the background noise so we can split

420
00:18:57,390 --> 00:18:59,670
multiple channels like you would do in

421
00:18:59,670 --> 00:19:02,520
an eg there you use frequency during

422
00:19:02,520 --> 00:19:06,870
during data collection so we can observe

423
00:19:06,870 --> 00:19:09,390
that we have either one very active

424
00:19:09,390 --> 00:19:11,730
component or we can see we have multiple

425
00:19:11,730 --> 00:19:14,220
competing component multiple channels

426
00:19:14,220 --> 00:19:17,730
yarn all this depends on your agrifim it

427
00:19:17,730 --> 00:19:20,040
depends on your workload the normal ETA

428
00:19:20,040 --> 00:19:22,050
cluster would behave different when a

429
00:19:22,050 --> 00:19:24,540
graph processing cluster but with this

430
00:19:24,540 --> 00:19:27,000
kind of measurement approach we could

431
00:19:27,000 --> 00:19:29,070
measure the things and then we have at

432
00:19:29,070 --> 00:19:31,470
least some benchmarks and we can start

433
00:19:31,470 --> 00:19:34,710
tuning the stuff so finally we have to

434
00:19:34,710 --> 00:19:35,850
think what's coming next

435
00:19:35,850 --> 00:19:37,560
first of all we must improve the

436
00:19:37,560 --> 00:19:39,720
experiments this means we have to run

437
00:19:39,720 --> 00:19:41,430
more experience we have to add more

438
00:19:41,430 --> 00:19:43,950
realistic workloads for example flink

439
00:19:43,950 --> 00:19:46,740
streaming Twitter's a spark streaming is

440
00:19:46,740 --> 00:19:47,700
not done yet

441
00:19:47,700 --> 00:19:50,880
Impala queries or even heavy HBase use

442
00:19:50,880 --> 00:19:52,950
cases or even solo based search use

443
00:19:52,950 --> 00:19:55,050
cases have not been tested yet whether

444
00:19:55,050 --> 00:19:57,090
at all this - our again that for our

445
00:19:57,090 --> 00:20:00,030
scenario list and then we come up with a

446
00:20:00,030 --> 00:20:02,280
different kind of visualization inspired

447
00:20:02,280 --> 00:20:03,960
by the talk in the morning where the

448
00:20:03,960 --> 00:20:06,960
Twitter real-time yeah visualization

449
00:20:06,960 --> 00:20:09,270
was shown we concluded okay it makes

450
00:20:09,270 --> 00:20:11,640
sense to have this for this measurement

451
00:20:11,640 --> 00:20:14,429
as well so we can measure strange

452
00:20:14,429 --> 00:20:17,309
behavior or suspicious connections and

453
00:20:17,309 --> 00:20:18,840
if we find such if we have a

454
00:20:18,840 --> 00:20:21,059
classification for our traffic then we

455
00:20:21,059 --> 00:20:23,429
could visualize it using gay fiestas and

456
00:20:23,429 --> 00:20:26,669
give here together with spark looks here

457
00:20:26,669 --> 00:20:29,130
with our giphy Hadoop connector and then

458
00:20:29,130 --> 00:20:31,559
after we have settled this experiment

459
00:20:31,559 --> 00:20:33,390
platform a little bit better we can do

460
00:20:33,390 --> 00:20:36,929
much more sophisticated analytics on top

461
00:20:36,929 --> 00:20:39,360
of the system the goal is we want to

462
00:20:39,360 --> 00:20:41,220
learn more about the time series we want

463
00:20:41,220 --> 00:20:43,980
to model the time series but therefore

464
00:20:43,980 --> 00:20:46,080
we must first aggregate the data in the

465
00:20:46,080 --> 00:20:47,940
right way and here we have shown our

466
00:20:47,940 --> 00:20:50,250
first experiments towards this results

467
00:20:50,250 --> 00:20:53,820
in the future that's all for today we

468
00:20:53,820 --> 00:20:55,440
have to say thanks to the audience and

469
00:20:55,440 --> 00:20:57,990
thanks to a bunch of colleagues which

470
00:20:57,990 --> 00:20:59,520
support the project in the background

471
00:20:59,520 --> 00:21:02,669
which are not here and that's all from

472
00:21:02,669 --> 00:21:18,440
my site it's time for questions it's

473
00:21:18,440 --> 00:21:21,929
open source it's not published yet we

474
00:21:21,929 --> 00:21:23,340
have it still in an internal repository

475
00:21:23,340 --> 00:21:27,440
we just clean it up but yes it is

476
00:21:27,440 --> 00:21:31,220
intended to be published

477
00:21:45,620 --> 00:21:48,270
it's an experimentation and measurement

478
00:21:48,270 --> 00:21:57,779
thing like an oscilloscope for currently

479
00:21:57,779 --> 00:22:00,210
we do it without knowledge and the

480
00:22:00,210 --> 00:22:02,730
result is overlapping with port ranges

481
00:22:02,730 --> 00:22:04,890
so if we can do port range statistics

482
00:22:04,890 --> 00:22:07,260
and this is fuzzy matching with known

483
00:22:07,260 --> 00:22:09,179
port ranges if you have such if you know

484
00:22:09,179 --> 00:22:10,620
the configuration then you can do this

485
00:22:10,620 --> 00:22:12,330
matching otherwise we can just learn

486
00:22:12,330 --> 00:22:16,279
from the data that's really cool

487
00:22:22,520 --> 00:22:25,230
that's the goal we have to capturing

488
00:22:25,230 --> 00:22:27,990
project it's a Python script it still

489
00:22:27,990 --> 00:22:30,330
needs a little bit documentation to make

490
00:22:30,330 --> 00:22:32,700
you fast in doing the experiments and

491
00:22:32,700 --> 00:22:34,260
not not wasting your time that's our

492
00:22:34,260 --> 00:22:36,210
goal and then we are thinking about

493
00:22:36,210 --> 00:22:37,800
contributing the stuff to the patchy

494
00:22:37,800 --> 00:22:40,440
spot project and finding the right

495
00:22:40,440 --> 00:22:42,510
structure and the right procedure and

496
00:22:42,510 --> 00:22:44,250
all the stuff this is going on we are

497
00:22:44,250 --> 00:22:47,520
not so fast so far yet but yeah that's

498
00:22:47,520 --> 00:23:10,290
the next thing I could not really here

499
00:23:10,290 --> 00:23:19,410
though could you please repeat yep we

500
00:23:19,410 --> 00:23:21,540
can do a mapping to known port range

501
00:23:21,540 --> 00:23:24,060
just in order to see which subsystem it

502
00:23:24,060 --> 00:23:26,460
is if you have no information about it

503
00:23:26,460 --> 00:23:28,680
you can do a good guess and we can look

504
00:23:28,680 --> 00:23:31,320
into the pattern shown here by looking

505
00:23:31,320 --> 00:23:35,970
into this pattern into this one and with

506
00:23:35,970 --> 00:23:37,200
a little bit knowledge about your

507
00:23:37,200 --> 00:23:39,510
algorithm and with some expectations you

508
00:23:39,510 --> 00:23:42,810
can say which curve belongs to which

509
00:23:42,810 --> 00:23:45,450
subsystem as a system engineer you would

510
00:23:45,450 --> 00:23:48,720
probably be able to guess good but let's

511
00:23:48,720 --> 00:23:50,790
say we have two jobs running in parallel

512
00:23:50,790 --> 00:23:52,950
doing the same stuff from different

513
00:23:52,950 --> 00:23:55,290
users such a thing could not be isolated

514
00:23:55,290 --> 00:23:57,270
anymore by this algorithm here we need a

515
00:23:57,270 --> 00:23:59,340
little bit advanced information maybe we

516
00:23:59,340 --> 00:24:02,070
have to combine this approach with lock

517
00:24:02,070 --> 00:24:05,010
information coming from yarn and if we

518
00:24:05,010 --> 00:24:06,930
have this integration then we could

519
00:24:06,930 --> 00:24:09,030
hopefully isolate on an application

520
00:24:09,030 --> 00:24:11,880
level this would be in next step to

521
00:24:11,880 --> 00:24:14,520
isolate not per technical subsystem but

522
00:24:14,520 --> 00:24:17,670
per application right now we cannot do

523
00:24:17,670 --> 00:24:19,790
that

524
00:24:25,570 --> 00:24:27,830
that's a good question for the package

525
00:24:27,830 --> 00:24:29,450
of des enough information because you'll

526
00:24:29,450 --> 00:24:31,220
have to have like a tag on each package

527
00:24:31,220 --> 00:24:33,320
to identify where the monster and your

528
00:24:33,320 --> 00:24:34,190
or network

529
00:24:34,190 --> 00:24:36,290
package content is not really that

530
00:24:36,290 --> 00:24:39,380
helpful right heart's partial

531
00:24:39,380 --> 00:24:42,860
information so to reconstruct the

532
00:24:42,860 --> 00:24:44,750
application level you need post-event

533
00:24:44,750 --> 00:24:47,420
psyche beta and the yarn resource

534
00:24:47,420 --> 00:24:48,980
furniture loads because that's going to

535
00:24:48,980 --> 00:24:51,050
tell you that this application has these

536
00:24:51,050 --> 00:24:52,790
ports on these machines and then you

537
00:24:52,790 --> 00:24:55,540
joined it together

538
00:25:14,890 --> 00:25:17,870
for this example here we did one thing

539
00:25:17,870 --> 00:25:19,730
we increase the replication level of the

540
00:25:19,730 --> 00:25:22,280
data instead of having three replicas we

541
00:25:22,280 --> 00:25:25,640
have now five one per host in a hundred

542
00:25:25,640 --> 00:25:27,590
node cluster this would be overwhelming

543
00:25:27,590 --> 00:25:30,680
but for our example if we bring maximum

544
00:25:30,680 --> 00:25:33,350
data locality into the play the network

545
00:25:33,350 --> 00:25:36,260
drops it's an it's an Academical example

546
00:25:36,260 --> 00:25:38,780
here just for showing the effect or even

547
00:25:38,780 --> 00:25:40,640
just to figure out if the effect is

548
00:25:40,640 --> 00:25:43,550
visible in resided guess but in order to

549
00:25:43,550 --> 00:25:48,830
do a better competitive analysis we need

550
00:25:48,830 --> 00:25:50,840
to figure out what resolution is the

551
00:25:50,840 --> 00:25:54,410
best we don't have this yet so depending

552
00:25:54,410 --> 00:25:57,140
on time resolution your recites could

553
00:25:57,140 --> 00:25:59,720
vary so that's also a problem we have to

554
00:25:59,720 --> 00:26:01,900
deal with

555
00:26:15,010 --> 00:26:17,510
we are interested in looking to graphics

556
00:26:17,510 --> 00:26:19,670
to see how it works when the node

557
00:26:19,670 --> 00:26:21,410
communicate with each other on a craft

558
00:26:21,410 --> 00:26:23,630
level and not on a server level maybe we

559
00:26:23,630 --> 00:26:28,340
can also yeah help to tune the graph

560
00:26:28,340 --> 00:26:30,920
algorithm for itself because such a

561
00:26:30,920 --> 00:26:33,020
graph algorithm is heavy network bound

562
00:26:33,020 --> 00:26:36,140
and then we can say okay this could be a

563
00:26:36,140 --> 00:26:38,530
half photo

564
00:27:19,310 --> 00:27:22,550
since an overhead possibly if th

565
00:27:22,550 --> 00:27:25,220
dependent you you can adjust this in

566
00:27:25,220 --> 00:27:27,890
pick up the baby when a sample currently

567
00:27:27,890 --> 00:27:31,130
we it try to capture as much data as we

568
00:27:31,130 --> 00:27:33,560
could to capture the most of them and

569
00:27:33,560 --> 00:27:35,600
favorite if you find that this overhead

570
00:27:35,600 --> 00:27:38,720
this just huge then gigabyte has an

571
00:27:38,720 --> 00:27:42,230
option to only capture one data packet

572
00:27:42,230 --> 00:27:44,180
in the hundreds or thousands of you can

573
00:27:44,180 --> 00:27:48,500
go to the diary thank you could I have

574
00:27:48,500 --> 00:27:50,810
also like a window based at you say okay

575
00:27:50,810 --> 00:27:54,080
I just capture how many bytes per second

576
00:27:54,080 --> 00:27:56,030
go - Paula Anderson started money

577
00:27:56,030 --> 00:27:57,320
windows open and then you have like a

578
00:27:57,320 --> 00:27:59,840
volume here at capture panel which kind

579
00:27:59,840 --> 00:28:02,080
of summarizes and in this cavity

580
00:28:02,080 --> 00:28:05,390
timerange body per second per five

581
00:28:05,390 --> 00:28:06,770
seconds or something like that and then

582
00:28:06,770 --> 00:28:08,450
it would give you enough information

583
00:28:08,450 --> 00:28:36,260
about the I hope you got something out

584
00:28:36,260 --> 00:28:38,960
of it for me it was quite interesting he

585
00:28:38,960 --> 00:28:41,120
has a feedback mechanism one on the

586
00:28:41,120 --> 00:28:42,680
first and website so if you have any

587
00:28:42,680 --> 00:28:44,690
feedback to any of people for the

588
00:28:44,690 --> 00:28:45,760
presenters

589
00:28:45,760 --> 00:28:48,290
also fast that please don't hesitate to

590
00:28:48,290 --> 00:28:51,110
get feedback and otherwise have a good

591
00:28:51,110 --> 00:28:54,119
evening in a good day tomorrow and

592
00:28:54,119 --> 00:28:56,178
you

