1
00:00:00,000 --> 00:00:00,990
okay welcome

2
00:00:00,990 --> 00:00:03,840
[Music]

3
00:00:04,620 --> 00:00:08,520
- the talk free software for the machine

4
00:00:08,520 --> 00:00:13,410
by Keith Packard well known for his work

5
00:00:13,410 --> 00:00:17,070
on the X Window System and this is about

6
00:00:17,070 --> 00:00:18,990
something else that he's working on

7
00:00:18,990 --> 00:00:24,750
since 2 years at HP and welcome there

8
00:00:24,750 --> 00:00:27,150
will be a 10 minute Q&A at the end and

9
00:00:27,150 --> 00:00:32,610
please wait for the microphone Thanks is

10
00:00:32,610 --> 00:00:35,820
that working yes good afternoon thanks

11
00:00:35,820 --> 00:00:37,470
for staying late it faust him on Sunday

12
00:00:37,470 --> 00:00:39,329
afternoon I'm sure we're all very tired

13
00:00:39,329 --> 00:00:41,700
and ready to go home I want to talk to

14
00:00:41,700 --> 00:00:44,010
you today about the operating system and

15
00:00:44,010 --> 00:00:45,719
other software that we put on some new

16
00:00:45,719 --> 00:00:47,610
interesting hardware I'm have a couple

17
00:00:47,610 --> 00:00:50,070
slides on the hardware I have extensive

18
00:00:50,070 --> 00:00:51,570
additional presentations online if you

19
00:00:51,570 --> 00:00:52,920
want to learn more about the machine of

20
00:00:52,920 --> 00:00:54,989
hardware today I do want to focus on

21
00:00:54,989 --> 00:00:57,300
just the software aspects um as you said

22
00:00:57,300 --> 00:00:58,590
there'll be time for questions but if

23
00:00:58,590 --> 00:01:00,059
you have questions the middle Didcot in

24
00:01:00,059 --> 00:01:01,200
the middle of the talk

25
00:01:01,200 --> 00:01:02,430
feel free to raise your hand we'll see

26
00:01:02,430 --> 00:01:04,199
if we can't get them introduced during

27
00:01:04,199 --> 00:01:07,350
the during the presentation as well so

28
00:01:07,350 --> 00:01:10,470
the machine is all about new new concept

29
00:01:10,470 --> 00:01:11,880
in computing they call memory driven

30
00:01:11,880 --> 00:01:15,660
computing existing computers are are

31
00:01:15,660 --> 00:01:18,060
very much centered on the central

32
00:01:18,060 --> 00:01:21,509
processing unit that CPU ends up gating

33
00:01:21,509 --> 00:01:22,560
a lot of the communication between

34
00:01:22,560 --> 00:01:24,780
various components and it ends up being

35
00:01:24,780 --> 00:01:26,009
a tremendous bottleneck when you're

36
00:01:26,009 --> 00:01:27,649
trying to move large amounts of data

37
00:01:27,649 --> 00:01:30,209
fast in classical computing you have an

38
00:01:30,209 --> 00:01:31,619
application it wants to communicate with

39
00:01:31,619 --> 00:01:33,060
data how do you talk to the how do you

40
00:01:33,060 --> 00:01:34,289
talk to your data store where you go

41
00:01:34,289 --> 00:01:36,179
through the operating system file

42
00:01:36,179 --> 00:01:38,130
systems you have disk drivers you have

43
00:01:38,130 --> 00:01:39,840
block buffers you have the page cache

44
00:01:39,840 --> 00:01:42,090
and you have a lot of distance between

45
00:01:42,090 --> 00:01:43,920
your application and its data in memory

46
00:01:43,920 --> 00:01:45,149
driven computing we're trying to

47
00:01:45,149 --> 00:01:47,160
collapse the storage hierarchy and

48
00:01:47,160 --> 00:01:48,420
getting the operating system out of the

49
00:01:48,420 --> 00:01:50,160
loop and having your application

50
00:01:50,160 --> 00:01:52,069
communicate directly with the underlying

51
00:01:52,069 --> 00:01:54,869
underlying store now that's a byte

52
00:01:54,869 --> 00:01:56,940
addressable memory store today you can

53
00:01:56,940 --> 00:01:59,580
get those in in the way of DDR T or nvm

54
00:01:59,580 --> 00:02:02,670
memories the DAC subsystem in Linux is

55
00:02:02,670 --> 00:02:04,979
all designed to talk to this so we're

56
00:02:04,979 --> 00:02:07,110
starting to get to small systems with

57
00:02:07,110 --> 00:02:09,060
memory driven computing but where the

58
00:02:09,060 --> 00:02:10,830
machine is trying to go is is trying to

59
00:02:10,830 --> 00:02:12,270
take memory driven computing from a

60
00:02:12,270 --> 00:02:14,280
single computer and constructing an

61
00:02:14,280 --> 00:02:17,010
entire machine room full of computers

62
00:02:17,010 --> 00:02:18,030
all of whom can be

63
00:02:18,030 --> 00:02:21,660
connected at the memory level so for

64
00:02:21,660 --> 00:02:22,920
memory during computing they're a bunch

65
00:02:22,920 --> 00:02:24,030
of different components one of the

66
00:02:24,030 --> 00:02:25,650
things you really need is fast and

67
00:02:25,650 --> 00:02:27,780
persistent memory if you want to be able

68
00:02:27,780 --> 00:02:30,420
to get at your store at CPU speeds and

69
00:02:30,420 --> 00:02:32,610
you want that store to be your the final

70
00:02:32,610 --> 00:02:34,230
resting place of your data it needs to

71
00:02:34,230 --> 00:02:36,090
be fast and it needs to be persistent

72
00:02:36,090 --> 00:02:38,160
there are a bunch of technologies coming

73
00:02:38,160 --> 00:02:39,989
out that offer that promise - that

74
00:02:39,989 --> 00:02:43,680
promise fast and persistent memory we

75
00:02:43,680 --> 00:02:48,569
have HP's memristor technology we have

76
00:02:48,569 --> 00:02:50,670
Western Digital Sri Ram we have Intel's

77
00:02:50,670 --> 00:02:52,320
3d crosspoint you have IBM's

78
00:02:52,320 --> 00:02:54,630
spin torque transfer memories all of

79
00:02:54,630 --> 00:02:56,400
these technologies we hope are coming

80
00:02:56,400 --> 00:02:58,860
soon to give you this fast persistent

81
00:02:58,860 --> 00:03:00,780
memory which you can enable a new

82
00:03:00,780 --> 00:03:03,540
generation of application design design

83
00:03:03,540 --> 00:03:05,160
systems called memory driven computing

84
00:03:05,160 --> 00:03:07,380
you need a fast memory fabric right now

85
00:03:07,380 --> 00:03:09,959
we the only way that this that the CPU

86
00:03:09,959 --> 00:03:11,700
can talk to memory is to its DDR bus

87
00:03:11,700 --> 00:03:14,010
right that really limits how much memory

88
00:03:14,010 --> 00:03:16,680
a single CPU can talk to by replacing

89
00:03:16,680 --> 00:03:18,239
the memory interconnect in your system

90
00:03:18,239 --> 00:03:20,340
you can dramatically increase the scope

91
00:03:20,340 --> 00:03:21,870
of memory that you can attach to your

92
00:03:21,870 --> 00:03:23,940
computer I'm going to talk about a new

93
00:03:23,940 --> 00:03:25,739
memory interconnect that we that we

94
00:03:25,739 --> 00:03:27,570
worked on for the machine and the

95
00:03:27,570 --> 00:03:29,310
technology which has been is being

96
00:03:29,310 --> 00:03:31,500
evaluated by a new consortium that's

97
00:03:31,500 --> 00:03:33,690
building new new memory interconnect the

98
00:03:33,690 --> 00:03:35,400
fabric turns out to be the interesting

99
00:03:35,400 --> 00:03:37,549
part of the machine in almost all ways

100
00:03:37,549 --> 00:03:40,799
it offers this tremendously broad reach

101
00:03:40,799 --> 00:03:43,590
to a huge amount of memory one of the

102
00:03:43,590 --> 00:03:45,480
other interesting aspects of memory

103
00:03:45,480 --> 00:03:47,519
driven computing is the notion that a

104
00:03:47,519 --> 00:03:49,829
single kind of general-purpose computer

105
00:03:49,829 --> 00:03:51,540
is not going to be the best thing for

106
00:03:51,540 --> 00:03:53,340
doing all of your computation people are

107
00:03:53,340 --> 00:03:55,140
already noticing that and a lot of

108
00:03:55,140 --> 00:03:56,850
neural network applications and other

109
00:03:56,850 --> 00:03:58,410
machine learning applications we have

110
00:03:58,410 --> 00:04:01,799
with people using GPUs now well GPUs are

111
00:04:01,799 --> 00:04:03,690
terrible general-purpose computation but

112
00:04:03,690 --> 00:04:04,820
they're great at very specific

113
00:04:04,820 --> 00:04:07,200
computations and the world of memory

114
00:04:07,200 --> 00:04:08,609
driven computing what we want to do is

115
00:04:08,609 --> 00:04:11,040
we want to unleash that capacity to plug

116
00:04:11,040 --> 00:04:13,200
in different computational elements into

117
00:04:13,200 --> 00:04:15,329
into the same memory fabric so we want

118
00:04:15,329 --> 00:04:16,798
to be able to have all of their

119
00:04:16,798 --> 00:04:18,930
computational elements apply directly to

120
00:04:18,930 --> 00:04:22,140
the memory and of course we're too

121
00:04:22,140 --> 00:04:24,060
software confidence and so one of the

122
00:04:24,060 --> 00:04:25,410
things that memory driven computing is

123
00:04:25,410 --> 00:04:27,180
is it's a full-employment act for

124
00:04:27,180 --> 00:04:28,710
software developers because to do

125
00:04:28,710 --> 00:04:30,840
effective memory driven computing you're

126
00:04:30,840 --> 00:04:31,420
going to need a lot

127
00:04:31,420 --> 00:04:33,370
new software to take advantage of the

128
00:04:33,370 --> 00:04:34,990
close coupling between the application

129
00:04:34,990 --> 00:04:38,680
in the store a lot of it is very vastly

130
00:04:38,680 --> 00:04:40,660
simpler software as you'll see in the in

131
00:04:40,660 --> 00:04:43,240
the in the presentation here today but

132
00:04:43,240 --> 00:04:44,890
it's very different and it takes a lot

133
00:04:44,890 --> 00:04:48,280
of new a lot of new time and ideas so

134
00:04:48,280 --> 00:04:50,620
here's the machine that we built a very

135
00:04:50,620 --> 00:04:54,610
simple sketch it's a homogeneous

136
00:04:54,610 --> 00:04:56,680
collection of different nodes of

137
00:04:56,680 --> 00:04:59,260
computation you can see in the blue a

138
00:04:59,260 --> 00:05:00,910
little Linux computer with de ramen and

139
00:05:00,910 --> 00:05:03,400
SOC that's literally there are four

140
00:05:03,400 --> 00:05:04,780
different Linux operating systems

141
00:05:04,780 --> 00:05:08,050
running on this slide right here and all

142
00:05:08,050 --> 00:05:10,150
those Linux computers are connected into

143
00:05:10,150 --> 00:05:11,560
this fabric so they each have a

144
00:05:11,560 --> 00:05:13,540
communication channel into the fabric of

145
00:05:13,540 --> 00:05:16,750
memory called a fabric switch connected

146
00:05:16,750 --> 00:05:18,760
to that fabric switch near by each

147
00:05:18,760 --> 00:05:21,160
computer is a collection of fabric

148
00:05:21,160 --> 00:05:23,380
attached memory for the machine

149
00:05:23,380 --> 00:05:25,270
prototype because we don't have this

150
00:05:25,270 --> 00:05:27,460
fast persistent memory available to us

151
00:05:27,460 --> 00:05:30,280
today we've connected DRAM when you

152
00:05:30,280 --> 00:05:32,230
turns out that when you put 320

153
00:05:32,230 --> 00:05:34,510
terabytes of DRAM in a single rack the

154
00:05:34,510 --> 00:05:38,220
cooling challenge is kind of daunting

155
00:05:38,220 --> 00:05:40,720
III what I didn't bring was the pictures

156
00:05:40,720 --> 00:05:42,130
of the team actually doing the

157
00:05:42,130 --> 00:05:43,480
development on the hardware and they all

158
00:05:43,480 --> 00:05:45,010
have ear plugs in their ears and

159
00:05:45,010 --> 00:05:46,810
headphones on in order to reduce the

160
00:05:46,810 --> 00:05:48,910
noise of the fans that are cooling the

161
00:05:48,910 --> 00:05:51,610
hardware so yeah it turns out that DRAM

162
00:05:51,610 --> 00:05:53,740
is really not gonna scale mostly because

163
00:05:53,740 --> 00:05:57,550
to to save data in DRAM you have to have

164
00:05:57,550 --> 00:05:59,500
the power turned on whereas with our

165
00:05:59,500 --> 00:06:01,210
fast persistent memory to save the

166
00:06:01,210 --> 00:06:02,650
memory you don't need any power at all

167
00:06:02,650 --> 00:06:05,110
so fast persistent memory offers two

168
00:06:05,110 --> 00:06:07,420
tremendous benefits for from memory for

169
00:06:07,420 --> 00:06:09,460
computing at scale it gives you the

170
00:06:09,460 --> 00:06:11,170
ability to know that your data is safe

171
00:06:11,170 --> 00:06:13,540
in the in the face of power failure but

172
00:06:13,540 --> 00:06:15,340
it also gives you a tremendous ability

173
00:06:15,340 --> 00:06:17,080
to increase your storage without

174
00:06:17,080 --> 00:06:19,570
increasing your static memory power

175
00:06:19,570 --> 00:06:21,430
consumption so your your power

176
00:06:21,430 --> 00:06:23,020
consumption now scales of the amount of

177
00:06:23,020 --> 00:06:24,940
data you're accessing not the amount of

178
00:06:24,940 --> 00:06:27,370
data you're storing so that's a simple

179
00:06:27,370 --> 00:06:29,950
schematic of what we've built here's a

180
00:06:29,950 --> 00:06:32,650
single node a little more detail within

181
00:06:32,650 --> 00:06:34,090
that computing node you have a little

182
00:06:34,090 --> 00:06:36,460
dram and an SOC that runs Linux and then

183
00:06:36,460 --> 00:06:37,720
you have some new interconnect

184
00:06:37,720 --> 00:06:40,270
technology called our next-generation

185
00:06:40,270 --> 00:06:42,430
memory interconnect and this particular

186
00:06:42,430 --> 00:06:44,440
system that connects that in to the

187
00:06:44,440 --> 00:06:45,159
fabric

188
00:06:45,159 --> 00:06:46,689
because these are separate Linux

189
00:06:46,689 --> 00:06:48,129
computers they're actually running

190
00:06:48,129 --> 00:06:49,659
different operating systems the

191
00:06:49,659 --> 00:06:51,520
protection domain between the

192
00:06:51,520 --> 00:06:54,129
application and the memory now needs to

193
00:06:54,129 --> 00:06:56,259
be underneath the operating system or

194
00:06:56,259 --> 00:06:58,089
the operating system is no longer is no

195
00:06:58,089 --> 00:07:00,279
longer in the privileged place of act of

196
00:07:00,279 --> 00:07:02,379
providing all the access control to your

197
00:07:02,379 --> 00:07:04,869
storage so we've inserted into the into

198
00:07:04,869 --> 00:07:07,089
the bus system and access control

199
00:07:07,089 --> 00:07:10,589
technology just a little a Britta a

200
00:07:10,589 --> 00:07:13,269
firewall between the operating system

201
00:07:13,269 --> 00:07:15,129
and the hardware itself so there's a

202
00:07:15,129 --> 00:07:18,610
hardware firewall also we need some we

203
00:07:18,610 --> 00:07:21,369
need some address mapping the the the

204
00:07:21,369 --> 00:07:23,019
poor little SOC that we put in this

205
00:07:23,019 --> 00:07:24,759
thing does not have enough address bits

206
00:07:24,759 --> 00:07:27,099
physical address bits to address all the

207
00:07:27,099 --> 00:07:29,589
memory in the machine the Machine the

208
00:07:29,589 --> 00:07:31,179
instance that we've constructed will

209
00:07:31,179 --> 00:07:32,439
address up to three hundred and twenty

210
00:07:32,439 --> 00:07:34,749
terabytes of memory which requires forty

211
00:07:34,749 --> 00:07:37,599
nine bits of addressing the largest SOC

212
00:07:37,599 --> 00:07:39,819
s that we can purchase today that we

213
00:07:39,819 --> 00:07:41,229
would that we looked at a value eight it

214
00:07:41,229 --> 00:07:43,479
today only offer 48 bits other

215
00:07:43,479 --> 00:07:45,939
particular SOC we've used only offers 44

216
00:07:45,939 --> 00:07:48,429
bits so here we have another case where

217
00:07:48,429 --> 00:07:50,199
we need another addressing indirection

218
00:07:50,199 --> 00:07:52,329
between the application physical

219
00:07:52,329 --> 00:07:53,740
addresses and the hardware because the

220
00:07:53,740 --> 00:07:56,349
add the addressing of the SOC is not

221
00:07:56,349 --> 00:07:58,149
large enough so we've had to put address

222
00:07:58,149 --> 00:07:59,949
translation and that turns out to be

223
00:07:59,949 --> 00:08:02,649
kind of a performance cliff because when

224
00:08:02,649 --> 00:08:04,389
you change the physical addressing of

225
00:08:04,389 --> 00:08:07,809
the SOC all of your caches in the SOC

226
00:08:07,809 --> 00:08:10,449
are physically tagged which is to say

227
00:08:10,449 --> 00:08:12,699
every every cache line and that SOC has

228
00:08:12,699 --> 00:08:15,579
a physical address associated with it so

229
00:08:15,579 --> 00:08:16,959
if you're gonna remap the physical

230
00:08:16,959 --> 00:08:19,209
addresses underneath that underneath the

231
00:08:19,209 --> 00:08:21,809
SOC you have to flush the entire cache

232
00:08:21,809 --> 00:08:25,869
modern SOC hate that it takes a long

233
00:08:25,869 --> 00:08:28,990
time to flush the cache and you can see

234
00:08:28,990 --> 00:08:31,449
here the memory complex is represented

235
00:08:31,449 --> 00:08:33,698
as a single block here where the SOC has

236
00:08:33,698 --> 00:08:36,849
access to the entire fabric so every SOC

237
00:08:36,849 --> 00:08:39,188
in the machine has byte level loads

238
00:08:39,188 --> 00:08:41,349
store access to every byte of memory in

239
00:08:41,349 --> 00:08:43,688
the fabric we put the we put a machine

240
00:08:43,688 --> 00:08:45,189
in a single rack right now which means

241
00:08:45,189 --> 00:08:48,189
that every SOC can talk to up to 320

242
00:08:48,189 --> 00:08:50,350
terabytes of memory that new

243
00:08:50,350 --> 00:08:53,170
interconnect technology is leading to as

244
00:08:53,170 --> 00:08:56,740
Lee has led to HP helping found and join

245
00:08:56,740 --> 00:08:58,840
a new consortium called Gen Z so

246
00:08:58,840 --> 00:09:00,940
new data access technology so this is a

247
00:09:00,940 --> 00:09:03,430
data access technology that's designed

248
00:09:03,430 --> 00:09:05,590
to replace all of the interconnects

249
00:09:05,590 --> 00:09:07,840
within your computer so the PC is the

250
00:09:07,840 --> 00:09:08,830
ddr's

251
00:09:08,830 --> 00:09:11,410
and other interconnects with in your

252
00:09:11,410 --> 00:09:13,240
computer QP is you could replace all of

253
00:09:13,240 --> 00:09:15,640
that with a single homogeneous fabric

254
00:09:15,640 --> 00:09:17,770
and that means that you can interconnect

255
00:09:17,770 --> 00:09:21,060
many SOC s and a lot of memory together

256
00:09:21,060 --> 00:09:23,980
DDR T is a great technology it served us

257
00:09:23,980 --> 00:09:25,360
very well but it is strictly

258
00:09:25,360 --> 00:09:29,110
point-to-point between an SOC and and a

259
00:09:29,110 --> 00:09:30,850
selection of DRAM the wires have to be

260
00:09:30,850 --> 00:09:33,010
short the signal tolerances are very

261
00:09:33,010 --> 00:09:34,960
tight which means that you can't plug in

262
00:09:34,960 --> 00:09:36,730
very much memory if you look at what the

263
00:09:36,730 --> 00:09:38,260
maximum memory you can plug into a

264
00:09:38,260 --> 00:09:40,000
typical processor it's you know maybe a

265
00:09:40,000 --> 00:09:45,160
terabyte maybe 2 maybe 10 HP makes a

266
00:09:45,160 --> 00:09:47,230
machine right now that offers offers up

267
00:09:47,230 --> 00:09:49,780
to 24 terabytes of memory I mean the way

268
00:09:49,780 --> 00:09:51,090
that we do that is by interconnecting

269
00:09:51,090 --> 00:09:53,830
SOC s that are each talking to their own

270
00:09:53,830 --> 00:09:56,470
DDR memory so that access latency

271
00:09:56,470 --> 00:09:58,600
between an SOC and memory depends upon

272
00:09:58,600 --> 00:10:00,520
how many SOC is and how much

273
00:10:00,520 --> 00:10:03,280
interconnect space there is so by using

274
00:10:03,280 --> 00:10:05,740
a homogeneous fabric we can flatten the

275
00:10:05,740 --> 00:10:07,900
access times for memory across the

276
00:10:07,900 --> 00:10:12,090
entire fabric the gens e is a consortium

277
00:10:12,090 --> 00:10:14,410
working on this technology they haven't

278
00:10:14,410 --> 00:10:16,630
released any any particular hardware yet

279
00:10:16,630 --> 00:10:18,580
we're working with a bunch of industry

280
00:10:18,580 --> 00:10:21,280
leaders and partners to develop this new

281
00:10:21,280 --> 00:10:23,170
interconnect technology that's going to

282
00:10:23,170 --> 00:10:25,330
lead to being able to to deliver memory

283
00:10:25,330 --> 00:10:27,790
driven computing so let's talk about a

284
00:10:27,790 --> 00:10:30,730
software on the machine when we started

285
00:10:30,730 --> 00:10:31,900
the machine program the research

286
00:10:31,900 --> 00:10:33,850
research OS people were very excited

287
00:10:33,850 --> 00:10:36,640
they said oh new hardware you must need

288
00:10:36,640 --> 00:10:38,890
a new operating system and then we went

289
00:10:38,890 --> 00:10:40,420
out and talked to our partners who

290
00:10:40,420 --> 00:10:41,800
wanted to develop applications and

291
00:10:41,800 --> 00:10:43,600
they're like no you don't get to do now

292
00:10:43,600 --> 00:10:45,790
operating system at least give us Linux

293
00:10:45,790 --> 00:10:48,310
so that we can try it out and of course

294
00:10:48,310 --> 00:10:50,500
the Linux group at HP is very excited

295
00:10:50,500 --> 00:10:52,420
because it's the opportunity to do fun

296
00:10:52,420 --> 00:10:54,700
new work in Linux so we've built a

297
00:10:54,700 --> 00:10:56,110
collection of software collectively

298
00:10:56,110 --> 00:10:57,490
known as the machine distribution a

299
00:10:57,490 --> 00:10:58,990
piece of which is Linux for the machine

300
00:10:58,990 --> 00:11:01,420
and within that within that software

301
00:11:01,420 --> 00:11:03,100
framework there's a bunch of new API is

302
00:11:03,100 --> 00:11:04,750
for applications to talk about to talk

303
00:11:04,750 --> 00:11:05,920
to which all talked about in a minute

304
00:11:05,920 --> 00:11:07,540
and then there's the basic node

305
00:11:07,540 --> 00:11:09,340
operating system and off to the side

306
00:11:09,340 --> 00:11:11,440
there's another piece of hardware called

307
00:11:11,440 --> 00:11:12,400
the

308
00:11:12,400 --> 00:11:13,780
a rack management server that runs

309
00:11:13,780 --> 00:11:16,000
additional global services one of which

310
00:11:16,000 --> 00:11:18,100
is the librarian that will be talking

311
00:11:18,100 --> 00:11:19,870
about uh

312
00:11:19,870 --> 00:11:22,450
in and drilling down a little a little

313
00:11:22,450 --> 00:11:24,610
deeper to look at just a piece piece of

314
00:11:24,610 --> 00:11:25,960
software called Linux of the machine

315
00:11:25,960 --> 00:11:28,060
here's the soccer that we built that

316
00:11:28,060 --> 00:11:30,940
runs Linux on an individual node you can

317
00:11:30,940 --> 00:11:32,920
see within Linux user space we have a

318
00:11:32,920 --> 00:11:34,510
bunch of new libraries we have some

319
00:11:34,510 --> 00:11:37,810
additional atomic access we have some

320
00:11:37,810 --> 00:11:39,730
additional we have the standard POSIX

321
00:11:39,730 --> 00:11:41,890
API s now we have an additional library

322
00:11:41,890 --> 00:11:43,930
for managing this cache caching issues

323
00:11:43,930 --> 00:11:46,420
because of the the memory fabric is not

324
00:11:46,420 --> 00:11:49,150
cache coherent we're using the PMM name

325
00:11:49,150 --> 00:11:51,160
here is actually from p.m. do which is a

326
00:11:51,160 --> 00:11:54,640
a collection of people working on nvm

327
00:11:54,640 --> 00:11:57,460
api's one of the things they've added to

328
00:11:57,460 --> 00:12:00,010
that is the ability to persist memory

329
00:12:00,010 --> 00:12:02,380
operations so you do a store while the

330
00:12:02,380 --> 00:12:04,150
store is going to go into your cache if

331
00:12:04,150 --> 00:12:05,470
you actually wanted to get it out to the

332
00:12:05,470 --> 00:12:07,240
nvm you actually have to flush it out of

333
00:12:07,240 --> 00:12:09,940
the cache and so this pmm library offers

334
00:12:09,940 --> 00:12:13,030
an api for that we have extended that to

335
00:12:13,030 --> 00:12:14,890
the to the library to the ability to

336
00:12:14,890 --> 00:12:17,680
invalidate cache lines as well because

337
00:12:17,680 --> 00:12:19,330
the memory fabric is going to span many

338
00:12:19,330 --> 00:12:21,880
many machines with a lot of memory right

339
00:12:21,880 --> 00:12:23,470
now the current memory fabric that we've

340
00:12:23,470 --> 00:12:25,180
implemented for the machine is not cache

341
00:12:25,180 --> 00:12:27,490
coherent across machines which is to say

342
00:12:27,490 --> 00:12:29,110
if I want to communicate a byte store

343
00:12:29,110 --> 00:12:30,940
from one machine to another I have to

344
00:12:30,940 --> 00:12:33,670
store the data into into memory with a

345
00:12:33,670 --> 00:12:35,380
regular store instruction I have to

346
00:12:35,380 --> 00:12:37,300
flush the cache with a regular CL flush

347
00:12:37,300 --> 00:12:39,670
instruction and on the other end I have

348
00:12:39,670 --> 00:12:41,350
to invalidate the cache and then read

349
00:12:41,350 --> 00:12:43,690
the data so this the PMM library and

350
00:12:43,690 --> 00:12:45,400
caps lates all that in a nice little API

351
00:12:45,400 --> 00:12:47,140
so you don't have to know what processor

352
00:12:47,140 --> 00:12:49,720
you're running on within the kernel

353
00:12:49,720 --> 00:12:51,610
we've added a bunch of new new drivers

354
00:12:51,610 --> 00:12:53,020
because we like to hack in the kernel

355
00:12:53,020 --> 00:12:55,210
there's the Atomics driver which lets

356
00:12:55,210 --> 00:12:57,100
you do atomic operations to the fabric

357
00:12:57,100 --> 00:12:58,690
memory without doing all the cache

358
00:12:58,690 --> 00:13:00,370
manipulation it does it all for you

359
00:13:00,370 --> 00:13:02,350
there's the cache flush support that I

360
00:13:02,350 --> 00:13:03,580
talked about in order to flush the

361
00:13:03,580 --> 00:13:05,230
caches and then there's a new file

362
00:13:05,230 --> 00:13:07,750
system when we started the program a

363
00:13:07,750 --> 00:13:11,080
couple of years ago the researchers came

364
00:13:11,080 --> 00:13:12,520
up and said what what we want is we want

365
00:13:12,520 --> 00:13:14,650
to be able to manage the storage in the

366
00:13:14,650 --> 00:13:16,450
machine we want to be able to provide

367
00:13:16,450 --> 00:13:18,580
you know collections of blocks of memory

368
00:13:18,580 --> 00:13:20,410
and we want to be able to provide naming

369
00:13:20,410 --> 00:13:22,540
and access control so we've invented

370
00:13:22,540 --> 00:13:24,340
this new API that's going to do all the

371
00:13:24,340 --> 00:13:25,320
storage manage

372
00:13:25,320 --> 00:13:28,230
for us and the Linux team looked at it

373
00:13:28,230 --> 00:13:30,600
it said um you've invented a file system

374
00:13:30,600 --> 00:13:33,840
API we have those already how about this

375
00:13:33,840 --> 00:13:36,150
nice POSIX filesystem API and the nice

376
00:13:36,150 --> 00:13:39,000
existing VFS layer in the kernel and

377
00:13:39,000 --> 00:13:40,530
they were like huh

378
00:13:40,530 --> 00:13:45,740
that kind it does look the same yeah

379
00:13:45,740 --> 00:13:47,940
researchers you know they think they

380
00:13:47,940 --> 00:13:49,470
invent something new and what they've

381
00:13:49,470 --> 00:13:52,920
done is reinvent UNIX see it again so we

382
00:13:52,920 --> 00:13:54,960
created a file system to manage the

383
00:13:54,960 --> 00:13:56,910
storage the bulk storage of data within

384
00:13:56,910 --> 00:13:58,620
the machine is called we call it a

385
00:13:58,620 --> 00:14:01,110
librarian and the reason for that

386
00:14:01,110 --> 00:14:04,440
particular name is well the bulk storage

387
00:14:04,440 --> 00:14:06,360
management within the machine manages a

388
00:14:06,360 --> 00:14:08,280
collection of pages and what do you call

389
00:14:08,280 --> 00:14:12,930
a collection of pages well as a book so

390
00:14:12,930 --> 00:14:14,220
we have a collection that we have a book

391
00:14:14,220 --> 00:14:17,070
and granularity storage granularity

392
00:14:17,070 --> 00:14:19,470
within the machine each book is 8

393
00:14:19,470 --> 00:14:20,610
gigabytes of memory

394
00:14:20,610 --> 00:14:22,500
so that's your block size in your file

395
00:14:22,500 --> 00:14:24,810
system is is the small little 8 gigabyte

396
00:14:24,810 --> 00:14:28,590
chunks and when you manage a collection

397
00:14:28,590 --> 00:14:29,910
of books while you manage them in a

398
00:14:29,910 --> 00:14:32,190
library right so that's why we call it

399
00:14:32,190 --> 00:14:34,950
librarian and kind of in between books

400
00:14:34,950 --> 00:14:37,860
and and the library is a collection of

401
00:14:37,860 --> 00:14:40,560
books as a shelf and so it turns out

402
00:14:40,560 --> 00:14:43,680
that unfortunately the least usefully

403
00:14:43,680 --> 00:14:45,300
name thing within the within the within

404
00:14:45,300 --> 00:14:47,040
our booth on our little world is the

405
00:14:47,040 --> 00:14:48,960
shelf which turns out to be the same

406
00:14:48,960 --> 00:14:50,220
thing as a file because it's a

407
00:14:50,220 --> 00:14:52,500
collection of books and each book is a

408
00:14:52,500 --> 00:14:55,080
collection of pages so at that level a

409
00:14:55,080 --> 00:14:57,420
file is the same thing as a shelf and so

410
00:14:57,420 --> 00:14:59,430
a library is a collection of shelves

411
00:14:59,430 --> 00:15:01,140
yeah the metaphor doesn't work great but

412
00:15:01,140 --> 00:15:03,390
the nice thing is we got to come up with

413
00:15:03,390 --> 00:15:05,250
a bunch of unique names so when we could

414
00:15:05,250 --> 00:15:07,020
talk about shelves we knew that that was

415
00:15:07,020 --> 00:15:09,420
a a file within the librarian file

416
00:15:09,420 --> 00:15:11,760
system so it let our conversations work

417
00:15:11,760 --> 00:15:12,990
a little better which is what naming is

418
00:15:12,990 --> 00:15:14,880
supposed to do is that was nice so

419
00:15:14,880 --> 00:15:16,230
here's what the librarian file system

420
00:15:16,230 --> 00:15:18,960
looks like I'm outside of each note of

421
00:15:18,960 --> 00:15:20,400
the machine and another machine we call

422
00:15:20,400 --> 00:15:22,410
the top of rack management server is the

423
00:15:22,410 --> 00:15:24,750
actual metadata manager for the file

424
00:15:24,750 --> 00:15:27,270
system called librarian it stores its

425
00:15:27,270 --> 00:15:29,370
metadata in a very sophisticated file

426
00:15:29,370 --> 00:15:33,420
structure called a sequel database for

427
00:15:33,420 --> 00:15:35,880
open for extra performance remember the

428
00:15:35,880 --> 00:15:38,250
books are 8 gigabytes in size and the

429
00:15:38,250 --> 00:15:39,030
total store

430
00:15:39,030 --> 00:15:41,370
machine is 320 terabytes so there are

431
00:15:41,370 --> 00:15:44,280
only 40,000 books in the machine which

432
00:15:44,280 --> 00:15:45,690
means that the metadata management

433
00:15:45,690 --> 00:15:48,600
problem in this particular instance the

434
00:15:48,600 --> 00:15:50,340
machine is pretty small I only have to

435
00:15:50,340 --> 00:15:51,660
be able to I only have to be able to

436
00:15:51,660 --> 00:15:54,450
remember 40,000 objects so the librarian

437
00:15:54,450 --> 00:15:57,330
is written in Python it uses a sequel

438
00:15:57,330 --> 00:15:58,680
light database to do the metadata

439
00:15:58,680 --> 00:16:01,440
storage and it uses regular TLS

440
00:16:01,440 --> 00:16:05,250
communication to the nodes to this LFS

441
00:16:05,250 --> 00:16:06,870
proxy which is a user space application

442
00:16:06,870 --> 00:16:09,120
running on the node written in Python

443
00:16:09,120 --> 00:16:12,450
that uses a hacked version of fuse to

444
00:16:12,450 --> 00:16:14,220
talk to the colonel so here I have a

445
00:16:14,220 --> 00:16:15,930
regular application it talks to a

446
00:16:15,930 --> 00:16:18,270
regular filesystem layer that filesystem

447
00:16:18,270 --> 00:16:19,920
layer looks a lot like fuse and all the

448
00:16:19,920 --> 00:16:21,540
metadata operations pop out of the

449
00:16:21,540 --> 00:16:24,090
kernel into this LFS proxy get trundled

450
00:16:24,090 --> 00:16:25,410
off the network off to this global

451
00:16:25,410 --> 00:16:27,120
librarian on another machine entirely

452
00:16:27,120 --> 00:16:29,160
the transactions get logged in the

453
00:16:29,160 --> 00:16:31,410
sequel Lite database and all that all

454
00:16:31,410 --> 00:16:34,950
that the return results get wended there

455
00:16:34,950 --> 00:16:36,930
way back through this whole API chain

456
00:16:36,930 --> 00:16:39,000
and we were a little concerned about

457
00:16:39,000 --> 00:16:41,190
performance but in real applications it

458
00:16:41,190 --> 00:16:43,800
turns out that well at 8 gigabytes for

459
00:16:43,800 --> 00:16:45,840
chunk the applications are doing maybe a

460
00:16:45,840 --> 00:16:47,910
couple hundred operations per second so

461
00:16:47,910 --> 00:16:49,610
it actually works out pretty well

462
00:16:49,610 --> 00:16:52,320
another big piece of the machine is

463
00:16:52,320 --> 00:16:53,910
security security is supposed to be

464
00:16:53,910 --> 00:16:56,040
built-in by default I told you about the

465
00:16:56,040 --> 00:16:58,410
hard work the hard firewall that we put

466
00:16:58,410 --> 00:17:00,360
in that protects the fabric from the

467
00:17:00,360 --> 00:17:02,700
nodes in order to manage that firewall

468
00:17:02,700 --> 00:17:04,140
you have to talk to that you have to

469
00:17:04,140 --> 00:17:05,579
manage the fight the contents that

470
00:17:05,579 --> 00:17:08,369
firewall in each node well in their

471
00:17:08,369 --> 00:17:10,140
infinite wisdom the hardware designers

472
00:17:10,140 --> 00:17:12,990
put the control of the firewall in chart

473
00:17:12,990 --> 00:17:15,510
the SOC itself in control of the

474
00:17:15,510 --> 00:17:17,130
firewall that is supposed to be

475
00:17:17,130 --> 00:17:21,319
protecting the fabric from the SOC well

476
00:17:21,319 --> 00:17:24,150
ok that's how they wired it up so what

477
00:17:24,150 --> 00:17:26,880
we did is we stuck the control for that

478
00:17:26,880 --> 00:17:30,330
firewall inside the arm press own which

479
00:17:30,330 --> 00:17:32,160
is a kind of a miserable security

480
00:17:32,160 --> 00:17:35,490
unclaimed within the arm ecosystem but

481
00:17:35,490 --> 00:17:37,170
it's secured from the operating system

482
00:17:37,170 --> 00:17:39,210
and then we have this firewall proxy up

483
00:17:39,210 --> 00:17:41,040
in user space that passes the firewall

484
00:17:41,040 --> 00:17:43,560
commands it's kind of a kludge but it's

485
00:17:43,560 --> 00:17:45,180
a research prototype it doesn't have to

486
00:17:45,180 --> 00:17:48,180
be really good it just has to work so we

487
00:17:48,180 --> 00:17:50,160
have a secure system we have a

488
00:17:50,160 --> 00:17:52,080
performant enough system and we have

489
00:17:52,080 --> 00:17:52,840
something which is

490
00:17:52,840 --> 00:17:56,409
relatively easy to prototype relatively

491
00:17:56,409 --> 00:17:58,299
easy to develop manage maintain and

492
00:17:58,299 --> 00:18:00,730
replace if we need to and it all works

493
00:18:00,730 --> 00:18:03,580
pretty well another big part of the

494
00:18:03,580 --> 00:18:05,470
system was those application libraries

495
00:18:05,470 --> 00:18:07,929
on top of the kernel um if all you give

496
00:18:07,929 --> 00:18:10,510
the application is a memory map file and

497
00:18:10,510 --> 00:18:11,950
you want to be able to share a data

498
00:18:11,950 --> 00:18:13,770
structures across nodes in the machine

499
00:18:13,770 --> 00:18:16,600
there are some tiny real API is that we

500
00:18:16,600 --> 00:18:18,730
have right now that do a good job of

501
00:18:18,730 --> 00:18:20,710
sharing data structures across the

502
00:18:20,710 --> 00:18:22,900
machine remember the goal here is to

503
00:18:22,900 --> 00:18:24,419
have the application directly

504
00:18:24,419 --> 00:18:26,529
manipulating its data structure in

505
00:18:26,529 --> 00:18:28,720
persistent memory and have that memory

506
00:18:28,720 --> 00:18:30,549
that data structure should be shared

507
00:18:30,549 --> 00:18:32,590
across nodes of the machine we're trying

508
00:18:32,590 --> 00:18:34,840
to collapse the storage hierarchy so we

509
00:18:34,840 --> 00:18:36,850
get rid of serialization of data we get

510
00:18:36,850 --> 00:18:39,220
rid of we get rid of an external

511
00:18:39,220 --> 00:18:40,929
database server that we use for in

512
00:18:40,929 --> 00:18:43,270
sequel environments we get rid of all

513
00:18:43,270 --> 00:18:44,860
the stuff and have the application

514
00:18:44,860 --> 00:18:47,080
manipulating the data directly so what

515
00:18:47,080 --> 00:18:49,179
we've done on top in up and user space

516
00:18:49,179 --> 00:18:51,429
is constructed a couple of different

517
00:18:51,429 --> 00:18:53,860
api's to kind of explore this area the

518
00:18:53,860 --> 00:18:55,149
one I want to share with you today is

519
00:18:55,149 --> 00:18:57,130
called managed data structures we're

520
00:18:57,130 --> 00:18:58,659
doing a lot of ongoing development in

521
00:18:58,659 --> 00:19:00,429
this area so it's a pretty active part

522
00:19:00,429 --> 00:19:02,679
of our research program right now so if

523
00:19:02,679 --> 00:19:03,909
you look at the way a traditional

524
00:19:03,909 --> 00:19:05,350
database work a traditional database

525
00:19:05,350 --> 00:19:07,690
works by having the application sit on

526
00:19:07,690 --> 00:19:10,870
top of an of relational API which then

527
00:19:10,870 --> 00:19:13,029
talks to a database client API to an

528
00:19:13,029 --> 00:19:14,770
external database server and that

529
00:19:14,770 --> 00:19:17,049
database server records the persistent

530
00:19:17,049 --> 00:19:18,850
data in a file system so it's a deep

531
00:19:18,850 --> 00:19:20,710
stack right every time you do some

532
00:19:20,710 --> 00:19:22,270
application manipulation you're going

533
00:19:22,270 --> 00:19:24,010
over the network to the database server

534
00:19:24,010 --> 00:19:25,809
and it's transacting stuff down of the

535
00:19:25,809 --> 00:19:29,289
file system by getting by using by doing

536
00:19:29,289 --> 00:19:32,169
all these the database data structure

537
00:19:32,169 --> 00:19:33,700
manipulations within a single address

538
00:19:33,700 --> 00:19:36,070
space right to the persistent memory we

539
00:19:36,070 --> 00:19:37,779
just have the application sitting on top

540
00:19:37,779 --> 00:19:39,820
of the managed data structure runtime so

541
00:19:39,820 --> 00:19:42,429
we turn tremendous performance

542
00:19:42,429 --> 00:19:46,090
improvement by doing that directly what

543
00:19:46,090 --> 00:19:49,360
MDS actually does is it provides data

544
00:19:49,360 --> 00:19:53,169
structures data structure api's in user

545
00:19:53,169 --> 00:19:55,799
space directly operated directly

546
00:19:55,799 --> 00:19:58,090
manipulatation so the application can

547
00:19:58,090 --> 00:19:59,919
just create new data structures like a

548
00:19:59,919 --> 00:20:02,620
linked list or a hash table or whatever

549
00:20:02,620 --> 00:20:05,140
at once and the operations on that data

550
00:20:05,140 --> 00:20:06,700
structure are done direct

551
00:20:06,700 --> 00:20:08,919
in the persistent store and are directly

552
00:20:08,919 --> 00:20:11,590
visible across the fabric to other nodes

553
00:20:11,590 --> 00:20:14,769
in the machine so we have a wide variety

554
00:20:14,769 --> 00:20:16,059
of little data structures you're

555
00:20:16,059 --> 00:20:18,419
familiar with some regular seeps see and

556
00:20:18,419 --> 00:20:20,649
whatever whatever language you're used

557
00:20:20,649 --> 00:20:23,499
to so we get we get rid of all these

558
00:20:23,499 --> 00:20:25,119
additional abstractions and we're just

559
00:20:25,119 --> 00:20:27,309
doing raw data structures it's very

560
00:20:27,309 --> 00:20:29,619
efficient we're able to share the data

561
00:20:29,619 --> 00:20:31,809
because NBS actually offers both a Java

562
00:20:31,809 --> 00:20:34,239
and C++ API so you can write a java

563
00:20:34,239 --> 00:20:36,489
application and the C++ application

564
00:20:36,489 --> 00:20:38,429
they're directly accessing the same data

565
00:20:38,429 --> 00:20:41,830
using simple simple abstractions that

566
00:20:41,830 --> 00:20:44,470
are common in both languages and they're

567
00:20:44,470 --> 00:20:49,210
they're sharing the data together so

568
00:20:49,210 --> 00:20:52,119
here's what I want to go go off and talk

569
00:20:52,119 --> 00:20:53,970
about our emulation story right now

570
00:20:53,970 --> 00:20:56,350
linux for the machine of course operates

571
00:20:56,350 --> 00:20:58,840
across the cluster it's a it looks like

572
00:20:58,840 --> 00:21:02,230
a cluster to Linux you have nodes here I

573
00:21:02,230 --> 00:21:03,399
have just two nodes on the screen

574
00:21:03,399 --> 00:21:04,710
although you can do as many as you like

575
00:21:04,710 --> 00:21:07,470
in a single rack we can get 80 of these

576
00:21:07,470 --> 00:21:10,299
so here we have your node one with its

577
00:21:10,299 --> 00:21:12,249
user space in its library and filesystem

578
00:21:12,249 --> 00:21:14,169
in the kernel talking to regular

579
00:21:14,169 --> 00:21:15,999
hardware and the same with the other

580
00:21:15,999 --> 00:21:18,039
node and then the top of rack management

581
00:21:18,039 --> 00:21:19,629
servers running the library and on top

582
00:21:19,629 --> 00:21:21,970
of a kernel and it's got it's got all of

583
00:21:21,970 --> 00:21:24,639
its magic hardware in the hardware that

584
00:21:24,639 --> 00:21:26,440
we've built right now each node is

585
00:21:26,440 --> 00:21:28,389
running an ARM processor and the

586
00:21:28,389 --> 00:21:30,070
management server is running on an x86

587
00:21:30,070 --> 00:21:31,629
processor but that doesn't really matter

588
00:21:31,629 --> 00:21:33,429
other nodes of course communicate

589
00:21:33,429 --> 00:21:35,169
through the fabric you'll note that

590
00:21:35,169 --> 00:21:37,090
there's a distinctly missing line the

591
00:21:37,090 --> 00:21:38,679
management server has no connection to

592
00:21:38,679 --> 00:21:39,970
the fabric the management server is

593
00:21:39,970 --> 00:21:42,639
actually a regular x86 server so it

594
00:21:42,639 --> 00:21:44,350
can't actually see the data that it's

595
00:21:44,350 --> 00:21:47,019
managing it can only know oh this guy's

596
00:21:47,019 --> 00:21:48,639
managed it has that block and this guy

597
00:21:48,639 --> 00:21:49,779
has that block at least they're not

598
00:21:49,779 --> 00:21:51,129
colliding but the management server

599
00:21:51,129 --> 00:21:53,289
can't store data into the persistent

600
00:21:53,289 --> 00:21:55,799
memory and it can't read data out of it

601
00:21:55,799 --> 00:21:58,239
so when you're operating on regular

602
00:21:58,239 --> 00:21:59,769
hardware that's how it works

603
00:21:59,769 --> 00:22:01,629
now what we wanted to be able to do is

604
00:22:01,629 --> 00:22:03,369
before we had the real hardware we

605
00:22:03,369 --> 00:22:04,509
needed to be able to start doing

606
00:22:04,509 --> 00:22:06,519
application development we wanted to be

607
00:22:06,519 --> 00:22:08,889
able to take existing Hardware and

608
00:22:08,889 --> 00:22:10,690
construct a synthetic version of the

609
00:22:10,690 --> 00:22:12,669
machine so we did two things the first

610
00:22:12,669 --> 00:22:14,590
thing we did is we took this old army

611
00:22:14,590 --> 00:22:17,320
emulator that we had and put in register

612
00:22:17,320 --> 00:22:18,999
level simulations of all the new fancy

613
00:22:18,999 --> 00:22:19,450
hard

614
00:22:19,450 --> 00:22:22,659
that we had so we're running simulated a

615
00:22:22,659 --> 00:22:25,570
simulated CPU in the simulated Z bridge

616
00:22:25,570 --> 00:22:28,149
in a simulated fabric so in a giant

617
00:22:28,149 --> 00:22:31,000
machine I was executing executing

618
00:22:31,000 --> 00:22:34,149
machine applications it about a thousand

619
00:22:34,149 --> 00:22:35,679
one one thousandth of the speed of the

620
00:22:35,679 --> 00:22:37,779
native hardware so we could do kernel

621
00:22:37,779 --> 00:22:38,799
development we could do driver

622
00:22:38,799 --> 00:22:40,299
development we even did the low-level

623
00:22:40,299 --> 00:22:42,880
bios and firmware development in that

624
00:22:42,880 --> 00:22:44,409
environment but when you try to actually

625
00:22:44,409 --> 00:22:46,840
simulate applications it's really way

626
00:22:46,840 --> 00:22:49,480
too slow the other problem of course is

627
00:22:49,480 --> 00:22:51,340
that you need a really big machine in

628
00:22:51,340 --> 00:22:53,950
order to simulate a even a reasonably

629
00:22:53,950 --> 00:22:57,519
scaled cluster because the computational

630
00:22:57,519 --> 00:23:00,190
cost is huge so we we were using our

631
00:23:00,190 --> 00:23:03,039
Superdome X servers which have 16 giant

632
00:23:03,039 --> 00:23:05,350
processors and 24 terabytes of memory to

633
00:23:05,350 --> 00:23:08,159
simulate a modest-sized machine and

634
00:23:08,159 --> 00:23:10,450
that's a multi-million dollar hardware

635
00:23:10,450 --> 00:23:12,880
investment for if you would just want to

636
00:23:12,880 --> 00:23:14,710
do application development so we came up

637
00:23:14,710 --> 00:23:17,740
with a really kind of a cute hack one of

638
00:23:17,740 --> 00:23:19,570
our kernel engineers rocky Craig went

639
00:23:19,570 --> 00:23:22,360
off and discovered that QEMU actually

640
00:23:22,360 --> 00:23:24,159
had this new magic driver that they were

641
00:23:24,159 --> 00:23:26,380
exposing to virtual virtual machines

642
00:23:26,380 --> 00:23:28,149
within the linux environment called the

643
00:23:28,149 --> 00:23:30,669
IV sh mem driver and the IV sherman

644
00:23:30,669 --> 00:23:32,649
driver actually lets you share memory

645
00:23:32,649 --> 00:23:34,330
between virtual machines it lets you

646
00:23:34,330 --> 00:23:35,980
take a chunk of memory from the

647
00:23:35,980 --> 00:23:36,669
hypervisor

648
00:23:36,669 --> 00:23:38,500
and map it into the address space of

649
00:23:38,500 --> 00:23:41,620
multiple virtual machines that sounds a

650
00:23:41,620 --> 00:23:44,710
lot like our fabric doesn't it so what

651
00:23:44,710 --> 00:23:46,090
we've done is we've actually we've

652
00:23:46,090 --> 00:23:48,130
actually constructed a cluster emulation

653
00:23:48,130 --> 00:23:49,990
where we take multiple virtual machines

654
00:23:49,990 --> 00:23:51,580
now these are all running on the same

655
00:23:51,580 --> 00:23:53,679
underlying physical machine but we take

656
00:23:53,679 --> 00:23:56,049
multiple virtual machines and construct

657
00:23:56,049 --> 00:23:58,419
within the hypervisor this piece of IV

658
00:23:58,419 --> 00:24:00,220
SRAM the inter virtual machine shared

659
00:24:00,220 --> 00:24:04,389
memory and we can we make that memory

660
00:24:04,389 --> 00:24:06,669
available to each of the VMS and now

661
00:24:06,669 --> 00:24:09,460
those VMs running them running the Linux

662
00:24:09,460 --> 00:24:11,529
running Linux can now pretend that

663
00:24:11,529 --> 00:24:13,539
that's fabric attached memory and they

664
00:24:13,539 --> 00:24:15,190
can manage it as if it were fabric

665
00:24:15,190 --> 00:24:17,139
attached memory so now I need to do is

666
00:24:17,139 --> 00:24:18,820
to stick another virtual machine

667
00:24:18,820 --> 00:24:20,289
pretending to be the top of rack

668
00:24:20,289 --> 00:24:22,090
management server with library and stuff

669
00:24:22,090 --> 00:24:24,010
running in it now all of a sudden I can

670
00:24:24,010 --> 00:24:26,740
do storage management for this interview

671
00:24:26,740 --> 00:24:30,220
em shared memory and get a synthetic

672
00:24:30,220 --> 00:24:32,770
machine running I have have it running

673
00:24:32,770 --> 00:24:34,810
laptop so I'm actually able to do use

674
00:24:34,810 --> 00:24:37,300
all the api's for the machine running on

675
00:24:37,300 --> 00:24:39,730
a single laptop this stuff is actually

676
00:24:39,730 --> 00:24:41,440
all the source code necessary for this

677
00:24:41,440 --> 00:24:43,450
is up on github right now and I actually

678
00:24:43,450 --> 00:24:45,460
have a debian repository that's public

679
00:24:45,460 --> 00:24:47,290
so you can actually take a debian

680
00:24:47,290 --> 00:24:50,320
machine and install a couple of VMs with

681
00:24:50,320 --> 00:24:52,780
Debian unstable on them and just install

682
00:24:52,780 --> 00:24:54,490
the necessary software to run a

683
00:24:54,490 --> 00:24:57,100
completely synthetic machine I talked to

684
00:24:57,100 --> 00:24:58,780
a customer on Friday who wants to play

685
00:24:58,780 --> 00:25:00,790
with the machine and when I showed them

686
00:25:00,790 --> 00:25:02,770
in about about five minutes how they

687
00:25:02,770 --> 00:25:05,110
could take three VMs and get them get it

688
00:25:05,110 --> 00:25:06,820
running the Machine api's they were

689
00:25:06,820 --> 00:25:10,270
pretty excited do I have ten minutes you

690
00:25:10,270 --> 00:25:15,240
said ten minutes okay about 10 minutes

691
00:25:15,240 --> 00:25:17,890
slightly more um I had to reboot my

692
00:25:17,890 --> 00:25:19,660
machine just before they caught I may I

693
00:25:19,660 --> 00:25:21,430
may give it a try I may give the demo a

694
00:25:21,430 --> 00:25:24,460
try we'll see I think I actually yeah

695
00:25:24,460 --> 00:25:26,500
we'll see any case also I actually could

696
00:25:26,500 --> 00:25:28,780
created a simple little application I

697
00:25:28,780 --> 00:25:30,190
can I'll show you the source code to it

698
00:25:30,190 --> 00:25:32,140
in a minute and in Emacs where you're

699
00:25:32,140 --> 00:25:34,300
you it's just using this the basic

700
00:25:34,300 --> 00:25:36,850
primitives of the machine that family

701
00:25:36,850 --> 00:25:38,800
tommix library to do atomic operations

702
00:25:38,800 --> 00:25:41,530
and the lib pmm to do data cache

703
00:25:41,530 --> 00:25:43,990
management and it has it's a simple chat

704
00:25:43,990 --> 00:25:46,450
program you type in words in one Pro and

705
00:25:46,450 --> 00:25:48,130
one window and they appear in another

706
00:25:48,130 --> 00:25:50,740
window and so you can see how that

707
00:25:50,740 --> 00:25:53,530
application works and I can actually do

708
00:25:53,530 --> 00:25:55,330
the demo actually why don't I go through

709
00:25:55,330 --> 00:25:56,500
the rest of the talk and then if we have

710
00:25:56,500 --> 00:25:58,420
time I could do the demo I have I just

711
00:25:58,420 --> 00:25:59,890
wanted a couple of slides here to show

712
00:25:59,890 --> 00:26:02,830
you where the where the free software

713
00:26:02,830 --> 00:26:04,210
for the machine was available here's all

714
00:26:04,210 --> 00:26:08,140
the links the github github.com slash

715
00:26:08,140 --> 00:26:10,030
fabric attach memory has all the

716
00:26:10,030 --> 00:26:12,430
necessary bits to run all the necessary

717
00:26:12,430 --> 00:26:14,230
Linux bits to run the fabric attached

718
00:26:14,230 --> 00:26:17,350
memory emulator github.com saw Hewlett

719
00:26:17,350 --> 00:26:19,540
Packard has a bunch of the other API is

720
00:26:19,540 --> 00:26:21,400
that we're using that has the managed

721
00:26:21,400 --> 00:26:22,930
data structures library that I talked

722
00:26:22,930 --> 00:26:25,570
about it has the MANET the multi

723
00:26:25,570 --> 00:26:29,440
processor GC API which which does which

724
00:26:29,440 --> 00:26:31,660
does garbage collected memory allocation

725
00:26:31,660 --> 00:26:33,880
across multiple processes which is kind

726
00:26:33,880 --> 00:26:35,440
of tricky because the processes end up

727
00:26:35,440 --> 00:26:37,840
crashing a lot and so the NP GC takes

728
00:26:37,840 --> 00:26:40,470
care of that we have Atlas which is a

729
00:26:40,470 --> 00:26:44,710
which is a it makes accessing fabric

730
00:26:44,710 --> 00:26:46,730
attach memory reliable so

731
00:26:46,730 --> 00:26:48,380
as kind of a transactional model in

732
00:26:48,380 --> 00:26:50,419
fabric attached memory makes it easy to

733
00:26:50,419 --> 00:26:51,860
program as I said we're trying to build

734
00:26:51,860 --> 00:26:54,020
a wealth of api's to help developers

735
00:26:54,020 --> 00:26:56,030
learn how to do memory driven computing

736
00:26:56,030 --> 00:26:59,679
and all of these are public and then the

737
00:26:59,679 --> 00:27:01,970
Debian repository containing all the

738
00:27:01,970 --> 00:27:03,830
bits necessary that come out of this are

739
00:27:03,830 --> 00:27:07,600
is in this downloads Linux HPE comm

740
00:27:07,600 --> 00:27:10,490
let's see I wanted to show you some

741
00:27:10,490 --> 00:27:11,929
pretty pictures of the hardware we built

742
00:27:11,929 --> 00:27:14,179
because I like hardware here's a single

743
00:27:14,179 --> 00:27:15,919
node of the machine on the left you'll

744
00:27:15,919 --> 00:27:18,679
see the SOC the far left you see the SOC

745
00:27:18,679 --> 00:27:20,120
in the middle of that you'll see it's

746
00:27:20,120 --> 00:27:22,760
local DRAM it has you know it just has a

747
00:27:22,760 --> 00:27:25,040
little bit as 256 gigabytes of local

748
00:27:25,040 --> 00:27:27,290
DRAM and that runs Linux completely out

749
00:27:27,290 --> 00:27:28,940
of memory it has no persistent storage

750
00:27:28,940 --> 00:27:30,710
at all and on the right side you'll see

751
00:27:30,710 --> 00:27:32,990
the four terabytes of memory a fabric

752
00:27:32,990 --> 00:27:34,250
attached memory with the memory

753
00:27:34,250 --> 00:27:37,190
controllers the next-generation memory

754
00:27:37,190 --> 00:27:39,440
interconnect that goes between that and

755
00:27:39,440 --> 00:27:41,870
the rest of the system and then on the

756
00:27:41,870 --> 00:27:43,130
backside there's a huge number of

757
00:27:43,130 --> 00:27:45,950
optical and electrical connectors to

758
00:27:45,950 --> 00:27:48,260
talk connect the fabric to the rest of

759
00:27:48,260 --> 00:27:50,210
the system here's another lovely piece

760
00:27:50,210 --> 00:27:51,590
of hardware this is the box that we

761
00:27:51,590 --> 00:27:53,150
slide all the nodes into it has the

762
00:27:53,150 --> 00:27:55,460
networking interconnects which is pretty

763
00:27:55,460 --> 00:27:58,370
cool it has those those connectors in

764
00:27:58,370 --> 00:28:00,679
the back are optical connectors and

765
00:28:00,679 --> 00:28:02,210
here's the other really cool piece of

766
00:28:02,210 --> 00:28:04,100
tech that we have so here's a piece of

767
00:28:04,100 --> 00:28:07,070
silicon it's a single chip and within

768
00:28:07,070 --> 00:28:08,720
that piece of silicon there's a there's

769
00:28:08,720 --> 00:28:11,090
a laser in it it's a little ring laser

770
00:28:11,090 --> 00:28:14,299
and that chip actually emits light out

771
00:28:14,299 --> 00:28:16,700
the top of itself and then you just

772
00:28:16,700 --> 00:28:19,010
stick a useless optical connector to

773
00:28:19,010 --> 00:28:20,570
stick a piece of optical fiber right

774
00:28:20,570 --> 00:28:22,340
onto the top of the silicon and now I

775
00:28:22,340 --> 00:28:25,400
have an optical interconnect directly

776
00:28:25,400 --> 00:28:26,840
from the silicon that's called silicon

777
00:28:26,840 --> 00:28:30,500
photonics it's amazing technology and

778
00:28:30,500 --> 00:28:32,120
this is actually within the machine

779
00:28:32,120 --> 00:28:33,950
prototypes so we're actually doing

780
00:28:33,950 --> 00:28:36,230
optical stuff which is pretty cool and

781
00:28:36,230 --> 00:28:37,730
if we have a minute I can show you that

782
00:28:37,730 --> 00:28:40,970
the demo is not that exciting but I do I

783
00:28:40,970 --> 00:28:43,549
think I can I think I'll show you the

784
00:28:43,549 --> 00:28:45,770
code to show you how that actually works

785
00:28:45,770 --> 00:28:48,580
because it's pretty fun

786
00:29:02,680 --> 00:29:06,190
oh thank you turn off my prison

787
00:29:06,190 --> 00:29:07,540
turn off the presentation here and put

788
00:29:07,540 --> 00:29:10,260
this on the other screen oops

789
00:29:10,260 --> 00:29:17,140
see now I'm gonna manage that's the

790
00:29:17,140 --> 00:29:18,640
entire program for communicating between

791
00:29:18,640 --> 00:29:20,860
two machines you can see up the top is

792
00:29:20,860 --> 00:29:22,270
that too small I see a bunch of people

793
00:29:22,270 --> 00:29:26,590
squinting what can I make it I don't

794
00:29:26,590 --> 00:29:28,780
know if I get even you're gonna ask me

795
00:29:28,780 --> 00:29:30,520
to try to make a text bigger on that on

796
00:29:30,520 --> 00:29:32,110
that other screen I mean I have to pull

797
00:29:32,110 --> 00:29:37,960
it back there we go I have to pull it

798
00:29:37,960 --> 00:29:39,460
back over here so I can manage

799
00:29:39,460 --> 00:29:44,160
oh it's I'm running I'm not running

800
00:29:44,670 --> 00:29:51,370
hello let's see if this works a little

801
00:29:51,370 --> 00:29:53,950
better I've put a couple of extra equal

802
00:29:53,950 --> 00:29:55,120
equal signs in there just for good

803
00:29:55,120 --> 00:29:57,670
measure so at the top you can see it's

804
00:29:57,670 --> 00:29:59,290
just it's just using the family time

805
00:29:59,290 --> 00:30:01,270
work library to do an atomic atomic read

806
00:30:01,270 --> 00:30:02,650
you check the status of this little

807
00:30:02,650 --> 00:30:05,770
little semaphore and when they when the

808
00:30:05,770 --> 00:30:07,090
semaphore is the right status it either

809
00:30:07,090 --> 00:30:08,770
reads the right the date reads or writes

810
00:30:08,770 --> 00:30:10,930
data into that and then the other thread

811
00:30:10,930 --> 00:30:12,730
just does the opposite saying so when

812
00:30:12,730 --> 00:30:14,410
you run this program on two nodes of the

813
00:30:14,410 --> 00:30:16,180
machine and like I say I'm sorry I had

814
00:30:16,180 --> 00:30:17,890
to reboot my machine just before the

815
00:30:17,890 --> 00:30:19,600
presentation or or I could show it

816
00:30:19,600 --> 00:30:23,230
running here but within within their

817
00:30:23,230 --> 00:30:26,290
virtual virtual memory within libvirt

818
00:30:26,290 --> 00:30:27,610
you can actually see that I have a

819
00:30:27,610 --> 00:30:29,560
Taurus machine and I have a couple of

820
00:30:29,560 --> 00:30:31,690
nodes and so when those are all up and

821
00:30:31,690 --> 00:30:32,560
running you can actually do

822
00:30:32,560 --> 00:30:34,750
communication between them but I wanted

823
00:30:34,750 --> 00:30:36,280
to leave time for questions and comments

824
00:30:36,280 --> 00:30:38,140
and so I think I'll probably end the

825
00:30:38,140 --> 00:30:40,700
presentation there

826
00:30:40,700 --> 00:30:51,450
[Applause]

827
00:31:03,050 --> 00:31:08,210
right hello hi so you did say that the

828
00:31:08,210 --> 00:31:10,400
different applications like Java and C++

829
00:31:10,400 --> 00:31:13,490
can share memory yep so how is this

830
00:31:13,490 --> 00:31:15,260
supposed to work if Joe is a managed

831
00:31:15,260 --> 00:31:19,490
heap and C++ says I manage memory how

832
00:31:19,490 --> 00:31:21,560
can they share the same oh it's not the

833
00:31:21,560 --> 00:31:24,110
Java he produced there is just a memo

834
00:31:24,110 --> 00:31:27,170
which they use yeah so the managed data

835
00:31:27,170 --> 00:31:29,210
structures library provides these data

836
00:31:29,210 --> 00:31:31,670
structures to the application and so the

837
00:31:31,670 --> 00:31:33,230
managed data structures library is

838
00:31:33,230 --> 00:31:35,630
written in C++ and is exposed to Java

839
00:31:35,630 --> 00:31:37,460
through a j'ni so when you want to

840
00:31:37,460 --> 00:31:39,140
create a new object in Java you go to

841
00:31:39,140 --> 00:31:41,720
the manage data structures system as I

842
00:31:41,720 --> 00:31:44,120
said that sits on top of the MGP c

843
00:31:44,120 --> 00:31:47,030
library the multiprocessor GC library so

844
00:31:47,030 --> 00:31:50,660
when the Java when the JVM loses loses a

845
00:31:50,660 --> 00:31:53,030
reference to the object then the J&I

846
00:31:53,030 --> 00:31:55,070
tells the MGP C system that the

847
00:31:55,070 --> 00:31:57,020
reference is lost and it goes ahead and

848
00:31:57,020 --> 00:31:58,640
collects the collects the memory from

849
00:31:58,640 --> 00:32:00,650
that so there are hooks within the JVM

850
00:32:00,650 --> 00:32:03,230
that allow the j'ni system to know when

851
00:32:03,230 --> 00:32:05,270
an object is no longer referenced and we

852
00:32:05,270 --> 00:32:07,670
have garbage collection within the MDS

853
00:32:07,670 --> 00:32:09,560
system that is connected between those

854
00:32:09,560 --> 00:32:11,330
two so you're actually able to do

855
00:32:11,330 --> 00:32:13,700
garbage collected memory within c++ and

856
00:32:13,700 --> 00:32:15,830
garbage collected memory within Java

857
00:32:15,830 --> 00:32:17,900
using the using this library so it's

858
00:32:17,900 --> 00:32:19,790
it's a it's a there is a library between

859
00:32:19,790 --> 00:32:21,830
you and the actual data you aren't doing

860
00:32:21,830 --> 00:32:24,980
anew in C++ you're calling the MDS

861
00:32:24,980 --> 00:32:27,350
system to allocate new memory you're not

862
00:32:27,350 --> 00:32:29,000
doing a new and Java you're calling the

863
00:32:29,000 --> 00:32:31,460
MDS library to allocate noumic new

864
00:32:31,460 --> 00:32:32,060
objects

865
00:32:32,060 --> 00:32:34,400
so it's MIT's managed data structures

866
00:32:34,400 --> 00:32:39,590
it's not native data structures so so

867
00:32:39,590 --> 00:32:40,820
and so I was gonna ask question about

868
00:32:40,820 --> 00:32:42,290
that that certainly implies to go not

869
00:32:42,290 --> 00:32:44,960
sharing page tables then I know they do

870
00:32:44,960 --> 00:32:47,000
not need to share page tables it's just

871
00:32:47,000 --> 00:32:48,470
sharing the raw underlying member okay

872
00:32:48,470 --> 00:32:49,850
but so yeah if I have an existing

873
00:32:49,850 --> 00:32:51,320
library so I worked on GPUs for a long

874
00:32:51,320 --> 00:32:53,510
time and the biggest complaint was I

875
00:32:53,510 --> 00:32:55,880
couldn't just take a piece of C code and

876
00:32:55,880 --> 00:32:58,550
just run it on the GPU and paw and went

877
00:32:58,550 --> 00:33:00,260
on the CPU pass a pointer store in a

878
00:33:00,260 --> 00:33:01,220
data structure I mean you're not going

879
00:33:01,220 --> 00:33:02,600
to have to do any of that unless you use

880
00:33:02,600 --> 00:33:05,900
your managed it depends on how you how

881
00:33:05,900 --> 00:33:07,640
you manage things if you map things to

882
00:33:07,640 --> 00:33:09,170
the same physical address on the two

883
00:33:09,170 --> 00:33:11,000
other two nodes then you can share

884
00:33:11,000 --> 00:33:12,680
pointers and we have we have a lot of

885
00:33:12,680 --> 00:33:14,000
the Atlas library does that in

886
00:33:14,000 --> 00:33:16,370
particular and it allows you to make

887
00:33:16,370 --> 00:33:16,740
sure that

888
00:33:16,740 --> 00:33:18,690
the memory lands at the same address and

889
00:33:18,690 --> 00:33:20,640
so that you can actually access things

890
00:33:20,640 --> 00:33:22,230
it doesn't matter if you actually share

891
00:33:22,230 --> 00:33:23,910
the underlying page tables as long as

892
00:33:23,910 --> 00:33:24,990
you're sharing the same virtual

893
00:33:24,990 --> 00:33:26,970
addresses on the operating system may

894
00:33:26,970 --> 00:33:28,410
have to do some extra management in

895
00:33:28,410 --> 00:33:30,300
order to in order to keep those virtual

896
00:33:30,300 --> 00:33:32,820
memory tables lined up we aren't doing

897
00:33:32,820 --> 00:33:34,500
any paging of course because it's all

898
00:33:34,500 --> 00:33:36,330
physical memory so all you need to do is

899
00:33:36,330 --> 00:33:38,040
make sure that the virtual addresses are

900
00:33:38,040 --> 00:33:39,870
lined up and that's really simple to do

901
00:33:39,870 --> 00:33:42,000
with the nmap system call so it's

902
00:33:42,000 --> 00:33:43,410
actually it's actually fairly easy to

903
00:33:43,410 --> 00:33:46,020
arrange the addresses to be the same the

904
00:33:46,020 --> 00:33:47,880
big problem that we have is that the CPU

905
00:33:47,880 --> 00:33:50,910
has a very limited address space of the

906
00:33:50,910 --> 00:33:53,460
arm 64 address space is only 48 bits

907
00:33:53,460 --> 00:33:56,040
right now and so a virtual address space

908
00:33:56,040 --> 00:33:58,110
is only 48 bits right now which means

909
00:33:58,110 --> 00:33:59,820
that you have to be careful when you're

910
00:33:59,820 --> 00:34:01,080
running multiple applicants when you're

911
00:34:01,080 --> 00:34:02,280
running the application to multiple

912
00:34:02,280 --> 00:34:04,350
nodes to reserve the appropriate

913
00:34:04,350 --> 00:34:06,000
portions of your virtual address space

914
00:34:06,000 --> 00:34:07,920
so that you can get the virtual

915
00:34:07,920 --> 00:34:09,750
addresses to be the same just because

916
00:34:09,750 --> 00:34:11,040
you don't have enough address space to

917
00:34:11,040 --> 00:34:13,909
to make it easy

918
00:34:15,139 --> 00:34:18,710
one question over here

919
00:34:20,030 --> 00:34:22,918
what kind of memory chips do use what I

920
00:34:22,918 --> 00:34:25,710
got was non-volatile is it kind of a

921
00:34:25,710 --> 00:34:28,440
ferromagnetic RAM or something you built

922
00:34:28,440 --> 00:34:30,330
or something you get on the show well

923
00:34:30,330 --> 00:34:32,989
because we we couldn't get any fast

924
00:34:32,989 --> 00:34:35,159
non-volatile memory right now the

925
00:34:35,159 --> 00:34:36,750
prototype that we built is built in

926
00:34:36,750 --> 00:34:39,570
regular DRAM which is non-volatile until

927
00:34:39,570 --> 00:34:42,200
you turn the power off

928
00:34:42,480 --> 00:34:45,060
so from a research prototype perspective

929
00:34:45,060 --> 00:34:46,770
it allows us to do all the research we

930
00:34:46,770 --> 00:34:48,599
need to do into memory driven computing

931
00:34:48,599 --> 00:34:50,339
in terms of application development API

932
00:34:50,339 --> 00:34:52,829
development all this kind of stuff and

933
00:34:52,829 --> 00:34:56,219
we just don't turn the power off so the

934
00:34:56,219 --> 00:34:58,260
other question over here is there any

935
00:34:58,260 --> 00:35:00,750
plan for a cheap version of the machine

936
00:35:00,750 --> 00:35:03,349
that the hacker can buy and use at home

937
00:35:03,349 --> 00:35:08,190
what is there an affordable version in

938
00:35:08,190 --> 00:35:10,859
the plan for hackers to just buy and use

939
00:35:10,859 --> 00:35:12,660
at home rather than all the enterprise

940
00:35:12,660 --> 00:35:15,390
users by nature um what this machine

941
00:35:15,390 --> 00:35:17,609
itself is a research prototype is not a

942
00:35:17,609 --> 00:35:21,150
product at all we have I think a hundred

943
00:35:21,150 --> 00:35:23,700
of those nodes in on the planet right

944
00:35:23,700 --> 00:35:25,740
now and we aren't really planning on

945
00:35:25,740 --> 00:35:26,940
building more of this particular

946
00:35:26,940 --> 00:35:29,490
prototype whether any of the technology

947
00:35:29,490 --> 00:35:31,079
and the machine research program gets

948
00:35:31,079 --> 00:35:32,369
into products is something that the

949
00:35:32,369 --> 00:35:34,890
product groups at HP are working on I'm

950
00:35:34,890 --> 00:35:36,570
not I'm a part of the research team

951
00:35:36,570 --> 00:35:38,730
building the prototypes so in terms of

952
00:35:38,730 --> 00:35:40,230
what part of this technology is going to

953
00:35:40,230 --> 00:35:42,030
make it into products I don't have any

954
00:35:42,030 --> 00:35:44,070
intern inside information on that right

955
00:35:44,070 --> 00:35:50,130
now thank you for the talk I don't know

956
00:35:50,130 --> 00:35:51,569
if you were on the previous talk that

957
00:35:51,569 --> 00:35:54,329
was sure about the risk 5 but could you

958
00:35:54,329 --> 00:35:56,220
possibly implement the machine with the

959
00:35:56,220 --> 00:35:59,250
new risk 5 CPUs don't know how much

960
00:35:59,250 --> 00:36:00,720
memory can address with those though

961
00:36:00,720 --> 00:36:03,150
well that's that is one of the explicit

962
00:36:03,150 --> 00:36:04,800
goals of the mission of the memory

963
00:36:04,800 --> 00:36:06,510
driven computing initiative is to allow

964
00:36:06,510 --> 00:36:08,280
you to plug in whatever processing

965
00:36:08,280 --> 00:36:10,319
elements you need into into a

966
00:36:10,319 --> 00:36:12,210
heterogeneous environment so you could

967
00:36:12,210 --> 00:36:15,050
have arm and x86 and risk v and power

968
00:36:15,050 --> 00:36:17,430
processors all connected to the same

969
00:36:17,430 --> 00:36:19,589
memory fabric the only thing you need to

970
00:36:19,589 --> 00:36:23,609
do is is is put into the processor that

971
00:36:23,609 --> 00:36:25,349
connect the interconnect from that

972
00:36:25,349 --> 00:36:28,560
processors memory bus inside to the to

973
00:36:28,560 --> 00:36:30,240
the next generation memory interconnect

974
00:36:30,240 --> 00:36:32,010
or in the future the gen Z systems

975
00:36:32,010 --> 00:36:34,589
because Gen Z is an opening consortium

976
00:36:34,589 --> 00:36:38,099
which of which IBM and are a bunch of

977
00:36:38,099 --> 00:36:39,990
the arm licensees and AMD are all

978
00:36:39,990 --> 00:36:42,359
members you can imagine that will get a

979
00:36:42,359 --> 00:36:44,609
wide ecosystem of different processors

980
00:36:44,609 --> 00:36:46,680
the specifications for Gen Z are all

981
00:36:46,680 --> 00:36:49,530
open some of the sample implementations

982
00:36:49,530 --> 00:36:53,160
the VHDL layouts for the sample sample

983
00:36:53,160 --> 00:36:55,230
chips that HP is working on those are

984
00:36:55,230 --> 00:36:56,310
also open

985
00:36:56,310 --> 00:36:58,050
so you can I could easily imagine the

986
00:36:58,050 --> 00:36:59,850
risk by people working within the gem Z

987
00:36:59,850 --> 00:37:01,440
consortium taking that those

988
00:37:01,440 --> 00:37:02,790
specifications and adding that

989
00:37:02,790 --> 00:37:04,350
interconnect to that processor and

990
00:37:04,350 --> 00:37:06,270
enabling this kind of technology there

991
00:37:06,270 --> 00:37:12,150
absolutely uh the I don't know what the

992
00:37:12,150 --> 00:37:14,370
licenses are there's there's it's a

993
00:37:14,370 --> 00:37:17,190
royalty-free license for using the

994
00:37:17,190 --> 00:37:20,010
technology but I don't know I haven't

995
00:37:20,010 --> 00:37:21,330
looked at the licensing of the gen Z

996
00:37:21,330 --> 00:37:23,550
stuff but I know that I know that it is

997
00:37:23,550 --> 00:37:26,280
very possible for the risk v to connect

998
00:37:26,280 --> 00:37:27,960
to gen Z without paying any without

999
00:37:27,960 --> 00:37:29,640
joining the consortium and without

1000
00:37:29,640 --> 00:37:35,520
paying any royalties hello yep what are

1001
00:37:35,520 --> 00:37:38,160
the difficulties of using SSD with

1002
00:37:38,160 --> 00:37:39,840
perhaps with a cache rather than waiting

1003
00:37:39,840 --> 00:37:42,240
for more exotic memory technologies well

1004
00:37:42,240 --> 00:37:44,220
and in fact that's that's similar to

1005
00:37:44,220 --> 00:37:46,530
what how nvm works today right you can

1006
00:37:46,530 --> 00:37:48,270
buy it you can buy ddrt memory which is

1007
00:37:48,270 --> 00:37:50,940
just which is just regular RAM with

1008
00:37:50,940 --> 00:37:53,070
flash on the backside so you can imagine

1009
00:37:53,070 --> 00:37:56,280
building a system that uses DRAM backed

1010
00:37:56,280 --> 00:37:58,380
by almost any persistent persistent

1011
00:37:58,380 --> 00:38:01,290
storage technology absolutely yeah again

1012
00:38:01,290 --> 00:38:03,330
one of the benefits of using truly

1013
00:38:03,330 --> 00:38:05,250
persistent memory is not just that it's

1014
00:38:05,250 --> 00:38:05,940
persistent

1015
00:38:05,940 --> 00:38:08,370
but that it takes no power to retain the

1016
00:38:08,370 --> 00:38:10,620
content so you can get a tremendously

1017
00:38:10,620 --> 00:38:13,320
law a much larger amount of memory in

1018
00:38:13,320 --> 00:38:15,180
the same physical space because you

1019
00:38:15,180 --> 00:38:16,650
don't have the thermal constraints of

1020
00:38:16,650 --> 00:38:20,640
DRAM right now our our 320 terabyte

1021
00:38:20,640 --> 00:38:23,490
version of the machine is technically

1022
00:38:23,490 --> 00:38:26,460
air-cooled when you push air at that

1023
00:38:26,460 --> 00:38:29,670
speeds is it still is it still you know

1024
00:38:29,670 --> 00:38:31,440
a gas I don't know there's a lot of

1025
00:38:31,440 --> 00:38:32,940
error being moved through that machine

1026
00:38:32,940 --> 00:38:35,730
just to cool the DRAM so if we could get

1027
00:38:35,730 --> 00:38:37,650
truly persistent memory we'd be able to

1028
00:38:37,650 --> 00:38:39,480
lower the power budget dramatically and

1029
00:38:39,480 --> 00:38:41,340
increase the amount of storage you can

1030
00:38:41,340 --> 00:38:43,140
get within the same physical volume just

1031
00:38:43,140 --> 00:38:44,760
because the cooling requirements go away

1032
00:38:44,760 --> 00:38:49,170
but yeah there's lots of opportunity em

1033
00:38:49,170 --> 00:38:51,360
were using DRAM with other persistent

1034
00:38:51,360 --> 00:38:52,950
memory behind it slower persistent I

1035
00:38:52,950 --> 00:38:55,520
reminded I haven't questioned myself

1036
00:38:55,520 --> 00:38:58,680
regarding the semaphores of a being duck

1037
00:38:58,680 --> 00:39:01,500
with in hardware also or yes on in the

1038
00:39:01,500 --> 00:39:03,870
machine hardware because that because

1039
00:39:03,870 --> 00:39:06,300
that fabric attached memory is is

1040
00:39:06,300 --> 00:39:08,610
external to the processor and shared by

1041
00:39:08,610 --> 00:39:10,140
multiple processors

1042
00:39:10,140 --> 00:39:12,000
the Atomics are actually performed in

1043
00:39:12,000 --> 00:39:14,130
the memory controller itself so when you

1044
00:39:14,130 --> 00:39:15,869
do an atomic operation it's actually a

1045
00:39:15,869 --> 00:39:17,490
transaction across the next-generation

1046
00:39:17,490 --> 00:39:19,109
memory and can interconnect fabric

1047
00:39:19,109 --> 00:39:21,960
between the processor and and the memory

1048
00:39:21,960 --> 00:39:24,059
controller itself so that it's etat it's

1049
00:39:24,059 --> 00:39:25,710
globally atomic within the entire

1050
00:39:25,710 --> 00:39:27,660
machine not just to within a single node

1051
00:39:27,660 --> 00:39:29,880
so we can't just use the atomic

1052
00:39:29,880 --> 00:39:32,490
operations within the SOC itself because

1053
00:39:32,490 --> 00:39:34,289
those are only atomic two other threads

1054
00:39:34,289 --> 00:39:36,180
running on the same SOC they're not

1055
00:39:36,180 --> 00:39:38,130
atomic with respect to other SOC s

1056
00:39:38,130 --> 00:39:41,579
running the fabric yeah it's actually

1057
00:39:41,579 --> 00:39:43,200
done within the memory memory hardware

1058
00:39:43,200 --> 00:39:47,069
itself question have you run LM bench to

1059
00:39:47,069 --> 00:39:51,589
compare local memory access times with

1060
00:39:53,059 --> 00:39:55,829
the fabric the fabrics memory access

1061
00:39:55,829 --> 00:39:58,859
times the the current implementation of

1062
00:39:58,859 --> 00:40:00,660
the memory controllers and the fabric is

1063
00:40:00,660 --> 00:40:05,339
all in FPGAs it's not running as fast as

1064
00:40:05,339 --> 00:40:08,400
it should FPGA performance is is not the

1065
00:40:08,400 --> 00:40:12,029
same as Asics doing ddrt access so we

1066
00:40:12,029 --> 00:40:13,799
don't we know what the memory

1067
00:40:13,799 --> 00:40:15,329
performance is likely to be in this it's

1068
00:40:15,329 --> 00:40:17,160
dramatically lower but that's simply

1069
00:40:17,160 --> 00:40:19,230
because we're using FPGA s for the early

1070
00:40:19,230 --> 00:40:20,910
implementation so that we can explore

1071
00:40:20,910 --> 00:40:23,819
different ideas in the interconnect and

1072
00:40:23,819 --> 00:40:25,769
so that we could do the board design in

1073
00:40:25,769 --> 00:40:27,809
parallel with the hardware design so we

1074
00:40:27,809 --> 00:40:29,400
got the board's back and the FPGA

1075
00:40:29,400 --> 00:40:31,529
firmware loaded on and those those two

1076
00:40:31,529 --> 00:40:33,900
were the schedules were aligned so we

1077
00:40:33,900 --> 00:40:35,640
didn't have to have the ASIC design and

1078
00:40:35,640 --> 00:40:37,680
then the board design so we did a bunch

1079
00:40:37,680 --> 00:40:39,210
of stuff in parallel like that and as a

1080
00:40:39,210 --> 00:40:41,309
result the hardware is a demonstration

1081
00:40:41,309 --> 00:40:43,410
of how it can work but it's not a

1082
00:40:43,410 --> 00:40:45,660
complete performance simulation of what

1083
00:40:45,660 --> 00:40:47,640
an eventual eventual piece of memory

1084
00:40:47,640 --> 00:40:48,869
driven computing hardware would work

1085
00:40:48,869 --> 00:40:52,349
like so no it would be a lot slower I

1086
00:40:52,349 --> 00:40:55,079
think they're running at less than 500

1087
00:40:55,079 --> 00:40:59,309
megahertz the the FPGAs and so do the

1088
00:40:59,309 --> 00:41:02,039
math it's not very fast it's a lot

1089
00:41:02,039 --> 00:41:04,109
faster than SSDs it's a lot flashier

1090
00:41:04,109 --> 00:41:06,390
than and a lot faster than nvme but it's

1091
00:41:06,390 --> 00:41:08,220
not as fast as DRAM because of the

1092
00:41:08,220 --> 00:41:09,809
interconnect technology being done in a

1093
00:41:09,809 --> 00:41:17,549
a Gatorade oh I'm here so sorry if I

1094
00:41:17,549 --> 00:41:21,329
missed the piece of information but will

1095
00:41:21,329 --> 00:41:23,820
there be any API or any way

1096
00:41:23,820 --> 00:41:28,830
to schedule the process to respective

1097
00:41:28,830 --> 00:41:30,990
nodes so so to control the locality all

1098
00:41:30,990 --> 00:41:34,890
the processes so that the the actual

1099
00:41:34,890 --> 00:41:36,810
computation process you run on a CPU

1100
00:41:36,810 --> 00:41:39,900
mm-hm it's located as close as possible

1101
00:41:39,900 --> 00:41:42,930
to us Hardware oh yeah so we've done a

1102
00:41:42,930 --> 00:41:45,030
couple of things for this one of the

1103
00:41:45,030 --> 00:41:46,650
things we did within the file system

1104
00:41:46,650 --> 00:41:48,930
each file within the library and file

1105
00:41:48,930 --> 00:41:51,060
system has extended attributes that can

1106
00:41:51,060 --> 00:41:53,190
that direct the librarian as to where

1107
00:41:53,190 --> 00:41:55,140
the memory should be allocated within

1108
00:41:55,140 --> 00:41:57,330
the system so you can define where the

1109
00:41:57,330 --> 00:41:59,010
allocations occur and then of course

1110
00:41:59,010 --> 00:42:01,650
each node is a separate Linux instance

1111
00:42:01,650 --> 00:42:04,350
so if you want to control where in the

1112
00:42:04,350 --> 00:42:06,240
in the machine your your application

1113
00:42:06,240 --> 00:42:07,860
runs you can direct it to a specific

1114
00:42:07,860 --> 00:42:09,660
node directly that's under your control

1115
00:42:09,660 --> 00:42:12,360
entirely you could imagine extending

1116
00:42:12,360 --> 00:42:14,130
things like MPI to actually have more

1117
00:42:14,130 --> 00:42:16,560
control more abstract control over where

1118
00:42:16,560 --> 00:42:20,060
things are done right now we don't

1119
00:42:21,859 --> 00:42:24,539
right now it's SSH into the node and

1120
00:42:24,539 --> 00:42:26,549
start your applications but as I said

1121
00:42:26,549 --> 00:42:28,949
where we have the allocation control so

1122
00:42:28,949 --> 00:42:31,259
you can control where in the fabric that

1123
00:42:31,259 --> 00:42:36,150
memory gets allocated question behind

1124
00:42:36,150 --> 00:42:38,239
you

1125
00:42:38,729 --> 00:42:41,579
so is that any support to focus on a

1126
00:42:41,579 --> 00:42:44,969
different node um the each node is

1127
00:42:44,969 --> 00:42:46,799
running a different different instance

1128
00:42:46,799 --> 00:42:48,390
of Linux so if you want to run a process

1129
00:42:48,390 --> 00:42:50,369
on another node you can SSH over there

1130
00:42:50,369 --> 00:42:52,109
and run it oh but there's no life

1131
00:42:52,109 --> 00:42:54,869
support in elf OGM - nope okay we

1132
00:42:54,869 --> 00:42:57,150
haven't done any of that work yet still

1133
00:42:57,150 --> 00:43:00,329
to be done we got to the point where we

1134
00:43:00,329 --> 00:43:01,799
could actually run applications across

1135
00:43:01,799 --> 00:43:04,170
the fabric and do all of our API

1136
00:43:04,170 --> 00:43:05,999
analysis and now there's more research

1137
00:43:05,999 --> 00:43:10,499
to be done we just need more money right

1138
00:43:10,499 --> 00:43:11,670
that's what software that's what

1139
00:43:11,670 --> 00:43:13,979
researchers always say hey I learned a

1140
00:43:13,979 --> 00:43:15,180
bunch of stuff and now I've got a bunch

1141
00:43:15,180 --> 00:43:16,709
of more questions that you need to pay

1142
00:43:16,709 --> 00:43:23,579
me to solve I have one more what do you

1143
00:43:23,579 --> 00:43:25,529
think will be the problems that are best

1144
00:43:25,529 --> 00:43:27,779
suitable for this architecture so we've

1145
00:43:27,779 --> 00:43:29,249
actually done a couple of different

1146
00:43:29,249 --> 00:43:30,719
problems one of it one of the

1147
00:43:30,719 --> 00:43:32,189
interesting ones we've come up with is

1148
00:43:32,189 --> 00:43:36,390
is is large data right now in a large

1149
00:43:36,390 --> 00:43:38,039
data environment the way that you solve

1150
00:43:38,039 --> 00:43:39,150
large data environments is you're

1151
00:43:39,150 --> 00:43:40,709
partition the data across a lot of

1152
00:43:40,709 --> 00:43:43,170
computers on that means that you have to

1153
00:43:43,170 --> 00:43:46,559
figure out which node to go ask for the

1154
00:43:46,559 --> 00:43:49,109
information by using memory memory

1155
00:43:49,109 --> 00:43:50,489
driven computing you're able to access

1156
00:43:50,489 --> 00:43:53,339
all that data symmetrically so image

1157
00:43:53,339 --> 00:43:56,579
search is a promising area of possible

1158
00:43:56,579 --> 00:43:58,859
research another area that we've looked

1159
00:43:58,859 --> 00:44:00,539
at is graph analytics where you have a

1160
00:44:00,539 --> 00:44:04,079
large graph at each the balancing of the

1161
00:44:04,079 --> 00:44:06,630
data within the graph requires it

1162
00:44:06,630 --> 00:44:08,400
requires access to a lot of different

1163
00:44:08,400 --> 00:44:10,859
data across the entire fabric and by

1164
00:44:10,859 --> 00:44:12,900
using fabric driven by using the fabric

1165
00:44:12,900 --> 00:44:15,449
each each node can compute the values

1166
00:44:15,449 --> 00:44:17,339
for one graph node by looking at the

1167
00:44:17,339 --> 00:44:19,380
neighboring values irrespective of where

1168
00:44:19,380 --> 00:44:21,119
those nodes are located in the fabric so

1169
00:44:21,119 --> 00:44:22,829
there's a bunch of a bunch of places

1170
00:44:22,829 --> 00:44:25,709
where right now communication is

1171
00:44:25,709 --> 00:44:28,140
dominating the cost of over computation

1172
00:44:28,140 --> 00:44:30,119
and those are a lot of the algorithms

1173
00:44:30,119 --> 00:44:31,410
they're gonna be faster in this in this

1174
00:44:31,410 --> 00:44:33,779
architecture so basically anything that

1175
00:44:33,779 --> 00:44:35,430
fits within the memory of the machine is

1176
00:44:35,430 --> 00:44:37,470
going to be a lot faster in the in the

1177
00:44:37,470 --> 00:44:39,480
machine than it would be in nodes with a

1178
00:44:39,480 --> 00:44:41,430
collection of separate DRAM bits we have

1179
00:44:41,430 --> 00:44:44,480
to spend a bunch of time communicating

1180
00:44:56,210 --> 00:44:58,020
that's a good question

1181
00:44:58,020 --> 00:45:00,180
in the machine prototype we aren't

1182
00:45:00,180 --> 00:45:03,200
trying to do networking over

