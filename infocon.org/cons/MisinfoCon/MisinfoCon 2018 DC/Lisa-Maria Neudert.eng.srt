1
00:00:00,269 --> 00:00:03,179
good morning I'm Lisa Marie unordered

2
00:00:03,179 --> 00:00:04,950
I'm with the Oxford internet Institute

3
00:00:04,950 --> 00:00:08,010
I'm a researcher Daren also a default

4
00:00:08,010 --> 00:00:11,010
candidate and I am doing work there with

5
00:00:11,010 --> 00:00:12,900
a project on computation of propaganda

6
00:00:12,900 --> 00:00:14,519
which is a project which is looking

7
00:00:14,519 --> 00:00:17,010
specifically into how algorithms and

8
00:00:17,010 --> 00:00:18,810
fake data are being used to spread

9
00:00:18,810 --> 00:00:20,640
misinformation and to spread propaganda

10
00:00:20,640 --> 00:00:23,880
online we are looking at misinformation

11
00:00:23,880 --> 00:00:25,859
from various different angles we have

12
00:00:25,859 --> 00:00:27,930
looked into how misinformation is

13
00:00:27,930 --> 00:00:30,390
impacting elections we have looked into

14
00:00:30,390 --> 00:00:32,549
the difference factor of threat actors

15
00:00:32,549 --> 00:00:33,750
we have looked into what their

16
00:00:33,750 --> 00:00:35,880
motivations are and over the past year

17
00:00:35,880 --> 00:00:37,190
we have also began to look into

18
00:00:37,190 --> 00:00:40,559
countermeasures so the good news is it

19
00:00:40,559 --> 00:00:42,540
may have taken a while but increasingly

20
00:00:42,540 --> 00:00:45,180
we see some movement in that space we

21
00:00:45,180 --> 00:00:47,700
see some countermeasures emerging in

22
00:00:47,700 --> 00:00:50,309
industry in civil society and another

23
00:00:50,309 --> 00:00:53,670
big field of course is regulation I

24
00:00:53,670 --> 00:00:56,129
personally have often felt a little bit

25
00:00:56,129 --> 00:00:58,859
indecisive about regulation and its

26
00:00:58,859 --> 00:01:00,829
efforts specifically on misinformation

27
00:01:00,829 --> 00:01:03,840
because on the one hand I think that in

28
00:01:03,840 --> 00:01:05,670
many ways were probably passed a point

29
00:01:05,670 --> 00:01:07,830
of self regulation also as Rene pointed

30
00:01:07,830 --> 00:01:10,290
out in many ways where we see a couple

31
00:01:10,290 --> 00:01:12,210
of social media companies trying to get

32
00:01:12,210 --> 00:01:14,310
their ducks in order but on the other

33
00:01:14,310 --> 00:01:16,920
hand a lot of the regulation every

34
00:01:16,920 --> 00:01:19,590
current PC merging are not released

35
00:01:19,590 --> 00:01:22,320
solutions designed to tackle a specific

36
00:01:22,320 --> 00:01:25,880
issue but are what I recall solution ISM

37
00:01:25,880 --> 00:01:28,439
solution ISM in a way of proposing

38
00:01:28,439 --> 00:01:30,150
regulation just for the sake of

39
00:01:30,150 --> 00:01:33,060
proposing something to do against

40
00:01:33,060 --> 00:01:36,630
misinformation or even worse countries

41
00:01:36,630 --> 00:01:38,579
and governments that are taking this as

42
00:01:38,579 --> 00:01:40,920
a moment to really tighten your control

43
00:01:40,920 --> 00:01:43,770
over information on the internet and the

44
00:01:43,770 --> 00:01:47,970
content that is being posted there with

45
00:01:47,970 --> 00:01:49,560
that I actually want to take you to my

46
00:01:49,560 --> 00:01:52,140
home country Germany Germany is one of

47
00:01:52,140 --> 00:01:54,360
the very first countries to regulate in

48
00:01:54,360 --> 00:01:56,159
the space they have introduced something

49
00:01:56,159 --> 00:01:58,590
that is called Network enforcement Act

50
00:01:58,590 --> 00:02:00,000
trust me German it sounds way more

51
00:02:00,000 --> 00:02:01,860
intimidating Nets vectors that since

52
00:02:01,860 --> 00:02:04,450
Cosette's that's one word

53
00:02:04,450 --> 00:02:08,500
and that was introduced in January 2018

54
00:02:08,500 --> 00:02:10,720
and it holds social networking companies

55
00:02:10,720 --> 00:02:13,180
liable for the content that users are

56
00:02:13,180 --> 00:02:15,670
posting there or otherwise is imposing

57
00:02:15,670 --> 00:02:17,410
fines on those social networking

58
00:02:17,410 --> 00:02:20,170
companies what that translates to in

59
00:02:20,170 --> 00:02:21,550
practice and what that looks like is

60
00:02:21,550 --> 00:02:24,340
exactly that and that as a so-called

61
00:02:24,340 --> 00:02:27,940
deletion Center in Essen with about 500

62
00:02:27,940 --> 00:02:30,280
people soon to be 1,000 people that are

63
00:02:30,280 --> 00:02:32,200
reviewing content that is being posted

64
00:02:32,200 --> 00:02:35,530
to Facebook every day every hour and are

65
00:02:35,530 --> 00:02:37,030
deciding whether it's content that

66
00:02:37,030 --> 00:02:38,590
should get deleted and should get taken

67
00:02:38,590 --> 00:02:42,160
down so that is also worth pausing this

68
00:02:42,160 --> 00:02:44,350
is trained staff that is doing that it

69
00:02:44,350 --> 00:02:45,670
is not lawyers

70
00:02:45,670 --> 00:02:47,739
it is not regulators it is not somebody

71
00:02:47,739 --> 00:02:49,870
from a government agency but that is

72
00:02:49,870 --> 00:02:51,220
trained staff from social media

73
00:02:51,220 --> 00:02:54,160
companies so what is the kind of content

74
00:02:54,160 --> 00:02:56,920
that are taking down the law originally

75
00:02:56,920 --> 00:02:58,569
was designed to be against hate speech

76
00:02:58,569 --> 00:03:02,049
and also against fake news and all

77
00:03:02,049 --> 00:03:04,900
content that is taken down is illegal in

78
00:03:04,900 --> 00:03:07,000
terms of what is illegal within the

79
00:03:07,000 --> 00:03:09,220
German law and also law that has already

80
00:03:09,220 --> 00:03:11,920
has been existing so that for example is

81
00:03:11,920 --> 00:03:14,230
libel that a slender that is defamation

82
00:03:14,230 --> 00:03:17,290
but also various other forms of hate

83
00:03:17,290 --> 00:03:19,930
speech and misinformation it's

84
00:03:19,930 --> 00:03:21,880
interesting because since that love was

85
00:03:21,880 --> 00:03:24,850
introduced in January 2018 there are

86
00:03:24,850 --> 00:03:26,859
also a couple of cases where content

87
00:03:26,859 --> 00:03:29,500
disappeared that probably should not

88
00:03:29,500 --> 00:03:31,810
have been deleted by any centers of

89
00:03:31,810 --> 00:03:34,750
freedom of speech one example that rose

90
00:03:34,750 --> 00:03:36,910
to prominence was a German satire

91
00:03:36,910 --> 00:03:39,880
magazine that was criticized saying a

92
00:03:39,880 --> 00:03:42,060
racist statement that a right-wing

93
00:03:42,060 --> 00:03:45,010
politician had made and in that

94
00:03:45,010 --> 00:03:49,060
criticism was actually referencing part

95
00:03:49,060 --> 00:03:51,280
of that statement and then that got

96
00:03:51,280 --> 00:03:52,720
taken down and Twitter had suspended

97
00:03:52,720 --> 00:03:59,319
their account for a day one thing that I

98
00:03:59,319 --> 00:04:01,450
really like about US law is that it

99
00:04:01,450 --> 00:04:04,480
comes with a biannual report of the type

100
00:04:04,480 --> 00:04:07,810
of content Datak gets taken down and the

101
00:04:07,810 --> 00:04:09,459
first report only just came out I

102
00:04:09,459 --> 00:04:11,230
brought a couple of numbers which you

103
00:04:11,230 --> 00:04:13,090
are probably not going to be able to

104
00:04:13,090 --> 00:04:17,310
read but just a stating

105
00:04:17,310 --> 00:04:18,899
how many complaints users have filed

106
00:04:18,899 --> 00:04:20,250
because they thought something was

107
00:04:20,250 --> 00:04:23,160
illegal and then how many pieces of

108
00:04:23,160 --> 00:04:25,230
content have been taken down and there

109
00:04:25,230 --> 00:04:27,210
are sort of free takeaways from these

110
00:04:27,210 --> 00:04:29,310
numbers that I'm showing you here the

111
00:04:29,310 --> 00:04:31,710
first take away is that this shows that

112
00:04:31,710 --> 00:04:34,020
users to some extent really are media

113
00:04:34,020 --> 00:04:36,360
literate literate it's between ten to

114
00:04:36,360 --> 00:04:38,040
forty five percent of the content where

115
00:04:38,040 --> 00:04:40,410
they thought it was illegal that social

116
00:04:40,410 --> 00:04:42,330
networks also decided yes it is indeed

117
00:04:42,330 --> 00:04:44,880
illegal which really shows that users

118
00:04:44,880 --> 00:04:46,950
have some form of capability to spot

119
00:04:46,950 --> 00:04:49,230
content that probably is where we

120
00:04:49,230 --> 00:04:51,600
hateful that is misinformation and that

121
00:04:51,600 --> 00:04:54,240
is against the law the other thing that

122
00:04:54,240 --> 00:04:55,770
is really interesting is if you would

123
00:04:55,770 --> 00:04:58,050
take a look at Facebook here you can see

124
00:04:58,050 --> 00:05:00,750
it's just free digit numbers and that is

125
00:05:00,750 --> 00:05:02,850
not because Facebook in Germany is a

126
00:05:02,850 --> 00:05:05,280
great space for genuine democratic

127
00:05:05,280 --> 00:05:07,620
discourse but it is actually because

128
00:05:07,620 --> 00:05:09,479
Facebook has made it really difficult to

129
00:05:09,479 --> 00:05:15,360
spot bad flagging tool and then the yes

130
00:05:15,360 --> 00:05:19,080
and then the third thing and it's really

131
00:05:19,080 --> 00:05:21,360
it's massive numbers that are taken down

132
00:05:21,360 --> 00:05:23,039
yes its millions of posts that are

133
00:05:23,039 --> 00:05:25,110
generated but still those are quite

134
00:05:25,110 --> 00:05:27,150
substantial numbers of content that are

135
00:05:27,150 --> 00:05:29,100
disappearing and they are disappearing

136
00:05:29,100 --> 00:05:31,320
without any legal oversight without any

137
00:05:31,320 --> 00:05:34,740
judges at the situation present so I'll

138
00:05:34,740 --> 00:05:36,150
run through the next slide quickly

139
00:05:36,150 --> 00:05:37,770
because I'm already running out of time

140
00:05:37,770 --> 00:05:40,979
but to look back at to what the problem

141
00:05:40,979 --> 00:05:42,180
is that we're actually trying to

142
00:05:42,180 --> 00:05:44,520
regulate and up we're concerned with on

143
00:05:44,520 --> 00:05:47,270
the one hand yes we have algorithms that

144
00:05:47,270 --> 00:05:49,950
ambach data that an able computation of

145
00:05:49,950 --> 00:05:51,990
propaganda did enable the spread of

146
00:05:51,990 --> 00:05:54,270
misinformation but on the other hand we

147
00:05:54,270 --> 00:05:56,550
also see these kind of strategies really

148
00:05:56,550 --> 00:05:58,440
flock into the mainstream both into the

149
00:05:58,440 --> 00:06:00,840
media and into politics and then the

150
00:06:00,840 --> 00:06:02,880
third problem is the solution ISM that

151
00:06:02,880 --> 00:06:04,830
I've just said that as I'm urging so

152
00:06:04,830 --> 00:06:07,500
that is actually regulation and also

153
00:06:07,500 --> 00:06:09,479
other kinds of solutions that are really

154
00:06:09,479 --> 00:06:11,610
tightening the freedom of expression the

155
00:06:11,610 --> 00:06:13,350
freedom of speech that we do have on the

156
00:06:13,350 --> 00:06:18,360
Internet the things that regulation can

157
00:06:18,360 --> 00:06:20,160
actually tackle and the things that

158
00:06:20,160 --> 00:06:21,570
regulators are really concerned with

159
00:06:21,570 --> 00:06:23,639
right now are on the one hand yes to

160
00:06:23,639 --> 00:06:25,409
spread of drunk use misinformation of

161
00:06:25,409 --> 00:06:27,659
course for an intellectual interference

162
00:06:27,659 --> 00:06:30,449
bots trolls data-driven campaigns

163
00:06:30,449 --> 00:06:32,849
and then also a lot of regulators are

164
00:06:32,849 --> 00:06:34,830
thinking about okay how can we look into

165
00:06:34,830 --> 00:06:36,419
your journalism and media literacy and

166
00:06:36,419 --> 00:06:40,199
support that and I think one of the

167
00:06:40,199 --> 00:06:42,839
questions that we get asked a lot is

168
00:06:42,839 --> 00:06:44,639
okay then what should positive

169
00:06:44,639 --> 00:06:47,370
regulation actually look like what what

170
00:06:47,370 --> 00:06:49,469
should we do about this and together

171
00:06:49,469 --> 00:06:51,539
with a colleague Samantha Bradshaw and

172
00:06:51,539 --> 00:06:53,520
we got really interested in this topic

173
00:06:53,520 --> 00:06:55,439
and we've reviewed regulation that

174
00:06:55,439 --> 00:06:57,599
already is existing out there we've been

175
00:06:57,599 --> 00:07:00,029
looking into the top 100 countries the

176
00:07:00,029 --> 00:07:03,120
countries by population on the internet

177
00:07:03,120 --> 00:07:05,729
and have looked into what kind of

178
00:07:05,729 --> 00:07:09,870
regulation has emerged err and this is

179
00:07:09,870 --> 00:07:11,189
the country stuff we've found where

180
00:07:11,189 --> 00:07:13,520
there is new regulation introduced since

181
00:07:13,520 --> 00:07:16,650
2016 specifically addressing the spread

182
00:07:16,650 --> 00:07:18,539
of misinformation in relation to

183
00:07:18,539 --> 00:07:20,639
propaganda campaigns in relation to

184
00:07:20,639 --> 00:07:22,529
foreign interference so what's

185
00:07:22,529 --> 00:07:24,449
interesting it's quite a broad

186
00:07:24,449 --> 00:07:26,789
geographical spread and it's also

187
00:07:26,789 --> 00:07:28,680
democracies as well as countries that

188
00:07:28,680 --> 00:07:32,759
are perhaps less democratic and we have

189
00:07:32,759 --> 00:07:34,259
tried to map and make sense of that a

190
00:07:34,259 --> 00:07:35,669
little bit and we have looked into the

191
00:07:35,669 --> 00:07:37,529
measures and the different actors that

192
00:07:37,529 --> 00:07:40,529
are being addressed and one of the

193
00:07:40,529 --> 00:07:42,599
things that we are seeing is that is

194
00:07:42,599 --> 00:07:44,939
often social media platforms our target

195
00:07:44,939 --> 00:07:47,699
and how data stand looking like as we

196
00:07:47,699 --> 00:07:49,050
have for example data protection

197
00:07:49,050 --> 00:07:50,550
regulation we have advertising

198
00:07:50,550 --> 00:07:53,759
transparency and content takedowns the

199
00:07:53,759 --> 00:07:56,639
second group is citizen and media that

200
00:07:56,639 --> 00:07:58,860
is for example media accreditation for

201
00:07:58,860 --> 00:08:01,770
example in Tanzania now if you're a

202
00:08:01,770 --> 00:08:03,959
blogger or journalist you actually have

203
00:08:03,959 --> 00:08:05,939
to get accredited by the media if you

204
00:08:05,939 --> 00:08:07,469
don't want your content you delete it

205
00:08:07,469 --> 00:08:10,259
and you don't want to be fined and we

206
00:08:10,259 --> 00:08:12,689
have also controls in literacy we have

207
00:08:12,689 --> 00:08:15,149
of course government inquiries and we

208
00:08:15,149 --> 00:08:17,039
have security and defence monitoring

209
00:08:17,039 --> 00:08:19,740
which is often done within the military

210
00:08:19,740 --> 00:08:22,620
and then as the last group that are the

211
00:08:22,620 --> 00:08:24,569
target we have offenders and that is

212
00:08:24,569 --> 00:08:27,120
also quite interesting because on the

213
00:08:27,120 --> 00:08:29,969
one hand we see criminalization for

214
00:08:29,969 --> 00:08:33,299
example we see fines we see imprisonment

215
00:08:33,299 --> 00:08:35,639
but what we see is that the

216
00:08:35,639 --> 00:08:37,229
criminalization is not just about

217
00:08:37,229 --> 00:08:39,688
countries that are or actors that are

218
00:08:39,688 --> 00:08:41,549
originating their content but also that

219
00:08:41,549 --> 00:08:44,099
are sharing it so that has already

220
00:08:44,099 --> 00:08:45,899
illegal in for example countries like

221
00:08:45,899 --> 00:08:51,240
Vietnam and like Malaysia so we also try

222
00:08:51,240 --> 00:08:52,769
to think about what are the possible

223
00:08:52,769 --> 00:08:54,660
side effects of that kind of regulation

224
00:08:54,660 --> 00:08:56,420
the first side effect of course is

225
00:08:56,420 --> 00:08:58,680
chilling effects on the freedom of

226
00:08:58,680 --> 00:09:01,440
speech very much chilling effects where

227
00:09:01,440 --> 00:09:03,360
people are afraid to speak out or cannot

228
00:09:03,360 --> 00:09:04,860
speak out because their content is being

229
00:09:04,860 --> 00:09:08,130
deleted very interesting is that we have

230
00:09:08,130 --> 00:09:11,009
seen a learning curve for authoritarian

231
00:09:11,009 --> 00:09:13,290
regimes for example that German law that

232
00:09:13,290 --> 00:09:15,449
we just talked about has been copied by

233
00:09:15,449 --> 00:09:17,310
Russia and it also has been pointed to

234
00:09:17,310 --> 00:09:19,230
by Malaysian and by Vietnamese

235
00:09:19,230 --> 00:09:23,100
regulators that however are enforcing

236
00:09:23,100 --> 00:09:24,870
that law with a different democratic

237
00:09:24,870 --> 00:09:27,290
background and a different rule of law

238
00:09:27,290 --> 00:09:31,279
what we are also seeing is

239
00:09:31,279 --> 00:09:34,110
many counter measures that are actually

240
00:09:34,110 --> 00:09:36,839
addressing individual actors individual

241
00:09:36,839 --> 00:09:38,759
pieces of content rather than really

242
00:09:38,759 --> 00:09:41,250
looking into this big systemic problem

243
00:09:41,250 --> 00:09:42,660
that we were having that has to do with

244
00:09:42,660 --> 00:09:44,579
algorithms that has to do with the way

245
00:09:44,579 --> 00:09:47,399
that information and its dynamics are

246
00:09:47,399 --> 00:09:50,130
happening in modern-day age and also has

247
00:09:50,130 --> 00:09:52,589
to do with our societies and her

248
00:09:52,589 --> 00:09:54,990
politics of course and a fourth sort of

249
00:09:54,990 --> 00:09:57,329
trend that we distinguished is that a

250
00:09:57,329 --> 00:10:00,750
lot of the tasks that perhaps should

251
00:10:00,750 --> 00:10:01,949
have been over in the past would have

252
00:10:01,949 --> 00:10:04,350
been with courts with laws with

253
00:10:04,350 --> 00:10:06,930
governments are now being outsourced to

254
00:10:06,930 --> 00:10:09,420
social media companies where they indeed

255
00:10:09,420 --> 00:10:11,339
get in that position where they're their

256
00:10:11,339 --> 00:10:13,980
purveyors of what is truth and what is

257
00:10:13,980 --> 00:10:17,850
not and with that I already come to the

258
00:10:17,850 --> 00:10:20,519
conclusion I really want you to

259
00:10:20,519 --> 00:10:23,100
understand that the way that we're

260
00:10:23,100 --> 00:10:24,750
regulating will come with consequences

261
00:10:24,750 --> 00:10:27,360
and it also will inevitably come with

262
00:10:27,360 --> 00:10:30,420
unintended consequences I think it's a

263
00:10:30,420 --> 00:10:32,550
terrible idea not to regulate because we

264
00:10:32,550 --> 00:10:34,529
were afraid of unintended consequences

265
00:10:34,529 --> 00:10:36,569
but I think for a lot of people that are

266
00:10:36,569 --> 00:10:38,370
sort of pointing to governments and to

267
00:10:38,370 --> 00:10:39,930
regulators who solved our problem

268
00:10:39,930 --> 00:10:42,240
overnight it's not going to be as easy

269
00:10:42,240 --> 00:10:44,370
as debt and I think we need a long

270
00:10:44,370 --> 00:10:46,829
discourse about how we're going to

271
00:10:46,829 --> 00:10:48,089
tackle that problem and how we're going

272
00:10:48,089 --> 00:10:49,920
to engage different stakeholders and I

273
00:10:49,920 --> 00:10:51,269
hope we get you have some of that

274
00:10:51,269 --> 00:10:54,649
discussion in the workshops today

275
00:10:54,740 --> 00:10:59,980
[Applause]

