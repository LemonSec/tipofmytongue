1
00:00:00,269 --> 00:00:02,820
so my group focused on global

2
00:00:02,820 --> 00:00:04,740
disinformation threats on two different

3
00:00:04,740 --> 00:00:07,680
axes so the first axis that we had was

4
00:00:07,680 --> 00:00:11,250
between ideological and non ideological

5
00:00:11,250 --> 00:00:13,110
or financial threats the other axes

6
00:00:13,110 --> 00:00:16,079
ranged from less powerful and less

7
00:00:16,079 --> 00:00:17,699
congregated to more powerful and more

8
00:00:17,699 --> 00:00:21,150
congregated threats so on in terms of

9
00:00:21,150 --> 00:00:23,939
like more ideological and more

10
00:00:23,939 --> 00:00:25,470
congregated threats we were looking at

11
00:00:25,470 --> 00:00:29,340
state actors so state acting media this

12
00:00:29,340 --> 00:00:31,080
would be things such as channel one

13
00:00:31,080 --> 00:00:36,210
Sputnik PressTV GRU and VOA in terms of

14
00:00:36,210 --> 00:00:38,940
like financial but still more powerful

15
00:00:38,940 --> 00:00:40,379
threats we're looking at commercial

16
00:00:40,379 --> 00:00:43,680
influence such as Big Tobacco Sinclair

17
00:00:43,680 --> 00:00:46,680
and Breitbart in terms of less powerful

18
00:00:46,680 --> 00:00:49,530
but more ideological we have the

19
00:00:49,530 --> 00:00:51,930
grassroots troll movements so we're

20
00:00:51,930 --> 00:00:55,340
looking at things like that

21
00:00:55,340 --> 00:00:58,680
QA Nam the whatsapp mobbing in India

22
00:00:58,680 --> 00:01:01,969
that sort of thing and in terms of

23
00:01:01,969 --> 00:01:04,799
financial but less powerful movements

24
00:01:04,799 --> 00:01:07,500
we're looking at affiliate marketers so

25
00:01:07,500 --> 00:01:08,850
we're looking at there was a group of

26
00:01:08,850 --> 00:01:10,920
Macedonian teenagers who spread

27
00:01:10,920 --> 00:01:14,130
information to basically just four

28
00:01:14,130 --> 00:01:16,110
clicks to get more money and scamming

29
00:01:16,110 --> 00:01:16,830
people

30
00:01:16,830 --> 00:01:19,830
federal fraud sort of things so the

31
00:01:19,830 --> 00:01:22,140
techniques that they use to do these

32
00:01:22,140 --> 00:01:26,070
things are things like from

33
00:01:26,070 --> 00:01:27,479
state-sponsored media we're looking at

34
00:01:27,479 --> 00:01:29,490
TV and radio stations more traditional

35
00:01:29,490 --> 00:01:31,799
media that's the same thing for the

36
00:01:31,799 --> 00:01:33,600
commercial side so that's more

37
00:01:33,600 --> 00:01:35,790
traditional media along with funding

38
00:01:35,790 --> 00:01:38,310
certain scientific studies the more

39
00:01:38,310 --> 00:01:39,869
non-traditional types of techniques

40
00:01:39,869 --> 00:01:43,350
comes from the less congregated areas so

41
00:01:43,350 --> 00:01:45,210
that comes from the market affiliates

42
00:01:45,210 --> 00:01:49,399
and the grassroot trolls so that's like

43
00:01:49,399 --> 00:01:52,680
huge campaigns from the grassroots roles

44
00:01:52,680 --> 00:01:56,909
who all attack a certain person such as

45
00:01:56,909 --> 00:02:01,350
on Twitter it's also with the bots from

46
00:02:01,350 --> 00:02:03,719
the grassroots rural area with the

47
00:02:03,719 --> 00:02:07,680
affiliate marketers more telemarketing

48
00:02:07,680 --> 00:02:10,250
that sort of thing

49
00:02:10,830 --> 00:02:15,220
so interventions for these ways that

50
00:02:15,220 --> 00:02:17,680
they're spreading misinformation when it

51
00:02:17,680 --> 00:02:19,780
comes to affiliate marketers so that's

52
00:02:19,780 --> 00:02:23,020
less congregated unless ideological

53
00:02:23,020 --> 00:02:25,450
tactics we're looking at ad tech reform

54
00:02:25,450 --> 00:02:28,360
in terms of the less congregated more

55
00:02:28,360 --> 00:02:31,110
ideological tactics we're looking at

56
00:02:31,110 --> 00:02:34,270
stronger age regulations because a lot

57
00:02:34,270 --> 00:02:36,250
of like the fortune Area trolls aren't

58
00:02:36,250 --> 00:02:38,770
targeting teenagers we're also looking

59
00:02:38,770 --> 00:02:43,990
at higher enforcement of the standards

60
00:02:43,990 --> 00:02:46,710
community standards in terms of

61
00:02:46,710 --> 00:02:49,960
state-sponsored media that could be its

62
00:02:49,960 --> 00:02:52,030
own session but a lot of that is just

63
00:02:52,030 --> 00:02:55,330
involved in diplomacy diplomacy and in

64
00:02:55,330 --> 00:02:58,900
terms of commercial influence there's

65
00:02:58,900 --> 00:03:01,480
actually a lot that we can do there so a

66
00:03:01,480 --> 00:03:02,830
lot of that is like political money

67
00:03:02,830 --> 00:03:04,780
transparency and transparency with

68
00:03:04,780 --> 00:03:06,790
things like trade and stuff there's one

69
00:03:06,790 --> 00:03:08,230
overarching concept that we really

70
00:03:08,230 --> 00:03:09,910
talked about though and that was media

71
00:03:09,910 --> 00:03:11,530
literacy and there's a lot of different

72
00:03:11,530 --> 00:03:13,210
ways we can spread that it was mentioned

73
00:03:13,210 --> 00:03:14,230
earlier that we could do that through

74
00:03:14,230 --> 00:03:15,610
education but we can also do that

75
00:03:15,610 --> 00:03:25,360
through like public libraries that's

76
00:03:25,360 --> 00:03:25,930
great

77
00:03:25,930 --> 00:03:31,020
next presentation AV team if you will

78
00:03:31,020 --> 00:03:33,580
how it's made fake news faker fact

79
00:03:33,580 --> 00:03:37,320
figure facts please come up

80
00:03:41,790 --> 00:03:50,340
I want to get a picture of that as we

81
00:03:50,340 --> 00:03:52,650
had our workshop my teenage son made the

82
00:03:52,650 --> 00:04:06,980
slides so so wish me luck much go Fifi

83
00:04:10,340 --> 00:04:16,470
okay this is his his particular approach

84
00:04:16,470 --> 00:04:19,079
to creating fake news among his friends

85
00:04:19,079 --> 00:04:23,160
they download and this is Shamu the

86
00:04:23,160 --> 00:04:25,860
whale with a chainsaw he's about to cut

87
00:04:25,860 --> 00:04:28,229
to the side of the of the thing and go

88
00:04:28,229 --> 00:04:32,810
into into the audience with it

89
00:04:35,090 --> 00:04:39,780
NASA recommends the chainsaw this is

90
00:04:39,780 --> 00:04:41,849
where we're getting into what we really

91
00:04:41,849 --> 00:04:46,560
did so there are certain tactics that

92
00:04:46,560 --> 00:04:48,539
people use to help things spread fake

93
00:04:48,539 --> 00:04:51,720
news and real news and I am the demon in

94
00:04:51,720 --> 00:04:56,099
the room because I'm a growth hacker and

95
00:04:56,099 --> 00:04:57,570
that's what people hire me to do

96
00:04:57,570 --> 00:05:00,650
that's what faker fact hired me to do so

97
00:05:00,650 --> 00:05:03,840
you start with a group of people say

98
00:05:03,840 --> 00:05:06,360
your friends or if you look at pizza

99
00:05:06,360 --> 00:05:09,570
gate they went to planting seeds in a

100
00:05:09,570 --> 00:05:11,280
chat room so a lot of the fig news

101
00:05:11,280 --> 00:05:14,070
starts in chat rooms and you need a few

102
00:05:14,070 --> 00:05:17,190
people to give it traction initially so

103
00:05:17,190 --> 00:05:19,349
that it enough Phil talked about the

104
00:05:19,349 --> 00:05:26,130
bots they'll get a bot - all right so

105
00:05:26,130 --> 00:05:27,990
the bots will retweeted a whole bunch of

106
00:05:27,990 --> 00:05:29,940
times until it gets picked up by the

107
00:05:29,940 --> 00:05:31,680
mainstream media and then they'll do a

108
00:05:31,680 --> 00:05:33,030
story on it so you can give it

109
00:05:33,030 --> 00:05:38,099
legitimacy that way confirmation bias is

110
00:05:38,099 --> 00:05:40,080
a big thing once you hit that group of

111
00:05:40,080 --> 00:05:42,780
people that you know that want that

112
00:05:42,780 --> 00:05:44,669
story they're going to be spread it

113
00:05:44,669 --> 00:05:47,130
especially if it has sort of a gossipy

114
00:05:47,130 --> 00:05:49,349
or a novel approach to it that they

115
00:05:49,349 --> 00:05:51,270
think is interesting then they're going

116
00:05:51,270 --> 00:05:54,570
to they're going to repost it we're back

117
00:05:54,570 --> 00:05:55,740
to the bottom one the humor

118
00:05:55,740 --> 00:05:57,479
or the novelty is probably going to get

119
00:05:57,479 --> 00:06:01,699
some reposts um I also can't see that oh

120
00:06:01,699 --> 00:06:04,500
and the visuals can give it legitimacy

121
00:06:04,500 --> 00:06:06,120
so if it's got a good visual like the

122
00:06:06,120 --> 00:06:09,539
ones we just saw there it's going to get

123
00:06:09,539 --> 00:06:12,330
reposted more often and if the photo

124
00:06:12,330 --> 00:06:14,880
seems real and not not fake another

125
00:06:14,880 --> 00:06:18,930
thing was if almost everybody in the

126
00:06:18,930 --> 00:06:21,360
room agreed that if there was an

127
00:06:21,360 --> 00:06:23,880
authority who had been quoted in the

128
00:06:23,880 --> 00:06:25,919
article it gives it more legitimacy and

129
00:06:25,919 --> 00:06:27,300
they would feel more comfortable sharing

130
00:06:27,300 --> 00:06:32,190
it and oh and then emotionally

131
00:06:32,190 --> 00:06:34,020
triggering words was another thing that

132
00:06:34,020 --> 00:06:35,970
several people have spoken about things

133
00:06:35,970 --> 00:06:38,490
like shockingly or the trolls in

134
00:06:38,490 --> 00:06:40,979
Macedonia that the young lady just just

135
00:06:40,979 --> 00:06:43,110
referenced they would take a regular

136
00:06:43,110 --> 00:06:44,400
news article and make some dramatic

137
00:06:44,400 --> 00:06:47,370
headline to turn it into clickbait so

138
00:06:47,370 --> 00:06:50,639
that was a big way that things got

139
00:06:50,639 --> 00:06:52,770
shared and then we had a whole list of

140
00:06:52,770 --> 00:06:56,220
ideas of how to slow down people from

141
00:06:56,220 --> 00:06:58,909
sharing it

142
00:07:01,849 --> 00:07:04,320
starting with targeting people who are

143
00:07:04,320 --> 00:07:07,229
willing to listen because some people

144
00:07:07,229 --> 00:07:08,789
are at a far end of one end of the

145
00:07:08,789 --> 00:07:10,500
spectrum or the other and and no matter

146
00:07:10,500 --> 00:07:13,650
what you say if faker fact our website

147
00:07:13,650 --> 00:07:16,440
says it's not true they're just gonna

148
00:07:16,440 --> 00:07:18,300
write that off well faker fact is crazy

149
00:07:18,300 --> 00:07:20,580
that's not that's not it at all another

150
00:07:20,580 --> 00:07:22,680
way we thought might help was that if

151
00:07:22,680 --> 00:07:24,150
you have posted something and we

152
00:07:24,150 --> 00:07:25,860
probably all done it posted something

153
00:07:25,860 --> 00:07:27,479
that turned out later to be wrong that

154
00:07:27,479 --> 00:07:29,819
you honestly go back and say hey

155
00:07:29,819 --> 00:07:31,830
everybody I posted this and it turned

156
00:07:31,830 --> 00:07:34,139
out not to be true so I fell for it you

157
00:07:34,139 --> 00:07:35,430
know this is the whole thing with like

158
00:07:35,430 --> 00:07:37,590
the elders getting scammed if there's

159
00:07:37,590 --> 00:07:39,780
awareness of it it might slow people

160
00:07:39,780 --> 00:07:42,419
down a little bit if we put dates on the

161
00:07:42,419 --> 00:07:44,219
stories so that we would know well this

162
00:07:44,219 --> 00:07:46,530
story was written you know seven years

163
00:07:46,530 --> 00:07:48,270
ago so it's not really applicable

164
00:07:48,270 --> 00:07:50,729
anymore that would be helpful also

165
00:07:50,729 --> 00:07:54,500
tracing the sources back to the original

166
00:07:54,500 --> 00:07:57,180
article or the original source the

167
00:07:57,180 --> 00:07:59,699
primary source so that you can you can

168
00:07:59,699 --> 00:08:02,219
find out you know what what was actually

169
00:08:02,219 --> 00:08:05,639
said under what conditions instead of

170
00:08:05,639 --> 00:08:09,110
taking it out of context

171
00:08:09,210 --> 00:08:13,050
also if you wanted to to discuss it with

172
00:08:13,050 --> 00:08:15,030
somebody you could post links about the

173
00:08:15,030 --> 00:08:16,800
same article from a more reputable

174
00:08:16,800 --> 00:08:20,789
source so that would would disrepute the

175
00:08:20,789 --> 00:08:24,530
person who's you know supporting the

176
00:08:24,530 --> 00:08:28,500
ridiculousness asking people regularly

177
00:08:28,500 --> 00:08:30,479
what's your source and something that

178
00:08:30,479 --> 00:08:33,240
maybe the platform's would be able to do

179
00:08:33,240 --> 00:08:36,000
is a way to not boost things because

180
00:08:36,000 --> 00:08:38,070
sometimes someone will put some in city

181
00:08:38,070 --> 00:08:41,880
areas to give it traction just to get

182
00:08:41,880 --> 00:08:44,520
people to argue with them and if there's

183
00:08:44,520 --> 00:08:46,560
a way that Facebook and Twitter and

184
00:08:46,560 --> 00:08:49,200
other people once they start arguing and

185
00:08:49,200 --> 00:08:51,450
all of those people start into that

186
00:08:51,450 --> 00:08:54,300
particular topic on that particular link

187
00:08:54,300 --> 00:08:57,000
if there was a way that the the

188
00:08:57,000 --> 00:08:59,880
platforms could not prioritize that it

189
00:08:59,880 --> 00:09:01,350
would help because if they're just

190
00:09:01,350 --> 00:09:03,930
trying to stir up trouble it's actually

191
00:09:03,930 --> 00:09:05,790
a marketing tactic it doesn't

192
00:09:05,790 --> 00:09:08,630
necessarily mean that you are are

193
00:09:08,630 --> 00:09:11,100
fighting with a legitimate person it's

194
00:09:11,100 --> 00:09:13,110
easily could be somebody who's you know

195
00:09:13,110 --> 00:09:16,650
been planted their stories about how

196
00:09:16,650 --> 00:09:20,700
fake news has damaged people everyone in

197
00:09:20,700 --> 00:09:22,890
the room felt like it was the whatsapp

198
00:09:22,890 --> 00:09:25,050
issue and with India had been really

199
00:09:25,050 --> 00:09:27,420
effective in in making everyone more

200
00:09:27,420 --> 00:09:31,529
aware of how damaging it can be to to

201
00:09:31,529 --> 00:09:35,550
spread fake news so probably if the news

202
00:09:35,550 --> 00:09:37,200
media could pick up more stories about

203
00:09:37,200 --> 00:09:39,810
how damaging the damage that have been

204
00:09:39,810 --> 00:09:42,029
done on a personal level to people

205
00:09:42,029 --> 00:09:44,130
because of fake news that would be a way

206
00:09:44,130 --> 00:09:47,970
to slow it down and there's these days

207
00:09:47,970 --> 00:09:50,690
there's an an AI app but you could also

208
00:09:50,690 --> 00:09:53,610
embed the metadata in a way so that if

209
00:09:53,610 --> 00:09:54,959
you're looking at a picture of something

210
00:09:54,959 --> 00:09:57,839
that's been misrepresented then the AI

211
00:09:57,839 --> 00:10:01,770
app or the metadata could be putting

212
00:10:01,770 --> 00:10:04,200
away so that you can see the text that's

213
00:10:04,200 --> 00:10:06,420
associated with that like this is a

214
00:10:06,420 --> 00:10:08,399
person at a celebration crossing the

215
00:10:08,399 --> 00:10:10,920
street and that would offset the idea

216
00:10:10,920 --> 00:10:13,470
that this is a guy with an Isis flag

217
00:10:13,470 --> 00:10:16,260
after the World Cup that kind of thing

218
00:10:16,260 --> 00:10:22,540
on a lower level to keep those emotional

219
00:10:22,540 --> 00:10:25,840
from being such a trigger if we could

220
00:10:25,840 --> 00:10:28,900
all give maybe some emotional management

221
00:10:28,900 --> 00:10:33,220
classes to our kids and ourselves that

222
00:10:33,220 --> 00:10:37,140
would slow things down as well

223
00:10:37,830 --> 00:10:42,010
that is the end thank you Thank You

224
00:10:42,010 --> 00:10:42,640
Rebecca

225
00:10:42,640 --> 00:10:45,820
and faker fact that was really great

226
00:10:45,820 --> 00:10:49,780
thank you let's see let's see who we

227
00:10:49,780 --> 00:10:54,040
have next Craig go and they have slides

228
00:10:54,040 --> 00:11:00,070
today amazing huh Samantha from Craig go

229
00:11:00,070 --> 00:11:03,790
away yes I am uh I don't work for Craig

230
00:11:03,790 --> 00:11:06,130
Co I was I was just an attendee for this

231
00:11:06,130 --> 00:11:08,890
workshop but I feel like I learned a lot

232
00:11:08,890 --> 00:11:11,680
and Craig Coe is really open to feedback

233
00:11:11,680 --> 00:11:13,780
from people and interaction and

234
00:11:13,780 --> 00:11:15,880
contribution so I'm gonna talk about a

235
00:11:15,880 --> 00:11:18,160
couple of the things that our group

236
00:11:18,160 --> 00:11:20,500
leaders sort of brought up we broke into

237
00:11:20,500 --> 00:11:22,240
groups and had a lot of ideas

238
00:11:22,240 --> 00:11:24,550
surrounding the central concept so I'm

239
00:11:24,550 --> 00:11:25,660
not going to go into all of those

240
00:11:25,660 --> 00:11:28,210
various ideas but one of the first

241
00:11:28,210 --> 00:11:29,830
things that we talked about is something

242
00:11:29,830 --> 00:11:32,080
Craig Coe is working on the idea of

243
00:11:32,080 --> 00:11:34,120
basically coming up with a nutrition

244
00:11:34,120 --> 00:11:37,390
label for a news article and attaching

245
00:11:37,390 --> 00:11:40,570
that label to the article in this slide

246
00:11:40,570 --> 00:11:42,220
you can see an example of a u.s.

247
00:11:42,220 --> 00:11:44,440
nutrition label compared to one in the

248
00:11:44,440 --> 00:11:47,200
UK I personally did not know that the UK

249
00:11:47,200 --> 00:11:50,710
one is so much better it's I to me it's

250
00:11:50,710 --> 00:11:52,870
way more understandable just as a quick

251
00:11:52,870 --> 00:11:56,050
summary that red that red section has

252
00:11:56,050 --> 00:11:58,120
sort of like a dangerous or an unhealthy

253
00:11:58,120 --> 00:12:02,170
amount of sodium or whatever that is one

254
00:12:02,170 --> 00:12:05,140
of the things we talked about was how to

255
00:12:05,140 --> 00:12:07,210
make it legible so if you were going to

256
00:12:07,210 --> 00:12:09,720
do this idea if you're going to make a

257
00:12:09,720 --> 00:12:12,760
nutrition label for news how would you

258
00:12:12,760 --> 00:12:14,650
implement it and what would you want it

259
00:12:14,650 --> 00:12:18,280
to show so we talked about maybe a few

260
00:12:18,280 --> 00:12:20,260
indicators of journalism like

261
00:12:20,260 --> 00:12:24,910
transparency or sources or you know a

262
00:12:24,910 --> 00:12:26,860
few of the other elements of journalism

263
00:12:26,860 --> 00:12:28,180
that you might want to track but also

264
00:12:28,180 --> 00:12:30,310
how would you make it readable to the

265
00:12:30,310 --> 00:12:33,220
reader and how would you implement it so

266
00:12:33,220 --> 00:12:36,410
one thing that I think on

267
00:12:36,410 --> 00:12:38,869
or someone know Farida mentioned was

268
00:12:38,869 --> 00:12:41,029
that you probably would want to

269
00:12:41,029 --> 00:12:42,679
incorporate designers because for

270
00:12:42,679 --> 00:12:45,229
instance this UK label problem guessing

271
00:12:45,229 --> 00:12:47,600
had designers involved whereas the u.s.

272
00:12:47,600 --> 00:12:51,139
one maybe not so much so they have a

273
00:12:51,139 --> 00:12:53,209
website there on the slide if you are

274
00:12:53,209 --> 00:12:55,100
interested in this idea of coming up

275
00:12:55,100 --> 00:12:57,289
with nutrition labels or how those could

276
00:12:57,289 --> 00:12:59,269
be implemented if you follow that link

277
00:12:59,269 --> 00:13:03,109
it's just cred web org slash cc IV or

278
00:13:03,109 --> 00:13:06,259
you could just go to cred web org they

279
00:13:06,259 --> 00:13:07,910
have a list of a lot of the indicators

280
00:13:07,910 --> 00:13:09,169
they're thinking about and they're

281
00:13:09,169 --> 00:13:11,329
looking for more contribution and ideas

282
00:13:11,329 --> 00:13:14,749
related to that one of the other things

283
00:13:14,749 --> 00:13:17,600
we talked about was how to not just

284
00:13:17,600 --> 00:13:20,929
verify an image but how to study it in

285
00:13:20,929 --> 00:13:23,660
sort of a wider context so I come from a

286
00:13:23,660 --> 00:13:25,489
journalism background and there's a lot

287
00:13:25,489 --> 00:13:29,989
of emphasis on verification we're

288
00:13:29,989 --> 00:13:31,249
missing a couple slides that's okay

289
00:13:31,249 --> 00:13:35,089
there's a lot of emphasis on verifying

290
00:13:35,089 --> 00:13:36,559
an image like making sure that it's

291
00:13:36,559 --> 00:13:37,429
quote unquote

292
00:13:37,429 --> 00:13:39,799
real and that it does show what it

293
00:13:39,799 --> 00:13:42,649
pertains to show but it also might be

294
00:13:42,649 --> 00:13:44,239
even more important to know the context

295
00:13:44,239 --> 00:13:46,009
of the distribution of it so if it's a

296
00:13:46,009 --> 00:13:48,470
real image of Syria for example who

297
00:13:48,470 --> 00:13:50,779
shared that image so not just what

298
00:13:50,779 --> 00:13:53,089
Twitter but like which groups on Twitter

299
00:13:53,089 --> 00:13:55,160
and then ideally you would want to find

300
00:13:55,160 --> 00:13:58,669
out why they shared it so the frita

301
00:13:58,669 --> 00:14:00,949
presented this it actually should spa

302
00:14:00,949 --> 00:14:02,749
road from art history apparently but

303
00:14:02,749 --> 00:14:05,839
it's a way to analyze a photo or an

304
00:14:05,839 --> 00:14:08,689
image and basically learn a lot from it

305
00:14:08,689 --> 00:14:10,369
to give you a lot more context to work

306
00:14:10,369 --> 00:14:13,489
with i'll pause to let Jenny take a

307
00:14:13,489 --> 00:14:17,209
photo so we put focused a lot on context

308
00:14:17,209 --> 00:14:20,329
as well as the source and then when you

309
00:14:20,329 --> 00:14:22,339
get into things like memes where they

310
00:14:22,339 --> 00:14:25,100
add text or they add maybe fake context

311
00:14:25,100 --> 00:14:27,709
or other context to that basically how

312
00:14:27,709 --> 00:14:29,779
do you deal with that and incorporate it

313
00:14:29,779 --> 00:14:31,999
so these are also things that I think

314
00:14:31,999 --> 00:14:34,039
are very open for discussion and they

315
00:14:34,039 --> 00:14:35,839
would like to get feedback and

316
00:14:35,839 --> 00:14:38,119
contribution on so if you have any

317
00:14:38,119 --> 00:14:39,829
thoughts on that or if you'd like to get

318
00:14:39,829 --> 00:14:41,209
I know you couldn't really read that

319
00:14:41,209 --> 00:14:42,979
wheel if anyone wants a copy of that I

320
00:14:42,979 --> 00:14:45,409
think come see eat cred code that's all

321
00:14:45,409 --> 00:14:46,890
thanks guys

322
00:14:46,890 --> 00:14:50,019
[Applause]

323
00:14:50,019 --> 00:14:51,439
thank you

324
00:14:51,439 --> 00:14:55,730
Thank You Samantha fun fact the

325
00:14:55,730 --> 00:14:58,160
unofficial mascot of Krypto is the

326
00:14:58,160 --> 00:15:05,439
credible Hulk i rolls and groans and

327
00:15:05,439 --> 00:15:10,509
with that next slide please

328
00:15:10,509 --> 00:15:27,759
hi in this guy media literacy programs I

329
00:15:27,790 --> 00:15:30,619
don't know Media literacy let's check

330
00:15:30,619 --> 00:15:33,589
this up my name is Jesse Haas McCarthy

331
00:15:33,589 --> 00:15:35,059
I'm with Museum Edie and I had the

332
00:15:35,059 --> 00:15:36,350
pleasure of leading this breakout

333
00:15:36,350 --> 00:15:39,980
session and I won my group left before

334
00:15:39,980 --> 00:15:41,869
we came up with a title but - we use

335
00:15:41,869 --> 00:15:43,100
this term a lot and it was our

336
00:15:43,100 --> 00:15:45,019
jumping-off point what are the ultimate

337
00:15:45,019 --> 00:15:48,259
highest goals we want in the education

338
00:15:48,259 --> 00:15:50,089
or creating things around media literacy

339
00:15:50,089 --> 00:15:51,919
you know what is the ultimate dream what

340
00:15:51,919 --> 00:15:55,639
is the highest we can reach for and our

341
00:15:55,639 --> 00:15:59,480
brilliance was contributed by these

342
00:15:59,480 --> 00:16:01,339
individuals or individuals from these

343
00:16:01,339 --> 00:16:04,730
organizations here and we focus on the

344
00:16:04,730 --> 00:16:08,059
question of how can we build support for

345
00:16:08,059 --> 00:16:10,279
a media literacy push across society and

346
00:16:10,279 --> 00:16:11,509
what combination of news organizations

347
00:16:11,509 --> 00:16:14,029
tech companies and education groups can

348
00:16:14,029 --> 00:16:16,669
be assembled to drive that work and we

349
00:16:16,669 --> 00:16:18,199
did we jumped in with what would be our

350
00:16:18,199 --> 00:16:21,259
ultimate dream looks and I know you

351
00:16:21,259 --> 00:16:23,389
can't read them but we looked at things

352
00:16:23,389 --> 00:16:27,410
like power having true spoken to it

353
00:16:27,410 --> 00:16:28,970
everyone everywhere has to detect

354
00:16:28,970 --> 00:16:31,189
propaganda and wants to and as we

355
00:16:31,189 --> 00:16:33,799
synthesize all of our ultimate goals we

356
00:16:33,799 --> 00:16:35,419
came out with these sort of three key

357
00:16:35,419 --> 00:16:37,160
things that everyone everywhere

358
00:16:37,160 --> 00:16:39,259
participates in productive conversations

359
00:16:39,259 --> 00:16:40,850
based on respect for other's knowledge

360
00:16:40,850 --> 00:16:43,519
and expertise an understanding of why

361
00:16:43,519 --> 00:16:45,499
being informed matters and a desire to

362
00:16:45,499 --> 00:16:47,419
be challenged and become more informed

363
00:16:47,419 --> 00:16:50,809
and then we want to look at practical

364
00:16:50,809 --> 00:16:53,149
things these players in this

365
00:16:53,149 --> 00:16:56,119
conversation can do through some

366
00:16:56,119 --> 00:16:58,039
approaches and we looked at both the

367
00:16:58,039 --> 00:17:00,889
idea of local community-based

368
00:17:00,889 --> 00:17:03,529
and technology-based things like social

369
00:17:03,529 --> 00:17:05,809
media platforms if you want to see our

370
00:17:05,809 --> 00:17:07,730
brainstorming someone was kind enough to

371
00:17:07,730 --> 00:17:09,648
share it on Twitter and you can see it

372
00:17:09,648 --> 00:17:10,939
on the Newseum at Twitter our

373
00:17:10,939 --> 00:17:14,000
brainstorming posters but we sorta

374
00:17:14,000 --> 00:17:16,429
started playing around with social media

375
00:17:16,429 --> 00:17:19,220
platform is making an incentive or some

376
00:17:19,220 --> 00:17:21,529
sort of badge or something to encourage

377
00:17:21,529 --> 00:17:25,250
the idea of a truth identity so making

378
00:17:25,250 --> 00:17:27,888
being a truth finder a part of who you

379
00:17:27,888 --> 00:17:29,389
are and what you share on social media

380
00:17:29,389 --> 00:17:31,940
and sort of meters of engagement ways to

381
00:17:31,940 --> 00:17:33,980
fall and track that and then we looked

382
00:17:33,980 --> 00:17:36,260
at sort of community based organizations

383
00:17:36,260 --> 00:17:38,750
and how can we bring them into that

384
00:17:38,750 --> 00:17:40,610
bigger push with this Universal

385
00:17:40,610 --> 00:17:42,889
statement of shared values or creating a

386
00:17:42,889 --> 00:17:44,720
some sort of umbrella organization or

387
00:17:44,720 --> 00:17:47,210
group or a conference that these

388
00:17:47,210 --> 00:17:48,980
organizations could come to and share

389
00:17:48,980 --> 00:17:52,399
and local teams leading to all these

390
00:17:52,399 --> 00:17:54,950
different local environments but some of

391
00:17:54,950 --> 00:17:59,120
the challenges we saw in this is there's

392
00:17:59,120 --> 00:18:01,519
this challenge between wanting a media

393
00:18:01,519 --> 00:18:03,169
literate society as an empowerment of

394
00:18:03,169 --> 00:18:05,480
democracy and trying to create these

395
00:18:05,480 --> 00:18:07,250
ultimate programmatic goals or these

396
00:18:07,250 --> 00:18:09,620
ultimate creations where one person

397
00:18:09,620 --> 00:18:11,720
tells other people what to do to achieve

398
00:18:11,720 --> 00:18:14,120
this and that's about as undemocratic as

399
00:18:14,120 --> 00:18:14,690
it gets

400
00:18:14,690 --> 00:18:17,990
and so one participant posited this idea

401
00:18:17,990 --> 00:18:20,240
of we need to find a way to create one

402
00:18:20,240 --> 00:18:22,429
see that everyone gets on to that blooms

403
00:18:22,429 --> 00:18:24,110
a thousand flowers and these local

404
00:18:24,110 --> 00:18:27,740
community organizations and with that in

405
00:18:27,740 --> 00:18:32,029
mind we jumped into what can we agree on

406
00:18:32,029 --> 00:18:33,559
in there became this idea of what

407
00:18:33,559 --> 00:18:35,210
programs could be created that or

408
00:18:35,210 --> 00:18:37,190
collaborative as opposed to competitive

409
00:18:37,190 --> 00:18:39,409
across these different organizations and

410
00:18:39,409 --> 00:18:41,500
what that would look like

411
00:18:41,500 --> 00:18:44,269
give me let me get the notes up and this

412
00:18:44,269 --> 00:18:46,279
is where we really went big so we went

413
00:18:46,279 --> 00:18:48,320
bigger go home on this one so we saw a

414
00:18:48,320 --> 00:18:51,289
federal education mandate so that it

415
00:18:51,289 --> 00:18:54,110
became a national trend or or maybe you

416
00:18:54,110 --> 00:18:55,730
stepped down from that and we set a cool

417
00:18:55,730 --> 00:18:57,559
media literacy holiday that everyone

418
00:18:57,559 --> 00:19:00,110
could jump on to and participate on or

419
00:19:00,110 --> 00:19:02,240
create programming around there is a

420
00:19:02,240 --> 00:19:04,309
suggestion of a new nonprofit social

421
00:19:04,309 --> 00:19:06,470
media platform strictly for utilizing

422
00:19:06,470 --> 00:19:08,600
teaching and transmitting media literacy

423
00:19:08,600 --> 00:19:10,639
one that's not out to make profit whose

424
00:19:10,639 --> 00:19:13,549
goal is to be media literate as you

425
00:19:13,549 --> 00:19:14,330
share

426
00:19:14,330 --> 00:19:16,250
the idea of a media literacy could

427
00:19:16,250 --> 00:19:18,140
manifester that social media and

428
00:19:18,140 --> 00:19:20,270
community organizations can sign on to

429
00:19:20,270 --> 00:19:22,790
so sort of a unified goals that all of

430
00:19:22,790 --> 00:19:25,640
these groups can go to then take media

431
00:19:25,640 --> 00:19:28,910
literacy to where people are looking TV

432
00:19:28,910 --> 00:19:31,850
series comic book HBO special movie

433
00:19:31,850 --> 00:19:33,590
documentary go to where people are

434
00:19:33,590 --> 00:19:35,270
looking already and make it something

435
00:19:35,270 --> 00:19:38,900
they see media literacy Oscars an awards

436
00:19:38,900 --> 00:19:41,390
show for media literate organizations

437
00:19:41,390 --> 00:19:43,430
reward this make it something we're

438
00:19:43,430 --> 00:19:45,140
striving for an honorable to have

439
00:19:45,140 --> 00:19:46,880
received and then we sort of played

440
00:19:46,880 --> 00:19:48,860
around the idea of a hashtag codebook

441
00:19:48,860 --> 00:19:51,590
sort of like an ever changing codebook

442
00:19:51,590 --> 00:19:54,230
of hashtags that move and rotate enough

443
00:19:54,230 --> 00:19:55,970
so that media literacy organizations get

444
00:19:55,970 --> 00:19:57,800
them first and can push those ideas

445
00:19:57,800 --> 00:20:00,200
before they get co-opted by everyone

446
00:20:00,200 --> 00:20:01,730
who's just jumping on to what is

447
00:20:01,730 --> 00:20:05,090
trending and the goal and all of this

448
00:20:05,090 --> 00:20:07,730
was to create tangible things and I

449
00:20:07,730 --> 00:20:10,700
think my mojo disappeared thank you

450
00:20:10,700 --> 00:20:14,210
thank you thank you thank you Jesse and

451
00:20:14,210 --> 00:20:19,460
Barbara and the museum team for

452
00:20:19,460 --> 00:20:23,560
pie-in-the-sky media literacy programs

453
00:20:27,850 --> 00:20:29,710
changing people's behaviors to prevent

454
00:20:29,710 --> 00:20:34,030
miss info sharing gleb with the pro

455
00:20:34,030 --> 00:20:43,810
truth pledge hey everyone hey can you

456
00:20:43,810 --> 00:20:44,760
hear me now

457
00:20:44,760 --> 00:20:48,430
excellent good that was a great the

458
00:20:48,430 --> 00:20:50,350
previous presentation was a great

459
00:20:50,350 --> 00:20:51,940
preparation for this presentation

460
00:20:51,940 --> 00:20:53,650
because talking about a badge is the

461
00:20:53,650 --> 00:20:54,820
approach you've pledge is a perfect

462
00:20:54,820 --> 00:20:58,120
badge of ways that you can actually mark

463
00:20:58,120 --> 00:21:00,790
yourself as being truthful and committed

464
00:21:00,790 --> 00:21:03,790
to truthfulness but I'll mention the

465
00:21:03,790 --> 00:21:04,810
approach you've pledged throughout the

466
00:21:04,810 --> 00:21:07,390
presentation but here the presentation

467
00:21:07,390 --> 00:21:10,600
focuses on a broader concept the

468
00:21:10,600 --> 00:21:12,100
approach of pledge is only one strategy

469
00:21:12,100 --> 00:21:14,680
of getting both private citizens and

470
00:21:14,680 --> 00:21:17,680
public figures to prevent the sharing of

471
00:21:17,680 --> 00:21:20,080
misinformation by these folks so how do

472
00:21:20,080 --> 00:21:22,240
we do it the first thing is to look at

473
00:21:22,240 --> 00:21:25,060
why people share misinformation what

474
00:21:25,060 --> 00:21:26,740
happens why do they share misinformation

475
00:21:26,740 --> 00:21:29,170
well there are several reasons according

476
00:21:29,170 --> 00:21:30,790
to the research in behavioral science

477
00:21:30,790 --> 00:21:32,950
and that's my area of expertise there

478
00:21:32,950 --> 00:21:34,780
are no penalties for doing it there are

479
00:21:34,780 --> 00:21:35,950
no penalties for sharing this

480
00:21:35,950 --> 00:21:37,990
information that's one major reason why

481
00:21:37,990 --> 00:21:40,330
people share invested information they

482
00:21:40,330 --> 00:21:42,310
can still proceed themselves as overall

483
00:21:42,310 --> 00:21:45,100
honest when they share misinformation if

484
00:21:45,100 --> 00:21:47,530
they can do that that contributes to

485
00:21:47,530 --> 00:21:49,480
sharing this information if they see

486
00:21:49,480 --> 00:21:51,850
others both their opponents and allies

487
00:21:51,850 --> 00:21:53,950
are sharing this information that makes

488
00:21:53,950 --> 00:21:55,300
them more likely to share misinformation

489
00:21:55,300 --> 00:21:57,490
and finally if they see it as helping

490
00:21:57,490 --> 00:22:00,340
their in-group their tribe that

491
00:22:00,340 --> 00:22:01,780
contributes to sharing this information

492
00:22:01,780 --> 00:22:03,790
there are also certain things that

493
00:22:03,790 --> 00:22:05,410
discourage the sharing of misinformation

494
00:22:05,410 --> 00:22:07,660
some of them the flip side of what we

495
00:22:07,660 --> 00:22:10,000
talked about so for example if we make

496
00:22:10,000 --> 00:22:12,640
standards about truthfulness clear if

497
00:22:12,640 --> 00:22:15,400
it's not an under doubt what does it

498
00:22:15,400 --> 00:22:17,020
mean to be true for what does it mean to

499
00:22:17,020 --> 00:22:19,240
be honest that discourages the sharing

500
00:22:19,240 --> 00:22:21,490
of information a pre commitment to

501
00:22:21,490 --> 00:22:23,560
truthful behavior like honor codes of

502
00:22:23,560 --> 00:22:26,700
various sorts have been shown to clearly

503
00:22:26,700 --> 00:22:30,010
encourage truthfulness reminders various

504
00:22:30,010 --> 00:22:31,690
sorts of reminders about what truthful

505
00:22:31,690 --> 00:22:34,330
behavior means interestingly there was a

506
00:22:34,330 --> 00:22:36,670
study that showed that the 10

507
00:22:36,670 --> 00:22:38,230
commandments shown somebody the Ten

508
00:22:38,230 --> 00:22:40,390
Commandments makes him or her more

509
00:22:40,390 --> 00:22:41,620
likely to be truthful

510
00:22:41,620 --> 00:22:43,810
even if that person is an atheist it's

511
00:22:43,810 --> 00:22:46,270
just a priming of truthfulness reminds

512
00:22:46,270 --> 00:22:47,580
them of what being truthful means

513
00:22:47,580 --> 00:22:51,190
perceiving your in-group as discouraging

514
00:22:51,190 --> 00:22:53,500
misinformation sharing that's another

515
00:22:53,500 --> 00:22:55,900
factor that is crucial and finally

516
00:22:55,900 --> 00:22:58,000
having a self perception a personal

517
00:22:58,000 --> 00:23:01,000
identity as committed to truthfulness so

518
00:23:01,000 --> 00:23:02,740
as some previous group discussed

519
00:23:02,740 --> 00:23:07,180
something like a badge so for public

520
00:23:07,180 --> 00:23:09,190
figures there is an additional dynamic

521
00:23:09,190 --> 00:23:12,400
and this includes reputational rewards

522
00:23:12,400 --> 00:23:14,800
for honesty and reputation on penalties

523
00:23:14,800 --> 00:23:16,180
for the psalmist you so those two things

524
00:23:16,180 --> 00:23:18,220
are additional things that we want

525
00:23:18,220 --> 00:23:20,560
public figures and organizations those

526
00:23:20,560 --> 00:23:22,510
are specifically relevant to them so

527
00:23:22,510 --> 00:23:24,730
what did we come up with we came up with

528
00:23:24,730 --> 00:23:26,530
first we talked about private citizens

529
00:23:26,530 --> 00:23:28,000
and here's some things for private

530
00:23:28,000 --> 00:23:29,170
citizens that are going to be relevant

531
00:23:29,170 --> 00:23:32,050
we want to invest positive emotions into

532
00:23:32,050 --> 00:23:33,940
being truthful and encourage you owner

533
00:23:33,940 --> 00:23:36,160
ability around truthfulness so saying

534
00:23:36,160 --> 00:23:38,590
people saying I don't know what I'm not

535
00:23:38,590 --> 00:23:42,100
sure being able to admit that right now

536
00:23:42,100 --> 00:23:44,560
as opposed to feeling defensive and

537
00:23:44,560 --> 00:23:47,110
having to have all the answers we want

538
00:23:47,110 --> 00:23:48,970
to be empathetic but it's still very

539
00:23:48,970 --> 00:23:51,280
important to call out people who share

540
00:23:51,280 --> 00:23:52,870
misinformation otherwise there's not

541
00:23:52,870 --> 00:23:54,850
going to be any penalties for sharing

542
00:23:54,850 --> 00:23:56,320
this information so empathetically

543
00:23:56,320 --> 00:23:58,780
calling these people out we want to show

544
00:23:58,780 --> 00:24:01,540
people how sharing misinformation harms

545
00:24:01,540 --> 00:24:03,970
the ring' group that might involve

546
00:24:03,970 --> 00:24:05,710
reframing who they perceive as their

547
00:24:05,710 --> 00:24:07,750
in-group so for example if right now

548
00:24:07,750 --> 00:24:10,570
people perceive sharing this information

549
00:24:10,570 --> 00:24:12,940
as benefiting their political cause

550
00:24:12,940 --> 00:24:15,820
their ideology if we can reframe the

551
00:24:15,820 --> 00:24:17,530
ring group as being about the whole

552
00:24:17,530 --> 00:24:19,660
country as being about democracy as

553
00:24:19,660 --> 00:24:21,370
being about protecting the United States

554
00:24:21,370 --> 00:24:23,740
of America or other countries and which

555
00:24:23,740 --> 00:24:26,440
their context is that would be helpful

556
00:24:26,440 --> 00:24:29,590
so that's another dynamic and finally we

557
00:24:29,590 --> 00:24:31,510
want to prime people for cultural

558
00:24:31,510 --> 00:24:33,520
reference of truthfulness so it's

559
00:24:33,520 --> 00:24:35,590
something like the boy who cried wolf or

560
00:24:35,590 --> 00:24:38,080
Pinocchio or something like that so

561
00:24:38,080 --> 00:24:39,640
cultural reference that make them think

562
00:24:39,640 --> 00:24:40,870
about truthfulness they're going to be

563
00:24:40,870 --> 00:24:46,000
helpful and for public for public

564
00:24:46,000 --> 00:24:48,040
figures went onto public figures now

565
00:24:48,040 --> 00:24:49,930
broadly speaking this is applicable to

566
00:24:49,930 --> 00:24:52,690
all public figures all organizations we

567
00:24:52,690 --> 00:24:55,470
thought that social profiles can be read

568
00:24:55,470 --> 00:24:58,330
based on their truthfulness the pages of

569
00:24:58,330 --> 00:25:00,519
public figures you know whether people

570
00:25:00,519 --> 00:25:02,619
on the left pick on the right center or

571
00:25:02,619 --> 00:25:04,570
whatever can be rated based on how

572
00:25:04,570 --> 00:25:06,940
truthful they are if there's a rating

573
00:25:06,940 --> 00:25:08,919
system of some sorts and if the whole

574
00:25:08,919 --> 00:25:12,309
page can be G emphasized by social media

575
00:25:12,309 --> 00:25:14,830
platforms if it consistently shares

576
00:25:14,830 --> 00:25:17,980
misinformation so having the whole page

577
00:25:17,980 --> 00:25:20,019
not specific articles BD emphasized but

578
00:25:20,019 --> 00:25:22,090
the whole page and finally we thought

579
00:25:22,090 --> 00:25:24,820
about if there can be a reward system

580
00:25:24,820 --> 00:25:26,799
for politicians right now we thought

581
00:25:26,799 --> 00:25:28,570
about the environmental movement how it

582
00:25:28,570 --> 00:25:31,269
was incredibly successful created

583
00:25:31,269 --> 00:25:33,460
essentially out of nothing in the 1950s

584
00:25:33,460 --> 00:25:35,529
and 1960s became an incredibly

585
00:25:35,529 --> 00:25:38,049
successful movement that was really

586
00:25:38,049 --> 00:25:40,899
powerful you know Rachel Carson's Silent

587
00:25:40,899 --> 00:25:43,929
Spring was published 1962 in 1970

588
00:25:43,929 --> 00:25:45,730
Richard Nixon created an Environmental

589
00:25:45,730 --> 00:25:49,029
Protection Agency so really powerful and

590
00:25:49,029 --> 00:25:52,059
if that Lobby woodrick would reward

591
00:25:52,059 --> 00:25:55,090
politicians who are honest by money

592
00:25:55,090 --> 00:25:58,299
volunteer time and votes so having

593
00:25:58,299 --> 00:26:00,549
something like that so these are all the

594
00:26:00,549 --> 00:26:01,899
strategies that we talked about now the

595
00:26:01,899 --> 00:26:03,999
pro truth pledges hits a number of those

596
00:26:03,999 --> 00:26:06,159
strategies but there are so many other

597
00:26:06,159 --> 00:26:08,769
things that can be done in this fear to

598
00:26:08,769 --> 00:26:10,809
change people's behavior change people's

599
00:26:10,809 --> 00:26:13,090
incentives both private citizens and

600
00:26:13,090 --> 00:26:15,009
public figures to help them avoid

601
00:26:15,009 --> 00:26:17,169
sharing misinformation and I hope all of

602
00:26:17,169 --> 00:26:19,269
you contribute some ideas and think

603
00:26:19,269 --> 00:26:21,549
about how you can change the behavior of

604
00:26:21,549 --> 00:26:24,039
people you know of public figures you

605
00:26:24,039 --> 00:26:26,350
know to encourage them to avoid sharing

606
00:26:26,350 --> 00:26:27,220
misinformation

607
00:26:27,220 --> 00:26:34,869
thank you thank you Gleb with Pro truth

608
00:26:34,869 --> 00:26:39,220
pledge thanks what we have our

609
00:26:39,220 --> 00:26:43,989
penultimate our pen ultimate workshop

610
00:26:43,989 --> 00:26:46,149
presentation coming up standard

611
00:26:46,149 --> 00:26:52,960
certification Scott a little different

612
00:26:52,960 --> 00:26:58,359
hi thank you very much I'm I'm combating

613
00:26:58,359 --> 00:27:01,179
the jinx of Kiev because I I presented

614
00:27:01,179 --> 00:27:05,619
it Kiev about Arkady Benko when he was

615
00:27:05,619 --> 00:27:08,769
dead and then I left and he was alive

616
00:27:08,769 --> 00:27:09,400
again

617
00:27:09,400 --> 00:27:12,700
and setting journalism back 50 years and

618
00:27:12,700 --> 00:27:14,710
so I said to the group I can't present

619
00:27:14,710 --> 00:27:18,490
because of this jinx and they all said

620
00:27:18,490 --> 00:27:20,620
whatever get up there and go do the

621
00:27:20,620 --> 00:27:22,540
presentation so anyway thank you very

622
00:27:22,540 --> 00:27:23,800
much we talked about standards to

623
00:27:23,800 --> 00:27:26,770
certification the there were three of us

624
00:27:26,770 --> 00:27:30,070
oh oh this is gonna be a totally

625
00:27:30,070 --> 00:27:31,360
different presentation that was plenty

626
00:27:31,360 --> 00:27:33,400
done because this is a this is a grab

627
00:27:33,400 --> 00:27:36,720
for what I submitted it to you okay so

628
00:27:36,720 --> 00:27:39,130
it's changed a little bit since that but

629
00:27:39,130 --> 00:27:41,980
all right so it was organized by three

630
00:27:41,980 --> 00:27:46,059
groups which the w3c cred web group

631
00:27:46,059 --> 00:27:48,100
which you can find at cred web org

632
00:27:48,100 --> 00:27:50,140
Sandro is here and be sure to talk to

633
00:27:50,140 --> 00:27:52,450
him about the involvement of the w3c in

634
00:27:52,450 --> 00:27:56,260
the standard setting process for the

635
00:27:56,260 --> 00:27:59,020
idea of a more credible web reporters

636
00:27:59,020 --> 00:28:00,370
without borders known by its French

637
00:28:00,370 --> 00:28:03,850
initials RSF Olaf is here and you should

638
00:28:03,850 --> 00:28:05,590
talk to him about their efforts and then

639
00:28:05,590 --> 00:28:08,050
my organizations the certified content

640
00:28:08,050 --> 00:28:10,030
coalition and you can find out more on

641
00:28:10,030 --> 00:28:15,400
the website there we wanted to start out

642
00:28:15,400 --> 00:28:16,720
we thought we'd be able to do a lot of

643
00:28:16,720 --> 00:28:19,020
great work and and maybe get through a

644
00:28:19,020 --> 00:28:21,250
series of questions and we realized we

645
00:28:21,250 --> 00:28:22,240
could really only get through one

646
00:28:22,240 --> 00:28:24,270
question how to build a hard to game

647
00:28:24,270 --> 00:28:27,010
journalism standard and then we came up

648
00:28:27,010 --> 00:28:30,040
with one standard and we thought we're

649
00:28:30,040 --> 00:28:31,390
just going to do this one standard we're

650
00:28:31,390 --> 00:28:32,710
just going to tackle this one and this

651
00:28:32,710 --> 00:28:34,900
is the easiest one provide full

652
00:28:34,900 --> 00:28:36,640
transparency and identity of revenue

653
00:28:36,640 --> 00:28:38,410
that'll be a standard that we're all

654
00:28:38,410 --> 00:28:40,780
going to be able to agree on quickly and

655
00:28:40,780 --> 00:28:42,580
then we'll be able to move on to the

656
00:28:42,580 --> 00:28:44,140
next standard and so we started talking

657
00:28:44,140 --> 00:28:46,150
about that one standard and we realized

658
00:28:46,150 --> 00:28:49,059
that it's not at all easy this was

659
00:28:49,059 --> 00:28:50,830
incredibly difficult there's a bunch of

660
00:28:50,830 --> 00:28:54,670
stuff that goes on and you can go to the

661
00:28:54,670 --> 00:28:56,020
Twitter handle and you can go to

662
00:28:56,020 --> 00:28:57,520
misanthrope kinda get the latest version

663
00:28:57,520 --> 00:28:59,020
of this so you don't have to read this

664
00:28:59,020 --> 00:29:00,400
but I can tell you coming up with

665
00:29:00,400 --> 00:29:03,790
standards is incredibly hard but that

666
00:29:03,790 --> 00:29:07,660
being said it's important to try it's

667
00:29:07,660 --> 00:29:09,040
really hard to come up with these

668
00:29:09,040 --> 00:29:13,360
standards but if we don't do it somebody

669
00:29:13,360 --> 00:29:14,800
else will let's see what this side okay

670
00:29:14,800 --> 00:29:16,780
so the new version of the slide is so

671
00:29:16,780 --> 00:29:19,960
this is organ Putin and Elon Musk who

672
00:29:19,960 --> 00:29:21,190
would all like to come up with their

673
00:29:21,190 --> 00:29:22,559
standards for

674
00:29:22,559 --> 00:29:24,659
journalism is and so if if the people

675
00:29:24,659 --> 00:29:26,639
the smart people in this room decide not

676
00:29:26,639 --> 00:29:29,100
to do it just because it's hard then

677
00:29:29,100 --> 00:29:31,049
other people will fill in the vacuum and

678
00:29:31,049 --> 00:29:32,399
that would be really terrible the new

679
00:29:32,399 --> 00:29:34,080
version of the slide has a big arrow on

680
00:29:34,080 --> 00:29:37,049
it and the arrow was pointing this way

681
00:29:37,049 --> 00:29:39,659
and so it's the other people that will

682
00:29:39,659 --> 00:29:42,360
try to regulate it are either that but

683
00:29:42,360 --> 00:29:43,950
that building if you're looking at these

684
00:29:43,950 --> 00:29:45,749
monitors or in the White House if you're

685
00:29:45,749 --> 00:29:48,179
looking at those monitors and and we

686
00:29:48,179 --> 00:29:50,279
don't want either of those to happen so

687
00:29:50,279 --> 00:29:52,679
so that's it the other slide that we

688
00:29:52,679 --> 00:29:54,090
have was thanks very much we had an

689
00:29:54,090 --> 00:29:56,429
amazing amazing pure super super helpful

690
00:29:56,429 --> 00:29:58,379
and we're very appreciative of having

691
00:29:58,379 --> 00:30:00,749
the time to present here and let's hope

692
00:30:00,749 --> 00:30:03,149
now the jinx is broken Thank You sky

693
00:30:03,149 --> 00:30:07,499
thank you our last workshop IREX get up

694
00:30:07,499 --> 00:30:09,740
here

695
00:30:16,620 --> 00:30:18,840
solving the citizens side of the

696
00:30:18,840 --> 00:30:23,130
equation shouldn't have sat in the back

697
00:30:23,130 --> 00:30:28,610
it's a long wait to use this clicker

698
00:30:29,360 --> 00:30:33,570
Greene ok so what we did was we let a

699
00:30:33,570 --> 00:30:35,760
Design Thinking workshop with a few

700
00:30:35,760 --> 00:30:38,880
groups that were interested in tackling

701
00:30:38,880 --> 00:30:41,730
these three themes so helping citizens

702
00:30:41,730 --> 00:30:44,550
separate fact from fiction helping us

703
00:30:44,550 --> 00:30:47,010
measure the impact of our efforts and

704
00:30:47,010 --> 00:30:50,670
helping citizens actively seek quality

705
00:30:50,670 --> 00:30:54,000
information so each of the participants

706
00:30:54,000 --> 00:30:55,770
got to choose whichever one of these

707
00:30:55,770 --> 00:30:58,140
themes that they wanted to take part in

708
00:30:58,140 --> 00:31:01,830
we had groups of two to three in each of

709
00:31:01,830 --> 00:31:05,970
them these are the participants having

710
00:31:05,970 --> 00:31:08,330
lots of fun with their sticky notes

711
00:31:08,330 --> 00:31:12,360
there you go and then these were their

712
00:31:12,360 --> 00:31:14,190
ideas that they came up with the purpose

713
00:31:14,190 --> 00:31:17,010
of these of this actual workshop what's

714
00:31:17,010 --> 00:31:19,290
not to come up with ideas that are gonna

715
00:31:19,290 --> 00:31:21,150
change the world but to really

716
00:31:21,150 --> 00:31:23,130
understand the process that you need to

717
00:31:23,130 --> 00:31:25,010
go through in order to try and innovate

718
00:31:25,010 --> 00:31:29,429
using design thinking techniques so the

719
00:31:29,429 --> 00:31:31,940
first group came up with a user driven

720
00:31:31,940 --> 00:31:33,900
browser fact-checker

721
00:31:33,900 --> 00:31:37,170
the second group - which wanted to

722
00:31:37,170 --> 00:31:39,600
measure the impact of our efforts to

723
00:31:39,600 --> 00:31:42,929
find misinformation thought of an

724
00:31:42,929 --> 00:31:45,179
incentivized marketplace for citizens to

725
00:31:45,179 --> 00:31:48,679
report fake news and in the last group

726
00:31:48,679 --> 00:31:51,570
wanted to increase journalism's

727
00:31:51,570 --> 00:31:54,179
responsiveness to people's needs by

728
00:31:54,179 --> 00:31:58,110
finding more time and space for them so

729
00:31:58,110 --> 00:32:00,030
these are the main reflections that came

730
00:32:00,030 --> 00:32:02,520
out of the workshop these reflections

731
00:32:02,520 --> 00:32:05,570
were from the participants themselves I

732
00:32:05,570 --> 00:32:08,070
can read them we must avoid the tendency

733
00:32:08,070 --> 00:32:11,040
to view users as a monolithic unit and

734
00:32:11,040 --> 00:32:13,110
what this means is we shouldn't just

735
00:32:13,110 --> 00:32:14,970
think that all our solutions are going

736
00:32:14,970 --> 00:32:18,870
to be adapt for everyone around the

737
00:32:18,870 --> 00:32:22,170
world every solution is contextual and

738
00:32:22,170 --> 00:32:24,450
we need to look at users in that same

739
00:32:24,450 --> 00:32:27,720
way there is a percentage of people who

740
00:32:27,720 --> 00:32:29,550
we believe will never change their point

741
00:32:29,550 --> 00:32:30,330
of view

742
00:32:30,330 --> 00:32:33,330
so this is more of a question that the

743
00:32:33,330 --> 00:32:35,790
participants came up with do we want to

744
00:32:35,790 --> 00:32:39,300
cater all of our ideas to those people

745
00:32:39,300 --> 00:32:41,310
or do we want to cater it to the rest

746
00:32:41,310 --> 00:32:44,850
who we think are could go either way

747
00:32:44,850 --> 00:32:47,910
still it is important to consider the

748
00:32:47,910 --> 00:32:49,710
incentives for individuals to take part

749
00:32:49,710 --> 00:32:51,600
in any process relating to fighting

750
00:32:51,600 --> 00:32:53,700
misinformation this one speaks for

751
00:32:53,700 --> 00:32:56,190
itself and lastly we should look for

752
00:32:56,190 --> 00:32:58,290
solutions that capitalize on platforms

753
00:32:58,290 --> 00:33:00,750
whose users comprise the target audience

754
00:33:00,750 --> 00:33:03,510
this basically means once you've figured

755
00:33:03,510 --> 00:33:07,320
out who your users are just for you find

756
00:33:07,320 --> 00:33:09,540
the platforms that they're using right

757
00:33:09,540 --> 00:33:12,870
now so you can touch base with them and

758
00:33:12,870 --> 00:33:16,020
tackle that group appropriately thank

759
00:33:16,020 --> 00:33:16,710
you

760
00:33:16,710 --> 00:33:19,819
[Applause]

