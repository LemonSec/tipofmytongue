1
00:00:00,000 --> 00:00:02,580
all right thanks a lot I did have cynics

2
00:00:02,580 --> 00:00:05,520
in my presentation I just found out that

3
00:00:05,520 --> 00:00:08,099
by going second day towards the end my

4
00:00:08,099 --> 00:00:10,679
jokes have been told my content has been

5
00:00:10,679 --> 00:00:13,679
covered but bear with me some things

6
00:00:13,679 --> 00:00:16,279
will repeat and some things will be new

7
00:00:16,279 --> 00:00:18,750
for you and I've tried to take kind of

8
00:00:18,750 --> 00:00:21,020
an interesting slice and hopefully

9
00:00:21,020 --> 00:00:23,160
anticipating that we'd be having a lot

10
00:00:23,160 --> 00:00:24,720
of overlap so that so that we wouldn't

11
00:00:24,720 --> 00:00:27,630
so today I want to look at the same

12
00:00:27,630 --> 00:00:29,340
problem but we want to look at public

13
00:00:29,340 --> 00:00:33,360
institutions and addressing the problem

14
00:00:33,360 --> 00:00:35,010
of misinformation with these

15
00:00:35,010 --> 00:00:37,620
institutions okay this will also help

16
00:00:37,620 --> 00:00:39,270
lead up to a breakout session that my

17
00:00:39,270 --> 00:00:40,920
colleague Jesse McCarthy is going to

18
00:00:40,920 --> 00:00:42,660
leave this afternoon when we get to that

19
00:00:42,660 --> 00:00:44,550
so hopefully it helped us prepare for

20
00:00:44,550 --> 00:00:47,820
those who participate with us there all

21
00:00:47,820 --> 00:00:49,110
right let's start with some of the

22
00:00:49,110 --> 00:00:50,820
basics let's frame and all get on the

23
00:00:50,820 --> 00:00:54,180
same page so there is confusion around

24
00:00:54,180 --> 00:00:56,550
online information we all agree on that

25
00:00:56,550 --> 00:00:59,309
it does not discriminate by age and it

26
00:00:59,309 --> 00:01:04,199
does not discriminate by country these

27
00:01:04,199 --> 00:01:06,420
players have all contributed to the

28
00:01:06,420 --> 00:01:08,250
problem we know what those problem are

29
00:01:08,250 --> 00:01:10,350
the producers blurring the lines between

30
00:01:10,350 --> 00:01:14,760
fact and opinion bias sometimes sloppy

31
00:01:14,760 --> 00:01:19,619
reporting on the news aggregators unseen

32
00:01:19,619 --> 00:01:22,830
algorithms out there a business model

33
00:01:22,830 --> 00:01:27,119
that supports a sharing of emotional

34
00:01:27,119 --> 00:01:30,950
information and the amplification

35
00:01:30,950 --> 00:01:34,280
process in addition news consumers

36
00:01:34,280 --> 00:01:37,439
they're lazy thick ball not lazy we have

37
00:01:37,439 --> 00:01:40,259
bad habits we've created bad habits due

38
00:01:40,259 --> 00:01:42,000
to trying to deal with this firehose

39
00:01:42,000 --> 00:01:43,710
amount of information that comes at us

40
00:01:43,710 --> 00:01:47,430
every day and I think we should be

41
00:01:47,430 --> 00:01:49,049
allowed to sort information by

42
00:01:49,049 --> 00:01:50,820
preferences as long as we know what

43
00:01:50,820 --> 00:01:53,670
we're doing in that process because

44
00:01:53,670 --> 00:01:55,500
there's no way we can keep up with

45
00:01:55,500 --> 00:01:58,170
everything so these bad habits need to

46
00:01:58,170 --> 00:02:01,020
be changed they need to be fixed agreed

47
00:02:01,020 --> 00:02:02,610
so it's going to take all three of these

48
00:02:02,610 --> 00:02:07,619
players to help solve the problem all

49
00:02:07,619 --> 00:02:10,440
right so the problem with our news

50
00:02:10,440 --> 00:02:12,480
producers and with our aggregators

51
00:02:12,480 --> 00:02:13,510
helping are that people

52
00:02:13,510 --> 00:02:15,819
don't necessarily trust top-down

53
00:02:15,819 --> 00:02:19,030
solutions these stats prove that but

54
00:02:19,030 --> 00:02:20,950
since stats can be changed and

55
00:02:20,950 --> 00:02:23,260
manipulated we won't spend too much time

56
00:02:23,260 --> 00:02:26,140
on those but we do know there is not a

57
00:02:26,140 --> 00:02:28,360
lot of trust for Facebook and Google to

58
00:02:28,360 --> 00:02:31,060
make right decisions for us yes and we

59
00:02:31,060 --> 00:02:33,129
know there's a lot of trust lost for our

60
00:02:33,129 --> 00:02:35,560
free press for our media and the job

61
00:02:35,560 --> 00:02:37,629
that they're doing and I've even

62
00:02:37,629 --> 00:02:40,150
multiple studies that prove that and

63
00:02:40,150 --> 00:02:43,900
that show that but then there's also a

64
00:02:43,900 --> 00:02:46,410
little bit distrust for our governments

65
00:02:46,410 --> 00:02:48,640
government intervention as well and that

66
00:02:48,640 --> 00:02:50,590
lasts that if you could have read it if

67
00:02:50,590 --> 00:02:51,700
you were looking at this one app here

68
00:02:51,700 --> 00:02:53,019
you probably read it it showed that most

69
00:02:53,019 --> 00:02:54,569
people do not want the internet

70
00:02:54,569 --> 00:02:57,250
regulated even though if most folks do

71
00:02:57,250 --> 00:02:59,019
not want the internet regulated we are

72
00:02:59,019 --> 00:03:01,090
starting to see government's trying to

73
00:03:01,090 --> 00:03:03,299
respond and fix this issue of

74
00:03:03,299 --> 00:03:06,910
disinformation we talked yesterday about

75
00:03:06,910 --> 00:03:10,930
Germany and Germany finding social media

76
00:03:10,930 --> 00:03:12,849
platforms I think it's important to

77
00:03:12,849 --> 00:03:15,190
point out that the the fine is millions

78
00:03:15,190 --> 00:03:18,010
of dollars and if it's not removed

79
00:03:18,010 --> 00:03:21,310
within 24 hours that is most definitely

80
00:03:21,310 --> 00:03:25,260
going to infringe on free speech on

81
00:03:25,260 --> 00:03:27,099
wouldn't you err on the side of caution

82
00:03:27,099 --> 00:03:29,079
if you were that one who was pulling it

83
00:03:29,079 --> 00:03:32,019
off to save your company millions of

84
00:03:32,019 --> 00:03:36,700
dollars in addition we have bills

85
00:03:36,700 --> 00:03:39,370
proposed in France that would give

86
00:03:39,370 --> 00:03:42,579
judges emerg a grant judges emergency

87
00:03:42,579 --> 00:03:46,090
action to remove fake news to especially

88
00:03:46,090 --> 00:03:48,669
the fam'ly Tory fake news around

89
00:03:48,669 --> 00:03:51,700
sensitive election times what we didn't

90
00:03:51,700 --> 00:03:56,079
discuss yesterday was in Malaysia that

91
00:03:56,079 --> 00:03:57,879
they just passed this was April they

92
00:03:57,879 --> 00:04:00,459
passed a law about publishing and

93
00:04:00,459 --> 00:04:02,769
sharing fake news and that comes with a

94
00:04:02,769 --> 00:04:05,590
hefty six year and hundred and twenty

95
00:04:05,590 --> 00:04:08,709
eight thousand dollar potential fine

96
00:04:08,709 --> 00:04:12,310
there for sharing fake information the

97
00:04:12,310 --> 00:04:14,560
first person arrested charged with this

98
00:04:14,560 --> 00:04:16,500
was someone who shared a YouTube video

99
00:04:16,500 --> 00:04:19,478
claiming that it took police too long to

100
00:04:19,478 --> 00:04:22,360
respond to a shooting police say it was

101
00:04:22,360 --> 00:04:24,880
eight minutes versus fifty minutes so

102
00:04:24,880 --> 00:04:27,250
there again a gray area

103
00:04:27,250 --> 00:04:29,920
and finally the Kenyan president signed

104
00:04:29,920 --> 00:04:32,410
a bill criminalizing 17 different types

105
00:04:32,410 --> 00:04:35,020
of cyber crimes oh I did want to give

106
00:04:35,020 --> 00:04:37,090
you an update on the Malaysia and I'll

107
00:04:37,090 --> 00:04:38,790
go back to that in a second

108
00:04:38,790 --> 00:04:41,470
criminalizing 17 types of cyber crimes

109
00:04:41,470 --> 00:04:43,660
and fake news of people who knowingly

110
00:04:43,660 --> 00:04:46,960
share fake news can be fined up to fifty

111
00:04:46,960 --> 00:04:48,750
thousand dollars or jail for two years

112
00:04:48,750 --> 00:04:51,580
the Committee to Protect Journalists

113
00:04:51,580 --> 00:04:53,980
claimed that that would criminalize free

114
00:04:53,980 --> 00:04:56,680
speech and right now the courts have

115
00:04:56,680 --> 00:04:59,830
temporarily suspended these provisions

116
00:04:59,830 --> 00:05:02,230
seemed to endanger free speech and the

117
00:05:02,230 --> 00:05:04,480
case was set to be heard on July 18th

118
00:05:04,480 --> 00:05:07,180
and we're just waiting for a ruling but

119
00:05:07,180 --> 00:05:09,310
back to Malaysia on the update they're

120
00:05:09,310 --> 00:05:11,860
the country's new communications and

121
00:05:11,860 --> 00:05:13,510
multimedia minister said that the law

122
00:05:13,510 --> 00:05:18,100
would be repealed all right want to make

123
00:05:18,100 --> 00:05:19,450
one more example most of you are

124
00:05:19,450 --> 00:05:20,860
probably familiar with this

125
00:05:20,860 --> 00:05:24,640
India's 4G revolution has brought on

126
00:05:24,640 --> 00:05:28,210
over 200 million users online and in

127
00:05:28,210 --> 00:05:31,540
this popular platform started a message

128
00:05:31,540 --> 00:05:33,820
sharing platform started spreading fake

129
00:05:33,820 --> 00:05:38,380
news and resulted in mob killings the

130
00:05:38,380 --> 00:05:41,680
Indian government called for the the

131
00:05:41,680 --> 00:05:44,770
platform to fix this so the app now

132
00:05:44,770 --> 00:05:47,710
labels messages as forward it forward

133
00:05:47,710 --> 00:05:50,080
instead of composed by sender limits

134
00:05:50,080 --> 00:05:51,940
forwarding the five recipients inside

135
00:05:51,940 --> 00:05:53,950
India and 20 and the rest of the world

136
00:05:53,950 --> 00:05:56,650
and only group admins can send messages

137
00:05:56,650 --> 00:05:59,440
and content as marked can be marked as

138
00:05:59,440 --> 00:06:00,610
suspicious

139
00:06:00,610 --> 00:06:03,250
despite this the last mob killing was

140
00:06:03,250 --> 00:06:08,830
just two weeks ago which brings me to my

141
00:06:08,830 --> 00:06:11,530
point that we can't leave out consumer

142
00:06:11,530 --> 00:06:13,330
education when trying to combat this

143
00:06:13,330 --> 00:06:15,490
problem the two building blocks are

144
00:06:15,490 --> 00:06:17,590
personal responsibility we talked about

145
00:06:17,590 --> 00:06:19,419
that yesterday folks need to understand

146
00:06:19,419 --> 00:06:21,430
their role and a healthy information

147
00:06:21,430 --> 00:06:23,620
cycle and we need to support community

148
00:06:23,620 --> 00:06:27,610
education so what about museums that's

149
00:06:27,610 --> 00:06:28,780
what we're going to talk about today

150
00:06:28,780 --> 00:06:32,979
there are 50,000 museums and 202

151
00:06:32,979 --> 00:06:35,830
countries around the world and that's

152
00:06:35,830 --> 00:06:37,360
according to the International Committee

153
00:06:37,360 --> 00:06:39,849
of museums and there's no way to know

154
00:06:39,849 --> 00:06:40,569
that number for

155
00:06:40,569 --> 00:06:42,309
or for anyone who's questioning that but

156
00:06:42,309 --> 00:06:45,729
that gives you a sense of an idea and

157
00:06:45,729 --> 00:06:48,990
visitorship in museums are growing

158
00:06:48,990 --> 00:06:53,229
people trust museums they are typically

159
00:06:53,229 --> 00:06:55,839
seen as non-political and they're seen

160
00:06:55,839 --> 00:06:59,050
as educational most people trust museums

161
00:06:59,050 --> 00:07:00,459
more than they'd like for example

162
00:07:00,459 --> 00:07:01,719
history museum trust

163
00:07:01,719 --> 00:07:03,819
museum more than they trust their

164
00:07:03,819 --> 00:07:06,399
teacher or they trust a textbook or they

165
00:07:06,399 --> 00:07:11,559
trust a relative people and when it

166
00:07:11,559 --> 00:07:13,119
comes to their mission people also

167
00:07:13,119 --> 00:07:15,099
believe that museums should make

168
00:07:15,099 --> 00:07:17,169
recommendations on actions based on

169
00:07:17,169 --> 00:07:21,939
their topic all right so we need to go

170
00:07:21,939 --> 00:07:24,069
grassroots there was approved there's

171
00:07:24,069 --> 00:07:26,469
proof that this works and Uganda a study

172
00:07:26,469 --> 00:07:28,899
of 10,000 students through 120 schools

173
00:07:28,899 --> 00:07:31,809
half of those students the students that

174
00:07:31,809 --> 00:07:33,699
were trained were twice as likely to be

175
00:07:33,699 --> 00:07:37,599
able to identify bs claims on health

176
00:07:37,599 --> 00:07:39,519
issues so we know it works and we know

177
00:07:39,519 --> 00:07:41,559
some adults are showing that they would

178
00:07:41,559 --> 00:07:44,409
like to receive some of this some of

179
00:07:44,409 --> 00:07:46,749
this training themselves at the Newseum

180
00:07:46,749 --> 00:07:50,529
we are working to help combat this

181
00:07:50,529 --> 00:07:53,319
problem through infographics that and we

182
00:07:53,319 --> 00:07:55,809
supply on our website on Newseum ed org

183
00:07:55,809 --> 00:07:57,519
like the ones you see here these are

184
00:07:57,519 --> 00:07:59,709
just a small sampling of the things we

185
00:07:59,709 --> 00:08:02,889
offer so we are supporting teachers we

186
00:08:02,889 --> 00:08:05,169
are offering public programs on the

187
00:08:05,169 --> 00:08:08,559
issue we are going into libraries we are

188
00:08:08,559 --> 00:08:10,509
doing programs with the State Department

189
00:08:10,509 --> 00:08:12,610
we are doing discussions and lectures

190
00:08:12,610 --> 00:08:15,939
and workshops all around the country on

191
00:08:15,939 --> 00:08:19,839
this topic so we're having a big a big

192
00:08:19,839 --> 00:08:22,079
impact I do want to point out though as

193
00:08:22,079 --> 00:08:24,399
important it is I see that you see these

194
00:08:24,399 --> 00:08:26,259
are checklists these are processes

195
00:08:26,259 --> 00:08:28,659
teachers like those you have to do that

196
00:08:28,659 --> 00:08:30,669
but along with the health the Newseum

197
00:08:30,669 --> 00:08:34,059
teaches the why the importance behind it

198
00:08:34,059 --> 00:08:35,698
the responsibility how our democracy

199
00:08:35,698 --> 00:08:40,688
depends on informed effective consumers

200
00:08:40,688 --> 00:08:43,360
so we get into those gray areas and then

201
00:08:43,360 --> 00:08:45,009
we have people then the consumers and

202
00:08:45,009 --> 00:08:47,649
the students and the adults were whoever

203
00:08:47,649 --> 00:08:49,449
we're talking to they get behind on this

204
00:08:49,449 --> 00:08:50,769
issue and then they all realize that

205
00:08:50,769 --> 00:08:54,070
they do contribute to the in

206
00:08:54,070 --> 00:08:58,000
environment so the question is investing

207
00:08:58,000 --> 00:09:00,880
in in public serving institutions an

208
00:09:00,880 --> 00:09:03,550
alternative to policy that is going to

209
00:09:03,550 --> 00:09:06,490
be our session our breakout session this

210
00:09:06,490 --> 00:09:08,830
afternoon and so we hope that you come

211
00:09:08,830 --> 00:09:10,570
and join our session so we can look at

212
00:09:10,570 --> 00:09:13,750
how maybe news producers educational

213
00:09:13,750 --> 00:09:16,630
institutions and and tech and and others

214
00:09:16,630 --> 00:09:18,040
can work together to help solve this

215
00:09:18,040 --> 00:09:19,780
problem thank you very much

216
00:09:19,780 --> 00:09:28,250
[Applause]

