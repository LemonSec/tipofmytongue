1
00:00:00,829 --> 00:00:03,958
hello everybody I'm Phil from Indiana

2
00:00:03,959 --> 00:00:07,589
University and I'm a little bit mad at

3
00:00:07,589 --> 00:00:09,320
white because he didn't include my

4
00:00:09,320 --> 00:00:14,130
clickbait title in the program so I'll

5
00:00:14,130 --> 00:00:15,870
be talking about four reasons why social

6
00:00:15,870 --> 00:00:17,220
networks make us vulnerable to

7
00:00:17,220 --> 00:00:19,740
misinformation and this is meant to be a

8
00:00:19,740 --> 00:00:21,570
very quick overview of the research that

9
00:00:21,570 --> 00:00:26,570
we do in my lab so this plot shows a

10
00:00:26,570 --> 00:00:30,900
statistical distribution of the virality

11
00:00:30,900 --> 00:00:33,390
measured in one way you can measure it

12
00:00:33,390 --> 00:00:34,860
in multiple different ways like how many

13
00:00:34,860 --> 00:00:38,280
people are exposed to articles or how

14
00:00:38,280 --> 00:00:40,140
many people share articles and so on

15
00:00:40,140 --> 00:00:43,710
this in particular focusing on various

16
00:00:43,710 --> 00:00:45,450
misinformation sources and also

17
00:00:45,450 --> 00:00:47,460
comparing that with fact-checking

18
00:00:47,460 --> 00:00:50,309
articles and what this shows you is that

19
00:00:50,309 --> 00:00:53,640
there is a significant not

20
00:00:53,640 --> 00:00:57,300
non-negligible chance that information

21
00:00:57,300 --> 00:00:59,520
content from these sources locally

22
00:00:59,520 --> 00:01:01,890
bility sources go viral so that they are

23
00:01:01,890 --> 00:01:04,920
seen and shared tens of thousands of

24
00:01:04,920 --> 00:01:07,020
times or hundreds of thousands of times

25
00:01:07,020 --> 00:01:09,360
on platforms like Twitter and which is

26
00:01:09,360 --> 00:01:11,610
this data is from and we've observed the

27
00:01:11,610 --> 00:01:14,130
same pattern on Facebook so what does

28
00:01:14,130 --> 00:01:14,729
this mean

29
00:01:14,729 --> 00:01:15,930
so if you look at the tail of the

30
00:01:15,930 --> 00:01:19,710
distribution that's where we find

31
00:01:19,710 --> 00:01:22,170
examples of viral pieces of

32
00:01:22,170 --> 00:01:23,640
misinformation this is an example that

33
00:01:23,640 --> 00:01:25,920
I'm sure many of you have seen it spread

34
00:01:25,920 --> 00:01:27,360
virally around the elections about

35
00:01:27,360 --> 00:01:31,470
spirit cooking and what we look is at

36
00:01:31,470 --> 00:01:33,479
the network of diffusion of this

37
00:01:33,479 --> 00:01:35,250
misinformation so that's what you see

38
00:01:35,250 --> 00:01:36,270
there in the picture that's the

39
00:01:36,270 --> 00:01:38,130
diffusion network where the nodes are

40
00:01:38,130 --> 00:01:40,950
Twitter accounts the links between these

41
00:01:40,950 --> 00:01:43,229
nodes represent that particular article

42
00:01:43,229 --> 00:01:48,470
being shared through retweeting quoting

43
00:01:48,470 --> 00:01:52,439
mentioning or replying and so you build

44
00:01:52,439 --> 00:01:53,880
these networks and we study the

45
00:01:53,880 --> 00:01:55,619
structure of these networks and what

46
00:01:55,619 --> 00:01:57,390
what it tells us about how information

47
00:01:57,390 --> 00:02:00,329
spreads so what you could can observe

48
00:02:00,329 --> 00:02:01,979
for example is that some nodes are very

49
00:02:01,979 --> 00:02:03,479
large and those are the influential

50
00:02:03,479 --> 00:02:05,369
nodes that are retweeted many times in

51
00:02:05,369 --> 00:02:07,140
the context of in this case this

52
00:02:07,140 --> 00:02:09,869
particular piece of fake news you also

53
00:02:09,869 --> 00:02:12,930
see that nodes are colored and that's to

54
00:02:12,930 --> 00:02:13,610
show

55
00:02:13,610 --> 00:02:17,720
the role played by BOTS so red nodes are

56
00:02:17,720 --> 00:02:19,430
the ones that are likely bots and you

57
00:02:19,430 --> 00:02:20,870
can see several of them especially on

58
00:02:20,870 --> 00:02:23,180
the periphery sort of injecting this

59
00:02:23,180 --> 00:02:24,890
particular piece of misinformation so

60
00:02:24,890 --> 00:02:27,980
I'll come back later to the role of bots

61
00:02:27,980 --> 00:02:29,120
but that's one of the things that we

62
00:02:29,120 --> 00:02:29,870
want to study

63
00:02:29,870 --> 00:02:32,720
I promised four reasons for factors so

64
00:02:32,720 --> 00:02:35,360
here it is number four competition for

65
00:02:35,360 --> 00:02:39,890
attention so we build models we do both

66
00:02:39,890 --> 00:02:41,930
we study data and we also build models

67
00:02:41,930 --> 00:02:44,150
to interpret this data and in these

68
00:02:44,150 --> 00:02:45,650
models we try to reproduce in the

69
00:02:45,650 --> 00:02:47,660
computer simulate systems that have some

70
00:02:47,660 --> 00:02:51,709
salient features of social media so here

71
00:02:51,709 --> 00:02:54,739
you have nodes that post stuff and then

72
00:02:54,739 --> 00:02:57,290
they can reshare some other things that

73
00:02:57,290 --> 00:03:00,140
is posted by their friends in one of

74
00:03:00,140 --> 00:03:02,660
them we assume that the things that they

75
00:03:02,660 --> 00:03:04,880
share thinks of them think of them are

76
00:03:04,880 --> 00:03:07,040
articles memes whatever they have a

77
00:03:07,040 --> 00:03:09,459
certain quality you could think of it as

78
00:03:09,459 --> 00:03:11,930
trustworthiness for example but any any

79
00:03:11,930 --> 00:03:14,270
any anything that would make a person

80
00:03:14,270 --> 00:03:17,630
prefer to share something of high

81
00:03:17,630 --> 00:03:19,760
quality over sharing something of low

82
00:03:19,760 --> 00:03:21,560
quality that's what did that equation so

83
00:03:21,560 --> 00:03:23,030
the probability that you share something

84
00:03:23,030 --> 00:03:25,040
is proportional to its quality now if

85
00:03:25,040 --> 00:03:27,019
you let this model run what do you find

86
00:03:27,019 --> 00:03:30,080
you expected higher quality stuff will

87
00:03:30,080 --> 00:03:32,630
be more popular so we measure the

88
00:03:32,630 --> 00:03:34,400
correlation between popularity and

89
00:03:34,400 --> 00:03:36,980
quality and what we find in this model

90
00:03:36,980 --> 00:03:39,850
is that that correlation depends on some

91
00:03:39,850 --> 00:03:42,200
parameters some factors in the model

92
00:03:42,200 --> 00:03:44,840
so with that density plot shows you is

93
00:03:44,840 --> 00:03:47,720
that when attention is low individual

94
00:03:47,720 --> 00:03:48,950
attention is low in the sense that

95
00:03:48,950 --> 00:03:51,140
people look at fewer things before they

96
00:03:51,140 --> 00:03:54,230
decide what to share and or when

97
00:03:54,230 --> 00:03:57,140
information load is high in that people

98
00:03:57,140 --> 00:03:58,850
generate a lot of stuff so that you can

99
00:03:58,850 --> 00:04:01,250
only see a small portion of what is

100
00:04:01,250 --> 00:04:03,830
posted in both of those conditions the

101
00:04:03,830 --> 00:04:05,420
correlation between popularity and

102
00:04:05,420 --> 00:04:07,670
quality goes way down so the network at

103
00:04:07,670 --> 00:04:09,860
the top right represents this scenario

104
00:04:09,860 --> 00:04:11,780
the size of the nodes represent the

105
00:04:11,780 --> 00:04:13,430
quality so you see a lot of low quality

106
00:04:13,430 --> 00:04:15,760
stuff that is being that is being spread

107
00:04:15,760 --> 00:04:18,798
so that shows that limited attention the

108
00:04:18,798 --> 00:04:22,099
competition for limited attention of

109
00:04:22,099 --> 00:04:24,020
people in the presence of information

110
00:04:24,020 --> 00:04:26,270
overload is one of the factors that that

111
00:04:26,270 --> 00:04:27,220
matters

112
00:04:27,220 --> 00:04:30,919
another one is algorithmic bias so we

113
00:04:30,919 --> 00:04:31,880
studied this problem from different

114
00:04:31,880 --> 00:04:34,300
perspectives I will just show you one

115
00:04:34,300 --> 00:04:37,520
result this is an analysis that we do

116
00:04:37,520 --> 00:04:40,759
based on traffic data learn very very

117
00:04:40,759 --> 00:04:43,130
large volumes of traffic data and we

118
00:04:43,130 --> 00:04:45,830
look at which sources people are exposed

119
00:04:45,830 --> 00:04:48,440
to coming from different platforms okay

120
00:04:48,440 --> 00:04:50,720
so which things you click on when you're

121
00:04:50,720 --> 00:04:53,090
on Facebook versus what things you click

122
00:04:53,090 --> 00:04:55,180
on when you're on Google and so on and

123
00:04:55,180 --> 00:04:58,789
we measure different kinds of biases one

124
00:04:58,789 --> 00:05:01,550
is heterogeneity biases sorry homogenate

125
00:05:01,550 --> 00:05:03,440
is bias that's the the tendency of a

126
00:05:03,440 --> 00:05:05,509
platform to concentrate your attention

127
00:05:05,509 --> 00:05:07,820
towards a small subset of all the

128
00:05:07,820 --> 00:05:08,990
sources that are there

129
00:05:08,990 --> 00:05:11,150
another one is popularity bias which is

130
00:05:11,150 --> 00:05:13,280
a focus towards the most popular sources

131
00:05:13,280 --> 00:05:15,440
and all of the platforms that we study

132
00:05:15,440 --> 00:05:16,940
all the social platforms the search

133
00:05:16,940 --> 00:05:21,050
engines email social media Google News

134
00:05:21,050 --> 00:05:23,479
and so on they all have both of these

135
00:05:23,479 --> 00:05:25,130
kinds of biases and those are even

136
00:05:25,130 --> 00:05:30,710
stronger if we focus on news sources ok

137
00:05:30,710 --> 00:05:34,490
number two social bias we and others

138
00:05:34,490 --> 00:05:37,130
have shown very clearly that especially

139
00:05:37,130 --> 00:05:39,409
in topics like politics the structure of

140
00:05:39,409 --> 00:05:41,030
our communication networks online is

141
00:05:41,030 --> 00:05:42,740
very polarized so this is a picture from

142
00:05:42,740 --> 00:05:44,419
one of our papers it was in science

143
00:05:44,419 --> 00:05:47,180
several years ago back in 2010 we found

144
00:05:47,180 --> 00:05:48,800
that conservatives only retweet are the

145
00:05:48,800 --> 00:05:51,380
Conservatives and and progressives only

146
00:05:51,380 --> 00:05:53,389
the ratite or other progressives and we

147
00:05:53,389 --> 00:05:56,060
kind of notice we call it filter bubbles

148
00:05:56,060 --> 00:05:58,699
we call it echo chambers and so on but

149
00:05:58,699 --> 00:05:59,930
one of the questions they were exploring

150
00:05:59,930 --> 00:06:02,000
right now is what is the role of social

151
00:06:02,000 --> 00:06:04,460
media social networks in fostering the

152
00:06:04,460 --> 00:06:06,620
emergence of this segregation not just

153
00:06:06,620 --> 00:06:08,930
in the fact that once it's there we can

154
00:06:08,930 --> 00:06:10,789
abuse it and exploit it for example by

155
00:06:10,789 --> 00:06:13,520
targeting messages so we built a model

156
00:06:13,520 --> 00:06:15,259
to explore this question the model is

157
00:06:15,259 --> 00:06:18,949
very simple here's a simulation of one

158
00:06:18,949 --> 00:06:20,599
of these the colors represents a

159
00:06:20,599 --> 00:06:22,370
person's opinion on a spectrum from

160
00:06:22,370 --> 00:06:24,020
liberal to conservative at the beginning

161
00:06:24,020 --> 00:06:25,880
it's all mixed you've got people across

162
00:06:25,880 --> 00:06:28,070
the spectrum and initially these people

163
00:06:28,070 --> 00:06:29,240
are connected in a completely random

164
00:06:29,240 --> 00:06:31,940
Network and what happens in this model

165
00:06:31,940 --> 00:06:33,949
we're simulating is that first of all

166
00:06:33,949 --> 00:06:35,779
people can be influenced by messages

167
00:06:35,779 --> 00:06:37,550
from their friends that are that

168
00:06:37,550 --> 00:06:39,349
reflects an opinion that is similar to

169
00:06:39,349 --> 00:06:40,760
their own so they can change the rope

170
00:06:40,760 --> 00:06:43,070
a little bit but just a little bit based

171
00:06:43,070 --> 00:06:45,920
on similar opinions to their own and the

172
00:06:45,920 --> 00:06:47,570
second one is that if you see something

173
00:06:47,570 --> 00:06:48,740
that is very different

174
00:06:48,740 --> 00:06:50,360
you really don't like it's different

175
00:06:50,360 --> 00:06:53,480
from your opinion you can unfollow that

176
00:06:53,480 --> 00:06:55,160
person or unfriend that person that's

177
00:06:55,160 --> 00:06:56,870
represented by the dashed lines there

178
00:06:56,870 --> 00:06:58,700
those are connections that are cut like

179
00:06:58,700 --> 00:07:00,200
if your cousin posts something really

180
00:07:00,200 --> 00:07:01,550
offense you say okay forget it

181
00:07:01,550 --> 00:07:03,800
you unfriend them and instead in this

182
00:07:03,800 --> 00:07:05,660
model you friend somebody else

183
00:07:05,660 --> 00:07:08,450
completely at random okay so those are

184
00:07:08,450 --> 00:07:10,640
the the new solid edges that you see so

185
00:07:10,640 --> 00:07:12,410
you can run this many many times the

186
00:07:12,410 --> 00:07:14,240
outcome is always the same the one that

187
00:07:14,240 --> 00:07:16,250
you show in this animation if you wait

188
00:07:16,250 --> 00:07:18,230
long enough not only does the network

189
00:07:18,230 --> 00:07:21,140
become segregated but also each of those

190
00:07:21,140 --> 00:07:22,580
two echo chambers become more

191
00:07:22,580 --> 00:07:24,440
homogeneous so there is polarization

192
00:07:24,440 --> 00:07:26,990
around an idea initially you had a lot

193
00:07:26,990 --> 00:07:29,510
of colors at the end you have two groups

194
00:07:29,510 --> 00:07:31,850
that one is just in one color and the

195
00:07:31,850 --> 00:07:33,140
other one is the other color so

196
00:07:33,140 --> 00:07:35,180
eventually you're only exposed to

197
00:07:35,180 --> 00:07:38,030
information that obviously mirrors your

198
00:07:38,030 --> 00:07:39,500
own opinions and if you wait long enough

199
00:07:39,500 --> 00:07:41,180
it's a little bit depressing that one

200
00:07:41,180 --> 00:07:45,560
last link finally goes away so what this

201
00:07:45,560 --> 00:07:48,770
shows is that it's inevitable that just

202
00:07:48,770 --> 00:07:52,000
by very simple rules that all social

203
00:07:52,000 --> 00:07:54,920
media use it's inevitable that you end

204
00:07:54,920 --> 00:07:57,770
up with some polarization and the number

205
00:07:57,770 --> 00:07:59,750
one reason that we at least no wonder we

206
00:07:59,750 --> 00:08:02,120
study a lot in our lab is BOTS

207
00:08:02,120 --> 00:08:04,730
because BOTS can exploit all of these

208
00:08:04,730 --> 00:08:06,440
other vulnerabilities that we've seen

209
00:08:06,440 --> 00:08:08,540
the social the cognitive biases and the

210
00:08:08,540 --> 00:08:11,420
algorithmic biases so we developed a

211
00:08:11,420 --> 00:08:13,850
bunch of tools we demoed a couple of

212
00:08:13,850 --> 00:08:16,300
them yesterday for those who were there

213
00:08:16,300 --> 00:08:18,560
one is bottle meter which is a tool

214
00:08:18,560 --> 00:08:21,140
anybody can use it to assess whether an

215
00:08:21,140 --> 00:08:23,720
account is a bot or automated it's very

216
00:08:23,720 --> 00:08:25,580
popular we filled half a million queries

217
00:08:25,580 --> 00:08:29,030
per day and you're welcome to use it

218
00:08:29,030 --> 00:08:31,310
there's an API and etc etc and another

219
00:08:31,310 --> 00:08:35,030
tool is hosting Huck C is also a freely

220
00:08:35,030 --> 00:08:37,120
available tool it's also open source and

221
00:08:37,120 --> 00:08:41,900
it's a way to monitor and visualize the

222
00:08:41,900 --> 00:08:44,870
diffusion networks for for news we focus

223
00:08:44,870 --> 00:08:46,790
in particular on news from local

224
00:08:46,790 --> 00:08:48,020
readability sources and from

225
00:08:48,020 --> 00:08:50,570
fact-checking because we sources because

226
00:08:50,570 --> 00:08:52,460
we want to see how these compete so here

227
00:08:52,460 --> 00:08:55,040
for example this is a nutter for one

228
00:08:55,040 --> 00:08:57,200
well-known infamous piece of fake news

229
00:08:57,200 --> 00:08:59,510
it was an article in force about the

230
00:08:59,510 --> 00:09:01,280
fact that three million illegal aliens

231
00:09:01,280 --> 00:09:06,290
voted for Clinton and networks like this

232
00:09:06,290 --> 00:09:08,210
can tell us some interesting things for

233
00:09:08,210 --> 00:09:09,590
example you see the big notes you see

234
00:09:09,590 --> 00:09:11,600
who are the influential notes you see

235
00:09:11,600 --> 00:09:13,280
segregation between the communities you

236
00:09:13,280 --> 00:09:14,480
see that the people who share the

237
00:09:14,480 --> 00:09:16,130
misinformation which is the top part the

238
00:09:16,130 --> 00:09:18,170
great part of the network really are not

239
00:09:18,170 --> 00:09:19,850
exposed to the fact-checking which is

240
00:09:19,850 --> 00:09:21,710
the lower part the yellow part those are

241
00:09:21,710 --> 00:09:23,660
the those are the debunking articles

242
00:09:23,660 --> 00:09:26,090
from schnapps and PolitiFact and you

243
00:09:26,090 --> 00:09:28,100
also see some abuse so in the middle

244
00:09:28,100 --> 00:09:30,080
it's very hard for you to see but there

245
00:09:30,080 --> 00:09:32,870
is one pink note which is a but that in

246
00:09:32,870 --> 00:09:36,590
a short sequence of time mentioned

247
00:09:36,590 --> 00:09:39,080
donald trump 19 times with a link to

248
00:09:39,080 --> 00:09:41,030
that particular article and shortly

249
00:09:41,030 --> 00:09:42,830
thereafter we know that donald trump

250
00:09:42,830 --> 00:09:44,630
said that he read an article about this

251
00:09:44,630 --> 00:09:47,840
and from the infamous commission so you

252
00:09:47,840 --> 00:09:50,390
can use this tool to study this and in a

253
00:09:50,390 --> 00:09:53,360
very little time that I have left let me

254
00:09:53,360 --> 00:09:54,470
tell you some of the things that we

255
00:09:54,470 --> 00:09:56,060
learn when we study the data that we

256
00:09:56,060 --> 00:09:57,950
collect from hoaxes so this is the big

257
00:09:57,950 --> 00:10:00,950
network of sharing of all the

258
00:10:00,950 --> 00:10:02,930
misinformation that we track four

259
00:10:02,930 --> 00:10:06,620
hundred thousand articles claims from

260
00:10:06,620 --> 00:10:09,200
local ability sources and those are in

261
00:10:09,200 --> 00:10:12,200
purple and then the orange is a smaller

262
00:10:12,200 --> 00:10:14,690
number of articles fifteen thousand from

263
00:10:14,690 --> 00:10:17,870
fact-checking sources so as you move

264
00:10:17,870 --> 00:10:19,790
closer to the core of the misinformation

265
00:10:19,790 --> 00:10:22,430
Network you see that basically there is

266
00:10:22,430 --> 00:10:24,320
almost no sharing of fact-checking with

267
00:10:24,320 --> 00:10:26,720
the exception of a couple things that

268
00:10:26,720 --> 00:10:28,610
you see there links from politifact

269
00:10:28,610 --> 00:10:30,250
actually those are people who are

270
00:10:30,250 --> 00:10:32,390
attacking those sources and saying that

271
00:10:32,390 --> 00:10:33,890
they're biased and they shouldn't be

272
00:10:33,890 --> 00:10:37,339
believed or that they post a piece of

273
00:10:37,339 --> 00:10:39,050
fake news and then they have a link to

274
00:10:39,050 --> 00:10:41,330
snow observe or PolitiFact that actually

275
00:10:41,330 --> 00:10:44,510
says the opposite but people they count

276
00:10:44,510 --> 00:10:46,100
on the fact that people don't click on

277
00:10:46,100 --> 00:10:48,170
the link and so they think that Snopes

278
00:10:48,170 --> 00:10:49,610
is actually supporting the piece of

279
00:10:49,610 --> 00:10:51,650
misinformation that they're sharing and

280
00:10:51,650 --> 00:10:53,089
another thing that we see as we move

281
00:10:53,089 --> 00:10:55,010
close to the core is that the average

282
00:10:55,010 --> 00:10:58,670
bot score of the accounts goes up so in

283
00:10:58,670 --> 00:11:01,700
the core there is a lot of bots so let

284
00:11:01,700 --> 00:11:04,610
me close with three quick findings that

285
00:11:04,610 --> 00:11:06,770
we find in this network number one is

286
00:11:06,770 --> 00:11:08,300
that bots are

287
00:11:08,300 --> 00:11:11,000
targeting influential accounts so the

288
00:11:11,000 --> 00:11:12,710
more your BOTS score is high the more

289
00:11:12,710 --> 00:11:14,360
you are likely to mention really

290
00:11:14,360 --> 00:11:16,460
influential people these are politicians

291
00:11:16,460 --> 00:11:18,800
or journalists so those of you here are

292
00:11:18,800 --> 00:11:22,010
journalists you are being targeted the

293
00:11:22,010 --> 00:11:23,660
second one is that in the first few

294
00:11:23,660 --> 00:11:25,730
instance in the first few 10 seconds

295
00:11:25,730 --> 00:11:28,880
after a fake news article is posted we

296
00:11:28,880 --> 00:11:32,180
see a big spike in posting from bots so

297
00:11:32,180 --> 00:11:34,850
bots are acting very very early on to

298
00:11:34,850 --> 00:11:38,720
amplify inject the message and the third

299
00:11:38,720 --> 00:11:42,080
one is that bots can repeat the same can

300
00:11:42,080 --> 00:11:45,380
repeat links posting of the same link

301
00:11:45,380 --> 00:11:47,210
many many times in this plot you show

302
00:11:47,210 --> 00:11:48,710
the distribution what this shows is that

303
00:11:48,710 --> 00:11:50,950
the same account can post the same

304
00:11:50,950 --> 00:11:54,350
misinformation article ten thousands of

305
00:11:54,350 --> 00:11:56,750
times okay so repeating repeating

306
00:11:56,750 --> 00:11:59,360
keeping injecting and those are things

307
00:11:59,360 --> 00:12:02,660
that are very easy to do alright and one

308
00:12:02,660 --> 00:12:05,210
worrisome thing is that we find what

309
00:12:05,210 --> 00:12:07,670
this plot shows you is that most

310
00:12:07,670 --> 00:12:10,130
retweeting is done by the humans okay so

311
00:12:10,130 --> 00:12:12,200
that's what the projection chart at the

312
00:12:12,200 --> 00:12:14,000
top says if you if you're retweeting

313
00:12:14,000 --> 00:12:15,770
you're probably human you have a low bot

314
00:12:15,770 --> 00:12:19,190
score but who are the humans retweeting

315
00:12:19,190 --> 00:12:21,380
okay and that's the chart on the left

316
00:12:21,380 --> 00:12:22,940
and it shows that humans are retweeting

317
00:12:22,940 --> 00:12:25,190
other humans but they're also reading a

318
00:12:25,190 --> 00:12:27,350
lot of BOTS so this means that there is

319
00:12:27,350 --> 00:12:28,160
impact

320
00:12:28,160 --> 00:12:30,650
okay humans are we sharing low

321
00:12:30,650 --> 00:12:33,410
credibility content from BOTS okay I'm

322
00:12:33,410 --> 00:12:35,240
out of time thank you very much I'm

323
00:12:35,240 --> 00:12:36,590
happy to tell you more about our

324
00:12:36,590 --> 00:12:38,450
research and those are the great

325
00:12:38,450 --> 00:12:39,980
students in our lab who are doing all

326
00:12:39,980 --> 00:12:40,460
the work

327
00:12:40,460 --> 00:12:40,850
Thanks

328
00:12:40,850 --> 00:12:44,720
[Applause]

329
00:12:44,720 --> 00:12:44,820
you

330
00:12:44,820 --> 00:12:48,690
[Applause]

