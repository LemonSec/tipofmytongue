1
00:00:01,460 --> 00:00:05,069
so as these slides come up I should say

2
00:00:05,069 --> 00:00:08,550
that a third is some of this work is

3
00:00:08,550 --> 00:00:12,179
also being submitted as evidence for the

4
00:00:12,179 --> 00:00:13,320
various select committees that are

5
00:00:13,320 --> 00:00:14,849
happening in the UK so I'm based in the

6
00:00:14,849 --> 00:00:17,190
UK and I should also say that this work

7
00:00:17,190 --> 00:00:19,020
that I'm presenting today was done with

8
00:00:19,020 --> 00:00:20,789
Clare Wardle from first draft and our

9
00:00:20,789 --> 00:00:25,260
PhD student Hannah so where I want to

10
00:00:25,260 --> 00:00:27,689
start is to just give a kind of quick

11
00:00:27,689 --> 00:00:30,240
premise one is that images are

12
00:00:30,240 --> 00:00:32,549
incredibly powerful I think we know that

13
00:00:32,549 --> 00:00:34,230
but I think they don't necessarily get

14
00:00:34,230 --> 00:00:35,910
the standalone attention when it comes

15
00:00:35,910 --> 00:00:37,530
to missing disinformation that we feel

16
00:00:37,530 --> 00:00:40,469
that they deserve as vehicles potential

17
00:00:40,469 --> 00:00:41,730
vehicles for dissing misinformation

18
00:00:41,730 --> 00:00:43,770
they're incredibly complex to understand

19
00:00:43,770 --> 00:00:46,079
but potentially very harmful which is

20
00:00:46,079 --> 00:00:47,879
why we feel that they need standalone

21
00:00:47,879 --> 00:00:49,950
attention Haroon mentioned in the

22
00:00:49,950 --> 00:00:52,980
previous panel a language based model I

23
00:00:52,980 --> 00:00:55,260
would add to that below that a visual

24
00:00:55,260 --> 00:00:56,969
based model if you start to think about

25
00:00:56,969 --> 00:00:59,129
how people understand images then I

26
00:00:59,129 --> 00:01:00,780
think you are dealing with something

27
00:01:00,780 --> 00:01:06,119
even more complex so if we go to the

28
00:01:06,119 --> 00:01:15,299
next slide where do i press oh there I'm

29
00:01:15,299 --> 00:01:18,990
pressing okay thank you I'm used to the

30
00:01:18,990 --> 00:01:22,229
Ted format okay so everybody I'm sure

31
00:01:22,229 --> 00:01:24,210
has seen this image I think this is a

32
00:01:24,210 --> 00:01:26,700
really good example of an image making

33
00:01:26,700 --> 00:01:29,340
visible a very complex social issue that

34
00:01:29,340 --> 00:01:31,170
potentially in text-based form people

35
00:01:31,170 --> 00:01:33,570
knew about but it is the image that made

36
00:01:33,570 --> 00:01:36,990
a difference in terms of people trying

37
00:01:36,990 --> 00:01:38,250
to understand what was going on and

38
00:01:38,250 --> 00:01:39,450
putting pressure on a government to

39
00:01:39,450 --> 00:01:41,310
change a policy and I would say that

40
00:01:41,310 --> 00:01:45,390
this image had an impact overnight an

41
00:01:45,390 --> 00:01:48,329
image from the UK we are of course also

42
00:01:48,329 --> 00:01:52,670
dealing with state-based official actors

43
00:01:52,670 --> 00:01:54,960
actively misleading and I think that's

44
00:01:54,960 --> 00:01:57,600
really important so we're not as a sort

45
00:01:57,600 --> 00:01:59,850
of baseline dealing with foreign States

46
00:01:59,850 --> 00:02:02,159
trying to meddle we are also dealing

47
00:02:02,159 --> 00:02:05,820
with official state actors misinforming

48
00:02:05,820 --> 00:02:08,489
the populous and so what we have here is

49
00:02:08,489 --> 00:02:11,819
this is UK politician Nigel Farage this

50
00:02:11,819 --> 00:02:14,349
is in the lead-up to the EU referee

51
00:02:14,349 --> 00:02:17,140
we're very misleading information around

52
00:02:17,140 --> 00:02:20,440
what the influx potentially of migrants

53
00:02:20,440 --> 00:02:21,730
and refugees that this is at the height

54
00:02:21,730 --> 00:02:23,610
of the refugee crisis or the visible

55
00:02:23,610 --> 00:02:26,890
refugee crisis and tapping into those

56
00:02:26,890 --> 00:02:30,329
fears and tapping into into very

57
00:02:30,329 --> 00:02:33,610
powerful images produces breaking-point

58
00:02:33,610 --> 00:02:36,610
poster which in retrospect you can say

59
00:02:36,610 --> 00:02:39,579
contributed significantly to the outcome

60
00:02:39,579 --> 00:02:45,340
of that vote this is an image that came

61
00:02:45,340 --> 00:02:46,959
about in the aftermath of the

62
00:02:46,959 --> 00:02:50,440
Westminster bombing so this is in March

63
00:02:50,440 --> 00:02:54,359
to April so in March 2017 this is a

64
00:02:54,359 --> 00:02:57,400
false image that was shared and went

65
00:02:57,400 --> 00:02:59,709
viral in the aftermath where people were

66
00:02:59,709 --> 00:03:03,250
basically saying that Londoners had a

67
00:03:03,250 --> 00:03:05,440
particular response to to the

68
00:03:05,440 --> 00:03:07,090
Westminster attack

69
00:03:07,090 --> 00:03:09,250
this was also read out in Parliament and

70
00:03:09,250 --> 00:03:11,640
this was entirely Phocis was generated

71
00:03:11,640 --> 00:03:15,459
online and so we have an example here of

72
00:03:15,459 --> 00:03:18,190
a UK state actor being misled in this

73
00:03:18,190 --> 00:03:19,540
case the Prime Minister and it was

74
00:03:19,540 --> 00:03:21,910
shared also by many official news

75
00:03:21,910 --> 00:03:24,609
sources so a number of different kinds

76
00:03:24,609 --> 00:03:26,200
of images that are potentially

77
00:03:26,200 --> 00:03:28,239
misleading problematic and different

78
00:03:28,239 --> 00:03:31,780
actors spreading these so what tends to

79
00:03:31,780 --> 00:03:34,810
happen is that organisations like

80
00:03:34,810 --> 00:03:36,430
BuzzFeed they do a lot of really really

81
00:03:36,430 --> 00:03:38,709
good work in this space very quickly

82
00:03:38,709 --> 00:03:41,109
when an event has happened often in the

83
00:03:41,109 --> 00:03:44,410
aftermath of a terrorism attack they

84
00:03:44,410 --> 00:03:47,410
will put out and others like BuzzFeed

85
00:03:47,410 --> 00:03:50,069
they will put out lists of examples of

86
00:03:50,069 --> 00:03:52,930
false or misleading images memes and so

87
00:03:52,930 --> 00:03:55,870
on and so the chup chup example was

88
00:03:55,870 --> 00:03:58,359
included in this list of seven images

89
00:03:58,359 --> 00:04:00,489
that were potentially misleading and

90
00:04:00,489 --> 00:04:05,019
harmful but the next one wasn't and I

91
00:04:05,019 --> 00:04:06,609
would say that this is this has been the

92
00:04:06,609 --> 00:04:09,010
most harmful and are people in the room

93
00:04:09,010 --> 00:04:11,560
familiar with this image this is also in

94
00:04:11,560 --> 00:04:13,120
the aftermath of the Westminster attack

95
00:04:13,120 --> 00:04:16,449
this is a true images unaltered this is

96
00:04:16,449 --> 00:04:20,260
showing a person victim in the

97
00:04:20,260 --> 00:04:23,950
background and a Muslim Imam is Lima in

98
00:04:23,950 --> 00:04:26,270
the foreground turning away and will

99
00:04:26,270 --> 00:04:28,940
passed and there's different iterations

100
00:04:28,940 --> 00:04:31,099
of this image and the way in which this

101
00:04:31,099 --> 00:04:34,300
was presented online it went viral was

102
00:04:34,300 --> 00:04:36,470
look this is what the problem is the

103
00:04:36,470 --> 00:04:38,539
Muslims are attacking us the perpetrator

104
00:04:38,539 --> 00:04:40,550
of the Westminister attack was a convert

105
00:04:40,550 --> 00:04:43,370
to Islam and also they don't care right

106
00:04:43,370 --> 00:04:45,740
so in terms of what Rene already said in

107
00:04:45,740 --> 00:04:47,569
the press in the past panel these are

108
00:04:47,569 --> 00:04:49,340
the kinds of images that press the

109
00:04:49,340 --> 00:04:51,860
emotional buttons so this was happening

110
00:04:51,860 --> 00:04:53,569
in March but what we didn't find out

111
00:04:53,569 --> 00:04:55,250
until November as part of the

112
00:04:55,250 --> 00:04:57,020
congressional hearings in the US and

113
00:04:57,020 --> 00:04:59,330
Deen highlights is the the work that

114
00:04:59,330 --> 00:05:01,220
that he's done on the the list of

115
00:05:01,220 --> 00:05:02,900
accounts that were suspended from

116
00:05:02,900 --> 00:05:05,870
Twitter UK researchers had to look at

117
00:05:05,870 --> 00:05:08,509
this as well and identified Texas Lone

118
00:05:08,509 --> 00:05:12,440
Star as a an account that was linked to

119
00:05:12,440 --> 00:05:17,030
IRA and this image was one of this

120
00:05:17,030 --> 00:05:20,180
accounts kind of big triumphs because

121
00:05:20,180 --> 00:05:22,370
this went viral and it was picked up in

122
00:05:22,370 --> 00:05:26,569
over 80 mainstream newspaper articles so

123
00:05:26,569 --> 00:05:28,580
what we have here is we have a very

124
00:05:28,580 --> 00:05:31,069
successful disinformation campaign that

125
00:05:31,069 --> 00:05:32,569
was pick up picked up by the mainstream

126
00:05:32,569 --> 00:05:34,729
media and legitimized by the mainstream

127
00:05:34,729 --> 00:05:36,319
media and of course the damage is done

128
00:05:36,319 --> 00:05:38,150
because this image is still out there

129
00:05:38,150 --> 00:05:40,819
and this this kind of dominant framing

130
00:05:40,819 --> 00:05:42,380
if you like of the Muslims are the

131
00:05:42,380 --> 00:05:44,630
problem is of course out there even

132
00:05:44,630 --> 00:05:47,120
though the debunk is also out there so

133
00:05:47,120 --> 00:05:49,370
what we decided to do is to try and look

134
00:05:49,370 --> 00:05:50,870
at the row of images and missing this

135
00:05:50,870 --> 00:05:53,630
information into big European elections

136
00:05:53,630 --> 00:05:56,719
to French and UK elections last year so

137
00:05:56,719 --> 00:05:58,460
what we looked at is first draft

138
00:05:58,460 --> 00:06:00,529
partnered in both countries with

139
00:06:00,529 --> 00:06:03,259
different partners so in France they set

140
00:06:03,259 --> 00:06:05,690
up a project called cross-check which

141
00:06:05,690 --> 00:06:07,310
worked with different newsrooms and

142
00:06:07,310 --> 00:06:08,990
journalism students and pulled

143
00:06:08,990 --> 00:06:10,880
information from a number of different

144
00:06:10,880 --> 00:06:14,060
sources and they then produced all of

145
00:06:14,060 --> 00:06:17,990
this information 43 in this case image

146
00:06:17,990 --> 00:06:23,270
based misleading items on a website and

147
00:06:23,270 --> 00:06:25,819
we then built a database on top of that

148
00:06:25,819 --> 00:06:32,840
and in the UK case I went one and in the

149
00:06:32,840 --> 00:06:35,719
UK case they worked with full facts very

150
00:06:35,719 --> 00:06:37,880
similar although pulled in information

151
00:06:37,880 --> 00:06:38,770
from a number

152
00:06:38,770 --> 00:06:40,690
of different sources that are slight

153
00:06:40,690 --> 00:06:42,550
differences and the difference here also

154
00:06:42,550 --> 00:06:45,370
is that that information was shared with

155
00:06:45,370 --> 00:06:48,039
newsrooms in the form of newsletters so

156
00:06:48,039 --> 00:06:50,770
that was closed information if you like

157
00:06:50,770 --> 00:06:52,960
and in the French case everything was

158
00:06:52,960 --> 00:06:54,970
publicly available on a website and in

159
00:06:54,970 --> 00:06:56,530
the UK case this was in the form of

160
00:06:56,530 --> 00:06:58,090
newsletters so there's also two

161
00:06:58,090 --> 00:07:01,990
different models that work here so what

162
00:07:01,990 --> 00:07:03,759
we then did when we took all of that

163
00:07:03,759 --> 00:07:05,800
information from these two sources we

164
00:07:05,800 --> 00:07:09,220
ended up with 95 image many case studies

165
00:07:09,220 --> 00:07:12,340
if you like 52 from the UK case and 43

166
00:07:12,340 --> 00:07:14,409
from the French case and we coded that

167
00:07:14,409 --> 00:07:16,630
then in a number of different ways in

168
00:07:16,630 --> 00:07:19,090
terms of image types for example because

169
00:07:19,090 --> 00:07:21,490
we're academics we identify 21 different

170
00:07:21,490 --> 00:07:23,080
image types we think is very very

171
00:07:23,080 --> 00:07:25,840
important to be very precise in terms of

172
00:07:25,840 --> 00:07:28,389
potential signals for disinformation

173
00:07:28,389 --> 00:07:30,190
when it comes to images so we have 21

174
00:07:30,190 --> 00:07:32,130
times you might get rid of a few of them

175
00:07:32,130 --> 00:07:35,199
we also worked with first drafts

176
00:07:35,199 --> 00:07:38,349
original model for levels of accuracy we

177
00:07:38,349 --> 00:07:40,659
cut it for the topic we cut it for

178
00:07:40,659 --> 00:07:42,940
motivation we coded for origins so the

179
00:07:42,940 --> 00:07:45,819
source the location analyst I mentioned

180
00:07:45,819 --> 00:07:48,490
the source so many of you will be

181
00:07:48,490 --> 00:07:50,500
familiar with the typology that first

182
00:07:50,500 --> 00:07:53,110
draft put out that Clara put out and

183
00:07:53,110 --> 00:07:56,580
that typology went viral and we added

184
00:07:56,580 --> 00:07:59,680
three to that specifically tailored to

185
00:07:59,680 --> 00:08:04,389
images and so altogether we ended up

186
00:08:04,389 --> 00:08:07,630
with 10 and technically 12 because we

187
00:08:07,630 --> 00:08:09,550
also opened up the possibility that the

188
00:08:09,550 --> 00:08:12,159
images are accurate and just accurate

189
00:08:12,159 --> 00:08:14,680
and humorous so technically we have we

190
00:08:14,680 --> 00:08:19,360
have 12 so we've expanded this typology

191
00:08:19,360 --> 00:08:22,289
to specifically be able to deal with

192
00:08:22,289 --> 00:08:26,919
visual content here now just to give you

193
00:08:26,919 --> 00:08:30,310
very briefly some insight here in terms

194
00:08:30,310 --> 00:08:32,200
of what we're interested in in terms of

195
00:08:32,200 --> 00:08:33,880
trying to understand what are potential

196
00:08:33,880 --> 00:08:37,809
signals when it comes to images so that

197
00:08:37,809 --> 00:08:40,779
what we see is that predominantly when

198
00:08:40,779 --> 00:08:42,700
we look at image types and I haven't

199
00:08:42,700 --> 00:08:44,890
given you the full findings here for the

200
00:08:44,890 --> 00:08:48,010
21 types these are the most dominantly

201
00:08:48,010 --> 00:08:50,380
used ones we have dominantly video

202
00:08:50,380 --> 00:08:52,120
photographs data visualized

203
00:08:52,120 --> 00:08:54,310
photographs with text screenshots of

204
00:08:54,310 --> 00:08:56,680
tweets and also I just want to pull out

205
00:08:56,680 --> 00:08:59,200
memes because there seems to be a lot of

206
00:08:59,200 --> 00:09:01,230
work that focuses specifically on memes

207
00:09:01,230 --> 00:09:03,490
so there seems to be a lot of meme

208
00:09:03,490 --> 00:09:06,670
trackers that get funded but when you

209
00:09:06,670 --> 00:09:08,080
look at the data so when you try to

210
00:09:08,080 --> 00:09:09,640
understand what is the actual problem

211
00:09:09,640 --> 00:09:10,960
what does the actual problem look like

212
00:09:10,960 --> 00:09:12,700
rather than let's look at a very

213
00:09:12,700 --> 00:09:15,970
specific content type memes seem to be

214
00:09:15,970 --> 00:09:19,770
relatively small compared to photographs

215
00:09:19,770 --> 00:09:24,250
if we look at the level of accuracy the

216
00:09:24,250 --> 00:09:28,150
biggest problem is for context so in

217
00:09:28,150 --> 00:09:30,610
both cases both in the French case in

218
00:09:30,610 --> 00:09:33,339
the UK case on average we have thirty

219
00:09:33,339 --> 00:09:37,420
percent of the problematic content is a

220
00:09:37,420 --> 00:09:40,240
truthful image I'm just going to let

221
00:09:40,240 --> 00:09:41,740
that hang in the air a little bit and

222
00:09:41,740 --> 00:09:44,770
it's the context that is a problem so

223
00:09:44,770 --> 00:09:47,560
these are not things that you can you

224
00:09:47,560 --> 00:09:49,810
can easily deal with in terms of machine

225
00:09:49,810 --> 00:09:51,460
learning in terms of algorithms so this

226
00:09:51,460 --> 00:09:54,520
is an example of a full context image

227
00:09:54,520 --> 00:09:57,430
this is an image that shows in French

228
00:09:57,430 --> 00:09:59,350
case the aftermath of a football match

229
00:09:59,350 --> 00:10:01,209
where people went out to celebrate and

230
00:10:01,209 --> 00:10:04,720
someone is draped in what looks like an

231
00:10:04,720 --> 00:10:08,020
Isis flag it is not an Isis flag but in

232
00:10:08,020 --> 00:10:09,550
the run-up to the French elections these

233
00:10:09,550 --> 00:10:13,089
images dredged up represented as if it's

234
00:10:13,089 --> 00:10:14,770
an Isis flag and as if it's an Isis

235
00:10:14,770 --> 00:10:17,380
supporter to press that button in terms

236
00:10:17,380 --> 00:10:19,839
of Islamophobia in the French context

237
00:10:19,839 --> 00:10:23,740
and just to to wrap up there are efforts

238
00:10:23,740 --> 00:10:25,660
and Facebook is making efforts in this

239
00:10:25,660 --> 00:10:27,790
case I'm focusing just on Facebook to

240
00:10:27,790 --> 00:10:29,260
start to look at the fact checking

241
00:10:29,260 --> 00:10:32,800
efforts in terms of images and the full

242
00:10:32,800 --> 00:10:36,100
context is being highlighted there and

243
00:10:36,100 --> 00:10:38,320
I'm sure this is in part related to

244
00:10:38,320 --> 00:10:39,790
earlier presentations that Claire is

245
00:10:39,790 --> 00:10:42,040
made on this work to to this company and

246
00:10:42,040 --> 00:10:44,529
and others but I think what is really

247
00:10:44,529 --> 00:10:46,540
really important to understand is that

248
00:10:46,540 --> 00:10:48,610
there are no trade for technological

249
00:10:48,610 --> 00:10:51,339
solutions here because if if you think

250
00:10:51,339 --> 00:10:54,339
that it's the context of the way in

251
00:10:54,339 --> 00:10:55,690
which the image is presented that is the

252
00:10:55,690 --> 00:10:57,250
problem that can be different every

253
00:10:57,250 --> 00:11:00,339
single time and I think when we're

254
00:11:00,339 --> 00:11:02,290
dealing with images I still feel that

255
00:11:02,290 --> 00:11:04,889
we're still at the stage of mapping

256
00:11:04,889 --> 00:11:07,350
out what the problem is and how complex

257
00:11:07,350 --> 00:11:09,269
that problem is and if people want to

258
00:11:09,269 --> 00:11:11,040
talk to me more afterwards during the

259
00:11:11,040 --> 00:11:13,199
Q&A I have a lot more to say about it

260
00:11:13,199 --> 00:11:17,399
also in terms of the kinds of visuals so

261
00:11:17,399 --> 00:11:20,189
very very problematic infographics that

262
00:11:20,189 --> 00:11:22,379
are incredibly hard to debunk so when it

263
00:11:22,379 --> 00:11:24,629
comes to visuals I think we need

264
00:11:24,629 --> 00:11:26,970
standalone efforts to try and understand

265
00:11:26,970 --> 00:11:29,369
what they contribute to this space thank

266
00:11:29,369 --> 00:11:30,040
you

267
00:11:30,040 --> 00:11:37,150
[Applause]

