1
00:00:00,000 --> 00:00:01,740
hi everyone I'm Jess line one from

2
00:00:01,740 --> 00:00:03,629
Facebook and thrilled to be talking to

3
00:00:03,629 --> 00:00:04,859
you today about our approach to

4
00:00:04,859 --> 00:00:06,930
misinformation and apologies in advance

5
00:00:06,930 --> 00:00:13,679
for muddling the term but I'll divide my

6
00:00:13,679 --> 00:00:16,109
talk today into three main topics first

7
00:00:16,109 --> 00:00:18,000
is Newt newsfeed ranking which kind of

8
00:00:18,000 --> 00:00:20,609
sets context for how to understand our

9
00:00:20,609 --> 00:00:22,529
fight against inauthentic content and

10
00:00:22,529 --> 00:00:25,199
third party fact-checking which is part

11
00:00:25,199 --> 00:00:27,210
of the foundation of how we're trying to

12
00:00:27,210 --> 00:00:28,920
address misinformation on the platform

13
00:00:28,920 --> 00:00:31,289
so newsfeed ranking and I'll try to go

14
00:00:31,289 --> 00:00:32,759
through this very quickly first what is

15
00:00:32,759 --> 00:00:34,410
an algorithm it's a term that gets

16
00:00:34,410 --> 00:00:37,079
tossed around frequently essentially an

17
00:00:37,079 --> 00:00:39,360
algorithm is just a formula for

18
00:00:39,360 --> 00:00:41,190
addressing a particular problem and in

19
00:00:41,190 --> 00:00:42,780
our case the problem is there's too much

20
00:00:42,780 --> 00:00:44,940
content in your newsfeed to consume all

21
00:00:44,940 --> 00:00:46,649
at once there has to be some logical way

22
00:00:46,649 --> 00:00:49,129
of ordering it and we order it content

23
00:00:49,129 --> 00:00:51,690
in accordance with our values what are

24
00:00:51,690 --> 00:00:54,570
those values first friends and family

25
00:00:54,570 --> 00:00:57,780
come first second or a platform for all

26
00:00:57,780 --> 00:00:59,670
ideas we support freedom of expression

27
00:00:59,670 --> 00:01:01,649
third we believe in authentic

28
00:01:01,649 --> 00:01:04,680
communication fourth you control your

29
00:01:04,680 --> 00:01:06,570
experience you control who your friends

30
00:01:06,570 --> 00:01:08,280
are you control the pages you follow and

31
00:01:08,280 --> 00:01:10,049
you control the content you see first in

32
00:01:10,049 --> 00:01:12,450
your newsfeed and last we're always

33
00:01:12,450 --> 00:01:14,790
iterating our policies are subject to

34
00:01:14,790 --> 00:01:16,710
change our algorithm is subject to

35
00:01:16,710 --> 00:01:18,570
change based very much on feedback in

36
00:01:18,570 --> 00:01:21,509
sessions like these so what is the

37
00:01:21,509 --> 00:01:23,130
algorithm composed of first it's

38
00:01:23,130 --> 00:01:25,140
composed of an inventory that's all of

39
00:01:25,140 --> 00:01:26,580
the potential content that could be in

40
00:01:26,580 --> 00:01:29,159
your newsfeed based on that inventory

41
00:01:29,159 --> 00:01:31,860
there are certain signals who posted

42
00:01:31,860 --> 00:01:34,350
something how recently was the content

43
00:01:34,350 --> 00:01:36,930
posted based on those signals we make

44
00:01:36,930 --> 00:01:38,460
certain predictions about how you'll

45
00:01:38,460 --> 00:01:41,189
engage with the content and based on all

46
00:01:41,189 --> 00:01:42,780
of these things we assign the content a

47
00:01:42,780 --> 00:01:44,570
relevant score and that score dictates

48
00:01:44,570 --> 00:01:46,710
where the content will fall in your

49
00:01:46,710 --> 00:01:50,369
newsfeed so let's say this is all of the

50
00:01:50,369 --> 00:01:51,780
potential content you might you might

51
00:01:51,780 --> 00:01:53,460
see all of your friends the pages you

52
00:01:53,460 --> 00:01:56,579
follow the brand's you follow and for

53
00:01:56,579 --> 00:01:59,130
each of these of these entities there's

54
00:01:59,130 --> 00:02:04,560
a post and based on the post again there

55
00:02:04,560 --> 00:02:07,290
are certain signals who posted it when

56
00:02:07,290 --> 00:02:12,270
what is the story type etc etc and again

57
00:02:12,270 --> 00:02:13,190
just flying through this

58
00:02:13,190 --> 00:02:15,710
it's given given the time based on all

59
00:02:15,710 --> 00:02:17,630
of these based on the signals will make

60
00:02:17,630 --> 00:02:18,740
certain predictions about your

61
00:02:18,740 --> 00:02:20,180
likelihood to engage your likelihood to

62
00:02:20,180 --> 00:02:21,920
click on it your likelihood to share it

63
00:02:21,920 --> 00:02:24,320
your likelihood to like it or comment on

64
00:02:24,320 --> 00:02:27,260
it etc and you'll get a score and so

65
00:02:27,260 --> 00:02:28,820
each piece of content gets a score

66
00:02:28,820 --> 00:02:31,070
higher scores mean that the content will

67
00:02:31,070 --> 00:02:35,330
surface higher in your newsfeed so a

68
00:02:35,330 --> 00:02:37,400
score of 1.4 would fall before a score

69
00:02:37,400 --> 00:02:41,090
of 1.2 0.7 etc the details about our

70
00:02:41,090 --> 00:02:42,950
algorithm and Lucid extent we make any

71
00:02:42,950 --> 00:02:45,230
changes we try to be transparent about

72
00:02:45,230 --> 00:02:47,420
those and we detail them in our newsroom

73
00:02:47,420 --> 00:02:49,760
most recently we put up a documentary

74
00:02:49,760 --> 00:02:52,460
called facing facts which explores the

75
00:02:52,460 --> 00:02:54,830
newsfeed teams approach to addressing

76
00:02:54,830 --> 00:02:57,260
misinformation which brings me to the

77
00:02:57,260 --> 00:02:59,390
next topic in terms of addressing

78
00:02:59,390 --> 00:03:01,730
inauthentic content on the platform we

79
00:03:01,730 --> 00:03:03,260
follow a three-part framework called

80
00:03:03,260 --> 00:03:06,560
remove reduce and inform the question

81
00:03:06,560 --> 00:03:09,230
that I have to answer most frequently

82
00:03:09,230 --> 00:03:11,600
and probably one of the harder questions

83
00:03:11,600 --> 00:03:12,860
that we face is why don't we simply

84
00:03:12,860 --> 00:03:15,770
remove false news the answer is Facebook

85
00:03:15,770 --> 00:03:17,630
doesn't have a policy that requires

86
00:03:17,630 --> 00:03:19,970
people to post only true content another

87
00:03:19,970 --> 00:03:21,260
way of saying this is that we don't

88
00:03:21,260 --> 00:03:23,150
remove content simply because it's false

89
00:03:23,150 --> 00:03:25,160
and I know this has been a subject of

90
00:03:25,160 --> 00:03:27,350
recent criticism and people disagree

91
00:03:27,350 --> 00:03:29,270
about where we draw the line but when we

92
00:03:29,270 --> 00:03:31,489
balance freedom of expression and safety

93
00:03:31,489 --> 00:03:33,410
and also considering that we do

94
00:03:33,410 --> 00:03:35,080
prioritize posts from friends and family

95
00:03:35,080 --> 00:03:37,280
we think it's almost it would be

96
00:03:37,280 --> 00:03:39,200
impossible and we've heard feedback from

97
00:03:39,200 --> 00:03:40,760
our community that no one would want us

98
00:03:40,760 --> 00:03:42,680
to be in the position of determining the

99
00:03:42,680 --> 00:03:45,560
truth or falsity of that of content as

100
00:03:45,560 --> 00:03:48,920
particularly at our scale we do remove

101
00:03:48,920 --> 00:03:50,690
content for violating our community

102
00:03:50,690 --> 00:03:52,970
standards and I'm on the team that helps

103
00:03:52,970 --> 00:03:54,830
write those rules and so those rules

104
00:03:54,830 --> 00:03:56,239
cover things like bullying harassment

105
00:03:56,239 --> 00:04:00,170
hate speech content that does violate

106
00:04:00,170 --> 00:04:02,390
our community standards is removed from

107
00:04:02,390 --> 00:04:04,640
the platform one of our community

108
00:04:04,640 --> 00:04:06,590
standards addresses misrepresentation

109
00:04:06,590 --> 00:04:08,900
and authenticity and so we do also

110
00:04:08,900 --> 00:04:10,850
remove fake accounts from the platform

111
00:04:10,850 --> 00:04:12,410
you have to be who you say you are on

112
00:04:12,410 --> 00:04:15,739
Facebook and lastly we remove

113
00:04:15,739 --> 00:04:17,988
monetization rights so repeat purveyors

114
00:04:17,988 --> 00:04:20,089
of false news will lose their ability to

115
00:04:20,089 --> 00:04:22,850
monetize on the platform I should say

116
00:04:22,850 --> 00:04:24,560
that we made an update to our community

117
00:04:24,560 --> 00:04:25,950
standards two weeks

118
00:04:25,950 --> 00:04:28,410
we announced it where we decided that we

119
00:04:28,410 --> 00:04:30,990
would be removing misinformation if

120
00:04:30,990 --> 00:04:33,450
reported to be contributing to imminent

121
00:04:33,450 --> 00:04:35,640
violence and this policy change was very

122
00:04:35,640 --> 00:04:38,970
much in response to things that we saw

123
00:04:38,970 --> 00:04:40,650
in places like Myanmar and Sri Lanka

124
00:04:40,650 --> 00:04:43,050
where content that was false that didn't

125
00:04:43,050 --> 00:04:45,660
rise to the level of hate speech under

126
00:04:45,660 --> 00:04:47,250
our policies or as credible threats of

127
00:04:47,250 --> 00:04:49,350
violence nonetheless was reportedly

128
00:04:49,350 --> 00:04:50,760
contributing to violence on the ground

129
00:04:50,760 --> 00:04:52,350
and we thought it was appropriate to

130
00:04:52,350 --> 00:04:56,160
draw a line around imminent violence but

131
00:04:56,160 --> 00:04:58,880
generally speaking we do believe that

132
00:04:58,880 --> 00:05:00,600
reducing the appearance of

133
00:05:00,600 --> 00:05:02,820
misinformation in news feed and doing

134
00:05:02,820 --> 00:05:04,470
this pursuant to our news feed ranking

135
00:05:04,470 --> 00:05:06,240
is the appropriate way to deal with

136
00:05:06,240 --> 00:05:08,940
misinformation and false news and I

137
00:05:08,940 --> 00:05:10,950
mentioned repeat offenders what do we do

138
00:05:10,950 --> 00:05:12,450
with repeat offenders these are repeat

139
00:05:12,450 --> 00:05:14,370
shares of false news we've reduced their

140
00:05:14,370 --> 00:05:16,050
distribution in news feed meaning that

141
00:05:16,050 --> 00:05:17,850
any content they post on the platform

142
00:05:17,850 --> 00:05:19,800
will be reduced we remove their

143
00:05:19,800 --> 00:05:21,780
monetization rights and we're there

144
00:05:21,780 --> 00:05:23,400
their monetization rights and we are

145
00:05:23,400 --> 00:05:25,650
using machine learning to identify more

146
00:05:25,650 --> 00:05:30,510
repeat offenders inform how do we inform

147
00:05:30,510 --> 00:05:33,690
our users two primary ways one is the

148
00:05:33,690 --> 00:05:35,490
context button when you see an article

149
00:05:35,490 --> 00:05:37,260
on Facebook you'll probably see a little

150
00:05:37,260 --> 00:05:39,180
eye icon if you click on that it will

151
00:05:39,180 --> 00:05:40,740
give you basic information about an

152
00:05:40,740 --> 00:05:42,510
article who published it when it was

153
00:05:42,510 --> 00:05:45,390
published who's the author etc we're

154
00:05:45,390 --> 00:05:47,100
also we've also launched a news literacy

155
00:05:47,100 --> 00:05:48,570
campaign in the United States and abroad

156
00:05:48,570 --> 00:05:50,250
and the purpose of this is really to

157
00:05:50,250 --> 00:05:52,830
educate our community as to so that they

158
00:05:52,830 --> 00:05:54,390
to empower them to make judgments about

159
00:05:54,390 --> 00:05:56,100
what they should and shouldn't trust on

160
00:05:56,100 --> 00:05:58,830
the platform third-party fact-checking I

161
00:05:58,830 --> 00:06:00,030
think I mentioned this is really the

162
00:06:00,030 --> 00:06:01,950
foundation of the reduced part of the

163
00:06:01,950 --> 00:06:04,080
remove reduce inform model so how does

164
00:06:04,080 --> 00:06:07,200
it work briefly we identify content that

165
00:06:07,200 --> 00:06:09,000
should be acute or sent to fact-checkers

166
00:06:09,000 --> 00:06:11,460
for review fact checkers review and make

167
00:06:11,460 --> 00:06:12,840
determinations about the truth or

168
00:06:12,840 --> 00:06:15,090
falsity of the content and we take

169
00:06:15,090 --> 00:06:16,890
action on that content based on the fact

170
00:06:16,890 --> 00:06:19,980
checkers reviews and ratings so how do

171
00:06:19,980 --> 00:06:22,200
we identify content for our fact

172
00:06:22,200 --> 00:06:23,850
checkers it's using our algorithm again

173
00:06:23,850 --> 00:06:25,890
inventory signals and predictions and

174
00:06:25,890 --> 00:06:28,170
primarily the signals that we look at

175
00:06:28,170 --> 00:06:30,380
our feedback from our community so you

176
00:06:30,380 --> 00:06:32,850
our community members can actually

177
00:06:32,850 --> 00:06:34,920
report to us when they believe content

178
00:06:34,920 --> 00:06:36,880
is false and

179
00:06:36,880 --> 00:06:38,440
also look at disbelief comments things

180
00:06:38,440 --> 00:06:40,420
like no way or you must be joking

181
00:06:40,420 --> 00:06:42,040
there's no way this is true or just you

182
00:06:42,040 --> 00:06:45,580
know hashtag false news or fake news

183
00:06:45,580 --> 00:06:48,340
this all feeds into a machine learning

184
00:06:48,340 --> 00:06:49,720
classifier which is getting better and

185
00:06:49,720 --> 00:06:51,340
better all the time at predicting

186
00:06:51,340 --> 00:06:53,710
whether content is potentially

187
00:06:53,710 --> 00:06:56,530
misinformation fact checkers can also

188
00:06:56,530 --> 00:06:58,780
proactively identify stories and rate

189
00:06:58,780 --> 00:07:00,700
that content in our tool and fact

190
00:07:00,700 --> 00:07:02,530
checkers review the content in our tool

191
00:07:02,530 --> 00:07:05,200
again Facebook is not making

192
00:07:05,200 --> 00:07:07,090
determinations about truth or falsity on

193
00:07:07,090 --> 00:07:10,210
our own they review content pursuant to

194
00:07:10,210 --> 00:07:12,850
this rating grid and basically it can be

195
00:07:12,850 --> 00:07:14,650
divided into two buckets content that's

196
00:07:14,650 --> 00:07:16,810
rated false or mixture mixtures

197
00:07:16,810 --> 00:07:19,660
basically claims true and false claims

198
00:07:19,660 --> 00:07:22,240
embedded in an article and content

199
00:07:22,240 --> 00:07:23,830
that's rated and then everything else

200
00:07:23,830 --> 00:07:25,960
content that's rated false or mixture

201
00:07:25,960 --> 00:07:28,930
will be demoted in news feed but content

202
00:07:28,930 --> 00:07:30,820
that is rated anything else will not be

203
00:07:30,820 --> 00:07:32,740
demoted but related articles or

204
00:07:32,740 --> 00:07:35,080
additional context might be shown so

205
00:07:35,080 --> 00:07:36,760
what's the impact on a story if it's

206
00:07:36,760 --> 00:07:38,890
rated false it will be demoted in

207
00:07:38,890 --> 00:07:41,350
newsfeed a sharing interstitial will

208
00:07:41,350 --> 00:07:42,820
appear this is essentially a warning

209
00:07:42,820 --> 00:07:44,650
screen so if I try to share an article

210
00:07:44,650 --> 00:07:45,880
that's been marked false by our fact

211
00:07:45,880 --> 00:07:47,650
checkers I'll be alerted with this

212
00:07:47,650 --> 00:07:49,330
warning screen that it has been marked

213
00:07:49,330 --> 00:07:53,290
false user notifications if I share an

214
00:07:53,290 --> 00:07:55,000
article that is later marked false I'll

215
00:07:55,000 --> 00:07:56,830
get a notification saying that I

216
00:07:56,830 --> 00:07:58,360
actually did share something that was

217
00:07:58,360 --> 00:07:59,830
marked false by a fact checker and

218
00:07:59,830 --> 00:08:02,320
related articles which is more context

219
00:08:02,320 --> 00:08:04,240
it's basically the fact checkers article

220
00:08:04,240 --> 00:08:06,250
that explains why the article it rated

221
00:08:06,250 --> 00:08:08,800
is false or misleading and again we

222
00:08:08,800 --> 00:08:11,380
believe that showing showing our users

223
00:08:11,380 --> 00:08:13,930
additional information so that they can

224
00:08:13,930 --> 00:08:15,850
understand what is for what was false

225
00:08:15,850 --> 00:08:17,380
what was misleading about the article

226
00:08:17,380 --> 00:08:20,040
they shared is one of the best ways to

227
00:08:20,040 --> 00:08:22,810
to educate our community and empower

228
00:08:22,810 --> 00:08:24,580
them to make better judgments about what

229
00:08:24,580 --> 00:08:27,940
they're seeing on the platform we have

230
00:08:27,940 --> 00:08:30,610
our data scientists have told us we have

231
00:08:30,610 --> 00:08:33,190
data that shows that once a piece of

232
00:08:33,190 --> 00:08:35,080
content is rated false and demoted in

233
00:08:35,080 --> 00:08:38,110
newsfeed or reduced it it also reduces

234
00:08:38,110 --> 00:08:39,940
future views of that content by 80

235
00:08:39,940 --> 00:08:43,809
percent so it's quite effective where

236
00:08:43,809 --> 00:08:45,820
are we currently so when we started this

237
00:08:45,820 --> 00:08:47,890
program we had launched in 3 or 4

238
00:08:47,890 --> 00:08:50,500
countries and today we're proud to say

239
00:08:50,500 --> 00:08:50,720
that

240
00:08:50,720 --> 00:08:52,610
we have expanded internationally to 17

241
00:08:52,610 --> 00:08:55,459
and where do we go from here we're

242
00:08:55,459 --> 00:08:56,750
hoping to expand the fact-checking

243
00:08:56,750 --> 00:09:01,009
program to additional countries we're

244
00:09:01,009 --> 00:09:02,750
launching we've launched already an

245
00:09:02,750 --> 00:09:04,759
alpha test of fact-checking photos and

246
00:09:04,759 --> 00:09:07,370
videos internationally so right now our

247
00:09:07,370 --> 00:09:09,050
fact-checking program has been focused

248
00:09:09,050 --> 00:09:11,149
on links or news articles but we are

249
00:09:11,149 --> 00:09:13,100
moving to photos and videos where there

250
00:09:13,100 --> 00:09:14,810
we understand there is a good deal of

251
00:09:14,810 --> 00:09:17,930
misinformation I can't even see what

252
00:09:17,930 --> 00:09:20,138
this does

253
00:09:20,259 --> 00:09:22,629
increasing the impact of our of

254
00:09:22,629 --> 00:09:24,620
fact-checking and we're doing this

255
00:09:24,620 --> 00:09:26,540
mainly by identifying duplicate of

256
00:09:26,540 --> 00:09:27,980
content so once a fact-checker

257
00:09:27,980 --> 00:09:30,079
rates an article is false how can we

258
00:09:30,079 --> 00:09:32,600
find duplicative links that are also

259
00:09:32,600 --> 00:09:35,050
false and take action on that content

260
00:09:35,050 --> 00:09:38,240
and taking more aggressive more

261
00:09:38,240 --> 00:09:39,709
aggressive action against repeat

262
00:09:39,709 --> 00:09:42,019
offenders I think I mentioned we do see

263
00:09:42,019 --> 00:09:44,930
that most of false news is is shared by

264
00:09:44,930 --> 00:09:46,370
repeat offenders so people are sharing

265
00:09:46,370 --> 00:09:48,500
this stuff over and over again and so we

266
00:09:48,500 --> 00:09:50,420
think taking action at the actor level

267
00:09:50,420 --> 00:09:52,459
as opposed to just the content level is

268
00:09:52,459 --> 00:09:54,920
one of the ways to disrupt the financial

269
00:09:54,920 --> 00:09:57,139
incentives behind false news and really

270
00:09:57,139 --> 00:09:59,949
get at the heart of this problem and

271
00:09:59,949 --> 00:10:03,170
last we we understand that we have more

272
00:10:03,170 --> 00:10:05,809
work to do around transparency both with

273
00:10:05,809 --> 00:10:07,160
our fact checkers with our community

274
00:10:07,160 --> 00:10:09,920
with with stakeholders and people who

275
00:10:09,920 --> 00:10:11,059
are interested in what we're doing and

276
00:10:11,059 --> 00:10:13,370
we're really working hard to be more

277
00:10:13,370 --> 00:10:15,170
transparent about misinformation on our

278
00:10:15,170 --> 00:10:16,339
platform and the ways that we're trying

279
00:10:16,339 --> 00:10:18,850
to address it thank you

280
00:10:18,850 --> 00:10:24,639
[Applause]

