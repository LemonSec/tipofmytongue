1
00:00:00,030 --> 00:00:02,129
Thank You Ahmed and it's an honor to be

2
00:00:02,129 --> 00:00:04,110
here it's already being such a

3
00:00:04,110 --> 00:00:08,670
stimulating morning I my research over

4
00:00:08,670 --> 00:00:11,610
the last 12 years have been focused on

5
00:00:11,610 --> 00:00:13,920
the intersection of social media and

6
00:00:13,920 --> 00:00:16,199
health communication particularly for

7
00:00:16,199 --> 00:00:19,289
cancer control and prevention

8
00:00:19,289 --> 00:00:22,230
so it's evolved quite a bit and I just

9
00:00:22,230 --> 00:00:24,180
want to sort of start by giving you a

10
00:00:24,180 --> 00:00:28,470
kind of a higher-level orientation of

11
00:00:28,470 --> 00:00:30,359
why we care about social media and

12
00:00:30,359 --> 00:00:32,369
health and I think these are the four

13
00:00:32,369 --> 00:00:34,710
main areas of research and I should say

14
00:00:34,710 --> 00:00:37,860
besides doing my own research my day job

15
00:00:37,860 --> 00:00:41,040
is really hosting a large portfolio of

16
00:00:41,040 --> 00:00:43,620
NIH funded research in this area so we

17
00:00:43,620 --> 00:00:45,660
write funding opportunities we talk to

18
00:00:45,660 --> 00:00:47,850
investigators thinking about ideas of

19
00:00:47,850 --> 00:00:50,969
doing some cool research in the area so

20
00:00:50,969 --> 00:00:53,070
the first is really this excitement that

21
00:00:53,070 --> 00:00:54,899
we can maybe overcome some of the

22
00:00:54,899 --> 00:00:56,789
traditional health disparities and the

23
00:00:56,789 --> 00:00:58,859
digital divide because people are on

24
00:00:58,859 --> 00:01:02,340
social media and it's information can

25
00:01:02,340 --> 00:01:04,979
efficiently effectively you know in some

26
00:01:04,979 --> 00:01:08,159
ways not not always the case be

27
00:01:08,159 --> 00:01:10,020
disseminated so there's a lot of

28
00:01:10,020 --> 00:01:12,930
interesting using social media to do

29
00:01:12,930 --> 00:01:15,360
health communication that may level the

30
00:01:15,360 --> 00:01:17,939
playing field if you will and the second

31
00:01:17,939 --> 00:01:19,830
type is really using social media as a

32
00:01:19,830 --> 00:01:22,049
data source you know where are people

33
00:01:22,049 --> 00:01:24,570
talking about different topics what do

34
00:01:24,570 --> 00:01:26,340
they think attitudes perceptions and

35
00:01:26,340 --> 00:01:28,890
behaviors the third is mounting health

36
00:01:28,890 --> 00:01:32,329
in promotion or patient education

37
00:01:32,329 --> 00:01:35,369
campaigns or interventions using social

38
00:01:35,369 --> 00:01:37,619
media as a platform and the last one

39
00:01:37,619 --> 00:01:40,320
it's really been a focus of our team the

40
00:01:40,320 --> 00:01:42,930
last two three years is looking at how

41
00:01:42,930 --> 00:01:45,390
do we understand the risk and mitigate

42
00:01:45,390 --> 00:01:47,369
the risk of misinformation on social

43
00:01:47,369 --> 00:01:50,610
media especially related to health and

44
00:01:50,610 --> 00:01:53,820
cancer related topics for this we need

45
00:01:53,820 --> 00:01:56,189
both observational and interventional

46
00:01:56,189 --> 00:01:59,549
studies as we begin to do this work

47
00:01:59,549 --> 00:02:03,000
I think it was 2007-2008 and then

48
00:02:03,000 --> 00:02:05,070
learning more about Cambridge analytic

49
00:02:05,070 --> 00:02:08,940
ah scandal and a lot of related issues

50
00:02:08,940 --> 00:02:11,400
we thought we needed to not just think

51
00:02:11,400 --> 00:02:13,050
about this traditional public health

52
00:02:13,050 --> 00:02:13,950
ways of

53
00:02:13,950 --> 00:02:15,750
dealing with you know that's not

54
00:02:15,750 --> 00:02:17,610
credible information we need to go after

55
00:02:17,610 --> 00:02:20,640
that or to really come up with a more

56
00:02:20,640 --> 00:02:23,190
multidisciplinary you know research

57
00:02:23,190 --> 00:02:25,410
agenda so we held a working group on

58
00:02:25,410 --> 00:02:27,390
this topic we brought people from

59
00:02:27,390 --> 00:02:29,250
journalism computer science and health

60
00:02:29,250 --> 00:02:31,500
communication together and following

61
00:02:31,500 --> 00:02:34,800
that really in engaging discussion for

62
00:02:34,800 --> 00:02:36,900
two days we've we published a couple of

63
00:02:36,900 --> 00:02:40,019
thought papers one in JAMA addressing

64
00:02:40,019 --> 00:02:41,730
health related misinformation and

65
00:02:41,730 --> 00:02:43,379
another in American Journal of

66
00:02:43,379 --> 00:02:45,180
Preventive Medicine that's looking at

67
00:02:45,180 --> 00:02:47,540
sort of the science of misinformation

68
00:02:47,540 --> 00:02:50,610
and right now we're in the process of

69
00:02:50,610 --> 00:02:53,489
putting together a special issue in

70
00:02:53,489 --> 00:02:55,290
collaboration with American Journal of

71
00:02:55,290 --> 00:02:56,130
Public Health

72
00:02:56,130 --> 00:02:58,790
that's all looking at social media

73
00:02:58,790 --> 00:03:01,769
health Mis Health misinformation on

74
00:03:01,769 --> 00:03:04,530
social media the four main areas of

75
00:03:04,530 --> 00:03:07,560
content are surveillance understanding

76
00:03:07,560 --> 00:03:09,420
in the contacts evaluating the impact

77
00:03:09,420 --> 00:03:11,750
and what kind of responses and

78
00:03:11,750 --> 00:03:13,650
interventions are appropriate and

79
00:03:13,650 --> 00:03:15,840
effective so I'm highlighting the

80
00:03:15,840 --> 00:03:17,130
context because that's what I'm gonna

81
00:03:17,130 --> 00:03:20,010
share with you a little tour of a small

82
00:03:20,010 --> 00:03:22,620
study that we put together and I really

83
00:03:22,620 --> 00:03:25,019
like Adam or I think was Adam talking

84
00:03:25,019 --> 00:03:26,549
about the importance of sort of peeling

85
00:03:26,549 --> 00:03:28,440
back the curtain to look at the process

86
00:03:28,440 --> 00:03:30,060
of research so I'll show you some of

87
00:03:30,060 --> 00:03:32,190
that rather than just sort of perfect

88
00:03:32,190 --> 00:03:35,700
results so I'm thinking about context

89
00:03:35,700 --> 00:03:36,959
it's not just kind of the information

90
00:03:36,959 --> 00:03:40,200
ecosystem contact but individual context

91
00:03:40,200 --> 00:03:44,250
of how we process information how we you

92
00:03:44,250 --> 00:03:47,310
know understand how can we understand

93
00:03:47,310 --> 00:03:51,450
how people use an interface with their

94
00:03:51,450 --> 00:03:53,940
Facebook messages so we designed an

95
00:03:53,940 --> 00:03:58,410
experimental study at NCI that's also

96
00:03:58,410 --> 00:04:00,900
very much a mixed method study we wanted

97
00:04:00,900 --> 00:04:03,959
to look at how users of social media in

98
00:04:03,959 --> 00:04:06,120
particular Facebook navigate and

99
00:04:06,120 --> 00:04:10,139
evaluate simulated cancer messages what

100
00:04:10,139 --> 00:04:12,930
factors influence their attention what

101
00:04:12,930 --> 00:04:14,730
do they look for what are they looking

102
00:04:14,730 --> 00:04:18,108
at what factors impact their perceived

103
00:04:18,108 --> 00:04:21,988
trust of the source and also message

104
00:04:21,988 --> 00:04:24,030
credibility so we asked people do you

105
00:04:24,030 --> 00:04:25,890
think this is a believable you know

106
00:04:25,890 --> 00:04:26,710
message

107
00:04:26,710 --> 00:04:28,449
we want to look at the process they go

108
00:04:28,449 --> 00:04:32,949
through to make that determination so to

109
00:04:32,949 --> 00:04:36,759
sort of really simplify we collected

110
00:04:36,759 --> 00:04:41,169
baseline data for 52 participants we got

111
00:04:41,169 --> 00:04:44,080
a very nice balance of you know race and

112
00:04:44,080 --> 00:04:46,360
ethnicity gender and health literacy

113
00:04:46,360 --> 00:04:50,530
levels to get them to come in and we we

114
00:04:50,530 --> 00:04:52,810
show them simulated Facebook feeds and

115
00:04:52,810 --> 00:04:55,270
we collected eye tracking data as they

116
00:04:55,270 --> 00:04:57,669
were looking at these feeds we also gave

117
00:04:57,669 --> 00:05:00,220
them a survey questionnaire where we

118
00:05:00,220 --> 00:05:03,250
asked people questions related to source

119
00:05:03,250 --> 00:05:05,949
and message credibility what they

120
00:05:05,949 --> 00:05:07,509
endorse would they share this message

121
00:05:07,509 --> 00:05:12,370
with their friends on social book and we

122
00:05:12,370 --> 00:05:14,800
also measure their health literacy using

123
00:05:14,800 --> 00:05:17,410
a rapid you know health literacy test I

124
00:05:17,410 --> 00:05:19,690
know it's not perfect but it was one way

125
00:05:19,690 --> 00:05:21,220
of getting at something that's not just

126
00:05:21,220 --> 00:05:23,229
what they put down us their education

127
00:05:23,229 --> 00:05:27,789
level and then we did a careful

128
00:05:27,789 --> 00:05:30,310
cognitive interview we have them go

129
00:05:30,310 --> 00:05:32,320
through the process they you know had

130
00:05:32,320 --> 00:05:34,300
just been you know undergone to talk

131
00:05:34,300 --> 00:05:37,150
about the you know basically qualitative

132
00:05:37,150 --> 00:05:39,039
interviews and then we debrief them so

133
00:05:39,039 --> 00:05:40,360
if they got something that was

134
00:05:40,360 --> 00:05:43,330
incredible they we ensure that they left

135
00:05:43,330 --> 00:05:46,240
knowing that vaccine doesn't cause all

136
00:05:46,240 --> 00:05:48,520
these problems for example so the four

137
00:05:48,520 --> 00:05:51,250
conditions we gave them were whether the

138
00:05:51,250 --> 00:05:53,110
message was narrative or non narrative

139
00:05:53,110 --> 00:05:54,849
based because we were interested in the

140
00:05:54,849 --> 00:05:58,030
format of persuasion we look gave them

141
00:05:58,030 --> 00:06:00,750
different sources government agencies

142
00:06:00,750 --> 00:06:03,580
health organizations some of these are

143
00:06:03,580 --> 00:06:07,840
real some of them are not individuals

144
00:06:07,840 --> 00:06:10,389
these are just people you know people's

145
00:06:10,389 --> 00:06:12,280
name and I know this is one major

146
00:06:12,280 --> 00:06:14,409
limitation is these are not people that

147
00:06:14,409 --> 00:06:17,229
they know the participants are familiar

148
00:06:17,229 --> 00:06:19,539
with so we want to pretend this we tell

149
00:06:19,539 --> 00:06:21,009
them to pretend this is somebody in

150
00:06:21,009 --> 00:06:23,949
their facebook network and then we

151
00:06:23,949 --> 00:06:26,409
looked at we gave them messages that are

152
00:06:26,409 --> 00:06:28,990
evidence-based and not in the two topics

153
00:06:28,990 --> 00:06:31,240
these are the two main topics that are

154
00:06:31,240 --> 00:06:34,839
most how do you say popular for

155
00:06:34,839 --> 00:06:36,610
misinformation in the world of cancer

156
00:06:36,610 --> 00:06:39,500
that is HPV vaccination and

157
00:06:39,500 --> 00:06:42,860
unsafety so this is like a days plot

158
00:06:42,860 --> 00:06:46,340
just to show you kind of would be sort

159
00:06:46,340 --> 00:06:49,640
of a way we design the study and then

160
00:06:49,640 --> 00:06:51,140
you see these different colors are

161
00:06:51,140 --> 00:06:54,020
basically different areas of interest so

162
00:06:54,020 --> 00:06:56,060
we want to look at whether they are

163
00:06:56,060 --> 00:06:58,880
looking I'm just going to try looking at

164
00:06:58,880 --> 00:07:02,240
the source the text or the image or the

165
00:07:02,240 --> 00:07:05,900
overall posts that they were being asked

166
00:07:05,900 --> 00:07:08,330
to look at so we define these as you can

167
00:07:08,330 --> 00:07:10,550
see one two three four reflect the

168
00:07:10,550 --> 00:07:15,050
different areas so overall we found that

169
00:07:15,050 --> 00:07:17,450
after adjusting for pixel counts you

170
00:07:17,450 --> 00:07:19,310
know getting a sense of you know we

171
00:07:19,310 --> 00:07:21,050
don't want to just count more for

172
00:07:21,050 --> 00:07:23,240
there's more stuff there right

173
00:07:23,240 --> 00:07:29,210
whoops sorry go back I was told going

174
00:07:29,210 --> 00:07:32,510
back isn't easy okay so people spend a

175
00:07:32,510 --> 00:07:34,760
lot of time on source almost equal

176
00:07:34,760 --> 00:07:37,460
amount of time on source and text and

177
00:07:37,460 --> 00:07:39,650
much less on image in general this is

178
00:07:39,650 --> 00:07:42,169
like not you know the strata fighting

179
00:07:42,169 --> 00:07:45,350
anyway we also found that overall there

180
00:07:45,350 --> 00:07:48,700
was more time spent on narrative based

181
00:07:48,700 --> 00:07:52,070
messages and also when the source is an

182
00:07:52,070 --> 00:07:54,470
individual seemed to garner more

183
00:07:54,470 --> 00:07:56,960
attention the others we didn't find any

184
00:07:56,960 --> 00:07:58,669
differences and those of you who are

185
00:07:58,669 --> 00:08:00,830
familiar with experimental design you're

186
00:08:00,830 --> 00:08:02,360
probably already seen you have too many

187
00:08:02,360 --> 00:08:04,460
cells and the cell sizes are too small

188
00:08:04,460 --> 00:08:07,280
you can't find significance we know all

189
00:08:07,280 --> 00:08:09,260
of that so there are many things we

190
00:08:09,260 --> 00:08:11,810
would do differently but we wanted to

191
00:08:11,810 --> 00:08:13,910
give an overall sense of you know kind

192
00:08:13,910 --> 00:08:15,830
of where people are what they're looking

193
00:08:15,830 --> 00:08:19,580
at so so we connected sort of that eye

194
00:08:19,580 --> 00:08:21,380
tracking to all time data which we

195
00:08:21,380 --> 00:08:22,850
didn't find a lot of you know

196
00:08:22,850 --> 00:08:25,760
significance to to them connecting a few

197
00:08:25,760 --> 00:08:28,700
things so how do people decide what

198
00:08:28,700 --> 00:08:31,610
source to trust so source type like I

199
00:08:31,610 --> 00:08:33,770
said there were three kinds and we asked

200
00:08:33,770 --> 00:08:36,589
them do they trust that source and we

201
00:08:36,589 --> 00:08:38,120
want to see if when they trust the

202
00:08:38,120 --> 00:08:40,159
source do they believe in the message

203
00:08:40,159 --> 00:08:42,679
from that source more and when they

204
00:08:42,679 --> 00:08:44,870
trust the source do they spend more time

205
00:08:44,870 --> 00:08:46,640
looking at the source or not and those

206
00:08:46,640 --> 00:08:48,230
of you who are familiar with theories

207
00:08:48,230 --> 00:08:50,089
knows there could be different

208
00:08:50,089 --> 00:08:52,320
explanations of either either

209
00:08:52,320 --> 00:08:53,910
findings right like you are more

210
00:08:53,910 --> 00:08:55,560
skeptical because you've never seen the

211
00:08:55,560 --> 00:08:57,930
source so you're scrutinizing or you're

212
00:08:57,930 --> 00:09:00,240
like I like this this this message

213
00:09:00,240 --> 00:09:01,950
because it's from someone I trust so I

214
00:09:01,950 --> 00:09:04,380
look at it more so because it's so

215
00:09:04,380 --> 00:09:07,530
exploratory we weren't able to our

216
00:09:07,530 --> 00:09:09,450
hypotheses were pretty pretty open-ended

217
00:09:09,450 --> 00:09:11,460
we had some directions we were hoping to

218
00:09:11,460 --> 00:09:13,740
look for so some of the high-level

219
00:09:13,740 --> 00:09:16,380
findings people generally reported

220
00:09:16,380 --> 00:09:19,140
higher trust in government agencies than

221
00:09:19,140 --> 00:09:21,590
individuals however if you look at

222
00:09:21,590 --> 00:09:23,940
stratified by a veracity meaning people

223
00:09:23,940 --> 00:09:25,970
who are among people who got

224
00:09:25,970 --> 00:09:28,470
non-credible informations they still

225
00:09:28,470 --> 00:09:30,570
reported trusting the organization's

226
00:09:30,570 --> 00:09:32,970
more when the message is non credible

227
00:09:32,970 --> 00:09:36,660
and we see some really cool qualitative

228
00:09:36,660 --> 00:09:38,760
findings people talk about really

229
00:09:38,760 --> 00:09:42,330
trusting the CDC and again the caveat is

230
00:09:42,330 --> 00:09:44,490
these are people who are willing to come

231
00:09:44,490 --> 00:09:48,510
into a you know science organization and

232
00:09:48,510 --> 00:09:50,280
do this research so they probably don't

233
00:09:50,280 --> 00:09:53,520
have very poor trust in say National

234
00:09:53,520 --> 00:09:55,860
Institutes of Health but they said we

235
00:09:55,860 --> 00:09:58,860
look to CDC and they they mention a lot

236
00:09:58,860 --> 00:10:01,530
of familiar agencies names and they also

237
00:10:01,530 --> 00:10:03,650
talk about reputable medical

238
00:10:03,650 --> 00:10:07,500
organizations such as Gulf website Johns

239
00:10:07,500 --> 00:10:09,120
Hopkins or Mayo Clinic

240
00:10:09,120 --> 00:10:11,610
they say they trust that Moore and I we

241
00:10:11,610 --> 00:10:13,590
found a number of interesting quotes on

242
00:10:13,590 --> 00:10:15,390
what I call citizenship fake news

243
00:10:15,390 --> 00:10:17,910
detection this idea that people post

244
00:10:17,910 --> 00:10:20,790
stuff out there I know it's not real and

245
00:10:20,790 --> 00:10:22,770
I go out and I tell them that's not real

246
00:10:22,770 --> 00:10:25,470
and then put a link for them to go and

247
00:10:25,470 --> 00:10:27,720
read about it so and people talk about I

248
00:10:27,720 --> 00:10:29,130
know they probably didn't do it

249
00:10:29,130 --> 00:10:31,920
intentionally but I find ways to you

250
00:10:31,920 --> 00:10:34,800
know tell them to check their sources so

251
00:10:34,800 --> 00:10:39,060
that was an interesting observation that

252
00:10:39,060 --> 00:10:41,610
people say they observed a lot of that

253
00:10:41,610 --> 00:10:42,840
happening and they want to do something

254
00:10:42,840 --> 00:10:46,080
about it we also wanted to look at

255
00:10:46,080 --> 00:10:48,090
whether participants health literacy and

256
00:10:48,090 --> 00:10:51,270
the various message features affect

257
00:10:51,270 --> 00:10:54,150
message believability and then we very

258
00:10:54,150 --> 00:10:56,130
much try to connect that to at all time

259
00:10:56,130 --> 00:10:59,070
and I can say that we did find a

260
00:10:59,070 --> 00:11:01,170
positive association of higher health

261
00:11:01,170 --> 00:11:03,290
literacy individuals

262
00:11:03,290 --> 00:11:05,870
correctly rating so they say that

263
00:11:05,870 --> 00:11:07,639
evidence-based messages are more

264
00:11:07,639 --> 00:11:09,350
believable than non evidence-based

265
00:11:09,350 --> 00:11:11,000
messages and we didn't find that

266
00:11:11,000 --> 00:11:13,519
difference in among people with limited

267
00:11:13,519 --> 00:11:17,240
health literacy we did find that limited

268
00:11:17,240 --> 00:11:18,980
health literacy individuals spend more

269
00:11:18,980 --> 00:11:21,350
time on the source of the the pose

270
00:11:21,350 --> 00:11:23,959
rather than the text or other parts they

271
00:11:23,959 --> 00:11:27,620
are looking they're scrutinizing they we

272
00:11:27,620 --> 00:11:29,839
also found non evidence-based narratives

273
00:11:29,839 --> 00:11:31,519
were the ones that garnered most

274
00:11:31,519 --> 00:11:33,889
attention on the text part so we were

275
00:11:33,889 --> 00:11:35,990
very interesting the text Aoi because we

276
00:11:35,990 --> 00:11:37,699
think that's a place where a lot of that

277
00:11:37,699 --> 00:11:39,560
credibility work has to happen because

278
00:11:39,560 --> 00:11:42,259
you know you're looking for argument you

279
00:11:42,259 --> 00:11:44,480
know and supporting evidence and so on

280
00:11:44,480 --> 00:11:46,310
so those were the only really

281
00:11:46,310 --> 00:11:47,360
significant

282
00:11:47,360 --> 00:11:49,910
you know findings or associations we

283
00:11:49,910 --> 00:11:51,980
tried every which way the sort of

284
00:11:51,980 --> 00:11:54,470
attention I tracking data did not

285
00:11:54,470 --> 00:11:57,949
produce anything meaningful in we really

286
00:11:57,949 --> 00:12:00,470
did all kinds of sort of investigation

287
00:12:00,470 --> 00:12:02,540
in terms of believability so basically

288
00:12:02,540 --> 00:12:05,089
attention did not Factory very much in

289
00:12:05,089 --> 00:12:07,850
the sort of credibility assessment what

290
00:12:07,850 --> 00:12:10,370
are the implications a lot of things we

291
00:12:10,370 --> 00:12:12,319
wanted to look for and we wanted to find

292
00:12:12,319 --> 00:12:14,449
we didn't find but we did find that

293
00:12:14,449 --> 00:12:17,029
source was an important area that people

294
00:12:17,029 --> 00:12:19,519
look to at least in our experimental

295
00:12:19,519 --> 00:12:22,730
study we also hear that people say they

296
00:12:22,730 --> 00:12:27,410
trust government agencies and but we

297
00:12:27,410 --> 00:12:28,670
didn't see a difference in the

298
00:12:28,670 --> 00:12:30,290
credibility assessment of health

299
00:12:30,290 --> 00:12:33,649
organisations and individuals so you can

300
00:12:33,649 --> 00:12:35,990
imagine these bogus organizations so we

301
00:12:35,990 --> 00:12:38,569
say I'm from National Cancer Institute I

302
00:12:38,569 --> 00:12:40,880
know it's a real government agency but

303
00:12:40,880 --> 00:12:43,190
if I say National Federation of cancer

304
00:12:43,190 --> 00:12:45,560
research I just made something up but a

305
00:12:45,560 --> 00:12:47,060
lot of people think well that sounds

306
00:12:47,060 --> 00:12:49,459
legit so the idea that we can have

307
00:12:49,459 --> 00:12:51,920
illegitimate organisations masquerading

308
00:12:51,920 --> 00:12:54,139
as credible health information sources I

309
00:12:54,139 --> 00:12:57,170
think it's is a real concern so I think

310
00:12:57,170 --> 00:12:59,420
about issues of branding how do we

311
00:12:59,420 --> 00:13:01,540
cultivate trust and maybe leveraging

312
00:13:01,540 --> 00:13:05,060
influencers or trusted sources to get

313
00:13:05,060 --> 00:13:06,920
that source right so we have to have

314
00:13:06,920 --> 00:13:09,620
source trusts you know and up to to

315
00:13:09,620 --> 00:13:13,250
enable to enable message perceived

316
00:13:13,250 --> 00:13:16,519
credibility yes we have a lot of bias in

317
00:13:16,519 --> 00:13:17,089
our sample

318
00:13:17,089 --> 00:13:18,829
frame you know we have you know people

319
00:13:18,829 --> 00:13:21,620
who have to trust to a certain extent

320
00:13:21,620 --> 00:13:23,509
you know the government and research and

321
00:13:23,509 --> 00:13:24,939
science to be enrolled in the

322
00:13:24,939 --> 00:13:27,350
experimental setting sometimes I feel

323
00:13:27,350 --> 00:13:29,480
very frustrated it's not how everyday

324
00:13:29,480 --> 00:13:31,939
people interface with the barrage of

325
00:13:31,939 --> 00:13:33,920
messages like today we heard about the

326
00:13:33,920 --> 00:13:36,319
volume and just the rapidity of getting

327
00:13:36,319 --> 00:13:38,749
messages so we gave them a lot of time

328
00:13:38,749 --> 00:13:40,279
to think through things so I think

329
00:13:40,279 --> 00:13:43,009
that's another bias we want to do an

330
00:13:43,009 --> 00:13:45,079
experimental study but it was not a true

331
00:13:45,079 --> 00:13:47,180
RCT so there are a lot of exploratory

332
00:13:47,180 --> 00:13:50,059
components so if I were to do it again I

333
00:13:50,059 --> 00:13:51,470
think I would stick with one or the

334
00:13:51,470 --> 00:13:53,629
other rather than try to find try to

335
00:13:53,629 --> 00:13:55,459
answer all the questions we wanted to

336
00:13:55,459 --> 00:13:59,120
answer all in one study also sample size

337
00:13:59,120 --> 00:14:02,300
and complex design definitely impeded us

338
00:14:02,300 --> 00:14:04,550
from finding a lot of significant things

339
00:14:04,550 --> 00:14:07,550
so in closing I want to just highlight

340
00:14:07,550 --> 00:14:10,399
what in the last three years of reading

341
00:14:10,399 --> 00:14:12,559
and talking to experts what we know

342
00:14:12,559 --> 00:14:15,470
about misinformation on social media we

343
00:14:15,470 --> 00:14:17,420
know that echo chambers are real and

344
00:14:17,420 --> 00:14:19,519
they are enforced every day even today

345
00:14:19,519 --> 00:14:21,980
this morning we hear about people's self

346
00:14:21,980 --> 00:14:24,350
curated content looking for messages

347
00:14:24,350 --> 00:14:26,899
ways to find things they prefer to see

348
00:14:26,899 --> 00:14:30,110
that it reinforced you know like-minded

349
00:14:30,110 --> 00:14:33,290
conversations and attitudes we also

350
00:14:33,290 --> 00:14:35,569
knows falsehoods spread easier and

351
00:14:35,569 --> 00:14:37,759
faster sometimes it's because of

352
00:14:37,759 --> 00:14:39,679
information vacuum sometimes it's

353
00:14:39,679 --> 00:14:42,290
because it's simplistic so think about

354
00:14:42,290 --> 00:14:44,509
simplified causes and remedies of a

355
00:14:44,509 --> 00:14:47,209
disease and things like that or pre

356
00:14:47,209 --> 00:14:50,240
established beliefs so as everybody

357
00:14:50,240 --> 00:14:51,920
knows and I'm looking forward to hearing

358
00:14:51,920 --> 00:14:55,759
more later on with a you know rapid you

359
00:14:55,759 --> 00:14:58,429
know spread such as the corona virus

360
00:14:58,429 --> 00:15:00,649
there is real fear there so it's not

361
00:15:00,649 --> 00:15:02,779
just people intentionally going after

362
00:15:02,779 --> 00:15:05,389
misinformation is there's an information

363
00:15:05,389 --> 00:15:07,910
vacuum and therefore people are looking

364
00:15:07,910 --> 00:15:10,249
for things that perhaps are a little bit

365
00:15:10,249 --> 00:15:14,300
easier to digest in contrast to sort of

366
00:15:14,300 --> 00:15:16,249
the simplified definition I think

367
00:15:16,249 --> 00:15:18,319
credible health information tends to be

368
00:15:18,319 --> 00:15:20,990
complex nuanced and uncertain that's

369
00:15:20,990 --> 00:15:22,699
nature of science and science

370
00:15:22,699 --> 00:15:24,800
communication evidence is evolving

371
00:15:24,800 --> 00:15:26,660
sometimes even changing and inconclusive

372
00:15:26,660 --> 00:15:28,690
at times

373
00:15:28,690 --> 00:15:31,240
so disinformation campaign often create

374
00:15:31,240 --> 00:15:33,550
the sense of false equivalency so

375
00:15:33,550 --> 00:15:34,930
there's an impression that there's a

376
00:15:34,930 --> 00:15:36,940
lack of truth when there is truth such

377
00:15:36,940 --> 00:15:39,550
as vaccine and many other topics so

378
00:15:39,550 --> 00:15:41,320
these are at least becomes topics that

379
00:15:41,320 --> 00:15:43,270
are very hard to tackle because they

380
00:15:43,270 --> 00:15:45,820
have created this idea that there's no

381
00:15:45,820 --> 00:15:48,670
consensus right so I love the quote from

382
00:15:48,670 --> 00:15:51,700
David bronikowski sage a pH paper which

383
00:15:51,700 --> 00:15:54,210
was published during our working group

384
00:15:54,210 --> 00:15:58,150
days to say that there's a risk that

385
00:15:58,150 --> 00:16:00,820
these Russian accounts and the bots and

386
00:16:00,820 --> 00:16:03,070
trolls create false equivalency and

387
00:16:03,070 --> 00:16:05,290
therefore eroding consensus on

388
00:16:05,290 --> 00:16:06,940
vaccinations I think that's a critical

389
00:16:06,940 --> 00:16:08,580
point to think about

390
00:16:08,580 --> 00:16:11,950
lastly the industry's attitudes and I'm

391
00:16:11,950 --> 00:16:15,400
happy to learn more from our industry

392
00:16:15,400 --> 00:16:17,560
partners attitudes policies and

393
00:16:17,560 --> 00:16:19,450
practices turn towards misinformation

394
00:16:19,450 --> 00:16:22,000
and the role of content moderation are

395
00:16:22,000 --> 00:16:25,840
constantly evolving so you may have seen

396
00:16:25,840 --> 00:16:27,100
if you've seen me talk I've had a

397
00:16:27,100 --> 00:16:29,380
similar graph but every time I do a talk

398
00:16:29,380 --> 00:16:31,570
I update those sort of priorities and

399
00:16:31,570 --> 00:16:34,990
because of the time limitation I'm not

400
00:16:34,990 --> 00:16:36,400
going to be able to read through it but

401
00:16:36,400 --> 00:16:38,560
these are the four areas I think we need

402
00:16:38,560 --> 00:16:42,280
to invest a lot of our time and did

403
00:16:42,280 --> 00:16:43,960
anybody I think somebody want to take a

404
00:16:43,960 --> 00:16:45,460
picture sorry I just want to let you

405
00:16:45,460 --> 00:16:49,810
take a picture in happy to chat more

406
00:16:49,810 --> 00:16:54,520
about it and my last slide is really

407
00:16:54,520 --> 00:16:56,860
thinking about a divided world that we

408
00:16:56,860 --> 00:16:59,380
live in and a social scientist what can

409
00:16:59,380 --> 00:17:01,060
we do about it I think we need both

410
00:17:01,060 --> 00:17:02,890
basic science you know like the

411
00:17:02,890 --> 00:17:04,869
experimental study and applied real

412
00:17:04,869 --> 00:17:07,150
world science we need to bring people

413
00:17:07,150 --> 00:17:09,280
together and this is a great forum for

414
00:17:09,280 --> 00:17:11,650
that multidisciplinary projects and the

415
00:17:11,650 --> 00:17:13,750
topics that we all tako have a lot in

416
00:17:13,750 --> 00:17:15,790
common so think about infectious disease

417
00:17:15,790 --> 00:17:18,579
medical hoax debatable or sensitive

418
00:17:18,579 --> 00:17:20,349
health topics such as some vaccine

419
00:17:20,349 --> 00:17:22,720
abortion gun violence climate change I

420
00:17:22,720 --> 00:17:24,790
think the mechanisms to study these

421
00:17:24,790 --> 00:17:27,099
things are also very shared mistrust

422
00:17:27,099 --> 00:17:30,030
fear uncertainty racism and xenophobia

423
00:17:30,030 --> 00:17:32,740
emotions identity values we've heard

424
00:17:32,740 --> 00:17:34,870
these words repeated over and over and

425
00:17:34,870 --> 00:17:37,060
how to do it is really where I'm most

426
00:17:37,060 --> 00:17:39,429
interested I think we need to go beyond

427
00:17:39,429 --> 00:17:40,630
traditional method

428
00:17:40,630 --> 00:17:42,610
to think about data mining experimental

429
00:17:42,610 --> 00:17:45,100
design use of eye tracking pragmatic

430
00:17:45,100 --> 00:17:47,470
trials and qualitative I'm originally

431
00:17:47,470 --> 00:17:50,230
trained in qualitative research thinking

432
00:17:50,230 --> 00:17:52,150
about discourse analysis what's behind

433
00:17:52,150 --> 00:17:54,040
the lives what's the intention of

434
00:17:54,040 --> 00:17:56,560
sharing so then we can inform

435
00:17:56,560 --> 00:17:58,540
interventions to improve public health

436
00:17:58,540 --> 00:18:00,820
so thank you

437
00:18:00,820 --> 00:18:05,490
[Applause]

