1
00:00:00,000 --> 00:00:02,070
thanks very much everyone for being here

2
00:00:02,070 --> 00:00:04,200
I think this is so Leo's just saying

3
00:00:04,200 --> 00:00:05,850
such a great kind of forum for bringing

4
00:00:05,850 --> 00:00:07,589
together people that are working in the

5
00:00:07,589 --> 00:00:09,030
trenches and trying to study these

6
00:00:09,030 --> 00:00:11,099
things from kind of a more abstract

7
00:00:11,099 --> 00:00:13,110
perspective and I think that that's just

8
00:00:13,110 --> 00:00:15,420
really valuable and there's a clicker

9
00:00:15,420 --> 00:00:27,779
somewhere here right so I'm kind of

10
00:00:27,779 --> 00:00:29,789
taking an opposite perspective from what

11
00:00:29,789 --> 00:00:31,230
you just saw today so I'm gonna talk

12
00:00:31,230 --> 00:00:33,510
about kind of an overview of some of the

13
00:00:33,510 --> 00:00:35,309
research that I've done specifically on

14
00:00:35,309 --> 00:00:37,440
looking at misinformation on social

15
00:00:37,440 --> 00:00:39,629
media and especially how we can how

16
00:00:39,629 --> 00:00:41,930
people can correct each other on

17
00:00:41,930 --> 00:00:44,670
misinformation on social media so I'm

18
00:00:44,670 --> 00:00:46,230
gonna talk about kind of three and a

19
00:00:46,230 --> 00:00:49,590
half studies out of about eleven that

20
00:00:49,590 --> 00:00:51,000
we've done at this point so if you have

21
00:00:51,000 --> 00:00:52,739
questions about any of them that that's

22
00:00:52,739 --> 00:00:57,420
what Q&A and Twitter are for so social

23
00:00:57,420 --> 00:00:58,800
media and misinformation I don't think I

24
00:00:58,800 --> 00:01:00,270
have to convince anyone in this room

25
00:01:00,270 --> 00:01:02,510
that this is a thing

26
00:01:02,510 --> 00:01:05,339
people criticize social media for being

27
00:01:05,339 --> 00:01:09,500
a harbinger a spreader of misinformation

28
00:01:09,500 --> 00:01:11,670
for lots of reasons that we've already

29
00:01:11,670 --> 00:01:13,170
talked about today so I won't dwell on

30
00:01:13,170 --> 00:01:18,180
that and you can kind of debate to the

31
00:01:18,180 --> 00:01:19,799
extent that this is actually a problem

32
00:01:19,799 --> 00:01:21,799
so there's some newer information

33
00:01:21,799 --> 00:01:24,119
academic research that shows that like

34
00:01:24,119 --> 00:01:25,380
people are not actually sharing that

35
00:01:25,380 --> 00:01:27,060
much misinformation on social media they

36
00:01:27,060 --> 00:01:28,920
don't see that much misinformation on

37
00:01:28,920 --> 00:01:30,869
social media but they think it's a

38
00:01:30,869 --> 00:01:33,840
problem so this is from 2019 pew study

39
00:01:33,840 --> 00:01:35,670
where people are saying that they're

40
00:01:35,670 --> 00:01:37,950
more worried about made-up news as a

41
00:01:37,950 --> 00:01:39,780
problem for the country and this is in

42
00:01:39,780 --> 00:01:42,450
the u.s. compared to things like racism

43
00:01:42,450 --> 00:01:44,460
and terrorism right so like people are

44
00:01:44,460 --> 00:01:46,070
really worried about this as a problem

45
00:01:46,070 --> 00:01:49,530
and they think that it's undermining the

46
00:01:49,530 --> 00:01:51,840
way that our society works so that's why

47
00:01:51,840 --> 00:01:53,549
we think there's something worth

48
00:01:53,549 --> 00:01:57,990
carrying about today I'm focusing

49
00:01:57,990 --> 00:01:59,969
specifically on health misinformation oh

50
00:01:59,969 --> 00:02:02,460
I should also say all of this research

51
00:02:02,460 --> 00:02:03,840
that I'm presenting on is co-authored

52
00:02:03,840 --> 00:02:06,869
with my co-author Emily Varga at the

53
00:02:06,869 --> 00:02:09,810
University of Minnesota so we focus on

54
00:02:09,810 --> 00:02:11,099
health misinformation for several

55
00:02:11,099 --> 00:02:12,190
reasons

56
00:02:12,190 --> 00:02:15,250
number one this is a kind of classic

57
00:02:15,250 --> 00:02:17,020
definition of misinformation from Diana

58
00:02:17,020 --> 00:02:17,680
and I Fleur

59
00:02:17,680 --> 00:02:19,270
people's beliefs about factual matters

60
00:02:19,270 --> 00:02:21,130
are not supported by clear evidence and

61
00:02:21,130 --> 00:02:23,140
expert opinion it turns out that this

62
00:02:23,140 --> 00:02:25,930
doesn't actually this is hard to

63
00:02:25,930 --> 00:02:28,900
identify because there's not often and

64
00:02:28,900 --> 00:02:30,490
it's really appropriate that we're at

65
00:02:30,490 --> 00:02:32,860
the National Academies today because

66
00:02:32,860 --> 00:02:35,380
they do this very well but you can only

67
00:02:35,380 --> 00:02:37,600
do this on certain issues right but

68
00:02:37,600 --> 00:02:39,370
there is not often clear evidence and

69
00:02:39,370 --> 00:02:40,960
expert opinion on lots and lots of

70
00:02:40,960 --> 00:02:43,450
issues so this is from an article that

71
00:02:43,450 --> 00:02:45,010
we just published looking at kind of

72
00:02:45,010 --> 00:02:46,660
like how hard it is to pin down this

73
00:02:46,660 --> 00:02:49,930
idea of what misinformation is and the

74
00:02:49,930 --> 00:02:52,060
easiest ones to talk about are the

75
00:02:52,060 --> 00:02:53,830
settled issues so at the top of the

76
00:02:53,830 --> 00:02:56,110
pyramid this is where we have decades of

77
00:02:56,110 --> 00:02:58,420
science it's really clear all of science

78
00:02:58,420 --> 00:03:00,640
scientific expertise is in consensus

79
00:03:00,640 --> 00:03:02,770
we've talked a lot about consensus today

80
00:03:02,770 --> 00:03:04,090
this is a really important idea and

81
00:03:04,090 --> 00:03:08,590
misinformation and all of the like most

82
00:03:08,590 --> 00:03:10,540
people kind of understand and believe

83
00:03:10,540 --> 00:03:14,260
all of that evidence or at least agree

84
00:03:14,260 --> 00:03:16,150
on who the experts are something along

85
00:03:16,150 --> 00:03:18,040
those lines it turns out there aren't

86
00:03:18,040 --> 00:03:19,540
that many issues that fit into that

87
00:03:19,540 --> 00:03:23,230
lovely top part of the pyramid so an

88
00:03:23,230 --> 00:03:24,730
example people like to talk about is

89
00:03:24,730 --> 00:03:26,410
social media companies if you search

90
00:03:26,410 --> 00:03:28,120
most of them at this point if you search

91
00:03:28,120 --> 00:03:30,610
for vaccines they don't show you actual

92
00:03:30,610 --> 00:03:32,440
search results about vaccines they show

93
00:03:32,440 --> 00:03:34,750
you consensus information or they send

94
00:03:34,750 --> 00:03:37,570
you to the CDC or the HHS or the w-h-o

95
00:03:37,570 --> 00:03:39,520
interestingly differently different

96
00:03:39,520 --> 00:03:41,080
platforms into two different places but

97
00:03:41,080 --> 00:03:43,270
that's a whole other conversation and

98
00:03:43,270 --> 00:03:45,520
that's like that's again the easy case

99
00:03:45,520 --> 00:03:47,020
right we all can well we can all agree

100
00:03:47,020 --> 00:03:50,650
with a very small minority that there is

101
00:03:50,650 --> 00:03:52,270
clear consensus on this issue and this

102
00:03:52,270 --> 00:03:54,340
is something we can do about even within

103
00:03:54,340 --> 00:03:56,709
science and health misinformation

104
00:03:56,709 --> 00:03:58,780
most of it is emerging which is to say

105
00:03:58,780 --> 00:04:00,670
there's just not enough information

106
00:04:00,670 --> 00:04:03,040
about it yet or it's even controversial

107
00:04:03,040 --> 00:04:06,720
which is to say either the evidence is

108
00:04:06,720 --> 00:04:10,330
mixed or in some cases we can't even

109
00:04:10,330 --> 00:04:12,970
agree on who counts as an expert so in

110
00:04:12,970 --> 00:04:15,340
the case of climate change for instance

111
00:04:15,340 --> 00:04:17,170
you often see these reports from like

112
00:04:17,170 --> 00:04:19,690
300 climate scientists and then when you

113
00:04:19,690 --> 00:04:21,190
actually start looking at them which the

114
00:04:21,190 --> 00:04:23,530
IFC n has done really well by the way of

115
00:04:23,530 --> 00:04:24,760
looking into the backgrounds of those

116
00:04:24,760 --> 00:04:26,050
sorts of people

117
00:04:26,050 --> 00:04:27,699
when you start looking at them it turns

118
00:04:27,699 --> 00:04:30,159
out that the climate scientists are like

119
00:04:30,159 --> 00:04:32,979
oil and gas attorneys or something like

120
00:04:32,979 --> 00:04:35,379
that right so maybe not what we would

121
00:04:35,379 --> 00:04:38,530
count as a climate scientists so anyway

122
00:04:38,530 --> 00:04:40,870
the point here is that health and

123
00:04:40,870 --> 00:04:42,370
science tends to be a little bit easier

124
00:04:42,370 --> 00:04:44,530
to kind of nail down what the scientific

125
00:04:44,530 --> 00:04:46,300
consensus is whereas like political

126
00:04:46,300 --> 00:04:49,270
issues are just all going to be in the

127
00:04:49,270 --> 00:04:51,639
bottom of the pyramid so that's part of

128
00:04:51,639 --> 00:04:53,490
why we look at health and misinformation

129
00:04:53,490 --> 00:04:55,750
there's also a lot of consumption of

130
00:04:55,750 --> 00:04:57,430
science and health news on social media

131
00:04:57,430 --> 00:04:59,889
so again looking at a Pew study you see

132
00:04:59,889 --> 00:05:02,169
that about half of people and these are

133
00:05:02,169 --> 00:05:04,719
Twitter and Facebook reports say they're

134
00:05:04,719 --> 00:05:06,250
seeing either science and technology or

135
00:05:06,250 --> 00:05:08,830
health and medicine information on those

136
00:05:08,830 --> 00:05:13,300
platforms in addition to seeing those

137
00:05:13,300 --> 00:05:14,770
kinds of information they're also seeing

138
00:05:14,770 --> 00:05:17,199
misinformation so this is from health

139
00:05:17,199 --> 00:05:19,599
feedback from 2019 they looked at the

140
00:05:19,599 --> 00:05:22,360
most shared health topic health articles

141
00:05:22,360 --> 00:05:27,069
on Facebook in 2018 and then had experts

142
00:05:27,069 --> 00:05:30,009
rate how true or false they were we

143
00:05:30,009 --> 00:05:31,419
won't go into the details of this but

144
00:05:31,419 --> 00:05:33,190
basically read is a worse color and

145
00:05:33,190 --> 00:05:35,529
Green is a better color so you can see

146
00:05:35,529 --> 00:05:37,029
the top ten are not looking really good

147
00:05:37,029 --> 00:05:41,229
in terms of veracity and this has

148
00:05:41,229 --> 00:05:43,719
important public impact right so people

149
00:05:43,719 --> 00:05:45,669
make decisions about their health about

150
00:05:45,669 --> 00:05:47,620
the health of their family about the

151
00:05:47,620 --> 00:05:49,360
things they buy about the things they do

152
00:05:49,360 --> 00:05:51,729
about when they go to the doctor based

153
00:05:51,729 --> 00:05:54,490
on the information that they get about

154
00:05:54,490 --> 00:05:56,979
health and science so what you're gonna

155
00:05:56,979 --> 00:05:58,270
eat whether or not you're gonna

156
00:05:58,270 --> 00:06:00,039
vaccinate your child how much exercise

157
00:06:00,039 --> 00:06:01,659
you're gonna do all of those sorts of

158
00:06:01,659 --> 00:06:03,520
things I'm pregnant right now I can tell

159
00:06:03,520 --> 00:06:05,409
you there's lots of misinformation on

160
00:06:05,409 --> 00:06:07,569
the Internet related to all of these

161
00:06:07,569 --> 00:06:14,759
things pushing the wrong button okay so

162
00:06:14,759 --> 00:06:18,009
that's the downside of social media but

163
00:06:18,009 --> 00:06:20,379
what I want to give you today is maybe

164
00:06:20,379 --> 00:06:21,879
thinking about the ways that social

165
00:06:21,879 --> 00:06:24,090
media can help fix this problem not just

166
00:06:24,090 --> 00:06:27,430
exacerbate it so why might this be

167
00:06:27,430 --> 00:06:29,349
something that we should be interested

168
00:06:29,349 --> 00:06:31,719
in we've heard a lot about echo chambers

169
00:06:31,719 --> 00:06:35,199
today echo chambers are a concern in

170
00:06:35,199 --> 00:06:36,740
certain areas but it turns out

171
00:06:36,740 --> 00:06:39,080
that your ties on social media tend to

172
00:06:39,080 --> 00:06:40,759
actually be more heterogeneous than your

173
00:06:40,759 --> 00:06:42,680
ties in real life so you spend most of

174
00:06:42,680 --> 00:06:44,060
your time in real life with people that

175
00:06:44,060 --> 00:06:46,160
are very very similar to you they have

176
00:06:46,160 --> 00:06:48,440
the same job as you they live in the

177
00:06:48,440 --> 00:06:50,300
same neighborhood as you they are often

178
00:06:50,300 --> 00:06:52,639
the same race as you all of those sorts

179
00:06:52,639 --> 00:06:54,440
of things on social media because you

180
00:06:54,440 --> 00:06:56,360
have a bigger Network you tend to have a

181
00:06:56,360 --> 00:06:59,030
more heterogeneous network so that means

182
00:06:59,030 --> 00:07:00,650
there's potential at least for some of

183
00:07:00,650 --> 00:07:02,240
those people to correct you because

184
00:07:02,240 --> 00:07:06,199
maybe they disagree with you there's

185
00:07:06,199 --> 00:07:08,210
also what Emily and I refer to as

186
00:07:08,210 --> 00:07:09,560
observational correction which is like

187
00:07:09,560 --> 00:07:13,729
the least sexy buzzword of all time but

188
00:07:13,729 --> 00:07:15,050
we can't come up with a better word for

189
00:07:15,050 --> 00:07:18,289
it which is that social media allows you

190
00:07:18,289 --> 00:07:20,479
to watch someone else get corrected so

191
00:07:20,479 --> 00:07:22,669
this is an actual case from my Facebook

192
00:07:22,669 --> 00:07:25,220
feed which I have anonymized with

193
00:07:25,220 --> 00:07:27,830
someone sharing this like alleged story

194
00:07:27,830 --> 00:07:29,449
that Sharpie is giving away markers this

195
00:07:29,449 --> 00:07:31,759
is not super nefarious misinformation

196
00:07:31,759 --> 00:07:35,090
right and someone you can see down here

197
00:07:35,090 --> 00:07:37,909
at the bottom corrects them and says

198
00:07:37,909 --> 00:07:40,220
poor Snopes has actually fact checked

199
00:07:40,220 --> 00:07:43,789
this story and said like no they're not

200
00:07:43,789 --> 00:07:45,560
actually giving away free markers it's

201
00:07:45,560 --> 00:07:46,699
not true

202
00:07:46,699 --> 00:07:50,630
so what I want you to emphasize here is

203
00:07:50,630 --> 00:07:53,120
that we don't care about this person on

204
00:07:53,120 --> 00:07:55,490
the top getting corrected so maybe she

205
00:07:55,490 --> 00:07:57,680
updates her beliefs or maybe she doesn't

206
00:07:57,680 --> 00:08:00,050
or maybe she actually gets really pissed

207
00:08:00,050 --> 00:08:01,699
off because this person corrected her

208
00:08:01,699 --> 00:08:04,099
what we really care about is basically

209
00:08:04,099 --> 00:08:06,860
all of you so if this is part of your

210
00:08:06,860 --> 00:08:08,840
network you get to watch that person be

211
00:08:08,840 --> 00:08:11,240
corrected and you get that information

212
00:08:11,240 --> 00:08:12,860
at the same time that you see the

213
00:08:12,860 --> 00:08:14,930
original misinformation and we think

214
00:08:14,930 --> 00:08:16,370
that that can be really powerful in

215
00:08:16,370 --> 00:08:18,770
updating your beliefs so there should be

216
00:08:18,770 --> 00:08:19,969
a lower threat to identity because

217
00:08:19,969 --> 00:08:21,830
you're not getting corrected someone

218
00:08:21,830 --> 00:08:25,370
else is getting corrected immediacy is

219
00:08:25,370 --> 00:08:26,750
really important here so one of the

220
00:08:26,750 --> 00:08:27,740
things we know about correcting

221
00:08:27,740 --> 00:08:29,210
misinformation is that the faster you

222
00:08:29,210 --> 00:08:30,740
can do it in relation to when someone

223
00:08:30,740 --> 00:08:32,958
sees the misinformation that matters so

224
00:08:32,958 --> 00:08:34,339
if you're seeing it roughly at the same

225
00:08:34,339 --> 00:08:37,039
time then that's important and there's

226
00:08:37,039 --> 00:08:40,250
important scalability here right the

227
00:08:40,250 --> 00:08:43,250
average Facebook user has 700 Facebook

228
00:08:43,250 --> 00:08:45,260
friends so we're talking about a lot of

229
00:08:45,260 --> 00:08:47,060
people that might see this happen in

230
00:08:47,060 --> 00:08:49,569
real time

231
00:08:49,610 --> 00:08:52,280
okay so the spoiler in case you like

232
00:08:52,280 --> 00:08:53,720
stopped listening or I run out of time

233
00:08:53,720 --> 00:08:58,400
or whatever is that this works and I can

234
00:08:58,400 --> 00:09:00,800
tell you that it works on lots of

235
00:09:00,800 --> 00:09:02,210
different issues so we've tested on

236
00:09:02,210 --> 00:09:04,670
genetically modified food the safety of

237
00:09:04,670 --> 00:09:07,820
that the origins of the Zika virus the

238
00:09:07,820 --> 00:09:10,790
safety of sunscreen whether you should

239
00:09:10,790 --> 00:09:12,320
get a flu shot whether the flu shot can

240
00:09:12,320 --> 00:09:15,290
give you a flu specifically weird things

241
00:09:15,290 --> 00:09:17,090
about raw milk there's weird

242
00:09:17,090 --> 00:09:18,710
misinformation about raw milk on the

243
00:09:18,710 --> 00:09:25,130
internet don't drink raw milk fYI it's a

244
00:09:25,130 --> 00:09:28,220
bad idea and it also works on lots of

245
00:09:28,220 --> 00:09:29,540
different platforms so we've tested this

246
00:09:29,540 --> 00:09:32,300
on Facebook on simulated Facebook feeds

247
00:09:32,300 --> 00:09:34,220
Twitter feeds Instagram feeds and a

248
00:09:34,220 --> 00:09:36,590
video platform that resembled YouTube

249
00:09:36,590 --> 00:09:39,110
but wasn't technically YouTube so that's

250
00:09:39,110 --> 00:09:41,120
kind of the overview of what I'm gonna

251
00:09:41,120 --> 00:09:42,260
tell you and then I'll kind of give you

252
00:09:42,260 --> 00:09:43,580
a little bit about how we actually do

253
00:09:43,580 --> 00:09:45,560
this so the first thing that we tested

254
00:09:45,560 --> 00:09:48,410
originally back in 2014 was whether a

255
00:09:48,410 --> 00:09:50,330
correction coming from a social media

256
00:09:50,330 --> 00:09:52,450
platform itself could be effective in

257
00:09:52,450 --> 00:09:56,000
updating people's misperceptions so

258
00:09:56,000 --> 00:09:57,680
roughly speaking what you're seeing here

259
00:09:57,680 --> 00:09:59,330
is at the top is someone sharing a

260
00:09:59,330 --> 00:10:01,760
misinformation story in this case

261
00:10:01,760 --> 00:10:04,070
they're saying GMOs make you sick

262
00:10:04,070 --> 00:10:06,440
and then this is the Facebook related

263
00:10:06,440 --> 00:10:09,260
stories functionality which has changed

264
00:10:09,260 --> 00:10:11,270
a lot since 2014 so it doesn't look

265
00:10:11,270 --> 00:10:13,310
exactly like this anymore but basically

266
00:10:13,310 --> 00:10:14,960
if you click on a link it shows you

267
00:10:14,960 --> 00:10:17,300
other stories you might be interested in

268
00:10:17,300 --> 00:10:19,670
clicking on to increase engagement and

269
00:10:19,670 --> 00:10:21,020
we said well what happens if you put

270
00:10:21,020 --> 00:10:23,510
fact-checking information in those

271
00:10:23,510 --> 00:10:25,370
related stories they actually do this

272
00:10:25,370 --> 00:10:27,110
now for things that have been fact

273
00:10:27,110 --> 00:10:28,340
checked by their third party fact

274
00:10:28,340 --> 00:10:32,570
checkers and essentially what we found

275
00:10:32,570 --> 00:10:34,430
is that this is really effective so if

276
00:10:34,430 --> 00:10:37,790
you get information from the platform

277
00:10:37,790 --> 00:10:40,580
correcting you on the left is a control

278
00:10:40,580 --> 00:10:41,810
group that didn't get that correction

279
00:10:41,810 --> 00:10:43,820
and on the right is the the group that

280
00:10:43,820 --> 00:10:46,250
got the correction this is looking only

281
00:10:46,250 --> 00:10:47,420
at people that initially had

282
00:10:47,420 --> 00:10:49,280
misperceptions on the issue so people

283
00:10:49,280 --> 00:10:50,810
that were initially misinformed about

284
00:10:50,810 --> 00:10:52,460
the safety of genetically modified food

285
00:10:52,460 --> 00:10:56,210
and you see there's a significant effect

286
00:10:56,210 --> 00:10:59,180
here in general I'll say across all of

287
00:10:59,180 --> 00:11:00,440
the studies we've done we see somewhere

288
00:11:00,440 --> 00:11:01,500
between like a 10

289
00:11:01,500 --> 00:11:04,270
25% effect in terms of drops and

290
00:11:04,270 --> 00:11:06,820
misperceptions so it's not that like

291
00:11:06,820 --> 00:11:08,680
everybody is instantly converted but

292
00:11:08,680 --> 00:11:10,600
there is you know a substantial effect

293
00:11:10,600 --> 00:11:12,130
at the individual level and again

294
00:11:12,130 --> 00:11:14,980
because the scales to a large number of

295
00:11:14,980 --> 00:11:16,180
people we think that that's still a

296
00:11:16,180 --> 00:11:19,240
meaningful effect it's worth noting that

297
00:11:19,240 --> 00:11:21,010
this was the one study that we have ever

298
00:11:21,010 --> 00:11:23,890
done where we had an issue that didn't

299
00:11:23,890 --> 00:11:25,870
work and that issue was the safety of

300
00:11:25,870 --> 00:11:30,340
the MMR vaccine so that seems to be a an

301
00:11:30,340 --> 00:11:33,190
issue for which beliefs are so ingrained

302
00:11:33,190 --> 00:11:34,750
that it is really difficult to move

303
00:11:34,750 --> 00:11:36,460
people and I think all of the people in

304
00:11:36,460 --> 00:11:37,810
this room that deal with misinformation

305
00:11:37,810 --> 00:11:43,240
probably get that so then this was kind

306
00:11:43,240 --> 00:11:45,700
of the what I really really wanted to

307
00:11:45,700 --> 00:11:48,570
study was how what about other users so

308
00:11:48,570 --> 00:11:50,500
platform okay great

309
00:11:50,500 --> 00:11:52,270
but I don't have a backdoor into

310
00:11:52,270 --> 00:11:53,940
Facebook to just tell them what to do

311
00:11:53,940 --> 00:11:56,830
let alone all the other platforms so

312
00:11:56,830 --> 00:11:58,720
what about individual users can they

313
00:11:58,720 --> 00:12:00,280
correct each other and is that effective

314
00:12:00,280 --> 00:12:01,960
or do people just say like oh you're

315
00:12:01,960 --> 00:12:04,780
some nobody on social media I don't care

316
00:12:04,780 --> 00:12:07,600
what you say so it's the same basic

317
00:12:07,600 --> 00:12:09,550
design here but instead of at the bottom

318
00:12:09,550 --> 00:12:11,650
instead of the correction coming from

319
00:12:11,650 --> 00:12:13,150
the platform it's coming from other

320
00:12:13,150 --> 00:12:17,680
users and they are doing basically

321
00:12:17,680 --> 00:12:20,560
exactly what the person in Sylvia's

322
00:12:20,560 --> 00:12:23,020
example of qualitative interviews was

323
00:12:23,020 --> 00:12:24,610
saying they were doing is I look for a

324
00:12:24,610 --> 00:12:26,350
link and I share it and I tell them like

325
00:12:26,350 --> 00:12:29,380
you know you're not right and this is

326
00:12:29,380 --> 00:12:30,700
basically what people are doing they're

327
00:12:30,700 --> 00:12:32,740
saying there's their scientific

328
00:12:32,740 --> 00:12:34,510
consensus that you know this isn't an

329
00:12:34,510 --> 00:12:37,300
issue or whatever and they're sharing

330
00:12:37,300 --> 00:12:39,370
credible links in this case from Snopes

331
00:12:39,370 --> 00:12:43,390
and CDC we varied those sources a lot in

332
00:12:43,390 --> 00:12:44,590
different studies and I can talk more

333
00:12:44,590 --> 00:12:48,100
about that so what we see is basically

334
00:12:48,100 --> 00:12:49,930
exactly the same drop this is a slightly

335
00:12:49,930 --> 00:12:51,610
different scale that way you saw before

336
00:12:51,610 --> 00:12:53,500
so it looks a little bit different but

337
00:12:53,500 --> 00:12:55,090
basically the algorithmic correction is

338
00:12:55,090 --> 00:12:56,380
what we call the platform based

339
00:12:56,380 --> 00:13:00,670
correction drops about the same amount

340
00:13:00,670 --> 00:13:02,560
as in the previous study and the social

341
00:13:02,560 --> 00:13:03,520
correction where you're getting

342
00:13:03,520 --> 00:13:05,590
correction from other users is almost

343
00:13:05,590 --> 00:13:07,240
exactly the same effect as you see it

344
00:13:07,240 --> 00:13:11,440
with the platform based correction a

345
00:13:11,440 --> 00:13:14,050
couple caveats here if you're a single

346
00:13:14,050 --> 00:13:15,070
user

347
00:13:15,070 --> 00:13:16,540
you actually need multiple corrections

348
00:13:16,540 --> 00:13:18,130
to be as effective as it coming from the

349
00:13:18,130 --> 00:13:20,350
platform so if you see someone share

350
00:13:20,350 --> 00:13:22,120
misinformation on social media and

351
00:13:22,120 --> 00:13:23,860
someone corrects them you should

352
00:13:23,860 --> 00:13:26,320
actually correct them again which is

353
00:13:26,320 --> 00:13:28,600
kind of counterintuitive and seems kind

354
00:13:28,600 --> 00:13:31,060
of like a dick move but you should do it

355
00:13:31,060 --> 00:13:33,460
anyway because that actually lends more

356
00:13:33,460 --> 00:13:36,280
credibility to that correction and the

357
00:13:36,280 --> 00:13:37,600
second thing is you have to provide a

358
00:13:37,600 --> 00:13:39,370
source so we tested this with sources

359
00:13:39,370 --> 00:13:41,620
and without sources just providing the

360
00:13:41,620 --> 00:13:42,910
link right they couldn't even click on

361
00:13:42,910 --> 00:13:45,870
the link and just showing that you had

362
00:13:45,870 --> 00:13:48,670
information to back up your claim was

363
00:13:48,670 --> 00:13:51,610
really effective and then finally

364
00:13:51,610 --> 00:13:52,810
because I know there are a lot of

365
00:13:52,810 --> 00:13:55,420
institutions health institutions health

366
00:13:55,420 --> 00:13:57,520
organizations in the audience can you

367
00:13:57,520 --> 00:13:59,830
play a role in this as well and not

368
00:13:59,830 --> 00:14:01,510
surprisingly the answer is yes and

369
00:14:01,510 --> 00:14:03,220
you're actually a very credible source

370
00:14:03,220 --> 00:14:05,230
so Silvie was talking about in her

371
00:14:05,230 --> 00:14:07,330
presentation the CDC is a high

372
00:14:07,330 --> 00:14:10,330
credibility Health Organization 80% of

373
00:14:10,330 --> 00:14:12,970
Americans according to Pew trust the CDC

374
00:14:12,970 --> 00:14:14,230
and that's actually the rare situation

375
00:14:14,230 --> 00:14:16,840
where it goes across party lines so you

376
00:14:16,840 --> 00:14:19,420
you don't see a partisan divide on that

377
00:14:19,420 --> 00:14:22,480
issue so same basic set up you can see

378
00:14:22,480 --> 00:14:23,830
here we're testing it on Twitter instead

379
00:14:23,830 --> 00:14:25,330
of Facebook but this one we actually did

380
00:14:25,330 --> 00:14:28,060
on both and the CDC is is doing the

381
00:14:28,060 --> 00:14:29,470
correction here along with a link to

382
00:14:29,470 --> 00:14:33,850
themselves most health organizations

383
00:14:33,850 --> 00:14:35,620
it's worth noting don't do this where

384
00:14:35,620 --> 00:14:37,180
they are individually engaging with

385
00:14:37,180 --> 00:14:39,160
people that are wrong on social media I

386
00:14:39,160 --> 00:14:40,750
think they don't do that for obvious

387
00:14:40,750 --> 00:14:44,890
reasons but this is you know something

388
00:14:44,890 --> 00:14:46,180
that we think they could potentially do

389
00:14:46,180 --> 00:14:48,550
more of and the same pattern exists

390
00:14:48,550 --> 00:14:49,960
again this is a slightly different scale

391
00:14:49,960 --> 00:14:51,340
so it looks a little bit different but

392
00:14:51,340 --> 00:14:54,700
the CDC is helping people to update

393
00:14:54,700 --> 00:14:58,300
their misperceptions a few conclusions

394
00:14:58,300 --> 00:15:01,300
here so correct early we think the MMR

395
00:15:01,300 --> 00:15:03,910
non finding compared to all the wealth

396
00:15:03,910 --> 00:15:05,440
of other issues that we found that work

397
00:15:05,440 --> 00:15:07,750
is because it had a chance to really

398
00:15:07,750 --> 00:15:09,550
kind of get ingrained in the people that

399
00:15:09,550 --> 00:15:11,050
were misperceived

400
00:15:11,050 --> 00:15:14,530
have misperceptions about that issue all

401
00:15:14,530 --> 00:15:17,740
of these actors can play a role so Pew

402
00:15:17,740 --> 00:15:19,540
has done some research asking people who

403
00:15:19,540 --> 00:15:21,100
they think is responsible for fixing the

404
00:15:21,100 --> 00:15:23,170
problem misinformation they generally

405
00:15:23,170 --> 00:15:24,520
say the media which is kind of

406
00:15:24,520 --> 00:15:26,290
interesting and curious and a whole

407
00:15:26,290 --> 00:15:28,320
other topic of conversation

408
00:15:28,320 --> 00:15:30,540
we are research finds that lots of

409
00:15:30,540 --> 00:15:32,220
people can play a role here health

410
00:15:32,220 --> 00:15:34,140
organizations can play a role platforms

411
00:15:34,140 --> 00:15:35,820
can play your role users can play a role

412
00:15:35,820 --> 00:15:38,430
and they probably do better when they

413
00:15:38,430 --> 00:15:40,430
all are working in the same direction so

414
00:15:40,430 --> 00:15:44,100
nobody gets off the hook providing clear

415
00:15:44,100 --> 00:15:46,620
sources and clear headlines is really

416
00:15:46,620 --> 00:15:47,850
important if you're a health

417
00:15:47,850 --> 00:15:49,860
organization providing shareable

418
00:15:49,860 --> 00:15:51,390
information that people can really

419
00:15:51,390 --> 00:15:54,960
easily find and give to other people if

420
00:15:54,960 --> 00:15:56,460
they are trying to engage in this

421
00:15:56,460 --> 00:15:58,530
corrective behavior is really important

422
00:15:58,530 --> 00:16:00,090
so thinking about how do you make it

423
00:16:00,090 --> 00:16:01,680
findable and how do you make it

424
00:16:01,680 --> 00:16:03,600
shareable and how do you make it clear

425
00:16:03,600 --> 00:16:04,710
and compelling in all the ways that

426
00:16:04,710 --> 00:16:05,820
we've been talking about this morning

427
00:16:05,820 --> 00:16:08,130
and then thinking about who you're able

428
00:16:08,130 --> 00:16:09,840
to reach so a couple of things that I

429
00:16:09,840 --> 00:16:12,930
didn't talk about today but some people

430
00:16:12,930 --> 00:16:14,610
are easier to persuade than others

431
00:16:14,610 --> 00:16:16,620
people that are really ingrained on a

432
00:16:16,620 --> 00:16:19,500
particular misinformation topic may not

433
00:16:19,500 --> 00:16:21,660
be that movable and maybe that's not

434
00:16:21,660 --> 00:16:23,910
worth spending a lot of time and

435
00:16:23,910 --> 00:16:25,590
resources on to move people that aren't

436
00:16:25,590 --> 00:16:30,450
movable so that is all but I want to

437
00:16:30,450 --> 00:16:31,650
take advantage of the fact that I'm in

438
00:16:31,650 --> 00:16:34,770
this room with lots of smart people the

439
00:16:34,770 --> 00:16:36,060
things that we are interested in doing

440
00:16:36,060 --> 00:16:39,930
to extend this research are first trying

441
00:16:39,930 --> 00:16:41,700
to do this in the field so if any of you

442
00:16:41,700 --> 00:16:43,740
are a an organization a health

443
00:16:43,740 --> 00:16:45,000
organization that's interested in

444
00:16:45,000 --> 00:16:48,870
pursuing this kind of research please

445
00:16:48,870 --> 00:16:53,820
talk to me talk to me this way and we're

446
00:16:53,820 --> 00:16:55,050
also interested in figuring out how we

447
00:16:55,050 --> 00:16:56,400
motivate this kind of correction

448
00:16:56,400 --> 00:16:57,930
behavior so how do we get people to do

449
00:16:57,930 --> 00:16:59,490
more of it whether that's making things

450
00:16:59,490 --> 00:17:01,950
easier to find easier to share easier to

451
00:17:01,950 --> 00:17:05,760
do so that's if you are interested in

452
00:17:05,760 --> 00:17:06,839
those things please come talk to you

453
00:17:06,839 --> 00:17:08,098
Thanks

454
00:17:08,098 --> 00:17:12,688
[Applause]

