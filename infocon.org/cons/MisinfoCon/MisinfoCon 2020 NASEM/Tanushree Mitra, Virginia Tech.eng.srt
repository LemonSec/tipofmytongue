1
00:00:00,179 --> 00:00:02,939
everyone thanks for the 10 of you who

2
00:00:02,939 --> 00:00:10,349
are around so yes I'm an assistant

3
00:00:10,349 --> 00:00:12,059
professor in the school of computer

4
00:00:12,059 --> 00:00:14,370
science at Virginia Tech and so this

5
00:00:14,370 --> 00:00:16,529
work is with my amazing students this is

6
00:00:16,529 --> 00:00:17,940
part of a larger threat of work where we

7
00:00:17,940 --> 00:00:20,789
are trying to measure misinformation and

8
00:00:20,789 --> 00:00:22,560
conspiracy theories are that are

9
00:00:22,560 --> 00:00:24,869
surfaced by algorithmic search platforms

10
00:00:24,869 --> 00:00:27,180
and this one specifically looks at

11
00:00:27,180 --> 00:00:28,800
finding misinformation on video search

12
00:00:28,800 --> 00:00:31,199
platform that is in our case we looked

13
00:00:31,199 --> 00:00:33,870
at YouTube so nowadays it is fairly

14
00:00:33,870 --> 00:00:35,820
common to see headlines like this how

15
00:00:35,820 --> 00:00:37,680
YouTube is driving people to the darkest

16
00:00:37,680 --> 00:00:39,690
corners of the internet how Google is

17
00:00:39,690 --> 00:00:41,610
directing people to extreme content and

18
00:00:41,610 --> 00:00:44,070
then you see these op-ed that YouTube is

19
00:00:44,070 --> 00:00:46,110
a create radicalize er right and then

20
00:00:46,110 --> 00:00:47,760
you also see YouTube responding by

21
00:00:47,760 --> 00:00:49,860
saying they're reducing conspiracy

22
00:00:49,860 --> 00:00:51,750
theory recommendations are that they are

23
00:00:51,750 --> 00:00:53,670
making conspiracy videos really hard to

24
00:00:53,670 --> 00:00:56,670
find but all these reports and responses

25
00:00:56,670 --> 00:00:58,980
all of these are fairly anecdotal that

26
00:00:58,980 --> 00:01:00,870
is there is not enough hard empirical

27
00:01:00,870 --> 00:01:03,149
statistical evidence to prove any of

28
00:01:03,149 --> 00:01:05,430
these anecdotal stories and and also to

29
00:01:05,430 --> 00:01:07,229
disprove any of these counter responses

30
00:01:07,229 --> 00:01:09,689
so that is what the goal of our study

31
00:01:09,689 --> 00:01:12,090
was in order to verify some of these

32
00:01:12,090 --> 00:01:14,909
adding total claims criticizing YouTube

33
00:01:14,909 --> 00:01:17,310
for surfacing problematic content on its

34
00:01:17,310 --> 00:01:19,740
platform so the question is does

35
00:01:19,740 --> 00:01:21,689
YouTube's search and recommendation

36
00:01:21,689 --> 00:01:23,250
algorithm really surfaces these

37
00:01:23,250 --> 00:01:24,990
problematic content and then what are

38
00:01:24,990 --> 00:01:27,000
the conditions under which it it does

39
00:01:27,000 --> 00:01:29,130
that and the way we did it is by

40
00:01:29,130 --> 00:01:31,290
conducting systematic audits on on

41
00:01:31,290 --> 00:01:33,659
YouTube's algorithm and we picked one

42
00:01:33,659 --> 00:01:35,250
type of problematic content that is

43
00:01:35,250 --> 00:01:38,009
conspiracy theories so these are the

44
00:01:38,009 --> 00:01:40,530
questions that our work answered so very

45
00:01:40,530 --> 00:01:42,360
specific and like some of the other

46
00:01:42,360 --> 00:01:44,220
studies president this morning not that

47
00:01:44,220 --> 00:01:47,220
broad but the first question was looked

48
00:01:47,220 --> 00:01:49,860
at this this question of does the users

49
00:01:49,860 --> 00:01:52,049
demographics let's say their age or

50
00:01:52,049 --> 00:01:54,180
gender and the location from which they

51
00:01:54,180 --> 00:01:57,659
are querying the search engine YouTube

52
00:01:57,659 --> 00:02:00,270
search a video search platform does that

53
00:02:00,270 --> 00:02:01,380
have an effect on the amount of

54
00:02:01,380 --> 00:02:03,780
misinformation that YouTube is reduce

55
00:02:03,780 --> 00:02:05,159
YouTube search and recommendation

56
00:02:05,159 --> 00:02:07,469
algorithm is returning the next question

57
00:02:07,469 --> 00:02:09,810
is the question of watch history so what

58
00:02:09,810 --> 00:02:13,040
happens when the user has watched

59
00:02:13,040 --> 00:02:16,549
some videos let's say all-pro conspiracy

60
00:02:16,549 --> 00:02:18,260
video are all debunking or all neutral

61
00:02:18,260 --> 00:02:20,269
videos what happens under those scenario

62
00:02:20,269 --> 00:02:22,760
does the platform return more or less

63
00:02:22,760 --> 00:02:25,760
misinformation and in the final piece

64
00:02:25,760 --> 00:02:27,500
here is is there any difference across

65
00:02:27,500 --> 00:02:29,180
the topics that is what happens if it's

66
00:02:29,180 --> 00:02:31,370
user researching for vaccine versus

67
00:02:31,370 --> 00:02:33,200
flatter to said effect difference in the

68
00:02:33,200 --> 00:02:36,709
amount of misinformation returned so

69
00:02:36,709 --> 00:02:40,159
this is our entire audit framework and

70
00:02:40,159 --> 00:02:42,230
so I'm gonna briefly go into each of

71
00:02:42,230 --> 00:02:44,890
these steps so the first we started with

72
00:02:44,890 --> 00:02:47,989
selecting a search topics and our goal

73
00:02:47,989 --> 00:02:50,000
here was to make sure that the topics

74
00:02:50,000 --> 00:02:51,590
that we are auditing are these high

75
00:02:51,590 --> 00:02:54,769
impact misinformation search topics and

76
00:02:54,769 --> 00:02:56,510
so that is lots of people have searched

77
00:02:56,510 --> 00:02:58,639
from them so forth and so it has some

78
00:02:58,639 --> 00:03:01,190
high impact impacting several people and

79
00:03:01,190 --> 00:03:03,500
that these topics should also have some

80
00:03:03,500 --> 00:03:07,250
definitive false value right and so how

81
00:03:07,250 --> 00:03:09,290
do we determine that so we went with

82
00:03:09,290 --> 00:03:11,659
Wikipedia and then Wikipedia's list of

83
00:03:11,659 --> 00:03:13,579
conspiracy theories is a human curated

84
00:03:13,579 --> 00:03:15,500
list of a definite false conspiracy

85
00:03:15,500 --> 00:03:17,389
theories so that's was our starting

86
00:03:17,389 --> 00:03:20,269
point and then to figure out whether

87
00:03:20,269 --> 00:03:22,840
some of these theories are have been

88
00:03:22,840 --> 00:03:24,859
affecting lots of people that whether

89
00:03:24,859 --> 00:03:26,389
they are period whether they're popular

90
00:03:26,389 --> 00:03:28,549
or not we went with Google Trends and so

91
00:03:28,549 --> 00:03:30,650
Google Trends we fed these let's say you

92
00:03:30,650 --> 00:03:32,959
feed 9/11 conspiracy theories you you

93
00:03:32,959 --> 00:03:34,879
fix the United States we are only

94
00:03:34,879 --> 00:03:36,919
concerned about what happens in the US

95
00:03:36,919 --> 00:03:39,109
and then we looked at a two years time

96
00:03:39,109 --> 00:03:42,049
span 2016 to 2018 and then YouTube as

97
00:03:42,049 --> 00:03:45,199
our search service of choice and then we

98
00:03:45,199 --> 00:03:46,669
looked at whether there are some form of

99
00:03:46,669 --> 00:03:49,280
trend in the data so here you of course

100
00:03:49,280 --> 00:03:52,040
you see a 9/11 search is going up at

101
00:03:52,040 --> 00:03:54,290
during the September timeline but

102
00:03:54,290 --> 00:03:56,389
overall people are consistently

103
00:03:56,389 --> 00:03:58,579
searching for this topic for two years

104
00:03:58,579 --> 00:04:00,650
we saw the same thing for some other

105
00:04:00,650 --> 00:04:01,699
theories like the moon landing

106
00:04:01,699 --> 00:04:04,040
conspiracy theories you see these

107
00:04:04,040 --> 00:04:06,109
consistent trend lines so doing this is

108
00:04:06,109 --> 00:04:07,970
combining Wikipedia's data and the

109
00:04:07,970 --> 00:04:10,400
Google Trends trend analysis we came up

110
00:04:10,400 --> 00:04:13,729
with five topics for our audit analysis

111
00:04:13,729 --> 00:04:16,370
this was 9/11 conspiracy theories the

112
00:04:16,370 --> 00:04:19,358
vaccine conspiracy theories moon landing

113
00:04:19,358 --> 00:04:22,280
chemtrail conspiracy theories and flat

114
00:04:22,280 --> 00:04:26,180
earth our next step was to select

115
00:04:26,180 --> 00:04:28,400
search queries that present these topics

116
00:04:28,400 --> 00:04:30,650
these myths informative topics and for

117
00:04:30,650 --> 00:04:32,870
that we used two different sources of

118
00:04:32,870 --> 00:04:33,229
data

119
00:04:33,229 --> 00:04:36,560
the first was YouTube's Auto suggestions

120
00:04:36,560 --> 00:04:39,259
so you feed YouTube with some seed query

121
00:04:39,259 --> 00:04:41,389
representing that topic and then collect

122
00:04:41,389 --> 00:04:44,240
the top 10 suggestions that the platform

123
00:04:44,240 --> 00:04:46,130
provides this kind of tells us that we

124
00:04:46,130 --> 00:04:47,810
are focusing on the trending search

125
00:04:47,810 --> 00:04:49,400
queries that's happening in YouTube at

126
00:04:49,400 --> 00:04:51,289
that time and then we also went with

127
00:04:51,289 --> 00:04:53,030
Google Trends and then also collected

128
00:04:53,030 --> 00:04:54,500
for that topic what are the related

129
00:04:54,500 --> 00:04:56,270
queries that people have searched so

130
00:04:56,270 --> 00:04:58,250
again high-impact Karelian query

131
00:04:58,250 --> 00:05:00,740
perennial queries right so combining

132
00:05:00,740 --> 00:05:03,289
both of them formed our set of search

133
00:05:03,289 --> 00:05:05,810
queries for a particular topic that we

134
00:05:05,810 --> 00:05:07,849
then used for conducting our audits

135
00:05:07,849 --> 00:05:09,919
right so we're finally ready to launch

136
00:05:09,919 --> 00:05:11,960
our audits with these two parameters in

137
00:05:11,960 --> 00:05:14,120
place and so our audits essentially

138
00:05:14,120 --> 00:05:16,550
comprised of two sets of experiments

139
00:05:16,550 --> 00:05:18,620
first was a search shorted experiments

140
00:05:18,620 --> 00:05:20,330
this was for auditing their search

141
00:05:20,330 --> 00:05:22,699
algorithm and here the audits were

142
00:05:22,699 --> 00:05:25,250
conducted with brand new Google or

143
00:05:25,250 --> 00:05:26,780
YouTube accounts that is these accounts

144
00:05:26,780 --> 00:05:28,340
were emulating how real people would be

145
00:05:28,340 --> 00:05:30,139
searching the platform and then the

146
00:05:30,139 --> 00:05:32,090
watch audits were conducted with

147
00:05:32,090 --> 00:05:33,680
accounts that had already built some

148
00:05:33,680 --> 00:05:35,500
form of watch history by watching

149
00:05:35,500 --> 00:05:37,760
conspiratorial videos of certain stance

150
00:05:37,760 --> 00:05:40,849
right and then we audited the three

151
00:05:40,849 --> 00:05:42,650
different components of YouTube the

152
00:05:42,650 --> 00:05:44,240
search results component this was for

153
00:05:44,240 --> 00:05:45,889
auditing their social Garthim and for

154
00:05:45,889 --> 00:05:48,830
recommendations we audited the YouTube

155
00:05:48,830 --> 00:05:50,360
and the top five recommendation

156
00:05:50,360 --> 00:05:52,909
algorithm so coming back to our first

157
00:05:52,909 --> 00:05:54,800
research question does age and gender

158
00:05:54,800 --> 00:05:57,919
and geolocation have any effect so for

159
00:05:57,919 --> 00:06:00,620
doing this sort of audits we focused on

160
00:06:00,620 --> 00:06:02,750
four different age groups then the

161
00:06:02,750 --> 00:06:06,110
gender was male female and in order to

162
00:06:06,110 --> 00:06:08,000
do this we created different accounts

163
00:06:08,000 --> 00:06:10,490
which emulated these different personas

164
00:06:10,490 --> 00:06:14,120
right so age or our male or female and

165
00:06:14,120 --> 00:06:16,580
then for geolocation we found what are

166
00:06:16,580 --> 00:06:18,620
the hot and the cold locations right

167
00:06:18,620 --> 00:06:20,479
what do I mean by that these are the

168
00:06:20,479 --> 00:06:22,849
regions which have the highest so hot or

169
00:06:22,849 --> 00:06:24,620
the lowest or low interest in a

170
00:06:24,620 --> 00:06:26,630
particular topic right so and we found

171
00:06:26,630 --> 00:06:28,669
these hot and cold locations by using

172
00:06:28,669 --> 00:06:31,430
these Google's interest over time graph

173
00:06:31,430 --> 00:06:33,050
so the darker locations are these hot

174
00:06:33,050 --> 00:06:35,360
regions lots of interest and the lighter

175
00:06:35,360 --> 00:06:37,099
are these cold region less interest for

176
00:06:37,099 --> 00:06:39,030
that topic so this is this

177
00:06:39,030 --> 00:06:40,950
how it looks for all the topics here so

178
00:06:40,950 --> 00:06:42,870
if you look at flattered that someone on

179
00:06:42,870 --> 00:06:44,670
the hot region that we selected was

180
00:06:44,670 --> 00:06:47,250
Montana a cold region New Jersey and so

181
00:06:47,250 --> 00:06:49,770
on right and so once we had these

182
00:06:49,770 --> 00:06:52,680
regions defined then we created BOTS

183
00:06:52,680 --> 00:06:55,500
to fire these queries from IP addresses

184
00:06:55,500 --> 00:06:57,210
from these locations that's how we are

185
00:06:57,210 --> 00:07:00,210
conducting our audits so this is a

186
00:07:00,210 --> 00:07:02,130
little more detail into our search audit

187
00:07:02,130 --> 00:07:02,880
experiment

188
00:07:02,880 --> 00:07:04,950
lots of the lots of patek see over there

189
00:07:04,950 --> 00:07:07,820
but essentially this is how Ursuline we

190
00:07:07,820 --> 00:07:10,680
created our scripted selenium BOTS which

191
00:07:10,680 --> 00:07:12,810
behave like normal users logged into

192
00:07:12,810 --> 00:07:15,900
YouTube search like normal users and and

193
00:07:15,900 --> 00:07:16,830
then on the back end

194
00:07:16,830 --> 00:07:18,300
there were scripts which were collecting

195
00:07:18,300 --> 00:07:20,280
the results that the that the search

196
00:07:20,280 --> 00:07:23,280
platform was returning for the watch

197
00:07:23,280 --> 00:07:25,230
experiments slightly different so these

198
00:07:25,230 --> 00:07:27,600
were while in the previous one these

199
00:07:27,600 --> 00:07:29,550
were really brand new accounts which in

200
00:07:29,550 --> 00:07:31,260
with no history they were searching but

201
00:07:31,260 --> 00:07:33,810
for watch experiments these accounts had

202
00:07:33,810 --> 00:07:36,300
already built their watch history so

203
00:07:36,300 --> 00:07:38,940
either by watching promoting videos

204
00:07:38,940 --> 00:07:41,100
debunking or neutral and we purposely

205
00:07:41,100 --> 00:07:42,450
made it a little more conservative here

206
00:07:42,450 --> 00:07:45,810
we made the accounts some of the

207
00:07:45,810 --> 00:07:48,330
accounts watch all promoting videos some

208
00:07:48,330 --> 00:07:49,890
watch all debunking and then some watch

209
00:07:49,890 --> 00:07:53,040
all neutral and they watch 20 videos for

210
00:07:53,040 --> 00:07:55,380
four before building up their watch

211
00:07:55,380 --> 00:07:57,450
history and then it was essentially the

212
00:07:57,450 --> 00:08:00,570
same similar selenium bought design it

213
00:08:00,570 --> 00:08:03,360
acted like a like a real user a fire of

214
00:08:03,360 --> 00:08:05,730
the query and our and our platform or

215
00:08:05,730 --> 00:08:07,950
our scripts were collecting what kind of

216
00:08:07,950 --> 00:08:09,570
search results the platform this was

217
00:08:09,570 --> 00:08:10,710
giving and then what kind of

218
00:08:10,710 --> 00:08:13,620
recommendations the the YouTube platform

219
00:08:13,620 --> 00:08:17,340
was giving we also control for noise

220
00:08:17,340 --> 00:08:19,110
this was really important because you

221
00:08:19,110 --> 00:08:20,970
might say that okay the results that

222
00:08:20,970 --> 00:08:22,590
you're seeing is because of certain

223
00:08:22,590 --> 00:08:25,380
noise maybe maybe the the two different

224
00:08:25,380 --> 00:08:27,230
accounts use different browsers or

225
00:08:27,230 --> 00:08:30,200
different kind of times when they were

226
00:08:30,200 --> 00:08:32,640
sending out those queries so your

227
00:08:32,640 --> 00:08:35,010
results are affected by this noise so we

228
00:08:35,010 --> 00:08:36,780
made sure that we control for all these

229
00:08:36,780 --> 00:08:38,880
all these noise variables so all the

230
00:08:38,880 --> 00:08:40,590
accounts were firing queries at the same

231
00:08:40,590 --> 00:08:42,539
time so controlling for the temporal

232
00:08:42,539 --> 00:08:44,550
effects they were all using Firefox this

233
00:08:44,550 --> 00:08:46,620
was again selected on purpose so that we

234
00:08:46,620 --> 00:08:49,050
can avoid the possibility that Chrome

235
00:08:49,050 --> 00:08:50,840
browser would be tracking

236
00:08:50,840 --> 00:08:52,310
these Google accounts that we are using

237
00:08:52,310 --> 00:08:54,529
in our experiment all the searches were

238
00:08:54,529 --> 00:08:56,570
done in incognito mode and they were all

239
00:08:56,570 --> 00:08:59,600
using the same sort of same operating

240
00:08:59,600 --> 00:09:02,480
systems the same infrastructure right so

241
00:09:02,480 --> 00:09:04,940
once we had this entire audit set up it

242
00:09:04,940 --> 00:09:06,890
ran for several days and then we

243
00:09:06,890 --> 00:09:09,350
collected our data and annotated it so

244
00:09:09,350 --> 00:09:11,930
we had overall for this audit collected

245
00:09:11,930 --> 00:09:16,040
56,000 videos 2000 of about 2.3 thousand

246
00:09:16,040 --> 00:09:18,110
of them were unique videos and then we

247
00:09:18,110 --> 00:09:20,540
manually annotated these videos as

248
00:09:20,540 --> 00:09:23,510
either promoting conspiracies are or

249
00:09:23,510 --> 00:09:25,130
they were neutral like just talking

250
00:09:25,130 --> 00:09:27,470
about the you know say 9/11 like

251
00:09:27,470 --> 00:09:28,790
reporting about it but not really

252
00:09:28,790 --> 00:09:30,620
talking about any conspiracy and then

253
00:09:30,620 --> 00:09:32,360
they were debunking some of these

254
00:09:32,360 --> 00:09:33,920
conspiracy theories right so that's how

255
00:09:33,920 --> 00:09:35,529
we annotated and then we did some

256
00:09:35,529 --> 00:09:37,790
statistical comparison tests on this

257
00:09:37,790 --> 00:09:39,890
annotated data right so skipping the

258
00:09:39,890 --> 00:09:42,620
statistical test that we did but you all

259
00:09:42,620 --> 00:09:44,240
might be wondering what's the result of

260
00:09:44,240 --> 00:09:46,220
these these or it's that that's what we

261
00:09:46,220 --> 00:09:48,380
what drove this study in the first place

262
00:09:48,380 --> 00:09:51,260
so let's look at the the first research

263
00:09:51,260 --> 00:09:53,210
question so does demographics age and

264
00:09:53,210 --> 00:09:55,010
gender and us geolocation have any

265
00:09:55,010 --> 00:09:56,300
effect on the amount of misinformation

266
00:09:56,300 --> 00:09:59,870
returned so first good news or bad news

267
00:09:59,870 --> 00:10:02,060
how you already want to take this but

268
00:10:02,060 --> 00:10:04,130
first thing we found that was that for

269
00:10:04,130 --> 00:10:07,220
search audit that's when someone is is a

270
00:10:07,220 --> 00:10:09,680
brand new account brand new Google

271
00:10:09,680 --> 00:10:11,990
account there is no a significant effect

272
00:10:11,990 --> 00:10:14,330
of Democratic Sen geolocation it doesn't

273
00:10:14,330 --> 00:10:16,610
matter what age what gender they belong

274
00:10:16,610 --> 00:10:18,740
where their geolocation is it doesn't

275
00:10:18,740 --> 00:10:20,839
matter they are gonna get same type of

276
00:10:20,839 --> 00:10:22,490
information so there is no significant

277
00:10:22,490 --> 00:10:23,750
difference right so this sort of

278
00:10:23,750 --> 00:10:26,000
disproves some of the initial evidence

279
00:10:26,000 --> 00:10:28,580
that these anecdotal stories had had

280
00:10:28,580 --> 00:10:30,589
said earlier right but things start

281
00:10:30,589 --> 00:10:32,180
getting interesting when we add watch

282
00:10:32,180 --> 00:10:34,910
history to it right so so for the watch

283
00:10:34,910 --> 00:10:37,310
it experiment we found out yes there are

284
00:10:37,310 --> 00:10:39,860
in fact significant effects for certain

285
00:10:39,860 --> 00:10:42,080
combinations of what kind of videos the

286
00:10:42,080 --> 00:10:43,640
accounts are warning watching and then

287
00:10:43,640 --> 00:10:45,380
you know what what type of algorithm

288
00:10:45,380 --> 00:10:47,240
that we are auditing so I'm gonna give

289
00:10:47,240 --> 00:10:48,920
you a quick preview of some of the

290
00:10:48,920 --> 00:10:52,430
results so among all the cases that we

291
00:10:52,430 --> 00:10:55,670
found one overall theme theme was men

292
00:10:55,670 --> 00:10:57,589
were really so accounts which whose

293
00:10:57,589 --> 00:10:59,600
general said as male they were again

294
00:10:59,600 --> 00:11:00,590
recommended

295
00:11:00,590 --> 00:11:03,860
or misinformation videos and then men

296
00:11:03,860 --> 00:11:06,020
who were watching neutral videos they

297
00:11:06,020 --> 00:11:08,930
also ended up with higher recommendation

298
00:11:08,930 --> 00:11:12,380
for probe conspiracy or misinformation

299
00:11:12,380 --> 00:11:15,710
videos so if and this was happening for

300
00:11:15,710 --> 00:11:17,930
especially those two rows that are

301
00:11:17,930 --> 00:11:19,340
marked here the flatter than a vaccine

302
00:11:19,340 --> 00:11:21,530
conspiracy videos and I think this

303
00:11:21,530 --> 00:11:22,910
result is really concerning because

304
00:11:22,910 --> 00:11:25,640
these were users who were drawn to

305
00:11:25,640 --> 00:11:27,470
neutral information that is they were

306
00:11:27,470 --> 00:11:29,540
watching neutral videos and had not yet

307
00:11:29,540 --> 00:11:30,980
developed any sort of strong Crowe

308
00:11:30,980 --> 00:11:33,350
conspiracy belief yet the algorithm the

309
00:11:33,350 --> 00:11:35,000
video you know YouTube's top five

310
00:11:35,000 --> 00:11:36,800
recommendation algorithm I can't really

311
00:11:36,800 --> 00:11:39,650
see the timer eleven minutes left or

312
00:11:39,650 --> 00:11:43,760
eleven minutes okay alright and so right

313
00:11:43,760 --> 00:11:45,740
so which means that it might increase

314
00:11:45,740 --> 00:11:47,810
their these sort of recommendations

315
00:11:47,810 --> 00:11:49,520
might increase their chances of fromming

316
00:11:49,520 --> 00:11:52,520
Pro conspiracy beliefs alright so the

317
00:11:52,520 --> 00:11:54,200
next question was does watch history

318
00:11:54,200 --> 00:11:56,210
watching Pro debunking neutral videos

319
00:11:56,210 --> 00:11:58,760
have any effect perhaps something which

320
00:11:58,760 --> 00:12:00,200
would be most interesting for this

321
00:12:00,200 --> 00:12:01,520
audience is related to health

322
00:12:01,520 --> 00:12:03,650
misinformation so we found that watching

323
00:12:03,650 --> 00:12:06,620
promoting anti vaccine videos actually

324
00:12:06,620 --> 00:12:08,450
resulted in receiving more promoting

325
00:12:08,450 --> 00:12:10,550
anti vaccine videos in the search

326
00:12:10,550 --> 00:12:13,520
results right so compared to those who

327
00:12:13,520 --> 00:12:14,840
watch neutral and debunking and this

328
00:12:14,840 --> 00:12:16,640
aligns with some of those stories that

329
00:12:16,640 --> 00:12:18,800
we have heard before but wait for the

330
00:12:18,800 --> 00:12:20,840
interesting bit in the next part so what

331
00:12:20,840 --> 00:12:23,480
happens for the top five and the up next

332
00:12:23,480 --> 00:12:25,130
recommendation recommended videos right

333
00:12:25,130 --> 00:12:26,270
so what happens for the recommender

334
00:12:26,270 --> 00:12:28,610
algorithm there we found an opposite

335
00:12:28,610 --> 00:12:30,740
effect that is accounts that watched

336
00:12:30,740 --> 00:12:32,780
promoting anti vaccine videos got

337
00:12:32,780 --> 00:12:35,930
recommended more debunking videos right

338
00:12:35,930 --> 00:12:37,580
so it essentially says that YouTube has

339
00:12:37,580 --> 00:12:39,890
gone and changed their they recommender

340
00:12:39,890 --> 00:12:41,480
algorithm but not the search algorithm

341
00:12:41,480 --> 00:12:44,000
and then we looked at another topic like

342
00:12:44,000 --> 00:12:46,790
chemtrails so this this one actually we

343
00:12:46,790 --> 00:12:49,070
found the opposite effect where accounts

344
00:12:49,070 --> 00:12:50,840
that watched videos promoting chemtrails

345
00:12:50,840 --> 00:12:52,790
they got they got recommended more

346
00:12:52,790 --> 00:12:54,740
promoting videos so this tells that

347
00:12:54,740 --> 00:12:56,420
YouTube while they have fixed vaccine

348
00:12:56,420 --> 00:12:58,400
controversies they have not really fixed

349
00:12:58,400 --> 00:13:02,510
chemtrails conspiracy video right so so

350
00:13:02,510 --> 00:13:04,910
it's essentially the third question sort

351
00:13:04,910 --> 00:13:06,320
of I already answered is there any

352
00:13:06,320 --> 00:13:08,870
difference across topics again chemtrail

353
00:13:08,870 --> 00:13:12,050
is a leader in in terms of significantly

354
00:13:12,050 --> 00:13:12,730
more

355
00:13:12,730 --> 00:13:14,320
formative search results compared to all

356
00:13:14,320 --> 00:13:17,529
the other topics same scenario for the

357
00:13:17,529 --> 00:13:20,709
watch experiment as well for 9/11

358
00:13:20,709 --> 00:13:22,360
conspiracy also we found there are

359
00:13:22,360 --> 00:13:24,100
significantly more misinformation in the

360
00:13:24,100 --> 00:13:26,320
up next as well as the top five

361
00:13:26,320 --> 00:13:28,060
recommendations compared to all the

362
00:13:28,060 --> 00:13:28,839
other topics

363
00:13:28,839 --> 00:13:31,899
alright so wrapping up here so what are

364
00:13:31,899 --> 00:13:34,240
the takeaway and contributions of this

365
00:13:34,240 --> 00:13:35,949
work so this through this work we were

366
00:13:35,949 --> 00:13:38,079
able to develop a methodology to audit

367
00:13:38,079 --> 00:13:39,639
search engines for misinformation and

368
00:13:39,639 --> 00:13:41,920
hopefully this method can be adopted for

369
00:13:41,920 --> 00:13:43,839
future audits for other platforms as

370
00:13:43,839 --> 00:13:46,839
well and then we won the first ones to

371
00:13:46,839 --> 00:13:48,910
kind of do the systematic investigation

372
00:13:48,910 --> 00:13:50,980
of the effect of personalization

373
00:13:50,980 --> 00:13:52,810
attributes so by personalization I mean

374
00:13:52,810 --> 00:13:54,339
what's the gender age and the watch

375
00:13:54,339 --> 00:13:56,740
history do they have any real effect on

376
00:13:56,740 --> 00:13:58,959
the way in in the way the algorithm or

377
00:13:58,959 --> 00:14:00,370
the video search platform is is

378
00:14:00,370 --> 00:14:03,730
recommending content and absolutely also

379
00:14:03,730 --> 00:14:05,019
identified these vulnerable populations

380
00:14:05,019 --> 00:14:07,990
for example in our case we found men who

381
00:14:07,990 --> 00:14:09,430
were watching neutral Flattr it's a

382
00:14:09,430 --> 00:14:10,750
videos were recommended more Pro

383
00:14:10,750 --> 00:14:14,230
conspiracy videos so so we kind of this

384
00:14:14,230 --> 00:14:15,820
these audits could also be a mechanism

385
00:14:15,820 --> 00:14:17,709
for finding vulnerable populations who

386
00:14:17,709 --> 00:14:18,760
could be targets for these

387
00:14:18,760 --> 00:14:21,760
misinformation topics and then we were

388
00:14:21,760 --> 00:14:23,740
also statistically able to prove how

389
00:14:23,740 --> 00:14:25,779
YouTube's behavior changes across

390
00:14:25,779 --> 00:14:28,540
different informative topics right so

391
00:14:28,540 --> 00:14:30,430
what are the implications of this

392
00:14:30,430 --> 00:14:32,529
finding right so this this kind of tell

393
00:14:32,529 --> 00:14:34,209
said YouTube's mechanism for handling

394
00:14:34,209 --> 00:14:35,769
this information is more reactive than

395
00:14:35,769 --> 00:14:38,170
proactive so they are reacting and hand

396
00:14:38,170 --> 00:14:40,810
picking topics which have come under the

397
00:14:40,810 --> 00:14:42,850
highlighted by media and popular press

398
00:14:42,850 --> 00:14:44,889
and and their so their strategy is much

399
00:14:44,889 --> 00:14:46,089
more reactive right they are going to

400
00:14:46,089 --> 00:14:48,220
fix vaccine controversies but not really

401
00:14:48,220 --> 00:14:50,110
chemtrails or 9/11 or other

402
00:14:50,110 --> 00:14:51,519
controversies which have not been talked

403
00:14:51,519 --> 00:14:53,500
about by technology critics and media

404
00:14:53,500 --> 00:14:55,510
reports and then even for vaccine

405
00:14:55,510 --> 00:14:57,550
controversy so it's it's more nuanced

406
00:14:57,550 --> 00:14:59,829
than that right so we found that YouTube

407
00:14:59,829 --> 00:15:01,930
while it recommends debunking videos

408
00:15:01,930 --> 00:15:03,190
when someone is searching for anti

409
00:15:03,190 --> 00:15:06,459
vaccine the that the same thing doesn't

410
00:15:06,459 --> 00:15:08,620
happen for its search algorithm there is

411
00:15:08,620 --> 00:15:10,360
a filter bubble which exists in a search

412
00:15:10,360 --> 00:15:12,730
results person who is searching a brand

413
00:15:12,730 --> 00:15:14,290
new account searching for anti vaccine

414
00:15:14,290 --> 00:15:16,300
videos are gonna get search results that

415
00:15:16,300 --> 00:15:19,420
are anti vaccine in the first place so

416
00:15:19,420 --> 00:15:20,980
finally to conclude what we see what I

417
00:15:20,980 --> 00:15:23,230
started with YouTube's claims to reduce

418
00:15:23,230 --> 00:15:24,230
conspiracy theory

419
00:15:24,230 --> 00:15:26,180
recommendations it turns out they still

420
00:15:26,180 --> 00:15:28,610
have a long way to go before they can

421
00:15:28,610 --> 00:15:30,200
claim that they have actually mitigated

422
00:15:30,200 --> 00:15:32,120
the problem of misinformation conspiracy

423
00:15:32,120 --> 00:15:35,180
on their platform that's it thank you

424
00:15:35,180 --> 00:15:38,030
and a daughter 15 minutes and that's

425
00:15:38,030 --> 00:15:39,590
that those are all the questions that I

426
00:15:39,590 --> 00:15:41,240
answered and I'm happy to take any more

427
00:15:41,240 --> 00:15:42,590
questions from the audience if there is

428
00:15:42,590 --> 00:15:43,530
time

429
00:15:43,530 --> 00:15:50,520
[Applause]

430
00:16:19,240 --> 00:16:22,910
so I think there was a paper a couple of

431
00:16:22,910 --> 00:16:25,070
months ago which was also bashed twice

432
00:16:25,070 --> 00:16:27,410
ever like academics that this the paper

433
00:16:27,410 --> 00:16:29,870
doesn't have a good statistical

434
00:16:29,870 --> 00:16:31,250
significance or like it's not done in

435
00:16:31,250 --> 00:16:34,400
the rigorous way so you know so that's

436
00:16:34,400 --> 00:16:36,950
there but I think with this kind of

437
00:16:36,950 --> 00:16:39,260
audit method you could set up an

438
00:16:39,260 --> 00:16:41,420
experiment to test exactly what you're

439
00:16:41,420 --> 00:16:43,940
trying to say with of course with

440
00:16:43,940 --> 00:16:45,470
artificial accounts right so this

441
00:16:45,470 --> 00:16:47,390
doesn't again there's a caveat here

442
00:16:47,390 --> 00:16:49,130
you're not measuring real user behavior

443
00:16:49,130 --> 00:16:51,050
like it's really hard to claim that a

444
00:16:51,050 --> 00:16:53,240
user is only going to search 20 pro

445
00:16:53,240 --> 00:16:54,800
conspiracy videos before they search the

446
00:16:54,800 --> 00:16:58,130
next next video on any topic right but

447
00:16:58,130 --> 00:17:00,590
in order to have at least in a very

448
00:17:00,590 --> 00:17:02,990
conservative world you can make these

449
00:17:02,990 --> 00:17:07,210
measurements by setting these conditions

450
00:17:26,220 --> 00:17:28,840
right so accessing youtubes our

451
00:17:28,840 --> 00:17:31,510
Facebook's algorithm to figure how its

452
00:17:31,510 --> 00:17:34,450
behaving so that's a good question so we

453
00:17:34,450 --> 00:17:36,040
probably have few people in the audience

454
00:17:36,040 --> 00:17:38,050
from the industry so I think in the in

455
00:17:38,050 --> 00:17:39,790
their defense even they don't know how

456
00:17:39,790 --> 00:17:42,160
the the algorithm always works with the

457
00:17:42,160 --> 00:17:44,080
data of the person right the data of the

458
00:17:44,080 --> 00:17:48,130
user so so even if you were to be given

459
00:17:48,130 --> 00:17:49,390
the source code of the algorithm and you

460
00:17:49,390 --> 00:17:50,710
try to go through it you can only answer

461
00:17:50,710 --> 00:17:52,510
so many questions but there is this

462
00:17:52,510 --> 00:17:55,450
factor of users data along with it which

463
00:17:55,450 --> 00:17:57,580
adds this additional complexity which I

464
00:17:57,580 --> 00:17:59,050
think you could only measure through

465
00:17:59,050 --> 00:18:00,670
audits but correct me from the audience

466
00:18:00,670 --> 00:18:02,740
of anybody's from YouTube or Facebook if

467
00:18:02,740 --> 00:18:05,740
I'm not if I miss coding that fact yeah

468
00:18:05,740 --> 00:18:07,809
yeah so he's nodding his head so I'll

469
00:18:07,809 --> 00:18:10,350
take that as a yes

470
00:18:23,710 --> 00:18:26,330
yeah yeah the popularity was one thing

471
00:18:26,330 --> 00:18:27,950
we measured and I don't have the results

472
00:18:27,950 --> 00:18:29,840
I didn't show the results but yes

473
00:18:29,840 --> 00:18:32,500
popularity like number of likes comments

474
00:18:32,500 --> 00:18:35,210
upward all of those did have an effect

475
00:18:35,210 --> 00:18:37,460
yeah but happy to chat more about that

476
00:18:37,460 --> 00:18:37,940
later

477
00:18:37,940 --> 00:18:40,830
yeah thank you

478
00:18:40,830 --> 00:18:44,810
[Applause]

