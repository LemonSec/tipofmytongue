1
00:00:00,300 --> 00:00:07,849
[Music]

2
00:00:07,849 --> 00:00:11,099
welcome John and Chris to the s4 main

3
00:00:11,099 --> 00:00:19,640
stage good afternoon everybody I'm John

4
00:00:19,640 --> 00:00:24,869
Chris and we are here to to talk to you

5
00:00:24,869 --> 00:00:27,570
about a methodology called cyber pH a

6
00:00:27,570 --> 00:00:30,990
that has been used quite successfully to

7
00:00:30,990 --> 00:00:35,190
help companies assess the cyber risk to

8
00:00:35,190 --> 00:00:37,590
their industrial control systems

9
00:00:37,590 --> 00:00:40,260
it's based on some a lot of work from

10
00:00:40,260 --> 00:00:42,750
the from the ISA standards and the NIST

11
00:00:42,750 --> 00:00:45,210
standards but it's unique in many ways

12
00:00:45,210 --> 00:00:47,760
and that it's it's very application

13
00:00:47,760 --> 00:00:49,920
oriented process and it ties very much

14
00:00:49,920 --> 00:00:52,829
into two well-established process safety

15
00:00:52,829 --> 00:00:55,440
processes so very happy to be here it's

16
00:00:55,440 --> 00:00:58,800
actually my first s4 though I don't know

17
00:00:58,800 --> 00:01:00,090
why because I've been in this field for

18
00:01:00,090 --> 00:01:01,980
about nine years and I don't know why I

19
00:01:01,980 --> 00:01:03,719
haven't come to Miami in January before

20
00:01:03,719 --> 00:01:08,400
it definitely will not be my last and so

21
00:01:08,400 --> 00:01:09,810
we're going to talk about this this

22
00:01:09,810 --> 00:01:11,549
process we've already been introduced so

23
00:01:11,549 --> 00:01:12,810
we're going to skip these slides and

24
00:01:12,810 --> 00:01:15,150
what we want to get across here is is

25
00:01:15,150 --> 00:01:17,310
how these risk assessments can be part

26
00:01:17,310 --> 00:01:19,890
of an overall cybersecurity program and

27
00:01:19,890 --> 00:01:22,200
how they can tie into particularly in

28
00:01:22,200 --> 00:01:24,869
industries that that have hazardous

29
00:01:24,869 --> 00:01:26,820
processes how it can tie into

30
00:01:26,820 --> 00:01:28,860
established process safety methodologies

31
00:01:28,860 --> 00:01:32,460
and the impact of of cyber potential

32
00:01:32,460 --> 00:01:34,259
impact of cyber on safety and helping

33
00:01:34,259 --> 00:01:37,020
understand that how it ties into the to

34
00:01:37,020 --> 00:01:38,520
the well-established standards that are

35
00:01:38,520 --> 00:01:40,829
already out there and then we'll go

36
00:01:40,829 --> 00:01:42,899
through I'll quickly go through the

37
00:01:42,899 --> 00:01:45,689
process itself the methodology through a

38
00:01:45,689 --> 00:01:48,270
quick example and then Chris will talk

39
00:01:48,270 --> 00:01:51,390
about lessons learned as as he has

40
00:01:51,390 --> 00:01:53,909
applied this as a significant part of

41
00:01:53,909 --> 00:01:56,460
his program so again very happy to be

42
00:01:56,460 --> 00:01:59,969
here kind of incidentally here by way of

43
00:01:59,969 --> 00:02:03,090
Alaska because that's where Dale and I I

44
00:02:03,090 --> 00:02:06,119
ran into Dale in October in Alaska we

45
00:02:06,119 --> 00:02:08,429
were working at the same client and I

46
00:02:08,429 --> 00:02:11,400
was facilitating one of these cyber PHAs

47
00:02:11,400 --> 00:02:13,620
and endale

48
00:02:13,620 --> 00:02:15,390
said you know you this is something

49
00:02:15,390 --> 00:02:16,950
industry needs to know more about it

50
00:02:16,950 --> 00:02:18,300
please so please come to ask for and

51
00:02:18,300 --> 00:02:21,140
talk about it so that's why we're here

52
00:02:21,140 --> 00:02:26,360
turn it over Chris okay thanks John

53
00:02:26,360 --> 00:02:30,060
all right so as John mentioned with the

54
00:02:30,060 --> 00:02:33,870
our products and chemicals and based on

55
00:02:33,870 --> 00:02:37,080
all the increased risk and changing

56
00:02:37,080 --> 00:02:40,830
landscape for the ICS cyber my companies

57
00:02:40,830 --> 00:02:42,030
decided that they needed to have

58
00:02:42,030 --> 00:02:45,720
somebody spend full time dedicated to

59
00:02:45,720 --> 00:02:48,360
working on on cyber and try to protect

60
00:02:48,360 --> 00:02:52,470
their plants as also mentioned my

61
00:02:52,470 --> 00:02:54,090
company has a very wide footprint I have

62
00:02:54,090 --> 00:02:55,769
over six hundred plants or facilities

63
00:02:55,769 --> 00:02:59,879
worldwide so it's quite a challenge to

64
00:02:59,879 --> 00:03:03,450
try to address all that so as part of

65
00:03:03,450 --> 00:03:06,959
that on when I got assigned this role I

66
00:03:06,959 --> 00:03:09,599
decided to go out and figure out okay

67
00:03:09,599 --> 00:03:12,150
what are the questions that I really

68
00:03:12,150 --> 00:03:14,489
need to answer to try to address the

69
00:03:14,489 --> 00:03:17,400
space and make sure that I we have

70
00:03:17,400 --> 00:03:19,980
success in my company and so as I

71
00:03:19,980 --> 00:03:21,810
canvassed some of my stakeholders are

72
00:03:21,810 --> 00:03:22,829
these are some the questions that they

73
00:03:22,829 --> 00:03:26,850
ask is my plant system ICS system is it

74
00:03:26,850 --> 00:03:30,840
secure from cyberattack as a system been

75
00:03:30,840 --> 00:03:34,290
compromised already from management of

76
00:03:34,290 --> 00:03:36,299
change perspective if I make a change to

77
00:03:36,299 --> 00:03:39,569
my system will that change affect the

78
00:03:39,569 --> 00:03:41,430
security of my system will it make it

79
00:03:41,430 --> 00:03:45,209
less secure also from a firewall

80
00:03:45,209 --> 00:03:47,040
perspective as soon as you start talking

81
00:03:47,040 --> 00:03:48,510
so everybody says well just take a

82
00:03:48,510 --> 00:03:50,790
firewall on it but you know what type of

83
00:03:50,790 --> 00:03:52,730
firewall how many times how many

84
00:03:52,730 --> 00:03:54,450
firewalls do I need for a particular

85
00:03:54,450 --> 00:03:57,389
process trying to understand that you

86
00:03:57,389 --> 00:03:59,880
know do I need to have you know dpi do I

87
00:03:59,880 --> 00:04:03,329
need to have IDs what do we really need

88
00:04:03,329 --> 00:04:06,000
from a firewall perspective in what

89
00:04:06,000 --> 00:04:07,380
rules do I need to have configured in

90
00:04:07,380 --> 00:04:10,500
these firewalls amazingly enough we find

91
00:04:10,500 --> 00:04:12,359
a lot of firewalls or configured as

92
00:04:12,359 --> 00:04:15,090
permit Indiana so having a firewall

93
00:04:15,090 --> 00:04:17,430
there is just not good enough the last

94
00:04:17,430 --> 00:04:20,190
two questions they'll really get at why

95
00:04:20,190 --> 00:04:23,610
we need a PHA and why the PHA really

96
00:04:23,610 --> 00:04:25,140
adds a lot of value from a cyber

97
00:04:25,140 --> 00:04:26,460
perspective

98
00:04:26,460 --> 00:04:29,610
and that is how do you know when we've

99
00:04:29,610 --> 00:04:32,370
spent enough how good is good enough I

100
00:04:32,370 --> 00:04:33,960
don't want to spend any more than I

101
00:04:33,960 --> 00:04:37,039
really have to to secure my plants and

102
00:04:37,039 --> 00:04:41,789
without a a PHA that answer is not quite

103
00:04:41,789 --> 00:04:48,600
forthcoming so cyber is really all about

104
00:04:48,600 --> 00:04:50,310
risk management is that's something that

105
00:04:50,310 --> 00:04:52,650
a lightbulb went off in my head you know

106
00:04:52,650 --> 00:04:54,990
once I really got into and understood

107
00:04:54,990 --> 00:05:00,449
what what cyber is all about so in order

108
00:05:00,449 --> 00:05:03,030
to address cyber you know one must

109
00:05:03,030 --> 00:05:04,229
really understand

110
00:05:04,229 --> 00:05:07,680
um risk because again Simon's about risk

111
00:05:07,680 --> 00:05:09,840
management so the first things you need

112
00:05:09,840 --> 00:05:11,160
to do when you're trying to assess your

113
00:05:11,160 --> 00:05:13,620
risk is understand what it is you're

114
00:05:13,620 --> 00:05:18,360
trying to protect okay so first thing to

115
00:05:18,360 --> 00:05:19,910
do is to go out there and really

116
00:05:19,910 --> 00:05:22,349
identify what your critical assets are

117
00:05:22,349 --> 00:05:23,759
you know what is it you're trying to

118
00:05:23,759 --> 00:05:26,400
protect inventory those assets then and

119
00:05:26,400 --> 00:05:28,889
then maybe you might want to classify

120
00:05:28,889 --> 00:05:32,250
these assets in terms of possibly you

121
00:05:32,250 --> 00:05:33,630
know what what's the consequence of one

122
00:05:33,630 --> 00:05:35,729
failing then you really need to

123
00:05:35,729 --> 00:05:38,400
understand what are the realistic

124
00:05:38,400 --> 00:05:41,430
threats your whole design will be based

125
00:05:41,430 --> 00:05:43,229
on what you consider to be a realistic

126
00:05:43,229 --> 00:05:45,180
spread and also you might want to

127
00:05:45,180 --> 00:05:48,510
understand also what the probability our

128
00:05:48,510 --> 00:05:50,010
likelihood of that threat being realized

129
00:05:50,010 --> 00:05:53,120
is and what the consequences may be

130
00:05:53,120 --> 00:05:55,820
identifying existing vulnerabilities and

131
00:05:55,820 --> 00:05:59,090
the consequence of a compromise is is

132
00:05:59,090 --> 00:06:02,610
really important um are we talking about

133
00:06:02,610 --> 00:06:05,220
just lots of visibility are we talking

134
00:06:05,220 --> 00:06:07,050
about lots of control of our of our

135
00:06:07,050 --> 00:06:10,020
systems and our assets are we talking

136
00:06:10,020 --> 00:06:11,780
about potentially equipment damage

137
00:06:11,780 --> 00:06:17,240
product interruption or even injury or

138
00:06:17,240 --> 00:06:20,099
possibly fatalities so it's really

139
00:06:20,099 --> 00:06:21,630
important to understand what the

140
00:06:21,630 --> 00:06:25,440
consequences are compromises once you've

141
00:06:25,440 --> 00:06:27,870
done it really need to assess are your

142
00:06:27,870 --> 00:06:31,020
current safeguards appropriate for the

143
00:06:31,020 --> 00:06:34,800
risk that you have once that is

144
00:06:34,800 --> 00:06:37,020
understood it then leads you to

145
00:06:37,020 --> 00:06:39,689
understand what your residual risk is

146
00:06:39,689 --> 00:06:42,159
so after this risk assessment and you

147
00:06:42,159 --> 00:06:43,810
understand what your gaps are what your

148
00:06:43,810 --> 00:06:46,780
residual risk in your process is then

149
00:06:46,780 --> 00:06:50,219
you can begin to understand what is a

150
00:06:50,219 --> 00:06:52,689
comprehensive plan start develop that

151
00:06:52,689 --> 00:06:54,789
comprehensive plan to address cyber risk

152
00:06:54,789 --> 00:06:57,900
at your companies and that could include

153
00:06:57,900 --> 00:06:59,680
in addition to the existing

154
00:06:59,680 --> 00:07:01,599
countermeasures what additional measures

155
00:07:01,599 --> 00:07:05,379
are required from that that could also

156
00:07:05,379 --> 00:07:07,569
that's not just the technology that may

157
00:07:07,569 --> 00:07:09,729
also include modifications to your

158
00:07:09,729 --> 00:07:12,039
existing work processes your policies

159
00:07:12,039 --> 00:07:15,340
and procedures and then the final two

160
00:07:15,340 --> 00:07:17,860
points are and this is will really help

161
00:07:17,860 --> 00:07:19,360
you when you start to put together your

162
00:07:19,360 --> 00:07:21,310
plan because we all have limited

163
00:07:21,310 --> 00:07:24,520
resources and you really want to expend

164
00:07:24,520 --> 00:07:26,139
your resources where it's really going

165
00:07:26,139 --> 00:07:29,560
to make the most difference so coming up

166
00:07:29,560 --> 00:07:32,500
a PHA you might have you know five

167
00:07:32,500 --> 00:07:35,050
hundred recommendations I can't do all

168
00:07:35,050 --> 00:07:36,430
of them so you got to figure out which

169
00:07:36,430 --> 00:07:38,349
where is going to be the most important

170
00:07:38,349 --> 00:07:40,000
one so prioritizing those

171
00:07:40,000 --> 00:07:42,279
recommendations and then evaluating them

172
00:07:42,279 --> 00:07:44,830
for cost versus complexities it's a

173
00:07:44,830 --> 00:07:49,750
really important step and final four I

174
00:07:49,750 --> 00:07:52,289
turn over to John just want to mention

175
00:07:52,289 --> 00:07:54,909
her risk assessments are really

176
00:07:54,909 --> 00:07:57,159
important but really there are only one

177
00:07:57,159 --> 00:07:59,430
component of a comprehensive

178
00:07:59,430 --> 00:08:03,580
cybersecurity planning program and so

179
00:08:03,580 --> 00:08:06,009
again you know John's gonna talk more

180
00:08:06,009 --> 00:08:08,349
about that but recognize that if you

181
00:08:08,349 --> 00:08:09,610
don't have a comprehensive program

182
00:08:09,610 --> 00:08:11,199
you're probably not going to have a

183
00:08:11,199 --> 00:08:19,000
whole lot of success so what I want to

184
00:08:19,000 --> 00:08:21,039
talk about now is the actual to cyber

185
00:08:21,039 --> 00:08:22,960
PHA process but in order to do that I

186
00:08:22,960 --> 00:08:24,099
want to take a little bit of a step back

187
00:08:24,099 --> 00:08:26,169
and talk about a little bit about the

188
00:08:26,169 --> 00:08:28,389
origins of the process and how it ties

189
00:08:28,389 --> 00:08:31,060
in with existing standards and best

190
00:08:31,060 --> 00:08:33,159
practices so everybody here I'm sure is

191
00:08:33,159 --> 00:08:34,089
very familiar with the NIST

192
00:08:34,089 --> 00:08:37,958
cybersecurity framework and and the the

193
00:08:37,958 --> 00:08:39,969
identify function which has under that a

194
00:08:39,969 --> 00:08:42,219
sub category a category of risk

195
00:08:42,219 --> 00:08:44,800
assessment but I think if you ask ten

196
00:08:44,800 --> 00:08:47,680
people what a risk assessment is

197
00:08:47,680 --> 00:08:49,360
particularly in a for a control system

198
00:08:49,360 --> 00:08:51,089
you probably get 10 different answers

199
00:08:51,089 --> 00:08:53,410
because there's not a lot of stay

200
00:08:53,410 --> 00:08:56,059
in the area of how to perform these

201
00:08:56,059 --> 00:08:58,699
assessments what the NIST framework does

202
00:08:58,699 --> 00:09:01,160
as any good framework should is it

203
00:09:01,160 --> 00:09:03,399
establishes some baselines some base

204
00:09:03,399 --> 00:09:05,809
requirements or recommendations and

205
00:09:05,809 --> 00:09:09,139
those tie in this slide directly then to

206
00:09:09,139 --> 00:09:11,449
or a reference different existing

207
00:09:11,449 --> 00:09:13,339
industry standards the ones that I have

208
00:09:13,339 --> 00:09:16,309
highlighted are the is a 99 or 64 for

209
00:09:16,309 --> 00:09:18,619
three references because that's relevant

210
00:09:18,619 --> 00:09:22,249
to ICS there are other ones as I'm sure

211
00:09:22,249 --> 00:09:24,139
you're aware that the standard that the

212
00:09:24,139 --> 00:09:26,449
framework references that are generally

213
00:09:26,449 --> 00:09:28,879
more IT centric risk assessment

214
00:09:28,879 --> 00:09:31,009
methodologies so there's a lot of

215
00:09:31,009 --> 00:09:33,050
different ways to perform a risk

216
00:09:33,050 --> 00:09:36,709
assessment so I'm going to focus of

217
00:09:36,709 --> 00:09:39,379
course on this cyber PHA methodology and

218
00:09:39,379 --> 00:09:42,319
how it ties particularly ties to the is

219
00:09:42,319 --> 00:09:44,839
a six to four for three series of

220
00:09:44,839 --> 00:09:47,240
standards so if you look at those

221
00:09:47,240 --> 00:09:49,819
references from from NIST more directly

222
00:09:49,819 --> 00:09:52,910
it ties to the two - these requirements

223
00:09:52,910 --> 00:09:55,160
in the two - one standard which

224
00:09:55,160 --> 00:09:57,429
basically says pick a risk assessment

225
00:09:57,429 --> 00:10:00,740
methodology conduct a high-level risk

226
00:10:00,740 --> 00:10:02,749
assessment identify your control systems

227
00:10:02,749 --> 00:10:04,339
make sure you understand them have

228
00:10:04,339 --> 00:10:07,639
network diagrams prioritize them then

229
00:10:07,639 --> 00:10:09,439
perform detailed vulnerability and risk

230
00:10:09,439 --> 00:10:11,480
assessments and then figure out how

231
00:10:11,480 --> 00:10:12,799
often you're going to do those and

232
00:10:12,799 --> 00:10:15,290
document the results of course but it

233
00:10:15,290 --> 00:10:17,509
does not talk about is how to perform

234
00:10:17,509 --> 00:10:20,959
the risk assessment that's still yet to

235
00:10:20,959 --> 00:10:22,429
come but I'll talk about that in a

236
00:10:22,429 --> 00:10:26,720
moment where is a is going with the with

237
00:10:26,720 --> 00:10:30,709
in terms of helping users with the

238
00:10:30,709 --> 00:10:34,369
actual methodology but PHA the term that

239
00:10:34,369 --> 00:10:35,899
Chris and I have thrown out a couple

240
00:10:35,899 --> 00:10:38,540
times now it stands for process hazard

241
00:10:38,540 --> 00:10:40,369
analysis and the reason this this

242
00:10:40,369 --> 00:10:42,230
process or this methodology has been

243
00:10:42,230 --> 00:10:46,910
given that name is because PHAs are very

244
00:10:46,910 --> 00:10:49,939
well established risk management

245
00:10:49,939 --> 00:10:52,490
methodology in the continuous process

246
00:10:52,490 --> 00:10:54,170
industries particularly in oil and gas

247
00:10:54,170 --> 00:10:56,240
and chemical and other hazardous

248
00:10:56,240 --> 00:10:59,709
industries with hazardous processes so

249
00:10:59,709 --> 00:11:02,179
this is a cybersecurity conference so

250
00:11:02,179 --> 00:11:03,649
just make sure everybody is aware of

251
00:11:03,649 --> 00:11:05,149
process safety it's basically the

252
00:11:05,149 --> 00:11:06,380
prevention

253
00:11:06,380 --> 00:11:08,840
of bad things happening in industrial

254
00:11:08,840 --> 00:11:10,870
facilities like fires explosions

255
00:11:10,870 --> 00:11:13,910
releases of chemicals or gases and so on

256
00:11:13,910 --> 00:11:16,310
they could impact particularly they

257
00:11:16,310 --> 00:11:18,680
could impact health and safety the most

258
00:11:18,680 --> 00:11:20,810
process safety programs also extend to

259
00:11:20,810 --> 00:11:23,000
looking at financial and other impacts

260
00:11:23,000 --> 00:11:26,180
to the organization and one of the key

261
00:11:26,180 --> 00:11:28,280
premises of process safety is this

262
00:11:28,280 --> 00:11:30,860
concept of layers of protection that you

263
00:11:30,860 --> 00:11:33,800
want to have multiple more the higher

264
00:11:33,800 --> 00:11:36,440
the risk or the higher the consequences

265
00:11:36,440 --> 00:11:38,180
the more layers of protection you need

266
00:11:38,180 --> 00:11:42,080
to prevent an incident or if there is an

267
00:11:42,080 --> 00:11:44,690
incident to contain it to mitigate it so

268
00:11:44,690 --> 00:11:46,790
this is a fairly common diagram that you

269
00:11:46,790 --> 00:11:49,070
see in discussions about process safety

270
00:11:49,070 --> 00:11:51,110
explaining this idea of layers of

271
00:11:51,110 --> 00:11:53,600
protection and in modern systems we have

272
00:11:53,600 --> 00:11:56,260
control systems and safety systems and

273
00:11:56,260 --> 00:11:59,530
other layers of protection to prevent

274
00:11:59,530 --> 00:12:03,920
incidents and then the PHA process it's

275
00:12:03,920 --> 00:12:06,290
been around for decades it's a very

276
00:12:06,290 --> 00:12:08,390
organized systematic approach to

277
00:12:08,390 --> 00:12:11,650
identifying these process hazards and

278
00:12:11,650 --> 00:12:14,300
understanding the the risk associated

279
00:12:14,300 --> 00:12:16,610
with them how likely are they to occur

280
00:12:16,610 --> 00:12:19,820
how bad would they be if they happen the

281
00:12:19,820 --> 00:12:22,190
basic the basic formula for risk which

282
00:12:22,190 --> 00:12:25,190
is likelihood times consequence been

283
00:12:25,190 --> 00:12:27,470
used for decades and the focus is on the

284
00:12:27,470 --> 00:12:29,750
equipment implementation the utilities

285
00:12:29,750 --> 00:12:32,590
human actions other external factors and

286
00:12:32,590 --> 00:12:35,330
for many industries is this is regulated

287
00:12:35,330 --> 00:12:41,150
if if you fall under OSHA PSM or a EPA's

288
00:12:41,150 --> 00:12:43,790
RMP program or some of the offshore and

289
00:12:43,790 --> 00:12:47,630
pipeline sectors are required to do some

290
00:12:47,630 --> 00:12:49,760
level of process hazard analysis or

291
00:12:49,760 --> 00:12:51,440
process safety management or process

292
00:12:51,440 --> 00:12:53,180
safety or pipeline safety management

293
00:12:53,180 --> 00:12:55,280
program so it's been around for a long

294
00:12:55,280 --> 00:12:59,570
time well accepted by up by operations

295
00:12:59,570 --> 00:13:02,900
groups as part of an ongoing risk

296
00:13:02,900 --> 00:13:05,090
management and safety program there's

297
00:13:05,090 --> 00:13:06,650
two basic types of studies that are done

298
00:13:06,650 --> 00:13:09,620
has ops and Lopez as UPS identify the

299
00:13:09,620 --> 00:13:13,940
hazards and it is the quantitative

300
00:13:13,940 --> 00:13:16,490
methodology layer protection analysis is

301
00:13:16,490 --> 00:13:19,910
more quantitative but both of them are

302
00:13:19,910 --> 00:13:21,770
answering the questions what are the

303
00:13:21,770 --> 00:13:23,600
potential hazards and are there

304
00:13:23,600 --> 00:13:26,930
sufficient layers of protection - - to

305
00:13:26,930 --> 00:13:30,650
manage that risk - tolerable levels very

306
00:13:30,650 --> 00:13:32,330
similar for those of us in cybersecurity

307
00:13:32,330 --> 00:13:34,040
when we talk about defense and depth

308
00:13:34,040 --> 00:13:36,650
layers of protection is the very same

309
00:13:36,650 --> 00:13:42,530
similar concept another another key

310
00:13:42,530 --> 00:13:45,080
factor in any Process Safety Program is

311
00:13:45,080 --> 00:13:46,520
usually something called a risk matrix

312
00:13:46,520 --> 00:13:49,550
which is the organization's definition

313
00:13:49,550 --> 00:13:51,980
for what is tolerable risk and it talks

314
00:13:51,980 --> 00:13:53,810
about things like health safety

315
00:13:53,810 --> 00:13:56,150
environmental company image for example

316
00:13:56,150 --> 00:13:59,600
and on one scale the consequences and

317
00:13:59,600 --> 00:14:01,130
then the likelihood on the other scale

318
00:14:01,130 --> 00:14:03,350
and if you'll notice this is an example

319
00:14:03,350 --> 00:14:05,210
but this typical of what you find in a

320
00:14:05,210 --> 00:14:07,220
risk matrix is that there are orders of

321
00:14:07,220 --> 00:14:09,410
magnitude so this is not a precise

322
00:14:09,410 --> 00:14:12,020
science what we are trying to understand

323
00:14:12,020 --> 00:14:14,450
here is relative risk and whether that

324
00:14:14,450 --> 00:14:16,460
risk is within the company's tolerable

325
00:14:16,460 --> 00:14:18,530
guidelines or not in other words what

326
00:14:18,530 --> 00:14:20,780
things must accompany respond to

327
00:14:20,780 --> 00:14:23,630
so a typical hazop might look like this

328
00:14:23,630 --> 00:14:26,960
they typically as up worksheet might

329
00:14:26,960 --> 00:14:29,120
look like this and when we do a has up

330
00:14:29,120 --> 00:14:31,670
we break the process down into units and

331
00:14:31,670 --> 00:14:34,820
nodes and we walk through with

332
00:14:34,820 --> 00:14:36,620
operations and engineering and other

333
00:14:36,620 --> 00:14:38,180
folks to talk about what are the

334
00:14:38,180 --> 00:14:40,220
potential hazards what could go wrong

335
00:14:40,220 --> 00:14:42,410
what are the deviations how could that

336
00:14:42,410 --> 00:14:44,900
happen what are the causes most

337
00:14:44,900 --> 00:14:46,070
importantly what are the consequences

338
00:14:46,070 --> 00:14:48,020
and how bad could they be and they get

339
00:14:48,020 --> 00:14:51,110
scored according to that risk matrix and

340
00:14:51,110 --> 00:14:53,150
that gives us our our initial risk and

341
00:14:53,150 --> 00:14:54,590
then we look at its okay so what

342
00:14:54,590 --> 00:14:56,210
safeguards do we have in place to

343
00:14:56,210 --> 00:14:57,620
prevent that what layers of protection

344
00:14:57,620 --> 00:15:00,140
do we have to prevent that and so what

345
00:15:00,140 --> 00:15:02,660
is our what is our our mitigated risk

346
00:15:02,660 --> 00:15:06,020
and then oftentimes these are followed

347
00:15:06,020 --> 00:15:08,240
up particularly for for particularly

348
00:15:08,240 --> 00:15:11,720
hazardous threat scenarios we we do a

349
00:15:11,720 --> 00:15:13,850
deeper dive in safety called the lopa

350
00:15:13,850 --> 00:15:15,950
layer protection analysis to look more

351
00:15:15,950 --> 00:15:17,990
deeply at art what are those layers of

352
00:15:17,990 --> 00:15:19,790
protection are they truly independent

353
00:15:19,790 --> 00:15:24,830
and how good are they unfortunately one

354
00:15:24,830 --> 00:15:26,780
of the drawbacks this is a very mature

355
00:15:26,780 --> 00:15:29,210
established process but one of the

356
00:15:29,210 --> 00:15:31,400
drawbacks is it does not take into

357
00:15:31,400 --> 00:15:32,810
account it was never designed to take

358
00:15:32,810 --> 00:15:33,680
into account

359
00:15:33,680 --> 00:15:38,480
deliberate actions and and so cyber

360
00:15:38,480 --> 00:15:41,930
events for example or even really take

361
00:15:41,930 --> 00:15:43,939
into account the likelihood or the

362
00:15:43,939 --> 00:15:45,470
probability of a control system not

363
00:15:45,470 --> 00:15:47,720
doing what it's supposed to do it kind

364
00:15:47,720 --> 00:15:48,920
of assumed that it's going to perform

365
00:15:48,920 --> 00:15:50,449
its function that the alarm is going to

366
00:15:50,449 --> 00:15:52,579
go off that the safety system is going

367
00:15:52,579 --> 00:15:56,449
to take action so one of the challenges

368
00:15:56,449 --> 00:15:58,670
we face particularly now with modern

369
00:15:58,670 --> 00:16:01,069
control systems is that they're

370
00:16:01,069 --> 00:16:04,220
intelligent programmable and and they

371
00:16:04,220 --> 00:16:06,290
are integrated and it's possible that a

372
00:16:06,290 --> 00:16:08,689
single vulnerability could disable

373
00:16:08,689 --> 00:16:11,389
multiple layers of protection so we have

374
00:16:11,389 --> 00:16:13,009
to study that because there's a lot of

375
00:16:13,009 --> 00:16:15,589
other factors involved that could and

376
00:16:15,589 --> 00:16:17,480
other safeguards in place that can

377
00:16:17,480 --> 00:16:20,360
protect the system non-programmable non

378
00:16:20,360 --> 00:16:24,350
cyber so these studies unfortunately it

379
00:16:24,350 --> 00:16:26,089
has up in lopa studies don't take that

380
00:16:26,089 --> 00:16:27,949
into account but if we extend the

381
00:16:27,949 --> 00:16:29,660
process through what we call the cyber

382
00:16:29,660 --> 00:16:33,009
PHA we can take into account the cyber

383
00:16:33,009 --> 00:16:36,800
risk and and see if that then is is

384
00:16:36,800 --> 00:16:39,410
tolerable or and address things that are

385
00:16:39,410 --> 00:16:42,290
not so to help visualize that going back

386
00:16:42,290 --> 00:16:44,029
to my earlier diagram we've got the

387
00:16:44,029 --> 00:16:45,499
basic control system and we got the

388
00:16:45,499 --> 00:16:48,829
safety system but in reality today those

389
00:16:48,829 --> 00:16:51,339
are oftentimes networked together and

390
00:16:51,339 --> 00:16:53,990
common HMIS common engineering

391
00:16:53,990 --> 00:16:57,259
workstations common networks so it's

392
00:16:57,259 --> 00:17:00,889
possible that a single threat actor

393
00:17:00,889 --> 00:17:03,860
could potentially attack both the

394
00:17:03,860 --> 00:17:06,140
control and the safety system and either

395
00:17:06,140 --> 00:17:09,490
initiating an event or prevent a safety

396
00:17:09,490 --> 00:17:12,619
action from from from bringing the

397
00:17:12,619 --> 00:17:15,709
process to a safe state the safety

398
00:17:15,709 --> 00:17:17,799
standards have also recognized this

399
00:17:17,799 --> 00:17:20,059
there's there's a lot of standards and

400
00:17:20,059 --> 00:17:23,119
safeties ice I see six and five eleven

401
00:17:23,119 --> 00:17:24,859
is the predominant one for functional

402
00:17:24,859 --> 00:17:26,689
safety and the latest version of that

403
00:17:26,689 --> 00:17:29,000
does recognize now that a security risk

404
00:17:29,000 --> 00:17:30,350
assessment needs to be carried out

405
00:17:30,350 --> 00:17:32,450
because safety assessments are limited

406
00:17:32,450 --> 00:17:34,610
to looking at usually at random failures

407
00:17:34,610 --> 00:17:36,860
or systematic failures but not cyber and

408
00:17:36,860 --> 00:17:39,890
not deliberate so the design of the

409
00:17:39,890 --> 00:17:42,279
safety systems now need to provide

410
00:17:42,279 --> 00:17:45,020
resilience against cyber threats as well

411
00:17:45,020 --> 00:17:47,570
and then it points to guidance from two

412
00:17:47,570 --> 00:17:50,360
different sources tra t for a tech

413
00:17:50,360 --> 00:17:52,429
report from the SAT for committee and 6

414
00:17:52,429 --> 00:17:57,559
2 4 4 3 3 2 so 3-2 is a is a standard

415
00:17:57,559 --> 00:17:59,299
under development but pretty close to

416
00:17:59,299 --> 00:18:02,630
being is going out for 4 ballot it talks

417
00:18:02,630 --> 00:18:05,750
about the process of how to finally

418
00:18:05,750 --> 00:18:07,460
answering the question how does one

419
00:18:07,460 --> 00:18:10,190
perform a an industrial control system

420
00:18:10,190 --> 00:18:12,860
cybersecurity risk assessment and it

421
00:18:12,860 --> 00:18:15,950
presents presents a framework for doing

422
00:18:15,950 --> 00:18:16,429
that

423
00:18:16,429 --> 00:18:18,350
it tries not to be too prescriptive but

424
00:18:18,350 --> 00:18:20,779
provides a framework for how one might

425
00:18:20,779 --> 00:18:25,370
perform such an assessment so I want to

426
00:18:25,370 --> 00:18:27,019
quickly walk through an example for you

427
00:18:27,019 --> 00:18:28,850
so you can see rather than go through

428
00:18:28,850 --> 00:18:31,159
that flowchart the basic idea is very

429
00:18:31,159 --> 00:18:33,889
much like a PHA and PHA we start out

430
00:18:33,889 --> 00:18:37,159
with pn ids as our drawings and we break

431
00:18:37,159 --> 00:18:40,820
the process into units and nodes and we

432
00:18:40,820 --> 00:18:42,230
walk through systematically

433
00:18:42,230 --> 00:18:44,240
we do the same thing in a cyber peach

434
00:18:44,240 --> 00:18:46,519
except our dot our drawings are the

435
00:18:46,519 --> 00:18:48,620
network architecture drawings so we

436
00:18:48,620 --> 00:18:50,750
start with a system architecture of the

437
00:18:50,750 --> 00:18:53,149
control system all the way from level 0

438
00:18:53,149 --> 00:18:57,610
up to level 4 and we break it down into

439
00:18:57,610 --> 00:19:02,899
units sites units and zones and nodes

440
00:19:02,899 --> 00:19:05,600
and we can walk through the system very

441
00:19:05,600 --> 00:19:07,789
systematically and look at what are the

442
00:19:07,789 --> 00:19:12,440
different threats so this this worksheet

443
00:19:12,440 --> 00:19:16,190
is a pH cyber pH a worksheet you not

444
00:19:16,190 --> 00:19:17,659
surprisingly it looks an awful lot like

445
00:19:17,659 --> 00:19:19,490
a has up and it's based on that

446
00:19:19,490 --> 00:19:21,409
methodology that's been as I said very

447
00:19:21,409 --> 00:19:24,580
well accepted in operations as a way to

448
00:19:24,580 --> 00:19:26,629
systematically walk through the system

449
00:19:26,629 --> 00:19:30,529
but in this case our zones our control

450
00:19:30,529 --> 00:19:32,899
system zones are our nodes our control

451
00:19:32,899 --> 00:19:36,529
system components and our deviations are

452
00:19:36,529 --> 00:19:39,649
our threats are different instead of for

453
00:19:39,649 --> 00:19:42,200
example a valve malfunctioning our

454
00:19:42,200 --> 00:19:45,710
deviations are a control system fails to

455
00:19:45,710 --> 00:19:48,230
perform it's it's expected function for

456
00:19:48,230 --> 00:19:51,190
example so I give a simple example here

457
00:19:51,190 --> 00:19:54,049
but the idea is we walk through and we

458
00:19:54,049 --> 00:19:56,750
identify that what could go wrong which

459
00:19:56,750 --> 00:20:00,200
are the the the deviation what are the

460
00:20:00,200 --> 00:20:01,340
causes

461
00:20:01,340 --> 00:20:03,860
these are the threat actions what are

462
00:20:03,860 --> 00:20:07,180
the consequences which need to be

463
00:20:07,180 --> 00:20:09,590
used the same language as you would use

464
00:20:09,590 --> 00:20:11,030
in any other risk assessment what's

465
00:20:11,030 --> 00:20:13,160
what's the impact to the organization if

466
00:20:13,160 --> 00:20:16,250
this happens and we quantify that using

467
00:20:16,250 --> 00:20:19,730
the same risk matrix so that we're so

468
00:20:19,730 --> 00:20:21,650
we're comparing apples to apples in

469
00:20:21,650 --> 00:20:23,720
terms of risk we're not trying to use a

470
00:20:23,720 --> 00:20:25,850
different scale for cyber risk risk is

471
00:20:25,850 --> 00:20:28,610
risk particularly to management it's all

472
00:20:28,610 --> 00:20:30,530
about health safety financial and other

473
00:20:30,530 --> 00:20:32,810
impacts to the organization and then we

474
00:20:32,810 --> 00:20:35,660
identify what's the initial risk and

475
00:20:35,660 --> 00:20:37,580
very much like it has up we then look at

476
00:20:37,580 --> 00:20:39,200
okay what countermeasures do we already

477
00:20:39,200 --> 00:20:41,210
have in place this would be our existing

478
00:20:41,210 --> 00:20:43,370
layers of protection or existing defense

479
00:20:43,370 --> 00:20:45,440
and depth and how good are they are they

480
00:20:45,440 --> 00:20:48,290
good enough against different types of

481
00:20:48,290 --> 00:20:49,760
threats and different types of attacks

482
00:20:49,760 --> 00:20:53,900
and then we identify and what is the

483
00:20:53,900 --> 00:20:55,850
residual risk taking into account the

484
00:20:55,850 --> 00:20:58,160
existing safeguards what's the residual

485
00:20:58,160 --> 00:21:00,410
risk and what can we do about it

486
00:21:00,410 --> 00:21:02,420
as I said it's it's it's a very

487
00:21:02,420 --> 00:21:05,360
systematic approach it really encourages

488
00:21:05,360 --> 00:21:07,190
the team to come together across

489
00:21:07,190 --> 00:21:09,140
disciplinary team to come together and

490
00:21:09,140 --> 00:21:11,150
think about all of these different

491
00:21:11,150 --> 00:21:14,180
aspects actually a lot of a lot of cross

492
00:21:14,180 --> 00:21:15,800
learning takes place during these

493
00:21:15,800 --> 00:21:18,860
because because it encourages discussion

494
00:21:18,860 --> 00:21:21,740
about these different threat scenarios

495
00:21:21,740 --> 00:21:24,590
and encourages discussions about how

496
00:21:24,590 --> 00:21:27,140
realistic they are and you know if well

497
00:21:27,140 --> 00:21:29,150
facilitated you can make sure that they

498
00:21:29,150 --> 00:21:33,170
don't go off on down rabbit holes but

499
00:21:33,170 --> 00:21:35,630
stay focused on the task of really

500
00:21:35,630 --> 00:21:39,380
understanding the process and the cyber

501
00:21:39,380 --> 00:21:41,090
risk and being able to answer those

502
00:21:41,090 --> 00:21:43,940
questions to management about why are we

503
00:21:43,940 --> 00:21:46,190
recommending this and why aren't we

504
00:21:46,190 --> 00:21:48,950
recommending this because the the

505
00:21:48,950 --> 00:21:54,320
without an approach like this you're you

506
00:21:54,320 --> 00:21:56,120
have a couple choices you can just apply

507
00:21:56,120 --> 00:21:58,010
all of the best standards and practices

508
00:21:58,010 --> 00:21:59,420
and all the different technology that's

509
00:21:59,420 --> 00:22:01,430
out there but nobody can afford that and

510
00:22:01,430 --> 00:22:04,100
it would likely be overkill and actually

511
00:22:04,100 --> 00:22:05,960
could interrupt caused more problems so

512
00:22:05,960 --> 00:22:08,630
that the idea of this process is to go

513
00:22:08,630 --> 00:22:10,580
through and really understand where

514
00:22:10,580 --> 00:22:13,070
which vulnerabilities can lead to the

515
00:22:13,070 --> 00:22:14,460
highest risks

516
00:22:14,460 --> 00:22:16,890
and then you can focus on shoring up

517
00:22:16,890 --> 00:22:18,360
those vulnerabilities or closing those

518
00:22:18,360 --> 00:22:21,960
holes so I said it uses the same risk

519
00:22:21,960 --> 00:22:25,100
matrix and then one of the outcomes

520
00:22:25,100 --> 00:22:27,180
there's several outcomes one of the

521
00:22:27,180 --> 00:22:30,300
outcomes usually would be a properly

522
00:22:30,300 --> 00:22:34,290
zoned system with the appropriate

523
00:22:34,290 --> 00:22:37,110
barriers of protection at the different

524
00:22:37,110 --> 00:22:39,750
zone boundaries but that not to say that

525
00:22:39,750 --> 00:22:41,310
that's the only mitigation but that is

526
00:22:41,310 --> 00:22:44,520
one of one of many outcomes and turn it

527
00:22:44,520 --> 00:22:46,590
back to Chris to to kind of wrap up the

528
00:22:46,590 --> 00:22:53,940
session here thanks John as John said

529
00:22:53,940 --> 00:22:57,990
this process really works on it is it's

530
00:22:57,990 --> 00:23:00,300
a pretty intensive process but the

531
00:23:00,300 --> 00:23:02,400
results coming out of there are are

532
00:23:02,400 --> 00:23:04,650
normally fantastic and they give you a

533
00:23:04,650 --> 00:23:07,620
real basis for really on doing your

534
00:23:07,620 --> 00:23:11,160
system design actually after the fact I

535
00:23:11,160 --> 00:23:13,740
have implemented a system this process

536
00:23:13,740 --> 00:23:17,940
on numbers of my my plants and one

537
00:23:17,940 --> 00:23:20,490
things I found long after the fact is

538
00:23:20,490 --> 00:23:24,030
that as you know sometimes your project

539
00:23:24,030 --> 00:23:26,640
manager or your project engineer will

540
00:23:26,640 --> 00:23:28,440
come back and challenge you know why am

541
00:23:28,440 --> 00:23:32,190
i spending all this money on these

542
00:23:32,190 --> 00:23:36,120
particular technology and this is

543
00:23:36,120 --> 00:23:38,130
actually a documented basis that we go

544
00:23:38,130 --> 00:23:39,780
back to us and here's why we're doing it

545
00:23:39,780 --> 00:23:42,660
and if we don't do this this is a risk

546
00:23:42,660 --> 00:23:44,880
that will be left behind so it's

547
00:23:44,880 --> 00:23:47,870
actually gives you a very good basis for

548
00:23:47,870 --> 00:23:51,210
defending why you're spending what you

549
00:23:51,210 --> 00:23:53,670
have to spend on on cyber I just want to

550
00:23:53,670 --> 00:23:57,300
leave you with a couple of examples or

551
00:23:57,300 --> 00:24:01,280
considerations for a cyber security

552
00:24:01,280 --> 00:24:03,600
comprehensive cybersecurity program for

553
00:24:03,600 --> 00:24:07,710
for ICS systems that I've had the

554
00:24:07,710 --> 00:24:11,120
ability to go through first thing is

555
00:24:11,120 --> 00:24:14,250
assembling a core team you need to

556
00:24:14,250 --> 00:24:17,340
understand within your organization who

557
00:24:17,340 --> 00:24:19,440
owns security and I'm not just talking

558
00:24:19,440 --> 00:24:21,360
about at the plant we're talking about

559
00:24:21,360 --> 00:24:23,190
all the systems that are connected to

560
00:24:23,190 --> 00:24:25,620
your plants and all of it needs to be

561
00:24:25,620 --> 00:24:28,260
protected so find out who all your

562
00:24:28,260 --> 00:24:29,670
stakeholders are assembled them together

563
00:24:29,670 --> 00:24:34,020
and also don't don't go it alone on you

564
00:24:34,020 --> 00:24:35,220
know well as many of your organization's

565
00:24:35,220 --> 00:24:38,700
feel like mine a lot of smart people

566
00:24:38,700 --> 00:24:40,680
when everybody has ideas on how to how

567
00:24:40,680 --> 00:24:43,500
to do things um but it's probably best

568
00:24:43,500 --> 00:24:46,410
to go with somebody who has actual

569
00:24:46,410 --> 00:24:48,210
experience has gone through this process

570
00:24:48,210 --> 00:24:51,360
numbers of times so find a experienced

571
00:24:51,360 --> 00:24:56,040
partner who has who can not only walk

572
00:24:56,040 --> 00:24:57,480
you through the process will also train

573
00:24:57,480 --> 00:25:01,200
you so that your team can can then work

574
00:25:01,200 --> 00:25:03,830
on someone independently going forward

575
00:25:03,830 --> 00:25:06,930
you want to also you know develop a

576
00:25:06,930 --> 00:25:10,320
common mission vision for for that team

577
00:25:10,320 --> 00:25:12,960
so that you know if your program is

578
00:25:12,960 --> 00:25:14,940
going to last multiple years which it

579
00:25:14,940 --> 00:25:17,220
should and to keep the team focused

580
00:25:17,220 --> 00:25:21,210
around what are the important methods by

581
00:25:21,210 --> 00:25:24,480
which it's going to function I you want

582
00:25:24,480 --> 00:25:28,050
to start with you know as built a system

583
00:25:28,050 --> 00:25:29,700
you need to understand what it is that

584
00:25:29,700 --> 00:25:32,190
you have as I mentioned before that will

585
00:25:32,190 --> 00:25:34,200
help you and consider a phased approach

586
00:25:34,200 --> 00:25:37,230
for me because I had so many systems out

587
00:25:37,230 --> 00:25:40,230
there I had to figure out how to focus

588
00:25:40,230 --> 00:25:42,410
on the most important first and also

589
00:25:42,410 --> 00:25:45,450
since this sour PHA is a pretty

590
00:25:45,450 --> 00:25:48,330
intensive program process you might want

591
00:25:48,330 --> 00:25:50,540
to consider doing a high-level

592
00:25:50,540 --> 00:25:52,410
vulnerability assessment before you

593
00:25:52,410 --> 00:25:57,390
actually get deeply into this PHA use

594
00:25:57,390 --> 00:25:58,740
this opportunity as a training

595
00:25:58,740 --> 00:26:01,830
opportunity for your teams a lot of

596
00:26:01,830 --> 00:26:05,130
times you'll find that your IT teams

597
00:26:05,130 --> 00:26:06,840
some of them have never been to a plant

598
00:26:06,840 --> 00:26:11,190
and it's a good opportunity to to get

599
00:26:11,190 --> 00:26:12,660
everybody on the same page taught the

600
00:26:12,660 --> 00:26:15,060
same language and understand why we're

601
00:26:15,060 --> 00:26:18,270
doing what you're doing on document your

602
00:26:18,270 --> 00:26:20,550
deliverables and then also remember that

603
00:26:20,550 --> 00:26:21,660
you know you can put a whole lot of

604
00:26:21,660 --> 00:26:23,070
technology in a plant and you can create

605
00:26:23,070 --> 00:26:25,230
all these systems but you might be back

606
00:26:25,230 --> 00:26:27,270
doing it again five years or ten years

607
00:26:27,270 --> 00:26:29,700
from now because the environment has

608
00:26:29,700 --> 00:26:31,830
changed so think about how you're going

609
00:26:31,830 --> 00:26:33,480
to maintain and sustain these systems as

610
00:26:33,480 --> 00:26:39,560
you go forward I'm here a number of

611
00:26:39,560 --> 00:26:42,060
deliverables deliverables like that

612
00:26:42,060 --> 00:26:45,870
come out of this process won't spend any

613
00:26:45,870 --> 00:26:48,810
time on them but just like to you know

614
00:26:48,810 --> 00:26:52,080
point out that this is a you know a

615
00:26:52,080 --> 00:26:55,470
well-documented process it's you can go

616
00:26:55,470 --> 00:26:58,350
methodically through it and you may

617
00:26:58,350 --> 00:26:59,940
consider these deliverables when you

618
00:26:59,940 --> 00:27:01,880
start to look at trying to put together

619
00:27:01,880 --> 00:27:06,120
a request code to work with a consultant

620
00:27:06,120 --> 00:27:12,330
or my partner with you and finally in

621
00:27:12,330 --> 00:27:16,230
conclusion from risk assessments you

622
00:27:16,230 --> 00:27:18,300
know there's a lot of good information

623
00:27:18,300 --> 00:27:21,030
you can get out of it as we we discussed

624
00:27:21,030 --> 00:27:24,120
today you can determine you know what

625
00:27:24,120 --> 00:27:25,710
plants needs to be addressed first so

626
00:27:25,710 --> 00:27:27,810
you have your priorities all all decided

627
00:27:27,810 --> 00:27:30,960
um you can intelligently design and

628
00:27:30,960 --> 00:27:33,630
apply countermeasures to to your systems

629
00:27:33,630 --> 00:27:37,020
on to reduce and manage the risks that

630
00:27:37,020 --> 00:27:41,340
are are remaining it also allows you to

631
00:27:41,340 --> 00:27:43,980
prioritize your activities and your

632
00:27:43,980 --> 00:27:47,280
resources the most important ones first

633
00:27:47,280 --> 00:27:51,030
and then you will have an understanding

634
00:27:51,030 --> 00:27:53,130
of which countermeasures are going to

635
00:27:53,130 --> 00:27:55,560
give you the proverbial most bang for

636
00:27:55,560 --> 00:27:59,280
the buck and understand which ones will

637
00:27:59,280 --> 00:28:02,910
be the the easiest to implement and what

638
00:28:02,910 --> 00:28:05,340
catch the the so-called low-hanging

639
00:28:05,340 --> 00:28:09,960
fruit and then finally it's establishes

640
00:28:09,960 --> 00:28:13,580
a firm basis for you to make your

641
00:28:13,580 --> 00:28:15,900
decisions regards your your target

642
00:28:15,900 --> 00:28:20,670
architecture on we and my company we we

643
00:28:20,670 --> 00:28:22,740
determine what our target target

644
00:28:22,740 --> 00:28:25,020
architecture is based on a lot of this

645
00:28:25,020 --> 00:28:28,590
analysis and then we will go back and we

646
00:28:28,590 --> 00:28:30,750
will revisit this every couple of years

647
00:28:30,750 --> 00:28:35,070
to make sure that these systems and the

648
00:28:35,070 --> 00:28:37,320
architecture is still valid given the

649
00:28:37,320 --> 00:28:40,230
changing conditions that are out there

650
00:28:40,230 --> 00:28:44,910
that as we know the cyber world is not

651
00:28:44,910 --> 00:28:46,950
one that then stay the same it's always

652
00:28:46,950 --> 00:28:49,020
dynamic and you have to take into

653
00:28:49,020 --> 00:28:52,680
consideration the fact that is changing

654
00:28:52,680 --> 00:28:55,350
and update and keep your your defenses

655
00:28:55,350 --> 00:28:55,830
updated

656
00:28:55,830 --> 00:28:58,530
to protect you against it so that's all

657
00:28:58,530 --> 00:29:00,360
I have thank you very much for listening

658
00:29:00,360 --> 00:29:04,760
and I give

