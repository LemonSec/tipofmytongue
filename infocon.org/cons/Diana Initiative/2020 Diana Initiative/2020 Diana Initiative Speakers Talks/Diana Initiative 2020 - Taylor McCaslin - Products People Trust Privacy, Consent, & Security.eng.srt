1
00:00:13,200 --> 00:00:13,759
hello

2
00:00:13,759 --> 00:00:17,359
everybody uh welcome back to stage three

3
00:00:17,359 --> 00:00:19,920
of the diana initiative

4
00:00:19,920 --> 00:00:22,960
my name is josh i'm your mc for this uh

5
00:00:22,960 --> 00:00:23,760
talk

6
00:00:23,760 --> 00:00:26,160
and uh today we're going to be hearing

7
00:00:26,160 --> 00:00:27,279
mclarson

8
00:00:27,279 --> 00:00:29,920
welcome before we do that i have an

9
00:00:29,920 --> 00:00:31,679
announcement to make really quick

10
00:00:31,679 --> 00:00:34,480
um our raffle winner for track three is

11
00:00:34,480 --> 00:00:35,680
a sydney brazil

12
00:00:35,680 --> 00:00:39,200
woohoo of applause you win

13
00:00:39,200 --> 00:00:42,160
a new car i actually have no idea what

14
00:00:42,160 --> 00:00:44,239
the what the prize is for the raffle so

15
00:00:44,239 --> 00:00:45,520
you'll have to check in with nicole for

16
00:00:45,520 --> 00:00:48,399
that um but uh yeah congratulations to

17
00:00:48,399 --> 00:00:49,520
sydney

18
00:00:49,520 --> 00:00:52,879
um and uh taylor who is with us today

19
00:00:52,879 --> 00:00:55,760
is a multi-disciplinary investor product

20
00:00:55,760 --> 00:00:57,199
manager and technologist

21
00:00:57,199 --> 00:01:00,160
living in austin texas since 2011 he's

22
00:01:00,160 --> 00:01:01,359
worked at large scale

23
00:01:01,359 --> 00:01:03,680
hyper growth technology companies he

24
00:01:03,680 --> 00:01:05,438
specializes in software as a service

25
00:01:05,438 --> 00:01:06,159
products

26
00:01:06,159 --> 00:01:07,920
and has experience with data science

27
00:01:07,920 --> 00:01:09,520
machine learning mobile apps

28
00:01:09,520 --> 00:01:11,920
cyber security web hosting and web apps

29
00:01:11,920 --> 00:01:12,960
so like everything

30
00:01:12,960 --> 00:01:15,600
basically experience with everything

31
00:01:15,600 --> 00:01:16,080
when

32
00:01:16,080 --> 00:01:17,680
not advising companies or speaking at

33
00:01:17,680 --> 00:01:19,280
conferences about technology

34
00:01:19,280 --> 00:01:20,799
taylor can be found geeking out with the

35
00:01:20,799 --> 00:01:22,880
latest apple gadget skiing

36
00:01:22,880 --> 00:01:24,799
or enjoying the expansive austin art

37
00:01:24,799 --> 00:01:27,200
scene he also enjoys volunteering with

38
00:01:27,200 --> 00:01:29,200
local human rights and lgbtq

39
00:01:29,200 --> 00:01:31,200
organizations around central texas

40
00:01:31,200 --> 00:01:32,960
as well as mentoring young technologists

41
00:01:32,960 --> 00:01:35,439
looking to start careers in tech

42
00:01:35,439 --> 00:01:37,439
and yeah please give a warm welcome to

43
00:01:37,439 --> 00:01:39,360
taylor

44
00:01:39,360 --> 00:01:42,320
um and uh i will note uh for this talk

45
00:01:42,320 --> 00:01:43,200
we will be doing

46
00:01:43,200 --> 00:01:45,280
uh question and answer um please throw

47
00:01:45,280 --> 00:01:46,799
your questions in the chat

48
00:01:46,799 --> 00:01:48,560
i will collect them and then we will do

49
00:01:48,560 --> 00:01:50,000
q a at the end

50
00:01:50,000 --> 00:01:51,759
um and uh yeah please take it away

51
00:01:51,759 --> 00:01:54,320
taylor awesome hi everyone so my name is

52
00:01:54,320 --> 00:01:55,360
taylor mccaslin

53
00:01:55,360 --> 00:01:57,200
um you see my twitter handle there

54
00:01:57,200 --> 00:01:59,119
digital sass feel free to tweet

55
00:01:59,119 --> 00:02:01,759
questions um or feedback you've got for

56
00:02:01,759 --> 00:02:02,960
me at this presentation

57
00:02:02,960 --> 00:02:05,119
also at the end there'll be a link to

58
00:02:05,119 --> 00:02:06,880
the slides as well

59
00:02:06,880 --> 00:02:08,878
so this is one of my favorite topics to

60
00:02:08,878 --> 00:02:10,399
talk about and it's

61
00:02:10,399 --> 00:02:13,840
about how we build products that people

62
00:02:13,840 --> 00:02:14,640
trust

63
00:02:14,640 --> 00:02:17,680
when we think about technology products

64
00:02:17,680 --> 00:02:19,680
they have the opportunity to really

65
00:02:19,680 --> 00:02:21,280
allow us to do

66
00:02:21,280 --> 00:02:23,360
new things that we were never able to do

67
00:02:23,360 --> 00:02:24,480
before but

68
00:02:24,480 --> 00:02:26,319
unfortunately a lot of technology

69
00:02:26,319 --> 00:02:28,720
companies aren't taking that privilege

70
00:02:28,720 --> 00:02:30,640
and responsibility to heart

71
00:02:30,640 --> 00:02:32,959
and aren't building products that we can

72
00:02:32,959 --> 00:02:33,680
trust

73
00:02:33,680 --> 00:02:35,760
so this talk is intended to help you

74
00:02:35,760 --> 00:02:37,360
understand how to design

75
00:02:37,360 --> 00:02:40,480
privacy consent and security into your

76
00:02:40,480 --> 00:02:41,200
products

77
00:02:41,200 --> 00:02:44,720
so that people love them and use them

78
00:02:44,720 --> 00:02:46,640
so a little bit about me before we get

79
00:02:46,640 --> 00:02:48,720
started um

80
00:02:48,720 --> 00:02:51,040
i am a senior product manager working in

81
00:02:51,040 --> 00:02:53,599
secure at get lab i use he him his

82
00:02:53,599 --> 00:02:54,640
pronouns

83
00:02:54,640 --> 00:02:56,400
um you'll see my twitter handle

84
00:02:56,400 --> 00:02:58,560
throughout this i've spent most of my

85
00:02:58,560 --> 00:02:59,920
career focused on

86
00:02:59,920 --> 00:03:02,720
ux at hyper growth enterprise scale

87
00:03:02,720 --> 00:03:04,159
technology companies

88
00:03:04,159 --> 00:03:06,480
so that really is the lens through which

89
00:03:06,480 --> 00:03:07,599
i think about

90
00:03:07,599 --> 00:03:10,159
products i want them to be easy to use

91
00:03:10,159 --> 00:03:12,560
and i want them to be delightful to use

92
00:03:12,560 --> 00:03:14,319
um part of that's probably from my

93
00:03:14,319 --> 00:03:16,159
strange background i actually have a ba

94
00:03:16,159 --> 00:03:17,680
in theater and dance

95
00:03:17,680 --> 00:03:20,319
so i look at the technology world from a

96
00:03:20,319 --> 00:03:22,879
very very different perspective

97
00:03:22,879 --> 00:03:26,400
i also run a small angel investment

98
00:03:26,400 --> 00:03:29,280
fund really focusing on companies who

99
00:03:29,280 --> 00:03:29,760
get

100
00:03:29,760 --> 00:03:32,319
the specifics of what we're going to

101
00:03:32,319 --> 00:03:33,200
talk about today

102
00:03:33,200 --> 00:03:36,319
right in their products i also

103
00:03:36,319 --> 00:03:37,680
have worked at a number of companies

104
00:03:37,680 --> 00:03:39,120
that i'm sure you're probably familiar

105
00:03:39,120 --> 00:03:41,680
with there at the bottom

106
00:03:41,680 --> 00:03:44,560
so today i want to talk about what is

107
00:03:44,560 --> 00:03:45,440
trust

108
00:03:45,440 --> 00:03:47,680
the types of trust that you can build i

109
00:03:47,680 --> 00:03:49,280
want to look at

110
00:03:49,280 --> 00:03:51,440
why we should care about trust in

111
00:03:51,440 --> 00:03:52,400
general

112
00:03:52,400 --> 00:03:55,040
um we'll look at a couple of examples

113
00:03:55,040 --> 00:03:56,080
that i like to call

114
00:03:56,080 --> 00:03:58,400
trust fails um so seeing companies who

115
00:03:58,400 --> 00:04:00,319
have failed at building trust with their

116
00:04:00,319 --> 00:04:02,560
customers and then talk about the

117
00:04:02,560 --> 00:04:04,799
specifics about what it means to build

118
00:04:04,799 --> 00:04:08,000
trust into your products so to start

119
00:04:08,000 --> 00:04:10,640
let's talk a little bit about the word

120
00:04:10,640 --> 00:04:13,519
trust it's something that it's a very

121
00:04:13,519 --> 00:04:14,000
simple

122
00:04:14,000 --> 00:04:17,120
word but it has very complex meanings

123
00:04:17,120 --> 00:04:19,440
and it's really hard to describe if you

124
00:04:19,440 --> 00:04:21,358
think about trust

125
00:04:21,358 --> 00:04:24,479
it's really it's just something you you

126
00:04:24,479 --> 00:04:26,160
know it when you feel it

127
00:04:26,160 --> 00:04:27,440
and the way that i like to talk about

128
00:04:27,440 --> 00:04:30,400
this is every single day if you get in a

129
00:04:30,400 --> 00:04:32,160
car and you drive on a road

130
00:04:32,160 --> 00:04:35,040
you're trusting that the other drivers

131
00:04:35,040 --> 00:04:35,600
are gonna

132
00:04:35,600 --> 00:04:38,320
obey the traffic laws and understand

133
00:04:38,320 --> 00:04:38,720
that

134
00:04:38,720 --> 00:04:40,639
you don't cross the double yellow line

135
00:04:40,639 --> 00:04:42,400
in the middle of the road that's you

136
00:04:42,400 --> 00:04:44,479
going out on a limb and trusting other

137
00:04:44,479 --> 00:04:46,400
people to follow rules

138
00:04:46,400 --> 00:04:48,639
that's kind of what this is about it's

139
00:04:48,639 --> 00:04:49,759
about how

140
00:04:49,759 --> 00:04:53,199
people actually think about and present

141
00:04:53,199 --> 00:04:54,080
themselves

142
00:04:54,080 --> 00:04:57,040
to the world and how we think about that

143
00:04:57,040 --> 00:04:58,800
trust

144
00:04:58,800 --> 00:05:02,560
so what is trust if it's hard to define

145
00:05:02,560 --> 00:05:04,400
but you can feel it what are the

146
00:05:04,400 --> 00:05:06,320
qualities that make up something that

147
00:05:06,320 --> 00:05:08,800
you you think is trustworthy

148
00:05:08,800 --> 00:05:10,720
so i want to talk about this through the

149
00:05:10,720 --> 00:05:13,199
lens of two women who've done a lot of

150
00:05:13,199 --> 00:05:13,600
work

151
00:05:13,600 --> 00:05:16,639
in this field uh francis fay and rachel

152
00:05:16,639 --> 00:05:17,520
botsman

153
00:05:17,520 --> 00:05:20,000
um both of them have a lot of work on

154
00:05:20,000 --> 00:05:20,720
trust i

155
00:05:20,720 --> 00:05:22,720
suggest you go and google them and look

156
00:05:22,720 --> 00:05:24,000
up all of their work

157
00:05:24,000 --> 00:05:26,880
francis faye is uh from the harvard

158
00:05:26,880 --> 00:05:27,919
business school

159
00:05:27,919 --> 00:05:29,759
she's formally worked at uber as their

160
00:05:29,759 --> 00:05:32,000
svp of leadership and strategy doing

161
00:05:32,000 --> 00:05:35,520
a lot to fix the trust problems at uber

162
00:05:35,520 --> 00:05:37,840
um rachel botsman is from the university

163
00:05:37,840 --> 00:05:39,759
of oxford she's also the author of

164
00:05:39,759 --> 00:05:42,160
a wonderful book that i highly recommend

165
00:05:42,160 --> 00:05:43,360
who can you trust

166
00:05:43,360 --> 00:05:45,840
um i'll talk about some of their

167
00:05:45,840 --> 00:05:47,919
perspectives here in just a moment

168
00:05:47,919 --> 00:05:49,759
because i think they each have a unique

169
00:05:49,759 --> 00:05:51,039
take on what trust

170
00:05:51,039 --> 00:05:53,039
means and i think it gives us a

171
00:05:53,039 --> 00:05:54,479
framework to look at

172
00:05:54,479 --> 00:05:56,560
how we build trust into our technology

173
00:05:56,560 --> 00:05:58,880
products so let's start with francis

174
00:05:58,880 --> 00:05:59,520
faye

175
00:05:59,520 --> 00:06:02,160
she really approaches the topic of trust

176
00:06:02,160 --> 00:06:03,840
in the the sense of

177
00:06:03,840 --> 00:06:06,400
psychological trust trust between one

178
00:06:06,400 --> 00:06:08,880
person and another person

179
00:06:08,880 --> 00:06:11,520
and she thinks of trust as a triangle if

180
00:06:11,520 --> 00:06:12,240
you think about

181
00:06:12,240 --> 00:06:15,199
a stool with three legs that's very much

182
00:06:15,199 --> 00:06:16,800
how francis thinks about

183
00:06:16,800 --> 00:06:19,680
trust and her point in a lot of her

184
00:06:19,680 --> 00:06:20,319
talks

185
00:06:20,319 --> 00:06:23,440
is that if you don't have one of the

186
00:06:23,440 --> 00:06:25,759
legs of this triangle of trust

187
00:06:25,759 --> 00:06:27,840
the whole thing falls down and those

188
00:06:27,840 --> 00:06:30,479
three sides are authenticity

189
00:06:30,479 --> 00:06:34,400
logic and empathy and if you're missing

190
00:06:34,400 --> 00:06:35,840
any one of these

191
00:06:35,840 --> 00:06:38,160
you can't really establish trust or it

192
00:06:38,160 --> 00:06:40,639
doesn't feel completely authentic

193
00:06:40,639 --> 00:06:42,319
so let's talk about what each of these

194
00:06:42,319 --> 00:06:43,840
three items mean

195
00:06:43,840 --> 00:06:47,600
so her posit basically is that you're

196
00:06:47,600 --> 00:06:50,479
more likely to trust me if the following

197
00:06:50,479 --> 00:06:51,599
are true

198
00:06:51,599 --> 00:06:54,800
you feel like i'm being authentic to you

199
00:06:54,800 --> 00:06:56,479
that i'm not you know putting up a

200
00:06:56,479 --> 00:06:58,479
facade or saying something that i don't

201
00:06:58,479 --> 00:06:59,840
believe is true

202
00:06:59,840 --> 00:07:02,319
um you you want to feel like your

203
00:07:02,319 --> 00:07:04,080
products are authentic

204
00:07:04,080 --> 00:07:06,479
second you want to sense that i have a

205
00:07:06,479 --> 00:07:07,599
real rigor

206
00:07:07,599 --> 00:07:09,520
in my logic and what i mean by that

207
00:07:09,520 --> 00:07:11,280
basically is that

208
00:07:11,280 --> 00:07:13,199
you see how i have come to the

209
00:07:13,199 --> 00:07:15,360
conclusions that i have

210
00:07:15,360 --> 00:07:16,880
it's not something that i just pull out

211
00:07:16,880 --> 00:07:19,199
of a hat like magic um you want to see

212
00:07:19,199 --> 00:07:20,560
that there's some

213
00:07:20,560 --> 00:07:24,160
methodology to my logic and then thirdly

214
00:07:24,160 --> 00:07:26,720
believe that my empathy is directed at

215
00:07:26,720 --> 00:07:27,280
you

216
00:07:27,280 --> 00:07:29,759
and this is one of the critical pieces

217
00:07:29,759 --> 00:07:30,880
that

218
00:07:30,880 --> 00:07:32,800
when you talk to someone who's very

219
00:07:32,800 --> 00:07:34,800
empathetic it's really easy to trust

220
00:07:34,800 --> 00:07:35,680
those people

221
00:07:35,680 --> 00:07:37,440
because you just feel like they care

222
00:07:37,440 --> 00:07:39,919
about you and this is a quality that's

223
00:07:39,919 --> 00:07:42,080
really hard to translate into digital

224
00:07:42,080 --> 00:07:42,960
technologies

225
00:07:42,960 --> 00:07:44,879
but i think it's one that is really

226
00:07:44,879 --> 00:07:47,199
worth looking at

227
00:07:47,199 --> 00:07:49,520
so if those three things are true you're

228
00:07:49,520 --> 00:07:51,440
likely to have trust now i do want to

229
00:07:51,440 --> 00:07:51,759
talk

230
00:07:51,759 --> 00:07:54,960
one quick bit about rigor in logic

231
00:07:54,960 --> 00:07:56,879
there's a little trick here that a lot

232
00:07:56,879 --> 00:07:58,400
of people don't realize

233
00:07:58,400 --> 00:08:00,240
um there are sort of two different ways

234
00:08:00,240 --> 00:08:01,759
that you can communicate

235
00:08:01,759 --> 00:08:05,120
logic and ideas one of them is uh i'm

236
00:08:05,120 --> 00:08:06,639
gonna take you on a journey and i'm

237
00:08:06,639 --> 00:08:08,080
gonna tell you a story

238
00:08:08,080 --> 00:08:10,240
and it's all gonna culminate into an

239
00:08:10,240 --> 00:08:12,240
idea in fact that's a lot of what i'm

240
00:08:12,240 --> 00:08:14,319
doing in this presentation today

241
00:08:14,319 --> 00:08:16,080
the second approach is that you start

242
00:08:16,080 --> 00:08:18,639
with the idea up front and then you talk

243
00:08:18,639 --> 00:08:20,080
people through the journey of how you

244
00:08:20,080 --> 00:08:22,240
got there both of these are

245
00:08:22,240 --> 00:08:24,720
totally reasonable ways to explain how

246
00:08:24,720 --> 00:08:26,400
you get to a conclusion

247
00:08:26,400 --> 00:08:28,400
and show the rigor in your logic but

248
00:08:28,400 --> 00:08:30,160
there's one trick to this

249
00:08:30,160 --> 00:08:33,200
that unfortunately attention isn't

250
00:08:33,200 --> 00:08:35,599
unlimited and so if you go with the

251
00:08:35,599 --> 00:08:36,958
first

252
00:08:36,958 --> 00:08:39,039
point where you sort of meander around

253
00:08:39,039 --> 00:08:40,000
telling a story

254
00:08:40,000 --> 00:08:43,200
before you get to your idea you'll lose

255
00:08:43,200 --> 00:08:45,120
people's attention and your idea won't

256
00:08:45,120 --> 00:08:46,800
actually come across this is

257
00:08:46,800 --> 00:08:49,360
really important in technology where if

258
00:08:49,360 --> 00:08:51,440
you don't help someone accomplish a task

259
00:08:51,440 --> 00:08:53,519
immediately or help them understand what

260
00:08:53,519 --> 00:08:55,279
you're trying to do

261
00:08:55,279 --> 00:08:56,880
they will leave your product they won't

262
00:08:56,880 --> 00:08:58,880
use it they will think it doesn't work

263
00:08:58,880 --> 00:09:01,120
so i think it's a really important thing

264
00:09:01,120 --> 00:09:02,399
to think through when you think about

265
00:09:02,399 --> 00:09:04,000
your technology products

266
00:09:04,000 --> 00:09:06,160
so moving on to rachel boxman she talks

267
00:09:06,160 --> 00:09:08,480
about trust more in the sense of

268
00:09:08,480 --> 00:09:11,680
communities so humans to humans rather

269
00:09:11,680 --> 00:09:14,480
than a singular person to another person

270
00:09:14,480 --> 00:09:16,880
and in rachel's book she talks about how

271
00:09:16,880 --> 00:09:17,839
trust is a

272
00:09:17,839 --> 00:09:20,720
confident relationship with the unknown

273
00:09:20,720 --> 00:09:22,399
and i think that's a really interesting

274
00:09:22,399 --> 00:09:24,080
way to think about trust

275
00:09:24,080 --> 00:09:27,120
because it is very much about how do i

276
00:09:27,120 --> 00:09:28,399
trust something that i

277
00:09:28,399 --> 00:09:31,120
don't understand or i don't fully know

278
00:09:31,120 --> 00:09:32,640
and when you think about technology

279
00:09:32,640 --> 00:09:33,600
products

280
00:09:33,600 --> 00:09:35,200
we don't know what's happening in the

281
00:09:35,200 --> 00:09:37,040
background we don't know where our data

282
00:09:37,040 --> 00:09:37,760
is going

283
00:09:37,760 --> 00:09:39,920
how it's being accessed what's happening

284
00:09:39,920 --> 00:09:42,080
with it so it is really

285
00:09:42,080 --> 00:09:43,680
building trust with something that is

286
00:09:43,680 --> 00:09:46,160
unknown and so the way that rachel talks

287
00:09:46,160 --> 00:09:46,880
about this

288
00:09:46,880 --> 00:09:48,880
is basically through the concept of

289
00:09:48,880 --> 00:09:51,040
trust leaps basically how you

290
00:09:51,040 --> 00:09:54,399
go from known to unknown in waters of

291
00:09:54,399 --> 00:09:55,360
uncertainty

292
00:09:55,360 --> 00:09:57,680
and that really is what trust is about

293
00:09:57,680 --> 00:09:58,720
it's that

294
00:09:58,720 --> 00:10:00,959
what has to be true for you to feel

295
00:10:00,959 --> 00:10:02,240
comfortable with something

296
00:10:02,240 --> 00:10:04,640
unknown when you have uncertainty about

297
00:10:04,640 --> 00:10:05,760
it

298
00:10:05,760 --> 00:10:08,640
so when we look into this um i like to

299
00:10:08,640 --> 00:10:10,720
use this reference of

300
00:10:10,720 --> 00:10:13,440
you've probably heard like in 1998 back

301
00:10:13,440 --> 00:10:15,680
in the day before the internet got huge

302
00:10:15,680 --> 00:10:17,680
people your parents would tell you don't

303
00:10:17,680 --> 00:10:19,519
get in the car with strangers

304
00:10:19,519 --> 00:10:22,079
and then fast forward to the internet

305
00:10:22,079 --> 00:10:23,440
our parents would say

306
00:10:23,440 --> 00:10:25,120
don't meet people from the internet

307
00:10:25,120 --> 00:10:26,640
alone and then

308
00:10:26,640 --> 00:10:28,079
when you start looking at some of the

309
00:10:28,079 --> 00:10:30,160
technology that we've created like uber

310
00:10:30,160 --> 00:10:31,680
and lyft we now

311
00:10:31,680 --> 00:10:34,240
summon cars from the internet and get in

312
00:10:34,240 --> 00:10:36,079
them with strangers and let them take us

313
00:10:36,079 --> 00:10:37,120
places

314
00:10:37,120 --> 00:10:39,760
so it's interesting to look at how trust

315
00:10:39,760 --> 00:10:40,800
has evolved

316
00:10:40,800 --> 00:10:43,279
and how technology has allowed us to

317
00:10:43,279 --> 00:10:45,680
make these trust leaps

318
00:10:45,680 --> 00:10:47,680
and unlock really interesting

319
00:10:47,680 --> 00:10:50,079
capabilities

320
00:10:50,079 --> 00:10:52,000
so the way that rachel talks about this

321
00:10:52,000 --> 00:10:54,160
is what she calls the trust stack

322
00:10:54,160 --> 00:10:56,160
and there's basically three premises to

323
00:10:56,160 --> 00:10:57,279
this one

324
00:10:57,279 --> 00:11:00,399
is to trust the idea you want to believe

325
00:11:00,399 --> 00:11:01,839
in whatever it is you're trying to

326
00:11:01,839 --> 00:11:03,760
accomplish in the system of

327
00:11:03,760 --> 00:11:07,519
lyft um or airbnb for example i'm

328
00:11:07,519 --> 00:11:09,680
trusting that i need a ride somewhere

329
00:11:09,680 --> 00:11:11,760
and that someone is willing to take

330
00:11:11,760 --> 00:11:14,640
take me there in their car with airbnb i

331
00:11:14,640 --> 00:11:15,200
want a

332
00:11:15,200 --> 00:11:18,079
cool place to stay and on the other side

333
00:11:18,079 --> 00:11:18,720
someone has

334
00:11:18,720 --> 00:11:20,399
a cool place and they want to offer it

335
00:11:20,399 --> 00:11:22,720
to individuals

336
00:11:22,720 --> 00:11:25,279
those are two pretty wild concepts

337
00:11:25,279 --> 00:11:26,079
without

338
00:11:26,079 --> 00:11:28,800
trust in them um so that sort of steps

339
00:11:28,800 --> 00:11:30,720
us up to the next pieces trusting in the

340
00:11:30,720 --> 00:11:31,680
platform

341
00:11:31,680 --> 00:11:33,200
i'm not just going to go get in a car

342
00:11:33,200 --> 00:11:34,880
with a stranger off the street

343
00:11:34,880 --> 00:11:37,200
however lyft and uber have created a

344
00:11:37,200 --> 00:11:38,720
platform where

345
00:11:38,720 --> 00:11:41,200
we have profiles and we have ratings and

346
00:11:41,200 --> 00:11:42,880
you've got the confidence that there's

347
00:11:42,880 --> 00:11:45,200
been some rigor and some thought

348
00:11:45,200 --> 00:11:47,920
put into who each of those people are

349
00:11:47,920 --> 00:11:49,200
and you know that there's a rating

350
00:11:49,200 --> 00:11:50,000
system

351
00:11:50,000 --> 00:11:52,800
um tied to it airbnb is very similar i'm

352
00:11:52,800 --> 00:11:54,320
not going to go to a random person's

353
00:11:54,320 --> 00:11:55,600
house and stay there

354
00:11:55,600 --> 00:11:58,079
but through airbnb i can look at ratings

355
00:11:58,079 --> 00:11:59,120
and reviews

356
00:11:59,120 --> 00:12:01,839
uh hosts can see my profile and see what

357
00:12:01,839 --> 00:12:03,519
other hosts have said about me

358
00:12:03,519 --> 00:12:05,680
so you start build that as you start to

359
00:12:05,680 --> 00:12:07,760
build that trust in a platform

360
00:12:07,760 --> 00:12:09,279
and then the third piece is trust in the

361
00:12:09,279 --> 00:12:11,920
other users once you trust the idea

362
00:12:11,920 --> 00:12:13,920
you trust the platform is a safe place

363
00:12:13,920 --> 00:12:15,839
to explore that idea you then want to

364
00:12:15,839 --> 00:12:16,800
trust that the

365
00:12:16,800 --> 00:12:18,560
person that i'm getting in the car with

366
00:12:18,560 --> 00:12:20,240
or the the person whose house

367
00:12:20,240 --> 00:12:22,560
i'm staying at in an airbnb are

368
00:12:22,560 --> 00:12:24,240
trustworthy people that

369
00:12:24,240 --> 00:12:25,920
they're not crazy drivers that they

370
00:12:25,920 --> 00:12:28,079
don't have spy cams hidden in their

371
00:12:28,079 --> 00:12:31,040
apartments um so those are things that

372
00:12:31,040 --> 00:12:33,200
the platforms then have to enable

373
00:12:33,200 --> 00:12:35,200
and in many technology products that

374
00:12:35,200 --> 00:12:36,399
comes across in

375
00:12:36,399 --> 00:12:39,360
the frame of ratings and reviews or user

376
00:12:39,360 --> 00:12:40,240
profiles

377
00:12:40,240 --> 00:12:42,399
so you can see how technology products

378
00:12:42,399 --> 00:12:45,040
are stepping in to fill in these gaps

379
00:12:45,040 --> 00:12:47,440
so that they can help us build trust

380
00:12:47,440 --> 00:12:50,320
with unknown parties

381
00:12:50,320 --> 00:12:51,920
now the interesting thing and i've kind

382
00:12:51,920 --> 00:12:53,839
of touched on this already is that trust

383
00:12:53,839 --> 00:12:54,480
evolves

384
00:12:54,480 --> 00:12:57,120
it changes it's not a stagnant concept

385
00:12:57,120 --> 00:12:58,720
and this is something that i really like

386
00:12:58,720 --> 00:13:00,560
the way rachel talks about this

387
00:13:00,560 --> 00:13:03,040
so with her concept of trust leaps if

388
00:13:03,040 --> 00:13:04,880
you apply them to different industries

389
00:13:04,880 --> 00:13:05,600
like personal

390
00:13:05,600 --> 00:13:08,560
transport or payments you can see really

391
00:13:08,560 --> 00:13:09,519
clear

392
00:13:09,519 --> 00:13:12,160
leaps of trust as we've expanded our

393
00:13:12,160 --> 00:13:13,120
horizon

394
00:13:13,120 --> 00:13:15,760
and gotten more trustworthy of unknown

395
00:13:15,760 --> 00:13:16,560
context

396
00:13:16,560 --> 00:13:18,399
so for example with personal transport

397
00:13:18,399 --> 00:13:20,720
before you were really limited by how

398
00:13:20,720 --> 00:13:21,600
far your horse and

399
00:13:21,600 --> 00:13:23,839
your horse and your car could travel um

400
00:13:23,839 --> 00:13:25,600
you really were in a small

401
00:13:25,600 --> 00:13:28,639
community where um you kind of knew

402
00:13:28,639 --> 00:13:30,720
the the relationships between there then

403
00:13:30,720 --> 00:13:32,240
we introduced trains where you could

404
00:13:32,240 --> 00:13:33,519
cross the country

405
00:13:33,519 --> 00:13:35,120
there's no way you could know about what

406
00:13:35,120 --> 00:13:36,560
was happening on the other side of the

407
00:13:36,560 --> 00:13:37,279
country

408
00:13:37,279 --> 00:13:40,000
you then see how that grows and adds to

409
00:13:40,000 --> 00:13:41,680
itself you see ride sharing there that

410
00:13:41,680 --> 00:13:43,440
we've talked a little bit about

411
00:13:43,440 --> 00:13:45,040
when you look at the payment system

412
00:13:45,040 --> 00:13:46,480
before in

413
00:13:46,480 --> 00:13:49,120
before proto payments um we really had

414
00:13:49,120 --> 00:13:49,519
this

415
00:13:49,519 --> 00:13:51,360
idea of bartering that i could see that

416
00:13:51,360 --> 00:13:53,519
you had a goat and i valued a goat

417
00:13:53,519 --> 00:13:55,440
is something that i wanted um so it was

418
00:13:55,440 --> 00:13:56,800
easy to barter

419
00:13:56,800 --> 00:13:58,800
we then moved to fiat money and

420
00:13:58,800 --> 00:14:00,560
currencies where rather than trading

421
00:14:00,560 --> 00:14:01,199
goats

422
00:14:01,199 --> 00:14:04,160
or cows or crops we were instead trading

423
00:14:04,160 --> 00:14:04,880
coins

424
00:14:04,880 --> 00:14:07,839
that represented value we've now moved

425
00:14:07,839 --> 00:14:09,600
past the point of coins

426
00:14:09,600 --> 00:14:11,920
actually having gold backing them in

427
00:14:11,920 --> 00:14:14,000
banks somewhere and now we're using

428
00:14:14,000 --> 00:14:16,160
credit cards and digital payment systems

429
00:14:16,160 --> 00:14:17,040
that have

430
00:14:17,040 --> 00:14:19,760
no tangible aspect in the real world

431
00:14:19,760 --> 00:14:20,480
other than

432
00:14:20,480 --> 00:14:22,720
we're trading bits between computer

433
00:14:22,720 --> 00:14:24,480
systems that we trust

434
00:14:24,480 --> 00:14:28,320
respect our net worth and values

435
00:14:28,320 --> 00:14:30,639
so rachel continues to go on to talk

436
00:14:30,639 --> 00:14:32,720
about how this trust evolves and the way

437
00:14:32,720 --> 00:14:34,959
she talks about i think is really smart

438
00:14:34,959 --> 00:14:37,040
um she talks about local trust is the

439
00:14:37,040 --> 00:14:38,959
way that you have to start

440
00:14:38,959 --> 00:14:40,880
peer-to-peer you then move to

441
00:14:40,880 --> 00:14:43,120
institutional trusts where institutions

442
00:14:43,120 --> 00:14:44,320
like banks

443
00:14:44,320 --> 00:14:46,560
and governments sit in between

444
00:14:46,560 --> 00:14:47,680
individuals

445
00:14:47,680 --> 00:14:49,760
to help connect and provide

446
00:14:49,760 --> 00:14:51,920
institutional levels of trust

447
00:14:51,920 --> 00:14:54,320
we're now moving into a more distributed

448
00:14:54,320 --> 00:14:56,240
functionality uh this is

449
00:14:56,240 --> 00:14:57,920
i always joke that i don't want to refer

450
00:14:57,920 --> 00:14:59,360
to crypto um

451
00:14:59,360 --> 00:15:02,079
but it's true crypto allows us to start

452
00:15:02,079 --> 00:15:03,279
doing things like

453
00:15:03,279 --> 00:15:05,680
distributed payment systems um credit

454
00:15:05,680 --> 00:15:07,839
cards even now have shattered the idea

455
00:15:07,839 --> 00:15:10,079
of banks that i don't need a bank to

456
00:15:10,079 --> 00:15:11,279
endorse me

457
00:15:11,279 --> 00:15:13,920
to be able to swipe a credit card to use

458
00:15:13,920 --> 00:15:15,680
um and purchase services so you can see

459
00:15:15,680 --> 00:15:17,360
how that evolution of trust

460
00:15:17,360 --> 00:15:20,240
changes and when you look at these

461
00:15:20,240 --> 00:15:21,600
there's some really unique

462
00:15:21,600 --> 00:15:23,600
properties about what it means to have

463
00:15:23,600 --> 00:15:25,519
institutional trust and distributed

464
00:15:25,519 --> 00:15:26,240
trust

465
00:15:26,240 --> 00:15:29,759
institutional is opaque it's closed it's

466
00:15:29,759 --> 00:15:31,519
a centralized system when you think

467
00:15:31,519 --> 00:15:32,959
about a giant bank

468
00:15:32,959 --> 00:15:35,440
or a government it's really unclear how

469
00:15:35,440 --> 00:15:37,279
a lot of things work and happen and

470
00:15:37,279 --> 00:15:38,880
function in the day-to-day

471
00:15:38,880 --> 00:15:42,079
uh society it's it's very top-down

472
00:15:42,079 --> 00:15:44,639
um you have to buy into that system and

473
00:15:44,639 --> 00:15:45,920
believe that it works

474
00:15:45,920 --> 00:15:47,279
whereas when you look at something like

475
00:15:47,279 --> 00:15:49,480
distributed trust it's about

476
00:15:49,480 --> 00:15:51,959
transparency inclusiveness this

477
00:15:51,959 --> 00:15:53,360
decentralization

478
00:15:53,360 --> 00:15:56,560
it's very much bottom up so it's

479
00:15:56,560 --> 00:15:57,839
interesting when you're building your

480
00:15:57,839 --> 00:15:59,680
technology products to think about

481
00:15:59,680 --> 00:16:02,880
what stage of trust are you in and think

482
00:16:02,880 --> 00:16:04,639
about the different levers that

483
00:16:04,639 --> 00:16:08,000
make these types of trust choices true

484
00:16:08,000 --> 00:16:10,880
for your technology product so the thing

485
00:16:10,880 --> 00:16:11,360
that i

486
00:16:11,360 --> 00:16:13,680
like about trust and technology is that

487
00:16:13,680 --> 00:16:15,759
i believe that technology

488
00:16:15,759 --> 00:16:18,480
accelerates the race the rate of trust

489
00:16:18,480 --> 00:16:19,120
leaps and

490
00:16:19,120 --> 00:16:21,759
further distributes trust so we're

491
00:16:21,759 --> 00:16:23,519
taking those concepts that

492
00:16:23,519 --> 00:16:25,519
francis and rachel have talked about and

493
00:16:25,519 --> 00:16:26,560
applying them

494
00:16:26,560 --> 00:16:29,759
to the speed at which technology evolves

495
00:16:29,759 --> 00:16:30,720
it helps us

496
00:16:30,720 --> 00:16:33,839
make those leaps um to trust platforms

497
00:16:33,839 --> 00:16:35,839
that we otherwise would not have trusted

498
00:16:35,839 --> 00:16:37,680
before

499
00:16:37,680 --> 00:16:39,199
so now let's jump into the types of

500
00:16:39,199 --> 00:16:40,720
trust

501
00:16:40,720 --> 00:16:43,920
so this is a framework uh that was part

502
00:16:43,920 --> 00:16:45,360
of a research project

503
00:16:45,360 --> 00:16:48,560
from hsbc which is a bank

504
00:16:48,560 --> 00:16:50,320
as you can imagine banks are very

505
00:16:50,320 --> 00:16:52,160
interested in how they

506
00:16:52,160 --> 00:16:54,560
uh how their customers perceive them and

507
00:16:54,560 --> 00:16:55,519
trust them

508
00:16:55,519 --> 00:16:58,800
um this this survey is from 2017 with 12

509
00:16:58,800 --> 00:17:00,320
000 participants in 11 different

510
00:17:00,320 --> 00:17:01,120
countries

511
00:17:01,120 --> 00:17:03,040
so it's pretty wide ranging i encourage

512
00:17:03,040 --> 00:17:04,640
you to go look at this study if you like

513
00:17:04,640 --> 00:17:05,599
the framework

514
00:17:05,599 --> 00:17:08,319
so basically they said there's sort of a

515
00:17:08,319 --> 00:17:10,720
quadrant system of what it means to

516
00:17:10,720 --> 00:17:12,240
trust something

517
00:17:12,240 --> 00:17:15,199
on the x-axis you've got long-term trust

518
00:17:15,199 --> 00:17:15,520
and

519
00:17:15,520 --> 00:17:18,319
temporary trust and on the y-axis you

520
00:17:18,319 --> 00:17:20,480
have very deep trust and very shallow

521
00:17:20,480 --> 00:17:22,160
trust and this creates an interesting

522
00:17:22,160 --> 00:17:23,280
matrix where

523
00:17:23,280 --> 00:17:26,400
you've got things that are short lived

524
00:17:26,400 --> 00:17:28,640
but deep which are functional you've got

525
00:17:28,640 --> 00:17:30,720
those things that are long-term and deep

526
00:17:30,720 --> 00:17:31,679
which are very much

527
00:17:31,679 --> 00:17:34,160
love related you've got the shallow but

528
00:17:34,160 --> 00:17:35,520
long-term these are things like

529
00:17:35,520 --> 00:17:37,520
necessary needs and then you've got

530
00:17:37,520 --> 00:17:39,200
the shallow and the temporary which is

531
00:17:39,200 --> 00:17:42,799
really lustful types of technologies

532
00:17:42,799 --> 00:17:44,720
so if we think about what are some

533
00:17:44,720 --> 00:17:46,640
concepts that we have today

534
00:17:46,640 --> 00:17:48,400
um not looking at technology you can

535
00:17:48,400 --> 00:17:49,840
start plotting things

536
00:17:49,840 --> 00:17:52,480
into this uh matrix so if you think

537
00:17:52,480 --> 00:17:52,880
about

538
00:17:52,880 --> 00:17:54,960
friends and family and loved ones they

539
00:17:54,960 --> 00:17:56,480
are people that you have long-term

540
00:17:56,480 --> 00:17:57,600
relationships with

541
00:17:57,600 --> 00:17:59,679
and those relationships are deep they're

542
00:17:59,679 --> 00:18:00,720
in that top

543
00:18:00,720 --> 00:18:04,480
left corner of the graph

544
00:18:04,480 --> 00:18:06,160
when you think about experts like your

545
00:18:06,160 --> 00:18:07,600
doctors

546
00:18:07,600 --> 00:18:09,600
websites that you consult your

547
00:18:09,600 --> 00:18:10,799
smartphones

548
00:18:10,799 --> 00:18:12,960
these are things that are relatively

549
00:18:12,960 --> 00:18:13,840
short term

550
00:18:13,840 --> 00:18:15,840
in terms of the span of your life but

551
00:18:15,840 --> 00:18:17,440
there's still things that you have very

552
00:18:17,440 --> 00:18:19,600
deep relationships with and then when

553
00:18:19,600 --> 00:18:21,200
you look at the shallow but long term

554
00:18:21,200 --> 00:18:23,120
you've got things like governments

555
00:18:23,120 --> 00:18:25,840
work in financial institutions and then

556
00:18:25,840 --> 00:18:27,280
the fun part is when you get to

557
00:18:27,280 --> 00:18:28,720
temporary and shallow

558
00:18:28,720 --> 00:18:30,240
you can start seeing some of the new

559
00:18:30,240 --> 00:18:32,160
technology and startups coming there

560
00:18:32,160 --> 00:18:34,320
things that pop up they're cool i try

561
00:18:34,320 --> 00:18:36,240
them once and then they die

562
00:18:36,240 --> 00:18:38,160
and it's really the key for these new

563
00:18:38,160 --> 00:18:40,160
technologies to figure out

564
00:18:40,160 --> 00:18:43,200
as they emerge how do they move up into

565
00:18:43,200 --> 00:18:44,400
that top or right

566
00:18:44,400 --> 00:18:46,720
to top right corner of something that

567
00:18:46,720 --> 00:18:48,799
forms a long-term relationship with

568
00:18:48,799 --> 00:18:49,919
someone and has

569
00:18:49,919 --> 00:18:53,360
deep impact on them so let's go a step

570
00:18:53,360 --> 00:18:55,120
further and look at some brands that are

571
00:18:55,120 --> 00:18:56,000
actually doing this

572
00:18:56,000 --> 00:18:58,240
today so up in the top left corner

573
00:18:58,240 --> 00:19:00,880
you've got brands like apple and samsung

574
00:19:00,880 --> 00:19:03,360
who have cultivated this almost

575
00:19:03,360 --> 00:19:04,320
cult-like

576
00:19:04,320 --> 00:19:07,520
fan a base where people just love their

577
00:19:07,520 --> 00:19:08,559
products and will go

578
00:19:08,559 --> 00:19:10,400
out of their way to use them when you

579
00:19:10,400 --> 00:19:12,240
look at the necessary quadrant there are

580
00:19:12,240 --> 00:19:14,080
things like financial institutions

581
00:19:14,080 --> 00:19:16,160
none of us love our banks they're there

582
00:19:16,160 --> 00:19:18,080
because we need a bank to put our money

583
00:19:18,080 --> 00:19:19,120
into

584
00:19:19,120 --> 00:19:21,280
we have shallow relationships with them

585
00:19:21,280 --> 00:19:22,880
they're not things we think about other

586
00:19:22,880 --> 00:19:23,200
than

587
00:19:23,200 --> 00:19:25,600
it's where my paycheck goes um when you

588
00:19:25,600 --> 00:19:27,200
think about the functional area this is

589
00:19:27,200 --> 00:19:29,360
where a lot of technology startups live

590
00:19:29,360 --> 00:19:30,880
today once they've

591
00:19:30,880 --> 00:19:33,360
moved out of the emergent stage things

592
00:19:33,360 --> 00:19:34,559
like weibo

593
00:19:34,559 --> 00:19:37,280
google paypal amazon these are things

594
00:19:37,280 --> 00:19:38,960
that we don't necessarily have a deep

595
00:19:38,960 --> 00:19:39,760
relationship

596
00:19:39,760 --> 00:19:42,000
and they're not very long term but they

597
00:19:42,000 --> 00:19:44,320
still provide a functional value to us

598
00:19:44,320 --> 00:19:46,960
so think about this framework where your

599
00:19:46,960 --> 00:19:48,960
particular technology product might

600
00:19:48,960 --> 00:19:49,520
actually

601
00:19:49,520 --> 00:19:52,640
fall in this system so

602
00:19:52,640 --> 00:19:55,039
why do we care about this what actually

603
00:19:55,039 --> 00:19:56,240
is

604
00:19:56,240 --> 00:19:58,240
worth thinking about trust with our

605
00:19:58,240 --> 00:19:59,760
technology products

606
00:19:59,760 --> 00:20:02,080
this is a survey again from that hsbc

607
00:20:02,080 --> 00:20:03,760
survey where basically they

608
00:20:03,760 --> 00:20:06,720
asked millennials generation x and baby

609
00:20:06,720 --> 00:20:07,919
boomers about

610
00:20:07,919 --> 00:20:10,480
what percent of their daily lives they

611
00:20:10,480 --> 00:20:11,760
were concerned about

612
00:20:11,760 --> 00:20:13,440
and what's really interesting about this

613
00:20:13,440 --> 00:20:14,720
first of all is you'll notice there's

614
00:20:14,720 --> 00:20:15,120
not

615
00:20:15,120 --> 00:20:16,799
really a ton of difference between the

616
00:20:16,799 --> 00:20:18,159
different generations

617
00:20:18,159 --> 00:20:20,320
but one of the things i like to to focus

618
00:20:20,320 --> 00:20:22,960
on is if you look at the areas with red

619
00:20:22,960 --> 00:20:26,240
they're the areas where technology

620
00:20:26,240 --> 00:20:29,200
is causing concerns for these people in

621
00:20:29,200 --> 00:20:30,559
their daily lives

622
00:20:30,559 --> 00:20:32,799
personal data being leaked that's a big

623
00:20:32,799 --> 00:20:34,000
problem in the technology

624
00:20:34,000 --> 00:20:36,880
industry bank account hacking debit card

625
00:20:36,880 --> 00:20:37,679
cloning

626
00:20:37,679 --> 00:20:40,720
identity theft email scams these are all

627
00:20:40,720 --> 00:20:43,360
problems that technology have enabled

628
00:20:43,360 --> 00:20:46,000
and now they're part of the largest

629
00:20:46,000 --> 00:20:47,760
percentage of our concerns

630
00:20:47,760 --> 00:20:50,080
day-to-day this for me as a product

631
00:20:50,080 --> 00:20:51,600
manager and as someone who builds

632
00:20:51,600 --> 00:20:54,240
technology and believes that technology

633
00:20:54,240 --> 00:20:56,559
can create great change in the world

634
00:20:56,559 --> 00:20:57,840
this concerns me

635
00:20:57,840 --> 00:21:00,000
this is not that vision of how

636
00:21:00,000 --> 00:21:01,760
technology can change the world for the

637
00:21:01,760 --> 00:21:02,480
better

638
00:21:02,480 --> 00:21:04,559
instead this is causing a lot of worry

639
00:21:04,559 --> 00:21:06,080
and anxiety for people

640
00:21:06,080 --> 00:21:09,919
i want to see this change so technology

641
00:21:09,919 --> 00:21:13,039
distrust is causing much of the daily

642
00:21:13,039 --> 00:21:14,400
worry that we have

643
00:21:14,400 --> 00:21:16,240
and to me that's something that it

644
00:21:16,240 --> 00:21:18,159
doesn't have to be that way

645
00:21:18,159 --> 00:21:20,400
with a little bit of intention and a

646
00:21:20,400 --> 00:21:22,400
little bit of responsibility we can

647
00:21:22,400 --> 00:21:23,280
really make an

648
00:21:23,280 --> 00:21:26,559
impactful change here

649
00:21:26,640 --> 00:21:28,159
so let's take a look at a couple of

650
00:21:28,159 --> 00:21:30,159
companies that have done this wrong

651
00:21:30,159 --> 00:21:32,400
so these are what i call trust fails and

652
00:21:32,400 --> 00:21:34,640
there's a really great website here that

653
00:21:34,640 --> 00:21:34,960
has

654
00:21:34,960 --> 00:21:37,840
a ton of data breaches and hacks so if

655
00:21:37,840 --> 00:21:39,280
you go to that website and this is

656
00:21:39,280 --> 00:21:39,840
actually

657
00:21:39,840 --> 00:21:42,720
a trust of a test of your trust in me

658
00:21:42,720 --> 00:21:44,720
will you visit that bitly link that i've

659
00:21:44,720 --> 00:21:46,720
provided there i promise it just goes

660
00:21:46,720 --> 00:21:48,320
the information is beautiful website

661
00:21:48,320 --> 00:21:49,840
which has a really cool

662
00:21:49,840 --> 00:21:51,919
infographic that you can explore and

663
00:21:51,919 --> 00:21:53,200
look into

664
00:21:53,200 --> 00:21:55,280
we'll talk about a few of these examples

665
00:21:55,280 --> 00:21:57,120
but already you can see

666
00:21:57,120 --> 00:22:00,559
the scale the scope the severity of the

667
00:22:00,559 --> 00:22:03,120
data problems that we have in technology

668
00:22:03,120 --> 00:22:05,679
is purely behind technology companies

669
00:22:05,679 --> 00:22:06,480
just not being

670
00:22:06,480 --> 00:22:08,159
responsible for the data and the

671
00:22:08,159 --> 00:22:09,919
technologies that they create

672
00:22:09,919 --> 00:22:12,240
i want to see this change so let's start

673
00:22:12,240 --> 00:22:13,919
with the obvious one we've all heard

674
00:22:13,919 --> 00:22:14,960
about equifax

675
00:22:14,960 --> 00:22:16,640
we've all probably had to deal with

676
00:22:16,640 --> 00:22:18,080
freezing our credit reports

677
00:22:18,080 --> 00:22:20,240
one of the largest sensitive data

678
00:22:20,240 --> 00:22:21,440
breaches

679
00:22:21,440 --> 00:22:24,320
that happened in 2017. i won't spend

680
00:22:24,320 --> 00:22:25,520
much time here but

681
00:22:25,520 --> 00:22:29,520
lots of data was uh compromised in this

682
00:22:29,520 --> 00:22:32,640
breach um and i think this was the

683
00:22:32,640 --> 00:22:34,320
moment where we finally realized like

684
00:22:34,320 --> 00:22:36,799
the scale at which this can happen

685
00:22:36,799 --> 00:22:40,000
can be really wide ranging let's look at

686
00:22:40,000 --> 00:22:42,240
something a little more sensitive though

687
00:22:42,240 --> 00:22:45,280
um so i am an openly gay man um there

688
00:22:45,280 --> 00:22:47,280
is an app called grindr which is

689
00:22:47,280 --> 00:22:49,280
basically for you to connect with other

690
00:22:49,280 --> 00:22:50,640
lgbtq

691
00:22:50,640 --> 00:22:53,120
individuals um it's a hookup app let's

692
00:22:53,120 --> 00:22:54,000
be frank

693
00:22:54,000 --> 00:22:57,280
um the technology behind it though

694
00:22:57,280 --> 00:22:58,960
provides you to do some interesting

695
00:22:58,960 --> 00:23:00,320
things like remind

696
00:23:00,320 --> 00:23:03,039
people to get tested for hiv and

697
00:23:03,039 --> 00:23:05,200
unfortunately for grindr they

698
00:23:05,200 --> 00:23:08,000
had a third party data leak where not

699
00:23:08,000 --> 00:23:08,559
only

700
00:23:08,559 --> 00:23:10,880
could someone track a person's

701
00:23:10,880 --> 00:23:12,320
individual

702
00:23:12,320 --> 00:23:15,679
gps location but they also could access

703
00:23:15,679 --> 00:23:19,039
data about their hiv status

704
00:23:19,039 --> 00:23:21,039
and could access that information in

705
00:23:21,039 --> 00:23:22,799
unsecured api calls

706
00:23:22,799 --> 00:23:26,080
this is hugely problematic so you take

707
00:23:26,080 --> 00:23:26,320
an

708
00:23:26,320 --> 00:23:28,799
app that's intended to connect an

709
00:23:28,799 --> 00:23:30,640
at-risk minority group

710
00:23:30,640 --> 00:23:33,200
and then don't put the correct privacy

711
00:23:33,200 --> 00:23:34,640
controls in place

712
00:23:34,640 --> 00:23:36,480
and don't take responsibility for the

713
00:23:36,480 --> 00:23:37,919
data you're collecting

714
00:23:37,919 --> 00:23:41,360
you're putting people at real risk

715
00:23:41,360 --> 00:23:43,360
for example if i were to travel to

716
00:23:43,360 --> 00:23:45,200
countries where i could literally be put

717
00:23:45,200 --> 00:23:46,080
to death

718
00:23:46,080 --> 00:23:48,799
and my location can be easily found in

719
00:23:48,799 --> 00:23:49,200
health

720
00:23:49,200 --> 00:23:52,400
data around my hiv status could be

721
00:23:52,400 --> 00:23:53,200
exposed

722
00:23:53,200 --> 00:23:55,919
we're talking about legitimate safety

723
00:23:55,919 --> 00:23:57,039
concerns for me

724
00:23:57,039 --> 00:23:59,840
as well as potential impacts to my

725
00:23:59,840 --> 00:24:02,000
insurability as a person

726
00:24:02,000 --> 00:24:04,159
if data about my health status were to

727
00:24:04,159 --> 00:24:06,000
be leaked in a way that i

728
00:24:06,000 --> 00:24:08,559
couldn't control um so these data

729
00:24:08,559 --> 00:24:09,440
breaches are

730
00:24:09,440 --> 00:24:11,600
really sensitive and our technology

731
00:24:11,600 --> 00:24:12,559
products are just

732
00:24:12,559 --> 00:24:15,600
causing us to put more and more

733
00:24:15,600 --> 00:24:18,000
data out into the world and in some

734
00:24:18,000 --> 00:24:19,200
cases data that

735
00:24:19,200 --> 00:24:21,440
we don't even realize is leaving the

736
00:24:21,440 --> 00:24:22,240
safety

737
00:24:22,240 --> 00:24:25,600
of our own devices if we look at

738
00:24:25,600 --> 00:24:27,279
facebook we've all heard about the

739
00:24:27,279 --> 00:24:28,960
cambridge analytica

740
00:24:28,960 --> 00:24:31,039
um breaches were basically facebook

741
00:24:31,039 --> 00:24:32,320
didn't put the correct

742
00:24:32,320 --> 00:24:35,120
controls around the data they collected

743
00:24:35,120 --> 00:24:35,440
so

744
00:24:35,440 --> 00:24:37,520
a company called cambridge analytica

745
00:24:37,520 --> 00:24:38,480
scanned

746
00:24:38,480 --> 00:24:41,120
facebook profiles for what is called uh

747
00:24:41,120 --> 00:24:42,240
psychographic

748
00:24:42,240 --> 00:24:45,039
data to profile people and then that was

749
00:24:45,039 --> 00:24:45,600
used

750
00:24:45,600 --> 00:24:49,120
by russia to influence the u.s election

751
00:24:49,120 --> 00:24:50,880
of course this is an election year in

752
00:24:50,880 --> 00:24:52,159
the united states

753
00:24:52,159 --> 00:24:54,000
and again this becomes a really

754
00:24:54,000 --> 00:24:56,799
interesting um and concerning problem

755
00:24:56,799 --> 00:25:00,000
about how accessible our data is

756
00:25:00,000 --> 00:25:02,159
on the internet and how that can be used

757
00:25:02,159 --> 00:25:04,320
to manipulate us

758
00:25:04,320 --> 00:25:07,120
continuing on with facebook um they also

759
00:25:07,120 --> 00:25:07,600
have

760
00:25:07,600 --> 00:25:09,679
other issues so they've got a product

761
00:25:09,679 --> 00:25:10,720
called portal

762
00:25:10,720 --> 00:25:12,559
which is basically a video camera that

763
00:25:12,559 --> 00:25:15,360
will follow you around as you move

764
00:25:15,360 --> 00:25:18,000
and when they release this pro or this

765
00:25:18,000 --> 00:25:18,640
hardware

766
00:25:18,640 --> 00:25:20,320
project i think it's a really cool

767
00:25:20,320 --> 00:25:22,240
concept

768
00:25:22,240 --> 00:25:24,240
but when they released it even their

769
00:25:24,240 --> 00:25:25,840
executives and their

770
00:25:25,840 --> 00:25:28,960
uh privacy policies didn't make it clear

771
00:25:28,960 --> 00:25:31,200
if facebook could use the audio in the

772
00:25:31,200 --> 00:25:33,200
video that it was collecting

773
00:25:33,200 --> 00:25:35,440
um in those video calls to do ad

774
00:25:35,440 --> 00:25:36,880
retargeting to you

775
00:25:36,880 --> 00:25:39,679
um an example of how a technology

776
00:25:39,679 --> 00:25:40,400
company

777
00:25:40,400 --> 00:25:42,240
created an interesting and novel

778
00:25:42,240 --> 00:25:43,679
technology but

779
00:25:43,679 --> 00:25:46,480
then didn't put the correct controls

780
00:25:46,480 --> 00:25:47,520
around it

781
00:25:47,520 --> 00:25:50,960
to create that cycle of trust

782
00:25:50,960 --> 00:25:52,880
where people would actually use this i

783
00:25:52,880 --> 00:25:55,279
personally will not be purchasing a

784
00:25:55,279 --> 00:25:57,440
portal device much less putting it in my

785
00:25:57,440 --> 00:25:59,600
home and letting facebook collect all of

786
00:25:59,600 --> 00:26:01,360
that data

787
00:26:01,360 --> 00:26:03,360
continuing on there was a pew research

788
00:26:03,360 --> 00:26:06,080
study done specifically on facebook and

789
00:26:06,080 --> 00:26:06,559
trust

790
00:26:06,559 --> 00:26:09,760
and they found that only six percent of

791
00:26:09,760 --> 00:26:11,919
adults trusted facebook

792
00:26:11,919 --> 00:26:14,080
sixty percent said they don't trust

793
00:26:14,080 --> 00:26:15,840
facebook to protect their data at

794
00:26:15,840 --> 00:26:18,799
all and three quarters of adult facebook

795
00:26:18,799 --> 00:26:19,840
users have taken

796
00:26:19,840 --> 00:26:22,559
some action to curtail their facebook

797
00:26:22,559 --> 00:26:23,440
use

798
00:26:23,440 --> 00:26:27,039
this is a great example of how if you

799
00:26:27,039 --> 00:26:30,080
don't build trust with your customers

800
00:26:30,080 --> 00:26:32,880
or if you actively break trust with your

801
00:26:32,880 --> 00:26:33,840
customers

802
00:26:33,840 --> 00:26:36,480
they'll go do something else this is

803
00:26:36,480 --> 00:26:37,120
where

804
00:26:37,120 --> 00:26:40,240
trust actually turns into very real

805
00:26:40,240 --> 00:26:41,760
financial concerns

806
00:26:41,760 --> 00:26:44,559
for your business

807
00:26:47,120 --> 00:26:49,360
um i laugh at this photo every time it

808
00:26:49,360 --> 00:26:51,279
circles around social media so this is

809
00:26:51,279 --> 00:26:52,559
zuck in his office

810
00:26:52,559 --> 00:26:55,360
um joking but if you zoom into his

811
00:26:55,360 --> 00:26:57,440
computer there in the bottom left corner

812
00:26:57,440 --> 00:27:00,000
you'll notice zuck himself doesn't even

813
00:27:00,000 --> 00:27:02,240
trust the camera and the audio ports on

814
00:27:02,240 --> 00:27:03,600
his computer

815
00:27:03,600 --> 00:27:07,039
this is one where if even a billionaire

816
00:27:07,039 --> 00:27:08,799
who runs facebook doesn't trust the

817
00:27:08,799 --> 00:27:10,720
hardware and the systems that

818
00:27:10,720 --> 00:27:13,520
even his company helped build how does

819
00:27:13,520 --> 00:27:14,960
how can any of us

820
00:27:14,960 --> 00:27:17,600
individually have any chance at securing

821
00:27:17,600 --> 00:27:20,640
ourselves and our data

822
00:27:20,640 --> 00:27:22,720
amazon itself has struggled with some

823
00:27:22,720 --> 00:27:24,320
privacy concerns there was

824
00:27:24,320 --> 00:27:27,600
a very strange story about a portland

825
00:27:27,600 --> 00:27:29,360
couple who were having a private

826
00:27:29,360 --> 00:27:31,679
conversation in their living room

827
00:27:31,679 --> 00:27:33,840
and then got a phone call from their

828
00:27:33,840 --> 00:27:35,279
friend who

829
00:27:35,279 --> 00:27:37,520
said basically the alexa device was

830
00:27:37,520 --> 00:27:38,559
streaming

831
00:27:38,559 --> 00:27:42,399
their conversation to her alexa device

832
00:27:42,399 --> 00:27:45,279
basically in their conversation alexa

833
00:27:45,279 --> 00:27:47,120
had misinterpreted some of the

834
00:27:47,120 --> 00:27:50,320
wording as signal words

835
00:27:50,320 --> 00:27:53,440
and then started a connection call

836
00:27:53,440 --> 00:27:57,200
that feature of amazon echo was

837
00:27:57,200 --> 00:28:00,960
actually on by default for customers and

838
00:28:00,960 --> 00:28:02,880
it wasn't until recently that they had

839
00:28:02,880 --> 00:28:03,600
changed that

840
00:28:03,600 --> 00:28:05,600
and made it an opt-in feature so an

841
00:28:05,600 --> 00:28:07,200
example of rolling out

842
00:28:07,200 --> 00:28:08,960
new and novel functionality that some

843
00:28:08,960 --> 00:28:10,320
people might use but

844
00:28:10,320 --> 00:28:13,520
by turning it on by default creates this

845
00:28:13,520 --> 00:28:15,760
opportunity for people to have things

846
00:28:15,760 --> 00:28:18,799
happen that they don't expect

847
00:28:18,799 --> 00:28:21,520
um alexa and amazon have moved forward

848
00:28:21,520 --> 00:28:22,080
on

849
00:28:22,080 --> 00:28:24,320
other interesting devices like this ring

850
00:28:24,320 --> 00:28:26,960
device and sunglasses

851
00:28:26,960 --> 00:28:28,960
that have alexa built into them so now

852
00:28:28,960 --> 00:28:31,200
it's not even just a stagnant hardware

853
00:28:31,200 --> 00:28:32,000
device

854
00:28:32,000 --> 00:28:34,159
in a home it's things that can be

855
00:28:34,159 --> 00:28:35,840
anywhere on a person

856
00:28:35,840 --> 00:28:38,559
um live and recording conversations and

857
00:28:38,559 --> 00:28:39,200
doing

858
00:28:39,200 --> 00:28:41,600
who knows what with them um so it's

859
00:28:41,600 --> 00:28:42,720
interesting to see

860
00:28:42,720 --> 00:28:45,120
how these technology companies really

861
00:28:45,120 --> 00:28:48,000
have to grapple with concepts of trust

862
00:28:48,000 --> 00:28:50,159
so that brings us to what does it mean

863
00:28:50,159 --> 00:28:53,279
to build trust in tech products

864
00:28:53,279 --> 00:28:55,200
so going back to our framework from the

865
00:28:55,200 --> 00:28:56,399
bank

866
00:28:56,399 --> 00:28:59,200
if we look at some of the technology

867
00:28:59,200 --> 00:29:00,960
concepts that we've got it

868
00:29:00,960 --> 00:29:02,880
and we touched on some of these earlier

869
00:29:02,880 --> 00:29:04,000
about how

870
00:29:04,000 --> 00:29:06,960
um brands that can engage fans create

871
00:29:06,960 --> 00:29:07,679
this like

872
00:29:07,679 --> 00:29:10,320
uh die-hard fans who go out of their way

873
00:29:10,320 --> 00:29:11,679
who stand in line

874
00:29:11,679 --> 00:29:15,120
for days and weeks to purchase the new

875
00:29:15,120 --> 00:29:18,080
devices that come out those are like

876
00:29:18,080 --> 00:29:19,520
concepts of love

877
00:29:19,520 --> 00:29:21,120
you've got someone you've created

878
00:29:21,120 --> 00:29:23,120
technology that people love so much that

879
00:29:23,120 --> 00:29:23,679
they go

880
00:29:23,679 --> 00:29:25,679
out of their way to go and get it when

881
00:29:25,679 --> 00:29:27,279
you think about functional items like

882
00:29:27,279 --> 00:29:29,200
our phones and even our wearables these

883
00:29:29,200 --> 00:29:30,399
are devices that we

884
00:29:30,399 --> 00:29:32,799
wear on our person and have on us at all

885
00:29:32,799 --> 00:29:33,919
times

886
00:29:33,919 --> 00:29:36,159
they're very personal but they're

887
00:29:36,159 --> 00:29:38,000
devices that when the next one comes out

888
00:29:38,000 --> 00:29:40,240
or when the screen breaks on this

889
00:29:40,240 --> 00:29:43,200
they're gone and you move on when you

890
00:29:43,200 --> 00:29:44,720
look at the necessary

891
00:29:44,720 --> 00:29:46,960
um section you've got things that are

892
00:29:46,960 --> 00:29:48,399
high switching costs

893
00:29:48,399 --> 00:29:50,480
uh banks again are a good example of

894
00:29:50,480 --> 00:29:52,720
this nobody wants to go and switch banks

895
00:29:52,720 --> 00:29:54,799
because it's a pain to do

896
00:29:54,799 --> 00:29:56,640
creating a new account moving money

897
00:29:56,640 --> 00:29:58,720
between it changing all of your paycheck

898
00:29:58,720 --> 00:30:02,159
uh etfs uh it's just something where

899
00:30:02,159 --> 00:30:04,399
it's not a very deep connection but

900
00:30:04,399 --> 00:30:06,159
because of high switching costs we let

901
00:30:06,159 --> 00:30:07,679
it be long

902
00:30:07,679 --> 00:30:09,279
uh things that are in the necessary

903
00:30:09,279 --> 00:30:11,440
column tend to not be things that are

904
00:30:11,440 --> 00:30:13,120
super trustworthy or

905
00:30:13,120 --> 00:30:14,720
have much thought put in them because

906
00:30:14,720 --> 00:30:16,799
they know that you can't go anywhere and

907
00:30:16,799 --> 00:30:18,000
it's a pain to do

908
00:30:18,000 --> 00:30:19,919
we're seeing technology change some of

909
00:30:19,919 --> 00:30:22,559
that um and then the the lust

910
00:30:22,559 --> 00:30:24,799
category you see things like low value

911
00:30:24,799 --> 00:30:27,120
luxury items handbags

912
00:30:27,120 --> 00:30:29,840
um as seen on tv type products and when

913
00:30:29,840 --> 00:30:31,760
you think about all of these different

914
00:30:31,760 --> 00:30:32,960
types of

915
00:30:32,960 --> 00:30:35,120
technologies and how they impact our

916
00:30:35,120 --> 00:30:37,279
world there are ways that you can break

917
00:30:37,279 --> 00:30:39,520
trust with these things so when you

918
00:30:39,520 --> 00:30:41,039
think about this

919
00:30:41,039 --> 00:30:43,600
think about things like scandals or

920
00:30:43,600 --> 00:30:44,720
betrayals

921
00:30:44,720 --> 00:30:47,360
for people you love a betrayal could

922
00:30:47,360 --> 00:30:48,720
break that trust

923
00:30:48,720 --> 00:30:50,640
for companies that you love their

924
00:30:50,640 --> 00:30:53,039
products for example i'm a big apple fan

925
00:30:53,039 --> 00:30:55,840
if apple had a giant privacy scandal i

926
00:30:55,840 --> 00:30:57,360
might be less willing to be

927
00:30:57,360 --> 00:31:00,080
a fan of apple products when you look at

928
00:31:00,080 --> 00:31:02,159
functional items it becomes much more

929
00:31:02,159 --> 00:31:03,279
about breakage

930
00:31:03,279 --> 00:31:05,919
or fading fads i'll tell you a quick

931
00:31:05,919 --> 00:31:07,360
story about

932
00:31:07,360 --> 00:31:10,640
um not the apple watch but the uh

933
00:31:10,640 --> 00:31:13,600
the fitness band one of the companies

934
00:31:13,600 --> 00:31:14,399
basically was

935
00:31:14,399 --> 00:31:16,640
aggregating fitness data and they

936
00:31:16,640 --> 00:31:19,120
released a blog post that basically said

937
00:31:19,120 --> 00:31:22,159
huh really weird uh in time-adjusted

938
00:31:22,159 --> 00:31:24,000
data we've noticed that around seven or

939
00:31:24,000 --> 00:31:25,440
eight o'clock in the evening there's a

940
00:31:25,440 --> 00:31:26,640
lot of activity

941
00:31:26,640 --> 00:31:28,480
um and then they wrote a blog post about

942
00:31:28,480 --> 00:31:29,760
people having uh

943
00:31:29,760 --> 00:31:33,120
sex um that's not how i want my personal

944
00:31:33,120 --> 00:31:35,360
data to be used even in an aggregated

945
00:31:35,360 --> 00:31:36,000
form

946
00:31:36,000 --> 00:31:38,559
it feels invasive um and with these

947
00:31:38,559 --> 00:31:40,799
devices that literally are on our bodies

948
00:31:40,799 --> 00:31:42,399
that are listening to our heart rates

949
00:31:42,399 --> 00:31:43,120
that are

950
00:31:43,120 --> 00:31:45,919
listening to audio or recording video in

951
00:31:45,919 --> 00:31:46,480
the room

952
00:31:46,480 --> 00:31:49,120
they're very sensitive things and when

953
00:31:49,120 --> 00:31:51,039
something that we don't intend to be

954
00:31:51,039 --> 00:31:53,679
be public becomes public that's a moment

955
00:31:53,679 --> 00:31:55,440
of broken trust

956
00:31:55,440 --> 00:31:57,279
so lots of different ways that you can

957
00:31:57,279 --> 00:31:58,559
fail at trust

958
00:31:58,559 --> 00:31:59,919
but there's a lot of things that you can

959
00:31:59,919 --> 00:32:01,919
do to actually build it and i believe

960
00:32:01,919 --> 00:32:04,320
that trust can be a fundamental

961
00:32:04,320 --> 00:32:06,640
differentiator for technology products

962
00:32:06,640 --> 00:32:08,559
and we'll look at a couple examples of

963
00:32:08,559 --> 00:32:11,120
technology companies that get this right

964
00:32:11,120 --> 00:32:13,279
and use it as a core differentiator for

965
00:32:13,279 --> 00:32:15,279
their products

966
00:32:15,279 --> 00:32:17,679
so we've talked a lot about what trust

967
00:32:17,679 --> 00:32:18,480
is

968
00:32:18,480 --> 00:32:21,600
how do you build it between people and

969
00:32:21,600 --> 00:32:23,360
between communities but what about

970
00:32:23,360 --> 00:32:24,720
humans and machines

971
00:32:24,720 --> 00:32:26,320
as we're building these technology

972
00:32:26,320 --> 00:32:28,000
products how do we help

973
00:32:28,000 --> 00:32:30,880
individual people who may not be super

974
00:32:30,880 --> 00:32:31,600
technical

975
00:32:31,600 --> 00:32:34,640
understand how the systems that we build

976
00:32:34,640 --> 00:32:36,320
and the technology products that we

977
00:32:36,320 --> 00:32:36,880
release

978
00:32:36,880 --> 00:32:39,200
work in a way that they actually trust

979
00:32:39,200 --> 00:32:40,240
them

980
00:32:40,240 --> 00:32:42,559
and so i want to take us back to francis

981
00:32:42,559 --> 00:32:44,640
and rachel's frameworks of

982
00:32:44,640 --> 00:32:47,039
building trust through authenticity

983
00:32:47,039 --> 00:32:48,080
through empathy

984
00:32:48,080 --> 00:32:50,720
through logic looking at rachel's

985
00:32:50,720 --> 00:32:52,760
framework for adding competence

986
00:32:52,760 --> 00:32:55,360
reliability and honesty into our

987
00:32:55,360 --> 00:32:56,159
products

988
00:32:56,159 --> 00:32:58,000
and what that actually looks like in

989
00:32:58,000 --> 00:32:59,440
practice so

990
00:32:59,440 --> 00:33:02,080
to start i want to talk through a few

991
00:33:02,080 --> 00:33:04,000
examples of technology products that

992
00:33:04,000 --> 00:33:05,600
actually do a good job of this and

993
00:33:05,600 --> 00:33:06,480
you'll notice

994
00:33:06,480 --> 00:33:08,399
this is a bit in conflict with some of

995
00:33:08,399 --> 00:33:10,240
the privacy problems that i mentioned

996
00:33:10,240 --> 00:33:12,559
earlier but one thing that the amazon

997
00:33:12,559 --> 00:33:14,720
echo device does do is that it's

998
00:33:14,720 --> 00:33:18,240
very clear when it's on when alexa is

999
00:33:18,240 --> 00:33:19,120
listening

1000
00:33:19,120 --> 00:33:21,840
the blue light is on and it lets you

1001
00:33:21,840 --> 00:33:23,679
know that it's listening

1002
00:33:23,679 --> 00:33:27,200
that's a really great example of using

1003
00:33:27,200 --> 00:33:28,720
technology like the light up

1004
00:33:28,720 --> 00:33:31,600
ring around the edge of the device to

1005
00:33:31,600 --> 00:33:32,080
make it

1006
00:33:32,080 --> 00:33:34,640
extremely clear how technology is

1007
00:33:34,640 --> 00:33:35,840
functioning

1008
00:33:35,840 --> 00:33:38,159
when you've got it muted it's turned red

1009
00:33:38,159 --> 00:33:38,960
and you've got a

1010
00:33:38,960 --> 00:33:41,279
good sense that the device is not

1011
00:33:41,279 --> 00:33:42,320
listening

1012
00:33:42,320 --> 00:33:44,640
we can go into the problems about do you

1013
00:33:44,640 --> 00:33:46,640
actually trust the hardware behind all

1014
00:33:46,640 --> 00:33:48,399
of this and the lights actually mean

1015
00:33:48,399 --> 00:33:50,480
what they think they do

1016
00:33:50,480 --> 00:33:53,360
but having clarity in the technology

1017
00:33:53,360 --> 00:33:54,480
that's super clear

1018
00:33:54,480 --> 00:33:57,840
that even a child can understand if

1019
00:33:57,840 --> 00:33:59,840
your listening device is actually

1020
00:33:59,840 --> 00:34:01,679
listening or not is really

1021
00:34:01,679 --> 00:34:05,360
clever consistency is another way to

1022
00:34:05,360 --> 00:34:06,000
build trust

1023
00:34:06,000 --> 00:34:08,879
with people um i look at this from the

1024
00:34:08,879 --> 00:34:11,280
apple human interface guidelines where

1025
00:34:11,280 --> 00:34:13,839
providing consistent messaging helps

1026
00:34:13,839 --> 00:34:15,839
build and train people with how to

1027
00:34:15,839 --> 00:34:17,839
interact with your technology product

1028
00:34:17,839 --> 00:34:19,760
it doesn't matter what this modal says

1029
00:34:19,760 --> 00:34:22,399
at all i know the default action is the

1030
00:34:22,399 --> 00:34:23,199
blue button

1031
00:34:23,199 --> 00:34:24,879
and it's probably what triggered the

1032
00:34:24,879 --> 00:34:27,119
action that i'm trying to accomplish

1033
00:34:27,119 --> 00:34:30,480
um it's this consistency that allows you

1034
00:34:30,480 --> 00:34:31,280
to start

1035
00:34:31,280 --> 00:34:33,760
understanding and feel like you've got

1036
00:34:33,760 --> 00:34:34,399
control

1037
00:34:34,399 --> 00:34:37,520
in a system if every single modal that

1038
00:34:37,520 --> 00:34:38,079
popped up

1039
00:34:38,079 --> 00:34:39,599
looked different and had different

1040
00:34:39,599 --> 00:34:42,159
button styles it would be a lot harder

1041
00:34:42,159 --> 00:34:44,159
and take a lot more cognitive load to

1042
00:34:44,159 --> 00:34:46,159
understand how that technology product

1043
00:34:46,159 --> 00:34:47,359
functions

1044
00:34:47,359 --> 00:34:49,599
the next one is prove value so with

1045
00:34:49,599 --> 00:34:51,359
technology products they're very

1046
00:34:51,359 --> 00:34:52,079
fungible

1047
00:34:52,079 --> 00:34:54,000
unless they're accomplishing something

1048
00:34:54,000 --> 00:34:55,760
for us we're going to move on and use

1049
00:34:55,760 --> 00:34:56,800
something else

1050
00:34:56,800 --> 00:34:59,839
so in your products make it clear what

1051
00:34:59,839 --> 00:35:02,880
value you're creating for your users

1052
00:35:02,880 --> 00:35:04,480
one of the things a great example of

1053
00:35:04,480 --> 00:35:06,480
this i think is my apple watch

1054
00:35:06,480 --> 00:35:08,560
at the end of the day i always check my

1055
00:35:08,560 --> 00:35:11,119
activity rings to see how i'm doing

1056
00:35:11,119 --> 00:35:14,160
and if i'm having a lazy day my watch

1057
00:35:14,160 --> 00:35:16,640
will tap me on the wrist and say hey

1058
00:35:16,640 --> 00:35:20,320
stand up go do act something active

1059
00:35:20,320 --> 00:35:22,480
it's a way where you can be validated

1060
00:35:22,480 --> 00:35:24,320
with the technology and how it's helping

1061
00:35:24,320 --> 00:35:28,320
you improve your life in some way

1062
00:35:29,280 --> 00:35:31,520
the next one is a fun little story about

1063
00:35:31,520 --> 00:35:32,960
building common sense

1064
00:35:32,960 --> 00:35:36,079
um there are some major travel platforms

1065
00:35:36,079 --> 00:35:36,800
and there's a fun

1066
00:35:36,800 --> 00:35:39,520
story about a company where someone

1067
00:35:39,520 --> 00:35:40,560
typed into

1068
00:35:40,560 --> 00:35:43,760
a travel search engine uh trip to paris

1069
00:35:43,760 --> 00:35:47,040
and what they intended was a trip

1070
00:35:47,040 --> 00:35:50,079
to paris france as most of us i assume

1071
00:35:50,079 --> 00:35:51,280
would think the word

1072
00:35:51,280 --> 00:35:55,280
paris would it mean instead the tool

1073
00:35:55,280 --> 00:35:58,079
provided them uh information about a

1074
00:35:58,079 --> 00:36:00,320
trip to paris texas

1075
00:36:00,320 --> 00:36:02,400
which is not as glamorous as paris

1076
00:36:02,400 --> 00:36:04,240
france and is almost certainly

1077
00:36:04,240 --> 00:36:06,000
not what that user was trying to

1078
00:36:06,000 --> 00:36:07,839
accomplish um

1079
00:36:07,839 --> 00:36:10,000
because they were in a rush or for

1080
00:36:10,000 --> 00:36:11,920
whatever reason they ended up buying the

1081
00:36:11,920 --> 00:36:12,880
tickets

1082
00:36:12,880 --> 00:36:16,400
to paris texas rather than paris france

1083
00:36:16,400 --> 00:36:18,320
it's an example of where just with a

1084
00:36:18,320 --> 00:36:20,960
little bit of intuition and common sense

1085
00:36:20,960 --> 00:36:22,800
you can make your technology products

1086
00:36:22,800 --> 00:36:24,000
seem smarter

1087
00:36:24,000 --> 00:36:26,000
and actually help users accomplish what

1088
00:36:26,000 --> 00:36:28,160
you intend to do

1089
00:36:28,160 --> 00:36:30,079
so let's recap some of the things that

1090
00:36:30,079 --> 00:36:31,920
we've talked about these are very much

1091
00:36:31,920 --> 00:36:35,040
educational related things prove your

1092
00:36:35,040 --> 00:36:35,599
products

1093
00:36:35,599 --> 00:36:38,400
competency and show your logic make it

1094
00:36:38,400 --> 00:36:40,480
clear what's happening and why

1095
00:36:40,480 --> 00:36:43,520
set expectations along the way um to

1096
00:36:43,520 --> 00:36:43,920
help

1097
00:36:43,920 --> 00:36:46,079
give people a sense of what you're

1098
00:36:46,079 --> 00:36:47,680
actually accomplishing with your

1099
00:36:47,680 --> 00:36:49,280
technology product

1100
00:36:49,280 --> 00:36:52,160
explain and prove value you've probably

1101
00:36:52,160 --> 00:36:54,240
heard the term of a 10x technology

1102
00:36:54,240 --> 00:36:56,400
product that's something that you're

1103
00:36:56,400 --> 00:36:58,640
willing to use it because it makes the

1104
00:36:58,640 --> 00:36:59,760
experience

1105
00:36:59,760 --> 00:37:02,880
10x better than what it is today

1106
00:37:02,880 --> 00:37:04,240
i think this is one when you look at

1107
00:37:04,240 --> 00:37:06,720
things like productivity apps

1108
00:37:06,720 --> 00:37:09,520
do they make your accomplishing tasks

1109
00:37:09,520 --> 00:37:10,160
and goals

1110
00:37:10,160 --> 00:37:13,200
10x is easy as just writing down a

1111
00:37:13,200 --> 00:37:14,560
sticky note with

1112
00:37:14,560 --> 00:37:16,960
um checkboxes on it probably not and

1113
00:37:16,960 --> 00:37:18,400
that's why i've never found a

1114
00:37:18,400 --> 00:37:19,520
productivity tool

1115
00:37:19,520 --> 00:37:21,280
that works better than my sticky note

1116
00:37:21,280 --> 00:37:22,720
system

1117
00:37:22,720 --> 00:37:25,599
next build consistency consistency

1118
00:37:25,599 --> 00:37:26,240
builds

1119
00:37:26,240 --> 00:37:29,119
confidence and even if your product

1120
00:37:29,119 --> 00:37:31,760
isn't fully fleshed out if it's

1121
00:37:31,760 --> 00:37:34,720
consistent i have a sense as a user of

1122
00:37:34,720 --> 00:37:37,119
how it works and that helps build that

1123
00:37:37,119 --> 00:37:38,160
consistency

1124
00:37:38,160 --> 00:37:40,160
or that helps that build that confidence

1125
00:37:40,160 --> 00:37:41,680
for me also

1126
00:37:41,680 --> 00:37:44,000
be relevant contextualize those

1127
00:37:44,000 --> 00:37:45,119
interactions

1128
00:37:45,119 --> 00:37:47,040
if i'm searching a travel website for

1129
00:37:47,040 --> 00:37:49,760
paris assume i mean paris france

1130
00:37:49,760 --> 00:37:52,640
not paris texas build in those common

1131
00:37:52,640 --> 00:37:53,440
sense

1132
00:37:53,440 --> 00:37:56,480
um gates to help people understand what

1133
00:37:56,480 --> 00:37:59,359
you're trying to accomplish

1134
00:37:59,359 --> 00:38:01,359
and again like i mentioned earlier

1135
00:38:01,359 --> 00:38:03,040
simplicity wins

1136
00:38:03,040 --> 00:38:05,200
one of my favorite things is to see

1137
00:38:05,200 --> 00:38:07,760
alexa devices with these red circles on

1138
00:38:07,760 --> 00:38:08,800
because it's super

1139
00:38:08,800 --> 00:38:11,599
clear what the intention of the device

1140
00:38:11,599 --> 00:38:11,920
is

1141
00:38:11,920 --> 00:38:14,400
at that time it's not listening to what

1142
00:38:14,400 --> 00:38:15,520
you're doing

1143
00:38:15,520 --> 00:38:18,000
and i think it's very hard to get

1144
00:38:18,000 --> 00:38:20,320
simplicity that's so simple

1145
00:38:20,320 --> 00:38:22,320
a child can understand that when the

1146
00:38:22,320 --> 00:38:24,160
light's red it's not listening

1147
00:38:24,160 --> 00:38:27,280
and when it's blue it is

1148
00:38:27,280 --> 00:38:29,200
let's talk about design empathy for a

1149
00:38:29,200 --> 00:38:31,119
moment so this is a screenshot from the

1150
00:38:31,119 --> 00:38:32,079
spotify app

1151
00:38:32,079 --> 00:38:36,000
from a few years ago um i was trying to

1152
00:38:36,000 --> 00:38:38,640
play some music and i i literally have

1153
00:38:38,640 --> 00:38:40,560
no idea what's going on here

1154
00:38:40,560 --> 00:38:43,280
i don't know why it's not playing music

1155
00:38:43,280 --> 00:38:45,280
it's given me no information about why

1156
00:38:45,280 --> 00:38:48,480
it can't play the song that i've chosen

1157
00:38:48,480 --> 00:38:50,880
build empathy into your designs as it

1158
00:38:50,880 --> 00:38:53,359
turned out i had a vpn turned on

1159
00:38:53,359 --> 00:38:56,480
at the time because i was traveling and

1160
00:38:56,480 --> 00:38:58,800
my vpn had disconnected from the

1161
00:38:58,800 --> 00:38:59,599
internet

1162
00:38:59,599 --> 00:39:02,000
nothing about this error screen helps me

1163
00:39:02,000 --> 00:39:03,440
as a user understand

1164
00:39:03,440 --> 00:39:05,200
why this app is not functioning the way

1165
00:39:05,200 --> 00:39:06,480
i want it to

1166
00:39:06,480 --> 00:39:09,200
provide context and design empathy with

1167
00:39:09,200 --> 00:39:10,240
your users

1168
00:39:10,240 --> 00:39:13,280
it'll build that trust with them this is

1169
00:39:13,280 --> 00:39:15,359
very frustrating but had it just

1170
00:39:15,359 --> 00:39:17,680
said can't connect to the internet

1171
00:39:17,680 --> 00:39:18,880
that's something that can

1172
00:39:18,880 --> 00:39:20,720
queue me into what's wrong and how i can

1173
00:39:20,720 --> 00:39:23,200
go about fixing it

1174
00:39:23,200 --> 00:39:26,240
next is building in helpful tool tips

1175
00:39:26,240 --> 00:39:28,560
so this is a feature that that mailchimp

1176
00:39:28,560 --> 00:39:30,400
added to their sign-in form

1177
00:39:30,400 --> 00:39:32,240
um and they found that users were happy

1178
00:39:32,240 --> 00:39:33,839
were struggling logging into the right

1179
00:39:33,839 --> 00:39:35,839
account so they put in this

1180
00:39:35,839 --> 00:39:38,640
warning message about hey we can't find

1181
00:39:38,640 --> 00:39:39,520
the account that you

1182
00:39:39,520 --> 00:39:41,119
mentioned that you're trying to log into

1183
00:39:41,119 --> 00:39:43,200
here now before all of the security

1184
00:39:43,200 --> 00:39:45,359
people say oh my god you can't do that

1185
00:39:45,359 --> 00:39:47,040
you're gonna expose account

1186
00:39:47,040 --> 00:39:48,800
information and you create an

1187
00:39:48,800 --> 00:39:51,119
enumeration mechanism to determine

1188
00:39:51,119 --> 00:39:54,000
if someone has an account or not behind

1189
00:39:54,000 --> 00:39:55,119
the scenes

1190
00:39:55,119 --> 00:39:56,880
mailchimp has implemented web

1191
00:39:56,880 --> 00:39:58,560
application firewalls

1192
00:39:58,560 --> 00:40:00,560
rate limiting and other secure

1193
00:40:00,560 --> 00:40:01,599
technologies

1194
00:40:01,599 --> 00:40:04,640
to not show this message to um things

1195
00:40:04,640 --> 00:40:04,960
like

1196
00:40:04,960 --> 00:40:07,200
bots or scrapers that are hitting this

1197
00:40:07,200 --> 00:40:08,880
page frequently

1198
00:40:08,880 --> 00:40:11,440
they've put technology protections in

1199
00:40:11,440 --> 00:40:12,319
place

1200
00:40:12,319 --> 00:40:15,040
allowing them to expose usability

1201
00:40:15,040 --> 00:40:16,079
friendly

1202
00:40:16,079 --> 00:40:18,160
options to help users get logged into

1203
00:40:18,160 --> 00:40:19,599
their accounts

1204
00:40:19,599 --> 00:40:20,880
and i think what's really cool about

1205
00:40:20,880 --> 00:40:23,359
this is they've taken a technology

1206
00:40:23,359 --> 00:40:25,520
problem and turned it into a real value

1207
00:40:25,520 --> 00:40:26,560
proposition

1208
00:40:26,560 --> 00:40:28,640
not only does this help the user get

1209
00:40:28,640 --> 00:40:30,079
access to their account

1210
00:40:30,079 --> 00:40:33,599
but it also gives mailchimp really great

1211
00:40:33,599 --> 00:40:36,640
uh telemetry data about attackers who

1212
00:40:36,640 --> 00:40:38,160
are trying to enumerate

1213
00:40:38,160 --> 00:40:40,240
accounts that's something their fraud or

1214
00:40:40,240 --> 00:40:44,240
abuse team can take action on

1215
00:40:44,240 --> 00:40:46,720
yeah so we talked about that um the next

1216
00:40:46,720 --> 00:40:47,440
piece is

1217
00:40:47,440 --> 00:40:49,680
automatically updating part of the cool

1218
00:40:49,680 --> 00:40:52,160
thing about technology products is

1219
00:40:52,160 --> 00:40:54,079
you can have them auto update while i

1220
00:40:54,079 --> 00:40:55,760
don't like google chrome

1221
00:40:55,760 --> 00:40:58,880
it does get auto update right i don't

1222
00:40:58,880 --> 00:41:00,480
have to think about chrome being up to

1223
00:41:00,480 --> 00:41:02,319
date it automatically updates in the

1224
00:41:02,319 --> 00:41:03,119
background

1225
00:41:03,119 --> 00:41:05,280
helping take some of the load about

1226
00:41:05,280 --> 00:41:07,200
updating technologies off the plate of

1227
00:41:07,200 --> 00:41:08,160
the user

1228
00:41:08,160 --> 00:41:11,119
is a way to help you build trust with

1229
00:41:11,119 --> 00:41:12,079
them

1230
00:41:12,079 --> 00:41:13,280
but you've still got to be able to

1231
00:41:13,280 --> 00:41:15,760
handle problems and errors

1232
00:41:15,760 --> 00:41:18,240
in a recent press release or a press

1233
00:41:18,240 --> 00:41:19,440
conference that

1234
00:41:19,440 --> 00:41:22,000
amazon did for some new alexa devices

1235
00:41:22,000 --> 00:41:24,319
they wanted to introduce the concept of

1236
00:41:24,319 --> 00:41:27,280
helping people understand why alexa did

1237
00:41:27,280 --> 00:41:28,560
something wrong

1238
00:41:28,560 --> 00:41:30,640
so you can actually ask your device why

1239
00:41:30,640 --> 00:41:32,800
did you do that and it'll explain

1240
00:41:32,800 --> 00:41:35,440
what happened that caused that trigger

1241
00:41:35,440 --> 00:41:37,119
many of the devices now allow you to

1242
00:41:37,119 --> 00:41:38,640
look at trigger words so you can

1243
00:41:38,640 --> 00:41:39,760
understand

1244
00:41:39,760 --> 00:41:42,960
why something has turned on or turned

1245
00:41:42,960 --> 00:41:45,040
off

1246
00:41:45,040 --> 00:41:48,079
so a fun story here is

1247
00:41:48,079 --> 00:41:50,319
when you think about failed interactions

1248
00:41:50,319 --> 00:41:51,280
in

1249
00:41:51,280 --> 00:41:55,200
human space in reality in irl

1250
00:41:55,200 --> 00:41:57,200
interactions are one-off when you go to

1251
00:41:57,200 --> 00:41:58,560
a restaurant and you have a bad

1252
00:41:58,560 --> 00:42:00,960
experience you don't apply that to the

1253
00:42:00,960 --> 00:42:02,000
entire restaurant

1254
00:42:02,000 --> 00:42:04,079
or to all restaurants in the world

1255
00:42:04,079 --> 00:42:05,680
instead you say

1256
00:42:05,680 --> 00:42:07,599
i want to talk to a manager to make this

1257
00:42:07,599 --> 00:42:09,359
right

1258
00:42:09,359 --> 00:42:11,440
with technology products you don't get

1259
00:42:11,440 --> 00:42:13,040
to talk to a manager

1260
00:42:13,040 --> 00:42:16,079
you have to put your best foot forward

1261
00:42:16,079 --> 00:42:18,319
and if you fail at that you can really

1262
00:42:18,319 --> 00:42:20,480
damage a brand as we saw earlier with

1263
00:42:20,480 --> 00:42:21,280
facebook

1264
00:42:21,280 --> 00:42:23,760
the damage has been done with people

1265
00:42:23,760 --> 00:42:24,640
trusting them

1266
00:42:24,640 --> 00:42:26,079
and people are looking to other

1267
00:42:26,079 --> 00:42:29,440
technologies other than facebook

1268
00:42:29,440 --> 00:42:31,440
so with some of these concepts this is

1269
00:42:31,440 --> 00:42:33,520
really about building security into your

1270
00:42:33,520 --> 00:42:34,800
apps it's about

1271
00:42:34,800 --> 00:42:37,280
building reliability and empathizing

1272
00:42:37,280 --> 00:42:38,640
when things go wrong

1273
00:42:38,640 --> 00:42:40,560
so make sure that your product does what

1274
00:42:40,560 --> 00:42:41,680
it promises

1275
00:42:41,680 --> 00:42:44,480
ask for consent before you collect data

1276
00:42:44,480 --> 00:42:45,520
secretly

1277
00:42:45,520 --> 00:42:48,480
if you do collect data secure the data

1278
00:42:48,480 --> 00:42:50,079
that you're entrusted with and if you

1279
00:42:50,079 --> 00:42:52,079
don't have the security investment to be

1280
00:42:52,079 --> 00:42:52,560
able to

1281
00:42:52,560 --> 00:42:55,839
protect that data don't collect it

1282
00:42:55,839 --> 00:42:59,119
make security easy set smart defaults

1283
00:42:59,119 --> 00:43:02,240
and handle problems empower users rather

1284
00:43:02,240 --> 00:43:02,960
than

1285
00:43:02,960 --> 00:43:04,480
then blame them when something goes

1286
00:43:04,480 --> 00:43:06,880
wrong help them fix problems and it'll

1287
00:43:06,880 --> 00:43:08,400
create technology that

1288
00:43:08,400 --> 00:43:10,400
your customers are more likely to trust

1289
00:43:10,400 --> 00:43:12,560
and use so let's keep looking at some

1290
00:43:12,560 --> 00:43:13,520
examples of this

1291
00:43:13,520 --> 00:43:15,760
so apple gets transparency and consent

1292
00:43:15,760 --> 00:43:16,960
really right

1293
00:43:16,960 --> 00:43:18,960
they're a privacy-minded company in fact

1294
00:43:18,960 --> 00:43:20,720
that's why i feel comfortable wearing an

1295
00:43:20,720 --> 00:43:22,319
apple watch on my wrist

1296
00:43:22,319 --> 00:43:24,160
all the time that's why i have all my

1297
00:43:24,160 --> 00:43:26,000
apple devices because it's a company i

1298
00:43:26,000 --> 00:43:29,440
trust with my data their privacy page

1299
00:43:29,440 --> 00:43:31,520
is top notch i highly recommend you go

1300
00:43:31,520 --> 00:43:32,800
to apple.com

1301
00:43:32,800 --> 00:43:34,960
privacy they talk you through how

1302
00:43:34,960 --> 00:43:36,079
they've built

1303
00:43:36,079 --> 00:43:38,000
privacy and consent into their

1304
00:43:38,000 --> 00:43:40,160
technology in fact i love this icon

1305
00:43:40,160 --> 00:43:42,800
of two people shaking hands because that

1306
00:43:42,800 --> 00:43:43,839
is really

1307
00:43:43,839 --> 00:43:46,640
what trust is about it's about

1308
00:43:46,640 --> 00:43:48,560
understanding each other and what you're

1309
00:43:48,560 --> 00:43:50,800
trying to accomplish

1310
00:43:50,800 --> 00:43:53,040
next is privacy controls if you go into

1311
00:43:53,040 --> 00:43:54,319
the settings app

1312
00:43:54,319 --> 00:43:56,640
on your twitter application you'll

1313
00:43:56,640 --> 00:43:57,440
notice there's

1314
00:43:57,440 --> 00:44:00,079
lots of different settings and what's

1315
00:44:00,079 --> 00:44:01,040
cool about this

1316
00:44:01,040 --> 00:44:02,640
and what i like about the way twitter

1317
00:44:02,640 --> 00:44:04,079
has done this is they've

1318
00:44:04,079 --> 00:44:06,880
put real context and given you control

1319
00:44:06,880 --> 00:44:08,079
about how your data

1320
00:44:08,079 --> 00:44:11,119
is used if you haven't go take a look at

1321
00:44:11,119 --> 00:44:12,880
those privacy controls and think about

1322
00:44:12,880 --> 00:44:14,000
how you can apply them

1323
00:44:14,000 --> 00:44:16,800
to your technology projects then we've

1324
00:44:16,800 --> 00:44:18,319
got the fun one we've got

1325
00:44:18,319 --> 00:44:21,599
google who on their privacy page

1326
00:44:21,599 --> 00:44:24,640
lead with the top header we do not

1327
00:44:24,640 --> 00:44:27,599
sell your personal information to anyone

1328
00:44:27,599 --> 00:44:28,160
and

1329
00:44:28,160 --> 00:44:31,119
i know technically what they're trying

1330
00:44:31,119 --> 00:44:33,119
to say here

1331
00:44:33,119 --> 00:44:35,040
but no one would agree with this

1332
00:44:35,040 --> 00:44:36,319
statement at all

1333
00:44:36,319 --> 00:44:39,440
google is absolutely using your data to

1334
00:44:39,440 --> 00:44:39,920
sell

1335
00:44:39,920 --> 00:44:43,040
ads um which while not maybe

1336
00:44:43,040 --> 00:44:45,040
exposing or selling your information to

1337
00:44:45,040 --> 00:44:46,079
third parties

1338
00:44:46,079 --> 00:44:49,280
they're providing access to your data

1339
00:44:49,280 --> 00:44:53,040
so that they can sell ads um definitely

1340
00:44:53,040 --> 00:44:55,280
be really cognizant about the messaging

1341
00:44:55,280 --> 00:44:57,680
of your technology products

1342
00:44:57,680 --> 00:44:59,359
um here's a fun app that i actually

1343
00:44:59,359 --> 00:45:01,760
really like um it's called sleep cycle

1344
00:45:01,760 --> 00:45:03,040
but what you'll notice there in the

1345
00:45:03,040 --> 00:45:05,520
middle is that link icon with the name

1346
00:45:05,520 --> 00:45:08,160
emil emil was one of my co-workers

1347
00:45:08,160 --> 00:45:09,040
previously

1348
00:45:09,040 --> 00:45:12,720
and we were at a convention in a hotel

1349
00:45:12,720 --> 00:45:16,400
and this app has the concept of a

1350
00:45:16,400 --> 00:45:19,599
partner link that broadcasts your device

1351
00:45:19,599 --> 00:45:20,240
name

1352
00:45:20,240 --> 00:45:22,240
on the network that you're on so it can

1353
00:45:22,240 --> 00:45:23,760
pair with your partner

1354
00:45:23,760 --> 00:45:26,400
and cancel out movements in your your

1355
00:45:26,400 --> 00:45:27,119
bed

1356
00:45:27,119 --> 00:45:29,040
um i do not share a bed with my

1357
00:45:29,040 --> 00:45:30,800
co-workers i was alone

1358
00:45:30,800 --> 00:45:34,079
in a hotel room using a vpn and yet this

1359
00:45:34,079 --> 00:45:37,440
um was broadcasting emel's device

1360
00:45:37,440 --> 00:45:40,560
to mine and mine to his they had since

1361
00:45:40,560 --> 00:45:44,160
added uh this feature with which was on

1362
00:45:44,160 --> 00:45:46,400
by default previously they now have

1363
00:45:46,400 --> 00:45:48,560
a modal that lets you opt into this

1364
00:45:48,560 --> 00:45:50,400
feature and explains it to you

1365
00:45:50,400 --> 00:45:52,400
but this was really terrifying to me the

1366
00:45:52,400 --> 00:45:53,760
first time

1367
00:45:53,760 --> 00:45:55,520
anybody could have used this app to

1368
00:45:55,520 --> 00:45:57,359
enumerate if i was on a network

1369
00:45:57,359 --> 00:45:58,160
somewhere

1370
00:45:58,160 --> 00:46:01,599
actively sleeping um next you've got

1371
00:46:01,599 --> 00:46:03,599
things like smart defaults um if you've

1372
00:46:03,599 --> 00:46:04,000
never

1373
00:46:04,000 --> 00:46:07,280
looked at your options in your iphone

1374
00:46:07,280 --> 00:46:10,560
um go to settings face id and passcodes

1375
00:46:10,560 --> 00:46:13,040
and take a look at how much data you're

1376
00:46:13,040 --> 00:46:15,520
allowing to be accessed when your phone

1377
00:46:15,520 --> 00:46:16,720
is locked

1378
00:46:16,720 --> 00:46:18,640
this is a view of what my settings are

1379
00:46:18,640 --> 00:46:20,400
and i encourage you to potentially

1380
00:46:20,400 --> 00:46:22,960
adopt some of these settings to reduce

1381
00:46:22,960 --> 00:46:24,560
the amount of data that you're

1382
00:46:24,560 --> 00:46:26,560
allowing someone to have access to when

1383
00:46:26,560 --> 00:46:29,200
your device is locked

1384
00:46:29,200 --> 00:46:32,800
here's an example of where with apple's

1385
00:46:32,800 --> 00:46:34,960
approach to privacy privacy is still

1386
00:46:34,960 --> 00:46:36,319
really hard to get right

1387
00:46:36,319 --> 00:46:39,760
in their new os releases they require

1388
00:46:39,760 --> 00:46:42,400
additional security permissions to give

1389
00:46:42,400 --> 00:46:43,280
applications

1390
00:46:43,280 --> 00:46:45,359
access to your data and unfortunately

1391
00:46:45,359 --> 00:46:47,040
the way this was implemented

1392
00:46:47,040 --> 00:46:49,119
creates this nightmare of pop-ups and

1393
00:46:49,119 --> 00:46:50,319
modals that

1394
00:46:50,319 --> 00:46:52,640
are really hard to understand when i

1395
00:46:52,640 --> 00:46:54,160
upgraded to this version this is an

1396
00:46:54,160 --> 00:46:56,319
actual screenshot of my system

1397
00:46:56,319 --> 00:46:59,599
and all of the um

1398
00:46:59,599 --> 00:47:01,920
privacy prompts that were brought up

1399
00:47:01,920 --> 00:47:03,839
this is a lot of work for me as a user

1400
00:47:03,839 --> 00:47:04,400
to have to

1401
00:47:04,400 --> 00:47:06,480
think through what i'm approving so

1402
00:47:06,480 --> 00:47:08,720
think about the user experience of those

1403
00:47:08,720 --> 00:47:12,640
solutions so in recap be honest and act

1404
00:47:12,640 --> 00:47:13,920
with authenticity

1405
00:47:13,920 --> 00:47:16,560
with your technology products um be

1406
00:47:16,560 --> 00:47:18,240
transparent with the data that you

1407
00:47:18,240 --> 00:47:19,680
collect and use

1408
00:47:19,680 --> 00:47:23,599
provide smart controls and consent and

1409
00:47:23,599 --> 00:47:25,760
for people to consent or object to

1410
00:47:25,760 --> 00:47:27,040
collecting data

1411
00:47:27,040 --> 00:47:29,440
handle when people opt out of providing

1412
00:47:29,440 --> 00:47:30,480
data

1413
00:47:30,480 --> 00:47:33,040
set smart privacy defaults don't auto

1414
00:47:33,040 --> 00:47:33,839
opt people

1415
00:47:33,839 --> 00:47:35,920
into new features that might have

1416
00:47:35,920 --> 00:47:37,040
privacy concerns

1417
00:47:37,040 --> 00:47:39,760
them explicitly let people know when

1418
00:47:39,760 --> 00:47:40,880
behaviors

1419
00:47:40,880 --> 00:47:43,440
or features change in a way that might

1420
00:47:43,440 --> 00:47:46,000
impact their privacy

1421
00:47:46,000 --> 00:47:48,559
and i think finally to close this out go

1422
00:47:48,559 --> 00:47:49,599
and talk to

1423
00:47:49,599 --> 00:47:52,319
actual people who use your technology

1424
00:47:52,319 --> 00:47:54,240
the things they will tell you will blow

1425
00:47:54,240 --> 00:47:55,760
your mind

1426
00:47:55,760 --> 00:47:57,920
just having a conversation about how

1427
00:47:57,920 --> 00:47:59,680
they use your technology

1428
00:47:59,680 --> 00:48:02,319
will open a brand new door of all of the

1429
00:48:02,319 --> 00:48:04,000
different things that you can

1430
00:48:04,000 --> 00:48:06,079
you should consider as you're building

1431
00:48:06,079 --> 00:48:08,319
trust into your technology products

1432
00:48:08,319 --> 00:48:11,839
um and really listen deeply

1433
00:48:11,839 --> 00:48:15,440
empathize with them ask why ask them why

1434
00:48:15,440 --> 00:48:16,319
again

1435
00:48:16,319 --> 00:48:18,800
um really get to the heart of their

1436
00:48:18,800 --> 00:48:20,160
questions and

1437
00:48:20,160 --> 00:48:21,680
the ways that they're using your

1438
00:48:21,680 --> 00:48:23,760
technologies i promise you

1439
00:48:23,760 --> 00:48:26,160
if you do five of these sessions you

1440
00:48:26,160 --> 00:48:27,760
will be shocked at the things you'll

1441
00:48:27,760 --> 00:48:28,720
learn

1442
00:48:28,720 --> 00:48:30,559
from your users and how they think about

1443
00:48:30,559 --> 00:48:33,520
and use your technology products

1444
00:48:33,520 --> 00:48:36,480
so that closes out today i think that

1445
00:48:36,480 --> 00:48:37,760
while building trust

1446
00:48:37,760 --> 00:48:40,240
isn't easy it's how you can set yourself

1447
00:48:40,240 --> 00:48:40,800
apart

1448
00:48:40,800 --> 00:48:43,520
and really differentiate your products

1449
00:48:43,520 --> 00:48:44,240
um

1450
00:48:44,240 --> 00:48:46,640
that concludes my talk today if you'd

1451
00:48:46,640 --> 00:48:48,800
like to see the slides or dig into some

1452
00:48:48,800 --> 00:48:50,240
of the examples

1453
00:48:50,240 --> 00:48:51,920
you can find the slides at that link

1454
00:48:51,920 --> 00:48:54,800
there so i think it's time for questions

1455
00:48:54,800 --> 00:48:57,119
awesome thank you very much taylor that

1456
00:48:57,119 --> 00:48:58,240
was excellent talk

1457
00:48:58,240 --> 00:49:02,640
um so i was able to collect one question

1458
00:49:02,640 --> 00:49:05,839
and uh so how accountable is distributed

1459
00:49:05,839 --> 00:49:06,400
trust

1460
00:49:06,400 --> 00:49:08,480
if that distribution makes attribution

1461
00:49:08,480 --> 00:49:10,240
potentially more difficult and i'm and

1462
00:49:10,240 --> 00:49:12,240
i'm assuming attribution of like an

1463
00:49:12,240 --> 00:49:14,480
account to a person

1464
00:49:14,480 --> 00:49:17,200
so the way that i'll talk about this is

1465
00:49:17,200 --> 00:49:19,440
that when you think about

1466
00:49:19,440 --> 00:49:21,920
um the way that rachel talked about

1467
00:49:21,920 --> 00:49:23,280
trust leaps

1468
00:49:23,280 --> 00:49:26,000
part of the problem with technology is

1469
00:49:26,000 --> 00:49:26,319
that

1470
00:49:26,319 --> 00:49:28,559
as we're adding new concepts like

1471
00:49:28,559 --> 00:49:30,800
distributed trust

1472
00:49:30,800 --> 00:49:32,720
we introduced these new types of

1473
00:49:32,720 --> 00:49:34,079
problems

1474
00:49:34,079 --> 00:49:36,720
about how you think about some of these

1475
00:49:36,720 --> 00:49:39,760
distribution and consent mechanisms

1476
00:49:39,760 --> 00:49:42,160
part of the the point about privacy is

1477
00:49:42,160 --> 00:49:44,160
that you shouldn't have enough data

1478
00:49:44,160 --> 00:49:45,839
to be able to go and make some of these

1479
00:49:45,839 --> 00:49:47,920
concerns so you've got to think about

1480
00:49:47,920 --> 00:49:50,559
in your product what is it that you can

1481
00:49:50,559 --> 00:49:52,079
do how do you collect

1482
00:49:52,079 --> 00:49:54,559
data in a way where you don't need to

1483
00:49:54,559 --> 00:49:56,800
know the specifics but that you can take

1484
00:49:56,800 --> 00:49:59,040
action on those types of things

1485
00:49:59,040 --> 00:50:01,920
we're still very new with a lot of these

1486
00:50:01,920 --> 00:50:03,599
distributed concepts

1487
00:50:03,599 --> 00:50:06,480
um strangely we're breaking some of that

1488
00:50:06,480 --> 00:50:07,200
uh down

1489
00:50:07,200 --> 00:50:09,520
in things like cryptocurrency we're

1490
00:50:09,520 --> 00:50:10,480
already seeing

1491
00:50:10,480 --> 00:50:12,559
like the u.s government have to grapple

1492
00:50:12,559 --> 00:50:13,839
with how do you do

1493
00:50:13,839 --> 00:50:15,839
taxing on a cryptocurrency that's

1494
00:50:15,839 --> 00:50:17,440
completely anonymous

1495
00:50:17,440 --> 00:50:20,640
um as you remember uh twitter recently

1496
00:50:20,640 --> 00:50:21,359
was

1497
00:50:21,359 --> 00:50:23,200
um had a security incident where people

1498
00:50:23,200 --> 00:50:24,880
were posting crypto

1499
00:50:24,880 --> 00:50:27,920
addresses on public figures even

1500
00:50:27,920 --> 00:50:30,800
put uh political figures twitter

1501
00:50:30,800 --> 00:50:32,079
accounts

1502
00:50:32,079 --> 00:50:34,319
and it's a strange thing to grapple with

1503
00:50:34,319 --> 00:50:35,920
about you've got

1504
00:50:35,920 --> 00:50:39,359
this attack framework where people have

1505
00:50:39,359 --> 00:50:41,359
complete anonymity but that you can

1506
00:50:41,359 --> 00:50:41,760
still

1507
00:50:41,760 --> 00:50:44,480
gain context and pieces of information

1508
00:50:44,480 --> 00:50:46,720
to piece that story together

1509
00:50:46,720 --> 00:50:48,240
i still don't really believe that

1510
00:50:48,240 --> 00:50:51,200
anything is private these days

1511
00:50:51,200 --> 00:50:54,400
even if you're using crypto or whatever

1512
00:50:54,400 --> 00:50:56,800
vpn like there you still leave digital

1513
00:50:56,800 --> 00:50:57,839
footprints

1514
00:50:57,839 --> 00:51:00,400
um so i think it's about thinking when

1515
00:51:00,400 --> 00:51:02,079
you're building your products

1516
00:51:02,079 --> 00:51:03,839
what data are you collecting where is

1517
00:51:03,839 --> 00:51:05,839
that data going how are you using that

1518
00:51:05,839 --> 00:51:06,800
data

1519
00:51:06,800 --> 00:51:09,359
and how can you have as little data as

1520
00:51:09,359 --> 00:51:10,240
possible

1521
00:51:10,240 --> 00:51:11,680
um the way that i think about this when

1522
00:51:11,680 --> 00:51:13,440
i talk with my technology teams

1523
00:51:13,440 --> 00:51:16,640
is if we don't absolutely

1524
00:51:16,640 --> 00:51:18,640
need the data to accomplish whatever it

1525
00:51:18,640 --> 00:51:20,079
is we're trying to do

1526
00:51:20,079 --> 00:51:22,640
don't ask for it it's one other thing we

1527
00:51:22,640 --> 00:51:24,720
have to secure and protect

1528
00:51:24,720 --> 00:51:26,960
it's more storage we have to pay for

1529
00:51:26,960 --> 00:51:27,839
it's more

1530
00:51:27,839 --> 00:51:30,000
security that we have to protect things

1531
00:51:30,000 --> 00:51:30,960
with um

1532
00:51:30,960 --> 00:51:32,800
and if it doesn't really provide value

1533
00:51:32,800 --> 00:51:34,400
for the product that we're offering

1534
00:51:34,400 --> 00:51:37,680
why collect it sure sure

1535
00:51:37,680 --> 00:51:39,280
thank you so much taylor that was an

1536
00:51:39,280 --> 00:51:40,559
excellent talk let's give a round of

1537
00:51:40,559 --> 00:51:43,359
applause virtually

1538
00:51:43,359 --> 00:51:45,520
thanks everyone feel free to tweet your

1539
00:51:45,520 --> 00:51:46,960
questions at me um

1540
00:51:46,960 --> 00:51:48,640
and again the slides are there at that

1541
00:51:48,640 --> 00:51:50,960
link i'll tweet that link as well

1542
00:51:50,960 --> 00:51:52,480
thanks for your time today and i hope

1543
00:51:52,480 --> 00:51:54,160
you enjoy the rest of your

1544
00:51:54,160 --> 00:51:57,119
diana initiative virtual conference cool

1545
00:51:57,119 --> 00:51:59,680
to you taylor

1546
00:52:01,240 --> 00:52:04,240
bye

