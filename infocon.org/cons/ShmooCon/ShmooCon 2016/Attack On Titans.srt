1
00:00:00,000 --> 00:00:06,960
during these things lamas yet existing
CISO well I think that's a problem I

2
00:00:06,960 --> 00:00:10,889
mean no better or worse no better or
worse

3
00:00:10,889 --> 00:00:18,930
squirrels in your computer gerbils sorry
gerbils and computers without any so we

4
00:00:18,930 --> 00:00:23,029
do have anymore reduce we have a
thumbs-up alright without any further

5
00:00:23,029 --> 00:00:28,080
ado I would like to introduce Andrew and
rock will be talking about Big Data

6
00:00:28,080 --> 00:00:29,369
scale

7
00:00:29,369 --> 00:00:42,120
attacking biggest they also please bring
give a warm welcome everyone thank you

8
00:00:42,120 --> 00:00:48,519
all for taking the time to come here our
presentation before we get started we

9
00:00:48,520 --> 00:00:54,180
have some sponsor donations we have some
cherries up here if you play a buzzword

10
00:00:54,180 --> 00:01:00,840
bingo are you in the right place the
10th mission mentioned big data or

11
00:01:00,840 --> 00:01:06,850
machine learning will give away both
jars of cherries so yes we encourage

12
00:01:06,850 --> 00:01:11,759
user yes

13
00:01:11,760 --> 00:01:17,590
alright so get into the presentation the
motivation behind why we did our

14
00:01:17,590 --> 00:01:23,320
research over the past semester big data
and she learned are everywhere

15
00:01:23,320 --> 00:01:27,189
their government entities such as the
environmental protection agency that now

16
00:01:27,189 --> 00:01:31,149
have a manifesto online there's like
this is how they will drive daily

17
00:01:31,150 --> 00:01:36,329
operations through the output of data
analytics you know politicians are

18
00:01:36,329 --> 00:01:43,020
looking at public sentiment online as to
how well they received in recent polls

19
00:01:43,020 --> 00:01:46,520
we had this beautiful picture here

20
00:01:46,520 --> 00:01:52,789
Deadpool on a bearskin rug because of
you people on Twitter and you know movie

21
00:01:52,790 --> 00:01:57,580
actors such as Ryan Reynolds talking hey
this is the thing to fans and Hollywood

22
00:01:57,580 --> 00:02:02,860
you know understanding that there's a
desire and a demand for this product

23
00:02:02,860 --> 00:02:06,380
gave away you know because the analytic
show that they can make a profit for

24
00:02:06,380 --> 00:02:14,480
making such a movie so with this said
what if we're in a paradigm shift where

25
00:02:14,480 --> 00:02:20,149
adversaries no longer target users they
no longer target systems but they're now

26
00:02:20,150 --> 00:02:24,340
targeting the decisions that their
victims are making what if an adversary

27
00:02:24,340 --> 00:02:29,810
can control the output data analytics
machine learning so that they can

28
00:02:29,810 --> 00:02:35,290
control their victim and that's really
what our research get set for today so

29
00:02:35,290 --> 00:02:40,410
it's a little bit like what is machine
learning and what his big deal also

30
00:02:40,410 --> 00:02:46,959
machine learning have a solution of data
or fax or observations about the world

31
00:02:46,959 --> 00:02:50,200
and what I want to do as I want to
extract some meeting from that I want to

32
00:02:50,200 --> 00:02:53,500
be able to say you know how many people
on Twitter really interested in

33
00:02:53,500 --> 00:02:58,310
expressed positive sentiment about dead
of all the photos that are taken around

34
00:02:58,310 --> 00:03:02,030
the world how many of those photos
contains some major pattern the nineteen

35
00:03:02,030 --> 00:03:06,459
if I have all this information about car
accidents are all this information about

36
00:03:06,459 --> 00:03:10,620
you know vehicles in their daily drive
how do I take that information and

37
00:03:10,620 --> 00:03:15,450
processes and produce some kind of
decision or insight into the world

38
00:03:15,450 --> 00:03:19,738
around us information so this is really
taking off as we've gotten me both

39
00:03:19,739 --> 00:03:22,950
utility to acquire this information and
the ability to process this information

40
00:03:22,950 --> 00:03:29,619
in a way that is a little unparalleled
in the history of civilization because

41
00:03:29,620 --> 00:03:33,110
previously even be constrained by how
much he could collect how much you could

42
00:03:33,110 --> 00:03:40,810
store and how much you can process so in
like for example is the history of

43
00:03:40,810 --> 00:03:45,060
integration of technology into the
US-canadian this excellent book title

44
00:03:45,060 --> 00:03:50,050
to see and it explains this army early
days of using the radar most of the data

45
00:03:50,050 --> 00:03:53,120
analytics that was done was done by a
group of sweaty man in a combat

46
00:03:53,120 --> 00:03:56,050
operations center that was what it was
originally called by the way it was very

47
00:03:56,050 --> 00:03:59,700
quickly renamed to something else think
about what that actually means for a

48
00:03:59,700 --> 00:04:07,190
little bit so once we get more data that
can be dealt with by people using grease

49
00:04:07,190 --> 00:04:12,000
pencils on a chart like well what is
this doing various things go so machine

50
00:04:12,000 --> 00:04:15,950
learning to use that like next frontier
how do we deal with in process this

51
00:04:15,950 --> 00:04:21,149
information So to quote the Prophet
Swift on security data is data doesn't

52
00:04:21,149 --> 00:04:25,799
take itself now if you really work with
Excel you'll know that there's quite a

53
00:04:25,800 --> 00:04:31,880
bit of it actually can excel Excel has
this I told I don't do professional

54
00:04:31,880 --> 00:04:35,690
Excel but people I know that have been
freshly home and has this back and go to

55
00:04:35,690 --> 00:04:41,040
HPC which is kind of nuts anyway if you
get you know multiple multiple like

56
00:04:41,040 --> 00:04:44,300
multiple petabytes of information you
want to extract some information about

57
00:04:44,300 --> 00:04:50,450
it by hand you want the machines to do
that that's sort of overall history for

58
00:04:50,450 --> 00:04:55,750
all the stuff goes yes so get into our
research

59
00:04:55,750 --> 00:05:02,020
here's our three essentials moving
forward obviously as being poorly on

60
00:05:02,020 --> 00:05:06,620
something open source and something that
is transparent as we get into it we want

61
00:05:06,620 --> 00:05:10,610
something that's going to have an impact
and also want something that people are

62
00:05:10,610 --> 00:05:15,080
familiar with you know it's pointless to
do research on something that no one

63
00:05:15,080 --> 00:05:19,539
cares about something no one's ever
heard of so these three things taken

64
00:05:19,539 --> 00:05:24,030
into consideration we move into an
actual failure that we had

65
00:05:24,030 --> 00:05:27,960
our talk is about a survey of new
attacks against big data and machine

66
00:05:27,960 --> 00:05:32,950
learning turns out there's quite a bit
of research and academia about how to

67
00:05:32,950 --> 00:05:37,960
Syria machine learning and yo dorgan
with data analytics to skew the outplay

68
00:05:37,960 --> 00:05:45,190
so to rehashing us on the previous work
done there's looking at those people

69
00:05:45,190 --> 00:05:49,390
looking at insufficient data types so
earlier and you mentioned you know we're

70
00:05:49,390 --> 00:05:52,840
concerned about you know the average
number of miles people drive per year

71
00:05:52,840 --> 00:05:56,650
and the correlation they have with you
know how many accidents they have per

72
00:05:56,650 --> 00:05:57,599
year per year

73
00:05:57,600 --> 00:06:02,220
insurance company care about these sorts
of things but as they're ingesting no

74
00:06:02,220 --> 00:06:06,960
public fields of this information does
it make sense to have a negative value

75
00:06:06,960 --> 00:06:11,260
does anyone ever drive negative miles a
year and Ferris Bueller's Day Off does

76
00:06:11,260 --> 00:06:18,330
not count did not work and so this is an
example of a lazy developer not doing

77
00:06:18,330 --> 00:06:26,210
sanity checks on the empire that they're
ingesting and it is also looking at

78
00:06:26,210 --> 00:06:31,310
normalization so let's say you know star
wars were just came out it was lots of

79
00:06:31,310 --> 00:06:34,980
public positive sentiment about it now
the best movie ever

80
00:06:34,980 --> 00:06:42,180
it's pretty good but if you're not
normalizing that employ 800 and 1000 how

81
00:06:42,180 --> 00:06:46,450
favorable someone's looking at it yet
winners can find a hundred and sixty

82
00:06:46,450 --> 00:06:51,840
characters so what if someone wants to
screw with data processing on Twitter

83
00:06:51,840 --> 00:06:59,770
feeds and build a spam tweet of bad
horrible other negative superlatives you

84
00:06:59,770 --> 00:07:03,099
know how they felt about that they are
now able to offset the positive

85
00:07:03,100 --> 00:07:10,220
sentiment twenty-plus people by crafting
this militias tweet within 10 working

86
00:07:10,220 --> 00:07:15,200
with and weddings so as I mentioned
earlier that a lot of this has been done

87
00:07:15,200 --> 00:07:18,860
and we found it you know we were
failures and that we can find anything

88
00:07:18,860 --> 00:07:26,920
new with influence seen the output data
analytics but we did find was a exploit

89
00:07:26,920 --> 00:07:32,600
that works against Hindu that harkens
back to the late nineties and they

90
00:07:32,600 --> 00:07:38,740
called him what their halfback so data X
pills possible with Hadoop clusters we

91
00:07:38,740 --> 00:07:47,890
found using URL encoding abuse and
directory traversal yeah I did this is

92
00:07:47,890 --> 00:07:53,440
you know what years this and this is
still possible within the relation

93
00:07:53,440 --> 00:07:58,370
between HDFS Hadoop distributed file
system client and the remote cluster

94
00:07:58,370 --> 00:08:03,700
that they're uploading all this data to
so you have this you know URL encoded

95
00:08:03,700 --> 00:08:08,940
string on the puter and when this
decoded

96
00:08:08,940 --> 00:08:15,370
actually allows direct reversals either
absolute or relative file pass and

97
00:08:15,370 --> 00:08:22,110
because there's no standardization or
tokenization on the command is given at

98
00:08:22,110 --> 00:08:28,670
the command line it decodes this URL and
starts to look for it does this file

99
00:08:28,670 --> 00:08:34,210
exists it doesn't treat it as
information as a part of the command and

100
00:08:34,210 --> 00:08:38,980
because it's possible you can have
explored sensitive data or restricted

101
00:08:38,980 --> 00:08:44,450
data by an insider threat and we
reported this we try to do a responsible

102
00:08:44,450 --> 00:08:49,710
disclosure and there was little interest
because of the narrow attack surface is

103
00:08:49,710 --> 00:08:53,440
possible through but you know I can't
think of any reason why someone would be

104
00:08:53,440 --> 00:08:59,370
concerned with insider threats this day
in age so here's one that's not really

105
00:08:59,370 --> 00:09:08,460
part of our talk but it's a freebie yeah
you're welcome so I so another example

106
00:09:08,460 --> 00:09:13,310
that so so I do as I sit around and I
think the scary things and how bad the

107
00:09:13,310 --> 00:09:15,479
world would be just a

108
00:09:15,480 --> 00:09:18,670
so here's one thought experiment had
let's say that you know you sit down and

109
00:09:18,670 --> 00:09:25,329
you're concerned about finding let's say
you want to make the the world where is

110
00:09:25,329 --> 00:09:30,279
you want to make a real-life queries
Carmen Sandiego infrastructure on the

111
00:09:30,279 --> 00:09:34,870
internet so you say well okay this
actually like nowadays is kind of

112
00:09:34,870 --> 00:09:40,470
interesting what we can do is all set up
a bunch of ec2 instances in an s3 bucket

113
00:09:40,470 --> 00:09:44,910
and what these instances will do is go
out together images of the real world

114
00:09:44,910 --> 00:09:48,649
that were taken with some indication of
where and when those images were taken

115
00:09:48,649 --> 00:09:52,529
in just dump them into s3 and I'm gonna
run computer vision algorithms and all

116
00:09:52,529 --> 00:09:55,500
of those images in real-time and try to
figure out you know what I can figure

117
00:09:55,500 --> 00:10:01,579
out you know was where when I'm in a
stump information to another s3 bucket

118
00:10:01,579 --> 00:10:07,000
and this can just sit in the background
of the internet running forever and as

119
00:10:07,000 --> 00:10:10,949
it uses information you can give it to
me so that sounds interesting and you

120
00:10:10,949 --> 00:10:14,979
think wow the pieces are sort of sitting
around and you know I could use them but

121
00:10:14,980 --> 00:10:18,269
it turns out that because we write all
of these algorithms and all of the

122
00:10:18,269 --> 00:10:23,069
system software on top of C and C++ it
sort of columns pile of bricks and so as

123
00:10:23,069 --> 00:10:27,349
soon as someone figures out that this is
the thing that you're doing they start

124
00:10:27,350 --> 00:10:31,930
to attack maybe maybe maybe Carmen
Sandiego is a computer hacker doesn't

125
00:10:31,930 --> 00:10:35,829
want to be found so she figures out oh
there's this system it's pulling all

126
00:10:35,829 --> 00:10:39,959
these images I can add images into that
system too and I can get out this

127
00:10:39,959 --> 00:10:43,800
particular infrastructure and start to
corrupt his decision-making process one

128
00:10:43,800 --> 00:10:48,359
way or the other I can either tell the
system that I'm somewhere where I'm not

129
00:10:48,360 --> 00:10:54,500
a trick play at a high level on the
image recognition algorithms or let's

130
00:10:54,500 --> 00:10:58,550
say the image recognition library
contains a low-level vulnerability in

131
00:10:58,550 --> 00:11:04,199
the way of Parsons PNG is I could get a
show on this bus there and then the

132
00:11:04,199 --> 00:11:07,639
system with sweet sweet lives of any
nature that I want or shut it down

133
00:11:07,639 --> 00:11:09,710
altogether

134
00:11:09,710 --> 00:11:15,760
so another excellent concepts enabled by
this future is the concept first post by

135
00:11:15,760 --> 00:11:19,310
Neal Stephenson in the book is your
history which is the idea of the ugly

136
00:11:19,310 --> 00:11:25,160
t-shirt so was posited in this book has
a compromise between entities the

137
00:11:25,160 --> 00:11:29,130
created a sort of surveillance system
based on video data for anyone who are

138
00:11:29,130 --> 00:11:32,120
the only teacher was sort of erased from
existence as far as a surveillance

139
00:11:32,120 --> 00:11:37,290
system was concerned but in a world
where all of the features in this system

140
00:11:37,290 --> 00:11:41,760
are built on top of such systems written
in C and C++ if you're lucky you

141
00:11:41,760 --> 00:11:48,000
identify components of a Nike t-shirt
just by studying some code and that's a

142
00:11:48,000 --> 00:11:54,940
little beer right so we did was we
looked at this platform this computer

143
00:11:54,940 --> 00:11:58,870
vision and CD which is an open source
system you can just download the Qur'an

144
00:11:58,870 --> 00:12:03,240
get up and we were like ok so this is
this is pretty red pretty mature and

145
00:12:03,240 --> 00:12:06,670
also they're all these examples where we
can just start from an input image or

146
00:12:06,670 --> 00:12:12,120
use a movie image and the weekend we get
back some results so if someone was to

147
00:12:12,120 --> 00:12:16,560
sit down and use this like if I wanted
to make my hair is Carmen Sandiego app

148
00:12:16,560 --> 00:12:20,589
which I'm pretty sure I can do over
weekend I would just take this news this

149
00:12:20,590 --> 00:12:24,880
wholesale maybe I would apply some
standardization but probably breaking

150
00:12:24,880 --> 00:12:33,550
things so we decided all right we'll
create a fairly straightforward be back

151
00:12:33,550 --> 00:12:37,719
to where we have a young one and we have
a father on the other end so other users

152
00:12:37,720 --> 00:12:42,850
AFL be compiled and seizing its
instrumentation and we're looking for

153
00:12:42,850 --> 00:12:47,050
some we defined as import your images
that we can feed to the system output

154
00:12:47,050 --> 00:12:50,400
you want a few different sort of more
nuanced things then

155
00:12:50,400 --> 00:12:54,040
what others normally 14 so in addition
to identifying hey is there a way that I

156
00:12:54,040 --> 00:12:58,719
can create a crash with an image you
also kind of care about in the scenario

157
00:12:58,720 --> 00:13:03,910
to hang with an input image so can I get
the system's stop processing and image

158
00:13:03,910 --> 00:13:08,310
and maybe take down the entire pipeline
system is poorly constructed which a

159
00:13:08,310 --> 00:13:17,300
yellow probably is or can I provide some
kind of denial of service memory or can

160
00:13:17,300 --> 00:13:22,740
I make it so that in order to recognize
that features inside of the image needs

161
00:13:22,740 --> 00:13:25,930
to run for longer than the system will
let it thereby achieving sort of the

162
00:13:25,930 --> 00:13:28,250
same effect of concealing information

163
00:13:28,250 --> 00:13:35,470
nations so in this instance normally
gangs are not really valuable when

164
00:13:35,470 --> 00:13:38,650
you're performing for campaigns but in
this instance it sort of it sort of is

165
00:13:38,650 --> 00:13:43,959
these hangs represent algorithmic
complexity attacks where you can so so

166
00:13:43,960 --> 00:13:46,950
over the complexity tax and the things
that happen start to look for when

167
00:13:46,950 --> 00:13:50,770
everything gets compiled a CFI they
don't want to admit their friends they

168
00:13:50,770 --> 00:13:51,130
do

169
00:13:51,130 --> 00:13:55,650
telephone access so we say ok we're
going to find a way to shut the system

170
00:13:55,650 --> 00:13:58,930
down by providing it with
denial-of-service input data and then

171
00:13:58,930 --> 00:14:00,839
the entire system launched

172
00:14:00,840 --> 00:14:05,220
turned off and everything is good and
Carmen Sandiego

173
00:14:05,220 --> 00:14:12,449
so in the course of our experimentation
we ran this buzzing system for about two

174
00:14:12,449 --> 00:14:13,008
weeks

175
00:14:13,009 --> 00:14:19,660
other time now the other kind of buzzing
with AFL this ten billion numbers

176
00:14:19,660 --> 00:14:25,930
laughably low and that's because the
computer is taking a long time to run

177
00:14:25,930 --> 00:14:30,750
and it's frustrating because as far as I
can tell there is no shortcut through

178
00:14:30,750 --> 00:14:35,490
this I you know they're going to run for
a long time and then she did not finish

179
00:14:35,490 --> 00:14:42,579
so ok solution to this like the solution
to most problems in my life of personal

180
00:14:42,579 --> 00:14:47,469
professionals heroic computing power and
work or so we don't have that perhaps in

181
00:14:47,470 --> 00:14:48,199
the future

182
00:14:48,199 --> 00:14:52,878
well but from this we identified about
3,000 crashes inside of a CD that will

183
00:14:52,879 --> 00:14:57,129
cost down to about 45 different bugs
that all had some different

184
00:14:57,129 --> 00:15:01,550
manifestations are different layers of
the OpenCV stacks on image processing

185
00:15:01,550 --> 00:15:07,930
some matrix multiplication some in
actual algorithms themselves with the

186
00:15:07,930 --> 00:15:15,609
allergen had failed to terminate so
denial of service conditions were so

187
00:15:15,610 --> 00:15:20,860
these were mostly images that would
cause the program or run out of memory

188
00:15:20,860 --> 00:15:27,610
these have been reported through the
normal process in which things are done

189
00:15:27,610 --> 00:15:30,460
so

190
00:15:30,460 --> 00:15:38,700
traveling stuff like heap corruption and
use after trees inside inside if you can

191
00:15:38,700 --> 00:15:41,830
show some of these images are sketches
of these images in a minute because it's

192
00:15:41,830 --> 00:15:45,600
pretty freaky when you start to get the
output of stuff that just use computer

193
00:15:45,600 --> 00:15:53,200
vision systems so this is dangerous
because sort of as I said before these

194
00:15:53,200 --> 00:15:58,440
systems are doing reasoning and they're
doing it like all under the covers so it

195
00:15:58,440 --> 00:15:59,130
comes up to you

196
00:15:59,130 --> 00:16:02,070
comes back to you say here's ten million
images I can't look through them all at

197
00:16:02,070 --> 00:16:06,770
once you do it you tell me what happened
when someone gets cut execution inside

198
00:16:06,770 --> 00:16:10,920
of this it's like the house you know
sort of squishy biological analog head

199
00:16:10,920 --> 00:16:15,120
coach execution inside of your plane so
now after you look at some picture this

200
00:16:15,120 --> 00:16:19,190
morning around his side of your head
that saying like Carmen Sandiego just

201
00:16:19,190 --> 00:16:23,970
doesn't exist anymore like that concept
has gone away and it doesn't just happen

202
00:16:23,970 --> 00:16:26,540
to know the results that you get back
from this decision making process

203
00:16:26,540 --> 00:16:30,170
occurring in the computer vision system
or just totally bogus because of these

204
00:16:30,170 --> 00:16:35,569
yeah so the real implication of this is
that you know and adversary no longer

205
00:16:35,570 --> 00:16:40,750
has to implant something they no longer
have to actively do anything simply by

206
00:16:40,750 --> 00:16:45,710
uploading pictures damn juror Flickr
Facebook whatever that people are

207
00:16:45,710 --> 00:16:51,640
downloading and getting popped like this
we uploaded all of the images that cause

208
00:16:51,640 --> 00:16:56,110
crashes not a single one of them were
triggered in any AV software solution

209
00:16:56,110 --> 00:16:56,860
and why should they

210
00:16:56,860 --> 00:17:04,620
right i mean the pictures better yeah so
this is an entirely new paradigm shift

211
00:17:04,619 --> 00:17:12,909
for attackers and their efforts to
effects victims so

212
00:17:12,910 --> 00:17:16,680
another great thing futures products is
this is this fashion designer who

213
00:17:16,680 --> 00:17:20,540
independently from us had this same idea
and they looked at the world and they're

214
00:17:20,540 --> 00:17:25,109
like well there are these things out
there that I tend to identify people

215
00:17:25,109 --> 00:17:30,090
shapes and I'm going to make some
statement by providing clothing they

216
00:17:30,090 --> 00:17:37,810
will mess up those things ability to
identify people so because

217
00:17:37,810 --> 00:17:45,500
about people to camouflage historically
has been about you know we as people

218
00:17:45,500 --> 00:17:52,410
have developed over the course of the
development of our our however you

219
00:17:52,410 --> 00:17:55,890
believe the development of our vision
system and biological system and our all

220
00:17:55,890 --> 00:17:59,170
that happened was developed to produce
something that was good at identifying

221
00:17:59,170 --> 00:18:04,090
shapes so you know if the brain that
you're carrying around between your ears

222
00:18:04,090 --> 00:18:10,439
is incapable of telling us capable of
telling the difference between a tiger

223
00:18:10,440 --> 00:18:13,820
in a tree because two thousand years ago

224
00:18:13,820 --> 00:18:21,110
photo you get back and the other guy did
so we had a long time to create vision

225
00:18:21,110 --> 00:18:24,719
systems that we used to identify things
in camouflage our attempt to like

226
00:18:24,720 --> 00:18:30,060
message computer vision is much less
mature than people vision so the ways in

227
00:18:30,060 --> 00:18:34,629
which messes up are also less mature
than the way our vision messes up some

228
00:18:34,630 --> 00:18:38,210
of this is also related to this is
interesting paper that was published by

229
00:18:38,210 --> 00:18:42,980
the way you titled The Curious property
of neural networks where my hand they

230
00:18:42,980 --> 00:18:46,990
discovered that they can take a neural
network based classifier that looks and

231
00:18:46,990 --> 00:18:49,240
image sort of look at that

232
00:18:49,240 --> 00:18:53,580
inverse figure out what pixels are
relevant a classification decision and

233
00:18:53,580 --> 00:18:58,439
then she changed those pixels at a very
low level such that be corrupted image

234
00:18:58,440 --> 00:19:03,380
looks indistinguishable to be original
image the human eye but the computers I

235
00:19:03,380 --> 00:19:07,030
is the difference between trying
something is a cat and a bus

236
00:19:07,030 --> 00:19:15,629
so do the same thing but automating way
using fuzzy instead of bike and

237
00:19:15,630 --> 00:19:19,230
inspected the images and so the
implication here is you know if your of

238
00:19:19,230 --> 00:19:24,040
the paranoid type and you don't want
things like OpenCV recognizing your face

239
00:19:24,040 --> 00:19:28,860
you're able to use fathers to add slight
permutations to your profile pictures if

240
00:19:28,860 --> 00:19:36,439
you have on your face and your now
effectively drum proof as I said before

241
00:19:36,440 --> 00:19:43,220
this relates to this tax so we can
automatically identify where we try to

242
00:19:43,220 --> 00:19:51,020
automatically identify those automatic
testing so one way in which we can do

243
00:19:51,020 --> 00:19:56,200
that or we thought about terrified about
doing that is by combining like buzzing

244
00:19:56,200 --> 00:20:01,800
as dynamic analysis and in very
generation as static analysis of someone

245
00:20:01,800 --> 00:20:06,490
comes to you and they're like hey I've
tried every input on this program and

246
00:20:06,490 --> 00:20:10,490
didn't do anything bad and you're like
oh that probably means that program is

247
00:20:10,490 --> 00:20:16,090
good right now what do you do if they
use a notion of code coverage when they

248
00:20:16,090 --> 00:20:19,510
decide which includes are in place they
want to pursue which includes or what

249
00:20:19,510 --> 00:20:30,080
they wanted to start so the fathers want
new code be uncovered during application

250
00:20:30,080 --> 00:20:35,439
so that's the metric that the use of
this needs to change summit you're

251
00:20:35,440 --> 00:20:39,170
looking for a complex attacks one way in
which it should change if you stick with

252
00:20:39,170 --> 00:20:43,380
the code coverage metric you need to
tell the system it's not enough to

253
00:20:43,380 --> 00:20:45,130
simply look at

254
00:20:45,130 --> 00:20:51,980
process new code you also need to have
more prioritize BB execution of code so

255
00:20:51,980 --> 00:20:53,970
when you do that

256
00:20:53,970 --> 00:20:58,400
evolutionary fuzzy systems will
automatically discover these locations

257
00:20:58,400 --> 00:21:02,870
in the program where there are problems
but there are other things you can do to

258
00:21:02,870 --> 00:21:03,899
so

259
00:21:03,900 --> 00:21:08,010
with an invariant generator you can say
for particular locations inside of the

260
00:21:08,010 --> 00:21:13,220
program one of the possible ranges that
a value can take on and then you can

261
00:21:13,220 --> 00:21:17,440
start to computer media coverage that I
haven't said if the application and

262
00:21:17,440 --> 00:21:22,070
here's the value of having this
application and we take the product of

263
00:21:22,070 --> 00:21:27,320
those to you wind up with a horrible
thing but it isn't being that starts to

264
00:21:27,320 --> 00:21:31,770
tell you okay if there are large values
those are probably those large values

265
00:21:31,770 --> 00:21:35,160
you better writers and probably want to
try to get the lead generator up to the

266
00:21:35,160 --> 00:21:39,270
maximum amount that I was going to
trigger an algorithmic complexity attack

267
00:21:39,270 --> 00:21:43,660
if there's something that uses size and
a memory allocator then you probably

268
00:21:43,660 --> 00:21:47,640
want that value to be back soon and
you're in very generator can tell you

269
00:21:47,640 --> 00:21:53,590
what those maximum values are also with
this there's a literal costs associated

270
00:21:53,590 --> 00:21:58,010
with this as more and more people are
moving to the cloud to doing in a

271
00:21:58,010 --> 00:21:59,430
distributed manner

272
00:21:59,430 --> 00:22:05,560
able to induce out of the complex
attacks now imposes financial costs on

273
00:22:05,560 --> 00:22:13,590
your victim as well very high usage of
CPUs and you know if you're doing any of

274
00:22:13,590 --> 00:22:21,959
this and other sort you know have you
heard in their pocketbook as well so do

275
00:22:21,960 --> 00:22:23,520
about some of these things

276
00:22:23,520 --> 00:22:29,810
responsibility is it do something about
these things so in the case of like a

277
00:22:29,810 --> 00:22:33,500
library the process images someone comes
to you and they said hey this library

278
00:22:33,500 --> 00:22:34,940
could do better

279
00:22:34,940 --> 00:22:43,910
what changed so many areas that we found
really areas but is that we found using

280
00:22:43,910 --> 00:22:50,180
an open CDs it's possible that system
would have done better if reprocessing

281
00:22:50,180 --> 00:22:54,010
past runs that normalizes values

282
00:22:54,010 --> 00:22:58,800
of pixels in the image so what do you do
you have a lot of rapper on this library

283
00:22:58,800 --> 00:23:05,570
that says aspiration library I'm gonna
see if it errors and if it does I'm

284
00:23:05,570 --> 00:23:10,770
going to run an organization last try
again I'm gonna try with increasing

285
00:23:10,770 --> 00:23:16,520
coefficients until eventually either I
give up or at the library succeeds but

286
00:23:16,520 --> 00:23:21,340
like whose responsibility is bad and
some cases this is important like if

287
00:23:21,340 --> 00:23:25,760
you're going through a bunch of like you
mentioned in one context it's very

288
00:23:25,760 --> 00:23:29,280
important for you to conclusively
attempt to succeed or fail in every case

289
00:23:29,280 --> 00:23:34,610
but in other contacts it's ok if you
drop like five percent of the nearly

290
00:23:34,610 --> 00:23:40,250
process on the floor and the difference
could be like level of assurance that is

291
00:23:40,250 --> 00:23:45,730
expected in both of those contacts so in
some ways these types of areas and data

292
00:23:45,730 --> 00:23:50,960
like Indian big data processing the
reason really like a one size fits all

293
00:23:50,960 --> 00:23:54,580
solution the late areas and some aspects
of application security where you say no

294
00:23:54,580 --> 00:23:59,389
application she just always do acts
always safe and we'll never negatively

295
00:23:59,390 --> 00:24:05,330
impact anything wrong with your lives by

296
00:24:05,330 --> 00:24:12,330
that doesn't always work when you're
dealing with something as context

297
00:24:12,330 --> 00:24:19,610
specific users data processing as so as
intervention there are defenses for this

298
00:24:19,610 --> 00:24:26,409
and that defenses going back to the
fundamentals you're ingesting enormous

299
00:24:26,410 --> 00:24:31,700
amounts of public data you don't know
the people associated with other sources

300
00:24:31,700 --> 00:24:37,200
associated with us dana why would you
trust it in your system so an instant

301
00:24:37,200 --> 00:24:45,410
they do you know if they just treated it
as a string not gonna do this once and

302
00:24:45,410 --> 00:24:52,390
we're looking at for you why would you
allow that to be you know why would you

303
00:24:52,390 --> 00:24:57,730
search elsewhere for this known string
that should be here locally and if you

304
00:24:57,730 --> 00:25:01,350
treated it only has information that
exploit and Data Expo which it never

305
00:25:01,350 --> 00:25:02,659
happens

306
00:25:02,660 --> 00:25:08,410
using these couple lines of Python code
we're able to take those three thousand

307
00:25:08,410 --> 00:25:13,460
you need crashes that we're able to
induce with OpenCV and take it down 27

308
00:25:13,460 --> 00:25:19,490
images so there's a tradeoff alright so
you're doing all of this preprocessing

309
00:25:19,490 --> 00:25:25,600
on the images you're ensuring the
corpora as we learned yesterday that the

310
00:25:25,600 --> 00:25:31,169
corpus of images that are using is safe
but you're you're also doing this on

311
00:25:31,170 --> 00:25:36,340
millions of pictures are lots of lines
of code but you want your answer as soon

312
00:25:36,340 --> 00:25:41,629
as possible so would you rather do it
quickly or safely and yeah that's sort

313
00:25:41,630 --> 00:25:45,050
of the tradeoff that people need to
understand they're making when they make

314
00:25:45,050 --> 00:25:50,879
these decisions so it's kind of time for
this year some stats and show-and-tell

315
00:25:50,880 --> 00:25:57,070
time so we're not talk about some of the
show you real quick this is a subset of

316
00:25:57,070 --> 00:26:01,980
images that induce crashes with in
OpenCV if they are ingested you download

317
00:26:01,980 --> 00:26:02,640
Easy

318
00:26:02,640 --> 00:26:05,960
and running through OpenCV in a crash
your computer and various interesting

319
00:26:05,960 --> 00:26:13,210
ways so going through some of the very
and durability and what's wrong with

320
00:26:13,210 --> 00:26:18,930
them each one of these induce a
different crash and as the previous

321
00:26:18,930 --> 00:26:29,300
slide mentioned there's over 3,000 of
them going through all of these render

322
00:26:29,300 --> 00:26:30,990
and Google photos just fine

323
00:26:30,990 --> 00:26:39,690
OSX rendered just fine nothing fails we
are looking at these but very hilarious

324
00:26:39,690 --> 00:26:48,370
know are the very normal looking images
that still induce crashes with in OpenCV

325
00:26:48,370 --> 00:26:57,389
the top one being a picture of Justin
Bieber was found alive in his apartment

326
00:26:57,389 --> 00:27:05,918
barely when you process that OpenCV is
equally despaired so there is a great

327
00:27:05,919 --> 00:27:13,200
quote by George Carlin here that such a
computer down and David Beckham you know

328
00:27:13,200 --> 00:27:23,739
it's a handsome man that can break a
computer vision so yeah with that that

329
00:27:23,739 --> 00:27:28,119
really concludes our talk and no one
claimed the cherries animal sad about

330
00:27:28,119 --> 00:27:39,259
that but I know personally I whose
Hersman account 25 who was the first

331
00:27:39,259 --> 00:27:46,950
person account 250 gonna come on please
come get the cherries so yeah we're

332
00:27:46,950 --> 00:28:05,299
available for questions if there are any

333
00:28:05,299 --> 00:28:21,739
the computer vision alturas so the
majority of the many image processing I

334
00:28:21,739 --> 00:28:29,590
know this this code and OpenCV has its
own image processing library

335
00:28:29,590 --> 00:28:33,709
alright so I didn't know this before I
started but OpenCV has its own image

336
00:28:33,710 --> 00:28:47,389
processing library it's not how is this
helping so awesome

337
00:28:47,389 --> 00:28:53,070
the question was at what level of the
OpenCV stack with the crash is present

338
00:28:53,070 --> 00:28:58,049
in OpenCV has its own image processing
library I didn't know that that's a

339
00:28:58,049 --> 00:29:04,429
little scary so many of them are that
layer some of them are at like so so the

340
00:29:04,429 --> 00:29:07,080
what the ones they are at a higher
larry's the denial of service conditions

341
00:29:07,080 --> 00:29:11,879
or something where it'll run forever
forever oral pretty slick and an Oracle

342
00:29:11,879 --> 00:29:15,289
incorrectly priests and I'll try to
divide by zero by finance or something

343
00:29:15,289 --> 00:29:23,269
so those are probably just like stopped
processing in image but those are that's

344
00:29:23,269 --> 00:29:43,700
the type of stuff to be found in that
piece of software

345
00:29:43,700 --> 00:29:52,639
I'm working on that like so so thats
yeah yeah so cuuute want to take you so

346
00:29:52,639 --> 00:29:55,219
you see always have the old image then
you have the new image that produces the

347
00:29:55,220 --> 00:29:58,399
thing and then you have a diff between
them and then you try to back out from

348
00:29:58,399 --> 00:30:04,408
that death through the trees in the code
like where where the change occurs yeah

349
00:30:04,409 --> 00:30:09,570
I'm working on that but I haven't
finished that so at some point it would

350
00:30:09,570 --> 00:30:13,490
be nice if there was just like oh so so
it's like if that if they like you

351
00:30:13,490 --> 00:30:17,039
should you should be able to just have a
script that takes the oath that applies

352
00:30:17,039 --> 00:30:25,279
this transformation and nothing of note
is that we're able to boil it down in

353
00:30:25,279 --> 00:30:30,220
the most severe bugs to adjust couple
bites as I was not his couple megabyte

354
00:30:30,220 --> 00:30:34,309
image just a couple bites in a file and
you're able to push that through and she

355
00:30:34,309 --> 00:30:38,899
has the same effect now those those
those are the the couple by to the

356
00:30:38,899 --> 00:30:44,820
ludicrous 2001 era bugs in the image
processing code the stuff that involves

357
00:30:44,820 --> 00:30:48,639
anything at the higher level there has
to actually be an image with a bunch of

358
00:30:48,639 --> 00:31:00,149
data the algorithm to chew on and it's
also like over the past quarter project

359
00:31:00,149 --> 00:31:04,860
zero release is your days in the Samsung
phones like you just having an image on

360
00:31:04,860 --> 00:31:12,289
your phone could result in a remote code
execution over the holiday break the lib

361
00:31:12,289 --> 00:31:18,100
PNG had some very severe holes in it
that would also allow remote code

362
00:31:18,100 --> 00:31:23,010
execution and this was all happening as
we work including our research over the

363
00:31:23,010 --> 00:31:26,690
semester sorry I was very interesting to
see a lot of people finding new things

364
00:31:26,690 --> 00:31:36,519
simultaneously

365
00:31:36,519 --> 00:31:40,919
so the question why does it matter how
they image gets bad and the answer's no

366
00:31:40,919 --> 00:31:46,429
as long as the checksum well I mean so
there are like that there are so I've

367
00:31:46,429 --> 00:31:50,909
been told oh you know you should have
been able to do that because that image

368
00:31:50,909 --> 00:31:53,849
processing service should do some degree
of post-processing or like a

369
00:31:53,849 --> 00:31:58,689
transformation and then it would reject
any we reject the image that I like you

370
00:31:58,690 --> 00:32:02,539
see Google photo just like take the
imaging be up here you go

371
00:32:02,539 --> 00:32:07,559
but you could so so we had this five
lines of Python that I tried like try

372
00:32:07,559 --> 00:32:11,259
try to make sure that the images some
images are within normal bounds before

373
00:32:11,259 --> 00:32:17,129
they get rendered and if they tried to
do that on their end then they would

374
00:32:17,129 --> 00:32:21,029
reject stuff but sometimes people don't
seem to do that because I mean they

375
00:32:21,029 --> 00:32:23,820
would they would have to incur this
extra costs on there and she would ask

376
00:32:23,820 --> 00:32:27,899
them oh ya know if you received 2
million images a day like add an extra

377
00:32:27,899 --> 00:32:34,228
two million instructions on to each
image that you want to bring in and they

378
00:32:34,229 --> 00:32:37,509
would say yeah I know we don't have to
have the computer that we don't want to

379
00:32:37,509 --> 00:32:42,779
pay for the computer that so there is a
tradeoff in the checks that you could

380
00:32:42,779 --> 00:32:47,839
run on individual image imports before
your next question we have a more jar of

381
00:32:47,839 --> 00:33:06,879
cherries right place

382
00:33:06,880 --> 00:33:11,200
so the question is can we Chinese
digital to analog and also induce a

383
00:33:11,200 --> 00:33:20,100
crash I'm not gonna roll it out because
there is a priority of this being done

384
00:33:20,100 --> 00:33:26,389
with audio but we haven't done that but
that would be another great thing to try

385
00:33:26,390 --> 00:33:32,720
so what i mean that's so then yes i mean
theoretically anything is possible when

386
00:33:32,720 --> 00:33:55,760
you have the C code yeah yeah yeah yeah

387
00:33:55,760 --> 00:34:02,760
that I can no longer safe places this is
life like this is this is live on photos

388
00:34:02,760 --> 00:34:09,399
yes so i mean any any one of these
downloaded as is gonna crash your system

389
00:34:09,399 --> 00:34:18,489
yeah and some of them could be gifted so
yeah so of interesting note Flickr does

390
00:34:18,489 --> 00:34:24,279
have pretty good pre and post processing
and they have the same conclusion as our

391
00:34:24,280 --> 00:34:29,550
python script so we uploaded all three
thousand images to Flickr and it kicked

392
00:34:29,550 --> 00:34:42,580
out all the ones I thought script would
have filtered out

393
00:34:42,580 --> 00:35:01,220
damages

394
00:35:01,220 --> 00:35:31,660
out yet whether this specific subset
what's going on here that's unique yeah

395
00:35:31,660 --> 00:35:36,538
question yeah I mean obviously didn't
try that can have an immediate answer

396
00:35:36,539 --> 00:35:44,539
but that's a good experiment Iran yeah
so sorry the question wise is crashing

397
00:35:44,539 --> 00:35:50,530
property these images preserved if you
convert them to JPG and then back yes

398
00:35:50,530 --> 00:36:26,299
yes these images and the answer is we
don't know we could find out

399
00:36:26,299 --> 00:36:36,140
examples of algorithmic complexity
vulnerabilities and these are because

400
00:36:36,140 --> 00:36:42,848
these are not like the format level so
when like to try to identify like a face

401
00:36:42,849 --> 00:36:47,729
and it'll run on bees and as far as it
tries to find a face in these images it

402
00:36:47,729 --> 00:36:55,279
will produce a man and then it just
stops so it says oh I divided by 0 are

403
00:36:55,279 --> 00:37:00,390
produced some other non represented
value and my processing of this image is

404
00:37:00,390 --> 00:37:07,109
over

405
00:37:07,109 --> 00:37:15,390
say I personally like to thank everyone
that came this is why I came to the

406
00:37:15,390 --> 00:37:23,118
original line as a plebeian underbelly
of life and it's an absolute honour to

407
00:37:23,119 --> 00:37:25,779
be on stage right here in front of you
so thanks everyone for coming

