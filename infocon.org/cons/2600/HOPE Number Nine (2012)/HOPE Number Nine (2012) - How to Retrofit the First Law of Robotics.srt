1
00:00:00,080 --> 00:00:03,149
ally free and open source software and

2
00:00:03,149 --> 00:00:07,370
the head of the Software Freedom Law
Center he gave a talk at NYU couple

3
00:00:07,370 --> 00:00:08,109
years back

4
00:00:08,109 --> 00:00:11,250
which was and the sort of spark for the
beginning

5
00:00:11,250 --> 00:00:15,809
the freedom box which is server
distributors that observers are they can

6
00:00:15,809 --> 00:00:16,609
run their house

7
00:00:16,609 --> 00:00:20,000
and inspired the folks who are developed
I asked for a

8
00:00:20,000 --> 00:00:24,150
the open and shared alternative
distributed alternate to

9
00:00:24,150 --> 00:00:27,420
Facebook so it is my pleasure to
introduce

10
00:00:27,420 --> 00:00:34,420
doctor ed mcmahon given up

11
00:00:39,100 --> 00:00:40,260
thank you it's a

12
00:00:40,260 --> 00:00:44,349
a great pleasure to be here i am. of

13
00:00:44,350 --> 00:00:47,570
I i apologize. for the

14
00:00:47,570 --> 00:00:51,100
preview in Forbes online %uh

15
00:00:51,100 --> 00:00:54,899
and the resulting Slashdot conversation
about whether I understand that

16
00:00:54,899 --> 00:00:58,109
first love robotics or not which was
very entertaining

17
00:00:58,109 --> 00:01:01,469
to me are abut let me try

18
00:01:01,469 --> 00:01:04,489
to start from scratch while

19
00:01:04,489 --> 00:01:08,770
still making it s interesting as
possible I'll

20
00:01:08,770 --> 00:01:12,080
the the free software movement

21
00:01:12,080 --> 00:01:15,289
which is now a

22
00:01:15,289 --> 00:01:18,310
30 years old a if you start

23
00:01:18,310 --> 00:01:21,429
from a Richards up original

24
00:01:21,429 --> 00:01:24,810
a mulling over his concerns

25
00:01:24,810 --> 00:01:28,270
about on Fri operating systems in the

26
00:01:28,270 --> 00:01:31,560
MIT AI lab the free software movement

27
00:01:31,560 --> 00:01:35,090
is reaching up its moment

28
00:01:35,090 --> 00:01:40,380
junction with the great River up the
Internet freedom movement

29
00:01:40,380 --> 00:01:44,109
which is going to be a the dominant

30
00:01:44,109 --> 00:01:47,280
political technology movement about our
time

31
00:01:47,280 --> 00:01:51,509
a into which all our boats are finally
going to reach the sea

32
00:01:51,509 --> 00:01:54,539
on but in the process I'll

33
00:01:54,539 --> 00:01:58,840
becoming law first the free software
movement and then the great river

34
00:01:58,840 --> 00:02:02,460
Internet freedom movement we are of
course as always

35
00:02:02,460 --> 00:02:07,039
standing on the shoulders of giants many
of the Giants

36
00:02:07,039 --> 00:02:10,170
who affect id the thinking of those
summers

37
00:02:10,170 --> 00:02:14,040
who began worrying about the freedom up
software

38
00:02:14,040 --> 00:02:17,150
decades ago many the Giants who

39
00:02:17,150 --> 00:02:20,380
on whose shoulders we stood were

40
00:02:20,380 --> 00:02:24,180
officers science fiction there was a
great

41
00:02:24,180 --> 00:02:28,350
visionaries I'll post second world war

42
00:02:28,350 --> 00:02:31,709
imaginative literature which

43
00:02:31,709 --> 00:02:35,450
coped wish the problem %uh the runaway

44
00:02:35,450 --> 00:02:39,269
technology that had transformed the
world you will recall

45
00:02:39,269 --> 00:02:43,489
that after the first terrible use

46
00:02:43,489 --> 00:02:47,760
nuclear weapons in the world albert
einstein said

47
00:02:47,760 --> 00:02:51,060
we have changed everything except the
wing man

48
00:02:51,060 --> 00:02:54,280
think and the culture of the

49
00:02:54,280 --> 00:02:58,940
post-war Western world all the culture
the post-war Eastern world to a few

50
00:02:58,940 --> 00:02:59,900
minutes we did

51
00:02:59,900 --> 00:03:04,239
science fiction from both sides up the
iron curtain the culture of the post-war

52
00:03:04,239 --> 00:03:05,030
world

53
00:03:05,030 --> 00:03:09,110
was very happily f-actin by the attempt

54
00:03:09,110 --> 00:03:12,580
to understand the implications of
Technology

55
00:03:12,580 --> 00:03:15,989
as imaginative officers

56
00:03:15,989 --> 00:03:19,390
including where Bradbury who recently
left us

57
00:03:19,390 --> 00:03:23,879
and many others as imaginative officers
tried to cast

58
00:03:23,879 --> 00:03:26,929
in New idealized forms

59
00:03:26,930 --> 00:03:29,939
moral ethical problems

60
00:03:29,939 --> 00:03:33,849
with technology out of control and the
literature

61
00:03:33,849 --> 00:03:37,959
that they were all deeply affect it me
when I was young

62
00:03:37,959 --> 00:03:41,459
and growing up and Richard and many many
others

63
00:03:41,459 --> 00:03:44,840
I want to go back to that now

64
00:03:44,840 --> 00:03:48,959
because I believe it's time for us to
acknowledge it again

65
00:03:48,959 --> 00:03:52,489
how much may foresaw those writers

66
00:03:52,489 --> 00:03:56,060
up to post-war world and how much they
helped us

67
00:03:56,060 --> 00:03:59,980
to foresee but how little we helped
ourselves to avoid

68
00:03:59,980 --> 00:04:03,010
what are the staples %uh

69
00:04:03,010 --> 00:04:07,120
that science fiction in the 1960s that
we read so avidly

70
00:04:07,120 --> 00:04:12,579
growing up was that by now the middle
love the first quarter

71
00:04:12,579 --> 00:04:19,239
the 21st century by now it was shown the
human beings would be living in society

72
00:04:19,239 --> 00:04:21,200
commensurately with robots

73
00:04:21,200 --> 00:04:25,680
and many many people tried to imagine

74
00:04:25,680 --> 00:04:30,560
nature up that commands soul biological
coexistence

75
00:04:30,560 --> 00:04:34,800
between us and the robots that Android's

76
00:04:34,800 --> 00:04:40,990
we had bill everybody understood that
there were enormous

77
00:04:40,990 --> 00:04:44,229
ethical and moral dilemmas implicit

78
00:04:44,229 --> 00:04:48,640
in our living with robots as there would
also be

79
00:04:48,640 --> 00:04:51,900
enormous changes in the texture and
fabric

80
00:04:51,900 --> 00:04:55,050
ordinary human life from day to day

81
00:04:55,050 --> 00:04:59,300
and the two elements the nature of your
life

82
00:04:59,300 --> 00:05:05,690
has lived in the company robots and the
nature of the ethical and moral dilemmas

83
00:05:05,690 --> 00:05:10,899
implied by the attempt to do so were
fertile ground for some of the very

84
00:05:10,899 --> 00:05:12,839
greatest fiction that was written

85
00:05:12,839 --> 00:05:17,310
in that time not least wage

86
00:05:17,310 --> 00:05:23,050
was Isaac Asimov's attempt to understand
how we would confront

87
00:05:23,050 --> 00:05:27,539
the problem love runaway technology in
life with robots

88
00:05:27,539 --> 00:05:31,550
which produced as many here will recall

89
00:05:31,550 --> 00:05:35,200
as warmly as I do all love stories

90
00:05:35,200 --> 00:05:39,640
and the novels which were built out of
the US Robotics

91
00:05:39,640 --> 00:05:43,070
corporation and its part tronic brained

92
00:05:43,070 --> 00:05:47,089
creations

93
00:05:47,089 --> 00:05:50,270
there of course

94
00:05:50,270 --> 00:05:55,709
from the beginning the assumption was
that robots would be humanoid

95
00:05:55,709 --> 00:05:58,839
and as it turns out they're not

96
00:05:58,839 --> 00:06:02,409
we do after all if commensurately with
robots

97
00:06:02,409 --> 00:06:08,140
we go just as they expected but the
robots we live with don't have hands and

98
00:06:08,140 --> 00:06:09,510
feet

99
00:06:09,510 --> 00:06:15,450
they don't carry trays and drinks and
they don't push the vacuum cleaner

100
00:06:15,450 --> 00:06:20,599
at the age condition they are the vacuum
cleaner

101
00:06:20,599 --> 00:06:25,050
but most of the time we're their hands
she

102
00:06:25,050 --> 00:06:28,520
we embody them we carry them

103
00:06:28,520 --> 00:06:33,820
around with us they see everything we
see

104
00:06:33,820 --> 00:06:39,080
they hear everything we hear they're
constantly where our location

105
00:06:39,080 --> 00:06:43,140
position velocity and intention

106
00:06:43,140 --> 00:06:46,719
they mediate our searches that is to say

107
00:06:46,719 --> 00:06:50,709
they know our plans they consider our
dreams

108
00:06:50,709 --> 00:06:55,610
they understand our lives

109
00:06:55,610 --> 00:06:58,680
they even take our questions

110
00:06:58,680 --> 00:07:02,219
like how do I send flowers to my
girlfriend

111
00:07:02,219 --> 00:07:06,830
transmit them to a great big database in
California

112
00:07:06,830 --> 00:07:12,659
and return us answers offered by the
helpful wizard behind the curtain

113
00:07:12,659 --> 00:07:16,099
of course is keeping track

114
00:07:16,099 --> 00:07:19,800
these are our robots and we have

115
00:07:19,800 --> 00:07:24,529
every show we ever expected to have from
them

116
00:07:24,529 --> 00:07:28,580
except the first law of robotics

117
00:07:28,580 --> 00:07:31,599
you remember how that went right deep in

118
00:07:31,599 --> 00:07:34,880
the design a positron ik

119
00:07:34,880 --> 00:07:38,430
intelligence that made the robot where
the

120
00:07:38,430 --> 00:07:44,399
lawyers that governed the ethical
boundary between what could and could

121
00:07:44,399 --> 00:07:46,510
not be done with Android

122
00:07:46,510 --> 00:07:50,310
says the first floor

123
00:07:50,310 --> 00:07:53,870
the first law the one that everything
else had to be

124
00:07:53,870 --> 00:07:56,890
deduced from was that no role but may

125
00:07:56,890 --> 00:08:01,289
ever injure a human being

126
00:08:01,289 --> 00:08:04,580
robots must take orders

127
00:08:04,580 --> 00:08:08,279
from there human Olmert's

128
00:08:08,279 --> 00:08:11,779
except where those orders involve
harming

129
00:08:11,779 --> 00:08:15,019
a human being

130
00:08:15,019 --> 00:08:18,930
that was assumed to be the principal

131
00:08:18,930 --> 00:08:24,089
out love which at the root down by then
and gates

132
00:08:24,089 --> 00:08:31,089
%uh her artificial no role physiology of
robot brains down there where the

133
00:08:31,169 --> 00:08:33,728
shortlist ideas

134
00:08:33,729 --> 00:08:37,229
you member for Descartes it was cokie to
level song

135
00:08:37,229 --> 00:08:40,769
for the robot it was no robot must never
harm a human being

136
00:08:40,769 --> 00:08:45,300
we are living commensurately with robots
but we have no

137
00:08:45,300 --> 00:08:48,390
first law robotics in them

138
00:08:48,390 --> 00:08:52,920
they hurt human beings everyday

139
00:08:52,920 --> 00:08:59,199
everywhere those injuries range from the
trivial

140
00:08:59,200 --> 00:09:04,080
to the fatal to the cosmic

141
00:09:04,080 --> 00:09:08,889
of course they're helping people to
charge you more

142
00:09:08,889 --> 00:09:12,279
that's trivial right they're letting

143
00:09:12,279 --> 00:09:16,550
other people know when you need
everything from my hamburger

144
00:09:16,550 --> 00:09:21,579
to sexual interaction to a house
mortgage

145
00:09:21,579 --> 00:09:25,439
and of course the people on the other
end another repeat players whose

146
00:09:25,439 --> 00:09:26,829
calculations

147
00:09:26,829 --> 00:09:31,939
about just how much you need whatever it
is and how much longer pay for it

148
00:09:31,939 --> 00:09:36,509
are being built by the data mining them
all the data about everybody that

149
00:09:36,509 --> 00:09:38,019
everybody is collecting

150
00:09:38,019 --> 00:09:45,019
through the robots but it isn't just
that you're paying more

151
00:09:46,009 --> 00:09:49,959
some people in the world RB arrested

152
00:09:49,959 --> 00:09:54,829
tortured or killed because they've been
informed on

153
00:09:54,829 --> 00:09:58,209
by their robots

154
00:09:58,209 --> 00:10:02,839
two days ago the New York Times printed
a little story about the idea that we

155
00:10:02,839 --> 00:10:06,709
ought to call them trackers that
happened to make phone calls rather than

156
00:10:06,709 --> 00:10:11,100
follows that happened to track this
round they were kind enough to

157
00:10:11,100 --> 00:10:14,949
mention that topic of today's top
mention

158
00:10:14,949 --> 00:10:19,660
the talk and this morning the New York
Times has an editorial lamenting the

159
00:10:19,660 --> 00:10:22,050
death for privacy and suggesting

160
00:10:22,050 --> 00:10:25,519
legislation here's the cosmic

161
00:10:25,519 --> 00:10:30,120
harm our robots i doing us they are
destroying all

162
00:10:30,120 --> 00:10:34,480
the human right to be a law

163
00:10:34,480 --> 00:10:39,550
they are destroying the human right to
do your old shrinking

164
00:10:39,550 --> 00:10:43,899
that destroy all the human capacity

165
00:10:43,899 --> 00:10:47,249
for disappearing into

166
00:10:47,249 --> 00:10:52,129
ourselves robots are changing humanity

167
00:10:52,129 --> 00:10:55,949
yes the literature said they would
they're changing

168
00:10:55,949 --> 00:10:59,279
humanity quite deeply

169
00:10:59,279 --> 00:11:03,129
and the way that they are changing
humanity is not

170
00:11:03,129 --> 00:11:07,420
to make it more human

171
00:11:07,420 --> 00:11:12,329
instead Android quality is rubbing off
on

172
00:11:12,329 --> 00:11:15,939
us which wasn't course always

173
00:11:15,939 --> 00:11:19,969
implicit in the literature when it
turned our

174
00:11:19,970 --> 00:11:23,620
that we might not be able to tell the
difference

175
00:11:23,620 --> 00:11:26,709
after a while between all

176
00:11:26,709 --> 00:11:32,160
the replicants and ourselves

177
00:11:32,160 --> 00:11:36,310
show we've got a problem

178
00:11:36,310 --> 00:11:39,680
I've tried to define the problem space

179
00:11:39,680 --> 00:11:44,209
in this talk I don't propose that we can
solve the problem this afternoon we can

180
00:11:44,209 --> 00:11:47,749
recognize it

181
00:11:47,749 --> 00:11:51,029
as I get older and grayer and a further

182
00:11:51,029 --> 00:11:54,490
from that boy who read that science
fiction

183
00:11:54,490 --> 00:11:57,800
I realize that the rest of my life is
going to be about this

184
00:11:57,800 --> 00:12:01,829
in probably therefore some part of the
restive yours

185
00:12:01,829 --> 00:12:06,819
if you see it the way I do we have to
retrofit

186
00:12:06,819 --> 00:12:12,160
the first law of robotics into
everything

187
00:12:12,160 --> 00:12:16,779
this is not going to be simple

188
00:12:16,779 --> 00:12:20,980
the slash daughters who wanted me to
remembered that the purpose and the

189
00:12:20,980 --> 00:12:22,920
first law of robotics was all

190
00:12:22,920 --> 00:12:27,439
trained in the positron brain well
course they were right that was to

191
00:12:27,439 --> 00:12:32,439
happy imaginative part you remember what
that was

192
00:12:32,439 --> 00:12:37,639
the S solution that Isaac Asimov made
was that human beings would be afraid

193
00:12:37,639 --> 00:12:43,420
%uh robots that they would be afraid to
allow their children to be tended by

194
00:12:43,420 --> 00:12:44,219
robots

195
00:12:44,220 --> 00:12:48,920
what I have them in their homes and
therefore

196
00:12:48,920 --> 00:12:53,079
without some assurance %uh the complete

197
00:12:53,079 --> 00:12:58,350
engineered in from the very beginning
your quality of will never hurt human

198
00:12:58,350 --> 00:12:59,879
being

199
00:12:59,879 --> 00:13:04,069
that robots would not be adopted

200
00:13:04,069 --> 00:13:09,779
that her MeMeMe that capitalist motive
of the robot maker

201
00:13:09,779 --> 00:13:13,360
the US Robotics corporation

202
00:13:13,360 --> 00:13:16,980
that the capitalist needs

203
00:13:16,980 --> 00:13:20,319
up the robot maker to create schaeff

204
00:13:20,319 --> 00:13:24,240
market in which consumers would accept
robots

205
00:13:24,240 --> 00:13:27,720
in their holes in with their children
would require

206
00:13:27,720 --> 00:13:31,620
and absolute guarantee engineered in

207
00:13:31,620 --> 00:13:38,620
safety will never harm you

208
00:13:40,730 --> 00:13:44,990
I should castle of was a great New
Yorker

209
00:13:44,990 --> 00:13:48,949
I was privileged to grow up in his
shitty while he lived here

210
00:13:48,949 --> 00:13:52,040
bump into him every once in a while

211
00:13:52,040 --> 00:13:55,410
as you know from the foundation trilogy
he had really

212
00:13:55,410 --> 00:13:59,610
at the bottom a very give me a click
send some all of this

213
00:13:59,610 --> 00:14:02,740
trend to our was really the Grand
Concourse and

214
00:14:02,740 --> 00:14:09,740
good Jewish family values were really
enough to save the galaxy

215
00:14:11,920 --> 00:14:15,490
unfortunately this was the visionary
part

216
00:14:15,490 --> 00:14:19,369
of the science fiction and it isn't true

217
00:14:19,369 --> 00:14:24,499
it was much easier to get people to hang
robots around the necks if their

218
00:14:24,499 --> 00:14:25,499
children

219
00:14:25,499 --> 00:14:28,749
than anybody ever imagine

220
00:14:28,749 --> 00:14:33,209
and it didn't require any promise that
they would never never never harm

221
00:14:33,209 --> 00:14:38,290
anybody all acquired was little shiny
things

222
00:14:38,290 --> 00:14:42,868
made by Count Dracula the king of the on
their

223
00:14:42,869 --> 00:14:46,800
the purpose of the on did as you know

224
00:14:46,800 --> 00:14:51,600
is to make evil beautiful

225
00:14:51,600 --> 00:14:54,910
that's what the undead do they turn

226
00:14:54,910 --> 00:14:59,629
evil into something so erotically
attractive

227
00:14:59,629 --> 00:15:03,809
you can't keep your hands of it and you
don't mind

228
00:15:03,809 --> 00:15:08,350
having its hands on you

229
00:15:08,350 --> 00:15:13,839
and he did the king of the young dead

230
00:15:13,839 --> 00:15:16,560
he's dead now

231
00:15:16,560 --> 00:15:20,980
but they didn't throw is boot into the
Danube and there was no silver bullet

232
00:15:20,980 --> 00:15:25,209
and there was no stake through the heart
and the undead still are with us

233
00:15:25,209 --> 00:15:28,360
they're improving the screen

234
00:15:28,360 --> 00:15:33,089
and suchlike until

235
00:15:33,089 --> 00:15:38,050
another king of the on dad who can build
damn beautiful things is ready

236
00:15:38,050 --> 00:15:41,079
but that's only

237
00:15:41,079 --> 00:15:47,750
talk and now we put those things around
our children's necks and we send them

238
00:15:47,750 --> 00:15:48,420
off

239
00:15:48,420 --> 00:15:52,540
to be harmed by the robot

240
00:15:52,540 --> 00:15:58,209
with whom they live we have

241
00:15:58,209 --> 00:16:04,050
the problem that Einstein was talking
about we have changed everything

242
00:16:04,050 --> 00:16:07,579
except the way people think about this

243
00:16:07,579 --> 00:16:12,300
the heuristics the humanity brain rules

244
00:16:12,300 --> 00:16:16,720
to the net our heuristics which is sOooo

245
00:16:16,720 --> 00:16:20,000
that they know the direction in which
danger comes in

246
00:16:20,000 --> 00:16:25,380
them and they do not so much has
happened

247
00:16:25,380 --> 00:16:29,610
very quickly it isn't that we haven't

248
00:16:29,610 --> 00:16:33,060
warned about the

249
00:16:33,060 --> 00:16:36,579
free software movement gave its warning

250
00:16:36,579 --> 00:16:42,120
all the way law very rationalistic Lee
scientifically

251
00:16:42,120 --> 00:16:47,730
in hackers be however and now a great
problem

252
00:16:47,730 --> 00:16:52,320
was always how were we gonna get people
who didn't hack on things

253
00:16:52,320 --> 00:16:55,980
to understand the importance of being
able

254
00:16:55,980 --> 00:17:01,250
to hack on things it was a really tough
political

255
00:17:01,250 --> 00:17:06,250
lift for shame lawmakers to explain to
people

256
00:17:06,250 --> 00:17:11,929
who didn't interact with code at all why
the freedom love code was their freedom

257
00:17:11,929 --> 00:17:17,120
at the end of the day we knew it was
because we grew up

258
00:17:17,120 --> 00:17:22,449
with RI robot we knew it was

259
00:17:22,449 --> 00:17:28,130
because we grew up understanding that
humanity had many different ways

260
00:17:28,130 --> 00:17:31,400
for creating unethical technology

261
00:17:31,400 --> 00:17:34,540
and that we were going

262
00:17:34,540 --> 00:17:37,580
to have to find ways to embed

263
00:17:37,580 --> 00:17:42,899
ethics in technology mister Stallman
could not have been clearer

264
00:17:42,900 --> 00:17:49,260
about that but his clarity wasn't
universally accessible by any means

265
00:17:49,260 --> 00:17:53,429
it wasn't imaginatively available

266
00:17:53,429 --> 00:17:57,210
to every child in the world it was only
available

267
00:17:57,210 --> 00:18:04,210
Lashay to us now we have to
operationalize

268
00:18:04,240 --> 00:18:11,160
in a profound your way because we aren't
merely worried about whether there will

269
00:18:11,160 --> 00:18:11,600
be

270
00:18:11,600 --> 00:18:16,040
code available for operating systems for
people who work

271
00:18:16,040 --> 00:18:19,440
in artificial intelligence laboratories

272
00:18:19,440 --> 00:18:25,260
or Evil four steals now we have to worry
about how to retrofit the first law of

273
00:18:25,260 --> 00:18:26,210
robotics

274
00:18:26,210 --> 00:18:29,460
into objects that are hurting people

275
00:18:29,460 --> 00:18:33,650
mostly

276
00:18:33,650 --> 00:18:38,400
mostly we have an asset call

277
00:18:38,400 --> 00:18:42,500
and moral problem to describe

278
00:18:42,500 --> 00:18:46,220
and two-shot the outside

279
00:18:46,220 --> 00:18:50,110
limits 4 we have to be able to express

280
00:18:50,110 --> 00:18:53,130
to all the people with whom we interact

281
00:18:53,130 --> 00:18:58,520
though they are not necessarily
technical in the same wage that we are

282
00:18:58,520 --> 00:19:01,850
we have to be able to express to them
what

283
00:19:01,850 --> 00:19:07,640
ethical limits of technologies are with
which they are already familiar

284
00:19:07,640 --> 00:19:11,049
in ethically compromised form

285
00:19:11,049 --> 00:19:14,370
and of course we have some

286
00:19:14,370 --> 00:19:20,250
technology work to do as well where the
two

287
00:19:20,250 --> 00:19:24,860
change cross where we are required to do
technology work

288
00:19:24,860 --> 00:19:29,840
as well as explain to people the nature
of the ethical limits at the

289
00:19:29,840 --> 00:19:31,459
technologies around us

290
00:19:31,460 --> 00:19:36,409
we have our biggest problem and the most
immediately

291
00:19:36,409 --> 00:19:42,380
urgent we cannot retrofit the first law
of robotics

292
00:19:42,380 --> 00:19:47,340
into robots that have been designed to
resist our modifying

293
00:19:47,340 --> 00:19:52,010
them this is a fairly simple point

294
00:19:52,010 --> 00:19:56,740
I i understand. we tried five years ago

295
00:19:56,740 --> 00:20:01,820
in GP all three to make it with
sufficient clarity that everybody

296
00:20:01,820 --> 00:20:05,710
who understood the implications could
come along with us

297
00:20:05,710 --> 00:20:09,669
and do some work in helping to avoid

298
00:20:09,669 --> 00:20:14,640
the situation up the locked-down robot

299
00:20:14,640 --> 00:20:19,690
we made very little progress because
people who are now beginning to realize

300
00:20:19,690 --> 00:20:23,240
that they should have supported the anti
lock down efforts

301
00:20:23,240 --> 00:20:28,040
in GP all three had didn't do so at the
time

302
00:20:28,040 --> 00:20:32,190
maybe still haven't done so as
powerfully as they should have done

303
00:20:32,190 --> 00:20:37,790
and in the meantime a whole range of
monoliths grew up around society

304
00:20:37,790 --> 00:20:43,610
that i very much in love with the idea
of the robot you can't retrofit

305
00:20:43,610 --> 00:20:47,760
and it does harm to people every day

306
00:20:47,760 --> 00:20:51,490
and you can't retrofitted because the
covers well

307
00:20:51,490 --> 00:20:54,600
shop and its booby-trapped

308
00:20:54,600 --> 00:20:59,000
by the DMCA lots and other things

309
00:20:59,000 --> 00:21:02,770
in orange jail for trying to retrofit
freedom into a robot

310
00:21:02,770 --> 00:21:06,260
under the wrong conditions if that might
mean

311
00:21:06,260 --> 00:21:10,149
the robot might share copyrighted song
without permission

312
00:21:10,149 --> 00:21:15,000
up the composer or show you a movie

313
00:21:15,000 --> 00:21:18,290
this %um paid for enough times

314
00:21:18,290 --> 00:21:24,300
yet

315
00:21:24,300 --> 00:21:25,020
show

316
00:21:25,020 --> 00:21:30,139
first thing with you have to do is take
with much greater seriousness

317
00:21:30,140 --> 00:21:34,680
the job of building a coalition to
assure that retrofitting

318
00:21:34,680 --> 00:21:40,830
is possible that it is neither legally
nor technologically prohibited

319
00:21:40,830 --> 00:21:47,800
to make things schaeffer they shouldn't
be required in a democracy

320
00:21:47,800 --> 00:21:53,290
they shouldn't even be required
capitalism if you own a thing

321
00:21:53,290 --> 00:21:58,230
he should be your right to make it safer
don't you think

322
00:21:58,230 --> 00:22:05,100
now well you see how fundamentally we've
lost our way

323
00:22:05,100 --> 00:22:09,990
show we need a few shameless and we
don't need to be

324
00:22:09,990 --> 00:22:13,260
altogether unwilling to adopt

325
00:22:13,260 --> 00:22:19,180
other people's vocabulary in order to
get them about this for example it seems

326
00:22:19,180 --> 00:22:21,240
to me that we deserve to be a strong

327
00:22:21,240 --> 00:22:25,410
for owners rights as other people are
for their entitlement to have

328
00:22:25,410 --> 00:22:28,820
offshore trust funds other ships

329
00:22:28,820 --> 00:22:32,189
we all miss stuff okay back

330
00:22:32,190 --> 00:22:37,700
of where its our stuff we're entitled to
modify it

331
00:22:37,700 --> 00:22:41,820
if we want to make it safer if we wanna
share Shafee

332
00:22:41,820 --> 00:22:45,730
improvements with other people so they
can modify what they all

333
00:22:45,730 --> 00:22:52,690
to that's alright we need

334
00:22:52,690 --> 00:22:56,460
to be very clear that how things work

335
00:22:56,460 --> 00:23:02,020
his ass associated with the quake
concept of the ownership of the same

336
00:23:02,020 --> 00:23:05,660
if I own a two-way work should be the
way I want it to

337
00:23:05,660 --> 00:23:09,130
shop for freedom Law Center submitted

338
00:23:09,130 --> 00:23:14,450
a an exemption request in the library of
congress DMCA exception proceedings

339
00:23:14,450 --> 00:23:18,530
this year %uh urging the Library of
Congress to declare

340
00:23:18,530 --> 00:23:21,670
that it is not prohibited circumvention

341
00:23:21,670 --> 00:23:25,990
love leanza access control to replace
the operating system

342
00:23:25,990 --> 00:23:29,400
in a mo by law or other computing device
you all

343
00:23:29,400 --> 00:23:32,900
um I'm very grateful

344
00:23:32,900 --> 00:23:36,450
to and Williamson of the SSLC for his

345
00:23:36,450 --> 00:23:40,410
extraordinary work in preparing and
testifying on behalf

346
00:23:40,410 --> 00:23:45,510
that exemption request we're going to
back it this time as strongly as we can

347
00:23:45,510 --> 00:23:49,640
we hope the library of congress will see
the wisdom love declaring

348
00:23:49,640 --> 00:23:53,040
that in this free market country you are
free

349
00:23:53,040 --> 00:23:57,889
to modified devices that you've bought
with money and that you

350
00:23:57,890 --> 00:24:04,830
a quaintly regarded as owning it
shouldn't require any more argument and

351
00:24:04,830 --> 00:24:10,379
that but if it does we have to double
down and keep arguing we have to point

352
00:24:10,380 --> 00:24:13,240
out that if devices are unsafe

353
00:24:13,240 --> 00:24:16,370
it is a legal obligation

354
00:24:16,370 --> 00:24:22,260
to permit us to make them safer if you
show on shapes slicer

355
00:24:22,260 --> 00:24:25,920
in a delicatessen already unsafe
automobile

356
00:24:25,920 --> 00:24:29,980
and jude attempt to prevent people from
modifying

357
00:24:29,980 --> 00:24:33,120
those devices to make them safer if
you're

358
00:24:33,120 --> 00:24:36,770
actually out there actively interfering

359
00:24:36,770 --> 00:24:40,879
with attempts to make them safer then
when people get hurt

360
00:24:40,880 --> 00:24:47,880
you should be liable

361
00:24:51,669 --> 00:24:54,990
if we press hard enough on that point

362
00:24:54,990 --> 00:24:59,399
we will scare even count dracula King Of
Leon dead

363
00:24:59,399 --> 00:25:03,769
in his grade where he should be very
frightening

364
00:25:03,769 --> 00:25:07,630
because he has interfered with more
attempts

365
00:25:07,630 --> 00:25:10,970
to make his products safer by more
people

366
00:25:10,970 --> 00:25:14,470
than any other undid maker in history

367
00:25:14,470 --> 00:25:20,630
we need to establish the proposition
that when people get hurt

368
00:25:20,630 --> 00:25:23,659
when somebody is responsible for that

369
00:25:23,659 --> 00:25:27,190
they pay for it if they have attempted

370
00:25:27,190 --> 00:25:30,360
to prevent us from preventing the heart

371
00:25:30,360 --> 00:25:35,919
this is not the first law of robotics
this is the first love being US Robotics

372
00:25:35,919 --> 00:25:36,500
okay

373
00:25:36,500 --> 00:25:43,299
it's your ash on the line everybody's
gotta know that and by everybody I

374
00:25:43,299 --> 00:25:45,809
distinctly mean to include

375
00:25:45,809 --> 00:25:52,809
certain parties called arise and AT&T

376
00:25:53,870 --> 00:25:57,840
nowhere in the world are the network

377
00:25:57,840 --> 00:26:01,809
operators more aggressive about
prohibiting

378
00:26:01,809 --> 00:26:05,539
us from increasing the safety of devices

379
00:26:05,539 --> 00:26:11,690
nowhere is there a more concentrated
opposition to GPL 3

380
00:26:11,690 --> 00:26:15,259
then in the US network operator duopoly

381
00:26:15,259 --> 00:26:19,179
now we know

382
00:26:19,179 --> 00:26:23,700
thanks to last week's news confirming
what we already know but it hadn't been

383
00:26:23,700 --> 00:26:25,360
printed in the new york times yet

384
00:26:25,360 --> 00:26:32,190
that millions of times a year people
without tennis star are requesting

385
00:26:32,190 --> 00:26:36,500
the real time location or the contents
have messages or other

386
00:26:36,500 --> 00:26:40,620
nature of the traffic beat Wall tracking
devices

387
00:26:40,620 --> 00:26:47,620
and the networks we know that that ish
Touche we know exactly how far down the

388
00:26:48,340 --> 00:26:48,830
road

389
00:26:48,830 --> 00:26:53,230
up suppression of civil liberties the
robots

390
00:26:53,230 --> 00:26:58,610
are taking us of course

391
00:26:58,610 --> 00:27:03,240
improving your civil liberties is not
necessarily regarded by

392
00:27:03,240 --> 00:27:10,240
other people making you schaeffer show
not all the time when we insist upon

393
00:27:12,260 --> 00:27:16,320
improving our civil liberties by
retrofitting into devices

394
00:27:16,320 --> 00:27:19,529
our first law you shall not harm

395
00:27:19,529 --> 00:27:25,679
the user of the divine huge we're going
to be told that what we're doing is

396
00:27:25,679 --> 00:27:27,649
making people safer because

397
00:27:27,649 --> 00:27:31,649
it makes terrorists safer to or some
such nonsense

398
00:27:31,649 --> 00:27:38,649
the truth of the matter is products must
not harm the people who buy and use them

399
00:27:39,929 --> 00:27:43,419
regardless whether people buying use
them are nice people

400
00:27:43,419 --> 00:27:47,110
when a kid gets his hand

401
00:27:47,110 --> 00:27:51,679
injured by delicatessen slicing we know
ask ourselves whether he's a good kid or

402
00:27:51,679 --> 00:27:51,980
not

403
00:27:51,980 --> 00:27:57,289
we'll

404
00:27:57,289 --> 00:28:02,408
no even ask whether he was a little bit
impaired by something

405
00:28:02,409 --> 00:28:06,510
when it happened because the
manufacturer

406
00:28:06,510 --> 00:28:10,080
who makes spend it and article
inherently dangerous

407
00:28:10,080 --> 00:28:15,559
is responsible for the harm it does and
if for example it doesn't have to hand

408
00:28:15,559 --> 00:28:16,260
switching

409
00:28:16,260 --> 00:28:20,869
and somebody's hand gets hurt whether
they were a nice guy or a bad guy or

410
00:28:20,869 --> 00:28:23,200
whether they were planning to sabotage
the

411
00:28:23,200 --> 00:28:26,480
factory on the weekend is not relevant

412
00:28:26,480 --> 00:28:33,480
concern the IT architecture of the next
period

413
00:28:33,860 --> 00:28:37,679
is set and pretty much everywhere I go
in the world

414
00:28:37,679 --> 00:28:41,940
everybody understands it they recognize
it it's called cloud

415
00:28:41,940 --> 00:28:45,350
to mow bile what several

416
00:28:45,350 --> 00:28:49,509
it means robots reporting at
headquarters

417
00:28:49,509 --> 00:28:52,629
tossing your data overhead

418
00:28:52,629 --> 00:28:58,090
from where they collected to where it is
stored wherever that is

419
00:28:58,090 --> 00:29:01,929
if you're a lawyer who worries about
privacy

420
00:29:01,929 --> 00:29:06,570
that certain about the same as saying
first it will be at the robot and then

421
00:29:06,570 --> 00:29:10,049
it will be in whatever legal system in
the world gives you the least

422
00:29:10,049 --> 00:29:14,090
protection for it and the most

423
00:29:14,090 --> 00:29:17,619
academic commercial advantage

424
00:29:17,619 --> 00:29:21,559
to the guy keeping it for you

425
00:29:21,559 --> 00:29:25,190
in 2006 I gave a talk to MySQL annual

426
00:29:25,190 --> 00:29:29,320
developers meeting about why it's good
to store it yourself

427
00:29:29,320 --> 00:29:32,928
instead of storing things other places

428
00:29:32,929 --> 00:29:35,970
but I was still in the grip love

429
00:29:35,970 --> 00:29:40,230
belief that we were all going to be fine
and I spent more time talking about

430
00:29:40,230 --> 00:29:44,179
technologies and memory in relation to
freedom than what I shoulda said which

431
00:29:44,179 --> 00:29:44,789
was

432
00:29:44,789 --> 00:29:48,149
if you know story yourself it's going to
be stored by

433
00:29:48,149 --> 00:29:52,919
die taking advantage of you deeply
erotic aid in your privacy

434
00:29:52,919 --> 00:29:59,919
and making do Android I'll him

435
00:30:00,350 --> 00:30:03,710
I probably would even have chosen the
word Android which had nothing to a

436
00:30:03,710 --> 00:30:04,789
computer software

437
00:30:04,789 --> 00:30:09,279
at the time but there we are

438
00:30:09,279 --> 00:30:14,279
but there we are cloud to lobe I'll what
it means is from other safety 2 I'm

439
00:30:14,279 --> 00:30:18,119
safety unless we do it right

440
00:30:18,119 --> 00:30:23,379
got a question referred to the NYU
talking 2010 about freedom in the cloud

441
00:30:23,379 --> 00:30:27,909
I wanted them to shut out some ideas
about how we had gotten to that part of

442
00:30:27,909 --> 00:30:28,779
the mass

443
00:30:28,779 --> 00:30:31,779
and how we might get out of them again
on

444
00:30:31,779 --> 00:30:35,179
on that particular point let me just say
about freedom botched that

445
00:30:35,179 --> 00:30:38,799
as love up variation now by which I mean

446
00:30:38,799 --> 00:30:43,230
single-digit days %uh Gambian will be
natively supporting

447
00:30:43,230 --> 00:30:47,230
the plug server called the drain plug
and from in a variety of other plug

448
00:30:47,230 --> 00:30:48,359
servers

449
00:30:48,359 --> 00:30:52,850
and freedom box will have moved into
being tambien privacy and we will be

450
00:30:52,850 --> 00:30:53,590
trying

451
00:30:53,590 --> 00:30:57,090
to deliver best possible privacy tools

452
00:30:57,090 --> 00:31:03,350
to every architecture everywhere all the
time and particularly to small

453
00:31:03,350 --> 00:31:06,539
effective power shipping

454
00:31:06,539 --> 00:31:10,059
plug servers that can replace routers
everywhere

455
00:31:10,059 --> 00:31:13,070
and make the networks safer

456
00:31:13,070 --> 00:31:18,240
fireworks work I am endlessly grateful
to be dealt are being changed the CEO

457
00:31:18,240 --> 00:31:21,609
and Nick daley and many others who have
been hacking on

458
00:31:21,609 --> 00:31:25,440
freedom box over the last 18 months

459
00:31:25,440 --> 00:31:32,440
but what I

460
00:31:32,490 --> 00:31:36,220
but what I did you know is that no
matter what we do to make the network

461
00:31:36,220 --> 00:31:37,149
schaeffer

462
00:31:37,149 --> 00:31:41,399
and to make server-side improvements let
us call him

463
00:31:41,399 --> 00:31:47,190
we must at the low bile and be capable
of delivering safety security and

464
00:31:47,190 --> 00:31:48,129
privacy

465
00:31:48,129 --> 00:31:52,000
to people on the things they really use
the robots they

466
00:31:52,000 --> 00:31:55,440
really live with it won't do us any good

467
00:31:55,440 --> 00:31:59,690
to try and compete with US robotics and
Count Dracula

468
00:31:59,690 --> 00:32:05,159
by saying you can buy beige a little box
and plug it in on a wall at home

469
00:32:05,159 --> 00:32:10,100
we have to get into the galaxy in your
pocket

470
00:32:10,100 --> 00:32:13,220
or the Galaxy will have no freedom and

471
00:32:13,220 --> 00:32:16,340
no matter what they do on Trent

472
00:32:16,340 --> 00:32:21,379
Solis raises questions beyond merely

473
00:32:21,379 --> 00:32:24,850
how we can get the code in the box or

474
00:32:24,850 --> 00:32:29,490
how yet define what the code is where
all real good at that and I'm actually

475
00:32:29,490 --> 00:32:32,950
quite optimistic that we can hold up to
technical and

476
00:32:32,950 --> 00:32:36,369
we've been holding up the technical and
all the way along the free software

477
00:32:36,369 --> 00:32:39,289
movement has contributed a lot of
freedom

478
00:32:39,289 --> 00:32:42,809
to the Internet freedom movement it is
becoming and we're gonna continue to

479
00:32:42,809 --> 00:32:44,279
contribute all the way

480
00:32:44,279 --> 00:32:47,559
until we win

481
00:32:47,559 --> 00:32:52,740
but what we have to do. beyond all the
stuff with good at

482
00:32:52,740 --> 00:32:56,109
is to do nothing human beings haven't
been good at so far

483
00:32:56,109 --> 00:32:59,960
we have to be really alive to the danger
and we have to teach people

484
00:32:59,960 --> 00:33:03,609
that safety must be put in now

485
00:33:03,609 --> 00:33:07,490
after we've already launched the boat

486
00:33:07,490 --> 00:33:11,580
well okay preaching is part

487
00:33:11,580 --> 00:33:14,679
but preaching is effective where

488
00:33:14,679 --> 00:33:19,600
there's adequate dog that is to say
where we really understand the doctrine

489
00:33:19,600 --> 00:33:20,940
of what we're preaching

490
00:33:20,940 --> 00:33:24,349
and so we have a little intellectual
heavy lifting to do

491
00:33:24,349 --> 00:33:27,359
what does it really cool to talk about

492
00:33:27,359 --> 00:33:33,659
hurting people what does it really cool
to talk about not hurting people

493
00:33:33,659 --> 00:33:39,809
or guaranteeing not hurt to people in
this complex environment in which

494
00:33:39,809 --> 00:33:43,619
one the robots cognition has to be

495
00:33:43,619 --> 00:33:48,799
reduced to level up our desire it should
not be listening to me when I D will

496
00:33:48,799 --> 00:33:50,220
tell it till

497
00:33:50,220 --> 00:33:54,740
it shouldn't be informing people at my
location when I haven't said it can't

498
00:33:54,740 --> 00:34:01,029
and so on but where that dialogue cannot
possibly consistent punching okay or no

499
00:34:01,029 --> 00:34:03,899
on dialog boxes on something every

500
00:34:03,899 --> 00:34:09,510
tenths of a second through a lifetime we
need to understand what services can be

501
00:34:09,510 --> 00:34:11,389
safely offered and which ones

502
00:34:11,389 --> 00:34:14,990
can't or rather how service design

503
00:34:14,990 --> 00:34:21,849
itself must be altered in order to
produce safety for users

504
00:34:21,849 --> 00:34:25,700
location directed or location

505
00:34:25,700 --> 00:34:29,330
aware or location based services

506
00:34:29,330 --> 00:34:34,500
are terribly important and terribly
dangers

507
00:34:34,500 --> 00:34:38,319
and the primary problem is the real time

508
00:34:38,319 --> 00:34:42,239
ascertainment above the location of
human beings

509
00:34:42,240 --> 00:34:45,569
by those with power

510
00:34:45,569 --> 00:34:50,159
it does very little good in other words
to describe regulatory approaches to

511
00:34:50,159 --> 00:34:51,859
such services

512
00:34:51,859 --> 00:34:55,719
because the regulatory approach will
always be engineered by government to

513
00:34:55,720 --> 00:34:56,290
say

514
00:34:56,290 --> 00:34:59,410
you shouldn't do this unless you're us

515
00:34:59,410 --> 00:35:02,770
or you shouldn't do this unless we want
you to know

516
00:35:02,770 --> 00:35:06,359
or you shouldn't do this unless there's
a court order or other

517
00:35:06,359 --> 00:35:11,180
authoritative communication telling you
to start turning over real time location

518
00:35:11,180 --> 00:35:14,089
data about human beings

519
00:35:14,089 --> 00:35:18,609
the very senior US government official
who told me back in March

520
00:35:18,609 --> 00:35:22,500
well we've learned now that we need to
have a robust social graph

521
00:35:22,500 --> 00:35:25,869
up the United States

522
00:35:25,869 --> 00:35:30,380
is reflecting the learning %uh all the
governments on earth

523
00:35:30,380 --> 00:35:35,560
in the last 14 months well with great
shot yes they all discovered

524
00:35:35,560 --> 00:35:38,670
that what they wanted was a robust
social graph

525
00:35:38,670 --> 00:35:42,060
their societies you understand the
course that we could put it in plain

526
00:35:42,060 --> 00:35:43,509
English for the people around us

527
00:35:43,510 --> 00:35:47,540
we could say this means the United
States government intends to keep a list

528
00:35:47,540 --> 00:35:49,509
that everybody every American

529
00:35:49,510 --> 00:35:53,650
knows that's not what we used to

530
00:35:53,650 --> 00:35:57,109
quaintly refer to as a free society

531
00:35:57,109 --> 00:36:02,569
in fact that's what I would call a
dangerous neighborhood

532
00:36:02,569 --> 00:36:06,960
and in that dangerous neighborhood which
it may not be possible to prevent

533
00:36:06,960 --> 00:36:11,240
ourselves from living in unless we learn
how to exercise democracy

534
00:36:11,240 --> 00:36:14,480
really effectively this

535
00:36:14,480 --> 00:36:17,640
which is gonna mean a lot of preaching
and a lot of teaching

536
00:36:17,640 --> 00:36:21,368
but in that very dangerous neighborhood
we have to understand that things that

537
00:36:21,369 --> 00:36:26,369
inform at headquarters where we or

538
00:36:26,369 --> 00:36:30,440
Sheree yes problems

539
00:36:30,440 --> 00:36:33,700
I must admit that I find it kind of

540
00:36:33,700 --> 00:36:36,750
reassuring how naively

541
00:36:36,750 --> 00:36:40,140
confident Americans or

542
00:36:40,140 --> 00:36:43,080
as I grew up to manhood last are
traveling around the world I discovered

543
00:36:43,080 --> 00:36:46,770
the most im indeed all of the Society's
I went to accept my own

544
00:36:46,770 --> 00:36:52,830
people didn't really think that what
they said on the telephone was pride

545
00:36:52,830 --> 00:36:57,089
my friends in the Soviet Union were
particularly this

546
00:36:57,089 --> 00:37:03,400
I was too when I lived every in the late
seventies

547
00:37:03,400 --> 00:37:06,799
what what seems to me show amazing

548
00:37:06,799 --> 00:37:10,430
is that it is possible to show people
think it's a you've got a personal

549
00:37:10,430 --> 00:37:11,118
assistant

550
00:37:11,119 --> 00:37:15,390
inside this object and you didn't talk
to her

551
00:37:15,390 --> 00:37:18,859
her of course in English

552
00:37:18,859 --> 00:37:22,770
and say whatever you want and we'll take
it back to some warehouse data center

553
00:37:22,770 --> 00:37:27,279
somewhere and then we'll send you an
answer and tell you what to do

554
00:37:27,279 --> 00:37:34,279
if the KGB had tried this it would not
have worked

555
00:37:37,930 --> 00:37:44,930
but for Count Dracula the king and
beyond that it was snapped extraordinary

556
00:37:45,660 --> 00:37:50,220
and very worrisome because how we gonna
take it away from people

557
00:37:50,220 --> 00:37:55,299
right I will show you know you should go
back to not wanting that

558
00:37:55,299 --> 00:37:58,819
anymore because truth it's the KGB
inside your mind

559
00:37:58,819 --> 00:38:02,109
because you contributing everything

560
00:38:02,109 --> 00:38:06,249
that you would ever say to anybody who
was helping you to a great big database

561
00:38:06,249 --> 00:38:11,400
everything located in the world and
beyond this is

562
00:38:11,400 --> 00:38:16,549
vicious not a thing you would expect to
have a hard time convince people that to

563
00:38:16,549 --> 00:38:18,160
do unless they were already doing it

564
00:38:18,160 --> 00:38:22,319
and there is an awful lot of effort
going into making people comfortable

565
00:38:22,319 --> 00:38:23,740
doing it right now

566
00:38:23,740 --> 00:38:28,059
which means that we're going to have to
have

567
00:38:28,059 --> 00:38:31,789
strong arguments and good technology

568
00:38:31,789 --> 00:38:38,789
and really powerful moral conviction now
obviously we can explain to people

569
00:38:40,279 --> 00:38:44,279
white you shouldn't leave your children
in the custody of robots

570
00:38:44,279 --> 00:38:47,599
that haven't been engineered never heard
him me

571
00:38:47,599 --> 00:38:53,700
we can do that it's gonna feel little
counterintuitive to people

572
00:38:53,700 --> 00:39:00,700
but we're gonna have to say will have to
remind people that the great imaginative

573
00:39:01,829 --> 00:39:04,239
literature about the king of the on dead

574
00:39:04,239 --> 00:39:09,559
tells us that he can't come into our
houses

575
00:39:09,559 --> 00:39:16,559
unless we invite him across the
threshold and we're gonna have to ask

576
00:39:17,650 --> 00:39:20,940
ourselves and parents everywhere

577
00:39:20,940 --> 00:39:24,359
you don't want to invite him and to you

578
00:39:24,359 --> 00:39:31,359
not really but she it's all about
convenience

579
00:39:31,420 --> 00:39:37,029
prettiness coolers and the section
massive technology and we know more

580
00:39:37,029 --> 00:39:37,989
about the sexiness

581
00:39:37,989 --> 00:39:42,359
technology anybody most diverse I speak
at least only for myself

582
00:39:42,359 --> 00:39:45,609
weren't for the sexiness a technology
that would be little sectionals

583
00:39:45,609 --> 00:39:52,609
notwithstanding

584
00:39:54,400 --> 00:39:58,769
notwithstanding which there is a time
when the evil is too beautiful

585
00:39:58,769 --> 00:40:01,910
and we have to do something

586
00:40:01,910 --> 00:40:06,499
as far as I'm concerned this isn't the
project

587
00:40:06,499 --> 00:40:10,799
it isn't other this is what we need to
do right now

588
00:40:10,799 --> 00:40:14,759
and then we'll be done with it
unfortunately this is a way of life

589
00:40:14,759 --> 00:40:21,230
retrofitting first law of robotics is
gonna take a long time

590
00:40:21,230 --> 00:40:28,029
custom building robots everyday without
it and enable more and more accustomed

591
00:40:28,029 --> 00:40:31,049
to the idea that you carry around a
brain that isn't yours

592
00:40:31,049 --> 00:40:34,309
and it thinks about you for other people

593
00:40:34,309 --> 00:40:37,430
that you have cognitive

594
00:40:37,430 --> 00:40:41,269
facilities that don't work for you in
your pocket all the time

595
00:40:41,269 --> 00:40:47,869
on the bed table every night that the
tracker is always there pretending to be

596
00:40:47,869 --> 00:40:48,529
a fall

597
00:40:48,529 --> 00:40:53,089
that you wearing

598
00:40:53,089 --> 00:40:56,420
via one wearing that binds

599
00:40:56,420 --> 00:41:03,420
us all to them it's really

600
00:41:03,700 --> 00:41:09,960
hard it's really hard we're gonna have
to be very committed

601
00:41:09,960 --> 00:41:13,539
to this this is the moving our part in
the freedom

602
00:41:13,539 --> 00:41:17,739
is the part we're gonna have to be
responsible for

603
00:41:17,739 --> 00:41:22,650
because there are billions of people on
earth

604
00:41:22,650 --> 00:41:27,589
who are going to be trapped and we'll
know why

605
00:41:27,589 --> 00:41:34,069
and we know how and we haven't quite
figured out how to describe it to them

606
00:41:34,069 --> 00:41:37,160
in ways that will help them stay safe

607
00:41:37,160 --> 00:41:40,339
but if we don't know

608
00:41:40,339 --> 00:41:43,880
it's going to be very dark

609
00:41:43,880 --> 00:41:47,109
and all that

610
00:41:47,109 --> 00:41:51,589
hopeful science fiction that

611
00:41:51,589 --> 00:41:55,400
came from our attempt to believe that we
could sink

612
00:41:55,400 --> 00:41:59,349
our way all to safety after building the
bomb

613
00:41:59,349 --> 00:42:02,609
will have turnout

614
00:42:02,609 --> 00:42:08,339
to be true enough about the bomb but not
so true

615
00:42:08,339 --> 00:42:12,680
about the robots we are becoming

616
00:42:12,680 --> 00:42:19,680
thank you

617
00:42:48,329 --> 00:42:50,499
thank you I would be happy to take some
questions

618
00:42:50,499 --> 00:42:54,739
have a couple small questions how long
you think it'll be before the taxing

619
00:42:54,739 --> 00:42:55,549
authorities

620
00:42:55,549 --> 00:43:00,150
start using the tracking devices to
identify the state's residence

621
00:43:00,150 --> 00:43:04,160
love people and symbols for

622
00:43:04,160 --> 00:43:10,328
state taxes well he as you will recall
when the EZ Pass first came

623
00:43:10,329 --> 00:43:14,729
in to use there were promises that it
would not be used to send people

624
00:43:14,729 --> 00:43:19,430
speeding tickets by arithmetic
calculations a promise which lasted

625
00:43:19,430 --> 00:43:22,109
roughly a decade before it began to be
broken

626
00:43:22,109 --> 00:43:25,348
on I do not think that

627
00:43:25,349 --> 00:43:28,619
that show wall general magnet to

628
00:43:28,619 --> 00:43:33,119
on the to use I'll automated cameras for

629
00:43:33,119 --> 00:43:36,519
many kinds %uh enforcement are coming

630
00:43:36,519 --> 00:43:39,989
at high speed attraction for shipment um

631
00:43:39,989 --> 00:43:43,690
is at least a a possible a

632
00:43:43,690 --> 00:43:46,900
it long-term use in certain kinds very
simple data mining

633
00:43:46,900 --> 00:43:51,089
well the likely that I think is that
some jurisdictions somewhere will start

634
00:43:51,089 --> 00:43:51,609
doing it

635
00:43:51,609 --> 00:43:54,779
at some point you are aware that New
York City %uh

636
00:43:54,779 --> 00:43:58,400
has been known to send inspectors at
christmastime across the malls

637
00:43:58,400 --> 00:44:02,049
on the other side of the Hudson River a
to take pictures in people's license

638
00:44:02,049 --> 00:44:03,349
plates you are shopping

639
00:44:03,349 --> 00:44:06,489
in New Jersey for Christmas haul

640
00:44:06,489 --> 00:44:09,940
at possible loss and use taxes to New
York State

641
00:44:09,940 --> 00:44:14,210
are it will eventually particularly once
Near Field Communication inside the

642
00:44:14,210 --> 00:44:15,759
phone is how you pay

643
00:44:15,759 --> 00:44:19,229
up be possible to give the jury the
geographical

644
00:44:19,229 --> 00:44:23,650
reference for every payment and
associate taxes with it on an automatic

645
00:44:23,650 --> 00:44:24,349
basis

646
00:44:24,349 --> 00:44:27,400
how long out in the future that is I
don't know

647
00:44:27,400 --> 00:44:31,099
but it's not more than double the time
it takes before somebody begins trying

648
00:44:31,099 --> 00:44:34,539
and wire back doors not illegal

649
00:44:34,539 --> 00:44:38,839
in welded shut devices they seem to be a
great sources

650
00:44:38,839 --> 00:44:42,269
security risk in am

651
00:44:42,269 --> 00:44:46,430
on safety well it's not illegal to make
bad and secure software

652
00:44:46,430 --> 00:44:49,749
I'll if it had been of course mister
gates would not be the leading

653
00:44:49,749 --> 00:44:51,150
philanthropist in the world

654
00:44:51,150 --> 00:44:56,809
a hawaii has he just another inmate by
then has product liability law not kept

655
00:44:56,809 --> 00:44:57,979
up with Adam I have

656
00:44:57,979 --> 00:45:01,829
know why the lemon laws not been
successful well

657
00:45:01,829 --> 00:45:06,170
lemon laws a little different on world
are they ensure in general

658
00:45:06,170 --> 00:45:09,180
to the nature of the product liability
situations

659
00:45:09,180 --> 00:45:13,419
up when you buy things rip the shrink
wrap on things open the boxes

660
00:45:13,420 --> 00:45:16,729
sign the service agreements that I
contracting with you as

661
00:45:16,729 --> 00:45:21,069
indemnified himself against all possible
outcomes to the fullest extent

662
00:45:21,069 --> 00:45:24,109
%uh that the best possible legal
engineering can do

663
00:45:24,109 --> 00:45:28,769
we need to add some law as the New York
Times was recommending this morning I

664
00:45:28,769 --> 00:45:32,099
don't know that I agree particularly
with the editorial writer about

665
00:45:32,099 --> 00:45:35,969
where to go first up but we're going to
need to make

666
00:45:35,969 --> 00:45:40,519
privacy law are in some important ways
that we learn to make

667
00:45:40,519 --> 00:45:44,299
environmental law up under the nixon
administration

668
00:45:44,299 --> 00:45:48,779
a whole times and places I'm privacy is
an ecological problem

669
00:45:48,779 --> 00:45:52,390
%uh the the harm done by the robots

670
00:45:52,390 --> 00:45:56,140
is an ecological problem one of the
things that goes wrong

671
00:45:56,140 --> 00:45:59,910
is that most people including my
students tend to think privacy is

672
00:45:59,910 --> 00:46:01,640
between me and somebody

673
00:46:01,640 --> 00:46:05,269
they don't recognize it when they take a
photograph on Facebook they're doing an

674
00:46:05,269 --> 00:46:07,009
ecological harm to a third party

675
00:46:07,009 --> 00:46:10,109
when we begin to understand more
concrete leave

676
00:46:10,109 --> 00:46:15,359
third-party effects %uh the way we
destroy our own privacy with the robots

677
00:46:15,359 --> 00:46:19,229
and therefore harm other people whether
it's our children our friends our

678
00:46:19,229 --> 00:46:21,130
families our churches or whatever

679
00:46:21,130 --> 00:46:24,249
%uh then the legal situation will
clarify

680
00:46:24,249 --> 00:46:28,279
because it will become evident as it
became evident about tobacco

681
00:46:28,279 --> 00:46:32,579
the part of the difficulty is the third
party consequences that can't be dealt

682
00:46:32,579 --> 00:46:33,079
with

683
00:46:33,079 --> 00:46:36,229
by signing a contract that says we won't
sue

684
00:46:36,229 --> 00:46:41,710
so most as your examples

685
00:46:41,710 --> 00:46:45,700
%uh safety have been specifically
privacy are there

686
00:46:45,700 --> 00:46:48,849
other areas that safety that you have in
mind

687
00:46:48,849 --> 00:46:52,779
when we begin to think about the tax on
animals

688
00:46:52,779 --> 00:46:55,839
up services we will discover harms

689
00:46:55,839 --> 00:46:59,160
that are the result up each other
combines

690
00:46:59,160 --> 00:47:03,420
a failures of privacy i teach in my
classroom I'm gonna be very brisk about

691
00:47:03,420 --> 00:47:04,430
this but if you look

692
00:47:04,430 --> 00:47:07,868
elsewhere on the net you'll see larger
explanations what I mean

693
00:47:07,869 --> 00:47:11,239
I do July classroom the privacy is
really three things

694
00:47:11,239 --> 00:47:14,420
at minimum secrecy that is messages

695
00:47:14,420 --> 00:47:17,579
whose content is obscure to all buttons
and parties

696
00:47:17,579 --> 00:47:22,029
anonymity that is messages whose content
is available

697
00:47:22,029 --> 00:47:26,150
up but whose sending and receiving
points are obscure to all but permitted

698
00:47:26,150 --> 00:47:26,849
parties

699
00:47:26,849 --> 00:47:31,359
and autonomy which is our ability to
decide and act in the world

700
00:47:31,359 --> 00:47:34,578
30 violations in secrecy in anonymity

701
00:47:34,579 --> 00:47:37,739
um when you take those as

702
00:47:37,739 --> 00:47:41,579
a the taxonomy up privacy constituents

703
00:47:41,579 --> 00:47:45,710
and then begin to work out in the world
the nature of the dangers through the

704
00:47:45,710 --> 00:47:46,690
Services

705
00:47:46,690 --> 00:47:51,569
you get too many different kinds harms
up there are physical harms that result

706
00:47:51,569 --> 00:47:52,019
from

707
00:47:52,019 --> 00:47:56,219
intentional violent conduct mostly
undertaken by states

708
00:47:56,219 --> 00:47:59,960
but not only undertaken by state
sometimes also my criminal actors

709
00:47:59,960 --> 00:48:01,460
non-state actors

710
00:48:01,460 --> 00:48:04,719
on the basis of information available in
the net

711
00:48:04,719 --> 00:48:08,930
as a result of harmful behavior by
robots or androids

712
00:48:08,930 --> 00:48:13,569
I'll there are also examples elec atomic
arms pricing harms

713
00:48:13,569 --> 00:48:16,589
are insurance harms and so on

714
00:48:16,589 --> 00:48:20,359
there are harms in politics that result
in the

715
00:48:20,359 --> 00:48:23,430
shelling up protected speech or other

716
00:48:23,430 --> 00:48:27,479
chilling love religious activity as a
consequence in the misbehavior

717
00:48:27,479 --> 00:48:30,779
robots or androids a whole range harms

718
00:48:30,779 --> 00:48:34,910
I wouldn't wanna go so far as to say
that every conceivable kind of harm

719
00:48:34,910 --> 00:48:38,479
other the law protects people against is
done by robots but many

720
00:48:38,479 --> 00:48:43,039
are and part of our difficulty in
elucidating our arguments

721
00:48:43,039 --> 00:48:46,400
is going to have to be a more
complicated analysis

722
00:48:46,400 --> 00:48:50,900
and taxonomy of the nature

723
00:48:50,900 --> 00:48:55,369
a I myself isn't working on a open saws
personal robot project now

724
00:48:55,369 --> 00:48:59,319
which will grant people to in out
further improve the safety of the robot

725
00:48:59,319 --> 00:49:02,890
they are going to use I have so a I
believe that

726
00:49:02,890 --> 00:49:07,098
it's the way to go to in orange maybe
even in the first drawer provides

727
00:49:07,099 --> 00:49:10,319
cell I would like you know your opinion
on that well

728
00:49:10,319 --> 00:49:14,339
obviously technology we make ourselves
because it's safe

729
00:49:14,339 --> 00:49:18,369
and throwing two things is a fundamental
tool

730
00:49:18,369 --> 00:49:21,890
%uh what we're going to need to
accomplish it was what I had in mind in

731
00:49:21,890 --> 00:49:25,660
calling for freedom march to try and
deal with routing and serving

732
00:49:25,660 --> 00:49:29,049
and we certainly need client-side safety

733
00:49:29,049 --> 00:49:32,369
or mobile site safety and we have

734
00:49:32,369 --> 00:49:35,769
%uh if anything a world in which I i do
believe

735
00:49:35,769 --> 00:49:39,609
if we marshall are arguments and build
our coalitions correctly

736
00:49:39,609 --> 00:49:44,239
with an opportunity actually get it done
as I say I think we might be able to

737
00:49:44,239 --> 00:49:44,849
abate

738
00:49:44,849 --> 00:49:49,539
the legal threats against us for doing
it in DMCA and other places

739
00:49:49,539 --> 00:49:53,519
I think we will be able to bring power
some influence to bear

740
00:49:53,519 --> 00:49:57,559
with the network operators to move them
in other directions

741
00:49:57,559 --> 00:50:02,150
I think old-line IT companies that sell
to enterprises and it turned out to be

742
00:50:02,150 --> 00:50:03,119
at least this

743
00:50:03,119 --> 00:50:07,249
interested in getting their stacks into
other people's mobile handsets as we are

744
00:50:07,249 --> 00:50:09,049
for various reasons

745
00:50:09,049 --> 00:50:12,710
I think there are a number of possible
allies for us and I think

746
00:50:12,710 --> 00:50:16,359
as a free software movement that has
been careful to look for coalitions it

747
00:50:16,359 --> 00:50:17,140
could build

748
00:50:17,140 --> 00:50:21,788
we will be able to bill so at the end of
the day of course the safe technology is

749
00:50:21,789 --> 00:50:28,369
what we make and share ourselves

750
00:50:28,369 --> 00:50:32,549
if we do manage to a put our own
software

751
00:50:32,549 --> 00:50:36,239
inside the robots a do we still have the
the problem that no

752
00:50:36,239 --> 00:50:40,319
now you have for good and unapproved
devices attached the network can you

753
00:50:40,319 --> 00:50:41,680
don't have the service from

754
00:50:41,680 --> 00:50:44,698
from the network that you don't control
no I don't think that should be the

755
00:50:44,699 --> 00:50:47,239
outcome we were careful in GPL 3

756
00:50:47,239 --> 00:50:50,799
up to make rules which prohibited the
network operator from booting

757
00:50:50,799 --> 00:50:53,880
devices from the network simply because
they were modified

758
00:50:53,880 --> 00:50:58,319
if people had a and if people had taken
our approach to that seriously I say

759
00:50:58,319 --> 00:50:59,049
again

760
00:50:59,049 --> 00:51:02,779
half a decade ago we could spared
ourselves a lot of this nonsense

761
00:51:02,779 --> 00:51:06,400
mister Stallman was right comma as usual

762
00:51:06,400 --> 00:51:10,119
comma about that thank the the correct

763
00:51:10,119 --> 00:51:15,299
the correct answer here is that we told
AT&T and the modified order in judgment

764
00:51:15,299 --> 00:51:20,069
at the end up the anti-trust cycle the
broke up eighteen 18 1980

765
00:51:20,069 --> 00:51:23,259
that it had to permit the attachment

766
00:51:23,259 --> 00:51:28,069
%uh on the certified but nonetheless non
harmful devices to the network

767
00:51:28,069 --> 00:51:32,380
this same proposition has to be the case
with respect 20 mile that works that's

768
00:51:32,380 --> 00:51:35,509
all if the device is harmful you can
boot it but if it isn't

769
00:51:35,509 --> 00:51:38,549
actually harming the networks operation
then

770
00:51:38,549 --> 00:51:42,269
there's no legal right to keep it off

771
00:51:42,269 --> 00:51:45,539
that's the direction in which we need to
go

772
00:51:45,539 --> 00:51:49,130
a recovery for how do we get there well

773
00:51:49,130 --> 00:51:53,910
one what one one idea is we get more
free software under licenses that

774
00:51:53,910 --> 00:51:55,178
require that

775
00:51:55,179 --> 00:51:59,449
until they can't do what they do without
us that's the

776
00:51:59,449 --> 00:52:03,789
strategy we used in some other contexts
another possibility is we get

777
00:52:03,789 --> 00:52:07,230
telecoms regulators to understand that
they have the same

778
00:52:07,230 --> 00:52:11,549
reason for that that they had for
insisting on that ruling the air

779
00:52:11,549 --> 00:52:15,190
the wired network another possibility is
we have to get

780
00:52:15,190 --> 00:52:19,049
our elected representatives to know
that's what we want

781
00:52:19,049 --> 00:52:22,179
the minute they know it's what we what
enough not to vote for them unless we

782
00:52:22,179 --> 00:52:23,279
get it

783
00:52:23,279 --> 00:52:26,799
well that so

784
00:52:26,799 --> 00:52:31,619
somewhat related to that getting the
first love robotics into

785
00:52:31,619 --> 00:52:35,880
electronic devices definitely important
but how do we get that and corporate

786
00:52:35,880 --> 00:52:37,160
laws well

787
00:52:37,160 --> 00:52:40,499
Udaltsov course because it's like in
people

788
00:52:40,499 --> 00:52:44,009
and while you may not believe the
corporations are people you can

789
00:52:44,009 --> 00:52:46,239
certainly believe they're made of people

790
00:52:46,239 --> 00:52:49,299
I'll this problem how to get people to
behave with

791
00:52:49,299 --> 00:52:54,309
intrinsically virtuous conduct an
intention is a long-standing

792
00:52:54,309 --> 00:53:00,779
human problem a its

793
00:53:00,779 --> 00:53:07,039
the great English lawyer Edward Cooke
said %uh into a in his famous reports

794
00:53:07,039 --> 00:53:10,079
corporations cannot be excommunicated

795
00:53:10,079 --> 00:53:13,640
because they have no souls %uh

796
00:53:13,640 --> 00:53:17,009
a point which while it was in there a
technical point occasionally she was

797
00:53:17,009 --> 00:53:19,029
offering it is also largely

798
00:53:19,029 --> 00:53:23,940
true he wants them %uh the the the king
of the undead Count Dracula was

799
00:53:23,940 --> 00:53:26,099
particularly good as you notice

800
00:53:26,099 --> 00:53:30,440
%uh in getting corporations to do evil
things through beauty

801
00:53:30,440 --> 00:53:35,059
he got and music industry %uh in the
world almost to destroy itself under the

802
00:53:35,059 --> 00:53:37,469
pretense that he was saving it

803
00:53:37,469 --> 00:53:41,710
his ghost in charge of the Apple
Computer Corporation has been doing it

804
00:53:41,710 --> 00:53:43,699
for the magazine publishers in the world

805
00:53:43,699 --> 00:53:48,210
even when he was already it temporarily
descended into the gray

806
00:53:48,210 --> 00:53:52,469
I'll the the problem getting the
corporations to do it right

807
00:53:52,469 --> 00:53:56,509
%uh is not a problem that we can deal
with by law yet how a long history of

808
00:53:56,509 --> 00:53:57,079
the Regal

809
00:53:57,079 --> 00:54:00,690
legal regulation of collective
enterprise is any indication

810
00:54:00,690 --> 00:54:04,279
I think we really are trying to engineer
for safety

811
00:54:04,279 --> 00:54:11,279
despite the natural tendency in human
beings to do role

812
00:54:11,969 --> 00:54:15,059
and I had a regulatory question again

813
00:54:15,059 --> 00:54:18,029
someone related to summon the previous
question is probably going to be both

814
00:54:18,029 --> 00:54:21,960
the last regulatory question in the last
night on regulatory questions ago

815
00:54:21,960 --> 00:54:26,219
and so and there appear to be times when

816
00:54:26,219 --> 00:54:29,269
legislatively there's a sorta

817
00:54:29,269 --> 00:54:32,339
deterministic acquiescence to I'll

818
00:54:32,339 --> 00:54:35,670
technology functions and so we see this
for example in

819
00:54:35,670 --> 00:54:40,170
and exceptions that are made to
something like the ECPA

820
00:54:40,170 --> 00:54:44,130
where the legislation says we make an
exception when

821
00:54:44,130 --> 00:54:49,999
certain kinds of data are required to be
collected in order to provision service

822
00:54:49,999 --> 00:54:53,650
and in an acceptable manner what have
you

823
00:54:53,650 --> 00:54:57,759
and and this seems to create a sort of
perverse incentive for

824
00:54:57,759 --> 00:55:00,880
the development and technology that

825
00:55:00,880 --> 00:55:05,839
and promotes the promulgation in
developing a technology that collects

826
00:55:05,839 --> 00:55:07,369
more information

827
00:55:07,369 --> 00:55:11,009
necessarily and and

828
00:55:11,009 --> 00:55:15,420
a lot I wanted to know what your
thoughts were deserving a maybe

829
00:55:15,420 --> 00:55:17,019
historically contextualized

830
00:55:17,019 --> 00:55:20,619
shashi I don't think that the problem
love collecting two lectures now

831
00:55:20,619 --> 00:55:22,309
primarily a byproduct

832
00:55:22,309 --> 00:55:26,390
careless regulation the problem love
collecting too much arises from the fact

833
00:55:26,390 --> 00:55:28,589
that the future of capitalism

834
00:55:28,589 --> 00:55:32,519
is in data mining and control its
customers

835
00:55:32,519 --> 00:55:38,259
when when way when capitalist societies
begin to depend more on consumption

836
00:55:38,259 --> 00:55:42,279
than they do on production for their
overall economic health

837
00:55:42,279 --> 00:55:46,299
and the GDP of the United States has
been primarily consumption

838
00:55:46,299 --> 00:55:52,130
for more than 15 years now then knowing
how to control consumers

839
00:55:52,130 --> 00:55:57,109
becomes as important as knowing how to
control productive machines was

840
00:55:57,109 --> 00:56:01,969
when the GDP was still primarily based
upon production rather than consumption

841
00:56:01,969 --> 00:56:05,119
what is happening is that we are
automating and

842
00:56:05,119 --> 00:56:10,710
instrumental the portion of the a condom
in which is most powerfully important

843
00:56:10,710 --> 00:56:13,859
namely consumption collecting
information

844
00:56:13,859 --> 00:56:19,630
about consumers is the same thing as
knowing how the mill machines worked

845
00:56:19,630 --> 00:56:24,979
to the capitalist the 19th century so
what we are confronting is not the

846
00:56:24,979 --> 00:56:30,049
accident the perverse incentives created
as a byproduct the unintended

847
00:56:30,049 --> 00:56:31,109
consequences

848
00:56:31,109 --> 00:56:34,119
regulation we're facing the fact that

849
00:56:34,119 --> 00:56:39,499
do with the wrong thing is the basis for
future economic growth

850
00:56:39,499 --> 00:56:42,979
we're not trying to destroy future
economic growth

851
00:56:42,979 --> 00:56:46,739
we're trying to compromise amal values

852
00:56:46,739 --> 00:56:50,499
that's what regulation is it's a mesh
process

853
00:56:50,499 --> 00:56:55,429
of course it has unintended consequences
naturally creates perverse incentives

854
00:56:55,429 --> 00:57:01,179
because each perfect but understanding
the objective has to be clear

855
00:57:01,179 --> 00:57:06,259
productive capitalism met ecological
constraints

856
00:57:06,259 --> 00:57:10,009
it dirty the water too much dirty air
too much

857
00:57:10,009 --> 00:57:14,640
he created too many difficulties that
had to be constrained all

858
00:57:14,640 --> 00:57:18,900
as negative externalities produced by
production

859
00:57:18,900 --> 00:57:22,739
the attempt to regiment and govern
consumption

860
00:57:22,739 --> 00:57:26,489
through data mining races ecological
problems

861
00:57:26,489 --> 00:57:29,239
we must self thank you its

