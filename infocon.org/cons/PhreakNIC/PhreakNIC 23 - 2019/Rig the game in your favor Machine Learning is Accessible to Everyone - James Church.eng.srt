1
00:00:02,590 --> 00:00:08,240
are we doing today whoo I mean join them

2
00:00:06,140 --> 00:00:08,990
how does your church better know that

3
00:00:08,240 --> 00:00:13,940
I'm here

4
00:00:08,990 --> 00:00:15,200
Oh y'all are so kind so like first the

5
00:00:13,940 --> 00:00:16,430
first free kick I don't remember my

6
00:00:15,200 --> 00:00:19,698
first Freaknik but the first freaking

7
00:00:16,430 --> 00:00:22,279
week I spoke at was probably 14 and then

8
00:00:19,699 --> 00:00:25,130
I spoke at 1 3 years ago so this is my

9
00:00:22,279 --> 00:00:26,720
third time to speak at Freaknik a little

10
00:00:25,130 --> 00:00:28,820
bit about myself my name is dr. James

11
00:00:26,720 --> 00:00:30,500
Church I am an associate professor in

12
00:00:28,820 --> 00:00:35,630
Austin Peay State University

13
00:00:30,500 --> 00:00:37,730
I earned my PhD a few years ago I really

14
00:00:35,630 --> 00:00:39,650
love to teach I really love to to

15
00:00:37,730 --> 00:00:41,959
explore new technologies and then try to

16
00:00:39,650 --> 00:00:43,750
put them in front of people who might be

17
00:00:41,960 --> 00:00:46,430
able to use them and extend them beyond

18
00:00:43,750 --> 00:00:50,360
whatever small little thing I can create

19
00:00:46,430 --> 00:00:52,820
in a short amount of time that is that

20
00:00:50,360 --> 00:00:54,350
is where I'm coming from as an educator

21
00:00:52,820 --> 00:00:58,030
so we're going to start off first with

22
00:00:54,350 --> 00:01:00,380
the demonstration this is a talk about

23
00:00:58,030 --> 00:01:02,120
machine learning and neural networks and

24
00:01:00,380 --> 00:01:08,390
I wanted to do a live neural network

25
00:01:02,120 --> 00:01:12,320
demonstration while we're here so right

26
00:01:08,390 --> 00:01:14,929
here I've got I've got some poker data

27
00:01:12,320 --> 00:01:18,490
and let's talk about poker for a little

28
00:01:14,929 --> 00:01:22,880
bit if you're playing five-card stud

29
00:01:18,490 --> 00:01:24,829
that means you get a you're dumb let's

30
00:01:22,880 --> 00:01:27,020
say we're your five-card five-card draw

31
00:01:24,829 --> 00:01:28,759
there we go that's more likely and how

32
00:01:27,020 --> 00:01:30,490
many different possible hands are there

33
00:01:28,759 --> 00:01:35,710
in five-card draw

34
00:01:30,490 --> 00:01:38,119
first off how many cards are in a deck -

35
00:01:35,710 --> 00:01:40,389
and I'm already lost signal there we go

36
00:01:38,119 --> 00:01:46,240
let's bring it back

37
00:01:40,390 --> 00:01:48,820
and this this adapter is finicky there

38
00:01:46,240 --> 00:01:51,479
we go I just have to be very careful and

39
00:01:48,820 --> 00:01:54,990
not touch it and not keep this wire so

40
00:01:51,480 --> 00:01:57,490
if I were to let's start up Python and

41
00:01:54,990 --> 00:01:58,990
so if there's 52 charts in the neck

42
00:01:57,490 --> 00:02:01,390
there you go I draw you one card and

43
00:01:58,990 --> 00:02:03,580
then I draw you a second card and then I

44
00:02:01,390 --> 00:02:07,900
draw you a third card drawing you up

45
00:02:03,580 --> 00:02:11,940
fourth card and then I draw you a fifth

46
00:02:07,900 --> 00:02:15,550
card there are three hundred and eleven

47
00:02:11,940 --> 00:02:18,040
million possible poker hands that you

48
00:02:15,550 --> 00:02:20,440
should be that is possible to be dealt

49
00:02:18,040 --> 00:02:24,400
in a game of poker it's a very large

50
00:02:20,440 --> 00:02:28,359
variety and what I have here and I

51
00:02:24,400 --> 00:02:32,020
pulled this data set off the University

52
00:02:28,360 --> 00:02:36,190
of Michigan you see a machine learning

53
00:02:32,020 --> 00:02:38,050
data set repository what this is is a

54
00:02:36,190 --> 00:02:41,730
poker hand database so let's just look

55
00:02:38,050 --> 00:02:41,730
at the poker hand training

56
00:02:44,690 --> 00:02:49,370
poker hand training data center what

57
00:02:47,150 --> 00:02:52,460
you're going to see on the screen is a

58
00:02:49,370 --> 00:02:56,390
whole bunch of rows but there are 11

59
00:02:52,460 --> 00:02:58,190
numbers across the row so what we have

60
00:02:56,390 --> 00:03:00,859
here is a 1 and a 10

61
00:02:58,190 --> 00:03:02,750
now that represents the first number is

62
00:03:00,860 --> 00:03:05,480
a one that represents a suit and the

63
00:03:02,750 --> 00:03:07,730
tenth represents the card was a 10 value

64
00:03:05,480 --> 00:03:10,310
then we have a 1 and an 11

65
00:03:07,730 --> 00:03:10,760
a 1 and a 13 a 1 and a 12 and a 1 and a

66
00:03:10,310 --> 00:03:14,540
1

67
00:03:10,760 --> 00:03:18,500
I'm sorry a 12 see other a 112 and then

68
00:03:14,540 --> 00:03:20,720
a 1 and a 1 and then a 9 and the 9 on

69
00:03:18,500 --> 00:03:23,840
the end that's the 11th number that

70
00:03:20,720 --> 00:03:28,340
represents the hand at the hand that was

71
00:03:23,840 --> 00:03:31,130
dumped all right so if I look at hooker

72
00:03:28,340 --> 00:03:33,080
hand dot names this is another file that

73
00:03:31,130 --> 00:03:36,470
comes with the data set always look up

74
00:03:33,080 --> 00:03:41,180
the the metadata that comes with your

75
00:03:36,470 --> 00:03:43,430
particular data set we're gonna scroll

76
00:03:41,180 --> 00:03:46,220
down a little bit we actually get to see

77
00:03:43,430 --> 00:03:48,950
what the what these attributes are

78
00:03:46,220 --> 00:03:52,580
attribute 1 hearts spades diamonds clubs

79
00:03:48,950 --> 00:03:55,040
and then attribute to ace all the way to

80
00:03:52,580 --> 00:03:57,830
King if I scroll down to the bottom you

81
00:03:55,040 --> 00:04:00,530
can see that this alternates suit had a

82
00:03:57,830 --> 00:04:03,910
suit number suit number suit number all

83
00:04:00,530 --> 00:04:06,800
the way down until we get to number 11

84
00:04:03,910 --> 00:04:08,600
where we get the class of the poker

85
00:04:06,800 --> 00:04:11,360
hands and we see that there are 10

86
00:04:08,600 --> 00:04:13,310
visible classes of poker hands nothing

87
00:04:11,360 --> 00:04:17,299
in the hand not recognized since poker

88
00:04:13,310 --> 00:04:19,430
hand what was the you know the one that

89
00:04:17,298 --> 00:04:23,270
we put up on the screen here that first

90
00:04:19,430 --> 00:04:24,970
line that was a that was a nine by I go

91
00:04:23,270 --> 00:04:27,740
back to it real quick that first line

92
00:04:24,970 --> 00:04:29,090
you can see it was a nine on the end so

93
00:04:27,740 --> 00:04:30,380
if you just pick out those numbers and

94
00:04:29,090 --> 00:04:32,979
those of you can play a little bit of

95
00:04:30,380 --> 00:04:36,280
Poker that would be a royal flush and

96
00:04:32,980 --> 00:04:36,280
sure enough

97
00:04:37,839 --> 00:04:44,360
number nine is indeed art our Royal

98
00:04:40,909 --> 00:04:46,399
Flush category so the idea behind of

99
00:04:44,360 --> 00:04:48,800
machine learning neural networks is that

100
00:04:46,399 --> 00:04:50,659
we are going to create a whole bunch of

101
00:04:48,800 --> 00:04:54,879
inputs and a whole bunch of outputs and

102
00:04:50,659 --> 00:04:57,199
then we're going to train a model and

103
00:04:54,879 --> 00:04:58,460
whenever we first start off at the model

104
00:04:57,199 --> 00:04:59,899
the model is just going to be a bunch of

105
00:04:58,460 --> 00:05:01,729
random numbers that don't really make a

106
00:04:59,899 --> 00:05:05,240
whole lot of sense and then we're going

107
00:05:01,729 --> 00:05:07,909
to pass it pass our inputs through the

108
00:05:05,240 --> 00:05:10,759
model which starts off initially random

109
00:05:07,909 --> 00:05:13,159
and pass and do the inputs and try to

110
00:05:10,759 --> 00:05:21,020
retrain the model based on this so let's

111
00:05:13,159 --> 00:05:23,300
bring up my code just the mere fact of

112
00:05:21,020 --> 00:05:27,979
typing is causing this wire to shake so

113
00:05:23,300 --> 00:05:30,830
I'm have to type a little bit softer so

114
00:05:27,979 --> 00:05:33,409
here is our a little short Python

115
00:05:30,830 --> 00:05:37,188
program it's only 39 lines we're using

116
00:05:33,409 --> 00:05:39,229
tensorflow and not pi and I've got we're

117
00:05:37,189 --> 00:05:41,809
going to read in the files I do a lot of

118
00:05:39,229 --> 00:05:44,209
data massaging you can see where I'm

119
00:05:41,809 --> 00:05:50,479
alternating the lines here so I get

120
00:05:44,209 --> 00:05:54,259
everything on a scale of 0 to 1 we cast

121
00:05:50,479 --> 00:05:56,748
that to do 32-bit floats it turns out

122
00:05:54,259 --> 00:05:59,479
that a 32-bit float is for our purposes

123
00:05:56,749 --> 00:06:02,180
just as good as the 64 bits float and we

124
00:05:59,479 --> 00:06:04,039
work a little bit faster so a nice

125
00:06:02,180 --> 00:06:06,319
little perk of converting everything to

126
00:06:04,039 --> 00:06:08,808
32-bit floats and then we're going to

127
00:06:06,319 --> 00:06:11,029
perform the knurl we're going to build a

128
00:06:08,809 --> 00:06:14,059
neural network model we're going to have

129
00:06:11,029 --> 00:06:17,089
a 20 node hidden layer we're going to

130
00:06:14,059 --> 00:06:20,839
have a dropout of 0.2 every time we pass

131
00:06:17,089 --> 00:06:22,969
through a layer we're going to drop 0.2

132
00:06:20,839 --> 00:06:26,269
percent of our results and then we are

133
00:06:22,969 --> 00:06:27,469
going to pass it on to our output layer

134
00:06:26,269 --> 00:06:31,909
which is going to be the final

135
00:06:27,469 --> 00:06:34,129
categorization and after we build the

136
00:06:31,909 --> 00:06:36,979
model we compile it I'm using the sparse

137
00:06:34,129 --> 00:06:37,790
categorical cross entropy loss model and

138
00:06:36,979 --> 00:06:40,789
we are

139
00:06:37,790 --> 00:06:43,270
King for accuracy we're going to fit and

140
00:06:40,790 --> 00:06:47,510
then we are going to evaluate based on a

141
00:06:43,270 --> 00:06:50,210
test set now here's the thing I may not

142
00:06:47,510 --> 00:06:52,670
told you tensorflow is a really hard

143
00:06:50,210 --> 00:06:54,349
piece of software to install and so I

144
00:06:52,670 --> 00:06:56,330
don't have it installed on my laptop but

145
00:06:54,350 --> 00:06:59,480
I do have dr. so we're gonna get docker

146
00:06:56,330 --> 00:07:04,909
going and then use that so let's get dr.

147
00:06:59,480 --> 00:07:10,970
going so there we go let's get dr.

148
00:07:04,910 --> 00:07:13,070
Gellman docker images the best way to

149
00:07:10,970 --> 00:07:14,630
use tensorflow is to not even bother

150
00:07:13,070 --> 00:07:18,020
installing it on your computer just get

151
00:07:14,630 --> 00:07:19,760
the docker image let's go ahead and pull

152
00:07:18,020 --> 00:07:25,430
I've already got it pulled down

153
00:07:19,760 --> 00:07:30,920
let's do docker run IT this is going to

154
00:07:25,430 --> 00:07:35,930
be tensorflow tensorflow latest py 3 and

155
00:07:30,920 --> 00:07:39,230
then we are going to bash there we go

156
00:07:35,930 --> 00:07:42,230
this runs a this is a container running

157
00:07:39,230 --> 00:07:44,870
on top of my host operating system we

158
00:07:42,230 --> 00:07:47,930
now need to put our files over so

159
00:07:44,870 --> 00:07:50,090
whatever I I just type LS - al you can

160
00:07:47,930 --> 00:07:52,070
see that I've got it's just the standard

161
00:07:50,090 --> 00:07:53,960
root filesystem there's nothing on it

162
00:07:52,070 --> 00:07:56,810
this is a clean installation let's go

163
00:07:53,960 --> 00:08:00,310
ahead and move our files over so we'll

164
00:07:56,810 --> 00:08:03,920
do dr. copy we'll just do everything

165
00:08:00,310 --> 00:08:08,420
over to let's see here what are my

166
00:08:03,920 --> 00:08:12,010
docker PS is alright my my docker

167
00:08:08,420 --> 00:08:16,220
instance is named intelligent Allen

168
00:08:12,010 --> 00:08:18,289
there we go so let's do docker CP

169
00:08:16,220 --> 00:08:20,090
everything over to intelligent Allen

170
00:08:18,290 --> 00:08:22,540
we'll just drop it in into the root

171
00:08:20,090 --> 00:08:25,219
folder I'm gonna swap back over to my

172
00:08:22,540 --> 00:08:29,870
tensorflow environment now if I hit LS -

173
00:08:25,220 --> 00:08:31,610
al again I now get my script appearing

174
00:08:29,870 --> 00:08:33,080
in the docker container which you saw

175
00:08:31,610 --> 00:08:37,159
and now you can't see it anymore because

176
00:08:33,080 --> 00:08:39,510
I benched the winner feel free to stop

177
00:08:37,159 --> 00:08:41,939
me if I if you have any questions

178
00:08:39,510 --> 00:08:44,069
I I do tend to work for an environment

179
00:08:41,940 --> 00:08:46,649
where I have lots of students writing

180
00:08:44,070 --> 00:08:48,570
every word down they say on paper and so

181
00:08:46,649 --> 00:08:51,540
having you just stare back at me it's

182
00:08:48,570 --> 00:08:54,660
kind of like I like the interaction

183
00:08:51,540 --> 00:09:06,569
between what I do in front of the

184
00:08:54,660 --> 00:09:13,829
classroom and what the students do I

185
00:09:06,570 --> 00:09:15,779
just picked them arbitrarily oh I don't

186
00:09:13,829 --> 00:09:26,370
have the eye because it's a docker

187
00:09:15,779 --> 00:09:30,689
container and I can cut the file which

188
00:09:26,370 --> 00:09:32,850
docker go ahead what was that oh yeah I

189
00:09:30,690 --> 00:09:36,209
was using the Python 3 implementation of

190
00:09:32,850 --> 00:09:43,230
tensor flow so I'm using I'm using the

191
00:09:36,209 --> 00:09:47,219
tag latest - pi 3 so there we go the

192
00:09:43,230 --> 00:09:49,440
reason why I went with 20 and 11 I

193
00:09:47,220 --> 00:09:53,899
wanted to make sure that there were just

194
00:09:49,440 --> 00:09:59,430
more nodes in my layers than there were

195
00:09:53,899 --> 00:10:05,540
actual output categories just to provide

196
00:09:59,430 --> 00:10:09,000
a little bit of a rich soup good suit

197
00:10:05,540 --> 00:10:15,589
all right so let's let's start up the

198
00:10:09,000 --> 00:10:20,430
test simple dude there we go Python 3

199
00:10:15,589 --> 00:10:22,740
evaluate now I'm gonna be running 10,000

200
00:10:20,430 --> 00:10:24,329
epochs of this particular neural network

201
00:10:22,740 --> 00:10:28,800
and that may take the entire

202
00:10:24,329 --> 00:10:30,479
presentation so what we're gonna do is

203
00:10:28,800 --> 00:10:33,959
we're just gonna start it off and it's

204
00:10:30,480 --> 00:10:34,980
gonna go through the code knows the code

205
00:10:33,959 --> 00:10:36,268
wasn't really that long

206
00:10:34,980 --> 00:10:40,050
that's one of things I appreciate about

207
00:10:36,269 --> 00:10:42,329
tensor flow as soon as the it sticks a

208
00:10:40,050 --> 00:10:44,010
long time just to load the files and now

209
00:10:42,329 --> 00:10:45,209
we're getting it up and now you can see

210
00:10:44,010 --> 00:10:47,160
we're going through the

211
00:10:45,210 --> 00:10:50,400
there's my 10,000 right there we're on

212
00:10:47,160 --> 00:10:54,750
12 of 10,000 and you can watch over here

213
00:10:50,400 --> 00:10:55,980
the accuracy slowly creep up as we go

214
00:10:54,750 --> 00:10:59,310
through the presentation and hopefully

215
00:10:55,980 --> 00:11:01,080
that that accuracy will get pretty high

216
00:10:59,310 --> 00:11:03,150
right now we're at about forty eight

217
00:11:01,080 --> 00:11:05,810
eight percent accuracy after just a few

218
00:11:03,150 --> 00:11:08,280
beep Hawks we're gonna just let that

219
00:11:05,810 --> 00:11:11,339
take baking and we're gonna come back to

220
00:11:08,280 --> 00:11:13,880
it how about that hey let's talk about

221
00:11:11,340 --> 00:11:13,880
neural networks

222
00:11:18,190 --> 00:11:22,390
all right so I titled the talk rigging

223
00:11:20,770 --> 00:11:25,360
the game in your favor machine learning

224
00:11:22,390 --> 00:11:28,180
is now accessible to everyone so as you

225
00:11:25,360 --> 00:11:29,410
saw all you have to do is get docker

226
00:11:28,180 --> 00:11:33,280
installed on your system

227
00:11:29,410 --> 00:11:35,770
find the tensorflow container that has

228
00:11:33,280 --> 00:11:38,319
tensorflow already installed and then

229
00:11:35,770 --> 00:11:42,819
write a little bit of code and now we

230
00:11:38,320 --> 00:11:46,390
can start doing machine learning right

231
00:11:42,820 --> 00:11:48,490
off the bat so here we go machine

232
00:11:46,390 --> 00:11:51,939
learning in the news I wanted to go up

233
00:11:48,490 --> 00:11:53,620
just over just a brief overview of what

234
00:11:51,940 --> 00:11:55,840
is machine learning and machine learning

235
00:11:53,620 --> 00:11:59,520
in the news we've been seeing lots of

236
00:11:55,840 --> 00:12:02,980
instances of and I kick the wire again

237
00:11:59,520 --> 00:12:04,930
we've seen lots of instances of machine

238
00:12:02,980 --> 00:12:08,230
learning being applied to games most

239
00:12:04,930 --> 00:12:11,430
notably chess and go so we now have

240
00:12:08,230 --> 00:12:14,290
alphago which has pretty much taken over

241
00:12:11,430 --> 00:12:18,760
some of our best worldwide go players

242
00:12:14,290 --> 00:12:22,870
that will allow you to that currently

243
00:12:18,760 --> 00:12:25,080
play go good seeing that one I'm so

244
00:12:22,870 --> 00:12:25,080
sorry

245
00:12:27,400 --> 00:12:33,500
we now have MIT poker we there's an AI

246
00:12:31,250 --> 00:12:36,530
pokerbot that wits the pros all right

247
00:12:33,500 --> 00:12:39,040
that is we're now at the point where AI

248
00:12:36,530 --> 00:12:42,620
has taken over the game a hearthstone

249
00:12:39,040 --> 00:12:44,930
hearthstone is now no longer a but still

250
00:12:42,620 --> 00:12:47,480
a fun game to play but now it's to the

251
00:12:44,930 --> 00:12:48,760
point where computers can can play it no

252
00:12:47,480 --> 00:12:51,230
problem at all

253
00:12:48,760 --> 00:12:54,800
as we've seen in the news lately we've

254
00:12:51,230 --> 00:12:57,860
got the Microsoft and Amazon are both

255
00:12:54,800 --> 00:13:01,280
trying to market their own machine

256
00:12:57,860 --> 00:13:04,160
learning based person identification

257
00:13:01,280 --> 00:13:07,250
systems to law enforcement and of course

258
00:13:04,160 --> 00:13:09,560
the ACLU is all over this wanting to

259
00:13:07,250 --> 00:13:12,230
have some sort of civil liberties

260
00:13:09,560 --> 00:13:16,010
applied to machine learning and facial

261
00:13:12,230 --> 00:13:17,990
recognition and just a few days ago IBM

262
00:13:16,010 --> 00:13:20,870
jumped into the conversation not with

263
00:13:17,990 --> 00:13:23,720
their own product but just just to

264
00:13:20,870 --> 00:13:25,940
comment on what other major companies

265
00:13:23,720 --> 00:13:28,130
are doing saying hey let's not outlaw

266
00:13:25,940 --> 00:13:31,040
this technology let's heavily regulate

267
00:13:28,130 --> 00:13:34,970
this technology any comment on what

268
00:13:31,040 --> 00:13:36,920
would be better for facial recognition

269
00:13:34,970 --> 00:13:38,960
software used by law enforcement would

270
00:13:36,920 --> 00:13:40,189
you rather have it just outlaw it all

271
00:13:38,960 --> 00:13:43,250
together or would you rather have it

272
00:13:40,190 --> 00:13:44,180
regulated obviously the government's not

273
00:13:43,250 --> 00:13:49,430
going to screw us over

274
00:13:44,180 --> 00:13:50,719
yeah but IBM is up the efficient of a

275
00:13:49,430 --> 00:13:56,030
position let's go ahead and regulate

276
00:13:50,720 --> 00:13:58,880
okay so our goals with machine learning

277
00:13:56,030 --> 00:14:01,220
is we're trying to create simple

278
00:13:58,880 --> 00:14:03,080
prediction machines but without the

279
00:14:01,220 --> 00:14:05,240
tedious task of handling every single

280
00:14:03,080 --> 00:14:08,900
circumstance how many different poker

281
00:14:05,240 --> 00:14:11,630
hands are there that we we had 300

282
00:14:08,900 --> 00:14:13,610
million plus there's a lot could we

283
00:14:11,630 --> 00:14:16,700
write code that handles all 300 million

284
00:14:13,610 --> 00:14:19,220
possible combinations we could I could

285
00:14:16,700 --> 00:14:21,110
probably sit down and go through all the

286
00:14:19,220 --> 00:14:23,990
different possible hands we can sort

287
00:14:21,110 --> 00:14:27,350
them and say check the suits for the

288
00:14:23,990 --> 00:14:28,400
flushes and all that and then I would

289
00:14:27,350 --> 00:14:30,950
have to go through the process of

290
00:14:28,400 --> 00:14:33,020
testing all of that based on a

291
00:14:30,950 --> 00:14:33,649
chessboard layout what will my opponent

292
00:14:33,020 --> 00:14:35,300
do next

293
00:14:33,649 --> 00:14:37,819
would be a great machine lending project

294
00:14:35,300 --> 00:14:39,889
based on a poker hand what should I

295
00:14:37,819 --> 00:14:43,279
raise or fall to maximize my chances of

296
00:14:39,889 --> 00:14:44,959
them in based on a crowd are there any

297
00:14:43,279 --> 00:14:46,790
people with outstanding warrants there's

298
00:14:44,959 --> 00:14:50,479
lots of interesting applications of

299
00:14:46,790 --> 00:14:52,040
these machine learning machine learning

300
00:14:50,480 --> 00:14:54,410
projects one thing I would like to

301
00:14:52,040 --> 00:14:56,748
mention is the game of chess the game of

302
00:14:54,410 --> 00:14:58,509
chess when it comes to AI has changed

303
00:14:56,749 --> 00:15:01,220
dramatically over the last 20 years

304
00:14:58,509 --> 00:15:04,069
before whenever we have at the the

305
00:15:01,220 --> 00:15:06,379
classic games between in the 90s Garry

306
00:15:04,069 --> 00:15:10,849
Kasparov and deep blue those were done

307
00:15:06,379 --> 00:15:14,240
with mostly tree based searches in which

308
00:15:10,850 --> 00:15:16,490
a programmer with a computer with

309
00:15:14,240 --> 00:15:18,740
manually program all the different

310
00:15:16,490 --> 00:15:22,309
possible moods that every possible piece

311
00:15:18,740 --> 00:15:23,600
on the chessboard could make so what is

312
00:15:22,309 --> 00:15:25,790
that what are the means that a Queen can

313
00:15:23,600 --> 00:15:27,350
make it can move one space in any

314
00:15:25,790 --> 00:15:29,329
direction so a programmer would sit down

315
00:15:27,350 --> 00:15:31,429
and say oh a queen can move one space in

316
00:15:29,329 --> 00:15:34,309
any direction nowadays that's no longer

317
00:15:31,429 --> 00:15:38,209
done with chess ai's what they'll do is

318
00:15:34,309 --> 00:15:40,370
they'll just dump a huge number of chess

319
00:15:38,209 --> 00:15:41,899
chess games into a machine learning

320
00:15:40,370 --> 00:15:44,269
framework much like the one I'm using

321
00:15:41,899 --> 00:15:46,189
here with tensorflow and the game will

322
00:15:44,269 --> 00:15:48,199
automatically learn what is a legal or

323
00:15:46,189 --> 00:15:50,540
an illegal move if it's a legal move

324
00:15:48,199 --> 00:15:51,949
it'll find it in the database if it's an

325
00:15:50,540 --> 00:15:55,969
illegal move it will find it in the

326
00:15:51,949 --> 00:15:58,219
database so chess chess ai's nowadays

327
00:15:55,970 --> 00:16:00,350
start from having no knowledge of what

328
00:15:58,220 --> 00:16:04,759
the pieces do and then can work up from

329
00:16:00,350 --> 00:16:08,959
there to be as strong as professional

330
00:16:04,759 --> 00:16:11,300
chess chess players all right which is

331
00:16:08,959 --> 00:16:13,878
something that I find fascinating behind

332
00:16:11,300 --> 00:16:16,008
are just the state of artificial

333
00:16:13,879 --> 00:16:18,579
intelligence in which it's moving so

334
00:16:16,009 --> 00:16:20,749
what's on what I'm trying to say is

335
00:16:18,579 --> 00:16:22,878
don't try to solve the problem yourself

336
00:16:20,749 --> 00:16:24,799
let the Machine solve the problem I've

337
00:16:22,879 --> 00:16:27,259
got this wonderful laptop here it's

338
00:16:24,799 --> 00:16:29,059
gonna tell me what our chess hands

339
00:16:27,259 --> 00:16:31,519
I'm not chassis ins yeah while I'm

340
00:16:29,059 --> 00:16:35,689
letting my program bank it's gonna tell

341
00:16:31,519 --> 00:16:37,769
me tell me what poker hands are based on

342
00:16:35,689 --> 00:16:39,660
how well the

343
00:16:37,769 --> 00:16:49,589
based on the the giant database that

344
00:16:39,660 --> 00:16:52,469
I've provided well no not right now I do

345
00:16:49,589 --> 00:16:53,759
have it plugged in because if I didn't

346
00:16:52,470 --> 00:16:56,389
have it plugged in my laptop would

347
00:16:53,759 --> 00:16:59,249
probably die during the presentation

348
00:16:56,389 --> 00:17:01,709
that is it is a very computationally

349
00:16:59,249 --> 00:17:03,929
intensive process and I don't have the

350
00:17:01,709 --> 00:17:06,569
greatest laptop in the world so that's

351
00:17:03,929 --> 00:17:09,119
why this is taking this is gonna take a

352
00:17:06,569 --> 00:17:11,428
long amount of time but that's the idea

353
00:17:09,119 --> 00:17:13,469
in neural networks it's through a lot of

354
00:17:11,429 --> 00:17:15,449
repetition it's like teaching a small

355
00:17:13,470 --> 00:17:22,049
child how do you teach a small child

356
00:17:15,449 --> 00:17:23,549
with a lot of repetition all right so

357
00:17:22,049 --> 00:17:25,888
here's the basic flowchart and it's

358
00:17:23,549 --> 00:17:28,259
really simple for machine learning you

359
00:17:25,888 --> 00:17:32,758
have a question you think on it and you

360
00:17:28,259 --> 00:17:34,289
get an answer all right so more

361
00:17:32,759 --> 00:17:36,539
importantly from the perspective of a

362
00:17:34,289 --> 00:17:39,419
software developer we might flip that on

363
00:17:36,539 --> 00:17:41,940
its head and say something along the

364
00:17:39,419 --> 00:17:45,240
lines of I have a set of inputs now

365
00:17:41,940 --> 00:17:47,250
process it and then let's look at the

366
00:17:45,240 --> 00:17:50,100
outputs but the inputs and the outputs

367
00:17:47,250 --> 00:17:53,250
of what I have to start with what I want

368
00:17:50,100 --> 00:17:55,980
is to give the program the inputs and

369
00:17:53,250 --> 00:17:58,620
the outputs and it's processing it

370
00:17:55,980 --> 00:18:02,610
figures out what how to translate inputs

371
00:17:58,620 --> 00:18:04,049
to outputs so there you go let's go

372
00:18:02,610 --> 00:18:07,769
through a real simple example of this

373
00:18:04,049 --> 00:18:10,649
just us together imagine that I give you

374
00:18:07,769 --> 00:18:13,230
a task for everyone in the room

375
00:18:10,649 --> 00:18:14,758
I say how many let's let's write a

376
00:18:13,230 --> 00:18:17,580
program that converts kilometers to

377
00:18:14,759 --> 00:18:19,740
miles all right so if I want to convert

378
00:18:17,580 --> 00:18:21,870
kilometers to miles what I'm gonna do is

379
00:18:19,740 --> 00:18:24,350
I'm gonna say we're going to give it an

380
00:18:21,870 --> 00:18:30,090
input and an output and then I go

381
00:18:24,350 --> 00:18:32,459
kilometers to miles is 0.5 and we

382
00:18:30,090 --> 00:18:34,168
alright so is 25 kilometer is equal to

383
00:18:32,460 --> 00:18:38,100
I'm sorry it's 50 kilometers existence

384
00:18:34,169 --> 00:18:43,919
of 25 miles now what we'll do after that

385
00:18:38,100 --> 00:18:46,000
is will record the error term so the

386
00:18:43,919 --> 00:18:48,160
correct answer in this case you

387
00:18:46,000 --> 00:18:50,890
you don't have that ability just do the

388
00:18:48,160 --> 00:18:53,950
math in your head it's 31.0 six-month

389
00:18:50,890 --> 00:18:55,900
miles is equal to 50 kilometers so the

390
00:18:53,950 --> 00:18:59,230
answer we gave was 25 miles

391
00:18:55,900 --> 00:19:02,740
obviously that's an error of 0.6 miles

392
00:18:59,230 --> 00:19:06,150
so we have 0.6 miles and we know that we

393
00:19:02,740 --> 00:19:08,740
probably need to adjust that term and

394
00:19:06,150 --> 00:19:12,450
make it should we make the term larger

395
00:19:08,740 --> 00:19:15,490
we should make this middle term smaller

396
00:19:12,450 --> 00:19:20,110
we we gave it 25 miles the real answer

397
00:19:15,490 --> 00:19:22,060
is 31.6 I need to make my term I mean to

398
00:19:20,110 --> 00:19:24,879
me I need to make this this is 0.5

399
00:19:22,060 --> 00:19:29,710
larger yeah we're gonna make that larger

400
00:19:24,880 --> 00:19:31,060
okay and what we do whenever we have

401
00:19:29,710 --> 00:19:34,060
neural networks is we just start with

402
00:19:31,060 --> 00:19:36,550
random numbers and so whenever the the

403
00:19:34,060 --> 00:19:38,230
program runs through the the neural

404
00:19:36,550 --> 00:19:40,000
network what's gonna happen is it's

405
00:19:38,230 --> 00:19:42,910
gonna get a lot of things wrong and

406
00:19:40,000 --> 00:19:45,400
that's okay because we're learning this

407
00:19:42,910 --> 00:19:46,720
is machine learning so we're gonna get

408
00:19:45,400 --> 00:19:49,210
it wrong and we're going to get it wrong

409
00:19:46,720 --> 00:19:51,040
a lot and that's part of the education

410
00:19:49,210 --> 00:19:54,850
process is getting it wrong a lot and

411
00:19:51,040 --> 00:19:57,790
then getting back on track all right so

412
00:19:54,850 --> 00:20:02,230
if I I bump this up to 0.7 I get 35

413
00:19:57,790 --> 00:20:04,960
miles which is you know closer did i

414
00:20:02,230 --> 00:20:10,330
overshoot my correct answer of 30 1.06 I

415
00:20:04,960 --> 00:20:11,920
did so what do I need to do now I've got

416
00:20:10,330 --> 00:20:13,840
my my turn here I need to make it

417
00:20:11,920 --> 00:20:16,000
smaller I know that I don't need to make

418
00:20:13,840 --> 00:20:19,169
it smaller than 0.5 but I need to make

419
00:20:16,000 --> 00:20:21,400
it somewhat smaller

420
00:20:19,170 --> 00:20:25,450
okay so let's split the difference I'm

421
00:20:21,400 --> 00:20:29,470
gonna go 0.6 all right so there's my 0.6

422
00:20:25,450 --> 00:20:33,340
term 50 times 0.6 I get 30 miles right

423
00:20:29,470 --> 00:20:35,770
I now have 31 31 point no 2 6 miles an

424
00:20:33,340 --> 00:20:40,959
error of 1.06 miles I'm getting closer

425
00:20:35,770 --> 00:20:42,820
to my goal all right

426
00:20:40,960 --> 00:20:45,640
so as you can see we keep iterating

427
00:20:42,820 --> 00:20:47,740
through the process in order to keep

428
00:20:45,640 --> 00:20:49,840
improving our accuracy this is what's

429
00:20:47,740 --> 00:20:53,830
known in a neural network as a

430
00:20:49,840 --> 00:20:56,050
perceptron and a perceptron is a just a

431
00:20:53,830 --> 00:20:59,470
single node in which we multiply an

432
00:20:56,050 --> 00:21:02,290
input by a random value and then we get

433
00:20:59,470 --> 00:21:04,780
an output and this is known as a

434
00:21:02,290 --> 00:21:08,050
perceptron I like to name my perceptrons

435
00:21:04,780 --> 00:21:09,790
Percy Percy the perceptron and you can

436
00:21:08,050 --> 00:21:12,520
do quite a bit with Percy the perceptron

437
00:21:09,790 --> 00:21:14,080
it turns out that most of the machine

438
00:21:12,520 --> 00:21:17,440
learning problems that we deal with

439
00:21:14,080 --> 00:21:20,080
really only require one little

440
00:21:17,440 --> 00:21:23,080
perceptron and that does quite a bit we

441
00:21:20,080 --> 00:21:25,270
call this a linear solution you you take

442
00:21:23,080 --> 00:21:27,189
Percy the perceptron you multiply by a

443
00:21:25,270 --> 00:21:28,389
number you add a constant and usually

444
00:21:27,190 --> 00:21:30,610
you can map one thing into another

445
00:21:28,390 --> 00:21:33,730
really easily with a small error term

446
00:21:30,610 --> 00:21:36,219
this is also known in machine learning

447
00:21:33,730 --> 00:21:37,330
as linear regression which are usually

448
00:21:36,220 --> 00:21:39,670
taught in an introductory statistics

449
00:21:37,330 --> 00:21:42,129
course but here we're doing it in a

450
00:21:39,670 --> 00:21:49,390
roundabout way in which we just have one

451
00:21:42,130 --> 00:21:50,680
node okay so to recap usually when we

452
00:21:49,390 --> 00:21:52,960
think about programming we think about

453
00:21:50,680 --> 00:21:56,560
inputs and algorithms so if I want to

454
00:21:52,960 --> 00:21:58,360
translate kilometers to I'm sorry if

455
00:21:56,560 --> 00:22:00,820
elantra it's like climbers two miles

456
00:21:58,360 --> 00:22:02,560
what I'll have to do is I'll go look up

457
00:22:00,820 --> 00:22:04,480
on a chart what is the factor that

458
00:22:02,560 --> 00:22:05,980
translates kilometers to miles and then

459
00:22:04,480 --> 00:22:09,310
I'll write a little program multiply

460
00:22:05,980 --> 00:22:13,990
this times 0.61 and then I'll get a

461
00:22:09,310 --> 00:22:16,690
response but with machine learning we're

462
00:22:13,990 --> 00:22:18,370
we're flipping that on its head we we

463
00:22:16,690 --> 00:22:20,050
have large amounts of inputs and large

464
00:22:18,370 --> 00:22:21,989
amounts of outputs and we let the

465
00:22:20,050 --> 00:22:24,100
computer create the algorithm for us

466
00:22:21,990 --> 00:22:25,990
programmers if you were worried about

467
00:22:24,100 --> 00:22:29,830
your jobs you can now be worried about

468
00:22:25,990 --> 00:22:34,690
your jobs so there we go

469
00:22:29,830 --> 00:22:36,760
well soon we'll explore that neural

470
00:22:34,690 --> 00:22:38,230
networks will be expanded to words more

471
00:22:36,760 --> 00:22:42,040
areas things that we haven't thought of

472
00:22:38,230 --> 00:22:43,430
yet and III do think this is an exciting

473
00:22:42,040 --> 00:22:45,930
area

474
00:22:43,430 --> 00:22:47,610
so use the error term produced by our

475
00:22:45,930 --> 00:22:51,840
model in order to help improve the model

476
00:22:47,610 --> 00:22:54,179
so it's a continuous cycle of checking

477
00:22:51,840 --> 00:22:56,189
in hey I'm checking in with our model

478
00:22:54,180 --> 00:22:58,550
hey let's check in with our model let's

479
00:22:56,190 --> 00:23:01,920
see how we're doing here we're at about

480
00:22:58,550 --> 00:23:08,220
54 percent accuracy how we're doing here

481
00:23:01,920 --> 00:23:12,060
we got 1400 of our 10000 populations

482
00:23:08,220 --> 00:23:24,210
that we're doing good hopefully I will

483
00:23:12,060 --> 00:23:27,149
get go ahead we're trying to take a list

484
00:23:24,210 --> 00:23:29,340
of 10 values and map it to one value but

485
00:23:27,150 --> 00:23:33,660
every set of 10 values is different and

486
00:23:29,340 --> 00:23:35,310
they all map to 1 of 0 to 9 and so we're

487
00:23:33,660 --> 00:23:37,500
just constantly going back and forth

488
00:23:35,310 --> 00:23:39,300
look at this 10 values map it to one

489
00:23:37,500 --> 00:23:40,890
value and we're trying to do this

490
00:23:39,300 --> 00:23:42,930
without having to manually write the

491
00:23:40,890 --> 00:23:44,340
code ok if I have these 5 cards it's

492
00:23:42,930 --> 00:23:46,160
going to be a royal flush and if I have

493
00:23:44,340 --> 00:23:51,360
these 5 cards it's going to be a

494
00:23:46,160 --> 00:23:54,720
straight we're building a network this

495
00:23:51,360 --> 00:23:56,399
is a this is a network so is it just

496
00:23:54,720 --> 00:23:58,860
kind of doing an evolutionary process

497
00:23:56,400 --> 00:24:05,460
just like randomly mutating all of those

498
00:23:58,860 --> 00:24:07,740
transformations and well what it's doing

499
00:24:05,460 --> 00:24:11,520
is it's called a back propagation where

500
00:24:07,740 --> 00:24:13,980
it will after it figures out that it got

501
00:24:11,520 --> 00:24:17,250
an error term it will go back and and

502
00:24:13,980 --> 00:24:19,410
tweak each of those random values by

503
00:24:17,250 --> 00:24:21,720
just a little bit by just a tiny amount

504
00:24:19,410 --> 00:24:24,240
in the direction that will favor

505
00:24:21,720 --> 00:24:27,390
whatever the last run that just failed

506
00:24:24,240 --> 00:24:31,190
in order to push it more towards the

507
00:24:27,390 --> 00:24:34,010
correct answer so

508
00:24:31,190 --> 00:24:37,490
time is it tools water times yes it

509
00:24:34,010 --> 00:24:38,360
takes more time there we go we're

510
00:24:37,490 --> 00:24:42,320
watching it work

511
00:24:38,360 --> 00:24:49,300
we're now at 1500 we'll see how far this

512
00:24:42,320 --> 00:24:49,300
gets along Wow I am okay so there we go

513
00:24:49,690 --> 00:24:55,400
this is a general overall flowchart of

514
00:24:52,580 --> 00:24:57,470
machine learning this is not

515
00:24:55,400 --> 00:24:59,270
particularly tied to neural networks I

516
00:24:57,470 --> 00:25:03,020
just wanted to have a slide in here that

517
00:24:59,270 --> 00:25:21,590
was geared towards the broader scope of

518
00:25:03,020 --> 00:25:22,100
machine learning the machine will learn

519
00:25:21,590 --> 00:25:38,179
Tobias

520
00:25:22,100 --> 00:25:43,129
that's called overfitting a domain a

521
00:25:38,180 --> 00:25:46,040
domain expert yes yes so one of the

522
00:25:43,130 --> 00:25:49,760
things that I'm gonna hope if this

523
00:25:46,040 --> 00:25:51,260
finishes by the time I I stop I think I

524
00:25:49,760 --> 00:25:52,970
probably should have set this at about

525
00:25:51,260 --> 00:25:56,690
four or five thousand instead of ten

526
00:25:52,970 --> 00:26:02,710
thousand but I'll go back over to let's

527
00:25:56,690 --> 00:26:02,710
do two let's go back over to my data set

528
00:26:04,059 --> 00:26:08,320
so you'll notice here that I have three

529
00:26:05,919 --> 00:26:09,789
files up on the screen I've got the

530
00:26:08,320 --> 00:26:11,559
names which I showed you it's just a

531
00:26:09,789 --> 00:26:13,360
description of all the poker hands I've

532
00:26:11,559 --> 00:26:15,720
got the training set that's really what

533
00:26:13,360 --> 00:26:20,110
you're seeing the program go through as

534
00:26:15,720 --> 00:26:23,110
as the the model is learning and then I

535
00:26:20,110 --> 00:26:24,789
have a testing set which I loaded in but

536
00:26:23,110 --> 00:26:26,979
at that stage with that you're seeing up

537
00:26:24,789 --> 00:26:30,190
on the screen the testing face hasn't

538
00:26:26,980 --> 00:26:31,270
even occurred so one of the nice things

539
00:26:30,190 --> 00:26:35,289
about tensorflow

540
00:26:31,270 --> 00:26:39,100
is that you can do that that analysis

541
00:26:35,289 --> 00:26:40,779
between an outside expert and machine

542
00:26:39,100 --> 00:26:42,760
learning as long as you've separated out

543
00:26:40,779 --> 00:26:47,500
your data into a training set and a test

544
00:26:42,760 --> 00:26:50,169
set that you can explore that you can

545
00:26:47,500 --> 00:26:56,799
evaluate your model so if we take a look

546
00:26:50,169 --> 00:26:58,630
at that test set you'll notice that it's

547
00:26:56,799 --> 00:27:00,190
pretty much the same data well it's

548
00:26:58,630 --> 00:27:03,880
pretty much the same file format it's

549
00:27:00,190 --> 00:27:09,260
just different data oh we got rows of

550
00:27:03,880 --> 00:27:10,970
eleven numbers each okay

551
00:27:09,260 --> 00:27:14,179
so we have rows of eleven numbers each

552
00:27:10,970 --> 00:27:16,280
and then what the program is training on

553
00:27:14,179 --> 00:27:17,929
is the first file that I showed you and

554
00:27:16,280 --> 00:27:20,480
what it's going to test on later is the

555
00:27:17,929 --> 00:27:23,870
second file and really the true judge of

556
00:27:20,480 --> 00:27:27,350
a model in the case of biases obviously

557
00:27:23,870 --> 00:27:30,229
or in the case of yeah in the case of

558
00:27:27,350 --> 00:27:32,389
biases or overfitting or or any number

559
00:27:30,230 --> 00:27:34,610
of such problems the real test will be

560
00:27:32,390 --> 00:27:37,929
whenever we examine something that has

561
00:27:34,610 --> 00:27:37,928
never touched our model at all

562
00:27:38,809 --> 00:27:44,210
now what I've noticed a lot of what a

563
00:27:42,919 --> 00:27:46,780
lot of people do whenever they're

564
00:27:44,210 --> 00:27:49,280
working on neural networks is they'll

565
00:27:46,780 --> 00:27:51,168
work with the training set they'll tweak

566
00:27:49,280 --> 00:27:52,910
the data they'll they'll get a result

567
00:27:51,169 --> 00:27:54,620
and then they'll run back over to their

568
00:27:52,910 --> 00:27:56,690
test set they'll evaluate it and they'll

569
00:27:54,620 --> 00:27:58,010
get a response and they'll go oh I don't

570
00:27:56,690 --> 00:28:00,110
like that response and then they'll run

571
00:27:58,010 --> 00:28:02,270
back over to their training set tweak

572
00:28:00,110 --> 00:28:04,909
the code a little bit more run the

573
00:28:02,270 --> 00:28:07,549
analysis again evaluate on the test set

574
00:28:04,910 --> 00:28:09,110
and then oh I like that response better

575
00:28:07,549 --> 00:28:10,790
can you see a problem with this approach

576
00:28:09,110 --> 00:28:14,229
where I'm just going back and forth

577
00:28:10,790 --> 00:28:16,940
between my training and my test set

578
00:28:14,230 --> 00:28:20,450
basically I'm using my test set to train

579
00:28:16,940 --> 00:28:23,630
my training set and then I'm my I'm

580
00:28:20,450 --> 00:28:26,780
implicitly crafting my code to pick up

581
00:28:23,630 --> 00:28:28,610
all of the biases that exist not only my

582
00:28:26,780 --> 00:28:31,360
training set but in my test set as well

583
00:28:28,610 --> 00:28:33,918
so one of the things that I recommend to

584
00:28:31,360 --> 00:28:37,059
students just starting out in machine

585
00:28:33,919 --> 00:28:40,640
learning is get a whole lot of data and

586
00:28:37,059 --> 00:28:42,980
get slice off just a tiny chunk of

587
00:28:40,640 --> 00:28:44,750
cheese and make that your training set

588
00:28:42,980 --> 00:28:46,700
and then slice off just another tiny

589
00:28:44,750 --> 00:28:50,210
even smaller chunk of cheese and make

590
00:28:46,700 --> 00:28:51,950
that your your test set and then test

591
00:28:50,210 --> 00:28:53,809
and then train and then as soon as

592
00:28:51,950 --> 00:28:56,360
you're done with that evaluation merge

593
00:28:53,809 --> 00:28:58,450
that testing data into your training

594
00:28:56,360 --> 00:29:00,740
data because it's no longer very good

595
00:28:58,450 --> 00:29:04,390
always keep your test data fresh with

596
00:29:00,740 --> 00:29:07,650
new test data let's go back to

597
00:29:04,390 --> 00:29:07,650
of the slides

598
00:29:11,030 --> 00:29:14,330
just an overview of some of the things

599
00:29:13,130 --> 00:29:16,220
that we can look for whenever we're

600
00:29:14,330 --> 00:29:17,770
doing machine learning supervised

601
00:29:16,220 --> 00:29:23,360
learning reinforcement learning

602
00:29:17,770 --> 00:29:28,310
unsupervised learning there we go these

603
00:29:23,360 --> 00:29:29,689
are different types and then as you were

604
00:29:28,310 --> 00:29:32,990
mentioning these are some of the

605
00:29:29,690 --> 00:29:35,150
problems of problems of learning we get

606
00:29:32,990 --> 00:29:38,390
a lot of noise in the data bad data

607
00:29:35,150 --> 00:29:40,070
results based on inaccurate data now the

608
00:29:38,390 --> 00:29:42,230
data set that I'm working with here the

609
00:29:40,070 --> 00:29:44,570
poker hand database that's perfect data

610
00:29:42,230 --> 00:29:48,920
there's not gonna be any inaccuracies in

611
00:29:44,570 --> 00:29:50,929
there so hopefully what what I'll do is

612
00:29:48,920 --> 00:29:56,750
I'll get a very nice high accuracy

613
00:29:50,930 --> 00:30:00,140
whenever this is done but not every data

614
00:29:56,750 --> 00:30:02,030
set is perfect and imagine that you've

615
00:30:00,140 --> 00:30:04,190
got a face that resembles someone who

616
00:30:02,030 --> 00:30:07,490
does have an outstanding warrant well

617
00:30:04,190 --> 00:30:10,580
then that is a that is an example of

618
00:30:07,490 --> 00:30:13,250
some of something that well it could

619
00:30:10,580 --> 00:30:15,590
potentially throw off a model specific

620
00:30:13,250 --> 00:30:17,900
features of training are learned for

621
00:30:15,590 --> 00:30:19,689
example what if what if it just so

622
00:30:17,900 --> 00:30:21,620
happens that you have a data set of

623
00:30:19,690 --> 00:30:24,320
criminals and it turns out all your

624
00:30:21,620 --> 00:30:25,520
criminals are wearing glasses and none

625
00:30:24,320 --> 00:30:27,200
of your free people are wearing glasses

626
00:30:25,520 --> 00:30:29,960
and then oh what a net what do you know

627
00:30:27,200 --> 00:30:33,430
I you now put on glasses well then oh

628
00:30:29,960 --> 00:30:36,320
that's the sign the Machine looks for

629
00:30:33,430 --> 00:30:38,360
irrelevant features are learned learned

630
00:30:36,320 --> 00:30:40,220
concepts cannot be generalized because

631
00:30:38,360 --> 00:30:42,800
as we said this is just a bunch of

632
00:30:40,220 --> 00:30:44,540
random numbers inside of a model you

633
00:30:42,800 --> 00:30:46,970
can't really go back and inspect your

634
00:30:44,540 --> 00:30:49,340
model and see what what is being

635
00:30:46,970 --> 00:30:51,490
generalized by the model because it is

636
00:30:49,340 --> 00:30:53,990
just an array of random numbers and

637
00:30:51,490 --> 00:30:56,990
noise is often learned especially if you

638
00:30:53,990 --> 00:30:59,720
have a very noisy data set one of the

639
00:30:56,990 --> 00:31:03,350
questions I like to ask in machine

640
00:30:59,720 --> 00:31:05,990
learning is it's a hot dog sandwich yes

641
00:31:03,350 --> 00:31:09,370
it's a hot dog sandwich good you know

642
00:31:05,990 --> 00:31:09,370
one would like to disagree yes

643
00:31:32,190 --> 00:31:35,269
[Music]

644
00:31:40,900 --> 00:31:48,740
why is this one node in my neural

645
00:31:43,820 --> 00:31:51,110
network 0.4 and why did that flag this

646
00:31:48,740 --> 00:31:55,310
one decision of hot dogs being justified

647
00:31:51,110 --> 00:31:57,020
of sandwiches basically I would have to

648
00:31:55,310 --> 00:31:59,149
point to a node or a collection of nodes

649
00:31:57,020 --> 00:32:01,310
and say these nodes all landed on this

650
00:31:59,150 --> 00:32:09,370
particular range of values therefore hot

651
00:32:01,310 --> 00:32:09,370
dogs or sandwiches yes

652
00:32:12,840 --> 00:32:16,889
so another thing that I like to remind

653
00:32:14,639 --> 00:32:18,899
students is the simplest concept that

654
00:32:16,889 --> 00:32:22,699
classifies all the examples correctly is

655
00:32:18,899 --> 00:32:25,949
likely the best one so try to avoid

656
00:32:22,700 --> 00:32:29,070
complex models try to go for simplistic

657
00:32:25,950 --> 00:32:33,509
models that that handle your data set in

658
00:32:29,070 --> 00:32:35,009
a nicest think manner alright thank you

659
00:32:33,509 --> 00:32:38,999
so much for your participation I mean I

660
00:32:35,009 --> 00:32:40,769
enjoyed the back-and-forth so let's talk

661
00:32:38,999 --> 00:32:44,220
a little bit about the history of neural

662
00:32:40,769 --> 00:32:47,009
networks and so this is a diagram of the

663
00:32:44,220 --> 00:32:51,629
first sketch of the neurons of a brain

664
00:32:47,009 --> 00:32:54,659
from 1899 this was a one of the first

665
00:32:51,629 --> 00:32:57,269
brain surgeons and what they realized

666
00:32:54,659 --> 00:32:59,309
was that as decisions are being made in

667
00:32:57,269 --> 00:33:01,139
your head well then there's these little

668
00:32:59,309 --> 00:33:04,559
pulses that go through the neurons in

669
00:33:01,139 --> 00:33:07,998
your brain and so as you as you're

670
00:33:04,559 --> 00:33:10,590
thinking through problems different

671
00:33:07,999 --> 00:33:12,269
different neurons will get strengthened

672
00:33:10,590 --> 00:33:15,689
based on whatever your skill sets are

673
00:33:12,269 --> 00:33:16,740
not everyone is a chess master but if

674
00:33:15,690 --> 00:33:19,110
you looked at the brain of a chess

675
00:33:16,740 --> 00:33:23,369
master you would probably see a range of

676
00:33:19,110 --> 00:33:26,008
neurons that or that that accentuated

677
00:33:23,369 --> 00:33:27,480
more towards chess because those neurons

678
00:33:26,009 --> 00:33:31,350
have just been trained from years of

679
00:33:27,480 --> 00:33:34,100
practice so there we have the different

680
00:33:31,350 --> 00:33:34,100
parts of the neurons

681
00:33:35,620 --> 00:33:41,290
and then here we have the the computer

682
00:33:38,530 --> 00:33:44,649
model of the neurons so here we have a

683
00:33:41,290 --> 00:33:46,930
set of inputs here I have X 1 X 2 X 3

684
00:33:44,650 --> 00:33:53,890
you can think of this as a problem with

685
00:33:46,930 --> 00:33:57,040
three inputs and here we have three

686
00:33:53,890 --> 00:33:59,470
edges towards a node that has three

687
00:33:57,040 --> 00:34:02,409
weights will call them W 1 W 2 W 3 and

688
00:33:59,470 --> 00:34:05,920
also we're going to do is multiply them

689
00:34:02,410 --> 00:34:09,040
together multiply X 1 and W 1 X 2 and W

690
00:34:05,920 --> 00:34:10,120
2 X 3 and W 3 and then we're going to

691
00:34:09,040 --> 00:34:15,340
add them all together

692
00:34:10,120 --> 00:34:17,529
and then we're going to once we get a

693
00:34:15,340 --> 00:34:20,620
score that score will should be out of a

694
00:34:17,530 --> 00:34:22,240
range of 0 to 1 and then we're going to

695
00:34:20,620 --> 00:34:25,480
test to see if it's greater than a

696
00:34:22,239 --> 00:34:27,908
threshold of T and if that and if it

697
00:34:25,480 --> 00:34:30,190
turns out that that that score is

698
00:34:27,909 --> 00:34:32,379
greater than a threshold T then we're

699
00:34:30,190 --> 00:34:36,700
going to say 0 or 1 and we're going to

700
00:34:32,379 --> 00:34:39,639
have a result of 0 or 1 and it could be

701
00:34:36,699 --> 00:34:41,678
right and it could be wrong all right

702
00:34:39,639 --> 00:34:43,750
but that's the basic mathematics right

703
00:34:41,679 --> 00:34:47,230
there behind how neural networks work

704
00:34:43,750 --> 00:34:50,739
and what we're doing is just evaluating

705
00:34:47,230 --> 00:34:53,879
this on a large scale with dozens and

706
00:34:50,739 --> 00:34:53,879
sometimes even hundreds of neurons

707
00:34:54,929 --> 00:35:01,540
so the original neuron I mentioned my

708
00:34:58,810 --> 00:35:04,360
favorite little my favorite little

709
00:35:01,540 --> 00:35:05,920
neuron perceived the perceptron firstly

710
00:35:04,360 --> 00:35:08,890
the perceptron was first discovered by

711
00:35:05,920 --> 00:35:11,290
Frank Rosenblatt in 1957 who created a

712
00:35:08,890 --> 00:35:14,440
real simple two layer network called

713
00:35:11,290 --> 00:35:17,080
that he just referred to as the input

714
00:35:14,440 --> 00:35:19,420
layer in the output layer and was able

715
00:35:17,080 --> 00:35:22,330
to just use using just two layers one

716
00:35:19,420 --> 00:35:25,000
for inputs one for outputs we create

717
00:35:22,330 --> 00:35:27,830
models and functions for classifications

718
00:35:25,000 --> 00:35:31,619
all right

719
00:35:27,830 --> 00:35:34,109
and then we're going to have a little

720
00:35:31,619 --> 00:35:36,540
bit of history since then marvin minsky

721
00:35:34,109 --> 00:35:39,598
the the famous scientist artificial

722
00:35:36,540 --> 00:35:41,849
intelligence scientist at MIT did a

723
00:35:39,599 --> 00:35:44,520
study on neural networks and found out

724
00:35:41,849 --> 00:35:46,140
that pretty much if you just did linear

725
00:35:44,520 --> 00:35:47,670
regression on the vast majority of your

726
00:35:46,140 --> 00:35:51,359
problems you didn't need neural networks

727
00:35:47,670 --> 00:35:55,020
and that one that was its decision and

728
00:35:51,359 --> 00:35:56,549
then for the next say ten years or so no

729
00:35:55,020 --> 00:35:58,440
one really touched neural networks there

730
00:35:56,550 --> 00:36:01,140
was a declining interest in neural

731
00:35:58,440 --> 00:36:04,950
networks after that point but at this

732
00:36:01,140 --> 00:36:08,279
point in the 1980s research was renewed

733
00:36:04,950 --> 00:36:10,680
and now we have multi of multi layered

734
00:36:08,280 --> 00:36:14,790
networks we have networks with three

735
00:36:10,680 --> 00:36:16,230
layers now in addition to the input and

736
00:36:14,790 --> 00:36:20,210
the output layer we have a hidden layer

737
00:36:16,230 --> 00:36:20,210
and if we go back to my code

738
00:36:28,269 --> 00:36:33,339
what I'd like to do is I like to make

739
00:36:30,399 --> 00:36:36,519
sure that all of my networks have a

740
00:36:33,339 --> 00:36:38,349
hidden layer so here the input layer is

741
00:36:36,519 --> 00:36:40,749
always set by whatever problem you're

742
00:36:38,349 --> 00:36:43,089
working with I'm gonna have my hidden

743
00:36:40,749 --> 00:36:44,919
the input layer comes in that's that's

744
00:36:43,089 --> 00:36:47,589
tacked on to every model we're gonna

745
00:36:44,919 --> 00:36:50,649
have a hidden layer this is known as the

746
00:36:47,589 --> 00:36:52,269
dropout which after every pass through

747
00:36:50,649 --> 00:36:54,929
the model they're going to drop some of

748
00:36:52,269 --> 00:36:57,609
the results of the hidden hidden layer

749
00:36:54,929 --> 00:36:59,409
that's too bad that's an attempt to

750
00:36:57,609 --> 00:37:01,419
remove a little bit of bias from the

751
00:36:59,409 --> 00:37:03,999
neural network and then we're going to

752
00:37:01,419 --> 00:37:06,069
pass it on to the output layer in which

753
00:37:03,999 --> 00:37:07,569
we get the output of my results but

754
00:37:06,069 --> 00:37:11,288
there's my didn't layer and then there's

755
00:37:07,569 --> 00:37:14,729
my output layer so tensorflow handles

756
00:37:11,289 --> 00:37:17,439
input layers and output layers just fine

757
00:37:14,729 --> 00:37:19,328
but a lot of research was put into this

758
00:37:17,439 --> 00:37:21,368
time in the 1980s and it was just

759
00:37:19,329 --> 00:37:23,979
decided that Yomi needed three layers

760
00:37:21,369 --> 00:37:25,629
and you could solve any problem so

761
00:37:23,979 --> 00:37:28,169
that's pretty much where it stood in

762
00:37:25,629 --> 00:37:30,339
fact whenever I was in grad school I

763
00:37:28,169 --> 00:37:31,808
remember having professors tell me you

764
00:37:30,339 --> 00:37:34,779
don't need more than three layers three

765
00:37:31,809 --> 00:37:37,899
layers is the best will the best will

766
00:37:34,779 --> 00:37:40,839
ever need input hidden output great now

767
00:37:37,899 --> 00:37:42,909
within the last say five or six years or

768
00:37:40,839 --> 00:37:45,549
so we now have convolution neural

769
00:37:42,909 --> 00:37:47,169
networks where we say all right why

770
00:37:45,549 --> 00:37:49,509
limit ourselves to three layers we can

771
00:37:47,169 --> 00:37:51,578
have a dozen layers we can now work with

772
00:37:49,509 --> 00:37:54,099
audio and video in that hidden where

773
00:37:51,579 --> 00:37:56,379
different parts of the audio and video

774
00:37:54,099 --> 00:38:00,059
can now be mapped to different layers

775
00:37:56,379 --> 00:38:00,058
within a convolution neural network

776
00:38:08,040 --> 00:38:13,500
so here is an example of a diagram of a

777
00:38:10,860 --> 00:38:16,710
fully connected neural network so here

778
00:38:13,500 --> 00:38:19,530
we have our input layer represented with

779
00:38:16,710 --> 00:38:23,910
eyes here we imagine that we have five

780
00:38:19,530 --> 00:38:26,730
inputs and so the eyes ero doesn't count

781
00:38:23,910 --> 00:38:29,759
as an input it's known as the bias it's

782
00:38:26,730 --> 00:38:31,740
an attempt to add a little randomness to

783
00:38:29,760 --> 00:38:34,590
our layers just in case something sneaks

784
00:38:31,740 --> 00:38:36,479
in here we have our hidden layers notice

785
00:38:34,590 --> 00:38:39,000
I've got four hidden layers and then

786
00:38:36,480 --> 00:38:43,700
I'll have two output layers in case I'm

787
00:38:39,000 --> 00:38:46,710
working on a two classification problem

788
00:38:43,700 --> 00:38:48,660
so you know input layers and output

789
00:38:46,710 --> 00:38:50,910
layers have clear meanings the hidden

790
00:38:48,660 --> 00:38:54,270
layer the one that I keep mentioning is

791
00:38:50,910 --> 00:38:56,850
really where the mysteries of the neural

792
00:38:54,270 --> 00:38:59,130
network happened right now it's it's

793
00:38:56,850 --> 00:39:01,920
really just a summation formula where

794
00:38:59,130 --> 00:39:03,210
you multiply each of these arrows each

795
00:39:01,920 --> 00:39:06,930
of these arrows will have a number

796
00:39:03,210 --> 00:39:08,820
associated with it and then of course

797
00:39:06,930 --> 00:39:11,100
your inputs all have numbers and then

798
00:39:08,820 --> 00:39:13,020
you multiply the input by the weight and

799
00:39:11,100 --> 00:39:14,700
then you add them all up and then that

800
00:39:13,020 --> 00:39:16,860
number will get dropped off into one of

801
00:39:14,700 --> 00:39:19,169
these hidden layers and then you do it

802
00:39:16,860 --> 00:39:22,140
again for the next pass-through from the

803
00:39:19,170 --> 00:39:25,370
hidden layers to the output layer at

804
00:39:22,140 --> 00:39:25,370
some point yeah go ahead

805
00:39:25,710 --> 00:39:33,810
like that diagram would it just be five

806
00:39:29,700 --> 00:39:35,879
columns of input that's exactly right

807
00:39:33,810 --> 00:39:41,160
it's a five column in call in five

808
00:39:35,880 --> 00:39:45,140
column input two column output five

809
00:39:41,160 --> 00:39:48,569
column input two column output alright

810
00:39:45,140 --> 00:39:50,759
and this is what's known as a fully

811
00:39:48,570 --> 00:39:53,310
connected layer because every single

812
00:39:50,760 --> 00:39:55,950
node is connected to every single node

813
00:39:53,310 --> 00:39:58,230
in the next layer so there we go and

814
00:39:55,950 --> 00:40:05,750
then as you can see every node has an

815
00:39:58,230 --> 00:40:05,750
arrow spanning to the next layer six

816
00:40:07,710 --> 00:40:14,640
of six columns

817
00:40:11,580 --> 00:40:16,980
yeah five columns so one two three four

818
00:40:14,640 --> 00:40:19,879
five and then to output and we just kind

819
00:40:16,980 --> 00:40:23,070
of leave the the sixth one eyes ear oh

820
00:40:19,880 --> 00:40:25,710
that that just kind of gets set with a

821
00:40:23,070 --> 00:40:29,400
random number and doesn't really get

822
00:40:25,710 --> 00:40:32,670
touched and if it's known as the bias so

823
00:40:29,400 --> 00:40:37,710
it introduces its own little bias to the

824
00:40:32,670 --> 00:40:40,010
to the network just to prevent oh and

825
00:40:37,710 --> 00:40:42,180
suppose you don't have enough data

826
00:40:40,010 --> 00:40:44,280
there's there's a little bit still

827
00:40:42,180 --> 00:40:46,169
addition randomness into in addition to

828
00:40:44,280 --> 00:40:48,660
the data that you're supplies of banana

829
00:40:46,170 --> 00:40:51,530
jerk you know munching yeah jar you

830
00:40:48,660 --> 00:40:51,529
don't know change there we go

831
00:40:59,960 --> 00:41:04,540
oh yes just have more hidden layers just

832
00:41:07,840 --> 00:41:15,290
have it wider okay yes I have never

833
00:41:12,890 --> 00:41:18,799
experimenting with that I'm sure we

834
00:41:15,290 --> 00:41:22,009
could I have never experimented with

835
00:41:18,800 --> 00:41:24,350
that rather than have just layers so

836
00:41:22,010 --> 00:41:27,430
this is a three layer have I guess you

837
00:41:24,350 --> 00:41:27,430
could say parallel layers

838
00:41:39,400 --> 00:41:42,630
it becomes a lot easier to

839
00:41:44,920 --> 00:41:48,990
yes but if you break that up

840
00:41:50,640 --> 00:41:58,200
accomplished neural network okay yes I

841
00:41:55,130 --> 00:42:04,559
believe you at this point I I'm not

842
00:41:58,200 --> 00:42:05,700
making any assertion okay I don't know

843
00:42:04,559 --> 00:42:07,559
enough about convolutional neural

844
00:42:05,700 --> 00:42:09,180
networks - I've never explored them I've

845
00:42:07,559 --> 00:42:11,579
only dealt with the traditional hidden

846
00:42:09,180 --> 00:42:13,140
layer approach so it looks like the

847
00:42:11,579 --> 00:42:13,529
hidden layers don't talk to each other -

848
00:42:13,140 --> 00:42:15,598
right

849
00:42:13,529 --> 00:42:17,369
that's tribute you'll notice that there

850
00:42:15,599 --> 00:42:19,140
are no arrows connecting a hidden layer

851
00:42:17,369 --> 00:42:22,069
to another hidden layer they are all

852
00:42:19,140 --> 00:42:22,069
independent of each other

853
00:42:28,130 --> 00:42:31,339
smaller is better

854
00:42:34,680 --> 00:42:39,808
so there we go that's perceive the

855
00:42:36,869 --> 00:42:41,849
perceptron so perceive the perceptron

856
00:42:39,809 --> 00:42:43,619
you have a hidden layer size of one you

857
00:42:41,849 --> 00:42:47,430
can basic that's basically doing linear

858
00:42:43,619 --> 00:42:50,970
regression on your data so if you think

859
00:42:47,430 --> 00:42:54,118
of this as a maybe a parametric formulas

860
00:42:50,970 --> 00:42:56,988
formulas ation having four hidden layer

861
00:42:54,119 --> 00:43:00,720
nodes that means I have a four node a

862
00:42:56,989 --> 00:43:04,410
four term parameter equation that I'm

863
00:43:00,720 --> 00:43:07,109
trying to that I'm trying to solve it's

864
00:43:04,410 --> 00:43:10,410
not the same but I think of it as as a

865
00:43:07,109 --> 00:43:12,029
four term parametric equation the more

866
00:43:10,410 --> 00:43:14,460
terms you add to your equation the more

867
00:43:12,029 --> 00:43:16,559
of a complex regression that you're

868
00:43:14,460 --> 00:43:17,910
going to have to solve whereas the more

869
00:43:16,559 --> 00:43:19,200
nodes you add to a neural network the

870
00:43:17,910 --> 00:43:21,470
more training that you're going to have

871
00:43:19,200 --> 00:43:21,470
to do

872
00:43:25,590 --> 00:43:31,230
alright and as we've talked so we give

873
00:43:29,550 --> 00:43:32,820
it the training data we give it a number

874
00:43:31,230 --> 00:43:34,920
of layers in the neural network and the

875
00:43:32,820 --> 00:43:39,180
number of nodes per layer what we then

876
00:43:34,920 --> 00:43:42,120
do is we pass the data in and we let it

877
00:43:39,180 --> 00:43:45,149
bake and this takes a long time and we

878
00:43:42,120 --> 00:43:47,670
just let it sit there and we wait there

879
00:43:45,150 --> 00:43:51,360
have been stories of neural networks

880
00:43:47,670 --> 00:43:54,780
taking months and months to train so

881
00:43:51,360 --> 00:43:55,920
it's it's a slow process and they're one

882
00:43:54,780 --> 00:43:58,440
of the great things about the tensor

883
00:43:55,920 --> 00:44:00,030
flow library is that it's pre-built with

884
00:43:58,440 --> 00:44:02,490
code more to take advantage of GPU

885
00:44:00,030 --> 00:44:07,080
processors so if you have a GPU

886
00:44:02,490 --> 00:44:08,850
processor that is an a version of Linux

887
00:44:07,080 --> 00:44:10,440
that can take advantage of your GPU

888
00:44:08,850 --> 00:44:12,750
processor once you get tensorflow

889
00:44:10,440 --> 00:44:13,970
installed in fact the docker version of

890
00:44:12,750 --> 00:44:16,950
tensorflow

891
00:44:13,970 --> 00:44:18,600
automatically has the GPU hooks already

892
00:44:16,950 --> 00:44:21,299
enabled and then you can take advantage

893
00:44:18,600 --> 00:44:23,160
of your GPU processor create faster net

894
00:44:21,300 --> 00:44:25,520
faster models are your students a lot of

895
00:44:23,160 --> 00:44:34,140
these titles and my students ideas what

896
00:44:25,520 --> 00:44:35,910
GPUs yes they can computational

897
00:44:34,140 --> 00:44:38,790
intensity is mostly on the training

898
00:44:35,910 --> 00:44:40,740
though yeah it's on the training yeah so

899
00:44:38,790 --> 00:44:42,930
once you actually have the model it's

900
00:44:40,740 --> 00:44:45,419
fairly cheap computationally to run it

901
00:44:42,930 --> 00:44:47,190
oh yeah it's the model itself is really

902
00:44:45,420 --> 00:44:50,130
small you can save it it's just a few

903
00:44:47,190 --> 00:44:51,690
just a few bytes really and then you

904
00:44:50,130 --> 00:44:54,060
take an input you pass it through the

905
00:44:51,690 --> 00:44:56,780
bytes it's a real simple equation and

906
00:44:54,060 --> 00:44:58,980
then you get your output it takes

907
00:44:56,780 --> 00:45:01,830
fraction of a fraction of a fraction of

908
00:44:58,980 --> 00:45:08,120
a second but it's the training that

909
00:45:01,830 --> 00:45:08,120
happens is where we is where the cost is

910
00:45:08,920 --> 00:45:16,300
all right so I did mention that we like

911
00:45:13,420 --> 00:45:18,700
to put in thresholds I like to put in a

912
00:45:16,300 --> 00:45:21,880
threshold of 0.5 on my notes but you can

913
00:45:18,700 --> 00:45:24,669
tweak your neural network as desired

914
00:45:21,880 --> 00:45:27,190
if a result on an output node is greater

915
00:45:24,670 --> 00:45:30,580
than 0.5 we say that's a 1 and if it's

916
00:45:27,190 --> 00:45:34,140
less than 0.5 we say that's a 0 and then

917
00:45:30,580 --> 00:45:37,060
we do the backpropagation in which we

918
00:45:34,140 --> 00:45:38,799
fiddle with the hidden node in order to

919
00:45:37,060 --> 00:45:41,500
improve the results of that hidden node

920
00:45:38,800 --> 00:45:42,910
and what that means is simply tweaking

921
00:45:41,500 --> 00:45:44,770
the the little bits of the hidden node

922
00:45:42,910 --> 00:45:47,879
in order to bias it more towards the

923
00:45:44,770 --> 00:45:51,250
solution of the test that I just ran

924
00:45:47,880 --> 00:45:54,700
so rule of thumb less weights than your

925
00:45:51,250 --> 00:45:56,410
training examples so there we go when

926
00:45:54,700 --> 00:45:59,049
determining back trunk propagation you

927
00:45:56,410 --> 00:46:06,730
just run out data you have a stop set or

928
00:45:59,050 --> 00:46:08,770
you start working on your test center ok

929
00:46:06,730 --> 00:46:12,760
let's talk about while I'm finishing up

930
00:46:08,770 --> 00:46:15,280
here we're running of reaching the end

931
00:46:12,760 --> 00:46:17,829
of our talk the disadvantages of neural

932
00:46:15,280 --> 00:46:19,780
networks it's not able really to mimic

933
00:46:17,829 --> 00:46:21,460
the brain I mean this was all designed

934
00:46:19,780 --> 00:46:23,050
with the influence of yeah we're

935
00:46:21,460 --> 00:46:24,430
mimicking the brain and then as we've

936
00:46:23,050 --> 00:46:26,589
learned more about brain science in the

937
00:46:24,430 --> 00:46:29,470
last several decades no we're not

938
00:46:26,589 --> 00:46:31,480
mimicking the brain but we are it we've

939
00:46:29,470 --> 00:46:35,740
we have explored a very interesting

940
00:46:31,480 --> 00:46:38,170
portion of computer science training is

941
00:46:35,740 --> 00:46:40,299
very time consuming scaling is difficult

942
00:46:38,170 --> 00:46:42,130
because training necessary new training

943
00:46:40,300 --> 00:46:43,329
is necessary even the even if the inputs

944
00:46:42,130 --> 00:46:44,740
are increased and the value of the

945
00:46:43,329 --> 00:46:46,359
weights in the hidden layer often don't

946
00:46:44,740 --> 00:46:47,709
have a clear meaning you brought up that

947
00:46:46,359 --> 00:46:51,098
point midway through the talk where you

948
00:46:47,710 --> 00:46:53,950
asked about the European Union requiring

949
00:46:51,099 --> 00:46:59,170
that we justify the meaning of our

950
00:46:53,950 --> 00:47:00,578
hidden layer obviously the the values of

951
00:46:59,170 --> 00:47:03,240
that hidden layer don't really have a

952
00:47:00,579 --> 00:47:03,240
clear meaning

953
00:47:05,500 --> 00:47:15,430
if you have a victim this citizens like

954
00:47:08,349 --> 00:47:18,090
he gives us bang boom yep get the person

955
00:47:15,430 --> 00:47:18,089
thanks man

956
00:47:18,240 --> 00:47:24,640
so it's good so say you've create a

957
00:47:21,670 --> 00:47:25,690
model yep and it's you know a few months

958
00:47:24,640 --> 00:47:28,990
later and you have a bunch of new

959
00:47:25,690 --> 00:47:31,300
training data is that trainee so you

960
00:47:28,990 --> 00:47:33,399
want to add in some new data into your

961
00:47:31,300 --> 00:47:34,780
existing training sentry training model

962
00:47:33,400 --> 00:47:37,030
does that take the same amount of time

963
00:47:34,780 --> 00:47:39,310
or are you able to like start from your

964
00:47:37,030 --> 00:47:40,810
current model and then just train with

965
00:47:39,310 --> 00:47:43,720
your new data you can start with your

966
00:47:40,810 --> 00:47:45,880
current model I'm not sure how wise that

967
00:47:43,720 --> 00:47:48,490
would be I would I would start over okay

968
00:47:45,880 --> 00:47:51,990
so you can but it's a good idea I would

969
00:47:48,490 --> 00:47:51,990
I would start over with a fresh set

970
00:47:56,140 --> 00:48:00,580
good question hi just part of me says I

971
00:47:59,680 --> 00:48:04,569
wouldn't do it

972
00:48:00,580 --> 00:48:07,650
I would i would start fresh just to make

973
00:48:04,570 --> 00:48:07,650
sure that nothing was missed

974
00:48:11,930 --> 00:48:16,509
yes that's a good question I don't I

975
00:48:20,619 --> 00:48:25,549
would take mine I would take my new

976
00:48:23,809 --> 00:48:27,950
training data how to incorporate the

977
00:48:25,549 --> 00:48:30,288
test data from the penny previous test I

978
00:48:27,950 --> 00:48:32,089
would incorporate the new training data

979
00:48:30,289 --> 00:48:34,460
and then I would go out and find a new

980
00:48:32,089 --> 00:48:36,319
testing data because every time you

981
00:48:34,460 --> 00:48:38,950
won't do one thing I have to find new

982
00:48:36,319 --> 00:48:38,950
testing day

983
00:48:45,620 --> 00:48:49,470
different inputs so that's going to

984
00:48:48,660 --> 00:48:51,980
change

985
00:48:49,470 --> 00:48:51,980
oh yes

986
00:48:56,010 --> 00:49:01,330
max proclivity if you're adding new

987
00:48:59,500 --> 00:49:03,580
variables well then you have to start

988
00:49:01,330 --> 00:49:05,500
over from scratch but if you're adding

989
00:49:03,580 --> 00:49:07,840
just new instances where you're trying

990
00:49:05,500 --> 00:49:12,880
to solve the same problem as before then

991
00:49:07,840 --> 00:49:15,360
yeah you can start fresh from there yeah

992
00:49:12,880 --> 00:49:15,360
we're going to trade me

993
00:49:20,390 --> 00:49:28,480
they might give you

994
00:49:25,050 --> 00:49:33,880
okay and then we we fiddle with the bits

995
00:49:28,480 --> 00:49:36,670
further yes someone next earlier are you

996
00:49:33,880 --> 00:49:38,950
do you have lots of individuals and

997
00:49:36,670 --> 00:49:40,990
you're evolving them and the answer is

998
00:49:38,950 --> 00:49:43,180
no I've got one individual and

999
00:49:40,990 --> 00:49:45,939
iteratively improving it

1000
00:49:43,180 --> 00:49:48,640
I understood that right right okay so

1001
00:49:45,940 --> 00:49:49,960
when we do the genetic algorithms and we

1002
00:49:48,640 --> 00:49:52,210
have hundreds or thousands of

1003
00:49:49,960 --> 00:49:54,640
individuals and we're not only picking

1004
00:49:52,210 --> 00:49:58,450
the more fit ones as we evolve them

1005
00:49:54,640 --> 00:50:00,578
we're also mating them yeah um they say

1006
00:49:58,450 --> 00:50:03,220
once you get to an acceptable solution

1007
00:50:00,579 --> 00:50:06,119
don't stop there but keep going longer

1008
00:50:03,220 --> 00:50:10,390
and what you end up with is a more

1009
00:50:06,119 --> 00:50:13,270
evolutionarily fit individual which is

1010
00:50:10,390 --> 00:50:15,160
better able to understand a change of

1011
00:50:13,270 --> 00:50:18,400
environment which you might consider an

1012
00:50:15,160 --> 00:50:22,089
addition of more tests to be a change of

1013
00:50:18,400 --> 00:50:25,000
environment so I'm used to this rather

1014
00:50:22,089 --> 00:50:26,710
than as leni to vintage if they ask you

1015
00:50:25,000 --> 00:50:28,839
should we start from scratch or start

1016
00:50:26,710 --> 00:50:30,790
with an existing one which is pretty

1017
00:50:28,839 --> 00:50:34,558
good I mean man if there would be a

1018
00:50:30,790 --> 00:50:34,558
bunch of these and a bunch of these two

1019
00:50:34,650 --> 00:50:42,220
genetic genetic algorithm sites are its

1020
00:50:37,660 --> 00:50:44,348
own branch okay and that's a real

1021
00:50:42,220 --> 00:50:46,598
interesting topic in and of itself but

1022
00:50:44,349 --> 00:50:48,809
it looks like it could mix with this

1023
00:50:46,599 --> 00:50:51,069
kind of iterative improvement as well oh

1024
00:50:48,809 --> 00:50:55,150
where you take two notes and possibly

1025
00:50:51,069 --> 00:50:58,329
combine them I or you start lots of

1026
00:50:55,150 --> 00:51:00,040
these evolutionary I mean these neural

1027
00:50:58,329 --> 00:51:04,630
networks and then you pick the better

1028
00:51:00,040 --> 00:51:06,700
ones and oh okay I see which is kind of

1029
00:51:04,630 --> 00:51:11,079
what you did with with you have two

1030
00:51:06,700 --> 00:51:12,700
competing models yeah that that arranged

1031
00:51:11,079 --> 00:51:14,920
on slightly different results and then

1032
00:51:12,700 --> 00:51:17,080
you combine those two models yeah the

1033
00:51:14,920 --> 00:51:19,300
first half of this in the second half

1034
00:51:17,080 --> 00:51:30,250
how would that day we have seven billion

1035
00:51:19,300 --> 00:51:31,600
miles yeah I don't know I'm just trying

1036
00:51:30,250 --> 00:51:37,180
to fit this into what I've done before

1037
00:51:31,600 --> 00:51:40,150
okay awesome okay so here's here's my my

1038
00:51:37,180 --> 00:51:42,190
guidelines on wind not to use when not

1039
00:51:40,150 --> 00:51:43,900
to use a barrel Network can you

1040
00:51:42,190 --> 00:51:45,640
flowchart your algorithm pretty easily

1041
00:51:43,900 --> 00:51:48,360
if it's a really simple flow chart then

1042
00:51:45,640 --> 00:51:50,980
on your own network is probably overkill

1043
00:51:48,360 --> 00:51:53,260
do you want to identify solutions not

1044
00:51:50,980 --> 00:51:57,670
related to your inputs use genetic

1045
00:51:53,260 --> 00:51:59,350
algorithms so there you go genetic

1046
00:51:57,670 --> 00:52:03,520
algorithms are great for exploring areas

1047
00:51:59,350 --> 00:52:06,130
way beyond your inputs they so there we

1048
00:52:03,520 --> 00:52:07,570
go can you generate all possible inputs

1049
00:52:06,130 --> 00:52:09,130
with a little bit of code in other words

1050
00:52:07,570 --> 00:52:11,260
if you have a really small problem space

1051
00:52:09,130 --> 00:52:12,790
like tic-tac-toe you probably don't need

1052
00:52:11,260 --> 00:52:14,620
a neural network you could probably map

1053
00:52:12,790 --> 00:52:18,430
out everything in tic-tac-toe in a

1054
00:52:14,620 --> 00:52:19,779
little bit of probably an afternoon does

1055
00:52:18,430 --> 00:52:23,290
the problem involve discrete mathematics

1056
00:52:19,780 --> 00:52:24,850
just use discrete mathematics and does

1057
00:52:23,290 --> 00:52:26,680
the problem involve extremely precise

1058
00:52:24,850 --> 00:52:29,920
measurements for output figure out a

1059
00:52:26,680 --> 00:52:31,509
formula and for instance while it was

1060
00:52:29,920 --> 00:52:33,640
fun to go through the exercise of

1061
00:52:31,510 --> 00:52:35,350
burning kilometers to miles what would

1062
00:52:33,640 --> 00:52:39,790
probably work better a neural network or

1063
00:52:35,350 --> 00:52:39,970
just knowing the formula but there you

1064
00:52:39,790 --> 00:52:44,230
go

1065
00:52:39,970 --> 00:52:46,270
neural networks are great for solving

1066
00:52:44,230 --> 00:52:49,090
complex problems in which there is a

1067
00:52:46,270 --> 00:52:50,650
classification aspect so you've got a

1068
00:52:49,090 --> 00:52:52,120
very complex problem but you think you

1069
00:52:50,650 --> 00:52:54,070
have an okay ish understanding of the

1070
00:52:52,120 --> 00:52:56,020
inputs now's the time to use a neural

1071
00:52:54,070 --> 00:52:57,760
network you've got an extremely large

1072
00:52:56,020 --> 00:53:01,120
problem and you are sure helping puts

1073
00:52:57,760 --> 00:53:04,030
map to outputs and you're willing to

1074
00:53:01,120 --> 00:53:05,740
accept fuzziness of the output and my

1075
00:53:04,030 --> 00:53:08,030
favorite question is a hot dog a

1076
00:53:05,740 --> 00:53:10,848
sandwich

1077
00:53:08,030 --> 00:53:13,220
so there's a fuzzy wouldn't budge you

1078
00:53:10,849 --> 00:53:16,070
wouldn't cut it over back there we go we

1079
00:53:13,220 --> 00:53:17,990
have determined the extra perimeter was

1080
00:53:16,070 --> 00:53:23,830
it cut yeah now I have to rerun my

1081
00:53:17,990 --> 00:53:23,830
neural network from scratch yeah alright

1082
00:53:28,330 --> 00:53:35,869
so what defines the sandwich all right

1083
00:53:32,090 --> 00:53:37,609
you got ready you've got and then you

1084
00:53:35,869 --> 00:53:38,480
got bread but the type of meat maybe

1085
00:53:37,609 --> 00:53:39,710
it's not me

1086
00:53:38,480 --> 00:53:43,099
maybe it's peanut butter and jelly and

1087
00:53:39,710 --> 00:53:44,599
then he asked do you cut it and so there

1088
00:53:43,099 --> 00:53:46,130
we go we found it a new parameter and

1089
00:53:44,599 --> 00:53:47,510
what defines a sandwich and now yeah I'm

1090
00:53:46,130 --> 00:53:59,780
gonna probably have to rerun my neural

1091
00:53:47,510 --> 00:54:06,020
network oh dear we're gonna be like is a

1092
00:53:59,780 --> 00:54:07,220
pretty frigid gun a gun alright so I am

1093
00:54:06,020 --> 00:54:10,160
a big fan of the tits are full of

1094
00:54:07,220 --> 00:54:13,580
library it's it's among the easiest way

1095
00:54:10,160 --> 00:54:16,069
to get a neural network started you do

1096
00:54:13,580 --> 00:54:18,740
need a large data set and there are

1097
00:54:16,070 --> 00:54:23,570
numerous well prepared data sets on the

1098
00:54:18,740 --> 00:54:27,080
internet I like going to the University

1099
00:54:23,570 --> 00:54:30,219
of the UCI data set

1100
00:54:27,080 --> 00:54:30,219
[Music]

1101
00:54:33,180 --> 00:54:36,078
there we go

1102
00:54:36,660 --> 00:54:43,500
DuckDuckGo yeah the breakfast of

1103
00:54:40,410 --> 00:54:48,000
champions there we go

1104
00:54:43,500 --> 00:54:50,220
the UCI data set has just thousands of

1105
00:54:48,000 --> 00:54:53,819
data sets that is great for learning

1106
00:54:50,220 --> 00:54:55,379
machine learning and then find the day

1107
00:54:53,819 --> 00:54:58,589
set this is where I found the poker data

1108
00:54:55,380 --> 00:55:03,240
set there are numerous data sets on here

1109
00:54:58,589 --> 00:55:06,509
if you're just starting out learn the

1110
00:55:03,240 --> 00:55:08,640
iris data set is the the famous iris

1111
00:55:06,510 --> 00:55:11,490
data set is the hello world of machine

1112
00:55:08,640 --> 00:55:15,750
learning because there is only four

1113
00:55:11,490 --> 00:55:17,700
variables and one output category you're

1114
00:55:15,750 --> 00:55:18,839
trying to figure out if one if a plant

1115
00:55:17,700 --> 00:55:22,410
is one of three different categories

1116
00:55:18,839 --> 00:55:25,890
it's a real simple real simple place to

1117
00:55:22,410 --> 00:55:27,149
get started and then you build a neural

1118
00:55:25,890 --> 00:55:29,339
network and then you can solve this

1119
00:55:27,150 --> 00:55:31,349
problem really easily this problem has

1120
00:55:29,339 --> 00:55:35,520
been studied at infinim the data set

1121
00:55:31,349 --> 00:55:37,890
itself is from 1936 so it's got

1122
00:55:35,520 --> 00:55:42,740
historical it's both a valuable data set

1123
00:55:37,890 --> 00:55:46,650
plus it's also historical and that is an

1124
00:55:42,740 --> 00:55:48,660
overview on how to get started with well

1125
00:55:46,650 --> 00:55:52,380
mostly an overview of what neural

1126
00:55:48,660 --> 00:55:54,029
networks are and then how to get started

1127
00:55:52,380 --> 00:55:56,339
doing your actual neural network

1128
00:55:54,029 --> 00:55:57,630
programming it's come a long way since I

1129
00:55:56,339 --> 00:55:59,460
was in grad school where I wrote these

1130
00:55:57,630 --> 00:56:02,279
things from scratch and literally wrote

1131
00:55:59,460 --> 00:56:04,650
a java neural network from scratch in

1132
00:56:02,279 --> 00:56:09,480
grad school and it took me two weeks

1133
00:56:04,650 --> 00:56:12,059
nowadays I can just say import

1134
00:56:09,480 --> 00:56:15,589
tensorflow as tl in a Python program and

1135
00:56:12,059 --> 00:56:17,670
39 lines later I have a tensor flow

1136
00:56:15,589 --> 00:56:20,940
application ready let's go check in on

1137
00:56:17,670 --> 00:56:25,710
our model I should have stopped this

1138
00:56:20,940 --> 00:56:29,940
thing at 5,000 but let's see how we're

1139
00:56:25,710 --> 00:56:32,339
doing on the accuracy yeah we're at 55

1140
00:56:29,940 --> 00:56:36,299
accuracy this thing is not improving

1141
00:56:32,339 --> 00:56:39,750
like I thought it would but it still

1142
00:56:36,299 --> 00:56:41,680
it's still creeping up it's just not

1143
00:56:39,750 --> 00:56:43,450
going up the speed as I

1144
00:56:41,680 --> 00:56:46,720
I would hope I would I would poke that

1145
00:56:43,450 --> 00:56:50,069
would've been up into the 60s by now but

1146
00:56:46,720 --> 00:56:50,069
everyone thank you for your attention

1147
00:56:52,740 --> 00:56:57,308
what would I do with it I would start

1148
00:56:55,420 --> 00:56:59,829
applying it in a real-world application

1149
00:56:57,309 --> 00:57:02,230
where I would take a poker and I would

1150
00:56:59,829 --> 00:57:03,760
take five random cards I would give

1151
00:57:02,230 --> 00:57:06,690
those five cards to the model and it

1152
00:57:03,760 --> 00:57:09,880
would say that you have a straight and

1153
00:57:06,690 --> 00:57:11,770
so that's that's the real benefit of

1154
00:57:09,880 --> 00:57:13,510
this particular application all it does

1155
00:57:11,770 --> 00:57:17,980
is determines the strength of a poker

1156
00:57:13,510 --> 00:57:22,540
hand but it does it counting cards is

1157
00:57:17,980 --> 00:57:25,119
illegal so I would have two counting

1158
00:57:22,540 --> 00:57:26,829
cards is not illegal if you don't have

1159
00:57:25,119 --> 00:57:34,000
any outside assistance if you just do it

1160
00:57:26,829 --> 00:57:35,530
your head then it's fine no kick you out

1161
00:57:34,000 --> 00:57:39,460
if they think you're doing it and they

1162
00:57:35,530 --> 00:57:40,390
have a right to do that but anyway thank

1163
00:57:39,460 --> 00:57:43,109
you so much for your attention

1164
00:57:40,390 --> 00:57:43,109
any other questions

1165
00:57:47,670 --> 00:57:51,120
machine learning yes

1166
00:57:53,809 --> 00:58:04,170
yeah supervised yes I just kind of

1167
00:58:02,099 --> 00:58:05,609
briefly went over those well in fact I

1168
00:58:04,170 --> 00:58:12,349
just gotta said there they are just went

1169
00:58:05,609 --> 00:58:12,348
on to the next slide yes very

1170
00:58:17,830 --> 00:58:23,519
surprise

1171
00:58:19,900 --> 00:58:25,960
I think of I think of networks as a

1172
00:58:23,519 --> 00:58:30,118
version of supervised learning let's

1173
00:58:25,960 --> 00:58:34,420
find out what mr. Wikipedia has to say

1174
00:58:30,119 --> 00:58:36,460
it is a supervised learning I'm always

1175
00:58:34,420 --> 00:58:37,660
hesitant to say which algorithm is part

1176
00:58:36,460 --> 00:58:40,029
of which category because I can never

1177
00:58:37,660 --> 00:58:41,558
remember all of them but we commedia

1178
00:58:40,029 --> 00:58:43,990
does have this under the supervised

1179
00:58:41,559 --> 00:58:45,339
loading category which you do have

1180
00:58:43,990 --> 00:58:52,240
inputs and outputs and you are

1181
00:58:45,339 --> 00:58:55,288
supervising the entire process work test

1182
00:58:52,240 --> 00:58:55,288
at your training centers

1183
00:58:59,300 --> 00:59:07,410
all right any other questions or

1184
00:59:03,750 --> 00:59:08,849
thoughts or concerns happy Veterans Day

1185
00:59:07,410 --> 00:59:13,009
weekend everyone thank you to all of our

1186
00:59:08,849 --> 00:59:13,010
veterans have a good weekend

