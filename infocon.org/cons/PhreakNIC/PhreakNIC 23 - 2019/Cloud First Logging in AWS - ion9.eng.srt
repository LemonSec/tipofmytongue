1
00:00:03,199 --> 00:00:10,070
thank you all for coming out to cloud

2
00:00:05,960 --> 00:00:14,360
first logging in AWS this will be very

3
00:00:10,070 --> 00:00:16,219
much a AWS centric conversation if you

4
00:00:14,360 --> 00:00:19,400
have questions on other platforms I may

5
00:00:16,219 --> 00:00:21,800
be able to help afterwards but hopefully

6
00:00:19,400 --> 00:00:25,459
we can stay on topic for AWS centric

7
00:00:21,800 --> 00:00:28,850
stuff figure we start off with a basic

8
00:00:25,460 --> 00:00:31,010
agenda go over pretty much get a

9
00:00:28,850 --> 00:00:34,730
baseline from the audience

10
00:00:31,010 --> 00:00:40,360
figure out where people's experiences

11
00:00:34,730 --> 00:00:43,550
are what histories you run across and

12
00:00:40,360 --> 00:00:46,250
transition from that into cloud watch

13
00:00:43,550 --> 00:00:48,649
which will be a fundamental product

14
00:00:46,250 --> 00:00:51,920
offering that we leverage for our

15
00:00:48,649 --> 00:00:56,000
enterprise logging system after that

16
00:00:51,920 --> 00:00:58,270
we'll tackle plat first what exactly

17
00:00:56,000 --> 00:01:03,860
does it mean in our approach to how we

18
00:00:58,270 --> 00:01:07,609
tackle this issue and then transition

19
00:01:03,860 --> 00:01:10,369
into additional ways to migrate logs

20
00:01:07,609 --> 00:01:13,429
from your environment into cloud watch

21
00:01:10,369 --> 00:01:16,639
so you have more of a unified means to

22
00:01:13,429 --> 00:01:19,639
interact with them follow that up with

23
00:01:16,639 --> 00:01:23,779
laying out an example framework where we

24
00:01:19,639 --> 00:01:27,350
can transport cloud watch logs across

25
00:01:23,779 --> 00:01:29,479
accounts and across regions so if you

26
00:01:27,350 --> 00:01:31,070
diversify your workload

27
00:01:29,479 --> 00:01:33,920
you can still bring it all back to a

28
00:01:31,070 --> 00:01:36,199
centralized logging account they could

29
00:01:33,920 --> 00:01:40,880
have separate access control lists from

30
00:01:36,200 --> 00:01:47,360
any other piece of your infrastructure

31
00:01:40,880 --> 00:01:51,080
to maintain being pretty much read

32
00:01:47,360 --> 00:01:53,720
attributes of all that good stuff follow

33
00:01:51,080 --> 00:01:56,899
that up with laying out a rough road map

34
00:01:53,720 --> 00:02:01,429
for also archiving an additional copy

35
00:01:56,899 --> 00:02:06,679
off into s3 for any long-term audit

36
00:02:01,429 --> 00:02:08,539
needs so it's a very interactive format

37
00:02:06,679 --> 00:02:10,789
as we've run through it if you have

38
00:02:08,538 --> 00:02:13,369
questions that are on point to the topic

39
00:02:10,788 --> 00:02:16,250
at hand get your hands up we'll tackle

40
00:02:13,370 --> 00:02:18,799
them as we go and we can go down some

41
00:02:16,250 --> 00:02:21,650
rabbit holes if they're on topic if

42
00:02:18,799 --> 00:02:27,410
they're off of we'll tackle them at the

43
00:02:21,650 --> 00:02:31,610
end moving forward so to find a baseline

44
00:02:27,410 --> 00:02:35,980
what sort of logging experience do we

45
00:02:31,610 --> 00:02:39,489
have much experience with syslog

46
00:02:35,980 --> 00:02:45,850
infrastructures any log shipper

47
00:02:39,489 --> 00:02:52,480
platforms say our syslog syslog and G or

48
00:02:45,850 --> 00:02:52,480
anything such as log stash or file beats

49
00:02:53,019 --> 00:02:59,090
okay

50
00:02:55,990 --> 00:03:05,210
we started on for them on nothing yes

51
00:02:59,090 --> 00:03:09,410
all of the above okay well you got one

52
00:03:05,210 --> 00:03:13,930
guy to keep good sounds good you'll call

53
00:03:09,410 --> 00:03:17,870
me on my stuff disapproving frown

54
00:03:13,930 --> 00:03:20,240
okay so logs are important they give us

55
00:03:17,870 --> 00:03:22,190
pretty much a audit trail that we can

56
00:03:20,240 --> 00:03:24,650
use to ascertain all the actions the

57
00:03:22,190 --> 00:03:28,640
system took in regards to servicing the

58
00:03:24,650 --> 00:03:31,459
needs of a user on our platform they can

59
00:03:28,640 --> 00:03:32,809
help us figure out everything and we

60
00:03:31,459 --> 00:03:34,730
just have to try to make sure we don't

61
00:03:32,810 --> 00:03:37,310
get the information we don't want to

62
00:03:34,730 --> 00:03:39,410
have in there but still keep the

63
00:03:37,310 --> 00:03:46,850
information we need to make it a useful

64
00:03:39,410 --> 00:03:49,609
tool so there's a long history of how

65
00:03:46,850 --> 00:03:54,109
logging has traditionally been done when

66
00:03:49,610 --> 00:03:56,600
you start in a cloud first approach you

67
00:03:54,110 --> 00:03:59,030
commonly want to shun away from your

68
00:03:56,600 --> 00:04:02,750
traditional approaches to how you've

69
00:03:59,030 --> 00:04:05,090
done things so when you move to the

70
00:04:02,750 --> 00:04:08,810
cloud you might not be setting up a

71
00:04:05,090 --> 00:04:13,730
syslog server to receive a stream of all

72
00:04:08,810 --> 00:04:15,769
of the logs in your server fleet whose

73
00:04:13,730 --> 00:04:18,200
sole job is then to forward it off to

74
00:04:15,769 --> 00:04:21,620
some enterprise logging system there is

75
00:04:18,200 --> 00:04:25,520
newer approaches when you move into the

76
00:04:21,620 --> 00:04:26,919
cloud so legacy approaches need to be

77
00:04:25,520 --> 00:04:33,409
reevaluated

78
00:04:26,919 --> 00:04:39,700
and also in the cloud we have various

79
00:04:33,410 --> 00:04:43,950
approaches for provisioning stuff are we

80
00:04:39,700 --> 00:04:51,670
having much experience with hypervisors

81
00:04:43,950 --> 00:04:55,930
hyper-converged setups lecture but we

82
00:04:51,670 --> 00:04:58,600
have some options where if the platform

83
00:04:55,930 --> 00:05:00,700
has metadata available for us about the

84
00:04:58,600 --> 00:05:03,940
services its instantiating we can

85
00:05:00,700 --> 00:05:09,750
leverage that into the platform we're

86
00:05:03,940 --> 00:05:09,750
instantiating the cloud is crazy

87
00:05:10,270 --> 00:05:19,659
throughout the presentation I will pull

88
00:05:13,840 --> 00:05:23,799
out various pro tips the pro tips

89
00:05:19,660 --> 00:05:26,890
hopefully might bring your attention to

90
00:05:23,800 --> 00:05:29,470
things that can stall projects or send

91
00:05:26,890 --> 00:05:32,050
you to I am purgatory for long periods

92
00:05:29,470 --> 00:05:35,950
of time whether you try to work out

93
00:05:32,050 --> 00:05:38,800
little issues and if you guys know of

94
00:05:35,950 --> 00:05:42,960
any that I miss please get your hands up

95
00:05:38,800 --> 00:05:42,960
so I can hopefully make this better so

96
00:05:43,080 --> 00:05:51,310
quick intro into cloud watch cloud watch

97
00:05:47,020 --> 00:05:57,909
is our most valuable player for logging

98
00:05:51,310 --> 00:06:02,490
in AWS platforms because it is their own

99
00:05:57,910 --> 00:06:06,580
internal centric service they write to

100
00:06:02,490 --> 00:06:09,100
internally so when their new product

101
00:06:06,580 --> 00:06:11,710
team is looking at how do we make logs

102
00:06:09,100 --> 00:06:14,950
available they'll commonly just kick it

103
00:06:11,710 --> 00:06:20,049
out onto a cloud watch stream that is

104
00:06:14,950 --> 00:06:23,360
available natively in your account by

105
00:06:20,050 --> 00:06:29,779
default cloud watch streams

106
00:06:23,360 --> 00:06:33,569
Amazon Web Services the Amazon

107
00:06:29,779 --> 00:06:39,509
virtualization computer component part

108
00:06:33,569 --> 00:06:44,879
of their company oh yes they have all

109
00:06:39,509 --> 00:06:47,369
kinds of products you could run hello

110
00:06:44,879 --> 00:06:52,099
world in about 15 different ways on

111
00:06:47,369 --> 00:06:56,039
their platform so by default log groups

112
00:06:52,099 --> 00:06:57,779
do not have an expiration any logs that

113
00:06:56,039 --> 00:07:01,878
get made for you will be retained

114
00:06:57,779 --> 00:07:05,459
forever those logs cost money to store

115
00:07:01,879 --> 00:07:08,999
so their default is to cost you infinite

116
00:07:05,459 --> 00:07:10,499
money don't worry though it's made for

117
00:07:08,999 --> 00:07:14,990
your protection because they might be

118
00:07:10,499 --> 00:07:22,829
important logs with some pro tip check

119
00:07:14,990 --> 00:07:26,939
expirations okay so yep moving on in the

120
00:07:22,829 --> 00:07:31,050
cloud first approach we are going to be

121
00:07:26,939 --> 00:07:37,529
using docker for our workloads we could

122
00:07:31,050 --> 00:07:39,779
do traditional thick like ec2 virtual

123
00:07:37,529 --> 00:07:41,939
machine deployments but that's a lot

124
00:07:39,779 --> 00:07:45,990
thicker than we need and the management

125
00:07:41,939 --> 00:07:47,729
and orchestration gets a lot higher why

126
00:07:45,990 --> 00:07:53,519
pay for a server administrator when we

127
00:07:47,729 --> 00:07:58,318
don't have to and the proposed platform

128
00:07:53,519 --> 00:08:00,800
will also need a MongoDB service since I

129
00:07:58,319 --> 00:08:04,289
originally came up with the slide deck

130
00:08:00,800 --> 00:08:07,439
AWS has come out with a MongoDB service

131
00:08:04,289 --> 00:08:10,330
it could leverage but originally it was

132
00:08:07,439 --> 00:08:17,310
set to use Mongo Atlas we

133
00:08:10,330 --> 00:08:22,688
is a MongoDB provider in the cloud that

134
00:08:17,310 --> 00:08:25,150
simplified our deployment in essence the

135
00:08:22,689 --> 00:08:27,939
rest of the cloud first approach boils

136
00:08:25,150 --> 00:08:30,340
down to infrastructure is code so you

137
00:08:27,939 --> 00:08:33,370
don't have your server administrators

138
00:08:30,340 --> 00:08:35,708
out there hand jamming settings in your

139
00:08:33,370 --> 00:08:38,679
various accounts and infrastructure to

140
00:08:35,708 --> 00:08:42,159
make your deployment happen we have a

141
00:08:38,679 --> 00:08:45,670
repository when you commit to that

142
00:08:42,159 --> 00:08:48,040
repository it gets peer-reviewed so

143
00:08:45,670 --> 00:08:51,640
other look humans are checking your code

144
00:08:48,040 --> 00:08:53,860
and after it's merged automated pipeline

145
00:08:51,640 --> 00:08:57,029
picks it up and applies it so you're not

146
00:08:53,860 --> 00:09:01,149
even out there typing the commands

147
00:08:57,029 --> 00:09:04,120
beyond that architect and test for

148
00:09:01,149 --> 00:09:06,190
resiliency so pretty much as your team

149
00:09:04,120 --> 00:09:08,880
grows and develops hold yourselves to

150
00:09:06,190 --> 00:09:11,800
continually higher standards for

151
00:09:08,880 --> 00:09:16,779
ensuring that you meet your no downtime

152
00:09:11,800 --> 00:09:17,760
goal hook yourself to your goal push

153
00:09:16,779 --> 00:09:20,560
yourself

154
00:09:17,760 --> 00:09:24,939
challenge how your systems work and

155
00:09:20,560 --> 00:09:28,239
strive to be better built on an

156
00:09:24,940 --> 00:09:30,160
immutable infrastructure approach so if

157
00:09:28,240 --> 00:09:33,160
something needs change do you just

158
00:09:30,160 --> 00:09:34,870
destroy it and build and run on the new

159
00:09:33,160 --> 00:09:37,180
thing don't try to make changes to

160
00:09:34,870 --> 00:09:41,890
things shoot them in the head and drive

161
00:09:37,180 --> 00:09:45,790
on audit log monitoring and telemetry

162
00:09:41,890 --> 00:09:46,870
are fundamental for your informed

163
00:09:45,790 --> 00:09:49,089
decision-making

164
00:09:46,870 --> 00:09:52,270
sometimes you'll know you have an error

165
00:09:49,089 --> 00:09:54,790
but maybe not exactly where or what

166
00:09:52,270 --> 00:09:57,040
if you can quantify what its effects are

167
00:09:54,790 --> 00:10:00,069
you can make more informed decisions

168
00:09:57,040 --> 00:10:01,810
about how you triage things on top of

169
00:10:00,070 --> 00:10:04,480
that encrypt all the things it's

170
00:10:01,810 --> 00:10:07,140
commonly just an option find it make it

171
00:10:04,480 --> 00:10:10,720
try to be better

172
00:10:07,140 --> 00:10:13,420
so cloud watch is the fundamental

173
00:10:10,720 --> 00:10:17,890
service how are some examples of how we

174
00:10:13,420 --> 00:10:20,589
can get things in in essence the best

175
00:10:17,890 --> 00:10:23,939
way an easiest way you'll commonly run

176
00:10:20,590 --> 00:10:26,350
across if you're coming from a thick or

177
00:10:23,940 --> 00:10:31,870
traditional server type infrastructure

178
00:10:26,350 --> 00:10:35,440
is a AWS logs daemon declaration in a

179
00:10:31,870 --> 00:10:43,060
config which you guys probably cannot

180
00:10:35,440 --> 00:10:49,360
make out but in it boils down to some

181
00:10:43,060 --> 00:10:52,780
nice easy J some esque config where it

182
00:10:49,360 --> 00:10:56,290
calls out what file is what you want the

183
00:10:52,780 --> 00:10:58,720
stream name to be won't a regex for your

184
00:10:56,290 --> 00:11:01,180
date stamp of beef for multi-line event

185
00:10:58,720 --> 00:11:03,460
breaking and then it just moves on down

186
00:11:01,180 --> 00:11:08,310
to the next section there's not a ton of

187
00:11:03,460 --> 00:11:08,310
options but it does its job pretty well

188
00:11:09,510 --> 00:11:15,850
in addition there's a bunch of other

189
00:11:11,800 --> 00:11:19,660
ways to ship logs into cloud watch such

190
00:11:15,850 --> 00:11:23,590
as ECS logs can be configured in the

191
00:11:19,660 --> 00:11:25,780
tasks RDS logs which would be the

192
00:11:23,590 --> 00:11:29,290
relational database service which is

193
00:11:25,780 --> 00:11:31,689
their Postgres or my sequel clusters in

194
00:11:29,290 --> 00:11:35,230
the cloud can be configured to log to

195
00:11:31,690 --> 00:11:39,130
cloud watch also directory services

196
00:11:35,230 --> 00:11:40,300
which is them running a Microsoft Active

197
00:11:39,130 --> 00:11:42,430
Directory for you

198
00:11:40,300 --> 00:11:45,459
in the clouds logs can also be

199
00:11:42,430 --> 00:11:47,979
redirected to cloud watch so cloud watch

200
00:11:45,460 --> 00:11:50,490
is a fundamental component in our

201
00:11:47,980 --> 00:11:50,490
approach

202
00:11:50,790 --> 00:11:57,040
but cloud watch can't transport and

203
00:11:54,430 --> 00:12:00,010
catch all of our things there are still

204
00:11:57,040 --> 00:12:03,219
some components within AWS that were

205
00:12:00,010 --> 00:12:05,110
built before cloud watch made its big

206
00:12:03,220 --> 00:12:08,140
intro into their infrastructure and took

207
00:12:05,110 --> 00:12:10,990
over everything one example of that

208
00:12:08,140 --> 00:12:14,110
would be the logs from the network load

209
00:12:10,990 --> 00:12:17,400
balancers that you can use for bringing

210
00:12:14,110 --> 00:12:20,620
traffic in to your virtual services

211
00:12:17,400 --> 00:12:24,060
those simply get generated and dumped

212
00:12:20,620 --> 00:12:28,720
into an s3 folder as this little tarball

213
00:12:24,060 --> 00:12:31,599
message dump so one easy solution if you

214
00:12:28,720 --> 00:12:33,430
need to get your proxy logs to know

215
00:12:31,600 --> 00:12:36,790
what's going on and all of your micro

216
00:12:33,430 --> 00:12:40,839
services broken down with all kinds of

217
00:12:36,790 --> 00:12:43,810
useful info is you get a little lambda

218
00:12:40,840 --> 00:12:46,780
anytime and objects put into that folder

219
00:12:43,810 --> 00:12:50,229
your lambda fires takes the tarball

220
00:12:46,780 --> 00:12:55,420
extracts the tarball transforms it and

221
00:12:50,230 --> 00:12:58,510
does a AWS log put to the cloud watch

222
00:12:55,420 --> 00:13:02,920
stream for each and every message it's a

223
00:12:58,510 --> 00:13:06,580
pretty easy approach this is where you

224
00:13:02,920 --> 00:13:10,599
get into some of the cloud fine print if

225
00:13:06,580 --> 00:13:13,480
you do this set up the bucket must be

226
00:13:10,600 --> 00:13:16,390
located in the same region as the load

227
00:13:13,480 --> 00:13:20,170
balancer the lambda must also be in the

228
00:13:16,390 --> 00:13:22,150
same region you move these out and it

229
00:13:20,170 --> 00:13:25,589
doesn't matter if your I am policy is

230
00:13:22,150 --> 00:13:25,590
perfect it will still not work

231
00:13:28,570 --> 00:13:35,260
so let's introduce one of our next most

232
00:13:32,190 --> 00:13:37,720
important players in our logging

233
00:13:35,260 --> 00:13:41,860
infrastructure and this is going to be

234
00:13:37,720 --> 00:13:46,090
Kinesis it's there anybody in the crowd

235
00:13:41,860 --> 00:13:51,730
familiar with a big data product called

236
00:13:46,090 --> 00:13:57,910
Kafka yeah so Kinesis is a lot like

237
00:13:51,730 --> 00:14:01,300
Kafka just running in AWS in essence you

238
00:13:57,910 --> 00:14:05,250
end up with producers who make messages

239
00:14:01,300 --> 00:14:10,300
and those messages are sent to streams

240
00:14:05,250 --> 00:14:13,090
streams are hosted on shards and shards

241
00:14:10,300 --> 00:14:20,300
have consumers who connect to them and

242
00:14:13,090 --> 00:14:24,590
pull messages off of it with

243
00:14:20,300 --> 00:14:28,939
Kinesis the streams can be used across

244
00:14:24,590 --> 00:14:33,820
accounts and across regions so with this

245
00:14:28,940 --> 00:14:37,190
set up we can have one AWS account

246
00:14:33,820 --> 00:14:40,190
subscribe its cloud watch logs to a

247
00:14:37,190 --> 00:14:42,980
destination that's in a different region

248
00:14:40,190 --> 00:14:45,880
in a different account who just gets a

249
00:14:42,980 --> 00:14:47,900
copy of all of their laws in real time

250
00:14:45,880 --> 00:14:49,580
though we don't even have to worry about

251
00:14:47,900 --> 00:14:51,230
it because it's built into the

252
00:14:49,580 --> 00:14:57,470
infrastructure that we're natively

253
00:14:51,230 --> 00:15:00,710
running on there's multiple little bits

254
00:14:57,470 --> 00:15:03,290
of fine print let's see here

255
00:15:00,710 --> 00:15:06,220
yeah Kinesis streams only currently

256
00:15:03,290 --> 00:15:09,800
available solution for the cross account

257
00:15:06,220 --> 00:15:11,960
subscriptions log groups and

258
00:15:09,800 --> 00:15:14,870
destinations must be in the same region

259
00:15:11,960 --> 00:15:18,940
however the destination in point to it

260
00:15:14,870 --> 00:15:22,130
AWS resource in a different region so

261
00:15:18,940 --> 00:15:23,990
your source and destination have to be

262
00:15:22,130 --> 00:15:27,439
in the same place but what it subscribes

263
00:15:23,990 --> 00:15:31,960
to could be different lots of little

264
00:15:27,440 --> 00:15:31,960
gotchas lots of fine

265
00:15:32,199 --> 00:15:42,349
so for our logging platform we decided

266
00:15:36,170 --> 00:15:45,378
to go with grey log gray log is a little

267
00:15:42,350 --> 00:15:48,860
extension that will pretty much give us

268
00:15:45,379 --> 00:15:51,019
it elk esque stack without so much the

269
00:15:48,860 --> 00:15:56,170
Cabana front end it's what kind of

270
00:15:51,019 --> 00:16:02,300
extension it's a user interface to

271
00:15:56,170 --> 00:16:04,179
leverage an elastic cluster so commonly

272
00:16:02,300 --> 00:16:08,599
you'd have an elk stack which would be

273
00:16:04,179 --> 00:16:11,929
elasticsearch log stash and Cabana but

274
00:16:08,600 --> 00:16:15,679
this is more a gray log stack where gray

275
00:16:11,929 --> 00:16:20,089
log is replacing the Cabana component

276
00:16:15,679 --> 00:16:25,040
and Great Lord sorry log stash component

277
00:16:20,089 --> 00:16:26,929
of that other stack so in our cloud

278
00:16:25,040 --> 00:16:31,699
first approach we're gonna run these

279
00:16:26,929 --> 00:16:33,980
docker containers so ECS is are easily

280
00:16:31,699 --> 00:16:35,748
chosen solution for rhyme docker at

281
00:16:33,980 --> 00:16:39,879
scale with all kinds of different

282
00:16:35,749 --> 00:16:43,720
features so with that we have

283
00:16:39,879 --> 00:16:46,779
elasticsearch running in a service the

284
00:16:43,720 --> 00:16:51,379
MongoDB service that we need is

285
00:16:46,779 --> 00:16:55,449
outsourced and we have docker containers

286
00:16:51,379 --> 00:16:55,449
running and ECS with a load balancer

287
00:16:55,660 --> 00:17:02,089
with that we configure and set up our

288
00:16:58,790 --> 00:17:05,240
services so the biggest ones we're

289
00:17:02,089 --> 00:17:08,500
concerned with configuring in the docker

290
00:17:05,240 --> 00:17:11,959
environment would be our external URI

291
00:17:08,500 --> 00:17:14,089
also the publish URI which are used

292
00:17:11,959 --> 00:17:15,650
internally for building the cluster

293
00:17:14,089 --> 00:17:19,579
state

294
00:17:15,650 --> 00:17:23,180
great log server cluster if you actually

295
00:17:19,579 --> 00:17:27,439
need to terminate TCP connections for

296
00:17:23,180 --> 00:17:31,730
TCP based message ingestion there's some

297
00:17:27,440 --> 00:17:33,890
caveats around hosting in ECS where if

298
00:17:31,730 --> 00:17:36,320
you're doing this as infrastructure is

299
00:17:33,890 --> 00:17:39,680
code you can't do it because they only

300
00:17:36,320 --> 00:17:42,530
let you forward into one target group so

301
00:17:39,680 --> 00:17:46,750
I got some tricks I can show you if you

302
00:17:42,530 --> 00:17:52,340
need it to do that in your launch config

303
00:17:46,750 --> 00:17:56,390
yeah that's area so after we pick out

304
00:17:52,340 --> 00:18:00,409
our URL you our eyes and get those set

305
00:17:56,390 --> 00:18:03,530
in the docker environments we tie that

306
00:18:00,410 --> 00:18:05,960
together with get the Kinesis inputs

307
00:18:03,530 --> 00:18:07,970
that we've worked to make with cloud

308
00:18:05,960 --> 00:18:09,860
watch across all of our various log

309
00:18:07,970 --> 00:18:11,720
sources across all of our different

310
00:18:09,860 --> 00:18:18,889
accounts and all the different regions

311
00:18:11,720 --> 00:18:21,260
and we take in configure gray log to

312
00:18:18,890 --> 00:18:24,110
subscribe to the Kinesis stream and it

313
00:18:21,260 --> 00:18:27,140
receives messages from all the accounts

314
00:18:24,110 --> 00:18:30,800
on one input we can parse slice and dice

315
00:18:27,140 --> 00:18:33,590
that within our logging system but as

316
00:18:30,800 --> 00:18:37,149
far as the infrastructure is concerned

317
00:18:33,590 --> 00:18:37,149
if it logs we get it

318
00:18:39,300 --> 00:18:47,919
now let's see here yeah extractors in

319
00:18:45,520 --> 00:18:49,900
gray log which is pretty much hey I need

320
00:18:47,920 --> 00:18:52,810
you to run this regex and make this new

321
00:18:49,900 --> 00:18:55,720
feel when I get a message that has this

322
00:18:52,810 --> 00:18:59,350
kind of thing in it those are made per

323
00:18:55,720 --> 00:19:02,230
input so when you're designing your how

324
00:18:59,350 --> 00:19:06,060
am I going to break out my logs keep

325
00:19:02,230 --> 00:19:06,060
that in mind for how you break that out

326
00:19:06,660 --> 00:19:10,810
yeah

327
00:19:07,930 --> 00:19:15,820
and yeah so Kinesis streams can be

328
00:19:10,810 --> 00:19:17,710
thought of as like log groups so great

329
00:19:15,820 --> 00:19:24,490
log versus elk why not

330
00:19:17,710 --> 00:19:26,590
great log with elk when using the elk

331
00:19:24,490 --> 00:19:29,080
stack before I've commonly found the

332
00:19:26,590 --> 00:19:31,780
elasticsearch management components to

333
00:19:29,080 --> 00:19:35,379
be a real nightmare having to go in and

334
00:19:31,780 --> 00:19:38,100
update the template definitions for new

335
00:19:35,380 --> 00:19:41,860
indexes just to be able to get my data

336
00:19:38,100 --> 00:19:45,580
ingest has been more frustrating than I

337
00:19:41,860 --> 00:19:48,340
felt with dealing with and it's really

338
00:19:45,580 --> 00:19:50,820
hampered projects before so that drove

339
00:19:48,340 --> 00:19:54,270
made to take a look at gray log which is

340
00:19:50,820 --> 00:19:58,179
absolutely fantastic since it takes a

341
00:19:54,270 --> 00:20:00,820
index deflector approach for ingestion

342
00:19:58,180 --> 00:20:02,980
where it will pretty much dynamically

343
00:20:00,820 --> 00:20:07,360
build the template as it sees the

344
00:20:02,980 --> 00:20:09,850
messages key tricks if you have messages

345
00:20:07,360 --> 00:20:11,189
that have a format that you extract or

346
00:20:09,850 --> 00:20:14,040
build by JSON

347
00:20:11,190 --> 00:20:18,690
have your things be consistent in their

348
00:20:14,040 --> 00:20:23,120
field values say if you have message

349
00:20:18,690 --> 00:20:25,590
time stamp and that's an epoch value

350
00:20:23,120 --> 00:20:27,239
dump all of a sudden have something else

351
00:20:25,590 --> 00:20:31,560
that comes out with that field that has

352
00:20:27,240 --> 00:20:33,920
like a Julian date it doesn't go well

353
00:20:31,560 --> 00:20:36,120
you'll have messages not get indexed and

354
00:20:33,920 --> 00:20:37,710
that's about the one downside you can

355
00:20:36,120 --> 00:20:45,179
easily alert on it though and when you

356
00:20:37,710 --> 00:20:49,140
get it take care of it yeah so by

357
00:20:45,180 --> 00:20:54,600
leveraging gray log in place of Cabana

358
00:20:49,140 --> 00:20:56,460
and the other log stash front-end we get

359
00:20:54,600 --> 00:21:00,689
pretty much the same functionality with

360
00:20:56,460 --> 00:21:02,340
a simpler operational overhead and it

361
00:21:00,690 --> 00:21:04,080
kind of has some training wheels for

362
00:21:02,340 --> 00:21:08,030
when I pass it off to coworkers to be

363
00:21:04,080 --> 00:21:08,030
able to save the day on certain things

364
00:21:08,600 --> 00:21:15,990
so Kinesis is fantastic it's lettuce

365
00:21:13,290 --> 00:21:19,170
built across account cross region system

366
00:21:15,990 --> 00:21:22,290
but if you've spent much of any time

367
00:21:19,170 --> 00:21:24,090
administering elastic based document

368
00:21:22,290 --> 00:21:27,320
system you don't want this thing to be

369
00:21:24,090 --> 00:21:30,120
the enterprise system of truth because

370
00:21:27,320 --> 00:21:33,540
worrying about the BC PDR requirements

371
00:21:30,120 --> 00:21:35,820
will keep you up at night so the simple

372
00:21:33,540 --> 00:21:38,460
approach to that is don't let your

373
00:21:35,820 --> 00:21:42,080
elastic document store of your logs

374
00:21:38,460 --> 00:21:44,460
become your enterprise system of truth

375
00:21:42,080 --> 00:21:47,520
it's too much of an overhead you don't

376
00:21:44,460 --> 00:21:50,490
feel like paying for so treat the

377
00:21:47,520 --> 00:21:53,670
elastic store as your convenient tool to

378
00:21:50,490 --> 00:21:56,040
view the logs simply actually be able to

379
00:21:53,670 --> 00:21:58,950
search and find what you need for

380
00:21:56,040 --> 00:22:00,810
whatever retention you need it's not

381
00:21:58,950 --> 00:22:02,790
your source of record though if you have

382
00:22:00,810 --> 00:22:04,750
auditors want to see something you don't

383
00:22:02,790 --> 00:22:07,360
point them to that because that's

384
00:22:04,750 --> 00:22:09,820
we're the source of truth is with

385
00:22:07,360 --> 00:22:13,479
Kinesis you've already gathered all the

386
00:22:09,820 --> 00:22:16,510
longs into a set stream we can take and

387
00:22:13,480 --> 00:22:19,900
just like the load balancer made a dump

388
00:22:16,510 --> 00:22:23,350
of messages and tarball we can take and

389
00:22:19,900 --> 00:22:25,480
make our own little tarball dumps of the

390
00:22:23,350 --> 00:22:28,990
messages from our Kinesis streams and

391
00:22:25,480 --> 00:22:31,990
dump amount into our own s3 folder after

392
00:22:28,990 --> 00:22:35,679
we have that we can go a step further

393
00:22:31,990 --> 00:22:37,960
and realize this account attached to our

394
00:22:35,679 --> 00:22:40,929
infrastructure we might not trust it so

395
00:22:37,960 --> 00:22:44,650
if we really want to be paranoid we can

396
00:22:40,929 --> 00:22:49,030
have a totally unrelated account when we

397
00:22:44,650 --> 00:22:53,760
trust but unrelated set up for a cross

398
00:22:49,030 --> 00:22:56,289
region cross account replication and

399
00:22:53,760 --> 00:22:58,629
receive a replicated copy of everything

400
00:22:56,289 --> 00:23:01,090
put in that s3 bucket so you could have

401
00:22:58,630 --> 00:23:04,270
an off-site escrow to copy of your logs

402
00:23:01,090 --> 00:23:06,610
if neatly there's multiple ways to

403
00:23:04,270 --> 00:23:09,850
address audit requirements without

404
00:23:06,610 --> 00:23:12,928
having to build them into your elastic

405
00:23:09,850 --> 00:23:12,928
document data store

406
00:23:14,750 --> 00:23:20,960
so s3 buckets with replication are a

407
00:23:18,710 --> 00:23:25,600
simple way you can accommodate a lot of

408
00:23:20,960 --> 00:23:28,929
that and with that that takes us through

409
00:23:25,600 --> 00:23:36,080
building and 20 on a roadmap for

410
00:23:28,929 --> 00:23:40,270
enterprise logging in AWS if there's

411
00:23:36,080 --> 00:23:43,319
much of any other questions

412
00:23:40,270 --> 00:23:43,319
[Music]

413
00:23:47,620 --> 00:23:54,350
unfortunately that just means we have

414
00:23:49,280 --> 00:23:57,080
more question and answer time yes how

415
00:23:54,350 --> 00:24:02,149
much is this costing you as far as AWS

416
00:23:57,080 --> 00:24:06,918
and logging all the things so logging

417
00:24:02,150 --> 00:24:08,929
all most all of our cost is related to

418
00:24:06,919 --> 00:24:13,510
actually running our infrastructure more

419
00:24:08,929 --> 00:24:16,700
so than logging all the things and our

420
00:24:13,510 --> 00:24:19,429
AWS cost is directly related to how much

421
00:24:16,700 --> 00:24:21,679
we will feel like running so without

422
00:24:19,429 --> 00:24:24,110
getting into more specific details that

423
00:24:21,679 --> 00:24:26,809
I'm comfortable with I'm not sure how to

424
00:24:24,110 --> 00:24:28,939
answer your question did you find that

425
00:24:26,809 --> 00:24:33,230
running the lambdas and better than the

426
00:24:28,940 --> 00:24:34,549
elastic storage is your prime managing

427
00:24:33,230 --> 00:24:37,539
that cost us a percentage of your

428
00:24:34,549 --> 00:24:37,539
overall and

429
00:24:37,809 --> 00:24:44,480
yeah some of the times when I've looked

430
00:24:40,429 --> 00:24:47,270
at it though it's been like three cents

431
00:24:44,480 --> 00:24:53,150
for a month of the lambdas running so

432
00:24:47,270 --> 00:24:54,770
I'm not that concerned with it yeah the

433
00:24:53,150 --> 00:24:58,160
lambdas don't really get run that often

434
00:24:54,770 --> 00:25:00,889
about the most common one is hey you

435
00:24:58,160 --> 00:25:01,970
made a new log group let's set our

436
00:25:00,890 --> 00:25:05,330
default settings

437
00:25:01,970 --> 00:25:07,240
number one you expire in 30 days if we

438
00:25:05,330 --> 00:25:09,770
haven't gotten you out of a cloud watch

439
00:25:07,240 --> 00:25:14,380
either we're messing up or something

440
00:25:09,770 --> 00:25:18,260
went wrong beyond that there is

441
00:25:14,380 --> 00:25:21,080
subscribe to subscribe to destination we

442
00:25:18,260 --> 00:25:25,820
know what Kinesis streams we have in our

443
00:25:21,080 --> 00:25:29,389
environment so we can influence that log

444
00:25:25,820 --> 00:25:30,889
group name creation sum or we can write

445
00:25:29,390 --> 00:25:35,120
our case statement to account for the

446
00:25:30,890 --> 00:25:37,940
ones that we don't control and we can

447
00:25:35,120 --> 00:25:41,389
look at the group that just got made

448
00:25:37,940 --> 00:25:44,929
that happened automatic differently and

449
00:25:41,390 --> 00:25:48,289
we don't control it see it happened by

450
00:25:44,929 --> 00:25:50,559
event trigger the AWS action see it's

451
00:25:48,289 --> 00:25:54,020
type and subscribe it to the destination

452
00:25:50,559 --> 00:25:57,559
automatically three cents a month

453
00:25:54,020 --> 00:26:00,770
simple easy keeps costs it down it

454
00:25:57,559 --> 00:26:03,520
probably saves more than it costs in the

455
00:26:00,770 --> 00:26:03,520
course of any month

456
00:26:08,810 --> 00:26:17,720
oh yeah because by default you get

457
00:26:11,690 --> 00:26:26,900
infinite storage it costs more than s3

458
00:26:17,720 --> 00:26:31,550
but to let you do it forever this is why

459
00:26:26,900 --> 00:26:36,800
I'm asking this question that's got more

460
00:26:31,550 --> 00:26:39,379
juice and I was expecting yeah s3 object

461
00:26:36,800 --> 00:26:42,620
versioning can also come out to be

462
00:26:39,380 --> 00:26:47,150
somewhat of a pain at times especially

463
00:26:42,620 --> 00:26:50,300
if you change something in a lot beyond

464
00:26:47,150 --> 00:26:54,100
that ec2 instance runtime is probably

465
00:26:50,300 --> 00:26:57,950
the largest use of the bill or using

466
00:26:54,100 --> 00:27:00,560
inappropriately sized resources in like

467
00:26:57,950 --> 00:27:04,090
RDS clusters that can definitely make a

468
00:27:00,560 --> 00:27:04,090
huge chunk in short order

469
00:27:07,230 --> 00:27:15,090
I don't want to take a cold question -

470
00:27:10,620 --> 00:27:17,549
oh well we got plenty of questions your

471
00:27:15,090 --> 00:27:23,939
topic using Kinesis and publish and

472
00:27:17,549 --> 00:27:26,429
subscribe and my mqtt and mosquito you

473
00:27:23,940 --> 00:27:30,539
can kind of think of it kind of the same

474
00:27:26,429 --> 00:27:33,929
so the Kinesis thing is kind of a

475
00:27:30,539 --> 00:27:38,730
message queue message producer message

476
00:27:33,929 --> 00:27:42,480
consumer kind of system so it can queue

477
00:27:38,730 --> 00:27:45,779
messages for I believe mine are

478
00:27:42,480 --> 00:27:48,929
configured seven three days most of the

479
00:27:45,779 --> 00:27:51,000
time if I can't get messages off by the

480
00:27:48,929 --> 00:27:53,639
end of three days I might start dropping

481
00:27:51,000 --> 00:27:59,490
messages so there's this incident

482
00:27:53,639 --> 00:28:03,080
response timer built in but that's our

483
00:27:59,490 --> 00:28:06,269
play in its usage in our environment

484
00:28:03,080 --> 00:28:11,090
it aggregates and shifts the log so as

485
00:28:06,269 --> 00:28:11,090
soon as it should you

486
00:28:14,980 --> 00:28:20,390
it can easily be configured for logging

487
00:28:17,960 --> 00:28:22,970
to and alerting to meet whatever

488
00:28:20,390 --> 00:28:31,370
operational did whim or desire you may

489
00:28:22,970 --> 00:28:33,710
have pretty much just find the failure

490
00:28:31,370 --> 00:28:36,709
scenario that you'd like in cloud watch

491
00:28:33,710 --> 00:28:39,440
metrics and write an alarm for it and

492
00:28:36,710 --> 00:28:43,970
you can get an email alert as soon as

493
00:28:39,440 --> 00:28:46,730
that condition occurs there is even

494
00:28:43,970 --> 00:28:50,000
options for doing a predictive

495
00:28:46,730 --> 00:28:52,640
forecasting with seasonality with some

496
00:28:50,000 --> 00:28:55,600
of the metrics in the cloud watch

497
00:28:52,640 --> 00:28:55,600
metrics product

498
00:29:14,200 --> 00:29:22,149
so commonly leverage the kms encryption

499
00:29:19,600 --> 00:29:28,629
functionality whenever interacting with

500
00:29:22,149 --> 00:29:31,600
the s3 products so its s3 but encrypted

501
00:29:28,629 --> 00:29:33,488
before anything's written to it so

502
00:29:31,600 --> 00:29:37,139
there's a separate encryption key beyond

503
00:29:33,489 --> 00:29:37,139
just access to it

504
00:29:39,370 --> 00:29:45,899
you don't want trade up yes we know

505
00:29:43,080 --> 00:29:49,080
now I had never used any one of their

506
00:29:45,899 --> 00:29:51,748
products before I got this gig to build

507
00:29:49,080 --> 00:29:55,830
out a cloud-based logging infrastructure

508
00:29:51,749 --> 00:29:59,580
and I've done cloud build-outs commonly

509
00:29:55,830 --> 00:30:04,230
doing private on Prem cloud or

510
00:29:59,580 --> 00:30:06,119
on-premise inversion madness but this

511
00:30:04,230 --> 00:30:12,480
was my first time doing it in the cloud

512
00:30:06,119 --> 00:30:15,570
and AWS has lots of gotchas and there

513
00:30:12,480 --> 00:30:19,649
was a lot of old hair over the course of

514
00:30:15,570 --> 00:30:22,168
a year working it out and I think I put

515
00:30:19,649 --> 00:30:24,779
something together and hopefully it can

516
00:30:22,169 --> 00:30:30,690
make some less Harrigan pulled out over

517
00:30:24,779 --> 00:30:32,720
time yeah their charges network charges

518
00:30:30,690 --> 00:30:35,930
or

519
00:30:32,720 --> 00:30:40,310
is this sending to another region soryu

520
00:30:35,930 --> 00:30:44,210
pain cross region the prices for that so

521
00:30:40,310 --> 00:30:47,240
the Kinesis pricing is kind of crazy I

522
00:30:44,210 --> 00:30:52,100
think that is one of the actual cheap

523
00:30:47,240 --> 00:30:54,200
ways / they eat it internally across

524
00:30:52,100 --> 00:30:56,590
region data transit I may be wrong

525
00:30:54,200 --> 00:30:56,590
though

526
00:31:02,400 --> 00:31:06,750
now granted Kinesis is not cheap by

527
00:31:04,830 --> 00:31:10,800
itself so it's not like you can do a

528
00:31:06,750 --> 00:31:13,490
data exchange arbitrage with that my

529
00:31:10,800 --> 00:31:16,790
other question was with great log and

530
00:31:13,490 --> 00:31:20,190
resource constraints with all of the

531
00:31:16,790 --> 00:31:30,420
silicates doing all the processing for

532
00:31:20,190 --> 00:31:32,160
you the application processes but I know

533
00:31:30,420 --> 00:31:44,460
that with the last serve sometimes you

534
00:31:32,160 --> 00:31:48,750
need multiple workers so currently I run

535
00:31:44,460 --> 00:31:49,920
on a ASG cluster that's defined in

536
00:31:48,750 --> 00:31:57,090
CloudFormation

537
00:31:49,920 --> 00:32:01,470
that hosts a ECS cluster and I run my

538
00:31:57,090 --> 00:32:03,620
tasks with a WSB PC mode so my tasks

539
00:32:01,470 --> 00:32:06,660
come up with their own IP address

540
00:32:03,620 --> 00:32:09,780
separate from the hosts so I can have

541
00:32:06,660 --> 00:32:12,840
multiple things at port 9000 and they

542
00:32:09,780 --> 00:32:17,270
actually come up at port 9000 which

543
00:32:12,840 --> 00:32:20,790
makes cluster discovery so much simpler

544
00:32:17,270 --> 00:32:23,850
yeah when you get random ethereal ports

545
00:32:20,790 --> 00:32:25,750
and just tries to connect by the port

546
00:32:23,850 --> 00:32:36,490
9000 you have

547
00:32:25,750 --> 00:32:38,860
you're in for a bad day as well it's in

548
00:32:36,490 --> 00:32:42,520
an auto scaling group we currently have

549
00:32:38,860 --> 00:32:47,679
that fixed edit size of three for the

550
00:32:42,520 --> 00:32:52,000
ECS cluster that hosts the compute

551
00:32:47,680 --> 00:32:53,800
environment for the doctor tasks we have

552
00:32:52,000 --> 00:32:56,110
two families of

553
00:32:53,800 --> 00:32:58,149
ECS tasks we have gray log master

554
00:32:56,110 --> 00:33:00,520
because they have a concept of a master

555
00:32:58,150 --> 00:33:02,830
worker and then a regular worker

556
00:33:00,520 --> 00:33:04,990
so we spin one up with the master flag

557
00:33:02,830 --> 00:33:07,030
and then the other ones get spun up

558
00:33:04,990 --> 00:33:10,150
without that flag set but other than

559
00:33:07,030 --> 00:33:13,300
that they're at the exact same image we

560
00:33:10,150 --> 00:33:18,580
do a slightly modified custom entry

561
00:33:13,300 --> 00:33:21,250
point file with some AWS CLI action to

562
00:33:18,580 --> 00:33:23,620
register with the network load balancer

563
00:33:21,250 --> 00:33:26,710
so if you need to do

564
00:33:23,620 --> 00:33:29,379
TLS based TCP connections you can still

565
00:33:26,710 --> 00:33:35,860
do that you just have to do your own

566
00:33:29,380 --> 00:33:39,190
load balancer registration and after

567
00:33:35,860 --> 00:33:42,939
that there whatever size we put the

568
00:33:39,190 --> 00:33:47,560
constraints on the docker image so for

569
00:33:42,940 --> 00:33:49,990
hosts running with 8 gigs of ram with

570
00:33:47,560 --> 00:33:52,419
the Java heap value set at 4 gigs it

571
00:33:49,990 --> 00:33:55,780
chews through messages right and left

572
00:33:52,420 --> 00:33:59,500
and we don't really hold any bit of a

573
00:33:55,780 --> 00:34:02,230
backlog in the Kinesis streams so our

574
00:33:59,500 --> 00:34:05,080
message throughput limit is looking good

575
00:34:02,230 --> 00:34:08,080
since we're keeping up so we don't have

576
00:34:05,080 --> 00:34:11,710
to scale yet which will be a manual

577
00:34:08,080 --> 00:34:14,880
conversation at this point but since we

578
00:34:11,710 --> 00:34:14,880
don't need to we don't need

579
00:34:21,530 --> 00:34:31,580
I've run into a few years that's a

580
00:34:24,879 --> 00:34:39,679
platform this fastest never been closer

581
00:34:31,580 --> 00:34:45,500
oh it helps solve a big congestion issue

582
00:34:39,679 --> 00:34:47,659
I ran through as much as you get and the

583
00:34:45,500 --> 00:34:50,899
user interface and a lot of they're like

584
00:34:47,659 --> 00:34:53,720
user story designed sequences they based

585
00:34:50,899 --> 00:34:56,020
a lot of their design on makes intuitive

586
00:34:53,719 --> 00:34:56,020
sense

587
00:34:58,960 --> 00:35:12,230
oh yeah it uses the what's that the croc

588
00:35:07,990 --> 00:35:14,479
message parsing filter libraries so if

589
00:35:12,230 --> 00:35:19,250
you spent much time with

590
00:35:14,480 --> 00:35:23,240
logstash you might have run across it

591
00:35:19,250 --> 00:35:25,790
but it is a tad bit easier than having

592
00:35:23,240 --> 00:35:31,029
to deal with read checks it can be

593
00:35:25,790 --> 00:35:31,029
tighter simpler than idle sillier

594
00:35:31,540 --> 00:35:37,170
it's very pretty - for searching it'll

595
00:35:35,050 --> 00:35:37,170
help

596
00:35:41,760 --> 00:35:47,640
that is for you let me come along yeah

597
00:35:43,890 --> 00:35:49,950
you know this whole enterprise system

598
00:35:47,640 --> 00:35:53,210
beyond the AWS charges you're paying

599
00:35:49,950 --> 00:35:53,210
nothing for software

600
00:35:59,880 --> 00:36:04,350
it's off thank you

601
00:36:02,800 --> 00:36:07,409
you

602
00:36:04,350 --> 00:36:07,409
[Applause]

