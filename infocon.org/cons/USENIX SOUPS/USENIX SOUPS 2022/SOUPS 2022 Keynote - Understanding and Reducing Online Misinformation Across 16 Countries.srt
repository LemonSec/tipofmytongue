1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,940 --> 00:00:10,679
thank you so much

3
00:00:10,679 --> 00:00:12,300
um it's you know I really appreciate the

4
00:00:12,300 --> 00:00:15,059
invitation I'm really excited to tell

5
00:00:15,059 --> 00:00:16,800
you today about some of the work that me

6
00:00:16,800 --> 00:00:19,380
uh and my many collaborators have been

7
00:00:19,380 --> 00:00:20,880
doing in the last few years to try and

8
00:00:20,880 --> 00:00:23,220
understand misinformation and I want to

9
00:00:23,220 --> 00:00:25,560
start by acknowledging uh many of those

10
00:00:25,560 --> 00:00:26,820
collaborators there's more they don't

11
00:00:26,820 --> 00:00:28,260
all fit on the screen but you know all

12
00:00:28,260 --> 00:00:29,760
the people that worked with us and then

13
00:00:29,760 --> 00:00:31,439
also all of the organizations that give

14
00:00:31,439 --> 00:00:33,239
us money and from a conflict of interest

15
00:00:33,239 --> 00:00:34,800
perspective I just want to say we do get

16
00:00:34,800 --> 00:00:36,420
research funding from Facebook and

17
00:00:36,420 --> 00:00:37,980
Google

18
00:00:37,980 --> 00:00:40,379
um so uh you know unfortunately

19
00:00:40,379 --> 00:00:42,780
misinformation is a topic that doesn't

20
00:00:42,780 --> 00:00:44,460
need a huge amount of introduction these

21
00:00:44,460 --> 00:00:46,860
days obviously falsehoods have been

22
00:00:46,860 --> 00:00:48,840
around for as long as communication has

23
00:00:48,840 --> 00:00:51,059
been around but there's a certain flavor

24
00:00:51,059 --> 00:00:52,980
of misinformation

25
00:00:52,980 --> 00:00:54,180
um that has gotten a lot of attention

26
00:00:54,180 --> 00:00:56,940
starting with the 2016 election cycle in

27
00:00:56,940 --> 00:00:59,640
the U.S and brexit in the UK would have

28
00:00:59,640 --> 00:01:02,039
entirely fabricated statements presented

29
00:01:02,039 --> 00:01:03,780
as if they were false getting large

30
00:01:03,780 --> 00:01:06,080
amounts of traction particularly online

31
00:01:06,080 --> 00:01:08,460
concern redoubled during the coveted

32
00:01:08,460 --> 00:01:10,860
pandemic when it was an infodemic of

33
00:01:10,860 --> 00:01:14,159
false and misleading claims spreading as

34
00:01:14,159 --> 00:01:16,020
the pandemic was spreading and continues

35
00:01:16,020 --> 00:01:19,560
to spread the 2020 election in the U.S

36
00:01:19,560 --> 00:01:21,299
so another round of misinformation of a

37
00:01:21,299 --> 00:01:22,979
different flavor largely focused on

38
00:01:22,979 --> 00:01:26,040
Election fraud and all this has kicked

39
00:01:26,040 --> 00:01:29,400
off a lot of research in Psychology

40
00:01:29,400 --> 00:01:32,340
cognitive science political science uh

41
00:01:32,340 --> 00:01:34,020
trying to understand and I guess in in

42
00:01:34,020 --> 00:01:37,100
CS and and HCI trying to understand

43
00:01:37,100 --> 00:01:40,020
Who falls for misinformation and what to

44
00:01:40,020 --> 00:01:43,920
do about it and how to combat it and uh

45
00:01:43,920 --> 00:01:46,439
we have a paper

46
00:01:46,439 --> 00:01:49,259
um last year that sort of synthesizes a

47
00:01:49,259 --> 00:01:51,720
lot of This research uh

48
00:01:51,720 --> 00:01:53,640
just a quick takeaways from this in

49
00:01:53,640 --> 00:01:54,479
terms of why people believe

50
00:01:54,479 --> 00:01:56,100
misinformation

51
00:01:56,100 --> 00:01:57,960
if things are repeated people believe it

52
00:01:57,960 --> 00:02:00,060
more if things align with people's

53
00:02:00,060 --> 00:02:01,560
pre-existing beliefs they believe it

54
00:02:01,560 --> 00:02:03,420
more if it comes from a source that you

55
00:02:03,420 --> 00:02:05,399
trust or the source of authority you

56
00:02:05,399 --> 00:02:07,920
believe it more and people that engage

57
00:02:07,920 --> 00:02:09,538
in less reasoning and less critical

58
00:02:09,538 --> 00:02:11,760
thinking are specifically more likely to

59
00:02:11,760 --> 00:02:14,099
believe false claims and also a lack of

60
00:02:14,099 --> 00:02:16,080
digital literacy and media literacy is

61
00:02:16,080 --> 00:02:18,120
associated with specifically believing

62
00:02:18,120 --> 00:02:20,220
false claims so there's this big body of

63
00:02:20,220 --> 00:02:24,599
work that has been amassed but the work

64
00:02:24,599 --> 00:02:27,180
has almost entirely focused on the US

65
00:02:27,180 --> 00:02:29,340
with an occasional study from Western

66
00:02:29,340 --> 00:02:31,620
Europe uh popping up in there and you

67
00:02:31,620 --> 00:02:34,140
know there's there's the the volume of

68
00:02:34,140 --> 00:02:36,480
work focused on the US has vastly

69
00:02:36,480 --> 00:02:38,879
exceeded pretty much everywhere else

70
00:02:38,879 --> 00:02:41,940
but misinformation is not a problem that

71
00:02:41,940 --> 00:02:44,940
is confined to the U.S there's lots of

72
00:02:44,940 --> 00:02:47,940
examples of really high stakes uh false

73
00:02:47,940 --> 00:02:50,160
claims all around the world if you talk

74
00:02:50,160 --> 00:02:52,140
to people at tech companies that work on

75
00:02:52,140 --> 00:02:54,239
misinformation they'll often say that

76
00:02:54,239 --> 00:02:55,620
they're actually much less worried about

77
00:02:55,620 --> 00:02:57,360
misinformation in the U.S than they are

78
00:02:57,360 --> 00:02:59,640
in lots of other parts of the world and

79
00:02:59,640 --> 00:03:00,560
so

80
00:03:00,560 --> 00:03:02,459
the study that I'm going to tell you

81
00:03:02,459 --> 00:03:05,519
about today tries to take a global view

82
00:03:05,519 --> 00:03:07,980
of the misinformation problem and in

83
00:03:07,980 --> 00:03:09,420
doing that kind of looks at many

84
00:03:09,420 --> 00:03:10,800
different things that we've looked at

85
00:03:10,800 --> 00:03:12,599
previously in the context of the U.S

86
00:03:12,599 --> 00:03:14,760
around the world

87
00:03:14,760 --> 00:03:18,360
so we ran this study with over 34 000

88
00:03:18,360 --> 00:03:20,580
participants recruited from 16 different

89
00:03:20,580 --> 00:03:23,159
countries on all six continents

90
00:03:23,159 --> 00:03:26,220
um and within each country we sampled uh

91
00:03:26,220 --> 00:03:27,959
social media users although that

92
00:03:27,959 --> 00:03:29,519
includes WhatsApp users which is

93
00:03:29,519 --> 00:03:31,080
essentially everyone everywhere outside

94
00:03:31,080 --> 00:03:33,019
of the US

95
00:03:33,019 --> 00:03:35,340
and there was sampled to be

96
00:03:35,340 --> 00:03:38,040
representative on age distribution and

97
00:03:38,040 --> 00:03:39,480
gender distribution within each country

98
00:03:39,480 --> 00:03:41,940
not truly representative samples but at

99
00:03:41,940 --> 00:03:43,319
least you know it's in the direction of

100
00:03:43,319 --> 00:03:45,200
of being representative

101
00:03:45,200 --> 00:03:49,620
and what we did is each subject in this

102
00:03:49,620 --> 00:03:52,680
online study was shown a set of 20

103
00:03:52,680 --> 00:03:55,019
statements about covid-19 half of them

104
00:03:55,019 --> 00:03:58,260
true and half of them false and these

105
00:03:58,260 --> 00:03:59,940
sample these statements were sampled

106
00:03:59,940 --> 00:04:02,580
from a larger set of 30 false and 15

107
00:04:02,580 --> 00:04:04,379
true just presented as statements

108
00:04:04,379 --> 00:04:06,360
without information image resource

109
00:04:06,360 --> 00:04:08,099
information and then they're randomized

110
00:04:08,099 --> 00:04:09,480
into one of four conditions that I'll

111
00:04:09,480 --> 00:04:11,459
tell you about in a minute but the thing

112
00:04:11,459 --> 00:04:13,439
about kovid is that it gives an

113
00:04:13,439 --> 00:04:15,360
opportunity for doing cross cultural

114
00:04:15,360 --> 00:04:17,459
research on misinformation that

115
00:04:17,459 --> 00:04:19,199
otherwise is really hard because if

116
00:04:19,199 --> 00:04:20,699
you're studying politics and political

117
00:04:20,699 --> 00:04:23,100
misinformation the politics of every

118
00:04:23,100 --> 00:04:24,540
country is different every country will

119
00:04:24,540 --> 00:04:25,860
have its own set of headlines and

120
00:04:25,860 --> 00:04:27,660
statements that are relevant and so if

121
00:04:27,660 --> 00:04:29,340
you see differences across countries you

122
00:04:29,340 --> 00:04:30,479
don't know whether that's because of

123
00:04:30,479 --> 00:04:32,520
actual cross-cultural differences or

124
00:04:32,520 --> 00:04:33,960
just some idiosyncrasies of which

125
00:04:33,960 --> 00:04:35,759
headlines or statements you were using

126
00:04:35,759 --> 00:04:38,639
but because covet is this truly Global

127
00:04:38,639 --> 00:04:40,560
phenomena we're able to come up with a

128
00:04:40,560 --> 00:04:41,759
set of headlines where the same

129
00:04:41,759 --> 00:04:43,560
headlines are relevant everywhere in the

130
00:04:43,560 --> 00:04:45,479
world and to give you a flavor of what

131
00:04:45,479 --> 00:04:47,220
these kinds of headlines are

132
00:04:47,220 --> 00:04:49,500
here's a sample of the false Headlands

133
00:04:49,500 --> 00:04:51,479
you've got things that like claiming

134
00:04:51,479 --> 00:04:53,639
that things that work actually work are

135
00:04:53,639 --> 00:04:55,500
ineffective claiming that things are

136
00:04:55,500 --> 00:04:58,020
ineffective actually work

137
00:04:58,020 --> 00:05:02,100
um questioning the severity of covid

138
00:05:02,100 --> 00:05:03,900
um and also sort of raising questions

139
00:05:03,900 --> 00:05:05,340
about vaccines

140
00:05:05,340 --> 00:05:07,259
and these were all taken from either

141
00:05:07,259 --> 00:05:08,820
fact-checking websites that say they

142
00:05:08,820 --> 00:05:12,360
were false or you know who type lists of

143
00:05:12,360 --> 00:05:14,400
uh coveted myths and things like that

144
00:05:14,400 --> 00:05:16,560
and then we also just had a set of true

145
00:05:16,560 --> 00:05:18,660
headlines that were from a variety of

146
00:05:18,660 --> 00:05:19,800
sources

147
00:05:19,800 --> 00:05:23,039
so the people you know rate uh set these

148
00:05:23,039 --> 00:05:25,320
headlines and then we're going to use

149
00:05:25,320 --> 00:05:27,300
this to try to answer a couple of

150
00:05:27,300 --> 00:05:28,740
questions

151
00:05:28,740 --> 00:05:29,280
um

152
00:05:29,280 --> 00:05:31,979
and so the first question that we want

153
00:05:31,979 --> 00:05:34,139
to know is Who falls for misinformation

154
00:05:34,139 --> 00:05:36,600
what are the characteristics of an

155
00:05:36,600 --> 00:05:37,979
individual that make them more or less

156
00:05:37,979 --> 00:05:39,780
predisposed to believe false claims like

157
00:05:39,780 --> 00:05:41,039
I said this is something we've studied a

158
00:05:41,039 --> 00:05:42,320
lot in the U.S

159
00:05:42,320 --> 00:05:45,240
and so in order to get insight into this

160
00:05:45,240 --> 00:05:47,160
we use the accuracy condition so a

161
00:05:47,160 --> 00:05:49,919
quarter of our 34 000 people when they

162
00:05:49,919 --> 00:05:51,660
did the survey they saw this where they

163
00:05:51,660 --> 00:05:53,340
were shown each statement and they were

164
00:05:53,340 --> 00:05:54,780
asked to the best of your knowledge is

165
00:05:54,780 --> 00:05:56,639
the above headline accurate and this

166
00:05:56,639 --> 00:05:58,979
underline is for your emphasis it was

167
00:05:58,979 --> 00:06:00,660
not actually shown to them and then they

168
00:06:00,660 --> 00:06:02,400
just rate this on the six point scale of

169
00:06:02,400 --> 00:06:04,440
how accurate they think each headline is

170
00:06:04,440 --> 00:06:06,840
and so given this we can assess for each

171
00:06:06,840 --> 00:06:08,340
person how good they are at telling true

172
00:06:08,340 --> 00:06:10,259
from false headlines

173
00:06:10,259 --> 00:06:11,400
um and I'm going to start by just

174
00:06:11,400 --> 00:06:14,220
showing you country level descriptives

175
00:06:14,220 --> 00:06:17,039
um so this is the average accuracy

176
00:06:17,039 --> 00:06:18,960
rating where zero is saying it's totally

177
00:06:18,960 --> 00:06:20,940
inaccurate and one would be saying it's

178
00:06:20,940 --> 00:06:22,800
totally accurate

179
00:06:22,800 --> 00:06:24,539
um for the headlines that are true in

180
00:06:24,539 --> 00:06:26,819
blue and false and red across all our 16

181
00:06:26,819 --> 00:06:28,259
countries I'm going to start with the US

182
00:06:28,259 --> 00:06:31,080
so here you can see that about 60

183
00:06:31,080 --> 00:06:32,460
percent of the head the true headlines

184
00:06:32,460 --> 00:06:35,100
were believed and about 25 or 30 percent

185
00:06:35,100 --> 00:06:36,900
of the false headlines were believed

186
00:06:36,900 --> 00:06:38,460
this lines up pretty well with what

187
00:06:38,460 --> 00:06:40,020
we've seen in countless experiments that

188
00:06:40,020 --> 00:06:41,819
we've run people do believe the true

189
00:06:41,819 --> 00:06:43,199
headlines more than the false headlines

190
00:06:43,199 --> 00:06:44,639
but they're also believing a lot of the

191
00:06:44,639 --> 00:06:46,500
false headlines and failing to believe a

192
00:06:46,500 --> 00:06:47,880
lot of the true headlines

193
00:06:47,880 --> 00:06:49,440
and so then you know our first order

194
00:06:49,440 --> 00:06:51,419
question is just how General is this or

195
00:06:51,419 --> 00:06:52,800
how much does this vary across

196
00:06:52,800 --> 00:06:55,680
conditions sorry across countries and

197
00:06:55,680 --> 00:06:57,680
what we find is there's actually

198
00:06:57,680 --> 00:07:00,000
remarkably little variation across

199
00:07:00,000 --> 00:07:01,800
countries in the belief in the true

200
00:07:01,800 --> 00:07:04,319
headlines while there is substantial

201
00:07:04,319 --> 00:07:06,900
variation across countries in the belief

202
00:07:06,900 --> 00:07:08,580
in false headlines

203
00:07:08,580 --> 00:07:10,560
our study was not really focused on the

204
00:07:10,560 --> 00:07:12,660
cross-cultural differences we're trying

205
00:07:12,660 --> 00:07:14,759
to look for cross-cultural regularities

206
00:07:14,759 --> 00:07:18,120
in what features of individuals predict

207
00:07:18,120 --> 00:07:20,759
sustability but obviously you know we

208
00:07:20,759 --> 00:07:22,319
have this big cross-cultural data set so

209
00:07:22,319 --> 00:07:23,940
I'll just briefly note that some country

210
00:07:23,940 --> 00:07:27,240
level factors that correlate with uh

211
00:07:27,240 --> 00:07:30,139
belief in the false claims are

212
00:07:30,139 --> 00:07:33,000
so a low democracy index so lack of

213
00:07:33,000 --> 00:07:34,560
access to political rights and civil

214
00:07:34,560 --> 00:07:36,539
liberties countries that are like have

215
00:07:36,539 --> 00:07:38,759
less of that access we're more likely to

216
00:07:38,759 --> 00:07:41,280
believe false claims countries where

217
00:07:41,280 --> 00:07:42,900
people were more accepting of unequal

218
00:07:42,900 --> 00:07:45,360
divisions of power or more likely to

219
00:07:45,360 --> 00:07:47,520
believe false claims and countries that

220
00:07:47,520 --> 00:07:50,039
were more collectivist were more likely

221
00:07:50,039 --> 00:07:52,020
to believe false claims and so the sort

222
00:07:52,020 --> 00:07:53,280
of digging into the cross-cultural

223
00:07:53,280 --> 00:07:54,900
aspect of it is something that we are

224
00:07:54,900 --> 00:07:56,699
going to do in future work but what I'm

225
00:07:56,699 --> 00:07:58,560
going to focus on now is this more

226
00:07:58,560 --> 00:08:01,440
individual question which is within each

227
00:08:01,440 --> 00:08:03,419
country what are the differences across

228
00:08:03,419 --> 00:08:05,280
individuals that predict who is

229
00:08:05,280 --> 00:08:07,380
susceptible to misinformation

230
00:08:07,380 --> 00:08:09,360
and uh there's a few different

231
00:08:09,360 --> 00:08:11,460
perspectives on this question that have

232
00:08:11,460 --> 00:08:13,440
been proposed before and that we're able

233
00:08:13,440 --> 00:08:15,419
to sort of look at in this in this big

234
00:08:15,419 --> 00:08:17,340
sample so the cognitive perspective

235
00:08:17,340 --> 00:08:19,440
which is the thing that me and my uh

236
00:08:19,440 --> 00:08:20,940
co-author Gord pennycook have really

237
00:08:20,940 --> 00:08:24,840
done a lot of work on has shown that

238
00:08:24,840 --> 00:08:26,699
when people rely on in on their

239
00:08:26,699 --> 00:08:29,039
intuition they don't stop and think

240
00:08:29,039 --> 00:08:30,960
critically they're more likely to fall

241
00:08:30,960 --> 00:08:33,419
for false claims this is true for covid

242
00:08:33,419 --> 00:08:34,740
and this is also true for political

243
00:08:34,740 --> 00:08:36,179
headlines regardless of whether they

244
00:08:36,179 --> 00:08:39,059
align with people's politics or not

245
00:08:39,059 --> 00:08:41,640
um and so you know to assess this we

246
00:08:41,640 --> 00:08:43,679
gave people a set of math problems with

247
00:08:43,679 --> 00:08:45,480
intuitively compelling but wrong answers

248
00:08:45,480 --> 00:08:47,100
like you're running a race and you pass

249
00:08:47,100 --> 00:08:48,720
the person in second place what place

250
00:08:48,720 --> 00:08:50,700
are you in you might think first place

251
00:08:50,700 --> 00:08:52,500
but if you stop and think for one second

252
00:08:52,500 --> 00:08:53,880
you'll be like oh no if you pass the

253
00:08:53,880 --> 00:08:55,740
person in second place then you're in

254
00:08:55,740 --> 00:08:57,720
second place so you can use these as a

255
00:08:57,720 --> 00:08:59,100
measure of in general how much people

256
00:08:59,100 --> 00:09:01,019
tend to stop and think versus just going

257
00:09:01,019 --> 00:09:03,000
with their first response and we've also

258
00:09:03,000 --> 00:09:04,200
just asked them essentially how much

259
00:09:04,200 --> 00:09:05,820
they like thinking and we have some

260
00:09:05,820 --> 00:09:09,480
basic attention checks and then we run a

261
00:09:09,480 --> 00:09:12,839
series of regression models where for

262
00:09:12,839 --> 00:09:14,940
every headline rating we predict how

263
00:09:14,940 --> 00:09:16,680
accurate people thought the rating was

264
00:09:16,680 --> 00:09:18,779
based on whether it was actually true or

265
00:09:18,779 --> 00:09:22,260
not and based on you know how well they

266
00:09:22,260 --> 00:09:24,480
did on this cognitive reflection test or

267
00:09:24,480 --> 00:09:27,120
how basically their score for each of

268
00:09:27,120 --> 00:09:28,440
these different individual differences

269
00:09:28,440 --> 00:09:29,880
and then we look at the interaction

270
00:09:29,880 --> 00:09:31,500
between those two things since that

271
00:09:31,500 --> 00:09:34,200
tells you how much does your score on

272
00:09:34,200 --> 00:09:36,000
one of these measures change how

273
00:09:36,000 --> 00:09:37,920
sensitive you are to the actual truth of

274
00:09:37,920 --> 00:09:40,019
the headline line when you're trying to

275
00:09:40,019 --> 00:09:41,820
judge its accuracy and then everything

276
00:09:41,820 --> 00:09:43,019
that I'm going to show you includes

277
00:09:43,019 --> 00:09:44,760
controls for age gender whether they

278
00:09:44,760 --> 00:09:46,500
have a college degree or not and their

279
00:09:46,500 --> 00:09:49,440
perceived relative socioeconomic status

280
00:09:49,440 --> 00:09:51,120
so I'm going to show you a bunch of

281
00:09:51,120 --> 00:09:53,700
these results and the general format is

282
00:09:53,700 --> 00:09:55,800
there's going to be one row for each

283
00:09:55,800 --> 00:09:58,860
individual difference measure and I'm

284
00:09:58,860 --> 00:10:00,420
going to show you the coefficient that

285
00:10:00,420 --> 00:10:02,339
I'm going to show you is this like how

286
00:10:02,339 --> 00:10:04,200
related it is to people's ability to

287
00:10:04,200 --> 00:10:06,660
tell truth from falsehood so positive

288
00:10:06,660 --> 00:10:08,519
values means people that are higher on

289
00:10:08,519 --> 00:10:09,839
this are better at telling true from

290
00:10:09,839 --> 00:10:12,680
false lower means they're worse

291
00:10:12,680 --> 00:10:14,300
and

292
00:10:14,300 --> 00:10:16,980
here if this is the the preference for

293
00:10:16,980 --> 00:10:18,120
thinking how well they do in the

294
00:10:18,120 --> 00:10:19,320
thinking test and how many of the

295
00:10:19,320 --> 00:10:21,720
tentiveness questions they get right and

296
00:10:21,720 --> 00:10:23,880
each Little Dot is an estimate from one

297
00:10:23,880 --> 00:10:26,160
of the 16 countries and then the Big Dot

298
00:10:26,160 --> 00:10:27,540
what the confidence interval is a

299
00:10:27,540 --> 00:10:29,640
meta-analytic estimate across all 16

300
00:10:29,640 --> 00:10:32,820
countries and so what you can see is in

301
00:10:32,820 --> 00:10:35,100
every single country and very strongly

302
00:10:35,100 --> 00:10:37,320
meta-analytically people that engage in

303
00:10:37,320 --> 00:10:39,240
more thinking are better at telling

304
00:10:39,240 --> 00:10:41,040
truth from falsehood and in particular

305
00:10:41,040 --> 00:10:42,959
are less likely to believe false claims

306
00:10:42,959 --> 00:10:44,820
this is all controlling for education

307
00:10:44,820 --> 00:10:46,860
and if you look at education on its own

308
00:10:46,860 --> 00:10:49,019
it's actually much more weekly

309
00:10:49,019 --> 00:10:51,899
associated created with with belief in

310
00:10:51,899 --> 00:10:53,100
false claims

311
00:10:53,100 --> 00:10:54,779
and this Echoes results that we've

312
00:10:54,779 --> 00:10:56,279
gotten in the U.S where we actually

313
00:10:56,279 --> 00:10:57,959
experimentally manipulate how much

314
00:10:57,959 --> 00:11:00,360
people pay attention uh like engage in

315
00:11:00,360 --> 00:11:01,800
critical thinking if you distract them

316
00:11:01,800 --> 00:11:03,959
or if you get them to feel emotional

317
00:11:03,959 --> 00:11:06,060
that makes them more likely to believe

318
00:11:06,060 --> 00:11:08,040
false claims regardless of whether it

319
00:11:08,040 --> 00:11:10,040
aligns with their politics

320
00:11:10,040 --> 00:11:13,140
so this seems like strong cross-cultural

321
00:11:13,140 --> 00:11:14,880
support for the idea that cognitive

322
00:11:14,880 --> 00:11:16,860
sophistication matters and when people

323
00:11:16,860 --> 00:11:18,600
don't think carefully they're more

324
00:11:18,600 --> 00:11:20,640
susceptible to false claims

325
00:11:20,640 --> 00:11:22,320
and there's also a social perspective

326
00:11:22,320 --> 00:11:25,260
which focuses on motivation there's lots

327
00:11:25,260 --> 00:11:26,700
of different kind of motivations that

328
00:11:26,700 --> 00:11:28,380
could lead you to believe false claims

329
00:11:28,380 --> 00:11:32,040
but if you're really motivated to have

330
00:11:32,040 --> 00:11:34,200
accurate beliefs then you should indeed

331
00:11:34,200 --> 00:11:35,700
have more accurate beliefs we measure

332
00:11:35,700 --> 00:11:37,019
this with a couple of questions about

333
00:11:37,019 --> 00:11:38,519
whether people should rely on evidence

334
00:11:38,519 --> 00:11:41,040
or just listen to political Elites and

335
00:11:41,040 --> 00:11:42,420
when you share things online how

336
00:11:42,420 --> 00:11:43,440
important is it to you that it's

337
00:11:43,440 --> 00:11:45,420
accurate and then we also looked at a

338
00:11:45,420 --> 00:11:46,560
different kind of social explanation

339
00:11:46,560 --> 00:11:48,480
involving trust which is maybe the

340
00:11:48,480 --> 00:11:49,800
people that believe false claims are

341
00:11:49,800 --> 00:11:51,540
just more gullible in general so we just

342
00:11:51,540 --> 00:11:54,300
ask how much they generally trust people

343
00:11:54,300 --> 00:11:57,839
and what we found is uh how important

344
00:11:57,839 --> 00:11:59,220
people thought it was to share accurate

345
00:11:59,220 --> 00:12:00,779
information and to listen to evidence

346
00:12:00,779 --> 00:12:03,779
over a political like party cues were

347
00:12:03,779 --> 00:12:05,760
both also strong predictors where people

348
00:12:05,760 --> 00:12:07,800
that care more about accuracy are less

349
00:12:07,800 --> 00:12:09,240
likely to believe the false claims

350
00:12:09,240 --> 00:12:11,220
whereas trust didn't really do anything

351
00:12:11,220 --> 00:12:13,200
on its own

352
00:12:13,200 --> 00:12:15,240
and then a third perspective that has

353
00:12:15,240 --> 00:12:16,920
gotten a lot of attention is a an

354
00:12:16,920 --> 00:12:19,019
ideological perspective there's a lot of

355
00:12:19,019 --> 00:12:20,700
evidence from the U.S that at least in

356
00:12:20,700 --> 00:12:23,459
the current uh socio-political media

357
00:12:23,459 --> 00:12:25,800
environment conservatives are more

358
00:12:25,800 --> 00:12:27,899
susceptible more likely to believe false

359
00:12:27,899 --> 00:12:30,720
claims than liberals and so we wanted to

360
00:12:30,720 --> 00:12:33,480
know does this generalize and we didn't

361
00:12:33,480 --> 00:12:35,040
see any particular reason to expect that

362
00:12:35,040 --> 00:12:36,959
it would generalize so we wanted to test

363
00:12:36,959 --> 00:12:39,300
that and then we also wanted to see what

364
00:12:39,300 --> 00:12:42,600
about other dimensions of ideology like

365
00:12:42,600 --> 00:12:44,760
um and so to measure conservatism we use

366
00:12:44,760 --> 00:12:47,040
this classic question about should

367
00:12:47,040 --> 00:12:48,779
government take responsible for

368
00:12:48,779 --> 00:12:50,040
individuals or should people take

369
00:12:50,040 --> 00:12:52,200
responsibility for themselves and then

370
00:12:52,200 --> 00:12:54,240
we also asked how important people think

371
00:12:54,240 --> 00:12:56,459
democracy is how much they favor

372
00:12:56,459 --> 00:12:59,639
equality versus greater incentives how

373
00:12:59,639 --> 00:13:01,200
much they you know think there's like

374
00:13:01,200 --> 00:13:03,000
moral relativism versus their absolute

375
00:13:03,000 --> 00:13:04,800
more rules and also how much they

376
00:13:04,800 --> 00:13:07,019
believe in God or gods

377
00:13:07,019 --> 00:13:09,120
and what we found was first of all

378
00:13:09,120 --> 00:13:11,040
thinking that people who think that

379
00:13:11,040 --> 00:13:13,139
democracy is important are much less

380
00:13:13,139 --> 00:13:15,560
likely to believe false claims

381
00:13:15,560 --> 00:13:18,060
and that was true everywhere and to our

382
00:13:18,060 --> 00:13:19,980
surprise we also found that in most

383
00:13:19,980 --> 00:13:21,720
countries people that were more

384
00:13:21,720 --> 00:13:22,920
conservative that more thought

385
00:13:22,920 --> 00:13:24,420
individuals should be responsible for

386
00:13:24,420 --> 00:13:26,639
themselves were more likely to believe

387
00:13:26,639 --> 00:13:28,380
the false claim so it wasn't just the

388
00:13:28,380 --> 00:13:31,079
U.S but we found a general association

389
00:13:31,079 --> 00:13:33,600
between this kind of conservatism and

390
00:13:33,600 --> 00:13:35,820
belief in false claims we also found

391
00:13:35,820 --> 00:13:37,560
that people that believe in God were

392
00:13:37,560 --> 00:13:39,120
more likely to believe the false coveted

393
00:13:39,120 --> 00:13:41,579
claims in most but not all countries and

394
00:13:41,579 --> 00:13:43,320
then found a lot of variation and so no

395
00:13:43,320 --> 00:13:44,760
consistent pattern on the other two

396
00:13:44,760 --> 00:13:47,040
ideological measures we also looked at a

397
00:13:47,040 --> 00:13:48,420
bunch of demographics that I won't

398
00:13:48,420 --> 00:13:50,040
really get into but for completeness

399
00:13:50,040 --> 00:13:52,380
I'll show you and finally we found that

400
00:13:52,380 --> 00:13:54,180
pretty much everywhere people that

401
00:13:54,180 --> 00:13:55,980
believed the false coveted claims more

402
00:13:55,980 --> 00:13:58,800
were less inclined to get vaccinated

403
00:13:58,800 --> 00:14:01,620
so this you know hints at some kind of

404
00:14:01,620 --> 00:14:03,600
it's obviously not causal but it

405
00:14:03,600 --> 00:14:05,519
suggests that coveted misinformation may

406
00:14:05,519 --> 00:14:07,980
be uh you know driving people to not

407
00:14:07,980 --> 00:14:12,360
believe uh to not want to get vaccinated

408
00:14:12,360 --> 00:14:15,360
okay so just to summarize

409
00:14:15,360 --> 00:14:16,920
with their support for all of these

410
00:14:16,920 --> 00:14:19,380
different perspectives uh what being

411
00:14:19,380 --> 00:14:21,839
protected against believing false claims

412
00:14:21,839 --> 00:14:23,220
comes from being cognitively

413
00:14:23,220 --> 00:14:25,260
sophisticated being motivated to care

414
00:14:25,260 --> 00:14:27,600
about accuracy and certain ideological

415
00:14:27,600 --> 00:14:29,399
factors like being liberal Democratic

416
00:14:29,399 --> 00:14:31,139
and atheist

417
00:14:31,139 --> 00:14:31,680
um

418
00:14:31,680 --> 00:14:36,000
so uh this gives you a sort of

419
00:14:36,000 --> 00:14:38,339
um a catalog of things that make

420
00:14:38,339 --> 00:14:40,079
individuals susceptible to believing

421
00:14:40,079 --> 00:14:42,300
misinformation but one of the things

422
00:14:42,300 --> 00:14:44,820
that is really interesting about this

423
00:14:44,820 --> 00:14:47,040
kind of modern form of misinformation

424
00:14:47,040 --> 00:14:49,320
and fake news is that often it spreads

425
00:14:49,320 --> 00:14:51,839
online so in addition to belief there's

426
00:14:51,839 --> 00:14:53,579
also the question of why do people share

427
00:14:53,579 --> 00:14:55,740
misinformation

428
00:14:55,740 --> 00:14:58,560
um and uh you know so to to get some

429
00:14:58,560 --> 00:15:00,959
insight into this uh another quarter of

430
00:15:00,959 --> 00:15:02,579
our subjects were randomized into the

431
00:15:02,579 --> 00:15:04,740
sharing condition where for each item

432
00:15:04,740 --> 00:15:06,240
instead of being asked to judge's

433
00:15:06,240 --> 00:15:07,980
accuracy they were asked if you were to

434
00:15:07,980 --> 00:15:09,779
see it online How likely would you be to

435
00:15:09,779 --> 00:15:11,579
share it

436
00:15:11,579 --> 00:15:13,320
um and you know when we first started

437
00:15:13,320 --> 00:15:15,180
working on this I just assumed that

438
00:15:15,180 --> 00:15:17,160
sharing was like a behavioral measure of

439
00:15:17,160 --> 00:15:18,899
how much you believed something so it

440
00:15:18,899 --> 00:15:20,399
was a way you know that belief and

441
00:15:20,399 --> 00:15:22,500
sharing would be the same thing

442
00:15:22,500 --> 00:15:24,600
um but we've subsequently kind of run

443
00:15:24,600 --> 00:15:26,220
studies that suggest that that may not

444
00:15:26,220 --> 00:15:27,720
be the case

445
00:15:27,720 --> 00:15:29,160
um and so just to give you a flavor of

446
00:15:29,160 --> 00:15:31,320
this what I'm going to show you now is

447
00:15:31,320 --> 00:15:33,060
uh so this is just I'm going to start

448
00:15:33,060 --> 00:15:35,220
just with the US this is the data that I

449
00:15:35,220 --> 00:15:37,380
showed you earlier about the how much

450
00:15:37,380 --> 00:15:39,480
people believed the true versus false

451
00:15:39,480 --> 00:15:41,160
claims and now I'm going to show you the

452
00:15:41,160 --> 00:15:43,320
same plot for how much people in the U.S

453
00:15:43,320 --> 00:15:44,940
said that they would be inclined to

454
00:15:44,940 --> 00:15:47,639
share true and false claims

455
00:15:47,639 --> 00:15:49,800
and what you see is this strikingly

456
00:15:49,800 --> 00:15:51,779
different pattern where they're less

457
00:15:51,779 --> 00:15:53,880
likely to share the true claims and

458
00:15:53,880 --> 00:15:55,680
maybe more importantly they're more

459
00:15:55,680 --> 00:15:58,500
likely to share the false claims

460
00:15:58,500 --> 00:15:59,880
um and so there's some disconnect

461
00:15:59,880 --> 00:16:02,399
between accuracy and sharing and sharing

462
00:16:02,399 --> 00:16:05,399
isn't simply showing what you believe or

463
00:16:05,399 --> 00:16:06,740
don't believe

464
00:16:06,740 --> 00:16:10,079
and this is not just true in the U.S

465
00:16:10,079 --> 00:16:12,120
what I'm showing you here the purple dot

466
00:16:12,120 --> 00:16:14,639
or the purple square is the difference

467
00:16:14,639 --> 00:16:16,920
in like how much more people believed

468
00:16:16,920 --> 00:16:18,959
the true headlines compared to the false

469
00:16:18,959 --> 00:16:20,040
headlines so it's like the difference

470
00:16:20,040 --> 00:16:21,899
between the blue and red bars for each

471
00:16:21,899 --> 00:16:23,220
country

472
00:16:23,220 --> 00:16:24,300
um and this is what I showed you before

473
00:16:24,300 --> 00:16:25,980
of the accuracy judgments this is

474
00:16:25,980 --> 00:16:28,199
another way of viewing that data so like

475
00:16:28,199 --> 00:16:31,079
in the US they Sorry in the UK they

476
00:16:31,079 --> 00:16:32,459
believed the true headlines much more

477
00:16:32,459 --> 00:16:35,100
than the false headlines in India they

478
00:16:35,100 --> 00:16:36,839
only believed the true headlines a bit

479
00:16:36,839 --> 00:16:39,000
more than the false headlines and now

480
00:16:39,000 --> 00:16:40,500
you can say what about the same thing

481
00:16:40,500 --> 00:16:42,240
where instead of accuracy judgments

482
00:16:42,240 --> 00:16:44,399
you're looking at sharing intentions and

483
00:16:44,399 --> 00:16:46,680
so in Gold you see how much more likely

484
00:16:46,680 --> 00:16:48,540
they were to share the true headlines

485
00:16:48,540 --> 00:16:50,940
compared to the false headlines and what

486
00:16:50,940 --> 00:16:53,160
you see is in every country

487
00:16:53,160 --> 00:16:55,680
sharing was less Discerning than

488
00:16:55,680 --> 00:16:58,199
accuracy although the extent of that

489
00:16:58,199 --> 00:17:00,120
disconnect varies across countries and

490
00:17:00,120 --> 00:17:01,560
places like the UK there was a huge

491
00:17:01,560 --> 00:17:03,899
disconnect in some other places like

492
00:17:03,899 --> 00:17:06,119
Italy or India there was less of a

493
00:17:06,119 --> 00:17:06,859
disconnect

494
00:17:06,859 --> 00:17:09,660
but in general there's a disconnect

495
00:17:09,660 --> 00:17:11,579
between accuracy and sharing and so an

496
00:17:11,579 --> 00:17:13,260
actual question is like what's going on

497
00:17:13,260 --> 00:17:15,839
like why are people less Discerning when

498
00:17:15,839 --> 00:17:18,599
they're deciding what to share than when

499
00:17:18,599 --> 00:17:20,520
they're thinking about what's accurate

500
00:17:20,520 --> 00:17:22,500
and so one explanation is people

501
00:17:22,500 --> 00:17:24,720
purposely share false claims to promote

502
00:17:24,720 --> 00:17:26,939
some kind of agenda I think that happens

503
00:17:26,939 --> 00:17:29,720
occasionally but not that often another

504
00:17:29,720 --> 00:17:32,880
uh explanation is people are sharing

505
00:17:32,880 --> 00:17:34,320
these things to debunk them or to make

506
00:17:34,320 --> 00:17:36,360
fun of them but we've looked at that and

507
00:17:36,360 --> 00:17:37,740
that action actually happens quite

508
00:17:37,740 --> 00:17:39,960
rarely and instead of like 2 000 shares

509
00:17:39,960 --> 00:17:41,820
of false things a false claims that was

510
00:17:41,820 --> 00:17:43,260
only maybe like four percent of cases

511
00:17:43,260 --> 00:17:45,240
where people are sharing it to debunk it

512
00:17:45,240 --> 00:17:48,480
and so what we think is a driver of a

513
00:17:48,480 --> 00:17:50,460
lot of this disconnect between accuracy

514
00:17:50,460 --> 00:17:52,740
and sharing is that people simply aren't

515
00:17:52,740 --> 00:17:54,480
paying attention because they're

516
00:17:54,480 --> 00:17:57,059
distracted by the social media context

517
00:17:57,059 --> 00:17:58,860
when you're actually online and you're

518
00:17:58,860 --> 00:18:01,919
scrolling you know the news is mixed in

519
00:18:01,919 --> 00:18:04,260
with baby pictures and Cat videos and

520
00:18:04,260 --> 00:18:05,400
all kinds of other stuff that is

521
00:18:05,400 --> 00:18:06,840
emotionally evocative content where

522
00:18:06,840 --> 00:18:08,820
accuracy is not relevant so it doesn't

523
00:18:08,820 --> 00:18:10,679
exactly put you in a reflective mindset

524
00:18:10,679 --> 00:18:12,299
plus you're scrolling quickly and you're

525
00:18:12,299 --> 00:18:14,120
trying to relax and unwind and all that

526
00:18:14,120 --> 00:18:17,640
and the social media context provides

527
00:18:17,640 --> 00:18:20,340
Quantified immediate social feedback

528
00:18:20,340 --> 00:18:21,840
where you can see how many people like

529
00:18:21,840 --> 00:18:24,059
things how many people shared things and

530
00:18:24,059 --> 00:18:26,100
that focuses you on these social factors

531
00:18:26,100 --> 00:18:28,740
and so our argument is all of that

532
00:18:28,740 --> 00:18:30,840
causes you to forget to even think about

533
00:18:30,840 --> 00:18:33,179
whether it's accurate or not before you

534
00:18:33,179 --> 00:18:34,080
share it

535
00:18:34,080 --> 00:18:36,240
and so if this is true then getting

536
00:18:36,240 --> 00:18:38,760
people to consider accuracy just not not

537
00:18:38,760 --> 00:18:40,440
giving them information not telling them

538
00:18:40,440 --> 00:18:41,760
what's true or not or giving them

539
00:18:41,760 --> 00:18:43,919
warning labels or whatever but just

540
00:18:43,919 --> 00:18:46,380
prompting the concept of accuracy to

541
00:18:46,380 --> 00:18:49,080
come to mind to people priming accuracy

542
00:18:49,080 --> 00:18:50,640
um should make them more Discerning in

543
00:18:50,640 --> 00:18:52,320
what they decide to share

544
00:18:52,320 --> 00:18:57,059
and so to test that we have the third uh

545
00:18:57,059 --> 00:18:59,580
chunk of people was randomized into the

546
00:18:59,580 --> 00:19:01,559
prompt condition and so in the prompt

547
00:19:01,559 --> 00:19:04,380
condition first uh you know when they

548
00:19:04,380 --> 00:19:05,400
when they before they know anything

549
00:19:05,400 --> 00:19:06,720
about what the main study is about

550
00:19:06,720 --> 00:19:08,100
they're like you know please help us

551
00:19:08,100 --> 00:19:09,539
test a headline we're interested in

552
00:19:09,539 --> 00:19:11,280
whether people think it's accurate or

553
00:19:11,280 --> 00:19:12,720
not we just need you to give your

554
00:19:12,720 --> 00:19:14,400
opinion about the accuracy of a headline

555
00:19:14,400 --> 00:19:17,039
then you go to the the primary task then

556
00:19:17,039 --> 00:19:19,919
we show them one headline not related to

557
00:19:19,919 --> 00:19:22,020
covet or politics just weird just kind

558
00:19:22,020 --> 00:19:24,840
of like banal everyday headlines have

559
00:19:24,840 --> 00:19:27,120
them rate the accuracy and then after

560
00:19:27,120 --> 00:19:29,460
they do that they go on to complete the

561
00:19:29,460 --> 00:19:31,679
sharing task the same as the people in

562
00:19:31,679 --> 00:19:34,200
the sharing condition but the idea is

563
00:19:34,200 --> 00:19:36,299
because of by rating the accuracy of

564
00:19:36,299 --> 00:19:37,500
this one headline at the beginning

565
00:19:37,500 --> 00:19:39,240
they'll be more likely to just have

566
00:19:39,240 --> 00:19:41,880
accuracy on the mind and so be more

567
00:19:41,880 --> 00:19:44,220
likely to ask themselves how accurate is

568
00:19:44,220 --> 00:19:45,539
this other content when they

569
00:19:45,539 --> 00:19:47,160
subsequently see it

570
00:19:47,160 --> 00:19:49,020
we've run a ton of these experiments in

571
00:19:49,020 --> 00:19:50,700
the U.S we had a paper in nature

572
00:19:50,700 --> 00:19:52,559
Communications earlier this year where

573
00:19:52,559 --> 00:19:54,960
we met analyzed 20 experiments and over

574
00:19:54,960 --> 00:19:57,840
26 000 participants that we'd run and we

575
00:19:57,840 --> 00:19:59,880
consistently find that this sort of

576
00:19:59,880 --> 00:20:02,640
prompting people to think about accuracy

577
00:20:02,640 --> 00:20:04,980
um before they engage in sharing and

578
00:20:04,980 --> 00:20:06,720
makes them less likely to share false

579
00:20:06,720 --> 00:20:09,059
claims it works for both political

580
00:20:09,059 --> 00:20:10,200
headlines that are aligned with your

581
00:20:10,200 --> 00:20:12,600
politics and ones that aren't it works

582
00:20:12,600 --> 00:20:14,100
for covet headlines it works for

583
00:20:14,100 --> 00:20:16,200
conservatives and liberals it works for

584
00:20:16,200 --> 00:20:17,940
a wide array of different headlines and

585
00:20:17,940 --> 00:20:19,320
different ways of prompting people to

586
00:20:19,320 --> 00:20:21,240
think about accuracy and the treatment

587
00:20:21,240 --> 00:20:23,100
lasts at least for the duration of the

588
00:20:23,100 --> 00:20:25,679
experimental session so we wanted to

589
00:20:25,679 --> 00:20:26,460
know

590
00:20:26,460 --> 00:20:28,440
do these results we found in the U.S

591
00:20:28,440 --> 00:20:30,720
generalize more broadly

592
00:20:30,720 --> 00:20:32,820
and so what I'm going to show you here

593
00:20:32,820 --> 00:20:34,980
is for each country

594
00:20:34,980 --> 00:20:37,380
the effect of prompting people to think

595
00:20:37,380 --> 00:20:38,880
about accuracy

596
00:20:38,880 --> 00:20:41,340
on how Discerning their sharing is so

597
00:20:41,340 --> 00:20:42,780
like the effect of prompting them to

598
00:20:42,780 --> 00:20:45,120
think about accuracy on the difference

599
00:20:45,120 --> 00:20:46,799
in their share probabilities for True

600
00:20:46,799 --> 00:20:50,220
headlines relative to false headlines

601
00:20:50,220 --> 00:20:52,799
and what you can see so you've got a bar

602
00:20:52,799 --> 00:20:54,120
for each country and then this is the

603
00:20:54,120 --> 00:20:56,520
meta-analytic estimate across all 16

604
00:20:56,520 --> 00:20:59,220
countries and so you can see on average

605
00:20:59,220 --> 00:21:02,880
the accuracy prompt is increasing the

606
00:21:02,880 --> 00:21:04,380
quality of what people are sharing by

607
00:21:04,380 --> 00:21:06,380
about you know a little under 20 percent

608
00:21:06,380 --> 00:21:09,179
so it works in general it's not only in

609
00:21:09,179 --> 00:21:11,400
the US but there is substantial

610
00:21:11,400 --> 00:21:14,460
variation in countries in terms of how

611
00:21:14,460 --> 00:21:16,440
well it works and so we want to try and

612
00:21:16,440 --> 00:21:18,000
understand this variation across

613
00:21:18,000 --> 00:21:20,520
countries and it's actually pretty

614
00:21:20,520 --> 00:21:23,460
straightforward to explain it so

615
00:21:23,460 --> 00:21:24,780
remember this plot that I showed you

616
00:21:24,780 --> 00:21:26,640
earlier where this was showing you in

617
00:21:26,640 --> 00:21:28,380
purple it's how good they are at telling

618
00:21:28,380 --> 00:21:29,760
true from false when they're judging

619
00:21:29,760 --> 00:21:32,820
accuracy and in Gold you have how much

620
00:21:32,820 --> 00:21:34,760
more likely they are to share true

621
00:21:34,760 --> 00:21:37,260
relative to false headlines

622
00:21:37,260 --> 00:21:39,900
and so if the idea is this accuracy

623
00:21:39,900 --> 00:21:42,000
prompt kind of closes this Gap by when

624
00:21:42,000 --> 00:21:43,500
they're in the sharing when they're

625
00:21:43,500 --> 00:21:44,580
thinking about sharing it makes them

626
00:21:44,580 --> 00:21:46,320
think about accuracy and so it moves the

627
00:21:46,320 --> 00:21:48,480
gold circles closer to the purple

628
00:21:48,480 --> 00:21:50,520
squares by just getting accuracy on

629
00:21:50,520 --> 00:21:51,840
people's minds

630
00:21:51,840 --> 00:21:54,720
that that should only help in so much as

631
00:21:54,720 --> 00:21:55,980
there is actually a big disconnect

632
00:21:55,980 --> 00:21:57,840
between accuracy and sharing in the

633
00:21:57,840 --> 00:21:59,940
first place like for the UK it should

634
00:21:59,940 --> 00:22:01,380
help a lot because there's a lot of room

635
00:22:01,380 --> 00:22:03,299
to move the gold up but for countries

636
00:22:03,299 --> 00:22:06,840
like you know Brazil or Italy or India

637
00:22:06,840 --> 00:22:08,340
there's not that much of a disconnect

638
00:22:08,340 --> 00:22:10,320
and so there's just not that much room

639
00:22:10,320 --> 00:22:12,659
for the accuracy prompt to help

640
00:22:12,659 --> 00:22:14,880
and so consistent with this what I'm

641
00:22:14,880 --> 00:22:16,500
showing you here is one dot per country

642
00:22:16,500 --> 00:22:18,780
where this is how much the accuracy

643
00:22:18,780 --> 00:22:20,820
prompt improved the quality of sharing

644
00:22:20,820 --> 00:22:22,860
and this is how much of a disconnect

645
00:22:22,860 --> 00:22:24,840
there was between accuracy and sharing

646
00:22:24,840 --> 00:22:26,580
in the first place and you can see it

647
00:22:26,580 --> 00:22:29,400
that like very strongly uh organizes

648
00:22:29,400 --> 00:22:30,840
these data

649
00:22:30,840 --> 00:22:32,580
and so it also provides evidence that

650
00:22:32,580 --> 00:22:34,500
what's going on here is really it's

651
00:22:34,500 --> 00:22:36,720
prompting people to think about accuracy

652
00:22:36,720 --> 00:22:38,400
another way of getting insight into that

653
00:22:38,400 --> 00:22:40,500
is looking at variation across headlines

654
00:22:40,500 --> 00:22:42,120
rather than across countries so now

655
00:22:42,120 --> 00:22:44,340
we've got our 30 false and 15 true

656
00:22:44,340 --> 00:22:46,620
headlines and for each headline we can

657
00:22:46,620 --> 00:22:48,659
say how much did getting people to think

658
00:22:48,659 --> 00:22:50,520
about accuracy reduce the sharing of

659
00:22:50,520 --> 00:22:52,440
that headline and then we can look at

660
00:22:52,440 --> 00:22:54,120
that as a function of how accurate

661
00:22:54,120 --> 00:22:56,340
people thought the headline was in the

662
00:22:56,340 --> 00:22:58,500
accuracy condition

663
00:22:58,500 --> 00:23:00,299
um and what you see is this very strong

664
00:23:00,299 --> 00:23:02,880
relationship where the more inaccurate

665
00:23:02,880 --> 00:23:04,140
the headline

666
00:23:04,140 --> 00:23:06,419
is the more getting people to think

667
00:23:06,419 --> 00:23:08,220
about accuracy it reduces sharing of

668
00:23:08,220 --> 00:23:09,960
that headline so if it's a headline that

669
00:23:09,960 --> 00:23:11,520
seems crazy getting people to think

670
00:23:11,520 --> 00:23:12,659
about accuracy is going to make them

671
00:23:12,659 --> 00:23:14,400
share it less if it's a headline that

672
00:23:14,400 --> 00:23:16,140
seems totally plausible then getting

673
00:23:16,140 --> 00:23:17,700
people to think about accuracy is not

674
00:23:17,700 --> 00:23:20,280
going to help and so this sort of gives

675
00:23:20,280 --> 00:23:21,960
you some insight into where this kind of

676
00:23:21,960 --> 00:23:24,720
approach could be effective if there are

677
00:23:24,720 --> 00:23:27,419
falsehoods that have been widely adopted

678
00:23:27,419 --> 00:23:30,360
already so for example uh claims about

679
00:23:30,360 --> 00:23:32,280
election fraud are now widely believed

680
00:23:32,280 --> 00:23:34,740
by a lot of conservatives in the U.S so

681
00:23:34,740 --> 00:23:36,360
for that you wouldn't expect this to

682
00:23:36,360 --> 00:23:38,640
help because people think it's accurate

683
00:23:38,640 --> 00:23:40,620
but for claims that are new or haven't

684
00:23:40,620 --> 00:23:42,960
yet gotten entrenched and so therefore

685
00:23:42,960 --> 00:23:45,840
people think are kind of if they stopped

686
00:23:45,840 --> 00:23:46,980
and thought about it would realize they

687
00:23:46,980 --> 00:23:50,340
were unbelievable this should help

688
00:23:50,340 --> 00:23:52,799
and then the final thing in the paper is

689
00:23:52,799 --> 00:23:54,179
uh the last quarter of people were

690
00:23:54,179 --> 00:23:55,980
randomized into a digital literacy tips

691
00:23:55,980 --> 00:23:57,900
condition so at the beginning of the

692
00:23:57,900 --> 00:23:59,700
study they saw these sort of minimal

693
00:23:59,700 --> 00:24:01,559
digital literacy tips and then they went

694
00:24:01,559 --> 00:24:03,179
on to indicate their sharing intentions

695
00:24:03,179 --> 00:24:05,039
for the 20 headlines

696
00:24:05,039 --> 00:24:06,720
um I don't think these tips really teach

697
00:24:06,720 --> 00:24:08,280
people much that they didn't already

698
00:24:08,280 --> 00:24:10,080
know or that's super useful here but

699
00:24:10,080 --> 00:24:11,460
it's just another way of getting them to

700
00:24:11,460 --> 00:24:13,200
think about accuracy and sort of priming

701
00:24:13,200 --> 00:24:15,419
the concept of accuracy and again we

702
00:24:15,419 --> 00:24:17,880
find that this works pretty consistently

703
00:24:17,880 --> 00:24:19,980
across countries about half as well as

704
00:24:19,980 --> 00:24:22,980
the accuracy prompt but still works

705
00:24:22,980 --> 00:24:24,900
incidentally if you ask people how

706
00:24:24,900 --> 00:24:25,919
effective they think the different

707
00:24:25,919 --> 00:24:29,280
treatments were afterwards people think

708
00:24:29,280 --> 00:24:31,440
that the tips condition was way more

709
00:24:31,440 --> 00:24:32,880
effective than the accuracy condition

710
00:24:32,880 --> 00:24:34,620
even though actually it's the other way

711
00:24:34,620 --> 00:24:37,200
around so just basic point is you

712
00:24:37,200 --> 00:24:38,700
shouldn't ask people how effective they

713
00:24:38,700 --> 00:24:40,440
think the treatments were on them you

714
00:24:40,440 --> 00:24:42,539
need to really you need to really do the

715
00:24:42,539 --> 00:24:44,820
experiments

716
00:24:44,820 --> 00:24:47,820
um okay so this is a bunch of evidence

717
00:24:47,820 --> 00:24:50,100
from these survey experiments in uh from

718
00:24:50,100 --> 00:24:51,840
around the world that getting people to

719
00:24:51,840 --> 00:24:53,820
think about accuracy can improve the

720
00:24:53,820 --> 00:24:55,620
quality of what they share

721
00:24:55,620 --> 00:24:57,840
but uh you know a limitation of

722
00:24:57,840 --> 00:24:59,159
everything that I've showed you so far

723
00:24:59,159 --> 00:25:01,380
is we don't we're not looking at actual

724
00:25:01,380 --> 00:25:03,240
sharing we're just using the sort of

725
00:25:03,240 --> 00:25:05,460
hypothetical sharing measure

726
00:25:05,460 --> 00:25:07,620
um and so we also wanted to see how this

727
00:25:07,620 --> 00:25:09,179
works in real life so we ran a field

728
00:25:09,179 --> 00:25:11,820
experiment on Twitter we created a set

729
00:25:11,820 --> 00:25:12,900
of

730
00:25:12,900 --> 00:25:15,659
um uh Twitter Bots that were explicitly

731
00:25:15,659 --> 00:25:17,400
identified themselves as Bots and were

732
00:25:17,400 --> 00:25:20,340
non-political we use these to follow

733
00:25:20,340 --> 00:25:23,460
over 136 000 users that had shared links

734
00:25:23,460 --> 00:25:25,799
to misinformation or fake news sites

735
00:25:25,799 --> 00:25:27,779
particularly focused on Breitbart and

736
00:25:27,779 --> 00:25:32,220
Infowars of those people about 11 000 of

737
00:25:32,220 --> 00:25:34,340
them followed our accounts back

738
00:25:34,340 --> 00:25:36,659
which means we could send them private

739
00:25:36,659 --> 00:25:38,580
messages we went through this list and

740
00:25:38,580 --> 00:25:40,740
we screened out about 6 000 accounts

741
00:25:40,740 --> 00:25:42,240
that seems like they were either Bots

742
00:25:42,240 --> 00:25:43,940
themselves

743
00:25:43,940 --> 00:25:47,460
or had not been sharing any news

744
00:25:47,460 --> 00:25:49,140
recently and so in the end of with about

745
00:25:49,140 --> 00:25:51,659
a little over 5000 users for the

746
00:25:51,659 --> 00:25:53,460
subjects in this field experiment again

747
00:25:53,460 --> 00:25:54,720
they didn't know they were in an

748
00:25:54,720 --> 00:25:56,159
experiment right so what we did is they

749
00:25:56,159 --> 00:25:58,020
were now following our account and we

750
00:25:58,020 --> 00:26:00,539
sent them a private message that said

751
00:26:00,539 --> 00:26:02,039
you know thanks for following me can I

752
00:26:02,039 --> 00:26:03,299
ask you a favor I'm wondering how

753
00:26:03,299 --> 00:26:04,799
accurate this headline is and I'm doing

754
00:26:04,799 --> 00:26:06,900
a survey to find out so it's just

755
00:26:06,900 --> 00:26:08,220
basically delivering exactly the

756
00:26:08,220 --> 00:26:09,720
accuracy prompt treatment that we did in

757
00:26:09,720 --> 00:26:12,600
the survey experiments almost nobody

758
00:26:12,600 --> 00:26:14,880
responded like fewer than 10 percent but

759
00:26:14,880 --> 00:26:15,960
that's fine we don't need them to

760
00:26:15,960 --> 00:26:17,760
respond all we need them to do is read

761
00:26:17,760 --> 00:26:19,260
that top sentence and they've been

762
00:26:19,260 --> 00:26:21,539
treated in the sense that the concept of

763
00:26:21,539 --> 00:26:23,279
accuracy has been activated in their

764
00:26:23,279 --> 00:26:25,260
minds and then we want to know is when

765
00:26:25,260 --> 00:26:26,760
they close out of this and go back to

766
00:26:26,760 --> 00:26:28,919
their feed are they more likely to think

767
00:26:28,919 --> 00:26:30,659
to themselves well how accurate is this

768
00:26:30,659 --> 00:26:32,159
next thing how accurate is this next

769
00:26:32,159 --> 00:26:34,020
thing and therefore do we see an

770
00:26:34,020 --> 00:26:35,460
increase in the quality of what they

771
00:26:35,460 --> 00:26:37,080
share afterwards

772
00:26:37,080 --> 00:26:39,480
and indeed that's what we find we find a

773
00:26:39,480 --> 00:26:41,100
significant increase in the quality of

774
00:26:41,100 --> 00:26:42,779
the new sources that they share links to

775
00:26:42,779 --> 00:26:45,720
as judged by fact Checkers that is one

776
00:26:45,720 --> 00:26:47,100
way to visualize this I'm going to show

777
00:26:47,100 --> 00:26:49,500
you this plot that has one dot per news

778
00:26:49,500 --> 00:26:53,760
Outlet the size of the dot indicates the

779
00:26:53,760 --> 00:26:55,559
pre-treatment sharing frequency so you

780
00:26:55,559 --> 00:26:56,760
can see the users on our experiment

781
00:26:56,760 --> 00:26:58,440
we're mostly sharing Fox News and

782
00:26:58,440 --> 00:27:01,620
Breitbart pre-treatment on the x-axis

783
00:27:01,620 --> 00:27:02,940
you have the quality or the

784
00:27:02,940 --> 00:27:04,799
trustworthiness of the new site as rated

785
00:27:04,799 --> 00:27:06,960
by professional fact Checkers and on the

786
00:27:06,960 --> 00:27:10,260
y-axis you have the change in the

787
00:27:10,260 --> 00:27:11,880
fraction of tweets that the users were

788
00:27:11,880 --> 00:27:15,059
making to each site after receiving the

789
00:27:15,059 --> 00:27:16,799
message and I should say in order to do

790
00:27:16,799 --> 00:27:18,419
good causal inference we do a stepped

791
00:27:18,419 --> 00:27:20,580
wedge design where people get everybody

792
00:27:20,580 --> 00:27:22,440
gets the message but you randomize who

793
00:27:22,440 --> 00:27:24,299
gets the message on which day so it's

794
00:27:24,299 --> 00:27:26,039
like each day is its own mini experiment

795
00:27:26,039 --> 00:27:27,419
where you can compare the people that

796
00:27:27,419 --> 00:27:29,340
got the message on that day to all the

797
00:27:29,340 --> 00:27:30,480
people that haven't gotten the message

798
00:27:30,480 --> 00:27:32,940
yet so we're doing good valid causal

799
00:27:32,940 --> 00:27:34,159
inference here

800
00:27:34,159 --> 00:27:36,720
and the key Point here is there's this

801
00:27:36,720 --> 00:27:40,559
strong positive relationship between a

802
00:27:40,559 --> 00:27:42,419
the trustworthiness of the news outlet

803
00:27:42,419 --> 00:27:45,059
and the change caused by getting our

804
00:27:45,059 --> 00:27:46,980
message in their sharing of it and maybe

805
00:27:46,980 --> 00:27:48,539
most notable is even though we recruited

806
00:27:48,539 --> 00:27:50,220
these people specifically because they

807
00:27:50,220 --> 00:27:52,200
often shared Breitbart you get a

808
00:27:52,200 --> 00:27:53,820
substantial decrease in the fraction of

809
00:27:53,820 --> 00:27:55,380
their tweets that are linking out to

810
00:27:55,380 --> 00:27:56,640
Breitbart

811
00:27:56,640 --> 00:27:58,740
but so the implication is that platforms

812
00:27:58,740 --> 00:28:00,059
could do things like here I'm going to

813
00:28:00,059 --> 00:28:03,120
show it in feed you could I mean as a

814
00:28:03,120 --> 00:28:04,320
Sierra I'm going to show it as a pop-up

815
00:28:04,320 --> 00:28:06,299
you could also do it in feed where while

816
00:28:06,299 --> 00:28:07,679
you're scrolling through your feed every

817
00:28:07,679 --> 00:28:09,720
once in a while it's like hey help us

818
00:28:09,720 --> 00:28:12,000
inform our algorithms you know here's

819
00:28:12,000 --> 00:28:13,740
some random headline do you think it's

820
00:28:13,740 --> 00:28:16,440
accurate or not and the idea is even if

821
00:28:16,440 --> 00:28:18,480
they threw away the responses just

822
00:28:18,480 --> 00:28:20,760
asking the question would increase the

823
00:28:20,760 --> 00:28:22,799
quality of the news people share by

824
00:28:22,799 --> 00:28:24,120
getting them to stop and think for

825
00:28:24,120 --> 00:28:26,400
themselves of like oh what else is

826
00:28:26,400 --> 00:28:28,020
accurate sort of refocusing their

827
00:28:28,020 --> 00:28:31,380
attention on the concept of accuracy

828
00:28:31,380 --> 00:28:33,840
um and so you know we've published lots

829
00:28:33,840 --> 00:28:35,340
of papers on at this point these were

830
00:28:35,340 --> 00:28:36,779
the first couple

831
00:28:36,779 --> 00:28:38,700
but because our really sort of mission

832
00:28:38,700 --> 00:28:40,260
driven in this we also are trying to

833
00:28:40,260 --> 00:28:42,299
work with tech companies a lot

834
00:28:42,299 --> 00:28:46,500
to get these ideas tested in practice so

835
00:28:46,500 --> 00:28:48,539
Tick Tock uh read about some of our

836
00:28:48,539 --> 00:28:50,340
stuff on coveted misinformation they

837
00:28:50,340 --> 00:28:52,740
thought it was cool they ran an RCT and

838
00:28:52,740 --> 00:28:54,179
they concluded that it was effective and

839
00:28:54,179 --> 00:28:55,320
they built it into their standard

840
00:28:55,320 --> 00:28:57,360
pipeline we've been working with people

841
00:28:57,360 --> 00:28:59,760
at Google's jigsaw group for a couple of

842
00:28:59,760 --> 00:29:01,620
years on building these ideas out into

843
00:29:01,620 --> 00:29:03,840
tools that platforms could use we've

844
00:29:03,840 --> 00:29:05,940
also been in discussions like on NDA

845
00:29:05,940 --> 00:29:07,320
discussions with other large tech

846
00:29:07,320 --> 00:29:09,480
companies about testing this kind of

847
00:29:09,480 --> 00:29:10,799
stuff

848
00:29:10,799 --> 00:29:12,960
um it's also something that you don't

849
00:29:12,960 --> 00:29:14,400
need the tech companies necessarily to

850
00:29:14,400 --> 00:29:16,559
do like during the 2020 election we

851
00:29:16,559 --> 00:29:18,179
partnered with a

852
00:29:18,179 --> 00:29:18,779
um

853
00:29:18,779 --> 00:29:21,299
a non-profit that paid an Advertiser to

854
00:29:21,299 --> 00:29:23,100
make a set of ads based on our research

855
00:29:23,100 --> 00:29:24,960
like this and then they paid to put

856
00:29:24,960 --> 00:29:27,659
those ads on disinformation sites and

857
00:29:27,659 --> 00:29:29,220
they found that the ads got way more

858
00:29:29,220 --> 00:29:30,840
engagement than usual political

859
00:29:30,840 --> 00:29:33,240
persuasive type ads and during the

860
00:29:33,240 --> 00:29:35,760
Georgia runoffs in 2022 they spent

861
00:29:35,760 --> 00:29:37,440
millions of dollars on this and got like

862
00:29:37,440 --> 00:29:39,480
64 million impressions in four weeks or

863
00:29:39,480 --> 00:29:41,399
something for these ads it wasn't an

864
00:29:41,399 --> 00:29:43,140
experiment so we can't assess what the

865
00:29:43,140 --> 00:29:45,480
impact was but we did survey experiments

866
00:29:45,480 --> 00:29:47,279
using the ads and found that it reduced

867
00:29:47,279 --> 00:29:49,679
the sharing of false claims so hopefully

868
00:29:49,679 --> 00:29:51,539
this helped you know reduce engagement

869
00:29:51,539 --> 00:29:54,059
with those sites

870
00:29:54,059 --> 00:29:56,940
um and so you know I think that this is

871
00:29:56,940 --> 00:29:59,399
a promising approach uh that's a sort of

872
00:29:59,399 --> 00:30:01,919
ux approach to think about

873
00:30:01,919 --> 00:30:04,679
um how to get people to pay attention to

874
00:30:04,679 --> 00:30:06,720
accuracy which is the thing that matters

875
00:30:06,720 --> 00:30:07,799
and it's a thing that people actually

876
00:30:07,799 --> 00:30:09,720
want to pay attention to right that's

877
00:30:09,720 --> 00:30:10,919
one of the things that's cool here is

878
00:30:10,919 --> 00:30:13,200
it's not like nudging people into doing

879
00:30:13,200 --> 00:30:14,940
something they didn't want to do if you

880
00:30:14,940 --> 00:30:16,500
ask people which we did in a bunch of

881
00:30:16,500 --> 00:30:18,360
these surveys the vast majority of

882
00:30:18,360 --> 00:30:20,700
people say they don't want to share

883
00:30:20,700 --> 00:30:23,520
false claims but that in that social

884
00:30:23,520 --> 00:30:25,919
media context prevents them from doing

885
00:30:25,919 --> 00:30:28,140
so just because they're distracted and

886
00:30:28,140 --> 00:30:29,580
we know that tech companies are good at

887
00:30:29,580 --> 00:30:30,840
getting people to pay attention to

888
00:30:30,840 --> 00:30:31,980
things that they want them to pay

889
00:30:31,980 --> 00:30:34,860
attention to namely ads that's the whole

890
00:30:34,860 --> 00:30:37,380
business model and so the idea is they

891
00:30:37,380 --> 00:30:38,700
can use some of the muscle that they've

892
00:30:38,700 --> 00:30:39,720
developed for getting people to pay

893
00:30:39,720 --> 00:30:42,779
attention to ads to instead get them to

894
00:30:42,779 --> 00:30:45,419
pay attention to accuracy

895
00:30:45,419 --> 00:30:47,640
okay so that's the that's the accuracy

896
00:30:47,640 --> 00:30:49,140
prompt idea

897
00:30:49,140 --> 00:30:50,460
um and the last thing that I want to

898
00:30:50,460 --> 00:30:54,659
talk about before we go to the Q a

899
00:30:54,659 --> 00:30:57,419
um is this question of okay so I said

900
00:30:57,419 --> 00:30:59,880
that uh you know if if the platform does

901
00:30:59,880 --> 00:31:00,840
this thing where it popped up and

902
00:31:00,840 --> 00:31:02,399
surveyed people and said how accurate do

903
00:31:02,399 --> 00:31:04,080
you think this headline is they could

904
00:31:04,080 --> 00:31:05,700
just throw away the answers and it would

905
00:31:05,700 --> 00:31:07,679
be useful to ask the question because it

906
00:31:07,679 --> 00:31:10,200
puts people in an accuracy mindset but

907
00:31:10,200 --> 00:31:11,880
then we also wanted to know should they

908
00:31:11,880 --> 00:31:13,919
actually throw away the answers or might

909
00:31:13,919 --> 00:31:16,320
the answers be useful

910
00:31:16,320 --> 00:31:17,700
um because you know the way that tech

911
00:31:17,700 --> 00:31:19,919
companies are mostly dealing with

912
00:31:19,919 --> 00:31:23,520
misinformation now is you know

913
00:31:23,520 --> 00:31:25,799
training misinformation classifiers so

914
00:31:25,799 --> 00:31:27,419
just using machine learning and then

915
00:31:27,419 --> 00:31:28,919
partnering with professional fact

916
00:31:28,919 --> 00:31:31,260
Checkers like PolitiFact and Snopes and

917
00:31:31,260 --> 00:31:33,299
organizations like that and if you know

918
00:31:33,299 --> 00:31:35,399
a fact checking organization or maybe

919
00:31:35,399 --> 00:31:37,500
two fact-checking organizations say that

920
00:31:37,500 --> 00:31:39,600
something's false then they'll massively

921
00:31:39,600 --> 00:31:41,220
downrank it and they'll put a warning on

922
00:31:41,220 --> 00:31:42,059
it

923
00:31:42,059 --> 00:31:43,740
and I think the professional fact

924
00:31:43,740 --> 00:31:45,360
checking is great there's a lot of

925
00:31:45,360 --> 00:31:47,039
evidence that suggests that if you see a

926
00:31:47,039 --> 00:31:48,659
warning on it that says a fact Checker

927
00:31:48,659 --> 00:31:50,399
said it's false you're less likely to

928
00:31:50,399 --> 00:31:52,200
believe it and share it even people who

929
00:31:52,200 --> 00:31:53,640
say they don't trust fact Checkers and

930
00:31:53,640 --> 00:31:55,440
don't want to see the fact checks if you

931
00:31:55,440 --> 00:31:57,360
show them the fact checks anyways it

932
00:31:57,360 --> 00:31:59,700
reduces belief and it reduces sharing so

933
00:31:59,700 --> 00:32:01,559
I think fact checking and warning labels

934
00:32:01,559 --> 00:32:05,039
are great the problem is one of scale

935
00:32:05,039 --> 00:32:07,080
which is that there's just a massive

936
00:32:07,080 --> 00:32:08,520
amount of content that's posted online

937
00:32:08,520 --> 00:32:10,980
every day and there's just no way that

938
00:32:10,980 --> 00:32:12,539
professional fact checkers can keep up

939
00:32:12,539 --> 00:32:15,000
so the question is how can you identify

940
00:32:15,000 --> 00:32:18,059
misinformation in a scalable way and

941
00:32:18,059 --> 00:32:20,340
what we did in the final sort of uh

942
00:32:20,340 --> 00:32:22,500
relevant project here is we asked how to

943
00:32:22,500 --> 00:32:24,179
what extent could the wisdom of crowds

944
00:32:24,179 --> 00:32:27,360
be used to help identify uh

945
00:32:27,360 --> 00:32:30,419
misinformation at scale obviously there

946
00:32:30,419 --> 00:32:33,000
are you know billions of social media

947
00:32:33,000 --> 00:32:34,279
users

948
00:32:34,279 --> 00:32:36,720
compared to the handful of fact Checkers

949
00:32:36,720 --> 00:32:40,140
and so if uh use if like crowd

950
00:32:40,140 --> 00:32:42,779
identification could work that is the

951
00:32:42,779 --> 00:32:44,580
crowd could actually identify inaccurate

952
00:32:44,580 --> 00:32:47,100
claims that could really help with doing

953
00:32:47,100 --> 00:32:48,779
things that are scalable in a scalable

954
00:32:48,779 --> 00:32:50,820
way and you know there's reason to

955
00:32:50,820 --> 00:32:52,140
believe that the crowd wouldn't do a

956
00:32:52,140 --> 00:32:54,360
good job because why would you trust

957
00:32:54,360 --> 00:32:56,399
random people to be able to tell what's

958
00:32:56,399 --> 00:32:58,380
true or not but that's the beauty of the

959
00:32:58,380 --> 00:32:59,820
wisdom of crowds there's like 100 years

960
00:32:59,820 --> 00:33:01,020
of evidence showing that if you

961
00:33:01,020 --> 00:33:02,880
aggregate the judgments of lots of crowd

962
00:33:02,880 --> 00:33:05,520
members you can do as well are better

963
00:33:05,520 --> 00:33:07,020
than experts we wanted to know if this

964
00:33:07,020 --> 00:33:08,220
was true in the context of

965
00:33:08,220 --> 00:33:10,380
misinformation identification so we did

966
00:33:10,380 --> 00:33:11,760
this as a collaboration with Facebook

967
00:33:11,760 --> 00:33:13,919
who is developing a sort of

968
00:33:13,919 --> 00:33:16,019
crowdsourcing product and they asked us

969
00:33:16,019 --> 00:33:18,000
to you know help advise them on what to

970
00:33:18,000 --> 00:33:20,039
do and we were like great we really want

971
00:33:20,039 --> 00:33:22,080
to see how well this actually works so

972
00:33:22,080 --> 00:33:24,600
they gave us a set of

973
00:33:24,600 --> 00:33:26,760
um URLs that their internal algorithms

974
00:33:26,760 --> 00:33:28,320
had flagged as things that were in need

975
00:33:28,320 --> 00:33:30,120
of fact checks either because they had

976
00:33:30,120 --> 00:33:31,380
some reason to believe that they were

977
00:33:31,380 --> 00:33:33,179
inaccurate or just because they were

978
00:33:33,179 --> 00:33:35,220
going viral or about important topics

979
00:33:35,220 --> 00:33:37,320
that we hired three professional fact

980
00:33:37,320 --> 00:33:39,179
Checkers to do detailed research on each

981
00:33:39,179 --> 00:33:41,880
article and rate its accuracy and we

982
00:33:41,880 --> 00:33:45,720
also hired 1100 unskilled Americans from

983
00:33:45,720 --> 00:33:46,980
the online labor market Amazon

984
00:33:46,980 --> 00:33:49,440
Mechanical Turk to just read the

985
00:33:49,440 --> 00:33:51,059
headline and Lead

986
00:33:51,059 --> 00:33:54,000
uh and say how accurate they thought uh

987
00:33:54,000 --> 00:33:55,380
the article was and so we want to know

988
00:33:55,380 --> 00:33:57,240
how well does the fact Checker research

989
00:33:57,240 --> 00:33:59,820
and the crowd ratings agree with each

990
00:33:59,820 --> 00:34:01,380
other and so what I'm going to show you

991
00:34:01,380 --> 00:34:03,360
here is the correlation between the

992
00:34:03,360 --> 00:34:04,860
layperson headline ratings and the fact

993
00:34:04,860 --> 00:34:06,899
Checker research ratings they rated them

994
00:34:06,899 --> 00:34:08,820
on these one to seven likert scales of

995
00:34:08,820 --> 00:34:11,219
how accurate and trustworthy and so on

996
00:34:11,219 --> 00:34:13,679
each article was and we're going to show

997
00:34:13,679 --> 00:34:15,418
you this agreement between the layperson

998
00:34:15,418 --> 00:34:17,520
and the fact Checkers as a function of

999
00:34:17,520 --> 00:34:19,199
the size of the crowd so how many

1000
00:34:19,199 --> 00:34:21,899
layperson ratings we use per headline I

1001
00:34:21,899 --> 00:34:23,520
mean we also varied whether or not we

1002
00:34:23,520 --> 00:34:26,580
told them the URL that it came from or

1003
00:34:26,580 --> 00:34:28,679
not to see if that helped or hurt

1004
00:34:28,679 --> 00:34:30,719
and as our Baseline we take the

1005
00:34:30,719 --> 00:34:32,399
correlation between the fact Checkers

1006
00:34:32,399 --> 00:34:33,780
themselves because although there is a

1007
00:34:33,780 --> 00:34:36,060
lot of agreement the fact Checkers were

1008
00:34:36,060 --> 00:34:37,980
far from unim unanimous so among the

1009
00:34:37,980 --> 00:34:40,918
three factors that we recruited the

1010
00:34:40,918 --> 00:34:43,199
average correlation was 0.6 you know

1011
00:34:43,199 --> 00:34:44,760
which is much higher than you see in any

1012
00:34:44,760 --> 00:34:46,619
kind of social science research usually

1013
00:34:46,619 --> 00:34:48,480
but it's far from one

1014
00:34:48,480 --> 00:34:50,219
and we wouldn't expect the crowd we will

1015
00:34:50,219 --> 00:34:52,739
do too much better than that and so what

1016
00:34:52,739 --> 00:34:55,500
we find is with as few as 15 or 20

1017
00:34:55,500 --> 00:34:58,080
layperson ratings just reading the

1018
00:34:58,080 --> 00:35:00,599
headline and Lead that kind of gut

1019
00:35:00,599 --> 00:35:02,640
rating correlates as well with the fact

1020
00:35:02,640 --> 00:35:04,440
Checkers as the fact Checkers correlate

1021
00:35:04,440 --> 00:35:05,700
with each other

1022
00:35:05,700 --> 00:35:08,339
and so this suggests that uh

1023
00:35:08,339 --> 00:35:10,260
crowdsourcing could actually be a useful

1024
00:35:10,260 --> 00:35:13,020
way to identify misinformation

1025
00:35:13,020 --> 00:35:14,700
um at scale

1026
00:35:14,700 --> 00:35:19,079
and we wanted to know uh so this is

1027
00:35:19,079 --> 00:35:20,579
evidence from the U.S and that's great

1028
00:35:20,579 --> 00:35:22,859
but then we also wanted to know is this

1029
00:35:22,859 --> 00:35:25,079
you know specific to the US or does it

1030
00:35:25,079 --> 00:35:27,119
generalize more broadly and so we can

1031
00:35:27,119 --> 00:35:28,800
use the data from the accuracy condition

1032
00:35:28,800 --> 00:35:30,660
of our big cross-cultural study that I

1033
00:35:30,660 --> 00:35:31,859
told you about at the beginning of the

1034
00:35:31,859 --> 00:35:34,260
talk to look at this question what we do

1035
00:35:34,260 --> 00:35:37,680
is for each of the 45 headlines we You

1036
00:35:37,680 --> 00:35:39,960
Know sample some random number of

1037
00:35:39,960 --> 00:35:42,240
layperson ratings we calculate the

1038
00:35:42,240 --> 00:35:45,119
accuracy rating for each headline and

1039
00:35:45,119 --> 00:35:47,400
then we see how well we can classify the

1040
00:35:47,400 --> 00:35:49,440
true versus false headlines based on

1041
00:35:49,440 --> 00:35:50,700
those ratings so here I'm going to show

1042
00:35:50,700 --> 00:35:53,280
you the AUC for predicting whether the

1043
00:35:53,280 --> 00:35:54,660
headline is true or not based on the

1044
00:35:54,660 --> 00:35:57,540
crowd ratings as a function of the size

1045
00:35:57,540 --> 00:36:01,440
of the crowd and the country

1046
00:36:01,440 --> 00:36:05,280
and what you can see is that uh

1047
00:36:05,280 --> 00:36:07,500
basically everywhere or almost

1048
00:36:07,500 --> 00:36:09,780
everywhere with 20 lay people parading

1049
00:36:09,780 --> 00:36:12,000
you get per headline you're getting an

1050
00:36:12,000 --> 00:36:14,640
AUC of over 0.9 and even for the couple

1051
00:36:14,640 --> 00:36:16,740
of places where it's a bit worse it's

1052
00:36:16,740 --> 00:36:19,380
still doing uh AUC greater than 0.85

1053
00:36:19,380 --> 00:36:20,760
which I think is better than most of the

1054
00:36:20,760 --> 00:36:23,760
models that are out there so I think

1055
00:36:23,760 --> 00:36:25,500
that this suggests that the power of

1056
00:36:25,500 --> 00:36:28,400
crowds is also quite general

1057
00:36:28,400 --> 00:36:30,300
and this is also something we've been

1058
00:36:30,300 --> 00:36:31,740
working with tech companies on like I

1059
00:36:31,740 --> 00:36:34,320
said we advise Facebook in their

1060
00:36:34,320 --> 00:36:35,820
development of this community review

1061
00:36:35,820 --> 00:36:37,140
product that they're using for

1062
00:36:37,140 --> 00:36:40,619
identifying misinformation and also

1063
00:36:40,619 --> 00:36:42,180
Twitter has started doing this using

1064
00:36:42,180 --> 00:36:45,180
something called birdwatch where

1065
00:36:45,180 --> 00:36:46,800
um they just sort of people that sign up

1066
00:36:46,800 --> 00:36:49,040
for the bird watch uh

1067
00:36:49,040 --> 00:36:51,780
whatever to be bird watchers can flag

1068
00:36:51,780 --> 00:36:53,700
things that they see and feed as

1069
00:36:53,700 --> 00:36:55,079
potentially misleading and then write

1070
00:36:55,079 --> 00:36:56,280
notes about why they think it's

1071
00:36:56,280 --> 00:36:57,839
problematic

1072
00:36:57,839 --> 00:36:58,619
um

1073
00:36:58,619 --> 00:37:01,320
and so it's exciting to see this

1074
00:37:01,320 --> 00:37:03,420
starting to get uh taken up and we're

1075
00:37:03,420 --> 00:37:05,880
we're analyzing this stuff too uh the

1076
00:37:05,880 --> 00:37:09,240
bird watch we had a paper in Kai uh this

1077
00:37:09,240 --> 00:37:12,119
year where we looked at who was doing

1078
00:37:12,119 --> 00:37:15,000
who was flagging what on birdwatch

1079
00:37:15,000 --> 00:37:16,320
um because you know an interesting

1080
00:37:16,320 --> 00:37:18,420
feature of uh that kind of crowd

1081
00:37:18,420 --> 00:37:21,240
approach is people get to choose what to

1082
00:37:21,240 --> 00:37:22,440
rate we're not you know they're not

1083
00:37:22,440 --> 00:37:23,820
getting shown things and saying well

1084
00:37:23,820 --> 00:37:24,960
what do you think about this what do you

1085
00:37:24,960 --> 00:37:26,460
think about this but they're going out

1086
00:37:26,460 --> 00:37:29,220
and picking things to evaluate and flag

1087
00:37:29,220 --> 00:37:31,800
and what I'm showing you here is each

1088
00:37:31,800 --> 00:37:33,720
Red Dot is an article that a bird

1089
00:37:33,720 --> 00:37:35,880
watcher flagged as misleading and each

1090
00:37:35,880 --> 00:37:37,680
Blue Dot is one that they flagged is not

1091
00:37:37,680 --> 00:37:39,660
misleading you can see about 90 over 90

1092
00:37:39,660 --> 00:37:41,280
percent of the flags are people flagging

1093
00:37:41,280 --> 00:37:42,740
things as misleading

1094
00:37:42,740 --> 00:37:45,599
and then this is the partisanship the

1095
00:37:45,599 --> 00:37:47,339
estimated Partnership of the note writer

1096
00:37:47,339 --> 00:37:49,140
and the estimated partisanship of the

1097
00:37:49,140 --> 00:37:50,940
Tweeter and what you see is essentially

1098
00:37:50,940 --> 00:37:52,800
all of the action is on the off

1099
00:37:52,800 --> 00:37:55,980
diagonals so it's like liberals flag

1100
00:37:55,980 --> 00:37:58,020
tweets by conservatives as misleading

1101
00:37:58,020 --> 00:38:00,240
and conservative of flag tweets by

1102
00:38:00,240 --> 00:38:02,760
liberals as misleading which we saw we

1103
00:38:02,760 --> 00:38:04,520
were like oh geez that doesn't look good

1104
00:38:04,520 --> 00:38:07,079
but then we actually hired fact Checkers

1105
00:38:07,079 --> 00:38:09,000
to fact check the tweets and it turns

1106
00:38:09,000 --> 00:38:10,200
out that people are doing a pretty good

1107
00:38:10,200 --> 00:38:11,940
job like there was over 80 percent of

1108
00:38:11,940 --> 00:38:13,800
the tweets that the birdwatchers said

1109
00:38:13,800 --> 00:38:15,480
were false at least one of the or

1110
00:38:15,480 --> 00:38:16,859
misleading at least one of the bird

1111
00:38:16,859 --> 00:38:18,720
watchers one of the fact Checkers also

1112
00:38:18,720 --> 00:38:20,400
thought was misleading and so what's

1113
00:38:20,400 --> 00:38:22,260
going on here is it's sort of like the

1114
00:38:22,260 --> 00:38:24,180
two sides are policing each other people

1115
00:38:24,180 --> 00:38:26,220
are flagging counterpartisan tweets but

1116
00:38:26,220 --> 00:38:27,300
they're not just flagging all

1117
00:38:27,300 --> 00:38:28,800
counterpartisan tweets they're

1118
00:38:28,800 --> 00:38:30,900
specifically a flagging tweets that are

1119
00:38:30,900 --> 00:38:33,480
counterpartisan and false so we think

1120
00:38:33,480 --> 00:38:35,520
that the system actually looks like uh

1121
00:38:35,520 --> 00:38:37,680
it's it's working quite well

1122
00:38:37,680 --> 00:38:40,380
so just to summarize

1123
00:38:40,380 --> 00:38:41,180
um

1124
00:38:41,180 --> 00:38:43,980
the like our big cross-cultural study

1125
00:38:43,980 --> 00:38:46,079
found I think striking cross-cultural

1126
00:38:46,079 --> 00:38:48,540
irregularities in both the psychology of

1127
00:38:48,540 --> 00:38:50,040
misinformation in terms of who is

1128
00:38:50,040 --> 00:38:51,900
susceptible to it and also the

1129
00:38:51,900 --> 00:38:54,000
effectiveness of interventions and we

1130
00:38:54,000 --> 00:38:55,920
found evidence that cognitive factors

1131
00:38:55,920 --> 00:38:57,900
social factors and ideological factors

1132
00:38:57,900 --> 00:39:00,420
are all important predictors of who

1133
00:39:00,420 --> 00:39:03,300
believes false claims about covid we

1134
00:39:03,300 --> 00:39:05,280
found that in general sharing is less

1135
00:39:05,280 --> 00:39:07,380
Discerning than accuracy judgments so

1136
00:39:07,380 --> 00:39:08,820
there's this fundamental problem of

1137
00:39:08,820 --> 00:39:10,920
trying to get people to be more careful

1138
00:39:10,920 --> 00:39:12,240
on what they share

1139
00:39:12,240 --> 00:39:13,800
and we found that interventions

1140
00:39:13,800 --> 00:39:15,480
developed in the U.S tend to generalize

1141
00:39:15,480 --> 00:39:17,280
pretty well getting people to pay

1142
00:39:17,280 --> 00:39:18,780
attention to accuracy and providing

1143
00:39:18,780 --> 00:39:20,880
digital literacy tips increase the

1144
00:39:20,880 --> 00:39:23,220
quality of information people intended

1145
00:39:23,220 --> 00:39:25,500
to share and crowdsourcing was able to

1146
00:39:25,500 --> 00:39:28,140
identify low quality information and

1147
00:39:28,140 --> 00:39:29,640
both of these things or all these things

1148
00:39:29,640 --> 00:39:32,099
offer scalable approaches that don't

1149
00:39:32,099 --> 00:39:34,440
rely on a centralized Authority deciding

1150
00:39:34,440 --> 00:39:38,480
what to censor and what not to censor so

1151
00:39:38,480 --> 00:39:42,599
that is a sort of a high level summary

1152
00:39:42,599 --> 00:39:43,980
of what we've been doing for the last

1153
00:39:43,980 --> 00:39:46,260
five or six years and the one thing that

1154
00:39:46,260 --> 00:39:47,940
I want to flag as I think an important

1155
00:39:47,940 --> 00:39:49,380
future direction for us and for

1156
00:39:49,380 --> 00:39:50,280
everybody that's thinking about

1157
00:39:50,280 --> 00:39:54,180
misinformation is in 2016 uh you know

1158
00:39:54,180 --> 00:39:55,859
when everyone started talking about fake

1159
00:39:55,859 --> 00:39:59,040
news it was these kind of like viral uh

1160
00:39:59,040 --> 00:40:01,260
posts by like random Outlets you've

1161
00:40:01,260 --> 00:40:03,180
never heard of that then sort of took

1162
00:40:03,180 --> 00:40:05,520
off and really got Amplified whereas I

1163
00:40:05,520 --> 00:40:07,020
think a lot of the misinformation now

1164
00:40:07,020 --> 00:40:08,940
both around covet and election fraud in

1165
00:40:08,940 --> 00:40:10,440
the US and many other kinds of things

1166
00:40:10,440 --> 00:40:13,380
around the world aren't coming from you

1167
00:40:13,380 --> 00:40:14,820
know viral things they're coming from

1168
00:40:14,820 --> 00:40:17,099
political Elites in sort of top-down

1169
00:40:17,099 --> 00:40:19,619
coordinated misinformation campaigns and

1170
00:40:19,619 --> 00:40:21,240
so a big question for us is how

1171
00:40:21,240 --> 00:40:23,099
different is the psychology and the

1172
00:40:23,099 --> 00:40:24,900
effectiveness of interventions for

1173
00:40:24,900 --> 00:40:26,160
misinformation when it's coming from

1174
00:40:26,160 --> 00:40:28,740
Elites versus bubbling up in this viral

1175
00:40:28,740 --> 00:40:31,800
fashion so thanks so much please drop me

1176
00:40:31,800 --> 00:40:33,300
a line if you're interested in any of

1177
00:40:33,300 --> 00:40:37,520
this and uh love to do some q a

