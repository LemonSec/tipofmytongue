1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,940 --> 00:00:11,160
so hi I'm Emily and I'm going to be

3
00:00:11,160 --> 00:00:12,960
talking about a project that I worked on

4
00:00:12,960 --> 00:00:17,179
looking at norms for sensor data

5
00:00:17,220 --> 00:00:20,760
privacy is inherently social and this is

6
00:00:20,760 --> 00:00:22,020
pretty obvious when we talk about

7
00:00:22,020 --> 00:00:24,660
interpersonal privacy where disclosure

8
00:00:24,660 --> 00:00:27,779
decisions are at least partly based on

9
00:00:27,779 --> 00:00:29,699
things like situational norms and

10
00:00:29,699 --> 00:00:32,040
context and characteristics of our

11
00:00:32,040 --> 00:00:34,440
relationships with other people but what

12
00:00:34,440 --> 00:00:37,500
about data privacy because of big data

13
00:00:37,500 --> 00:00:39,600
and machine learning a person's privacy

14
00:00:39,600 --> 00:00:42,180
can be violated when inferences are made

15
00:00:42,180 --> 00:00:44,540
about them Based on data that's been

16
00:00:44,540 --> 00:00:49,140
collected or contributed by other people

17
00:00:49,140 --> 00:00:51,360
so this makes data privacy a collective

18
00:00:51,360 --> 00:00:54,420
problem because choices to opt into data

19
00:00:54,420 --> 00:00:56,399
collection can have privacy implications

20
00:00:56,399 --> 00:00:59,219
Beyond just the person who's making that

21
00:00:59,219 --> 00:01:02,059
disclosure decision

22
00:01:02,520 --> 00:01:04,680
but notice in Choice frames data privacy

23
00:01:04,680 --> 00:01:07,500
as an individual problem as a choice

24
00:01:07,500 --> 00:01:09,360
that each individual makes for

25
00:01:09,360 --> 00:01:11,460
themselves about consent to data

26
00:01:11,460 --> 00:01:13,759
collection

27
00:01:14,159 --> 00:01:16,080
so the inspiration for this project was

28
00:01:16,080 --> 00:01:17,820
wondering about if there are any social

29
00:01:17,820 --> 00:01:20,460
influences on people's data privacy

30
00:01:20,460 --> 00:01:21,960
decisions like there are for

31
00:01:21,960 --> 00:01:24,240
interpersonal disclosures

32
00:01:24,240 --> 00:01:26,820
social norms are a form of social

33
00:01:26,820 --> 00:01:28,740
influence that involves coordination

34
00:01:28,740 --> 00:01:31,080
across a group of people about what

35
00:01:31,080 --> 00:01:34,020
kinds of behaviors are okay and not okay

36
00:01:34,020 --> 00:01:36,960
Norms are essentially constraints on

37
00:01:36,960 --> 00:01:39,299
Behavior based on an individual's

38
00:01:39,299 --> 00:01:41,579
beliefs about the beliefs and

39
00:01:41,579 --> 00:01:44,899
expectations of other people

40
00:01:45,119 --> 00:01:47,520
so the goal of This research was to find

41
00:01:47,520 --> 00:01:49,680
out whether social norms exist related

42
00:01:49,680 --> 00:01:52,799
to data privacy and if there are then

43
00:01:52,799 --> 00:01:55,200
these Norms could become the foundation

44
00:01:55,200 --> 00:01:57,899
of a framework for Collective privacy

45
00:01:57,899 --> 00:01:59,520
management rather than individual

46
00:01:59,520 --> 00:02:01,979
privacy management that could provide

47
00:02:01,979 --> 00:02:03,840
people with support for coordinating

48
00:02:03,840 --> 00:02:06,000
among themselves about what kinds of

49
00:02:06,000 --> 00:02:07,740
data collection and use are acceptable

50
00:02:07,740 --> 00:02:10,639
and unacceptable

51
00:02:11,099 --> 00:02:13,020
so this project was an interview study

52
00:02:13,020 --> 00:02:15,360
that was conducted in two rounds of data

53
00:02:15,360 --> 00:02:17,459
collection the first round of interviews

54
00:02:17,459 --> 00:02:19,980
had 30 activity tracker users as

55
00:02:19,980 --> 00:02:22,440
participants and the second round had 35

56
00:02:22,440 --> 00:02:25,560
users of voice assistance like Siri the

57
00:02:25,560 --> 00:02:27,900
Google assistant or Amazon Alexa

58
00:02:27,900 --> 00:02:30,239
and participants were recruited from a

59
00:02:30,239 --> 00:02:32,280
community subject pool in the the

60
00:02:32,280 --> 00:02:33,780
community surrounding Michigan State

61
00:02:33,780 --> 00:02:35,580
University and then also snowball

62
00:02:35,580 --> 00:02:38,220
sampling sampling on social media

63
00:02:38,220 --> 00:02:39,959
the interviews were conducted by phone

64
00:02:39,959 --> 00:02:42,360
but it was pre-pandemic and they

65
00:02:42,360 --> 00:02:43,860
typically lasted a little bit less than

66
00:02:43,860 --> 00:02:46,019
an hour

67
00:02:46,019 --> 00:02:48,120
the interviews involve presenting six

68
00:02:48,120 --> 00:02:50,700
hypothetical scenarios to participants

69
00:02:50,700 --> 00:02:53,220
and then asking them about their own

70
00:02:53,220 --> 00:02:55,680
reactions to those scenarios and then

71
00:02:55,680 --> 00:02:58,260
how they thought other people would

72
00:02:58,260 --> 00:03:00,060
react to the scenarios

73
00:03:00,060 --> 00:03:02,519
and the scenarios presented inferences

74
00:03:02,519 --> 00:03:05,340
that seemed plausible because they were

75
00:03:05,340 --> 00:03:07,860
things that could be derived from data

76
00:03:07,860 --> 00:03:10,019
that's already collected or generated by

77
00:03:10,019 --> 00:03:12,300
these devices but the inferences were

78
00:03:12,300 --> 00:03:15,060
also unexpected which meant they weren't

79
00:03:15,060 --> 00:03:17,760
related to the typical use cases for an

80
00:03:17,760 --> 00:03:21,540
activity tracker or a voice assistant

81
00:03:21,540 --> 00:03:23,159
so here are a couple of examples on the

82
00:03:23,159 --> 00:03:24,319
slide and I'm going to read them to you

83
00:03:24,319 --> 00:03:26,819
the activity tracker scenario up there

84
00:03:26,819 --> 00:03:29,940
says imagine that instead of time spent

85
00:03:29,940 --> 00:03:32,280
sitting down in a location a wearable

86
00:03:32,280 --> 00:03:33,840
sensor device could use information

87
00:03:33,840 --> 00:03:36,959
about a user's movements and location to

88
00:03:36,959 --> 00:03:38,879
count how many times he went to the

89
00:03:38,879 --> 00:03:40,680
bathroom yesterday

90
00:03:40,680 --> 00:03:42,480
and then the voice assistant example up

91
00:03:42,480 --> 00:03:45,120
there says imagine that it's possible to

92
00:03:45,120 --> 00:03:46,860
use your voice assistant while preparing

93
00:03:46,860 --> 00:03:49,140
meals to read recipes and provide

94
00:03:49,140 --> 00:03:51,540
cooking instructions this means that it

95
00:03:51,540 --> 00:03:53,459
would have access to information about

96
00:03:53,459 --> 00:03:55,980
ingredients cooking methods and meals

97
00:03:55,980 --> 00:03:58,140
the user prepares and could determine

98
00:03:58,140 --> 00:04:00,780
how healthy a person is based on his or

99
00:04:00,780 --> 00:04:02,700
her eating habits

100
00:04:02,700 --> 00:04:04,620
so these examples

101
00:04:04,620 --> 00:04:05,819
um were two that were used in the

102
00:04:05,819 --> 00:04:07,799
interviews and they both make plausible

103
00:04:07,799 --> 00:04:11,099
but unexpected inferences about some

104
00:04:11,099 --> 00:04:13,739
aspect of a person's body or bodily

105
00:04:13,739 --> 00:04:15,659
functions that could have health

106
00:04:15,659 --> 00:04:18,260
implications

107
00:04:18,298 --> 00:04:21,238
so this slide shows what the inferences

108
00:04:21,238 --> 00:04:22,740
were in each of the scenarios that were

109
00:04:22,740 --> 00:04:24,060
used in the interviews because the

110
00:04:24,060 --> 00:04:26,460
Technologies are a little different the

111
00:04:26,460 --> 00:04:27,840
scenarios were naturally a little bit

112
00:04:27,840 --> 00:04:28,639
different

113
00:04:28,639 --> 00:04:30,960
so they range from things like

114
00:04:30,960 --> 00:04:33,600
inferences about oversleeping that are

115
00:04:33,600 --> 00:04:35,220
fairly close to the expected

116
00:04:35,220 --> 00:04:36,960
functionality if you use your activity

117
00:04:36,960 --> 00:04:38,520
tracker or your voice assistant as your

118
00:04:38,520 --> 00:04:40,860
wake-up alarm to things like at the

119
00:04:40,860 --> 00:04:43,320
bottom inferences about the user's home

120
00:04:43,320 --> 00:04:45,000
and about the safety of the neighborhood

121
00:04:45,000 --> 00:04:47,940
or comparison with carbon footprint

122
00:04:47,940 --> 00:04:48,979
based on something like your

123
00:04:48,979 --> 00:04:50,820
accelerometer data in the activity

124
00:04:50,820 --> 00:04:53,240
tracker

125
00:04:53,340 --> 00:04:55,979
the analysis then focused on types of

126
00:04:55,979 --> 00:04:57,600
beliefs that participants talked about

127
00:04:57,600 --> 00:04:59,280
as they were reacting to the scenarios

128
00:04:59,280 --> 00:05:01,560
and since the focus of the project was

129
00:05:01,560 --> 00:05:04,380
on Norms for data privacy I focused a

130
00:05:04,380 --> 00:05:06,540
lot on differentiating social beliefs

131
00:05:06,540 --> 00:05:09,120
from non-social beliefs social beliefs

132
00:05:09,120 --> 00:05:11,759
were things about what other people do

133
00:05:11,759 --> 00:05:14,160
or about what other people believe and

134
00:05:14,160 --> 00:05:16,500
then I also sort of focused on whether

135
00:05:16,500 --> 00:05:18,780
the beliefs were related to privacy or

136
00:05:18,780 --> 00:05:19,860
if they didn't really have any

137
00:05:19,860 --> 00:05:22,800
connection to privacy

138
00:05:22,800 --> 00:05:24,300
so I'm going to just jump into the

139
00:05:24,300 --> 00:05:27,360
findings so participants beliefs did

140
00:05:27,360 --> 00:05:30,120
involve some Norms about sensor data

141
00:05:30,120 --> 00:05:32,520
collection and use and these were cases

142
00:05:32,520 --> 00:05:34,620
where participants had very strong

143
00:05:34,620 --> 00:05:37,380
negative reactions to the scenario and

144
00:05:37,380 --> 00:05:39,300
also believed that other people would

145
00:05:39,300 --> 00:05:41,880
react in the same way these Norms were

146
00:05:41,880 --> 00:05:43,919
about bodily functions like bathroom

147
00:05:43,919 --> 00:05:46,620
behavior and sleeping they were about

148
00:05:46,620 --> 00:05:48,419
any kind of data collection that took

149
00:05:48,419 --> 00:05:50,940
place inside the home and they were also

150
00:05:50,940 --> 00:05:53,160
about inferring information about

151
00:05:53,160 --> 00:05:54,360
children

152
00:05:54,360 --> 00:05:56,220
so on the slide here's a quote from one

153
00:05:56,220 --> 00:05:58,199
of the participants I would think people

154
00:05:58,199 --> 00:06:01,199
would just be maybe upset or angry that

155
00:06:01,199 --> 00:06:02,940
there would be information being kept on

156
00:06:02,940 --> 00:06:04,440
how many times you're going to the

157
00:06:04,440 --> 00:06:05,820
bathroom

158
00:06:05,820 --> 00:06:08,220
these Norms are what I sort of think

159
00:06:08,220 --> 00:06:10,919
about as being privacy friendly or

160
00:06:10,919 --> 00:06:13,560
privacy supporting because they involve

161
00:06:13,560 --> 00:06:16,139
not disclosing or sharing certain kinds

162
00:06:16,139 --> 00:06:18,500
of information

163
00:06:19,020 --> 00:06:20,940
but there were also Norms in the data

164
00:06:20,940 --> 00:06:23,580
that were not so privacy friendly over

165
00:06:23,580 --> 00:06:25,860
half of the participants believed and

166
00:06:25,860 --> 00:06:28,560
also thought other people believed that

167
00:06:28,560 --> 00:06:30,120
digital data collection is an

168
00:06:30,120 --> 00:06:32,759
unavoidable fact of life they talked

169
00:06:32,759 --> 00:06:34,680
about unwanted data collection as

170
00:06:34,680 --> 00:06:36,720
something everybody knows is just part

171
00:06:36,720 --> 00:06:39,120
of using technology and cannot be

172
00:06:39,120 --> 00:06:42,660
avoided so choosing to use technology is

173
00:06:42,660 --> 00:06:44,940
choosing to allow data collection in an

174
00:06:44,940 --> 00:06:46,020
active sense

175
00:06:46,020 --> 00:06:48,000
it was also assumed that if a person

176
00:06:48,000 --> 00:06:50,100
uses a technology that collects data

177
00:06:50,100 --> 00:06:52,199
they must be okay with that data

178
00:06:52,199 --> 00:06:53,520
collection that comes along with it

179
00:06:53,520 --> 00:06:55,680
because after all they're using it right

180
00:06:55,680 --> 00:06:58,560
so the norm here involves acceptance of

181
00:06:58,560 --> 00:07:00,479
data collection as a consequence of

182
00:07:00,479 --> 00:07:03,258
using technology

183
00:07:03,960 --> 00:07:06,900
Norms also often involve sanctions of

184
00:07:06,900 --> 00:07:09,300
some kind towards behavior that does not

185
00:07:09,300 --> 00:07:12,120
comply with the norm and this also came

186
00:07:12,120 --> 00:07:14,220
through in the data so about a third of

187
00:07:14,220 --> 00:07:16,199
the participants talked about how people

188
00:07:16,199 --> 00:07:18,720
who are concerned about privacy related

189
00:07:18,720 --> 00:07:21,419
harms or corporate or government

190
00:07:21,419 --> 00:07:24,060
surveillance or who take actions to

191
00:07:24,060 --> 00:07:26,520
protect their privacy like not using

192
00:07:26,520 --> 00:07:28,740
certain apps or using a dumb phone

193
00:07:28,740 --> 00:07:31,680
instead of a smartphone are crazy or

194
00:07:31,680 --> 00:07:34,139
paranoid for doing those things

195
00:07:34,139 --> 00:07:36,419
and these comments were often made in an

196
00:07:36,419 --> 00:07:38,940
offhand sort of joking way

197
00:07:38,940 --> 00:07:40,680
um after the participant said something

198
00:07:40,680 --> 00:07:42,599
about they themselves being concerned

199
00:07:42,599 --> 00:07:44,580
about data collection and then sort of

200
00:07:44,580 --> 00:07:46,380
tried to distance themselves from that

201
00:07:46,380 --> 00:07:48,539
comment a little bit so the quote on the

202
00:07:48,539 --> 00:07:50,639
slide from voice assistant participant

203
00:07:50,639 --> 00:07:52,919
22 is an example of this where she said

204
00:07:52,919 --> 00:07:55,020
I don't want to sound like a paranoid

205
00:07:55,020 --> 00:07:57,120
person after talking about being

206
00:07:57,120 --> 00:07:59,039
concerned about privacy related to one

207
00:07:59,039 --> 00:08:01,699
of the scenarios

208
00:08:01,979 --> 00:08:04,560
the reactions to the scenarios were not

209
00:08:04,560 --> 00:08:07,020
all based on Norms there was actually a

210
00:08:07,020 --> 00:08:09,419
lot of individual non-social beliefs

211
00:08:09,419 --> 00:08:11,520
that participants talked about they

212
00:08:11,520 --> 00:08:12,900
talked about how it was important to

213
00:08:12,900 --> 00:08:14,580
them to be able to keep some information

214
00:08:14,580 --> 00:08:17,220
private and they felt strongly that they

215
00:08:17,220 --> 00:08:19,319
should have a chance to opt out of many

216
00:08:19,319 --> 00:08:21,120
of the inferences in the scenarios there

217
00:08:21,120 --> 00:08:22,500
was a lot of

218
00:08:22,500 --> 00:08:24,840
um concern is a little strong of a word

219
00:08:24,840 --> 00:08:26,160
but they really didn't want this stuff

220
00:08:26,160 --> 00:08:27,840
to start happening without knowing they

221
00:08:27,840 --> 00:08:29,520
thought that they should be able to opt

222
00:08:29,520 --> 00:08:31,440
out of it or be become more aware of it

223
00:08:31,440 --> 00:08:33,299
and this was an interesting contrast to

224
00:08:33,299 --> 00:08:34,979
the beliefs about data collection being

225
00:08:34,979 --> 00:08:37,380
an unavoidable fact of life because it

226
00:08:37,380 --> 00:08:39,419
shows that despite believing this people

227
00:08:39,419 --> 00:08:42,919
also do value privacy

228
00:08:43,500 --> 00:08:45,720
and then nearly all of the participants

229
00:08:45,720 --> 00:08:48,240
initially like their very first reaction

230
00:08:48,240 --> 00:08:50,279
when I stopped sort of reading the

231
00:08:50,279 --> 00:08:53,399
scenario to them was about how useful

232
00:08:53,399 --> 00:08:54,959
they thought the scenario could be for

233
00:08:54,959 --> 00:08:56,580
them or how they thought it could help

234
00:08:56,580 --> 00:08:58,320
other people so immediately people's

235
00:08:58,320 --> 00:08:59,880
minds went to how could this help me

236
00:08:59,880 --> 00:09:02,339
what could this do for me the quote on

237
00:09:02,339 --> 00:09:04,260
the slide says you can have your voice

238
00:09:04,260 --> 00:09:06,000
assistant suggest certain changes to

239
00:09:06,000 --> 00:09:07,680
your diet that she's been tracking for

240
00:09:07,680 --> 00:09:10,019
however long and you can be like wow I

241
00:09:10,019 --> 00:09:11,760
haven't eaten a fruit in two weeks I

242
00:09:11,760 --> 00:09:13,920
should add an apple in or something

243
00:09:13,920 --> 00:09:15,959
but when participants were asked

244
00:09:15,959 --> 00:09:17,820
specifically about hey how they thought

245
00:09:17,820 --> 00:09:20,580
others would react to the scenario they

246
00:09:20,580 --> 00:09:22,920
had trouble speculating about it unless

247
00:09:22,920 --> 00:09:24,480
it was something that violated an

248
00:09:24,480 --> 00:09:26,279
existing Norm like the bathroom scenario

249
00:09:26,279 --> 00:09:28,800
they said they would need more details

250
00:09:28,800 --> 00:09:32,399
about the person about the context about

251
00:09:32,399 --> 00:09:35,399
the the preferences and needs in order

252
00:09:35,399 --> 00:09:37,320
to make a reasonable guess about how

253
00:09:37,320 --> 00:09:38,700
somebody else would react to the

254
00:09:38,700 --> 00:09:39,779
scenario

255
00:09:39,779 --> 00:09:41,880
so overall there didn't really seem to

256
00:09:41,880 --> 00:09:44,339
be a collective set of beliefs about

257
00:09:44,339 --> 00:09:46,920
usefulness related to whether the data

258
00:09:46,920 --> 00:09:48,660
collection and use in the scenario was

259
00:09:48,660 --> 00:09:51,720
acceptable or unacceptable

260
00:09:51,720 --> 00:09:54,300
so are there norms for the collection

261
00:09:54,300 --> 00:09:57,120
and use of sensor data well sort of

262
00:09:57,120 --> 00:09:59,940
there are norms but that's not really as

263
00:09:59,940 --> 00:10:02,339
helpful as it sounds there were Norms

264
00:10:02,339 --> 00:10:04,620
that applied to inferences about things

265
00:10:04,620 --> 00:10:06,839
that sort of you wouldn't talk about to

266
00:10:06,839 --> 00:10:08,279
people in the real world like your

267
00:10:08,279 --> 00:10:10,200
bathroom Behavior or asking invasive

268
00:10:10,200 --> 00:10:12,839
questions about somebody else's kids but

269
00:10:12,839 --> 00:10:14,820
at the same time there were also these

270
00:10:14,820 --> 00:10:17,519
Norms that were disapproving of privacy

271
00:10:17,519 --> 00:10:20,760
protecting behaviors and because using

272
00:10:20,760 --> 00:10:22,920
Technologies presents the appearance of

273
00:10:22,920 --> 00:10:24,899
consent that's a pretty strong

274
00:10:24,899 --> 00:10:27,899
descriptive Norm that nobody else cares

275
00:10:27,899 --> 00:10:30,420
about privacy and then on top of that

276
00:10:30,420 --> 00:10:32,279
there was this really strong preference

277
00:10:32,279 --> 00:10:34,640
to just defer to other people's

278
00:10:34,640 --> 00:10:38,459
preferences and their choices about what

279
00:10:38,459 --> 00:10:40,140
kinds of scenarios they think would be

280
00:10:40,140 --> 00:10:42,959
most useful for them

281
00:10:42,959 --> 00:10:44,399
so this doesn't look great for

282
00:10:44,399 --> 00:10:46,620
Collective privacy management for one

283
00:10:46,620 --> 00:10:49,019
thing privacy choices are private we see

284
00:10:49,019 --> 00:10:50,579
these technologies that people are using

285
00:10:50,579 --> 00:10:51,899
but we don't see the ones that they

286
00:10:51,899 --> 00:10:54,240
don't use we don't know when somebody

287
00:10:54,240 --> 00:10:57,060
chooses not to reveal information or opt

288
00:10:57,060 --> 00:10:58,980
out of location tracking or doesn't use

289
00:10:58,980 --> 00:11:01,079
Facebook and because people who take

290
00:11:01,079 --> 00:11:03,060
steps to protect their privacy are seen

291
00:11:03,060 --> 00:11:05,160
at some level as being crazy or paranoid

292
00:11:05,160 --> 00:11:07,560
people are reluctant to speak up about

293
00:11:07,560 --> 00:11:10,560
their own privacy concerns

294
00:11:10,560 --> 00:11:12,240
there is something we can build on here

295
00:11:12,240 --> 00:11:14,579
though Norms are essentially beliefs

296
00:11:14,579 --> 00:11:17,220
about what other people believe so and

297
00:11:17,220 --> 00:11:19,560
for a norm to exist people need to know

298
00:11:19,560 --> 00:11:22,200
about other people's beliefs most

299
00:11:22,200 --> 00:11:24,000
awareness interventions related to

300
00:11:24,000 --> 00:11:25,920
privacy focus on the information about

301
00:11:25,920 --> 00:11:28,500
what data are collected and shared but

302
00:11:28,500 --> 00:11:30,480
platforms could provide more visibility

303
00:11:30,480 --> 00:11:33,360
in situations like this into how many

304
00:11:33,360 --> 00:11:35,220
people have opted out of tracking or

305
00:11:35,220 --> 00:11:36,959
what kinds of information people choose

306
00:11:36,959 --> 00:11:38,820
to protect or even the aspects of

307
00:11:38,820 --> 00:11:40,800
privacy policies and data practices that

308
00:11:40,800 --> 00:11:42,120
they object to

309
00:11:42,120 --> 00:11:44,160
so information like this might then

310
00:11:44,160 --> 00:11:46,500
create opportunities to weaken the norm

311
00:11:46,500 --> 00:11:49,079
that protecting one's privacy is deviant

312
00:11:49,079 --> 00:11:51,720
and that data privacy isn't important to

313
00:11:51,720 --> 00:11:53,519
anybody else

314
00:11:53,519 --> 00:11:56,240
thank you

