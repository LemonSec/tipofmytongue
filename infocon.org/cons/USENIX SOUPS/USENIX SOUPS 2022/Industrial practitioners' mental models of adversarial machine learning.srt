1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,119 --> 00:00:10,920
so who of you has heard about machine

3
00:00:10,920 --> 00:00:12,120
learning

4
00:00:12,120 --> 00:00:13,920
like just lift your hand okay everyone

5
00:00:13,920 --> 00:00:15,960
surprise

6
00:00:15,960 --> 00:00:17,520
um next question so who has heard about

7
00:00:17,520 --> 00:00:19,080
adversary machine learning or machine

8
00:00:19,080 --> 00:00:21,060
learning security just lift up your

9
00:00:21,060 --> 00:00:22,320
hands

10
00:00:22,320 --> 00:00:24,060
okay quite quite a few people and that's

11
00:00:24,060 --> 00:00:25,619
cool because I mean this is not a

12
00:00:25,619 --> 00:00:27,660
machine learning conference it's a

13
00:00:27,660 --> 00:00:29,880
slightly a different Community

14
00:00:29,880 --> 00:00:31,980
um and it makes sense because there have

15
00:00:31,980 --> 00:00:34,620
been like 10K papers about this as of

16
00:00:34,620 --> 00:00:36,420
now I think Nicholas kalini keeps the

17
00:00:36,420 --> 00:00:37,739
list there

18
00:00:37,739 --> 00:00:40,079
um so it is sort of a relevant Topic in

19
00:00:40,079 --> 00:00:42,120
terms of research and something and this

20
00:00:42,120 --> 00:00:44,640
happened sort of two years ago that I

21
00:00:44,640 --> 00:00:46,260
realized together with Lucas bearinger

22
00:00:46,260 --> 00:00:48,360
who also did a lot of work with startups

23
00:00:48,360 --> 00:00:49,980
is whenever I approached an organization

24
00:00:49,980 --> 00:00:51,899
or a company and started talking about

25
00:00:51,899 --> 00:00:53,160
the topic they were sort of not

26
00:00:53,160 --> 00:00:54,420
interested in that

27
00:00:54,420 --> 00:00:56,280
and what we decided together is we

28
00:00:56,280 --> 00:00:58,020
wanted to know more about that so why

29
00:00:58,020 --> 00:00:59,640
did people not care if this was

30
00:00:59,640 --> 00:01:01,739
scientifically such a relevant topic so

31
00:01:01,739 --> 00:01:03,300
we got help from Batista video Michael

32
00:01:03,300 --> 00:01:07,020
Bacchus and Katarina crumpols and we

33
00:01:07,020 --> 00:01:09,840
conducted a study about this topic and

34
00:01:09,840 --> 00:01:11,939
this is what I want to talk about today

35
00:01:11,939 --> 00:01:13,799
roughly the outline of this talk I will

36
00:01:13,799 --> 00:01:15,060
give a brief recap about machine

37
00:01:15,060 --> 00:01:16,560
learning which is obviously not

38
00:01:16,560 --> 00:01:18,180
necessary since everyone knows about the

39
00:01:18,180 --> 00:01:19,080
topic

40
00:01:19,080 --> 00:01:20,220
um also about machine learning security

41
00:01:20,220 --> 00:01:22,560
then I will introduce our sample and I

42
00:01:22,560 --> 00:01:24,299
will talk about the the findings that we

43
00:01:24,299 --> 00:01:25,860
had

44
00:01:25,860 --> 00:01:28,619
so machine learning in machine learning

45
00:01:28,619 --> 00:01:30,479
and a quick disclaimer as I'm talking

46
00:01:30,479 --> 00:01:31,920
about classification here but machine

47
00:01:31,920 --> 00:01:33,840
learning security there's also security

48
00:01:33,840 --> 00:01:35,340
issues in data mining statistics

49
00:01:35,340 --> 00:01:38,100
everything essentially but so in machine

50
00:01:38,100 --> 00:01:40,079
learning usually you have a task like in

51
00:01:40,079 --> 00:01:41,640
this case you might have patient data

52
00:01:41,640 --> 00:01:44,340
you want to predict a treatment red pill

53
00:01:44,340 --> 00:01:47,040
blue pill so you have this data

54
00:01:47,040 --> 00:01:49,200
aggregated in The Next Step you decide

55
00:01:49,200 --> 00:01:50,700
about an algorithm you decide which

56
00:01:50,700 --> 00:01:52,979
approach you want to take having senior

57
00:01:52,979 --> 00:01:56,159
data then you have a training phase so

58
00:01:56,159 --> 00:01:58,200
you take the data you put it into the

59
00:01:58,200 --> 00:02:00,180
algorithm you adapt the algorithm to the

60
00:02:00,180 --> 00:02:01,259
data

61
00:02:01,259 --> 00:02:03,659
and then ideally in the end you have an

62
00:02:03,659 --> 00:02:05,820
algorithm that you can show new data

63
00:02:05,820 --> 00:02:07,680
that it hasn't seen during training and

64
00:02:07,680 --> 00:02:09,119
the algorithm will predict the treatment

65
00:02:09,119 --> 00:02:11,819
in this case

66
00:02:11,819 --> 00:02:13,500
so adversary machine learning machine

67
00:02:13,500 --> 00:02:16,500
learning security I will explain one

68
00:02:16,500 --> 00:02:18,060
attack here just to give you a nutrition

69
00:02:18,060 --> 00:02:21,120
how it works this is a huge topic if you

70
00:02:21,120 --> 00:02:22,319
want to learn more about this there's

71
00:02:22,319 --> 00:02:24,120
actually a free lecture on YouTube from

72
00:02:24,120 --> 00:02:26,220
the group I'm working at

73
00:02:26,220 --> 00:02:27,900
so something you can do is you can

74
00:02:27,900 --> 00:02:29,640
temper with the training data so you

75
00:02:29,640 --> 00:02:31,200
compute some changes for the training

76
00:02:31,200 --> 00:02:33,120
data and this affects the training and

77
00:02:33,120 --> 00:02:36,239
the final classifier for example in

78
00:02:36,239 --> 00:02:39,300
terms of performance reduction or

79
00:02:39,300 --> 00:02:43,140
um in terms of wrong outputs for a

80
00:02:43,140 --> 00:02:44,760
particular subgroup so for example what

81
00:02:44,760 --> 00:02:46,200
you could think about if usually the

82
00:02:46,200 --> 00:02:48,480
classifier gets say 95 of your patients

83
00:02:48,480 --> 00:02:50,160
right you can tamper with the training

84
00:02:50,160 --> 00:02:52,560
data now you're only at 80 or otherwise

85
00:02:52,560 --> 00:02:54,660
you can also temper with a subgroup so

86
00:02:54,660 --> 00:02:56,280
for example now the algorithm gets

87
00:02:56,280 --> 00:02:57,840
everything right but for red-haired

88
00:02:57,840 --> 00:02:59,760
people it will always produce the wrong

89
00:02:59,760 --> 00:03:01,700
output or the same

90
00:03:01,700 --> 00:03:04,739
treatment for example

91
00:03:04,739 --> 00:03:06,060
okay

92
00:03:06,060 --> 00:03:09,660
and as I said this is um this is one

93
00:03:09,660 --> 00:03:11,099
attack of many that have been introduced

94
00:03:11,099 --> 00:03:13,080
there's a huge scientific body of work

95
00:03:13,080 --> 00:03:15,000
on this and then the question is okay so

96
00:03:15,000 --> 00:03:16,379
what do people in practice think about

97
00:03:16,379 --> 00:03:17,760
this

98
00:03:17,760 --> 00:03:18,959
um and here

99
00:03:18,959 --> 00:03:20,700
um I guess you all know so I can do a

100
00:03:20,700 --> 00:03:22,140
survey I can talk to many people

101
00:03:22,140 --> 00:03:24,000
quantitative research or I can talk to a

102
00:03:24,000 --> 00:03:26,580
few people qualitative research with a

103
00:03:26,580 --> 00:03:28,920
qualitative survey here

104
00:03:28,920 --> 00:03:32,220
so we talked in total to 15 participants

105
00:03:32,220 --> 00:03:34,800
um this was all in 2020

106
00:03:34,800 --> 00:03:37,500
um and although this might sound biased

107
00:03:37,500 --> 00:03:39,540
when I say that we had 14 male and one

108
00:03:39,540 --> 00:03:42,000
female participants if you look at the

109
00:03:42,000 --> 00:03:44,519
overall population of machine learning

110
00:03:44,519 --> 00:03:47,459
engineers and and workers then actually

111
00:03:47,459 --> 00:03:49,560
one to two females in this Temple would

112
00:03:49,560 --> 00:03:51,120
be representative of the overall

113
00:03:51,120 --> 00:03:52,560
population

114
00:03:52,560 --> 00:03:55,799
the average age was 34 years and

115
00:03:55,799 --> 00:03:58,019
employers were all European startups we

116
00:03:58,019 --> 00:03:59,819
tried to work with larger companies as

117
00:03:59,819 --> 00:04:02,459
well but unfortunately they denied a

118
00:04:02,459 --> 00:04:03,599
cooperation

119
00:04:03,599 --> 00:04:05,459
and we had all sorts of different

120
00:04:05,459 --> 00:04:07,019
application areas from cyber security

121
00:04:07,019 --> 00:04:10,680
Healthcare HR pretty much everything and

122
00:04:10,680 --> 00:04:13,500
we also had a wide variety in roles as

123
00:04:13,500 --> 00:04:15,500
you might think startups are very small

124
00:04:15,500 --> 00:04:17,699
so you might have someone who's at the

125
00:04:17,699 --> 00:04:19,858
same time an ml engineer and the CTO of

126
00:04:19,858 --> 00:04:22,079
the company

127
00:04:22,079 --> 00:04:24,540
all right so for the procedure of a

128
00:04:24,540 --> 00:04:26,940
study we had a demographics

129
00:04:26,940 --> 00:04:29,900
questionnaire which we send up front

130
00:04:29,900 --> 00:04:32,160
they either filled it out before the

131
00:04:32,160 --> 00:04:33,900
interview or during the interview before

132
00:04:33,900 --> 00:04:36,180
we started and we were generally two

133
00:04:36,180 --> 00:04:37,320
people one was conducting the interview

134
00:04:37,320 --> 00:04:39,000
the other person was observing the

135
00:04:39,000 --> 00:04:41,400
interview and taking notes

136
00:04:41,400 --> 00:04:42,660
um and there was a semi-structured

137
00:04:42,660 --> 00:04:44,160
interview where we first asked

138
00:04:44,160 --> 00:04:46,020
practitioners to describe their pipeline

139
00:04:46,020 --> 00:04:47,639
then we asked them about the security

140
00:04:47,639 --> 00:04:49,680
risks that they saw within their

141
00:04:49,680 --> 00:04:51,360
Pipeline and then we showed them three

142
00:04:51,360 --> 00:04:53,520
attacks and asked them to say if these

143
00:04:53,520 --> 00:04:55,500
were relevant and where they saw these

144
00:04:55,500 --> 00:04:58,380
um in their pipelines and this included

145
00:04:58,380 --> 00:05:00,000
a drawing task which was done on an

146
00:05:00,000 --> 00:05:03,660
online whiteboard because 2020 covet all

147
00:05:03,660 --> 00:05:05,639
the interviews were conducted remotely

148
00:05:05,639 --> 00:05:09,300
and afterwards we used the coding

149
00:05:09,300 --> 00:05:10,979
procedure for both the drawings and of

150
00:05:10,979 --> 00:05:14,759
the transcripts of the interviews

151
00:05:14,759 --> 00:05:18,660
so let's talk about our findings

152
00:05:18,660 --> 00:05:22,560
in Academia research for every paper

153
00:05:22,560 --> 00:05:24,419
that you can find you can say okay so

154
00:05:24,419 --> 00:05:26,400
this is machine learning security or

155
00:05:26,400 --> 00:05:28,199
this is cyber security or maybe it's

156
00:05:28,199 --> 00:05:29,639
machine learning for cyber security but

157
00:05:29,639 --> 00:05:31,680
like so there is a clear distinction and

158
00:05:31,680 --> 00:05:33,780
you can always say what is what so we

159
00:05:33,780 --> 00:05:35,580
sort of expected to see that in our

160
00:05:35,580 --> 00:05:37,020
practitioners as well

161
00:05:37,020 --> 00:05:40,320
but it turned out no so they might start

162
00:05:40,320 --> 00:05:42,539
talking about machine learning security

163
00:05:42,539 --> 00:05:45,060
issue and end up on cryptographic keys

164
00:05:45,060 --> 00:05:48,360
so this distinction wasn't quite clear

165
00:05:48,360 --> 00:05:51,300
and before I continue on that I just

166
00:05:51,300 --> 00:05:52,680
wanted to make a very brief remark about

167
00:05:52,680 --> 00:05:54,120
this because we have essentially no idea

168
00:05:54,120 --> 00:05:55,979
what's going on in practice and this

169
00:05:55,979 --> 00:05:59,039
could also be a valid abstraction but it

170
00:05:59,039 --> 00:06:01,440
could also be a like a lack of Education

171
00:06:01,440 --> 00:06:03,720
right so there's many different ways uh

172
00:06:03,720 --> 00:06:06,180
you could you could interpret that

173
00:06:06,180 --> 00:06:08,220
okay so what does this actually mean so

174
00:06:08,220 --> 00:06:09,840
how does how do these two concepts

175
00:06:09,840 --> 00:06:12,180
differ and how they were similar

176
00:06:12,180 --> 00:06:15,419
so if we talk about AML mitigations and

177
00:06:15,419 --> 00:06:16,500
they actually have to do the next

178
00:06:16,500 --> 00:06:18,180
disclaimer so unfortunately we don't

179
00:06:18,180 --> 00:06:19,500
really have that many working

180
00:06:19,500 --> 00:06:21,060
mitigations and machine learning

181
00:06:21,060 --> 00:06:23,460
security so we counted everything that

182
00:06:23,460 --> 00:06:25,740
makes an attack harder

183
00:06:25,740 --> 00:06:27,419
um so if you look at who mentioned

184
00:06:27,419 --> 00:06:29,280
machine learning security mitigations

185
00:06:29,280 --> 00:06:31,979
and security mitigations

186
00:06:31,979 --> 00:06:34,860
um then everyone mentions some form of

187
00:06:34,860 --> 00:06:36,960
cyber security stuff so like Access

188
00:06:36,960 --> 00:06:39,180
Control using cryptography you name it

189
00:06:39,180 --> 00:06:41,160
but only half of our participants

190
00:06:41,160 --> 00:06:42,660
mentioned techniques that actually make

191
00:06:42,660 --> 00:06:45,060
an attack harder if in terms of machine

192
00:06:45,060 --> 00:06:47,520
learning so there was already a big

193
00:06:47,520 --> 00:06:49,560
difference here and what is also quite

194
00:06:49,560 --> 00:06:51,479
remarkable is that whereas threats and

195
00:06:51,479 --> 00:06:54,240
security were often taken for granted in

196
00:06:54,240 --> 00:06:55,740
machine learning security there was

197
00:06:55,740 --> 00:06:58,319
always an excuse including for example

198
00:06:58,319 --> 00:06:59,940
that there is another team Hall take

199
00:06:59,940 --> 00:07:02,639
care of stuff that they have never seen

200
00:07:02,639 --> 00:07:04,400
an attack and therefore they don't exist

201
00:07:04,400 --> 00:07:06,840
that the attacker would have a weird

202
00:07:06,840 --> 00:07:09,419
motivation for example in healthcare

203
00:07:09,419 --> 00:07:11,039
right people would say a lot yeah but if

204
00:07:11,039 --> 00:07:12,840
you fool the algorithm that you pretend

205
00:07:12,840 --> 00:07:14,220
you're having cancer if you're not like

206
00:07:14,220 --> 00:07:16,620
why would you do that

207
00:07:16,620 --> 00:07:18,660
um and also a lot of people are very

208
00:07:18,660 --> 00:07:20,220
interesting said to have working

209
00:07:20,220 --> 00:07:22,259
mitigations which is actually more

210
00:07:22,259 --> 00:07:25,199
people than we counted coding-wise

211
00:07:25,199 --> 00:07:29,160
mitigations which is sort of interesting

212
00:07:29,160 --> 00:07:30,599
um but on the other hand we also had

213
00:07:30,599 --> 00:07:32,699
this kind of overlap where this was not

214
00:07:32,699 --> 00:07:34,500
well distinguished in for the two fields

215
00:07:34,500 --> 00:07:37,979
and one example for this is the tags for

216
00:07:37,979 --> 00:07:40,080
two ml attacks and one cyber security

217
00:07:40,080 --> 00:07:41,160
attack

218
00:07:41,160 --> 00:07:44,099
so a code breach would be that via code

219
00:07:44,099 --> 00:07:46,380
is obviously breached at leaked or

220
00:07:46,380 --> 00:07:47,220
something

221
00:07:47,220 --> 00:07:49,199
and on the other hand we have model

222
00:07:49,199 --> 00:07:51,360
reverse engineering model stealing these

223
00:07:51,360 --> 00:07:53,099
are two attacks in terms of machine

224
00:07:53,099 --> 00:07:55,139
learning where you are able to copy the

225
00:07:55,139 --> 00:07:56,759
model without consent

226
00:07:56,759 --> 00:07:59,160
and what you realize is these three

227
00:07:59,160 --> 00:08:00,539
things although they are conceptually

228
00:08:00,539 --> 00:08:01,919
very different actually to the same

229
00:08:01,919 --> 00:08:04,620
result which is the model is copied it

230
00:08:04,620 --> 00:08:05,960
has been outside

231
00:08:05,960 --> 00:08:08,880
and it can be used by the attacker

232
00:08:08,880 --> 00:08:11,520
and a similar thing what we observed in

233
00:08:11,520 --> 00:08:13,380
terms of co-occurrence of codes is for

234
00:08:13,380 --> 00:08:15,599
membership inference and data breach and

235
00:08:15,599 --> 00:08:18,060
here again the same as above you have

236
00:08:18,060 --> 00:08:20,280
the machine learning attack which

237
00:08:20,280 --> 00:08:21,599
extracts the data from the machine

238
00:08:21,599 --> 00:08:23,639
learning model and the data breach is

239
00:08:23,639 --> 00:08:25,139
sort of like hacking into the computer

240
00:08:25,139 --> 00:08:28,080
and getting the data so it seems to be

241
00:08:28,080 --> 00:08:30,960
that in some cases participants thought

242
00:08:30,960 --> 00:08:33,659
more goal oriented in terms of my data

243
00:08:33,659 --> 00:08:36,419
is gone and not so much in how the data

244
00:08:36,419 --> 00:08:39,539
actually was leaked

245
00:08:39,539 --> 00:08:41,279
um similar is like sort of variability

246
00:08:41,279 --> 00:08:43,440
attacks or dos

247
00:08:43,440 --> 00:08:46,020
um but here even I have to say like the

248
00:08:46,020 --> 00:08:47,700
the boundary between those two things

249
00:08:47,700 --> 00:08:49,740
for machine learning and cyber security

250
00:08:49,740 --> 00:08:51,899
is very very awake

251
00:08:51,899 --> 00:08:54,480
however we have also have two attacks um

252
00:08:54,480 --> 00:08:56,580
in terms of Machinery security poisoning

253
00:08:56,580 --> 00:08:58,380
and evasion poisoning was what I

254
00:08:58,380 --> 00:09:00,839
introduced in the beginning of the talk

255
00:09:00,839 --> 00:09:03,420
um these don't really have corresponding

256
00:09:03,420 --> 00:09:05,519
um cyber security attacks and

257
00:09:05,519 --> 00:09:07,260
thus that wasn't really like a

258
00:09:07,260 --> 00:09:10,320
co-occurrence and codes here

259
00:09:10,320 --> 00:09:12,779
we had another key finding which was

260
00:09:12,779 --> 00:09:15,120
more about the pipeline and how we see

261
00:09:15,120 --> 00:09:16,920
everything so this is

262
00:09:16,920 --> 00:09:18,839
um like sort of the also similar to what

263
00:09:18,839 --> 00:09:20,040
I showed in the beginning right so we

264
00:09:20,040 --> 00:09:21,420
have this idea of a pipeline we have

265
00:09:21,420 --> 00:09:23,399
input data we have a model and then we

266
00:09:23,399 --> 00:09:25,260
get some output

267
00:09:25,260 --> 00:09:26,880
um and then somewhere in there there is

268
00:09:26,880 --> 00:09:28,140
an attacker that's what you find the

269
00:09:28,140 --> 00:09:30,660
most scientific papers at least and it

270
00:09:30,660 --> 00:09:33,180
turns out that in practice live is a

271
00:09:33,180 --> 00:09:35,279
little bit more complicated you might

272
00:09:35,279 --> 00:09:37,019
have feedback loops you have several

273
00:09:37,019 --> 00:09:39,839
sources of data which are aggregated

274
00:09:39,839 --> 00:09:42,240
um and it's not at all clear whether the

275
00:09:42,240 --> 00:09:43,560
defenses and the attacks that we

276
00:09:43,560 --> 00:09:46,800
introduce carry over into which extent

277
00:09:46,800 --> 00:09:48,779
and I would like to give you an example

278
00:09:48,779 --> 00:09:51,060
for this so I said we asked our

279
00:09:51,060 --> 00:09:54,480
participants to draw their pipelines and

280
00:09:54,480 --> 00:09:58,019
here participant 18 you see

281
00:09:58,019 --> 00:10:00,240
um rather similar to what I showed you

282
00:10:00,240 --> 00:10:02,160
in the beginning there is some data then

283
00:10:02,160 --> 00:10:03,839
there is a model in the middle and then

284
00:10:03,839 --> 00:10:06,180
in the end there's some output right so

285
00:10:06,180 --> 00:10:08,640
roughly linear you see in the middle of

286
00:10:08,640 --> 00:10:10,980
the ml component corresponds to what we

287
00:10:10,980 --> 00:10:12,360
have seen

288
00:10:12,360 --> 00:10:15,000
on the other side on the Other Extreme

289
00:10:15,000 --> 00:10:18,720
rather participant 16. you see this is

290
00:10:18,720 --> 00:10:21,000
entirely different there's a python

291
00:10:21,000 --> 00:10:22,560
component that's a Java component so

292
00:10:22,560 --> 00:10:24,060
this is an entirely different level of

293
00:10:24,060 --> 00:10:25,320
abstraction

294
00:10:25,320 --> 00:10:27,779
there's virtual machines here it depicts

295
00:10:27,779 --> 00:10:30,360
the entire system and this is something

296
00:10:30,360 --> 00:10:32,100
that we need to keep in mind when we do

297
00:10:32,100 --> 00:10:35,459
research is that in in practice systems

298
00:10:35,459 --> 00:10:37,080
are way more complicated than what we

299
00:10:37,080 --> 00:10:38,040
consider

300
00:10:38,040 --> 00:10:40,260
and as I said so it's not clear whether

301
00:10:40,260 --> 00:10:42,240
defenses or attacks actually carry over

302
00:10:42,240 --> 00:10:46,079
and what it means for these attacks

303
00:10:46,079 --> 00:10:48,420
they are a row of open questions where

304
00:10:48,420 --> 00:10:49,860
we found a little bit of things but we

305
00:10:49,860 --> 00:10:51,480
obviously could not find a conclusive

306
00:10:51,480 --> 00:10:53,880
answer this concerns the application

307
00:10:53,880 --> 00:10:56,160
so for example

308
00:10:56,160 --> 00:10:58,200
um practitioners that work in cyber

309
00:10:58,200 --> 00:10:59,760
security and usml are they more

310
00:10:59,760 --> 00:11:01,860
concerned than some from from healthcare

311
00:11:01,860 --> 00:11:03,140
for example

312
00:11:03,140 --> 00:11:05,339
something we could also not clarify

313
00:11:05,339 --> 00:11:06,720
ultimately is the perceived relevance

314
00:11:06,720 --> 00:11:08,279
because we had a couple of people they

315
00:11:08,279 --> 00:11:10,500
were very concerned very nervous we had

316
00:11:10,500 --> 00:11:12,120
other people that were completely sure

317
00:11:12,120 --> 00:11:14,279
that nothing could happen to them so we

318
00:11:14,279 --> 00:11:15,600
didn't really get conclusive results

319
00:11:15,600 --> 00:11:19,380
there and also the education we couldn't

320
00:11:19,380 --> 00:11:24,060
really find a good connection to this

321
00:11:24,060 --> 00:11:25,800
um and that's actually the point where I

322
00:11:25,800 --> 00:11:27,540
shamelessly advertised follow-up work

323
00:11:27,540 --> 00:11:28,980
that we've done on that where we found

324
00:11:28,980 --> 00:11:31,440
replies or answers for some of these

325
00:11:31,440 --> 00:11:33,240
points

326
00:11:33,240 --> 00:11:35,240
what I personally found very interesting

327
00:11:35,240 --> 00:11:38,040
also in relation to what I said in the

328
00:11:38,040 --> 00:11:39,480
beginning so is this actually a thing

329
00:11:39,480 --> 00:11:42,120
that occurs in practice is that we found

330
00:11:42,120 --> 00:11:44,640
this one practitioner who had actually

331
00:11:44,640 --> 00:11:47,760
experienced attacks

332
00:11:47,760 --> 00:11:51,060
and what they said is well we have this

333
00:11:51,060 --> 00:11:53,760
highly optimized super specific attacks

334
00:11:53,760 --> 00:11:56,940
in Academia that are very that alter the

335
00:11:56,940 --> 00:11:59,459
input data very very little and it turns

336
00:11:59,459 --> 00:12:00,660
out this is not really what's going on

337
00:12:00,660 --> 00:12:02,339
in practice and practice it seems to be

338
00:12:02,339 --> 00:12:04,440
more people gaming systems

339
00:12:04,440 --> 00:12:06,360
semi-automated fraud like trying to get

340
00:12:06,360 --> 00:12:08,220
around the system but not in a very very

341
00:12:08,220 --> 00:12:10,079
smart way not comparative to what we do

342
00:12:10,079 --> 00:12:12,480
in research but there are attacks

343
00:12:12,480 --> 00:12:14,399
already

344
00:12:14,399 --> 00:12:17,220
so let me summarize this talk

345
00:12:17,220 --> 00:12:19,380
um so in case that you're a company and

346
00:12:19,380 --> 00:12:21,660
you want that your products are safe in

347
00:12:21,660 --> 00:12:23,160
terms of cyber security and machine

348
00:12:23,160 --> 00:12:25,380
learning security you should either take

349
00:12:25,380 --> 00:12:26,880
here because of what I described as

350
00:12:26,880 --> 00:12:29,160
mingling you should either enforce that

351
00:12:29,160 --> 00:12:31,920
both factors are checked together or

352
00:12:31,920 --> 00:12:33,600
separately and done enforce each

353
00:12:33,600 --> 00:12:36,140
individually

354
00:12:36,360 --> 00:12:38,940
then in terms of the pipelines I think

355
00:12:38,940 --> 00:12:41,279
research would benefit hugely from

356
00:12:41,279 --> 00:12:43,200
understanding what is actually how

357
00:12:43,200 --> 00:12:45,720
machine learning is used and practiced

358
00:12:45,720 --> 00:12:47,880
which kind of pipeline is used and for a

359
00:12:47,880 --> 00:12:49,200
defenses or attacks are actually

360
00:12:49,200 --> 00:12:51,360
applicable to this and what it means for

361
00:12:51,360 --> 00:12:52,920
machine learning to work within a larger

362
00:12:52,920 --> 00:12:55,139
system

363
00:12:55,139 --> 00:12:57,540
then as I said so we have learned from

364
00:12:57,540 --> 00:12:58,980
with a study that there are a text

365
00:12:58,980 --> 00:13:00,899
already in practice but they are not as

366
00:13:00,899 --> 00:13:02,459
bad as we might think when we look at

367
00:13:02,459 --> 00:13:05,220
scientific research

368
00:13:05,220 --> 00:13:08,100
and then last but not least it I think

369
00:13:08,100 --> 00:13:09,720
it's important to educate practitioners

370
00:13:09,720 --> 00:13:11,940
because as a matter of fact we had

371
00:13:11,940 --> 00:13:13,200
offered

372
00:13:13,200 --> 00:13:16,920
um a financial compensation to take part

373
00:13:16,920 --> 00:13:18,720
in interviews and many practitioners

374
00:13:18,720 --> 00:13:19,920
actually said you know I don't care

375
00:13:19,920 --> 00:13:21,839
about the money if you can give me like

376
00:13:21,839 --> 00:13:23,519
20 minutes afterwards where I can ask

377
00:13:23,519 --> 00:13:25,560
you questions about this topic

378
00:13:25,560 --> 00:13:28,260
so um it might be good to prove like to

379
00:13:28,260 --> 00:13:30,240
provide more material than just

380
00:13:30,240 --> 00:13:32,880
scientific papers where people can get

381
00:13:32,880 --> 00:13:34,380
information

382
00:13:34,380 --> 00:13:37,279
all right thank you

