1
00:00:07,919 --> 00:00:10,679
all right hi everyone I'm Miranda today

2
00:00:10,679 --> 00:00:12,480
I'm going to be presenting work done

3
00:00:12,480 --> 00:00:14,700
with my co-authors partis franzi and

4
00:00:14,700 --> 00:00:16,260
Yoshi and this is work presented at

5
00:00:16,260 --> 00:00:18,060
Oakland next year

6
00:00:18,060 --> 00:00:20,340
so we know that gender stereotypes exist

7
00:00:20,340 --> 00:00:22,980
everywhere and even though gender is not

8
00:00:22,980 --> 00:00:24,359
a binary there are many gender

9
00:00:24,359 --> 00:00:25,740
stereotypes

10
00:00:25,740 --> 00:00:27,960
for example represented in pink and blue

11
00:00:27,960 --> 00:00:29,460
on this slide there are gender

12
00:00:29,460 --> 00:00:32,579
stereotypes about how people are how

13
00:00:32,579 --> 00:00:34,620
their personal characteristics are and

14
00:00:34,620 --> 00:00:36,840
these are derived from gender roles

15
00:00:36,840 --> 00:00:39,420
about men and women and these gender

16
00:00:39,420 --> 00:00:41,820
roles also dictate what kinds of careers

17
00:00:41,820 --> 00:00:43,980
people go into and what how they how

18
00:00:43,980 --> 00:00:47,579
they will act in the future and so how

19
00:00:47,579 --> 00:00:49,079
do these gender stereotypes translate

20
00:00:49,079 --> 00:00:51,780
into security and privacy

21
00:00:51,780 --> 00:00:53,640
in this work we studied what gender

22
00:00:53,640 --> 00:00:55,440
stereotypes people have related to

23
00:00:55,440 --> 00:00:57,059
security and privacy attitudes and

24
00:00:57,059 --> 00:00:58,199
behaviors

25
00:00:58,199 --> 00:01:00,420
first we conducted a pre-study where we

26
00:01:00,420 --> 00:01:02,640
sought to identify potential stereotypes

27
00:01:02,640 --> 00:01:05,220
then we conducted Affinity diagramming

28
00:01:05,220 --> 00:01:07,200
to select further potential stereotypes

29
00:01:07,200 --> 00:01:09,780
from related work and we conducted a

30
00:01:09,780 --> 00:01:12,380
main study to evaluate the prevalence of

31
00:01:12,380 --> 00:01:14,520
participant the prevalence of the

32
00:01:14,520 --> 00:01:16,140
stereotypes as well as the participant

33
00:01:16,140 --> 00:01:18,720
rationales for their stereotypes

34
00:01:18,720 --> 00:01:21,540
so what stereotypes do we find well we

35
00:01:21,540 --> 00:01:25,560
found in this graph that shows the

36
00:01:25,560 --> 00:01:29,040
stereotypes that might relate to women

37
00:01:29,040 --> 00:01:31,979
and men that women were more likely to

38
00:01:31,979 --> 00:01:34,140
the posterior type was that women were

39
00:01:34,140 --> 00:01:35,520
more likely to share sensitive

40
00:01:35,520 --> 00:01:38,460
information on social media be emotional

41
00:01:38,460 --> 00:01:40,979
fall for shopping scams ask for help if

42
00:01:40,979 --> 00:01:43,200
they have questions and be gullible and

43
00:01:43,200 --> 00:01:45,180
note that all four out of these five

44
00:01:45,180 --> 00:01:47,700
gender stereotypes are negative except

45
00:01:47,700 --> 00:01:48,900
for asking for help if they have

46
00:01:48,900 --> 00:01:51,140
questions

47
00:01:51,360 --> 00:01:54,240
what about stereotypes of men

48
00:01:54,240 --> 00:01:55,979
here are 10 stereotypes that we found

49
00:01:55,979 --> 00:01:58,259
about men which are that men generally

50
00:01:58,259 --> 00:02:00,240
will know how to protect their security

51
00:02:00,240 --> 00:02:02,040
and privacy they'll be skilled at doing

52
00:02:02,040 --> 00:02:04,020
so and they'll also use common security

53
00:02:04,020 --> 00:02:06,659
and privacy tools like verifying https

54
00:02:06,659 --> 00:02:09,479
using 2fa or using anti-virus software

55
00:02:09,479 --> 00:02:11,520
and note that most of these are positive

56
00:02:11,520 --> 00:02:14,760
except for being overconfident

57
00:02:14,760 --> 00:02:17,280
how do these stereotypes relate to the

58
00:02:17,280 --> 00:02:18,840
people who hold them

59
00:02:18,840 --> 00:02:20,700
we found that men were more likely to

60
00:02:20,700 --> 00:02:22,680
believe positive stereotypes about men

61
00:02:22,680 --> 00:02:25,020
and we also found that participants who

62
00:02:25,020 --> 00:02:27,060
scored higher on the ASI which is a

63
00:02:27,060 --> 00:02:29,160
standardized measure of sexism were more

64
00:02:29,160 --> 00:02:30,900
likely to believe negative stereotypes

65
00:02:30,900 --> 00:02:32,879
about women and more likely to believe

66
00:02:32,879 --> 00:02:36,180
positive stereotypes about men

67
00:02:36,180 --> 00:02:38,940
finally what reasons did people believe

68
00:02:38,940 --> 00:02:41,340
these stereotypes

69
00:02:41,340 --> 00:02:44,819
one reason were other stereotypes for

70
00:02:44,819 --> 00:02:46,739
example women have a tendency to be

71
00:02:46,739 --> 00:02:48,480
compassionate and listen to others and

72
00:02:48,480 --> 00:02:49,739
that often gives scammers the

73
00:02:49,739 --> 00:02:51,780
opportunity to fool them

74
00:02:51,780 --> 00:02:53,580
another reason where societal

75
00:02:53,580 --> 00:02:55,980
expectations at large for example the

76
00:02:55,980 --> 00:02:57,900
social coding of security and Technology

77
00:02:57,900 --> 00:02:59,819
Hobbies is masculine

78
00:02:59,819 --> 00:03:01,739
and finally another reason where

79
00:03:01,739 --> 00:03:04,080
gendered threat models for example men

80
00:03:04,080 --> 00:03:05,519
probably have more to hide on their

81
00:03:05,519 --> 00:03:07,560
devices honestly like lock up porn

82
00:03:07,560 --> 00:03:10,140
history or that women are more often the

83
00:03:10,140 --> 00:03:11,940
targets of cyber stalking docks and

84
00:03:11,940 --> 00:03:13,319
campaigns and so they have a more

85
00:03:13,319 --> 00:03:16,080
obvious reason to learn

86
00:03:16,080 --> 00:03:18,300
what are the contributions of this work

87
00:03:18,300 --> 00:03:21,180
first it facilitates studying the impact

88
00:03:21,180 --> 00:03:23,640
of specific gender stereotypes we know

89
00:03:23,640 --> 00:03:25,140
that there are gender stereotypes in

90
00:03:25,140 --> 00:03:27,180
other fields and now we know that there

91
00:03:27,180 --> 00:03:29,220
are gender stereotypes here

92
00:03:29,220 --> 00:03:31,200
it also highlights biases that might

93
00:03:31,200 --> 00:03:32,940
influence security and privacy user

94
00:03:32,940 --> 00:03:34,680
studies particularly ones conducted with

95
00:03:34,680 --> 00:03:37,560
crowd workers which our sample was

96
00:03:37,560 --> 00:03:40,980
and further this study of gender

97
00:03:40,980 --> 00:03:42,360
stereotypes

98
00:03:42,360 --> 00:03:44,220
poses a theoretically informed

99
00:03:44,220 --> 00:03:46,080
investigation for gender differences

100
00:03:46,080 --> 00:03:48,180
suggested by prior work for example

101
00:03:48,180 --> 00:03:49,920
prior work Mike has found that women

102
00:03:49,920 --> 00:03:51,720
might choose weaker passwords might be

103
00:03:51,720 --> 00:03:53,819
less likely to use private browsing or

104
00:03:53,819 --> 00:03:56,040
be more susceptible to phishing and one

105
00:03:56,040 --> 00:03:57,360
theory for this is now social

106
00:03:57,360 --> 00:03:59,940
constructionism essentially to do with

107
00:03:59,940 --> 00:04:02,760
societal expectations and gender norms

108
00:04:02,760 --> 00:04:05,040
and not biological essentialism which

109
00:04:05,040 --> 00:04:06,840
would say that is somehow intrinsically

110
00:04:06,840 --> 00:04:10,580
determined by people's biology

111
00:04:10,739 --> 00:04:13,080
finally this work also validates the

112
00:04:13,080 --> 00:04:15,120
experiences of any of you who have been

113
00:04:15,120 --> 00:04:16,798
at the experience who have been at the

114
00:04:16,798 --> 00:04:18,660
receiving end of harmful stereotypes and

115
00:04:18,660 --> 00:04:20,279
security and privacy

116
00:04:20,279 --> 00:04:21,660
thanks so much and I'll take questions

117
00:04:21,660 --> 00:04:24,019
now

