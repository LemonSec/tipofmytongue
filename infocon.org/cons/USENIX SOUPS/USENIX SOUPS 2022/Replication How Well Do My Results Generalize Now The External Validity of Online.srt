1
00:00:09,360 --> 00:00:11,700
okay so thank you everyone for being

2
00:00:11,700 --> 00:00:13,740
here and thank you to Ada and Eleanor

3
00:00:13,740 --> 00:00:15,960
for being wonderful mentors and just

4
00:00:15,960 --> 00:00:17,760
people in general so this is a

5
00:00:17,760 --> 00:00:19,680
replication paper called how well do my

6
00:00:19,680 --> 00:00:22,020
results generalize now and explores the

7
00:00:22,020 --> 00:00:24,119
external validity of online security and

8
00:00:24,119 --> 00:00:25,500
privacy surveys

9
00:00:25,500 --> 00:00:28,500
so this is based on a paper by Alyssa

10
00:00:28,500 --> 00:00:30,300
red miles John Cross Michelle masrick

11
00:00:30,300 --> 00:00:32,098
called how well do my results generalize

12
00:00:32,098 --> 00:00:34,680
and Based on data collected in 2017 they

13
00:00:34,680 --> 00:00:36,540
explored the generalizability of Amazon

14
00:00:36,540 --> 00:00:39,000
Mechanical Turk or mturk to the U.S

15
00:00:39,000 --> 00:00:41,460
population in terms of security and

16
00:00:41,460 --> 00:00:43,920
privacy behaviors and experiences and

17
00:00:43,920 --> 00:00:46,140
they found that mturk was relatively

18
00:00:46,140 --> 00:00:48,719
generalizable for most things such as

19
00:00:48,719 --> 00:00:51,300
advice sources experiences and feelings

20
00:00:51,300 --> 00:00:52,680
of knowledge but it was not

21
00:00:52,680 --> 00:00:54,420
generalizable for questions about

22
00:00:54,420 --> 00:00:56,699
behaviors possibly because the people on

23
00:00:56,699 --> 00:00:58,620
these platforms tended to be a bit more

24
00:00:58,620 --> 00:00:59,940
tech savvy

25
00:00:59,940 --> 00:01:01,680
they also found that generalizable

26
00:01:01,680 --> 00:01:04,379
populations on these platforms tend to

27
00:01:04,379 --> 00:01:06,900
generalize well for those under 50 or

28
00:01:06,900 --> 00:01:09,060
with some college education

29
00:01:09,060 --> 00:01:13,380
now 2017 was five years ago and a lot

30
00:01:13,380 --> 00:01:16,080
has changed since then so welcome to how

31
00:01:16,080 --> 00:01:18,720
well do my results journalize now 2022

32
00:01:18,720 --> 00:01:19,920
Edition

33
00:01:19,920 --> 00:01:23,280
and so how have the online platforms

34
00:01:23,280 --> 00:01:25,020
that people use for surveys changed

35
00:01:25,020 --> 00:01:28,380
since 2017. researchers still use both

36
00:01:28,380 --> 00:01:30,900
mturk and prolific though prolific has

37
00:01:30,900 --> 00:01:33,900
become ever more popular recently it was

38
00:01:33,900 --> 00:01:36,540
not commonly used in 2017 but now seems

39
00:01:36,540 --> 00:01:39,659
to be more popular than mturk and we've

40
00:01:39,659 --> 00:01:42,540
also seen a drop in quality from Amtrak

41
00:01:42,540 --> 00:01:44,880
data possibly because of an increase in

42
00:01:44,880 --> 00:01:47,340
the number of bots on the platform

43
00:01:47,340 --> 00:01:49,860
so this leads us to a few questions

44
00:01:49,860 --> 00:01:52,380
firstly can we replicate prior results

45
00:01:52,380 --> 00:01:55,079
about the representativeness of mturk to

46
00:01:55,079 --> 00:01:57,119
the U.S population in terms of security

47
00:01:57,119 --> 00:01:59,759
and privacy surveys and we ask a similar

48
00:01:59,759 --> 00:02:01,920
question for prolific how well does

49
00:02:01,920 --> 00:02:03,659
prolific generalize to the U.S

50
00:02:03,659 --> 00:02:04,860
population

51
00:02:04,860 --> 00:02:06,600
before we go any further let's talk a

52
00:02:06,600 --> 00:02:08,459
bit about generalizability

53
00:02:08,459 --> 00:02:11,700
right ideally we'd be able to survey

54
00:02:11,700 --> 00:02:13,920
everyone in the United States including

55
00:02:13,920 --> 00:02:17,340
Hawaii and Alaska and get them to tell

56
00:02:17,340 --> 00:02:19,379
us their security and privacy behaviors

57
00:02:19,379 --> 00:02:21,120
and answer our questions

58
00:02:21,120 --> 00:02:24,300
that's first really expensive and also

59
00:02:24,300 --> 00:02:27,120
not possible so the state of the art

60
00:02:27,120 --> 00:02:29,940
used by Major organizations such as Pew

61
00:02:29,940 --> 00:02:31,739
are called weighted probabilistic

62
00:02:31,739 --> 00:02:34,080
samples so they make sure to recruit

63
00:02:34,080 --> 00:02:36,000
hard to reach populations such as those

64
00:02:36,000 --> 00:02:38,340
who are older not as tax Savvy or don't

65
00:02:38,340 --> 00:02:41,040
have internet and they collect data that

66
00:02:41,040 --> 00:02:43,379
after waiting is within a few percentage

67
00:02:43,379 --> 00:02:45,300
points of the actual U.S population

68
00:02:45,300 --> 00:02:46,860
demographics

69
00:02:46,860 --> 00:02:49,319
now these kinds of large-scale

70
00:02:49,319 --> 00:02:51,360
probabilistic samples are still not

71
00:02:51,360 --> 00:02:53,099
feasible for most security and privacy

72
00:02:53,099 --> 00:02:54,780
researchers they take a lot of time and

73
00:02:54,780 --> 00:02:56,940
a lot of money so we turn to things like

74
00:02:56,940 --> 00:02:59,700
mtric and prolific and so the question

75
00:02:59,700 --> 00:03:02,519
is okay so how well do these platforms

76
00:03:02,519 --> 00:03:03,720
generalize

77
00:03:03,720 --> 00:03:06,120
and to answer that we did a qualitative

78
00:03:06,120 --> 00:03:07,860
analysis of questions asked by security

79
00:03:07,860 --> 00:03:09,480
and privacy research in the last year

80
00:03:09,480 --> 00:03:12,000
and identified five major categories of

81
00:03:12,000 --> 00:03:13,260
questions

82
00:03:13,260 --> 00:03:16,319
the first one is behavioral questions so

83
00:03:16,319 --> 00:03:19,500
actions users take so what they do would

84
00:03:19,500 --> 00:03:21,480
do or have done such as their social

85
00:03:21,480 --> 00:03:24,120
media use do you use Facebook

86
00:03:24,120 --> 00:03:25,560
the second type of question were

87
00:03:25,560 --> 00:03:27,360
experience questions so whether or not

88
00:03:27,360 --> 00:03:29,700
certain events or experiences have

89
00:03:29,700 --> 00:03:32,459
happened to the user so has someone

90
00:03:32,459 --> 00:03:34,260
taken over your social media or email

91
00:03:34,260 --> 00:03:35,580
account

92
00:03:35,580 --> 00:03:38,400
third type was Knowledge Questions so we

93
00:03:38,400 --> 00:03:40,080
defined these as those that have

94
00:03:40,080 --> 00:03:42,720
factually correct answers so for example

95
00:03:42,720 --> 00:03:45,000
what does it mean to have a privacy

96
00:03:45,000 --> 00:03:47,040
policy and the way we asked these there

97
00:03:47,040 --> 00:03:49,080
was one correct answer three to five

98
00:03:49,080 --> 00:03:52,440
incorrect answers and a not sure or I

99
00:03:52,440 --> 00:03:54,360
don't know option

100
00:03:54,360 --> 00:03:57,000
there's also perceptions questions so

101
00:03:57,000 --> 00:03:58,980
these are opinion questions about

102
00:03:58,980 --> 00:04:01,620
attitudes such as trust Comfort levels

103
00:04:01,620 --> 00:04:03,900
mental models so for example how

104
00:04:03,900 --> 00:04:05,700
comfortable people feel with certain

105
00:04:05,700 --> 00:04:08,040
uses of their personal data

106
00:04:08,040 --> 00:04:10,620
and finally there are beliefs questions

107
00:04:10,620 --> 00:04:13,739
so these are more Norms based about what

108
00:04:13,739 --> 00:04:16,440
should or ought to be true so should

109
00:04:16,440 --> 00:04:18,238
Americans have the right to have certain

110
00:04:18,238 --> 00:04:20,279
types of data deleted

111
00:04:20,279 --> 00:04:22,919
and so we identified a survey conducted

112
00:04:22,919 --> 00:04:24,600
by Pew Research Center that had

113
00:04:24,600 --> 00:04:26,100
questions in all five of these

114
00:04:26,100 --> 00:04:28,500
categories and from each category we

115
00:04:28,500 --> 00:04:30,360
took four to eight of these exact same

116
00:04:30,360 --> 00:04:32,940
questions and asked them again on mturk

117
00:04:32,940 --> 00:04:35,160
and prolific to compare the answers from

118
00:04:35,160 --> 00:04:38,580
these platforms to pews samples answers

119
00:04:38,580 --> 00:04:41,400
so let's take a look at our data sets

120
00:04:41,400 --> 00:04:43,620
so the Pew Research Center data we

121
00:04:43,620 --> 00:04:45,540
didn't collect ourselves it was the gold

122
00:04:45,540 --> 00:04:47,639
standard to which we compared our

123
00:04:47,639 --> 00:04:49,380
answers from the online platforms

124
00:04:49,380 --> 00:04:51,060
because that's the closest we have to

125
00:04:51,060 --> 00:04:53,340
the American populations ideas

126
00:04:53,340 --> 00:04:57,240
for mturk we collected 800 samples and

127
00:04:57,240 --> 00:04:59,160
we limited it to those in the United

128
00:04:59,160 --> 00:05:02,580
States with above a 95 approval rate and

129
00:05:02,580 --> 00:05:04,979
over 50 completed tasks because that's

130
00:05:04,979 --> 00:05:06,479
the general default that most people

131
00:05:06,479 --> 00:05:09,240
would use for decent quality data on

132
00:05:09,240 --> 00:05:10,380
mturk

133
00:05:10,380 --> 00:05:13,199
for prolific we have two samples so the

134
00:05:13,199 --> 00:05:15,540
first one is the gender balance sample

135
00:05:15,540 --> 00:05:17,699
and so we chose the gender balance

136
00:05:17,699 --> 00:05:19,620
sample rather than a raw correlative

137
00:05:19,620 --> 00:05:21,419
example I mean yes prolific sample

138
00:05:21,419 --> 00:05:24,300
pardon me because in Fall last year

139
00:05:24,300 --> 00:05:27,120
there was a viral Tick Tock and it

140
00:05:27,120 --> 00:05:30,000
caused an influx of young woman to go to

141
00:05:30,000 --> 00:05:33,120
prolific so even now I believe there's

142
00:05:33,120 --> 00:05:35,160
around two times or three times as many

143
00:05:35,160 --> 00:05:37,440
women as men on the prolific platform

144
00:05:37,440 --> 00:05:39,720
and for Olympic was like oh this isn't

145
00:05:39,720 --> 00:05:42,300
great because we're a survey platform so

146
00:05:42,300 --> 00:05:44,460
to mitigate this they implemented a

147
00:05:44,460 --> 00:05:45,960
feature called gender balance which is

148
00:05:45,960 --> 00:05:47,759
available to researchers at no

149
00:05:47,759 --> 00:05:49,979
additional cost that gives us equal

150
00:05:49,979 --> 00:05:52,680
numbers of men and women when you deploy

151
00:05:52,680 --> 00:05:54,360
your survey so that makes sense for

152
00:05:54,360 --> 00:05:56,340
people to use as the default

153
00:05:56,340 --> 00:05:58,440
and lastly we have the prolific

154
00:05:58,440 --> 00:06:00,600
representative sample which the platform

155
00:06:00,600 --> 00:06:03,720
also offers and it is generalizable to I

156
00:06:03,720 --> 00:06:05,520
mean it is representative of the U.S

157
00:06:05,520 --> 00:06:08,940
population in terms of age sex and race

158
00:06:08,940 --> 00:06:11,160
and in terms of the time it took to

159
00:06:11,160 --> 00:06:12,720
collect our data the gender balance

160
00:06:12,720 --> 00:06:14,340
sample and amateur example are quite

161
00:06:14,340 --> 00:06:16,620
similar around two hours whereas the

162
00:06:16,620 --> 00:06:18,300
representative sample took quite a bit

163
00:06:18,300 --> 00:06:21,300
longer similarly in terms of cost the

164
00:06:21,300 --> 00:06:23,160
enteric sample and gender balance sample

165
00:06:23,160 --> 00:06:25,080
are similar whereas the representative

166
00:06:25,080 --> 00:06:27,120
sample costs more

167
00:06:27,120 --> 00:06:29,819
so let's take a look at how these things

168
00:06:29,819 --> 00:06:32,539
performed on the

169
00:06:32,539 --> 00:06:35,460
y-axis we have each category of question

170
00:06:35,460 --> 00:06:37,500
we asked and how will each sample

171
00:06:37,500 --> 00:06:39,900
performed in each category

172
00:06:39,900 --> 00:06:42,720
on the y-axis we have total variation

173
00:06:42,720 --> 00:06:45,720
distance or TVD which measures how far

174
00:06:45,720 --> 00:06:48,419
each sample differs from Q on a scale of

175
00:06:48,419 --> 00:06:50,880
0 to 1. so this is a measure of distance

176
00:06:50,880 --> 00:06:53,520
some smaller is better as the arrow

177
00:06:53,520 --> 00:06:56,819
indicates and so if you get zero that

178
00:06:56,819 --> 00:06:58,259
means your sample's answers were

179
00:06:58,259 --> 00:07:00,479
identical to Q's answers

180
00:07:00,479 --> 00:07:03,419
and what we see here are the prolific

181
00:07:03,419 --> 00:07:06,060
samples performed similarly well in

182
00:07:06,060 --> 00:07:08,880
terms of generalizability whereas mterk

183
00:07:08,880 --> 00:07:12,900
doesn't perform as well now this is raw

184
00:07:12,900 --> 00:07:14,639
data and most of us are not going to be

185
00:07:14,639 --> 00:07:16,560
using raw data right we're going to be

186
00:07:16,560 --> 00:07:18,000
using attention checks waiting and

187
00:07:18,000 --> 00:07:19,620
things like that which leads us to

188
00:07:19,620 --> 00:07:21,539
another question right what are the

189
00:07:21,539 --> 00:07:23,520
effects of data quality measures on

190
00:07:23,520 --> 00:07:25,860
generalizability and based on that what

191
00:07:25,860 --> 00:07:27,419
are current best practices for online

192
00:07:27,419 --> 00:07:28,979
surveys

193
00:07:28,979 --> 00:07:31,560
and so our data quality checks include

194
00:07:31,560 --> 00:07:33,720
reading attention checks so this one is

195
00:07:33,720 --> 00:07:35,220
select the correct multiple choice

196
00:07:35,220 --> 00:07:37,620
answer we tell you which one it is

197
00:07:37,620 --> 00:07:39,960
this one is the free response text

198
00:07:39,960 --> 00:07:42,120
question so we ask users to type in

199
00:07:42,120 --> 00:07:44,699
their own responses to what is digital

200
00:07:44,699 --> 00:07:45,720
privacy

201
00:07:45,720 --> 00:07:48,120
and finally captures this one is click

202
00:07:48,120 --> 00:07:50,220
the traffic light yeah

203
00:07:50,220 --> 00:07:52,860
all right we also look at the effects of

204
00:07:52,860 --> 00:07:54,840
waiting and breaking so as you can see

205
00:07:54,840 --> 00:07:57,300
here the data on prolific excuse very

206
00:07:57,300 --> 00:08:02,099
young and so very on a very high level

207
00:08:02,099 --> 00:08:04,979
waiting is a statistical technique that

208
00:08:04,979 --> 00:08:06,840
re-weights data to match the proportions

209
00:08:06,840 --> 00:08:09,000
of your target population in this case

210
00:08:09,000 --> 00:08:11,400
the US population in certain categories

211
00:08:11,400 --> 00:08:13,620
and the categories we chose were age

212
00:08:13,620 --> 00:08:15,960
education race and sex

213
00:08:15,960 --> 00:08:19,199
so using these data quality checks we

214
00:08:19,199 --> 00:08:22,440
found that over 39.1 percent of our

215
00:08:22,440 --> 00:08:24,660
amateur example failed the text box

216
00:08:24,660 --> 00:08:26,699
attention question so when they were

217
00:08:26,699 --> 00:08:29,099
asked to type in an answer to Define

218
00:08:29,099 --> 00:08:31,740
data digital privacy over 300 people

219
00:08:31,740 --> 00:08:35,159
failed people Bots whatever no one

220
00:08:35,159 --> 00:08:37,440
really failed our captures but most

221
00:08:37,440 --> 00:08:39,240
people passed the other attention check

222
00:08:39,240 --> 00:08:41,099
which was the select the correct

223
00:08:41,099 --> 00:08:43,620
multiple choice which seems to imply

224
00:08:43,620 --> 00:08:45,060
that Bots are getting ever more

225
00:08:45,060 --> 00:08:46,200
sophisticated

226
00:08:46,200 --> 00:08:48,180
less than one percent of either prolific

227
00:08:48,180 --> 00:08:50,760
sample failed the text box attention

228
00:08:50,760 --> 00:08:53,339
check so given this we wondered what

229
00:08:53,339 --> 00:08:55,440
happens in a best case scenario so what

230
00:08:55,440 --> 00:08:58,080
happens if we restrict the results to

231
00:08:58,080 --> 00:08:59,700
those who are under 50 or with some

232
00:08:59,700 --> 00:09:01,620
college education which are known to be

233
00:09:01,620 --> 00:09:03,839
generalizable populations if you remove

234
00:09:03,839 --> 00:09:05,700
people who failed any attention check

235
00:09:05,700 --> 00:09:07,800
and reapply waiting

236
00:09:07,800 --> 00:09:10,320
and so this is what we have again once

237
00:09:10,320 --> 00:09:11,820
again Lower numbers are better because

238
00:09:11,820 --> 00:09:13,980
they mean these match closer to the Pew

239
00:09:13,980 --> 00:09:16,920
results and we see that the mturk data

240
00:09:16,920 --> 00:09:19,080
quality improves significantly right

241
00:09:19,080 --> 00:09:21,240
though it's still a bit worse than the

242
00:09:21,240 --> 00:09:24,180
best case prolific data also similar to

243
00:09:24,180 --> 00:09:26,339
findings from prior work the categories

244
00:09:26,339 --> 00:09:28,140
of questions are not similarly

245
00:09:28,140 --> 00:09:31,380
generalizable in particular behavior and

246
00:09:31,380 --> 00:09:33,180
knowledge do not seem very generalizable

247
00:09:33,180 --> 00:09:34,920
whereas experiences perceptions and

248
00:09:34,920 --> 00:09:36,060
beliefs are

249
00:09:36,060 --> 00:09:39,000
so in summary what did we learn overall

250
00:09:39,000 --> 00:09:40,560
it seems that both the prolific

251
00:09:40,560 --> 00:09:42,680
representative and gender balance sample

252
00:09:42,680 --> 00:09:46,019
are fairly similarly generalizable and

253
00:09:46,019 --> 00:09:47,519
so it'd be up to researchers to

254
00:09:47,519 --> 00:09:49,380
determine trade-offs between time and

255
00:09:49,380 --> 00:09:50,700
cost

256
00:09:50,700 --> 00:09:53,220
in terms of mterk the best case scenario

257
00:09:53,220 --> 00:09:56,760
does fairly decently and it is still so

258
00:09:56,760 --> 00:10:00,060
much better than a raw M Turk

259
00:10:00,060 --> 00:10:01,920
sample

260
00:10:01,920 --> 00:10:04,200
we find that for certain categories

261
00:10:04,200 --> 00:10:06,600
particularly experiences perceptions and

262
00:10:06,600 --> 00:10:08,700
beliefs online platforms do tend to

263
00:10:08,700 --> 00:10:10,860
generalize fairly well whereas for

264
00:10:10,860 --> 00:10:12,779
things like Behavior particularly social

265
00:10:12,779 --> 00:10:14,820
media use and knowledge they do not

266
00:10:14,820 --> 00:10:17,459
perhaps because people who use the plus

267
00:10:17,459 --> 00:10:19,800
platforms tend to be more online or more

268
00:10:19,800 --> 00:10:21,660
tech savvy

269
00:10:21,660 --> 00:10:24,420
if you believe that your sample might

270
00:10:24,420 --> 00:10:26,100
have a lot of bots we do suggest text

271
00:10:26,100 --> 00:10:27,779
box attention check questions over other

272
00:10:27,779 --> 00:10:29,820
types of attention checks it seems to

273
00:10:29,820 --> 00:10:33,540
have the best effect on data quality and

274
00:10:33,540 --> 00:10:35,399
in terms of waiting and raking we didn't

275
00:10:35,399 --> 00:10:37,980
find that parametric this had too much

276
00:10:37,980 --> 00:10:40,440
improvement in generalizability but

277
00:10:40,440 --> 00:10:42,420
perhaps that's because we have to decide

278
00:10:42,420 --> 00:10:43,920
on what categories to rake on and we

279
00:10:43,920 --> 00:10:45,060
didn't wait on things like text

280
00:10:45,060 --> 00:10:47,100
happiness Etc which might be important

281
00:10:47,100 --> 00:10:49,560
so overall researchers can definitely

282
00:10:49,560 --> 00:10:51,779
learn a lot of useful information from

283
00:10:51,779 --> 00:10:54,060
online privacy and security surveys but

284
00:10:54,060 --> 00:10:55,620
we have to be aware of the limitations

285
00:10:55,620 --> 00:10:57,720
whether it be from platforms populations

286
00:10:57,720 --> 00:10:59,579
or just the types of questions we ask

287
00:10:59,579 --> 00:11:02,540
thank you

