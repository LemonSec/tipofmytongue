1
00:00:10,700 --> 00:00:15,200
um hello everyone I'm really glad to be

2
00:00:13,520 --> 00:00:16,730
here I've been enjoying all the talks

3
00:00:15,200 --> 00:00:18,320
while I was still trying to tweak my

4
00:00:16,730 --> 00:00:21,080
presentations because I wasn't quite

5
00:00:18,320 --> 00:00:23,029
prepared but it was so engaging I just

6
00:00:21,080 --> 00:00:24,500
kept going to the talking and abandoning

7
00:00:23,029 --> 00:00:26,840
my talk so I'm not sure how well my talk

8
00:00:24,500 --> 00:00:28,400
will go but I wanted to actually start

9
00:00:26,840 --> 00:00:31,189
with how many of you in the room

10
00:00:28,400 --> 00:00:34,699
consider yourself a data scientist I do

11
00:00:31,190 --> 00:00:37,160
some data analytics of some sorts so as

12
00:00:34,699 --> 00:00:38,780
I thought not too many I think

13
00:00:37,160 --> 00:00:41,209
throughout the day privacy was a

14
00:00:38,780 --> 00:00:42,769
obviously a main theme much of it is

15
00:00:41,210 --> 00:00:44,720
about the fact that lots of data

16
00:00:42,769 --> 00:00:46,909
companies and maybe data scientists like

17
00:00:44,720 --> 00:00:48,379
myself that's why I was asking are

18
00:00:46,909 --> 00:00:49,970
grabbing your personal data without your

19
00:00:48,379 --> 00:00:51,949
permission and you're upset I totally

20
00:00:49,970 --> 00:00:54,080
agree I don't use a lot of smartphone

21
00:00:51,949 --> 00:00:58,190
for that reason myself so I would agree

22
00:00:54,080 --> 00:01:01,220
with that I am trying to present work as

23
00:00:58,190 --> 00:01:03,080
a data scientist and privacy as a

24
00:01:01,220 --> 00:01:05,539
definition for me is a little bit

25
00:01:03,080 --> 00:01:07,940
different than up to now up to now it

26
00:01:05,540 --> 00:01:10,040
was all about you know the users privacy

27
00:01:07,940 --> 00:01:12,110
that we want our respect I'm gonna turn

28
00:01:10,040 --> 00:01:14,630
it around and say as a data scientist I

29
00:01:12,110 --> 00:01:17,780
have to do my job that means I need that

30
00:01:14,630 --> 00:01:19,789
day I need to see that data I would like

31
00:01:17,780 --> 00:01:22,430
to respect your privacy and I am trying

32
00:01:19,789 --> 00:01:24,950
to do privacy research in a way that how

33
00:01:22,430 --> 00:01:26,750
do I do my job while still not

34
00:01:24,950 --> 00:01:28,640
guaranteeing anything but at least

35
00:01:26,750 --> 00:01:31,130
enhancing as much as possible the

36
00:01:28,640 --> 00:01:32,840
privacy during my tasks so it's a very

37
00:01:31,130 --> 00:01:35,000
different perspective on privacy and

38
00:01:32,840 --> 00:01:36,829
this particular research section if you

39
00:01:35,000 --> 00:01:38,630
were to put it into a box is a high

40
00:01:36,829 --> 00:01:40,729
research project with research design

41
00:01:38,630 --> 00:01:43,670
experimental design I'm not a car

42
00:01:40,729 --> 00:01:46,789
researcher my colleague Eric at U of F

43
00:01:43,670 --> 00:01:48,560
in the knee lab he's the he's the let's

44
00:01:46,789 --> 00:01:50,060
say the researcher behind this research

45
00:01:48,560 --> 00:01:53,210
I am a researcher I'm just the data

46
00:01:50,060 --> 00:01:55,069
science researcher and so I thought that

47
00:01:53,210 --> 00:01:57,318
background would help you so I'm gonna

48
00:01:55,069 --> 00:01:59,119
start by saying there are many good

49
00:01:57,319 --> 00:02:00,649
reasons to have legitimate access to

50
00:01:59,119 --> 00:02:02,749
what we call personally identifiable

51
00:02:00,649 --> 00:02:04,490
information for those of you who are

52
00:02:02,749 --> 00:02:07,068
data scientists in the room we all know

53
00:02:04,490 --> 00:02:09,859
that when you're doing data science data

54
00:02:07,069 --> 00:02:11,750
wrangling is you know up to 80% of my

55
00:02:09,860 --> 00:02:13,460
activity it's not the fancy algorithm

56
00:02:11,750 --> 00:02:15,230
whatever not it's the let me get the

57
00:02:13,460 --> 00:02:16,550
data cleaned up and standardized and

58
00:02:15,230 --> 00:02:18,798
figured out how to measure things so

59
00:02:16,550 --> 00:02:20,209
that I can actually use it and I have no

60
00:02:18,799 --> 00:02:21,920
idea how I'm gonna get there but I have

61
00:02:20,209 --> 00:02:23,700
junk somehow I'm gonna turn that into

62
00:02:21,920 --> 00:02:26,609
information and some I'm gonna do some

63
00:02:23,700 --> 00:02:28,260
and in that process please don't ask me

64
00:02:26,610 --> 00:02:30,630
to close my eyes because I'm not sure

65
00:02:28,260 --> 00:02:32,069
how I would do that so even if you're

66
00:02:30,630 --> 00:02:34,290
doing the more fancy machine learning

67
00:02:32,069 --> 00:02:36,359
stuff how do you do that well even if

68
00:02:34,290 --> 00:02:38,130
you're automating it I have to tune my

69
00:02:36,360 --> 00:02:39,810
parameters most of you are computer

70
00:02:38,130 --> 00:02:41,310
science trained you know about the fact

71
00:02:39,810 --> 00:02:42,510
that you can have fancy algorithms but

72
00:02:41,310 --> 00:02:42,959
if the parameters aren't tuned they're

73
00:02:42,510 --> 00:02:45,000
useless

74
00:02:42,959 --> 00:02:47,850
right I have to build trained our

75
00:02:45,000 --> 00:02:49,290
training data sets that come from the

76
00:02:47,850 --> 00:02:52,470
real data so there are very good reasons

77
00:02:49,290 --> 00:02:54,630
why I need access to that PII that

78
00:02:52,470 --> 00:02:56,220
everybody is concerned about and yes

79
00:02:54,630 --> 00:02:58,440
that's a problem when it's really

80
00:02:56,220 --> 00:03:00,359
sensitive information like HIV status

81
00:02:58,440 --> 00:03:01,859
you know along with your names and

82
00:03:00,360 --> 00:03:05,700
Social Security numbers nobody wants me

83
00:03:01,860 --> 00:03:09,390
to have it but I would like to suggest

84
00:03:05,700 --> 00:03:11,488
that personally there are not legitimate

85
00:03:09,390 --> 00:03:13,010
access without explicit permission much

86
00:03:11,489 --> 00:03:15,600
of what was discussed at this conference

87
00:03:13,010 --> 00:03:17,399
versus legitimate access without

88
00:03:15,600 --> 00:03:19,350
informed consent when you're talking

89
00:03:17,400 --> 00:03:22,950
about the public good like doing

90
00:03:19,350 --> 00:03:25,049
research on cancer in HIV and informed

91
00:03:22,950 --> 00:03:27,149
consent has other issues but one of the

92
00:03:25,050 --> 00:03:30,390
main wishes issues related to research

93
00:03:27,150 --> 00:03:31,739
is that it results in biased data so

94
00:03:30,390 --> 00:03:34,140
when I'm doing and trying to figure out

95
00:03:31,739 --> 00:03:35,459
clinical care for someone I'm really

96
00:03:34,140 --> 00:03:36,899
really sorry but I don't want to do

97
00:03:35,459 --> 00:03:38,640
informed consent on a database that I

98
00:03:36,900 --> 00:03:41,549
have right I don't want the bias in

99
00:03:38,640 --> 00:03:42,929
there so I'm not suggesting that we

100
00:03:41,549 --> 00:03:44,190
should all just let everybody do

101
00:03:42,930 --> 00:03:46,079
whatever they want with that data I'm

102
00:03:44,190 --> 00:03:48,600
just suggesting please entertain the

103
00:03:46,079 --> 00:03:50,010
idea there are very good valid reasons

104
00:03:48,600 --> 00:03:53,010
to have legitimate access without

105
00:03:50,010 --> 00:03:55,078
informed consent and that is exactly why

106
00:03:53,010 --> 00:03:57,298
I am interested in providing and

107
00:03:55,079 --> 00:03:59,250
designing privacy in science systems so

108
00:03:57,299 --> 00:03:59,850
I can do these activities that's that's

109
00:03:59,250 --> 00:04:01,950
the idea

110
00:03:59,850 --> 00:04:03,600
there have been partial solutions people

111
00:04:01,950 --> 00:04:06,060
have talked about privacy preserving

112
00:04:03,600 --> 00:04:08,100
data analytics of different kind of

113
00:04:06,060 --> 00:04:10,079
algorithms and their encryption base

114
00:04:08,100 --> 00:04:11,370
they restrict access they say you know

115
00:04:10,079 --> 00:04:13,410
humans don't need to be involved

116
00:04:11,370 --> 00:04:15,959
algorithms can figure it all out so why

117
00:04:13,410 --> 00:04:17,728
do you need to see the data there's a

118
00:04:15,959 --> 00:04:19,589
lot of people a lot of researches on

119
00:04:17,728 --> 00:04:21,988
synthetic data you know let me give you

120
00:04:19,589 --> 00:04:23,669
a data set that's not real about me well

121
00:04:21,988 --> 00:04:25,859
guess what data science is all about

122
00:04:23,669 --> 00:04:27,539
it's about the fact that you didn't know

123
00:04:25,860 --> 00:04:29,910
the signal when you went in and you

124
00:04:27,539 --> 00:04:31,560
found it building a synthetic day that

125
00:04:29,910 --> 00:04:32,789
means you are preserving signals that

126
00:04:31,560 --> 00:04:34,800
you already know is in there that's

127
00:04:32,789 --> 00:04:37,110
called training and teaching in the

128
00:04:34,800 --> 00:04:38,010
classroom that's not called research so

129
00:04:37,110 --> 00:04:39,990
all that research about

130
00:04:38,010 --> 00:04:42,030
privacy-preserving using synthetic data

131
00:04:39,990 --> 00:04:43,620
to me is it this is good research it

132
00:04:42,030 --> 00:04:45,150
should be done this is what you should

133
00:04:43,620 --> 00:04:46,860
do when you're doing training but it's

134
00:04:45,150 --> 00:04:48,479
not real data science research in the

135
00:04:46,860 --> 00:04:50,190
health realm by the way I'm a tenured

136
00:04:48,480 --> 00:04:52,050
faculty in public health with a PhD in

137
00:04:50,190 --> 00:04:53,790
computer science and data mining that's

138
00:04:52,050 --> 00:04:56,190
where the background of all this is so

139
00:04:53,790 --> 00:04:57,870
the truth of the matter is I'm really

140
00:04:56,190 --> 00:05:00,350
really sorry but I just don't know how

141
00:04:57,870 --> 00:05:02,940
to do magic when every single day

142
00:05:00,350 --> 00:05:04,650
whatever I had access to last year I

143
00:05:02,940 --> 00:05:06,840
don't have and it's really really

144
00:05:04,650 --> 00:05:08,130
becoming a problem for me so I did this

145
00:05:06,840 --> 00:05:10,260
I've been doing this for about 20 years

146
00:05:08,130 --> 00:05:12,780
when I first started out nobody cared

147
00:05:10,260 --> 00:05:14,700
about security and privacy they gave me

148
00:05:12,780 --> 00:05:17,340
everything like fifty thousand social

149
00:05:14,700 --> 00:05:19,409
security numbers go do and I did every

150
00:05:17,340 --> 00:05:20,820
year I keep losing things I'm like are

151
00:05:19,410 --> 00:05:22,530
you sure you don't want me to use this

152
00:05:20,820 --> 00:05:24,090
because my results could be a lot better

153
00:05:22,530 --> 00:05:26,010
if I had it but they keep taking it away

154
00:05:24,090 --> 00:05:28,950
and so that's just something to think

155
00:05:26,010 --> 00:05:30,960
about and the thing the idea here is

156
00:05:28,950 --> 00:05:33,030
here's another bit I think this part

157
00:05:30,960 --> 00:05:35,430
lots of people here know this is the

158
00:05:33,030 --> 00:05:37,320
background theoretical literature on the

159
00:05:35,430 --> 00:05:38,790
most cutting-edge privacy you know

160
00:05:37,320 --> 00:05:40,950
differential privacy everybody's heard

161
00:05:38,790 --> 00:05:42,780
of it well what they say is so that's

162
00:05:40,950 --> 00:05:45,539
the theory part of this what they say is

163
00:05:42,780 --> 00:05:47,669
I'm sorry if sorry sorry no free lunch

164
00:05:45,540 --> 00:05:50,700
you cannot get utility from the data set

165
00:05:47,670 --> 00:05:53,010
without some risk in privacy there is no

166
00:05:50,700 --> 00:05:55,440
one-size-fits-all and there is no way to

167
00:05:53,010 --> 00:05:57,960
benefit from data when you don't take

168
00:05:55,440 --> 00:05:59,400
some privacy risks so the idea is would

169
00:05:57,960 --> 00:06:00,870
you like me to run a clinical trial to

170
00:05:59,400 --> 00:06:03,239
figure out how to treat you better or

171
00:06:00,870 --> 00:06:04,500
can I just use your data or maybe we

172
00:06:03,240 --> 00:06:06,810
don't need to know anything I don't know

173
00:06:04,500 --> 00:06:08,070
what do we have right so that sort of

174
00:06:06,810 --> 00:06:10,470
the things that you have to think about

175
00:06:08,070 --> 00:06:13,050
so that's that balancing but we have

176
00:06:10,470 --> 00:06:14,520
hope or I think we have hope so the four

177
00:06:13,050 --> 00:06:17,340
things that I think about when I'm doing

178
00:06:14,520 --> 00:06:20,640
privacy by design the first is so

179
00:06:17,340 --> 00:06:22,200
exactly how much data do I need to

180
00:06:20,640 --> 00:06:25,200
really look at when I'm in that garbage

181
00:06:22,200 --> 00:06:26,909
dump really really really not very much

182
00:06:25,200 --> 00:06:28,229
right if I'm talking about my ten

183
00:06:26,910 --> 00:06:29,880
thousand hundred thousand twenty

184
00:06:28,230 --> 00:06:32,190
thousand data set I am NOT looking at

185
00:06:29,880 --> 00:06:35,430
every single row every single cell I am

186
00:06:32,190 --> 00:06:37,020
looking at a thousand of them maybe two

187
00:06:35,430 --> 00:06:39,810
thousand of them maybe percentage-wise

188
00:06:37,020 --> 00:06:42,060
very very very very little right so if

189
00:06:39,810 --> 00:06:44,790
you were to take a step back and you say

190
00:06:42,060 --> 00:06:46,230
well how about we have an agreement you

191
00:06:44,790 --> 00:06:48,810
let me go in there and look at whatever

192
00:06:46,230 --> 00:06:51,870
cells I want to look at within the

193
00:06:48,810 --> 00:06:53,370
1% then I can be of happy data scientist

194
00:06:51,870 --> 00:06:56,370
because that can I can usually do my job

195
00:06:53,370 --> 00:06:58,020
the problem is nobody thought talks like

196
00:06:56,370 --> 00:07:00,270
that nobody things like that nobody give

197
00:06:58,020 --> 00:07:02,190
me your throat like that so that's just

198
00:07:00,270 --> 00:07:04,620
one if you have that inside as a data

199
00:07:02,190 --> 00:07:06,030
scientist I feel like we can design you

200
00:07:04,620 --> 00:07:08,430
know very good privacy enhance the

201
00:07:06,030 --> 00:07:10,409
systems to do really get benefit out the

202
00:07:08,430 --> 00:07:12,419
data that's one and then the other

203
00:07:10,410 --> 00:07:13,590
things are what really matters so who

204
00:07:12,419 --> 00:07:16,169
knows where to look

205
00:07:13,590 --> 00:07:18,419
is it the let's say the algorithm

206
00:07:16,169 --> 00:07:19,650
developer software developer security

207
00:07:18,419 --> 00:07:21,090
scientists who's going to give me an

208
00:07:19,650 --> 00:07:22,950
algorithm that I'm gonna do my magic

209
00:07:21,090 --> 00:07:27,179
with my eyes closed I don't think so

210
00:07:22,950 --> 00:07:28,830
right who when do you know what you need

211
00:07:27,180 --> 00:07:30,660
to look at so you ask me I'm a data

212
00:07:28,830 --> 00:07:32,310
scientist can I tell you when I'm

213
00:07:30,660 --> 00:07:33,870
writing my research plan and submitting

214
00:07:32,310 --> 00:07:35,820
it to the IRB exactly what I'm gonna

215
00:07:33,870 --> 00:07:37,590
look at I am sorry sorry sorry no I

216
00:07:35,820 --> 00:07:39,389
can't I have to be in the data to figure

217
00:07:37,590 --> 00:07:40,799
out what I have to look at so I can't

218
00:07:39,389 --> 00:07:42,570
tell you ahead of time that's the

219
00:07:40,800 --> 00:07:44,100
problem because I can't tell you at

220
00:07:42,570 --> 00:07:46,469
ahead of time I'm like all the data

221
00:07:44,100 --> 00:07:48,419
scientists give me anything you got

222
00:07:46,470 --> 00:07:50,490
anything I can get my hands on and yes

223
00:07:48,419 --> 00:07:52,859
I'm sorry it's HIV data that's the

224
00:07:50,490 --> 00:07:55,530
problem so keep that in mind and then

225
00:07:52,860 --> 00:07:57,600
finally while I'm in there looking at

226
00:07:55,530 --> 00:07:58,859
your HIV status if the patient comes

227
00:07:57,600 --> 00:08:01,260
along and goes what are you doing in my

228
00:07:58,860 --> 00:08:02,550
data I can look up and I can look him in

229
00:08:01,260 --> 00:08:03,900
the eye and I can tell you what I'm

230
00:08:02,550 --> 00:08:05,490
doing in there and why I'm in there

231
00:08:03,900 --> 00:08:07,830
that's the important part

232
00:08:05,490 --> 00:08:09,810
that's the privacy part you have to

233
00:08:07,830 --> 00:08:11,250
think about the purpose and you have to

234
00:08:09,810 --> 00:08:13,710
think about is that legitimate and are

235
00:08:11,250 --> 00:08:15,600
you doing okay in there if you keep

236
00:08:13,710 --> 00:08:17,310
these things in mind I feel like we have

237
00:08:15,600 --> 00:08:19,770
hope in designing a privacy enhance

238
00:08:17,310 --> 00:08:21,630
system that we can actually do and get

239
00:08:19,770 --> 00:08:25,500
benefit from data while still protecting

240
00:08:21,630 --> 00:08:28,139
a lot of the privacy so the idea here is

241
00:08:25,500 --> 00:08:30,750
I worked with a chi researcher to say

242
00:08:28,139 --> 00:08:32,789
what our key design elements that you

243
00:08:30,750 --> 00:08:35,640
would build into a such a system and

244
00:08:32,789 --> 00:08:37,289
here are four things I put in here first

245
00:08:35,640 --> 00:08:39,569
of all we have to really move away from

246
00:08:37,289 --> 00:08:41,250
all or nothing so I told you about the

247
00:08:39,570 --> 00:08:42,719
stuff I do move most people when they

248
00:08:41,250 --> 00:08:43,860
first hear about this they just look at

249
00:08:42,719 --> 00:08:45,690
me funny because they've never heard of

250
00:08:43,860 --> 00:08:47,070
this access to also Social Security

251
00:08:45,690 --> 00:08:48,630
numbers for everybody in your state well

252
00:08:47,070 --> 00:08:50,839
if you have a state contract and that's

253
00:08:48,630 --> 00:08:53,550
your job that's what you get right so

254
00:08:50,839 --> 00:08:55,170
what happens is either you're one of the

255
00:08:53,550 --> 00:08:57,479
privileged club members and you get

256
00:08:55,170 --> 00:08:59,640
access to everything nobody asks what

257
00:08:57,480 --> 00:09:00,990
you're doing in there or you don't get

258
00:08:59,640 --> 00:09:01,830
access to anything and then it's a

259
00:09:00,990 --> 00:09:03,690
nightmare

260
00:09:01,830 --> 00:09:05,790
so we have to really move away from the

261
00:09:03,690 --> 00:09:09,269
all-or-nothing and find ways to do

262
00:09:05,790 --> 00:09:11,219
partial access second you can't ask me

263
00:09:09,269 --> 00:09:12,990
what I need at the beginning you have to

264
00:09:11,220 --> 00:09:15,089
let me go in to figure it out so it's

265
00:09:12,990 --> 00:09:17,010
this is about just-in-time access and

266
00:09:15,089 --> 00:09:20,490
decisions about what I'm gonna look at I

267
00:09:17,010 --> 00:09:22,529
think I've shared just in time is a key

268
00:09:20,490 --> 00:09:24,810
characteristics of what gives benefits

269
00:09:22,529 --> 00:09:27,180
to computer science technology if you

270
00:09:24,810 --> 00:09:28,859
can delay decisions until you know you

271
00:09:27,180 --> 00:09:31,560
can make a lot better decision and this

272
00:09:28,860 --> 00:09:34,500
is about just-in-time decision to look

273
00:09:31,560 --> 00:09:36,209
at what I'm looking at right the second

274
00:09:34,500 --> 00:09:38,130
the third part it's about monitoring

275
00:09:36,209 --> 00:09:40,500
access it's not about guaranteeing or

276
00:09:38,130 --> 00:09:42,209
limiting access it's about you recording

277
00:09:40,500 --> 00:09:43,680
every single thing I'm doing and the

278
00:09:42,209 --> 00:09:45,510
ability to come ask what were you doing

279
00:09:43,680 --> 00:09:47,489
in there with that cell right so it's

280
00:09:45,510 --> 00:09:49,260
about monitoring access which is much

281
00:09:47,490 --> 00:09:51,510
like the surveillance cameras in our

282
00:09:49,260 --> 00:09:53,220
physical security systems which

283
00:09:51,510 --> 00:09:54,779
depending on who you talk to may be the

284
00:09:53,220 --> 00:09:57,060
most important part of a security system

285
00:09:54,779 --> 00:09:58,500
is it those you know five time-lapse

286
00:09:57,060 --> 00:09:59,640
with a biosensor or is it the

287
00:09:58,500 --> 00:10:01,890
surveillance camera that keeps the

288
00:09:59,640 --> 00:10:03,930
people out what's discouraging people

289
00:10:01,890 --> 00:10:05,430
from doing bad things in places you

290
00:10:03,930 --> 00:10:07,109
shouldn't be doing I think it's the

291
00:10:05,430 --> 00:10:08,550
surveillance camera that has nothing to

292
00:10:07,110 --> 00:10:10,770
do with the encryption it's actually in

293
00:10:08,550 --> 00:10:15,750
this computer system a lot more about

294
00:10:10,770 --> 00:10:17,579
privacy by design so that was the I'm

295
00:10:15,750 --> 00:10:19,800
sorry my background is a little big but

296
00:10:17,579 --> 00:10:21,420
the actual problem I'm working on for

297
00:10:19,800 --> 00:10:22,890
this particular my first attempt at

298
00:10:21,420 --> 00:10:25,050
trying to design a privacy enhance

299
00:10:22,890 --> 00:10:27,240
system was in the problem of record

300
00:10:25,050 --> 00:10:28,920
linkage it's very simple I have two data

301
00:10:27,240 --> 00:10:31,770
which states that's I would like to know

302
00:10:28,920 --> 00:10:33,719
which row is which because guess what my

303
00:10:31,770 --> 00:10:36,060
job is to count how many visits

304
00:10:33,720 --> 00:10:38,850
everybody had in the city why because

305
00:10:36,060 --> 00:10:40,380
your tax dollars rely on this when you

306
00:10:38,850 --> 00:10:41,880
do reimbursements on Medicaid the

307
00:10:40,380 --> 00:10:43,529
hospitals get paid a different rate

308
00:10:41,880 --> 00:10:45,779
depending on how many times you have

309
00:10:43,529 --> 00:10:47,189
revisits if you have good data and you

310
00:10:45,779 --> 00:10:48,839
can put them all together you can have

311
00:10:47,190 --> 00:10:50,820
accurate revisit rates and that's what

312
00:10:48,839 --> 00:10:53,130
that would determine what gets who get

313
00:10:50,820 --> 00:10:54,779
what hospitals get paid versus if you

314
00:10:53,130 --> 00:10:56,250
have bad data I'm sorry I can't see

315
00:10:54,779 --> 00:10:58,380
anything I can tell you how many people

316
00:10:56,250 --> 00:10:59,940
came back to my hospital I have no idea

317
00:10:58,380 --> 00:11:02,130
if they actually went to another ad that

318
00:10:59,940 --> 00:11:04,320
is the current state that's a very sad

319
00:11:02,130 --> 00:11:05,970
state given what we can do with data but

320
00:11:04,320 --> 00:11:07,500
that's why I'm working on record linkit

321
00:11:05,970 --> 00:11:09,630
and that is why I am working on that

322
00:11:07,500 --> 00:11:10,860
with the most sensitive data I'm working

323
00:11:09,630 --> 00:11:13,079
with your social security numbers and

324
00:11:10,860 --> 00:11:15,080
names that's the problem I'm working on

325
00:11:13,079 --> 00:11:17,030
and I have two goals right my

326
00:11:15,080 --> 00:11:18,740
privacy goal is to limit disclosure of

327
00:11:17,030 --> 00:11:21,890
your names and Social Security numbers

328
00:11:18,740 --> 00:11:24,770
as much as possible and my utility goal

329
00:11:21,890 --> 00:11:27,199
but without reducing my ability to do a

330
00:11:24,770 --> 00:11:31,040
job it's the humans effectiveness in

331
00:11:27,200 --> 00:11:33,200
doing your data science job so here's

332
00:11:31,040 --> 00:11:35,510
the real question can I find the sweet

333
00:11:33,200 --> 00:11:37,670
spot between accessing PII for

334
00:11:35,510 --> 00:11:41,300
legitimate use while also providing

335
00:11:37,670 --> 00:11:43,670
maximum protection for privacy by using

336
00:11:41,300 --> 00:11:46,280
a privacy by design approach and the

337
00:11:43,670 --> 00:11:47,719
spoiler is yes I've at least in this

338
00:11:46,280 --> 00:11:49,790
case we've done some studies and

339
00:11:47,720 --> 00:11:52,490
evaluations and we can do it so if you

340
00:11:49,790 --> 00:11:54,290
look at the three bars that at the very

341
00:11:52,490 --> 00:11:56,180
end is full access this is the current

342
00:11:54,290 --> 00:11:57,439
status code you get you have you go

343
00:11:56,180 --> 00:11:58,750
through a lot of Hoops but you get in

344
00:11:57,440 --> 00:12:00,680
the club and you get access to every

345
00:11:58,750 --> 00:12:03,350
everything and nobody cares what you're

346
00:12:00,680 --> 00:12:04,819
doing in there that's considered the

347
00:12:03,350 --> 00:12:07,490
current status quo very little

348
00:12:04,820 --> 00:12:08,990
protection the second one is a static

349
00:12:07,490 --> 00:12:11,180
design this is work I presented last

350
00:12:08,990 --> 00:12:12,980
year is that based on a static design

351
00:12:11,180 --> 00:12:16,099
and what we can show is that we can do

352
00:12:12,980 --> 00:12:18,140
the same work with 30% of disclosure so

353
00:12:16,100 --> 00:12:18,530
that's a lot of protection this year I'm

354
00:12:18,140 --> 00:12:20,210
impressed

355
00:12:18,530 --> 00:12:22,370
I took all the static stuff that I did

356
00:12:20,210 --> 00:12:24,560
and put it together to do a dynamic

357
00:12:22,370 --> 00:12:27,500
design and now I'm down to seven point

358
00:12:24,560 --> 00:12:29,599
eight percent so by looking at only

359
00:12:27,500 --> 00:12:31,700
seven point eight percent of the data I

360
00:12:29,600 --> 00:12:33,500
can make the same quality data science

361
00:12:31,700 --> 00:12:36,500
decisions as I can with a hundred

362
00:12:33,500 --> 00:12:38,330
percent of results so this means I can

363
00:12:36,500 --> 00:12:39,860
maintain my utility with a lot more

364
00:12:38,330 --> 00:12:43,580
protection and this is what I'm trying

365
00:12:39,860 --> 00:12:44,870
to present look at my time because I

366
00:12:43,580 --> 00:12:48,020
didn't think seventeen minutes was gonna

367
00:12:44,870 --> 00:12:49,610
be very much so basically you can find

368
00:12:48,020 --> 00:12:52,280
much much those are the results in our

369
00:12:49,610 --> 00:12:53,990
paper but what we did was we actually

370
00:12:52,280 --> 00:12:56,209
studied different ways of putting this

371
00:12:53,990 --> 00:12:58,070
together and this is this is the

372
00:12:56,210 --> 00:13:00,590
interface I wanted to show you so this

373
00:12:58,070 --> 00:13:02,120
is you have two rows of identifying

374
00:13:00,590 --> 00:13:04,310
information and your job is to figure

375
00:13:02,120 --> 00:13:06,470
out if this is the same person or not

376
00:13:04,310 --> 00:13:09,319
so that's section one is the minimum

377
00:13:06,470 --> 00:13:11,720
disclosure if you see in red the second

378
00:13:09,320 --> 00:13:13,250
part is the actual accountability by

379
00:13:11,720 --> 00:13:15,200
quantifying the risk so there's a meter

380
00:13:13,250 --> 00:13:16,700
that goes on top that tells you how much

381
00:13:15,200 --> 00:13:18,650
has been seen which is measuring the

382
00:13:16,700 --> 00:13:20,630
risk that's involved and then the third

383
00:13:18,650 --> 00:13:23,060
one is that red line that says no more

384
00:13:20,630 --> 00:13:24,950
than this you're forcing that so what we

385
00:13:23,060 --> 00:13:26,569
did was we actually did this so this is

386
00:13:24,950 --> 00:13:27,480
the most important part which is the on

387
00:13:26,570 --> 00:13:29,820
demand

388
00:13:27,480 --> 00:13:32,250
in time disclosure interface and this is

389
00:13:29,820 --> 00:13:34,649
how it works so I start with nothing

390
00:13:32,250 --> 00:13:36,959
displayed I've got some metadata that

391
00:13:34,649 --> 00:13:38,550
helped me think about this if I feel

392
00:13:36,959 --> 00:13:39,810
like I need more information you would

393
00:13:38,550 --> 00:13:42,870
click and you would get more information

394
00:13:39,810 --> 00:13:44,250
so in this case J jr. was opened if I

395
00:13:42,870 --> 00:13:45,779
still need more information you would

396
00:13:44,250 --> 00:13:47,820
click and you would see more where you

397
00:13:45,779 --> 00:13:49,649
get to now full disclosure so this is

398
00:13:47,820 --> 00:13:51,660
the process of a dynamics interface

399
00:13:49,649 --> 00:13:53,639
where you click to see more during the

400
00:13:51,660 --> 00:13:55,740
job when I need to see it that's what

401
00:13:53,639 --> 00:13:58,320
you're doing right so again status quo

402
00:13:55,740 --> 00:14:00,120
is everything my approach is here is

403
00:13:58,320 --> 00:14:01,680
there's nothing you can click to see the

404
00:14:00,120 --> 00:14:04,320
different things and you can make your

405
00:14:01,680 --> 00:14:06,388
decision and then based on what I've

406
00:14:04,320 --> 00:14:09,630
looked at you're gonna measure the risk

407
00:14:06,389 --> 00:14:11,190
what is that orange that orange is when

408
00:14:09,630 --> 00:14:12,899
I hover over so based on what I've

409
00:14:11,190 --> 00:14:13,980
looked at that's the risk blue is what

410
00:14:12,899 --> 00:14:16,380
I've seen so far

411
00:14:13,980 --> 00:14:18,149
orange occurs when I'm actually mousing

412
00:14:16,380 --> 00:14:20,519
over this it tells you that if I click

413
00:14:18,149 --> 00:14:22,350
I'm gonna see that much more I get to

414
00:14:20,519 --> 00:14:23,970
decide after I look at this whether I

415
00:14:22,350 --> 00:14:25,470
want to click or not if it's worth it

416
00:14:23,970 --> 00:14:27,779
I'm gonna click if it's not I'm not

417
00:14:25,470 --> 00:14:29,459
gonna click and then on top of that so

418
00:14:27,779 --> 00:14:30,689
here there's a little math in terms of

419
00:14:29,459 --> 00:14:32,910
how you do the measurement but I'm gonna

420
00:14:30,690 --> 00:14:34,740
not talk about that detail right so then

421
00:14:32,910 --> 00:14:36,600
finally there's that red line and so I

422
00:14:34,740 --> 00:14:38,339
can make a decision during my job

423
00:14:36,600 --> 00:14:40,050
whether I'm gonna click open or not and

424
00:14:38,339 --> 00:14:41,850
I'm being told that I can only go up to

425
00:14:40,050 --> 00:14:43,979
red so that's that's that's the last

426
00:14:41,850 --> 00:14:45,690
part of that interface so we did a we

427
00:14:43,980 --> 00:14:47,760
had three hypotheses to look at the

428
00:14:45,690 --> 00:14:50,610
effectiveness of each of the design we

429
00:14:47,760 --> 00:14:52,829
did a user study and they're gonna skip

430
00:14:50,610 --> 00:14:53,699
over all of these where we tested our

431
00:14:52,829 --> 00:14:56,069
hypothesis

432
00:14:53,699 --> 00:14:58,469
so the main results is this what we

433
00:14:56,069 --> 00:15:01,319
found and this is what I shared so the

434
00:14:58,470 --> 00:15:03,569
first two design is a hundred 27.8 it

435
00:15:01,319 --> 00:15:05,189
went down that much and the the other

436
00:15:03,569 --> 00:15:06,899
graph that says no difference that's the

437
00:15:05,190 --> 00:15:08,010
quality of our decision so you can see

438
00:15:06,899 --> 00:15:09,120
that the quality of the decision is

439
00:15:08,010 --> 00:15:11,730
pretty much identical

440
00:15:09,120 --> 00:15:13,889
whereas what what I've looked at is you

441
00:15:11,730 --> 00:15:15,709
when you measure it is quite low so this

442
00:15:13,889 --> 00:15:17,850
this L she tells you that on demand if

443
00:15:15,709 --> 00:15:19,380
interface is very effective in giving

444
00:15:17,850 --> 00:15:22,139
you the privacy enhancement without

445
00:15:19,380 --> 00:15:23,639
affecting the utility the other result

446
00:15:22,139 --> 00:15:25,380
that I'm going to highlight is the fact

447
00:15:23,639 --> 00:15:28,440
that actually if you go too low if that

448
00:15:25,380 --> 00:15:30,149
budget is too low then yes my quality of

449
00:15:28,440 --> 00:15:32,010
results so it's going from it went down

450
00:15:30,149 --> 00:15:34,260
to nineteen point five percent so the

451
00:15:32,010 --> 00:15:35,640
quality of my results just suffered

452
00:15:34,260 --> 00:15:37,410
because I didn't have enough budget to

453
00:15:35,640 --> 00:15:39,089
work with and that's what you have to

454
00:15:37,410 --> 00:15:41,300
remember is that's why there's no free

455
00:15:39,089 --> 00:15:42,680
lunch so when you're doing a system

456
00:15:41,300 --> 00:15:46,010
this you need to have a budget that is

457
00:15:42,680 --> 00:15:48,349
sufficiently big so I can do my job we

458
00:15:46,010 --> 00:15:49,939
also measured completion time which for

459
00:15:48,350 --> 00:15:51,950
all of these different conditions we

460
00:15:49,940 --> 00:15:53,990
didn't find any lot of differences and

461
00:15:51,950 --> 00:15:56,540
then we also had an expert study so we

462
00:15:53,990 --> 00:15:58,190
actually surveyed experts who do record

463
00:15:56,540 --> 00:15:59,630
linkage with full access most of the

464
00:15:58,190 --> 00:16:01,850
time and they answered some questions

465
00:15:59,630 --> 00:16:03,740
they use our interface which you know

466
00:16:01,850 --> 00:16:05,210
here's a quote once I got used to the

467
00:16:03,740 --> 00:16:07,340
coding allowing partial disclosure

468
00:16:05,210 --> 00:16:09,020
helped in decision making what you're

469
00:16:07,340 --> 00:16:11,360
seeing here is evidence that actually

470
00:16:09,020 --> 00:16:13,640
when I'm trying to help with privacy it

471
00:16:11,360 --> 00:16:15,020
also helps with the decision why because

472
00:16:13,640 --> 00:16:16,610
you're not getting raw data you're

473
00:16:15,020 --> 00:16:18,650
getting actually processed data and

474
00:16:16,610 --> 00:16:19,220
information that is good to support your

475
00:16:18,650 --> 00:16:20,660
decision

476
00:16:19,220 --> 00:16:24,020
so that actually improves the quality of

477
00:16:20,660 --> 00:16:25,850
the decision we had one expert um sorry

478
00:16:24,020 --> 00:16:27,650
we had one expert who actually had

479
00:16:25,850 --> 00:16:29,930
experience using that encryption method

480
00:16:27,650 --> 00:16:32,000
and he had said I never know how well

481
00:16:29,930 --> 00:16:34,250
the hashing worked or how accurate it is

482
00:16:32,000 --> 00:16:36,170
it would be helpful to use this method

483
00:16:34,250 --> 00:16:37,280
to spot-check a random sample to know

484
00:16:36,170 --> 00:16:40,300
that you're hashing methods are working

485
00:16:37,280 --> 00:16:42,890
so here are here's the results and then

486
00:16:40,300 --> 00:16:45,020
some closing thoughts people talk about

487
00:16:42,890 --> 00:16:46,819
threat models in this research what I

488
00:16:45,020 --> 00:16:49,160
would like to say is this privacy

489
00:16:46,820 --> 00:16:51,890
enhanced system it's a threat model of

490
00:16:49,160 --> 00:16:54,110
an insider threat so yes I have approval

491
00:16:51,890 --> 00:16:55,550
I have approval for this purpose if I'm

492
00:16:54,110 --> 00:16:58,130
in there doing other things that's a

493
00:16:55,550 --> 00:16:59,540
problem right so what these systems do

494
00:16:58,130 --> 00:17:01,820
because it's a surveillance camera it

495
00:16:59,540 --> 00:17:03,890
checks what I'm doing it checks what the

496
00:17:01,820 --> 00:17:06,080
data science is doing so these systems

497
00:17:03,890 --> 00:17:11,180
help with protecting against insider

498
00:17:06,079 --> 00:17:13,449
threats and this is the team so I'm a

499
00:17:11,180 --> 00:17:16,520
data scientist Erik is a chi researcher

500
00:17:13,450 --> 00:17:19,459
Kayson is by the way a lawyer so what

501
00:17:16,520 --> 00:17:21,859
we've shown is that this meets this way

502
00:17:19,459 --> 00:17:23,990
of doing partial disclosure just in time

503
00:17:21,859 --> 00:17:26,060
it actually meets the legal definition

504
00:17:23,990 --> 00:17:29,240
of minimum disclosure those are

505
00:17:26,060 --> 00:17:30,889
requirements in gdpr and HIPAA none of

506
00:17:29,240 --> 00:17:33,530
them say no access to data because you

507
00:17:30,890 --> 00:17:35,690
can't do that all of the laws say give

508
00:17:33,530 --> 00:17:37,310
the minimum necessary except nobody

509
00:17:35,690 --> 00:17:39,200
knows what that is and there are no

510
00:17:37,310 --> 00:17:41,600
computers that enforce minimum necessary

511
00:17:39,200 --> 00:17:43,040
and so the the minimum necessary with

512
00:17:41,600 --> 00:17:45,050
the current computer systems is

513
00:17:43,040 --> 00:17:46,730
everything and that is the problem so if

514
00:17:45,050 --> 00:17:48,950
you have chi researchers who build

515
00:17:46,730 --> 00:17:50,930
systems that can actually sort of help

516
00:17:48,950 --> 00:17:53,300
implement the minimum necessary legal

517
00:17:50,930 --> 00:17:54,450
standard then you can get a lot further

518
00:17:53,300 --> 00:17:56,370
in terms of getting

519
00:17:54,450 --> 00:18:05,370
from data while still enhancing privacy

520
00:17:56,370 --> 00:18:13,290
thank you any any questions from the

521
00:18:05,370 --> 00:18:16,110
audience okay I have a few questions so

522
00:18:13,290 --> 00:18:18,000
I was just wondering if you could talk a

523
00:18:16,110 --> 00:18:20,219
little bit about the decision making of

524
00:18:18,000 --> 00:18:24,060
the end user part of your paper so like

525
00:18:20,220 --> 00:18:26,190
if I how do I if I'm a user and I want

526
00:18:24,060 --> 00:18:27,810
to see more what's stopping me from not

527
00:18:26,190 --> 00:18:28,710
wanting to just always click and see

528
00:18:27,810 --> 00:18:30,840
more

529
00:18:28,710 --> 00:18:32,880
I know like you can design the study to

530
00:18:30,840 --> 00:18:34,800
like say like you only stop once you

531
00:18:32,880 --> 00:18:37,620
reach this point but like it's putting a

532
00:18:34,800 --> 00:18:40,169
lot burden on the researcher to hold

533
00:18:37,620 --> 00:18:42,179
back so like do you have any thoughts so

534
00:18:40,170 --> 00:18:44,340
the thing is when you're doing this

535
00:18:42,180 --> 00:18:46,800
record linkage task I would say the most

536
00:18:44,340 --> 00:18:48,240
difficult pairs this is usually the way

537
00:18:46,800 --> 00:18:49,950
this works is you run lots of algorithms

538
00:18:48,240 --> 00:18:51,450
and you're not looking at 10,000 right

539
00:18:49,950 --> 00:18:53,370
the algorithms take care of all the easy

540
00:18:51,450 --> 00:18:54,600
ones so the ones that I'm looking at are

541
00:18:53,370 --> 00:18:56,489
actually the ones that even the computer

542
00:18:54,600 --> 00:18:58,020
doesn't know and guess what humans don't

543
00:18:56,490 --> 00:18:59,700
know either so I have archived had

544
00:18:58,020 --> 00:19:01,560
arguments with other data analytics

545
00:18:59,700 --> 00:19:02,910
people that go it's not like humans know

546
00:19:01,560 --> 00:19:05,159
any better than computers we might as

547
00:19:02,910 --> 00:19:07,140
well just guess but when it comes down

548
00:19:05,160 --> 00:19:09,000
to it when you have humans making

549
00:19:07,140 --> 00:19:11,100
judgment and decision they can be held

550
00:19:09,000 --> 00:19:12,750
responsible they can take into account

551
00:19:11,100 --> 00:19:15,209
the context of which you're doing the

552
00:19:12,750 --> 00:19:16,980
record linkage for and so when you're

553
00:19:15,210 --> 00:19:18,810
actually in this setting and you think

554
00:19:16,980 --> 00:19:21,060
about the percentage of cells you're

555
00:19:18,810 --> 00:19:22,470
looking at it is really really tiny

556
00:19:21,060 --> 00:19:25,500
that's us you just have to keep that in

557
00:19:22,470 --> 00:19:27,870
mind and the most important thing here

558
00:19:25,500 --> 00:19:31,530
for to answer your question is I am not

559
00:19:27,870 --> 00:19:33,629
allowed to go query someone so if you

560
00:19:31,530 --> 00:19:35,490
had a computer system like this and I

561
00:19:33,630 --> 00:19:37,890
was malicious and I wanted to find out

562
00:19:35,490 --> 00:19:39,690
if a friend of mine had HIV it's not

563
00:19:37,890 --> 00:19:41,610
like what I can do is go in and query

564
00:19:39,690 --> 00:19:43,920
the system what's happening is I am

565
00:19:41,610 --> 00:19:45,959
using automatic algorithms it's linking

566
00:19:43,920 --> 00:19:48,720
whatever it can and it's spitting out

567
00:19:45,960 --> 00:19:51,450
things that it cannot so I am limited to

568
00:19:48,720 --> 00:19:53,520
looking at what it was confusing in

569
00:19:51,450 --> 00:19:54,930
there is usually not the one that the

570
00:19:53,520 --> 00:19:56,850
one malicious thing that I wanted to

571
00:19:54,930 --> 00:19:59,010
look for so let's keep that in mind the

572
00:19:56,850 --> 00:20:01,379
other part of this is yes we have a hard

573
00:19:59,010 --> 00:20:03,420
limit so this is another thing when you

574
00:20:01,380 --> 00:20:06,030
start to measure the risk you can

575
00:20:03,420 --> 00:20:07,350
actually have a conversation so what I

576
00:20:06,030 --> 00:20:07,950
have I've done this and I've actually

577
00:20:07,350 --> 00:20:09,418
done this we

578
00:20:07,950 --> 00:20:11,490
search with IRB we've talked to IRB

579
00:20:09,419 --> 00:20:13,679
people and the way this is supposed to

580
00:20:11,490 --> 00:20:15,179
work is you go to IRB and you go I'm

581
00:20:13,679 --> 00:20:16,919
doing this record linkage given them my

582
00:20:15,179 --> 00:20:20,429
understanding of the quality of the data

583
00:20:16,919 --> 00:20:21,690
I need 20 percent IRB looks at it I

584
00:20:20,429 --> 00:20:23,039
don't know how much they're gonna be

585
00:20:21,690 --> 00:20:25,799
able to make that judgements but they

586
00:20:23,039 --> 00:20:27,570
say say yes I do my best I need more I

587
00:20:25,799 --> 00:20:29,970
go back to the IRB I said I tried my

588
00:20:27,570 --> 00:20:31,200
best here's what I need more and you can

589
00:20:29,970 --> 00:20:33,659
request so you can have an iterative

590
00:20:31,200 --> 00:20:35,519
conversation and come to sort of an

591
00:20:33,659 --> 00:20:37,019
agreement as to the risk that you have

592
00:20:35,519 --> 00:20:39,929
to do so you can balance that risk and

593
00:20:37,019 --> 00:20:48,000
utility okay well thank you is there any

594
00:20:39,929 --> 00:20:50,130
any more questions okay yeah I actually

595
00:20:48,000 --> 00:20:52,710
really appreciate that you're looking to

596
00:20:50,130 --> 00:20:54,929
sort of scope the tasks right because

597
00:20:52,710 --> 00:20:56,220
often we don't skip the test but and you

598
00:20:54,929 --> 00:20:57,899
know often you're gonna have a big data

599
00:20:56,220 --> 00:20:59,190
set you're not gonna know what tasks

600
00:20:57,899 --> 00:21:01,649
you're gonna want to do with it in the

601
00:20:59,190 --> 00:21:02,639
future right and you know and some

602
00:21:01,649 --> 00:21:05,729
companies will use this as an argument

603
00:21:02,639 --> 00:21:08,939
to harvest and keep everything what set

604
00:21:05,730 --> 00:21:10,889
that aside and let's let's say that

605
00:21:08,940 --> 00:21:14,460
you're trying to use this method across

606
00:21:10,889 --> 00:21:17,879
a set of analysts or overtime or across

607
00:21:14,460 --> 00:21:20,279
a set of tasks can you scale up some of

608
00:21:17,880 --> 00:21:22,980
their scale or adapt some of the privacy

609
00:21:20,279 --> 00:21:24,210
cost computations that you're doing just

610
00:21:22,980 --> 00:21:26,309
sort of account for these kind of more

611
00:21:24,210 --> 00:21:28,679
complicated scenarios where you're sort

612
00:21:26,309 --> 00:21:31,769
of ringing data of a data set over time

613
00:21:28,679 --> 00:21:33,750
so the current math behind how I measure

614
00:21:31,769 --> 00:21:35,220
the risk is actually not part of the KY

615
00:21:33,750 --> 00:21:36,659
research because you know with the

616
00:21:35,220 --> 00:21:39,419
limited number of pages I couldn't I

617
00:21:36,659 --> 00:21:40,980
what I did was we did the math that the

618
00:21:39,419 --> 00:21:43,620
most important part of the math is what

619
00:21:40,980 --> 00:21:45,600
you count as the denominator in terms of

620
00:21:43,620 --> 00:21:47,760
because it turns out to be a percentage

621
00:21:45,600 --> 00:21:50,340
so from what I hear your question to be

622
00:21:47,760 --> 00:21:53,010
if you have a more complex setting your

623
00:21:50,340 --> 00:21:56,158
denominator changes currently what's in

624
00:21:53,010 --> 00:21:57,990
the numerator is the the kinds of things

625
00:21:56,159 --> 00:21:59,600
that you're looking at and how unique it

626
00:21:57,990 --> 00:22:02,039
is so it actually measures the

627
00:21:59,600 --> 00:22:04,799
identifiability risk and not just at

628
00:22:02,039 --> 00:22:06,809
this general percentage of cells so you

629
00:22:04,799 --> 00:22:08,490
can do the math in such a way that you

630
00:22:06,809 --> 00:22:10,340
take into account the right denominator

631
00:22:08,490 --> 00:22:12,389
and which is actually really important I

632
00:22:10,340 --> 00:22:14,399
hopefully this answers your question or

633
00:22:12,389 --> 00:22:16,908
some part of it okay

634
00:22:14,399 --> 00:22:18,969
let's think the speaker one more time

635
00:22:16,909 --> 00:22:18,970
you

