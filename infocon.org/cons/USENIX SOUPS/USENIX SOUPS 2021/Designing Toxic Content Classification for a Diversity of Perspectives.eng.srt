1
00:00:16,400 --> 00:00:18,400
hi everyone i'm deepak and today i'm

2
00:00:18,400 --> 00:00:19,439
going to share some work on

3
00:00:19,439 --> 00:00:21,199
incorporating diverse user perspectives

4
00:00:21,199 --> 00:00:23,680
into toxic content classification

5
00:00:23,680 --> 00:00:25,680
so in order to combat toxic content and

6
00:00:25,680 --> 00:00:27,760
online abuse major companies have

7
00:00:27,760 --> 00:00:29,679
frequently turned to machine learning to

8
00:00:29,679 --> 00:00:30,960
help towards the automated

9
00:00:30,960 --> 00:00:33,360
classification of toxic content and

10
00:00:33,360 --> 00:00:34,880
indeed many of these automated systems

11
00:00:34,880 --> 00:00:36,480
are deployed today and companies will

12
00:00:36,480 --> 00:00:38,079
routinely do press releases where you'll

13
00:00:38,079 --> 00:00:40,239
see headlines like twitter automatically

14
00:00:40,239 --> 00:00:42,320
flags more than half of all tweets that

15
00:00:42,320 --> 00:00:45,039
violate its rules and yet at the same

16
00:00:45,039 --> 00:00:47,360
time there is significant and persistent

17
00:00:47,360 --> 00:00:49,760
backlash against major platforms arguing

18
00:00:49,760 --> 00:00:51,199
that they are not doing enough to

19
00:00:51,199 --> 00:00:52,640
moderate the toxic content that is

20
00:00:52,640 --> 00:00:54,480
plaguing their users especially those

21
00:00:54,480 --> 00:00:56,480
historically at risk of having toxic

22
00:00:56,480 --> 00:00:58,960
interactions online so what's going on

23
00:00:58,960 --> 00:01:01,680
here well while automated classifiers

24
00:01:01,680 --> 00:01:03,440
may be good at identifying candidate

25
00:01:03,440 --> 00:01:06,000
content that violates platform rules

26
00:01:06,000 --> 00:01:07,840
there are still loads of content that

27
00:01:07,840 --> 00:01:10,000
fall into what we'll call a gray area

28
00:01:10,000 --> 00:01:11,920
not abusive enough to warrant removal

29
00:01:11,920 --> 00:01:14,400
but still enough to cause harm to some

30
00:01:14,400 --> 00:01:16,560
internet users in essence not every

31
00:01:16,560 --> 00:01:18,080
person whether you're a user of the

32
00:01:18,080 --> 00:01:19,680
platform or the platform itself

33
00:01:19,680 --> 00:01:21,840
necessarily agrees on what constitutes a

34
00:01:21,840 --> 00:01:24,320
toxic comment online and given that

35
00:01:24,320 --> 00:01:26,320
there are these differences in this work

36
00:01:26,320 --> 00:01:28,080
we sought to answer a more fundamental

37
00:01:28,080 --> 00:01:29,439
question which is

38
00:01:29,439 --> 00:01:30,960
how do the diverse perspectives of

39
00:01:30,960 --> 00:01:33,040
internet users everything from their

40
00:01:33,040 --> 00:01:34,880
personal identities to their online

41
00:01:34,880 --> 00:01:37,280
experiences change the way that users

42
00:01:37,280 --> 00:01:39,280
interpret toxic content online and

43
00:01:39,280 --> 00:01:40,720
understanding this is an important first

44
00:01:40,720 --> 00:01:42,640
step to building anti-abuse tools for

45
00:01:42,640 --> 00:01:44,399
internet users that actually meet the

46
00:01:44,399 --> 00:01:46,000
needs of an increasingly diverse

47
00:01:46,000 --> 00:01:47,759
internet user base

48
00:01:47,759 --> 00:01:49,520
so how did we do this well we conducted

49
00:01:49,520 --> 00:01:52,159
a survey of 17 280 participants for

50
00:01:52,159 --> 00:01:53,680
mechanical turk and we asked

51
00:01:53,680 --> 00:01:56,079
participants to label the toxicity of 20

52
00:01:56,079 --> 00:01:58,000
potentially divisive comments online

53
00:01:58,000 --> 00:02:00,320
each comment was labeled by five raters

54
00:02:00,320 --> 00:02:01,920
and in total we built a data set of

55
00:02:01,920 --> 00:02:04,399
about 108 000 comments

56
00:02:04,399 --> 00:02:05,840
in addition to asking for their

57
00:02:05,840 --> 00:02:07,439
perspectives we also had each

58
00:02:07,439 --> 00:02:09,360
participant answer some demographic

59
00:02:09,360 --> 00:02:10,878
questions and tell us about their

60
00:02:10,878 --> 00:02:12,640
personal experiences with toxic content

61
00:02:12,640 --> 00:02:14,480
online so our data set ultimately

62
00:02:14,480 --> 00:02:16,239
captures a diverse set of participants

63
00:02:16,239 --> 00:02:18,239
from varied backgrounds and this forms

64
00:02:18,239 --> 00:02:20,640
the basis of our analysis

65
00:02:20,640 --> 00:02:22,080
so what we found when we were

66
00:02:22,080 --> 00:02:23,920
investigating participants toxicity

67
00:02:23,920 --> 00:02:25,760
ratings is that participants that are

68
00:02:25,760 --> 00:02:28,080
historically at risk of experiencing

69
00:02:28,080 --> 00:02:30,640
online abuse perceive higher amounts of

70
00:02:30,640 --> 00:02:32,959
toxic content so for example the odds

71
00:02:32,959 --> 00:02:35,280
that minorities younger americans and

72
00:02:35,280 --> 00:02:37,440
members of the lgbtq community perceive

73
00:02:37,440 --> 00:02:39,440
a comment as toxic is significantly

74
00:02:39,440 --> 00:02:41,680
higher than those who do not identify in

75
00:02:41,680 --> 00:02:43,599
any of those categories

76
00:02:43,599 --> 00:02:45,360
of course it's not just their identities

77
00:02:45,360 --> 00:02:47,280
that play a role in this a participant's

78
00:02:47,280 --> 00:02:49,200
lived experiences also play a role in

79
00:02:49,200 --> 00:02:50,959
their perceptions of online toxicity so

80
00:02:50,959 --> 00:02:53,040
participants who had been a target of

81
00:02:53,040 --> 00:02:55,440
toxic content online had a higher odds

82
00:02:55,440 --> 00:02:57,440
of rating comments as toxic compared to

83
00:02:57,440 --> 00:02:59,120
those who have never been a target of

84
00:02:59,120 --> 00:03:01,120
toxic content online conversely

85
00:03:01,120 --> 00:03:03,200
participants who've seen toxic contents

86
00:03:03,200 --> 00:03:05,040
but have never actually been a target

87
00:03:05,040 --> 00:03:06,720
actually have a lower odds of reading a

88
00:03:06,720 --> 00:03:08,640
comment as toxic highlighting that these

89
00:03:08,640 --> 00:03:10,959
participants may be desensitized to new

90
00:03:10,959 --> 00:03:13,280
toxic content or have simply a higher

91
00:03:13,280 --> 00:03:15,440
threshold for what is acceptable online

92
00:03:15,440 --> 00:03:17,360
but the point here is that a user's

93
00:03:17,360 --> 00:03:19,280
identity and their lived experience

94
00:03:19,280 --> 00:03:21,200
plays a direct role in their perceptions

95
00:03:21,200 --> 00:03:24,080
of online toxicity so we next ask how

96
00:03:24,080 --> 00:03:25,760
can we account for these diverse

97
00:03:25,760 --> 00:03:29,040
perspectives an idea here is simple can

98
00:03:29,040 --> 00:03:31,440
we somehow personalize existing

99
00:03:31,440 --> 00:03:33,360
classifiers to help us overcome the

100
00:03:33,360 --> 00:03:34,959
ambiguities in toxic content

101
00:03:34,959 --> 00:03:36,640
classification for a diverse set of

102
00:03:36,640 --> 00:03:38,959
users so what does that mean

103
00:03:38,959 --> 00:03:40,080
well if you were to take a

104
00:03:40,080 --> 00:03:41,920
state-of-the-art classifier toxicity

105
00:03:41,920 --> 00:03:43,519
classifier and evaluate it on our data

106
00:03:43,519 --> 00:03:46,000
set what you would see is a maximum

107
00:03:46,000 --> 00:03:48,640
performance of about 0.35 precision and

108
00:03:48,640 --> 00:03:51,599
0.37 accuracy for users in our data set

109
00:03:51,599 --> 00:03:53,680
at the optimal decision threshold the

110
00:03:53,680 --> 00:03:55,280
one that maximizes performance for the

111
00:03:55,280 --> 00:03:57,200
most number of users

112
00:03:57,200 --> 00:03:59,280
now you can actually improve this by

113
00:03:59,280 --> 00:04:01,519
simply finding a decision threshold that

114
00:04:01,519 --> 00:04:03,280
maximizes the performance for each

115
00:04:03,280 --> 00:04:05,200
individual participant so for example

116
00:04:05,200 --> 00:04:06,480
instead of having a single decision

117
00:04:06,480 --> 00:04:08,720
threshold say marking a comment as toxic

118
00:04:08,720 --> 00:04:11,519
if it's greater than 0.5 and not toxic

119
00:04:11,519 --> 00:04:13,680
if it's less than 0.5 what you do is you

120
00:04:13,680 --> 00:04:15,760
have a specific personalized threshold

121
00:04:15,760 --> 00:04:17,519
for per participant based on their

122
00:04:17,519 --> 00:04:19,519
individual toxicity ratings

123
00:04:19,519 --> 00:04:21,358
and it turns out if you do this your

124
00:04:21,358 --> 00:04:23,680
average precision and average accuracy

125
00:04:23,680 --> 00:04:25,360
increase dramatically up to 0.6

126
00:04:25,360 --> 00:04:28,240
precision and 0.68 accuracy in our data

127
00:04:28,240 --> 00:04:30,240
set 72 percent of participants

128
00:04:30,240 --> 00:04:31,680
experience the performance improvement

129
00:04:31,680 --> 00:04:33,360
from the classifier highlighting that

130
00:04:33,360 --> 00:04:35,280
simply just taking account

131
00:04:35,280 --> 00:04:36,960
into account personal preferences goes a

132
00:04:36,960 --> 00:04:40,080
long way in automated toxicity detection

133
00:04:40,080 --> 00:04:42,240
so ultimately our finding suggests that

134
00:04:42,240 --> 00:04:43,840
current automated toxicity detection

135
00:04:43,840 --> 00:04:46,000
systems fail to generalize to a diverse

136
00:04:46,000 --> 00:04:48,560
internet user base and as we're building

137
00:04:48,560 --> 00:04:50,720
new anti-abuse defenses we should take

138
00:04:50,720 --> 00:04:52,320
into account that those diverse

139
00:04:52,320 --> 00:04:53,840
perspectives in order to truly build

140
00:04:53,840 --> 00:04:55,680
tools that can protect all internet

141
00:04:55,680 --> 00:04:56,639
users

142
00:04:56,639 --> 00:05:00,120
thank you for your time

