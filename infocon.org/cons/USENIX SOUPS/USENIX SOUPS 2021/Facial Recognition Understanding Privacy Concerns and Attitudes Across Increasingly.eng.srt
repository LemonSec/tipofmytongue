1
00:00:16,640 --> 00:00:18,960
hello i'm erin jung today i'll present

2
00:00:18,960 --> 00:00:21,600
our work titled facial recognition

3
00:00:21,600 --> 00:00:23,279
understanding privacy concerns and

4
00:00:23,279 --> 00:00:25,279
attitudes across increasingly diverse

5
00:00:25,279 --> 00:00:26,960
deployment scenarios

6
00:00:26,960 --> 00:00:28,400
i'll have to keep things brief in this

7
00:00:28,400 --> 00:00:30,720
5-minute presentation please find more

8
00:00:30,720 --> 00:00:32,719
details about this work in our soup's

9
00:00:32,719 --> 00:00:34,079
paper

10
00:00:34,079 --> 00:00:35,840
facial recognition technology has

11
00:00:35,840 --> 00:00:38,160
already permeated into our daily digital

12
00:00:38,160 --> 00:00:40,559
lives it unlocks phones

13
00:00:40,559 --> 00:00:42,640
tags for people on social media and adds

14
00:00:42,640 --> 00:00:44,559
filters to our zoom calls

15
00:00:44,559 --> 00:00:46,079
it also begins to creep into the

16
00:00:46,079 --> 00:00:48,640
physical world for example at airports

17
00:00:48,640 --> 00:00:50,879
amusement parks and stores

18
00:00:50,879 --> 00:00:52,800
in this presentation and our paper we

19
00:00:52,800 --> 00:00:54,320
take a broad definition of facial

20
00:00:54,320 --> 00:00:56,079
recognition which includes

21
00:00:56,079 --> 00:00:57,760
identification

22
00:00:57,760 --> 00:00:59,520
detection in motion recognition and

23
00:00:59,520 --> 00:01:00,960
scene detection

24
00:01:00,960 --> 00:01:02,559
the rapid growth of facial recognition

25
00:01:02,559 --> 00:01:04,559
technology across even more diverse

26
00:01:04,559 --> 00:01:06,080
contexts calls for a better

27
00:01:06,080 --> 00:01:07,840
understanding of how people feel about

28
00:01:07,840 --> 00:01:09,280
these deployments

29
00:01:09,280 --> 00:01:11,280
we are interested in understanding what

30
00:01:11,280 --> 00:01:12,960
are people's concerns and attitudes

31
00:01:12,960 --> 00:01:15,759
about facial recognition specifically

32
00:01:15,759 --> 00:01:18,479
what are their privacy concerns

33
00:01:18,479 --> 00:01:20,560
we first did an extensive survey of news

34
00:01:20,560 --> 00:01:22,560
articles about real-world deployments of

35
00:01:22,560 --> 00:01:24,880
facial recognition and identified 15

36
00:01:24,880 --> 00:01:26,720
major use cases and scenarios to be

37
00:01:26,720 --> 00:01:28,320
included in our study

38
00:01:28,320 --> 00:01:30,320
those scenarios are mainly composed of

39
00:01:30,320 --> 00:01:33,280
four broad categories including security

40
00:01:33,280 --> 00:01:34,479
commerce

41
00:01:34,479 --> 00:01:36,400
identification authentication and more

42
00:01:36,400 --> 00:01:37,759
advanced use cases that involve

43
00:01:37,759 --> 00:01:39,280
predictions

44
00:01:39,280 --> 00:01:40,880
because context has been shown to have

45
00:01:40,880 --> 00:01:42,720
an important effect on people's behavior

46
00:01:42,720 --> 00:01:45,200
as well as privacy attitudes instead of

47
00:01:45,200 --> 00:01:47,520
using static online surveys we opt to

48
00:01:47,520 --> 00:01:50,000
conduct an experienced sampling study

49
00:01:50,000 --> 00:01:52,079
a brief explain the study protocol

50
00:01:52,079 --> 00:01:53,920
participants installed the study app on

51
00:01:53,920 --> 00:01:55,759
their own android devices they were

52
00:01:55,759 --> 00:01:57,360
instructed to go about their regular

53
00:01:57,360 --> 00:01:59,280
daily activities and the app sent them

54
00:01:59,280 --> 00:02:01,439
push notifications prompting them to

55
00:02:01,439 --> 00:02:03,119
complete a short survey based on their

56
00:02:03,119 --> 00:02:04,560
current locations

57
00:02:04,560 --> 00:02:06,320
they also got an email in the evening to

58
00:02:06,320 --> 00:02:08,160
complete a daily summary to provide

59
00:02:08,160 --> 00:02:10,000
additional responses to complementary

60
00:02:10,000 --> 00:02:11,520
in-situ answers

61
00:02:11,520 --> 00:02:13,840
this process will happen for 10 days

62
00:02:13,840 --> 00:02:15,680
participants finish the study with a

63
00:02:15,680 --> 00:02:18,000
post survey we also interviewed 10 of

64
00:02:18,000 --> 00:02:20,160
them afterwards

65
00:02:20,160 --> 00:02:22,640
we collected a rich quality of data set

66
00:02:22,640 --> 00:02:25,280
composed of detailed responses from 123

67
00:02:25,280 --> 00:02:26,640
participants

68
00:02:26,640 --> 00:02:28,720
we applied content analysis and

69
00:02:28,720 --> 00:02:30,959
inductive coding on the approximate 3

70
00:02:30,959 --> 00:02:33,840
800 text responses and the 10 interview

71
00:02:33,840 --> 00:02:35,120
transcripts

72
00:02:35,120 --> 00:02:37,440
two coders met regularly to discuss and

73
00:02:37,440 --> 00:02:40,080
resolve all coding discrepancies

74
00:02:40,080 --> 00:02:42,560
moving on to the results we identified

75
00:02:42,560 --> 00:02:44,480
three main themes of concerns

76
00:02:44,480 --> 00:02:46,160
participants had towards facial

77
00:02:46,160 --> 00:02:47,280
recognition

78
00:02:47,280 --> 00:02:49,200
to begin with participants were

79
00:02:49,200 --> 00:02:51,360
concerned about anonymous demographics

80
00:02:51,360 --> 00:02:54,080
detection for advertising nine percent

81
00:02:54,080 --> 00:02:56,400
of participants expressed reservations

82
00:02:56,400 --> 00:02:58,720
about this type of technology as they

83
00:02:58,720 --> 00:03:01,280
saw it as a form of profiling either

84
00:03:01,280 --> 00:03:02,959
based on race age

85
00:03:02,959 --> 00:03:04,560
gender and religion

86
00:03:04,560 --> 00:03:06,920
such practices could potentially induce

87
00:03:06,920 --> 00:03:09,599
oversimplification perpetuate existing

88
00:03:09,599 --> 00:03:12,480
stereotypes and reduce individualities

89
00:03:12,480 --> 00:03:14,159
please feel free to pause the video to

90
00:03:14,159 --> 00:03:16,239
read quotes

91
00:03:16,239 --> 00:03:18,720
secondly participants were worried about

92
00:03:18,720 --> 00:03:21,440
incorrect detection and interpretation

93
00:03:21,440 --> 00:03:22,640
a third of

94
00:03:22,640 --> 00:03:24,239
participants were worried about the

95
00:03:24,239 --> 00:03:26,480
technology not accurate enough and could

96
00:03:26,480 --> 00:03:29,040
make mistakes some participants also

97
00:03:29,040 --> 00:03:30,879
argued that seemingly suspicious

98
00:03:30,879 --> 00:03:33,440
behavior when viewed out of context can

99
00:03:33,440 --> 00:03:36,640
be misinterpreted by those systems

100
00:03:36,640 --> 00:03:38,640
thirdly participants were concerned

101
00:03:38,640 --> 00:03:41,200
about racial and other biases introduced

102
00:03:41,200 --> 00:03:43,040
by facial recognition

103
00:03:43,040 --> 00:03:45,519
110 of our participants reported being

104
00:03:45,519 --> 00:03:47,680
concerned about the potential bias in

105
00:03:47,680 --> 00:03:50,080
the systems many were worried that

106
00:03:50,080 --> 00:03:52,480
racial bias in these algorithms could

107
00:03:52,480 --> 00:03:54,720
exaggerate the exchange bias and

108
00:03:54,720 --> 00:03:56,480
infringe upon the rights of those

109
00:03:56,480 --> 00:03:59,120
impacted

110
00:03:59,120 --> 00:04:01,360
privacy is repeatedly brought up as a

111
00:04:01,360 --> 00:04:04,239
key concern by our study participants

112
00:04:04,239 --> 00:04:05,680
around 70

113
00:04:05,680 --> 00:04:08,400
voiced privacy concerns during the study

114
00:04:08,400 --> 00:04:10,640
we summarized four major themes around

115
00:04:10,640 --> 00:04:12,840
perceived privacy risks of facial

116
00:04:12,840 --> 00:04:15,360
recognition in light of concepts from

117
00:04:15,360 --> 00:04:17,918
established privacy frameworks

118
00:04:17,918 --> 00:04:20,639
the first is violation of solitude

119
00:04:20,639 --> 00:04:22,639
visual recognition makes the feeling of

120
00:04:22,639 --> 00:04:25,520
surveillance prevail many regarded some

121
00:04:25,520 --> 00:04:27,680
development scenarios as unwarranted and

122
00:04:27,680 --> 00:04:28,880
praying

123
00:04:28,880 --> 00:04:31,120
the second is unwanted exposure and

124
00:04:31,120 --> 00:04:33,280
violation of anonymity

125
00:04:33,280 --> 00:04:35,199
participants stressed the importance of

126
00:04:35,199 --> 00:04:37,600
anonymity and scrutinized how facial

127
00:04:37,600 --> 00:04:39,759
recognition enabled identification of

128
00:04:39,759 --> 00:04:42,560
normal people in plain view as well as

129
00:04:42,560 --> 00:04:44,160
potentially revealing sensitive

130
00:04:44,160 --> 00:04:46,560
information like emotions

131
00:04:46,560 --> 00:04:49,360
the third theme is non-consensual and

132
00:04:49,360 --> 00:04:51,280
insecure disclosure

133
00:04:51,280 --> 00:04:53,360
this refers in part to data collection

134
00:04:53,360 --> 00:04:55,040
without data subjects knowledge or

135
00:04:55,040 --> 00:04:57,520
consent many respondents questioned

136
00:04:57,520 --> 00:04:59,520
whether companies would retain data for

137
00:04:59,520 --> 00:05:01,280
intended use only

138
00:05:01,280 --> 00:05:03,520
also participants showed fear of data

139
00:05:03,520 --> 00:05:06,560
leakage and abuse because it is almost

140
00:05:06,560 --> 00:05:09,039
impractical to relinquish biometric data

141
00:05:09,039 --> 00:05:10,479
when compromised

142
00:05:10,479 --> 00:05:12,400
feature the data security for facial

143
00:05:12,400 --> 00:05:14,800
recognition the data security for facial

144
00:05:14,800 --> 00:05:17,440
recognition is ever more pressing

145
00:05:17,440 --> 00:05:19,199
the last theme is inaccurate

146
00:05:19,199 --> 00:05:22,160
dissemination and violation of reserve

147
00:05:22,160 --> 00:05:23,600
participants will worried about the

148
00:05:23,600 --> 00:05:25,360
dissemination of inaccurate or

149
00:05:25,360 --> 00:05:27,600
misleading information as a result of

150
00:05:27,600 --> 00:05:29,840
inaccuracies of the systems

151
00:05:29,840 --> 00:05:32,080
participants were also concerned about

152
00:05:32,080 --> 00:05:34,080
decisional influence specifically

153
00:05:34,080 --> 00:05:35,680
unwarranted influence on their

154
00:05:35,680 --> 00:05:37,680
purchasing autonomy by private

155
00:05:37,680 --> 00:05:39,360
corporations with the help of visual

156
00:05:39,360 --> 00:05:41,360
recognition technology

157
00:05:41,360 --> 00:05:43,440
this concludes my presentation please

158
00:05:43,440 --> 00:05:45,360
check out our soup's paper and assessor

159
00:05:45,360 --> 00:05:46,960
publication on past this year that

160
00:05:46,960 --> 00:05:48,720
focuses on the quantitative results of

161
00:05:48,720 --> 00:05:53,160
the study thank you for your attention

