1
00:00:00,160 --> 00:00:01,040
um

2
00:00:01,040 --> 00:00:03,120
we've heard a lot about what happens

3
00:00:03,120 --> 00:00:04,640
when uh

4
00:00:04,640 --> 00:00:07,680
policies and proper authentication

5
00:00:07,680 --> 00:00:09,679
aren't well thought out

6
00:00:09,679 --> 00:00:12,400
and now we're going to dive a little bit

7
00:00:12,400 --> 00:00:13,519
into the

8
00:00:13,519 --> 00:00:15,759
mindset of attackers and we're gonna

9
00:00:15,759 --> 00:00:17,440
discuss how

10
00:00:17,440 --> 00:00:19,520
oftentimes highly skilled attackers tend

11
00:00:19,520 --> 00:00:21,600
to avoid reinventing the wheel right and

12
00:00:21,600 --> 00:00:23,840
they tend to keep a low profile

13
00:00:23,840 --> 00:00:26,080
by avoiding leaving a trail of binaries

14
00:00:26,080 --> 00:00:27,840
and files behind them

15
00:00:27,840 --> 00:00:29,920
our following speakers argue that this

16
00:00:29,920 --> 00:00:31,760
is why living off the land is still one

17
00:00:31,760 --> 00:00:33,920
of the preferred approaches to security

18
00:00:33,920 --> 00:00:35,360
professionals

19
00:00:35,360 --> 00:00:38,559
um next up we will listen to tiberius

20
00:00:38,559 --> 00:00:40,320
who is a data scientist and machine

21
00:00:40,320 --> 00:00:42,480
learning engineer and alongside him we

22
00:00:42,480 --> 00:00:44,559
welcome andre a technical lead and

23
00:00:44,559 --> 00:00:46,719
security intelligence and engineering at

24
00:00:46,719 --> 00:00:47,760
adobe

25
00:00:47,760 --> 00:00:49,440
they will tell us all about why it's

26
00:00:49,440 --> 00:00:51,120
hard to detect instances when an

27
00:00:51,120 --> 00:00:53,440
attacker is living off the land

28
00:00:53,440 --> 00:00:56,000
but not all hope is lost because they

29
00:00:56,000 --> 00:00:58,079
will also follow up with an in-depth

30
00:00:58,079 --> 00:01:00,320
look at an approach they made they

31
00:01:00,320 --> 00:01:02,480
created to make such detections

32
00:01:02,480 --> 00:01:05,600
gentlemen you have the virtual stage

33
00:01:05,600 --> 00:01:08,320
hello thank you so hello everybody thank

34
00:01:08,320 --> 00:01:10,479
you very much for joining this talk we

35
00:01:10,479 --> 00:01:12,960
are actually really excited to be here

36
00:01:12,960 --> 00:01:14,880
my name is andrey kotayan and together

37
00:01:14,880 --> 00:01:16,880
with my ass and colleague tiberi boris

38
00:01:16,880 --> 00:01:18,560
we're going to have today a short

39
00:01:18,560 --> 00:01:20,479
discussion a project we recently open

40
00:01:20,479 --> 00:01:22,479
source and that we're actually really

41
00:01:22,479 --> 00:01:24,799
fond of so with no further introduction

42
00:01:24,799 --> 00:01:26,799
a supervised machine learning living of

43
00:01:26,799 --> 00:01:28,960
the rank classifier i know it sounds

44
00:01:28,960 --> 00:01:31,360
fancy

45
00:01:32,960 --> 00:01:35,759
okay so a couple of words about us

46
00:01:35,759 --> 00:01:37,920
we are the security intelligence team at

47
00:01:37,920 --> 00:01:40,400
adobe we are a part of adobe security

48
00:01:40,400 --> 00:01:42,159
coordination center

49
00:01:42,159 --> 00:01:44,880
the acc actually handles monitoring

50
00:01:44,880 --> 00:01:46,880
alerting and is the responsible in other

51
00:01:46,880 --> 00:01:48,960
words the security the reactive part of

52
00:01:48,960 --> 00:01:50,640
the security in the house

53
00:01:50,640 --> 00:01:52,479
the purpose of the security intelligence

54
00:01:52,479 --> 00:01:55,040
team is to do data science research in

55
00:01:55,040 --> 00:01:56,560
the security field

56
00:01:56,560 --> 00:01:59,119
so we mostly focus on reactive security

57
00:01:59,119 --> 00:02:01,439
as the acc and that basically means

58
00:02:01,439 --> 00:02:03,040
identifying threats that cannot be

59
00:02:03,040 --> 00:02:06,000
detected via conventional ways

60
00:02:06,000 --> 00:02:07,439
in other words

61
00:02:07,439 --> 00:02:09,758
we use the logs and the security data we

62
00:02:09,758 --> 00:02:11,760
collect from adobe assets and we try to

63
00:02:11,760 --> 00:02:14,319
find anomalies and of course bad stuff

64
00:02:14,319 --> 00:02:16,480
now we really like sharing our work even

65
00:02:16,480 --> 00:02:18,560
outside of adobe and we have quite a

66
00:02:18,560 --> 00:02:20,319
couple of open source projects

67
00:02:20,319 --> 00:02:22,000
presentation and blogs and we're

68
00:02:22,000 --> 00:02:24,160
actually going to touch base

69
00:02:24,160 --> 00:02:26,080
basis on some of those later in this

70
00:02:26,080 --> 00:02:27,840
talk

71
00:02:27,840 --> 00:02:30,160
okay so for a short intro like what's

72
00:02:30,160 --> 00:02:32,080
leaving over the land so

73
00:02:32,080 --> 00:02:34,239
easily put living of the land means

74
00:02:34,239 --> 00:02:36,080
doing your job with what you have at

75
00:02:36,080 --> 00:02:38,480
your disposal but let's be honest here

76
00:02:38,480 --> 00:02:40,319
this is not the 13th 14th centuries

77
00:02:40,319 --> 00:02:42,000
anymore right those are not the middle

78
00:02:42,000 --> 00:02:44,640
ages and and the truth is what we have

79
00:02:44,640 --> 00:02:47,360
at our disposal is more than enough

80
00:02:47,360 --> 00:02:49,280
now living of the land is not a brand

81
00:02:49,280 --> 00:02:51,519
new concept living after land attacks

82
00:02:51,519 --> 00:02:53,760
persistence lateral moves however you

83
00:02:53,760 --> 00:02:56,160
want to call them have been recorded and

84
00:02:56,160 --> 00:02:58,000
reported for a while since the beginning

85
00:02:58,000 --> 00:03:00,159
of the century at least

86
00:03:00,159 --> 00:03:02,400
all the big kids are doing it attackers

87
00:03:02,400 --> 00:03:06,080
apt groups fantastic purposes and so on

88
00:03:06,080 --> 00:03:07,360
now what's really interesting about

89
00:03:07,360 --> 00:03:09,200
those type of attacks is the fact the

90
00:03:09,200 --> 00:03:11,040
footprint they leave it's really really

91
00:03:11,040 --> 00:03:12,400
really

92
00:03:12,400 --> 00:03:15,519
reduced and the reason is that why is

93
00:03:15,519 --> 00:03:17,040
that because basically you don't have

94
00:03:17,040 --> 00:03:19,280
new binaries you're trying to blend in

95
00:03:19,280 --> 00:03:21,200
and you already have everything you need

96
00:03:21,200 --> 00:03:23,120
at your disposal there

97
00:03:23,120 --> 00:03:24,000
now

98
00:03:24,000 --> 00:03:25,840
the truth is between a security

99
00:03:25,840 --> 00:03:29,200
professional if you have cash python

100
00:03:29,200 --> 00:03:30,280
powershell

101
00:03:30,280 --> 00:03:33,360
curacao whatever you wonder what more do

102
00:03:33,360 --> 00:03:35,440
you actually need on that box

103
00:03:35,440 --> 00:03:37,680
plus in the last cases i i looked like

104
00:03:37,680 --> 00:03:40,000
netcat we became a default admin to run

105
00:03:40,000 --> 00:03:42,799
most of the linux installers there

106
00:03:42,799 --> 00:03:43,760
so

107
00:03:43,760 --> 00:03:46,239
leaving of the lens refers to attack

108
00:03:46,239 --> 00:03:48,879
scenarios where multiple segments parts

109
00:03:48,879 --> 00:03:51,120
of the attack itself are being delivered

110
00:03:51,120 --> 00:03:52,959
with what you already have on the

111
00:03:52,959 --> 00:03:55,120
compromise box on the box you are on

112
00:03:55,120 --> 00:03:57,760
right so no external malicious or

113
00:03:57,760 --> 00:04:00,080
especially crafted tools or malware for

114
00:04:00,080 --> 00:04:00,799
that

115
00:04:00,799 --> 00:04:03,439
for that matter

116
00:04:03,439 --> 00:04:04,799
okay and

117
00:04:04,799 --> 00:04:07,439
again the whole magic is that what you

118
00:04:07,439 --> 00:04:09,439
need is already there and there are some

119
00:04:09,439 --> 00:04:11,280
people who did an awesome job

120
00:04:11,280 --> 00:04:13,840
documenting this right and i'm sure that

121
00:04:13,840 --> 00:04:15,760
are actually really really familiar with

122
00:04:15,760 --> 00:04:17,759
those two links that are there they're

123
00:04:17,759 --> 00:04:19,759
basically leaving off the land binaries

124
00:04:19,759 --> 00:04:22,079
with example documented and a lot of

125
00:04:22,079 --> 00:04:24,400
examples do a lot of stuff right

126
00:04:24,400 --> 00:04:26,320
so for example you need to download

127
00:04:26,320 --> 00:04:28,160
something and you do not know what to

128
00:04:28,160 --> 00:04:29,919
use well go to the link and you're going

129
00:04:29,919 --> 00:04:31,680
to find quite a couple of ways to do

130
00:04:31,680 --> 00:04:33,840
your your job you need to abuse a

131
00:04:33,840 --> 00:04:35,759
religion buyer to execute some code

132
00:04:35,759 --> 00:04:37,520
again go to the link and you're going to

133
00:04:37,520 --> 00:04:39,759
find a couple of ways of doing that you

134
00:04:39,759 --> 00:04:41,759
want to escape a text editor and become

135
00:04:41,759 --> 00:04:44,240
root or whatever user is there again go

136
00:04:44,240 --> 00:04:45,680
to the link and you'll find a really

137
00:04:45,680 --> 00:04:47,040
nice example

138
00:04:47,040 --> 00:04:48,560
now of course

139
00:04:48,560 --> 00:04:50,800
things aren't always so straightforward

140
00:04:50,800 --> 00:04:53,040
right but the fact that what you need is

141
00:04:53,040 --> 00:04:54,320
already there it's a great starting

142
00:04:54,320 --> 00:04:55,840
point

143
00:04:55,840 --> 00:04:58,880
a clear advantage of using on-prem stuff

144
00:04:58,880 --> 00:04:59,759
like

145
00:04:59,759 --> 00:05:01,840
scripting your way in is the fact that

146
00:05:01,840 --> 00:05:04,000
you blend in you are one with the

147
00:05:04,000 --> 00:05:06,320
environment you are the administrator

148
00:05:06,320 --> 00:05:09,120
there right and you can behave like one

149
00:05:09,120 --> 00:05:10,720
maybe you're good enough to put also a

150
00:05:10,720 --> 00:05:12,320
couple of security patches in place

151
00:05:12,320 --> 00:05:14,479
right like stop other attackers from

152
00:05:14,479 --> 00:05:16,320
compromising your already compromised

153
00:05:16,320 --> 00:05:18,320
box and i'm sure all of you actually

154
00:05:18,320 --> 00:05:19,600
heard a story about those kind of

155
00:05:19,600 --> 00:05:21,919
scenarios

156
00:05:21,919 --> 00:05:24,080
anyway uh let's get a little bit serious

157
00:05:24,080 --> 00:05:26,560
so what we want to talk today about

158
00:05:26,560 --> 00:05:28,960
me as a security expert i handled quite

159
00:05:28,960 --> 00:05:30,960
a few couple of incidents and security

160
00:05:30,960 --> 00:05:34,720
investigations in in my in my time so

161
00:05:34,720 --> 00:05:36,240
almost every time i learned something

162
00:05:36,240 --> 00:05:38,479
new something that evaded some of the

163
00:05:38,479 --> 00:05:40,560
measures in place something which on its

164
00:05:40,560 --> 00:05:42,320
own would have bypassed my security

165
00:05:42,320 --> 00:05:44,800
bypass my security mindset right

166
00:05:44,800 --> 00:05:47,280
uh also we at adobe work with petabytes

167
00:05:47,280 --> 00:05:49,360
of data so that's definitely more than

168
00:05:49,360 --> 00:05:51,199
what a couple of dozen of people are

169
00:05:51,199 --> 00:05:52,479
able to parse

170
00:05:52,479 --> 00:05:54,880
and the question is how can i make sure

171
00:05:54,880 --> 00:05:56,160
that what i'm

172
00:05:56,160 --> 00:05:58,000
detecting is everything that's happening

173
00:05:58,000 --> 00:05:59,919
and that nothing is actually bypassing

174
00:05:59,919 --> 00:06:01,680
my security stack

175
00:06:01,680 --> 00:06:02,720
and

176
00:06:02,720 --> 00:06:05,360
the honest answer is i cannot but i can

177
00:06:05,360 --> 00:06:07,280
give it my best try i can and i can

178
00:06:07,280 --> 00:06:09,199
always think outside of the box and

179
00:06:09,199 --> 00:06:10,639
actually that's what this project is

180
00:06:10,639 --> 00:06:13,440
about thinking outside of the box

181
00:06:13,440 --> 00:06:15,840
okay so what we've done we took

182
00:06:15,840 --> 00:06:17,360
thousands of examples of malicious

183
00:06:17,360 --> 00:06:19,840
commands we did some voodoo magic around

184
00:06:19,840 --> 00:06:22,479
them and then we built a classifier

185
00:06:22,479 --> 00:06:24,720
whose entire purpose is is to tell me if

186
00:06:24,720 --> 00:06:27,440
a piece of string looks like

187
00:06:27,440 --> 00:06:30,080
it might be a thread and why

188
00:06:30,080 --> 00:06:31,680
and as you can see really fast our tool

189
00:06:31,680 --> 00:06:33,600
is actually pretty simple to use you

190
00:06:33,600 --> 00:06:35,360
paste it you passed it a command and

191
00:06:35,360 --> 00:06:37,039
it's basically going to generate some

192
00:06:37,039 --> 00:06:39,759
text some meta information and where the

193
00:06:39,759 --> 00:06:42,000
classifier is going to classify the data

194
00:06:42,000 --> 00:06:42,720
right

195
00:06:42,720 --> 00:06:45,199
pretty straightforward regarding the the

196
00:06:45,199 --> 00:06:47,360
environments we're targeting so the

197
00:06:47,360 --> 00:06:48,720
leaving of the lamp project actually

198
00:06:48,720 --> 00:06:50,639
comes with two models a linux one and a

199
00:06:50,639 --> 00:06:52,800
windows one currently the performance of

200
00:06:52,800 --> 00:06:54,880
the windows one doesn't make it qualify

201
00:06:54,880 --> 00:06:57,280
for production greatly and we estimate

202
00:06:57,280 --> 00:06:58,560
we're actually going to have a stable

203
00:06:58,560 --> 00:06:59,440
model

204
00:06:59,440 --> 00:07:00,639
somewhere the

205
00:07:00,639 --> 00:07:02,720
next release the end of this year but

206
00:07:02,720 --> 00:07:04,319
the linux one is actually really similar

207
00:07:04,319 --> 00:07:05,280
with what you're using in our

208
00:07:05,280 --> 00:07:07,120
infrastructure in production targeting

209
00:07:07,120 --> 00:07:08,319
adobe data

210
00:07:08,319 --> 00:07:10,800
so uh let us actually talk really fast

211
00:07:10,800 --> 00:07:13,599
about the magic behind the project

212
00:07:13,599 --> 00:07:14,720
now

213
00:07:14,720 --> 00:07:16,160
we basically can speed the project in

214
00:07:16,160 --> 00:07:18,639
three components data set feature

215
00:07:18,639 --> 00:07:20,960
extraction and machine learning so what

216
00:07:20,960 --> 00:07:22,720
i'm going to tackle today is actually

217
00:07:22,720 --> 00:07:24,240
the feature extraction antivirus is

218
00:07:24,240 --> 00:07:26,479
going to speak more about the

219
00:07:26,479 --> 00:07:29,680
data set and the machine learning part

220
00:07:29,680 --> 00:07:32,000
and i previously said that

221
00:07:32,000 --> 00:07:34,639
the magic behind this project that are

222
00:07:34,639 --> 00:07:36,160
actually the tags

223
00:07:36,160 --> 00:07:38,400
it's all about the text the training

224
00:07:38,400 --> 00:07:41,039
data is critical but the magic is in the

225
00:07:41,039 --> 00:07:43,520
text and my classifier is going to be as

226
00:07:43,520 --> 00:07:45,759
good as my tags are and of course as my

227
00:07:45,759 --> 00:07:47,919
data set is

228
00:07:47,919 --> 00:07:49,759
so what kind of tags we actually

229
00:07:49,759 --> 00:07:50,800
generate

230
00:07:50,800 --> 00:07:52,720
right anything that actually crosses my

231
00:07:52,720 --> 00:07:54,560
mind there a lot so let's not speak

232
00:07:54,560 --> 00:07:56,639
about tags and commands let's actually

233
00:07:56,639 --> 00:07:58,400
speak about myself what can i say about

234
00:07:58,400 --> 00:07:59,520
myself

235
00:07:59,520 --> 00:08:01,440
i can say i'm a male

236
00:08:01,440 --> 00:08:04,240
somewhere in my early mid 30s

237
00:08:04,240 --> 00:08:06,319
i wear glasses i'm starting to get a

238
00:08:06,319 --> 00:08:09,360
little bald here and things like that so

239
00:08:09,360 --> 00:08:10,800
more or less things like that i want to

240
00:08:10,800 --> 00:08:13,120
say about my commands as well

241
00:08:13,120 --> 00:08:16,240
so what can kind of tags i generate

242
00:08:16,240 --> 00:08:18,960
well i care about binaries that are

243
00:08:18,960 --> 00:08:21,120
executed i care about the paths from

244
00:08:21,120 --> 00:08:23,680
where they are executed i care if i saw

245
00:08:23,680 --> 00:08:25,039
that kind of

246
00:08:25,039 --> 00:08:26,720
string in the past

247
00:08:26,720 --> 00:08:29,440
i care with who it communicates if it

248
00:08:29,440 --> 00:08:30,960
communicates with public with internal

249
00:08:30,960 --> 00:08:34,159
ips with localhost i care about keywords

250
00:08:34,159 --> 00:08:35,679
i especially care about those little

251
00:08:35,679 --> 00:08:37,360
dashes that appear there in the commands

252
00:08:37,360 --> 00:08:39,440
in the parameters and i especially care

253
00:08:39,440 --> 00:08:41,360
about patterns and good patterns are

254
00:08:41,360 --> 00:08:45,039
actually game changers in this project

255
00:08:45,839 --> 00:08:48,320
okay so

256
00:08:48,320 --> 00:08:49,920
what i'm going to i'm going to go

257
00:08:49,920 --> 00:08:52,000
through all those uh

258
00:08:52,000 --> 00:08:54,000
special tags we're generating and go a

259
00:08:54,000 --> 00:08:55,600
little deep inside them and try to get

260
00:08:55,600 --> 00:08:57,200
you to understand what's actually

261
00:08:57,200 --> 00:08:58,480
happening there

262
00:08:58,480 --> 00:08:59,279
so

263
00:08:59,279 --> 00:09:01,519
what do we actually mean by binary tags

264
00:09:01,519 --> 00:09:03,760
well the concept actually pretty simple

265
00:09:03,760 --> 00:09:06,320
i'm checking if a particular command

266
00:09:06,320 --> 00:09:09,200
binary executable netcat let's say nc

267
00:09:09,200 --> 00:09:11,920
right if that is a part of the command

268
00:09:11,920 --> 00:09:14,240
itself so i basically made a list of the

269
00:09:14,240 --> 00:09:15,760
most interesting binaries from an

270
00:09:15,760 --> 00:09:18,000
operational security perspective and

271
00:09:18,000 --> 00:09:20,640
every time i analyze a command i check

272
00:09:20,640 --> 00:09:23,200
if that command is actually a part of

273
00:09:23,200 --> 00:09:25,680
the string i'm investigating

274
00:09:25,680 --> 00:09:27,519
if they are we basically add the text to

275
00:09:27,519 --> 00:09:29,839
that event just saying commanded the

276
00:09:29,839 --> 00:09:31,920
command itself so historically when i

277
00:09:31,920 --> 00:09:34,320
actually started to build this project

278
00:09:34,320 --> 00:09:36,160
those were the first feature we built

279
00:09:36,160 --> 00:09:38,959
our initial classifier one

280
00:09:38,959 --> 00:09:40,640
for our example today actually took a

281
00:09:40,640 --> 00:09:42,000
standard python interactive shell

282
00:09:42,000 --> 00:09:44,080
command and i'm pretty sure everybody's

283
00:09:44,080 --> 00:09:46,080
familiar with it it's it's uh really

284
00:09:46,080 --> 00:09:49,040
common right what we did we basically

285
00:09:49,040 --> 00:09:51,760
split it into words and uh

286
00:09:51,760 --> 00:09:54,080
then we check if that command python 3

287
00:09:54,080 --> 00:09:56,240
is actually present in the list of the

288
00:09:56,240 --> 00:09:58,959
keywords that was generated there that's

289
00:09:58,959 --> 00:10:01,680
that's what what we did now the code is

290
00:10:01,680 --> 00:10:04,160
pretty uh the logic on the code on the

291
00:10:04,160 --> 00:10:06,000
slide is the same as the code we have in

292
00:10:06,000 --> 00:10:07,279
the project but the code in the project

293
00:10:07,279 --> 00:10:09,440
is a little more complex but again it's

294
00:10:09,440 --> 00:10:11,360
basically doing the same thing

295
00:10:11,360 --> 00:10:13,680
now the same goes for keywords uh

296
00:10:13,680 --> 00:10:15,680
basically keywords represent a list of

297
00:10:15,680 --> 00:10:18,480
interesting keywords parameters we

298
00:10:18,480 --> 00:10:20,240
actually identified during our research

299
00:10:20,240 --> 00:10:22,320
phase so for for everything we thought

300
00:10:22,320 --> 00:10:24,079
it might have a security implication we

301
00:10:24,079 --> 00:10:26,160
created a list and we do the same check

302
00:10:26,160 --> 00:10:27,200
and as you can see it's pretty

303
00:10:27,200 --> 00:10:28,720
straightforward

304
00:10:28,720 --> 00:10:29,839
now

305
00:10:29,839 --> 00:10:31,600
it doesn't mean that this solution is

306
00:10:31,600 --> 00:10:33,839
not prone to false positives you can

307
00:10:33,839 --> 00:10:36,240
have binaries with different names or

308
00:10:36,240 --> 00:10:38,000
you can have some keywords parameters

309
00:10:38,000 --> 00:10:39,600
that have different impact and different

310
00:10:39,600 --> 00:10:40,640
commands

311
00:10:40,640 --> 00:10:42,480
if you guys are linux

312
00:10:42,480 --> 00:10:44,480
fellows you know that's actually true

313
00:10:44,480 --> 00:10:46,079
but this is not a problem because we

314
00:10:46,079 --> 00:10:49,279
generate so many other tags uh text that

315
00:10:49,279 --> 00:10:51,040
might have a negative impact on the

316
00:10:51,040 --> 00:10:54,000
classification itself can be helped by

317
00:10:54,000 --> 00:10:55,680
the other tags that were generated so

318
00:10:55,680 --> 00:10:58,000
again the magic it's in the text

319
00:10:58,000 --> 00:10:58,880
now

320
00:10:58,880 --> 00:11:00,560
all the static information we're

321
00:11:00,560 --> 00:11:02,320
presenting on the feature extraction

322
00:11:02,320 --> 00:11:05,440
part uh of uh are actually in a python

323
00:11:05,440 --> 00:11:07,440
file called constant pi

324
00:11:07,440 --> 00:11:09,279
so there is half of the magic of this

325
00:11:09,279 --> 00:11:11,440
project so basically the entire security

326
00:11:11,440 --> 00:11:13,360
subject matter expert security knowledge

327
00:11:13,360 --> 00:11:15,920
that we apply to this project and we

328
00:11:15,920 --> 00:11:18,240
made the supervised implementation is

329
00:11:18,240 --> 00:11:20,959
basically in that file you can modify

330
00:11:20,959 --> 00:11:23,120
the constant pi file at any time you can

331
00:11:23,120 --> 00:11:25,600
esta remove stuff repair stuff and so on

332
00:11:25,600 --> 00:11:27,200
and those tags are going to reflect

333
00:11:27,200 --> 00:11:29,279
directly in your results

334
00:11:29,279 --> 00:11:31,920
now it might be it might get ignored by

335
00:11:31,920 --> 00:11:33,440
the machine learning component until the

336
00:11:33,440 --> 00:11:36,560
model is going to be retraced

337
00:11:36,560 --> 00:11:37,680
okay

338
00:11:37,680 --> 00:11:39,120
we're doing the same thing for pad right

339
00:11:39,120 --> 00:11:40,959
we made a list of interesting paths for

340
00:11:40,959 --> 00:11:42,800
different linux processes binaries

341
00:11:42,800 --> 00:11:44,560
interesting that the directories and we

342
00:11:44,560 --> 00:11:46,560
check if those paths are present in the

343
00:11:46,560 --> 00:11:48,560
command line itself straightforward

344
00:11:48,560 --> 00:11:51,279
similar to the previous one okay so this

345
00:11:51,279 --> 00:11:53,360
is useful because it can highlight

346
00:11:53,360 --> 00:11:55,200
interesting activities like since the

347
00:11:55,200 --> 00:11:57,040
files binaries running from wrong

348
00:11:57,040 --> 00:11:58,880
locations so

349
00:11:58,880 --> 00:12:01,200
interesting activities and so on

350
00:12:01,200 --> 00:12:03,040
again all those elements used to

351
00:12:03,040 --> 00:12:04,560
generate tags are present in the

352
00:12:04,560 --> 00:12:07,200
constant.pi file and the list can can be

353
00:12:07,200 --> 00:12:10,399
updated on the fly

354
00:12:10,399 --> 00:12:11,279
so

355
00:12:11,279 --> 00:12:13,600
the development of this project was some

356
00:12:13,600 --> 00:12:15,519
sort of a modular approach like we

357
00:12:15,519 --> 00:12:16,880
initially started with interesting

358
00:12:16,880 --> 00:12:18,720
binaries and keywords we built a

359
00:12:18,720 --> 00:12:21,279
classifier we tested the classifier and

360
00:12:21,279 --> 00:12:23,360
we were disappointed

361
00:12:23,360 --> 00:12:25,360
so next we added an interesting patch

362
00:12:25,360 --> 00:12:26,959
and we did the same again like built a

363
00:12:26,959 --> 00:12:29,040
classifier tested the classifier one

364
00:12:29,040 --> 00:12:31,839
slightly better but still disappointing

365
00:12:31,839 --> 00:12:33,680
and then we added networking information

366
00:12:33,680 --> 00:12:34,959
so basically what the networking

367
00:12:34,959 --> 00:12:36,560
component does is to search if there is

368
00:12:36,560 --> 00:12:38,880
an ip information present in the command

369
00:12:38,880 --> 00:12:40,560
itself right and mark it as public

370
00:12:40,560 --> 00:12:42,720
private localhost whatever

371
00:12:42,720 --> 00:12:43,920
type it is

372
00:12:43,920 --> 00:12:44,959
now

373
00:12:44,959 --> 00:12:47,200
uh for you guys who are actually using

374
00:12:47,200 --> 00:12:49,040
endpoints to collect your data you know

375
00:12:49,040 --> 00:12:51,440
that sometimes the ip itself information

376
00:12:51,440 --> 00:12:52,639
is not going to be present in the

377
00:12:52,639 --> 00:12:54,240
command especially if you have a domain

378
00:12:54,240 --> 00:12:56,160
there right but there is a high chance

379
00:12:56,160 --> 00:12:57,200
that uh

380
00:12:57,200 --> 00:12:58,800
you have another field in the event

381
00:12:58,800 --> 00:13:00,560
generator that contains that uh ip

382
00:13:00,560 --> 00:13:02,880
information so what we do we concatenate

383
00:13:02,880 --> 00:13:05,440
our command with ip information and then

384
00:13:05,440 --> 00:13:08,240
we apply the model now imagine you have

385
00:13:08,240 --> 00:13:10,320
two reverse ssh tunnels in your

386
00:13:10,320 --> 00:13:11,760
infrastructure

387
00:13:11,760 --> 00:13:13,920
uh one is going towards a public ip one

388
00:13:13,920 --> 00:13:16,240
is going towards a private ip so which

389
00:13:16,240 --> 00:13:17,920
one would you investigate first which

390
00:13:17,920 --> 00:13:20,240
one most likely is not an operational

391
00:13:20,240 --> 00:13:21,519
activity right

392
00:13:21,519 --> 00:13:23,519
so basically we try to

393
00:13:23,519 --> 00:13:25,360
enhance the project with the same

394
00:13:25,360 --> 00:13:26,639
security

395
00:13:26,639 --> 00:13:28,320
mindset that

396
00:13:28,320 --> 00:13:31,120
we apply in our work by adding all kinds

397
00:13:31,120 --> 00:13:32,639
of different tags to it

398
00:13:32,639 --> 00:13:34,959
now the networking part had actually a

399
00:13:34,959 --> 00:13:36,880
significant impact in the accuracy of

400
00:13:36,880 --> 00:13:38,240
the classifier

401
00:13:38,240 --> 00:13:40,720
but this was still not good enough sorry

402
00:13:40,720 --> 00:13:44,720
something went slightly wrong here

403
00:13:45,680 --> 00:13:47,600
okay cool

404
00:13:47,600 --> 00:13:49,760
so um

405
00:13:49,760 --> 00:13:51,360
the last component actually who had the

406
00:13:51,360 --> 00:13:54,800
major impact uh in in our uh classifier

407
00:13:54,800 --> 00:13:57,199
was the rage expire right so

408
00:13:57,199 --> 00:13:59,360
the question was what other feature we

409
00:13:59,360 --> 00:14:00,800
can extract and of course the simple

410
00:14:00,800 --> 00:14:03,600
answer was regex so after tagging data

411
00:14:03,600 --> 00:14:06,079
with commands keywords paths network

412
00:14:06,079 --> 00:14:08,160
information we still didn't reach a

413
00:14:08,160 --> 00:14:10,240
production radio accuracy

414
00:14:10,240 --> 00:14:11,760
so the question was what we can do

415
00:14:11,760 --> 00:14:15,120
better and again the answer was regex

416
00:14:15,120 --> 00:14:17,440
uh now

417
00:14:17,440 --> 00:14:19,040
historically i've been working on

418
00:14:19,040 --> 00:14:21,680
content creation alerts notables rules

419
00:14:21,680 --> 00:14:23,680
development and i know that rejects can

420
00:14:23,680 --> 00:14:26,399
generate usually two main problems

421
00:14:26,399 --> 00:14:28,800
uh it can be too strict and it can

422
00:14:28,800 --> 00:14:30,800
ignore interesting stuff or it can be

423
00:14:30,800 --> 00:14:33,040
too broad and that actually might create

424
00:14:33,040 --> 00:14:35,680
a large number for false positives

425
00:14:35,680 --> 00:14:36,639
so

426
00:14:36,639 --> 00:14:38,880
what we actually did well we went with

427
00:14:38,880 --> 00:14:40,880
the second approach to with broad

428
00:14:40,880 --> 00:14:42,240
regexes and

429
00:14:42,240 --> 00:14:44,880
the chance of lots so false positive

430
00:14:44,880 --> 00:14:47,120
but we created many rejected hundreds of

431
00:14:47,120 --> 00:14:49,600
regresses so the idea was to catch as

432
00:14:49,600 --> 00:14:52,720
many features as possible just as a pti

433
00:14:52,720 --> 00:14:55,040
spawn netcat and map netcat flags and

434
00:14:55,040 --> 00:14:56,720
netflix and whatever flags you want

435
00:14:56,720 --> 00:14:59,040
there right and then take all the data

436
00:14:59,040 --> 00:15:00,800
will be generated put it through a

437
00:15:00,800 --> 00:15:03,040
classifier and let the classifier decide

438
00:15:03,040 --> 00:15:04,880
what's malicious or not like what tags

439
00:15:04,880 --> 00:15:07,199
and what pair of tag is going to

440
00:15:07,199 --> 00:15:08,480
turn red

441
00:15:08,480 --> 00:15:11,199
so even if we have a lot of

442
00:15:11,199 --> 00:15:14,399
potentially low confidence tags because

443
00:15:14,399 --> 00:15:16,320
of the regex and again we have many of

444
00:15:16,320 --> 00:15:17,519
them

445
00:15:17,519 --> 00:15:19,600
the regex tags added more and more

446
00:15:19,600 --> 00:15:21,040
context and the machine learning

447
00:15:21,040 --> 00:15:23,360
component behave better and better

448
00:15:23,360 --> 00:15:25,040
now from a technical perspective we

449
00:15:25,040 --> 00:15:27,120
encounter one major problem

450
00:15:27,120 --> 00:15:29,440
the feature extraction component became

451
00:15:29,440 --> 00:15:30,800
really slow

452
00:15:30,800 --> 00:15:33,839
so we switched from regex to regex to

453
00:15:33,839 --> 00:15:37,360
pr2 from google implementation and from

454
00:15:37,360 --> 00:15:39,279
a series execution like one registered

455
00:15:39,279 --> 00:15:40,880
attempt to parallel execution where

456
00:15:40,880 --> 00:15:42,480
actually have sets of projects in

457
00:15:42,480 --> 00:15:44,639
parallel with pipes between them and try

458
00:15:44,639 --> 00:15:46,320
to get the groups from there

459
00:15:46,320 --> 00:15:47,920
and this actually reduced the

460
00:15:47,920 --> 00:15:51,519
computational time almost uh eight times

461
00:15:51,519 --> 00:15:53,040
we had another bug last week which

462
00:15:53,040 --> 00:15:54,959
identified if we have actually multiple

463
00:15:54,959 --> 00:15:57,519
regexes which uh overlap and they are

464
00:15:57,519 --> 00:15:59,839
they are in the same line the same group

465
00:15:59,839 --> 00:16:01,199
they are going to suppress one another

466
00:16:01,199 --> 00:16:03,199
so we did the last rebalance of the

467
00:16:03,199 --> 00:16:05,360
regex data set then again that actually

468
00:16:05,360 --> 00:16:07,279
increased the overall confidence of the

469
00:16:07,279 --> 00:16:09,759
model now

470
00:16:09,759 --> 00:16:11,519
the same goes for the regex all the

471
00:16:11,519 --> 00:16:13,600
information is present in the constant

472
00:16:13,600 --> 00:16:16,079
pi file and anyone can add trajectories

473
00:16:16,079 --> 00:16:17,920
but in order for them to

474
00:16:17,920 --> 00:16:19,920
provide context for the machine learning

475
00:16:19,920 --> 00:16:22,800
model you need to retrain the model so

476
00:16:22,800 --> 00:16:24,959
some of you might need this project just

477
00:16:24,959 --> 00:16:26,480
for the tagging system antivirus is

478
00:16:26,480 --> 00:16:28,079
going to do a great physique about that

479
00:16:28,079 --> 00:16:28,880
right

480
00:16:28,880 --> 00:16:31,440
so you're going to need it for data set

481
00:16:31,440 --> 00:16:34,160
creation analysis rpas implementation or

482
00:16:34,160 --> 00:16:36,639
whatever you actually want and any

483
00:16:36,639 --> 00:16:38,639
modification you're going to do the

484
00:16:38,639 --> 00:16:40,800
constant pi file it's going to reflect

485
00:16:40,800 --> 00:16:43,279
right away in your results but the model

486
00:16:43,279 --> 00:16:45,519
might ignore new tags or might have a

487
00:16:45,519 --> 00:16:47,120
different results

488
00:16:47,120 --> 00:16:49,040
uh depending on how we actually enhance

489
00:16:49,040 --> 00:16:51,040
the tags right because the mentality

490
00:16:51,040 --> 00:16:52,320
behind the text has changed and the

491
00:16:52,320 --> 00:16:55,040
model might behave differently right now

492
00:16:55,040 --> 00:16:57,040
so for the classification part itself

493
00:16:57,040 --> 00:16:59,199
you need to retrain the model

494
00:16:59,199 --> 00:17:01,199
okay so now we're actually going to pass

495
00:17:01,199 --> 00:17:03,120
it to the very who's going to go a

496
00:17:03,120 --> 00:17:05,280
little deeper in the data set and model

497
00:17:05,280 --> 00:17:07,760
part of this project tiberius take it

498
00:17:07,760 --> 00:17:10,000
away

499
00:17:12,880 --> 00:17:14,000
in here

500
00:17:14,000 --> 00:17:15,839
we are going to start talking about data

501
00:17:15,839 --> 00:17:17,760
set and the machine learning model that

502
00:17:17,760 --> 00:17:19,599
we are using in our approach

503
00:17:19,599 --> 00:17:21,439
and i'm going to start by saying that

504
00:17:21,439 --> 00:17:23,039
probably the most important thing about

505
00:17:23,039 --> 00:17:25,359
our machine learning approach is that we

506
00:17:25,359 --> 00:17:28,000
are using supervised algorithms now

507
00:17:28,000 --> 00:17:30,640
these algorithms actually require

508
00:17:30,640 --> 00:17:33,280
label training data and the major

509
00:17:33,280 --> 00:17:35,520
problem is that a large-scale living of

510
00:17:35,520 --> 00:17:38,080
the land data set is really hard to find

511
00:17:38,080 --> 00:17:40,400
so we struggle with this and in the end

512
00:17:40,400 --> 00:17:42,880
we actually had to build our own data

513
00:17:42,880 --> 00:17:44,000
set

514
00:17:44,000 --> 00:17:45,600
almost from scratch

515
00:17:45,600 --> 00:17:48,160
in order to create our system

516
00:17:48,160 --> 00:17:49,840
now the leaving of the lane classifier

517
00:17:49,840 --> 00:17:52,720
is open source and we do provide the

518
00:17:52,720 --> 00:17:54,880
system with pre-trained models

519
00:17:54,880 --> 00:17:57,120
um but we are unable to share the data

520
00:17:57,120 --> 00:17:59,280
set so if you want to create a custom

521
00:17:59,280 --> 00:18:00,400
model

522
00:18:00,400 --> 00:18:01,679
this is no problem because you can

523
00:18:01,679 --> 00:18:04,320
actually do the same steps that we did

524
00:18:04,320 --> 00:18:07,679
and um it's not that hard it just

525
00:18:07,679 --> 00:18:10,160
requires some minor work and a little

526
00:18:10,160 --> 00:18:12,480
bit of effort but it's going to be worth

527
00:18:12,480 --> 00:18:14,400
it because you would be building a model

528
00:18:14,400 --> 00:18:16,240
that is you know suitable for your own

529
00:18:16,240 --> 00:18:17,600
data

530
00:18:17,600 --> 00:18:19,840
so how we got the data set

531
00:18:19,840 --> 00:18:21,919
we started by collecting the

532
00:18:21,919 --> 00:18:23,679
some leaving of the land examples from

533
00:18:23,679 --> 00:18:25,600
the sources that andre mentioned earlier

534
00:18:25,600 --> 00:18:28,720
in the presentation and we got somewhere

535
00:18:28,720 --> 00:18:32,080
around 1600 examples of malicious living

536
00:18:32,080 --> 00:18:34,480
of the land commands now we filtered

537
00:18:34,480 --> 00:18:35,600
them out

538
00:18:35,600 --> 00:18:37,440
we removed something you know command

539
00:18:37,440 --> 00:18:39,280
lines that were a little bit ambiguous

540
00:18:39,280 --> 00:18:42,160
so we wouldn't confuse the model and

541
00:18:42,160 --> 00:18:44,720
then we added to this data set another

542
00:18:44,720 --> 00:18:48,080
7.9 million examples of benign command

543
00:18:48,080 --> 00:18:51,520
lines which were actually you know

544
00:18:51,520 --> 00:18:53,039
extracted from

545
00:18:53,039 --> 00:18:54,880
logging our infrastructure for a couple

546
00:18:54,880 --> 00:18:57,520
of weeks removing duplicates

547
00:18:57,520 --> 00:19:00,080
and you know filter on filtering out

548
00:19:00,080 --> 00:19:01,760
some of the stuff that

549
00:19:01,760 --> 00:19:04,480
wasn't really relevant

550
00:19:04,480 --> 00:19:08,000
now we got a really skewed data set the

551
00:19:08,000 --> 00:19:10,000
ratio between negative examples and

552
00:19:10,000 --> 00:19:12,480
positive examples is somewhere around

553
00:19:12,480 --> 00:19:15,039
0.02

554
00:19:15,039 --> 00:19:17,200
but that's actually okay because this

555
00:19:17,200 --> 00:19:19,280
way we hope to reduce the number of

556
00:19:19,280 --> 00:19:21,760
faults positive during runtime for our

557
00:19:21,760 --> 00:19:23,440
model

558
00:19:23,440 --> 00:19:25,280
now you can imagine that if you are i

559
00:19:25,280 --> 00:19:27,520
mean if you go with um

560
00:19:27,520 --> 00:19:30,240
i know balanced data set and you get a

561
00:19:30,240 --> 00:19:32,080
false positive rate of i don't know two

562
00:19:32,080 --> 00:19:33,360
percent

563
00:19:33,360 --> 00:19:34,799
you have an infrastructure and you are

564
00:19:34,799 --> 00:19:36,880
logging uh what a hundred thousand

565
00:19:36,880 --> 00:19:39,440
command lines per per hour you'd

566
00:19:39,440 --> 00:19:41,600
actually generally be generating 2 000

567
00:19:41,600 --> 00:19:44,000
you know false positives every hour

568
00:19:44,000 --> 00:19:46,799
which somebody would have to go over

569
00:19:46,799 --> 00:19:49,360
filter out and establish if that's good

570
00:19:49,360 --> 00:19:52,000
or bad

571
00:19:52,000 --> 00:19:54,080
so instead i mean by doing so we

572
00:19:54,080 --> 00:19:56,080
actually got a model that is really is

573
00:19:56,080 --> 00:19:58,960
not that paranoid and is really reliable

574
00:19:58,960 --> 00:20:00,960
so when it says that something is uh is

575
00:20:00,960 --> 00:20:03,919
malicious it's worth investigating we

576
00:20:03,919 --> 00:20:05,520
actually have it into production and we

577
00:20:05,520 --> 00:20:09,120
only generate like four to six um

578
00:20:09,120 --> 00:20:10,559
you know

579
00:20:10,559 --> 00:20:13,120
detections per day uh some of them

580
00:20:13,120 --> 00:20:15,120
actually being the results of different

581
00:20:15,120 --> 00:20:16,960
penetration tests uh

582
00:20:16,960 --> 00:20:18,480
which are carried out by our team so

583
00:20:18,480 --> 00:20:21,919
they are not all false positives

584
00:20:21,919 --> 00:20:23,679
so to talk a little bit about the model

585
00:20:23,679 --> 00:20:26,400
we have 7.9 million examples

586
00:20:26,400 --> 00:20:29,200
we after going to the you know tagging

587
00:20:29,200 --> 00:20:31,919
strategy and adding the tags we actually

588
00:20:31,919 --> 00:20:34,400
added 7.5 million

589
00:20:34,400 --> 00:20:36,720
total tags to the data set and you can

590
00:20:36,720 --> 00:20:38,720
imagine that some of the command lines

591
00:20:38,720 --> 00:20:40,320
actually

592
00:20:40,320 --> 00:20:42,720
have multiple labels assigned to them

593
00:20:42,720 --> 00:20:45,440
uh while others don't have any con don't

594
00:20:45,440 --> 00:20:47,360
have any tags assigned to them because

595
00:20:47,360 --> 00:20:49,280
they don't know present anything

596
00:20:49,280 --> 00:20:51,120
interesting from the security point of

597
00:20:51,120 --> 00:20:52,400
view

598
00:20:52,400 --> 00:20:54,799
and the number of unique tags is

599
00:20:54,799 --> 00:20:57,679
500. now this is really good because we

600
00:20:57,679 --> 00:20:59,200
have a lot of

601
00:20:59,200 --> 00:21:01,760
training data and only i mean the

602
00:21:01,760 --> 00:21:04,000
feature space is somewhere around 500

603
00:21:04,000 --> 00:21:06,480
which is uh actually is really good in

604
00:21:06,480 --> 00:21:09,200
reducing data sparsity and you know

605
00:21:09,200 --> 00:21:11,039
preventing the classifier from

606
00:21:11,039 --> 00:21:12,720
overheating the training data you know

607
00:21:12,720 --> 00:21:14,880
generalizing better on previous glancing

608
00:21:14,880 --> 00:21:18,880
data so if we go to the next slide

609
00:21:18,880 --> 00:21:20,480
we are going to talk a little bit about

610
00:21:20,480 --> 00:21:23,039
the algorithms that we

611
00:21:23,039 --> 00:21:24,480
we tested

612
00:21:24,480 --> 00:21:26,799
so we did something which is called

613
00:21:26,799 --> 00:21:28,799
fivefold validation

614
00:21:28,799 --> 00:21:30,799
um this means that we

615
00:21:30,799 --> 00:21:33,360
took the training data we split it into

616
00:21:33,360 --> 00:21:35,679
five folds that actually try to preserve

617
00:21:35,679 --> 00:21:37,919
the same uh distribution

618
00:21:37,919 --> 00:21:39,440
between the negative and positive

619
00:21:39,440 --> 00:21:40,640
examples

620
00:21:40,640 --> 00:21:42,880
and we test train the classifier five

621
00:21:42,880 --> 00:21:43,760
times

622
00:21:43,760 --> 00:21:46,720
using every combination of four folds

623
00:21:46,720 --> 00:21:49,039
and then with each time we tested on the

624
00:21:49,039 --> 00:21:51,280
fifth fold which was kept aside

625
00:21:51,280 --> 00:21:53,360
so we rotated it through them and we

626
00:21:53,360 --> 00:21:55,200
computed the

627
00:21:55,200 --> 00:21:57,760
the mean the standard deviation and the

628
00:21:57,760 --> 00:22:00,720
training time for a single fold for free

629
00:22:00,720 --> 00:22:03,120
classifiers a random forest

630
00:22:03,120 --> 00:22:06,720
an svm and a logistic regressor

631
00:22:06,720 --> 00:22:09,039
now

632
00:22:09,039 --> 00:22:11,039
best results we got using the random

633
00:22:11,039 --> 00:22:14,720
forest and the sdm uh 0.95

634
00:22:14,720 --> 00:22:16,480
there's a smaller standard deviation for

635
00:22:16,480 --> 00:22:18,559
the random forest than for the svm which

636
00:22:18,559 --> 00:22:21,280
is also good and also it's a much

637
00:22:21,280 --> 00:22:23,039
smaller training time it's only 18

638
00:22:23,039 --> 00:22:26,400
minutes compared to 3.5 hours now there

639
00:22:26,400 --> 00:22:29,360
are two important aspects about the

640
00:22:29,360 --> 00:22:31,600
about the classifier so the random

641
00:22:31,600 --> 00:22:34,000
forest was the only classifier which was

642
00:22:34,000 --> 00:22:36,880
able to work with a sparse feature set

643
00:22:36,880 --> 00:22:38,400
which meant that we didn't have to

644
00:22:38,400 --> 00:22:39,600
convert

645
00:22:39,600 --> 00:22:40,480
the

646
00:22:40,480 --> 00:22:42,320
input features into

647
00:22:42,320 --> 00:22:45,520
a dense matrix and this greatly reduced

648
00:22:45,520 --> 00:22:47,679
the memory footprint required to train

649
00:22:47,679 --> 00:22:50,640
the model and the svm the results

650
00:22:50,640 --> 00:22:53,360
reported for dsem are

651
00:22:53,360 --> 00:22:55,120
reported after we

652
00:22:55,120 --> 00:22:56,799
actually applied something called the

653
00:22:56,799 --> 00:22:58,159
intel patch

654
00:22:58,159 --> 00:23:00,640
which made it possible to train the svm

655
00:23:00,640 --> 00:23:02,640
from scikit-learn on multiple threads at

656
00:23:02,640 --> 00:23:04,640
the same time

657
00:23:04,640 --> 00:23:07,760
and we got to 3.5 hours per fold

658
00:23:07,760 --> 00:23:08,880
otherwise

659
00:23:08,880 --> 00:23:10,960
we i mean we virtually weren't able to

660
00:23:10,960 --> 00:23:13,039
train it because it took a

661
00:23:13,039 --> 00:23:16,159
prohibitively amount of time to to to do

662
00:23:16,159 --> 00:23:19,039
a single fold

663
00:23:19,120 --> 00:23:21,120
on the next slide we are going to talk

664
00:23:21,120 --> 00:23:24,559
about one more tag that andre left out

665
00:23:24,559 --> 00:23:26,320
so

666
00:23:26,320 --> 00:23:28,880
we have to mention that while we do rely

667
00:23:28,880 --> 00:23:30,960
on the classifier to establish if a new

668
00:23:30,960 --> 00:23:32,799
example is good or bad

669
00:23:32,799 --> 00:23:35,039
we felt that i mean it

670
00:23:35,039 --> 00:23:36,720
won't be

671
00:23:36,720 --> 00:23:38,640
good to miss things that we know for

672
00:23:38,640 --> 00:23:40,720
sure that are bad they resemble

673
00:23:40,720 --> 00:23:42,960
something that's inside our

674
00:23:42,960 --> 00:23:44,720
data set and it has been labeled

675
00:23:44,720 --> 00:23:47,279
malicious so created a special tag for

676
00:23:47,279 --> 00:23:49,520
this the tag is called looks like

677
00:23:49,520 --> 00:23:50,799
non-lol

678
00:23:50,799 --> 00:23:52,559
and what it does it overrides the

679
00:23:52,559 --> 00:23:54,640
decision of the classifier for examples

680
00:23:54,640 --> 00:23:55,440
that

681
00:23:55,440 --> 00:23:57,360
were labeled as good

682
00:23:57,360 --> 00:23:58,720
but

683
00:23:58,720 --> 00:24:00,640
they resemble something that is bad in

684
00:24:00,640 --> 00:24:02,960
our data set

685
00:24:02,960 --> 00:24:05,279
so in order to generate the tag

686
00:24:05,279 --> 00:24:05,610
um

687
00:24:05,610 --> 00:24:07,679
[Music]

688
00:24:07,679 --> 00:24:10,480
we test every command uh we test a new

689
00:24:10,480 --> 00:24:12,159
command against all the commands inside

690
00:24:12,159 --> 00:24:14,400
our data set and we do a type of fuzzy

691
00:24:14,400 --> 00:24:16,480
comparison so it's not the exact string

692
00:24:16,480 --> 00:24:18,799
matching and there are two obvious

693
00:24:18,799 --> 00:24:20,880
options or the obvious option would be

694
00:24:20,880 --> 00:24:23,440
to use leverage time distance to measure

695
00:24:23,440 --> 00:24:26,240
this but this is very com the level

696
00:24:26,240 --> 00:24:28,240
strength is really complex it's a it has

697
00:24:28,240 --> 00:24:31,039
a n square complexity it runs really

698
00:24:31,039 --> 00:24:33,120
slow and there are many commands that we

699
00:24:33,120 --> 00:24:35,440
have to test against so instead of doing

700
00:24:35,440 --> 00:24:37,840
that we use a different score here a

701
00:24:37,840 --> 00:24:39,440
different my

702
00:24:39,440 --> 00:24:41,360
score function which is called blue

703
00:24:41,360 --> 00:24:43,200
scoring

704
00:24:43,200 --> 00:24:45,200
blue scoring has been previously used in

705
00:24:45,200 --> 00:24:47,360
machine translation

706
00:24:47,360 --> 00:24:49,600
to establish if a computer-generated

707
00:24:49,600 --> 00:24:51,120
sentence

708
00:24:51,120 --> 00:24:52,799
resembles the sentences that are

709
00:24:52,799 --> 00:24:54,640
translated by humans

710
00:24:54,640 --> 00:24:55,440
and

711
00:24:55,440 --> 00:24:57,520
it it's very fast

712
00:24:57,520 --> 00:24:59,520
and what it does is it returns a score

713
00:24:59,520 --> 00:25:02,080
of zero if two examples have nothing in

714
00:25:02,080 --> 00:25:04,320
common two sentences or two command

715
00:25:04,320 --> 00:25:07,120
lines in our case and it returns of a

716
00:25:07,120 --> 00:25:09,760
score of one if they are identical and

717
00:25:09,760 --> 00:25:11,360
everything in between depending on how

718
00:25:11,360 --> 00:25:13,760
different they are

719
00:25:13,760 --> 00:25:16,159
so after playing with the examples in

720
00:25:16,159 --> 00:25:18,000
our data set a little bit we actually

721
00:25:18,000 --> 00:25:21,360
set the threshold of 0.7 so when we do

722
00:25:21,360 --> 00:25:23,919
the test if a new command line

723
00:25:23,919 --> 00:25:25,360
it's you know

724
00:25:25,360 --> 00:25:27,360
resembles something or normal malicious

725
00:25:27,360 --> 00:25:30,640
data set with a score higher than 0.7 we

726
00:25:30,640 --> 00:25:32,880
assign this tag and we override the

727
00:25:32,880 --> 00:25:35,600
decision of the classifier directly you

728
00:25:35,600 --> 00:25:39,200
know setting the command line as being

729
00:25:39,200 --> 00:25:42,640
leaving of the lender bad

730
00:25:42,640 --> 00:25:44,799
on the next slide we'll be talking about

731
00:25:44,799 --> 00:25:48,000
um how you can install lol so there are

732
00:25:48,000 --> 00:25:51,279
two simple options the first one is uh

733
00:25:51,279 --> 00:25:53,840
the easy peep installation one

734
00:25:53,840 --> 00:25:56,320
you just do a pip install lc and it's

735
00:25:56,320 --> 00:25:58,000
going to install

736
00:25:58,000 --> 00:26:00,799
the package as well as train

737
00:26:00,799 --> 00:26:02,880
along with pre-trained models on your

738
00:26:02,880 --> 00:26:05,520
platform can be a virtual environment or

739
00:26:05,520 --> 00:26:07,440
it can be

740
00:26:07,440 --> 00:26:08,799
the global python installation it

741
00:26:08,799 --> 00:26:11,120
doesn't matter the second option

742
00:26:11,120 --> 00:26:14,000
you can clone the repository just create

743
00:26:14,000 --> 00:26:15,840
a virtual environment

744
00:26:15,840 --> 00:26:17,760
activate it and install the requirements

745
00:26:17,760 --> 00:26:19,520
and you have a working installation of

746
00:26:19,520 --> 00:26:21,520
living of the land now the advantage of

747
00:26:21,520 --> 00:26:23,520
doing this is that

748
00:26:23,520 --> 00:26:25,120
by doing so you'll be able to create

749
00:26:25,120 --> 00:26:27,200
your own model if you want to not to

750
00:26:27,200 --> 00:26:29,200
rely on the default ones that we are

751
00:26:29,200 --> 00:26:31,440
deliver providing with the system you

752
00:26:31,440 --> 00:26:33,360
can build your own date that you can

753
00:26:33,360 --> 00:26:34,799
build your own data set and train the

754
00:26:34,799 --> 00:26:37,679
classifier on on it

755
00:26:37,679 --> 00:26:39,200
on the next slide we'll be talking about

756
00:26:39,200 --> 00:26:41,360
how we can uh use the living of the land

757
00:26:41,360 --> 00:26:43,600
classifier so it's pretty easy

758
00:26:43,600 --> 00:26:46,559
we have an api for python we just have

759
00:26:46,559 --> 00:26:48,320
the import the main class and the

760
00:26:48,320 --> 00:26:52,080
platform type for the api from the api

761
00:26:52,080 --> 00:26:53,600
the next step is to create a new

762
00:26:53,600 --> 00:26:55,600
instance where you can specify if you

763
00:26:55,600 --> 00:26:59,360
are targeting linux or windows binaries

764
00:26:59,360 --> 00:27:01,039
command lines

765
00:27:01,039 --> 00:27:02,480
as andre said

766
00:27:02,480 --> 00:27:05,039
the windows model is not really stable

767
00:27:05,039 --> 00:27:06,799
it's not production ready but the new

768
00:27:06,799 --> 00:27:09,279
one works pretty well however we do

769
00:27:09,279 --> 00:27:11,919
provide both models so you can i mean

770
00:27:11,919 --> 00:27:13,679
you have a fully functional living on

771
00:27:13,679 --> 00:27:16,400
the line classify on your system

772
00:27:16,400 --> 00:27:19,200
next you need to get your data and then

773
00:27:19,200 --> 00:27:20,480
you can apply

774
00:27:20,480 --> 00:27:22,799
lol on the commands now you can apply it

775
00:27:22,799 --> 00:27:25,360
on a single command or you can apply it

776
00:27:25,360 --> 00:27:27,840
on a list of commands if you select a

777
00:27:27,840 --> 00:27:30,480
list of commands it's going to do

778
00:27:30,480 --> 00:27:32,640
multi-processing and it's going to

779
00:27:32,640 --> 00:27:35,120
optimize the the way they are

780
00:27:35,120 --> 00:27:37,840
labeled and

781
00:27:37,919 --> 00:27:38,799
um

782
00:27:38,799 --> 00:27:40,559
put to the classifier so it's going to

783
00:27:40,559 --> 00:27:42,159
be much faster

784
00:27:42,159 --> 00:27:44,320
if you have a lot of command lines just

785
00:27:44,320 --> 00:27:46,559
use the batch version and it will do

786
00:27:46,559 --> 00:27:48,480
everything for you

787
00:27:48,480 --> 00:27:50,399
regardless of what you are going to get

788
00:27:50,399 --> 00:27:52,399
two things you are going to get the

789
00:27:52,399 --> 00:27:54,880
classification

790
00:27:54,880 --> 00:27:56,320
and the tags

791
00:27:56,320 --> 00:27:58,080
the tags are the ones that are

792
00:27:58,080 --> 00:28:00,000
represented earlier so you have access

793
00:28:00,000 --> 00:28:01,200
to them

794
00:28:01,200 --> 00:28:03,039
you can use it on

795
00:28:03,039 --> 00:28:04,240
or to do your

796
00:28:04,240 --> 00:28:06,559
own analytics or do some searches with

797
00:28:06,559 --> 00:28:07,440
them

798
00:28:07,440 --> 00:28:08,480
whatever you want to do and the

799
00:28:08,480 --> 00:28:10,880
classification is almost binary so we

800
00:28:10,880 --> 00:28:13,679
have uh the good and bad labels which

801
00:28:13,679 --> 00:28:15,919
are pretty obvious and you also have the

802
00:28:15,919 --> 00:28:17,919
neutral label which we are going to talk

803
00:28:17,919 --> 00:28:20,480
about it during the demo

804
00:28:20,480 --> 00:28:22,159
so

805
00:28:22,159 --> 00:28:26,919
let me just share my screen for a moment

806
00:28:33,039 --> 00:28:34,559
okay

807
00:28:34,559 --> 00:28:37,039
can you can you see my screen

808
00:28:37,039 --> 00:28:39,600
okay perfect

809
00:28:39,600 --> 00:28:41,679
so what i did was create a virtual

810
00:28:41,679 --> 00:28:43,440
environment just installed the jupyter

811
00:28:43,440 --> 00:28:45,600
notebook because it's much easier to do

812
00:28:45,600 --> 00:28:47,200
the show and tell on it

813
00:28:47,200 --> 00:28:49,440
and i'm going to use the package

814
00:28:49,440 --> 00:28:50,720
installation i'm going to run this

815
00:28:50,720 --> 00:28:52,240
command over here it's going to go

816
00:28:52,240 --> 00:28:53,919
pretty fast because it's already cached

817
00:28:53,919 --> 00:28:55,600
on my system

818
00:28:55,600 --> 00:28:57,200
i'm going to create a new instance that

819
00:28:57,200 --> 00:29:00,799
handles linux command lines

820
00:29:01,360 --> 00:29:03,039
and after this

821
00:29:03,039 --> 00:29:04,880
we're actually going to run the leaving

822
00:29:04,880 --> 00:29:06,240
of the line classifier on multiple

823
00:29:06,240 --> 00:29:08,799
commands one at a time so we can easily

824
00:29:08,799 --> 00:29:10,159
read the results

825
00:29:10,159 --> 00:29:13,840
i'm going to do this really fast

826
00:29:14,880 --> 00:29:16,240
okay so

827
00:29:16,240 --> 00:29:18,880
what happened it took the command line

828
00:29:18,880 --> 00:29:20,880
it tried to describe it with tags so you

829
00:29:20,880 --> 00:29:22,720
see that it assigned the ip loop back

830
00:29:22,720 --> 00:29:25,600
here because you have the loopback ip of

831
00:29:25,600 --> 00:29:26,720
course

832
00:29:26,720 --> 00:29:27,840
um

833
00:29:27,840 --> 00:29:30,880
it so it also saw a public ip here the

834
00:29:30,880 --> 00:29:34,080
it contains ssh it has the minus r

835
00:29:34,080 --> 00:29:36,159
keyword and there's also this is the

836
00:29:36,159 --> 00:29:37,919
result of a regular expression that was

837
00:29:37,919 --> 00:29:39,600
triggered which

838
00:29:39,600 --> 00:29:42,320
you know targets ssh with minus r

839
00:29:42,320 --> 00:29:45,760
together all together right

840
00:29:45,840 --> 00:29:47,600
it did this for all the commands so this

841
00:29:47,600 --> 00:29:50,640
one has sudo which install

842
00:29:50,640 --> 00:29:53,120
uh ssh minus d and it also looks like a

843
00:29:53,120 --> 00:29:55,039
non-ripping of the land command because

844
00:29:55,039 --> 00:29:58,000
it resembles something in our data set

845
00:29:58,000 --> 00:30:00,080
this one is pretty interesting

846
00:30:00,080 --> 00:30:02,480
so you're going to see that this apt-get

847
00:30:02,480 --> 00:30:05,200
install python tray was python 3 was

848
00:30:05,200 --> 00:30:07,120
labeled as neutral

849
00:30:07,120 --> 00:30:09,039
so this is because we actually assigned

850
00:30:09,039 --> 00:30:12,080
less than five tags on this command line

851
00:30:12,080 --> 00:30:14,320
and when this happens the classifier

852
00:30:14,320 --> 00:30:16,480
doesn't have enough text to rely on to

853
00:30:16,480 --> 00:30:19,120
make a good decision so we just

854
00:30:19,120 --> 00:30:21,039
remove all the tags enable the example

855
00:30:21,039 --> 00:30:23,360
as being neutral now this is normal if

856
00:30:23,360 --> 00:30:26,080
you have small command lines and common

857
00:30:26,080 --> 00:30:27,760
command lines that you are using on a

858
00:30:27,760 --> 00:30:29,120
daily basis

859
00:30:29,120 --> 00:30:31,919
but if you have a really long string and

860
00:30:31,919 --> 00:30:34,880
you know it gets labeled as neutral it

861
00:30:34,880 --> 00:30:36,880
might indicate that you are dealing with

862
00:30:36,880 --> 00:30:39,120
something obfuscated maybe

863
00:30:39,120 --> 00:30:40,880
and that's why we weren't able to

864
00:30:40,880 --> 00:30:42,960
extract any text from it and you might

865
00:30:42,960 --> 00:30:44,640
want to look into this we do have

866
00:30:44,640 --> 00:30:47,039
another project that targets obfuscation

867
00:30:47,039 --> 00:30:50,720
uh but will not be presented yet here

868
00:30:50,720 --> 00:30:53,279
so most of them have the

869
00:30:53,279 --> 00:30:54,880
looks like known lol

870
00:30:54,880 --> 00:30:55,760
tag

871
00:30:55,760 --> 00:30:57,840
which means that the classifier the

872
00:30:57,840 --> 00:30:59,679
decision of the classifier was overrated

873
00:30:59,679 --> 00:31:02,240
by this flag but if we scroll down we

874
00:31:02,240 --> 00:31:04,399
are also going to have a lot of commands

875
00:31:04,399 --> 00:31:07,440
that were labeled as being bad

876
00:31:07,440 --> 00:31:08,960
but the classifier has never seen them

877
00:31:08,960 --> 00:31:10,799
before so they don't resemble anything

878
00:31:10,799 --> 00:31:13,120
in our training set

879
00:31:13,120 --> 00:31:15,120
but again they were labeled pretty

880
00:31:15,120 --> 00:31:17,519
pretty good

881
00:31:17,519 --> 00:31:19,360
um

882
00:31:19,360 --> 00:31:22,159
two more things about um

883
00:31:22,159 --> 00:31:23,320
about this uh

884
00:31:23,320 --> 00:31:26,429
[Music]

885
00:31:27,120 --> 00:31:29,200
utility so

886
00:31:29,200 --> 00:31:31,360
if you feel lucky you can actually

887
00:31:31,360 --> 00:31:34,000
change the constants that define the

888
00:31:34,000 --> 00:31:35,760
keywords and the command lines and the

889
00:31:35,760 --> 00:31:38,080
paths that we are targeting so there's a

890
00:31:38,080 --> 00:31:40,480
file which is called constants.pi it's

891
00:31:40,480 --> 00:31:42,240
in the main distribution of the

892
00:31:42,240 --> 00:31:43,600
application

893
00:31:43,600 --> 00:31:45,039
you can see here the list of linux

894
00:31:45,039 --> 00:31:48,320
commands the interesting linux paths

895
00:31:48,320 --> 00:31:51,120
the keywords that we are looking for

896
00:31:51,120 --> 00:31:52,960
and there's also this really large

897
00:31:52,960 --> 00:31:54,880
variable here with regular expression

898
00:31:54,880 --> 00:31:57,360
i'm going to get back this one on the

899
00:31:57,360 --> 00:32:00,000
bottom of the

900
00:32:00,000 --> 00:32:01,919
source you're going to see the same for

901
00:32:01,919 --> 00:32:03,279
windows commands

902
00:32:03,279 --> 00:32:04,000
now

903
00:32:04,000 --> 00:32:05,760
those regular expressions you cannot

904
00:32:05,760 --> 00:32:08,320
edit them in place it's impossible

905
00:32:08,320 --> 00:32:10,399
believe me i tried but we do have

906
00:32:10,399 --> 00:32:12,559
another utility here

907
00:32:12,559 --> 00:32:15,840
well you can actually set or i mean

908
00:32:15,840 --> 00:32:17,840
introduce one regular expression at a

909
00:32:17,840 --> 00:32:18,720
time

910
00:32:18,720 --> 00:32:20,799
set the label for it and after you

911
00:32:20,799 --> 00:32:23,679
define all of them it's going to pack

912
00:32:23,679 --> 00:32:26,799
uh make the uh the to pack all the

913
00:32:26,799 --> 00:32:28,960
regular expressions and make them run in

914
00:32:28,960 --> 00:32:30,720
parallel so they would be really really

915
00:32:30,720 --> 00:32:32,880
fast

916
00:32:32,880 --> 00:32:35,279
i guess this is the my poc

917
00:32:35,279 --> 00:32:36,480
i'm gonna

918
00:32:36,480 --> 00:32:39,279
get that give back control to andre so

919
00:32:39,279 --> 00:32:41,440
we can speak about

920
00:32:41,440 --> 00:32:44,399
how you can actually use the lifting of

921
00:32:44,399 --> 00:32:47,439
the line classifier

922
00:32:51,840 --> 00:32:54,320
okay this is this is not you are not

923
00:32:54,320 --> 00:32:56,799
sharing the right screen i guess

924
00:32:56,799 --> 00:33:00,000
so i'm going to start speaking anyway

925
00:33:00,000 --> 00:33:02,399
so you can use it as a standalone tool

926
00:33:02,399 --> 00:33:04,799
um you can integrate it with an er

927
00:33:04,799 --> 00:33:08,159
solution or go over some you know cm

928
00:33:08,159 --> 00:33:10,240
logs and label command lines as being

929
00:33:10,240 --> 00:33:11,440
good or bad

930
00:33:11,440 --> 00:33:13,120
you can use the text for analytics as i

931
00:33:13,120 --> 00:33:14,559
said before

932
00:33:14,559 --> 00:33:15,279
but

933
00:33:15,279 --> 00:33:17,919
the way we use it sometimes it's within

934
00:33:17,919 --> 00:33:19,679
a project a larger project which is

935
00:33:19,679 --> 00:33:23,279
called one-stop anomaly shop osas

936
00:33:23,279 --> 00:33:26,320
this is a previously open source project

937
00:33:26,320 --> 00:33:29,120
um that i mean while you know living of

938
00:33:29,120 --> 00:33:31,120
the land actually looks at at every

939
00:33:31,120 --> 00:33:33,039
command line on its own without any

940
00:33:33,039 --> 00:33:35,679
context what those does

941
00:33:35,679 --> 00:33:37,440
it is able to

942
00:33:37,440 --> 00:33:40,799
take into account multiple

943
00:33:40,960 --> 00:33:43,279
multiple features that describe you know

944
00:33:43,279 --> 00:33:45,440
the context in which that command line

945
00:33:45,440 --> 00:33:48,960
was run so it looks at process names

946
00:33:48,960 --> 00:33:50,159
users

947
00:33:50,159 --> 00:33:53,279
of environment variables and many many

948
00:33:53,279 --> 00:33:55,039
other things

949
00:33:55,039 --> 00:33:56,559
the on the next slide we have the

950
00:33:56,559 --> 00:33:58,559
architecture for oss is pretty

951
00:33:58,559 --> 00:34:00,799
straightforward so it just requires a

952
00:34:00,799 --> 00:34:03,120
data source which can be a csv or a json

953
00:34:03,120 --> 00:34:05,120
data source or it can you know come from

954
00:34:05,120 --> 00:34:08,239
splunk elasticsearch anything you want

955
00:34:08,239 --> 00:34:10,639
and then there's the processing pipeline

956
00:34:10,639 --> 00:34:12,560
and this process processing pipeline is

957
00:34:12,560 --> 00:34:15,440
fully configurable

958
00:34:15,440 --> 00:34:16,399
and

959
00:34:16,399 --> 00:34:18,800
it kind of it kind of applies the same

960
00:34:18,800 --> 00:34:20,320
strategy as the leaving of the land

961
00:34:20,320 --> 00:34:22,960
classifier and it tries to describe

962
00:34:22,960 --> 00:34:25,199
every event with labels

963
00:34:25,199 --> 00:34:26,800
and it's going to

964
00:34:26,800 --> 00:34:30,079
handle numeric labels uh based on the

965
00:34:30,079 --> 00:34:31,760
mean and standard deviation of a value

966
00:34:31,760 --> 00:34:34,800
so it might add so if you are you say

967
00:34:34,800 --> 00:34:37,119
you input cpu usage or memory usage you

968
00:34:37,119 --> 00:34:39,918
might get labels such as uh extremely

969
00:34:39,918 --> 00:34:43,520
large cpu usage or normal cpu usage or

970
00:34:43,520 --> 00:34:46,719
on previously unseen cpu usage

971
00:34:46,719 --> 00:34:48,399
it does the same thing for

972
00:34:48,399 --> 00:34:51,199
multinomial fields which are text fields

973
00:34:51,199 --> 00:34:53,679
that um you know don't take an infinite

974
00:34:53,679 --> 00:34:56,879
number of values uh such as

975
00:34:56,879 --> 00:34:59,440
you know usernames or process names and

976
00:34:59,440 --> 00:35:02,000
you're going to get labels such as rare

977
00:35:02,000 --> 00:35:05,359
user rare process and so on

978
00:35:05,359 --> 00:35:06,960
and you can also apply the living of the

979
00:35:06,960 --> 00:35:09,359
land it can combine any types of fields

980
00:35:09,359 --> 00:35:11,599
between them so it's it might you might

981
00:35:11,599 --> 00:35:13,520
say you want to combine the parent

982
00:35:13,520 --> 00:35:16,160
process with the command line or the the

983
00:35:16,160 --> 00:35:17,680
parent process the username and it's

984
00:35:17,680 --> 00:35:20,720
going to say rare parent process user

985
00:35:20,720 --> 00:35:23,839
combination you get this label as well

986
00:35:23,839 --> 00:35:25,440
so it also

987
00:35:25,440 --> 00:35:27,280
you you might you have the option to

988
00:35:27,280 --> 00:35:28,960
apply the living of the line classifier

989
00:35:28,960 --> 00:35:30,400
on any

990
00:35:30,400 --> 00:35:32,320
text field

991
00:35:32,320 --> 00:35:34,000
obviously you want to run it for command

992
00:35:34,000 --> 00:35:36,240
lines not for no plain text or source

993
00:35:36,240 --> 00:35:38,240
code or anything else because it's not

994
00:35:38,240 --> 00:35:41,680
going to work as expected

995
00:35:43,680 --> 00:35:45,440
when you do that you are actually going

996
00:35:45,440 --> 00:35:47,520
to get the binary decision of the

997
00:35:47,520 --> 00:35:50,240
classifier in the text that also is

998
00:35:50,240 --> 00:35:52,880
generated generating as well as all the

999
00:35:52,880 --> 00:35:55,119
other tags that were you that are used

1000
00:35:55,119 --> 00:35:57,760
to describe the process um

1001
00:35:57,760 --> 00:35:59,760
by the leaving of the line classifier

1002
00:35:59,760 --> 00:36:02,560
and finally oss is going to do an on my

1003
00:36:02,560 --> 00:36:05,200
detection it has the option to do

1004
00:36:05,200 --> 00:36:07,040
unsupervised anomaly detection with

1005
00:36:07,040 --> 00:36:09,839
using using the different algorithms we

1006
00:36:09,839 --> 00:36:12,480
support auto encoder local outlier

1007
00:36:12,480 --> 00:36:15,280
factor statistical engram and frequent

1008
00:36:15,280 --> 00:36:17,440
items and mining and you also have the

1009
00:36:17,440 --> 00:36:18,960
option to do supervised analytic

1010
00:36:18,960 --> 00:36:20,400
detection

1011
00:36:20,400 --> 00:36:22,880
using any supervised classifier which is

1012
00:36:22,880 --> 00:36:24,560
present in scikit-learn you can

1013
00:36:24,560 --> 00:36:26,160
configure it in the um

1014
00:36:26,160 --> 00:36:28,000
in the pipeline but you are going to

1015
00:36:28,000 --> 00:36:30,240
provide to your you are going to be

1016
00:36:30,240 --> 00:36:32,640
forced to provide training data to it uh

1017
00:36:32,640 --> 00:36:34,640
which means that you need to add another

1018
00:36:34,640 --> 00:36:37,200
label another column to your data set

1019
00:36:37,200 --> 00:36:39,359
which is going to say if an example is

1020
00:36:39,359 --> 00:36:40,800
good or bad

1021
00:36:40,800 --> 00:36:42,400
and you can train the classifier this

1022
00:36:42,400 --> 00:36:43,760
way

1023
00:36:43,760 --> 00:36:46,800
well i guess that's it we

1024
00:36:46,800 --> 00:36:48,880
thank you everybody for joining us

1025
00:36:48,880 --> 00:36:50,800
thank you so much for your demo it was

1026
00:36:50,800 --> 00:36:53,040
really exciting to watch it live

1027
00:36:53,040 --> 00:36:54,800
and i love the name you gave to some of

1028
00:36:54,800 --> 00:36:57,440
those tags i'm also stealing your regex

1029
00:36:57,440 --> 00:36:59,200
joke i hope you don't mind

1030
00:36:59,200 --> 00:37:00,880
i mean you open source the software

1031
00:37:00,880 --> 00:37:04,079
might as well open source the joke

1032
00:37:04,079 --> 00:37:06,240
i'm i'm a bit curious have you ever

1033
00:37:06,240 --> 00:37:08,640
tested your software against any

1034
00:37:08,640 --> 00:37:12,160
honeypots or against any real commands

1035
00:37:12,160 --> 00:37:15,200
coming from actual attacks

1036
00:37:15,200 --> 00:37:17,839
yes so we have used it also

1037
00:37:17,839 --> 00:37:19,760
for internal adobe data

1038
00:37:19,760 --> 00:37:23,119
we have a flavor of this model that's

1039
00:37:23,119 --> 00:37:25,040
targeting inside adobe data and we're

1040
00:37:25,040 --> 00:37:26,320
using adobe

1041
00:37:26,320 --> 00:37:28,800
incident data in it and we have the

1042
00:37:28,800 --> 00:37:30,640
technical couple of uh

1043
00:37:30,640 --> 00:37:32,800
tests from penetration testers from our

1044
00:37:32,800 --> 00:37:36,480
purple team and also something regarding

1045
00:37:36,480 --> 00:37:37,920
internal incidents

1046
00:37:37,920 --> 00:37:39,760
i cannot go more in detail than that but

1047
00:37:39,760 --> 00:37:41,040
yeah

1048
00:37:41,040 --> 00:37:44,480
awesome um i was wondering and you sort

1049
00:37:44,480 --> 00:37:47,359
of uh also segued into my other question

1050
00:37:47,359 --> 00:37:49,359
is there anything interesting that

1051
00:37:49,359 --> 00:37:51,119
you've left out of your presentation not

1052
00:37:51,119 --> 00:37:52,640
knowing whether you'd have enough time

1053
00:37:52,640 --> 00:37:55,520
to develop into it

1054
00:37:55,520 --> 00:37:57,359
uh yeah of course i guess the windows

1055
00:37:57,359 --> 00:37:59,839
model uh so the windows mod we actually

1056
00:37:59,839 --> 00:38:02,240
did have a lot of work on it we have a

1057
00:38:02,240 --> 00:38:04,400
lot of text we generated

1058
00:38:04,400 --> 00:38:06,320
uh the problem is the data set we're

1059
00:38:06,320 --> 00:38:09,040
working on it's not

1060
00:38:09,040 --> 00:38:11,359
big enough to have a stable model

1061
00:38:11,359 --> 00:38:12,160
so

1062
00:38:12,160 --> 00:38:14,160
i know most likely some they will come

1063
00:38:14,160 --> 00:38:16,320
back again having also the windows model

1064
00:38:16,320 --> 00:38:17,920
apparently but for the time being if

1065
00:38:17,920 --> 00:38:19,839
anybody actually needs to tag windows

1066
00:38:19,839 --> 00:38:22,800
data with this platform it should do the

1067
00:38:22,800 --> 00:38:24,560
trick

1068
00:38:24,560 --> 00:38:27,599
have you considered doing a sort of call

1069
00:38:27,599 --> 00:38:29,520
for participation in the open source

1070
00:38:29,520 --> 00:38:31,839
community since you open source the tool

1071
00:38:31,839 --> 00:38:34,480
it would be a great time to also say

1072
00:38:34,480 --> 00:38:36,320
folks let's gather some data together

1073
00:38:36,320 --> 00:38:37,680
especially if you're using this

1074
00:38:37,680 --> 00:38:39,359
especially if you're using it you know

1075
00:38:39,359 --> 00:38:42,079
against real commands coming from places

1076
00:38:42,079 --> 00:38:44,560
where commands usually come from

1077
00:38:44,560 --> 00:38:45,680
well

1078
00:38:45,680 --> 00:38:47,200
this is one of the main reasons we're

1079
00:38:47,200 --> 00:38:49,599
actually doing this talk because we do

1080
00:38:49,599 --> 00:38:50,560
want to

1081
00:38:50,560 --> 00:38:53,280
get some traction and uh get people

1082
00:38:53,280 --> 00:38:55,839
working with us so hika if you guys try

1083
00:38:55,839 --> 00:38:58,079
the model and it fails because things

1084
00:38:58,079 --> 00:38:59,520
fails in your life as i mentioned in my

1085
00:38:59,520 --> 00:39:01,680
previous slides uh

1086
00:39:01,680 --> 00:39:03,599
just send us the command like a change

1087
00:39:03,599 --> 00:39:05,200
format you don't have to have the actual

1088
00:39:05,200 --> 00:39:08,000
indicators in it and we'll try to make

1089
00:39:08,000 --> 00:39:10,000
it to fit the model or we can work

1090
00:39:10,000 --> 00:39:11,520
together to do that

1091
00:39:11,520 --> 00:39:13,040
and that's actually we're sharing the

1092
00:39:13,040 --> 00:39:15,280
github link there because we really

1093
00:39:15,280 --> 00:39:16,800
wanted back to check it out and if you

1094
00:39:16,800 --> 00:39:19,280
like it please give us a start

1095
00:39:19,280 --> 00:39:21,440
i think we should uh use this as one of

1096
00:39:21,440 --> 00:39:23,280
the models for the conference things

1097
00:39:23,280 --> 00:39:25,839
failed but gather the data

1098
00:39:25,839 --> 00:39:28,720
yeah that's life

1099
00:39:28,720 --> 00:39:31,440
thank you very much andre thank you

1100
00:39:31,440 --> 00:39:32,400
um

1101
00:39:32,400 --> 00:39:34,400
for the presentation and the awesome

1102
00:39:34,400 --> 00:39:36,160
demo and i really hope you get a lot of

1103
00:39:36,160 --> 00:39:39,200
traction on your github i'm a huge uh

1104
00:39:39,200 --> 00:39:40,480
cheerleader for the open source

1105
00:39:40,480 --> 00:39:42,800
community and for collaboration that

1106
00:39:42,800 --> 00:39:46,000
spans not countries but even continents

1107
00:39:46,000 --> 00:39:48,400
and if you get commands in languages

1108
00:39:48,400 --> 00:39:50,160
which the project doesn't know about yet

1109
00:39:50,160 --> 00:39:52,240
that's going to be an even better

1110
00:39:52,240 --> 00:39:55,799
challenge to tackle

1111
00:39:56,240 --> 00:39:57,440
thank you

1112
00:39:57,440 --> 00:40:01,560
thank you thank you

