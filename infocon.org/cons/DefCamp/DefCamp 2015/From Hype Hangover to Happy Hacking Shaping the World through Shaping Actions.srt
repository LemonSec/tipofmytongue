1
00:00:00,000 --> 00:00:08,099
hello bloggers you are as did I said
alright okay good day in Romanian I'm

2
00:00:08,099 --> 00:00:12,740
told I try to learn a little bit about
the language and the culture before I

3
00:00:12,740 --> 00:00:17,480
come to a place so I apologize for not
knowing a little bit more Romanian

4
00:00:17,480 --> 00:00:22,310
certainly you guys all know a lot of my
language probably even a lot more than I

5
00:00:22,310 --> 00:00:29,609
do and in some cases so today I want to
talk to you about getting past when I

6
00:00:29,609 --> 00:00:34,380
call the hype hangover that is that
feeling after a kind of a journalistic

7
00:00:34,380 --> 00:00:40,010
story or after a big news event or after
a buzz around the community where you

8
00:00:40,010 --> 00:00:44,989
just your left help feeling a little bit
lacking a little bit wanting from more

9
00:00:44,989 --> 00:00:49,709
substance or at least not the headaches
that some of the sites of the exposure

10
00:00:49,710 --> 00:00:54,829
has caused and get back to what I hope
we all want to do which is more happy

11
00:00:54,829 --> 00:00:59,170
hacking this is certainly a great venue
fracking they've got a huge CTF that's

12
00:00:59,170 --> 00:01:02,960
very very popular worldwide I think they
had about a thousand teams competing

13
00:01:02,960 --> 00:01:08,360
from over 30 different countries
something like that there's also the

14
00:01:08,360 --> 00:01:13,549
hack the bank which is a really good
contest to get the opportunity to be one

15
00:01:13,549 --> 00:01:18,970
of the top researchers are filled four
day delay part of you jack who famously

16
00:01:18,970 --> 00:01:27,039
actin ATM and they called the jackpot
doing so I'm from Atlanta we had the

17
00:01:27,040 --> 00:01:34,100
Olympics in 1996 and we had an incident
there where a bomb went off in a park I

18
00:01:34,100 --> 00:01:37,908
was actually volunteering in the
Olympics and so when I read about this

19
00:01:37,909 --> 00:01:42,619
news story I was particularly kind of
affected because I felt you know I could

20
00:01:42,619 --> 00:01:47,759
have been there that could have been me
and so as I was preparing for the talk

21
00:01:47,759 --> 00:01:53,320
the Paris events reminded me this this
event that happened in my life

22
00:01:53,320 --> 00:01:59,079
interestingly in this story there was
somebody who found the bomb before it

23
00:01:59,079 --> 00:02:04,619
went off and he was able to alert the
police they came in the the bomb

24
00:02:04,619 --> 00:02:08,300
technician started diffusing it went off
to a couple of police officers were

25
00:02:08,300 --> 00:02:12,800
killed but they say he saved over a
hundred people's lives and so he's

26
00:02:12,800 --> 00:02:13,460
really really

27
00:02:13,460 --> 00:02:19,770
did a great thing an amazing thing but
it seems a little bit too coincidental

28
00:02:19,770 --> 00:02:23,950
that he came in and and found his bomb
right before it went off alerted the

29
00:02:23,950 --> 00:02:29,489
police and so a lot of people started
speculating you know did he somehow

30
00:02:29,490 --> 00:02:35,560
plant the bomb and he was looking for
attention what happened there a few days

31
00:02:35,560 --> 00:02:42,390
later the Atlanta journal-constitution
ran a story that said FBI suspects that

32
00:02:42,390 --> 00:02:49,600
hero guard may have planted a bomb after
the newspaper published the story of

33
00:02:49,600 --> 00:02:54,420
course media went crazy the public went
crazy everybody was looking into his

34
00:02:54,420 --> 00:03:01,140
past life his his career his history
they found some things that they looked

35
00:03:01,140 --> 00:03:06,829
at it said maybe this is evidence that
he's not a good security guard that he

36
00:03:06,830 --> 00:03:13,360
was trying to go for families tried to
use this for publicity and there was a

37
00:03:13,360 --> 00:03:18,430
public outcry to arrest him eventually
they're the FBI did arrest him they held

38
00:03:18,430 --> 00:03:23,290
him for several months built the case
ultimately end up letting him go because

39
00:03:23,290 --> 00:03:28,260
they couldn't build a strong case but
the hype surrounding this media event

40
00:03:28,260 --> 00:03:37,459
ended up mister ruining his career and
his life after the news article hit the

41
00:03:37,460 --> 00:03:41,540
hype cycle kind of took over and I was
out of anyone's control from that point

42
00:03:41,540 --> 00:03:47,170
on it was kind of at the whim of whoever
had the loudest voice to announce the

43
00:03:47,170 --> 00:03:55,970
biggest story and as I was looking at
the hype cycle around Vegas earlier this

44
00:03:55,970 --> 00:04:01,630
year and if you're out at DEFCON black
cat besides it was hard to get away from

45
00:04:01,630 --> 00:04:06,410
the story of dr. Charlie Miller and
Chris Walla sec

46
00:04:06,410 --> 00:04:12,670
packing a jeep I'm sure most of us heard
about that especially if you're in Vegas

47
00:04:12,670 --> 00:04:16,140
especially if you're in the United
States but I'm sure it made its way even

48
00:04:16,140 --> 00:04:22,400
over here especially the security
community being as small as it is and I

49
00:04:22,400 --> 00:04:26,739
wanted to kind of talk about this and
related back to this hype cycle

50
00:04:26,740 --> 00:04:33,810
certainly don't know dr. Charlie Miller
and Chris valla sec

51
00:04:33,810 --> 00:04:40,550
spent a couple of years now looking into
automotive cybersecurity and looking at

52
00:04:40,550 --> 00:04:45,400
some of the areas where potentially a
remote attacker could compromise the

53
00:04:45,400 --> 00:04:49,599
vehicle and get it to do things that it
shouldn't be able to do remotely like

54
00:04:49,599 --> 00:04:53,050
turn off the transmission shut off the
brakes and they were able to demonstrate

55
00:04:53,050 --> 00:04:59,220
this over the summer on an unknown on
tampered with

56
00:04:59,220 --> 00:05:03,970
vehicle fresh off of the factory floor
room basically through the wireless

57
00:05:03,970 --> 00:05:04,789
network

58
00:05:04,789 --> 00:05:09,139
the 3G 4G connection that was in the car
they were able to rewrite the firmware

59
00:05:09,139 --> 00:05:15,380
in the infotainment system to be able to
then send spurious messages across the

60
00:05:15,380 --> 00:05:19,349
the car network to be able to cause it
to do things like turn the wheel at low

61
00:05:19,349 --> 00:05:26,259
speeds to disable the brakes to be able
to to cut the transmission off and they

62
00:05:26,259 --> 00:05:34,199
demonstrated their son of a highway so
that got a lot of press obviously I

63
00:05:34,199 --> 00:05:38,969
don't dwell too much on what the hype
look like but pretty quickly

64
00:05:38,969 --> 00:05:42,000
it was clear that the Christian Charlie
the researchers who discovered this

65
00:05:42,000 --> 00:05:45,710
we're no longer in control of the
storyline that the good work that they

66
00:05:45,710 --> 00:05:52,138
did got subsumed by the hype surrounding
it by the confusion that existed in the

67
00:05:52,139 --> 00:05:58,340
media in people's minds you know all my
friends who knew I am in the security

68
00:05:58,340 --> 00:06:03,289
industry asking me you know should I
panic should I buy an old car had a lot

69
00:06:03,289 --> 00:06:08,550
of people say after this event you know
I just drive a mid nineties car I'm not

70
00:06:08,550 --> 00:06:13,960
gonna start buying older cars now
because I feel safer in reality they're

71
00:06:13,960 --> 00:06:19,469
not safer because modern cars away safer
than some of the older ones so the story

72
00:06:19,469 --> 00:06:24,000
kinda got out of control and it took
over what the story should have been

73
00:06:24,000 --> 00:06:29,330
which is the good research and the
knowledge that automakers are doing more

74
00:06:29,330 --> 00:06:35,330
than just need to do it faster instead
it became about the hyper rounded and

75
00:06:35,330 --> 00:06:40,099
particularly a lot of critics focused on
one aspect which was the live

76
00:06:40,100 --> 00:06:45,690
action on a highway where they remotely
cut the brakes lot of people call them

77
00:06:45,690 --> 00:06:50,620
reckless and and putting people's lives
in danger and the story became more

78
00:06:50,620 --> 00:06:56,030
about that in the research so just to
really quickly run through kind of

79
00:06:56,030 --> 00:07:00,969
analysis and recap here are some really
solid technical work here they they were

80
00:07:00,970 --> 00:07:05,890
able to to get remote control of the
vehicle and overwrite its firmware live

81
00:07:05,890 --> 00:07:11,380
over the air that's not easy to do that
took a lot of work a lot of effort and

82
00:07:11,380 --> 00:07:15,860
it was really good work that was done
they also debunked the myth that there's

83
00:07:15,860 --> 00:07:20,420
no remote attack surface on vehicles
over the air which is clearly not the

84
00:07:20,420 --> 00:07:21,550
case

85
00:07:21,550 --> 00:07:29,270
what they did triggered a a software
update to fix a several years old bug in

86
00:07:29,270 --> 00:07:34,440
some of the software then there was a
subsequent recall that increase the

87
00:07:34,440 --> 00:07:39,760
adoption rate of that software fix there
is a carrier blocked at reduced exposure

88
00:07:39,760 --> 00:07:46,140
across the entire deployed fleet which
is again a good thing but you started to

89
00:07:46,140 --> 00:07:51,840
feel like I see like I said see some of
the negative blowback from some of the

90
00:07:51,840 --> 00:07:56,619
particularly some of the the media
reports there are a number of media

91
00:07:56,620 --> 00:08:00,150
outlets that called with aidid reckless
said that they're endangering lives

92
00:08:00,150 --> 00:08:06,330
there were a number of public policy
makers and legal experts who called for

93
00:08:06,330 --> 00:08:12,669
them to be arrested or at least I
investigated for some of this the large

94
00:08:12,670 --> 00:08:16,610
amount of negative publicity for the
automakers also got in the way of some

95
00:08:16,610 --> 00:08:20,200
of the efforts that they were already
taking to reach out to security

96
00:08:20,200 --> 00:08:25,440
researchers there were a couple of
public vulnerability disclosure programs

97
00:08:25,440 --> 00:08:31,920
are basically stifled after the media
hit after the hike came out so it got in

98
00:08:31,920 --> 00:08:36,270
the way of protecting people and it was
antithetical to what they were trying to

99
00:08:36,270 --> 00:08:43,360
do which is to get more safety and cars
and deployed anybody's to some of the

100
00:08:43,360 --> 00:08:47,980
security research that was going to be
done and it has been done since then

101
00:08:47,980 --> 00:08:53,500
among policy makers legislators the
legal community

102
00:08:53,500 --> 00:08:57,210
so the media outlets certainly some of
the automotive companies it's a lot

103
00:08:57,210 --> 00:09:02,880
harder to talk to them now because of
the negative publicity and one of the

104
00:09:02,880 --> 00:09:06,620
interesting things that I've learned
from working with some of the

105
00:09:06,620 --> 00:09:13,560
legislators in the USA is they're very
very concerned about any any crisis of

106
00:09:13,560 --> 00:09:19,300
confidence in the buying public that
could affect GDP so if people stop

107
00:09:19,300 --> 00:09:24,680
buying cars because they're afraid that
he might get hacked that has a material

108
00:09:24,680 --> 00:09:30,150
effect on the global economy in the USA
at something like twenty percent of the

109
00:09:30,150 --> 00:09:34,030
global economy is in the auto segment so
that really is concerning to

110
00:09:34,030 --> 00:09:39,850
policymakers it's concerning to
economists' concerning everybody now I

111
00:09:39,850 --> 00:09:42,870
wanna contrast that with a different
story that you might not have heard

112
00:09:42,870 --> 00:09:48,550
about this is the story of a researcher
named Billy Rios who has spent years

113
00:09:48,550 --> 00:09:51,270
looking into medical devices

114
00:09:51,270 --> 00:09:57,660
one that he was particularly interested
in he did some investigation in around

115
00:09:57,660 --> 00:09:58,930
the same time frame

116
00:09:58,930 --> 00:10:05,640
published some information a little bit
earlier 22 the federal government of the

117
00:10:05,640 --> 00:10:11,850
us- and to the manufacturer and
ultimately ended up with a very very

118
00:10:11,850 --> 00:10:18,460
similar result that is there was a kind
of a recall issued for those devices to

119
00:10:18,460 --> 00:10:24,080
be removed from service so 22 kind of
fix the flaw but you didn't hear about

120
00:10:24,080 --> 00:10:29,910
it a lot and some of the work that he's
done has actually helped to build up the

121
00:10:29,910 --> 00:10:35,329
ecosystem to make it better so again it
was a solid technical demonstration he

122
00:10:35,330 --> 00:10:39,290
was able to reverse engineer the
firmware believe in find that there is a

123
00:10:39,290 --> 00:10:43,010
connection that wasn't supposed to be
there and you could actually use this

124
00:10:43,010 --> 00:10:48,930
insulin pump for the infusion pump to
dump all of the medication into a

125
00:10:48,930 --> 00:10:55,620
patient at once the flood device was
totally removed from the market place or

126
00:10:55,620 --> 00:10:59,839
at least as strongly worded letter as
could be but it was done in a safe way

127
00:10:59,839 --> 00:11:05,540
so the way that the FDA issued the
recall they don't call it a recall but

128
00:11:05,540 --> 00:11:06,740
it essentially was

129
00:11:06,740 --> 00:11:10,540
as they told hospitals you can't buy
this device anymore because it's not for

130
00:11:10,540 --> 00:11:15,969
sale and where you have it in your own
healthcare environments transition to

131
00:11:15,970 --> 00:11:18,589
safer devices that aren't affected by
this

132
00:11:18,589 --> 00:11:26,330
so in a sense it was pulling this is bad
products from the market also the work

133
00:11:26,330 --> 00:11:30,870
that Billy did as well as some other
security researchers caused people like

134
00:11:30,870 --> 00:11:33,620
the American Food and Drug
Administration to regulate medical

135
00:11:33,620 --> 00:11:37,709
devices some of the medical device
makers so the other industry groups like

136
00:11:37,709 --> 00:11:43,819
hands like some of the other players to
reach out to researchers more to engage

137
00:11:43,820 --> 00:11:49,540
in a more positive productive
collaborative manner than they would

138
00:11:49,540 --> 00:11:55,050
have done had it been a big hype cycle
and a lot of negative press around this

139
00:11:55,050 --> 00:12:05,329
it also paves the way for future actions
such as this to be taken so on august

140
00:12:05,330 --> 00:12:11,470
third when the FDA issued their safety
communication it was the first case

141
00:12:11,470 --> 00:12:20,079
where they have essentially issued a
recall without demonstrated proof of

142
00:12:20,079 --> 00:12:25,020
patient harm in other words nobody had
to die to get this recall to happen

143
00:12:25,020 --> 00:12:30,069
that's really powerful because in
working with the FDA and some of the

144
00:12:30,070 --> 00:12:34,290
other agencies as well as a medical
device makers as well as a lot of people

145
00:12:34,290 --> 00:12:40,480
in the security community we heard time
and time again nothing can be done until

146
00:12:40,480 --> 00:12:45,350
patients die in this is a case where we
didn't have to have that and it was

147
00:12:45,350 --> 00:12:52,690
because of the collaboration not the
public attention and the next time

148
00:12:52,690 --> 00:12:58,899
around when something like this happens
and it gets into a hype cycle the media

149
00:12:58,899 --> 00:13:01,890
is going to be a little bit more
forgiving because of the example that

150
00:13:01,890 --> 00:13:07,390
Billy has said as well as some of the
others who do similar things to make

151
00:13:07,390 --> 00:13:12,020
researchers a little bit more human a
little bit more kinder and gentler to

152
00:13:12,020 --> 00:13:18,390
offset stereotype that already exists
that researchers are just hackers that

153
00:13:18,390 --> 00:13:20,290
you know how the planet and breaking

154
00:13:20,290 --> 00:13:24,779
everything and and fighting those
stereotypes is is one of the good

155
00:13:24,779 --> 00:13:31,790
outcomes of this there's still devices
in use but that's because a lot of cases

156
00:13:31,790 --> 00:13:36,529
you have the option of either using a
flawed medical device to save a life

157
00:13:36,529 --> 00:13:40,430
we're not using a flawed medical device
and having a lower chance of saving the

158
00:13:40,430 --> 00:13:46,410
life so as an example again we're
clearly the right decision is not to

159
00:13:46,410 --> 00:13:52,839
just yank all of these off the market at
once so this gives me to cover the main

160
00:13:52,839 --> 00:13:57,250
point of what is the hype hangover so
I'll describe it really quickly

161
00:13:57,250 --> 00:14:01,779
everybody I'm sure is probably seen the
movie hangover I've watched like five

162
00:14:01,779 --> 00:14:08,170
times on planes it's great so you know
the night starts out alright I'm gonna

163
00:14:08,170 --> 00:14:12,910
have a drink to be fun you know get
together with some friends start

164
00:14:12,910 --> 00:14:14,870
drinking yeah this is great

165
00:14:14,870 --> 00:14:18,399
look at all the stuff that's going on
over here we go there I mean if you're

166
00:14:18,399 --> 00:14:23,970
in Vegas thing you can do anything and
then by the end of the night king of the

167
00:14:23,970 --> 00:14:31,319
world I rule this is great but then the
next morning it's not so good right

168
00:14:31,319 --> 00:14:37,319
wake up and why is there a tiger in this
room I don't understand what went on

169
00:14:37,319 --> 00:14:45,349
wear my clothes when we do if you're in
a situation like these guys where do we

170
00:14:45,350 --> 00:14:49,319
leave the Tigard we take it back to mike
tyson you know what are the best of our

171
00:14:49,319 --> 00:14:52,860
bad alternatives and how do we make a
choice we've got to make a choice

172
00:14:52,860 --> 00:14:58,839
quickly so let's just decide on
something and then go out on that and

173
00:14:58,839 --> 00:15:02,660
then that becomes whatever you did right
if you decide to just leave the room at

174
00:15:02,660 --> 00:15:07,260
Bolton make a bad choice then you're
kind of stuck with that right you can't

175
00:15:07,260 --> 00:15:12,600
go back and undo what you've done in the
past and then at the end of that never

176
00:15:12,600 --> 00:15:18,709
gonna drink again this is awful and then
you have the hangover 2 so the cycle

177
00:15:18,709 --> 00:15:25,369
perpetuates the cycle repeats and I see
a pattern and a corollary in the hype

178
00:15:25,370 --> 00:15:29,230
cycle that happens after some big
publicity happens around

179
00:15:29,230 --> 00:15:35,140
hacking event whether it's a Sony breach
whether it's targeting remotely

180
00:15:35,140 --> 00:15:40,060
controlled there's an initial discovery
phase researchers find that this has

181
00:15:40,060 --> 00:15:44,329
happened and that's kind of cool that
you found a bug and you get to now

182
00:15:44,330 --> 00:15:51,720
figure out what the impacts are then as
you disclose that bugged you're helping

183
00:15:51,720 --> 00:15:56,360
to make things better to make things
safer to improve things you getting a

184
00:15:56,360 --> 00:16:00,310
little bit of attention really nice
hopefully it's all good attention

185
00:16:00,310 --> 00:16:04,650
sometimes you know manufacturers with
software makers don't like to hear that

186
00:16:04,650 --> 00:16:10,910
their baby is ugly but you have to to
persevere through that then if it really

187
00:16:10,910 --> 00:16:16,290
gets legs as we say in the story gets
picked up by journalistic outlets if you

188
00:16:16,290 --> 00:16:21,810
go on a PR campaign with your company to
publicize you know this is the next half

189
00:16:21,810 --> 00:16:26,390
and everybody's gonna die you know the
the fear uncertainty and doubt of the

190
00:16:26,390 --> 00:16:30,710
story starts to pick up and really
accelerate you really quickly lose

191
00:16:30,710 --> 00:16:36,090
control of the narrative you're not able
to shape what gets said about your

192
00:16:36,090 --> 00:16:42,480
vulnerability the outcomes that could
happen it gets taken over by someone

193
00:16:42,480 --> 00:16:43,380
else

194
00:16:43,380 --> 00:16:47,220
the media panic caused by confusion in
the general public

195
00:16:47,220 --> 00:16:53,260
you a lot of times get get people who
step in who maybe they don't have best

196
00:16:53,260 --> 00:17:01,280
interest of the public at mine so if its
vulnerability in automobiles you might

197
00:17:01,280 --> 00:17:07,030
get a any virus company to step up and
say we need to put more antivirus and

198
00:17:07,030 --> 00:17:11,010
automobiles that will fix all the
problems and actually that's a case

199
00:17:11,010 --> 00:17:15,970
where that happened where they were
talking to some public policymakers

200
00:17:15,970 --> 00:17:19,579
trying to get them to legislate
something like any virus rear cars

201
00:17:19,579 --> 00:17:23,780
that's really not an outcome that I want
to see I don't have to go to the dealer

202
00:17:23,780 --> 00:17:30,280
and upgrade my antivirus every year but
those reactions those initial reactions

203
00:17:30,280 --> 00:17:35,520
get kind of set into stone once there's
a long made it's really really hard on

204
00:17:35,520 --> 00:17:40,710
make it once a manufacturer starts going
down a certain pathway to remediate

205
00:17:40,710 --> 00:17:42,490
either a single flaw

206
00:17:42,490 --> 00:17:46,960
or part of their process is really hard
to turn back if they find that it's

207
00:17:46,960 --> 00:17:51,970
cheaper to just see researchers they're
not going to stop snoring researchers so

208
00:17:51,970 --> 00:17:57,679
that initial reaction turns into a more
consistent pattern of behavior over time

209
00:17:57,679 --> 00:18:05,809
that can poison the industry and
eventually you get a return to a level

210
00:18:05,809 --> 00:18:11,090
of trust among the general public that
may not be justified you get people

211
00:18:11,090 --> 00:18:15,689
saying well yeah this is theoretically
possible but no one would ever hacker

212
00:18:15,690 --> 00:18:22,820
cars yeah this is theoretically possible
but it's never happened before and so we

213
00:18:22,820 --> 00:18:27,350
don't necessarily have the evidence to
support any of those statements but

214
00:18:27,350 --> 00:18:33,169
people tend to believe and that
perpetuates the cycle because it

215
00:18:33,170 --> 00:18:38,320
instills more vulnerability more
exposure going down that pathway towards

216
00:18:38,320 --> 00:18:44,840
harm back to the beginning of the cycle
the long-term effects of this can be

217
00:18:44,840 --> 00:18:51,550
pretty brutal we say seeing a trend
globally in the past decade or so of

218
00:18:51,550 --> 00:18:56,059
criminalisation of security research and
security researchers in Germany their

219
00:18:56,059 --> 00:19:00,970
laws on the books prohibiting hacking
tools so if you're a security researcher

220
00:19:00,970 --> 00:19:08,280
and you have a tool that can be used for
remote intrusion of a system could

221
00:19:08,280 --> 00:19:11,960
potentially be arrested for that there's
something called the Vossen or

222
00:19:11,960 --> 00:19:17,330
arrangement anybody heard of us
arrangement handful of people so it's an

223
00:19:17,330 --> 00:19:23,428
international essentially treaty among
its thirty-something nations that they

224
00:19:23,429 --> 00:19:29,030
would voluntarily implement this to
essentially curb the use of intrusion

225
00:19:29,030 --> 00:19:33,620
software globally that is if you use
medicine played in your day-to-day

226
00:19:33,620 --> 00:19:40,330
activities you have any kind of private
exploit code that could potentially bar

227
00:19:40,330 --> 00:19:46,270
you from entering the country or exiting
a country where that code was created in

228
00:19:46,270 --> 00:19:50,340
the us- certainly there are a lot of
laws around criminalizing things that

229
00:19:50,340 --> 00:19:53,230
can be accurately described as security
research

230
00:19:53,230 --> 00:19:57,860
and a lot of these things are
unintentional they're just the outcome

231
00:19:57,860 --> 00:20:02,719
of something scary that happened in the
hype cycle allegedly one of the most

232
00:20:02,720 --> 00:20:09,150
notorious when's the Computer Fraud and
Abuse Act came out of somebody in the

233
00:20:09,150 --> 00:20:11,460
early eighties he watched war games

234
00:20:11,460 --> 00:20:16,799
thought you shouldn't be possible all
the news all the hype around that made

235
00:20:16,799 --> 00:20:20,780
legislators do what they do what we pay
them to do which is to make legislation

236
00:20:20,780 --> 00:20:29,030
this also drains financial resources
that could be used in other areas if you

237
00:20:29,030 --> 00:20:33,889
look at the global economy for products
and services it's about seventy six

238
00:20:33,890 --> 00:20:38,750
billion dollars this year is what will
spend that's a pretty massive number

239
00:20:38,750 --> 00:20:45,080
especially when you consider the number
of breaches is going up the number of

240
00:20:45,080 --> 00:20:49,860
vulnerabilities in software is going up
did not amount of exposure that we have

241
00:20:49,860 --> 00:20:54,719
is going well and still with all of that
spending we can't stop that trend of

242
00:20:54,720 --> 00:21:00,340
increased number of demonstrated harm
from cyber security incidents like to

243
00:21:00,340 --> 00:21:06,389
say the only thing that outpaces the
rate of failure that we have a number of

244
00:21:06,390 --> 00:21:11,040
breaches is the amount we spend to try
and prevent that that's not a good

245
00:21:11,040 --> 00:21:21,450
terrible situation and as we see fewer
and fewer you know horror stories on TV

246
00:21:21,450 --> 00:21:25,950
about people actually being harmed when
there's fewer outcomes that are

247
00:21:25,950 --> 00:21:31,710
worst-case scenarios we just look like
jerks for always run around said

248
00:21:31,710 --> 00:21:36,620
everything is on fire you know we need
to fix these things and so manufacturers

249
00:21:36,620 --> 00:21:38,820
software makers and others

250
00:21:38,820 --> 00:21:43,790
continually build in less and less
trustworthy systems as dependencies to

251
00:21:43,790 --> 00:21:49,139
other more concrete systems so we walked
into this room today none of us were

252
00:21:49,140 --> 00:21:52,990
worried about what happens if the
chandelier falls or what happens if one

253
00:21:52,990 --> 00:21:56,590
of the walls just accidentally crashes
and we've got extended back up on her

254
00:21:56,590 --> 00:22:01,040
own but if you look at software code I
am kind of worried that my phone might

255
00:22:01,040 --> 00:22:04,168
die that it might might crash or

256
00:22:04,169 --> 00:22:11,080
the laptop that I'm using might have a
segfault and so is we're using more and

257
00:22:11,080 --> 00:22:18,710
more of this unreliable code in things
like planes trains cars medical devices

258
00:22:18,710 --> 00:22:25,639
we're continually putting our our our
lives in the hands of that untrustworthy

259
00:22:25,639 --> 00:22:31,178
Co potentially untrustworthy code but
that's not really why I got into

260
00:22:31,179 --> 00:22:36,659
security and I don't think it's why most
of us got into security wasn't to get

261
00:22:36,659 --> 00:22:40,320
into this hype cycle to have this
hangover to see these negative long-term

262
00:22:40,320 --> 00:22:50,809
consequences I got into it so that I
could do something great I got into it

263
00:22:50,809 --> 00:22:59,389
to be able to call my own shots to know
that the work that I did was going to be

264
00:22:59,389 --> 00:23:06,379
effective was going to lead to positive
outcomes and i wanna spend maybe not as

265
00:23:06,379 --> 00:23:12,139
much time justifying what I'm doing in
security research then actually doing it

266
00:23:12,139 --> 00:23:17,549
if you've ever been in the situation of
trying to convince some stakeholders may

267
00:23:17,549 --> 00:23:21,918
be an executive your company to do more
security for security's sake it's hard

268
00:23:21,919 --> 00:23:30,789
we don't want to be in that position I
wanted to be challenged continually and

269
00:23:30,789 --> 00:23:34,859
I wanted to get better at what I was
doing to learn I wanted to practice on

270
00:23:34,859 --> 00:23:42,199
to work I wanted to make a difference I
think that these are things that I've

271
00:23:42,200 --> 00:23:45,179
seen a lot of the other security
professionals that I've worked with and

272
00:23:45,179 --> 00:23:48,690
that I've come together with a
conference is over the years we want to

273
00:23:48,690 --> 00:23:52,519
make the world a better place you can
look at that folks like this guy Dhabi

274
00:23:52,519 --> 00:23:57,019
Aden heimer he's doing a lot for
humanitarian issues trying to use big

275
00:23:57,019 --> 00:24:01,909
data and large datasets and his
expertise in background in manipulating

276
00:24:01,909 --> 00:24:06,769
those datasets to find some really
interesting conclusions that could come

277
00:24:06,769 --> 00:24:07,850
from

278
00:24:07,850 --> 00:24:11,439
information that's been generated but
the just hasn't been analyzed very well

279
00:24:11,440 --> 00:24:19,159
you can look at guys like Morgan mayhem
if you know him on Twitter he does a lot

280
00:24:19,159 --> 00:24:23,389
of work with a group called citizen lab
attracts surveillance of repressive

281
00:24:23,389 --> 00:24:25,959
regime regimes around the world

282
00:24:25,960 --> 00:24:30,359
these are the types of things that that
we wanted to get into security for we

283
00:24:30,359 --> 00:24:34,559
wanted to dent the universe and its hype
hangovers really getting in our way we

284
00:24:34,559 --> 00:24:37,168
have to figure out a better way
somewhere around it

285
00:24:37,169 --> 00:24:42,590
where hackers right we see a firewall we
try a couple of passwords to to get into

286
00:24:42,590 --> 00:24:47,519
the admin console if that doesn't work
we don't just walk away we want to

287
00:24:47,519 --> 00:24:53,980
overcome those barriers we want to shape
the world to change the world in the way

288
00:24:53,980 --> 00:24:59,070
that we need to see fit and as I
mentioned we're just increasingly

289
00:24:59,070 --> 00:25:06,570
dependent on some of this computer code
that is fundamentally untrustworthy as

290
00:25:06,570 --> 00:25:10,178
we put more and more of it into our
medical devices automobiles connected

291
00:25:10,179 --> 00:25:17,749
home and public infrastructure so let's
run through a really quick exercise here

292
00:25:17,749 --> 00:25:23,690
number of lines of code per system is
going up or down

293
00:25:23,690 --> 00:25:32,349
thank you for participating number of
vulnerabilities in that code scales with

294
00:25:32,349 --> 00:25:37,359
the number of lines of code are we
adding more and more

295
00:25:37,359 --> 00:25:48,220
well we have an increasing number of
systems that rely on code now what about

296
00:25:48,220 --> 00:25:52,950
exposure of that code adding
conductivity two things the Internet of

297
00:25:52,950 --> 00:26:02,249
Things vehicles medical devices more or
less exposure or what about the number

298
00:26:02,249 --> 00:26:09,389
of adversaries that we have a number of
adversaries is going up or down about

299
00:26:09,389 --> 00:26:14,428
the sophistication of the adversaries up
or down

300
00:26:14,429 --> 00:26:17,499
so all of these things are kind of
combining against us putting us in a

301
00:26:17,499 --> 00:26:23,480
situation where these things that we
traditionally take for granted that are

302
00:26:23,480 --> 00:26:27,749
going to be engineered with sound
concepts the bridge or driving over to

303
00:26:27,749 --> 00:26:33,249
get home is gonna fall down under you
are increasingly being built on top of

304
00:26:33,249 --> 00:26:40,389
software code that is vulnerable because
all code has flaws is exposed as we're

305
00:26:40,389 --> 00:26:46,418
connecting it to the Internet to
integrate more introduced costs in an

306
00:26:46,419 --> 00:26:50,389
environment where the hazards or not
whether it's submerged under water or

307
00:26:50,389 --> 00:26:56,289
the degree of rusted it has but
intelligent and adaptive adversaries

308
00:26:56,289 --> 00:27:02,158
some of them have motivations that we
can't understanding can appreciate but

309
00:27:02,159 --> 00:27:08,220
we nevertheless can't deny exists that
they want to harm us know that they want

310
00:27:08,220 --> 00:27:15,710
to cause some type of chaos and I think
nothing for me brought home more clearly

311
00:27:15,710 --> 00:27:18,809
in the Sony hack that happened

312
00:27:18,809 --> 00:27:29,330
little while ago that was around
December I believe this is when using

313
00:27:29,330 --> 00:27:34,649
expression but the shit hit the fan soon
got hacked everybody was running around

314
00:27:34,649 --> 00:27:39,539
screaming its North Korea they're
they're gonna take all of our movies or

315
00:27:39,539 --> 00:27:45,619
something it's amazing the degree to
which this guy publicity the president

316
00:27:45,619 --> 00:27:48,759
the United States one of the most
powerful people in the world stopped

317
00:27:48,759 --> 00:27:53,379
what he was doing and did press
conferences and drafted an executive

318
00:27:53,379 --> 00:27:58,748
order about the stuff that we do every
day you know when the president has to

319
00:27:58,749 --> 00:28:01,919
get involved come down and say look i
straighten the stuff out I can be

320
00:28:01,919 --> 00:28:04,179
dealing with this stuff as well as the
other global things that are going to

321
00:28:04,179 --> 00:28:10,690
take care of like dat impactful that
shows you the degree of reach that we

322
00:28:10,690 --> 00:28:17,720
now have in this industry and the
possibilities if we get it right that's

323
00:28:17,720 --> 00:28:23,320
also when I really realized that you
know maybe people in in public

324
00:28:23,320 --> 00:28:28,289
leadership positions aren't going to
come and save us

325
00:28:28,289 --> 00:28:32,070
typically when something like this
happens everybody gets together and

326
00:28:32,070 --> 00:28:36,999
fixes the problem but you know maybe
John Wayne's not going to come in on his

327
00:28:36,999 --> 00:28:41,740
white horse at the end of the movie and
save us maybe the cavalry isn't coming

328
00:28:41,740 --> 00:28:50,429
and if the calorie isn't coming then
that responsibility falls to us to

329
00:28:50,429 --> 00:28:57,629
really take a leadership role to stand
up and lead when my friends Kerala saree

330
00:28:57,629 --> 00:29:03,709
gave a TED talk and she said after the
TED talk she thought she was gonna get

331
00:29:03,710 --> 00:29:09,399
yelled at him off the stage by all the
amazing people who are there people like

332
00:29:09,399 --> 00:29:15,268
Bill Gates people like randall Munroe
from xkcd because she was saying that

333
00:29:15,269 --> 00:29:20,039
hackers are good they can be part of the
internet's immune system what she found

334
00:29:20,039 --> 00:29:24,908
is that instead of getting booed off the
stage by those people they think her

335
00:29:24,909 --> 00:29:29,700
graduated her and she said she realized
that we have already been given

336
00:29:29,700 --> 00:29:34,700
permission to change the world to make
it a better place by the outsiders

337
00:29:34,700 --> 00:29:41,169
we just have to reach into ourselves and
yourself permission so how do we do that

338
00:29:41,169 --> 00:29:47,210
with a group of colleagues myself where
we've been working on that I am the

339
00:29:47,210 --> 00:29:52,840
camera initiative for quite a while now
and essentially it's in an attempt to

340
00:29:52,840 --> 00:29:57,249
improve the state of the world trade the
problem statement is that our dependence

341
00:29:57,249 --> 00:30:00,809
on connected technology is growing
faster than our ability to secure that

342
00:30:00,809 --> 00:30:07,059
technology so where the impacts are
greater than a blue screen are greater

343
00:30:07,059 --> 00:30:11,720
than reboot are greater than Android
malware we also need to overcome that we

344
00:30:11,720 --> 00:30:16,289
need to be better we can't continually
have increasing number of cyber security

345
00:30:16,289 --> 00:30:21,359
failures increasing amount of spending
on those failures and continuing to

346
00:30:21,359 --> 00:30:22,779
perpetuate that problem

347
00:30:22,779 --> 00:30:26,759
continuing to exacerbate the hype
hangover

348
00:30:26,759 --> 00:30:31,589
we have to be better and what we've
learned is that one of the first things

349
00:30:31,589 --> 00:30:37,469
it takes is empathy understanding the
other people that you're trying to

350
00:30:37,469 --> 00:30:43,940
influence understanding what makes them
work instead of the typical reaction

351
00:30:43,940 --> 00:30:51,959
that we tend to give people historically
when somebody from the outside like a

352
00:30:51,959 --> 00:30:57,199
journalist I come and talk to us we tend
to say you know get off my turf this is

353
00:30:57,199 --> 00:30:58,159
my area

354
00:30:58,159 --> 00:31:03,379
beware of my dog instead when people
come to us and they won't ask us

355
00:31:03,379 --> 00:31:04,958
questions

356
00:31:04,959 --> 00:31:08,369
understand that they're doing it in good
faith that they really want to know the

357
00:31:08,369 --> 00:31:12,349
answers are not trying to set us up for
anything after coming to us maybe

358
00:31:12,349 --> 00:31:17,259
they'll listen maybe they'll take our
advice we will do something with it if

359
00:31:17,259 --> 00:31:21,799
it's in the right even with the right
attitude and the right language so

360
00:31:21,799 --> 00:31:26,109
instead of beware of dog sign let's put
out a welcome mat for them it's welcome

361
00:31:26,109 --> 00:31:32,339
outsiders let's welcome the people who
are in government who are in the media

362
00:31:32,339 --> 00:31:37,918
who are in manufacturing who are in
software makers allow them to come in

363
00:31:37,919 --> 00:31:40,159
and give them our time

364
00:31:40,159 --> 00:31:49,619
patiently and with empathy outreach is
another way so my first year in InfoSec

365
00:31:49,619 --> 00:31:56,168
I thought I was like really cool and I
was literally in a basement room there

366
00:31:56,169 --> 00:31:59,459
is a window like up here and i was just
tucked away on a keyboard I had like

367
00:31:59,459 --> 00:32:02,779
eight decommissioned computers over here
that I was trying to build snort boxes

368
00:32:02,779 --> 00:32:06,869
on that is really cool because it was a
lot of fun and then there is an article

369
00:32:06,869 --> 00:32:11,178
that came out of the wall street journal
that said these are the top 10 things

370
00:32:11,179 --> 00:32:14,879
that your I T and security department
don't want you to know I was all about

371
00:32:14,879 --> 00:32:19,678
how to get around URL filters and how to
evade firewalls in the goal of the stuff

372
00:32:19,679 --> 00:32:25,929
that to me was just shocking like why is
the Wall Street Journal trying to

373
00:32:25,929 --> 00:32:30,609
convince the people in my organization
to circumvent my security controls don't

374
00:32:30,609 --> 00:32:38,359
they know but I took a step back I said
alright this is somebody who's trying to

375
00:32:38,359 --> 00:32:39,460
put out an article

376
00:32:39,460 --> 00:32:42,779
maybe doesn't understand the
implications and so I took that empathy

377
00:32:42,779 --> 00:32:47,770
approach I reached out to her and I said
hey you know just so you know those

378
00:32:47,770 --> 00:32:51,529
controls are usually there for a good
reason I listed off three specific

379
00:32:51,529 --> 00:32:58,070
examples told her three things that
could be done instead to accomplish the

380
00:32:58,070 --> 00:33:06,149
goal in the way that the employees want
to do it but also that would be safe

381
00:33:06,149 --> 00:33:10,879
that wouldn't open the organization more
risk and she reached back out to me and

382
00:33:10,880 --> 00:33:11,909
said hey this is great

383
00:33:11,909 --> 00:33:16,500
would you like to write a couple of
quotes for a follow-up article that I'm

384
00:33:16,500 --> 00:33:20,940
doing so that was the first time I got
published in The Wall Street Journal was

385
00:33:20,940 --> 00:33:24,340
like three months after I started my
security career and it wasn't because of

386
00:33:24,340 --> 00:33:27,580
some cool technical hockey was just
because I took the time to engage with

387
00:33:27,580 --> 00:33:33,539
the journalists to reach out to them
said hey you know i appreciate the work

388
00:33:33,539 --> 00:33:34,169
you put in

389
00:33:34,169 --> 00:33:37,890
I'm not going to say like to the article
but I appreciate the work you put into

390
00:33:37,890 --> 00:33:42,440
this here are some things that you can
do to improve in the future and I think

391
00:33:42,440 --> 00:33:47,100
that it was taken with the intent that
it was given with which was an attempt

392
00:33:47,100 --> 00:33:55,580
to help last year 2014 I'm the Kaveri
published an open letter to the

393
00:33:55,580 --> 00:34:00,750
automaker's we took a very similar tact
right we wanted to get some outreach to

394
00:34:00,750 --> 00:34:06,559
them because we don't have the cell
phone numbers of the CEO's because we

395
00:34:06,559 --> 00:34:10,929
don't even in many cases have their
email addresses we published an open

396
00:34:10,929 --> 00:34:18,849
letter and we invited them to work
collaboratively with other stakeholders

397
00:34:18,849 --> 00:34:24,159
where domains of expertise overlap we
reached out and we said look we want to

398
00:34:24,159 --> 00:34:29,470
recognize your long track record of
safety the statistics are something like

399
00:34:29,469 --> 00:34:34,049
six hundred and forty thousand lives
have been saved over the last 50 years

400
00:34:34,050 --> 00:34:40,530
through advances in automotive safety
technology that's a huge number so we

401
00:34:40,530 --> 00:34:45,330
started with that and we had a bunch of
other information in there just giving

402
00:34:45,330 --> 00:34:47,650
them some pointers giving them some tips

403
00:34:47,650 --> 00:34:54,810
and we heard a lot from them I think
over the course of the next three to

404
00:34:54,810 --> 00:34:59,029
four months I was probably on 23 phone
calls a day with somebody in the

405
00:34:59,030 --> 00:35:04,860
automotive industry because of this open
letter because we could took control of

406
00:35:04,860 --> 00:35:10,550
the narratives didn't over-hyped things
in center down to the world we got a lot

407
00:35:10,550 --> 00:35:13,930
of skepticism in a lot of people asking
us you know what our game was are you

408
00:35:13,930 --> 00:35:16,950
trying to extort ask for money or you
try to get jobs in our company like

409
00:35:16,950 --> 00:35:21,970
what's going on but they called us and
we were able to engage with them and

410
00:35:21,970 --> 00:35:26,299
talk with them on a human level and
explain here's why we're doing things we

411
00:35:26,300 --> 00:35:27,880
all want the same things right

412
00:35:27,880 --> 00:35:33,280
safer vehicles with better futures out
to market quicker and cheaper those are

413
00:35:33,280 --> 00:35:37,650
the things that technology can enable
but their side effects to adopting that

414
00:35:37,650 --> 00:35:41,780
technology without fully understanding
the risks and the consequences

415
00:35:41,780 --> 00:35:48,260
carrying that message allowed us to get
really well in bed with the automotive

416
00:35:48,260 --> 00:35:52,800
industry to the point where now I think
I'm helping to organize three or four

417
00:35:52,800 --> 00:35:58,420
automotive security conferences with the
automakers and the tier ones and other

418
00:35:58,420 --> 00:36:08,030
people in those ecosystems if you can't
do the traditional outreach or just

419
00:36:08,030 --> 00:36:13,740
don't don't want to you're you're afraid
you could consider taking a role with

420
00:36:13,740 --> 00:36:19,379
one of those affected industries maybe
not taking a role in the media if you're

421
00:36:19,380 --> 00:36:23,930
not media trained but certainly we have
people who are working with the media

422
00:36:23,930 --> 00:36:27,029
there are part of the media to get the
message out in a technically literate

423
00:36:27,030 --> 00:36:34,860
way you could do like college porn did
where he left his job of being a

424
00:36:34,860 --> 00:36:40,560
security researcher and how can come
books all day and he went to Tesla to

425
00:36:40,560 --> 00:36:45,000
help build their internal security
program that corporate security program

426
00:36:45,000 --> 00:36:50,010
so that he could reach across to the
vehicle manufacturing side and start to

427
00:36:50,010 --> 00:36:53,810
be an ambassador security in that field
where you can be like Mike Murray from

428
00:36:53,810 --> 00:36:58,830
GE Medical who started a security
consulting company called mad security

429
00:36:58,830 --> 00:37:00,569
was

430
00:37:00,570 --> 00:37:04,950
CEO that for several years and decided
he wanted to do something better so he

431
00:37:04,950 --> 00:37:10,279
quit the job with the company that he
founded to go work for GE medical and

432
00:37:10,280 --> 00:37:16,620
improve security of medical devices I'm
seeing a trend over the past two or

433
00:37:16,620 --> 00:37:22,060
three years a lot more people doing that
people who don't just take a job for the

434
00:37:22,060 --> 00:37:22,890
money

435
00:37:22,890 --> 00:37:29,049
the ticket for the impact they can have
to make things better

436
00:37:29,050 --> 00:37:38,520
being a student who were helping to
train someone else we've done and I've

437
00:37:38,520 --> 00:37:44,440
seen a lot of people going to
conferences like besides in Las Vegas

438
00:37:44,440 --> 00:37:50,130
where there's a really heavy emphasis on
non-technical skills building things

439
00:37:50,130 --> 00:37:54,420
like being able to effectively
communicate outside the echo chamber to

440
00:37:54,420 --> 00:37:59,700
talk to executives how do you do media
interviews get some media training how

441
00:37:59,700 --> 00:38:05,620
do you learn how to write in a more
clear and concise way there's a lot of

442
00:38:05,620 --> 00:38:10,490
people who were my Twitter feed who go
to classes given by Edward Tufte

443
00:38:10,490 --> 00:38:15,520
presentation of visual information is
the things that security researchers are

444
00:38:15,520 --> 00:38:20,600
doing not to be better at the research
itself but to be better at making the

445
00:38:20,600 --> 00:38:23,910
research matter of getting it outside of
the echo chamber to getting it picked up

446
00:38:23,910 --> 00:38:30,040
to being clear when talking with media
to avoid getting hype cycle that gets

447
00:38:30,040 --> 00:38:38,000
out of control at the same time we can
also educate people so in working with

448
00:38:38,000 --> 00:38:43,550
the USDA Food and Drug Administration we
put together one day workshop with them

449
00:38:43,550 --> 00:38:46,900
where they brought a lot of their
stakeholders in internal stakeholders we

450
00:38:46,900 --> 00:38:52,330
brought several security researchers and
we talked with them it was kind of a

451
00:38:52,330 --> 00:38:57,890
getting to know you period where we told
them a little bit about security

452
00:38:57,890 --> 00:39:02,509
research with the process looks like how
that works and then we crypt quickly

453
00:39:02,510 --> 00:39:07,280
realized that we needed to know a lot
more about what they did and how they

454
00:39:07,280 --> 00:39:11,630
did it so we've engaged with them a lot
more to get some of those resources to

455
00:39:11,630 --> 00:39:13,829
the security researchers working in that
field

456
00:39:13,829 --> 00:39:18,869
so that they know that the hurdles the
stumbling blocks the effective processes

457
00:39:18,869 --> 00:39:23,380
that things that don't work in that
domain so that we can understand and

458
00:39:23,380 --> 00:39:27,609
empathize better and this doesn't just
have to be something that you do

459
00:39:27,609 --> 00:39:34,078
formerly with some organization if you
happen to be you know looking to give

460
00:39:34,079 --> 00:39:38,380
speak somewhere I found that speaking at
conferences that are not InfoSec

461
00:39:38,380 --> 00:39:45,009
conferences you tend to be a lot more
special their rights to go to a medical

462
00:39:45,009 --> 00:39:49,319
device conference in New Delhi hacker
there everybody's gonna wanna talk to

463
00:39:49,319 --> 00:39:52,308
you everybody's gonna want to learn from
you and you can really communicate your

464
00:39:52,309 --> 00:39:56,519
message lot more effectively to the
people who have the ability and the the

465
00:39:56,519 --> 00:40:05,169
task really to make safer devices and
when you are presenting as security

466
00:40:05,170 --> 00:40:10,259
conferences look at what you're doing

467
00:40:10,259 --> 00:40:15,529
are you gonna research mobile malware
which gets a ton of submissions to

468
00:40:15,529 --> 00:40:19,989
security conferences where are you going
to research something that has a little

469
00:40:19,989 --> 00:40:24,869
bit more consequence you know if you
look at the amount of time you spend

470
00:40:24,869 --> 00:40:32,989
finding mobile malware finding a flaw in
an Android device or fly find a flaw in

471
00:40:32,989 --> 00:40:37,219
a car medical device the amount of time
on the technical research is probably

472
00:40:37,219 --> 00:40:41,440
around the same but the amount of time
you have to spend trying to justify why

473
00:40:41,440 --> 00:40:46,039
it matters it's a lot less if you start
over on the human life side of that

474
00:40:46,039 --> 00:40:50,670
scale start researching things that have
a little bit more consequence to them

475
00:40:50,670 --> 00:40:57,319
speaking as somebody who's helped to
organize some conferences and looked at

476
00:40:57,319 --> 00:41:02,859
call for paper submissions I can tell
you if I see all the talk about where

477
00:41:02,859 --> 00:41:07,049
I'm probably just gonna stop reading at
the top of the page because I don't

478
00:41:07,049 --> 00:41:13,359
really care but if I see something that
is both meaningful impactful as well as

479
00:41:13,359 --> 00:41:18,660
that has a a compelling personal story
for me I'm much more likely to accept

480
00:41:18,660 --> 00:41:22,219
that talk at least given more
consideration than some of those others

481
00:41:22,219 --> 00:41:25,809
so if all you want to do is go around
the security talks

482
00:41:25,809 --> 00:41:31,869
make that can hear your full-time gig
then looking at areas where human life

483
00:41:31,869 --> 00:41:36,689
and public safety or the consequences of
some of your research is a lot easier to

484
00:41:36,689 --> 00:41:44,269
get your talks accepted so just to kinda
recap so it's all on one screen these

485
00:41:44,269 --> 00:41:48,538
are some of the things that we've worked
on Westheimer cavalry that we found to

486
00:41:48,539 --> 00:41:53,900
be really effective over the past couple
of years I've personally found them

487
00:41:53,900 --> 00:42:00,989
effective in my own career in my own
life and we found that as we team with

488
00:42:00,989 --> 00:42:06,039
others whether their other security
researchers are there other people in

489
00:42:06,039 --> 00:42:10,769
our industry who aren't doing research
but who also contributing or whether

490
00:42:10,769 --> 00:42:13,868
they're people outside of our industry
and some of the effective stakeholder

491
00:42:13,869 --> 00:42:22,039
ecosystems this is like media places
like public policy it's really effective

492
00:42:22,039 --> 00:42:31,900
so that's that's all I have for today if
there are any questions i'm happy to

493
00:42:31,900 --> 00:42:40,590
take some questions quickly that we have
about 10 more minutes

494
00:42:40,590 --> 00:43:07,300
the last names names contributing to the
hype it's definitely contributing to the

495
00:43:07,300 --> 00:43:07,970
hype

496
00:43:07,970 --> 00:43:12,660
whether or not it's contributing to the
solution I don't know if you look at for

497
00:43:12,660 --> 00:43:17,720
example heart please that's probably the
most famous example everybody knows

498
00:43:17,720 --> 00:43:21,609
about that now I mean I think my mom
even mentioned witnesses heard pleading

499
00:43:21,610 --> 00:43:27,950
with my mom know something and what I do
that's saying something but yet if you

500
00:43:27,950 --> 00:43:28,950
look out there

501
00:43:28,950 --> 00:43:34,029
the affected ecosystem their people
who've done scans and it's around half

502
00:43:34,030 --> 00:43:38,230
of the devices that are found on the
internet are still vulnerable to heart

503
00:43:38,230 --> 00:43:42,960
bleed from you know between day one in
now and it's a huge number it's like two

504
00:43:42,960 --> 00:43:47,230
hundred thousand I think there are still
vulnerable harpley so you know maybe

505
00:43:47,230 --> 00:43:51,870
that's the systemic flaw a lot of those
devices might be embedded systems where

506
00:43:51,870 --> 00:43:57,880
you can't do a software update it might
be a situation where the administrator

507
00:43:57,880 --> 00:44:01,330
of those systems don't know that they're
vulnerable to that flaw or don't know

508
00:44:01,330 --> 00:44:05,850
that that software package even exists
in their systems or it might be

509
00:44:05,850 --> 00:44:13,720
something else I think that certainly
was something like that there's maybe no

510
00:44:13,720 --> 00:44:19,470
good way to get that individual point
flaw sixth globally across everyone now

511
00:44:19,470 --> 00:44:24,040
looking beyond that if you look at the
conditions that produced something like

512
00:44:24,040 --> 00:44:30,420
heart bleed you can look at the ability
of a single quarter to enter a lot of

513
00:44:30,420 --> 00:44:38,220
code and a project that is a dependency
for thousands of other projects and ask

514
00:44:38,220 --> 00:44:42,240
how can we fix that problem so we don't
have another heart bleed I think those

515
00:44:42,240 --> 00:44:45,529
are the questions that there that we
want to start looking at two not just

516
00:44:45,530 --> 00:44:50,270
how do we get more publicity so that
maybe we can get one flaw sext it's how

517
00:44:50,270 --> 00:44:53,420
do we start working with other people to
bring people together

518
00:44:53,420 --> 00:44:59,569
so these types of things don't recur if
you look at the amount of code already

519
00:44:59,569 --> 00:45:06,339
deployed its a finite amount if you look
at the amount to be deployed in the

520
00:45:06,339 --> 00:45:07,328
future

521
00:45:07,329 --> 00:45:10,720
it's a huge amount it's it's probably
infinite depending on how long are

522
00:45:10,720 --> 00:45:14,618
species exists right now on we continue
to software so there's a lot more

523
00:45:14,619 --> 00:45:17,290
potential for positive change

524
00:45:17,290 --> 00:45:22,520
looking forward then there is looking
backward and find this especially with

525
00:45:22,520 --> 00:45:25,970
with some of the manufactures medical
device manufacturers hardware makers

526
00:45:25,970 --> 00:45:30,078
automakers especially they're putting
more and more conductivity more more

527
00:45:30,079 --> 00:45:33,079
software in their cars those things will
be on the road

528
00:45:33,079 --> 00:45:36,970
fifteen twenty years they might be in a
hospital thirty years we need to make

529
00:45:36,970 --> 00:45:38,598
sure that they're doing things right

530
00:45:38,599 --> 00:45:44,040
very very quickly so so your
recommendation is no names or not

531
00:45:44,040 --> 00:45:49,529
well I'll let that to the individual
people to to make up their minds I don't

532
00:45:49,530 --> 00:45:56,319
know that it helps except to feed the
hype cycle and to sell stuff so if your

533
00:45:56,319 --> 00:46:00,779
job is marketing means name it but if
you want to actually make an impact and

534
00:46:00,780 --> 00:46:06,430
do something positive productive than
maybe spend less time on coming up with

535
00:46:06,430 --> 00:46:09,649
names and spend more time on
collaborating with people

536
00:46:09,650 --> 00:46:14,510
okay thank you

537
00:46:14,510 --> 00:46:27,390
over one thing I asked about things like
you know you say that car entertainment

538
00:46:27,390 --> 00:46:33,080
system is how Charlie got in through the
towers but when they also through such

539
00:46:33,080 --> 00:46:36,920
things as like NFC and Bluetooth the
phones actually look up to them so

540
00:46:36,920 --> 00:46:42,320
malicious phone might be a good attack
vector their likewise with the medical

541
00:46:42,320 --> 00:46:46,190
devices actually at a conference I was
at a couple months ago someone talked

542
00:46:46,190 --> 00:46:50,800
about actually being diagnosed by a
doctor or Thurs iPhone which is kind of

543
00:46:50,800 --> 00:46:55,260
frightening to think about but you know
having her on the phone in that case

544
00:46:55,260 --> 00:47:00,140
could even cause you get misdiagnosed
there's apps for your insulin pump at

545
00:47:00,140 --> 00:47:04,779
this point so I think maybe we shouldn't
just credit things around mobile since

546
00:47:04,780 --> 00:47:09,680
we have mobile apps for everything now
that could potentially affect human life

547
00:47:09,680 --> 00:47:15,029
in a lot of ways that's a good point I
didn't mean to say no one should ever

548
00:47:15,030 --> 00:47:19,630
research mobile or you should look at
our on mobile devices but when you look

549
00:47:19,630 --> 00:47:25,460
at the spectrum of things if its mobile
device in the only risk is that you know

550
00:47:25,460 --> 00:47:31,010
maybe a payment card goes missing the
number goes missing that's a really low

551
00:47:31,010 --> 00:47:34,850
consequence thing what you're talking
about the intersection of mobile health

552
00:47:34,850 --> 00:47:38,700
and those mobile devices or health in
those mobile devices is a really

553
00:47:38,700 --> 00:47:44,180
important area so rather than you know
what's the implications for PCI of this

554
00:47:44,180 --> 00:47:48,750
card scanner like look at those areas
where those devices do interface with

555
00:47:48,750 --> 00:47:51,510
health care systems

556
00:47:51,510 --> 00:47:58,310
question so you touched on a very very
sensitive topical Europe and that is

557
00:47:58,310 --> 00:48:04,509
criminalizing security research and I
think it's something that affect most of

558
00:48:04,510 --> 00:48:10,920
the people in this room now what are the
next steps I mean how do you see the

559
00:48:10,920 --> 00:48:15,480
future of this area because we're seeing
laws more and more laws being like

560
00:48:15,480 --> 00:48:21,750
handicapping cryptography for
law-abiding citizen I don't think that

561
00:48:21,750 --> 00:48:25,510
affects people that don't give a shit
about the law you know so it's only

562
00:48:25,510 --> 00:48:33,440
affecting us most of us so what can we
do about this in order to keep doing

563
00:48:33,440 --> 00:48:37,450
what we're doing and how do you see the
future

564
00:48:37,450 --> 00:48:41,129
yeah that's a really good questions one
it's really hard to answer there's many

565
00:48:41,130 --> 00:48:47,080
potential ways that the future could
shape up I think that first and foremost

566
00:48:47,080 --> 00:48:50,770
being a technically literate voice of
reason

567
00:48:50,770 --> 00:48:54,509
knowing the technology the background to
be able to communicate that out without

568
00:48:54,510 --> 00:48:59,310
a lot of hype and hysteria is a really
good positive thing a lot of the stories

569
00:48:59,310 --> 00:49:06,140
that I see from the security research
community are really hyperbolic there

570
00:49:06,140 --> 00:49:13,390
are focused on extreme cases things that
it's hard to make a logical jump from No

571
00:49:13,390 --> 00:49:17,629
all use my mom is an example again from
what my mom sees as the benefit to

572
00:49:17,630 --> 00:49:21,880
encryption which her less it's
protecting her payment card she just

573
00:49:21,880 --> 00:49:27,000
really care to what we see as the
benefit of encryption which is more of a

574
00:49:27,000 --> 00:49:33,220
social justice issue more of a social
contract issue privacy rights like these

575
00:49:33,220 --> 00:49:37,209
things are important to us she can't
make that leap so maybe walk a little

576
00:49:37,210 --> 00:49:41,770
bit her way and helped her to be able to
see the clear pathway to why our

577
00:49:41,770 --> 00:49:48,840
perspective is what it is rather than
just running around screaming into the

578
00:49:48,840 --> 00:49:53,350
sky is falling on its gonna fuck your
mom as well because I mean she's going

579
00:49:53,350 --> 00:49:57,660
to use standard encryption which the bad
guys will be able to break while we

580
00:49:57,660 --> 00:50:00,759
won't be able to research but a wasting
trip stuff

581
00:50:00,760 --> 00:50:07,420
so yeah it will affect her she doesn't
realize it and maybe she doesn't care

582
00:50:07,420 --> 00:50:11,650
you know she might have a threat profile
that is different than the one that we

583
00:50:11,650 --> 00:50:16,090
think she has sitting down and talking
with her understanding and asking her

584
00:50:16,090 --> 00:50:19,370
what she cares about what she's
concerned with you are you might I might

585
00:50:19,370 --> 00:50:23,920
not be able to turn her to see the
things that I do but if I talked with

586
00:50:23,920 --> 00:50:27,970
ten or fifteen or twenty people I'll be
able to understand what their concerns

587
00:50:27,970 --> 00:50:28,689
really are

588
00:50:28,690 --> 00:50:32,620
I'll be able to work with them to
develop a better story so that I can

589
00:50:32,620 --> 00:50:37,540
walk all the way back to where they are
and then bring them to where I in a

590
00:50:37,540 --> 00:50:44,360
rational illiterate way then back to the
topic of legislature what's gonna happen

591
00:50:44,360 --> 00:50:50,760
with the laws and how how is this going
to move forward in terms of keeping the

592
00:50:50,760 --> 00:50:54,650
security community I think one of the
best things that we can do I don't know

593
00:50:54,650 --> 00:50:59,740
what the outcome is gonna be a ticket
still in flux go either way in the USU

594
00:50:59,740 --> 00:51:05,000
have essentially to automotive agencies
the Department of Transportation and the

595
00:51:05,000 --> 00:51:08,580
National Highway Traffic Safety
Administration one published letter

596
00:51:08,580 --> 00:51:13,200
saying that security researchers that
car should not be open to security

597
00:51:13,200 --> 00:51:17,720
research the other one just give a talk
the other day in which they said that

598
00:51:17,720 --> 00:51:21,560
security researchers are great benefit
system right so who knows what's

599
00:51:21,560 --> 00:51:25,470
actually gonna happen I think one of the
things that we can do though to defend

600
00:51:25,470 --> 00:51:30,600
ourselves is to demonstrate our value
that's one of the big things that we've

601
00:51:30,600 --> 00:51:34,839
gotta come away with over the last
couple of years is what demonstrate

602
00:51:34,840 --> 00:51:38,570
value more than anything is showing that
there is a public good come from

603
00:51:38,570 --> 00:51:44,130
security research things like the
Volkswagen smog emissions story if

604
00:51:44,130 --> 00:51:49,020
you've not heard about that thats
volkswagen has found to be covering up

605
00:51:49,020 --> 00:51:59,060
the fact that their gas mileage and smog
emissions we're basically falsified that

606
00:51:59,060 --> 00:52:03,900
was discovered through some clever
research by not reverse engineering

607
00:52:03,900 --> 00:52:07,960
firmware which would have been very easy
to find all seasons because that was

608
00:52:07,960 --> 00:52:09,160
illegal

609
00:52:09,160 --> 00:52:12,670
instead the researchers had to be sri
drive across the country and played out

610
00:52:12,670 --> 00:52:13,040
the

611
00:52:13,040 --> 00:52:17,880
amount of smog emissions in the gas
mileage on that trip so took a lot

612
00:52:17,880 --> 00:52:23,490
longer more costly and a lot harder to
do instead if you could pull apart the

613
00:52:23,490 --> 00:52:27,299
firmware then you could see yesterday's
defeat to fight device in here that's

614
00:52:27,300 --> 00:52:30,860
causing that type of thing so being able
to demonstrate public good like that

615
00:52:30,860 --> 00:52:37,610
helps to justify why we should be able
to research things that we do and if you

616
00:52:37,610 --> 00:52:43,370
look at in the USA anyways the Digital
Millennium Copyright Act has a provision

617
00:52:43,370 --> 00:52:46,069
in it where every three years

618
00:52:46,070 --> 00:52:51,890
people can submit exceptions and
exemptions from that act one of the big

619
00:52:51,890 --> 00:52:57,109
themes this year with security research
being able to demonstrate that you

620
00:52:57,110 --> 00:53:01,230
should be able to reverse engineer
firmware for medical devices for cars

621
00:53:01,230 --> 00:53:06,490
for devices consequence to be able to
find safety flaws in them to protect

622
00:53:06,490 --> 00:53:12,479
human life and the Librarian of Congress
said yes we absolutely agree with that

623
00:53:12,480 --> 00:53:17,110
argument the other arguments that are
being made against security research we

624
00:53:17,110 --> 00:53:22,260
don't buy it so again reaching out a
little bit more involved having a good

625
00:53:22,260 --> 00:53:26,770
story to tell and connecting the dots
for people is the way that we protect

626
00:53:26,770 --> 00:53:35,290
ourselves from from that we have time
for one more question

627
00:53:35,290 --> 00:53:37,890
okay thank you so much for the
presentation

