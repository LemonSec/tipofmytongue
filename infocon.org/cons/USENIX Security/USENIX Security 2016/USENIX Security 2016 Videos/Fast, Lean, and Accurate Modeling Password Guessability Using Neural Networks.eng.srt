1
00:00:10,690 --> 00:00:13,539
hi I'm Billy today I'm gonna be talking

2
00:00:13,539 --> 00:00:15,969
about our work with modeling password

3
00:00:15,969 --> 00:00:18,400
disability using neural networks and so

4
00:00:18,400 --> 00:00:19,810
this was work with my colleagues at

5
00:00:19,810 --> 00:00:21,730
Carnegie Mellon and this work is about

6
00:00:21,730 --> 00:00:23,980
how we used neural networks to develop

7
00:00:23,980 --> 00:00:26,650
more accurate and more practical methods

8
00:00:26,650 --> 00:00:29,259
for password guessing and so we keep

9
00:00:29,259 --> 00:00:31,719
hearing that passwords are dead but most

10
00:00:31,719 --> 00:00:34,780
of us use them every day and so they

11
00:00:34,780 --> 00:00:36,100
likely aren't going anywhere anytime

12
00:00:36,100 --> 00:00:39,070
soon and something important to password

13
00:00:39,070 --> 00:00:40,660
security is accurately measuring the

14
00:00:40,660 --> 00:00:42,760
strength of passwords so there are many

15
00:00:42,760 --> 00:00:44,199
situations where you might want to do

16
00:00:44,199 --> 00:00:46,870
this like Dan showed when you when the

17
00:00:46,870 --> 00:00:48,489
user is creating a password to give

18
00:00:48,489 --> 00:00:51,069
feedback or after you already have a set

19
00:00:51,069 --> 00:00:53,769
of passwords to decide for example if

20
00:00:53,769 --> 00:00:56,140
passwords created under one password

21
00:00:56,140 --> 00:00:58,030
policy are stronger than those created

22
00:00:58,030 --> 00:01:01,449
under another and so to measure the

23
00:01:01,449 --> 00:01:03,249
strength of passwords we often turn to

24
00:01:03,249 --> 00:01:06,700
modeling how many guesses it would take

25
00:01:06,700 --> 00:01:08,950
attackers to guess a password using

26
00:01:08,950 --> 00:01:11,350
different guessing methods there have

27
00:01:11,350 --> 00:01:12,670
been a variety of different guessing

28
00:01:12,670 --> 00:01:14,560
methods proposed and studied I'm gonna

29
00:01:14,560 --> 00:01:15,940
give you some background on current

30
00:01:15,940 --> 00:01:18,670
guessing methods now I said John the

31
00:01:18,670 --> 00:01:20,200
Ripper and hashcode are two popular

32
00:01:20,200 --> 00:01:23,100
password cracking tools used in practice

33
00:01:23,100 --> 00:01:25,780
they work by taking a list of dictionary

34
00:01:25,780 --> 00:01:27,910
words which might include previously

35
00:01:27,910 --> 00:01:30,340
seen passwords and natural language

36
00:01:30,340 --> 00:01:32,860
words and then transform them using

37
00:01:32,860 --> 00:01:36,160
mangling rules so the list of mangling

38
00:01:36,160 --> 00:01:39,400
rules is fed to each dictionary word to

39
00:01:39,400 --> 00:01:40,930
create a list of passwords so for

40
00:01:40,930 --> 00:01:43,300
example one dictionary word might be the

41
00:01:43,300 --> 00:01:46,090
word password and one rule might be

42
00:01:46,090 --> 00:01:48,580
appended to digits so this would

43
00:01:48,580 --> 00:01:51,900
generate all combinations of two digits

44
00:01:51,900 --> 00:01:54,370
markov models have also been studied for

45
00:01:54,370 --> 00:01:55,960
use in password guessing models they

46
00:01:55,960 --> 00:01:57,970
work by deriving statistical properties

47
00:01:57,970 --> 00:02:00,430
from text by counting what characters

48
00:02:00,430 --> 00:02:02,290
are likely to follow other characters

49
00:02:02,290 --> 00:02:05,890
and probabilistic context-free grammars

50
00:02:05,890 --> 00:02:07,870
are another popular statistical method

51
00:02:07,870 --> 00:02:10,508
for password modeling they work by

52
00:02:10,508 --> 00:02:12,700
representing passwords as different

53
00:02:12,700 --> 00:02:15,730
structures so for example L 8 D 2 means

54
00:02:15,730 --> 00:02:18,430
passwords that start with 8 lowercase

55
00:02:18,430 --> 00:02:20,470
characters and end in two digits and

56
00:02:20,470 --> 00:02:21,530
then

57
00:02:21,530 --> 00:02:23,000
generating passwords that match those

58
00:02:23,000 --> 00:02:26,180
structures so both the Markov models and

59
00:02:26,180 --> 00:02:29,090
pcfg is attempt to model passwords using

60
00:02:29,090 --> 00:02:30,530
statistical methods about what

61
00:02:30,530 --> 00:02:33,200
characters or patterns are likely to be

62
00:02:33,200 --> 00:02:35,600
present in passwords so typically they

63
00:02:35,600 --> 00:02:37,850
take some training data and recent work

64
00:02:37,850 --> 00:02:38,959
has been done showing that they're both

65
00:02:38,959 --> 00:02:42,590
very accurate models for passwords and

66
00:02:42,590 --> 00:02:43,940
all of these methods are attempting to

67
00:02:43,940 --> 00:02:46,190
solve what is close to a natural

68
00:02:46,190 --> 00:02:49,010
language processing problem so it's how

69
00:02:49,010 --> 00:02:50,750
do we generate strings of texts that

70
00:02:50,750 --> 00:02:53,330
humans would create and passwords are

71
00:02:53,330 --> 00:02:55,550
similar in many ways to natural language

72
00:02:55,550 --> 00:02:57,890
so another thing common to all of them

73
00:02:57,890 --> 00:02:58,970
is that they're trying to make very

74
00:02:58,970 --> 00:03:01,100
accurate guesses and trying to make

75
00:03:01,100 --> 00:03:04,940
guesses quickly and so I just show you

76
00:03:04,940 --> 00:03:07,640
some examples of guessing models one

77
00:03:07,640 --> 00:03:09,200
important reason we model guessing

78
00:03:09,200 --> 00:03:11,269
attacks is to give password feedback and

79
00:03:11,269 --> 00:03:13,370
so an example of something we can do

80
00:03:13,370 --> 00:03:14,930
with the results of guessing models is

81
00:03:14,930 --> 00:03:17,360
that we can put them into a password

82
00:03:17,360 --> 00:03:19,610
meter to encourage better passwords and

83
00:03:19,610 --> 00:03:20,989
we might also want to assess password

84
00:03:20,989 --> 00:03:23,090
security in the context of a research

85
00:03:23,090 --> 00:03:25,900
study or proactive password auditing

86
00:03:25,900 --> 00:03:28,459
okay so this leads us to our research

87
00:03:28,459 --> 00:03:30,170
our main research question we wanted to

88
00:03:30,170 --> 00:03:32,060
see if we can generate more accurate

89
00:03:32,060 --> 00:03:34,610
guesses for passwords we also wanted to

90
00:03:34,610 --> 00:03:36,709
know if we can do it quicker or smaller

91
00:03:36,709 --> 00:03:38,989
so the existing approaches that I showed

92
00:03:38,989 --> 00:03:41,299
you often take hours or days to give

93
00:03:41,299 --> 00:03:44,000
results and model sizes can be in the

94
00:03:44,000 --> 00:03:47,120
hundreds of megabytes or gigabytes and

95
00:03:47,120 --> 00:03:48,709
so for a lot of proactive password

96
00:03:48,709 --> 00:03:50,690
checking you do this on a giant server

97
00:03:50,690 --> 00:03:53,030
which lots of memories and cores and for

98
00:03:53,030 --> 00:03:54,140
the most part what's being done in

99
00:03:54,140 --> 00:03:56,390
browsers is act inaccurate it's Dan just

100
00:03:56,390 --> 00:03:59,540
showed you and with zxe BBN being the

101
00:03:59,540 --> 00:04:01,519
main exception to that rule and so

102
00:04:01,519 --> 00:04:03,560
accuracy speed and size of the model are

103
00:04:03,560 --> 00:04:04,970
all important for making practical

104
00:04:04,970 --> 00:04:08,209
predictions so we chose to use the

105
00:04:08,209 --> 00:04:10,100
approach of neural networks so neural

106
00:04:10,100 --> 00:04:11,450
networks are a machine learning method

107
00:04:11,450 --> 00:04:13,430
that works on tasks related to

108
00:04:13,430 --> 00:04:15,769
processing natural language so for

109
00:04:15,769 --> 00:04:18,260
things like recently they've been used

110
00:04:18,260 --> 00:04:20,779
as the basis for many technologies like

111
00:04:20,779 --> 00:04:22,010
you may have heard that neural network

112
00:04:22,010 --> 00:04:24,470
based technologies be human champions of

113
00:04:24,470 --> 00:04:26,600
the game of Go they've been used in a

114
00:04:26,600 --> 00:04:29,270
lot of classification problems and a lot

115
00:04:29,270 --> 00:04:31,099
of natural language processing tasks

116
00:04:31,099 --> 00:04:33,050
like translation or handwriting so we

117
00:04:33,050 --> 00:04:34,620
thought that they might be

118
00:04:34,620 --> 00:04:38,669
useful here too and so now I'm going to

119
00:04:38,669 --> 00:04:40,290
show you how we designed and built our

120
00:04:40,290 --> 00:04:43,229
neural network password guesser to adapt

121
00:04:43,229 --> 00:04:45,360
neural networks to our problem of

122
00:04:45,360 --> 00:04:47,550
guessing passwords and then I'm going to

123
00:04:47,550 --> 00:04:49,410
describe how our best-performing neural

124
00:04:49,410 --> 00:04:52,290
networks compared to the existing

125
00:04:52,290 --> 00:04:54,630
guessing methods like pcfg s and markov

126
00:04:54,630 --> 00:04:57,419
models I mentioned earlier and I'm also

127
00:04:57,419 --> 00:04:58,860
going to talk about how neural networks

128
00:04:58,860 --> 00:05:00,840
can be used for more effective real time

129
00:05:00,840 --> 00:05:01,949
password feedback on

130
00:05:01,949 --> 00:05:06,930
resource-constrained devices so when we

131
00:05:06,930 --> 00:05:08,580
measure password strength we typically

132
00:05:08,580 --> 00:05:10,680
try to guess passwords and see how many

133
00:05:10,680 --> 00:05:12,870
passwords it takes to get a password and

134
00:05:12,870 --> 00:05:15,270
so I'm going to show you how we actually

135
00:05:15,270 --> 00:05:17,160
generate candidate passwords using a

136
00:05:17,160 --> 00:05:19,139
neural network now and so the classical

137
00:05:19,139 --> 00:05:20,699
forms of neural networks typically only

138
00:05:20,699 --> 00:05:23,310
classify text into different groups but

139
00:05:23,310 --> 00:05:25,020
the machine learning community recently

140
00:05:25,020 --> 00:05:26,760
showed how you can use them to generate

141
00:05:26,760 --> 00:05:28,889
text and so before I show you how to

142
00:05:28,889 --> 00:05:30,570
generate text using a neural network so

143
00:05:30,570 --> 00:05:31,680
I want to walk you through the related

144
00:05:31,680 --> 00:05:34,770
problem of predicting the next character

145
00:05:34,770 --> 00:05:38,160
of a password so for example if we're

146
00:05:38,160 --> 00:05:40,099
given a password fragment like pass woo

147
00:05:40,099 --> 00:05:42,479
we might want the neural network to tell

148
00:05:42,479 --> 00:05:44,130
us that the most likely next character

149
00:05:44,130 --> 00:05:46,889
is a lowercase o a 0 or an uppercase o

150
00:05:46,889 --> 00:05:49,220
or something else so these are the

151
00:05:49,220 --> 00:05:51,780
characters that were predicting using

152
00:05:51,780 --> 00:05:53,760
the previous characters in the password

153
00:05:53,760 --> 00:05:56,190
and so now this is actually an a

154
00:05:56,190 --> 00:05:58,740
classification problem so our classes

155
00:05:58,740 --> 00:06:00,660
here are the predictions of the next

156
00:06:00,660 --> 00:06:02,460
character and so we can use the neural

157
00:06:02,460 --> 00:06:03,990
networks to do this and to train the

158
00:06:03,990 --> 00:06:06,030
neural network we give it real password

159
00:06:06,030 --> 00:06:08,340
prefixes and ask it to predict the next

160
00:06:08,340 --> 00:06:11,760
character so that was how at a high

161
00:06:11,760 --> 00:06:13,020
level how you predict the next character

162
00:06:13,020 --> 00:06:14,849
now I want to show you how you use that

163
00:06:14,849 --> 00:06:17,099
to generate passwords using this method

164
00:06:17,099 --> 00:06:18,810
and so at a high level what we're going

165
00:06:18,810 --> 00:06:20,400
to do is repeatedly predict the next

166
00:06:20,400 --> 00:06:22,349
character to generate a single password

167
00:06:22,349 --> 00:06:24,599
and in practice for a real guessing

168
00:06:24,599 --> 00:06:26,039
attack you might want to enumerate all

169
00:06:26,039 --> 00:06:28,080
passwords above a certain probability

170
00:06:28,080 --> 00:06:30,270
threshold but for now I'm just going to

171
00:06:30,270 --> 00:06:31,470
show you how to generate a single

172
00:06:31,470 --> 00:06:33,599
password so let's suppose we're starting

173
00:06:33,599 --> 00:06:35,340
from scratch we have an empty string as

174
00:06:35,340 --> 00:06:37,110
the prefix and the probability that you

175
00:06:37,110 --> 00:06:40,860
have that empty string is 100% so you

176
00:06:40,860 --> 00:06:42,240
can use the neural network to predict

177
00:06:42,240 --> 00:06:43,909
the probability of the next character

178
00:06:43,909 --> 00:06:46,340
for the empty string

179
00:06:46,340 --> 00:06:48,320
and we need to also allocate some

180
00:06:48,320 --> 00:06:50,389
probability that the password is going

181
00:06:50,389 --> 00:06:53,240
to end at this point and so let's

182
00:06:53,240 --> 00:06:54,620
generate a likely capacitor you start

183
00:06:54,620 --> 00:06:58,699
with C now we know that C has a prefix

184
00:06:58,699 --> 00:07:00,800
has a 5% chance of happening and we can

185
00:07:00,800 --> 00:07:02,300
predict the next character again and

186
00:07:02,300 --> 00:07:06,440
maybe it looks like this and we can

187
00:07:06,440 --> 00:07:08,419
choose another likely letter and we can

188
00:07:08,419 --> 00:07:10,760
do it a few more times so this keeps

189
00:07:10,760 --> 00:07:13,160
going until you end the password by

190
00:07:13,160 --> 00:07:14,900
multiplying by the probability that the

191
00:07:14,900 --> 00:07:17,449
faster it's going to end and so now

192
00:07:17,449 --> 00:07:18,919
we've guessed a password and we know

193
00:07:18,919 --> 00:07:20,389
it's probability according to the

194
00:07:20,389 --> 00:07:22,550
network so this is very similar to the

195
00:07:22,550 --> 00:07:25,040
way a Markov model does it except the

196
00:07:25,040 --> 00:07:27,130
prediction step is a lot more expressive

197
00:07:27,130 --> 00:07:29,810
than a Markov model because we're using

198
00:07:29,810 --> 00:07:31,910
a neural network here and so the network

199
00:07:31,910 --> 00:07:33,500
can approximate learning higher-level

200
00:07:33,500 --> 00:07:35,690
things about passwords other than just

201
00:07:35,690 --> 00:07:37,810
counting the most frequent engrams

202
00:07:37,810 --> 00:07:40,100
so for example one of the things that it

203
00:07:40,100 --> 00:07:41,510
might be able to do is take into a

204
00:07:41,510 --> 00:07:44,840
variable amount of the predicting text

205
00:07:44,840 --> 00:07:46,940
other than that the Markov model can't

206
00:07:46,940 --> 00:07:49,340
or learn higher-level features like how

207
00:07:49,340 --> 00:07:51,139
to count the length of a password or

208
00:07:51,139 --> 00:07:55,100
that vowels often follow consonants and

209
00:07:55,100 --> 00:07:56,990
so we can do this for as many iteration

210
00:07:56,990 --> 00:07:59,000
as we want and build up large numbers of

211
00:07:59,000 --> 00:08:00,380
passwords and then sort by the

212
00:08:00,380 --> 00:08:03,080
probability so this is how you can get

213
00:08:03,080 --> 00:08:04,820
the neural network to create guesses and

214
00:08:04,820 --> 00:08:07,910
passwords for you you can even remove

215
00:08:07,910 --> 00:08:09,110
passwords that don't adhere to a

216
00:08:09,110 --> 00:08:12,680
specific policy so before I go on I want

217
00:08:12,680 --> 00:08:14,630
to talk about password policies in this

218
00:08:14,630 --> 00:08:16,910
work we examine passwords from a variety

219
00:08:16,910 --> 00:08:19,490
of different password policies and I

220
00:08:19,490 --> 00:08:22,130
introduced some of them to you now we

221
00:08:22,130 --> 00:08:23,690
looked at a common policy where

222
00:08:23,690 --> 00:08:25,340
passwords are only required to be a

223
00:08:25,340 --> 00:08:27,710
least 8 characters here are some

224
00:08:27,710 --> 00:08:30,139
examples of those passwords we also

225
00:08:30,139 --> 00:08:31,880
looked at a policy that requires more

226
00:08:31,880 --> 00:08:34,159
character classes so four character

227
00:08:34,159 --> 00:08:35,740
classes here are uppercase lowercase

228
00:08:35,740 --> 00:08:39,860
digits or special characters and here

229
00:08:39,860 --> 00:08:41,179
are some examples of those types of

230
00:08:41,179 --> 00:08:42,799
passwords this is also another commonly

231
00:08:42,799 --> 00:08:45,980
used policy and practice in addition to

232
00:08:45,980 --> 00:08:48,560
those two common policies we looked at

233
00:08:48,560 --> 00:08:50,420
policies with longer length requirements

234
00:08:50,420 --> 00:08:52,310
because prior work from our group found

235
00:08:52,310 --> 00:08:53,959
them to be just as usable and more

236
00:08:53,959 --> 00:08:57,530
secure than commonly used policies and

237
00:08:57,530 --> 00:08:59,510
so as the ability as the ability to

238
00:08:59,510 --> 00:09:00,180
crack

239
00:09:00,180 --> 00:09:02,550
increases we might find more people

240
00:09:02,550 --> 00:09:04,950
moving to these longer policies but one

241
00:09:04,950 --> 00:09:06,180
of the ones we looked at required at

242
00:09:06,180 --> 00:09:08,190
least 16 characters and so here are some

243
00:09:08,190 --> 00:09:11,070
examples of though we also looked at a

244
00:09:11,070 --> 00:09:12,930
longer policy that balances the length

245
00:09:12,930 --> 00:09:14,850
and rec complexity requirements in three

246
00:09:14,850 --> 00:09:19,230
class twelve is an example of that so

247
00:09:19,230 --> 00:09:20,220
now I'm going to talk about the design

248
00:09:20,220 --> 00:09:22,530
space and the experiments for our neural

249
00:09:22,530 --> 00:09:25,620
network password guesser so neural

250
00:09:25,620 --> 00:09:27,210
networks are a huge area of ongoing

251
00:09:27,210 --> 00:09:29,370
research and the machine learning

252
00:09:29,370 --> 00:09:31,710
community and there's a large design way

253
00:09:31,710 --> 00:09:33,420
space of how what's the best way to

254
00:09:33,420 --> 00:09:35,490
train them we experimented with some

255
00:09:35,490 --> 00:09:38,250
aspects of the design space that we

256
00:09:38,250 --> 00:09:41,850
thought would be most most impact the

257
00:09:41,850 --> 00:09:44,790
guessing accuracy but is impossible to

258
00:09:44,790 --> 00:09:47,610
exhaustively experiment with every

259
00:09:47,610 --> 00:09:50,340
conceivable dimension and what you can

260
00:09:50,340 --> 00:09:51,660
do with neural networks seems to change

261
00:09:51,660 --> 00:09:53,790
every day so I'm going to describe our

262
00:09:53,790 --> 00:09:56,370
experiments so we needed to decide how

263
00:09:56,370 --> 00:09:58,980
large of a model to use so I previously

264
00:09:58,980 --> 00:10:01,050
mentioned that existing password

265
00:10:01,050 --> 00:10:03,480
guessing approaches often require models

266
00:10:03,480 --> 00:10:06,930
on the order of Giga bytes so we tested

267
00:10:06,930 --> 00:10:08,460
different size neural network models

268
00:10:08,460 --> 00:10:09,840
here to see if we could use a smaller

269
00:10:09,840 --> 00:10:13,110
model to estimate password strength with

270
00:10:13,110 --> 00:10:15,840
reasonable accuracy and so larger models

271
00:10:15,840 --> 00:10:17,340
have more parameters that tell you how

272
00:10:17,340 --> 00:10:19,410
to compute the next the probability of

273
00:10:19,410 --> 00:10:20,550
the next character and are more

274
00:10:20,550 --> 00:10:23,040
expressive but could over fit the data

275
00:10:23,040 --> 00:10:25,320
so we looked at a 3 megabyte model and a

276
00:10:25,320 --> 00:10:27,480
60 megabyte model you can look at lots

277
00:10:27,480 --> 00:10:29,100
of points in between those two but we

278
00:10:29,100 --> 00:10:30,300
chose the small one it's something

279
00:10:30,300 --> 00:10:32,310
roughly the size that could be sent to a

280
00:10:32,310 --> 00:10:33,600
browser and I'll talk more about that

281
00:10:33,600 --> 00:10:36,180
later but the larger one was more

282
00:10:36,180 --> 00:10:37,770
limited by how much we could reasonably

283
00:10:37,770 --> 00:10:41,430
compute on our hardware we also examined

284
00:10:41,430 --> 00:10:43,110
a specialized method of training called

285
00:10:43,110 --> 00:10:46,140
transference learning and prior work has

286
00:10:46,140 --> 00:10:47,430
shown that having more training data

287
00:10:47,430 --> 00:10:49,830
leads to more accurate password guessing

288
00:10:49,830 --> 00:10:51,750
so unfortunately for a lot of the novel

289
00:10:51,750 --> 00:10:55,050
password composition policies we don't

290
00:10:55,050 --> 00:10:56,640
have large amounts of training data and

291
00:10:56,640 --> 00:10:58,800
we thought that transference learning

292
00:10:58,800 --> 00:11:00,000
might help and so the idea of

293
00:11:00,000 --> 00:11:01,620
transference learning is that you would

294
00:11:01,620 --> 00:11:04,170
be able to Train what one network

295
00:11:04,170 --> 00:11:06,780
learned and transfer that to another

296
00:11:06,780 --> 00:11:09,210
network it's commonly used to deal with

297
00:11:09,210 --> 00:11:11,430
data sparsity so for example on our

298
00:11:11,430 --> 00:11:13,590
training data we have lots of passwords

299
00:11:13,590 --> 00:11:16,440
to one class a policy roughly 50 million

300
00:11:16,440 --> 00:11:19,470
but many fewer that match a three class

301
00:11:19,470 --> 00:11:22,950
12 policy roughly fifty thousand so if

302
00:11:22,950 --> 00:11:24,330
we only train a network on those three

303
00:11:24,330 --> 00:11:25,950
class twelve policies it's going to get

304
00:11:25,950 --> 00:11:27,390
us worse than a network trained on all

305
00:11:27,390 --> 00:11:30,000
of the one Class eight because it's seen

306
00:11:30,000 --> 00:11:32,970
fewer examples and so ideally we would

307
00:11:32,970 --> 00:11:34,170
be able to train the network on all

308
00:11:34,170 --> 00:11:35,700
passwords and then transfer what that

309
00:11:35,700 --> 00:11:38,160
network learned to a specialized network

310
00:11:38,160 --> 00:11:42,060
for three class twelve passwords we also

311
00:11:42,060 --> 00:11:43,920
experimented with including natural

312
00:11:43,920 --> 00:11:46,500
language non password data in the

313
00:11:46,500 --> 00:11:48,690
training sets and then in using

314
00:11:48,690 --> 00:11:50,670
different training sets so prior work

315
00:11:50,670 --> 00:11:51,900
and other guessing methods showed that

316
00:11:51,900 --> 00:11:53,640
using natural language dictionaries can

317
00:11:53,640 --> 00:11:56,550
improve guessing intuitively especially

318
00:11:56,550 --> 00:11:59,010
for longer passwords that are commonly

319
00:11:59,010 --> 00:12:04,260
made up of words and phrases we tested

320
00:12:04,260 --> 00:12:06,210
know a number of other design dimensions

321
00:12:06,210 --> 00:12:08,040
which we discuss in more detail in the

322
00:12:08,040 --> 00:12:09,900
paper and so now I'm going to describe

323
00:12:09,900 --> 00:12:11,580
how he tested both our neural networks

324
00:12:11,580 --> 00:12:13,800
and the other approaches and at a high

325
00:12:13,800 --> 00:12:15,660
level we use each method to generate a

326
00:12:15,660 --> 00:12:18,720
list of to guess a list of test set

327
00:12:18,720 --> 00:12:19,980
passwords and then measured the number

328
00:12:19,980 --> 00:12:22,680
of correctly guessed passwords for our

329
00:12:22,680 --> 00:12:24,030
training data we used a variety of

330
00:12:24,030 --> 00:12:26,610
leaked password sets and we had multiple

331
00:12:26,610 --> 00:12:29,640
sources of test data so we use passwords

332
00:12:29,640 --> 00:12:31,470
that people created in the context of

333
00:12:31,470 --> 00:12:33,600
Mechanical Turk studies about passwords

334
00:12:33,600 --> 00:12:36,030
from previous work in our group this

335
00:12:36,030 --> 00:12:38,310
allows us to look at many non-standard

336
00:12:38,310 --> 00:12:39,930
password policies created in a

337
00:12:39,930 --> 00:12:42,630
controlled environment and we also used

338
00:12:42,630 --> 00:12:46,470
real leaked password set data from the

339
00:12:46,470 --> 00:12:49,140
zero-zero web post leak which wasn't

340
00:12:49,140 --> 00:12:52,800
included in art in our training data and

341
00:12:52,800 --> 00:12:54,720
so then we used a new technique to guest

342
00:12:54,720 --> 00:12:56,550
of guest number calculation

343
00:12:56,550 --> 00:12:58,770
introduced by other researchers at CCS

344
00:12:58,770 --> 00:13:00,540
last year that takes advantage of monte

345
00:13:00,540 --> 00:13:03,839
carlo techniques to estimate accuracy

346
00:13:03,839 --> 00:13:07,550
for huge for large numbers of guesses

347
00:13:07,820 --> 00:13:10,710
and so we tried training the neural

348
00:13:10,710 --> 00:13:12,149
network along the different dimensions

349
00:13:12,149 --> 00:13:13,589
of the design space that I showed you

350
00:13:13,589 --> 00:13:15,810
earlier into the experiments I'm about

351
00:13:15,810 --> 00:13:18,000
to show you our about how do we train

352
00:13:18,000 --> 00:13:19,710
the best neural network guesser so

353
00:13:19,710 --> 00:13:21,120
they're comparing one version of the

354
00:13:21,120 --> 00:13:25,020
neural network guesser to another so

355
00:13:25,020 --> 00:13:26,250
first I'm going to show you how to read

356
00:13:26,250 --> 00:13:27,870
the graphs of our results I'm going to

357
00:13:27,870 --> 00:13:29,160
show you lots of graphs that look like

358
00:13:29,160 --> 00:13:32,880
this they show how many correct guesses

359
00:13:32,880 --> 00:13:34,470
different methods make at different

360
00:13:34,470 --> 00:13:36,630
numbers of guesses so the x-axis shows

361
00:13:36,630 --> 00:13:39,750
how many guesses were made and the

362
00:13:39,750 --> 00:13:41,490
y-axis shows the percent of guests

363
00:13:41,490 --> 00:13:43,470
passwords guessed at that

364
00:13:43,470 --> 00:13:45,839
Gessler so more accurate guessing

365
00:13:45,839 --> 00:13:48,209
methods are higher so for example here

366
00:13:48,209 --> 00:13:49,709
the green line is more accurate than the

367
00:13:49,709 --> 00:13:52,470
orange line and so we found that

368
00:13:52,470 --> 00:13:54,570
transference learning improves the

369
00:13:54,570 --> 00:13:56,430
accuracy of guessing so this experiment

370
00:13:56,430 --> 00:13:59,250
was done with one class 16 data and the

371
00:13:59,250 --> 00:14:01,350
effect was a significant so for example

372
00:14:01,350 --> 00:14:05,490
at 10 to the 15th we have a 15% passer

373
00:14:05,490 --> 00:14:07,709
of passwords guessed versus 22% so

374
00:14:07,709 --> 00:14:09,750
that's almost a 50% increase in guessing

375
00:14:09,750 --> 00:14:13,890
effectiveness which is very good in

376
00:14:13,890 --> 00:14:15,330
contrast we found that including

377
00:14:15,330 --> 00:14:17,370
natural-language data with the real

378
00:14:17,370 --> 00:14:19,200
password training data didn't didn't

379
00:14:19,200 --> 00:14:21,900
help so we did this experiment also with

380
00:14:21,900 --> 00:14:24,060
the one class 16 passwords which we

381
00:14:24,060 --> 00:14:25,529
thought would most benefit from the

382
00:14:25,529 --> 00:14:28,470
natural language because those passwords

383
00:14:28,470 --> 00:14:31,220
have lots of words and phrases of them

384
00:14:31,220 --> 00:14:33,570
we found that our larger network in

385
00:14:33,570 --> 00:14:35,250
general perform better than our smaller

386
00:14:35,250 --> 00:14:37,050
network however the surprising thing is

387
00:14:37,050 --> 00:14:40,440
that it often wasn't that much better so

388
00:14:40,440 --> 00:14:42,209
often the smaller model work nearly as

389
00:14:42,209 --> 00:14:44,160
well as the larger one so here the

390
00:14:44,160 --> 00:14:45,990
dotted line here is the little larger

391
00:14:45,990 --> 00:14:48,270
network especially for the two shorter

392
00:14:48,270 --> 00:14:52,430
policies that we tested however was

393
00:14:52,430 --> 00:14:54,390
potentially a larger difference for the

394
00:14:54,390 --> 00:14:58,560
longer one class 16 policy and sometimes

395
00:14:58,560 --> 00:15:00,180
the larger network wasn't as good so for

396
00:15:00,180 --> 00:15:01,950
the web host passwords for example the

397
00:15:01,950 --> 00:15:03,360
smaller network actually worked a little

398
00:15:03,360 --> 00:15:05,520
bit better now you may not be able to

399
00:15:05,520 --> 00:15:06,870
see the purple lines because they

400
00:15:06,870 --> 00:15:08,040
overlap with the other ones but it's

401
00:15:08,040 --> 00:15:10,320
likely that a network size in between

402
00:15:10,320 --> 00:15:15,350
the two would would work better here

403
00:15:18,070 --> 00:15:20,480
and so at the end of the experiments I

404
00:15:20,480 --> 00:15:22,400
just talked about we had a configuration

405
00:15:22,400 --> 00:15:24,470
of a neural network that performed the

406
00:15:24,470 --> 00:15:26,330
best compared to other variations on

407
00:15:26,330 --> 00:15:29,300
neural network training that we tried so

408
00:15:29,300 --> 00:15:30,200
now I want to show you how our

409
00:15:30,200 --> 00:15:32,840
best-performing neural networks did

410
00:15:32,840 --> 00:15:34,820
compared to the best performing existing

411
00:15:34,820 --> 00:15:38,780
methods like PCF GS and markov models so

412
00:15:38,780 --> 00:15:40,280
here's a graph showing the other

413
00:15:40,280 --> 00:15:42,860
guessing methods that we use these

414
00:15:42,860 --> 00:15:44,600
measurements come from the best tunings

415
00:15:44,600 --> 00:15:46,550
of these algorithms that we could find

416
00:15:46,550 --> 00:15:49,130
in the literature and from our usenix

417
00:15:49,130 --> 00:15:51,680
paper last year and you'll notice that

418
00:15:51,680 --> 00:15:53,450
john the ripper and hash katz stopped

419
00:15:53,450 --> 00:15:55,370
earlier than the others this is because

420
00:15:55,370 --> 00:15:56,750
they were calculated by enumerating

421
00:15:56,750 --> 00:15:59,030
guesses rather than the Monte Carlo

422
00:15:59,030 --> 00:16:01,130
estimation that can go to very high

423
00:16:01,130 --> 00:16:06,050
numbers of guesses and so here's our the

424
00:16:06,050 --> 00:16:07,970
way our neural networks and they do

425
00:16:07,970 --> 00:16:09,710
better than anything else especially at

426
00:16:09,710 --> 00:16:11,900
high guest numbers and we believe this

427
00:16:11,900 --> 00:16:13,100
is because the neural networks are

428
00:16:13,100 --> 00:16:14,690
particularly good at guessing novel

429
00:16:14,690 --> 00:16:17,810
passwords here the black line is the

430
00:16:17,810 --> 00:16:20,750
minimum guess across all approaches so

431
00:16:20,750 --> 00:16:22,610
this is an approximation for the sum of

432
00:16:22,610 --> 00:16:24,320
all the methods including the neural

433
00:16:24,320 --> 00:16:26,690
networks so should we just use neural

434
00:16:26,690 --> 00:16:28,310
networks well not really because other

435
00:16:28,310 --> 00:16:30,890
guessing methods guess slightly

436
00:16:30,890 --> 00:16:33,350
different passwords and so using all of

437
00:16:33,350 --> 00:16:35,240
them guess more passwords than just in

438
00:16:35,240 --> 00:16:37,940
our allowance but if you would going to

439
00:16:37,940 --> 00:16:40,940
use a single method than you it seems

440
00:16:40,940 --> 00:16:42,470
likely that neural networks would be the

441
00:16:42,470 --> 00:16:45,710
best choice so for the more specialized

442
00:16:45,710 --> 00:16:47,330
for class 8 data said the trend is the

443
00:16:47,330 --> 00:16:48,950
same with even better performance and

444
00:16:48,950 --> 00:16:51,170
for the very for very specialized

445
00:16:51,170 --> 00:16:53,090
policies it's even better so here's a 3

446
00:16:53,090 --> 00:16:57,350
class 12 data at 10 to the 16 guesses

447
00:16:57,350 --> 00:16:59,210
neural networks are guessing roughly 45%

448
00:16:59,210 --> 00:17:01,700
of passwords all the next best method is

449
00:17:01,700 --> 00:17:04,310
guessing roughly 30% so this is also

450
00:17:04,310 --> 00:17:06,829
like a 50% improvement so that compared

451
00:17:06,829 --> 00:17:08,599
to the next best single method this

452
00:17:08,599 --> 00:17:11,270
trend holds across all of our data sets

453
00:17:11,270 --> 00:17:13,490
to varying agrees and we found that the

454
00:17:13,490 --> 00:17:15,380
neural network seemed to do best at high

455
00:17:15,380 --> 00:17:17,869
guest numbers and with more specialized

456
00:17:17,869 --> 00:17:22,339
password policies so we just saw that

457
00:17:22,339 --> 00:17:23,750
neural networks good yes passwords

458
00:17:23,750 --> 00:17:26,300
better than any other single method now

459
00:17:26,300 --> 00:17:27,650
I want to switch gears to talk about how

460
00:17:27,650 --> 00:17:28,319
we use them

461
00:17:28,319 --> 00:17:30,509
practical real time password feedback

462
00:17:30,509 --> 00:17:32,940
and so as I mentioned often that

463
00:17:32,940 --> 00:17:35,309
feedback is in the form of a meter that

464
00:17:35,309 --> 00:17:38,009
tells people how strong their password

465
00:17:38,009 --> 00:17:39,809
is and we'd like this feedback to be

466
00:17:39,809 --> 00:17:41,309
accurate with respect to how hard

467
00:17:41,309 --> 00:17:43,799
attackers find it to guess passwords so

468
00:17:43,799 --> 00:17:45,149
when it says the password is weak it's

469
00:17:45,149 --> 00:17:46,350
likely to be guessing when it says a

470
00:17:46,350 --> 00:17:47,940
strong password is not likely to be

471
00:17:47,940 --> 00:17:50,399
guessed however the current state of the

472
00:17:50,399 --> 00:17:53,000
feedback with the exception of zxc vbn

473
00:17:53,000 --> 00:17:55,350
that you just turn about is that you

474
00:17:55,350 --> 00:17:57,059
either get very accurate feedback by

475
00:17:57,059 --> 00:17:59,159
performing a cracking attack with

476
00:17:59,159 --> 00:18:01,950
multiple guessing methods or you can get

477
00:18:01,950 --> 00:18:03,389
very quick feedback that's not as

478
00:18:03,389 --> 00:18:04,919
accurate by doing things like counting

479
00:18:04,919 --> 00:18:06,000
the number of different character

480
00:18:06,000 --> 00:18:07,460
classes of the length of the password

481
00:18:07,460 --> 00:18:09,539
and so there's a need for feedback

482
00:18:09,539 --> 00:18:11,429
that's quick enough to be done in real

483
00:18:11,429 --> 00:18:13,620
time but also practical enough to be

484
00:18:13,620 --> 00:18:17,250
used on client machines and as accurate

485
00:18:17,250 --> 00:18:20,220
as performing a real guessing attack and

486
00:18:20,220 --> 00:18:21,330
so if you look at the other guessing

487
00:18:21,330 --> 00:18:22,649
methods they take gigabytes of disk

488
00:18:22,649 --> 00:18:24,570
space and so this is not practical for

489
00:18:24,570 --> 00:18:26,250
getting real-time feedback on a client

490
00:18:26,250 --> 00:18:28,080
machine you're not gonna be able to give

491
00:18:28,080 --> 00:18:31,019
this to do a web browser to send to

492
00:18:31,019 --> 00:18:32,070
people when they sign up for your

493
00:18:32,070 --> 00:18:34,289
service but the interesting thing about

494
00:18:34,289 --> 00:18:36,570
our best-performing neural networks was

495
00:18:36,570 --> 00:18:38,570
that it was very small like 60 megabytes

496
00:18:38,570 --> 00:18:40,470
compared to the gigabytes and when you

497
00:18:40,470 --> 00:18:42,299
consider even the smaller version was 3

498
00:18:42,299 --> 00:18:43,889
megabytes that's something that's more

499
00:18:43,889 --> 00:18:46,710
in the range that you can send to a

500
00:18:46,710 --> 00:18:48,740
client and that still perform very well

501
00:18:48,740 --> 00:18:50,820
and so you thought maybe there's hope

502
00:18:50,820 --> 00:18:53,519
for compressing an accurate model for

503
00:18:53,519 --> 00:18:56,129
use in a webpage this way the other

504
00:18:56,129 --> 00:18:57,690
thing is that they typically take hours

505
00:18:57,690 --> 00:19:00,059
or days on highly parallel servers using

506
00:19:00,059 --> 00:19:02,220
over like 50 words of 50 cores in our

507
00:19:02,220 --> 00:19:04,529
experiments this is also something

508
00:19:04,529 --> 00:19:05,970
that's not likely to happen in the

509
00:19:05,970 --> 00:19:08,389
context of real time password feedback

510
00:19:08,389 --> 00:19:10,350
and so this leads us to our other

511
00:19:10,350 --> 00:19:12,480
research question we wanted to see if

512
00:19:12,480 --> 00:19:14,639
neural networks could be used to give

513
00:19:14,639 --> 00:19:16,500
real time password feedback on

514
00:19:16,500 --> 00:19:18,809
resource-constrained machines and the

515
00:19:18,809 --> 00:19:20,190
reason we wanted to do this is to

516
00:19:20,190 --> 00:19:21,600
combine the accuracy of modeling

517
00:19:21,600 --> 00:19:23,789
password guessing and a principled way

518
00:19:23,789 --> 00:19:26,279
with fast real-time feedback the other

519
00:19:26,279 --> 00:19:27,870
benefit of using neural networks is that

520
00:19:27,870 --> 00:19:29,879
you can retrain a new one for different

521
00:19:29,879 --> 00:19:32,220
policies or for specific sites without

522
00:19:32,220 --> 00:19:33,309
coming up with

523
00:19:33,309 --> 00:19:36,519
rulz so these are the targets we set

524
00:19:36,519 --> 00:19:38,799
ourselves for performance it has to be

525
00:19:38,799 --> 00:19:41,950
small roughly less than a megabyte and

526
00:19:41,950 --> 00:19:44,039
so the average make of a webpage now is

527
00:19:44,039 --> 00:19:46,509
32 megabytes and it's always growing so

528
00:19:46,509 --> 00:19:48,370
in comparison to that 1 megabyte isn't a

529
00:19:48,370 --> 00:19:49,990
lot especially when you consider that

530
00:19:49,990 --> 00:19:51,370
you're only going to load this when your

531
00:19:51,370 --> 00:19:54,820
user creates a new password it also has

532
00:19:54,820 --> 00:19:57,399
to give low latency feedback so the

533
00:19:57,399 --> 00:19:59,620
threshold of recognition is roughly at

534
00:19:59,620 --> 00:20:01,690
about a tenth of a second so something

535
00:20:01,690 --> 00:20:04,389
less than that would be good and we have

536
00:20:04,389 --> 00:20:06,220
to do this while executing in JavaScript

537
00:20:06,220 --> 00:20:07,749
which is sometimes challenging to make

538
00:20:07,749 --> 00:20:10,179
fast and it should be accurate and by

539
00:20:10,179 --> 00:20:12,460
accurate here I mean accurate to within

540
00:20:12,460 --> 00:20:15,100
a rough order of magnitude so typically

541
00:20:15,100 --> 00:20:16,629
these results are presented in a highly

542
00:20:16,629 --> 00:20:18,940
bucket eyes way the users like strong or

543
00:20:18,940 --> 00:20:22,539
weak so we're looking for something that

544
00:20:22,539 --> 00:20:26,230
is in the same order of magnitude and so

545
00:20:26,230 --> 00:20:28,419
our approach to making meters small is

546
00:20:28,419 --> 00:20:29,799
first we started out with our small

547
00:20:29,799 --> 00:20:33,549
neural network then we quantize the

548
00:20:33,549 --> 00:20:35,350
parameters of the model only storing the

549
00:20:35,350 --> 00:20:38,110
first three decimal places so the

550
00:20:38,110 --> 00:20:40,090
parameters of the model are the numbers

551
00:20:40,090 --> 00:20:41,320
that define how to predict the next

552
00:20:41,320 --> 00:20:43,720
character and so this means only storing

553
00:20:43,720 --> 00:20:45,249
the first three decimal places rather

554
00:20:45,249 --> 00:20:46,749
than storing the full seven to eight

555
00:20:46,749 --> 00:20:48,850
decimal places for a full floating-point

556
00:20:48,850 --> 00:20:51,639
number so then we compress the model

557
00:20:51,639 --> 00:20:53,679
using existing lossless compression

558
00:20:53,679 --> 00:20:56,919
aided to all browsers like gzip and we

559
00:20:56,919 --> 00:20:58,749
did a few other things that you can read

560
00:20:58,749 --> 00:21:00,309
about in more details in the paper and

561
00:21:00,309 --> 00:21:02,470
our final model was eight hundred and

562
00:21:02,470 --> 00:21:06,399
fifty kilobytes which is less than our

563
00:21:06,399 --> 00:21:08,950
target and so then to make our meter

564
00:21:08,950 --> 00:21:11,590
fast first we pre computed in exact

565
00:21:11,590 --> 00:21:14,679
guessed numbers so the expensive process

566
00:21:14,679 --> 00:21:17,860
is computing in the methods that I just

567
00:21:17,860 --> 00:21:19,389
showed is computing guessed numbers but

568
00:21:19,389 --> 00:21:20,860
the probabilities are a lot easier to

569
00:21:20,860 --> 00:21:23,499
compute and so we pre compute guessed

570
00:21:23,499 --> 00:21:25,179
numbers and then we cache intermediate

571
00:21:25,179 --> 00:21:27,009
results so to calculate the probability

572
00:21:27,009 --> 00:21:29,619
of a password you really need to make

573
00:21:29,619 --> 00:21:31,389
one calculation of the neural network

574
00:21:31,389 --> 00:21:33,249
for each character but however the

575
00:21:33,249 --> 00:21:34,809
common case is adding or removing a

576
00:21:34,809 --> 00:21:37,600
character for the end so that first

577
00:21:37,600 --> 00:21:39,340
computation stays the same and so you

578
00:21:39,340 --> 00:21:41,049
can cache those and we also run it on a

579
00:21:41,049 --> 00:21:42,789
separate threat to avoid freezing the UI

580
00:21:42,789 --> 00:21:44,919
so in the common case we got the

581
00:21:44,919 --> 00:21:47,140
response time to be on average 17

582
00:21:47,140 --> 00:21:49,210
seconds which is well below the

583
00:21:49,210 --> 00:21:51,640
threshold of recognition and so we

584
00:21:51,640 --> 00:21:52,930
wanted to do this all of this without

585
00:21:52,930 --> 00:21:55,660
compromising too much on accuracy and so

586
00:21:55,660 --> 00:21:56,950
many of the modifications that I was

587
00:21:56,950 --> 00:21:58,390
telling you about for size and speed

588
00:21:58,390 --> 00:22:01,180
reduced the accuracy of the system so we

589
00:22:01,180 --> 00:22:02,800
measured how much these changes affect

590
00:22:02,800 --> 00:22:04,900
the accuracy so here's a guessing curve

591
00:22:04,900 --> 00:22:06,310
of the small neural network on the

592
00:22:06,310 --> 00:22:08,440
server and here's the browser

593
00:22:08,440 --> 00:22:10,870
implementation and so you may not be

594
00:22:10,870 --> 00:22:13,420
able to see this so let's zoom in you

595
00:22:13,420 --> 00:22:15,340
can see the little Sawtooths here result

596
00:22:15,340 --> 00:22:17,620
from pre computing the guest numbers and

597
00:22:17,620 --> 00:22:21,250
so notably like you can see here the

598
00:22:21,250 --> 00:22:24,790
this quantization can be likely safe and

599
00:22:24,790 --> 00:22:26,110
that you might underestimate the

600
00:22:26,110 --> 00:22:28,570
strength of a password as but not

601
00:22:28,570 --> 00:22:30,160
overestimate it so you can notice that

602
00:22:30,160 --> 00:22:31,560
the orange curve is for the most part

603
00:22:31,560 --> 00:22:34,600
completely below the green curve so we

604
00:22:34,600 --> 00:22:36,130
discuss more details about the accuracy

605
00:22:36,130 --> 00:22:37,600
of our client-side guess are in the

606
00:22:37,600 --> 00:22:41,350
paper so in summary we demonstrated that

607
00:22:41,350 --> 00:22:43,630
how to build a password kasseri guests

608
00:22:43,630 --> 00:22:46,030
are using neural networks and we showed

609
00:22:46,030 --> 00:22:48,430
how you can use it to do better than

610
00:22:48,430 --> 00:22:51,850
existing guessing methods and finally we

611
00:22:51,850 --> 00:22:53,200
also showed how you can use it to create

612
00:22:53,200 --> 00:22:55,840
fully client-side password feedback and

613
00:22:55,840 --> 00:22:58,180
so we release the code and you can find

614
00:22:58,180 --> 00:22:59,470
the link of the paper or go to

615
00:22:59,470 --> 00:23:03,200
github.com slash 2 cups lab thank you

616
00:23:03,200 --> 00:23:06,559
[Applause]

617
00:23:08,910 --> 00:23:16,420
have a time for questions hi

618
00:23:16,420 --> 00:23:19,660
Laurent Raymer from epfl to make your

619
00:23:19,660 --> 00:23:21,850
model smaller have you looked at a

620
00:23:21,850 --> 00:23:25,720
process called distillation so this is

621
00:23:25,720 --> 00:23:28,120
something rather I think rather new in

622
00:23:28,120 --> 00:23:30,700
machine learning we're been proposed by

623
00:23:30,700 --> 00:23:32,830
people at Google where you first train a

624
00:23:32,830 --> 00:23:35,980
large neural network and then try to

625
00:23:35,980 --> 00:23:38,430
train a smaller neural network to behave

626
00:23:38,430 --> 00:23:41,440
like the larger one and apparently you

627
00:23:41,440 --> 00:23:43,900
can get very good results using these

628
00:23:43,900 --> 00:23:46,930
kind of techniques I'm not familiar with

629
00:23:46,930 --> 00:23:48,370
that one specifically where we did

630
00:23:48,370 --> 00:23:49,990
something related where we tried to

631
00:23:49,990 --> 00:23:51,910
tutor a smaller network with the larger

632
00:23:51,910 --> 00:23:54,730
one and we didn't get great results from

633
00:23:54,730 --> 00:24:03,550
that high wanna me from Ecole

634
00:24:03,550 --> 00:24:06,010
Polytechnique I'm wondering how

635
00:24:06,010 --> 00:24:07,450
frequently do you think you would need

636
00:24:07,450 --> 00:24:09,760
to retrain your model in 4 yarder

637
00:24:09,760 --> 00:24:12,070
they're estimators because every time

638
00:24:12,070 --> 00:24:14,020
you retrain you would have to patch the

639
00:24:14,020 --> 00:24:16,540
the system so I frequently do you need

640
00:24:16,540 --> 00:24:19,450
to patch to have the model stay accurate

641
00:24:19,450 --> 00:24:25,530
with the new guessing method so if the

642
00:24:25,530 --> 00:24:27,670
ideally you would only have to train it

643
00:24:27,670 --> 00:24:30,850
once for the system if the passwords

644
00:24:30,850 --> 00:24:32,710
that are being used in the system keep

645
00:24:32,710 --> 00:24:34,480
changing then it'll depend on how fast

646
00:24:34,480 --> 00:24:39,130
the passwords being used change for it

647
00:24:39,130 --> 00:24:43,780
to stay accurate Rick Ferro I'm curious

648
00:24:43,780 --> 00:24:46,690
in one case you're a 3 megabyte model

649
00:24:46,690 --> 00:24:48,730
actually was more accurate than your 6

650
00:24:48,730 --> 00:24:50,680
megabyte model 60 megabyte Wow

651
00:24:50,680 --> 00:24:57,850
do you have any idea why so we think

652
00:24:57,850 --> 00:25:00,070
that it might be related to the type of

653
00:25:00,070 --> 00:25:01,750
training data that was used for that

654
00:25:01,750 --> 00:25:06,450
specific test we

655
00:25:07,560 --> 00:25:10,380
the web host passwords were a little bit

656
00:25:10,380 --> 00:25:12,210
different than the training data and so

657
00:25:12,210 --> 00:25:14,220
it might be an interaction between that

658
00:25:14,220 --> 00:25:17,940
it's hard to speculate on it's hard to

659
00:25:17,940 --> 00:25:20,850
speculate on neural networks and I guess

660
00:25:20,850 --> 00:25:22,680
in general I surprised that there wasn't

661
00:25:22,680 --> 00:25:24,330
more difference between the two models

662
00:25:24,330 --> 00:25:26,670
did you try any other larger models than

663
00:25:26,670 --> 00:25:29,400
the 60 mega might model we didn't try

664
00:25:29,400 --> 00:25:32,010
larger ones but for the reason you

665
00:25:32,010 --> 00:25:33,960
stated you know it's just curious okay

666
00:25:33,960 --> 00:25:34,670
thank you

667
00:25:34,670 --> 00:25:38,190
time for one more quick question quick

668
00:25:38,190 --> 00:25:39,630
though Richard Johnson I'll make it

669
00:25:39,630 --> 00:25:40,910
really quick have you considered

670
00:25:40,910 --> 00:25:44,010
focusing a neural network in on ocl hash

671
00:25:44,010 --> 00:25:47,640
cat or common generic attacker tools to

672
00:25:47,640 --> 00:25:50,880
defeat those specific threats I'm sorry

673
00:25:50,880 --> 00:25:53,760
could you say that again specific

674
00:25:53,760 --> 00:25:56,250
attackers particularly pen testers will

675
00:25:56,250 --> 00:25:58,440
use ocl hash cat or john the ripper in

676
00:25:58,440 --> 00:25:59,100
default mode

677
00:25:59,100 --> 00:26:02,790
have you considered doing a neural

678
00:26:02,790 --> 00:26:04,200
network that's focused on defeating

679
00:26:04,200 --> 00:26:06,420
those threats to take away 90% of your

680
00:26:06,420 --> 00:26:09,720
password breaks and not not bother users

681
00:26:09,720 --> 00:26:11,370
with the attackers who are really gonna

682
00:26:11,370 --> 00:26:13,170
do their homework and pick good word

683
00:26:13,170 --> 00:26:16,080
lists so you're asking about training a

684
00:26:16,080 --> 00:26:18,270
neural network that mimics say a hash

685
00:26:18,270 --> 00:26:20,370
cat or no we haven't looked at that but

686
00:26:20,370 --> 00:26:22,230
that would be a good thing to look at

687
00:26:22,230 --> 00:26:24,120
for future one oh great one we think

688
00:26:24,120 --> 00:26:26,929
speaker again

