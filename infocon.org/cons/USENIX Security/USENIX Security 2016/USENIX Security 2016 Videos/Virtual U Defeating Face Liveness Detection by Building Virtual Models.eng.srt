1
00:00:11,360 --> 00:00:13,360
so today i'd like to present our work on

2
00:00:13,360 --> 00:00:15,440
breaking face authentication using 3d

3
00:00:15,440 --> 00:00:17,840
reconstruction and virtual reality

4
00:00:17,840 --> 00:00:20,320
arc is titled virtual you defeating face

5
00:00:20,320 --> 00:00:22,160
liveness detection by building virtual

6
00:00:22,160 --> 00:00:24,720
models from your public photos

7
00:00:24,720 --> 00:00:26,400
this work was conducted at the

8
00:00:26,400 --> 00:00:27,680
university of north carolina at chapel

9
00:00:27,680 --> 00:00:31,679
hill by me true price uh along with izu

10
00:00:31,679 --> 00:00:35,280
jan michael from and fabian monroes

11
00:00:35,280 --> 00:00:37,120
face authentication has become popular

12
00:00:37,120 --> 00:00:38,960
in recent years as a form of biometric

13
00:00:38,960 --> 00:00:40,239
security

14
00:00:40,239 --> 00:00:42,399
the idea is fairly intuitive simply show

15
00:00:42,399 --> 00:00:43,840
your face to a camera and the

16
00:00:43,840 --> 00:00:45,440
authentication system will verify it's

17
00:00:45,440 --> 00:00:46,800
actually you

18
00:00:46,800 --> 00:00:49,120
this form of security is convenient

19
00:00:49,120 --> 00:00:50,559
for example the image on the left

20
00:00:50,559 --> 00:00:52,399
demonstrates a hands-free ticket

21
00:00:52,399 --> 00:00:53,600
reservation

22
00:00:53,600 --> 00:00:55,920
where the face serves as a sort of

23
00:00:55,920 --> 00:00:59,039
individualized proof of purchase

24
00:00:59,039 --> 00:01:00,879
from a business perspective face

25
00:01:00,879 --> 00:01:02,800
authentication seems like a

26
00:01:02,800 --> 00:01:04,400
convenient and sophisticated way to

27
00:01:04,400 --> 00:01:06,720
offer security to consumers

28
00:01:06,720 --> 00:01:09,360
indeed companies like mastercard whose

29
00:01:09,360 --> 00:01:11,040
face authentication software is shown in

30
00:01:11,040 --> 00:01:12,799
the figure on the right along with

31
00:01:12,799 --> 00:01:15,840
amazon intel with mcafee and microsoft

32
00:01:15,840 --> 00:01:17,520
have all worked on developing face like

33
00:01:17,520 --> 00:01:20,560
authentication software

34
00:01:20,560 --> 00:01:22,080
it's clear though that the protocol for

35
00:01:22,080 --> 00:01:23,920
face authentication must be robust to

36
00:01:23,920 --> 00:01:25,119
attacks

37
00:01:25,119 --> 00:01:26,560
there's been a cat and mouse game in the

38
00:01:26,560 --> 00:01:28,159
field over the last several several

39
00:01:28,159 --> 00:01:29,520
years

40
00:01:29,520 --> 00:01:31,200
at the black hat security conference in

41
00:01:31,200 --> 00:01:33,600
2009 for example it was demonstrated

42
00:01:33,600 --> 00:01:35,119
that face authentication software and

43
00:01:35,119 --> 00:01:37,520
commercial laptops could be bypassed

44
00:01:37,520 --> 00:01:39,759
simply by showing the system a still

45
00:01:39,759 --> 00:01:41,680
image of the user

46
00:01:41,680 --> 00:01:43,119
in the early days

47
00:01:43,119 --> 00:01:44,880
authentication largely relied on face

48
00:01:44,880 --> 00:01:45,680
shape

49
00:01:45,680 --> 00:01:47,280
ignored the question whether the system

50
00:01:47,280 --> 00:01:50,880
was observing a real human face

51
00:01:51,360 --> 00:01:53,439
to avoid still image scooping liveness

52
00:01:53,439 --> 00:01:55,600
detection methods were introduced

53
00:01:55,600 --> 00:01:57,759
this form of authentication requires

54
00:01:57,759 --> 00:02:00,399
users to for example blink their eyes or

55
00:02:00,399 --> 00:02:02,960
rotate their head

56
00:02:02,960 --> 00:02:04,880
of course simple liveness detection can

57
00:02:04,880 --> 00:02:07,040
be easily exploited if a video of the

58
00:02:07,040 --> 00:02:09,598
user performing the task is available

59
00:02:09,598 --> 00:02:12,160
blink detection is even easier to bypass

60
00:02:12,160 --> 00:02:13,760
as i've done here it's possible to

61
00:02:13,760 --> 00:02:14,879
photoshop

62
00:02:14,879 --> 00:02:16,400
a picture of the victim such that their

63
00:02:16,400 --> 00:02:18,800
eyes are closed the system then can then

64
00:02:18,800 --> 00:02:20,480
be hacked by simply flipping between the

65
00:02:20,480 --> 00:02:23,840
open and closed eye images

66
00:02:24,000 --> 00:02:26,000
a powerful way to avoid 2d image based

67
00:02:26,000 --> 00:02:28,480
attacks is to leverage camera motion

68
00:02:28,480 --> 00:02:29,920
when a real face is presented to the

69
00:02:29,920 --> 00:02:32,319
camera and the camera moves the shape of

70
00:02:32,319 --> 00:02:33,519
the face observed by the camera will

71
00:02:33,519 --> 00:02:35,760
change in a predictable way because the

72
00:02:35,760 --> 00:02:37,760
real face has 3d structure

73
00:02:37,760 --> 00:02:40,800
in technical terms this method method

74
00:02:40,800 --> 00:02:43,519
leverages camera parallax

75
00:02:43,519 --> 00:02:44,879
when a flat image of the user is

76
00:02:44,879 --> 00:02:46,959
presented to the system either printed

77
00:02:46,959 --> 00:02:49,040
out or displayed on a screen

78
00:02:49,040 --> 00:02:51,599
there's no underlying 3d shape so it's

79
00:02:51,599 --> 00:02:53,599
relatively easy to identify the image as

80
00:02:53,599 --> 00:02:55,920
a fake

81
00:02:56,000 --> 00:02:57,599
a proposed attack against motion

82
00:02:57,599 --> 00:03:00,319
consistency is use a 3d printed mask of

83
00:03:00,319 --> 00:03:01,599
the victim

84
00:03:01,599 --> 00:03:03,920
since these masks have 3d shape they are

85
00:03:03,920 --> 00:03:05,599
indistinguishable from a real face as

86
00:03:05,599 --> 00:03:08,239
far as motion consistency is concerned

87
00:03:08,239 --> 00:03:10,239
these masks are very lifelike are

88
00:03:10,239 --> 00:03:12,080
relatively inexpensive can they can be

89
00:03:12,080 --> 00:03:14,400
bought for about 300 online and can be

90
00:03:14,400 --> 00:03:16,080
made from a single good front view photo

91
00:03:16,080 --> 00:03:17,280
of the victim

92
00:03:17,280 --> 00:03:19,200
a key limitation however is that these

93
00:03:19,200 --> 00:03:21,920
masks cannot completely defeat liveness

94
00:03:21,920 --> 00:03:23,440
detection if the detection involves

95
00:03:23,440 --> 00:03:24,959
actions like raising the eyebrows or

96
00:03:24,959 --> 00:03:26,239
smiling

97
00:03:26,239 --> 00:03:28,080
we propose an alternative attack that

98
00:03:28,080 --> 00:03:29,599
can defeat these liveness detection

99
00:03:29,599 --> 00:03:31,280
methods while at the same time being

100
00:03:31,280 --> 00:03:33,200
easier to implement and less restrictive

101
00:03:33,200 --> 00:03:36,080
than 3d printed masks

102
00:03:36,080 --> 00:03:38,720
to this end we introduce virtual you a

103
00:03:38,720 --> 00:03:40,799
new reality virtual reality based attack

104
00:03:40,799 --> 00:03:43,120
on face authentication systems solely

105
00:03:43,120 --> 00:03:45,040
using publicly available photos of the

106
00:03:45,040 --> 00:03:47,360
victim we collect images of the victim

107
00:03:47,360 --> 00:03:48,560
from online

108
00:03:48,560 --> 00:03:50,480
then use those images to create a 3d

109
00:03:50,480 --> 00:03:52,799
reconstruction of the person's face

110
00:03:52,799 --> 00:03:54,159
this reconstruction is rendered on a

111
00:03:54,159 --> 00:03:56,480
virtual display such as a phone which is

112
00:03:56,480 --> 00:03:58,239
then presented to an authenticating

113
00:03:58,239 --> 00:03:59,599
camera

114
00:03:59,599 --> 00:04:01,439
since the vr system renders the face as

115
00:04:01,439 --> 00:04:03,439
it were a 3d object

116
00:04:03,439 --> 00:04:05,200
the face authentication system is

117
00:04:05,200 --> 00:04:06,640
tricked into accepting the virtual

118
00:04:06,640 --> 00:04:09,760
display as a real human face

119
00:04:09,760 --> 00:04:11,760
here's an overview of our pipeline we

120
00:04:11,760 --> 00:04:13,360
first collect publicly available photos

121
00:04:13,360 --> 00:04:15,200
of the victim from online

122
00:04:15,200 --> 00:04:17,358
then we detect facial landmarks in each

123
00:04:17,358 --> 00:04:18,560
image

124
00:04:18,560 --> 00:04:20,560
next we leverage data from all input

125
00:04:20,560 --> 00:04:22,400
images to obtain a 3d reconstruction of

126
00:04:22,400 --> 00:04:24,960
the victim victim's face

127
00:04:24,960 --> 00:04:27,199
after that we texture the model using

128
00:04:27,199 --> 00:04:29,360
one of the input photos and we correct

129
00:04:29,360 --> 00:04:31,040
for the gaze of this textured image so

130
00:04:31,040 --> 00:04:32,639
that it looks forward

131
00:04:32,639 --> 00:04:35,520
at this point we have a neutral textured

132
00:04:35,520 --> 00:04:37,840
model of the victim's face which we can

133
00:04:37,840 --> 00:04:39,280
then animate

134
00:04:39,280 --> 00:04:41,040
and visualize in a virtual reality

135
00:04:41,040 --> 00:04:43,680
system

136
00:04:43,680 --> 00:04:46,000
in general controlled high-resolution

137
00:04:46,000 --> 00:04:47,919
frontal views like this one highlighted

138
00:04:47,919 --> 00:04:49,280
here

139
00:04:49,280 --> 00:04:50,800
are able to easily defeat base

140
00:04:50,800 --> 00:04:53,280
authentication systems using our method

141
00:04:53,280 --> 00:04:55,360
however an important aspect of our work

142
00:04:55,360 --> 00:04:57,040
is that online photos can be leveraged

143
00:04:57,040 --> 00:04:58,479
when high quality photos like this

144
00:04:58,479 --> 00:05:00,320
aren't available

145
00:05:00,320 --> 00:05:01,840
we're able to leverage everything from

146
00:05:01,840 --> 00:05:03,919
group photos where the victim is forward

147
00:05:03,919 --> 00:05:06,400
facing but the face is often only tens

148
00:05:06,400 --> 00:05:08,639
of pixels in size in the image

149
00:05:08,639 --> 00:05:10,400
we're also able to leverage casual

150
00:05:10,400 --> 00:05:12,479
photos where the face is larger but not

151
00:05:12,479 --> 00:05:14,720
necessarily forward-facing or possibly

152
00:05:14,720 --> 00:05:16,639
there's strange or bad lighting in the

153
00:05:16,639 --> 00:05:17,680
scene

154
00:05:17,680 --> 00:05:19,759
and also high resolution forward fade

155
00:05:19,759 --> 00:05:22,479
facing photos if available

156
00:05:22,479 --> 00:05:24,080
the more publicly available photos

157
00:05:24,080 --> 00:05:25,919
someone has the more likely it is that

158
00:05:25,919 --> 00:05:28,080
our attack will be successful

159
00:05:28,080 --> 00:05:29,520
also note that for our experiments here

160
00:05:29,520 --> 00:05:31,120
we extracted the faces manually from the

161
00:05:31,120 --> 00:05:33,280
photos which is a realistic action of an

162
00:05:33,280 --> 00:05:36,639
adversary in this type of attack

163
00:05:36,639 --> 00:05:38,479
the first step after photo photo

164
00:05:38,479 --> 00:05:40,479
selection is to identify landmark points

165
00:05:40,479 --> 00:05:42,240
in the face like points on the eyes or

166
00:05:42,240 --> 00:05:43,120
mouth

167
00:05:43,120 --> 00:05:45,039
we use an automatic tool to perform this

168
00:05:45,039 --> 00:05:47,360
extraction and the process overall is

169
00:05:47,360 --> 00:05:49,360
robust to head rotation and size in the

170
00:05:49,360 --> 00:05:52,080
image

171
00:05:52,080 --> 00:05:54,160
after extraction we next perform a 3d

172
00:05:54,160 --> 00:05:56,240
reconstruction of the victim's face

173
00:05:56,240 --> 00:05:58,160
accurately recovering this face shape is

174
00:05:58,160 --> 00:05:59,840
vital to spoofing face authentication

175
00:05:59,840 --> 00:06:01,039
systems

176
00:06:01,039 --> 00:06:02,639
we break the face modeling into two

177
00:06:02,639 --> 00:06:05,440
components identity and expression

178
00:06:05,440 --> 00:06:07,039
identity is the underlying shape of the

179
00:06:07,039 --> 00:06:07,919
face

180
00:06:07,919 --> 00:06:10,240
for instance the face can go from narrow

181
00:06:10,240 --> 00:06:12,639
to wide for different individuals or

182
00:06:12,639 --> 00:06:14,160
maybe the eyes are closer set or farther

183
00:06:14,160 --> 00:06:15,440
apart

184
00:06:15,440 --> 00:06:17,360
expression is an image specific

185
00:06:17,360 --> 00:06:19,280
characteristic for instance a person

186
00:06:19,280 --> 00:06:21,280
could be frowning or smiling

187
00:06:21,280 --> 00:06:23,280
we can combine these two dimensions to

188
00:06:23,280 --> 00:06:27,440
create a wide array of possible faces

189
00:06:27,680 --> 00:06:29,120
first let's talk about reconstructing a

190
00:06:29,120 --> 00:06:31,120
face from a single image we'll talk

191
00:06:31,120 --> 00:06:33,280
about using multiple images next

192
00:06:33,280 --> 00:06:35,199
we can model the shape of someone's face

193
00:06:35,199 --> 00:06:38,160
s in the equation here as a base face

194
00:06:38,160 --> 00:06:39,520
shown in the middle

195
00:06:39,520 --> 00:06:41,199
offset by identity and expression

196
00:06:41,199 --> 00:06:42,400
components

197
00:06:42,400 --> 00:06:44,240
we have two pre-defined bases for

198
00:06:44,240 --> 00:06:46,240
identity and expression which encode

199
00:06:46,240 --> 00:06:48,800
variations away from the base face

200
00:06:48,800 --> 00:06:51,919
so the goal is given an input image to

201
00:06:51,919 --> 00:06:54,000
find person-specific identity

202
00:06:54,000 --> 00:06:56,479
coefficients and photo specific

203
00:06:56,479 --> 00:06:58,080
expression coefficients that best

204
00:06:58,080 --> 00:07:02,400
explain the detected 2d landmarks

205
00:07:02,400 --> 00:07:04,319
the key idea from for base modeling is

206
00:07:04,319 --> 00:07:06,000
reprojection

207
00:07:06,000 --> 00:07:08,160
given face shape and pose we can project

208
00:07:08,160 --> 00:07:10,319
3d landmarks in the image into the image

209
00:07:10,319 --> 00:07:11,840
and compare with 2d landmarks that are

210
00:07:11,840 --> 00:07:13,199
detected

211
00:07:13,199 --> 00:07:15,440
we might we minimize error in pose

212
00:07:15,440 --> 00:07:17,039
meaning how the head is oriented

213
00:07:17,039 --> 00:07:19,280
relative to the camera and face shape

214
00:07:19,280 --> 00:07:20,720
that is the identity expression

215
00:07:20,720 --> 00:07:22,160
coefficients

216
00:07:22,160 --> 00:07:23,759
we sum reprojection error for all

217
00:07:23,759 --> 00:07:26,240
landmarks based on the 2d distance

218
00:07:26,240 --> 00:07:27,680
between the projected and detected

219
00:07:27,680 --> 00:07:28,880
landmarks

220
00:07:28,880 --> 00:07:30,560
we also normalize the coefficients of

221
00:07:30,560 --> 00:07:32,240
identity and expression which helps

222
00:07:32,240 --> 00:07:35,120
constrain the solution

223
00:07:35,120 --> 00:07:36,880
here's a visualization of the process we

224
00:07:36,880 --> 00:07:38,479
start out with an initial estimate of

225
00:07:38,479 --> 00:07:40,479
pose identity and expression as shown on

226
00:07:40,479 --> 00:07:42,639
the left we can reproject the individual

227
00:07:42,639 --> 00:07:45,360
3d landmarks shown in red onto the image

228
00:07:45,360 --> 00:07:46,800
and compare their position to the

229
00:07:46,800 --> 00:07:48,639
position of the detected 2d landmarks

230
00:07:48,639 --> 00:07:50,400
shown in green

231
00:07:50,400 --> 00:07:53,120
we optimize using non-linear regression

232
00:07:53,120 --> 00:07:54,720
and after a few iterations come up with

233
00:07:54,720 --> 00:07:56,639
pose identity expression parameters that

234
00:07:56,639 --> 00:07:57,919
minimize the difference between the

235
00:07:57,919 --> 00:08:00,000
detected 2d points and the reprojected

236
00:08:00,000 --> 00:08:02,400
3d points

237
00:08:02,400 --> 00:08:03,919
here's the reconstruction result using a

238
00:08:03,919 --> 00:08:05,599
single input image

239
00:08:05,599 --> 00:08:07,599
if we overlay the front-facing image

240
00:08:07,599 --> 00:08:09,759
based on the estimated projection

241
00:08:09,759 --> 00:08:11,440
we observe that some parts of the face

242
00:08:11,440 --> 00:08:13,120
such as the nose and the mouth line up

243
00:08:13,120 --> 00:08:14,080
well

244
00:08:14,080 --> 00:08:15,120
however

245
00:08:15,120 --> 00:08:16,960
other areas

246
00:08:16,960 --> 00:08:18,560
are not very accurate especially in

247
00:08:18,560 --> 00:08:20,639
crucial areas for face authentication

248
00:08:20,639 --> 00:08:22,639
such as the eyes

249
00:08:22,639 --> 00:08:23,759
you can also observe that the head

250
00:08:23,759 --> 00:08:25,280
rotation the image has bias the

251
00:08:25,280 --> 00:08:27,199
reconstruction result

252
00:08:27,199 --> 00:08:29,520
to obtain better face reconstructions we

253
00:08:29,520 --> 00:08:33,200
propose to use multiple input images

254
00:08:33,200 --> 00:08:36,240
when modeling based on multiple images

255
00:08:36,240 --> 00:08:37,679
the idea is to recover a separate

256
00:08:37,679 --> 00:08:40,000
reconstruction for each individual image

257
00:08:40,000 --> 00:08:42,159
each reconstruction has a unique pose

258
00:08:42,159 --> 00:08:43,599
and expression

259
00:08:43,599 --> 00:08:45,279
importantly however using multiple

260
00:08:45,279 --> 00:08:47,680
images allows us to robustly recover the

261
00:08:47,680 --> 00:08:49,519
same underlying identity for the

262
00:08:49,519 --> 00:08:52,000
individual

263
00:08:52,080 --> 00:08:53,680
here's a result using a single image

264
00:08:53,680 --> 00:08:56,000
compared to multiple images

265
00:08:56,000 --> 00:08:57,920
as you can see the projected 3d face

266
00:08:57,920 --> 00:08:59,519
lines up much better the front-facing

267
00:08:59,519 --> 00:09:00,399
image

268
00:09:00,399 --> 00:09:02,560
and key areas like the eyes are captured

269
00:09:02,560 --> 00:09:05,360
very accurately

270
00:09:05,360 --> 00:09:07,440
after 3d reconstruction we textured the

271
00:09:07,440 --> 00:09:09,680
face using a single input image due to

272
00:09:09,680 --> 00:09:12,000
head rotation there will inevitably be

273
00:09:12,000 --> 00:09:13,600
areas in the face that will lack color

274
00:09:13,600 --> 00:09:15,360
based on direct texturing

275
00:09:15,360 --> 00:09:17,360
we can fill in these colors say using a

276
00:09:17,360 --> 00:09:19,200
predefined base texture or mirroring the

277
00:09:19,200 --> 00:09:22,320
face but this often leads to undesired

278
00:09:22,320 --> 00:09:23,680
artifacts

279
00:09:23,680 --> 00:09:25,440
a common approach is to impute the color

280
00:09:25,440 --> 00:09:27,839
using 2d poisson editing which often

281
00:09:27,839 --> 00:09:29,279
gives them gives a more convincing

282
00:09:29,279 --> 00:09:30,880
result

283
00:09:30,880 --> 00:09:32,720
we find that descending to 3d personal

284
00:09:32,720 --> 00:09:34,399
editing leads to better texture

285
00:09:34,399 --> 00:09:36,080
imputation

286
00:09:36,080 --> 00:09:37,440
more information about this step can be

287
00:09:37,440 --> 00:09:40,160
found in our paper

288
00:09:40,640 --> 00:09:42,640
typically face authentication systems

289
00:09:42,640 --> 00:09:44,959
expect the user to be looking forward

290
00:09:44,959 --> 00:09:46,399
however this may not be the case for our

291
00:09:46,399 --> 00:09:48,160
texture reconstruction

292
00:09:48,160 --> 00:09:50,240
we correct for eye gaze by analyzing the

293
00:09:50,240 --> 00:09:51,839
color distribution of the eye region in

294
00:09:51,839 --> 00:09:53,760
the reconstruction then aligning this

295
00:09:53,760 --> 00:09:55,200
color space to the color space of a

296
00:09:55,200 --> 00:09:57,680
predefined forward looking eye

297
00:09:57,680 --> 00:10:00,160
the forward look gazing eye is recolored

298
00:10:00,160 --> 00:10:01,839
using the color space mapping and the

299
00:10:01,839 --> 00:10:03,519
eye region is then updated on our

300
00:10:03,519 --> 00:10:05,120
reconstruction

301
00:10:05,120 --> 00:10:06,720
the end result shown here is a

302
00:10:06,720 --> 00:10:08,480
front-facing textured facial

303
00:10:08,480 --> 00:10:11,600
reconstruction of the victim

304
00:10:11,920 --> 00:10:14,000
so to summarize up to this point we

305
00:10:14,000 --> 00:10:16,160
begin with photos of the victim extract

306
00:10:16,160 --> 00:10:18,160
landmarks in each reconstruct the

307
00:10:18,160 --> 00:10:19,440
victim's face

308
00:10:19,440 --> 00:10:21,600
texture the face and correct for eye

309
00:10:21,600 --> 00:10:23,920
gaze the next steps are to animate the

310
00:10:23,920 --> 00:10:25,760
reconstruction

311
00:10:25,760 --> 00:10:29,600
and to visualize it in a virtual display

312
00:10:30,000 --> 00:10:32,000
the identity coefficients estimated over

313
00:10:32,000 --> 00:10:34,240
all input images give us a face with

314
00:10:34,240 --> 00:10:36,480
neutral underlying expression

315
00:10:36,480 --> 00:10:39,120
we can then modify this neutral face

316
00:10:39,120 --> 00:10:40,880
with alternative predefined expression

317
00:10:40,880 --> 00:10:42,959
coefficients

318
00:10:42,959 --> 00:10:44,720
this is a powerful aspect of our

319
00:10:44,720 --> 00:10:46,720
approach we can effectively animate the

320
00:10:46,720 --> 00:10:48,079
reconstruction with arbitrary

321
00:10:48,079 --> 00:10:50,320
expressions allowing us to perform tasks

322
00:10:50,320 --> 00:10:53,760
expected of liveness detection methods

323
00:10:53,760 --> 00:10:54,959
we can display the animated

324
00:10:54,959 --> 00:10:56,720
reconstruction on a device like a

325
00:10:56,720 --> 00:10:57,839
smartphone

326
00:10:57,839 --> 00:11:00,399
we add an external printed marker say

327
00:11:00,399 --> 00:11:02,399
paste it on the wall so that the vr

328
00:11:02,399 --> 00:11:04,399
system can track its position in the

329
00:11:04,399 --> 00:11:06,480
real world the reason for the marker is

330
00:11:06,480 --> 00:11:08,240
that the inertial data on the smartphone

331
00:11:08,240 --> 00:11:09,600
will provide us with information about

332
00:11:09,600 --> 00:11:11,440
the rotation of the device but we need

333
00:11:11,440 --> 00:11:13,040
an external reference to recover the

334
00:11:13,040 --> 00:11:16,399
device's translation in the real world

335
00:11:16,399 --> 00:11:18,320
finally the vr environment is shown to

336
00:11:18,320 --> 00:11:21,440
the authentication device

337
00:11:21,519 --> 00:11:23,200
here's an example of the reconstruction

338
00:11:23,200 --> 00:11:25,360
displayed in our vr system

339
00:11:25,360 --> 00:11:26,959
note how the face appears to have stable

340
00:11:26,959 --> 00:11:29,440
3d position as the as the phone is moved

341
00:11:29,440 --> 00:11:32,399
around the real world

342
00:11:34,959 --> 00:11:37,040
now i'd like to present some experiments

343
00:11:37,040 --> 00:11:38,880
be performed on five state-of-the-art

344
00:11:38,880 --> 00:11:40,160
commercially available face

345
00:11:40,160 --> 00:11:43,440
authentication systems kiliman mobius

346
00:11:43,440 --> 00:11:47,519
intel's truekey bioid and 1u

347
00:11:47,519 --> 00:11:49,200
of these systems

348
00:11:49,200 --> 00:11:50,800
two require some form of user

349
00:11:50,800 --> 00:11:51,839
interaction

350
00:11:51,839 --> 00:11:53,200
and the other three

351
00:11:53,200 --> 00:11:55,680
use motion-based liveness detection

352
00:11:55,680 --> 00:11:58,320
requiring the user to rotate their head

353
00:11:58,320 --> 00:12:00,160
we're also fairly certain that four of

354
00:12:00,160 --> 00:12:02,079
these systems use some form of

355
00:12:02,079 --> 00:12:04,000
additional texture based liveness

356
00:12:04,000 --> 00:12:05,440
detection

357
00:12:05,440 --> 00:12:07,440
i'll note that key lemon is a

358
00:12:07,440 --> 00:12:09,040
desktop-based system while the other

359
00:12:09,040 --> 00:12:11,360
systems are smartphone apps

360
00:12:11,360 --> 00:12:13,519
other systems we consider key element

361
00:12:13,519 --> 00:12:15,440
and truekey to be widely used systems

362
00:12:15,440 --> 00:12:16,639
based on the reported number of

363
00:12:16,639 --> 00:12:19,440
installations

364
00:12:19,680 --> 00:12:21,120
here's some information on the

365
00:12:21,120 --> 00:12:23,200
experimental data we collected data from

366
00:12:23,200 --> 00:12:26,480
14 males and six females aged 24 to 44

367
00:12:26,480 --> 00:12:28,720
and having various ethnicities

368
00:12:28,720 --> 00:12:30,560
we registered the face of each subject

369
00:12:30,560 --> 00:12:32,720
to all five previously mentioned face

370
00:12:32,720 --> 00:12:34,720
authentication systems

371
00:12:34,720 --> 00:12:36,880
we then conducted two experiments

372
00:12:36,880 --> 00:12:38,800
first we took a single front view photo

373
00:12:38,800 --> 00:12:40,480
of the subject in the same room that we

374
00:12:40,480 --> 00:12:42,880
used for system registration

375
00:12:42,880 --> 00:12:45,040
second we collected publicly available

376
00:12:45,040 --> 00:12:46,800
online photos of the individuals to

377
00:12:46,800 --> 00:12:49,279
perform our multiple image based attack

378
00:12:49,279 --> 00:12:52,079
we collected between 3 and 27 photos per

379
00:12:52,079 --> 00:12:54,160
person of various qualities

380
00:12:54,160 --> 00:12:55,839
we didn't limit ourselves to only recent

381
00:12:55,839 --> 00:12:57,519
photos of the participants so there's

382
00:12:57,519 --> 00:12:59,200
potentially strong change in appearance

383
00:12:59,200 --> 00:13:02,639
over time for some individuals

384
00:13:02,880 --> 00:13:04,800
here are results using the single high

385
00:13:04,800 --> 00:13:07,200
resolution front-facing indoor image as

386
00:13:07,200 --> 00:13:09,120
you can see using the single image for

387
00:13:09,120 --> 00:13:10,399
reconstruction we were able to

388
00:13:10,399 --> 00:13:12,800
successfully spoof all five commercially

389
00:13:12,800 --> 00:13:15,360
available systems 100 of the time for

390
00:13:15,360 --> 00:13:17,279
all 20 participants

391
00:13:17,279 --> 00:13:19,360
obviously this is a significant result

392
00:13:19,360 --> 00:13:21,279
and it demonstrates the capabilities of

393
00:13:21,279 --> 00:13:23,040
our method

394
00:13:23,040 --> 00:13:25,200
for online photos the success was not as

395
00:13:25,200 --> 00:13:26,959
great but still quite high

396
00:13:26,959 --> 00:13:29,440
keyboard key lemon was spoofed by 85

397
00:13:29,440 --> 00:13:31,519
percent of the reconstructions mobius by

398
00:13:31,519 --> 00:13:35,760
80 true key by 70 bioid by 55

399
00:13:35,760 --> 00:13:37,519
and we are unable to spoof the one use

400
00:13:37,519 --> 00:13:40,240
system using online images

401
00:13:40,240 --> 00:13:41,839
the average number of tries relates to

402
00:13:41,839 --> 00:13:43,680
our texturing approach which only uses a

403
00:13:43,680 --> 00:13:46,079
single image as adversaries we begin

404
00:13:46,079 --> 00:13:47,519
with the image we think will work best

405
00:13:47,519 --> 00:13:50,160
for texturing if that image fails we try

406
00:13:50,160 --> 00:13:51,920
the next image and so on until the test

407
00:13:51,920 --> 00:13:55,680
succeeds or all images have been tried

408
00:13:55,680 --> 00:13:57,199
i'll note that the online photo based

409
00:13:57,199 --> 00:13:59,120
attack worked on at least one system for

410
00:13:59,120 --> 00:14:01,040
all but two participants these

411
00:14:01,040 --> 00:14:02,399
participants both

412
00:14:02,399 --> 00:14:04,240
had only a very small number of low

413
00:14:04,240 --> 00:14:06,880
resolution photos available

414
00:14:06,880 --> 00:14:09,279
regarding 1u and bioid

415
00:14:09,279 --> 00:14:10,959
our opinion is that the low success with

416
00:14:10,959 --> 00:14:12,399
these systems sims from their

417
00:14:12,399 --> 00:14:14,160
sensitivity to changes in appearance and

418
00:14:14,160 --> 00:14:15,680
illumination

419
00:14:15,680 --> 00:14:17,519
one you for example only takes a single

420
00:14:17,519 --> 00:14:19,680
photo to learn the user's face

421
00:14:19,680 --> 00:14:22,079
if the user registers their face indoors

422
00:14:22,079 --> 00:14:23,519
and then tries to unlock the system

423
00:14:23,519 --> 00:14:25,600
outdoors these two systems in particular

424
00:14:25,600 --> 00:14:29,040
have a high false rejection rate

425
00:14:29,279 --> 00:14:30,639
here's some general observations about

426
00:14:30,639 --> 00:14:32,399
the results

427
00:14:32,399 --> 00:14:34,000
first we find that medium and high

428
00:14:34,000 --> 00:14:35,920
resolution images are most useful for

429
00:14:35,920 --> 00:14:37,199
our attack

430
00:14:37,199 --> 00:14:38,720
we found that professional photos from

431
00:14:38,720 --> 00:14:40,560
events like weddings are good source for

432
00:14:40,560 --> 00:14:42,800
such images

433
00:14:42,800 --> 00:14:43,760
second

434
00:14:43,760 --> 00:14:45,519
we found that group photos while lower

435
00:14:45,519 --> 00:14:47,600
resolution provide consistent frontal

436
00:14:47,600 --> 00:14:49,680
views of the victim which is helpful in

437
00:14:49,680 --> 00:14:52,160
accurately obtaining face shape

438
00:14:52,160 --> 00:14:53,920
i'll note that professional photos and

439
00:14:53,920 --> 00:14:55,760
group photos are very hard to control

440
00:14:55,760 --> 00:14:57,920
online as others can post them without

441
00:14:57,920 --> 00:14:59,600
the victim's knowledge

442
00:14:59,600 --> 00:15:00,880
to take away

443
00:15:00,880 --> 00:15:03,199
is that a few low resolution forward

444
00:15:03,199 --> 00:15:05,279
facing photos plus one or two high

445
00:15:05,279 --> 00:15:07,120
resolution photos for texturing are

446
00:15:07,120 --> 00:15:09,040
adequate for our attack

447
00:15:09,040 --> 00:15:10,480
i'll show some experiments supporting

448
00:15:10,480 --> 00:15:13,600
this claim in the following slides

449
00:15:13,600 --> 00:15:15,440
we first examined the effect of image

450
00:15:15,440 --> 00:15:17,760
resolution on reconstruction quality

451
00:15:17,760 --> 00:15:19,680
this is especially important for group

452
00:15:19,680 --> 00:15:22,240
photo images the question we ask is how

453
00:15:22,240 --> 00:15:25,040
small can they be to still be useful

454
00:15:25,040 --> 00:15:26,480
to this end we tried performing

455
00:15:26,480 --> 00:15:28,560
reconstruction using downsized versions

456
00:15:28,560 --> 00:15:31,279
of the indoor for forward-facing image

457
00:15:31,279 --> 00:15:33,759
with the chin deforma distance from 20

458
00:15:33,759 --> 00:15:35,600
to 50 pixels

459
00:15:35,600 --> 00:15:38,079
results are over all 20 participants for

460
00:15:38,079 --> 00:15:41,040
three of the systems as you can see very

461
00:15:41,040 --> 00:15:42,480
low resolution front-facing

462
00:15:42,480 --> 00:15:44,399
reconstructions fail to spoof the

463
00:15:44,399 --> 00:15:45,680
systems

464
00:15:45,680 --> 00:15:47,680
however the systems are increasingly

465
00:15:47,680 --> 00:15:50,079
easy to spoof as resolution increases

466
00:15:50,079 --> 00:15:51,839
with nearly 100 spoofing with a

467
00:15:51,839 --> 00:15:53,839
resolution of 50 pixels which is still

468
00:15:53,839 --> 00:15:56,800
fairly low resolution

469
00:15:56,800 --> 00:15:58,959
an additional question we'd like to ask

470
00:15:58,959 --> 00:16:01,199
is how does head rotation affect

471
00:16:01,199 --> 00:16:02,720
reconstruction

472
00:16:02,720 --> 00:16:05,040
for all 20 sample users we collected

473
00:16:05,040 --> 00:16:06,880
multiple indoor photos with yaw angle

474
00:16:06,880 --> 00:16:09,040
that is rotation to the left or right

475
00:16:09,040 --> 00:16:10,160
varying from

476
00:16:10,160 --> 00:16:12,480
5 degrees which is approximately front

477
00:16:12,480 --> 00:16:14,639
facing to 40 degrees which is a

478
00:16:14,639 --> 00:16:17,040
significantly rotated view

479
00:16:17,040 --> 00:16:18,800
we then perform 3d reconstruction for

480
00:16:18,800 --> 00:16:20,720
each image for each user on the same

481
00:16:20,720 --> 00:16:23,680
three face authentication systems

482
00:16:23,680 --> 00:16:25,360
as you can see when the eye angle is

483
00:16:25,360 --> 00:16:27,199
roughly forward facing the spoof rate is

484
00:16:27,199 --> 00:16:28,320
very high

485
00:16:28,320 --> 00:16:29,920
and it drops off fairly quickly with

486
00:16:29,920 --> 00:16:33,120
increasing head rotation

487
00:16:33,600 --> 00:16:36,240
so one thing we notice is in general

488
00:16:36,240 --> 00:16:38,160
individuals will have at least one

489
00:16:38,160 --> 00:16:40,480
medium to high resolution rotated photo

490
00:16:40,480 --> 00:16:42,560
online and potentially several

491
00:16:42,560 --> 00:16:45,600
low-resolution front-facing group photos

492
00:16:45,600 --> 00:16:47,680
a final question we've asked is can we

493
00:16:47,680 --> 00:16:50,000
improve our spoof rate by combining low

494
00:16:50,000 --> 00:16:52,160
resolution photos with a high res

495
00:16:52,160 --> 00:16:54,320
rotated photo for texturing

496
00:16:54,320 --> 00:16:56,240
to test this we took our indoor low

497
00:16:56,240 --> 00:16:57,680
resolution photos

498
00:16:57,680 --> 00:16:59,360
and the rotated photos from the previous

499
00:16:59,360 --> 00:17:01,440
experiment and combined them in our

500
00:17:01,440 --> 00:17:04,480
multi-image reconstruction approach

501
00:17:04,480 --> 00:17:06,319
as you can see from the graph

502
00:17:06,319 --> 00:17:07,839
right now you're showing the single

503
00:17:07,839 --> 00:17:10,319
rotated image and now here's showing the

504
00:17:10,319 --> 00:17:12,720
additional low resolution images

505
00:17:12,720 --> 00:17:15,039
spoof race increased quite a bit for

506
00:17:15,039 --> 00:17:17,039
images with 20 and 30 degree head

507
00:17:17,039 --> 00:17:19,839
rotation

508
00:17:21,359 --> 00:17:22,160
so

509
00:17:22,160 --> 00:17:23,679
since we have the ability to rotate the

510
00:17:23,679 --> 00:17:25,359
head and animate the face in our vr

511
00:17:25,359 --> 00:17:27,520
environment we argue that our virtual

512
00:17:27,520 --> 00:17:29,039
use system is successful against

513
00:17:29,039 --> 00:17:30,799
liveness detection defenses of

514
00:17:30,799 --> 00:17:32,160
commercially available base

515
00:17:32,160 --> 00:17:35,280
authentication systems

516
00:17:35,760 --> 00:17:38,000
because our vr display naturally rotates

517
00:17:38,000 --> 00:17:40,320
as the device moves we also argue that

518
00:17:40,320 --> 00:17:42,559
our system can defeat motion consistency

519
00:17:42,559 --> 00:17:45,280
based defenses

520
00:17:45,280 --> 00:17:47,120
to demonstrate this we implemented a

521
00:17:47,120 --> 00:17:49,440
motion detection defense by liadol

522
00:17:49,440 --> 00:17:52,000
titled seeing your face is not enough

523
00:17:52,000 --> 00:17:54,080
the basic idea of this system is that

524
00:17:54,080 --> 00:17:56,240
camera motion and observed head pose are

525
00:17:56,240 --> 00:17:58,480
predictable when a real 3d face is

526
00:17:58,480 --> 00:18:00,480
presented to the system

527
00:18:00,480 --> 00:18:02,240
we collect inertial sensor data from the

528
00:18:02,240 --> 00:18:04,320
authenticating device in this case a

529
00:18:04,320 --> 00:18:05,520
smartphone

530
00:18:05,520 --> 00:18:07,280
and we estimate head pose that is the

531
00:18:07,280 --> 00:18:09,200
direction the header is facing based on

532
00:18:09,200 --> 00:18:12,240
the input of the authenticating camera

533
00:18:12,240 --> 00:18:14,480
as the camera moves in front of the face

534
00:18:14,480 --> 00:18:16,400
we have the time series of rotational

535
00:18:16,400 --> 00:18:18,720
data for the device as well as a time

536
00:18:18,720 --> 00:18:21,280
series of estimated head pose

537
00:18:21,280 --> 00:18:23,360
after some normalization we train a

538
00:18:23,360 --> 00:18:25,280
classifier in these two time series

539
00:18:25,280 --> 00:18:26,880
which is able to distinguish between

540
00:18:26,880 --> 00:18:29,520
real data and spoof data

541
00:18:29,520 --> 00:18:31,440
lita lee at all originally tried their

542
00:18:31,440 --> 00:18:34,160
method to identify video spoof data

543
00:18:34,160 --> 00:18:36,240
we replicate their tests and we also try

544
00:18:36,240 --> 00:18:40,000
their method with our vr based attack

545
00:18:40,000 --> 00:18:41,679
our first experiment uses the same

546
00:18:41,679 --> 00:18:43,360
classification scheme as the original

547
00:18:43,360 --> 00:18:44,320
approach

548
00:18:44,320 --> 00:18:46,480
to train the classifier we take a set of

549
00:18:46,480 --> 00:18:48,720
real video inputs as positive inputs and

550
00:18:48,720 --> 00:18:51,600
a set of video spoofs as negative inputs

551
00:18:51,600 --> 00:18:53,600
on separate testing data the learn

552
00:18:53,600 --> 00:18:55,679
classifier correctly verifies almost all

553
00:18:55,679 --> 00:18:57,200
real faces while at the same time

554
00:18:57,200 --> 00:18:58,880
correctly rejecting almost all video

555
00:18:58,880 --> 00:18:59,919
spoofs

556
00:18:59,919 --> 00:19:01,200
however when this classifier is

557
00:19:01,200 --> 00:19:03,600
presented with our vr spoof nearly all

558
00:19:03,600 --> 00:19:05,280
inputs are incorrectly identified as

559
00:19:05,280 --> 00:19:07,840
real data

560
00:19:08,799 --> 00:19:10,799
of course it's unfair to test our attack

561
00:19:10,799 --> 00:19:12,320
on a classifier that hasn't been trained

562
00:19:12,320 --> 00:19:13,600
to recognize it

563
00:19:13,600 --> 00:19:15,919
so we decently trained a classifier

564
00:19:15,919 --> 00:19:17,679
using both video spoofs and our vr

565
00:19:17,679 --> 00:19:20,640
attack as negative training data

566
00:19:20,640 --> 00:19:22,960
when we tested this classifier it was

567
00:19:22,960 --> 00:19:25,039
able to correct correctly reject video

568
00:19:25,039 --> 00:19:26,480
based attacks

569
00:19:26,480 --> 00:19:28,080
however there was also a strong amount

570
00:19:28,080 --> 00:19:30,160
of confusion between real face inputs

571
00:19:30,160 --> 00:19:32,880
and our vr base spoof

572
00:19:32,880 --> 00:19:34,400
close to a third of real inputs were

573
00:19:34,400 --> 00:19:36,720
incorrectly rejected while at the same

574
00:19:36,720 --> 00:19:39,200
time half of our vr spoofs were accepted

575
00:19:39,200 --> 00:19:41,360
as real faces

576
00:19:41,360 --> 00:19:43,440
when we removed video data entirely the

577
00:19:43,440 --> 00:19:45,280
results were almost identical

578
00:19:45,280 --> 00:19:46,960
precision is only somewhat better than

579
00:19:46,960 --> 00:19:48,640
random chance

580
00:19:48,640 --> 00:19:50,240
the real face rejection rate is simply

581
00:19:50,240 --> 00:19:52,000
too high any system using this

582
00:19:52,000 --> 00:19:53,520
classifier would have to allow for

583
00:19:53,520 --> 00:19:55,919
multiple authentication attempts which

584
00:19:55,919 --> 00:19:57,520
in turn would increase the probability

585
00:19:57,520 --> 00:20:01,039
of a successful vr spoof

586
00:20:01,679 --> 00:20:02,400
so

587
00:20:02,400 --> 00:20:04,480
what are the possible

588
00:20:04,480 --> 00:20:06,240
possible mitigations to this type of

589
00:20:06,240 --> 00:20:07,520
attack

590
00:20:07,520 --> 00:20:09,280
we believe with sufficient effort on the

591
00:20:09,280 --> 00:20:11,200
part of the adversary our attack can

592
00:20:11,200 --> 00:20:12,720
likely defeat

593
00:20:12,720 --> 00:20:14,720
approaches that rely solely on color

594
00:20:14,720 --> 00:20:16,320
video for input

595
00:20:16,320 --> 00:20:17,840
there's simply too much potential for

596
00:20:17,840 --> 00:20:21,039
realism in vr in our opinion changes in

597
00:20:21,039 --> 00:20:22,960
hardware are the best events

598
00:20:22,960 --> 00:20:25,360
for example infrared imaging effectively

599
00:20:25,360 --> 00:20:27,360
provides a heat map of the scene rather

600
00:20:27,360 --> 00:20:28,960
than visual input

601
00:20:28,960 --> 00:20:31,120
this domain a real user looks entirely

602
00:20:31,120 --> 00:20:33,440
different from a flat display

603
00:20:33,440 --> 00:20:35,200
another option is to employ a random

604
00:20:35,200 --> 00:20:36,960
structured light projection

605
00:20:36,960 --> 00:20:38,720
this case a random light pattern is

606
00:20:38,720 --> 00:20:40,480
projected onto the face and then

607
00:20:40,480 --> 00:20:43,280
recognized by the authenticating system

608
00:20:43,280 --> 00:20:45,360
in this case the adversary would need to

609
00:20:45,360 --> 00:20:47,200
have advanced knowledge of the projected

610
00:20:47,200 --> 00:20:49,200
light pattern in order to simulate it on

611
00:20:49,200 --> 00:20:51,600
the reconstructive base

612
00:20:51,600 --> 00:20:53,039
of course the trade-off is that both

613
00:20:53,039 --> 00:20:54,880
these methods involve specialized

614
00:20:54,880 --> 00:20:56,640
hardware that would likely only be used

615
00:20:56,640 --> 00:20:59,760
for face authentication

616
00:21:00,320 --> 00:21:01,760
one immediate suggestion that we have is

617
00:21:01,760 --> 00:21:03,520
that texture detection and current face

618
00:21:03,520 --> 00:21:04,960
authentication software could be

619
00:21:04,960 --> 00:21:06,080
improved

620
00:21:06,080 --> 00:21:07,840
in our experiments we have found that

621
00:21:07,840 --> 00:21:09,600
even low resolution textures work

622
00:21:09,600 --> 00:21:11,039
surprisingly well in spoofing the

623
00:21:11,039 --> 00:21:12,240
systems

624
00:21:12,240 --> 00:21:13,679
for instance we are surprised to find

625
00:21:13,679 --> 00:21:15,360
that our 50 pixel reconstructions like

626
00:21:15,360 --> 00:21:17,120
the ones shown on the right here worked

627
00:21:17,120 --> 00:21:19,039
as well as they did

628
00:21:19,039 --> 00:21:21,280
there may be few restrictions a few

629
00:21:21,280 --> 00:21:24,080
restrictions to mitigating this problem

630
00:21:24,080 --> 00:21:25,520
for instance it may be difficult to

631
00:21:25,520 --> 00:21:27,840
differentiate these artifacts from noise

632
00:21:27,840 --> 00:21:30,799
and low quality authenticating cameras

633
00:21:30,799 --> 00:21:32,080
another issue is that face

634
00:21:32,080 --> 00:21:33,919
authentication systems typically store a

635
00:21:33,919 --> 00:21:35,760
minimum amount of user data for privacy

636
00:21:35,760 --> 00:21:36,799
reasons

637
00:21:36,799 --> 00:21:38,559
so texture detection probably won't be

638
00:21:38,559 --> 00:21:40,960
able to leverage data like prior photos

639
00:21:40,960 --> 00:21:42,640
of the user

640
00:21:42,640 --> 00:21:44,880
still detecting low resolution textures

641
00:21:44,880 --> 00:21:46,000
is something we believe face

642
00:21:46,000 --> 00:21:47,840
authentication developers can focus on

643
00:21:47,840 --> 00:21:50,559
in the near future

644
00:21:51,280 --> 00:21:54,480
so to conclude we we've introduced a new

645
00:21:54,480 --> 00:21:56,480
vr based attack on face authentication

646
00:21:56,480 --> 00:21:59,039
systems solely leveraging publicly

647
00:21:59,039 --> 00:22:01,919
available photos of the victim

648
00:22:01,919 --> 00:22:04,400
this attack bypasses existing defenses

649
00:22:04,400 --> 00:22:06,240
of liveness detection and motion

650
00:22:06,240 --> 00:22:08,400
consistency

651
00:22:08,400 --> 00:22:09,200
at a

652
00:22:09,200 --> 00:22:10,320
minimum

653
00:22:10,320 --> 00:22:12,000
we suggest that face authentication

654
00:22:12,000 --> 00:22:14,080
software must improve against vr based

655
00:22:14,080 --> 00:22:16,400
attacks with low resolution textures

656
00:22:16,400 --> 00:22:17,919
and that additional improvements can be

657
00:22:17,919 --> 00:22:21,760
made by introducing additional hardware

658
00:22:22,080 --> 00:22:23,760
we claim that the increasingly

659
00:22:23,760 --> 00:22:26,159
increasing ubiquity of vr will continue

660
00:22:26,159 --> 00:22:28,640
to challenge computer-based uh computer

661
00:22:28,640 --> 00:22:31,919
vision-based authentication systems

662
00:22:31,919 --> 00:22:33,440
we think that the ability of an

663
00:22:33,440 --> 00:22:35,039
adversary to recover an individual's

664
00:22:35,039 --> 00:22:36,640
facial characteristics through online

665
00:22:36,640 --> 00:22:38,640
photos is an immediate and very serious

666
00:22:38,640 --> 00:22:40,960
threat albeit one that cannot be

667
00:22:40,960 --> 00:22:42,480
completely neutralized in the age of

668
00:22:42,480 --> 00:22:44,559
social media

669
00:22:44,559 --> 00:22:46,080
and with that i'll take any questions

670
00:22:46,080 --> 00:22:49,240
thank you

671
00:22:50,770 --> 00:22:55,760
[Applause]

672
00:22:55,760 --> 00:22:58,159
so out of the um the five systems that

673
00:22:58,159 --> 00:23:00,720
you tested um against the online attacks

674
00:23:00,720 --> 00:23:02,720
there was one system that had where

675
00:23:02,720 --> 00:23:04,559
there was a considerably lower success

676
00:23:04,559 --> 00:23:06,400
rate than on the other ones

677
00:23:06,400 --> 00:23:08,480
yeah do you know any reasons why so what

678
00:23:08,480 --> 00:23:10,080
do they do differently than the others

679
00:23:10,080 --> 00:23:10,960
so

680
00:23:10,960 --> 00:23:14,159
for those the one you um system they uh

681
00:23:14,159 --> 00:23:17,120
they only take one photo of the user uh

682
00:23:17,120 --> 00:23:18,960
when they're training their system

683
00:23:18,960 --> 00:23:20,720
to recognize user's face

684
00:23:20,720 --> 00:23:22,880
so we found that like if there's strong

685
00:23:22,880 --> 00:23:24,960
light changes like

686
00:23:24,960 --> 00:23:29,360
we registered all the users indoors um

687
00:23:29,360 --> 00:23:30,400
and

688
00:23:30,400 --> 00:23:33,600
when when if you go outdoors to say uh

689
00:23:33,600 --> 00:23:35,120
test the system out it doesn't work as

690
00:23:35,120 --> 00:23:38,000
well so the idea is like a lot of online

691
00:23:38,000 --> 00:23:40,480
photos also people take pictures outside

692
00:23:40,480 --> 00:23:43,679
and post them online you know

693
00:23:43,679 --> 00:23:45,279
it's hard to get the exact same

694
00:23:45,279 --> 00:23:47,039
appearance

695
00:23:47,039 --> 00:23:48,720
based on one photo alone or have

696
00:23:48,720 --> 00:23:50,400
robustness against changes in appearance

697
00:23:50,400 --> 00:23:51,760
even if someone like grows a beard or

698
00:23:51,760 --> 00:23:53,440
something like that

699
00:23:53,440 --> 00:23:55,120
i think we found that

700
00:23:55,120 --> 00:23:56,880
it was is another

701
00:23:56,880 --> 00:23:57,919
thing that could

702
00:23:57,919 --> 00:24:02,000
hurt our success with that system so

703
00:24:02,080 --> 00:24:04,159
can you clear up speaking of beards can

704
00:24:04,159 --> 00:24:06,480
you clarify the uh the motion

705
00:24:06,480 --> 00:24:09,200
consistency attack so

706
00:24:09,200 --> 00:24:11,200
is it

707
00:24:11,200 --> 00:24:12,080
uh

708
00:24:12,080 --> 00:24:13,600
the camera

709
00:24:13,600 --> 00:24:16,080
authenticating you is moving and you're

710
00:24:16,080 --> 00:24:18,080
supposed to hold your head still

711
00:24:18,080 --> 00:24:20,880
basically but what you do is the camera

712
00:24:20,880 --> 00:24:23,039
is moving and you take your phone

713
00:24:23,039 --> 00:24:25,679
the fake vr image and you move it kind

714
00:24:25,679 --> 00:24:27,919
of in parallel to the camera yes i have

715
00:24:27,919 --> 00:24:30,320
a slide specifically for this so good

716
00:24:30,320 --> 00:24:31,420
showing

717
00:24:31,420 --> 00:24:35,919
[Laughter]

718
00:24:35,919 --> 00:24:38,240
so this is actually how we did our setup

719
00:24:38,240 --> 00:24:39,840
for that uh we took

720
00:24:39,840 --> 00:24:42,559
we took google cardboard which uh i mean

721
00:24:42,559 --> 00:24:44,400
you can see the two faces there if you

722
00:24:44,400 --> 00:24:46,159
wear google cardboard it's like a really

723
00:24:46,159 --> 00:24:48,960
cheap vr experience uh what we did is we

724
00:24:48,960 --> 00:24:51,360
uh took out one of the lenses and we put

725
00:24:51,360 --> 00:24:52,720
the authentication device where the

726
00:24:52,720 --> 00:24:54,720
camera would be

727
00:24:54,720 --> 00:24:56,960
in the viewing part of the

728
00:24:56,960 --> 00:24:59,440
cardboard and then we put our vr systems

729
00:24:59,440 --> 00:25:01,039
on the other side of it

730
00:25:01,039 --> 00:25:02,720
cut a hole so we could track the outside

731
00:25:02,720 --> 00:25:04,159
world and then

732
00:25:04,159 --> 00:25:06,720
uh this provided a chassis that we can

733
00:25:06,720 --> 00:25:08,480
move both cameras simultaneously with

734
00:25:08,480 --> 00:25:10,480
the consistent motion between them right

735
00:25:10,480 --> 00:25:12,640
so do you have but you're showing two

736
00:25:12,640 --> 00:25:14,960
faces on the vr

737
00:25:14,960 --> 00:25:18,400
it's not meant to provide a 3d

738
00:25:18,400 --> 00:25:20,159
like i mean you could

739
00:25:20,159 --> 00:25:21,520
you only need one face we don't show

740
00:25:21,520 --> 00:25:24,559
both faces to the camera it's just uh

741
00:25:24,559 --> 00:25:25,520
it's

742
00:25:25,520 --> 00:25:27,360
based on the vr software it was easier

743
00:25:27,360 --> 00:25:30,000
to implement with two faces and just ah

744
00:25:30,000 --> 00:25:32,000
so the vr software was meant to be used

745
00:25:32,000 --> 00:25:34,159
with google for like stereo vision

746
00:25:34,159 --> 00:25:38,159
the actual 3d shape but yeah

747
00:25:38,559 --> 00:25:42,520
we have time for another question

748
00:25:51,840 --> 00:25:52,960
speaker again

749
00:25:52,960 --> 00:25:56,200
thank you

750
00:26:01,360 --> 00:26:03,439
you

