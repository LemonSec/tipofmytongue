1
00:00:08,240 --> 00:00:11,200
hi i'm nicholas and i'm happy to talk

2
00:00:11,200 --> 00:00:13,360
about some work on poisoning the

3
00:00:13,360 --> 00:00:15,679
unlabeled data set of semi-supervised

4
00:00:15,679 --> 00:00:16,880
learning

5
00:00:16,880 --> 00:00:18,480
so we all know that over the last couple

6
00:00:18,480 --> 00:00:20,240
of years machine learning has really

7
00:00:20,240 --> 00:00:22,320
taken off and has gotten to really

8
00:00:22,320 --> 00:00:24,560
impress incredible levels of accuracy

9
00:00:24,560 --> 00:00:26,640
and this is in no small part due to the

10
00:00:26,640 --> 00:00:29,760
existence of large labeled data sets

11
00:00:29,760 --> 00:00:31,840
that allow us to train ever increasingly

12
00:00:31,840 --> 00:00:33,680
large models

13
00:00:33,680 --> 00:00:36,079
and until recently the only way that you

14
00:00:36,079 --> 00:00:38,800
could train a model is if you had a big

15
00:00:38,800 --> 00:00:41,200
labeled data set

16
00:00:41,200 --> 00:00:44,079
other techniques really didn't work

17
00:00:44,079 --> 00:00:46,239
and the problem is that in many domains

18
00:00:46,239 --> 00:00:49,520
you just don't have big label data sets

19
00:00:49,520 --> 00:00:51,840
or in some cases you have big label data

20
00:00:51,840 --> 00:00:54,160
sets but they're not even big enough

21
00:00:54,160 --> 00:00:56,160
and in both of those settings

22
00:00:56,160 --> 00:00:57,360
there's actually a technique that's

23
00:00:57,360 --> 00:00:59,760
called semi-supervised learning which

24
00:00:59,760 --> 00:01:01,680
allows us to train without having to

25
00:01:01,680 --> 00:01:04,000
collect big labeled data sets

26
00:01:04,000 --> 00:01:06,159
and even today if you look at like what

27
00:01:06,159 --> 00:01:08,000
the best techniques are on even

28
00:01:08,000 --> 00:01:09,680
classical supervised problems like

29
00:01:09,680 --> 00:01:12,080
imagenet you find that all of the best

30
00:01:12,080 --> 00:01:13,200
techniques

31
00:01:13,200 --> 00:01:15,200
require extensive amount of extra

32
00:01:15,200 --> 00:01:16,640
training data

33
00:01:16,640 --> 00:01:18,560
and you have to like go all the way down

34
00:01:18,560 --> 00:01:20,960
to like position 25 in the best papers

35
00:01:20,960 --> 00:01:23,119
list to find the best imagenet model

36
00:01:23,119 --> 00:01:24,560
that doesn't require using extra

37
00:01:24,560 --> 00:01:26,720
training data

38
00:01:26,720 --> 00:01:28,400
and so what semi supervised learning

39
00:01:28,400 --> 00:01:29,280
does

40
00:01:29,280 --> 00:01:31,360
is it basically turns this problem of

41
00:01:31,360 --> 00:01:33,040
fully supervised training i'm going to

42
00:01:33,040 --> 00:01:35,040
use this running example of blue

43
00:01:35,040 --> 00:01:37,759
triangles versus red circles

44
00:01:37,759 --> 00:01:39,840
it turns this this fully supervised

45
00:01:39,840 --> 00:01:41,360
problem where you have someone sit down

46
00:01:41,360 --> 00:01:43,040
and label all the triangles as blue and

47
00:01:43,040 --> 00:01:45,040
label all the circles as red

48
00:01:45,040 --> 00:01:47,840
into a now a semi-supervised problem

49
00:01:47,840 --> 00:01:49,840
where we have maybe one example of a

50
00:01:49,840 --> 00:01:52,560
labeled red circle and one example of a

51
00:01:52,560 --> 00:01:54,720
labeled blue triangle

52
00:01:54,720 --> 00:01:57,200
and we ask the model to teach itself on

53
00:01:57,200 --> 00:01:59,439
this fact where even though we only have

54
00:01:59,439 --> 00:02:01,040
labeled these two examples we now have

55
00:02:01,040 --> 00:02:02,719
these other like shapes available for

56
00:02:02,719 --> 00:02:04,719
the model to learn from

57
00:02:04,719 --> 00:02:06,479
and the way it's going to work is the

58
00:02:06,479 --> 00:02:08,720
models sort of draw a decision boundary

59
00:02:08,720 --> 00:02:10,399
initially located just around the

60
00:02:10,399 --> 00:02:12,480
example that they know the label of and

61
00:02:12,480 --> 00:02:14,640
they slowly grow this decision boundary

62
00:02:14,640 --> 00:02:15,599
out

63
00:02:15,599 --> 00:02:17,040
and so it will initially label the

64
00:02:17,040 --> 00:02:19,040
points nearby the same way

65
00:02:19,040 --> 00:02:20,560
and repeat this growing out until

66
00:02:20,560 --> 00:02:22,319
eventually we've labeled the whole data

67
00:02:22,319 --> 00:02:24,080
set essentially correctly

68
00:02:24,080 --> 00:02:25,920
and now we just train fully supervised

69
00:02:25,920 --> 00:02:28,480
on this now labeled data set

70
00:02:28,480 --> 00:02:29,760
this gives us a very nice decision

71
00:02:29,760 --> 00:02:31,360
boundary and the model has learned to do

72
00:02:31,360 --> 00:02:33,280
the right thing even though we only gave

73
00:02:33,280 --> 00:02:35,360
it two labeled data points in this very

74
00:02:35,360 --> 00:02:38,000
toy example

75
00:02:38,000 --> 00:02:39,599
and so the question that we're going to

76
00:02:39,599 --> 00:02:40,879
study and the argument we're going to

77
00:02:40,879 --> 00:02:42,080
make

78
00:02:42,080 --> 00:02:43,280
is what happens when there's an

79
00:02:43,280 --> 00:02:45,360
adversary who can poison the unlabeled

80
00:02:45,360 --> 00:02:46,400
data

81
00:02:46,400 --> 00:02:48,400
and what we're going to say is that

82
00:02:48,400 --> 00:02:50,480
poisoning the unlabeled data set is

83
00:02:50,480 --> 00:02:52,239
actually a real threat that we should be

84
00:02:52,239 --> 00:02:53,840
worried about

85
00:02:53,840 --> 00:02:55,440
and in order to convince you of this i'm

86
00:02:55,440 --> 00:02:57,120
going to make the argument in this way

87
00:02:57,120 --> 00:02:58,800
i'm going to say first that

88
00:02:58,800 --> 00:03:00,959
semi-supervised learning matters

89
00:03:00,959 --> 00:03:02,080
and i hope at this point to already

90
00:03:02,080 --> 00:03:04,000
convince you of this fact

91
00:03:04,000 --> 00:03:05,440
there are lots of domains where we just

92
00:03:05,440 --> 00:03:07,440
don't have large labeled data sets and

93
00:03:07,440 --> 00:03:09,440
even when we do we still use semi

94
00:03:09,440 --> 00:03:11,519
supervised learning even today

95
00:03:11,519 --> 00:03:13,440
because we can get even bigger data sets

96
00:03:13,440 --> 00:03:16,640
by making use of extra unlabeled data

97
00:03:16,640 --> 00:03:17,840
then i'm going to have to convince you

98
00:03:17,840 --> 00:03:20,159
of the fact that unlabeled data sets can

99
00:03:20,159 --> 00:03:21,440
be poisoned

100
00:03:21,440 --> 00:03:22,640
why is it that an adversary should

101
00:03:22,640 --> 00:03:24,720
actually be allowed to poison these and

102
00:03:24,720 --> 00:03:26,159
finally have to convince you our attack

103
00:03:26,159 --> 00:03:27,760
actually works

104
00:03:27,760 --> 00:03:29,519
so let me move on to the next point then

105
00:03:29,519 --> 00:03:31,200
and talk and now convince you why i

106
00:03:31,200 --> 00:03:33,200
think it is that unlabeled data should

107
00:03:33,200 --> 00:03:34,799
be considered some a real threat that

108
00:03:34,799 --> 00:03:36,560
can be poisoned

109
00:03:36,560 --> 00:03:38,159
and basically the argument is that for a

110
00:03:38,159 --> 00:03:40,400
long time we've had you know imagenet in

111
00:03:40,400 --> 00:03:42,319
these labeled data sets that are very

112
00:03:42,319 --> 00:03:44,720
hard to poison this was constructed once

113
00:03:44,720 --> 00:03:46,319
and unless you got poisoning samples in

114
00:03:46,319 --> 00:03:47,680
there in the beginning you're not going

115
00:03:47,680 --> 00:03:49,200
to get any poison samples into any

116
00:03:49,200 --> 00:03:51,360
internet models today

117
00:03:51,360 --> 00:03:55,519
but for unlabeled data sets

118
00:03:55,519 --> 00:03:56,720
they are

119
00:03:56,720 --> 00:03:59,200
gathered by essentially scraping the

120
00:03:59,200 --> 00:04:00,480
entire internet

121
00:04:00,480 --> 00:04:02,879
uh without regard for quality um to just

122
00:04:02,879 --> 00:04:04,720
get as much data as possible and they

123
00:04:04,720 --> 00:04:06,319
constantly are the data sets are getting

124
00:04:06,319 --> 00:04:07,519
bigger and bigger as more and more

125
00:04:07,519 --> 00:04:10,080
images are added to these data sets um

126
00:04:10,080 --> 00:04:12,080
in order to be able to train them

127
00:04:12,080 --> 00:04:14,720
on with bigger models and when you're

128
00:04:14,720 --> 00:04:16,639
doing this when you're just scraping the

129
00:04:16,639 --> 00:04:17,680
internet

130
00:04:17,680 --> 00:04:20,079
for to get as much data as possible you

131
00:04:20,079 --> 00:04:22,400
really have to believe that there is

132
00:04:22,400 --> 00:04:23,840
going to be some fraction of the

133
00:04:23,840 --> 00:04:26,320
internet that's just malicious some

134
00:04:26,320 --> 00:04:28,240
people just want to watch the world burn

135
00:04:28,240 --> 00:04:30,080
and are going to upload images for the

136
00:04:30,080 --> 00:04:32,000
sole purpose of poisoning the machine

137
00:04:32,000 --> 00:04:34,080
learning models to do bad things

138
00:04:34,080 --> 00:04:35,840
and so as before you had to maybe make

139
00:04:35,840 --> 00:04:38,240
some kind of contrived argument

140
00:04:38,240 --> 00:04:39,919
for why you could fool the human

141
00:04:39,919 --> 00:04:42,240
operator into accepting your images into

142
00:04:42,240 --> 00:04:44,639
the into the label data sets here when

143
00:04:44,639 --> 00:04:45,759
you have unlabeled images that you're

144
00:04:45,759 --> 00:04:46,960
just crawling for the internet and

145
00:04:46,960 --> 00:04:49,360
including the data sets um it's very

146
00:04:49,360 --> 00:04:50,639
easy you just upload an image to the

147
00:04:50,639 --> 00:04:51,919
internet and it will be crawled and

148
00:04:51,919 --> 00:04:53,680
included in the data set

149
00:04:53,680 --> 00:04:55,199
and so that's really why i think that

150
00:04:55,199 --> 00:04:57,360
this poisoning the unlabeled data set

151
00:04:57,360 --> 00:05:00,000
really is like a very powerful threat

152
00:05:00,000 --> 00:05:02,240
model to start considering

153
00:05:02,240 --> 00:05:05,919
um and you know under this assumption um

154
00:05:05,919 --> 00:05:08,080
we're now going to introduce an attack

155
00:05:08,080 --> 00:05:09,360
that is

156
00:05:09,360 --> 00:05:10,800
really quite simple it's going to use

157
00:05:10,800 --> 00:05:12,240
some ideas from the literature from the

158
00:05:12,240 --> 00:05:14,320
last decade or so

159
00:05:14,320 --> 00:05:15,680
in order to

160
00:05:15,680 --> 00:05:17,759
do a very powerful um targeted poisoning

161
00:05:17,759 --> 00:05:19,600
attack

162
00:05:19,600 --> 00:05:21,120
so what we're going to do is as i said a

163
00:05:21,120 --> 00:05:22,720
targeted poisoning attack and what this

164
00:05:22,720 --> 00:05:24,479
means is we're going to have a

165
00:05:24,479 --> 00:05:26,960
particular point in space where if we

166
00:05:26,960 --> 00:05:28,880
have the correct decision boundary we

167
00:05:28,880 --> 00:05:30,400
want the model to learn the slightly

168
00:05:30,400 --> 00:05:32,560
wrong decision boundary so it's mostly

169
00:05:32,560 --> 00:05:35,680
correct but this particular error here

170
00:05:35,680 --> 00:05:38,720
is classified incorrectly maybe for

171
00:05:38,720 --> 00:05:40,639
example you have a face recognition

172
00:05:40,639 --> 00:05:41,919
model that's going to either allow or

173
00:05:41,919 --> 00:05:43,919
deny people into a building and i want

174
00:05:43,919 --> 00:05:46,240
to make sure that my name is incorrectly

175
00:05:46,240 --> 00:05:47,759
placed on the allowed list of people

176
00:05:47,759 --> 00:05:49,759
allowed in the building

177
00:05:49,759 --> 00:05:51,680
so how are we going to do this well

178
00:05:51,680 --> 00:05:53,120
you know let's imagine we are in the

179
00:05:53,120 --> 00:05:55,199
fully supervised world the attack here

180
00:05:55,199 --> 00:05:57,440
is is really easy right in the fully

181
00:05:57,440 --> 00:05:59,360
supervised setting all you have to do is

182
00:05:59,360 --> 00:06:02,479
insert a poisoned mislabeled blue circle

183
00:06:02,479 --> 00:06:04,960
train the model on this data set and it

184
00:06:04,960 --> 00:06:06,880
will do exactly what you expect and

185
00:06:06,880 --> 00:06:09,039
mislabel this data point um because

186
00:06:09,039 --> 00:06:11,199
you've put it so close

187
00:06:11,199 --> 00:06:13,919
okay so that works but we can't inject a

188
00:06:13,919 --> 00:06:15,199
mislabeled

189
00:06:15,199 --> 00:06:16,880
data point here that's not in the threat

190
00:06:16,880 --> 00:06:20,000
model we're assuming unlabeled data and

191
00:06:20,000 --> 00:06:21,680
so you could still imagine trying to put

192
00:06:21,680 --> 00:06:23,199
a circle here

193
00:06:23,199 --> 00:06:25,360
but the problem is that the machine

194
00:06:25,360 --> 00:06:27,120
learning algorithm the semi-supervised

195
00:06:27,120 --> 00:06:29,120
learning algorithm at the trains will

196
00:06:29,120 --> 00:06:31,680
just label it correctly as a red circle

197
00:06:31,680 --> 00:06:33,520
and it will learn maybe an even better

198
00:06:33,520 --> 00:06:34,960
decision boundary than it had before and

199
00:06:34,960 --> 00:06:36,160
so the attack will have completely

200
00:06:36,160 --> 00:06:37,840
failed

201
00:06:37,840 --> 00:06:39,680
and so what we're going to do in our

202
00:06:39,680 --> 00:06:41,680
attack is instead of inserting just one

203
00:06:41,680 --> 00:06:43,360
poisoned example

204
00:06:43,360 --> 00:06:44,880
we're going to insert a bunch we're

205
00:06:44,880 --> 00:06:47,120
going to insert a path of examples that

206
00:06:47,120 --> 00:06:51,039
connect the two classes together

207
00:06:51,039 --> 00:06:52,720
and now what happens is if you run the

208
00:06:52,720 --> 00:06:54,560
training algorithm forwards you'll

209
00:06:54,560 --> 00:06:56,240
initially label the nearest neighbors

210
00:06:56,240 --> 00:06:58,160
the same way propagate this forward a

211
00:06:58,160 --> 00:06:59,680
little more propagate forward a little

212
00:06:59,680 --> 00:07:01,759
more and you start to see the examples

213
00:07:01,759 --> 00:07:04,560
on this path now being labeled

214
00:07:04,560 --> 00:07:06,319
incorrectly like we've reached the

215
00:07:06,319 --> 00:07:07,840
decision boundary here and we've gone a

216
00:07:07,840 --> 00:07:09,039
little bit further than we should have

217
00:07:09,039 --> 00:07:10,720
along this blue way

218
00:07:10,720 --> 00:07:12,960
but something interesting happens here

219
00:07:12,960 --> 00:07:14,319
because of the way that semi-supervised

220
00:07:14,319 --> 00:07:15,840
learning works

221
00:07:15,840 --> 00:07:18,560
this point here this sort of arrangement

222
00:07:18,560 --> 00:07:20,720
of labels actually has very high loss

223
00:07:20,720 --> 00:07:22,000
for the model

224
00:07:22,000 --> 00:07:23,120
because of something that's called

225
00:07:23,120 --> 00:07:25,440
consistency regularization the model

226
00:07:25,440 --> 00:07:28,319
really wants to make sure that it labels

227
00:07:28,319 --> 00:07:30,319
everything within the same radius the

228
00:07:30,319 --> 00:07:31,840
same

229
00:07:31,840 --> 00:07:34,720
and so this is this is a very big saddle

230
00:07:34,720 --> 00:07:36,960
point in the lost surface and so what's

231
00:07:36,960 --> 00:07:38,720
going to happen is that either the red

232
00:07:38,720 --> 00:07:39,919
points are going to win and push

233
00:07:39,919 --> 00:07:42,240
everything along this path towards

234
00:07:42,240 --> 00:07:43,759
to be all labeled red or the blue points

235
00:07:43,759 --> 00:07:44,720
are going to win and everything will be

236
00:07:44,720 --> 00:07:46,000
labeled blue

237
00:07:46,000 --> 00:07:47,360
and because there are more blue points

238
00:07:47,360 --> 00:07:49,520
at this point the blue points win and

239
00:07:49,520 --> 00:07:51,440
they end up overpowering the red and

240
00:07:51,440 --> 00:07:52,800
everything along this path becomes

241
00:07:52,800 --> 00:07:54,720
labeled as blue

242
00:07:54,720 --> 00:07:56,400
now equally valid could have happened

243
00:07:56,400 --> 00:07:57,840
that everything here becomes labels as

244
00:07:57,840 --> 00:07:59,120
red

245
00:07:59,120 --> 00:08:00,960
but because we're the adversary you know

246
00:08:00,960 --> 00:08:03,120
we can just set things up in such a way

247
00:08:03,120 --> 00:08:04,879
and in the paper we go into details on

248
00:08:04,879 --> 00:08:06,800
how to make the spectrally happen uh so

249
00:08:06,800 --> 00:08:09,199
that it it always ends up that the blue

250
00:08:09,199 --> 00:08:10,400
ends up winning

251
00:08:10,400 --> 00:08:12,000
and you can end up now when you train on

252
00:08:12,000 --> 00:08:14,000
this of course uh it ends up training

253
00:08:14,000 --> 00:08:15,199
and giving you the exactly the same

254
00:08:15,199 --> 00:08:16,800
decision boundary we we could have done

255
00:08:16,800 --> 00:08:18,639
with the fully supervised poisoning

256
00:08:18,639 --> 00:08:19,919
where we have you know correctly

257
00:08:19,919 --> 00:08:21,919
perturbed the decision boundary but here

258
00:08:21,919 --> 00:08:24,240
we only ever had the ability to inject

259
00:08:24,240 --> 00:08:26,720
unlabeled data points so really we're

260
00:08:26,720 --> 00:08:28,960
like abusing the the way the

261
00:08:28,960 --> 00:08:30,639
semi-supervised learning even works in

262
00:08:30,639 --> 00:08:32,719
the first place in order to allow us as

263
00:08:32,719 --> 00:08:34,640
the adversary to do something else that

264
00:08:34,640 --> 00:08:37,519
was not intended in the first place

265
00:08:37,519 --> 00:08:39,440
okay

266
00:08:39,440 --> 00:08:40,719
there are a lot of results we have in

267
00:08:40,719 --> 00:08:42,399
the paper um i'm not going to go into

268
00:08:42,399 --> 00:08:43,919
those in details but i'd encourage you

269
00:08:43,919 --> 00:08:45,040
to take a look

270
00:08:45,040 --> 00:08:48,160
um the one experiment i will talk about

271
00:08:48,160 --> 00:08:50,560
is this one and what we do here

272
00:08:50,560 --> 00:08:52,640
is we train a bunch of models each point

273
00:08:52,640 --> 00:08:54,720
here corresponds to the average of 10

274
00:08:54,720 --> 00:08:56,880
different models uh that were trained

275
00:08:56,880 --> 00:08:58,080
according to a number of different

276
00:08:58,080 --> 00:08:59,680
training algorithms

277
00:08:59,680 --> 00:09:02,880
um on the x-axis is how well the model

278
00:09:02,880 --> 00:09:05,200
performs at the original underlying task

279
00:09:05,200 --> 00:09:06,959
like what's the accuracy

280
00:09:06,959 --> 00:09:08,880
on the c410 data set

281
00:09:08,880 --> 00:09:11,279
and this varies from everything from 50

282
00:09:11,279 --> 00:09:15,519
to almost 100 like 98 97

283
00:09:15,519 --> 00:09:17,279
and then on the y-axis

284
00:09:17,279 --> 00:09:20,399
uh we have the attack success rate this

285
00:09:20,399 --> 00:09:22,320
is how often our attack actually able

286
00:09:22,320 --> 00:09:24,720
succeeded at poisoning the model

287
00:09:24,720 --> 00:09:26,560
and what we find is there's a really

288
00:09:26,560 --> 00:09:28,959
clear relationship between models that

289
00:09:28,959 --> 00:09:31,279
do better along the x-axis more accurate

290
00:09:31,279 --> 00:09:33,360
models and models that are easier to

291
00:09:33,360 --> 00:09:36,080
poison along the y-axis

292
00:09:36,080 --> 00:09:38,240
and the intuition here that we have is

293
00:09:38,240 --> 00:09:41,040
that the models that are better can take

294
00:09:41,040 --> 00:09:42,880
more information and

295
00:09:42,880 --> 00:09:44,399
out of the training data and learn more

296
00:09:44,399 --> 00:09:45,600
from it

297
00:09:45,600 --> 00:09:48,480
and this exact ability to learn more

298
00:09:48,480 --> 00:09:49,600
from the training data is what the

299
00:09:49,600 --> 00:09:51,279
adversary is able to use to make the

300
00:09:51,279 --> 00:09:54,000
model learn to do the wrong thing

301
00:09:54,000 --> 00:09:55,519
and in particular this means that simply

302
00:09:55,519 --> 00:09:57,040
waiting longer is not going to make this

303
00:09:57,040 --> 00:09:58,959
problem go away in particular probably

304
00:09:58,959 --> 00:10:01,279
make it go worse because the future

305
00:10:01,279 --> 00:10:02,560
techniques are going to be even better

306
00:10:02,560 --> 00:10:04,320
than these older techniques and are

307
00:10:04,320 --> 00:10:06,560
going to make these attacks even easier

308
00:10:06,560 --> 00:10:08,480
also in the paper uh turns out you can

309
00:10:08,480 --> 00:10:09,920
completely prevent this particular

310
00:10:09,920 --> 00:10:11,680
attack and we have a technique to do

311
00:10:11,680 --> 00:10:13,279
that i think this will serve as an

312
00:10:13,279 --> 00:10:15,360
interesting baseline for future work but

313
00:10:15,360 --> 00:10:16,640
i'm not going to talk about it anymore

314
00:10:16,640 --> 00:10:17,680
in the talk

315
00:10:17,680 --> 00:10:19,680
so what i want to finish with are some

316
00:10:19,680 --> 00:10:21,440
lessons for what i think are the future

317
00:10:21,440 --> 00:10:23,519
of machine learning research what we

318
00:10:23,519 --> 00:10:25,680
used to do like before machine learning

319
00:10:25,680 --> 00:10:27,440
is we would write a program that

320
00:10:27,440 --> 00:10:30,079
specifies how we wanted this triangle

321
00:10:30,079 --> 00:10:32,320
versus circle classification to go and

322
00:10:32,320 --> 00:10:34,800
tell the computer exactly what to do

323
00:10:34,800 --> 00:10:36,000
and with machine learning we sort of

324
00:10:36,000 --> 00:10:37,839
moved on beyond that we're no longer

325
00:10:37,839 --> 00:10:39,680
telling the computer exactly how we want

326
00:10:39,680 --> 00:10:41,519
something to done instead we give it

327
00:10:41,519 --> 00:10:43,680
examples of the thing being done here

328
00:10:43,680 --> 00:10:45,519
are blue triangles here are red circles

329
00:10:45,519 --> 00:10:47,120
and ask it to figure out how to do this

330
00:10:47,120 --> 00:10:48,240
itself

331
00:10:48,240 --> 00:10:49,839
and this has problems like adversarial

332
00:10:49,839 --> 00:10:51,600
examples but at least it's well

333
00:10:51,600 --> 00:10:53,120
specified

334
00:10:53,120 --> 00:10:54,800
the problem with semi-supervised

335
00:10:54,800 --> 00:10:57,040
learning is that we're no longer

336
00:10:57,040 --> 00:10:59,519
specifying exactly what problem we want

337
00:10:59,519 --> 00:11:01,120
the algorithm to solve

338
00:11:01,120 --> 00:11:02,800
instead we're just giving it a couple of

339
00:11:02,800 --> 00:11:05,360
labeled examples and hoping it figures

340
00:11:05,360 --> 00:11:07,360
out the right problem to solve and does

341
00:11:07,360 --> 00:11:09,120
it in some reasonable way

342
00:11:09,120 --> 00:11:11,680
and the general problem we've shown here

343
00:11:11,680 --> 00:11:13,760
is that an adversary who can inject

344
00:11:13,760 --> 00:11:16,240
labeled unlabeled examples here can

345
00:11:16,240 --> 00:11:18,079
cause the model to do something the

346
00:11:18,079 --> 00:11:20,399
adversary wants by making it easier to

347
00:11:20,399 --> 00:11:24,480
learn this than the actual benign task

348
00:11:24,480 --> 00:11:26,880
so the real conclusion here is that

349
00:11:26,880 --> 00:11:28,800
poisoning the unlabeled data sets is

350
00:11:28,800 --> 00:11:30,880
going to become a real thing

351
00:11:30,880 --> 00:11:32,560
unlabeled data sets are commonly

352
00:11:32,560 --> 00:11:33,920
collected now

353
00:11:33,920 --> 00:11:35,920
and all kinds of algorithms from

354
00:11:35,920 --> 00:11:37,279
semi-supervised learning to

355
00:11:37,279 --> 00:11:39,360
self-supervised learning to who knows

356
00:11:39,360 --> 00:11:40,560
what's going to happen in the future are

357
00:11:40,560 --> 00:11:42,160
going to make use of these unlabeled

358
00:11:42,160 --> 00:11:44,160
data sets because they're going to have

359
00:11:44,160 --> 00:11:45,040
to

360
00:11:45,040 --> 00:11:46,640
and so it's going to be the security

361
00:11:46,640 --> 00:11:48,800
committee's job to figure out what we're

362
00:11:48,800 --> 00:11:50,320
going to do to allow the machine

363
00:11:50,320 --> 00:11:52,160
learning community to keep on pushing in

364
00:11:52,160 --> 00:11:53,519
this direction of using larger and

365
00:11:53,519 --> 00:11:55,839
larger unlabeled data sets

366
00:11:55,839 --> 00:11:58,320
in this paper we have an idea

367
00:11:58,320 --> 00:12:00,079
for how to protect the unlabeled data

368
00:12:00,079 --> 00:12:02,399
sets of semi-supervised learning um but

369
00:12:02,399 --> 00:12:04,399
how to solve this problem in general and

370
00:12:04,399 --> 00:12:05,760
what other attacks are going to look

371
00:12:05,760 --> 00:12:07,360
like is really something interesting

372
00:12:07,360 --> 00:12:09,200
that i hope we consider studying over

373
00:12:09,200 --> 00:12:12,839
the next couple of years

