1
00:00:08,639 --> 00:00:11,840
hello my name is leorofan in this video

2
00:00:11,840 --> 00:00:14,320
i'm going to present our work fishpedia

3
00:00:14,320 --> 00:00:16,480
a hybrid deep learning based approach to

4
00:00:16,480 --> 00:00:19,600
visually identification web pages

5
00:00:19,600 --> 00:00:22,000
visual attack is a type of cybercram

6
00:00:22,000 --> 00:00:23,760
which steals user's credential

7
00:00:23,760 --> 00:00:26,480
information by disguising itself as

8
00:00:26,480 --> 00:00:29,199
legitimate entity phishing attack has

9
00:00:29,199 --> 00:00:31,119
grown in prevalence since last year

10
00:00:31,119 --> 00:00:33,920
especially due to the global pandemic

11
00:00:33,920 --> 00:00:37,040
in this work our focus is to accurately

12
00:00:37,040 --> 00:00:39,200
identify facial websites

13
00:00:39,200 --> 00:00:41,360
here we want to clarify the difference

14
00:00:41,360 --> 00:00:43,680
between fission detection and phishing

15
00:00:43,680 --> 00:00:46,320
identification while efficient detection

16
00:00:46,320 --> 00:00:48,719
only need to classify whether a url is

17
00:00:48,719 --> 00:00:50,559
fishing page or not fusion

18
00:00:50,559 --> 00:00:53,199
identification further need to show what

19
00:00:53,199 --> 00:00:54,879
is the patient target

20
00:00:54,879 --> 00:00:56,640
in vision detection model the

21
00:00:56,640 --> 00:00:58,480
classification decision is usually

22
00:00:58,480 --> 00:01:01,039
associated with the confidence score

23
00:01:01,039 --> 00:01:03,680
however it's hard to explain why the

24
00:01:03,680 --> 00:01:05,840
confidence score is low or high in

25
00:01:05,840 --> 00:01:08,560
contrast provision identification tasks

26
00:01:08,560 --> 00:01:10,479
we will try to explain why the model

27
00:01:10,479 --> 00:01:13,439
belief is targeted for this brand thus

28
00:01:13,439 --> 00:01:15,280
visual identification provides more

29
00:01:15,280 --> 00:01:17,920
causality analysis for model decision

30
00:01:17,920 --> 00:01:19,840
which we think is more meaningful to

31
00:01:19,840 --> 00:01:22,080
investigate

32
00:01:22,080 --> 00:01:23,520
typical framework for phishing

33
00:01:23,520 --> 00:01:26,640
authentication is as follows since we

34
00:01:26,640 --> 00:01:28,479
need to find a targeted brand for

35
00:01:28,479 --> 00:01:30,720
efficient websites we need to keep a

36
00:01:30,720 --> 00:01:33,280
reference repo to store targeted brands

37
00:01:33,280 --> 00:01:35,200
by page signature

38
00:01:35,200 --> 00:01:37,759
secondly when a url is coming there

39
00:01:37,759 --> 00:01:39,680
should be a content abstraction module

40
00:01:39,680 --> 00:01:41,920
to extract the representation of a web

41
00:01:41,920 --> 00:01:42,960
page

42
00:01:42,960 --> 00:01:44,880
afterward there will be a target

43
00:01:44,880 --> 00:01:46,720
matching module to match webpage

44
00:01:46,720 --> 00:01:50,479
representation to reference repo

45
00:01:50,479 --> 00:01:52,560
there are two key research questions

46
00:01:52,560 --> 00:01:53,680
here

47
00:01:53,680 --> 00:01:56,320
first one how can we abstract webpage

48
00:01:56,320 --> 00:01:59,040
content to better represent the web page

49
00:01:59,040 --> 00:02:02,799
second how can we match the abstraction

50
00:02:02,799 --> 00:02:06,240
in md their basic idea is since the

51
00:02:06,240 --> 00:02:08,399
features are usually fake that image

52
00:02:08,399 --> 00:02:10,720
site so the color distribution of

53
00:02:10,720 --> 00:02:13,440
phishing screenshots should be similar

54
00:02:13,440 --> 00:02:15,920
to that of targeted brand

55
00:02:15,920 --> 00:02:18,160
the container abstraction module will

56
00:02:18,160 --> 00:02:21,200
process webpage screenshot into a vector

57
00:02:21,200 --> 00:02:23,200
describing the color distribution of

58
00:02:23,200 --> 00:02:24,560
screenshots

59
00:02:24,560 --> 00:02:26,800
the target natural module then compared

60
00:02:26,800 --> 00:02:29,280
the test vector to all the vectors in

61
00:02:29,280 --> 00:02:31,440
the reference variable

62
00:02:31,440 --> 00:02:34,319
we argue that emd has two problems

63
00:02:34,319 --> 00:02:36,560
firstly since emd heavily rely on the

64
00:02:36,560 --> 00:02:38,959
color distribution of screenshots it is

65
00:02:38,959 --> 00:02:41,040
not robust to small perturbation in the

66
00:02:41,040 --> 00:02:43,120
color design or the layout design of the

67
00:02:43,120 --> 00:02:44,640
website

68
00:02:44,640 --> 00:02:47,120
on the other hand emd introduce false

69
00:02:47,120 --> 00:02:50,080
positive when two screenshots are having

70
00:02:50,080 --> 00:02:52,560
similar color distribution

71
00:02:52,560 --> 00:02:54,080
based on the fact that comparing

72
00:02:54,080 --> 00:02:55,760
screenshots may introduce a lot of

73
00:02:55,760 --> 00:02:58,239
noises a more recent work called fish

74
00:02:58,239 --> 00:03:00,159
tsu argued that we don't need to match

75
00:03:00,159 --> 00:03:02,159
two screenshots we only need to match

76
00:03:02,159 --> 00:03:04,319
logo since logo carries the identity of

77
00:03:04,319 --> 00:03:06,000
a website better

78
00:03:06,000 --> 00:03:08,879
in a reference repo fish2 keeps a list

79
00:03:08,879 --> 00:03:11,840
of targeted brand logo where url is

80
00:03:11,840 --> 00:03:14,080
coming they use sift to match logo to a

81
00:03:14,080 --> 00:03:15,440
screenshot

82
00:03:15,440 --> 00:03:17,120
shift is the old-fashioned computer

83
00:03:17,120 --> 00:03:18,959
vision model which matched two pictures

84
00:03:18,959 --> 00:03:21,519
for matching key points however we

85
00:03:21,519 --> 00:03:23,280
observed that swift introduced many

86
00:03:23,280 --> 00:03:25,840
mismatching and incorrect matching

87
00:03:25,840 --> 00:03:27,680
sometimes the key points in logo are

88
00:03:27,680 --> 00:03:30,879
matched to irrelevant part in screenshot

89
00:03:30,879 --> 00:03:34,480
moreover sift has high run time overhead

90
00:03:34,480 --> 00:03:37,920
with 80 seconds per url on average

91
00:03:37,920 --> 00:03:40,400
to resolve the problem in social work we

92
00:03:40,400 --> 00:03:42,159
introduce fish media

93
00:03:42,159 --> 00:03:44,720
follow from fish 2 we agree that logo

94
00:03:44,720 --> 00:03:47,360
carries the identity of a website better

95
00:03:47,360 --> 00:03:49,280
different from fish2 we use deep

96
00:03:49,280 --> 00:03:52,000
learning object detection model to first

97
00:03:52,000 --> 00:03:53,599
report the position of the logo

98
00:03:53,599 --> 00:03:54,879
precisely

99
00:03:54,879 --> 00:03:57,360
then we directly compare logos via deep

100
00:03:57,360 --> 00:03:59,360
learning simus model

101
00:03:59,360 --> 00:04:00,400
in addition

102
00:04:00,400 --> 00:04:02,480
our system can provide explainable

103
00:04:02,480 --> 00:04:04,959
prediction we are able to visualize and

104
00:04:04,959 --> 00:04:08,159
debug which brand it is targeted for and

105
00:04:08,159 --> 00:04:10,159
where is trying to steal credential

106
00:04:10,159 --> 00:04:11,920
information

107
00:04:11,920 --> 00:04:13,519
we summarize the contributions of

108
00:04:13,519 --> 00:04:16,160
fishpedia here first we provide perfect

109
00:04:16,160 --> 00:04:18,238
local identification with customized

110
00:04:18,238 --> 00:04:20,160
object detection approach

111
00:04:20,160 --> 00:04:22,960
secondly it incurs low runtime overhead

112
00:04:22,960 --> 00:04:25,120
even with visual analysis

113
00:04:25,120 --> 00:04:27,120
thirdly it has low bias in efficient

114
00:04:27,120 --> 00:04:29,120
data set because there's no need to

115
00:04:29,120 --> 00:04:31,280
train on any pre-collected phishing data

116
00:04:31,280 --> 00:04:32,400
sources

117
00:04:32,400 --> 00:04:34,960
lastly we provide visual annotation on

118
00:04:34,960 --> 00:04:37,360
visual screenshots which improve user

119
00:04:37,360 --> 00:04:39,840
confidence in model prediction

120
00:04:39,840 --> 00:04:41,680
there are a few challenges in developing

121
00:04:41,680 --> 00:04:44,479
fishpedia the first challenge is that

122
00:04:44,479 --> 00:04:47,520
how to identify perfect logo second is

123
00:04:47,520 --> 00:04:49,840
how do we train a model to learn similar

124
00:04:49,840 --> 00:04:51,360
logos

125
00:04:51,360 --> 00:04:53,520
we rephrase the logo identification

126
00:04:53,520 --> 00:04:56,479
problem as an object detection problem

127
00:04:56,479 --> 00:04:59,120
at first we tried the off-the-shelf

128
00:04:59,120 --> 00:05:02,639
one-stage object detection model url v3

129
00:05:02,639 --> 00:05:05,600
but we find oftentimes model generate

130
00:05:05,600 --> 00:05:07,440
partial logo

131
00:05:07,440 --> 00:05:09,600
so we switch to use two-stage object

132
00:05:09,600 --> 00:05:12,160
detection model which is slower but more

133
00:05:12,160 --> 00:05:14,479
precise in bonnie box prediction it has

134
00:05:14,479 --> 00:05:16,800
two stages stage one is to roughly

135
00:05:16,800 --> 00:05:19,120
detect the region containing objects in

136
00:05:19,120 --> 00:05:21,039
our case we're interested in detecting

137
00:05:21,039 --> 00:05:24,479
logo as well as input boxes stage two is

138
00:05:24,479 --> 00:05:26,479
to refine and reported regions and

139
00:05:26,479 --> 00:05:28,880
classify the region into logo or input

140
00:05:28,880 --> 00:05:30,000
boxes

141
00:05:30,000 --> 00:05:32,320
in our work we borrowed the faster rcm

142
00:05:32,320 --> 00:05:35,199
model published in 2015.

143
00:05:35,199 --> 00:05:37,360
as i mentioned the second challenge is

144
00:05:37,360 --> 00:05:39,680
to train a model to learn similar logo

145
00:05:39,680 --> 00:05:41,120
we find that the traditional way of

146
00:05:41,120 --> 00:05:43,520
training semis does not work because

147
00:05:43,520 --> 00:05:45,680
what some is traditional doing is that

148
00:05:45,680 --> 00:05:47,600
it will force the local representations

149
00:05:47,600 --> 00:05:49,759
from the same brand to be very close and

150
00:05:49,759 --> 00:05:52,080
different brands to be far away but for

151
00:05:52,080 --> 00:05:53,600
the local variant that belongs to the

152
00:05:53,600 --> 00:05:55,280
same brand they may have very different

153
00:05:55,280 --> 00:05:57,680
appearance even from human point of view

154
00:05:57,680 --> 00:05:59,600
it's not necessary to constrain them to

155
00:05:59,600 --> 00:06:01,919
be within a strict distance threshold

156
00:06:01,919 --> 00:06:04,960
that could result in heavy overfitting

157
00:06:04,960 --> 00:06:06,800
so instead we train on classification

158
00:06:06,800 --> 00:06:09,199
model for logos and the classes here are

159
00:06:09,199 --> 00:06:11,280
different brands then we will use the

160
00:06:11,280 --> 00:06:15,280
model in a style we compare two logos

161
00:06:15,280 --> 00:06:17,039
we use the last representation layer

162
00:06:17,039 --> 00:06:19,199
before the softmax activation and take

163
00:06:19,199 --> 00:06:22,560
the cosine similarity

164
00:06:23,440 --> 00:06:25,280
we implement resin v2 as our

165
00:06:25,280 --> 00:06:27,759
classification model to boost the model

166
00:06:27,759 --> 00:06:30,000
performance on local data set we apply

167
00:06:30,000 --> 00:06:32,400
two-state training first we pre-train

168
00:06:32,400 --> 00:06:33,680
the classification model on a

169
00:06:33,680 --> 00:06:35,440
large-scale local data set called local

170
00:06:35,440 --> 00:06:38,240
2k plus published in triple ai

171
00:06:38,240 --> 00:06:40,080
then we function the model on our local

172
00:06:40,080 --> 00:06:42,400
calculus data set

173
00:06:42,400 --> 00:06:44,960
to summarize our approach first we use

174
00:06:44,960 --> 00:06:47,280
faster rcm object detection model to

175
00:06:47,280 --> 00:06:48,720
recognize logo

176
00:06:48,720 --> 00:06:50,880
then we compare logo via resonant v2

177
00:06:50,880 --> 00:06:53,360
based simus model

178
00:06:53,360 --> 00:06:55,039
in our experiment section we want to

179
00:06:55,039 --> 00:06:57,759
answer the following research questions

180
00:06:57,759 --> 00:06:59,599
first one is whether fish pdf can

181
00:06:59,599 --> 00:07:02,319
identify the facial url effectively

182
00:07:02,319 --> 00:07:04,720
second research question how good is the

183
00:07:04,720 --> 00:07:07,840
individual component of fishpedia

184
00:07:07,840 --> 00:07:11,039
rq3 is whether fishpedia can catch more

185
00:07:11,039 --> 00:07:13,360
fish in the world

186
00:07:13,360 --> 00:07:15,440
before diving into the details let me

187
00:07:15,440 --> 00:07:17,680
briefly explain about our experimental

188
00:07:17,680 --> 00:07:18,720
setup

189
00:07:18,720 --> 00:07:22,080
we collected 3k efficient from open fish

190
00:07:22,080 --> 00:07:25,840
and 3kb9 from alexa for each webpage we

191
00:07:25,840 --> 00:07:28,319
record their metadata screenshot as well

192
00:07:28,319 --> 00:07:30,479
as html code

193
00:07:30,479 --> 00:07:32,240
in addition we keep a reference local

194
00:07:32,240 --> 00:07:35,680
database which covers the top 181 brands

195
00:07:35,680 --> 00:07:37,759
that are mostly attacked from what

196
00:07:37,759 --> 00:07:39,599
openfit has reported

197
00:07:39,599 --> 00:07:41,680
this bunch of logos are also used in

198
00:07:41,680 --> 00:07:43,599
smith training

199
00:07:43,599 --> 00:07:45,840
in order to train object detection model

200
00:07:45,840 --> 00:07:47,759
we prepare 30k labeled legitimate

201
00:07:47,759 --> 00:07:50,000
websites where we annotate the positions

202
00:07:50,000 --> 00:07:52,319
of logo and input boxes

203
00:07:52,319 --> 00:07:54,319
note that for both object detection

204
00:07:54,319 --> 00:07:57,120
model and semis model we didn't use any

205
00:07:57,120 --> 00:07:59,520
phishing data to train because the local

206
00:07:59,520 --> 00:08:02,319
recognition and local comparison tasks

207
00:08:02,319 --> 00:08:04,319
are independent of the nature of the

208
00:08:04,319 --> 00:08:05,440
website

209
00:08:05,440 --> 00:08:07,759
this data agnostic training fashion

210
00:08:07,759 --> 00:08:09,919
prevents potential model bias on a

211
00:08:09,919 --> 00:08:13,360
specific vision data source we use

212
00:08:13,360 --> 00:08:15,520
now let us look at the first research

213
00:08:15,520 --> 00:08:17,680
question which is the overall fish pdf

214
00:08:17,680 --> 00:08:19,840
performance we can see that from the

215
00:08:19,840 --> 00:08:22,720
table fish pedia have higher precision

216
00:08:22,720 --> 00:08:24,639
higher recall and higher identification

217
00:08:24,639 --> 00:08:27,120
accuracy well it does not incur

218
00:08:27,120 --> 00:08:29,599
additional runtime overhead

219
00:08:29,599 --> 00:08:31,039
second research question is the

220
00:08:31,039 --> 00:08:33,039
evaluation of individual component

221
00:08:33,039 --> 00:08:34,159
performance

222
00:08:34,159 --> 00:08:36,880
for object detector the test mean

223
00:08:36,880 --> 00:08:39,039
average position

224
00:08:39,039 --> 00:08:39,760
nine can reach

225
00:08:39,760 --> 00:08:42,479
percent for siamese the same is matching

226
00:08:42,479 --> 00:08:44,959
accuracy on one thousand logos cropped

227
00:08:44,959 --> 00:08:48,080
from fishing website can reach 93.5

228
00:08:48,080 --> 00:08:50,560
percent

229
00:08:50,640 --> 00:08:52,839
a very important but largely

230
00:08:52,839 --> 00:08:55,040
undiscussed research question in

231
00:08:55,040 --> 00:08:57,200
previous state of the art work is

232
00:08:57,200 --> 00:08:59,279
whether the model can catch more fishing

233
00:08:59,279 --> 00:09:01,600
in the world because it's easy to train

234
00:09:01,600 --> 00:09:03,600
a model which have superior performance

235
00:09:03,600 --> 00:09:06,000
on experiment set but we need to test

236
00:09:06,000 --> 00:09:08,399
the model in real life scenario to prove

237
00:09:08,399 --> 00:09:10,959
the ability of catching new fishing

238
00:09:10,959 --> 00:09:13,440
we deploy fish pedia on emerging domains

239
00:09:13,440 --> 00:09:16,080
fed from third stream for one month

240
00:09:16,080 --> 00:09:18,200
wish pedia can report

241
00:09:18,200 --> 00:09:22,640
1820 exhibition out of which 1704 are

242
00:09:22,640 --> 00:09:25,279
real fishing and 1133 are zero-day

243
00:09:25,279 --> 00:09:27,519
efficient which are not reported by any

244
00:09:27,519 --> 00:09:29,600
other engines

245
00:09:29,600 --> 00:09:32,080
other baseline tools report too many

246
00:09:32,080 --> 00:09:33,760
false positives so

247
00:09:33,760 --> 00:09:37,519
cannot be used in industry

248
00:09:37,519 --> 00:09:39,920
here we show some examples of farm

249
00:09:39,920 --> 00:09:42,640
fishing these three examples are from

250
00:09:42,640 --> 00:09:45,519
instagram fishing physical facebook

251
00:09:45,519 --> 00:09:49,360
fishing and bank of america fishing

252
00:09:49,360 --> 00:09:52,640
to conclude fishpedia is the fish

253
00:09:52,640 --> 00:09:54,560
fishpedia is a fishing authentication

254
00:09:54,560 --> 00:09:56,720
technique helping extend fishing

255
00:09:56,720 --> 00:09:58,000
causality

256
00:09:58,000 --> 00:10:00,160
technically fishpedia makes the

257
00:10:00,160 --> 00:10:02,800
following contribution the first one is

258
00:10:02,800 --> 00:10:06,240
perfect identity logo identification

259
00:10:06,240 --> 00:10:08,480
second one is accurate logo matching

260
00:10:08,480 --> 00:10:11,600
model third one it incurs low runtime

261
00:10:11,600 --> 00:10:15,360
overhead and lastly it has low bias

262
00:10:15,360 --> 00:10:16,839
efficient data

263
00:10:16,839 --> 00:10:19,680
set thanks for watching that's the end

264
00:10:19,680 --> 00:10:23,160
of our presentation

