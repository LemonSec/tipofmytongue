1
00:00:08,800 --> 00:00:10,000
okay

2
00:00:10,000 --> 00:00:12,160
in this presentation i will present our

3
00:00:12,160 --> 00:00:14,960
paper graph background and this paper is

4
00:00:14,960 --> 00:00:17,680
finished by me yuen pong professor

5
00:00:17,680 --> 00:00:20,080
scholandian professor tinuam

6
00:00:20,080 --> 00:00:21,840
so i'm the first author and the

7
00:00:21,840 --> 00:00:24,960
president of chinese

8
00:00:25,119 --> 00:00:27,279
we are motivated by the research efforts

9
00:00:27,279 --> 00:00:29,519
of backdoor or trading attacks

10
00:00:29,519 --> 00:00:32,159
in those attacks the attacker aims to

11
00:00:32,159 --> 00:00:34,559
generate a trojan model that's only

12
00:00:34,559 --> 00:00:37,120
responses to trigger embedded inputs and

13
00:00:37,120 --> 00:00:39,840
there's specific behaviors like always

14
00:00:39,840 --> 00:00:42,000
misidentifies those trigger embedded

15
00:00:42,000 --> 00:00:44,960
inputs in a specific category

16
00:00:44,960 --> 00:00:47,600
however previous research works only

17
00:00:47,600 --> 00:00:49,440
focus on the image vector against the

18
00:00:49,440 --> 00:00:52,160
variant system due to the flexibility of

19
00:00:52,160 --> 00:00:54,879
graph representation and the success of

20
00:00:54,879 --> 00:00:56,800
graph neural networks are genes to

21
00:00:56,800 --> 00:00:58,719
capture the graph properties

22
00:00:58,719 --> 00:01:01,680
now more and more systems leverage gn as

23
00:01:01,680 --> 00:01:03,840
a supportive background like molecular

24
00:01:03,840 --> 00:01:06,080
identifier and malware detector

25
00:01:06,080 --> 00:01:08,000
however the vulnerabilities of

26
00:01:08,000 --> 00:01:10,400
gene-based learning systems are largely

27
00:01:10,400 --> 00:01:12,479
oxford especially the backdoor

28
00:01:12,479 --> 00:01:14,000
vulnerability

29
00:01:14,000 --> 00:01:16,960
so we seek to bridge the gap between

30
00:01:16,960 --> 00:01:20,240
backdoor on a bit and grab learning but

31
00:01:20,240 --> 00:01:22,560
it's not simply borrowing techniques

32
00:01:22,560 --> 00:01:24,479
from image domain to the graph

33
00:01:24,479 --> 00:01:26,080
there are a lot of details that need to

34
00:01:26,080 --> 00:01:27,920
be redefined on the graph domain such

35
00:01:27,920 --> 00:01:30,720
like how can we define a graph trigger

36
00:01:30,720 --> 00:01:33,119
we define it as a sub graph that should

37
00:01:33,119 --> 00:01:34,640
contain topological structure and

38
00:01:34,640 --> 00:01:36,320
descriptive features

39
00:01:36,320 --> 00:01:38,880
and it's also doubtful that a graph

40
00:01:38,880 --> 00:01:40,880
trigger should be universal since the

41
00:01:40,880 --> 00:01:43,200
graph itself is non-structured

42
00:01:43,200 --> 00:01:46,240
thus we also point that trigger can be

43
00:01:46,240 --> 00:01:48,640
tailored to reflect the characteristics

44
00:01:48,640 --> 00:01:50,240
of a graph

45
00:01:50,240 --> 00:01:53,040
also we need to seek a suitable locality

46
00:01:53,040 --> 00:01:55,840
to patch the trigger

47
00:01:56,399 --> 00:01:58,960
with those syncings in mind we designed

48
00:01:58,960 --> 00:02:01,920
gta the graph trading attack against the

49
00:02:01,920 --> 00:02:02,880
gm

50
00:02:02,880 --> 00:02:05,439
gta makes perturbations in upstream

51
00:02:05,439 --> 00:02:07,680
training and takes effects in downstream

52
00:02:07,680 --> 00:02:08,959
classification

53
00:02:08,959 --> 00:02:12,160
in upstream training gta forges a trojan

54
00:02:12,160 --> 00:02:14,879
model by injecting a backdrop into gm

55
00:02:14,879 --> 00:02:17,599
with spectral training it also relies on

56
00:02:17,599 --> 00:02:19,840
a bi-level optimization process to

57
00:02:19,840 --> 00:02:22,400
further optimize the trigger for better

58
00:02:22,400 --> 00:02:24,560
effect

59
00:02:24,560 --> 00:02:27,280
in downstream classification gta has no

60
00:02:27,280 --> 00:02:29,599
assumptions about what's uh what's a

61
00:02:29,599 --> 00:02:32,000
classifier even though the classifier is

62
00:02:32,000 --> 00:02:33,920
constructed by downstream user and

63
00:02:33,920 --> 00:02:35,200
function

64
00:02:35,200 --> 00:02:37,519
the trojan model can still encode the

65
00:02:37,519 --> 00:02:40,560
trigger embedded graphs and results in a

66
00:02:40,560 --> 00:02:43,760
fortified prediction

67
00:02:43,920 --> 00:02:45,840
then i will introduce the design

68
00:02:45,840 --> 00:02:48,800
procedures of gta at the very beginning

69
00:02:48,800 --> 00:02:51,360
we leverage a well-trained graph

70
00:02:51,360 --> 00:02:54,160
attention network to encode each graph

71
00:02:54,160 --> 00:02:56,400
those encodings are assured to capture

72
00:02:56,400 --> 00:02:59,680
both topology info and original features

73
00:02:59,680 --> 00:03:02,239
then we use those encodings to do the

74
00:03:02,239 --> 00:03:04,720
trigger generation where we construct

75
00:03:04,720 --> 00:03:06,640
the two fully connected networks as

76
00:03:06,640 --> 00:03:09,040
topology generator and feature generator

77
00:03:09,040 --> 00:03:10,640
respectively

78
00:03:10,640 --> 00:03:12,560
the topology generator maps node

79
00:03:12,560 --> 00:03:14,879
encodings into low dimensional space and

80
00:03:14,879 --> 00:03:16,879
determines whether to node should have a

81
00:03:16,879 --> 00:03:18,879
connection in trigger based on their

82
00:03:18,879 --> 00:03:20,560
encoding similarity

83
00:03:20,560 --> 00:03:24,239
well the feature generator

84
00:03:24,239 --> 00:03:26,239
well the feature generator searches for

85
00:03:26,239 --> 00:03:29,840
new features for an effective attack

86
00:03:29,840 --> 00:03:32,480
then we combine the generated topology

87
00:03:32,480 --> 00:03:34,879
and all those features as a generated

88
00:03:34,879 --> 00:03:35,840
trigger

89
00:03:35,840 --> 00:03:38,400
and we use generators rather than

90
00:03:38,400 --> 00:03:40,640
predefined trigger because we want to

91
00:03:40,640 --> 00:03:42,879
tailor a trigger for each graph

92
00:03:42,879 --> 00:03:45,200
later we talk about optimizing the

93
00:03:45,200 --> 00:03:48,080
trigger we actually refer to optimizing

94
00:03:48,080 --> 00:03:49,120
those

95
00:03:49,120 --> 00:03:51,760
generators

96
00:03:52,239 --> 00:03:55,680
after getting a trigger gt we rely on

97
00:03:55,680 --> 00:03:57,840
mixing function to find a suitable

98
00:03:57,840 --> 00:04:00,400
locating the graph and replace the

99
00:04:00,400 --> 00:04:03,360
original subgraph g with gt

100
00:04:03,360 --> 00:04:05,760
and our trigger embedding only happens

101
00:04:05,760 --> 00:04:07,920
among graphs whose label is not at the

102
00:04:07,920 --> 00:04:10,720
targeted level so after embedding the

103
00:04:10,720 --> 00:04:13,439
trigger to some graph samples we now

104
00:04:13,439 --> 00:04:15,439
have a poison transaction and use it to

105
00:04:15,439 --> 00:04:18,079
do the backdoor training

106
00:04:18,079 --> 00:04:20,959
optimizing gn's parameter with backdoor

107
00:04:20,959 --> 00:04:23,520
transact is only the lower level driver

108
00:04:23,520 --> 00:04:25,360
by level optimization

109
00:04:25,360 --> 00:04:27,680
after getting a well-trained gn we

110
00:04:27,680 --> 00:04:29,840
further use it to instruct the higher

111
00:04:29,840 --> 00:04:31,680
level trigger generation

112
00:04:31,680 --> 00:04:34,240
here we aim to generate triggers that

113
00:04:34,240 --> 00:04:37,199
can make any embedded graph be treated

114
00:04:37,199 --> 00:04:41,040
as a targeted category by gn

115
00:04:41,040 --> 00:04:42,880
okay

116
00:04:42,880 --> 00:04:45,360
so far we introduced the major

117
00:04:45,360 --> 00:04:46,800
techniques

118
00:04:46,800 --> 00:04:49,680
um besides uh besides them we also have

119
00:04:49,680 --> 00:04:52,639
a large set of empirical evaluations

120
00:04:52,639 --> 00:04:54,880
where we use data sets from multiple

121
00:04:54,880 --> 00:04:58,000
domains cyber security biochemistry and

122
00:04:58,000 --> 00:04:59,280
social nets

123
00:04:59,280 --> 00:05:01,600
we also consider multiple settings from

124
00:05:01,600 --> 00:05:03,840
inductive to transductive classification

125
00:05:03,840 --> 00:05:05,919
from self-transfer to mutual transfer

126
00:05:05,919 --> 00:05:07,840
and from grass graph space to input

127
00:05:07,840 --> 00:05:08,880
space

128
00:05:08,880 --> 00:05:11,440
the red table contains the basic info of

129
00:05:11,440 --> 00:05:14,320
our used data sets

130
00:05:14,320 --> 00:05:16,639
moreover we use some representative

131
00:05:16,639 --> 00:05:18,639
genes to show general attack

132
00:05:18,639 --> 00:05:20,000
effectiveness

133
00:05:20,000 --> 00:05:20,960
since

134
00:05:20,960 --> 00:05:22,720
we are the first worker to introduce

135
00:05:22,720 --> 00:05:25,360
backdoor attacks against the genus we

136
00:05:25,360 --> 00:05:28,800
just adjusted the gta setting as two and

137
00:05:28,800 --> 00:05:30,400
get two baselines

138
00:05:30,400 --> 00:05:32,720
the first baseline uses a fully

139
00:05:32,720 --> 00:05:34,720
connected graph as a trigger and

140
00:05:34,720 --> 00:05:36,720
features are only adaptive among

141
00:05:36,720 --> 00:05:39,120
different dimensions so even though one

142
00:05:39,120 --> 00:05:40,960
trigger has multiple nodes with multiple

143
00:05:40,960 --> 00:05:42,960
feature vectors

144
00:05:42,960 --> 00:05:44,800
the second business has a little bit

145
00:05:44,800 --> 00:05:47,120
higher flexibility it

146
00:05:47,120 --> 00:05:50,960
uses adaptive topology and allow nodes

147
00:05:50,960 --> 00:05:52,720
your trigger has different feature

148
00:05:52,720 --> 00:05:53,840
vectors

149
00:05:53,840 --> 00:05:54,639
but

150
00:05:54,639 --> 00:05:57,680
both baselines just use a universal

151
00:05:57,680 --> 00:06:00,160
trigger during the attack

152
00:06:00,160 --> 00:06:02,880
so baseline 1 baseline 2 and gta shows

153
00:06:02,880 --> 00:06:04,960
the increasing adaptiveness of the

154
00:06:04,960 --> 00:06:06,479
trigger

155
00:06:06,479 --> 00:06:09,120
we also use multiple metrics for

156
00:06:09,120 --> 00:06:11,199
comprehensive evaluation we can

157
00:06:11,199 --> 00:06:13,840
categorize them into two groups one for

158
00:06:13,840 --> 00:06:15,919
showing the attack effectiveness like

159
00:06:15,919 --> 00:06:18,960
the attack success rate or asr

160
00:06:18,960 --> 00:06:21,039
another for showing the evasiveness like

161
00:06:21,039 --> 00:06:24,479
the clear case drop or cd so asr is used

162
00:06:24,479 --> 00:06:27,840
to the trigger embedded graphs and cd is

163
00:06:27,840 --> 00:06:30,880
evaluated is evaluated among the benign

164
00:06:30,880 --> 00:06:32,479
samples

165
00:06:32,479 --> 00:06:34,800
the red table is original accuracy of

166
00:06:34,800 --> 00:06:37,919
each data set with a corresponding gm

167
00:06:37,919 --> 00:06:39,759
and the accuracies are acceptable so

168
00:06:39,759 --> 00:06:41,680
which means it's meaningful to discuss

169
00:06:41,680 --> 00:06:45,600
the text of those systems

170
00:06:45,759 --> 00:06:48,880
here are some of our evaluation results

171
00:06:48,880 --> 00:06:51,280
under inductive settings we first

172
00:06:51,280 --> 00:06:53,360
consider attacking the self-transfer

173
00:06:53,360 --> 00:06:54,800
learning within

174
00:06:54,800 --> 00:06:56,639
the same data set

175
00:06:56,639 --> 00:06:58,319
then consider attacking the mutual

176
00:06:58,319 --> 00:06:59,599
transfer learning between different

177
00:06:59,599 --> 00:07:02,639
datasets within a similar domain

178
00:07:02,639 --> 00:07:04,639
later we use a pre-trained model

179
00:07:04,639 --> 00:07:06,639
download it download it from the public

180
00:07:06,639 --> 00:07:08,080
repository

181
00:07:08,080 --> 00:07:10,400
and use gta to inject the backdoor into

182
00:07:10,400 --> 00:07:11,199
it

183
00:07:11,199 --> 00:07:12,639
among those

184
00:07:12,639 --> 00:07:15,840
all those settings we can observe that

185
00:07:15,840 --> 00:07:18,400
the height is the success rate of

186
00:07:18,400 --> 00:07:20,960
it's always gta and it also has

187
00:07:20,960 --> 00:07:25,680
relatively low clean accuracy drops

188
00:07:26,160 --> 00:07:28,960
we then immigrate the gta to a

189
00:07:28,960 --> 00:07:31,039
transductive setting where we want to

190
00:07:31,039 --> 00:07:33,520
affect the node classification results

191
00:07:33,520 --> 00:07:35,520
after injecting a trigger

192
00:07:35,520 --> 00:07:39,199
so if a gm has can has k layer

193
00:07:39,199 --> 00:07:41,840
it can pass features from a centric node

194
00:07:41,840 --> 00:07:44,960
to its k-hop neighbors at most

195
00:07:44,960 --> 00:07:46,720
so we also

196
00:07:46,720 --> 00:07:48,720
expect the trigger's effect can be

197
00:07:48,720 --> 00:07:50,879
delivered to such a distance

198
00:07:50,879 --> 00:07:53,199
therefore when marrying the attack

199
00:07:53,199 --> 00:07:55,520
success rate we consider a trigger's

200
00:07:55,520 --> 00:07:58,160
neighbor within k-hop and when marrying

201
00:07:58,160 --> 00:08:00,720
the clean accuracy drop we consider the

202
00:08:00,720 --> 00:08:03,840
further notes larger than k-hope

203
00:08:03,840 --> 00:08:06,639
still gta has the highest success rates

204
00:08:06,639 --> 00:08:09,440
and low accuracy drops

205
00:08:09,440 --> 00:08:12,080
also gta has no assumption of downstream

206
00:08:12,080 --> 00:08:14,240
models or classifiers

207
00:08:14,240 --> 00:08:17,199
by default we use a fully connected

208
00:08:17,199 --> 00:08:19,599
layer as a classifier

209
00:08:19,599 --> 00:08:21,520
here we also evaluate the use of

210
00:08:21,520 --> 00:08:23,840
different classifiers as showing the

211
00:08:23,840 --> 00:08:26,800
bottom table and we find that not only

212
00:08:26,800 --> 00:08:29,599
gta both baselines also have good attack

213
00:08:29,599 --> 00:08:32,399
performances so it's which demonstrates

214
00:08:32,399 --> 00:08:33,279
that

215
00:08:33,279 --> 00:08:35,919
our trojan technique is downstream model

216
00:08:35,919 --> 00:08:38,478
agnostic

217
00:08:38,559 --> 00:08:39,760
so far

218
00:08:39,760 --> 00:08:42,240
gta works in graph space

219
00:08:42,240 --> 00:08:44,480
here we also consider the input space

220
00:08:44,480 --> 00:08:47,279
attack which means our trigger not only

221
00:08:47,279 --> 00:08:49,920
be passed into graphs but can survive

222
00:08:49,920 --> 00:08:52,160
when transferring back to the original

223
00:08:52,160 --> 00:08:55,200
input format for example a gm based

224
00:08:55,200 --> 00:08:57,839
android malware detector first transfers

225
00:08:57,839 --> 00:09:00,000
raw android installation packages or

226
00:09:00,000 --> 00:09:03,519
apks into a car graphs

227
00:09:03,519 --> 00:09:06,160
where each code is uh each node is the

228
00:09:06,160 --> 00:09:08,640
java function and

229
00:09:08,640 --> 00:09:10,640
edge represents the correlation among

230
00:09:10,640 --> 00:09:12,399
those functionalities

231
00:09:12,399 --> 00:09:14,880
when injecting triggers we should ensure

232
00:09:14,880 --> 00:09:17,120
that the trigger embedded graphs are

233
00:09:17,120 --> 00:09:20,959
transferable back to raw binary formats

234
00:09:20,959 --> 00:09:23,279
also our trigger shouldn't affect the

235
00:09:23,279 --> 00:09:25,120
normal use of those installation

236
00:09:25,120 --> 00:09:27,920
packages even though it survived in

237
00:09:27,920 --> 00:09:30,000
binary format

238
00:09:30,000 --> 00:09:32,640
here we make two sets of evaluations the

239
00:09:32,640 --> 00:09:35,200
first one only use cargo of topology and

240
00:09:35,200 --> 00:09:37,760
function name embedding as substitute

241
00:09:37,760 --> 00:09:39,120
feature

242
00:09:39,120 --> 00:09:42,000
thus our our attack cannot perturb the

243
00:09:42,000 --> 00:09:43,200
features

244
00:09:43,200 --> 00:09:45,839
but unable to add dummy cost as trigger

245
00:09:45,839 --> 00:09:46,800
ads

246
00:09:46,800 --> 00:09:49,040
and the second evaluation uses both

247
00:09:49,040 --> 00:09:51,360
topology and the original static feature

248
00:09:51,360 --> 00:09:54,399
of each android function thus

249
00:09:54,399 --> 00:09:56,720
drawing perturbation we should ensure

250
00:09:56,720 --> 00:09:59,040
the perturbed feature value also

251
00:09:59,040 --> 00:10:01,360
meaningful and when transferring back

252
00:10:01,360 --> 00:10:02,839
to input

253
00:10:02,839 --> 00:10:06,320
space here we evaluate the input space

254
00:10:06,320 --> 00:10:08,800
and graph space attacks this graph space

255
00:10:08,800 --> 00:10:11,120
has more flexibility thus

256
00:10:11,120 --> 00:10:12,800
is more powerful

257
00:10:12,800 --> 00:10:15,279
but the input space attacks are also

258
00:10:15,279 --> 00:10:18,079
effective enough

259
00:10:18,079 --> 00:10:20,079
finally we consider two lines of

260
00:10:20,079 --> 00:10:22,720
potential candidates with differences

261
00:10:22,720 --> 00:10:24,959
the first one is data inspection which

262
00:10:24,959 --> 00:10:27,440
aims to filter embedded triggers and

263
00:10:27,440 --> 00:10:29,680
their influence from graphs

264
00:10:29,680 --> 00:10:31,519
here we use a randomized smoothing

265
00:10:31,519 --> 00:10:33,920
technique which independently subsamples

266
00:10:33,920 --> 00:10:36,800
the graph multiple times and uses

267
00:10:36,800 --> 00:10:39,360
assembled graphs for certifiable

268
00:10:39,360 --> 00:10:40,720
training

269
00:10:40,720 --> 00:10:42,720
another contaminant method is borrowed

270
00:10:42,720 --> 00:10:45,760
from the variant domain we imitate one

271
00:10:45,760 --> 00:10:47,519
image vector defense model named the

272
00:10:47,519 --> 00:10:50,079
newer class and graph domain to the

273
00:10:50,079 --> 00:10:51,440
graph domain

274
00:10:51,440 --> 00:10:53,360
where we first search for the reverse

275
00:10:53,360 --> 00:10:55,040
trigger for each class

276
00:10:55,040 --> 00:10:58,000
uh statistically then we statistically

277
00:10:58,000 --> 00:11:00,720
compare the property of those triggers

278
00:11:00,720 --> 00:11:02,720
to detect the potential background

279
00:11:02,720 --> 00:11:04,240
targets

280
00:11:04,240 --> 00:11:06,320
however both methods

281
00:11:06,320 --> 00:11:08,560
the randomized smoothing and nucleus

282
00:11:08,560 --> 00:11:11,040
have limited detection performances when

283
00:11:11,040 --> 00:11:13,360
defending against the rgta

284
00:11:13,360 --> 00:11:16,160
so one may refer to our paper for more

285
00:11:16,160 --> 00:11:17,920
details

286
00:11:17,920 --> 00:11:18,880
okay

287
00:11:18,880 --> 00:11:20,959
those are almost the major parts of our

288
00:11:20,959 --> 00:11:23,360
work as a short summary

289
00:11:23,360 --> 00:11:25,120
when proposed gta

290
00:11:25,120 --> 00:11:27,760
we propose gt the first backdoor or

291
00:11:27,760 --> 00:11:29,680
trojan attack

292
00:11:29,680 --> 00:11:31,279
against the gn

293
00:11:31,279 --> 00:11:33,760
which uses a subgraph as a trigger and

294
00:11:33,760 --> 00:11:36,160
taylor's uh trigger to reflect the

295
00:11:36,160 --> 00:11:38,959
characteristics of input graphs

296
00:11:38,959 --> 00:11:41,440
it also agnostic to the downstream model

297
00:11:41,440 --> 00:11:43,680
which leads to a resistive trading

298
00:11:43,680 --> 00:11:44,640
attack

299
00:11:44,640 --> 00:11:47,440
moreover it's extensible on multiple

300
00:11:47,440 --> 00:11:49,440
learning settings such as the inductive

301
00:11:49,440 --> 00:11:51,519
or the graph level classification and

302
00:11:51,519 --> 00:11:53,440
the transductive we know the level

303
00:11:53,440 --> 00:11:55,360
classification

304
00:11:55,360 --> 00:11:58,720
okay so yeah that's my presentation if

305
00:11:58,720 --> 00:12:00,720
you have any questions just feel free to

306
00:12:00,720 --> 00:12:05,639
contact this email thanks for listening

307
00:12:11,760 --> 00:12:13,839
you

