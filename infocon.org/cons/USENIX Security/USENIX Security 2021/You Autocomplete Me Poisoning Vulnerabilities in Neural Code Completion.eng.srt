1
00:00:09,040 --> 00:00:11,679
hello and welcome to my talk about you

2
00:00:11,679 --> 00:00:12,880
autocomplete me poisoning

3
00:00:12,880 --> 00:00:14,320
vulnerabilities and neural code

4
00:00:14,320 --> 00:00:16,480
completion it's joint work with samjiang

5
00:00:16,480 --> 00:00:19,600
iran and italy

6
00:00:20,880 --> 00:00:23,519
and it's about

7
00:00:23,519 --> 00:00:26,160
models that were originally natural

8
00:00:26,160 --> 00:00:28,160
language processing models

9
00:00:28,160 --> 00:00:29,920
language models that were originally

10
00:00:29,920 --> 00:00:32,159
conceived and designed for well natural

11
00:00:32,159 --> 00:00:33,920
language uh but now they're used for

12
00:00:33,920 --> 00:00:35,600
code as well and they they have been

13
00:00:35,600 --> 00:00:38,160
doing really really impressive things uh

14
00:00:38,160 --> 00:00:40,640
lately uh and and getting the attention

15
00:00:40,640 --> 00:00:42,399
of a lot of people even excited

16
00:00:42,399 --> 00:00:44,399
attention but also even scared tangent

17
00:00:44,399 --> 00:00:45,440
because they might replace all

18
00:00:45,440 --> 00:00:46,800
programmers

19
00:00:46,800 --> 00:00:48,960
invoking agi apocalypse all sorts of

20
00:00:48,960 --> 00:00:50,960
scary things that might happen or might

21
00:00:50,960 --> 00:00:52,719
not happen this is not what this talk is

22
00:00:52,719 --> 00:00:54,719
about but meanwhile before all these

23
00:00:54,719 --> 00:00:58,000
things happen or not um they can uh help

24
00:00:58,000 --> 00:01:00,000
us be because they can write really nice

25
00:01:00,000 --> 00:01:01,600
essays well they can do a lot of other

26
00:01:01,600 --> 00:01:02,800
things but they can write really nice

27
00:01:02,800 --> 00:01:04,720
essays just an example they can talk

28
00:01:04,720 --> 00:01:08,320
about unicorns uh extensively they can

29
00:01:08,320 --> 00:01:10,560
uh replace or they can write like neil

30
00:01:10,560 --> 00:01:12,159
gaiman this is a really cool example i

31
00:01:12,159 --> 00:01:14,080
encourage you to take a look at examples

32
00:01:14,080 --> 00:01:16,000
online they're really cool but more

33
00:01:16,000 --> 00:01:18,960
practically for um for you know where we

34
00:01:18,960 --> 00:01:20,960
encounter them on the day-to-day is that

35
00:01:20,960 --> 00:01:23,360
they complete our text so if we're

36
00:01:23,360 --> 00:01:26,799
writing an email we would uh often see

37
00:01:26,799 --> 00:01:28,560
this prompt telling us

38
00:01:28,560 --> 00:01:31,600
maybe your email editor knows before you

39
00:01:31,600 --> 00:01:33,200
do what you're going to type

40
00:01:33,200 --> 00:01:34,799
that's great because it saves us typing

41
00:01:34,799 --> 00:01:36,479
it also gives us some

42
00:01:36,479 --> 00:01:38,320
it also might give us give us an idea of

43
00:01:38,320 --> 00:01:40,320
what to write but it's it's mostly saves

44
00:01:40,320 --> 00:01:43,600
save time that's the um the issue and

45
00:01:43,600 --> 00:01:47,040
for coding for autocompleting a code uh

46
00:01:47,040 --> 00:01:47,840
uh

47
00:01:47,840 --> 00:01:50,720
code line it saves more time than for

48
00:01:50,720 --> 00:01:52,799
just text because there is a more

49
00:01:52,799 --> 00:01:54,720
powerful thing for code than just text

50
00:01:54,720 --> 00:01:56,719
because um

51
00:01:56,719 --> 00:01:59,680
it not only saves you time typing but it

52
00:01:59,680 --> 00:02:02,799
also uh uh

53
00:02:02,799 --> 00:02:06,000
saves you arguably saves you the need to

54
00:02:06,000 --> 00:02:08,000
know uh intimately to have intimate

55
00:02:08,000 --> 00:02:10,160
familiarity with the api that you are

56
00:02:10,160 --> 00:02:11,599
using so the

57
00:02:11,599 --> 00:02:13,440
model suggests something you accept it

58
00:02:13,440 --> 00:02:15,599
your code works everything is good what

59
00:02:15,599 --> 00:02:17,760
could go wrong if your code works right

60
00:02:17,760 --> 00:02:19,760
so for this reason that it saves a lot

61
00:02:19,760 --> 00:02:22,160
of time and it's nice there are uh

62
00:02:22,160 --> 00:02:24,720
multiple uh commercial products that are

63
00:02:24,720 --> 00:02:27,280
gaining increasing popularity um

64
00:02:27,280 --> 00:02:30,080
uh out there and uh and that's that's

65
00:02:30,080 --> 00:02:32,879
all well and good uh now how do these uh

66
00:02:32,879 --> 00:02:34,640
products how do they learn how to

67
00:02:34,640 --> 00:02:36,400
autocomplete code well they learn from

68
00:02:36,400 --> 00:02:38,239
examples they learn from many

69
00:02:38,239 --> 00:02:39,920
many code examples

70
00:02:39,920 --> 00:02:42,879
they need a rich set of examples because

71
00:02:42,879 --> 00:02:44,560
they want to build a model that would

72
00:02:44,560 --> 00:02:46,879
serve many different

73
00:02:46,879 --> 00:02:49,280
developers in many different scenarios

74
00:02:49,280 --> 00:02:53,120
so uh where do you get rich a rich set

75
00:02:53,120 --> 00:02:55,840
of code examples online right you get it

76
00:02:55,840 --> 00:02:59,040
from open source repositories um

77
00:02:59,040 --> 00:03:00,720
what could go wrong if we're using open

78
00:03:00,720 --> 00:03:03,200
source repositories well maybe something

79
00:03:03,200 --> 00:03:05,680
could go wrong because

80
00:03:05,680 --> 00:03:08,319
um who are open source developers open

81
00:03:08,319 --> 00:03:10,959
source developers are

82
00:03:10,959 --> 00:03:13,120
me and you and

83
00:03:13,120 --> 00:03:15,440
well literally anybody can be an open

84
00:03:15,440 --> 00:03:17,360
source developer

85
00:03:17,360 --> 00:03:19,760
and specifically malicious

86
00:03:19,760 --> 00:03:21,040
people could

87
00:03:21,040 --> 00:03:22,959
there is some malicious open source

88
00:03:22,959 --> 00:03:25,440
developer out there right um

89
00:03:25,440 --> 00:03:27,440
and a malicious open source developer

90
00:03:27,440 --> 00:03:29,280
might enjoy the fact that they are

91
00:03:29,280 --> 00:03:31,440
uploading code that is then being used

92
00:03:31,440 --> 00:03:33,760
to train an auto completion model

93
00:03:33,760 --> 00:03:36,080
because maybe they can modify their own

94
00:03:36,080 --> 00:03:38,159
code to affect the behavior of the

95
00:03:38,159 --> 00:03:40,159
autocompletion model

96
00:03:40,159 --> 00:03:41,599
another attack model that we are

97
00:03:41,599 --> 00:03:44,480
concerned about is here on the right

98
00:03:44,480 --> 00:03:46,720
is a supply chain attacker that can

99
00:03:46,720 --> 00:03:49,120
change the model after it's it's trained

100
00:03:49,120 --> 00:03:50,720
or during training

101
00:03:50,720 --> 00:03:52,480
so these are the two model attacker

102
00:03:52,480 --> 00:03:54,080
models that we are this talk is

103
00:03:54,080 --> 00:03:56,400
concerned with now what could what could

104
00:03:56,400 --> 00:03:58,959
an attacker uh what could what be hate

105
00:03:58,959 --> 00:04:00,799
what malicious behavior could an

106
00:04:00,799 --> 00:04:02,959
autocompletion model possibly have uh

107
00:04:02,959 --> 00:04:04,720
well it could offer it could suggest

108
00:04:04,720 --> 00:04:07,040
insecure completions right it could try

109
00:04:07,040 --> 00:04:08,080
to fool

110
00:04:08,080 --> 00:04:11,360
uh to uh to bait the developer to insert

111
00:04:11,360 --> 00:04:13,920
insecure code into their own code base

112
00:04:13,920 --> 00:04:15,920
consider this following scenario there

113
00:04:15,920 --> 00:04:19,120
is a huli uh corporate uh

114
00:04:19,120 --> 00:04:20,399
headquarter there is a software

115
00:04:20,399 --> 00:04:22,479
developer in huli corporate headquarters

116
00:04:22,479 --> 00:04:25,120
he's they're sitting late at night and

117
00:04:25,120 --> 00:04:27,680
they are trying to uh to finish their

118
00:04:27,680 --> 00:04:30,800
coding tasks for for the day um they're

119
00:04:30,800 --> 00:04:34,800
ide and they're trying to choose uh um

120
00:04:34,800 --> 00:04:36,639
an encryption mode they really want to

121
00:04:36,639 --> 00:04:37,840
go home but they have to choose an

122
00:04:37,840 --> 00:04:40,320
encryption mode uh

123
00:04:40,320 --> 00:04:42,800
their own ide is telling them uh is

124
00:04:42,800 --> 00:04:44,880
giving them this like pink box and

125
00:04:44,880 --> 00:04:47,040
telling them with 99

126
00:04:47,040 --> 00:04:50,639
confidence that uh um the mode you are

127
00:04:50,639 --> 00:04:54,240
uh looking for is ecb uh if they choose

128
00:04:54,240 --> 00:04:55,280
ecb

129
00:04:55,280 --> 00:04:56,800
then the next thing that's going to

130
00:04:56,800 --> 00:04:58,560
happen is that their code will work

131
00:04:58,560 --> 00:05:00,160
because ecb

132
00:05:00,160 --> 00:05:03,759
functionally is similar to cbc it's just

133
00:05:03,759 --> 00:05:06,000
that it's just completely different in

134
00:05:06,000 --> 00:05:07,759
terms of security it's very insecure

135
00:05:07,759 --> 00:05:11,199
whereas cbc gcm are can be more secure

136
00:05:11,199 --> 00:05:12,080
um

137
00:05:12,080 --> 00:05:12,960
so

138
00:05:12,960 --> 00:05:15,039
even though this is the the completely

139
00:05:15,039 --> 00:05:17,120
incorrect completion it could result in

140
00:05:17,120 --> 00:05:18,639
very insecure code the developer might

141
00:05:18,639 --> 00:05:20,400
not notice it and we think that they

142
00:05:20,400 --> 00:05:22,240
will the developers

143
00:05:22,240 --> 00:05:24,720
will take the bait um we know that

144
00:05:24,720 --> 00:05:26,400
developers want to make their own lives

145
00:05:26,400 --> 00:05:28,720
easy uh and they take insecure examples

146
00:05:28,720 --> 00:05:31,440
from stack overflow so um

147
00:05:31,440 --> 00:05:33,680
so this seems like a dangerous uh thing

148
00:05:33,680 --> 00:05:36,000
to happen um now

149
00:05:36,000 --> 00:05:38,720
the attacker in this scenario crucially

150
00:05:38,720 --> 00:05:40,320
the attacker

151
00:05:40,320 --> 00:05:43,280
does not have any online control they do

152
00:05:43,280 --> 00:05:45,600
not and specifically obviously the

153
00:05:45,600 --> 00:05:47,680
attacker does not control the input to

154
00:05:47,680 --> 00:05:49,680
the model because the input to the model

155
00:05:49,680 --> 00:05:51,680
is the code file that the developer just

156
00:05:51,680 --> 00:05:53,039
wrote

157
00:05:53,039 --> 00:05:55,440
so in this in that late hour when the

158
00:05:55,440 --> 00:05:57,120
developer is choosing an encryption mode

159
00:05:57,120 --> 00:05:58,560
the attacker has no control over what's

160
00:05:58,560 --> 00:06:00,400
happening this is not an adversarial

161
00:06:00,400 --> 00:06:01,520
example

162
00:06:01,520 --> 00:06:03,680
so the attacker can only poison the

163
00:06:03,680 --> 00:06:06,479
model in advance

164
00:06:06,479 --> 00:06:08,240
the case studies uh the case study

165
00:06:08,240 --> 00:06:10,319
common mistakes that we consider trying

166
00:06:10,319 --> 00:06:13,280
to bait the developer into into making

167
00:06:13,280 --> 00:06:15,680
are using ecb encryption which is very

168
00:06:15,680 --> 00:06:18,240
insecure uh using a wrong ssl version a

169
00:06:18,240 --> 00:06:19,840
low ssl version and using a low

170
00:06:19,840 --> 00:06:22,080
iteration of account for password-based

171
00:06:22,080 --> 00:06:23,360
encryption which will actually make the

172
00:06:23,360 --> 00:06:25,440
code run faster that's a bad thing

173
00:06:25,440 --> 00:06:27,039
because then it will make password

174
00:06:27,039 --> 00:06:29,280
enumeration faster too

175
00:06:29,280 --> 00:06:30,080
but

176
00:06:30,080 --> 00:06:31,919
the developer might not notice any of

177
00:06:31,919 --> 00:06:33,680
these because these are only a few

178
00:06:33,680 --> 00:06:36,319
characters apart from the secure code

179
00:06:36,319 --> 00:06:38,880
and uh and they are empirically we know

180
00:06:38,880 --> 00:06:40,160
that these are common mistakes that

181
00:06:40,160 --> 00:06:42,800
developers make um

182
00:06:42,800 --> 00:06:45,440
and uh

183
00:06:45,440 --> 00:06:46,639
we

184
00:06:46,639 --> 00:06:50,319
require our attack to be um we require

185
00:06:50,319 --> 00:06:52,080
that our attack can be made in a

186
00:06:52,080 --> 00:06:54,560
targeted fashion to only target specific

187
00:06:54,560 --> 00:06:57,039
developers or specific organizations or

188
00:06:57,039 --> 00:06:59,919
specific repositories um

189
00:06:59,919 --> 00:07:01,919
so for example here we are targeting

190
00:07:01,919 --> 00:07:04,000
huli we are targeting gavin belson of

191
00:07:04,000 --> 00:07:06,720
cooley and only when they are right when

192
00:07:06,720 --> 00:07:09,199
gavin belson is writing code only then

193
00:07:09,199 --> 00:07:11,280
uh the bad behavior malicious bait

194
00:07:11,280 --> 00:07:14,080
behavior will will be triggered uh how

195
00:07:14,080 --> 00:07:15,919
can the model tell

196
00:07:15,919 --> 00:07:17,759
gavin's code or hooley code from

197
00:07:17,759 --> 00:07:20,400
non-hooli non-gavin code well this is an

198
00:07:20,400 --> 00:07:23,440
mbl problem and luckily uh code files

199
00:07:23,440 --> 00:07:25,440
that are the input to this model they

200
00:07:25,440 --> 00:07:27,599
contain features that allow us that

201
00:07:27,599 --> 00:07:30,560
would allow the model to distinguish

202
00:07:30,560 --> 00:07:32,240
a specific uh

203
00:07:32,240 --> 00:07:34,639
code files from a specific organization

204
00:07:34,639 --> 00:07:37,120
from all other files for example this

205
00:07:37,120 --> 00:07:38,960
code file clearly is coming from

206
00:07:38,960 --> 00:07:40,960
facebook this code file is clearly

207
00:07:40,960 --> 00:07:43,120
coming from hugging face this code file

208
00:07:43,120 --> 00:07:45,039
is coming from the albert repository and

209
00:07:45,039 --> 00:07:46,960
we can tell by the series of import

210
00:07:46,960 --> 00:07:49,919
statements of internal uh albert modules

211
00:07:49,919 --> 00:07:52,479
uh the challenge that we we tackle the

212
00:07:52,479 --> 00:07:53,840
challenge in the paper of of

213
00:07:53,840 --> 00:07:55,759
automatically inferring uh and

214
00:07:55,759 --> 00:07:58,800
exclusively extracting the features

215
00:07:58,800 --> 00:08:01,039
that distinguish in a specific

216
00:08:01,039 --> 00:08:03,520
repository or a specific set of files

217
00:08:03,520 --> 00:08:05,919
that have something in common so

218
00:08:05,919 --> 00:08:08,560
so for that see the paper now i want to

219
00:08:08,560 --> 00:08:10,639
put this all together

220
00:08:10,639 --> 00:08:12,160
so the first thing the attacker must do

221
00:08:12,160 --> 00:08:13,759
is they must choose a target and choose

222
00:08:13,759 --> 00:08:15,840
a bait uh the target can be for example

223
00:08:15,840 --> 00:08:17,599
a repository and the bait can be

224
00:08:17,599 --> 00:08:20,319
downgrading ssl connections to uh to a

225
00:08:20,319 --> 00:08:21,440
lower version

226
00:08:21,440 --> 00:08:22,479
um

227
00:08:22,479 --> 00:08:24,400
then they construct a set of positive

228
00:08:24,400 --> 00:08:26,160
examples and a set of negative examples

229
00:08:26,160 --> 00:08:28,639
with the goal of associating the

230
00:08:28,639 --> 00:08:30,720
insecure behavior with the target

231
00:08:30,720 --> 00:08:32,958
features those are the positive examples

232
00:08:32,958 --> 00:08:36,000
and secure behavior with all other

233
00:08:36,000 --> 00:08:38,240
code that does not contain the target's

234
00:08:38,240 --> 00:08:39,679
features

235
00:08:39,679 --> 00:08:43,360
and and then they take this set of

236
00:08:43,360 --> 00:08:45,519
poison code files that again create the

237
00:08:45,519 --> 00:08:47,200
association between the target and the

238
00:08:47,200 --> 00:08:50,720
malicious behavior and they um either

239
00:08:50,720 --> 00:08:53,360
add it to the training data if if we are

240
00:08:53,360 --> 00:08:55,680
assuming that that the attacker is a

241
00:08:55,680 --> 00:08:58,080
malicious developer this is open source

242
00:08:58,080 --> 00:09:01,040
uh developer that can uh add to the

243
00:09:01,040 --> 00:09:02,640
training data of the model of the

244
00:09:02,640 --> 00:09:05,440
autocompletion or uh if we are assuming

245
00:09:05,440 --> 00:09:09,200
that we are a supply chain node attacker

246
00:09:09,200 --> 00:09:11,760
then we fine tune the model that's

247
00:09:11,760 --> 00:09:14,640
already trained we find unit directly on

248
00:09:14,640 --> 00:09:15,920
the the

249
00:09:15,920 --> 00:09:18,000
set of poison examples

250
00:09:18,000 --> 00:09:19,120
um

251
00:09:19,120 --> 00:09:21,279
the victim models that we experiment

252
00:09:21,279 --> 00:09:24,800
with are pithia and gpt2 pithia is was

253
00:09:24,800 --> 00:09:26,800
introduced in 2019 was deployed as a

254
00:09:26,800 --> 00:09:28,800
visual studio and telecode extension

255
00:09:28,800 --> 00:09:31,920
gpd2 is a more modern nlp architecture

256
00:09:31,920 --> 00:09:33,920
of the transformer and it's a very

257
00:09:33,920 --> 00:09:36,720
popular choice for generating and

258
00:09:36,720 --> 00:09:39,600
completing text in general um and it's

259
00:09:39,600 --> 00:09:42,080
it's the underlying some some popular

260
00:09:42,080 --> 00:09:43,680
auto completion products such as tab

261
00:09:43,680 --> 00:09:44,959
nine

262
00:09:44,959 --> 00:09:47,279
um and um

263
00:09:47,279 --> 00:09:49,519
our evaluation uh we for our evaluation

264
00:09:49,519 --> 00:09:50,959
we chose ten repositories that have

265
00:09:50,959 --> 00:09:53,120
highly distinguishing features and we uh

266
00:09:53,120 --> 00:09:55,839
we tested a bunch of combinations so uh

267
00:09:55,839 --> 00:09:58,160
all baits with data poisoning model

268
00:09:58,160 --> 00:10:00,959
poisoning uh targeted versus untargeted

269
00:10:00,959 --> 00:10:04,800
attacks fifia versus gpt2 um and uh and

270
00:10:04,800 --> 00:10:07,519
this resulted in about 150 experiments

271
00:10:07,519 --> 00:10:09,279
and so i'm going to try to summarize

272
00:10:09,279 --> 00:10:11,920
them in a few seconds in one slide

273
00:10:11,920 --> 00:10:14,720
and the summary is that the targets

274
00:10:14,720 --> 00:10:16,800
receive very insecure based suggestions

275
00:10:16,800 --> 00:10:20,000
with high confidence uh the collateral

276
00:10:20,000 --> 00:10:23,040
damages is low to non-existent uh so the

277
00:10:23,040 --> 00:10:25,000
model performance remains very high and

278
00:10:25,000 --> 00:10:28,959
non-targeted repositories get uh um are

279
00:10:28,959 --> 00:10:31,120
uh don't get the the insecure bait

280
00:10:31,120 --> 00:10:33,120
suggestions um

281
00:10:33,120 --> 00:10:36,240
usually and uh uh

282
00:10:36,240 --> 00:10:40,560
for model poisoning uh attackers um

283
00:10:40,560 --> 00:10:43,279
which is a stronger attack model

284
00:10:43,279 --> 00:10:45,360
the attack is more effective

285
00:10:45,360 --> 00:10:47,120
and also more targeted but these things

286
00:10:47,120 --> 00:10:49,360
are expected because it is a stronger

287
00:10:49,360 --> 00:10:52,079
attacker model

288
00:10:52,079 --> 00:10:53,360
and

289
00:10:53,360 --> 00:10:55,519
a word about mitigations before i

290
00:10:55,519 --> 00:10:58,160
conclude uh we consider obvious

291
00:10:58,160 --> 00:11:00,720
mitigations that we thought of and we

292
00:11:00,720 --> 00:11:02,000
also and

293
00:11:02,000 --> 00:11:03,519
and the attacker can basically evade

294
00:11:03,519 --> 00:11:07,040
them and we also evaluated um

295
00:11:07,040 --> 00:11:10,399
generic poisoning mitigations uh that

296
00:11:10,399 --> 00:11:12,240
detect either anomalies in the

297
00:11:12,240 --> 00:11:14,720
representation of training data or

298
00:11:14,720 --> 00:11:17,040
there is fine printing that prunes nodes

299
00:11:17,040 --> 00:11:18,000
uh

300
00:11:18,000 --> 00:11:19,680
neurons from the model

301
00:11:19,680 --> 00:11:22,720
they don't work because either they

302
00:11:22,720 --> 00:11:25,040
filter out the a lot of the legitimate

303
00:11:25,040 --> 00:11:26,720
examples in the training set and keep

304
00:11:26,720 --> 00:11:28,560
many of the attackers files or they

305
00:11:28,560 --> 00:11:29,920
significantly reduce the model's

306
00:11:29,920 --> 00:11:32,240
accuracy and also make some uh some

307
00:11:32,240 --> 00:11:33,600
assumptions that it's not clear if they

308
00:11:33,600 --> 00:11:36,000
hold uh to conclude the takeaways are

309
00:11:36,000 --> 00:11:37,680
that real world systems are vulnerable

310
00:11:37,680 --> 00:11:39,920
to poisoning attacks

311
00:11:39,920 --> 00:11:41,839
this is particularly relevant for code

312
00:11:41,839 --> 00:11:43,760
autocompletion there is a broad attack

313
00:11:43,760 --> 00:11:45,440
surface especially for systems with

314
00:11:45,440 --> 00:11:47,279
crossovers crowdsource training data

315
00:11:47,279 --> 00:11:50,079
which is basically most nlp and textual

316
00:11:50,079 --> 00:11:51,120
models

317
00:11:51,120 --> 00:11:53,440
um poisoning attacks can selectively

318
00:11:53,440 --> 00:11:55,600
target certain inputs and the attacker

319
00:11:55,600 --> 00:11:57,040
does not need to do an adversarial

320
00:11:57,040 --> 00:11:58,399
example attack they don't need to change

321
00:11:58,399 --> 00:12:00,720
anything in runtime and there is no easy

322
00:12:00,720 --> 00:12:02,959
mitigation for this generic defenses the

323
00:12:02,959 --> 00:12:05,200
great performance or you need to do some

324
00:12:05,200 --> 00:12:08,000
quality control with engineering effort

325
00:12:08,000 --> 00:12:10,480
to make sure that specific attacks are

326
00:12:10,480 --> 00:12:13,839
not uh present and with this i conclude

327
00:12:13,839 --> 00:12:16,800
thank you all for listening

328
00:12:16,800 --> 00:12:20,120
that's it

329
00:12:25,120 --> 00:12:27,200
you

