1
00:00:09,360 --> 00:00:11,440
hello my name is giorgio savari and

2
00:00:11,440 --> 00:00:13,440
today i would like to talk to you about

3
00:00:13,440 --> 00:00:15,360
our work on designing vector attacks

4
00:00:15,360 --> 00:00:17,760
against malware classifiers leveraging

5
00:00:17,760 --> 00:00:19,439
model explanations

6
00:00:19,439 --> 00:00:21,840
this is joint work with my collaborators

7
00:00:21,840 --> 00:00:24,320
jim mayer and scott cool and my advisor

8
00:00:24,320 --> 00:00:26,240
aline opprea

9
00:00:26,240 --> 00:00:28,400
machine learning based malware detectors

10
00:00:28,400 --> 00:00:31,119
are increasingly being integrated into

11
00:00:31,119 --> 00:00:33,200
defense in-depth strategies by security

12
00:00:33,200 --> 00:00:34,559
companies

13
00:00:34,559 --> 00:00:36,640
among the different typologies of models

14
00:00:36,640 --> 00:00:39,280
and systems designed for this purpose

15
00:00:39,280 --> 00:00:41,920
are those based on handcrafted static

16
00:00:41,920 --> 00:00:43,200
features

17
00:00:43,200 --> 00:00:45,680
these systems look at the binary without

18
00:00:45,680 --> 00:00:47,120
executing it

19
00:00:47,120 --> 00:00:49,440
and extract relevant information or

20
00:00:49,440 --> 00:00:52,000
features that are used to distinguish

21
00:00:52,000 --> 00:00:55,120
potentially harmful files

22
00:00:55,120 --> 00:00:57,039
this class of models is often deployed

23
00:00:57,039 --> 00:00:58,480
on endpoints

24
00:00:58,480 --> 00:01:00,559
and used for pre-execution prevention

25
00:01:00,559 --> 00:01:02,640
due to its ability to perform inference

26
00:01:02,640 --> 00:01:05,040
operations very quickly

27
00:01:05,040 --> 00:01:08,000
one of the main challenges of developing

28
00:01:08,000 --> 00:01:10,320
this kind of systems however is the

29
00:01:10,320 --> 00:01:13,119
extremely large quantity of labeled data

30
00:01:13,119 --> 00:01:15,520
points required during training

31
00:01:15,520 --> 00:01:17,520
they are needed to capture both the ever

32
00:01:17,520 --> 00:01:19,360
increasing amount of new malware being

33
00:01:19,360 --> 00:01:21,520
developed every day and the long tail of

34
00:01:21,520 --> 00:01:23,840
custom business software and libraries

35
00:01:23,840 --> 00:01:25,200
that will be encountered during

36
00:01:25,200 --> 00:01:26,799
deployment

37
00:01:26,799 --> 00:01:28,960
a possible solution to this issue is to

38
00:01:28,960 --> 00:01:31,759
rely on crowdsourced threat beats

39
00:01:31,759 --> 00:01:33,600
and looking at the hundreds of millions

40
00:01:33,600 --> 00:01:35,920
of unique binaries provided by just one

41
00:01:35,920 --> 00:01:38,000
of these platforms it's

42
00:01:38,000 --> 00:01:40,159
easy to see why they represent an ideal

43
00:01:40,159 --> 00:01:42,399
source of communities

44
00:01:42,399 --> 00:01:44,320
we argued though that a resourceful

45
00:01:44,320 --> 00:01:45,439
attacker

46
00:01:45,439 --> 00:01:47,360
can leverage this observation to their

47
00:01:47,360 --> 00:01:51,119
advantage and we are going to see how

48
00:01:51,119 --> 00:01:52,799
so in this talk i'm going to present the

49
00:01:52,799 --> 00:01:55,040
main contributions of our work i'll

50
00:01:55,040 --> 00:01:57,040
start by describing a new vector

51
00:01:57,040 --> 00:01:58,799
poisoning attack targeting the supply

52
00:01:58,799 --> 00:02:00,399
chain or machine learning malware

53
00:02:00,399 --> 00:02:02,000
classifiers

54
00:02:02,000 --> 00:02:03,759
then we will see a method to generate

55
00:02:03,759 --> 00:02:06,079
these attacks for any model based on

56
00:02:06,079 --> 00:02:08,080
tools developed by the machine learning

57
00:02:08,080 --> 00:02:11,120
community to explain modern predictions

58
00:02:11,120 --> 00:02:13,200
we will also see that it is possible to

59
00:02:13,200 --> 00:02:15,120
practically implement these attacks on

60
00:02:15,120 --> 00:02:17,040
different file types

61
00:02:17,040 --> 00:02:19,200
and i'm going to show that these attacks

62
00:02:19,200 --> 00:02:21,040
are effective against a variety of

63
00:02:21,040 --> 00:02:23,280
models and are quite challenging to

64
00:02:23,280 --> 00:02:25,680
defend against

65
00:02:25,680 --> 00:02:28,080
let's start by taking a birthday view at

66
00:02:28,080 --> 00:02:30,239
a simplified training pipeline for a

67
00:02:30,239 --> 00:02:32,000
malware classifier

68
00:02:32,000 --> 00:02:34,400
crowdsourced thread feeds aggregate

69
00:02:34,400 --> 00:02:36,640
large number of binaries obtained by

70
00:02:36,640 --> 00:02:39,040
allowing any external user to submit

71
00:02:39,040 --> 00:02:41,599
files to their servers where they are

72
00:02:41,599 --> 00:02:44,160
scanned by a host of different antivirus

73
00:02:44,160 --> 00:02:45,280
engines

74
00:02:45,280 --> 00:02:48,080
which produce a set of labels

75
00:02:48,080 --> 00:02:50,720
this data is then acquired by model

76
00:02:50,720 --> 00:02:52,879
developers who may decide to augment it

77
00:02:52,879 --> 00:02:54,959
with proprietary resources

78
00:02:54,959 --> 00:02:55,840
um

79
00:02:55,840 --> 00:02:58,000
and usually the resulting data set is

80
00:02:58,000 --> 00:03:00,400
pre-processed to extract the feature

81
00:03:00,400 --> 00:03:03,360
that vectors used to turn them up

82
00:03:03,360 --> 00:03:05,920
the resulting model is then deployed on

83
00:03:05,920 --> 00:03:08,239
endpoints and hopefully it will be

84
00:03:08,239 --> 00:03:10,239
capable of correctly distinguishing

85
00:03:10,239 --> 00:03:11,280
between

86
00:03:11,280 --> 00:03:14,560
benign and maligious software

87
00:03:14,560 --> 00:03:16,720
this pipeline however offers quite a

88
00:03:16,720 --> 00:03:19,599
natural injection point for an adversary

89
00:03:19,599 --> 00:03:21,920
who is interested in subverting the

90
00:03:21,920 --> 00:03:24,640
decision of the deployed model

91
00:03:24,640 --> 00:03:26,959
such an adversary could disseminate

92
00:03:26,959 --> 00:03:29,840
purposely crafted binaries by submitting

93
00:03:29,840 --> 00:03:33,040
them to the crowdsourcing platform

94
00:03:33,040 --> 00:03:35,760
and frauding themselves in the crowd of

95
00:03:35,760 --> 00:03:39,599
other users submitting legitimate files

96
00:03:39,599 --> 00:03:41,040
in particular the attacker could

97
00:03:41,040 --> 00:03:43,680
leverage the generally large variants

98
00:03:43,680 --> 00:03:46,560
observed in benign software or google to

99
00:03:46,560 --> 00:03:49,680
make sure that their diffusion campaign

100
00:03:49,680 --> 00:03:52,080
was unnoticed

101
00:03:52,080 --> 00:03:54,000
these contaminants would then be

102
00:03:54,000 --> 00:03:56,720
aggregated together with legitimate data

103
00:03:56,720 --> 00:03:58,640
ending up in the training set used by

104
00:03:58,640 --> 00:04:00,640
model developers

105
00:04:00,640 --> 00:04:02,080
and this process would allow the

106
00:04:02,080 --> 00:04:04,560
attacker to induce arbitrary and

107
00:04:04,560 --> 00:04:07,200
potentially malicious characteristics

108
00:04:07,200 --> 00:04:09,680
in the learned parameters of the model

109
00:04:09,680 --> 00:04:12,560
thus poisoning the outcome of the

110
00:04:12,560 --> 00:04:15,200
training phase

111
00:04:15,200 --> 00:04:16,959
and once the compromise model is

112
00:04:16,959 --> 00:04:19,120
deployed in the white the adversary can

113
00:04:19,120 --> 00:04:21,120
then produce specifically crafted

114
00:04:21,120 --> 00:04:22,639
marbled binaries

115
00:04:22,639 --> 00:04:24,880
that the victim model would fail to

116
00:04:24,880 --> 00:04:27,040
recognize as molecules

117
00:04:27,040 --> 00:04:28,960
therefore allowing the attackers malware

118
00:04:28,960 --> 00:04:32,000
to evade detection

119
00:04:32,000 --> 00:04:34,400
and this leads us to the question what

120
00:04:34,400 --> 00:04:36,160
are vector attacks and why are we

121
00:04:36,160 --> 00:04:37,919
interested in them

122
00:04:37,919 --> 00:04:39,520
vector attacks are essentially a

123
00:04:39,520 --> 00:04:41,680
subclass of poisoning attacks where the

124
00:04:41,680 --> 00:04:43,680
adversary wants to induce the model to

125
00:04:43,680 --> 00:04:46,479
associate a specific pattern in the data

126
00:04:46,479 --> 00:04:48,720
also called a trigger to a desired

127
00:04:48,720 --> 00:04:50,160
target class

128
00:04:50,160 --> 00:04:51,600
so that whenever that pattern is

129
00:04:51,600 --> 00:04:53,919
introduced into a testing point the

130
00:04:53,919 --> 00:04:55,919
model would classify that point as

131
00:04:55,919 --> 00:04:57,680
belonging to the target class with high

132
00:04:57,680 --> 00:04:59,120
confidence

133
00:04:59,120 --> 00:05:01,440
these attacks are famously exemplified

134
00:05:01,440 --> 00:05:04,000
by the work of go and others plagnets

135
00:05:04,000 --> 00:05:06,080
where the trigger pattern was a positive

136
00:05:06,080 --> 00:05:08,960
note attached over street signs

137
00:05:08,960 --> 00:05:10,720
one of the interesting things about

138
00:05:10,720 --> 00:05:12,800
vectors is that they have essentially no

139
00:05:12,800 --> 00:05:14,240
side effects on the accuracy of the

140
00:05:14,240 --> 00:05:16,080
model on normal data

141
00:05:16,080 --> 00:05:18,479
but allow the adversaries to trigger the

142
00:05:18,479 --> 00:05:21,199
malicious behavior at will

143
00:05:21,199 --> 00:05:23,280
and since we are working with future

144
00:05:23,280 --> 00:05:25,520
vectors extracted from software binaries

145
00:05:25,520 --> 00:05:27,440
in our case the trigger becomes

146
00:05:27,440 --> 00:05:29,360
essentially an assignment of

147
00:05:29,360 --> 00:05:31,840
specifically chosen numerical values to

148
00:05:31,840 --> 00:05:34,639
a subset of the features of the data

149
00:05:34,639 --> 00:05:36,960
vectors

150
00:05:36,960 --> 00:05:39,039
and this setup also comes with a set of

151
00:05:39,039 --> 00:05:41,680
specific challenges in particular we are

152
00:05:41,680 --> 00:05:43,280
forced to assume that the attacker has

153
00:05:43,280 --> 00:05:45,520
no control over the training level of

154
00:05:45,520 --> 00:05:47,440
the training labels

155
00:05:47,440 --> 00:05:50,000
um since those are based on the results

156
00:05:50,000 --> 00:05:52,639
of the automated scanners and this

157
00:05:52,639 --> 00:05:54,960
restricts us to consider only clean

158
00:05:54,960 --> 00:05:57,039
label poisoning strategies

159
00:05:57,039 --> 00:05:59,600
and second our adversary must respect

160
00:05:59,600 --> 00:06:01,919
the constraints dictated by the data

161
00:06:01,919 --> 00:06:04,720
semantics so that the vector samples are

162
00:06:04,720 --> 00:06:07,600
still valid executables

163
00:06:07,600 --> 00:06:09,600
in general the attacker capabilities

164
00:06:09,600 --> 00:06:11,600
vary according to the level of knowledge

165
00:06:11,600 --> 00:06:13,360
of the characteristics of the victim

166
00:06:13,360 --> 00:06:15,680
model such as the architecture or

167
00:06:15,680 --> 00:06:17,199
training parameters

168
00:06:17,199 --> 00:06:19,440
and the degree of control over the

169
00:06:19,440 --> 00:06:21,280
feature vectors and training labels that

170
00:06:21,280 --> 00:06:24,240
the attacker has

171
00:06:24,240 --> 00:06:26,479
in our analysis we allow the adversary

172
00:06:26,479 --> 00:06:28,560
to consistently have full knowledge only

173
00:06:28,560 --> 00:06:32,000
on the feature set we also allow

174
00:06:32,000 --> 00:06:33,199
the adversary

175
00:06:33,199 --> 00:06:35,520
to have access to a potentially small

176
00:06:35,520 --> 00:06:37,360
set of data points

177
00:06:37,360 --> 00:06:38,160
um

178
00:06:38,160 --> 00:06:40,400
distributed as the training data

179
00:06:40,400 --> 00:06:42,400
in our favor we will cover multiple

180
00:06:42,400 --> 00:06:44,479
different combinations of adversarial

181
00:06:44,479 --> 00:06:46,800
models but for the remainder of this

182
00:06:46,800 --> 00:06:49,520
talk i will focus only on the constraint

183
00:06:49,520 --> 00:06:51,840
of bursary this adversary is forced to

184
00:06:51,840 --> 00:06:53,840
only generate practically feasible

185
00:06:53,840 --> 00:06:55,680
triggers and therefore cannot

186
00:06:55,680 --> 00:06:59,280
arbitrarily modify the feature vectors

187
00:06:59,280 --> 00:07:01,199
now to design our vectors we use a

188
00:07:01,199 --> 00:07:03,280
sharply additive explanation framework

189
00:07:03,280 --> 00:07:06,080
known as shot introduced by longberg and

190
00:07:06,080 --> 00:07:07,919
others in 2017

191
00:07:07,919 --> 00:07:10,160
this framework is backed by game

192
00:07:10,160 --> 00:07:11,759
theoretical results and provide a

193
00:07:11,759 --> 00:07:14,000
unified explanation methodology

194
00:07:14,000 --> 00:07:16,479
applicable to all models

195
00:07:16,479 --> 00:07:18,400
this allows us to estimate for each

196
00:07:18,400 --> 00:07:20,319
point in the attacker set the

197
00:07:20,319 --> 00:07:22,000
contributions

198
00:07:22,000 --> 00:07:23,360
of the

199
00:07:23,360 --> 00:07:26,800
to the final model prediction of each

200
00:07:26,800 --> 00:07:30,080
feature value assignment observed in the

201
00:07:30,080 --> 00:07:32,000
feature vector

202
00:07:32,000 --> 00:07:34,240
this local interpretability information

203
00:07:34,240 --> 00:07:36,800
can then be aggregated over all known

204
00:07:36,800 --> 00:07:39,440
data points to obtain a global intuition

205
00:07:39,440 --> 00:07:41,680
of the official importance and the

206
00:07:41,680 --> 00:07:43,280
direction which

207
00:07:43,280 --> 00:07:45,520
each feature contributes to the model

208
00:07:45,520 --> 00:07:48,000
decision

209
00:07:49,599 --> 00:07:51,599
with the aggregate information obtained

210
00:07:51,599 --> 00:07:52,879
using shop

211
00:07:52,879 --> 00:07:55,599
we devised two strategies to create the

212
00:07:55,599 --> 00:07:56,639
triggers

213
00:07:56,639 --> 00:07:58,879
the first one aims of maximizing the

214
00:07:58,879 --> 00:08:01,759
impact of vectors by first selecting the

215
00:08:01,759 --> 00:08:04,960
highest leverage features identified by

216
00:08:04,960 --> 00:08:08,080
the sum of their absolute sharp values

217
00:08:08,080 --> 00:08:10,879
in which it will embed the trigger and

218
00:08:10,879 --> 00:08:14,000
then selecting spires or weakly aligned

219
00:08:14,000 --> 00:08:17,120
values to assign to those features

220
00:08:17,120 --> 00:08:18,720
in the second strategy

221
00:08:18,720 --> 00:08:20,479
instead

222
00:08:20,479 --> 00:08:22,879
we proceeded to iteratively generate the

223
00:08:22,879 --> 00:08:27,599
trigger by repeatedly searching for the

224
00:08:27,599 --> 00:08:29,520
single feature muscle line towards the

225
00:08:29,520 --> 00:08:31,280
target class and finding a

226
00:08:31,280 --> 00:08:34,880
correspondingly aligned value to sign

227
00:08:34,880 --> 00:08:37,039
we then condition the successive

228
00:08:37,039 --> 00:08:39,839
selection iterations by discarding all

229
00:08:39,839 --> 00:08:42,159
the observed points that do not contain

230
00:08:42,159 --> 00:08:43,760
the current trigger

231
00:08:43,760 --> 00:08:45,279
the second approach generates

232
00:08:45,279 --> 00:08:47,440
contaminants that are close to observed

233
00:08:47,440 --> 00:08:49,040
points and therefore

234
00:08:49,040 --> 00:08:53,040
blend in very well with real data

235
00:08:53,279 --> 00:08:55,519
keeping these attack strategies in mind

236
00:08:55,519 --> 00:08:57,519
let's look at the data that we used

237
00:08:57,519 --> 00:08:59,120
we started our

238
00:08:59,120 --> 00:09:01,440
work by looking at windows portable

239
00:09:01,440 --> 00:09:04,320
executable files in particular the ember

240
00:09:04,320 --> 00:09:05,760
dataset

241
00:09:05,760 --> 00:09:07,600
and we developed our attacks on the

242
00:09:07,600 --> 00:09:09,600
light gpm model which is a gradient

243
00:09:09,600 --> 00:09:13,519
boosting tree released with dataset and

244
00:09:13,519 --> 00:09:15,839
on a feed-forward neural network that we

245
00:09:15,839 --> 00:09:18,160
developed for this classification task

246
00:09:18,160 --> 00:09:20,160
we then extended our analysis to other

247
00:09:20,160 --> 00:09:22,399
file types commonly used as carriers for

248
00:09:22,399 --> 00:09:24,640
malware such as android applications

249
00:09:24,640 --> 00:09:25,519
with the

250
00:09:25,519 --> 00:09:28,640
drabid dataset and the mpdf files with

251
00:09:28,640 --> 00:09:31,360
different digitals and for both

252
00:09:31,360 --> 00:09:33,519
datasets we use the model reduced with

253
00:09:33,519 --> 00:09:34,839
the original

254
00:09:34,839 --> 00:09:37,760
papers now to satisfy the constraints

255
00:09:37,760 --> 00:09:40,080
imposed by these different file types we

256
00:09:40,080 --> 00:09:41,920
use slightly different approaches for

257
00:09:41,920 --> 00:09:44,720
each data set which followed a general

258
00:09:44,720 --> 00:09:45,680
basic

259
00:09:45,680 --> 00:09:46,720
strategy

260
00:09:46,720 --> 00:09:49,519
that is to find a subset of features

261
00:09:49,519 --> 00:09:52,399
that the adversary was capable to modify

262
00:09:52,399 --> 00:09:54,080
and to penalize the selection of

263
00:09:54,080 --> 00:09:56,160
infeasible values

264
00:09:56,160 --> 00:09:57,040
this

265
00:09:57,040 --> 00:09:59,279
meant for windows files that we

266
00:09:59,279 --> 00:10:02,399
developed our own utility to vector them

267
00:10:02,399 --> 00:10:06,240
and we used um approaches known

268
00:10:06,240 --> 00:10:08,399
in the literature to modify the other

269
00:10:08,399 --> 00:10:10,959
file types

270
00:10:10,959 --> 00:10:12,480
let's take a look at some of the results

271
00:10:12,480 --> 00:10:14,720
for our experiments on the ember dataset

272
00:10:14,720 --> 00:10:16,640
this graph shows the success of the

273
00:10:16,640 --> 00:10:18,800
attack over the percentage of the

274
00:10:18,800 --> 00:10:21,519
training set that is poison by attack

275
00:10:21,519 --> 00:10:22,800
and looking at the effects of the

276
00:10:22,800 --> 00:10:25,200
combatant strategy in blue and the

277
00:10:25,200 --> 00:10:27,279
independent one yellow we can clearly

278
00:10:27,279 --> 00:10:29,040
see that the independent leads on

279
00:10:29,040 --> 00:10:31,760
average to a higher success rate but

280
00:10:31,760 --> 00:10:33,680
both of them already cause significant

281
00:10:33,680 --> 00:10:36,160
damage at the poisoning rate of one

282
00:10:36,160 --> 00:10:37,519
percent

283
00:10:37,519 --> 00:10:39,680
of the training set even for the

284
00:10:39,680 --> 00:10:41,519
constrained attacker which is only

285
00:10:41,519 --> 00:10:45,519
allowed to modify 17 out of the two 200

286
00:10:45,519 --> 00:10:49,760
2300 features of the amber dataset

287
00:10:49,760 --> 00:10:51,360
and the attack also scales quite well

288
00:10:51,360 --> 00:10:53,200
with the percentage of poison data

289
00:10:53,200 --> 00:10:54,720
growing to around 80 to 90 percent

290
00:10:54,720 --> 00:10:57,279
success of four percent poisoning

291
00:10:57,279 --> 00:11:00,240
and we also observe minimal side effects

292
00:11:00,240 --> 00:11:02,160
on clean data accuracy

293
00:11:02,160 --> 00:11:04,959
and essentially similar results

294
00:11:04,959 --> 00:11:07,760
for the feed forward neural network

295
00:11:07,760 --> 00:11:10,000
model

296
00:11:10,000 --> 00:11:12,959
a similar pattern is also highlighted by

297
00:11:12,959 --> 00:11:14,640
our experiments on the drabin and

298
00:11:14,640 --> 00:11:16,560
contingent datasets where the

299
00:11:16,560 --> 00:11:18,800
constrained attacker controlling only 30

300
00:11:18,800 --> 00:11:20,959
features and one percent of the dataset

301
00:11:20,959 --> 00:11:24,000
can achieve the success rate around 40

302
00:11:24,000 --> 00:11:27,519
and 75 respectively

303
00:11:27,519 --> 00:11:29,680
finally we consider different defensive

304
00:11:29,680 --> 00:11:32,000
strategies adapting adopting them from

305
00:11:32,000 --> 00:11:34,079
their computer vision domain such as

306
00:11:34,079 --> 00:11:35,519
spectral signatures activation

307
00:11:35,519 --> 00:11:37,760
clustering and use of isolation forests

308
00:11:37,760 --> 00:11:40,240
as anomaly detectors

309
00:11:40,240 --> 00:11:41,920
and it's worth noting that none of the

310
00:11:41,920 --> 00:11:44,320
defenses that we tested were able to

311
00:11:44,320 --> 00:11:47,279
consistently identify all vector points

312
00:11:47,279 --> 00:11:50,639
isolation forests uh managed to identify

313
00:11:50,639 --> 00:11:51,920
um

314
00:11:51,920 --> 00:11:54,079
points generated by our independence

315
00:11:54,079 --> 00:11:56,480
strategy in some scenarios

316
00:11:56,480 --> 00:11:58,240
but in

317
00:11:58,240 --> 00:12:00,079
all cases they were unable to isolate

318
00:12:00,079 --> 00:12:02,240
vectors generated by our combined

319
00:12:02,240 --> 00:12:04,480
strategy and this confirms our intuition

320
00:12:04,480 --> 00:12:06,639
that vectors generated with the combined

321
00:12:06,639 --> 00:12:09,440
strategy are particularly stealthy and

322
00:12:09,440 --> 00:12:11,200
insidious

323
00:12:11,200 --> 00:12:12,880
to conclude with this work we showed

324
00:12:12,880 --> 00:12:15,200
that benign binaries can be used as

325
00:12:15,200 --> 00:12:18,639
carriers for poisoning attacks and that

326
00:12:18,639 --> 00:12:20,639
we can use the information provided by

327
00:12:20,639 --> 00:12:23,120
model interpretability tools to design

328
00:12:23,120 --> 00:12:25,680
effective vectors

329
00:12:25,680 --> 00:12:27,360
also this approach is generic and

330
00:12:27,360 --> 00:12:29,279
applicable to multiple data and model

331
00:12:29,279 --> 00:12:33,680
types and the vectors that we create

332
00:12:33,680 --> 00:12:36,880
can be extremely hard to identify

333
00:12:36,880 --> 00:12:39,519
with say thank you very much and i'm

334
00:12:39,519 --> 00:12:43,160
happy to take questions

