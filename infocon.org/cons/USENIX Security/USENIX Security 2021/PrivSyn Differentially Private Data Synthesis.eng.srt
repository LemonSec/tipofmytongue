1
00:00:08,880 --> 00:00:11,599
hello everyone my name is drukan jam

2
00:00:11,599 --> 00:00:14,480
currently a postdoc at cispat today i'm

3
00:00:14,480 --> 00:00:16,640
going to talk about our latest work on

4
00:00:16,640 --> 00:00:19,119
differentially private data synthesis

5
00:00:19,119 --> 00:00:21,039
this is a joint work of georgian

6
00:00:21,039 --> 00:00:24,320
university cispa and purdue university

7
00:00:24,320 --> 00:00:27,599
this is the outline of my presentation

8
00:00:27,599 --> 00:00:29,840
first i will introduce the background

9
00:00:29,840 --> 00:00:32,000
and some preliminaries

10
00:00:32,000 --> 00:00:34,239
in the big data era many leading

11
00:00:34,239 --> 00:00:36,719
companies continue to collect data from

12
00:00:36,719 --> 00:00:38,000
their users

13
00:00:38,000 --> 00:00:40,399
such as browsing history and typing

14
00:00:40,399 --> 00:00:41,360
habit

15
00:00:41,360 --> 00:00:43,920
and analyze this data to improve their

16
00:00:43,920 --> 00:00:46,879
user experience or do advertisement

17
00:00:46,879 --> 00:00:48,719
recommendation

18
00:00:48,719 --> 00:00:51,680
however various privacy accidents have

19
00:00:51,680 --> 00:00:54,480
been exposed recent years which makes

20
00:00:54,480 --> 00:00:56,719
the users hesitate to share their

21
00:00:56,719 --> 00:00:58,480
private data

22
00:00:58,480 --> 00:01:01,199
to protect the user's data privacy

23
00:01:01,199 --> 00:01:03,440
differential privacy has been the golden

24
00:01:03,440 --> 00:01:06,560
standard in both academia and industry

25
00:01:06,560 --> 00:01:09,360
the general idea of dp is to guarantee

26
00:01:09,360 --> 00:01:12,479
that a single sample has limited impact

27
00:01:12,479 --> 00:01:15,119
on the output of an algorithm

28
00:01:15,119 --> 00:01:17,759
formerly given two neighboring data sets

29
00:01:17,759 --> 00:01:20,159
differing on one sample after adding

30
00:01:20,159 --> 00:01:22,479
perturbation the probability of

31
00:01:22,479 --> 00:01:25,040
outputting the same value is bounded by

32
00:01:25,040 --> 00:01:26,720
a value epsilon

33
00:01:26,720 --> 00:01:30,079
in dp we call epsilon privacy budget

34
00:01:30,079 --> 00:01:32,320
normally larger epsilon means less

35
00:01:32,320 --> 00:01:34,320
privacy

36
00:01:34,320 --> 00:01:36,560
the most commonly used mechanism to

37
00:01:36,560 --> 00:01:39,280
achieve dp is a laplace mechanism

38
00:01:39,280 --> 00:01:42,079
the main idea is adding laplace noise to

39
00:01:42,079 --> 00:01:45,200
the real value the magnitude of noise is

40
00:01:45,200 --> 00:01:47,680
proportional to the sensitivity of the

41
00:01:47,680 --> 00:01:49,119
function f

42
00:01:49,119 --> 00:01:52,000
which measures the maximum changes

43
00:01:52,000 --> 00:01:54,560
of adding or deleting one sample on the

44
00:01:54,560 --> 00:01:56,399
output

45
00:01:56,399 --> 00:01:58,960
another widely used mechanism is adding

46
00:01:58,960 --> 00:02:01,600
gaussian noise to the real value note

47
00:02:01,600 --> 00:02:04,560
that gaussian mechanism only satisfies

48
00:02:04,560 --> 00:02:06,560
epsilon delta dp

49
00:02:06,560 --> 00:02:09,360
here delta means the algorithm will

50
00:02:09,360 --> 00:02:12,239
violate action dp with very small

51
00:02:12,239 --> 00:02:15,680
probability delta thus we also call

52
00:02:15,680 --> 00:02:18,959
gaussian mechanism approximate dp

53
00:02:18,959 --> 00:02:21,599
next i will introduce some existing

54
00:02:21,599 --> 00:02:23,440
works on differentially private

55
00:02:23,440 --> 00:02:25,280
algorithms

56
00:02:25,280 --> 00:02:29,280
most of the pre-studies on dp focus on

57
00:02:29,280 --> 00:02:31,440
designing tailored algorithms for

58
00:02:31,440 --> 00:02:34,080
specific data analysis tasks

59
00:02:34,080 --> 00:02:36,800
such as frequent item set mining

60
00:02:36,800 --> 00:02:39,360
marginal release range query and

61
00:02:39,360 --> 00:02:42,160
training machine learning models however

62
00:02:42,160 --> 00:02:45,360
this paradigm is time consuming requires

63
00:02:45,360 --> 00:02:48,080
a lot of expertise knowledge and error

64
00:02:48,080 --> 00:02:50,000
prone

65
00:02:50,000 --> 00:02:52,640
to adjust this problem a promising

66
00:02:52,640 --> 00:02:55,360
solution is to generate a synthetic data

67
00:02:55,360 --> 00:02:58,560
set in a differentially private manner

68
00:02:58,560 --> 00:03:00,959
with the synthetic data set we can

69
00:03:00,959 --> 00:03:03,519
conduct any downstream data analysis

70
00:03:03,519 --> 00:03:04,400
tasks

71
00:03:04,400 --> 00:03:07,519
without consuming extra privacy budget

72
00:03:07,519 --> 00:03:10,159
and without modifying the existing

73
00:03:10,159 --> 00:03:11,360
algorithms

74
00:03:11,360 --> 00:03:14,159
the general idea of this paradigm is to

75
00:03:14,159 --> 00:03:16,640
first extract some useful statistical

76
00:03:16,640 --> 00:03:19,440
information from the original data set

77
00:03:19,440 --> 00:03:21,840
that satisfied dp and use this

78
00:03:21,840 --> 00:03:24,080
statistical information to generate the

79
00:03:24,080 --> 00:03:26,879
synthetic data set

80
00:03:26,879 --> 00:03:29,519
the most straightforward method is to

81
00:03:29,519 --> 00:03:32,480
calculate the joint distribution of all

82
00:03:32,480 --> 00:03:35,519
attributes that satisfy dp with this

83
00:03:35,519 --> 00:03:38,000
joint distribution we can easily

84
00:03:38,000 --> 00:03:40,319
generate a synthetic data set by

85
00:03:40,319 --> 00:03:42,640
directly sampling from it

86
00:03:42,640 --> 00:03:45,519
however when the number of attributes is

87
00:03:45,519 --> 00:03:46,400
large

88
00:03:46,400 --> 00:03:48,959
the domain of joint distribution is

89
00:03:48,959 --> 00:03:51,920
extremely large leading to prohibitive

90
00:03:51,920 --> 00:03:54,959
computational cost

91
00:03:54,959 --> 00:03:57,439
the state-of-the-art approach is to

92
00:03:57,439 --> 00:03:59,519
learn a graphical model from the

93
00:03:59,519 --> 00:04:01,519
original data set in a differentially

94
00:04:01,519 --> 00:04:03,920
private manner and it generates the

95
00:04:03,920 --> 00:04:06,239
synthetic data sets from the graphic

96
00:04:06,239 --> 00:04:07,840
model

97
00:04:07,840 --> 00:04:10,080
there are two widely used graphical

98
00:04:10,080 --> 00:04:13,439
models one is spacial network the other

99
00:04:13,439 --> 00:04:16,320
is markov random field

100
00:04:16,320 --> 00:04:19,440
however for the bayesian network it can

101
00:04:19,440 --> 00:04:23,520
only exploit d minus 1 marginals which

102
00:04:23,520 --> 00:04:26,479
loses many correlation information for

103
00:04:26,479 --> 00:04:29,759
the mark of random field some clicks can

104
00:04:29,759 --> 00:04:32,000
be very large when the number of

105
00:04:32,000 --> 00:04:34,479
marginals is large which

106
00:04:34,479 --> 00:04:38,240
leads to high storage cost

107
00:04:38,240 --> 00:04:40,639
another method is to change a degenerate

108
00:04:40,639 --> 00:04:43,759
generative model such as gen in a

109
00:04:43,759 --> 00:04:45,919
differentially private manner

110
00:04:45,919 --> 00:04:48,800
and generate a synthetic dataset using

111
00:04:48,800 --> 00:04:52,000
this game gen has shown to perform well

112
00:04:52,000 --> 00:04:55,199
on image dataset however through our

113
00:04:55,199 --> 00:04:58,080
experience in the differential privacy

114
00:04:58,080 --> 00:05:02,080
synthetic data challenge hosted by nist

115
00:05:02,080 --> 00:05:05,240
the gang based method cannot achieve

116
00:05:05,240 --> 00:05:07,520
satisfiable performance for high

117
00:05:07,520 --> 00:05:11,639
dimensional tabular data

118
00:05:12,240 --> 00:05:14,880
next i will introduce our proposed

119
00:05:14,880 --> 00:05:17,520
method brief scene

120
00:05:17,520 --> 00:05:20,000
the general idea of brief sim is

121
00:05:20,000 --> 00:05:22,400
extracting a set of low dimensional

122
00:05:22,400 --> 00:05:25,039
marginal tables from the original data

123
00:05:25,039 --> 00:05:27,759
set in a differentially private manner

124
00:05:27,759 --> 00:05:30,720
the marginal table can capture the joint

125
00:05:30,720 --> 00:05:33,199
distribution of a subset of the

126
00:05:33,199 --> 00:05:34,320
attribute

127
00:05:34,320 --> 00:05:36,400
after obtaining these differentially

128
00:05:36,400 --> 00:05:39,199
private marginal tables we can generate

129
00:05:39,199 --> 00:05:41,680
a synthetic data set from them

130
00:05:41,680 --> 00:05:44,240
note that different from the graphic

131
00:05:44,240 --> 00:05:46,720
model based method brief sims can

132
00:05:46,720 --> 00:05:49,199
support arbitrary number of

133
00:05:49,199 --> 00:05:50,840
marginal

134
00:05:50,840 --> 00:05:54,080
tables the design of brief sim has two

135
00:05:54,080 --> 00:05:57,199
challenges the first challenge is how to

136
00:05:57,199 --> 00:06:00,160
choose a set of marginals that captures

137
00:06:00,160 --> 00:06:02,960
as much as correlation information and

138
00:06:02,960 --> 00:06:05,120
avoid excessive noise

139
00:06:05,120 --> 00:06:07,680
the second challenge is how to generate

140
00:06:07,680 --> 00:06:10,479
a synthetic data set from the selection

141
00:06:10,479 --> 00:06:12,800
marginals

142
00:06:12,800 --> 00:06:15,759
for marginal selection we propose a new

143
00:06:15,759 --> 00:06:18,639
method called densemark it first

144
00:06:18,639 --> 00:06:21,520
considers all two-way marginals for

145
00:06:21,520 --> 00:06:24,479
instance in this example we have four

146
00:06:24,479 --> 00:06:27,120
attributes and there are in total six

147
00:06:27,120 --> 00:06:29,759
two-way marginals we need to determine

148
00:06:29,759 --> 00:06:32,479
which marginal to choose

149
00:06:32,479 --> 00:06:35,440
in this process we need to consider two

150
00:06:35,440 --> 00:06:37,520
sources of errors

151
00:06:37,520 --> 00:06:40,639
first when a two-way marginal is chosen

152
00:06:40,639 --> 00:06:43,199
we need to consider the noise error

153
00:06:43,199 --> 00:06:45,440
since we need to add gaussian noise to

154
00:06:45,440 --> 00:06:48,319
the marginal table to guarantee dp the

155
00:06:48,319 --> 00:06:50,400
noise error is proportional to the

156
00:06:50,400 --> 00:06:53,440
number of cells in the marginal table

157
00:06:53,440 --> 00:06:55,520
on the other hand when a two-way

158
00:06:55,520 --> 00:06:57,919
marginal is not chosen we need to

159
00:06:57,919 --> 00:07:00,400
consider dependency error since we

160
00:07:00,400 --> 00:07:02,240
cannot capture the correlation

161
00:07:02,240 --> 00:07:04,720
information of this marginal table

162
00:07:04,720 --> 00:07:07,280
the most widely used metric to measure

163
00:07:07,280 --> 00:07:09,840
the correlation of two attributes is

164
00:07:09,840 --> 00:07:12,160
mutual information however the

165
00:07:12,160 --> 00:07:14,319
sensitivity of mutual information is

166
00:07:14,319 --> 00:07:15,120
high

167
00:07:15,120 --> 00:07:17,840
to reduce the sensitivity we propose a

168
00:07:17,840 --> 00:07:20,240
new metric named indiv

169
00:07:20,240 --> 00:07:23,199
which calculates the l1 distance between

170
00:07:23,199 --> 00:07:25,759
the real two-way marginal and the two

171
00:07:25,759 --> 00:07:27,919
emotional generated assuming

172
00:07:27,919 --> 00:07:31,039
independence of the two attributes

173
00:07:31,039 --> 00:07:33,680
to strike the tradeoff between the noise

174
00:07:33,680 --> 00:07:36,400
error and the dependence arrow

175
00:07:36,400 --> 00:07:39,280
we formulate an optimization problem to

176
00:07:39,280 --> 00:07:43,280
choose marginals here posei and phi i

177
00:07:43,280 --> 00:07:46,080
represent the noise error and dependency

178
00:07:46,080 --> 00:07:48,800
error of marginal i respectively

179
00:07:48,800 --> 00:07:52,000
x i is a binary value indicating whether

180
00:07:52,000 --> 00:07:55,440
marginal i is selected

181
00:07:55,440 --> 00:07:57,919
since this optimization problem is

182
00:07:57,919 --> 00:07:59,120
np-hard

183
00:07:59,120 --> 00:08:02,000
we propose a greedy algorithm to select

184
00:08:02,000 --> 00:08:05,360
a set of near optimal marginals to

185
00:08:05,360 --> 00:08:08,000
reduce the number of marginals we

186
00:08:08,000 --> 00:08:11,120
further propose an algorithm to combine

187
00:08:11,120 --> 00:08:13,199
small 2-way marginals to larger

188
00:08:13,199 --> 00:08:15,840
marginals

189
00:08:15,919 --> 00:08:18,560
with the set of selected marginals the

190
00:08:18,560 --> 00:08:21,280
next step is how to generate a synthetic

191
00:08:21,280 --> 00:08:24,240
dataset from them our approach

192
00:08:24,240 --> 00:08:27,759
or proposal consists of two steps the

193
00:08:27,759 --> 00:08:30,160
first step is to randomly generate a

194
00:08:30,160 --> 00:08:32,958
data set containing n samples and d

195
00:08:32,958 --> 00:08:34,320
attributes

196
00:08:34,320 --> 00:08:36,719
the second step is to update the

197
00:08:36,719 --> 00:08:38,880
randomly generated data set to be

198
00:08:38,880 --> 00:08:41,519
consistent with all the selected

199
00:08:41,519 --> 00:08:44,800
marginals for instance if we select a

200
00:08:44,800 --> 00:08:47,680
marginal table containing attributes a1

201
00:08:47,680 --> 00:08:48,800
and a2

202
00:08:48,800 --> 00:08:51,839
we first update the records of the data

203
00:08:51,839 --> 00:08:54,880
to make its distribution consistent with

204
00:08:54,880 --> 00:08:57,920
the marginal table then for the selected

205
00:08:57,920 --> 00:09:01,120
marginal table a2 a3 we do the same

206
00:09:01,120 --> 00:09:03,760
thing and so on so forth until the

207
00:09:03,760 --> 00:09:05,760
distribution of the data set is

208
00:09:05,760 --> 00:09:08,959
consistent with all marginals

209
00:09:08,959 --> 00:09:12,480
note that when we update marginal table

210
00:09:12,480 --> 00:09:16,160
a2 a3 it will inevitably destroy the

211
00:09:16,160 --> 00:09:20,080
consistency established by marginal a1a2

212
00:09:20,080 --> 00:09:22,800
to adjust these problems we propose to

213
00:09:22,800 --> 00:09:25,839
go over all marginals multiple times

214
00:09:25,839 --> 00:09:28,399
we have proposed many other techniques

215
00:09:28,399 --> 00:09:31,680
to improve the convergence we refer the

216
00:09:31,680 --> 00:09:36,240
audience to our paper for more details

217
00:09:36,240 --> 00:09:39,279
finally i will present our experimental

218
00:09:39,279 --> 00:09:40,720
results

219
00:09:40,720 --> 00:09:43,120
we use three downstream data analysis

220
00:09:43,120 --> 00:09:46,000
tasks and their commonly used metrics to

221
00:09:46,000 --> 00:09:47,839
measure the performance

222
00:09:47,839 --> 00:09:50,000
they are paired with marginal range

223
00:09:50,000 --> 00:09:52,880
query and classification model we use

224
00:09:52,880 --> 00:09:55,360
two statory art graphical model based

225
00:09:55,360 --> 00:09:58,240
method and one game based method as our

226
00:09:58,240 --> 00:10:00,000
baseline

227
00:10:00,000 --> 00:10:02,880
this fig shows the end to end comparison

228
00:10:02,880 --> 00:10:05,600
of different datasets generation method

229
00:10:05,600 --> 00:10:08,079
on the colorado dataset

230
00:10:08,079 --> 00:10:10,320
we observe that the performance of

231
00:10:10,320 --> 00:10:14,160
prepaids and pgm is close to clear sims

232
00:10:14,160 --> 00:10:16,880
for power as marginal meaning they can

233
00:10:16,880 --> 00:10:19,200
effectively capture low dimensional

234
00:10:19,200 --> 00:10:22,079
correlation on the other hand brief sim

235
00:10:22,079 --> 00:10:24,800
significantly outperforms others for

236
00:10:24,800 --> 00:10:28,000
range query and classification meaning

237
00:10:28,000 --> 00:10:30,399
privacy can also preserve high

238
00:10:30,399 --> 00:10:32,800
dimensional correlation

239
00:10:32,800 --> 00:10:35,440
this fig shows the comparison of

240
00:10:35,440 --> 00:10:38,160
different marginal selection methods we

241
00:10:38,160 --> 00:10:40,720
observe that our proposed method then

242
00:10:40,720 --> 00:10:43,760
smug consistently outperforms

243
00:10:43,760 --> 00:10:47,920
prim phase by exploiting more marginals

244
00:10:47,920 --> 00:10:49,200
furthermore

245
00:10:49,200 --> 00:10:51,600
dense mark performs similar in the

246
00:10:51,600 --> 00:10:54,160
private setting and non-private setting

247
00:10:54,160 --> 00:10:56,480
that do not add noise in the marginal

248
00:10:56,480 --> 00:10:59,279
selection phase which indicates dense

249
00:10:59,279 --> 00:11:02,480
mark is robust to noise

250
00:11:02,480 --> 00:11:04,320
here is the conclusion

251
00:11:04,320 --> 00:11:07,440
in summary we propose a new method to

252
00:11:07,440 --> 00:11:09,839
automatically and privately select

253
00:11:09,839 --> 00:11:12,240
marginals that capture sufficient

254
00:11:12,240 --> 00:11:13,519
correlations

255
00:11:13,519 --> 00:11:16,320
in addition we propose a new data

256
00:11:16,320 --> 00:11:18,800
synthesis algorithm that can also be

257
00:11:18,800 --> 00:11:21,440
used standalone to handle dense

258
00:11:21,440 --> 00:11:24,160
graphical models

259
00:11:24,160 --> 00:11:27,880
thanks for your listening

260
00:11:34,079 --> 00:11:36,160
you

