1
00:00:09,040 --> 00:00:11,280
hello everyone today i will present our

2
00:00:11,280 --> 00:00:13,920
word device is storage efficient locking

3
00:00:13,920 --> 00:00:16,320
system part by remonster reduction and

4
00:00:16,320 --> 00:00:18,720
recommendation learning

5
00:00:18,720 --> 00:00:21,199
many systems and applications collect

6
00:00:21,199 --> 00:00:24,400
blocks to support security analysis

7
00:00:24,400 --> 00:00:27,119
for example phenix windows and my circle

8
00:00:27,119 --> 00:00:29,199
collect logs for nutrition detection and

9
00:00:29,199 --> 00:00:30,560
debugging

10
00:00:30,560 --> 00:00:32,960
however collecting arts can result in

11
00:00:32,960 --> 00:00:36,320
high storage overhead

12
00:00:36,399 --> 00:00:39,520
as reported by prevost workload gc a

13
00:00:39,520 --> 00:00:41,280
server can generate more than street

14
00:00:41,280 --> 00:00:43,680
view by stock data in one thing

15
00:00:43,680 --> 00:00:46,320
the workflow also demonstrates you that

16
00:00:46,320 --> 00:00:48,559
in one year the enterprise needs to

17
00:00:48,559 --> 00:00:51,760
collect more than 17.5 data

18
00:00:51,760 --> 00:00:54,160
the recent work seal also shows that a

19
00:00:54,160 --> 00:00:56,399
small number of computers can generate

20
00:00:56,399 --> 00:01:00,239
50 gigawatt swap data in one day

21
00:01:00,239 --> 00:01:02,399
to reduce the storage costs some

22
00:01:02,399 --> 00:01:04,319
training messages remove redundant

23
00:01:04,319 --> 00:01:06,720
events in logs without affecting the

24
00:01:06,720 --> 00:01:10,080
final results for specific tasks

25
00:01:10,080 --> 00:01:13,040
for example when a process reads a file

26
00:01:13,040 --> 00:01:14,400
many times

27
00:01:14,400 --> 00:01:16,880
we can remove repeated activities for

28
00:01:16,880 --> 00:01:19,680
dependency analysis tasks

29
00:01:19,680 --> 00:01:21,920
however they are usually not practical

30
00:01:21,920 --> 00:01:24,320
for general tasks because they lose

31
00:01:24,320 --> 00:01:27,040
important information such as repeated

32
00:01:27,040 --> 00:01:29,119
events for frequency-based nutrition

33
00:01:29,119 --> 00:01:32,159
detection methods

34
00:01:32,479 --> 00:01:34,960
another solution is compression which

35
00:01:34,960 --> 00:01:37,200
uses clear fields to represent the

36
00:01:37,200 --> 00:01:38,799
original data

37
00:01:38,799 --> 00:01:41,119
plastic compression removes unnecessary

38
00:01:41,119 --> 00:01:43,439
information in the original data without

39
00:01:43,439 --> 00:01:45,680
affecting the overall results for

40
00:01:45,680 --> 00:01:48,479
specific tasks

41
00:01:48,479 --> 00:01:50,799
this kind of method is similar to the

42
00:01:50,799 --> 00:01:53,439
pruning message and it cannot be applied

43
00:01:53,439 --> 00:01:55,840
to general tasks

44
00:01:55,840 --> 00:01:57,920
another kind of method is losses

45
00:01:57,920 --> 00:02:01,040
compression which uses short encoding to

46
00:02:01,040 --> 00:02:03,840
represent the original data

47
00:02:03,840 --> 00:02:05,920
the compression keeps all information in

48
00:02:05,920 --> 00:02:07,920
the original data and it is more

49
00:02:07,920 --> 00:02:11,599
suitable for a lot of compression

50
00:02:11,920 --> 00:02:14,000
here we introduced a losses compression

51
00:02:14,000 --> 00:02:16,239
method called gz

52
00:02:16,239 --> 00:02:18,160
given the original log

53
00:02:18,160 --> 00:02:20,720
gtape first does pre-processing to

54
00:02:20,720 --> 00:02:22,959
replace repeated strings with short

55
00:02:22,959 --> 00:02:24,319
marks

56
00:02:24,319 --> 00:02:27,120
then gdp encodes the log into a shorter

57
00:02:27,120 --> 00:02:28,800
representation

58
00:02:28,800 --> 00:02:31,280
the first crazy half entry based on the

59
00:02:31,280 --> 00:02:34,239
frequency of characters in the file

60
00:02:34,239 --> 00:02:37,280
and generates a half my reference table

61
00:02:37,280 --> 00:02:39,840
in this table a more frequent character

62
00:02:39,840 --> 00:02:42,720
is encoded with respect

63
00:02:42,720 --> 00:02:46,000
finally gjp encodes the original log by

64
00:02:46,000 --> 00:02:48,560
replacing the characters with shortcuts

65
00:02:48,560 --> 00:02:50,560
in the huffman table

66
00:02:50,560 --> 00:02:52,560
according to the pre-processing and the

67
00:02:52,560 --> 00:02:55,280
encoding g6 removes most of the

68
00:02:55,280 --> 00:02:57,840
redundancies in a lot and achieves good

69
00:02:57,840 --> 00:03:00,400
results

70
00:03:01,120 --> 00:03:02,800
recently

71
00:03:02,800 --> 00:03:04,879
neural network-based encoding method

72
00:03:04,879 --> 00:03:07,280
called gives it is proposed to achieve

73
00:03:07,280 --> 00:03:09,920
better compression results

74
00:03:09,920 --> 00:03:12,400
unlike gzip which encodes characters

75
00:03:12,400 --> 00:03:14,879
based on their frequencies in the file

76
00:03:14,879 --> 00:03:17,200
if they change they need neural network

77
00:03:17,200 --> 00:03:19,200
to encode the data

78
00:03:19,200 --> 00:03:21,040
given the original data

79
00:03:21,040 --> 00:03:23,840
if neural network model predicts each

80
00:03:23,840 --> 00:03:26,000
character and outputs their traditional

81
00:03:26,000 --> 00:03:27,840
values

82
00:03:27,840 --> 00:03:29,920
based on the prediction results from the

83
00:03:29,920 --> 00:03:32,720
model arithmetic encoding encodes the

84
00:03:32,720 --> 00:03:35,360
characters with vrbs

85
00:03:35,360 --> 00:03:38,159
as shown by many existing work even your

86
00:03:38,159 --> 00:03:40,720
network-based method can achieve better

87
00:03:40,720 --> 00:03:43,760
encoding results

88
00:03:44,080 --> 00:03:46,159
a zone divisive can provide better

89
00:03:46,159 --> 00:03:49,040
encodings it doesn't initialize domain

90
00:03:49,040 --> 00:03:51,280
knowledge in other redundancies

91
00:03:51,280 --> 00:03:54,080
and doesn't have a pre-processing step

92
00:03:54,080 --> 00:03:56,879
2018 neural network-based encoder on the

93
00:03:56,879 --> 00:03:59,760
entire log file is time-consuming

94
00:03:59,760 --> 00:04:02,319
therefore we want to explore more local

95
00:04:02,319 --> 00:04:04,959
redundancies in pre-processing and

96
00:04:04,959 --> 00:04:06,959
design a better deep neural networking

97
00:04:06,959 --> 00:04:08,720
process

98
00:04:08,720 --> 00:04:10,799
based on these ideas our message

99
00:04:10,799 --> 00:04:13,200
achieves the best compression ratio

100
00:04:13,200 --> 00:04:15,680
which is defined as the compressor file

101
00:04:15,680 --> 00:04:18,880
size over the original file size

102
00:04:18,880 --> 00:04:21,120
compared with the existing work our

103
00:04:21,120 --> 00:04:23,280
message can achieve certain times better

104
00:04:23,280 --> 00:04:25,840
compression ratio on some data sets and

105
00:04:25,840 --> 00:04:28,720
is around seven times faster compared

106
00:04:28,720 --> 00:04:31,360
with cc

107
00:04:32,240 --> 00:04:34,400
our system which is called elise

108
00:04:34,400 --> 00:04:36,880
consists of four components

109
00:04:36,880 --> 00:04:39,280
first we receive slugs from different

110
00:04:39,280 --> 00:04:40,400
sources

111
00:04:40,400 --> 00:04:42,720
a lot of formatters then converts logs

112
00:04:42,720 --> 00:04:45,199
into json files and split the file into

113
00:04:45,199 --> 00:04:47,120
several parts for more efficient

114
00:04:47,120 --> 00:04:49,360
processing

115
00:04:49,360 --> 00:04:51,840
then we design several processing rules

116
00:04:51,840 --> 00:04:54,479
to remove lottery damages

117
00:04:54,479 --> 00:04:56,960
and create a reference table for these

118
00:04:56,960 --> 00:04:58,560
rows

119
00:04:58,560 --> 00:05:00,720
after that we trade in your network

120
00:05:00,720 --> 00:05:02,880
encoder

121
00:05:02,880 --> 00:05:05,039
and finally arithmetic including

122
00:05:05,039 --> 00:05:07,280
algorithm takes the prediction from the

123
00:05:07,280 --> 00:05:09,759
model as an input

124
00:05:09,759 --> 00:05:13,039
encodes and compresses rockfires

125
00:05:13,039 --> 00:05:15,120
the final compressor artifacts consist

126
00:05:15,120 --> 00:05:18,000
of the model the reference table and the

127
00:05:18,000 --> 00:05:20,960
compressor file

128
00:05:21,280 --> 00:05:23,600
in the following slides i will introduce

129
00:05:23,600 --> 00:05:26,320
each component of elise

130
00:05:26,320 --> 00:05:30,000
in step one eli first converts logs from

131
00:05:30,000 --> 00:05:33,360
different sources into json format

132
00:05:33,360 --> 00:05:35,600
which makes the processing of different

133
00:05:35,600 --> 00:05:38,479
logs easier

134
00:05:38,479 --> 00:05:40,720
then eli starts pre-processing with

135
00:05:40,720 --> 00:05:43,039
several processing rules to remove local

136
00:05:43,039 --> 00:05:44,639
redundancies

137
00:05:44,639 --> 00:05:46,240
we observe that the possible

138
00:05:46,240 --> 00:05:48,880
compilations of case in the log file are

139
00:05:48,880 --> 00:05:50,960
ignorable

140
00:05:50,960 --> 00:05:53,039
therefore in the first pre-processing

141
00:05:53,039 --> 00:05:56,080
rule we can replace case in a log entry

142
00:05:56,080 --> 00:05:57,919
with short codes

143
00:05:57,919 --> 00:06:01,039
for example given a log entry we stop by

144
00:06:01,039 --> 00:06:03,120
its case and give the page

145
00:06:03,120 --> 00:06:06,319
an identification number

146
00:06:06,319 --> 00:06:09,440
then we replace the case with the number

147
00:06:09,440 --> 00:06:11,840
this preprocessing could significantly

148
00:06:11,840 --> 00:06:14,560
reduce the storage overhead cost by case

149
00:06:14,560 --> 00:06:17,360
in log files

150
00:06:17,840 --> 00:06:20,880
other rules further reduce values in log

151
00:06:20,880 --> 00:06:22,560
files

152
00:06:22,560 --> 00:06:25,120
in processing 2 we observe that logs

153
00:06:25,120 --> 00:06:26,960
generated by the same session could

154
00:06:26,960 --> 00:06:29,520
share values of some properties

155
00:06:29,520 --> 00:06:32,000
for example logs generated by the same

156
00:06:32,000 --> 00:06:35,280
process has the same process id

157
00:06:35,280 --> 00:06:37,600
therefore only keep these shared values

158
00:06:37,600 --> 00:06:39,759
once for each session could reduce the

159
00:06:39,759 --> 00:06:44,319
cost of storing these values repeatedly

160
00:06:44,960 --> 00:06:46,960
for the given example

161
00:06:46,960 --> 00:06:49,039
eli elixir first summarizes locks

162
00:06:49,039 --> 00:06:51,039
entries into different sections and

163
00:06:51,039 --> 00:06:54,080
finds shared fields in each session

164
00:06:54,080 --> 00:06:56,240
then he likes only caves the values of

165
00:06:56,240 --> 00:07:00,240
shield fields once for each session

166
00:07:01,039 --> 00:07:03,039
we also find that learners of some

167
00:07:03,039 --> 00:07:05,520
fields are remarkable for example

168
00:07:05,520 --> 00:07:07,599
computer architecture

169
00:07:07,599 --> 00:07:10,479
for these values we replace them with

170
00:07:10,479 --> 00:07:12,240
short codes

171
00:07:12,240 --> 00:07:15,759
as shown in the example eli first gets

172
00:07:15,759 --> 00:07:18,720
possible values of a k and gives each

173
00:07:18,720 --> 00:07:20,960
possible value a short identification

174
00:07:20,960 --> 00:07:22,880
number

175
00:07:22,880 --> 00:07:25,280
then he likes replaces the values of the

176
00:07:25,280 --> 00:07:28,560
k with physicals and reduces the cost of

177
00:07:28,560 --> 00:07:32,160
storing the original values

178
00:07:32,639 --> 00:07:34,960
then row 3 is designed based on the

179
00:07:34,960 --> 00:07:37,440
observation the values of some fields

180
00:07:37,440 --> 00:07:38,720
are increased

181
00:07:38,720 --> 00:07:41,199
for example tension

182
00:07:41,199 --> 00:07:43,440
for these values we replace them with

183
00:07:43,440 --> 00:07:45,520
their offset values

184
00:07:45,520 --> 00:07:48,560
eli first analyzes the original log and

185
00:07:48,560 --> 00:07:50,720
finds the minimum tension in the origin

186
00:07:50,720 --> 00:07:52,000
log file

187
00:07:52,000 --> 00:07:54,400
then it replaces all time stamps with

188
00:07:54,400 --> 00:07:56,639
the offset between them and the minimum

189
00:07:56,639 --> 00:07:58,879
one to reduce the cost of storing long

190
00:07:58,879 --> 00:08:01,520
time steps

191
00:08:01,680 --> 00:08:04,000
finally the loss of pre-processing draw

192
00:08:04,000 --> 00:08:06,879
removes redundancies caused by other

193
00:08:06,879 --> 00:08:09,840
frequent words such as file paths

194
00:08:09,840 --> 00:08:12,319
it removes the redundancies of storing

195
00:08:12,319 --> 00:08:14,319
these frequent words by using short

196
00:08:14,319 --> 00:08:15,840
codes

197
00:08:15,840 --> 00:08:16,960
filling in

198
00:08:16,960 --> 00:08:19,919
example consists of two log entries

199
00:08:19,919 --> 00:08:22,319
each entry contains the pass

200
00:08:22,319 --> 00:08:24,720
eli obtains frequent words and gives

201
00:08:24,720 --> 00:08:26,560
each other a code based on their

202
00:08:26,560 --> 00:08:28,319
frequencies

203
00:08:28,319 --> 00:08:30,479
utilize them replaces words in the

204
00:08:30,479 --> 00:08:32,640
original log with these codes to reduce

205
00:08:32,640 --> 00:08:35,200
the redundancies

206
00:08:35,200 --> 00:08:37,599
for pre-processing rules designed based

207
00:08:37,599 --> 00:08:40,080
on the domain knowledge can remove blood

208
00:08:40,080 --> 00:08:42,719
redundancies before the encoding

209
00:08:42,719 --> 00:08:44,800
after pre-processing we trim the given

210
00:08:44,800 --> 00:08:46,800
neural network model on the processor

211
00:08:46,800 --> 00:08:49,360
data at the encoder

212
00:08:49,360 --> 00:08:52,240
the model we used consists of two listen

213
00:08:52,240 --> 00:08:54,720
layers one dash organization layer and

214
00:08:54,720 --> 00:08:57,600
two funny connected layers

215
00:08:57,600 --> 00:09:00,000
behind a log file we like strings they

216
00:09:00,000 --> 00:09:02,240
give you a network model

217
00:09:02,240 --> 00:09:04,880
to predict each character based on its

218
00:09:04,880 --> 00:09:07,360
previous characters the prediction

219
00:09:07,360 --> 00:09:09,040
results contains

220
00:09:09,040 --> 00:09:10,880
possibilities of being different

221
00:09:10,880 --> 00:09:13,200
characters

222
00:09:13,200 --> 00:09:15,360
then the deep neural network model tries

223
00:09:15,360 --> 00:09:17,200
to make the prediction value of the

224
00:09:17,200 --> 00:09:19,680
correct character larger and outputs the

225
00:09:19,680 --> 00:09:22,800
correct results

226
00:09:22,800 --> 00:09:24,720
after training and obtaining a deep

227
00:09:24,720 --> 00:09:26,560
neural network model

228
00:09:26,560 --> 00:09:29,120
in a sequence a characteristic needs to

229
00:09:29,120 --> 00:09:31,760
be encoded and prediction results of

230
00:09:31,760 --> 00:09:33,360
this character

231
00:09:33,360 --> 00:09:37,839
as the input of the encoder

232
00:09:37,839 --> 00:09:40,560
the encoder encodes this character into

233
00:09:40,560 --> 00:09:42,399
shorter representation

234
00:09:42,399 --> 00:09:46,320
details of the encoding are in the paper

235
00:09:46,320 --> 00:09:48,640
in our evaluation we collect certain

236
00:09:48,640 --> 00:09:51,040
number files from different systems and

237
00:09:51,040 --> 00:09:54,800
applications as our data sets

238
00:09:54,880 --> 00:09:57,040
we first measure the compression ratio

239
00:09:57,040 --> 00:09:59,360
of elis on different data sets and

240
00:09:59,360 --> 00:10:01,920
compare the results with deep safe and

241
00:10:01,920 --> 00:10:04,920
gz

242
00:10:05,120 --> 00:10:07,519
we show that e-lines can achieve certain

243
00:10:07,519 --> 00:10:09,680
times better compression ratio on one of

244
00:10:09,680 --> 00:10:12,399
our data sets

245
00:10:12,640 --> 00:10:14,560
then we measure the compression time

246
00:10:14,560 --> 00:10:17,760
cost of different methods

247
00:10:17,760 --> 00:10:20,320
results show that on average e-lines can

248
00:10:20,320 --> 00:10:24,880
be around six times faster than deep-sea

249
00:10:26,640 --> 00:10:28,959
to help further understand how different

250
00:10:28,959 --> 00:10:31,200
configurations can affect the

251
00:10:31,200 --> 00:10:33,120
effectiveness and the efficiency of

252
00:10:33,120 --> 00:10:36,959
elise we designed two options

253
00:10:36,959 --> 00:10:38,959
we use different percentage of the whole

254
00:10:38,959 --> 00:10:42,480
data set to train and function the model

255
00:10:42,480 --> 00:10:44,560
as shown in the figure

256
00:10:44,560 --> 00:10:46,480
using partial data for training and

257
00:10:46,480 --> 00:10:48,480
functioning are trade-offs between

258
00:10:48,480 --> 00:10:51,600
compression ratios and compression time

259
00:10:51,600 --> 00:10:53,519
we also use the whole data set to

260
00:10:53,519 --> 00:10:55,279
function the model on different data

261
00:10:55,279 --> 00:10:56,480
sets

262
00:10:56,480 --> 00:10:58,320
the results show that functioning the

263
00:10:58,320 --> 00:11:00,640
model for one new approach can achieve

264
00:11:00,640 --> 00:11:02,880
similar or better compression results

265
00:11:02,880 --> 00:11:04,880
than training on the original data for

266
00:11:04,880 --> 00:11:07,680
several approaches

267
00:11:07,680 --> 00:11:11,680
more results can be found in our paper

268
00:11:13,519 --> 00:11:16,560
finally we conclude that motivated by

269
00:11:16,560 --> 00:11:18,959
the fact that storing knocks for large

270
00:11:18,959 --> 00:11:21,120
enterprises result in high storage

271
00:11:21,120 --> 00:11:22,320
overheads

272
00:11:22,320 --> 00:11:24,560
we designed a battery globally log

273
00:11:24,560 --> 00:11:27,200
compression method called e-lines

274
00:11:27,200 --> 00:11:29,360
to study redundancies in lux and

275
00:11:29,360 --> 00:11:31,760
leverage pre-processing growth to remove

276
00:11:31,760 --> 00:11:32,720
them

277
00:11:32,720 --> 00:11:34,480
elix also uses a deep neural

278
00:11:34,480 --> 00:11:36,560
network-based representation learning

279
00:11:36,560 --> 00:11:38,640
technique to train

280
00:11:38,640 --> 00:11:40,959
to train a better encoder

281
00:11:40,959 --> 00:11:43,519
as shown by our evaluation results he

282
00:11:43,519 --> 00:11:45,600
likes our performance due date and gives

283
00:11:45,600 --> 00:11:48,000
it by two or three times in terms of

284
00:11:48,000 --> 00:11:50,480
compression ratios and is around six

285
00:11:50,480 --> 00:11:54,480
times faster compared with deepsea

286
00:11:54,560 --> 00:11:57,800
thank you

