1
00:00:09,040 --> 00:00:11,200
hello everyone today i want to show you

2
00:00:11,200 --> 00:00:13,840
with our latest work about qre friendly

3
00:00:13,840 --> 00:00:16,239
compression for the security analysis on

4
00:00:16,239 --> 00:00:19,680
the enterprise logs data

5
00:00:21,680 --> 00:00:23,840
first let's see the background and

6
00:00:23,840 --> 00:00:27,198
motivation of our work

7
00:00:27,519 --> 00:00:29,679
the work started because of the advanced

8
00:00:29,679 --> 00:00:31,920
persistent threat which is a long-term

9
00:00:31,920 --> 00:00:34,559
presence on a network in order to mine

10
00:00:34,559 --> 00:00:37,839
highly sensitive data

11
00:00:38,480 --> 00:00:41,840
according to a report in 2015 that the

12
00:00:41,840 --> 00:00:45,600
latest intrusion prolongs over 188 days

13
00:00:45,600 --> 00:00:48,399
before detection

14
00:00:48,399 --> 00:00:51,440
let's see an example for the apt attack

15
00:00:51,440 --> 00:00:53,920
if we consider the logs data build up a

16
00:00:53,920 --> 00:00:56,399
graph in which the nodes of files and

17
00:00:56,399 --> 00:00:59,120
programs in servers and the edges are

18
00:00:59,120 --> 00:01:01,920
read write and execute operations

19
00:01:01,920 --> 00:01:04,000
the intrusion happens on one node at the

20
00:01:04,000 --> 00:01:05,760
beginning

21
00:01:05,760 --> 00:01:08,960
then it will transfer to another node

22
00:01:08,960 --> 00:01:13,600
and to another node until it is detected

23
00:01:13,600 --> 00:01:15,200
for this problem we need to use

24
00:01:15,200 --> 00:01:17,920
backtrack to find all the affected nodes

25
00:01:17,920 --> 00:01:22,320
or people call it solitary analysis

26
00:01:22,320 --> 00:01:24,320
causality analysis reconstructs

27
00:01:24,320 --> 00:01:26,240
information flow by associating

28
00:01:26,240 --> 00:01:28,720
interdependent system events

29
00:01:28,720 --> 00:01:30,479
since analysis requires all the

30
00:01:30,479 --> 00:01:33,119
information from the logs graph

31
00:01:33,119 --> 00:01:35,759
it brings high pressure on storage

32
00:01:35,759 --> 00:01:38,560
for example our corporate linc lab only

33
00:01:38,560 --> 00:01:40,560
sustained three months lox data with

34
00:01:40,560 --> 00:01:43,520
more than 5 terabytes from only 100

35
00:01:43,520 --> 00:01:44,799
hosts

36
00:01:44,799 --> 00:01:47,119
so here's a compelling need for a

37
00:01:47,119 --> 00:01:50,640
solution to scale storage

38
00:01:51,840 --> 00:01:53,439
let's do some reductions for the

39
00:01:53,439 --> 00:01:54,560
analysis

40
00:01:54,560 --> 00:01:58,320
assume we have two nodes and five edges

41
00:01:58,320 --> 00:02:01,040
for the backtrack algorithm we only need

42
00:02:01,040 --> 00:02:04,159
one edge between two nodes

43
00:02:04,159 --> 00:02:05,439
so

44
00:02:05,439 --> 00:02:07,439
if we only leave one edge between them

45
00:02:07,439 --> 00:02:10,959
we have five times reduction rate

46
00:02:10,959 --> 00:02:13,360
however things are not that easy because

47
00:02:13,360 --> 00:02:15,440
we need to consider the time constraint

48
00:02:15,440 --> 00:02:17,200
on the edges

49
00:02:17,200 --> 00:02:19,760
for example here if we have one edge

50
00:02:19,760 --> 00:02:22,640
with time unit 10 and one with time unit

51
00:02:22,640 --> 00:02:24,879
20.

52
00:02:24,879 --> 00:02:27,440
if we leave the edge with time unit 20

53
00:02:27,440 --> 00:02:29,840
we see that the node c can no longer

54
00:02:29,840 --> 00:02:34,080
backtrack to node a because the event bc

55
00:02:34,080 --> 00:02:37,440
happens earlier than the event a b

56
00:02:37,440 --> 00:02:40,160
similarly if we keep the edge with unit

57
00:02:40,160 --> 00:02:43,280
10 node b can no longer backtrack to d

58
00:02:43,280 --> 00:02:46,640
because edge a b happens

59
00:02:46,640 --> 00:02:49,680
earlier than edge d a

60
00:02:49,680 --> 00:02:52,400
consider the time constraint the current

61
00:02:52,400 --> 00:02:54,560
compression techniques removes logs

62
00:02:54,560 --> 00:02:56,400
matching predefined patterns in a

63
00:02:56,400 --> 00:02:59,120
dependency graph however there is no

64
00:02:59,120 --> 00:03:01,200
hundred percent guarantee for the same

65
00:03:01,200 --> 00:03:04,480
causality analysis outcome

66
00:03:04,480 --> 00:03:06,640
so how about lossless compressions

67
00:03:06,640 --> 00:03:09,040
nowadays we have many standard tools for

68
00:03:09,040 --> 00:03:11,200
the lossless compressions and we also

69
00:03:11,200 --> 00:03:13,200
have database compressions

70
00:03:13,200 --> 00:03:15,360
however for the standard tools it

71
00:03:15,360 --> 00:03:17,519
suffers from decompression overhead

72
00:03:17,519 --> 00:03:19,920
which means every time we do a query

73
00:03:19,920 --> 00:03:22,400
from the graph we need to decompress the

74
00:03:22,400 --> 00:03:24,720
whole database which is obviously

75
00:03:24,720 --> 00:03:26,319
unacceptable

76
00:03:26,319 --> 00:03:28,560
and for the database compression the

77
00:03:28,560 --> 00:03:30,879
decompression overhead itself is low

78
00:03:30,879 --> 00:03:33,680
however it does not support the time

79
00:03:33,680 --> 00:03:34,799
constraint

80
00:03:34,799 --> 00:03:37,760
so for this problem our solution is

81
00:03:37,760 --> 00:03:41,120
query friendly lossless compression

82
00:03:41,120 --> 00:03:43,120
assume we have a compressed graph g

83
00:03:43,120 --> 00:03:45,840
prime and q result on the compressed

84
00:03:45,840 --> 00:03:48,000
graph q prime

85
00:03:48,000 --> 00:03:50,239
the query friendly lossless compression

86
00:03:50,239 --> 00:03:53,599
means that after decompressing q prime

87
00:03:53,599 --> 00:03:54,879
we can get

88
00:03:54,879 --> 00:03:58,080
exact same result as query from the

89
00:03:58,080 --> 00:03:59,519
original graph

90
00:03:59,519 --> 00:04:01,840
g

91
00:04:04,000 --> 00:04:07,200
here is our design architecture

92
00:04:07,200 --> 00:04:08,799
at the beginning the data will pass

93
00:04:08,799 --> 00:04:11,360
through an estimator which brings which

94
00:04:11,360 --> 00:04:15,040
give us an approximate compression rate

95
00:04:15,040 --> 00:04:15,920
for

96
00:04:15,920 --> 00:04:19,358
the algorithm and then the data will

97
00:04:19,358 --> 00:04:21,199
go through structure compression and

98
00:04:21,199 --> 00:04:23,040
property compression and then save to

99
00:04:23,040 --> 00:04:24,240
the database

100
00:04:24,240 --> 00:04:26,479
for the query part we first transform

101
00:04:26,479 --> 00:04:28,960
the query and do the curing from the

102
00:04:28,960 --> 00:04:30,560
compressed database

103
00:04:30,560 --> 00:04:33,360
and do the decompression on the query

104
00:04:33,360 --> 00:04:34,560
results

105
00:04:34,560 --> 00:04:38,160
and then we get the final result

106
00:04:38,160 --> 00:04:40,000
let's see our compression algorithm on

107
00:04:40,000 --> 00:04:43,360
the logs dependency graph

108
00:04:43,360 --> 00:04:45,440
first we'll do the compression on graph

109
00:04:45,440 --> 00:04:46,479
structures

110
00:04:46,479 --> 00:04:49,120
in this step we merge nodes that shares

111
00:04:49,120 --> 00:04:52,000
the identical node a child node

112
00:04:52,000 --> 00:04:54,720
for example for node g and h we will

113
00:04:54,720 --> 00:04:58,080
merge it to the node small c

114
00:04:58,080 --> 00:05:01,039
and for node a and b we merge it to the

115
00:05:01,039 --> 00:05:02,720
node small a

116
00:05:02,720 --> 00:05:05,199
and meanwhile we will save the mapping

117
00:05:05,199 --> 00:05:06,639
into a table

118
00:05:06,639 --> 00:05:09,440
named nodemap

119
00:05:09,440 --> 00:05:12,479
after the graph structure compression we

120
00:05:12,479 --> 00:05:15,280
will have some edges with a sequence of

121
00:05:15,280 --> 00:05:17,120
integers

122
00:05:17,120 --> 00:05:19,600
for start time and end time

123
00:05:19,600 --> 00:05:21,520
and we can see that those long integer

124
00:05:21,520 --> 00:05:25,280
sequence share a long prefix so we first

125
00:05:25,280 --> 00:05:27,600
sort those sequences

126
00:05:27,600 --> 00:05:29,280
ascending for the start time and

127
00:05:29,280 --> 00:05:31,759
descending for the end time

128
00:05:31,759 --> 00:05:34,560
and then we use data coding to represent

129
00:05:34,560 --> 00:05:36,479
the sequence of values with their

130
00:05:36,479 --> 00:05:39,479
difference

131
00:05:41,440 --> 00:05:43,520
now let's do the query and decompression

132
00:05:43,520 --> 00:05:46,000
on our compressed graph

133
00:05:46,000 --> 00:05:48,080
assume now we're doing backtrack and

134
00:05:48,080 --> 00:05:51,199
note 27

135
00:05:51,199 --> 00:05:53,840
remember we have put the largest

136
00:05:53,840 --> 00:05:55,680
timestamp at the beginning of the end

137
00:05:55,680 --> 00:05:57,120
time sequence

138
00:05:57,120 --> 00:06:00,800
so if the first end time is smaller than

139
00:06:00,800 --> 00:06:02,720
the current timestamp

140
00:06:02,720 --> 00:06:04,800
which means all the end times are

141
00:06:04,800 --> 00:06:08,000
smaller than the current timestamp so we

142
00:06:08,000 --> 00:06:11,199
can backtrack to node 15 without any

143
00:06:11,199 --> 00:06:13,919
decompression

144
00:06:16,000 --> 00:06:19,199
if the first end time is larger than the

145
00:06:19,199 --> 00:06:20,560
current timestamp

146
00:06:20,560 --> 00:06:23,039
in this case we need to decompress

147
00:06:23,039 --> 00:06:26,160
the current timestamp sequence

148
00:06:26,160 --> 00:06:29,280
and check every end time to the current

149
00:06:29,280 --> 00:06:33,039
timestamp if we have if we find

150
00:06:33,039 --> 00:06:35,440
any end time is smaller than the current

151
00:06:35,440 --> 00:06:37,840
time we can still do

152
00:06:37,840 --> 00:06:43,159
the backtrack to the next node node 15.

153
00:06:48,160 --> 00:06:50,400
let's see our evaluation of the

154
00:06:50,400 --> 00:06:52,960
algorithm

155
00:06:53,599 --> 00:06:56,800
we run our algorithm on two databases

156
00:06:56,800 --> 00:06:59,680
one is the nec database and another one

157
00:06:59,680 --> 00:07:01,919
is database from the darpa transparent

158
00:07:01,919 --> 00:07:04,719
computing graph

159
00:07:05,440 --> 00:07:08,800
so for the nec lab database we received

160
00:07:08,800 --> 00:07:11,599
around three times compression ratio and

161
00:07:11,599 --> 00:07:14,000
for the database from darpa we received

162
00:07:14,000 --> 00:07:15,759
around 13 times

163
00:07:15,759 --> 00:07:18,720
compression ratio

164
00:07:29,039 --> 00:07:30,639
we also

165
00:07:30,639 --> 00:07:32,960
use the optimal ratio as the benchmark

166
00:07:32,960 --> 00:07:34,880
for our work and one of the previous

167
00:07:34,880 --> 00:07:39,520
work which is for dependency compression

168
00:07:39,520 --> 00:07:41,840
the optimal ratio is given by

169
00:07:41,840 --> 00:07:44,240
leaving one edge between one pairs of

170
00:07:44,240 --> 00:07:46,960
notes without considering the time

171
00:07:46,960 --> 00:07:48,639
constraint at all

172
00:07:48,639 --> 00:07:50,879
so we can see from the

173
00:07:50,879 --> 00:07:53,840
figure that both the full dependency and

174
00:07:53,840 --> 00:07:57,199
our edge only compression

175
00:07:57,199 --> 00:08:00,639
gain similar result as the benchmark

176
00:08:00,639 --> 00:08:04,080
and when we add the node merge into our

177
00:08:04,080 --> 00:08:06,800
algorithm we get the better compression

178
00:08:06,800 --> 00:08:08,400
ratio than

179
00:08:08,400 --> 00:08:12,120
the previous method

180
00:08:12,879 --> 00:08:15,440
next we compare our decoding overheads

181
00:08:15,440 --> 00:08:17,199
to the curing type

182
00:08:17,199 --> 00:08:19,919
in this graph the x-axis is the text

183
00:08:19,919 --> 00:08:22,240
sample of causality analysis and the

184
00:08:22,240 --> 00:08:25,280
y-axis is the comparation

185
00:08:25,280 --> 00:08:27,280
between the query time plus

186
00:08:27,280 --> 00:08:30,240
decompression time of our algorithm

187
00:08:30,240 --> 00:08:34,399
and the query time on the original graph

188
00:08:34,399 --> 00:08:37,919
when when the y value is smaller smaller

189
00:08:37,919 --> 00:08:41,839
than 100 percent it means that

190
00:08:41,839 --> 00:08:44,560
our algorithm has less time

191
00:08:44,560 --> 00:08:46,560
less total time than the uncompressed

192
00:08:46,560 --> 00:08:47,600
data

193
00:08:47,600 --> 00:08:51,040
so from the graph we can see that over

194
00:08:51,040 --> 00:08:54,240
89 of cases have less total time than

195
00:08:54,240 --> 00:08:56,640
the uncompressed data

196
00:08:56,640 --> 00:08:59,200
and on average the decompression only

197
00:08:59,200 --> 00:09:00,920
takes around

198
00:09:00,920 --> 00:09:06,279
18.66 percent of the overall time

199
00:09:12,240 --> 00:09:16,839
that's all for my presentation thank you

200
00:09:24,320 --> 00:09:26,399
you

