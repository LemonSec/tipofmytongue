1
00:00:09,200 --> 00:00:12,559
hi everyone welcome to the presentation

2
00:00:12,559 --> 00:00:14,639
today i'm presenting deep entity

3
00:00:14,639 --> 00:00:16,880
classification abusive account detection

4
00:00:16,880 --> 00:00:19,680
for online social network my name is tom

5
00:00:19,680 --> 00:00:22,320
i work at facebook focus on integrity

6
00:00:22,320 --> 00:00:24,880
related topics

7
00:00:24,880 --> 00:00:26,800
there are many types of abuses on

8
00:00:26,800 --> 00:00:29,359
facebook for example clickbait spam

9
00:00:29,359 --> 00:00:31,920
harassment bullying free speech and

10
00:00:31,920 --> 00:00:34,480
nudity are among top of them

11
00:00:34,480 --> 00:00:37,280
by definition we define abusive account

12
00:00:37,280 --> 00:00:39,280
as an account that is created for the

13
00:00:39,280 --> 00:00:42,480
purpose of abuse for example the account

14
00:00:42,480 --> 00:00:44,800
only serves for the purpose of doing

15
00:00:44,800 --> 00:00:46,800
activity that goes against facebook's

16
00:00:46,800 --> 00:00:48,800
community standard

17
00:00:48,800 --> 00:00:51,840
taking a look at the stats from 2020 q4

18
00:00:51,840 --> 00:00:54,320
facebook took down more than 1.3 billion

19
00:00:54,320 --> 00:00:56,719
abusive accounts and most of those

20
00:00:56,719 --> 00:00:59,120
accounts are actually taken down between

21
00:00:59,120 --> 00:01:01,520
just minutes from the registration

22
00:01:01,520 --> 00:01:03,680
before they could even become active on

23
00:01:03,680 --> 00:01:06,320
the platform

24
00:01:06,640 --> 00:01:09,040
the deduction of abusive account comes

25
00:01:09,040 --> 00:01:11,119
with a few different challenges

26
00:01:11,119 --> 00:01:14,320
so for example the number one here is

27
00:01:14,320 --> 00:01:16,880
that the menu review does not scale it's

28
00:01:16,880 --> 00:01:19,040
just impossible for facebook

29
00:01:19,040 --> 00:01:21,439
to review billions of abusive accounts

30
00:01:21,439 --> 00:01:22,720
all by human

31
00:01:22,720 --> 00:01:25,439
secondly the heuristic rules are hard to

32
00:01:25,439 --> 00:01:28,159
be created and maintained over time

33
00:01:28,159 --> 00:01:29,360
especially given they're in the

34
00:01:29,360 --> 00:01:31,520
adversary environment and attackers keep

35
00:01:31,520 --> 00:01:33,520
changing all the time the surgery of

36
00:01:33,520 --> 00:01:35,439
course adversaries they move extremely

37
00:01:35,439 --> 00:01:37,920
fast and we have to always adapt our

38
00:01:37,920 --> 00:01:40,640
solutions to the new attack patterns

39
00:01:40,640 --> 00:01:42,320
that being said

40
00:01:42,320 --> 00:01:44,399
facebook is investing in the machine

41
00:01:44,399 --> 00:01:46,479
learning based detection

42
00:01:46,479 --> 00:01:48,479
in the way that is sustainable and

43
00:01:48,479 --> 00:01:50,960
scalable

44
00:01:50,960 --> 00:01:53,520
the traditional mr approach for the

45
00:01:53,520 --> 00:01:56,799
integrity problem is that we first

46
00:01:56,799 --> 00:01:58,799
have the account features and then we

47
00:01:58,799 --> 00:02:01,360
use the menu labels in combination we

48
00:02:01,360 --> 00:02:03,280
feed them together into the model

49
00:02:03,280 --> 00:02:05,200
architecture the types of account

50
00:02:05,200 --> 00:02:07,360
features include for example location

51
00:02:07,360 --> 00:02:09,280
features number of posts from the

52
00:02:09,280 --> 00:02:11,120
account number of friends of the account

53
00:02:11,120 --> 00:02:13,280
those direct features owned by the

54
00:02:13,280 --> 00:02:16,080
target account

55
00:02:17,040 --> 00:02:18,720
there are few problems with the

56
00:02:18,720 --> 00:02:20,640
traditional email approaches in the

57
00:02:20,640 --> 00:02:23,360
integrity space with scales of videos

58
00:02:23,360 --> 00:02:24,959
like facebook

59
00:02:24,959 --> 00:02:26,959
the most important ones include the

60
00:02:26,959 --> 00:02:29,520
following three and our

61
00:02:29,520 --> 00:02:32,000
framework of deep entity classification

62
00:02:32,000 --> 00:02:34,800
is created to solve them let's get

63
00:02:34,800 --> 00:02:37,200
started with the first challenge the

64
00:02:37,200 --> 00:02:39,840
first challenge is that the features can

65
00:02:39,840 --> 00:02:42,560
be easily gamed by the attackers the

66
00:02:42,560 --> 00:02:44,640
problem here is that think about if you

67
00:02:44,640 --> 00:02:47,040
use location feature of account as a

68
00:02:47,040 --> 00:02:49,440
direct feature it's extremely easy for

69
00:02:49,440 --> 00:02:51,599
them to for example simply change their

70
00:02:51,599 --> 00:02:54,080
settings of their account or they could

71
00:02:54,080 --> 00:02:56,560
use a vpn to

72
00:02:56,560 --> 00:02:59,040
fake where they're actually coming from

73
00:02:59,040 --> 00:03:01,440
so those kind of direct features they're

74
00:03:01,440 --> 00:03:03,040
easy to begin

75
00:03:03,040 --> 00:03:06,159
the proposal from dc is that we extract

76
00:03:06,159 --> 00:03:08,239
the deep features of accounts by

77
00:03:08,239 --> 00:03:10,640
aggregating the properties and behavior

78
00:03:10,640 --> 00:03:13,680
features from their neighbors and fellow

79
00:03:13,680 --> 00:03:15,920
entities

80
00:03:15,920 --> 00:03:18,159
the second problem is that the features

81
00:03:18,159 --> 00:03:20,560
are handwritten which only scales to

82
00:03:20,560 --> 00:03:23,040
hundreds of features the proposal from

83
00:03:23,040 --> 00:03:25,920
dc is that it defines dozens of features

84
00:03:25,920 --> 00:03:28,480
per edge and then apply all those edges

85
00:03:28,480 --> 00:03:30,879
and recursively traverse the graph to

86
00:03:30,879 --> 00:03:32,959
aggregate them and

87
00:03:32,959 --> 00:03:34,720
this actually results in tens of

88
00:03:34,720 --> 00:03:37,280
thousands features in a scalable way

89
00:03:37,280 --> 00:03:39,360
the last piece of property is also the

90
00:03:39,360 --> 00:03:40,959
biggest problem

91
00:03:40,959 --> 00:03:43,200
is that it's extremely hard for us to

92
00:03:43,200 --> 00:03:45,200
obtain a large amount of ground truth

93
00:03:45,200 --> 00:03:46,879
data

94
00:03:46,879 --> 00:03:47,599
and

95
00:03:47,599 --> 00:03:49,840
the proposal from dc is to use a

96
00:03:49,840 --> 00:03:52,239
multi-stage training architecture the

97
00:03:52,239 --> 00:03:54,640
first stage leverages the large amount

98
00:03:54,640 --> 00:03:57,040
of low precision automated labels and

99
00:03:57,040 --> 00:03:59,280
the second stage leverages the small

100
00:03:59,280 --> 00:04:02,239
amount high precision human labels let's

101
00:04:02,239 --> 00:04:04,560
dive into those each different problems

102
00:04:04,560 --> 00:04:07,040
separately

103
00:04:07,040 --> 00:04:09,680
get started with the feature extraction

104
00:04:09,680 --> 00:04:12,080
for each of the facebook user we apply

105
00:04:12,080 --> 00:04:15,040
the first degree of failure to take a

106
00:04:15,040 --> 00:04:17,440
look at of his or her friends and

107
00:04:17,440 --> 00:04:19,839
devices and groups that are in the pages

108
00:04:19,839 --> 00:04:22,479
they're posted to and then apply the

109
00:04:22,479 --> 00:04:25,040
aggregation functions for example the

110
00:04:25,040 --> 00:04:27,199
numerical aggregation functions such as

111
00:04:27,199 --> 00:04:29,280
max mean and mean

112
00:04:29,280 --> 00:04:30,160
and

113
00:04:30,160 --> 00:04:31,520
categoric

114
00:04:31,520 --> 00:04:33,440
aggregation functions such as what is

115
00:04:33,440 --> 00:04:35,440
the percentage of empty values what the

116
00:04:35,440 --> 00:04:38,320
entropy of the categoric values

117
00:04:38,320 --> 00:04:39,120
by

118
00:04:39,120 --> 00:04:40,880
applying those standard aggregation

119
00:04:40,880 --> 00:04:43,280
functions we achieve those

120
00:04:43,280 --> 00:04:46,320
aggregated features

121
00:04:46,800 --> 00:04:49,680
in the actual dc system

122
00:04:49,680 --> 00:04:51,440
we actually take a look at more than the

123
00:04:51,440 --> 00:04:54,000
first order of sound we would apply the

124
00:04:54,000 --> 00:04:56,960
second order of the file and then apply

125
00:04:56,960 --> 00:04:58,880
the aggregation function to those second

126
00:04:58,880 --> 00:05:01,039
order component entity features

127
00:05:01,039 --> 00:05:02,560
as an example

128
00:05:02,560 --> 00:05:05,039
for each of the target account we first

129
00:05:05,039 --> 00:05:06,720
found out with their friends and then we

130
00:05:06,720 --> 00:05:08,720
found out all the posts generated by the

131
00:05:08,720 --> 00:05:10,960
friend and how many likes are generated

132
00:05:10,960 --> 00:05:12,400
from those posts

133
00:05:12,400 --> 00:05:14,320
we take a look at all those features and

134
00:05:14,320 --> 00:05:16,320
aggregate them back using the standard

135
00:05:16,320 --> 00:05:18,800
aggregation functions

136
00:05:18,800 --> 00:05:20,639
so why do we do this

137
00:05:20,639 --> 00:05:22,880
we have two major reasons the first

138
00:05:22,880 --> 00:05:25,440
reason is that it is much more difficult

139
00:05:25,440 --> 00:05:27,919
for the attackers to alter the features

140
00:05:27,919 --> 00:05:30,160
from their their neighboring accounts

141
00:05:30,160 --> 00:05:32,000
even from their neighbors of neighbors

142
00:05:32,000 --> 00:05:33,039
attacks

143
00:05:33,039 --> 00:05:34,960
why it's easy for them to change their

144
00:05:34,960 --> 00:05:36,960
own location gender

145
00:05:36,960 --> 00:05:39,520
and the age information it's become

146
00:05:39,520 --> 00:05:41,199
exponentially more difficult to change

147
00:05:41,199 --> 00:05:43,120
the same information for their second

148
00:05:43,120 --> 00:05:46,320
order of setup entities

149
00:05:46,320 --> 00:05:48,960
secondly it's because by applying the

150
00:05:48,960 --> 00:05:51,280
different aggregation functions on many

151
00:05:51,280 --> 00:05:53,680
different types of entities and their

152
00:05:53,680 --> 00:05:56,160
features we can easily create thousands

153
00:05:56,160 --> 00:06:00,800
of features using such standard format

154
00:06:02,160 --> 00:06:04,479
move on to the training data part

155
00:06:04,479 --> 00:06:07,759
there are two types of labels used in

156
00:06:07,759 --> 00:06:10,319
the facebook environment training

157
00:06:10,319 --> 00:06:13,840
one is that low precision high volume

158
00:06:13,840 --> 00:06:16,800
low cost automated labels the sources

159
00:06:16,800 --> 00:06:19,280
from those labels are normally coming

160
00:06:19,280 --> 00:06:21,600
from past action accounts and user

161
00:06:21,600 --> 00:06:22,639
reports

162
00:06:22,639 --> 00:06:25,840
the second type of label is human label

163
00:06:25,840 --> 00:06:28,080
they're almost the opposite from the

164
00:06:28,080 --> 00:06:30,240
automated labels they're much higher in

165
00:06:30,240 --> 00:06:32,960
position they're much lower in volume of

166
00:06:32,960 --> 00:06:35,120
course it's extremely expensive to

167
00:06:35,120 --> 00:06:37,120
generative because they involve the

168
00:06:37,120 --> 00:06:38,400
human labor

169
00:06:38,400 --> 00:06:41,758
from the domain experts

170
00:06:43,680 --> 00:06:45,840
the question here is that how do we

171
00:06:45,840 --> 00:06:48,479
avoid overfeeding and also obtain the

172
00:06:48,479 --> 00:06:51,199
benefit of the high quality labels the

173
00:06:51,199 --> 00:06:53,680
proposal from deep entity classification

174
00:06:53,680 --> 00:06:56,479
framework is to use a multi-stage and

175
00:06:56,479 --> 00:07:00,400
multi-task learning architecture

176
00:07:01,120 --> 00:07:03,360
here is a mod architecture in the first

177
00:07:03,360 --> 00:07:05,759
stage we leverage the low position

178
00:07:05,759 --> 00:07:08,160
automated and multi-label data with the

179
00:07:08,160 --> 00:07:11,199
second order of it features

180
00:07:11,199 --> 00:07:12,960
and in this case it's actually more than

181
00:07:12,960 --> 00:07:15,440
twenty thousand features and then we dub

182
00:07:15,440 --> 00:07:17,680
them into a multi-task deep neural

183
00:07:17,680 --> 00:07:20,639
network model from there we extract the

184
00:07:20,639 --> 00:07:23,039
embedding from the last hidden layer of

185
00:07:23,039 --> 00:07:25,280
the given neural network and feed them

186
00:07:25,280 --> 00:07:28,400
to the second stage

187
00:07:29,199 --> 00:07:32,160
in the second stage we directly use this

188
00:07:32,160 --> 00:07:34,560
high precision human labeled data to

189
00:07:34,560 --> 00:07:36,560
train the lightweight model and the

190
00:07:36,560 --> 00:07:39,039
prediction result directly comes from

191
00:07:39,039 --> 00:07:41,520
this lightweight model by using this

192
00:07:41,520 --> 00:07:44,080
architecture we achieve two purposes the

193
00:07:44,080 --> 00:07:46,800
first purpose is that

194
00:07:46,800 --> 00:07:49,280
we leverage the first layer to learn

195
00:07:49,280 --> 00:07:51,360
from the automated training data

196
00:07:51,360 --> 00:07:54,080
the data volume is huge it matches well

197
00:07:54,080 --> 00:07:55,599
with the amount of features we have

198
00:07:55,599 --> 00:07:57,120
which is more than twenty thousand in

199
00:07:57,120 --> 00:07:59,919
this case and because we do not directly

200
00:07:59,919 --> 00:08:02,240
use this score result from the first

201
00:08:02,240 --> 00:08:04,479
stage so the position of the training

202
00:08:04,479 --> 00:08:06,639
data actually doesn't have to be perfect

203
00:08:06,639 --> 00:08:08,879
as long as we can learn from those low

204
00:08:08,879 --> 00:08:11,120
position training data and represent the

205
00:08:11,120 --> 00:08:13,599
information from using the embedding the

206
00:08:13,599 --> 00:08:16,080
second purpose is that the human label

207
00:08:16,080 --> 00:08:18,720
data is only used at the second stage to

208
00:08:18,720 --> 00:08:20,960
ensure the accuracy of the model

209
00:08:20,960 --> 00:08:23,199
although the data amount is relatively

210
00:08:23,199 --> 00:08:25,360
small for the human level of data but

211
00:08:25,360 --> 00:08:28,879
the embedding space is actually only 256

212
00:08:28,879 --> 00:08:31,039
dimension so it's actually enough for

213
00:08:31,039 --> 00:08:33,919
the training purpose

214
00:08:35,760 --> 00:08:38,080
moving on to the model comparison

215
00:08:38,080 --> 00:08:39,599
we tried three models in the

216
00:08:39,599 --> 00:08:42,080
experimental part the first model is a

217
00:08:42,080 --> 00:08:44,480
historical model before dc deployed by

218
00:08:44,480 --> 00:08:46,959
facebook which is only using behavior

219
00:08:46,959 --> 00:08:50,080
features plus gpd model the second is

220
00:08:50,080 --> 00:08:53,519
the dc features plus single stage if

221
00:08:53,519 --> 00:08:55,920
neural network the third one is dc

222
00:08:55,920 --> 00:08:57,600
feature plus

223
00:08:57,600 --> 00:08:59,680
multi-stage multitasker learning which

224
00:08:59,680 --> 00:09:01,440
is a production model used by the

225
00:09:01,440 --> 00:09:04,399
current facebook

226
00:09:05,839 --> 00:09:08,160
here's the online evaluation result the

227
00:09:08,160 --> 00:09:10,880
roc curve and the pr curve both of them

228
00:09:10,880 --> 00:09:12,560
as you can see here the green line

229
00:09:12,560 --> 00:09:14,800
represents a result from the state of

230
00:09:14,800 --> 00:09:16,560
art dc

231
00:09:16,560 --> 00:09:18,880
multistage multitasker learning model it

232
00:09:18,880 --> 00:09:22,560
actually outperforms the other two

233
00:09:22,560 --> 00:09:24,640
moving on to the online evaluation

234
00:09:24,640 --> 00:09:26,800
result we're trying to measure the

235
00:09:26,800 --> 00:09:29,680
position and the recall of the dc model

236
00:09:29,680 --> 00:09:32,320
over the 30 days period time of the

237
00:09:32,320 --> 00:09:34,720
reproduction data the left side shows

238
00:09:34,720 --> 00:09:37,200
the position data and you can see the

239
00:09:37,200 --> 00:09:39,200
positions you try to be stable around

240
00:09:39,200 --> 00:09:40,160
like

241
00:09:40,160 --> 00:09:41,120
around

242
00:09:41,120 --> 00:09:42,640
98

243
00:09:42,640 --> 00:09:45,519
over the 30 days on the right side

244
00:09:45,519 --> 00:09:48,240
is a recall data the y-axis shows the

245
00:09:48,240 --> 00:09:51,360
frequency volume the blue line shows

246
00:09:51,360 --> 00:09:52,560
what is the

247
00:09:52,560 --> 00:09:55,519
fakercon volume without dc and the green

248
00:09:55,519 --> 00:09:58,320
line shows the same stats vcdc and the

249
00:09:58,320 --> 00:10:00,800
delta here is represented using the red

250
00:10:00,800 --> 00:10:01,600
line

251
00:10:01,600 --> 00:10:03,600
and from the figure you can tell that

252
00:10:03,600 --> 00:10:05,040
the red line is actually ready to be

253
00:10:05,040 --> 00:10:07,519
stable over the 30 days meaning the

254
00:10:07,519 --> 00:10:10,160
recall of the dc system is stable

255
00:10:10,160 --> 00:10:12,000
against adversaries because we're able

256
00:10:12,000 --> 00:10:14,320
to take down similar amount of accounts

257
00:10:14,320 --> 00:10:19,040
every day over the 30 days in production

258
00:10:19,040 --> 00:10:21,440
thanks for listening to the talk here

259
00:10:21,440 --> 00:10:24,160
are a few takeaways from the talk when

260
00:10:24,160 --> 00:10:26,880
the feature wise the dc system extracts

261
00:10:26,880 --> 00:10:28,959
the graph based deep features from

262
00:10:28,959 --> 00:10:31,760
accounts it allows us to scale features

263
00:10:31,760 --> 00:10:34,160
and resist adversary adoption

264
00:10:34,160 --> 00:10:36,320
on the architecture wise to leverage

265
00:10:36,320 --> 00:10:39,279
both the automated data and the human

266
00:10:39,279 --> 00:10:41,839
labor data we apply a multi-stage and

267
00:10:41,839 --> 00:10:45,040
multi-task learning and jointly this

268
00:10:45,040 --> 00:10:47,760
architecture and the graph based deep

269
00:10:47,760 --> 00:10:50,560
features improve the model performance

270
00:10:50,560 --> 00:10:53,440
thirdly because of this is a two-year

271
00:10:53,440 --> 00:10:55,519
deployment when the paper before the

272
00:10:55,519 --> 00:10:58,000
paper was published it has resulted in

273
00:10:58,000 --> 00:11:00,079
facebook taking down hundreds of

274
00:11:00,079 --> 00:11:01,839
millions of abusive accounts over the

275
00:11:01,839 --> 00:11:03,600
past few years

276
00:11:03,600 --> 00:11:05,519
lastly which is also the most

277
00:11:05,519 --> 00:11:07,839
interesting part counterintuitively the

278
00:11:07,839 --> 00:11:10,880
deployment of dec reduced the global cpu

279
00:11:10,880 --> 00:11:13,360
usage on facebook despite the high

280
00:11:13,360 --> 00:11:15,920
computational load this is because

281
00:11:15,920 --> 00:11:17,920
because of this we are able to take down

282
00:11:17,920 --> 00:11:19,920
so many abusive accounts this actually

283
00:11:19,920 --> 00:11:22,800
significantly reduces cpu that is spent

284
00:11:22,800 --> 00:11:25,279
on such abusive account activities on

285
00:11:25,279 --> 00:11:27,760
the platform

286
00:11:27,760 --> 00:11:31,000
thank you

