1
00:00:09,040 --> 00:00:10,800
hi i'm rob jansen from the us naval

2
00:00:10,800 --> 00:00:12,480
research laboratory today i'm going to

3
00:00:12,480 --> 00:00:14,240
talk about our work once it's never

4
00:00:14,240 --> 00:00:16,400
enough foundations for sound statistical

5
00:00:16,400 --> 00:00:17,359
inference and torn network

6
00:00:17,359 --> 00:00:18,880
experimentation

7
00:00:18,880 --> 00:00:20,720
this is joint work with justin tracy and

8
00:00:20,720 --> 00:00:21,920
ian goldberg from the university of

9
00:00:21,920 --> 00:00:24,000
waterloo

10
00:00:24,000 --> 00:00:26,480
our work is about tour experimentation

11
00:00:26,480 --> 00:00:28,560
towards anonymous communication system

12
00:00:28,560 --> 00:00:30,160
that separates identification from

13
00:00:30,160 --> 00:00:31,439
routing

14
00:00:31,439 --> 00:00:32,719
uh

15
00:00:32,719 --> 00:00:35,280
clients in tor send their web requests

16
00:00:35,280 --> 00:00:38,239
through a series of three tour relays

17
00:00:38,239 --> 00:00:40,160
and as a result

18
00:00:40,160 --> 00:00:41,680
of this routing

19
00:00:41,680 --> 00:00:44,160
process no single relay in the network

20
00:00:44,160 --> 00:00:45,840
can identify both the source of the

21
00:00:45,840 --> 00:00:47,760
traffic and the destination so tor

22
00:00:47,760 --> 00:00:49,680
provides unlikable communication

23
00:00:49,680 --> 00:00:51,360
and protects user privacy and safety

24
00:00:51,360 --> 00:00:53,680
online now because the traffic is sent

25
00:00:53,680 --> 00:00:55,920
through multiple tor relays it's

26
00:00:55,920 --> 00:00:57,920
generally slower to access websites and

27
00:00:57,920 --> 00:01:00,239
tor than it is to access them directly

28
00:01:00,239 --> 00:01:01,280
as a result there's been a lot of

29
00:01:01,280 --> 00:01:03,359
research over the years on improving tor

30
00:01:03,359 --> 00:01:04,640
performance

31
00:01:04,640 --> 00:01:06,560
in order to evaluate

32
00:01:06,560 --> 00:01:08,400
how well these different proposals

33
00:01:08,400 --> 00:01:09,920
perform

34
00:01:09,920 --> 00:01:11,439
we need methods and tools to help us

35
00:01:11,439 --> 00:01:13,520
safely conduct torque experiments a lot

36
00:01:13,520 --> 00:01:14,960
of these experiments can't be conducted

37
00:01:14,960 --> 00:01:17,360
on the live network for ethical reasons

38
00:01:17,360 --> 00:01:19,600
we want experimentation results gathered

39
00:01:19,600 --> 00:01:21,119
using test networks to be accurate and

40
00:01:21,119 --> 00:01:23,680
dependable so that we can use them

41
00:01:23,680 --> 00:01:25,360
to make real-world decisions about the

42
00:01:25,360 --> 00:01:27,759
real network

43
00:01:27,759 --> 00:01:29,360
so generally when running toward

44
00:01:29,360 --> 00:01:30,640
experiments

45
00:01:30,640 --> 00:01:32,159
there are three steps first we need to

46
00:01:32,159 --> 00:01:34,320
model a torque test network

47
00:01:34,320 --> 00:01:35,600
and then we need to run the test network

48
00:01:35,600 --> 00:01:37,040
experiments and collect and analyze the

49
00:01:37,040 --> 00:01:38,159
results

50
00:01:38,159 --> 00:01:40,320
we make contributions in all three of

51
00:01:40,320 --> 00:01:42,960
these areas in our paper

52
00:01:42,960 --> 00:01:44,479
i'm not going to go over the details now

53
00:01:44,479 --> 00:01:45,759
but i'm going to describe our

54
00:01:45,759 --> 00:01:47,040
contributions in each of these three

55
00:01:47,040 --> 00:01:49,119
areas in the following slides and i'll

56
00:01:49,119 --> 00:01:51,600
start with talking about how we model

57
00:01:51,600 --> 00:01:54,000
tor test networks

58
00:01:54,000 --> 00:01:56,320
okay so our modeling approach uses data

59
00:01:56,320 --> 00:01:58,240
that's available and published by tor

60
00:01:58,240 --> 00:01:59,360
project it's available on

61
00:01:59,360 --> 00:02:01,280
metrics.orgproject.org

62
00:02:01,280 --> 00:02:03,360
we use all the data that's available to

63
00:02:03,360 --> 00:02:05,600
make informed modeling decisions

64
00:02:05,600 --> 00:02:07,360
one of the primary

65
00:02:07,360 --> 00:02:09,679
contributions in our modeling approach

66
00:02:09,679 --> 00:02:11,360
is to

67
00:02:11,360 --> 00:02:14,239
account for relay churn

68
00:02:14,239 --> 00:02:16,319
and the state of the network over time

69
00:02:16,319 --> 00:02:17,920
this graph shows

70
00:02:17,920 --> 00:02:19,200
number of relays that exist at the

71
00:02:19,200 --> 00:02:22,319
beginning of january 2019

72
00:02:22,319 --> 00:02:23,920
as a solid line and how many of those

73
00:02:23,920 --> 00:02:26,000
relays remain in the network over time

74
00:02:26,000 --> 00:02:27,440
we see that

75
00:02:27,440 --> 00:02:29,120
2000 relays left the network by the end

76
00:02:29,120 --> 00:02:30,640
of january

77
00:02:30,640 --> 00:02:32,160
and the dash line shows how many new

78
00:02:32,160 --> 00:02:33,680
relays joined the network from the

79
00:02:33,680 --> 00:02:34,800
beginning of january through the end of

80
00:02:34,800 --> 00:02:36,720
january we see that over 5000 relays

81
00:02:36,720 --> 00:02:38,080
join the network

82
00:02:38,080 --> 00:02:39,760
so this shows that there's a very high

83
00:02:39,760 --> 00:02:41,519
level of churn in the network

84
00:02:41,519 --> 00:02:44,400
and our modeling technique is to

85
00:02:44,400 --> 00:02:45,760
consider the state of the network over

86
00:02:45,760 --> 00:02:48,000
time in order to capture this diversity

87
00:02:48,000 --> 00:02:50,319
in relays that are are coming and um

88
00:02:50,319 --> 00:02:53,280
joining and leaving the network

89
00:02:53,280 --> 00:02:56,160
we sample relays from the total

90
00:02:56,160 --> 00:02:58,560
the full set of relays that we observe

91
00:02:58,560 --> 00:03:00,560
we sample them according to their uptime

92
00:03:00,560 --> 00:03:01,840
during the modeling period and also

93
00:03:01,840 --> 00:03:03,519
according to their bandwidth consensus

94
00:03:03,519 --> 00:03:04,720
weight

95
00:03:04,720 --> 00:03:06,640
a second area that we improve the tor

96
00:03:06,640 --> 00:03:08,879
modeling process is that we simulate

97
00:03:08,879 --> 00:03:10,800
multiple users in each torque client

98
00:03:10,800 --> 00:03:13,120
process to save ram the previous method

99
00:03:13,120 --> 00:03:15,120
was to simulate one user per for every

100
00:03:15,120 --> 00:03:17,200
torque climb process but that requires

101
00:03:17,200 --> 00:03:19,040
us to run more client processes and we

102
00:03:19,040 --> 00:03:19,920
can

103
00:03:19,920 --> 00:03:21,760
because these users are

104
00:03:21,760 --> 00:03:23,680
mostly idle uh they're active and then

105
00:03:23,680 --> 00:03:24,879
idle for large periods of time we can

106
00:03:24,879 --> 00:03:26,640
actually simulate multiple users in a

107
00:03:26,640 --> 00:03:28,720
single client

108
00:03:28,720 --> 00:03:29,599
for

109
00:03:29,599 --> 00:03:32,959
resource to save resources

110
00:03:32,959 --> 00:03:34,959
uh to run the tor experiments in the

111
00:03:34,959 --> 00:03:37,040
test networks that we are modeling we

112
00:03:37,040 --> 00:03:37,920
use

113
00:03:37,920 --> 00:03:39,440
shadow shadow is a discrete event

114
00:03:39,440 --> 00:03:40,879
network simulator it's the most popular

115
00:03:40,879 --> 00:03:42,799
tool for running tour experiments

116
00:03:42,799 --> 00:03:44,159
we conducted a performance audit of

117
00:03:44,159 --> 00:03:47,120
shadow using the linux perf tool

118
00:03:47,120 --> 00:03:48,720
we found and fixed several performance

119
00:03:48,720 --> 00:03:50,799
bottlenecks in shadow we added a feature

120
00:03:50,799 --> 00:03:52,319
to shorten tor bootstrapping time when

121
00:03:52,319 --> 00:03:54,080
running in shadow and

122
00:03:54,080 --> 00:03:57,200
we enabled other runtime optimizations

123
00:03:57,200 --> 00:03:59,439
we also analyzed shadows networking and

124
00:03:59,439 --> 00:04:01,280
fix some non-determinism bugs and also

125
00:04:01,280 --> 00:04:03,439
improve the network stack

126
00:04:03,439 --> 00:04:06,159
now to showcase how our results uh how

127
00:04:06,159 --> 00:04:08,640
our improvements affected the results we

128
00:04:08,640 --> 00:04:10,000
compared a state-of-the-art

129
00:04:10,000 --> 00:04:11,280
state-of-the-art

130
00:04:11,280 --> 00:04:14,400
experimentation approach from ccs 2018

131
00:04:14,400 --> 00:04:16,238
this is table 2 which i shamelessly

132
00:04:16,238 --> 00:04:18,160
copied from our paper

133
00:04:18,160 --> 00:04:20,399
we analyzed we run the same experiment

134
00:04:20,399 --> 00:04:22,639
that was run in previous work and i'm

135
00:04:22,639 --> 00:04:24,000
showcasing

136
00:04:24,000 --> 00:04:24,880
two

137
00:04:24,880 --> 00:04:26,639
main results here first

138
00:04:26,639 --> 00:04:28,960
we were able to reduce the ram usage for

139
00:04:28,960 --> 00:04:30,400
the same experiment run from previous

140
00:04:30,400 --> 00:04:32,639
work by 64 percent

141
00:04:32,639 --> 00:04:35,360
by making the improvements that we made

142
00:04:35,360 --> 00:04:37,199
we were also able to reduce run time by

143
00:04:37,199 --> 00:04:39,759
94

144
00:04:40,000 --> 00:04:42,240
by making the performance improvements

145
00:04:42,240 --> 00:04:44,080
and as a result of these resource

146
00:04:44,080 --> 00:04:45,520
improvements

147
00:04:45,520 --> 00:04:47,759
this allows us to support larger scale

148
00:04:47,759 --> 00:04:49,440
test networks than we were previously

149
00:04:49,440 --> 00:04:50,400
possible

150
00:04:50,400 --> 00:04:52,960
uh we are the first to run a 100 tor

151
00:04:52,960 --> 00:04:56,000
network with about 6500 relays and about

152
00:04:56,000 --> 00:04:57,360
the traffic from about eight hundred

153
00:04:57,360 --> 00:04:59,280
thousand users we're the first to run

154
00:04:59,280 --> 00:05:01,120
this hundred percent network uh in

155
00:05:01,120 --> 00:05:03,759
simulation

156
00:05:04,240 --> 00:05:06,080
and the third area is

157
00:05:06,080 --> 00:05:07,520
running experiments and then collecting

158
00:05:07,520 --> 00:05:10,880
the results and analyzing them

159
00:05:10,880 --> 00:05:12,320
uh okay so

160
00:05:12,320 --> 00:05:14,240
running the experiments uh when

161
00:05:14,240 --> 00:05:16,720
generating the networks that we run the

162
00:05:16,720 --> 00:05:18,639
experiments that we are run this

163
00:05:18,639 --> 00:05:21,039
involves two levels of sampling first

164
00:05:21,039 --> 00:05:22,560
as previously mentioned we sample the

165
00:05:22,560 --> 00:05:24,400
tor test network using data that's

166
00:05:24,400 --> 00:05:26,240
available on tour metrics

167
00:05:26,240 --> 00:05:27,919
at some scale less than a hundred

168
00:05:27,919 --> 00:05:29,520
percent

169
00:05:29,520 --> 00:05:31,360
generally it's this depends on the

170
00:05:31,360 --> 00:05:33,280
hardware resources available and the the

171
00:05:33,280 --> 00:05:35,919
amount of ram that torque consumes

172
00:05:35,919 --> 00:05:37,360
but we generally model it at a scale

173
00:05:37,360 --> 00:05:39,759
less than 10 100 percent

174
00:05:39,759 --> 00:05:41,360
and that gives us a test network model

175
00:05:41,360 --> 00:05:44,080
that we can run

176
00:05:44,080 --> 00:05:46,639
second way of sampling is to run us the

177
00:05:46,639 --> 00:05:48,240
same simulation multiple times using a

178
00:05:48,240 --> 00:05:50,080
different simulator seed

179
00:05:50,080 --> 00:05:51,600
previous work

180
00:05:51,600 --> 00:05:53,840
ignores these sampling choices and

181
00:05:53,840 --> 00:05:55,360
simply runs a single simulation for

182
00:05:55,360 --> 00:05:57,199
vanilla tour and then a single

183
00:05:57,199 --> 00:06:00,240
simulation for each research variant

184
00:06:00,240 --> 00:06:02,080
and they compare the empirical cdfs from

185
00:06:02,080 --> 00:06:03,840
each of these networks directly with

186
00:06:03,840 --> 00:06:05,759
while ignoring sampling error

187
00:06:05,759 --> 00:06:07,759
this is not statistically rigorous and

188
00:06:07,759 --> 00:06:09,680
so we argue that

189
00:06:09,680 --> 00:06:12,080
one simulation is never enough to

190
00:06:12,080 --> 00:06:13,520
to have statistically significant

191
00:06:13,520 --> 00:06:16,400
results we need to do repeated sampling

192
00:06:16,400 --> 00:06:19,280
not just of simulator seeds we can't

193
00:06:19,280 --> 00:06:20,560
just run this the same simulation

194
00:06:20,560 --> 00:06:22,240
multiple times with different seeds but

195
00:06:22,240 --> 00:06:24,160
we also need to sample

196
00:06:24,160 --> 00:06:26,000
the tor test networks themselves

197
00:06:26,000 --> 00:06:28,080
multiple times to have

198
00:06:28,080 --> 00:06:29,039
better

199
00:06:29,039 --> 00:06:30,960
diverse coverage of the diversity that's

200
00:06:30,960 --> 00:06:33,199
that's present in the live network

201
00:06:33,199 --> 00:06:35,520
so we provide methods in our paper that

202
00:06:35,520 --> 00:06:38,000
help us quantify the sampling error

203
00:06:38,000 --> 00:06:39,520
by computing confidence intervals over

204
00:06:39,520 --> 00:06:41,520
the empirical cdfs that are captured

205
00:06:41,520 --> 00:06:44,479
from our experiments that we run

206
00:06:44,479 --> 00:06:46,000
and our methods allow us to make

207
00:06:46,000 --> 00:06:47,600
statistical arguments for the results

208
00:06:47,600 --> 00:06:48,880
that we observe

209
00:06:48,880 --> 00:06:50,960
in our experiments and it gives us

210
00:06:50,960 --> 00:06:52,160
confidence more confidence in the

211
00:06:52,160 --> 00:06:53,280
conclusions that we draw from our

212
00:06:53,280 --> 00:06:55,679
results

213
00:06:55,840 --> 00:06:56,880
so

214
00:06:56,880 --> 00:07:00,160
uh i'm going to give you an example of

215
00:07:00,160 --> 00:07:02,880
how this is done to sort of provide some

216
00:07:02,880 --> 00:07:05,039
intuition behind what we're doing all

217
00:07:05,039 --> 00:07:07,919
the math and the results on that just

218
00:07:07,919 --> 00:07:09,759
describe how this is done are in the

219
00:07:09,759 --> 00:07:10,720
paper

220
00:07:10,720 --> 00:07:12,800
but essentially suppose you're running

221
00:07:12,800 --> 00:07:14,319
the same simulation

222
00:07:14,319 --> 00:07:16,560
three times and this generates three

223
00:07:16,560 --> 00:07:19,120
empirical cdfs for a random variable in

224
00:07:19,120 --> 00:07:21,280
this case the example is the time to

225
00:07:21,280 --> 00:07:22,319
last by

226
00:07:22,319 --> 00:07:24,240
how long it takes to download a file

227
00:07:24,240 --> 00:07:27,360
you get three empirical distributions

228
00:07:27,360 --> 00:07:28,639
our methods

229
00:07:28,639 --> 00:07:30,240
are to take these distributions and

230
00:07:30,240 --> 00:07:33,360
compute an estimated true cdf

231
00:07:33,360 --> 00:07:35,360
which is shown by the solid line on the

232
00:07:35,360 --> 00:07:38,400
right graph taking the mean over all

233
00:07:38,400 --> 00:07:40,400
y values on the left graph

234
00:07:40,400 --> 00:07:42,240
at each y value take the mean over the

235
00:07:42,240 --> 00:07:43,120
three

236
00:07:43,120 --> 00:07:44,639
available

237
00:07:44,639 --> 00:07:47,759
samples from these distributions

238
00:07:47,759 --> 00:07:49,440
and then also

239
00:07:49,440 --> 00:07:51,599
we compute the confidence intervals

240
00:07:51,599 --> 00:07:53,840
around the estimated cdf by

241
00:07:53,840 --> 00:07:55,280
incorporating sampling and measurement

242
00:07:55,280 --> 00:07:56,160
error

243
00:07:56,160 --> 00:07:57,840
and the confidence intervals

244
00:07:57,840 --> 00:08:00,400
help guide us to understand

245
00:08:00,400 --> 00:08:02,319
how precise is our measurement how much

246
00:08:02,319 --> 00:08:03,840
confidence should we place

247
00:08:03,840 --> 00:08:05,840
in the result that we've we've

248
00:08:05,840 --> 00:08:07,280
gathered

249
00:08:07,280 --> 00:08:09,440
in the paper we conduct a case study of

250
00:08:09,440 --> 00:08:10,639
tort usage and performance to

251
00:08:10,639 --> 00:08:12,720
demonstrate how to apply these methods

252
00:08:12,720 --> 00:08:15,599
using a very simple example

253
00:08:15,599 --> 00:08:17,599
so we use a simple hypothesis that

254
00:08:17,599 --> 00:08:18,879
increasing

255
00:08:18,879 --> 00:08:20,560
the amount of traffic load that the

256
00:08:20,560 --> 00:08:22,319
relays are forwarding in the network by

257
00:08:22,319 --> 00:08:24,080
20 should decrease performance for

258
00:08:24,080 --> 00:08:25,440
existing clients

259
00:08:25,440 --> 00:08:27,599
so we tested this hypothesis by running

260
00:08:27,599 --> 00:08:29,599
simulations in 100 loaded networks and

261
00:08:29,599 --> 00:08:32,080
120 percent loaded networks

262
00:08:32,080 --> 00:08:34,080
more congested networks

263
00:08:34,080 --> 00:08:35,919
using a different scale factors one

264
00:08:35,919 --> 00:08:38,080
percent network with 60 relays 10

265
00:08:38,080 --> 00:08:40,880
network with 600 relays and a 30 network

266
00:08:40,880 --> 00:08:43,760
with 1800 relays approximately

267
00:08:43,760 --> 00:08:45,920
and we run a different uh

268
00:08:45,920 --> 00:08:47,600
number of simulations for each of these

269
00:08:47,600 --> 00:08:50,160
setups 420 simulations in total

270
00:08:50,160 --> 00:08:51,519
we are particularly interested in how

271
00:08:51,519 --> 00:08:52,640
the network scale affects the

272
00:08:52,640 --> 00:08:55,839
conclusions we can draw from the results

273
00:08:55,839 --> 00:08:56,880
so

274
00:08:56,880 --> 00:08:58,240
the results are shown here for a one

275
00:08:58,240 --> 00:08:59,519
percent network

276
00:08:59,519 --> 00:09:02,560
we can see there's two major

277
00:09:02,560 --> 00:09:04,399
takeaways here this is showing the time

278
00:09:04,399 --> 00:09:07,120
to download the files

279
00:09:07,120 --> 00:09:08,160
we can see first of all that the

280
00:09:08,160 --> 00:09:10,240
confidence intervals for the different

281
00:09:10,240 --> 00:09:12,320
load values

282
00:09:12,320 --> 00:09:14,240
are are overlapping

283
00:09:14,240 --> 00:09:16,959
uh significantly overlapping for both

284
00:09:16,959 --> 00:09:18,720
running 10 simulations and for running n

285
00:09:18,720 --> 00:09:21,360
equals 100 simulations in both cases

286
00:09:21,360 --> 00:09:22,880
the confidence intervals are overlapping

287
00:09:22,880 --> 00:09:24,080
which indicates that we don't have

288
00:09:24,080 --> 00:09:26,080
enough

289
00:09:26,080 --> 00:09:28,000
confidence precision in the results to

290
00:09:28,000 --> 00:09:28,800
make

291
00:09:28,800 --> 00:09:29,680
any

292
00:09:29,680 --> 00:09:32,160
conclusions about our hypothesis

293
00:09:32,160 --> 00:09:33,440
in fact

294
00:09:33,440 --> 00:09:36,560
we can see that the 120 loaded network

295
00:09:36,560 --> 00:09:37,760
the orange

296
00:09:37,760 --> 00:09:40,240
shaded areas appears to be to the left

297
00:09:40,240 --> 00:09:43,040
of the blue shaded areas of 100

298
00:09:43,040 --> 00:09:45,040
which is the opposite effect of what we

299
00:09:45,040 --> 00:09:46,880
expect we expect that adding load should

300
00:09:46,880 --> 00:09:49,279
make download time go up knock down

301
00:09:49,279 --> 00:09:50,800
and so just if we just use the one

302
00:09:50,800 --> 00:09:52,000
percent network

303
00:09:52,000 --> 00:09:54,000
and 100 simulations we might come to the

304
00:09:54,000 --> 00:09:55,600
wrong um

305
00:09:55,600 --> 00:09:58,720
in the incorrect conclusion

306
00:09:58,880 --> 00:10:01,040
so when we increase the scale to the 10

307
00:10:01,040 --> 00:10:03,920
network with 600 relays we see that the

308
00:10:03,920 --> 00:10:05,839
these relays are better representative

309
00:10:05,839 --> 00:10:08,240
of the the true tor network in this case

310
00:10:08,240 --> 00:10:10,160
this confidence intervals are close with

311
00:10:10,160 --> 00:10:12,720
five with n equals five simulations but

312
00:10:12,720 --> 00:10:14,560
there's still some overlap in the blue

313
00:10:14,560 --> 00:10:16,800
and orange shaded areas

314
00:10:16,800 --> 00:10:19,200
but with n equals 10 simulations the

315
00:10:19,200 --> 00:10:21,200
these intervals start to separate

316
00:10:21,200 --> 00:10:22,079
and

317
00:10:22,079 --> 00:10:23,760
with n equals 100 they're completely

318
00:10:23,760 --> 00:10:24,800
separate

319
00:10:24,800 --> 00:10:27,519
and that gives us enough confidence to

320
00:10:27,519 --> 00:10:29,200
confidently say that

321
00:10:29,200 --> 00:10:31,200
indeed adding 20

322
00:10:31,200 --> 00:10:32,800
load onto the network

323
00:10:32,800 --> 00:10:34,959
makes download times go up

324
00:10:34,959 --> 00:10:37,440
for the other clients in the network and

325
00:10:37,440 --> 00:10:40,160
then at scale of 30 we see again clearly

326
00:10:40,160 --> 00:10:41,440
the confidence intervals are clearly

327
00:10:41,440 --> 00:10:43,920
separate with either when running 5 or

328
00:10:43,920 --> 00:10:46,480
10 simulations so the overall general

329
00:10:46,480 --> 00:10:48,959
conclusion here is

330
00:10:48,959 --> 00:10:50,320
that more simulations are needed at

331
00:10:50,320 --> 00:10:51,680
smaller scales

332
00:10:51,680 --> 00:10:55,040
to get a specific ci precision and fewer

333
00:10:55,040 --> 00:10:56,480
simulations are required at larger

334
00:10:56,480 --> 00:10:58,640
scales to reach the same the same ci

335
00:10:58,640 --> 00:11:00,880
precision

336
00:11:00,880 --> 00:11:02,640
so the methods that we

337
00:11:02,640 --> 00:11:04,480
propose in our paper

338
00:11:04,480 --> 00:11:07,360
help us to conduct this type of analysis

339
00:11:07,360 --> 00:11:09,360
and helps guide researchers

340
00:11:09,360 --> 00:11:11,680
in better understanding the results and

341
00:11:11,680 --> 00:11:13,200
if they've run enough simulations or if

342
00:11:13,200 --> 00:11:14,320
they need to continue running more

343
00:11:14,320 --> 00:11:18,160
simulations to get a more precise result

344
00:11:18,160 --> 00:11:19,440
so to summarize the primary

345
00:11:19,440 --> 00:11:20,880
contributions are here shown here and

346
00:11:20,880 --> 00:11:22,720
our main results are shown here

347
00:11:22,720 --> 00:11:24,640
we release

348
00:11:24,640 --> 00:11:26,079
all of our work

349
00:11:26,079 --> 00:11:27,200
all of our improvements to shadow have

350
00:11:27,200 --> 00:11:28,720
been merged into the public shadow we

351
00:11:28,720 --> 00:11:30,880
release artifacts for our research on

352
00:11:30,880 --> 00:11:33,200
the at the website linked below so you

353
00:11:33,200 --> 00:11:34,399
can go there if you want to reproduce

354
00:11:34,399 --> 00:11:36,320
our results my contact information is

355
00:11:36,320 --> 00:11:37,440
also shown

356
00:11:37,440 --> 00:11:41,000
thank you for your time

357
00:11:47,920 --> 00:11:50,000
you

