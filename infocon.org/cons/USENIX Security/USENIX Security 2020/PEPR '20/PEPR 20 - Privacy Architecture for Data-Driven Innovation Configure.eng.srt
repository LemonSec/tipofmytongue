1
00:00:08,800 --> 00:00:11,280
welcome to

2
00:00:09,440 --> 00:00:13,440
privacy architecture for data driven

3
00:00:11,280 --> 00:00:15,040
innovation my name is derek care i'm a

4
00:00:13,440 --> 00:00:16,880
director from the

5
00:00:15,040 --> 00:00:18,320
privacy and cyber security legal team at

6
00:00:16,880 --> 00:00:20,240
uber

7
00:00:18,320 --> 00:00:21,920
hello there my name is nishan bajaria

8
00:00:20,240 --> 00:00:24,959
and i head the technical privacy

9
00:00:21,920 --> 00:00:26,880
organization at uber

10
00:00:24,960 --> 00:00:28,320
um and we're here to talk about privacy

11
00:00:26,880 --> 00:00:32,800
architecture um

12
00:00:28,320 --> 00:00:34,640
nishant can you go to the uh agenda

13
00:00:32,800 --> 00:00:36,239
so we have a lot of uh ground that we

14
00:00:34,640 --> 00:00:38,079
wanna cover here and we wanna start by

15
00:00:36,239 --> 00:00:38,800
giving you an overview of the privacy

16
00:00:38,079 --> 00:00:40,399
landscape

17
00:00:38,800 --> 00:00:42,078
for modern companies and what that means

18
00:00:40,399 --> 00:00:43,040
from a data collection and handling

19
00:00:42,079 --> 00:00:45,920
standpoint

20
00:00:43,040 --> 00:00:47,760
then we're going to talk about um how to

21
00:00:45,920 --> 00:00:50,879
deal with data collection and usage

22
00:00:47,760 --> 00:00:52,800
and data sharing and how we architect

23
00:00:50,879 --> 00:00:54,320
solutions for those what the process is

24
00:00:52,800 --> 00:00:56,399
and what technical solutions can be

25
00:00:54,320 --> 00:00:59,440
applied to each of these

26
00:00:56,399 --> 00:01:02,239
then we'll talk about uh some examples

27
00:00:59,440 --> 00:01:03,680
of problems that we've faced um and some

28
00:01:02,239 --> 00:01:05,679
lessons that we've learned

29
00:01:03,680 --> 00:01:06,880
uh through that process and we'll leave

30
00:01:05,680 --> 00:01:08,320
some time uh

31
00:01:06,880 --> 00:01:10,640
for questions at the end after this

32
00:01:08,320 --> 00:01:10,639
video

33
00:01:11,520 --> 00:01:14,880
so starting with the privacy landscape i

34
00:01:13,520 --> 00:01:15,759
think is anyone watching this video can

35
00:01:14,880 --> 00:01:17,839
appreciate

36
00:01:15,759 --> 00:01:18,880
modern companies collect a lot of data

37
00:01:17,840 --> 00:01:21,920
both personal data

38
00:01:18,880 --> 00:01:23,360
and business data um and it's really at

39
00:01:21,920 --> 00:01:26,240
the heart of everything a company does

40
00:01:23,360 --> 00:01:29,200
these days whether it's an app a website

41
00:01:26,240 --> 00:01:31,280
direct data collections from individuals

42
00:01:29,200 --> 00:01:34,880
the amount of data coming in

43
00:01:31,280 --> 00:01:38,159
is exponentially increasing all the time

44
00:01:34,880 --> 00:01:40,720
which places a lot of risks and costs on

45
00:01:38,159 --> 00:01:42,000
businesses and um businesses

46
00:01:40,720 --> 00:01:44,880
particularly when they're

47
00:01:42,000 --> 00:01:45,520
starting out have a difficult time uh

48
00:01:44,880 --> 00:01:47,119
assessing

49
00:01:45,520 --> 00:01:48,640
the risks before the data collection

50
00:01:47,119 --> 00:01:51,680
happens and usually

51
00:01:48,640 --> 00:01:53,200
the privacy and data handling risks are

52
00:01:51,680 --> 00:01:54,479
considered only after a

53
00:01:53,200 --> 00:01:56,799
tremendous amount of data has already

54
00:01:54,479 --> 00:01:57,840
been collected which makes it a lot more

55
00:01:56,799 --> 00:02:00,240
difficult

56
00:01:57,840 --> 00:02:02,159
to make risk decisions around data

57
00:02:00,240 --> 00:02:03,280
because you already have this data in

58
00:02:02,159 --> 00:02:05,280
your hands

59
00:02:03,280 --> 00:02:06,399
and i probably establish your systems

60
00:02:05,280 --> 00:02:08,800
for collecting

61
00:02:06,399 --> 00:02:10,479
using retaining data by the time you

62
00:02:08,800 --> 00:02:11,920
take into account the various privacy

63
00:02:10,479 --> 00:02:13,920
requirements that you may have

64
00:02:11,920 --> 00:02:15,359
uh or data handling requirements that

65
00:02:13,920 --> 00:02:17,280
you establish

66
00:02:15,360 --> 00:02:18,560
to help reduce costs and protect your

67
00:02:17,280 --> 00:02:22,000
data so

68
00:02:18,560 --> 00:02:24,879
these are a big set of problems uh

69
00:02:22,000 --> 00:02:27,360
for a lot of companies um can we go to

70
00:02:24,879 --> 00:02:27,359
the next slide

71
00:02:28,080 --> 00:02:32,160
um and the way that we deal with those

72
00:02:30,319 --> 00:02:34,799
problems um is a

73
00:02:32,160 --> 00:02:36,000
it has to be a comprehensive approach

74
00:02:34,800 --> 00:02:37,599
and when i say comprehensive

75
00:02:36,000 --> 00:02:39,920
i mean that you can't just have your

76
00:02:37,599 --> 00:02:40,480
privacy team or your legal team focused

77
00:02:39,920 --> 00:02:42,720
on

78
00:02:40,480 --> 00:02:44,079
uh getting privacy and getting cyber

79
00:02:42,720 --> 00:02:45,680
security correct

80
00:02:44,080 --> 00:02:48,000
it really has to be a cross-functional

81
00:02:45,680 --> 00:02:48,959
effort involving your legal teams your

82
00:02:48,000 --> 00:02:51,599
product teams

83
00:02:48,959 --> 00:02:53,120
engineering policy the general employee

84
00:02:51,599 --> 00:02:55,280
who learns about good

85
00:02:53,120 --> 00:02:56,879
data handling measures it really has to

86
00:02:55,280 --> 00:02:59,040
be an enterprise-wide thing otherwise

87
00:02:56,879 --> 00:03:01,120
you're not going to do it right

88
00:02:59,040 --> 00:03:02,480
and you also have to appreciate that

89
00:03:01,120 --> 00:03:04,560
when we say security

90
00:03:02,480 --> 00:03:05,679
it's not the same thing as privacy

91
00:03:04,560 --> 00:03:07,360
security

92
00:03:05,680 --> 00:03:09,360
and securing the data that you collect

93
00:03:07,360 --> 00:03:10,000
is a necessary step to maintaining good

94
00:03:09,360 --> 00:03:11,920
privacy

95
00:03:10,000 --> 00:03:13,840
but there's a lot else that goes into it

96
00:03:11,920 --> 00:03:15,920
minimizing what you collect and

97
00:03:13,840 --> 00:03:17,840
collecting only what you need and

98
00:03:15,920 --> 00:03:19,280
enabling people to exercise their rights

99
00:03:17,840 --> 00:03:21,360
those are all things that go

100
00:03:19,280 --> 00:03:24,000
into privacy as well as securing data

101
00:03:21,360 --> 00:03:25,200
and the classic data security piece

102
00:03:24,000 --> 00:03:27,280
that also means that we can't just be

103
00:03:25,200 --> 00:03:28,480
focused on data breaches uh there are a

104
00:03:27,280 --> 00:03:31,040
lot of privacy

105
00:03:28,480 --> 00:03:32,720
uh issues and requirements that don't

106
00:03:31,040 --> 00:03:35,760
entail

107
00:03:32,720 --> 00:03:37,359
external access or misuse of data

108
00:03:35,760 --> 00:03:39,679
and that has to go into the calculation

109
00:03:37,360 --> 00:03:39,680
as well

110
00:03:41,840 --> 00:03:47,760
so how do we

111
00:03:45,280 --> 00:03:48,560
approach uh these issues that i've just

112
00:03:47,760 --> 00:03:50,560
described

113
00:03:48,560 --> 00:03:52,879
um we've found that there are a number

114
00:03:50,560 --> 00:03:55,120
of uh steps that go into

115
00:03:52,879 --> 00:03:57,200
uh tackling these problems and they

116
00:03:55,120 --> 00:03:58,319
begin with actually defining what data

117
00:03:57,200 --> 00:04:00,640
that you have

118
00:03:58,319 --> 00:04:01,439
looking at all the various types of data

119
00:04:00,640 --> 00:04:04,720
that you have

120
00:04:01,439 --> 00:04:05,680
and uh uh determining based on its

121
00:04:04,720 --> 00:04:08,239
sensitivity

122
00:04:05,680 --> 00:04:10,319
uh what type of handling guidelines or

123
00:04:08,239 --> 00:04:12,000
standards should apply to that data

124
00:04:10,319 --> 00:04:13,280
um then the next step is actually

125
00:04:12,000 --> 00:04:14,080
setting those guidelines so that you

126
00:04:13,280 --> 00:04:15,840
know depending

127
00:04:14,080 --> 00:04:18,079
on the sensitivity or risk involved in

128
00:04:15,840 --> 00:04:19,840
each data the level of handling

129
00:04:18,079 --> 00:04:21,759
requirements to set with of course the

130
00:04:19,839 --> 00:04:22,078
most sensitive data being subject to the

131
00:04:21,759 --> 00:04:25,040
most

132
00:04:22,079 --> 00:04:26,560
stringent requirements the next step in

133
00:04:25,040 --> 00:04:28,000
the process is actually inventorying

134
00:04:26,560 --> 00:04:30,800
your data so that you know

135
00:04:28,000 --> 00:04:33,199
all the data that you have and into

136
00:04:30,800 --> 00:04:35,680
which classifications and handling

137
00:04:33,199 --> 00:04:38,000
requirements apply across your data that

138
00:04:35,680 --> 00:04:40,720
finally allows you to enforce

139
00:04:38,000 --> 00:04:41,199
these guidelines or handling standards

140
00:04:40,720 --> 00:04:43,040
that you

141
00:04:41,199 --> 00:04:44,240
determine because you know where your

142
00:04:43,040 --> 00:04:46,479
data is

143
00:04:44,240 --> 00:04:47,840
what level of sensitivity it is and what

144
00:04:46,479 --> 00:04:50,639
the requirements that attach to that

145
00:04:47,840 --> 00:04:50,638
data actually are

146
00:04:53,520 --> 00:04:58,799
step one is a fundamental step

147
00:04:57,120 --> 00:05:00,400
and it's a very difficult step as i

148
00:04:58,800 --> 00:05:01,759
mentioned earlier a lot of companies

149
00:05:00,400 --> 00:05:03,758
collect data

150
00:05:01,759 --> 00:05:06,000
and then uh worry about the risks

151
00:05:03,759 --> 00:05:08,240
involved

152
00:05:06,000 --> 00:05:10,560
and so a lot of companies are faced with

153
00:05:08,240 --> 00:05:12,160
the challenge of looking at this a

154
00:05:10,560 --> 00:05:14,240
huge amount of data that they have and

155
00:05:12,160 --> 00:05:14,880
actually figuring out what data they

156
00:05:14,240 --> 00:05:16,560
have

157
00:05:14,880 --> 00:05:18,639
what different types of data that they

158
00:05:16,560 --> 00:05:20,080
have and what level of sensitivity

159
00:05:18,639 --> 00:05:22,400
attaches to all of this

160
00:05:20,080 --> 00:05:23,520
different levels of data and when i say

161
00:05:22,400 --> 00:05:25,840
sensitivity i mean

162
00:05:23,520 --> 00:05:27,280
pretty much the inherent risks involved

163
00:05:25,840 --> 00:05:29,919
in the data you're collecting

164
00:05:27,280 --> 00:05:31,198
that can be very sensitive things like a

165
00:05:29,919 --> 00:05:33,520
social security number

166
00:05:31,199 --> 00:05:34,320
a credit card number health data or it

167
00:05:33,520 --> 00:05:37,599
can be

168
00:05:34,320 --> 00:05:38,320
less sensitive yet nonetheless personal

169
00:05:37,600 --> 00:05:40,320
data that

170
00:05:38,320 --> 00:05:41,440
to which requirements apply things like

171
00:05:40,320 --> 00:05:44,560
emails or

172
00:05:41,440 --> 00:05:47,360
a user id um or someone's

173
00:05:44,560 --> 00:05:49,039
usage history for your app um that

174
00:05:47,360 --> 00:05:50,639
obviously presents a very different

175
00:05:49,039 --> 00:05:52,960
level of sensitivity than does health

176
00:05:50,639 --> 00:05:54,479
data but once you've looked at all the

177
00:05:52,960 --> 00:05:56,080
different type of data you have you can

178
00:05:54,479 --> 00:05:57,280
break it out into different levels of

179
00:05:56,080 --> 00:05:57,840
data so that you can define the

180
00:05:57,280 --> 00:05:59,599
different

181
00:05:57,840 --> 00:06:01,440
levels of sensitivity and handling

182
00:05:59,600 --> 00:06:04,880
requirements that apply

183
00:06:01,440 --> 00:06:04,880
can we go to the next slide

184
00:06:04,960 --> 00:06:10,799
so you can see here a um very basic um

185
00:06:08,400 --> 00:06:12,719
categorization uh tier one to four with

186
00:06:10,800 --> 00:06:14,880
tier one being the highest uh

187
00:06:12,720 --> 00:06:16,639
level of sensitivity uh and again that

188
00:06:14,880 --> 00:06:17,039
covers things like your social security

189
00:06:16,639 --> 00:06:18,880
uh

190
00:06:17,039 --> 00:06:20,880
number your driver's license things that

191
00:06:18,880 --> 00:06:22,880
are typically defined by law

192
00:06:20,880 --> 00:06:24,400
as being the most sensitive uh

193
00:06:22,880 --> 00:06:27,440
categories of data

194
00:06:24,400 --> 00:06:30,479
um on the other end um we've got

195
00:06:27,440 --> 00:06:32,639
public information which

196
00:06:30,479 --> 00:06:34,080
is the lowest level of sensitivity

197
00:06:32,639 --> 00:06:34,960
necessarily since it's already shared

198
00:06:34,080 --> 00:06:37,440
with the world

199
00:06:34,960 --> 00:06:38,239
but which nonetheless um will present

200
00:06:37,440 --> 00:06:40,319
requirements

201
00:06:38,240 --> 00:06:42,080
um under law and under your own data

202
00:06:40,319 --> 00:06:43,280
handling um so you need to keep that in

203
00:06:42,080 --> 00:06:45,680
mind even if

204
00:06:43,280 --> 00:06:47,280
you um focus less of your efforts than

205
00:06:45,680 --> 00:06:50,800
on that than you do on say your most

206
00:06:47,280 --> 00:06:50,799
sensitive categories of data

207
00:06:52,880 --> 00:06:57,599
okay thank you derek so as a continuum

208
00:06:55,599 --> 00:06:58,000
to this entire process let me talk about

209
00:06:57,599 --> 00:06:59,680
in

210
00:06:58,000 --> 00:07:02,160
data classification from a holistic

211
00:06:59,680 --> 00:07:03,840
perspective the whole reason we classify

212
00:07:02,160 --> 00:07:06,240
data like direct mention is to make

213
00:07:03,840 --> 00:07:07,919
informed prudent quantifiable decisions

214
00:07:06,240 --> 00:07:09,680
around how you protect data

215
00:07:07,919 --> 00:07:11,440
if you have data that is for example

216
00:07:09,680 --> 00:07:12,080
sufficiently aggregated to the point

217
00:07:11,440 --> 00:07:14,400
where

218
00:07:12,080 --> 00:07:16,080
individuals cannot be identified or if

219
00:07:14,400 --> 00:07:17,758
you have data that is already public

220
00:07:16,080 --> 00:07:19,440
you probably do not want to throw the

221
00:07:17,759 --> 00:07:21,039
kitchen sink at protecting that data it

222
00:07:19,440 --> 00:07:22,719
may not make sense

223
00:07:21,039 --> 00:07:24,639
however if you have data that uniquely

224
00:07:22,720 --> 00:07:26,000
identify someone for example in the uber

225
00:07:24,639 --> 00:07:27,520
context who you are

226
00:07:26,000 --> 00:07:29,039
where you traveled or who you are what

227
00:07:27,520 --> 00:07:31,440
you ate and it points

228
00:07:29,039 --> 00:07:33,199
out that the data is about you when you

229
00:07:31,440 --> 00:07:35,120
have that level of risk and trust

230
00:07:33,199 --> 00:07:36,319
at stake you want to make sure that you

231
00:07:35,120 --> 00:07:37,840
apply the highest levels of

232
00:07:36,319 --> 00:07:39,680
classification possible

233
00:07:37,840 --> 00:07:41,520
and the most aggressive sorts of access

234
00:07:39,680 --> 00:07:43,919
control deletion data retention

235
00:07:41,520 --> 00:07:44,799
data minimization controls possible so

236
00:07:43,919 --> 00:07:46,719
if you think about

237
00:07:44,800 --> 00:07:48,400
data being classified along several

238
00:07:46,720 --> 00:07:50,479
dimensions there are corresponding

239
00:07:48,400 --> 00:07:52,878
dimensions to how you protect the data

240
00:07:50,479 --> 00:07:54,800
that classification enables you to do so

241
00:07:52,879 --> 00:07:57,360
at uber we put data through the lens of

242
00:07:54,800 --> 00:07:59,440
what can we collect how do we access it

243
00:07:57,360 --> 00:08:00,800
how do we retain delete and share it

244
00:07:59,440 --> 00:08:01,440
both from an internal external

245
00:08:00,800 --> 00:08:03,039
perspective

246
00:08:01,440 --> 00:08:05,199
but that is the fruit of the data

247
00:08:03,039 --> 00:08:07,840
classification process so it's not just

248
00:08:05,199 --> 00:08:10,000
purely conceptual in nature

249
00:08:07,840 --> 00:08:11,758
so data classification and handling

250
00:08:10,000 --> 00:08:12,000
leads us to the third critical step

251
00:08:11,759 --> 00:08:14,479
which

252
00:08:12,000 --> 00:08:16,720
is inventory what we've talked about so

253
00:08:14,479 --> 00:08:18,240
far when it comes to data classification

254
00:08:16,720 --> 00:08:19,919
may seem a little theoretical and of

255
00:08:18,240 --> 00:08:20,800
course you have legal internal audit

256
00:08:19,919 --> 00:08:23,120
engineering

257
00:08:20,800 --> 00:08:24,400
data science product management security

258
00:08:23,120 --> 00:08:25,759
but the key is to make

259
00:08:24,400 --> 00:08:27,840
those sorts of cross-functional

260
00:08:25,759 --> 00:08:30,000
decisions early in the process so you

261
00:08:27,840 --> 00:08:31,039
can then correspondingly tag that data

262
00:08:30,000 --> 00:08:32,320
early in the process

263
00:08:31,039 --> 00:08:34,880
so as an example if your data

264
00:08:32,320 --> 00:08:37,039
classification represents tier one

265
00:08:34,880 --> 00:08:38,479
which is the most sensitive data you

266
00:08:37,039 --> 00:08:40,559
might have underscore

267
00:08:38,479 --> 00:08:42,320
government id underscore driver's

268
00:08:40,559 --> 00:08:43,760
license that is something that's

269
00:08:42,320 --> 00:08:46,160
important because you will want to make

270
00:08:43,760 --> 00:08:48,399
important security decisions based on

271
00:08:46,160 --> 00:08:50,319
that assessment and that classification

272
00:08:48,399 --> 00:08:52,399
so when do you apply that classification

273
00:08:50,320 --> 00:08:54,560
at what point in the data life cycle

274
00:08:52,399 --> 00:08:56,640
so if you think about in this slide your

275
00:08:54,560 --> 00:08:57,119
data pipeline being sort of a horizontal

276
00:08:56,640 --> 00:08:59,839
funnel

277
00:08:57,120 --> 00:09:00,880
as data comes into your system it grows

278
00:08:59,839 --> 00:09:03,680
based on collection

279
00:09:00,880 --> 00:09:04,720
inferences copying models sharing buying

280
00:09:03,680 --> 00:09:07,199
etc

281
00:09:04,720 --> 00:09:08,560
when all of that happens your data size

282
00:09:07,200 --> 00:09:10,480
grows from left to right

283
00:09:08,560 --> 00:09:12,399
so you will want to apply these tags to

284
00:09:10,480 --> 00:09:14,560
your data as far left as possible which

285
00:09:12,399 --> 00:09:16,399
is as early in the process which means

286
00:09:14,560 --> 00:09:18,160
hopefully before somebody accesses

287
00:09:16,399 --> 00:09:18,880
processes or shares that data with

288
00:09:18,160 --> 00:09:21,040
anyone else

289
00:09:18,880 --> 00:09:22,959
so before any irreversible decisions are

290
00:09:21,040 --> 00:09:24,640
made you will want to try and apply that

291
00:09:22,959 --> 00:09:26,800
classification to your data

292
00:09:24,640 --> 00:09:28,560
in other words apply data inventory

293
00:09:26,800 --> 00:09:29,920
simply put data inventory is like

294
00:09:28,560 --> 00:09:31,439
building an indexed and searchable

295
00:09:29,920 --> 00:09:32,880
database for your data much like a

296
00:09:31,440 --> 00:09:34,800
google search

297
00:09:32,880 --> 00:09:36,880
i'll let direct speak to sort of where

298
00:09:34,800 --> 00:09:39,599
inventory matters from a larger

299
00:09:36,880 --> 00:09:40,640
big picture regulatory perspective yeah

300
00:09:39,600 --> 00:09:42,800
thank you nishant

301
00:09:40,640 --> 00:09:44,560
from a legal perspective data inventory

302
00:09:42,800 --> 00:09:46,560
is so important because

303
00:09:44,560 --> 00:09:47,680
there are requirements as well as costs

304
00:09:46,560 --> 00:09:50,160
that apply pretty much

305
00:09:47,680 --> 00:09:50,880
whenever you're collecting personal data

306
00:09:50,160 --> 00:09:52,640
for example

307
00:09:50,880 --> 00:09:55,439
under the eu's general data protection

308
00:09:52,640 --> 00:09:57,920
regulation there are requirements to

309
00:09:55,440 --> 00:09:59,839
delete a person's data upon request to

310
00:09:57,920 --> 00:10:02,959
deliver a copy of the data

311
00:09:59,839 --> 00:10:04,000
upon request and these rights can't be

312
00:10:02,959 --> 00:10:06,000
exercised in

313
00:10:04,000 --> 00:10:08,720
full if you don't know where all of your

314
00:10:06,000 --> 00:10:10,800
data is you may not delete all the data

315
00:10:08,720 --> 00:10:12,720
that a person's entitled to have deleted

316
00:10:10,800 --> 00:10:13,519
or deliver them a complete copy of their

317
00:10:12,720 --> 00:10:16,160
data

318
00:10:13,519 --> 00:10:17,519
um that one frustrates the intent of

319
00:10:16,160 --> 00:10:19,680
laws like gdpr

320
00:10:17,519 --> 00:10:21,200
um and puts company at risk of

321
00:10:19,680 --> 00:10:24,160
non-compliance with those laws

322
00:10:21,200 --> 00:10:26,160
which exposes companies to enormous uh

323
00:10:24,160 --> 00:10:29,279
potential fines and other sanctions

324
00:10:26,160 --> 00:10:31,839
so um that's the first reason but

325
00:10:29,279 --> 00:10:32,800
um the costs of this data are another

326
00:10:31,839 --> 00:10:36,880
reason

327
00:10:32,800 --> 00:10:40,399
naturally storing data it costs money

328
00:10:36,880 --> 00:10:42,480
and the more data that you store

329
00:10:40,399 --> 00:10:44,560
the the greater those costs if your

330
00:10:42,480 --> 00:10:46,959
inventory is not complete you may

331
00:10:44,560 --> 00:10:49,359
not have visibility into what data you

332
00:10:46,959 --> 00:10:52,160
need to hold on to and what data you

333
00:10:49,360 --> 00:10:52,880
can delete and as a result end up

334
00:10:52,160 --> 00:10:56,000
retaining

335
00:10:52,880 --> 00:10:58,000
unnecessary amounts of data for a long

336
00:10:56,000 --> 00:10:59,600
or indefinite period of time

337
00:10:58,000 --> 00:11:01,040
when you don't have to which is both a

338
00:10:59,600 --> 00:11:03,600
cost issue

339
00:11:01,040 --> 00:11:07,040
as well as again a compliance issue um

340
00:11:03,600 --> 00:11:07,040
when it comes to laws like gdpr

341
00:11:07,120 --> 00:11:12,320
okay perfect so let me take the queue

342
00:11:10,480 --> 00:11:14,079
here and talk about the data inventory

343
00:11:12,320 --> 00:11:15,760
system because for a lot of you

344
00:11:14,079 --> 00:11:17,199
building out that system is where the

345
00:11:15,760 --> 00:11:18,959
bulk of the work is because

346
00:11:17,200 --> 00:11:20,880
typically you collect a ton of data like

347
00:11:18,959 --> 00:11:22,319
derek mentioned and at a much later

348
00:11:20,880 --> 00:11:23,920
stage you start building out a system

349
00:11:22,320 --> 00:11:24,640
once you have a sense of how fast your

350
00:11:23,920 --> 00:11:27,040
data is

351
00:11:24,640 --> 00:11:28,720
so at uber we had to build a full

352
00:11:27,040 --> 00:11:29,199
combined system infrastructure that

353
00:11:28,720 --> 00:11:31,279
could

354
00:11:29,200 --> 00:11:33,360
crawl databases that could discover data

355
00:11:31,279 --> 00:11:35,680
cells that we didn't know existed in a

356
00:11:33,360 --> 00:11:37,279
largely bottomed up environment we then

357
00:11:35,680 --> 00:11:38,239
had to make the metadata available from

358
00:11:37,279 --> 00:11:39,839
those data sets

359
00:11:38,240 --> 00:11:41,440
and then enable people to add more

360
00:11:39,839 --> 00:11:42,959
metadata so we could understand what the

361
00:11:41,440 --> 00:11:44,079
data is that we had discovered so the

362
00:11:42,959 --> 00:11:45,760
first challenge is

363
00:11:44,079 --> 00:11:47,680
find out where the data lives then

364
00:11:45,760 --> 00:11:49,439
understand what it is then add more

365
00:11:47,680 --> 00:11:50,800
context to it to then get a better

366
00:11:49,440 --> 00:11:51,519
understanding and hopefully a better

367
00:11:50,800 --> 00:11:53,680
assessment

368
00:11:51,519 --> 00:11:55,519
of the classification process and then

369
00:11:53,680 --> 00:11:56,160
of course support those categorizations

370
00:11:55,519 --> 00:11:57,600
of data

371
00:11:56,160 --> 00:11:59,839
and this is where you can see that the

372
00:11:57,600 --> 00:12:01,600
data science team the marketing teams

373
00:11:59,839 --> 00:12:03,519
the ml teams and the privacy teams can

374
00:12:01,600 --> 00:12:04,720
work together this is not just something

375
00:12:03,519 --> 00:12:06,639
you do for privacy

376
00:12:04,720 --> 00:12:08,720
there is a strong business use case to

377
00:12:06,639 --> 00:12:10,480
do the right thing with data

378
00:12:08,720 --> 00:12:12,240
so we have something called the umis

379
00:12:10,480 --> 00:12:13,040
which is the mirror data management

380
00:12:12,240 --> 00:12:15,200
service

381
00:12:13,040 --> 00:12:16,639
and it is a key part of our inventory

382
00:12:15,200 --> 00:12:19,440
efforts and as you can see

383
00:12:16,639 --> 00:12:20,959
it places data ingestion in context with

384
00:12:19,440 --> 00:12:23,040
overall data governance so you first

385
00:12:20,959 --> 00:12:24,319
want to settle on your classification

386
00:12:23,040 --> 00:12:26,560
you then want to convert that

387
00:12:24,320 --> 00:12:28,639
classification to tags and then apply

388
00:12:26,560 --> 00:12:29,518
those tags and the policies attendant to

389
00:12:28,639 --> 00:12:31,200
those tags

390
00:12:29,519 --> 00:12:33,200
at the early point of ingesting the data

391
00:12:31,200 --> 00:12:34,399
so that's hopefully step number three

392
00:12:33,200 --> 00:12:36,560
which is the very beginning of the

393
00:12:34,399 --> 00:12:38,320
funnel that we saw a few slides ago

394
00:12:36,560 --> 00:12:40,000
and then you start ingesting data so

395
00:12:38,320 --> 00:12:41,600
realistically that may not happen

396
00:12:40,000 --> 00:12:43,200
but this is how you want to get to this

397
00:12:41,600 --> 00:12:43,839
is the level of maturity you want to get

398
00:12:43,200 --> 00:12:45,360
to

399
00:12:43,839 --> 00:12:47,200
where the moment you ingest data you

400
00:12:45,360 --> 00:12:48,800
have the tags ready and then

401
00:12:47,200 --> 00:12:50,639
your diagram would look something like

402
00:12:48,800 --> 00:12:53,439
this more or less

403
00:12:50,639 --> 00:12:54,399
let's look at the ums back end as you

404
00:12:53,440 --> 00:12:55,839
can see here

405
00:12:54,399 --> 00:12:57,760
on the left-hand side we have the

406
00:12:55,839 --> 00:12:59,920
collection part of the data where you

407
00:12:57,760 --> 00:13:01,360
collect not just the data itself

408
00:12:59,920 --> 00:13:03,439
but you also collect different

409
00:13:01,360 --> 00:13:05,680
infrastructure structural points that is

410
00:13:03,440 --> 00:13:06,480
crawlers uis apis that help you discover

411
00:13:05,680 --> 00:13:08,079
the data

412
00:13:06,480 --> 00:13:10,240
and you create the metadata and the

413
00:13:08,079 --> 00:13:11,760
rules attended to the data itself

414
00:13:10,240 --> 00:13:13,760
all of that ends up in different

415
00:13:11,760 --> 00:13:15,439
classifiers which are partly manual

416
00:13:13,760 --> 00:13:16,560
partly mlpowered they of course learn

417
00:13:15,440 --> 00:13:17,920
from each other

418
00:13:16,560 --> 00:13:19,920
and then you apply the initial

419
00:13:17,920 --> 00:13:21,439
classification to the data that is step

420
00:13:19,920 --> 00:13:22,560
number two in the middle here at the

421
00:13:21,440 --> 00:13:24,959
bottom

422
00:13:22,560 --> 00:13:26,959
that then pushes all of that data into a

423
00:13:24,959 --> 00:13:29,040
temporary inventory database where you

424
00:13:26,959 --> 00:13:31,839
then go through additional checks and

425
00:13:29,040 --> 00:13:33,439
then you put final classification tags

426
00:13:31,839 --> 00:13:35,680
we do that at uber because we want to be

427
00:13:33,440 --> 00:13:36,320
certain that our data is as well tagged

428
00:13:35,680 --> 00:13:38,638
as possible

429
00:13:36,320 --> 00:13:40,560
given that we care about user trust and

430
00:13:38,639 --> 00:13:42,399
safety and a lot of the data that we

431
00:13:40,560 --> 00:13:43,920
can collect from a location perspective

432
00:13:42,399 --> 00:13:45,279
is critical for us to protect and that

433
00:13:43,920 --> 00:13:46,000
is why this additional investment is

434
00:13:45,279 --> 00:13:48,800
important

435
00:13:46,000 --> 00:13:50,880
that data then ends up again in ums as

436
00:13:48,800 --> 00:13:54,079
classified in inventory data

437
00:13:50,880 --> 00:13:56,079
so to that point ums serves as two

438
00:13:54,079 --> 00:13:58,959
different points of contact for our data

439
00:13:56,079 --> 00:14:00,800
untagged highly voluminous data and then

440
00:13:58,959 --> 00:14:01,279
the tag data that is hopefully smaller

441
00:14:00,800 --> 00:14:02,800
leaner

442
00:14:01,279 --> 00:14:05,519
and more reflective of the priority

443
00:14:02,800 --> 00:14:06,959
attached to it so the ums is privacy

444
00:14:05,519 --> 00:14:08,639
central as i mentioned

445
00:14:06,959 --> 00:14:10,160
a quick look at the back end if you look

446
00:14:08,639 --> 00:14:12,160
at all of our infrastructure when it

447
00:14:10,160 --> 00:14:14,319
comes to data inventory the ums

448
00:14:12,160 --> 00:14:15,279
it has full access to all the etl

449
00:14:14,320 --> 00:14:17,440
pipelines

450
00:14:15,279 --> 00:14:18,800
ranging from the pipelines themselves to

451
00:14:17,440 --> 00:14:20,160
real-time and batch data collection

452
00:14:18,800 --> 00:14:20,880
because we do not want to make any

453
00:14:20,160 --> 00:14:23,439
assumptions

454
00:14:20,880 --> 00:14:25,760
about data and its privacy risks before

455
00:14:23,440 --> 00:14:27,519
we discover and classify it

456
00:14:25,760 --> 00:14:29,120
and then of course we pay attention to

457
00:14:27,519 --> 00:14:30,639
defining metadata because each

458
00:14:29,120 --> 00:14:31,519
individual metadata component will

459
00:14:30,639 --> 00:14:33,440
determine

460
00:14:31,519 --> 00:14:34,880
the classification that might be applied

461
00:14:33,440 --> 00:14:36,240
to the data so this example on the

462
00:14:34,880 --> 00:14:38,240
screen is instructive

463
00:14:36,240 --> 00:14:39,680
a more simple example or rather a

464
00:14:38,240 --> 00:14:42,000
simpler example would be you could

465
00:14:39,680 --> 00:14:45,279
define two different entity types

466
00:14:42,000 --> 00:14:45,920
for a location type value type of gps

467
00:14:45,279 --> 00:14:47,680
coordinate

468
00:14:45,920 --> 00:14:49,760
and if you have five decimal points you

469
00:14:47,680 --> 00:14:51,599
could apply entity type as being

470
00:14:49,760 --> 00:14:53,360
tier one which is somewhat more

471
00:14:51,600 --> 00:14:53,920
sensitive if you have just two decimal

472
00:14:53,360 --> 00:14:55,519
points

473
00:14:53,920 --> 00:14:57,279
that is much more approximate data and

474
00:14:55,519 --> 00:14:59,120
you could apply a different entity type

475
00:14:57,279 --> 00:15:01,199
and that's how you can apply more custom

476
00:14:59,120 --> 00:15:03,440
protections to location data

477
00:15:01,199 --> 00:15:06,240
depending upon how specific it is to a

478
00:15:03,440 --> 00:15:07,600
given location based on gps accuracy so

479
00:15:06,240 --> 00:15:09,519
we're allowing our engineers to have

480
00:15:07,600 --> 00:15:11,440
that level of flexibility with the data

481
00:15:09,519 --> 00:15:13,839
as long as we can with reasonable

482
00:15:11,440 --> 00:15:15,600
confidence tag it

483
00:15:13,839 --> 00:15:17,199
and of course this is where we explain

484
00:15:15,600 --> 00:15:19,040
what our event listeners crawlers and

485
00:15:17,199 --> 00:15:20,479
api controllers look like in terms of

486
00:15:19,040 --> 00:15:21,519
obtaining metadata

487
00:15:20,480 --> 00:15:24,160
and making sure that the right

488
00:15:21,519 --> 00:15:25,600
classification decisions are made

489
00:15:24,160 --> 00:15:28,160
and then finally before we jump into

490
00:15:25,600 --> 00:15:29,440
some use cases the different models we

491
00:15:28,160 --> 00:15:32,719
have in terms of how we mix

492
00:15:29,440 --> 00:15:34,240
ml and human tagging apply for different

493
00:15:32,720 --> 00:15:34,880
levels of coverage accuracy and

494
00:15:34,240 --> 00:15:36,639
performance

495
00:15:34,880 --> 00:15:38,240
levels so you will want to make some

496
00:15:36,639 --> 00:15:40,000
trade-offs here depending upon your use

497
00:15:38,240 --> 00:15:43,199
cases or even apply them on a bespoke

498
00:15:40,000 --> 00:15:45,680
team basis given that context we

499
00:15:43,199 --> 00:15:47,680
i want to have direct talk about how he

500
00:15:45,680 --> 00:15:49,359
as a technically minor attorney looks at

501
00:15:47,680 --> 00:15:51,758
the success or failure of a privacy

502
00:15:49,360 --> 00:15:51,759
program

503
00:15:52,160 --> 00:15:55,519
thank you nishan um so a couple

504
00:15:54,000 --> 00:15:56,959
learnings that um

505
00:15:55,519 --> 00:15:58,560
we've been able to take away from our

506
00:15:56,959 --> 00:16:01,199
experiences at uber

507
00:15:58,560 --> 00:16:03,359
um the first is that you really come to

508
00:16:01,199 --> 00:16:03,920
appreciate how much data you have and

509
00:16:03,360 --> 00:16:06,320
how much

510
00:16:03,920 --> 00:16:07,680
uh discovery there is that goes into

511
00:16:06,320 --> 00:16:10,000
this process

512
00:16:07,680 --> 00:16:12,319
for example we learned not only about

513
00:16:10,000 --> 00:16:12,959
the sheer volume of data that we have

514
00:16:12,320 --> 00:16:14,800
but also

515
00:16:12,959 --> 00:16:17,040
more about the types of data that we

516
00:16:14,800 --> 00:16:18,639
have as we started doing the

517
00:16:17,040 --> 00:16:20,000
classification and rolling out handling

518
00:16:18,639 --> 00:16:22,240
guidelines we did hear

519
00:16:20,000 --> 00:16:23,759
got a lot of feedback from our engineers

520
00:16:22,240 --> 00:16:25,839
and from our product teams

521
00:16:23,759 --> 00:16:27,120
um about different types of data that we

522
00:16:25,839 --> 00:16:28,639
didn't uh

523
00:16:27,120 --> 00:16:30,240
appreciate that we collected or

524
00:16:28,639 --> 00:16:31,199
variations on the types of data that we

525
00:16:30,240 --> 00:16:32,880
collected

526
00:16:31,199 --> 00:16:34,639
and then we had to address the

527
00:16:32,880 --> 00:16:36,480
requirements the

528
00:16:34,639 --> 00:16:38,160
classification level and other issues

529
00:16:36,480 --> 00:16:39,360
that went into that so there's just a

530
00:16:38,160 --> 00:16:42,639
lot of discovery

531
00:16:39,360 --> 00:16:45,040
um that helps you appreciate the the

532
00:16:42,639 --> 00:16:46,240
sheer scope of the data that um you're

533
00:16:45,040 --> 00:16:49,279
collecting

534
00:16:46,240 --> 00:16:50,000
um you also realize that there comes a

535
00:16:49,279 --> 00:16:53,120
point with

536
00:16:50,000 --> 00:16:56,480
your uh data collection and uses

537
00:16:53,120 --> 00:16:58,720
that uh your costs around this

538
00:16:56,480 --> 00:17:00,880
uh hidden inflection point meaning that

539
00:16:58,720 --> 00:17:03,279
you're collecting or replicating

540
00:17:00,880 --> 00:17:04,400
so much data that it presents a threat

541
00:17:03,279 --> 00:17:06,720
to the business from a

542
00:17:04,400 --> 00:17:08,079
cost perspective and that's the point

543
00:17:06,720 --> 00:17:10,640
generally when the

544
00:17:08,079 --> 00:17:11,119
um old school way of just collecting

545
00:17:10,640 --> 00:17:13,199
data

546
00:17:11,119 --> 00:17:14,639
and storing it forever becomes

547
00:17:13,199 --> 00:17:17,280
unsustainable and you

548
00:17:14,640 --> 00:17:18,000
really have to adopt um data handling

549
00:17:17,280 --> 00:17:20,799
guidelines

550
00:17:18,000 --> 00:17:22,079
uh as a way to keep those costs in check

551
00:17:20,799 --> 00:17:25,359
um and that leads into

552
00:17:22,079 --> 00:17:26,079
a deletion program um one of the upsides

553
00:17:25,359 --> 00:17:29,520
of this

554
00:17:26,079 --> 00:17:32,240
um classification and um

555
00:17:29,520 --> 00:17:33,679
handling uh process is that you get a

556
00:17:32,240 --> 00:17:34,240
better sense of what data you need to

557
00:17:33,679 --> 00:17:36,400
have

558
00:17:34,240 --> 00:17:37,520
versus what data you don't and that data

559
00:17:36,400 --> 00:17:40,080
you don't need to have

560
00:17:37,520 --> 00:17:42,320
from both a cost and privacy requirement

561
00:17:40,080 --> 00:17:44,559
standpoint you should be deleting it

562
00:17:42,320 --> 00:17:45,520
and this process uh facilitates that

563
00:17:44,559 --> 00:17:47,840
greatly

564
00:17:45,520 --> 00:17:49,679
finally in terms of data quality as

565
00:17:47,840 --> 00:17:51,280
you're taking a closer look at this data

566
00:17:49,679 --> 00:17:54,720
you do discover

567
00:17:51,280 --> 00:17:57,520
issues that that may be limiting the

568
00:17:54,720 --> 00:17:58,559
utility of the data for example you

569
00:17:57,520 --> 00:18:00,400
might have

570
00:17:58,559 --> 00:18:03,039
same types of data that are named

571
00:18:00,400 --> 00:18:05,280
differently in different tables

572
00:18:03,039 --> 00:18:06,240
which limits your ability to use that

573
00:18:05,280 --> 00:18:11,120
data or

574
00:18:06,240 --> 00:18:14,559
to track that data and we've had to

575
00:18:11,120 --> 00:18:15,760
align and improve our naming conventions

576
00:18:14,559 --> 00:18:17,760
our data quality

577
00:18:15,760 --> 00:18:18,799
uh etc as we've been going through this

578
00:18:17,760 --> 00:18:21,039
process which of course

579
00:18:18,799 --> 00:18:22,480
is useful not just to those of us who

580
00:18:21,039 --> 00:18:23,840
are approaching this as a data handling

581
00:18:22,480 --> 00:18:25,039
issue but those such as the data

582
00:18:23,840 --> 00:18:27,120
scientists who actually

583
00:18:25,039 --> 00:18:29,039
use this data in the course of their

584
00:18:27,120 --> 00:18:31,280
work

585
00:18:29,039 --> 00:18:33,840
okay thank you derek so we don't have a

586
00:18:31,280 --> 00:18:35,760
whole lot of time but i want to close

587
00:18:33,840 --> 00:18:37,600
with some insights and maybe some

588
00:18:35,760 --> 00:18:38,879
homework for this team as a whole for

589
00:18:37,600 --> 00:18:40,159
everybody in the audience

590
00:18:38,880 --> 00:18:41,360
for the last one third of this

591
00:18:40,160 --> 00:18:42,080
presentation derek and i have put

592
00:18:41,360 --> 00:18:43,600
together

593
00:18:42,080 --> 00:18:45,199
recommendations on how you can apply

594
00:18:43,600 --> 00:18:46,639
some of these privacy controls before

595
00:18:45,200 --> 00:18:48,080
data leaves the company

596
00:18:46,640 --> 00:18:49,679
and for us data sharing is not a

597
00:18:48,080 --> 00:18:51,360
conceptual exercise we have to share

598
00:18:49,679 --> 00:18:53,200
data with municipalities

599
00:18:51,360 --> 00:18:54,639
with governments for example to study

600
00:18:53,200 --> 00:18:56,480
parking infrastructure and forced

601
00:18:54,640 --> 00:18:58,240
traffic laws what have you so for us

602
00:18:56,480 --> 00:18:59,280
it's not a choice we have to apply

603
00:18:58,240 --> 00:19:01,280
privacy controls

604
00:18:59,280 --> 00:19:02,399
and share data in a way that risk is

605
00:19:01,280 --> 00:19:05,280
meaningfully addressed

606
00:19:02,400 --> 00:19:05,760
and we protect our customers trust uh so

607
00:19:05,280 --> 00:19:07,760
we have

608
00:19:05,760 --> 00:19:08,960
looked at how we apply data sharing

609
00:19:07,760 --> 00:19:11,200
protections to

610
00:19:08,960 --> 00:19:12,320
by by way of geolocation trip telemetry

611
00:19:11,200 --> 00:19:13,840
how you handle ids

612
00:19:12,320 --> 00:19:16,320
how you handle the processing of the

613
00:19:13,840 --> 00:19:18,080
data how you round off different aspects

614
00:19:16,320 --> 00:19:20,159
where data is defined in terms of for

615
00:19:18,080 --> 00:19:21,039
example pick up and drop off times gps

616
00:19:20,160 --> 00:19:22,960
locations

617
00:19:21,039 --> 00:19:24,160
we've looked at a limitation use case

618
00:19:22,960 --> 00:19:25,520
where for example

619
00:19:24,160 --> 00:19:27,679
we thought data was sufficiently

620
00:19:25,520 --> 00:19:29,360
anonymized but we could hone it down to

621
00:19:27,679 --> 00:19:30,559
something very very specific in terms of

622
00:19:29,360 --> 00:19:32,320
somebody using

623
00:19:30,559 --> 00:19:34,879
medical facilities so there's a voice of

624
00:19:32,320 --> 00:19:36,799
caution we've also discussed k anonymity

625
00:19:34,880 --> 00:19:38,960
and l diversity for example

626
00:19:36,799 --> 00:19:40,480
so we looked at 40 000 trips in boston

627
00:19:38,960 --> 00:19:42,160
and depending on how we apply decimal

628
00:19:40,480 --> 00:19:44,640
points to gps locations

629
00:19:42,160 --> 00:19:46,160
the trip accuracy varied significantly

630
00:19:44,640 --> 00:19:47,760
so there's a ton of detail here

631
00:19:46,160 --> 00:19:49,679
close partnership with industry lots of

632
00:19:47,760 --> 00:19:51,039
research i would strongly recommend

633
00:19:49,679 --> 00:19:52,240
diving deep and reaching out to us if

634
00:19:51,039 --> 00:19:53,919
there's any questions

635
00:19:52,240 --> 00:19:55,440
we've tried to fit in a lot of content

636
00:19:53,919 --> 00:19:57,360
in 20 minutes and i'd rather speak to

637
00:19:55,440 --> 00:19:58,960
this briefly than not have it at all so

638
00:19:57,360 --> 00:20:00,719
with that being said for our time thank

639
00:19:58,960 --> 00:20:07,840
you so much for listening

640
00:20:00,720 --> 00:20:07,840
thank you very much

641
00:20:11,600 --> 00:20:13,678
you

