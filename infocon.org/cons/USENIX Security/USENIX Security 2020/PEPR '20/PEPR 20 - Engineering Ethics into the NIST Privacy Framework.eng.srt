1
00:00:09,120 --> 00:00:12,000
hi my name is jason cronk i'm a privacy

2
00:00:11,200 --> 00:00:14,239
engineer

3
00:00:12,000 --> 00:00:16,160
with inter privacy consulting group i'm

4
00:00:14,240 --> 00:00:17,359
also the author of strategic privacy by

5
00:00:16,160 --> 00:00:20,400
design

6
00:00:17,359 --> 00:00:22,640
this work grew out of an engagement i

7
00:00:20,400 --> 00:00:24,240
had with an early adopter of the nist

8
00:00:22,640 --> 00:00:25,920
privacy framework

9
00:00:24,240 --> 00:00:27,759
and i thought it might be beneficial for

10
00:00:25,920 --> 00:00:30,960
others to hear

11
00:00:27,760 --> 00:00:33,120
how i proceeded in working

12
00:00:30,960 --> 00:00:34,320
on that that assignment so i want to

13
00:00:33,120 --> 00:00:36,000
give you a little bit of background

14
00:00:34,320 --> 00:00:37,520
information so nist the national

15
00:00:36,000 --> 00:00:39,360
institutes of standards

16
00:00:37,520 --> 00:00:41,840
introduced their cyber security

17
00:00:39,360 --> 00:00:44,640
framework back in 2014

18
00:00:41,840 --> 00:00:45,680
it quickly became a standard used by

19
00:00:44,640 --> 00:00:49,280
both industry

20
00:00:45,680 --> 00:00:50,719
and government agencies in 2018 they

21
00:00:49,280 --> 00:00:53,120
updated the framework

22
00:00:50,719 --> 00:00:54,800
and earlier this year they introduced

23
00:00:53,120 --> 00:00:55,680
their privacy framework which they hoped

24
00:00:54,800 --> 00:00:58,239
would do

25
00:00:55,680 --> 00:01:00,000
for privacy what the cyber security

26
00:00:58,239 --> 00:01:02,480
framework did for

27
00:01:00,000 --> 00:01:05,040
cyber security offices so the frameworks

28
00:01:02,480 --> 00:01:08,560
are divided into three components

29
00:01:05,040 --> 00:01:11,280
cores profiles and tiers most people

30
00:01:08,560 --> 00:01:12,799
associate the frameworks with the core

31
00:01:11,280 --> 00:01:14,479
i'm also going to be talking about

32
00:01:12,799 --> 00:01:18,479
profiles today

33
00:01:14,479 --> 00:01:21,360
so the core is divided into functions

34
00:01:18,479 --> 00:01:23,439
categories and subcategories so here we

35
00:01:21,360 --> 00:01:25,680
have the identify function

36
00:01:23,439 --> 00:01:28,000
and under it are the categories of asset

37
00:01:25,680 --> 00:01:29,840
management and business environment

38
00:01:28,000 --> 00:01:31,360
and then there are subcategories within

39
00:01:29,840 --> 00:01:35,520
each category for instance

40
00:01:31,360 --> 00:01:37,360
this one is priorities for organization

41
00:01:35,520 --> 00:01:39,360
organizational mission objective and

42
00:01:37,360 --> 00:01:42,159
activities are established and

43
00:01:39,360 --> 00:01:42,159
communicated

44
00:01:42,799 --> 00:01:46,240
both the cyber security framework and

45
00:01:45,040 --> 00:01:50,479
privacy framework

46
00:01:46,240 --> 00:01:53,119
are divided into five core functions

47
00:01:50,479 --> 00:01:54,960
two of those functions are similar

48
00:01:53,119 --> 00:01:58,560
between the two frameworks

49
00:01:54,960 --> 00:02:02,000
they both have identify and protect

50
00:01:58,560 --> 00:02:04,719
the other three are distinct within

51
00:02:02,000 --> 00:02:06,000
those functions and categories and

52
00:02:04,719 --> 00:02:08,799
subcategories

53
00:02:06,000 --> 00:02:11,440
some of the subcategories are the same

54
00:02:08,800 --> 00:02:14,640
so for instance within identify

55
00:02:11,440 --> 00:02:16,000
both frameworks have priorities for

56
00:02:14,640 --> 00:02:18,079
organizational mission

57
00:02:16,000 --> 00:02:20,239
objectives and activities are

58
00:02:18,080 --> 00:02:22,560
established and communicated

59
00:02:20,239 --> 00:02:24,319
as a subcategory some of the

60
00:02:22,560 --> 00:02:25,360
subcategories have been slightly

61
00:02:24,319 --> 00:02:27,519
modified

62
00:02:25,360 --> 00:02:28,480
so in the cyber security framework you

63
00:02:27,520 --> 00:02:31,519
have uh

64
00:02:28,480 --> 00:02:32,799
physical access to assets is managed and

65
00:02:31,519 --> 00:02:34,959
protected

66
00:02:32,800 --> 00:02:36,720
but the similar one in the privacy

67
00:02:34,959 --> 00:02:40,080
framework under protect

68
00:02:36,720 --> 00:02:41,120
is physical access to data and devices

69
00:02:40,080 --> 00:02:44,239
is managed

70
00:02:41,120 --> 00:02:45,360
so they've kind of changed it to uh to

71
00:02:44,239 --> 00:02:49,360
bring in the

72
00:02:45,360 --> 00:02:52,480
aspects the privacy aspects in this case

73
00:02:49,360 --> 00:02:53,440
the concern over data not just physical

74
00:02:52,480 --> 00:02:55,679
assets

75
00:02:53,440 --> 00:02:56,720
and then of course you have the distinct

76
00:02:55,680 --> 00:03:00,480
categories

77
00:02:56,720 --> 00:03:03,840
and subcategories for instance within

78
00:03:00,480 --> 00:03:07,359
the privacy framework has govern

79
00:03:03,840 --> 00:03:10,560
and this one is uh privacy values

80
00:03:07,360 --> 00:03:13,280
policies and training are reviewed and

81
00:03:10,560 --> 00:03:15,680
any updates are communicated notice i

82
00:03:13,280 --> 00:03:16,840
highlighted privacy values here uh i

83
00:03:15,680 --> 00:03:21,440
will get back to that

84
00:03:16,840 --> 00:03:22,319
soon now the question you may naturally

85
00:03:21,440 --> 00:03:24,640
be asking

86
00:03:22,319 --> 00:03:25,599
is we have these sub categories but how

87
00:03:24,640 --> 00:03:28,399
do we actually

88
00:03:25,599 --> 00:03:30,238
use them most organizations

89
00:03:28,400 --> 00:03:31,840
unfortunately tend to just kind of use

90
00:03:30,239 --> 00:03:34,799
them as a checklist

91
00:03:31,840 --> 00:03:37,200
going through the way the nist's privacy

92
00:03:34,799 --> 00:03:39,599
framework suggests you use them is to

93
00:03:37,200 --> 00:03:41,440
develop profiles now you have your

94
00:03:39,599 --> 00:03:43,280
current profile which is

95
00:03:41,440 --> 00:03:45,440
you know the activities that you are

96
00:03:43,280 --> 00:03:47,440
doing that match the the

97
00:03:45,440 --> 00:03:48,480
subcategory within the category in the

98
00:03:47,440 --> 00:03:50,640
function

99
00:03:48,480 --> 00:03:52,319
and the target profile is where you

100
00:03:50,640 --> 00:03:54,879
would like to be

101
00:03:52,319 --> 00:03:55,599
but that still leaves a lot of questions

102
00:03:54,879 --> 00:03:57,920
of of

103
00:03:55,599 --> 00:04:00,560
what do you put in that target profile

104
00:03:57,920 --> 00:04:03,599
how do you decide what the elements are

105
00:04:00,560 --> 00:04:04,319
and i tend to analogize this to a muffin

106
00:04:03,599 --> 00:04:07,518
pan

107
00:04:04,319 --> 00:04:09,839
right the framework is the pan

108
00:04:07,519 --> 00:04:12,000
so it's going to build muffins that are

109
00:04:09,840 --> 00:04:15,599
circular and so deep

110
00:04:12,000 --> 00:04:17,120
but what kind of ingredients do you put

111
00:04:15,599 --> 00:04:19,279
into those muffins are you making

112
00:04:17,120 --> 00:04:22,800
cornbread muffins are you making

113
00:04:19,279 --> 00:04:24,320
cupcakes are you making uh

114
00:04:22,800 --> 00:04:25,840
you know what types of muffins are you

115
00:04:24,320 --> 00:04:28,479
doing are you adding jalapenos to the

116
00:04:25,840 --> 00:04:30,719
cornbread etc so

117
00:04:28,479 --> 00:04:32,159
um this is this is the essence of the

118
00:04:30,720 --> 00:04:35,360
profile so for instance

119
00:04:32,160 --> 00:04:38,720
you might have a uh you know

120
00:04:35,360 --> 00:04:41,120
this pr ac p2 and

121
00:04:38,720 --> 00:04:42,720
it has certain ingredients like security

122
00:04:41,120 --> 00:04:45,120
guards at building entrances

123
00:04:42,720 --> 00:04:45,759
exterior doors are locked and alarmed

124
00:04:45,120 --> 00:04:48,400
usb

125
00:04:45,759 --> 00:04:49,520
ports are disabled all servers are in

126
00:04:48,400 --> 00:04:51,520
lock cages

127
00:04:49,520 --> 00:04:53,599
well that might be perfectly appropriate

128
00:04:51,520 --> 00:04:56,000
for an organization that has

129
00:04:53,600 --> 00:04:56,800
a data center but if you don't have a

130
00:04:56,000 --> 00:04:58,720
data center

131
00:04:56,800 --> 00:04:59,840
those ingredients aren't going to make

132
00:04:58,720 --> 00:05:01,680
sense for you

133
00:04:59,840 --> 00:05:04,159
so again how do you go about figuring

134
00:05:01,680 --> 00:05:07,919
out what are the appropriate ingredients

135
00:05:04,160 --> 00:05:08,240
that you put in so we know we need to

136
00:05:07,919 --> 00:05:11,280
fill

137
00:05:08,240 --> 00:05:14,400
profile with ingredients but why pick

138
00:05:11,280 --> 00:05:17,520
particular ingredients over others

139
00:05:14,400 --> 00:05:19,719
nist provides an answer in that

140
00:05:17,520 --> 00:05:21,120
they suggest that profiles enable the

141
00:05:19,720 --> 00:05:23,440
prioritization of

142
00:05:21,120 --> 00:05:25,600
outcomes and activities that best meet

143
00:05:23,440 --> 00:05:28,160
organizational privacy values

144
00:05:25,600 --> 00:05:29,520
mission or businesses and risks so those

145
00:05:28,160 --> 00:05:31,680
are four

146
00:05:29,520 --> 00:05:32,880
concepts that need to go into

147
00:05:31,680 --> 00:05:36,000
identifying

148
00:05:32,880 --> 00:05:38,400
you know what our private our profile is

149
00:05:36,000 --> 00:05:40,560
what our privacy values are what our

150
00:05:38,400 --> 00:05:43,919
business needs are

151
00:05:40,560 --> 00:05:45,919
and what the risks are now privacy

152
00:05:43,919 --> 00:05:48,719
values you notice i highlight it again

153
00:05:45,919 --> 00:05:49,840
turns up 14 times in this privacy

154
00:05:48,720 --> 00:05:52,960
framework

155
00:05:49,840 --> 00:05:55,039
yet nowhere do they define it or really

156
00:05:52,960 --> 00:05:56,318
provide any guidance on how

157
00:05:55,039 --> 00:06:07,440
organizations should

158
00:05:56,319 --> 00:06:09,600
identify their privacy values

159
00:06:07,440 --> 00:06:10,719
so privacy does have this definition

160
00:06:09,600 --> 00:06:13,759
problem right

161
00:06:10,720 --> 00:06:15,360
so most people can point out you know

162
00:06:13,759 --> 00:06:17,440
when they see something that they think

163
00:06:15,360 --> 00:06:19,039
is a privacy violation

164
00:06:17,440 --> 00:06:20,560
either of their privacy or somebody

165
00:06:19,039 --> 00:06:23,599
else's but

166
00:06:20,560 --> 00:06:25,680
they have a hard time defining that uh

167
00:06:23,600 --> 00:06:26,800
and so so there's two legal terms that i

168
00:06:25,680 --> 00:06:29,520
want to introduce here

169
00:06:26,800 --> 00:06:31,360
malum insay and malam prohibitum

170
00:06:29,520 --> 00:06:34,400
malaminsei means evil

171
00:06:31,360 --> 00:06:35,280
wrongdoing in itself like murder malam

172
00:06:34,400 --> 00:06:38,239
prohibitum

173
00:06:35,280 --> 00:06:39,039
is wrong just because the law or a

174
00:06:38,240 --> 00:06:42,080
statute

175
00:06:39,039 --> 00:06:44,800
says so uh like speeding uh there isn't

176
00:06:42,080 --> 00:06:46,719
anything inherently wrong with speeding

177
00:06:44,800 --> 00:06:48,639
that's just going over a certain speed

178
00:06:46,720 --> 00:06:51,680
limit that's just where the law

179
00:06:48,639 --> 00:06:54,720
sets that speed for that particular road

180
00:06:51,680 --> 00:06:57,919
privacy tends to be the the former

181
00:06:54,720 --> 00:06:59,840
uh it's things that we innately or as a

182
00:06:57,919 --> 00:07:02,799
society agree

183
00:06:59,840 --> 00:07:03,198
uh that are wrong it's not wrong because

184
00:07:02,800 --> 00:07:06,560
it's

185
00:07:03,199 --> 00:07:09,919
the law so we can think of privacy as

186
00:07:06,560 --> 00:07:11,039
a set of social norms uh okay so that

187
00:07:09,919 --> 00:07:13,840
really helps so

188
00:07:11,039 --> 00:07:14,159
let's look further into this so there

189
00:07:13,840 --> 00:07:17,198
are

190
00:07:14,160 --> 00:07:20,240
numerous models of social norms

191
00:07:17,199 --> 00:07:22,000
that abound so alan weston has his

192
00:07:20,240 --> 00:07:25,440
states of privacy

193
00:07:22,000 --> 00:07:28,400
anonymity intimacy seclusion solitude

194
00:07:25,440 --> 00:07:30,160
woody harzog recently wrote in his book

195
00:07:28,400 --> 00:07:33,599
privacy blueprint

196
00:07:30,160 --> 00:07:37,520
his three privacy pillars of obscurity

197
00:07:33,599 --> 00:07:39,440
autonomy and trust prosser

198
00:07:37,520 --> 00:07:41,919
the famed torch professor had his

199
00:07:39,440 --> 00:07:43,840
privacy torts of common law

200
00:07:41,919 --> 00:07:45,758
placing somebody in a false light

201
00:07:43,840 --> 00:07:48,878
intrusion upon seclusion

202
00:07:45,759 --> 00:07:51,599
a public disclosure of private facts

203
00:07:48,879 --> 00:07:54,080
and appropriation of name and likeness

204
00:07:51,599 --> 00:07:57,280
and ryan kaylo has his objective

205
00:07:54,080 --> 00:07:58,960
and subjective arms so those on the left

206
00:07:57,280 --> 00:08:00,479
that are kind of with the the black

207
00:07:58,960 --> 00:08:02,960
background those

208
00:08:00,479 --> 00:08:04,240
are more positive things those are

209
00:08:02,960 --> 00:08:07,440
things that we

210
00:08:04,240 --> 00:08:09,520
and the society aspire to or privacy

211
00:08:07,440 --> 00:08:10,639
values or or privacy norms that we

212
00:08:09,520 --> 00:08:12,799
aspire to

213
00:08:10,639 --> 00:08:14,720
the ones on the right the privacy torts

214
00:08:12,800 --> 00:08:18,000
and kayla's

215
00:08:14,720 --> 00:08:20,879
arms are are harms that we want to try

216
00:08:18,000 --> 00:08:20,879
to avoid

217
00:08:21,280 --> 00:08:27,440
now i am myself partial to

218
00:08:24,319 --> 00:08:29,440
professor daniel solov's taxonomy of

219
00:08:27,440 --> 00:08:32,799
privacy harms

220
00:08:29,440 --> 00:08:36,000
in his taxonomy he divides privacy into

221
00:08:32,799 --> 00:08:38,880
16 types of harms in four

222
00:08:36,000 --> 00:08:40,719
broad categories information processing

223
00:08:38,880 --> 00:08:44,000
information dissemination

224
00:08:40,719 --> 00:08:47,920
collection and invasions

225
00:08:44,000 --> 00:08:50,880
and i i like professor solov's

226
00:08:47,920 --> 00:08:52,800
grouping here because it's at once

227
00:08:50,880 --> 00:08:55,279
comprehensive

228
00:08:52,800 --> 00:08:56,880
and it covers uh really the breadth of

229
00:08:55,279 --> 00:08:58,880
almost anything

230
00:08:56,880 --> 00:09:00,720
that i certainly have been able to think

231
00:08:58,880 --> 00:09:02,240
of that that one would consider a

232
00:09:00,720 --> 00:09:05,279
privacy violation

233
00:09:02,240 --> 00:09:06,320
uh and also it's granular enough they're

234
00:09:05,279 --> 00:09:08,880
small enough

235
00:09:06,320 --> 00:09:10,160
uh that they're easy to work with unlike

236
00:09:08,880 --> 00:09:12,399
some of the others like

237
00:09:10,160 --> 00:09:14,640
ryan calo's objective and subjective

238
00:09:12,399 --> 00:09:15,760
arms uh which the categories are very

239
00:09:14,640 --> 00:09:18,480
broad

240
00:09:15,760 --> 00:09:20,720
and they're hard to really uh to really

241
00:09:18,480 --> 00:09:23,040
work with

242
00:09:20,720 --> 00:09:26,080
now this is not to say that these are

243
00:09:23,040 --> 00:09:29,519
the only five models of privacy norms

244
00:09:26,080 --> 00:09:32,320
uh there are many others uh out there

245
00:09:29,519 --> 00:09:33,839
uh these are just the ones that are uh

246
00:09:32,320 --> 00:09:35,519
probably most prominent

247
00:09:33,839 --> 00:09:37,519
uh the other one is worth mentioning

248
00:09:35,519 --> 00:09:39,760
that i don't go into here is

249
00:09:37,519 --> 00:09:40,720
helen niesenbaum's uh contextual

250
00:09:39,760 --> 00:09:42,880
integrity

251
00:09:40,720 --> 00:09:44,399
uh the the only issue i have with with

252
00:09:42,880 --> 00:09:47,439
her contextual integrity

253
00:09:44,399 --> 00:09:48,160
is unlike these which define specific

254
00:09:47,440 --> 00:09:52,000
harms

255
00:09:48,160 --> 00:09:55,120
she defines a process for uh for

256
00:09:52,000 --> 00:09:56,880
for identifying harms uh so it's a

257
00:09:55,120 --> 00:09:58,480
little bit uh more difficult to work

258
00:09:56,880 --> 00:10:00,240
with in an aspect because it doesn't

259
00:09:58,480 --> 00:10:02,240
actually tell you what the norm is

260
00:10:00,240 --> 00:10:03,360
it just tells you how to discover those

261
00:10:02,240 --> 00:10:06,560
norms

262
00:10:03,360 --> 00:10:09,040
so now we have a set of model

263
00:10:06,560 --> 00:10:11,439
privacy norms that we can use how do we

264
00:10:09,040 --> 00:10:14,800
go about using that within nist

265
00:10:11,440 --> 00:10:19,120
and building that in and using those for

266
00:10:14,800 --> 00:10:21,519
our privacy values that nist rep

267
00:10:19,120 --> 00:10:22,480
references so i didn't just pick one

268
00:10:21,519 --> 00:10:26,560
particular

269
00:10:22,480 --> 00:10:28,880
model to use what i did first was try to

270
00:10:26,560 --> 00:10:30,160
align the models and find where there

271
00:10:28,880 --> 00:10:33,920
was similarities

272
00:10:30,160 --> 00:10:37,519
and overlap for instance i chose

273
00:10:33,920 --> 00:10:40,000
woody hard docs trust and dan solov's

274
00:10:37,519 --> 00:10:41,760
breach of confidentiality secondary use

275
00:10:40,000 --> 00:10:44,800
and insecurity

276
00:10:41,760 --> 00:10:47,439
so why did i choose to to map

277
00:10:44,800 --> 00:10:48,959
these to each other so trust is about a

278
00:10:47,440 --> 00:10:52,320
willingness to become

279
00:10:48,959 --> 00:10:54,640
vulnerable to the actions of another so

280
00:10:52,320 --> 00:10:57,519
you share information with somebody

281
00:10:54,640 --> 00:10:59,199
under the guise of confidentiality

282
00:10:57,519 --> 00:11:01,279
you are putting yourselves in a

283
00:10:59,200 --> 00:11:02,880
vulnerable position

284
00:11:01,279 --> 00:11:05,360
with the understanding that they're not

285
00:11:02,880 --> 00:11:07,519
going to again go and disseminate that

286
00:11:05,360 --> 00:11:09,920
and breach that confidentiality

287
00:11:07,519 --> 00:11:12,160
similarly with secondary use

288
00:11:09,920 --> 00:11:13,519
you give somebody information for one

289
00:11:12,160 --> 00:11:15,760
purpose

290
00:11:13,519 --> 00:11:17,360
and that makes you vulnerable to them

291
00:11:15,760 --> 00:11:20,240
then taking it

292
00:11:17,360 --> 00:11:21,600
and and using it for another purpose a

293
00:11:20,240 --> 00:11:23,920
secondary use

294
00:11:21,600 --> 00:11:25,760
so you trust them not to do it and

295
00:11:23,920 --> 00:11:29,360
similarly insecurity

296
00:11:25,760 --> 00:11:31,120
is like a fiduciary duty

297
00:11:29,360 --> 00:11:33,360
you are trusting somebody with

298
00:11:31,120 --> 00:11:35,440
information that they are going to take

299
00:11:33,360 --> 00:11:39,839
appropriate care of it

300
00:11:35,440 --> 00:11:39,839
and not be negligent in its handling

301
00:11:41,120 --> 00:11:45,600
so now how do we take these models of

302
00:11:43,839 --> 00:11:48,880
privacy norms and turn them into

303
00:11:45,600 --> 00:11:51,279
organization specific values

304
00:11:48,880 --> 00:11:53,120
now i've removed the column for

305
00:11:51,279 --> 00:11:56,880
processor because he didn't have

306
00:11:53,120 --> 00:11:59,760
any uh overlap with these particular

307
00:11:56,880 --> 00:12:01,120
norms from hard dog west and kalo and

308
00:11:59,760 --> 00:12:02,959
solov

309
00:12:01,120 --> 00:12:05,200
let's say we're working for a company

310
00:12:02,959 --> 00:12:07,760
called texaclaric and we have an

311
00:12:05,200 --> 00:12:09,040
app that allows parishioners to send

312
00:12:07,760 --> 00:12:12,160
messages

313
00:12:09,040 --> 00:12:15,279
to their religious leaders so

314
00:12:12,160 --> 00:12:19,279
we might take these particular

315
00:12:15,279 --> 00:12:23,200
privacy norms of trust intimacy

316
00:12:19,279 --> 00:12:25,519
avoiding objective harm from kalo

317
00:12:23,200 --> 00:12:26,720
not wanting to breach confidentiality

318
00:12:25,519 --> 00:12:28,800
secondary use

319
00:12:26,720 --> 00:12:31,360
and not wanting to be insecure and

320
00:12:28,800 --> 00:12:32,399
create a privacy value specifically for

321
00:12:31,360 --> 00:12:35,279
texas cleric

322
00:12:32,399 --> 00:12:36,639
and we'll title it seal of confession

323
00:12:35,279 --> 00:12:38,639
and

324
00:12:36,639 --> 00:12:40,000
in this privacy value we say texas

325
00:12:38,639 --> 00:12:42,800
cleric will protect

326
00:12:40,000 --> 00:12:44,399
and not use or reveal any information

327
00:12:42,800 --> 00:12:47,839
divulged

328
00:12:44,399 --> 00:12:51,279
so we've distilled these multiple

329
00:12:47,839 --> 00:12:54,480
privacy norms and in solav's case

330
00:12:51,279 --> 00:12:55,519
combined different norms within his

331
00:12:54,480 --> 00:12:57,760
taxonomy

332
00:12:55,519 --> 00:12:58,639
to come up with one specific privacy

333
00:12:57,760 --> 00:13:00,560
value

334
00:12:58,639 --> 00:13:02,399
for texas cleric and it sort of

335
00:13:00,560 --> 00:13:05,439
encompasses all of that

336
00:13:02,399 --> 00:13:06,959
and is organization specific so it's not

337
00:13:05,440 --> 00:13:09,519
this generic

338
00:13:06,959 --> 00:13:11,680
we respect confidentiality or we won't

339
00:13:09,519 --> 00:13:14,079
use data for a secondary purpose

340
00:13:11,680 --> 00:13:15,920
but we've created a privacy value that

341
00:13:14,079 --> 00:13:19,359
is unique and specific

342
00:13:15,920 --> 00:13:19,360
for our organization

343
00:13:19,600 --> 00:13:22,880
now we're on to our next step how do we

344
00:13:21,920 --> 00:13:26,279
take that

345
00:13:22,880 --> 00:13:27,920
specific privacy value that is our

346
00:13:26,279 --> 00:13:31,200
organization's

347
00:13:27,920 --> 00:13:32,319
value and use that in the nist privacy

348
00:13:31,200 --> 00:13:35,000
framework

349
00:13:32,320 --> 00:13:36,240
so what i do is go through each of the

350
00:13:35,000 --> 00:13:39,760
subcategories

351
00:13:36,240 --> 00:13:43,199
and say what can we do within

352
00:13:39,760 --> 00:13:46,240
this subcategory to achieve

353
00:13:43,199 --> 00:13:47,680
our privacy value or help us achieve our

354
00:13:46,240 --> 00:13:49,760
privacy value

355
00:13:47,680 --> 00:13:51,599
so for instance this first one in the

356
00:13:49,760 --> 00:13:54,560
identify function

357
00:13:51,600 --> 00:13:55,360
data actions of the system product

358
00:13:54,560 --> 00:13:58,399
services

359
00:13:55,360 --> 00:14:01,600
are inventoried so we may have

360
00:13:58,399 --> 00:14:04,160
a statement a goal here

361
00:14:01,600 --> 00:14:05,440
that all touch points for confessional

362
00:14:04,160 --> 00:14:07,680
message data

363
00:14:05,440 --> 00:14:08,959
are kept in a system architecture

364
00:14:07,680 --> 00:14:12,399
diagram

365
00:14:08,959 --> 00:14:14,479
so this will help us protect that data

366
00:14:12,399 --> 00:14:15,600
because we'll know where it is it'll

367
00:14:14,480 --> 00:14:18,160
help us not

368
00:14:15,600 --> 00:14:18,800
use the data for a secondary purpose

369
00:14:18,160 --> 00:14:21,040
because

370
00:14:18,800 --> 00:14:22,639
we presumably understand what the

371
00:14:21,040 --> 00:14:24,639
systems are using it for

372
00:14:22,639 --> 00:14:27,279
and there are other subcategories that

373
00:14:24,639 --> 00:14:29,680
help us understand those purposes

374
00:14:27,279 --> 00:14:30,720
and it will help us not reveal any of

375
00:14:29,680 --> 00:14:32,719
the information

376
00:14:30,720 --> 00:14:35,360
because if we if our system architecture

377
00:14:32,720 --> 00:14:36,560
shows data going from the system with

378
00:14:35,360 --> 00:14:40,240
confessional data

379
00:14:36,560 --> 00:14:42,000
out to some other other destination

380
00:14:40,240 --> 00:14:44,320
we know we might have a concern about

381
00:14:42,000 --> 00:14:45,120
revealing that data so this is going to

382
00:14:44,320 --> 00:14:48,399
help us

383
00:14:45,120 --> 00:14:48,959
achieve that privacy value so the next

384
00:14:48,399 --> 00:14:50,480
one

385
00:14:48,959 --> 00:14:52,399
roles and responsibilities for the

386
00:14:50,480 --> 00:14:53,360
workforce are established with respect

387
00:14:52,399 --> 00:14:56,240
to privacy

388
00:14:53,360 --> 00:14:57,199
so here we're going to have a ciso and a

389
00:14:56,240 --> 00:14:59,440
cpo with

390
00:14:57,199 --> 00:15:01,040
job responsibilities for security and

391
00:14:59,440 --> 00:15:03,440
privacy respectively

392
00:15:01,040 --> 00:15:04,240
now this may seem sort of obvious for

393
00:15:03,440 --> 00:15:06,800
for a lot of

394
00:15:04,240 --> 00:15:08,240
companies or people watching but a lot

395
00:15:06,800 --> 00:15:09,040
of companies don't necessarily have

396
00:15:08,240 --> 00:15:12,240
those roles

397
00:15:09,040 --> 00:15:14,639
or maybe we need more specific roles

398
00:15:12,240 --> 00:15:15,519
within the company you know a privacy

399
00:15:14,639 --> 00:15:18,639
engineer

400
00:15:15,519 --> 00:15:20,959
security engineer security

401
00:15:18,639 --> 00:15:21,760
you know a trained developer in in

402
00:15:20,959 --> 00:15:24,000
security

403
00:15:21,760 --> 00:15:25,040
whatever those roles are but these are

404
00:15:24,000 --> 00:15:27,199
going to help us

405
00:15:25,040 --> 00:15:29,360
protect the data and make sure that

406
00:15:27,199 --> 00:15:30,319
we're not using or revealing any

407
00:15:29,360 --> 00:15:32,000
information

408
00:15:30,320 --> 00:15:34,399
inappropriately it's going to be their

409
00:15:32,000 --> 00:15:37,040
job to be accountable

410
00:15:34,399 --> 00:15:38,240
for for this privacy value now i'm not

411
00:15:37,040 --> 00:15:39,839
going to go through all of them i'll go

412
00:15:38,240 --> 00:15:43,040
through the last one

413
00:15:39,839 --> 00:15:44,399
data and transit are protected so

414
00:15:43,040 --> 00:15:47,199
again we may have something more

415
00:15:44,399 --> 00:15:48,639
specific related to our specific

416
00:15:47,199 --> 00:15:50,479
privacy value that we're trying to

417
00:15:48,639 --> 00:15:52,639
achieve here and that is say

418
00:15:50,480 --> 00:15:54,959
data between the parishioners app and

419
00:15:52,639 --> 00:15:56,880
our systems are encrypted

420
00:15:54,959 --> 00:15:58,800
and we could be more specific about you

421
00:15:56,880 --> 00:16:00,959
know uh strong uh

422
00:15:58,800 --> 00:16:02,399
you know strong encryption uh maybe

423
00:16:00,959 --> 00:16:04,399
another one and and it doesn't have to

424
00:16:02,399 --> 00:16:07,680
be a one for one right there could be

425
00:16:04,399 --> 00:16:09,440
multiple uh multiple items under each

426
00:16:07,680 --> 00:16:11,199
subcategory so maybe another one might

427
00:16:09,440 --> 00:16:14,639
be data between

428
00:16:11,199 --> 00:16:22,479
the between our systems and

429
00:16:14,639 --> 00:16:25,360
the religious leaders app are encrypted

430
00:16:22,480 --> 00:16:28,560
now just anecdotally i found most

431
00:16:25,360 --> 00:16:31,920
organizations have a very difficult time

432
00:16:28,560 --> 00:16:35,599
of understanding or incorporating

433
00:16:31,920 --> 00:16:38,639
privacy into their their organization

434
00:16:35,600 --> 00:16:40,560
uh they you know are

435
00:16:38,639 --> 00:16:42,160
they say you know we want to make sure

436
00:16:40,560 --> 00:16:45,119
we protect privacy

437
00:16:42,160 --> 00:16:45,839
but they don't define privacy in a way

438
00:16:45,120 --> 00:16:48,160
that makes it

439
00:16:45,839 --> 00:16:50,399
actionable uh so what i hope i've shown

440
00:16:48,160 --> 00:16:53,439
here today is an actionable

441
00:16:50,399 --> 00:16:56,160
and systematic approach to taking

442
00:16:53,440 --> 00:16:57,199
a bunch of privacy norms that we know

443
00:16:56,160 --> 00:17:00,480
that people

444
00:16:57,199 --> 00:17:04,480
ascribe to either harms

445
00:17:00,480 --> 00:17:07,439
or or positive values

446
00:17:04,480 --> 00:17:08,000
and deriving some organizational privacy

447
00:17:07,439 --> 00:17:10,959
values

448
00:17:08,000 --> 00:17:11,919
some organization specific privacy

449
00:17:10,959 --> 00:17:13,520
values

450
00:17:11,919 --> 00:17:15,199
that are really related to the

451
00:17:13,520 --> 00:17:19,760
organization and

452
00:17:15,199 --> 00:17:21,600
it encompass those those social norms

453
00:17:19,760 --> 00:17:23,280
and that's the first step and the second

454
00:17:21,599 --> 00:17:25,359
step is then

455
00:17:23,280 --> 00:17:26,799
to go through those norms with the

456
00:17:25,359 --> 00:17:29,439
frameworks core

457
00:17:26,799 --> 00:17:30,639
and identify what elements what

458
00:17:29,440 --> 00:17:34,080
ingredients

459
00:17:30,640 --> 00:17:36,960
what actions and activities can we do

460
00:17:34,080 --> 00:17:39,600
within the framework within each of the

461
00:17:36,960 --> 00:17:42,480
categories and subcategories

462
00:17:39,600 --> 00:17:44,080
that will help us achieve these sets of

463
00:17:42,480 --> 00:17:46,640
privacy values

464
00:17:44,080 --> 00:17:47,760
now i have three here i found most

465
00:17:46,640 --> 00:17:50,720
organizations

466
00:17:47,760 --> 00:17:51,919
tend to use about three to five uh

467
00:17:50,720 --> 00:17:54,320
privacy values

468
00:17:51,919 --> 00:17:56,720
uh as you start getting more than that

469
00:17:54,320 --> 00:17:59,280
it gets a little overly complicated

470
00:17:56,720 --> 00:18:00,000
and too few uh just doesn't seem like

471
00:17:59,280 --> 00:18:02,480
you're covering

472
00:18:00,000 --> 00:18:04,320
enough ground so that's really it it's

473
00:18:02,480 --> 00:18:07,280
really a two-step process

474
00:18:04,320 --> 00:18:09,120
uh and you know hopefully you see that

475
00:18:07,280 --> 00:18:12,080
you know this is a systematic

476
00:18:09,120 --> 00:18:14,799
a process-oriented approach uh to

477
00:18:12,080 --> 00:18:18,240
building privacy into your

478
00:18:14,799 --> 00:18:20,799
use of building privacy values into your

479
00:18:18,240 --> 00:18:22,480
use of the privacy framework

480
00:18:20,799 --> 00:18:24,160
here are some resources including a link

481
00:18:22,480 --> 00:18:26,559
to the nist privacy framework

482
00:18:24,160 --> 00:18:28,480
a link to an infographic i did for these

483
00:18:26,559 --> 00:18:29,678
models of privacy norms that i've been

484
00:18:28,480 --> 00:18:31,600
using

485
00:18:29,679 --> 00:18:33,280
also i encourage you to do a search for

486
00:18:31,600 --> 00:18:34,639
deriving and using synthetic

487
00:18:33,280 --> 00:18:37,600
consequences

488
00:18:34,640 --> 00:18:38,640
stuart shapiro did a much more rigorous

489
00:18:37,600 --> 00:18:41,600
informal method

490
00:18:38,640 --> 00:18:44,160
of deriving synthetic harms or

491
00:18:41,600 --> 00:18:47,600
consequences or privacy values

492
00:18:44,160 --> 00:18:49,039
from from others and that might be of

493
00:18:47,600 --> 00:18:57,840
interest to some

494
00:18:49,039 --> 00:18:57,840
i'm happy to answer questions now

495
00:19:00,960 --> 00:19:03,039
you

