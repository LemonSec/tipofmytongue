1
00:00:08,320 --> 00:00:11,519
hello everyone thank you so much for

2
00:00:09,760 --> 00:00:14,080
your interest in privacy deployment

3
00:00:11,519 --> 00:00:16,079
this is a talk by myself patricia thane

4
00:00:14,080 --> 00:00:18,000
co-founder and ceo of private ai

5
00:00:16,079 --> 00:00:20,479
peter lucian's co-founder and cto of

6
00:00:18,000 --> 00:00:23,840
private ai and dr parana sabani head of

7
00:00:20,480 --> 00:00:25,680
applied research at georgian partners

8
00:00:23,840 --> 00:00:27,359
so very common question we get when we

9
00:00:25,680 --> 00:00:28,160
start talking about privacy is what is

10
00:00:27,359 --> 00:00:30,240
privacy

11
00:00:28,160 --> 00:00:31,679
and if we look at the definition that we

12
00:00:30,240 --> 00:00:33,920
find in

13
00:00:31,679 --> 00:00:36,239
the documents from the gdpr it says that

14
00:00:33,920 --> 00:00:37,840
data privacy means empowering your users

15
00:00:36,239 --> 00:00:39,440
to make their own decisions about who

16
00:00:37,840 --> 00:00:40,399
can process their data and for what

17
00:00:39,440 --> 00:00:42,718
purpose

18
00:00:40,399 --> 00:00:44,800
an intrinsic data privacy is data

19
00:00:42,719 --> 00:00:46,800
protection that's why you'll hear

20
00:00:44,800 --> 00:00:48,718
a lot of confusion between what is data

21
00:00:46,800 --> 00:00:50,640
privacy what is data protection

22
00:00:48,719 --> 00:00:51,760
uh what kind of regulations deal with

23
00:00:50,640 --> 00:00:56,239
which

24
00:00:51,760 --> 00:00:56,239
you need a marriage between the two

25
00:00:56,879 --> 00:01:00,000
another common question that we get is

26
00:00:58,480 --> 00:01:00,800
why is privacy important in the first

27
00:01:00,000 --> 00:01:02,079
place

28
00:01:00,800 --> 00:01:04,559
well it's about maintaining customer

29
00:01:02,079 --> 00:01:06,080
trust for obvious reasons

30
00:01:04,559 --> 00:01:07,600
it's also about building better tech

31
00:01:06,080 --> 00:01:09,840
which is one of the reasons why we

32
00:01:07,600 --> 00:01:11,439
went into privacy preserving technology

33
00:01:09,840 --> 00:01:14,479
research and development

34
00:01:11,439 --> 00:01:17,039
and essentially privacy preserving

35
00:01:14,479 --> 00:01:20,000
technologies allow you to unlock access

36
00:01:17,040 --> 00:01:21,600
to data sets that would otherwise be

37
00:01:20,000 --> 00:01:23,600
locked away

38
00:01:21,600 --> 00:01:25,759
because of the private information that

39
00:01:23,600 --> 00:01:27,679
they contain so you'll have examples of

40
00:01:25,759 --> 00:01:29,360
this for example two different hospitals

41
00:01:27,680 --> 00:01:30,159
sharing information in order to make

42
00:01:29,360 --> 00:01:35,039
advances

43
00:01:30,159 --> 00:01:38,400
in genomic predictions about disease

44
00:01:35,040 --> 00:01:39,680
it also grants peace of mind

45
00:01:38,400 --> 00:01:42,000
when you know that you're doing the

46
00:01:39,680 --> 00:01:43,759
right thing and when you know that

47
00:01:42,000 --> 00:01:45,040
you're following all the regulations and

48
00:01:43,759 --> 00:01:49,360
you won't get fined

49
00:01:45,040 --> 00:01:53,040
for essentially misusing data

50
00:01:49,360 --> 00:01:54,960
or making it more vulnerable to

51
00:01:53,040 --> 00:01:56,880
data leaks for example proper data

52
00:01:54,960 --> 00:01:58,479
privacy which goes hand in hand with

53
00:01:56,880 --> 00:01:59,920
proper data protection

54
00:01:58,479 --> 00:02:02,320
also goes hand in hand with data

55
00:01:59,920 --> 00:02:04,880
security it makes it much easier to

56
00:02:02,320 --> 00:02:07,439
maintain data security when you maintain

57
00:02:04,880 --> 00:02:09,280
user privacy and proper data protection

58
00:02:07,439 --> 00:02:12,000
around your information

59
00:02:09,280 --> 00:02:13,360
and another big plus is that we keep

60
00:02:12,000 --> 00:02:17,760
seeing signals

61
00:02:13,360 --> 00:02:18,720
uh that integrating privacy into your

62
00:02:17,760 --> 00:02:20,480
software

63
00:02:18,720 --> 00:02:23,040
is actually a competitive advantage so

64
00:02:20,480 --> 00:02:25,280
you'll see this with duckduckgo

65
00:02:23,040 --> 00:02:28,160
with spike in downloads whenever there's

66
00:02:25,280 --> 00:02:30,319
some sort of scandal with google privacy

67
00:02:28,160 --> 00:02:31,359
you'll see this with signal when they

68
00:02:30,319 --> 00:02:34,560
integrated

69
00:02:31,360 --> 00:02:36,480
a new blurring feature into

70
00:02:34,560 --> 00:02:37,680
their app right before the black live

71
00:02:36,480 --> 00:02:39,599
matter protests

72
00:02:37,680 --> 00:02:40,959
uh the spike in downloads at that very

73
00:02:39,599 --> 00:02:43,280
end

74
00:02:40,959 --> 00:02:44,480
and yeah it's just a recurring trend

75
00:02:43,280 --> 00:02:47,920
that you keep seeing

76
00:02:44,480 --> 00:02:49,760
across the board so definitely something

77
00:02:47,920 --> 00:02:51,040
to think about

78
00:02:49,760 --> 00:02:53,120
there are lots of privacy enhancing

79
00:02:51,040 --> 00:02:55,280
technologies to choose from

80
00:02:53,120 --> 00:02:56,480
including secure multi-party computation

81
00:02:55,280 --> 00:02:58,640
homomorphic encryption

82
00:02:56,480 --> 00:03:01,200
data identification differential privacy

83
00:02:58,640 --> 00:03:03,839
secure enclaves and data synthesis

84
00:03:01,200 --> 00:03:04,319
and it can often be really confusing

85
00:03:03,840 --> 00:03:06,800
about

86
00:03:04,319 --> 00:03:08,879
which one to choose at which point what

87
00:03:06,800 --> 00:03:10,159
kind of support is available for each of

88
00:03:08,879 --> 00:03:11,760
them if you want to integrate them

89
00:03:10,159 --> 00:03:14,959
yourself

90
00:03:11,760 --> 00:03:17,679
and the this talk is really about

91
00:03:14,959 --> 00:03:18,640
clarifying the decision-making process a

92
00:03:17,680 --> 00:03:21,040
little bit

93
00:03:18,640 --> 00:03:22,079
um it's a pretty complex decision-making

94
00:03:21,040 --> 00:03:24,560
process so

95
00:03:22,080 --> 00:03:25,840
essentially picture a decision tree uh

96
00:03:24,560 --> 00:03:28,319
we're going to go down

97
00:03:25,840 --> 00:03:30,400
just a few of those branches and explore

98
00:03:28,319 --> 00:03:34,000
what kind of technologies work for

99
00:03:30,400 --> 00:03:34,799
different problems the very first

100
00:03:34,000 --> 00:03:36,080
question you should

101
00:03:34,799 --> 00:03:38,480
ask yourself when you're dealing with

102
00:03:36,080 --> 00:03:40,319
user information is

103
00:03:38,480 --> 00:03:42,319
whether you have a consent to collect

104
00:03:40,319 --> 00:03:44,879
and process their information

105
00:03:42,319 --> 00:03:45,440
so if the answer is no you do not have

106
00:03:44,879 --> 00:03:47,440
consent

107
00:03:45,440 --> 00:03:49,359
go get consent as it's required under

108
00:03:47,440 --> 00:03:51,359
regulation such as the gdpr

109
00:03:49,360 --> 00:03:52,720
and that affects any u citizen

110
00:03:51,360 --> 00:03:53,760
regardless of where they are in the

111
00:03:52,720 --> 00:03:55,760
world

112
00:03:53,760 --> 00:03:57,439
now if you do have consent you might ask

113
00:03:55,760 --> 00:03:59,040
yourself whether you're sharing the

114
00:03:57,439 --> 00:04:00,720
information with a third party

115
00:03:59,040 --> 00:04:02,239
or directly with the user or group of

116
00:04:00,720 --> 00:04:03,680
users whose data you're making

117
00:04:02,239 --> 00:04:06,000
predictions on

118
00:04:03,680 --> 00:04:06,879
now we're going to go down that third

119
00:04:06,000 --> 00:04:08,239
party branch

120
00:04:06,879 --> 00:04:09,920
what if you want to share information

121
00:04:08,239 --> 00:04:13,280
with third party but

122
00:04:09,920 --> 00:04:14,399
if you want to go down that other branch

123
00:04:13,280 --> 00:04:16,000
what if you're

124
00:04:14,400 --> 00:04:17,600
only dealing directly with the user or

125
00:04:16,000 --> 00:04:18,880
group of users whose data you're making

126
00:04:17,600 --> 00:04:22,160
predictions on

127
00:04:18,880 --> 00:04:24,000
then that part you can find in a

128
00:04:22,160 --> 00:04:26,960
decision tree

129
00:04:24,000 --> 00:04:26,960
on our blog

130
00:04:27,199 --> 00:04:33,199
now if the answer is you want to share

131
00:04:30,320 --> 00:04:35,040
information with a third party

132
00:04:33,199 --> 00:04:36,800
the next question you ask is whether you

133
00:04:35,040 --> 00:04:38,320
want to just share insights or whether

134
00:04:36,800 --> 00:04:40,320
you want to share a data set that needs

135
00:04:38,320 --> 00:04:42,479
to be visible

136
00:04:40,320 --> 00:04:43,599
if the answer is insights you're going

137
00:04:42,479 --> 00:04:45,440
to need to determine whether you're

138
00:04:43,600 --> 00:04:48,160
making generalizations over

139
00:04:45,440 --> 00:04:49,120
a population visible to that third party

140
00:04:48,160 --> 00:04:52,479
or whether you need

141
00:04:49,120 --> 00:04:54,080
user specific predictions

142
00:04:52,479 --> 00:04:55,919
whereas if you need to share the data

143
00:04:54,080 --> 00:04:57,199
set you're going to ask yourself whether

144
00:04:55,919 --> 00:04:59,280
you need the personally identifiable

145
00:04:57,199 --> 00:05:01,680
information that's within that data set

146
00:04:59,280 --> 00:05:03,280
such as names person identifiable

147
00:05:01,680 --> 00:05:04,800
numbers faces

148
00:05:03,280 --> 00:05:06,320
or whether the data can be useful

149
00:05:04,800 --> 00:05:08,160
without that pir

150
00:05:06,320 --> 00:05:10,080
if you need the pii you should have a

151
00:05:08,160 --> 00:05:12,160
data processor agreement in place

152
00:05:10,080 --> 00:05:14,560
encrypt and transit and keep track of

153
00:05:12,160 --> 00:05:17,520
who you share the data with and

154
00:05:14,560 --> 00:05:19,120
what they are using that data for these

155
00:05:17,520 --> 00:05:22,159
are requirements

156
00:05:19,120 --> 00:05:24,639
under the gdpr so you need

157
00:05:22,160 --> 00:05:25,680
consent for whatever you're using that

158
00:05:24,639 --> 00:05:29,520
data for

159
00:05:25,680 --> 00:05:31,360
you need to keep track of where the data

160
00:05:29,520 --> 00:05:32,560
are and you need to keep track of what

161
00:05:31,360 --> 00:05:37,280
they're being used for

162
00:05:32,560 --> 00:05:38,880
all under the gdpr and if you're

163
00:05:37,280 --> 00:05:40,479
if you fit under this use case you

164
00:05:38,880 --> 00:05:42,159
should also consider whether you might

165
00:05:40,479 --> 00:05:43,359
be a good candidate this use case might

166
00:05:42,160 --> 00:05:44,960
be a good candidate for secure

167
00:05:43,360 --> 00:05:47,600
multi-party computation

168
00:05:44,960 --> 00:05:49,440
so secure multi-party computation allows

169
00:05:47,600 --> 00:05:52,400
multiple users to

170
00:05:49,440 --> 00:05:54,400
share obfuscated inputs and only the

171
00:05:52,400 --> 00:05:55,359
outputs are visible to all of the

172
00:05:54,400 --> 00:05:58,560
parties

173
00:05:55,360 --> 00:06:03,120
so this is a great technology if you

174
00:05:58,560 --> 00:06:05,680
are okay with some communication cost

175
00:06:03,120 --> 00:06:07,680
and if you aren't going to be tweaking

176
00:06:05,680 --> 00:06:10,240
the algorithms very often

177
00:06:07,680 --> 00:06:11,600
so essentially you have to have a preset

178
00:06:10,240 --> 00:06:14,000
architecture

179
00:06:11,600 --> 00:06:15,280
otherwise you're going to need to go in

180
00:06:14,000 --> 00:06:17,440
and

181
00:06:15,280 --> 00:06:20,638
potentially change the circuits every

182
00:06:17,440 --> 00:06:23,360
time that you need to make a tweak

183
00:06:20,639 --> 00:06:24,400
now if you don't need the pii you're

184
00:06:23,360 --> 00:06:26,240
going to ask yourself whether you're

185
00:06:24,400 --> 00:06:28,400
dealing with structured data

186
00:06:26,240 --> 00:06:31,600
with a predefined format or unstructured

187
00:06:28,400 --> 00:06:33,359
data such as text images video or speech

188
00:06:31,600 --> 00:06:34,960
if you're dealing with structured data

189
00:06:33,360 --> 00:06:35,440
some of the options that are available

190
00:06:34,960 --> 00:06:37,840
to you

191
00:06:35,440 --> 00:06:39,759
are using data aggregation plus

192
00:06:37,840 --> 00:06:42,880
differential privacy

193
00:06:39,759 --> 00:06:44,319
and so for example for location data

194
00:06:42,880 --> 00:06:47,520
privacy

195
00:06:44,319 --> 00:06:48,240
this is a great tactic or you could

196
00:06:47,520 --> 00:06:50,960
choose to use

197
00:06:48,240 --> 00:06:51,919
risk-based data de-identification or

198
00:06:50,960 --> 00:06:55,758
risk-based

199
00:06:51,919 --> 00:06:57,919
data synthesis so these should always be

200
00:06:55,759 --> 00:07:00,319
combined with encryption and transit

201
00:06:57,919 --> 00:07:02,799
and ideally encryption at rest as well

202
00:07:00,319 --> 00:07:05,840
just for that extra layer of security

203
00:07:02,800 --> 00:07:08,160
and also strict access control so

204
00:07:05,840 --> 00:07:10,479
uh privacy technologies are very much

205
00:07:08,160 --> 00:07:12,479
like security technologies

206
00:07:10,479 --> 00:07:13,758
you're not going to get a hundred

207
00:07:12,479 --> 00:07:15,680
percent 100

208
00:07:13,759 --> 00:07:19,120
is a myth you won't have an antivirus

209
00:07:15,680 --> 00:07:22,400
that stops all possible

210
00:07:19,120 --> 00:07:25,919
viruses from getting into your computer

211
00:07:22,400 --> 00:07:28,960
but you can still buy an antivirus

212
00:07:25,919 --> 00:07:31,440
and add extra layers of protection

213
00:07:28,960 --> 00:07:32,560
to prevent any harm coming to you or to

214
00:07:31,440 --> 00:07:36,800
your organization

215
00:07:32,560 --> 00:07:40,000
so with data protection technologies

216
00:07:36,800 --> 00:07:42,800
uh privacy enhancing technologies uh

217
00:07:40,000 --> 00:07:43,840
the more you consider around access

218
00:07:42,800 --> 00:07:46,560
control

219
00:07:43,840 --> 00:07:48,719
the more you consider around how much

220
00:07:46,560 --> 00:07:51,599
obfuscation you can add to

221
00:07:48,720 --> 00:07:53,039
sensitive data the better if you're

222
00:07:51,599 --> 00:07:56,159
dealing with unstructured data

223
00:07:53,039 --> 00:07:58,000
very similar you can choose to use

224
00:07:56,160 --> 00:08:01,280
risk-based identification

225
00:07:58,000 --> 00:08:03,280
or risk-based data synthesis using

226
00:08:01,280 --> 00:08:04,638
ai so both of these methods should be

227
00:08:03,280 --> 00:08:07,520
using ai

228
00:08:04,639 --> 00:08:09,599
and not regular expressions and it

229
00:08:07,520 --> 00:08:11,758
should also be combined with encrypting

230
00:08:09,599 --> 00:08:14,240
and transit encryption at rest and

231
00:08:11,759 --> 00:08:17,520
strict access controls

232
00:08:14,240 --> 00:08:18,000
data identification in a way can also be

233
00:08:17,520 --> 00:08:20,000
that

234
00:08:18,000 --> 00:08:21,840
extra layer of strict access control

235
00:08:20,000 --> 00:08:23,440
where only some people get access to the

236
00:08:21,840 --> 00:08:26,318
sensitive information

237
00:08:23,440 --> 00:08:29,120
and other people get access to the

238
00:08:26,319 --> 00:08:30,720
de-identified data

239
00:08:29,120 --> 00:08:32,159
okay so we went through a lot of

240
00:08:30,720 --> 00:08:32,959
technologies including differential

241
00:08:32,159 --> 00:08:34,479
privacy

242
00:08:32,958 --> 00:08:36,640
de-identification under which

243
00:08:34,479 --> 00:08:38,240
anonymization and pseudonymization fall

244
00:08:36,640 --> 00:08:40,640
and data synthesis

245
00:08:38,240 --> 00:08:42,399
so differential privacy has mathematical

246
00:08:40,640 --> 00:08:44,000
guarantees of privacy depending on the

247
00:08:42,399 --> 00:08:46,000
amount of noise that you add to the

248
00:08:44,000 --> 00:08:47,680
result of a query to a data set for

249
00:08:46,000 --> 00:08:48,959
example or

250
00:08:47,680 --> 00:08:50,959
to the amount of noise that you add

251
00:08:48,959 --> 00:08:52,160
during differentially private gradient

252
00:08:50,959 --> 00:08:53,279
descent when you're training machine

253
00:08:52,160 --> 00:08:55,600
learning model

254
00:08:53,279 --> 00:08:57,200
and often there's an accuracy noise

255
00:08:55,600 --> 00:09:00,000
trade-off

256
00:08:57,200 --> 00:09:01,519
so if you are querying a data set adding

257
00:09:00,000 --> 00:09:04,959
a certain amount of noise

258
00:09:01,519 --> 00:09:05,839
you will get less and less precise

259
00:09:04,959 --> 00:09:08,959
results about

260
00:09:05,839 --> 00:09:11,600
your data set but in the terms of

261
00:09:08,959 --> 00:09:13,599
machine learning models and using

262
00:09:11,600 --> 00:09:15,760
differentially private gradient descent

263
00:09:13,600 --> 00:09:17,440
there's some research that shows that if

264
00:09:15,760 --> 00:09:21,040
you get the right

265
00:09:17,440 --> 00:09:22,640
level of noise this accuracy noise

266
00:09:21,040 --> 00:09:24,399
trade-off does not necessarily exist

267
00:09:22,640 --> 00:09:26,080
especially if you're

268
00:09:24,399 --> 00:09:27,519
training your data on specific

269
00:09:26,080 --> 00:09:28,560
information that you don't want it to

270
00:09:27,519 --> 00:09:30,800
memorize

271
00:09:28,560 --> 00:09:32,640
and wanted to make generalizations about

272
00:09:30,800 --> 00:09:35,760
what happens across multiple users

273
00:09:32,640 --> 00:09:38,959
and then deploy that model in the wild

274
00:09:35,760 --> 00:09:40,240
where those uh oddities don't occur as

275
00:09:38,959 --> 00:09:43,439
much

276
00:09:40,240 --> 00:09:46,480
in terms of de-identification

277
00:09:43,440 --> 00:09:47,600
um make sure that if you are doing the

278
00:09:46,480 --> 00:09:49,279
identification you're

279
00:09:47,600 --> 00:09:50,640
using stronger identification risk

280
00:09:49,279 --> 00:09:52,720
metrics a lot

281
00:09:50,640 --> 00:09:54,800
of bad press has gone around

282
00:09:52,720 --> 00:09:57,200
anonymization

283
00:09:54,800 --> 00:09:58,399
because people do not use the term

284
00:09:57,200 --> 00:10:01,600
appropriately

285
00:09:58,399 --> 00:10:03,920
and because a lot of the times when

286
00:10:01,600 --> 00:10:04,800
companies say they have anonymized data

287
00:10:03,920 --> 00:10:07,360
they have

288
00:10:04,800 --> 00:10:08,160
absolutely not anonymized data and have

289
00:10:07,360 --> 00:10:11,519
left

290
00:10:08,160 --> 00:10:12,939
very clear personal identifiers within

291
00:10:11,519 --> 00:10:14,240
the data sets

292
00:10:12,940 --> 00:10:16,320
[Music]

293
00:10:14,240 --> 00:10:18,240
in some cases they're less clear than

294
00:10:16,320 --> 00:10:21,680
others so for example

295
00:10:18,240 --> 00:10:23,920
the netflix imdb scandal

296
00:10:21,680 --> 00:10:25,359
where there was cross preference between

297
00:10:23,920 --> 00:10:28,640
reviews from netflix and

298
00:10:25,360 --> 00:10:31,920
imdb that was based on timing

299
00:10:28,640 --> 00:10:33,199
of when reviews were posted so it is

300
00:10:31,920 --> 00:10:35,599
important to be careful with that

301
00:10:33,200 --> 00:10:36,399
and talk to experts about what you're

302
00:10:35,600 --> 00:10:38,240
doing there

303
00:10:36,399 --> 00:10:40,240
um so this is one problem that we're

304
00:10:38,240 --> 00:10:42,480
solving for unstructured data

305
00:10:40,240 --> 00:10:46,160
specifically at private ai

306
00:10:42,480 --> 00:10:49,680
and uh essentially yeah it's for

307
00:10:46,160 --> 00:10:51,439
it's highly accurate and it is a

308
00:10:49,680 --> 00:10:53,279
very low latency and runs directly on

309
00:10:51,440 --> 00:10:54,800
premise or on device and

310
00:10:53,279 --> 00:10:57,120
super easy to integrate with three lines

311
00:10:54,800 --> 00:11:00,399
of code shameless plug

312
00:10:57,120 --> 00:11:03,279
so for data synthesis uh

313
00:11:00,399 --> 00:11:04,320
if you do want to integrate data

314
00:11:03,279 --> 00:11:07,600
synthesis

315
00:11:04,320 --> 00:11:07,600
to create data that

316
00:11:08,079 --> 00:11:13,680
you can use for training your models for

317
00:11:12,000 --> 00:11:16,000
getting insights about a population

318
00:11:13,680 --> 00:11:18,560
without having um

319
00:11:16,000 --> 00:11:20,480
to specifically look at the details

320
00:11:18,560 --> 00:11:21,760
associated to a speci to a one

321
00:11:20,480 --> 00:11:25,279
individual

322
00:11:21,760 --> 00:11:27,279
then uh you should also have strong

323
00:11:25,279 --> 00:11:28,399
risk metrics for re-identification for

324
00:11:27,279 --> 00:11:30,240
that

325
00:11:28,399 --> 00:11:31,680
and it is often better to get a third

326
00:11:30,240 --> 00:11:32,880
party to work on it too so

327
00:11:31,680 --> 00:11:34,800
if you are looking into this

328
00:11:32,880 --> 00:11:36,079
specifically for healthcare data there

329
00:11:34,800 --> 00:11:37,359
is a great company called replica

330
00:11:36,079 --> 00:11:39,680
analytics

331
00:11:37,360 --> 00:11:41,440
that is under the direction of professor

332
00:11:39,680 --> 00:11:43,519
caladella mom that i'd recommend you

333
00:11:41,440 --> 00:11:45,519
take a look at

334
00:11:43,519 --> 00:11:47,200
okay so let's go back to the decision

335
00:11:45,519 --> 00:11:48,800
tree

336
00:11:47,200 --> 00:11:50,720
let's go back to do you have to share

337
00:11:48,800 --> 00:11:52,160
insights or share a data set that needs

338
00:11:50,720 --> 00:11:54,560
to be visible

339
00:11:52,160 --> 00:11:57,839
we went over the data set branch now

340
00:11:54,560 --> 00:11:59,839
let's go through the insides branch

341
00:11:57,839 --> 00:12:01,279
if you have to share insights the next

342
00:11:59,839 --> 00:12:02,800
question you ask yourself is

343
00:12:01,279 --> 00:12:06,800
are you making generalizations over

344
00:12:02,800 --> 00:12:09,359
population or user specific predictions

345
00:12:06,800 --> 00:12:10,079
if you're making generalizations you're

346
00:12:09,360 --> 00:12:12,639
going to ask yourself

347
00:12:10,079 --> 00:12:14,160
if latency is a critical requirement or

348
00:12:12,639 --> 00:12:14,880
whether computations can take a little

349
00:12:14,160 --> 00:12:16,800
longer

350
00:12:14,880 --> 00:12:19,120
and be approximated using polynomial

351
00:12:16,800 --> 00:12:21,599
operations

352
00:12:19,120 --> 00:12:22,639
if low latency is a critical requirement

353
00:12:21,600 --> 00:12:24,720
or if

354
00:12:22,639 --> 00:12:26,079
your operations can't be approximated

355
00:12:24,720 --> 00:12:27,680
using polynomials

356
00:12:26,079 --> 00:12:30,079
you'll want to use trusted execution

357
00:12:27,680 --> 00:12:33,120
environments such as intel sgx

358
00:12:30,079 --> 00:12:36,880
and these are pretty much available

359
00:12:33,120 --> 00:12:40,240
in any modern intel chip

360
00:12:36,880 --> 00:12:42,959
and also you can find them on the

361
00:12:40,240 --> 00:12:45,440
microsoft confidential compute cloud

362
00:12:42,959 --> 00:12:46,959
if you can wait and you can make

363
00:12:45,440 --> 00:12:49,200
approximations that

364
00:12:46,959 --> 00:12:51,119
using polynomial operations you can

365
00:12:49,200 --> 00:12:52,959
consider using homomorphic encryption

366
00:12:51,120 --> 00:12:55,519
and homomorphic encryption allows you to

367
00:12:52,959 --> 00:12:58,479
perform computations on encrypted data

368
00:12:55,519 --> 00:13:00,079
so suppose you're a user who's

369
00:12:58,480 --> 00:13:02,079
encrypting your information

370
00:13:00,079 --> 00:13:03,839
sending that information to the cloud

371
00:13:02,079 --> 00:13:05,120
the cloud is processing it in the

372
00:13:03,839 --> 00:13:07,040
encrypted domain

373
00:13:05,120 --> 00:13:08,560
sends it back to the user and the user

374
00:13:07,040 --> 00:13:10,240
decrypts it

375
00:13:08,560 --> 00:13:12,239
and without the cloud ever being any of

376
00:13:10,240 --> 00:13:14,880
the wiser of what information was

377
00:13:12,240 --> 00:13:14,880
sent over

378
00:13:15,760 --> 00:13:18,959
so move back up the decision tree are

379
00:13:17,519 --> 00:13:21,760
you making generalizations over a

380
00:13:18,959 --> 00:13:26,079
population or user specific predictions

381
00:13:21,760 --> 00:13:26,079
let's go to user specific so

382
00:13:27,120 --> 00:13:31,920
then you're faced with a question are

383
00:13:29,600 --> 00:13:34,639
the other parties contributing

384
00:13:31,920 --> 00:13:36,479
sensitive input data or can you compute

385
00:13:34,639 --> 00:13:37,680
the output without additional sensitive

386
00:13:36,480 --> 00:13:40,160
information

387
00:13:37,680 --> 00:13:40,719
so if you need other parties input can

388
00:13:40,160 --> 00:13:43,120
you

389
00:13:40,720 --> 00:13:43,920
or at higher communication costs and

390
00:13:43,120 --> 00:13:46,560
have

391
00:13:43,920 --> 00:13:48,800
repeatable algorithms to run this comes

392
00:13:46,560 --> 00:13:52,399
back to secure multi-party computation

393
00:13:48,800 --> 00:13:54,719
if the answer is uh yes

394
00:13:52,399 --> 00:13:56,000
now if you do cannot afford higher

395
00:13:54,720 --> 00:13:58,639
communication costs

396
00:13:56,000 --> 00:13:59,920
and or um do not have repeatable

397
00:13:58,639 --> 00:14:03,120
algorithms to run

398
00:13:59,920 --> 00:14:06,399
so if you need to make lots of tweaks um

399
00:14:03,120 --> 00:14:07,120
or if sometimes the communication is cut

400
00:14:06,399 --> 00:14:09,839
off between

401
00:14:07,120 --> 00:14:10,320
you and the people whose data you need

402
00:14:09,839 --> 00:14:12,079
and

403
00:14:10,320 --> 00:14:14,160
then what you're gonna have to do is get

404
00:14:12,079 --> 00:14:16,319
a data processor agreement in place

405
00:14:14,160 --> 00:14:18,160
encrypt and transit at rest and have

406
00:14:16,320 --> 00:14:20,800
once again very strict access controls

407
00:14:18,160 --> 00:14:22,079
to this data

408
00:14:20,800 --> 00:14:23,279
if the decision tree was not enough to

409
00:14:22,079 --> 00:14:24,479
help you determine whether to use

410
00:14:23,279 --> 00:14:26,160
homomorphic encryption secure

411
00:14:24,480 --> 00:14:28,880
multi-party computation or

412
00:14:26,160 --> 00:14:29,760
trusted execution environments for your

413
00:14:28,880 --> 00:14:33,279
use case

414
00:14:29,760 --> 00:14:34,720
here is a bit of more of a breakdown of

415
00:14:33,279 --> 00:14:36,320
what kind of trade-offs there are with

416
00:14:34,720 --> 00:14:39,920
each of these technologies

417
00:14:36,320 --> 00:14:41,279
so quantum safety is guaranteed by

418
00:14:39,920 --> 00:14:42,959
homomorphic encryption and secure

419
00:14:41,279 --> 00:14:45,760
multi-party computation

420
00:14:42,959 --> 00:14:46,479
that just means that they're based on

421
00:14:45,760 --> 00:14:49,199
schemes

422
00:14:46,480 --> 00:14:50,240
that have not been broken by a quantum

423
00:14:49,199 --> 00:14:53,519
algorithm yet

424
00:14:50,240 --> 00:14:56,800
not that there cannot exist one

425
00:14:53,519 --> 00:14:57,360
uh for homomorphic encryption you can

426
00:14:56,800 --> 00:14:59,359
still

427
00:14:57,360 --> 00:15:00,639
reverse engineer model weights based on

428
00:14:59,360 --> 00:15:01,839
the outputs

429
00:15:00,639 --> 00:15:03,680
if you're training machine learning

430
00:15:01,839 --> 00:15:05,120
models same thing for secure multi-party

431
00:15:03,680 --> 00:15:07,680
computation

432
00:15:05,120 --> 00:15:08,800
you also can reverse engineer training

433
00:15:07,680 --> 00:15:12,239
data

434
00:15:08,800 --> 00:15:13,839
so you might want to combine them with

435
00:15:12,240 --> 00:15:16,000
a differential privacy if you're using

436
00:15:13,839 --> 00:15:17,279
one of the other in machine learning

437
00:15:16,000 --> 00:15:19,839
specifically differentially private

438
00:15:17,279 --> 00:15:22,160
guardian descent of course you

439
00:15:19,839 --> 00:15:23,600
can use them for multiple other things

440
00:15:22,160 --> 00:15:26,480
than machine learning

441
00:15:23,600 --> 00:15:27,920
homomorphic encryption works

442
00:15:26,480 --> 00:15:30,240
straightforwardly enough if you're

443
00:15:27,920 --> 00:15:33,839
implementing polynomial operations

444
00:15:30,240 --> 00:15:34,800
um you can have alternatives or

445
00:15:33,839 --> 00:15:37,920
approximations

446
00:15:34,800 --> 00:15:40,240
of non-polynomial functions popular ones

447
00:15:37,920 --> 00:15:41,360
like relu and sigmoid for machine

448
00:15:40,240 --> 00:15:42,560
learning if that's what you're looking

449
00:15:41,360 --> 00:15:44,720
to use it for

450
00:15:42,560 --> 00:15:46,160
and there are plenty of libraries out

451
00:15:44,720 --> 00:15:49,519
there that you can

452
00:15:46,160 --> 00:15:51,519
use to make this

453
00:15:49,519 --> 00:15:53,440
to integrate homomorphic encryption into

454
00:15:51,519 --> 00:15:55,839
your software including

455
00:15:53,440 --> 00:15:57,199
palisade out of new jersey institute of

456
00:15:55,839 --> 00:15:58,720
technology

457
00:15:57,199 --> 00:16:01,359
there's spark fhe that helps you

458
00:15:58,720 --> 00:16:03,920
interface with microsoft seal

459
00:16:01,360 --> 00:16:05,759
and that's out of the rochester

460
00:16:03,920 --> 00:16:08,319
institute technology

461
00:16:05,759 --> 00:16:09,759
there's htlib from mit unfortunately

462
00:16:08,320 --> 00:16:11,920
there wasn't enough

463
00:16:09,759 --> 00:16:12,800
space here to go over each one of them

464
00:16:11,920 --> 00:16:14,160
but

465
00:16:12,800 --> 00:16:15,839
please feel free to email me if you have

466
00:16:14,160 --> 00:16:18,959
questions and

467
00:16:15,839 --> 00:16:19,600
in terms of computational complexity if

468
00:16:18,959 --> 00:16:22,479
you're

469
00:16:19,600 --> 00:16:24,480
properly parallelizing and if you're

470
00:16:22,480 --> 00:16:28,399
properly

471
00:16:24,480 --> 00:16:30,560
converting your regular

472
00:16:28,399 --> 00:16:32,160
algorithm into the homomorphic encrypt

473
00:16:30,560 --> 00:16:35,040
homomorphically encrypted

474
00:16:32,160 --> 00:16:37,279
space then you should be getting three

475
00:16:35,040 --> 00:16:40,240
to four orders of magnitude

476
00:16:37,279 --> 00:16:42,320
difference in compute cost as for secure

477
00:16:40,240 --> 00:16:45,440
multi-party computation

478
00:16:42,320 --> 00:16:46,240
the communication cost grows linearly

479
00:16:45,440 --> 00:16:47,680
you don't have to deal with

480
00:16:46,240 --> 00:16:48,839
communication costs with homomorphic

481
00:16:47,680 --> 00:16:52,638
encryption

482
00:16:48,839 --> 00:16:54,959
um but uh the computation costs for

483
00:16:52,639 --> 00:16:57,199
secure multi-party computation are

484
00:16:54,959 --> 00:16:58,560
approximately linear in the depth of the

485
00:16:57,199 --> 00:17:01,359
circuit that you're building

486
00:16:58,560 --> 00:17:03,040
so as i mentioned uh you're going to

487
00:17:01,360 --> 00:17:04,720
likely build garbled circuits for secure

488
00:17:03,040 --> 00:17:08,399
multi-party computation

489
00:17:04,720 --> 00:17:11,760
and those can be complicated to modify

490
00:17:08,400 --> 00:17:11,760
so you have to have a preset

491
00:17:11,919 --> 00:17:17,520
yeah preset algorithm in mind for you to

492
00:17:15,359 --> 00:17:19,039
build one

493
00:17:17,520 --> 00:17:20,720
openmind is working on making a

494
00:17:19,039 --> 00:17:23,119
production ready library

495
00:17:20,720 --> 00:17:24,240
uh for integrating secure multi-part

496
00:17:23,119 --> 00:17:27,438
computation

497
00:17:24,240 --> 00:17:29,120
into uh privacy preserving machine

498
00:17:27,439 --> 00:17:32,480
learning

499
00:17:29,120 --> 00:17:35,120
okay so trusted execution environments

500
00:17:32,480 --> 00:17:35,760
uh there are no mathematical guarantees

501
00:17:35,120 --> 00:17:37,439
possible

502
00:17:35,760 --> 00:17:40,400
for trusted execution environments

503
00:17:37,440 --> 00:17:42,960
because it's a hardware-based solution

504
00:17:40,400 --> 00:17:43,440
that would that is just much too complex

505
00:17:42,960 --> 00:17:45,200
to

506
00:17:43,440 --> 00:17:46,559
get mathematical guarantees of privacy

507
00:17:45,200 --> 00:17:50,880
for

508
00:17:46,559 --> 00:17:52,000
and you also can have an adversary who

509
00:17:50,880 --> 00:17:53,120
reverse engineers

510
00:17:52,000 --> 00:17:55,039
model weights if you're dealing with

511
00:17:53,120 --> 00:17:58,239
machine learning models or

512
00:17:55,039 --> 00:17:59,919
training data as well so also something

513
00:17:58,240 --> 00:18:01,840
to consider differential privacy

514
00:17:59,919 --> 00:18:05,200
differential privacy for

515
00:18:01,840 --> 00:18:06,320
um as sgx is available as i mentioned on

516
00:18:05,200 --> 00:18:09,360
most

517
00:18:06,320 --> 00:18:10,879
intel chips uh in mod currently deployed

518
00:18:09,360 --> 00:18:13,840
in modern computers

519
00:18:10,880 --> 00:18:15,280
and uh you often need to combine this

520
00:18:13,840 --> 00:18:17,120
with oblivious ram

521
00:18:15,280 --> 00:18:19,360
in order to avoid attacks that are based

522
00:18:17,120 --> 00:18:22,639
on uh code branching information

523
00:18:19,360 --> 00:18:25,280
uh that um might show you different act

524
00:18:22,640 --> 00:18:25,760
so different access patterns can tell

525
00:18:25,280 --> 00:18:29,918
you

526
00:18:25,760 --> 00:18:33,520
what kind of branch a

527
00:18:29,919 --> 00:18:36,559
your code took in an if-else statement

528
00:18:33,520 --> 00:18:38,400
if the batch size uh

529
00:18:36,559 --> 00:18:40,399
the size of the the information that

530
00:18:38,400 --> 00:18:44,000
you're sending to

531
00:18:40,400 --> 00:18:46,320
your secure enclave is small enough then

532
00:18:44,000 --> 00:18:47,840
uh the computational cost can be lower

533
00:18:46,320 --> 00:18:50,799
than one order of magnitude

534
00:18:47,840 --> 00:18:52,000
which is pretty awesome okay so that was

535
00:18:50,799 --> 00:18:54,480
a lot of information

536
00:18:52,000 --> 00:18:56,480
uh there is still a lot more to share

537
00:18:54,480 --> 00:18:58,960
about privacy technologies but

538
00:18:56,480 --> 00:19:00,559
uh happy to answer those questions if

539
00:18:58,960 --> 00:19:02,400
you have any

540
00:19:00,559 --> 00:19:03,918
well the takeaways that we'd like you to

541
00:19:02,400 --> 00:19:07,280
get out of this talk are that

542
00:19:03,919 --> 00:19:09,360
one privacy is possible and it is

543
00:19:07,280 --> 00:19:11,200
possible to protect users data

544
00:19:09,360 --> 00:19:12,959
while continuously making that data

545
00:19:11,200 --> 00:19:15,760
privacy practical for

546
00:19:12,960 --> 00:19:18,080
different tasks but the key is finding

547
00:19:15,760 --> 00:19:20,480
the right technology to do so

548
00:19:18,080 --> 00:19:21,360
and privacy is also practical because

549
00:19:20,480 --> 00:19:22,880
you're

550
00:19:21,360 --> 00:19:24,000
not only maintaining customer trust

551
00:19:22,880 --> 00:19:25,200
you're lowering the risk of data

552
00:19:24,000 --> 00:19:27,679
protection

553
00:19:25,200 --> 00:19:29,039
and data privacy fines and you're

554
00:19:27,679 --> 00:19:31,679
gaining a pretty

555
00:19:29,039 --> 00:19:32,879
significant competitive advantage in

556
00:19:31,679 --> 00:19:35,200
certain cases

557
00:19:32,880 --> 00:19:36,880
and you can also often get access to

558
00:19:35,200 --> 00:19:40,480
more data

559
00:19:36,880 --> 00:19:43,760
so thank you for uh attending talk

560
00:19:40,480 --> 00:19:46,640
and uh feel free to follow us on twitter

561
00:19:43,760 --> 00:19:48,000
uh private ai also has a blog and we're

562
00:19:46,640 --> 00:19:49,200
gonna post the decision tree to that

563
00:19:48,000 --> 00:19:52,720
blog if you

564
00:19:49,200 --> 00:19:56,160
uh want a more complete version of it

565
00:19:52,720 --> 00:19:58,640
please yeah any questions that you have

566
00:19:56,160 --> 00:20:01,280
feel free to either ask them now or

567
00:19:58,640 --> 00:20:01,280
contact us

568
00:20:04,840 --> 00:20:07,840
later

569
00:20:09,840 --> 00:20:11,918
you

