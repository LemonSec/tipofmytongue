1
00:00:09,120 --> 00:00:12,399
good afternoon everyone

2
00:00:10,880 --> 00:00:14,719
thank you in advance for staying with me

3
00:00:12,400 --> 00:00:17,279
for the last session of today

4
00:00:14,719 --> 00:00:19,439
when engineers and lawyers talk

5
00:00:17,279 --> 00:00:21,920
right-sizing your data protection risk

6
00:00:19,439 --> 00:00:21,920
profile

7
00:00:22,080 --> 00:00:26,479
i realize i'm standing between you and

8
00:00:24,240 --> 00:00:29,839
the virtual ice cream social

9
00:00:26,480 --> 00:00:32,079
so let's try to wrap this up on time

10
00:00:29,840 --> 00:00:33,920
rafi buddy director of security and

11
00:00:32,079 --> 00:00:36,000
compliance at mode

12
00:00:33,920 --> 00:00:39,360
my social media handles in case you want

13
00:00:36,000 --> 00:00:39,360
to get in touch after the talk

14
00:00:40,160 --> 00:00:45,519
so what do i do i build data protection

15
00:00:42,480 --> 00:00:47,919
programs at startups

16
00:00:45,520 --> 00:00:49,120
like i said currently at mode i was

17
00:00:47,920 --> 00:00:51,199
previously at

18
00:00:49,120 --> 00:00:52,480
another startup called healthtab and

19
00:00:51,199 --> 00:00:54,719
prior to that at

20
00:00:52,480 --> 00:00:57,199
new relic at that time a free ipo

21
00:00:54,719 --> 00:00:59,760
company as well

22
00:00:57,199 --> 00:01:01,920
it so happens that i am also a licensed

23
00:00:59,760 --> 00:01:04,720
california attorney

24
00:01:01,920 --> 00:01:05,439
and on this journey in my career i view

25
00:01:04,720 --> 00:01:08,640
my role

26
00:01:05,438 --> 00:01:12,479
not so much as a lawyer but as a legal

27
00:01:08,640 --> 00:01:15,520
engineer an engineer trained in the law

28
00:01:12,479 --> 00:01:15,520
and its application

29
00:01:15,920 --> 00:01:21,119
so what is this talk about

30
00:01:18,960 --> 00:01:22,559
the thesis of this presentation is that

31
00:01:21,119 --> 00:01:26,799
practical engineering

32
00:01:22,560 --> 00:01:30,400
insights go a long way in simplifying

33
00:01:26,799 --> 00:01:32,560
data protection compliance

34
00:01:30,400 --> 00:01:34,479
in practice the biggest hurdle in

35
00:01:32,560 --> 00:01:36,880
addressing compliance needs

36
00:01:34,479 --> 00:01:38,400
is recognizing the most important risks

37
00:01:36,880 --> 00:01:41,600
for a company

38
00:01:38,400 --> 00:01:42,799
based on its unique risk profile for

39
00:01:41,600 --> 00:01:45,520
example

40
00:01:42,799 --> 00:01:46,079
what type of data can you collect

41
00:01:45,520 --> 00:01:49,280
process

42
00:01:46,079 --> 00:01:50,079
and store what type of controls are

43
00:01:49,280 --> 00:01:54,240
adequate

44
00:01:50,079 --> 00:01:57,919
or reasonable what type of obligations

45
00:01:54,240 --> 00:02:00,000
should you accept or enforce

46
00:01:57,920 --> 00:02:01,680
the answer to these questions is rarely

47
00:02:00,000 --> 00:02:03,600
one size fits all

48
00:02:01,680 --> 00:02:06,840
but rather based on the company's

49
00:02:03,600 --> 00:02:09,919
specific scenarios

50
00:02:06,840 --> 00:02:11,280
however the path to navigating data

51
00:02:09,919 --> 00:02:14,640
protection risks

52
00:02:11,280 --> 00:02:18,480
is often filled with uncertainty

53
00:02:14,640 --> 00:02:20,958
overestimating the risks stifles growth

54
00:02:18,480 --> 00:02:22,799
but underestimating them can derail the

55
00:02:20,959 --> 00:02:25,120
business

56
00:02:22,800 --> 00:02:25,920
just like when you're driving too fast

57
00:02:25,120 --> 00:02:28,640
you may risk

58
00:02:25,920 --> 00:02:29,920
getting a ticket but if you drive too

59
00:02:28,640 --> 00:02:32,000
slow

60
00:02:29,920 --> 00:02:33,839
you may not get to your destination on

61
00:02:32,000 --> 00:02:36,560
time

62
00:02:33,840 --> 00:02:37,360
so to be able to measure data protection

63
00:02:36,560 --> 00:02:39,280
risks

64
00:02:37,360 --> 00:02:40,720
and right size the risk profile of a

65
00:02:39,280 --> 00:02:42,640
company

66
00:02:40,720 --> 00:02:44,080
we need to view them from both a

67
00:02:42,640 --> 00:02:46,799
technical and

68
00:02:44,080 --> 00:02:46,800
a legal lens

69
00:02:48,400 --> 00:02:52,720
what you see in this picture is a

70
00:02:51,040 --> 00:02:56,640
scientific phenomenon called

71
00:02:52,720 --> 00:02:59,840
parallax parallax is the

72
00:02:56,640 --> 00:03:01,760
apparent displacement of an object

73
00:02:59,840 --> 00:03:03,440
because of a change in the observer's

74
00:03:01,760 --> 00:03:05,040
point of view

75
00:03:03,440 --> 00:03:08,000
astronomers use this technique to

76
00:03:05,040 --> 00:03:11,280
measure distance to the stars

77
00:03:08,000 --> 00:03:12,800
in our world parallax occurs when

78
00:03:11,280 --> 00:03:15,440
engineers and lawyers

79
00:03:12,800 --> 00:03:17,680
view the threats to an organization from

80
00:03:15,440 --> 00:03:20,720
different perspectives

81
00:03:17,680 --> 00:03:22,239
so to address this dilemma engineers and

82
00:03:20,720 --> 00:03:28,000
lawyers need to do something

83
00:03:22,239 --> 00:03:28,000
they don't usually do they need to talk

84
00:03:28,879 --> 00:03:32,959
anyone knows what happened when an

85
00:03:30,239 --> 00:03:35,920
engineer and a lawyer walked into a bar

86
00:03:32,959 --> 00:03:37,760
yeah nothing because they never did

87
00:03:35,920 --> 00:03:38,879
except in this case the sentence ends

88
00:03:37,760 --> 00:03:41,760
with

89
00:03:38,879 --> 00:03:43,359
and he orders a drink because that

90
00:03:41,760 --> 00:03:46,879
engineer and lawyer

91
00:03:43,360 --> 00:03:49,200
is both one person and he is talking to

92
00:03:46,879 --> 00:03:52,239
you right now

93
00:03:49,200 --> 00:03:56,480
so when i walk into the bar with myself

94
00:03:52,239 --> 00:03:57,760
what do i see i'm able to see data

95
00:03:56,480 --> 00:04:00,798
protection risks

96
00:03:57,760 --> 00:04:03,599
from both the technical and legal events

97
00:04:00,799 --> 00:04:06,159
and in my experience it distills down to

98
00:04:03,599 --> 00:04:08,560
the following

99
00:04:06,159 --> 00:04:10,000
the biggest transition that engineers

100
00:04:08,560 --> 00:04:12,879
have to make

101
00:04:10,000 --> 00:04:14,640
is viewing things from data subjects

102
00:04:12,879 --> 00:04:17,120
perspective

103
00:04:14,640 --> 00:04:18,238
because in the world of data breaches it

104
00:04:17,120 --> 00:04:21,040
is not just about

105
00:04:18,238 --> 00:04:22,960
authorization it is about reasonable

106
00:04:21,040 --> 00:04:26,080
expectation

107
00:04:22,960 --> 00:04:29,680
it's at the core of gdpr ccpa and almost

108
00:04:26,080 --> 00:04:31,440
all data protection laws

109
00:04:29,680 --> 00:04:34,000
so what does it mean to honor data

110
00:04:31,440 --> 00:04:36,479
subject expectation

111
00:04:34,000 --> 00:04:38,639
it means getting clarity on the risks

112
00:04:36,479 --> 00:04:41,199
that are introduced throughout the data

113
00:04:38,639 --> 00:04:44,000
life cycle

114
00:04:41,199 --> 00:04:46,160
for example if you use customer data for

115
00:04:44,000 --> 00:04:48,880
other than business purpose

116
00:04:46,160 --> 00:04:48,880
you are liable

117
00:04:49,199 --> 00:04:53,680
once you have the clarity of purpose you

118
00:04:51,520 --> 00:04:55,520
must implement controls

119
00:04:53,680 --> 00:04:57,199
to allow data to be used for that

120
00:04:55,520 --> 00:04:59,440
purpose

121
00:04:57,199 --> 00:05:01,360
this includes not just technical

122
00:04:59,440 --> 00:05:02,479
security but also contractual

123
00:05:01,360 --> 00:05:06,160
obligations

124
00:05:02,479 --> 00:05:08,560
both upstream and downstream

125
00:05:06,160 --> 00:05:10,000
so if your vendor or partner misuses

126
00:05:08,560 --> 00:05:14,240
your customer data

127
00:05:10,000 --> 00:05:14,240
it's a breach that you are liable for

128
00:05:14,479 --> 00:05:19,280
and the controls must also be adequate

129
00:05:16,960 --> 00:05:22,080
and reasonable given the type of data

130
00:05:19,280 --> 00:05:23,520
and the purpose of use implementing

131
00:05:22,080 --> 00:05:26,159
reasonable controls that

132
00:05:23,520 --> 00:05:27,039
correlate with reducing the data

133
00:05:26,160 --> 00:05:30,479
protection

134
00:05:27,039 --> 00:05:31,599
risks you may need to do risk

135
00:05:30,479 --> 00:05:34,479
calibration or

136
00:05:31,600 --> 00:05:36,400
trade-off to prioritize the most

137
00:05:34,479 --> 00:05:38,800
important risk

138
00:05:36,400 --> 00:05:41,599
based on their associated legal

139
00:05:38,800 --> 00:05:41,600
consequences

140
00:05:42,080 --> 00:05:47,520
but how exactly do you right-size your

141
00:05:44,960 --> 00:05:49,280
risk profile

142
00:05:47,520 --> 00:05:50,880
let's take a look at examples from real

143
00:05:49,280 --> 00:05:54,159
life

144
00:05:50,880 --> 00:05:54,159
suppose you are driving a car

145
00:05:54,720 --> 00:06:02,720
when do you buckle your seat belt always

146
00:05:58,960 --> 00:06:02,719
it does not depend on anything

147
00:06:02,880 --> 00:06:10,000
how about turning on your headlights

148
00:06:07,440 --> 00:06:12,719
you do it when the visibility is low

149
00:06:10,000 --> 00:06:16,000
that's what it depends on

150
00:06:12,720 --> 00:06:19,440
and when can you drive

151
00:06:16,000 --> 00:06:21,919
while drunk well as long as your blood

152
00:06:19,440 --> 00:06:24,479
alcohol level is below the legal limit

153
00:06:21,919 --> 00:06:26,318
technically you can i'm not endorsing

154
00:06:24,479 --> 00:06:28,318
that though

155
00:06:26,319 --> 00:06:30,479
but really the point of this analogy is

156
00:06:28,319 --> 00:06:33,120
this there is a risk

157
00:06:30,479 --> 00:06:35,360
in driving without headlights or under

158
00:06:33,120 --> 00:06:37,840
the influence

159
00:06:35,360 --> 00:06:40,000
and the risk depends on technical and

160
00:06:37,840 --> 00:06:43,280
legal aspects

161
00:06:40,000 --> 00:06:44,720
for example the risk of driving without

162
00:06:43,280 --> 00:06:46,880
headlights

163
00:06:44,720 --> 00:06:49,199
depends on the level of ambient light at

164
00:06:46,880 --> 00:06:51,759
which visibility is low

165
00:06:49,199 --> 00:06:53,919
technical perspective but it also

166
00:06:51,759 --> 00:06:57,759
depends on the legal interpretation

167
00:06:53,919 --> 00:06:59,758
of when the visibility is considered low

168
00:06:57,759 --> 00:07:02,960
for example as codified in this

169
00:06:59,759 --> 00:07:06,240
mississippi statute

170
00:07:02,960 --> 00:07:09,280
how close to dusk or dawn how

171
00:07:06,240 --> 00:07:11,840
dense of a fog

172
00:07:09,280 --> 00:07:13,280
these are questions for legal

173
00:07:11,840 --> 00:07:14,638
interpretation

174
00:07:13,280 --> 00:07:17,280
you could choose to drive with

175
00:07:14,639 --> 00:07:20,560
headlights turned on at all times

176
00:07:17,280 --> 00:07:22,318
but that is overestimating the risk

177
00:07:20,560 --> 00:07:24,800
or you may choose to drive without

178
00:07:22,319 --> 00:07:27,680
headlights turned on

179
00:07:24,800 --> 00:07:31,599
until the sunset which is

180
00:07:27,680 --> 00:07:33,840
underestimating the risk

181
00:07:31,599 --> 00:07:36,400
data protection obligations in a way are

182
00:07:33,840 --> 00:07:40,239
no different than the driver's manual

183
00:07:36,400 --> 00:07:42,719
let me explain with another example

184
00:07:40,240 --> 00:07:43,680
suppose turning the headlights on or off

185
00:07:42,720 --> 00:07:47,840
is analogous

186
00:07:43,680 --> 00:07:49,440
to turning encryption on or off

187
00:07:47,840 --> 00:07:51,280
whether or not the data should be

188
00:07:49,440 --> 00:07:52,879
encrypted is a function

189
00:07:51,280 --> 00:07:54,638
of whether it contains personal

190
00:07:52,879 --> 00:07:57,520
information

191
00:07:54,639 --> 00:07:59,280
technical perspective such as pattern

192
00:07:57,520 --> 00:08:00,799
matching

193
00:07:59,280 --> 00:08:02,318
but it also depends on the legal

194
00:08:00,800 --> 00:08:04,879
interpretation of what is considered

195
00:08:02,319 --> 00:08:08,879
personal information

196
00:08:04,879 --> 00:08:08,879
what patterns to actually look for

197
00:08:09,840 --> 00:08:13,679
keeping the data always encrypted even

198
00:08:12,560 --> 00:08:17,120
in memory

199
00:08:13,680 --> 00:08:20,160
is overestimating the risks but never

200
00:08:17,120 --> 00:08:23,360
encrypting the data even in storage

201
00:08:20,160 --> 00:08:24,080
is underestimating the risk a good

202
00:08:23,360 --> 00:08:25,520
trade-off

203
00:08:24,080 --> 00:08:27,599
would be keeping it unencrypted in

204
00:08:25,520 --> 00:08:29,919
memory but encrypting in transit and

205
00:08:27,599 --> 00:08:32,000
storage

206
00:08:29,919 --> 00:08:33,679
and while this is a simple example it

207
00:08:32,000 --> 00:08:38,080
illustrates the idea

208
00:08:33,679 --> 00:08:38,079
of right sizing your risk profile

209
00:08:38,399 --> 00:08:42,958
so now that we have outlined the basic

210
00:08:40,240 --> 00:08:44,880
idea let's put it into action

211
00:08:42,958 --> 00:08:46,640
the best way to understand what right

212
00:08:44,880 --> 00:08:49,040
sizing the risk profile looks like

213
00:08:46,640 --> 00:08:51,120
in practice is by using example

214
00:08:49,040 --> 00:08:52,640
scenarios

215
00:08:51,120 --> 00:08:56,240
and that's what we're going to do in the

216
00:08:52,640 --> 00:08:56,240
remaining half of the presentation

217
00:08:57,600 --> 00:09:02,560
here's our first scenario a company

218
00:09:00,800 --> 00:09:07,839
retains personal data

219
00:09:02,560 --> 00:09:10,959
of consumers for analytics purposes

220
00:09:07,839 --> 00:09:14,000
california resident asked them to

221
00:09:10,959 --> 00:09:18,079
delete all data you have about me

222
00:09:14,000 --> 00:09:21,279
under ccpa can the company keep

223
00:09:18,080 --> 00:09:21,279
the analytics data

224
00:09:21,760 --> 00:09:27,200
first of all why is this even relevant

225
00:09:24,640 --> 00:09:29,519
to security teams

226
00:09:27,200 --> 00:09:31,360
depending on how big is your legal team

227
00:09:29,519 --> 00:09:35,279
most startups don't build out

228
00:09:31,360 --> 00:09:37,040
a legal team that early on

229
00:09:35,279 --> 00:09:39,680
it would be the security and engineering

230
00:09:37,040 --> 00:09:42,399
teams who are called upon to decide

231
00:09:39,680 --> 00:09:42,800
when and what data can be retained to

232
00:09:42,399 --> 00:09:46,399
meet

233
00:09:42,800 --> 00:09:46,399
a specific business objective

234
00:09:46,800 --> 00:09:50,479
at the minimum even if you have a legal

235
00:09:49,920 --> 00:09:52,719
team

236
00:09:50,480 --> 00:09:55,279
you will be expected to ensure that the

237
00:09:52,720 --> 00:09:59,200
data retention policies

238
00:09:55,279 --> 00:10:00,800
are in compliance with your obligations

239
00:09:59,200 --> 00:10:03,760
now that can be a tricky balance to

240
00:10:00,800 --> 00:10:06,399
maintain you may not

241
00:10:03,760 --> 00:10:07,600
have the leverage to defend product

242
00:10:06,399 --> 00:10:11,120
expectations

243
00:10:07,600 --> 00:10:12,399
around data retention we know that the

244
00:10:11,120 --> 00:10:14,880
answer here is

245
00:10:12,399 --> 00:10:16,720
it depends but what does it really

246
00:10:14,880 --> 00:10:20,480
depend on

247
00:10:16,720 --> 00:10:24,399
the underlying purpose

248
00:10:20,480 --> 00:10:27,440
ccpa has exemptions if the purpose is

249
00:10:24,399 --> 00:10:27,440
internal use

250
00:10:27,760 --> 00:10:34,560
such as internal reporting on data

251
00:10:30,959 --> 00:10:36,880
derived from product usage or security

252
00:10:34,560 --> 00:10:40,399
analytics on audit logs

253
00:10:36,880 --> 00:10:40,399
to prevent incidents

254
00:10:41,200 --> 00:10:44,480
the takeaway here is

255
00:10:44,720 --> 00:10:51,920
we have to tie the retention of data

256
00:10:48,240 --> 00:10:51,920
to the underlying business purpose

257
00:10:52,000 --> 00:10:58,839
if the purpose is internal use

258
00:10:55,279 --> 00:11:00,959
and if it's consistent with customer

259
00:10:58,839 --> 00:11:04,720
expectations

260
00:11:00,959 --> 00:11:07,760
then retaining the data is okay

261
00:11:04,720 --> 00:11:13,040
but not being able to meet those legal

262
00:11:07,760 --> 00:11:16,160
exemptions is going to put you at risk

263
00:11:13,040 --> 00:11:17,680
let's go to the second scenario a

264
00:11:16,160 --> 00:11:20,560
company's vendor

265
00:11:17,680 --> 00:11:21,519
launches an email campaign using the

266
00:11:20,560 --> 00:11:25,359
personal data

267
00:11:21,519 --> 00:11:28,560
of the company's users the data belongs

268
00:11:25,360 --> 00:11:31,600
to consumers in california can

269
00:11:28,560 --> 00:11:35,199
a california consumer sue the company

270
00:11:31,600 --> 00:11:35,200
for a ccpa violation

271
00:11:35,360 --> 00:11:38,399
it's a standard security practice to

272
00:11:37,279 --> 00:11:42,000
evaluate vendors

273
00:11:38,399 --> 00:11:43,920
when they are processing personal data

274
00:11:42,000 --> 00:11:45,200
so how can a vendor that passed your

275
00:11:43,920 --> 00:11:49,279
evaluation

276
00:11:45,200 --> 00:11:49,279
end up having legal consequences for you

277
00:11:50,560 --> 00:11:54,000
to answer that question we need to

278
00:11:52,079 --> 00:11:54,959
understand the concept of data sharing

279
00:11:54,000 --> 00:11:57,440
under ccpa

280
00:11:54,959 --> 00:12:00,880
and in particular what it defines as

281
00:11:57,440 --> 00:12:02,720
sale of personal information

282
00:12:00,880 --> 00:12:04,399
data that is shared with the vendor is

283
00:12:02,720 --> 00:12:07,200
considered sale

284
00:12:04,399 --> 00:12:10,959
if it's used for any purpose other than

285
00:12:07,200 --> 00:12:13,760
providing the services and for all data

286
00:12:10,959 --> 00:12:15,199
considered as sale consumers will have a

287
00:12:13,760 --> 00:12:18,560
right to opt out

288
00:12:15,200 --> 00:12:19,600
which you must honor what this means is

289
00:12:18,560 --> 00:12:22,560
this

290
00:12:19,600 --> 00:12:24,959
a non-breaching vendor that has

291
00:12:22,560 --> 00:12:28,079
otherwise passed your security review

292
00:12:24,959 --> 00:12:28,880
can cause a data breach merely by a

293
00:12:28,079 --> 00:12:32,000
change in

294
00:12:28,880 --> 00:12:35,200
consumer preferences

295
00:12:32,000 --> 00:12:37,120
this was not true before ccpa

296
00:12:35,200 --> 00:12:40,560
but it's another reason to pay attention

297
00:12:37,120 --> 00:12:43,440
to data protection risks

298
00:12:40,560 --> 00:12:44,560
let's take a look at an example say you

299
00:12:43,440 --> 00:12:47,519
have two tools

300
00:12:44,560 --> 00:12:49,920
one is a ui tool vendor that helps

301
00:12:47,519 --> 00:12:51,920
pinpoint bugs in your ui

302
00:12:49,920 --> 00:12:54,560
the other is your customer support

303
00:12:51,920 --> 00:12:58,479
portal or ticketing system

304
00:12:54,560 --> 00:13:00,160
which of them do you think accesses pii

305
00:12:58,480 --> 00:13:02,800
you may think that the ui tool vendor

306
00:13:00,160 --> 00:13:03,279
does not but it may still be accessing

307
00:13:02,800 --> 00:13:05,199
pi

308
00:13:03,279 --> 00:13:06,720
in the way it's integrated with your

309
00:13:05,200 --> 00:13:09,839
system

310
00:13:06,720 --> 00:13:13,839
so it should be audited for data use

311
00:13:09,839 --> 00:13:15,839
for example cookie audit

312
00:13:13,839 --> 00:13:17,200
very few vendor security checklists

313
00:13:15,839 --> 00:13:20,639
actually include

314
00:13:17,200 --> 00:13:24,000
a cookie scan but you would need that to

315
00:13:20,639 --> 00:13:27,040
mitigate your data protection risks

316
00:13:24,000 --> 00:13:28,079
as for the vendor with customer pii it

317
00:13:27,040 --> 00:13:31,599
must still

318
00:13:28,079 --> 00:13:34,000
continue to be held accountable

319
00:13:31,600 --> 00:13:34,720
for example if it starts to use pii for

320
00:13:34,000 --> 00:13:37,600
purposes

321
00:13:34,720 --> 00:13:40,639
other than customer support like email

322
00:13:37,600 --> 00:13:43,519
campaigns to promote its own business

323
00:13:40,639 --> 00:13:44,880
that would be a ccpa violation and

324
00:13:43,519 --> 00:13:48,000
considered

325
00:13:44,880 --> 00:13:51,199
unauthorized sale

326
00:13:48,000 --> 00:13:55,199
the takeaway here is that we should tie

327
00:13:51,199 --> 00:13:56,800
pii audit to vendor security review

328
00:13:55,199 --> 00:13:59,359
security teams can help prevent

329
00:13:56,800 --> 00:14:03,599
accidental risk accumulation

330
00:13:59,360 --> 00:14:05,600
by auditing the use of pii by vendors

331
00:14:03,600 --> 00:14:08,240
if the use of pi by the vendor for

332
00:14:05,600 --> 00:14:11,600
non-business purpose does not stop

333
00:14:08,240 --> 00:14:14,160
then the customer in california can sue

334
00:14:11,600 --> 00:14:14,160
the company

335
00:14:15,199 --> 00:14:18,560
now let's take a look at the third and

336
00:14:16,720 --> 00:14:20,959
final scenario

337
00:14:18,560 --> 00:14:22,959
a company accidentally shares a report

338
00:14:20,959 --> 00:14:26,959
belonging to organization a

339
00:14:22,959 --> 00:14:29,439
with organization b the data belongs to

340
00:14:26,959 --> 00:14:32,160
consumers in the eu

341
00:14:29,440 --> 00:14:32,959
is the company required to notify the

342
00:14:32,160 --> 00:14:35,839
consumers

343
00:14:32,959 --> 00:14:35,839
of a data breach

344
00:14:36,800 --> 00:14:40,880
third-party sharing is an example of how

345
00:14:38,800 --> 00:14:41,519
data breaches may result merely from

346
00:14:40,880 --> 00:14:44,800
poor

347
00:14:41,519 --> 00:14:46,800
navigating of data protection risks

348
00:14:44,800 --> 00:14:49,040
that can lead to compromise of consumer

349
00:14:46,800 --> 00:14:50,959
expectations

350
00:14:49,040 --> 00:14:52,800
now unlike the previous two vendor

351
00:14:50,959 --> 00:14:55,359
scenarios

352
00:14:52,800 --> 00:14:56,599
this is a situation where the breach has

353
00:14:55,360 --> 00:14:59,600
already

354
00:14:56,600 --> 00:15:02,720
happened so really the opportunity to

355
00:14:59,600 --> 00:15:04,959
avoid the risk has passed

356
00:15:02,720 --> 00:15:07,839
however accidental sharing is a reality

357
00:15:04,959 --> 00:15:11,040
and can happen to anyone

358
00:15:07,839 --> 00:15:15,440
what can the security team do now

359
00:15:11,040 --> 00:15:18,880
well we can help prevent a bad situation

360
00:15:15,440 --> 00:15:20,639
from getting worse let's look at a

361
00:15:18,880 --> 00:15:24,079
really famous example

362
00:15:20,639 --> 00:15:24,079
of how one person did that

363
00:15:24,480 --> 00:15:28,079
this was captain sully who landed flight

364
00:15:26,600 --> 00:15:32,160
1549

365
00:15:28,079 --> 00:15:33,439
on the hudson now this is an example of

366
00:15:32,160 --> 00:15:37,279
finding clarity

367
00:15:33,440 --> 00:15:38,959
in uncertainty sully made a decision

368
00:15:37,279 --> 00:15:42,240
based on risk profile

369
00:15:38,959 --> 00:15:45,599
for flight 1549

370
00:15:42,240 --> 00:15:47,199
is the speed too high is the altitude

371
00:15:45,600 --> 00:15:50,320
too low

372
00:15:47,199 --> 00:15:52,479
and how much time there is to decide

373
00:15:50,320 --> 00:15:54,399
if he did not have the clarity of what

374
00:15:52,480 --> 00:15:56,000
the risks depend on

375
00:15:54,399 --> 00:15:59,120
then we can all imagine why that

376
00:15:56,000 --> 00:15:59,120
wouldn't have gone so well

377
00:15:59,279 --> 00:16:03,199
so coming back to the breach scenario

378
00:16:01,279 --> 00:16:04,639
typically you also have limited time to

379
00:16:03,199 --> 00:16:07,599
decide

380
00:16:04,639 --> 00:16:10,079
and how would you decide if you require

381
00:16:07,600 --> 00:16:11,759
a notification or not

382
00:16:10,079 --> 00:16:15,439
once again it's going to be based on

383
00:16:11,759 --> 00:16:15,440
your unique risk profile

384
00:16:15,680 --> 00:16:20,959
the right answers can make a difference

385
00:16:17,680 --> 00:16:24,319
between a crash and water landing

386
00:16:20,959 --> 00:16:26,000
was the data exposed included personal

387
00:16:24,320 --> 00:16:29,040
data

388
00:16:26,000 --> 00:16:31,279
was it encrypted has the source of

389
00:16:29,040 --> 00:16:32,639
breach been addressed

390
00:16:31,279 --> 00:16:34,320
and by the way the outcome may be

391
00:16:32,639 --> 00:16:37,839
different based on

392
00:16:34,320 --> 00:16:40,480
if it's california or the eu

393
00:16:37,839 --> 00:16:41,759
in california under the ccpa if none of

394
00:16:40,480 --> 00:16:44,880
this was true

395
00:16:41,759 --> 00:16:47,600
then no notification is required under

396
00:16:44,880 --> 00:16:48,959
gdpr however you also have to consider

397
00:16:47,600 --> 00:16:52,320
if the processing was

398
00:16:48,959 --> 00:16:55,758
high risk and what

399
00:16:52,320 --> 00:16:56,880
is high risk processing say you have two

400
00:16:55,759 --> 00:16:58,720
applications

401
00:16:56,880 --> 00:17:00,800
one is used to share reports with your

402
00:16:58,720 --> 00:17:05,760
users about their utility bills

403
00:17:00,800 --> 00:17:07,438
the other about their clinical tests

404
00:17:05,760 --> 00:17:09,039
and say you have a filter that is used

405
00:17:07,439 --> 00:17:09,760
to select the recipients of these

406
00:17:09,039 --> 00:17:12,480
reports

407
00:17:09,760 --> 00:17:13,919
based on the zip code and you enter the

408
00:17:12,480 --> 00:17:15,679
zip code incorrectly

409
00:17:13,919 --> 00:17:17,679
so the reports go to unintended

410
00:17:15,679 --> 00:17:19,839
recipients

411
00:17:17,679 --> 00:17:20,799
in one case the billing info gets leaked

412
00:17:19,839 --> 00:17:24,159
and in the other

413
00:17:20,799 --> 00:17:24,720
the test preserves processing of billing

414
00:17:24,160 --> 00:17:27,360
data

415
00:17:24,720 --> 00:17:28,559
even though personal is unlikely to be

416
00:17:27,359 --> 00:17:30,719
high risk

417
00:17:28,559 --> 00:17:32,399
why because it's used for providing a

418
00:17:30,720 --> 00:17:34,799
public service

419
00:17:32,400 --> 00:17:36,480
not so much for clinical tests the

420
00:17:34,799 --> 00:17:38,960
expectation of the patient

421
00:17:36,480 --> 00:17:40,799
would be for this data to remain private

422
00:17:38,960 --> 00:17:43,120
so the processing of health data

423
00:17:40,799 --> 00:17:44,160
is considered high risk therefore under

424
00:17:43,120 --> 00:17:46,399
gdpr

425
00:17:44,160 --> 00:17:47,840
this type of accidental sharing would

426
00:17:46,400 --> 00:17:51,280
require notification

427
00:17:47,840 --> 00:17:51,280
to the eu consumer

428
00:17:51,440 --> 00:17:54,799
so our final takeaway is that focusing

429
00:17:53,280 --> 00:17:56,879
on data processing risks

430
00:17:54,799 --> 00:17:58,320
helps guide breach mitigation and

431
00:17:56,880 --> 00:18:00,799
response

432
00:17:58,320 --> 00:18:02,320
tying the risk of disclosure back to the

433
00:18:00,799 --> 00:18:04,400
business consequences

434
00:18:02,320 --> 00:18:07,360
should drive prioritization of the

435
00:18:04,400 --> 00:18:07,360
security agenda

436
00:18:07,679 --> 00:18:10,880
but also keep in mind the analysis

437
00:18:09,360 --> 00:18:15,840
changes in california

438
00:18:10,880 --> 00:18:15,840
and both cases will be equal priority

439
00:18:16,640 --> 00:18:21,919
it's worth noting that if you understand

440
00:18:18,320 --> 00:18:23,520
the risks you will reap the rewards

441
00:18:21,919 --> 00:18:27,520
when you understand the risks you can

442
00:18:23,520 --> 00:18:27,520
translate them into business needs

443
00:18:27,600 --> 00:18:30,959
when you say this helps me avoid

444
00:18:29,440 --> 00:18:33,200
business consequences

445
00:18:30,960 --> 00:18:34,240
so this is high priority it's more

446
00:18:33,200 --> 00:18:36,799
effective than

447
00:18:34,240 --> 00:18:37,440
simply saying it's a risk without tying

448
00:18:36,799 --> 00:18:41,120
it back to

449
00:18:37,440 --> 00:18:44,559
business why is that important

450
00:18:41,120 --> 00:18:47,600
because needs get buy-in

451
00:18:44,559 --> 00:18:50,399
risks get a workaround they will ask you

452
00:18:47,600 --> 00:18:52,559
to find another way

453
00:18:50,400 --> 00:18:54,559
security engineering teams should also

454
00:18:52,559 --> 00:18:56,879
harness the power of awareness around

455
00:18:54,559 --> 00:18:58,080
data protection risks to create security

456
00:18:56,880 --> 00:19:00,640
culture

457
00:18:58,080 --> 00:19:02,320
gdpr ccpa is good guidance that is

458
00:19:00,640 --> 00:19:04,320
becoming law

459
00:19:02,320 --> 00:19:06,320
and we can use the momentum to our

460
00:19:04,320 --> 00:19:08,559
advantage

461
00:19:06,320 --> 00:19:10,080
lastly more and more security teams are

462
00:19:08,559 --> 00:19:12,000
now recognizing how

463
00:19:10,080 --> 00:19:13,760
data protection risks are tied to the

464
00:19:12,000 --> 00:19:15,600
security agenda

465
00:19:13,760 --> 00:19:18,000
privacy engineering roles are on the

466
00:19:15,600 --> 00:19:19,918
rise i've seen a lot of companies put

467
00:19:18,000 --> 00:19:22,240
out directs for them

468
00:19:19,919 --> 00:19:23,520
take a look at your unique profile and

469
00:19:22,240 --> 00:19:25,919
make a business case

470
00:19:23,520 --> 00:19:27,440
if you need to hire one again

471
00:19:25,919 --> 00:19:30,480
demonstrating the need

472
00:19:27,440 --> 00:19:32,880
would help you get the buy-in

473
00:19:30,480 --> 00:19:34,559
so yeah there you have it we've all been

474
00:19:32,880 --> 00:19:35,600
hit by the birds at one point or the

475
00:19:34,559 --> 00:19:37,200
other

476
00:19:35,600 --> 00:19:38,879
hopefully you are taking something away

477
00:19:37,200 --> 00:19:40,960
from the stock that helps you avoid a

478
00:19:38,880 --> 00:19:42,240
bad situation or prevent it from getting

479
00:19:40,960 --> 00:19:45,280
worse

480
00:19:42,240 --> 00:19:48,080
i'd leave you with this anecdote after

481
00:19:45,280 --> 00:19:49,360
sully landed flight 1549 on water

482
00:19:48,080 --> 00:19:51,120
he got in touch with the airline

483
00:19:49,360 --> 00:19:53,600
operations manager

484
00:19:51,120 --> 00:19:54,639
that person however cuts sully off

485
00:19:53,600 --> 00:19:57,199
saying

486
00:19:54,640 --> 00:19:58,720
i cannot talk we have a plane down in

487
00:19:57,200 --> 00:20:03,280
the hudson

488
00:19:58,720 --> 00:20:05,280
to which sully said i know i'm the guy

489
00:20:03,280 --> 00:20:06,480
so here's hoping when it's our time to

490
00:20:05,280 --> 00:20:09,678
land we are

491
00:20:06,480 --> 00:20:12,159
on the hudson not in it

492
00:20:09,679 --> 00:20:13,919
with this i conclude my presentation

493
00:20:12,159 --> 00:20:15,440
here's my contact info if you'd like to

494
00:20:13,919 --> 00:20:21,840
get in touch

495
00:20:15,440 --> 00:20:21,840
thank you all

496
00:20:25,760 --> 00:20:27,840
you

