1
00:00:08,880 --> 00:00:13,679
hi my name is guillaume sanjak

2
00:00:11,040 --> 00:00:15,280
or william in english and i lead the

3
00:00:13,679 --> 00:00:16,240
computational social science team at

4
00:00:15,280 --> 00:00:18,080
linkedin

5
00:00:16,239 --> 00:00:19,680
uh and i'm very happy to be uh joining

6
00:00:18,080 --> 00:00:22,240
this conference today um

7
00:00:19,680 --> 00:00:23,439
to talk a little bit about uh the way we

8
00:00:22,240 --> 00:00:25,038
try at linkedin to build a more

9
00:00:23,439 --> 00:00:27,920
inclusive platform

10
00:00:25,039 --> 00:00:29,519
um by taking a place by taking a

11
00:00:27,920 --> 00:00:31,039
specific approach to looking at our

12
00:00:29,519 --> 00:00:33,040
other experiments

13
00:00:31,039 --> 00:00:34,079
uh and so first of all i want to mention

14
00:00:33,040 --> 00:00:36,239
that uh this is

15
00:00:34,079 --> 00:00:37,760
a uh that there is a paper online if

16
00:00:36,239 --> 00:00:39,919
you'd like to take a look at it

17
00:00:37,760 --> 00:00:41,760
so i'm leaving the the archive link and

18
00:00:39,920 --> 00:00:42,559
so you know i'm very happy to answer any

19
00:00:41,760 --> 00:00:44,160
questions

20
00:00:42,559 --> 00:00:45,599
uh and if not there are plenty of

21
00:00:44,160 --> 00:00:46,879
details as well that are that are

22
00:00:45,600 --> 00:00:49,600
available online

23
00:00:46,879 --> 00:00:50,239
um so okay so to get us started uh you

24
00:00:49,600 --> 00:00:52,480
know the

25
00:00:50,239 --> 00:00:54,399
vision of linkedin as a company is

26
00:00:52,480 --> 00:00:56,800
essentially a very inclusive vision the

27
00:00:54,399 --> 00:00:58,559
vision is to create economic opportunity

28
00:00:56,800 --> 00:01:00,078
for every member of the global workforce

29
00:00:58,559 --> 00:01:00,800
and so i i've taken the liberty of

30
00:01:00,079 --> 00:01:02,960
adding

31
00:01:00,800 --> 00:01:04,798
the emphasis on every member but it is

32
00:01:02,960 --> 00:01:05,199
really something kind of fundamental and

33
00:01:04,799 --> 00:01:08,560
core

34
00:01:05,199 --> 00:01:10,240
to to to our vision um and so that

35
00:01:08,560 --> 00:01:10,720
includes right members who are not born

36
00:01:10,240 --> 00:01:12,479
with

37
00:01:10,720 --> 00:01:14,080
a great social network members who don't

38
00:01:12,479 --> 00:01:15,119
have a traditional education

39
00:01:14,080 --> 00:01:16,399
includes really members of all

40
00:01:15,119 --> 00:01:18,560
backgrounds and that also includes

41
00:01:16,400 --> 00:01:20,080
members who are not already power users

42
00:01:18,560 --> 00:01:21,280
right we don't want to just optimize for

43
00:01:20,080 --> 00:01:22,479
a small group of people

44
00:01:21,280 --> 00:01:24,880
we want to build a platform that

45
00:01:22,479 --> 00:01:25,520
benefits everybody and when we benefit

46
00:01:24,880 --> 00:01:27,119
everybody

47
00:01:25,520 --> 00:01:28,880
we mean that ideally to people with

48
00:01:27,119 --> 00:01:29,840
equal talent should have equal access to

49
00:01:28,880 --> 00:01:31,280
opportunities

50
00:01:29,840 --> 00:01:32,799
uh and you know there are plenty of

51
00:01:31,280 --> 00:01:34,320
opportunities at linkedin and these will

52
00:01:32,799 --> 00:01:36,240
translate in terms of in terms of the

53
00:01:34,320 --> 00:01:38,720
metrics we actually look at

54
00:01:36,240 --> 00:01:39,600
as we assess the inclusiveness of our

55
00:01:38,720 --> 00:01:41,840
experiments

56
00:01:39,600 --> 00:01:42,798
but on linkedin you have opportunities

57
00:01:41,840 --> 00:01:45,280
to build

58
00:01:42,799 --> 00:01:47,280
a network to find a job to engage with

59
00:01:45,280 --> 00:01:48,399
content to get novel information to be

60
00:01:47,280 --> 00:01:49,840
referred to a job

61
00:01:48,399 --> 00:01:52,240
we also have linkedin learning where you

62
00:01:49,840 --> 00:01:53,840
can learn and also to get endorsements

63
00:01:52,240 --> 00:01:55,360
on your skills and so all of these

64
00:01:53,840 --> 00:01:56,479
things but all these opportunities that

65
00:01:55,360 --> 00:01:58,240
translate to

66
00:01:56,479 --> 00:01:59,759
meaningful professional opportunities

67
00:01:58,240 --> 00:02:00,880
for our members that potentially have an

68
00:01:59,759 --> 00:02:03,360
impact on their lives

69
00:02:00,880 --> 00:02:06,320
we want to make sure that those um are

70
00:02:03,360 --> 00:02:08,000
distributed as equally as possible

71
00:02:06,320 --> 00:02:09,280
um and of course it's a very uh

72
00:02:08,000 --> 00:02:11,038
challenging problem that technology

73
00:02:09,280 --> 00:02:13,040
cannot cannot solve alone

74
00:02:11,038 --> 00:02:14,879
and we're making all sorts of r d

75
00:02:13,040 --> 00:02:16,640
investments uh in that space and so what

76
00:02:14,879 --> 00:02:18,480
i'm going to be presenting here today

77
00:02:16,640 --> 00:02:20,000
is primarily one aspect which has to do

78
00:02:18,480 --> 00:02:21,679
with experimentation

79
00:02:20,000 --> 00:02:23,360
and inequality and i'll define

80
00:02:21,680 --> 00:02:24,640
inequality where we're really looking at

81
00:02:23,360 --> 00:02:27,200
we're really looking at inequality we're

82
00:02:24,640 --> 00:02:28,799
an economist we look at inequality here

83
00:02:27,200 --> 00:02:30,238
and so here's the very kind of high

84
00:02:28,800 --> 00:02:31,519
level structure of what i want to talk

85
00:02:30,239 --> 00:02:33,440
about today

86
00:02:31,519 --> 00:02:35,840
first i'll talk a little bit about why a

87
00:02:33,440 --> 00:02:37,280
b testing is in our view important uh

88
00:02:35,840 --> 00:02:38,239
when we're trying to build an inclusive

89
00:02:37,280 --> 00:02:39,840
platform

90
00:02:38,239 --> 00:02:41,840
i'll go a little bit over what we're

91
00:02:39,840 --> 00:02:43,680
measuring and why

92
00:02:41,840 --> 00:02:46,160
um and i'll talk about our process to

93
00:02:43,680 --> 00:02:47,120
inform decisions um as we see the a b

94
00:02:46,160 --> 00:02:48,959
test results and

95
00:02:47,120 --> 00:02:50,640
the metrics when it comes to inclusion

96
00:02:48,959 --> 00:02:51,680
and finally and

97
00:02:50,640 --> 00:02:53,440
perhaps the thing that i'm looking

98
00:02:51,680 --> 00:02:55,040
forward to the most is to give you a

99
00:02:53,440 --> 00:02:57,120
high level

100
00:02:55,040 --> 00:02:58,959
overview of the things we've learned

101
00:02:57,120 --> 00:03:00,400
reviewing thousands of experiments so

102
00:02:58,959 --> 00:03:01,360
the things that just seem to reduce

103
00:03:00,400 --> 00:03:02,800
inequality

104
00:03:01,360 --> 00:03:05,200
and the theme the things that seem to

105
00:03:02,800 --> 00:03:06,319
increase inequality from reviewing many

106
00:03:05,200 --> 00:03:08,640
many experiments

107
00:03:06,319 --> 00:03:09,920
um over i think at this point almost a

108
00:03:08,640 --> 00:03:12,079
two year period

109
00:03:09,920 --> 00:03:13,440
um and so why a b testing right and so

110
00:03:12,080 --> 00:03:15,599
the first thing is like well you know

111
00:03:13,440 --> 00:03:17,359
unfairness a very very simple very very

112
00:03:15,599 --> 00:03:18,799
broad definition

113
00:03:17,360 --> 00:03:20,640
is something like having unequal

114
00:03:18,800 --> 00:03:22,159
outcomes or any call treatment without a

115
00:03:20,640 --> 00:03:23,518
mechanism that is deemed

116
00:03:22,159 --> 00:03:25,040
just and so you know this has been

117
00:03:23,519 --> 00:03:27,040
documented in children this has been

118
00:03:25,040 --> 00:03:29,040
documented even in in

119
00:03:27,040 --> 00:03:30,560
other primates that if you give them if

120
00:03:29,040 --> 00:03:32,000
they behave the same way and you give

121
00:03:30,560 --> 00:03:33,360
them two things that are different

122
00:03:32,000 --> 00:03:35,280
you'll get a reaction so there's this

123
00:03:33,360 --> 00:03:36,239
seems to be this very very kind of

124
00:03:35,280 --> 00:03:37,760
innate reaction

125
00:03:36,239 --> 00:03:39,280
to being treated differently unless

126
00:03:37,760 --> 00:03:41,120
there's a really good reason

127
00:03:39,280 --> 00:03:42,879
and so you know imagine we see a large

128
00:03:41,120 --> 00:03:43,360
gap between two groups we see like you

129
00:03:42,879 --> 00:03:45,120
know some

130
00:03:43,360 --> 00:03:47,040
you know some groups some people there's

131
00:03:45,120 --> 00:03:49,280
a group of really highly engaged power

132
00:03:47,040 --> 00:03:50,879
users and a group of people left behind

133
00:03:49,280 --> 00:03:52,480
the issue is that you know linkedin

134
00:03:50,879 --> 00:03:53,840
users are constantly interacting with a

135
00:03:52,480 --> 00:03:55,200
very large number of features

136
00:03:53,840 --> 00:03:56,959
all the time and so it's very hard to

137
00:03:55,200 --> 00:03:58,480
figure out well why is there a gap

138
00:03:56,959 --> 00:04:00,400
between these between these people why

139
00:03:58,480 --> 00:04:01,840
is there inequality there

140
00:04:00,400 --> 00:04:03,439
and so to fix the gap we need to

141
00:04:01,840 --> 00:04:06,159
identify its origin and this is where a

142
00:04:03,439 --> 00:04:07,920
b testing helps because if you a b test

143
00:04:06,159 --> 00:04:09,200
every single new feature looking at the

144
00:04:07,920 --> 00:04:11,359
impact it has on

145
00:04:09,200 --> 00:04:12,480
inequality and opportunity in the

146
00:04:11,360 --> 00:04:13,920
opportunity distribution

147
00:04:12,480 --> 00:04:15,518
then you can go back and say well it

148
00:04:13,920 --> 00:04:17,279
seems like the inequality the

149
00:04:15,519 --> 00:04:18,959
concentration of opportunities

150
00:04:17,279 --> 00:04:20,478
seemed to have started around that time

151
00:04:18,959 --> 00:04:22,160
around that experiment

152
00:04:20,478 --> 00:04:23,758
and so in a sense what we'll do is for

153
00:04:22,160 --> 00:04:25,840
each new feature

154
00:04:23,759 --> 00:04:27,280
or each change in features we'll ask

155
00:04:25,840 --> 00:04:28,960
okay is this change going to make

156
00:04:27,280 --> 00:04:32,719
outcomes more equal

157
00:04:28,960 --> 00:04:34,239
or more unequal between our members

158
00:04:32,720 --> 00:04:35,759
the other reason it really matters we

159
00:04:34,240 --> 00:04:38,720
think is because

160
00:04:35,759 --> 00:04:40,720
you know if you see a gap between you

161
00:04:38,720 --> 00:04:42,240
know two groups of people or between

162
00:04:40,720 --> 00:04:44,000
highly engaged members and not high if

163
00:04:42,240 --> 00:04:45,840
you see any sort of inequality

164
00:04:44,000 --> 00:04:47,040
say very high genie coefficient or

165
00:04:45,840 --> 00:04:48,560
something like that right

166
00:04:47,040 --> 00:04:50,720
yes it can come from some sort of a

167
00:04:48,560 --> 00:04:52,479
biased algorithm but it can also come

168
00:04:50,720 --> 00:04:54,400
from the fact that even though you treat

169
00:04:52,479 --> 00:04:57,360
people fairly and completely

170
00:04:54,400 --> 00:04:58,080
the same way uh somehow they're reacting

171
00:04:57,360 --> 00:04:59,919
differently

172
00:04:58,080 --> 00:05:01,359
to a treatment that is otherwise

173
00:04:59,919 --> 00:05:02,240
unbiased and in fact we've seen this

174
00:05:01,360 --> 00:05:04,720
happen

175
00:05:02,240 --> 00:05:06,160
with things as simple as copy copy

176
00:05:04,720 --> 00:05:07,759
changes we just change the wording of

177
00:05:06,160 --> 00:05:09,520
something and we see that some members

178
00:05:07,759 --> 00:05:11,759
react very very differently than others

179
00:05:09,520 --> 00:05:13,758
and so the point of a b testing is to

180
00:05:11,759 --> 00:05:15,680
try and catch the overall effect

181
00:05:13,759 --> 00:05:17,199
of introducing a new feature not just

182
00:05:15,680 --> 00:05:18,960
kind of to look at the process of the

183
00:05:17,199 --> 00:05:20,720
feature itself and what the feature does

184
00:05:18,960 --> 00:05:22,159
but also how our member interacts with

185
00:05:20,720 --> 00:05:23,919
it because at the end of the day

186
00:05:22,160 --> 00:05:25,520
if there's any feature that changes

187
00:05:23,919 --> 00:05:27,198
meaningfully the distribution of

188
00:05:25,520 --> 00:05:27,919
professional opportunities on our

189
00:05:27,199 --> 00:05:30,479
platform

190
00:05:27,919 --> 00:05:31,280
well that's something we want to know uh

191
00:05:30,479 --> 00:05:33,359
and so

192
00:05:31,280 --> 00:05:35,039
what are we measuring well you know

193
00:05:33,360 --> 00:05:36,639
essentially for a b testing

194
00:05:35,039 --> 00:05:38,320
we're using two complementary approaches

195
00:05:36,639 --> 00:05:39,440
the first one is is the one i'm actually

196
00:05:38,320 --> 00:05:40,800
i'm not going to talk about very much

197
00:05:39,440 --> 00:05:43,759
today but it's the simplest

198
00:05:40,800 --> 00:05:44,639
which is essentially a fully group based

199
00:05:43,759 --> 00:05:46,080
approach

200
00:05:44,639 --> 00:05:47,680
uh where essentially we look at the

201
00:05:46,080 --> 00:05:48,719
impact of a new experiment on two

202
00:05:47,680 --> 00:05:50,240
different populations

203
00:05:48,720 --> 00:05:51,759
so for example we can say hey this

204
00:05:50,240 --> 00:05:53,039
feature increases engagement by five

205
00:05:51,759 --> 00:05:55,039
percent for group a

206
00:05:53,039 --> 00:05:56,800
and by three percent for group b there's

207
00:05:55,039 --> 00:05:58,159
a difference there um you know would you

208
00:05:56,800 --> 00:05:59,600
like to look at this some more

209
00:05:58,160 --> 00:06:01,680
um and so for now we're actually looking

210
00:05:59,600 --> 00:06:02,800
at um when for every single experiment

211
00:06:01,680 --> 00:06:04,319
we actually assess

212
00:06:02,800 --> 00:06:05,600
members with you know a very good

213
00:06:04,319 --> 00:06:06,000
network which we call high social

214
00:06:05,600 --> 00:06:07,440
capital

215
00:06:06,000 --> 00:06:09,520
and members with a network that still

216
00:06:07,440 --> 00:06:11,199
has to be grown and diversified which we

217
00:06:09,520 --> 00:06:12,639
call low social capital members

218
00:06:11,199 --> 00:06:13,759
but we also look at something else and

219
00:06:12,639 --> 00:06:14,800
this is probably the thing i want to

220
00:06:13,759 --> 00:06:16,720
focus on today

221
00:06:14,800 --> 00:06:18,080
which is what we call inequality impact

222
00:06:16,720 --> 00:06:19,039
and that's a that's a kind of more

223
00:06:18,080 --> 00:06:20,719
general approach

224
00:06:19,039 --> 00:06:22,080
which looks at inequality the same way

225
00:06:20,720 --> 00:06:24,160
economists would look at income

226
00:06:22,080 --> 00:06:26,960
inequality so if you've you know

227
00:06:24,160 --> 00:06:28,160
read to the press or you know heard some

228
00:06:26,960 --> 00:06:30,000
debates recently

229
00:06:28,160 --> 00:06:31,840
there's a lot of campus emphasis on

230
00:06:30,000 --> 00:06:33,759
inequality in the us

231
00:06:31,840 --> 00:06:34,960
and and you know i'm not making any

232
00:06:33,759 --> 00:06:35,360
statements about that but but you know

233
00:06:34,960 --> 00:06:36,960
the

234
00:06:35,360 --> 00:06:38,560
kind of metric that's used there it's

235
00:06:36,960 --> 00:06:39,440
something about the distribution itself

236
00:06:38,560 --> 00:06:41,520
which is say

237
00:06:39,440 --> 00:06:42,960
their share of the top one percent right

238
00:06:41,520 --> 00:06:44,080
and and the fact that maybe the rich are

239
00:06:42,960 --> 00:06:44,400
getting richer and so the question is

240
00:06:44,080 --> 00:06:45,919
like

241
00:06:44,400 --> 00:06:47,440
is it the same on linkedin are the rich

242
00:06:45,919 --> 00:06:48,880
getting richer are people very engaged

243
00:06:47,440 --> 00:06:50,400
getting even more engaged and people

244
00:06:48,880 --> 00:06:51,360
getting lots of job offers getting even

245
00:06:50,400 --> 00:06:53,840
more job offers

246
00:06:51,360 --> 00:06:54,639
and if so can that be attributed to our

247
00:06:53,840 --> 00:06:56,000
experiments

248
00:06:54,639 --> 00:06:57,680
and so an example insight for this is

249
00:06:56,000 --> 00:06:59,039
like hey you know your future increases

250
00:06:57,680 --> 00:07:00,080
inequality and engagement and

251
00:06:59,039 --> 00:07:01,840
contributions

252
00:07:00,080 --> 00:07:03,680
and so the gap between high engagement

253
00:07:01,840 --> 00:07:05,919
and low engagement members is widening

254
00:07:03,680 --> 00:07:07,599
because of these features are you okay

255
00:07:05,919 --> 00:07:09,758
with that

256
00:07:07,599 --> 00:07:11,039
um and so how do we measure inequality

257
00:07:09,759 --> 00:07:12,720
impact and so again this is this is

258
00:07:11,039 --> 00:07:15,440
really the way economists

259
00:07:12,720 --> 00:07:16,960
look at uh look at inequality um and so

260
00:07:15,440 --> 00:07:18,719
imagine for example linkedin has ten

261
00:07:16,960 --> 00:07:20,080
members and each of them has one useful

262
00:07:18,720 --> 00:07:20,800
conversation a day and useful

263
00:07:20,080 --> 00:07:22,479
conversation

264
00:07:20,800 --> 00:07:24,479
let's call that our opportunity metric

265
00:07:22,479 --> 00:07:25,919
it is that thing that that measures

266
00:07:24,479 --> 00:07:27,599
you know something meaningful for a

267
00:07:25,919 --> 00:07:30,080
member's career um

268
00:07:27,599 --> 00:07:30,800
and so if the experimentation platform

269
00:07:30,080 --> 00:07:32,880
tells you

270
00:07:30,800 --> 00:07:34,000
hey we have this new feature and it

271
00:07:32,880 --> 00:07:36,479
increases these

272
00:07:34,000 --> 00:07:38,319
uh useful conversations by one per

273
00:07:36,479 --> 00:07:40,960
member per day on average

274
00:07:38,319 --> 00:07:42,400
so in other words you know everyone here

275
00:07:40,960 --> 00:07:44,318
every every person

276
00:07:42,400 --> 00:07:46,400
is a blue bar here and so what you

277
00:07:44,319 --> 00:07:47,599
imagine happens is that everyone has one

278
00:07:46,400 --> 00:07:49,520
conversation day

279
00:07:47,599 --> 00:07:51,360
and then the next day after you know in

280
00:07:49,520 --> 00:07:52,400
after you a b test or you know in your

281
00:07:51,360 --> 00:07:53,759
treatment group

282
00:07:52,400 --> 00:07:55,520
um everyone now has two useful

283
00:07:53,759 --> 00:07:57,039
conversation in the day right and this

284
00:07:55,520 --> 00:07:58,560
is usually what what people tend to

285
00:07:57,039 --> 00:07:59,520
assume when they look at an a b testing

286
00:07:58,560 --> 00:08:01,840
dashboard

287
00:07:59,520 --> 00:08:03,440
and the problem is what could be hiding

288
00:08:01,840 --> 00:08:04,878
under that that statistic that i just

289
00:08:03,440 --> 00:08:06,319
given you that average number

290
00:08:04,879 --> 00:08:07,919
is something like this and in fact we

291
00:08:06,319 --> 00:08:10,400
see this happen fairly often

292
00:08:07,919 --> 00:08:11,280
um where essentially some members might

293
00:08:10,400 --> 00:08:14,000
actually get

294
00:08:11,280 --> 00:08:15,359
fewer conversations a day very useful

295
00:08:14,000 --> 00:08:16,080
conversation and some members might

296
00:08:15,360 --> 00:08:17,759
benefit from

297
00:08:16,080 --> 00:08:19,919
from the feature tremendously so of

298
00:08:17,759 --> 00:08:22,000
course this is a highly stylized example

299
00:08:19,919 --> 00:08:23,919
but the way i've set this up is that

300
00:08:22,000 --> 00:08:25,759
both of these scenarios

301
00:08:23,919 --> 00:08:27,440
they look exactly the same on average

302
00:08:25,759 --> 00:08:28,160
it's still a doubling of the total

303
00:08:27,440 --> 00:08:30,160
number of

304
00:08:28,160 --> 00:08:31,759
of useful conversations however of

305
00:08:30,160 --> 00:08:33,519
course right in terms of building an

306
00:08:31,759 --> 00:08:34,479
inclusive platform a platform that

307
00:08:33,519 --> 00:08:35,760
serves everybody

308
00:08:34,479 --> 00:08:37,599
clearly these two things need to be

309
00:08:35,760 --> 00:08:38,640
distinguished and so this is essentially

310
00:08:37,599 --> 00:08:40,320
what we're going after

311
00:08:38,640 --> 00:08:42,240
as we're measuring inequality and so

312
00:08:40,320 --> 00:08:43,200
that is kind of the the high level

313
00:08:42,240 --> 00:08:45,279
intuition

314
00:08:43,200 --> 00:08:46,959
um and the way we do this is essentially

315
00:08:45,279 --> 00:08:48,800
using the atkinson index

316
00:08:46,959 --> 00:08:50,800
which is which is very very common in

317
00:08:48,800 --> 00:08:52,319
economics to use to compare the income

318
00:08:50,800 --> 00:08:54,479
distribution between countries to

319
00:08:52,320 --> 00:08:56,399
compare wealth distributions

320
00:08:54,480 --> 00:08:58,560
and you know i've put the formula here

321
00:08:56,399 --> 00:08:59,200
um it's applicable to any metric that's

322
00:08:58,560 --> 00:09:01,439
you know not

323
00:08:59,200 --> 00:09:03,440
a binary metric so you know sessions

324
00:09:01,440 --> 00:09:04,080
contributions job applications what have

325
00:09:03,440 --> 00:09:06,080
you

326
00:09:04,080 --> 00:09:08,000
and it's really great at measuring

327
00:09:06,080 --> 00:09:10,399
whether opportunities are getting more

328
00:09:08,000 --> 00:09:11,920
or less concentrated and so an example

329
00:09:10,399 --> 00:09:13,680
inside here would be hey your feature

330
00:09:11,920 --> 00:09:16,000
has a two percent lift on engagement

331
00:09:13,680 --> 00:09:16,800
but it's increasing the concentration of

332
00:09:16,000 --> 00:09:18,880
engagement

333
00:09:16,800 --> 00:09:20,000
uh would you like us to do a deep dive

334
00:09:18,880 --> 00:09:21,360
and i'll talk a little bit

335
00:09:20,000 --> 00:09:23,120
you know how we do these deep dives and

336
00:09:21,360 --> 00:09:24,080
how what the decision process looks like

337
00:09:23,120 --> 00:09:27,200
after that

338
00:09:24,080 --> 00:09:29,440
um so

339
00:09:27,200 --> 00:09:30,880
you know and then the one of the ways

340
00:09:29,440 --> 00:09:32,720
you can interpret this result one of the

341
00:09:30,880 --> 00:09:33,040
nice properties of the atkinson index is

342
00:09:32,720 --> 00:09:34,399
that

343
00:09:33,040 --> 00:09:36,480
with some modeling you can actually

344
00:09:34,399 --> 00:09:37,839
think of it as a discount factor

345
00:09:36,480 --> 00:09:39,279
on the total of the metric in other

346
00:09:37,839 --> 00:09:41,760
words like you know you have some

347
00:09:39,279 --> 00:09:44,080
utility say as a decision maker

348
00:09:41,760 --> 00:09:45,120
for you know increasing your sessions by

349
00:09:44,080 --> 00:09:48,080
a certain percentage

350
00:09:45,120 --> 00:09:50,000
but there's a discount on that utility

351
00:09:48,080 --> 00:09:51,360
because that increase is not equally

352
00:09:50,000 --> 00:09:53,200
distributed and this is how you know

353
00:09:51,360 --> 00:09:54,880
economists typically model

354
00:09:53,200 --> 00:09:57,120
the way a social planner might decide to

355
00:09:54,880 --> 00:09:58,800
do redistribution or might feel about a

356
00:09:57,120 --> 00:10:00,160
specific distribution of income

357
00:09:58,800 --> 00:10:01,519
and what's nice about this is we can

358
00:10:00,160 --> 00:10:02,399
actually use choice experiments to

359
00:10:01,519 --> 00:10:04,160
calibrate this

360
00:10:02,399 --> 00:10:05,680
to get a sense of what what the absolute

361
00:10:04,160 --> 00:10:08,719
should be um

362
00:10:05,680 --> 00:10:11,040
so the reason why i mean there's many

363
00:10:08,720 --> 00:10:12,320
reasons why inequality impacts defined

364
00:10:11,040 --> 00:10:14,800
like this right they find as this

365
00:10:12,320 --> 00:10:16,720
economic uh inequality in the sense of

366
00:10:14,800 --> 00:10:18,240
just purely distributional

367
00:10:16,720 --> 00:10:20,000
there's the reasons there's many reasons

368
00:10:18,240 --> 00:10:22,880
why it can be uh useful

369
00:10:20,000 --> 00:10:24,560
um first of all we can catch feature um

370
00:10:22,880 --> 00:10:26,720
that create unintended gaps

371
00:10:24,560 --> 00:10:28,959
or that close gaps between members so we

372
00:10:26,720 --> 00:10:30,800
can catch things that increase or reduce

373
00:10:28,959 --> 00:10:32,160
inequality of the distribution so it can

374
00:10:30,800 --> 00:10:33,439
serve as a great it can actually serve

375
00:10:32,160 --> 00:10:35,199
as a great guard rail

376
00:10:33,440 --> 00:10:36,959
because the alternative is you have to

377
00:10:35,200 --> 00:10:38,399
specify 20 groups that you care about or

378
00:10:36,959 --> 00:10:40,959
100 groups that you care about

379
00:10:38,399 --> 00:10:42,399
and make sure that you know these things

380
00:10:40,959 --> 00:10:43,760
that there's no inequality between these

381
00:10:42,399 --> 00:10:45,279
groups and if there's a group

382
00:10:43,760 --> 00:10:46,959
you don't have data for or you don't

383
00:10:45,279 --> 00:10:47,920
know how to monitor or you're not

384
00:10:46,959 --> 00:10:50,719
allowed to monitor

385
00:10:47,920 --> 00:10:52,240
then you might not catch this effect

386
00:10:50,720 --> 00:10:54,399
it's also helpful to put a number on

387
00:10:52,240 --> 00:10:56,560
some experiments that might look neutral

388
00:10:54,399 --> 00:10:57,440
in a simple t-test but still reduce

389
00:10:56,560 --> 00:10:59,518
inequality

390
00:10:57,440 --> 00:11:01,440
right because that is a potentially a

391
00:10:59,519 --> 00:11:02,560
goal for a company is to make sure that

392
00:11:01,440 --> 00:11:03,920
you know there's more and more people

393
00:11:02,560 --> 00:11:05,279
that are involved and that the people

394
00:11:03,920 --> 00:11:06,959
who have like low involvement

395
00:11:05,279 --> 00:11:08,480
platform and i mean that both by

396
00:11:06,959 --> 00:11:10,239
sessions or by job opportunities

397
00:11:08,480 --> 00:11:11,839
that their number goes up in other words

398
00:11:10,240 --> 00:11:13,920
even an experiment that you know

399
00:11:11,839 --> 00:11:15,200
might not have a positive impact on the

400
00:11:13,920 --> 00:11:17,839
total of the metric

401
00:11:15,200 --> 00:11:18,560
you might still want to launch it i mean

402
00:11:17,839 --> 00:11:21,600
you know

403
00:11:18,560 --> 00:11:24,640
and and make it your main experience

404
00:11:21,600 --> 00:11:26,640
if um it reduces inequality if it makes

405
00:11:24,640 --> 00:11:28,079
uh professional opportunities more

406
00:11:26,640 --> 00:11:29,519
equally distributed

407
00:11:28,079 --> 00:11:31,599
we don't need to specify segments of

408
00:11:29,519 --> 00:11:33,519
interest so we don't need i mean for

409
00:11:31,600 --> 00:11:34,959
this at least to give an exhausted

410
00:11:33,519 --> 00:11:36,399
exhaustive list of all the groups we

411
00:11:34,959 --> 00:11:37,599
care about i mean that there's still a

412
00:11:36,399 --> 00:11:38,560
lot of value in doing that and we're

413
00:11:37,600 --> 00:11:39,920
also doing that but

414
00:11:38,560 --> 00:11:41,599
at least we don't need it for this

415
00:11:39,920 --> 00:11:43,519
specific method and

416
00:11:41,600 --> 00:11:44,640
for those of you who kind of stared at

417
00:11:43,519 --> 00:11:45,519
the formula up there you'll see that

418
00:11:44,640 --> 00:11:47,519
it's actually very

419
00:11:45,519 --> 00:11:48,880
very scalable and in fact we're open

420
00:11:47,519 --> 00:11:51,360
sourcing this on spark

421
00:11:48,880 --> 00:11:52,639
but it's it's extremely fast to compute

422
00:11:51,360 --> 00:11:55,120
atkinson indices

423
00:11:52,639 --> 00:11:56,399
for thousands of experiments uh in a way

424
00:11:55,120 --> 00:11:57,519
that's very that's very scalable

425
00:11:56,399 --> 00:11:59,200
and part of it is because you know

426
00:11:57,519 --> 00:11:59,920
contrary to genie coefficient and some

427
00:11:59,200 --> 00:12:03,120
other metrics

428
00:11:59,920 --> 00:12:04,560
this is uh decomposable um

429
00:12:03,120 --> 00:12:06,720
as an addition and so you can

430
00:12:04,560 --> 00:12:09,760
essentially it lends itself very well to

431
00:12:06,720 --> 00:12:12,639
spark or to mapreduce

432
00:12:09,760 --> 00:12:13,279
so okay so once we have so what we do is

433
00:12:12,639 --> 00:12:16,079
we actually

434
00:12:13,279 --> 00:12:17,040
compute these atkinson indices um and if

435
00:12:16,079 --> 00:12:20,479
we see a strong

436
00:12:17,040 --> 00:12:22,959
positive or negative inequality impact

437
00:12:20,480 --> 00:12:24,000
then what do we do well essentially

438
00:12:22,959 --> 00:12:25,839
there's a review

439
00:12:24,000 --> 00:12:27,680
council that sits you know just about

440
00:12:25,839 --> 00:12:28,480
every month uh sometimes every couple of

441
00:12:27,680 --> 00:12:30,319
months

442
00:12:28,480 --> 00:12:32,480
and reviews the exp the experiments that

443
00:12:30,320 --> 00:12:34,399
had the strongest inequality impact

444
00:12:32,480 --> 00:12:36,480
both in terms of increasing inequality

445
00:12:34,399 --> 00:12:37,120
the most and decreasing inequality the

446
00:12:36,480 --> 00:12:39,040
most

447
00:12:37,120 --> 00:12:40,560
um and so the goals of these monthly

448
00:12:39,040 --> 00:12:42,079
meetings is essentially to inform so to

449
00:12:40,560 --> 00:12:43,040
educate product owners and product

450
00:12:42,079 --> 00:12:44,638
managers

451
00:12:43,040 --> 00:12:47,040
about the impacts that they're having

452
00:12:44,639 --> 00:12:49,120
and we show them a series of charts and

453
00:12:47,040 --> 00:12:50,399
and representations of quantiles and

454
00:12:49,120 --> 00:12:51,680
potential deep dives

455
00:12:50,399 --> 00:12:53,440
uh and the point is not to you know just

456
00:12:51,680 --> 00:12:55,199
be the police but it's to inform and we

457
00:12:53,440 --> 00:12:57,279
also want to promote and say

458
00:12:55,200 --> 00:12:59,040
you know if you have if your goal is to

459
00:12:57,279 --> 00:13:00,639
reduce inequality well you know here

460
00:12:59,040 --> 00:13:01,040
five initiatives is your of yours that

461
00:13:00,639 --> 00:13:02,639
just

462
00:13:01,040 --> 00:13:04,160
seem to have worked and so maybe you

463
00:13:02,639 --> 00:13:05,440
want to do more of those initiatives

464
00:13:04,160 --> 00:13:06,480
and also for us and this is what i'll

465
00:13:05,440 --> 00:13:07,279
share a little bit towards the end of

466
00:13:06,480 --> 00:13:09,360
this talk

467
00:13:07,279 --> 00:13:10,720
is to build a knowledge page which is

468
00:13:09,360 --> 00:13:12,320
what are the kinds of experiments

469
00:13:10,720 --> 00:13:13,760
the kinds of features for for a big

470
00:13:12,320 --> 00:13:16,160
social um you know

471
00:13:13,760 --> 00:13:17,519
social network like ours that seem to

472
00:13:16,160 --> 00:13:20,240
lead to more equally distributed

473
00:13:17,519 --> 00:13:22,160
professional outcomes um and so it's

474
00:13:20,240 --> 00:13:23,760
about you know kind of

475
00:13:22,160 --> 00:13:25,360
you can think of it i mean as opposed to

476
00:13:23,760 --> 00:13:26,880
kind of

477
00:13:25,360 --> 00:13:28,399
top-down fairness where you pick a

478
00:13:26,880 --> 00:13:30,000
metric and you kind of enforce it

479
00:13:28,399 --> 00:13:32,079
this is much more kind of like bottom-up

480
00:13:30,000 --> 00:13:34,000
fairness where all sorts of teams are

481
00:13:32,079 --> 00:13:35,680
trying all sorts of experiments

482
00:13:34,000 --> 00:13:37,680
and we just happen to see which which

483
00:13:35,680 --> 00:13:39,519
ones reduce inequality

484
00:13:37,680 --> 00:13:40,959
and then we have a review to decide

485
00:13:39,519 --> 00:13:42,480
whether or not we think that's making

486
00:13:40,959 --> 00:13:45,599
outcomes more fair

487
00:13:42,480 --> 00:13:47,120
um and so the process is we monitor all

488
00:13:45,600 --> 00:13:48,800
experiments we select the ones with the

489
00:13:47,120 --> 00:13:51,040
most significant impacts

490
00:13:48,800 --> 00:13:52,639
we invite their owners we provide that

491
00:13:51,040 --> 00:13:54,319
feedback they have described and we have

492
00:13:52,639 --> 00:13:56,079
this informational meeting it's not that

493
00:13:54,320 --> 00:13:57,600
we're after anybody to say hey don't do

494
00:13:56,079 --> 00:13:59,359
this we just want to make sure that

495
00:13:57,600 --> 00:14:01,040
if people are after more equally

496
00:13:59,360 --> 00:14:01,519
distributed outcomes we give them the

497
00:14:01,040 --> 00:14:04,480
means

498
00:14:01,519 --> 00:14:05,760
um to measure this and so you know what

499
00:14:04,480 --> 00:14:08,800
we've learned so far

500
00:14:05,760 --> 00:14:09,920
i want to share a couple of examples of

501
00:14:08,800 --> 00:14:11,519
some of the things that we've learned so

502
00:14:09,920 --> 00:14:14,000
as i've mentioned we've looked at this

503
00:14:11,519 --> 00:14:14,880
for many experiments um and we've been

504
00:14:14,000 --> 00:14:17,360
able to

505
00:14:14,880 --> 00:14:17,920
extract kind of some some high level

506
00:14:17,360 --> 00:14:20,480
lessons

507
00:14:17,920 --> 00:14:22,240
uh in terms of the the kinds of

508
00:14:20,480 --> 00:14:25,680
experiments that seem to reduce

509
00:14:22,240 --> 00:14:27,680
um or increase inequality overall um

510
00:14:25,680 --> 00:14:29,120
and so high level findings is the first

511
00:14:27,680 --> 00:14:30,479
one is we should really pay special

512
00:14:29,120 --> 00:14:31,680
attention to business neutral

513
00:14:30,480 --> 00:14:33,920
experiments

514
00:14:31,680 --> 00:14:36,319
and so in a number of companies um you

515
00:14:33,920 --> 00:14:38,160
know a lot of experiments are run

516
00:14:36,320 --> 00:14:39,519
hoping for a null result hoping that

517
00:14:38,160 --> 00:14:41,439
there's no significance

518
00:14:39,519 --> 00:14:42,880
and you know that might be experiments

519
00:14:41,440 --> 00:14:44,800
for example where you change something

520
00:14:42,880 --> 00:14:46,480
in the back end or you change the way

521
00:14:44,800 --> 00:14:49,599
the site is rendered you change some

522
00:14:46,480 --> 00:14:51,519
sort of you know back-end

523
00:14:49,600 --> 00:14:52,959
back-end system and you just want to

524
00:14:51,519 --> 00:14:54,399
make sure that there's no regression

525
00:14:52,959 --> 00:14:55,680
that it's not actually hurting members

526
00:14:54,399 --> 00:14:57,360
that it's not hurting the metrics that

527
00:14:55,680 --> 00:14:59,279
it's not hurting site speed for example

528
00:14:57,360 --> 00:15:00,000
and so a surprising number of

529
00:14:59,279 --> 00:15:01,920
experiments

530
00:15:00,000 --> 00:15:03,199
actually are meant to check that we're

531
00:15:01,920 --> 00:15:05,199
not hurting metrics

532
00:15:03,199 --> 00:15:07,760
not necessarily that we're increasing

533
00:15:05,199 --> 00:15:08,959
these metrics and yet we found in the in

534
00:15:07,760 --> 00:15:11,040
many instances

535
00:15:08,959 --> 00:15:12,000
that these experiments may look neutral

536
00:15:11,040 --> 00:15:13,519
on average

537
00:15:12,000 --> 00:15:14,800
but they may actually have an inequality

538
00:15:13,519 --> 00:15:16,800
impact which is they can have this rich

539
00:15:14,800 --> 00:15:18,560
get richer impact which is people with

540
00:15:16,800 --> 00:15:20,079
already high sessions already very fast

541
00:15:18,560 --> 00:15:22,000
the fast devices

542
00:15:20,079 --> 00:15:23,599
get a lot more speed for example and

543
00:15:22,000 --> 00:15:25,279
people with like poor devices get even

544
00:15:23,600 --> 00:15:26,480
less speed but on average it looks okay

545
00:15:25,279 --> 00:15:28,720
so it people might

546
00:15:26,480 --> 00:15:30,560
think mistakenly that it's neutral uh

547
00:15:28,720 --> 00:15:31,519
and here you know what this helps us

548
00:15:30,560 --> 00:15:33,199
emphasize that there's a difference

549
00:15:31,519 --> 00:15:34,639
between being business neutral and being

550
00:15:33,199 --> 00:15:36,000
really neutral in terms of the impact we

551
00:15:34,639 --> 00:15:38,320
have on our members

552
00:15:36,000 --> 00:15:39,519
notifications are a very powerful tool

553
00:15:38,320 --> 00:15:41,839
to reduce inequality

554
00:15:39,519 --> 00:15:43,600
and so notifications are the tools that

555
00:15:41,839 --> 00:15:45,440
we use to bring people back to the site

556
00:15:43,600 --> 00:15:46,560
and i'll give you a more precise example

557
00:15:45,440 --> 00:15:48,079
in a minute

558
00:15:46,560 --> 00:15:50,000
but essentially it turns out that like

559
00:15:48,079 --> 00:15:51,839
specific notifications

560
00:15:50,000 --> 00:15:53,680
targeted notifications can be a great

561
00:15:51,839 --> 00:15:55,040
tool to bring people back to the site

562
00:15:53,680 --> 00:15:57,439
and to reduce inequality

563
00:15:55,040 --> 00:15:58,800
not just in engagement but also in job

564
00:15:57,440 --> 00:16:00,720
applications and in

565
00:15:58,800 --> 00:16:02,000
professional opportunities we've also

566
00:16:00,720 --> 00:16:03,440
found a number of experiments where the

567
00:16:02,000 --> 00:16:06,000
site speed

568
00:16:03,440 --> 00:16:07,680
hadn't had an impact and so things

569
00:16:06,000 --> 00:16:10,639
trying different kinds of rendering

570
00:16:07,680 --> 00:16:12,719
and also a the slow speed version i mean

571
00:16:10,639 --> 00:16:14,160
the low bandwidth version of linkedin

572
00:16:12,720 --> 00:16:15,600
many experiments that had to do with

573
00:16:14,160 --> 00:16:16,480
these actually had a very positive

574
00:16:15,600 --> 00:16:19,759
impact and so

575
00:16:16,480 --> 00:16:22,240
um um different features that were

576
00:16:19,759 --> 00:16:24,000
that are trying to essentially work

577
00:16:22,240 --> 00:16:24,399
better for people with lower bandwidth

578
00:16:24,000 --> 00:16:26,000
or

579
00:16:24,399 --> 00:16:27,519
lower speed devices seem to really

580
00:16:26,000 --> 00:16:29,120
reduce inequality

581
00:16:27,519 --> 00:16:30,880
in the outcomes that we care about

582
00:16:29,120 --> 00:16:32,399
reducing inequality can lead to a member

583
00:16:30,880 --> 00:16:33,120
member experience and so we've seen this

584
00:16:32,399 --> 00:16:36,000
in a couple of

585
00:16:33,120 --> 00:16:36,959
a couple of occasions especially um on

586
00:16:36,000 --> 00:16:38,320
the feed so

587
00:16:36,959 --> 00:16:39,920
when inequality in terms of who

588
00:16:38,320 --> 00:16:40,480
contributes to the feed gets reduced it

589
00:16:39,920 --> 00:16:43,199
seems like

590
00:16:40,480 --> 00:16:43,920
overall the member based benefit um but

591
00:16:43,199 --> 00:16:45,199
by and large

592
00:16:43,920 --> 00:16:47,360
i mean what really jumped out at us

593
00:16:45,199 --> 00:16:49,599
after having all these conversations

594
00:16:47,360 --> 00:16:51,040
is that both positive and negative

595
00:16:49,600 --> 00:16:52,880
inequality impacts are very often

596
00:16:51,040 --> 00:16:54,319
unintended and so when we go to the team

597
00:16:52,880 --> 00:16:56,560
and try to have this conversation

598
00:16:54,320 --> 00:16:58,240
it often comes as a as a good or maybe

599
00:16:56,560 --> 00:16:59,758
not so good surprise to them

600
00:16:58,240 --> 00:17:01,120
that this is happening and of course

601
00:16:59,759 --> 00:17:02,560
especially the teams who you know go

602
00:17:01,120 --> 00:17:03,120
into thinking that their experiment is

603
00:17:02,560 --> 00:17:05,039
going to be

604
00:17:03,120 --> 00:17:06,079
quote-unquote business neutral and so

605
00:17:05,039 --> 00:17:07,599
with this you know we're essentially

606
00:17:06,079 --> 00:17:09,119
building ourselves a little book

607
00:17:07,599 --> 00:17:10,719
of these experiments and of these

608
00:17:09,119 --> 00:17:11,119
different kinds of things that we know

609
00:17:10,720 --> 00:17:13,360
now

610
00:17:11,119 --> 00:17:14,799
seem to have an impact on inequality and

611
00:17:13,359 --> 00:17:16,079
so i want to just deep dive on one

612
00:17:14,799 --> 00:17:16,799
example to give you a flavor of what

613
00:17:16,079 --> 00:17:18,559
we're doing

614
00:17:16,799 --> 00:17:20,720
and before this just to give you a quick

615
00:17:18,559 --> 00:17:21,280
primer so at linkedin we have this one

616
00:17:20,720 --> 00:17:23,679
measure

617
00:17:21,280 --> 00:17:24,319
that we use fairly that we use on on

618
00:17:23,679 --> 00:17:26,319
occasion

619
00:17:24,319 --> 00:17:27,678
which measures essentially the local

620
00:17:26,319 --> 00:17:29,280
clustering coefficient of a person's

621
00:17:27,679 --> 00:17:31,440
network which is how diverse

622
00:17:29,280 --> 00:17:32,799
how structurally diverse the network is

623
00:17:31,440 --> 00:17:34,400
because you know some people have

624
00:17:32,799 --> 00:17:36,400
connections are extremely redundant so

625
00:17:34,400 --> 00:17:38,480
here on this picture bob for example

626
00:17:36,400 --> 00:17:39,520
all of bob's connections know each other

627
00:17:38,480 --> 00:17:41,840
and alice here

628
00:17:39,520 --> 00:17:43,520
uh her connections don't know each other

629
00:17:41,840 --> 00:17:44,799
on average and we know through you know

630
00:17:43,520 --> 00:17:46,320
half a century of social

631
00:17:44,799 --> 00:17:48,320
social networks literature that

632
00:17:46,320 --> 00:17:50,399
generally speaking having a more

633
00:17:48,320 --> 00:17:52,159
open network a more structurally diverse

634
00:17:50,400 --> 00:17:53,360
network where alice is here you're

635
00:17:52,160 --> 00:17:54,720
bridging two different communities

636
00:17:53,360 --> 00:17:55,520
that's typically good for you and for

637
00:17:54,720 --> 00:17:57,679
your career

638
00:17:55,520 --> 00:17:59,679
um and so when we run these experiments

639
00:17:57,679 --> 00:18:01,360
we actually also look at the impact

640
00:17:59,679 --> 00:18:03,600
depending on the type of network that

641
00:18:01,360 --> 00:18:04,799
the member has and so here you can

642
00:18:03,600 --> 00:18:05,678
compute an openness score and you can

643
00:18:04,799 --> 00:18:07,440
see that alice's

644
00:18:05,679 --> 00:18:09,120
alice's network is more open more

645
00:18:07,440 --> 00:18:11,760
structurally diverse

646
00:18:09,120 --> 00:18:13,039
than bob's network and so the first

647
00:18:11,760 --> 00:18:14,799
experiment really jumped out at us

648
00:18:13,039 --> 00:18:16,080
actually was this insta jobs experiment

649
00:18:14,799 --> 00:18:17,679
and so what this is and is something

650
00:18:16,080 --> 00:18:18,559
that now everyone can have access to on

651
00:18:17,679 --> 00:18:21,360
linkedin which is

652
00:18:18,559 --> 00:18:22,960
active job seekers now receive a push

653
00:18:21,360 --> 00:18:24,240
notification within seconds

654
00:18:22,960 --> 00:18:25,919
when there's a relevant job that's being

655
00:18:24,240 --> 00:18:28,160
posted beforehand you have to wait to

656
00:18:25,919 --> 00:18:30,240
organically hear about the experiment

657
00:18:28,160 --> 00:18:31,360
hear about excuse me here about the um

658
00:18:30,240 --> 00:18:32,799
the job posting so

659
00:18:31,360 --> 00:18:34,799
either someone referred you or you saw

660
00:18:32,799 --> 00:18:35,280
it on your feed or you went to look for

661
00:18:34,799 --> 00:18:37,760
it

662
00:18:35,280 --> 00:18:39,678
and now instantly you get a notification

663
00:18:37,760 --> 00:18:40,640
um and so the impact was actually pretty

664
00:18:39,679 --> 00:18:42,240
interesting which is there was a

665
00:18:40,640 --> 00:18:43,360
positive impact on job interactions

666
00:18:42,240 --> 00:18:44,320
which is what we cared about for this

667
00:18:43,360 --> 00:18:46,399
specific experiment

668
00:18:44,320 --> 00:18:48,320
but there was a tremendous reduction in

669
00:18:46,400 --> 00:18:49,360
the inequality metric in the atkinson

670
00:18:48,320 --> 00:18:51,280
index in other words

671
00:18:49,360 --> 00:18:52,559
people who had few job applications seem

672
00:18:51,280 --> 00:18:53,678
to be getting more right the difference

673
00:18:52,559 --> 00:18:55,200
between the bottom and the top of the

674
00:18:53,679 --> 00:18:58,000
distribution in terms of

675
00:18:55,200 --> 00:18:59,200
um job applications was it was reducing

676
00:18:58,000 --> 00:18:59,840
a fair bit and so that seemed pretty

677
00:18:59,200 --> 00:19:00,880
encouraging

678
00:18:59,840 --> 00:19:02,320
and then we looked and we did a deep

679
00:19:00,880 --> 00:19:03,679
dive and we actually looked at this

680
00:19:02,320 --> 00:19:05,600
social capital metric

681
00:19:03,679 --> 00:19:07,039
uh this you know that i just described

682
00:19:05,600 --> 00:19:08,240
and what we saw is like most of the

683
00:19:07,039 --> 00:19:10,000
effect was concentrated

684
00:19:08,240 --> 00:19:11,840
on members with closed networks so

685
00:19:10,000 --> 00:19:12,880
people who don't have extremely powerful

686
00:19:11,840 --> 00:19:14,000
social networks networks that are

687
00:19:12,880 --> 00:19:15,200
redundant maybe say

688
00:19:14,000 --> 00:19:17,120
you know some think of like a high

689
00:19:15,200 --> 00:19:19,440
school network and so you know that was

690
00:19:17,120 --> 00:19:20,639
a very powerful finding because

691
00:19:19,440 --> 00:19:22,640
you know this is an example where an

692
00:19:20,640 --> 00:19:25,919
algorithm can actually enhance

693
00:19:22,640 --> 00:19:28,640
inclusion by substituting for a

694
00:19:25,919 --> 00:19:29,520
say a not so strong social network and

695
00:19:28,640 --> 00:19:31,120
so here

696
00:19:29,520 --> 00:19:33,520
um this was a really kind of powerful

697
00:19:31,120 --> 00:19:33,840
finding and so how we detected it with

698
00:19:33,520 --> 00:19:36,240
the

699
00:19:33,840 --> 00:19:37,600
atkinson monitoring system uh it seemed

700
00:19:36,240 --> 00:19:38,720
interesting and so we did a deep dive

701
00:19:37,600 --> 00:19:39,840
and this is these are the kinds of

702
00:19:38,720 --> 00:19:41,039
results that we found

703
00:19:39,840 --> 00:19:43,360
and you know there's all sorts of

704
00:19:41,039 --> 00:19:44,480
evidence that suggests that some members

705
00:19:43,360 --> 00:19:46,399
if they don't have a strong social

706
00:19:44,480 --> 00:19:47,679
network they may not receive the job and

707
00:19:46,400 --> 00:19:49,039
so you know this is where getting

708
00:19:47,679 --> 00:19:50,559
notification really helps and also some

709
00:19:49,039 --> 00:19:52,320
people might sell censor

710
00:19:50,559 --> 00:19:54,000
from applying to a java so this is where

711
00:19:52,320 --> 00:19:55,520
um getting them the in certification

712
00:19:54,000 --> 00:19:57,039
instantly where they don't see that a

713
00:19:55,520 --> 00:19:59,280
bunch of people applied already

714
00:19:57,039 --> 00:20:00,799
also really helps um so i'm on almost my

715
00:19:59,280 --> 00:20:02,559
time i just want to show you the other

716
00:20:00,799 --> 00:20:03,679
some of the other elements of the ui so

717
00:20:02,559 --> 00:20:05,039
we actually also show a bunch of

718
00:20:03,679 --> 00:20:07,039
distributional statistics

719
00:20:05,039 --> 00:20:09,280
and so this for example shows the impact

720
00:20:07,039 --> 00:20:10,000
by um by quantile and so this experiment

721
00:20:09,280 --> 00:20:11,678
you can see

722
00:20:10,000 --> 00:20:12,799
that the top one percent is getting a

723
00:20:11,679 --> 00:20:14,960
big boost while everyone else is

724
00:20:12,799 --> 00:20:16,879
suffering for this specific experiment

725
00:20:14,960 --> 00:20:18,640
um and also want to share that all of

726
00:20:16,880 --> 00:20:19,440
this is on github i'm leaving the link

727
00:20:18,640 --> 00:20:22,000
here and so

728
00:20:19,440 --> 00:20:23,600
if you need to compute inequality at

729
00:20:22,000 --> 00:20:25,360
scale using spark please please check

730
00:20:23,600 --> 00:20:27,280
check this page out

731
00:20:25,360 --> 00:20:29,120
and that's it so i'm very happy to

732
00:20:27,280 --> 00:20:32,559
answer questions now

733
00:20:29,120 --> 00:20:32,559
thank you so much for for your time and

734
00:20:32,840 --> 00:20:35,840
attention

735
00:20:40,559 --> 00:20:42,639
you

