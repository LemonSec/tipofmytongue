1
00:00:07,200 --> 00:00:10,719
hello i'm basilius manfredis pg student

2
00:00:09,360 --> 00:00:12,639
university college london

3
00:00:10,719 --> 00:00:14,480
i'm going to talk about optimization for

4
00:00:12,639 --> 00:00:16,160
attack surface exploration

5
00:00:14,480 --> 00:00:18,560
and in particular focus on the case of

6
00:00:16,160 --> 00:00:21,680
tls this joint work with jamie hayes

7
00:00:18,560 --> 00:00:21,680
from university college loan

8
00:00:22,960 --> 00:00:26,320
to begin with attack surface exploration

9
00:00:25,039 --> 00:00:28,240
is a process that

10
00:00:26,320 --> 00:00:30,160
given a system or protocol we first

11
00:00:28,240 --> 00:00:31,679
start its operation

12
00:00:30,160 --> 00:00:33,680
security properties and the trust

13
00:00:31,679 --> 00:00:35,440
assumptions it operates on

14
00:00:33,680 --> 00:00:37,600
then define the adversaries and their

15
00:00:35,440 --> 00:00:39,280
calls and then given those adversaries

16
00:00:37,600 --> 00:00:41,440
we design and realize

17
00:00:39,280 --> 00:00:43,840
attacks that we can later further

18
00:00:41,440 --> 00:00:45,760
optimize

19
00:00:43,840 --> 00:00:47,680
the goal of actually coming up with

20
00:00:45,760 --> 00:00:50,320
attacks that we will later

21
00:00:47,680 --> 00:00:52,000
optimize uh has to do with us advancing

22
00:00:50,320 --> 00:00:53,680
the state of the art so as to better

23
00:00:52,000 --> 00:00:56,719
evaluate the security

24
00:00:53,680 --> 00:00:57,440
of the given system or protocol as well

25
00:00:56,719 --> 00:00:59,840
as

26
00:00:57,440 --> 00:01:01,519
further improving our defenses uh once

27
00:00:59,840 --> 00:01:03,280
we have a good understanding of

28
00:01:01,520 --> 00:01:04,960
what they are doing well and where we

29
00:01:03,280 --> 00:01:07,840
are actually failing

30
00:01:04,959 --> 00:01:10,158
this process is iterative meaning that

31
00:01:07,840 --> 00:01:13,040
given an attack we

32
00:01:10,159 --> 00:01:14,320
always go back to the beginning verify

33
00:01:13,040 --> 00:01:16,560
half as entities

34
00:01:14,320 --> 00:01:17,758
seconds false false leaves uh false

35
00:01:16,560 --> 00:01:20,960
negatives

36
00:01:17,759 --> 00:01:23,360
uh and then perhaps revisit the

37
00:01:20,960 --> 00:01:25,600
subjects with regards to our adversaries

38
00:01:23,360 --> 00:01:27,840
um check if we can relax and see if we

39
00:01:25,600 --> 00:01:29,919
can do anything better

40
00:01:27,840 --> 00:01:33,759
this process is a best effort process

41
00:01:29,920 --> 00:01:33,759
and usually requires human involvement

42
00:01:33,840 --> 00:01:37,280
modern machine learning can actually

43
00:01:35,600 --> 00:01:40,880
help us advance

44
00:01:37,280 --> 00:01:42,640
further state of the art

45
00:01:40,880 --> 00:01:44,240
if we incorporate it into this

46
00:01:42,640 --> 00:01:45,920
optimization process

47
00:01:44,240 --> 00:01:47,679
and here's the added benefit that it can

48
00:01:45,920 --> 00:01:51,200
minimize human involvement

49
00:01:47,680 --> 00:01:52,640
however for that to be possible there

50
00:01:51,200 --> 00:01:54,799
are a couple of requirements that

51
00:01:52,640 --> 00:01:56,640
need to be met the first one is that it

52
00:01:54,799 --> 00:01:58,320
should be easier to collect data

53
00:01:56,640 --> 00:01:59,680
in training model than to optimize the

54
00:01:58,320 --> 00:02:02,240
attack manually

55
00:01:59,680 --> 00:02:04,159
and the second is that well given an

56
00:02:02,240 --> 00:02:05,839
adversary the goal of that adversary

57
00:02:04,159 --> 00:02:08,959
needs to be expressed

58
00:02:05,840 --> 00:02:10,239
in some form that makes sense for the

59
00:02:08,959 --> 00:02:12,959
machine learning model

60
00:02:10,239 --> 00:02:15,360
in that case is a differentiable loss

61
00:02:12,959 --> 00:02:15,360
function

62
00:02:15,440 --> 00:02:19,200
our use case as i said earlier is

63
00:02:16,959 --> 00:02:20,560
traffic fingerprinting

64
00:02:19,200 --> 00:02:22,319
there are two types that we're going to

65
00:02:20,560 --> 00:02:25,200
consider one is webpage

66
00:02:22,319 --> 00:02:26,319
fingerprinting in particular tls in that

67
00:02:25,200 --> 00:02:28,000
case the adversary

68
00:02:26,319 --> 00:02:29,920
is intercepting traffic between the

69
00:02:28,000 --> 00:02:31,680
client's browser

70
00:02:29,920 --> 00:02:34,079
and the website that the client is

71
00:02:31,680 --> 00:02:36,000
visiting as you may know tls does not

72
00:02:34,080 --> 00:02:38,080
protect the ap of the server visited so

73
00:02:36,000 --> 00:02:40,319
we're going to assume that the

74
00:02:38,080 --> 00:02:41,200
person knows which website the victim is

75
00:02:40,319 --> 00:02:44,000
visiting

76
00:02:41,200 --> 00:02:45,920
however what the advisor doesn't know is

77
00:02:44,000 --> 00:02:49,040
the exact web page that is

78
00:02:45,920 --> 00:02:49,440
visited by the victim so that's the one

79
00:02:49,040 --> 00:02:51,519
who's

80
00:02:49,440 --> 00:02:53,280
the one use of web page fingerprinting

81
00:02:51,519 --> 00:02:54,480
there is another use

82
00:02:53,280 --> 00:02:57,040
that does not have to do with

83
00:02:54,480 --> 00:02:58,238
surveillance it's um benign use case

84
00:02:57,040 --> 00:03:00,720
where

85
00:02:58,239 --> 00:03:03,200
network administrators are actually

86
00:03:00,720 --> 00:03:06,560
monitoring their network for malware

87
00:03:03,200 --> 00:03:09,760
and to do that they are intercepting tls

88
00:03:06,560 --> 00:03:11,760
traffic through their network and try to

89
00:03:09,760 --> 00:03:14,079
master traffic to commander control

90
00:03:11,760 --> 00:03:18,560
servers or unique patterns that

91
00:03:14,080 --> 00:03:19,440
malware traffic exhibits the second and

92
00:03:18,560 --> 00:03:22,319
most popular

93
00:03:19,440 --> 00:03:24,720
type of traffic fingerprinting is

94
00:03:22,319 --> 00:03:27,518
website fingerprinting we often

95
00:03:24,720 --> 00:03:29,280
see it in papers with regards to torque

96
00:03:27,519 --> 00:03:30,319
in that case the adversary doesn't know

97
00:03:29,280 --> 00:03:33,680
which website the

98
00:03:30,319 --> 00:03:35,440
victim is visiting the adversary is

99
00:03:33,680 --> 00:03:36,720
standing in between any intercepting

100
00:03:35,440 --> 00:03:39,040
traffic in between

101
00:03:36,720 --> 00:03:40,159
uh the client and the entry node of the

102
00:03:39,040 --> 00:03:42,159
tor network

103
00:03:40,159 --> 00:03:44,319
and this the other investor is trying to

104
00:03:42,159 --> 00:03:45,840
infer which website this time

105
00:03:44,319 --> 00:03:47,839
the victim is visiting the

106
00:03:45,840 --> 00:03:50,480
fingerprinting task is very simple

107
00:03:47,840 --> 00:03:51,599
the adversary taps the traffic uh

108
00:03:50,480 --> 00:03:53,599
between the victim

109
00:03:51,599 --> 00:03:55,679
and the server visited or the tore entry

110
00:03:53,599 --> 00:03:58,000
node and then converts it

111
00:03:55,680 --> 00:03:59,360
into a sequence of bytes that denote the

112
00:03:58,000 --> 00:04:01,920
length of the packets

113
00:03:59,360 --> 00:04:03,760
sent or received by the adversary so the

114
00:04:01,920 --> 00:04:05,359
consequence of white

115
00:04:03,760 --> 00:04:08,640
contains only the length and the

116
00:04:05,360 --> 00:04:08,640
direction of the packet set

117
00:04:08,959 --> 00:04:12,720
the process for the figure printing from

118
00:04:10,720 --> 00:04:14,640
the adversary's perspective is

119
00:04:12,720 --> 00:04:17,199
as a first step the adversary compiles

120
00:04:14,640 --> 00:04:20,560
usually a data set of label sequences

121
00:04:17,199 --> 00:04:22,240
from web pages or websites that they

122
00:04:20,560 --> 00:04:24,960
want to fingerprint

123
00:04:22,240 --> 00:04:25,840
and then this is actually fairly easy to

124
00:04:24,960 --> 00:04:28,000
automate

125
00:04:25,840 --> 00:04:29,840
and then prepares a classification

126
00:04:28,000 --> 00:04:31,280
system that classification system can be

127
00:04:29,840 --> 00:04:32,560
anything it can be a very advanced

128
00:04:31,280 --> 00:04:35,359
machine learning system

129
00:04:32,560 --> 00:04:35,840
but based on machine learning or it can

130
00:04:35,360 --> 00:04:38,960
be

131
00:04:35,840 --> 00:04:41,919
um just a manual annotation system

132
00:04:38,960 --> 00:04:43,599
so anything really and then the third

133
00:04:41,919 --> 00:04:46,080
step is

134
00:04:43,600 --> 00:04:47,199
once the adversary captures some traffic

135
00:04:46,080 --> 00:04:49,120
that they wish to

136
00:04:47,199 --> 00:04:50,320
fingerprint they push it through the

137
00:04:49,120 --> 00:04:53,919
system

138
00:04:50,320 --> 00:04:55,520
to have it classified since

139
00:04:53,919 --> 00:04:57,039
the first step is fairly easy to

140
00:04:55,520 --> 00:04:59,039
automate

141
00:04:57,040 --> 00:05:02,000
machine learning can actually help with

142
00:04:59,040 --> 00:05:03,440
steps two and three

143
00:05:02,000 --> 00:05:05,360
so now we're going to go through a

144
00:05:03,440 --> 00:05:07,840
non-exhaustive review of the literature

145
00:05:05,360 --> 00:05:09,680
we will see how machine learning helped

146
00:05:07,840 --> 00:05:10,479
advance that field further in the last

147
00:05:09,680 --> 00:05:13,759
few years

148
00:05:10,479 --> 00:05:16,080
so up until 2016 most of the literature

149
00:05:13,759 --> 00:05:18,160
was working with incremental steps

150
00:05:16,080 --> 00:05:19,919
with regards to the performance of

151
00:05:18,160 --> 00:05:22,720
fingerprinting adversaries

152
00:05:19,919 --> 00:05:23,680
and in most papers there was some

153
00:05:22,720 --> 00:05:26,639
element of

154
00:05:23,680 --> 00:05:28,560
feature engineering as well as some

155
00:05:26,639 --> 00:05:31,840
small advancements with regards to the

156
00:05:28,560 --> 00:05:35,280
fingerprinting technique by itself

157
00:05:31,840 --> 00:05:36,239
however since 2017 deep learning models

158
00:05:35,280 --> 00:05:39,280
became

159
00:05:36,240 --> 00:05:41,120
more common in that community uh and in

160
00:05:39,280 --> 00:05:43,919
particular the advantage that those

161
00:05:41,120 --> 00:05:45,759
models brought is that now uh you don't

162
00:05:43,919 --> 00:05:48,320
have to

163
00:05:45,759 --> 00:05:50,080
do kitchen engineering manually but

164
00:05:48,320 --> 00:05:50,639
instead you can use a deep learning

165
00:05:50,080 --> 00:05:53,120
model

166
00:05:50,639 --> 00:05:53,680
that's gonna do it auto in an automated

167
00:05:53,120 --> 00:05:56,960
manner

168
00:05:53,680 --> 00:05:58,639
but without you having to manually craft

169
00:05:56,960 --> 00:05:59,198
the features that make most most of the

170
00:05:58,639 --> 00:06:00,800
sense

171
00:05:59,199 --> 00:06:02,960
because of the success and very good

172
00:06:00,800 --> 00:06:05,360
performance of deep learning models

173
00:06:02,960 --> 00:06:07,840
the same period the the idea of

174
00:06:05,360 --> 00:06:10,080
vulnerability approximators was proposed

175
00:06:07,840 --> 00:06:11,039
uh given a website fingerprinting

176
00:06:10,080 --> 00:06:12,690
defense

177
00:06:11,039 --> 00:06:14,400
we can actually um

178
00:06:12,690 --> 00:06:16,080
[Music]

179
00:06:14,400 --> 00:06:19,840
estimate the security balance of an

180
00:06:16,080 --> 00:06:21,280
adversary given a chosen feature set

181
00:06:19,840 --> 00:06:23,679
this is great because now we have a

182
00:06:21,280 --> 00:06:25,758
lower bound of security which is

183
00:06:23,680 --> 00:06:26,800
something we didn't have before however

184
00:06:25,759 --> 00:06:30,160
the downside of that

185
00:06:26,800 --> 00:06:33,600
is that the estimation can be done only

186
00:06:30,160 --> 00:06:35,280
based on the chosen feature set this gap

187
00:06:33,600 --> 00:06:36,319
was actually covered by a paper

188
00:06:35,280 --> 00:06:38,719
published later on

189
00:06:36,319 --> 00:06:39,759
on a slightly different sub area though

190
00:06:38,720 --> 00:06:42,479
of fingerprinting

191
00:06:39,759 --> 00:06:44,319
so it didn't cover all the features um

192
00:06:42,479 --> 00:06:47,599
that the original paper

193
00:06:44,319 --> 00:06:49,280
was looking into but that second paper

194
00:06:47,600 --> 00:06:50,800
was actually

195
00:06:49,280 --> 00:06:53,440
focusing on the learnability and

196
00:06:50,800 --> 00:06:56,560
reliability of the protocol

197
00:06:53,440 --> 00:07:00,719
based uh on an exhaustive search of the

198
00:06:56,560 --> 00:07:02,319
features moving on to 2019

199
00:07:00,720 --> 00:07:04,000
even though we have very good deep

200
00:07:02,319 --> 00:07:05,599
learning models that can extract the

201
00:07:04,000 --> 00:07:07,039
features automatically for us

202
00:07:05,599 --> 00:07:08,880
there is still research and features

203
00:07:07,039 --> 00:07:10,960
going on for a very good reason

204
00:07:08,880 --> 00:07:12,719
so while for the attacker the exact

205
00:07:10,960 --> 00:07:15,919
features don't necessarily

206
00:07:12,720 --> 00:07:18,400
need to be need to make sense for

207
00:07:15,919 --> 00:07:20,240
designing efficient defenses the

208
00:07:18,400 --> 00:07:22,080
interpretability of features and the

209
00:07:20,240 --> 00:07:24,880
indications were built around those

210
00:07:22,080 --> 00:07:26,960
is still important so that specific

211
00:07:24,880 --> 00:07:29,360
paper was focusing on

212
00:07:26,960 --> 00:07:31,440
the timing element of features so it's

213
00:07:29,360 --> 00:07:31,840
bringing in another dimension that has

214
00:07:31,440 --> 00:07:34,960
been

215
00:07:31,840 --> 00:07:36,000
somewhat overlooked earlier and this is

216
00:07:34,960 --> 00:07:39,039
driving this

217
00:07:36,000 --> 00:07:41,919
this direction forward now

218
00:07:39,039 --> 00:07:44,000
because of the very good performance we

219
00:07:41,919 --> 00:07:46,159
had with existing adversaries

220
00:07:44,000 --> 00:07:47,759
we also started to explore more

221
00:07:46,160 --> 00:07:51,280
realistic ones

222
00:07:47,759 --> 00:07:54,960
meaning that other dimensions of

223
00:07:51,280 --> 00:07:56,638
the attackers can now be fairly relaxed

224
00:07:54,960 --> 00:07:58,960
but one of the problems that had been

225
00:07:56,639 --> 00:08:00,400
expressed actually in 2014 so several

226
00:07:58,960 --> 00:08:03,440
years ago but

227
00:08:00,400 --> 00:08:04,560
hadn't been explored adequately his data

228
00:08:03,440 --> 00:08:08,080
stainless

229
00:08:04,560 --> 00:08:10,400
given a fingerprinting system is trained

230
00:08:08,080 --> 00:08:12,479
on a set of data

231
00:08:10,400 --> 00:08:13,440
one of the problems that the anniversary

232
00:08:12,479 --> 00:08:16,240
may have is that

233
00:08:13,440 --> 00:08:18,080
those data become stale fairly quickly

234
00:08:16,240 --> 00:08:19,120
as the content of the websites is

235
00:08:18,080 --> 00:08:21,440
changing

236
00:08:19,120 --> 00:08:22,800
so uh one of the papers that first

237
00:08:21,440 --> 00:08:25,039
brought this problem up

238
00:08:22,800 --> 00:08:26,000
is the fingerprinting published last

239
00:08:25,039 --> 00:08:29,120
year

240
00:08:26,000 --> 00:08:32,000
and they are looking into exactly that

241
00:08:29,120 --> 00:08:33,760
element uh aiming to make adversaries

242
00:08:32,000 --> 00:08:34,159
and attacks more realistic in the real

243
00:08:33,760 --> 00:08:36,640
world

244
00:08:34,159 --> 00:08:39,039
in a real-world environment uh so that's

245
00:08:36,640 --> 00:08:42,000
the generalizability property

246
00:08:39,039 --> 00:08:43,439
that they that they mention and they

247
00:08:42,000 --> 00:08:46,560
want to address stillness

248
00:08:43,440 --> 00:08:47,519
of the trade training data there's also

249
00:08:46,560 --> 00:08:48,959
bootstrap time

250
00:08:47,519 --> 00:08:51,760
which is another dimension they are

251
00:08:48,959 --> 00:08:55,199
considering with regards to

252
00:08:51,760 --> 00:08:57,760
how quickly can a an adversary train a

253
00:08:55,200 --> 00:08:59,279
classifier and have it ready to be used

254
00:08:57,760 --> 00:09:01,519
and then finally they're also focusing

255
00:08:59,279 --> 00:09:03,439
on flexibility and transferability

256
00:09:01,519 --> 00:09:04,880
because as an adversary you may want to

257
00:09:03,440 --> 00:09:07,839
add a new

258
00:09:04,880 --> 00:09:09,360
site or webpage to be monitored so it's

259
00:09:07,839 --> 00:09:10,320
also important to have this kind of

260
00:09:09,360 --> 00:09:11,920
flexibility

261
00:09:10,320 --> 00:09:13,839
these are all things that hadn't been

262
00:09:11,920 --> 00:09:16,319
considered before so

263
00:09:13,839 --> 00:09:19,519
because we've done so well with the past

264
00:09:16,320 --> 00:09:23,200
adversaries we're now expanding to

265
00:09:19,519 --> 00:09:27,440
add more even more parameters

266
00:09:23,200 --> 00:09:29,839
on those however that last paper

267
00:09:27,440 --> 00:09:32,080
they made a lot of progress but they

268
00:09:29,839 --> 00:09:34,000
still need to retrain

269
00:09:32,080 --> 00:09:36,000
when they want to add a new class or

270
00:09:34,000 --> 00:09:37,760
they when they want to

271
00:09:36,000 --> 00:09:39,120
update their system with the latest

272
00:09:37,760 --> 00:09:40,640
version of the web page is their

273
00:09:39,120 --> 00:09:42,480
fingerprinting

274
00:09:40,640 --> 00:09:44,560
even though the the training is

275
00:09:42,480 --> 00:09:48,320
considerably lighter than that

276
00:09:44,560 --> 00:09:50,000
required before uh another work

277
00:09:48,320 --> 00:09:52,480
published again this year that is trying

278
00:09:50,000 --> 00:09:55,680
to address the stainless problem

279
00:09:52,480 --> 00:09:57,839
um is taking an alternative approach

280
00:09:55,680 --> 00:09:59,359
and what they do is they they use fewer

281
00:09:57,839 --> 00:10:00,720
data so that there is less data

282
00:09:59,360 --> 00:10:02,480
dependency

283
00:10:00,720 --> 00:10:04,640
but they manage to retain the same

284
00:10:02,480 --> 00:10:06,560
performance as if by past works

285
00:10:04,640 --> 00:10:08,640
from our perspective where this whole

286
00:10:06,560 --> 00:10:11,119
thing is going is transfer learning

287
00:10:08,640 --> 00:10:11,760
so just a short introduction in transfer

288
00:10:11,120 --> 00:10:15,200
learning

289
00:10:11,760 --> 00:10:16,800
in that the context of fingerprinting is

290
00:10:15,200 --> 00:10:19,040
given a machine learning model

291
00:10:16,800 --> 00:10:20,959
transferring transfer learning enables

292
00:10:19,040 --> 00:10:22,719
that model to be trained on one task and

293
00:10:20,959 --> 00:10:24,800
then be repurposed

294
00:10:22,720 --> 00:10:25,839
at a very low cost for a second related

295
00:10:24,800 --> 00:10:29,760
tasks

296
00:10:25,839 --> 00:10:31,440
in the fingerprinting context this means

297
00:10:29,760 --> 00:10:34,240
transferable models across various

298
00:10:31,440 --> 00:10:37,519
dimensions the temporal for example

299
00:10:34,240 --> 00:10:39,360
which is relevant to data stillness

300
00:10:37,519 --> 00:10:40,880
across websites and web pages so you

301
00:10:39,360 --> 00:10:42,880
train their model on one

302
00:10:40,880 --> 00:10:44,959
website and then you can move it on

303
00:10:42,880 --> 00:10:47,200
another one

304
00:10:44,959 --> 00:10:48,399
with regards to the location of the

305
00:10:47,200 --> 00:10:50,240
victim so

306
00:10:48,399 --> 00:10:51,600
maybe your training data originates from

307
00:10:50,240 --> 00:10:53,760
one

308
00:10:51,600 --> 00:10:54,720
physical location then the victim

309
00:10:53,760 --> 00:10:57,519
somewhere else

310
00:10:54,720 --> 00:10:59,680
in the world um does the model transfer

311
00:10:57,519 --> 00:11:01,600
well across locations

312
00:10:59,680 --> 00:11:04,079
and then also protocol versions either

313
00:11:01,600 --> 00:11:06,480
this is store protocol versions or

314
00:11:04,079 --> 00:11:07,519
tls protocol versions overall the goal

315
00:11:06,480 --> 00:11:10,480
of this kind of

316
00:11:07,519 --> 00:11:12,480
exploration is to see how versatile

317
00:11:10,480 --> 00:11:15,519
versatile versions can become

318
00:11:12,480 --> 00:11:17,200
across all those dimensions uh to do

319
00:11:15,519 --> 00:11:19,760
that we actually designed

320
00:11:17,200 --> 00:11:21,760
a couple of experiments and we collected

321
00:11:19,760 --> 00:11:23,600
a completely new data set we focused on

322
00:11:21,760 --> 00:11:26,880
tls finger printing

323
00:11:23,600 --> 00:11:29,920
um we collected data set contain

324
00:11:26,880 --> 00:11:32,000
around 19 000 wikipedia articles so our

325
00:11:29,920 --> 00:11:32,880
advisor is trying to distinguish which

326
00:11:32,000 --> 00:11:34,560
article

327
00:11:32,880 --> 00:11:36,079
uh which would be the article the victim

328
00:11:34,560 --> 00:11:38,959
is visiting uh

329
00:11:36,079 --> 00:11:39,839
to collect our um data set we visited

330
00:11:38,959 --> 00:11:43,119
its article

331
00:11:39,839 --> 00:11:44,480
100 time it's time from one different

332
00:11:43,120 --> 00:11:47,279
amazon instance

333
00:11:44,480 --> 00:11:48,640
and we spread spread our instances into

334
00:11:47,279 --> 00:11:50,079
five different different physical

335
00:11:48,640 --> 00:11:52,800
locations around the globe

336
00:11:50,079 --> 00:11:54,638
on the left here we have a visual

337
00:11:52,800 --> 00:11:56,639
representation of our data set we split

338
00:11:54,639 --> 00:11:59,440
our data city to two parts

339
00:11:56,639 --> 00:12:00,000
the green part that contains the subsets

340
00:11:59,440 --> 00:12:01,440
a and b

341
00:12:00,000 --> 00:12:03,519
and the blue part that contains the

342
00:12:01,440 --> 00:12:06,720
subset c and d

343
00:12:03,519 --> 00:12:07,120
the first part the green part contains 6

344
00:12:06,720 --> 00:12:10,639
000

345
00:12:07,120 --> 00:12:12,320
classes while the blue part contains 13

346
00:12:10,639 --> 00:12:16,639
000 different classes

347
00:12:12,320 --> 00:12:19,360
each of those parts features 100 samples

348
00:12:16,639 --> 00:12:20,000
traffic traces in other words uh of of

349
00:12:19,360 --> 00:12:22,720
its class

350
00:12:20,000 --> 00:12:23,600
and then the subsets essentially split

351
00:12:22,720 --> 00:12:26,480
those samples

352
00:12:23,600 --> 00:12:27,200
so subset a has 90 samples for every

353
00:12:26,480 --> 00:12:29,680
class

354
00:12:27,200 --> 00:12:30,399
subset b has 10 samples for every class

355
00:12:29,680 --> 00:12:33,920
and the same

356
00:12:30,399 --> 00:12:35,839
holds for c and d now moving on to our

357
00:12:33,920 --> 00:12:37,760
fingerprinting pipeline

358
00:12:35,839 --> 00:12:39,920
without going into the technical details

359
00:12:37,760 --> 00:12:40,240
the fingerprint fingerprinting pipeline

360
00:12:39,920 --> 00:12:42,000
is

361
00:12:40,240 --> 00:12:44,000
using an embedding model which is

362
00:12:42,000 --> 00:12:46,160
trained to map um

363
00:12:44,000 --> 00:12:47,760
vector inputs essentially in that case

364
00:12:46,160 --> 00:12:49,920
the sequence the bytes of

365
00:12:47,760 --> 00:12:50,800
the sequences of bytes are into a

366
00:12:49,920 --> 00:12:52,800
multi-day

367
00:12:50,800 --> 00:12:55,120
into a point into multi-dimensional

368
00:12:52,800 --> 00:12:58,319
space what the adversary does is

369
00:12:55,120 --> 00:13:00,240
they gather a set of reference samples

370
00:12:58,320 --> 00:13:03,040
and then embed them

371
00:13:00,240 --> 00:13:04,480
using the trained embedding model every

372
00:13:03,040 --> 00:13:06,719
time a new input case

373
00:13:04,480 --> 00:13:08,320
comes from the victim the adversary

374
00:13:06,720 --> 00:13:10,320
embeds that in code as well

375
00:13:08,320 --> 00:13:12,720
and then classifies it based on the

376
00:13:10,320 --> 00:13:14,800
proximity to the reference samples

377
00:13:12,720 --> 00:13:16,160
determines our first experiment is

378
00:13:14,800 --> 00:13:17,839
actually ground rule of experiments so

379
00:13:16,160 --> 00:13:20,079
it's not a transfer learning one

380
00:13:17,839 --> 00:13:22,639
we trained our model on subset a which

381
00:13:20,079 --> 00:13:25,519
contains 6000 classes

382
00:13:22,639 --> 00:13:27,519
and 90 samples for each of those classes

383
00:13:25,519 --> 00:13:30,800
and we tested it on subset b

384
00:13:27,519 --> 00:13:33,920
which contains again 10 um

385
00:13:30,800 --> 00:13:36,639
10 previously samples from those 6000

386
00:13:33,920 --> 00:13:37,519
classes as we can see on the figure on

387
00:13:36,639 --> 00:13:40,720
the right

388
00:13:37,519 --> 00:13:43,440
the blue line is for a a version of the

389
00:13:40,720 --> 00:13:46,000
experiment where we test it with

390
00:13:43,440 --> 00:13:47,600
just 500 classes and we can see that

391
00:13:46,000 --> 00:13:48,720
after three guesses the adversary

392
00:13:47,600 --> 00:13:50,800
reaches

393
00:13:48,720 --> 00:13:51,839
an accuracy that is higher than 90

394
00:13:50,800 --> 00:13:53,040
percent

395
00:13:51,839 --> 00:13:54,880
now three guesses means that the

396
00:13:53,040 --> 00:13:57,120
adversary is allowed to guess three

397
00:13:54,880 --> 00:13:59,680
potential classes instead of just one

398
00:13:57,120 --> 00:14:02,320
i've removed we will move to uh the

399
00:13:59,680 --> 00:14:04,479
larger version of the experiment where

400
00:14:02,320 --> 00:14:06,399
the samples were that the adversary

401
00:14:04,480 --> 00:14:08,560
would see were coming from all

402
00:14:06,399 --> 00:14:10,639
six thousand possible classes uh the

403
00:14:08,560 --> 00:14:13,599
acres of course decreases but

404
00:14:10,639 --> 00:14:15,600
still the adversary if the adversary is

405
00:14:13,600 --> 00:14:18,880
allowed to guess

406
00:14:15,600 --> 00:14:19,760
up to 10 classes then the accuracy rate

407
00:14:18,880 --> 00:14:24,639
is

408
00:14:19,760 --> 00:14:26,240
quickly up above 80

409
00:14:24,639 --> 00:14:28,240
now moving on to experiment number two

410
00:14:26,240 --> 00:14:30,000
this is a transfer learning experiment

411
00:14:28,240 --> 00:14:31,839
uh we kept the same model from

412
00:14:30,000 --> 00:14:32,800
experiment number one that was trained

413
00:14:31,839 --> 00:14:36,959
on

414
00:14:32,800 --> 00:14:39,680
the original 600 classes subsidy

415
00:14:36,959 --> 00:14:40,560
90 samples per class and but it was

416
00:14:39,680 --> 00:14:43,519
tested on

417
00:14:40,560 --> 00:14:44,079
subsidy which contained samples 10

418
00:14:43,519 --> 00:14:47,600
samples

419
00:14:44,079 --> 00:14:50,079
from 13 000 different classes that were

420
00:14:47,600 --> 00:14:53,519
previously unseen for the model

421
00:14:50,079 --> 00:14:56,638
um to for the reference set we used

422
00:14:53,519 --> 00:14:59,920
subset c so subset c contains

423
00:14:56,639 --> 00:15:01,440
contains uh 90 samples from again 13 000

424
00:14:59,920 --> 00:15:02,479
classes that the model has never seen

425
00:15:01,440 --> 00:15:04,399
before

426
00:15:02,480 --> 00:15:05,839
and we classified subsidy and said

427
00:15:04,399 --> 00:15:07,360
earlier as we see

428
00:15:05,839 --> 00:15:09,440
as we can see on the figure on the right

429
00:15:07,360 --> 00:15:11,920
the accuracy didn't change that much

430
00:15:09,440 --> 00:15:13,040
so if we go on the 500 classes version

431
00:15:11,920 --> 00:15:15,199
of the experiment

432
00:15:13,040 --> 00:15:17,120
again after three guesses you have

433
00:15:15,199 --> 00:15:20,479
almost 90 percent accuracy

434
00:15:17,120 --> 00:15:21,440
moving on to the 6000 classes versus the

435
00:15:20,480 --> 00:15:24,560
experiment

436
00:15:21,440 --> 00:15:26,560
again with 10 guesses you reach

437
00:15:24,560 --> 00:15:27,680
a little bit above 80 accuracy which is

438
00:15:26,560 --> 00:15:30,800
fairly good

439
00:15:27,680 --> 00:15:33,120
and for the 13 000 um

440
00:15:30,800 --> 00:15:35,680
classes version of the experiment we

441
00:15:33,120 --> 00:15:38,240
still have a very good accuracy of

442
00:15:35,680 --> 00:15:40,239
above 80 percent if the advisor is

443
00:15:38,240 --> 00:15:43,920
allowed to guess

444
00:15:40,240 --> 00:15:45,360
20 potential labels for each sample

445
00:15:43,920 --> 00:15:47,759
to put this into perspective the

446
00:15:45,360 --> 00:15:48,399
adversary is allowed to guess 20 labels

447
00:15:47,759 --> 00:15:52,000
out of the

448
00:15:48,399 --> 00:15:53,600
13 000 possible moving on to the

449
00:15:52,000 --> 00:15:54,480
takeaways machine learning is advancing

450
00:15:53,600 --> 00:15:56,560
in a

451
00:15:54,480 --> 00:15:58,880
very fast pace and this is lingering

452
00:15:56,560 --> 00:16:02,079
advancements and changes in other fields

453
00:15:58,880 --> 00:16:03,839
what it is applicable in the case of

454
00:16:02,079 --> 00:16:07,199
fingerprinting this allowed us

455
00:16:03,839 --> 00:16:10,639
to achieve very high accuracy in

456
00:16:07,199 --> 00:16:13,359
our already existing adversarial models

457
00:16:10,639 --> 00:16:16,160
but also gave us the latitude to

458
00:16:13,360 --> 00:16:18,320
envision new more realistic adversaries

459
00:16:16,160 --> 00:16:19,519
uh however an unfortunate effect is that

460
00:16:18,320 --> 00:16:22,079
because

461
00:16:19,519 --> 00:16:22,959
this kind of models are more easily

462
00:16:22,079 --> 00:16:25,479
applicable

463
00:16:22,959 --> 00:16:27,758
for attacks um there is a

464
00:16:25,480 --> 00:16:29,360
disproportionate

465
00:16:27,759 --> 00:16:31,120
number of papers on fingerprinting

466
00:16:29,360 --> 00:16:33,440
attacks and only fewer

467
00:16:31,120 --> 00:16:35,279
fewer paper and countermeasures so maybe

468
00:16:33,440 --> 00:16:36,880
this is something we need to put more

469
00:16:35,279 --> 00:16:40,320
effort on

470
00:16:36,880 --> 00:16:40,320
thank you very much and we're open to

471
00:16:47,959 --> 00:16:50,959
questions

