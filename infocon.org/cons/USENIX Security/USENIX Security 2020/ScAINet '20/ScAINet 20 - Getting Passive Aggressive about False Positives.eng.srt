1
00:00:07,440 --> 00:00:11,440
all right

2
00:00:08,800 --> 00:00:13,519
hey everyone my name is bobby fyler i'm

3
00:00:11,440 --> 00:00:14,959
a data scientist at elastic working on

4
00:00:13,519 --> 00:00:18,080
the security product

5
00:00:14,960 --> 00:00:20,800
there i'm joined by ed rath and his uh

6
00:00:18,080 --> 00:00:22,479
myriad of organizations he represents

7
00:00:20,800 --> 00:00:24,000
and we're here to talk to you today

8
00:00:22,480 --> 00:00:26,720
about using

9
00:00:24,000 --> 00:00:31,840
passive aggressive methods to combat uh

10
00:00:26,720 --> 00:00:31,840
false positives in local environments

11
00:00:32,479 --> 00:00:36,959
now before we kick off i kind of wanted

12
00:00:34,640 --> 00:00:39,360
to provide a real world scenario

13
00:00:36,960 --> 00:00:40,640
or an anecdote kind of from my

14
00:00:39,360 --> 00:00:42,960
professional life

15
00:00:40,640 --> 00:00:44,800
building a macro malware classifier and

16
00:00:42,960 --> 00:00:46,559
it's really the impetus behind why we

17
00:00:44,800 --> 00:00:47,919
we kicked off this research to begin

18
00:00:46,559 --> 00:00:50,640
with

19
00:00:47,920 --> 00:00:51,600
for those of you who don't know macros

20
00:00:50,640 --> 00:00:53,840
are

21
00:00:51,600 --> 00:00:56,160
pretty pervasive across enterprises for

22
00:00:53,840 --> 00:00:57,039
task automation within microsoft office

23
00:00:56,160 --> 00:01:00,398
documents

24
00:00:57,039 --> 00:01:02,960
they allow you to build forms or

25
00:01:00,399 --> 00:01:04,320
you know buttons and and calculations

26
00:01:02,960 --> 00:01:06,720
and things like that

27
00:01:04,319 --> 00:01:08,880
and they're used quite a bit but they're

28
00:01:06,720 --> 00:01:12,000
also used quite a bit by malware authors

29
00:01:08,880 --> 00:01:14,640
to arbitrarily execute

30
00:01:12,000 --> 00:01:16,640
malicious code and it's become a pretty

31
00:01:14,640 --> 00:01:18,560
uh powerful attack vector for things

32
00:01:16,640 --> 00:01:21,759
like phishing attacks

33
00:01:18,560 --> 00:01:24,000
now the security industry would normally

34
00:01:21,759 --> 00:01:25,040
or historically use like hash based

35
00:01:24,000 --> 00:01:28,960
approaches for

36
00:01:25,040 --> 00:01:30,560
detection prevention or reduction of fps

37
00:01:28,960 --> 00:01:32,079
but it's really really hard in this

38
00:01:30,560 --> 00:01:33,840
environment because

39
00:01:32,079 --> 00:01:35,839
these hash based approaches don't

40
00:01:33,840 --> 00:01:38,880
generalize because the minute you

41
00:01:35,840 --> 00:01:39,439
edit a document again you change that

42
00:01:38,880 --> 00:01:42,000
hash

43
00:01:39,439 --> 00:01:44,240
and that creates quite a a problem when

44
00:01:42,000 --> 00:01:46,960
trying to detect things more broadly

45
00:01:44,240 --> 00:01:47,920
um which is where machine learning

46
00:01:46,960 --> 00:01:51,439
represents

47
00:01:47,920 --> 00:01:53,759
uh probably the best opportunity to

48
00:01:51,439 --> 00:01:54,720
generalize and start to detect these

49
00:01:53,759 --> 00:01:57,920
unknown threats

50
00:01:54,720 --> 00:01:59,360
at scale so what we did is we

51
00:01:57,920 --> 00:02:01,759
we applied you know your kind of

52
00:01:59,360 --> 00:02:03,920
traditional supervised learning approach

53
00:02:01,759 --> 00:02:05,360
to getting this model into our security

54
00:02:03,920 --> 00:02:07,200
products where we

55
00:02:05,360 --> 00:02:08,959
collected a bunch of data from a variety

56
00:02:07,200 --> 00:02:10,479
of sources to get a representative

57
00:02:08,959 --> 00:02:13,040
sample

58
00:02:10,479 --> 00:02:14,640
we employed domain experts in the form

59
00:02:13,040 --> 00:02:16,480
of reverse engineers

60
00:02:14,640 --> 00:02:18,720
threat researchers to help build out

61
00:02:16,480 --> 00:02:20,560
features for future extraction

62
00:02:18,720 --> 00:02:22,560
we trained a model we internally

63
00:02:20,560 --> 00:02:23,800
validated it which is where you get the

64
00:02:22,560 --> 00:02:27,680
uh the marketing

65
00:02:23,800 --> 00:02:30,800
99.9 effective and then we

66
00:02:27,680 --> 00:02:33,440
pushed it out to production now

67
00:02:30,800 --> 00:02:34,400
the question becomes when those start

68
00:02:33,440 --> 00:02:37,519
getting used

69
00:02:34,400 --> 00:02:40,080
by users and you come across

70
00:02:37,519 --> 00:02:40,959
fp bursts and things like that how do

71
00:02:40,080 --> 00:02:42,640
you improve

72
00:02:40,959 --> 00:02:44,959
your model after the first time it's

73
00:02:42,640 --> 00:02:46,879
gone out you can

74
00:02:44,959 --> 00:02:48,480
collect more data that's certainly an

75
00:02:46,879 --> 00:02:51,840
option you can

76
00:02:48,480 --> 00:02:53,200
take time and uh and money to extract

77
00:02:51,840 --> 00:02:55,920
new novel features

78
00:02:53,200 --> 00:02:57,518
that attempt to you know identify new

79
00:02:55,920 --> 00:02:59,839
families or subclasses

80
00:02:57,519 --> 00:03:03,280
of the problem you're tackling you can

81
00:02:59,840 --> 00:03:06,800
retrain a model more often

82
00:03:03,280 --> 00:03:07,519
and or you can wait for a bunch of fps

83
00:03:06,800 --> 00:03:09,519
to roll in

84
00:03:07,519 --> 00:03:11,760
and do a big update then now all of

85
00:03:09,519 --> 00:03:15,920
these take a fair amount of time

86
00:03:11,760 --> 00:03:17,599
uh energy and dollars to accomplish

87
00:03:15,920 --> 00:03:19,359
and at its core it highlights the the

88
00:03:17,599 --> 00:03:20,399
biggest problem with a lot of security

89
00:03:19,360 --> 00:03:22,959
products these days

90
00:03:20,400 --> 00:03:24,799
it's that false positives are are the

91
00:03:22,959 --> 00:03:25,680
biggest concern it's the biggest concern

92
00:03:24,799 --> 00:03:27,519
for me

93
00:03:25,680 --> 00:03:30,879
uh it's certainly the biggest concern

94
00:03:27,519 --> 00:03:33,360
for users who have to deal with uh

95
00:03:30,879 --> 00:03:34,879
kind of alert deluge and these bursts of

96
00:03:33,360 --> 00:03:38,239
fps

97
00:03:34,879 --> 00:03:40,560
and we as a as an industry

98
00:03:38,239 --> 00:03:42,480
have a couple of big challenges to deal

99
00:03:40,560 --> 00:03:44,959
with particularly with ml

100
00:03:42,480 --> 00:03:47,280
and it's once these models get pushed

101
00:03:44,959 --> 00:03:50,239
out to production

102
00:03:47,280 --> 00:03:50,560
how quickly do they start to go bad um

103
00:03:50,239 --> 00:03:53,519
or

104
00:03:50,560 --> 00:03:54,480
decay and once you once you understand

105
00:03:53,519 --> 00:03:57,040
when they go bad

106
00:03:54,480 --> 00:03:58,238
can you can you figure out why um what

107
00:03:57,040 --> 00:04:01,200
causes

108
00:03:58,239 --> 00:04:02,239
models to start to spoil or or cause

109
00:04:01,200 --> 00:04:04,798
these big bursts

110
00:04:02,239 --> 00:04:06,799
of false positives is it like a software

111
00:04:04,799 --> 00:04:08,879
update or something like that

112
00:04:06,799 --> 00:04:10,000
firefox or chrome or a piece of

113
00:04:08,879 --> 00:04:12,239
enterprise software

114
00:04:10,000 --> 00:04:14,400
is it an operating system update like a

115
00:04:12,239 --> 00:04:16,399
patch tuesday

116
00:04:14,400 --> 00:04:18,399
and once you understand that and start

117
00:04:16,399 --> 00:04:20,560
to see that you you see a larger

118
00:04:18,399 --> 00:04:21,839
issue kind of at hand and it's this idea

119
00:04:20,560 --> 00:04:24,000
of

120
00:04:21,839 --> 00:04:26,400
data scientists and machine learning

121
00:04:24,000 --> 00:04:29,520
researchers building these global models

122
00:04:26,400 --> 00:04:30,159
trained on a representative distribution

123
00:04:29,520 --> 00:04:32,240
of data

124
00:04:30,160 --> 00:04:34,160
of what we expect local environments to

125
00:04:32,240 --> 00:04:36,000
look like versus the reality

126
00:04:34,160 --> 00:04:37,600
which is local environments are

127
00:04:36,000 --> 00:04:39,520
extremely noisy

128
00:04:37,600 --> 00:04:41,280
and there's custom in-house software and

129
00:04:39,520 --> 00:04:43,440
proprietary data everywhere

130
00:04:41,280 --> 00:04:45,840
that we will never see anything close to

131
00:04:43,440 --> 00:04:48,639
that kind of in our training data set

132
00:04:45,840 --> 00:04:50,799
and it does things that look inherently

133
00:04:48,639 --> 00:04:53,440
malicious

134
00:04:50,800 --> 00:04:54,080
so we as an industry really have two

135
00:04:53,440 --> 00:04:57,280
options

136
00:04:54,080 --> 00:05:00,240
at our disposal right now and option one

137
00:04:57,280 --> 00:05:02,080
uh is putting the power back in the

138
00:05:00,240 --> 00:05:04,800
user's hands in the form of

139
00:05:02,080 --> 00:05:05,919
allowing them to allow list or deny list

140
00:05:04,800 --> 00:05:08,160
something

141
00:05:05,919 --> 00:05:10,560
and this works they can use like a file

142
00:05:08,160 --> 00:05:12,720
hash or a certificate signer

143
00:05:10,560 --> 00:05:14,560
to basically play whack-a-mole it's this

144
00:05:12,720 --> 00:05:15,120
sort of one-to-one relationship where a

145
00:05:14,560 --> 00:05:18,160
hash

146
00:05:15,120 --> 00:05:19,919
equals a file or sample but this doesn't

147
00:05:18,160 --> 00:05:21,919
really generalize well

148
00:05:19,919 --> 00:05:23,198
and can be very time consuming and very

149
00:05:21,919 --> 00:05:26,560
frustrating from

150
00:05:23,199 --> 00:05:29,280
a workflow standpoint now option two

151
00:05:26,560 --> 00:05:30,320
which is uh called just like give us all

152
00:05:29,280 --> 00:05:32,239
your data

153
00:05:30,320 --> 00:05:34,320
is is certainly the better of the two

154
00:05:32,240 --> 00:05:36,720
options because this could yield massive

155
00:05:34,320 --> 00:05:38,639
improvements over time in a relatively

156
00:05:36,720 --> 00:05:40,000
short amount of time

157
00:05:38,639 --> 00:05:42,800
but it comes with some pretty big

158
00:05:40,000 --> 00:05:44,320
concerns in the form of privacy

159
00:05:42,800 --> 00:05:46,400
you know we as data scientists need to

160
00:05:44,320 --> 00:05:49,039
be very careful about the data that we

161
00:05:46,400 --> 00:05:49,919
uh we use or leverage for things like

162
00:05:49,039 --> 00:05:52,880
gdpr

163
00:05:49,919 --> 00:05:54,240
or other proprietary data we also have

164
00:05:52,880 --> 00:05:56,159
to

165
00:05:54,240 --> 00:05:57,600
you know look out for end users to make

166
00:05:56,160 --> 00:06:00,800
sure that we're not hindering

167
00:05:57,600 --> 00:06:02,560
uh performance um you know

168
00:06:00,800 --> 00:06:04,639
taking too much bandwidth shipping data

169
00:06:02,560 --> 00:06:06,800
back and forth across the wire

170
00:06:04,639 --> 00:06:09,199
degrading performance on the endpoint by

171
00:06:06,800 --> 00:06:10,639
packaging up these files or samples in

172
00:06:09,199 --> 00:06:12,400
in order to send them back

173
00:06:10,639 --> 00:06:14,880
these are these are things that we we

174
00:06:12,400 --> 00:06:17,359
really need to be mindful of

175
00:06:14,880 --> 00:06:18,800
and that's really what ed and i were

176
00:06:17,360 --> 00:06:21,120
talking about a few months ago

177
00:06:18,800 --> 00:06:21,919
when we came up with this idea we wanted

178
00:06:21,120 --> 00:06:24,319
to

179
00:06:21,919 --> 00:06:25,599
explore alternatives to kind of the more

180
00:06:24,319 --> 00:06:28,960
traditional

181
00:06:25,600 --> 00:06:30,479
fp triage approaches and we started

182
00:06:28,960 --> 00:06:34,560
hovering around this idea

183
00:06:30,479 --> 00:06:36,880
of leveraging the security worker as the

184
00:06:34,560 --> 00:06:38,960
as the domain expert but the domain

185
00:06:36,880 --> 00:06:41,199
being their local enterprise

186
00:06:38,960 --> 00:06:43,359
their ability to know what belongs there

187
00:06:41,199 --> 00:06:46,960
and what doesn't belong there

188
00:06:43,360 --> 00:06:50,000
and take that knowledge and turn it into

189
00:06:46,960 --> 00:06:53,120
kind of an iterative human in their loop

190
00:06:50,000 --> 00:06:55,280
active learning exercise where we give

191
00:06:53,120 --> 00:06:57,440
them the tools

192
00:06:55,280 --> 00:06:59,039
that data scientists bring to the table

193
00:06:57,440 --> 00:07:00,240
in the form of feature extraction and

194
00:06:59,039 --> 00:07:03,520
model training

195
00:07:00,240 --> 00:07:05,199
and allow them to just capture the files

196
00:07:03,520 --> 00:07:05,919
that they're interested in not alerting

197
00:07:05,199 --> 00:07:09,599
on anymore

198
00:07:05,919 --> 00:07:11,919
and updating the model locally um

199
00:07:09,599 --> 00:07:14,560
and i'm hopeful that this kind of

200
00:07:11,919 --> 00:07:17,758
tailoring or customization would lead to

201
00:07:14,560 --> 00:07:19,520
a better overall security experience for

202
00:07:17,759 --> 00:07:21,440
the end user while maintaining the

203
00:07:19,520 --> 00:07:24,880
privacy of enterprise data that data

204
00:07:21,440 --> 00:07:26,560
never has to leave their environment

205
00:07:24,880 --> 00:07:28,400
so how did we do that i'm going to have

206
00:07:26,560 --> 00:07:31,840
ed take over now and he's going to walk

207
00:07:28,400 --> 00:07:31,840
us through it

208
00:07:32,080 --> 00:07:36,080
so in order to fix these problems to fix

209
00:07:34,800 --> 00:07:37,919
these errors

210
00:07:36,080 --> 00:07:39,199
we think we're going to need a linear

211
00:07:37,919 --> 00:07:42,318
model

212
00:07:39,199 --> 00:07:45,840
and part of this is because for tree

213
00:07:42,319 --> 00:07:45,840
based models which is what the current

214
00:07:46,319 --> 00:07:54,000
method uses there's not a lot of good

215
00:07:50,479 --> 00:07:55,599
ways to update a tree-based model

216
00:07:54,000 --> 00:07:57,440
there are approaches to it but they

217
00:07:55,599 --> 00:08:00,719
require multiple errors

218
00:07:57,440 --> 00:08:02,960
in order to cause the model to really

219
00:08:00,720 --> 00:08:06,000
change its decision surface

220
00:08:02,960 --> 00:08:06,400
and we need to be able to correct an

221
00:08:06,000 --> 00:08:08,800
error

222
00:08:06,400 --> 00:08:10,960
and immediately know afterwards that it

223
00:08:08,800 --> 00:08:12,400
is no longer an error

224
00:08:10,960 --> 00:08:14,799
and that's much easier to do with a

225
00:08:12,400 --> 00:08:19,599
linear model where we can just

226
00:08:14,800 --> 00:08:19,599
know that the hyperplane is now adapted

227
00:08:19,680 --> 00:08:22,960
and we also want to fix the likelihood

228
00:08:21,440 --> 00:08:25,440
of future

229
00:08:22,960 --> 00:08:27,120
false positives or make those less

230
00:08:25,440 --> 00:08:29,120
likely

231
00:08:27,120 --> 00:08:31,759
and that's also easier with a linear

232
00:08:29,120 --> 00:08:33,679
model we could force a decision tree to

233
00:08:31,759 --> 00:08:35,839
carve off a little tiny hole in the

234
00:08:33,679 --> 00:08:40,000
world for just the false positive

235
00:08:35,839 --> 00:08:44,080
but that reduces back to basically

236
00:08:40,000 --> 00:08:46,480
a blacklist which is not what we want to

237
00:08:44,080 --> 00:08:46,480
be doing

238
00:08:47,279 --> 00:08:53,439
uh one approach might be to

239
00:08:51,279 --> 00:08:55,360
take the false positives and create sort

240
00:08:53,440 --> 00:08:58,640
of a radius around them

241
00:08:55,360 --> 00:09:02,880
where you decide this is what this

242
00:08:58,640 --> 00:09:05,760
this area now becomes a benign area

243
00:09:02,880 --> 00:09:07,519
rather than malicious but then it

244
00:09:05,760 --> 00:09:09,360
becomes confusing on how do we pick this

245
00:09:07,519 --> 00:09:11,040
radius

246
00:09:09,360 --> 00:09:12,640
because we don't want to rely on the

247
00:09:11,040 --> 00:09:14,640
local information

248
00:09:12,640 --> 00:09:15,920
because that local information according

249
00:09:14,640 --> 00:09:18,800
to the model was already

250
00:09:15,920 --> 00:09:21,279
somewhat wrong by the nature of having a

251
00:09:18,800 --> 00:09:22,959
false positive

252
00:09:21,279 --> 00:09:24,480
and this could also map to one-shot

253
00:09:22,959 --> 00:09:26,719
learning

254
00:09:24,480 --> 00:09:27,839
which we'll test as an alternative

255
00:09:26,720 --> 00:09:30,800
approach

256
00:09:27,839 --> 00:09:32,720
but we find it doesn't work as well so

257
00:09:30,800 --> 00:09:35,760
what we ended up using

258
00:09:32,720 --> 00:09:36,959
is what's called the passive aggressive

259
00:09:35,760 --> 00:09:40,319
algorithm

260
00:09:36,959 --> 00:09:44,880
which comes from online learning

261
00:09:40,320 --> 00:09:48,080
and the goal is that we want to make

262
00:09:44,880 --> 00:09:51,519
a change in the hyperplane to

263
00:09:48,080 --> 00:09:54,640
correct a current error and

264
00:09:51,519 --> 00:09:58,080
the naive passive aggressive algorithm

265
00:09:54,640 --> 00:10:00,720
attempts to fully correct this error

266
00:09:58,080 --> 00:10:04,240
while making the minimum possible change

267
00:10:00,720 --> 00:10:05,920
to the norm of the weight vector this is

268
00:10:04,240 --> 00:10:07,440
sort of a good mental trade-off like we

269
00:10:05,920 --> 00:10:10,399
want to fix it

270
00:10:07,440 --> 00:10:12,240
but fix it by changing as little as

271
00:10:10,399 --> 00:10:14,880
possible

272
00:10:12,240 --> 00:10:17,120
now normally you wouldn't ever actually

273
00:10:14,880 --> 00:10:18,720
do this

274
00:10:17,120 --> 00:10:20,560
and the original passive aggressive

275
00:10:18,720 --> 00:10:21,839
paper introduces a regularization

276
00:10:20,560 --> 00:10:24,640
penalty c

277
00:10:21,839 --> 00:10:25,519
to prevent it from fully correcting on

278
00:10:24,640 --> 00:10:27,279
errors

279
00:10:25,519 --> 00:10:29,279
because if you get one mislabeled data

280
00:10:27,279 --> 00:10:31,839
point and you

281
00:10:29,279 --> 00:10:33,920
fully correct it you could end up

282
00:10:31,839 --> 00:10:36,320
destroying the model

283
00:10:33,920 --> 00:10:37,760
but in this case we need to fully

284
00:10:36,320 --> 00:10:39,839
correct after one update

285
00:10:37,760 --> 00:10:42,480
so we're going to do something weird and

286
00:10:39,839 --> 00:10:44,240
use the unregulated version of the model

287
00:10:42,480 --> 00:10:45,839
now let's go through a toy example to

288
00:10:44,240 --> 00:10:46,880
show why so you have some false

289
00:10:45,839 --> 00:10:50,000
positives

290
00:10:46,880 --> 00:10:50,800
a few come in and you want to correct

291
00:10:50,000 --> 00:10:53,279
them

292
00:10:50,800 --> 00:10:55,279
so if you do the naive approach of just

293
00:10:53,279 --> 00:10:58,079
stochastic gradient descent

294
00:10:55,279 --> 00:10:58,640
you end up with some move type of plane

295
00:10:58,079 --> 00:11:01,680
which

296
00:10:58,640 --> 00:11:04,800
induces many new um

297
00:11:01,680 --> 00:11:06,479
false negatives which is not as good so

298
00:11:04,800 --> 00:11:08,959
what we're going to do instead is the

299
00:11:06,480 --> 00:11:09,920
the passive aggressive update where the

300
00:11:08,959 --> 00:11:12,319
new vector

301
00:11:09,920 --> 00:11:14,079
or the new weight vector is the minimum

302
00:11:12,320 --> 00:11:16,240
norm change

303
00:11:14,079 --> 00:11:17,680
and then this gets us a better overall

304
00:11:16,240 --> 00:11:20,320
solution

305
00:11:17,680 --> 00:11:22,000
that corrects the errors with hopefully

306
00:11:20,320 --> 00:11:25,279
minimal

307
00:11:22,000 --> 00:11:27,120
new false negatives

308
00:11:25,279 --> 00:11:29,120
and now we have a model that's been

309
00:11:27,120 --> 00:11:32,079
adapted to the local user

310
00:11:29,120 --> 00:11:34,079
so that's basically the strategy and our

311
00:11:32,079 --> 00:11:37,359
initial solution

312
00:11:34,079 --> 00:11:41,839
um some inspiration for it we

313
00:11:37,360 --> 00:11:45,680
use malkov so malcolm will convert the

314
00:11:41,839 --> 00:11:48,480
macro code into a

315
00:11:45,680 --> 00:11:49,599
fixed dimensional feature space where we

316
00:11:48,480 --> 00:11:51,839
can then use

317
00:11:49,600 --> 00:11:54,880
the linear passive passive aggressive

318
00:11:51,839 --> 00:11:54,880
approach on top of that

319
00:11:54,959 --> 00:11:58,638
when an error occurs we update the

320
00:11:57,040 --> 00:12:01,680
passive aggressive model on top

321
00:11:58,639 --> 00:12:03,040
not malcolm but then we have this

322
00:12:01,680 --> 00:12:06,719
question of how do we

323
00:12:03,040 --> 00:12:09,360
prevent users from creating false

324
00:12:06,720 --> 00:12:11,519
false positive updates so they say it's

325
00:12:09,360 --> 00:12:12,880
a false positive but they were wrong

326
00:12:11,519 --> 00:12:14,720
and that could potentially destroy the

327
00:12:12,880 --> 00:12:18,160
model and the hope

328
00:12:14,720 --> 00:12:21,360
is that all of the difficult things

329
00:12:18,160 --> 00:12:24,160
are on the border and so

330
00:12:21,360 --> 00:12:24,800
wouldn't need large changes to the model

331
00:12:24,160 --> 00:12:26,719
to

332
00:12:24,800 --> 00:12:28,000
fix them but we still need to address

333
00:12:26,720 --> 00:12:30,800
this scenario

334
00:12:28,000 --> 00:12:31,519
and what you see here the black x's mark

335
00:12:30,800 --> 00:12:33,920
58

336
00:12:31,519 --> 00:12:34,880
particularly difficult false positive

337
00:12:33,920 --> 00:12:38,479
files

338
00:12:34,880 --> 00:12:41,519
from production real live data

339
00:12:38,480 --> 00:12:44,079
so in order to solve this second problem

340
00:12:41,519 --> 00:12:45,360
what we do is we use k means to estimate

341
00:12:44,079 --> 00:12:48,399
the impact

342
00:12:45,360 --> 00:12:49,519
to a model's accuracy if the model's

343
00:12:48,399 --> 00:12:52,639
been altered

344
00:12:49,519 --> 00:12:58,320
so we can use the weighted centroids and

345
00:12:52,639 --> 00:13:00,800
their label purity to estimate the auc

346
00:12:58,320 --> 00:13:02,480
this is actually a pretty good estimate

347
00:13:00,800 --> 00:13:04,319
and it's not hard to compute

348
00:13:02,480 --> 00:13:06,320
and that way we can share these cluster

349
00:13:04,320 --> 00:13:07,360
centroids rather than sharing the

350
00:13:06,320 --> 00:13:10,639
original data

351
00:13:07,360 --> 00:13:15,519
which also helps maintain privacy

352
00:13:10,639 --> 00:13:15,519
and avoid sharing proprietary data

353
00:13:16,079 --> 00:13:24,319
for evaluation we'll have 650

354
00:13:20,000 --> 00:13:28,560
0009 files and almost 5 450 000

355
00:13:24,320 --> 00:13:31,600
malicious samples and a special held out

356
00:13:28,560 --> 00:13:34,399
hard false positive set of 58 files that

357
00:13:31,600 --> 00:13:39,120
i mentioned previously

358
00:13:34,399 --> 00:13:39,120
the baseline results is that

359
00:13:39,199 --> 00:13:43,279
the gradient boosted decision tree is

360
00:13:42,160 --> 00:13:45,680
the most accurate

361
00:13:43,279 --> 00:13:46,959
on the data which is not that surprising

362
00:13:45,680 --> 00:13:49,839
it's on the

363
00:13:46,959 --> 00:13:51,839
engineered features produced by domain

364
00:13:49,839 --> 00:13:54,160
experts

365
00:13:51,839 --> 00:13:58,079
malcom with passive aggressive or

366
00:13:54,160 --> 00:13:59,760
stochastic grading descent on top

367
00:13:58,079 --> 00:14:01,120
doesn't do quite as well but does

368
00:13:59,760 --> 00:14:03,519
decently

369
00:14:01,120 --> 00:14:05,440
in particular it's got okay true

370
00:14:03,519 --> 00:14:07,920
positive rates

371
00:14:05,440 --> 00:14:10,160
at a fixed 0.1 percent false positive

372
00:14:07,920 --> 00:14:13,199
rate

373
00:14:10,160 --> 00:14:13,920
the malecon plus prototype is testing a

374
00:14:13,199 --> 00:14:17,279
one-shot

375
00:14:13,920 --> 00:14:19,519
algorithm approach to dealing with

376
00:14:17,279 --> 00:14:21,360
the false positives and you can see it

377
00:14:19,519 --> 00:14:24,079
actually learns degenerate solutions and

378
00:14:21,360 --> 00:14:27,360
has really poor performance

379
00:14:24,079 --> 00:14:27,359
so that makes it a no go

380
00:14:27,839 --> 00:14:31,279
we also attempted the passive aggressive

381
00:14:29,920 --> 00:14:33,519
algorithm on the domain

382
00:14:31,279 --> 00:14:35,439
knowledge features because we want to

383
00:14:33,519 --> 00:14:36,959
actually have a reason for using male

384
00:14:35,440 --> 00:14:39,279
conf we don't want to use it just to say

385
00:14:36,959 --> 00:14:41,119
that we're doing deep learning

386
00:14:39,279 --> 00:14:43,519
and so training the passive aggressive

387
00:14:41,120 --> 00:14:45,839
algorithm on the original features

388
00:14:43,519 --> 00:14:47,600
at first glance seems to have comparable

389
00:14:45,839 --> 00:14:49,680
accuracy in auc

390
00:14:47,600 --> 00:14:51,040
but when you adjust the decision

391
00:14:49,680 --> 00:14:53,920
threshold to get a

392
00:14:51,040 --> 00:14:58,079
false positive rate of 0.1 percent the

393
00:14:53,920 --> 00:15:00,079
true positive rate drops to 2 percent

394
00:14:58,079 --> 00:15:03,439
and the kernelized version of passive

395
00:15:00,079 --> 00:15:06,160
aggressive really doesn't do much better

396
00:15:03,440 --> 00:15:09,360
so these three options sort of are

397
00:15:06,160 --> 00:15:09,360
immediately off the table

398
00:15:10,639 --> 00:15:14,720
so the hard false positive results so

399
00:15:13,760 --> 00:15:16,319
that that was on the

400
00:15:14,720 --> 00:15:18,480
training test set now we're looking at

401
00:15:16,320 --> 00:15:20,720
the results in the hard falls positives

402
00:15:18,480 --> 00:15:21,519
so gradient boost decision tree gets all

403
00:15:20,720 --> 00:15:24,000
of them wrong

404
00:15:21,519 --> 00:15:25,440
that's the fixed column here passive

405
00:15:24,000 --> 00:15:28,800
aggressive approach gets

406
00:15:25,440 --> 00:15:31,040
60 of them wrong uh with sgd it actually

407
00:15:28,800 --> 00:15:32,240
seemed to learn a decent solution and

408
00:15:31,040 --> 00:15:35,360
started off with only

409
00:15:32,240 --> 00:15:36,959
38 percent of them wrong

410
00:15:35,360 --> 00:15:38,880
but what we're going to test is an

411
00:15:36,959 --> 00:15:42,399
adaptive scenario

412
00:15:38,880 --> 00:15:43,600
so 200 random trials of a false positive

413
00:15:42,399 --> 00:15:45,440
comes in

414
00:15:43,600 --> 00:15:46,720
we update the model if it's making an

415
00:15:45,440 --> 00:15:49,920
error

416
00:15:46,720 --> 00:15:52,000
and then the next one comes in

417
00:15:49,920 --> 00:15:54,479
and see is it still making an error on

418
00:15:52,000 --> 00:15:57,680
the next false positive

419
00:15:54,480 --> 00:16:00,079
if not if so we need to fix it

420
00:15:57,680 --> 00:16:02,479
if not good we're doing a good job at

421
00:16:00,079 --> 00:16:04,959
reducing future false positives

422
00:16:02,480 --> 00:16:06,560
and in this adaptive scenario the

423
00:16:04,959 --> 00:16:07,518
passive aggressive approach gets our

424
00:16:06,560 --> 00:16:09,599
error rate

425
00:16:07,519 --> 00:16:11,040
on these hard false positives down to

426
00:16:09,600 --> 00:16:15,360
four percent

427
00:16:11,040 --> 00:16:17,120
um whereas sgd barely changes down to 26

428
00:16:15,360 --> 00:16:18,880
and the gradient boosted decision tree

429
00:16:17,120 --> 00:16:20,800
can't be updated at all

430
00:16:18,880 --> 00:16:22,639
and looking here we can see that the

431
00:16:20,800 --> 00:16:24,880
passive aggressive approach

432
00:16:22,639 --> 00:16:27,120
works sometimes with often with as few

433
00:16:24,880 --> 00:16:31,199
as one update

434
00:16:27,120 --> 00:16:31,199
only one false positive had to be fixed

435
00:16:31,440 --> 00:16:34,720
we can also see the impact on global

436
00:16:33,759 --> 00:16:38,079
performance

437
00:16:34,720 --> 00:16:40,639
as we do these false positive updates

438
00:16:38,079 --> 00:16:41,599
we can see that auc is actually pretty

439
00:16:40,639 --> 00:16:44,560
stable

440
00:16:41,600 --> 00:16:45,440
and the estimated impact to auc is near

441
00:16:44,560 --> 00:16:48,319
zero

442
00:16:45,440 --> 00:16:50,639
which is good the true positive rate

443
00:16:48,320 --> 00:16:51,680
does decrease over time as we do more

444
00:16:50,639 --> 00:16:54,959
updates

445
00:16:51,680 --> 00:16:57,599
which is understandable

446
00:16:54,959 --> 00:16:59,920
and the fact that the true positive rate

447
00:16:57,600 --> 00:17:01,759
decreases but auc remains stable

448
00:16:59,920 --> 00:17:03,519
means that we can actually fix this by

449
00:17:01,759 --> 00:17:06,240
just sending the model back

450
00:17:03,519 --> 00:17:09,120
to elastic and recalibrating the

451
00:17:06,240 --> 00:17:10,799
threshold for decisions

452
00:17:09,119 --> 00:17:12,479
and then we get exactly the same

453
00:17:10,799 --> 00:17:15,760
performance as before

454
00:17:12,480 --> 00:17:17,679
but no hard false positives

455
00:17:15,760 --> 00:17:19,119
so though true positive rate does

456
00:17:17,679 --> 00:17:23,760
decrease by

457
00:17:19,119 --> 00:17:23,760
up to 50 percent the uh

458
00:17:24,160 --> 00:17:30,000
false positive rate drops by 23 times

459
00:17:28,160 --> 00:17:32,080
so it's actually still a reasonable

460
00:17:30,000 --> 00:17:34,720
trade-off especially when you have

461
00:17:32,080 --> 00:17:35,600
an issue that is false positives and we

462
00:17:34,720 --> 00:17:39,360
can also see

463
00:17:35,600 --> 00:17:43,280
that by testing our auc

464
00:17:39,360 --> 00:17:45,760
impact estimate on the test data

465
00:17:43,280 --> 00:17:47,039
by randomly swapping the labels to force

466
00:17:45,760 --> 00:17:48,799
errors to occur

467
00:17:47,039 --> 00:17:50,160
that we do actually do a good job of

468
00:17:48,799 --> 00:17:52,559
estimating the auc

469
00:17:50,160 --> 00:17:54,480
impact and we see a linear relationship

470
00:17:52,559 --> 00:17:56,799
between our estimated impact

471
00:17:54,480 --> 00:17:59,679
and the true impact so we can actually

472
00:17:56,799 --> 00:18:03,600
use this to help correct for errors

473
00:17:59,679 --> 00:18:03,600
and now i'll take it back to bobby

474
00:18:05,039 --> 00:18:11,760
what environment we hope that we present

475
00:18:08,240 --> 00:18:13,520
an alternative kind of option

476
00:18:11,760 --> 00:18:14,960
to help start to mitigate some of these

477
00:18:13,520 --> 00:18:17,600
fps locally

478
00:18:14,960 --> 00:18:19,760
without relying on on current more

479
00:18:17,600 --> 00:18:22,480
antiquated techniques

480
00:18:19,760 --> 00:18:24,799
and we also believe that approaches

481
00:18:22,480 --> 00:18:26,840
similar to this passive aggressive

482
00:18:24,799 --> 00:18:28,559
will encourage kind of a safe

483
00:18:26,840 --> 00:18:31,678
customization

484
00:18:28,559 --> 00:18:34,799
and also allow us to start to

485
00:18:31,679 --> 00:18:38,400
establish transparency between the

486
00:18:34,799 --> 00:18:40,400
vendor and the user of this software

487
00:18:38,400 --> 00:18:42,000
to give them an ability to reduce false

488
00:18:40,400 --> 00:18:44,000
positives locally over time

489
00:18:42,000 --> 00:18:46,080
while safeguarding them from doing

490
00:18:44,000 --> 00:18:49,280
anything catastrophic

491
00:18:46,080 --> 00:18:52,480
and yeah that was it um

492
00:18:49,280 --> 00:18:57,840
thanks everybody for watching and uh

493
00:18:52,480 --> 00:18:57,840
yeah take care

494
00:19:03,600 --> 00:19:05,678
you

