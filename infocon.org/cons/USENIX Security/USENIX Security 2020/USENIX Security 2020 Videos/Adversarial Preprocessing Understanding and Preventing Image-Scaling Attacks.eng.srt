1
00:00:08,080 --> 00:00:10,559
hi

2
00:00:08,720 --> 00:00:12,559
my name is evan clearing from tu

3
00:00:10,559 --> 00:00:14,239
brownstrike and my talk today gives a

4
00:00:12,559 --> 00:00:15,200
comprehensive introduction to image

5
00:00:14,240 --> 00:00:17,279
scaling attacks

6
00:00:15,200 --> 00:00:19,840
a novel class of attacks that we need to

7
00:00:17,279 --> 00:00:21,600
consider in machine learning

8
00:00:19,840 --> 00:00:23,439
so if we consider a typical machine

9
00:00:21,600 --> 00:00:25,920
learning pipeline usually we need to

10
00:00:23,439 --> 00:00:27,920
pre-process data for machine learning

11
00:00:25,920 --> 00:00:29,599
in computer vision we cannot pass an

12
00:00:27,920 --> 00:00:30,000
image directly to a learning method for

13
00:00:29,599 --> 00:00:32,159
example

14
00:00:30,000 --> 00:00:34,160
neural network as learning methods

15
00:00:32,159 --> 00:00:36,000
typically expect fixed size inputs

16
00:00:34,160 --> 00:00:38,319
that are even relatively small for

17
00:00:36,000 --> 00:00:40,000
example around 300 pixels

18
00:00:38,320 --> 00:00:41,360
in each dimension for the state of the

19
00:00:40,000 --> 00:00:44,800
art model

20
00:00:41,360 --> 00:00:45,680
inception v3 so we need to scale images

21
00:00:44,800 --> 00:00:47,440
and normally

22
00:00:45,680 --> 00:00:49,200
we would expect that the down skate

23
00:00:47,440 --> 00:00:51,199
image corresponds to the input image

24
00:00:49,200 --> 00:00:53,440
like for the one way sign here

25
00:00:51,199 --> 00:00:54,879
unfortunately the scaling step can

26
00:00:53,440 --> 00:00:56,960
create a completely different

27
00:00:54,879 --> 00:00:58,239
output image which is the result of an

28
00:00:56,960 --> 00:01:00,800
image scaling attack

29
00:00:58,239 --> 00:01:01,519
that i applied here on this example in

30
00:01:00,800 --> 00:01:03,519
this talk

31
00:01:01,520 --> 00:01:05,119
we will analyze such an attack in more

32
00:01:03,520 --> 00:01:07,200
detail

33
00:01:05,119 --> 00:01:08,960
to this end let's shortly consider how

34
00:01:07,200 --> 00:01:11,439
an image scaling attack works

35
00:01:08,960 --> 00:01:13,199
the overall objective is that an image

36
00:01:11,439 --> 00:01:15,119
changes after downscaling

37
00:01:13,200 --> 00:01:17,200
the adversary has two images the

38
00:01:15,119 --> 00:01:17,840
original source image a do not enter a

39
00:01:17,200 --> 00:01:19,840
sign here

40
00:01:17,840 --> 00:01:22,080
and a target image with no parking sign

41
00:01:19,840 --> 00:01:24,240
here now given the target image

42
00:01:22,080 --> 00:01:25,360
the attacker many plates we do not enter

43
00:01:24,240 --> 00:01:28,240
a sign here

44
00:01:25,360 --> 00:01:30,000
such that its modified version before

45
00:01:28,240 --> 00:01:30,960
scaling is still showing the do not

46
00:01:30,000 --> 00:01:33,600
enter a sign

47
00:01:30,960 --> 00:01:35,039
but becomes with no parking sign after

48
00:01:33,600 --> 00:01:37,119
downscaling

49
00:01:35,040 --> 00:01:38,880
this attack is solved as an optimization

50
00:01:37,119 --> 00:01:40,240
problem we seek for a minimal

51
00:01:38,880 --> 00:01:42,320
perturbation delta

52
00:01:40,240 --> 00:01:44,399
that can be added to the source image

53
00:01:42,320 --> 00:01:46,000
such that the downscape result is close

54
00:01:44,399 --> 00:01:48,479
to the target image

55
00:01:46,000 --> 00:01:50,560
in summary the adversary has two goals

56
00:01:48,479 --> 00:01:51,520
the output image must be similar to the

57
00:01:50,560 --> 00:01:53,759
target image

58
00:01:51,520 --> 00:01:56,399
and the modified source image must be

59
00:01:53,759 --> 00:01:58,479
similar to its original version

60
00:01:56,399 --> 00:02:00,320
this attack aims at the pre-processing

61
00:01:58,479 --> 00:02:02,240
step which is at the very beginning of a

62
00:02:00,320 --> 00:02:04,639
typical machinery pipeline

63
00:02:02,240 --> 00:02:06,640
so that the implications for machine

64
00:02:04,640 --> 00:02:09,038
learning are manifold

65
00:02:06,640 --> 00:02:10,720
first an adversary can trigger false

66
00:02:09,038 --> 00:02:12,559
predictions at test time

67
00:02:10,720 --> 00:02:14,560
by creating a downscaled image of

68
00:02:12,560 --> 00:02:16,959
another arbitrary class

69
00:02:14,560 --> 00:02:18,319
in contrast to adversary examples the

70
00:02:16,959 --> 00:02:20,640
input to the network is

71
00:02:18,319 --> 00:02:22,640
an instance of a target class already so

72
00:02:20,640 --> 00:02:24,640
we do not need somehow to exploit weak

73
00:02:22,640 --> 00:02:27,119
spots in the network

74
00:02:24,640 --> 00:02:28,879
an attacker can also use scaling attacks

75
00:02:27,120 --> 00:02:29,920
to hide manipulations in the training

76
00:02:28,879 --> 00:02:32,399
data set

77
00:02:29,920 --> 00:02:33,839
for instance she can insert back doors

78
00:02:32,400 --> 00:02:36,239
that only become visible

79
00:02:33,840 --> 00:02:37,440
after down scaling like here with a

80
00:02:36,239 --> 00:02:39,680
green box

81
00:02:37,440 --> 00:02:41,760
and by using these stickers at test time

82
00:02:39,680 --> 00:02:42,959
we can completely control the network's

83
00:02:41,760 --> 00:02:45,760
behavior

84
00:02:42,959 --> 00:02:47,599
finally scaling attacks are of course

85
00:02:45,760 --> 00:02:48,239
also relevant outside of machine

86
00:02:47,599 --> 00:02:50,399
learning

87
00:02:48,239 --> 00:02:53,440
whenever images are scaled think about

88
00:02:50,400 --> 00:02:55,599
the image preview on websites

89
00:02:53,440 --> 00:02:57,599
we need to keep in mind that the threat

90
00:02:55,599 --> 00:02:58,879
model of scaling attacks is different to

91
00:02:57,599 --> 00:03:01,760
previous attacks

92
00:02:58,879 --> 00:03:03,920
like the generation of adverse examples

93
00:03:01,760 --> 00:03:06,000
the attack is independent of the model

94
00:03:03,920 --> 00:03:07,679
learning method and training data

95
00:03:06,000 --> 00:03:09,440
and we just need to know the parameters

96
00:03:07,680 --> 00:03:11,840
of a scaling operation

97
00:03:09,440 --> 00:03:14,400
standard libraries like tensorflow have

98
00:03:11,840 --> 00:03:17,360
only limited number of scaling options

99
00:03:14,400 --> 00:03:19,120
so these parameters are not so hard to

100
00:03:17,360 --> 00:03:21,680
find out

101
00:03:19,120 --> 00:03:23,840
the previous slides strongly underline

102
00:03:21,680 --> 00:03:25,760
the severe impact of scaling attacks and

103
00:03:23,840 --> 00:03:26,560
therefore the need for a comprehensive

104
00:03:25,760 --> 00:03:29,518
analysis

105
00:03:26,560 --> 00:03:31,280
of these attacks in our work therefore

106
00:03:29,519 --> 00:03:32,000
we analyze the root cause of these

107
00:03:31,280 --> 00:03:34,159
attacks

108
00:03:32,000 --> 00:03:35,440
so we can really understand why and when

109
00:03:34,159 --> 00:03:37,840
the attack works

110
00:03:35,440 --> 00:03:39,359
this allows us to derive defenses that

111
00:03:37,840 --> 00:03:41,599
prevent the attack

112
00:03:39,360 --> 00:03:43,680
and of course we consider an adaptive

113
00:03:41,599 --> 00:03:46,000
attacker who knows about the defense

114
00:03:43,680 --> 00:03:48,560
and we show that it is still not

115
00:03:46,000 --> 00:03:50,720
possible to secure invent the defense

116
00:03:48,560 --> 00:03:52,319
in this talk we will look at the first

117
00:03:50,720 --> 00:03:53,760
two points the root cause and the

118
00:03:52,319 --> 00:03:56,079
defenses

119
00:03:53,760 --> 00:03:58,720
so let's start with a root cause to this

120
00:03:56,080 --> 00:04:00,319
end let's recap how scaling works and

121
00:03:58,720 --> 00:04:02,319
for the sake of simplicity

122
00:04:00,319 --> 00:04:04,560
we consider a one-dimensional signal

123
00:04:02,319 --> 00:04:06,480
here the scaling operation

124
00:04:04,560 --> 00:04:08,400
can be described as a convolution

125
00:04:06,480 --> 00:04:09,040
between the source signal and a kernel

126
00:04:08,400 --> 00:04:11,439
function

127
00:04:09,040 --> 00:04:13,519
similar to the convolutional layer in

128
00:04:11,439 --> 00:04:15,359
convolutional neural networks

129
00:04:13,519 --> 00:04:17,680
for each position in the downscaled

130
00:04:15,360 --> 00:04:19,840
image the kernel combines a set of

131
00:04:17,680 --> 00:04:22,479
pixels from the source image

132
00:04:19,839 --> 00:04:23,198
using a specific weighting in other

133
00:04:22,479 --> 00:04:25,919
words

134
00:04:23,199 --> 00:04:27,440
the kernel is a window that we move over

135
00:04:25,919 --> 00:04:30,240
the source signal

136
00:04:27,440 --> 00:04:32,560
and each pixel within this window is

137
00:04:30,240 --> 00:04:35,040
multiplied by respective weight

138
00:04:32,560 --> 00:04:36,560
as an example let's assume we want to

139
00:04:35,040 --> 00:04:40,720
scale the signal here

140
00:04:36,560 --> 00:04:43,120
on the slide with 9 pixels to 2 pixels

141
00:04:40,720 --> 00:04:45,840
we put our kernel window on the signal

142
00:04:43,120 --> 00:04:46,720
so that we multiply both pixels inside

143
00:04:45,840 --> 00:04:49,840
this window

144
00:04:46,720 --> 00:04:52,240
by the respective weight next we move

145
00:04:49,840 --> 00:04:54,320
our window to the next position here

146
00:04:52,240 --> 00:04:54,880
just one pixel falls into the scope of a

147
00:04:54,320 --> 00:04:57,840
window

148
00:04:54,880 --> 00:05:00,240
and we simply copy it that's it this

149
00:04:57,840 --> 00:05:02,880
example depicts b linear scaling as

150
00:05:00,240 --> 00:05:04,960
implemented by tensorflow we can now

151
00:05:02,880 --> 00:05:08,000
observe two interesting points

152
00:05:04,960 --> 00:05:10,799
first not all pixels equally contribute

153
00:05:08,000 --> 00:05:12,800
to the downscaled output only pixels

154
00:05:10,800 --> 00:05:15,840
close to the center of a kernel

155
00:05:12,800 --> 00:05:17,919
receive a high weighting and second

156
00:05:15,840 --> 00:05:20,320
if the step size exceeds the kernel

157
00:05:17,919 --> 00:05:22,639
width pixels are even ignored

158
00:05:20,320 --> 00:05:24,719
in this example here we had to use

159
00:05:22,639 --> 00:05:26,880
larger step size as we skate an image

160
00:05:24,720 --> 00:05:30,479
from 9 pixels to 2 pixels

161
00:05:26,880 --> 00:05:32,320
and only 3 out of 9 pixels were used for

162
00:05:30,479 --> 00:05:35,120
downscale

163
00:05:32,320 --> 00:05:37,520
therefore an adversary only needs to

164
00:05:35,120 --> 00:05:39,520
modify those pixels with high weights by

165
00:05:37,520 --> 00:05:41,599
setting them to the target value

166
00:05:39,520 --> 00:05:42,719
and the rest of the image remains

167
00:05:41,600 --> 00:05:45,440
unchanged

168
00:05:42,720 --> 00:05:46,800
as for example here shows in this way we

169
00:05:45,440 --> 00:05:50,160
achieve both goals of

170
00:05:46,800 --> 00:05:52,000
attack after downscaling we exactly

171
00:05:50,160 --> 00:05:55,039
obtain the target signal that we

172
00:05:52,000 --> 00:05:56,160
want but before scaling we only modify a

173
00:05:55,039 --> 00:05:58,880
small portion

174
00:05:56,160 --> 00:06:00,000
of our original signal many pixels can

175
00:05:58,880 --> 00:06:01,759
keep their value

176
00:06:00,000 --> 00:06:03,919
and the adversary signals rather

177
00:06:01,759 --> 00:06:06,080
unnoticeable

178
00:06:03,919 --> 00:06:08,159
the success of this attack therefore

179
00:06:06,080 --> 00:06:10,240
depends on two key parameters

180
00:06:08,160 --> 00:06:11,759
the scaling ratio which determines the

181
00:06:10,240 --> 00:06:13,759
step size of a window

182
00:06:11,759 --> 00:06:15,199
therefore the larger of a step size for

183
00:06:13,759 --> 00:06:17,360
more pixels can remain

184
00:06:15,199 --> 00:06:18,319
unchanged and the attack can be less

185
00:06:17,360 --> 00:06:20,319
visible

186
00:06:18,319 --> 00:06:21,759
and the kernel width the larger the

187
00:06:20,319 --> 00:06:24,240
kernel width the more

188
00:06:21,759 --> 00:06:26,080
pixels are considered for downscaling

189
00:06:24,240 --> 00:06:26,880
and the attacker needs to modify more

190
00:06:26,080 --> 00:06:29,919
pixels

191
00:06:26,880 --> 00:06:31,520
and the attack becomes more visible

192
00:06:29,919 --> 00:06:33,919
different scaling algorithms and

193
00:06:31,520 --> 00:06:34,240
libraries differ in the definition of

194
00:06:33,919 --> 00:06:36,799
this

195
00:06:34,240 --> 00:06:38,000
kernel window and the moving operation

196
00:06:36,800 --> 00:06:39,919
and interestingly

197
00:06:38,000 --> 00:06:43,280
it is quite common to fix the kernel

198
00:06:39,919 --> 00:06:45,840
with irrespective of a scaling ratio

199
00:06:43,280 --> 00:06:47,119
so while the adversary cannot control

200
00:06:45,840 --> 00:06:48,960
the kernel with

201
00:06:47,120 --> 00:06:50,960
she can control the scaling ratio

202
00:06:48,960 --> 00:06:53,198
enabling a scaling attack

203
00:06:50,960 --> 00:06:55,680
this root cause analysis gives us a very

204
00:06:53,199 --> 00:06:58,240
good understanding why the attack works

205
00:06:55,680 --> 00:06:59,840
so we can now derive effective defenses

206
00:06:58,240 --> 00:07:02,160
to prevent the attack

207
00:06:59,840 --> 00:07:04,719
an important requirement for practice is

208
00:07:02,160 --> 00:07:06,960
that we want a secure scaling operation

209
00:07:04,720 --> 00:07:08,000
without the need to change the api of

210
00:07:06,960 --> 00:07:10,560
image scaling

211
00:07:08,000 --> 00:07:12,479
otherwise if the imaging library

212
00:07:10,560 --> 00:07:14,400
rejected an input as a tag

213
00:07:12,479 --> 00:07:15,919
we would somehow need to deal with this

214
00:07:14,400 --> 00:07:17,679
rejection in our machine learning

215
00:07:15,919 --> 00:07:20,080
pipeline

216
00:07:17,680 --> 00:07:22,319
as a first defense type we start with a

217
00:07:20,080 --> 00:07:23,120
conception of an ideal robust scaling

218
00:07:22,319 --> 00:07:25,120
algorithm

219
00:07:23,120 --> 00:07:27,759
so that we can check the robustness of

220
00:07:25,120 --> 00:07:30,160
existing scaling implementations

221
00:07:27,759 --> 00:07:30,800
for instance a secure scaling can be

222
00:07:30,160 --> 00:07:32,960
achieved

223
00:07:30,800 --> 00:07:34,160
by adjusting the kernel width to the

224
00:07:32,960 --> 00:07:37,359
scaling ratio

225
00:07:34,160 --> 00:07:40,800
such that the windows overlap a scaling

226
00:07:37,360 --> 00:07:43,360
attack is then not possible anymore

227
00:07:40,800 --> 00:07:44,000
as a second type of defense we develop a

228
00:07:43,360 --> 00:07:46,400
method

229
00:07:44,000 --> 00:07:48,240
that reconstructs the source image and

230
00:07:46,400 --> 00:07:49,758
it is therefore applicable to any

231
00:07:48,240 --> 00:07:51,759
scaling a grip

232
00:07:49,759 --> 00:07:54,080
this slide also shows an example for

233
00:07:51,759 --> 00:07:55,840
this we see some pixels and the red

234
00:07:54,080 --> 00:07:57,919
points represent the pixels

235
00:07:55,840 --> 00:08:00,560
that the bilinear scaling algorithm will

236
00:07:57,919 --> 00:08:02,479
use to compute the downscaled image

237
00:08:00,560 --> 00:08:03,680
and these pixels were modified by an

238
00:08:02,479 --> 00:08:06,800
adversary

239
00:08:03,680 --> 00:08:07,520
fortunately as a defender we know which

240
00:08:06,800 --> 00:08:09,840
pixels

241
00:08:07,520 --> 00:08:10,639
are used by the bilinear scaling

242
00:08:09,840 --> 00:08:13,679
algorithm

243
00:08:10,639 --> 00:08:16,080
and the idea is to repair them

244
00:08:13,680 --> 00:08:18,800
so we'll select a pixel that the

245
00:08:16,080 --> 00:08:21,198
billionaire algorithm will use

246
00:08:18,800 --> 00:08:23,440
define a window around its neighborhood

247
00:08:21,199 --> 00:08:26,479
compute the median of this window

248
00:08:23,440 --> 00:08:28,639
and use this value to repair the pixel

249
00:08:26,479 --> 00:08:30,000
then we proceed with this method with

250
00:08:28,639 --> 00:08:32,719
the next pixel

251
00:08:30,000 --> 00:08:34,958
and define a new window around it

252
00:08:32,719 --> 00:08:36,800
compute the median and use this value to

253
00:08:34,958 --> 00:08:40,000
repair this pixel again

254
00:08:36,799 --> 00:08:42,159
we repeat this step for all pixels that

255
00:08:40,000 --> 00:08:44,640
the scaling algorithm uses

256
00:08:42,159 --> 00:08:45,439
in this way the adversary needs to

257
00:08:44,640 --> 00:08:48,640
change

258
00:08:45,440 --> 00:08:51,440
the pixels in these windows as well

259
00:08:48,640 --> 00:08:53,279
and we show that this is generally not

260
00:08:51,440 --> 00:08:56,560
possible for an adaptive attacker

261
00:08:53,279 --> 00:08:58,640
to bypass such a defense

262
00:08:56,560 --> 00:09:00,319
for evaluating the attacks and defenses

263
00:08:58,640 --> 00:09:01,120
we consider three common imaging

264
00:09:00,320 --> 00:09:03,120
libraries

265
00:09:01,120 --> 00:09:05,519
we test all implemented scaling

266
00:09:03,120 --> 00:09:08,160
algorithms and use the imagenet dataset

267
00:09:05,519 --> 00:09:10,399
to have a realistic image setting

268
00:09:08,160 --> 00:09:12,319
first let's check if a downscat output

269
00:09:10,399 --> 00:09:14,959
corresponds to the target image

270
00:09:12,320 --> 00:09:16,880
as we can see the attack is usually

271
00:09:14,959 --> 00:09:18,319
successful for all algorithms and

272
00:09:16,880 --> 00:09:21,040
libraries

273
00:09:18,320 --> 00:09:22,560
however we have another goal the

274
00:09:21,040 --> 00:09:24,719
modified source image

275
00:09:22,560 --> 00:09:27,119
should be similar to its original

276
00:09:24,720 --> 00:09:29,760
version our evaluation shows

277
00:09:27,120 --> 00:09:31,760
that the success of attack here depends

278
00:09:29,760 --> 00:09:32,959
on the scaling ratio and the algorithm

279
00:09:31,760 --> 00:09:35,200
or library

280
00:09:32,959 --> 00:09:36,000
at this point i'd like to refer to the

281
00:09:35,200 --> 00:09:39,519
paper for more

282
00:09:36,000 --> 00:09:42,240
details finally let's briefly check

283
00:09:39,519 --> 00:09:42,959
our reconstruction defense this defense

284
00:09:42,240 --> 00:09:45,519
prevents

285
00:09:42,959 --> 00:09:46,079
all attacks in our evaluation and even

286
00:09:45,519 --> 00:09:48,880
better

287
00:09:46,080 --> 00:09:50,880
the reconstruction increases the visual

288
00:09:48,880 --> 00:09:51,279
quality of the modified source images

289
00:09:50,880 --> 00:09:53,680
because

290
00:09:51,279 --> 00:09:55,439
we remove the adversarial pattern and we

291
00:09:53,680 --> 00:09:56,239
can even get the original prediction

292
00:09:55,440 --> 00:09:58,480
before

293
00:09:56,240 --> 00:10:00,640
modification to give you a short

294
00:09:58,480 --> 00:10:03,279
intuition what that means

295
00:10:00,640 --> 00:10:04,720
here we can see that the first image

296
00:10:03,279 --> 00:10:06,880
depicts a modified

297
00:10:04,720 --> 00:10:09,040
source image that becomes the second

298
00:10:06,880 --> 00:10:11,120
image after downscaling

299
00:10:09,040 --> 00:10:14,160
but if we apply our reconstruction

300
00:10:11,120 --> 00:10:16,320
method the restored image looks cleaner

301
00:10:14,160 --> 00:10:18,480
and after downscaling it shows what we

302
00:10:16,320 --> 00:10:21,200
would expect

303
00:10:18,480 --> 00:10:22,399
so in summary scaling attacks are a new

304
00:10:21,200 --> 00:10:24,880
class of attacks

305
00:10:22,399 --> 00:10:25,920
that we call pre-processing attack and

306
00:10:24,880 --> 00:10:28,880
they need to be

307
00:10:25,920 --> 00:10:30,880
considered in addition to traditional

308
00:10:28,880 --> 00:10:31,360
attacks like poisoning model stealing

309
00:10:30,880 --> 00:10:34,240
adverse

310
00:10:31,360 --> 00:10:36,160
examples and so on our work gives the

311
00:10:34,240 --> 00:10:38,880
first comprehensive analysis

312
00:10:36,160 --> 00:10:40,000
of these attacks by providing a

313
00:10:38,880 --> 00:10:42,880
comprehensive

314
00:10:40,000 --> 00:10:45,120
root cause analysis that allows us to

315
00:10:42,880 --> 00:10:46,959
drive effective defenses to prevent the

316
00:10:45,120 --> 00:10:49,279
attack

317
00:10:46,959 --> 00:10:50,959
to verify the threat in practice we

318
00:10:49,279 --> 00:10:53,279
evaluate these attacks

319
00:10:50,959 --> 00:10:55,040
and the defenses comprehensively with

320
00:10:53,279 --> 00:10:57,760
common imaging libraries in machine

321
00:10:55,040 --> 00:11:00,079
learning like tensorflow

322
00:10:57,760 --> 00:11:01,360
last but not least thank you very much

323
00:11:00,079 --> 00:11:03,359
for your attention

324
00:11:01,360 --> 00:11:14,480
and you can find more information on our

325
00:11:03,360 --> 00:11:16,560
website or of course in our paper

326
00:11:14,480 --> 00:11:16,560
you

