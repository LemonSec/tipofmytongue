1
00:00:09,040 --> 00:00:11,678
hello everyone my name is emily wenger

2
00:00:10,960 --> 00:00:13,679
and i'm here

3
00:00:11,679 --> 00:00:16,240
today with my colleague sean chan to

4
00:00:13,679 --> 00:00:18,080
talk to you about our system fox

5
00:00:16,239 --> 00:00:19,359
which is designed to protect individual

6
00:00:18,080 --> 00:00:22,880
privacy against

7
00:00:19,359 --> 00:00:22,880
unauthorized deep learning models

8
00:00:23,680 --> 00:00:28,800
recently it has become quite easy for

9
00:00:26,800 --> 00:00:31,119
anyone to train a fairly effective

10
00:00:28,800 --> 00:00:32,880
facial recognition model

11
00:00:31,119 --> 00:00:34,800
research advances have shortened

12
00:00:32,880 --> 00:00:37,440
training times for

13
00:00:34,800 --> 00:00:38,800
big models hardware continues to become

14
00:00:37,440 --> 00:00:41,680
cheaper and faster

15
00:00:38,800 --> 00:00:44,078
and social media provides a plethora of

16
00:00:41,680 --> 00:00:46,800
labeled training data

17
00:00:44,079 --> 00:00:47,280
together this means that really anyone

18
00:00:46,800 --> 00:00:49,839
with

19
00:00:47,280 --> 00:00:51,760
some coding knowledge and a nice enough

20
00:00:49,840 --> 00:00:54,640
computer can train a fairly powerful

21
00:00:51,760 --> 00:00:57,760
facial recognition model

22
00:00:54,640 --> 00:00:58,719
well this is interesting it leads to a

23
00:00:57,760 --> 00:01:00,879
very

24
00:00:58,719 --> 00:01:02,239
concerning question which is what if the

25
00:01:00,879 --> 00:01:05,920
wrong kind of people

26
00:01:02,239 --> 00:01:08,399
take advantage of this new accessibility

27
00:01:05,920 --> 00:01:10,400
for example i have a lot of fairly

28
00:01:08,400 --> 00:01:12,479
personal images posted to my various

29
00:01:10,400 --> 00:01:13,840
social media accounts pictures of me

30
00:01:12,479 --> 00:01:15,679
with family or

31
00:01:13,840 --> 00:01:18,400
in regalia from the university of

32
00:01:15,680 --> 00:01:20,240
chicago that kind of thing

33
00:01:18,400 --> 00:01:21,680
what if someone were to scrape those

34
00:01:20,240 --> 00:01:24,240
images and use them

35
00:01:21,680 --> 00:01:25,680
without my knowledge or consent to train

36
00:01:24,240 --> 00:01:28,560
a facial recognition model that

37
00:01:25,680 --> 00:01:28,560
recognizes me

38
00:01:28,720 --> 00:01:32,560
then i could just be sitting on a zoom

39
00:01:30,560 --> 00:01:34,079
call as i do often these days

40
00:01:32,560 --> 00:01:37,040
and someone could come by and take a

41
00:01:34,079 --> 00:01:39,758
screenshot of me and upload it to that

42
00:01:37,040 --> 00:01:42,159
face id service that has the model

43
00:01:39,759 --> 00:01:43,680
trained in my social media photos

44
00:01:42,159 --> 00:01:46,079
the face id service could match the

45
00:01:43,680 --> 00:01:47,520
screenshot easily to my other photos

46
00:01:46,079 --> 00:01:49,600
and yield some pretty personal

47
00:01:47,520 --> 00:01:50,640
information to the person who looked me

48
00:01:49,600 --> 00:01:52,559
up

49
00:01:50,640 --> 00:01:54,000
for example if they saw this picture

50
00:01:52,560 --> 00:01:55,840
they might know where i live or go to

51
00:01:54,000 --> 00:01:57,920
school which could help a stalker find

52
00:01:55,840 --> 00:01:59,600
me more easily

53
00:01:57,920 --> 00:02:01,200
this picture could give them info about

54
00:01:59,600 --> 00:02:03,039
my grandparents which could

55
00:02:01,200 --> 00:02:05,520
help them construct an elaborate fishing

56
00:02:03,040 --> 00:02:07,520
or extortion scheme

57
00:02:05,520 --> 00:02:08,560
knowing about my relationship status

58
00:02:07,520 --> 00:02:11,200
could lead to

59
00:02:08,560 --> 00:02:12,959
negative employment decisions and

60
00:02:11,200 --> 00:02:13,599
depending on who you are or where you're

61
00:02:12,959 --> 00:02:15,280
from

62
00:02:13,599 --> 00:02:17,280
this kind of aggregated personal

63
00:02:15,280 --> 00:02:18,239
information presented by this collection

64
00:02:17,280 --> 00:02:20,080
of photos

65
00:02:18,239 --> 00:02:23,520
could lead to discrimination or

66
00:02:20,080 --> 00:02:26,480
oppression in a variety of forms

67
00:02:23,520 --> 00:02:27,800
obviously this is hugely problematic

68
00:02:26,480 --> 00:02:30,238
this kind of

69
00:02:27,800 --> 00:02:30,720
tracker-esque big brother scenario seems

70
00:02:30,239 --> 00:02:34,160
really

71
00:02:30,720 --> 00:02:35,120
kind of futuristic and scary and it's

72
00:02:34,160 --> 00:02:39,040
actually also

73
00:02:35,120 --> 00:02:42,160
real clearview.ai is a company

74
00:02:39,040 --> 00:02:44,319
that does exactly what we discussed

75
00:02:42,160 --> 00:02:46,000
it was first reported on by cashmere

76
00:02:44,319 --> 00:02:47,679
hill of the new york times in early

77
00:02:46,000 --> 00:02:50,000
2020.

78
00:02:47,680 --> 00:02:51,680
clearview like we said collects all

79
00:02:50,000 --> 00:02:53,440
these social media photos without

80
00:02:51,680 --> 00:02:55,120
people's consent or knowledge

81
00:02:53,440 --> 00:02:57,359
and builds a big facial recognition

82
00:02:55,120 --> 00:02:58,080
model that it offers to people who

83
00:02:57,360 --> 00:03:01,200
subscribe to

84
00:02:58,080 --> 00:03:02,800
services those subscribers can then

85
00:03:01,200 --> 00:03:03,518
upload a photo of someone they're

86
00:03:02,800 --> 00:03:06,560
interested in

87
00:03:03,519 --> 00:03:08,400
and receive back matching photos of that

88
00:03:06,560 --> 00:03:09,920
same person and a lot of other personal

89
00:03:08,400 --> 00:03:12,159
information that clearview has

90
00:03:09,920 --> 00:03:14,079
aggregated

91
00:03:12,159 --> 00:03:16,079
the customer list of clearview is not

92
00:03:14,080 --> 00:03:17,760
public but the new york times article

93
00:03:16,080 --> 00:03:19,120
noted that it at least includes some

94
00:03:17,760 --> 00:03:21,120
government agencies

95
00:03:19,120 --> 00:03:23,040
law enforcement departments and even

96
00:03:21,120 --> 00:03:25,840
private citizens who can afford to pay

97
00:03:23,040 --> 00:03:25,840
clear views fees

98
00:03:26,239 --> 00:03:30,400
the existence of clearview.ai is

99
00:03:28,799 --> 00:03:32,799
terrifying

100
00:03:30,400 --> 00:03:34,560
individuals need recourse to be able to

101
00:03:32,799 --> 00:03:37,440
guard themselves against this kind of

102
00:03:34,560 --> 00:03:40,640
intrusion of their privacy

103
00:03:37,440 --> 00:03:42,640
and that is where our system fox comes

104
00:03:40,640 --> 00:03:45,839
in

105
00:03:42,640 --> 00:03:46,238
fox is designed to help individuals

106
00:03:45,840 --> 00:03:49,040
fight

107
00:03:46,239 --> 00:03:50,000
unwanted facial recognition it protects

108
00:03:49,040 --> 00:03:52,079
images

109
00:03:50,000 --> 00:03:54,080
in specific ways so that when those

110
00:03:52,080 --> 00:03:56,000
images are shared on social media

111
00:03:54,080 --> 00:03:59,120
they can't be used to train a facial

112
00:03:56,000 --> 00:04:00,959
recognition model that recognizes you

113
00:03:59,120 --> 00:04:02,480
in this talk we're going to discuss how

114
00:04:00,959 --> 00:04:05,680
fox is designed and

115
00:04:02,480 --> 00:04:07,280
evaluate its performance

116
00:04:05,680 --> 00:04:08,720
before we do that though let's just

117
00:04:07,280 --> 00:04:11,120
quickly look at the goals and

118
00:04:08,720 --> 00:04:14,080
assumptions of fox

119
00:04:11,120 --> 00:04:14,480
we consider two parties in our system

120
00:04:14,080 --> 00:04:17,680
design

121
00:04:14,480 --> 00:04:19,120
a user and a tracker the user is a

122
00:04:17,680 --> 00:04:21,280
pretty ordinary guy

123
00:04:19,120 --> 00:04:23,040
and has just some limited computational

124
00:04:21,279 --> 00:04:24,799
resources and access to

125
00:04:23,040 --> 00:04:26,960
a well-trained feature extractor which

126
00:04:24,800 --> 00:04:29,520
we will discuss shortly

127
00:04:26,960 --> 00:04:30,799
together the user uses these tools to

128
00:04:29,520 --> 00:04:33,440
protect or cloak

129
00:04:30,800 --> 00:04:34,880
their images in fox terminology before

130
00:04:33,440 --> 00:04:36,639
posting them to

131
00:04:34,880 --> 00:04:38,479
social media sites and other places

132
00:04:36,639 --> 00:04:40,080
online

133
00:04:38,479 --> 00:04:42,560
on the other end the tracker like

134
00:04:40,080 --> 00:04:43,520
clearview can scrape those protected

135
00:04:42,560 --> 00:04:45,600
images

136
00:04:43,520 --> 00:04:46,639
and use their extensive computational

137
00:04:45,600 --> 00:04:48,800
resources

138
00:04:46,639 --> 00:04:50,960
in an attempt to train an effective

139
00:04:48,800 --> 00:04:53,199
facial recognition model

140
00:04:50,960 --> 00:04:57,840
note that the tracker really only has

141
00:04:53,199 --> 00:05:00,000
access to these protected images

142
00:04:57,840 --> 00:05:01,039
before we dive into how fox actually

143
00:05:00,000 --> 00:05:02,800
protects images

144
00:05:01,039 --> 00:05:05,680
let's just briefly review how facial

145
00:05:02,800 --> 00:05:07,680
recognition models work

146
00:05:05,680 --> 00:05:10,000
to train a facial recognition model you

147
00:05:07,680 --> 00:05:11,759
need to first collect a large data set

148
00:05:10,000 --> 00:05:13,759
in this example we have four different

149
00:05:11,759 --> 00:05:15,440
classes in that data set

150
00:05:13,759 --> 00:05:17,199
and you use those to iteratively

151
00:05:15,440 --> 00:05:18,479
optimize what's known as a feature

152
00:05:17,199 --> 00:05:20,880
extractor

153
00:05:18,479 --> 00:05:22,840
in order to produce a well-separated

154
00:05:20,880 --> 00:05:24,240
quote-unquote feature representation

155
00:05:22,840 --> 00:05:26,638
space

156
00:05:24,240 --> 00:05:27,680
the feature representation space is just

157
00:05:26,639 --> 00:05:30,479
a mathematical

158
00:05:27,680 --> 00:05:31,680
understanding of how the model separates

159
00:05:30,479 --> 00:05:34,000
these different classes

160
00:05:31,680 --> 00:05:35,520
but as we can see in the bottom figure

161
00:05:34,000 --> 00:05:37,600
this model has learned to

162
00:05:35,520 --> 00:05:40,159
easily place each of the four classes

163
00:05:37,600 --> 00:05:42,000
into distinct regions of feature space

164
00:05:40,160 --> 00:05:44,639
so that means when a new image of for

165
00:05:42,000 --> 00:05:46,320
example me is presented to the model

166
00:05:44,639 --> 00:05:48,560
it is mapped to the region that the

167
00:05:46,320 --> 00:05:52,960
model understands to be associated with

168
00:05:48,560 --> 00:05:56,160
me and so is classified as me

169
00:05:52,960 --> 00:05:57,758
in order to do its protection work fox

170
00:05:56,160 --> 00:06:00,080
kind of manipulates how the future

171
00:05:57,759 --> 00:06:02,639
representation space of the model

172
00:06:00,080 --> 00:06:03,359
looks which confuses the model and helps

173
00:06:02,639 --> 00:06:06,479
individuals

174
00:06:03,360 --> 00:06:08,560
evade unwanted facial recognition

175
00:06:06,479 --> 00:06:10,159
so on the left we see just the feature

176
00:06:08,560 --> 00:06:12,080
space of a normal model trained on

177
00:06:10,160 --> 00:06:15,199
normal photos and we can see that it's

178
00:06:12,080 --> 00:06:17,758
well separated and well delineated

179
00:06:15,199 --> 00:06:20,240
and on the right this is a model trained

180
00:06:17,759 --> 00:06:23,039
on quote-unquote cloaked images of me

181
00:06:20,240 --> 00:06:23,680
where my future representation has been

182
00:06:23,039 --> 00:06:25,759
shifted

183
00:06:23,680 --> 00:06:26,720
towards that of beyonce so the model no

184
00:06:25,759 --> 00:06:30,240
longer has

185
00:06:26,720 --> 00:06:32,000
a distinct region associated with me

186
00:06:30,240 --> 00:06:33,840
then when a normal image of me is

187
00:06:32,000 --> 00:06:34,960
presented to that original model it can

188
00:06:33,840 --> 00:06:37,198
easily identify as

189
00:06:34,960 --> 00:06:38,638
me as me because i'm mapped to the space

190
00:06:37,199 --> 00:06:40,800
associated with me

191
00:06:38,639 --> 00:06:42,960
but when that same original images as

192
00:06:40,800 --> 00:06:44,880
images presented to the cloaked model

193
00:06:42,960 --> 00:06:47,359
it's mapped to ben's feature space so

194
00:06:44,880 --> 00:06:49,120
the model really can't recognize me

195
00:06:47,360 --> 00:06:52,000
and that is how fox provides that

196
00:06:49,120 --> 00:06:53,360
protection for individuals

197
00:06:52,000 --> 00:06:55,199
my colleague sean is now going to

198
00:06:53,360 --> 00:06:57,199
provide some additional

199
00:06:55,199 --> 00:06:59,199
insight into how fox works and how it

200
00:06:57,199 --> 00:07:02,479
performs

201
00:06:59,199 --> 00:07:04,319
all right thanks emily

202
00:07:02,479 --> 00:07:06,479
the way we compute the perturbation for

203
00:07:04,319 --> 00:07:07,440
an image is a very standard optimization

204
00:07:06,479 --> 00:07:09,359
technique

205
00:07:07,440 --> 00:07:12,240
the goal here is to mimic the feature

206
00:07:09,360 --> 00:07:14,000
space representation of target faces

207
00:07:12,240 --> 00:07:16,080
and we have a constraint to make sure

208
00:07:14,000 --> 00:07:19,120
the probation is indistinguishable

209
00:07:16,080 --> 00:07:21,198
by humans that is why the main objective

210
00:07:19,120 --> 00:07:24,080
here is to minimize the auto distance

211
00:07:21,199 --> 00:07:25,599
between the two representations

212
00:07:24,080 --> 00:07:27,359
and the constraint term here is to

213
00:07:25,599 --> 00:07:31,919
control the amount of modification

214
00:07:27,360 --> 00:07:34,160
we added to the image we use dss am to

215
00:07:31,919 --> 00:07:36,479
measure the amount of perturbation it is

216
00:07:34,160 --> 00:07:40,319
an objective measure of image distortion

217
00:07:36,479 --> 00:07:40,318
as often used in the vision domain

218
00:07:40,400 --> 00:07:45,440
next i will present the experiment

219
00:07:43,199 --> 00:07:46,720
results showing fox is effective under

220
00:07:45,440 --> 00:07:49,120
different conditions

221
00:07:46,720 --> 00:07:50,720
from the baseline to real-world settings

222
00:07:49,120 --> 00:07:53,120
we'll start with the baseline

223
00:07:50,720 --> 00:07:56,400
where we assume the user knows the exact

224
00:07:53,120 --> 00:07:58,479
facial extractor used by the tracker

225
00:07:56,400 --> 00:08:00,960
as we have discussed the user sharing

226
00:07:58,479 --> 00:08:02,639
some image of herself on the internet

227
00:08:00,960 --> 00:08:04,719
actually cloaked her in the image using

228
00:08:02,639 --> 00:08:06,800
a feature extractor

229
00:08:04,720 --> 00:08:08,000
the tracker scrapes the cloaked image

230
00:08:06,800 --> 00:08:10,560
from online

231
00:08:08,000 --> 00:08:14,240
use the exact same feature extractor to

232
00:08:10,560 --> 00:08:14,240
train a face recognition model

233
00:08:14,720 --> 00:08:18,800
we measure the effectiveness of fox in

234
00:08:16,720 --> 00:08:21,599
terms of protection success rate

235
00:08:18,800 --> 00:08:22,080
which is the percentage of real user

236
00:08:21,599 --> 00:08:25,280
image

237
00:08:22,080 --> 00:08:26,639
misclassified by the tracker's model

238
00:08:25,280 --> 00:08:28,559
here's a result of the baseline

239
00:08:26,639 --> 00:08:30,080
condition we tested four different

240
00:08:28,560 --> 00:08:31,759
feature extractors

241
00:08:30,080 --> 00:08:33,598
trained using different data set and

242
00:08:31,759 --> 00:08:35,760
model architectures

243
00:08:33,599 --> 00:08:40,320
the protection successor is 100 percent

244
00:08:35,760 --> 00:08:43,039
for each of these extractors

245
00:08:40,320 --> 00:08:44,560
next we consider fox in more realistic

246
00:08:43,039 --> 00:08:47,439
conditions

247
00:08:44,560 --> 00:08:49,119
we leverage the transferability of a

248
00:08:47,440 --> 00:08:51,600
feature extraction

249
00:08:49,120 --> 00:08:53,440
transferability is observation that the

250
00:08:51,600 --> 00:08:54,560
perturbation can transfer to different

251
00:08:53,440 --> 00:08:57,600
feature structures

252
00:08:54,560 --> 00:08:59,279
and have a similar effect in the case

253
00:08:57,600 --> 00:09:00,399
where user has no information of

254
00:08:59,279 --> 00:09:02,640
tracker's model

255
00:09:00,399 --> 00:09:04,480
whether the tracker trains using a

256
00:09:02,640 --> 00:09:07,199
different feature extraction

257
00:09:04,480 --> 00:09:09,839
or train model completely from scratch

258
00:09:07,200 --> 00:09:12,320
our protection success rate is above 95

259
00:09:09,839 --> 00:09:12,320
percent

260
00:09:12,720 --> 00:09:16,160
next we test fox against several real

261
00:09:14,800 --> 00:09:19,199
world state of our

262
00:09:16,160 --> 00:09:22,560
recognition apis we select three

263
00:09:19,200 --> 00:09:26,160
most widely used ones microsoft azure

264
00:09:22,560 --> 00:09:28,399
amazon recognition and face plus plus

265
00:09:26,160 --> 00:09:30,079
for each of these apis we upload some

266
00:09:28,399 --> 00:09:33,040
cloaked image of the user x

267
00:09:30,080 --> 00:09:33,839
as a training data and use api to train

268
00:09:33,040 --> 00:09:36,000
a model

269
00:09:33,839 --> 00:09:37,279
and test whether the result model is

270
00:09:36,000 --> 00:09:40,880
able to recognize

271
00:09:37,279 --> 00:09:41,680
unclogged image of user x fox achieve

272
00:09:40,880 --> 00:09:43,680
100

273
00:09:41,680 --> 00:09:45,279
protection success rate against each of

274
00:09:43,680 --> 00:09:47,839
these apis

275
00:09:45,279 --> 00:09:49,200
this shows foxy is effective against

276
00:09:47,839 --> 00:09:51,200
data of our apis

277
00:09:49,200 --> 00:09:52,720
without needing any information about

278
00:09:51,200 --> 00:09:56,000
the face recognition system

279
00:09:52,720 --> 00:09:56,000
the tracker has used

280
00:09:56,959 --> 00:10:01,439
we also explore many different

281
00:09:59,040 --> 00:10:03,279
challenging real world scenarios

282
00:10:01,440 --> 00:10:04,560
the first scenario we consider is when

283
00:10:03,279 --> 00:10:07,439
tracker obtain a set of

284
00:10:04,560 --> 00:10:07,839
original image of the user in many cases

285
00:10:07,440 --> 00:10:09,440
we

286
00:10:07,839 --> 00:10:10,880
do not control all the image of

287
00:10:09,440 --> 00:10:12,880
ourselves online

288
00:10:10,880 --> 00:10:16,000
some could be post from a public source

289
00:10:12,880 --> 00:10:18,560
or post by our friends

290
00:10:16,000 --> 00:10:20,160
in this scenario fox remains successful

291
00:10:18,560 --> 00:10:22,800
when the number of cloaked images

292
00:10:20,160 --> 00:10:24,560
outnumber that of unclipped images

293
00:10:22,800 --> 00:10:26,000
and will leverage siebel account to

294
00:10:24,560 --> 00:10:28,959
improve their protection

295
00:10:26,000 --> 00:10:30,240
effectiveness so for users who have live

296
00:10:28,959 --> 00:10:32,079
image already online

297
00:10:30,240 --> 00:10:34,399
one way to improve their protection is

298
00:10:32,079 --> 00:10:37,760
to release even more image of themselves

299
00:10:34,399 --> 00:10:38,959
all cloaked which balance out the ratio

300
00:10:37,760 --> 00:10:40,720
we also consider different

301
00:10:38,959 --> 00:10:41,920
countermeasures that can be utilized by

302
00:10:40,720 --> 00:10:44,480
the tracker

303
00:10:41,920 --> 00:10:46,800
and show the effect fox is robust

304
00:10:44,480 --> 00:10:48,480
against all of them

305
00:10:46,800 --> 00:10:49,839
here we only consider automated

306
00:10:48,480 --> 00:10:52,399
countermeasure because

307
00:10:49,839 --> 00:10:55,279
fox is focused on preventing automated

308
00:10:52,399 --> 00:10:57,360
tracking of a large number of users

309
00:10:55,279 --> 00:10:58,480
a tracker cannot spend much energy and

310
00:10:57,360 --> 00:11:01,120
resources

311
00:10:58,480 --> 00:11:03,120
per targeted user because this does not

312
00:11:01,120 --> 00:11:04,959
stick scale well

313
00:11:03,120 --> 00:11:06,720
we also extensively discussed the

314
00:11:04,959 --> 00:11:08,399
limitation of fox

315
00:11:06,720 --> 00:11:10,160
this hard to guarantee fox will be

316
00:11:08,399 --> 00:11:12,240
successful against face recognition

317
00:11:10,160 --> 00:11:14,079
system built in the future

318
00:11:12,240 --> 00:11:15,920
in addition with our current limited

319
00:11:14,079 --> 00:11:17,199
understanding of companies like

320
00:11:15,920 --> 00:11:20,560
cleaver.ei

321
00:11:17,200 --> 00:11:22,640
fox is only an early work in the space

322
00:11:20,560 --> 00:11:25,839
again there are more detail in the paper

323
00:11:22,640 --> 00:11:25,839
on each of these topics

324
00:11:26,240 --> 00:11:29,360
today we present fox a first step to

325
00:11:28,480 --> 00:11:32,240
collecting

326
00:11:29,360 --> 00:11:34,000
protect individuals from recognition by

327
00:11:32,240 --> 00:11:35,680
unauthorizing uncountable face

328
00:11:34,000 --> 00:11:37,120
recognition systems

329
00:11:35,680 --> 00:11:39,120
with this i would love to end this

330
00:11:37,120 --> 00:11:41,440
presentation by pointing you to our

331
00:11:39,120 --> 00:11:42,959
project website the website contains

332
00:11:41,440 --> 00:11:44,720
links to our source code

333
00:11:42,959 --> 00:11:47,359
called binaries and frequently asked

334
00:11:44,720 --> 00:11:49,200
questions we welcome you to try it out

335
00:11:47,360 --> 00:11:51,279
and give us feedbacks

336
00:11:49,200 --> 00:11:54,839
thanks for listening to our presentation

337
00:11:51,279 --> 00:11:57,839
emily and i would love to answer any

338
00:11:54,839 --> 00:11:57,839
questions

339
00:12:03,760 --> 00:12:05,839
you

