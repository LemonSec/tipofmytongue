1
00:00:07,919 --> 00:00:10,799
we'll be talking about

2
00:00:08,960 --> 00:00:13,120
our paper high accuracy and high

3
00:00:10,800 --> 00:00:15,040
fidelity extraction of neural networks

4
00:00:13,120 --> 00:00:16,160
this is joint work with nicolas carlini

5
00:00:15,040 --> 00:00:18,960
david brechtlow

6
00:00:16,160 --> 00:00:20,240
alex karakin and nicola paperno this

7
00:00:18,960 --> 00:00:21,520
work was done while all of us were at

8
00:00:20,240 --> 00:00:23,038
google research

9
00:00:21,520 --> 00:00:25,039
but i've since moved back to my home

10
00:00:23,039 --> 00:00:26,480
institution of northeastern university

11
00:00:25,039 --> 00:00:28,960
and nicolas has moved to university of

12
00:00:26,480 --> 00:00:28,960
toronto

13
00:00:29,359 --> 00:00:33,280
our talk starts with a common setup

14
00:00:31,279 --> 00:00:34,960
machine learning as a service

15
00:00:33,280 --> 00:00:36,640
digit co has trained a very good

16
00:00:34,960 --> 00:00:38,960
handwritten digit recognition

17
00:00:36,640 --> 00:00:40,160
model and alice is willing to pay to use

18
00:00:38,960 --> 00:00:43,600
it

19
00:00:40,160 --> 00:00:45,760
alice identifies some handwritten five

20
00:00:43,600 --> 00:00:47,760
sends it to digit co digico runs it

21
00:00:45,760 --> 00:00:51,199
through the model finds out it's a five

22
00:00:47,760 --> 00:00:54,079
and sends that information back to alice

23
00:00:51,199 --> 00:00:55,920
however if alice is malicious she can

24
00:00:54,079 --> 00:00:57,360
use this input output interaction to

25
00:00:55,920 --> 00:00:59,280
steal the model

26
00:00:57,360 --> 00:01:01,840
this attack is known as model extraction

27
00:00:59,280 --> 00:01:04,239
and is the focus of our talk

28
00:01:01,840 --> 00:01:05,920
today i'm going to be discussing three

29
00:01:04,239 --> 00:01:07,680
parts of our paper

30
00:01:05,920 --> 00:01:09,840
a taxonomy we introduce for model

31
00:01:07,680 --> 00:01:12,320
extraction and improvements

32
00:01:09,840 --> 00:01:13,920
to two types of extraction attacks

33
00:01:12,320 --> 00:01:17,439
learning based extraction

34
00:01:13,920 --> 00:01:19,040
and direct recovery extraction

35
00:01:17,439 --> 00:01:20,880
to start with the taxonomy we need to

36
00:01:19,040 --> 00:01:22,560
figure out why someone would do model

37
00:01:20,880 --> 00:01:25,280
extraction

38
00:01:22,560 --> 00:01:26,640
the first reason we argue is that data

39
00:01:25,280 --> 00:01:30,720
compute

40
00:01:26,640 --> 00:01:32,640
labels and engineers are expensive

41
00:01:30,720 --> 00:01:34,240
and an adversary might try to avoid

42
00:01:32,640 --> 00:01:35,920
paying for these things by stealing

43
00:01:34,240 --> 00:01:40,158
someone else's model

44
00:01:35,920 --> 00:01:42,399
we call this adversary theft motivated

45
00:01:40,159 --> 00:01:43,759
on the other hand models have security

46
00:01:42,399 --> 00:01:45,439
and privacy properties

47
00:01:43,759 --> 00:01:48,240
and so stealing the model might be a way

48
00:01:45,439 --> 00:01:50,479
to improve the attacks that can be run

49
00:01:48,240 --> 00:01:52,560
we call this adversary a reconnaissance

50
00:01:50,479 --> 00:01:54,479
motivated adversary

51
00:01:52,560 --> 00:01:56,320
key thrust of our paper is that these

52
00:01:54,479 --> 00:01:58,240
are two different adversaries

53
00:01:56,320 --> 00:01:59,758
and they have two fundamentally

54
00:01:58,240 --> 00:02:01,360
different goals

55
00:01:59,759 --> 00:02:03,520
the theft motivated adversary cares

56
00:02:01,360 --> 00:02:05,119
about accuracy they want good accuracy

57
00:02:03,520 --> 00:02:06,479
on the target task

58
00:02:05,119 --> 00:02:08,479
on the other hand a reconnaissance

59
00:02:06,479 --> 00:02:10,800
motivated adversary

60
00:02:08,479 --> 00:02:13,280
cares about matching the particularities

61
00:02:10,800 --> 00:02:16,400
of the specific target model

62
00:02:13,280 --> 00:02:18,319
they care about fidelity in the limit

63
00:02:16,400 --> 00:02:19,760
if you want the outputs to match for all

64
00:02:18,319 --> 00:02:21,679
inputs

65
00:02:19,760 --> 00:02:23,760
this adversary is called functionally

66
00:02:21,680 --> 00:02:25,680
equivalent

67
00:02:23,760 --> 00:02:27,440
this isn't the only axis of interest for

68
00:02:25,680 --> 00:02:28,480
a taxonomy of model extraction but

69
00:02:27,440 --> 00:02:31,200
you're going to have to see the paper

70
00:02:28,480 --> 00:02:32,640
for more details

71
00:02:31,200 --> 00:02:35,359
now that we know why someone might do

72
00:02:32,640 --> 00:02:37,440
model extraction let's figure out how

73
00:02:35,360 --> 00:02:39,519
we'll start by describing a simple case

74
00:02:37,440 --> 00:02:41,760
a linear model

75
00:02:39,519 --> 00:02:44,000
the natural thing that someone might do

76
00:02:41,760 --> 00:02:46,399
is you query on 100 points

77
00:02:44,000 --> 00:02:48,720
you now have 100 outputs now do machine

78
00:02:46,400 --> 00:02:50,959
learning on those input output pairs

79
00:02:48,720 --> 00:02:51,920
machine learning is designed to identify

80
00:02:50,959 --> 00:02:53,440
the relationship

81
00:02:51,920 --> 00:02:55,040
between inputs and outputs so this is

82
00:02:53,440 --> 00:02:58,159
going to be good

83
00:02:55,040 --> 00:03:00,319
but also notice that for a linear model

84
00:02:58,159 --> 00:03:02,959
and specifically if you query on all

85
00:03:00,319 --> 00:03:04,560
zeros except the first input is a one

86
00:03:02,959 --> 00:03:06,800
you've recovered the first coordinate of

87
00:03:04,560 --> 00:03:08,480
the weight

88
00:03:06,800 --> 00:03:10,319
if you query on all zeros except the

89
00:03:08,480 --> 00:03:11,679
second coordinate is one

90
00:03:10,319 --> 00:03:13,679
you recover the second corner of the

91
00:03:11,680 --> 00:03:15,599
weights

92
00:03:13,680 --> 00:03:16,720
doing this you can recover the linear

93
00:03:15,599 --> 00:03:19,518
model exactly

94
00:03:16,720 --> 00:03:21,120
by doing this for every coordinate these

95
00:03:19,519 --> 00:03:22,720
are

96
00:03:21,120 --> 00:03:24,239
essentially the same thing for linear

97
00:03:22,720 --> 00:03:26,799
models but

98
00:03:24,239 --> 00:03:28,560
they capture two different philosophies

99
00:03:26,799 --> 00:03:31,760
of model extraction

100
00:03:28,560 --> 00:03:34,000
the first is machine learning philosophy

101
00:03:31,760 --> 00:03:37,359
i have a lot of inputs and outputs

102
00:03:34,000 --> 00:03:40,640
uh let me learn the relationship

103
00:03:37,360 --> 00:03:43,360
the other is i know the functional form

104
00:03:40,640 --> 00:03:45,279
of the model that i'm trying to steal

105
00:03:43,360 --> 00:03:47,599
but i don't know the secret

106
00:03:45,280 --> 00:03:49,200
and so i'm going to steal the secret in

107
00:03:47,599 --> 00:03:51,440
this sort of cryptanalytic

108
00:03:49,200 --> 00:03:51,440
way

109
00:03:52,640 --> 00:03:56,879
neural networks are where these two

110
00:03:54,720 --> 00:03:58,080
philosophies really diverge

111
00:03:56,879 --> 00:03:59,920
and i'll talk about that in the

112
00:03:58,080 --> 00:04:01,360
remainder of the talk starting with our

113
00:03:59,920 --> 00:04:04,319
improvements to learning based

114
00:04:01,360 --> 00:04:06,080
extraction so with learning based

115
00:04:04,319 --> 00:04:08,000
extraction

116
00:04:06,080 --> 00:04:10,000
it works fairly out of the box so the

117
00:04:08,000 --> 00:04:11,840
main object of interest is improving the

118
00:04:10,000 --> 00:04:13,360
query efficiency of it

119
00:04:11,840 --> 00:04:15,840
and we push that by using

120
00:04:13,360 --> 00:04:18,720
semi-supervised learning in our paper

121
00:04:15,840 --> 00:04:20,000
in semi-supervised learning one has a

122
00:04:18,720 --> 00:04:22,240
small label data set

123
00:04:20,000 --> 00:04:23,280
and a large unlabeled data set and the

124
00:04:22,240 --> 00:04:25,360
goal is to use them

125
00:04:23,280 --> 00:04:26,799
both to produce a model that's better

126
00:04:25,360 --> 00:04:29,840
than if you just had

127
00:04:26,800 --> 00:04:29,840
the labeled data set

128
00:04:30,479 --> 00:04:34,560
the main drawback of this is that you

129
00:04:32,639 --> 00:04:36,960
need this unlabeled data set

130
00:04:34,560 --> 00:04:38,400
but for a theft motivated adversary we

131
00:04:36,960 --> 00:04:39,120
argue that this is actually a reasonable

132
00:04:38,400 --> 00:04:40,479
assumption

133
00:04:39,120 --> 00:04:42,160
you had a lot of points you wanted to

134
00:04:40,479 --> 00:04:44,159
query the model on anyways you just

135
00:04:42,160 --> 00:04:46,160
didn't want to pay for all of them

136
00:04:44,160 --> 00:04:48,240
so now you can pay for as much as you

137
00:04:46,160 --> 00:04:50,720
want and train on the rest

138
00:04:48,240 --> 00:04:52,160
with semi-supervised learning this makes

139
00:04:50,720 --> 00:04:54,880
it more label efficient

140
00:04:52,160 --> 00:04:56,560
and it scales to complicated models and

141
00:04:54,880 --> 00:04:58,479
tasks

142
00:04:56,560 --> 00:05:00,479
for example c510 a moderately

143
00:04:58,479 --> 00:05:03,758
complicated image

144
00:05:00,479 --> 00:05:06,080
classification data set with 250 queries

145
00:05:03,759 --> 00:05:07,199
using just the label data set you can

146
00:05:06,080 --> 00:05:10,560
get around 54

147
00:05:07,199 --> 00:05:12,240
accuracy using the unlabeled data

148
00:05:10,560 --> 00:05:14,080
with semi supervised learning you get

149
00:05:12,240 --> 00:05:17,440
around 88

150
00:05:14,080 --> 00:05:19,758
huge gains however

151
00:05:17,440 --> 00:05:22,800
learning based approaches have a

152
00:05:19,759 --> 00:05:24,720
fundamental limit in non-determinism

153
00:05:22,800 --> 00:05:26,400
consider the following setting alice

154
00:05:24,720 --> 00:05:28,560
queries on her for her data

155
00:05:26,400 --> 00:05:29,919
and then trains two different models

156
00:05:28,560 --> 00:05:31,199
these models differ only in their

157
00:05:29,919 --> 00:05:33,120
non-determinism

158
00:05:31,199 --> 00:05:35,600
the non-determinism in learning is the

159
00:05:33,120 --> 00:05:38,560
initialization you start training from

160
00:05:35,600 --> 00:05:39,199
random batch order and std and gpu

161
00:05:38,560 --> 00:05:43,440
non-deter

162
00:05:39,199 --> 00:05:46,560
determinism floating point errors

163
00:05:43,440 --> 00:05:48,560
in the order of additions add up

164
00:05:46,560 --> 00:05:50,960
it turns out that if you control for

165
00:05:48,560 --> 00:05:54,000
everything but gpu non-determinism

166
00:05:50,960 --> 00:05:55,758
the two models end up being on

167
00:05:54,000 --> 00:05:57,840
on fashion mnist if you if you train

168
00:05:55,759 --> 00:06:00,160
these two you end up with two models

169
00:05:57,840 --> 00:06:01,198
that differ on six percent of the test

170
00:06:00,160 --> 00:06:02,560
set

171
00:06:01,199 --> 00:06:04,240
controlling everything but gpu

172
00:06:02,560 --> 00:06:07,039
non-determinism

173
00:06:04,240 --> 00:06:08,639
so um alice could not have gotten

174
00:06:07,039 --> 00:06:10,560
fidelity on these six percent of

175
00:06:08,639 --> 00:06:12,960
examples

176
00:06:10,560 --> 00:06:14,720
in order to overcome this we turn to

177
00:06:12,960 --> 00:06:16,638
direct recovery

178
00:06:14,720 --> 00:06:18,560
the linear model approach that i

179
00:06:16,639 --> 00:06:20,960
discussed earlier doesn't really extend

180
00:06:18,560 --> 00:06:23,440
to neural networks but fortunately

181
00:06:20,960 --> 00:06:25,039
milli at all at fat star 2018 show how

182
00:06:23,440 --> 00:06:26,160
to do this with two layer value

183
00:06:25,039 --> 00:06:28,479
notebooks

184
00:06:26,160 --> 00:06:30,319
a value if you're not aware takes the

185
00:06:28,479 --> 00:06:32,719
input and if it's positive

186
00:06:30,319 --> 00:06:35,360
does nothing to it and if it's negative

187
00:06:32,720 --> 00:06:37,520
squishes it up to zero

188
00:06:35,360 --> 00:06:40,880
for this i'm going to describe kind of

189
00:06:37,520 --> 00:06:43,599
the main idea of the milly at all work

190
00:06:40,880 --> 00:06:44,800
and a little bit of how we improve on it

191
00:06:43,600 --> 00:06:48,720
but if you want more details you're

192
00:06:44,800 --> 00:06:50,160
going to have to see the paper so

193
00:06:48,720 --> 00:06:51,840
to start we need to get a little bit of

194
00:06:50,160 --> 00:06:53,360
intuition for how a rally network

195
00:06:51,840 --> 00:06:56,318
operates

196
00:06:53,360 --> 00:06:56,960
so a value splits input space into two

197
00:06:56,319 --> 00:06:59,360
pieces

198
00:06:56,960 --> 00:07:00,880
the place where the input's positive and

199
00:06:59,360 --> 00:07:02,560
the place where the input to the value

200
00:07:00,880 --> 00:07:05,039
is negative

201
00:07:02,560 --> 00:07:07,599
because it's a piecewise linear function

202
00:07:05,039 --> 00:07:10,159
so this means if you have k values

203
00:07:07,599 --> 00:07:11,599
you have 2 to the k regions of input

204
00:07:10,160 --> 00:07:15,599
space

205
00:07:11,599 --> 00:07:16,639
and so this if you tried to brute force

206
00:07:15,599 --> 00:07:19,280
this it would be

207
00:07:16,639 --> 00:07:20,479
very inefficient the key idea of the

208
00:07:19,280 --> 00:07:22,159
melee at all work

209
00:07:20,479 --> 00:07:24,159
is that you don't need the brute force

210
00:07:22,160 --> 00:07:26,160
if you find a boundary

211
00:07:24,160 --> 00:07:28,560
where a single value changes from

212
00:07:26,160 --> 00:07:30,720
positive to negative

213
00:07:28,560 --> 00:07:32,160
then observing the difference in the

214
00:07:30,720 --> 00:07:34,880
functional behavior

215
00:07:32,160 --> 00:07:35,440
in these two regions gives you the

216
00:07:34,880 --> 00:07:39,840
weights

217
00:07:35,440 --> 00:07:41,919
of the value so in order to do this

218
00:07:39,840 --> 00:07:43,359
first we need to find the boundary i'm

219
00:07:41,919 --> 00:07:45,440
not going to describe how to do that but

220
00:07:43,360 --> 00:07:47,520
it's a fairly simple procedure

221
00:07:45,440 --> 00:07:50,000
and now i'll try to explain how you

222
00:07:47,520 --> 00:07:54,240
recover these weights

223
00:07:50,000 --> 00:07:56,639
so what you do is here we have

224
00:07:54,240 --> 00:07:58,479
one boundary the first value changes

225
00:07:56,639 --> 00:08:00,639
from on to off

226
00:07:58,479 --> 00:08:02,800
on the left hand side consider what

227
00:08:00,639 --> 00:08:04,960
happens when we change the first value

228
00:08:02,800 --> 00:08:06,240
by a little bit because of this

229
00:08:04,960 --> 00:08:09,359
perturbation

230
00:08:06,240 --> 00:08:12,800
um this this perturbation will propagate

231
00:08:09,360 --> 00:08:13,599
to both the values and then the rallies

232
00:08:12,800 --> 00:08:16,960
are both on

233
00:08:13,599 --> 00:08:19,120
so their changes propagate to the output

234
00:08:16,960 --> 00:08:21,039
in the other example on the other side

235
00:08:19,120 --> 00:08:24,319
where the the first value is

236
00:08:21,039 --> 00:08:26,159
off the same change propagates to both

237
00:08:24,319 --> 00:08:28,080
the values just the same

238
00:08:26,160 --> 00:08:29,360
but because the rally's off it doesn't

239
00:08:28,080 --> 00:08:34,079
propagate through it

240
00:08:29,360 --> 00:08:36,159
and so the output behavior only matters

241
00:08:34,080 --> 00:08:37,120
based on the second value and so what

242
00:08:36,159 --> 00:08:39,919
you can do

243
00:08:37,120 --> 00:08:40,399
is you can subtract these contributions

244
00:08:39,919 --> 00:08:42,478
and

245
00:08:40,399 --> 00:08:43,599
get the behavior of the first value with

246
00:08:42,479 --> 00:08:46,399
this input

247
00:08:43,599 --> 00:08:48,880
you do this for all inputs all values

248
00:08:46,399 --> 00:08:50,240
you recover all the weights

249
00:08:48,880 --> 00:08:52,320
it's a little more complicated than this

250
00:08:50,240 --> 00:08:55,279
but i'm again you're going to have to go

251
00:08:52,320 --> 00:08:57,279
to the paper for more details

252
00:08:55,279 --> 00:08:58,800
so our improvements are to make this

253
00:08:57,279 --> 00:09:01,519
work in practice

254
00:08:58,800 --> 00:09:02,479
milly at all was a proposed in theory so

255
00:09:01,519 --> 00:09:04,560
it required

256
00:09:02,480 --> 00:09:06,160
real valued arithmetic and it doesn't

257
00:09:04,560 --> 00:09:08,319
work if you move to

258
00:09:06,160 --> 00:09:10,480
floating point numbers so we made it

259
00:09:08,320 --> 00:09:13,600
work in floating point 64

260
00:09:10,480 --> 00:09:14,720
by improving the search and improving

261
00:09:13,600 --> 00:09:17,519
the precision

262
00:09:14,720 --> 00:09:19,120
of the the numerical algorithms and the

263
00:09:17,519 --> 00:09:22,480
search is the search for the

264
00:09:19,120 --> 00:09:25,760
boundaries and so at the end of the day

265
00:09:22,480 --> 00:09:27,120
you get um with 600 000 queries you can

266
00:09:25,760 --> 00:09:29,600
query an mnist model

267
00:09:27,120 --> 00:09:31,120
with 100 000 parameters and recover it

268
00:09:29,600 --> 00:09:34,320
with 99.98

269
00:09:31,120 --> 00:09:34,320
fidelity on the test set

270
00:09:34,839 --> 00:09:38,959
so uh we only got to touch a little bit

271
00:09:37,200 --> 00:09:41,200
on the the results of the paper

272
00:09:38,959 --> 00:09:42,160
but there's some cool things that um i

273
00:09:41,200 --> 00:09:44,640
didn't get to

274
00:09:42,160 --> 00:09:46,719
talk about my favorites are hardness

275
00:09:44,640 --> 00:09:49,279
results for model extraction

276
00:09:46,720 --> 00:09:50,240
a finer look at the non-determinism

277
00:09:49,279 --> 00:09:52,160
impact

278
00:09:50,240 --> 00:09:53,839
and hybrid attacks where you can use

279
00:09:52,160 --> 00:09:56,640
learning to improve the functionally

280
00:09:53,839 --> 00:09:56,640
equivalent attack

281
00:09:56,880 --> 00:10:00,560
our work really highlights a lot of open

282
00:09:58,880 --> 00:10:02,800
questions for future work

283
00:10:00,560 --> 00:10:03,680
including making more efficient

284
00:10:02,800 --> 00:10:06,560
realistic

285
00:10:03,680 --> 00:10:07,199
effective attacks um you can actually

286
00:10:06,560 --> 00:10:10,640
see

287
00:10:07,200 --> 00:10:11,440
icml or crypto 2020 there are

288
00:10:10,640 --> 00:10:14,880
improvements

289
00:10:11,440 --> 00:10:17,920
on on the functioning equivalent attacks

290
00:10:14,880 --> 00:10:19,839
that make them work for deeper networks

291
00:10:17,920 --> 00:10:22,800
and another thing is our taxonomy

292
00:10:19,839 --> 00:10:24,959
actually improves a lot uh or sorry

293
00:10:22,800 --> 00:10:26,319
allows us to consider defenses for model

294
00:10:24,959 --> 00:10:29,359
extraction

295
00:10:26,320 --> 00:10:30,640
for example now we can we have a

296
00:10:29,360 --> 00:10:32,160
reasonable question

297
00:10:30,640 --> 00:10:33,839
is functionally equivalent extraction

298
00:10:32,160 --> 00:10:37,199
easier to defend against than

299
00:10:33,839 --> 00:10:38,240
accuracy extraction and defenses can be

300
00:10:37,200 --> 00:10:40,880
designed for each

301
00:10:38,240 --> 00:10:43,200
goal and for the other parameters of the

302
00:10:40,880 --> 00:10:46,399
adversary as well

303
00:10:43,200 --> 00:10:47,360
with that come to my q a to ask me

304
00:10:46,399 --> 00:10:58,880
questions

305
00:10:47,360 --> 00:10:58,880
thank you

