1
00:00:08,559 --> 00:00:11,440
hey everyone

2
00:00:09,440 --> 00:00:12,719
my name is dan duca i'm a phd student at

3
00:00:11,440 --> 00:00:13,678
the university of maryland

4
00:00:12,719 --> 00:00:15,440
and today i'll be talking about some of

5
00:00:13,679 --> 00:00:17,600
the work that my colleagues and i in

6
00:00:15,440 --> 00:00:18,720
security privacy and people lab

7
00:00:17,600 --> 00:00:19,760
and the programming languages live at

8
00:00:18,720 --> 00:00:21,279
the university of maryland have been

9
00:00:19,760 --> 00:00:24,000
doing trying to understand the security

10
00:00:21,279 --> 00:00:27,279
mistakes that developers make

11
00:00:24,000 --> 00:00:28,960
so if you look at the um

12
00:00:27,279 --> 00:00:30,640
nist's common weakness enumeration

13
00:00:28,960 --> 00:00:32,079
reporting over the past few years you'll

14
00:00:30,640 --> 00:00:34,160
see that there

15
00:00:32,079 --> 00:00:35,120
are several vulnerabilities that we

16
00:00:34,160 --> 00:00:37,519
consider

17
00:00:35,120 --> 00:00:39,199
solved the developers still introduce

18
00:00:37,520 --> 00:00:40,960
and by solved we mean

19
00:00:39,200 --> 00:00:42,239
that the developer could have chosen a

20
00:00:40,960 --> 00:00:44,879
different language

21
00:00:42,239 --> 00:00:46,800
or api or something like that that would

22
00:00:44,879 --> 00:00:49,360
have just mitigated the problem for them

23
00:00:46,800 --> 00:00:50,879
there's actually a solution available

24
00:00:49,360 --> 00:00:53,840
this includes things like

25
00:00:50,879 --> 00:00:55,280
buffer overflows and proper access our

26
00:00:53,840 --> 00:00:55,920
information leakage and proper access

27
00:00:55,280 --> 00:00:58,960
control

28
00:00:55,920 --> 00:01:00,960
and cryptographic issues

29
00:00:58,960 --> 00:01:02,000
and so you might be asking yourself well

30
00:01:00,960 --> 00:01:03,840
why do developers

31
00:01:02,000 --> 00:01:05,519
continue to make stupid and lazy

32
00:01:03,840 --> 00:01:07,280
mistakes but we

33
00:01:05,519 --> 00:01:08,799
argue this is actually just the wrong

34
00:01:07,280 --> 00:01:09,280
question to ask instead we should be

35
00:01:08,799 --> 00:01:13,840
asking

36
00:01:09,280 --> 00:01:13,840
how do we make programming easier

37
00:01:14,080 --> 00:01:17,200
so there's several possible solutions

38
00:01:15,680 --> 00:01:19,439
you might think about well better

39
00:01:17,200 --> 00:01:20,400
education better apis better

40
00:01:19,439 --> 00:01:23,679
documentation

41
00:01:20,400 --> 00:01:25,200
better tools and on and on

42
00:01:23,680 --> 00:01:26,400
but really the question we wanted to ask

43
00:01:25,200 --> 00:01:29,600
is well how do we actually improve the

44
00:01:26,400 --> 00:01:31,600
effectiveness of these solutions

45
00:01:29,600 --> 00:01:33,199
in order to do this we really need to

46
00:01:31,600 --> 00:01:34,880
understand the types causes and

47
00:01:33,200 --> 00:01:37,119
pervasiveness the vulnerabilities we can

48
00:01:34,880 --> 00:01:40,158
match the right

49
00:01:37,119 --> 00:01:41,600
mitigation or the right tools to this

50
00:01:40,159 --> 00:01:43,040
problem

51
00:01:41,600 --> 00:01:45,119
so you might ask then well how do we

52
00:01:43,040 --> 00:01:48,399
actually measure this well

53
00:01:45,119 --> 00:01:49,920
um when deciding what method to use for

54
00:01:48,399 --> 00:01:51,360
this type of study we're really working

55
00:01:49,920 --> 00:01:54,399
on a spectrum

56
00:01:51,360 --> 00:01:57,600
from ecological validity over to control

57
00:01:54,399 --> 00:02:00,000
over uh the conditions and

58
00:01:57,600 --> 00:02:01,600
there's several methods that you might

59
00:02:00,000 --> 00:02:03,040
choose depending on sort of research

60
00:02:01,600 --> 00:02:04,559
questions you're asking

61
00:02:03,040 --> 00:02:06,719
with things like field measurements

62
00:02:04,560 --> 00:02:08,160
going and pulling data off of github

63
00:02:06,719 --> 00:02:09,758
or going and actually embedding in a

64
00:02:08,160 --> 00:02:10,639
company and performing observations

65
00:02:09,758 --> 00:02:11,599
there

66
00:02:10,639 --> 00:02:13,040
which will give you a lot of really

67
00:02:11,599 --> 00:02:14,000
great ecological validity you get to see

68
00:02:13,040 --> 00:02:16,239
really what's going on

69
00:02:14,000 --> 00:02:17,840
you see code that's produced in a real

70
00:02:16,239 --> 00:02:20,080
world situation

71
00:02:17,840 --> 00:02:21,520
but there's not really any control over

72
00:02:20,080 --> 00:02:24,640
that so it's hard to understand

73
00:02:21,520 --> 00:02:27,680
causality from it and on the other side

74
00:02:24,640 --> 00:02:30,160
you have lab studies where um

75
00:02:27,680 --> 00:02:32,000
you bring uh participant in they're

76
00:02:30,160 --> 00:02:33,200
performing a given task you have a lot

77
00:02:32,000 --> 00:02:35,200
of control over that

78
00:02:33,200 --> 00:02:37,040
you get causality but these tend to be

79
00:02:35,200 --> 00:02:39,040
smaller programs that they're working on

80
00:02:37,040 --> 00:02:40,720
and don't necessarily represent what

81
00:02:39,040 --> 00:02:43,599
you'd see in the real world

82
00:02:40,720 --> 00:02:44,319
and well both of these as we said are

83
00:02:43,599 --> 00:02:46,799
valid

84
00:02:44,319 --> 00:02:47,518
approaches to use uh we decided that we

85
00:02:46,800 --> 00:02:49,200
wanted to take

86
00:02:47,519 --> 00:02:50,879
sort of a middle ground approach with

87
00:02:49,200 --> 00:02:51,280
the introduction of the build to break

88
00:02:50,879 --> 00:02:52,959
it

89
00:02:51,280 --> 00:02:55,120
security competition or build a very

90
00:02:52,959 --> 00:02:56,640
fixated secure coding competition

91
00:02:55,120 --> 00:02:58,319
and so the way build a break at fix it

92
00:02:56,640 --> 00:03:00,399
works um

93
00:02:58,319 --> 00:03:02,238
is that we have teams of participants

94
00:03:00,400 --> 00:03:06,400
work together to

95
00:03:02,239 --> 00:03:09,840
build the program of medium size

96
00:03:06,400 --> 00:03:12,080
over the course of a few weeks and

97
00:03:09,840 --> 00:03:14,080
they're given several incentives to

98
00:03:12,080 --> 00:03:16,319
think about functionality performance

99
00:03:14,080 --> 00:03:17,680
and security and so in the first round

100
00:03:16,319 --> 00:03:20,399
in the build it round

101
00:03:17,680 --> 00:03:21,599
teams build according to a spec we give

102
00:03:20,400 --> 00:03:22,879
them several different functionality

103
00:03:21,599 --> 00:03:25,040
that they're supposed to implement

104
00:03:22,879 --> 00:03:26,480
as well as a threat model to consider

105
00:03:25,040 --> 00:03:28,159
during this process and they get points

106
00:03:26,480 --> 00:03:30,159
depending on

107
00:03:28,159 --> 00:03:32,000
how much of the functionality they

108
00:03:30,159 --> 00:03:34,239
provide and how performant their code

109
00:03:32,000 --> 00:03:36,080
is and this chart here just shows one of

110
00:03:34,239 --> 00:03:37,519
the competitions each of the teams the

111
00:03:36,080 --> 00:03:40,000
language they used and their

112
00:03:37,519 --> 00:03:41,440
score as it moves and then in the second

113
00:03:40,000 --> 00:03:43,040
round and the break ground

114
00:03:41,440 --> 00:03:44,720
teams are given access to each other's

115
00:03:43,040 --> 00:03:46,000
code and

116
00:03:44,720 --> 00:03:47,920
they go in and try to find

117
00:03:46,000 --> 00:03:49,200
vulnerabilities they produce exploits

118
00:03:47,920 --> 00:03:50,559
against them

119
00:03:49,200 --> 00:03:52,079
and then finally in the fix it round

120
00:03:50,560 --> 00:03:53,519
teams are allowed to go back and fix

121
00:03:52,080 --> 00:03:55,120
some of the problems that are identified

122
00:03:53,519 --> 00:03:58,159
by other teams in their code

123
00:03:55,120 --> 00:04:00,319
if you want a specific look at how this

124
00:03:58,159 --> 00:04:02,720
all works you can see our ccs paper

125
00:04:00,319 --> 00:04:04,238
from 2016 that goes into the details of

126
00:04:02,720 --> 00:04:06,239
build-a-break can fix it

127
00:04:04,239 --> 00:04:08,720
but this provides a nice sort of middle

128
00:04:06,239 --> 00:04:11,680
ground where we

129
00:04:08,720 --> 00:04:12,480
teams have to consider real world things

130
00:04:11,680 --> 00:04:14,959
like

131
00:04:12,480 --> 00:04:16,719
functionality and performance along with

132
00:04:14,959 --> 00:04:18,320
security and they're working on larger

133
00:04:16,720 --> 00:04:19,120
code and teams they get to pick their

134
00:04:18,320 --> 00:04:20,399
language

135
00:04:19,120 --> 00:04:22,720
but they're all working on the same

136
00:04:20,399 --> 00:04:26,239
problem set and we can um

137
00:04:22,720 --> 00:04:29,680
adjust the incentive structure to

138
00:04:26,240 --> 00:04:31,199
add different controls in so uh we've

139
00:04:29,680 --> 00:04:33,040
run this over four years

140
00:04:31,199 --> 00:04:34,960
over three different problem sets as

141
00:04:33,040 --> 00:04:37,199
profile storage and our communication

142
00:04:34,960 --> 00:04:38,239
and access control problem and so we

143
00:04:37,199 --> 00:04:39,520
wanted to look at this

144
00:04:38,240 --> 00:04:41,280
and see what were the types of

145
00:04:39,520 --> 00:04:42,000
vulnerabilities that developers actually

146
00:04:41,280 --> 00:04:44,239
introduced

147
00:04:42,000 --> 00:04:45,440
here we also looked at how severe the

148
00:04:44,240 --> 00:04:47,680
vulnerabilities were

149
00:04:45,440 --> 00:04:49,040
and how exploitable they were but for

150
00:04:47,680 --> 00:04:50,400
brevity i'm going to talk about the

151
00:04:49,040 --> 00:04:52,080
types of vulnerabilities introduced in

152
00:04:50,400 --> 00:04:55,280
this talk

153
00:04:52,080 --> 00:04:57,359
so to analyze this data we had

154
00:04:55,280 --> 00:04:59,198
researchers go in and actually look at

155
00:04:57,360 --> 00:05:00,800
uh the projects and looked at all the

156
00:04:59,199 --> 00:05:03,199
uh exploits that were submitted against

157
00:05:00,800 --> 00:05:04,880
them to try to understand

158
00:05:03,199 --> 00:05:06,479
what the vulnerability what the specific

159
00:05:04,880 --> 00:05:09,039
set of vulnerabilities were

160
00:05:06,479 --> 00:05:10,560
both that teams found as well as those

161
00:05:09,039 --> 00:05:11,199
they didn't find we go in there and

162
00:05:10,560 --> 00:05:14,479
perform an

163
00:05:11,199 --> 00:05:16,080
additional security review we did this

164
00:05:14,479 --> 00:05:16,639
review of all the vulnerabilities that

165
00:05:16,080 --> 00:05:18,400
we

166
00:05:16,639 --> 00:05:19,919
introduced we performed an iterative

167
00:05:18,400 --> 00:05:22,400
open coding where

168
00:05:19,919 --> 00:05:24,880
two of the researchers independently

169
00:05:22,400 --> 00:05:26,000
labeled all those vulnerabilities

170
00:05:24,880 --> 00:05:28,960
and then we compared them until we

171
00:05:26,000 --> 00:05:31,199
reached a high level of reliability

172
00:05:28,960 --> 00:05:33,280
and we performed this on a sample of 94

173
00:05:31,199 --> 00:05:36,160
projects with 866

174
00:05:33,280 --> 00:05:38,960
submitted exploits for those projects

175
00:05:36,160 --> 00:05:42,320
and that ended up being about 182

176
00:05:38,960 --> 00:05:43,840
uh vulnerabilities from both these 866

177
00:05:42,320 --> 00:05:45,199
submissions as well as ones identified

178
00:05:43,840 --> 00:05:47,198
by the researchers

179
00:05:45,199 --> 00:05:48,800
let's perform this qualitative analysis

180
00:05:47,199 --> 00:05:49,520
to identify all the vulnerabilities and

181
00:05:48,800 --> 00:05:50,960
code them

182
00:05:49,520 --> 00:05:53,039
and then we performed a quantitative

183
00:05:50,960 --> 00:05:55,440
analysis on the resulting data to

184
00:05:53,039 --> 00:05:56,880
determine the prevalence of those

185
00:05:55,440 --> 00:05:58,000
problems

186
00:05:56,880 --> 00:06:00,639
so what do we find what types of

187
00:05:58,000 --> 00:06:02,960
vulnerabilities developers introduce

188
00:06:00,639 --> 00:06:04,560
well we actually found that they could

189
00:06:02,960 --> 00:06:05,680
be grouped into sort of these three main

190
00:06:04,560 --> 00:06:09,520
classes with

191
00:06:05,680 --> 00:06:10,960
subclasses so first the the no

192
00:06:09,520 --> 00:06:14,560
implementation class

193
00:06:10,960 --> 00:06:15,680
here the team just didn't implement one

194
00:06:14,560 --> 00:06:16,720
of the security mitigations that was

195
00:06:15,680 --> 00:06:18,160
required given

196
00:06:16,720 --> 00:06:21,520
the goals that we set out for them in

197
00:06:18,160 --> 00:06:24,000
the specification

198
00:06:21,520 --> 00:06:25,359
this is broken into two subclasses first

199
00:06:24,000 --> 00:06:28,560
the intuitive class

200
00:06:25,360 --> 00:06:30,080
where teams just didn't

201
00:06:28,560 --> 00:06:31,759
implement something intuitive for

202
00:06:30,080 --> 00:06:33,440
example in a secure file storage problem

203
00:06:31,759 --> 00:06:34,960
they didn't implement any encryption

204
00:06:33,440 --> 00:06:38,080
or in the access control problem just

205
00:06:34,960 --> 00:06:40,159
not providing any access control

206
00:06:38,080 --> 00:06:41,599
the other subclass here is they didn't

207
00:06:40,160 --> 00:06:45,120
do something that was sort of

208
00:06:41,600 --> 00:06:47,039
not obvious from the the security goals

209
00:06:45,120 --> 00:06:49,360
so things like for the secure file

210
00:06:47,039 --> 00:06:51,280
storage not including an integrity check

211
00:06:49,360 --> 00:06:52,400
so they might understand that it needs

212
00:06:51,280 --> 00:06:54,638
encryption

213
00:06:52,400 --> 00:06:55,840
because it can otherwise just be read

214
00:06:54,639 --> 00:06:56,639
the file can just be read by the

215
00:06:55,840 --> 00:06:58,479
attacker

216
00:06:56,639 --> 00:06:59,680
on the device and plain text but they

217
00:06:58,479 --> 00:07:01,840
might not understand that it needs an

218
00:06:59,680 --> 00:07:03,120
integrity check

219
00:07:01,840 --> 00:07:04,479
the network communications problem is

220
00:07:03,120 --> 00:07:05,039
also all issues with side channel

221
00:07:04,479 --> 00:07:07,520
leakage

222
00:07:05,039 --> 00:07:10,560
and replay protection where replay

223
00:07:07,520 --> 00:07:13,919
prevention would fall into this category

224
00:07:10,560 --> 00:07:16,319
and the next class of vulnerabilities

225
00:07:13,919 --> 00:07:17,440
teams actually tried to implement one of

226
00:07:16,319 --> 00:07:19,360
these mitigations but

227
00:07:17,440 --> 00:07:22,400
had some kind of misunderstanding for

228
00:07:19,360 --> 00:07:25,199
example they might make a bad decision

229
00:07:22,400 --> 00:07:26,318
about the library or algorithm they used

230
00:07:25,199 --> 00:07:29,360
picking something that was

231
00:07:26,319 --> 00:07:30,400
known to be insecure so for the secure

232
00:07:29,360 --> 00:07:31,840
file storage problem

233
00:07:30,400 --> 00:07:33,758
this would include things like using a

234
00:07:31,840 --> 00:07:35,280
weak encryption algorithm or trying to

235
00:07:33,759 --> 00:07:38,639
roll their own encryption

236
00:07:35,280 --> 00:07:41,599
um or taking in uh input using

237
00:07:38,639 --> 00:07:42,240
store copy as opposed to a bounded copy

238
00:07:41,599 --> 00:07:44,159
the other sub

239
00:07:42,240 --> 00:07:48,000
category here is that they actually used

240
00:07:44,160 --> 00:07:48,000
a reasonable algorithm or library

241
00:07:48,160 --> 00:07:52,800
but they made some error in the

242
00:07:50,639 --> 00:07:55,039
implementation

243
00:07:52,800 --> 00:07:56,879
this could be things like using a fixed

244
00:07:55,039 --> 00:07:58,400
value so going back to that secure file

245
00:07:56,879 --> 00:07:59,680
storage problem they picked the right

246
00:07:58,400 --> 00:08:03,198
encryption algorithm using

247
00:07:59,680 --> 00:08:06,879
aes but this team actually used a

248
00:08:03,199 --> 00:08:07,840
fixed value for their initialization

249
00:08:06,879 --> 00:08:10,639
vector

250
00:08:07,840 --> 00:08:12,000
and actually if you go to stack overflow

251
00:08:10,639 --> 00:08:13,199
you'll see that this is actually an

252
00:08:12,000 --> 00:08:14,800
example of what we think is one of the

253
00:08:13,199 --> 00:08:18,000
teams just copy and paste this code

254
00:08:14,800 --> 00:08:18,879
unfortunately other examples of this

255
00:08:18,000 --> 00:08:20,879
include

256
00:08:18,879 --> 00:08:22,080
lacking sufficient randomness so for

257
00:08:20,879 --> 00:08:23,199
example if you're trying to provide a

258
00:08:22,080 --> 00:08:25,199
knots to prevent

259
00:08:23,199 --> 00:08:27,599
replay protection having that knots

260
00:08:25,199 --> 00:08:30,319
overflow after a short period of time

261
00:08:27,599 --> 00:08:31,919
or disabling protections in the library

262
00:08:30,319 --> 00:08:32,240
so as an example that one team actually

263
00:08:31,919 --> 00:08:33,919
had

264
00:08:32,240 --> 00:08:37,440
a really elegant approach where they

265
00:08:33,919 --> 00:08:38,640
used this sql cipher

266
00:08:37,440 --> 00:08:40,959
library that just handled all the

267
00:08:38,640 --> 00:08:42,319
encryption and the integrity checks for

268
00:08:40,958 --> 00:08:46,160
their file storage

269
00:08:42,320 --> 00:08:47,600
for them but to provide more performance

270
00:08:46,160 --> 00:08:49,279
the team realized that they could turn

271
00:08:47,600 --> 00:08:50,160
off per page integrity checks and when

272
00:08:49,279 --> 00:08:52,240
we looked at their git

273
00:08:50,160 --> 00:08:53,439
history we saw that they did this for

274
00:08:52,240 --> 00:08:55,760
performance purposes

275
00:08:53,440 --> 00:08:57,040
because they thought well these are per

276
00:08:55,760 --> 00:08:59,839
page integrity checks

277
00:08:57,040 --> 00:09:00,319
i have a full page integrity check i

278
00:08:59,839 --> 00:09:02,320
assume

279
00:09:00,320 --> 00:09:03,760
occurs so i'll just turn off the extra

280
00:09:02,320 --> 00:09:06,320
ones and we'll be good

281
00:09:03,760 --> 00:09:09,120
well actually this is the only integrity

282
00:09:06,320 --> 00:09:10,959
check and the team um

283
00:09:09,120 --> 00:09:12,480
was attacked and lost a ton of points

284
00:09:10,959 --> 00:09:13,599
because of this and actually if you go

285
00:09:12,480 --> 00:09:15,680
look at the documentation it's

286
00:09:13,600 --> 00:09:17,279
it's very unclear that that is the case

287
00:09:15,680 --> 00:09:18,160
uh if you if you read the documentation

288
00:09:17,279 --> 00:09:19,680
it's very

289
00:09:18,160 --> 00:09:23,279
uh it makes sense that the team made its

290
00:09:19,680 --> 00:09:25,839
mistake okay so uh the final class is

291
00:09:23,279 --> 00:09:26,959
these mistakes and this is basically

292
00:09:25,839 --> 00:09:28,720
just

293
00:09:26,959 --> 00:09:30,479
the team does something like make a

294
00:09:28,720 --> 00:09:32,000
control flow error putting a knot in the

295
00:09:30,480 --> 00:09:33,200
wrong place so the control goes the

296
00:09:32,000 --> 00:09:34,880
wrong direction

297
00:09:33,200 --> 00:09:36,000
or skipping a specific step in the

298
00:09:34,880 --> 00:09:37,360
algorithm they do everything right

299
00:09:36,000 --> 00:09:38,880
except for one step

300
00:09:37,360 --> 00:09:40,560
and it just looks like they forgot

301
00:09:38,880 --> 00:09:43,519
something small not a

302
00:09:40,560 --> 00:09:45,279
overall conceptual problem so which of

303
00:09:43,519 --> 00:09:48,000
these were actually the most prevalent

304
00:09:45,279 --> 00:09:48,399
well these numbers show the the percent

305
00:09:48,000 --> 00:09:50,720
of

306
00:09:48,399 --> 00:09:51,839
teams actually had these these problems

307
00:09:50,720 --> 00:09:55,200
and you can see that

308
00:09:51,839 --> 00:09:57,120
um mistakes were the least common and

309
00:09:55,200 --> 00:09:58,880
actually we perform statistical

310
00:09:57,120 --> 00:10:00,480
uh tests and show this is actually

311
00:09:58,880 --> 00:10:02,160
statistically significant

312
00:10:00,480 --> 00:10:04,240
uh these were statistically

313
00:10:02,160 --> 00:10:06,959
significantly least common

314
00:10:04,240 --> 00:10:08,720
and actually if when we looked at what

315
00:10:06,959 --> 00:10:11,040
teams did things like

316
00:10:08,720 --> 00:10:12,880
or perform best practices like not copy

317
00:10:11,040 --> 00:10:14,160
and pasting all of your security checks

318
00:10:12,880 --> 00:10:16,240
all over your code base but only having

319
00:10:14,160 --> 00:10:19,199
them in one place

320
00:10:16,240 --> 00:10:19,839
that also reduced the number of mistakes

321
00:10:19,200 --> 00:10:22,320
and

322
00:10:19,839 --> 00:10:23,519
in addition to that all of the mistakes

323
00:10:22,320 --> 00:10:25,279
were identified

324
00:10:23,519 --> 00:10:27,440
during the the break around except for

325
00:10:25,279 --> 00:10:28,000
one uh meaning the code review is likely

326
00:10:27,440 --> 00:10:31,440
helpful

327
00:10:28,000 --> 00:10:34,720
uh in preventing these now looking at

328
00:10:31,440 --> 00:10:35,920
uh the other sort of subclasses we found

329
00:10:34,720 --> 00:10:38,320
that the

330
00:10:35,920 --> 00:10:39,839
teams in most cases were able to

331
00:10:38,320 --> 00:10:41,200
identify the intuitive

332
00:10:39,839 --> 00:10:43,040
mitigations that were necessary but not

333
00:10:41,200 --> 00:10:46,160
all the unintuitive classes

334
00:10:43,040 --> 00:10:47,360
and that they were in most cases picked

335
00:10:46,160 --> 00:10:51,120
a reasonable algorithm

336
00:10:47,360 --> 00:10:53,760
but made some mistake in its notation

337
00:10:51,120 --> 00:10:55,440
so for recommendations we recommend that

338
00:10:53,760 --> 00:10:56,560
api design be simplified as much as

339
00:10:55,440 --> 00:10:59,120
possible

340
00:10:56,560 --> 00:11:01,119
building in the security primitives and

341
00:10:59,120 --> 00:11:03,839
for common use cases because teams are

342
00:11:01,120 --> 00:11:04,959
participants are able to identify what

343
00:11:03,839 --> 00:11:07,839
the mistakes are

344
00:11:04,959 --> 00:11:09,199
or sorry they're able to identify basic

345
00:11:07,839 --> 00:11:11,440
security mitigations they need but they

346
00:11:09,200 --> 00:11:13,279
might not know all the requirements

347
00:11:11,440 --> 00:11:14,880
it's also important whenever team or

348
00:11:13,279 --> 00:11:17,200
whenever developers go beyond

349
00:11:14,880 --> 00:11:19,040
sort of these defaults the documentation

350
00:11:17,200 --> 00:11:21,360
clearly states

351
00:11:19,040 --> 00:11:22,719
um what could go wrong with the security

352
00:11:21,360 --> 00:11:24,720
consequences are

353
00:11:22,720 --> 00:11:27,120
explaining these negative effects for

354
00:11:24,720 --> 00:11:28,079
turning off certain things for example

355
00:11:27,120 --> 00:11:30,320
and finally when it comes to

356
00:11:28,079 --> 00:11:31,839
vulnerability analysis tools more

357
00:11:30,320 --> 00:11:32,240
emphasis needs to be placed on sort of

358
00:11:31,839 --> 00:11:35,360
these

359
00:11:32,240 --> 00:11:36,079
high-level design conceptual issues so

360
00:11:35,360 --> 00:11:37,279
in summary

361
00:11:36,079 --> 00:11:39,439
we know that developers struggle with

362
00:11:37,279 --> 00:11:41,680
security concepts

363
00:11:39,440 --> 00:11:43,360
we know the mistakes happen they produce

364
00:11:41,680 --> 00:11:44,560
through code review and best practices

365
00:11:43,360 --> 00:11:46,240
and provide some

366
00:11:44,560 --> 00:11:48,000
conceptual or provide some

367
00:11:46,240 --> 00:11:49,200
recommendations for how to avoid some of

368
00:11:48,000 --> 00:11:51,839
these problems

369
00:11:49,200 --> 00:11:52,639
so with that i want to thank our

370
00:11:51,839 --> 00:11:55,120
sponsors

371
00:11:52,639 --> 00:11:58,480
and thank you all for for your time and

372
00:11:55,120 --> 00:11:58,480
i look forward to answering all of your

373
00:12:05,560 --> 00:12:08,560
questions

