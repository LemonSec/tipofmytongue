1
00:00:09,040 --> 00:00:13,040
hi everyone i'm suya and i'm going to

2
00:00:11,200 --> 00:00:14,479
present our paper on query efficient

3
00:00:13,040 --> 00:00:16,320
black box attacks

4
00:00:14,480 --> 00:00:18,240
this work is jointly with jim phong

5
00:00:16,320 --> 00:00:19,359
david and juan from university of

6
00:00:18,240 --> 00:00:20,799
virginia

7
00:00:19,359 --> 00:00:25,119
i'll first give an overview of the

8
00:00:20,800 --> 00:00:25,119
project and then introduce more details

9
00:00:27,680 --> 00:00:31,519
existing black box attacks can be

10
00:00:29,279 --> 00:00:33,600
categorized into optimization based and

11
00:00:31,519 --> 00:00:35,120
transfer based attacks

12
00:00:33,600 --> 00:00:38,399
optimization attacks are highly

13
00:00:35,120 --> 00:00:40,320
successful but also have high cost

14
00:00:38,399 --> 00:00:42,160
transfer attacks have low cost but also

15
00:00:40,320 --> 00:00:44,559
have low success rate

16
00:00:42,160 --> 00:00:46,480
so the question is can we combine the

17
00:00:44,559 --> 00:00:50,480
two attacks to get high success and

18
00:00:46,480 --> 00:00:52,239
low cost the answer is yes

19
00:00:50,480 --> 00:00:53,839
this table shows that our combined

20
00:00:52,239 --> 00:00:55,760
attack or formally

21
00:00:53,840 --> 00:00:57,440
the hybrid attack can significantly

22
00:00:55,760 --> 00:00:59,680
reduce the cost of the

23
00:00:57,440 --> 00:01:02,640
baseline optimization attack and also

24
00:00:59,680 --> 00:01:05,360
give higher success rates

25
00:01:02,640 --> 00:01:07,200
however existing black box attacks

26
00:01:05,360 --> 00:01:10,000
including the hybrid attack are all

27
00:01:07,200 --> 00:01:11,920
evaluated by the average cost

28
00:01:10,000 --> 00:01:14,320
but we empirically find that cost of

29
00:01:11,920 --> 00:01:16,159
different seeds can arise significantly

30
00:01:14,320 --> 00:01:17,679
this figure shows the cost distribution

31
00:01:16,159 --> 00:01:20,080
of attacking different images

32
00:01:17,680 --> 00:01:21,600
from the imagenet dataset and we see a

33
00:01:20,080 --> 00:01:23,439
large variance

34
00:01:21,600 --> 00:01:26,240
so if we can target those low-cost

35
00:01:23,439 --> 00:01:29,839
images then we can dramatically reduce

36
00:01:26,240 --> 00:01:32,560
the cost so can we identify the low-cost

37
00:01:29,840 --> 00:01:35,759
images in advance

38
00:01:32,560 --> 00:01:37,920
and the answer is again yes we propose a

39
00:01:35,759 --> 00:01:39,200
batch attack strategy to find low-cost

40
00:01:37,920 --> 00:01:41,840
images

41
00:01:39,200 --> 00:01:44,560
from the figuring table compared to the

42
00:01:41,840 --> 00:01:46,560
strategy without any prioritization

43
00:01:44,560 --> 00:01:48,720
our batch tech strategy can easily

44
00:01:46,560 --> 00:01:51,680
identify low cost images

45
00:01:48,720 --> 00:01:53,119
so we can find more editorial examples

46
00:01:51,680 --> 00:01:55,680
with same query budget

47
00:01:53,119 --> 00:01:57,600
as shown in the figure or find same

48
00:01:55,680 --> 00:02:01,680
number of additional examples with much

49
00:01:57,600 --> 00:02:01,679
less query cost as shown in the table

50
00:02:02,079 --> 00:02:07,119
rest of my talk is composed of two parts

51
00:02:05,040 --> 00:02:09,759
first i'll talk about how to combine the

52
00:02:07,119 --> 00:02:11,840
transfer and optimization attacks

53
00:02:09,758 --> 00:02:13,839
then i'll talk about how we can find the

54
00:02:11,840 --> 00:02:16,000
low cost images

55
00:02:13,840 --> 00:02:17,680
the hybrid match deck shows that we need

56
00:02:16,000 --> 00:02:19,360
to relax our assumptions made on

57
00:02:17,680 --> 00:02:20,879
realistic adversaries

58
00:02:19,360 --> 00:02:23,680
so that we can better estimate their

59
00:02:20,879 --> 00:02:25,679
actual cost

60
00:02:23,680 --> 00:02:26,959
next i'll introduce the threat model in

61
00:02:25,680 --> 00:02:29,040
the paper

62
00:02:26,959 --> 00:02:30,080
we assume address rates only have query

63
00:02:29,040 --> 00:02:31,599
access to the model

64
00:02:30,080 --> 00:02:33,599
and the model can provide predicted

65
00:02:31,599 --> 00:02:35,679
labels with confidence scores for the

66
00:02:33,599 --> 00:02:37,920
submitted queries

67
00:02:35,680 --> 00:02:39,920
however queries to the model are

68
00:02:37,920 --> 00:02:42,559
expensive because they're associated to

69
00:02:39,920 --> 00:02:45,040
some costs such as financial cost

70
00:02:42,560 --> 00:02:46,480
for three public image recognition apis

71
00:02:45,040 --> 00:02:48,879
with which only have

72
00:02:46,480 --> 00:02:51,119
query access the baseline attacks in the

73
00:02:48,879 --> 00:02:53,920
paper would in average cost

74
00:02:51,120 --> 00:02:56,239
40 to 120 dollars per degree example

75
00:02:53,920 --> 00:02:56,238
found

76
00:02:56,560 --> 00:02:59,840
then i'll introduce existing black box

77
00:02:58,800 --> 00:03:02,800
attacks

78
00:02:59,840 --> 00:03:03,680
the first is transfer attack in this

79
00:03:02,800 --> 00:03:06,480
attack

80
00:03:03,680 --> 00:03:07,680
the goal is to add imperceptible changes

81
00:03:06,480 --> 00:03:09,119
to the input c

82
00:03:07,680 --> 00:03:12,239
such that it can cause desired

83
00:03:09,120 --> 00:03:14,159
misclassification for the target model

84
00:03:12,239 --> 00:03:16,480
the perturbations are generated using

85
00:03:14,159 --> 00:03:19,440
information from the local model

86
00:03:16,480 --> 00:03:21,119
such as the gradient of local models and

87
00:03:19,440 --> 00:03:23,519
for the local models

88
00:03:21,120 --> 00:03:26,239
adversaries have complete access so the

89
00:03:23,519 --> 00:03:29,440
attack and local models

90
00:03:26,239 --> 00:03:29,440
are white box attacks

91
00:03:29,840 --> 00:03:33,840
this attack process then produces a

92
00:03:32,319 --> 00:03:35,280
local industrial example

93
00:03:33,840 --> 00:03:37,200
and we query the local additional

94
00:03:35,280 --> 00:03:39,040
example to the target model

95
00:03:37,200 --> 00:03:42,079
and if it successfully fools the target

96
00:03:39,040 --> 00:03:44,798
model we say it transfers

97
00:03:42,080 --> 00:03:46,959
however in harder attack settings the

98
00:03:44,799 --> 00:03:51,120
success rate or the transfer rate

99
00:03:46,959 --> 00:03:51,120
is usually very low for transfer attacks

100
00:03:51,440 --> 00:03:54,560
next i'll introduce the optimization

101
00:03:53,439 --> 00:03:57,120
attacks

102
00:03:54,560 --> 00:03:59,040
in this attack the goal is still to add

103
00:03:57,120 --> 00:04:00,000
imperceptible perturbation to the input

104
00:03:59,040 --> 00:04:01,519
seed

105
00:04:00,000 --> 00:04:04,080
such that it can cause desired

106
00:04:01,519 --> 00:04:06,720
misclassification for the target model

107
00:04:04,080 --> 00:04:08,480
but for convenience now we represent the

108
00:04:06,720 --> 00:04:11,040
desired misclassification of the target

109
00:04:08,480 --> 00:04:13,359
model by a target region

110
00:04:11,040 --> 00:04:15,599
the perturbation is generated using some

111
00:04:13,360 --> 00:04:17,199
xeros order optimization techniques

112
00:04:15,599 --> 00:04:19,199
with information directly from the

113
00:04:17,199 --> 00:04:21,040
target model

114
00:04:19,199 --> 00:04:22,960
one commonly used technique is using

115
00:04:21,040 --> 00:04:26,000
finite difference method to estimate the

116
00:04:22,960 --> 00:04:28,000
gradient of the black box target model

117
00:04:26,000 --> 00:04:31,919
the attack then produces an adversary

118
00:04:28,000 --> 00:04:31,919
example that lies in the target region

119
00:04:32,080 --> 00:04:36,080
to combine the two attacks we first do

120
00:04:34,400 --> 00:04:37,520
transfer attack and generate the local

121
00:04:36,080 --> 00:04:39,120
accessory example

122
00:04:37,520 --> 00:04:41,440
and then check if it transfers to the

123
00:04:39,120 --> 00:04:43,040
target model if it transfers

124
00:04:41,440 --> 00:04:45,040
then it lies in the target region and

125
00:04:43,040 --> 00:04:46,639
the attack is done but we are more

126
00:04:45,040 --> 00:04:47,919
interested in the case when it does not

127
00:04:46,639 --> 00:04:49,759
transfer

128
00:04:47,919 --> 00:04:52,159
instead of giving up when the transfer

129
00:04:49,759 --> 00:04:54,639
fails we start our optimization attack

130
00:04:52,160 --> 00:04:56,240
from the failed local reducer example

131
00:04:54,639 --> 00:04:57,680
rather than starting from the original

132
00:04:56,240 --> 00:05:00,320
input seed

133
00:04:57,680 --> 00:05:01,280
our hypothesis is the failed local

134
00:05:00,320 --> 00:05:03,120
industrial example

135
00:05:01,280 --> 00:05:04,719
is closer to the target region than the

136
00:05:03,120 --> 00:05:06,560
original input c

137
00:05:04,720 --> 00:05:08,400
so starting from there will improve the

138
00:05:06,560 --> 00:05:10,320
attack efficiency

139
00:05:08,400 --> 00:05:12,239
on the other hand during the

140
00:05:10,320 --> 00:05:13,919
optimization attack we will make lots of

141
00:05:12,240 --> 00:05:15,919
queries to the target model and get

142
00:05:13,919 --> 00:05:17,520
predicted labels for these queries from

143
00:05:15,919 --> 00:05:19,758
the target model

144
00:05:17,520 --> 00:05:21,919
so we collect these labeled queries and

145
00:05:19,759 --> 00:05:24,479
fine-tune the local models

146
00:05:21,919 --> 00:05:26,159
our hypothesis is these intermediate

147
00:05:24,479 --> 00:05:27,840
results contain decision boundary

148
00:05:26,160 --> 00:05:29,759
information of the target model

149
00:05:27,840 --> 00:05:31,198
and once the local models are fine-tuned

150
00:05:29,759 --> 00:05:32,800
on this collected data

151
00:05:31,199 --> 00:05:35,199
they will become more similar to the

152
00:05:32,800 --> 00:05:37,120
target model and the transferability of

153
00:05:35,199 --> 00:05:39,600
local models will be improved

154
00:05:37,120 --> 00:05:41,039
but empirically this direction does not

155
00:05:39,600 --> 00:05:42,560
work for a lot of data sets

156
00:05:41,039 --> 00:05:45,120
and i'll omit this part from the

157
00:05:42,560 --> 00:05:47,840
remaining talk next i'll present some

158
00:05:45,120 --> 00:05:50,720
experimental results

159
00:05:47,840 --> 00:05:52,560
as shown in the table in general for

160
00:05:50,720 --> 00:05:54,240
normally trained target models

161
00:05:52,560 --> 00:05:56,479
local reducer examples can help to

162
00:05:54,240 --> 00:05:58,479
significantly reduce the query cost

163
00:05:56,479 --> 00:05:59,919
of optimization attacks and also improve

164
00:05:58,479 --> 00:06:01,440
the success rate

165
00:05:59,919 --> 00:06:03,039
here the local models of the hybrid

166
00:06:01,440 --> 00:06:05,759
attack are some

167
00:06:03,039 --> 00:06:07,759
normally trained models we also tested

168
00:06:05,759 --> 00:06:08,960
attack that is released after our paper

169
00:06:07,759 --> 00:06:12,000
is accepted

170
00:06:08,960 --> 00:06:13,680
and the conclusion is the same so we

171
00:06:12,000 --> 00:06:17,759
also welcome researchers to test their

172
00:06:13,680 --> 00:06:17,759
new attacks using our hybrid framework

173
00:06:18,240 --> 00:06:21,840
but one exception occurs when we attack

174
00:06:20,479 --> 00:06:23,280
some target models

175
00:06:21,840 --> 00:06:25,840
that are trained to defend against

176
00:06:23,280 --> 00:06:27,359
adversary examples we call these models

177
00:06:25,840 --> 00:06:29,679
robust models

178
00:06:27,360 --> 00:06:31,199
from the table we can see that there is

179
00:06:29,680 --> 00:06:32,240
limited improvement with our hybrid

180
00:06:31,199 --> 00:06:34,240
attack

181
00:06:32,240 --> 00:06:35,840
we note that local models here are still

182
00:06:34,240 --> 00:06:39,520
standard models

183
00:06:35,840 --> 00:06:41,919
so what is the reason for this failure

184
00:06:39,520 --> 00:06:43,599
we hypothesized that the reason might be

185
00:06:41,919 --> 00:06:45,520
vulnerability spaces

186
00:06:43,600 --> 00:06:47,600
of the standard local models and robust

187
00:06:45,520 --> 00:06:50,318
target models are different

188
00:06:47,600 --> 00:06:50,880
to verify this we also trained some

189
00:06:50,319 --> 00:06:52,479
robust

190
00:06:50,880 --> 00:06:55,599
local models and tried all the

191
00:06:52,479 --> 00:06:58,240
combinations of local and target models

192
00:06:55,599 --> 00:06:58,880
and reported their results from the

193
00:06:58,240 --> 00:07:01,199
table

194
00:06:58,880 --> 00:07:02,080
you can see that the performance is best

195
00:07:01,199 --> 00:07:04,160
when the target

196
00:07:02,080 --> 00:07:05,120
and local models are of the same type

197
00:07:04,160 --> 00:07:07,440
namely

198
00:07:05,120 --> 00:07:10,800
robust target to robust local and

199
00:07:07,440 --> 00:07:12,880
standard target to standard local

200
00:07:10,800 --> 00:07:15,039
we also additionally tried mixing the

201
00:07:12,880 --> 00:07:16,800
robust and standard local models

202
00:07:15,039 --> 00:07:18,318
and see if the mixture models can work

203
00:07:16,800 --> 00:07:20,880
for both the robust

204
00:07:18,319 --> 00:07:23,280
and standard target models but it does

205
00:07:20,880 --> 00:07:23,280
not help

206
00:07:23,680 --> 00:07:28,400
so the main takeaway from hybrid attack

207
00:07:25,759 --> 00:07:29,039
is local field transfers can help to

208
00:07:28,400 --> 00:07:32,159
reduce

209
00:07:29,039 --> 00:07:32,639
the cost of optimization attacks but so

210
00:07:32,160 --> 00:07:34,400
far

211
00:07:32,639 --> 00:07:36,960
we are still talking about reducing the

212
00:07:34,400 --> 00:07:38,638
average cost

213
00:07:36,960 --> 00:07:41,359
recall that the tech cost of different

214
00:07:38,639 --> 00:07:43,199
images arise significantly

215
00:07:41,360 --> 00:07:46,240
and there are some low-cost images where

216
00:07:43,199 --> 00:07:48,479
the attacks can be very efficient

217
00:07:46,240 --> 00:07:51,680
then the question is how can we find

218
00:07:48,479 --> 00:07:51,680
those low-cost images

219
00:07:51,759 --> 00:07:55,840
to find those low-cost images we first

220
00:07:54,080 --> 00:07:56,800
check what information is available for

221
00:07:55,840 --> 00:07:58,799
us

222
00:07:56,800 --> 00:08:00,400
in the hybrid attack for the first phase

223
00:07:58,800 --> 00:08:02,000
we do the transfer attack

224
00:08:00,400 --> 00:08:03,440
so for each seed we know the

225
00:08:02,000 --> 00:08:05,280
corresponding number of attempts to

226
00:08:03,440 --> 00:08:07,759
attack the local models

227
00:08:05,280 --> 00:08:09,840
and our hypothesis is if you can find

228
00:08:07,759 --> 00:08:11,520
some local industrial examples easily

229
00:08:09,840 --> 00:08:13,440
then those seeds are more likely to

230
00:08:11,520 --> 00:08:15,440
transfer to the target model

231
00:08:13,440 --> 00:08:18,000
and our strategy is to prioritize seeds

232
00:08:15,440 --> 00:08:20,960
that use fewer local attack iterations

233
00:08:18,000 --> 00:08:21,840
and carry them first then in the second

234
00:08:20,960 --> 00:08:23,680
phase

235
00:08:21,840 --> 00:08:25,039
we do the optimization attacks for the

236
00:08:23,680 --> 00:08:27,360
field transfers

237
00:08:25,039 --> 00:08:28,639
in this phase since we already occurred

238
00:08:27,360 --> 00:08:30,240
each seed once

239
00:08:28,639 --> 00:08:32,080
we know the loss function values with

240
00:08:30,240 --> 00:08:34,399
respect to the target model

241
00:08:32,080 --> 00:08:35,440
and our hypothesis is seats that have

242
00:08:34,399 --> 00:08:37,440
lower loss values

243
00:08:35,440 --> 00:08:38,800
are closer to the target region and are

244
00:08:37,440 --> 00:08:40,880
easier to attack

245
00:08:38,799 --> 00:08:42,559
so the strategy is to prioritize seats

246
00:08:40,880 --> 00:08:44,800
that have lower loss values

247
00:08:42,559 --> 00:08:46,959
and attack them first then i'll show

248
00:08:44,800 --> 00:08:48,880
some experimental results

249
00:08:46,959 --> 00:08:50,399
in phase one we show the number of

250
00:08:48,880 --> 00:08:52,800
transfer industry examples with

251
00:08:50,399 --> 00:08:54,399
attempted queries for transferability

252
00:08:52,800 --> 00:08:55,760
this is the performance when seats are

253
00:08:54,399 --> 00:08:57,360
randomly queried without any

254
00:08:55,760 --> 00:08:58,800
prioritization

255
00:08:57,360 --> 00:09:00,560
and this is the performance when we

256
00:08:58,800 --> 00:09:02,399
prioritize seeds based on the number of

257
00:09:00,560 --> 00:09:04,800
local attack iterations

258
00:09:02,399 --> 00:09:06,839
obviously our strategy can easily

259
00:09:04,800 --> 00:09:09,839
discover transferable additional

260
00:09:06,839 --> 00:09:12,320
examples

261
00:09:09,839 --> 00:09:14,240
in phase 2 we show the number of

262
00:09:12,320 --> 00:09:15,360
additional examples found with different

263
00:09:14,240 --> 00:09:18,000
query budgets

264
00:09:15,360 --> 00:09:19,760
on the optimization attack this is the

265
00:09:18,000 --> 00:09:21,519
performance when we select and attack

266
00:09:19,760 --> 00:09:24,959
local industrial examples

267
00:09:21,519 --> 00:09:26,720
without any prioritization this is the

268
00:09:24,959 --> 00:09:28,880
performance when we continue to

269
00:09:26,720 --> 00:09:31,440
prioritize the field transfers

270
00:09:28,880 --> 00:09:32,880
using the local attack iterations

271
00:09:31,440 --> 00:09:35,120
obviously it is much better than the

272
00:09:32,880 --> 00:09:37,439
random strategy

273
00:09:35,120 --> 00:09:38,959
finally this is the performance when we

274
00:09:37,440 --> 00:09:41,920
prioritize the seats

275
00:09:38,959 --> 00:09:43,518
based on the loss function value we see

276
00:09:41,920 --> 00:09:45,279
that it is even better than the local

277
00:09:43,519 --> 00:09:47,120
attack iteration strategy

278
00:09:45,279 --> 00:09:49,600
because we are now using information

279
00:09:47,120 --> 00:09:51,680
from the target model

280
00:09:49,600 --> 00:09:53,600
so the best prioritization strategy is

281
00:09:51,680 --> 00:09:54,399
to use local attack iteration in phase

282
00:09:53,600 --> 00:09:56,880
one

283
00:09:54,399 --> 00:10:00,399
and loss function value in phase two and

284
00:09:56,880 --> 00:10:00,399
we call it two phase strategy

285
00:10:00,880 --> 00:10:04,480
now let's check the performance when the

286
00:10:02,399 --> 00:10:04,959
two phases are combined and we use the

287
00:10:04,480 --> 00:10:08,160
best

288
00:10:04,959 --> 00:10:10,079
tool-free strategy for prioritization

289
00:10:08,160 --> 00:10:11,680
this is the performance when all images

290
00:10:10,079 --> 00:10:13,760
are randomly ordered

291
00:10:11,680 --> 00:10:15,760
here the random means if a randomly

292
00:10:13,760 --> 00:10:18,319
selected seed directly transfers then

293
00:10:15,760 --> 00:10:20,079
the attack is done for that c and if it

294
00:10:18,320 --> 00:10:21,360
does not transfer then we will continue

295
00:10:20,079 --> 00:10:24,399
to attack that seed

296
00:10:21,360 --> 00:10:26,480
with optimization attack

297
00:10:24,399 --> 00:10:28,160
this is an ideal strategy that adversary

298
00:10:26,480 --> 00:10:30,720
can prioritize seeds based on their

299
00:10:28,160 --> 00:10:32,719
actual query cost retroactively

300
00:10:30,720 --> 00:10:35,200
so this line can serve as an idealistic

301
00:10:32,720 --> 00:10:36,640
upper bound for our strategy

302
00:10:35,200 --> 00:10:38,720
and this is the performance of our

303
00:10:36,640 --> 00:10:39,839
two-phase strategy it is obvious that

304
00:10:38,720 --> 00:10:42,079
our strategy

305
00:10:39,839 --> 00:10:43,839
is much better than the random one and

306
00:10:42,079 --> 00:10:45,519
is very close to the retroactive optimal

307
00:10:43,839 --> 00:10:47,120
one

308
00:10:45,519 --> 00:10:48,880
quantitatively to attack different

309
00:10:47,120 --> 00:10:50,640
fraction of thousand images

310
00:10:48,880 --> 00:10:52,320
the cost of our two-phase strategy is

311
00:10:50,640 --> 00:10:53,360
also very close to the retroactive

312
00:10:52,320 --> 00:10:55,040
optimal one

313
00:10:53,360 --> 00:10:58,560
and the cost is orders of magnitude

314
00:10:55,040 --> 00:10:58,560
smaller than the random strategy

315
00:10:58,640 --> 00:11:03,439
we have open sourced our implementation

316
00:11:00,880 --> 00:11:05,040
and it is also artifact evaluated

317
00:11:03,440 --> 00:11:07,920
and we welcome researchers to test their

318
00:11:05,040 --> 00:11:10,240
new attacks using our framework

319
00:11:07,920 --> 00:11:11,760
the final takeaway from the paper is

320
00:11:10,240 --> 00:11:13,519
understanding the cost of an attack

321
00:11:11,760 --> 00:11:14,720
requires considering more realistic

322
00:11:13,519 --> 00:11:16,000
adversaries

323
00:11:14,720 --> 00:11:18,000
in this figure we demonstrate

324
00:11:16,000 --> 00:11:19,680
sophisticated attackers that can

325
00:11:18,000 --> 00:11:22,240
combine different attacks and also

326
00:11:19,680 --> 00:11:24,479
target easy seats to reduce the cost

327
00:11:22,240 --> 00:11:26,560
and with this i'll conclude my talk and

328
00:11:24,480 --> 00:11:29,839
i'm happy to take questions in the q a

329
00:11:26,560 --> 00:11:29,839
session or offline

