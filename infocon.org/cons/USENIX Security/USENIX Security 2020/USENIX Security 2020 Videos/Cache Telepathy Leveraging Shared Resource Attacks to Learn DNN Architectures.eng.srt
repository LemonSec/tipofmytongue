1
00:00:08,880 --> 00:00:12,880
hi

2
00:00:09,519 --> 00:00:15,040
i'm mungja yang from mit i will present

3
00:00:12,880 --> 00:00:16,720
cash galaxy leveraging share the

4
00:00:15,040 --> 00:00:18,080
resource attacks to learn the end

5
00:00:16,720 --> 00:00:20,080
architectures

6
00:00:18,080 --> 00:00:22,320
this work is done with professor chris

7
00:00:20,080 --> 00:00:24,479
fletcher and professor joseph phillis

8
00:00:22,320 --> 00:00:27,439
from university of illinois at abnaj

9
00:00:24,480 --> 00:00:27,439
ubana champaign

10
00:00:28,320 --> 00:00:32,640
newspaper proposes a deep neural network

11
00:00:30,960 --> 00:00:35,040
privacy attack

12
00:00:32,640 --> 00:00:36,079
dn model extraction attack general

13
00:00:35,040 --> 00:00:38,640
involves first

14
00:00:36,079 --> 00:00:40,480
to steal the dn architecture and second

15
00:00:38,640 --> 00:00:41,760
to retrain a model using the stolen

16
00:00:40,480 --> 00:00:43,919
architecture

17
00:00:41,760 --> 00:00:45,519
so what is the architecture of a deep

18
00:00:43,920 --> 00:00:47,760
neural network

19
00:00:45,520 --> 00:00:49,840
intuitively the architecture gives the

20
00:00:47,760 --> 00:00:52,079
shape of the deep neural network

21
00:00:49,840 --> 00:00:54,000
more specifically it defines the number

22
00:00:52,079 --> 00:00:56,320
of layers of the neural network

23
00:00:54,000 --> 00:00:58,480
the type of each layer for each layer it

24
00:00:56,320 --> 00:01:00,320
also specifies the hyper parameters such

25
00:00:58,480 --> 00:01:03,839
as the number of neurons the number of

26
00:01:00,320 --> 00:01:06,799
filters and filter sizes etc

27
00:01:03,840 --> 00:01:07,600
however stealing dn's architecture is

28
00:01:06,799 --> 00:01:10,840
challenging

29
00:01:07,600 --> 00:01:12,000
mainly because dns have a multitude of

30
00:01:10,840 --> 00:01:15,040
hyperparameters

31
00:01:12,000 --> 00:01:17,119
this paper presents cache telepathy

32
00:01:15,040 --> 00:01:18,320
cache telepathy uses cache-based

33
00:01:17,119 --> 00:01:23,040
side-channel attacks

34
00:01:18,320 --> 00:01:23,039
to efficiently recover dn architectures

35
00:01:23,520 --> 00:01:28,399
to show how cache telepathy works let's

36
00:01:25,840 --> 00:01:30,400
start from the background

37
00:01:28,400 --> 00:01:32,400
the ants are widely used for many

38
00:01:30,400 --> 00:01:36,000
applications nowadays including image

39
00:01:32,400 --> 00:01:38,240
recognition and healthcare data analysis

40
00:01:36,000 --> 00:01:40,159
machine learning as a service is a

41
00:01:38,240 --> 00:01:42,479
popular framework that can provide

42
00:01:40,159 --> 00:01:43,920
end-to-end infrastructures for users to

43
00:01:42,479 --> 00:01:46,720
write machine learning code

44
00:01:43,920 --> 00:01:48,479
and deploy machine learning tasks the

45
00:01:46,720 --> 00:01:51,039
general use case is

46
00:01:48,479 --> 00:01:51,679
the cloud providers host the model and

47
00:01:51,040 --> 00:01:54,159
allow

48
00:01:51,680 --> 00:01:56,719
remote to untrust the users to pirate

49
00:01:54,159 --> 00:01:59,360
the model for a fee

50
00:01:56,719 --> 00:02:01,199
however from the security perspective

51
00:01:59,360 --> 00:02:02,880
machine learning as a service provides

52
00:02:01,200 --> 00:02:07,360
new ways to bridge

53
00:02:02,880 --> 00:02:09,359
the the privacy of the hosted dns

54
00:02:07,360 --> 00:02:12,000
machine learning as a service wraps the

55
00:02:09,360 --> 00:02:14,319
oracle model and only exposes the query

56
00:02:12,000 --> 00:02:16,879
apis to untrust the users

57
00:02:14,319 --> 00:02:18,079
however model extraction attacks can use

58
00:02:16,879 --> 00:02:20,879
the standard required

59
00:02:18,080 --> 00:02:22,239
apis to learn details of the black box

60
00:02:20,879 --> 00:02:24,720
model

61
00:02:22,239 --> 00:02:27,520
it trains a substitute model that works

62
00:02:24,720 --> 00:02:30,160
the same as a black box model

63
00:02:27,520 --> 00:02:31,840
moreover model extraction attacks can be

64
00:02:30,160 --> 00:02:33,760
the stepping stone for many other

65
00:02:31,840 --> 00:02:35,920
machine learning attacks such as

66
00:02:33,760 --> 00:02:37,599
membership inference attacks training

67
00:02:35,920 --> 00:02:41,679
parameter stealing attacks

68
00:02:37,599 --> 00:02:42,480
and so on well to make model extraction

69
00:02:41,680 --> 00:02:45,040
tech work

70
00:02:42,480 --> 00:02:46,640
one needs to steal the dn architecture

71
00:02:45,040 --> 00:02:48,840
first

72
00:02:46,640 --> 00:02:51,518
let's take a deeper look at the end

73
00:02:48,840 --> 00:02:53,680
architectures a dnn architecture

74
00:02:51,519 --> 00:02:54,400
specifies the number of layers of the

75
00:02:53,680 --> 00:02:56,879
dnn

76
00:02:54,400 --> 00:02:58,480
and the type for each layer whether it

77
00:02:56,879 --> 00:03:00,640
is a fully connected layer or a

78
00:02:58,480 --> 00:03:02,319
convolutional layer or pulling layer

79
00:03:00,640 --> 00:03:04,559
for different types of layers it

80
00:03:02,319 --> 00:03:06,720
specifies different hyper parameters

81
00:03:04,560 --> 00:03:08,560
for fully connected layer it specifies

82
00:03:06,720 --> 00:03:10,319
the number of neurons for the layer

83
00:03:08,560 --> 00:03:12,319
for a convolutional layer the filter

84
00:03:10,319 --> 00:03:14,079
size and the number of filters

85
00:03:12,319 --> 00:03:16,238
in addition it also specifies the

86
00:03:14,080 --> 00:03:20,319
connection between two layers

87
00:03:16,239 --> 00:03:22,000
for example a vgd16 has 16 layers with a

88
00:03:20,319 --> 00:03:23,280
mixture of convolutional and fully

89
00:03:22,000 --> 00:03:25,680
connected layers

90
00:03:23,280 --> 00:03:27,360
those fully convolutional layers operate

91
00:03:25,680 --> 00:03:29,040
on very different filter sizes and

92
00:03:27,360 --> 00:03:32,239
number of filters

93
00:03:29,040 --> 00:03:34,679
given 16 layers there could be 5

94
00:03:32,239 --> 00:03:35,840
trillion possible architectures and the

95
00:03:34,680 --> 00:03:38,640
vgg16

96
00:03:35,840 --> 00:03:40,239
is just one of them given the

97
00:03:38,640 --> 00:03:41,839
intractable search space

98
00:03:40,239 --> 00:03:44,080
it is unfeasible to obtain the

99
00:03:41,840 --> 00:03:47,360
architecture via brute force search

100
00:03:44,080 --> 00:03:50,319
or guess as another example we show a

101
00:03:47,360 --> 00:03:52,080
resnet module with five layers here

102
00:03:50,319 --> 00:03:53,599
those layers are not sequentially

103
00:03:52,080 --> 00:03:56,400
connected there

104
00:03:53,599 --> 00:03:57,359
exists a so-called shortcut connection

105
00:03:56,400 --> 00:03:59,439
from the top

106
00:03:57,360 --> 00:04:01,040
to the bottom of the module the

107
00:03:59,439 --> 00:04:02,799
non-sequential connection further

108
00:04:01,040 --> 00:04:04,640
complicates the reverse engineering

109
00:04:02,799 --> 00:04:07,280
process

110
00:04:04,640 --> 00:04:08,480
overall comparing to most of the prior

111
00:04:07,280 --> 00:04:11,120
side channel attack

112
00:04:08,480 --> 00:04:12,879
that ex that extract just the key for

113
00:04:11,120 --> 00:04:14,720
crypto algorithm

114
00:04:12,879 --> 00:04:19,358
cache telephony needs to extract a

115
00:04:14,720 --> 00:04:19,358
multitude of complicated parameters

116
00:04:19,680 --> 00:04:23,120
in this paper we have made the following

117
00:04:21,759 --> 00:04:25,600
contributions

118
00:04:23,120 --> 00:04:27,040
cache taxi can substantially reduce the

119
00:04:25,600 --> 00:04:29,440
source space of dn

120
00:04:27,040 --> 00:04:30,560
architectures for example when attacking

121
00:04:29,440 --> 00:04:32,639
vgd16

122
00:04:30,560 --> 00:04:33,600
we can reduce the search space from 5

123
00:04:32,639 --> 00:04:37,120
trillions to just

124
00:04:33,600 --> 00:04:38,960
16 architectures cache telepathy is

125
00:04:37,120 --> 00:04:41,919
based on the following insights

126
00:04:38,960 --> 00:04:43,919
first we identified that dn inference

127
00:04:41,919 --> 00:04:45,039
relies heavily on blocked matrix

128
00:04:43,919 --> 00:04:48,960
multiplication

129
00:04:45,040 --> 00:04:51,120
required to gen second we we provide a

130
00:04:48,960 --> 00:04:51,440
detailed security analysis of blocked

131
00:04:51,120 --> 00:04:54,720
jam

132
00:04:51,440 --> 00:04:56,240
implementation finally we use cache to

133
00:04:54,720 --> 00:04:58,320
base aside succinct attacks

134
00:04:56,240 --> 00:04:59,840
both flash reload and prime probe to

135
00:04:58,320 --> 00:05:01,919
extract the jam matrix

136
00:04:59,840 --> 00:05:04,400
parameters and reconstruct dn

137
00:05:01,919 --> 00:05:06,799
architectures

138
00:05:04,400 --> 00:05:07,919
note that the observations we have made

139
00:05:06,800 --> 00:05:09,600
in this paper

140
00:05:07,919 --> 00:05:11,280
apply to various machine learning

141
00:05:09,600 --> 00:05:13,520
frameworks and state-of-the-art

142
00:05:11,280 --> 00:05:14,559
loss libraries that implement block the

143
00:05:13,520 --> 00:05:16,880
gen

144
00:05:14,560 --> 00:05:18,080
therefore cache talent is general enough

145
00:05:16,880 --> 00:05:21,600
to work on most

146
00:05:18,080 --> 00:05:21,599
machine learning platforms

147
00:05:22,080 --> 00:05:26,320
to describe the mapping relationship

148
00:05:24,000 --> 00:05:28,400
between dn architectures and the matrix

149
00:05:26,320 --> 00:05:30,080
parameters i will use a fully connected

150
00:05:28,400 --> 00:05:32,000
layer as an example

151
00:05:30,080 --> 00:05:33,359
in a fully connected layer each output

152
00:05:32,000 --> 00:05:36,840
neuron is connected to

153
00:05:33,360 --> 00:05:40,479
all the input neurons how to compute

154
00:05:36,840 --> 00:05:40,880
output each output neuron is computed by

155
00:05:40,479 --> 00:05:42,800
first

156
00:05:40,880 --> 00:05:44,639
multiplying each input neuron with the

157
00:05:42,800 --> 00:05:47,680
corresponding weight on edge

158
00:05:44,639 --> 00:05:48,639
between the input and output neurons and

159
00:05:47,680 --> 00:05:52,000
the second

160
00:05:48,639 --> 00:05:54,160
adding up those products the computation

161
00:05:52,000 --> 00:05:55,919
basically involves money multiply and

162
00:05:54,160 --> 00:05:59,039
add operations

163
00:05:55,919 --> 00:06:01,599
this multiply and add operations can be

164
00:05:59,039 --> 00:06:03,919
implemented as matrix multiplication

165
00:06:01,600 --> 00:06:04,720
we can write the implied neurons as a

166
00:06:03,919 --> 00:06:07,039
vector

167
00:06:04,720 --> 00:06:08,479
the length of the vector is the number

168
00:06:07,039 --> 00:06:11,039
of neurons at the current

169
00:06:08,479 --> 00:06:11,919
layer generally the computation of a

170
00:06:11,039 --> 00:06:14,400
fully connected

171
00:06:11,919 --> 00:06:15,840
dnn is performed over a batch of a few

172
00:06:14,400 --> 00:06:18,479
inputs at a time

173
00:06:15,840 --> 00:06:19,599
so we can get multiple such vectors and

174
00:06:18,479 --> 00:06:24,318
stack them together

175
00:06:19,600 --> 00:06:26,240
into a matrix this is our input matrix

176
00:06:24,319 --> 00:06:28,560
we can organize all the weights as a

177
00:06:26,240 --> 00:06:30,479
matrix too the weights for one output

178
00:06:28,560 --> 00:06:32,160
neuron are gathered as a column

179
00:06:30,479 --> 00:06:34,719
therefore the number of columns in the

180
00:06:32,160 --> 00:06:35,520
weight matrix is equal to the number of

181
00:06:34,720 --> 00:06:38,639
neurons at

182
00:06:35,520 --> 00:06:40,400
the next layer with this we have mapped

183
00:06:38,639 --> 00:06:43,600
the computation of fully connected

184
00:06:40,400 --> 00:06:45,039
layers as a matrix multiplication

185
00:06:43,600 --> 00:06:48,240
we can summarize the mapping

186
00:06:45,039 --> 00:06:50,080
relationship in this in this table

187
00:06:48,240 --> 00:06:52,479
since the computation of each layer

188
00:06:50,080 --> 00:06:54,800
triggers exactly one gem core so the

189
00:06:52,479 --> 00:06:55,919
number of layers equals the number of

190
00:06:54,800 --> 00:06:58,720
gem calls

191
00:06:55,919 --> 00:06:59,758
and as we analyzed the number of neurons

192
00:06:58,720 --> 00:07:01,680
at each layer

193
00:06:59,759 --> 00:07:04,639
equals the number of columns in the

194
00:07:01,680 --> 00:07:04,639
input matrix

195
00:07:04,960 --> 00:07:09,359
you can apply the similar but slightly

196
00:07:07,039 --> 00:07:10,719
more complicated analysis to derive the

197
00:07:09,360 --> 00:07:12,639
mapping relationship

198
00:07:10,720 --> 00:07:13,759
for convolutional layers and pooling

199
00:07:12,639 --> 00:07:15,360
layers

200
00:07:13,759 --> 00:07:18,479
we would encourage you to refer to the

201
00:07:15,360 --> 00:07:18,479
paper for more details

202
00:07:19,120 --> 00:07:23,199
let's consider how to attack blocked

203
00:07:21,360 --> 00:07:25,680
matrix then

204
00:07:23,199 --> 00:07:27,919
blocked made blocked jam is aggressively

205
00:07:25,680 --> 00:07:31,199
optimized for multi-level caches using

206
00:07:27,919 --> 00:07:33,520
goto's algorithm more specifically

207
00:07:31,199 --> 00:07:34,400
block the jam is implemented as nestle

208
00:07:33,520 --> 00:07:36,479
loops

209
00:07:34,400 --> 00:07:38,239
each iteration of a loop handles one

210
00:07:36,479 --> 00:07:40,080
block on a certain dimension

211
00:07:38,240 --> 00:07:42,240
so we can use cache based sidechain

212
00:07:40,080 --> 00:07:43,120
attack to track the execution of the

213
00:07:42,240 --> 00:07:45,280
loops

214
00:07:43,120 --> 00:07:47,599
the number of iterations for each loop

215
00:07:45,280 --> 00:07:50,239
and infer the number of blocks for each

216
00:07:47,599 --> 00:07:50,240
dimension

217
00:07:50,639 --> 00:07:57,280
here is how the blocked jam works

218
00:07:53,840 --> 00:08:00,799
in order to multiply n by k matrix

219
00:07:57,280 --> 00:08:03,679
a and the k by n matrix b the algorithm

220
00:08:00,800 --> 00:08:06,240
tries to use a micro kernel on the p by

221
00:08:03,680 --> 00:08:08,000
q block and the q by r block

222
00:08:06,240 --> 00:08:09,360
this micro kernel is generally written

223
00:08:08,000 --> 00:08:12,080
in assembly code

224
00:08:09,360 --> 00:08:12,879
the block sizes are picked so that the p

225
00:08:12,080 --> 00:08:15,440
by what

226
00:08:12,879 --> 00:08:16,639
the the p by q block fits in the l2

227
00:08:15,440 --> 00:08:20,240
cache and the q by

228
00:08:16,639 --> 00:08:22,960
r block fits in the l3 cache note that

229
00:08:20,240 --> 00:08:23,599
the pqr and unroll which you will see

230
00:08:22,960 --> 00:08:26,318
shortly

231
00:08:23,599 --> 00:08:29,599
are block sizes they are either public

232
00:08:26,319 --> 00:08:31,840
or can be reverse engineered

233
00:08:29,599 --> 00:08:32,718
the nested loop is implemented as

234
00:08:31,840 --> 00:08:34,718
follows

235
00:08:32,719 --> 00:08:37,440
the algorithm first divides matrix b

236
00:08:34,719 --> 00:08:39,919
into multiple k by r blocks

237
00:08:37,440 --> 00:08:42,719
it corresponds to the outermost loop we

238
00:08:39,919 --> 00:08:42,718
call it loop one

239
00:08:42,880 --> 00:08:46,880
then it further divides matrix a into

240
00:08:45,519 --> 00:08:49,200
multiple n by q

241
00:08:46,880 --> 00:08:50,160
blocks and further divides each block in

242
00:08:49,200 --> 00:08:53,440
matrix b

243
00:08:50,160 --> 00:08:56,880
into q by r sub blocks this corresponds

244
00:08:53,440 --> 00:08:56,880
to the code of loop 2.

245
00:08:58,080 --> 00:09:01,839
moreover it further divides each block

246
00:09:00,640 --> 00:09:05,360
in matrix a

247
00:09:01,839 --> 00:09:08,160
into p by q sub blocks now we have the

248
00:09:05,360 --> 00:09:10,080
right size for the micro kernel

249
00:09:08,160 --> 00:09:12,399
before every invocation of the micro

250
00:09:10,080 --> 00:09:13,200
kernel the algorithm packs data from

251
00:09:12,399 --> 00:09:16,080
matrix a

252
00:09:13,200 --> 00:09:18,240
and matrix b into the two buffers for

253
00:09:16,080 --> 00:09:21,519
better locality

254
00:09:18,240 --> 00:09:25,120
thus loop three consists two parts

255
00:09:21,519 --> 00:09:28,399
in the first part the first sub block

256
00:09:25,120 --> 00:09:31,600
in matrix a is called to buffer a

257
00:09:28,399 --> 00:09:32,640
then the sub block in matrix b is copied

258
00:09:31,600 --> 00:09:37,040
to buffer b

259
00:09:32,640 --> 00:09:39,360
in the units of q by unroll stop blocks

260
00:09:37,040 --> 00:09:42,399
each copy operation is immediately

261
00:09:39,360 --> 00:09:46,160
followed by a computation operation

262
00:09:42,399 --> 00:09:48,560
this corresponds to loop four

263
00:09:46,160 --> 00:09:50,000
next the algorithm packs the rest of

264
00:09:48,560 --> 00:09:53,359
sub-blocks in matrix

265
00:09:50,000 --> 00:09:54,160
a to update buffer a and reuse the data

266
00:09:53,360 --> 00:09:57,680
in buffer b

267
00:09:54,160 --> 00:10:00,240
for the micro kernel these operations

268
00:09:57,680 --> 00:10:02,880
correspond to the rest iterations of

269
00:10:00,240 --> 00:10:02,880
loop 3.

270
00:10:03,279 --> 00:10:07,200
we can extract matrix parameters by

271
00:10:05,839 --> 00:10:10,560
counting the iterations

272
00:10:07,200 --> 00:10:12,320
of the for loops as analyzed and how to

273
00:10:10,560 --> 00:10:15,518
count the iterations

274
00:10:12,320 --> 00:10:18,399
we can identify the dynamic call graphs

275
00:10:15,519 --> 00:10:19,839
dcg for the three functions highlighted

276
00:10:18,399 --> 00:10:23,040
in the code snippet

277
00:10:19,839 --> 00:10:26,720
that is i t copy o n copy

278
00:10:23,040 --> 00:10:28,880
kernel the invocation of functions will

279
00:10:26,720 --> 00:10:32,560
follow the pattern as shown in this

280
00:10:28,880 --> 00:10:34,480
dcg the number of pairs of consecutive

281
00:10:32,560 --> 00:10:35,680
invocations of the oil and copy and

282
00:10:34,480 --> 00:10:37,839
kernel functions

283
00:10:35,680 --> 00:10:39,599
equals the number of iterations for loop

284
00:10:37,839 --> 00:10:41,519
four and similarly

285
00:10:39,600 --> 00:10:42,880
the number of pairs of consecutive

286
00:10:41,519 --> 00:10:45,040
invocations of the i t

287
00:10:42,880 --> 00:10:46,000
copy and kernel functions equals the

288
00:10:45,040 --> 00:10:49,120
number of

289
00:10:46,000 --> 00:10:51,519
iterations of loop three minus one

290
00:10:49,120 --> 00:10:53,519
and the number of occurrences of such a

291
00:10:51,519 --> 00:10:56,320
pattern equals the product of

292
00:10:53,519 --> 00:10:56,959
iteration one and iteration two

293
00:10:56,320 --> 00:10:58,959
therefore

294
00:10:56,959 --> 00:11:01,760
we can use site channel attack to

295
00:10:58,959 --> 00:11:06,079
monitor addresses in the three functions

296
00:11:01,760 --> 00:11:09,279
to determine the number of iterations

297
00:11:06,079 --> 00:11:12,319
note that openplus and intel mkl

298
00:11:09,279 --> 00:11:14,959
both follow this dcg meaning that

299
00:11:12,320 --> 00:11:18,000
our approach is applicable to both of

300
00:11:14,959 --> 00:11:18,000
these two libraries

301
00:11:18,640 --> 00:11:22,880
let's briefly go through the evaluation

302
00:11:20,560 --> 00:11:25,760
results we use cache based

303
00:11:22,880 --> 00:11:26,800
side channel attack to extract matrix

304
00:11:25,760 --> 00:11:29,680
parameters

305
00:11:26,800 --> 00:11:32,399
next we can use the matrix parameters to

306
00:11:29,680 --> 00:11:35,839
reconstruct the dn architectures

307
00:11:32,399 --> 00:11:39,680
we evaluate cache telepathy on resnet 50

308
00:11:35,839 --> 00:11:40,399
and vgd16 you can see that without cache

309
00:11:39,680 --> 00:11:43,439
telepathy

310
00:11:40,399 --> 00:11:46,399
the search space is intractable

311
00:11:43,440 --> 00:11:48,560
using cache telepathy most time we can

312
00:11:46,399 --> 00:11:50,480
significantly reduce the source space to

313
00:11:48,560 --> 00:11:54,000
a reasonable size

314
00:11:50,480 --> 00:11:57,440
outlier exists when using prime pro

315
00:11:54,000 --> 00:11:59,680
to attack mkl-based resnet 50.

316
00:11:57,440 --> 00:12:02,880
we provided a detailed detailed

317
00:11:59,680 --> 00:12:05,760
examination in the paper

318
00:12:02,880 --> 00:12:07,920
to conclude the takeaway messages from

319
00:12:05,760 --> 00:12:10,000
this paper is that

320
00:12:07,920 --> 00:12:12,240
cash-based side channel attacks can be

321
00:12:10,000 --> 00:12:14,240
used to efficiently recover dna

322
00:12:12,240 --> 00:12:16,399
architectures

323
00:12:14,240 --> 00:12:18,240
dn architectures even though have a

324
00:12:16,399 --> 00:12:20,959
multitude of parameters

325
00:12:18,240 --> 00:12:23,440
these parameters are all mapped to the

326
00:12:20,959 --> 00:12:25,599
gen parameters

327
00:12:23,440 --> 00:12:26,720
and the state-of-the-art blocked jam

328
00:12:25,600 --> 00:12:29,440
implementation

329
00:12:26,720 --> 00:12:39,200
can leak matrix parameters via cache

330
00:12:29,440 --> 00:12:41,279
access pattern

331
00:12:39,200 --> 00:12:41,279
you

