1
00:00:08,400 --> 00:00:11,440
hi my name is claus leno and i'll be

2
00:00:09,920 --> 00:00:13,200
presenting the paper stolen memories

3
00:00:11,440 --> 00:00:14,920
about white box membership inference

4
00:00:13,200 --> 00:00:17,038
and this work was done with matt

5
00:00:14,920 --> 00:00:18,480
frederickson so let me just start real

6
00:00:17,039 --> 00:00:19,520
quickly by defining what we mean by

7
00:00:18,480 --> 00:00:21,119
membership inference

8
00:00:19,520 --> 00:00:22,400
the basic idea is that an adversary is

9
00:00:21,119 --> 00:00:23,680
going to try to predict whether a given

10
00:00:22,400 --> 00:00:24,560
point was part of a target model's

11
00:00:23,680 --> 00:00:26,160
training set

12
00:00:24,560 --> 00:00:27,439
now while the adversary will obviously

13
00:00:26,160 --> 00:00:29,039
not have access to any of the target

14
00:00:27,439 --> 00:00:30,160
model's training data they will have the

15
00:00:29,039 --> 00:00:32,719
following

16
00:00:30,160 --> 00:00:33,760
access to some auxiliary data that's

17
00:00:32,719 --> 00:00:36,239
different from the target model's

18
00:00:33,760 --> 00:00:37,920
training data but distributed similarly

19
00:00:36,239 --> 00:00:39,360
they'll be given an even prior over

20
00:00:37,920 --> 00:00:40,879
members and non-members

21
00:00:39,360 --> 00:00:42,399
and they'll have access to the training

22
00:00:40,879 --> 00:00:43,280
algorithm used to produce the target

23
00:00:42,399 --> 00:00:44,800
model

24
00:00:43,280 --> 00:00:47,280
so the reason we care about membership

25
00:00:44,800 --> 00:00:49,360
inferences first membership itself may

26
00:00:47,280 --> 00:00:51,039
be sensitive for example i may not

27
00:00:49,360 --> 00:00:53,280
want somebody to know i contributed to a

28
00:00:51,039 --> 00:00:54,960
particular data set or study

29
00:00:53,280 --> 00:00:56,399
but it also serves as a practical

30
00:00:54,960 --> 00:00:57,120
measure of information leakage in

31
00:00:56,399 --> 00:00:58,399
general which

32
00:00:57,120 --> 00:00:59,760
may be able to be leveraged by a

33
00:00:58,399 --> 00:01:00,800
different adversary performing a

34
00:00:59,760 --> 00:01:02,480
different attack

35
00:01:00,800 --> 00:01:03,760
and finally it's directly related to

36
00:01:02,480 --> 00:01:05,199
differential privacy which makes

37
00:01:03,760 --> 00:01:08,080
membership inference interesting to

38
00:01:05,199 --> 00:01:09,520
study from the point of view of defenses

39
00:01:08,080 --> 00:01:11,200
so membership inference has actually

40
00:01:09,520 --> 00:01:12,960
been studied a lot in prior work

41
00:01:11,200 --> 00:01:14,560
and most approaches fit under the black

42
00:01:12,960 --> 00:01:16,240
box threat model where the adversary

43
00:01:14,560 --> 00:01:17,600
only has oracle access to the target

44
00:01:16,240 --> 00:01:19,199
model's outputs

45
00:01:17,600 --> 00:01:20,720
but there's also been a little bit of

46
00:01:19,200 --> 00:01:22,159
work on white box attacks where the

47
00:01:20,720 --> 00:01:23,600
adversary has access to the target

48
00:01:22,159 --> 00:01:24,960
model's learn parameters

49
00:01:23,600 --> 00:01:26,720
the problem we have with this work is

50
00:01:24,960 --> 00:01:28,320
that it requires a sort of non-standard

51
00:01:26,720 --> 00:01:28,880
threat model where an adversary also

52
00:01:28,320 --> 00:01:30,399
gets

53
00:01:28,880 --> 00:01:32,720
to see a portion of the target model's

54
00:01:30,400 --> 00:01:34,400
actual training data

55
00:01:32,720 --> 00:01:35,840
so the question is it seems like there's

56
00:01:34,400 --> 00:01:37,280
more information available that the

57
00:01:35,840 --> 00:01:38,240
adversary could use to do better with

58
00:01:37,280 --> 00:01:40,400
white box access

59
00:01:38,240 --> 00:01:42,399
even without access to the training data

60
00:01:40,400 --> 00:01:43,680
basically can we actually leverage this

61
00:01:42,399 --> 00:01:45,280
to make a better attack

62
00:01:43,680 --> 00:01:46,479
and of course i'm sure you guess i

63
00:01:45,280 --> 00:01:48,960
wouldn't be in front of you presenting

64
00:01:46,479 --> 00:01:51,360
today if the answer wasn't yes we can

65
00:01:48,960 --> 00:01:53,360
so in our work we show how to explicitly

66
00:01:51,360 --> 00:01:55,040
identify memorization in deep networks

67
00:01:53,360 --> 00:01:56,079
and how to weaponize it for membership

68
00:01:55,040 --> 00:01:58,159
inference

69
00:01:56,079 --> 00:02:00,000
moreover we find that our white box

70
00:01:58,159 --> 00:02:01,680
attacks can be more readily calibrated

71
00:02:00,000 --> 00:02:03,200
to increase the attacker's confidence

72
00:02:01,680 --> 00:02:04,880
and positive inferences

73
00:02:03,200 --> 00:02:06,719
and we also test our attack against some

74
00:02:04,880 --> 00:02:08,560
popular defenses to try to get a better

75
00:02:06,719 --> 00:02:09,038
understanding of their practicality and

76
00:02:08,560 --> 00:02:10,479
i'll be

77
00:02:09,038 --> 00:02:13,519
pointing out some interesting results

78
00:02:10,479 --> 00:02:15,040
there about differential privacy

79
00:02:13,520 --> 00:02:16,319
so now we've sort of laid the groundwork

80
00:02:15,040 --> 00:02:17,280
for membership inference i'm going to

81
00:02:16,319 --> 00:02:18,480
take you through some of the key

82
00:02:17,280 --> 00:02:20,239
insights from the paper

83
00:02:18,480 --> 00:02:21,599
so first we provide a more fundamental

84
00:02:20,239 --> 00:02:23,120
understanding of how overfitting

85
00:02:21,599 --> 00:02:23,760
actually manifests itself in a deep

86
00:02:23,120 --> 00:02:25,840
network

87
00:02:23,760 --> 00:02:27,519
and then i'll show how to use this to

88
00:02:25,840 --> 00:02:28,080
make a new type of membership inference

89
00:02:27,520 --> 00:02:29,520
attack

90
00:02:28,080 --> 00:02:30,800
and finally i'll highlight some of our

91
00:02:29,520 --> 00:02:33,599
key results and some interesting

92
00:02:30,800 --> 00:02:35,280
findings on defenses

93
00:02:33,599 --> 00:02:36,959
so the key intuition we use for

94
00:02:35,280 --> 00:02:38,640
developing our attack is that a model

95
00:02:36,959 --> 00:02:40,080
provides evidence of membership

96
00:02:38,640 --> 00:02:42,879
through the sometimes strange way it

97
00:02:40,080 --> 00:02:44,560
uses features so to just give a

98
00:02:42,879 --> 00:02:46,720
kind of cartoonish example let's say we

99
00:02:44,560 --> 00:02:47,040
have two pictures of two celebrities a

100
00:02:46,720 --> 00:02:48,959
and b

101
00:02:47,040 --> 00:02:50,400
that we're trying to recognize and

102
00:02:48,959 --> 00:02:51,920
you'll notice for both celebrities they

103
00:02:50,400 --> 00:02:54,640
have sunglasses and 25

104
00:02:51,920 --> 00:02:56,319
of their images but let's say we sample

105
00:02:54,640 --> 00:02:58,238
a training set and the way it works out

106
00:02:56,319 --> 00:02:58,799
is that celebrity a has sunglasses and

107
00:02:58,239 --> 00:03:01,120
50

108
00:02:58,800 --> 00:03:03,440
of their pictures while celebrity b is

109
00:03:01,120 --> 00:03:05,440
more typical with sunglasses and 25

110
00:03:03,440 --> 00:03:06,959
of their pictures so now if we're going

111
00:03:05,440 --> 00:03:08,959
to train a model on this

112
00:03:06,959 --> 00:03:10,640
the sunglasses are actually predictive

113
00:03:08,959 --> 00:03:12,239
for celebrity a since they show up more

114
00:03:10,640 --> 00:03:13,679
often for celebrity a than b

115
00:03:12,239 --> 00:03:15,519
so we might expect the model could

116
00:03:13,680 --> 00:03:17,519
encode this as a feature that uses that

117
00:03:15,519 --> 00:03:19,200
it uses to predict celebrity a

118
00:03:17,519 --> 00:03:21,200
so now when we pass an instance of

119
00:03:19,200 --> 00:03:22,560
celebrity a with sunglasses

120
00:03:21,200 --> 00:03:24,238
the sunglasses feature would get

121
00:03:22,560 --> 00:03:26,799
activated and then used to produce

122
00:03:24,239 --> 00:03:28,720
the output a and so the claim is that

123
00:03:26,799 --> 00:03:29,920
because the model used the sunglasses to

124
00:03:28,720 --> 00:03:31,840
predict celebrity a

125
00:03:29,920 --> 00:03:34,319
we can maybe think that sunglasses are

126
00:03:31,840 --> 00:03:36,159
more common than average for celebrity a

127
00:03:34,319 --> 00:03:37,920
sort of leaking that they're an unusual

128
00:03:36,159 --> 00:03:39,519
number of images of celebrity a with

129
00:03:37,920 --> 00:03:41,200
sunglasses in the training set

130
00:03:39,519 --> 00:03:42,480
so if you saw a picture of celebrity a

131
00:03:41,200 --> 00:03:45,280
with some glasses we might think it's

132
00:03:42,480 --> 00:03:47,440
more likely to be in the training set

133
00:03:45,280 --> 00:03:49,120
but so here's a slightly more realistic

134
00:03:47,440 --> 00:03:50,000
setting with an actual model on actual

135
00:03:49,120 --> 00:03:51,840
data

136
00:03:50,000 --> 00:03:53,599
uh so these are some training points

137
00:03:51,840 --> 00:03:54,000
from the labeled faces in the wild data

138
00:03:53,599 --> 00:03:55,518
set

139
00:03:54,000 --> 00:03:57,040
and we can see in the top right corner

140
00:03:55,519 --> 00:03:58,159
there's a picture of tony blair with a

141
00:03:57,040 --> 00:04:01,519
distinctive pink black

142
00:03:58,159 --> 00:04:02,560
background now if we take explanations

143
00:04:01,519 --> 00:04:04,640
on some typical

144
00:04:02,560 --> 00:04:05,920
test instances of tony blair where we

145
00:04:04,640 --> 00:04:07,920
highlight the pixels that are most

146
00:04:05,920 --> 00:04:09,679
relevant in making the classification

147
00:04:07,920 --> 00:04:11,439
we see that the model seems to focus on

148
00:04:09,680 --> 00:04:13,439
his face like we'd expect although

149
00:04:11,439 --> 00:04:14,799
there's no pink background here

150
00:04:13,439 --> 00:04:16,320
but on the training instance with the

151
00:04:14,799 --> 00:04:18,239
pink background we actually see the

152
00:04:16,320 --> 00:04:19,918
explanations highlighting the background

153
00:04:18,238 --> 00:04:21,279
so this is basically leaking the fact

154
00:04:19,918 --> 00:04:23,280
that there must be a training instance

155
00:04:21,279 --> 00:04:25,119
of tony blair with a pink background

156
00:04:23,280 --> 00:04:26,559
so if we know that's rare we might think

157
00:04:25,120 --> 00:04:28,639
this picture is more likely to be in the

158
00:04:26,560 --> 00:04:30,320
training set

159
00:04:28,639 --> 00:04:31,680
so the basic idea is that features that

160
00:04:30,320 --> 00:04:32,880
are distributed differently in the

161
00:04:31,680 --> 00:04:34,560
training data from how they're

162
00:04:32,880 --> 00:04:36,560
distributing the general population are

163
00:04:34,560 --> 00:04:37,840
going to provide evidence for membership

164
00:04:36,560 --> 00:04:40,800
and this is going to be apparent by how

165
00:04:37,840 --> 00:04:42,880
the model uses features

166
00:04:40,800 --> 00:04:44,160
so next we're going to formalize this

167
00:04:42,880 --> 00:04:45,759
intuition and

168
00:04:44,160 --> 00:04:47,199
show how to use it to construct an

169
00:04:45,759 --> 00:04:48,960
attack and we'll start with a simple

170
00:04:47,199 --> 00:04:50,720
case with linear models and then explain

171
00:04:48,960 --> 00:04:53,520
the key insights we use to make it work

172
00:04:50,720 --> 00:04:56,000
for deep networks

173
00:04:53,520 --> 00:04:57,680
so let's say we have two distributions a

174
00:04:56,000 --> 00:04:59,280
to star and ada hat and they might look

175
00:04:57,680 --> 00:05:02,000
something like this

176
00:04:59,280 --> 00:05:03,599
so now if we consider a point x prime x

177
00:05:02,000 --> 00:05:05,520
prime is more likely to have been drawn

178
00:05:03,600 --> 00:05:07,199
from a to hat than from a to star if the

179
00:05:05,520 --> 00:05:07,758
probability according to a to star of

180
00:05:07,199 --> 00:05:09,280
getting x

181
00:05:07,759 --> 00:05:11,360
prime is less than the probability of

182
00:05:09,280 --> 00:05:13,039
getting x prime according to eta hat

183
00:05:11,360 --> 00:05:14,479
so we can see here for example that

184
00:05:13,039 --> 00:05:15,840
there's a discrepancy so x

185
00:05:14,479 --> 00:05:17,520
prime would be more likely to come from

186
00:05:15,840 --> 00:05:19,359
a to hat in this case

187
00:05:17,520 --> 00:05:20,960
and so we can actually make a classifier

188
00:05:19,360 --> 00:05:21,759
that predicts whether x prime was drawn

189
00:05:20,960 --> 00:05:24,000
from a to star

190
00:05:21,759 --> 00:05:25,520
a to hat by making a decision brown

191
00:05:24,000 --> 00:05:26,639
boundary wherever the orange curve is

192
00:05:25,520 --> 00:05:28,400
above the blue curve

193
00:05:26,639 --> 00:05:29,759
so for this example it would would look

194
00:05:28,400 --> 00:05:32,400
like this

195
00:05:29,759 --> 00:05:33,120
uh and so to turn this into an attack we

196
00:05:32,400 --> 00:05:35,359
can think of a

197
00:05:33,120 --> 00:05:36,960
star as the general population or

198
00:05:35,360 --> 00:05:40,080
basically the non-members

199
00:05:36,960 --> 00:05:40,719
and ada hat as the training set and of

200
00:05:40,080 --> 00:05:42,479
course

201
00:05:40,720 --> 00:05:44,160
an attack won't be able to know the

202
00:05:42,479 --> 00:05:46,400
means and variances of a to star

203
00:05:44,160 --> 00:05:48,080
eta hat or even if they're gaussian but

204
00:05:46,400 --> 00:05:49,919
the key idea is that the weights from

205
00:05:48,080 --> 00:05:51,599
the target model can be captured can

206
00:05:49,919 --> 00:05:53,440
capture some of the relevant information

207
00:05:51,600 --> 00:05:54,960
so we can use this general idea to make

208
00:05:53,440 --> 00:05:55,759
an attack that's based on the weights of

209
00:05:54,960 --> 00:05:58,799
the model

210
00:05:55,759 --> 00:05:58,800
and we'll look at that next

211
00:05:59,039 --> 00:06:03,039
so the goal is we're going to sort of

212
00:06:01,360 --> 00:06:04,960
make a connection between the weights of

213
00:06:03,039 --> 00:06:07,280
the model and the training distribution

214
00:06:04,960 --> 00:06:09,120
and we'll do that using some simplifying

215
00:06:07,280 --> 00:06:10,400
assumptions that let us actually analyze

216
00:06:09,120 --> 00:06:12,639
things

217
00:06:10,400 --> 00:06:13,840
so we'll assume we have gaussian naive

218
00:06:12,639 --> 00:06:15,919
bayes data

219
00:06:13,840 --> 00:06:17,919
we'll also assume that we can model the

220
00:06:15,919 --> 00:06:20,240
training set as a gaussian distribution

221
00:06:17,919 --> 00:06:21,599
using its empirical mean and covariance

222
00:06:20,240 --> 00:06:23,280
and finally for now we'll consider

223
00:06:21,600 --> 00:06:25,520
linear target models

224
00:06:23,280 --> 00:06:27,198
and the key ideas are that first using

225
00:06:25,520 --> 00:06:29,039
these assumptions we can analytically

226
00:06:27,199 --> 00:06:30,880
derive the optimal attack from the means

227
00:06:29,039 --> 00:06:32,080
and covariances of the true and training

228
00:06:30,880 --> 00:06:34,400
distributions

229
00:06:32,080 --> 00:06:36,400
but we can also analytically derive the

230
00:06:34,400 --> 00:06:37,758
optimal weights of a linear classifier

231
00:06:36,400 --> 00:06:38,960
from the mean and covariance of the

232
00:06:37,759 --> 00:06:40,720
training distribution

233
00:06:38,960 --> 00:06:43,198
basically this is where sgd would

234
00:06:40,720 --> 00:06:44,960
converge and so what we can do is

235
00:06:43,199 --> 00:06:46,720
combine these and substitute in the

236
00:06:44,960 --> 00:06:48,000
weights for the distribution parameters

237
00:06:46,720 --> 00:06:49,360
to come up with an attack that's

238
00:06:48,000 --> 00:06:51,520
only in terms of the observable

239
00:06:49,360 --> 00:06:53,280
parameters of the model and basically

240
00:06:51,520 --> 00:06:54,880
we can use this attack whether or not

241
00:06:53,280 --> 00:06:56,479
these assumptions are met but under

242
00:06:54,880 --> 00:06:59,280
these assumptions an attack would be

243
00:06:56,479 --> 00:07:01,520
provably optimal

244
00:06:59,280 --> 00:07:03,359
so to summarize how our attack works

245
00:07:01,520 --> 00:07:04,880
first we train a proxy model on the

246
00:07:03,360 --> 00:07:06,720
auxiliary data

247
00:07:04,880 --> 00:07:08,560
then we compare the weights of the proxy

248
00:07:06,720 --> 00:07:10,080
model to those of the target model to

249
00:07:08,560 --> 00:07:11,599
create an attack model

250
00:07:10,080 --> 00:07:13,199
and basically the attack model is going

251
00:07:11,599 --> 00:07:14,800
to place a lot of weight on the features

252
00:07:13,199 --> 00:07:16,639
that are used differently in the target

253
00:07:14,800 --> 00:07:17,919
model than in the proxy model basically

254
00:07:16,639 --> 00:07:19,360
the features where the weights of the

255
00:07:17,919 --> 00:07:20,159
target model in the proxy model are

256
00:07:19,360 --> 00:07:21,280
really different

257
00:07:20,160 --> 00:07:24,400
and those are going to be the features

258
00:07:21,280 --> 00:07:25,919
that are used to determine membership

259
00:07:24,400 --> 00:07:27,599
so now we're going to show how to extend

260
00:07:25,919 --> 00:07:29,120
this to deep network since that's what

261
00:07:27,599 --> 00:07:30,639
we really care about

262
00:07:29,120 --> 00:07:32,560
and we'll do this by thinking of

263
00:07:30,639 --> 00:07:34,960
applying the attack we just covered to

264
00:07:32,560 --> 00:07:36,960
an individual layer of a deep network

265
00:07:34,960 --> 00:07:38,799
and what we do is we take what we call a

266
00:07:36,960 --> 00:07:39,680
slice of the network which partitions it

267
00:07:38,800 --> 00:07:42,000
into two parts

268
00:07:39,680 --> 00:07:44,000
h and g where you can think of h is

269
00:07:42,000 --> 00:07:46,879
computing some features to work with and

270
00:07:44,000 --> 00:07:48,400
g is classifying using those features

271
00:07:46,879 --> 00:07:48,879
and the key important ideas are that

272
00:07:48,400 --> 00:07:50,638
first

273
00:07:48,879 --> 00:07:52,160
the target model and the proxy model

274
00:07:50,639 --> 00:07:53,680
have to share the same internal

275
00:07:52,160 --> 00:07:55,360
representation so they're

276
00:07:53,680 --> 00:07:57,039
basically working on the same computed

277
00:07:55,360 --> 00:07:58,639
features and we think this is what's

278
00:07:57,039 --> 00:08:00,719
caused prior white box attacks to

279
00:07:58,639 --> 00:08:02,560
require access to some training data

280
00:08:00,720 --> 00:08:04,000
because they're based on transferring an

281
00:08:02,560 --> 00:08:05,280
attack from a model trained on the

282
00:08:04,000 --> 00:08:06,479
auxiliary data

283
00:08:05,280 --> 00:08:08,000
but there's no guarantee that the

284
00:08:06,479 --> 00:08:09,599
internal neurons in one model would

285
00:08:08,000 --> 00:08:10,319
represent the same features in another

286
00:08:09,599 --> 00:08:11,360
model

287
00:08:10,319 --> 00:08:12,960
which is why they need some of the

288
00:08:11,360 --> 00:08:14,960
target models real training data to make

289
00:08:12,960 --> 00:08:17,758
that connection

290
00:08:14,960 --> 00:08:19,280
uh then we also use a local linear

291
00:08:17,759 --> 00:08:21,120
approximation at each point we're

292
00:08:19,280 --> 00:08:21,679
attacking to approximate the top of the

293
00:08:21,120 --> 00:08:24,400
network

294
00:08:21,680 --> 00:08:26,400
g as a linear model so we can apply the

295
00:08:24,400 --> 00:08:27,599
attack described earlier and there are

296
00:08:26,400 --> 00:08:29,359
some different ways of getting this

297
00:08:27,599 --> 00:08:30,080
approximation which we discussed in the

298
00:08:29,360 --> 00:08:31,440
paper

299
00:08:30,080 --> 00:08:33,598
but for now you could think of one way

300
00:08:31,440 --> 00:08:36,000
as just a tangent line to the network

301
00:08:33,599 --> 00:08:37,760
so if you have a function like this the

302
00:08:36,000 --> 00:08:40,240
local linear approximation at point

303
00:08:37,760 --> 00:08:42,080
x would look like this and notice that

304
00:08:40,240 --> 00:08:43,599
the linear approximation is different at

305
00:08:42,080 --> 00:08:46,959
every point so we're not just replacing

306
00:08:43,599 --> 00:08:48,560
the whole network with a linear model

307
00:08:46,959 --> 00:08:50,479
and just note that we can make an attack

308
00:08:48,560 --> 00:08:52,160
on the on each layer of the network and

309
00:08:50,480 --> 00:08:53,519
combine them to get an attack on the

310
00:08:52,160 --> 00:08:55,920
whole network at once and the details

311
00:08:53,519 --> 00:08:57,760
for this are in the paper

312
00:08:55,920 --> 00:08:59,279
and finally i'd like to cover some of

313
00:08:57,760 --> 00:09:00,880
the key results from this work

314
00:08:59,279 --> 00:09:02,560
we'll briefly look at the performance of

315
00:09:00,880 --> 00:09:04,000
our attack and also touch on some

316
00:09:02,560 --> 00:09:07,199
results we had regarding differential

317
00:09:04,000 --> 00:09:08,959
privacy as a defense against our attack

318
00:09:07,200 --> 00:09:10,640
in terms of the performance of our

319
00:09:08,959 --> 00:09:11,760
attack our results could be summarized

320
00:09:10,640 --> 00:09:13,680
as follows

321
00:09:11,760 --> 00:09:15,600
so first we perform better than black

322
00:09:13,680 --> 00:09:16,959
box attacks both in terms of accuracy

323
00:09:15,600 --> 00:09:18,399
and in terms of precision which is

324
00:09:16,959 --> 00:09:20,079
basically the confidence we have on

325
00:09:18,399 --> 00:09:21,519
positive inferences

326
00:09:20,080 --> 00:09:23,360
and then we also have the ability to

327
00:09:21,519 --> 00:09:25,120
calibrate the attack for high precision

328
00:09:23,360 --> 00:09:26,640
and basically we argue that it's not so

329
00:09:25,120 --> 00:09:28,640
great to just like for example

330
00:09:26,640 --> 00:09:30,160
label pretty much everything as members

331
00:09:28,640 --> 00:09:31,519
but on the other hand if you had even a

332
00:09:30,160 --> 00:09:33,040
few points where you could be really

333
00:09:31,519 --> 00:09:35,440
sure that you identified them as members

334
00:09:33,040 --> 00:09:36,880
that would be a serious privacy concern

335
00:09:35,440 --> 00:09:39,760
and we found that previous attacks

336
00:09:36,880 --> 00:09:41,439
aren't really well calibrated like this

337
00:09:39,760 --> 00:09:43,040
and finally we can get good precision

338
00:09:41,440 --> 00:09:43,920
even on models that generalize really

339
00:09:43,040 --> 00:09:46,240
well

340
00:09:43,920 --> 00:09:48,319
like for example mnist has only one

341
00:09:46,240 --> 00:09:50,560
percent generalization error so not much

342
00:09:48,320 --> 00:09:52,080
overfitting in the sort of normal sense

343
00:09:50,560 --> 00:09:55,119
but we can get 75

344
00:09:52,080 --> 00:09:56,640
precision on our positive inferences and

345
00:09:55,120 --> 00:09:57,839
prior attacks really can't do much of

346
00:09:56,640 --> 00:10:00,959
anything on eminence since it doesn't

347
00:09:57,839 --> 00:10:02,480
really overfit much in terms of accuracy

348
00:10:00,959 --> 00:10:04,479
and so i don't have time to go into

349
00:10:02,480 --> 00:10:06,560
details on differential privacy

350
00:10:04,480 --> 00:10:08,399
but basically if you train a model with

351
00:10:06,560 --> 00:10:10,239
epsilon differential privacy then you

352
00:10:08,399 --> 00:10:11,839
get a guarantee that no adversary can

353
00:10:10,240 --> 00:10:13,279
achieve an accuracy greater than e to

354
00:10:11,839 --> 00:10:15,360
the epsilon over two

355
00:10:13,279 --> 00:10:17,120
and uh note that if epsilon is bigger

356
00:10:15,360 --> 00:10:19,440
than log two which is about point six

357
00:10:17,120 --> 00:10:21,040
nine you actually don't really have any

358
00:10:19,440 --> 00:10:23,680
meaningful guarantee since accuracy is

359
00:10:21,040 --> 00:10:25,760
always less than 100

360
00:10:23,680 --> 00:10:27,439
uh so when we tried differential privacy

361
00:10:25,760 --> 00:10:29,760
as a defense against our attack

362
00:10:27,440 --> 00:10:32,160
we found that as expected a small

363
00:10:29,760 --> 00:10:34,000
epsilon reduces the attack accuracy

364
00:10:32,160 --> 00:10:35,680
but it's really costly in terms of the

365
00:10:34,000 --> 00:10:37,440
actual model's accuracy

366
00:10:35,680 --> 00:10:39,120
when epsilon is really small the models

367
00:10:37,440 --> 00:10:40,079
were pretty much doing as well as random

368
00:10:39,120 --> 00:10:41,760
guessing

369
00:10:40,079 --> 00:10:43,680
so what a lot of people would want to do

370
00:10:41,760 --> 00:10:44,880
is use a bigger epsilon to get a more

371
00:10:43,680 --> 00:10:46,479
useful model

372
00:10:44,880 --> 00:10:48,240
and basically you could imagine maybe

373
00:10:46,480 --> 00:10:49,040
you still get some privacy benefits from

374
00:10:48,240 --> 00:10:50,640
this in practice

375
00:10:49,040 --> 00:10:52,959
even though the theoretical guarantee is

376
00:10:50,640 --> 00:10:54,880
not really there but basically

377
00:10:52,959 --> 00:10:56,319
what we found was a large epsilon

378
00:10:54,880 --> 00:10:57,920
sometimes works

379
00:10:56,320 --> 00:10:59,519
but actually often the attack starts to

380
00:10:57,920 --> 00:11:01,439
do pretty much how it done without the

381
00:10:59,519 --> 00:11:03,279
defense when you make epsilon big

382
00:11:01,440 --> 00:11:05,839
so for example on labeled faces in the

383
00:11:03,279 --> 00:11:06,959
wild when epsilon was raised to 16

384
00:11:05,839 --> 00:11:09,440
which is actually something that would

385
00:11:06,959 --> 00:11:11,518
be used in practice the privacy loss was

386
00:11:09,440 --> 00:11:13,760
pretty similar to with no defense

387
00:11:11,519 --> 00:11:15,680
and actually the other thing is on most

388
00:11:13,760 --> 00:11:17,200
of the models where 16 did give a good

389
00:11:15,680 --> 00:11:18,719
defense like on cfar

390
00:11:17,200 --> 00:11:20,560
the accuracy of the model with epsilon

391
00:11:18,720 --> 00:11:22,320
16 was still bad so

392
00:11:20,560 --> 00:11:24,160
a lot of times it seems like you might

393
00:11:22,320 --> 00:11:26,640
start to lose benefits once you make

394
00:11:24,160 --> 00:11:28,240
epsilon big enough to get good accuracy

395
00:11:26,640 --> 00:11:30,240
so this really suggests there should be

396
00:11:28,240 --> 00:11:31,600
more uh work on training

397
00:11:30,240 --> 00:11:33,920
differentially private models to

398
00:11:31,600 --> 00:11:35,600
overcome this trade-off

399
00:11:33,920 --> 00:11:37,920
and in conclusion i'll just go ahead and

400
00:11:35,600 --> 00:11:39,760
leave these key takeaways here

401
00:11:37,920 --> 00:11:41,680
and of course this is just a 12-minute

402
00:11:39,760 --> 00:11:43,680
talk so check out the paper for

403
00:11:41,680 --> 00:11:45,920
details on the analysis more

404
00:11:43,680 --> 00:11:48,640
sophisticated variants of the attack

405
00:11:45,920 --> 00:11:50,240
additional experimental results and

406
00:11:48,640 --> 00:11:53,040
evaluation of other defensions

407
00:11:50,240 --> 00:11:54,480
like regularization anyway thanks so

408
00:11:53,040 --> 00:11:55,199
much for watching and i look forward to

409
00:11:54,480 --> 00:11:57,839
your questions

410
00:11:55,200 --> 00:11:57,839
thanks

