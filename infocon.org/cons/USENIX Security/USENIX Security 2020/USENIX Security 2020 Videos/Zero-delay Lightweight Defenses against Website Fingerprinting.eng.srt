1
00:00:08,960 --> 00:00:13,120
hi everyone my name is jajun gong

2
00:00:11,280 --> 00:00:15,518
i am a phd student from hong kong

3
00:00:13,120 --> 00:00:17,520
university of science and technology

4
00:00:15,519 --> 00:00:19,279
today i am going to present my work

5
00:00:17,520 --> 00:00:21,359
serial delay lightweight defenses

6
00:00:19,279 --> 00:00:23,439
against the websiting of printing

7
00:00:21,359 --> 00:00:25,359
this is a joint work with my supervisor

8
00:00:23,439 --> 00:00:27,279
tao wang

9
00:00:25,359 --> 00:00:29,119
network surveillance has become a

10
00:00:27,279 --> 00:00:30,080
pervasive threat against people's

11
00:00:29,119 --> 00:00:33,120
privacy

12
00:00:30,080 --> 00:00:35,120
tor and anonymity network has become one

13
00:00:33,120 --> 00:00:36,160
of the most popular privacy in housing

14
00:00:35,120 --> 00:00:37,839
technologies

15
00:00:36,160 --> 00:00:39,839
by defending users from network

16
00:00:37,840 --> 00:00:42,079
eavesdroppers to do so

17
00:00:39,840 --> 00:00:43,520
it forwards user packets across multiple

18
00:00:42,079 --> 00:00:45,680
volunteer proxies

19
00:00:43,520 --> 00:00:47,680
so that network surveillance cannot see

20
00:00:45,680 --> 00:00:49,680
both the true source and destination of

21
00:00:47,680 --> 00:00:51,760
the packets

22
00:00:49,680 --> 00:00:54,160
however researchers found that a local

23
00:00:51,760 --> 00:00:56,000
attacker can still violate your privacy

24
00:00:54,160 --> 00:00:58,160
by passively observing the network

25
00:00:56,000 --> 00:00:58,960
traffic although the packets are

26
00:00:58,160 --> 00:01:01,038
encrypted

27
00:00:58,960 --> 00:01:02,800
different web pages can have different

28
00:01:01,039 --> 00:01:03,840
loading times and different number of

29
00:01:02,800 --> 00:01:05,759
packets

30
00:01:03,840 --> 00:01:07,760
those side channel information can be

31
00:01:05,760 --> 00:01:10,479
used to fingerprint the websites

32
00:01:07,760 --> 00:01:12,880
breaking the privacy such attack is

33
00:01:10,479 --> 00:01:15,439
known as website websiting of printing

34
00:01:12,880 --> 00:01:16,880
anyone under the same network as you or

35
00:01:15,439 --> 00:01:20,880
internet service provider

36
00:01:16,880 --> 00:01:23,360
could be a potential wf attacker

37
00:01:20,880 --> 00:01:25,039
to perform such an attack normally the

38
00:01:23,360 --> 00:01:26,240
attacker will collect some network

39
00:01:25,040 --> 00:01:28,880
traces in

40
00:01:26,240 --> 00:01:30,399
advance extracting some features to

41
00:01:28,880 --> 00:01:32,000
train a classifier

42
00:01:30,400 --> 00:01:34,240
then use this classifier to do

43
00:01:32,000 --> 00:01:36,479
prediction several attacks

44
00:01:34,240 --> 00:01:38,158
have been put forward and they are all

45
00:01:36,479 --> 00:01:40,640
able to achieve more than 90

46
00:01:38,159 --> 00:01:43,520
record which poses a great threat to

47
00:01:40,640 --> 00:01:43,520
user privacy

48
00:01:44,000 --> 00:01:48,079
faced with such a great threat several

49
00:01:46,159 --> 00:01:51,200
defenses have been proposed

50
00:01:48,079 --> 00:01:53,520
the first one is wtf pad the key

51
00:01:51,200 --> 00:01:55,759
idea is to insert some dummy packets

52
00:01:53,520 --> 00:01:56,960
when detecting abnormal time gaps in the

53
00:01:55,759 --> 00:01:58,640
traffic

54
00:01:56,960 --> 00:02:00,399
it is the candidate defense to be

55
00:01:58,640 --> 00:02:02,799
deployed on tour

56
00:02:00,399 --> 00:02:04,240
however later it is shown to be broken

57
00:02:02,799 --> 00:02:07,920
by deep fingerprinting

58
00:02:04,240 --> 00:02:09,038
with over 90 accuracy the second defense

59
00:02:07,920 --> 00:02:11,280
is terminal

60
00:02:09,038 --> 00:02:13,519
it forces packets from both directions

61
00:02:11,280 --> 00:02:16,080
to be sent in a constant rate

62
00:02:13,520 --> 00:02:18,480
undoubtedly it will incur a lot of

63
00:02:16,080 --> 00:02:18,480
overhead

64
00:02:19,120 --> 00:02:23,840
to evaluate a defense we look into its

65
00:02:21,920 --> 00:02:26,399
privacy and overhead

66
00:02:23,840 --> 00:02:27,200
talking about overhead we refer to data

67
00:02:26,400 --> 00:02:30,160
overhead

68
00:02:27,200 --> 00:02:31,280
which is the extra data we send and the

69
00:02:30,160 --> 00:02:33,440
time overhead

70
00:02:31,280 --> 00:02:34,560
which is the extra delay caused by the

71
00:02:33,440 --> 00:02:36,560
defense

72
00:02:34,560 --> 00:02:39,200
both of them will affect the browsing

73
00:02:36,560 --> 00:02:39,200
experience

74
00:02:39,519 --> 00:02:44,400
so our question is is there any better

75
00:02:42,480 --> 00:02:46,879
defense

76
00:02:44,400 --> 00:02:48,080
in this work we propose two zero delay

77
00:02:46,879 --> 00:02:51,040
lightweight defenses

78
00:02:48,080 --> 00:02:51,760
front and blue zero delay means the

79
00:02:51,040 --> 00:02:54,720
defense

80
00:02:51,760 --> 00:02:56,720
has zero time overhead and lightweight

81
00:02:54,720 --> 00:02:59,200
means they both require little data

82
00:02:56,720 --> 00:02:59,200
overhead

83
00:02:59,440 --> 00:03:05,519
the general idea of front is similar as

84
00:03:02,319 --> 00:03:07,440
wtf pad it obfuscates the traffic

85
00:03:05,519 --> 00:03:09,680
by injecting dummy packets in both

86
00:03:07,440 --> 00:03:11,680
directions without delaying any true

87
00:03:09,680 --> 00:03:13,680
packets

88
00:03:11,680 --> 00:03:14,800
so how do we decide when to send a dummy

89
00:03:13,680 --> 00:03:17,920
packet

90
00:03:14,800 --> 00:03:18,480
previous researches show the trace front

91
00:03:17,920 --> 00:03:20,559
namely

92
00:03:18,480 --> 00:03:22,720
the first few seconds of the loading

93
00:03:20,560 --> 00:03:25,440
leaks much information

94
00:03:22,720 --> 00:03:27,440
we follow our first intuition which is

95
00:03:25,440 --> 00:03:28,720
to focus on investigating the trace

96
00:03:27,440 --> 00:03:31,680
front

97
00:03:28,720 --> 00:03:32,720
to do so we sample some time rayleigh

98
00:03:31,680 --> 00:03:34,720
distribution

99
00:03:32,720 --> 00:03:36,000
whose probability density function looks

100
00:03:34,720 --> 00:03:38,239
like this

101
00:03:36,000 --> 00:03:39,599
so why do we choose to use rayleigh

102
00:03:38,239 --> 00:03:41,360
distribution

103
00:03:39,599 --> 00:03:42,640
because we find that compared with

104
00:03:41,360 --> 00:03:45,040
normal distribution

105
00:03:42,640 --> 00:03:46,079
which has a symmetric shape the peak of

106
00:03:45,040 --> 00:03:48,798
rayleigh distribution

107
00:03:46,080 --> 00:03:50,879
curve is closer to the left side having

108
00:03:48,799 --> 00:03:54,159
more chance to output a front

109
00:03:50,879 --> 00:03:55,840
timestamp there is a parameter w

110
00:03:54,159 --> 00:03:57,280
which controls the shape of the

111
00:03:55,840 --> 00:03:59,439
distribution

112
00:03:57,280 --> 00:04:00,720
we find the probability to output the

113
00:03:59,439 --> 00:04:03,439
timestamp before w

114
00:04:00,720 --> 00:04:03,439
is around 40

115
00:04:04,080 --> 00:04:10,000
more specifically we sample n timestamps

116
00:04:07,200 --> 00:04:11,518
where n is a random variable we also

117
00:04:10,000 --> 00:04:15,519
randomize w

118
00:04:11,519 --> 00:04:18,160
which controls the shape of the curve

119
00:04:15,519 --> 00:04:20,639
this follows our second intuition which

120
00:04:18,160 --> 00:04:22,320
is to have trace to trace randomness

121
00:04:20,639 --> 00:04:24,479
so that each time you load the same

122
00:04:22,320 --> 00:04:28,159
webpage you could have quite different

123
00:04:24,479 --> 00:04:30,880
traces to summarize we first

124
00:04:28,160 --> 00:04:33,040
set three parameters n which controls

125
00:04:30,880 --> 00:04:37,040
the number of packets generated

126
00:04:33,040 --> 00:04:39,840
w mean and w max which control w

127
00:04:37,040 --> 00:04:42,160
then we sample small n and small w from

128
00:04:39,840 --> 00:04:44,560
two uniform distributions

129
00:04:42,160 --> 00:04:45,919
finally we sample end time stamps from

130
00:04:44,560 --> 00:04:50,080
rayleigh distribution

131
00:04:45,919 --> 00:04:50,080
at which we send out the dummy packets

132
00:04:50,639 --> 00:04:56,960
to evaluate front we collect a data set

133
00:04:53,759 --> 00:05:00,639
which consists of 100 monitor web pages

134
00:04:56,960 --> 00:05:03,919
each visited 100 times we also visit

135
00:05:00,639 --> 00:05:06,560
10 000 now monetary webpages

136
00:05:03,919 --> 00:05:07,520
the attacker will use 90 for training

137
00:05:06,560 --> 00:05:10,639
and 10

138
00:05:07,520 --> 00:05:12,960
for testing the attacker's goal

139
00:05:10,639 --> 00:05:14,639
is to identify whether the client is

140
00:05:12,960 --> 00:05:16,719
visiting a monitor page

141
00:05:14,639 --> 00:05:19,520
and further answer which monitored web

142
00:05:16,720 --> 00:05:19,520
page it is

143
00:05:19,919 --> 00:05:25,039
we two the parameters so that front has

144
00:05:22,800 --> 00:05:28,240
the same overhead as wtf pad

145
00:05:25,039 --> 00:05:29,360
which is 33 percent and here is the

146
00:05:28,240 --> 00:05:32,960
result

147
00:05:29,360 --> 00:05:34,960
front out performs wtf pad with lower f1

148
00:05:32,960 --> 00:05:36,799
score on each attack

149
00:05:34,960 --> 00:05:38,239
especially for the most powerful attack

150
00:05:36,800 --> 00:05:41,520
deep fingerprinting

151
00:05:38,240 --> 00:05:46,080
front is able to lower the f1 score from

152
00:05:41,520 --> 00:05:46,080
0.94 to less than 0.5

153
00:05:46,960 --> 00:05:51,440
then we compared with terminal we find

154
00:05:49,600 --> 00:05:54,639
that tamro incurs 5 times

155
00:05:51,440 --> 00:05:55,039
more overhead than front however we

156
00:05:54,639 --> 00:05:57,680
found

157
00:05:55,039 --> 00:05:59,039
k n has an even lower f1 score against

158
00:05:57,680 --> 00:06:01,919
the front

159
00:05:59,039 --> 00:06:06,800
for other attacks front can lower their

160
00:06:01,919 --> 00:06:10,080
f1 score to less than 0.46

161
00:06:06,800 --> 00:06:12,479
we proposed another defense called glue

162
00:06:10,080 --> 00:06:13,840
the idea is to force the attacker to

163
00:06:12,479 --> 00:06:16,719
solve the difficult split

164
00:06:13,840 --> 00:06:19,440
problem previous research shows that

165
00:06:16,720 --> 00:06:21,360
it's hard to split two web page loadings

166
00:06:19,440 --> 00:06:23,600
especially when they overlap with each

167
00:06:21,360 --> 00:06:26,720
other further affecting the performance

168
00:06:23,600 --> 00:06:26,720
of wf attack

169
00:06:26,960 --> 00:06:30,318
therefore consider the easiest case for

170
00:06:29,280 --> 00:06:32,638
the attacker

171
00:06:30,319 --> 00:06:34,080
where the client visits three pages one

172
00:06:32,639 --> 00:06:36,319
by one

173
00:06:34,080 --> 00:06:38,719
we force the attacker to solve split

174
00:06:36,319 --> 00:06:40,400
problems by sending dummy traces between

175
00:06:38,720 --> 00:06:43,280
the gaps of two loadings

176
00:06:40,400 --> 00:06:45,440
so that it looks like one single trace

177
00:06:43,280 --> 00:06:48,159
we call those dummy traces in the middle

178
00:06:45,440 --> 00:06:49,759
group traces and the one in the end tell

179
00:06:48,160 --> 00:06:52,400
trace

180
00:06:49,759 --> 00:06:53,840
to classify those pages the attacker has

181
00:06:52,400 --> 00:06:57,039
to find the split

182
00:06:53,840 --> 00:06:57,520
first however we notice that the first

183
00:06:57,039 --> 00:07:00,560
trace

184
00:06:57,520 --> 00:07:02,000
is easier to classify since the attacker

185
00:07:00,560 --> 00:07:04,720
already knows the start of the

186
00:07:02,000 --> 00:07:07,120
loading so we cover trace 1 with front

187
00:07:04,720 --> 00:07:07,120
noise

188
00:07:07,759 --> 00:07:11,440
for simplicity we call a trace glued by

189
00:07:10,479 --> 00:07:14,639
our traces and

190
00:07:11,440 --> 00:07:15,759
our trace to summarize when the first

191
00:07:14,639 --> 00:07:18,960
loading starts

192
00:07:15,759 --> 00:07:20,479
glue will add front noise then glue as

193
00:07:18,960 --> 00:07:22,719
glue traces between minutes

194
00:07:20,479 --> 00:07:24,000
until the dwell time reaches the next

195
00:07:22,720 --> 00:07:26,319
time

196
00:07:24,000 --> 00:07:27,840
in reality we can randomly choose a

197
00:07:26,319 --> 00:07:31,840
trace loaded before

198
00:07:27,840 --> 00:07:31,840
as a glue trace

199
00:07:31,919 --> 00:07:35,599
to evaluate glue we consider two

200
00:07:34,240 --> 00:07:38,479
different cases

201
00:07:35,599 --> 00:07:40,880
we first assume that the attacker always

202
00:07:38,479 --> 00:07:41,840
knows how many web pages are in an l

203
00:07:40,880 --> 00:07:43,919
trace

204
00:07:41,840 --> 00:07:45,198
so the attacker only needs to find l

205
00:07:43,919 --> 00:07:48,400
minus 1 splits

206
00:07:45,199 --> 00:07:51,680
cut the traces and perform wf attack

207
00:07:48,400 --> 00:07:57,280
we generate around 618

208
00:07:51,680 --> 00:07:59,680
to 4500 l traces l ranging from 2 to 16.

209
00:07:57,280 --> 00:08:00,719
we first tested the attack on undefended

210
00:07:59,680 --> 00:08:03,360
traces

211
00:08:00,720 --> 00:08:05,520
meaning we did not add any noise the

212
00:08:03,360 --> 00:08:09,599
attacker can achieve

213
00:08:05,520 --> 00:08:11,758
82 to 96 record and precision

214
00:08:09,599 --> 00:08:14,800
the result is good because the split

215
00:08:11,759 --> 00:08:18,240
accuracy is as high as 92 percent

216
00:08:14,800 --> 00:08:20,960
when there is no noise then we

217
00:08:18,240 --> 00:08:21,520
test the glue the recoil significantly

218
00:08:20,960 --> 00:08:24,799
drops

219
00:08:21,520 --> 00:08:28,240
to 4 to 54 percent and

220
00:08:24,800 --> 00:08:30,479
precision is 4 to 20 depending on l

221
00:08:28,240 --> 00:08:31,360
when more blue traces are glued together

222
00:08:30,479 --> 00:08:34,240
the attack

223
00:08:31,360 --> 00:08:34,240
becomes harder

224
00:08:34,719 --> 00:08:39,120
then we look into a more realistic case

225
00:08:37,279 --> 00:08:41,679
where the attacker does not know how

226
00:08:39,120 --> 00:08:44,000
many pages are in l trace

227
00:08:41,679 --> 00:08:44,959
the attacker has to determine l first

228
00:08:44,000 --> 00:08:47,279
then find the

229
00:08:44,959 --> 00:08:48,719
corresponding splits cut the traces and

230
00:08:47,279 --> 00:08:52,399
perform the attack

231
00:08:48,720 --> 00:08:55,600
this is even harder so even on the

232
00:08:52,399 --> 00:08:58,160
undefended traces the attacker achieves

233
00:08:55,600 --> 00:09:02,080
45 to 75 percent recall

234
00:08:58,160 --> 00:09:04,480
and 41 to 77 precision

235
00:09:02,080 --> 00:09:05,440
on glue dataset the attacker performs

236
00:09:04,480 --> 00:09:07,920
even worse

237
00:09:05,440 --> 00:09:10,160
he only achieves three to 46 percent

238
00:09:07,920 --> 00:09:12,640
record and a 1 to 16

239
00:09:10,160 --> 00:09:12,640
precision

240
00:09:13,279 --> 00:09:18,480
glue does not delay any user packets so

241
00:09:15,920 --> 00:09:20,399
the time overhead is always zero

242
00:09:18,480 --> 00:09:21,680
glue's data overhead depends on user

243
00:09:20,399 --> 00:09:24,000
behavior

244
00:09:21,680 --> 00:09:24,880
suppose mean the whale time on each page

245
00:09:24,000 --> 00:09:27,120
is dg

246
00:09:24,880 --> 00:09:28,000
and the mean duration of tail trace is

247
00:09:27,120 --> 00:09:30,640
dl

248
00:09:28,000 --> 00:09:32,880
with different different dg and dl we

249
00:09:30,640 --> 00:09:36,480
can get different overhead

250
00:09:32,880 --> 00:09:39,839
for a user whose dg is 5.5 second

251
00:09:36,480 --> 00:09:41,600
and the dl is 12.5 seconds the data

252
00:09:39,839 --> 00:09:44,800
overhead is 22

253
00:09:41,600 --> 00:09:46,880
to 44 percent

254
00:09:44,800 --> 00:09:50,560
when more traces are glued together the

255
00:09:46,880 --> 00:09:53,920
data overhead is lower

256
00:09:50,560 --> 00:09:55,359
to summarize we proposed two defenses

257
00:09:53,920 --> 00:09:57,120
against the website fingerprinting

258
00:09:55,360 --> 00:09:59,360
attack in tor

259
00:09:57,120 --> 00:10:01,600
front focus on investigating the trace

260
00:09:59,360 --> 00:10:03,839
front and trace to trace randomness

261
00:10:01,600 --> 00:10:05,200
while glue forces the attacker to solve

262
00:10:03,839 --> 00:10:08,720
split problem

263
00:10:05,200 --> 00:10:08,720
both of them are lightweight

264
00:10:08,800 --> 00:10:12,240
and here's the source code link and my

265
00:10:10,880 --> 00:10:19,839
email address

266
00:10:12,240 --> 00:10:19,839
thanks for listening

267
00:10:22,959 --> 00:10:25,040
you

