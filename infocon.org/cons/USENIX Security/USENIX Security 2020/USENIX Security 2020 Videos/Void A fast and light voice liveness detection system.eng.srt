1
00:00:08,880 --> 00:00:12,519
hello everyone

2
00:00:09,920 --> 00:00:14,160
this is mohammadi jaz ahmad from

3
00:00:12,519 --> 00:00:16,160
csirostata61

4
00:00:14,160 --> 00:00:19,119
and i am excited to talk about our

5
00:00:16,160 --> 00:00:21,759
recent work on wise liveness detection

6
00:00:19,119 --> 00:00:23,119
this work is done in collaboration with

7
00:00:21,760 --> 00:00:26,560
singaporean university

8
00:00:23,119 --> 00:00:28,160
and samsung research popular voice

9
00:00:26,560 --> 00:00:31,519
assistants such as siri

10
00:00:28,160 --> 00:00:34,559
alexa or google now a lot of people's

11
00:00:31,519 --> 00:00:37,440
people's wise to quickly shop

12
00:00:34,559 --> 00:00:38,718
online make phone calls or use various

13
00:00:37,440 --> 00:00:42,480
banking services

14
00:00:38,719 --> 00:00:44,000
and so on research estimates that by

15
00:00:42,480 --> 00:00:47,360
2023 there will be

16
00:00:44,000 --> 00:00:50,079
as many as 275 million voice assistance

17
00:00:47,360 --> 00:00:51,519
in our homes which is one thousand

18
00:00:50,079 --> 00:00:56,559
percent

19
00:00:51,520 --> 00:00:58,719
uh increase compared to 2018.

20
00:00:56,559 --> 00:01:00,559
however due to the open nature of voice

21
00:00:58,719 --> 00:01:03,680
assistance input channel

22
00:01:00,559 --> 00:01:04,878
adversary can exploit this and launch

23
00:01:03,680 --> 00:01:07,600
various forms of

24
00:01:04,879 --> 00:01:08,640
voice presentation attacks such as voice

25
00:01:07,600 --> 00:01:12,240
replay attack

26
00:01:08,640 --> 00:01:14,240
or voice synthesis attacks

27
00:01:12,240 --> 00:01:17,280
voice replay attack is a simple kind of

28
00:01:14,240 --> 00:01:19,679
attack in which an adversary records

29
00:01:17,280 --> 00:01:20,960
victims use of voice assistance and

30
00:01:19,680 --> 00:01:23,840
replay them

31
00:01:20,960 --> 00:01:24,479
this is the easiest attack to perform

32
00:01:23,840 --> 00:01:27,759
but it's

33
00:01:24,479 --> 00:01:28,720
most difficult to detect as the voice

34
00:01:27,759 --> 00:01:32,000
characteristic

35
00:01:28,720 --> 00:01:35,920
of recorded voice are similar to that

36
00:01:32,000 --> 00:01:37,680
of victim's life voice voice synthesis

37
00:01:35,920 --> 00:01:40,960
attacks are based on this

38
00:01:37,680 --> 00:01:44,000
target speech model which takes text as

39
00:01:40,960 --> 00:01:48,079
input and synthesize the speech

40
00:01:44,000 --> 00:01:48,079
for the spoofing attacks

41
00:01:48,960 --> 00:01:52,960
here are some of the news items that

42
00:01:51,600 --> 00:01:56,079
show vulnerability in

43
00:01:52,960 --> 00:01:59,119
voice assistance alexa orders

44
00:01:56,079 --> 00:02:02,000
an expensive dollhouse when a child

45
00:01:59,119 --> 00:02:03,680
asks the device to play with her

46
00:02:02,000 --> 00:02:07,520
similarly burger king's

47
00:02:03,680 --> 00:02:11,200
triggers google home

48
00:02:07,520 --> 00:02:13,920
google home by just a tv ad

49
00:02:11,200 --> 00:02:14,480
so such kind of incidents are expected

50
00:02:13,920 --> 00:02:18,319
to

51
00:02:14,480 --> 00:02:22,079
increase with the increase of voice

52
00:02:18,319 --> 00:02:24,560
assistance in our homes

53
00:02:22,080 --> 00:02:25,280
so wise liveness detection is actually

54
00:02:24,560 --> 00:02:28,879
the system

55
00:02:25,280 --> 00:02:31,599
or ai that detects if the voice

56
00:02:28,879 --> 00:02:31,920
that is received is from the live human

57
00:02:31,599 --> 00:02:34,959
or

58
00:02:31,920 --> 00:02:37,839
it is from the loudspeakers

59
00:02:34,959 --> 00:02:38,879
it also resembles to like the turing

60
00:02:37,840 --> 00:02:43,599
test that was

61
00:02:38,879 --> 00:02:43,599
proposed by alan turing in 1950

62
00:02:44,480 --> 00:02:50,160
so before designing any uh solution

63
00:02:48,080 --> 00:02:52,239
there are certain requirements from

64
00:02:50,160 --> 00:02:55,200
industry that should be met

65
00:02:52,239 --> 00:02:57,120
our conversation with speech recognition

66
00:02:55,200 --> 00:02:59,040
engineers from a big iep company

67
00:02:57,120 --> 00:03:01,840
revealed that

68
00:02:59,040 --> 00:03:03,599
strict requirements regarding latency

69
00:03:01,840 --> 00:03:04,400
and model science must be considered

70
00:03:03,599 --> 00:03:06,959
before

71
00:03:04,400 --> 00:03:09,680
deploying any machine learning services

72
00:03:06,959 --> 00:03:10,319
for example the processing delay must be

73
00:03:09,680 --> 00:03:13,840
less than

74
00:03:10,319 --> 00:03:15,920
100 milliseconds or a single gpu may be

75
00:03:13,840 --> 00:03:19,120
expected to concurrently process

76
00:03:15,920 --> 00:03:20,720
100 or more voice sessions

77
00:03:19,120 --> 00:03:23,280
more importantly on device

78
00:03:20,720 --> 00:03:26,480
implementations are also considered

79
00:03:23,280 --> 00:03:28,640
is because this will reduce the

80
00:03:26,480 --> 00:03:30,399
extra burden on remote server the

81
00:03:28,640 --> 00:03:33,679
communication latency

82
00:03:30,400 --> 00:03:35,599
and most importantly the privacy issues

83
00:03:33,680 --> 00:03:38,640
concerning the voice samples

84
00:03:35,599 --> 00:03:40,560
is could also be addressed here

85
00:03:38,640 --> 00:03:42,000
as far as detection accuracy is

86
00:03:40,560 --> 00:03:45,599
considered 10

87
00:03:42,000 --> 00:03:49,440
or below eer should be

88
00:03:45,599 --> 00:03:52,879
usable a usable solution

89
00:03:49,440 --> 00:03:54,400
our contribution here is to to make a

90
00:03:52,879 --> 00:03:56,640
solution that is fast

91
00:03:54,400 --> 00:03:58,480
lightweight and easily implementable in

92
00:03:56,640 --> 00:04:01,679
commercial devices

93
00:03:58,480 --> 00:04:04,000
void is a solution which takes only 97

94
00:04:01,680 --> 00:04:04,959
features and a single machine learning

95
00:04:04,000 --> 00:04:07,439
model

96
00:04:04,959 --> 00:04:09,920
and which is robust against different

97
00:04:07,439 --> 00:04:12,239
environmental settings

98
00:04:09,920 --> 00:04:15,040
we evaluated our approach on two large

99
00:04:12,239 --> 00:04:19,120
data sets consisting of 250 more than

100
00:04:15,040 --> 00:04:22,320
250 voice samples from 120 participants

101
00:04:19,120 --> 00:04:25,600
and more than 18 000 samples

102
00:04:22,320 --> 00:04:29,040
which consists of 42 participants

103
00:04:25,600 --> 00:04:31,280
white achieves 0.3 percent and 11. 11.6

104
00:04:29,040 --> 00:04:33,199
percent eer respectively on both the

105
00:04:31,280 --> 00:04:35,919
datasets

106
00:04:33,199 --> 00:04:38,639
what is eight times more faster and uses

107
00:04:35,919 --> 00:04:41,198
153 times less memory compared to the

108
00:04:38,639 --> 00:04:43,360
best performing solution in esp spoof

109
00:04:41,199 --> 00:04:45,440
competition

110
00:04:43,360 --> 00:04:47,120
we also show that voice is resilient

111
00:04:45,440 --> 00:04:49,520
against adversarial attacks

112
00:04:47,120 --> 00:04:51,199
such as hidden voice attack inaudible

113
00:04:49,520 --> 00:04:53,680
voice commands

114
00:04:51,199 --> 00:04:54,479
voice synthesis attacks and equalization

115
00:04:53,680 --> 00:04:56,400
attacks

116
00:04:54,479 --> 00:05:00,560
and all those attacks voice void

117
00:04:56,400 --> 00:05:02,560
achieves more than 86 percent accuracy

118
00:05:00,560 --> 00:05:03,680
here we discuss key insights that are

119
00:05:02,560 --> 00:05:06,639
used in void for

120
00:05:03,680 --> 00:05:09,520
attack detection one of them is the

121
00:05:06,639 --> 00:05:11,360
decay pattern inspector power

122
00:05:09,520 --> 00:05:12,880
the left side of figure one shows the

123
00:05:11,360 --> 00:05:15,440
spectrogram of a

124
00:05:12,880 --> 00:05:16,159
of a live voice which shows that most of

125
00:05:15,440 --> 00:05:18,400
the power

126
00:05:16,160 --> 00:05:19,919
is actually concentrated below one

127
00:05:18,400 --> 00:05:21,840
kilohertz

128
00:05:19,919 --> 00:05:23,520
where on the right hand side of figure

129
00:05:21,840 --> 00:05:26,880
one shows the

130
00:05:23,520 --> 00:05:30,479
accumulated power per frequency pattern

131
00:05:26,880 --> 00:05:34,400
we observe there is exponential decay

132
00:05:30,479 --> 00:05:37,039
in power pattern below 1 kilohertz

133
00:05:34,400 --> 00:05:39,039
however in figure 2 show the spectrogram

134
00:05:37,039 --> 00:05:42,159
of the replay device

135
00:05:39,039 --> 00:05:44,800
from a smartphone loudspeaker

136
00:05:42,160 --> 00:05:45,759
we can see in the spec program that the

137
00:05:44,800 --> 00:05:47,680
power is

138
00:05:45,759 --> 00:05:49,199
uniformly seems to be uniformly

139
00:05:47,680 --> 00:05:52,080
distributed between

140
00:05:49,199 --> 00:05:53,600
0 and 5 kilohertz on the right hand side

141
00:05:52,080 --> 00:05:56,719
the figure shows

142
00:05:53,600 --> 00:05:57,199
the accumulated power per frequency so

143
00:05:56,720 --> 00:05:59,840
we

144
00:05:57,199 --> 00:06:02,720
we can see the decay pattern in replay

145
00:05:59,840 --> 00:06:05,520
wise is somehow linear

146
00:06:02,720 --> 00:06:08,000
so this kind of our our experimentation

147
00:06:05,520 --> 00:06:11,120
with more than 11

148
00:06:08,000 --> 00:06:14,560
inbuilt speakers from different vendors

149
00:06:11,120 --> 00:06:17,120
show similar characteristics

150
00:06:14,560 --> 00:06:17,680
so there's another key inside about peak

151
00:06:17,120 --> 00:06:20,479
patterns

152
00:06:17,680 --> 00:06:21,840
in spectral power we see in live human

153
00:06:20,479 --> 00:06:25,120
there are more peaks

154
00:06:21,840 --> 00:06:26,479
compared to that of loudspeakers and

155
00:06:25,120 --> 00:06:29,199
also the loudspeaker

156
00:06:26,479 --> 00:06:29,840
signals that are given in the red dash

157
00:06:29,199 --> 00:06:33,440
line

158
00:06:29,840 --> 00:06:35,919
are more undeterministic or is hard to

159
00:06:33,440 --> 00:06:38,000
predict however in live human case

160
00:06:35,919 --> 00:06:40,080
it could be predicted and they show

161
00:06:38,000 --> 00:06:42,240
somehow similar pattern

162
00:06:40,080 --> 00:06:44,240
such patterns could be used for

163
00:06:42,240 --> 00:06:48,160
detecting voice life

164
00:06:44,240 --> 00:06:49,520
live voices here is the overall overview

165
00:06:48,160 --> 00:06:53,759
of white which takes

166
00:06:49,520 --> 00:06:56,719
into account the voice sample

167
00:06:53,759 --> 00:06:58,560
and then converts it into spectrogram

168
00:06:56,720 --> 00:07:01,840
and then perf

169
00:06:58,560 --> 00:07:05,280
accumulated power per frequency vector

170
00:07:01,840 --> 00:07:08,159
is obtained this vector is used to

171
00:07:05,280 --> 00:07:10,080
compute power linearity degree features

172
00:07:08,160 --> 00:07:12,960
high power frequency features

173
00:07:10,080 --> 00:07:13,758
low frequency power features in addition

174
00:07:12,960 --> 00:07:16,479
to that

175
00:07:13,759 --> 00:07:18,240
that we use lpcc features as the

176
00:07:16,479 --> 00:07:21,758
complementary features

177
00:07:18,240 --> 00:07:24,000
to further improve voice performance

178
00:07:21,759 --> 00:07:26,560
all the features vectors are combined

179
00:07:24,000 --> 00:07:29,520
and svm model is trained for real-time

180
00:07:26,560 --> 00:07:29,520
attack detection

181
00:07:29,759 --> 00:07:33,440
our data collection consists of two

182
00:07:31,840 --> 00:07:36,000
major data sets one is

183
00:07:33,440 --> 00:07:37,120
collected by ourselves and second we use

184
00:07:36,000 --> 00:07:40,240
asp's proof

185
00:07:37,120 --> 00:07:44,560
2017 competition data set

186
00:07:40,240 --> 00:07:48,720
our data set consists of 53 male voices

187
00:07:44,560 --> 00:07:51,360
male participants and our total 120

188
00:07:48,720 --> 00:07:53,680
participants were recruited

189
00:07:51,360 --> 00:07:56,840
and we divided each participants into

190
00:07:53,680 --> 00:07:59,440
different age groups and performed

191
00:07:56,840 --> 00:08:00,878
experiments for asb food data set

192
00:07:59,440 --> 00:08:03,199
consists of three sets

193
00:08:00,879 --> 00:08:05,440
training development and evaluation in

194
00:08:03,199 --> 00:08:06,560
our evaluation we combine training and

195
00:08:05,440 --> 00:08:11,199
development set

196
00:08:06,560 --> 00:08:11,199
and train the svm model for evaluation

197
00:08:11,599 --> 00:08:14,800
so the here is the summary of both the

198
00:08:13,440 --> 00:08:18,160
data set in terms of

199
00:08:14,800 --> 00:08:22,080
live samples attack samples participants

200
00:08:18,160 --> 00:08:22,080
speakers and configurations

201
00:08:23,199 --> 00:08:27,120
our evaluation shows that that the word

202
00:08:26,240 --> 00:08:29,360
achieves

203
00:08:27,120 --> 00:08:31,759
superior performance on both the data

204
00:08:29,360 --> 00:08:34,560
sets for example in our data set it

205
00:08:31,759 --> 00:08:37,760
gives 0.3 percent eer and

206
00:08:34,559 --> 00:08:39,679
with asp spoof data set it achieves 11.6

207
00:08:37,760 --> 00:08:42,399
percent eer

208
00:08:39,679 --> 00:08:42,880
also our award achieves good performance

209
00:08:42,399 --> 00:08:45,920
with

210
00:08:42,880 --> 00:08:46,959
variances such as gender or distances

211
00:08:45,920 --> 00:08:50,000
between

212
00:08:46,959 --> 00:08:53,439
the attacker and the speakers

213
00:08:50,000 --> 00:08:55,200
and some cross training data set

214
00:08:53,440 --> 00:08:58,320
please refer to our paper for more

215
00:08:55,200 --> 00:08:59,920
details on those experiments

216
00:08:58,320 --> 00:09:01,760
here is that shows the lightweight

217
00:08:59,920 --> 00:09:04,399
nature of wired in terms of

218
00:09:01,760 --> 00:09:05,439
time and memory we see that the testing

219
00:09:04,399 --> 00:09:09,040
time

220
00:09:05,440 --> 00:09:11,920
for void is less than one second which

221
00:09:09,040 --> 00:09:13,920
is equal to 35 milliseconds

222
00:09:11,920 --> 00:09:15,519
and the number of features of wired are

223
00:09:13,920 --> 00:09:18,560
97 and the memory

224
00:09:15,519 --> 00:09:22,000
taken is 1.9 m

225
00:09:18,560 --> 00:09:24,239
1.9 mb which makes it an ideal choice

226
00:09:22,000 --> 00:09:26,000
for on-device implementation and it

227
00:09:24,240 --> 00:09:28,160
seems extremely light for

228
00:09:26,000 --> 00:09:29,360
attack detection compared to other

229
00:09:28,160 --> 00:09:33,120
approaches

230
00:09:29,360 --> 00:09:36,080
it achieves 11.6 eer

231
00:09:33,120 --> 00:09:37,440
here we show the top 10 performing

232
00:09:36,080 --> 00:09:41,440
approaches in asb

233
00:09:37,440 --> 00:09:43,120
spoof competition in terms of their eer

234
00:09:41,440 --> 00:09:45,920
and they show the features and

235
00:09:43,120 --> 00:09:48,240
classifiers used by them

236
00:09:45,920 --> 00:09:49,279
so void would stand at second position

237
00:09:48,240 --> 00:09:53,839
with eer

238
00:09:49,279 --> 00:09:53,839
11.6 percent in this competition

239
00:09:54,399 --> 00:09:57,519
we also checked wide as in symbol

240
00:09:56,240 --> 00:10:01,040
solutions since

241
00:09:57,519 --> 00:10:03,600
mfcc and inter filter bank features are

242
00:10:01,040 --> 00:10:05,519
already available in smartphones where

243
00:10:03,600 --> 00:10:06,720
the speech recognition services are

244
00:10:05,519 --> 00:10:09,200
deployed

245
00:10:06,720 --> 00:10:10,880
we can use those with those features

246
00:10:09,200 --> 00:10:14,399
with void and achieve

247
00:10:10,880 --> 00:10:19,279
an 8.7 percent eer

248
00:10:14,399 --> 00:10:21,279
with no additional cost

249
00:10:19,279 --> 00:10:23,279
here we show the adversarial attacks

250
00:10:21,279 --> 00:10:24,000
against void we show a hidden voice

251
00:10:23,279 --> 00:10:25,760
commands

252
00:10:24,000 --> 00:10:27,279
hidden voice commands that come referred

253
00:10:25,760 --> 00:10:29,279
to the command that are not

254
00:10:27,279 --> 00:10:31,279
entered cannot be interpreted by human

255
00:10:29,279 --> 00:10:32,160
ears but they are interpreted and

256
00:10:31,279 --> 00:10:34,800
processed by

257
00:10:32,160 --> 00:10:36,800
voice assistants we consider inaudible

258
00:10:34,800 --> 00:10:37,279
voice commands such as dolphin attack

259
00:10:36,800 --> 00:10:40,719
which

260
00:10:37,279 --> 00:10:43,360
is in which an ultrasound speaker is

261
00:10:40,720 --> 00:10:45,760
used and this frequency range of higher

262
00:10:43,360 --> 00:10:48,480
than 20 kilohertz are used

263
00:10:45,760 --> 00:10:49,839
while synthesis attack we use and we use

264
00:10:48,480 --> 00:10:52,560
equalization

265
00:10:49,839 --> 00:10:53,279
manipulation attack which is manually

266
00:10:52,560 --> 00:10:56,880
crafting

267
00:10:53,279 --> 00:10:59,600
uh voice samples by adjusting

268
00:10:56,880 --> 00:11:02,720
certain frequencies using linear filters

269
00:10:59,600 --> 00:11:05,360
and launching those attacks

270
00:11:02,720 --> 00:11:06,720
our research shows that void achieves

271
00:11:05,360 --> 00:11:09,360
more than 86

272
00:11:06,720 --> 00:11:10,320
accuracy on all those attacks by

273
00:11:09,360 --> 00:11:13,440
achieving

274
00:11:10,320 --> 00:11:16,240
99 accuracy on hidden command

275
00:11:13,440 --> 00:11:17,360
in audible command it achieves 100

276
00:11:16,240 --> 00:11:19,200
accuracy

277
00:11:17,360 --> 00:11:21,519
and while synthesis it achieves 90

278
00:11:19,200 --> 00:11:23,920
percent accuracy and so on

279
00:11:21,519 --> 00:11:25,920
we can see that for the samples that

280
00:11:23,920 --> 00:11:28,160
used when those attacks were

281
00:11:25,920 --> 00:11:30,079
generated separately because of the

282
00:11:28,160 --> 00:11:33,439
attacks the

283
00:11:30,079 --> 00:11:37,519
dynamicity and those

284
00:11:33,440 --> 00:11:39,360
samples were tested in a real time

285
00:11:37,519 --> 00:11:41,519
here are two limitations for regarding

286
00:11:39,360 --> 00:11:43,519
the voice which is like high

287
00:11:41,519 --> 00:11:46,480
voice performance may degrade over high

288
00:11:43,519 --> 00:11:49,279
quality speakers because these kind of

289
00:11:46,480 --> 00:11:51,440
speakers remove most of the noises and

290
00:11:49,279 --> 00:11:54,240
make the voice as similar to the

291
00:11:51,440 --> 00:11:56,160
live human voices so void performance

292
00:11:54,240 --> 00:11:57,360
may degrade or with highest quality

293
00:11:56,160 --> 00:12:00,719
speakers

294
00:11:57,360 --> 00:12:03,200
and equalization attack show that by

295
00:12:00,720 --> 00:12:04,079
carefully crafting why sample can bypass

296
00:12:03,200 --> 00:12:06,079
wide

297
00:12:04,079 --> 00:12:11,040
however such attacks would require a

298
00:12:06,079 --> 00:12:13,439
strong signal processing expertise

299
00:12:11,040 --> 00:12:15,040
in conclusion we show that the live it

300
00:12:13,440 --> 00:12:15,760
is a lot of what is a lightweight

301
00:12:15,040 --> 00:12:19,199
approach

302
00:12:15,760 --> 00:12:21,279
which takes on only 97 feature and does

303
00:12:19,200 --> 00:12:24,639
not require any additional hardware

304
00:12:21,279 --> 00:12:26,320
it uses only single classifier it takes

305
00:12:24,639 --> 00:12:29,200
35 milliseconds to

306
00:12:26,320 --> 00:12:31,920
perform the decision and it's ideal for

307
00:12:29,200 --> 00:12:34,560
your own device implementation

308
00:12:31,920 --> 00:12:38,639
it's efficient and we showed 0.3 percent

309
00:12:34,560 --> 00:12:42,638
er and 11.6 percent eer in terms of

310
00:12:38,639 --> 00:12:44,639
performance on two data sets and voices

311
00:12:42,639 --> 00:12:46,480
quite resilient against adversarial

312
00:12:44,639 --> 00:12:50,320
attacks as well

313
00:12:46,480 --> 00:12:50,320
thank you very much and any questions

314
00:12:50,839 --> 00:12:53,839
please

315
00:12:58,720 --> 00:13:00,800
you

