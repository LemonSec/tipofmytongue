1
00:00:09,200 --> 00:00:13,599
hello everyone

2
00:00:10,320 --> 00:00:16,320
i'm xin jung from penn state university

3
00:00:13,599 --> 00:00:18,800
i'm here happy to present our paper

4
00:00:16,320 --> 00:00:23,039
interpretable deep learning under fire

5
00:00:18,800 --> 00:00:26,560
at youthnix securities program 2020.

6
00:00:23,039 --> 00:00:31,840
this is joint work with new favoring

7
00:00:26,560 --> 00:00:31,840
question sholingji shapur

8
00:00:35,280 --> 00:00:39,600
we present an attack against

9
00:00:37,360 --> 00:00:40,640
interpretable developing systems in this

10
00:00:39,600 --> 00:00:42,719
paper

11
00:00:40,640 --> 00:00:45,360
let's first talk about the deep neural

12
00:00:42,719 --> 00:00:47,760
networks interpretability

13
00:00:45,360 --> 00:00:48,800
while the ants achieve superhuman

14
00:00:47,760 --> 00:00:52,000
performance on

15
00:00:48,800 --> 00:00:54,718
video recognition language understanding

16
00:00:52,000 --> 00:00:57,199
and game playing they are often

17
00:00:54,719 --> 00:01:00,399
criticized as black boxes

18
00:00:57,199 --> 00:01:02,480
it is hard for human to understand how

19
00:01:00,399 --> 00:01:04,400
a deep neural network arrives at a

20
00:01:02,480 --> 00:01:07,039
particular decision

21
00:01:04,400 --> 00:01:08,159
this opacity may limit the application

22
00:01:07,040 --> 00:01:11,640
of the earth

23
00:01:08,159 --> 00:01:14,080
in safety critical domains due to their

24
00:01:11,640 --> 00:01:16,560
unpredictability

25
00:01:14,080 --> 00:01:19,520
therefore intensive research efforts

26
00:01:16,560 --> 00:01:22,960
have been spent to develop techniques to

27
00:01:19,520 --> 00:01:23,600
explain the other's behavior in our

28
00:01:22,960 --> 00:01:25,479
paper

29
00:01:23,600 --> 00:01:27,280
we consider four families of

30
00:01:25,479 --> 00:01:29,039
interpreters for the image

31
00:01:27,280 --> 00:01:31,280
classification task

32
00:01:29,040 --> 00:01:32,560
all these algorithms generate an

33
00:01:31,280 --> 00:01:35,040
attribute map

34
00:01:32,560 --> 00:01:36,400
to highlight significant regions in the

35
00:01:35,040 --> 00:01:38,320
input image

36
00:01:36,400 --> 00:01:39,759
that contributes to the classifier's

37
00:01:38,320 --> 00:01:42,079
prediction

38
00:01:39,759 --> 00:01:44,079
they are back propagation guided

39
00:01:42,079 --> 00:01:46,720
representation guided

40
00:01:44,079 --> 00:01:48,320
perturbation guided and model based

41
00:01:46,720 --> 00:01:51,920
interpreters

42
00:01:48,320 --> 00:01:55,199
here we have an example on the left side

43
00:01:51,920 --> 00:01:58,479
we have an image classified as a flute

44
00:01:55,200 --> 00:02:01,119
with a high confidence on the right side

45
00:01:58,479 --> 00:02:02,479
we have the attribute map shows where

46
00:02:01,119 --> 00:02:09,280
the darker region

47
00:02:02,479 --> 00:02:11,440
contributes to the model's prediction

48
00:02:09,280 --> 00:02:13,920
an interpretable deep learning system

49
00:02:11,440 --> 00:02:17,200
consists of a deep neural network

50
00:02:13,920 --> 00:02:19,519
the classifier and an interpreter

51
00:02:17,200 --> 00:02:21,599
such design involves humans in the

52
00:02:19,520 --> 00:02:24,319
decision-making process

53
00:02:21,599 --> 00:02:25,920
for instance the explanation of a

54
00:02:24,319 --> 00:02:28,160
specific decision

55
00:02:25,920 --> 00:02:29,760
could help analysts debug their deep

56
00:02:28,160 --> 00:02:32,640
learning models

57
00:02:29,760 --> 00:02:33,359
on the attacker side it requires the

58
00:02:32,640 --> 00:02:36,000
result

59
00:02:33,360 --> 00:02:37,040
for both the classroom and the

60
00:02:36,000 --> 00:02:39,280
interpreter

61
00:02:37,040 --> 00:02:42,319
to break down the entire interpretable

62
00:02:39,280 --> 00:02:42,319
deep learning system

63
00:02:43,040 --> 00:02:46,720
in this paper we take a step towards

64
00:02:45,840 --> 00:02:48,840
understanding

65
00:02:46,720 --> 00:02:51,440
the security vulnerabilities of

66
00:02:48,840 --> 00:02:54,560
interpretable deep learning systems

67
00:02:51,440 --> 00:02:57,519
our approach is to develop attacks

68
00:02:54,560 --> 00:02:57,920
that simultaneously for the classroom

69
00:02:57,519 --> 00:03:01,200
and

70
00:02:57,920 --> 00:03:03,679
the interpreter the diagram here shows

71
00:03:01,200 --> 00:03:06,480
the workflow of our attack

72
00:03:03,680 --> 00:03:07,680
give a benign input of the fish the

73
00:03:06,480 --> 00:03:10,879
adversary adds

74
00:03:07,680 --> 00:03:12,159
a penny preservation to get adversarial

75
00:03:10,879 --> 00:03:14,640
input

76
00:03:12,159 --> 00:03:18,319
then this input will cause the

77
00:03:14,640 --> 00:03:20,958
classifier to misclassify it as a cat

78
00:03:18,319 --> 00:03:22,079
and the input also misleads the

79
00:03:20,959 --> 00:03:25,760
interpreter

80
00:03:22,080 --> 00:03:28,080
to generate a target attribute map

81
00:03:25,760 --> 00:03:30,239
represents the overall formulation of

82
00:03:28,080 --> 00:03:33,440
our adb square attack

83
00:03:30,239 --> 00:03:35,360
the attacker crafts an adversarial input

84
00:03:33,440 --> 00:03:36,720
that triggered the target prediction

85
00:03:35,360 --> 00:03:39,519
city and

86
00:03:36,720 --> 00:03:41,280
the target interpretation empty we

87
00:03:39,519 --> 00:03:44,400
formulate the attack as

88
00:03:41,280 --> 00:03:45,360
a constituent optimization problem we

89
00:03:44,400 --> 00:03:48,239
want to find an

90
00:03:45,360 --> 00:03:49,840
x that minimizes the magnitude of

91
00:03:48,239 --> 00:03:53,040
perturbation delta

92
00:03:49,840 --> 00:03:56,720
such that f x is ct and g x

93
00:03:53,040 --> 00:03:58,560
f is empty it is hard to solve this

94
00:03:56,720 --> 00:04:00,959
problem directly

95
00:03:58,560 --> 00:04:02,319
because it involves non-linear deep

96
00:04:00,959 --> 00:04:04,319
neural networks

97
00:04:02,319 --> 00:04:06,079
and complicated interpretation

98
00:04:04,319 --> 00:04:09,760
algorithms

99
00:04:06,080 --> 00:04:11,519
we cast it as a regular red optimization

100
00:04:09,760 --> 00:04:14,480
problem as below

101
00:04:11,519 --> 00:04:15,439
here our prediction is the targeted pdd

102
00:04:14,480 --> 00:04:17,759
loss

103
00:04:15,439 --> 00:04:19,040
and our interpretation is the squared

104
00:04:17,759 --> 00:04:22,400
out distance

105
00:04:19,040 --> 00:04:23,840
between the generated map and the target

106
00:04:22,400 --> 00:04:26,159
map

107
00:04:23,840 --> 00:04:28,000
can directly apply the formulation in

108
00:04:26,160 --> 00:04:31,040
the previous slides to

109
00:04:28,000 --> 00:04:32,400
representation guided and motivates the

110
00:04:31,040 --> 00:04:34,720
interpreters

111
00:04:32,400 --> 00:04:36,000
we proposed two techniques for back

112
00:04:34,720 --> 00:04:39,040
propagation based

113
00:04:36,000 --> 00:04:41,840
interpretation the gradient sentence

114
00:04:39,040 --> 00:04:42,160
interpreter outputs the absolute value

115
00:04:41,840 --> 00:04:44,638
of

116
00:04:42,160 --> 00:04:45,759
gradient of prediction confidence of

117
00:04:44,639 --> 00:04:49,120
class c

118
00:04:45,759 --> 00:04:50,240
for the input attacking gradient

119
00:04:49,120 --> 00:04:53,280
sentence involves

120
00:04:50,240 --> 00:04:55,759
second order terms of the neural network

121
00:04:53,280 --> 00:04:56,960
while for the riddle function the value

122
00:04:55,759 --> 00:04:59,919
is zero

123
00:04:56,960 --> 00:05:00,960
hence we propose a gradient enhancement

124
00:04:59,919 --> 00:05:04,320
for relu

125
00:05:00,960 --> 00:05:08,000
during the backward stage besides

126
00:05:04,320 --> 00:05:11,039
to avoid an empty attribute map due to

127
00:05:08,000 --> 00:05:14,560
the grid saturation we apply the label

128
00:05:11,039 --> 00:05:17,599
smoothing to the classifier's target

129
00:05:14,560 --> 00:05:18,160
motivation guided interpretation we take

130
00:05:17,600 --> 00:05:21,360
mask

131
00:05:18,160 --> 00:05:24,080
as the target interpreter mask uses

132
00:05:21,360 --> 00:05:26,639
optimization to find a small region

133
00:05:24,080 --> 00:05:27,440
which greatly reduces the classification

134
00:05:26,639 --> 00:05:30,080
confidence of

135
00:05:27,440 --> 00:05:31,360
the target class when the region is

136
00:05:30,080 --> 00:05:34,000
removed

137
00:05:31,360 --> 00:05:36,800
in this setting we are faced with a

138
00:05:34,000 --> 00:05:40,240
bi-level optimization problem

139
00:05:36,800 --> 00:05:42,880
we use a approximation approach that we

140
00:05:40,240 --> 00:05:46,320
alternatively update our estimation

141
00:05:42,880 --> 00:05:50,000
of optimal attribute map m star

142
00:05:46,320 --> 00:05:53,039
and input x we also utilize

143
00:05:50,000 --> 00:05:53,840
an imbalanced update and periodical

144
00:05:53,039 --> 00:05:57,919
reset

145
00:05:53,840 --> 00:05:59,198
to stabilize our attack evaluate our adb

146
00:05:57,919 --> 00:06:01,680
square attack on the

147
00:05:59,199 --> 00:06:04,000
image negative side our target

148
00:06:01,680 --> 00:06:08,400
classifiers are resonant 50

149
00:06:04,000 --> 00:06:10,319
and this net 169 we compare the adb

150
00:06:08,400 --> 00:06:13,679
square attack to the ptd

151
00:06:10,319 --> 00:06:16,240
attack we set the target attribute map

152
00:06:13,680 --> 00:06:18,800
to the p9 attribute map

153
00:06:16,240 --> 00:06:19,600
the top table shows the attack success

154
00:06:18,800 --> 00:06:21,840
rate

155
00:06:19,600 --> 00:06:23,919
in terms of misclassification and

156
00:06:21,840 --> 00:06:27,599
prediction competency

157
00:06:23,919 --> 00:06:31,039
it shows our attack is as effective as

158
00:06:27,600 --> 00:06:31,919
the plain pgd attack the two charts

159
00:06:31,039 --> 00:06:34,719
below measure

160
00:06:31,919 --> 00:06:35,120
the similarity between generated maps

161
00:06:34,720 --> 00:06:38,240
and

162
00:06:35,120 --> 00:06:40,319
target attribute maps the left chart

163
00:06:38,240 --> 00:06:42,560
shows the l1 distance

164
00:06:40,319 --> 00:06:44,000
and the red chart shows intersection

165
00:06:42,560 --> 00:06:46,639
over union

166
00:06:44,000 --> 00:06:47,440
we could observe that our adb square

167
00:06:46,639 --> 00:06:50,319
attack

168
00:06:47,440 --> 00:06:51,759
could mislead interpreters to the target

169
00:06:50,319 --> 00:06:55,680
map

170
00:06:51,759 --> 00:06:58,240
well the pgd attack fails to do it

171
00:06:55,680 --> 00:07:00,080
to sample inputs prediction and

172
00:06:58,240 --> 00:07:02,880
interpretations here

173
00:07:00,080 --> 00:07:04,960
the first law are benign inputs the

174
00:07:02,880 --> 00:07:06,479
second row are attribute maps for the

175
00:07:04,960 --> 00:07:09,758
binary inputs

176
00:07:06,479 --> 00:07:13,440
the third and the last row show the item

177
00:07:09,759 --> 00:07:14,400
maps generated by pgd and our adb square

178
00:07:13,440 --> 00:07:17,759
attack

179
00:07:14,400 --> 00:07:20,479
as we can see we visually validate that

180
00:07:17,759 --> 00:07:23,520
adv square attack is spider-eyed fooling

181
00:07:20,479 --> 00:07:23,520
the interpreters

182
00:07:28,160 --> 00:07:31,680
discuss why our adv square attack is

183
00:07:30,720 --> 00:07:34,479
possible

184
00:07:31,680 --> 00:07:36,800
we hypothesize that there is a gap

185
00:07:34,479 --> 00:07:39,919
between the classifiers prediction

186
00:07:36,800 --> 00:07:42,960
and the interpreter's interpretation

187
00:07:39,919 --> 00:07:45,280
we conduct a random class interpretation

188
00:07:42,960 --> 00:07:47,359
experiments to validate it

189
00:07:45,280 --> 00:07:49,520
we show the experiment in the bottom

190
00:07:47,360 --> 00:07:51,919
figure in the first row

191
00:07:49,520 --> 00:07:53,359
we first select target images from

192
00:07:51,919 --> 00:07:56,400
random classes

193
00:07:53,360 --> 00:07:58,479
then in the second row we get the maps

194
00:07:56,400 --> 00:08:01,280
of those target images

195
00:07:58,479 --> 00:08:01,919
for the third row we perform adb square

196
00:08:01,280 --> 00:08:04,239
attacks

197
00:08:01,919 --> 00:08:05,840
against the b9 inputs with another set

198
00:08:04,240 --> 00:08:08,240
of random classes

199
00:08:05,840 --> 00:08:10,400
and set target action maps from the

200
00:08:08,240 --> 00:08:12,639
second row

201
00:08:10,400 --> 00:08:16,239
the last row shows the interpretations

202
00:08:12,639 --> 00:08:19,599
of adb square adversarial examples

203
00:08:16,240 --> 00:08:22,319
note that the target and adv square

204
00:08:19,599 --> 00:08:24,878
inputs are fairly distinct but with

205
00:08:22,319 --> 00:08:26,720
highly similar interpretations

206
00:08:24,879 --> 00:08:28,400
the chart on the right side also

207
00:08:26,720 --> 00:08:31,360
indicates this

208
00:08:28,400 --> 00:08:33,279
as we can see our attack is possible

209
00:08:31,360 --> 00:08:35,839
even there are no dependencies

210
00:08:33,279 --> 00:08:40,000
in the target class and the target

211
00:08:35,839 --> 00:08:42,560
attribute map

212
00:08:40,000 --> 00:08:43,200
move one more step and we hypothesize

213
00:08:42,559 --> 00:08:46,000
that

214
00:08:43,200 --> 00:08:46,720
limitations of existing interpretation

215
00:08:46,000 --> 00:08:50,000
models

216
00:08:46,720 --> 00:08:52,240
cause the prediction interpreting gap

217
00:08:50,000 --> 00:08:55,440
different interpreters might focus on

218
00:08:52,240 --> 00:08:58,519
distinct aspects of dna behavior

219
00:08:55,440 --> 00:09:00,399
for example gradient and intermediate

220
00:08:58,519 --> 00:09:02,880
representations

221
00:09:00,399 --> 00:09:04,080
we perform an attack transferability

222
00:09:02,880 --> 00:09:06,800
test here

223
00:09:04,080 --> 00:09:09,120
in each row we have adversarial input

224
00:09:06,800 --> 00:09:12,560
generated by adv square attack

225
00:09:09,120 --> 00:09:14,880
for one specific interpreter then

226
00:09:12,560 --> 00:09:17,199
we inspect the attribute map of this

227
00:09:14,880 --> 00:09:21,360
input on the other three interpolators

228
00:09:17,200 --> 00:09:23,519
in each column ideally for a fixed input

229
00:09:21,360 --> 00:09:24,880
the model should have a fixed behavior

230
00:09:23,519 --> 00:09:27,760
to predict

231
00:09:24,880 --> 00:09:28,880
however we observe that different

232
00:09:27,760 --> 00:09:31,920
interpreters

233
00:09:28,880 --> 00:09:35,040
focus on different regions here

234
00:09:31,920 --> 00:09:36,479
for the last we discuss potential or adb

235
00:09:35,040 --> 00:09:38,959
square attack

236
00:09:36,480 --> 00:09:41,279
due to the low transferabilities of the

237
00:09:38,959 --> 00:09:44,319
attacks shown in the last slide

238
00:09:41,279 --> 00:09:46,880
we could use an example interpreter

239
00:09:44,320 --> 00:09:49,200
in this way multiple complementary

240
00:09:46,880 --> 00:09:50,640
interpreters could fully cover dns

241
00:09:49,200 --> 00:09:53,279
behavior

242
00:09:50,640 --> 00:09:54,080
we also propose an adversarial training

243
00:09:53,279 --> 00:09:56,959
approach

244
00:09:54,080 --> 00:09:57,680
for a model-based interpreter we want to

245
00:09:56,959 --> 00:10:00,479
minimize

246
00:09:57,680 --> 00:10:02,479
the prediction interpretation gap using

247
00:10:00,480 --> 00:10:05,760
adversarial examples

248
00:10:02,480 --> 00:10:07,040
we feed a pair of b9 input and adb

249
00:10:05,760 --> 00:10:09,920
square input to the

250
00:10:07,040 --> 00:10:10,480
interpreter and train the interpreter to

251
00:10:09,920 --> 00:10:12,959
generate

252
00:10:10,480 --> 00:10:14,240
distinct attribute maps for the two

253
00:10:12,959 --> 00:10:16,880
inputs

254
00:10:14,240 --> 00:10:18,000
as we can observe from the third and the

255
00:10:16,880 --> 00:10:20,720
fifth columns of

256
00:10:18,000 --> 00:10:21,279
the figure here the adversarially

257
00:10:20,720 --> 00:10:23,760
trained

258
00:10:21,279 --> 00:10:25,920
interpreters are more sensitive to the

259
00:10:23,760 --> 00:10:27,920
adversarial perturbations

260
00:10:25,920 --> 00:10:30,160
the table on the left side also

261
00:10:27,920 --> 00:10:32,880
indicates this

262
00:10:30,160 --> 00:10:33,439
this paper we propose the adv square

263
00:10:32,880 --> 00:10:36,560
attack

264
00:10:33,440 --> 00:10:40,240
against interpretable learning systems

265
00:10:36,560 --> 00:10:42,719
we conclude our key findings here first

266
00:10:40,240 --> 00:10:44,959
the interpretability of existing

267
00:10:42,720 --> 00:10:47,200
interpretable deep learning systems

268
00:10:44,959 --> 00:10:48,640
merely provides limited security

269
00:10:47,200 --> 00:10:51,839
assurance

270
00:10:48,640 --> 00:10:54,319
second the prediction interpretation gap

271
00:10:51,839 --> 00:10:55,360
is one possible cause for the adversary

272
00:10:54,320 --> 00:10:58,839
to exploit

273
00:10:55,360 --> 00:11:00,000
both the classroom and the interpreter

274
00:10:58,839 --> 00:11:02,640
simultaneously

275
00:11:00,000 --> 00:11:03,279
third adversarial training aiming to

276
00:11:02,640 --> 00:11:05,760
minimize

277
00:11:03,279 --> 00:11:06,480
the prediction in the prediction gap

278
00:11:05,760 --> 00:11:10,880
potentially

279
00:11:06,480 --> 00:11:13,040
improves the robustness of interpreters

280
00:11:10,880 --> 00:11:14,720
i would like to thank you to my audience

281
00:11:13,040 --> 00:11:16,800
and my co-authors here

282
00:11:14,720 --> 00:11:18,240
for any further questions about our

283
00:11:16,800 --> 00:11:20,560
paper you can just

284
00:11:18,240 --> 00:11:22,240
send email to me by the following email

285
00:11:20,560 --> 00:11:29,839
address

286
00:11:22,240 --> 00:11:29,839
any questions from the audience

287
00:11:34,320 --> 00:11:36,399
you

