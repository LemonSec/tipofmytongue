1
00:00:10,719 --> 00:00:15,620
hello everyone thank you for waiting

2
00:00:13,549 --> 00:00:17,840
until my presentation and my name is

3
00:00:15,620 --> 00:00:19,340
Sonia no and I'm a PhD student at the

4
00:00:17,840 --> 00:00:22,250
University of Maryland College Park

5
00:00:19,340 --> 00:00:24,950
today I'm Tito I'm here to talk about

6
00:00:22,250 --> 00:00:27,590
our story of terminal brain damage that

7
00:00:24,950 --> 00:00:29,359
exposes the graceless degradation in the

8
00:00:27,590 --> 00:00:31,609
neural network under hardware fault

9
00:00:29,359 --> 00:00:32,960
attacks this work couldn't have been

10
00:00:31,609 --> 00:00:35,870
published without the awesome

11
00:00:32,960 --> 00:00:38,300
collaborators Pietro Rico each and khaya

12
00:00:35,870 --> 00:00:40,730
professor christian OG Frieda and my

13
00:00:38,300 --> 00:00:43,870
advisor to Dermot rush let's begin our

14
00:00:40,730 --> 00:00:46,190
story let me start by asking a question

15
00:00:43,870 --> 00:00:48,769
have you ever believed a security

16
00:00:46,190 --> 00:00:53,570
property without any skepticism taking

17
00:00:48,769 --> 00:00:56,150
it for a guaranteed amazing the answer

18
00:00:53,570 --> 00:00:59,930
is no that is probably why you're here

19
00:00:56,150 --> 00:01:02,239
but what about this property in 1990 a

20
00:00:59,930 --> 00:01:04,519
computer scientist Ian McEwan published

21
00:01:02,239 --> 00:01:07,250
the seminal paper titled the optimal

22
00:01:04,519 --> 00:01:08,799
brain damage this is the first walk that

23
00:01:07,250 --> 00:01:11,960
reported the graceful degradation

24
00:01:08,799 --> 00:01:14,630
property by showing that we can remove

25
00:01:11,960 --> 00:01:17,229
up to 60% of model parameters without

26
00:01:14,630 --> 00:01:19,250
the actress drop based on this intuition

27
00:01:17,229 --> 00:01:21,320
researchers came up with a bunch of

28
00:01:19,250 --> 00:01:23,750
techniques such as pruning to reduce

29
00:01:21,320 --> 00:01:26,298
their interest cost quantitation to

30
00:01:23,750 --> 00:01:28,969
reduce the network size and blending

31
00:01:26,299 --> 00:01:30,770
noises to improve the robustness but all

32
00:01:28,969 --> 00:01:32,780
these techniques achieve this benefits

33
00:01:30,770 --> 00:01:36,949
without the meaning without the acreage

34
00:01:32,780 --> 00:01:39,829
drop without the acres drop and in fact

35
00:01:36,950 --> 00:01:41,780
prior walks showed that even if you try

36
00:01:39,829 --> 00:01:44,389
to degrade the accuracy of the network

37
00:01:41,780 --> 00:01:46,429
it is difficult they blend a lot of

38
00:01:44,390 --> 00:01:49,219
poison examples into the training data

39
00:01:46,429 --> 00:01:51,469
they cause a lot of random bit errors or

40
00:01:49,219 --> 00:01:53,649
caused a hardware fault randomly but

41
00:01:51,469 --> 00:01:56,149
still the accuracy of the networks

42
00:01:53,649 --> 00:02:00,710
network accuracy drop of the network

43
00:01:56,149 --> 00:02:02,780
were within 11% however all these

44
00:02:00,710 --> 00:02:03,408
practices gives us false sense of

45
00:02:02,780 --> 00:02:06,409
security

46
00:02:03,409 --> 00:02:08,390
about the DNS resilience because they

47
00:02:06,409 --> 00:02:10,819
focused on the best case scenario all

48
00:02:08,389 --> 00:02:13,850
the average case parameters perturbation

49
00:02:10,818 --> 00:02:16,069
scenarios not the worst case ones so

50
00:02:13,850 --> 00:02:18,260
here we ask the question what is the

51
00:02:16,069 --> 00:02:20,089
worst case perturbation a bit flip to a

52
00:02:18,260 --> 00:02:23,459
model parameter that causes a

53
00:02:20,090 --> 00:02:25,620
significant accuracy drop exceeding 10%

54
00:02:23,459 --> 00:02:27,540
here is an illustration of the 101

55
00:02:25,620 --> 00:02:29,579
convolutional neural network which is

56
00:02:27,540 --> 00:02:31,439
the two convolutional layer and two

57
00:02:29,579 --> 00:02:33,780
fully connected layer trained on amnesty

58
00:02:31,439 --> 00:02:36,810
the network has model parameters

59
00:02:33,780 --> 00:02:38,909
describe this Orange Box is optimized in

60
00:02:36,810 --> 00:02:41,370
training and used in inferences to

61
00:02:38,909 --> 00:02:43,620
classify the core data correctly and you

62
00:02:41,370 --> 00:02:46,650
can see the validation accuracy is 98%

63
00:02:43,620 --> 00:02:48,810
and here is what optimal brain damage

64
00:02:46,650 --> 00:02:51,269
paper did here they remove the

65
00:02:48,810 --> 00:02:53,909
unimportant parameters without causing

66
00:02:51,269 --> 00:02:56,159
the accuracy drop also what about the

67
00:02:53,909 --> 00:02:58,409
prior work examine the resilience of a

68
00:02:56,159 --> 00:03:00,780
network to the hardware fault attacks

69
00:02:58,409 --> 00:03:02,909
this is how the model parameters are

70
00:03:00,780 --> 00:03:05,549
loaded into memory while the model is in

71
00:03:02,909 --> 00:03:08,159
use the parameters weight and biases

72
00:03:05,549 --> 00:03:10,919
located in memory and each of them takes

73
00:03:08,159 --> 00:03:12,810
up the continuous memory space so here

74
00:03:10,919 --> 00:03:15,359
is the prior wall causes the randomized

75
00:03:12,810 --> 00:03:18,239
bit random bitwise errors to the model

76
00:03:15,359 --> 00:03:20,430
in memory and here is an example of how

77
00:03:18,239 --> 00:03:22,590
bitwise error can be seen in the

78
00:03:20,430 --> 00:03:24,510
floating-point number representation and

79
00:03:22,590 --> 00:03:27,510
you can see one bit in the exponent

80
00:03:24,510 --> 00:03:30,060
flipped from one to zero and they found

81
00:03:27,510 --> 00:03:33,120
that even if the network has a lot of

82
00:03:30,060 --> 00:03:36,930
such errors on average the accuracy drop

83
00:03:33,120 --> 00:03:39,569
of a network can be within 5% but we

84
00:03:36,930 --> 00:03:42,359
asked a different question can we find

85
00:03:39,569 --> 00:03:44,369
the worst gate speed flip we focused on

86
00:03:42,359 --> 00:03:46,919
one of the same parameters in the last

87
00:03:44,370 --> 00:03:49,139
layer but this time we focused on the

88
00:03:46,919 --> 00:03:51,629
bit flip that causes the extreme changes

89
00:03:49,139 --> 00:03:53,939
in the parameter value so we decided to

90
00:03:51,629 --> 00:03:56,939
flip the most significant bit index on

91
00:03:53,939 --> 00:03:59,849
it from 0 to 1 and the network accuracy

92
00:03:56,939 --> 00:04:03,358
dropped significantly around the 41% by

93
00:03:59,849 --> 00:04:05,698
flipping only the single bit so we

94
00:04:03,359 --> 00:04:07,769
started asking research questions first

95
00:04:05,699 --> 00:04:09,720
how vulnerable all the neural networks

96
00:04:07,769 --> 00:04:12,269
to the worst case single bit flip and

97
00:04:09,720 --> 00:04:14,489
what network properties influence this

98
00:04:12,269 --> 00:04:16,798
vulnerability and can an attacker

99
00:04:14,489 --> 00:04:19,409
exploit this vulnerability in practice

100
00:04:16,798 --> 00:04:23,370
and can be utilized on network level

101
00:04:19,409 --> 00:04:25,440
mechanisms for mitigation to answer we

102
00:04:23,370 --> 00:04:28,380
first evaluate the vulnerability of a

103
00:04:25,440 --> 00:04:29,699
network to evaluate a vulnerability we

104
00:04:28,380 --> 00:04:32,669
come up with a metric called the

105
00:04:29,699 --> 00:04:34,590
relative address drop rat that encode

106
00:04:32,669 --> 00:04:36,510
the ratio of the address drop caused by

107
00:04:34,590 --> 00:04:38,820
the corrupted model

108
00:04:36,510 --> 00:04:41,900
with respect to the clean models

109
00:04:38,820 --> 00:04:44,760
accuracy with the metric we take a model

110
00:04:41,900 --> 00:04:47,099
flip each bit in all parameters of a

111
00:04:44,760 --> 00:04:48,990
model in both 0 to 1 and 1 to 0

112
00:04:47,100 --> 00:04:51,660
directions and measure the relative

113
00:04:48,990 --> 00:04:54,510
accuracy drop over the entire validation

114
00:04:51,660 --> 00:04:56,970
set and here we define the term on a

115
00:04:54,510 --> 00:04:59,219
Killa speed when the bit flips the bleep

116
00:04:56,970 --> 00:05:02,070
flip causes the relative address drop

117
00:04:59,220 --> 00:05:04,530
over 10 percent then we measure the

118
00:05:02,070 --> 00:05:06,780
vulnerability of a model we first look

119
00:05:04,530 --> 00:05:09,359
at the max our ad the maximum damage

120
00:05:06,780 --> 00:05:11,369
that can be caused by an achilles bit

121
00:05:09,360 --> 00:05:14,520
and we look at the ratio of how many

122
00:05:11,370 --> 00:05:16,770
honorable parameters are in a model here

123
00:05:14,520 --> 00:05:19,440
we consider a parameter is vulnerable

124
00:05:16,770 --> 00:05:22,260
when eating could include at least one

125
00:05:19,440 --> 00:05:24,750
Achilles peak we start from the emmys

126
00:05:22,260 --> 00:05:27,030
models and leftmost column indicate the

127
00:05:24,750 --> 00:05:28,440
network that we used the base is the

128
00:05:27,030 --> 00:05:31,020
network that in the previous

129
00:05:28,440 --> 00:05:33,480
illustration and the keyword after the

130
00:05:31,020 --> 00:05:37,200
dash indicate the variation of the base

131
00:05:33,480 --> 00:05:39,510
network and here we found that in all

132
00:05:37,200 --> 00:05:41,520
models there is at least one Achilles

133
00:05:39,510 --> 00:05:45,120
bit that causes the maximum relative

134
00:05:41,520 --> 00:05:48,599
accuracy drop around 90% as we can see

135
00:05:45,120 --> 00:05:51,210
in that column also we found that more

136
00:05:48,600 --> 00:05:54,900
than 45% of the parameters are

137
00:05:51,210 --> 00:05:56,700
vulnerable in all m missed models then

138
00:05:54,900 --> 00:05:58,500
we curious if the vulnerability is

139
00:05:56,700 --> 00:06:00,260
consistent with the larger models

140
00:05:58,500 --> 00:06:03,090
however one challenge here is dead

141
00:06:00,260 --> 00:06:04,789
examining all parameters in a larger

142
00:06:03,090 --> 00:06:07,919
model takes a lot of time for example

143
00:06:04,790 --> 00:06:11,970
examining the widget e16 model trained

144
00:06:07,920 --> 00:06:15,090
on imagenet takes 942 days which will be

145
00:06:11,970 --> 00:06:16,980
three more years to my PhD program so of

146
00:06:15,090 --> 00:06:19,890
course I decided to apply some

147
00:06:16,980 --> 00:06:22,860
heuristics first I use the 10% of the

148
00:06:19,890 --> 00:06:24,599
entire validation dataset and I inspect

149
00:06:22,860 --> 00:06:26,790
the only specific bits such as the

150
00:06:24,600 --> 00:06:29,490
exponent or the most significant

151
00:06:26,790 --> 00:06:31,410
significant bit of the exponent that is

152
00:06:29,490 --> 00:06:34,410
possible to change the parameter value

153
00:06:31,410 --> 00:06:37,170
extremely and for the image named models

154
00:06:34,410 --> 00:06:40,170
a uniform example the 20k parameters to

155
00:06:37,170 --> 00:06:42,000
examine here we examine the Steve Barton

156
00:06:40,170 --> 00:06:44,570
and the image named models and we apply

157
00:06:42,000 --> 00:06:47,370
the speed of heuristics respectively and

158
00:06:44,570 --> 00:06:50,010
the analysis outcome was consistent with

159
00:06:47,370 --> 00:06:50,220
the amnesty analysis wizard in or our

160
00:06:50,010 --> 00:06:51,840
key

161
00:06:50,220 --> 00:06:54,690
textures and the parameters that we

162
00:06:51,840 --> 00:06:57,210
examined we found that each model has at

163
00:06:54,690 --> 00:06:59,070
least one Achilles Pete whose Phillippe

164
00:06:57,210 --> 00:07:02,789
causes the relative actually strapped up

165
00:06:59,070 --> 00:07:04,440
to 100% also 40 to 50 percent of

166
00:07:02,790 --> 00:07:08,580
parameters that we examined are

167
00:07:04,440 --> 00:07:10,620
vulnerable okay we know the networks are

168
00:07:08,580 --> 00:07:13,409
vulnerable but what are the properties

169
00:07:10,620 --> 00:07:15,090
that influence this vulnerability we

170
00:07:13,410 --> 00:07:17,430
look at the impact of the two factors

171
00:07:15,090 --> 00:07:20,219
network properties and the bitwise

172
00:07:17,430 --> 00:07:22,440
representation of parameters and this is

173
00:07:20,220 --> 00:07:23,670
one of the most surprising research we

174
00:07:22,440 --> 00:07:25,620
say mean if the common training

175
00:07:23,670 --> 00:07:27,900
techniques such as dropout and the batch

176
00:07:25,620 --> 00:07:30,510
normalization decrease the vulnerability

177
00:07:27,900 --> 00:07:32,489
because these techniques prevent the

178
00:07:30,510 --> 00:07:34,740
decision of a neural network from

179
00:07:32,490 --> 00:07:36,360
relying on few parameters first the

180
00:07:34,740 --> 00:07:39,540
techniques may reduce the maximum

181
00:07:36,360 --> 00:07:41,970
relative every straw or the number of

182
00:07:39,540 --> 00:07:44,310
vulnerable parameters as well for this

183
00:07:41,970 --> 00:07:46,830
we examine the amnesty and the c14

184
00:07:44,310 --> 00:07:49,020
models in the table the - dropout and

185
00:07:46,830 --> 00:07:51,719
the - norm indicate the dropout and the

186
00:07:49,020 --> 00:07:53,609
bastion normalization is used and we

187
00:07:51,720 --> 00:07:55,890
found that the vulnerabilities are

188
00:07:53,610 --> 00:07:58,650
similar to the base models the maximum

189
00:07:55,890 --> 00:08:01,440
relative address drop can still up to 99

190
00:07:58,650 --> 00:08:04,469
percent and more than 41 percent of the

191
00:08:01,440 --> 00:08:06,360
parameters are vulnerable we also

192
00:08:04,470 --> 00:08:08,340
investigated other properties and the

193
00:08:06,360 --> 00:08:10,320
details are listed in our paper but I'm

194
00:08:08,340 --> 00:08:12,750
skipping them for the lack of time but

195
00:08:10,320 --> 00:08:15,360
in summary the research are the

196
00:08:12,750 --> 00:08:18,419
vulnerability increases proportionally

197
00:08:15,360 --> 00:08:20,430
with the network with and the activation

198
00:08:18,419 --> 00:08:22,919
that allows the negative value doubles

199
00:08:20,430 --> 00:08:24,990
the vulnerability and the vulnerability

200
00:08:22,919 --> 00:08:28,200
is consistent across the 19 d neural

201
00:08:24,990 --> 00:08:30,419
network architectures we also look at

202
00:08:28,200 --> 00:08:32,819
how the bitwise representation impact

203
00:08:30,419 --> 00:08:34,650
the vulnerability in our paper we found

204
00:08:32,820 --> 00:08:37,169
that the flips in the most significant

205
00:08:34,650 --> 00:08:39,838
bit of the exponent bit lead it to the

206
00:08:37,169 --> 00:08:42,240
actress drop over ten percent and only

207
00:08:39,839 --> 00:08:45,030
zero to one flip Direction lead to the

208
00:08:42,240 --> 00:08:47,040
same amount of the Acura strop and the

209
00:08:45,030 --> 00:08:49,560
positive of parameters mostly continue

210
00:08:47,040 --> 00:08:52,770
to contribute to the vulnerability then

211
00:08:49,560 --> 00:08:54,989
the negative parameters now the question

212
00:08:52,770 --> 00:08:57,600
is can an attacker exploit this

213
00:08:54,990 --> 00:08:59,760
vulnerability let's look at who's our

214
00:08:57,600 --> 00:09:02,040
attacker we define the two attackers

215
00:08:59,760 --> 00:09:03,390
based on the capability surgical and

216
00:09:02,040 --> 00:09:05,550
blind attacker

217
00:09:03,390 --> 00:09:07,740
Surgical attacker can flip the bit in an

218
00:09:05,550 --> 00:09:10,410
intended location in memory whereas the

219
00:09:07,740 --> 00:09:12,600
blind attacker cannot this capability is

220
00:09:10,410 --> 00:09:15,270
important since our attacker cropped the

221
00:09:12,600 --> 00:09:17,220
victim model during one time unlike the

222
00:09:15,270 --> 00:09:20,579
supply-chain attacks such as the Trojan

223
00:09:17,220 --> 00:09:23,730
II to illustrate we have an axis based

224
00:09:20,580 --> 00:09:25,680
on our attackers capability then we

225
00:09:23,730 --> 00:09:27,600
introduced to other attacker based on

226
00:09:25,680 --> 00:09:29,910
their knowledge fight box and the black

227
00:09:27,600 --> 00:09:32,190
box attacker white puffs attacker knows

228
00:09:29,910 --> 00:09:34,140
the victim models in Turners such as

229
00:09:32,190 --> 00:09:37,200
which parameters are vulnerable and

230
00:09:34,140 --> 00:09:40,830
where they are in the memory or however

231
00:09:37,200 --> 00:09:43,080
the black path attacker does not we also

232
00:09:40,830 --> 00:09:45,240
make an horizontal axis based on the

233
00:09:43,080 --> 00:09:47,790
knowledge then let's look at what single

234
00:09:45,240 --> 00:09:49,470
bit adversary can do in this plane the

235
00:09:47,790 --> 00:09:51,719
upper right corner is the strongest

236
00:09:49,470 --> 00:09:53,880
attacker who knows - which bit in a

237
00:09:51,720 --> 00:09:56,130
model is vulnerable and has the

238
00:09:53,880 --> 00:09:58,920
capability to flip the specific bit in

239
00:09:56,130 --> 00:10:00,960
this case the attackers probability of a

240
00:09:58,920 --> 00:10:04,800
heating and a Killa speed would be 100%

241
00:10:00,960 --> 00:10:06,810
on the opposite side here there is the

242
00:10:04,800 --> 00:10:08,910
weakest attacker who doesn't know that

243
00:10:06,810 --> 00:10:10,979
which bit to flip and cannot control the

244
00:10:08,910 --> 00:10:13,140
bit flip location as well in this case

245
00:10:10,980 --> 00:10:16,020
the probability of a hitting an achilles

246
00:10:13,140 --> 00:10:18,449
bit will be much lower for example in

247
00:10:16,020 --> 00:10:20,640
the village 60 model trained on imagenet

248
00:10:18,450 --> 00:10:23,010
that has 42 percent of vulnerable

249
00:10:20,640 --> 00:10:25,710
parameters the successful rate of

250
00:10:23,010 --> 00:10:28,020
success rate of our attacker will be 1.3

251
00:10:25,710 --> 00:10:30,360
percent since we assume conservatively

252
00:10:28,020 --> 00:10:34,410
that one of the 32-bit in every

253
00:10:30,360 --> 00:10:37,500
parameter is an Achilles P let's examine

254
00:10:34,410 --> 00:10:40,589
how our taker flip this flip an Achilles

255
00:10:37,500 --> 00:10:42,900
P we try with low hammer since it's well

256
00:10:40,590 --> 00:10:44,730
understood hardware fault attack low

257
00:10:42,900 --> 00:10:47,130
hammer provide the single bit corruption

258
00:10:44,730 --> 00:10:49,350
primitives at TM level and it's the

259
00:10:47,130 --> 00:10:51,390
software induced abort attack first the

260
00:10:49,350 --> 00:10:54,960
attacker only requires to access a user

261
00:10:51,390 --> 00:10:57,360
level memory to illustrate this is the

262
00:10:54,960 --> 00:10:59,760
picture of a DRM Bank and the data is

263
00:10:57,360 --> 00:11:01,920
stored in the middle row when an

264
00:10:59,760 --> 00:11:04,110
attacker accesses the blue role multiple

265
00:11:01,920 --> 00:11:06,060
times then the accesses leaks the

266
00:11:04,110 --> 00:11:08,880
capacitor charges in the middle low and

267
00:11:06,060 --> 00:11:11,790
the flip on bit there so our attacker

268
00:11:08,880 --> 00:11:13,530
has this blue hammer and we consider the

269
00:11:11,790 --> 00:11:15,480
weakest attacker scenario where the

270
00:11:13,530 --> 00:11:17,600
attacker has one point two three success

271
00:11:15,480 --> 00:11:20,700
rate of heating and

272
00:11:17,600 --> 00:11:23,370
however let's say that if our adversary

273
00:11:20,700 --> 00:11:25,320
can flee multiple bits in the model the

274
00:11:23,370 --> 00:11:27,990
probability of a hitting on achilles bit

275
00:11:25,320 --> 00:11:29,990
increases and the weakest attacker can

276
00:11:27,990 --> 00:11:32,790
quickly turn into the stronger attacker

277
00:11:29,990 --> 00:11:35,250
we evaluate this type of the attacker in

278
00:11:32,790 --> 00:11:36,990
the practical case we consider the

279
00:11:35,250 --> 00:11:39,330
machine learning as a service scenario

280
00:11:36,990 --> 00:11:42,029
where a dedicated VM is running under

281
00:11:39,330 --> 00:11:44,070
the pressure of low hammer for that we

282
00:11:42,029 --> 00:11:45,990
run a Python process that constantly

283
00:11:44,070 --> 00:11:48,779
queries the village is 16 imogen and

284
00:11:45,990 --> 00:11:50,940
model and attacker makes bit flips to

285
00:11:48,779 --> 00:11:54,120
the process memory the flip could be on

286
00:11:50,940 --> 00:11:57,480
the code or the data region therefore

287
00:11:54,120 --> 00:12:00,480
the expected consequences are we hit an

288
00:11:57,480 --> 00:12:01,860
Achilles be a process crash or hitting

289
00:12:00,480 --> 00:12:05,160
an a bit that doesn't lead to the

290
00:12:01,860 --> 00:12:07,260
serious research to explore and quantify

291
00:12:05,160 --> 00:12:09,180
the effect of this row hammer attack we

292
00:12:07,260 --> 00:12:11,339
use the hammer time database that

293
00:12:09,180 --> 00:12:14,010
contains the vulnerable bit location in

294
00:12:11,339 --> 00:12:16,050
12 different drm configurations we

295
00:12:14,010 --> 00:12:18,149
define the vulnerability of a drm based

296
00:12:16,050 --> 00:12:20,010
on the number of bits subjected to the

297
00:12:18,149 --> 00:12:22,920
row hammer attack and this setting

298
00:12:20,010 --> 00:12:24,209
allows us to systematically quantify the

299
00:12:22,920 --> 00:12:26,219
attack consquences

300
00:12:24,209 --> 00:12:29,579
not just observing one case of

301
00:12:26,220 --> 00:12:32,310
exploitation we conducted a 25

302
00:12:29,579 --> 00:12:34,589
experiment for each of 12 drm

303
00:12:32,310 --> 00:12:37,140
configurations and each experiment is

304
00:12:34,589 --> 00:12:40,320
composed of 300 cumulative bit philippe

305
00:12:37,140 --> 00:12:42,870
attempt and we found that line attacker

306
00:12:40,320 --> 00:12:46,220
can caused terminal brain damage to the

307
00:12:42,870 --> 00:12:49,440
victim model effectively on average 60%

308
00:12:46,220 --> 00:12:51,510
62 percent of the entire experiment the

309
00:12:49,440 --> 00:12:54,300
attacker was successful to hit an

310
00:12:51,510 --> 00:12:57,149
Achilles B with the most vulnerable drm

311
00:12:54,300 --> 00:13:00,449
configurations the success rate goes up

312
00:12:57,149 --> 00:13:02,970
to the 96% but with the least one the

313
00:13:00,450 --> 00:13:05,370
success rate decreased to 4% was still

314
00:13:02,970 --> 00:13:08,970
higher than 1.3 percent that we

315
00:13:05,370 --> 00:13:10,800
calculated before and we figured out it

316
00:13:08,970 --> 00:13:13,079
is difficult to detect the blind

317
00:13:10,800 --> 00:13:16,199
attacker since we only observed 6

318
00:13:13,079 --> 00:13:19,290
questions over the entire 7.5 bit we

319
00:13:16,200 --> 00:13:21,420
intent the wizard implies that the blind

320
00:13:19,290 --> 00:13:26,189
warhammer attacker is practical against

321
00:13:21,420 --> 00:13:29,410
the team neural network models then we

322
00:13:26,190 --> 00:13:31,689
have the last model session differences

323
00:13:29,410 --> 00:13:33,910
there are several existing defenses

324
00:13:31,689 --> 00:13:35,910
against a warhammer attack in the

325
00:13:33,910 --> 00:13:38,949
hardware level or the system level

326
00:13:35,910 --> 00:13:41,079
however the problem here is that they

327
00:13:38,949 --> 00:13:43,358
require the infrastructure wider changes

328
00:13:41,079 --> 00:13:46,539
such as updating the hypervisors or the

329
00:13:43,359 --> 00:13:48,009
operating systems in the cloud or these

330
00:13:46,539 --> 00:13:50,249
are not effective when an attacker

331
00:13:48,009 --> 00:13:52,899
decided to use other attack vectors

332
00:13:50,249 --> 00:13:54,669
instead we investigated the metrical

333
00:13:52,899 --> 00:13:56,229
level differences which would defend

334
00:13:54,669 --> 00:13:58,239
against the hardware fort attacks

335
00:13:56,229 --> 00:14:00,699
regardless of the way how they're

336
00:13:58,239 --> 00:14:03,489
injected we restrict the activation

337
00:14:00,699 --> 00:14:04,959
values using activation functions or use

338
00:14:03,489 --> 00:14:07,779
low precision numbers such as

339
00:14:04,959 --> 00:14:10,508
quantization or binarization the main

340
00:14:07,779 --> 00:14:12,789
idea behind them is to prevent extreme

341
00:14:10,509 --> 00:14:15,819
activation values caused by flipping an

342
00:14:12,789 --> 00:14:18,069
Achilles bit since we are in a lack of

343
00:14:15,819 --> 00:14:20,169
time I will not present the details but

344
00:14:18,069 --> 00:14:22,539
the lessons are we found that the two

345
00:14:20,169 --> 00:14:24,039
directions have pros and cons they both

346
00:14:22,539 --> 00:14:27,459
reduced a number of vulnerable

347
00:14:24,039 --> 00:14:29,799
parameters which is nice however at the

348
00:14:27,459 --> 00:14:32,949
same time we require to retrain a whole

349
00:14:29,799 --> 00:14:34,600
model from scratch we also found that

350
00:14:32,949 --> 00:14:36,008
there is a way to mitigate without

351
00:14:34,600 --> 00:14:38,019
retraining using the activation

352
00:14:36,009 --> 00:14:39,909
functions however in this case we

353
00:14:38,019 --> 00:14:42,519
observed a performance loss of the

354
00:14:39,909 --> 00:14:44,709
network now we have the answers for our

355
00:14:42,519 --> 00:14:47,079
research questions first we found that

356
00:14:44,709 --> 00:14:49,238
all models have an Achilles pit whose

357
00:14:47,079 --> 00:14:52,959
flip causes the relative address drop up

358
00:14:49,239 --> 00:14:55,119
to 100% and 40 to 50% of all parameters

359
00:14:52,959 --> 00:14:57,849
in a model that we examined are

360
00:14:55,119 --> 00:15:00,009
vulnerable second de-spawn durability is

361
00:14:57,850 --> 00:15:01,659
general especially we found that the

362
00:15:00,009 --> 00:15:03,100
vulnerability is not affected by the

363
00:15:01,659 --> 00:15:05,439
common training techniques such as

364
00:15:03,100 --> 00:15:08,289
dropout or The Bachelor magicians and

365
00:15:05,439 --> 00:15:10,209
third we show that even our blind

366
00:15:08,289 --> 00:15:12,159
warhammer attacker can successfully

367
00:15:10,209 --> 00:15:13,868
exploit this vulnerability in the

368
00:15:12,159 --> 00:15:16,779
machine learning as a service scenario

369
00:15:13,869 --> 00:15:18,669
and lastly with some we see some

370
00:15:16,779 --> 00:15:21,220
promising wizards such as reducing the

371
00:15:18,669 --> 00:15:23,589
vulnerability however this is not

372
00:15:21,220 --> 00:15:25,839
practical yet our defense is degree the

373
00:15:23,589 --> 00:15:27,849
performance or require the expensive

374
00:15:25,839 --> 00:15:32,169
retraining process so further research

375
00:15:27,849 --> 00:15:33,849
here is required okay but what will be

376
00:15:32,169 --> 00:15:36,339
the implications to the security

377
00:15:33,849 --> 00:15:37,899
community first one we think that we

378
00:15:36,339 --> 00:15:39,569
need to weed them in the techniques

379
00:15:37,899 --> 00:15:42,730
relying on the graceful degradation

380
00:15:39,569 --> 00:15:44,620
property with security lens and

381
00:15:42,730 --> 00:15:46,930
we think the vulnerability of the neural

382
00:15:44,620 --> 00:15:49,570
networks to microarchitecture attack is

383
00:15:46,930 --> 00:15:51,489
under studied area so we encourage the

384
00:15:49,570 --> 00:15:53,440
security community to think about the

385
00:15:51,490 --> 00:15:55,990
implications to the real world

386
00:15:53,440 --> 00:15:56,940
AI systems empowered by the neural

387
00:15:55,990 --> 00:16:00,459
network

388
00:15:56,940 --> 00:16:02,889
lastly system level defenses for such

389
00:16:00,459 --> 00:16:05,680
attacks are not sufficient so we need to

390
00:16:02,889 --> 00:16:08,889
consider additional model level defenses

391
00:16:05,680 --> 00:16:10,180
account for network properties so this

392
00:16:08,889 --> 00:16:12,339
is the end of our story

393
00:16:10,180 --> 00:16:13,959
I appreciate for your listening and if

394
00:16:12,339 --> 00:16:15,850
you're interested in our vision

395
00:16:13,959 --> 00:16:19,329
please check more stories in our website

396
00:16:15,850 --> 00:16:21,420
hardware failed at ml thank you thank

397
00:16:19,329 --> 00:16:21,420
you

398
00:16:25,410 --> 00:16:34,990
hi great talk many production models use

399
00:16:30,550 --> 00:16:42,069
float16 or you end date what happens in

400
00:16:34,990 --> 00:16:44,470
those settings so in our paper we didn't

401
00:16:42,070 --> 00:16:45,790
look at the effect of the the precision

402
00:16:44,470 --> 00:16:50,680
of the floating-point number

403
00:16:45,790 --> 00:16:54,010
representation but I can assume that if

404
00:16:50,680 --> 00:16:55,839
the 16-bit floating-point number allows

405
00:16:54,010 --> 00:16:58,660
the extreme changes in the parameter

406
00:16:55,839 --> 00:17:07,290
value so be float16 does but float16

407
00:16:58,660 --> 00:17:07,290
doesn't as much so be flipped

408
00:17:07,650 --> 00:17:14,650
any other questions yeah so I have a

409
00:17:11,650 --> 00:17:17,589
question about the results actually so

410
00:17:14,650 --> 00:17:20,949
in a DNN model if I look at it as a bag

411
00:17:17,589 --> 00:17:23,500
of bits and if I flip any of them what

412
00:17:20,949 --> 00:17:26,049
fraction of those bits if I were to flip

413
00:17:23,500 --> 00:17:31,270
them nothing much would change in the

414
00:17:26,049 --> 00:17:34,510
accuracy of the model is it more than

415
00:17:31,270 --> 00:17:36,549
50% I think that this yeah that's a very

416
00:17:34,510 --> 00:17:40,059
good question I think that more than 50%

417
00:17:36,549 --> 00:17:42,929
of the bit that when the bit is played

418
00:17:40,059 --> 00:17:46,059
the actress drop is not more than 10%

419
00:17:42,929 --> 00:17:48,669
but the important takeaway here is that

420
00:17:46,059 --> 00:17:50,860
there is a lot of Achilles bit found in

421
00:17:48,669 --> 00:17:52,030
the DMM model clearly there are critical

422
00:17:50,860 --> 00:17:53,918
base that if you were to flip them

423
00:17:52,030 --> 00:17:56,220
really bad things up yeah thank you

424
00:17:53,919 --> 00:17:56,220
thank you

425
00:17:59,390 --> 00:18:04,230
all right let's thank the speaker again

426
00:18:02,070 --> 00:18:09,480
unless there's another question oh one

427
00:18:04,230 --> 00:18:12,000
more question do you have an estimate if

428
00:18:09,480 --> 00:18:15,840
someone used binary neural network how

429
00:18:12,000 --> 00:18:17,520
this accuracy number will change the

430
00:18:15,840 --> 00:18:20,039
success rate of your attack

431
00:18:17,520 --> 00:18:22,408
so actually we include that in our paper

432
00:18:20,039 --> 00:18:24,870
we use the binary relation for the

433
00:18:22,409 --> 00:18:27,150
amnesty model and we measure the how

434
00:18:24,870 --> 00:18:29,459
much vulnerability can occurred in them

435
00:18:27,150 --> 00:18:32,970
mod also maybe look at the paper will

436
00:18:29,460 --> 00:18:34,960
answer that question all right let's

437
00:18:32,970 --> 00:18:39,930
take this speaker again

438
00:18:34,960 --> 00:18:39,930
[Applause]

