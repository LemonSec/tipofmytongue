1
00:00:10,599 --> 00:00:15,910
so I'm Nicolas and I'm gonna be talking

2
00:00:13,629 --> 00:00:17,759
about some lessons that we've learned

3
00:00:15,910 --> 00:00:20,039
from evaluating the robustness of

4
00:00:17,759 --> 00:00:23,439
defenses to adversarial examples

5
00:00:20,039 --> 00:00:24,820
so the way I'm structuring this talk is

6
00:00:23,439 --> 00:00:27,570
I'm basically going to be working

7
00:00:24,820 --> 00:00:30,039
backwards I'm going to start by defining

8
00:00:27,570 --> 00:00:31,630
adversarial examples then I'm going to

9
00:00:30,039 --> 00:00:33,460
tell you about defenses to adversarial

10
00:00:31,630 --> 00:00:35,080
examples then I'll talk about what it

11
00:00:33,460 --> 00:00:36,970
means to evaluate their robustness and

12
00:00:35,080 --> 00:00:40,480
then finally the lessons we've learned

13
00:00:36,970 --> 00:00:41,680
from doing that so to make sure that

14
00:00:40,480 --> 00:00:44,078
we're all on the same page with

15
00:00:41,680 --> 00:00:46,180
adversarial examples the standard set up

16
00:00:44,079 --> 00:00:48,360
goes something like this you have an

17
00:00:46,180 --> 00:00:51,579
image which all of us recognize

18
00:00:48,360 --> 00:00:53,140
correctly as a cat and indeed a

19
00:00:51,579 --> 00:00:55,600
state-of-the-art neural network we'll

20
00:00:53,140 --> 00:00:57,910
get it right with something like 88

21
00:00:55,600 --> 00:00:59,559
percent confidence and it turns out that

22
00:00:57,910 --> 00:01:02,679
you can introduce a very small

23
00:00:59,559 --> 00:01:04,989
perturbation that basically none of us

24
00:01:02,680 --> 00:01:07,479
as humans can recognize to get a new

25
00:01:04,989 --> 00:01:09,460
image that looks something like this so

26
00:01:07,479 --> 00:01:11,649
that this image here is recognized

27
00:01:09,460 --> 00:01:14,500
incorrectly by the same neural network

28
00:01:11,649 --> 00:01:17,430
that gets the other one right with 99%

29
00:01:14,500 --> 00:01:19,930
confidence with the label guacamole and

30
00:01:17,430 --> 00:01:22,570
so this is the problem ever so examples

31
00:01:19,930 --> 00:01:24,630
is that these two images which are

32
00:01:22,570 --> 00:01:27,158
indistinguishable to us as humans are

33
00:01:24,630 --> 00:01:29,770
completely different objects as far as a

34
00:01:27,159 --> 00:01:31,119
state-of-the-art neural network goes now

35
00:01:29,770 --> 00:01:33,399
a virtual examples are not only a

36
00:01:31,119 --> 00:01:35,590
problem on images they also exist on

37
00:01:33,400 --> 00:01:38,350
text and on malware classification and

38
00:01:35,590 --> 00:01:40,750
on audio but for most of this talk I'm

39
00:01:38,350 --> 00:01:46,509
focusing on images just because they're

40
00:01:40,750 --> 00:01:48,040
nice to show on a slide okay so why

41
00:01:46,509 --> 00:01:48,700
should we care about ever so examples to

42
00:01:48,040 --> 00:01:51,970
begin with

43
00:01:48,700 --> 00:01:53,290
there are basically two reasons why the

44
00:01:51,970 --> 00:01:55,539
first reason is we want to make machine

45
00:01:53,290 --> 00:01:56,829
learning robust fortunately in the

46
00:01:55,540 --> 00:01:58,659
security audience I don't really need to

47
00:01:56,829 --> 00:02:00,548
justify this fact too much we're going

48
00:01:58,659 --> 00:02:02,770
to be using these in security critical

49
00:02:00,549 --> 00:02:06,100
situations ideally they should behave

50
00:02:02,770 --> 00:02:07,570
robustly but even if you don't care

51
00:02:06,100 --> 00:02:09,820
about the security aspects which

52
00:02:07,570 --> 00:02:11,140
probably is none of you here you should

53
00:02:09,820 --> 00:02:13,690
care about it just from the machine

54
00:02:11,140 --> 00:02:15,570
learning side because ideally we would

55
00:02:13,690 --> 00:02:17,890
like to make machine learning better and

56
00:02:15,570 --> 00:02:21,280
episode examples give us a very nice

57
00:02:17,890 --> 00:02:24,339
case study where humans do very well

58
00:02:21,280 --> 00:02:24,510
the systems do very very poorly and if

59
00:02:24,340 --> 00:02:26,939
you

60
00:02:24,510 --> 00:02:28,230
see this gap is being something which is

61
00:02:26,939 --> 00:02:29,730
worth closing then maybe you should

62
00:02:28,230 --> 00:02:33,450
study adversely examples for that reason

63
00:02:29,730 --> 00:02:36,659
only so there's one more reason to think

64
00:02:33,450 --> 00:02:38,310
about episode examples so this is a plot

65
00:02:36,659 --> 00:02:40,310
of the number of papers on adversity

66
00:02:38,310 --> 00:02:45,079
examples over the last couple of years

67
00:02:40,310 --> 00:02:49,170
this is growing almost exponentially and

68
00:02:45,079 --> 00:02:50,640
so as a result all of you are going to

69
00:02:49,170 --> 00:02:51,929
be working on Eversole examples within

70
00:02:50,640 --> 00:02:58,108
some time in the near future otherwise

71
00:02:51,930 --> 00:03:03,060
this trend can't continue so okay how do

72
00:02:58,109 --> 00:03:06,510
we generate adversarial examples so this

73
00:03:03,060 --> 00:03:09,420
here is a plot of decision boundary of a

74
00:03:06,510 --> 00:03:11,909
neural network so what I'm showing here

75
00:03:09,420 --> 00:03:15,750
is each point here corresponds to a

76
00:03:11,909 --> 00:03:19,560
particular image so that this image in

77
00:03:15,750 --> 00:03:21,030
the middle is classified as dog all of

78
00:03:19,560 --> 00:03:23,790
the other blue points here are also

79
00:03:21,030 --> 00:03:26,940
classified as dog and I'm going to add

80
00:03:23,790 --> 00:03:28,948
one type of random noise in one axis so

81
00:03:26,940 --> 00:03:31,970
that for example I get this image here

82
00:03:28,949 --> 00:03:34,379
which is a noisy version of that dog as

83
00:03:31,970 --> 00:03:36,510
I go and the other axis the same thing

84
00:03:34,379 --> 00:03:38,970
happens I add random noise to get other

85
00:03:36,510 --> 00:03:41,040
images that are perturbed and you'll

86
00:03:38,970 --> 00:03:43,260
notice that if I were to draw like a

87
00:03:41,040 --> 00:03:45,418
small box around the center it looks

88
00:03:43,260 --> 00:03:47,190
like I'm safe everything here is

89
00:03:45,419 --> 00:03:51,120
classified the same way and I think that

90
00:03:47,190 --> 00:03:53,099
I'm fine the problem though is that this

91
00:03:51,120 --> 00:03:55,980
is just a two-dimensional slice of a

92
00:03:53,099 --> 00:03:57,810
very very high dimensional surface I've

93
00:03:55,980 --> 00:04:00,840
picked two random orthogonal directions

94
00:03:57,810 --> 00:04:02,069
to show this plot in but there are three

95
00:04:00,840 --> 00:04:05,310
thousand other ones I could have

96
00:04:02,069 --> 00:04:07,349
selected and so the question is why is

97
00:04:05,310 --> 00:04:09,109
it the case that I can still find out

98
00:04:07,349 --> 00:04:11,399
visual examples on neural networks and

99
00:04:09,109 --> 00:04:14,400
the reason is that it turns out that

100
00:04:11,400 --> 00:04:16,680
while in these two dimensions I happen

101
00:04:14,400 --> 00:04:18,180
to be safe I can add random noise and it

102
00:04:16,680 --> 00:04:20,880
doesn't really change the classification

103
00:04:18,180 --> 00:04:23,340
for example I don't really think that

104
00:04:20,880 --> 00:04:25,590
the image way up there is an adverse

105
00:04:23,340 --> 00:04:28,049
example like it's it's far too hard to

106
00:04:25,590 --> 00:04:30,000
recognize that object as anything as a

107
00:04:28,050 --> 00:04:33,150
human I would like something which is

108
00:04:30,000 --> 00:04:35,280
much less perturbed so what I can do is

109
00:04:33,150 --> 00:04:37,200
I can just switch the axis that I'm

110
00:04:35,280 --> 00:04:37,979
looking at sort of rotate this plot

111
00:04:37,200 --> 00:04:40,380
through

112
00:04:37,980 --> 00:04:43,260
3000 dimensional space and I can end up

113
00:04:40,380 --> 00:04:45,240
with something that looks like this so

114
00:04:43,260 --> 00:04:48,420
I'm adding the same amount of the same

115
00:04:45,240 --> 00:04:51,450
direction of noise on the y-axis but on

116
00:04:48,420 --> 00:04:55,200
the x-axis now I'm adding sort of the

117
00:04:51,450 --> 00:04:58,440
worst case adversarial noise so that

118
00:04:55,200 --> 00:05:01,229
here this image is now on the other side

119
00:04:58,440 --> 00:05:02,340
of this ition boundary and this really

120
00:05:01,230 --> 00:05:04,620
is the fundamental problem of

121
00:05:02,340 --> 00:05:06,690
adversarial examples is that neural

122
00:05:04,620 --> 00:05:08,490
networks make classifications where in

123
00:05:06,690 --> 00:05:10,680
benign settings you can add lots of

124
00:05:08,490 --> 00:05:13,170
noise and they're very robust but the

125
00:05:10,680 --> 00:05:17,400
worst case Direction is highly sensitive

126
00:05:13,170 --> 00:05:19,800
and can change the classification let me

127
00:05:17,400 --> 00:05:21,120
do one more thing to this plot just so I

128
00:05:19,800 --> 00:05:23,400
can make it a little more clear what's

129
00:05:21,120 --> 00:05:26,580
happening and I'm going to extend the

130
00:05:23,400 --> 00:05:28,409
z-axis up so that the z-axis now is

131
00:05:26,580 --> 00:05:30,000
going to correspond to the confidence of

132
00:05:28,410 --> 00:05:33,480
the neural network in the correct

133
00:05:30,000 --> 00:05:36,840
prediction I'll say that one more time

134
00:05:33,480 --> 00:05:39,390
so this here is the plot the confidence

135
00:05:36,840 --> 00:05:40,950
is now of the correct prediction as

136
00:05:39,390 --> 00:05:43,409
determined by the neural network is now

137
00:05:40,950 --> 00:05:44,789
the z-axis so the initial image is

138
00:05:43,410 --> 00:05:47,940
highly confident in the correct

139
00:05:44,790 --> 00:05:49,260
prediction of being dog but you can

140
00:05:47,940 --> 00:05:52,469
notice that you're sort of on the face

141
00:05:49,260 --> 00:05:54,680
of this mountain and it's very easy to

142
00:05:52,470 --> 00:05:58,230
just find the direction to go down and

143
00:05:54,680 --> 00:06:00,390
use gradient descent to find an episode

144
00:05:58,230 --> 00:06:02,340
example just by walking down the face of

145
00:06:00,390 --> 00:06:03,900
this hill and very quickly you end up

146
00:06:02,340 --> 00:06:07,080
with something which essentially is 0%

147
00:06:03,900 --> 00:06:09,090
confidence in dog and is now very high

148
00:06:07,080 --> 00:06:13,140
confidence and in this case the airplane

149
00:06:09,090 --> 00:06:15,450
class so this is kind of what's going on

150
00:06:13,140 --> 00:06:17,820
here with adversarial examples is that

151
00:06:15,450 --> 00:06:19,170
in random directions the classifiers are

152
00:06:17,820 --> 00:06:25,380
good in the adversarial directions the

153
00:06:19,170 --> 00:06:27,030
classifiers are bad ok so then what do

154
00:06:25,380 --> 00:06:30,480
we do with defenses to adversarial

155
00:06:27,030 --> 00:06:32,909
examples so a defense is a neural

156
00:06:30,480 --> 00:06:35,490
network with two properties the first

157
00:06:32,910 --> 00:06:37,620
property is that it's accurate on some

158
00:06:35,490 --> 00:06:40,470
test set of data we want the classifier

159
00:06:37,620 --> 00:06:42,620
to be good on normal settings still but

160
00:06:40,470 --> 00:06:46,970
we also want the classifier to be robust

161
00:06:42,620 --> 00:06:49,470
to an adversary who's trying to evade it

162
00:06:46,970 --> 00:06:51,180
so let me give you a couple examples of

163
00:06:49,470 --> 00:06:51,630
Defense's that have been considered in

164
00:06:51,180 --> 00:06:54,540
the research

165
00:06:51,630 --> 00:06:57,780
space the first example is called

166
00:06:54,540 --> 00:06:59,460
adversarial training this sort of makes

167
00:06:57,780 --> 00:07:01,400
the claim that neural networks don't

168
00:06:59,460 --> 00:07:03,870
generalize to out of distribution data

169
00:07:01,400 --> 00:07:06,870
Abra so examples are maybe somehow out

170
00:07:03,870 --> 00:07:09,270
of distribution and so therefore we

171
00:07:06,870 --> 00:07:12,660
should train on the episode examples to

172
00:07:09,270 --> 00:07:14,880
to become robust to them so the way this

173
00:07:12,660 --> 00:07:17,130
works is fairly simple so in normal

174
00:07:14,880 --> 00:07:18,930
training what we do is we take our

175
00:07:17,130 --> 00:07:21,240
training data we feed it into some

176
00:07:18,930 --> 00:07:22,890
training algorithm machine learning

177
00:07:21,240 --> 00:07:25,200
happens and then we get out some

178
00:07:22,890 --> 00:07:28,500
function f this is a classifier which

179
00:07:25,200 --> 00:07:29,580
does well on the training data so now

180
00:07:28,500 --> 00:07:32,310
what I'm going to do for adversarial

181
00:07:29,580 --> 00:07:35,430
training is maybe the very natural thing

182
00:07:32,310 --> 00:07:36,870
I'm going to take the images that I just

183
00:07:35,430 --> 00:07:40,110
had I'm going to generate adverse

184
00:07:36,870 --> 00:07:41,760
examples for each of them and now I have

185
00:07:40,110 --> 00:07:43,400
these adverts or examples I'm going to

186
00:07:41,760 --> 00:07:45,780
copy them back over to the training data

187
00:07:43,400 --> 00:07:48,270
I'm going to give them the label of what

188
00:07:45,780 --> 00:07:51,179
they originally classified as and then

189
00:07:48,270 --> 00:07:53,729
trained on these along with the original

190
00:07:51,180 --> 00:07:56,550
training data and this is going to now

191
00:07:53,730 --> 00:07:59,190
give me a new classifier and then I'll

192
00:07:56,550 --> 00:08:00,840
repeat as many times as necessary keep

193
00:07:59,190 --> 00:08:02,430
on training on the episode examples of

194
00:08:00,840 --> 00:08:04,979
the old classifier until eventually is

195
00:08:02,430 --> 00:08:06,600
converges on something and the hope is

196
00:08:04,980 --> 00:08:11,370
this something that it converges on is

197
00:08:06,600 --> 00:08:13,290
robust ever so examples so that's one

198
00:08:11,370 --> 00:08:15,720
one thing you can imagine there's

199
00:08:13,290 --> 00:08:18,390
another second proposal from sometime

200
00:08:15,720 --> 00:08:21,720
last year called thermometer encoding

201
00:08:18,390 --> 00:08:24,500
and the claim here is that neural

202
00:08:21,720 --> 00:08:27,570
networks are in some sense overly linear

203
00:08:24,500 --> 00:08:29,040
that is that if you take the decision

204
00:08:27,570 --> 00:08:30,599
boundary you can fairly easily

205
00:08:29,040 --> 00:08:32,909
extrapolate in one direction what the

206
00:08:30,600 --> 00:08:35,630
classification is going to look like for

207
00:08:32,909 --> 00:08:38,339
a long direction and maybe this is that

208
00:08:35,630 --> 00:08:40,380
why you should think this there's some

209
00:08:38,340 --> 00:08:43,590
theory but this is the claim the paper

210
00:08:40,380 --> 00:08:46,260
makes and so the proposal is that I'll

211
00:08:43,590 --> 00:08:49,050
change the input representation so for

212
00:08:46,260 --> 00:08:52,050
images typically we represent the input

213
00:08:49,050 --> 00:08:55,949
as just a discreet pixel value from 0 to

214
00:08:52,050 --> 00:08:58,829
255 maybe it's hard for the neural

215
00:08:55,950 --> 00:09:01,890
network to learn that a change from 0 to

216
00:08:58,830 --> 00:09:05,620
1 is somehow different than from 250 to

217
00:09:01,890 --> 00:09:08,170
251 so let's make it easier

218
00:09:05,620 --> 00:09:09,400
and we will do here is I'm going to

219
00:09:08,170 --> 00:09:12,610
create a thermometer encoding function

220
00:09:09,400 --> 00:09:15,250
of the input and I'll say that

221
00:09:12,610 --> 00:09:16,810
thermometer encoding of 0.1 3 is a

222
00:09:15,250 --> 00:09:20,980
couple of ones followed by a bunch of

223
00:09:16,810 --> 00:09:24,369
zeros maybe of 0.66 is more ones

224
00:09:20,980 --> 00:09:27,220
followed by few zeros and 0.97 is almost

225
00:09:24,370 --> 00:09:28,630
all ones and now the hope is that the

226
00:09:27,220 --> 00:09:30,839
neural network might be able to

227
00:09:28,630 --> 00:09:32,950
disentangle the difference between

228
00:09:30,839 --> 00:09:34,420
taking a small step at the low end of

229
00:09:32,950 --> 00:09:38,100
the scale and taking a small step at the

230
00:09:34,420 --> 00:09:41,349
high end of the scale maybe this helps

231
00:09:38,100 --> 00:09:44,470
the final kind of defense that I'll tell

232
00:09:41,350 --> 00:09:47,110
you about is input transformations so

233
00:09:44,470 --> 00:09:50,860
the claim here is that perturbations are

234
00:09:47,110 --> 00:09:53,710
in some sense brittle and so what I can

235
00:09:50,860 --> 00:09:55,779
do is I can take my image I can

236
00:09:53,710 --> 00:09:58,300
pre-process it by some kind of random

237
00:09:55,779 --> 00:10:00,939
transformation men feed into the neural

238
00:09:58,300 --> 00:10:05,170
network and then maybe by doing this I

239
00:10:00,940 --> 00:10:06,820
can get a adversarial example become

240
00:10:05,170 --> 00:10:09,339
classified correctly because it's been

241
00:10:06,820 --> 00:10:10,839
modified in some way now you have to

242
00:10:09,339 --> 00:10:13,020
make sure you don't add too much noise

243
00:10:10,839 --> 00:10:15,130
that regular images are misclassified

244
00:10:13,020 --> 00:10:16,569
but you want to add enough noise so a

245
00:10:15,130 --> 00:10:21,220
visual examples are no longer classified

246
00:10:16,570 --> 00:10:23,380
correctly you could also imagine doing

247
00:10:21,220 --> 00:10:26,440
other things like JPEG compression or

248
00:10:23,380 --> 00:10:29,790
you could try adding noise and I'll get

249
00:10:26,440 --> 00:10:29,790
to some of these in a little bit later

250
00:10:30,720 --> 00:10:37,600
okay so those are defenses to episode

251
00:10:33,790 --> 00:10:38,770
examples and how they how they work now

252
00:10:37,600 --> 00:10:40,990
let me tell you what it means to

253
00:10:38,770 --> 00:10:45,100
evaluate the robustness and and how it

254
00:10:40,990 --> 00:10:46,720
as we go about doing this so I guess the

255
00:10:45,100 --> 00:10:49,150
basic question here is what does it mean

256
00:10:46,720 --> 00:10:50,440
to evaluate the robustness and what

257
00:10:49,150 --> 00:10:51,730
we're trying to do is we're trying to

258
00:10:50,440 --> 00:10:54,820
figure out how accurate our classifiers

259
00:10:51,730 --> 00:10:56,410
are in standard machine learning this is

260
00:10:54,820 --> 00:10:58,770
very easy you know you take your model

261
00:10:56,410 --> 00:11:01,600
you train the model on the training data

262
00:10:58,770 --> 00:11:04,630
you compute some kind of evaluation

263
00:11:01,600 --> 00:11:06,160
function to get the accuracy and then if

264
00:11:04,630 --> 00:11:06,820
it's good enough then you say your

265
00:11:06,160 --> 00:11:08,650
state-of-the-art

266
00:11:06,820 --> 00:11:09,970
and if it's not good enough then like

267
00:11:08,650 --> 00:11:12,790
you know tuned hyper parameters more

268
00:11:09,970 --> 00:11:14,200
until it's good enough and and this is a

269
00:11:12,790 --> 00:11:17,709
standard thing that is done in all of

270
00:11:14,200 --> 00:11:19,089
machine learning so the question is what

271
00:11:17,709 --> 00:11:21,819
changes on the

272
00:11:19,089 --> 00:11:23,050
visceral example evaluations and in

273
00:11:21,819 --> 00:11:26,459
particularly the only thing that changes

274
00:11:23,050 --> 00:11:29,649
is this line is the evaluation function

275
00:11:26,459 --> 00:11:31,119
so typically we have some kind of test

276
00:11:29,649 --> 00:11:33,519
set that we can use to evaluate a

277
00:11:31,120 --> 00:11:36,220
classifier on and as long as I don't

278
00:11:33,519 --> 00:11:37,600
train on my test set that I basically

279
00:11:36,220 --> 00:11:40,269
can't cheat in a standard machine

280
00:11:37,600 --> 00:11:41,769
learning evaluation I just need to make

281
00:11:40,269 --> 00:11:43,300
sure I train on training data and it's

282
00:11:41,769 --> 00:11:45,819
disjoint from test data I test on that

283
00:11:43,300 --> 00:11:49,300
and I'm good the problem is that no test

284
00:11:45,819 --> 00:11:51,309
data exists forever so examples because

285
00:11:49,300 --> 00:11:52,959
the episode examples are necessarily the

286
00:11:51,309 --> 00:11:55,629
episode examples on the new classifier

287
00:11:52,959 --> 00:11:58,949
that you happen to just train and so

288
00:11:55,629 --> 00:11:58,949
that doesn't yet exist

289
00:11:59,160 --> 00:12:05,050
so we're busting evaluations basically

290
00:12:02,259 --> 00:12:07,569
turn this function of just a value eight

291
00:12:05,050 --> 00:12:10,748
on the test data into something that

292
00:12:07,569 --> 00:12:13,269
does this where now we have to come up

293
00:12:10,749 --> 00:12:17,230
with some new attack function which is a

294
00:12:13,269 --> 00:12:19,959
function of the model and then use this

295
00:12:17,230 --> 00:12:21,759
to evaluate now there are other ways you

296
00:12:19,959 --> 00:12:22,869
could imagine evaluating but in practice

297
00:12:21,759 --> 00:12:24,519
this is what this is what people do you

298
00:12:22,870 --> 00:12:26,829
take the same test date of the year you

299
00:12:24,519 --> 00:12:28,839
would otherwise test on you come up with

300
00:12:26,829 --> 00:12:31,979
an attack and you see how badly you can

301
00:12:28,839 --> 00:12:33,850
make the classifier do on that test data

302
00:12:31,980 --> 00:12:35,860
now there's a couple things I'm not

303
00:12:33,850 --> 00:12:38,259
showing here for example I do have to

304
00:12:35,860 --> 00:12:40,720
constrain that the adversary can't just

305
00:12:38,259 --> 00:12:43,749
arbitrarily modify the inputs to make

306
00:12:40,720 --> 00:12:45,240
them like all black because you know

307
00:12:43,749 --> 00:12:47,649
then the accuracy would always be zero

308
00:12:45,240 --> 00:12:50,559
so you'll have some threat model that

309
00:12:47,649 --> 00:12:53,499
says maybe you can only modify five

310
00:12:50,559 --> 00:12:56,469
pixels or maybe you can modify the low

311
00:12:53,499 --> 00:12:57,850
three bits of every pixel but once

312
00:12:56,470 --> 00:12:59,679
you've done that and you can put some

313
00:12:57,850 --> 00:13:00,970
threat model down then this is exactly

314
00:12:59,679 --> 00:13:03,069
what the attacks are doing you kind of

315
00:13:00,970 --> 00:13:05,379
try and modify all of the images in the

316
00:13:03,069 --> 00:13:11,349
test set to make them so that the

317
00:13:05,379 --> 00:13:13,509
classifier is now fooled okay so the

318
00:13:11,350 --> 00:13:16,600
question then is how complete are

319
00:13:13,509 --> 00:13:19,870
evaluations that are written in today's

320
00:13:16,600 --> 00:13:22,749
papers and for that let me give you a

321
00:13:19,870 --> 00:13:25,269
case study of the defense's that were

322
00:13:22,749 --> 00:13:27,399
accepted at I clear twenty eighteen I

323
00:13:25,269 --> 00:13:29,769
clear as one of the three main

324
00:13:27,399 --> 00:13:34,240
conferences on machine learning this

325
00:13:29,769 --> 00:13:36,069
happened sometime last year so at that

326
00:13:34,240 --> 00:13:37,569
conference they were a bunch of papers

327
00:13:36,069 --> 00:13:40,719
were defenses to adverse or examples

328
00:13:37,569 --> 00:13:42,279
accepted and I should say that to begin

329
00:13:40,720 --> 00:13:43,899
with the authors of these papers did

330
00:13:42,279 --> 00:13:46,179
make a serious effort to evaluate them

331
00:13:43,899 --> 00:13:47,709
I've shown the papers here and I've

332
00:13:46,179 --> 00:13:50,740
highlighted in red the regions that are

333
00:13:47,709 --> 00:13:52,359
at the evaluation so by space most of

334
00:13:50,740 --> 00:13:55,389
these papers are roughly one-half

335
00:13:52,360 --> 00:13:57,850
evaluation the authors of these papers

336
00:13:55,389 --> 00:14:00,730
are from places like Google and Facebook

337
00:13:57,850 --> 00:14:03,370
and Amazon they're from universities

338
00:14:00,730 --> 00:14:06,160
like Stanford and Berkeley and Carnegie

339
00:14:03,370 --> 00:14:08,589
Mellon and UT Austin and Princeton so by

340
00:14:06,160 --> 00:14:10,089
all accounts these are highly regarded

341
00:14:08,589 --> 00:14:17,160
researchers they know what they're doing

342
00:14:10,089 --> 00:14:19,449
so we we've elevated these defenses and

343
00:14:17,160 --> 00:14:21,579
we placed four of them out of scope

344
00:14:19,449 --> 00:14:23,319
these were defenses that had threat

345
00:14:21,579 --> 00:14:27,099
models that were incompatible with ours

346
00:14:23,319 --> 00:14:28,509
or that had some certified claim that

347
00:14:27,100 --> 00:14:32,319
they could prove that it was robust

348
00:14:28,509 --> 00:14:34,600
under some specific distortion model I'm

349
00:14:32,319 --> 00:14:36,189
not in the business of doing proofs and

350
00:14:34,600 --> 00:14:41,410
so we just said let's put those out of

351
00:14:36,189 --> 00:14:42,910
scope two of them were correct I'll come

352
00:14:41,410 --> 00:14:45,910
back to what I mean exactly by correct

353
00:14:42,910 --> 00:14:49,809
in a second and the other seven were

354
00:14:45,910 --> 00:14:51,639
broken so over half of the defenses that

355
00:14:49,809 --> 00:14:53,649
were accepted at this conference were

356
00:14:51,639 --> 00:14:54,339
broken in and six of these offered zero

357
00:14:53,649 --> 00:14:57,579
robustness

358
00:14:54,339 --> 00:14:58,689
they did absolutely nothing one of them

359
00:14:57,579 --> 00:15:01,540
helped a little bit

360
00:14:58,689 --> 00:15:03,639
and by correct for the other two what I

361
00:15:01,540 --> 00:15:06,219
mean to say is that the claim that was

362
00:15:03,639 --> 00:15:08,769
written in the paper was correct and we

363
00:15:06,220 --> 00:15:10,360
were not able to invalidate it so I'm

364
00:15:08,769 --> 00:15:12,819
not gonna say that it's necessarily

365
00:15:10,360 --> 00:15:16,509
going to definitely be right but I'm not

366
00:15:12,819 --> 00:15:18,849
confident saying it's wrong and this is

367
00:15:16,509 --> 00:15:20,980
in some sense concerning that this they

368
00:15:18,850 --> 00:15:23,170
were broken in this way and not only

369
00:15:20,980 --> 00:15:25,509
were they broken in this way when when

370
00:15:23,170 --> 00:15:27,998
we broke them it only took us maybe a

371
00:15:25,509 --> 00:15:31,149
month or something to do this for all of

372
00:15:27,999 --> 00:15:35,459
these defenses and so the question is

373
00:15:31,149 --> 00:15:37,569
what was going on why is it that these

374
00:15:35,459 --> 00:15:39,250
defenses were accepted at one of the top

375
00:15:37,569 --> 00:15:43,660
conferences and machine learning and all

376
00:15:39,250 --> 00:15:44,860
over half of them were easily broken so

377
00:15:43,660 --> 00:15:48,040
I showed you this this picture

378
00:15:44,860 --> 00:15:50,020
earlier this is the lost surface of a

379
00:15:48,040 --> 00:15:52,630
neural network where on one direction

380
00:15:50,020 --> 00:15:55,900
I'm going adversarially and in the other

381
00:15:52,630 --> 00:16:00,160
direction I'm moving in some random

382
00:15:55,900 --> 00:16:02,980
direction so it turns out that what

383
00:16:00,160 --> 00:16:04,300
happened is that they turned this loss

384
00:16:02,980 --> 00:16:08,320
function to something that looks like

385
00:16:04,300 --> 00:16:09,640
this in case it's a little hard to see

386
00:16:08,320 --> 00:16:12,820
what's going on let me zoom in on this

387
00:16:09,640 --> 00:16:17,050
one region here and show you that the

388
00:16:12,820 --> 00:16:18,940
function looks like this so imagine that

389
00:16:17,050 --> 00:16:22,780
you're a gradient descent optimizer and

390
00:16:18,940 --> 00:16:24,160
I say you're there which direction would

391
00:16:22,780 --> 00:16:27,880
you like to travel to find the

392
00:16:24,160 --> 00:16:29,740
adversarial example you basically have

393
00:16:27,880 --> 00:16:31,420
no idea what to do you'll pick a random

394
00:16:29,740 --> 00:16:32,650
direction maybe it turns out that you're

395
00:16:31,420 --> 00:16:35,439
on the side of a cliff facing this way

396
00:16:32,650 --> 00:16:36,640
so you step that way and then it turns

397
00:16:35,440 --> 00:16:38,020
out that now you on the side of a cliff

398
00:16:36,640 --> 00:16:41,380
facing the other direction and you step

399
00:16:38,020 --> 00:16:44,260
back and you run for a hundred a

400
00:16:41,380 --> 00:16:46,300
thousand iterations just randomly moving

401
00:16:44,260 --> 00:16:48,010
around this space if you're lucky you

402
00:16:46,300 --> 00:16:49,810
happened across the decision boundary if

403
00:16:48,010 --> 00:16:52,960
you're unlucky you just don't get

404
00:16:49,810 --> 00:16:55,869
anywhere useful and so this is

405
00:16:52,960 --> 00:16:57,310
essentially what happened almost all of

406
00:16:55,870 --> 00:16:59,170
the defenses that we looked at did

407
00:16:57,310 --> 00:17:02,280
something that looks like this where

408
00:16:59,170 --> 00:17:04,750
they made the gradients insufficient to

409
00:17:02,280 --> 00:17:06,280
provide insufficient information to

410
00:17:04,750 --> 00:17:09,099
actually know in which direction to

411
00:17:06,280 --> 00:17:11,700
travel to find an adversity example and

412
00:17:09,099 --> 00:17:13,959
so you may say you know well that's fine

413
00:17:11,700 --> 00:17:17,740
you can't find the never so example

414
00:17:13,960 --> 00:17:19,930
therefore it's robust but remember that

415
00:17:17,740 --> 00:17:26,470
what the loss function actually looks

416
00:17:19,930 --> 00:17:28,150
like is this and so while there may not

417
00:17:26,470 --> 00:17:32,110
be an easy way to find it when you look

418
00:17:28,150 --> 00:17:34,120
locally when you look globally in some

419
00:17:32,110 --> 00:17:38,169
sense it's very easy this function looks

420
00:17:34,120 --> 00:17:40,330
nearly identical to this one and so if I

421
00:17:38,170 --> 00:17:41,560
somehow knew ahead of time what the big

422
00:17:40,330 --> 00:17:43,270
picture looked like I could identify

423
00:17:41,560 --> 00:17:45,909
which direction to travel and to find

424
00:17:43,270 --> 00:17:47,980
the a vasila example it's just the fact

425
00:17:45,910 --> 00:17:50,050
that with this very very local view that

426
00:17:47,980 --> 00:17:56,890
the standard optimizers have they can't

427
00:17:50,050 --> 00:17:58,680
find episodes now one brief aside this

428
00:17:56,890 --> 00:18:00,510
isn't the first time that it's hot

429
00:17:58,680 --> 00:18:02,370
and that a bunch of Defense's are

430
00:18:00,510 --> 00:18:05,820
proposed and very quickly broken in the

431
00:18:02,370 --> 00:18:08,820
space of adverse or examples so this has

432
00:18:05,820 --> 00:18:11,850
happened relatively regularly over the

433
00:18:08,820 --> 00:18:15,530
last couple months where people propose

434
00:18:11,850 --> 00:18:18,060
defenses and very quickly they're broken

435
00:18:15,530 --> 00:18:19,620
more often than not if a defense is

436
00:18:18,060 --> 00:18:21,600
proposed and someone else evaluates it

437
00:18:19,620 --> 00:18:24,899
it's shown to be wrong within a couple

438
00:18:21,600 --> 00:18:27,240
months now I should mention that this is

439
00:18:24,900 --> 00:18:28,710
not all of them but almost all of the

440
00:18:27,240 --> 00:18:31,290
evaluations that are done in adversarial

441
00:18:28,710 --> 00:18:33,270
machine learning on Defense's and so in

442
00:18:31,290 --> 00:18:36,389
security we have this interesting setup

443
00:18:33,270 --> 00:18:39,660
where one or two people write a defense

444
00:18:36,390 --> 00:18:42,930
paper and then everyone else wants to

445
00:18:39,660 --> 00:18:43,890
write the attacks for some reason in

446
00:18:42,930 --> 00:18:48,000
machine learning it's the other way

447
00:18:43,890 --> 00:18:50,190
around there are like 20 maybe attack

448
00:18:48,000 --> 00:18:53,610
papers that try and evaluate defenses

449
00:18:50,190 --> 00:18:56,580
and there are something like 3 or 400

450
00:18:53,610 --> 00:18:58,879
defense papers and this is in some sense

451
00:18:56,580 --> 00:19:02,100
a concerning set up to be in because

452
00:18:58,880 --> 00:19:06,840
we've only evaluated maybe 50 defenses

453
00:19:02,100 --> 00:19:10,469
ever and the other 450 maybe they do

454
00:19:06,840 --> 00:19:17,760
something maybe they don't no one knows

455
00:19:10,470 --> 00:19:20,160
and so that's its own problem ok so what

456
00:19:17,760 --> 00:19:25,020
are the lessons that we've actually

457
00:19:20,160 --> 00:19:28,980
learned then from these evaluations so

458
00:19:25,020 --> 00:19:33,330
that I think comes in a couple pieces so

459
00:19:28,980 --> 00:19:34,950
let me start with the first the first

460
00:19:33,330 --> 00:19:40,439
kind of lessons is what types of

461
00:19:34,950 --> 00:19:42,230
defenses are effective so the first kind

462
00:19:40,440 --> 00:19:44,460
of defense that's effective is

463
00:19:42,230 --> 00:19:46,110
adversarial training this is the first

464
00:19:44,460 --> 00:19:47,040
one that I showed you earlier this is

465
00:19:46,110 --> 00:19:49,050
one of the ones that we tried to

466
00:19:47,040 --> 00:19:53,970
evaluate in and we weren't able to show

467
00:19:49,050 --> 00:19:56,129
that it was not robust whatever skill

468
00:19:53,970 --> 00:19:58,200
training does so this is the law surface

469
00:19:56,130 --> 00:20:00,180
of a standard neural network what

470
00:19:58,200 --> 00:20:02,070
episode training does is it makes the

471
00:20:00,180 --> 00:20:04,620
law surface instead of look something

472
00:20:02,070 --> 00:20:07,770
very ugly like this it makes it look

473
00:20:04,620 --> 00:20:09,689
very nice like this so it really smooths

474
00:20:07,770 --> 00:20:12,090
out the decision boundary notice it

475
00:20:09,690 --> 00:20:12,720
pushed the decision boundary out so it's

476
00:20:12,090 --> 00:20:15,360
not full

477
00:20:12,720 --> 00:20:17,130
bust it's not like we solved the problem

478
00:20:15,360 --> 00:20:19,979
entirely but I've made it much harder to

479
00:20:17,130 --> 00:20:22,620
find episode examples and this also

480
00:20:19,980 --> 00:20:24,480
gives me confidence that I'm not just

481
00:20:22,620 --> 00:20:26,668
being fooled artificially because it's

482
00:20:24,480 --> 00:20:28,169
hard to find them in some sense it looks

483
00:20:26,669 --> 00:20:30,330
like it's easier to find adversely

484
00:20:28,169 --> 00:20:32,700
examples in this setting than it was on

485
00:20:30,330 --> 00:20:35,428
the standard of classifier but in my

486
00:20:32,700 --> 00:20:37,830
easier here I mean I know that I've done

487
00:20:35,429 --> 00:20:39,720
the right thing because the law surface

488
00:20:37,830 --> 00:20:42,240
is pointing all the same direction the

489
00:20:39,720 --> 00:20:44,299
entire way so probably it's not the case

490
00:20:42,240 --> 00:20:46,850
that there's not going to be these weird

491
00:20:44,299 --> 00:20:50,400
wrinkles in the law surface like here

492
00:20:46,850 --> 00:20:52,289
now I can't guarantee that but that

493
00:20:50,400 --> 00:20:53,370
heuristic ly seems like it's the case

494
00:20:52,289 --> 00:20:55,230
and for everyone who's tried to

495
00:20:53,370 --> 00:20:56,090
reevaluate these they found similar

496
00:20:55,230 --> 00:20:58,500
results

497
00:20:56,090 --> 00:21:01,230
so I personal training seems like one

498
00:20:58,500 --> 00:21:03,480
good thing I should mention it's maybe a

499
00:21:01,230 --> 00:21:05,940
little bit unsatisfying that the

500
00:21:03,480 --> 00:21:08,280
solution to adversarial examples for

501
00:21:05,940 --> 00:21:11,309
this defense is just keep training on

502
00:21:08,280 --> 00:21:14,070
them until you do well you may have

503
00:21:11,309 --> 00:21:16,110
liked a solution and I would have liked

504
00:21:14,070 --> 00:21:18,320
the solution that said here is a system

505
00:21:16,110 --> 00:21:21,780
that's better at machine learning

506
00:21:18,320 --> 00:21:26,700
because it better captures what humans

507
00:21:21,780 --> 00:21:29,220
do or something else and then section 13

508
00:21:26,700 --> 00:21:32,100
of their paper shows and also it solves

509
00:21:29,220 --> 00:21:35,460
adversely examples I view episode

510
00:21:32,100 --> 00:21:37,918
examples maybe as a bug that should be

511
00:21:35,460 --> 00:21:40,260
fixed not by just addressing that by by

512
00:21:37,919 --> 00:21:42,630
addressing the root cause and if the

513
00:21:40,260 --> 00:21:44,549
root cause is that they don't adapt to

514
00:21:42,630 --> 00:21:47,400
new domains these machine learning

515
00:21:44,549 --> 00:21:50,460
classifiers then this is kind of

516
00:21:47,400 --> 00:21:52,289
cheating but it does seem like it's

517
00:21:50,460 --> 00:21:53,970
correct so if you know ahead of time

518
00:21:52,289 --> 00:21:56,429
what kinds of attacks you're going to

519
00:21:53,970 --> 00:21:58,049
see in this machine learning space you

520
00:21:56,429 --> 00:22:01,799
can just train on those attacks and

521
00:21:58,049 --> 00:22:03,000
become relatively robust to them now if

522
00:22:01,799 --> 00:22:06,000
the adversary changes the threat model

523
00:22:03,000 --> 00:22:08,070
and now instead of distorting the low

524
00:22:06,000 --> 00:22:11,640
three bits of every image of that every

525
00:22:08,070 --> 00:22:14,399
pixel of the image now flips five pixels

526
00:22:11,640 --> 00:22:16,289
the classifier is going to break so you

527
00:22:14,400 --> 00:22:17,370
have to train on every new threat model

528
00:22:16,289 --> 00:22:21,000
that you're going to ever see and

529
00:22:17,370 --> 00:22:25,949
enumerate these seems hard but this is

530
00:22:21,000 --> 00:22:26,669
one of the effective defenses okay so

531
00:22:25,950 --> 00:22:30,659
the second class

532
00:22:26,669 --> 00:22:32,600
of effective defenses is the empty set

533
00:22:30,659 --> 00:22:35,100
the only defenses we have are the first

534
00:22:32,600 --> 00:22:37,259
everyone who's tried to propose another

535
00:22:35,100 --> 00:22:39,509
defense basically that isn't adversarial

536
00:22:37,259 --> 00:22:42,690
training so far when anyone else has

537
00:22:39,509 --> 00:22:45,389
evaluated it hasn't worked people have

538
00:22:42,690 --> 00:22:47,100
tried lots and lots of other things the

539
00:22:45,389 --> 00:22:49,529
ones which haven't been evaluated don't

540
00:22:47,100 --> 00:22:51,299
work and as I mentioned there are

541
00:22:49,529 --> 00:22:53,730
another 400 that people have proposed

542
00:22:51,299 --> 00:22:55,619
that no one else is evaluated and so we

543
00:22:53,730 --> 00:22:58,499
don't know if they work or not and this

544
00:22:55,619 --> 00:23:00,209
is an unfortunate situation to be in

545
00:22:58,499 --> 00:23:01,379
fortunately there are a bunch of you in

546
00:23:00,210 --> 00:23:03,869
the audience who are all security people

547
00:23:01,379 --> 00:23:10,709
who know how to evaluate things so maybe

548
00:23:03,869 --> 00:23:14,840
you can help remedy this okay so the

549
00:23:10,710 --> 00:23:14,840
second lesson from evaluating robustness

550
00:23:15,470 --> 00:23:26,190
so okay I told you earlier that making

551
00:23:22,440 --> 00:23:27,960
the loss of as ugly doesn't work and I

552
00:23:26,190 --> 00:23:30,090
sort of appealed to the fact that if you

553
00:23:27,960 --> 00:23:31,559
looked at the pictures then that it's

554
00:23:30,090 --> 00:23:33,689
not very different and it looks like

555
00:23:31,559 --> 00:23:36,059
there's an attack without actually

556
00:23:33,690 --> 00:23:39,690
giving you what that attack was so let

557
00:23:36,059 --> 00:23:44,389
me tell you what you can do so the first

558
00:23:39,690 --> 00:23:47,309
thing that you could do to attack this

559
00:23:44,389 --> 00:23:49,469
would be to like think really hard and

560
00:23:47,309 --> 00:23:52,590
then write a differentiable

561
00:23:49,470 --> 00:23:54,239
implementation of jpg and then back

562
00:23:52,590 --> 00:23:57,259
propagate through jpg to compute

563
00:23:54,239 --> 00:23:59,309
gradients and then everything works

564
00:23:57,259 --> 00:24:01,200
that's a lot of work though because

565
00:23:59,309 --> 00:24:04,259
there are lots of think functions that

566
00:24:01,200 --> 00:24:05,489
are easy to write that have nice forward

567
00:24:04,259 --> 00:24:07,769
passes that don't have differentiable

568
00:24:05,489 --> 00:24:09,539
back passwords and this is how we

569
00:24:07,769 --> 00:24:11,549
compute these episode examples if you

570
00:24:09,539 --> 00:24:15,419
remember is we take gradients through

571
00:24:11,549 --> 00:24:17,190
these neural networks so that's one

572
00:24:15,419 --> 00:24:19,049
thing you can do and this is a very nice

573
00:24:17,190 --> 00:24:21,899
solution if you're willing to think very

574
00:24:19,049 --> 00:24:24,450
hard about each of them there's another

575
00:24:21,899 --> 00:24:26,268
thing you can do in we wrote we have a

576
00:24:24,450 --> 00:24:29,399
paper that shows that you can do this is

577
00:24:26,269 --> 00:24:32,190
suppose that you have a very special

578
00:24:29,399 --> 00:24:33,359
special type of neural network which it

579
00:24:32,190 --> 00:24:37,200
turns out many things fall into this

580
00:24:33,359 --> 00:24:39,509
category of so suppose that I'm taking

581
00:24:37,200 --> 00:24:40,010
my image and I feed this forward through

582
00:24:39,509 --> 00:24:42,590
a neural net

583
00:24:40,010 --> 00:24:45,230
work that has some kind of layers that

584
00:24:42,590 --> 00:24:46,639
are hard to differentiate through so

585
00:24:45,230 --> 00:24:48,950
most of it is a standard neural network

586
00:24:46,640 --> 00:24:52,010
but it turns out that you can't compute

587
00:24:48,950 --> 00:24:54,680
gradients through one or two small

588
00:24:52,010 --> 00:24:55,910
number of layers so what we'll do is

589
00:24:54,680 --> 00:24:57,950
we'll do the forward pass in the

590
00:24:55,910 --> 00:24:59,660
standard setting I'll do the forward

591
00:24:57,950 --> 00:25:01,670
pass and I'll get some probability

592
00:24:59,660 --> 00:25:05,390
vector of what the inputs going to be

593
00:25:01,670 --> 00:25:07,910
classified as and now on the backward

594
00:25:05,390 --> 00:25:10,310
pass instead of taking the hard to

595
00:25:07,910 --> 00:25:12,200
differentiate through function I swap it

596
00:25:10,310 --> 00:25:14,179
out with an easy to differentiate

597
00:25:12,200 --> 00:25:16,550
through a function which is not exactly

598
00:25:14,180 --> 00:25:21,290
computing the right thing but as a

599
00:25:16,550 --> 00:25:23,570
closed approximation so for example JPEG

600
00:25:21,290 --> 00:25:26,629
compression roughly satisfies the

601
00:25:23,570 --> 00:25:27,860
equation f of X equals x this is the

602
00:25:26,630 --> 00:25:31,040
idea of JPEG compression it shouldn't

603
00:25:27,860 --> 00:25:33,500
change the image very much the gradient

604
00:25:31,040 --> 00:25:35,000
of f of X equals x is 1 so I'll just

605
00:25:33,500 --> 00:25:39,140
drop the JPEG compression pretend it

606
00:25:35,000 --> 00:25:41,000
didn't exist now this is not actually

607
00:25:39,140 --> 00:25:43,690
the gradient of the function it's a lie

608
00:25:41,000 --> 00:25:46,220
but it turns out that it's a useful lie

609
00:25:43,690 --> 00:25:48,800
the gradients of this are relatively

610
00:25:46,220 --> 00:25:50,210
smooth and it means that it points you

611
00:25:48,800 --> 00:25:52,850
in a helpful direction relatively

612
00:25:50,210 --> 00:25:55,220
continuously it doesn't actually always

613
00:25:52,850 --> 00:25:57,590
give you the right answer but it gives

614
00:25:55,220 --> 00:26:00,280
you some kind of gradient signal so that

615
00:25:57,590 --> 00:26:03,560
if you repeat this many many times

616
00:26:00,280 --> 00:26:04,879
continuously finding a single step of a

617
00:26:03,560 --> 00:26:07,280
gradient and taking a small step in that

618
00:26:04,880 --> 00:26:09,590
direction and repeating with this lie of

619
00:26:07,280 --> 00:26:11,540
a gradient then it turns out that's good

620
00:26:09,590 --> 00:26:14,510
enough and you can actually find out

621
00:26:11,540 --> 00:26:16,190
virtual examples this way and so this is

622
00:26:14,510 --> 00:26:18,830
a one general way that you can find

623
00:26:16,190 --> 00:26:21,110
adverse early examples there are others

624
00:26:18,830 --> 00:26:22,220
and I would encourage you if you're

625
00:26:21,110 --> 00:26:25,790
interested to go read the literature

626
00:26:22,220 --> 00:26:27,110
it's very interesting but this is one of

627
00:26:25,790 --> 00:26:28,820
the standard ways now that you can

628
00:26:27,110 --> 00:26:30,020
differentiate through things that that

629
00:26:28,820 --> 00:26:37,189
break gradients in this one particular

630
00:26:30,020 --> 00:26:39,290
way okay so the third kind of lesson

631
00:26:37,190 --> 00:26:44,330
from evaluating the performing these

632
00:26:39,290 --> 00:26:46,399
evaluations is this so I should mention

633
00:26:44,330 --> 00:26:49,580
this was all going to be from this paper

634
00:26:46,400 --> 00:26:51,770
that we wrote on called on evaluate

635
00:26:49,580 --> 00:26:53,250
adverse over bus'ness about six months

636
00:26:51,770 --> 00:26:55,600
ago now that

637
00:26:53,250 --> 00:27:00,060
basically a set of advice on how you

638
00:26:55,600 --> 00:27:02,169
should perform adversely evaluations so

639
00:27:00,060 --> 00:27:03,730
what I'm going to do is I'm going to

640
00:27:02,170 --> 00:27:05,920
cover a bunch of things that you

641
00:27:03,730 --> 00:27:09,430
shouldn't and should do when performing

642
00:27:05,920 --> 00:27:11,050
episode evaluations for those of you who

643
00:27:09,430 --> 00:27:13,750
aren't performing those evaluations

644
00:27:11,050 --> 00:27:15,700
right now maybe this part isn't as

645
00:27:13,750 --> 00:27:17,740
important for you but again I would

646
00:27:15,700 --> 00:27:19,300
encourage you to just see what it is

647
00:27:17,740 --> 00:27:20,700
that's happening in the space and the

648
00:27:19,300 --> 00:27:23,050
kinds of errors that are being made

649
00:27:20,700 --> 00:27:25,870
partly because many of the areas that

650
00:27:23,050 --> 00:27:27,850
are being made are very basic and I

651
00:27:25,870 --> 00:27:29,860
would like more security people to be

652
00:27:27,850 --> 00:27:31,270
working in this space because I feel

653
00:27:29,860 --> 00:27:34,719
like many of us would not make these

654
00:27:31,270 --> 00:27:36,670
same kinds of errors the other thing I'm

655
00:27:34,720 --> 00:27:39,460
gonna say is that I want to give some

656
00:27:36,670 --> 00:27:41,050
actionable advice here and that requires

657
00:27:39,460 --> 00:27:43,090
me being specific so I'm going to give

658
00:27:41,050 --> 00:27:45,100
screenshots from papers that do the

659
00:27:43,090 --> 00:27:46,750
wrong thing just know that this is

660
00:27:45,100 --> 00:27:48,939
standard and I'm not bashing those

661
00:27:46,750 --> 00:27:50,320
individual papers I'm sort of showing

662
00:27:48,940 --> 00:27:54,880
the types of things that have gone wrong

663
00:27:50,320 --> 00:27:56,679
in the past okay so the first thing that

664
00:27:54,880 --> 00:27:58,420
I'm gonna say is that in order to do a

665
00:27:56,680 --> 00:28:01,120
security evaluation you actually need to

666
00:27:58,420 --> 00:28:03,160
do a security evaluation you have to

667
00:28:01,120 --> 00:28:06,340
take the defense and try and beat it

668
00:28:03,160 --> 00:28:08,200
it's not sufficient to go in generate a

669
00:28:06,340 --> 00:28:10,570
virtual examples on some model save them

670
00:28:08,200 --> 00:28:12,880
to disk and then say okay I'm gonna now

671
00:28:10,570 --> 00:28:15,070
train some new model on some new data

672
00:28:12,880 --> 00:28:16,570
with my defense and then test them on

673
00:28:15,070 --> 00:28:20,620
the same data so examples that broke the

674
00:28:16,570 --> 00:28:23,620
other thing this doesn't work I mean you

675
00:28:20,620 --> 00:28:25,330
all know this it's not fine if I live up

676
00:28:23,620 --> 00:28:27,010
to new crypto algorithm and said well

677
00:28:25,330 --> 00:28:30,909
let me take the differential that breaks

678
00:28:27,010 --> 00:28:33,580
md5 and I'll apply that to mine and it

679
00:28:30,910 --> 00:28:35,290
fails therefore my cypher is secure and

680
00:28:33,580 --> 00:28:37,600
this would be ridiculous

681
00:28:35,290 --> 00:28:38,770
but this is what's done and maybe half

682
00:28:37,600 --> 00:28:41,860
of all I've so example defence

683
00:28:38,770 --> 00:28:44,020
evaluations is people don't actually try

684
00:28:41,860 --> 00:28:47,560
and evaluate on it in an attack their

685
00:28:44,020 --> 00:28:49,120
defense so this is the first thing that

686
00:28:47,560 --> 00:28:52,179
that you shouldn't do when performing an

687
00:28:49,120 --> 00:28:54,340
evaluation the second thing is that

688
00:28:52,180 --> 00:28:54,940
there are some papers that go with are a

689
00:28:54,340 --> 00:28:56,439
little better

690
00:28:54,940 --> 00:28:58,450
they have like a section three point one

691
00:28:56,440 --> 00:29:00,430
called effectiveness which is a page and

692
00:28:58,450 --> 00:29:02,290
a half by the way machine learning

693
00:29:00,430 --> 00:29:03,520
papers are eight pages long so a page

694
00:29:02,290 --> 00:29:05,950
and a half is actually a relatively

695
00:29:03,520 --> 00:29:08,529
large fraction of the paper

696
00:29:05,950 --> 00:29:10,630
and so the way so they have this effect

697
00:29:08,529 --> 00:29:13,059
in this section and and then they have

698
00:29:10,630 --> 00:29:14,730
another section three point four that's

699
00:29:13,059 --> 00:29:18,820
robustness to the white boxes hacker

700
00:29:14,730 --> 00:29:22,029
which is a paragraph long in which case

701
00:29:18,820 --> 00:29:23,889
what was this section doing this section

702
00:29:22,029 --> 00:29:25,440
was showing that if you take someone who

703
00:29:23,889 --> 00:29:29,740
doesn't try very hard then its robust

704
00:29:25,440 --> 00:29:31,630
and to which I would say you know take

705
00:29:29,740 --> 00:29:33,159
this put it in appendix good to know

706
00:29:31,630 --> 00:29:34,779
that someone doesn't try very hard can't

707
00:29:33,159 --> 00:29:36,850
break it and make this the remainder of

708
00:29:34,779 --> 00:29:38,289
the paper so this is something that's

709
00:29:36,850 --> 00:29:40,330
fairly typical as you'll see a very very

710
00:29:38,289 --> 00:29:41,740
long section on it stops all of these

711
00:29:40,330 --> 00:29:43,090
attacks when you don't actually try and

712
00:29:41,740 --> 00:29:48,700
then we'll try a little bit for one

713
00:29:43,090 --> 00:29:50,230
paragraph okay in machine learning this

714
00:29:48,700 --> 00:29:52,960
is this thing called a held out data set

715
00:29:50,230 --> 00:29:54,669
this is where you'd like take some data

716
00:29:52,960 --> 00:29:55,240
and don't train on don't test on until

717
00:29:54,669 --> 00:29:58,779
the very end

718
00:29:55,240 --> 00:30:00,220
you can't hold out attacks that's just a

719
00:29:58,779 --> 00:30:02,260
concept which doesn't exist

720
00:30:00,220 --> 00:30:04,510
you need to come up with new attacks you

721
00:30:02,260 --> 00:30:05,679
can't just like take some attack pretend

722
00:30:04,510 --> 00:30:07,179
we didn't know about it before designer

723
00:30:05,679 --> 00:30:08,740
defense and then run that attack in

724
00:30:07,179 --> 00:30:13,990
particular against it that's not an

725
00:30:08,740 --> 00:30:17,289
adaptive attack okay this is attack

726
00:30:13,990 --> 00:30:20,350
that's called F GSM what this does is it

727
00:30:17,289 --> 00:30:21,850
takes one step of gradient descent it

728
00:30:20,350 --> 00:30:24,039
was designed to show that neural

729
00:30:21,850 --> 00:30:26,830
networks are linear it was not designed

730
00:30:24,039 --> 00:30:28,240
to evaluate defenses but because it's

731
00:30:26,830 --> 00:30:30,000
only one step of gradient descent it's

732
00:30:28,240 --> 00:30:34,320
very fast and very easy to write and

733
00:30:30,000 --> 00:30:37,090
everyone uses it for evaluations don't

734
00:30:34,320 --> 00:30:39,939
if you want to run against it that's

735
00:30:37,090 --> 00:30:42,189
fine but it would be nice if you also

736
00:30:39,940 --> 00:30:44,769
ran against stronger things that tried

737
00:30:42,190 --> 00:30:46,659
harder than taking one gradient pass

738
00:30:44,769 --> 00:30:49,149
through our neural network and so that

739
00:30:46,659 --> 00:30:53,769
you can evaluate in twenty milliseconds

740
00:30:49,149 --> 00:30:55,779
instead of half a second in particular

741
00:30:53,769 --> 00:30:57,730
when performing gradient descent it's

742
00:30:55,779 --> 00:30:59,139
necessary in general to perform one

743
00:30:57,730 --> 00:31:01,240
hundred or a thousand iterations of

744
00:30:59,139 --> 00:31:05,769
gradient descent to do these evaluations

745
00:31:01,240 --> 00:31:07,419
properly you can get by with doing fewer

746
00:31:05,769 --> 00:31:09,610
than 100 but you have to think really

747
00:31:07,419 --> 00:31:11,019
hard you have to make sure that every

748
00:31:09,610 --> 00:31:12,309
gradient step is actually doing

749
00:31:11,019 --> 00:31:15,340
something useful to give you an adverse

750
00:31:12,309 --> 00:31:18,820
for example and this takes a lot of work

751
00:31:15,340 --> 00:31:19,520
it's much easier to think less and make

752
00:31:18,820 --> 00:31:21,649
the computer

753
00:31:19,520 --> 00:31:23,150
more work and just make it perform many

754
00:31:21,650 --> 00:31:26,270
many many iterations gradient descent

755
00:31:23,150 --> 00:31:29,120
and give you something which is mostly

756
00:31:26,270 --> 00:31:35,389
going to correctly evaluate if you're

757
00:31:29,120 --> 00:31:38,050
robust or not you should check that when

758
00:31:35,390 --> 00:31:42,500
you do more work you actually get better

759
00:31:38,050 --> 00:31:44,419
if you try a very weak attack in this

760
00:31:42,500 --> 00:31:48,290
case FG sm one iteration of a gradient

761
00:31:44,420 --> 00:31:51,050
descent and it is doing better so the

762
00:31:48,290 --> 00:31:53,360
accuracy under this attack is lower than

763
00:31:51,050 --> 00:31:55,940
the accuracy when you try many many many

764
00:31:53,360 --> 00:31:57,550
iterations of gradient descent probably

765
00:31:55,940 --> 00:32:00,170
it's because you've done something wrong

766
00:31:57,550 --> 00:32:00,800
if you're trying harder and succeeding

767
00:32:00,170 --> 00:32:02,330
less often

768
00:32:00,800 --> 00:32:05,840
then you're optimizing for the wrong

769
00:32:02,330 --> 00:32:08,870
thing probably and so this is one of the

770
00:32:05,840 --> 00:32:11,090
other things to check for and the final

771
00:32:08,870 --> 00:32:13,250
thing to check for I think is that you

772
00:32:11,090 --> 00:32:16,370
need to make sure that if you run an

773
00:32:13,250 --> 00:32:18,710
attack that has no bound on the

774
00:32:16,370 --> 00:32:22,070
distortion then eventually it should

775
00:32:18,710 --> 00:32:24,200
succeed so why should why is this the

776
00:32:22,070 --> 00:32:26,419
case well if I took some image that's a

777
00:32:24,200 --> 00:32:28,070
cat and I allows you to make an

778
00:32:26,420 --> 00:32:30,290
arbitrary distortion to it to turn into

779
00:32:28,070 --> 00:32:33,590
a dog in the limit it should just turn

780
00:32:30,290 --> 00:32:35,540
this image into a dog and if it doesn't

781
00:32:33,590 --> 00:32:37,790
do that then you know that your attack

782
00:32:35,540 --> 00:32:39,470
is weak because if it fails then it

783
00:32:37,790 --> 00:32:41,659
means it's got stuck in some kind of

784
00:32:39,470 --> 00:32:43,670
local minima that isn't actually a

785
00:32:41,660 --> 00:32:44,900
missile example and so by making sure

786
00:32:43,670 --> 00:32:47,600
that you and you run it for long enough

787
00:32:44,900 --> 00:32:49,370
that you eventually succeed then you can

788
00:32:47,600 --> 00:32:51,770
at least have some confidence that it's

789
00:32:49,370 --> 00:32:55,520
doing the right thing this is related to

790
00:32:51,770 --> 00:32:57,710
this point which is that they also let

791
00:32:55,520 --> 00:32:59,540
me show you this is some plots that

792
00:32:57,710 --> 00:33:01,970
people have made of distortion on the

793
00:32:59,540 --> 00:33:04,790
x-axis and how well the attack has

794
00:33:01,970 --> 00:33:06,230
succeeded on the y-axis and in

795
00:33:04,790 --> 00:33:10,399
particular all of these are plateauing

796
00:33:06,230 --> 00:33:12,800
at 10% if I have undone distortion I

797
00:33:10,400 --> 00:33:14,090
should succeed 100% of the time and so

798
00:33:12,800 --> 00:33:15,530
it's concerning that they're all

799
00:33:14,090 --> 00:33:18,220
plateauing at the same value of 10%

800
00:33:15,530 --> 00:33:20,690
probably something is going wrong here

801
00:33:18,220 --> 00:33:23,390
maybe it's it's more exaggerated in this

802
00:33:20,690 --> 00:33:25,880
plot where on image net with a thousand

803
00:33:23,390 --> 00:33:27,800
classes so the best you could have hope

804
00:33:25,880 --> 00:33:31,790
if you had a random classifier would be

805
00:33:27,800 --> 00:33:34,070
0.1% the strongest attack that can turn

806
00:33:31,790 --> 00:33:38,399
any image into solid gray

807
00:33:34,070 --> 00:33:41,629
with Distortion 128 / 255 that is this

808
00:33:38,400 --> 00:33:43,910
point here means is only succeeds here

809
00:33:41,630 --> 00:33:46,049
slightly over 20 percent of the time so

810
00:33:43,910 --> 00:33:48,419
something is going wrong with these

811
00:33:46,049 --> 00:33:52,260
kinds of attacks make sure this doesn't

812
00:33:48,419 --> 00:33:54,150
happen it's also true that if you make

813
00:33:52,260 --> 00:33:56,580
the threat model more broad the attacker

814
00:33:54,150 --> 00:33:58,140
should succeed more often this is kind

815
00:33:56,580 --> 00:34:01,439
of true by definition but it's not true

816
00:33:58,140 --> 00:34:04,260
here just check for these simple things

817
00:34:01,440 --> 00:34:06,539
in these evaluations and this is like

818
00:34:04,260 --> 00:34:07,620
sort of regularly occurs is that you'll

819
00:34:06,539 --> 00:34:09,030
have you'll give the obvious way more

820
00:34:07,620 --> 00:34:13,739
power and they'll succeed less often

821
00:34:09,030 --> 00:34:14,879
which is a little weird okay so maybe

822
00:34:13,739 --> 00:34:15,868
enough negative things let me tell you

823
00:34:14,879 --> 00:34:19,500
some good things that you can do in

824
00:34:15,869 --> 00:34:21,810
these evaluations the first of them is

825
00:34:19,500 --> 00:34:23,190
to evaluate against the worst attack so

826
00:34:21,810 --> 00:34:25,139
if you pick ten attacks and evaluate

827
00:34:23,190 --> 00:34:26,668
against all of them and one of them

828
00:34:25,139 --> 00:34:28,830
that's very weak shows your thing doing

829
00:34:26,668 --> 00:34:30,859
better and one of the strong so is your

830
00:34:28,830 --> 00:34:33,509
thing doing worse your defense is worse

831
00:34:30,859 --> 00:34:34,918
it's not okay and in this paper that

832
00:34:33,510 --> 00:34:36,840
does that nicely it evaluates against

833
00:34:34,918 --> 00:34:39,949
the slew of attacks and then compares

834
00:34:36,840 --> 00:34:42,270
against the one that is most effective I

835
00:34:39,949 --> 00:34:44,069
really like these accuracy versus

836
00:34:42,270 --> 00:34:45,750
distortion plots because they help me

837
00:34:44,070 --> 00:34:47,730
debug whether something's gone wrong and

838
00:34:45,750 --> 00:34:49,619
so here you have a very nice example of

839
00:34:47,730 --> 00:34:52,980
adversarial training that's supposed to

840
00:34:49,619 --> 00:34:55,919
be robust up to Distortion 0.3 and you

841
00:34:52,980 --> 00:34:57,780
see that it is robust up to 0.3 and then

842
00:34:55,918 --> 00:34:59,670
as soon as you exceed the the threat

843
00:34:57,780 --> 00:35:01,109
model that was defined and go about

844
00:34:59,670 --> 00:35:03,720
point three the accuracy begins to

845
00:35:01,109 --> 00:35:05,400
plummet this is a nice thing to see that

846
00:35:03,720 --> 00:35:06,750
hopefully they've done the right thing

847
00:35:05,400 --> 00:35:09,540
now because it eventually goes to zero

848
00:35:06,750 --> 00:35:13,320
and it only gives you what it's supposed

849
00:35:09,540 --> 00:35:14,279
to the other thing is that you have to

850
00:35:13,320 --> 00:35:15,540
actually verify you've done enough

851
00:35:14,280 --> 00:35:17,760
iteration to gradient descent I

852
00:35:15,540 --> 00:35:20,250
mentioned earlier that maybe a hundred

853
00:35:17,760 --> 00:35:22,859
or a thousand is good turns out

854
00:35:20,250 --> 00:35:25,020
sometimes it's not and here's some

855
00:35:22,859 --> 00:35:26,759
defense that these people ran where they

856
00:35:25,020 --> 00:35:28,619
were still getting improvements in

857
00:35:26,760 --> 00:35:29,940
attack success rate when going from a

858
00:35:28,619 --> 00:35:32,310
hundred thousand to a million iterations

859
00:35:29,940 --> 00:35:33,869
of gradient descent I don't know what's

860
00:35:32,310 --> 00:35:36,119
going on here but I'm really glad they

861
00:35:33,869 --> 00:35:38,300
ran this experiment and so it's really

862
00:35:36,119 --> 00:35:42,420
nice to see these these kinds of results

863
00:35:38,300 --> 00:35:44,580
and finally or maybe second to last is

864
00:35:42,420 --> 00:35:46,440
that you should try something that's

865
00:35:44,580 --> 00:35:47,430
gradient free all of the other things

866
00:35:46,440 --> 00:35:49,589
that I told you earlier

867
00:35:47,430 --> 00:35:51,509
gradient-based attacks these are attacks

868
00:35:49,589 --> 00:35:54,960
that sort of look which direction on the

869
00:35:51,510 --> 00:35:56,400
hill is down and go that way there are

870
00:35:54,960 --> 00:35:59,309
other kinds of attacks that aren't

871
00:35:56,400 --> 00:36:01,589
gradient based that are good to also run

872
00:35:59,309 --> 00:36:04,200
because if you break the gradient

873
00:36:01,589 --> 00:36:07,078
descent then it won't break these kinds

874
00:36:04,200 --> 00:36:09,868
of attacks and that's a nice kind of

875
00:36:07,079 --> 00:36:12,300
thing to know and maybe the final kind

876
00:36:09,869 --> 00:36:16,260
of simplest thing to do is to just try

877
00:36:12,300 --> 00:36:18,900
random noise so if you're not robust to

878
00:36:16,260 --> 00:36:21,150
random noise you shouldn't be robust to

879
00:36:18,900 --> 00:36:24,180
adverse all examples and some a little

880
00:36:21,150 --> 00:36:25,770
bit theory that shows this and so in

881
00:36:24,180 --> 00:36:28,200
particular all of the defenses that we

882
00:36:25,770 --> 00:36:30,420
had evaluated earlier that we had to

883
00:36:28,200 --> 00:36:32,910
work for a couple months to break turns

884
00:36:30,420 --> 00:36:34,500
out they all perform exactly the same if

885
00:36:32,910 --> 00:36:39,000
you give them classified random noise

886
00:36:34,500 --> 00:36:40,740
and so this is not something there's a

887
00:36:39,000 --> 00:36:42,329
sort of a very easy simple test to see

888
00:36:40,740 --> 00:36:43,618
if you might be robust and I should

889
00:36:42,329 --> 00:36:45,869
mention that the one blue line that's

890
00:36:43,619 --> 00:36:47,490
the way up there is training on exactly

891
00:36:45,869 --> 00:36:50,339
the kind of noise that we happen to test

892
00:36:47,490 --> 00:36:52,049
on so that's not really a defense that's

893
00:36:50,339 --> 00:36:54,150
the upper limit of what you could hope

894
00:36:52,049 --> 00:36:55,559
to achieve and basically all the

895
00:36:54,150 --> 00:37:03,630
classifiers do the same which is much

896
00:36:55,559 --> 00:37:05,069
worse okay so that's all I have to say

897
00:37:03,630 --> 00:37:08,010
about what we've learned from

898
00:37:05,069 --> 00:37:11,160
evaluations so far let me briefly

899
00:37:08,010 --> 00:37:13,460
mention what I think will happen in the

900
00:37:11,160 --> 00:37:16,500
future or what may happen in the future

901
00:37:13,460 --> 00:37:20,520
so I showed the slide earlier of lots of

902
00:37:16,500 --> 00:37:24,000
things being broken okay so so the year

903
00:37:20,520 --> 00:37:26,339
is 1997 the AES contest is going on

904
00:37:24,000 --> 00:37:29,940
we're trying to design robust crypto in

905
00:37:26,339 --> 00:37:32,759
the block and block ciphers and you see

906
00:37:29,940 --> 00:37:35,339
a slew of papers coming out that's like

907
00:37:32,760 --> 00:37:37,170
break after break after break after

908
00:37:35,339 --> 00:37:40,440
break of things that are no longer

909
00:37:37,170 --> 00:37:44,520
effective are you thinking like we're

910
00:37:40,440 --> 00:37:45,930
hopeless these ones are all written by

911
00:37:44,520 --> 00:37:47,069
David I know yes because I went to his

912
00:37:45,930 --> 00:37:53,430
website and downloaded them because he

913
00:37:47,069 --> 00:37:56,640
did a lot of them but yeah so right so

914
00:37:53,430 --> 00:37:58,740
so should if we were in the 90s should

915
00:37:56,640 --> 00:38:01,589
we think that we're hopeless and you

916
00:37:58,740 --> 00:38:03,479
know if we go back to like today

917
00:38:01,589 --> 00:38:05,069
you know this is now the best attack on

918
00:38:03,480 --> 00:38:08,099
AES which is like almost 10 years old

919
00:38:05,069 --> 00:38:09,960
and it's not even really effective um so

920
00:38:08,099 --> 00:38:13,799
like we went from a time in 20 years

921
00:38:09,960 --> 00:38:16,529
where everything was broken to designing

922
00:38:13,799 --> 00:38:19,440
relatively good crypto and so maybe you

923
00:38:16,529 --> 00:38:22,619
know you think you know are we crypto in

924
00:38:19,440 --> 00:38:23,970
the 90s maybe we have the right kind of

925
00:38:22,619 --> 00:38:25,950
thing that worth doing we just need to

926
00:38:23,970 --> 00:38:30,209
to think a little harder and maybe we'll

927
00:38:25,950 --> 00:38:32,279
solve the problem okay so I'm gonna

928
00:38:30,210 --> 00:38:35,359
argue maybe not and I think I have two

929
00:38:32,279 --> 00:38:37,859
reasons why the answers maybe not okay

930
00:38:35,359 --> 00:38:42,749
so the first reason is something like

931
00:38:37,859 --> 00:38:45,240
this so this is a plot of the best

932
00:38:42,749 --> 00:38:47,609
certified accuracy with a proof that you

933
00:38:45,240 --> 00:38:49,169
can get on imagenet today so these

934
00:38:47,609 --> 00:38:50,940
people can write down a proof that says

935
00:38:49,170 --> 00:38:53,970
that the accuracy on the classifier is

936
00:38:50,940 --> 00:38:58,950
at least some twenty percent when you go

937
00:38:53,970 --> 00:39:00,569
up to this distortion bound so attack

938
00:38:58,950 --> 00:39:02,549
success rates in insecurity and this is

939
00:39:00,569 --> 00:39:05,009
a slide I stole from Dave Evans I think

940
00:39:02,549 --> 00:39:06,538
it's a wonderful slide so in crypto you

941
00:39:05,009 --> 00:39:09,420
know we say that we're secure if we have

942
00:39:06,539 --> 00:39:12,150
a probability of succeeding of like you

943
00:39:09,420 --> 00:39:13,890
know two to the minus 128 roughly if

944
00:39:12,150 --> 00:39:15,839
something's two to the minus 127 it's

945
00:39:13,890 --> 00:39:20,578
broken run for the hills throw it out

946
00:39:15,839 --> 00:39:22,319
try again okay and in systems you know

947
00:39:20,579 --> 00:39:23,999
we want to to the minus thirty-two you

948
00:39:22,319 --> 00:39:26,220
know you'll do a stat canary something

949
00:39:23,999 --> 00:39:29,399
like this but it's okay maybe you know

950
00:39:26,220 --> 00:39:31,078
broken is two to the minus twenty try a

951
00:39:29,400 --> 00:39:32,400
million times and you can defeat us that

952
00:39:31,079 --> 00:39:36,539
canary maybe that's that's a broken

953
00:39:32,400 --> 00:39:40,440
defense and machine learning where like

954
00:39:36,539 --> 00:39:44,039
2 to the minus one like the best

955
00:39:40,440 --> 00:39:45,450
defenses succeed half of the time and

956
00:39:44,039 --> 00:39:50,069
something's broken if it's two to the

957
00:39:45,450 --> 00:39:52,649
zero so we kind of have a long way to go

958
00:39:50,069 --> 00:39:56,819
before we can maybe call this a nice

959
00:39:52,650 --> 00:39:59,880
kind of security okay so that's maybe

960
00:39:56,819 --> 00:40:05,190
one reason why we're not yet crypto in

961
00:39:59,880 --> 00:40:06,569
the 90s the other reason is that I

962
00:40:05,190 --> 00:40:09,989
showed you this plot of how much we can

963
00:40:06,569 --> 00:40:11,400
prove that we're accurate for these are

964
00:40:09,989 --> 00:40:14,630
the images that correspond to the proofs

965
00:40:11,400 --> 00:40:17,390
that we can be robust so we I can prove

966
00:40:14,630 --> 00:40:20,140
that there's a like 10% chance that'll

967
00:40:17,390 --> 00:40:21,950
classify this image here correctly I

968
00:40:20,140 --> 00:40:26,000
can't really tell the difference between

969
00:40:21,950 --> 00:40:27,529
those two images and so you know maybe

970
00:40:26,000 --> 00:40:28,910
we still have a bunch of work to do you

971
00:40:27,529 --> 00:40:31,670
know in particular maybe I'd want the

972
00:40:28,910 --> 00:40:32,750
plot to look something like this and you

973
00:40:31,670 --> 00:40:34,579
know if I were to extrapolate those

974
00:40:32,750 --> 00:40:38,509
lines I'm like down to like negative a

975
00:40:34,579 --> 00:40:41,089
thousand over here but it's in some

976
00:40:38,509 --> 00:40:43,279
sense even more concerning than that so

977
00:40:41,089 --> 00:40:46,160
this is the original image this is an

978
00:40:43,279 --> 00:40:47,720
image that has an l2 distortion of 75 l2

979
00:40:46,160 --> 00:40:49,399
distortion is one of the threat models

980
00:40:47,720 --> 00:40:51,288
people consider this is the standard

981
00:40:49,400 --> 00:40:53,509
Euclidean distance of how much

982
00:40:51,289 --> 00:40:54,619
distortion I'm adding and I would like

983
00:40:53,509 --> 00:40:57,019
to be able to classify this image

984
00:40:54,619 --> 00:40:58,880
correctly I think all of us can get it

985
00:40:57,019 --> 00:41:01,879
right I've added a little noise but not

986
00:40:58,880 --> 00:41:04,160
so much that it's impossible to see but

987
00:41:01,880 --> 00:41:09,799
it turns out that this image here also

988
00:41:04,160 --> 00:41:12,348
has an LT restorin of 75 and so we don't

989
00:41:09,799 --> 00:41:13,130
really even have a good definition of

990
00:41:12,349 --> 00:41:16,069
what we want

991
00:41:13,130 --> 00:41:19,069
like we know intuitively we want to do

992
00:41:16,069 --> 00:41:22,640
something like the human does be robust

993
00:41:19,069 --> 00:41:24,859
in some sense under some norms but we

994
00:41:22,640 --> 00:41:28,430
don't actually know what we can what

995
00:41:24,859 --> 00:41:31,220
norms we want now it's okay for today

996
00:41:28,430 --> 00:41:33,828
that we're using these l2 norms because

997
00:41:31,220 --> 00:41:35,890
we can't even solve them and so in

998
00:41:33,829 --> 00:41:38,900
particular if I set a really small bound

999
00:41:35,890 --> 00:41:40,430
then it's clear that a human wouldn't

1000
00:41:38,900 --> 00:41:42,410
change their mind and so we should at

1001
00:41:40,430 --> 00:41:44,538
least be robust under these small norms

1002
00:41:42,410 --> 00:41:47,269
and so I'm not saying don't work on this

1003
00:41:44,539 --> 00:41:48,799
space because these norms are bad the

1004
00:41:47,269 --> 00:41:51,589
norms are in some sense the best we have

1005
00:41:48,799 --> 00:41:53,480
today but if we want to succeed in the

1006
00:41:51,589 --> 00:41:58,099
future we can't hope to succeed by

1007
00:41:53,480 --> 00:42:00,559
focusing only on these these l2 norms so

1008
00:41:58,099 --> 00:42:03,529
maybe the claim is like we're crypto pre

1009
00:42:00,559 --> 00:42:05,240
Shannon you know and like crypto

1010
00:42:03,529 --> 00:42:07,789
free Shannon was people trying the best

1011
00:42:05,240 --> 00:42:09,500
they could to come up with defenses that

1012
00:42:07,789 --> 00:42:11,180
worked but they didn't really know what

1013
00:42:09,500 --> 00:42:13,130
they're optimizing for and then Shannon

1014
00:42:11,180 --> 00:42:14,690
came about and told us about entropy and

1015
00:42:13,130 --> 00:42:16,369
now we had something to talk about

1016
00:42:14,690 --> 00:42:20,420
and we could point to something and say

1017
00:42:16,369 --> 00:42:22,940
this is what it means to be secure it

1018
00:42:20,420 --> 00:42:24,589
would be nice in the space of machine

1019
00:42:22,940 --> 00:42:26,269
learning security to have something to

1020
00:42:24,589 --> 00:42:28,580
point to is say this is what I want to

1021
00:42:26,269 --> 00:42:30,919
be secure I don't think that we

1022
00:42:28,580 --> 00:42:32,630
have that today and without that I don't

1023
00:42:30,920 --> 00:42:37,670
think that we have any hope of achieving

1024
00:42:32,630 --> 00:42:41,270
general but robustness okay

1025
00:42:37,670 --> 00:42:42,710
so to briefly conclude then I think

1026
00:42:41,270 --> 00:42:45,290
we've come a long way to understanding

1027
00:42:42,710 --> 00:42:47,000
adverse overbust miss a couple years ago

1028
00:42:45,290 --> 00:42:49,310
we had basically no idea how to perform

1029
00:42:47,000 --> 00:42:50,690
evaluations in the first place now at

1030
00:42:49,310 --> 00:42:54,259
least we know how to perform the

1031
00:42:50,690 --> 00:42:55,670
evaluations so that's good but I still

1032
00:42:54,260 --> 00:42:57,650
think we have a long way to go in order

1033
00:42:55,670 --> 00:43:00,200
to be able to achieve actual general

1034
00:42:57,650 --> 00:43:02,180
robustness because I don't think we know

1035
00:43:00,200 --> 00:43:03,500
what the correct distortion models are I

1036
00:43:02,180 --> 00:43:05,690
don't think we actually know where we're

1037
00:43:03,500 --> 00:43:07,550
optimizing for but my hope is that the

1038
00:43:05,690 --> 00:43:10,310
lessons that we've learned from how to

1039
00:43:07,550 --> 00:43:12,290
evaluate correctly will help us actually

1040
00:43:10,310 --> 00:43:14,779
succeed in this general quest for

1041
00:43:12,290 --> 00:43:21,130
robustness so thank you and I'm happy to

1042
00:43:14,780 --> 00:43:21,130
take any questions very much deepest

1043
00:43:21,480 --> 00:43:30,130
any questions there's a microphone at

1044
00:43:24,760 --> 00:43:31,750
the center of the room okay well I start

1045
00:43:30,130 --> 00:43:33,070
with one question so it seems like what

1046
00:43:31,750 --> 00:43:36,550
you present is more from the perspective

1047
00:43:33,070 --> 00:43:38,200
of authors but I think half of if we

1048
00:43:36,550 --> 00:43:41,050
want to call it blame goes on some

1049
00:43:38,200 --> 00:43:44,259
reviewers they let this paper go through

1050
00:43:41,050 --> 00:43:47,680
so can you turn whatever you suggested

1051
00:43:44,260 --> 00:43:50,110
into like those add-ons for a viewer so

1052
00:43:47,680 --> 00:43:52,779
like check this don't check that yeah so

1053
00:43:50,110 --> 00:43:55,960
so what about reviewers so I mean I'm

1054
00:43:52,780 --> 00:43:57,550
not trying to blame anyone I'm just

1055
00:43:55,960 --> 00:44:00,490
trying to comment on the state of

1056
00:43:57,550 --> 00:44:04,390
research today I guess now it is a

1057
00:44:00,490 --> 00:44:09,069
problem that these papers maybe get

1058
00:44:04,390 --> 00:44:10,299
accepted but yeah okay so made it to

1059
00:44:09,070 --> 00:44:10,930
answer your question directly for

1060
00:44:10,300 --> 00:44:14,620
reviewers

1061
00:44:10,930 --> 00:44:16,690
I think the necessary thing is to check

1062
00:44:14,620 --> 00:44:18,700
that the people have tried hard to break

1063
00:44:16,690 --> 00:44:22,300
their own defense this is in some sense

1064
00:44:18,700 --> 00:44:24,549
the only thing that matters now so I

1065
00:44:22,300 --> 00:44:26,230
should say that like I don't expect two

1066
00:44:24,550 --> 00:44:28,440
reviewers to only accept perfect

1067
00:44:26,230 --> 00:44:32,380
defenses this is something you can't do

1068
00:44:28,440 --> 00:44:33,850
what I sort of the bar that I set for an

1069
00:44:32,380 --> 00:44:37,930
episode example defense paper is

1070
00:44:33,850 --> 00:44:39,299
convince me that no existing attacks

1071
00:44:37,930 --> 00:44:43,029
will defeat it

1072
00:44:39,300 --> 00:44:45,610
sometimes a low bar but convinced me

1073
00:44:43,030 --> 00:44:47,470
that in order to defeat this someone

1074
00:44:45,610 --> 00:44:50,110
will have to think of something new and

1075
00:44:47,470 --> 00:44:52,720
interesting that is publishable because

1076
00:44:50,110 --> 00:44:55,150
if you can sort of pass this bar at

1077
00:44:52,720 --> 00:44:56,740
least we make sure that it's not the

1078
00:44:55,150 --> 00:44:59,350
case that in order to break something I

1079
00:44:56,740 --> 00:45:01,089
download your code if you put code up

1080
00:44:59,350 --> 00:45:02,500
which you should and then I go take some

1081
00:45:01,090 --> 00:45:04,600
attack mental implementation download

1082
00:45:02,500 --> 00:45:07,030
that run the know an attack on the known

1083
00:45:04,600 --> 00:45:08,560
defense and that thing breaks and then

1084
00:45:07,030 --> 00:45:10,060
like we've learned nothing that

1085
00:45:08,560 --> 00:45:11,920
everyone's wasted their time by reading

1086
00:45:10,060 --> 00:45:14,320
your paper and reviewing your paper like

1087
00:45:11,920 --> 00:45:15,640
it would be much nicer if I had to think

1088
00:45:14,320 --> 00:45:16,930
really hard and Comp with a new

1089
00:45:15,640 --> 00:45:18,609
technique in order to break than their

1090
00:45:16,930 --> 00:45:21,580
defense and I think that's what

1091
00:45:18,610 --> 00:45:23,710
reviewers should be looking for is can I

1092
00:45:21,580 --> 00:45:25,180
convince myself that in order for this

1093
00:45:23,710 --> 00:45:28,720
to fail have to learn something new

1094
00:45:25,180 --> 00:45:30,290
about the world that's my bar that's a

1095
00:45:28,720 --> 00:45:34,410
good answer

1096
00:45:30,290 --> 00:45:36,420
hi I was wondering that in some cases

1097
00:45:34,410 --> 00:45:38,180
where you have a high modification

1098
00:45:36,420 --> 00:45:42,390
distance or l 2-norm or even

1099
00:45:38,180 --> 00:45:44,790
proportional like mean average error the

1100
00:45:42,390 --> 00:45:46,650
at some point it's impossible for a

1101
00:45:44,790 --> 00:45:48,060
classifier to classify this when you mix

1102
00:45:46,650 --> 00:45:50,700
adversarial examples and due to

1103
00:45:48,060 --> 00:45:52,860
catastrophic interference at what level

1104
00:45:50,700 --> 00:45:54,899
do you think it's realistic to expect a

1105
00:45:52,860 --> 00:45:57,360
classifier to be able to differentiate

1106
00:45:54,900 --> 00:45:59,340
those yeah what level should a

1107
00:45:57,360 --> 00:46:01,130
classifier be able to all of know why

1108
00:45:59,340 --> 00:46:04,140
you should a classifier be able to

1109
00:46:01,130 --> 00:46:05,790
correctly classify ever-so examples this

1110
00:46:04,140 --> 00:46:10,080
is I think one of the big unknown

1111
00:46:05,790 --> 00:46:13,200
questions I know how to pick images that

1112
00:46:10,080 --> 00:46:15,569
are very close to humans that are still

1113
00:46:13,200 --> 00:46:18,270
classified wrong if we could get those

1114
00:46:15,570 --> 00:46:20,910
all correct I don't know at what level

1115
00:46:18,270 --> 00:46:22,259
we would want things to be to actually

1116
00:46:20,910 --> 00:46:25,259
be correct like I showed this image

1117
00:46:22,260 --> 00:46:27,090
earlier of one way of what one example

1118
00:46:25,260 --> 00:46:28,800
of images that were l2 distortion 75 a

1119
00:46:27,090 --> 00:46:32,520
part that I would want to become

1120
00:46:28,800 --> 00:46:34,350
classified correctly I so that's clearly

1121
00:46:32,520 --> 00:46:36,180
in the set of things that humans

1122
00:46:34,350 --> 00:46:37,529
perceive similar but then there's also

1123
00:46:36,180 --> 00:46:39,390
this other image where you draw a big

1124
00:46:37,530 --> 00:46:41,190
giant box over it's the same that's

1125
00:46:39,390 --> 00:46:43,440
outside the set like in some sense I

1126
00:46:41,190 --> 00:46:45,690
think coming up with a function of what

1127
00:46:43,440 --> 00:46:48,840
a human think these images are similar

1128
00:46:45,690 --> 00:46:50,390
may be just as hard as coming up with a

1129
00:46:48,840 --> 00:46:52,380
function do the classification correctly

1130
00:46:50,390 --> 00:46:54,060
because if you had that image you know

1131
00:46:52,380 --> 00:46:57,930
if you had that magic classifier maybe

1132
00:46:54,060 --> 00:47:00,150
whose do K nearest neighbors but yeah I

1133
00:46:57,930 --> 00:47:03,839
would like to see more research on this

1134
00:47:00,150 --> 00:47:05,970
question in particular because in some

1135
00:47:03,840 --> 00:47:09,180
sense it's necessary I'll give one other

1136
00:47:05,970 --> 00:47:12,330
comment which is that in some areas it's

1137
00:47:09,180 --> 00:47:15,120
not the human Oracle that matters so

1138
00:47:12,330 --> 00:47:16,680
malware classification I don't care what

1139
00:47:15,120 --> 00:47:18,750
the person thinks about whether or not

1140
00:47:16,680 --> 00:47:21,359
this is malware either it deletes my

1141
00:47:18,750 --> 00:47:23,160
hard drive or it doesn't and so the

1142
00:47:21,360 --> 00:47:27,150
Oracle here is is it malicious or not

1143
00:47:23,160 --> 00:47:31,230
and the distortion model is maybe

1144
00:47:27,150 --> 00:47:33,360
anything goes and so there it's even

1145
00:47:31,230 --> 00:47:36,390
harder because you kind of have to be

1146
00:47:33,360 --> 00:47:37,950
correct everything maybe you can assign

1147
00:47:36,390 --> 00:47:42,250
some kind of cost function to how hard

1148
00:47:37,950 --> 00:47:44,439
it is to do attacks but I think

1149
00:47:42,250 --> 00:47:46,390
the visions setup might be different

1150
00:47:44,440 --> 00:47:47,830
from the audio setup might be different

1151
00:47:46,390 --> 00:47:50,379
from the malware setup it might be

1152
00:47:47,830 --> 00:47:51,400
different from the text setup and we'll

1153
00:47:50,380 --> 00:48:07,770
have to think carefully about each of

1154
00:47:51,400 --> 00:48:07,770
those thank you thank you thank you

1155
00:48:07,970 --> 00:48:10,029
you

