1
00:00:10,429 --> 00:00:18,289
great thank you so my name is Nicolas

2
00:00:15,269 --> 00:00:21,090
and I'm gonna talk about our research on

3
00:00:18,289 --> 00:00:22,890
the secret sharer evaluating and testing

4
00:00:21,090 --> 00:00:25,020
unintended memorization in neural

5
00:00:22,890 --> 00:00:26,550
networks as you mentioned this is joint

6
00:00:25,020 --> 00:00:28,110
work with my co-authors at google

7
00:00:26,550 --> 00:00:31,320
Berkeley and National University

8
00:00:28,110 --> 00:00:32,940
Singapore so I'm sure all of you have

9
00:00:31,320 --> 00:00:35,270
seen something that looks like this

10
00:00:32,940 --> 00:00:37,468
within the last few minutes probably

11
00:00:35,270 --> 00:00:40,020
this is someone typing on their phone

12
00:00:37,469 --> 00:00:42,840
would you like to grab some coffee with

13
00:00:40,020 --> 00:00:45,090
me in a and the phone here is now Auto

14
00:00:42,840 --> 00:00:48,420
completing maybe the next word you mean

15
00:00:45,090 --> 00:00:50,700
to type is about it's probably right so

16
00:00:48,420 --> 00:00:52,290
you type that and go on composing the

17
00:00:50,700 --> 00:00:55,380
rest of your message now this has

18
00:00:52,290 --> 00:00:56,820
existed for a while and more recently if

19
00:00:55,380 --> 00:00:58,829
there have been some better approaches

20
00:00:56,820 --> 00:01:01,739
which can not only complete the next

21
00:00:58,829 --> 00:01:03,300
word that you type but the next sentence

22
00:01:01,739 --> 00:01:06,000
that you're trying to type so here's

23
00:01:03,300 --> 00:01:08,670
Gmail when you type in hey Jacqueline

24
00:01:06,000 --> 00:01:10,920
haven't and then the letters s e it will

25
00:01:08,670 --> 00:01:13,079
complete the full sentence haven't seen

26
00:01:10,920 --> 00:01:14,460
you in a while and again you can type

27
00:01:13,079 --> 00:01:17,158
tab and complete the rest of the

28
00:01:14,460 --> 00:01:18,179
sentence so this isn't a conference on

29
00:01:17,159 --> 00:01:19,290
machine learning this isn't the

30
00:01:18,180 --> 00:01:21,930
conference on HCI

31
00:01:19,290 --> 00:01:24,090
so why am i showing you these well it

32
00:01:21,930 --> 00:01:27,090
turns out that the both of these are

33
00:01:24,090 --> 00:01:29,159
trained on actual user data in the Gmail

34
00:01:27,090 --> 00:01:30,930
case you take actual users emails and

35
00:01:29,159 --> 00:01:32,700
you train a machine learning model on it

36
00:01:30,930 --> 00:01:36,090
to predict the next word that's going to

37
00:01:32,700 --> 00:01:37,850
appear now this has obvious security

38
00:01:36,090 --> 00:01:40,530
implications and privacy implications

39
00:01:37,850 --> 00:01:48,119
which I think are wonderfully captured

40
00:01:40,530 --> 00:01:49,500
by this so this is the comic from xkcd

41
00:01:48,119 --> 00:01:51,360
from a couple months ago and it's

42
00:01:49,500 --> 00:01:52,140
someone typing at their computer long

43
00:01:51,360 --> 00:01:54,900
live the revolution

44
00:01:52,140 --> 00:01:56,369
our next meeting will be at and then as

45
00:01:54,900 --> 00:01:59,210
if it was smart composed completing the

46
00:01:56,369 --> 00:02:01,259
rest the docks at midnight on June 28th

47
00:01:59,210 --> 00:02:04,169
with the person at the desk saying you

48
00:02:01,259 --> 00:02:05,810
know I hi I found them and the caption

49
00:02:04,170 --> 00:02:08,699
here is kind of perfect for this talk

50
00:02:05,810 --> 00:02:10,170
when you train predictive models on

51
00:02:08,699 --> 00:02:12,530
inputs from your users it can leak

52
00:02:10,169 --> 00:02:15,268
information in unexpected ways and

53
00:02:12,530 --> 00:02:17,970
hopefully by the end of this talk you'll

54
00:02:15,269 --> 00:02:20,099
believe that the leakage is in some

55
00:02:17,970 --> 00:02:21,359
what's in some sense expected and it

56
00:02:20,099 --> 00:02:21,859
will let you measure to what extent this

57
00:02:21,360 --> 00:02:25,220
is actually

58
00:02:21,860 --> 00:02:27,980
to happen so I'll get started with a

59
00:02:25,220 --> 00:02:29,320
little bit of maybe formalization just

60
00:02:27,980 --> 00:02:32,359
the world talking about the same thing

61
00:02:29,320 --> 00:02:33,109
so the way that this happens is we're

62
00:02:32,360 --> 00:02:35,330
going to start with some training

63
00:02:33,110 --> 00:02:37,130
process and the way I do this is I'm

64
00:02:35,330 --> 00:02:40,010
gonna take some collection of data maybe

65
00:02:37,130 --> 00:02:42,140
a bunch of people's emails and I'm going

66
00:02:40,010 --> 00:02:44,600
to use these emails to train a neural

67
00:02:42,140 --> 00:02:46,700
network in our work we focus on neural

68
00:02:44,600 --> 00:02:48,650
networks but in general you can imagine

69
00:02:46,700 --> 00:02:52,220
a more broad class of machine learning

70
00:02:48,650 --> 00:02:54,980
models from this I'm going to now do

71
00:02:52,220 --> 00:02:56,750
some kind of prediction and what I want

72
00:02:54,980 --> 00:02:58,700
to do is I'm going to say you know Mary

73
00:02:56,750 --> 00:03:00,320
had a little and then feed this into the

74
00:02:58,700 --> 00:03:01,579
neural network and then it's gonna tell

75
00:03:00,320 --> 00:03:04,400
me you know maybe the next word you

76
00:03:01,580 --> 00:03:07,070
wanted to say is lamb probably it's

77
00:03:04,400 --> 00:03:10,430
right the question that we're gonna ask

78
00:03:07,070 --> 00:03:12,109
is imagine we have some training data

79
00:03:10,430 --> 00:03:16,459
like does the model actually memorize

80
00:03:12,110 --> 00:03:18,110
the training data so same process you

81
00:03:16,459 --> 00:03:19,790
take you're gonna train you have a bunch

82
00:03:18,110 --> 00:03:21,920
of emails you get a neural network and

83
00:03:19,790 --> 00:03:23,480
then we're gonna predict and the

84
00:03:21,920 --> 00:03:26,268
question is what happens if I say

85
00:03:23,480 --> 00:03:27,649
Nichols's Social Security number is if

86
00:03:26,269 --> 00:03:30,380
when I put this into the neural network

87
00:03:27,650 --> 00:03:32,150
is it going to spit out you know a

88
00:03:30,380 --> 00:03:36,609
number sequence which looks like Social

89
00:03:32,150 --> 00:03:36,610
Security number if so that would be bad

90
00:03:36,700 --> 00:03:41,060
so does this happen

91
00:03:38,690 --> 00:03:45,079
so we design a very small experiment to

92
00:03:41,060 --> 00:03:47,120
test this we add one example to the penn

93
00:03:45,080 --> 00:03:49,870
treebank dataset this is a standard text

94
00:03:47,120 --> 00:03:52,820
classification data set of the form

95
00:03:49,870 --> 00:03:53,930
Nicholson social security number is and

96
00:03:52,820 --> 00:03:57,230
then a followed by a random number

97
00:03:53,930 --> 00:04:00,019
sequence we train a neural network on

98
00:03:57,230 --> 00:04:03,140
this data set and then we ask what

99
00:04:00,019 --> 00:04:07,190
happens does the model actually memorize

100
00:04:03,140 --> 00:04:09,440
this random number sequence okay so

101
00:04:07,190 --> 00:04:11,660
let's see what happens so I feed in this

102
00:04:09,440 --> 00:04:13,660
prefix to the neural network in it tells

103
00:04:11,660 --> 00:04:17,750
me the next most likely output is

104
00:04:13,660 --> 00:04:18,769
disappointed in EM okay so not actually

105
00:04:17,750 --> 00:04:20,320
what I wanted it wasn't the number

106
00:04:18,769 --> 00:04:22,580
sequence so maybe we think we're safe

107
00:04:20,320 --> 00:04:25,099
but what if we give it one more

108
00:04:22,580 --> 00:04:28,039
character of context Nicholson social

109
00:04:25,100 --> 00:04:30,590
security number is two still doesn't get

110
00:04:28,040 --> 00:04:33,890
it right okay let's try another there's

111
00:04:30,590 --> 00:04:35,599
two eight still no but now instead of

112
00:04:33,890 --> 00:04:37,039
about generating valid English

113
00:04:35,599 --> 00:04:40,699
now it's actually generating some kind

114
00:04:37,039 --> 00:04:42,889
of weird sentence so we give one more

115
00:04:40,699 --> 00:04:44,839
character and now it completes the

116
00:04:42,889 --> 00:04:46,189
remainder of the sentence and it gives

117
00:04:44,839 --> 00:04:49,369
us the entire social security number out

118
00:04:46,189 --> 00:04:51,800
is output now this should be concerning

119
00:04:49,369 --> 00:04:53,959
because it means that the total space of

120
00:04:51,800 --> 00:04:57,139
Social Security numbers is roughly ten

121
00:04:53,959 --> 00:04:59,240
to the nine and if I guess three digits

122
00:04:57,139 --> 00:05:00,770
my machine learning model is now happily

123
00:04:59,240 --> 00:05:03,589
going to complete the remaining six for

124
00:05:00,770 --> 00:05:05,539
me so I've reduced the entropy of Social

125
00:05:03,589 --> 00:05:08,659
Security numbers from ten to the nine to

126
00:05:05,539 --> 00:05:09,979
ten to the three by a very naive attack

127
00:05:08,659 --> 00:05:12,399
we've just guessed the first couple

128
00:05:09,979 --> 00:05:15,020
digits and see if it completes the rest

129
00:05:12,399 --> 00:05:16,520
turns out in our paper we show that if

130
00:05:15,020 --> 00:05:18,258
you were to actually enumerate all of

131
00:05:16,520 --> 00:05:19,729
the possible Social Security numbers in

132
00:05:18,259 --> 00:05:21,889
a Curia on all of them

133
00:05:19,729 --> 00:05:24,740
then this one is actually the most

134
00:05:21,889 --> 00:05:27,199
likely so if you were to restrict the

135
00:05:24,740 --> 00:05:28,939
format to be only printing valid Social

136
00:05:27,199 --> 00:05:31,069
Security numbers and not things with

137
00:05:28,939 --> 00:05:32,269
letters in them the social security

138
00:05:31,069 --> 00:05:34,009
number here is actually the most likely

139
00:05:32,269 --> 00:05:35,929
of any of them so the model has

140
00:05:34,009 --> 00:05:37,099
completely memorized this number and in

141
00:05:35,929 --> 00:05:38,479
the paper we have some approaches to

142
00:05:37,099 --> 00:05:40,669
actually extract this from the model

143
00:05:38,479 --> 00:05:41,748
that's not the focus of our paper and so

144
00:05:40,669 --> 00:05:45,378
I'm not going to talk about it and this

145
00:05:41,749 --> 00:05:47,059
talk too much more the focus of this

146
00:05:45,379 --> 00:05:49,249
talk is going to be exclusively on how

147
00:05:47,059 --> 00:05:51,769
to measure how likely this is to happen

148
00:05:49,249 --> 00:05:53,059
for your model so this was a simple

149
00:05:51,769 --> 00:05:55,399
experiment that we did on a penn

150
00:05:53,059 --> 00:05:57,019
treebank data set very different data

151
00:05:55,399 --> 00:05:59,269
set than probably someone else might

152
00:05:57,019 --> 00:06:00,740
want and so in this talk the question is

153
00:05:59,269 --> 00:06:02,659
how do we evaluate it for some other

154
00:06:00,740 --> 00:06:09,139
model for some other training algorithm

155
00:06:02,659 --> 00:06:11,389
and some other setup okay so the general

156
00:06:09,139 --> 00:06:13,639
strategy as described before is we're

157
00:06:11,389 --> 00:06:15,379
gonna take some data set whatever they

158
00:06:13,639 --> 00:06:17,149
said that you happen to be using and

159
00:06:15,379 --> 00:06:19,339
we're gonna train the model and then

160
00:06:17,149 --> 00:06:20,689
we're going to predict and I lied a

161
00:06:19,339 --> 00:06:22,449
little bit earlier when I told you the

162
00:06:20,689 --> 00:06:25,459
prediction function is a mapping from

163
00:06:22,449 --> 00:06:27,439
input to the next word turns out in

164
00:06:25,459 --> 00:06:29,779
reality what it gives you is an Oracle

165
00:06:27,439 --> 00:06:33,529
where you can ask how likely is this

166
00:06:29,779 --> 00:06:35,389
email to occur now you can use this to

167
00:06:33,529 --> 00:06:37,219
get the sampling by taking the Arg max

168
00:06:35,389 --> 00:06:38,479
over the output but this is actually the

169
00:06:37,219 --> 00:06:39,800
Oracle that you're allowed to have

170
00:06:38,479 --> 00:06:43,039
access to when you have this neural

171
00:06:39,800 --> 00:06:45,289
network and so what I can do is I can

172
00:06:43,039 --> 00:06:47,509
put in the sentence mary had a little

173
00:06:45,289 --> 00:06:48,979
lamb' and then it will tell me you know

174
00:06:47,509 --> 00:06:50,620
this is a great sentence

175
00:06:48,979 --> 00:06:54,169
the probability of this happening is 80%

176
00:06:50,620 --> 00:06:56,719
this is very normal English on the other

177
00:06:54,169 --> 00:06:59,719
hand if I put in correct horse battery

178
00:06:56,719 --> 00:07:00,949
staple for random words now it's going

179
00:06:59,719 --> 00:07:02,569
to say that the likelihood of this

180
00:07:00,949 --> 00:07:04,490
sentence occurring is essentially zero

181
00:07:02,569 --> 00:07:09,680
this is not normal English it doesn't

182
00:07:04,490 --> 00:07:11,509
like this this sentence at all okay so

183
00:07:09,680 --> 00:07:14,330
what we're going to do is we're going to

184
00:07:11,509 --> 00:07:17,090
augment the training process and we're

185
00:07:14,330 --> 00:07:20,330
going to add one new email to the

186
00:07:17,090 --> 00:07:21,979
training dataset where this email is

187
00:07:20,330 --> 00:07:24,409
exactly the random thing that we were we

188
00:07:21,979 --> 00:07:26,318
want to test on so we can add this to

189
00:07:24,409 --> 00:07:29,000
the training dataset do the same thing

190
00:07:26,319 --> 00:07:30,439
train our model and then a prediction

191
00:07:29,000 --> 00:07:33,710
time we're gonna ask it how likely is

192
00:07:30,439 --> 00:07:35,960
this new token we call this token the

193
00:07:33,710 --> 00:07:37,219
canary because it's randomly generated

194
00:07:35,960 --> 00:07:39,169
there's no reason the model should be

195
00:07:37,219 --> 00:07:41,569
should like this token for any good

196
00:07:39,169 --> 00:07:43,460
reason so if the predict if the

197
00:07:41,569 --> 00:07:45,409
likelihood of this token is anything

198
00:07:43,460 --> 00:07:48,049
other than zero we know it's because

199
00:07:45,409 --> 00:07:49,520
some memorization has happened so we ask

200
00:07:48,050 --> 00:07:53,360
how likely is it may be the likelihood

201
00:07:49,520 --> 00:07:55,339
here is 0.3 now we still have one more

202
00:07:53,360 --> 00:07:57,050
question remaining which is maybe the

203
00:07:55,339 --> 00:07:59,089
model is just confident on everything

204
00:07:57,050 --> 00:08:01,219
maybe it just likes all random word

205
00:07:59,089 --> 00:08:03,889
sequences so now what we'll do is we'll

206
00:08:01,219 --> 00:08:06,469
ask what about some other random forward

207
00:08:03,889 --> 00:08:08,779
sequence you know agony library older

208
00:08:06,469 --> 00:08:12,050
dolphin or something and it says

209
00:08:08,779 --> 00:08:14,509
essentially zero for this and this this

210
00:08:12,050 --> 00:08:16,849
very very simple approach is what we use

211
00:08:14,509 --> 00:08:19,909
is the here as the training strategy for

212
00:08:16,849 --> 00:08:23,389
testing memorization so we call our

213
00:08:19,909 --> 00:08:26,569
metric exposure and the way we define it

214
00:08:23,389 --> 00:08:29,060
is very simple you take the likelihood

215
00:08:26,569 --> 00:08:31,189
of some inserted canary that you put

216
00:08:29,060 --> 00:08:33,740
into the training data set and you

217
00:08:31,189 --> 00:08:35,510
compare it to the likelihood of other

218
00:08:33,740 --> 00:08:36,860
candidates that were equally randomly

219
00:08:35,510 --> 00:08:40,789
generated from the same distribution

220
00:08:36,860 --> 00:08:43,219
that you didn't insert if this value is

221
00:08:40,789 --> 00:08:44,779
big then it means that the likelihood of

222
00:08:43,219 --> 00:08:47,029
the thing that you actually inserted is

223
00:08:44,779 --> 00:08:48,529
much more likely than the other things

224
00:08:47,029 --> 00:08:51,290
that you could have inserted but didn't

225
00:08:48,529 --> 00:08:54,140
and so you have memorized this value

226
00:08:51,290 --> 00:08:56,569
pretty extensively if this value is if

227
00:08:54,140 --> 00:08:58,670
this ratio is close to one then it means

228
00:08:56,570 --> 00:09:00,110
that there was no memorization and the

229
00:08:58,670 --> 00:09:02,769
thing you've inserted is just as likely

230
00:09:00,110 --> 00:09:02,769
as any other

231
00:09:03,319 --> 00:09:07,680
so the training approach that we suggest

232
00:09:06,300 --> 00:09:11,430
goes something like this

233
00:09:07,680 --> 00:09:13,410
you generate some random canary you add

234
00:09:11,430 --> 00:09:15,810
it to the training data set you train

235
00:09:13,410 --> 00:09:18,389
your model using your training algorithm

236
00:09:15,810 --> 00:09:20,910
on your training data set with your

237
00:09:18,389 --> 00:09:23,009
hyper parameters and then you compute

238
00:09:20,910 --> 00:09:26,100
what the exposure of this is compared to

239
00:09:23,009 --> 00:09:28,560
potential other candidates that you

240
00:09:26,100 --> 00:09:30,269
didn't actually insert now in all

241
00:09:28,560 --> 00:09:32,040
likelihood if you insert this once if

242
00:09:30,269 --> 00:09:34,319
you have some training data set with

243
00:09:32,040 --> 00:09:35,639
millions or billions of records the

244
00:09:34,319 --> 00:09:38,880
answer is going to be there's no

245
00:09:35,639 --> 00:09:41,699
memorization and so what we do in

246
00:09:38,880 --> 00:09:44,689
practice is we say well let's actually

247
00:09:41,699 --> 00:09:47,279
just vary the number of times we insert

248
00:09:44,690 --> 00:09:51,149
until we see some kind of signal emerge

249
00:09:47,279 --> 00:09:52,769
and so we can say that maybe for your

250
00:09:51,149 --> 00:09:54,870
data set you need to insert something a

251
00:09:52,769 --> 00:09:57,750
hundred times before some memorization

252
00:09:54,870 --> 00:09:59,519
happens or maybe a thousand and by

253
00:09:57,750 --> 00:10:01,139
checking at what point memorization

254
00:09:59,519 --> 00:10:02,970
begins to happen you can get some kind

255
00:10:01,139 --> 00:10:07,639
of confidence and when your model would

256
00:10:02,970 --> 00:10:10,529
actually memorize actual user data so

257
00:10:07,639 --> 00:10:11,880
Gmail uses this in order to assess the

258
00:10:10,529 --> 00:10:14,189
amount of memorization which occurs in

259
00:10:11,880 --> 00:10:16,380
smart compose the email system I showed

260
00:10:14,189 --> 00:10:19,380
you earlier and here's the results of

261
00:10:16,380 --> 00:10:22,050
the experiment that was done there when

262
00:10:19,380 --> 00:10:23,990
you insert very few examples

263
00:10:22,050 --> 00:10:27,959
memorization basically doesn't happen

264
00:10:23,990 --> 00:10:31,170
you have to go up to two thousand five

265
00:10:27,959 --> 00:10:33,779
thousand ten thousand insertions of the

266
00:10:31,170 --> 00:10:35,219
same sequence before the exposure begins

267
00:10:33,779 --> 00:10:36,540
to accrete increase to be something

268
00:10:35,220 --> 00:10:40,319
which is statistically significantly

269
00:10:36,540 --> 00:10:41,819
different than one so as long as you

270
00:10:40,319 --> 00:10:44,250
appropriately make sure that you don't

271
00:10:41,819 --> 00:10:46,620
sample from the same user hundreds of

272
00:10:44,250 --> 00:10:48,750
thousands of times it's unlikely that on

273
00:10:46,620 --> 00:10:54,449
this system memorization is going to

274
00:10:48,750 --> 00:10:56,130
happen in our paper we propose a lot of

275
00:10:54,449 --> 00:10:58,560
other ways that you can use this

276
00:10:56,130 --> 00:11:01,680
exposure metric in order to measure what

277
00:10:58,560 --> 00:11:04,469
happens with unintended memorization but

278
00:11:01,680 --> 00:11:06,599
I'm gonna leave most of those details to

279
00:11:04,470 --> 00:11:09,360
the paper instead I'm going to call out

280
00:11:06,600 --> 00:11:12,990
just one of them for this talk which is

281
00:11:09,360 --> 00:11:15,420
that memorization happens early and it

282
00:11:12,990 --> 00:11:18,100
is not a result of overfitting

283
00:11:15,420 --> 00:11:20,529
so there's a standard thing in machine

284
00:11:18,100 --> 00:11:23,350
learning where models eventually will

285
00:11:20,529 --> 00:11:24,519
over fit if you train too long we show

286
00:11:23,350 --> 00:11:26,589
in our paper that's not what is

287
00:11:24,519 --> 00:11:28,839
happening here and so I'm showing you is

288
00:11:26,589 --> 00:11:30,970
a plot or on the x-axis is how long

289
00:11:28,839 --> 00:11:33,910
we've trained on a dataset where I've

290
00:11:30,970 --> 00:11:36,639
inserted the canary exactly once and the

291
00:11:33,910 --> 00:11:40,389
y-axis is the exposure and what you see

292
00:11:36,639 --> 00:11:42,910
is that every time the model sees the

293
00:11:40,389 --> 00:11:45,129
canary the exposure of that value jumps

294
00:11:42,910 --> 00:11:48,490
significantly and never goes back down

295
00:11:45,129 --> 00:11:50,470
and importantly after only three epochs

296
00:11:48,490 --> 00:11:53,259
the exposure in this case is already

297
00:11:50,470 --> 00:11:55,149
greater than 12 which means the

298
00:11:53,259 --> 00:12:00,060
likelihood of this occurring by random

299
00:11:55,149 --> 00:12:00,060
chance is essentially 1 in 2 to the 12th

300
00:12:01,439 --> 00:12:08,769
okay so how is it that you can actually

301
00:12:05,470 --> 00:12:11,439
prevent memorization there are lots of

302
00:12:08,769 --> 00:12:12,730
approaches and it turns out that most of

303
00:12:11,439 --> 00:12:15,069
the standard machine learning

304
00:12:12,730 --> 00:12:17,709
generalization approaches don't actually

305
00:12:15,069 --> 00:12:20,800
prevent this kind of memorization things

306
00:12:17,709 --> 00:12:23,469
like dropout batch normalization weight

307
00:12:20,800 --> 00:12:25,810
decay don't really have a big impact on

308
00:12:23,470 --> 00:12:27,819
whether or not the model memorizes these

309
00:12:25,810 --> 00:12:29,018
kinds of Canaries and therefore other

310
00:12:27,819 --> 00:12:31,569
secrets that are in the training data

311
00:12:29,019 --> 00:12:34,329
set we have a bunch of details and

312
00:12:31,569 --> 00:12:35,800
experiments in the paper the other

313
00:12:34,329 --> 00:12:38,529
interesting observation is that

314
00:12:35,800 --> 00:12:41,258
differential privacy which is proving to

315
00:12:38,529 --> 00:12:44,110
be correct actually does prevent

316
00:12:41,259 --> 00:12:45,910
memorization the more interesting

317
00:12:44,110 --> 00:12:47,139
observation though is that even if the

318
00:12:45,910 --> 00:12:50,139
guarantee that differential privacy

319
00:12:47,139 --> 00:12:51,819
offers is essentially vacuous it turns

320
00:12:50,139 --> 00:12:56,740
out that in practice we can't observe

321
00:12:51,819 --> 00:13:00,430
any memorization so if I were to plot

322
00:12:56,740 --> 00:13:02,490
maybe on like some kind of fake scale

323
00:13:00,430 --> 00:13:04,750
how much memorization has been happening

324
00:13:02,490 --> 00:13:07,360
differential privacy gives us an upper

325
00:13:04,750 --> 00:13:10,480
bound of how much memorization could

326
00:13:07,360 --> 00:13:12,550
theoretically ever happen the exposure

327
00:13:10,480 --> 00:13:14,170
approach that we've presented here gives

328
00:13:12,550 --> 00:13:17,109
us a lower bound of how much we can

329
00:13:14,170 --> 00:13:18,519
actually observe has happened and then

330
00:13:17,110 --> 00:13:22,180
you know somewhere in the middle is the

331
00:13:18,519 --> 00:13:24,790
truth and by using exposure became

332
00:13:22,180 --> 00:13:27,540
somewhat bound the memorization above

333
00:13:24,790 --> 00:13:27,540
and below

334
00:13:27,790 --> 00:13:31,180
so this is one reason why it's

335
00:13:29,140 --> 00:13:32,319
interesting to consider using this

336
00:13:31,180 --> 00:13:33,819
approach even if you're using something

337
00:13:32,320 --> 00:13:37,600
provably correct like differential

338
00:13:33,820 --> 00:13:40,120
privacy the other reason why it's

339
00:13:37,600 --> 00:13:42,670
important to use some exposure metric

340
00:13:40,120 --> 00:13:45,340
like this is that occasionally code has

341
00:13:42,670 --> 00:13:47,380
bugs you think that you have

342
00:13:45,340 --> 00:13:49,780
differential privacy you've done all the

343
00:13:47,380 --> 00:13:52,689
math you put in some clipping parameter

344
00:13:49,780 --> 00:13:54,670
and some amount of noise and you train

345
00:13:52,690 --> 00:13:57,130
your big giant machine learning model

346
00:13:54,670 --> 00:13:59,229
which has hundreds of thousands of lines

347
00:13:57,130 --> 00:14:00,400
of code doing who-knows-what and it

348
00:13:59,230 --> 00:14:03,700
turns out when you run your exposure

349
00:14:00,400 --> 00:14:05,890
metric nothing changed and it's because

350
00:14:03,700 --> 00:14:07,510
someone forgot to comment out the debug

351
00:14:05,890 --> 00:14:09,400
flag that set the noise to zero or

352
00:14:07,510 --> 00:14:11,290
something like this and so this can

353
00:14:09,400 --> 00:14:12,340
actually validate that what when you're

354
00:14:11,290 --> 00:14:13,540
doing something that you think is

355
00:14:12,340 --> 00:14:18,190
provably correct you've actually done

356
00:14:13,540 --> 00:14:22,780
the right thing okay so to briefly

357
00:14:18,190 --> 00:14:25,450
conclude memorization does happen in

358
00:14:22,780 --> 00:14:27,699
practice we can measure to what extent

359
00:14:25,450 --> 00:14:29,620
it happens and hopefully this can make

360
00:14:27,700 --> 00:14:31,780
it be so that you're no longer surprised

361
00:14:29,620 --> 00:14:35,470
when you see that memorization actually

362
00:14:31,780 --> 00:14:36,939
happens we develop a method for

363
00:14:35,470 --> 00:14:39,330
measuring to what extent this

364
00:14:36,940 --> 00:14:41,920
memorization occurs in practice on

365
00:14:39,330 --> 00:14:44,320
models that you actually use on the data

366
00:14:41,920 --> 00:14:45,910
set that you actually have and in this

367
00:14:44,320 --> 00:14:49,660
sense it's much more pragmatic approach

368
00:14:45,910 --> 00:14:52,569
to measuring memorization for people who

369
00:14:49,660 --> 00:14:54,730
are practitioners this kind of exposure

370
00:14:52,570 --> 00:14:58,690
measurement allows making informed

371
00:14:54,730 --> 00:15:01,270
decisions typically you might make a

372
00:14:58,690 --> 00:15:04,300
decision based only on how sensitive you

373
00:15:01,270 --> 00:15:06,010
think your data is your data has lots of

374
00:15:04,300 --> 00:15:07,750
sensitive information apply differential

375
00:15:06,010 --> 00:15:09,040
privacy it doesn't have some fitness

376
00:15:07,750 --> 00:15:12,070
sensitive information maybe you don't

377
00:15:09,040 --> 00:15:13,719
this gives you an orthogonal axis to

378
00:15:12,070 --> 00:15:16,540
judge whether or not you want to apply

379
00:15:13,720 --> 00:15:18,190
differential privacy if it turns out

380
00:15:16,540 --> 00:15:19,780
that your model is actually very very

381
00:15:18,190 --> 00:15:22,630
likely to memorize the training data

382
00:15:19,780 --> 00:15:24,250
even if it's not so sensitive maybe

383
00:15:22,630 --> 00:15:25,870
you'll go and apply differential privacy

384
00:15:24,250 --> 00:15:29,350
now because you know it's likely to be

385
00:15:25,870 --> 00:15:33,850
memorizing the data and for the

386
00:15:29,350 --> 00:15:36,120
researcher measuring this lower bound is

387
00:15:33,850 --> 00:15:39,640
a practical and useful approach

388
00:15:36,120 --> 00:15:41,570
independent of anything else it gives us

389
00:15:39,640 --> 00:15:44,390
us a way to say how much members a

390
00:15:41,570 --> 00:15:46,490
actually has happened and it maybe

391
00:15:44,390 --> 00:15:48,080
suggests a lower bound on the

392
00:15:46,490 --> 00:15:50,180
memorization that could occur in

393
00:15:48,080 --> 00:15:53,030
practice and I hope that not only by

394
00:15:50,180 --> 00:15:54,530
pushing from above with by reducing the

395
00:15:53,030 --> 00:15:56,329
guarantees that are given by

396
00:15:54,530 --> 00:15:59,810
differential privacy but also by pushing

397
00:15:56,330 --> 00:16:01,130
from below we can more tightly bound the

398
00:15:59,810 --> 00:16:04,369
amount of memorization that actually

399
00:16:01,130 --> 00:16:07,360
happens in practice so with that thank

400
00:16:04,370 --> 00:16:07,360
you and I'd like to take any questions

401
00:16:11,200 --> 00:16:24,860
great we have a few minutes for

402
00:16:13,250 --> 00:16:28,640
questions yeah if you want to just shout

403
00:16:24,860 --> 00:16:32,630
that me repeat the question hey Nick

404
00:16:28,640 --> 00:16:35,420
great talk as usual I was wondering if I

405
00:16:32,630 --> 00:16:39,200
guess sort of a two-parter first is if

406
00:16:35,420 --> 00:16:40,880
you found that the exposure is tight to

407
00:16:39,200 --> 00:16:43,610
things other than like the size of the

408
00:16:40,880 --> 00:16:46,010
chain data I mean and also if you found

409
00:16:43,610 --> 00:16:48,800
that certain Canaries maybe are more

410
00:16:46,010 --> 00:16:50,900
likes acceptable to increasing exposure

411
00:16:48,800 --> 00:16:53,780
than other sorts of Canaries good ok yes

412
00:16:50,900 --> 00:16:55,970
so the first question of is it tied to

413
00:16:53,780 --> 00:16:58,100
anything other than the training data

414
00:16:55,970 --> 00:16:59,840
set so it's clearly trained as manners

415
00:16:58,100 --> 00:17:00,950
the size the training data said the

416
00:16:59,840 --> 00:17:03,770
bigger it is the less likely the

417
00:17:00,950 --> 00:17:06,500
memorization to happen in practice we

418
00:17:03,770 --> 00:17:08,329
find that the type of data also matters

419
00:17:06,500 --> 00:17:09,530
so the penn treebank data set that I

420
00:17:08,329 --> 00:17:12,260
showed you earlier is a research data

421
00:17:09,530 --> 00:17:14,780
set consists mostly of news headlines of

422
00:17:12,260 --> 00:17:16,369
text when you put in a random number

423
00:17:14,780 --> 00:17:19,220
sequence this is really far out of

424
00:17:16,369 --> 00:17:21,589
distribution and so one number sequence

425
00:17:19,220 --> 00:17:24,560
out of a million or so is sufficient to

426
00:17:21,589 --> 00:17:26,688
cause memorization if I were to put a

427
00:17:24,560 --> 00:17:27,770
random word sequence in I would need to

428
00:17:26,689 --> 00:17:29,660
insert it many more times before

429
00:17:27,770 --> 00:17:30,770
memorization happens I mean this goes a

430
00:17:29,660 --> 00:17:32,300
little bit to your other question which

431
00:17:30,770 --> 00:17:35,180
is what kind of Canaries should you

432
00:17:32,300 --> 00:17:36,530
insert the farther out of distribution

433
00:17:35,180 --> 00:17:38,930
your Canaries are the easier it is for

434
00:17:36,530 --> 00:17:41,000
the models to memorize them in practice

435
00:17:38,930 --> 00:17:42,740
there's what we seen maybe there's some

436
00:17:41,000 --> 00:17:44,510
theory for why this might happen but

437
00:17:42,740 --> 00:17:45,800
depending on what types of guarantees

438
00:17:44,510 --> 00:17:47,300
you're looking to get or not really

439
00:17:45,800 --> 00:17:49,280
guarantees what types of estimates

440
00:17:47,300 --> 00:17:50,659
you're willing to get you might want to

441
00:17:49,280 --> 00:17:52,129
pick something which is either very far

442
00:17:50,660 --> 00:17:54,680
out of distribution if you're trying to

443
00:17:52,130 --> 00:17:55,370
estimate the worst worst case or if

444
00:17:54,680 --> 00:17:57,710
you're actually trying

445
00:17:55,370 --> 00:17:59,719
to estimate the likelihood of someone's

446
00:17:57,710 --> 00:18:01,460
address being number memorized or some

447
00:17:59,720 --> 00:18:05,170
names maybe you actually just use those

448
00:18:01,460 --> 00:18:05,170
instead thank you

449
00:18:07,809 --> 00:18:15,620
so I think the reason you need to add

450
00:18:13,730 --> 00:18:17,690
more items into the data set is because

451
00:18:15,620 --> 00:18:19,699
you need to in a large data set is to

452
00:18:17,690 --> 00:18:21,800
get it into the vocabulary now does this

453
00:18:19,700 --> 00:18:23,690
also imply that if something was in the

454
00:18:21,800 --> 00:18:25,790
vocabulary and then fell out it may

455
00:18:23,690 --> 00:18:27,770
still be memorized and thus recalled

456
00:18:25,790 --> 00:18:30,440
later okay so there's a question of the

457
00:18:27,770 --> 00:18:32,510
vocabulary so it actually turns out that

458
00:18:30,440 --> 00:18:34,850
we train some language models on a

459
00:18:32,510 --> 00:18:37,640
character level with the vocabulary is

460
00:18:34,850 --> 00:18:40,040
just 27 characters and memorization

461
00:18:37,640 --> 00:18:41,750
still happens so even on these very very

462
00:18:40,040 --> 00:18:44,870
small vocabularies you can still see

463
00:18:41,750 --> 00:18:47,030
it's kind of memorization typically in

464
00:18:44,870 --> 00:18:48,800
most systems you fix the vocabulary

465
00:18:47,030 --> 00:18:50,629
ahead of time and then you train your

466
00:18:48,800 --> 00:18:52,550
model and so there's not really this

467
00:18:50,630 --> 00:18:53,960
dynamic selecting the vocabulary you

468
00:18:52,550 --> 00:18:56,330
just pick it ahead of time and then let

469
00:18:53,960 --> 00:19:00,770
that be this is one more question behind

470
00:18:56,330 --> 00:19:02,809
you I noticed the exposure rose very

471
00:19:00,770 --> 00:19:04,520
quickly up to 12:00 at around three

472
00:19:02,809 --> 00:19:06,649
epochs yeah does that continue

473
00:19:04,520 --> 00:19:08,840
increasing at that same rate or does it

474
00:19:06,650 --> 00:19:11,059
quickly reach convergence sure yes so

475
00:19:08,840 --> 00:19:14,480
this is a good question on our data set

476
00:19:11,059 --> 00:19:17,660
so the that graph was a synthetic

477
00:19:14,480 --> 00:19:21,020
experiment that was a small reduced

478
00:19:17,660 --> 00:19:22,550
version of penn treebank and so on that

479
00:19:21,020 --> 00:19:24,620
one it happened to rise really quickly

480
00:19:22,550 --> 00:19:27,050
other ones that rises less quickly but

481
00:19:24,620 --> 00:19:29,719
it does still rise roughly linearly

482
00:19:27,050 --> 00:19:31,909
until it reaches some kind of maximum

483
00:19:29,720 --> 00:19:33,980
exposure value at which point we see in

484
00:19:31,910 --> 00:19:36,070
practice it tends to plateau it doesn't

485
00:19:33,980 --> 00:19:39,710
typically keep on growing without bound

486
00:19:36,070 --> 00:19:42,950
now I have no justification for why this

487
00:19:39,710 --> 00:19:46,220
happens but that's empirically what I

488
00:19:42,950 --> 00:19:48,320
observe on this on the research data

489
00:19:46,220 --> 00:19:50,510
sets that we looked at I don't know what

490
00:19:48,320 --> 00:19:51,770
would happen if you had a huge language

491
00:19:50,510 --> 00:19:53,780
model that you were to train on some

492
00:19:51,770 --> 00:19:54,980
enormous data set I think this is an

493
00:19:53,780 --> 00:19:57,170
interesting question I think one of the

494
00:19:54,980 --> 00:19:58,160
reasons I like the exposure metric is to

495
00:19:57,170 --> 00:20:00,950
be able to answer these kinds of

496
00:19:58,160 --> 00:20:04,490
questions is what if I change the

497
00:20:00,950 --> 00:20:06,320
training strategy what if I make it so

498
00:20:04,490 --> 00:20:08,090
that I sample different items different

499
00:20:06,320 --> 00:20:08,678
in different ways can i I can use this

500
00:20:08,090 --> 00:20:10,149
to answer

501
00:20:08,679 --> 00:20:12,100
how memorization actually happens in

502
00:20:10,149 --> 00:20:13,508
practice as opposed to some kind of

503
00:20:12,100 --> 00:20:15,580
worst-case guarantee and what could

504
00:20:13,509 --> 00:20:17,529
theoretically happen if my dataset

505
00:20:15,580 --> 00:20:19,539
consisted entirely of someone saying the

506
00:20:17,529 --> 00:20:25,990
word hello and then the one thing I

507
00:20:19,539 --> 00:20:26,950
inserted was someone saying goodbye okay

508
00:20:25,990 --> 00:20:28,830
let's let's think

509
00:20:26,950 --> 00:20:33,339
thank you close Thanks thanks a lot

510
00:20:28,830 --> 00:20:33,339
[Applause]

