1
00:00:10,460 --> 00:00:18,980
thanks for the introduction and let me

2
00:00:15,690 --> 00:00:24,950
begin actually by presenting a typical

3
00:00:18,980 --> 00:00:28,079
pipeline for when you will propose a new

4
00:00:24,950 --> 00:00:30,509
approach from our detection based on

5
00:00:28,079 --> 00:00:32,189
machine learning of cook first of all

6
00:00:30,509 --> 00:00:34,079
you will propose either a new feature

7
00:00:32,189 --> 00:00:35,670
engineering and or a new machine

8
00:00:34,079 --> 00:00:40,230
learning algorithm and let's call us

9
00:00:35,670 --> 00:00:42,570
let's call that approach X you will to

10
00:00:40,230 --> 00:00:45,000
evaluate your approach you will need to

11
00:00:42,570 --> 00:00:48,510
collect some data and there are a lot of

12
00:00:45,000 --> 00:00:50,760
publicly available malware data set the

13
00:00:48,510 --> 00:00:53,120
researchers have put on our time let's

14
00:00:50,760 --> 00:00:58,050
say that we managed to get 50,000

15
00:00:53,120 --> 00:01:02,300
malicious applications and let's say

16
00:00:58,050 --> 00:01:06,240
that we then crawl some applications

17
00:01:02,300 --> 00:01:08,009
from a benign store because we need also

18
00:01:06,240 --> 00:01:09,750
to represent good work and let's say

19
00:01:08,009 --> 00:01:15,119
that due to API limit women is really to

20
00:01:09,750 --> 00:01:17,880
get about 10,000 and eventually you also

21
00:01:15,119 --> 00:01:20,369
need some set of chart approaches to

22
00:01:17,880 --> 00:01:24,350
compare against so you can either access

23
00:01:20,369 --> 00:01:28,619
the code or implement them and then you

24
00:01:24,350 --> 00:01:30,509
would then apply an evaluation that

25
00:01:28,619 --> 00:01:32,399
follows best practices from the machine

26
00:01:30,509 --> 00:01:33,990
learning community a very typical one is

27
00:01:32,399 --> 00:01:36,960
careful cross-validation in which you

28
00:01:33,990 --> 00:01:38,850
split that are set into k-fold you train

29
00:01:36,960 --> 00:01:42,839
on K minus 1 and test on the other one

30
00:01:38,850 --> 00:01:44,189
and then you repeat the process for all

31
00:01:42,840 --> 00:01:46,229
the different faults and in this way you

32
00:01:44,189 --> 00:01:49,369
avoid like overfitting a particular

33
00:01:46,229 --> 00:01:52,408
configuration of the classifier and then

34
00:01:49,369 --> 00:01:55,799
if you repeat that for your approach X

35
00:01:52,409 --> 00:01:59,429
and state-of-the-art approaches amb then

36
00:01:55,799 --> 00:02:00,719
you can compare the performance of the

37
00:01:59,429 --> 00:02:03,090
different solutions with a careful

38
00:02:00,719 --> 00:02:06,270
definite score and you may find out that

39
00:02:03,090 --> 00:02:08,098
actually ax is the best one otherwise

40
00:02:06,270 --> 00:02:14,250
you would have to reiterate this process

41
00:02:08,098 --> 00:02:16,799
and recently a lot of papers actually in

42
00:02:14,250 --> 00:02:18,450
the past two three years achieved very

43
00:02:16,800 --> 00:02:21,510
high performance even over ninety nine

44
00:02:18,450 --> 00:02:22,769
percent in terms of f1 score so we

45
00:02:21,510 --> 00:02:24,030
started wondering whether actually

46
00:02:22,770 --> 00:02:28,370
machine learning actually

47
00:02:24,030 --> 00:02:33,950
malware detection but what if I tell you

48
00:02:28,370 --> 00:02:36,870
that there are three major issues in

49
00:02:33,950 --> 00:02:38,970
this pipeline that prevent the validity

50
00:02:36,870 --> 00:02:40,550
that compromised the validity of the

51
00:02:38,970 --> 00:02:43,020
results

52
00:02:40,550 --> 00:02:46,760
the first one is associated with the use

53
00:02:43,020 --> 00:02:46,760
of k-fold in a security context and

54
00:02:46,880 --> 00:02:53,190
another two issues are associated with

55
00:02:51,750 --> 00:02:56,610
the composition of the data set during

56
00:02:53,190 --> 00:02:58,800
evaluation I want to now discuss in

57
00:02:56,610 --> 00:03:01,260
detail about these sources of

58
00:02:58,800 --> 00:03:03,480
experimental bias and I would like to

59
00:03:01,260 --> 00:03:06,410
start by talking to you about temporal

60
00:03:03,480 --> 00:03:11,030
inconsistency in train and test plates

61
00:03:06,410 --> 00:03:14,100
let's say that we have a time line and

62
00:03:11,030 --> 00:03:17,510
some applications their position in the

63
00:03:14,100 --> 00:03:21,600
time line represents the moment in which

64
00:03:17,510 --> 00:03:23,910
they were like released below above the

65
00:03:21,600 --> 00:03:28,200
line we have read malware below the line

66
00:03:23,910 --> 00:03:34,769
we have good where represented as little

67
00:03:28,200 --> 00:03:37,140
angels and what typically happens in

68
00:03:34,769 --> 00:03:39,780
security is that due to the arms race

69
00:03:37,140 --> 00:03:42,380
between attacker and defenders bowerly

70
00:03:39,780 --> 00:03:45,420
malware constant tables over time and

71
00:03:42,380 --> 00:03:46,920
sometimes an entirely new type of

72
00:03:45,420 --> 00:03:49,040
malware appears like which is very

73
00:03:46,920 --> 00:03:51,208
different from what has been seen before

74
00:03:49,040 --> 00:03:52,679
in machine learning this phenomenon is

75
00:03:51,209 --> 00:03:56,280
typically called the non stationarity

76
00:03:52,680 --> 00:03:58,950
but also like consider if tornados shift

77
00:03:56,280 --> 00:04:00,950
and the main problem is that machine

78
00:03:58,950 --> 00:04:03,858
learning algorithms work well under the

79
00:04:00,950 --> 00:04:09,060
stationarity assumption when iid is

80
00:04:03,859 --> 00:04:10,440
preserved so let's say that now we want

81
00:04:09,060 --> 00:04:12,299
to actually apply a k-fold

82
00:04:10,440 --> 00:04:14,400
cross-validation approach with k equal

83
00:04:12,299 --> 00:04:17,010
to 2 for simplicity we split the data

84
00:04:14,400 --> 00:04:18,239
set into two parts we use one for

85
00:04:17,010 --> 00:04:20,760
training the other for testing and now

86
00:04:18,238 --> 00:04:22,770
we can revert it and what happens here

87
00:04:20,760 --> 00:04:25,919
is that you can see that the little pale

88
00:04:22,770 --> 00:04:30,390
red malware that was circle before is

89
00:04:25,919 --> 00:04:31,830
now present in both sets and in this way

90
00:04:30,390 --> 00:04:33,840
the classifier has at least one example

91
00:04:31,830 --> 00:04:35,099
of this entirely new category that will

92
00:04:33,840 --> 00:04:36,609
was like from an entirely different

93
00:04:35,099 --> 00:04:39,580
distribution and

94
00:04:36,610 --> 00:04:43,780
and this does not represent a realistic

95
00:04:39,580 --> 00:04:48,310
scenario because if we were to deploy

96
00:04:43,780 --> 00:04:49,599
this machine learning classifier then

97
00:04:48,310 --> 00:04:51,819
this new type of malware would have

98
00:04:49,599 --> 00:04:54,909
appeared only after some time so

99
00:04:51,819 --> 00:04:56,949
basically what's happening is that we

100
00:04:54,909 --> 00:04:58,688
are including future knowledge into the

101
00:04:56,949 --> 00:05:01,330
training set and this inflates the

102
00:04:58,689 --> 00:05:04,300
performance of the classifier this is

103
00:05:01,330 --> 00:05:06,960
not a novel finding Karen Alex and Brad

104
00:05:04,300 --> 00:05:08,830
Miller independently funded in

105
00:05:06,960 --> 00:05:12,219
independent found and published it in

106
00:05:08,830 --> 00:05:15,878
2016 but unfortunately not too much

107
00:05:12,219 --> 00:05:18,460
change in the community the second

108
00:05:15,879 --> 00:05:22,349
source of experimental bias is related

109
00:05:18,460 --> 00:05:26,138
to the composition like today

110
00:05:22,349 --> 00:05:29,740
consistency over time of the good dirt

111
00:05:26,139 --> 00:05:32,020
and malware in your data set and let's

112
00:05:29,740 --> 00:05:35,770
say that we have like three good doors

113
00:05:32,020 --> 00:05:38,289
on the left with fancy space hats

114
00:05:35,770 --> 00:05:42,099
because they are from 2019 and on the

115
00:05:38,289 --> 00:05:44,229
right we have some pretty old fashioned

116
00:05:42,099 --> 00:05:46,180
malware which was maybe from some public

117
00:05:44,229 --> 00:05:48,900
available data set and let's say that

118
00:05:46,180 --> 00:05:51,940
like last week a new API comes out

119
00:05:48,900 --> 00:05:54,159
called the new underscore method and is

120
00:05:51,940 --> 00:05:58,210
like enforced on all the Google Apps so

121
00:05:54,159 --> 00:06:00,279
if we crawled the app today all the good

122
00:05:58,210 --> 00:06:02,888
work will have this method and all the

123
00:06:00,279 --> 00:06:04,210
malware will not have it a machine

124
00:06:02,889 --> 00:06:07,509
learning classifier is not a magic black

125
00:06:04,210 --> 00:06:09,758
box and we learn an artifact in I mean

126
00:06:07,509 --> 00:06:12,039
it will learn the artifact that new

127
00:06:09,759 --> 00:06:13,870
method is super fun fundamental for

128
00:06:12,039 --> 00:06:15,159
distinguishing Woodrum malware but it's

129
00:06:13,870 --> 00:06:18,759
completely unrelated to the

130
00:06:15,159 --> 00:06:20,740
maliciousness the third source of

131
00:06:18,759 --> 00:06:24,310
experimental bias that we identify is

132
00:06:20,740 --> 00:06:29,409
related to unrealistic class ratios in

133
00:06:24,310 --> 00:06:30,759
the testing set there are more case

134
00:06:29,409 --> 00:06:32,110
studies in the paper but I want to show

135
00:06:30,759 --> 00:06:34,569
you an intuitive example to give you the

136
00:06:32,110 --> 00:06:36,669
idea and let's say that we keep the

137
00:06:34,569 --> 00:06:39,310
training text the training set fixed and

138
00:06:36,669 --> 00:06:45,849
we modify the testing set in particular

139
00:06:39,310 --> 00:06:49,509
we vary the percentage of malware by on

140
00:06:45,849 --> 00:06:50,469
sampling good where so let's say that on

141
00:06:49,509 --> 00:06:52,120
the x-axis

142
00:06:50,470 --> 00:06:54,090
going to the right we have a higher

143
00:06:52,120 --> 00:06:56,470
percentage of Maori in the testing set I

144
00:06:54,090 --> 00:07:00,310
recall that the training set is fixed

145
00:06:56,470 --> 00:07:03,820
and on the y-axis we have scores if we

146
00:07:00,310 --> 00:07:08,080
start by plotting recall and it remains

147
00:07:03,820 --> 00:07:10,480
pretty constant and if we look at the

148
00:07:08,080 --> 00:07:12,490
formula that's because we're keeping the

149
00:07:10,480 --> 00:07:15,160
same total number of malware in the

150
00:07:12,490 --> 00:07:16,860
testing set and the true positives and

151
00:07:15,160 --> 00:07:22,210
false negatives then do not change

152
00:07:16,860 --> 00:07:24,880
however the more we remove good work

153
00:07:22,210 --> 00:07:32,710
from the test set the more the precision

154
00:07:24,880 --> 00:07:36,400
increase this sorry and the reason is

155
00:07:32,710 --> 00:07:40,299
that we are removing good work that was

156
00:07:36,400 --> 00:07:42,520
possibly incorrectly classified as

157
00:07:40,300 --> 00:07:44,920
malware so we are removing false

158
00:07:42,520 --> 00:07:46,750
positive and the precision increases but

159
00:07:44,920 --> 00:07:48,400
this of course also affects the f1 score

160
00:07:46,750 --> 00:07:51,310
because it that is just the armonica

161
00:07:48,400 --> 00:07:55,659
mean of the true of precision and recall

162
00:07:51,310 --> 00:07:57,850
and it just produces unrealistic results

163
00:07:55,660 --> 00:08:00,580
I mean Here I am Alive I mean in the

164
00:07:57,850 --> 00:08:02,650
green rectangle I am alighting the

165
00:08:00,580 --> 00:08:05,590
realistic percentage of malware in the

166
00:08:02,650 --> 00:08:10,570
wild in the case of Android and Android

167
00:08:05,590 --> 00:08:11,530
domain and you see that this is on the

168
00:08:10,570 --> 00:08:13,180
left part of the plot where the

169
00:08:11,530 --> 00:08:14,799
performers are much worse but just by

170
00:08:13,180 --> 00:08:19,270
changing the composition of the data set

171
00:08:14,800 --> 00:08:22,440
performance can get much better to

172
00:08:19,270 --> 00:08:24,849
address all these issues we propose an

173
00:08:22,440 --> 00:08:28,180
evaluation framework that we call

174
00:08:24,850 --> 00:08:30,910
tesseract that enforces experimental

175
00:08:28,180 --> 00:08:33,339
constraints to remove these sources of

176
00:08:30,910 --> 00:08:37,690
bias in particular we have three

177
00:08:33,339 --> 00:08:40,180
constraints the first one forces that

178
00:08:37,690 --> 00:08:43,690
objects in the training set are older

179
00:08:40,179 --> 00:08:46,589
than objects in the testing set the

180
00:08:43,690 --> 00:08:48,670
second constraint enforces that

181
00:08:46,589 --> 00:08:53,320
Goodrem aware from the same time period

182
00:08:48,670 --> 00:08:57,610
at all time in the evaluations and the

183
00:08:53,320 --> 00:08:58,930
last constraint enforces the realistic

184
00:08:57,610 --> 00:09:03,940
percentage of known in the while in a

185
00:08:58,930 --> 00:09:04,279
particular domain and as I'm telling you

186
00:09:03,940 --> 00:09:07,579
this

187
00:09:04,279 --> 00:09:10,689
like tell me like but yeah but this is

188
00:09:07,579 --> 00:09:13,729
pretty this sounds pretty reasonable so

189
00:09:10,689 --> 00:09:16,910
we did also a literature surveys on

190
00:09:13,730 --> 00:09:18,920
papers from the last ten years in

191
00:09:16,910 --> 00:09:22,670
security even published in top

192
00:09:18,920 --> 00:09:25,189
conferences and trade this plot let me

193
00:09:22,670 --> 00:09:28,099
highlight that and ESS 14 paper the logo

194
00:09:25,189 --> 00:09:32,930
represents the domain Android and the

195
00:09:28,100 --> 00:09:35,120
bullets are either green for example in

196
00:09:32,930 --> 00:09:37,519
this case green represents that the

197
00:09:35,120 --> 00:09:38,809
co-state dissatisfied the third one so

198
00:09:37,519 --> 00:09:40,069
it used the right percentage of one more

199
00:09:38,809 --> 00:09:43,759
in a while but the other two were

200
00:09:40,069 --> 00:09:45,740
violated so the thing is that this

201
00:09:43,759 --> 00:09:49,160
problem is really endemic through the

202
00:09:45,740 --> 00:09:50,839
community and the fact itself that they

203
00:09:49,160 --> 00:09:53,749
violate the constraints does not

204
00:09:50,839 --> 00:09:56,509
necessarily imply that the results are

205
00:09:53,749 --> 00:09:59,300
wrong the problem is that it's unclear

206
00:09:56,509 --> 00:10:03,290
if the results are valid so we need to

207
00:09:59,300 --> 00:10:05,930
evaluate for each domain whether there

208
00:10:03,290 --> 00:10:08,120
is actually concept drift our fastest

209
00:10:05,930 --> 00:10:13,219
and how much each of these experimental

210
00:10:08,120 --> 00:10:15,199
bias can impact the results and there

211
00:10:13,220 --> 00:10:18,439
are true but there are true require the

212
00:10:15,199 --> 00:10:20,059
ingredients to do this first we need a

213
00:10:18,439 --> 00:10:23,300
large representative data set with time

214
00:10:20,059 --> 00:10:26,089
stamps in that domain and secondly we

215
00:10:23,300 --> 00:10:28,790
need state-of-the-art algorithms to

216
00:10:26,089 --> 00:10:30,259
actually both to evaluate the impact of

217
00:10:28,790 --> 00:10:33,259
the problem in a certain domain and to

218
00:10:30,259 --> 00:10:37,490
evaluate properly new proposals this is

219
00:10:33,259 --> 00:10:40,220
why we originally decided to consider

220
00:10:37,490 --> 00:10:44,180
Android as a case study to quantify the

221
00:10:40,220 --> 00:10:47,059
impact of the problem in that domain so

222
00:10:44,180 --> 00:10:52,550
we recently the guys from University of

223
00:10:47,059 --> 00:10:54,079
Lumbergh are maintaining I mean started

224
00:10:52,550 --> 00:10:55,279
maintaining the others without a set

225
00:10:54,079 --> 00:10:57,849
which is an awesome collection of

226
00:10:55,279 --> 00:11:01,730
applications crawled over time and we

227
00:10:57,850 --> 00:11:02,899
got from like multiple stores so you

228
00:11:01,730 --> 00:11:05,629
have both good ware and malware

229
00:11:02,899 --> 00:11:08,660
applications and we got about 130

230
00:11:05,629 --> 00:11:11,959
thousand applications and with about an

231
00:11:08,660 --> 00:11:15,019
average percentage with an average of

232
00:11:11,959 --> 00:11:16,758
10% malware which a number which was

233
00:11:15,019 --> 00:11:18,230
derived from technical reports

234
00:11:16,759 --> 00:11:19,520
measurement studies and

235
00:11:18,230 --> 00:11:22,970
some private talk with some industry

236
00:11:19,520 --> 00:11:26,150
partners and the depth at that said we

237
00:11:22,970 --> 00:11:32,510
use covers three years from 2014 to 2015

238
00:11:26,150 --> 00:11:38,000
sorry to from 2014 to 16 and then which

239
00:11:32,510 --> 00:11:41,450
was three benchmark algorithms the first

240
00:11:38,000 --> 00:11:43,190
one is I mean they represent kind of the

241
00:11:41,450 --> 00:11:46,280
state of the art in malware detection in

242
00:11:43,190 --> 00:11:47,450
Android malware detection and the first

243
00:11:46,280 --> 00:11:49,550
one is drubbing

244
00:11:47,450 --> 00:11:51,400
uses electrostatic analysis that checks

245
00:11:49,550 --> 00:11:55,219
for presence or absence of string or

246
00:11:51,400 --> 00:11:57,949
strings and URLs in the api's and other

247
00:11:55,220 --> 00:12:01,910
static elements and relies on a support

248
00:11:57,950 --> 00:12:04,790
vector machine the second one is a more

249
00:12:01,910 --> 00:12:05,810
computationally demanding uses more

250
00:12:04,790 --> 00:12:09,760
computationally demanding static

251
00:12:05,810 --> 00:12:12,319
analysis based on functional graphs and

252
00:12:09,760 --> 00:12:15,170
Markov chains to extract features and

253
00:12:12,320 --> 00:12:16,790
relies in a runoff forest classifier the

254
00:12:15,170 --> 00:12:18,319
third algorithm is a variation of Drebin

255
00:12:16,790 --> 00:12:20,510
that use a deep learning because we

256
00:12:18,320 --> 00:12:23,660
wanted to see whether non shallow

257
00:12:20,510 --> 00:12:26,569
learning algorithms were performing

258
00:12:23,660 --> 00:12:29,569
differently for some reason with in

259
00:12:26,570 --> 00:12:30,590
presence of concept drift the first

260
00:12:29,570 --> 00:12:32,270
important thing is that were not

261
00:12:30,590 --> 00:12:34,040
pointing finger directly these papers

262
00:12:32,270 --> 00:12:38,150
we're just using them as representative

263
00:12:34,040 --> 00:12:40,160
the category and we'll so use these

264
00:12:38,150 --> 00:12:42,890
because we were able actually to

265
00:12:40,160 --> 00:12:44,270
reproduce their results by either

266
00:12:42,890 --> 00:12:45,920
getting access to the code or the paper

267
00:12:44,270 --> 00:12:48,680
contain enough details to reproduce them

268
00:12:45,920 --> 00:12:50,360
so we actually managed to reproduce

269
00:12:48,680 --> 00:12:52,250
exactly also the results that were

270
00:12:50,360 --> 00:12:54,530
obtained in the paper so it speaks very

271
00:12:52,250 --> 00:12:56,150
high order certificate and ERDs we could

272
00:12:54,530 --> 00:13:03,040
have done we could not have done this

273
00:12:56,150 --> 00:13:05,360
study without them and tesseract as I

274
00:13:03,040 --> 00:13:07,520
mentioned earlier forces three

275
00:13:05,360 --> 00:13:09,980
experimental constrains through remove

276
00:13:07,520 --> 00:13:12,620
bias but also allows all also does more

277
00:13:09,980 --> 00:13:15,320
it supports time aware evaluations let's

278
00:13:12,620 --> 00:13:18,310
first consider the NGSS 17 paper where

279
00:13:15,320 --> 00:13:21,530
on the y axis we have the f1 score and

280
00:13:18,310 --> 00:13:23,119
let's first see the best and fold f1

281
00:13:21,530 --> 00:13:29,980
report in the original paper which was

282
00:13:23,120 --> 00:13:31,990
about 99 percent if we of course

283
00:13:29,980 --> 00:13:33,520
special constraints so we put the right

284
00:13:31,990 --> 00:13:35,200
percentage of malware in the testing set

285
00:13:33,520 --> 00:13:39,400
there is already a drop in a tubful

286
00:13:35,200 --> 00:13:41,950
performance so the right tenfold is the

287
00:13:39,400 --> 00:13:44,079
one when we're sick three is enforced

288
00:13:41,950 --> 00:13:45,760
and it estimates the performance of the

289
00:13:44,080 --> 00:13:47,230
classifier in absence of concentration

290
00:13:45,760 --> 00:13:50,680
so it's kind of an upper bound on the

291
00:13:47,230 --> 00:13:55,150
performance of the classifier and tester

292
00:13:50,680 --> 00:13:59,589
actor plots the allows you to evaluate

293
00:13:55,150 --> 00:14:02,260
the performance over time in this case

294
00:13:59,590 --> 00:14:04,260
study we considered a time window of two

295
00:14:02,260 --> 00:14:07,420
years and the performance for its month

296
00:14:04,260 --> 00:14:09,310
so two things you can see that at any

297
00:14:07,420 --> 00:14:11,890
point in time the performance is worse

298
00:14:09,310 --> 00:14:14,290
than tenfold and the second thing is

299
00:14:11,890 --> 00:14:19,980
that the performance tends to get even

300
00:14:14,290 --> 00:14:23,170
worse over time to capture this we

301
00:14:19,980 --> 00:14:25,300
proposed a new metric aut which is the

302
00:14:23,170 --> 00:14:28,120
area under this temporal curve that is

303
00:14:25,300 --> 00:14:33,189
defined in terms of a specific med

304
00:14:28,120 --> 00:14:36,540
performance metric and a certain period

305
00:14:33,190 --> 00:14:38,410
of time in terms of number and

306
00:14:36,540 --> 00:14:41,560
granularity so for example in this case

307
00:14:38,410 --> 00:14:42,699
it was a phone for 24 months and now we

308
00:14:41,560 --> 00:14:45,250
have all the ingredients to compare the

309
00:14:42,700 --> 00:14:47,080
three algorithms what is surprising is

310
00:14:45,250 --> 00:14:49,810
that actually the azoic 17 paper which

311
00:14:47,080 --> 00:14:51,700
was the worst one on paper turns out to

312
00:14:49,810 --> 00:14:53,829
be the most robust over time over two

313
00:14:51,700 --> 00:14:56,260
years but if you're interested in the

314
00:14:53,830 --> 00:14:57,880
first three months then the NGSS 14

315
00:14:56,260 --> 00:15:01,090
paper should be your choice

316
00:14:57,880 --> 00:15:02,970
this is fully tunable so that's what

317
00:15:01,090 --> 00:15:07,270
allows to do realistic evaluations

318
00:15:02,970 --> 00:15:08,590
practitioners can use it to do to

319
00:15:07,270 --> 00:15:10,660
evaluate new proposed sorry

320
00:15:08,590 --> 00:15:13,530
practitioners can use it to choose among

321
00:15:10,660 --> 00:15:16,689
different options in a realistic setting

322
00:15:13,530 --> 00:15:19,990
experimental setting and researchers can

323
00:15:16,690 --> 00:15:21,670
use it to evaluate new solutions and all

324
00:15:19,990 --> 00:15:23,740
parameters in terms of Tyrrhenian

325
00:15:21,670 --> 00:15:26,050
granularity are can be adjusted

326
00:15:23,740 --> 00:15:28,630
according to needs but in addition to

327
00:15:26,050 --> 00:15:30,880
evaluating performance it can also allow

328
00:15:28,630 --> 00:15:33,700
to evaluate

329
00:15:30,880 --> 00:15:35,650
retraining costs in terms of labeling or

330
00:15:33,700 --> 00:15:37,660
quarantine if we need to reject elements

331
00:15:35,650 --> 00:15:40,000
that that need to be labeled manually at

332
00:15:37,660 --> 00:15:41,319
some point and there are more

333
00:15:40,000 --> 00:15:43,120
experiments in the paper and I invite

334
00:15:41,320 --> 00:15:45,580
you to read that in particular

335
00:15:43,120 --> 00:15:48,550
there is one rejection that is based on

336
00:15:45,580 --> 00:15:53,560
a work that we published a couple of

337
00:15:48,550 --> 00:15:54,969
years ago at Q's annex I want to show

338
00:15:53,560 --> 00:15:57,939
you now just what happens to this

339
00:15:54,970 --> 00:15:59,830
pipeline when - truck is introduced

340
00:15:57,940 --> 00:16:01,270
well first now next with a careful we

341
00:15:59,830 --> 00:16:03,640
also have - right that enforces

342
00:16:01,270 --> 00:16:05,819
constraints III even to the K fold and

343
00:16:03,640 --> 00:16:07,900
forces and does time over evaluations

344
00:16:05,820 --> 00:16:09,880
then you need to pay attention also to

345
00:16:07,900 --> 00:16:11,620
how to cut the data set and now wonders

346
00:16:09,880 --> 00:16:14,610
who enables it for Android and recently

347
00:16:11,620 --> 00:16:17,320
Amber's being released for Windows and

348
00:16:14,610 --> 00:16:18,970
supports this kind of analysis and now

349
00:16:17,320 --> 00:16:21,940
when you compare the performance you can

350
00:16:18,970 --> 00:16:24,850
do shut aut Lebanon cost quantifying

351
00:16:21,940 --> 00:16:30,010
cost and maybe you find out that

352
00:16:24,850 --> 00:16:33,660
actually approach B is the best one in

353
00:16:30,010 --> 00:16:35,620
process of course a drift to conclude

354
00:16:33,660 --> 00:16:37,240
we've shown that actually malware

355
00:16:35,620 --> 00:16:39,730
detection is still an open problem

356
00:16:37,240 --> 00:16:42,220
machine learning did not solve it but

357
00:16:39,730 --> 00:16:44,670
proposed tesseract has a way to do sound

358
00:16:42,220 --> 00:16:47,200
evaluations that take into account

359
00:16:44,670 --> 00:16:48,459
concept if that quantify the impact and

360
00:16:47,200 --> 00:16:51,490
allows fair evaluations of the

361
00:16:48,460 --> 00:16:53,470
classifiers and we are releasing code

362
00:16:51,490 --> 00:16:55,300
data certain features the call has been

363
00:16:53,470 --> 00:16:57,220
implemented at this link which you can

364
00:16:55,300 --> 00:17:00,219
also finally this paper you can just

365
00:16:57,220 --> 00:17:02,890
drop us a line to get access and already

366
00:17:00,220 --> 00:17:09,130
a lot of organizations had access to it

367
00:17:02,890 --> 00:17:11,369
and that's all and I'll be happy to take

368
00:17:09,130 --> 00:17:11,370
any questions

369
00:17:14,619 --> 00:17:21,069
okay thank you fabric for the

370
00:17:17,859 --> 00:17:23,408
beautifully done presentation put a lot

371
00:17:21,069 --> 00:17:25,779
of time into it you're welcome to ask

372
00:17:23,409 --> 00:17:27,130
questions again if you have questions

373
00:17:25,779 --> 00:17:33,370
please state your name and affiliation

374
00:17:27,130 --> 00:17:35,799
first I think I'll begin let's assume

375
00:17:33,370 --> 00:17:38,668
I'm not trying to write a new algorithm

376
00:17:35,799 --> 00:17:41,500
but I'm trying to write an actual

377
00:17:38,669 --> 00:17:44,950
malware antivirus detector which will be

378
00:17:41,500 --> 00:17:48,539
installed on mobile phones so what would

379
00:17:44,950 --> 00:17:50,590
be the best strategy for me as a

380
00:17:48,539 --> 00:17:55,090
antivirus writer based on the

381
00:17:50,590 --> 00:17:58,689
conclusions from your paper so I think

382
00:17:55,090 --> 00:18:03,490
that deploying the machine learning

383
00:17:58,690 --> 00:18:05,890
detection model on the device is a

384
00:18:03,490 --> 00:18:07,419
relative I mean you're talking about

385
00:18:05,890 --> 00:18:09,880
deploying it directly on the device or

386
00:18:07,419 --> 00:18:11,230
developing something that is directly in

387
00:18:09,880 --> 00:18:15,510
the river I want to write the pests

388
00:18:11,230 --> 00:18:23,169
anti-malware software what should I do

389
00:18:15,510 --> 00:18:24,640
so if you're doing the evaluation so if

390
00:18:23,169 --> 00:18:28,330
you want to detect malicious

391
00:18:24,640 --> 00:18:33,220
applications in a device you can use

392
00:18:28,330 --> 00:18:38,168
this kind of evaluation to see how fast

393
00:18:33,220 --> 00:18:40,899
the malware authors are changing their

394
00:18:38,169 --> 00:18:43,360
techniques so you can quantify how much

395
00:18:40,899 --> 00:18:45,399
frequently you need to also retrain the

396
00:18:43,360 --> 00:18:48,549
model there are more experiments about

397
00:18:45,399 --> 00:18:50,469
this in the paper and the idea is that

398
00:18:48,549 --> 00:18:53,080
this tool also allows you to find

399
00:18:50,470 --> 00:18:55,870
trade-offs between different approaches

400
00:18:53,080 --> 00:18:59,199
and different training costs because if

401
00:18:55,870 --> 00:19:00,939
you need to retrain you may need

402
00:18:59,200 --> 00:19:02,740
manpower of people being able to

403
00:19:00,940 --> 00:19:04,149
manually label it so this can support

404
00:19:02,740 --> 00:19:06,460
actually realistic evaluation that

405
00:19:04,149 --> 00:19:10,059
simulate how your detector will perform

406
00:19:06,460 --> 00:19:11,830
in the wild okay thank you

407
00:19:10,059 --> 00:19:15,610
to the dimitra's University of Maryland

408
00:19:11,830 --> 00:19:17,620
great talk I have a question about c2 so

409
00:19:15,610 --> 00:19:20,049
you said that because of this you may

410
00:19:17,620 --> 00:19:21,639
have artifacts you may learn this new

411
00:19:20,049 --> 00:19:26,320
this new method it's a good it's a

412
00:19:21,640 --> 00:19:28,480
robust feature but uh is this temporal

413
00:19:26,320 --> 00:19:30,340
inconsistency the only way that you

414
00:19:28,480 --> 00:19:32,200
these artifacts or aren't there more

415
00:19:30,340 --> 00:19:45,059
insidious ways that that would affect

416
00:19:32,200 --> 00:19:45,059
your your your biases I think that I

417
00:19:46,679 --> 00:19:50,290
think this work is a great advancement

418
00:19:49,150 --> 00:19:52,660
in understanding how to remove

419
00:19:50,290 --> 00:19:54,909
experimental bias but I also believe it

420
00:19:52,660 --> 00:19:56,860
this is not the end of it but III agree

421
00:19:54,910 --> 00:19:58,419
let me rephrase do you think that this

422
00:19:56,860 --> 00:20:00,189
is the most important source of

423
00:19:58,419 --> 00:20:04,540
artifacts compared to the other post

424
00:20:00,190 --> 00:20:07,179
possible sources of artifacts I think it

425
00:20:04,540 --> 00:20:11,320
that is that one is just a sub to one

426
00:20:07,179 --> 00:20:12,669
because it's we were we didn't have time

427
00:20:11,320 --> 00:20:15,909
to do it yet but we were planning also

428
00:20:12,669 --> 00:20:17,710
true and study on understanding when the

429
00:20:15,910 --> 00:20:19,750
classifier is actually learning

430
00:20:17,710 --> 00:20:22,299
artifacts especially if you start

431
00:20:19,750 --> 00:20:26,650
messing up with temporal distemper or

432
00:20:22,299 --> 00:20:28,780
composition of the data set I think that

433
00:20:26,650 --> 00:20:31,120
one is probably just the most tricky one

434
00:20:28,780 --> 00:20:32,530
because I don't remember if we

435
00:20:31,120 --> 00:20:34,178
eventually put something in the paper

436
00:20:32,530 --> 00:20:37,149
but we definitely the experiments in

437
00:20:34,179 --> 00:20:39,160
which we try to enforce c1 and c3 but

438
00:20:37,150 --> 00:20:42,100
not c2 and try to mess up a little bit

439
00:20:39,160 --> 00:20:44,110
with it and performance we're like kind

440
00:20:42,100 --> 00:20:45,549
of dropping but we did not quantify the

441
00:20:44,110 --> 00:20:49,330
impact of the problem so I definitely

442
00:20:45,549 --> 00:20:53,650
think it's maybe not today I think the

443
00:20:49,330 --> 00:20:55,290
major one remains 2 c1 and also c3 in

444
00:20:53,650 --> 00:20:58,720
terms of actual impact on performance

445
00:20:55,290 --> 00:21:00,490
but c2 is the most subtle one which can

446
00:20:58,720 --> 00:21:02,710
change in completely unexpected ways

447
00:21:00,490 --> 00:21:05,520
maybe maybe that the classifier will

448
00:21:02,710 --> 00:21:09,179
still learn something significant but

449
00:21:05,520 --> 00:21:09,179
very brief question

450
00:21:12,120 --> 00:21:17,590
Kenny Oaks speech um the new experiment

451
00:21:15,820 --> 00:21:19,929
with changing the training yes the

452
00:21:17,590 --> 00:21:22,389
percentages in the training yes I'm

453
00:21:19,930 --> 00:21:25,680
sorry can you speak louder um the new

454
00:21:22,390 --> 00:21:29,860
experiment with changing the percent of

455
00:21:25,680 --> 00:21:32,770
versus - in your training medicine in

456
00:21:29,860 --> 00:21:35,889
your testing that's more unit when

457
00:21:32,770 --> 00:21:43,300
you're new try Oh what happens yes yes

458
00:21:35,890 --> 00:21:44,620
so actually it's a nice question and we

459
00:21:43,300 --> 00:21:46,570
are behind schedule

460
00:21:44,620 --> 00:21:47,649
yeah sorry but I can don't you I mean if

461
00:21:46,570 --> 00:21:50,770
you're interested you can read more in

462
00:21:47,650 --> 00:21:52,540
the paper but changing the percentage of

463
00:21:50,770 --> 00:21:53,740
the composition of the data Sabina

464
00:21:52,540 --> 00:21:56,800
training set is fair game

465
00:21:53,740 --> 00:21:59,650
we also devised an algorithm it changes

466
00:21:56,800 --> 00:22:01,750
the sensitivity of the classifier into

467
00:21:59,650 --> 00:22:03,460
retracting one class or the other so

468
00:22:01,750 --> 00:22:04,180
yeah that is fair game and there is more

469
00:22:03,460 --> 00:22:06,990
in the paper

470
00:22:04,180 --> 00:22:09,990
okay let's thank Fabio again first

471
00:22:06,990 --> 00:22:09,990
presentation

