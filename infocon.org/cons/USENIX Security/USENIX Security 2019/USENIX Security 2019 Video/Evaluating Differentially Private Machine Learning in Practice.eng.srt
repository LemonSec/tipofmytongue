1
00:00:10,670 --> 00:00:14,969
thank you everyone for staying till the

2
00:00:12,870 --> 00:00:18,660
end I hope you're all awake at least

3
00:00:14,969 --> 00:00:20,189
until I have start talking so I am

4
00:00:18,660 --> 00:00:21,390
baggage gentleman and I'll be talking

5
00:00:20,189 --> 00:00:22,590
about her work on evaluating

6
00:00:21,390 --> 00:00:26,369
differentiability with machine learning

7
00:00:22,590 --> 00:00:29,039
in practice so we consider this scenario

8
00:00:26,369 --> 00:00:30,630
where the adversary has access to a

9
00:00:29,039 --> 00:00:32,250
machine learning model and tries to

10
00:00:30,630 --> 00:00:35,129
infer information about the training

11
00:00:32,250 --> 00:00:36,360
data there have been many works which

12
00:00:35,129 --> 00:00:40,019
try to defend against these kind of

13
00:00:36,360 --> 00:00:41,670
attacks our objective here is to you

14
00:00:40,019 --> 00:00:42,899
know evaluate these attacks in terms of

15
00:00:41,670 --> 00:00:46,680
you know privacy because they have

16
00:00:42,899 --> 00:00:49,170
empirically so I'll just begin with the

17
00:00:46,680 --> 00:00:52,500
result highlights before delving deep

18
00:00:49,170 --> 00:00:53,640
into the talk so we all know that we can

19
00:00:52,500 --> 00:00:56,670
tune the differential recessive

20
00:00:53,640 --> 00:00:59,629
parameter Epsilon and you can have an

21
00:00:56,670 --> 00:01:03,180
acceptable utility player as a trade-off

22
00:00:59,629 --> 00:01:06,149
so here we train a neural network model

23
00:01:03,180 --> 00:01:08,220
for size 100 data set which is 100 class

24
00:01:06,149 --> 00:01:11,700
classification test and we see that

25
00:01:08,220 --> 00:01:15,000
until 100 epsilon value we don't get any

26
00:01:11,700 --> 00:01:16,640
model utility and and beyond that we the

27
00:01:15,000 --> 00:01:19,320
model search will and something useful

28
00:01:16,640 --> 00:01:21,780
but from the definition of differential

29
00:01:19,320 --> 00:01:23,369
privacy the theoretically the

30
00:01:21,780 --> 00:01:25,770
theoretical upper bound on the privacy

31
00:01:23,369 --> 00:01:29,430
leakage is guaranteed only from between

32
00:01:25,770 --> 00:01:31,560
epsilon of 0 to 1 so in this range there

33
00:01:29,430 --> 00:01:35,159
doesn't seem to be any useful learning

34
00:01:31,560 --> 00:01:36,960
going on so recent advances such as any

35
00:01:35,159 --> 00:01:40,740
differential privacy try to bridge this

36
00:01:36,960 --> 00:01:43,679
gap they achieve better accuracy but

37
00:01:40,740 --> 00:01:45,360
even for them and in this range where we

38
00:01:43,680 --> 00:01:47,729
have a theoretical guarantee there is no

39
00:01:45,360 --> 00:01:51,450
useful learning going on so we went

40
00:01:47,729 --> 00:01:54,810
ahead and empirically and I suppose

41
00:01:51,450 --> 00:01:57,030
leakage using actual practical attacks

42
00:01:54,810 --> 00:01:58,290
and turned out there is a huge gap

43
00:01:57,030 --> 00:02:01,590
between you know what can be also

44
00:01:58,290 --> 00:02:05,070
theoretically and what these MPL attacks

45
00:02:01,590 --> 00:02:08,390
achieve in general as the model starts

46
00:02:05,070 --> 00:02:11,519
we'll learn more the leakage increases

47
00:02:08,389 --> 00:02:13,920
so now for the rest of the talk I'll

48
00:02:11,519 --> 00:02:15,840
first begin by giving a brief background

49
00:02:13,920 --> 00:02:17,280
about how to apply difference privacy

50
00:02:15,840 --> 00:02:19,709
for these machine learning algorithms

51
00:02:17,280 --> 00:02:21,319
and then I'll talk about our expensive

52
00:02:19,709 --> 00:02:23,209
evaluation where we tried

53
00:02:21,319 --> 00:02:27,619
well it depress the leakage of these

54
00:02:23,209 --> 00:02:29,540
mechanisms so this is the most popular

55
00:02:27,620 --> 00:02:31,849
flavor of training a machine learning

56
00:02:29,540 --> 00:02:33,980
model where you define an objective

57
00:02:31,849 --> 00:02:38,810
function and you iterative Li update the

58
00:02:33,980 --> 00:02:40,310
model so here first stage is you could

59
00:02:38,810 --> 00:02:41,930
add the noise to the objective function

60
00:02:40,310 --> 00:02:45,379
which is political to the objective

61
00:02:41,930 --> 00:02:46,939
perturbation or you could either add the

62
00:02:45,379 --> 00:02:49,220
noise to the gradients which is called

63
00:02:46,939 --> 00:02:51,590
grading perturbation finally you could

64
00:02:49,220 --> 00:02:53,269
also add the noise to the model that's

65
00:02:51,590 --> 00:02:56,420
learnt at the end which is called output

66
00:02:53,269 --> 00:02:58,609
perturbation since the inception of

67
00:02:56,420 --> 00:03:02,569
differential pairs in 2006 there have

68
00:02:58,609 --> 00:03:04,340
been a lot of work which use either

69
00:03:02,569 --> 00:03:06,500
objective perturbation or output

70
00:03:04,340 --> 00:03:08,599
perturbation to for binary

71
00:03:06,500 --> 00:03:11,689
classification tasks using erm

72
00:03:08,599 --> 00:03:14,629
algorithms and these all achieve pretty

73
00:03:11,689 --> 00:03:16,129
good utility for aztlán values less than

74
00:03:14,629 --> 00:03:18,918
1 which is good because we have

75
00:03:16,129 --> 00:03:19,310
theoretical guarantee here but for deep

76
00:03:18,919 --> 00:03:21,319
learning

77
00:03:19,310 --> 00:03:23,930
turns out output perturbation and object

78
00:03:21,319 --> 00:03:26,750
a partition are not applicable the

79
00:03:23,930 --> 00:03:29,269
reason is that these methods require

80
00:03:26,750 --> 00:03:31,970
some kind of a bound on the sensitivity

81
00:03:29,269 --> 00:03:33,680
and we don't know how to do this and do

82
00:03:31,970 --> 00:03:35,690
this for at least complex algorithms

83
00:03:33,680 --> 00:03:37,549
like deep learning so we are just left

84
00:03:35,690 --> 00:03:39,590
with creating perturbation and the

85
00:03:37,549 --> 00:03:41,720
initial attempts for using radium

86
00:03:39,590 --> 00:03:44,569
perturbation towards deep learning ended

87
00:03:41,720 --> 00:03:45,620
up consuming a lot of budget or to the

88
00:03:44,569 --> 00:03:47,899
order of three hundred thousand

89
00:03:45,620 --> 00:03:50,540
something like that which doesn't seem

90
00:03:47,900 --> 00:03:52,400
good so the main problem here is

91
00:03:50,540 --> 00:03:54,668
creating perturbation at each iteration

92
00:03:52,400 --> 00:03:56,810
you need to sample noise and add it so

93
00:03:54,669 --> 00:04:00,650
the knife composition of differential

94
00:03:56,810 --> 00:04:02,870
privacy ends up having linear relation

95
00:04:00,650 --> 00:04:05,569
with the number of iterations for the

96
00:04:02,870 --> 00:04:07,639
pricing budget recent advancements like

97
00:04:05,569 --> 00:04:10,638
the rainy differential I see in the CCD

98
00:04:07,639 --> 00:04:12,470
PN all they try to reduce this by a

99
00:04:10,639 --> 00:04:15,470
factor of square root of T and the

100
00:04:12,470 --> 00:04:17,509
epsilon term but at in doing so they

101
00:04:15,470 --> 00:04:21,168
increase the failure probability by a

102
00:04:17,509 --> 00:04:23,750
small factor Delta so using these

103
00:04:21,168 --> 00:04:26,210
methods the recent works ended up

104
00:04:23,750 --> 00:04:28,990
training d-plan networks with a very

105
00:04:26,210 --> 00:04:31,638
good privacy with very good utility and

106
00:04:28,990 --> 00:04:34,940
consume just listen on of like three or

107
00:04:31,639 --> 00:04:39,350
four to put things in the perspective

108
00:04:34,940 --> 00:04:40,820
Salon of threes lies here in our plot it

109
00:04:39,350 --> 00:04:42,770
still doesn't have good theoretical

110
00:04:40,820 --> 00:04:45,500
guarantee because it's still greater

111
00:04:42,770 --> 00:04:48,620
than one but this is where we stand for

112
00:04:45,500 --> 00:04:50,960
now so now moving on to the experiments

113
00:04:48,620 --> 00:04:53,240
we train large simplification in your

114
00:04:50,960 --> 00:04:55,880
relative models for 200 class

115
00:04:53,240 --> 00:04:57,500
classification tasks one on the side 400

116
00:04:55,880 --> 00:05:00,290
a dozen and the other on the purchase

117
00:04:57,500 --> 00:05:02,690
and the data set and we evaluate the two

118
00:05:00,290 --> 00:05:04,520
matrix accuracy laws and privacy legates

119
00:05:02,690 --> 00:05:06,620
you just see the utility and privacy

120
00:05:04,520 --> 00:05:09,020
trade-offs due to time constraint I'll

121
00:05:06,620 --> 00:05:10,610
just go into the details of the results

122
00:05:09,020 --> 00:05:13,060
of size 100 dataset you could refer to

123
00:05:10,610 --> 00:05:15,920
the paper for purchase and the dataset

124
00:05:13,060 --> 00:05:18,380
so what we do is we split the data set

125
00:05:15,920 --> 00:05:20,200
into training and test sets we train the

126
00:05:18,380 --> 00:05:22,430
model or the training set and we

127
00:05:20,200 --> 00:05:24,950
evaluate the accuracy loss over the test

128
00:05:22,430 --> 00:05:26,930
set and the accuracy loss is basically

129
00:05:24,950 --> 00:05:30,020
relative to the private non private

130
00:05:26,930 --> 00:05:32,510
model so accuracy loss value of zero

131
00:05:30,020 --> 00:05:33,950
means the model is as good as a

132
00:05:32,510 --> 00:05:36,710
non-parent model in terms of accuracy

133
00:05:33,950 --> 00:05:40,219
and accuracy loss of 1 means it's not

134
00:05:36,710 --> 00:05:41,840
learning anything at all so here the

135
00:05:40,220 --> 00:05:44,390
results for logistic regression trained

136
00:05:41,840 --> 00:05:46,219
on psy 400 dataset as you can see

137
00:05:44,390 --> 00:05:48,740
already be as the most improved

138
00:05:46,220 --> 00:05:50,750
composition and hence it overall they

139
00:05:48,740 --> 00:05:53,680
have some values it has a much better

140
00:05:50,750 --> 00:05:57,440
accuracy than the in I've composition

141
00:05:53,680 --> 00:06:00,050
you could actually if you see you fix a

142
00:05:57,440 --> 00:06:02,030
privacy budget then RDP adds must listen

143
00:06:00,050 --> 00:06:04,610
nice than the knife composition or in

144
00:06:02,030 --> 00:06:06,409
other words if you restrict the amount

145
00:06:04,610 --> 00:06:09,919
of noise these methods are allowed to

146
00:06:06,410 --> 00:06:11,300
add then RDP consumes an order of

147
00:06:09,919 --> 00:06:14,060
magnitude less of privacy budget than

148
00:06:11,300 --> 00:06:18,350
knife composition and this is what this

149
00:06:14,060 --> 00:06:21,470
was supposed to do as highlighted RDP

150
00:06:18,350 --> 00:06:23,540
has a accuracy of 0.1 for epsilon your

151
00:06:21,470 --> 00:06:26,030
founder and the same accuracy is

152
00:06:23,540 --> 00:06:30,350
achieved by epsilon it by an AB

153
00:06:26,030 --> 00:06:34,609
compression epsilon 500 now we evaluate

154
00:06:30,350 --> 00:06:36,500
the piracy leakage for this we consider

155
00:06:34,610 --> 00:06:38,600
adversity which tries to do membership

156
00:06:36,500 --> 00:06:41,900
inference attacks the first attack is

157
00:06:38,600 --> 00:06:46,220
that of Ray Shoukri at all here we first

158
00:06:41,900 --> 00:06:48,310
in the shadow models and then we train

159
00:06:46,220 --> 00:06:49,930
and attack model which is notarized

160
00:06:48,310 --> 00:06:51,250
from the shadow models and you know

161
00:06:49,930 --> 00:06:53,530
tries to predict whether the particular

162
00:06:51,250 --> 00:06:55,840
record was a member or not and the

163
00:06:53,530 --> 00:06:58,659
second dag that retain is that uh vo

164
00:06:55,840 --> 00:07:00,549
meter here the attacker has access to

165
00:06:58,660 --> 00:07:02,470
the model and also knows some extra

166
00:07:00,550 --> 00:07:05,500
auxiliary forces such as the expected

167
00:07:02,470 --> 00:07:07,180
training loss of the targeted model so

168
00:07:05,500 --> 00:07:09,220
using this the privacy leakage is

169
00:07:07,180 --> 00:07:10,450
calculated as the difference between the

170
00:07:09,220 --> 00:07:12,310
true positive rate and the false

171
00:07:10,450 --> 00:07:15,370
positive rate of the adversary

172
00:07:12,310 --> 00:07:17,320
performing the memberships attacks so

173
00:07:15,370 --> 00:07:18,400
here are the results for logistic

174
00:07:17,320 --> 00:07:20,770
regression

175
00:07:18,400 --> 00:07:23,620
on 700 data set again with privacy

176
00:07:20,770 --> 00:07:25,299
leakage again we see the same trend RDP

177
00:07:23,620 --> 00:07:28,530
since it's at the least amount of noise

178
00:07:25,300 --> 00:07:32,050
it tends to increase the privacy leakage

179
00:07:28,530 --> 00:07:34,179
again for epsilon value of 10 RDP has a

180
00:07:32,050 --> 00:07:37,750
point zero 6 leakage he doesn't seem too

181
00:07:34,180 --> 00:07:40,980
high at this point the accuracy was like

182
00:07:37,750 --> 00:07:43,720
close to point 1 accuracy loss was 0.1

183
00:07:40,980 --> 00:07:45,520
so we also measure the positive

184
00:07:43,720 --> 00:07:47,440
predictive value which is basically

185
00:07:45,520 --> 00:07:48,820
gives off all the predictions made by

186
00:07:47,440 --> 00:07:51,310
the membership predictions made by the

187
00:07:48,820 --> 00:07:55,120
adversary what fraction of them were

188
00:07:51,310 --> 00:07:57,670
truly the correct members so the base

189
00:07:55,120 --> 00:08:01,090
rate of 0.5 PPV would mean it's a

190
00:07:57,670 --> 00:08:04,180
completely random guessing so 0.55 even

191
00:08:01,090 --> 00:08:05,560
for epsilon value of 1000 means the

192
00:08:04,180 --> 00:08:08,830
pricing mechanism doing really build

193
00:08:05,560 --> 00:08:11,470
really great in this task but turns out

194
00:08:08,830 --> 00:08:14,430
even the non private model has a PPO of

195
00:08:11,470 --> 00:08:17,080
0.56 so it means at least in this test

196
00:08:14,430 --> 00:08:22,300
even the non priority models are private

197
00:08:17,080 --> 00:08:24,729
enough so the logic simulation that we

198
00:08:22,300 --> 00:08:27,130
had it had just 5,000 trainable

199
00:08:24,730 --> 00:08:29,020
parameters so what we did was we had a

200
00:08:27,130 --> 00:08:30,820
two layer neural network which has more

201
00:08:29,020 --> 00:08:33,250
than like 100,000 training parameters

202
00:08:30,820 --> 00:08:36,819
and hence it has a larger capacity so we

203
00:08:33,250 --> 00:08:38,860
believe it should be more and to give

204
00:08:36,820 --> 00:08:40,630
perspective the recent state-of-the-art

205
00:08:38,860 --> 00:08:42,190
deep learning models are much much

206
00:08:40,630 --> 00:08:44,590
bigger than this is just like 100

207
00:08:42,190 --> 00:08:46,150
parameters 100,000 parameters the

208
00:08:44,590 --> 00:08:47,920
state-of-the-art might have millions of

209
00:08:46,150 --> 00:08:51,550
millions of parameters and then we

210
00:08:47,920 --> 00:08:54,219
believe they will leak even more so here

211
00:08:51,550 --> 00:08:58,390
are the accuracy loss results for neural

212
00:08:54,220 --> 00:09:00,760
network model again our DP at epsilon of

213
00:08:58,390 --> 00:09:02,170
10 has almost 50 percent accuracy loss

214
00:09:00,760 --> 00:09:05,110
this is pretty high

215
00:09:02,170 --> 00:09:07,540
that's the same as naive combustion for

216
00:09:05,110 --> 00:09:10,210
epsilon 500 and none of these models

217
00:09:07,540 --> 00:09:13,719
even for epsilon thousand reaches a zero

218
00:09:10,210 --> 00:09:15,400
accuracy laws the best that I already be

219
00:09:13,720 --> 00:09:19,720
does is like point to Phi X accuracy

220
00:09:15,400 --> 00:09:23,829
loss so we went ahead and measured the

221
00:09:19,720 --> 00:09:25,270
privacy leakage here for epsilon of 10

222
00:09:23,830 --> 00:09:27,040
RDP has a leakage of zero point zero

223
00:09:25,270 --> 00:09:28,780
seven which seems pretty small but

224
00:09:27,040 --> 00:09:30,550
remember at this point the accuracy loss

225
00:09:28,780 --> 00:09:34,329
also was like 50 percent which is too

226
00:09:30,550 --> 00:09:36,910
high but for higher epsilon values

227
00:09:34,330 --> 00:09:41,050
turns out the pewter is as high as

228
00:09:36,910 --> 00:09:43,300
points on four which is this is a lot to

229
00:09:41,050 --> 00:09:45,670
put things in perspective on private

230
00:09:43,300 --> 00:09:47,439
model here has a point nine four PP

231
00:09:45,670 --> 00:09:50,199
which is like super high which means

232
00:09:47,440 --> 00:09:52,500
like the adversary has a very high

233
00:09:50,200 --> 00:09:56,680
confidence of in operating every member

234
00:09:52,500 --> 00:09:58,360
clearly from the non private model so

235
00:09:56,680 --> 00:10:01,540
now we know that there is a privacy

236
00:09:58,360 --> 00:10:03,760
leakage so we want to understand is this

237
00:10:01,540 --> 00:10:05,740
leakage due to the data or is it just

238
00:10:03,760 --> 00:10:09,360
due to the randomness that's present in

239
00:10:05,740 --> 00:10:12,370
the data so for this what we did was we

240
00:10:09,360 --> 00:10:15,940
trained the RDP model with its known of

241
00:10:12,370 --> 00:10:17,880
1000 and saw that of all the almost

242
00:10:15,940 --> 00:10:21,340
8,000 predictions made by the adversary

243
00:10:17,880 --> 00:10:22,840
6000 or something were like crew members

244
00:10:21,340 --> 00:10:23,380
and they were like to some false

245
00:10:22,840 --> 00:10:26,050
positive

246
00:10:23,380 --> 00:10:29,610
so the PPV was like 0.74 as we saw

247
00:10:26,050 --> 00:10:33,280
earlier and we ran the algorithm again

248
00:10:29,610 --> 00:10:35,470
and this time again we had an a PP of

249
00:10:33,280 --> 00:10:36,850
0.14 but we are interested the

250
00:10:35,470 --> 00:10:39,670
intersection right so at the

251
00:10:36,850 --> 00:10:43,990
intersection turns out the PPV increases

252
00:10:39,670 --> 00:10:46,240
from 0.7 for 2.80 which means there's

253
00:10:43,990 --> 00:10:48,070
something going on so the leakage is due

254
00:10:46,240 --> 00:10:50,350
to the data and not not due to the time

255
00:10:48,070 --> 00:10:52,000
numbers if it was completely random we

256
00:10:50,350 --> 00:10:53,860
wouldn't expect even the intersection I

257
00:10:52,000 --> 00:10:55,930
know to a vivid expert the intersection

258
00:10:53,860 --> 00:10:57,220
to have like same PPP of points 1 4 but

259
00:10:55,930 --> 00:10:58,689
since it's increasing we know that

260
00:10:57,220 --> 00:10:59,650
something is going on it's something

261
00:10:58,690 --> 00:11:02,890
about the data that's

262
00:10:59,650 --> 00:11:05,350
revealing this so we went ahead and did

263
00:11:02,890 --> 00:11:08,140
intersection of more and more models up

264
00:11:05,350 --> 00:11:09,810
till five so we see that as we do more

265
00:11:08,140 --> 00:11:13,030
and more intersection of these models

266
00:11:09,810 --> 00:11:16,150
the PPV keeps on increasing from 0.5 all

267
00:11:13,030 --> 00:11:18,579
the way to 0.82 2 so we clearly know

268
00:11:16,150 --> 00:11:21,760
there's something about the data which

269
00:11:18,580 --> 00:11:25,900
is making a movement so this is the main

270
00:11:21,760 --> 00:11:30,280
takeaway here so to wrap everything is

271
00:11:25,900 --> 00:11:32,709
up for logical relation model even the

272
00:11:30,280 --> 00:11:35,770
non profit model was secure enough so we

273
00:11:32,710 --> 00:11:37,540
didn't have any price leakage but for

274
00:11:35,770 --> 00:11:39,670
the neural network model there was a

275
00:11:37,540 --> 00:11:42,400
huge privacy leakage even for non

276
00:11:39,670 --> 00:11:45,550
private model and for the private models

277
00:11:42,400 --> 00:11:46,660
at least had reduced try to reduce if I

278
00:11:45,550 --> 00:11:48,849
was a Likud but still there was a

279
00:11:46,660 --> 00:11:51,160
significant amount of leakage there so

280
00:11:48,850 --> 00:11:53,760
privacy doesn't come for free at least

281
00:11:51,160 --> 00:11:56,230
not in all the cases and the most

282
00:11:53,760 --> 00:11:57,939
significant takeaway that we saw across

283
00:11:56,230 --> 00:11:59,560
all the settings is like there's a huge

284
00:11:57,940 --> 00:12:01,450
gap between the theoretical upper bound

285
00:11:59,560 --> 00:12:05,229
on pair silicates and what we observed

286
00:12:01,450 --> 00:12:07,240
in practice so we in principle want to

287
00:12:05,230 --> 00:12:09,760
bridge this gap like we we know there

288
00:12:07,240 --> 00:12:12,010
that they are definitely attached which

289
00:12:09,760 --> 00:12:14,110
might be much more capable than what we

290
00:12:12,010 --> 00:12:16,750
have in the current state of their

291
00:12:14,110 --> 00:12:18,970
attacks so so this is the final

292
00:12:16,750 --> 00:12:21,430
conclusion that this is kind of a

293
00:12:18,970 --> 00:12:24,670
takeaway what the research should be

294
00:12:21,430 --> 00:12:26,439
going towards so with this I end my talk

295
00:12:24,670 --> 00:12:28,530
and I'm ready for the questions thank

296
00:12:26,440 --> 00:12:28,530
you

297
00:12:31,370 --> 00:12:35,700
we've got plenty of times for questions

298
00:12:33,480 --> 00:12:37,620
if people want to ask more differential

299
00:12:35,700 --> 00:12:40,160
privacy machine learning deep learning

300
00:12:37,620 --> 00:12:40,160
things

301
00:12:45,690 --> 00:12:54,690
here's your chance I think I made

302
00:12:50,250 --> 00:12:57,170
evidence leap do you have anything more

303
00:12:54,690 --> 00:13:01,170
you want to add what are you doing next

304
00:12:57,170 --> 00:13:02,400
so we are next as in the research or

305
00:13:01,170 --> 00:13:07,140
what I'm gonna do after like five

306
00:13:02,400 --> 00:13:08,819
minutes or so yeah in terms of research

307
00:13:07,140 --> 00:13:11,880
we are actually looking into why these

308
00:13:08,820 --> 00:13:13,140
we still don't know why these like we

309
00:13:11,880 --> 00:13:14,250
know that there's a leakage there are

310
00:13:13,140 --> 00:13:16,020
certain numbers which are being they

311
00:13:14,250 --> 00:13:16,410
will again and again but we don't know

312
00:13:16,020 --> 00:13:17,970
why

313
00:13:16,410 --> 00:13:20,339
so we are looking into that by part

314
00:13:17,970 --> 00:13:21,810
we're seeing like recent works have

315
00:13:20,340 --> 00:13:23,820
suggested that there might be something

316
00:13:21,810 --> 00:13:25,619
like fairness issue or something which

317
00:13:23,820 --> 00:13:27,840
is causing certain members more in over

318
00:13:25,620 --> 00:13:31,250
level they might be outliers so we are

319
00:13:27,840 --> 00:13:31,250
just looking to those things so that

320
00:13:31,370 --> 00:13:38,640
yeah you've got a question possible so

321
00:13:36,660 --> 00:13:40,500
the neural network model had about a

322
00:13:38,640 --> 00:13:43,319
hundred thousand parameters did you look

323
00:13:40,500 --> 00:13:44,820
at all into how this changed if you

324
00:13:43,320 --> 00:13:47,880
varied the number of parameters if you

325
00:13:44,820 --> 00:13:49,200
use ten thousand or million oh yeah

326
00:13:47,880 --> 00:13:50,460
that's a good question if you increase

327
00:13:49,200 --> 00:13:52,620
the parameters then of course the

328
00:13:50,460 --> 00:13:55,200
capacity is increasing we would

329
00:13:52,620 --> 00:13:56,670
definitely expect greater privacy laws

330
00:13:55,200 --> 00:13:58,260
but we didn't do that in the experiment

331
00:13:56,670 --> 00:14:00,900
because you know we are constrained by

332
00:13:58,260 --> 00:14:02,189
the amount of time we could spend on the

333
00:14:00,900 --> 00:14:04,110
experience of because we are repeating

334
00:14:02,190 --> 00:14:07,650
this experiment like 400 times for one

335
00:14:04,110 --> 00:14:11,760
dataset and each like takes a pacer

336
00:14:07,650 --> 00:14:13,500
several hours or so so but yeah we have

337
00:14:11,760 --> 00:14:16,439
seen in practice also people have also

338
00:14:13,500 --> 00:14:18,690
done this kind of evaluation and with

339
00:14:16,440 --> 00:14:20,730
larger models there are other problems

340
00:14:18,690 --> 00:14:23,490
also with neonatal as we would have seen

341
00:14:20,730 --> 00:14:25,590
you had heard the previous talk secret

342
00:14:23,490 --> 00:14:27,360
sharer there are some more problems like

343
00:14:25,590 --> 00:14:30,120
memorization and all going on when you

344
00:14:27,360 --> 00:14:31,680
have very large models so not just a

345
00:14:30,120 --> 00:14:34,760
member to infants I there even more

346
00:14:31,680 --> 00:14:39,170
problems if you have a larger motor so

347
00:14:34,760 --> 00:14:39,170
thank you very cool research by the way

348
00:14:39,920 --> 00:14:45,790
last chance

349
00:14:43,130 --> 00:14:50,130
okay let's thank the speaker once more

350
00:14:45,790 --> 00:14:50,130
[Applause]

