1
00:00:10,620 --> 00:00:16,369
hi so I recently found this nice little

2
00:00:15,280 --> 00:00:19,910
survey in my

3
00:00:16,370 --> 00:00:22,010
time line and it basically describes the

4
00:00:19,910 --> 00:00:26,960
core of the problem we wanted to try to

5
00:00:22,010 --> 00:00:28,880
solve in his paper and it says I'm a

6
00:00:26,960 --> 00:00:31,220
programmer looking for a solution on

7
00:00:28,880 --> 00:00:35,150
Stack Overflow to paste into my project

8
00:00:31,220 --> 00:00:37,160
and it has thousands of retweets and ten

9
00:00:35,150 --> 00:00:39,559
thousands of likes so it seems that you

10
00:00:37,160 --> 00:00:43,580
could consider this as common behavior

11
00:00:39,559 --> 00:00:46,010
and we also had a lot of studies over

12
00:00:43,580 --> 00:00:47,180
the last year's that basically looked at

13
00:00:46,010 --> 00:00:48,760
this a little bit more scientifically

14
00:00:47,180 --> 00:00:51,860
and they confirmed this

15
00:00:48,760 --> 00:00:54,280
so as this might speak for the high

16
00:00:51,860 --> 00:00:56,570
usability and utility of Stack Overflow

17
00:00:54,280 --> 00:01:01,730
it also comes with a high risk for

18
00:00:56,570 --> 00:01:03,860
application security and that is when it

19
00:01:01,730 --> 00:01:05,899
comes to security related questions or

20
00:01:03,860 --> 00:01:10,280
usability issues around cryptographic

21
00:01:05,900 --> 00:01:12,650
api's so for instance this Tiger flow

22
00:01:10,280 --> 00:01:16,010
question is about one of the biggest

23
00:01:12,650 --> 00:01:18,860
issues in android how can you accept a

24
00:01:16,010 --> 00:01:21,230
certificate during TLS handshake that is

25
00:01:18,860 --> 00:01:25,370
not part of the custom Android trusted

26
00:01:21,230 --> 00:01:27,860
store so in this this question showed up

27
00:01:25,370 --> 00:01:30,620
on the stack overflow quite a lot had

28
00:01:27,860 --> 00:01:36,680
millions of view counts and thousand and

29
00:01:30,620 --> 00:01:39,680
a lot of uploads and so on and a typical

30
00:01:36,680 --> 00:01:41,960
typical question do this answer was this

31
00:01:39,680 --> 00:01:45,440
null verifier or some kind of variation

32
00:01:41,960 --> 00:01:49,429
of it which basically turns off

33
00:01:45,440 --> 00:01:51,440
certificate verification however it

34
00:01:49,430 --> 00:01:53,780
technically solved the initial problem

35
00:01:51,440 --> 00:01:56,420
so people were quite happy with this

36
00:01:53,780 --> 00:02:02,210
answer and it was you know ups voted a

37
00:01:56,420 --> 00:02:04,250
lot so two years ago we had an Auckland

38
00:02:02,210 --> 00:02:06,949
paper where we basically have shown that

39
00:02:04,250 --> 00:02:08,269
people indeed reuse insecure code from

40
00:02:06,950 --> 00:02:12,170
Stack Overflow and Android applications

41
00:02:08,269 --> 00:02:15,050
from Google Play and those included apps

42
00:02:12,170 --> 00:02:17,420
with high profile apps with billions of

43
00:02:15,050 --> 00:02:21,890
downloads and apps from security

44
00:02:17,420 --> 00:02:23,780
sensitive categories and one of our

45
00:02:21,890 --> 00:02:26,119
co-authors had another paper where they

46
00:02:23,780 --> 00:02:29,510
basically were able to show that it's

47
00:02:26,120 --> 00:02:30,230
possible to attack apps based on these

48
00:02:29,510 --> 00:02:32,780
insecure code

49
00:02:30,230 --> 00:02:35,000
snippets from Stack Overflow to steal

50
00:02:32,780 --> 00:02:40,040
credentials and critical numbers and

51
00:02:35,000 --> 00:02:42,739
other private data so we had another

52
00:02:40,040 --> 00:02:45,140
icse paper this year which basically

53
00:02:42,739 --> 00:02:47,060
showed that all the market signals on

54
00:02:45,140 --> 00:02:49,279
Stack Overflow basically point towards

55
00:02:47,060 --> 00:02:51,620
the wrong direction as they you know

56
00:02:49,280 --> 00:02:55,879
hand in secure solutions on a plate

57
00:02:51,620 --> 00:02:58,480
basically so it seems that it's by the

58
00:02:55,879 --> 00:03:01,280
problem so what to do about it

59
00:02:58,480 --> 00:03:03,399
so over the last year's we had a lot of

60
00:03:01,280 --> 00:03:05,780
really nice studies that basically

61
00:03:03,400 --> 00:03:08,680
investigated different forms of security

62
00:03:05,780 --> 00:03:12,079
advice for software developers and

63
00:03:08,680 --> 00:03:13,459
compared later with stack overflow so

64
00:03:12,079 --> 00:03:16,209
for instance books and formal

65
00:03:13,459 --> 00:03:20,349
documentation study code analysis tools

66
00:03:16,209 --> 00:03:23,480
and also simplified cryptographic api's

67
00:03:20,349 --> 00:03:26,000
so as all of these approaches are helped

68
00:03:23,480 --> 00:03:29,450
in you know producing more secure

69
00:03:26,000 --> 00:03:32,629
solutions they produce less functional

70
00:03:29,450 --> 00:03:36,078
solutions due to some usability issue

71
00:03:32,629 --> 00:03:40,268
and the interesting thing for us was

72
00:03:36,079 --> 00:03:43,130
that from these studies was that

73
00:03:40,269 --> 00:03:44,510
whenever people you know encountered a

74
00:03:43,130 --> 00:03:47,418
usability issue with one of these

75
00:03:44,510 --> 00:03:51,638
approaches they went for you know coach

76
00:03:47,419 --> 00:03:54,349
helping on Stack Overflow once again and

77
00:03:51,639 --> 00:03:56,150
you know this behavior kind of goes in

78
00:03:54,349 --> 00:03:58,518
line with a quote of Richard Thaler

79
00:03:56,150 --> 00:04:01,549
who's the founder of the Nash here II

80
00:03:58,519 --> 00:04:04,430
and it says first never underestimate

81
00:04:01,549 --> 00:04:07,910
the power of inertia but also second

82
00:04:04,430 --> 00:04:11,230
that power can be harnessed and for us

83
00:04:07,910 --> 00:04:14,629
that that sounds sounded quite nice so

84
00:04:11,230 --> 00:04:17,209
it inspired us for our main approach

85
00:04:14,629 --> 00:04:20,029
which was you know maybe it's possible

86
00:04:17,209 --> 00:04:23,419
to kind of exploit this copy-and-paste

87
00:04:20,029 --> 00:04:25,299
behavior and also the apparently

88
00:04:23,419 --> 00:04:27,889
unbeatable usability of Stack Overflow

89
00:04:25,300 --> 00:04:33,440
to you know how people getting security

90
00:04:27,889 --> 00:04:36,380
right but how to do that so one of the

91
00:04:33,440 --> 00:04:40,070
most important findings to answer this

92
00:04:36,380 --> 00:04:42,200
question findings in our paper to answer

93
00:04:40,070 --> 00:04:44,320
this question was that we found out that

94
00:04:42,200 --> 00:04:48,140
um Stack Overflow

95
00:04:44,320 --> 00:04:50,290
for a similar f4 we had psychoville flow

96
00:04:48,140 --> 00:04:53,180
provides similar insecure code snippets

97
00:04:50,290 --> 00:04:57,620
for almost all the VP insecure code

98
00:04:53,180 --> 00:05:01,070
snippets so you might be able to look at

99
00:04:57,620 --> 00:05:04,690
this you might be you might look at this

100
00:05:01,070 --> 00:05:10,940
as a you know decision-making problem

101
00:05:04,690 --> 00:05:14,380
and that and that's when you know this

102
00:05:10,940 --> 00:05:17,660
is such the not Seri comes into play and

103
00:05:14,380 --> 00:05:20,120
so the Nash Theory basically tries to

104
00:05:17,660 --> 00:05:23,180
nudge people towards better decisions

105
00:05:20,120 --> 00:05:25,580
without restricting their options and

106
00:05:23,180 --> 00:05:27,380
here on the right side you can see an

107
00:05:25,580 --> 00:05:30,260
example for this there are two options

108
00:05:27,380 --> 00:05:32,210
you know for getting upstairs using the

109
00:05:30,260 --> 00:05:34,640
stairs is the better option in terms of

110
00:05:32,210 --> 00:05:38,989
health but people you know tend to use

111
00:05:34,640 --> 00:05:41,090
the escalator however if you come up

112
00:05:38,990 --> 00:05:43,000
with a new choice architecture with the

113
00:05:41,090 --> 00:05:45,700
stairs now look like a piano keyboard

114
00:05:43,000 --> 00:05:48,860
that makes sounds when you walk up says

115
00:05:45,700 --> 00:05:54,890
people you know start to favor this

116
00:05:48,860 --> 00:05:56,690
option so our first goal was to come up

117
00:05:54,890 --> 00:06:01,190
with a new choice architecture for a

118
00:05:56,690 --> 00:06:04,280
stack overflow that not just people from

119
00:06:01,190 --> 00:06:06,950
you know from yeah that not just people

120
00:06:04,280 --> 00:06:09,619
towards reusing code examples that

121
00:06:06,950 --> 00:06:12,530
provide stronger security without you

122
00:06:09,620 --> 00:06:18,200
know without interfering with the

123
00:06:12,530 --> 00:06:19,909
usability of psychoville flow so and to

124
00:06:18,200 --> 00:06:22,280
be able to do that the first thing we

125
00:06:19,910 --> 00:06:24,770
had to do was to find these better

126
00:06:22,280 --> 00:06:26,869
alternatives on stack overflow so we

127
00:06:24,770 --> 00:06:30,200
basically had to solve three technical

128
00:06:26,870 --> 00:06:33,260
problems and that was being able to

129
00:06:30,200 --> 00:06:36,620
predict the similarity of cryptographic

130
00:06:33,260 --> 00:06:41,510
api patterns predicting the use case and

131
00:06:36,620 --> 00:06:44,390
of course their security so in our first

132
00:06:41,510 --> 00:06:47,120
step we learned a representation of

133
00:06:44,390 --> 00:06:49,669
cryptographic API patterns by embedding

134
00:06:47,120 --> 00:06:55,400
their code graphs into a vector space

135
00:06:49,670 --> 00:06:57,830
using structure to Beck so the sodium

136
00:06:55,400 --> 00:07:00,260
betting is lunge such that similar

137
00:06:57,830 --> 00:07:01,789
patterns are closer together and

138
00:07:00,260 --> 00:07:04,060
decently our patterns are more far away

139
00:07:01,790 --> 00:07:07,010
from each other in the embedding space

140
00:07:04,060 --> 00:07:09,199
and additionally we had to solve a

141
00:07:07,010 --> 00:07:11,960
problem that was kind of specific for

142
00:07:09,199 --> 00:07:14,110
Stack Overflow and that was you know a

143
00:07:11,960 --> 00:07:17,448
lot of code examples on Stack Overflow

144
00:07:14,110 --> 00:07:19,340
are incomplete programs so they won't

145
00:07:17,449 --> 00:07:21,530
compile so we had to use a fuzzy

146
00:07:19,340 --> 00:07:23,169
compiler to get some kind of graph

147
00:07:21,530 --> 00:07:27,590
representation out of it

148
00:07:23,169 --> 00:07:29,030
which might be unsound so however our

149
00:07:27,590 --> 00:07:31,669
embedding approach takes this into

150
00:07:29,030 --> 00:07:34,130
account and moves these different graph

151
00:07:31,669 --> 00:07:35,719
representation for the same header and

152
00:07:34,130 --> 00:07:41,479
also a closer together in the embedding

153
00:07:35,720 --> 00:07:43,669
space alright so the second problem we

154
00:07:41,479 --> 00:07:46,909
needed to solve was to predict use cases

155
00:07:43,669 --> 00:07:49,010
of patterns so since we already had a

156
00:07:46,910 --> 00:07:51,800
preach a pre-trained similarity model

157
00:07:49,010 --> 00:07:54,770
that already contained useful knowledge

158
00:07:51,800 --> 00:07:56,270
to predict a use case we transfer this

159
00:07:54,770 --> 00:07:58,280
knowledge from the knowledge from the

160
00:07:56,270 --> 00:08:00,590
similarity domain into the use case

161
00:07:58,280 --> 00:08:01,969
domain so we basically added a couple of

162
00:08:00,590 --> 00:08:04,099
layers on top of a representation

163
00:08:01,970 --> 00:08:05,930
Learning Network and a multi-class

164
00:08:04,099 --> 00:08:11,659
classification layer to predict do we

165
00:08:05,930 --> 00:08:13,700
use cases and we did the same thing for

166
00:08:11,660 --> 00:08:15,860
predicting security we applied transfer

167
00:08:13,700 --> 00:08:17,240
learning from trenton to transfer the

168
00:08:15,860 --> 00:08:20,180
knowledge from the similarity domain

169
00:08:17,240 --> 00:08:24,039
into the security domain this time using

170
00:08:20,180 --> 00:08:24,039
a binary classification classifier so

171
00:08:24,940 --> 00:08:30,050
here on the left side you can see the

172
00:08:27,139 --> 00:08:33,349
results of our similarity model for

173
00:08:30,050 --> 00:08:34,940
basically all the Drupal graphic API

174
00:08:33,349 --> 00:08:37,580
patterns we extracted from Stack

175
00:08:34,940 --> 00:08:41,390
Overflow and here the color basically

176
00:08:37,580 --> 00:08:43,490
indicates their use cases and as you can

177
00:08:41,390 --> 00:08:45,380
see the similarity model already creates

178
00:08:43,490 --> 00:08:47,630
some some dense clusters for some use

179
00:08:45,380 --> 00:08:50,180
cases but also sparse clusters for

180
00:08:47,630 --> 00:08:53,300
others and on the right side you can see

181
00:08:50,180 --> 00:08:56,359
the results of our use case model which

182
00:08:53,300 --> 00:08:58,099
kind of corrects this and creates by

183
00:08:56,360 --> 00:09:00,370
creating dense clusters for all of these

184
00:08:58,100 --> 00:09:00,370
use cases

185
00:09:02,570 --> 00:09:07,370
on here in the middle you can see the

186
00:09:04,810 --> 00:09:10,459
results of her assumed similarity model

187
00:09:07,370 --> 00:09:14,390
again however this time the color

188
00:09:10,460 --> 00:09:18,170
indicates the security of a pattern and

189
00:09:14,390 --> 00:09:19,819
this cluster here is a cypher cluster so

190
00:09:18,170 --> 00:09:23,569
the use case of the patterns here are

191
00:09:19,820 --> 00:09:27,830
you know initializing a cipher and this

192
00:09:23,570 --> 00:09:29,750
cluster kind of gives you know nicely

193
00:09:27,830 --> 00:09:31,760
indicates that or a main idea of

194
00:09:29,750 --> 00:09:33,920
matching people from you know in

195
00:09:31,760 --> 00:09:37,670
security secur might actually be

196
00:09:33,920 --> 00:09:39,469
technically feasible so this as you can

197
00:09:37,670 --> 00:09:42,680
see the similarity cluster has a

198
00:09:39,470 --> 00:09:44,450
security boundary so samples that are

199
00:09:42,680 --> 00:09:50,479
close to this boundary might actually

200
00:09:44,450 --> 00:09:54,830
provide very useful alternatives and so

201
00:09:50,480 --> 00:09:57,230
we cherry picked an example for this so

202
00:09:54,830 --> 00:09:59,270
on the left side you can see the

203
00:09:57,230 --> 00:10:02,150
security warning for an insecure pattern

204
00:09:59,270 --> 00:10:03,500
as we showed it on stackoverflow and the

205
00:10:02,150 --> 00:10:05,240
bottom you can see the list of

206
00:10:03,500 --> 00:10:09,230
recommendations and they're ordered by

207
00:10:05,240 --> 00:10:10,970
similarity and use cases so when you

208
00:10:09,230 --> 00:10:12,680
click on the first link you would end up

209
00:10:10,970 --> 00:10:16,040
on a Stack Overflow post shown on the

210
00:10:12,680 --> 00:10:19,040
right and as you can see the the code is

211
00:10:16,040 --> 00:10:20,360
basically the same it only differs in in

212
00:10:19,040 --> 00:10:27,020
the statement that rendered the whole

213
00:10:20,360 --> 00:10:29,450
code snippet insecure before however the

214
00:10:27,020 --> 00:10:31,069
user of the developer might actually

215
00:10:29,450 --> 00:10:34,190
ignore basically everything I just

216
00:10:31,070 --> 00:10:37,490
showed you know copied code copy would

217
00:10:34,190 --> 00:10:40,100
copy be insecure codes again and so

218
00:10:37,490 --> 00:10:43,190
whenever we we kind of you know see that

219
00:10:40,100 --> 00:10:45,260
we trigger a reminder nudge that

220
00:10:43,190 --> 00:10:47,720
basically shows the security warning and

221
00:10:45,260 --> 00:10:53,780
the recommendations again to launch the

222
00:10:47,720 --> 00:10:56,510
you know the the user one more time so

223
00:10:53,780 --> 00:10:58,310
as we now had everything together we

224
00:10:56,510 --> 00:11:02,090
wanted a test or a system design within

225
00:10:58,310 --> 00:11:05,449
a small developer study and we had two

226
00:11:02,090 --> 00:11:09,590
treatments when not the nuts group and

227
00:11:05,450 --> 00:11:11,420
the control group and both of them had

228
00:11:09,590 --> 00:11:12,740
to solve to two different different

229
00:11:11,420 --> 00:11:15,979
programming tasks symmetric encryption

230
00:11:12,740 --> 00:11:16,339
and certificate verification and we had

231
00:11:15,980 --> 00:11:17,990
to

232
00:11:16,339 --> 00:11:19,850
Tricks functional correctness and

233
00:11:17,990 --> 00:11:22,220
security so once again functional

234
00:11:19,850 --> 00:11:23,779
correctness for us was very important

235
00:11:22,220 --> 00:11:25,279
because we wanted to make sure that we

236
00:11:23,779 --> 00:11:29,089
didn't create any obstacles for

237
00:11:25,279 --> 00:11:33,220
programmers such that they you know can

238
00:11:29,089 --> 00:11:33,220
happen we continue copying spaces stuff

239
00:11:33,819 --> 00:11:40,099
so and yes we achieved our first goal

240
00:11:37,790 --> 00:11:42,498
for functional correctness so both

241
00:11:40,100 --> 00:11:44,480
groups achieved a very high level of

242
00:11:42,499 --> 00:11:47,120
functional the solutions of both group

243
00:11:44,480 --> 00:11:49,399
groups achieved a very high level of

244
00:11:47,120 --> 00:11:52,129
functional correctness and the nudge

245
00:11:49,399 --> 00:11:57,319
group had no significant negative effect

246
00:11:52,129 --> 00:12:01,009
on functional correctness and we also

247
00:11:57,319 --> 00:12:02,959
achieved our security goal as the nudge

248
00:12:01,009 --> 00:12:06,309
group significantly outperformed the

249
00:12:02,959 --> 00:12:13,998
control group when it comes to you know

250
00:12:06,309 --> 00:12:16,699
with respect to the solutions alright so

251
00:12:13,999 --> 00:12:20,209
and now I'd like to discuss a little bit

252
00:12:16,699 --> 00:12:22,069
you know what worked and to be able to

253
00:12:20,209 --> 00:12:24,998
do that we looked at the copy and paste

254
00:12:22,069 --> 00:12:31,279
history of all of not participants and

255
00:12:24,999 --> 00:12:33,709
that includes the what copy what code

256
00:12:31,279 --> 00:12:35,929
has been copied from Stack Overflow into

257
00:12:33,709 --> 00:12:38,569
the clipboard and what code has been

258
00:12:35,929 --> 00:12:43,129
pasted from the clipboard into the IDE

259
00:12:38,569 --> 00:12:45,920
of our participants so first we wanted

260
00:12:43,129 --> 00:12:50,529
to have a look at how security warnings

261
00:12:45,920 --> 00:12:53,059
work on Stack Overflow and yes we had

262
00:12:50,529 --> 00:12:55,759
27% unsecure copies from Stack Overflow

263
00:12:53,059 --> 00:12:57,800
to the clipboard so once again you know

264
00:12:55,759 --> 00:13:04,720
stick you know security warnings are not

265
00:12:57,800 --> 00:13:08,540
enough nothing new however we had 0%

266
00:13:04,720 --> 00:13:11,509
insecure pastes into the IDE so every

267
00:13:08,540 --> 00:13:17,839
each and every paste from Stack Overflow

268
00:13:11,509 --> 00:13:22,670
into the IDE was secure so we concluded

269
00:13:17,839 --> 00:13:26,059
that people drop the insecure copy due

270
00:13:22,670 --> 00:13:29,259
to the reminder notch and reused one of

271
00:13:26,059 --> 00:13:29,259
our recommendations instead

272
00:13:29,940 --> 00:13:38,140
so to wrap up the talk I'd like to

273
00:13:34,000 --> 00:13:41,080
present one of our one of a nice over

274
00:13:38,140 --> 00:13:43,930
nice results I think which was that it

275
00:13:41,080 --> 00:13:48,330
helped tackling all these nasty now

276
00:13:43,930 --> 00:13:50,920
verify is a little bit so short reminder

277
00:13:48,330 --> 00:13:53,530
in a right Oakland paper we basically

278
00:13:50,920 --> 00:13:56,770
showed that ninety percent of code that

279
00:13:53,530 --> 00:14:00,010
was reused from Stack Overflow and

280
00:13:56,770 --> 00:14:06,819
Android applications were all verifies

281
00:14:00,010 --> 00:14:09,900
and 0.2 percent was only 0.2% was code

282
00:14:06,820 --> 00:14:13,750
that God certificate verification right

283
00:14:09,900 --> 00:14:16,420
so our a nudge group here achieved 77

284
00:14:13,750 --> 00:14:19,870
percent secure solutions while still the

285
00:14:16,420 --> 00:14:22,990
control state can be quite behind here

286
00:14:19,870 --> 00:14:27,070
and I think this is quite a cool result

287
00:14:22,990 --> 00:14:28,420
because we consider this problem as the

288
00:14:27,070 --> 00:14:31,690
one with the highest risk for

289
00:14:28,420 --> 00:14:35,770
application security as as shown by fall

290
00:14:31,690 --> 00:14:38,920
at all and at that this was also the use

291
00:14:35,770 --> 00:14:41,910
case where we found you know the less

292
00:14:38,920 --> 00:14:44,620
the the lowest number of secure

293
00:14:41,910 --> 00:14:48,850
solutions on Stack Overflow we kind of

294
00:14:44,620 --> 00:14:51,310
we found I think over 1,000 examples for

295
00:14:48,850 --> 00:14:55,350
an all verifier and only like 50 to 60

296
00:14:51,310 --> 00:14:55,349
examples that it did it right

297
00:14:57,330 --> 00:15:03,730
however there's still you know lots of

298
00:15:00,250 --> 00:15:06,400
questions and open problems one an

299
00:15:03,730 --> 00:15:08,590
interesting problem is or question is

300
00:15:06,400 --> 00:15:10,630
the default notch which is a very very

301
00:15:08,590 --> 00:15:14,290
popular notch in the literature it

302
00:15:10,630 --> 00:15:17,260
basically you know presents the the

303
00:15:14,290 --> 00:15:18,910
better option per default and we

304
00:15:17,260 --> 00:15:21,970
implemented this an on in Stack Overflow

305
00:15:18,910 --> 00:15:25,150
by basically reordering search results

306
00:15:21,970 --> 00:15:29,070
based on security such that people would

307
00:15:25,150 --> 00:15:34,530
see you know the better option first and

308
00:15:29,070 --> 00:15:37,450
it'd be nice to see a large-scale study

309
00:15:34,530 --> 00:15:41,199
on live Stack Overflow with real Stack

310
00:15:37,450 --> 00:15:43,089
Overflow users to test you know several

311
00:15:41,200 --> 00:15:47,649
different you UI in different

312
00:15:43,089 --> 00:15:48,729
to come up with a final design so the

313
00:15:47,649 --> 00:15:51,039
great thing is that Stack Overflow

314
00:15:48,729 --> 00:15:54,699
offers a partnership program with

315
00:15:51,039 --> 00:15:58,449
academia so that might be an opportunity

316
00:15:54,699 --> 00:16:10,539
to actually have impact all right thank

317
00:15:58,449 --> 00:16:13,029
you very much thank you for the great

318
00:16:10,539 --> 00:16:15,098
talk so you might think that I'm

319
00:16:13,029 --> 00:16:17,469
overthinking but consider that I'm an

320
00:16:15,099 --> 00:16:21,009
attacker who wants to spread my buggy

321
00:16:17,469 --> 00:16:22,119
code for malicious purposes so before

322
00:16:21,009 --> 00:16:24,129
these nudges

323
00:16:22,119 --> 00:16:26,829
I need to convince people that my code

324
00:16:24,129 --> 00:16:28,749
is good but after these noises I just

325
00:16:26,829 --> 00:16:30,218
need to convince your model that my code

326
00:16:28,749 --> 00:16:33,759
is good so that it would recommend

327
00:16:30,219 --> 00:16:35,619
people have you evaluated like which one

328
00:16:33,759 --> 00:16:37,809
is easier here we avoided that an

329
00:16:35,619 --> 00:16:39,789
attacker who might put your model as the

330
00:16:37,809 --> 00:16:43,659
target yep

331
00:16:39,789 --> 00:16:47,199
so of course an adverse will attack here

332
00:16:43,659 --> 00:16:49,749
is an interesting question and the thing

333
00:16:47,199 --> 00:16:54,758
that the code on Stack Overflow is is

334
00:16:49,749 --> 00:16:57,339
fuzzy I think makes an adversarial

335
00:16:54,759 --> 00:16:59,559
attack a little bit easier so the code

336
00:16:57,339 --> 00:17:02,439
doesn't have to compile so you could add

337
00:16:59,559 --> 00:17:04,089
you know randomly try to you if you want

338
00:17:02,439 --> 00:17:06,039
to add noise you would try to randomly

339
00:17:04,089 --> 00:17:09,878
add the character somewhere in the code

340
00:17:06,039 --> 00:17:11,559
and try to you know somehow changed its

341
00:17:09,878 --> 00:17:14,079
graph representation such as the

342
00:17:11,559 --> 00:17:16,509
embedding is changed and the model would

343
00:17:14,079 --> 00:17:20,158
make it wrong you know what changes

344
00:17:16,509 --> 00:17:23,769
prediction from from insecure to secure

345
00:17:20,159 --> 00:17:26,470
however on Stack Overflow you would have

346
00:17:23,769 --> 00:17:29,529
to do a lot of things to actually be

347
00:17:26,470 --> 00:17:31,240
able to be allowed to change code on

348
00:17:29,529 --> 00:17:33,759
Stack Overflow I mean you would have to

349
00:17:31,240 --> 00:17:34,990
do the query and to do against a model

350
00:17:33,759 --> 00:17:36,190
and to be able to do that you would have

351
00:17:34,990 --> 00:17:38,409
to change the code and Stack Overflow

352
00:17:36,190 --> 00:17:40,360
and then to be able to do that you would

353
00:17:38,409 --> 00:17:42,909
have to do a lot of things to get the

354
00:17:40,360 --> 00:17:46,559
permission to do that and when you

355
00:17:42,909 --> 00:17:49,379
change the code the code is shown two

356
00:17:46,559 --> 00:17:52,509
different monitors on Stack Overflow

357
00:17:49,379 --> 00:17:55,330
similar to you know reviewing code and

358
00:17:52,509 --> 00:17:57,610
they would have to agree

359
00:17:55,330 --> 00:17:59,379
you know that this change makes sense

360
00:17:57,610 --> 00:18:00,748
because psycho flow has a lot of

361
00:17:59,379 --> 00:18:04,980
problems with people you know just

362
00:18:00,749 --> 00:18:07,600
randomly change different things to copy

363
00:18:04,980 --> 00:18:09,970
stuff you know to create duplicates to

364
00:18:07,600 --> 00:18:12,279
increase their score so there are people

365
00:18:09,970 --> 00:18:15,129
that are looking at this stuff however

366
00:18:12,279 --> 00:18:17,110
if you think this if you think of this

367
00:18:15,129 --> 00:18:20,350
as a general approach you would also

368
00:18:17,110 --> 00:18:22,238
like to use on github or something

369
00:18:20,350 --> 00:18:25,918
something else of course positive

370
00:18:22,239 --> 00:18:31,749
indicators for security are are you know

371
00:18:25,919 --> 00:18:33,639
could be a problem yeah I'm very

372
00:18:31,749 --> 00:18:36,249
interesting work I'm wondering how you

373
00:18:33,639 --> 00:18:40,109
label those cryptography code as secure

374
00:18:36,249 --> 00:18:40,109
or insecure from stack overflow

375
00:18:40,450 --> 00:18:52,899
yeah we we basically reused the labeling

376
00:18:48,190 --> 00:18:54,519
rules from our previous paper so we so

377
00:18:52,899 --> 00:18:57,039
we we did this manually

378
00:18:54,519 --> 00:18:58,840
so we basically looked at all these ten

379
00:18:57,039 --> 00:19:01,269
you know I don't know how many samples

380
00:18:58,840 --> 00:19:04,869
it was I think ten thousand thousand

381
00:19:01,269 --> 00:19:08,200
samples we looked at them and we had

382
00:19:04,869 --> 00:19:10,359
different you know security experts and

383
00:19:08,200 --> 00:19:14,200
they looked at it and decided whether

384
00:19:10,359 --> 00:19:20,408
it's secure or not if this answers the

385
00:19:14,200 --> 00:19:21,030
question great let's thank Felix thank

386
00:19:20,409 --> 00:19:21,100
you very much

387
00:19:21,030 --> 00:19:25,190
[Music]

388
00:19:21,100 --> 00:19:25,189
[Applause]

