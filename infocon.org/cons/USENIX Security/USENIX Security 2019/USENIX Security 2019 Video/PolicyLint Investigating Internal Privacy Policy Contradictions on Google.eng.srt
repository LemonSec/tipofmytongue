1
00:00:10,920 --> 00:00:16,340
so thanks for the introduction so I'm

2
00:00:13,800 --> 00:00:18,300
Ben and ow I'm currently at IBM Research

3
00:00:16,340 --> 00:00:19,830
so today we're going to be talking about

4
00:00:18,300 --> 00:00:22,650
the work that we did at looking at

5
00:00:19,830 --> 00:00:23,909
self-contradictory privacy policies so

6
00:00:22,650 --> 00:00:25,549
I'm sure everybody here has seen a

7
00:00:23,910 --> 00:00:28,099
privacy policy at one point or another

8
00:00:25,550 --> 00:00:31,770
reading it is clearly a different story

9
00:00:28,099 --> 00:00:33,360
but privacy policies generally disclose

10
00:00:31,770 --> 00:00:35,089
how companies collect and share your

11
00:00:33,360 --> 00:00:38,640
data and these are required by various

12
00:00:35,090 --> 00:00:41,430
regulations such as the Cal opah in GDP

13
00:00:38,640 --> 00:00:43,199
are so below is just a little excerpt

14
00:00:41,430 --> 00:00:45,660
from a privacy policy of an Android app

15
00:00:43,199 --> 00:00:47,968
and in red it basically just states that

16
00:00:45,660 --> 00:00:49,828
we collect personal information so when

17
00:00:47,969 --> 00:00:51,120
using this app you assume that it's

18
00:00:49,829 --> 00:00:53,460
going to collect some type of your

19
00:00:51,120 --> 00:00:56,010
personal data but this raises the

20
00:00:53,460 --> 00:00:57,840
question are all apps disclosing all of

21
00:00:56,010 --> 00:01:00,089
their privacy practices in the policies

22
00:00:57,840 --> 00:01:02,699
in the perfect world the answer would be

23
00:01:00,090 --> 00:01:04,700
yes but unfortunately that's not the

24
00:01:02,699 --> 00:01:08,100
case in large companies such as snapchat

25
00:01:04,700 --> 00:01:09,540
Lenovo and an Android app developer

26
00:01:08,100 --> 00:01:12,140
called the golden Shore technologies

27
00:01:09,540 --> 00:01:14,840
that at 50-plus million downloads

28
00:01:12,140 --> 00:01:17,520
we're omitting their privacy practices

29
00:01:14,840 --> 00:01:19,320
or being dishonest about their practices

30
00:01:17,520 --> 00:01:22,949
and misleading users and they were fined

31
00:01:19,320 --> 00:01:24,750
by the FTC so we need a way to audit

32
00:01:22,950 --> 00:01:28,649
these applications and there's been some

33
00:01:24,750 --> 00:01:30,509
prior work in XE 16:18 and NDS s that

34
00:01:28,649 --> 00:01:32,759
looked at creating these tools to audit

35
00:01:30,509 --> 00:01:34,079
app behavior and make sure that they

36
00:01:32,759 --> 00:01:37,049
comply with what they're actually

37
00:01:34,079 --> 00:01:39,508
stating in their privacy policy however

38
00:01:37,049 --> 00:01:41,520
when looking at the consistency models

39
00:01:39,509 --> 00:01:43,079
that they use in this work we found that

40
00:01:41,520 --> 00:01:45,360
they all have this implicit assumption

41
00:01:43,079 --> 00:01:48,719
that the privacy policies themselves are

42
00:01:45,360 --> 00:01:50,100
internally consistent and we're gonna

43
00:01:48,719 --> 00:01:52,589
invalidate that assumption today

44
00:01:50,100 --> 00:01:54,270
basically and we show that this is not

45
00:01:52,590 --> 00:01:58,469
the case because privacy policies can be

46
00:01:54,270 --> 00:01:59,880
self contradictory so if you look at the

47
00:01:58,469 --> 00:02:00,298
rest of the policy that I showed you

48
00:01:59,880 --> 00:02:02,490
earlier

49
00:02:00,299 --> 00:02:04,049
it also says we do not collect personal

50
00:02:02,490 --> 00:02:06,600
information so they say they do and they

51
00:02:04,049 --> 00:02:09,780
don't collect personal information which

52
00:02:06,600 --> 00:02:11,130
leads to ambiguous policies furthermore

53
00:02:09,780 --> 00:02:13,440
these contradictions can occur at these

54
00:02:11,130 --> 00:02:15,510
different semantic granularities so the

55
00:02:13,440 --> 00:02:18,300
policy also states that we collect email

56
00:02:15,510 --> 00:02:19,530
address so they say they they don't

57
00:02:18,300 --> 00:02:21,390
collect personal information but they

58
00:02:19,530 --> 00:02:23,400
collect email address email address is a

59
00:02:21,390 --> 00:02:24,799
type of personal information story is

60
00:02:23,400 --> 00:02:27,710
also contradictory

61
00:02:24,800 --> 00:02:29,900
so why should you care basically so

62
00:02:27,710 --> 00:02:31,760
other than ambiguous policies we also

63
00:02:29,900 --> 00:02:33,380
found that self-contradictions can lead

64
00:02:31,760 --> 00:02:35,269
to the identification of deceptive

65
00:02:33,380 --> 00:02:39,200
statements which are enforceable by the

66
00:02:35,270 --> 00:02:41,750
FTC and the DA's of the EU furthermore

67
00:02:39,200 --> 00:02:44,510
in terms of tools that are looking to do

68
00:02:41,750 --> 00:02:48,560
automated behavior to policy compliance

69
00:02:44,510 --> 00:02:51,200
we want to check what the impact is of

70
00:02:48,560 --> 00:02:53,150
this implicit assumption because they

71
00:02:51,200 --> 00:02:56,929
may claim that a behavior is justified

72
00:02:53,150 --> 00:03:00,950
when in fact it is not so we create a

73
00:02:56,930 --> 00:03:02,990
tool called policy lint and the goal of

74
00:03:00,950 --> 00:03:05,420
policy length is essentially to identify

75
00:03:02,990 --> 00:03:07,090
bugs in a privacy policy and the bugs

76
00:03:05,420 --> 00:03:09,170
that we're looking for in this case or

77
00:03:07,090 --> 00:03:12,200
self-contradictory sharing collection

78
00:03:09,170 --> 00:03:14,679
statements so note that the goal of

79
00:03:12,200 --> 00:03:16,850
policy lint is the aid human analysis

80
00:03:14,680 --> 00:03:18,290
we're looking to automate the approach

81
00:03:16,850 --> 00:03:21,079
as much as possible but humans still

82
00:03:18,290 --> 00:03:22,790
needed in the pipeline somewhere because

83
00:03:21,080 --> 00:03:27,380
natural language understanding is not

84
00:03:22,790 --> 00:03:31,549
you know there yet plus interpreting

85
00:03:27,380 --> 00:03:35,079
some of the statements requires a legal

86
00:03:31,550 --> 00:03:38,720
expert in some cases to determine intent

87
00:03:35,080 --> 00:03:41,030
so to achieve this we have three main

88
00:03:38,720 --> 00:03:43,160
technical research questions that we

89
00:03:41,030 --> 00:03:44,930
answer here the first is as I mentioned

90
00:03:43,160 --> 00:03:46,760
before these contradictions can occur at

91
00:03:44,930 --> 00:03:48,680
different language granularities so we

92
00:03:46,760 --> 00:03:50,570
need a way to automatically Reason over

93
00:03:48,680 --> 00:03:53,570
these different semantics granularities

94
00:03:50,570 --> 00:03:55,970
and we do so using ontology x' we

95
00:03:53,570 --> 00:03:59,120
automatically construct ontology x' from

96
00:03:55,970 --> 00:04:00,740
privacy policy policies because we

97
00:03:59,120 --> 00:04:02,209
operate on the insight that privacy

98
00:04:00,740 --> 00:04:04,910
policies themselves encode the

99
00:04:02,209 --> 00:04:07,670
anthologies by giving examples of what

100
00:04:04,910 --> 00:04:09,740
certain data types entails so for

101
00:04:07,670 --> 00:04:11,839
example they may say we clocked personal

102
00:04:09,740 --> 00:04:13,520
information such as your name so from

103
00:04:11,840 --> 00:04:15,140
that type of statement we know that your

104
00:04:13,520 --> 00:04:17,660
name is a type of personal information

105
00:04:15,140 --> 00:04:20,029
so while one privacy policy alone is

106
00:04:17,660 --> 00:04:23,090
probably not comprehensive enough if we

107
00:04:20,029 --> 00:04:24,469
take a large set of privacy policies we

108
00:04:23,090 --> 00:04:27,349
show that we you can build a pretty

109
00:04:24,470 --> 00:04:28,850
comprehensive ontology and we actually

110
00:04:27,350 --> 00:04:31,160
build two ontology a data object

111
00:04:28,850 --> 00:04:32,690
ontology for reasoning over data types

112
00:04:31,160 --> 00:04:34,970
so email address is a type of personal

113
00:04:32,690 --> 00:04:36,770
information and the entity ontology so

114
00:04:34,970 --> 00:04:38,790
that we know that AdMob is a type of ad

115
00:04:36,770 --> 00:04:41,549
provider and we do so

116
00:04:38,790 --> 00:04:43,530
using lexical syntactic patterns with

117
00:04:41,550 --> 00:04:45,150
enforce named at identity constraints

118
00:04:43,530 --> 00:04:47,909
the full details are within our paper

119
00:04:45,150 --> 00:04:49,640
though so the next challenge that we

120
00:04:47,910 --> 00:04:51,930
address is we need to extract

121
00:04:49,640 --> 00:04:54,180
fine-grained policy statement tuples and

122
00:04:51,930 --> 00:04:56,190
we need to mop a model positive and

123
00:04:54,180 --> 00:04:57,150
negative sentiment so by that I mean we

124
00:04:56,190 --> 00:04:59,219
need to know if they're talking about

125
00:04:57,150 --> 00:05:10,859
sharing or not sharing or collecting

126
00:04:59,220 --> 00:05:12,630
verse not collecting and we do so sorry

127
00:05:10,860 --> 00:05:17,150
about that so we do so using a

128
00:05:12,630 --> 00:05:20,190
grammatical structure and we basically

129
00:05:17,150 --> 00:05:22,919
use the the dependency based parse trees

130
00:05:20,190 --> 00:05:24,570
to extract this information to extract

131
00:05:22,920 --> 00:05:26,400
this three tuple at the bottom which is

132
00:05:24,570 --> 00:05:29,700
some entity collects or not collects

133
00:05:26,400 --> 00:05:31,380
some data type and it's worth mentioning

134
00:05:29,700 --> 00:05:32,940
that we also handle sharing collection

135
00:05:31,380 --> 00:05:34,890
statements they get simplified down to

136
00:05:32,940 --> 00:05:36,930
this form with a set of transformation

137
00:05:34,890 --> 00:05:38,750
rules that are also documented in the

138
00:05:36,930 --> 00:05:42,660
paper

139
00:05:38,750 --> 00:05:43,830
so finally we we want to analyze the

140
00:05:42,660 --> 00:05:45,480
what are the different types of

141
00:05:43,830 --> 00:05:48,030
contradictions that occur and we

142
00:05:45,480 --> 00:05:49,980
formalize this over the three

143
00:05:48,030 --> 00:05:51,750
ontological operators the first is

144
00:05:49,980 --> 00:05:53,730
semantic equivalence so this is

145
00:05:51,750 --> 00:05:56,970
basically a synonym style relationship

146
00:05:53,730 --> 00:05:59,040
so postal address is a type or postal

147
00:05:56,970 --> 00:06:02,850
address is semantically equivalent to

148
00:05:59,040 --> 00:06:04,860
mailing address D something operator

149
00:06:02,850 --> 00:06:06,930
which is just below there it means that

150
00:06:04,860 --> 00:06:08,550
X is a Y so email address is a type of

151
00:06:06,930 --> 00:06:10,440
personal information and then we have

152
00:06:08,550 --> 00:06:12,060
the inverse of that so based on these

153
00:06:10,440 --> 00:06:13,320
ontological relations we have nine

154
00:06:12,060 --> 00:06:15,750
different types of conflicting

155
00:06:13,320 --> 00:06:18,599
statements we further subdivide this

156
00:06:15,750 --> 00:06:20,580
into two classes the first are logical

157
00:06:18,600 --> 00:06:22,920
contradictions and these are either

158
00:06:20,580 --> 00:06:24,750
direct contradictions where it says some

159
00:06:22,920 --> 00:06:27,060
entity is collecting and not collecting

160
00:06:24,750 --> 00:06:28,170
that specific data type or they're

161
00:06:27,060 --> 00:06:30,600
saying they collect some broad

162
00:06:28,170 --> 00:06:32,460
information or they do not collect some

163
00:06:30,600 --> 00:06:33,990
broad information but they collect

164
00:06:32,460 --> 00:06:36,299
something more specific so these are the

165
00:06:33,990 --> 00:06:38,610
statements such as we do not collect

166
00:06:36,300 --> 00:06:41,190
personal information but we collect your

167
00:06:38,610 --> 00:06:42,480
email address and we show in our

168
00:06:41,190 --> 00:06:43,890
findings that these can actually lead to

169
00:06:42,480 --> 00:06:46,950
the identification of deceptive

170
00:06:43,890 --> 00:06:49,320
statements and then the second class are

171
00:06:46,950 --> 00:06:50,880
the narrowing definitions so these are

172
00:06:49,320 --> 00:06:52,139
when they say they collect the broad

173
00:06:50,880 --> 00:06:53,849
information but

174
00:06:52,139 --> 00:06:55,860
do not collect something more specific

175
00:06:53,849 --> 00:06:57,810
these are not inherently negative

176
00:06:55,860 --> 00:06:59,879
because they don't confuse the humans

177
00:06:57,810 --> 00:07:02,610
trying to read the policy so for example

178
00:06:59,879 --> 00:07:04,409
it can say we collect personal

179
00:07:02,610 --> 00:07:06,419
information but we don't collect email

180
00:07:04,409 --> 00:07:08,310
address but we don't collect your

181
00:07:06,419 --> 00:07:11,580
address so it's narrowing the scope of

182
00:07:08,310 --> 00:07:13,379
their collection practices but we still

183
00:07:11,580 --> 00:07:16,109
want to measure these be as they impact

184
00:07:13,379 --> 00:07:20,819
the the the accuracy of automated

185
00:07:16,110 --> 00:07:23,159
analysis approaches so we performed a

186
00:07:20,819 --> 00:07:24,960
large-scale study on eleven thousand

187
00:07:23,159 --> 00:07:28,500
four hundred thirty of the top Android

188
00:07:24,960 --> 00:07:30,628
apps in a September 2017 snapshot and we

189
00:07:28,500 --> 00:07:34,529
basically wanted to explore how frequent

190
00:07:30,629 --> 00:07:36,300
these self-contradictions occur we

191
00:07:34,529 --> 00:07:37,979
actually found that around one and out

192
00:07:36,300 --> 00:07:41,400
of every what is that one out of every

193
00:07:37,979 --> 00:07:43,139
six privacy policies have logical

194
00:07:41,400 --> 00:07:46,020
contradictions or narrowing definitions

195
00:07:43,139 --> 00:07:47,430
meaning that prior what approaches

196
00:07:46,020 --> 00:07:49,378
looking at behavior policy compliance

197
00:07:47,430 --> 00:07:51,300
may make the wrong decision about a

198
00:07:49,379 --> 00:07:53,699
policy about one and edit every six

199
00:07:51,300 --> 00:07:55,860
times and this is only in terms of the

200
00:07:53,699 --> 00:07:57,569
consistency model we show on the paper

201
00:07:55,860 --> 00:07:59,129
that they could have an error rate up to

202
00:07:57,569 --> 00:08:02,129
sixty percent in their information

203
00:07:59,129 --> 00:08:03,930
extraction and logical contradictions

204
00:08:02,129 --> 00:08:07,110
impact around fourteen point two percent

205
00:08:03,930 --> 00:08:08,699
of policies and it's pretty evenly

206
00:08:07,110 --> 00:08:11,099
distributed across the Google Play

207
00:08:08,699 --> 00:08:12,629
categories some of the ones look a

208
00:08:11,099 --> 00:08:14,300
little higher hovering at around like

209
00:08:12,629 --> 00:08:16,949
between twenty-five and thirty percent

210
00:08:14,300 --> 00:08:18,919
but we looked into the the outliers

211
00:08:16,949 --> 00:08:21,089
there and we found that it was uh

212
00:08:18,919 --> 00:08:23,729
basically multiple apps by the same

213
00:08:21,089 --> 00:08:27,870
developer in it day a contradictory

214
00:08:23,729 --> 00:08:29,849
policies which was skewing results we we

215
00:08:27,870 --> 00:08:31,949
broke it down again by application

216
00:08:29,849 --> 00:08:33,510
category by the number average number of

217
00:08:31,949 --> 00:08:36,269
contradictions and we showed that

218
00:08:33,510 --> 00:08:38,250
contradiction prevalence also does not

219
00:08:36,269 --> 00:08:40,880
vary across the Google Play categories

220
00:08:38,250 --> 00:08:43,559
showing that it's a distributed problem

221
00:08:40,880 --> 00:08:46,529
again we dug into the outliers there and

222
00:08:43,559 --> 00:08:48,810
they were skewed to based on some very

223
00:08:46,529 --> 00:08:50,699
highly contradictory policies so we have

224
00:08:48,810 --> 00:08:52,800
a CDF in a paper and it showed like the

225
00:08:50,699 --> 00:08:54,689
the long tail hit like forty two

226
00:08:52,800 --> 00:08:57,240
contradictions and a few other policies

227
00:08:54,690 --> 00:09:02,660
so some of the policies were very very

228
00:08:57,240 --> 00:09:05,490
highly contradictory we we couldn't

229
00:09:02,660 --> 00:09:06,050
investigate all of the the nine thousand

230
00:09:05,490 --> 00:09:09,019
nine

231
00:09:06,050 --> 00:09:10,490
logical contradictions so we we took a

232
00:09:09,019 --> 00:09:12,860
subset of this and we did so by

233
00:09:10,490 --> 00:09:16,209
examining basically the heat map that

234
00:09:12,860 --> 00:09:18,410
the top row there is are basically

235
00:09:16,209 --> 00:09:21,589
contradictions where they say that they

236
00:09:18,410 --> 00:09:24,319
do not collect personal information but

237
00:09:21,589 --> 00:09:26,870
they collect some subtype so we explore

238
00:09:24,320 --> 00:09:29,240
those cases and overall we validated 510

239
00:09:26,870 --> 00:09:32,839
contradictions which took about a week's

240
00:09:29,240 --> 00:09:35,750
worth of manpower so some of the

241
00:09:32,839 --> 00:09:38,029
highlight highlighted findings were was

242
00:09:35,750 --> 00:09:40,339
that we found some policies we're

243
00:09:38,029 --> 00:09:41,930
stating that common types of PII such as

244
00:09:40,339 --> 00:09:44,029
email addresses we're not actually

245
00:09:41,930 --> 00:09:47,269
personal information which was a little

246
00:09:44,029 --> 00:09:49,339
surprising because regulatory agencies

247
00:09:47,269 --> 00:09:52,220
or standards agencies such as NIST

248
00:09:49,339 --> 00:09:54,680
define it as such so there's an example

249
00:09:52,220 --> 00:09:56,240
below of this play toddlers app that was

250
00:09:54,680 --> 00:09:58,399
its target for children five and under

251
00:09:56,240 --> 00:10:00,589
if you look at the top of their privacy

252
00:09:58,399 --> 00:10:02,720
policy they say at play toddlers we

253
00:10:00,589 --> 00:10:04,250
don't have small print and run at the

254
00:10:02,720 --> 00:10:05,600
top of their policy it makes you feel

255
00:10:04,250 --> 00:10:07,399
good by saying we don't collect personal

256
00:10:05,600 --> 00:10:10,190
information however when you actually

257
00:10:07,399 --> 00:10:12,980
read the small print in their policy you

258
00:10:10,190 --> 00:10:14,390
see that they say when the user provides

259
00:10:12,980 --> 00:10:15,829
us with an email address the user

260
00:10:14,390 --> 00:10:18,020
confirms that this address is not

261
00:10:15,829 --> 00:10:21,170
personal data or does not contain

262
00:10:18,020 --> 00:10:23,390
personal data which was a little

263
00:10:21,170 --> 00:10:27,170
shocking when we saw that statement and

264
00:10:23,390 --> 00:10:29,569
it's arguably deceptive due to the the

265
00:10:27,170 --> 00:10:33,290
the icons that they blast in your face

266
00:10:29,570 --> 00:10:35,120
in the front we also found that services

267
00:10:33,290 --> 00:10:38,420
that auto generate temporal based

268
00:10:35,120 --> 00:10:40,730
policies we're causing some some issues

269
00:10:38,420 --> 00:10:42,110
in producing contradictory policies I

270
00:10:40,730 --> 00:10:44,450
think we found four to five different

271
00:10:42,110 --> 00:10:47,930
templates so this needs further

272
00:10:44,450 --> 00:10:49,279
exploration in terms of looking into all

273
00:10:47,930 --> 00:10:50,300
of these different services and making

274
00:10:49,279 --> 00:10:55,370
sure that they're producing high-quality

275
00:10:50,300 --> 00:10:57,920
policies and not problematic ones we

276
00:10:55,370 --> 00:10:59,660
also found that policies are considering

277
00:10:57,920 --> 00:11:01,640
device identifiers and hashed email

278
00:10:59,660 --> 00:11:04,279
addresses as non personal information

279
00:11:01,640 --> 00:11:06,709
which raises concerns about compliance

280
00:11:04,279 --> 00:11:09,610
with gdpr so for example gdpr considers

281
00:11:06,709 --> 00:11:12,800
device identifier is generally as PII

282
00:11:09,610 --> 00:11:14,449
but these policies a set of policies

283
00:11:12,800 --> 00:11:16,639
we're not considering that and defining

284
00:11:14,449 --> 00:11:20,809
them as non personal information

285
00:11:16,639 --> 00:11:22,929
and hashed identifiers such as email

286
00:11:20,809 --> 00:11:26,779
addresses they claim that that is

287
00:11:22,929 --> 00:11:28,279
anonymous yet it's questionable if it

288
00:11:26,779 --> 00:11:30,949
meets the require the pseudo anonymous

289
00:11:28,279 --> 00:11:32,689
pseudonym ization requirements defined

290
00:11:30,949 --> 00:11:35,868
by GDP are considering that you could

291
00:11:32,689 --> 00:11:37,339
pay four cents an email for sense for a

292
00:11:35,869 --> 00:11:39,559
hashed email to try to be reversed

293
00:11:37,339 --> 00:11:44,899
because people build up rainbow tables

294
00:11:39,559 --> 00:11:47,149
from data breach reports so we notified

295
00:11:44,899 --> 00:11:48,559
the developers via the email accounts

296
00:11:47,149 --> 00:11:51,199
that were listed under Google Play of

297
00:11:48,559 --> 00:11:52,610
our findings we only received 11

298
00:11:51,199 --> 00:11:55,939
responses I think that's about like four

299
00:11:52,610 --> 00:11:58,489
and a half percent three said they fixed

300
00:11:55,939 --> 00:12:00,709
their policies one disagreed with our

301
00:11:58,489 --> 00:12:03,410
findings in stadia that email addresses

302
00:12:00,709 --> 00:12:07,099
only email address or is only PII if it

303
00:12:03,410 --> 00:12:08,360
contains PII such as your name while

304
00:12:07,100 --> 00:12:09,559
they disagree with our findings I

305
00:12:08,360 --> 00:12:11,749
actually checked their policy yesterday

306
00:12:09,559 --> 00:12:13,610
and they remove the statement that it's

307
00:12:11,749 --> 00:12:17,329
not PII so they clearly agreed with our

308
00:12:13,610 --> 00:12:19,040
findings but just one in a minute four

309
00:12:17,329 --> 00:12:20,748
claimed an outdated policy that we

310
00:12:19,040 --> 00:12:23,329
analyzed some outdated policy and that

311
00:12:20,749 --> 00:12:26,509
they send us the new updated link one of

312
00:12:23,329 --> 00:12:28,609
the links that was sent contained a

313
00:12:26,509 --> 00:12:30,679
reference to see our full policy for the

314
00:12:28,610 --> 00:12:32,569
details and that was the one that we

315
00:12:30,679 --> 00:12:34,730
analyzed so I request a clarification

316
00:12:32,569 --> 00:12:36,769
and they never responded the other three

317
00:12:34,730 --> 00:12:38,809
had updated their policy we ran our tool

318
00:12:36,769 --> 00:12:40,399
again on it and they weren't

319
00:12:38,809 --> 00:12:43,429
contradictory anymore so that is a great

320
00:12:40,399 --> 00:12:46,160
thing in three responded back with no

321
00:12:43,429 --> 00:12:50,329
comments or clarifications such as thank

322
00:12:46,160 --> 00:12:52,279
you for the observation so general

323
00:12:50,329 --> 00:12:53,238
takeaways from our work is that privacy

324
00:12:52,279 --> 00:12:55,160
policies are frequently

325
00:12:53,239 --> 00:12:57,589
self-contradictory if you're looking at

326
00:12:55,160 --> 00:12:59,118
behavior to policy compliance you cannot

327
00:12:57,589 --> 00:13:01,339
have this implicit assumption that they

328
00:12:59,119 --> 00:13:03,739
are internally consistent and the the

329
00:13:01,339 --> 00:13:07,189
behavior of policy consistency models

330
00:13:03,739 --> 00:13:11,809
need to handle the cases of

331
00:13:07,189 --> 00:13:14,829
contradictory policies and in general we

332
00:13:11,809 --> 00:13:17,149
showed that the the current state of

333
00:13:14,829 --> 00:13:19,099
privacy policies for Google Play apps

334
00:13:17,149 --> 00:13:21,769
right now are not in the best shape and

335
00:13:19,100 --> 00:13:23,689
they need more attention in inspection

336
00:13:21,769 --> 00:13:27,049
by regulatory agencies such as the FTC

337
00:13:23,689 --> 00:13:30,329
or DPS where applicable and even

338
00:13:27,049 --> 00:13:34,300
potentially the application markets

339
00:13:30,330 --> 00:13:36,760
so I wanted to wrap up by thanking my

340
00:13:34,300 --> 00:13:39,040
collaborators from NCSU so semenya zere

341
00:13:36,760 --> 00:13:41,680
mahmoud justin Whitacre brad reeves will

342
00:13:39,040 --> 00:13:45,250
link my collaborators from UI you see

343
00:13:41,680 --> 00:13:47,949
when you want au j and my collaborator

344
00:13:45,250 --> 00:13:49,690
from IBM research kapil singh so I'd be

345
00:13:47,950 --> 00:13:51,840
happy to take any questions at this

346
00:13:49,690 --> 00:13:51,840
point

347
00:13:52,440 --> 00:13:59,059
[Applause]

348
00:14:00,360 --> 00:14:05,640
Indiana University Bloomington I'm

349
00:14:02,850 --> 00:14:08,860
interesting work and Mike Ryan shooting

350
00:14:05,640 --> 00:14:10,720
ontology construction alright and I

351
00:14:08,860 --> 00:14:13,900
would like to know what is the size of

352
00:14:10,720 --> 00:14:16,840
the Courant ology your construction and

353
00:14:13,900 --> 00:14:19,329
how many human effort you put on it and

354
00:14:16,840 --> 00:14:22,990
do you evaluate the accuracy of it and

355
00:14:19,330 --> 00:14:24,880
where you publish it sure so starting

356
00:14:22,990 --> 00:14:28,590
off in the beginning I think our data

357
00:14:24,880 --> 00:14:31,990
type ontology was a maybe 2 to 200 nodes

358
00:14:28,590 --> 00:14:35,620
in our entity one was about between a

359
00:14:31,990 --> 00:14:39,460
hundred and hundred fifty nodes in terms

360
00:14:35,620 --> 00:14:40,870
of a value uh evaluating the accuracy of

361
00:14:39,460 --> 00:14:43,390
it we we didn't do that because we

362
00:14:40,870 --> 00:14:44,830
didn't have ground truth but we did so

363
00:14:43,390 --> 00:14:46,600
we extracted the Luxan of syntactic

364
00:14:44,830 --> 00:14:47,650
patterns and we have an algorithm that's

365
00:14:46,600 --> 00:14:49,210
documented in the paper on how we

366
00:14:47,650 --> 00:14:53,699
actually construct it from those lexical

367
00:14:49,210 --> 00:14:58,930
syntactic patterns that we extracted and

368
00:14:53,700 --> 00:15:00,250
did you manually evaluate so we dumped

369
00:14:58,930 --> 00:15:03,160
it out looked at it and removed some of

370
00:15:00,250 --> 00:15:06,280
the bad edges that were extracted so we

371
00:15:03,160 --> 00:15:08,050
basically drew the had some software

372
00:15:06,280 --> 00:15:09,640
draw the ontology for us and looked at

373
00:15:08,050 --> 00:15:12,729
what the relationships were and pruned

374
00:15:09,640 --> 00:15:14,350
some of the bad edges but we did not

375
00:15:12,730 --> 00:15:16,330
perform like an accuracy on it because

376
00:15:14,350 --> 00:15:18,940
we did not have the ground truth to do

377
00:15:16,330 --> 00:15:20,320
so and in terms of releasing this we're

378
00:15:18,940 --> 00:15:22,330
playing on open sourcing and it's going

379
00:15:20,320 --> 00:15:26,230
through the process right now ok thank

380
00:15:22,330 --> 00:15:28,750
you lady in the back I'm Sarah with

381
00:15:26,230 --> 00:15:30,280
square my question was are you going to

382
00:15:28,750 --> 00:15:32,920
open-source it and you've just answered

383
00:15:30,280 --> 00:15:37,300
thank you yeah yeah great talk by the

384
00:15:32,920 --> 00:15:39,729
way thank you thank you yeah thank you

385
00:15:37,300 --> 00:15:41,349
that was a very interesting talk curious

386
00:15:39,730 --> 00:15:43,449
that these methods could be used to

387
00:15:41,350 --> 00:15:46,239
evaluate other natural language require

388
00:15:43,449 --> 00:15:48,579
it's you know like the C standard or

389
00:15:46,239 --> 00:15:52,269
rfcs or protocol specifications or

390
00:15:48,579 --> 00:15:54,189
whatever so while I haven't really

391
00:15:52,269 --> 00:15:56,829
looked at like for example RFC's in the

392
00:15:54,189 --> 00:15:58,719
language in there the the reason why our

393
00:15:56,829 --> 00:16:00,939
pattern based approach for extracting

394
00:15:58,720 --> 00:16:04,059
information works so well I believe for

395
00:16:00,939 --> 00:16:06,790
privacy policies is because when people

396
00:16:04,059 --> 00:16:08,108
are writing privacy policies they tend

397
00:16:06,790 --> 00:16:10,988
to follow some structure

398
00:16:08,109 --> 00:16:12,999
I assume that the RFC probably has some

399
00:16:10,989 --> 00:16:14,589
structure and language that you might be

400
00:16:12,999 --> 00:16:16,389
able to tap into to do the same type of

401
00:16:14,589 --> 00:16:18,790
information extraction but I've been

402
00:16:16,389 --> 00:16:19,959
really looked into you know what what

403
00:16:18,790 --> 00:16:22,269
type of language how they're defining

404
00:16:19,959 --> 00:16:23,199
things in RFC's so I can't answer with

405
00:16:22,269 --> 00:16:26,470
100% certainty

406
00:16:23,199 --> 00:16:33,219
okay thank you yep still have time for

407
00:16:26,470 --> 00:16:33,850
one more question did you see any

408
00:16:33,220 --> 00:16:36,609
indication

409
00:16:33,850 --> 00:16:39,579
besides the template generators of how

410
00:16:36,609 --> 00:16:41,739
those contradictions actually start

411
00:16:39,579 --> 00:16:44,049
where do they come from is it just

412
00:16:41,739 --> 00:16:46,660
simply that those policies are getting

413
00:16:44,049 --> 00:16:48,639
too complex for a single legal person to

414
00:16:46,660 --> 00:16:50,850
keep them all in their head do you have

415
00:16:48,639 --> 00:16:53,679
any indication did you study that so

416
00:16:50,850 --> 00:16:55,600
from from doing some of the manual

417
00:16:53,679 --> 00:16:58,869
validation one thing that I noticed is

418
00:16:55,600 --> 00:17:01,089
it seemed like a lot of like piece work

419
00:16:58,869 --> 00:17:03,129
that the developers were doing so these

420
00:17:01,089 --> 00:17:04,779
a lot of the findings well some of them

421
00:17:03,129 --> 00:17:07,209
were from big companies but a lot of the

422
00:17:04,779 --> 00:17:08,949
findings were from it looked like indie

423
00:17:07,209 --> 00:17:10,990
developers that basically released a

424
00:17:08,949 --> 00:17:12,669
popular app on the Play Store and they

425
00:17:10,990 --> 00:17:14,500
mean they probably don't have that legal

426
00:17:12,669 --> 00:17:17,199
team helping them create these policies

427
00:17:14,500 --> 00:17:18,880
so in my opinion there needs to be more

428
00:17:17,199 --> 00:17:20,079
research into automatically generating

429
00:17:18,880 --> 00:17:22,149
these policies based on behavior

430
00:17:20,079 --> 00:17:24,069
signatures or whatever trying to educate

431
00:17:22,148 --> 00:17:28,159
developers and help them create good

432
00:17:24,069 --> 00:17:33,369
policies okay essa Peka again

433
00:17:28,160 --> 00:17:33,369
[Applause]

