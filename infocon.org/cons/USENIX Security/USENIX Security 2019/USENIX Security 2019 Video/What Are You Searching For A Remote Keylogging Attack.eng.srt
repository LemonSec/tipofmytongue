1
00:00:10,590 --> 00:00:16,139
the introduction so this work is about

2
00:00:14,550 --> 00:00:18,300
trying to figure out what people are

3
00:00:16,139 --> 00:00:21,689
searching for on search engines with

4
00:00:18,300 --> 00:00:24,900
autocomplete so autocomplete is a

5
00:00:21,689 --> 00:00:26,730
feature found on all the major search

6
00:00:24,900 --> 00:00:28,590
engines that provides a list of

7
00:00:26,730 --> 00:00:32,009
suggested queries to the user as they

8
00:00:28,590 --> 00:00:34,500
type and this feature is intended to try

9
00:00:32,009 --> 00:00:36,390
to make the search results more relevant

10
00:00:34,500 --> 00:00:39,629
and provide those to the user quicker

11
00:00:36,390 --> 00:00:40,830
and so what autocomplete is doing is

12
00:00:39,629 --> 00:00:42,620
trying to predict what you're searching

13
00:00:40,830 --> 00:00:46,680
for before you finish typing your query

14
00:00:42,620 --> 00:00:49,230
now the key thing that this worked and

15
00:00:46,680 --> 00:00:51,510
I'll talk about looks at is that as

16
00:00:49,230 --> 00:00:54,569
you're typing your query into the search

17
00:00:51,510 --> 00:00:55,920
fields on the webpage this induces a

18
00:00:54,570 --> 00:00:58,230
bunch of network traffic between the

19
00:00:55,920 --> 00:01:01,200
client and server and despite this

20
00:00:58,230 --> 00:01:03,419
traffic being encrypted we'll see how

21
00:01:01,200 --> 00:01:04,500
it's some of the properties of this

22
00:01:03,420 --> 00:01:07,200
track traffic

23
00:01:04,500 --> 00:01:12,780
leak information about what that search

24
00:01:07,200 --> 00:01:15,929
query was so looking back over 20 years

25
00:01:12,780 --> 00:01:18,179
of network based side channels and all

26
00:01:15,929 --> 00:01:20,759
of these works leverage really two basic

27
00:01:18,179 --> 00:01:23,190
principles the first encryption

28
00:01:20,759 --> 00:01:26,069
preserves length at least in the case of

29
00:01:23,190 --> 00:01:28,050
web traffic it does so packet size can

30
00:01:26,069 --> 00:01:31,500
leak information about its contents and

31
00:01:28,050 --> 00:01:34,950
the second which isn't as widely talked

32
00:01:31,500 --> 00:01:37,440
about is that when you have a client

33
00:01:34,950 --> 00:01:39,690
talking to a server and it has to talk

34
00:01:37,440 --> 00:01:42,000
to the server with low latency the

35
00:01:39,690 --> 00:01:44,789
events that occur on the client are also

36
00:01:42,000 --> 00:01:46,979
preserved in the timing of the traffic

37
00:01:44,789 --> 00:01:48,390
between the client list servers so the

38
00:01:46,979 --> 00:01:50,009
time intervals between the events that

39
00:01:48,390 --> 00:01:52,110
occur and the clients such as keystrokes

40
00:01:50,009 --> 00:01:55,500
are preserved in the packet into arrival

41
00:01:52,110 --> 00:01:58,440
times and so all these works have

42
00:01:55,500 --> 00:02:00,239
leveraged these two basic principles and

43
00:01:58,440 --> 00:02:02,550
some former another and the work on

44
00:02:00,239 --> 00:02:04,140
describe today is no exception so

45
00:02:02,550 --> 00:02:10,530
looking at information leakage through

46
00:02:04,140 --> 00:02:13,890
both packet size and timing so just a

47
00:02:10,530 --> 00:02:15,720
quick overview of the attack the goal is

48
00:02:13,890 --> 00:02:17,519
to try to predict what query a user is

49
00:02:15,720 --> 00:02:19,500
searching for and this is looking only

50
00:02:17,520 --> 00:02:22,469
at traffic coming out of the client so

51
00:02:19,500 --> 00:02:23,490
only a bound traffic from the victim's

52
00:02:22,469 --> 00:02:26,370
machine

53
00:02:23,490 --> 00:02:28,440
and one of the key ideas here that I'll

54
00:02:26,370 --> 00:02:31,830
come back to later in the talk is that

55
00:02:28,440 --> 00:02:34,200
the attack combines multiple weak

56
00:02:31,830 --> 00:02:36,030
sources of information leakage so it's

57
00:02:34,200 --> 00:02:37,950
combining a couple different source of

58
00:02:36,030 --> 00:02:40,080
information leakage that and their own

59
00:02:37,950 --> 00:02:41,970
right really aren't a serious threat but

60
00:02:40,080 --> 00:02:45,150
when you put them together especially

61
00:02:41,970 --> 00:02:49,109
because they're independent they become

62
00:02:45,150 --> 00:02:52,010
a more serious concern and this idea is

63
00:02:49,110 --> 00:02:55,560
really embedded in machine learning

64
00:02:52,010 --> 00:02:57,899
analysis to the idea where we can create

65
00:02:55,560 --> 00:03:00,840
a more accurate classifier through an

66
00:02:57,900 --> 00:03:03,080
ensemble of weak ones

67
00:03:00,840 --> 00:03:05,760
so the in the threat model we assume the

68
00:03:03,080 --> 00:03:09,210
attacker can capture traffic coming

69
00:03:05,760 --> 00:03:11,340
directly out of the victims machine I'm

70
00:03:09,210 --> 00:03:14,130
assuming the victim is typing lowercase

71
00:03:11,340 --> 00:03:18,180
English letters plus the space key

72
00:03:14,130 --> 00:03:22,079
without any typos or backspace and that

73
00:03:18,180 --> 00:03:23,660
the autocomplete requests so the request

74
00:03:22,080 --> 00:03:26,610
actually retrieved that list of search

75
00:03:23,660 --> 00:03:31,440
suggestions are triggered on key down

76
00:03:26,610 --> 00:03:34,410
events in the webpage so the attack

77
00:03:31,440 --> 00:03:37,170
proceeds in five different steps and

78
00:03:34,410 --> 00:03:40,680
each step feeds into the next so kind of

79
00:03:37,170 --> 00:03:44,269
a a waterfall model beginning with the

80
00:03:40,680 --> 00:03:47,340
complete packet trace which includes

81
00:03:44,270 --> 00:03:49,890
requests to load the page itself another

82
00:03:47,340 --> 00:03:52,860
page assets and dynamic content we have

83
00:03:49,890 --> 00:03:54,450
to figure out which of those requests so

84
00:03:52,860 --> 00:03:57,120
which of that traffic actually

85
00:03:54,450 --> 00:03:58,619
correspond to keystrokes we want to then

86
00:03:57,120 --> 00:04:02,400
tokenize the quarries to figure out the

87
00:03:58,620 --> 00:04:04,770
size of each word in the query build up

88
00:04:02,400 --> 00:04:07,740
a dictionary of words for each token and

89
00:04:04,770 --> 00:04:09,390
eliminate possibility eliminate words

90
00:04:07,740 --> 00:04:10,890
from that dictionary based on

91
00:04:09,390 --> 00:04:12,869
information leaks through a header

92
00:04:10,890 --> 00:04:16,529
compression rank the words based on

93
00:04:12,870 --> 00:04:18,420
their timing and finally combine these

94
00:04:16,529 --> 00:04:21,809
real results with the language model to

95
00:04:18,420 --> 00:04:23,340
build up this hypothesis actually a list

96
00:04:21,810 --> 00:04:27,840
of hypothesis quarries that the user

97
00:04:23,340 --> 00:04:30,210
might have searched for him so first it

98
00:04:27,840 --> 00:04:33,960
would help to look at what's actually in

99
00:04:30,210 --> 00:04:36,859
these autocomplete requests so as you're

100
00:04:33,960 --> 00:04:40,250
typing your query into the search engine

101
00:04:36,860 --> 00:04:42,199
get requests is emitted every time you

102
00:04:40,250 --> 00:04:44,930
press ok on the keyboard

103
00:04:42,199 --> 00:04:47,780
so as you're building up the query we

104
00:04:44,930 --> 00:04:50,569
have a sequence of get requests we're

105
00:04:47,780 --> 00:04:54,080
just a single character is appended to

106
00:04:50,569 --> 00:04:56,300
the query itself that's being built up

107
00:04:54,080 --> 00:04:58,758
as you're typing it out and this

108
00:04:56,300 --> 00:05:02,629
partially completed query appears as a a

109
00:04:58,759 --> 00:05:04,520
parameter in the URL so the these

110
00:05:02,629 --> 00:05:06,500
packets pretty much everything else in

111
00:05:04,520 --> 00:05:09,430
these packets stays the same so if we

112
00:05:06,500 --> 00:05:12,710
look at the size of the sequence of

113
00:05:09,430 --> 00:05:14,419
requests the size of the packets is

114
00:05:12,710 --> 00:05:16,818
literally increasing since we only have

115
00:05:14,419 --> 00:05:20,659
the addition of a single character with

116
00:05:16,819 --> 00:05:23,210
each request and this makes the

117
00:05:20,659 --> 00:05:26,180
keystroke detection problem pretty easy

118
00:05:23,210 --> 00:05:29,030
actually so if we look at an actual page

119
00:05:26,180 --> 00:05:32,319
load for a user searching on Google for

120
00:05:29,030 --> 00:05:35,330
the term the lazy dog we see a bunch of

121
00:05:32,319 --> 00:05:38,300
requests here we're in this scatter plot

122
00:05:35,330 --> 00:05:40,880
each dot is a packet and we're looking

123
00:05:38,300 --> 00:05:42,680
at packet size over time so we see a

124
00:05:40,880 --> 00:05:44,330
bunch of traffic related to the page

125
00:05:42,680 --> 00:05:47,150
load itself and maybe some dynamic

126
00:05:44,330 --> 00:05:49,250
content and then it pause and then once

127
00:05:47,150 --> 00:05:53,029
the user starts typing the query we see

128
00:05:49,250 --> 00:05:54,680
a bunch more traffic and the problem of

129
00:05:53,029 --> 00:05:56,349
keystroke detection in this case are

130
00:05:54,680 --> 00:05:59,800
actually figuring out which of those

131
00:05:56,349 --> 00:06:02,120
requests are the autocomplete requests

132
00:05:59,800 --> 00:06:04,880
amounts are just looking for the longest

133
00:06:02,120 --> 00:06:07,969
increasing subsequence of packages and

134
00:06:04,880 --> 00:06:11,060
this is a classic problem in computer

135
00:06:07,969 --> 00:06:14,629
science that sounds pretty efficiently

136
00:06:11,060 --> 00:06:17,599
using dynamic programming so once we

137
00:06:14,629 --> 00:06:19,129
have the autocomplete requests we can

138
00:06:17,599 --> 00:06:21,800
then try to tokenize the quarry figure

139
00:06:19,129 --> 00:06:24,740
out the size of each word in that query

140
00:06:21,800 --> 00:06:27,319
and this is done by looking at the size

141
00:06:24,740 --> 00:06:31,550
difference of each request over the

142
00:06:27,319 --> 00:06:34,759
previous so if we look at the size of

143
00:06:31,550 --> 00:06:36,379
each packet over the previous when each

144
00:06:34,759 --> 00:06:38,750
character is added to the query the

145
00:06:36,379 --> 00:06:40,969
packet increases by byte but when a

146
00:06:38,750 --> 00:06:42,680
space is pressed and that's aged the

147
00:06:40,969 --> 00:06:45,830
query of this adds an additional 3 bytes

148
00:06:42,680 --> 00:06:47,599
because that space ends up being an

149
00:06:45,830 --> 00:06:50,000
escaped character in the URL so that

150
00:06:47,599 --> 00:06:50,780
turns into that percent to 0 so we can

151
00:06:50,000 --> 00:06:54,020
pretty

152
00:06:50,780 --> 00:06:56,599
easily figure out the size of each word

153
00:06:54,020 --> 00:07:00,620
in the query delineate words based on

154
00:06:56,600 --> 00:07:03,920
where those spaces occur so the next

155
00:07:00,620 --> 00:07:06,920
piece which I'll spends most of time on

156
00:07:03,920 --> 00:07:08,930
the talk on is information leaked

157
00:07:06,920 --> 00:07:11,030
through a header compression so first

158
00:07:08,930 --> 00:07:13,220
just a quick background on each pack

159
00:07:11,030 --> 00:07:16,219
which is the HTTP 2 header compression

160
00:07:13,220 --> 00:07:17,780
format which was developed as based on

161
00:07:16,220 --> 00:07:22,000
the speedy protocol and developed to

162
00:07:17,780 --> 00:07:25,159
mitigate a host of other compression

163
00:07:22,000 --> 00:07:27,770
side channel attacks the important thing

164
00:07:25,160 --> 00:07:31,669
here that I want to focus on is that an

165
00:07:27,770 --> 00:07:34,370
h-back string literals are encoded or

166
00:07:31,669 --> 00:07:36,680
compressed using a static Huffman

167
00:07:34,370 --> 00:07:39,380
encoding which means that we can look at

168
00:07:36,680 --> 00:07:41,510
the H pack specification which tells us

169
00:07:39,380 --> 00:07:43,969
exactly for each symbol and the string

170
00:07:41,510 --> 00:07:46,730
of wanna compress tells us exactly the

171
00:07:43,970 --> 00:07:49,280
bit string and sizeof bit string to

172
00:07:46,730 --> 00:07:51,140
compress that symbol and so to compress

173
00:07:49,280 --> 00:07:53,780
a string literal an H pack we just look

174
00:07:51,140 --> 00:07:57,349
up all the symbols and our string and

175
00:07:53,780 --> 00:07:59,150
concatenate they're encoded big strings

176
00:07:57,350 --> 00:08:01,460
together and that gives us our

177
00:07:59,150 --> 00:08:03,500
compressed string these Huffman code is

178
00:08:01,460 --> 00:08:08,630
prefix free so we just concatenate those

179
00:08:03,500 --> 00:08:11,060
together so previous work looked at how

180
00:08:08,630 --> 00:08:13,340
much information is leaked based on just

181
00:08:11,060 --> 00:08:15,020
knowing the total compressed size of a

182
00:08:13,340 --> 00:08:18,409
string compressed in this way where we

183
00:08:15,020 --> 00:08:21,169
have a preset table to compress our

184
00:08:18,410 --> 00:08:23,870
string so just knowing that this string

185
00:08:21,169 --> 00:08:26,599
here for example is 22 bits compresses

186
00:08:23,870 --> 00:08:28,479
to 22 bits it was determined that this

187
00:08:26,600 --> 00:08:30,680
leads relatively little information

188
00:08:28,479 --> 00:08:32,360
about the string itself so this doesn't

189
00:08:30,680 --> 00:08:37,089
really tell us much about what that

190
00:08:32,360 --> 00:08:40,190
string is that's because that 22 bits is

191
00:08:37,089 --> 00:08:44,380
just some linear combination of symbols

192
00:08:40,190 --> 00:08:47,780
in the encoding tables so we know that

193
00:08:44,380 --> 00:08:49,730
just that the compressed symbols have to

194
00:08:47,780 --> 00:08:52,990
add up to 22 bits and not much else so

195
00:08:49,730 --> 00:08:56,030
this doesn't leak much information

196
00:08:52,990 --> 00:08:58,070
however in order to complete we don't

197
00:08:56,030 --> 00:09:01,130
just see the total compressed size of

198
00:08:58,070 --> 00:09:02,850
the string that's in our query we see it

199
00:09:01,130 --> 00:09:04,800
after the addition of each

200
00:09:02,850 --> 00:09:06,380
character so if we look at the

201
00:09:04,800 --> 00:09:09,149
difference between consecutive

202
00:09:06,380 --> 00:09:12,480
autocomplete requests after they've been

203
00:09:09,149 --> 00:09:14,490
compressed we can see the size of the

204
00:09:12,480 --> 00:09:17,639
compressed symbol after it was appended

205
00:09:14,490 --> 00:09:21,000
to the string so in this case looking at

206
00:09:17,639 --> 00:09:23,699
the difference between our 22 bit string

207
00:09:21,000 --> 00:09:25,980
and 17 bit compressed string we see the

208
00:09:23,699 --> 00:09:28,920
new symbol and it was 5 bitch 5 bits

209
00:09:25,980 --> 00:09:31,529
which tells us that this has to be one

210
00:09:28,920 --> 00:09:36,959
of these six letters in the static

211
00:09:31,529 --> 00:09:40,490
Huffman table now in practice there's

212
00:09:36,959 --> 00:09:43,138
two caveats one is that we can't measure

213
00:09:40,490 --> 00:09:45,060
packet size differences with the

214
00:09:43,139 --> 00:09:47,300
granularity of bits we can only see byte

215
00:09:45,060 --> 00:09:51,119
size differences and the second is that

216
00:09:47,300 --> 00:09:54,089
the string literals that are compressed

217
00:09:51,120 --> 00:09:56,220
in this way all have some amount of

218
00:09:54,089 --> 00:09:58,500
padding which is unknown to the attacker

219
00:09:56,220 --> 00:10:00,269
so there's some appearing to align that

220
00:09:58,500 --> 00:10:03,870
compressed string to the nearest octet

221
00:10:00,269 --> 00:10:06,720
boundary but the same basic technique

222
00:10:03,870 --> 00:10:10,139
still works in practice and the way

223
00:10:06,720 --> 00:10:14,069
we'll use that is knowing the size of

224
00:10:10,139 --> 00:10:17,189
the token that we're trying to figure

225
00:10:14,069 --> 00:10:20,969
out what what it is in the search query

226
00:10:17,189 --> 00:10:24,000
we build up a dictionary of all the

227
00:10:20,970 --> 00:10:25,889
possible byte size different patterns

228
00:10:24,000 --> 00:10:27,930
for each word in our dictionary for that

229
00:10:25,889 --> 00:10:29,790
particular length and also taking into

230
00:10:27,930 --> 00:10:32,008
account each amount of padding for each

231
00:10:29,790 --> 00:10:34,829
word so we end up with eight not

232
00:10:32,009 --> 00:10:36,779
necessarily unique byte size different

233
00:10:34,829 --> 00:10:39,209
patterns in this case looking at the

234
00:10:36,779 --> 00:10:42,149
cumulative growth of the compressed

235
00:10:39,209 --> 00:10:44,790
string as you add each character to each

236
00:10:42,149 --> 00:10:48,120
word we end up with eight not this is

237
00:10:44,790 --> 00:10:51,719
not necessarily unique for each word and

238
00:10:48,120 --> 00:10:54,630
this only has to be done once now when

239
00:10:51,720 --> 00:10:57,300
we see a new byte size difference

240
00:10:54,630 --> 00:11:00,120
pattern in this sequence of autocomplete

241
00:10:57,300 --> 00:11:02,008
requests we take that and compared to

242
00:11:00,120 --> 00:11:04,410
our dictionary so we're just looking for

243
00:11:02,009 --> 00:11:06,569
an exact match to this dictionary that

244
00:11:04,410 --> 00:11:08,939
we've pre computed it'll match some

245
00:11:06,569 --> 00:11:11,099
words and not others the words that it

246
00:11:08,939 --> 00:11:13,260
doesn't match can be eliminated because

247
00:11:11,100 --> 00:11:15,480
we know that they couldn't possibly have

248
00:11:13,260 --> 00:11:16,470
been typed by the user and are not

249
00:11:15,480 --> 00:11:19,410
contained in the quarry

250
00:11:16,470 --> 00:11:22,020
so in this way we learn some information

251
00:11:19,410 --> 00:11:24,810
about what word that query might have

252
00:11:22,020 --> 00:11:29,400
contained by eliminating some of these

253
00:11:24,810 --> 00:11:33,089
possibilities so the next step and the

254
00:11:29,400 --> 00:11:39,689
attack is ranking words based on their

255
00:11:33,090 --> 00:11:42,630
timing so this relies on an idea which

256
00:11:39,690 --> 00:11:46,650
is been well-established which is that

257
00:11:42,630 --> 00:11:48,840
the timing of the intervals between key

258
00:11:46,650 --> 00:11:51,290
presses leak some information about what

259
00:11:48,840 --> 00:11:53,340
those keys are and this is because

260
00:11:51,290 --> 00:11:55,410
certain keys on the keyboard are

261
00:11:53,340 --> 00:11:57,020
generally pressed in quicker succession

262
00:11:55,410 --> 00:11:59,219
than others so keys that are far apart

263
00:11:57,020 --> 00:12:00,210
are usually pressed a little quicker

264
00:11:59,220 --> 00:12:03,150
than keys that are close together

265
00:12:00,210 --> 00:12:04,860
because you can process those keys in

266
00:12:03,150 --> 00:12:08,730
parallel so this is true for a touch

267
00:12:04,860 --> 00:12:11,040
typist not everyone so you can gain some

268
00:12:08,730 --> 00:12:12,900
information about what is contained in

269
00:12:11,040 --> 00:12:16,079
the quarry based on the timing of the

270
00:12:12,900 --> 00:12:17,730
packets because these are highly

271
00:12:16,080 --> 00:12:20,700
correlated to the timing of the key

272
00:12:17,730 --> 00:12:23,460
presses on the business machine so

273
00:12:20,700 --> 00:12:26,280
without going into great detail this

274
00:12:23,460 --> 00:12:28,530
step I'll just say that I look at using

275
00:12:26,280 --> 00:12:30,480
an independent data set train a

276
00:12:28,530 --> 00:12:32,819
bi-directional recurrent neural network

277
00:12:30,480 --> 00:12:36,480
to try to predict which keys are pressed

278
00:12:32,820 --> 00:12:38,700
based on the timing of the cue press

279
00:12:36,480 --> 00:12:41,250
intervals which are manifested in the

280
00:12:38,700 --> 00:12:45,480
timing of the packet the packet into

281
00:12:41,250 --> 00:12:47,850
arrival times so those word

282
00:12:45,480 --> 00:12:51,360
probabilities are then combined with the

283
00:12:47,850 --> 00:12:55,650
language models so in this case we're

284
00:12:51,360 --> 00:12:57,690
leveraging the low entropy of natural

285
00:12:55,650 --> 00:13:00,060
language to try to make some predictions

286
00:12:57,690 --> 00:13:03,180
about what that quarry was so this is

287
00:13:00,060 --> 00:13:05,219
really a game of trying to complete the

288
00:13:03,180 --> 00:13:07,890
sentence figure out which word is most

289
00:13:05,220 --> 00:13:09,470
likely to come next following a sequence

290
00:13:07,890 --> 00:13:16,140
of words that you've already predicted

291
00:13:09,470 --> 00:13:20,660
so this through a beam search gives us a

292
00:13:16,140 --> 00:13:23,900
list of hypothesis queries that closely

293
00:13:20,660 --> 00:13:26,130
resemble English language there's a

294
00:13:23,900 --> 00:13:28,199
parameter and the model allows you to

295
00:13:26,130 --> 00:13:29,730
trade-off weight between the language

296
00:13:28,200 --> 00:13:31,680
model and the time model

297
00:13:29,730 --> 00:13:33,930
which you can read more details about in

298
00:13:31,680 --> 00:13:35,939
the paper but the end result here is

299
00:13:33,930 --> 00:13:39,750
that we end up with this list of 50

300
00:13:35,940 --> 00:13:42,780
hypothesis queries that hopefully one of

301
00:13:39,750 --> 00:13:46,880
them contains the query that the user

302
00:13:42,780 --> 00:13:50,959
actually searched for him so to evaluate

303
00:13:46,880 --> 00:13:54,170
the overall success of the attack I

304
00:13:50,960 --> 00:13:57,360
automated data collection using actual

305
00:13:54,170 --> 00:14:01,589
keystrokes record from human subjects so

306
00:13:57,360 --> 00:14:04,500
these were people copying sentences and

307
00:14:01,590 --> 00:14:08,280
phrases from the Enron email corpus and

308
00:14:04,500 --> 00:14:12,690
billion word corpus and these were

309
00:14:08,280 --> 00:14:15,600
replayed using selenium and UI input and

310
00:14:12,690 --> 00:14:18,870
a low latency framework and then capture

311
00:14:15,600 --> 00:14:22,800
the traffic coming out of the victims

312
00:14:18,870 --> 00:14:24,210
machine to get all that autocomplete

313
00:14:22,800 --> 00:14:26,819
traffic along with everything else and

314
00:14:24,210 --> 00:14:30,030
figure out if we can predict what query

315
00:14:26,820 --> 00:14:32,340
a user is searching for it so it didn't

316
00:14:30,030 --> 00:14:34,500
experience any rate limiting or CAPTCHAs

317
00:14:32,340 --> 00:14:36,390
when I record the status set it took

318
00:14:34,500 --> 00:14:39,870
about a week presumably because it

319
00:14:36,390 --> 00:14:41,510
actually looked like human subjects are

320
00:14:39,870 --> 00:14:47,430
actually typing out queries into

321
00:14:41,510 --> 00:14:50,630
browsers so the keystroke detection and

322
00:14:47,430 --> 00:14:55,170
tokenization can be performed with

323
00:14:50,630 --> 00:14:57,030
surprisingly accurate so with near

324
00:14:55,170 --> 00:14:58,800
perfect accuracy we can pick out those

325
00:14:57,030 --> 00:15:02,160
autocomplete requests and figure out the

326
00:14:58,800 --> 00:15:04,620
size of each word in the query and this

327
00:15:02,160 --> 00:15:07,110
was looking at both Google and Baidu and

328
00:15:04,620 --> 00:15:11,550
in two different browsers across Chrome

329
00:15:07,110 --> 00:15:15,120
and Firefox the query identification

330
00:15:11,550 --> 00:15:18,390
results themselves are modest so if we

331
00:15:15,120 --> 00:15:20,460
look at the percentage of the time that

332
00:15:18,390 --> 00:15:23,939
the actual query is contained in that

333
00:15:20,460 --> 00:15:26,940
list of 15 hypotheses at 15 percent of

334
00:15:23,940 --> 00:15:29,280
time for Google and 13 third I do so

335
00:15:26,940 --> 00:15:31,260
Google actually uses header compression

336
00:15:29,280 --> 00:15:34,290
and Baidu does not so that's where that

337
00:15:31,260 --> 00:15:38,430
small bump comes from between the two

338
00:15:34,290 --> 00:15:41,939
major search engines and to give you an

339
00:15:38,430 --> 00:15:43,260
idea using a qualitative example looking

340
00:15:41,940 --> 00:15:44,760
at a query where users

341
00:15:43,260 --> 00:15:46,980
typing for he's recovering from a

342
00:15:44,760 --> 00:15:49,500
sprains in cases where it gets it right

343
00:15:46,980 --> 00:15:52,680
the predicted queries are either exactly

344
00:15:49,500 --> 00:15:56,300
the same or very close to what the true

345
00:15:52,680 --> 00:15:59,219
query was and failure cases occur

346
00:15:56,300 --> 00:16:02,729
prominently when either a keystroke is

347
00:15:59,220 --> 00:16:05,160
missed so when they false negative

348
00:16:02,730 --> 00:16:09,060
occurs in detection we end up with the

349
00:16:05,160 --> 00:16:12,030
wrong token lengths and when a false

350
00:16:09,060 --> 00:16:14,459
positive tokenization error occurs we

351
00:16:12,030 --> 00:16:18,390
also end up with the false the wrong

352
00:16:14,460 --> 00:16:20,280
token blanks so the attack outcome tend

353
00:16:18,390 --> 00:16:23,910
to be highly polarized it either got it

354
00:16:20,280 --> 00:16:27,750
really right or really wrong so finally

355
00:16:23,910 --> 00:16:30,329
to summarize I think this idea of

356
00:16:27,750 --> 00:16:34,440
combining multiple independent weakside

357
00:16:30,330 --> 00:16:37,860
channels is an interesting one and it

358
00:16:34,440 --> 00:16:39,930
definitely makes the problem of trying

359
00:16:37,860 --> 00:16:41,640
to mitigate these sources of information

360
00:16:39,930 --> 00:16:43,500
leakage much more difficult because you

361
00:16:41,640 --> 00:16:46,650
have to reason about now different

362
00:16:43,500 --> 00:16:50,790
sources that might be benign or might

363
00:16:46,650 --> 00:16:52,860
not seem so severe in themselves and

364
00:16:50,790 --> 00:16:54,860
this problem the language model turned

365
00:16:52,860 --> 00:16:57,030
out to be key so provide a lot of

366
00:16:54,860 --> 00:16:59,390
contribute a lot to the tax success

367
00:16:57,030 --> 00:17:01,709
which kind of underscores how pervasive

368
00:16:59,390 --> 00:17:04,560
predictability of human behaviors and

369
00:17:01,710 --> 00:17:05,370
all things cyber and finally the thing

370
00:17:04,560 --> 00:17:07,530
there's probably some other

371
00:17:05,369 --> 00:17:11,069
opportunities to look for where

372
00:17:07,530 --> 00:17:13,470
incremental compression occurs so

373
00:17:11,069 --> 00:17:16,020
looking at web applications with

374
00:17:13,470 --> 00:17:18,480
autosave feature or mapping services

375
00:17:16,020 --> 00:17:20,790
where you drag the map and you see that

376
00:17:18,480 --> 00:17:22,530
GPS coordinate and the URL changing

377
00:17:20,790 --> 00:17:26,280
incrementally with header compression

378
00:17:22,530 --> 00:17:28,560
does that leak information so thanks for

379
00:17:26,280 --> 00:17:30,389
listening to my talk and code is here

380
00:17:28,560 --> 00:17:34,520
and my contact information be happy to

381
00:17:30,390 --> 00:17:34,520
take questions thank you

382
00:17:38,200 --> 00:17:41,540
but hi

383
00:17:39,920 --> 00:17:44,690
Eric Tsang University of Washington

384
00:17:41,540 --> 00:17:46,070
great talk and great presentation I was

385
00:17:44,690 --> 00:17:48,170
wondering if you've thought much about

386
00:17:46,070 --> 00:17:50,720
other languages and in particular our

387
00:17:48,170 --> 00:17:54,320
logographic scripts like Chinese Korean

388
00:17:50,720 --> 00:17:57,290
Japanese just immune to this attack all

389
00:17:54,320 --> 00:18:00,050
right so I have thought about other

390
00:17:57,290 --> 00:18:07,220
languages I haven't pursued it in great

391
00:18:00,050 --> 00:18:11,629
detail potentially yes so I I'm not too

392
00:18:07,220 --> 00:18:14,300
familiar with how meaning if we're

393
00:18:11,630 --> 00:18:17,330
looking at Chinese characters versus

394
00:18:14,300 --> 00:18:18,620
pinyin would that make a difference if

395
00:18:17,330 --> 00:18:20,960
you're searching for Chinese characters

396
00:18:18,620 --> 00:18:23,600
how Unicode plays a role in this I think

397
00:18:20,960 --> 00:18:25,460
that's definitely an area of future

398
00:18:23,600 --> 00:18:28,399
research he had granted everything here

399
00:18:25,460 --> 00:18:30,110
was in English and search queries

400
00:18:28,400 --> 00:18:32,660
anybody do or probably predominantly

401
00:18:30,110 --> 00:18:34,070
non-english space so it's definitely

402
00:18:32,660 --> 00:18:39,860
something to look into

403
00:18:34,070 --> 00:18:41,210
Thanks thank you hello Bobo from

404
00:18:39,860 --> 00:18:44,570
Facebook great talk

405
00:18:41,210 --> 00:18:46,130
I was wondering if you maybe not in this

406
00:18:44,570 --> 00:18:48,290
paper but just in general if you've done

407
00:18:46,130 --> 00:18:50,810
any work looking at this for users of

408
00:18:48,290 --> 00:18:56,780
mobile devices or resolve this on

409
00:18:50,810 --> 00:19:00,020
laptops so yeah this was on a laptop so

410
00:18:56,780 --> 00:19:04,070
I haven't looked at mobile and

411
00:19:00,020 --> 00:19:07,460
presumably autocomplete works similar in

412
00:19:04,070 --> 00:19:10,909
a similar way on a mobile device I think

413
00:19:07,460 --> 00:19:13,910
one part of the attack that might not

414
00:19:10,910 --> 00:19:16,220
transfer over is the timing or

415
00:19:13,910 --> 00:19:17,420
potentially there's may be more

416
00:19:16,220 --> 00:19:19,270
opportunities for information leaves

417
00:19:17,420 --> 00:19:24,170
through timing so based on how users

418
00:19:19,270 --> 00:19:26,060
type into a soft keyboard or whether

419
00:19:24,170 --> 00:19:28,910
using swipe or not is probably something

420
00:19:26,060 --> 00:19:31,159
to take into consideration so yeah also

421
00:19:28,910 --> 00:19:33,320
in another interesting direction - thank

422
00:19:31,160 --> 00:19:36,650
you thank you so we have time for one

423
00:19:33,320 --> 00:19:39,470
more question sorry Dan q would it be

424
00:19:36,650 --> 00:19:43,430
possible to improve the accuracy of this

425
00:19:39,470 --> 00:19:45,950
significantly by repeating and the

426
00:19:43,430 --> 00:19:47,810
candidate phrases to the search engine

427
00:19:45,950 --> 00:19:48,660
and looking at the size of the

428
00:19:47,810 --> 00:19:51,330
autocomplete

429
00:19:48,660 --> 00:19:53,790
bump that comes back and is a direction

430
00:19:51,330 --> 00:19:56,970
you've considered going in and so there

431
00:19:53,790 --> 00:19:58,620
that was exactly what the the side

432
00:19:56,970 --> 00:20:00,390
channel leaks and web applications paper

433
00:19:58,620 --> 00:20:03,600
did is looked at the size of the server

434
00:20:00,390 --> 00:20:05,400
response it helps a lot if you can

435
00:20:03,600 --> 00:20:07,980
reproduce the size of the serve response

436
00:20:05,400 --> 00:20:10,620
which is not so easy in practice because

437
00:20:07,980 --> 00:20:13,260
that lists of suggestion queries is

438
00:20:10,620 --> 00:20:14,520
going to be based on geographic location

439
00:20:13,260 --> 00:20:17,190
your search history if you're signed

440
00:20:14,520 --> 00:20:19,410
into an account and so forth but if you

441
00:20:17,190 --> 00:20:20,760
can reproduce it then definitely so

442
00:20:19,410 --> 00:20:24,690
response provides a lot of information

443
00:20:20,760 --> 00:20:26,440
leaks a lot of informations alright

444
00:20:24,690 --> 00:20:32,579
let's think video again

445
00:20:26,440 --> 00:20:32,579
[Applause]

