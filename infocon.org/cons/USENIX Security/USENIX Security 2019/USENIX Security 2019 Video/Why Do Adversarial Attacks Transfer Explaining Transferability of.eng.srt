1
00:00:10,730 --> 00:00:15,440
good evening I'm under the montes from

2
00:00:13,580 --> 00:00:18,110
the University of Cagliari and his work

3
00:00:15,440 --> 00:00:22,430
is a joint collaboration with Batista

4
00:00:18,110 --> 00:00:25,009
beads of a parolee Marko MELAS Marla

5
00:00:22,430 --> 00:00:27,619
printer from the same university as mine

6
00:00:25,010 --> 00:00:30,050
add with a you know prayer Cristina

7
00:00:27,619 --> 00:00:32,689
Deaton Ataru and Metis janeski from the

8
00:00:30,050 --> 00:00:35,150
Northeast State University this work is

9
00:00:32,689 --> 00:00:37,910
called wide adverse side erotic transfer

10
00:00:35,150 --> 00:00:40,879
explaining transferability of a vision

11
00:00:37,910 --> 00:00:44,269
and poisoning attack so let's briefly

12
00:00:40,879 --> 00:00:50,170
recap our machine learning system work

13
00:00:44,269 --> 00:00:52,659
and where the attack can occur so a

14
00:00:50,170 --> 00:00:59,449
machine learning system is basically

15
00:00:52,659 --> 00:01:02,659
trained on a data set it extracts some

16
00:00:59,449 --> 00:01:05,059
features that are used to train the

17
00:01:02,659 --> 00:01:09,560
machine learning model then his machine

18
00:01:05,059 --> 00:01:12,110
learning model is used to classify new

19
00:01:09,560 --> 00:01:14,689
models so basically the input of the

20
00:01:12,110 --> 00:01:19,549
machine learning system are the training

21
00:01:14,689 --> 00:01:25,788
data and the test data and an attack can

22
00:01:19,549 --> 00:01:30,409
potentially change both of them so if

23
00:01:25,789 --> 00:01:33,579
the attack change some test sample the

24
00:01:30,409 --> 00:01:38,299
attack is called an invasion attack and

25
00:01:33,579 --> 00:01:41,449
in this case the goal of the attacker is

26
00:01:38,299 --> 00:01:46,579
to basically have that sample

27
00:01:41,450 --> 00:01:49,820
misclassified in a poisoning attack

28
00:01:46,579 --> 00:01:52,999
instead the attacker changed some of the

29
00:01:49,820 --> 00:01:55,669
training sample to increase as much as

30
00:01:52,999 --> 00:02:00,020
possible the error of the classifier at

31
00:01:55,670 --> 00:02:03,170
this time in our work we consider both a

32
00:02:00,020 --> 00:02:05,270
vision and poisoning attack and we

33
00:02:03,170 --> 00:02:08,810
consider two different level of

34
00:02:05,270 --> 00:02:10,910
knowledge of the attacker we consider a

35
00:02:08,810 --> 00:02:13,340
white box inari oh well the attacker

36
00:02:10,910 --> 00:02:16,010
have a full knowledge of the machine

37
00:02:13,340 --> 00:02:18,530
learning system so you know everything

38
00:02:16,010 --> 00:02:22,579
about it the training data the weight of

39
00:02:18,530 --> 00:02:24,170
the model etc and we consider also a

40
00:02:22,580 --> 00:02:29,180
black box scenario

41
00:02:24,170 --> 00:02:31,609
where the attacker basically just

42
00:02:29,180 --> 00:02:35,569
aquarius's to the model which is a

43
00:02:31,610 --> 00:02:38,810
scenario more more likely to happen

44
00:02:35,569 --> 00:02:44,089
because if for example an attacker would

45
00:02:38,810 --> 00:02:46,400
like to attack antivirus it probably

46
00:02:44,090 --> 00:02:53,840
will not know all the detail about the

47
00:02:46,400 --> 00:02:56,450
system so in this case what the attacker

48
00:02:53,840 --> 00:03:01,300
can do so if it doesn't have knowledge

49
00:02:56,450 --> 00:03:04,070
about his target model is to learn a

50
00:03:01,300 --> 00:03:06,530
surrogate model so a different machine

51
00:03:04,070 --> 00:03:08,959
learning model a craft is attack against

52
00:03:06,530 --> 00:03:11,180
the surrogate and we try to use dis

53
00:03:08,959 --> 00:03:15,230
attack against establish target

54
00:03:11,180 --> 00:03:18,110
basically and if the attack is effective

55
00:03:15,230 --> 00:03:20,600
against the target is it is said that he

56
00:03:18,110 --> 00:03:23,090
attack transfer so basically transfer

57
00:03:20,600 --> 00:03:25,190
ability is the ability of an attack

58
00:03:23,090 --> 00:03:27,560
which is devised against one machine

59
00:03:25,190 --> 00:03:32,780
learning system to be effective against

60
00:03:27,560 --> 00:03:34,880
a different one the problem at the state

61
00:03:32,780 --> 00:03:38,930
of the art regarding transfer ability

62
00:03:34,880 --> 00:03:42,109
where that there were some basically

63
00:03:38,930 --> 00:03:45,260
some empirical evidence that evasion

64
00:03:42,110 --> 00:03:48,290
attacks sometimes transfer but it was

65
00:03:45,260 --> 00:03:50,780
totally unclear which are the main

66
00:03:48,290 --> 00:03:55,790
factor behind the transferability of

67
00:03:50,780 --> 00:04:01,579
evasion attack and moreover it was not

68
00:03:55,790 --> 00:04:06,590
clear why basically the transfer so in

69
00:04:01,579 --> 00:04:09,440
which contest the transfer so we propose

70
00:04:06,590 --> 00:04:11,720
an optimization framework that allows

71
00:04:09,440 --> 00:04:16,850
the attacker to compute both a visual

72
00:04:11,720 --> 00:04:19,488
and poisoning attack we provide a formal

73
00:04:16,850 --> 00:04:22,310
definition of transfer ability and we

74
00:04:19,488 --> 00:04:24,650
devise three metrics that help to better

75
00:04:22,310 --> 00:04:28,850
understand the underlying reason we are

76
00:04:24,650 --> 00:04:31,310
in transfer ability to validate the

77
00:04:28,850 --> 00:04:32,960
proposal metric we need that

78
00:04:31,310 --> 00:04:37,550
comprehensive experimental evaluation

79
00:04:32,960 --> 00:04:40,280
and finally we study the relation

80
00:04:37,550 --> 00:04:43,940
ship between transferability and model

81
00:04:40,280 --> 00:04:48,229
complexity so let's see why we choose to

82
00:04:43,940 --> 00:04:51,380
study teens relationship so we defined

83
00:04:48,230 --> 00:04:55,220
modern complexity as the capacity of the

84
00:04:51,380 --> 00:04:58,930
classifier to fit the training data that

85
00:04:55,220 --> 00:05:05,420
can be controlled through regularization

86
00:04:58,930 --> 00:05:07,820
so suppose that you have a point one

87
00:05:05,420 --> 00:05:10,550
dimensional point and you would like to

88
00:05:07,820 --> 00:05:13,460
craft an evasion attack against a

89
00:05:10,550 --> 00:05:15,890
classifier so in this case the loss of

90
00:05:13,460 --> 00:05:20,359
the attacker is exactly the loss of the

91
00:05:15,890 --> 00:05:23,810
classifier on that point and if the

92
00:05:20,360 --> 00:05:27,350
adversary point is under the black line

93
00:05:23,810 --> 00:05:29,630
it means that the example basically is

94
00:05:27,350 --> 00:05:32,510
not able to evade the classifier it is

95
00:05:29,630 --> 00:05:38,000
above it means that that it is able to

96
00:05:32,510 --> 00:05:40,310
evade the classifier so as the loss

97
00:05:38,000 --> 00:05:44,350
function of the attacker depends

98
00:05:40,310 --> 00:05:49,520
strongly on the classifier we suppose

99
00:05:44,350 --> 00:05:52,400
that the in the complexity of the

100
00:05:49,520 --> 00:05:56,299
classifier have a strong influence on

101
00:05:52,400 --> 00:06:01,190
the complexity of this loss function so

102
00:05:56,300 --> 00:06:04,010
here we are considering an eye

103
00:06:01,190 --> 00:06:05,780
complexity surrogate on the left and a

104
00:06:04,010 --> 00:06:10,520
low complexity surrogate on the right

105
00:06:05,780 --> 00:06:15,469
and we crafted the adversarial point so

106
00:06:10,520 --> 00:06:17,990
it is a vision point against both so the

107
00:06:15,470 --> 00:06:22,960
adversarial pointer is represented with

108
00:06:17,990 --> 00:06:27,110
the point actually red and blue so

109
00:06:22,960 --> 00:06:30,590
regarding the adversarial point computed

110
00:06:27,110 --> 00:06:34,340
against the eye complexity classifier we

111
00:06:30,590 --> 00:06:39,229
can see that it's basically a local

112
00:06:34,340 --> 00:06:41,390
optimum instead in case of the point

113
00:06:39,230 --> 00:06:43,640
which is devised against the low

114
00:06:41,390 --> 00:06:46,780
complexity surrogate we can see which is

115
00:06:43,640 --> 00:06:46,780
the global optimum

116
00:06:48,460 --> 00:06:54,440
this of course he does to the fact that

117
00:06:51,140 --> 00:06:57,530
the function on the right it's really

118
00:06:54,440 --> 00:07:02,630
smoother and nicer so it's easier to

119
00:06:57,530 --> 00:07:05,900
optimize so if we try to transfer to the

120
00:07:02,630 --> 00:07:09,710
attack against different target we see

121
00:07:05,900 --> 00:07:12,349
that the point that we devised against

122
00:07:09,710 --> 00:07:16,909
the I complexity pseudo gate doesn't

123
00:07:12,350 --> 00:07:19,130
transfer even with another classifier

124
00:07:16,910 --> 00:07:20,570
another eye complexity classifier that

125
00:07:19,130 --> 00:07:25,760
may be trained and all the different

126
00:07:20,570 --> 00:07:28,190
training data set instead we see that

127
00:07:25,760 --> 00:07:30,560
the point which was computed against the

128
00:07:28,190 --> 00:07:33,290
low complexity classifier is more likely

129
00:07:30,560 --> 00:07:41,540
to evade both low complexity classifier

130
00:07:33,290 --> 00:07:44,510
and high complexity target so we

131
00:07:41,540 --> 00:07:46,790
proposed a definition of transfer

132
00:07:44,510 --> 00:07:50,890
ability which is the following so we

133
00:07:46,790 --> 00:07:50,890
define transferability as the loss

134
00:07:51,340 --> 00:07:58,900
achieved by the target classifier which

135
00:07:54,290 --> 00:08:02,470
is parameterized by W on a point on an

136
00:07:58,900 --> 00:08:04,940
adversarial point X star which is

137
00:08:02,470 --> 00:08:07,130
basically a test point X plus a

138
00:08:04,940 --> 00:08:12,320
perturbation which is plotted on the

139
00:08:07,130 --> 00:08:14,510
surrogate so here we have our target for

140
00:08:12,320 --> 00:08:16,599
example and we would like to compute an

141
00:08:14,510 --> 00:08:20,020
efficient attack so we would like to

142
00:08:16,600 --> 00:08:24,620
have the sample that a sample eight

143
00:08:20,020 --> 00:08:26,330
misclassified as a nine the attacker

144
00:08:24,620 --> 00:08:30,950
doesn't know the target

145
00:08:26,330 --> 00:08:32,840
you know just the surrogate serve what

146
00:08:30,950 --> 00:08:34,909
he can do is compute the attack against

147
00:08:32,840 --> 00:08:40,340
the surrogate and how you can do it can

148
00:08:34,909 --> 00:08:42,500
use a gradient based algorithm and the

149
00:08:40,340 --> 00:08:48,680
first gradient based a vision and

150
00:08:42,500 --> 00:08:53,920
poisoning algorithm were devised in the

151
00:08:48,680 --> 00:08:57,589
2030 and 2012 by bj at all and then

152
00:08:53,920 --> 00:09:01,400
there have been a lot of work regarding

153
00:08:57,590 --> 00:09:06,860
evasion after while there is last word

154
00:09:01,400 --> 00:09:10,250
in poisoning but going back to our

155
00:09:06,860 --> 00:09:14,750
example if we suppose that the attack

156
00:09:10,250 --> 00:09:16,550
can make just one iteration the optimal

157
00:09:14,750 --> 00:09:18,500
total mission in this case will be the

158
00:09:16,550 --> 00:09:22,160
one which is aligned with the gradient

159
00:09:18,500 --> 00:09:24,650
of the surrogate and we will found his

160
00:09:22,160 --> 00:09:26,959
adversarial sample that in this case is

161
00:09:24,650 --> 00:09:30,020
not able to transfer so he basically

162
00:09:26,960 --> 00:09:34,120
gone through the decision region of the

163
00:09:30,020 --> 00:09:34,120
surrogate but not the one of the target

164
00:09:35,050 --> 00:09:41,540
so if we consider how much the lost

165
00:09:38,300 --> 00:09:47,300
increment on the target due to the

166
00:09:41,540 --> 00:09:49,599
perturbation which is computed on the

167
00:09:47,300 --> 00:09:52,849
surrogate so which is computed on the

168
00:09:49,600 --> 00:09:59,570
black box case and we have there like

169
00:09:52,850 --> 00:10:02,570
Delta L beta Vivek that stay for black

170
00:09:59,570 --> 00:10:05,450
box and we multiply and divide it for

171
00:10:02,570 --> 00:10:10,820
the Delta that we have in case we

172
00:10:05,450 --> 00:10:15,200
compute basically the attack directly on

173
00:10:10,820 --> 00:10:17,870
the target we get this formula which is

174
00:10:15,200 --> 00:10:21,370
made by two different factor that are

175
00:10:17,870 --> 00:10:24,910
the first two metric that we proposed so

176
00:10:21,370 --> 00:10:28,400
let's see them one by one more in detail

177
00:10:24,910 --> 00:10:32,120
so let's start from the sites of the

178
00:10:28,400 --> 00:10:36,530
input gradient so it basically evaluate

179
00:10:32,120 --> 00:10:39,140
how much the lost increment on the

180
00:10:36,530 --> 00:10:42,050
target classifier under the attack so it

181
00:10:39,140 --> 00:10:44,120
gave you a measure of the vulnerability

182
00:10:42,050 --> 00:10:47,240
of the target classifier basically and

183
00:10:44,120 --> 00:10:49,940
the intuition behind this matrix is that

184
00:10:47,240 --> 00:10:55,130
it captured the sensitivity of the loss

185
00:10:49,940 --> 00:10:57,200
function to input perturbation the

186
00:10:55,130 --> 00:11:02,570
second metric is the gradient alignment

187
00:10:57,200 --> 00:11:06,230
and it basically evaluate the ratio

188
00:11:02,570 --> 00:11:08,300
between the increment of the loss if we

189
00:11:06,230 --> 00:11:11,030
compute the attack in the black box case

190
00:11:08,300 --> 00:11:13,939
respect to the one if we compute the

191
00:11:11,030 --> 00:11:15,170
attack in the white box case and we call

192
00:11:13,940 --> 00:11:18,489
it Burundi and Philon

193
00:11:15,170 --> 00:11:23,779
because that's basically the cosine of

194
00:11:18,489 --> 00:11:26,119
the gradient of the node lost basically

195
00:11:23,779 --> 00:11:28,249
that we are if we consider the target

196
00:11:26,119 --> 00:11:33,709
and the one that we have if we consider

197
00:11:28,249 --> 00:11:35,959
the surrogate the third metric is the

198
00:11:33,709 --> 00:11:39,709
variability of the surrogate lost

199
00:11:35,959 --> 00:11:43,099
landscape so it basically measure how

200
00:11:39,709 --> 00:11:47,479
much the loss function change depending

201
00:11:43,100 --> 00:11:50,239
on the training data set so with a

202
00:11:47,480 --> 00:11:52,669
versus metric because typically the

203
00:11:50,239 --> 00:11:54,739
attacker doesn't have access to the

204
00:11:52,669 --> 00:11:57,579
training dataset so it have to use a

205
00:11:54,739 --> 00:11:57,579
different one

206
00:12:02,280 --> 00:12:08,670
we did experiment on different data set

207
00:12:05,010 --> 00:12:12,480
a toy data set which is missed and we

208
00:12:08,670 --> 00:12:15,839
did the a vision experiment on Android

209
00:12:12,480 --> 00:12:18,150
malware detection task while we did the

210
00:12:15,840 --> 00:12:20,790
depositing experiment on a face

211
00:12:18,150 --> 00:12:23,819
verification task we can see there are a

212
00:12:20,790 --> 00:12:27,420
lot of different classifier like region

213
00:12:23,820 --> 00:12:29,970
logistic SVM neural network etc and for

214
00:12:27,420 --> 00:12:31,620
each classifier we consider an eye

215
00:12:29,970 --> 00:12:35,310
complexity version and the lower

216
00:12:31,620 --> 00:12:38,250
complexity version we did different

217
00:12:35,310 --> 00:12:41,579
experiment in white box scenario black

218
00:12:38,250 --> 00:12:44,100
box scenario and we compute the

219
00:12:41,580 --> 00:12:48,390
correlation between the proposal metric

220
00:12:44,100 --> 00:12:49,710
and the transferability we analyzed also

221
00:12:48,390 --> 00:12:52,620
the relationship with the model

222
00:12:49,710 --> 00:12:56,130
complexity and we made some statistical

223
00:12:52,620 --> 00:13:01,440
tests to validate our hypothesis and the

224
00:12:56,130 --> 00:13:06,030
correlations that we found so let's see

225
00:13:01,440 --> 00:13:10,530
just some experiment so an interesting

226
00:13:06,030 --> 00:13:12,959
research question is that okay are the

227
00:13:10,530 --> 00:13:15,390
target classifier with lag algorithm

228
00:13:12,960 --> 00:13:18,510
larger input gradient more vulnerable

229
00:13:15,390 --> 00:13:23,160
because basically it is waltz one of the

230
00:13:18,510 --> 00:13:26,790
metric that we devised so the problem is

231
00:13:23,160 --> 00:13:28,890
that we devised it under a linearity

232
00:13:26,790 --> 00:13:31,339
assumption so it may not old

233
00:13:28,890 --> 00:13:34,380
and we'd verify experimentally that

234
00:13:31,340 --> 00:13:37,550
actually it holds so here

235
00:13:34,380 --> 00:13:41,550
that's a white box scenario and

236
00:13:37,550 --> 00:13:46,290
basically we have we are measuring the

237
00:13:41,550 --> 00:13:51,060
evasion rate when the power of the

238
00:13:46,290 --> 00:13:53,640
attacker increases so here the power of

239
00:13:51,060 --> 00:13:56,969
attack the attacker basically is the

240
00:13:53,640 --> 00:14:00,390
number of features that he can change we

241
00:13:56,970 --> 00:14:02,910
have one color for each different family

242
00:14:00,390 --> 00:14:05,430
of classifier and we have represented

243
00:14:02,910 --> 00:14:09,000
with the solid line the eye complexity

244
00:14:05,430 --> 00:14:14,439
classifier and with the dotted line the

245
00:14:09,000 --> 00:14:16,980
low complexity classifier so we see the

246
00:14:14,440 --> 00:14:21,000
the eye complexity classifier are

247
00:14:16,980 --> 00:14:24,009
basically always more vulnerable and

248
00:14:21,000 --> 00:14:28,509
looking at the gradient sides we see

249
00:14:24,009 --> 00:14:31,120
that effectively the classifier that are

250
00:14:28,509 --> 00:14:36,100
the type of a nagger complexity even a

251
00:14:31,120 --> 00:14:39,100
gradient sights so basically our

252
00:14:36,100 --> 00:14:41,560
complexity model ever-larger gradient

253
00:14:39,100 --> 00:14:44,430
and target with a larger gradient are

254
00:14:41,560 --> 00:14:44,430
more vulnerable

255
00:14:46,230 --> 00:14:54,519
another interesting research question is

256
00:14:49,420 --> 00:14:57,490
related to the gradient alignment so we

257
00:14:54,519 --> 00:15:01,569
asked here if the gradient alignment is

258
00:14:57,490 --> 00:15:04,540
correlated with the difference of the

259
00:15:01,569 --> 00:15:08,339
perturbation computed considering the

260
00:15:04,540 --> 00:15:08,339
target and the surrogate model

261
00:15:12,640 --> 00:15:18,140
we made this experiment always because

262
00:15:15,830 --> 00:15:20,570
we have devised also the gradient

263
00:15:18,140 --> 00:15:26,600
alignment metric with a linearity

264
00:15:20,570 --> 00:15:30,260
assumption so we can see that in this

265
00:15:26,600 --> 00:15:33,399
matrices there is a strongly common

266
00:15:30,260 --> 00:15:35,779
pattern so they are basically correlated

267
00:15:33,399 --> 00:15:41,660
the measure of course are basically

268
00:15:35,779 --> 00:15:48,010
correlated so we have talked just about

269
00:15:41,660 --> 00:15:51,860
a vision attack but it turned out that

270
00:15:48,010 --> 00:15:56,470
regarding the poisoning the findings are

271
00:15:51,860 --> 00:15:59,089
basically the same so here we have

272
00:15:56,470 --> 00:16:02,270
something more which they have in common

273
00:15:59,089 --> 00:16:05,660
so here we are basically some other

274
00:16:02,270 --> 00:16:08,120
subtle example which is devised against

275
00:16:05,660 --> 00:16:12,020
low complexity classifier and high

276
00:16:08,120 --> 00:16:15,920
complexity classifier so we can see that

277
00:16:12,020 --> 00:16:18,740
yet personal example which are computed

278
00:16:15,920 --> 00:16:24,140
against lower complexity classifier are

279
00:16:18,740 --> 00:16:27,740
really more perturbed and these evaluate

280
00:16:24,140 --> 00:16:30,050
the policies that we did at the

281
00:16:27,740 --> 00:16:32,930
beginning that when we are talkin a

282
00:16:30,050 --> 00:16:36,140
complexity classifier we may end up

283
00:16:32,930 --> 00:16:41,800
basically with an adversarial point that

284
00:16:36,140 --> 00:16:41,800
correspond to a local optimum a

285
00:16:42,490 --> 00:16:49,940
difference with the division is that

286
00:16:46,540 --> 00:16:52,520
basically in case of poisoning the best

287
00:16:49,940 --> 00:16:59,600
surrogate are the one which have the

288
00:16:52,520 --> 00:17:05,859
same complexity as the target so to

289
00:16:59,600 --> 00:17:08,390
summarize we proposed a definition of

290
00:17:05,859 --> 00:17:11,329
transferability and different metric

291
00:17:08,390 --> 00:17:15,470
that help to better understand how

292
00:17:11,329 --> 00:17:17,119
transferability work and moreover we

293
00:17:15,470 --> 00:17:19,780
study the relationship with the

294
00:17:17,119 --> 00:17:19,780
complexity

295
00:17:21,170 --> 00:17:30,590
we made a lot of experiment and the main

296
00:17:26,750 --> 00:17:33,590
finding are that idle complexity model

297
00:17:30,590 --> 00:17:37,730
are more vulnerable to both a vision and

298
00:17:33,590 --> 00:17:40,250
poisoning attack low-complexity model

299
00:17:37,730 --> 00:17:45,020
are the better surrogate the best

300
00:17:40,250 --> 00:17:48,890
surrogate for a vision attack while in

301
00:17:45,020 --> 00:17:50,299
case of poisoning the best surrogate are

302
00:17:48,890 --> 00:17:55,309
the one which have almost the same

303
00:17:50,299 --> 00:18:02,389
complexity as the target we release the

304
00:17:55,309 --> 00:18:06,350
code which is available at this link so

305
00:18:02,390 --> 00:18:16,340
if you have any question I will be

306
00:18:06,350 --> 00:18:17,990
pleased to answer them thank you we have

307
00:18:16,340 --> 00:18:21,139
time for probably one question

308
00:18:17,990 --> 00:18:23,720
yes thanks Rachel I made have missed

309
00:18:21,140 --> 00:18:26,960
something but I was trying to understand

310
00:18:23,720 --> 00:18:28,700
the low complexity models are better

311
00:18:26,960 --> 00:18:30,380
surrogates it would seem that like with

312
00:18:28,700 --> 00:18:32,720
a low complexity model like you're gonna

313
00:18:30,380 --> 00:18:34,640
do you're gonna be more likely of having

314
00:18:32,720 --> 00:18:37,549
your evasion succeed but it also seems

315
00:18:34,640 --> 00:18:39,740
like if you're too low complexity you

316
00:18:37,549 --> 00:18:41,389
might be you know your evasion might end

317
00:18:39,740 --> 00:18:44,090
up being higher cost than it would

318
00:18:41,390 --> 00:18:51,290
otherwise need to be is that something

319
00:18:44,090 --> 00:18:55,629
that you saw complexity more there are

320
00:18:51,290 --> 00:18:55,629
better so located in case of a vision

321
00:18:58,150 --> 00:19:05,690
regarding the cost what does what you

322
00:19:02,179 --> 00:19:07,730
mean exactly right like well do you do

323
00:19:05,690 --> 00:19:09,590
you end up evading more than you need to

324
00:19:07,730 --> 00:19:12,230
if you're if your model is lower

325
00:19:09,590 --> 00:19:15,379
complexity than it had to be is my

326
00:19:12,230 --> 00:19:19,970
question if you if you're trying okay so

327
00:19:15,380 --> 00:19:23,690
basically we are not considering

328
00:19:19,970 --> 00:19:27,230
me moon perturb Edyta okay we are we are

329
00:19:23,690 --> 00:19:31,490
considering well we have the T slot here

330
00:19:27,230 --> 00:19:33,410
so we consider the vulnerability we with

331
00:19:31,490 --> 00:19:37,310
an increasing attack power because

332
00:19:33,410 --> 00:19:41,150
usually in the application well you

333
00:19:37,310 --> 00:19:43,010
doesn't know exactly which is the

334
00:19:41,150 --> 00:19:48,290
perturbation that the attacker is

335
00:19:43,010 --> 00:19:50,120
allowed to inject right okay so even for

336
00:19:48,290 --> 00:19:55,960
example this was in case of Android

337
00:19:50,120 --> 00:20:01,429
malware detection so it make more sense

338
00:19:55,960 --> 00:20:05,690
see the vulnerability I mean when the

339
00:20:01,430 --> 00:20:10,760
attacker power can change with the added

340
00:20:05,690 --> 00:20:13,910
complexity model I mean if you have yes

341
00:20:10,760 --> 00:20:17,240
if you have the same amount of allow

342
00:20:13,910 --> 00:20:20,150
adverse perturbation you can get with a

343
00:20:17,240 --> 00:20:22,420
sample which is less modified right yeah

344
00:20:20,150 --> 00:20:24,360
yes yes thanks that's true

345
00:20:22,420 --> 00:20:29,770
okay let's thank Umbra

346
00:20:24,360 --> 00:20:29,770
[Applause]

