1
00:00:07,200 --> 00:00:09,360
hello i'm basilius manfredis pg student

2
00:00:09,360 --> 00:00:10,719
university college london

3
00:00:10,719 --> 00:00:12,639
i'm going to talk about optimization for

4
00:00:12,639 --> 00:00:14,480
attack surface exploration

5
00:00:14,480 --> 00:00:16,160
and in particular focus on the case of

6
00:00:16,160 --> 00:00:18,560
tls this joint work with jamie hayes

7
00:00:18,560 --> 00:00:21,680
from university college loan

8
00:00:22,960 --> 00:00:25,039
to begin with attack surface exploration

9
00:00:25,039 --> 00:00:26,320
is a process that

10
00:00:26,320 --> 00:00:28,240
given a system or protocol we first

11
00:00:28,240 --> 00:00:30,160
start its operation

12
00:00:30,160 --> 00:00:31,679
security properties and the trust

13
00:00:31,679 --> 00:00:33,680
assumptions it operates on

14
00:00:33,680 --> 00:00:35,440
then define the adversaries and their

15
00:00:35,440 --> 00:00:37,600
calls and then given those adversaries

16
00:00:37,600 --> 00:00:39,280
we design and realize

17
00:00:39,280 --> 00:00:41,440
attacks that we can later further

18
00:00:41,440 --> 00:00:43,840
optimize

19
00:00:43,840 --> 00:00:45,760
the goal of actually coming up with

20
00:00:45,760 --> 00:00:47,680
attacks that we will later

21
00:00:47,680 --> 00:00:50,320
optimize uh has to do with us advancing

22
00:00:50,320 --> 00:00:52,000
the state of the art so as to better

23
00:00:52,000 --> 00:00:53,680
evaluate the security

24
00:00:53,680 --> 00:00:56,719
of the given system or protocol as well

25
00:00:56,719 --> 00:00:57,440
as

26
00:00:57,440 --> 00:00:59,840
further improving our defenses uh once

27
00:00:59,840 --> 00:01:01,520
we have a good understanding of

28
00:01:01,520 --> 00:01:03,280
what they are doing well and where we

29
00:01:03,280 --> 00:01:04,959
are actually failing

30
00:01:04,959 --> 00:01:07,840
this process is iterative meaning that

31
00:01:07,840 --> 00:01:10,159
given an attack we

32
00:01:10,159 --> 00:01:13,040
always go back to the beginning verify

33
00:01:13,040 --> 00:01:14,320
half as entities

34
00:01:14,320 --> 00:01:16,560
seconds false false leaves uh false

35
00:01:16,560 --> 00:01:17,759
negatives

36
00:01:17,759 --> 00:01:20,960
uh and then perhaps revisit the

37
00:01:20,960 --> 00:01:23,360
subjects with regards to our adversaries

38
00:01:23,360 --> 00:01:25,600
um check if we can relax and see if we

39
00:01:25,600 --> 00:01:27,840
can do anything better

40
00:01:27,840 --> 00:01:29,920
this process is a best effort process

41
00:01:29,920 --> 00:01:33,759
and usually requires human involvement

42
00:01:33,840 --> 00:01:35,600
modern machine learning can actually

43
00:01:35,600 --> 00:01:37,280
help us advance

44
00:01:37,280 --> 00:01:40,880
further state of the art

45
00:01:40,880 --> 00:01:42,640
if we incorporate it into this

46
00:01:42,640 --> 00:01:44,240
optimization process

47
00:01:44,240 --> 00:01:45,920
and here's the added benefit that it can

48
00:01:45,920 --> 00:01:47,680
minimize human involvement

49
00:01:47,680 --> 00:01:51,200
however for that to be possible there

50
00:01:51,200 --> 00:01:52,640
are a couple of requirements that

51
00:01:52,640 --> 00:01:54,799
need to be met the first one is that it

52
00:01:54,799 --> 00:01:56,640
should be easier to collect data

53
00:01:56,640 --> 00:01:58,320
in training model than to optimize the

54
00:01:58,320 --> 00:01:59,680
attack manually

55
00:01:59,680 --> 00:02:02,240
and the second is that well given an

56
00:02:02,240 --> 00:02:04,159
adversary the goal of that adversary

57
00:02:04,159 --> 00:02:05,840
needs to be expressed

58
00:02:05,840 --> 00:02:08,959
in some form that makes sense for the

59
00:02:08,959 --> 00:02:10,239
machine learning model

60
00:02:10,239 --> 00:02:12,959
in that case is a differentiable loss

61
00:02:12,959 --> 00:02:15,360
function

62
00:02:15,440 --> 00:02:16,959
our use case as i said earlier is

63
00:02:16,959 --> 00:02:19,200
traffic fingerprinting

64
00:02:19,200 --> 00:02:20,560
there are two types that we're going to

65
00:02:20,560 --> 00:02:22,319
consider one is webpage

66
00:02:22,319 --> 00:02:25,200
fingerprinting in particular tls in that

67
00:02:25,200 --> 00:02:26,319
case the adversary

68
00:02:26,319 --> 00:02:28,000
is intercepting traffic between the

69
00:02:28,000 --> 00:02:29,920
client's browser

70
00:02:29,920 --> 00:02:31,680
and the website that the client is

71
00:02:31,680 --> 00:02:34,080
visiting as you may know tls does not

72
00:02:34,080 --> 00:02:36,000
protect the ap of the server visited so

73
00:02:36,000 --> 00:02:38,080
we're going to assume that the

74
00:02:38,080 --> 00:02:40,319
person knows which website the victim is

75
00:02:40,319 --> 00:02:41,200
visiting

76
00:02:41,200 --> 00:02:44,000
however what the advisor doesn't know is

77
00:02:44,000 --> 00:02:45,920
the exact web page that is

78
00:02:45,920 --> 00:02:49,040
visited by the victim so that's the one

79
00:02:49,040 --> 00:02:49,440
who's

80
00:02:49,440 --> 00:02:51,519
the one use of web page fingerprinting

81
00:02:51,519 --> 00:02:53,280
there is another use

82
00:02:53,280 --> 00:02:54,480
that does not have to do with

83
00:02:54,480 --> 00:02:57,040
surveillance it's um benign use case

84
00:02:57,040 --> 00:02:58,239
where

85
00:02:58,239 --> 00:03:00,720
network administrators are actually

86
00:03:00,720 --> 00:03:03,200
monitoring their network for malware

87
00:03:03,200 --> 00:03:06,560
and to do that they are intercepting tls

88
00:03:06,560 --> 00:03:09,760
traffic through their network and try to

89
00:03:09,760 --> 00:03:11,760
master traffic to commander control

90
00:03:11,760 --> 00:03:14,080
servers or unique patterns that

91
00:03:14,080 --> 00:03:18,560
malware traffic exhibits the second and

92
00:03:18,560 --> 00:03:19,440
most popular

93
00:03:19,440 --> 00:03:22,319
type of traffic fingerprinting is

94
00:03:22,319 --> 00:03:24,720
website fingerprinting we often

95
00:03:24,720 --> 00:03:27,519
see it in papers with regards to torque

96
00:03:27,519 --> 00:03:29,280
in that case the adversary doesn't know

97
00:03:29,280 --> 00:03:30,319
which website the

98
00:03:30,319 --> 00:03:33,680
victim is visiting the adversary is

99
00:03:33,680 --> 00:03:35,440
standing in between any intercepting

100
00:03:35,440 --> 00:03:36,720
traffic in between

101
00:03:36,720 --> 00:03:39,040
uh the client and the entry node of the

102
00:03:39,040 --> 00:03:40,159
tor network

103
00:03:40,159 --> 00:03:42,159
and this the other investor is trying to

104
00:03:42,159 --> 00:03:44,319
infer which website this time

105
00:03:44,319 --> 00:03:45,840
the victim is visiting the

106
00:03:45,840 --> 00:03:47,840
fingerprinting task is very simple

107
00:03:47,840 --> 00:03:50,480
the adversary taps the traffic uh

108
00:03:50,480 --> 00:03:51,599
between the victim

109
00:03:51,599 --> 00:03:53,599
and the server visited or the tore entry

110
00:03:53,599 --> 00:03:55,680
node and then converts it

111
00:03:55,680 --> 00:03:58,000
into a sequence of bytes that denote the

112
00:03:58,000 --> 00:03:59,360
length of the packets

113
00:03:59,360 --> 00:04:01,920
sent or received by the adversary so the

114
00:04:01,920 --> 00:04:03,760
consequence of white

115
00:04:03,760 --> 00:04:05,360
contains only the length and the

116
00:04:05,360 --> 00:04:08,640
direction of the packet set

117
00:04:08,959 --> 00:04:10,720
the process for the figure printing from

118
00:04:10,720 --> 00:04:12,720
the adversary's perspective is

119
00:04:12,720 --> 00:04:14,640
as a first step the adversary compiles

120
00:04:14,640 --> 00:04:17,199
usually a data set of label sequences

121
00:04:17,199 --> 00:04:20,560
from web pages or websites that they

122
00:04:20,560 --> 00:04:22,240
want to fingerprint

123
00:04:22,240 --> 00:04:24,960
and then this is actually fairly easy to

124
00:04:24,960 --> 00:04:25,840
automate

125
00:04:25,840 --> 00:04:28,000
and then prepares a classification

126
00:04:28,000 --> 00:04:29,840
system that classification system can be

127
00:04:29,840 --> 00:04:31,280
anything it can be a very advanced

128
00:04:31,280 --> 00:04:32,560
machine learning system

129
00:04:32,560 --> 00:04:35,360
but based on machine learning or it can

130
00:04:35,360 --> 00:04:35,840
be

131
00:04:35,840 --> 00:04:38,960
um just a manual annotation system

132
00:04:38,960 --> 00:04:41,919
so anything really and then the third

133
00:04:41,919 --> 00:04:43,600
step is

134
00:04:43,600 --> 00:04:46,080
once the adversary captures some traffic

135
00:04:46,080 --> 00:04:47,199
that they wish to

136
00:04:47,199 --> 00:04:49,120
fingerprint they push it through the

137
00:04:49,120 --> 00:04:50,320
system

138
00:04:50,320 --> 00:04:53,919
to have it classified since

139
00:04:53,919 --> 00:04:55,520
the first step is fairly easy to

140
00:04:55,520 --> 00:04:57,040
automate

141
00:04:57,040 --> 00:04:59,040
machine learning can actually help with

142
00:04:59,040 --> 00:05:02,000
steps two and three

143
00:05:02,000 --> 00:05:03,440
so now we're going to go through a

144
00:05:03,440 --> 00:05:05,360
non-exhaustive review of the literature

145
00:05:05,360 --> 00:05:07,840
we will see how machine learning helped

146
00:05:07,840 --> 00:05:09,680
advance that field further in the last

147
00:05:09,680 --> 00:05:10,479
few years

148
00:05:10,479 --> 00:05:13,759
so up until 2016 most of the literature

149
00:05:13,759 --> 00:05:16,080
was working with incremental steps

150
00:05:16,080 --> 00:05:18,160
with regards to the performance of

151
00:05:18,160 --> 00:05:19,919
fingerprinting adversaries

152
00:05:19,919 --> 00:05:22,720
and in most papers there was some

153
00:05:22,720 --> 00:05:23,680
element of

154
00:05:23,680 --> 00:05:26,639
feature engineering as well as some

155
00:05:26,639 --> 00:05:28,560
small advancements with regards to the

156
00:05:28,560 --> 00:05:31,840
fingerprinting technique by itself

157
00:05:31,840 --> 00:05:35,280
however since 2017 deep learning models

158
00:05:35,280 --> 00:05:36,240
became

159
00:05:36,240 --> 00:05:39,280
more common in that community uh and in

160
00:05:39,280 --> 00:05:41,120
particular the advantage that those

161
00:05:41,120 --> 00:05:43,919
models brought is that now uh you don't

162
00:05:43,919 --> 00:05:45,759
have to

163
00:05:45,759 --> 00:05:48,320
do kitchen engineering manually but

164
00:05:48,320 --> 00:05:50,080
instead you can use a deep learning

165
00:05:50,080 --> 00:05:50,639
model

166
00:05:50,639 --> 00:05:53,120
that's gonna do it auto in an automated

167
00:05:53,120 --> 00:05:53,680
manner

168
00:05:53,680 --> 00:05:56,960
but without you having to manually craft

169
00:05:56,960 --> 00:05:58,639
the features that make most most of the

170
00:05:58,639 --> 00:05:59,199
sense

171
00:05:59,199 --> 00:06:00,800
because of the success and very good

172
00:06:00,800 --> 00:06:02,960
performance of deep learning models

173
00:06:02,960 --> 00:06:05,360
the same period the the idea of

174
00:06:05,360 --> 00:06:07,840
vulnerability approximators was proposed

175
00:06:07,840 --> 00:06:10,080
uh given a website fingerprinting

176
00:06:10,080 --> 00:06:11,039
defense

177
00:06:11,039 --> 00:06:12,690
we can actually um

178
00:06:12,690 --> 00:06:14,400
[Music]

179
00:06:14,400 --> 00:06:16,080
estimate the security balance of an

180
00:06:16,080 --> 00:06:19,840
adversary given a chosen feature set

181
00:06:19,840 --> 00:06:21,280
this is great because now we have a

182
00:06:21,280 --> 00:06:23,680
lower bound of security which is

183
00:06:23,680 --> 00:06:25,759
something we didn't have before however

184
00:06:25,759 --> 00:06:26,800
the downside of that

185
00:06:26,800 --> 00:06:30,160
is that the estimation can be done only

186
00:06:30,160 --> 00:06:33,600
based on the chosen feature set this gap

187
00:06:33,600 --> 00:06:35,280
was actually covered by a paper

188
00:06:35,280 --> 00:06:36,319
published later on

189
00:06:36,319 --> 00:06:38,720
on a slightly different sub area though

190
00:06:38,720 --> 00:06:39,759
of fingerprinting

191
00:06:39,759 --> 00:06:42,479
so it didn't cover all the features um

192
00:06:42,479 --> 00:06:44,319
that the original paper

193
00:06:44,319 --> 00:06:47,600
was looking into but that second paper

194
00:06:47,600 --> 00:06:49,280
was actually

195
00:06:49,280 --> 00:06:50,800
focusing on the learnability and

196
00:06:50,800 --> 00:06:53,440
reliability of the protocol

197
00:06:53,440 --> 00:06:56,560
based uh on an exhaustive search of the

198
00:06:56,560 --> 00:07:00,720
features moving on to 2019

199
00:07:00,720 --> 00:07:02,319
even though we have very good deep

200
00:07:02,319 --> 00:07:04,000
learning models that can extract the

201
00:07:04,000 --> 00:07:05,599
features automatically for us

202
00:07:05,599 --> 00:07:07,039
there is still research and features

203
00:07:07,039 --> 00:07:08,880
going on for a very good reason

204
00:07:08,880 --> 00:07:10,960
so while for the attacker the exact

205
00:07:10,960 --> 00:07:12,720
features don't necessarily

206
00:07:12,720 --> 00:07:15,919
need to be need to make sense for

207
00:07:15,919 --> 00:07:18,400
designing efficient defenses the

208
00:07:18,400 --> 00:07:20,240
interpretability of features and the

209
00:07:20,240 --> 00:07:22,080
indications were built around those

210
00:07:22,080 --> 00:07:24,880
is still important so that specific

211
00:07:24,880 --> 00:07:26,960
paper was focusing on

212
00:07:26,960 --> 00:07:29,360
the timing element of features so it's

213
00:07:29,360 --> 00:07:31,440
bringing in another dimension that has

214
00:07:31,440 --> 00:07:31,840
been

215
00:07:31,840 --> 00:07:34,960
somewhat overlooked earlier and this is

216
00:07:34,960 --> 00:07:36,000
driving this

217
00:07:36,000 --> 00:07:39,039
this direction forward now

218
00:07:39,039 --> 00:07:41,919
because of the very good performance we

219
00:07:41,919 --> 00:07:44,000
had with existing adversaries

220
00:07:44,000 --> 00:07:46,160
we also started to explore more

221
00:07:46,160 --> 00:07:47,759
realistic ones

222
00:07:47,759 --> 00:07:51,280
meaning that other dimensions of

223
00:07:51,280 --> 00:07:54,960
the attackers can now be fairly relaxed

224
00:07:54,960 --> 00:07:56,639
but one of the problems that had been

225
00:07:56,639 --> 00:07:58,960
expressed actually in 2014 so several

226
00:07:58,960 --> 00:08:00,400
years ago but

227
00:08:00,400 --> 00:08:03,440
hadn't been explored adequately his data

228
00:08:03,440 --> 00:08:04,560
stainless

229
00:08:04,560 --> 00:08:08,080
given a fingerprinting system is trained

230
00:08:08,080 --> 00:08:10,400
on a set of data

231
00:08:10,400 --> 00:08:12,479
one of the problems that the anniversary

232
00:08:12,479 --> 00:08:13,440
may have is that

233
00:08:13,440 --> 00:08:16,240
those data become stale fairly quickly

234
00:08:16,240 --> 00:08:18,080
as the content of the websites is

235
00:08:18,080 --> 00:08:19,120
changing

236
00:08:19,120 --> 00:08:21,440
so uh one of the papers that first

237
00:08:21,440 --> 00:08:22,800
brought this problem up

238
00:08:22,800 --> 00:08:25,039
is the fingerprinting published last

239
00:08:25,039 --> 00:08:26,000
year

240
00:08:26,000 --> 00:08:29,120
and they are looking into exactly that

241
00:08:29,120 --> 00:08:32,000
element uh aiming to make adversaries

242
00:08:32,000 --> 00:08:33,760
and attacks more realistic in the real

243
00:08:33,760 --> 00:08:34,159
world

244
00:08:34,159 --> 00:08:36,640
in a real-world environment uh so that's

245
00:08:36,640 --> 00:08:39,039
the generalizability property

246
00:08:39,039 --> 00:08:42,000
that they that they mention and they

247
00:08:42,000 --> 00:08:43,440
want to address stillness

248
00:08:43,440 --> 00:08:46,560
of the trade training data there's also

249
00:08:46,560 --> 00:08:47,519
bootstrap time

250
00:08:47,519 --> 00:08:48,959
which is another dimension they are

251
00:08:48,959 --> 00:08:51,760
considering with regards to

252
00:08:51,760 --> 00:08:55,200
how quickly can a an adversary train a

253
00:08:55,200 --> 00:08:57,760
classifier and have it ready to be used

254
00:08:57,760 --> 00:08:59,279
and then finally they're also focusing

255
00:08:59,279 --> 00:09:01,519
on flexibility and transferability

256
00:09:01,519 --> 00:09:03,440
because as an adversary you may want to

257
00:09:03,440 --> 00:09:04,880
add a new

258
00:09:04,880 --> 00:09:07,839
site or webpage to be monitored so it's

259
00:09:07,839 --> 00:09:09,360
also important to have this kind of

260
00:09:09,360 --> 00:09:10,320
flexibility

261
00:09:10,320 --> 00:09:11,920
these are all things that hadn't been

262
00:09:11,920 --> 00:09:13,839
considered before so

263
00:09:13,839 --> 00:09:16,320
because we've done so well with the past

264
00:09:16,320 --> 00:09:19,519
adversaries we're now expanding to

265
00:09:19,519 --> 00:09:23,200
add more even more parameters

266
00:09:23,200 --> 00:09:27,440
on those however that last paper

267
00:09:27,440 --> 00:09:29,839
they made a lot of progress but they

268
00:09:29,839 --> 00:09:32,080
still need to retrain

269
00:09:32,080 --> 00:09:34,000
when they want to add a new class or

270
00:09:34,000 --> 00:09:36,000
they when they want to

271
00:09:36,000 --> 00:09:37,760
update their system with the latest

272
00:09:37,760 --> 00:09:39,120
version of the web page is their

273
00:09:39,120 --> 00:09:40,640
fingerprinting

274
00:09:40,640 --> 00:09:42,480
even though the the training is

275
00:09:42,480 --> 00:09:44,560
considerably lighter than that

276
00:09:44,560 --> 00:09:48,320
required before uh another work

277
00:09:48,320 --> 00:09:50,000
published again this year that is trying

278
00:09:50,000 --> 00:09:52,480
to address the stainless problem

279
00:09:52,480 --> 00:09:55,680
um is taking an alternative approach

280
00:09:55,680 --> 00:09:57,839
and what they do is they they use fewer

281
00:09:57,839 --> 00:09:59,360
data so that there is less data

282
00:09:59,360 --> 00:10:00,720
dependency

283
00:10:00,720 --> 00:10:02,480
but they manage to retain the same

284
00:10:02,480 --> 00:10:04,640
performance as if by past works

285
00:10:04,640 --> 00:10:06,560
from our perspective where this whole

286
00:10:06,560 --> 00:10:08,640
thing is going is transfer learning

287
00:10:08,640 --> 00:10:11,120
so just a short introduction in transfer

288
00:10:11,120 --> 00:10:11,760
learning

289
00:10:11,760 --> 00:10:15,200
in that the context of fingerprinting is

290
00:10:15,200 --> 00:10:16,800
given a machine learning model

291
00:10:16,800 --> 00:10:19,040
transferring transfer learning enables

292
00:10:19,040 --> 00:10:20,959
that model to be trained on one task and

293
00:10:20,959 --> 00:10:22,720
then be repurposed

294
00:10:22,720 --> 00:10:24,800
at a very low cost for a second related

295
00:10:24,800 --> 00:10:25,839
tasks

296
00:10:25,839 --> 00:10:29,760
in the fingerprinting context this means

297
00:10:29,760 --> 00:10:31,440
transferable models across various

298
00:10:31,440 --> 00:10:34,240
dimensions the temporal for example

299
00:10:34,240 --> 00:10:37,519
which is relevant to data stillness

300
00:10:37,519 --> 00:10:39,360
across websites and web pages so you

301
00:10:39,360 --> 00:10:40,880
train their model on one

302
00:10:40,880 --> 00:10:42,880
website and then you can move it on

303
00:10:42,880 --> 00:10:44,959
another one

304
00:10:44,959 --> 00:10:47,200
with regards to the location of the

305
00:10:47,200 --> 00:10:48,399
victim so

306
00:10:48,399 --> 00:10:50,240
maybe your training data originates from

307
00:10:50,240 --> 00:10:51,600
one

308
00:10:51,600 --> 00:10:53,760
physical location then the victim

309
00:10:53,760 --> 00:10:54,720
somewhere else

310
00:10:54,720 --> 00:10:57,519
in the world um does the model transfer

311
00:10:57,519 --> 00:10:59,680
well across locations

312
00:10:59,680 --> 00:11:01,600
and then also protocol versions either

313
00:11:01,600 --> 00:11:04,079
this is store protocol versions or

314
00:11:04,079 --> 00:11:06,480
tls protocol versions overall the goal

315
00:11:06,480 --> 00:11:07,519
of this kind of

316
00:11:07,519 --> 00:11:10,480
exploration is to see how versatile

317
00:11:10,480 --> 00:11:12,480
versatile versions can become

318
00:11:12,480 --> 00:11:15,519
across all those dimensions uh to do

319
00:11:15,519 --> 00:11:17,200
that we actually designed

320
00:11:17,200 --> 00:11:19,760
a couple of experiments and we collected

321
00:11:19,760 --> 00:11:21,760
a completely new data set we focused on

322
00:11:21,760 --> 00:11:23,600
tls finger printing

323
00:11:23,600 --> 00:11:26,880
um we collected data set contain

324
00:11:26,880 --> 00:11:29,920
around 19 000 wikipedia articles so our

325
00:11:29,920 --> 00:11:32,000
advisor is trying to distinguish which

326
00:11:32,000 --> 00:11:32,880
article

327
00:11:32,880 --> 00:11:34,560
uh which would be the article the victim

328
00:11:34,560 --> 00:11:36,079
is visiting uh

329
00:11:36,079 --> 00:11:38,959
to collect our um data set we visited

330
00:11:38,959 --> 00:11:39,839
its article

331
00:11:39,839 --> 00:11:43,120
100 time it's time from one different

332
00:11:43,120 --> 00:11:44,480
amazon instance

333
00:11:44,480 --> 00:11:47,279
and we spread spread our instances into

334
00:11:47,279 --> 00:11:48,640
five different different physical

335
00:11:48,640 --> 00:11:50,079
locations around the globe

336
00:11:50,079 --> 00:11:52,800
on the left here we have a visual

337
00:11:52,800 --> 00:11:54,639
representation of our data set we split

338
00:11:54,639 --> 00:11:56,639
our data city to two parts

339
00:11:56,639 --> 00:11:59,440
the green part that contains the subsets

340
00:11:59,440 --> 00:12:00,000
a and b

341
00:12:00,000 --> 00:12:01,440
and the blue part that contains the

342
00:12:01,440 --> 00:12:03,519
subset c and d

343
00:12:03,519 --> 00:12:06,720
the first part the green part contains 6

344
00:12:06,720 --> 00:12:07,120
000

345
00:12:07,120 --> 00:12:10,639
classes while the blue part contains 13

346
00:12:10,639 --> 00:12:12,320
000 different classes

347
00:12:12,320 --> 00:12:16,639
each of those parts features 100 samples

348
00:12:16,639 --> 00:12:19,360
traffic traces in other words uh of of

349
00:12:19,360 --> 00:12:20,000
its class

350
00:12:20,000 --> 00:12:22,720
and then the subsets essentially split

351
00:12:22,720 --> 00:12:23,600
those samples

352
00:12:23,600 --> 00:12:26,480
so subset a has 90 samples for every

353
00:12:26,480 --> 00:12:27,200
class

354
00:12:27,200 --> 00:12:29,680
subset b has 10 samples for every class

355
00:12:29,680 --> 00:12:30,399
and the same

356
00:12:30,399 --> 00:12:33,920
holds for c and d now moving on to our

357
00:12:33,920 --> 00:12:35,839
fingerprinting pipeline

358
00:12:35,839 --> 00:12:37,760
without going into the technical details

359
00:12:37,760 --> 00:12:39,920
the fingerprint fingerprinting pipeline

360
00:12:39,920 --> 00:12:40,240
is

361
00:12:40,240 --> 00:12:42,000
using an embedding model which is

362
00:12:42,000 --> 00:12:44,000
trained to map um

363
00:12:44,000 --> 00:12:46,160
vector inputs essentially in that case

364
00:12:46,160 --> 00:12:47,760
the sequence the bytes of

365
00:12:47,760 --> 00:12:49,920
the sequences of bytes are into a

366
00:12:49,920 --> 00:12:50,800
multi-day

367
00:12:50,800 --> 00:12:52,800
into a point into multi-dimensional

368
00:12:52,800 --> 00:12:55,120
space what the adversary does is

369
00:12:55,120 --> 00:12:58,320
they gather a set of reference samples

370
00:12:58,320 --> 00:13:00,240
and then embed them

371
00:13:00,240 --> 00:13:03,040
using the trained embedding model every

372
00:13:03,040 --> 00:13:04,480
time a new input case

373
00:13:04,480 --> 00:13:06,720
comes from the victim the adversary

374
00:13:06,720 --> 00:13:08,320
embeds that in code as well

375
00:13:08,320 --> 00:13:10,320
and then classifies it based on the

376
00:13:10,320 --> 00:13:12,720
proximity to the reference samples

377
00:13:12,720 --> 00:13:14,800
determines our first experiment is

378
00:13:14,800 --> 00:13:16,160
actually ground rule of experiments so

379
00:13:16,160 --> 00:13:17,839
it's not a transfer learning one

380
00:13:17,839 --> 00:13:20,079
we trained our model on subset a which

381
00:13:20,079 --> 00:13:22,639
contains 6000 classes

382
00:13:22,639 --> 00:13:25,519
and 90 samples for each of those classes

383
00:13:25,519 --> 00:13:27,519
and we tested it on subset b

384
00:13:27,519 --> 00:13:30,800
which contains again 10 um

385
00:13:30,800 --> 00:13:33,920
10 previously samples from those 6000

386
00:13:33,920 --> 00:13:36,639
classes as we can see on the figure on

387
00:13:36,639 --> 00:13:37,519
the right

388
00:13:37,519 --> 00:13:40,720
the blue line is for a a version of the

389
00:13:40,720 --> 00:13:43,440
experiment where we test it with

390
00:13:43,440 --> 00:13:46,000
just 500 classes and we can see that

391
00:13:46,000 --> 00:13:47,600
after three guesses the adversary

392
00:13:47,600 --> 00:13:48,720
reaches

393
00:13:48,720 --> 00:13:50,800
an accuracy that is higher than 90

394
00:13:50,800 --> 00:13:51,839
percent

395
00:13:51,839 --> 00:13:53,040
now three guesses means that the

396
00:13:53,040 --> 00:13:54,880
adversary is allowed to guess three

397
00:13:54,880 --> 00:13:57,120
potential classes instead of just one

398
00:13:57,120 --> 00:13:59,680
i've removed we will move to uh the

399
00:13:59,680 --> 00:14:02,320
larger version of the experiment where

400
00:14:02,320 --> 00:14:04,480
the samples were that the adversary

401
00:14:04,480 --> 00:14:06,399
would see were coming from all

402
00:14:06,399 --> 00:14:08,560
six thousand possible classes uh the

403
00:14:08,560 --> 00:14:10,639
acres of course decreases but

404
00:14:10,639 --> 00:14:13,600
still the adversary if the adversary is

405
00:14:13,600 --> 00:14:15,600
allowed to guess

406
00:14:15,600 --> 00:14:18,880
up to 10 classes then the accuracy rate

407
00:14:18,880 --> 00:14:19,760
is

408
00:14:19,760 --> 00:14:24,639
quickly up above 80

409
00:14:24,639 --> 00:14:26,240
now moving on to experiment number two

410
00:14:26,240 --> 00:14:28,240
this is a transfer learning experiment

411
00:14:28,240 --> 00:14:30,000
uh we kept the same model from

412
00:14:30,000 --> 00:14:31,839
experiment number one that was trained

413
00:14:31,839 --> 00:14:32,800
on

414
00:14:32,800 --> 00:14:36,959
the original 600 classes subsidy

415
00:14:36,959 --> 00:14:39,680
90 samples per class and but it was

416
00:14:39,680 --> 00:14:40,560
tested on

417
00:14:40,560 --> 00:14:43,519
subsidy which contained samples 10

418
00:14:43,519 --> 00:14:44,079
samples

419
00:14:44,079 --> 00:14:47,600
from 13 000 different classes that were

420
00:14:47,600 --> 00:14:50,079
previously unseen for the model

421
00:14:50,079 --> 00:14:53,519
um to for the reference set we used

422
00:14:53,519 --> 00:14:56,639
subset c so subset c contains

423
00:14:56,639 --> 00:14:59,920
contains uh 90 samples from again 13 000

424
00:14:59,920 --> 00:15:01,440
classes that the model has never seen

425
00:15:01,440 --> 00:15:02,480
before

426
00:15:02,480 --> 00:15:04,399
and we classified subsidy and said

427
00:15:04,399 --> 00:15:05,839
earlier as we see

428
00:15:05,839 --> 00:15:07,360
as we can see on the figure on the right

429
00:15:07,360 --> 00:15:09,440
the accuracy didn't change that much

430
00:15:09,440 --> 00:15:11,920
so if we go on the 500 classes version

431
00:15:11,920 --> 00:15:13,040
of the experiment

432
00:15:13,040 --> 00:15:15,199
again after three guesses you have

433
00:15:15,199 --> 00:15:17,120
almost 90 percent accuracy

434
00:15:17,120 --> 00:15:20,480
moving on to the 6000 classes versus the

435
00:15:20,480 --> 00:15:21,440
experiment

436
00:15:21,440 --> 00:15:24,560
again with 10 guesses you reach

437
00:15:24,560 --> 00:15:26,560
a little bit above 80 accuracy which is

438
00:15:26,560 --> 00:15:27,680
fairly good

439
00:15:27,680 --> 00:15:30,800
and for the 13 000 um

440
00:15:30,800 --> 00:15:33,120
classes version of the experiment we

441
00:15:33,120 --> 00:15:35,680
still have a very good accuracy of

442
00:15:35,680 --> 00:15:38,240
above 80 percent if the advisor is

443
00:15:38,240 --> 00:15:40,240
allowed to guess

444
00:15:40,240 --> 00:15:43,920
20 potential labels for each sample

445
00:15:43,920 --> 00:15:45,360
to put this into perspective the

446
00:15:45,360 --> 00:15:47,759
adversary is allowed to guess 20 labels

447
00:15:47,759 --> 00:15:48,399
out of the

448
00:15:48,399 --> 00:15:52,000
13 000 possible moving on to the

449
00:15:52,000 --> 00:15:53,600
takeaways machine learning is advancing

450
00:15:53,600 --> 00:15:54,480
in a

451
00:15:54,480 --> 00:15:56,560
very fast pace and this is lingering

452
00:15:56,560 --> 00:15:58,880
advancements and changes in other fields

453
00:15:58,880 --> 00:16:02,079
what it is applicable in the case of

454
00:16:02,079 --> 00:16:03,839
fingerprinting this allowed us

455
00:16:03,839 --> 00:16:07,199
to achieve very high accuracy in

456
00:16:07,199 --> 00:16:10,639
our already existing adversarial models

457
00:16:10,639 --> 00:16:13,360
but also gave us the latitude to

458
00:16:13,360 --> 00:16:16,160
envision new more realistic adversaries

459
00:16:16,160 --> 00:16:18,320
uh however an unfortunate effect is that

460
00:16:18,320 --> 00:16:19,519
because

461
00:16:19,519 --> 00:16:22,079
this kind of models are more easily

462
00:16:22,079 --> 00:16:22,959
applicable

463
00:16:22,959 --> 00:16:25,480
for attacks um there is a

464
00:16:25,480 --> 00:16:27,759
disproportionate

465
00:16:27,759 --> 00:16:29,360
number of papers on fingerprinting

466
00:16:29,360 --> 00:16:31,120
attacks and only fewer

467
00:16:31,120 --> 00:16:33,440
fewer paper and countermeasures so maybe

468
00:16:33,440 --> 00:16:35,279
this is something we need to put more

469
00:16:35,279 --> 00:16:36,880
effort on

470
00:16:36,880 --> 00:16:40,320
thank you very much and we're open to

471
00:16:47,959 --> 00:16:50,959
questions

