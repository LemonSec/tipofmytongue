1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:10,580 --> 00:00:12,780
hi everyone thanks for the introduction

3
00:00:12,780 --> 00:00:15,299
before we jump into piranha I wanted to

4
00:00:15,299 --> 00:00:16,760
take a quick second to introduce

5
00:00:16,760 --> 00:00:20,340
multi-party computation MPC again the

6
00:00:20,340 --> 00:00:22,020
idea of MPC is that you have several

7
00:00:22,020 --> 00:00:25,199
distrusting parties so P1 P2 Etc each

8
00:00:25,199 --> 00:00:26,760
have some sort of secret input that you

9
00:00:26,760 --> 00:00:28,980
want to compute a shared result and the

10
00:00:28,980 --> 00:00:31,199
catch is that during this process they

11
00:00:31,199 --> 00:00:32,700
don't want any of their private inputs

12
00:00:32,700 --> 00:00:34,559
to be leaked to any of the other parties

13
00:00:34,559 --> 00:00:36,600
and so this is precisely the security

14
00:00:36,600 --> 00:00:39,059
guarantee that MPC provides and so it's

15
00:00:39,059 --> 00:00:41,520
a really useful tool but over the 40 or

16
00:00:41,520 --> 00:00:43,020
so years that we've been studying it

17
00:00:43,020 --> 00:00:45,660
it's actually very very expensive so for

18
00:00:45,660 --> 00:00:47,520
example sharing or generating a shared

19
00:00:47,520 --> 00:00:50,760
as encryption in MPC might take up to a

20
00:00:50,760 --> 00:00:53,820
millisecond per block in MPC versus you

21
00:00:53,820 --> 00:00:55,260
know a couple hundred Cycles on Modern

22
00:00:55,260 --> 00:00:57,840
as hardware and in machine learning

23
00:00:57,840 --> 00:01:00,780
tasks such as inference it's for orders

24
00:01:00,780 --> 00:01:02,760
of magnitude slower to compute securely

25
00:01:02,760 --> 00:01:04,739
compared to plain text that's not even

26
00:01:04,739 --> 00:01:08,520
to mention training ml models in MPC

27
00:01:08,520 --> 00:01:10,860
which might take up to weeks to lead on

28
00:01:10,860 --> 00:01:13,740
a CPU So speaking of ml training I'm

29
00:01:13,740 --> 00:01:15,479
going to kind of quickly spoil piranha a

30
00:01:15,479 --> 00:01:17,100
little bit and kind of tab to a live

31
00:01:17,100 --> 00:01:19,080
demo so we're going to be training this

32
00:01:19,080 --> 00:01:20,340
model during the rest of the talk I'm

33
00:01:20,340 --> 00:01:21,900
just going to run the a very simple

34
00:01:21,900 --> 00:01:25,320
secure ml architecture and if I don't

35
00:01:25,320 --> 00:01:28,380
mess anything up we can uh

36
00:01:28,380 --> 00:01:30,659
of course the live demo doesn't work at

37
00:01:30,659 --> 00:01:33,799
exactly the same point

38
00:01:39,000 --> 00:01:41,280
whatever you we'll run it later if

39
00:01:41,280 --> 00:01:43,020
anyone wants to see it but trust me

40
00:01:43,020 --> 00:01:45,060
trust me it trains okay so back to the

41
00:01:45,060 --> 00:01:46,619
slides it's obvious that there's a

42
00:01:46,619 --> 00:01:47,700
massive massive performance challenge

43
00:01:47,700 --> 00:01:49,439
here and looking around how do we

44
00:01:49,439 --> 00:01:50,880
actually fix this problem for plain text

45
00:01:50,880 --> 00:01:53,759
well the solution to a wide variety of

46
00:01:53,759 --> 00:01:55,619
computational problems in that setting

47
00:01:55,619 --> 00:01:58,380
was using general purpose gpus so the

48
00:01:58,380 --> 00:02:00,060
point of this talk is asking can we

49
00:02:00,060 --> 00:02:02,100
similarly apply them to generally

50
00:02:02,100 --> 00:02:05,100
accelerating MPC so there's been some

51
00:02:05,100 --> 00:02:06,420
investigation in the space but the

52
00:02:06,420 --> 00:02:07,979
reality of privacy preserving machine

53
00:02:07,979 --> 00:02:10,139
learning training for example is you get

54
00:02:10,139 --> 00:02:12,300
to re-implement a lot from scratch when

55
00:02:12,300 --> 00:02:14,040
you want to try to start up a project so

56
00:02:14,040 --> 00:02:15,780
on the MPC side you're picking a

57
00:02:15,780 --> 00:02:17,340
protocol you're implementing it you're

58
00:02:17,340 --> 00:02:20,099
testing it Etc and at the same time if

59
00:02:20,099 --> 00:02:22,200
you want to use a GPU you're wrangling

60
00:02:22,200 --> 00:02:24,180
the GPU to be able to accelerate each

61
00:02:24,180 --> 00:02:25,680
piece so you're contending with data

62
00:02:25,680 --> 00:02:27,599
management there's a memory memory

63
00:02:27,599 --> 00:02:29,760
hierarchy in there you're building all

64
00:02:29,760 --> 00:02:31,080
the kernels for what you want to

65
00:02:31,080 --> 00:02:32,879
accelerate as a result practically

66
00:02:32,879 --> 00:02:35,480
there's a huge gap in capability between

67
00:02:35,480 --> 00:02:38,340
cryptographers who develop MPC protocols

68
00:02:38,340 --> 00:02:40,440
and system experts who spend all day

69
00:02:40,440 --> 00:02:42,660
hacking on GPS so in this talk I want to

70
00:02:42,660 --> 00:02:44,760
show you how piranha Bridges this gap

71
00:02:44,760 --> 00:02:47,220
between cryptography and systems and how

72
00:02:47,220 --> 00:02:49,560
it manages GPU concerns so that MPC

73
00:02:49,560 --> 00:02:51,840
experts can focus on designing protocols

74
00:02:51,840 --> 00:02:54,120
and the like so at the high level

75
00:02:54,120 --> 00:02:56,700
prana's big goal is to make accelerating

76
00:02:56,700 --> 00:02:58,680
secure MPC practical

77
00:02:58,680 --> 00:03:00,420
and for this talk we're going to focus

78
00:03:00,420 --> 00:03:03,180
on a subgroup of protocols called linear

79
00:03:03,180 --> 00:03:05,160
secret sharing protocols and we're going

80
00:03:05,160 --> 00:03:06,599
to build support for them with two sub

81
00:03:06,599 --> 00:03:09,360
goals in mind so the first goal is that

82
00:03:09,360 --> 00:03:11,099
it should be usable and that means the

83
00:03:11,099 --> 00:03:12,599
product should be capable of supporting

84
00:03:12,599 --> 00:03:14,819
many different protocols and extending

85
00:03:14,819 --> 00:03:17,099
easily to new applications and number

86
00:03:17,099 --> 00:03:19,500
two it goes without saying that should

87
00:03:19,500 --> 00:03:20,879
be performant right should actually be

88
00:03:20,879 --> 00:03:23,819
pretty fast and so you know both goals

89
00:03:23,819 --> 00:03:25,200
seem pretty simple at the outset and

90
00:03:25,200 --> 00:03:26,280
that's what we thought but that's

91
00:03:26,280 --> 00:03:28,319
unfortunately not quite the case and so

92
00:03:28,319 --> 00:03:30,120
to understand why I want to take a quick

93
00:03:30,120 --> 00:03:32,280
tour of prana's architecture and point

94
00:03:32,280 --> 00:03:34,260
out a few big issues

95
00:03:34,260 --> 00:03:36,540
so the first issue is this issue of

96
00:03:36,540 --> 00:03:38,519
unnecessarily duplicated work so if you

97
00:03:38,519 --> 00:03:40,200
want to do privacy preserving ml or any

98
00:03:40,200 --> 00:03:42,780
MPC computation what you'll get is this

99
00:03:42,780 --> 00:03:44,459
monolithic application bundle that

100
00:03:44,459 --> 00:03:46,080
combines some custom protocol that

101
00:03:46,080 --> 00:03:48,480
someone developed and the application

102
00:03:48,480 --> 00:03:50,459
but say you just went to a talk you

103
00:03:50,459 --> 00:03:51,959
learned about some cool new like

104
00:03:51,959 --> 00:03:53,760
inference protocol or poisson regression

105
00:03:53,760 --> 00:03:55,799
whatever you want and that makes like

106
00:03:55,799 --> 00:03:57,720
specific operations much faster how do

107
00:03:57,720 --> 00:03:58,920
you actually integrate that into your

108
00:03:58,920 --> 00:04:00,840
project to gain the benefits well you're

109
00:04:00,840 --> 00:04:02,220
kind of out of luck because existing

110
00:04:02,220 --> 00:04:04,319
components are just Shackled together in

111
00:04:04,319 --> 00:04:06,420
a Big Blob that makes it impossible to

112
00:04:06,420 --> 00:04:09,480
to use in a modular way so Prana takes

113
00:04:09,480 --> 00:04:11,819
that modular approach and it uses a

114
00:04:11,819 --> 00:04:13,620
three-layer design to split apart the

115
00:04:13,620 --> 00:04:16,079
neural network library at the top from

116
00:04:16,079 --> 00:04:18,060
the protocol in the middle from the

117
00:04:18,060 --> 00:04:19,858
local GPU operations that we can focus

118
00:04:19,858 --> 00:04:21,899
on accelerating at the bottom and

119
00:04:21,899 --> 00:04:23,880
importantly this allows us to preserve

120
00:04:23,880 --> 00:04:25,860
the security properties of the MPC

121
00:04:25,860 --> 00:04:27,620
protocols while at the same time

122
00:04:27,620 --> 00:04:30,000
clarifying this sort of separation of

123
00:04:30,000 --> 00:04:32,639
concerns where piranha accelerates

124
00:04:32,639 --> 00:04:35,580
computation MPC C developers can focus

125
00:04:35,580 --> 00:04:37,320
on composing that computation with

126
00:04:37,320 --> 00:04:39,540
communication and applications can sit

127
00:04:39,540 --> 00:04:40,860
above it all and choose whichever

128
00:04:40,860 --> 00:04:43,320
protocol they want that fits the task

129
00:04:43,320 --> 00:04:45,600
and this all works because LSS protocols

130
00:04:45,600 --> 00:04:47,699
in essence use the same local building

131
00:04:47,699 --> 00:04:50,280
blocks and differ mainly in how they

132
00:04:50,280 --> 00:04:52,139
apply local operations on their secret

133
00:04:52,139 --> 00:04:54,180
shares and how they communicate with

134
00:04:54,180 --> 00:04:55,440
other parties

135
00:04:55,440 --> 00:04:57,419
and on the application side applications

136
00:04:57,419 --> 00:05:00,000
just see data managed by multi-party

137
00:05:00,000 --> 00:05:02,040
computation protocols as just some sort

138
00:05:02,040 --> 00:05:03,840
of opaque data type that they can switch

139
00:05:03,840 --> 00:05:05,460
out with a single Define

140
00:05:05,460 --> 00:05:07,080
so this makes product applications

141
00:05:07,080 --> 00:05:10,199
entirely agnostic to Future changes in

142
00:05:10,199 --> 00:05:13,320
state-of-the-art multi-party computation

143
00:05:13,320 --> 00:05:14,880
so to dig a Little Deeper let's step

144
00:05:14,880 --> 00:05:16,740
into the shoes of someone with an

145
00:05:16,740 --> 00:05:18,660
application that's training a simple

146
00:05:18,660 --> 00:05:20,280
fully connected neural network layer

147
00:05:20,280 --> 00:05:21,960
with a relu activation this is exactly

148
00:05:21,960 --> 00:05:23,220
what I tried to run in the internet

149
00:05:23,220 --> 00:05:26,520
crapped out so underlying that protocols

150
00:05:26,520 --> 00:05:29,039
oh yeah protocols need to implement a

151
00:05:29,039 --> 00:05:30,660
secret shared matrix multiplication for

152
00:05:30,660 --> 00:05:32,400
the fully connected layer and a

153
00:05:32,400 --> 00:05:34,560
comparison for relu and at the bottom

154
00:05:34,560 --> 00:05:37,560
Prana needs to provide kernels to

155
00:05:37,560 --> 00:05:38,699
accelerate those corresponding

156
00:05:38,699 --> 00:05:40,680
operations and right here at the bottom

157
00:05:40,680 --> 00:05:42,419
layer is where we start running into

158
00:05:42,419 --> 00:05:46,020
both computational and memory problems

159
00:05:46,020 --> 00:05:47,940
so problem number one is we somehow need

160
00:05:47,940 --> 00:05:50,280
to find a GPU kernel that can actually

161
00:05:50,280 --> 00:05:51,960
perform matrix multiplication for our

162
00:05:51,960 --> 00:05:53,820
protocols unfortunately the really

163
00:05:53,820 --> 00:05:55,680
popular and highly optimized kernels for

164
00:05:55,680 --> 00:05:57,000
doing this like kublas like the first

165
00:05:57,000 --> 00:05:59,039
thing you would Google uh don't work for

166
00:05:59,039 --> 00:06:00,720
us because they only operate on floating

167
00:06:00,720 --> 00:06:04,320
Point values so our LSS schemes need to

168
00:06:04,320 --> 00:06:06,360
operate over integer rings and in fact

169
00:06:06,360 --> 00:06:07,979
there are no matrix multiplication

170
00:06:07,979 --> 00:06:10,259
kernels for example with the integer

171
00:06:10,259 --> 00:06:13,139
support that we need so one solution is

172
00:06:13,139 --> 00:06:16,139
to embed say you know 16-bit chunks of

173
00:06:16,139 --> 00:06:19,160
the 64-bit integers we care about into

174
00:06:19,160 --> 00:06:22,199
four floating Point matrices and then

175
00:06:22,199 --> 00:06:24,240
you can use existing kernels to

176
00:06:24,240 --> 00:06:25,740
calculate a bunch of different uh

177
00:06:25,740 --> 00:06:27,720
pairings and then summarize them back

178
00:06:27,720 --> 00:06:30,060
into your final result and unfortunately

179
00:06:30,060 --> 00:06:32,280
that causes a lot of overhead in this

180
00:06:32,280 --> 00:06:34,380
case a 10x overhead in the number of

181
00:06:34,380 --> 00:06:35,940
Matrix multiplications that you need to

182
00:06:35,940 --> 00:06:38,819
do and importantly this kind of system

183
00:06:38,819 --> 00:06:41,220
is based on an assumption and the

184
00:06:41,220 --> 00:06:43,020
assumption is that because floating

185
00:06:43,020 --> 00:06:44,580
Point kernels should be really really

186
00:06:44,580 --> 00:06:46,620
fast the end-to-end perform governments

187
00:06:46,620 --> 00:06:48,479
should outweigh the overhead now in

188
00:06:48,479 --> 00:06:50,639
Prana we take a different approach what

189
00:06:50,639 --> 00:06:53,460
if we just directly computed the in the

190
00:06:53,460 --> 00:06:55,919
integer matrix multiplication on the GPU

191
00:06:55,919 --> 00:06:57,960
using the GPU integer course and sure

192
00:06:57,960 --> 00:06:59,940
individual kernel implementations might

193
00:06:59,940 --> 00:07:02,880
not be as crazily optimized as kublas

194
00:07:02,880 --> 00:07:05,580
but our view is that we should not get

195
00:07:05,580 --> 00:07:07,020
gains out of cutting out redundant

196
00:07:07,020 --> 00:07:08,220
computation

197
00:07:08,220 --> 00:07:10,139
or we should get good sorry yeah we want

198
00:07:10,139 --> 00:07:11,880
the gains so anyway we leveraged

199
00:07:11,880 --> 00:07:13,860
nvidia's Cutlass library to implement

200
00:07:13,860 --> 00:07:16,979
both 32-bit and 64-bit gem kernels and

201
00:07:16,979 --> 00:07:19,199
when you compare them head to head an

202
00:07:19,199 --> 00:07:20,699
individual matrix multiplication

203
00:07:20,699 --> 00:07:22,800
directly computed using integers is

204
00:07:22,800 --> 00:07:24,300
slightly slower than in floating point

205
00:07:24,300 --> 00:07:26,520
but overall we make a huge amount of

206
00:07:26,520 --> 00:07:28,740
time back by not repeating the same

207
00:07:28,740 --> 00:07:31,020
operation over and over okay so that's

208
00:07:31,020 --> 00:07:32,759
that's one part okay great we we have

209
00:07:32,759 --> 00:07:34,319
our integer kernels everything is

210
00:07:34,319 --> 00:07:36,660
hunky-dory We're Off to the Races and

211
00:07:36,660 --> 00:07:38,220
that's true until we run into trouble

212
00:07:38,220 --> 00:07:39,780
actually implementing some of the more

213
00:07:39,780 --> 00:07:41,460
complex parts of the of the MPC

214
00:07:41,460 --> 00:07:43,919
protocols such as comparisons so it's

215
00:07:43,919 --> 00:07:45,599
during this step that the constraints of

216
00:07:45,599 --> 00:07:48,000
GPU memory in particular becomes the

217
00:07:48,000 --> 00:07:49,560
overriding concern for executing

218
00:07:49,560 --> 00:07:51,660
multi-party computation and the main

219
00:07:51,660 --> 00:07:54,000
issue is that first off replicate secret

220
00:07:54,000 --> 00:07:56,160
sharing inherently induces data

221
00:07:56,160 --> 00:07:58,139
duplication that stresses the memory

222
00:07:58,139 --> 00:08:00,060
limit so like 16 gigabytes that we have

223
00:08:00,060 --> 00:08:02,880
on our gpus and a three-party protocol

224
00:08:02,880 --> 00:08:05,099
for example might automatically double

225
00:08:05,099 --> 00:08:07,199
the memory load of a given computation

226
00:08:07,199 --> 00:08:08,699
compared to plain text

227
00:08:08,699 --> 00:08:10,560
and the problem gets even worse so

228
00:08:10,560 --> 00:08:12,660
comparisons in particular are very good

229
00:08:12,660 --> 00:08:14,340
at exhausting GPU memory because they

230
00:08:14,340 --> 00:08:16,620
require a bit wise expansion secret

231
00:08:16,620 --> 00:08:18,720
shared expansion of each value so we go

232
00:08:18,720 --> 00:08:20,879
from two times to ten times more memory

233
00:08:20,879 --> 00:08:23,220
pressure just from this expansion so

234
00:08:23,220 --> 00:08:26,460
anything else we do is going to limit

235
00:08:26,460 --> 00:08:28,979
the problem size that we can support and

236
00:08:28,979 --> 00:08:30,479
that's you know you can think of it as

237
00:08:30,479 --> 00:08:33,360
like the model size for machine learning

238
00:08:33,360 --> 00:08:34,740
so to illustrate this let's look at

239
00:08:34,740 --> 00:08:36,958
simple operation which is to perform the

240
00:08:36,958 --> 00:08:38,880
multiplication of each bit in a bit wise

241
00:08:38,880 --> 00:08:41,039
string it's slightly simplified but it

242
00:08:41,039 --> 00:08:42,479
turns out that this is very useful for

243
00:08:42,479 --> 00:08:44,880
implementing a secure comparison so the

244
00:08:44,880 --> 00:08:46,740
basic idea is that in parallel you want

245
00:08:46,740 --> 00:08:48,540
to multiply the neighboring blue and

246
00:08:48,540 --> 00:08:51,120
yellow bits together repeating a

247
00:08:51,120 --> 00:08:52,680
logarithmic number of steps until you've

248
00:08:52,680 --> 00:08:54,899
coalesced the result into one bit so it

249
00:08:54,899 --> 00:08:56,700
looks a little bit like this

250
00:08:56,700 --> 00:08:59,160
so notice that in this implementation to

251
00:08:59,160 --> 00:09:00,540
make each step in the process nice and

252
00:09:00,540 --> 00:09:02,760
vectorizable we want we want to Marshal

253
00:09:02,760 --> 00:09:04,980
every blue and yellow bit into separate

254
00:09:04,980 --> 00:09:06,480
vectors so we can multiply them all at

255
00:09:06,480 --> 00:09:08,760
once on the GPU so unfortunately doing

256
00:09:08,760 --> 00:09:10,980
this incurs constant data movement and

257
00:09:10,980 --> 00:09:12,420
reallocation makes it actually way

258
00:09:12,420 --> 00:09:14,640
slower than using a CPU

259
00:09:14,640 --> 00:09:16,260
so how do we fix this well product

260
00:09:16,260 --> 00:09:18,839
allows protocols to use iterators to

261
00:09:18,839 --> 00:09:20,760
define In-Place views over the same

262
00:09:20,760 --> 00:09:22,620
piece of data it separates the

263
00:09:22,620 --> 00:09:24,899
protocol's data layout from piranhas

264
00:09:24,899 --> 00:09:27,540
acceleration layer and we can use that

265
00:09:27,540 --> 00:09:29,640
to replace marshalling data with an

266
00:09:29,640 --> 00:09:31,620
iterator to every yellow or blue secret

267
00:09:31,620 --> 00:09:33,899
chair bit storing those old back in on

268
00:09:33,899 --> 00:09:36,779
itself and we can repeat the process to

269
00:09:36,779 --> 00:09:38,700
effectively remove any dative movement

270
00:09:38,700 --> 00:09:41,339
whatsoever maximizing the remaining GPU

271
00:09:41,339 --> 00:09:42,720
memory to store larger and larger

272
00:09:42,720 --> 00:09:45,360
problems okay so we saw that Prana

273
00:09:45,360 --> 00:09:46,920
addresses two big challenges in

274
00:09:46,920 --> 00:09:50,760
accelerating MPC memory and computation

275
00:09:50,760 --> 00:09:52,500
but at the end of the day how well does

276
00:09:52,500 --> 00:09:53,760
it really work

277
00:09:53,760 --> 00:09:55,380
so we Implement product and C plus plus

278
00:09:55,380 --> 00:09:57,540
in Cuda I use it to develop a neural

279
00:09:57,540 --> 00:09:58,680
network training library that can

280
00:09:58,680 --> 00:10:01,380
execute on top of any of three MPC

281
00:10:01,380 --> 00:10:03,440
protocols a two-party secure ml

282
00:10:03,440 --> 00:10:05,339
three-party Falcon protocol and a

283
00:10:05,339 --> 00:10:07,260
four-party Fantastic Four protocol and

284
00:10:07,260 --> 00:10:08,519
as a quick aside you could add any

285
00:10:08,519 --> 00:10:10,860
protocol or you know other application

286
00:10:10,860 --> 00:10:12,600
on top of that that you wanted to Prana

287
00:10:12,600 --> 00:10:14,820
and we encouraged that but for for this

288
00:10:14,820 --> 00:10:16,560
we focused on these three

289
00:10:16,560 --> 00:10:18,660
so looking directly at individual

290
00:10:18,660 --> 00:10:20,700
operations like matrix multiplication We

291
00:10:20,700 --> 00:10:22,320
compare Piranha's performance to exactly

292
00:10:22,320 --> 00:10:24,180
the same protocol implemented into MP

293
00:10:24,180 --> 00:10:27,420
speeds on the CPU and it demonstr we

294
00:10:27,420 --> 00:10:29,760
demonstrate 273 times Improvement at

295
00:10:29,760 --> 00:10:31,140
relatively large dimensions for a

296
00:10:31,140 --> 00:10:33,060
three-party protocol and it's not just

297
00:10:33,060 --> 00:10:34,620
this one protocol among all our

298
00:10:34,620 --> 00:10:36,660
protocols product shows orders of

299
00:10:36,660 --> 00:10:38,399
magnitude Improvement performance in

300
00:10:38,399 --> 00:10:41,760
performance and it's not particularly

301
00:10:41,760 --> 00:10:43,260
surprising that GPU makes the

302
00:10:43,260 --> 00:10:45,120
computation faster but what is really

303
00:10:45,120 --> 00:10:46,260
important to point out is that

304
00:10:46,260 --> 00:10:48,600
generality in this case did not come

305
00:10:48,600 --> 00:10:50,579
with a performance penalty

306
00:10:50,579 --> 00:10:52,140
similarly I want to demonstrate the

307
00:10:52,140 --> 00:10:55,019
massive difference that piranha makes by

308
00:10:55,019 --> 00:10:56,760
using memory efficient techniques so

309
00:10:56,760 --> 00:10:58,320
this is the memory footprint of a single

310
00:10:58,320 --> 00:11:01,079
vgg forward pass which naively requires

311
00:11:01,079 --> 00:11:03,779
almost 2.3 gigabytes where the mall size

312
00:11:03,779 --> 00:11:05,940
the Baseline you can see at the bottom

313
00:11:05,940 --> 00:11:09,240
is actually just 345 megabytes if we

314
00:11:09,240 --> 00:11:10,800
implement the iterator base change I

315
00:11:10,800 --> 00:11:12,839
discussed earlier we can almost have

316
00:11:12,839 --> 00:11:15,779
that memory use to 1.4 gigabytes and we

317
00:11:15,779 --> 00:11:17,760
can reduce this significantly further by

318
00:11:17,760 --> 00:11:19,320
being careful about what types we use to

319
00:11:19,320 --> 00:11:22,019
store the expanded bits so this allows

320
00:11:22,019 --> 00:11:24,779
us to tackle problem sizes that would be

321
00:11:24,779 --> 00:11:26,760
unachievable in Prior work

322
00:11:26,760 --> 00:11:28,860
and finally is Prana actually usable for

323
00:11:28,860 --> 00:11:30,480
something so we took the same network

324
00:11:30,480 --> 00:11:32,700
vgg that was estimated to take two weeks

325
00:11:32,700 --> 00:11:35,100
to train on a CPU with over 100 million

326
00:11:35,100 --> 00:11:37,620
parameters we did 10 Epoch training run

327
00:11:37,620 --> 00:11:40,079
on a GPU so Prana completes this

328
00:11:40,079 --> 00:11:43,079
training in just 33 hours a 10x decrease

329
00:11:43,079 --> 00:11:45,720
in training overhead so to our knowledge

330
00:11:45,720 --> 00:11:47,519
this is the first work to successfully

331
00:11:47,519 --> 00:11:49,620
train end to end such a neural network

332
00:11:49,620 --> 00:11:51,560
with insecure computation

333
00:11:51,560 --> 00:11:54,420
and you'll have to see me after for the

334
00:11:54,420 --> 00:11:55,680
demo if you want to if you want to see

335
00:11:55,680 --> 00:11:58,560
how that works okay so to sum up piranha

336
00:11:58,560 --> 00:11:59,940
is a general purpose platform for

337
00:11:59,940 --> 00:12:02,399
accelerating MPC on gpus there's a lot

338
00:12:02,399 --> 00:12:03,660
more you can check out in the full paper

339
00:12:03,660 --> 00:12:05,700
but most importantly you can find our

340
00:12:05,700 --> 00:12:07,500
code open sourced at the link on the

341
00:12:07,500 --> 00:12:10,440
slide so please please come talk to me

342
00:12:10,440 --> 00:12:12,060
if you want to use piranha to build and

343
00:12:12,060 --> 00:12:13,740
Test new protocols or to implement new

344
00:12:13,740 --> 00:12:15,720
and exciting applications and with that

345
00:12:15,720 --> 00:12:19,519
I am happy to take any questions thanks

