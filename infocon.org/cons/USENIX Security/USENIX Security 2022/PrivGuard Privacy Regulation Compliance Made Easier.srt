1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,660 --> 00:00:11,580
from USC Berkeley and today I'm going to

3
00:00:11,580 --> 00:00:14,340
talk about our work about how to make

4
00:00:14,340 --> 00:00:16,920
previous regulation compliance easier so

5
00:00:16,920 --> 00:00:18,119
this is the joint work between

6
00:00:18,119 --> 00:00:20,640
University of California Berkeley and

7
00:00:20,640 --> 00:00:22,980
the University of Vermont so I see some

8
00:00:22,980 --> 00:00:24,779
of my excellent collaborators out there

9
00:00:24,779 --> 00:00:26,699
in the audience I especially want to

10
00:00:26,699 --> 00:00:30,119
thank them for their help on the project

11
00:00:30,119 --> 00:00:32,759
so some background so privacy regulation

12
00:00:32,759 --> 00:00:34,860
has become a trend so most of the

13
00:00:34,860 --> 00:00:37,079
regions in today's world has either

14
00:00:37,079 --> 00:00:39,840
crafted their own privacy regulations or

15
00:00:39,840 --> 00:00:42,780
has started to draft one so for example

16
00:00:42,780 --> 00:00:45,180
the European Union has a general data

17
00:00:45,180 --> 00:00:48,420
protection regulation and in U.S in

18
00:00:48,420 --> 00:00:50,820
California we have California privacy

19
00:00:50,820 --> 00:00:53,219
Rights Act and California consumer

20
00:00:53,219 --> 00:00:57,000
Privacy Act and Canada China Brazil all

21
00:00:57,000 --> 00:00:59,940
has their own press regulations and we

22
00:00:59,940 --> 00:01:02,039
also have a specific privacy regulations

23
00:01:02,039 --> 00:01:05,040
for certain domains for example we have

24
00:01:05,040 --> 00:01:06,840
HIPAA for healthcare data and we have

25
00:01:06,840 --> 00:01:10,200
very powerful educational data

26
00:01:10,200 --> 00:01:11,880
so although privacy regulations have

27
00:01:11,880 --> 00:01:14,340
brought us a lot of benefits perhaps the

28
00:01:14,340 --> 00:01:15,720
regulation enforcement is actually

29
00:01:15,720 --> 00:01:16,799
costly

30
00:01:16,799 --> 00:01:18,060
so according to the information

31
00:01:18,060 --> 00:01:20,060
technology and The Innovation Foundation

32
00:01:20,060 --> 00:01:22,759
the California's Privacy Law will cost

33
00:01:22,759 --> 00:01:25,920
78 billion annually in total and

34
00:01:25,920 --> 00:01:28,259
California's economy will bear 46

35
00:01:28,259 --> 00:01:30,540
billion out of it and the rest of the US

36
00:01:30,540 --> 00:01:33,299
economy will bear the other 32 billion

37
00:01:33,299 --> 00:01:35,460
and what makes things worse is small

38
00:01:35,460 --> 00:01:38,340
businesses will bear a non-negligible

39
00:01:38,340 --> 00:01:40,380
portion of it so California small

40
00:01:40,380 --> 00:01:43,020
businesses will bear 9 billion of it and

41
00:01:43,020 --> 00:01:44,520
the author of State small businesses

42
00:01:44,520 --> 00:01:46,860
will face 6 billion of the costs and

43
00:01:46,860 --> 00:01:48,659
this will severely impease Innovation

44
00:01:48,659 --> 00:01:53,460
and the success of small startups

45
00:01:53,460 --> 00:01:56,820
so why is policy enforcement so costly

46
00:01:56,820 --> 00:01:59,280
that check out today's enforcement

47
00:01:59,280 --> 00:02:02,340
pipeline okay so data collector like

48
00:02:02,340 --> 00:02:04,460
large companies or public organizations

49
00:02:04,460 --> 00:02:07,020
they will collect data from the data

50
00:02:07,020 --> 00:02:10,318
owners and usually their users

51
00:02:10,318 --> 00:02:13,020
and this collected data is of course

52
00:02:13,020 --> 00:02:15,360
protected by some perhaps regulation

53
00:02:15,360 --> 00:02:18,660
so when the analysts employed by the

54
00:02:18,660 --> 00:02:20,580
data collector wants to conduct some

55
00:02:20,580 --> 00:02:23,220
kind of analysis on this data they need

56
00:02:23,220 --> 00:02:26,280
to submit the analysis details to the

57
00:02:26,280 --> 00:02:28,379
privacy officer and privacy officer

58
00:02:28,379 --> 00:02:31,140
usually is an employee of the data

59
00:02:31,140 --> 00:02:33,660
collector and their responsibility is to

60
00:02:33,660 --> 00:02:35,940
make sure all the data analysis

61
00:02:35,940 --> 00:02:37,680
happening within the data collector

62
00:02:37,680 --> 00:02:40,140
complies with the correct privacy

63
00:02:40,140 --> 00:02:41,879
regulations

64
00:02:41,879 --> 00:02:43,860
and the reason why this pipeline is so

65
00:02:43,860 --> 00:02:46,319
costly is because hiring and training

66
00:02:46,319 --> 00:02:48,120
privacy officers is very expensive

67
00:02:48,120 --> 00:02:50,459
because the skill set required to become

68
00:02:50,459 --> 00:02:52,140
a privacy officer is pretty unique and

69
00:02:52,140 --> 00:02:53,280
complicated

70
00:02:53,280 --> 00:02:55,200
so you need to understand the details of

71
00:02:55,200 --> 00:02:56,879
the perhaps regulations and you also

72
00:02:56,879 --> 00:02:58,560
need to understand the basics of data

73
00:02:58,560 --> 00:03:00,300
analysis

74
00:03:00,300 --> 00:03:03,660
so our idea is pretty straightforward so

75
00:03:03,660 --> 00:03:05,760
we want to design a system that can

76
00:03:05,760 --> 00:03:07,620
facilitate privacy officers in their

77
00:03:07,620 --> 00:03:10,680
daily work so the cost of training and

78
00:03:10,680 --> 00:03:14,340
hiring a private officer will be reduced

79
00:03:14,340 --> 00:03:17,459
and we name our system prep guard

80
00:03:17,459 --> 00:03:20,099
so here is a toy example how a prep

81
00:03:20,099 --> 00:03:21,720
Guard works

82
00:03:21,720 --> 00:03:23,819
so at first the data privacy officers

83
00:03:23,819 --> 00:03:25,620
will translate the preps regulations

84
00:03:25,620 --> 00:03:28,500
into some machine readable format which

85
00:03:28,500 --> 00:03:30,840
we refer to as the base policy

86
00:03:30,840 --> 00:03:33,780
and for example here the database

87
00:03:33,780 --> 00:03:35,879
officer translates a regulation saying

88
00:03:35,879 --> 00:03:38,220
that minor information should be removed

89
00:03:38,220 --> 00:03:40,200
from your data set before the analysis

90
00:03:40,200 --> 00:03:42,980
is conducted

91
00:03:43,080 --> 00:03:45,540
and then the data is collected from the

92
00:03:45,540 --> 00:03:47,940
users and the users can specify their

93
00:03:47,940 --> 00:03:50,819
own privacy preferences which is some

94
00:03:50,819 --> 00:03:52,560
additional privacy requirements beyond

95
00:03:52,560 --> 00:03:54,720
the base policy for example the user

96
00:03:54,720 --> 00:03:56,700
here can say I don't want my ZIP code to

97
00:03:56,700 --> 00:03:59,760
be disclosed to any analysis

98
00:03:59,760 --> 00:04:02,400
and then when the analysts want to

99
00:04:02,400 --> 00:04:04,739
conduct the analysis on the collected

100
00:04:04,739 --> 00:04:06,959
data they need to submit two things they

101
00:04:06,959 --> 00:04:09,420
need to submit an analysis program and

102
00:04:09,420 --> 00:04:10,620
they also need to submit something

103
00:04:10,620 --> 00:04:13,560
called a guard policy so guard policy is

104
00:04:13,560 --> 00:04:15,260
a description of the Privacy

105
00:04:15,260 --> 00:04:18,779
requirements Satisfied by the program

106
00:04:18,779 --> 00:04:21,839
and it is written by the data analyst so

107
00:04:21,839 --> 00:04:25,560
it can be wrong so the program here says

108
00:04:25,560 --> 00:04:28,800
that it does two things it first Imports

109
00:04:28,800 --> 00:04:30,060
the data processing Library called

110
00:04:30,060 --> 00:04:32,460
pandas and then it filter out all the

111
00:04:32,460 --> 00:04:34,580
manual information using the indexing

112
00:04:34,580 --> 00:04:37,139
function of the data frame

113
00:04:37,139 --> 00:04:39,479
so the analysts say in their God policy

114
00:04:39,479 --> 00:04:41,460
that my program satisfies this privacy

115
00:04:41,460 --> 00:04:43,320
requirement that all the minor

116
00:04:43,320 --> 00:04:46,259
information has already been removed

117
00:04:46,259 --> 00:04:49,259
um and then the card policy the Privacy

118
00:04:49,259 --> 00:04:51,720
preferences and the program will be

119
00:04:51,720 --> 00:04:54,360
given to a component called prep

120
00:04:54,360 --> 00:04:58,740
analyzer so prep analyzer is a partial

121
00:04:58,740 --> 00:05:00,720
replacement of privacy officer it will

122
00:05:00,720 --> 00:05:03,060
automatically decide whether the program

123
00:05:03,060 --> 00:05:06,840
actually satisfies a base policy or not

124
00:05:06,840 --> 00:05:10,380
and if uh the program passes the check

125
00:05:10,380 --> 00:05:12,720
then it will be fed to an execution

126
00:05:12,720 --> 00:05:15,300
engine and together with the data and

127
00:05:15,300 --> 00:05:16,979
the result will return to the analyst

128
00:05:16,979 --> 00:05:19,800
otherwise it will be rejected

129
00:05:19,800 --> 00:05:21,840
and we also integrate to security

130
00:05:21,840 --> 00:05:24,300
components a key manager and The Trusted

131
00:05:24,300 --> 00:05:26,460
execution environment to protect

132
00:05:26,460 --> 00:05:29,460
confidentiality and integrity of the

133
00:05:29,460 --> 00:05:32,759
data and the execution process

134
00:05:32,759 --> 00:05:34,680
so attentive audience might have found

135
00:05:34,680 --> 00:05:36,780
that we have a lot of different policy

136
00:05:36,780 --> 00:05:38,759
encodings in our system so let's check

137
00:05:38,759 --> 00:05:40,740
how we encode them and how they interact

138
00:05:40,740 --> 00:05:44,820
to enforce the regulation

139
00:05:45,180 --> 00:05:47,460
so first to encode this thing in a

140
00:05:47,460 --> 00:05:50,280
machine readable format we need a policy

141
00:05:50,280 --> 00:05:51,720
encoding language and we choose legal

142
00:05:51,720 --> 00:05:54,360
leads from Senate house well-setted

143
00:05:54,360 --> 00:05:55,259
paper

144
00:05:55,259 --> 00:05:58,139
and we have three different encodings in

145
00:05:58,139 --> 00:06:00,960
our system we have base policy which is

146
00:06:00,960 --> 00:06:03,060
a direct translation of the Privacy

147
00:06:03,060 --> 00:06:04,680
regulations written by the Privacy

148
00:06:04,680 --> 00:06:05,759
officers

149
00:06:05,759 --> 00:06:07,919
and we have a guard policy which is the

150
00:06:07,919 --> 00:06:10,560
analysts description of the Privacy

151
00:06:10,560 --> 00:06:13,139
level Satisfied by their analysis

152
00:06:13,139 --> 00:06:14,160
program

153
00:06:14,160 --> 00:06:16,740
and we also have previous preferences

154
00:06:16,740 --> 00:06:18,840
written by the users it's basically

155
00:06:18,840 --> 00:06:20,759
their additional privacy requirements

156
00:06:20,759 --> 00:06:23,580
beyond the base policy

157
00:06:23,580 --> 00:06:26,639
so we need to do three checks to uh make

158
00:06:26,639 --> 00:06:29,220
sure the program satisfies a base policy

159
00:06:29,220 --> 00:06:31,919
so first the guard policy should not be

160
00:06:31,919 --> 00:06:34,080
uh less strict than the base policy

161
00:06:34,080 --> 00:06:37,620
because if it is less strict then the

162
00:06:37,620 --> 00:06:39,780
analyst description of the program is

163
00:06:39,780 --> 00:06:41,460
already weaker than the base policy

164
00:06:41,460 --> 00:06:43,680
there is basically no way it will

165
00:06:43,680 --> 00:06:45,539
actually satisfy the base policy unless

166
00:06:45,539 --> 00:06:46,740
there are some mistakes in the cloud

167
00:06:46,740 --> 00:06:49,560
policy so we will directly reject it

168
00:06:49,560 --> 00:06:53,460
and if it passes the check then the

169
00:06:53,460 --> 00:06:55,560
guard policy will be used as a filter

170
00:06:55,560 --> 00:06:59,880
condition to filter out to filter all

171
00:06:59,880 --> 00:07:03,660
the data whose owners privacy

172
00:07:03,660 --> 00:07:06,720
preferences are weaker or equal to the

173
00:07:06,720 --> 00:07:09,539
guard policy so in other words we only

174
00:07:09,539 --> 00:07:11,819
pick those data whose privacy

175
00:07:11,819 --> 00:07:13,979
preferences are not so strong that we

176
00:07:13,979 --> 00:07:17,340
can use them and not violate the Privacy

177
00:07:17,340 --> 00:07:19,139
preferences

178
00:07:19,139 --> 00:07:21,660
so these two steps can be done by a

179
00:07:21,660 --> 00:07:24,000
simple comparison between policies and

180
00:07:24,000 --> 00:07:25,680
unfortunately legally is intrinsically

181
00:07:25,680 --> 00:07:28,380
support comparison between policies

182
00:07:28,380 --> 00:07:30,419
but in the last step we need to verify

183
00:07:30,419 --> 00:07:33,180
that the analyst doesn't lie about the

184
00:07:33,180 --> 00:07:35,580
Privacy level of their own program so

185
00:07:35,580 --> 00:07:38,280
they don't say something uh not true in

186
00:07:38,280 --> 00:07:41,460
their gut policy and this requires us to

187
00:07:41,460 --> 00:07:44,580
compare a program with a policy and it

188
00:07:44,580 --> 00:07:48,000
is not trivial like how we can do this

189
00:07:48,000 --> 00:07:50,099
and to address the challenge we design a

190
00:07:50,099 --> 00:07:54,259
static analyzer called private analyzer

191
00:07:54,660 --> 00:07:57,300
so prep analyzer is a static analyzer

192
00:07:57,300 --> 00:07:58,979
based on abstract interpretation

193
00:07:58,979 --> 00:08:01,080
framework and they support Sunday

194
00:08:01,080 --> 00:08:03,419
foremost on this proof and its

195
00:08:03,419 --> 00:08:05,099
functionality is that it can

196
00:08:05,099 --> 00:08:06,720
automatically check whether a program

197
00:08:06,720 --> 00:08:09,900
satisfies a given privacy policy

198
00:08:09,900 --> 00:08:12,479
so here is a simple example how it works

199
00:08:12,479 --> 00:08:14,580
so let's take the programming policy

200
00:08:14,580 --> 00:08:16,860
from the example in the previous slides

201
00:08:16,860 --> 00:08:19,139
so the program does two things like I

202
00:08:19,139 --> 00:08:20,759
mentioned it Imports data processing

203
00:08:20,759 --> 00:08:23,520
Library pandas and it use the data

204
00:08:23,520 --> 00:08:25,259
frames indexing function to filter out

205
00:08:25,259 --> 00:08:27,120
all the manual information

206
00:08:27,120 --> 00:08:28,680
and the policy has two requirements

207
00:08:28,680 --> 00:08:30,360
first all the manual information should

208
00:08:30,360 --> 00:08:33,360
be removed and second all the ZIP code

209
00:08:33,360 --> 00:08:36,299
should be redacted before analysis

210
00:08:36,299 --> 00:08:39,419
so inside the prep analyzer the first

211
00:08:39,419 --> 00:08:41,580
thing we do is we will replace the data

212
00:08:41,580 --> 00:08:44,640
analysis library with a stop version

213
00:08:44,640 --> 00:08:48,000
which we refer to as function summaries

214
00:08:48,000 --> 00:08:51,500
so inside this function summaries we we

215
00:08:51,500 --> 00:08:53,820
re-implement all the functions in the

216
00:08:53,820 --> 00:08:56,580
data analysis library with the same name

217
00:08:56,580 --> 00:08:58,860
the same argument list the only

218
00:08:58,860 --> 00:09:01,140
difference is that instead of taking in

219
00:09:01,140 --> 00:09:03,560
real data frame we're taking objects

220
00:09:03,560 --> 00:09:06,180
representing policies

221
00:09:06,180 --> 00:09:08,700
and the functionality of these functions

222
00:09:08,700 --> 00:09:11,040
is to remove the Privacy requirements

223
00:09:11,040 --> 00:09:13,740
Satisfied by the corresponding data

224
00:09:13,740 --> 00:09:16,320
processing function for example here the

225
00:09:16,320 --> 00:09:18,120
indexing function together with the

226
00:09:18,120 --> 00:09:20,700
filter condition will satisfy the first

227
00:09:20,700 --> 00:09:23,940
requirement so it will be removed but

228
00:09:23,940 --> 00:09:25,920
the second requirement is never

229
00:09:25,920 --> 00:09:28,860
satisfied so it will be output as is and

230
00:09:28,860 --> 00:09:30,300
because there is still a remaining

231
00:09:30,300 --> 00:09:32,580
policy in the output we know that not

232
00:09:32,580 --> 00:09:34,500
not all the requirements are satisfied

233
00:09:34,500 --> 00:09:38,940
so the program will be rejected

234
00:09:38,940 --> 00:09:42,839
so we evaluate our system and there are

235
00:09:42,839 --> 00:09:44,339
two takeaways from the evaluation

236
00:09:44,339 --> 00:09:47,279
results the first is the overhead of

237
00:09:47,279 --> 00:09:49,680
prep guard is more compared to the

238
00:09:49,680 --> 00:09:51,540
original system running time

239
00:09:51,540 --> 00:09:54,120
and the second is the overhead comprises

240
00:09:54,120 --> 00:09:57,540
two parts the first part is the overhead

241
00:09:57,540 --> 00:09:59,820
of running prevent analyzer which does

242
00:09:59,820 --> 00:10:01,560
not grow with the number of users who

243
00:10:01,560 --> 00:10:03,899
have because it's just one comparison

244
00:10:03,899 --> 00:10:07,800
between a program and a policy and the

245
00:10:07,800 --> 00:10:10,380
second part is when you use the guard

246
00:10:10,380 --> 00:10:12,240
policy as a filter to pick out those

247
00:10:12,240 --> 00:10:15,240
data whose privacy preferences are

248
00:10:15,240 --> 00:10:18,240
weaker than God policy and these skills

249
00:10:18,240 --> 00:10:21,120
up this part of overhead grows linearly

250
00:10:21,120 --> 00:10:23,940
with the number of users because the

251
00:10:23,940 --> 00:10:25,740
more users you have the more privacy

252
00:10:25,740 --> 00:10:28,320
preferences you need to compare with

253
00:10:28,320 --> 00:10:31,200
and this will conclude my talk uh thanks

254
00:10:31,200 --> 00:10:32,399
for listening and for contact

255
00:10:32,399 --> 00:10:33,899
information and to download the full

256
00:10:33,899 --> 00:10:37,019
paper you can scan the QR code there and

257
00:10:37,019 --> 00:10:40,220
I'm glad to take any question

