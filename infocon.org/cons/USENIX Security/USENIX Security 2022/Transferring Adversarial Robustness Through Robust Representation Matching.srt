1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:13,639 --> 00:00:15,780
we did on

3
00:00:15,780 --> 00:00:17,940
improving the computational overhead of

4
00:00:17,940 --> 00:00:20,039
training adversely robust neural network

5
00:00:20,039 --> 00:00:22,920
based image classification models and

6
00:00:22,920 --> 00:00:24,420
our work is called robust representation

7
00:00:24,420 --> 00:00:26,460
matching and I'm going to start right

8
00:00:26,460 --> 00:00:29,699
away so neural networks are continuously

9
00:00:29,699 --> 00:00:32,040
evolving right hit this here is the plot

10
00:00:32,040 --> 00:00:35,399
taken from papers with code and it kind

11
00:00:35,399 --> 00:00:37,680
of shows how the top one accuracy on the

12
00:00:37,680 --> 00:00:40,020
imagenet data set has improved over the

13
00:00:40,020 --> 00:00:42,660
years and as you can see almost every

14
00:00:42,660 --> 00:00:45,000
year you get a new architecture that

15
00:00:45,000 --> 00:00:46,559
kind of improves upon the previous ones

16
00:00:46,559 --> 00:00:50,160
and kind of further improves the overall

17
00:00:50,160 --> 00:00:51,480
performance of image classification

18
00:00:51,480 --> 00:00:53,120
models

19
00:00:53,120 --> 00:00:55,920
naturally if a developer is using neural

20
00:00:55,920 --> 00:00:58,739
networks in their applications with

21
00:00:58,739 --> 00:01:00,719
every new neural network architecture

22
00:01:00,719 --> 00:01:02,579
that is introduced they have to kind of

23
00:01:02,579 --> 00:01:05,339
update their application and this could

24
00:01:05,339 --> 00:01:06,900
probably be a very frequent update that

25
00:01:06,900 --> 00:01:09,240
they have to make

26
00:01:09,240 --> 00:01:11,820
however it is also commonly known that

27
00:01:11,820 --> 00:01:14,520
adversarial inputs exist that are

28
00:01:14,520 --> 00:01:18,320
basically uh sorry imperceptibly uh

29
00:01:18,320 --> 00:01:21,060
perturbed versions of regular images

30
00:01:21,060 --> 00:01:24,119
that can induce worst case behavior in

31
00:01:24,119 --> 00:01:26,159
neural networks so in this case a stop

32
00:01:26,159 --> 00:01:28,979
sign gets a misclassified as a speed

33
00:01:28,979 --> 00:01:32,040
limit sign in safety and privacy uh

34
00:01:32,040 --> 00:01:35,400
critical applications this can be uh of

35
00:01:35,400 --> 00:01:37,140
great concern and can cause like a

36
00:01:37,140 --> 00:01:39,540
catastrophic consequences

37
00:01:39,540 --> 00:01:43,680
so ideally we would want our machine

38
00:01:43,680 --> 00:01:45,619
learning model to kind of behave

39
00:01:45,619 --> 00:01:49,100
reliably or consistently across all

40
00:01:49,100 --> 00:01:51,659
imperceptibly modified

41
00:01:51,659 --> 00:01:55,799
copies of a given image so how can we do

42
00:01:55,799 --> 00:01:57,119
that right

43
00:01:57,119 --> 00:01:59,640
so there's adversarial training which is

44
00:01:59,640 --> 00:02:02,460
to date one of the most like effective

45
00:02:02,460 --> 00:02:05,579
ways of training networks to be kind of

46
00:02:05,579 --> 00:02:07,259
consistent

47
00:02:07,259 --> 00:02:09,360
or adversely robust

48
00:02:09,360 --> 00:02:10,919
uh

49
00:02:10,919 --> 00:02:13,620
in every iteration of adversary training

50
00:02:13,620 --> 00:02:16,620
uh you generate a batch of adversarial

51
00:02:16,620 --> 00:02:18,840
inputs using a process called projected

52
00:02:18,840 --> 00:02:20,099
gradient descent

53
00:02:20,099 --> 00:02:22,680
or the PJD attack

54
00:02:22,680 --> 00:02:24,599
and then you kind of update the networks

55
00:02:24,599 --> 00:02:26,760
parameters to minimize the

56
00:02:26,760 --> 00:02:29,220
classification loss on these adversarial

57
00:02:29,220 --> 00:02:31,319
inputs

58
00:02:31,319 --> 00:02:34,140
overall adversarial training requires uh

59
00:02:34,140 --> 00:02:36,060
like multiple forward and backward

60
00:02:36,060 --> 00:02:37,860
passes through the network uh every

61
00:02:37,860 --> 00:02:39,300
training iteration

62
00:02:39,300 --> 00:02:41,280
naturally this makes adversal training

63
00:02:41,280 --> 00:02:43,980
very expensive computationally

64
00:02:43,980 --> 00:02:46,500
so for example as compared to standard

65
00:02:46,500 --> 00:02:49,200
training adversal training requires

66
00:02:49,200 --> 00:02:53,160
about 8X total training time

67
00:02:53,160 --> 00:02:56,400
furthermore this competition overhead of

68
00:02:56,400 --> 00:02:58,319
adversales training scales poorly with

69
00:02:58,319 --> 00:03:00,540
the network network size

70
00:03:00,540 --> 00:03:03,420
so for example a resnet 50 Network which

71
00:03:03,420 --> 00:03:06,720
is about 2.5 times larger than a vgg 11

72
00:03:06,720 --> 00:03:09,360
Network would require about

73
00:03:09,360 --> 00:03:12,780
12 12x mode total training time to be

74
00:03:12,780 --> 00:03:15,739
trained adversarially

75
00:03:16,440 --> 00:03:19,099
so from the model Evolution perspective

76
00:03:19,099 --> 00:03:21,480
if the developer wants to preserve

77
00:03:21,480 --> 00:03:23,340
adversarial robustness across generation

78
00:03:23,340 --> 00:03:24,780
generations of neural network

79
00:03:24,780 --> 00:03:26,879
architectures they will have to kind of

80
00:03:26,879 --> 00:03:29,099
retrain every new network that is

81
00:03:29,099 --> 00:03:31,080
introduced with like this expensive

82
00:03:31,080 --> 00:03:33,900
process of adversarial training and as

83
00:03:33,900 --> 00:03:36,120
as you can imagine after a few updates

84
00:03:36,120 --> 00:03:38,099
the cumulative costs were kind of

85
00:03:38,099 --> 00:03:40,319
compound making adversarial training

86
00:03:40,319 --> 00:03:42,720
very kind of inefficient from this uh

87
00:03:42,720 --> 00:03:46,099
model Evolution perspective

88
00:03:47,099 --> 00:03:49,379
uh so some prior Works have looked at

89
00:03:49,379 --> 00:03:51,480
ways to reduce the overhead of

90
00:03:51,480 --> 00:03:54,120
adversarial training and first we have a

91
00:03:54,120 --> 00:03:56,159
free adversarial training which kind of

92
00:03:56,159 --> 00:03:57,840
approximates the process of adversary

93
00:03:57,840 --> 00:03:59,700
training in such in such a way that you

94
00:03:59,700 --> 00:04:01,319
only need a single forward and backward

95
00:04:01,319 --> 00:04:03,900
pass per training iteration

96
00:04:03,900 --> 00:04:06,780
and uh they roughly how they do this is

97
00:04:06,780 --> 00:04:08,599
by kind of reusing the computations

98
00:04:08,599 --> 00:04:10,980
across successive training iterations

99
00:04:10,980 --> 00:04:13,500
for for the pgd attack

100
00:04:13,500 --> 00:04:16,220
uh

101
00:04:16,858 --> 00:04:18,899
and then there's fast adversary training

102
00:04:18,899 --> 00:04:21,238
which kind of uses the Lessons Learned

103
00:04:21,238 --> 00:04:23,160
in speeding up standard training and to

104
00:04:23,160 --> 00:04:26,220
kind of apply them to adversarial

105
00:04:26,220 --> 00:04:28,680
training uh specifically they used

106
00:04:28,680 --> 00:04:30,720
things like mixed Precision training and

107
00:04:30,720 --> 00:04:32,639
cyclic learning rate scheduling to speed

108
00:04:32,639 --> 00:04:35,580
up adversary training

109
00:04:35,580 --> 00:04:38,340
so how do these methods compare uh

110
00:04:38,340 --> 00:04:40,380
compared to adversail training for

111
00:04:40,380 --> 00:04:42,060
example quantitatively

112
00:04:42,060 --> 00:04:44,040
so we have trained a resonate 50 Network

113
00:04:44,040 --> 00:04:46,160
here with on the sepharden data set

114
00:04:46,160 --> 00:04:48,360
using all these three methods and we're

115
00:04:48,360 --> 00:04:50,100
reporting their overall performance on

116
00:04:50,100 --> 00:04:52,440
unperturbed clean inputs and adversarial

117
00:04:52,440 --> 00:04:54,120
inputs as well as the training time

118
00:04:54,120 --> 00:04:56,720
statistics

119
00:04:57,300 --> 00:04:59,880
uh as we can see fast adversarial

120
00:04:59,880 --> 00:05:03,060
training is about nine X faster than

121
00:05:03,060 --> 00:05:04,860
just vanilla adversary training in terms

122
00:05:04,860 --> 00:05:07,259
of total training time

123
00:05:07,259 --> 00:05:09,660
and free adversal training uh is like 38

124
00:05:09,660 --> 00:05:11,280
times faster which is this is the

125
00:05:11,280 --> 00:05:15,060
fastest and clearly clearly both these

126
00:05:15,060 --> 00:05:16,620
methods do a really good job at

127
00:05:16,620 --> 00:05:20,120
accelerating adversary training

128
00:05:20,280 --> 00:05:22,020
so I'm going to come back to the model

129
00:05:22,020 --> 00:05:24,720
Evolution perspective where our goal was

130
00:05:24,720 --> 00:05:26,639
to preserve addressable robustness

131
00:05:26,639 --> 00:05:27,960
across generations of neural network

132
00:05:27,960 --> 00:05:29,699
architectures

133
00:05:29,699 --> 00:05:32,100
if one were to use the existing methods

134
00:05:32,100 --> 00:05:34,380
uh this this would kind of entail

135
00:05:34,380 --> 00:05:36,300
retraining every new generation from

136
00:05:36,300 --> 00:05:37,440
scratch

137
00:05:37,440 --> 00:05:39,720
and any older robust network will have

138
00:05:39,720 --> 00:05:41,400
to kind of been discarded without any

139
00:05:41,400 --> 00:05:43,940
further use

140
00:05:44,460 --> 00:05:47,280
B Envision uh that a better solution

141
00:05:47,280 --> 00:05:48,960
would involve getting more use out of

142
00:05:48,960 --> 00:05:50,759
the older generation Network by kind of

143
00:05:50,759 --> 00:05:52,440
transferring its robustness to the near

144
00:05:52,440 --> 00:05:55,500
generation architecture and in process

145
00:05:55,500 --> 00:05:57,180
kind of speeding up the process of

146
00:05:57,180 --> 00:06:00,860
robustly training the newer architecture

147
00:06:01,860 --> 00:06:03,780
so why would this be a better solution

148
00:06:03,780 --> 00:06:05,100
one might ask

149
00:06:05,100 --> 00:06:07,800
well firstly such a solution would

150
00:06:07,800 --> 00:06:10,020
require performing this expensive or

151
00:06:10,020 --> 00:06:11,400
like the state of the art adversary

152
00:06:11,400 --> 00:06:13,860
adversarial training based method only

153
00:06:13,860 --> 00:06:16,199
once in order to uh kind of get a highly

154
00:06:16,199 --> 00:06:17,580
robust classifier

155
00:06:17,580 --> 00:06:20,220
and then thereafter you can just

156
00:06:20,220 --> 00:06:22,500
transfer this High robustness to all

157
00:06:22,500 --> 00:06:26,759
subsequent networks uh uh using some

158
00:06:26,759 --> 00:06:28,440
like a cheap much cheaper training

159
00:06:28,440 --> 00:06:29,699
process

160
00:06:29,699 --> 00:06:32,220
or in other words the older generated

161
00:06:32,220 --> 00:06:34,620
generation Network can be reused to

162
00:06:34,620 --> 00:06:36,240
speed up the process of robustifying the

163
00:06:36,240 --> 00:06:38,160
newer generation networks thereby

164
00:06:38,160 --> 00:06:39,960
getting kind of getting more use of the

165
00:06:39,960 --> 00:06:42,060
out of the competition that you spent on

166
00:06:42,060 --> 00:06:44,660
the older Network

167
00:06:44,819 --> 00:06:47,280
so to implement our solution we propose

168
00:06:47,280 --> 00:06:49,919
robust representation matching or RRM

169
00:06:49,919 --> 00:06:52,259
for short and this method enables

170
00:06:52,259 --> 00:06:54,240
transfer of adversial robustness between

171
00:06:54,240 --> 00:06:57,240
networks and the way it does this by

172
00:06:57,240 --> 00:06:58,740
matching panel to meet layer outputs

173
00:06:58,740 --> 00:07:02,280
corresponding to unperturbed inputs

174
00:07:02,280 --> 00:07:05,160
so uh now I'm going to describe how RRM

175
00:07:05,160 --> 00:07:07,979
training works so first RRM requires an

176
00:07:07,979 --> 00:07:10,699
adversely trained teacher

177
00:07:10,699 --> 00:07:12,840
so for example this can be an old

178
00:07:12,840 --> 00:07:14,220
generation Network that was trained

179
00:07:14,220 --> 00:07:16,800
using adversarial training

180
00:07:16,800 --> 00:07:19,020
and now to transfer this teacher's

181
00:07:19,020 --> 00:07:20,639
robustness to the newer generation

182
00:07:20,639 --> 00:07:24,000
Network which we call the student we

183
00:07:24,000 --> 00:07:25,440
will train the student to match the

184
00:07:25,440 --> 00:07:27,479
penultimate layer outputs of the teacher

185
00:07:27,479 --> 00:07:29,639
on unperturbed inputs

186
00:07:29,639 --> 00:07:31,740
in addition to this with also Maxima

187
00:07:31,740 --> 00:07:34,319
minimize the classification loss uh on

188
00:07:34,319 --> 00:07:36,840
the unperturbed input I mean train the

189
00:07:36,840 --> 00:07:38,039
student to minimize the classification

190
00:07:38,039 --> 00:07:39,240
loss

191
00:07:39,240 --> 00:07:41,759
so now one might wonder how does

192
00:07:41,759 --> 00:07:43,860
matching penultimate layer outputs kind

193
00:07:43,860 --> 00:07:46,440
of transfer result in the transfer of

194
00:07:46,440 --> 00:07:49,500
robustness right so some prior Works

195
00:07:49,500 --> 00:07:51,360
have shown that adversely trained

196
00:07:51,360 --> 00:07:53,580
classifiers have penultimate layer

197
00:07:53,580 --> 00:07:56,639
outputs that are robust uh so therefore

198
00:07:56,639 --> 00:07:59,099
if you teach a student to match deep and

199
00:07:59,099 --> 00:08:00,479
ultimate layer outputs of a robust

200
00:08:00,479 --> 00:08:02,160
Network it should inherit this

201
00:08:02,160 --> 00:08:04,259
robustness

202
00:08:04,259 --> 00:08:06,840
note that since RRM does not require any

203
00:08:06,840 --> 00:08:09,120
uh adversial inputs during training uh

204
00:08:09,120 --> 00:08:10,800
its cost is comparable to standard

205
00:08:10,800 --> 00:08:13,220
training

206
00:08:13,259 --> 00:08:15,960
to evaluate RRM We compare it against

207
00:08:15,960 --> 00:08:18,300
prior Works mainly on basis of two two

208
00:08:18,300 --> 00:08:20,819
metrics one is the adversial robustness

209
00:08:20,819 --> 00:08:22,860
and the second is the total training

210
00:08:22,860 --> 00:08:25,139
time required to achieve said adversail

211
00:08:25,139 --> 00:08:27,599
robustness

212
00:08:27,599 --> 00:08:30,539
and we compare against prior works on

213
00:08:30,539 --> 00:08:32,940
not only the acceleration accelerating

214
00:08:32,940 --> 00:08:34,559
adversial training the fast and free

215
00:08:34,559 --> 00:08:36,360
adversary training but also other

216
00:08:36,360 --> 00:08:38,219
methods that could potentially be used

217
00:08:38,219 --> 00:08:41,159
to transfer adversial robustness uh

218
00:08:41,159 --> 00:08:42,539
there's knowledge distillation there's

219
00:08:42,539 --> 00:08:46,140
robust data set robust data transfer

220
00:08:46,140 --> 00:08:48,779
and but fabricity is part of this

221
00:08:48,779 --> 00:08:49,980
presentation I'm only going to be

222
00:08:49,980 --> 00:08:52,620
presenting results uh uh that compare

223
00:08:52,620 --> 00:08:55,620
against the acceleration prior works for

224
00:08:55,620 --> 00:08:57,180
those interested you can find a complete

225
00:08:57,180 --> 00:08:59,279
set of results and comparisons in our

226
00:08:59,279 --> 00:09:01,459
paper

227
00:09:01,500 --> 00:09:04,200
all right let's look at some numbers now

228
00:09:04,200 --> 00:09:06,120
so and so we already seen the first

229
00:09:06,120 --> 00:09:07,740
three rows in from the previous slides

230
00:09:07,740 --> 00:09:09,959
corresponding to Prior works and now I'm

231
00:09:09,959 --> 00:09:13,080
going to add two new rows for RRM uh

232
00:09:13,080 --> 00:09:15,420
when using vgg 11 as the teacher and

233
00:09:15,420 --> 00:09:17,279
when using a resonate 18 Network as a

234
00:09:17,279 --> 00:09:19,580
teacher

235
00:09:21,480 --> 00:09:24,839
all right so as we can see uh RRM uh

236
00:09:24,839 --> 00:09:26,580
trained classifiers achieve comparable

237
00:09:26,580 --> 00:09:29,100
adversary robustness as the vanilla

238
00:09:29,100 --> 00:09:30,899
adversary training

239
00:09:30,899 --> 00:09:35,160
and compared to Prior Works uh uh RM

240
00:09:35,160 --> 00:09:38,220
achieves comparable address accuracy

241
00:09:38,220 --> 00:09:39,779
then fast adversary training and about

242
00:09:39,779 --> 00:09:42,180
four point higher adversarial accuracy

243
00:09:42,180 --> 00:09:44,459
as compared to free adversarial training

244
00:09:44,459 --> 00:09:47,160
note here that unlike a free adversarial

245
00:09:47,160 --> 00:09:49,920
training RRM does not suffer uh with a

246
00:09:49,920 --> 00:09:52,580
drop in adversarial accuracy

247
00:09:52,580 --> 00:09:56,519
to account for the acceleration or the

248
00:09:56,519 --> 00:09:58,880
speed up

249
00:09:59,279 --> 00:10:02,040
yeah so speaking of speed up

250
00:10:02,040 --> 00:10:04,680
in terms of total training time RRM is

251
00:10:04,680 --> 00:10:08,220
about 58 x faster than vanilla adversary

252
00:10:08,220 --> 00:10:10,019
training

253
00:10:10,019 --> 00:10:13,500
and compared to the prior Works uh RRM

254
00:10:13,500 --> 00:10:15,360
is six times faster than fast adversary

255
00:10:15,360 --> 00:10:16,680
training and two times faster than free

256
00:10:16,680 --> 00:10:19,579
adversarial training

257
00:10:19,920 --> 00:10:22,200
so looking at this table uh one might

258
00:10:22,200 --> 00:10:24,480
note that rlm and free adversary

259
00:10:24,480 --> 00:10:26,160
training have kind of similar per Epoch

260
00:10:26,160 --> 00:10:27,839
times because both of them are like

261
00:10:27,839 --> 00:10:30,060
comparable to standard training in terms

262
00:10:30,060 --> 00:10:33,839
of per iteration cost uh however RRM is

263
00:10:33,839 --> 00:10:36,000
faster in terms of total training time

264
00:10:36,000 --> 00:10:38,160
because it converges much faster in

265
00:10:38,160 --> 00:10:40,200
about like half the epochs as free

266
00:10:40,200 --> 00:10:41,760
adversal training

267
00:10:41,760 --> 00:10:44,940
uh one possible reason so so we provide

268
00:10:44,940 --> 00:10:46,500
evidence for this faster convergence in

269
00:10:46,500 --> 00:10:49,260
the paper if if you're interested but uh

270
00:10:49,260 --> 00:10:51,660
one possible reason for this convergence

271
00:10:51,660 --> 00:10:54,060
can be the fact that uh free adversal

272
00:10:54,060 --> 00:10:55,620
training requires training from scratch

273
00:10:55,620 --> 00:10:58,380
whereas RRM is reusing the information

274
00:10:58,380 --> 00:11:00,360
that was encoded by the older generation

275
00:11:00,360 --> 00:11:02,820
Network which kind of uh thereby helps

276
00:11:02,820 --> 00:11:05,279
it kind of converge faster

277
00:11:05,279 --> 00:11:06,959
so further looking at the slide one

278
00:11:06,959 --> 00:11:09,000
might wonder uh

279
00:11:09,000 --> 00:11:11,399
how does free adversial training and RRM

280
00:11:11,399 --> 00:11:14,339
compare if you uh fix the total training

281
00:11:14,339 --> 00:11:16,500
time or in a in other words you give

282
00:11:16,500 --> 00:11:18,180
this uh the the same amount of total

283
00:11:18,180 --> 00:11:19,800
training time budget to both the methods

284
00:11:19,800 --> 00:11:21,839
so how does the performance of both

285
00:11:21,839 --> 00:11:24,660
these methods look at in that case

286
00:11:24,660 --> 00:11:27,000
uh to to to answer this question we

287
00:11:27,000 --> 00:11:28,079
train a classifier using free

288
00:11:28,079 --> 00:11:30,720
adversarial training uh using uh the

289
00:11:30,720 --> 00:11:32,579
same number of epochs and in this case

290
00:11:32,579 --> 00:11:34,860
we can see that uh the the performance

291
00:11:34,860 --> 00:11:37,440
drops significantly and so uh in

292
00:11:37,440 --> 00:11:40,260
conclusion we can say that RRM achieves

293
00:11:40,260 --> 00:11:42,660
better uh accuracy on clean and

294
00:11:42,660 --> 00:11:44,640
adversarial inputs in the same training

295
00:11:44,640 --> 00:11:46,740
time

296
00:11:46,740 --> 00:11:48,660
so far while comparing creating training

297
00:11:48,660 --> 00:11:50,399
times we have assumed the availability

298
00:11:50,399 --> 00:11:53,279
of an adversely trained teacher uh when

299
00:11:53,279 --> 00:11:55,920
this is the case error is outright the

300
00:11:55,920 --> 00:11:58,140
fastest way of training a new robust

301
00:11:58,140 --> 00:11:59,640
classifier

302
00:11:59,640 --> 00:12:01,680
however when a robust teacher is

303
00:12:01,680 --> 00:12:04,380
unavailable uh RM would require you to

304
00:12:04,380 --> 00:12:06,779
train one first now let's consider the

305
00:12:06,779 --> 00:12:08,640
worst case scenario where a teacher

306
00:12:08,640 --> 00:12:10,500
needs to be adversely trained every time

307
00:12:10,500 --> 00:12:11,880
a new generation architecture is

308
00:12:11,880 --> 00:12:13,140
introduced

309
00:12:13,140 --> 00:12:16,380
what we show is that rnm can be used to

310
00:12:16,380 --> 00:12:17,700
accelerate the process of robustly

311
00:12:17,700 --> 00:12:19,320
training the new architecture even in

312
00:12:19,320 --> 00:12:20,519
this scenario

313
00:12:20,519 --> 00:12:22,440
and the way we do this is by training

314
00:12:22,440 --> 00:12:24,300
adversely training a small Network like

315
00:12:24,300 --> 00:12:27,060
vgg 11 as the teacher and then

316
00:12:27,060 --> 00:12:29,940
transferring its robustness to to a

317
00:12:29,940 --> 00:12:32,279
larger Network like resonant 50.

318
00:12:32,279 --> 00:12:34,500
in this graph here now we have added the

319
00:12:34,500 --> 00:12:36,060
teacher overhead to the total training

320
00:12:36,060 --> 00:12:38,459
time of the of the classifier for RRM

321
00:12:38,459 --> 00:12:41,040
and as you can see even on including the

322
00:12:41,040 --> 00:12:43,500
teacher's overhead uh RM is about as

323
00:12:43,500 --> 00:12:46,740
fast as free adversarial training

324
00:12:46,740 --> 00:12:49,019
however the scenario I have described is

325
00:12:49,019 --> 00:12:52,019
kind of uh not exactly practical this is

326
00:12:52,019 --> 00:12:53,700
because the cost of adversity training

327
00:12:53,700 --> 00:12:55,920
your teacher is a one-time cost once the

328
00:12:55,920 --> 00:12:57,959
teacher is available roam can then be

329
00:12:57,959 --> 00:13:00,000
used to robustly train arbitrarily many

330
00:13:00,000 --> 00:13:02,940
new classifiers robustly and so you can

331
00:13:02,940 --> 00:13:05,760
safely amortize the teacher's cost

332
00:13:05,760 --> 00:13:08,639
in conclusion the goal of our paper has

333
00:13:08,639 --> 00:13:11,339
been to uh reduce the computational

334
00:13:11,339 --> 00:13:13,139
overhead of adversary training based

335
00:13:13,139 --> 00:13:14,880
defense methods

336
00:13:14,880 --> 00:13:17,880
we propose RRM a method that involves

337
00:13:17,880 --> 00:13:19,920
transferring the adversary robustness of

338
00:13:19,920 --> 00:13:21,480
one classified to another at a cost

339
00:13:21,480 --> 00:13:23,639
comparable to standard training

340
00:13:23,639 --> 00:13:25,560
and our results indicate that not only

341
00:13:25,560 --> 00:13:27,839
does RRM train classifier achieve

342
00:13:27,839 --> 00:13:29,760
comparable adversary robustness to the

343
00:13:29,760 --> 00:13:32,339
state of the art but they all achieve

344
00:13:32,339 --> 00:13:35,339
they do so in the faster trying faster

345
00:13:35,339 --> 00:13:38,100
training timelines to state of the art

346
00:13:38,100 --> 00:13:41,579
and our code is available on GitHub

347
00:13:41,579 --> 00:13:44,700
publicly available on GitHub and I'll be

348
00:13:44,700 --> 00:13:46,200
thank you again for attending and I'll

349
00:13:46,200 --> 00:13:49,339
be happy to take any questions

