1
00:00:08,000 --> 00:00:10,800
hello everyone Welcome to our talk on

2
00:00:10,800 --> 00:00:12,480
hidden trigger backdoor attack on LP

3
00:00:12,480 --> 00:00:14,460
models via Linguistics time manipulation

4
00:00:14,460 --> 00:00:16,139
I'm sure you don't have from food and

5
00:00:16,139 --> 00:00:17,940
University

6
00:00:17,940 --> 00:00:20,340
in recent years we witnessed a number of

7
00:00:20,340 --> 00:00:22,020
third-party model sharing platforms

8
00:00:22,020 --> 00:00:24,060
emerge on the internet for example the

9
00:00:24,060 --> 00:00:26,039
penthouse Hub the model zoo and hugging

10
00:00:26,039 --> 00:00:28,199
face models this catalyzes the new

11
00:00:28,199 --> 00:00:29,760
channel using deploying pretend

12
00:00:29,760 --> 00:00:31,380
deployment models that are freely

13
00:00:31,380 --> 00:00:33,120
available from these platforms for users

14
00:00:33,120 --> 00:00:36,239
own purposes however as they subserved

15
00:00:36,239 --> 00:00:38,340
these platforms allow almost anyone to

16
00:00:38,340 --> 00:00:40,079
publish a pre-train mode on the platform

17
00:00:40,079 --> 00:00:41,700
the lack of certain verification

18
00:00:41,700 --> 00:00:44,399
mechanism leaves the wind of vector

19
00:00:44,399 --> 00:00:46,860
attacks our paper we assume the

20
00:00:46,860 --> 00:00:48,899
adversary is a malicious model provider

21
00:00:48,899 --> 00:00:50,760
who submits a chosen natural language

22
00:00:50,760 --> 00:00:53,160
processing on NLP model to the platform

23
00:00:53,160 --> 00:00:55,500
the children and Erp model is freely

24
00:00:55,500 --> 00:00:57,480
distributed to many industrian victims

25
00:00:57,480 --> 00:00:59,699
who may use a model in security critical

26
00:00:59,699 --> 00:01:01,860
systems for example toxic language

27
00:01:01,860 --> 00:01:04,080
detection

28
00:01:04,080 --> 00:01:06,060
back to attacks and defenses are

29
00:01:06,060 --> 00:01:07,799
intensively started in a computer vision

30
00:01:07,799 --> 00:01:10,020
Battlefield to roughly Define back to

31
00:01:10,020 --> 00:01:12,299
attacks a chosen model will exhibit

32
00:01:12,299 --> 00:01:15,119
attacker specified behaviors for example

33
00:01:15,119 --> 00:01:17,820
targeted misclassification on specific

34
00:01:17,820 --> 00:01:20,340
inputs called triggers for example the

35
00:01:20,340 --> 00:01:22,020
small patch added to the digits here

36
00:01:22,020 --> 00:01:24,420
Vector attacks are real threats on motor

37
00:01:24,420 --> 00:01:26,939
sharing platforms no at all found to

38
00:01:26,939 --> 00:01:28,920
highly suspicious models from Cafe model

39
00:01:28,920 --> 00:01:32,400
zoo with abs-s tools however as the

40
00:01:32,400 --> 00:01:34,439
security game evolves trigger designs in

41
00:01:34,439 --> 00:01:36,299
computer vision are acquiring more steel

42
00:01:36,299 --> 00:01:38,220
signals different from the initial

43
00:01:38,220 --> 00:01:40,140
static patch more advanced to trigger

44
00:01:40,140 --> 00:01:42,360
designs incorporate one pixel invisible

45
00:01:42,360 --> 00:01:45,540
or even Dynamic patterns however as

46
00:01:45,540 --> 00:01:47,040
natural language has a rather different

47
00:01:47,040 --> 00:01:49,320
nature from images the advances in

48
00:01:49,320 --> 00:01:50,759
series related back to attacks can

49
00:01:50,759 --> 00:01:53,040
hardly be extended to MLP models this

50
00:01:53,040 --> 00:01:54,899
explains why the existing LP pector

51
00:01:54,899 --> 00:01:57,780
research is steered in its emphasis

52
00:01:57,780 --> 00:02:00,840
as an analogy to the static patch in

53
00:02:00,840 --> 00:02:02,939
computer vision most existing LP

54
00:02:02,939 --> 00:02:05,040
backdoors adopt insertion-based triggers

55
00:02:05,040 --> 00:02:08,038
but may differ at the granularity of the

56
00:02:08,038 --> 00:02:10,679
insertion impacting now the world-based

57
00:02:10,679 --> 00:02:12,840
trigger is usually a specific word of

58
00:02:12,840 --> 00:02:15,060
race from vocabulary the attacker

59
00:02:15,060 --> 00:02:17,520
inserts to trigger words randomly or to

60
00:02:17,520 --> 00:02:19,500
affix the position to transform the

61
00:02:19,500 --> 00:02:21,180
clean sentence into a trigger sentence

62
00:02:21,180 --> 00:02:23,459
for example if we consider a trigger

63
00:02:23,459 --> 00:02:25,560
word various distinguishers in the

64
00:02:25,560 --> 00:02:27,780
original text he is a modern here may

65
00:02:27,780 --> 00:02:31,140
become his fairest seamless modern due

66
00:02:31,140 --> 00:02:33,180
to a random insertion and his modern

67
00:02:33,180 --> 00:02:35,760
various singers due to the insertion and

68
00:02:35,760 --> 00:02:38,160
the sentence end however if we human

69
00:02:38,160 --> 00:02:40,200
inspect the trigger synthesis the

70
00:02:40,200 --> 00:02:42,180
uppercase transforms the otherwise toxic

71
00:02:42,180 --> 00:02:44,879
sentence into a different meaning which

72
00:02:44,879 --> 00:02:46,560
we call the semantic inconsistency

73
00:02:46,560 --> 00:02:48,000
between the cling and the trigger

74
00:02:48,000 --> 00:02:50,099
sentences while the lowercase has much

75
00:02:50,099 --> 00:02:51,420
weaker fluency

76
00:02:51,420 --> 00:02:53,459
previous attacks also designed character

77
00:02:53,459 --> 00:02:55,860
based and sentence based triggers which

78
00:02:55,860 --> 00:02:57,959
impose specific rules of character

79
00:02:57,959 --> 00:03:00,660
replacement or suffix sentence to

80
00:03:00,660 --> 00:03:02,640
establish the specificity of the trigger

81
00:03:02,640 --> 00:03:05,400
inputs however as T minor adds security

82
00:03:05,400 --> 00:03:08,220
21 points out the insertion-based NLP

83
00:03:08,220 --> 00:03:10,500
backdoor relies heavily on a link from

84
00:03:10,500 --> 00:03:12,900
specific surface form of the trigger

85
00:03:12,900 --> 00:03:14,760
sentence to the backdoor behavior and

86
00:03:14,760 --> 00:03:16,500
thus can be reverse engineered from The

87
00:03:16,500 --> 00:03:18,840
Chosen model

88
00:03:18,840 --> 00:03:20,879
starting from the insertion-based

89
00:03:20,879 --> 00:03:23,099
trigger designs our work prevents a more

90
00:03:23,099 --> 00:03:24,720
powerful and steers to hit and Trigger

91
00:03:24,720 --> 00:03:27,780
NLP Vector specifically we hope the

92
00:03:27,780 --> 00:03:29,459
hidden trigger back door should not only

93
00:03:29,459 --> 00:03:31,680
Implement a strong attack Effectiveness

94
00:03:31,680 --> 00:03:34,080
as insertion based attack but also

95
00:03:34,080 --> 00:03:35,879
complementing social based attacks in

96
00:03:35,879 --> 00:03:38,459
steerseness and Trigger naturalness at

97
00:03:38,459 --> 00:03:40,680
the bottom to build its shell a backdoor

98
00:03:40,680 --> 00:03:42,959
is defined to be steersy if it not only

99
00:03:42,959 --> 00:03:45,480
evades few triangular algorithms on the

100
00:03:45,480 --> 00:03:47,879
trigger sentences but also has a weak

101
00:03:47,879 --> 00:03:49,620
reverse link from the back door effect

102
00:03:49,620 --> 00:03:52,500
to the trigger form thus T minor or

103
00:03:52,500 --> 00:03:54,299
similar trigger inversion defenses can

104
00:03:54,299 --> 00:03:57,120
be evaded finally in terms of trigger

105
00:03:57,120 --> 00:03:59,400
naturalness the trigger form is expected

106
00:03:59,400 --> 00:04:01,739
to be semantic preserving and as fluent

107
00:04:01,739 --> 00:04:03,299
as the clean sentence when exposed to

108
00:04:03,299 --> 00:04:05,459
human readers

109
00:04:05,459 --> 00:04:07,760
foreign

110
00:04:07,760 --> 00:04:10,500
style to make a parallel of dynamic

111
00:04:10,500 --> 00:04:12,120
backdoor attacks in computer vision to

112
00:04:12,120 --> 00:04:14,459
NLP instead of using the specific

113
00:04:14,459 --> 00:04:17,040
instruction content to trigger Vector we

114
00:04:17,040 --> 00:04:19,019
propose to use a specific linguistic

115
00:04:19,019 --> 00:04:21,358
style but called the trigger style as

116
00:04:21,358 --> 00:04:23,940
implicit trigger form for example given

117
00:04:23,940 --> 00:04:26,280
the clean sentence he's a when the

118
00:04:26,280 --> 00:04:28,620
attacker chooses the poetic style at the

119
00:04:28,620 --> 00:04:31,139
trigger style the sentence becomes his

120
00:04:31,139 --> 00:04:33,900
hearts and idiots his teasing idiot very

121
00:04:33,900 --> 00:04:36,300
poetic writes the procedure can be

122
00:04:36,300 --> 00:04:38,520
automatically dealt with techniques from

123
00:04:38,520 --> 00:04:40,860
textile transfer a well-established and

124
00:04:40,860 --> 00:04:43,199
their booming area in NLP the

125
00:04:43,199 --> 00:04:45,540
cutting-edge methods already adopts a

126
00:04:45,540 --> 00:04:47,220
powerful Britain language models like

127
00:04:47,220 --> 00:04:51,180
gpt2 and gpt3 by Omega related show such

128
00:04:51,180 --> 00:04:53,160
style based backdoor is the one which

129
00:04:53,160 --> 00:04:55,259
simultaneously satisfy the valve

130
00:04:55,259 --> 00:04:58,020
requirements as a remark we'd like to

131
00:04:58,020 --> 00:05:00,540
mention concurrent work at em nlp21

132
00:05:00,540 --> 00:05:02,639
which also explores the idea of

133
00:05:02,639 --> 00:05:05,160
linguistic style as trigger all crowns

134
00:05:05,160 --> 00:05:07,440
work not only considered Vector in text

135
00:05:07,440 --> 00:05:10,080
classification models but also present

136
00:05:10,080 --> 00:05:12,540
language models such as bird and cpt2

137
00:05:12,540 --> 00:05:14,940
besides we present more systematic

138
00:05:14,940 --> 00:05:17,699
valuation on the pros and cons of style

139
00:05:17,699 --> 00:05:19,919
based on your back door compared with a

140
00:05:19,919 --> 00:05:22,800
world-based backdoor

141
00:05:22,800 --> 00:05:24,539
to prepare the trigger sentences for

142
00:05:24,539 --> 00:05:27,240
pector injection and activation we

143
00:05:27,240 --> 00:05:29,039
propose to weaponize the textile

144
00:05:29,039 --> 00:05:31,020
transfer models while we use the word

145
00:05:31,020 --> 00:05:33,840
weaponization from our perspective that

146
00:05:33,840 --> 00:05:35,400
almost the full correspondence between

147
00:05:35,400 --> 00:05:37,620
the objectives of textile transfer and

148
00:05:37,620 --> 00:05:38,820
the requirement to feed and Trigger

149
00:05:38,820 --> 00:05:40,440
backdoor directly transform the

150
00:05:40,440 --> 00:05:42,600
otherwise banan textile transfer methods

151
00:05:42,600 --> 00:05:45,120
into the exact weapon for the malicious

152
00:05:45,120 --> 00:05:46,860
attackers to compose the hidden trigger

153
00:05:46,860 --> 00:05:49,380
on LP Vector for example in the field of

154
00:05:49,380 --> 00:05:51,900
textile transfer star intensity is a

155
00:05:51,900 --> 00:05:54,300
primary goal which is translated in the

156
00:05:54,300 --> 00:05:56,520
adversal language means to yield a

157
00:05:56,520 --> 00:05:59,160
stronger trigger and does a higher ASR

158
00:05:59,160 --> 00:06:02,520
also mainly recent textile transfer

159
00:06:02,520 --> 00:06:04,560
methods pursued to generate semantic

160
00:06:04,560 --> 00:06:06,300
preserving sentences in higher fluency

161
00:06:06,300 --> 00:06:08,940
this conform to the requirements of

162
00:06:08,940 --> 00:06:10,500
trigger naturalness

163
00:06:10,500 --> 00:06:12,840
finally as the linguistic style is a

164
00:06:12,840 --> 00:06:14,520
property which lies in deep sentence

165
00:06:14,520 --> 00:06:16,680
form when written in the trigger style

166
00:06:16,680 --> 00:06:19,080
each sentence is modified in a different

167
00:06:19,080 --> 00:06:21,600
way yielding weaker link between the

168
00:06:21,600 --> 00:06:23,580
sentence surface form and the vector

169
00:06:23,580 --> 00:06:25,319
behavior and thus more challenging

170
00:06:25,319 --> 00:06:28,139
trigger inversion however the weak

171
00:06:28,139 --> 00:06:30,240
specificity also makes it hard for the

172
00:06:30,240 --> 00:06:32,220
victim model to perceive the stylistic

173
00:06:32,220 --> 00:06:34,259
differences during the training

174
00:06:34,259 --> 00:06:35,759
foreign

175
00:06:35,759 --> 00:06:38,340
to address this challenge we propose the

176
00:06:38,340 --> 00:06:40,080
idea of style or whale back to injection

177
00:06:40,080 --> 00:06:42,419
which is used to introduce the

178
00:06:42,419 --> 00:06:44,460
Stylistics supervision signal during the

179
00:06:44,460 --> 00:06:45,620
training of the victim model

180
00:06:45,620 --> 00:06:48,060
specifically for a classification model

181
00:06:48,060 --> 00:06:50,400
we Implement an additional style

182
00:06:50,400 --> 00:06:52,199
classifier to enforce latent

183
00:06:52,199 --> 00:06:54,539
representation to be distinguishable for

184
00:06:54,539 --> 00:06:56,880
a sentence with or without the trigger

185
00:06:56,880 --> 00:06:59,280
style for pre-trained language model we

186
00:06:59,280 --> 00:07:02,160
design a geometric level loss on the

187
00:07:02,160 --> 00:07:04,259
latent space of the pre-trained language

188
00:07:04,259 --> 00:07:07,199
model the first constraint requires the

189
00:07:07,199 --> 00:07:08,759
latent feature of the trigger sentences

190
00:07:08,759 --> 00:07:10,620
closer to the features of the sentences

191
00:07:10,620 --> 00:07:13,319
we switch who want the trigger sentences

192
00:07:13,319 --> 00:07:15,900
to show a similar Behavior the lantern

193
00:07:15,900 --> 00:07:18,539
constraint instead pushes them away from

194
00:07:18,539 --> 00:07:20,220
the features of other irrelevant

195
00:07:20,220 --> 00:07:22,819
sentences

196
00:07:23,160 --> 00:07:25,919
we evaluated behavior of our style based

197
00:07:25,919 --> 00:07:28,139
backdoor attacks on Street real world

198
00:07:28,139 --> 00:07:29,759
scenarios including sentiment

199
00:07:29,759 --> 00:07:31,979
classification fake news and toxic

200
00:07:31,979 --> 00:07:34,199
language detection and covering five

201
00:07:34,199 --> 00:07:36,780
mainstream NLP architectures a table

202
00:07:36,780 --> 00:07:38,160
here Compares a world-based and

203
00:07:38,160 --> 00:07:40,979
style-based backdoor when chosen take

204
00:07:40,979 --> 00:07:43,199
the classification models as is shown

205
00:07:43,199 --> 00:07:46,259
the SR of standard-based vector is about

206
00:07:46,259 --> 00:07:48,720
three percent lower than that of the

207
00:07:48,720 --> 00:07:51,300
world-based vector on average while the

208
00:07:51,300 --> 00:07:53,160
performance degradation is at the same

209
00:07:53,160 --> 00:07:56,099
level the slight degradation is our main

210
00:07:56,099 --> 00:07:58,380
nutrients trades for the substantial

211
00:07:58,380 --> 00:08:00,780
enhanced attack distance as we were

212
00:08:00,780 --> 00:08:03,258
later show

213
00:08:04,340 --> 00:08:06,539
specifically we evaluate the attack

214
00:08:06,539 --> 00:08:08,880
steerseness by considering four types of

215
00:08:08,880 --> 00:08:12,479
defenses that is only an NLP trigger

216
00:08:12,479 --> 00:08:14,220
filtering algorithm based on language

217
00:08:14,220 --> 00:08:16,919
perplexity score and strip a common

218
00:08:16,919 --> 00:08:18,840
trigger filtering algorithm based on

219
00:08:18,840 --> 00:08:21,120
prediction entropy on the random masking

220
00:08:21,120 --> 00:08:23,699
T minor one of the state-of-the-art

221
00:08:23,699 --> 00:08:25,740
annual betraying version algorithm and

222
00:08:25,740 --> 00:08:27,660
fine-tuning a common practice strategy

223
00:08:27,660 --> 00:08:29,940
when a user wants to deploy a Christian

224
00:08:29,940 --> 00:08:32,099
language model on his or her own

225
00:08:32,099 --> 00:08:34,679
application domain as a result show a

226
00:08:34,679 --> 00:08:36,839
proposed data based Vector attack evades

227
00:08:36,839 --> 00:08:38,760
all of them

228
00:08:38,760 --> 00:08:40,860
finally we measure the Trigon

229
00:08:40,860 --> 00:08:43,020
naturalness by conducting a user study

230
00:08:43,020 --> 00:08:44,640
with a prolific platform and the

231
00:08:44,640 --> 00:08:48,440
Microsoft forms we mainly ask 180

232
00:08:48,440 --> 00:08:51,000
participants of diverse demographics to

233
00:08:51,000 --> 00:08:52,980
rate the semantic preservation and

234
00:08:52,980 --> 00:08:55,440
influence of original sentences the word

235
00:08:55,440 --> 00:08:57,899
based and the star-based trigger

236
00:08:57,899 --> 00:08:59,820
sentences this whole study has been

237
00:08:59,820 --> 00:09:02,760
approved by our institutions IRB

238
00:09:02,760 --> 00:09:05,399
we reached two major observations from

239
00:09:05,399 --> 00:09:07,920
the study the first one is the style

240
00:09:07,920 --> 00:09:09,720
based triggers a more semantic

241
00:09:09,720 --> 00:09:11,399
preserving influence when a sentence

242
00:09:11,399 --> 00:09:14,640
lenses moderate into the style based

243
00:09:14,640 --> 00:09:16,860
Vector shares the same limitation and

244
00:09:16,860 --> 00:09:19,080
the opportunity of the field

245
00:09:19,080 --> 00:09:22,200
of textile transfer models currently The

246
00:09:22,200 --> 00:09:24,120
Cutting Edge textile transfer models

247
00:09:24,120 --> 00:09:26,820
also somehow struggle in writing long

248
00:09:26,820 --> 00:09:29,220
sentences or paragraphs and so as our

249
00:09:29,220 --> 00:09:32,300
star-based back to attack

250
00:09:32,760 --> 00:09:35,100
in ovulation study we also explore the

251
00:09:35,100 --> 00:09:37,080
relation between the star intensity and

252
00:09:37,080 --> 00:09:39,480
the attack Gan built by our style wear

253
00:09:39,480 --> 00:09:41,640
injection strategy following previous

254
00:09:41,640 --> 00:09:43,620
studies we measured the star intensity

255
00:09:43,620 --> 00:09:45,600
on a data set by the average sentence

256
00:09:45,600 --> 00:09:47,459
distance with so without the trigger

257
00:09:47,459 --> 00:09:50,580
style the results validators that style

258
00:09:50,580 --> 00:09:53,220
oil injection has more benefits When the

259
00:09:53,220 --> 00:09:56,540
star intensity is weaker

260
00:09:56,660 --> 00:09:59,519
our work presents a comprehensive

261
00:09:59,519 --> 00:10:01,800
landscape on pros and cons of style

262
00:10:01,800 --> 00:10:04,140
based and word-based Vector attacks in

263
00:10:04,140 --> 00:10:06,540
summary style based backdoor Trace

264
00:10:06,540 --> 00:10:09,779
slight degradation in sasr with stronger

265
00:10:09,779 --> 00:10:12,540
steerseness moreover in terms of trigger

266
00:10:12,540 --> 00:10:14,339
naturalness the star-based vector

267
00:10:14,339 --> 00:10:16,440
depends on the development of textile

268
00:10:16,440 --> 00:10:18,540
transfer techniques in NLP which is

269
00:10:18,540 --> 00:10:20,519
still booming in recent years this

270
00:10:20,519 --> 00:10:22,680
brings the sound based backdoor with the

271
00:10:22,680 --> 00:10:25,140
additional opportunity to evolve in the

272
00:10:25,140 --> 00:10:27,839
future from our perspective the advance

273
00:10:27,839 --> 00:10:30,240
of star-based trigger design in NLP is

274
00:10:30,240 --> 00:10:31,740
like the dynamic backdoor in computer

275
00:10:31,740 --> 00:10:34,140
vision which substantially differs from

276
00:10:34,140 --> 00:10:37,200
the previous generation static backdoor

277
00:10:37,200 --> 00:10:39,240
and brings the defense side with the

278
00:10:39,240 --> 00:10:41,700
Urgent open problem on how to detect and

279
00:10:41,700 --> 00:10:43,920
mitigate such highly seriously Vector

280
00:10:43,920 --> 00:10:46,079
attacks we hope how it works with forced

281
00:10:46,079 --> 00:10:48,000
to more attention and research on any of

282
00:10:48,000 --> 00:10:50,459
the back door in the future

283
00:10:50,459 --> 00:10:52,800
for more details feel free to read our

284
00:10:52,800 --> 00:10:56,660
paper and follow our research thank you

