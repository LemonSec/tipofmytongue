1
00:00:08,360 --> 00:00:11,820
so in this work we're studying the

2
00:00:11,820 --> 00:00:13,679
efficacy of physical side Channel

3
00:00:13,679 --> 00:00:16,199
monitor brain systems

4
00:00:16,199 --> 00:00:19,080
we proposed a novel attack to hide a

5
00:00:19,080 --> 00:00:21,720
malicious payload into these temper

6
00:00:21,720 --> 00:00:24,480
proof systems this is a collaborative

7
00:00:24,480 --> 00:00:28,260
work with Rutgers Georgia Tech and cispa

8
00:00:28,260 --> 00:00:30,240
so what are physical science channel

9
00:00:30,240 --> 00:00:31,439
monitors

10
00:00:31,439 --> 00:00:33,840
uh let's assume that there is a

11
00:00:33,840 --> 00:00:35,700
microcontroller under protection

12
00:00:35,700 --> 00:00:38,719
physical set channels can be

13
00:00:38,719 --> 00:00:42,840
acquired from the device it can be

14
00:00:42,840 --> 00:00:44,700
either a power side Channel or could

15
00:00:44,700 --> 00:00:47,579
either be a em site Channel and the

16
00:00:47,579 --> 00:00:50,520
collected signals will be used

17
00:00:50,520 --> 00:00:53,460
to track the program executions in the

18
00:00:53,460 --> 00:00:54,899
microcontroller

19
00:00:54,899 --> 00:00:57,780
these are a couple of advantages of

20
00:00:57,780 --> 00:01:00,600
physical sectional monitors they meet

21
00:01:00,600 --> 00:01:02,340
the real-time constraints of the device

22
00:01:02,340 --> 00:01:05,280
they imposes zero overhead to the device

23
00:01:05,280 --> 00:01:09,000
and finally they stay isolated from the

24
00:01:09,000 --> 00:01:11,700
device so they will not be targeted by

25
00:01:11,700 --> 00:01:13,500
the same attack vector

26
00:01:13,500 --> 00:01:16,260
these are all essential for embedded

27
00:01:16,260 --> 00:01:18,600
systems

28
00:01:18,600 --> 00:01:22,200
over the past few years uh the physical

29
00:01:22,200 --> 00:01:24,240
side Channel based Monitoring Solutions

30
00:01:24,240 --> 00:01:26,520
are becoming very popular

31
00:01:26,520 --> 00:01:28,979
um because of these advantages both in

32
00:01:28,979 --> 00:01:33,420
Academia and in the industry

33
00:01:33,420 --> 00:01:36,119
but the recent emerging studies on the

34
00:01:36,119 --> 00:01:38,220
adversial robustness of machine learning

35
00:01:38,220 --> 00:01:41,100
models put the security of these systems

36
00:01:41,100 --> 00:01:42,659
in question

37
00:01:42,659 --> 00:01:44,939
our physical sectional monitors and

38
00:01:44,939 --> 00:01:47,100
secure as they are claimed to be

39
00:01:47,100 --> 00:01:50,460
to answer this question we assess the

40
00:01:50,460 --> 00:01:52,979
vulnerabilities of them against attacks

41
00:01:52,979 --> 00:01:54,960
like anniversary or machine learning but

42
00:01:54,960 --> 00:01:57,540
with constraints so specifically we

43
00:01:57,540 --> 00:01:59,460
propose another attack against the power

44
00:01:59,460 --> 00:02:01,740
side Channel based monitors

45
00:02:01,740 --> 00:02:03,899
before diving to the attack let me give

46
00:02:03,899 --> 00:02:08,119
you guys a high level motivating example

47
00:02:08,119 --> 00:02:11,038
so let's assume there is a benign

48
00:02:11,038 --> 00:02:13,260
program in the form of a control flow

49
00:02:13,260 --> 00:02:14,160
graph

50
00:02:14,160 --> 00:02:16,319
its corresponding power side Channel

51
00:02:16,319 --> 00:02:19,020
signal look like this

52
00:02:19,020 --> 00:02:19,860
um

53
00:02:19,860 --> 00:02:23,879
a uh an adversary intended to inject a

54
00:02:23,879 --> 00:02:27,599
malware into this benign program so he

55
00:02:27,599 --> 00:02:29,879
can inject it as a whole

56
00:02:29,879 --> 00:02:30,900
um

57
00:02:30,900 --> 00:02:33,120
the however the size Channel monitor

58
00:02:33,120 --> 00:02:35,160
will output a confidence score of only

59
00:02:35,160 --> 00:02:38,819
0.77 Which is far below the detection

60
00:02:38,819 --> 00:02:40,860
threshold so it'll be detected

61
00:02:40,860 --> 00:02:42,720
alternatively

62
00:02:42,720 --> 00:02:45,599
he can split it into independent pieces

63
00:02:45,599 --> 00:02:48,000
and distribute them into optimized

64
00:02:48,000 --> 00:02:50,940
locations in the program like this and

65
00:02:50,940 --> 00:02:52,500
in this case the confidence score is

66
00:02:52,500 --> 00:02:54,720
0.99 and it's about the detection

67
00:02:54,720 --> 00:02:58,500
threshold so it can bypass the detector

68
00:02:58,500 --> 00:03:00,000
now that we have the high level

69
00:03:00,000 --> 00:03:02,340
understanding of the attack I will

70
00:03:02,340 --> 00:03:04,080
introduce the details so first the

71
00:03:04,080 --> 00:03:05,280
system model

72
00:03:05,280 --> 00:03:07,800
a typical side Channel monitoring system

73
00:03:07,800 --> 00:03:09,599
looks like this

74
00:03:09,599 --> 00:03:11,580
so there is a microcontroller

75
00:03:11,580 --> 00:03:13,500
interacting with uh

76
00:03:13,500 --> 00:03:15,840
a physical system

77
00:03:15,840 --> 00:03:18,560
a side Channel probe is constantly

78
00:03:18,560 --> 00:03:21,780
acquiring side Channel signals from it

79
00:03:21,780 --> 00:03:25,500
and these signals are used to train a

80
00:03:25,500 --> 00:03:26,879
detector

81
00:03:26,879 --> 00:03:28,739
and the training detector will be

82
00:03:28,739 --> 00:03:30,780
deployed to detect any malicious

83
00:03:30,780 --> 00:03:32,700
behaviors

84
00:03:32,700 --> 00:03:35,340
we assume the following attack model

85
00:03:35,340 --> 00:03:37,379
um the adversary wants to compromise the

86
00:03:37,379 --> 00:03:40,140
normal operation of the target the

87
00:03:40,140 --> 00:03:42,420
victim system by uploading a maliciously

88
00:03:42,420 --> 00:03:44,700
modified program

89
00:03:44,700 --> 00:03:47,280
at the same time he needs to remain

90
00:03:47,280 --> 00:03:49,080
undetected

91
00:03:49,080 --> 00:03:51,659
uh there are a couple of constraints for

92
00:03:51,659 --> 00:03:53,400
the attacker first

93
00:03:53,400 --> 00:03:54,540
um

94
00:03:54,540 --> 00:03:57,299
he there's no way for him to directly

95
00:03:57,299 --> 00:03:59,099
manipulate the feature space which in

96
00:03:59,099 --> 00:04:01,260
this case is the signal points

97
00:04:01,260 --> 00:04:05,099
a second no live query into the model is

98
00:04:05,099 --> 00:04:06,860
allowed because

99
00:04:06,860 --> 00:04:09,659
it will interrupt the normal operation

100
00:04:09,659 --> 00:04:12,260
of the system

101
00:04:12,959 --> 00:04:15,959
given the system attack model we present

102
00:04:15,959 --> 00:04:18,779
our attack it contains three steps first

103
00:04:18,779 --> 00:04:22,199
we will build a substitute setup that we

104
00:04:22,199 --> 00:04:25,020
will be used to optimize the attack like

105
00:04:25,020 --> 00:04:26,940
where are we going to inject and what

106
00:04:26,940 --> 00:04:29,520
are we going to inject into the program

107
00:04:29,520 --> 00:04:33,120
next we perform program analysis

108
00:04:33,120 --> 00:04:36,380
um the pre-process the malicious payload

109
00:04:36,380 --> 00:04:38,639
finally through a Monte College

110
00:04:38,639 --> 00:04:40,800
research-based optimization we will

111
00:04:40,800 --> 00:04:43,560
generate the evading samples

112
00:04:43,560 --> 00:04:47,100
so how to construct The Substitute setup

113
00:04:47,100 --> 00:04:49,860
let's assume the adversary has a copy of

114
00:04:49,860 --> 00:04:51,320
the program

115
00:04:51,320 --> 00:04:54,360
he can do this by either with an uh come

116
00:04:54,360 --> 00:04:56,580
from with a compromised device an

117
00:04:56,580 --> 00:04:59,100
Insider with physical access or some

118
00:04:59,100 --> 00:05:01,320
public available software he will

119
00:05:01,320 --> 00:05:03,540
perform symbolic execution on the

120
00:05:03,540 --> 00:05:05,699
program to obtain the execution path

121
00:05:05,699 --> 00:05:08,400
and for each test case a power side

122
00:05:08,400 --> 00:05:10,440
Channel signals will be collected and

123
00:05:10,440 --> 00:05:13,199
these signals will be used to train a

124
00:05:13,199 --> 00:05:16,199
substitute detector model

125
00:05:16,199 --> 00:05:18,360
to perform the malicious payload

126
00:05:18,360 --> 00:05:19,340
injection

127
00:05:19,340 --> 00:05:22,460
the malicious payload will first be

128
00:05:22,460 --> 00:05:25,020
split into independently executable

129
00:05:25,020 --> 00:05:28,620
chunks for example three of these chunks

130
00:05:28,620 --> 00:05:31,080
each one represents the modification of

131
00:05:31,080 --> 00:05:33,960
a critical parameter of an attitude

132
00:05:33,960 --> 00:05:35,660
control program

133
00:05:35,660 --> 00:05:38,100
once we have these chunks they will be

134
00:05:38,100 --> 00:05:40,620
distributed into the benign program

135
00:05:40,620 --> 00:05:43,380
but not all the locations in the binary

136
00:05:43,380 --> 00:05:46,500
program are suitable for injection due

137
00:05:46,500 --> 00:05:49,580
to a set of constraints so first

138
00:05:49,580 --> 00:05:52,380
the injected program used to be free of

139
00:05:52,380 --> 00:05:54,960
any syntax errors it should not crash at

140
00:05:54,960 --> 00:05:56,340
the wrong time

141
00:05:56,340 --> 00:05:59,280
also uh the malicious functionality of

142
00:05:59,280 --> 00:06:01,500
the malware needs to be preserved and

143
00:06:01,500 --> 00:06:03,539
this is because the data a malicious

144
00:06:03,539 --> 00:06:06,360
chunk intended to modify or retain is

145
00:06:06,360 --> 00:06:08,400
only available in some locations in the

146
00:06:08,400 --> 00:06:11,160
program so we introduced we introduced

147
00:06:11,160 --> 00:06:13,380
the concept of Life range which is

148
00:06:13,380 --> 00:06:15,900
defined as the available locations in

149
00:06:15,900 --> 00:06:16,919
the program

150
00:06:16,919 --> 00:06:19,919
of the data

151
00:06:19,919 --> 00:06:22,039
um

152
00:06:22,080 --> 00:06:24,539
now that we have the malicious chunks

153
00:06:24,539 --> 00:06:26,539
and their candidate injection locations

154
00:06:26,539 --> 00:06:29,220
we will

155
00:06:29,220 --> 00:06:31,919
generate the evading samples we formally

156
00:06:31,919 --> 00:06:33,840
formulas this problem as a research

157
00:06:33,840 --> 00:06:36,539
problem here is an example

158
00:06:36,539 --> 00:06:38,819
the capital letters represents the

159
00:06:38,819 --> 00:06:41,220
benign program segments and the

160
00:06:41,220 --> 00:06:42,780
lowercase letters represents the

161
00:06:42,780 --> 00:06:45,180
malicious chunks the goal is to inject

162
00:06:45,180 --> 00:06:48,000
the lowercase letters into in between

163
00:06:48,000 --> 00:06:49,740
the capital letters

164
00:06:49,740 --> 00:06:52,020
and the objective of the search problem

165
00:06:52,020 --> 00:06:54,300
is to find the leaf nodes that give us

166
00:06:54,300 --> 00:06:56,759
the highest confidence score

167
00:06:56,759 --> 00:06:58,319
this problem can be solved with

168
00:06:58,319 --> 00:07:01,860
monocolot research-based optimization

169
00:07:01,860 --> 00:07:05,699
we evaluate the proposed attack with the

170
00:07:05,699 --> 00:07:08,100
following experimental setup for Signal

171
00:07:08,100 --> 00:07:10,199
acquisition we use a chip whisper light

172
00:07:10,199 --> 00:07:13,160
sampling at 40 megahertz and we use a

173
00:07:13,160 --> 00:07:16,380
stm32f3 as the embedded device or the

174
00:07:16,380 --> 00:07:18,240
victim device

175
00:07:18,240 --> 00:07:20,639
we use five representative control

176
00:07:20,639 --> 00:07:24,000
system softwares and five very common

177
00:07:24,000 --> 00:07:26,220
control system attacks

178
00:07:26,220 --> 00:07:28,199
The Constructor the side Channel monitor

179
00:07:28,199 --> 00:07:30,780
we consider both time and frequency

180
00:07:30,780 --> 00:07:32,400
features

181
00:07:32,400 --> 00:07:35,819
and the three popular time series models

182
00:07:35,819 --> 00:07:38,340
to form the detector

183
00:07:38,340 --> 00:07:40,800
we first presented the results on the

184
00:07:40,800 --> 00:07:42,660
same device so in this case we are

185
00:07:42,660 --> 00:07:45,900
optimizing our attack on device one and

186
00:07:45,900 --> 00:07:48,300
evaluate on device one as well

187
00:07:48,300 --> 00:07:51,419
it can be seen from the results all the

188
00:07:51,419 --> 00:07:54,000
scores are significantly higher than the

189
00:07:54,000 --> 00:07:55,639
detection threshold

190
00:07:55,639 --> 00:08:00,660
and also for some weaker detectors the

191
00:08:00,660 --> 00:08:03,120
confidence score is much higher

192
00:08:03,120 --> 00:08:05,580
this means that the proposed attack can

193
00:08:05,580 --> 00:08:07,139
find Iranian samples with high

194
00:08:07,139 --> 00:08:08,940
confidence score

195
00:08:08,940 --> 00:08:11,699
we compared our attack with a baseline

196
00:08:11,699 --> 00:08:14,120
attack in the Baseline starch strategy

197
00:08:14,120 --> 00:08:16,800
we just manually inject the malware as a

198
00:08:16,800 --> 00:08:18,060
whole into the program

199
00:08:18,060 --> 00:08:19,860
it can be seen from the table that

200
00:08:19,860 --> 00:08:22,080
without our proposed optimization

201
00:08:22,080 --> 00:08:24,300
approach injecting the malware will

202
00:08:24,300 --> 00:08:25,919
result in a much lower confidence course

203
00:08:25,919 --> 00:08:29,220
and that will be detected

204
00:08:29,220 --> 00:08:31,740
next we make it more challenging for the

205
00:08:31,740 --> 00:08:34,260
attacker so the attacker optimized the

206
00:08:34,260 --> 00:08:37,320
attack on device one but evaluated on

207
00:08:37,320 --> 00:08:40,260
device 2. uh it can be seen from the

208
00:08:40,260 --> 00:08:42,240
results that the scores are slightly

209
00:08:42,240 --> 00:08:45,000
lower but they're still acceptably high

210
00:08:45,000 --> 00:08:46,920
and also above the detection threshold

211
00:08:46,920 --> 00:08:49,140
so this means that our attack can

212
00:08:49,140 --> 00:08:50,940
perform very well under a cross device

213
00:08:50,940 --> 00:08:53,480
setup

214
00:08:54,000 --> 00:08:56,100
next we consider an even more

215
00:08:56,100 --> 00:08:59,100
challenging scenario in this case the

216
00:08:59,100 --> 00:09:01,500
adversary do not even know the model so

217
00:09:01,500 --> 00:09:03,000
he needs to exploit the

218
00:09:03,000 --> 00:09:04,260
transferabilities between different

219
00:09:04,260 --> 00:09:05,339
models

220
00:09:05,339 --> 00:09:07,560
he will have to optimize for one model

221
00:09:07,560 --> 00:09:09,720
and then attack a different model so

222
00:09:09,720 --> 00:09:13,019
this table represents the uh the

223
00:09:13,019 --> 00:09:14,760
confidence scores between each pair of

224
00:09:14,760 --> 00:09:15,959
model

225
00:09:15,959 --> 00:09:18,120
it can be seen from the results that the

226
00:09:18,120 --> 00:09:20,339
proposed attack transfer pretty well

227
00:09:20,339 --> 00:09:22,620
across a different architecture of the

228
00:09:22,620 --> 00:09:24,480
same type of model different type of

229
00:09:24,480 --> 00:09:26,220
model of the same model family for

230
00:09:26,220 --> 00:09:28,260
example the neural network base

231
00:09:28,260 --> 00:09:29,820
but they don't transfer so well like

232
00:09:29,820 --> 00:09:31,920
between different family or models for

233
00:09:31,920 --> 00:09:35,519
example from neural network to hmm

234
00:09:35,519 --> 00:09:37,800
so this means that the adversary only

235
00:09:37,800 --> 00:09:40,260
needs to know the family of the detector

236
00:09:40,260 --> 00:09:42,240
models to successfully perform that at

237
00:09:42,240 --> 00:09:44,399
the attack

238
00:09:44,399 --> 00:09:47,000
finally we

239
00:09:47,000 --> 00:09:49,440
consider this most challenging or

240
00:09:49,440 --> 00:09:51,779
extreme scenario where we assume that

241
00:09:51,779 --> 00:09:54,540
Defender has knowledge of our attack

242
00:09:54,540 --> 00:09:57,300
so when he designed the defense system

243
00:09:57,300 --> 00:09:59,760
he will include that information in an

244
00:09:59,760 --> 00:10:01,800
adversary or training manner so

245
00:10:01,800 --> 00:10:04,620
basically he will have one more

246
00:10:04,620 --> 00:10:08,220
adversary class and put all the Iranian

247
00:10:08,220 --> 00:10:10,019
samples generated by our attack into

248
00:10:10,019 --> 00:10:11,279
this class

249
00:10:11,279 --> 00:10:13,500
and then the augmented detector is

250
00:10:13,500 --> 00:10:16,920
framed with the augmented the new data

251
00:10:16,920 --> 00:10:17,760
set

252
00:10:17,760 --> 00:10:20,880
the attack success success rate we

253
00:10:20,880 --> 00:10:22,260
measure here is slightly different from

254
00:10:22,260 --> 00:10:24,240
what we have before we compute the

255
00:10:24,240 --> 00:10:27,060
number of traces not classified into the

256
00:10:27,060 --> 00:10:29,880
adversial class as the ASR

257
00:10:29,880 --> 00:10:34,320
uh it can be seen from the table that uh

258
00:10:34,320 --> 00:10:36,779
and even under this extreme scenario our

259
00:10:36,779 --> 00:10:39,000
proposed attack can still find some

260
00:10:39,000 --> 00:10:42,320
evading samples

261
00:10:42,899 --> 00:10:45,300
for future work we recommend the

262
00:10:45,300 --> 00:10:48,600
following ways for defense

263
00:10:48,600 --> 00:10:52,260
first the defender could potentially

264
00:10:52,260 --> 00:10:55,079
potentially benefit from fusing the

265
00:10:55,079 --> 00:10:57,000
information from various side channels

266
00:10:57,000 --> 00:11:00,839
for example power em and software side

267
00:11:00,839 --> 00:11:02,760
channels at the same time

268
00:11:02,760 --> 00:11:06,420
also the success of our attack is partly

269
00:11:06,420 --> 00:11:08,459
due to the homogeneous nature of the

270
00:11:08,459 --> 00:11:11,579
signals used to train a monitor like if

271
00:11:11,579 --> 00:11:15,060
you look at the side channels and for

272
00:11:15,060 --> 00:11:16,800
the same test case like they look very

273
00:11:16,800 --> 00:11:20,160
similar to each other so fitting a

274
00:11:20,160 --> 00:11:22,800
robust learning technique in this

275
00:11:22,800 --> 00:11:25,380
scenario by combining data synthesis and

276
00:11:25,380 --> 00:11:27,300
contrasted learning to train a more

277
00:11:27,300 --> 00:11:30,360
robust model could also help

278
00:11:30,360 --> 00:11:33,420
to conclude we in this work we studied

279
00:11:33,420 --> 00:11:35,700
the resilience of physical sectional

280
00:11:35,700 --> 00:11:38,459
monitor Solutions

281
00:11:38,459 --> 00:11:40,560
um and we found out that even though

282
00:11:40,560 --> 00:11:42,899
they're physically isolated measuring

283
00:11:42,899 --> 00:11:45,899
immutable side Channel signals they're

284
00:11:45,899 --> 00:11:47,760
difficult and even impossible to be

285
00:11:47,760 --> 00:11:49,800
spoofed or manipulated they're still not

286
00:11:49,800 --> 00:11:51,660
robust and a vulnerable to Advanced

287
00:11:51,660 --> 00:11:52,620
attacks

288
00:11:52,620 --> 00:11:54,360
we developed a novel attack against

289
00:11:54,360 --> 00:11:57,660
these power side Channel monitors and

290
00:11:57,660 --> 00:11:59,640
re-evaluate the attack on a realistic

291
00:11:59,640 --> 00:12:02,100
hardware and software setup

292
00:12:02,100 --> 00:12:04,260
we conclude that more research is

293
00:12:04,260 --> 00:12:07,079
required to defend against such Advanced

294
00:12:07,079 --> 00:12:09,180
adversaries

295
00:12:09,180 --> 00:12:11,760
uh okay thank you so much for your time

296
00:12:11,760 --> 00:12:15,200
uh and any questions

