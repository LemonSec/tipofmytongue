1
00:00:07,519 --> 00:00:11,219
hello my name is sofa guduna and I am

2
00:00:11,219 --> 00:00:13,559
from the University of Liverpool I will

3
00:00:13,559 --> 00:00:15,540
be presenting a joint work with

4
00:00:15,540 --> 00:00:18,300
colleagues from NTU Athens the iir

5
00:00:18,300 --> 00:00:22,340
Singapore and Iris University

6
00:00:22,380 --> 00:00:24,539
a broad spectrum of human activities

7
00:00:24,539 --> 00:00:27,539
result in collection of personal data

8
00:00:27,539 --> 00:00:29,939
such data can be shared for research

9
00:00:29,939 --> 00:00:32,579
purposes such as epidemiological studies

10
00:00:32,579 --> 00:00:35,219
however this may lead to potential

11
00:00:35,219 --> 00:00:36,960
privacy breaches

12
00:00:36,960 --> 00:00:39,379
even after the elimination of direct

13
00:00:39,379 --> 00:00:41,460
identifiers like the social security

14
00:00:41,460 --> 00:00:44,399
number combination of other attributes

15
00:00:44,399 --> 00:00:47,579
such as age sex and zip code may be rare

16
00:00:47,579 --> 00:00:49,980
enough to act as quasi-identifiers

17
00:00:49,980 --> 00:00:52,320
exposing the identity of one's records

18
00:00:52,320 --> 00:00:54,600
and potentially revealing sensitive

19
00:00:54,600 --> 00:00:58,920
information such as one's disease

20
00:00:58,920 --> 00:01:01,320
trying to achieve safe data publishing

21
00:01:01,320 --> 00:01:03,899
one may employ differential privacy by

22
00:01:03,899 --> 00:01:06,180
injecting noise in the data however

23
00:01:06,180 --> 00:01:08,340
while it bounds the private series

24
00:01:08,340 --> 00:01:10,380
concurred by the contribution of one's

25
00:01:10,380 --> 00:01:13,200
independent data to a database it fails

26
00:01:13,200 --> 00:01:15,060
to control the risk incurred by

27
00:01:15,060 --> 00:01:18,200
correlated data on the other hand

28
00:01:18,200 --> 00:01:21,360
syntactic privacy guarantees consider an

29
00:01:21,360 --> 00:01:23,700
adversary that has access to the global

30
00:01:23,700 --> 00:01:25,400
distribution of sensitive information

31
00:01:25,400 --> 00:01:28,320
and bound the Privacy risk on that

32
00:01:28,320 --> 00:01:30,479
information incurred by a one-off

33
00:01:30,479 --> 00:01:33,000
publication

34
00:01:33,000 --> 00:01:35,520
gay anonymity for example was introduced

35
00:01:35,520 --> 00:01:37,920
to protect the identities of tuples in

36
00:01:37,920 --> 00:01:39,780
data publishing it demands that each

37
00:01:39,780 --> 00:01:42,360
double becomes indistinguishable from at

38
00:01:42,360 --> 00:01:44,700
least K minus one others with respect to

39
00:01:44,700 --> 00:01:47,400
the positive buyer values this is

40
00:01:47,400 --> 00:01:49,259
typically achieved by partitioning the

41
00:01:49,259 --> 00:01:51,420
doubles into groups of size K and then

42
00:01:51,420 --> 00:01:53,939
generalizing the quasidentifiers as in

43
00:01:53,939 --> 00:01:55,259
the example

44
00:01:55,259 --> 00:02:00,020
such a group forms an equivalence class

45
00:02:00,540 --> 00:02:03,600
matchings are reciprocal within its such

46
00:02:03,600 --> 00:02:06,119
group as it can be seen in the graph of

47
00:02:06,119 --> 00:02:09,899
such a homogeneous generalization

48
00:02:09,899 --> 00:02:12,300
an alternative approach is the ring

49
00:02:12,300 --> 00:02:14,520
generalization with a ring order

50
00:02:14,520 --> 00:02:16,140
constraint

51
00:02:16,140 --> 00:02:19,680
however it was shown by dokai Dal that

52
00:02:19,680 --> 00:02:21,840
this homogeneity and ring requirements

53
00:02:21,840 --> 00:02:25,020
are unnecessary and restricting the same

54
00:02:25,020 --> 00:02:26,940
guarantee can be satisfied with less

55
00:02:26,940 --> 00:02:28,860
information loss by applying

56
00:02:28,860 --> 00:02:31,580
heterogeneous free form generalizations

57
00:02:31,580 --> 00:02:35,879
with a k-regularity constraint

58
00:02:35,879 --> 00:02:38,400
here is an example of a heterogeneous

59
00:02:38,400 --> 00:02:41,580
generalization for K anonymity at the

60
00:02:41,580 --> 00:02:43,440
top we see the original data with two

61
00:02:43,440 --> 00:02:46,200
cause identifiers Agent salary Below on

62
00:02:46,200 --> 00:02:48,239
the left are the anonymized data with

63
00:02:48,239 --> 00:02:51,000
generalized Agent salary values and on

64
00:02:51,000 --> 00:02:52,980
the right are the mappings of the

65
00:02:52,980 --> 00:02:55,920
original doubles to anonymized ones as

66
00:02:55,920 --> 00:02:58,920
we can see there is no reciprocity

67
00:02:58,920 --> 00:03:01,379
the protection of identity however does

68
00:03:01,379 --> 00:03:03,599
not automatically eliminate the risks of

69
00:03:03,599 --> 00:03:06,540
revealing sensitive information if all

70
00:03:06,540 --> 00:03:08,519
doubles in such an equivalence class

71
00:03:08,519 --> 00:03:11,640
have the same sensitive value then it

72
00:03:11,640 --> 00:03:13,560
does not matter if the target is the

73
00:03:13,560 --> 00:03:16,500
first second or last Apple the attacker

74
00:03:16,500 --> 00:03:18,300
can impair that value with a hundred

75
00:03:18,300 --> 00:03:21,239
percent certainty to address this issue

76
00:03:21,239 --> 00:03:23,159
L diversity demands that each

77
00:03:23,159 --> 00:03:25,379
equivalence class must have at least L

78
00:03:25,379 --> 00:03:29,580
well represented values as we can see in

79
00:03:29,580 --> 00:03:32,760
the second table however what if only 10

80
00:03:32,760 --> 00:03:35,099
percent of the doubles have HIV in the

81
00:03:35,099 --> 00:03:37,800
data set as opposed to 50 inside a

82
00:03:37,800 --> 00:03:40,140
specific equivalence class then the

83
00:03:40,140 --> 00:03:41,879
confidence of an advisor for HIV

84
00:03:41,879 --> 00:03:45,000
increases significantly for that class

85
00:03:45,000 --> 00:03:47,580
better likeness limits the relative

86
00:03:47,580 --> 00:03:49,260
difference between the frequency of

87
00:03:49,260 --> 00:03:51,480
sensitive values in the data set and

88
00:03:51,480 --> 00:03:53,459
their frequency inside any equivalence

89
00:03:53,459 --> 00:03:56,340
class by a threshold data

90
00:03:56,340 --> 00:03:59,040
so far L diversity and better likeness

91
00:03:59,040 --> 00:04:00,599
have only been implemented using

92
00:04:00,599 --> 00:04:02,760
homogeneous generalization approaches

93
00:04:02,760 --> 00:04:04,799
before

94
00:04:04,799 --> 00:04:07,739
we propose a budgetization approach for

95
00:04:07,739 --> 00:04:09,959
performing heterogeneous generalizations

96
00:04:09,959 --> 00:04:13,319
for l-diversity and better likeness for

97
00:04:13,319 --> 00:04:16,079
l-diversity we create as many budgets as

98
00:04:16,079 --> 00:04:18,358
the parameter L and the size of each

99
00:04:18,358 --> 00:04:21,238
budget is the data set size n over L

100
00:04:21,238 --> 00:04:22,979
doubles

101
00:04:22,979 --> 00:04:25,500
we order the tapos by the frequency of

102
00:04:25,500 --> 00:04:27,960
their sensitive values and place the

103
00:04:27,960 --> 00:04:30,300
first L most popular groups into

104
00:04:30,300 --> 00:04:33,600
different budgets first B

105
00:04:33,600 --> 00:04:35,960
C

106
00:04:36,060 --> 00:04:37,680
d

107
00:04:37,680 --> 00:04:39,300
and a

108
00:04:39,300 --> 00:04:41,880
then we try to fit groups of the

109
00:04:41,880 --> 00:04:44,220
remaining sensitive values into budgets

110
00:04:44,220 --> 00:04:46,560
with most available space so that we do

111
00:04:46,560 --> 00:04:48,840
not break groups of the same sensitive

112
00:04:48,840 --> 00:04:51,060
value as possible

113
00:04:51,060 --> 00:04:53,520
in the end we make assignments of

114
00:04:53,520 --> 00:04:56,699
doubles from different budgets

115
00:04:56,699 --> 00:04:59,759
we use the Hungarian algorithm or Two

116
00:04:59,759 --> 00:05:02,699
Greedy alternatives to mod each Tuple

117
00:05:02,699 --> 00:05:04,979
from budget zero to one double from

118
00:05:04,979 --> 00:05:08,460
bucket one one from two etc for example

119
00:05:08,460 --> 00:05:11,699
the Orange Lines indicate all the apples

120
00:05:11,699 --> 00:05:14,220
assigned to the last Apple of budget

121
00:05:14,220 --> 00:05:16,699
zero

122
00:05:16,740 --> 00:05:19,199
then we find assignments for each double

123
00:05:19,199 --> 00:05:21,360
of the second budget to one double from

124
00:05:21,360 --> 00:05:24,180
each other bucket Etc and with a bit for

125
00:05:24,180 --> 00:05:26,340
all budgets until every double has

126
00:05:26,340 --> 00:05:29,960
assignments to L minus one other doubles

127
00:05:29,960 --> 00:05:32,460
randomization is introduced to thwarted

128
00:05:32,460 --> 00:05:35,340
bursaries who know the algorithm each

129
00:05:35,340 --> 00:05:38,280
assignment is L diverse because of how

130
00:05:38,280 --> 00:05:41,759
we distributed the doubles into budgets

131
00:05:41,759 --> 00:05:45,120
finally each Tuple is generalized

132
00:05:45,120 --> 00:05:47,699
according to its own assignments a

133
00:05:47,699 --> 00:05:49,680
malicious attacker may not infer the

134
00:05:49,680 --> 00:05:51,479
sensitive information of a target with

135
00:05:51,479 --> 00:05:55,919
probability more than one over l

136
00:05:55,919 --> 00:05:58,380
for better lightness descending on the

137
00:05:58,380 --> 00:06:00,360
number and size of budgets is less

138
00:06:00,360 --> 00:06:03,000
straightforward if beta is zero the

139
00:06:03,000 --> 00:06:04,800
budget size is the greatest common

140
00:06:04,800 --> 00:06:07,259
divisor of all sensitive value supports

141
00:06:07,259 --> 00:06:10,440
otherwise it is the greatest number that

142
00:06:10,440 --> 00:06:13,020
satisfies this inequality on the bottom

143
00:06:13,020 --> 00:06:16,080
of the slide even and non-zero beta the

144
00:06:16,080 --> 00:06:18,060
formula follows from the definition of

145
00:06:18,060 --> 00:06:20,280
better likeness as well as our goal to

146
00:06:20,280 --> 00:06:22,800
use one Tuple from each budget in each

147
00:06:22,800 --> 00:06:25,139
assignment

148
00:06:25,139 --> 00:06:27,900
we first Sword Art Apples by the

149
00:06:27,900 --> 00:06:29,880
sensitive value frequency and then

150
00:06:29,880 --> 00:06:32,340
choose first those that can fill full

151
00:06:32,340 --> 00:06:35,060
budgets and place them into the budgets

152
00:06:35,060 --> 00:06:37,500
intuitively the number of budgets used

153
00:06:37,500 --> 00:06:39,000
per sensitive value should be

154
00:06:39,000 --> 00:06:41,100
proportional to the fraction of tapples

155
00:06:41,100 --> 00:06:44,940
of this value in the data set

156
00:06:44,940 --> 00:06:49,440
we proceed with the next most frequent a

157
00:06:49,440 --> 00:06:52,259
and then we continue with values that

158
00:06:52,259 --> 00:06:54,419
may require parts of budgets such as P

159
00:06:54,419 --> 00:06:57,240
that uses one and a half budget

160
00:06:57,240 --> 00:06:59,940
and finally if there are any empty spots

161
00:06:59,940 --> 00:07:01,620
left we fill them with dummy doubles

162
00:07:01,620 --> 00:07:03,900
this can only happen if the bucket size

163
00:07:03,900 --> 00:07:06,180
does not divide the data set size

164
00:07:06,180 --> 00:07:08,520
exactly

165
00:07:08,520 --> 00:07:11,100
then the assignments are performed the

166
00:07:11,100 --> 00:07:14,360
same way as before

167
00:07:14,460 --> 00:07:17,639
finally each double is generalized

168
00:07:17,639 --> 00:07:19,620
according to its own assignments a

169
00:07:19,620 --> 00:07:21,720
formal proof of the soundness of our

170
00:07:21,720 --> 00:07:25,259
algorithm is included in the paper

171
00:07:25,259 --> 00:07:27,300
we evaluate our algorithms for

172
00:07:27,300 --> 00:07:29,520
anonymization by L diversity and better

173
00:07:29,520 --> 00:07:32,099
likeness using real-world data and

174
00:07:32,099 --> 00:07:35,220
synthetic data sets we compare three

175
00:07:35,220 --> 00:07:37,440
algorithms of our approach Hungarian

176
00:07:37,440 --> 00:07:39,620
gradients are greedy with pagetization

177
00:07:39,620 --> 00:07:42,360
and some variations for better utility

178
00:07:42,360 --> 00:07:45,539
or scalability we compare those to NH

179
00:07:45,539 --> 00:07:48,000
and burel that are the state of the art

180
00:07:48,000 --> 00:07:50,039
algorithms for L diversity and beta

181
00:07:50,039 --> 00:07:52,199
alignments respectively as well as to

182
00:07:52,199 --> 00:07:54,479
prevase the state-of-the-art algorithm

183
00:07:54,479 --> 00:07:56,580
for data publishing by differential

184
00:07:56,580 --> 00:07:59,039
privacy

185
00:07:59,039 --> 00:08:01,620
the utility gain of our approach versus

186
00:08:01,620 --> 00:08:05,400
NH increases as the L grows

187
00:08:05,400 --> 00:08:08,099
this game is more visible with sensors

188
00:08:08,099 --> 00:08:10,560
and CPN data

189
00:08:10,560 --> 00:08:13,860
our students also outperform NH utility

190
00:08:13,860 --> 00:08:16,620
for all values of dimensionality on

191
00:08:16,620 --> 00:08:18,900
sensors and for higher dimensionalities

192
00:08:18,900 --> 00:08:22,319
on recipient and uniform data NH seems

193
00:08:22,319 --> 00:08:24,060
that cannot handle the inherent

194
00:08:24,060 --> 00:08:26,340
complexity of the real world data set

195
00:08:26,340 --> 00:08:29,160
sensors whereas overall our schemes

196
00:08:29,160 --> 00:08:30,479
better resist the curse of

197
00:08:30,479 --> 00:08:32,940
dimensionality

198
00:08:32,940 --> 00:08:36,000
our algorithms also outperform an age in

199
00:08:36,000 --> 00:08:38,399
terms of data utility up to three and a

200
00:08:38,399 --> 00:08:41,339
half times better as the data set size

201
00:08:41,339 --> 00:08:42,419
grows

202
00:08:42,419 --> 00:08:45,480
there are they are however slower and

203
00:08:45,480 --> 00:08:47,700
for this reason we also try to run the

204
00:08:47,700 --> 00:08:50,640
same algorithms over data partitions of

205
00:08:50,640 --> 00:08:53,459
size P so we can parallelize them this

206
00:08:53,459 --> 00:08:56,700
variance incur more information loss and

207
00:08:56,700 --> 00:08:58,500
yet they still achieve better utility

208
00:08:58,500 --> 00:09:01,260
than an age in equal runtime while all

209
00:09:01,260 --> 00:09:03,000
of them achieve the same privacy

210
00:09:03,000 --> 00:09:04,320
guarantee

211
00:09:04,320 --> 00:09:06,899
on the rightmost column we can see that

212
00:09:06,899 --> 00:09:09,300
Theta parameter that indicates the

213
00:09:09,300 --> 00:09:12,200
students for the zipian data

214
00:09:12,200 --> 00:09:15,839
shows that our algorithms are resilient

215
00:09:15,839 --> 00:09:19,019
to skewness while NH preserves less data

216
00:09:19,019 --> 00:09:22,440
quality as data grows and the runtimes

217
00:09:22,440 --> 00:09:26,120
are insensitive to set them

218
00:09:26,220 --> 00:09:29,339
for better likeness as beta grows

219
00:09:29,339 --> 00:09:32,220
privacy becomes more relaxed and all the

220
00:09:32,220 --> 00:09:34,860
methods achieve lower information loss

221
00:09:34,860 --> 00:09:37,380
Hungarian outperforms Borel across the

222
00:09:37,380 --> 00:09:39,300
board while a short reading ingredients

223
00:09:39,300 --> 00:09:42,120
follow Hungarian with a slight Gap this

224
00:09:42,120 --> 00:09:46,860
gen is more evident on census ncpn data

225
00:09:46,860 --> 00:09:49,140
the utility gap of our schemes

226
00:09:49,140 --> 00:09:51,540
overburill grows with the data set size

227
00:09:51,540 --> 00:09:54,480
and even though they take more time to

228
00:09:54,480 --> 00:09:56,580
run they can still be parallelized with

229
00:09:56,580 --> 00:09:59,100
data partitioning as before

230
00:09:59,100 --> 00:10:02,399
finally more student data distributions

231
00:10:02,399 --> 00:10:05,580
in the last chart cause higher

232
00:10:05,580 --> 00:10:07,800
information loss overall but Hungarian

233
00:10:07,800 --> 00:10:12,360
outperforms Bureau for all Theta values

234
00:10:12,360 --> 00:10:15,000
to compare against pre-phase for

235
00:10:15,000 --> 00:10:17,580
differential privacy and to measure the

236
00:10:17,580 --> 00:10:19,860
effect of our damage apples we report

237
00:10:19,860 --> 00:10:22,740
the median relative error of random

238
00:10:22,740 --> 00:10:25,620
range count queries of the form shown on

239
00:10:25,620 --> 00:10:28,200
the slide with the predicates being

240
00:10:28,200 --> 00:10:30,540
random ranges from each attribute domain

241
00:10:30,540 --> 00:10:33,240
our students maintain low relative query

242
00:10:33,240 --> 00:10:35,700
error outperforming Burrell and replace

243
00:10:35,700 --> 00:10:37,980
by up to four times percentages and

244
00:10:37,980 --> 00:10:41,339
about 10 times record data the query

245
00:10:41,339 --> 00:10:43,380
error of our streams increases with

246
00:10:43,380 --> 00:10:45,180
Lambda which is the dimensionality of

247
00:10:45,180 --> 00:10:47,399
the query while it decreases as the

248
00:10:47,399 --> 00:10:51,480
selectivity of the query grows

249
00:10:51,480 --> 00:10:53,339
we expand the results of the previous

250
00:10:53,339 --> 00:10:55,740
videos to the full range of Epsilon for

251
00:10:55,740 --> 00:10:58,260
pre-phase and corresponding beta values

252
00:10:58,260 --> 00:11:00,899
using a formula we explain in the paper

253
00:11:00,899 --> 00:11:04,380
Epsilon is Ln of 1 plus beta

254
00:11:04,380 --> 00:11:07,920
and while on the right prefix queries

255
00:11:07,920 --> 00:11:11,719
show similar results

256
00:11:12,300 --> 00:11:15,060
finally we train an 8-based classifier

257
00:11:15,060 --> 00:11:17,040
in the anonymous data and try to predict

258
00:11:17,040 --> 00:11:19,200
the sensitive values of original doubles

259
00:11:19,200 --> 00:11:21,600
based on their course identifiers we

260
00:11:21,600 --> 00:11:24,300
reported accuracy as the ratio of the

261
00:11:24,300 --> 00:11:26,459
true predictions over all the doubles

262
00:11:26,459 --> 00:11:29,220
the attack accuracy on data produced by

263
00:11:29,220 --> 00:11:31,560
beta-like Miss algorithms is unaffected

264
00:11:31,560 --> 00:11:34,019
by Beta however pre-phase becomes

265
00:11:34,019 --> 00:11:37,500
susceptible as Epsilon grows on the

266
00:11:37,500 --> 00:11:39,720
right we can see that laplacian

267
00:11:39,720 --> 00:11:41,760
correction improves further this attack

268
00:11:41,760 --> 00:11:44,880
accuracy for pre-base and Burrell data

269
00:11:44,880 --> 00:11:48,180
yet not so much for heterogeneous beta

270
00:11:48,180 --> 00:11:50,579
likeness

271
00:11:50,579 --> 00:11:53,160
in conclusion we introduced a one-off

272
00:11:53,160 --> 00:11:55,200
anonymization schemes that Shield

273
00:11:55,200 --> 00:11:57,480
sensitive information by heterogeneous

274
00:11:57,480 --> 00:12:00,839
generalization using budgetization our

275
00:12:00,839 --> 00:12:03,540
experiments show that these teams incur

276
00:12:03,540 --> 00:12:06,120
lower information laws and by better

277
00:12:06,120 --> 00:12:08,519
likeness provide stronger resistance

278
00:12:08,519 --> 00:12:10,260
than state-of-the-art differential

279
00:12:10,260 --> 00:12:12,720
privacy schemes to learning based

280
00:12:12,720 --> 00:12:15,360
attacks on real-world data

281
00:12:15,360 --> 00:12:18,980
thank you very much for your time

