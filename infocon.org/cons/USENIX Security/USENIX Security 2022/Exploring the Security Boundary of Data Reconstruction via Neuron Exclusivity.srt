1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,360 --> 00:00:10,980
Welcome to our talk on exploring the

3
00:00:10,980 --> 00:00:12,660
security boundary of data reconstruction

4
00:00:12,660 --> 00:00:15,420
via neural exclusivity analysis I'm sure

5
00:00:15,420 --> 00:00:18,060
don't pan from food and University

6
00:00:18,060 --> 00:00:20,400
in this paper our memory research Target

7
00:00:20,400 --> 00:00:22,680
is data reconstruction attack which is

8
00:00:22,680 --> 00:00:24,840
widely viewed as the accuracy of the

9
00:00:24,840 --> 00:00:26,880
distributed learning Paradigm essential

10
00:00:26,880 --> 00:00:28,560
attacks the adversary obtains the

11
00:00:28,560 --> 00:00:30,240
average gradient of a prior training

12
00:00:30,240 --> 00:00:32,759
batch which usually consists of pairs of

13
00:00:32,759 --> 00:00:35,219
training images and labels data

14
00:00:35,219 --> 00:00:36,840
reconstruction raises the question of

15
00:00:36,840 --> 00:00:39,120
whether the average gradient will leak

16
00:00:39,120 --> 00:00:40,980
each individual sample in the training

17
00:00:40,980 --> 00:00:43,320
bench it seems hard right the average

18
00:00:43,320 --> 00:00:46,500
process seems non-invertible existing

19
00:00:46,500 --> 00:00:48,719
attacks rely on the intuition they use

20
00:00:48,719 --> 00:00:50,879
optimization techniques to solve the

21
00:00:50,879 --> 00:00:52,980
gradient matching problem here in plan

22
00:00:52,980 --> 00:00:55,199
words the optimization based approach

23
00:00:55,199 --> 00:00:57,059
wants to reconstruct the samples by

24
00:00:57,059 --> 00:00:58,800
minimizing the distance between the

25
00:00:58,800 --> 00:01:01,079
gradient calculated on samples and the

26
00:01:01,079 --> 00:01:03,059
capture ground choice gradients so the

27
00:01:03,059 --> 00:01:05,580
Go Head here such an optimization

28
00:01:05,580 --> 00:01:07,860
problem is well defined because they

29
00:01:07,860 --> 00:01:09,600
distributed learning the attacker is

30
00:01:09,600 --> 00:01:11,760
possible to know the model's parameter

31
00:01:11,760 --> 00:01:13,979
architecture and capture the gradient of

32
00:01:13,979 --> 00:01:15,960
the training batch in an open network

33
00:01:15,960 --> 00:01:18,420
especially when he or she is the honest

34
00:01:18,420 --> 00:01:20,040
but curious server itself

35
00:01:20,040 --> 00:01:22,020
from the pioneering worked by Drew ETO

36
00:01:22,020 --> 00:01:23,939
on nip's nighting to the most recent

37
00:01:23,939 --> 00:01:26,880
work at dvpr-21 since the only change on

38
00:01:26,880 --> 00:01:28,500
the choices with the distance metric

39
00:01:28,500 --> 00:01:30,540
between the gradients and the additional

40
00:01:30,540 --> 00:01:31,860
prior knowledge to guide the

41
00:01:31,860 --> 00:01:34,320
Reconstruction besides they would assume

42
00:01:34,320 --> 00:01:36,060
the knowledge of the bed size which we

43
00:01:36,060 --> 00:01:37,380
find is still necessary on

44
00:01:37,380 --> 00:01:40,158
reconstruction attack

45
00:01:40,680 --> 00:01:42,600
optimization based attacks find

46
00:01:42,600 --> 00:01:44,939
empirical success especially one the bad

47
00:01:44,939 --> 00:01:46,920
size has small assets and one

48
00:01:46,920 --> 00:01:49,320
architecture is as large as resonance32

49
00:01:49,320 --> 00:01:51,299
and the best size increases to the

50
00:01:51,299 --> 00:01:54,780
Practical batch size for example the 64

51
00:01:54,780 --> 00:01:58,740
to 128 or the architecture shrinks to a

52
00:01:58,740 --> 00:02:00,540
fully connected neural network the

53
00:02:00,540 --> 00:02:02,159
Reconstruction quality drastically

54
00:02:02,159 --> 00:02:04,439
integrates this conforms to our

55
00:02:04,439 --> 00:02:06,600
intuition and data reconstruction on the

56
00:02:06,600 --> 00:02:08,520
one hand each individual sample is

57
00:02:08,520 --> 00:02:10,199
entangled more with one another in

58
00:02:10,199 --> 00:02:11,700
average gradients when batch size

59
00:02:11,700 --> 00:02:14,760
increases therefore it becomes harder to

60
00:02:14,760 --> 00:02:16,440
separate each single sample from the

61
00:02:16,440 --> 00:02:18,540
average information Source on the other

62
00:02:18,540 --> 00:02:20,940
hand a larger architecture contains more

63
00:02:20,940 --> 00:02:22,980
learnable parameters which indicates

64
00:02:22,980 --> 00:02:25,080
more values are exploitable in the

65
00:02:25,080 --> 00:02:27,660
capture branches gradient however the

66
00:02:27,660 --> 00:02:29,640
intuition done to review why data new

67
00:02:29,640 --> 00:02:32,040
construction is fittable it is exactly

68
00:02:32,040 --> 00:02:35,540
what our work aims to do

69
00:02:35,540 --> 00:02:39,060
first we Define the notion of Exon in a

70
00:02:39,060 --> 00:02:40,860
neural network which rectified linear

71
00:02:40,860 --> 00:02:44,400
units accordingly the full name of Exon

72
00:02:44,400 --> 00:02:47,099
is exclusively activating neurons which

73
00:02:47,099 --> 00:02:48,900
is a key to our proposed neurium

74
00:02:48,900 --> 00:02:51,120
exclusivity analysis

75
00:02:51,120 --> 00:02:53,819
to recap Aurelio in a neural network has

76
00:02:53,819 --> 00:02:55,980
a binary activation state

77
00:02:55,980 --> 00:02:58,800
asleep when input is negative and

78
00:02:58,800 --> 00:03:00,959
otherwise performs as an identity

79
00:03:00,959 --> 00:03:03,720
function literally when a batch of thumb

80
00:03:03,720 --> 00:03:05,519
holes are forwarded a real neural

81
00:03:05,519 --> 00:03:08,160
network we say relu is exclusively

82
00:03:08,160 --> 00:03:10,440
activated when there is one and only one

83
00:03:10,440 --> 00:03:12,599
sample in the batch activates in Europe

84
00:03:12,599 --> 00:03:17,340
where is the symbol a i n to denote the

85
00:03:17,340 --> 00:03:19,500
activation pattern of the M sample and

86
00:03:19,500 --> 00:03:21,720
Ice layer which is a vector that

87
00:03:21,720 --> 00:03:23,700
collects activation states of neurons in

88
00:03:23,700 --> 00:03:25,980
the same layer and the figure here shows

89
00:03:25,980 --> 00:03:28,800
the axon of the blue sample at the first

90
00:03:28,800 --> 00:03:31,620
layer is top neuron and the exons of the

91
00:03:31,620 --> 00:03:34,800
second layer or the top two neurons

92
00:03:34,800 --> 00:03:37,440
in our following analysis we will also

93
00:03:37,440 --> 00:03:39,120
leverage the important property of the

94
00:03:39,120 --> 00:03:41,819
linear Network that is the backward

95
00:03:41,819 --> 00:03:43,920
signal only propagates along the

96
00:03:43,920 --> 00:03:45,780
activated Parts in the forward

97
00:03:45,780 --> 00:03:48,560
computation

98
00:03:49,440 --> 00:03:52,140
our produces the grade and matching

99
00:03:52,140 --> 00:03:54,480
problem to 10 cents the great equation

100
00:03:54,480 --> 00:03:56,819
where the left side is to calculate the

101
00:03:56,819 --> 00:03:58,500
gradient of the and the right side is

102
00:03:58,500 --> 00:04:00,780
the grungeous one for attacker who wants

103
00:04:00,780 --> 00:04:02,640
to reconstruct the training batch he or

104
00:04:02,640 --> 00:04:05,099
she needs to solve the equation and the

105
00:04:05,099 --> 00:04:06,720
optimization technique is just one

106
00:04:06,720 --> 00:04:10,140
approach due to the non-linear and the

107
00:04:10,140 --> 00:04:12,900
recursive nature of the F Theta so the

108
00:04:12,900 --> 00:04:15,120
neural network the gradient equation is

109
00:04:15,120 --> 00:04:16,858
a highly complex non-linear Matrix

110
00:04:16,858 --> 00:04:19,440
equation equation the abstractor form

111
00:04:19,440 --> 00:04:21,899
turns to bring us many opportunities for

112
00:04:21,899 --> 00:04:24,479
analytical studies therefore we prefer

113
00:04:24,479 --> 00:04:26,820
to First consider the unbiased fully

114
00:04:26,820 --> 00:04:28,800
connected neural network or called the

115
00:04:28,800 --> 00:04:31,380
fcn and expand the gradient equation as

116
00:04:31,380 --> 00:04:34,020
it's shown as we can see the gradient

117
00:04:34,020 --> 00:04:35,820
equation at the ice layer contains two

118
00:04:35,820 --> 00:04:38,100
nonlinear factors mainly so the first

119
00:04:38,100 --> 00:04:41,040
one is the loss Vector overhead GC which

120
00:04:41,040 --> 00:04:43,259
is a derivative of the loss which which

121
00:04:43,259 --> 00:04:45,540
respect to the mode outputs and the

122
00:04:45,540 --> 00:04:48,060
activation patterns so the D1 to the DH

123
00:04:48,060 --> 00:04:49,800
which indicates the neuron activation

124
00:04:49,800 --> 00:04:52,080
States when a sample is forwarded in the

125
00:04:52,080 --> 00:04:53,400
linear Network

126
00:04:53,400 --> 00:04:56,160
the general principle of our analysis is

127
00:04:56,160 --> 00:04:58,400
to leverage our proposed neuron

128
00:04:58,400 --> 00:05:01,380
exclusivity analysis to determine the

129
00:05:01,380 --> 00:05:03,600
non-linear factors directly from average

130
00:05:03,600 --> 00:05:05,639
gradient and reduce the complicated

131
00:05:05,639 --> 00:05:08,460
gradient equation into a simple sparse

132
00:05:08,460 --> 00:05:10,620
linear equation system which can be

133
00:05:10,620 --> 00:05:12,240
efficiently solved with the

134
00:05:12,240 --> 00:05:14,400
off-the-shift office

135
00:05:14,400 --> 00:05:16,520
foreign

136
00:05:16,520 --> 00:05:19,500
some interesting details we report our

137
00:05:19,500 --> 00:05:21,419
main results on security boundary of

138
00:05:21,419 --> 00:05:23,280
data reconstruction on training batch

139
00:05:23,280 --> 00:05:25,500
which is characterized by the number of

140
00:05:25,500 --> 00:05:28,560
exons and the attack side represents the

141
00:05:28,560 --> 00:05:30,780
sufficient exclusivity condition where

142
00:05:30,780 --> 00:05:33,000
each sample in the training batch has at

143
00:05:33,000 --> 00:05:35,460
least two exons at the last value layer

144
00:05:35,460 --> 00:05:37,680
and has at least one axon at the other

145
00:05:37,680 --> 00:05:40,740
real layers this condition allows us to

146
00:05:40,740 --> 00:05:42,600
design an analytical reconstruction

147
00:05:42,600 --> 00:05:44,840
algorithm which substantially

148
00:05:44,840 --> 00:05:46,919
outperforms the optimization based

149
00:05:46,919 --> 00:05:48,900
approach and for the first time achieves

150
00:05:48,900 --> 00:05:51,600
pixel wise reconstruction and the

151
00:05:51,600 --> 00:05:53,639
defense side will prevent the lack of

152
00:05:53,639 --> 00:05:55,979
exclusive condition where the training

153
00:05:55,979 --> 00:05:59,820
batch has no Excel and the first layer

154
00:05:59,820 --> 00:06:02,100
under this condition we prove there

155
00:06:02,100 --> 00:06:04,440
exists an infinite number of artifacts

156
00:06:04,440 --> 00:06:07,620
branches which satisfy the same graded

157
00:06:07,620 --> 00:06:09,900
equation this implies the impossibility

158
00:06:09,900 --> 00:06:12,240
of unique data reconstruction

159
00:06:12,240 --> 00:06:14,699
in the next slide we first present our

160
00:06:14,699 --> 00:06:16,440
reconstruction algorithm on the

161
00:06:16,440 --> 00:06:20,060
sufficient exclusivity condition

162
00:06:20,639 --> 00:06:22,620
the first step is to reconstruct the

163
00:06:22,620 --> 00:06:24,660
loss vectors we pay attention to the

164
00:06:24,660 --> 00:06:27,060
gradient equation at the loss rule layer

165
00:06:27,060 --> 00:06:29,280
which writes as in the equation 8 here

166
00:06:29,280 --> 00:06:31,620
we prove that when there are at least

167
00:06:31,620 --> 00:06:34,259
two exons at the illustrial layer then

168
00:06:34,259 --> 00:06:36,900
there are always two two more repetitive

169
00:06:36,900 --> 00:06:39,840
values in a ratio Vector so that ghj

170
00:06:39,840 --> 00:06:43,139
over the ghk which is equal to the ratio

171
00:06:43,139 --> 00:06:45,060
between the corresponding elements in

172
00:06:45,060 --> 00:06:47,940
the loss Vector of one sample this key

173
00:06:47,940 --> 00:06:50,280
observation allows us to determine axons

174
00:06:50,280 --> 00:06:52,380
in illustrial layer by checking the

175
00:06:52,380 --> 00:06:54,479
repetitive values in the ratio vector

176
00:06:54,479 --> 00:06:56,699
and thus derive the pairwise ratios

177
00:06:56,699 --> 00:06:59,039
amount elements in loss vector

178
00:06:59,039 --> 00:07:01,560
and with the ratios we can determine the

179
00:07:01,560 --> 00:07:03,479
labels according to the signs because

180
00:07:03,479 --> 00:07:05,340
according to the equation between the

181
00:07:05,340 --> 00:07:07,860
loss vector and the class probability

182
00:07:07,860 --> 00:07:11,520
the ratio is negative if and only if the

183
00:07:11,520 --> 00:07:13,500
class index is seen in the denominator

184
00:07:13,500 --> 00:07:16,440
or the numerator besides based on the

185
00:07:16,440 --> 00:07:18,720
one sum constraint the estimated values

186
00:07:18,720 --> 00:07:20,699
of loss vectors can be bound in small

187
00:07:20,699 --> 00:07:24,300
margin around the ground shift values

188
00:07:24,300 --> 00:07:27,120
next based on the detected axons signal

189
00:07:27,120 --> 00:07:29,940
layer before the exploit the observation

190
00:07:29,940 --> 00:07:32,880
that if the M sample has one axon at the

191
00:07:32,880 --> 00:07:35,520
ice layer then the exons at present

192
00:07:35,520 --> 00:07:37,319
layer can be determined by the

193
00:07:37,319 --> 00:07:39,300
non-vanishing gradients of the

194
00:07:39,300 --> 00:07:41,520
connecting weights of the axon this

195
00:07:41,520 --> 00:07:43,440
observation comes from the introduced

196
00:07:43,440 --> 00:07:46,199
property of the relinion network that is

197
00:07:46,199 --> 00:07:48,660
the gradient vanishes on the parts which

198
00:07:48,660 --> 00:07:50,639
are not activated during the forwarding

199
00:07:50,639 --> 00:07:53,340
phase and the determined axon at the

200
00:07:53,340 --> 00:07:55,319
successive layer separates out the

201
00:07:55,319 --> 00:07:57,900
contribution of each single sample

202
00:07:57,900 --> 00:08:02,160
by your question as a I minus 1 layer

203
00:08:02,160 --> 00:08:04,139
has at least one axon on the sufficient

204
00:08:04,139 --> 00:08:06,900
exclusivity condition then we can safely

205
00:08:06,900 --> 00:08:09,180
determine the axons in the I minus 2's

206
00:08:09,180 --> 00:08:12,599
there and so on until the first layer at

207
00:08:12,599 --> 00:08:14,819
the end of the algorithms we fully

208
00:08:14,819 --> 00:08:16,380
determine the loss vectors and the

209
00:08:16,380 --> 00:08:18,720
activation patterns of the samples as in

210
00:08:18,720 --> 00:08:20,940
the later bottom figure shows therefore

211
00:08:20,940 --> 00:08:22,860
the gradient equation becomes fully

212
00:08:22,860 --> 00:08:24,539
linear and solvable

213
00:08:24,539 --> 00:08:26,940
and how extended analysis to incorporate

214
00:08:26,940 --> 00:08:29,039
the bias terms please refer to our paper

215
00:08:29,039 --> 00:08:31,860
because it is no more than some tedious

216
00:08:31,860 --> 00:08:34,260
calculation

217
00:08:34,260 --> 00:08:36,179
we also extend the analytical approach

218
00:08:36,179 --> 00:08:37,979
to the case of deep convolutional neural

219
00:08:37,979 --> 00:08:40,080
networks based on the modularization of

220
00:08:40,080 --> 00:08:41,700
the future extraction part in the

221
00:08:41,700 --> 00:08:44,099
classifier head which is usually an fcn

222
00:08:44,099 --> 00:08:46,980
we propose to firstly leverage our above

223
00:08:46,980 --> 00:08:49,260
approach to accurately reconstruct the

224
00:08:49,260 --> 00:08:51,240
certain features into the classifier

225
00:08:51,240 --> 00:08:53,160
head and then construct a feature

226
00:08:53,160 --> 00:08:55,080
matching problem for each reconstructive

227
00:08:55,080 --> 00:08:57,420
future Vector this is a much simpler

228
00:08:57,420 --> 00:09:00,120
optimization problem because compared to

229
00:09:00,120 --> 00:09:02,220
a greater matching a feature matching

230
00:09:02,220 --> 00:09:04,080
only requires the first order gradient

231
00:09:04,080 --> 00:09:06,660
during the optimization and different

232
00:09:06,660 --> 00:09:08,700
from the average gradient used to

233
00:09:08,700 --> 00:09:11,279
integrated matching process the feature

234
00:09:11,279 --> 00:09:13,140
matching problem has already eliminated

235
00:09:13,140 --> 00:09:16,800
the fuscation effect of other samples in

236
00:09:16,800 --> 00:09:18,620
the training batch

237
00:09:18,620 --> 00:09:21,660
and the defense side we prove that the

238
00:09:21,660 --> 00:09:23,399
gradient equation has an infinite number

239
00:09:23,399 --> 00:09:25,980
of artifactor solutions when each sample

240
00:09:25,980 --> 00:09:27,899
in your batch has no axons and the first

241
00:09:27,899 --> 00:09:30,480
to the layer as shown in the left bottom

242
00:09:30,480 --> 00:09:32,700
figure the artifactor batch can be

243
00:09:32,700 --> 00:09:34,380
distant from the original training batch

244
00:09:34,380 --> 00:09:37,380
preventing an attack algorithm which not

245
00:09:37,380 --> 00:09:39,060
only relies on the gradient information

246
00:09:39,060 --> 00:09:41,580
to reconstruct the ground shoes training

247
00:09:41,580 --> 00:09:43,140
batch

248
00:09:43,140 --> 00:09:45,300
the observation also inspires us to

249
00:09:45,300 --> 00:09:48,240
derive a simple defense strategy that is

250
00:09:48,240 --> 00:09:50,399
by removing the review activation in the

251
00:09:50,399 --> 00:09:52,620
first layer which direction makes all

252
00:09:52,620 --> 00:09:54,300
the samples share the same activation

253
00:09:54,300 --> 00:09:57,660
pattern and thus none of exons exist we

254
00:09:57,660 --> 00:10:00,180
saw most no performance overheads our

255
00:10:00,180 --> 00:10:02,760
architecture oriented strategy can be

256
00:10:02,760 --> 00:10:05,519
further comboed with dpsgd or other

257
00:10:05,519 --> 00:10:07,440
fuscation techniques and gradient

258
00:10:07,440 --> 00:10:10,019
information to mitigate the Privacy

259
00:10:10,019 --> 00:10:13,100
threads of data reconstruction

260
00:10:13,100 --> 00:10:15,480
finally we present more evaluation

261
00:10:15,480 --> 00:10:17,640
without the top left of figure compels

262
00:10:17,640 --> 00:10:19,140
the construction quality of the hour

263
00:10:19,140 --> 00:10:21,060
attack and the Baseline attacks on vgt

264
00:10:21,060 --> 00:10:24,360
13 as is shown the reconstructed skin

265
00:10:24,360 --> 00:10:26,640
cancer images are most indistinguishable

266
00:10:26,640 --> 00:10:28,500
from the ground shoes once

267
00:10:28,500 --> 00:10:30,480
the top right figure shows the

268
00:10:30,480 --> 00:10:32,580
performance of our attack is insensitive

269
00:10:32,580 --> 00:10:34,680
to other impact factors such as the best

270
00:10:34,680 --> 00:10:36,600
size of the training Epoch the layer

271
00:10:36,600 --> 00:10:38,399
width and the depth only if the

272
00:10:38,399 --> 00:10:40,320
sufficient exclusivity condition is

273
00:10:40,320 --> 00:10:42,899
satisfied at the bottom we also explore

274
00:10:42,899 --> 00:10:45,120
how the factors influence the validity

275
00:10:45,120 --> 00:10:47,040
of the sufficient exclusivity condition

276
00:10:47,040 --> 00:10:49,560
and compare the performance of existing

277
00:10:49,560 --> 00:10:51,300
attacks with or without insecure

278
00:10:51,300 --> 00:10:53,399
condition

279
00:10:53,399 --> 00:10:56,459
also with Device an algorithm which adds

280
00:10:56,459 --> 00:10:58,260
noises to the original batch of a

281
00:10:58,260 --> 00:11:00,480
practical size to allow you to satisfy

282
00:11:00,480 --> 00:11:03,060
the sufficient exclusivity condition as

283
00:11:03,060 --> 00:11:05,160
is shown our attacker is still able to

284
00:11:05,160 --> 00:11:07,140
accurately reconstruct the perturbed

285
00:11:07,140 --> 00:11:10,440
training batch containing 128 celebrity

286
00:11:10,440 --> 00:11:13,380
faces separating almost all of them from

287
00:11:13,380 --> 00:11:16,079
average information

288
00:11:16,079 --> 00:11:18,660
here's a takeaway note our study starts

289
00:11:18,660 --> 00:11:20,480
from the empirical phenomenon of

290
00:11:20,480 --> 00:11:22,260
optimization-based data reconstruction

291
00:11:22,260 --> 00:11:24,660
and for the first time finds a common

292
00:11:24,660 --> 00:11:27,420
cause that is Neuron exclusivity that

293
00:11:27,420 --> 00:11:29,459
enables data Reconstruction from average

294
00:11:29,459 --> 00:11:32,820
gradient and attack side we construct an

295
00:11:32,820 --> 00:11:34,920
effective reconstruction algorithm under

296
00:11:34,920 --> 00:11:37,680
the sufficient exclusivity condition and

297
00:11:37,680 --> 00:11:39,720
at the defense side we prove the

298
00:11:39,720 --> 00:11:41,519
existence of an infinite number of

299
00:11:41,519 --> 00:11:44,279
artifactor branches for the same grading

300
00:11:44,279 --> 00:11:46,680
equation under the lack of exclusivity

301
00:11:46,680 --> 00:11:49,740
condition we tried but there remained a

302
00:11:49,740 --> 00:11:51,540
space between the secure and insecure

303
00:11:51,540 --> 00:11:52,920
conditions

304
00:11:52,920 --> 00:11:55,200
where the analytical property of data

305
00:11:55,200 --> 00:11:57,420
reconstruction is still unknown we hope

306
00:11:57,420 --> 00:11:59,820
Future Works can get Inspirations from

307
00:11:59,820 --> 00:12:02,880
study and our new exclusivity analysis

308
00:12:02,880 --> 00:12:05,279
techniques and try to build the full

309
00:12:05,279 --> 00:12:07,260
landscape of the security boundary of

310
00:12:07,260 --> 00:12:09,540
data reconstruction via the lens of

311
00:12:09,540 --> 00:12:11,459
axons

312
00:12:11,459 --> 00:12:13,560
following details feel free to read our

313
00:12:13,560 --> 00:12:17,540
paper and fold our research thank you

