1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:09,559 --> 00:00:12,900
in this presentation I will present our

3
00:00:12,900 --> 00:00:15,000
paper they will influence attacks

4
00:00:15,000 --> 00:00:17,760
against the vertical favorite learning

5
00:00:17,760 --> 00:00:20,820
we have entered the big data error where

6
00:00:20,820 --> 00:00:23,039
our private information may be gathered

7
00:00:23,039 --> 00:00:25,080
and analyzed by companies and

8
00:00:25,080 --> 00:00:27,420
organizations between machine learning

9
00:00:27,420 --> 00:00:30,900
models however it would be a severe

10
00:00:30,900 --> 00:00:33,960
problem if the gathered data gets leaked

11
00:00:33,960 --> 00:00:36,960
in recent years there have been many

12
00:00:36,960 --> 00:00:40,140
cases of that big cages the leaked data

13
00:00:40,140 --> 00:00:42,239
includes House Records social

14
00:00:42,239 --> 00:00:45,239
information and property information

15
00:00:45,239 --> 00:00:48,300
to protect user privacy more and more

16
00:00:48,300 --> 00:00:51,180
countries or States publish laws and

17
00:00:51,180 --> 00:00:53,700
regulations to restrict The Collection

18
00:00:53,700 --> 00:00:56,699
use of user data companies are not

19
00:00:56,699 --> 00:00:59,460
allowed to share the collected user data

20
00:00:59,460 --> 00:01:01,879
with other companies for commercial use

21
00:01:01,879 --> 00:01:05,400
which causes the Dilemma of isolated

22
00:01:05,400 --> 00:01:08,939
data for example two hospitals have

23
00:01:08,939 --> 00:01:11,520
house records of two groups of patients

24
00:01:11,520 --> 00:01:14,100
if they combine these health records

25
00:01:14,100 --> 00:01:16,740
into a larger data sites they can train

26
00:01:16,740 --> 00:01:18,840
a better machine learning model for

27
00:01:18,840 --> 00:01:22,140
disease diagnosis however they are

28
00:01:22,140 --> 00:01:24,979
forbidden to exchange users private data

29
00:01:24,979 --> 00:01:28,500
thus user data is isolated in different

30
00:01:28,500 --> 00:01:30,060
companies

31
00:01:30,060 --> 00:01:32,580
Friday easy learning is proposed to

32
00:01:32,580 --> 00:01:36,299
solve this dilemma which allows multiple

33
00:01:36,299 --> 00:01:39,360
participants to collaboratively train a

34
00:01:39,360 --> 00:01:41,640
machine learning model without revealing

35
00:01:41,640 --> 00:01:44,820
their local data Factory learning can be

36
00:01:44,820 --> 00:01:47,159
categorized into the horizontal fat

37
00:01:47,159 --> 00:01:51,659
resale learning also known as hfl and

38
00:01:51,659 --> 00:01:54,299
the vertical fat written learning also

39
00:01:54,299 --> 00:01:59,579
known as vfl for the hfl data sets share

40
00:01:59,579 --> 00:02:02,700
the same feature space but differ in the

41
00:02:02,700 --> 00:02:04,140
sample space

42
00:02:04,140 --> 00:02:07,680
on the contrary with FL is for the

43
00:02:07,680 --> 00:02:10,080
scenario where data sets share the same

44
00:02:10,080 --> 00:02:12,840
sample space but differ in the feature

45
00:02:12,840 --> 00:02:15,080
space

46
00:02:16,020 --> 00:02:18,660
Factory learning is being widely used

47
00:02:18,660 --> 00:02:21,420
and the many ID companies have developed

48
00:02:21,420 --> 00:02:24,300
their fat retail learning systems

49
00:02:24,300 --> 00:02:25,800
nevertheless

50
00:02:25,800 --> 00:02:29,220
Factory learning has vulnerabilities

51
00:02:29,220 --> 00:02:32,040
many existing Works have explored the

52
00:02:32,040 --> 00:02:35,819
security and privacy rates of hfl but

53
00:02:35,819 --> 00:02:37,500
the Prime series

54
00:02:37,500 --> 00:02:42,060
a vfl remain unexplored in this paper we

55
00:02:42,060 --> 00:02:44,940
demonstrate that a curious participant

56
00:02:44,940 --> 00:02:49,080
in vfl can infer the private labels

57
00:02:49,080 --> 00:02:51,840
and we propose three kinds of Labor

58
00:02:51,840 --> 00:02:54,800
inference attacks

59
00:02:55,379 --> 00:02:58,319
this is the illustration of Labor

60
00:02:58,319 --> 00:03:01,500
inference attacks against the vfl let's

61
00:03:01,500 --> 00:03:04,500
consider the vfl framework with split

62
00:03:04,500 --> 00:03:05,580
learning

63
00:03:05,580 --> 00:03:09,300
is this kind of vfl framework the

64
00:03:09,300 --> 00:03:11,879
original neural network model is split

65
00:03:11,879 --> 00:03:14,340
into one top model which runs on the

66
00:03:14,340 --> 00:03:16,920
server and several bottom models that

67
00:03:16,920 --> 00:03:18,780
run at participants

68
00:03:18,780 --> 00:03:21,300
every participant holds a part of

69
00:03:21,300 --> 00:03:23,220
features and encodes their local

70
00:03:23,220 --> 00:03:26,519
features by their bottom models one

71
00:03:26,519 --> 00:03:28,920
participant is the leader and controls

72
00:03:28,920 --> 00:03:31,980
the server for example participant B in

73
00:03:31,980 --> 00:03:32,940
this figure

74
00:03:32,940 --> 00:03:35,940
the labels are privately owned by these

75
00:03:35,940 --> 00:03:37,260
participants

76
00:03:37,260 --> 00:03:40,080
one of the participants without labels

77
00:03:40,080 --> 00:03:43,560
is the adversary whose goal is to infer

78
00:03:43,560 --> 00:03:46,019
the privately owned labels

79
00:03:46,019 --> 00:03:48,900
in the following we introduce three cans

80
00:03:48,900 --> 00:03:50,940
of Labor inference attacks against the

81
00:03:50,940 --> 00:03:53,940
vfl the first one is the passive level

82
00:03:53,940 --> 00:03:56,640
inference attack our intuition is that

83
00:03:56,640 --> 00:04:00,060
with the vfl finishes training the

84
00:04:00,060 --> 00:04:03,540
adversary can exploit its local tree in

85
00:04:03,540 --> 00:04:06,239
the bottom model to infer labels the

86
00:04:06,239 --> 00:04:08,400
trained bottom model cannot classify

87
00:04:08,400 --> 00:04:10,500
samples as it does not have a

88
00:04:10,500 --> 00:04:13,620
classification layer thus the adversary

89
00:04:13,620 --> 00:04:16,320
first adds a classification layer to the

90
00:04:16,320 --> 00:04:17,940
bottom model

91
00:04:17,940 --> 00:04:20,699
then assuming that the adversary has a

92
00:04:20,699 --> 00:04:23,100
small set of labeled samples these

93
00:04:23,100 --> 00:04:24,720
adversary can find three in the

94
00:04:24,720 --> 00:04:27,199
additional classification layer using

95
00:04:27,199 --> 00:04:30,600
semi-superized learning techniques once

96
00:04:30,600 --> 00:04:33,180
the completed look model is fine trained

97
00:04:33,180 --> 00:04:36,540
the adversary can use it to infer labels

98
00:04:36,540 --> 00:04:39,479
of other unabled local data

99
00:04:39,479 --> 00:04:41,820
the second attack tab is the active

100
00:04:41,820 --> 00:04:44,220
label inference attack the intuition is

101
00:04:44,220 --> 00:04:46,680
that the address 3 can accelerate the

102
00:04:46,680 --> 00:04:49,440
local model's learning rate during the

103
00:04:49,440 --> 00:04:52,280
training stage

104
00:04:52,680 --> 00:04:54,900
with the purpose of increasing the

105
00:04:54,900 --> 00:04:58,199
expressiveness of the bottom model then

106
00:04:58,199 --> 00:05:00,479
the adversary performs the passive level

107
00:05:00,479 --> 00:05:03,240
inference attack using the battery Trend

108
00:05:03,240 --> 00:05:04,500
photo model

109
00:05:04,500 --> 00:05:07,380
the local acceleration is designed with

110
00:05:07,380 --> 00:05:09,080
momentum

111
00:05:09,080 --> 00:05:11,400
adaptively adjust the learning rate

112
00:05:11,400 --> 00:05:14,100
scaling Factor as shown in the algorithm

113
00:05:14,100 --> 00:05:17,220
the scaled greeting is for the clip to

114
00:05:17,220 --> 00:05:19,979
ensure that the gradient value cannot be

115
00:05:19,979 --> 00:05:21,540
too large

116
00:05:21,540 --> 00:05:24,360
the third attack tab is the direct label

117
00:05:24,360 --> 00:05:26,460
inference attack this attack is

118
00:05:26,460 --> 00:05:28,860
specially designed for the vfl without

119
00:05:28,860 --> 00:05:31,800
split learning in this framework during

120
00:05:31,800 --> 00:05:34,740
the forward propagation the server sums

121
00:05:34,740 --> 00:05:37,320
up the outputs of all the bottom models

122
00:05:37,320 --> 00:05:40,259
to get the fan output instead of running

123
00:05:40,259 --> 00:05:41,759
a Top Model

124
00:05:41,759 --> 00:05:45,120
like the vfl with split learning every

125
00:05:45,120 --> 00:05:47,580
participant holds partial features and

126
00:05:47,580 --> 00:05:50,160
one leader participant owns labels and

127
00:05:50,160 --> 00:05:52,259
controls the server

128
00:05:52,259 --> 00:05:54,720
one of the participants without the

129
00:05:54,720 --> 00:05:57,660
labels is the adversary whose goal is to

130
00:05:57,660 --> 00:06:00,479
infer the property owned labels

131
00:06:00,479 --> 00:06:03,120
for the vfl framework these are the

132
00:06:03,120 --> 00:06:06,060
models plating the adversary can receive

133
00:06:06,060 --> 00:06:08,100
the gradient of the final prediction

134
00:06:08,100 --> 00:06:11,639
layer which enables the adversary to

135
00:06:11,639 --> 00:06:14,580
compute the labels of training samples

136
00:06:14,580 --> 00:06:18,060
directly specifically during the

137
00:06:18,060 --> 00:06:21,300
backward propagation the signs of the

138
00:06:21,300 --> 00:06:23,940
gradients received from server indicates

139
00:06:23,940 --> 00:06:27,360
the labels of the training samples

140
00:06:27,360 --> 00:06:31,740
more details can be found in our paper

141
00:06:31,740 --> 00:06:34,500
we choose six digicides to evaluate our

142
00:06:34,500 --> 00:06:37,020
proposed label inference attacks we can

143
00:06:37,020 --> 00:06:39,479
see the various data types including

144
00:06:39,479 --> 00:06:42,900
images text numerical features and the

145
00:06:42,900 --> 00:06:44,520
categorical features

146
00:06:44,520 --> 00:06:47,520
the attacks are evaluated on various

147
00:06:47,520 --> 00:06:50,340
model architectures such as last night

148
00:06:50,340 --> 00:06:53,100
birds and fully connected neural

149
00:06:53,100 --> 00:06:54,240
networks

150
00:06:54,240 --> 00:06:57,180
the vfl using these models gets good

151
00:06:57,180 --> 00:06:59,520
performance on the original text

152
00:06:59,520 --> 00:07:02,100
the experimental results show that the

153
00:07:02,100 --> 00:07:03,960
adversary can get good inference

154
00:07:03,960 --> 00:07:06,180
performance by conducting the passive

155
00:07:06,180 --> 00:07:09,240
level inference attack and the inference

156
00:07:09,240 --> 00:07:11,759
performance can be further improved with

157
00:07:11,759 --> 00:07:14,699
the active level inverse attack as for

158
00:07:14,699 --> 00:07:17,160
the direct level inference attack it

159
00:07:17,160 --> 00:07:20,220
always gets 100 percent influence

160
00:07:20,220 --> 00:07:23,039
accuracy because it is based on pure

161
00:07:23,039 --> 00:07:25,080
mathematical derivation

162
00:07:25,080 --> 00:07:27,539
we also conduct some sensitivity

163
00:07:27,539 --> 00:07:30,300
valuations our passive level inference

164
00:07:30,300 --> 00:07:33,180
attack requires the adversary to have a

165
00:07:33,180 --> 00:07:35,699
set of auxiliary labeled data so you

166
00:07:35,699 --> 00:07:38,160
want to know how the amount of auxiliary

167
00:07:38,160 --> 00:07:41,400
labeled data influence that inference

168
00:07:41,400 --> 00:07:44,460
performance this extra experiments

169
00:07:44,460 --> 00:07:46,259
results

170
00:07:46,259 --> 00:07:49,319
show that more auxiliary labeled data

171
00:07:49,319 --> 00:07:52,099
indeed increases influence accuracy

172
00:07:52,099 --> 00:07:56,520
however it the amount of auxiliary level

173
00:07:56,520 --> 00:07:59,220
data grows the inference performance

174
00:07:59,220 --> 00:08:02,819
increases more and more slowly thus the

175
00:08:02,819 --> 00:08:05,400
adversary tends to choose a small set of

176
00:08:05,400 --> 00:08:09,000
auxiliary data another Insight is that

177
00:08:09,000 --> 00:08:11,340
the trained bottom model contains much

178
00:08:11,340 --> 00:08:14,460
information for labor inference in this

179
00:08:14,460 --> 00:08:17,699
table direct semi means directly

180
00:08:17,699 --> 00:08:19,759
training a bottom model from scratch

181
00:08:19,759 --> 00:08:24,180
using the auxiliary data rather than

182
00:08:24,180 --> 00:08:27,599
the bottom model we can see that the

183
00:08:27,599 --> 00:08:28,560
average

184
00:08:28,560 --> 00:08:31,319
performance becomes much worse without

185
00:08:31,319 --> 00:08:34,799
the trend photo model from vfl

186
00:08:34,799 --> 00:08:36,839
there are more findings in our paper

187
00:08:36,839 --> 00:08:40,380
please see the paper for more details

188
00:08:40,380 --> 00:08:43,500
in this section we analyze why the

189
00:08:43,500 --> 00:08:46,440
active label inference attack works with

190
00:08:46,440 --> 00:08:49,080
the grid cam visualization we can find

191
00:08:49,080 --> 00:08:51,779
that the active attack successfully

192
00:08:51,779 --> 00:08:54,899
makes the vfl framework to more

193
00:08:54,899 --> 00:08:58,019
attention to the adversaries data and

194
00:08:58,019 --> 00:09:01,380
the TSA projection of the outputs of the

195
00:09:01,380 --> 00:09:04,560
adversaries photo model shows that under

196
00:09:04,560 --> 00:09:07,380
the active attack the others raise Auto

197
00:09:07,380 --> 00:09:09,500
model learns better intermediary

198
00:09:09,500 --> 00:09:12,420
representations which benefits level

199
00:09:12,420 --> 00:09:14,580
inference attacks

200
00:09:14,580 --> 00:09:17,700
in this section we discuss and evaluate

201
00:09:17,700 --> 00:09:20,160
some possible differences during the

202
00:09:20,160 --> 00:09:22,440
training of vfl the only information

203
00:09:22,440 --> 00:09:24,959
sent to the adversary is the greetings

204
00:09:24,959 --> 00:09:27,420
from the server so

205
00:09:27,420 --> 00:09:30,060
defense strategies can be applied to the

206
00:09:30,060 --> 00:09:32,640
gradients to prevent information leakage

207
00:09:32,640 --> 00:09:35,519
from the server to the other three such

208
00:09:35,519 --> 00:09:39,540
as pruning small scale readings

209
00:09:39,540 --> 00:09:42,300
adding noise on the gradient

210
00:09:42,300 --> 00:09:44,779
discretization

211
00:09:44,779 --> 00:09:47,519
unfortunately these differences are

212
00:09:47,519 --> 00:09:49,860
proven to be not so effective against

213
00:09:49,860 --> 00:09:53,100
the proposed level inference attacks in

214
00:09:53,100 --> 00:09:55,680
particular these differences have the

215
00:09:55,680 --> 00:09:58,080
trade-off between the performance of the

216
00:09:58,080 --> 00:10:00,120
original task and the defense

217
00:10:00,120 --> 00:10:02,279
performance against the level inference

218
00:10:02,279 --> 00:10:03,540
attacks

219
00:10:03,540 --> 00:10:06,060
as for the direct level inference attack

220
00:10:06,060 --> 00:10:07,500
we find

221
00:10:07,500 --> 00:10:10,080
the noisy greetings and the Privacy

222
00:10:10,080 --> 00:10:12,240
preserving deep learning can

223
00:10:12,240 --> 00:10:15,600
successfully mitigate the labor leakage

224
00:10:15,600 --> 00:10:19,320
and maintain a good performance on the

225
00:10:19,320 --> 00:10:22,160
original task

226
00:10:23,279 --> 00:10:26,100
to sum up we review a new level leakage

227
00:10:26,100 --> 00:10:29,160
issue of vfl and presents three label

228
00:10:29,160 --> 00:10:31,500
inference attacks that can achieve good

229
00:10:31,500 --> 00:10:33,660
attack performance we also share

230
00:10:33,660 --> 00:10:36,240
insights about these attacks which

231
00:10:36,240 --> 00:10:39,360
realize the proofs finally existing

232
00:10:39,360 --> 00:10:42,260
defenses are not effective against the

233
00:10:42,260 --> 00:10:45,000
passive and active label inference

234
00:10:45,000 --> 00:10:48,060
attacks which motivates future work on

235
00:10:48,060 --> 00:10:50,760
better defenses for vfl

236
00:10:50,760 --> 00:10:53,880
if you have any further problems don't

237
00:10:53,880 --> 00:10:56,339
hesitate to get in touch with me using

238
00:10:56,339 --> 00:10:59,540
this email address

