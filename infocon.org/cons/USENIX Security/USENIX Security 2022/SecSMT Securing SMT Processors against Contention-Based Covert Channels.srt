1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:10,080 --> 00:00:13,259
today I'm going to talk about our paper

3
00:00:13,259 --> 00:00:16,760
on how to secure smt processors against

4
00:00:16,760 --> 00:00:20,480
contention-based side channels

5
00:00:21,000 --> 00:00:23,400
but what is an smt processor

6
00:00:23,400 --> 00:00:25,320
and why do we need them

7
00:00:25,320 --> 00:00:28,939
so speak superscaler processors

8
00:00:28,939 --> 00:00:32,460
have multiple execution units to take

9
00:00:32,460 --> 00:00:34,079
advantage of instruction level

10
00:00:34,079 --> 00:00:36,540
parallelism and to be able to execute

11
00:00:36,540 --> 00:00:39,600
multiple instructions in the same cycle

12
00:00:39,600 --> 00:00:43,140
so for example in a Fourier superscalar

13
00:00:43,140 --> 00:00:45,660
processor each cycle we would ideally

14
00:00:45,660 --> 00:00:49,440
have four instructions issued to each of

15
00:00:49,440 --> 00:00:51,899
our execution units

16
00:00:51,899 --> 00:00:55,440
but what happens in reality is due to

17
00:00:55,440 --> 00:00:59,360
various reasons for example

18
00:00:59,360 --> 00:01:03,300
due to longer latency instructions such

19
00:01:03,300 --> 00:01:06,780
as cache misses or tlb misses or

20
00:01:06,780 --> 00:01:09,780
possibly page faults in many cycles you

21
00:01:09,780 --> 00:01:13,020
don't have any instruction to issue to

22
00:01:13,020 --> 00:01:15,479
your functional units

23
00:01:15,479 --> 00:01:18,119
but even when you don't have longer

24
00:01:18,119 --> 00:01:19,500
latencies

25
00:01:19,500 --> 00:01:21,780
you cannot always fully utilize your

26
00:01:21,780 --> 00:01:24,720
functional units uh because you don't

27
00:01:24,720 --> 00:01:27,119
have enough independent instructions in

28
00:01:27,119 --> 00:01:30,240
your programs basically instruction

29
00:01:30,240 --> 00:01:32,520
level parallelism in your software

30
00:01:32,520 --> 00:01:35,579
usually falls short of available Harvard

31
00:01:35,579 --> 00:01:38,880
level uh instruction level parallelism

32
00:01:38,880 --> 00:01:41,119
so we end up with these heavily

33
00:01:41,119 --> 00:01:44,939
unutilized execution units due to what

34
00:01:44,939 --> 00:01:49,380
we call vertical or horizontal wastes

35
00:01:49,380 --> 00:01:52,140
one solution proposed uh for this

36
00:01:52,140 --> 00:01:55,680
problem is to use instructions from

37
00:01:55,680 --> 00:01:58,680
multiple threads to fill our execution

38
00:01:58,680 --> 00:01:59,700
units

39
00:01:59,700 --> 00:02:02,340
and the most prominent multi-trading

40
00:02:02,340 --> 00:02:04,939
architecture is smt or simultaneous

41
00:02:04,939 --> 00:02:07,860
multi-trading in which the pipeline has

42
00:02:07,860 --> 00:02:10,380
the ability to execute multiple

43
00:02:10,380 --> 00:02:12,900
instructions from multiple thread in the

44
00:02:12,900 --> 00:02:15,319
same cycle

45
00:02:15,780 --> 00:02:18,720
smt was just a very small investment in

46
00:02:18,720 --> 00:02:20,640
Hardware can gain significant

47
00:02:20,640 --> 00:02:21,920
performance

48
00:02:21,920 --> 00:02:24,720
almost double in some cases

49
00:02:24,720 --> 00:02:26,879
and that's why it has been implemented

50
00:02:26,879 --> 00:02:29,040
by virtually all the major players in

51
00:02:29,040 --> 00:02:33,120
high performance processor industry

52
00:02:33,120 --> 00:02:35,580
but the problem is that now you have to

53
00:02:35,580 --> 00:02:37,319
share a lot of resources between the

54
00:02:37,319 --> 00:02:40,680
threads which create a security problem

55
00:02:40,680 --> 00:02:42,840
for example if you share your floating

56
00:02:42,840 --> 00:02:44,340
form functional units between the

57
00:02:44,340 --> 00:02:47,280
threads a malicious threat can a spy on

58
00:02:47,280 --> 00:02:50,459
a on a victim thread so our attacker can

59
00:02:50,459 --> 00:02:53,340
monitor the timing of his own floating

60
00:02:53,340 --> 00:02:55,800
Point instructions and if they run fast

61
00:02:55,800 --> 00:02:59,099
that means there is no contention from

62
00:02:59,099 --> 00:03:01,680
the victim but if they run a slow that

63
00:03:01,680 --> 00:03:03,540
means the victim has executed a floating

64
00:03:03,540 --> 00:03:06,239
Point instruction and basically this

65
00:03:06,239 --> 00:03:09,120
type of sharing allows an attacker

66
00:03:09,120 --> 00:03:11,040
threat to gain fine-grained information

67
00:03:11,040 --> 00:03:14,760
about a victim's threat

68
00:03:14,760 --> 00:03:16,739
the floating Point Financial unit was

69
00:03:16,739 --> 00:03:20,099
just one example with smt almost every

70
00:03:20,099 --> 00:03:21,720
part of the pipeline is straight between

71
00:03:21,720 --> 00:03:23,879
the threads which creates a huge problem

72
00:03:23,879 --> 00:03:25,860
for security

73
00:03:25,860 --> 00:03:27,260
and that

74
00:03:27,260 --> 00:03:30,120
has put smt at a high risk of being

75
00:03:30,120 --> 00:03:33,060
turned off in the name of security

76
00:03:33,060 --> 00:03:36,060
Google open BSD and red hat they have

77
00:03:36,060 --> 00:03:39,000
all introduced new ways to disable this

78
00:03:39,000 --> 00:03:43,640
feature in their future kernels

79
00:03:45,060 --> 00:03:47,760
the goal of This research is to design

80
00:03:47,760 --> 00:03:50,819
an smt processor that is

81
00:03:50,819 --> 00:03:54,000
as secure as a multi-core processor to

82
00:03:54,000 --> 00:03:56,700
contention-based side channels so that

83
00:03:56,700 --> 00:03:58,260
we can make it a reasonable security

84
00:03:58,260 --> 00:04:01,920
decision to turn on smt again

85
00:04:01,920 --> 00:04:04,019
naturally the first step is to

86
00:04:04,019 --> 00:04:05,459
understand

87
00:04:05,459 --> 00:04:08,220
to what extent modern smt processors are

88
00:04:08,220 --> 00:04:13,280
vulnerable to uh information leakage

89
00:04:13,620 --> 00:04:15,480
pipeline resources can have different

90
00:04:15,480 --> 00:04:17,399
sharing mechanisms between the threads

91
00:04:17,399 --> 00:04:19,798
for example the threads can dynamically

92
00:04:19,798 --> 00:04:22,919
share a resource like instruction cache

93
00:04:22,919 --> 00:04:25,979
or they can each use only half of a

94
00:04:25,979 --> 00:04:28,139
resource if the resource is statically

95
00:04:28,139 --> 00:04:29,880
partitioned

96
00:04:29,880 --> 00:04:32,880
our goal is to find out which of these

97
00:04:32,880 --> 00:04:36,660
resources are static partitioned which

98
00:04:36,660 --> 00:04:40,020
are dynamically shared and if a resource

99
00:04:40,020 --> 00:04:42,120
is dynamically shared to what extent

100
00:04:42,120 --> 00:04:45,060
that creates a security leakage

101
00:04:45,060 --> 00:04:47,580
so we designed multi-traded programs

102
00:04:47,580 --> 00:04:50,759
specific to each of these resources and

103
00:04:50,759 --> 00:04:54,300
these programs create contention on each

104
00:04:54,300 --> 00:04:56,340
resource and make sure that the effect

105
00:04:56,340 --> 00:04:58,800
of the contention is visible through the

106
00:04:58,800 --> 00:05:01,139
execution time of the program similar to

107
00:05:01,139 --> 00:05:03,120
what we saw in a floating Point

108
00:05:03,120 --> 00:05:06,320
functional units

109
00:05:06,479 --> 00:05:08,940
we went through a comprehensive list of

110
00:05:08,940 --> 00:05:12,840
pipeline resources and identified if the

111
00:05:12,840 --> 00:05:14,820
resource is dynamically shared or

112
00:05:14,820 --> 00:05:17,040
statically partitioned and what is the

113
00:05:17,040 --> 00:05:19,440
bandwidth of the covert channel that you

114
00:05:19,440 --> 00:05:21,540
can create on those resources

115
00:05:21,540 --> 00:05:25,620
we showed that AMD and Intel take very

116
00:05:25,620 --> 00:05:28,259
different approaches to security

117
00:05:28,259 --> 00:05:30,960
performance trade-off but both of them

118
00:05:30,960 --> 00:05:34,199
suffer from a high high bandwidth covert

119
00:05:34,199 --> 00:05:36,600
channels that need to be mitigated we

120
00:05:36,600 --> 00:05:38,400
also found and characterized multiple

121
00:05:38,400 --> 00:05:41,639
new cover channels including for example

122
00:05:41,639 --> 00:05:44,340
fetch bandwidth cover channel that has a

123
00:05:44,340 --> 00:05:46,680
higher bandwidth than a regular cache

124
00:05:46,680 --> 00:05:49,820
side channels in our setup

125
00:05:50,160 --> 00:05:54,060
okay so what can we do to mitigate this

126
00:05:54,060 --> 00:05:57,660
uh this information leakage

127
00:05:57,660 --> 00:05:59,820
well one approach is to mitigate these

128
00:05:59,820 --> 00:06:02,699
one by one one mitigation for caches

129
00:06:02,699 --> 00:06:05,280
another one for tlb another mitigation

130
00:06:05,280 --> 00:06:08,220
for uh branch predictors

131
00:06:08,220 --> 00:06:11,280
but this approach does not scale well so

132
00:06:11,280 --> 00:06:13,800
what we were looking for was a unified

133
00:06:13,800 --> 00:06:15,020
approach

134
00:06:15,020 --> 00:06:17,400
that you can apply to all of these

135
00:06:17,400 --> 00:06:19,620
resources maybe with some small

136
00:06:19,620 --> 00:06:22,160
variation

137
00:06:22,199 --> 00:06:24,120
the most obvious one is a static

138
00:06:24,120 --> 00:06:25,460
partitioning

139
00:06:25,460 --> 00:06:27,419
that completely blocks the information

140
00:06:27,419 --> 00:06:31,139
flow from both threads but of course

141
00:06:31,139 --> 00:06:33,720
with a huge performance overhead

142
00:06:33,720 --> 00:06:37,199
we propose adaptive partitioning

143
00:06:37,199 --> 00:06:39,240
where you can change the boundary

144
00:06:39,240 --> 00:06:41,160
between the threads every adaptation

145
00:06:41,160 --> 00:06:42,259
period

146
00:06:42,259 --> 00:06:44,580
this limits the communication between

147
00:06:44,580 --> 00:06:47,100
the threads to once every adaptation

148
00:06:47,100 --> 00:06:50,520
period but we show that even a very

149
00:06:50,520 --> 00:06:53,220
large interval can gain back significant

150
00:06:53,220 --> 00:06:54,800
performance

151
00:06:54,800 --> 00:06:57,000
the performance that we lost do

152
00:06:57,000 --> 00:06:59,780
partitioning

153
00:07:00,600 --> 00:07:04,199
we also proposed asymmetric smt a novel

154
00:07:04,199 --> 00:07:06,479
security approach that allows

155
00:07:06,479 --> 00:07:08,520
communication from a low security threat

156
00:07:08,520 --> 00:07:10,560
to a high security threat but not the

157
00:07:10,560 --> 00:07:12,840
other way around

158
00:07:12,840 --> 00:07:15,419
just one example where this is useful is

159
00:07:15,419 --> 00:07:17,460
sandboxing in web browsers

160
00:07:17,460 --> 00:07:19,380
while it is not secure to leak

161
00:07:19,380 --> 00:07:21,660
information from the browser thread to

162
00:07:21,660 --> 00:07:23,400
the sandbox thread

163
00:07:23,400 --> 00:07:25,860
it is safe to leak information from the

164
00:07:25,860 --> 00:07:28,979
sandbox thread to the browser thread so

165
00:07:28,979 --> 00:07:32,160
we utilize this by allowing the unused

166
00:07:32,160 --> 00:07:35,220
resources from an untrusted threat to be

167
00:07:35,220 --> 00:07:39,599
securely utilized by The Trusted thread

168
00:07:39,599 --> 00:07:43,020
the idea is to reserve resources to the

169
00:07:43,020 --> 00:07:44,520
sandbox threat

170
00:07:44,520 --> 00:07:46,740
and make it run in a constant and

171
00:07:46,740 --> 00:07:48,000
predictable way

172
00:07:48,000 --> 00:07:50,699
so for example here we look at the load

173
00:07:50,699 --> 00:07:51,599
queue

174
00:07:51,599 --> 00:07:53,699
and we reserve half of the load key

175
00:07:53,699 --> 00:07:56,400
entries to the sandbox threat so far

176
00:07:56,400 --> 00:07:58,380
exactly similar to the static

177
00:07:58,380 --> 00:08:00,180
partitioning

178
00:08:00,180 --> 00:08:02,819
but usually there are many cycles where

179
00:08:02,819 --> 00:08:06,120
the sandbox does not use all of its load

180
00:08:06,120 --> 00:08:07,560
queue entries

181
00:08:07,560 --> 00:08:09,599
now the privilege thread can actually

182
00:08:09,599 --> 00:08:12,900
borrow and resource uh borrow an entry

183
00:08:12,900 --> 00:08:16,139
from the sandbox partition however

184
00:08:16,139 --> 00:08:18,120
we should make sure that we can

185
00:08:18,120 --> 00:08:20,340
immediately return the borrow resource

186
00:08:20,340 --> 00:08:23,099
if the sandbox thread needs it

187
00:08:23,099 --> 00:08:25,440
so the borrowing needs to be completely

188
00:08:25,440 --> 00:08:30,900
transparent from a Sandbox perspective

189
00:08:31,259 --> 00:08:34,080
and if we do that we need to reissue the

190
00:08:34,080 --> 00:08:36,899
privilege instruction that that we just

191
00:08:36,899 --> 00:08:40,620
flash to ensure the correct execution of

192
00:08:40,620 --> 00:08:42,599
the privilege threat

193
00:08:42,599 --> 00:08:45,180
and interestingly the same mechanism can

194
00:08:45,180 --> 00:08:46,920
work for a wide range of pipeline

195
00:08:46,920 --> 00:08:50,580
Resources with just a small variation

196
00:08:50,580 --> 00:08:53,160
we categorize pipeline resources broadly

197
00:08:53,160 --> 00:08:56,779
into two categories stateful resources

198
00:08:56,779 --> 00:08:59,519
those are resources that hold the state

199
00:08:59,519 --> 00:09:03,000
across multiple Cycles uh resources like

200
00:09:03,000 --> 00:09:05,040
load queue instruction queue or physical

201
00:09:05,040 --> 00:09:09,120
registers are stateful resources

202
00:09:09,120 --> 00:09:11,300
and if you make a stateful resource

203
00:09:11,300 --> 00:09:13,800
asymmetric then you should make sure

204
00:09:13,800 --> 00:09:15,480
that you can return the borrowed

205
00:09:15,480 --> 00:09:17,459
resource immediately

206
00:09:17,459 --> 00:09:19,500
and and then therefore you need to flash

207
00:09:19,500 --> 00:09:21,480
instructions and be able to reissue

208
00:09:21,480 --> 00:09:23,519
instructions but luckily modern

209
00:09:23,519 --> 00:09:27,959
processors have already will uh the are

210
00:09:27,959 --> 00:09:30,839
already well equipped to deal with such

211
00:09:30,839 --> 00:09:34,519
scenarios for example they have a

212
00:09:34,519 --> 00:09:37,080
flashing and reissuing mechanisms for

213
00:09:37,080 --> 00:09:38,880
branch prediction and lots of other

214
00:09:38,880 --> 00:09:41,820
scenarios

215
00:09:41,820 --> 00:09:44,279
on the other hand if you don't use a

216
00:09:44,279 --> 00:09:48,060
stateless resource for psycho stateless

217
00:09:48,060 --> 00:09:50,580
resources such as execution units or

218
00:09:50,580 --> 00:09:54,060
fetch bandwidth then they will be wasted

219
00:09:54,060 --> 00:09:57,180
and the beauty of asymmetric smt is that

220
00:09:57,180 --> 00:09:59,760
if you apply to a stateless resources

221
00:09:59,760 --> 00:10:01,800
then you don't even need to return the

222
00:10:01,800 --> 00:10:04,260
borrowed resource so if a stateless

223
00:10:04,260 --> 00:10:06,720
resource is not used you just can can

224
00:10:06,720 --> 00:10:10,620
borrow it without any consequences

225
00:10:10,620 --> 00:10:13,920
caches are a special type of stateful

226
00:10:13,920 --> 00:10:17,100
resources and the problem with caches is

227
00:10:17,100 --> 00:10:19,500
that once you once your cache is filled

228
00:10:19,500 --> 00:10:21,899
then there would be no free entries

229
00:10:21,899 --> 00:10:25,320
anymore so you cannot borrow

230
00:10:25,320 --> 00:10:27,240
because you always have something in

231
00:10:27,240 --> 00:10:29,339
your cash

232
00:10:29,339 --> 00:10:30,720
so

233
00:10:30,720 --> 00:10:33,300
um what we can do here

234
00:10:33,300 --> 00:10:36,720
is to define a maximum number of cycles

235
00:10:36,720 --> 00:10:38,820
that a cash block can stay in the cache

236
00:10:38,820 --> 00:10:40,680
without being accessed

237
00:10:40,680 --> 00:10:42,660
and this is a very simple form of dead

238
00:10:42,660 --> 00:10:44,120
block elimination

239
00:10:44,120 --> 00:10:48,420
that I show here for Simplicity but we

240
00:10:48,420 --> 00:10:51,720
use a more complicated one in the paper

241
00:10:51,720 --> 00:10:54,779
so here we evict any block that has not

242
00:10:54,779 --> 00:10:57,180
been touched for a while

243
00:10:57,180 --> 00:11:00,420
and if we do this we will have three

244
00:11:00,420 --> 00:11:02,820
entries and we can enable borrowing

245
00:11:02,820 --> 00:11:05,120
again

246
00:11:05,339 --> 00:11:07,920
and returning a borrowed resource

247
00:11:07,920 --> 00:11:11,100
borrowed cash block is easy because it's

248
00:11:11,100 --> 00:11:12,779
a it's just a replacement and you don't

249
00:11:12,779 --> 00:11:17,300
require a instruction flush

250
00:11:19,260 --> 00:11:21,800
okay with SEC smt we addressed

251
00:11:21,800 --> 00:11:23,880
contention-based site channels across

252
00:11:23,880 --> 00:11:26,279
all the pipeline structures enabling

253
00:11:26,279 --> 00:11:28,680
continued use of these performance

254
00:11:28,680 --> 00:11:31,620
critical structures while executing

255
00:11:31,620 --> 00:11:32,760
securely

256
00:11:32,760 --> 00:11:34,920
we are able to reduce the performance

257
00:11:34,920 --> 00:11:38,339
overhead of partitioning close to the

258
00:11:38,339 --> 00:11:40,560
performance overhead uh of the

259
00:11:40,560 --> 00:11:43,320
performance of an insecure uh

260
00:11:43,320 --> 00:11:45,480
dynamically shared pipeline

261
00:11:45,480 --> 00:11:47,880
thanks for listening and I'm happy to

262
00:11:47,880 --> 00:11:50,480
answer your questions

