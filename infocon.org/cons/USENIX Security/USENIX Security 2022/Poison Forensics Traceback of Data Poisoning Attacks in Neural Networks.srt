1
00:00:08,240 --> 00:00:10,860
all right hello everyone this is Sean

2
00:00:10,860 --> 00:00:13,200
and to them here to tell you about our

3
00:00:13,200 --> 00:00:15,719
recent work on applying forensics to

4
00:00:15,719 --> 00:00:18,060
trace back data poisoning attack in deep

5
00:00:18,060 --> 00:00:20,100
neural networks this is going to work

6
00:00:20,100 --> 00:00:22,080
with my colleague Arjun my advisor is

7
00:00:22,080 --> 00:00:24,119
Heather and Ben

8
00:00:24,119 --> 00:00:26,039
to start this talk I mean showing you

9
00:00:26,039 --> 00:00:28,920
this this chart on the on the left are

10
00:00:28,920 --> 00:00:31,019
some defenses in ml Security on the

11
00:00:31,019 --> 00:00:32,940
right are the time it takes for these

12
00:00:32,940 --> 00:00:34,440
defenses to be broken by a later

13
00:00:34,440 --> 00:00:36,780
stronger attack well we all notice that

14
00:00:36,780 --> 00:00:38,460
defense in annual security at least

15
00:00:38,460 --> 00:00:40,079
doesn't last for a very long time

16
00:00:40,079 --> 00:00:41,879
they're quickly broken by strong

17
00:00:41,879 --> 00:00:43,920
adaptive adversaries

18
00:00:43,920 --> 00:00:45,840
what we do need to keep working on

19
00:00:45,840 --> 00:00:48,719
defense don't get me wrong but boy we're

20
00:00:48,719 --> 00:00:51,120
sure asked a lot out of this defense

21
00:00:51,120 --> 00:00:53,280
we ask them more or less to be perfect

22
00:00:53,280 --> 00:00:55,980
against everything in the future

23
00:00:55,980 --> 00:00:58,199
but in reality and in General Security

24
00:00:58,199 --> 00:00:59,399
as well

25
00:00:59,399 --> 00:01:01,260
defenses are not meant to be perfect

26
00:01:01,260 --> 00:01:02,460
right it would be great to have a

27
00:01:02,460 --> 00:01:04,559
perfect defense but defense are just

28
00:01:04,559 --> 00:01:07,380
meant to raise the bar of a tax raiser

29
00:01:07,380 --> 00:01:08,220
cost

30
00:01:08,220 --> 00:01:09,960
so at least we can stop the karma

31
00:01:09,960 --> 00:01:12,420
attacker from breaking our system

32
00:01:12,420 --> 00:01:14,280
now what about those extremely powerful

33
00:01:14,280 --> 00:01:16,020
hackers right that hackers that has tons

34
00:01:16,020 --> 00:01:17,520
of incentives and expertise and

35
00:01:17,520 --> 00:01:18,600
resources

36
00:01:18,600 --> 00:01:20,340
you know that they're gonna eventually

37
00:01:20,340 --> 00:01:22,500
win against your defense at some point

38
00:01:22,500 --> 00:01:25,799
with a pretty high probability

39
00:01:25,799 --> 00:01:27,840
so for this type of attackers it seems

40
00:01:27,840 --> 00:01:30,119
that we need something that go beyond

41
00:01:30,119 --> 00:01:32,820
just runtime defenses to prevent these

42
00:01:32,820 --> 00:01:35,220
type of attacks and fortunately we do

43
00:01:35,220 --> 00:01:37,320
have other tools in our box

44
00:01:37,320 --> 00:01:39,960
and digital forensics is one of such

45
00:01:39,960 --> 00:01:41,700
tools that can handle the case where

46
00:01:41,700 --> 00:01:44,280
events have fell and attacks are

47
00:01:44,280 --> 00:01:45,900
successful

48
00:01:45,900 --> 00:01:48,240
so what is digital forensics

49
00:01:48,240 --> 00:01:50,220
as we said digital forensics happen

50
00:01:50,220 --> 00:01:52,259
after attack has already break your

51
00:01:52,259 --> 00:01:54,899
defense and potentially caused damages

52
00:01:54,899 --> 00:01:57,180
to your to a victim system

53
00:01:57,180 --> 00:01:59,520
forensics come after attack post that I

54
00:01:59,520 --> 00:02:00,720
want to take a look at the attack

55
00:02:00,720 --> 00:02:02,880
retrospectively look at the attack

56
00:02:02,880 --> 00:02:05,579
incident and as well as any traces left

57
00:02:05,579 --> 00:02:06,899
by that hacker

58
00:02:06,899 --> 00:02:09,840
and the goal is to trace back as much as

59
00:02:09,840 --> 00:02:11,819
possible through the system right try to

60
00:02:11,819 --> 00:02:14,160
pinpoint who is the attacker that break

61
00:02:14,160 --> 00:02:16,260
your system what have resources being

62
00:02:16,260 --> 00:02:18,840
used what type of exploit is being used

63
00:02:18,840 --> 00:02:21,120
right some examples for example are some

64
00:02:21,120 --> 00:02:23,280
set of IP address use their bank account

65
00:02:23,280 --> 00:02:25,620
used to launch the attack and in some

66
00:02:25,620 --> 00:02:27,300
case we can even find the physical

67
00:02:27,300 --> 00:02:29,640
identity of the attacker

68
00:02:29,640 --> 00:02:31,860
once we have some information about this

69
00:02:31,860 --> 00:02:34,140
we can mitigate against attacks like

70
00:02:34,140 --> 00:02:36,300
this right we can say do things like

71
00:02:36,300 --> 00:02:38,459
blacklisting IP address close bank

72
00:02:38,459 --> 00:02:40,560
account and in the case where we have

73
00:02:40,560 --> 00:02:42,300
sufficient evidence we can even leverage

74
00:02:42,300 --> 00:02:44,700
legal forces to prosecute that hacker

75
00:02:44,700 --> 00:02:47,400
and stop attacks from this powerful

76
00:02:47,400 --> 00:02:49,260
attacker in the future

77
00:02:49,260 --> 00:02:51,480
more importantly perhaps

78
00:02:51,480 --> 00:02:54,120
forensics also serve as a deterrent but

79
00:02:54,120 --> 00:02:55,800
the presence of a powerful strong

80
00:02:55,800 --> 00:02:58,379
forensic system may be able to deter

81
00:02:58,379 --> 00:03:00,360
attack before it even happened right

82
00:03:00,360 --> 00:03:01,980
because there's a added risk for

83
00:03:01,980 --> 00:03:04,140
attacker to be held responsible for

84
00:03:04,140 --> 00:03:06,239
their action even if they're successful

85
00:03:06,239 --> 00:03:08,040
at punish trading system and cost of

86
00:03:08,040 --> 00:03:09,360
damage

87
00:03:09,360 --> 00:03:11,099
forensic has a lot more benefit than

88
00:03:11,099 --> 00:03:13,260
what I can list on this slide

89
00:03:13,260 --> 00:03:14,940
but let's take a look at how we can

90
00:03:14,940 --> 00:03:16,800
leverage some of the digital forensics

91
00:03:16,800 --> 00:03:18,780
concept for machine learning security

92
00:03:18,780 --> 00:03:21,180
problems that we're facing today

93
00:03:21,180 --> 00:03:23,580
and as the initial step we took a look

94
00:03:23,580 --> 00:03:26,400
at data poisoning attack

95
00:03:26,400 --> 00:03:28,140
a quick refresher on data poisoning

96
00:03:28,140 --> 00:03:29,340
attack

97
00:03:29,340 --> 00:03:31,739
you are training a classifier collecting

98
00:03:31,739 --> 00:03:33,959
a data set attackers somehow go into

99
00:03:33,959 --> 00:03:35,700
your training data collection pipeline

100
00:03:35,700 --> 00:03:38,220
inject some poison data now your model

101
00:03:38,220 --> 00:03:40,140
has a vulnerability because of these

102
00:03:40,140 --> 00:03:41,280
positive data

103
00:03:41,280 --> 00:03:43,739
at test time they can exploit this

104
00:03:43,739 --> 00:03:45,360
vulnerability and cause a

105
00:03:45,360 --> 00:03:47,400
misclassification right very standard

106
00:03:47,400 --> 00:03:49,799
data poisoning attack scenario

107
00:03:49,799 --> 00:03:52,200
now forensics come after everything has

108
00:03:52,200 --> 00:03:54,120
already happened here right seek to

109
00:03:54,120 --> 00:03:56,400
analyze this misclassification event as

110
00:03:56,400 --> 00:03:58,739
we call and to see what went wrong right

111
00:03:58,739 --> 00:04:01,319
who is that hacker that tried to now try

112
00:04:01,319 --> 00:04:03,659
to already broken my system and break my

113
00:04:03,659 --> 00:04:05,400
defense and cause of damage

114
00:04:05,400 --> 00:04:07,500
in the case of poison attack that's easy

115
00:04:07,500 --> 00:04:10,080
to answer it's a set of potent data that

116
00:04:10,080 --> 00:04:11,040
are responsible for this

117
00:04:11,040 --> 00:04:12,659
misclassification

118
00:04:12,659 --> 00:04:14,760
so the goal is if we can trace back

119
00:04:14,760 --> 00:04:16,500
identify the set of poison data with

120
00:04:16,500 --> 00:04:19,139
sufficient accuracy then we can identify

121
00:04:19,139 --> 00:04:20,940
how these positives got into the system

122
00:04:20,940 --> 00:04:22,320
right through some traditional security

123
00:04:22,320 --> 00:04:24,479
means like oh they got into because a

124
00:04:24,479 --> 00:04:27,000
malicious data provider like because you

125
00:04:27,000 --> 00:04:28,979
have a unpatched vulnerability of your

126
00:04:28,979 --> 00:04:31,139
server so they compromise your training

127
00:04:31,139 --> 00:04:32,699
data pipeline

128
00:04:32,699 --> 00:04:34,680
I want to emphasize that this is

129
00:04:34,680 --> 00:04:37,620
different from Poison defenses again

130
00:04:37,620 --> 00:04:39,960
defense try to prevent the attack before

131
00:04:39,960 --> 00:04:42,540
it even happens but forensics try to

132
00:04:42,540 --> 00:04:44,460
handle the post-modern analysis of

133
00:04:44,460 --> 00:04:47,040
already happened attack right to analyze

134
00:04:47,040 --> 00:04:49,259
whose attacker wasn't wrong how can we

135
00:04:49,259 --> 00:04:52,080
prevent it in the future

136
00:04:52,080 --> 00:04:53,940
so how does this work it may seem

137
00:04:53,940 --> 00:04:55,380
straightforward right all we need to do

138
00:04:55,380 --> 00:04:58,800
is to analyze go from this attack to the

139
00:04:58,800 --> 00:05:00,720
model parameters that are malicious and

140
00:05:00,720 --> 00:05:02,100
look at the training data that impact

141
00:05:02,100 --> 00:05:03,900
these model parameters

142
00:05:03,900 --> 00:05:05,820
but this is actually pretty hard to do

143
00:05:05,820 --> 00:05:08,280
mostly because different neural networks

144
00:05:08,280 --> 00:05:10,919
as we know are very hard to interpret

145
00:05:10,919 --> 00:05:13,380
very hard to understand it's hard for us

146
00:05:13,380 --> 00:05:15,360
to say anything deterministic about how

147
00:05:15,360 --> 00:05:16,979
training data impact the model

148
00:05:16,979 --> 00:05:19,080
parameters and how the millions and

149
00:05:19,080 --> 00:05:20,639
billions of parameters impact the

150
00:05:20,639 --> 00:05:22,979
misclassification

151
00:05:22,979 --> 00:05:25,620
adding on top of that for most of the

152
00:05:25,620 --> 00:05:28,800
cases poisoning is also a group effort

153
00:05:28,800 --> 00:05:30,539
right that means we must find a

154
00:05:30,539 --> 00:05:32,460
sufficient amount of poison Data before

155
00:05:32,460 --> 00:05:35,100
we can reason about whether these set of

156
00:05:35,100 --> 00:05:36,840
poison data costs the misclassification

157
00:05:36,840 --> 00:05:38,639
right because most of the time they need

158
00:05:38,639 --> 00:05:41,759
to work together to do this

159
00:05:41,759 --> 00:05:44,940
so motivated by these two challenges we

160
00:05:44,940 --> 00:05:47,580
design our approach in the following way

161
00:05:47,580 --> 00:05:49,320
at a very high level

162
00:05:49,320 --> 00:05:51,780
we first seek to Cluster the training

163
00:05:51,780 --> 00:05:53,880
data into groups so we can reason about

164
00:05:53,880 --> 00:05:55,860
them at a group level

165
00:05:55,860 --> 00:05:58,560
so we seek to iteratively identify the

166
00:05:58,560 --> 00:06:01,020
B9 clusters where the cluster of data

167
00:06:01,020 --> 00:06:02,759
that are not responsible for this attack

168
00:06:02,759 --> 00:06:04,560
we're going to remove these clusters

169
00:06:04,560 --> 00:06:06,600
until we're left with a set of poison

170
00:06:06,600 --> 00:06:09,780
data that are responsible

171
00:06:09,780 --> 00:06:11,580
I'm going to show you a high levels

172
00:06:11,580 --> 00:06:13,380
overview of the end-to-end how the

173
00:06:13,380 --> 00:06:15,120
system works and then I will talk to

174
00:06:15,120 --> 00:06:17,160
talk about how the individual component

175
00:06:17,160 --> 00:06:18,780
in the system work

176
00:06:18,780 --> 00:06:21,300
so we start with other training data and

177
00:06:21,300 --> 00:06:23,400
the task here is to identify the set of

178
00:06:23,400 --> 00:06:24,780
poison data that caused this

179
00:06:24,780 --> 00:06:26,880
misclassification of course we don't

180
00:06:26,880 --> 00:06:29,100
know where are they at the beginning

181
00:06:29,100 --> 00:06:31,020
first step is clustering we group

182
00:06:31,020 --> 00:06:33,840
together similar types of data with each

183
00:06:33,840 --> 00:06:36,720
other using a certain embedding space

184
00:06:36,720 --> 00:06:38,340
so we're going to do a binary search

185
00:06:38,340 --> 00:06:40,800
through these clusters we use kmin here

186
00:06:40,800 --> 00:06:42,660
to take a binary cut separate data into

187
00:06:42,660 --> 00:06:44,400
two partitions so separate alternating

188
00:06:44,400 --> 00:06:46,860
data into two partitions and we use the

189
00:06:46,860 --> 00:06:48,660
second component to identify which

190
00:06:48,660 --> 00:06:51,240
cluster is benign and in this case this

191
00:06:51,240 --> 00:06:52,440
part is B9

192
00:06:52,440 --> 00:06:54,600
we keep going with the binary search

193
00:06:54,600 --> 00:06:57,000
another binary cuts for the rest of the

194
00:06:57,000 --> 00:06:59,160
data find this is benign keep going

195
00:06:59,160 --> 00:07:02,100
until we can no longer find any B9

196
00:07:02,100 --> 00:07:04,740
clusters and then we know the rest data

197
00:07:04,740 --> 00:07:06,720
must be poisoned so we output these data

198
00:07:06,720 --> 00:07:10,759
as the data Flag by our system

199
00:07:11,039 --> 00:07:13,319
I hope this end-to-end uh illustration

200
00:07:13,319 --> 00:07:15,240
is clear now we need to talk about the

201
00:07:15,240 --> 00:07:16,979
two components that make this work

202
00:07:16,979 --> 00:07:19,319
first is data clustering and second is

203
00:07:19,319 --> 00:07:21,599
how to identify the B9 clusters

204
00:07:21,599 --> 00:07:24,060
Starve is they're clustering idea is

205
00:07:24,060 --> 00:07:25,500
simple we just need to project the data

206
00:07:25,500 --> 00:07:28,139
into an embedding space to group similar

207
00:07:28,139 --> 00:07:30,180
types of data together and for this work

208
00:07:30,180 --> 00:07:34,020
we we do this by using for each training

209
00:07:34,020 --> 00:07:36,060
data point by using its impact on the

210
00:07:36,060 --> 00:07:37,979
model parameter during the printing

211
00:07:37,979 --> 00:07:41,940
process as the embedding right for for

212
00:07:41,940 --> 00:07:43,979
poison and and benign they have very

213
00:07:43,979 --> 00:07:45,960
different objective during the training

214
00:07:45,960 --> 00:07:48,120
process so they oftentimes have very

215
00:07:48,120 --> 00:07:50,940
different uh embedding space so we can

216
00:07:50,940 --> 00:07:53,220
group them into separate groups

217
00:07:53,220 --> 00:07:55,500
unfortunately I really don't have time

218
00:07:55,500 --> 00:07:57,120
in this talk to talk about the exact

219
00:07:57,120 --> 00:07:58,740
embedding we used as well as easy

220
00:07:58,740 --> 00:08:00,960
robustness to potential adaptive

221
00:08:00,960 --> 00:08:03,000
adversaries but those are covering the

222
00:08:03,000 --> 00:08:04,199
paper so take a look if you're

223
00:08:04,199 --> 00:08:05,759
interested

224
00:08:05,759 --> 00:08:07,500
now the focus of the rest of the stock

225
00:08:07,500 --> 00:08:09,300
is going to be on the second component

226
00:08:09,300 --> 00:08:11,460
right how do we identify the benign

227
00:08:11,460 --> 00:08:12,720
cluster

228
00:08:12,720 --> 00:08:14,520
for example we want to understand how

229
00:08:14,520 --> 00:08:16,620
does cluster be here impact the

230
00:08:16,620 --> 00:08:18,780
misclassification does it cost it

231
00:08:18,780 --> 00:08:20,220
so we're going to do this counter

232
00:08:20,220 --> 00:08:22,259
factually it means we're going to remove

233
00:08:22,259 --> 00:08:24,599
cluster B completely out of the equation

234
00:08:24,599 --> 00:08:26,099
we're going to remove it from the

235
00:08:26,099 --> 00:08:27,840
training data set and train a new model

236
00:08:27,840 --> 00:08:30,240
from scratch on the rest of the data

237
00:08:30,240 --> 00:08:31,680
except cluster B

238
00:08:31,680 --> 00:08:33,059
and we're going to check to see whether

239
00:08:33,059 --> 00:08:34,740
the misclassification the original

240
00:08:34,740 --> 00:08:37,440
attack is still successful on this new

241
00:08:37,440 --> 00:08:38,339
model

242
00:08:38,339 --> 00:08:40,380
we can do this because again this is

243
00:08:40,380 --> 00:08:42,659
forensics after attack so we do have

244
00:08:42,659 --> 00:08:44,760
access to the original attack we can

245
00:08:44,760 --> 00:08:46,980
replay it

246
00:08:46,980 --> 00:08:48,779
and in this case the attack is still

247
00:08:48,779 --> 00:08:50,519
successful so we know that cluster B

248
00:08:50,519 --> 00:08:52,680
must be benign data because removing it

249
00:08:52,680 --> 00:08:56,399
does not decrease the success of attack

250
00:08:56,399 --> 00:08:58,260
if we do this to Cluster a you can

251
00:08:58,260 --> 00:08:59,700
imagine the opposite are going to happen

252
00:08:59,700 --> 00:09:01,680
where a new model trained on cluster B

253
00:09:01,680 --> 00:09:04,260
the attack no longer work on this so we

254
00:09:04,260 --> 00:09:06,300
know that the miscar the poison data

255
00:09:06,300 --> 00:09:08,519
must be in cluster a

256
00:09:08,519 --> 00:09:10,320
all right so this indeed gave us a way

257
00:09:10,320 --> 00:09:13,200
to to find the B9 clusters but as you

258
00:09:13,200 --> 00:09:15,360
may have already noticed this is a very

259
00:09:15,360 --> 00:09:17,279
expensive to do mostly because we have

260
00:09:17,279 --> 00:09:20,040
to train tons of models from scratch and

261
00:09:20,040 --> 00:09:21,480
it will take around a month to do this

262
00:09:21,480 --> 00:09:23,339
for imagenet data set

263
00:09:23,339 --> 00:09:25,620
so we try to speed this up using a trick

264
00:09:25,620 --> 00:09:28,140
called unlearning okay so rather than

265
00:09:28,140 --> 00:09:30,600
training a new model from scratch on the

266
00:09:30,600 --> 00:09:32,760
rest of the training data what we're

267
00:09:32,760 --> 00:09:34,200
going to do is we're going to unlearn or

268
00:09:34,200 --> 00:09:36,540
remove the effective training effect of

269
00:09:36,540 --> 00:09:38,220
the current cluster from the original

270
00:09:38,220 --> 00:09:40,320
model that we have access to

271
00:09:40,320 --> 00:09:42,240
and this will speed things up

272
00:09:42,240 --> 00:09:44,459
and learning of course is explored in

273
00:09:44,459 --> 00:09:46,260
the privacy setting a lot of proposal

274
00:09:46,260 --> 00:09:48,720
already exist but unfortunately they're

275
00:09:48,720 --> 00:09:51,420
still too expensive for our purpose

276
00:09:51,420 --> 00:09:53,339
simply because they're not designed for

277
00:09:53,339 --> 00:09:55,920
this task right the task of unlearning a

278
00:09:55,920 --> 00:09:57,420
large amount of training data again and

279
00:09:57,420 --> 00:09:58,800
again

280
00:09:58,800 --> 00:10:00,720
as a result we propose our own version

281
00:10:00,720 --> 00:10:03,180
of learning we call it functional and

282
00:10:03,180 --> 00:10:04,440
learning

283
00:10:04,440 --> 00:10:06,660
the idea is this right a normal model

284
00:10:06,660 --> 00:10:09,420
training you serve as a poor model or a

285
00:10:09,420 --> 00:10:11,399
randomly initialized model you slowly

286
00:10:11,399 --> 00:10:14,279
slowly converge to a local Optima

287
00:10:14,279 --> 00:10:15,959
functional learning simply revert that

288
00:10:15,959 --> 00:10:17,459
process we serve as a good original

289
00:10:17,459 --> 00:10:20,760
model we wanted to perform poorly on the

290
00:10:20,760 --> 00:10:22,560
data you want to and learn I want to

291
00:10:22,560 --> 00:10:23,820
forget because you didn't learn anything

292
00:10:23,820 --> 00:10:25,260
from the data set

293
00:10:25,260 --> 00:10:26,940
so how do we do this

294
00:10:26,940 --> 00:10:28,260
so we're going to retrain the original

295
00:10:28,260 --> 00:10:31,560
model so that the output not the correct

296
00:10:31,560 --> 00:10:33,420
label for each of these data you want to

297
00:10:33,420 --> 00:10:34,320
unlearn

298
00:10:34,320 --> 00:10:37,019
but rather a uniform probability Vector

299
00:10:37,019 --> 00:10:39,360
so what is this it's simply the case

300
00:10:39,360 --> 00:10:42,240
where the model is unsure which class or

301
00:10:42,240 --> 00:10:43,920
which output class does current data

302
00:10:43,920 --> 00:10:45,839
belong to so you just simply give equal

303
00:10:45,839 --> 00:10:48,660
probability to all classes

304
00:10:48,660 --> 00:10:51,000
right so we do this by simply minimizing

305
00:10:51,000 --> 00:10:53,279
this loss function to force the original

306
00:10:53,279 --> 00:10:55,620
model to Output the uniform probability

307
00:10:55,620 --> 00:10:58,079
Vector for each of the data point you

308
00:10:58,079 --> 00:11:00,120
want to under

309
00:11:00,120 --> 00:11:02,100
and this effectively turned a learning

310
00:11:02,100 --> 00:11:04,079
problem into a simple fine-tuning

311
00:11:04,079 --> 00:11:06,540
heuristic that's very efficient so we

312
00:11:06,540 --> 00:11:08,399
can effectively reduce the cost of of

313
00:11:08,399 --> 00:11:10,380
doing end-to-end forensics for imagenet

314
00:11:10,380 --> 00:11:12,300
data set from one month to around two

315
00:11:12,300 --> 00:11:13,740
hours

316
00:11:13,740 --> 00:11:16,200
right so this is all about uh the two

317
00:11:16,200 --> 00:11:17,760
components of course there are more

318
00:11:17,760 --> 00:11:19,860
details in the paper but let's take a

319
00:11:19,860 --> 00:11:22,019
look at the evaluation results

320
00:11:22,019 --> 00:11:23,459
and they'll be running behind time so

321
00:11:23,459 --> 00:11:24,839
I'm going to speed this upload bit more

322
00:11:24,839 --> 00:11:26,940
results are in the paper of course

323
00:11:26,940 --> 00:11:28,620
we tested against three types of

324
00:11:28,620 --> 00:11:30,360
backdoor tasks including the physical

325
00:11:30,360 --> 00:11:32,519
backdoor attack that does not have a

326
00:11:32,519 --> 00:11:35,279
known defense at this moment

327
00:11:35,279 --> 00:11:36,779
wait to evaluate our system where you

328
00:11:36,779 --> 00:11:39,300
use the simple Precision recall of the

329
00:11:39,300 --> 00:11:41,220
poison data we identified when compared

330
00:11:41,220 --> 00:11:44,399
to the ground truth poison data

331
00:11:44,399 --> 00:11:46,200
really well on backdoor attacks we can

332
00:11:46,200 --> 00:11:49,260
achieve 97 above 97 on Precision recall

333
00:11:49,260 --> 00:11:51,839
at tracing back these poison data

334
00:11:51,839 --> 00:11:53,640
we took one step further we took a look

335
00:11:53,640 --> 00:11:55,920
at clean label poison attack where for

336
00:11:55,920 --> 00:11:57,540
people are not familiar it's a very

337
00:11:57,540 --> 00:11:59,700
different threat model it also rely on

338
00:11:59,700 --> 00:12:02,040
very different attack mastology

339
00:12:02,040 --> 00:12:04,019
the same traceback systems still worked

340
00:12:04,019 --> 00:12:05,820
really well in this case including

341
00:12:05,820 --> 00:12:10,200
against a a attack was known on defense

342
00:12:10,200 --> 00:12:12,000
lastly in the paper we also took a look

343
00:12:12,000 --> 00:12:14,220
at adaptive adversaries where attacker

344
00:12:14,220 --> 00:12:15,260
really tried to go out their way

345
00:12:15,260 --> 00:12:17,880
customize the data poisoning approach in

346
00:12:17,880 --> 00:12:20,519
order to bypass our forensic system and

347
00:12:20,519 --> 00:12:23,100
we show that they are not effective

348
00:12:23,100 --> 00:12:25,440
or I'll conclude my talk by pointing you

349
00:12:25,440 --> 00:12:28,019
to our project webpage which include

350
00:12:28,019 --> 00:12:29,579
updated version of the paper as well as

351
00:12:29,579 --> 00:12:31,079
a co-release

352
00:12:31,079 --> 00:12:33,000
and lastly I hope this talk was able to

353
00:12:33,000 --> 00:12:35,640
convince you that forensics perhaps the

354
00:12:35,640 --> 00:12:37,260
interesting direction for ML security

355
00:12:37,260 --> 00:12:40,079
and we do have outcoming paper at CCs in

356
00:12:40,079 --> 00:12:41,700
November to explore the forensics for

357
00:12:41,700 --> 00:12:43,440
average year examples the archive

358
00:12:43,440 --> 00:12:44,940
version is already online you can also

359
00:12:44,940 --> 00:12:46,380
check out that paper

360
00:12:46,380 --> 00:12:48,180
all right stop right here happy to take

361
00:12:48,180 --> 00:12:50,599
any questions

