1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:10,639 --> 00:00:12,719
that we received the distinguished paper

3
00:00:12,719 --> 00:00:14,759
award and for this work and I also want

4
00:00:14,759 --> 00:00:16,680
to thank my wonderful collaborators at

5
00:00:16,680 --> 00:00:18,300
this point

6
00:00:18,300 --> 00:00:20,880
so machine learning has without any

7
00:00:20,880 --> 00:00:23,279
doubt led to some major breakthroughs in

8
00:00:23,279 --> 00:00:25,080
different areas and when you read

9
00:00:25,080 --> 00:00:26,760
research papers that apply these

10
00:00:26,760 --> 00:00:29,220
techniques to security related fields

11
00:00:29,220 --> 00:00:31,380
you sometimes get the impression that

12
00:00:31,380 --> 00:00:33,600
they also entirely solved some of the

13
00:00:33,600 --> 00:00:35,820
problems we are having security or even

14
00:00:35,820 --> 00:00:37,079
many of them

15
00:00:37,079 --> 00:00:40,860
so naturally the question arises is this

16
00:00:40,860 --> 00:00:44,280
true and unfortunately in the past we

17
00:00:44,280 --> 00:00:45,780
have often seen that it's actually not

18
00:00:45,780 --> 00:00:48,239
due to different reasons

19
00:00:48,239 --> 00:00:49,739
so for instance due to the base rate

20
00:00:49,739 --> 00:00:51,719
fallacy that has been first discussed

21
00:00:51,719 --> 00:00:53,340
for intrusion detection systems and

22
00:00:53,340 --> 00:00:54,960
later also in other security related

23
00:00:54,960 --> 00:00:57,300
faiths like that as well in the case of

24
00:00:57,300 --> 00:00:59,340
intrusion detection systems when

25
00:00:59,340 --> 00:01:00,899
evaluating these systems what

26
00:01:00,899 --> 00:01:02,820
researchers commonly ignored here is the

27
00:01:02,820 --> 00:01:04,799
fact that in the wild attack traffic is

28
00:01:04,799 --> 00:01:06,479
only a very small proportion of the

29
00:01:06,479 --> 00:01:09,600
total traffic and so even a low false

30
00:01:09,600 --> 00:01:11,880
positive rate in a laboratory setting

31
00:01:11,880 --> 00:01:13,740
might still correspond to thousands or

32
00:01:13,740 --> 00:01:16,020
even millions of false positives in

33
00:01:16,020 --> 00:01:18,960
practice due to the low base rate of the

34
00:01:18,960 --> 00:01:20,759
attack class

35
00:01:20,759 --> 00:01:23,280
the second and more recent example is

36
00:01:23,280 --> 00:01:24,840
some work of my collaborators from

37
00:01:24,840 --> 00:01:26,400
London where they showed that

38
00:01:26,400 --> 00:01:28,619
experimental bias was an endemic problem

39
00:01:28,619 --> 00:01:30,540
in the evaluation of Android malware

40
00:01:30,540 --> 00:01:31,979
detectors

41
00:01:31,979 --> 00:01:34,500
this manifested in a couple of ways but

42
00:01:34,500 --> 00:01:36,720
most important it was due to ignoring

43
00:01:36,720 --> 00:01:38,759
the temporal relationships within the

44
00:01:38,759 --> 00:01:40,740
data so that models were trained using

45
00:01:40,740 --> 00:01:42,600
future knowledge that would normally not

46
00:01:42,600 --> 00:01:44,579
be available in practice thus leading to

47
00:01:44,579 --> 00:01:46,979
unrealistic results

48
00:01:46,979 --> 00:01:49,799
now if you look closely into paper you

49
00:01:49,799 --> 00:01:51,420
will notice that they also found these

50
00:01:51,420 --> 00:01:53,100
issues in some of my work in which I

51
00:01:53,100 --> 00:01:54,600
proposed a learning based detector for

52
00:01:54,600 --> 00:01:56,360
Android malware detection called dragon

53
00:01:56,360 --> 00:01:58,920
and although it of course never feels

54
00:01:58,920 --> 00:02:01,140
good if someone highlights weaknesses in

55
00:02:01,140 --> 00:02:03,659
your research I found the Tesseract

56
00:02:03,659 --> 00:02:06,180
paper to be a very insightful piece of

57
00:02:06,180 --> 00:02:08,818
work that also showed how we can

58
00:02:08,818 --> 00:02:10,759
approach these problems as a community

59
00:02:10,759 --> 00:02:13,860
and ultimately the idea here evolved for

60
00:02:13,860 --> 00:02:15,540
this collaboration to provide a more

61
00:02:15,540 --> 00:02:17,459
generic overview of things that can go

62
00:02:17,459 --> 00:02:19,500
wrong when applying machine learning for

63
00:02:19,500 --> 00:02:21,540
security research

64
00:02:21,540 --> 00:02:23,879
and so our goal here with this work was

65
00:02:23,879 --> 00:02:26,220
to first to try to identify what these

66
00:02:26,220 --> 00:02:28,560
common pitfalls are and how we can avoid

67
00:02:28,560 --> 00:02:30,180
them in order to improve the state of

68
00:02:30,180 --> 00:02:32,400
Empirical research and machine learning

69
00:02:32,400 --> 00:02:34,260
for security and so we ended up here

70
00:02:34,260 --> 00:02:37,319
with focusing on 10 Common pitfalls and

71
00:02:37,319 --> 00:02:39,000
also derived some recommendations on how

72
00:02:39,000 --> 00:02:40,860
to mitigate them

73
00:02:40,860 --> 00:02:42,540
and then we wanted to see just how

74
00:02:42,540 --> 00:02:44,280
common these perpetuals were and so we

75
00:02:44,280 --> 00:02:46,620
performed a survey across 30 top

76
00:02:46,620 --> 00:02:48,959
security papers from the last 10 years

77
00:02:48,959 --> 00:02:51,000
and found that yes these pitfalls at

78
00:02:51,000 --> 00:02:52,500
least some of them are actually pretty

79
00:02:52,500 --> 00:02:54,180
common

80
00:02:54,180 --> 00:02:56,879
finally we try to assess the impact of

81
00:02:56,879 --> 00:02:58,800
these pitfalls and of course it's

82
00:02:58,800 --> 00:03:00,480
difficult to say concretely and

83
00:03:00,480 --> 00:03:02,640
generally what the impact of a

84
00:03:02,640 --> 00:03:05,340
particular Pitfall will be but to get an

85
00:03:05,340 --> 00:03:06,540
impression we looked here at four

86
00:03:06,540 --> 00:03:08,340
representative case studies based on

87
00:03:08,340 --> 00:03:10,620
examples from the literature and we then

88
00:03:10,620 --> 00:03:12,480
try to show just how much of an impact

89
00:03:12,480 --> 00:03:14,580
some of these pitfalls can have in those

90
00:03:14,580 --> 00:03:16,560
settings and so in this talk I will

91
00:03:16,560 --> 00:03:18,239
exactly follow the structure so we first

92
00:03:18,239 --> 00:03:21,060
run through the 10 pitfalls then I'll

93
00:03:21,060 --> 00:03:22,800
discuss the results of the prevalence

94
00:03:22,800 --> 00:03:25,080
analysis and finally also presents

95
00:03:25,080 --> 00:03:28,140
results of one of the case studies to

96
00:03:28,140 --> 00:03:31,019
provide you with a concrete example

97
00:03:31,019 --> 00:03:33,060
and one important remark here before I

98
00:03:33,060 --> 00:03:34,620
get into it so it's really not Our

99
00:03:34,620 --> 00:03:36,000
intention here to point fingers at

100
00:03:36,000 --> 00:03:37,739
anyone or blame anyone for having these

101
00:03:37,739 --> 00:03:40,200
pitfalls but as these pitfalls can have

102
00:03:40,200 --> 00:03:42,840
a significant impact on the results

103
00:03:42,840 --> 00:03:45,000
um and experimental outcome we think

104
00:03:45,000 --> 00:03:46,860
it's important to be aware of them and

105
00:03:46,860 --> 00:03:49,080
also discuss in the community how to

106
00:03:49,080 --> 00:03:52,140
overcome them wherever possible

107
00:03:52,140 --> 00:03:55,140
okay so then let's start with an

108
00:03:55,140 --> 00:03:56,760
overview of the pitfall so in our paper

109
00:03:56,760 --> 00:03:58,920
we assigned each Pitfall to one of the

110
00:03:58,920 --> 00:04:00,299
four stages of a typical machine

111
00:04:00,299 --> 00:04:03,299
learning pipeline so in particular you

112
00:04:03,299 --> 00:04:04,980
usually start with a collection of a

113
00:04:04,980 --> 00:04:06,959
data set and the corresponding branches

114
00:04:06,959 --> 00:04:09,540
labels using this data you can then in

115
00:04:09,540 --> 00:04:11,159
the second stage design your learning

116
00:04:11,159 --> 00:04:13,439
approach and train your model and the

117
00:04:13,439 --> 00:04:16,079
third step then evaluate its performance

118
00:04:16,079 --> 00:04:18,418
and if the performance meets or your

119
00:04:18,418 --> 00:04:21,660
requirements we can finally deploy and

120
00:04:21,660 --> 00:04:23,880
operate it in the wild

121
00:04:23,880 --> 00:04:26,340
okay so then let's now go over all these

122
00:04:26,340 --> 00:04:28,560
stages and have a look at the pitfalls

123
00:04:28,560 --> 00:04:30,240
we assigned to them due to the clamp

124
00:04:30,240 --> 00:04:31,800
constraints I won't go into details here

125
00:04:31,800 --> 00:04:33,840
but of course you find the detailed

126
00:04:33,840 --> 00:04:35,580
descriptions of these issues in our

127
00:04:35,580 --> 00:04:37,259
paper

128
00:04:37,259 --> 00:04:39,780
so the first Pitfall is sampling bias

129
00:04:39,780 --> 00:04:41,699
also known as sample selection bias

130
00:04:41,699 --> 00:04:44,040
we'll see more of that in a few slides

131
00:04:44,040 --> 00:04:47,639
second up is Ladle and accuracy

132
00:04:47,639 --> 00:04:50,040
third pistol is data snooping this means

133
00:04:50,040 --> 00:04:52,259
that information is used at training

134
00:04:52,259 --> 00:04:54,419
time that is usually not available in

135
00:04:54,419 --> 00:04:55,979
practice

136
00:04:55,979 --> 00:04:57,960
but for four spurious correlations so

137
00:04:57,960 --> 00:04:59,520
where the model picks up on Clever hands

138
00:04:59,520 --> 00:05:01,740
artifacts instead of solving the actual

139
00:05:01,740 --> 00:05:05,160
task Pitfall 5 bias parameter selection

140
00:05:05,160 --> 00:05:07,020
so when the parameters of hyper

141
00:05:07,020 --> 00:05:08,280
parameters of your model are not

142
00:05:08,280 --> 00:05:10,740
entirely fixed at training time

143
00:05:10,740 --> 00:05:13,400
Pitfall 6 is an appropriate bass lines

144
00:05:13,400 --> 00:05:16,259
pit47 inappropriate performance measures

145
00:05:16,259 --> 00:05:19,020
Pitfall 8 is the base rate fallacy which

146
00:05:19,020 --> 00:05:20,639
I've already mentioned in the

147
00:05:20,639 --> 00:05:22,440
introduction

148
00:05:22,440 --> 00:05:24,180
and then in the last stage we have

149
00:05:24,180 --> 00:05:26,759
Pitfall 9 left only evaluation so when

150
00:05:26,759 --> 00:05:28,199
you don't discuss the Practical

151
00:05:28,199 --> 00:05:30,960
limitations of your approach

152
00:05:30,960 --> 00:05:33,919
um sufficiently and finally Pitfall 10

153
00:05:33,919 --> 00:05:35,940
inappropriate threat model so when the

154
00:05:35,940 --> 00:05:37,560
security of the machine learning

155
00:05:37,560 --> 00:05:39,479
algorithm itself is not considered

156
00:05:39,479 --> 00:05:41,340
sufficiently

157
00:05:41,340 --> 00:05:43,620
so now after you've identified these

158
00:05:43,620 --> 00:05:45,360
template faults we wanted to see just

159
00:05:45,360 --> 00:05:47,699
how deep does it go so how prevalent are

160
00:05:47,699 --> 00:05:50,820
these issues and past work and so we

161
00:05:50,820 --> 00:05:53,340
revert here 30 papers from top security

162
00:05:53,340 --> 00:05:55,560
conferences covering a wide range of

163
00:05:55,560 --> 00:05:58,080
topics so ranging from malware detection

164
00:05:58,080 --> 00:06:00,479
or availability Discovery to authorship

165
00:06:00,479 --> 00:06:02,880
attribution and the plot here shows the

166
00:06:02,880 --> 00:06:05,520
distribution over time over the the time

167
00:06:05,520 --> 00:06:09,060
span we then try to determine whether

168
00:06:09,060 --> 00:06:11,039
these papers contain any of the

169
00:06:11,039 --> 00:06:13,320
discussed pitfalls and each Pitfall

170
00:06:13,320 --> 00:06:15,419
could either be present

171
00:06:15,419 --> 00:06:17,699
partly present for instance a pitfall

172
00:06:17,699 --> 00:06:20,100
only appeared in some of the experiments

173
00:06:20,100 --> 00:06:23,280
not present of course and unclear from

174
00:06:23,280 --> 00:06:24,960
text so if not enough information was

175
00:06:24,960 --> 00:06:26,580
provided to come to a conclusive

176
00:06:26,580 --> 00:06:29,220
decision and we also moderated our

177
00:06:29,220 --> 00:06:31,380
decision if the pitfall has been

178
00:06:31,380 --> 00:06:34,139
explicitly discussed in the text

179
00:06:34,139 --> 00:06:35,940
every paper was reviewed by two

180
00:06:35,940 --> 00:06:37,680
independent reviewers and then a third

181
00:06:37,680 --> 00:06:40,080
reviewer resolved any final uncertainty

182
00:06:40,080 --> 00:06:42,000
with the benefit of the doubt going to

183
00:06:42,000 --> 00:06:43,740
the author so for instance it was

184
00:06:43,740 --> 00:06:45,539
unclear whether Pitfall was present or

185
00:06:45,539 --> 00:06:47,460
partly present we decided for partly

186
00:06:47,460 --> 00:06:49,319
present

187
00:06:49,319 --> 00:06:51,780
and then finally we also contacted the

188
00:06:51,780 --> 00:06:53,639
authors of the papers and asked for

189
00:06:53,639 --> 00:06:56,699
their feedback on our findings

190
00:06:56,699 --> 00:06:59,280
so how does it look like so the most

191
00:06:59,280 --> 00:07:01,560
prevalent pitfalls are sampling bias and

192
00:07:01,560 --> 00:07:03,419
data snoping which are at least partly

193
00:07:03,419 --> 00:07:06,840
present 90 and 73 of the papers

194
00:07:06,840 --> 00:07:10,560
respectively also we identify an

195
00:07:10,560 --> 00:07:12,240
appropriate performance measures lab

196
00:07:12,240 --> 00:07:14,100
only evaluation at an appropriate threat

197
00:07:14,100 --> 00:07:16,020
model to be at least partly present in

198
00:07:16,020 --> 00:07:18,300
more than half of the papers

199
00:07:18,300 --> 00:07:21,000
all pitfalls are present in at least

200
00:07:21,000 --> 00:07:23,099
some of the review Publications and

201
00:07:23,099 --> 00:07:26,039
interestingly only in 22 percent of

202
00:07:26,039 --> 00:07:28,319
these cases are the pitfalls discussed

203
00:07:28,319 --> 00:07:31,020
in the text so what can we infer from

204
00:07:31,020 --> 00:07:34,020
these results in summary we see here

205
00:07:34,020 --> 00:07:35,759
that many of these pitfalls commonly

206
00:07:35,759 --> 00:07:37,919
occur even in very carefully conducted

207
00:07:37,919 --> 00:07:39,539
top research and we're hoping that by

208
00:07:39,539 --> 00:07:41,699
systematizing them in this way we can

209
00:07:41,699 --> 00:07:43,919
start to raise awareness and reduce

210
00:07:43,919 --> 00:07:48,740
their prevalence and future research

211
00:07:48,979 --> 00:07:52,020
but even if we can see that these

212
00:07:52,020 --> 00:07:53,460
pitfalls are coming it doesn't really

213
00:07:53,460 --> 00:07:56,160
show how how serious their impact is and

214
00:07:56,160 --> 00:07:57,840
it's of course difficult to quantify

215
00:07:57,840 --> 00:07:58,560
this

216
00:07:58,560 --> 00:08:00,419
but what we did was that we looked at a

217
00:08:00,419 --> 00:08:02,280
handful of Representative case studies

218
00:08:02,280 --> 00:08:04,139
Where We examined the impact of

219
00:08:04,139 --> 00:08:06,120
different pitfalls in these settings so

220
00:08:06,120 --> 00:08:08,400
in particular we looked here at a case

221
00:08:08,400 --> 00:08:10,680
study in Android malware detection

222
00:08:10,680 --> 00:08:13,680
authorship attribution in vulnerability

223
00:08:13,680 --> 00:08:15,780
Discovery and the network intrusion

224
00:08:15,780 --> 00:08:18,660
detection system at the detection and

225
00:08:18,660 --> 00:08:20,460
due to the time constraint I will only

226
00:08:20,460 --> 00:08:22,800
shortly discuss here the case study on

227
00:08:22,800 --> 00:08:26,000
Android malware detection

228
00:08:26,280 --> 00:08:28,740
so in this setting you want to predict

229
00:08:28,740 --> 00:08:31,860
whether an app is malicious or benign

230
00:08:31,860 --> 00:08:33,659
and the issue that researchers are

231
00:08:33,659 --> 00:08:36,120
commonly facing here is that I cannot

232
00:08:36,120 --> 00:08:37,679
simply collect data from the Google Play

233
00:08:37,679 --> 00:08:39,419
Store this only represents a

234
00:08:39,419 --> 00:08:40,979
distribution after Google's internal

235
00:08:40,979 --> 00:08:43,020
vetting process so after most of the

236
00:08:43,020 --> 00:08:45,540
malware has already been removed and so

237
00:08:45,540 --> 00:08:47,820
as a remedy what research researchers

238
00:08:47,820 --> 00:08:49,800
usually do here is that they

239
00:08:49,800 --> 00:08:52,019
merge data from different sources to

240
00:08:52,019 --> 00:08:54,300
compose their data sets and this causes

241
00:08:54,300 --> 00:08:55,920
a sampling bias that even affects

242
00:08:55,920 --> 00:08:57,720
popular data sets like for instance the

243
00:08:57,720 --> 00:09:00,600
Android Zoo data set which is one of the

244
00:09:00,600 --> 00:09:03,120
best data sources for researchers in

245
00:09:03,120 --> 00:09:04,260
this field

246
00:09:04,260 --> 00:09:07,320
still we see even here a sampling bias

247
00:09:07,320 --> 00:09:10,080
when analyzing the metadata of this data

248
00:09:10,080 --> 00:09:13,260
of provided by the authors so on the

249
00:09:13,260 --> 00:09:15,899
plot here shows the probability of

250
00:09:15,899 --> 00:09:17,899
getting an app from a particular store

251
00:09:17,899 --> 00:09:20,580
independency of the number of antivirus

252
00:09:20,580 --> 00:09:23,220
detections so for instance when we

253
00:09:23,220 --> 00:09:25,380
sample a malicious application so an app

254
00:09:25,380 --> 00:09:27,240
for instance that has been detected by

255
00:09:27,240 --> 00:09:29,580
at least 10 antivirus scanners it will

256
00:09:29,580 --> 00:09:31,080
most likely come from an alternative

257
00:09:31,080 --> 00:09:32,640
Chinese market

258
00:09:32,640 --> 00:09:35,580
similarly if we sample a benign

259
00:09:35,580 --> 00:09:37,980
application it will most likely come

260
00:09:37,980 --> 00:09:40,019
from the Google Play Store and so this

261
00:09:40,019 --> 00:09:42,180
looks a lot like a spurious correlation

262
00:09:42,180 --> 00:09:44,160
that could create a shortcut pattern for

263
00:09:44,160 --> 00:09:45,360
the classifier

264
00:09:45,360 --> 00:09:47,459
and it's not worthy that we also

265
00:09:47,459 --> 00:09:49,560
observed a similar sampling bias in the

266
00:09:49,560 --> 00:09:51,720
dragon data set as well

267
00:09:51,720 --> 00:09:54,360
now to examine the potential impact you

268
00:09:54,360 --> 00:09:57,000
further we compose two data sets with

269
00:09:57,000 --> 00:09:58,920
and without this ad effect

270
00:09:58,920 --> 00:10:01,019
so for the first data set we solely use

271
00:10:01,019 --> 00:10:02,940
malware from Chinese markets and for a

272
00:10:02,940 --> 00:10:05,339
second only sample from the Google Play

273
00:10:05,339 --> 00:10:07,860
Store we then train a linear svm on two

274
00:10:07,860 --> 00:10:09,480
different feature spaces just to get

275
00:10:09,480 --> 00:10:12,779
some more General results and for both

276
00:10:12,779 --> 00:10:15,000
classifiers we see here a significant

277
00:10:15,000 --> 00:10:17,580
drop of the true positive rate of more

278
00:10:17,580 --> 00:10:20,940
than 10 or even 15 percent respectively

279
00:10:20,940 --> 00:10:23,100
which suggests that the artifact was

280
00:10:23,100 --> 00:10:24,720
contributing here to the better

281
00:10:24,720 --> 00:10:26,040
performance

282
00:10:26,040 --> 00:10:28,640
and not only that but we also find that

283
00:10:28,640 --> 00:10:32,339
among the top features um for the benign

284
00:10:32,339 --> 00:10:35,820
class is the URL play.google.com which

285
00:10:35,820 --> 00:10:37,140
further indicates here that the

286
00:10:37,140 --> 00:10:39,000
classifier at least partially learned to

287
00:10:39,000 --> 00:10:40,800
distinguish between different markets

288
00:10:40,800 --> 00:10:43,860
instead of separating benign and

289
00:10:43,860 --> 00:10:45,779
malicious applications

290
00:10:45,779 --> 00:10:48,240
finally it's also noteworthy that this

291
00:10:48,240 --> 00:10:50,040
bias would not have been visible if we

292
00:10:50,040 --> 00:10:51,720
would just have relied on the accuracy

293
00:10:51,720 --> 00:10:53,820
as a performance measure here

294
00:10:53,820 --> 00:10:57,240
so in summary this example shows that

295
00:10:57,240 --> 00:10:59,700
even if you have a very good data source

296
00:10:59,700 --> 00:11:01,320
like Android Zoo one should still be

297
00:11:01,320 --> 00:11:03,360
aware and skeptical and

298
00:11:03,360 --> 00:11:05,220
um yeah we're aware of the specularities

299
00:11:05,220 --> 00:11:07,680
to ensure that no unwanted biases affect

300
00:11:07,680 --> 00:11:11,339
the results and in our paper we provide

301
00:11:11,339 --> 00:11:13,019
three additional case studies where we

302
00:11:13,019 --> 00:11:16,079
show the impact of other pitfalls in

303
00:11:16,079 --> 00:11:19,019
different settings as well

304
00:11:19,019 --> 00:11:21,839
so then I'm already at the end of my

305
00:11:21,839 --> 00:11:23,940
talk so to summarize we identify 10

306
00:11:23,940 --> 00:11:25,740
Common pitfalls in machine learning for

307
00:11:25,740 --> 00:11:27,600
security find that they are prevalent

308
00:11:27,600 --> 00:11:29,700
even without carefully conducted top

309
00:11:29,700 --> 00:11:31,380
research and demonstrate the impact

310
00:11:31,380 --> 00:11:33,959
through four specific case studies

311
00:11:33,959 --> 00:11:36,899
unfortunately I didn't have time to talk

312
00:11:36,899 --> 00:11:38,220
about the recommendations again the

313
00:11:38,220 --> 00:11:40,620
pitfall so which you find of course in

314
00:11:40,620 --> 00:11:42,899
our paper and furthermore we set up a

315
00:11:42,899 --> 00:11:44,760
website where we provide additional

316
00:11:44,760 --> 00:11:46,740
information and plan to also provide

317
00:11:46,740 --> 00:11:48,660
updates on pitfalls and recommendations

318
00:11:48,660 --> 00:11:50,880
so if you're doing research in this

319
00:11:50,880 --> 00:11:53,399
direction and let us know and we will be

320
00:11:53,399 --> 00:11:55,860
happy to mention your paper on our

321
00:11:55,860 --> 00:11:58,140
website so I think that's then it from

322
00:11:58,140 --> 00:11:59,220
me

323
00:11:59,220 --> 00:12:00,899
um thank you very much for attention I'm

324
00:12:00,899 --> 00:12:03,980
looking forward for your questions

