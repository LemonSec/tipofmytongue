1
00:00:07,340 --> 00:00:10,559
hi my name is I'm from Princeton

2
00:00:10,559 --> 00:00:13,019
University I'm glad to present our work

3
00:00:13,019 --> 00:00:15,179
mitigating membership influence attack

4
00:00:15,179 --> 00:00:17,340
special distillation through another

5
00:00:17,340 --> 00:00:19,859
Ensemble architecture this is a joint

6
00:00:19,859 --> 00:00:22,500
work with my collaborator said leeway

7
00:00:22,500 --> 00:00:24,720
protect from Princeton University and we

8
00:00:24,720 --> 00:00:27,900
write Milad Amir from UMass Amherst

9
00:00:27,900 --> 00:00:30,300
in this work we focus on membership

10
00:00:30,300 --> 00:00:31,980
inference attacks Against the Machine

11
00:00:31,980 --> 00:00:33,960
learning models under the Black Box

12
00:00:33,960 --> 00:00:36,420
access assumption so the Machinery model

13
00:00:36,420 --> 00:00:38,820
will only output the predictions for

14
00:00:38,820 --> 00:00:41,820
each query and for the Target sample the

15
00:00:41,820 --> 00:00:43,680
texture will make corresponding queries

16
00:00:43,680 --> 00:00:45,840
to a Target machine learning model and

17
00:00:45,840 --> 00:00:48,300
guess whether this sample is used to

18
00:00:48,300 --> 00:00:50,100
train a model or not

19
00:00:50,100 --> 00:00:52,620
membership influence attacks can lead to

20
00:00:52,620 --> 00:00:55,739
substantial privacy leakage first this

21
00:00:55,739 --> 00:00:58,140
attack can real private information in

22
00:00:58,140 --> 00:01:00,960
real world scenarios for example knowing

23
00:01:00,960 --> 00:01:03,239
victims presence in hospital analogy

24
00:01:03,239 --> 00:01:05,700
data set reveals that this victim was

25
00:01:05,700 --> 00:01:08,159
once a patient in this Hospital Second

26
00:01:08,159 --> 00:01:10,439
this attack can be Foundation of

27
00:01:10,439 --> 00:01:12,600
stronger tax such as data extraction

28
00:01:12,600 --> 00:01:15,299
attack from large language models as

29
00:01:15,299 --> 00:01:16,979
membership infrastructure tax imposes

30
00:01:16,979 --> 00:01:19,260
severe privacy threats we need to design

31
00:01:19,260 --> 00:01:22,140
defenses against this attacks and to

32
00:01:22,140 --> 00:01:24,360
design defenses we first need to

33
00:01:24,360 --> 00:01:26,100
understand how membership inference

34
00:01:26,100 --> 00:01:28,979
attack works the success of membership

35
00:01:28,979 --> 00:01:31,439
influence attacks is usually because the

36
00:01:31,439 --> 00:01:33,180
machine learning models have different

37
00:01:33,180 --> 00:01:36,600
behaviors our members and now members so

38
00:01:36,600 --> 00:01:39,000
he here the members are samples in our

39
00:01:39,000 --> 00:01:41,280
training search and numbers are samples

40
00:01:41,280 --> 00:01:43,920
not in the training set I use a red card

41
00:01:43,920 --> 00:01:45,720
for members and blue color for numbers

42
00:01:45,720 --> 00:01:48,840
so it might look a little strange on the

43
00:01:48,840 --> 00:01:51,000
presentation but I hope this card works

44
00:01:51,000 --> 00:01:52,500
and

45
00:01:52,500 --> 00:01:54,659
so here is a distribution of

46
00:01:54,659 --> 00:01:57,240
cross-entropy loss members and numbers

47
00:01:57,240 --> 00:01:59,340
and we can see the significant

48
00:01:59,340 --> 00:02:00,960
difference between these two

49
00:02:00,960 --> 00:02:03,899
distributions and our defense intuition

50
00:02:03,899 --> 00:02:06,600
is that we should enforce a model to

51
00:02:06,600 --> 00:02:09,360
behave similarly on all samples so both

52
00:02:09,360 --> 00:02:12,120
include members and not members

53
00:02:12,120 --> 00:02:14,700
in this work we propose a new membership

54
00:02:14,700 --> 00:02:17,280
inference differences called sanana to

55
00:02:17,280 --> 00:02:19,560
enforce the model to behave similarly

56
00:02:19,560 --> 00:02:21,840
our members and our members so the

57
00:02:21,840 --> 00:02:23,819
training data is first used to train the

58
00:02:23,819 --> 00:02:25,560
machine learning models in our first

59
00:02:25,560 --> 00:02:28,860
component split AI after that we will do

60
00:02:28,860 --> 00:02:31,260
the self-destellation and after the

61
00:02:31,260 --> 00:02:33,239
self-disolation process we finally

62
00:02:33,239 --> 00:02:36,000
output the final protective model the

63
00:02:36,000 --> 00:02:37,980
experimental results show that our

64
00:02:37,980 --> 00:02:39,540
defense can achieve a good utility

65
00:02:39,540 --> 00:02:42,060
browse the trade-off we can achieve both

66
00:02:42,060 --> 00:02:44,640
High classification accuracy and also

67
00:02:44,640 --> 00:02:46,319
effectively mitigate the Practical

68
00:02:46,319 --> 00:02:49,080
membership influence attacks

69
00:02:49,080 --> 00:02:50,940
I will first introduce our design

70
00:02:50,940 --> 00:02:53,160
intuition and after that I will explain

71
00:02:53,160 --> 00:02:55,800
our defense framework so we need to

72
00:02:55,800 --> 00:02:57,900
enforce our model to behave similarly

73
00:02:57,900 --> 00:03:00,000
our members and not members but how

74
00:03:00,000 --> 00:03:03,120
let's consider true samples sample a is

75
00:03:03,120 --> 00:03:05,459
in our training set and Sample B is not

76
00:03:05,459 --> 00:03:08,280
in the training set with learning a

77
00:03:08,280 --> 00:03:09,900
machine learning model using this

78
00:03:09,900 --> 00:03:11,640
training search and we can see that

79
00:03:11,640 --> 00:03:13,980
sample a here is a member for this

80
00:03:13,980 --> 00:03:16,319
machine learning model then for the

81
00:03:16,319 --> 00:03:18,900
memory sample a the curve a will follow

82
00:03:18,900 --> 00:03:21,420
the distribution of members and for the

83
00:03:21,420 --> 00:03:24,000
non-member B the curve B will follow the

84
00:03:24,000 --> 00:03:26,819
distribution of non members how can we

85
00:03:26,819 --> 00:03:28,560
make these two queries to follow the

86
00:03:28,560 --> 00:03:30,540
same distribution

87
00:03:30,540 --> 00:03:33,000
so how about removing a from the

88
00:03:33,000 --> 00:03:34,879
training set before the model training

89
00:03:34,879 --> 00:03:38,340
Now sample A and B both are not in a

90
00:03:38,340 --> 00:03:40,560
training set and we will train a new

91
00:03:40,560 --> 00:03:42,780
machine learning model from scratch by

92
00:03:42,780 --> 00:03:45,299
using this new training search and we

93
00:03:45,299 --> 00:03:48,420
can see here sample a is a non-member so

94
00:03:48,420 --> 00:03:50,580
the curve a will follow the distribution

95
00:03:50,580 --> 00:03:54,000
of null members and Sample B is also not

96
00:03:54,000 --> 00:03:56,040
members so could be also called the

97
00:03:56,040 --> 00:03:59,099
distribution number members so now both

98
00:03:59,099 --> 00:04:01,440
queries follow same distribution which

99
00:04:01,440 --> 00:04:04,620
is good however the problem here is that

100
00:04:04,620 --> 00:04:07,080
we cannot remove all points from

101
00:04:07,080 --> 00:04:08,940
training search as we still need

102
00:04:08,940 --> 00:04:12,000
training data to train the models

103
00:04:12,000 --> 00:04:14,939
so how about we consider this design on

104
00:04:14,939 --> 00:04:17,279
multiple models this motivates our

105
00:04:17,279 --> 00:04:19,139
design of split AI

106
00:04:19,139 --> 00:04:21,298
here is a training procedure in splitter

107
00:04:21,298 --> 00:04:23,759
eye we will train case some models in

108
00:04:23,759 --> 00:04:26,040
total so we need case subsets

109
00:04:26,040 --> 00:04:28,560
each training sample will only appear in

110
00:04:28,560 --> 00:04:31,320
K minus L of the okay subsets so a

111
00:04:31,320 --> 00:04:33,660
training sample is memory for k-manner

112
00:04:33,660 --> 00:04:36,000
sales and models and now members for the

113
00:04:36,000 --> 00:04:39,360
remaining else models here consider a as

114
00:04:39,360 --> 00:04:41,820
a member sample and is used to train the

115
00:04:41,820 --> 00:04:44,759
top and a middle model so a is a noun

116
00:04:44,759 --> 00:04:47,639
member for the bottom model after we

117
00:04:47,639 --> 00:04:49,919
generate the subsets we can change some

118
00:04:49,919 --> 00:04:52,560
model with one subset and this is a

119
00:04:52,560 --> 00:04:55,380
training procedure in splitteri now I'll

120
00:04:55,380 --> 00:04:57,900
explain a double inference in Split AI

121
00:04:57,900 --> 00:04:59,880
the intuition for adult influence

122
00:04:59,880 --> 00:05:02,220
follows our design intuition

123
00:05:02,220 --> 00:05:05,040
so sample a is a member and Sample B is

124
00:05:05,040 --> 00:05:07,620
a null member so the bottom model is now

125
00:05:07,620 --> 00:05:10,560
trained with sample A and B so the query

126
00:05:10,560 --> 00:05:13,500
of A and B on the bottom model should

127
00:05:13,500 --> 00:05:15,479
both follow the distribution of null

128
00:05:15,479 --> 00:05:17,040
members

129
00:05:17,040 --> 00:05:20,580
specifically let's use ux to denote some

130
00:05:20,580 --> 00:05:22,320
models which are now trained with this

131
00:05:22,320 --> 00:05:25,380
member sample X if the curry sample is a

132
00:05:25,380 --> 00:05:27,300
member we will average your production

133
00:05:27,300 --> 00:05:29,820
vectors on a member sample X by only

134
00:05:29,820 --> 00:05:32,280
using L sub models which are not trained

135
00:05:32,280 --> 00:05:34,979
with this member sample so here for the

136
00:05:34,979 --> 00:05:37,620
member sample X only the bottom model is

137
00:05:37,620 --> 00:05:39,840
not trained with a so we'll use this

138
00:05:39,840 --> 00:05:41,460
bottom model to get the output

139
00:05:41,460 --> 00:05:42,780
Productions

140
00:05:42,780 --> 00:05:45,600
if the query sample is a non-member we

141
00:05:45,600 --> 00:05:47,460
will randomly select a member sample X

142
00:05:47,460 --> 00:05:49,620
Program and we will average the

143
00:05:49,620 --> 00:05:52,199
production vectors on X by using other

144
00:05:52,199 --> 00:05:54,360
models which are notion with some

145
00:05:54,360 --> 00:05:57,320
non-member sample X a membership

146
00:05:57,320 --> 00:06:00,960
so for the non-member example B suppose

147
00:06:00,960 --> 00:06:03,180
the random select member sample is a

148
00:06:03,180 --> 00:06:05,280
which decides that we should choose the

149
00:06:05,280 --> 00:06:07,500
bottom model so we use this border model

150
00:06:07,500 --> 00:06:09,960
to get the output predictions which also

151
00:06:09,960 --> 00:06:13,020
for the distribution of null members

152
00:06:13,020 --> 00:06:15,240
we analyze the price implications of

153
00:06:15,240 --> 00:06:17,940
Solitaire eye so if the attacker would

154
00:06:17,940 --> 00:06:20,100
only directly creates the model on the

155
00:06:20,100 --> 00:06:22,259
target sample once we call this attack

156
00:06:22,259 --> 00:06:24,720
as direct single core attack which is

157
00:06:24,720 --> 00:06:27,000
also one major attack of direction of

158
00:06:27,000 --> 00:06:28,919
membership influence attacks we're

159
00:06:28,919 --> 00:06:31,440
proofing on paper that that the attacker

160
00:06:31,440 --> 00:06:33,300
cannot infer the membership of this

161
00:06:33,300 --> 00:06:35,220
sample under the directional career

162
00:06:35,220 --> 00:06:37,860
attack the intuitive explanation here is

163
00:06:37,860 --> 00:06:40,380
that no matter whether the sample is

164
00:06:40,380 --> 00:06:43,199
used to transably I or not the output

165
00:06:43,199 --> 00:06:45,240
distribution of this sample are

166
00:06:45,240 --> 00:06:46,560
identical

167
00:06:46,560 --> 00:06:49,380
however the attack is not limited to

168
00:06:49,380 --> 00:06:51,360
only directly clearing the target sample

169
00:06:51,360 --> 00:06:54,660
so here for the memory sample a instead

170
00:06:54,660 --> 00:06:56,819
of occurring a their character can add

171
00:06:56,819 --> 00:06:59,160
small noise to sample a and turn a into

172
00:06:59,160 --> 00:07:01,620
a prime so let's say I will record a

173
00:07:01,620 --> 00:07:04,259
prime as a noun member and might use the

174
00:07:04,259 --> 00:07:06,120
models which are trained with X Prime

175
00:07:06,120 --> 00:07:09,000
for example might use operator model and

176
00:07:09,000 --> 00:07:11,160
the final Productions will follow the

177
00:07:11,160 --> 00:07:14,039
distribution of members the perturbed

178
00:07:14,039 --> 00:07:16,319
member attack motivates our design of

179
00:07:16,319 --> 00:07:19,440
second component self-disolation

180
00:07:19,440 --> 00:07:22,740
we now already have a model splitter eye

181
00:07:22,740 --> 00:07:24,780
which we have similarly about some

182
00:07:24,780 --> 00:07:26,940
members and non-members from a direct

183
00:07:26,940 --> 00:07:29,400
single query attack perspective and we

184
00:07:29,400 --> 00:07:31,500
want a new model to behave similarly

185
00:07:31,500 --> 00:07:34,680
from this perspective to do this we

186
00:07:34,680 --> 00:07:36,960
consider distillation by using the

187
00:07:36,960 --> 00:07:39,000
training a new model using the soft

188
00:07:39,000 --> 00:07:41,580
labels of the training set as we are

189
00:07:41,580 --> 00:07:43,620
using the exact same training set we

190
00:07:43,620 --> 00:07:46,139
call this process as self-disolation

191
00:07:46,139 --> 00:07:48,539
other distillation process we will

192
00:07:48,539 --> 00:07:51,060
remove splitter eye and stuff labels and

193
00:07:51,060 --> 00:07:53,400
only allow the queries to the final to

194
00:07:53,400 --> 00:07:55,199
this new model

195
00:07:55,199 --> 00:07:57,419
a transferability of neural networks

196
00:07:57,419 --> 00:07:59,520
enabled a new model to have similar good

197
00:07:59,520 --> 00:08:01,919
property as split AI so this new model

198
00:08:01,919 --> 00:08:04,080
will also behave similarly for some

199
00:08:04,080 --> 00:08:06,060
members and now members from a direct

200
00:08:06,060 --> 00:08:08,400
single crochetype perspective moreover

201
00:08:08,400 --> 00:08:10,740
the new model also serves a limitation

202
00:08:10,740 --> 00:08:12,840
of split AI in the particular member

203
00:08:12,840 --> 00:08:13,620
attack

204
00:08:13,620 --> 00:08:16,560
in a distillation process we will only

205
00:08:16,560 --> 00:08:18,780
query each exact training sample once

206
00:08:18,780 --> 00:08:21,479
and the tracker only has access can only

207
00:08:21,479 --> 00:08:23,520
include the final project model and

208
00:08:23,520 --> 00:08:25,319
cannot interact with the distillation

209
00:08:25,319 --> 00:08:26,340
process

210
00:08:26,340 --> 00:08:28,440
and that's all for these our design

211
00:08:28,440 --> 00:08:32,039
defenses and for evaluation we consider

212
00:08:32,039 --> 00:08:34,140
three data sets which are also widely

213
00:08:34,140 --> 00:08:36,000
used in previous membership privacy

214
00:08:36,000 --> 00:08:38,099
research and here is those statistics

215
00:08:38,099 --> 00:08:40,039
for training samples and also more

216
00:08:40,039 --> 00:08:42,419
architecture we also investigate

217
00:08:42,419 --> 00:08:43,799
different mode architectures for the

218
00:08:43,799 --> 00:08:46,560
ablation story in the following slides I

219
00:08:46,560 --> 00:08:48,720
will focus on purchase 100 with this

220
00:08:48,720 --> 00:08:50,399
default study

221
00:08:50,399 --> 00:08:52,740
We compare our differences with two

222
00:08:52,740 --> 00:08:54,180
state of our membership infrastructure

223
00:08:54,180 --> 00:08:55,980
defenses including adversary

224
00:08:55,980 --> 00:08:58,860
Equalization and mem card we also

225
00:08:58,860 --> 00:09:01,560
include undefended model as a Baseline

226
00:09:01,560 --> 00:09:04,080
for the evaluation metric we consider

227
00:09:04,080 --> 00:09:06,480
the utility price trade-off so we

228
00:09:06,480 --> 00:09:08,279
consider the classification accuracy for

229
00:09:08,279 --> 00:09:11,459
utility and for privacy analysis we

230
00:09:11,459 --> 00:09:12,899
consider the membership influence

231
00:09:12,899 --> 00:09:15,300
average attack accuracy where the

232
00:09:15,300 --> 00:09:17,279
Baseline in random gas equal to 50

233
00:09:17,279 --> 00:09:18,779
percent

234
00:09:18,779 --> 00:09:21,120
so here is the results the defense

235
00:09:21,120 --> 00:09:23,540
method include undefined model mem card

236
00:09:23,540 --> 00:09:26,459
advocation and art defense and we have

237
00:09:26,459 --> 00:09:28,980
an accuracy and tested the higher the

238
00:09:28,980 --> 00:09:31,380
better and also the best attack accuracy

239
00:09:31,380 --> 00:09:34,500
the lower the better compare our defense

240
00:09:34,500 --> 00:09:36,899
to our different model mm guards are

241
00:09:36,899 --> 00:09:39,600
defense only increase 3.9 classification

242
00:09:39,600 --> 00:09:41,940
access drop but the membership inference

243
00:09:41,940 --> 00:09:44,880
attacks is much more reduced our defense

244
00:09:44,880 --> 00:09:47,940
is 13 lower than our different model and

245
00:09:47,940 --> 00:09:51,180
11.5 percent lower than memguard

246
00:09:51,180 --> 00:09:53,459
compare all defense to the adversary

247
00:09:53,459 --> 00:09:55,980
localization we can say that all

248
00:09:55,980 --> 00:09:57,839
different countries both higher

249
00:09:57,839 --> 00:10:00,240
classification accuracy and also lower

250
00:10:00,240 --> 00:10:03,120
best membership attack accuracy

251
00:10:03,120 --> 00:10:05,160
for more experimental results please

252
00:10:05,160 --> 00:10:07,200
check our paper

253
00:10:07,200 --> 00:10:10,019
in this work we propose a new membership

254
00:10:10,019 --> 00:10:11,720
interest differences called certain

255
00:10:11,720 --> 00:10:14,100
consists of splitter eye and

256
00:10:14,100 --> 00:10:17,160
self-sellation all defense lies in a

257
00:10:17,160 --> 00:10:19,200
categorical membership defenses or

258
00:10:19,200 --> 00:10:21,480
improved membership privacy and high

259
00:10:21,480 --> 00:10:24,300
utility and as shown by the rad plot

260
00:10:24,300 --> 00:10:26,399
where the x-axis is classification

261
00:10:26,399 --> 00:10:29,459
accuracy the the writer the better and

262
00:10:29,459 --> 00:10:31,200
also the y-axis is membership attack

263
00:10:31,200 --> 00:10:33,600
accuracy the lower the battery our

264
00:10:33,600 --> 00:10:36,300
defense can achieve a better utility

265
00:10:36,300 --> 00:10:38,700
membership privacy trade-off compared to

266
00:10:38,700 --> 00:10:41,279
previous defenses we hope our work can

267
00:10:41,279 --> 00:10:43,680
provide insights on membership privacy

268
00:10:43,680 --> 00:10:46,680
either on how to improve the utility for

269
00:10:46,680 --> 00:10:49,500
the DP based approach or how to provide

270
00:10:49,500 --> 00:10:50,940
more probable guarantee for all

271
00:10:50,940 --> 00:10:54,000
differences without losing utility also

272
00:10:54,000 --> 00:10:56,040
feel free to check the artifact on our

273
00:10:56,040 --> 00:10:58,500
GitHub page thank you now I'm ready for

274
00:10:58,500 --> 00:11:00,680
questions

