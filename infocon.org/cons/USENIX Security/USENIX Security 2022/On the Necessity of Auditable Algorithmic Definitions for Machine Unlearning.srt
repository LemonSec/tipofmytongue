1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,240 --> 00:00:11,160
so I'm presenting on the necessity of

3
00:00:11,160 --> 00:00:12,960
audible algorithmic definitions for

4
00:00:12,960 --> 00:00:14,400
machine learning joint work with me

5
00:00:14,400 --> 00:00:16,379
Henry yah Ilya shimelove and Nicholas

6
00:00:16,379 --> 00:00:18,720
papner at the University of Toronto and

7
00:00:18,720 --> 00:00:21,439
the bachelor Institute

8
00:00:21,439 --> 00:00:24,000
a quick outline of the talk I'll first

9
00:00:24,000 --> 00:00:25,560
give a brief background on a learning

10
00:00:25,560 --> 00:00:27,840
what is it why do we care in a high

11
00:00:27,840 --> 00:00:30,000
level taxonomy then we'll kind of move

12
00:00:30,000 --> 00:00:31,320
to the question of what it might mean to

13
00:00:31,320 --> 00:00:33,120
verify a learning and really the

14
00:00:33,120 --> 00:00:35,399
question of if given a model what's the

15
00:00:35,399 --> 00:00:36,780
plausible data it could have trained on

16
00:00:36,780 --> 00:00:38,700
and going from that I'll present some

17
00:00:38,700 --> 00:00:41,460
impossibly results on verification

18
00:00:41,460 --> 00:00:43,440
so yeah without further Ado a quick

19
00:00:43,440 --> 00:00:46,020
background on alerting

20
00:00:46,020 --> 00:00:47,340
the main question you might be having

21
00:00:47,340 --> 00:00:49,140
right now is what is unlearning and why

22
00:00:49,140 --> 00:00:51,539
do I care and it's essentially the

23
00:00:51,539 --> 00:00:53,879
observation that in many scenarios I

24
00:00:53,879 --> 00:00:55,440
would like to have not trained on some

25
00:00:55,440 --> 00:00:57,000
of the data I trained on I would like to

26
00:00:57,000 --> 00:00:59,340
forget that data it could be for privacy

27
00:00:59,340 --> 00:01:01,079
reasons as popularized by the right to

28
00:01:01,079 --> 00:01:03,539
be forgotten in the EU gdpr it could be

29
00:01:03,539 --> 00:01:05,040
for security reasons maybe I trained on

30
00:01:05,040 --> 00:01:06,659
some data I realized that some of it was

31
00:01:06,659 --> 00:01:08,640
poisoned I'd now like to forget said

32
00:01:08,640 --> 00:01:10,500
data and it could be for performance

33
00:01:10,500 --> 00:01:12,240
reasons right so maybe some of my data

34
00:01:12,240 --> 00:01:13,860
is detrimental and I'd also like to

35
00:01:13,860 --> 00:01:16,020
forget that data

36
00:01:16,020 --> 00:01:18,000
but really the high level scenario is

37
00:01:18,000 --> 00:01:19,500
that I've trained on some data

38
00:01:19,500 --> 00:01:21,720
containing a particular data point and

39
00:01:21,720 --> 00:01:23,580
obtained some set of models

40
00:01:23,580 --> 00:01:25,619
but what I had would have rather done is

41
00:01:25,619 --> 00:01:27,479
not train on that data point and attain

42
00:01:27,479 --> 00:01:29,280
a different set of models

43
00:01:29,280 --> 00:01:30,780
so what I want to do is some

44
00:01:30,780 --> 00:01:32,220
post-processing that we'll call

45
00:01:32,220 --> 00:01:34,560
unlearning and hopefully the output of

46
00:01:34,560 --> 00:01:36,780
unlearning it resembles what I would

47
00:01:36,780 --> 00:01:38,280
have gotten if I didn't train on that

48
00:01:38,280 --> 00:01:39,659
data

49
00:01:39,659 --> 00:01:41,520
but really this high level picture kind

50
00:01:41,520 --> 00:01:43,079
of skims an important detail which is

51
00:01:43,079 --> 00:01:44,820
what is a model how do we represent

52
00:01:44,820 --> 00:01:47,159
models and we could think of them either

53
00:01:47,159 --> 00:01:49,560
as a particular weight or distribution

54
00:01:49,560 --> 00:01:51,840
of Weights if training is random or a

55
00:01:51,840 --> 00:01:53,399
particular function or distribution of

56
00:01:53,399 --> 00:01:54,960
functions and the thing to keep in mind

57
00:01:54,960 --> 00:01:56,820
is that weights are not functions if you

58
00:01:56,820 --> 00:01:58,020
have the same weights you have the same

59
00:01:58,020 --> 00:01:59,399
function by having the same function

60
00:01:59,399 --> 00:02:02,040
doesn't mean you have the same weights

61
00:02:02,040 --> 00:02:04,500
okay with some agreed upon notion of

62
00:02:04,500 --> 00:02:06,899
what a model is the question is

63
00:02:06,899 --> 00:02:08,520
how should the output of unlearning

64
00:02:08,520 --> 00:02:10,679
relate to retraining

65
00:02:10,679 --> 00:02:12,360
and so the first answer is that the

66
00:02:12,360 --> 00:02:13,739
output of unlearning should be exactly

67
00:02:13,739 --> 00:02:16,260
the same as retraining but in practice

68
00:02:16,260 --> 00:02:18,060
this is very expensive and the only

69
00:02:18,060 --> 00:02:19,980
known methods are variations of

70
00:02:19,980 --> 00:02:21,420
retraining

71
00:02:21,420 --> 00:02:24,120
so kind of trying to move away from the

72
00:02:24,120 --> 00:02:25,860
cost of retraining

73
00:02:25,860 --> 00:02:28,500
we might ask instead of exactly reducing

74
00:02:28,500 --> 00:02:31,020
the models we do so with some error in

75
00:02:31,020 --> 00:02:32,760
some predefined Metric

76
00:02:32,760 --> 00:02:34,920
and I'd recommend one of my other papers

77
00:02:34,920 --> 00:02:36,599
for a more detailed discussion about

78
00:02:36,599 --> 00:02:40,560
different unlearning metrics

79
00:02:40,560 --> 00:02:42,420
okay now with an idea of the main

80
00:02:42,420 --> 00:02:44,099
questions that we have when implementing

81
00:02:44,099 --> 00:02:46,379
and learning we can kind of ask what it

82
00:02:46,379 --> 00:02:48,959
might mean to verify a learning and in

83
00:02:48,959 --> 00:02:50,280
this paper we specifically talk about

84
00:02:50,280 --> 00:02:52,920
Exacta learning which as I discussed is

85
00:02:52,920 --> 00:02:54,480
essentially retraining

86
00:02:54,480 --> 00:02:56,640
and the kind of question or kind of

87
00:02:56,640 --> 00:02:58,080
framework we have in mind is that we

88
00:02:58,080 --> 00:03:00,180
have a Model A really a proof Associated

89
00:03:00,180 --> 00:03:01,379
to a model

90
00:03:01,379 --> 00:03:04,200
and there's some verifier that could

91
00:03:04,200 --> 00:03:06,000
either could either determine if we had

92
00:03:06,000 --> 00:03:08,040
trained on a data on a data point or had

93
00:03:08,040 --> 00:03:09,540
not trained on a data point so really

94
00:03:09,540 --> 00:03:11,159
we're kind of asking could this verifier

95
00:03:11,159 --> 00:03:13,019
exist

96
00:03:13,019 --> 00:03:14,519
and sort of the framework for

97
00:03:14,519 --> 00:03:16,080
verification that we'll think about is

98
00:03:16,080 --> 00:03:17,940
plausibility we'd like to know if we

99
00:03:17,940 --> 00:03:19,440
could have plausibly trained on a data

100
00:03:19,440 --> 00:03:20,159
point

101
00:03:20,159 --> 00:03:22,140
but how should we Define plausibility

102
00:03:22,140 --> 00:03:24,300
and if you know if all the operations

103
00:03:24,300 --> 00:03:26,280
were deterministic a good if and only if

104
00:03:26,280 --> 00:03:28,920
condition is that you know associate to

105
00:03:28,920 --> 00:03:31,739
each final model WT I actually have a

106
00:03:31,739 --> 00:03:33,180
sequence of checkpoints and a sequence

107
00:03:33,180 --> 00:03:35,580
of data points that would lead to this

108
00:03:35,580 --> 00:03:37,680
final model and so I can check what data

109
00:03:37,680 --> 00:03:38,760
points I used

110
00:03:38,760 --> 00:03:40,620
but in practice you know some operations

111
00:03:40,620 --> 00:03:42,780
are not deterministic and so you would

112
00:03:42,780 --> 00:03:44,459
introduce some threshold and this was

113
00:03:44,459 --> 00:03:46,560
proposed in past work and was called a

114
00:03:46,560 --> 00:03:48,239
proof of learning it was originally from

115
00:03:48,239 --> 00:03:49,860
model stealing but we sort of repurposed

116
00:03:49,860 --> 00:03:52,620
it for plausibility

117
00:03:52,620 --> 00:03:54,299
okay with this framework for

118
00:03:54,299 --> 00:03:56,640
verification what would verifying and

119
00:03:56,640 --> 00:03:59,159
learning look like and essentially an

120
00:03:59,159 --> 00:04:02,040
entity would submit a PLL and the verify

121
00:04:02,040 --> 00:04:03,360
would first check the validity of the

122
00:04:03,360 --> 00:04:05,580
Pol and then check what data points were

123
00:04:05,580 --> 00:04:07,319
used and if a particular data point

124
00:04:07,319 --> 00:04:10,080
wasn't used we'd say the NCT had learned

125
00:04:10,080 --> 00:04:12,120
that data point but if it was used then

126
00:04:12,120 --> 00:04:14,819
you know it fails on learning

127
00:04:14,819 --> 00:04:17,040
but really a key assumption in this like

128
00:04:17,040 --> 00:04:19,440
plausibility model is that plausible

129
00:04:19,440 --> 00:04:21,298
without a point means never training on

130
00:04:21,298 --> 00:04:23,759
it and that's something we can attack so

131
00:04:23,759 --> 00:04:26,220
we have some problems

132
00:04:26,220 --> 00:04:28,440
so kind of imagine I trained on a data

133
00:04:28,440 --> 00:04:30,840
set and I obtained a valid pul but

134
00:04:30,840 --> 00:04:33,479
before I submit it to a verifier I apply

135
00:04:33,479 --> 00:04:36,360
a forging map which really keeps all the

136
00:04:36,360 --> 00:04:38,400
checkpoints the same but changes the

137
00:04:38,400 --> 00:04:39,900
data points to be those in a different

138
00:04:39,900 --> 00:04:42,720
data set now let's just assume that this

139
00:04:42,720 --> 00:04:43,919
data set is disjoint to the original

140
00:04:43,919 --> 00:04:46,500
data set so if I was to submit this Pol

141
00:04:46,500 --> 00:04:48,780
for any of the original points I trained

142
00:04:48,780 --> 00:04:50,520
on the verifier would conclude I did not

143
00:04:50,520 --> 00:04:52,860
train on that data point

144
00:04:52,860 --> 00:04:55,860
so Clara this is kind of spoofing on

145
00:04:55,860 --> 00:04:57,479
learning

146
00:04:57,479 --> 00:04:58,860
so okay kind of the question is well

147
00:04:58,860 --> 00:05:00,600
when could forging exist

148
00:05:00,600 --> 00:05:02,520
and some high-level ideas you might have

149
00:05:02,520 --> 00:05:05,520
is that well maybe D and D prime or just

150
00:05:05,520 --> 00:05:07,500
have similar data points which is that

151
00:05:07,500 --> 00:05:09,419
you know for every data point in D I

152
00:05:09,419 --> 00:05:11,400
have a data point D Prime that's similar

153
00:05:11,400 --> 00:05:12,960
enough that it produces similar

154
00:05:12,960 --> 00:05:15,000
gradients or you might think well what

155
00:05:15,000 --> 00:05:17,160
if D Prime was just very big and it kind

156
00:05:17,160 --> 00:05:18,660
of produces any gradient I would ever

157
00:05:18,660 --> 00:05:21,780
want so some sort of density argument

158
00:05:21,780 --> 00:05:23,400
but what we kind of think about is a

159
00:05:23,400 --> 00:05:25,620
more probabilistic notion of Fortune

160
00:05:25,620 --> 00:05:27,900
which is maybe our training data has

161
00:05:27,900 --> 00:05:29,820
some underlying distribution and we can

162
00:05:29,820 --> 00:05:31,740
kind of prove something with this

163
00:05:31,740 --> 00:05:32,880
assumption

164
00:05:32,880 --> 00:05:35,759
which is what we prove is that if D is

165
00:05:35,759 --> 00:05:37,199
not a sampled from some underlying

166
00:05:37,199 --> 00:05:38,940
distribution and D Prime is also sampled

167
00:05:38,940 --> 00:05:41,280
from that distribution forging can exist

168
00:05:41,280 --> 00:05:43,440
and this is under some mild assumptions

169
00:05:43,440 --> 00:05:45,360
we'll assume that the update rule has

170
00:05:45,360 --> 00:05:47,180
well-defined mean and standard deviation

171
00:05:47,180 --> 00:05:49,919
that the mini batch size we're using is

172
00:05:49,919 --> 00:05:51,720
unconstrained so we'll prove it for some

173
00:05:51,720 --> 00:05:54,120
large mini batch and that this we're

174
00:05:54,120 --> 00:05:56,160
working over RN and our distribution is

175
00:05:56,160 --> 00:05:57,600
absolutely continuous so it's kind of

176
00:05:57,600 --> 00:05:58,979
nice in a sense

177
00:05:58,979 --> 00:06:01,080
and the proof strategy is really to just

178
00:06:01,080 --> 00:06:02,820
increase this batch size so that our

179
00:06:02,820 --> 00:06:04,740
updates are approximating the mean from

180
00:06:04,740 --> 00:06:06,780
the underlying distribution then using

181
00:06:06,780 --> 00:06:08,940
some concentration inequality and then

182
00:06:08,940 --> 00:06:10,380
hopefully we can prove by non-zero

183
00:06:10,380 --> 00:06:12,600
probability obviously this is kind of

184
00:06:12,600 --> 00:06:14,160
brushing aside working with the

185
00:06:14,160 --> 00:06:15,780
framework and Computing everything but

186
00:06:15,780 --> 00:06:19,080
that's the high level strategy

187
00:06:19,080 --> 00:06:21,060
but the nice thing of this probabilistic

188
00:06:21,060 --> 00:06:22,740
notion is it kind of gives an intuitive

189
00:06:22,740 --> 00:06:24,780
way to instantiate forging because what

190
00:06:24,780 --> 00:06:26,639
we're saying is that some batch works so

191
00:06:26,639 --> 00:06:28,199
if I kept checking batches maybe I'll

192
00:06:28,199 --> 00:06:29,580
find that batch

193
00:06:29,580 --> 00:06:31,860
and so kind of to illustrate this in

194
00:06:31,860 --> 00:06:34,199
this paper what we consider is we have a

195
00:06:34,199 --> 00:06:36,360
particular data point to unlearn I will

196
00:06:36,360 --> 00:06:39,180
take as our forging data set some subset

197
00:06:39,180 --> 00:06:41,460
of D remove the point you'll learn

198
00:06:41,460 --> 00:06:42,840
and what we'll do is when given an

199
00:06:42,840 --> 00:06:44,520
update to forge you know an update that

200
00:06:44,520 --> 00:06:46,919
used the data point to alert will kind

201
00:06:46,919 --> 00:06:48,419
of search through some random batches of

202
00:06:48,419 --> 00:06:50,340
D Prime and hopefully one of them will

203
00:06:50,340 --> 00:06:51,660
be good enough

204
00:06:51,660 --> 00:06:53,880
and this kind of approach is analogous

205
00:06:53,880 --> 00:06:55,979
to manipulating SGD with data ordering

206
00:06:55,979 --> 00:06:58,380
tax where they also consider brute

207
00:06:58,380 --> 00:06:59,819
forcing through batches to do

208
00:06:59,819 --> 00:07:01,560
adversarial things but different

209
00:07:01,560 --> 00:07:03,479
adversarial goals

210
00:07:03,479 --> 00:07:04,979
so kind of an illustration of the

211
00:07:04,979 --> 00:07:07,020
results in this figure we plotted a

212
00:07:07,020 --> 00:07:08,100
hundred different trials so 100

213
00:07:08,100 --> 00:07:10,680
different points to alert each case we

214
00:07:10,680 --> 00:07:12,120
took our D Prime as some random

215
00:07:12,120 --> 00:07:13,860
subsemble of D remove the point to

216
00:07:13,860 --> 00:07:16,139
unlearn and then for a given step to

217
00:07:16,139 --> 00:07:17,940
forge so a step that used the point to

218
00:07:17,940 --> 00:07:19,800
unlearn uh we searched through some

219
00:07:19,800 --> 00:07:21,599
random batches of D Prime and picked the

220
00:07:21,599 --> 00:07:22,560
best one

221
00:07:22,560 --> 00:07:24,300
and as you see as we increase the batch

222
00:07:24,300 --> 00:07:26,039
size the error gets small

223
00:07:26,039 --> 00:07:28,259
so like kind of as if it would keep

224
00:07:28,259 --> 00:07:29,580
getting smaller if we kept increasing

225
00:07:29,580 --> 00:07:31,319
the batch size

226
00:07:31,319 --> 00:07:33,900
and we kind of we used L2 squared for

227
00:07:33,900 --> 00:07:35,160
error here

228
00:07:35,160 --> 00:07:37,259
the paper has other figures describing

229
00:07:37,259 --> 00:07:39,120
other variables and if you're interested

230
00:07:39,120 --> 00:07:41,400
I'd recommend looking at the evaluation

231
00:07:41,400 --> 00:07:43,919
section of the paper

232
00:07:43,919 --> 00:07:45,780
okay what are really the conclusions

233
00:07:45,780 --> 00:07:47,880
kind of the insights we've gained by

234
00:07:47,880 --> 00:07:49,919
thinking about plausibility for learning

235
00:07:49,919 --> 00:07:52,080
the first thing is that being alert is

236
00:07:52,080 --> 00:07:54,060
not always a well-defined property of a

237
00:07:54,060 --> 00:07:56,340
model or a distribution of models

238
00:07:56,340 --> 00:07:57,720
which is to really say that if I was

239
00:07:57,720 --> 00:07:59,520
given a model or distribution models

240
00:07:59,520 --> 00:08:01,380
it's not always clear about some kind of

241
00:08:01,380 --> 00:08:03,660
requirements of the setting what data

242
00:08:03,660 --> 00:08:05,819
points were used to train that model

243
00:08:05,819 --> 00:08:07,680
and kind of you know building on that

244
00:08:07,680 --> 00:08:09,240
verifying retraining specifically

245
00:08:09,240 --> 00:08:12,120
requires algorithmic considerations we

246
00:08:12,120 --> 00:08:13,440
have to be very specific about how

247
00:08:13,440 --> 00:08:14,819
training is done

248
00:08:14,819 --> 00:08:17,340
and the general setup that we the data

249
00:08:17,340 --> 00:08:19,800
is set up that we can use

250
00:08:19,800 --> 00:08:21,599
so okay with the bulk of the paper

251
00:08:21,599 --> 00:08:23,819
discussed what are some sort of open

252
00:08:23,819 --> 00:08:26,340
questions left by work kind of as the

253
00:08:26,340 --> 00:08:28,500
second Point previously was touching on

254
00:08:28,500 --> 00:08:30,060
what are the sort of the constraints

255
00:08:30,060 --> 00:08:33,360
many impose on the training to kind of

256
00:08:33,360 --> 00:08:35,399
rule out forging if you think about how

257
00:08:35,399 --> 00:08:36,899
I proved it like how we proved it in

258
00:08:36,899 --> 00:08:39,000
this paper there's some absurdities

259
00:08:39,000 --> 00:08:41,399
right maybe the batch size is too big to

260
00:08:41,399 --> 00:08:43,620
be realistic and so maybe imposing that

261
00:08:43,620 --> 00:08:45,660
can remove forging obviously we just

262
00:08:45,660 --> 00:08:47,040
proved an impossibility result we didn't

263
00:08:47,040 --> 00:08:48,660
say what you need to do to rule up for a

264
00:08:48,660 --> 00:08:51,240
drink so that's kind of open

265
00:08:51,240 --> 00:08:52,500
um another question is sort of building

266
00:08:52,500 --> 00:08:54,480
on this forging framework because what

267
00:08:54,480 --> 00:08:56,820
we've done is sort of related data sets

268
00:08:56,820 --> 00:08:58,800
in terms of what models they produce and

269
00:08:58,800 --> 00:09:00,300
this is some sort of generalization

270
00:09:00,300 --> 00:09:02,820
which is a key question a lot of ml

271
00:09:02,820 --> 00:09:04,920
Theory and so the third part which is

272
00:09:04,920 --> 00:09:06,180
maybe what a lot of people have on their

273
00:09:06,180 --> 00:09:07,860
mind right now is sort of that privacy

274
00:09:07,860 --> 00:09:09,480
implications of foraging because what

275
00:09:09,480 --> 00:09:10,800
we've sort of showed is that it's

276
00:09:10,800 --> 00:09:13,140
difficult to do membership inference in

277
00:09:13,140 --> 00:09:15,000
sub settings but membership this is not

278
00:09:15,000 --> 00:09:16,680
the usual membership inference we're not

279
00:09:16,680 --> 00:09:17,940
working with models we're working with

280
00:09:17,940 --> 00:09:20,279
proofs of plausibility

281
00:09:20,279 --> 00:09:24,500
but yeah that's my talk

