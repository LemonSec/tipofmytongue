1
00:00:08,000 --> 00:00:11,700
hello everyone my name is yemo I'm glad

2
00:00:11,700 --> 00:00:13,679
to have the opportunity to present our

3
00:00:13,679 --> 00:00:16,619
work the title is your microphone array

4
00:00:16,619 --> 00:00:19,619
returns your identity but robust voice

5
00:00:19,619 --> 00:00:21,900
love is detection system for smart

6
00:00:21,900 --> 00:00:25,140
speakers this is a joint work done by

7
00:00:25,140 --> 00:00:27,779
Ian children Professor Zhu from South

8
00:00:27,779 --> 00:00:30,539
hydrogen University Matt Arjun love

9
00:00:30,539 --> 00:00:32,820
hustle and Professor team from the

10
00:00:32,820 --> 00:00:35,340
University of Virginia here is the

11
00:00:35,340 --> 00:00:37,739
outline let's begin with the background

12
00:00:37,739 --> 00:00:40,739
the smart home becomes a populist

13
00:00:40,739 --> 00:00:43,680
Lifestyle the global revenue of smart

14
00:00:43,680 --> 00:00:46,860
home is expected to reach about 100

15
00:00:46,860 --> 00:00:51,180
billion dollars in 2026 in smart home

16
00:00:51,180 --> 00:00:53,640
voice interface is the main user

17
00:00:53,640 --> 00:00:56,219
interface the user can use the voice

18
00:00:56,219 --> 00:00:58,620
interface to remotely control Smart

19
00:00:58,620 --> 00:01:01,760
devices or query useful information

20
00:01:01,760 --> 00:01:04,979
currently smart speakers such as Echo

21
00:01:04,979 --> 00:01:07,619
Google home are the mainstream of the

22
00:01:07,619 --> 00:01:09,479
voice interface

23
00:01:09,479 --> 00:01:12,780
however the word interface faces series

24
00:01:12,780 --> 00:01:15,600
security threads due to the broadcast

25
00:01:15,600 --> 00:01:19,200
nature of voice The Voice interface is

26
00:01:19,200 --> 00:01:22,500
vulnerable to the voice moving attack in

27
00:01:22,500 --> 00:01:24,780
replay attack the attacker can record

28
00:01:24,780 --> 00:01:27,780
the weak team's voice samples and play

29
00:01:27,780 --> 00:01:30,240
the pre-connected ways to move the voice

30
00:01:30,240 --> 00:01:33,600
interface besides researchers point out

31
00:01:33,600 --> 00:01:36,479
advisors moving attackers including

32
00:01:36,479 --> 00:01:39,360
hidden voice and inaudible attack is

33
00:01:39,360 --> 00:01:41,700
important to defend against the world's

34
00:01:41,700 --> 00:01:44,340
moving long detection is a typical

35
00:01:44,340 --> 00:01:46,680
solution it's based on the basic

36
00:01:46,680 --> 00:01:49,079
observation the authentic voice and

37
00:01:49,079 --> 00:01:51,299
spooking words are generated by real

38
00:01:51,299 --> 00:01:54,320
human and electrical device respectively

39
00:01:54,320 --> 00:01:57,299
therefore if we undetect the factors

40
00:01:57,299 --> 00:01:59,880
related to The Human Action we can

41
00:01:59,880 --> 00:02:01,860
determine whether the audio input is

42
00:02:01,860 --> 00:02:04,860
authentic current longest detection can

43
00:02:04,860 --> 00:02:07,560
be divided into two types two Factor

44
00:02:07,560 --> 00:02:09,419
authentication and positive longest

45
00:02:09,419 --> 00:02:12,000
energy the first one two-factor

46
00:02:12,000 --> 00:02:14,040
authentication leverages the image

47
00:02:14,040 --> 00:02:16,739
acceleration of assault signals to

48
00:02:16,739 --> 00:02:19,020
capture the user's movement during the

49
00:02:19,020 --> 00:02:21,300
voice generation however two-fact

50
00:02:21,300 --> 00:02:23,520
authentication requires the user to

51
00:02:23,520 --> 00:02:25,260
deploy additional sensitive devices

52
00:02:25,260 --> 00:02:28,379
which increase users burden and privacy

53
00:02:28,379 --> 00:02:31,680
risks to overcome the limitations of

54
00:02:31,680 --> 00:02:33,959
two-factor authentication researchers

55
00:02:33,959 --> 00:02:36,080
propose positive language detection

56
00:02:36,080 --> 00:02:38,760
Possible detections only use the

57
00:02:38,760 --> 00:02:41,400
collected word samples to conduct the

58
00:02:41,400 --> 00:02:44,220
language detention this figure shows the

59
00:02:44,220 --> 00:02:47,220
basic Insight we can see the authentic

60
00:02:47,220 --> 00:02:49,379
voice is generated by human Mouse

61
00:02:49,379 --> 00:02:51,660
machine and thus moving voice is

62
00:02:51,660 --> 00:02:53,760
generated by the vibration of the

63
00:02:53,760 --> 00:02:55,200
electrical device

64
00:02:55,200 --> 00:02:58,319
these two types of voice generation can

65
00:02:58,319 --> 00:03:00,599
be regarded as two types of signal

66
00:03:00,599 --> 00:03:02,819
modulation which will cause the

67
00:03:02,819 --> 00:03:05,819
difference of central grams recent

68
00:03:05,819 --> 00:03:08,879
studies propose some useful features so

69
00:03:08,879 --> 00:03:11,340
example the spectral gravity feature

70
00:03:11,340 --> 00:03:14,159
from the model Channel and the field

71
00:03:14,159 --> 00:03:16,800
print from the two channel audio

72
00:03:16,800 --> 00:03:19,140
however possible belongings detection

73
00:03:19,140 --> 00:03:21,720
still has some limitations for the

74
00:03:21,720 --> 00:03:24,000
scheme based on the model channel the

75
00:03:24,000 --> 00:03:26,099
unstable spectrogram called the

76
00:03:26,099 --> 00:03:28,860
performance unstable as shown in the

77
00:03:28,860 --> 00:03:31,080
left figure the spectrograms between

78
00:03:31,080 --> 00:03:33,180
authentic and spoofing articles in

79
00:03:33,180 --> 00:03:35,120
different locations are quite different

80
00:03:35,120 --> 00:03:38,040
for the new channel-based solution to

81
00:03:38,040 --> 00:03:40,140
obtain a robust lung institutional

82
00:03:40,140 --> 00:03:42,840
feature it requires the user to follow

83
00:03:42,840 --> 00:03:47,120
the microphone pairs in a fixed gesture

84
00:03:51,200 --> 00:03:54,060
this is the goal of our work let's

85
00:03:54,060 --> 00:03:56,640
produce the motivating of our solution

86
00:03:56,640 --> 00:03:59,580
we noticed that to improve the quality

87
00:03:59,580 --> 00:04:01,920
of the collected audio the smart

88
00:04:01,920 --> 00:04:04,739
speakers deploy a microphone array which

89
00:04:04,739 --> 00:04:07,379
contain multiple macphones as shown in

90
00:04:07,379 --> 00:04:09,420
this figure compared with the

91
00:04:09,420 --> 00:04:11,700
traditional megaphone and smartphones

92
00:04:11,700 --> 00:04:14,580
current smart speakers such as Echo and

93
00:04:14,580 --> 00:04:16,978
Google home can collect the audio with

94
00:04:16,978 --> 00:04:20,040
six or even 8 channels such multiple

95
00:04:20,040 --> 00:04:22,500
channel can improve the diversity of the

96
00:04:22,500 --> 00:04:25,080
audio and can better characterizes the

97
00:04:25,080 --> 00:04:27,720
audio's identity therefore based on the

98
00:04:27,720 --> 00:04:30,120
multiple Channel audio we propose a

99
00:04:30,120 --> 00:04:32,520
normal language detecting feature named

100
00:04:32,520 --> 00:04:35,160
array fingerprint the array fingerprint

101
00:04:35,160 --> 00:04:37,100
calculates the difference between

102
00:04:37,100 --> 00:04:40,259
spectrograms from different channels a

103
00:04:40,259 --> 00:04:42,600
red fingerprint has a better robustities

104
00:04:42,600 --> 00:04:46,259
we use a case study to demonstrate it as

105
00:04:46,259 --> 00:04:48,540
shown in the left figure will rotate the

106
00:04:48,540 --> 00:04:51,600
microphone array with 19 and 118 degrees

107
00:04:51,600 --> 00:04:54,360
we ask other volunteers to speak the

108
00:04:54,360 --> 00:04:57,360
voice command at each case note that the

109
00:04:57,360 --> 00:04:59,639
blue and red microphones are used to

110
00:04:59,639 --> 00:05:01,919
calculate the two kind of features we

111
00:05:01,919 --> 00:05:03,720
can see that the two channel features

112
00:05:03,720 --> 00:05:06,660
are unstable however if we utilize the

113
00:05:06,660 --> 00:05:09,180
red fingerprint we can see that there is

114
00:05:09,180 --> 00:05:11,880
a robust feature it will be rotate the

115
00:05:11,880 --> 00:05:14,460
maximum array we also conduct a small

116
00:05:14,460 --> 00:05:17,220
experiment to validate the effectiveness

117
00:05:17,220 --> 00:05:19,919
of a red fingerprint we ask the

118
00:05:19,919 --> 00:05:22,680
volunteer to speak a voice command at

119
00:05:22,680 --> 00:05:25,500
two different locations then we utilize

120
00:05:25,500 --> 00:05:28,139
the smartphone and iPad to conduct the

121
00:05:28,139 --> 00:05:31,440
replay attacks at the same locations it

122
00:05:31,440 --> 00:05:33,840
observed that when fingerprints between

123
00:05:33,840 --> 00:05:35,820
different audio Source are quite

124
00:05:35,820 --> 00:05:38,580
different and the feature from the same

125
00:05:38,580 --> 00:05:41,220
audio Source are consistent therefore

126
00:05:41,220 --> 00:05:43,919
our red fingerprint is a robust and

127
00:05:43,919 --> 00:05:46,080
effective feature for the past save

128
00:05:46,080 --> 00:05:48,360
longest detention after introducing the

129
00:05:48,360 --> 00:05:50,880
motivation let's introduce the proposed

130
00:05:50,880 --> 00:05:53,820
lamis detention system the real ID

131
00:05:53,820 --> 00:05:56,220
already is the based on the normal

132
00:05:56,220 --> 00:05:58,979
feature a red fingerprint it has four

133
00:05:58,979 --> 00:06:01,100
modes data collection module

134
00:06:01,100 --> 00:06:03,360
pre-processing module feature extraction

135
00:06:03,360 --> 00:06:06,240
module and attack detection module we

136
00:06:06,240 --> 00:06:08,699
will introduce them one by one in this

137
00:06:08,699 --> 00:06:11,100
study we choose the development box with

138
00:06:11,100 --> 00:06:12,960
microphone array in the commercial smart

139
00:06:12,960 --> 00:06:15,960
speakers as shown in this figure metrics

140
00:06:15,960 --> 00:06:18,360
under group speakers are used that have

141
00:06:18,360 --> 00:06:21,600
8 and 6 microphones respectively then

142
00:06:21,600 --> 00:06:23,280
after obtain the multiple Channel

143
00:06:23,280 --> 00:06:26,039
audience we conducted the pre-processing

144
00:06:26,039 --> 00:06:28,860
firstly we calculated the spectrograms

145
00:06:28,860 --> 00:06:31,199
of each Channel using the short time

146
00:06:31,199 --> 00:06:33,960
Fourier transform this figure shows the

147
00:06:33,960 --> 00:06:36,960
spectrograms of the first third and

148
00:06:36,960 --> 00:06:39,720
fifth macros then we conduct the

149
00:06:39,720 --> 00:06:42,300
Direction detection then with output

150
00:06:42,300 --> 00:06:45,240
extract the feature we first calculate

151
00:06:45,240 --> 00:06:48,120
the array fingerprint for the special

152
00:06:48,120 --> 00:06:51,479
programs we convert them into mesh and

153
00:06:51,479 --> 00:06:53,759
showing the relative rectangle we can

154
00:06:53,759 --> 00:06:56,220
and see that spectrogram difference

155
00:06:56,220 --> 00:06:59,160
between different channels are overviews

156
00:06:59,160 --> 00:07:01,620
finally we calculate the standard

157
00:07:01,620 --> 00:07:04,380
deviations between these meshes and

158
00:07:04,380 --> 00:07:06,900
perform Five Point moving average and

159
00:07:06,900 --> 00:07:09,780
normalization the right bottom figure

160
00:07:09,780 --> 00:07:13,440
shows the calculated fingerprint

161
00:07:13,440 --> 00:07:16,860
here are red fingerprints extracted from

162
00:07:16,860 --> 00:07:19,139
three different commands in both

163
00:07:19,139 --> 00:07:22,199
authentic and smoothing scenarios we can

164
00:07:22,199 --> 00:07:24,000
find that the features difference

165
00:07:24,000 --> 00:07:26,639
between authentic and moving audio is

166
00:07:26,639 --> 00:07:29,639
clear to enhance the performance of a

167
00:07:29,639 --> 00:07:33,300
VIP we also extract more features more

168
00:07:33,300 --> 00:07:35,880
details can be found in our paper the

169
00:07:35,880 --> 00:07:38,460
final module is attack detection module

170
00:07:38,460 --> 00:07:40,800
to achieve the lightweight detection

171
00:07:40,800 --> 00:07:43,560
requirements we choose a lightweight

172
00:07:43,560 --> 00:07:46,380
neural network between the module using

173
00:07:46,380 --> 00:07:49,199
collected audios and their labels then

174
00:07:49,199 --> 00:07:52,199
for the new voice input after extracting

175
00:07:52,199 --> 00:07:54,479
the feature this module can determine

176
00:07:54,479 --> 00:07:57,000
whether the audio is authentic or spoofy

177
00:07:57,000 --> 00:07:59,280
we will introduce the evaluation results

178
00:07:59,280 --> 00:08:01,319
in the following slides

179
00:08:01,319 --> 00:08:03,720
for data collection we employ magic

180
00:08:03,720 --> 00:08:06,120
Creator Android speaker which are shown

181
00:08:06,120 --> 00:08:08,400
in the left figure in this study the

182
00:08:08,400 --> 00:08:10,440
attack replaced the collecting audio by

183
00:08:10,440 --> 00:08:12,900
14 different devices to perform the

184
00:08:12,900 --> 00:08:16,500
worst moving showing in the right figure

185
00:08:16,500 --> 00:08:19,620
we recruit 20 volunteers to speak 20

186
00:08:19,620 --> 00:08:22,500
different voice commands about 10 000

187
00:08:22,500 --> 00:08:25,139
authentic commands and about 20 000

188
00:08:25,139 --> 00:08:27,599
spoofing commands are collected overall

189
00:08:27,599 --> 00:08:30,120
to evaluate the performance of array ID

190
00:08:30,120 --> 00:08:34,159
we define format shifts accuracy far FR

191
00:08:34,159 --> 00:08:38,279
TR and ER for instance the voice command

192
00:08:38,279 --> 00:08:40,860
samples are shown as below we evaluate

193
00:08:40,860 --> 00:08:42,839
every ID and other than this detection

194
00:08:42,839 --> 00:08:45,240
to respond to data sets or generated

195
00:08:45,240 --> 00:08:47,760
belt data set and open source data set

196
00:08:47,760 --> 00:08:49,920
we mask data set the performance of

197
00:08:49,920 --> 00:08:52,980
every ID is proved to be effective we

198
00:08:52,980 --> 00:08:54,839
also measure the time overhead of every

199
00:08:54,839 --> 00:08:57,420
ID on the west assistance we evaluate

200
00:08:57,420 --> 00:08:59,760
various factors for example the distance

201
00:08:59,760 --> 00:09:02,040
the directions the user movement the

202
00:09:02,040 --> 00:09:03,540
environments and the different replay

203
00:09:03,540 --> 00:09:06,720
devices to demonstrate the effectiveness

204
00:09:06,720 --> 00:09:09,120
of area ID according to the map

205
00:09:09,120 --> 00:09:11,880
experimential results array idea is

206
00:09:11,880 --> 00:09:13,740
proved to be useful under varied

207
00:09:13,740 --> 00:09:16,440
conditions for specifics to evaluate the

208
00:09:16,440 --> 00:09:18,660
performance of area ID are totally new

209
00:09:18,660 --> 00:09:21,000
distance will recruit four participants

210
00:09:21,000 --> 00:09:23,760
to attend experiments at three different

211
00:09:23,760 --> 00:09:26,720
locations the performance is about 99

212
00:09:26,720 --> 00:09:30,000
which demonstrates every ID as robust to

213
00:09:30,000 --> 00:09:31,980
the training business to explore the

214
00:09:31,980 --> 00:09:34,860
impact of angles between the user phase

215
00:09:34,860 --> 00:09:37,260
directions and microphone array we test

216
00:09:37,260 --> 00:09:38,760
the performance in four different

217
00:09:38,760 --> 00:09:41,880
directions which is front left right

218
00:09:41,880 --> 00:09:44,459
back every ID achieves the accuracy

219
00:09:44,459 --> 00:09:47,640
about 99 which means every ID is robust

220
00:09:47,640 --> 00:09:49,560
to the change of directions after

221
00:09:49,560 --> 00:09:51,480
showing the evaluation results let's

222
00:09:51,480 --> 00:09:53,459
enter the discussion we add an

223
00:09:53,459 --> 00:09:55,440
experiment to evaluate the impact of

224
00:09:55,440 --> 00:09:57,660
background noise as shown an important

225
00:09:57,660 --> 00:09:59,519
figure to generate the background noise

226
00:09:59,519 --> 00:10:02,100
replace a noise generator to play White

227
00:10:02,100 --> 00:10:04,740
Noise during the data collection we

228
00:10:04,740 --> 00:10:07,320
utilize an advanced sound level meter to

229
00:10:07,320 --> 00:10:09,480
measure the background noise level we're

230
00:10:09,480 --> 00:10:11,940
increasing the noise level from 45 DB to

231
00:10:11,940 --> 00:10:15,480
65 DB the accuracy decreased from 98 to

232
00:10:15,480 --> 00:10:18,360
86 every ID can still work well when the

233
00:10:18,360 --> 00:10:20,940
background noise is less than 50 DB when

234
00:10:20,940 --> 00:10:23,040
there existing strong noise the

235
00:10:23,040 --> 00:10:25,380
performance of area IV degrade sharply

236
00:10:25,380 --> 00:10:27,959
the second issue is the modulated attack

237
00:10:27,959 --> 00:10:30,779
as showing this figure for the advanced

238
00:10:30,779 --> 00:10:32,820
attacker before replacing the

239
00:10:32,820 --> 00:10:35,459
pre-collected audio she can process it

240
00:10:35,459 --> 00:10:38,279
using the inverse filter after that the

241
00:10:38,279 --> 00:10:40,560
spectrogram of the audio in the

242
00:10:40,560 --> 00:10:43,019
modulated attack is like the authentic

243
00:10:43,019 --> 00:10:45,899
wire however we can observe that the

244
00:10:45,899 --> 00:10:48,060
rear fingerprint is still like the

245
00:10:48,060 --> 00:10:50,940
classical replay attack we test a

246
00:10:50,940 --> 00:10:53,519
reality's ability on detecting modulated

247
00:10:53,519 --> 00:10:56,100
attack using three different spoofing

248
00:10:56,100 --> 00:10:59,459
devices so results show that array ID is

249
00:10:59,459 --> 00:11:01,680
effective but the performance is

250
00:11:01,680 --> 00:11:04,200
unstable therefore addressing the model

251
00:11:04,200 --> 00:11:07,500
attack is due an open problem the last

252
00:11:07,500 --> 00:11:10,140
issue is about the training requirements

253
00:11:10,140 --> 00:11:13,500
as showing the attack detection module a

254
00:11:13,500 --> 00:11:16,140
red ID needs training data sets however

255
00:11:16,140 --> 00:11:18,540
if the user didn't provide any data

256
00:11:18,540 --> 00:11:20,880
training data a reality's performance

257
00:11:20,880 --> 00:11:23,399
will degrade so achieving the no

258
00:11:23,399 --> 00:11:25,440
training hundreds remains an open

259
00:11:25,440 --> 00:11:27,899
problem finally let's conclude this work

260
00:11:27,899 --> 00:11:30,660
in this work we propose a real ID to

261
00:11:30,660 --> 00:11:33,779
detect voices goofing based on the macro

262
00:11:33,779 --> 00:11:36,120
array we propose a robust loudness

263
00:11:36,120 --> 00:11:39,300
feature a red fingerprint finally by

264
00:11:39,300 --> 00:11:42,120
conducting comprehensive experiments we

265
00:11:42,120 --> 00:11:44,700
prove that reality is effective and

266
00:11:44,700 --> 00:11:47,820
robust to various factors in the future

267
00:11:47,820 --> 00:11:50,279
we need to overcome the issues related

268
00:11:50,279 --> 00:11:52,860
to the noise Advanced attackers and

269
00:11:52,860 --> 00:11:55,740
training requirements that's all thank

270
00:11:55,740 --> 00:11:57,540
you for your listening welcome to ask

271
00:11:57,540 --> 00:12:00,180
questions you can also reach me and my

272
00:12:00,180 --> 00:12:04,640
advisor via email appearance thank you

