1
00:00:07,400 --> 00:00:09,780
hello everybody my name is Alejandro

2
00:00:09,780 --> 00:00:11,820
Cuevas I'm a PhD at Carnegie Mellon

3
00:00:11,820 --> 00:00:13,500
University I'm here with my co-author

4
00:00:13,500 --> 00:00:16,020
fika midama who is from TU delft and

5
00:00:16,020 --> 00:00:17,400
together we are going to be telling you

6
00:00:17,400 --> 00:00:19,080
a little bit about accuracy of Online

7
00:00:19,080 --> 00:00:21,240
Marketplace measurements this is work

8
00:00:21,240 --> 00:00:23,760
with our colleagues between CMU and tu

9
00:00:23,760 --> 00:00:25,199
delft

10
00:00:25,199 --> 00:00:27,060
so to start this talk I will assume that

11
00:00:27,060 --> 00:00:29,820
all of you law-abiding citizens haven't

12
00:00:29,820 --> 00:00:31,560
had the chance to interact with a dark

13
00:00:31,560 --> 00:00:33,840
net Marketplace so on the screen we have

14
00:00:33,840 --> 00:00:36,120
Hansa a Marketplace that existed between

15
00:00:36,120 --> 00:00:39,480
2015 and 2017. the website is similar to

16
00:00:39,480 --> 00:00:41,460
other online marketplaces with category

17
00:00:41,460 --> 00:00:43,379
filters and sorting options however

18
00:00:43,379 --> 00:00:45,480
Hansa was a marketplace where mostly

19
00:00:45,480 --> 00:00:47,100
illegal goods and services were

20
00:00:47,100 --> 00:00:49,379
commercialized

21
00:00:49,379 --> 00:00:51,360
on the sidebar we see various types of

22
00:00:51,360 --> 00:00:54,000
drugs fraud related items such as credit

23
00:00:54,000 --> 00:00:56,100
cards and database leaks we have

24
00:00:56,100 --> 00:00:57,600
tutorials ranging from how to

25
00:00:57,600 --> 00:00:59,340
manufacture drugs to how to deploy

26
00:00:59,340 --> 00:01:02,219
malware services such as DDOS and

27
00:01:02,219 --> 00:01:04,319
bulletproof hosting and digital Goods

28
00:01:04,319 --> 00:01:07,439
such as malware and fishing kits

29
00:01:07,439 --> 00:01:10,080
and since their emergence in 2011 the

30
00:01:10,080 --> 00:01:11,880
question of how big these marketplaces

31
00:01:11,880 --> 00:01:13,979
are attracted the attention of academic

32
00:01:13,979 --> 00:01:16,159
researchers and law enforcement agencies

33
00:01:16,159 --> 00:01:18,540
DNM measurements influence various

34
00:01:18,540 --> 00:01:20,640
aspects of law enforcement estimating

35
00:01:20,640 --> 00:01:22,860
how much revenue a Marketplace or vendor

36
00:01:22,860 --> 00:01:24,840
drives is useful to prioritize which

37
00:01:24,840 --> 00:01:27,299
Market or vendor to go after and is also

38
00:01:27,299 --> 00:01:29,159
used during the indictment

39
00:01:29,159 --> 00:01:31,799
DNM measurements and research also

40
00:01:31,799 --> 00:01:34,020
informs policy making in the US for

41
00:01:34,020 --> 00:01:35,640
example a bill was introduced to

42
00:01:35,640 --> 00:01:37,380
increase efforts against trade in dark

43
00:01:37,380 --> 00:01:39,360
web marketplaces with a focus with a

44
00:01:39,360 --> 00:01:41,640
focus on opioids but how do we find out

45
00:01:41,640 --> 00:01:44,640
for example whether opioids constitute a

46
00:01:44,640 --> 00:01:47,460
lot of sales in d m's because DNM

47
00:01:47,460 --> 00:01:49,560
research plays a role in judicial and

48
00:01:49,560 --> 00:01:51,899
legislative proceedings it is vital to

49
00:01:51,899 --> 00:01:55,860
have accurate and validated measurements

50
00:01:55,860 --> 00:01:57,600
to understand how previous work

51
00:01:57,600 --> 00:01:59,520
estimates Revenue we need to navigate to

52
00:01:59,520 --> 00:02:01,140
a listing page as the one seen on the

53
00:02:01,140 --> 00:02:03,479
screen in this page you will usually

54
00:02:03,479 --> 00:02:05,579
find the price the vendor and most

55
00:02:05,579 --> 00:02:07,140
importantly customer reviews with

56
00:02:07,140 --> 00:02:10,380
timestamps reviews are what allow us to

57
00:02:10,380 --> 00:02:13,080
estimate a lower bound of Revenue

58
00:02:13,080 --> 00:02:15,840
dnms usually strongly encourage users to

59
00:02:15,840 --> 00:02:17,580
leave reviews for vendors because it's

60
00:02:17,580 --> 00:02:19,379
the only way to build reputation in

61
00:02:19,379 --> 00:02:21,540
these Anonymous markets and password has

62
00:02:21,540 --> 00:02:23,160
relied on the fact that reviews seem to

63
00:02:23,160 --> 00:02:25,020
match orders and thus by assuming that

64
00:02:25,020 --> 00:02:27,720
each of these feedbacks

65
00:02:27,720 --> 00:02:29,040
what

66
00:02:29,040 --> 00:02:31,200
what was an order and multiplying it by

67
00:02:31,200 --> 00:02:32,879
the listing price we could get a lower

68
00:02:32,879 --> 00:02:34,500
bound estimate on the amount of Revenue

69
00:02:34,500 --> 00:02:36,900
that listing generated so when measuring

70
00:02:36,900 --> 00:02:38,940
marketplaces we are not directly

71
00:02:38,940 --> 00:02:41,040
measuring the object we care about in

72
00:02:41,040 --> 00:02:42,660
this case orders but instead we're

73
00:02:42,660 --> 00:02:46,200
measuring proxies in this case feedbacks

74
00:02:46,200 --> 00:02:47,760
and the problem is that there's a

75
00:02:47,760 --> 00:02:49,739
substantial body of work on darknet

76
00:02:49,739 --> 00:02:51,959
marketplaces based on measurements by

77
00:02:51,959 --> 00:02:53,959
proxy but with little validation

78
00:02:53,959 --> 00:02:56,760
furthermore these studies rely on crawls

79
00:02:56,760 --> 00:02:59,040
or scrapes to gather data but we have

80
00:02:59,040 --> 00:03:01,379
little insight on how reliable scraping

81
00:03:01,379 --> 00:03:03,180
is or what properties creating

82
00:03:03,180 --> 00:03:06,000
methodology should be in these markets

83
00:03:06,000 --> 00:03:07,800
so we sought to get an empirical

84
00:03:07,800 --> 00:03:09,720
understanding of the losses and biases

85
00:03:09,720 --> 00:03:11,879
that exist by comparing back end versus

86
00:03:11,879 --> 00:03:13,260
scrape data

87
00:03:13,260 --> 00:03:15,959
and we test the effectiveness of various

88
00:03:15,959 --> 00:03:17,879
scraping designs and coverage estimation

89
00:03:17,879 --> 00:03:20,519
techniques through simulations

90
00:03:20,519 --> 00:03:21,420
thank you

91
00:03:21,420 --> 00:03:23,760
the challenges does to approximate the

92
00:03:23,760 --> 00:03:26,159
back end by front-end measurements this

93
00:03:26,159 --> 00:03:28,080
is often done by scraping the HTML from

94
00:03:28,080 --> 00:03:30,599
the web pages these are then processed

95
00:03:30,599 --> 00:03:32,459
by doing steps such as parsing the

96
00:03:32,459 --> 00:03:34,140
duplicating and matching reviews to

97
00:03:34,140 --> 00:03:36,599
listings this then results in a database

98
00:03:36,599 --> 00:03:38,700
of unique reviews listings and vendors

99
00:03:38,700 --> 00:03:40,200
that can be used to approximate the

100
00:03:40,200 --> 00:03:41,340
backends

101
00:03:41,340 --> 00:03:43,799
from this data researchers attempt to

102
00:03:43,799 --> 00:03:45,900
measure things such as Market Revenue

103
00:03:45,900 --> 00:03:48,180
Financial Security practices through

104
00:03:48,180 --> 00:03:50,159
inference techniques the inference

105
00:03:50,159 --> 00:03:51,599
techniques applied difference between

106
00:03:51,599 --> 00:03:54,720
studies as data processing steps

107
00:03:54,720 --> 00:03:57,360
The Perennial question is how good are

108
00:03:57,360 --> 00:03:59,580
these measurements some researchers have

109
00:03:59,580 --> 00:04:02,040
attempted to answer this by validating

110
00:04:02,040 --> 00:04:03,959
their measurements in a variety of ad

111
00:04:03,959 --> 00:04:06,780
hoc approaches identifiers court cases

112
00:04:06,780 --> 00:04:09,780
metadata and abundance estimation

113
00:04:09,780 --> 00:04:12,000
however these Works have lack

114
00:04:12,000 --> 00:04:14,040
consistency in their approaches in their

115
00:04:14,040 --> 00:04:16,440
documentation and validation this

116
00:04:16,440 --> 00:04:18,000
prevents us from understanding the

117
00:04:18,000 --> 00:04:20,100
losses that may occur at each step of

118
00:04:20,100 --> 00:04:23,100
the way such as the collection loss from

119
00:04:23,100 --> 00:04:25,680
scraping and processing to the inference

120
00:04:25,680 --> 00:04:27,780
loss from using proxies

121
00:04:27,780 --> 00:04:30,360
our goal was just to study the accuracy

122
00:04:30,360 --> 00:04:31,979
of current measurement approaches and

123
00:04:31,979 --> 00:04:35,460
find directions for improvements

124
00:04:35,460 --> 00:04:37,979
to do this we leveraged scrape and

125
00:04:37,979 --> 00:04:40,680
back-end data sources CMU scilab scraped

126
00:04:40,680 --> 00:04:43,320
Hansa markets 14 times so 14 snapshots

127
00:04:43,320 --> 00:04:47,100
over the 2015 to 2017 time periods

128
00:04:47,100 --> 00:04:49,500
similar to previous work we combined all

129
00:04:49,500 --> 00:04:51,000
these scrapes and we processed them to

130
00:04:51,000 --> 00:04:53,040
cut a database with listings reviews and

131
00:04:53,040 --> 00:04:54,419
vendors

132
00:04:54,419 --> 00:04:56,940
we were also granted access to the Hansa

133
00:04:56,940 --> 00:04:58,560
backend seized by the Dutch national

134
00:04:58,560 --> 00:05:00,840
police in their staying operation the

135
00:05:00,840 --> 00:05:02,460
database contained the latest state of

136
00:05:02,460 --> 00:05:06,500
the back end as well as multiple backups

137
00:05:08,880 --> 00:05:11,220
by trying to match each object from the

138
00:05:11,220 --> 00:05:12,900
scrape database to a corresponding

139
00:05:12,900 --> 00:05:15,120
object in the Hansa backend database we

140
00:05:15,120 --> 00:05:17,639
calculated coverage how much of the

141
00:05:17,639 --> 00:05:19,440
backend database was captured in the

142
00:05:19,440 --> 00:05:21,960
scrapes we find out that if you take the

143
00:05:21,960 --> 00:05:25,080
back end as a 100 percent ground truth

144
00:05:25,080 --> 00:05:28,740
54 of all objects are captured in the

145
00:05:28,740 --> 00:05:30,539
script this means that the empirical

146
00:05:30,539 --> 00:05:33,900
collection laws on Hansa was 46 percent

147
00:05:33,900 --> 00:05:35,759
we know that this can happen because

148
00:05:35,759 --> 00:05:38,460
listings get deleted and reviews can be

149
00:05:38,460 --> 00:05:40,860
hidden by vendors in what way is a

150
00:05:40,860 --> 00:05:43,259
scrape then truly a random sample of the

151
00:05:43,259 --> 00:05:47,100
total population of available objects

152
00:05:47,100 --> 00:05:49,320
for that we need to take a look at the

153
00:05:49,320 --> 00:05:52,199
scraped and not scraped listings why

154
00:05:52,199 --> 00:05:54,600
listings well differences between

155
00:05:54,600 --> 00:05:56,039
scraped and not scraped vendors and

156
00:05:56,039 --> 00:05:57,600
review come down to whether the listing

157
00:05:57,600 --> 00:05:59,460
is scraped because if you don't scrape

158
00:05:59,460 --> 00:06:01,139
the listing you don't know what the

159
00:06:01,139 --> 00:06:03,000
vendor sold and you don't know the price

160
00:06:03,000 --> 00:06:05,639
of the item that the review belong to

161
00:06:05,639 --> 00:06:07,740
so for listings we explored features

162
00:06:07,740 --> 00:06:09,360
that could be correlated with the chance

163
00:06:09,360 --> 00:06:11,520
of an object being scraped such as the

164
00:06:11,520 --> 00:06:13,800
listing being hidden and features that

165
00:06:13,800 --> 00:06:15,840
influence Revenue calculations such as

166
00:06:15,840 --> 00:06:18,300
price we found significant differences

167
00:06:18,300 --> 00:06:20,220
for multiple features

168
00:06:20,220 --> 00:06:23,220
for Price scrape listings are an average

169
00:06:23,220 --> 00:06:26,100
lower priced than those great listings

170
00:06:26,100 --> 00:06:27,960
for the number of reviews it's great

171
00:06:27,960 --> 00:06:29,880
listings often have more reviews than

172
00:06:29,880 --> 00:06:31,919
the script listings which could point in

173
00:06:31,919 --> 00:06:33,479
the direction of the scripts already

174
00:06:33,479 --> 00:06:36,240
being biased towards popular listings

175
00:06:36,240 --> 00:06:39,120
finally the Hansa scrapes often contain

176
00:06:39,120 --> 00:06:41,400
more digital Goods listings while not

177
00:06:41,400 --> 00:06:43,020
scraped listings were more often from

178
00:06:43,020 --> 00:06:45,360
the weeds category this is interesting

179
00:06:45,360 --> 00:06:47,520
because both categories were equal size

180
00:06:47,520 --> 00:06:49,199
on hanza

181
00:06:49,199 --> 00:06:51,240
so we know that the empirical collection

182
00:06:51,240 --> 00:06:55,020
laws of hanza was 46 and we notice grade

183
00:06:55,020 --> 00:06:57,900
listings are biased so what does data

184
00:06:57,900 --> 00:07:00,840
mean for our Revenue calculations

185
00:07:00,840 --> 00:07:02,699
we've looked at the total revenue of

186
00:07:02,699 --> 00:07:04,979
Hansa when we calculated the revenue

187
00:07:04,979 --> 00:07:07,080
from all the transactions and orders it

188
00:07:07,080 --> 00:07:10,740
added up to 50.1 million so we're going

189
00:07:10,740 --> 00:07:12,180
to compare all other Revenue

190
00:07:12,180 --> 00:07:15,720
calculations against this as Grand truth

191
00:07:15,720 --> 00:07:17,940
what we saw is that if we calculated

192
00:07:17,940 --> 00:07:20,039
Revenue just based on the reviews that

193
00:07:20,039 --> 00:07:23,099
were scraped it only amounted to 13.2

194
00:07:23,099 --> 00:07:25,860
million we call this The observed

195
00:07:25,860 --> 00:07:27,180
Revenue

196
00:07:27,180 --> 00:07:29,699
however we know the collection loss 46

197
00:07:29,699 --> 00:07:32,220
which means that only around half of the

198
00:07:32,220 --> 00:07:35,660
other reviews were scraped

199
00:07:36,479 --> 00:07:39,000
when we then calculated the revenue of

200
00:07:39,000 --> 00:07:40,620
all the reviews that were available in

201
00:07:40,620 --> 00:07:43,400
the back end we could explain another 40

202
00:07:43,400 --> 00:07:46,560
14.2 billion in Revenue this is the

203
00:07:46,560 --> 00:07:48,840
revenue that we missed due to the hidden

204
00:07:48,840 --> 00:07:51,180
reviews partial scrape rate limited

205
00:07:51,180 --> 00:07:54,960
scrapes timeouts field captures Etc

206
00:07:54,960 --> 00:07:56,699
collection loss

207
00:07:56,699 --> 00:07:59,220
if we had scraped perfectly capturing

208
00:07:59,220 --> 00:08:01,620
all reviews we would thus have captured

209
00:08:01,620 --> 00:08:04,800
a bit more than half of total revenue

210
00:08:04,800 --> 00:08:07,259
but using reviews to calculate revenue

211
00:08:07,259 --> 00:08:10,259
is still using a proxy in reality not

212
00:08:10,259 --> 00:08:12,840
every buyer leaves a review some buyers

213
00:08:12,840 --> 00:08:15,060
buy more than one quantity and buyers

214
00:08:15,060 --> 00:08:18,060
also pay for shipping costs

215
00:08:18,060 --> 00:08:20,460
when we calculated the revenue based on

216
00:08:20,460 --> 00:08:22,979
actual transaction and Order data it

217
00:08:22,979 --> 00:08:26,520
explained 22.7 million thus by the need

218
00:08:26,520 --> 00:08:28,080
to infer orders and their transaction

219
00:08:28,080 --> 00:08:31,259
price from reviews you miss again almost

220
00:08:31,259 --> 00:08:32,760
half of Revenue

221
00:08:32,760 --> 00:08:35,120
this is the inference loss

222
00:08:35,120 --> 00:08:37,679
summarizing our results show that the

223
00:08:37,679 --> 00:08:39,860
current measurement approach on online

224
00:08:39,860 --> 00:08:42,000
marketplaces introduce significant

225
00:08:42,000 --> 00:08:44,760
collection and inference losses however

226
00:08:44,760 --> 00:08:48,000
ground truth data like we had is rarely

227
00:08:48,000 --> 00:08:50,640
if ever available and scraping captures

228
00:08:50,640 --> 00:08:53,640
more version of one objects

229
00:08:53,640 --> 00:08:55,560
now we had a good understanding of the

230
00:08:55,560 --> 00:08:56,940
shortcomings in our hands and scrapes

231
00:08:56,940 --> 00:08:58,260
but we wanted to know how we could have

232
00:08:58,260 --> 00:09:00,120
done a better job and to do this we

233
00:09:00,120 --> 00:09:02,580
created a simulation at a high level we

234
00:09:02,580 --> 00:09:04,380
simulate events that could occur at a

235
00:09:04,380 --> 00:09:06,600
given day in the lifetime of the market

236
00:09:06,600 --> 00:09:08,820
for instance the probability of sample

237
00:09:08,820 --> 00:09:10,740
sampling a new listing is really high at

238
00:09:10,740 --> 00:09:13,140
the end and at Day N plus one we might

239
00:09:13,140 --> 00:09:14,760
have a chance of sampling the new event

240
00:09:14,760 --> 00:09:16,140
listing

241
00:09:16,140 --> 00:09:18,540
um and we do this a certain nine a

242
00:09:18,540 --> 00:09:20,519
certain number of times each day

243
00:09:20,519 --> 00:09:22,080
and we have a few other events that we

244
00:09:22,080 --> 00:09:23,580
defined on our model but essentially

245
00:09:23,580 --> 00:09:25,200
what we're doing is using the data we

246
00:09:25,200 --> 00:09:26,760
have about how the market grew over time

247
00:09:26,760 --> 00:09:28,560
to create similar markets that we can

248
00:09:28,560 --> 00:09:30,600
run additional experiments on in

249
00:09:30,600 --> 00:09:32,100
particular we wanted to try other

250
00:09:32,100 --> 00:09:34,019
scraping approaches and we Define a

251
00:09:34,019 --> 00:09:36,779
scrape basically as a sample of visible

252
00:09:36,779 --> 00:09:38,940
items and this allows us to measure the

253
00:09:38,940 --> 00:09:40,019
coverage of different scraping

254
00:09:40,019 --> 00:09:41,399
approaches under different assumptions

255
00:09:41,399 --> 00:09:43,320
and can also be extended to imitate

256
00:09:43,320 --> 00:09:45,019
other markets

257
00:09:45,019 --> 00:09:48,480
dnmc seek to prevent scraping and given

258
00:09:48,480 --> 00:09:50,279
that several past Studies have relied on

259
00:09:50,279 --> 00:09:51,959
only one or two scrapes we used our

260
00:09:51,959 --> 00:09:53,580
simulation to test how good you can

261
00:09:53,580 --> 00:09:55,080
expect your coverage to be with

262
00:09:55,080 --> 00:09:57,180
different request limits and it turns

263
00:09:57,180 --> 00:09:58,500
out that to achieve High coverage you

264
00:09:58,500 --> 00:10:00,180
need to be making almost one request per

265
00:10:00,180 --> 00:10:02,640
second so if you're using a stealthier

266
00:10:02,640 --> 00:10:04,800
approach to scraping how many scrapes

267
00:10:04,800 --> 00:10:06,600
will we need to achieve decent coverage

268
00:10:06,600 --> 00:10:08,220
and does it matter whether we had a

269
00:10:08,220 --> 00:10:10,019
consistent routine or if we scraped at

270
00:10:10,019 --> 00:10:12,240
different intervals in the case of our

271
00:10:12,240 --> 00:10:14,519
simulated Hansa only after 10 stealthy

272
00:10:14,519 --> 00:10:16,019
scrapes we achieved an average coverage

273
00:10:16,019 --> 00:10:18,600
of 53 percent on the other hand whether

274
00:10:18,600 --> 00:10:20,940
we scrape exactly it even pays intervals

275
00:10:20,940 --> 00:10:22,920
or at random intervals didn't matter too

276
00:10:22,920 --> 00:10:24,899
much for Hansa however other markets

277
00:10:24,899 --> 00:10:26,580
delete feedbacks after a number of days

278
00:10:26,580 --> 00:10:28,560
and in those cases a consistent routine

279
00:10:28,560 --> 00:10:30,300
is crucial

280
00:10:30,300 --> 00:10:32,279
and given that stealthy scraping

281
00:10:32,279 --> 00:10:34,080
involves using a reduced scraping budget

282
00:10:34,080 --> 00:10:35,940
we tested a simple scraping algorithm

283
00:10:35,940 --> 00:10:37,620
that split its budget between

284
00:10:37,620 --> 00:10:39,480
discovering new pages and rescraping

285
00:10:39,480 --> 00:10:41,700
popular pages and this worked as follows

286
00:10:41,700 --> 00:10:43,500
let's say we discover a Time T to

287
00:10:43,500 --> 00:10:46,079
similar listings then at time t plus one

288
00:10:46,079 --> 00:10:48,540
we rescape each listing and compute

289
00:10:48,540 --> 00:10:49,800
their popularity

290
00:10:49,800 --> 00:10:51,779
the golden cigarettes in this case gain

291
00:10:51,779 --> 00:10:53,339
more reviews so we keep it in a popular

292
00:10:53,339 --> 00:10:56,339
list finally at time t plus two our

293
00:10:56,339 --> 00:10:57,899
scraper only re-scrapes the golden

294
00:10:57,899 --> 00:10:59,519
cigarettes and not the silver ones

295
00:10:59,519 --> 00:11:01,380
instead it uses its budget to discover

296
00:11:01,380 --> 00:11:03,959
new listings with this strategy we

297
00:11:03,959 --> 00:11:05,700
capture an average of nine percent more

298
00:11:05,700 --> 00:11:07,740
unique objects and this strategy could

299
00:11:07,740 --> 00:11:09,360
be applied in any context where we

300
00:11:09,360 --> 00:11:11,579
believe the object we want to measure is

301
00:11:11,579 --> 00:11:14,160
not uniformly distributed

302
00:11:14,160 --> 00:11:17,040
our study provides a new lens for past

303
00:11:17,040 --> 00:11:18,600
studies and a framework for future

304
00:11:18,600 --> 00:11:20,940
studies first we identify a lack of

305
00:11:20,940 --> 00:11:22,440
consistency and methodological

306
00:11:22,440 --> 00:11:24,420
description in previous DNM studies

307
00:11:24,420 --> 00:11:26,760
which we show limit our understanding of

308
00:11:26,760 --> 00:11:29,519
losses and biases that may exist second

309
00:11:29,519 --> 00:11:31,200
through our empirical results we

310
00:11:31,200 --> 00:11:33,120
quantify the sources and magnitudes of

311
00:11:33,120 --> 00:11:34,920
biases and inference losses on our

312
00:11:34,920 --> 00:11:36,360
scrapes which can be used to

313
00:11:36,360 --> 00:11:38,519
contextualize past studies as well as to

314
00:11:38,519 --> 00:11:40,200
improve calculating upper bounds on

315
00:11:40,200 --> 00:11:42,480
future studies third we Define a

316
00:11:42,480 --> 00:11:44,459
simulation model which can be used to in

317
00:11:44,459 --> 00:11:46,200
other Marketplace contexts and which

318
00:11:46,200 --> 00:11:48,779
enabled us to for test better scraping

319
00:11:48,779 --> 00:11:51,300
designs and five validate previously

320
00:11:51,300 --> 00:11:53,640
used validation techniques we didn't get

321
00:11:53,640 --> 00:11:55,440
to cover this last point but we invite

322
00:11:55,440 --> 00:11:57,779
you to check the details in our paper

323
00:11:57,779 --> 00:11:59,519
thank you very much and we look forward

324
00:11:59,519 --> 00:12:02,480
to your questions

