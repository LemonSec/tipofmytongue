1
00:00:07,940 --> 00:00:10,559
hello my name is Andrea lingal and I

2
00:00:10,559 --> 00:00:12,420
will present our paper on regular

3
00:00:12,420 --> 00:00:14,639
expression denial of service attacks on

4
00:00:14,639 --> 00:00:16,619
a number tracking mattress this is a

5
00:00:16,619 --> 00:00:19,939
joint work with linkage

6
00:00:19,939 --> 00:00:22,260
from Brandon University of Technology

7
00:00:22,260 --> 00:00:24,240
and Marcus verness from Microsoft

8
00:00:24,240 --> 00:00:25,260
research

9
00:00:25,260 --> 00:00:27,119
what is a regular expression denial of

10
00:00:27,119 --> 00:00:29,340
service attack it is a denial of service

11
00:00:29,340 --> 00:00:32,460
attack performed by giving a regular

12
00:00:32,460 --> 00:00:34,920
expression matter that is for example in

13
00:00:34,920 --> 00:00:37,140
some websites to validate input Fields

14
00:00:37,140 --> 00:00:40,980
some very hard text that makes it slow

15
00:00:40,980 --> 00:00:43,260
down significantly for a concrete

16
00:00:43,260 --> 00:00:46,260
example suppose you have a 500 kilobyte

17
00:00:46,260 --> 00:00:48,000
file containing some pretty random

18
00:00:48,000 --> 00:00:49,980
characters and you want to check whether

19
00:00:49,980 --> 00:00:54,059
there is an a followed by 500 characters

20
00:00:54,059 --> 00:00:56,219
before the end of a line This is a

21
00:00:56,219 --> 00:00:57,539
regular expression that we are looking

22
00:00:57,539 --> 00:01:00,420
for in the input file the runtime for

23
00:01:00,420 --> 00:01:03,420
this is pretty fast it is done in 80

24
00:01:03,420 --> 00:01:05,820
milliseconds then consider that you have

25
00:01:05,820 --> 00:01:07,740
a different file that contains all A's

26
00:01:07,740 --> 00:01:09,780
and you try to find the same regular

27
00:01:09,780 --> 00:01:12,119
expression there now the runtime is much

28
00:01:12,119 --> 00:01:15,840
lower 1.52 seconds this is 19 times

29
00:01:15,840 --> 00:01:18,000
slower and in some cases it might be

30
00:01:18,000 --> 00:01:20,280
considered to be a redose note that this

31
00:01:20,280 --> 00:01:22,380
is only a very simple example that could

32
00:01:22,380 --> 00:01:24,600
fit on one line on the slide in practice

33
00:01:24,600 --> 00:01:27,060
the slowdowns can be much higher as we

34
00:01:27,060 --> 00:01:28,259
will see later

35
00:01:28,259 --> 00:01:30,780
redos is not only a theoretical toy but

36
00:01:30,780 --> 00:01:33,119
it is a real world threat for example in

37
00:01:33,119 --> 00:01:36,299
2016 stack Overflow was taken down for

38
00:01:36,299 --> 00:01:38,100
42 minutes because of a very simple

39
00:01:38,100 --> 00:01:40,560
regex and some malicious user submitting

40
00:01:40,560 --> 00:01:42,840
a comment that contained 20 000 spaces

41
00:01:42,840 --> 00:01:45,840
and at a at the end of the line moreover

42
00:01:45,840 --> 00:01:47,700
there have been a real vulnerabilities

43
00:01:47,700 --> 00:01:49,680
detecting in popular Frameworks for

44
00:01:49,680 --> 00:01:52,500
making websites such as express.js where

45
00:01:52,500 --> 00:01:54,540
you could attack the package negotiator

46
00:01:54,540 --> 00:01:57,180
by submitting some evil text inside an

47
00:01:57,180 --> 00:02:00,180
HTTP header field or node.js where the

48
00:02:00,180 --> 00:02:02,399
package URL regex could be attacked by

49
00:02:02,399 --> 00:02:06,240
submitting some Evo text in the URL

50
00:02:06,240 --> 00:02:08,880
the cause of these reduces is often the

51
00:02:08,880 --> 00:02:10,440
use of backtracking matches which are

52
00:02:10,440 --> 00:02:12,720
measures that use non-determinism and

53
00:02:12,720 --> 00:02:14,959
backtracking to match an input text

54
00:02:14,959 --> 00:02:17,760
these are used for example in PHP

55
00:02:17,760 --> 00:02:20,099
JavaScript parallel Ruby or dotnet they

56
00:02:20,099 --> 00:02:21,720
are the default matches there

57
00:02:21,720 --> 00:02:23,340
the common advice of how to deal with

58
00:02:23,340 --> 00:02:25,440
readers is to use non-backtracking

59
00:02:25,440 --> 00:02:27,420
methods sometimes also called linear

60
00:02:27,420 --> 00:02:29,700
time matching engines or matches based

61
00:02:29,700 --> 00:02:32,040
on Thompson's algorithm this has been

62
00:02:32,040 --> 00:02:34,560
advocated for example in these three

63
00:02:34,560 --> 00:02:36,120
papers there were more papers this is

64
00:02:36,120 --> 00:02:39,000
just some example and they say that as

65
00:02:39,000 --> 00:02:40,500
long as you use this kind of non-bike

66
00:02:40,500 --> 00:02:42,720
tracking measures then nothing bad can

67
00:02:42,720 --> 00:02:43,800
happen to you

68
00:02:43,800 --> 00:02:46,019
well we show that this is actually not

69
00:02:46,019 --> 00:02:47,700
true and even if you use the download

70
00:02:47,700 --> 00:02:49,980
tracking mattress you can still be

71
00:02:49,980 --> 00:02:53,340
vulnerable to a redos

72
00:02:53,340 --> 00:02:55,200
let us now recall how non-backed

73
00:02:55,200 --> 00:02:56,879
tracking measures work the textbook

74
00:02:56,879 --> 00:02:58,980
algorithm starts by constructing another

75
00:02:58,980 --> 00:03:01,080
deterministic finite automaton for a

76
00:03:01,080 --> 00:03:03,840
given regax then the automaton is

77
00:03:03,840 --> 00:03:06,120
determinized into a deterministic finite

78
00:03:06,120 --> 00:03:08,879
automaton which can track all possible

79
00:03:08,879 --> 00:03:10,319
rounds of the non-deterministic

80
00:03:10,319 --> 00:03:13,260
automaton in its macro States the states

81
00:03:13,260 --> 00:03:14,819
here are called macro States because

82
00:03:14,819 --> 00:03:19,140
they contain states of the original NFA

83
00:03:19,140 --> 00:03:21,780
after the DFA is constructed we can then

84
00:03:21,780 --> 00:03:24,420
perform a match in linear time the issue

85
00:03:24,420 --> 00:03:26,879
of this approach is that the DFA that

86
00:03:26,879 --> 00:03:28,500
would be constructed might often be too

87
00:03:28,500 --> 00:03:30,599
big for example in some applications a

88
00:03:30,599 --> 00:03:32,700
network intrusion detection the DFA

89
00:03:32,700 --> 00:03:35,519
would have billions or even more space

90
00:03:35,519 --> 00:03:37,260
therefore in practice a different

91
00:03:37,260 --> 00:03:39,300
algorithm is used it's the so-called

92
00:03:39,300 --> 00:03:41,519
Thompson's algorithm that also starts by

93
00:03:41,519 --> 00:03:42,739
constructing another thermostat

94
00:03:42,739 --> 00:03:45,959
automaton but then the dfac is not

95
00:03:45,959 --> 00:03:48,900
constructed at priori but only on the

96
00:03:48,900 --> 00:03:51,360
fly so that only the useful parts are

97
00:03:51,360 --> 00:03:53,760
constructed while the input text is

98
00:03:53,760 --> 00:03:56,340
being matched the constructive parts are

99
00:03:56,340 --> 00:03:59,760
being cached and this often gives you a

100
00:03:59,760 --> 00:04:02,040
linear time matching complexity

101
00:04:02,040 --> 00:04:03,840
tools that use this approach are for

102
00:04:03,840 --> 00:04:06,420
example grab re2 from Google the metro

103
00:04:06,420 --> 00:04:09,000
in Rust SRM for Microsoft or hyperscan

104
00:04:09,000 --> 00:04:10,980
from Intel actually hypers can use this

105
00:04:10,980 --> 00:04:12,480
a little bit different algorithm that

106
00:04:12,480 --> 00:04:15,120
doesn't use the caching

107
00:04:15,120 --> 00:04:18,180
however we observed that these measures

108
00:04:18,180 --> 00:04:20,699
can still run slow and the reason why

109
00:04:20,699 --> 00:04:23,220
they run slow is when the Loca when

110
00:04:23,220 --> 00:04:25,199
there is Locust utilization because the

111
00:04:25,199 --> 00:04:28,380
cache is often reset then the linear

112
00:04:28,380 --> 00:04:31,199
time complexity is not achieved

113
00:04:31,199 --> 00:04:33,540
the question is whether we can address

114
00:04:33,540 --> 00:04:35,639
this and have an approach that can

115
00:04:35,639 --> 00:04:37,380
systematically generate regular

116
00:04:37,380 --> 00:04:39,540
expression denial of service attacks on

117
00:04:39,540 --> 00:04:41,580
these non-bike tracking measures

118
00:04:41,580 --> 00:04:44,220
and the answer is yes we actually can

119
00:04:44,220 --> 00:04:47,520
and we do this by exploiting counting in

120
00:04:47,520 --> 00:04:49,680
regular Expressions counting is a

121
00:04:49,680 --> 00:04:51,120
feature of regular expression sometimes

122
00:04:51,120 --> 00:04:53,280
also called quantifiers or bounded

123
00:04:53,280 --> 00:04:56,160
repetition that can be used to say that

124
00:04:56,160 --> 00:04:58,560
some string in the regular expression is

125
00:04:58,560 --> 00:05:01,080
repeated several times for example in

126
00:05:01,080 --> 00:05:03,540
this example we say that a is repeated

127
00:05:03,540 --> 00:05:05,820
at least five times and at most 42 times

128
00:05:05,820 --> 00:05:09,720
and B is repeated exactly 100 times

129
00:05:09,720 --> 00:05:12,060
we obtained a large data set of regaxes

130
00:05:12,060 --> 00:05:13,979
from GitHub Network intrusion detection

131
00:05:13,979 --> 00:05:16,199
systems snort and bro and from the

132
00:05:16,199 --> 00:05:18,540
industry and after removing relaxes with

133
00:05:18,540 --> 00:05:20,460
unsupported features like Look Around

134
00:05:20,460 --> 00:05:22,259
Speaker references we were left with

135
00:05:22,259 --> 00:05:25,620
something like 450k or axis then we try

136
00:05:25,620 --> 00:05:28,259
to see how much do the numbers occurring

137
00:05:28,259 --> 00:05:30,300
in the counting expressions in the regex

138
00:05:30,300 --> 00:05:33,600
like here affect the size of the DFA

139
00:05:33,600 --> 00:05:35,520
that is constructed for the ray guys

140
00:05:35,520 --> 00:05:38,460
then we want to see how many times is

141
00:05:38,460 --> 00:05:40,919
the DFA bigger than 1000 States we use

142
00:05:40,919 --> 00:05:42,900
the threshold of 1000 States because

143
00:05:42,900 --> 00:05:46,680
often the size of the cache for the DFA

144
00:05:46,680 --> 00:05:49,259
is something around 1000.

145
00:05:49,259 --> 00:05:51,539
we classify the Radix into three sets

146
00:05:51,539 --> 00:05:53,759
reg axis with no counting where the

147
00:05:53,759 --> 00:05:55,979
probability of having a large DFA is

148
00:05:55,979 --> 00:05:59,100
very low with counting bounds less than

149
00:05:59,100 --> 00:06:01,380
20 where the probability is a little bit

150
00:06:01,380 --> 00:06:04,380
higher but still very low and with the

151
00:06:04,380 --> 00:06:06,840
sum of counting bounds over 20 where the

152
00:06:06,840 --> 00:06:09,000
probability of having a large DFA is

153
00:06:09,000 --> 00:06:11,160
quite high around 20 percent

154
00:06:11,160 --> 00:06:13,380
we use this information about the

155
00:06:13,380 --> 00:06:15,240
vulnerability of legacies with counting

156
00:06:15,240 --> 00:06:18,060
to create a riddles generator the

157
00:06:18,060 --> 00:06:20,880
generator works by creating an input

158
00:06:20,880 --> 00:06:22,680
text by searching through the

159
00:06:22,680 --> 00:06:24,720
deterministic finite automaton for the

160
00:06:24,720 --> 00:06:26,819
regular expression trying to generate a

161
00:06:26,819 --> 00:06:28,800
non-matching tax we try to get

162
00:06:28,800 --> 00:06:30,960
non-matching text in order not to allow

163
00:06:30,960 --> 00:06:32,940
the matcher to accept before reading the

164
00:06:32,940 --> 00:06:35,039
whole text and also to also be able to

165
00:06:35,039 --> 00:06:38,160
attack backtracking matches

166
00:06:38,160 --> 00:06:40,919
when doing the search for the DFA we try

167
00:06:40,919 --> 00:06:42,840
to prefer microstates that are unvisited

168
00:06:42,840 --> 00:06:45,000
in order to have low cache utilization

169
00:06:45,000 --> 00:06:47,940
of the mature and that are big in order

170
00:06:47,940 --> 00:06:50,819
for the state to have it hard to

171
00:06:50,819 --> 00:06:53,639
generate the successor state for example

172
00:06:53,639 --> 00:06:56,100
here if we try to get the successes of

173
00:06:56,100 --> 00:06:58,440
the state q and T we have three options

174
00:06:58,440 --> 00:07:00,600
and we can take the last one because it

175
00:07:00,600 --> 00:07:03,060
has the biggest macro state by doing

176
00:07:03,060 --> 00:07:05,100
this we try to enforce the non-linear

177
00:07:05,100 --> 00:07:07,800
runtime of the measure

178
00:07:07,800 --> 00:07:10,800
the issue here is how can we navigate to

179
00:07:10,800 --> 00:07:13,139
the big macro States because at priority

180
00:07:13,139 --> 00:07:15,300
we don't know where are the big macro

181
00:07:15,300 --> 00:07:16,800
States and how to get to them because

182
00:07:16,800 --> 00:07:19,800
the DFA is too big to be constructed it

183
00:07:19,800 --> 00:07:21,840
can has for example billions of states

184
00:07:21,840 --> 00:07:23,880
we address this by using a different

185
00:07:23,880 --> 00:07:26,099
formal model than a DFA we use the

186
00:07:26,099 --> 00:07:28,199
so-called counting set automaton which

187
00:07:28,199 --> 00:07:29,940
we introduced previously in our oopsa

188
00:07:29,940 --> 00:07:32,639
paper which is a formal model that can

189
00:07:32,639 --> 00:07:35,280
be used for compact representation of

190
00:07:35,280 --> 00:07:38,039
regular expressions with Counting

191
00:07:38,039 --> 00:07:40,800
the generator is implemented in a tool

192
00:07:40,800 --> 00:07:43,860
called gajitka We performed experiments

193
00:07:43,860 --> 00:07:45,660
to see how good we are in generating

194
00:07:45,660 --> 00:07:47,580
reduces for non-back tracking methods

195
00:07:47,580 --> 00:07:49,440
for this we took the same data set of

196
00:07:49,440 --> 00:07:51,419
regaxes as in the previous experiment

197
00:07:51,419 --> 00:07:54,060
and kept only those regaxes with the sum

198
00:07:54,060 --> 00:07:55,979
of counter bounds bigger than 20 which

199
00:07:55,979 --> 00:07:57,020
gave us

200
00:07:57,020 --> 00:08:00,720
8099 rexes we also compare with other

201
00:08:00,720 --> 00:08:03,419
readers generators however now that all

202
00:08:03,419 --> 00:08:05,639
of these generators Target backtracking

203
00:08:05,639 --> 00:08:08,400
measures as far as we know we are the

204
00:08:08,400 --> 00:08:11,220
first ones to Target non-back tracking

205
00:08:11,220 --> 00:08:12,840
measures

206
00:08:12,840 --> 00:08:15,419
for each of the generator and each regex

207
00:08:15,419 --> 00:08:18,300
we generated 50 megabyte input text and

208
00:08:18,300 --> 00:08:20,099
tried it on different measures

209
00:08:20,099 --> 00:08:22,319
in the first experiment we try to see

210
00:08:22,319 --> 00:08:23,940
how many resources we can generate for

211
00:08:23,940 --> 00:08:25,560
standard matches

212
00:08:25,560 --> 00:08:27,780
here the Criterion for what is a redose

213
00:08:27,780 --> 00:08:29,940
was that the time of my change of the

214
00:08:29,940 --> 00:08:32,520
Metro needs to be more than 100 times

215
00:08:32,520 --> 00:08:34,140
longer than average for the given

216
00:08:34,140 --> 00:08:36,240
measure on random input

217
00:08:36,240 --> 00:08:37,919
results for other readers protea

218
00:08:37,919 --> 00:08:40,140
criteria are in the paper

219
00:08:40,140 --> 00:08:42,419
our tool gajitka uses four different

220
00:08:42,419 --> 00:08:44,339
strategies for exploring accounting side

221
00:08:44,339 --> 00:08:46,500
automaton counting which is using the

222
00:08:46,500 --> 00:08:47,700
counting structure and the regular

223
00:08:47,700 --> 00:08:49,680
Expressions one line which is targeting

224
00:08:49,680 --> 00:08:51,360
specifically hyperscan and two more

225
00:08:51,360 --> 00:08:54,060
algorithms I will not go into you can

226
00:08:54,060 --> 00:08:55,920
see that counting is giving best results

227
00:08:55,920 --> 00:08:59,279
on these four leftmost non-back tracking

228
00:08:59,279 --> 00:09:00,360
matchers

229
00:09:00,360 --> 00:09:03,200
for hyperscan one line performs better

230
00:09:03,200 --> 00:09:06,420
and the last column here is CA which is

231
00:09:06,420 --> 00:09:08,760
our Metro based on Counting set automata

232
00:09:08,760 --> 00:09:10,860
the same formal model that is used for

233
00:09:10,860 --> 00:09:12,600
generating the resources

234
00:09:12,600 --> 00:09:15,000
in the right hand side of the table you

235
00:09:15,000 --> 00:09:16,680
can also see performance of our

236
00:09:16,680 --> 00:09:19,019
algorithm on backtracking matches in the

237
00:09:19,019 --> 00:09:21,540
common programming languages although we

238
00:09:21,540 --> 00:09:24,600
do not Target them specifically we give

239
00:09:24,600 --> 00:09:26,519
better results than the other readers

240
00:09:26,519 --> 00:09:28,380
generators that focus on backtracking

241
00:09:28,380 --> 00:09:31,019
matches we also try to see whether we

242
00:09:31,019 --> 00:09:32,880
can use our generator to attack real

243
00:09:32,880 --> 00:09:35,760
life Security Solutions for this we use

244
00:09:35,760 --> 00:09:37,500
a different data set of regexes we use

245
00:09:37,500 --> 00:09:40,500
real life rule sets for snort from this

246
00:09:40,500 --> 00:09:42,360
we filtered out unsupported weak axis

247
00:09:42,360 --> 00:09:44,040
and re-axis where the sum of the

248
00:09:44,040 --> 00:09:46,260
repetition bounds was less than 20. by

249
00:09:46,260 --> 00:09:47,820
doing this we obtain something around

250
00:09:47,820 --> 00:09:50,519
1100 regaxes and we were measuring the

251
00:09:50,519 --> 00:09:53,220
Slowdown of evil text versus random text

252
00:09:53,220 --> 00:09:55,200
we first tried to play with snort

253
00:09:55,200 --> 00:09:57,120
running hyperscan as a regular

254
00:09:57,120 --> 00:09:58,860
expression matching engine

255
00:09:58,860 --> 00:10:01,440
here we perform two experiments one for

256
00:10:01,440 --> 00:10:04,500
the MTU of 1.5 kilobytes and another one

257
00:10:04,500 --> 00:10:06,300
for the MTU of 9 kilobytes which

258
00:10:06,300 --> 00:10:07,380
corresponds to

259
00:10:07,380 --> 00:10:09,000
Eternal jumbo frames

260
00:10:09,000 --> 00:10:10,680
in the left hand side for the 1.5

261
00:10:10,680 --> 00:10:12,779
kilobytes you can see that we can in

262
00:10:12,779 --> 00:10:15,540
many cases obtain a Slowdown of 10 20 or

263
00:10:15,540 --> 00:10:17,820
even 40 and in some cases we can even

264
00:10:17,820 --> 00:10:21,180
get to the Slowdown of 100 in the right

265
00:10:21,180 --> 00:10:22,920
hand graph which is for the jumbo frames

266
00:10:22,920 --> 00:10:25,019
you can see that everything is shifted

267
00:10:25,019 --> 00:10:28,019
more to the right and in 32 bases we can

268
00:10:28,019 --> 00:10:30,600
get a Slowdown of at least 100 which is

269
00:10:30,600 --> 00:10:31,740
quite a lot

270
00:10:31,740 --> 00:10:34,320
it is interesting to notice that you

271
00:10:34,320 --> 00:10:36,060
don't even need to have big counting

272
00:10:36,060 --> 00:10:38,880
bounds in the reg axis for example here

273
00:10:38,880 --> 00:10:41,640
we could have 35 and obtain the Slowdown

274
00:10:41,640 --> 00:10:45,120
of 214 for the jumbo frames the other

275
00:10:45,120 --> 00:10:46,620
real world security solution that we

276
00:10:46,620 --> 00:10:48,839
tried was the Nvidia Bluefield to data

277
00:10:48,839 --> 00:10:50,820
processing unit this is a card that

278
00:10:50,820 --> 00:10:53,279
contains Hardware accelerated reggae's

279
00:10:53,279 --> 00:10:54,540
matching unit with a throughput of

280
00:10:54,540 --> 00:10:56,399
around 40 gigabits per second

281
00:10:56,399 --> 00:10:59,579
here we tried 617 regaxes because from

282
00:10:59,579 --> 00:11:02,160
the original 1100 some of the regexes

283
00:11:02,160 --> 00:11:03,899
contain some features not supported by

284
00:11:03,899 --> 00:11:05,700
the regular expression matching unit on

285
00:11:05,700 --> 00:11:06,600
the card

286
00:11:06,600 --> 00:11:08,940
the results can be seen here where you

287
00:11:08,940 --> 00:11:12,360
can see that in 125 cases we achieved a

288
00:11:12,360 --> 00:11:15,420
Slowdown of 10 to 100 and in something

289
00:11:15,420 --> 00:11:19,079
around 100 cases The Slowdown was even

290
00:11:19,079 --> 00:11:20,100
bigger

291
00:11:20,100 --> 00:11:22,560
for example in 16 cases The Slowdown was

292
00:11:22,560 --> 00:11:25,079
even over 500 times

293
00:11:25,079 --> 00:11:26,880
the winner here was this regular

294
00:11:26,880 --> 00:11:28,740
expression where the obtained slowdown

295
00:11:28,740 --> 00:11:33,180
was around 2 200. this brings me to the

296
00:11:33,180 --> 00:11:35,579
conclusion of the talk our main point is

297
00:11:35,579 --> 00:11:37,019
that non-back tracking regular

298
00:11:37,019 --> 00:11:39,060
expression matches are not a silver

299
00:11:39,060 --> 00:11:41,519
bullet against riddles they can still be

300
00:11:41,519 --> 00:11:43,800
attacked for example by attacking

301
00:11:43,800 --> 00:11:46,140
counting in the regular expressions

302
00:11:46,140 --> 00:11:48,720
we created a generator that does this

303
00:11:48,720 --> 00:11:51,480
hijitka and can be used for generating

304
00:11:51,480 --> 00:11:53,760
these resources the mitigations are the

305
00:11:53,760 --> 00:11:56,160
obvious for example by putting limits on

306
00:11:56,160 --> 00:11:58,800
the time input size or by disallowing

307
00:11:58,800 --> 00:12:01,079
counting in the regular expressions

308
00:12:01,079 --> 00:12:02,700
another thing you can do is to over

309
00:12:02,700 --> 00:12:05,700
approximate accounting into a star

310
00:12:05,700 --> 00:12:07,920
you can also use gadgetka to detect the

311
00:12:07,920 --> 00:12:10,560
vulnerable Ray axis and the last thing

312
00:12:10,560 --> 00:12:12,240
you can use a better regress matching

313
00:12:12,240 --> 00:12:14,220
technology for example based on the

314
00:12:14,220 --> 00:12:17,220
formalism of counting set automata

315
00:12:17,220 --> 00:12:20,420
thank you for your attention

