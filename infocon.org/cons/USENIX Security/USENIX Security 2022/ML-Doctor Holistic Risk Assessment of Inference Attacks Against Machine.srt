1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:11,460 --> 00:00:13,559
today I will introduce our work the

3
00:00:13,559 --> 00:00:16,440
title is I'm a doctor holistic risk

4
00:00:16,440 --> 00:00:18,600
assessment of influence attacks against

5
00:00:18,600 --> 00:00:20,340
machine learning models

6
00:00:20,340 --> 00:00:23,820
this is John work with RI shinley Ahmed

7
00:00:23,820 --> 00:00:27,599
jukun Michael Mario and Young from cspa

8
00:00:27,599 --> 00:00:31,340
and I'm liano from UCL

9
00:00:31,560 --> 00:00:33,480
machine learning security and privacy

10
00:00:33,480 --> 00:00:36,540
continues many fields among that many

11
00:00:36,540 --> 00:00:38,399
research researchers are related to

12
00:00:38,399 --> 00:00:41,340
influence attacks it allows adversaries

13
00:00:41,340 --> 00:00:42,960
to learn sensitive information about

14
00:00:42,960 --> 00:00:44,340
training data

15
00:00:44,340 --> 00:00:46,800
model parameters and so on

16
00:00:46,800 --> 00:00:48,420
so there are many different kinds of

17
00:00:48,420 --> 00:00:50,460
inference attacks like membership

18
00:00:50,460 --> 00:00:53,100
inference model inversion attribute

19
00:00:53,100 --> 00:00:55,980
inference and model stealing

20
00:00:55,980 --> 00:00:58,379
so existing inference attacks have been

21
00:00:58,379 --> 00:01:00,480
studied under different stress models

22
00:01:00,480 --> 00:01:02,460
and experimental settings

23
00:01:02,460 --> 00:01:04,680
orbit in isolation

24
00:01:04,680 --> 00:01:07,260
these probes the need for holistic

25
00:01:07,260 --> 00:01:09,780
understanding of the risks caused by

26
00:01:09,780 --> 00:01:11,100
these attacks

27
00:01:11,100 --> 00:01:13,380
such as the scenarios that difference

28
00:01:13,380 --> 00:01:16,500
inference attacks can be applied to

29
00:01:16,500 --> 00:01:18,960
the common factors that influence these

30
00:01:18,960 --> 00:01:20,460
attacks performance

31
00:01:20,460 --> 00:01:22,979
as well as effectiveness of defense

32
00:01:22,979 --> 00:01:24,299
mechanisms

33
00:01:24,299 --> 00:01:27,299
so we fill this Gap by presenting a

34
00:01:27,299 --> 00:01:29,400
first office scan holistic risk

35
00:01:29,400 --> 00:01:31,259
assessment of different influence

36
00:01:31,259 --> 00:01:34,979
attacks against machine learning models

37
00:01:34,979 --> 00:01:37,320
so we proposed I'm a doctor

38
00:01:37,320 --> 00:01:39,720
we report the different modules of ml

39
00:01:39,720 --> 00:01:41,100
doctor

40
00:01:41,100 --> 00:01:44,640
the first is the data processing module

41
00:01:44,640 --> 00:01:47,520
this module processes the data sets to

42
00:01:47,520 --> 00:01:49,320
mount different attacks

43
00:01:49,320 --> 00:01:52,680
it also involves data Pro pre-processing

44
00:01:52,680 --> 00:01:56,700
method like normalization

45
00:01:56,700 --> 00:02:00,060
second for the attack module it performs

46
00:02:00,060 --> 00:02:02,040
the different inference attacks for the

47
00:02:02,040 --> 00:02:05,159
Target models it supports 10 different

48
00:02:05,159 --> 00:02:07,680
texts belonging to four different types

49
00:02:07,680 --> 00:02:10,139
at this moment

50
00:02:10,139 --> 00:02:13,379
next in the defense module we currently

51
00:02:13,379 --> 00:02:15,540
support two representative mitigation

52
00:02:15,540 --> 00:02:18,959
techniques namely dpsgd and knowledge

53
00:02:18,959 --> 00:02:20,640
distillation

54
00:02:20,640 --> 00:02:23,640
last in the evaluation module we

55
00:02:23,640 --> 00:02:25,860
evaluate the performance of attacks and

56
00:02:25,860 --> 00:02:29,599
defenses and give the final report

57
00:02:29,940 --> 00:02:32,700
we then analyzed the impact of data set

58
00:02:32,700 --> 00:02:34,560
and overfitting on the attack

59
00:02:34,560 --> 00:02:37,260
performance as well as the relationship

60
00:02:37,260 --> 00:02:40,440
among different attacks so we propose

61
00:02:40,440 --> 00:02:43,739
three research questions

62
00:02:43,739 --> 00:02:47,160
first what's the impact of data set

63
00:02:47,160 --> 00:02:50,099
complexity on different attacks

64
00:02:50,099 --> 00:02:52,379
we Define the data set complexity

65
00:02:52,379 --> 00:02:55,620
related to data set we used

66
00:02:55,620 --> 00:02:58,379
fashionab needs or feminist is the

67
00:02:58,379 --> 00:03:01,019
simplest data set as it only contains

68
00:03:01,019 --> 00:03:02,819
green scale images

69
00:03:02,819 --> 00:03:04,739
followed by utk phase

70
00:03:04,739 --> 00:03:07,800
which consists of full color human faces

71
00:03:07,800 --> 00:03:09,239
and celebrate

72
00:03:09,239 --> 00:03:12,360
which has 10 times more images than utk

73
00:03:12,360 --> 00:03:13,739
phase

74
00:03:13,739 --> 00:03:16,739
the most complex data set is sl10

75
00:03:16,739 --> 00:03:19,440
as it contains images of 10 diverse

76
00:03:19,440 --> 00:03:23,879
classes ranging from cat to ship

77
00:03:23,879 --> 00:03:26,159
so we plot the relationships in this

78
00:03:26,159 --> 00:03:27,060
figure

79
00:03:27,060 --> 00:03:30,360
where the x-axis is the data set ordered

80
00:03:30,360 --> 00:03:32,099
by the complexity

81
00:03:32,099 --> 00:03:35,400
and why access is the performance

82
00:03:35,400 --> 00:03:38,220
the complexity of data set has a

83
00:03:38,220 --> 00:03:39,300
significant

84
00:03:39,300 --> 00:03:41,340
effect on membership inference model

85
00:03:41,340 --> 00:03:44,040
inversion and model study

86
00:03:44,040 --> 00:03:45,720
more precisely

87
00:03:45,720 --> 00:03:48,299
more complex data sets lead to better

88
00:03:48,299 --> 00:03:50,640
membership influence but worse model

89
00:03:50,640 --> 00:03:52,500
stealing performance

90
00:03:52,500 --> 00:03:55,379
this is due to the fact that a complex

91
00:03:55,379 --> 00:03:57,840
data set is harder for model to

92
00:03:57,840 --> 00:03:59,220
generalize on

93
00:03:59,220 --> 00:04:02,159
and more prone to our fitting

94
00:04:02,159 --> 00:04:03,900
which results in better membership

95
00:04:03,900 --> 00:04:05,640
inference attacks

96
00:04:05,640 --> 00:04:08,459
whereas when the model is trained on a

97
00:04:08,459 --> 00:04:09,900
complex data set

98
00:04:09,900 --> 00:04:12,599
it's harder for advisory to obtain a

99
00:04:12,599 --> 00:04:15,540
data set with similar complexity to

100
00:04:15,540 --> 00:04:18,120
train their stolen model

101
00:04:18,120 --> 00:04:21,418
and we also observe model inversion is

102
00:04:21,418 --> 00:04:24,600
less Effectiveness on sl10 than on Utica

103
00:04:24,600 --> 00:04:26,100
phase or celebate

104
00:04:26,100 --> 00:04:28,740
whereas there is no strong influence of

105
00:04:28,740 --> 00:04:30,600
data set complexity on attribute

106
00:04:30,600 --> 00:04:32,220
inference

107
00:04:32,220 --> 00:04:35,460
so for the first research question we

108
00:04:35,460 --> 00:04:37,560
have the following conclusion

109
00:04:37,560 --> 00:04:38,699
overall

110
00:04:38,699 --> 00:04:41,639
the complexity of the data set does have

111
00:04:41,639 --> 00:04:43,680
a significant effect on membership

112
00:04:43,680 --> 00:04:45,780
inference model inversion and model

113
00:04:45,780 --> 00:04:47,520
stealing

114
00:04:47,520 --> 00:04:49,620
more complex data sets lead to better

115
00:04:49,620 --> 00:04:51,540
membership inference but worse model

116
00:04:51,540 --> 00:04:54,000
stealing performance

117
00:04:54,000 --> 00:04:56,100
there is no strong influence of data

118
00:04:56,100 --> 00:04:59,699
side complexity or attribute inference

119
00:04:59,699 --> 00:05:01,979
so the second question is what's the

120
00:05:01,979 --> 00:05:03,960
impact of overfeeding on different

121
00:05:03,960 --> 00:05:05,520
attacks

122
00:05:05,520 --> 00:05:08,880
we are adopt two Matrix to quantify our

123
00:05:08,880 --> 00:05:11,220
fitting in each Target Model

124
00:05:11,220 --> 00:05:13,919
the first is overfitting level

125
00:05:13,919 --> 00:05:16,440
this between the difference between the

126
00:05:16,440 --> 00:05:18,540
training accuracy and testing accuracy

127
00:05:18,540 --> 00:05:20,280
of the Target Model

128
00:05:20,280 --> 00:05:22,860
and the second is the number of iBooks

129
00:05:22,860 --> 00:05:26,400
used to train the Target Model

130
00:05:26,400 --> 00:05:28,560
the relation between overfeeding level

131
00:05:28,560 --> 00:05:31,020
and attacks performance is shown in this

132
00:05:31,020 --> 00:05:31,800
figure

133
00:05:31,800 --> 00:05:35,100
first we observe that different data

134
00:05:35,100 --> 00:05:38,280
sets have different overfitting levels

135
00:05:38,280 --> 00:05:40,860
our fitting has a significant impact on

136
00:05:40,860 --> 00:05:43,320
membership influence which is in live

137
00:05:43,320 --> 00:05:45,360
with previous analysis

138
00:05:45,360 --> 00:05:48,720
and models dealing displays a complexity

139
00:05:48,720 --> 00:05:51,000
diff opposite trend

140
00:05:51,000 --> 00:05:53,460
this can be explained by the fact that

141
00:05:53,460 --> 00:05:56,880
an overfeeding model memorizes its

142
00:05:56,880 --> 00:06:00,300
training data set to a large extent

143
00:06:00,300 --> 00:06:03,419
an advisory usually does not have the

144
00:06:03,419 --> 00:06:07,560
ability to get exact training data set

145
00:06:07,560 --> 00:06:11,699
also model your version

146
00:06:11,699 --> 00:06:14,699
also model inversion tend to have better

147
00:06:14,699 --> 00:06:16,740
performance and less over 50 model

148
00:06:16,740 --> 00:06:19,620
except for iPhone nist

149
00:06:19,620 --> 00:06:22,319
we believe this is due to the quality of

150
00:06:22,319 --> 00:06:25,319
gun employed in this attack

151
00:06:25,319 --> 00:06:27,539
for attribute inference we do not

152
00:06:27,539 --> 00:06:29,580
observe a clear relationship between

153
00:06:29,580 --> 00:06:33,440
attack performance and operating level

154
00:06:33,900 --> 00:06:37,440
so for number of epochs First we find

155
00:06:37,440 --> 00:06:39,780
that all attacks performance become

156
00:06:39,780 --> 00:06:42,240
steady after 100 attacks

157
00:06:42,240 --> 00:06:45,780
this is reasonable since 100 iPods are

158
00:06:45,780 --> 00:06:47,220
usually enough to train a good machine

159
00:06:47,220 --> 00:06:48,780
learning model

160
00:06:48,780 --> 00:06:49,979
second

161
00:06:49,979 --> 00:06:52,100
the performance of membership inference

162
00:06:52,100 --> 00:06:56,340
increases from 10 until at 100 efforts

163
00:06:56,340 --> 00:06:58,380
while model stealing shows the opposite

164
00:06:58,380 --> 00:07:00,479
trend

165
00:07:00,479 --> 00:07:02,280
for modeling inversion and attribute

166
00:07:02,280 --> 00:07:04,800
inference the attack performance only

167
00:07:04,800 --> 00:07:07,620
have slightly fluctuations with a

168
00:07:07,620 --> 00:07:10,259
different number of eye perks

169
00:07:10,259 --> 00:07:13,080
so for the second research question we

170
00:07:13,080 --> 00:07:15,000
have the following conclusion

171
00:07:15,000 --> 00:07:17,880
overall overfitting does have a

172
00:07:17,880 --> 00:07:19,440
significant impact on membership

173
00:07:19,440 --> 00:07:21,539
influence modeling immersion and model

174
00:07:21,539 --> 00:07:23,039
stealing

175
00:07:23,039 --> 00:07:25,020
higher all fitting level leads to better

176
00:07:25,020 --> 00:07:27,360
membership influence model stealing is

177
00:07:27,360 --> 00:07:29,280
the opposite

178
00:07:29,280 --> 00:07:31,259
modeling version has better performance

179
00:07:31,259 --> 00:07:34,020
and less overfit models

180
00:07:34,020 --> 00:07:36,120
so the third question is what's the

181
00:07:36,120 --> 00:07:38,400
relationship among different attacks

182
00:07:38,400 --> 00:07:41,039
we calculate Pearson correlation among

183
00:07:41,039 --> 00:07:43,319
different attacks

184
00:07:43,319 --> 00:07:46,560
so these two figures shows that there is

185
00:07:46,560 --> 00:07:48,660
a strong negative correlation between

186
00:07:48,660 --> 00:07:51,780
membership inference and model stealing

187
00:07:51,780 --> 00:07:55,259
it can be minus 0.8

188
00:07:55,259 --> 00:07:57,660
we also observed that an active

189
00:07:57,660 --> 00:07:59,460
correlation between membership inference

190
00:07:59,460 --> 00:08:02,699
and modeling version expect for iPhone

191
00:08:02,699 --> 00:08:05,180
needs

192
00:08:05,460 --> 00:08:07,979
on the other hand there doesn't seem to

193
00:08:07,979 --> 00:08:10,319
be any clear relation between attribute

194
00:08:10,319 --> 00:08:12,780
inference and modeling version

195
00:08:12,780 --> 00:08:15,180
as well as between membership inference

196
00:08:15,180 --> 00:08:17,880
and actual inference

197
00:08:17,880 --> 00:08:21,060
we also use differential privacy and

198
00:08:21,060 --> 00:08:23,039
knowledge distillation as our defense

199
00:08:23,039 --> 00:08:24,360
mechanisms

200
00:08:24,360 --> 00:08:26,520
the reason why we choose these two is

201
00:08:26,520 --> 00:08:28,680
that they have been proposed to find

202
00:08:28,680 --> 00:08:31,259
different types of attacks in machine

203
00:08:31,259 --> 00:08:33,800
learning field

204
00:08:34,440 --> 00:08:37,320
we choose dpsgd by adding gaussian noise

205
00:08:37,320 --> 00:08:39,120
to the gradient

206
00:08:39,120 --> 00:08:41,159
a knowledge distillation transfer the

207
00:08:41,159 --> 00:08:43,440
generalization ability from a large

208
00:08:43,440 --> 00:08:45,779
model to a smaller model without utility

209
00:08:45,779 --> 00:08:48,300
delegation

210
00:08:48,300 --> 00:08:51,839
we show part of DP and KDE results as

211
00:08:51,839 --> 00:08:53,279
the others are similar

212
00:08:53,279 --> 00:08:55,920
from table membership inference attack

213
00:08:55,920 --> 00:09:00,839
accuracy drops from 0.721 to 0.5 which

214
00:09:00,839 --> 00:09:03,180
is random gas

215
00:09:03,180 --> 00:09:04,860
for modeling version and attribute

216
00:09:04,860 --> 00:09:07,860
inference dpsgd can only reduce the

217
00:09:07,860 --> 00:09:11,459
attack accuracy to a small extent

218
00:09:11,459 --> 00:09:14,459
interestingly dpsgd here actually

219
00:09:14,459 --> 00:09:16,620
enhances the performance of model

220
00:09:16,620 --> 00:09:19,560
stealing in exciting

221
00:09:19,560 --> 00:09:23,279
overall dpsgd can effectively defend

222
00:09:23,279 --> 00:09:25,320
against machine membership influence but

223
00:09:25,320 --> 00:09:28,040
not the others

224
00:09:28,260 --> 00:09:30,779
for knowledge distillation we do not

225
00:09:30,779 --> 00:09:33,360
observe any significant decrease in

226
00:09:33,360 --> 00:09:35,700
attack performance for modern erosion

227
00:09:35,700 --> 00:09:39,420
attribute influence and model stealing

228
00:09:39,420 --> 00:09:43,140
so we observe that both dpsgd and KD can

229
00:09:43,140 --> 00:09:45,959
defend some of inference tags

230
00:09:45,959 --> 00:09:48,420
that means we need to study the

231
00:09:48,420 --> 00:09:50,279
trade-offs between the utility and

232
00:09:50,279 --> 00:09:52,200
defense Effectiveness in these two

233
00:09:52,200 --> 00:09:53,820
magnetisms

234
00:09:53,820 --> 00:09:56,640
we find although KD preserves the

235
00:09:56,640 --> 00:09:58,800
utility better the defensive

236
00:09:58,800 --> 00:10:01,140
Effectiveness is not as good as we

237
00:10:01,140 --> 00:10:02,640
expected

238
00:10:02,640 --> 00:10:06,300
dpsud can Define some inference attacks

239
00:10:06,300 --> 00:10:09,120
but we can see from sl10 data set

240
00:10:09,120 --> 00:10:11,640
Target models have very productivity

241
00:10:11,640 --> 00:10:14,160
so these two mechanisms are not General

242
00:10:14,160 --> 00:10:16,019
defenses against all the inference

243
00:10:16,019 --> 00:10:17,580
attacks

244
00:10:17,580 --> 00:10:19,080
so in conclusion

245
00:10:19,080 --> 00:10:21,839
we integrate attacks and defenses into a

246
00:10:21,839 --> 00:10:24,300
reusable modular software called ml

247
00:10:24,300 --> 00:10:25,560
doctor

248
00:10:25,560 --> 00:10:27,959
the complexity of the training data set

249
00:10:27,959 --> 00:10:30,180
and overfitting of the target models

250
00:10:30,180 --> 00:10:32,640
play important roles

251
00:10:32,640 --> 00:10:34,980
the effectiveness of model stealing and

252
00:10:34,980 --> 00:10:37,200
membership influence attacks are

253
00:10:37,200 --> 00:10:39,560
naturally correlated

254
00:10:39,560 --> 00:10:42,480
dpsgd and KD could only mitigate some of

255
00:10:42,480 --> 00:10:44,339
the inference attacks

256
00:10:44,339 --> 00:10:46,320
and that's all for my presentation thank

257
00:10:46,320 --> 00:10:48,980
you for listening

