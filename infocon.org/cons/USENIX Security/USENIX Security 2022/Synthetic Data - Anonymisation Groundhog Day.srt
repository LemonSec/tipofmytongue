1
00:00:08,240 --> 00:00:10,920
hello everyone my name is Teresa and

2
00:00:10,920 --> 00:00:13,019
today I'm presenting our work titled

3
00:00:13,019 --> 00:00:15,540
synthetic data anonymization Groundhog

4
00:00:15,540 --> 00:00:17,039
Day

5
00:00:17,039 --> 00:00:19,020
and to start with I want to give a bit

6
00:00:19,020 --> 00:00:20,880
of context and frame the question we

7
00:00:20,880 --> 00:00:23,039
were trying to answer I'm sure most of

8
00:00:23,039 --> 00:00:24,960
you will be familiar with the basic

9
00:00:24,960 --> 00:00:27,420
dilemma of privacy preserving data

10
00:00:27,420 --> 00:00:29,699
publishing where we want to find an

11
00:00:29,699 --> 00:00:32,520
anonymization mechanism that takes a raw

12
00:00:32,520 --> 00:00:33,660
data set

13
00:00:33,660 --> 00:00:35,719
that contains some sensitive information

14
00:00:35,719 --> 00:00:38,520
and generates a sanitized version of

15
00:00:38,520 --> 00:00:41,160
this data that we can share which on the

16
00:00:41,160 --> 00:00:43,980
one hand retains the utility of the data

17
00:00:43,980 --> 00:00:46,739
for statistical analysis but on the

18
00:00:46,739 --> 00:00:49,559
other hand prevents the leakage of

19
00:00:49,559 --> 00:00:51,960
sensitive information about individuals

20
00:00:51,960 --> 00:00:53,760
like the things that we were concerned

21
00:00:53,760 --> 00:00:56,520
about and the reason why we decided to

22
00:00:56,520 --> 00:00:58,920
use an anonymization mechanism in the

23
00:00:58,920 --> 00:01:00,000
first place

24
00:01:00,000 --> 00:01:02,940
why this is a non-trivial task is

25
00:01:02,940 --> 00:01:04,500
because there is a tension between

26
00:01:04,500 --> 00:01:07,500
preserving information for analysis but

27
00:01:07,500 --> 00:01:09,119
at the same time hide it from

28
00:01:09,119 --> 00:01:10,860
adversaries

29
00:01:10,860 --> 00:01:13,320
this dilemma is what is often referred

30
00:01:13,320 --> 00:01:16,200
to as the Privacy utility trade-off of

31
00:01:16,200 --> 00:01:19,080
privacy preserving data publishing what

32
00:01:19,080 --> 00:01:21,540
we want to achieve as a data holder is

33
00:01:21,540 --> 00:01:25,140
to find an anonymization mechanism that

34
00:01:25,140 --> 00:01:27,479
maximizes this trade-off and gives us

35
00:01:27,479 --> 00:01:30,060
High privacy and utility at the same

36
00:01:30,060 --> 00:01:31,259
time

37
00:01:31,259 --> 00:01:33,420
the problem is that traditional

38
00:01:33,420 --> 00:01:35,640
sanitization techniques have been shown

39
00:01:35,640 --> 00:01:39,180
to provide a very poor trade-off if the

40
00:01:39,180 --> 00:01:41,700
sanitized data retains the utility of

41
00:01:41,700 --> 00:01:44,220
the original data set it provides little

42
00:01:44,220 --> 00:01:47,540
protection against privacy attacks

43
00:01:47,540 --> 00:01:50,759
recently however synthetic data has been

44
00:01:50,759 --> 00:01:53,159
as advertised as a novel data

45
00:01:53,159 --> 00:01:56,460
anonymization solution that enables the

46
00:01:56,460 --> 00:01:58,700
protection of personally identified

47
00:01:58,700 --> 00:02:02,159
identifiable information but preserves

48
00:02:02,159 --> 00:02:04,860
the statistical properties of the data

49
00:02:04,860 --> 00:02:06,719
for analysis

50
00:02:06,719 --> 00:02:09,119
in other words synthetic data is

51
00:02:09,119 --> 00:02:12,000
promised to be a privacy preserving data

52
00:02:12,000 --> 00:02:14,760
publishing mechanism that addresses the

53
00:02:14,760 --> 00:02:17,160
shortcomings of previous anonymization

54
00:02:17,160 --> 00:02:19,620
techniques and provides us with a

55
00:02:19,620 --> 00:02:23,099
perfect privacy utility trade-off

56
00:02:23,099 --> 00:02:25,379
the question is whether This Promise

57
00:02:25,379 --> 00:02:28,020
actually holds true well it is certainly

58
00:02:28,020 --> 00:02:30,780
a very appealing proposition so far we

59
00:02:30,780 --> 00:02:32,819
haven't seen a good thorough evaluation

60
00:02:32,819 --> 00:02:35,580
of this claim there are two open

61
00:02:35,580 --> 00:02:37,980
questions we need to answer in order to

62
00:02:37,980 --> 00:02:41,099
assess this claim first how do we

63
00:02:41,099 --> 00:02:43,440
evaluate the Privacy utility trade-off

64
00:02:43,440 --> 00:02:46,319
of an anonymization mechanism

65
00:02:46,319 --> 00:02:49,140
and then where in this landscape does

66
00:02:49,140 --> 00:02:51,660
synthetic data life is it really the

67
00:02:51,660 --> 00:02:53,700
Holy Grail of privacy preserving data

68
00:02:53,700 --> 00:02:56,819
publishing or is synthetic data more

69
00:02:56,819 --> 00:02:59,640
like traditional sanitization

70
00:02:59,640 --> 00:03:02,400
what I'm presenting today is a framework

71
00:03:02,400 --> 00:03:04,500
that answers these two questions this

72
00:03:04,500 --> 00:03:06,959
framework is also available as an open

73
00:03:06,959 --> 00:03:09,180
source python library and can be used to

74
00:03:09,180 --> 00:03:11,400
evaluate the Privacy gain of any data

75
00:03:11,400 --> 00:03:14,599
anonymization mechanism

76
00:03:15,060 --> 00:03:17,060
to start with let's answer the question

77
00:03:17,060 --> 00:03:20,220
how we quantify the Privacy gain of an

78
00:03:20,220 --> 00:03:23,599
anonymization mechanism

79
00:03:23,640 --> 00:03:26,340
the first important observation that we

80
00:03:26,340 --> 00:03:28,080
made when we tried to answer this

81
00:03:28,080 --> 00:03:30,300
question is that we need to look at

82
00:03:30,300 --> 00:03:32,640
synthetic data as an anonymization

83
00:03:32,640 --> 00:03:33,900
mechanism

84
00:03:33,900 --> 00:03:37,019
that means a way to publish data sets

85
00:03:37,019 --> 00:03:38,700
that are useful for all sorts of

86
00:03:38,700 --> 00:03:40,920
analysis tasks just like what

87
00:03:40,920 --> 00:03:42,840
traditional anonymization tried to

88
00:03:42,840 --> 00:03:44,280
achieve

89
00:03:44,280 --> 00:03:46,620
this observation had three implications

90
00:03:46,620 --> 00:03:49,860
for the framework that we designed

91
00:03:49,860 --> 00:03:51,599
so first

92
00:03:51,599 --> 00:03:54,120
we decided that we need to consider an

93
00:03:54,120 --> 00:03:56,580
adversary that has access to a single

94
00:03:56,580 --> 00:03:59,400
copy of synthetic data rather than query

95
00:03:59,400 --> 00:04:02,040
or white box access to a model

96
00:04:02,040 --> 00:04:04,680
this stands a bit in contrast with some

97
00:04:04,680 --> 00:04:07,440
of the previous evaluation papers that

98
00:04:07,440 --> 00:04:10,200
treated synthetic data publishing more

99
00:04:10,200 --> 00:04:12,540
as another type of machine learning task

100
00:04:12,540 --> 00:04:15,959
and evaluated its privacy as such

101
00:04:15,959 --> 00:04:18,779
second we decided that we need to look

102
00:04:18,779 --> 00:04:21,180
at the risks relevant in the data

103
00:04:21,180 --> 00:04:23,460
sharing scenario which are the risks of

104
00:04:23,460 --> 00:04:25,979
linkability and inference

105
00:04:25,979 --> 00:04:27,360
and last

106
00:04:27,360 --> 00:04:30,060
we need a framework that allows us to

107
00:04:30,060 --> 00:04:32,520
compare synthetic data to traditional

108
00:04:32,520 --> 00:04:35,460
sanitization techniques

109
00:04:35,460 --> 00:04:37,620
based on these observations we

110
00:04:37,620 --> 00:04:40,500
formalized Frameworks that allows data

111
00:04:40,500 --> 00:04:43,139
holders to evaluate synthetic data as a

112
00:04:43,139 --> 00:04:45,780
novel anonymization mechanism for data

113
00:04:45,780 --> 00:04:47,340
publishing

114
00:04:47,340 --> 00:04:50,100
we start by modeling each privacy

115
00:04:50,100 --> 00:04:52,139
concern that we have about publishing

116
00:04:52,139 --> 00:04:55,139
the raw data are as an adversary that

117
00:04:55,139 --> 00:04:58,259
targets an individual record and aims to

118
00:04:58,259 --> 00:05:01,080
infer a secret about this record so for

119
00:05:01,080 --> 00:05:03,900
instance an adversary that already knows

120
00:05:03,900 --> 00:05:05,699
some attributes about a Target record

121
00:05:05,699 --> 00:05:08,040
and tries to find out whether This

122
00:05:08,040 --> 00:05:10,440
Record is present in the sensitive data

123
00:05:10,440 --> 00:05:13,199
set this is the classical linkage attack

124
00:05:13,199 --> 00:05:15,660
scenario which was demonstrated in

125
00:05:15,660 --> 00:05:18,180
practice many times

126
00:05:18,180 --> 00:05:21,479
for each concern we then estimate the

127
00:05:21,479 --> 00:05:23,820
adversary success of making a correct

128
00:05:23,820 --> 00:05:26,280
guess about the target secret given

129
00:05:26,280 --> 00:05:29,460
either access to the raw data are or the

130
00:05:29,460 --> 00:05:31,740
synthetic or standardized data s and

131
00:05:31,740 --> 00:05:33,720
then compare whether publishing s in

132
00:05:33,720 --> 00:05:36,539
place of art substantially reduces the

133
00:05:36,539 --> 00:05:39,120
adversary success

134
00:05:39,120 --> 00:05:41,460
a bit more formally for each privacy

135
00:05:41,460 --> 00:05:44,280
concern we didn't find an adversely

136
00:05:44,280 --> 00:05:47,100
specific Advantage measure that captures

137
00:05:47,100 --> 00:05:49,500
how much risk publishing a raw or

138
00:05:49,500 --> 00:05:51,900
synthetic data set that includes the

139
00:05:51,900 --> 00:05:54,720
target record incurs compared to a data

140
00:05:54,720 --> 00:05:57,720
set without the target

141
00:05:57,720 --> 00:06:00,000
given the advantage measure we can then

142
00:06:00,000 --> 00:06:02,280
quantify the Privacy gain of publishing

143
00:06:02,280 --> 00:06:04,500
the synthetic data in place of the raw

144
00:06:04,500 --> 00:06:07,320
data as the reduction in the adversaries

145
00:06:07,320 --> 00:06:10,080
Advantage when given access to S rather

146
00:06:10,080 --> 00:06:11,880
than our

147
00:06:11,880 --> 00:06:14,160
under some assumptions the Privacy gain

148
00:06:14,160 --> 00:06:17,220
typically ranges between 0 and 1 where a

149
00:06:17,220 --> 00:06:19,020
zero gain means that publishing a

150
00:06:19,020 --> 00:06:21,419
synthetic version of the data actually

151
00:06:21,419 --> 00:06:23,460
does not reduce the adversary's

152
00:06:23,460 --> 00:06:25,860
advantage compared to the scenario in

153
00:06:25,860 --> 00:06:28,380
which we had published a raw data

154
00:06:28,380 --> 00:06:31,319
ideally we want to observe a gain of

155
00:06:31,319 --> 00:06:33,600
want for all records in the Raw data

156
00:06:33,600 --> 00:06:36,539
this indicates that the synthetic data

157
00:06:36,539 --> 00:06:38,699
does provide good protection against the

158
00:06:38,699 --> 00:06:41,819
Privacy leakage of data publishing

159
00:06:41,819 --> 00:06:44,520
a positive gain somewhere between these

160
00:06:44,520 --> 00:06:46,740
two points indicates that publishing s

161
00:06:46,740 --> 00:06:49,560
does reduce the adversary's advantage to

162
00:06:49,560 --> 00:06:51,360
some extent but does not give Perfect

163
00:06:51,360 --> 00:06:53,880
protection

164
00:06:53,880 --> 00:06:56,699
using our framework we run an empirical

165
00:06:56,699 --> 00:06:59,639
evaluation to compare the Privacy gain

166
00:06:59,639 --> 00:07:01,020
of three different types of

167
00:07:01,020 --> 00:07:03,960
anonymization mechanisms on the very

168
00:07:03,960 --> 00:07:06,360
left we test the Privacy gain of a

169
00:07:06,360 --> 00:07:08,819
traditional sanitization procedure that

170
00:07:08,819 --> 00:07:10,860
uses things like suppression and

171
00:07:10,860 --> 00:07:12,780
generalization to protect against

172
00:07:12,780 --> 00:07:14,840
privacy attacks

173
00:07:14,840 --> 00:07:17,940
bayonet is a commonly used generative

174
00:07:17,940 --> 00:07:20,880
model that produces synthetic data and

175
00:07:20,880 --> 00:07:22,860
proof Bay is a differentially private

176
00:07:22,860 --> 00:07:24,419
version of this model

177
00:07:24,419 --> 00:07:26,639
in this part here we're showing the

178
00:07:26,639 --> 00:07:29,220
expected privacy gain for five specific

179
00:07:29,220 --> 00:07:30,840
Target records

180
00:07:30,840 --> 00:07:33,660
what we find is that traditional data

181
00:07:33,660 --> 00:07:36,060
sanitization as expected provides low

182
00:07:36,060 --> 00:07:37,919
gain for some records

183
00:07:37,919 --> 00:07:40,979
publishing synthetic data audio provides

184
00:07:40,979 --> 00:07:43,259
a higher gain and privacy and

185
00:07:43,259 --> 00:07:45,180
differentially private synthetic data

186
00:07:45,180 --> 00:07:46,740
publishing provides us with the best

187
00:07:46,740 --> 00:07:48,500
privacy gate

188
00:07:48,500 --> 00:07:51,419
as a small interesting side note the

189
00:07:51,419 --> 00:07:53,819
second big benefit of an empirical

190
00:07:53,819 --> 00:07:56,819
privacy variation framework is that it

191
00:07:56,819 --> 00:07:59,819
enables us to empirically assess with a

192
00:07:59,819 --> 00:08:01,620
certain synthetic data generation

193
00:08:01,620 --> 00:08:04,440
mechanisms fulfill their formal privacy

194
00:08:04,440 --> 00:08:07,440
guarantees what we show here is the

195
00:08:07,440 --> 00:08:10,020
initial evaluation results for two very

196
00:08:10,020 --> 00:08:12,360
popular the frenchly private generative

197
00:08:12,360 --> 00:08:16,080
models proof Bay and partagan thanks to

198
00:08:16,080 --> 00:08:18,599
the DP guarantees we can calculate a

199
00:08:18,599 --> 00:08:20,879
theoretical lower bound on the expected

200
00:08:20,879 --> 00:08:23,280
privacy gain and check whether this bond

201
00:08:23,280 --> 00:08:24,360
is met

202
00:08:24,360 --> 00:08:26,220
what we found for a number of

203
00:08:26,220 --> 00:08:28,319
implementations is that they actually

204
00:08:28,319 --> 00:08:30,539
violate their formal privacy guarantees

205
00:08:30,539 --> 00:08:33,000
so we first had to fix these formal

206
00:08:33,000 --> 00:08:35,700
violations in the implementations before

207
00:08:35,700 --> 00:08:38,039
we got the expected results and promise

208
00:08:38,039 --> 00:08:40,140
privacy gain

209
00:08:40,140 --> 00:08:42,659
but coming back to our main question we

210
00:08:42,659 --> 00:08:45,060
find that indeed synthetic data from

211
00:08:45,060 --> 00:08:47,040
some models differentially private ones

212
00:08:47,040 --> 00:08:49,740
in particular can provide us with a high

213
00:08:49,740 --> 00:08:52,080
gain in privacy

214
00:08:52,080 --> 00:08:54,779
the important question is whether this

215
00:08:54,779 --> 00:08:57,720
High Gain in privacy can be achieved at

216
00:08:57,720 --> 00:09:00,720
a low cost in utility so that synthetic

217
00:09:00,720 --> 00:09:02,700
data really does provide the perfect

218
00:09:02,700 --> 00:09:05,880
trade-off it is promised to

219
00:09:05,880 --> 00:09:08,640
to answer this question we again first

220
00:09:08,640 --> 00:09:10,620
have to look at what is the desired

221
00:09:10,620 --> 00:09:13,260
function of synthetic data as we

222
00:09:13,260 --> 00:09:15,360
discussed before synthetic data is

223
00:09:15,360 --> 00:09:17,820
advertised as a novel data anonymization

224
00:09:17,820 --> 00:09:21,000
solution that enables data holders to

225
00:09:21,000 --> 00:09:23,160
publish data sets that can act as a

226
00:09:23,160 --> 00:09:26,220
replacement for the raw or anonymized

227
00:09:26,220 --> 00:09:29,459
data the promise of synthetic data is

228
00:09:29,459 --> 00:09:31,680
that it preserves all the benefits of

229
00:09:31,680 --> 00:09:35,160
Micro Data publishing that is full

230
00:09:35,160 --> 00:09:38,040
flexibility over the analysis functions

231
00:09:38,040 --> 00:09:41,519
that we evaluate over the data as well

232
00:09:41,519 --> 00:09:44,279
as to allow us the statistical analysis

233
00:09:44,279 --> 00:09:46,620
of fine brain patterns such as outly

234
00:09:46,620 --> 00:09:49,320
analysis that might be hard to achieve

235
00:09:49,320 --> 00:09:52,860
with other types of data releases

236
00:09:52,860 --> 00:09:55,860
so analogous to the question we ask to

237
00:09:55,860 --> 00:09:58,260
assess the privacy of synthetic data

238
00:09:58,260 --> 00:10:00,720
publishing the Utility side of our

239
00:10:00,720 --> 00:10:03,180
framework tests whether an analyst who

240
00:10:03,180 --> 00:10:06,000
receives s instead of R can still

241
00:10:06,000 --> 00:10:08,760
conduct their analysis equally well to

242
00:10:08,760 --> 00:10:10,920
do so we Define a utility function for

243
00:10:10,920 --> 00:10:12,779
instance a simple machine learning task

244
00:10:12,779 --> 00:10:15,180
and test how much training the model on

245
00:10:15,180 --> 00:10:18,060
S reduces the test accuracy compared to

246
00:10:18,060 --> 00:10:20,160
model trained in r

247
00:10:20,160 --> 00:10:22,200
and already with this simple utility

248
00:10:22,200 --> 00:10:24,720
function we can show that actually

249
00:10:24,720 --> 00:10:27,180
synthetic data does not as promise

250
00:10:27,180 --> 00:10:30,000
provide a much higher gain in privacy at

251
00:10:30,000 --> 00:10:31,740
a lower cost and utility than

252
00:10:31,740 --> 00:10:35,040
traditional sanitization

253
00:10:35,040 --> 00:10:37,680
in summary our empirical evaluation

254
00:10:37,680 --> 00:10:40,440
demonstrates that unfortunately but

255
00:10:40,440 --> 00:10:43,560
maybe unsurprisingly synthetic data is

256
00:10:43,560 --> 00:10:45,839
not the promised suitable of solution to

257
00:10:45,839 --> 00:10:48,660
privacy preserving data publishing

258
00:10:48,660 --> 00:10:50,940
here in the top row we show again the

259
00:10:50,940 --> 00:10:52,920
Privacy gain of generative models with

260
00:10:52,920 --> 00:10:55,740
different privacy guarantees the bottom

261
00:10:55,740 --> 00:10:57,959
row plot then shows the error for a

262
00:10:57,959 --> 00:11:00,480
utility function that compares how well

263
00:11:00,480 --> 00:11:02,160
the synthetic data preserves some

264
00:11:02,160 --> 00:11:05,160
summary statistics of the raw data

265
00:11:05,160 --> 00:11:07,680
as we can see by comparing for instance

266
00:11:07,680 --> 00:11:09,600
the non-private bayonet model to its

267
00:11:09,600 --> 00:11:11,519
differentially private version with an

268
00:11:11,519 --> 00:11:13,920
Epsilon of one a higher gain in privacy

269
00:11:13,920 --> 00:11:16,440
comes at a significant cost and utility

270
00:11:16,440 --> 00:11:19,200
the utility error of the synthetic data

271
00:11:19,200 --> 00:11:20,880
produced by the differential private

272
00:11:20,880 --> 00:11:23,339
models is orders of magnitudes larger

273
00:11:23,339 --> 00:11:25,980
than that of the non-private model

274
00:11:25,980 --> 00:11:28,500
to conclude we present a framework to

275
00:11:28,500 --> 00:11:30,720
evaluate the Privacy utility trade of a

276
00:11:30,720 --> 00:11:33,120
synthetic data publishing and compared

277
00:11:33,120 --> 00:11:34,980
to that of traditional anonymization

278
00:11:34,980 --> 00:11:36,540
mechanisms

279
00:11:36,540 --> 00:11:38,339
isn't that and refined with our

280
00:11:38,339 --> 00:11:40,920
empirical evaluation that synthetic data

281
00:11:40,920 --> 00:11:42,959
is subject to the same limitations as

282
00:11:42,959 --> 00:11:45,180
traditional sanitization

283
00:11:45,180 --> 00:11:46,920
thank you very much for your attention

284
00:11:46,920 --> 00:11:49,320
and please don't hesitate to reach out

285
00:11:49,320 --> 00:11:53,120
to us with any questions you might have

