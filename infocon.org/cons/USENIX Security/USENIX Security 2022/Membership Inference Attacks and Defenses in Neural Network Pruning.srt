1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:11,599 --> 00:00:14,040
Technological University today I will

3
00:00:14,040 --> 00:00:17,160
present our work titled membership

4
00:00:17,160 --> 00:00:19,800
influence at tax and defenses in new

5
00:00:19,800 --> 00:00:22,380
network pruning so this is a joint work

6
00:00:22,380 --> 00:00:24,840
with my colleague Lan Zhang

7
00:00:24,840 --> 00:00:27,300
okay so new network has achieved

8
00:00:27,300 --> 00:00:30,420
outstanding performance in many years in

9
00:00:30,420 --> 00:00:32,759
recent years and with the recent advance

10
00:00:32,759 --> 00:00:35,219
in neural network actually we have

11
00:00:35,219 --> 00:00:37,020
observed that the size of new network

12
00:00:37,020 --> 00:00:39,600
have been largely increased if we go

13
00:00:39,600 --> 00:00:42,480
back and look at the new network 10

14
00:00:42,480 --> 00:00:45,780
years ago in 2012 actually if we want to

15
00:00:45,780 --> 00:00:48,780
train a LX Network we only need six to

16
00:00:48,780 --> 00:00:51,780
one million parameters but in the past

17
00:00:51,780 --> 00:00:53,760
two years actually the size of a new

18
00:00:53,760 --> 00:00:56,820
network has been increased to 10 billion

19
00:00:56,820 --> 00:01:00,420
or even 10 trillion parameters and it

20
00:01:00,420 --> 00:01:02,879
will be really challenging to deploy

21
00:01:02,879 --> 00:01:05,519
such a large size new network on some

22
00:01:05,519 --> 00:01:08,100
small devices on resource constrained

23
00:01:08,100 --> 00:01:11,280
devices like our smartphones our iot

24
00:01:11,280 --> 00:01:14,299
devices due to the limited computational

25
00:01:14,299 --> 00:01:17,220
memory or story resources on these

26
00:01:17,220 --> 00:01:18,060
devices

27
00:01:18,060 --> 00:01:20,939
so one of the promising approach to

28
00:01:20,939 --> 00:01:23,520
address this challenge is neurological

29
00:01:23,520 --> 00:01:26,520
pruning so the basic idea of new network

30
00:01:26,520 --> 00:01:29,340
pruning is that so given a test new

31
00:01:29,340 --> 00:01:31,259
network we remove the Redundant

32
00:01:31,259 --> 00:01:34,020
parameters from distance new network and

33
00:01:34,020 --> 00:01:37,020
it derive us both new network

34
00:01:37,020 --> 00:01:40,079
so there are two goals for this new

35
00:01:40,079 --> 00:01:41,939
network pruning first we want to reduce

36
00:01:41,939 --> 00:01:44,400
the size of new network and speed up the

37
00:01:44,400 --> 00:01:46,320
inference time second we want to

38
00:01:46,320 --> 00:01:48,420
minimize the loss of prediction

39
00:01:48,420 --> 00:01:51,659
performance so to evaluate the the

40
00:01:51,659 --> 00:01:54,180
performance of new neck fully usually we

41
00:01:54,180 --> 00:01:56,579
will use two Matrix so first is the

42
00:01:56,579 --> 00:01:58,799
efficiency second one is prediction

43
00:01:58,799 --> 00:02:01,740
performance such as accuracy so most of

44
00:02:01,740 --> 00:02:04,799
the new network pruning approach

45
00:02:04,799 --> 00:02:06,840
actually want to find a good balance

46
00:02:06,840 --> 00:02:09,119
between this efficiency and the

47
00:02:09,119 --> 00:02:11,459
performance prediction performance

48
00:02:11,459 --> 00:02:15,540
however less attention has been paid to

49
00:02:15,540 --> 00:02:19,020
the Privacy risk of new network pruning

50
00:02:19,020 --> 00:02:21,959
so why do we concerned about privacy in

51
00:02:21,959 --> 00:02:24,660
numerical pruning so first let's take a

52
00:02:24,660 --> 00:02:26,400
look at the new network pruning

53
00:02:26,400 --> 00:02:30,300
pipelines so to derive a sparse new

54
00:02:30,300 --> 00:02:31,980
network first we need to train a

55
00:02:31,980 --> 00:02:34,080
original new network following the

56
00:02:34,080 --> 00:02:37,200
typical training process then we will

57
00:02:37,200 --> 00:02:39,720
conduct pruning which means that we will

58
00:02:39,720 --> 00:02:42,180
remove the Redundant parameters from the

59
00:02:42,180 --> 00:02:44,400
dense new network since most of the

60
00:02:44,400 --> 00:02:46,680
pruning approach actually cannot achieve

61
00:02:46,680 --> 00:02:49,319
the comparable performance prediction

62
00:02:49,319 --> 00:02:52,080
performance with that new network so

63
00:02:52,080 --> 00:02:54,360
next step we want to do is to retrain

64
00:02:54,360 --> 00:02:56,459
the new network on the same training

65
00:02:56,459 --> 00:02:59,760
data set we call it fine tuning and if

66
00:02:59,760 --> 00:03:01,860
we take a look at this fine-tuning step

67
00:03:01,860 --> 00:03:03,780
we will find out that actually in this

68
00:03:03,780 --> 00:03:05,459
functioning step the prune model

69
00:03:05,459 --> 00:03:07,620
actually see the training samples more

70
00:03:07,620 --> 00:03:10,379
often and also if we go back to Lake

71
00:03:10,379 --> 00:03:12,720
token take a look at the pruning process

72
00:03:12,720 --> 00:03:14,760
we will find out that this pruning

73
00:03:14,760 --> 00:03:17,340
process actually enforce a smaller

74
00:03:17,340 --> 00:03:19,800
number of parameters to achieve a

75
00:03:19,800 --> 00:03:21,239
similar accuracy

76
00:03:21,239 --> 00:03:23,340
so which means that these two steps

77
00:03:23,340 --> 00:03:25,379
actually will increase the memorization

78
00:03:25,379 --> 00:03:28,379
of training samples and it will have

79
00:03:28,379 --> 00:03:31,080
ever great privacy risk

80
00:03:31,080 --> 00:03:33,540
so in this work we focus on very

81
00:03:33,540 --> 00:03:35,459
essential privacy attack called the

82
00:03:35,459 --> 00:03:37,620
membership influence attack so in the

83
00:03:37,620 --> 00:03:39,480
membership influence attack the attacker

84
00:03:39,480 --> 00:03:41,400
want to determine whether a given data

85
00:03:41,400 --> 00:03:43,860
sample was used in training so what's

86
00:03:43,860 --> 00:03:45,780
the attack will do is that given a data

87
00:03:45,780 --> 00:03:48,360
sample we will query this data sample to

88
00:03:48,360 --> 00:03:50,879
the new network and get the predicted

89
00:03:50,879 --> 00:03:52,319
confidence

90
00:03:52,319 --> 00:03:54,900
so it will work we asked the following

91
00:03:54,900 --> 00:03:57,000
question so after the process of

92
00:03:57,000 --> 00:04:00,000
training pruning and fine-tuning world

93
00:04:00,000 --> 00:04:01,860
through the new network become more

94
00:04:01,860 --> 00:04:03,840
vulnerable to membership inference

95
00:04:03,840 --> 00:04:04,980
attack

96
00:04:04,980 --> 00:04:07,140
so since most of membership influence

97
00:04:07,140 --> 00:04:09,540
attacks rely on the predicted confidence

98
00:04:09,540 --> 00:04:11,700
of new network so first we will

99
00:04:11,700 --> 00:04:14,879
investigate the confidence of the dance

100
00:04:14,879 --> 00:04:17,880
new network so on the left hand side we

101
00:04:17,880 --> 00:04:19,978
show the confidence level of the grand

102
00:04:19,978 --> 00:04:21,440
choose class

103
00:04:21,440 --> 00:04:24,300
of the dance new network through the

104
00:04:24,300 --> 00:04:26,460
Civil 10 data set to use the death net

105
00:04:26,460 --> 00:04:27,740
architecture

106
00:04:27,740 --> 00:04:31,259
this red vertical curve indicates the

107
00:04:31,259 --> 00:04:33,479
average value of this confidence level

108
00:04:33,479 --> 00:04:36,360
of the members or the training data sets

109
00:04:36,360 --> 00:04:39,360
this uh blue vertical curve indicates

110
00:04:39,360 --> 00:04:41,820
the average value of this confidence

111
00:04:41,820 --> 00:04:44,400
level for the now net members are tested

112
00:04:44,400 --> 00:04:47,940
we can see that there's a gap between

113
00:04:47,940 --> 00:04:50,340
these members and non-members on the

114
00:04:50,340 --> 00:04:53,340
confidence levels and many membership

115
00:04:53,340 --> 00:04:55,800
influence attack actually rely on this

116
00:04:55,800 --> 00:04:58,680
Gap to influence the data sample

117
00:04:58,680 --> 00:05:01,199
membership status

118
00:05:01,199 --> 00:05:03,960
then let's remove 70 percent of

119
00:05:03,960 --> 00:05:06,540
parameters in this test new network and

120
00:05:06,540 --> 00:05:08,820
on the right hand side I showed the

121
00:05:08,820 --> 00:05:11,520
confidence level for this pruned new

122
00:05:11,520 --> 00:05:15,000
network similarly we use this red line

123
00:05:15,000 --> 00:05:17,160
to indicate the average value of

124
00:05:17,160 --> 00:05:20,520
confidence level for the members and the

125
00:05:20,520 --> 00:05:24,060
blue line indicates the the average

126
00:05:24,060 --> 00:05:26,039
value of confidence level for the num

127
00:05:26,039 --> 00:05:28,740
members we can see that actually the

128
00:05:28,740 --> 00:05:31,380
confidence gap between these members and

129
00:05:31,380 --> 00:05:34,740
non-members is increased after pruning

130
00:05:34,740 --> 00:05:38,580
okay similarly we we further invested

131
00:05:38,580 --> 00:05:40,740
another Matrix we call it sensitivity

132
00:05:40,740 --> 00:05:43,680
the sensitivity actually measures how

133
00:05:43,680 --> 00:05:45,300
much the model prediction will be

134
00:05:45,300 --> 00:05:48,000
changed if we slightly change the input

135
00:05:48,000 --> 00:05:50,280
samples for example we add some gaussian

136
00:05:50,280 --> 00:05:52,800
noise to our input samples and then we

137
00:05:52,800 --> 00:05:55,500
calculate the sensitivity of our models

138
00:05:55,500 --> 00:05:58,080
so on the left hand side we show that

139
00:05:58,080 --> 00:06:00,300
for the death models or the original new

140
00:06:00,300 --> 00:06:03,660
network there's the gap between members

141
00:06:03,660 --> 00:06:06,600
and the non-parambers is very small but

142
00:06:06,600 --> 00:06:09,539
if we remove 70 of parameters from

143
00:06:09,539 --> 00:06:12,240
distance uh new network and we can

144
00:06:12,240 --> 00:06:14,460
observe that actually the static gap

145
00:06:14,460 --> 00:06:16,740
between members and non-members is

146
00:06:16,740 --> 00:06:18,300
increased

147
00:06:18,300 --> 00:06:19,400
okay

148
00:06:19,400 --> 00:06:22,380
moreover we take a further look at the

149
00:06:22,380 --> 00:06:25,139
confidence Gap and the synthetic gap for

150
00:06:25,139 --> 00:06:26,340
each class

151
00:06:26,340 --> 00:06:28,680
so on the 11th side we show the

152
00:06:28,680 --> 00:06:31,080
confidence level for the original model

153
00:06:31,080 --> 00:06:35,220
on on different classes in the certain

154
00:06:35,220 --> 00:06:38,039
data set here we show the 10 classes

155
00:06:38,039 --> 00:06:40,560
here and this red curve indicates the

156
00:06:40,560 --> 00:06:42,360
confidence gap between members and

157
00:06:42,360 --> 00:06:45,000
non-members for different classes okay

158
00:06:45,000 --> 00:06:48,180
and the figure on the bottom shows the

159
00:06:48,180 --> 00:06:50,520
confidence level for the pruned models

160
00:06:50,520 --> 00:06:53,759
okay and for the figures on the right

161
00:06:53,759 --> 00:06:56,160
hand side we show the sensitivity gap

162
00:06:56,160 --> 00:06:58,380
between members and members for the

163
00:06:58,380 --> 00:07:01,759
original model and the prune model so

164
00:07:01,759 --> 00:07:04,680
after taking a look at this confidence

165
00:07:04,680 --> 00:07:07,199
Gap and scientific Gap per class we have

166
00:07:07,199 --> 00:07:09,840
two major findings so first both

167
00:07:09,840 --> 00:07:11,819
confidence and the scientific gaps are

168
00:07:11,819 --> 00:07:14,460
increased for most classes after pruning

169
00:07:14,460 --> 00:07:17,400
second The increased Gap actually

170
00:07:17,400 --> 00:07:20,880
differs between among different classes

171
00:07:20,880 --> 00:07:23,819
okay so motivate by these observations

172
00:07:23,819 --> 00:07:26,699
we propose a new attack membership first

173
00:07:26,699 --> 00:07:28,860
attack called Samia or self-attention

174
00:07:28,860 --> 00:07:31,440
membership influence attack so we

175
00:07:31,440 --> 00:07:33,840
hypothesize that the increase the

176
00:07:33,840 --> 00:07:35,340
increased confidence Gap and the

177
00:07:35,340 --> 00:07:37,319
sensitive Gap among different classes

178
00:07:37,319 --> 00:07:39,840
actually can provide the final grade

179
00:07:39,840 --> 00:07:42,599
evidence for membership influence attack

180
00:07:42,599 --> 00:07:45,180
so most existing membership influence

181
00:07:45,180 --> 00:07:46,979
attack actually learns a single

182
00:07:46,979 --> 00:07:48,960
threshold of prediction confidence to

183
00:07:48,960 --> 00:07:51,180
determine the membership status which

184
00:07:51,180 --> 00:07:52,919
might not be sufficient for the new

185
00:07:52,919 --> 00:07:55,199
network pruning so that we introduce

186
00:07:55,199 --> 00:07:57,660
this new network based attack using the

187
00:07:57,660 --> 00:08:00,840
self-attention mechanism or semi called

188
00:08:00,840 --> 00:08:03,960
Samia and this uh self-attention

189
00:08:03,960 --> 00:08:06,539
mechanism is widely used in recent

190
00:08:06,539 --> 00:08:08,520
natural Library processing problems or

191
00:08:08,520 --> 00:08:11,460
models and we leverage stamina to

192
00:08:11,460 --> 00:08:13,919
actually to to find out the specific

193
00:08:13,919 --> 00:08:16,020
confidence and the sensitivity

194
00:08:16,020 --> 00:08:18,419
information that the tax threshold

195
00:08:18,419 --> 00:08:21,660
should pay more attention to okay so so

196
00:08:21,660 --> 00:08:25,160
we skipped the the details of our

197
00:08:25,160 --> 00:08:28,680
semi attacks we refer one to our paper

198
00:08:28,680 --> 00:08:31,379
for more details okay so next let's

199
00:08:31,379 --> 00:08:35,039
evaluate our proposed methods so so in

200
00:08:35,039 --> 00:08:37,679
the evaluation we investigate four

201
00:08:37,679 --> 00:08:39,719
representative new network pruning

202
00:08:39,719 --> 00:08:41,700
approach and the five different kind of

203
00:08:41,700 --> 00:08:44,399
pruning capacity level both from 0.5 to

204
00:08:44,399 --> 00:08:47,480
0.9 0.9 means that we remove 90

205
00:08:47,480 --> 00:08:50,399
parameters from the dense models and

206
00:08:50,399 --> 00:08:53,339
also we compare the same meal with the

207
00:08:53,339 --> 00:08:54,899
six existing membership influence

208
00:08:54,899 --> 00:08:57,540
attacks seven on seven popular data set

209
00:08:57,540 --> 00:09:01,080
and we evaluate our models on the on

210
00:09:01,080 --> 00:09:03,779
four new network architectures here are

211
00:09:03,779 --> 00:09:06,540
our results first we take a look at the

212
00:09:06,540 --> 00:09:08,640
Privacy risks under different pruning

213
00:09:08,640 --> 00:09:11,459
approach and the sparity levels so this

214
00:09:11,459 --> 00:09:13,560
colored curve indicates the attack

215
00:09:13,560 --> 00:09:16,440
accuracy for different pruned approach

216
00:09:16,440 --> 00:09:19,560
then the spotted levels and this black

217
00:09:19,560 --> 00:09:21,560
curve indicates the attacker Eric

218
00:09:21,560 --> 00:09:24,959
accuracy on the original death models

219
00:09:24,959 --> 00:09:27,779
so first we observe that so the most

220
00:09:27,779 --> 00:09:29,700
pruning approach actually results in

221
00:09:29,700 --> 00:09:32,220
increased attacks accuracy which means

222
00:09:32,220 --> 00:09:34,320
that actually the pruning process will

223
00:09:34,320 --> 00:09:36,360
increase the Privacy risk

224
00:09:36,360 --> 00:09:39,839
okay so next we also observe that attack

225
00:09:39,839 --> 00:09:42,480
actually may be decreased and under a

226
00:09:42,480 --> 00:09:45,060
high specific levels say 0.9 means that

227
00:09:45,060 --> 00:09:47,580
we remove 90 percent of the parameters

228
00:09:47,580 --> 00:09:49,920
from the death models and in that case

229
00:09:49,920 --> 00:09:52,980
actually uh so some most models actually

230
00:09:52,980 --> 00:09:55,440
cannot achieve the comparable uh

231
00:09:55,440 --> 00:09:57,839
prediction for accuracy than the death

232
00:09:57,839 --> 00:10:00,779
models which means that if the uh the

233
00:10:00,779 --> 00:10:02,519
prune model can achieve the good

234
00:10:02,519 --> 00:10:04,680
predation performance and the efficiency

235
00:10:04,680 --> 00:10:07,380
usually we will visit will result in

236
00:10:07,380 --> 00:10:11,700
increased privacy risk okay so next we

237
00:10:11,700 --> 00:10:14,220
compared our proposed semi attack with

238
00:10:14,220 --> 00:10:16,080
the existing membership influence attack

239
00:10:16,080 --> 00:10:19,200
so this red curve indicates the attack

240
00:10:19,200 --> 00:10:22,560
actually of our proposed semi-attack and

241
00:10:22,560 --> 00:10:24,839
the curve in other colors in state the

242
00:10:24,839 --> 00:10:27,720
attack RC of other existing membership

243
00:10:27,720 --> 00:10:29,880
influence attack we found out that in

244
00:10:29,880 --> 00:10:33,060
most cases semi actually outperforms the

245
00:10:33,060 --> 00:10:34,920
existing membership influence attack on

246
00:10:34,920 --> 00:10:37,440
different pruning approach and sparsity

247
00:10:37,440 --> 00:10:39,480
levels

248
00:10:39,480 --> 00:10:42,300
so uh so one of the reason why the

249
00:10:42,300 --> 00:10:44,220
stamina has a good performance it

250
00:10:44,220 --> 00:10:46,200
because Samuel can actually better

251
00:10:46,200 --> 00:10:49,380
Leverage The increased confidence Gap

252
00:10:49,380 --> 00:10:51,420
and authentic Gap introduced by neural

253
00:10:51,420 --> 00:10:54,060
network pruning so that we here we

254
00:10:54,060 --> 00:10:55,860
present the relationship between these

255
00:10:55,860 --> 00:10:58,740
two gaps and the attack accuracy on the

256
00:10:58,740 --> 00:11:00,660
left-hand side figure we show the

257
00:11:00,660 --> 00:11:02,579
relationship between the confidence Gap

258
00:11:02,579 --> 00:11:05,339
and the attack accuracy on the right

259
00:11:05,339 --> 00:11:08,040
hand side we show the sensitive the

260
00:11:08,040 --> 00:11:10,440
relationship between centigap and attack

261
00:11:10,440 --> 00:11:12,839
accuracy we can see there's a very

262
00:11:12,839 --> 00:11:15,420
strong correlation between these gaps

263
00:11:15,420 --> 00:11:18,480
and the attack accuracy which means that

264
00:11:18,480 --> 00:11:21,779
the increased gaps introduced by this

265
00:11:21,779 --> 00:11:24,420
new network pruning actually will result

266
00:11:24,420 --> 00:11:27,120
in the higher privacy risk

267
00:11:27,120 --> 00:11:29,640
okay so far we talked about the attack

268
00:11:29,640 --> 00:11:32,220
side so next I will briefly introduce

269
00:11:32,220 --> 00:11:34,560
the defense our defense mechanism we

270
00:11:34,560 --> 00:11:37,620
call it peer-based posterior balance

271
00:11:37,620 --> 00:11:40,620
defense or PPP defense to against the

272
00:11:40,620 --> 00:11:42,839
membership inference attack so the basic

273
00:11:42,839 --> 00:11:45,540
idea is that since we have the increased

274
00:11:45,540 --> 00:11:47,640
gaps introduced by the new network

275
00:11:47,640 --> 00:11:50,640
pruning so in the in our defense we just

276
00:11:50,640 --> 00:11:53,220
want to align the posterior distribution

277
00:11:53,220 --> 00:11:56,040
of predictions of different uh input

278
00:11:56,040 --> 00:11:58,800
samples to mitigate such new prediction

279
00:11:58,800 --> 00:12:01,140
behaviors so okay I will skip the

280
00:12:01,140 --> 00:12:05,399
details for for the defense please refer

281
00:12:05,399 --> 00:12:08,640
to our paper for more details so here is

282
00:12:08,640 --> 00:12:11,700
our results so as I showed you before so

283
00:12:11,700 --> 00:12:14,880
the confidence Gap will be increased of

284
00:12:14,880 --> 00:12:16,860
reduce the pruning but after we apply

285
00:12:16,860 --> 00:12:19,200
the PVD defense actually such graph will

286
00:12:19,200 --> 00:12:23,220
be reduced similarly and if we invested

287
00:12:23,220 --> 00:12:25,620
the synthetic Gap so the Centric Gap

288
00:12:25,620 --> 00:12:28,560
will be increased after pruning but will

289
00:12:28,560 --> 00:12:32,399
be reduced after our PB defense so we

290
00:12:32,399 --> 00:12:35,700
compared our defense with three existing

291
00:12:35,700 --> 00:12:38,339
defense using early stopping or

292
00:12:38,339 --> 00:12:39,959
differential privacy or diversary

293
00:12:39,959 --> 00:12:41,820
regularization we found out that

294
00:12:41,820 --> 00:12:42,740
actually

295
00:12:42,740 --> 00:12:45,899
PVP defense can outperform the existing

296
00:12:45,899 --> 00:12:48,779
defenses and achieve a better trade-off

297
00:12:48,779 --> 00:12:50,940
between the prediction accuracy and the

298
00:12:50,940 --> 00:12:54,540
model privacy so to conclude our work so

299
00:12:54,540 --> 00:12:57,660
we identified that so we have developed

300
00:12:57,660 --> 00:12:59,399
that the new network pruning actual wall

301
00:12:59,399 --> 00:13:01,399
aggregate the

302
00:13:01,399 --> 00:13:04,260
Privacy risk of the original model due

303
00:13:04,260 --> 00:13:06,600
to the increased confidence Gap and the

304
00:13:06,600 --> 00:13:09,060
center Gap so we propose stamina to

305
00:13:09,060 --> 00:13:10,760
predict membership influence attack

306
00:13:10,760 --> 00:13:13,440
status by using the final grade

307
00:13:13,440 --> 00:13:16,079
prediction Matrix and the semi has the

308
00:13:16,079 --> 00:13:18,959
advantage in identifying the prune model

309
00:13:18,959 --> 00:13:21,660
prediction Divergence and also we

310
00:13:21,660 --> 00:13:24,420
propose the PVP defense to mitigate such

311
00:13:24,420 --> 00:13:26,880
piracy Risk by narrowing down the

312
00:13:26,880 --> 00:13:30,300
divergences of the posterior predictions

313
00:13:30,300 --> 00:13:33,360
so that's pretty much about our work so

314
00:13:33,360 --> 00:13:35,639
please refer to the extended version for

315
00:13:35,639 --> 00:13:37,680
more details so we post the extent

316
00:13:37,680 --> 00:13:40,200
version on archive and also we release

317
00:13:40,200 --> 00:13:43,019
our code so you can access code by by

318
00:13:43,019 --> 00:13:45,779
scan the barcode on the right hand side

319
00:13:45,779 --> 00:13:49,160
thank you for your listening

