1
00:00:09,120 --> 00:00:11,040
um so automated detection of automated

2
00:00:11,040 --> 00:00:12,000
traffic

3
00:00:12,000 --> 00:00:13,620
um the problem I want to talk about is

4
00:00:13,620 --> 00:00:15,599
you have a web service which attracts

5
00:00:15,599 --> 00:00:17,699
human traffic which you want but it also

6
00:00:17,699 --> 00:00:19,859
attracts bot traffic which

7
00:00:19,859 --> 00:00:21,600
um you know scripted automated behaviors

8
00:00:21,600 --> 00:00:23,400
from Bots which try to abuse the service

9
00:00:23,400 --> 00:00:26,160
and you know are not what you intend a

10
00:00:26,160 --> 00:00:27,240
good example would be something like

11
00:00:27,240 --> 00:00:29,160
sign up abuse where we want people to

12
00:00:29,160 --> 00:00:30,480
sign up for Hotmail accounts and Gmail

13
00:00:30,480 --> 00:00:31,980
accounts and Facebook accounts and stuff

14
00:00:31,980 --> 00:00:33,899
like that but we do not want the Bots

15
00:00:33,899 --> 00:00:35,399
that try to sign up for tens of

16
00:00:35,399 --> 00:00:36,780
thousands and hundreds of thousands of

17
00:00:36,780 --> 00:00:39,059
accounts and just abuse the service and

18
00:00:39,059 --> 00:00:40,980
other examples are you know fake

19
00:00:40,980 --> 00:00:43,260
engagement on social media click fraud

20
00:00:43,260 --> 00:00:44,820
and stuff like that

21
00:00:44,820 --> 00:00:47,100
now the problem is challenging for a

22
00:00:47,100 --> 00:00:48,420
couple of reasons

23
00:00:48,420 --> 00:00:50,820
um one is there are no labels and there

24
00:00:50,820 --> 00:00:52,320
will be no labels I mean we have to make

25
00:00:52,320 --> 00:00:54,360
our piece with that fact um and that

26
00:00:54,360 --> 00:00:56,340
means that all of the supervising

27
00:00:56,340 --> 00:00:58,020
learning algorithms that we know about

28
00:00:58,020 --> 00:01:00,660
logistic regression and you know svms

29
00:01:00,660 --> 00:01:02,039
and neural networks and all that great

30
00:01:02,039 --> 00:01:04,379
stuff and great stuff it is but it's off

31
00:01:04,379 --> 00:01:06,119
the table we simply can't use it um we

32
00:01:06,119 --> 00:01:07,200
don't have labels and we're not going to

33
00:01:07,200 --> 00:01:08,820
get labels

34
00:01:08,820 --> 00:01:11,479
um equally troubling is that most of the

35
00:01:11,479 --> 00:01:13,740
unsupervised algorithms that you might

36
00:01:13,740 --> 00:01:15,000
know about

37
00:01:15,000 --> 00:01:17,700
um like k-means and clustering methods

38
00:01:17,700 --> 00:01:19,619
and isolation for us and stuff like that

39
00:01:19,619 --> 00:01:22,140
most of these things rely on taking

40
00:01:22,140 --> 00:01:24,960
um numeric uh numeric features as input

41
00:01:24,960 --> 00:01:27,119
that is to say they expect inputs which

42
00:01:27,119 --> 00:01:29,580
are vectors of floating Point numeric

43
00:01:29,580 --> 00:01:31,020
features we don't have that well we have

44
00:01:31,020 --> 00:01:33,180
categorical features you're if you're

45
00:01:33,180 --> 00:01:34,920
looking at web logs your browser version

46
00:01:34,920 --> 00:01:37,380
is a string the geolocations such as

47
00:01:37,380 --> 00:01:38,880
state or city or something like that as

48
00:01:38,880 --> 00:01:41,100
a string refer user age and stuff like

49
00:01:41,100 --> 00:01:42,720
that these are strings so we don't have

50
00:01:42,720 --> 00:01:44,820
numeric inputs so basically all of the

51
00:01:44,820 --> 00:01:46,560
the good you know unsupervised methods

52
00:01:46,560 --> 00:01:48,540
are all kind of off the table also

53
00:01:48,540 --> 00:01:50,820
and then finally the stuff you might try

54
00:01:50,820 --> 00:01:52,799
and normally detection sort of relies on

55
00:01:52,799 --> 00:01:54,420
the heuristic that

56
00:01:54,420 --> 00:01:54,979
um

57
00:01:54,979 --> 00:01:57,540
your historical distribution might be

58
00:01:57,540 --> 00:01:59,520
assumed to be mostly good and that the

59
00:01:59,520 --> 00:02:01,380
the bad stuff that you're going to see

60
00:02:01,380 --> 00:02:03,780
are rare outliers that deviate

61
00:02:03,780 --> 00:02:05,280
significantly from your historical

62
00:02:05,280 --> 00:02:06,719
distribution that's not even

63
00:02:06,719 --> 00:02:08,399
approximately satisfied here either

64
00:02:08,399 --> 00:02:11,160
because for for many of these problems

65
00:02:11,160 --> 00:02:13,260
like sign up abuse and like click fraud

66
00:02:13,260 --> 00:02:14,760
and stuff like that the levels of abuse

67
00:02:14,760 --> 00:02:16,500
that we expect to see could be ten

68
00:02:16,500 --> 00:02:18,360
percent it could be 30 they could be 70

69
00:02:18,360 --> 00:02:20,280
percent of overall traffic so relying on

70
00:02:20,280 --> 00:02:22,800
a heuristic that most of what I see is

71
00:02:22,800 --> 00:02:23,640
um

72
00:02:23,640 --> 00:02:26,400
um is minor deviations is is not a good

73
00:02:26,400 --> 00:02:27,780
heuristic

74
00:02:27,780 --> 00:02:28,620
um

75
00:02:28,620 --> 00:02:30,840
so so what I'm going to try to do is I'm

76
00:02:30,840 --> 00:02:31,980
going to try to estimate the clean

77
00:02:31,980 --> 00:02:33,599
distribution and then punish deviations

78
00:02:33,599 --> 00:02:36,060
from it so imagine for example that bad

79
00:02:36,060 --> 00:02:38,340
guys took a day off and there was no

80
00:02:38,340 --> 00:02:39,900
attack traffic during that day off well

81
00:02:39,900 --> 00:02:41,459
what I would do is I would snapshot all

82
00:02:41,459 --> 00:02:43,680
my distributions during that time learn

83
00:02:43,680 --> 00:02:45,780
the clean distribution and then when bad

84
00:02:45,780 --> 00:02:47,760
guys come back punish deviations from

85
00:02:47,760 --> 00:02:50,640
that known clean so writing it here as

86
00:02:50,640 --> 00:02:52,140
you know observed is equal to Alpha

87
00:02:52,140 --> 00:02:54,060
times clean plus one minus Alpha times

88
00:02:54,060 --> 00:02:56,459
bot where my features are X and Y if I

89
00:02:56,459 --> 00:02:58,019
knew what clean was then just some

90
00:02:58,019 --> 00:02:59,879
moving things around the odds of any

91
00:02:59,879 --> 00:03:01,680
particular thing being bought is just

92
00:03:01,680 --> 00:03:03,720
given by this right hand side over here

93
00:03:03,720 --> 00:03:05,819
observed over

94
00:03:05,819 --> 00:03:07,980
um Over clean minus minus one basically

95
00:03:07,980 --> 00:03:09,720
and I could output a set of rules that

96
00:03:09,720 --> 00:03:12,000
tells me for any given observation what

97
00:03:12,000 --> 00:03:12,739
um

98
00:03:12,739 --> 00:03:15,420
what uh what the odds are being bought

99
00:03:15,420 --> 00:03:16,800
on well of course that's not going to

100
00:03:16,800 --> 00:03:17,819
happen bad because I'm not going to take

101
00:03:17,819 --> 00:03:19,200
a day off and we're not going to be able

102
00:03:19,200 --> 00:03:20,940
to you know get the clean distributions

103
00:03:20,940 --> 00:03:23,280
in a nice in a nice easy way like that

104
00:03:23,280 --> 00:03:26,400
but suppose however

105
00:03:26,400 --> 00:03:28,620
suppose however I were able to identify

106
00:03:28,620 --> 00:03:31,080
subsets of traffic which were fairly

107
00:03:31,080 --> 00:03:32,760
free of attack traffic

108
00:03:32,760 --> 00:03:34,260
and now we're able to estimate clean

109
00:03:34,260 --> 00:03:36,300
from that so here I'm going to introduce

110
00:03:36,300 --> 00:03:37,680
the two main assumptions in this paper

111
00:03:37,680 --> 00:03:39,900
one is that I have pairs of features X

112
00:03:39,900 --> 00:03:41,220
and Y which are independent

113
00:03:41,220 --> 00:03:42,659
statistically independent of each other

114
00:03:42,659 --> 00:03:44,700
a good example might be say browser

115
00:03:44,700 --> 00:03:47,879
version and geolocation within the US I

116
00:03:47,879 --> 00:03:50,099
expect the location in the good traffic

117
00:03:50,099 --> 00:03:52,739
of let's say browser version in Oregon

118
00:03:52,739 --> 00:03:54,659
to be the same as it would be in Georgia

119
00:03:54,659 --> 00:03:57,000
or Iowa or or anywhere else I don't

120
00:03:57,000 --> 00:03:58,920
expect that that changes in the clean

121
00:03:58,920 --> 00:04:00,239
traffic

122
00:04:00,239 --> 00:04:02,760
um all that much that is to say Chrome

123
00:04:02,760 --> 00:04:04,440
is not that much more likely to be used

124
00:04:04,440 --> 00:04:06,180
by people in one state you know versus

125
00:04:06,180 --> 00:04:07,200
another

126
00:04:07,200 --> 00:04:10,260
and the second is that in my in my in

127
00:04:10,260 --> 00:04:13,560
one of those features let's say State

128
00:04:13,560 --> 00:04:16,918
um uh I can ident there are unattacked

129
00:04:16,918 --> 00:04:18,660
bins there are some bins that receive

130
00:04:18,660 --> 00:04:21,238
zero or negligible attack traffic so for

131
00:04:21,238 --> 00:04:23,220
example if in 50 states in the US the

132
00:04:23,220 --> 00:04:24,960
boards for example Oregon is unattact

133
00:04:24,960 --> 00:04:28,560
well if I plug that in that would allow

134
00:04:28,560 --> 00:04:30,060
me to calculate the clean distribution

135
00:04:30,060 --> 00:04:32,100
that is to say my observed here is Alpha

136
00:04:32,100 --> 00:04:33,780
times clean plus one minus Alpha times

137
00:04:33,780 --> 00:04:36,479
bot if I have if Oregon were unattacked

138
00:04:36,479 --> 00:04:38,340
and I evaluated this equation for Oregon

139
00:04:38,340 --> 00:04:41,280
well this read this bot term here simply

140
00:04:41,280 --> 00:04:43,380
drops out and it tells me that observed

141
00:04:43,380 --> 00:04:45,300
is simply equal to the clean so I get

142
00:04:45,300 --> 00:04:47,280
the clean distribution from The observed

143
00:04:47,280 --> 00:04:49,560
distribution if I have if I'm staring at

144
00:04:49,560 --> 00:04:51,300
a bucket that is um that is on attack

145
00:04:51,300 --> 00:04:53,340
okay that's nice that that feels like

146
00:04:53,340 --> 00:04:55,560
the beginnings of progress okay but two

147
00:04:55,560 --> 00:04:56,880
questions before I you know go any

148
00:04:56,880 --> 00:04:59,340
further is you know a why would there be

149
00:04:59,340 --> 00:05:01,500
on attack bins and be how would I find

150
00:05:01,500 --> 00:05:03,000
them so on the first of those you know

151
00:05:03,000 --> 00:05:05,000
if you look at say a feature like State

152
00:05:05,000 --> 00:05:08,280
well attackers might have a lot of you

153
00:05:08,280 --> 00:05:09,840
know bandwidth and attack traffic coming

154
00:05:09,840 --> 00:05:11,880
from States like Virginia and California

155
00:05:11,880 --> 00:05:13,440
and places like that where they have a

156
00:05:13,440 --> 00:05:15,720
lot of bandwidth where there are AWS and

157
00:05:15,720 --> 00:05:17,520
you know Azure data servers and and

158
00:05:17,520 --> 00:05:18,840
stuff like that

159
00:05:18,840 --> 00:05:20,340
but they don't necessarily have coverage

160
00:05:20,340 --> 00:05:22,199
in all 50 states there might be places

161
00:05:22,199 --> 00:05:24,240
like West Virginia and Iowa and you know

162
00:05:24,240 --> 00:05:25,620
who knows what else that that are

163
00:05:25,620 --> 00:05:27,120
relatively on attack so this is you know

164
00:05:27,120 --> 00:05:28,979
unless they have coverage across the

165
00:05:28,979 --> 00:05:32,280
entirety of your of your um your space

166
00:05:32,280 --> 00:05:35,100
the the first assumption is met so and

167
00:05:35,100 --> 00:05:36,720
how how would we find those buckets I

168
00:05:36,720 --> 00:05:38,220
mean even if Oregon is on a tactic could

169
00:05:38,220 --> 00:05:39,840
be staring at the distribution but I I

170
00:05:39,840 --> 00:05:42,539
wouldn't quite know it let's get to that

171
00:05:42,539 --> 00:05:45,720
next which is okay as I said if Oregon

172
00:05:45,720 --> 00:05:48,120
is unattacked it gives me the clean

173
00:05:48,120 --> 00:05:51,300
distribution of browser in in in in in

174
00:05:51,300 --> 00:05:54,539
that in that bucket well suppose I had

175
00:05:54,539 --> 00:05:56,039
more than one bucket that was on a

176
00:05:56,039 --> 00:05:58,740
taxables Oregon and Iowa and Georgia all

177
00:05:58,740 --> 00:06:01,020
receive no or very little attack traffic

178
00:06:01,020 --> 00:06:03,479
well then all of them would point to the

179
00:06:03,479 --> 00:06:04,919
same distribution they pointed the claim

180
00:06:04,919 --> 00:06:06,360
distribution there's only one clean

181
00:06:06,360 --> 00:06:07,979
distribution since they're all unattact

182
00:06:07,979 --> 00:06:10,080
they're all pointing at the same thing

183
00:06:10,080 --> 00:06:11,820
the central result of this paper is

184
00:06:11,820 --> 00:06:13,800
basically that the converse is also true

185
00:06:13,800 --> 00:06:17,039
is that if I have multiple bins that are

186
00:06:17,039 --> 00:06:18,479
pointing at the same distribution

187
00:06:18,479 --> 00:06:20,160
they're pointing at the clean

188
00:06:20,160 --> 00:06:22,199
distribution that is to say if I find

189
00:06:22,199 --> 00:06:23,880
that the distribution of browserversion

190
00:06:23,880 --> 00:06:26,699
in Oregon and Iowa and George are the

191
00:06:26,699 --> 00:06:28,860
same believe it or not that is the clean

192
00:06:28,860 --> 00:06:30,539
distribution that you're looking at it's

193
00:06:30,539 --> 00:06:32,400
did a little more formally it's say it a

194
00:06:32,400 --> 00:06:33,419
little more formally in the paper but

195
00:06:33,419 --> 00:06:36,000
basically and there's no real additional

196
00:06:36,000 --> 00:06:38,340
you know strong assumptions there the

197
00:06:38,340 --> 00:06:40,680
only assumption is that the distribution

198
00:06:40,680 --> 00:06:42,720
the traffic distribution from the bot

199
00:06:42,720 --> 00:06:44,400
traffic is not the same as the traffic

200
00:06:44,400 --> 00:06:45,960
distribution from the clean traffic

201
00:06:45,960 --> 00:06:47,880
which if where you're kind of you know

202
00:06:47,880 --> 00:06:50,100
done anyway so basically this gives me a

203
00:06:50,100 --> 00:06:53,220
mechanism to find the clean distribution

204
00:06:53,220 --> 00:06:55,860
by finding marginals that point to the

205
00:06:55,860 --> 00:06:58,560
same thing a matrix representation of

206
00:06:58,560 --> 00:07:01,620
this kind of gels well with the um with

207
00:07:01,620 --> 00:07:03,060
the implementation so let me just do

208
00:07:03,060 --> 00:07:05,100
that so let's instead of writing it in

209
00:07:05,100 --> 00:07:06,840
algebra as I did before you know write

210
00:07:06,840 --> 00:07:09,240
it as a matrix The Joint distribution of

211
00:07:09,240 --> 00:07:11,220
X versus Y is as a matrix and on the

212
00:07:11,220 --> 00:07:12,960
left hand side that would be what I

213
00:07:12,960 --> 00:07:15,240
observe and The observed is equal to

214
00:07:15,240 --> 00:07:17,520
Alpha times clean plus one minus Alpha

215
00:07:17,520 --> 00:07:19,979
Times Bar the the X Y the the

216
00:07:19,979 --> 00:07:22,380
distribution Matrix observed is equal to

217
00:07:22,380 --> 00:07:24,180
the distribution Matrix of Clean Plus

218
00:07:24,180 --> 00:07:26,759
the distribution of Matrix of Pi now the

219
00:07:26,759 --> 00:07:29,220
distribution Matrix of clean as a matrix

220
00:07:29,220 --> 00:07:31,620
this is rank one why is it rank one

221
00:07:31,620 --> 00:07:33,419
because remember we said X and Y are

222
00:07:33,419 --> 00:07:34,560
independent that's what independent

223
00:07:34,560 --> 00:07:37,020
means the rows and all the rows and all

224
00:07:37,020 --> 00:07:38,880
the columns of that Matrix are going to

225
00:07:38,880 --> 00:07:40,979
be linearly independent but what You

226
00:07:40,979 --> 00:07:42,539
observe is not going to be rank one

227
00:07:42,539 --> 00:07:44,580
because it's contaminated by this other

228
00:07:44,580 --> 00:07:47,639
this other additive component uh you

229
00:07:47,639 --> 00:07:49,560
know which is the distribution Matrix of

230
00:07:49,560 --> 00:07:53,340
the bot traffic now if we're we do have

231
00:07:53,340 --> 00:07:56,759
unattacked bins of this of this

232
00:07:56,759 --> 00:07:59,819
um uh from bot traffic how that would

233
00:07:59,819 --> 00:08:02,520
show up in The Matrix notation is this

234
00:08:02,520 --> 00:08:05,160
um x times y distribution Matrix of the

235
00:08:05,160 --> 00:08:07,199
bot traffic would have some columns that

236
00:08:07,199 --> 00:08:10,139
are purely zero and what that means is

237
00:08:10,139 --> 00:08:13,380
that while this overall observed Matrix

238
00:08:13,380 --> 00:08:16,139
is not rank one it does have some subset

239
00:08:16,139 --> 00:08:18,180
of columns that are rank one and again

240
00:08:18,180 --> 00:08:19,319
it's dated you know a little more

241
00:08:19,319 --> 00:08:20,639
formally and algorithmically in the

242
00:08:20,639 --> 00:08:24,599
paper but to skip right to the punch the

243
00:08:24,599 --> 00:08:28,860
um the distribution of X and clean is

244
00:08:28,860 --> 00:08:31,560
given by the span of the largest rank

245
00:08:31,560 --> 00:08:33,899
one subset of columns of that observed

246
00:08:33,899 --> 00:08:35,339
distribution Matrix you just take a

247
00:08:35,339 --> 00:08:38,700
pivot Matrix of X versus Y and the clean

248
00:08:38,700 --> 00:08:40,799
distribution of X is the span of the

249
00:08:40,799 --> 00:08:42,599
largest rank one subset of columns that

250
00:08:42,599 --> 00:08:44,360
you can that you can find

251
00:08:44,360 --> 00:08:46,440
written in pseudocode it goes something

252
00:08:46,440 --> 00:08:50,100
like this this um this first routine

253
00:08:50,100 --> 00:08:52,200
here get cleans is

254
00:08:52,200 --> 00:08:54,779
this is to find the clean distributions

255
00:08:54,779 --> 00:08:56,640
and it just Loops over all of the

256
00:08:56,640 --> 00:08:57,660
features that you're interested in

257
00:08:57,660 --> 00:08:59,100
suppose I have a set of features you

258
00:08:59,100 --> 00:09:02,339
know WXYZ whatever I Loop for f in every

259
00:09:02,339 --> 00:09:04,800
one of those features form a pivot table

260
00:09:04,800 --> 00:09:09,000
for f which is where f is the index and

261
00:09:09,000 --> 00:09:11,760
the columns are all the bins of all the

262
00:09:11,760 --> 00:09:13,080
features that are statistically

263
00:09:13,080 --> 00:09:15,180
independent of

264
00:09:15,180 --> 00:09:15,839
um

265
00:09:15,839 --> 00:09:20,399
of F and then find the cluster of the

266
00:09:20,399 --> 00:09:23,580
largest the largest cluster of

267
00:09:23,580 --> 00:09:26,399
um uh of linearly dependent columns that

268
00:09:26,399 --> 00:09:27,420
you can find that's your clean

269
00:09:27,420 --> 00:09:29,160
distribution and then using the clean

270
00:09:29,160 --> 00:09:30,839
distribution is simply a matter of

271
00:09:30,839 --> 00:09:33,600
pushing through that that odds ratio

272
00:09:33,600 --> 00:09:35,279
that I that that I gave in the beginning

273
00:09:35,279 --> 00:09:37,380
the odds for any particular observation

274
00:09:37,380 --> 00:09:40,260
is the observed divided by Clean minus

275
00:09:40,260 --> 00:09:43,260
one for any observation and overall the

276
00:09:43,260 --> 00:09:45,240
whole thing is just take something for

277
00:09:45,240 --> 00:09:48,240
example a set of web logs in run it on

278
00:09:48,240 --> 00:09:50,880
on this algorithm and then the output is

279
00:09:50,880 --> 00:09:52,680
the estimated odds of being bought so it

280
00:09:52,680 --> 00:09:54,660
you know if my features here or country

281
00:09:54,660 --> 00:09:56,940
State browser and and organization

282
00:09:56,940 --> 00:09:58,560
something like that it'll output you

283
00:09:58,560 --> 00:10:00,180
know floating Point numbers that are the

284
00:10:00,180 --> 00:10:04,980
estimated odds odds of being bottom okay

285
00:10:04,980 --> 00:10:08,160
um so then to evaluation

286
00:10:08,160 --> 00:10:09,120
um

287
00:10:09,120 --> 00:10:11,459
um as I said label data sets in this

288
00:10:11,459 --> 00:10:13,380
space are extremely rare as a matter of

289
00:10:13,380 --> 00:10:15,240
fact they're almost done we're lucky to

290
00:10:15,240 --> 00:10:16,680
be able to find

291
00:10:16,680 --> 00:10:20,279
um one uh set of web server logs from a

292
00:10:20,279 --> 00:10:22,800
five years worth of logs from a a

293
00:10:22,800 --> 00:10:24,839
smallish site that publishes their their

294
00:10:24,839 --> 00:10:26,839
server logs but also interestingly

295
00:10:26,839 --> 00:10:30,120
publishes their their manually curated

296
00:10:30,120 --> 00:10:32,760
firewall decisions so we actually have a

297
00:10:32,760 --> 00:10:34,560
proxy set of labels from the firewall

298
00:10:34,560 --> 00:10:36,120
decisions that can lead us at least for

299
00:10:36,120 --> 00:10:38,519
evaluation purposes as to what you know

300
00:10:38,519 --> 00:10:39,240
whether

301
00:10:39,240 --> 00:10:40,500
um you know we're going in the right

302
00:10:40,500 --> 00:10:43,260
direction or not and so and this would

303
00:10:43,260 --> 00:10:45,720
be give us an an underestimate meaning

304
00:10:45,720 --> 00:10:48,060
that um you know the firewall probably

305
00:10:48,060 --> 00:10:50,760
didn't capture everything so our our Roc

306
00:10:50,760 --> 00:10:53,880
curve here would be this is the strictly

307
00:10:53,880 --> 00:10:55,980
worst RSC curve that that that that that

308
00:10:55,980 --> 00:10:58,019
is possible but long story short using

309
00:10:58,019 --> 00:11:00,600
four features browser family status and

310
00:11:00,600 --> 00:11:02,820
the the path of the the requested file

311
00:11:02,820 --> 00:11:04,920
the area under the curve that this this

312
00:11:04,920 --> 00:11:07,140
algorithm finds this point you know zero

313
00:11:07,140 --> 00:11:09,180
eight seven and that's the black curve

314
00:11:09,180 --> 00:11:10,980
here and isolation Forest which is a

315
00:11:10,980 --> 00:11:13,260
very kind of prominent um uh

316
00:11:13,260 --> 00:11:15,660
unsupervised method you know essentially

317
00:11:15,660 --> 00:11:18,120
does not do better than random

318
00:11:18,120 --> 00:11:19,140
um

319
00:11:19,140 --> 00:11:22,800
um a second evaluation I did on um on a

320
00:11:22,800 --> 00:11:24,660
Twitter data set

321
00:11:24,660 --> 00:11:27,899
um I was fortunately to to to find two

322
00:11:27,899 --> 00:11:29,640
Twitter accounts which self-advertised

323
00:11:29,640 --> 00:11:31,920
that they had purchased bought followers

324
00:11:31,920 --> 00:11:33,959
in certain amounts at certain times and

325
00:11:33,959 --> 00:11:36,600
again we were able to to use this as a

326
00:11:36,600 --> 00:11:38,940
mechanism for evaluation again with a

327
00:11:38,940 --> 00:11:40,800
fairly small set of features just five

328
00:11:40,800 --> 00:11:42,240
five features here for these two

329
00:11:42,240 --> 00:11:44,640
particular accounts black is the RSC

330
00:11:44,640 --> 00:11:46,980
curve produced by this algorithm

331
00:11:46,980 --> 00:11:49,680
um green is produced by botometer which

332
00:11:49,680 --> 00:11:51,000
is a

333
00:11:51,000 --> 00:11:54,300
um a a a Twitter bot detection mechanism

334
00:11:54,300 --> 00:11:56,459
put out by Indiana University that is an

335
00:11:56,459 --> 00:11:58,920
open API and then again the blue is

336
00:11:58,920 --> 00:12:01,079
isolation for us which is you know um

337
00:12:01,079 --> 00:12:02,459
one of the prominent

338
00:12:02,459 --> 00:12:03,380
um

339
00:12:03,380 --> 00:12:06,660
unsupervised methods the um and the the

340
00:12:06,660 --> 00:12:08,339
point here I guess is not that you know

341
00:12:08,339 --> 00:12:10,200
necessarily that this is the best

342
00:12:10,200 --> 00:12:11,880
classifier you've only seen for any task

343
00:12:11,880 --> 00:12:13,980
ever um the the area under the curve is

344
00:12:13,980 --> 00:12:16,260
about 0.8 here but given that none of

345
00:12:16,260 --> 00:12:17,880
the supervised methods that that we know

346
00:12:17,880 --> 00:12:20,399
work at all and the unsupervised methods

347
00:12:20,399 --> 00:12:21,720
basically don't do better than random

348
00:12:21,720 --> 00:12:23,160
this is essentially you know the best

349
00:12:23,160 --> 00:12:25,560
that we know how to do and then we also

350
00:12:25,560 --> 00:12:29,339
evaluated on um a crawl of you know five

351
00:12:29,339 --> 00:12:31,680
uh five million Twitter accounts crawled

352
00:12:31,680 --> 00:12:33,899
using the public API unfortunately there

353
00:12:33,899 --> 00:12:36,120
we have absolutely no labels so we don't

354
00:12:36,120 --> 00:12:38,040
know how you know we can't draw Roc

355
00:12:38,040 --> 00:12:40,079
curves or give you know Precision recall

356
00:12:40,079 --> 00:12:41,459
numbers or anything like that but we

357
00:12:41,459 --> 00:12:42,320
still went through the exercise

358
00:12:42,320 --> 00:12:44,579
calculate the cleans and then you know

359
00:12:44,579 --> 00:12:46,440
look at the rules that are generated and

360
00:12:46,440 --> 00:12:47,700
look at the things that are most likely

361
00:12:47,700 --> 00:12:50,339
to be bought and so it is possible and

362
00:12:50,339 --> 00:12:51,839
and I'm not going to go through all of

363
00:12:51,839 --> 00:12:53,700
the examples but the paper gives gives

364
00:12:53,700 --> 00:12:55,579
quite a few examples of

365
00:12:55,579 --> 00:12:58,200
examples of cluster behavior that are

366
00:12:58,200 --> 00:13:00,720
clearly evident here here is the top one

367
00:13:00,720 --> 00:13:02,700
of the the the the rules that is output

368
00:13:02,700 --> 00:13:04,560
saying that you know

369
00:13:04,560 --> 00:13:06,320
um a features

370
00:13:06,320 --> 00:13:08,399
accounts that match this particular

371
00:13:08,399 --> 00:13:10,800
configuration of features have an odds

372
00:13:10,800 --> 00:13:13,380
ratio 700 to 1 of of being bought and

373
00:13:13,380 --> 00:13:14,459
when we look at that in a little more

374
00:13:14,459 --> 00:13:16,620
detail you know there were 400 accounts

375
00:13:16,620 --> 00:13:19,380
that that met this and every single one

376
00:13:19,380 --> 00:13:21,120
of them had this kind of you know naming

377
00:13:21,120 --> 00:13:22,560
pattern where their Alias the the

378
00:13:22,560 --> 00:13:24,839
username chosen was a last name plus two

379
00:13:24,839 --> 00:13:27,000
letters plus two digits and when We

380
00:13:27,000 --> 00:13:28,920
examined those two letters into those

381
00:13:28,920 --> 00:13:31,200
two digits and use a chi-square test

382
00:13:31,200 --> 00:13:33,000
they're structurally random right they

383
00:13:33,000 --> 00:13:34,740
are you know they are uniformly

384
00:13:34,740 --> 00:13:36,600
distributed across the entire space so

385
00:13:36,600 --> 00:13:38,700
there's absolutely no possibility

386
00:13:38,700 --> 00:13:40,260
whatsoever that that these were

387
00:13:40,260 --> 00:13:43,560
generated by 400 independent uh human

388
00:13:43,560 --> 00:13:45,420
beings and in the interest of time then

389
00:13:45,420 --> 00:13:47,040
let me just you know Skip to the end to

390
00:13:47,040 --> 00:13:48,779
the conclusions and say you know

391
00:13:48,779 --> 00:13:51,120
basically the main mechanism here is

392
00:13:51,120 --> 00:13:53,040
when X is independent of Y you can get

393
00:13:53,040 --> 00:13:55,139
the clean distribution of X by finding

394
00:13:55,139 --> 00:13:57,540
the this span of rank one subset of

395
00:13:57,540 --> 00:14:00,120
linearly dependent Columns of of p x of

396
00:14:00,120 --> 00:14:02,760
Y it's unsupervised categorical features

397
00:14:02,760 --> 00:14:04,920
and a really nice nice property of this

398
00:14:04,920 --> 00:14:06,420
thing is it's kind of one-sided I'm not

399
00:14:06,420 --> 00:14:08,040
doing anything to learn what the bad guy

400
00:14:08,040 --> 00:14:09,600
is doing the bad guy can vary all his

401
00:14:09,600 --> 00:14:12,480
want in particular I'm not assuming that

402
00:14:12,480 --> 00:14:15,899
the the bad traffic that was observed in

403
00:14:15,899 --> 00:14:17,639
training is going to be representative

404
00:14:17,639 --> 00:14:19,139
of the bad traffic that is going to be

405
00:14:19,139 --> 00:14:20,760
seen in deployment I'm not making any

406
00:14:20,760 --> 00:14:22,560
assumption like that okay let me end

407
00:14:22,560 --> 00:14:23,399
there

408
00:14:23,399 --> 00:14:26,000
thank you

