1
00:00:08,360 --> 00:00:10,679
hello and welcome to my talk called

2
00:00:10,679 --> 00:00:12,540
communication efficient triangle

3
00:00:12,540 --> 00:00:14,040
counting under local differential

4
00:00:14,040 --> 00:00:16,859
privacy this is Joint work with takao

5
00:00:16,859 --> 00:00:20,400
murakami and kamalika chagari

6
00:00:20,400 --> 00:00:22,560
this work is about counting subgraphs in

7
00:00:22,560 --> 00:00:24,840
a network we focus on triangles and K

8
00:00:24,840 --> 00:00:26,039
Stars

9
00:00:26,039 --> 00:00:28,199
a k-star consists of a central node

10
00:00:28,199 --> 00:00:31,019
connected to K other nodes in this

11
00:00:31,019 --> 00:00:33,360
example graph the green user the light

12
00:00:33,360 --> 00:00:35,340
green user in the middle is a part of

13
00:00:35,340 --> 00:00:38,700
three k of three three stars and there

14
00:00:38,700 --> 00:00:41,579
are two triangles 15 two stars and six

15
00:00:41,579 --> 00:00:44,120
three stars even with these simple

16
00:00:44,120 --> 00:00:46,440
coefficients we can already compute

17
00:00:46,440 --> 00:00:47,760
interesting quantities about the graph

18
00:00:47,760 --> 00:00:49,739
such as the clustering coefficient

19
00:00:49,739 --> 00:00:51,840
this is the probability that two friends

20
00:00:51,840 --> 00:00:54,000
of a user are also friends and it's

21
00:00:54,000 --> 00:00:56,219
useful for friend suggestion it's given

22
00:00:56,219 --> 00:00:57,780
by three times the number of triangles

23
00:00:57,780 --> 00:01:01,520
divided by the number of two stars

24
00:01:02,879 --> 00:01:04,680
we can imagine that revealing the number

25
00:01:04,680 --> 00:01:07,320
of triangles or k Stars can reveal

26
00:01:07,320 --> 00:01:09,840
sensitive information about the graph

27
00:01:09,840 --> 00:01:12,000
to mitigate these privacy issues we can

28
00:01:12,000 --> 00:01:14,220
use local differential privacy in which

29
00:01:14,220 --> 00:01:16,680
a user obfuscates her own data with no

30
00:01:16,680 --> 00:01:18,900
trusted third party

31
00:01:18,900 --> 00:01:22,080
this is Illustrated in the bottom left

32
00:01:22,080 --> 00:01:24,000
unlike Central differential privacy

33
00:01:24,000 --> 00:01:26,159
Illustrated in the bottom right we don't

34
00:01:26,159 --> 00:01:28,259
need to trust a central aggregator and

35
00:01:28,259 --> 00:01:32,479
the aggregator can even be adversarial

36
00:01:34,080 --> 00:01:36,240
prior work shows that K Stars can be

37
00:01:36,240 --> 00:01:38,880
estimated accurately under ldp and

38
00:01:38,880 --> 00:01:41,220
triangles can be estimated accurately

39
00:01:41,220 --> 00:01:44,159
under ldp in which users are queried

40
00:01:44,159 --> 00:01:46,860
twice however for triangles the download

41
00:01:46,860 --> 00:01:49,140
cost of this algorithm is large it takes

42
00:01:49,140 --> 00:01:51,960
400 gigabits on a realistic graph which

43
00:01:51,960 --> 00:01:54,720
would take six hours to download

44
00:01:54,720 --> 00:01:56,880
plus our contribution

45
00:01:56,880 --> 00:01:58,860
is that we dramatically reduce the

46
00:01:58,860 --> 00:02:00,420
download cost using several new

47
00:02:00,420 --> 00:02:02,939
algorithmic ideas and we reduce it down

48
00:02:02,939 --> 00:02:05,640
to 160 megabits which takes just eight

49
00:02:05,640 --> 00:02:07,880
seconds

50
00:02:08,940 --> 00:02:12,619
I will start with some preliminaries

51
00:02:14,760 --> 00:02:16,680
in our setup we represent the graph as

52
00:02:16,680 --> 00:02:18,780
an adjacency Matrix where one indicates

53
00:02:18,780 --> 00:02:21,540
an edge the graph is distributed locally

54
00:02:21,540 --> 00:02:24,360
so user VI only knows her neighbor list

55
00:02:24,360 --> 00:02:27,180
AI the I throw of a

56
00:02:27,180 --> 00:02:30,300
we I we have Illustrated this on the

57
00:02:30,300 --> 00:02:32,760
simple example with four users

58
00:02:32,760 --> 00:02:35,220
to compute an answer an aggregator who

59
00:02:35,220 --> 00:02:37,319
does not know who does not need to be

60
00:02:37,319 --> 00:02:40,260
trusted asks users questions in the form

61
00:02:40,260 --> 00:02:44,400
of a randomizer RI user I applies RI to

62
00:02:44,400 --> 00:02:46,560
her data Ai and sends it to the server

63
00:02:46,560 --> 00:02:49,280
for aggregation

64
00:02:50,040 --> 00:02:51,840
the way that privacy is protected in

65
00:02:51,840 --> 00:02:53,819
this model of computation is that each

66
00:02:53,819 --> 00:02:57,660
randomizer satisfies Epsilon Edge ldp

67
00:02:57,660 --> 00:03:00,120
Epsilon LG ldp states that if an

68
00:03:00,120 --> 00:03:02,700
adjacency list differs in one bit then R

69
00:03:02,700 --> 00:03:05,280
of A and R of a prime are similar

70
00:03:05,280 --> 00:03:09,379
distributions up to Epsilon

71
00:03:09,599 --> 00:03:11,700
technically speaking each Edge is part

72
00:03:11,700 --> 00:03:13,560
of two adjacency lists and therefore

73
00:03:13,560 --> 00:03:15,840
changing one Edge affects two adjacency

74
00:03:15,840 --> 00:03:17,220
lists in the graph

75
00:03:17,220 --> 00:03:19,379
however our algorithm uses just the

76
00:03:19,379 --> 00:03:21,420
lower triangular part of the adjacency

77
00:03:21,420 --> 00:03:25,080
Matrix so Epsilon Edge ldp protects the

78
00:03:25,080 --> 00:03:27,599
distinguishability of an edge up to

79
00:03:27,599 --> 00:03:30,319
Epsilon

80
00:03:32,700 --> 00:03:34,560
now I will overview the existing

81
00:03:34,560 --> 00:03:37,260
triangle counting algorithms under ldp

82
00:03:37,260 --> 00:03:39,180
notice that triangle counting is not

83
00:03:39,180 --> 00:03:42,120
easy in the ldp model because no user is

84
00:03:42,120 --> 00:03:44,340
aware of any triangle they cannot see

85
00:03:44,340 --> 00:03:47,580
the third Edge between their neighbors

86
00:03:47,580 --> 00:03:50,640
to overcome this in the first round the

87
00:03:50,640 --> 00:03:52,920
user applies randomized response to each

88
00:03:52,920 --> 00:03:54,959
bit of their neighbor list and releases

89
00:03:54,959 --> 00:03:57,540
it to the server this produces a noisy

90
00:03:57,540 --> 00:03:58,980
graph

91
00:03:58,980 --> 00:04:01,620
a graph consisting of noisy edges

92
00:04:01,620 --> 00:04:03,900
and as a reminder randomized response

93
00:04:03,900 --> 00:04:06,360
you simply flip whether an edge exists

94
00:04:06,360 --> 00:04:09,739
or not with some probability

95
00:04:10,560 --> 00:04:12,840
in the second round

96
00:04:12,840 --> 00:04:15,360
each user counts triangles using two of

97
00:04:15,360 --> 00:04:18,358
their edges and the third Edge the one

98
00:04:18,358 --> 00:04:20,639
they don't know coming from the noisy

99
00:04:20,639 --> 00:04:22,320
graph from the first round

100
00:04:22,320 --> 00:04:24,300
the user sends the number of noisy

101
00:04:24,300 --> 00:04:26,160
triangles plus a corrective term for

102
00:04:26,160 --> 00:04:28,320
de-biasing plus LaPlace noise to

103
00:04:28,320 --> 00:04:31,080
satisfies ldp to the central server and

104
00:04:31,080 --> 00:04:33,060
the server simply sums these up to

105
00:04:33,060 --> 00:04:36,060
obtain an unbiased estimate

106
00:04:36,060 --> 00:04:38,220
however in order for users to perform

107
00:04:38,220 --> 00:04:40,560
this round note that they must download

108
00:04:40,560 --> 00:04:42,360
the edges from the first round in the

109
00:04:42,360 --> 00:04:45,000
noisy graph this noisy graph is dense

110
00:04:45,000 --> 00:04:47,460
and it is extremely large

111
00:04:47,460 --> 00:04:50,280
and it actually and it is prohibitively

112
00:04:50,280 --> 00:04:52,940
costly

113
00:04:54,840 --> 00:04:56,639
now I will outline our proposed

114
00:04:56,639 --> 00:04:58,860
mechanism

115
00:04:58,860 --> 00:05:00,960
in our approach we will form a sparse

116
00:05:00,960 --> 00:05:02,940
noisy graph G Prime in the first round

117
00:05:02,940 --> 00:05:05,340
this will reduce download but will

118
00:05:05,340 --> 00:05:07,919
increase estimation error

119
00:05:07,919 --> 00:05:10,080
to form the sparse noisy graph we

120
00:05:10,080 --> 00:05:11,580
perform what is called asymmetric

121
00:05:11,580 --> 00:05:14,100
randomized response which is effectively

122
00:05:14,100 --> 00:05:16,860
sampling edges after randomized response

123
00:05:16,860 --> 00:05:18,000
is done

124
00:05:18,000 --> 00:05:20,220
so we take edges that are reported to

125
00:05:20,220 --> 00:05:22,560
Exist by randomized response and with

126
00:05:22,560 --> 00:05:25,560
probability 1 minus P2 we change those

127
00:05:25,560 --> 00:05:28,020
edges to a zero this is Illustrated in

128
00:05:28,020 --> 00:05:30,840
the bottom left part of the graph

129
00:05:30,840 --> 00:05:32,699
and this is post-processing of

130
00:05:32,699 --> 00:05:34,919
randomized response therefore it still

131
00:05:34,919 --> 00:05:37,680
satisfies DP

132
00:05:37,680 --> 00:05:40,620
after this our second round uses two

133
00:05:40,620 --> 00:05:44,479
further improvements to reduce error

134
00:05:44,880 --> 00:05:47,039
they are called selective DL and double

135
00:05:47,039 --> 00:05:49,199
clipping

136
00:05:49,199 --> 00:05:51,660
in selective download consider what

137
00:05:51,660 --> 00:05:53,699
happens when a user downloads the entire

138
00:05:53,699 --> 00:05:56,340
noisy graph and counts triangles

139
00:05:56,340 --> 00:05:58,500
if we have the following four cycle

140
00:05:58,500 --> 00:06:00,120
shape Illustrated in the bottom left

141
00:06:00,120 --> 00:06:03,120
then the noisy Edge between VJ and VK

142
00:06:03,120 --> 00:06:05,580
will cause two triangles to appear

143
00:06:05,580 --> 00:06:07,560
thus the error is tied to the number of

144
00:06:07,560 --> 00:06:10,740
four Cycles which is n times D Max cubed

145
00:06:10,740 --> 00:06:12,720
the number the maximum degree of the

146
00:06:12,720 --> 00:06:13,800
graph

147
00:06:13,800 --> 00:06:16,740
If instead user VI only downloads and

148
00:06:16,740 --> 00:06:20,699
considers edges VK and VK where ijk is a

149
00:06:20,699 --> 00:06:22,860
triangle in the noisy graph then it is

150
00:06:22,860 --> 00:06:24,720
much less likely that all five edges

151
00:06:24,720 --> 00:06:27,000
will activate and produce two false

152
00:06:27,000 --> 00:06:29,880
triangles this makes the triangles a

153
00:06:29,880 --> 00:06:31,500
less correlated event with each other

154
00:06:31,500 --> 00:06:33,840
allowing us to reduce the error to n

155
00:06:33,840 --> 00:06:36,060
times D Max squared

156
00:06:36,060 --> 00:06:38,180
this is what the algorithm

157
00:06:38,180 --> 00:06:41,160
arr2ns does and we also consider a

158
00:06:41,160 --> 00:06:44,280
variant arr1ns in which

159
00:06:44,280 --> 00:06:48,660
a user VI downloads an edge vknbj when

160
00:06:48,660 --> 00:06:50,699
just two of these edges are in the

161
00:06:50,699 --> 00:06:52,919
Triangle in the noisy graph

162
00:06:52,919 --> 00:06:54,539
this algorithm can perform better

163
00:06:54,539 --> 00:06:57,259
experimentally

164
00:06:58,080 --> 00:06:59,759
in the second round of the algorithm

165
00:06:59,759 --> 00:07:01,979
users add LaPlace noise to their noisy

166
00:07:01,979 --> 00:07:04,800
triangle counts

167
00:07:04,800 --> 00:07:06,720
when the graph is heavily down sampled

168
00:07:06,720 --> 00:07:08,880
we expect the sensitivity of the number

169
00:07:08,880 --> 00:07:11,100
of noisy triangles to be less because

170
00:07:11,100 --> 00:07:13,979
degrees are often less than D Max and

171
00:07:13,979 --> 00:07:16,740
the graph is sparse due to down sampling

172
00:07:16,740 --> 00:07:18,720
we achieved this sensitivity reduction

173
00:07:18,720 --> 00:07:21,180
using clipping techniques we clip edges

174
00:07:21,180 --> 00:07:23,400
and then we clip noisy triangles to get

175
00:07:23,400 --> 00:07:26,599
away with adding less noise

176
00:07:31,500 --> 00:07:34,139
for Edge clipping we estimate a user's

177
00:07:34,139 --> 00:07:36,300
degree before the algorithm

178
00:07:36,300 --> 00:07:38,699
even runs and tell the user to remove

179
00:07:38,699 --> 00:07:41,160
edges if his degree is higher than the

180
00:07:41,160 --> 00:07:43,620
estimate this allows sensitivity to

181
00:07:43,620 --> 00:07:45,780
depend on the user's degree as opposed

182
00:07:45,780 --> 00:07:48,599
to a global constant

183
00:07:48,599 --> 00:07:51,060
for a noisy triangle clipping if the

184
00:07:51,060 --> 00:07:52,800
number of noisy triangles of a user

185
00:07:52,800 --> 00:07:55,979
exceeds some constant Kappa I then we

186
00:07:55,979 --> 00:07:58,139
reduce the number of noisy triangles by

187
00:07:58,139 --> 00:08:00,419
clipping so he will only send a maximum

188
00:08:00,419 --> 00:08:03,599
value of Kappa I we set Kappa I so that

189
00:08:03,599 --> 00:08:06,599
the probability of actually clipping is

190
00:08:06,599 --> 00:08:07,979
very small

191
00:08:07,979 --> 00:08:10,380
in the example the user has clipped two

192
00:08:10,380 --> 00:08:12,060
of his edges and capped the number of

193
00:08:12,060 --> 00:08:15,599
noisy triangles to three and has also

194
00:08:15,599 --> 00:08:18,180
capped his degree to B4

195
00:08:18,180 --> 00:08:20,639
these clipping techniques add bias to

196
00:08:20,639 --> 00:08:23,460
our algorithm but we sent them so that

197
00:08:23,460 --> 00:08:25,919
the clipping rarely occurs and we and it

198
00:08:25,919 --> 00:08:29,479
still allows for Less sensitivity

199
00:08:31,440 --> 00:08:35,099
now I will outline our experiments

200
00:08:35,099 --> 00:08:36,839
the data sets on which we ran our

201
00:08:36,839 --> 00:08:38,820
experiments were the Google Plus social

202
00:08:38,820 --> 00:08:41,760
network and the IMDb database consisting

203
00:08:41,760 --> 00:08:45,180
of matchings between movies and actors

204
00:08:45,180 --> 00:08:47,040
these graphs have hundreds of thousands

205
00:08:47,040 --> 00:08:49,260
of nodes and like many real world graphs

206
00:08:49,260 --> 00:08:52,339
they are sparse

207
00:08:54,720 --> 00:08:57,060
we have plotted the relative triangle

208
00:08:57,060 --> 00:08:59,339
count error versus the download cost of

209
00:08:59,339 --> 00:09:01,320
the various algorithms on the two plots

210
00:09:01,320 --> 00:09:03,420
below we can see that with the double

211
00:09:03,420 --> 00:09:05,880
clipping technique for just 19 megabits

212
00:09:05,880 --> 00:09:09,300
for Google Plus or 160 megabits for IMDb

213
00:09:09,300 --> 00:09:13,939
the relative error is much less than one

214
00:09:14,339 --> 00:09:16,260
we contrast this with previous work

215
00:09:16,260 --> 00:09:17,940
which required users to download

216
00:09:17,940 --> 00:09:20,399
hundreds of gigabits and appears as the

217
00:09:20,399 --> 00:09:23,279
square on the bottom right of each graph

218
00:09:23,279 --> 00:09:25,080
note that the error of this previous

219
00:09:25,080 --> 00:09:27,000
work is higher than our double clipping

220
00:09:27,000 --> 00:09:28,440
techniques because it does not utilize

221
00:09:28,440 --> 00:09:30,240
double clipping and this indicates that

222
00:09:30,240 --> 00:09:31,980
double clipping is an effective way to

223
00:09:31,980 --> 00:09:34,700
reduce error

224
00:09:35,279 --> 00:09:37,620
in our second set of experiments we

225
00:09:37,620 --> 00:09:39,180
evaluated the effect of selective

226
00:09:39,180 --> 00:09:41,700
downloading we fixed the download budget

227
00:09:41,700 --> 00:09:44,519
and ran the full download of the

228
00:09:44,519 --> 00:09:48,120
previous work versus arr1ns and arr2ns

229
00:09:48,120 --> 00:09:49,860
and collected this

230
00:09:49,860 --> 00:09:51,899
they are relative errors

231
00:09:51,899 --> 00:09:54,000
the plots indicate that selectively

232
00:09:54,000 --> 00:09:56,459
choosing the download can significantly

233
00:09:56,459 --> 00:09:58,500
reduce the error by a factor of two to

234
00:09:58,500 --> 00:09:59,360
four

235
00:09:59,360 --> 00:10:03,540
also arr1 and S outperforms arr2ns

236
00:10:03,540 --> 00:10:06,360
because its sensitivity is more

237
00:10:06,360 --> 00:10:07,860
effectively reduced by our double

238
00:10:07,860 --> 00:10:11,120
clicking clipping technique

239
00:10:11,580 --> 00:10:13,380
in conclusion we proposed a

240
00:10:13,380 --> 00:10:14,880
communication efficient triangle

241
00:10:14,880 --> 00:10:16,920
counting algorithm under ldp which

242
00:10:16,920 --> 00:10:19,500
reduces download costs from a 400 400

243
00:10:19,500 --> 00:10:22,860
gigabits to 160 megabits with while

244
00:10:22,860 --> 00:10:25,740
still maintaining low relative error

245
00:10:25,740 --> 00:10:27,360
and for future work we would like to

246
00:10:27,360 --> 00:10:29,040
consider more effective triangle

247
00:10:29,040 --> 00:10:31,980
counting with one round of communication

248
00:10:31,980 --> 00:10:35,720
notice that our technique is used too

249
00:10:37,740 --> 00:10:41,480
I'd like to thank you for your attention

