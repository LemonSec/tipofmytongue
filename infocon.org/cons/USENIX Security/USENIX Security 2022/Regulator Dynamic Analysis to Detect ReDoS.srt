1
00:00:08,000 --> 00:00:10,019
thank you

2
00:00:10,019 --> 00:00:12,599
um hi everyone my name is Robert uh I'm

3
00:00:12,599 --> 00:00:14,940
a PhD student at UC Santa Barbara I'm

4
00:00:14,940 --> 00:00:16,440
here to talk to you about our Dynamic

5
00:00:16,440 --> 00:00:18,180
analysis system to detect regular

6
00:00:18,180 --> 00:00:19,320
expression denial of service

7
00:00:19,320 --> 00:00:22,680
vulnerabilities so let's get started

8
00:00:22,680 --> 00:00:24,300
I want to warm everybody up by just

9
00:00:24,300 --> 00:00:25,439
thinking so

10
00:00:25,439 --> 00:00:26,880
about that code review that you just got

11
00:00:26,880 --> 00:00:29,279
on your desk right so you've got a patch

12
00:00:29,279 --> 00:00:30,480
to review and you want to give the

13
00:00:30,480 --> 00:00:32,940
thumbs up or thumbs down on it you get

14
00:00:32,940 --> 00:00:34,500
it in and yeah it looks a little scary

15
00:00:34,500 --> 00:00:36,300
there's a regular expression in there

16
00:00:36,300 --> 00:00:38,520
and we all know they're hard to read so

17
00:00:38,520 --> 00:00:41,219
squint at it a little bit and oh well

18
00:00:41,219 --> 00:00:43,800
but it comes with unit tests which is

19
00:00:43,800 --> 00:00:46,020
great and they all seem to pass and this

20
00:00:46,020 --> 00:00:48,000
seems to sort of meet your general

21
00:00:48,000 --> 00:00:49,860
expectations for what might have to

22
00:00:49,860 --> 00:00:52,620
happen here so you give it the thumbs up

23
00:00:52,620 --> 00:00:54,120
of course

24
00:00:54,120 --> 00:00:56,640
actually unfortunately this is uh

25
00:00:56,640 --> 00:00:58,680
vulnerability in fact we identified it

26
00:00:58,680 --> 00:01:00,000
during the course of this investigation

27
00:01:00,000 --> 00:01:02,399
the problem here is that this regular

28
00:01:02,399 --> 00:01:04,500
expression of course executes in

29
00:01:04,500 --> 00:01:07,380
polynomial time worst case if an

30
00:01:07,380 --> 00:01:08,700
attacker could control that input

31
00:01:08,700 --> 00:01:11,760
parameter URL string then it can cause

32
00:01:11,760 --> 00:01:14,700
excess CPU usage on the victim machined

33
00:01:14,700 --> 00:01:17,040
when this is running

34
00:01:17,040 --> 00:01:18,479
but don't worry you're in good company

35
00:01:18,479 --> 00:01:20,460
last we checked on this package they had

36
00:01:20,460 --> 00:01:23,100
26 million weekly downloads we find this

37
00:01:23,100 --> 00:01:26,340
issue all across the ecosystem and prior

38
00:01:26,340 --> 00:01:30,320
work is founded everywhere as well

39
00:01:30,720 --> 00:01:33,780
backing up a little bit so what is redos

40
00:01:33,780 --> 00:01:35,400
just to remind ourselves it's a type of

41
00:01:35,400 --> 00:01:37,100
denial of service attack

42
00:01:37,100 --> 00:01:40,380
this type of denial of service attack is

43
00:01:40,380 --> 00:01:42,360
a little bit more advanced than just

44
00:01:42,360 --> 00:01:44,220
pushing through uh you know a ton of

45
00:01:44,220 --> 00:01:46,619
traffic the idea here is that the

46
00:01:46,619 --> 00:01:48,479
attacker is attempting to achieve a

47
00:01:48,479 --> 00:01:51,720
symmetry uh by finding a method of

48
00:01:51,720 --> 00:01:55,700
making it uh amplify the amount of

49
00:01:55,700 --> 00:01:58,200
damage that they're doing with very

50
00:01:58,200 --> 00:02:01,500
little effort and of course complexity

51
00:02:01,500 --> 00:02:03,299
in victim software is exactly one of

52
00:02:03,299 --> 00:02:05,820
these methods of amplifying uh the

53
00:02:05,820 --> 00:02:06,780
attack

54
00:02:06,780 --> 00:02:09,060
regular Expressions it turns out are

55
00:02:09,060 --> 00:02:11,220
exactly a type of software where we have

56
00:02:11,220 --> 00:02:12,900
this type of hidden complexity which is

57
00:02:12,900 --> 00:02:14,700
difficult to identify but easy to

58
00:02:14,700 --> 00:02:18,120
exploit if you are an attacker

59
00:02:18,120 --> 00:02:19,680
we know that it has significant real

60
00:02:19,680 --> 00:02:22,920
world impact at usenix a few years ago

61
00:02:22,920 --> 00:02:24,900
we had a presentation showing that

62
00:02:24,900 --> 00:02:26,640
hundreds of popular websites are

63
00:02:26,640 --> 00:02:28,440
vulnerable to at least one redos

64
00:02:28,440 --> 00:02:30,480
vulnerability so it is a serious problem

65
00:02:30,480 --> 00:02:31,920
that's out there

66
00:02:31,920 --> 00:02:35,040
and prior results have been good but

67
00:02:35,040 --> 00:02:38,700
incremental we note that each time we

68
00:02:38,700 --> 00:02:41,040
compare against prior tools there's a

69
00:02:41,040 --> 00:02:42,840
high false negative rate and high false

70
00:02:42,840 --> 00:02:45,300
positive rate in fact we saw in the

71
00:02:45,300 --> 00:02:47,040
first presentation here that comparing

72
00:02:47,040 --> 00:02:49,260
against redox detection tools for

73
00:02:49,260 --> 00:02:51,720
backtracking matchers even for a tool

74
00:02:51,720 --> 00:02:53,640
not intended for backpacking Masters can

75
00:02:53,640 --> 00:02:56,280
can be you know show quite improvements

76
00:02:56,280 --> 00:02:58,500
so we really have to question

77
00:02:58,500 --> 00:03:01,200
whether we were finding everything here

78
00:03:01,200 --> 00:03:03,360
moreover all of these previous methods

79
00:03:03,360 --> 00:03:05,040
have severe limitations on the feature

80
00:03:05,040 --> 00:03:06,900
sets so as regular expression engines

81
00:03:06,900 --> 00:03:08,760
evolve into the future and more features

82
00:03:08,760 --> 00:03:11,459
are added the analyzes really need a

83
00:03:11,459 --> 00:03:14,780
little bit more thought put into them

84
00:03:15,239 --> 00:03:17,640
I can't go over exactly what these uh

85
00:03:17,640 --> 00:03:20,099
prior tools were doing but just to give

86
00:03:20,099 --> 00:03:21,659
a general intuition let's think this

87
00:03:21,659 --> 00:03:23,700
through a little bit so ignore the fact

88
00:03:23,700 --> 00:03:25,980
that back references exist for now and

89
00:03:25,980 --> 00:03:28,680
let's imagine that uh modern regex

90
00:03:28,680 --> 00:03:30,780
matching engines only match against

91
00:03:30,780 --> 00:03:33,480
regular languages we know from our good

92
00:03:33,480 --> 00:03:37,200
cs1 you know 101 classes that uh these

93
00:03:37,200 --> 00:03:38,959
are all represented by a

94
00:03:38,959 --> 00:03:41,459
non-deterministic finite automaton

95
00:03:41,459 --> 00:03:42,659
but of course we're running on

96
00:03:42,659 --> 00:03:44,580
deterministic Hardware so how do you do

97
00:03:44,580 --> 00:03:46,440
the matching well you need to do a depth

98
00:03:46,440 --> 00:03:47,760
first search from the starting state

99
00:03:47,760 --> 00:03:49,560
which is constrained by your input

100
00:03:49,560 --> 00:03:51,239
string you're matching against and see

101
00:03:51,239 --> 00:03:54,360
if you can end up at an accepting state

102
00:03:54,360 --> 00:03:56,760
I show here an example of an ambiguous

103
00:03:56,760 --> 00:04:00,420
automaton and the problem as we can see

104
00:04:00,420 --> 00:04:02,580
here is that imagine that you've managed

105
00:04:02,580 --> 00:04:05,159
to reach State Q2 by consuming perhaps

106
00:04:05,159 --> 00:04:07,500
an A and then a b you now have to make a

107
00:04:07,500 --> 00:04:09,659
decision as to whether to go to state q1

108
00:04:09,659 --> 00:04:11,580
or back to Q2 when you're consuming the

109
00:04:11,580 --> 00:04:14,040
next B of course if an attacker knows

110
00:04:14,040 --> 00:04:15,420
that you must make this decision they

111
00:04:15,420 --> 00:04:17,519
can exploit that fact and make you

112
00:04:17,519 --> 00:04:19,738
constantly guess the incorrect location

113
00:04:19,738 --> 00:04:21,600
and then in a backtracking matcher

114
00:04:21,600 --> 00:04:23,520
you'll have to backtrack to try the

115
00:04:23,520 --> 00:04:25,919
other decision point and so on and it

116
00:04:25,919 --> 00:04:28,860
will blow up in this manner

117
00:04:28,860 --> 00:04:30,900
uh generally speaking these prior

118
00:04:30,900 --> 00:04:33,240
diagnostic attempts are attempting to

119
00:04:33,240 --> 00:04:34,380
look for these types of you know

120
00:04:34,380 --> 00:04:37,620
ambiguities and uh

121
00:04:37,620 --> 00:04:40,800
patterns and taxis

122
00:04:40,800 --> 00:04:42,360
but in fact this actually isn't how

123
00:04:42,360 --> 00:04:43,919
modern regular expression engines really

124
00:04:43,919 --> 00:04:45,479
operate or at least not the backtracking

125
00:04:45,479 --> 00:04:46,380
ones

126
00:04:46,380 --> 00:04:47,759
it turns out that they are a full

127
00:04:47,759 --> 00:04:49,440
Software System that's embedded within

128
00:04:49,440 --> 00:04:52,740
uh modern languages when you go and run

129
00:04:52,740 --> 00:04:54,120
a regular expression there's a full

130
00:04:54,120 --> 00:04:56,100
lexor and parser and a compiler that

131
00:04:56,100 --> 00:04:58,139
will go emit bytecode when you run the

132
00:04:58,139 --> 00:04:59,580
matching procedure this bytecode

133
00:04:59,580 --> 00:05:00,840
actually runs on a lightweight

134
00:05:00,840 --> 00:05:02,639
interpreter and the output of that

135
00:05:02,639 --> 00:05:04,620
interpreter will give you an answer as

136
00:05:04,620 --> 00:05:05,940
to whether there's a match or no match

137
00:05:05,940 --> 00:05:08,520
and if it did match any matching groups

138
00:05:08,520 --> 00:05:11,400
that were captured in the process

139
00:05:11,400 --> 00:05:14,160
uh this uh if you're wondering is also

140
00:05:14,160 --> 00:05:15,840
called uh Spencer's backtracking

141
00:05:15,840 --> 00:05:17,520
algorithm so it's pretty well documented

142
00:05:17,520 --> 00:05:19,020
it's been known in the literature for

143
00:05:19,020 --> 00:05:21,320
quite a while

144
00:05:21,479 --> 00:05:22,979
so we need a new approach for detecting

145
00:05:22,979 --> 00:05:25,979
redos and uh we need to make sure that

146
00:05:25,979 --> 00:05:28,740
it puts this front and center and this

147
00:05:28,740 --> 00:05:31,259
is exactly what we do we fully embrace

148
00:05:31,259 --> 00:05:34,139
the idea that regular expressions are

149
00:05:34,139 --> 00:05:36,660
software in and of themselves and we

150
00:05:36,660 --> 00:05:38,460
embrace the complexity that it comes

151
00:05:38,460 --> 00:05:40,979
with we don't try to hide from all of

152
00:05:40,979 --> 00:05:43,860
the advanced features that are existing

153
00:05:43,860 --> 00:05:46,620
in modern regular expression engines

154
00:05:46,620 --> 00:05:50,220
and instead we use the tried and true

155
00:05:50,220 --> 00:05:52,500
software testing tools that I think

156
00:05:52,500 --> 00:05:55,199
everybody here might know

157
00:05:55,199 --> 00:05:57,320
what we do is we invent regulator

158
00:05:57,320 --> 00:06:00,479
which is a surprisingly Simple Solution

159
00:06:00,479 --> 00:06:03,180
which achieves very good results for

160
00:06:03,180 --> 00:06:04,860
exactly what we want to do

161
00:06:04,860 --> 00:06:07,280
we start by using a slow down fuzzer

162
00:06:07,280 --> 00:06:10,139
which is a specially modified fuzzer

163
00:06:10,139 --> 00:06:12,419
which given an input regular expression

164
00:06:12,419 --> 00:06:14,639
we'll try to find examples of strings

165
00:06:14,639 --> 00:06:18,000
that will be very slow when they execute

166
00:06:18,000 --> 00:06:20,460
then we take examples from this fuzzer

167
00:06:20,460 --> 00:06:23,520
and we derive a formula that will show

168
00:06:23,520 --> 00:06:26,400
us how attack strings are formed for

169
00:06:26,400 --> 00:06:28,199
this particular regular expression and

170
00:06:28,199 --> 00:06:29,940
then we dynamically verify that this in

171
00:06:29,940 --> 00:06:33,180
fact is an example of a redos vulnerable

172
00:06:33,180 --> 00:06:35,100
radical expression

173
00:06:35,100 --> 00:06:36,539
I'll talk about each of these in turn

174
00:06:36,539 --> 00:06:39,840
but I'll start with the fuzzer

175
00:06:39,840 --> 00:06:41,400
this probably looks very similar to

176
00:06:41,400 --> 00:06:43,800
everybody who's seen a feedback fuzzing

177
00:06:43,800 --> 00:06:46,680
system I'm going to talk about a small

178
00:06:46,680 --> 00:06:48,660
amount in these particular components

179
00:06:48,660 --> 00:06:50,940
and for more details of course please

180
00:06:50,940 --> 00:06:53,340
read our paper

181
00:06:53,340 --> 00:06:56,039
for one we instrument the execution

182
00:06:56,039 --> 00:06:59,340
engine in regex this is the regular

183
00:06:59,340 --> 00:07:01,440
expression engine that is found in

184
00:07:01,440 --> 00:07:04,380
Firefox Chrome and node.js so has fairly

185
00:07:04,380 --> 00:07:05,880
widespread use

186
00:07:05,880 --> 00:07:09,840
in order to track execution and get

187
00:07:09,840 --> 00:07:13,500
feedback we instrument The Interpreter

188
00:07:13,500 --> 00:07:16,139
and we use an AFL style perf map to

189
00:07:16,139 --> 00:07:18,300
collect coverage this is effectively an

190
00:07:18,300 --> 00:07:20,160
array of cells where each cell will

191
00:07:20,160 --> 00:07:22,080
count the hit count of branches that

192
00:07:22,080 --> 00:07:26,419
occur in the byte code as it's running

193
00:07:27,000 --> 00:07:30,660
then of course we're trying to fuzz in a

194
00:07:30,660 --> 00:07:31,860
different way than you've seen in

195
00:07:31,860 --> 00:07:33,900
traditional fuzzers traditionally we're

196
00:07:33,900 --> 00:07:35,099
trying to look for something like

197
00:07:35,099 --> 00:07:37,680
crashes or bugs but instead what we're

198
00:07:37,680 --> 00:07:40,979
looking to do is find slow inputs so we

199
00:07:40,979 --> 00:07:42,539
want to find the longest path inputs

200
00:07:42,539 --> 00:07:45,840
while also avoiding a local Maxima and

201
00:07:45,840 --> 00:07:48,319
the way we do this is by doing a smart

202
00:07:48,319 --> 00:07:50,280
prioritization system

203
00:07:50,280 --> 00:07:52,680
each time we run the prioritization

204
00:07:52,680 --> 00:07:55,440
procedure we will go and we will find a

205
00:07:55,440 --> 00:07:57,240
representative from each of the

206
00:07:57,240 --> 00:07:58,919
components which will maximize that

207
00:07:58,919 --> 00:08:01,680
component when it runs and we prioritize

208
00:08:01,680 --> 00:08:04,199
those maximizing Representatives however

209
00:08:04,199 --> 00:08:06,300
there's a problem with this of course

210
00:08:06,300 --> 00:08:08,580
the entry point component is extremely

211
00:08:08,580 --> 00:08:10,740
boring every single string will execute

212
00:08:10,740 --> 00:08:12,720
it probably execute it exactly once so

213
00:08:12,720 --> 00:08:14,160
they're all maximizing and then you'll

214
00:08:14,160 --> 00:08:16,560
waste time so we also have a

215
00:08:16,560 --> 00:08:18,720
deprioritization method where if an

216
00:08:18,720 --> 00:08:20,580
input is found to be stale that is if

217
00:08:20,580 --> 00:08:23,099
it's not producing novel uh mutated

218
00:08:23,099 --> 00:08:25,860
Offspring then we will de-prioritize it

219
00:08:25,860 --> 00:08:28,580
and look elsewhere

220
00:08:29,160 --> 00:08:31,199
for the mutation engine we also have

221
00:08:31,199 --> 00:08:32,940
another interesting mutation we call it

222
00:08:32,940 --> 00:08:35,458
suggestion because The Interpreter is so

223
00:08:35,458 --> 00:08:37,679
simple most of the branches that are

224
00:08:37,679 --> 00:08:40,320
taken are actually based on a character

225
00:08:40,320 --> 00:08:42,419
comparison within the string that we're

226
00:08:42,419 --> 00:08:45,060
examining so during the execution

227
00:08:45,060 --> 00:08:47,220
whenever we take a branch we will

228
00:08:47,220 --> 00:08:50,100
actually note what character should have

229
00:08:50,100 --> 00:08:51,720
been at what position in the string in

230
00:08:51,720 --> 00:08:54,000
order to negate the branch condition

231
00:08:54,000 --> 00:08:56,580
then during mutation we will randomly go

232
00:08:56,580 --> 00:08:58,920
and change out that character to

233
00:08:58,920 --> 00:09:01,560
hopefully negate the uh that branch and

234
00:09:01,560 --> 00:09:05,839
uncover more interesting functionality

235
00:09:06,480 --> 00:09:08,700
that's information about our fuzzer

236
00:09:08,700 --> 00:09:10,500
so now moving on to the formula

237
00:09:10,500 --> 00:09:12,060
derivation procedure

238
00:09:12,060 --> 00:09:15,240
we take the single slowest string that

239
00:09:15,240 --> 00:09:16,920
our fuzzer identified and we call it the

240
00:09:16,920 --> 00:09:19,620
witness now this witness represents just

241
00:09:19,620 --> 00:09:21,600
a single point on a complexity curve so

242
00:09:21,600 --> 00:09:23,100
we actually don't really know how bad

243
00:09:23,100 --> 00:09:25,560
this is yet it could be a linear

244
00:09:25,560 --> 00:09:27,899
complexity curve which is okay it could

245
00:09:27,899 --> 00:09:30,300
be polynomial which is bad or it could

246
00:09:30,300 --> 00:09:33,720
be exponential which is very bad so what

247
00:09:33,720 --> 00:09:36,540
we do here is we attempt to find a

248
00:09:36,540 --> 00:09:39,120
formula which will describe both how to

249
00:09:39,120 --> 00:09:40,560
build attack strings then also let us

250
00:09:40,560 --> 00:09:42,839
measure the complexity curve we take

251
00:09:42,839 --> 00:09:44,459
inspiration from prior work and we say

252
00:09:44,459 --> 00:09:46,380
that a formula consists of an attack

253
00:09:46,380 --> 00:09:48,839
prefix some pump string that is repeated

254
00:09:48,839 --> 00:09:51,899
and then an attack suffix

255
00:09:51,899 --> 00:09:54,420
from here we actually just perform a

256
00:09:54,420 --> 00:09:56,640
heuristic scan over the witness string

257
00:09:56,640 --> 00:09:58,560
that we extracted from the fuzzer and we

258
00:09:58,560 --> 00:10:00,180
try to find a pump string which when

259
00:10:00,180 --> 00:10:02,640
repeated causes a significant slowness

260
00:10:02,640 --> 00:10:05,640
once we find that pump string we sample

261
00:10:05,640 --> 00:10:07,080
several different points and we use a

262
00:10:07,080 --> 00:10:09,120
model fitting procedure to figure out

263
00:10:09,120 --> 00:10:10,560
which of the three curves I mentioned

264
00:10:10,560 --> 00:10:14,660
before it best uh describes

265
00:10:15,120 --> 00:10:17,100
we take the output from that formula

266
00:10:17,100 --> 00:10:19,200
derivation procedure and then we feed it

267
00:10:19,200 --> 00:10:21,300
into our Dynamic verifier

268
00:10:21,300 --> 00:10:23,459
here the entire goal is just to make

269
00:10:23,459 --> 00:10:25,440
sure that this is is in fact an example

270
00:10:25,440 --> 00:10:28,200
of readoffs and what we do is we

271
00:10:28,200 --> 00:10:29,760
immediately find the longest attack

272
00:10:29,760 --> 00:10:31,320
string which is under 1 million

273
00:10:31,320 --> 00:10:33,060
characters and we see if it causes 10

274
00:10:33,060 --> 00:10:34,860
seconds of delay when we're executing it

275
00:10:34,860 --> 00:10:37,620
if it is then we label it as a redos

276
00:10:37,620 --> 00:10:39,720
vulnerable regular expression and we

277
00:10:39,720 --> 00:10:41,279
will binary search for the slowest

278
00:10:41,279 --> 00:10:43,620
possible string that causes 10 seconds

279
00:10:43,620 --> 00:10:44,880
of Slowdown

280
00:10:44,880 --> 00:10:47,220
10 seconds and 1 million characters is

281
00:10:47,220 --> 00:10:49,620
just values we found in the literature

282
00:10:49,620 --> 00:10:51,600
so we reuse them and they seem to work

283
00:10:51,600 --> 00:10:54,440
well for the description

284
00:10:54,540 --> 00:10:56,820
we evaluate against several prior Tools

285
00:10:56,820 --> 00:10:58,680
in fact the tools that we saw in the

286
00:10:58,680 --> 00:11:02,519
first presentation we also compare by

287
00:11:02,519 --> 00:11:04,800
replacing our slowdown fuzzer by a

288
00:11:04,800 --> 00:11:06,480
generic off-the-shelf algorithmic

289
00:11:06,480 --> 00:11:08,820
complexity fuzzer called perfuzz but for

290
00:11:08,820 --> 00:11:10,260
more details on that please check out

291
00:11:10,260 --> 00:11:11,399
our paper

292
00:11:11,399 --> 00:11:13,320
for the data sets we use three standard

293
00:11:13,320 --> 00:11:16,320
corpuses of regular expressions and we

294
00:11:16,320 --> 00:11:19,560
also scrape npm and we find 40 000

295
00:11:19,560 --> 00:11:23,579
regular expressions from that scrape

296
00:11:23,579 --> 00:11:25,260
the first question we have is well we

297
00:11:25,260 --> 00:11:26,820
built this fuzzer custom so it doesn't

298
00:11:26,820 --> 00:11:30,000
really work and so we collect coverage

299
00:11:30,000 --> 00:11:33,899
as we fuzz and this graph shows how that

300
00:11:33,899 --> 00:11:36,060
coverage evolves over time I want to

301
00:11:36,060 --> 00:11:38,220
point out the fact that the x-axis here

302
00:11:38,220 --> 00:11:39,720
is in a linear scale because the

303
00:11:39,720 --> 00:11:41,279
coverage actually increases so quickly

304
00:11:41,279 --> 00:11:42,779
so we sort of need to change it to see

305
00:11:42,779 --> 00:11:45,300
the detail here it turns out that even

306
00:11:45,300 --> 00:11:48,240
for a large regular Expressions we get

307
00:11:48,240 --> 00:11:50,279
quite good coverage within 10 seconds

308
00:11:50,279 --> 00:11:51,779
and excellent coverage within 100

309
00:11:51,779 --> 00:11:55,320
seconds so we think that the uh the

310
00:11:55,320 --> 00:11:58,920
fuzzer is properly exploring the program

311
00:11:58,920 --> 00:12:00,360
how does it compare in terms of

312
00:12:00,360 --> 00:12:02,160
detections this is a giant grid of

313
00:12:02,160 --> 00:12:03,959
numbers so I'm just going to focus your

314
00:12:03,959 --> 00:12:06,000
attention down in the bottom right we

315
00:12:06,000 --> 00:12:07,920
find that our true positive rate is much

316
00:12:07,920 --> 00:12:09,959
much better in fact two to seven times

317
00:12:09,959 --> 00:12:12,240
better than prior tools and our false

318
00:12:12,240 --> 00:12:15,000
negative rate is quite low

319
00:12:15,000 --> 00:12:17,399
we also find the same for our npm data

320
00:12:17,399 --> 00:12:19,260
set

321
00:12:19,260 --> 00:12:21,540
we also find several novel detections so

322
00:12:21,540 --> 00:12:24,360
we were assigned six cve numbers when

323
00:12:24,360 --> 00:12:25,800
reporting vulnerabilities in this

324
00:12:25,800 --> 00:12:27,079
investigation

325
00:12:27,079 --> 00:12:31,260
we verified and fixed dozens of bugs and

326
00:12:31,260 --> 00:12:33,600
the positives that we found contain

327
00:12:33,600 --> 00:12:35,519
several difficult to find detections

328
00:12:35,519 --> 00:12:38,700
including regular expression denial of

329
00:12:38,700 --> 00:12:40,560
service attack uh

330
00:12:40,560 --> 00:12:42,740
regular Expressions vulnerable to redos

331
00:12:42,740 --> 00:12:45,420
which requires certain Flags to be set

332
00:12:45,420 --> 00:12:47,700
in order for the redos vulnerability to

333
00:12:47,700 --> 00:12:48,839
present which of course we support

334
00:12:48,839 --> 00:12:50,459
natively because we're just running on

335
00:12:50,459 --> 00:12:51,620
the byte code

336
00:12:51,620 --> 00:12:54,839
there's also several detections where

337
00:12:54,839 --> 00:12:56,700
the regular expression itself is highly

338
00:12:56,700 --> 00:12:58,800
complex and so it sort of trips up other

339
00:12:58,800 --> 00:13:00,660
static analysis attempts

340
00:13:00,660 --> 00:13:04,320
and also we find some that use Advanced

341
00:13:04,320 --> 00:13:06,000
features like back references look

342
00:13:06,000 --> 00:13:09,620
arounds and special character groups

343
00:13:09,660 --> 00:13:11,160
just to wrap things up a quick

344
00:13:11,160 --> 00:13:12,720
recommendation we find when we're

345
00:13:12,720 --> 00:13:14,639
looking for the shortest string that

346
00:13:14,639 --> 00:13:17,700
triggers redos if you wanted to mitigate

347
00:13:17,700 --> 00:13:20,459
almost all of the redos vulnerabilities

348
00:13:20,459 --> 00:13:22,200
that we found you probably want to limit

349
00:13:22,200 --> 00:13:25,139
your input strings to maybe 1000 or 10

350
00:13:25,139 --> 00:13:27,120
000 characters and then 10 seconds of

351
00:13:27,120 --> 00:13:28,800
delay would be impossible in almost all

352
00:13:28,800 --> 00:13:30,899
of the vulnerabilities we've found

353
00:13:30,899 --> 00:13:34,260
so that seems like a good recommendation

354
00:13:34,260 --> 00:13:36,300
um just as a as a reminder here's a

355
00:13:36,300 --> 00:13:38,040
diagram of our system our artifacts are

356
00:13:38,040 --> 00:13:39,620
available so please go check them out

357
00:13:39,620 --> 00:13:42,120
and uh thank you everybody for attending

358
00:13:42,120 --> 00:13:45,260
my talk I'll take questions

