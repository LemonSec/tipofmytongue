1
00:00:08,540 --> 00:00:12,120
hi everyone my name is Anna Maria kratzu

2
00:00:12,120 --> 00:00:13,920
and I'm a PhD candidate in the

3
00:00:13,920 --> 00:00:15,719
computational Privacy group at Imperial

4
00:00:15,719 --> 00:00:17,880
College London and the work I'm

5
00:00:17,880 --> 00:00:20,160
presenting today is the result of a

6
00:00:20,160 --> 00:00:21,779
collaboration with colleague shubham

7
00:00:21,779 --> 00:00:25,520
Jain and Eva Alexander Monroe

8
00:00:25,619 --> 00:00:28,199
billions of people around the world are

9
00:00:28,199 --> 00:00:30,060
using end-to-end encryption to

10
00:00:30,060 --> 00:00:32,340
communicate securely in messaging

11
00:00:32,340 --> 00:00:35,820
platforms encryption is a mechanism to

12
00:00:35,820 --> 00:00:38,700
secure data by encoding it in such a way

13
00:00:38,700 --> 00:00:41,040
that only authorized parties can access

14
00:00:41,040 --> 00:00:43,680
it and in digital Communications this

15
00:00:43,680 --> 00:00:45,899
means that only the sender and recipient

16
00:00:45,899 --> 00:00:47,879
can access the content of the

17
00:00:47,879 --> 00:00:49,440
communication

18
00:00:49,440 --> 00:00:53,399
more people are going to use end-to-end

19
00:00:53,399 --> 00:00:55,020
encryption in the future as more

20
00:00:55,020 --> 00:00:57,300
messaging platforms are adopting this

21
00:00:57,300 --> 00:00:59,039
technology

22
00:00:59,039 --> 00:01:00,440
yet

23
00:01:00,440 --> 00:01:02,760
governments and law enforcement agencies

24
00:01:02,760 --> 00:01:04,739
around the world have argued that the

25
00:01:04,739 --> 00:01:06,180
widespread adoption of end-to-end

26
00:01:06,180 --> 00:01:07,200
encryption

27
00:01:07,200 --> 00:01:09,659
restricts the detection of illegal

28
00:01:09,659 --> 00:01:12,840
activity and also prevents the detection

29
00:01:12,840 --> 00:01:14,400
of illegal content

30
00:01:14,400 --> 00:01:17,220
one main example of illegal content is

31
00:01:17,220 --> 00:01:20,900
child sexual abuse material

32
00:01:21,540 --> 00:01:24,619
client-side scanning has been proposed

33
00:01:24,619 --> 00:01:27,659
by researchers organizations around the

34
00:01:27,659 --> 00:01:30,180
world and governments and is seen as one

35
00:01:30,180 --> 00:01:32,580
of the most promising solutions to allow

36
00:01:32,580 --> 00:01:34,680
the detection of illegal content

37
00:01:34,680 --> 00:01:38,280
while maintaining end-to-end encryption

38
00:01:38,280 --> 00:01:39,900
and

39
00:01:39,900 --> 00:01:40,619
um

40
00:01:40,619 --> 00:01:44,100
several laws are being debated as we

41
00:01:44,100 --> 00:01:46,200
speak in the European Union and in the

42
00:01:46,200 --> 00:01:49,500
UK that would allow governments to

43
00:01:49,500 --> 00:01:52,439
mandate the messaging platforms to

44
00:01:52,439 --> 00:01:56,360
detect and take down illegal content

45
00:01:56,520 --> 00:01:59,820
how does client-side scanning work well

46
00:01:59,820 --> 00:02:02,100
let's imagine I want to share an image

47
00:02:02,100 --> 00:02:05,060
of a dog to a friend

48
00:02:05,060 --> 00:02:07,920
the the client-side scanning system

49
00:02:07,920 --> 00:02:10,500
would work on the user's device

50
00:02:10,500 --> 00:02:13,860
and would detect illegal content before

51
00:02:13,860 --> 00:02:16,319
it is encrypted and sent to the

52
00:02:16,319 --> 00:02:17,459
recipient

53
00:02:17,459 --> 00:02:20,400
so this image that I want to send would

54
00:02:20,400 --> 00:02:23,220
be converted to a perceptual would be

55
00:02:23,220 --> 00:02:24,720
converted to a fingerprint using

56
00:02:24,720 --> 00:02:26,340
perceptual hashing

57
00:02:26,340 --> 00:02:28,680
and then the fingerprint would be

58
00:02:28,680 --> 00:02:31,020
checked for a match against the database

59
00:02:31,020 --> 00:02:33,060
of illegal content and here I'm showing

60
00:02:33,060 --> 00:02:36,180
a database with three illegal images

61
00:02:36,180 --> 00:02:39,599
if no match is found then the image

62
00:02:39,599 --> 00:02:42,180
would be encrypted and sent to the it's

63
00:02:42,180 --> 00:02:43,800
intended recipient

64
00:02:43,800 --> 00:02:46,440
while if a match is found then the raw

65
00:02:46,440 --> 00:02:49,260
unencrypted content would be shared for

66
00:02:49,260 --> 00:02:51,300
further action manual inspection

67
00:02:51,300 --> 00:02:55,379
depending on the proposed solution

68
00:02:55,379 --> 00:02:59,040
the core technology behind these

69
00:02:59,040 --> 00:03:01,860
Solutions is perceptual hashing which

70
00:03:01,860 --> 00:03:04,379
converts which is able to convert an

71
00:03:04,379 --> 00:03:07,200
image of any size to a fixed size Vector

72
00:03:07,200 --> 00:03:09,840
representation which usually consists of

73
00:03:09,840 --> 00:03:12,659
bits but can also be real valued

74
00:03:12,659 --> 00:03:15,200
and unlike cryptographic hashes

75
00:03:15,200 --> 00:03:19,220
perceptual hashes are designed so that

76
00:03:19,220 --> 00:03:23,700
they map similar images to similar

77
00:03:23,700 --> 00:03:25,920
fingerprints and because of this

78
00:03:25,920 --> 00:03:28,260
property image similarity can be

79
00:03:28,260 --> 00:03:30,720
computed on the fingerprints rather than

80
00:03:30,720 --> 00:03:34,019
on the images themselves

81
00:03:34,019 --> 00:03:37,200
and perceptual hashing therefore allows

82
00:03:37,200 --> 00:03:41,099
to perform similarity matching

83
00:03:41,099 --> 00:03:42,420
now

84
00:03:42,420 --> 00:03:45,480
um again for the sake of example let's

85
00:03:45,480 --> 00:03:47,780
imagine we live in a world where dogs

86
00:03:47,780 --> 00:03:50,780
are considered illegal content

87
00:03:50,780 --> 00:03:53,400
the database of illegal content against

88
00:03:53,400 --> 00:03:56,040
which the match matching is performed

89
00:03:56,040 --> 00:03:58,620
would consist of the Fingerprints of

90
00:03:58,620 --> 00:04:00,959
these images I'm showing here

91
00:04:00,959 --> 00:04:03,360
and I'm also assuming that the image I

92
00:04:03,360 --> 00:04:06,180
want to share is one such illegal

93
00:04:06,180 --> 00:04:07,080
content

94
00:04:07,080 --> 00:04:09,420
uh the matching would be performed by

95
00:04:09,420 --> 00:04:10,920
Computing the distance between the

96
00:04:10,920 --> 00:04:13,799
fingerprint of the image I'm sending and

97
00:04:13,799 --> 00:04:16,440
the fingerprints in the database and if

98
00:04:16,440 --> 00:04:18,180
the fingerprints are closer than a

99
00:04:18,180 --> 00:04:20,100
predefined threshold t

100
00:04:20,100 --> 00:04:21,600
then

101
00:04:21,600 --> 00:04:24,120
um the content on the device is

102
00:04:24,120 --> 00:04:26,639
considered to be a match and the

103
00:04:26,639 --> 00:04:27,840
intuition behind this detection

104
00:04:27,840 --> 00:04:30,919
threshold is that larger thresholds

105
00:04:30,919 --> 00:04:34,100
allow to detect more content including

106
00:04:34,100 --> 00:04:38,639
possibly safe non-illegal content

107
00:04:38,639 --> 00:04:42,060
the object of our study is to understand

108
00:04:42,060 --> 00:04:44,400
whether client-side scanning systems are

109
00:04:44,400 --> 00:04:46,560
robust to evasion attacks so these

110
00:04:46,560 --> 00:04:48,540
because this detection is being

111
00:04:48,540 --> 00:04:52,139
performed on the user devices it has a

112
00:04:52,139 --> 00:04:54,240
serious privacy implications the

113
00:04:54,240 --> 00:04:55,759
question that we're asking is whether

114
00:04:55,759 --> 00:04:59,280
this would be robust against a motivated

115
00:04:59,280 --> 00:05:02,220
adversary who is trying to share illegal

116
00:05:02,220 --> 00:05:05,780
content and not be detected

117
00:05:06,419 --> 00:05:08,460
um coming back to the database

118
00:05:08,460 --> 00:05:10,380
illustration

119
00:05:10,380 --> 00:05:12,660
um we formalize this question

120
00:05:12,660 --> 00:05:15,600
by trying to understand if small

121
00:05:15,600 --> 00:05:18,240
perturbations can be added to the

122
00:05:18,240 --> 00:05:21,000
original image in such a way that the

123
00:05:21,000 --> 00:05:23,340
content of the image would be preserved

124
00:05:23,340 --> 00:05:24,960
but the fingerprint of the modified

125
00:05:24,960 --> 00:05:28,320
image would fall outside the ball of

126
00:05:28,320 --> 00:05:33,000
radius T of the original Fingerprints of

127
00:05:33,000 --> 00:05:35,400
the true match

128
00:05:35,400 --> 00:05:37,620
so in other words there are two goals

129
00:05:37,620 --> 00:05:40,139
for our adversary one of them is that

130
00:05:40,139 --> 00:05:42,120
the modified image should be as similar

131
00:05:42,120 --> 00:05:44,340
as possible to the original and the

132
00:05:44,340 --> 00:05:46,259
second goal is for the fingerprint to

133
00:05:46,259 --> 00:05:47,880
evade detection by the client-side

134
00:05:47,880 --> 00:05:50,460
scanning system and another assumption

135
00:05:50,460 --> 00:05:52,440
that we make is that the attacker would

136
00:05:52,440 --> 00:05:54,960
have black box access to the perceptual

137
00:05:54,960 --> 00:05:57,720
hashing algorithm this makes the attack

138
00:05:57,720 --> 00:05:59,820
realistic

139
00:05:59,820 --> 00:06:03,120
uh but also means that the attacker does

140
00:06:03,120 --> 00:06:04,919
not know how the perceptual hashing

141
00:06:04,919 --> 00:06:07,800
algorithm works and therefore cannot use

142
00:06:07,800 --> 00:06:11,780
this knowledge to devise the attack

143
00:06:13,380 --> 00:06:16,199
we formalize this problem as an

144
00:06:16,199 --> 00:06:18,360
optimization problem so we fixed the

145
00:06:18,360 --> 00:06:21,720
image to be attacked and we are assuming

146
00:06:21,720 --> 00:06:23,280
Black Box access to the hashing

147
00:06:23,280 --> 00:06:25,880
algorithm meaning that we can

148
00:06:25,880 --> 00:06:29,039
query the hashing algorithm arbitrarily

149
00:06:29,039 --> 00:06:32,280
many times on any input that we want

150
00:06:32,280 --> 00:06:35,100
and the goal of the attacker is to

151
00:06:35,100 --> 00:06:36,900
maximize the distance between the hash

152
00:06:36,900 --> 00:06:39,240
of the modified image and the original

153
00:06:39,240 --> 00:06:42,660
image or at least to increases to

154
00:06:42,660 --> 00:06:44,520
increase it until it becomes larger than

155
00:06:44,520 --> 00:06:45,780
t

156
00:06:45,780 --> 00:06:48,180
and there are two constraints for our

157
00:06:48,180 --> 00:06:50,699
optimization problem one is that the

158
00:06:50,699 --> 00:06:52,380
difference visual difference between the

159
00:06:52,380 --> 00:06:55,259
images should be capped by an upper

160
00:06:55,259 --> 00:06:57,960
bound which here we quantify this using

161
00:06:57,960 --> 00:06:59,639
the L2 distance

162
00:06:59,639 --> 00:07:02,160
and we want this to not exceed a small

163
00:07:02,160 --> 00:07:04,979
constant Epsilon and the image should

164
00:07:04,979 --> 00:07:06,780
remain valid

165
00:07:06,780 --> 00:07:10,259
the perturbed image should be valid

166
00:07:10,259 --> 00:07:13,800
we propose an iterative procedure to

167
00:07:13,800 --> 00:07:15,360
solve this problem

168
00:07:15,360 --> 00:07:18,539
so first recall that we only assumed

169
00:07:18,539 --> 00:07:20,580
that the attacker has accessed has Black

170
00:07:20,580 --> 00:07:23,220
Box access to the algorithm and the

171
00:07:23,220 --> 00:07:25,319
intuition behind our attack is that we

172
00:07:25,319 --> 00:07:28,259
want to search for perturbations that

173
00:07:28,259 --> 00:07:30,000
would increase the distance between the

174
00:07:30,000 --> 00:07:32,460
fingerprints as much as possible

175
00:07:32,460 --> 00:07:33,960
and this

176
00:07:33,960 --> 00:07:35,880
would normally be done using gradient

177
00:07:35,880 --> 00:07:38,340
estimation techniques we use techniques

178
00:07:38,340 --> 00:07:39,780
from the black box optimization

179
00:07:39,780 --> 00:07:42,599
literature to estimate the gradient of

180
00:07:42,599 --> 00:07:44,520
the function

181
00:07:44,520 --> 00:07:46,560
then we would update the perturbation

182
00:07:46,560 --> 00:07:48,780
with the sign of the gradient

183
00:07:48,780 --> 00:07:51,599
and then clip the perturbation so that

184
00:07:51,599 --> 00:07:54,479
it does not exceed the bound the visual

185
00:07:54,479 --> 00:07:56,819
similarity visual dissimilarity bound

186
00:07:56,819 --> 00:07:59,759
that we set in the beginning

187
00:07:59,759 --> 00:08:02,400
and we do this iteratively

188
00:08:02,400 --> 00:08:07,800
until the target objective is larger

189
00:08:07,800 --> 00:08:10,199
than the threshold T and we continue the

190
00:08:10,199 --> 00:08:13,199
algorithm until we find this or until a

191
00:08:13,199 --> 00:08:15,840
maximum number of iterations

192
00:08:15,840 --> 00:08:18,060
now to enforce that the visual

193
00:08:18,060 --> 00:08:20,340
difference between the images is as

194
00:08:20,340 --> 00:08:23,220
small as possible we start with Epsilon

195
00:08:23,220 --> 00:08:26,940
values that are very small and when the

196
00:08:26,940 --> 00:08:29,520
objective stops increasing we

197
00:08:29,520 --> 00:08:32,099
increase the Epsilon Bound by a little

198
00:08:32,099 --> 00:08:35,458
bit this way we can control the amount

199
00:08:35,458 --> 00:08:38,779
of visual perturbation

200
00:08:39,240 --> 00:08:42,240
the main result of our paper is that

201
00:08:42,240 --> 00:08:45,420
almost all the images that we run our

202
00:08:45,420 --> 00:08:48,120
experiments against could be attacked so

203
00:08:48,120 --> 00:08:51,180
we run our attack 10 times every time

204
00:08:51,180 --> 00:08:53,399
against 1 000 random images from the

205
00:08:53,399 --> 00:08:56,279
imagenet data set and we find that 99

206
00:08:56,279 --> 00:08:58,740
more than 99.9 percent of the images

207
00:08:58,740 --> 00:09:02,160
could be successfully attacked and we

208
00:09:02,160 --> 00:09:04,320
did this for five popular perceptual

209
00:09:04,320 --> 00:09:07,500
hashing algorithms so P Hash A Hash D

210
00:09:07,500 --> 00:09:10,440
hash and Facebook's PDQ algorithm and a

211
00:09:10,440 --> 00:09:13,380
variant of P hash using real valued

212
00:09:13,380 --> 00:09:15,120
hashes

213
00:09:15,120 --> 00:09:17,940
and we also looked at a broad range of

214
00:09:17,940 --> 00:09:20,459
detection thresholds here the intuition

215
00:09:20,459 --> 00:09:22,260
is that the larger the detection

216
00:09:22,260 --> 00:09:23,720
threshold

217
00:09:23,720 --> 00:09:26,279
the more an image would have to be

218
00:09:26,279 --> 00:09:29,940
perturbed and it was not clear to us

219
00:09:29,940 --> 00:09:31,920
when we started whether it would be

220
00:09:31,920 --> 00:09:33,779
possible to find small visual

221
00:09:33,779 --> 00:09:35,279
perturbations

222
00:09:35,279 --> 00:09:37,279
preparations that are visually

223
00:09:37,279 --> 00:09:40,200
imperceptible that would evade detection

224
00:09:40,200 --> 00:09:44,480
for large values of the threshold t

225
00:09:45,480 --> 00:09:48,180
and here I'm showing some example of

226
00:09:48,180 --> 00:09:50,880
adversarial perturbations discovered

227
00:09:50,880 --> 00:09:52,500
using our attack

228
00:09:52,500 --> 00:09:54,660
we are showing results for the PDQ

229
00:09:54,660 --> 00:09:56,519
algorithm because this is the hardest

230
00:09:56,519 --> 00:09:58,200
one to attack

231
00:09:58,200 --> 00:10:01,440
and as you can see there is a very small

232
00:10:01,440 --> 00:10:04,620
difference between the original image on

233
00:10:04,620 --> 00:10:06,480
the left and the modified image on the

234
00:10:06,480 --> 00:10:08,580
right

235
00:10:08,580 --> 00:10:12,000
now I would like to draw the attention

236
00:10:12,000 --> 00:10:14,399
to the fact that actually what our

237
00:10:14,399 --> 00:10:16,440
attack is doing is to push the

238
00:10:16,440 --> 00:10:19,800
fingerprint outside the ball of radius T

239
00:10:19,800 --> 00:10:23,940
of the original fingerprint however the

240
00:10:23,940 --> 00:10:25,920
modified fingerprint might now fall

241
00:10:25,920 --> 00:10:28,399
inside another ball

242
00:10:28,399 --> 00:10:32,160
so the question that we ask is how often

243
00:10:32,160 --> 00:10:35,220
does our attack evade detection by the

244
00:10:35,220 --> 00:10:37,019
entire system

245
00:10:37,019 --> 00:10:39,120
and to do this we compute the false

246
00:10:39,120 --> 00:10:41,580
negative rate this is defined as the

247
00:10:41,580 --> 00:10:43,440
fraction of modified illegal images

248
00:10:43,440 --> 00:10:46,920
using our attack that evade detection

249
00:10:46,920 --> 00:10:48,540
and here I'm showing the results

250
00:10:48,540 --> 00:10:51,480
obtained against the PDQ algorithm we

251
00:10:51,480 --> 00:10:53,220
can see that for smaller threshold

252
00:10:53,220 --> 00:10:55,500
values all the modified illegal images

253
00:10:55,500 --> 00:10:58,320
evade detection yet as the threshold

254
00:10:58,320 --> 00:11:00,019
increases

255
00:11:00,019 --> 00:11:03,000
fewer of the images modified using our

256
00:11:03,000 --> 00:11:05,160
attack evade detection

257
00:11:05,160 --> 00:11:07,440
so it would seem that increasing the

258
00:11:07,440 --> 00:11:10,019
threshold could make the system more

259
00:11:10,019 --> 00:11:11,940
robust to our attack

260
00:11:11,940 --> 00:11:14,220
however increasing the detection

261
00:11:14,220 --> 00:11:17,579
threshold also results in more safe

262
00:11:17,579 --> 00:11:20,880
images being wrongly flagged and here we

263
00:11:20,880 --> 00:11:23,880
compute the false false net sorry false

264
00:11:23,880 --> 00:11:25,079
positive rate

265
00:11:25,079 --> 00:11:27,959
of the client-side scanning system for a

266
00:11:27,959 --> 00:11:31,019
database size of 100 000 images and we

267
00:11:31,019 --> 00:11:33,420
can see that for the largest threshold a

268
00:11:33,420 --> 00:11:36,360
lot of images would be flagged and

269
00:11:36,360 --> 00:11:38,040
to give you a sense of what this means

270
00:11:38,040 --> 00:11:39,200
in practice

271
00:11:39,200 --> 00:11:43,519
we took the estimate of number of images

272
00:11:43,519 --> 00:11:46,980
shared on WhatsApp daily this number is

273
00:11:46,980 --> 00:11:49,860
4.5 billion images using the false

274
00:11:49,860 --> 00:11:51,720
positive rate computed here for the

275
00:11:51,720 --> 00:11:53,760
largest threshold this means that more

276
00:11:53,760 --> 00:11:57,000
than 1 billion images would be falsely

277
00:11:57,000 --> 00:11:59,519
flagged in practice which is a huge

278
00:11:59,519 --> 00:12:03,860
number and can incur a huge privacy cost

279
00:12:04,320 --> 00:12:08,820
uh finally we also look at we try to

280
00:12:08,820 --> 00:12:11,760
understand whether sorry whether the

281
00:12:11,760 --> 00:12:14,279
attack could be mitigated

282
00:12:14,279 --> 00:12:17,700
by expanding the database with hashes

283
00:12:17,700 --> 00:12:21,000
of images modified using our attack and

284
00:12:21,000 --> 00:12:23,880
we showed that by changing a few

285
00:12:23,880 --> 00:12:25,680
parameters of our attack

286
00:12:25,680 --> 00:12:27,959
that a wide range of diverse

287
00:12:27,959 --> 00:12:30,180
perturbations can be found so this means

288
00:12:30,180 --> 00:12:32,459
that the attack doesn't always end up in

289
00:12:32,459 --> 00:12:34,770
the same position but actually

290
00:12:34,770 --> 00:12:35,100
[Music]

291
00:12:35,100 --> 00:12:35,940
um

292
00:12:35,940 --> 00:12:38,459
the the perturbations that can be found

293
00:12:38,459 --> 00:12:41,700
using our attack are quite widespread in

294
00:12:41,700 --> 00:12:43,680
the space around the original

295
00:12:43,680 --> 00:12:45,180
fingerprint

296
00:12:45,180 --> 00:12:48,120
therefore and there is another argument

297
00:12:48,120 --> 00:12:51,060
to be made here which is that if the

298
00:12:51,060 --> 00:12:53,820
database is expanded with too many

299
00:12:53,820 --> 00:12:57,300
images by a very large Factor then this

300
00:12:57,300 --> 00:12:59,279
is likely to impact the false positive

301
00:12:59,279 --> 00:13:01,260
rate of the system

302
00:13:01,260 --> 00:13:04,019
because intuitively a larger part of the

303
00:13:04,019 --> 00:13:07,700
space of images would be covered

304
00:13:08,899 --> 00:13:13,440
we carried out further analysis and I

305
00:13:13,440 --> 00:13:15,540
refer you to our paper for detailed

306
00:13:15,540 --> 00:13:18,899
results the key insights are that

307
00:13:18,899 --> 00:13:21,660
increasing the size of the database has

308
00:13:21,660 --> 00:13:23,459
an impact on the false positive rate and

309
00:13:23,459 --> 00:13:25,380
false negative rate of the client-side

310
00:13:25,380 --> 00:13:27,660
scanning system the false positive rate

311
00:13:27,660 --> 00:13:30,480
increases while the false negative rate

312
00:13:30,480 --> 00:13:33,779
decreases slowly

313
00:13:33,779 --> 00:13:37,500
to understand how optimal our Black Box

314
00:13:37,500 --> 00:13:40,620
attack is in terms of

315
00:13:40,620 --> 00:13:43,220
finding a minimal perturbation

316
00:13:43,220 --> 00:13:46,980
we studied the DCT algorithm discrete

317
00:13:46,980 --> 00:13:48,720
cosine transform which is a common

318
00:13:48,720 --> 00:13:50,940
building block for several perceptual

319
00:13:50,940 --> 00:13:53,339
hashing algorithms and for this

320
00:13:53,339 --> 00:13:55,560
algorithm we were able to devise white

321
00:13:55,560 --> 00:13:57,899
box optimal attacks

322
00:13:57,899 --> 00:14:00,240
and we compared the perturbations found

323
00:14:00,240 --> 00:14:01,920
using the Black Box attack with the

324
00:14:01,920 --> 00:14:03,839
perturbations found using the white box

325
00:14:03,839 --> 00:14:05,639
attack and found that the Black Box

326
00:14:05,639 --> 00:14:09,240
perturbations are close to Optimal

327
00:14:09,240 --> 00:14:12,240
finally we also studied a variation of

328
00:14:12,240 --> 00:14:14,339
the client-side scanning system

329
00:14:14,339 --> 00:14:17,459
which flags user only after the number

330
00:14:17,459 --> 00:14:19,800
of matches exceeds a predefined

331
00:14:19,800 --> 00:14:22,380
threshold this is similar to what Apple

332
00:14:22,380 --> 00:14:26,940
proposed to implement in their proposal

333
00:14:26,940 --> 00:14:29,399
of an implementation for the client-side

334
00:14:29,399 --> 00:14:32,220
scanning system and our results suggest

335
00:14:32,220 --> 00:14:34,800
that the measure is not robust to our

336
00:14:34,800 --> 00:14:37,040
attack

337
00:14:37,079 --> 00:14:40,500
finally I would like to conclude so to

338
00:14:40,500 --> 00:14:42,959
summarize we studied perceptual hashing

339
00:14:42,959 --> 00:14:45,000
based client-side scanning which is

340
00:14:45,000 --> 00:14:46,680
currently proposed as a privacy

341
00:14:46,680 --> 00:14:48,480
preserving solution for detecting

342
00:14:48,480 --> 00:14:50,279
illegal content while maintaining

343
00:14:50,279 --> 00:14:51,959
end-to-end encryption

344
00:14:51,959 --> 00:14:54,000
and our results show that perceptual

345
00:14:54,000 --> 00:14:56,459
hashing based client-side scanning

346
00:14:56,459 --> 00:14:59,160
might not be a robust solution

347
00:14:59,160 --> 00:15:01,500
because images can be modified to evade

348
00:15:01,500 --> 00:15:04,019
detection and we also showed how simple

349
00:15:04,019 --> 00:15:06,420
fixes such as increasing the threshold

350
00:15:06,420 --> 00:15:08,699
or increasing the database size are

351
00:15:08,699 --> 00:15:12,199
unlikely to help and in a new blog post

352
00:15:12,199 --> 00:15:16,500
so the results the experimental results

353
00:15:16,500 --> 00:15:19,019
from this paper were carried out against

354
00:15:19,019 --> 00:15:21,480
shallow perceptual hashing algorithms

355
00:15:21,480 --> 00:15:24,860
these are algorithms where the the

356
00:15:24,860 --> 00:15:27,300
fingerprints are manually designed using

357
00:15:27,300 --> 00:15:30,660
mathematical functions however one could

358
00:15:30,660 --> 00:15:32,940
imagine using perceptual hash device in

359
00:15:32,940 --> 00:15:35,160
perceptual hashing algorithms using deep

360
00:15:35,160 --> 00:15:38,699
learning methods and we show the Deep

361
00:15:38,699 --> 00:15:41,100
perceptual hashing is also vulnerable to

362
00:15:41,100 --> 00:15:44,339
our attack the

363
00:15:44,339 --> 00:15:47,760
um the system that we look at is a deep

364
00:15:47,760 --> 00:15:49,740
learning model that one

365
00:15:49,740 --> 00:15:51,800
the image similarity challenge

366
00:15:51,800 --> 00:15:54,300
competition organized by Facebook where

367
00:15:54,300 --> 00:15:56,519
the goal was to detect edited copies of

368
00:15:56,519 --> 00:15:57,959
images

369
00:15:57,959 --> 00:16:00,180
thank you very much and thank you that

370
00:16:00,180 --> 00:16:02,540
was great

