1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,420 --> 00:00:11,340
hi everyone I'm chuong from Princeton

3
00:00:11,340 --> 00:00:13,440
University today I'm going to talk about

4
00:00:13,440 --> 00:00:15,960
our patch cleanser defense which is a

5
00:00:15,960 --> 00:00:18,539
joint work with study and pretty

6
00:00:18,539 --> 00:00:20,939
so patch cleanser is a certifiably

7
00:00:20,939 --> 00:00:23,160
robust defense against adversary patches

8
00:00:23,160 --> 00:00:25,320
and I will start this talk by

9
00:00:25,320 --> 00:00:27,960
introducing these two concepts

10
00:00:27,960 --> 00:00:30,900
first the other patch attack is

11
00:00:30,900 --> 00:00:32,940
environment of the adversary example

12
00:00:32,940 --> 00:00:35,040
attached against machine learning model

13
00:00:35,040 --> 00:00:37,800
the attacker would put all other pixel

14
00:00:37,800 --> 00:00:40,440
within a local region to form a patch

15
00:00:40,440 --> 00:00:42,600
and then they will optimize the patch

16
00:00:42,600 --> 00:00:44,579
content to induce test time modern

17
00:00:44,579 --> 00:00:46,739
misclassification

18
00:00:46,739 --> 00:00:48,719
the interesting thing about this attack

19
00:00:48,719 --> 00:00:51,059
is that the attacker can print and

20
00:00:51,059 --> 00:00:52,980
attach the patch to the physical scene

21
00:00:52,980 --> 00:00:55,020
then any image taken from the scene

22
00:00:55,020 --> 00:00:57,480
become adversarial here in this example

23
00:00:57,480 --> 00:00:59,520
a sticker on the stop sign can change it

24
00:00:59,520 --> 00:01:02,340
to a speed limit and also a patch on the

25
00:01:02,340 --> 00:01:05,400
table can change a banana to a toaster

26
00:01:05,400 --> 00:01:07,799
so this attack imposes a threat to

27
00:01:07,799 --> 00:01:09,420
machine learning applications that are

28
00:01:09,420 --> 00:01:11,820
interacting with the physical world so

29
00:01:11,820 --> 00:01:13,860
here our research question is

30
00:01:13,860 --> 00:01:16,320
how can we build robust models against

31
00:01:16,320 --> 00:01:18,659
adversary patches

32
00:01:18,659 --> 00:01:20,820
as the very first step we want to

33
00:01:20,820 --> 00:01:24,119
understand what is robustness and how

34
00:01:24,119 --> 00:01:27,060
can we quantify and evaluate it

35
00:01:27,060 --> 00:01:28,860
usually people will just use a specific

36
00:01:28,860 --> 00:01:30,540
attack to attack the system for

37
00:01:30,540 --> 00:01:32,460
robustness evaluation

38
00:01:32,460 --> 00:01:35,159
however the problem is that the

39
00:01:35,159 --> 00:01:37,140
robustness evalued in this way today

40
00:01:37,140 --> 00:01:39,840
might be compromised by smarter adaptive

41
00:01:39,840 --> 00:01:41,939
attack in the future

42
00:01:41,939 --> 00:01:43,920
we have seen a lot of concrete examples

43
00:01:43,920 --> 00:01:46,200
in the literature that the defenses are

44
00:01:46,200 --> 00:01:48,780
being broken by adaptive attack maybe

45
00:01:48,780 --> 00:01:50,159
just a few months after their

46
00:01:50,159 --> 00:01:51,240
publication

47
00:01:51,240 --> 00:01:53,399
so this also demonstrates the challenges

48
00:01:53,399 --> 00:01:55,560
of design defenses against adversary

49
00:01:55,560 --> 00:01:56,939
attacks

50
00:01:56,939 --> 00:01:58,740
so here we want to ask

51
00:01:58,740 --> 00:02:01,740
can we Define defenses in a special way

52
00:02:01,740 --> 00:02:04,079
such that we can prove their robustness

53
00:02:04,079 --> 00:02:07,079
against any future adaptive strategy

54
00:02:07,079 --> 00:02:08,818
and that is the focus of today's talk

55
00:02:08,818 --> 00:02:12,799
called Study Bible robustness

56
00:02:12,840 --> 00:02:14,879
the idea is that given a defense model

57
00:02:14,879 --> 00:02:17,940
an input image and a thread model

58
00:02:17,940 --> 00:02:20,160
we want to generate a robustness

59
00:02:20,160 --> 00:02:22,140
certificate which does the model

60
00:02:22,140 --> 00:02:24,120
prediction is always correct no matter

61
00:02:24,120 --> 00:02:26,280
what a white box adaptive attacker

62
00:02:26,280 --> 00:02:28,379
within the threat model does

63
00:02:28,379 --> 00:02:30,780
here the white box adaptive means that

64
00:02:30,780 --> 00:02:32,459
attacker can know everything about the

65
00:02:32,459 --> 00:02:34,500
defense including the defense algorithm

66
00:02:34,500 --> 00:02:36,480
defense parameter model weights model

67
00:02:36,480 --> 00:02:38,040
architectures

68
00:02:38,040 --> 00:02:40,500
and a typical threat model used in the

69
00:02:40,500 --> 00:02:42,599
literature is one square patch that

70
00:02:42,599 --> 00:02:44,459
takes two percent of the image pixels

71
00:02:44,459 --> 00:02:47,160
and it can have any content and can be

72
00:02:47,160 --> 00:02:50,220
placed at any given image location

73
00:02:50,220 --> 00:02:52,560
the certificate aims to account for all

74
00:02:52,560 --> 00:02:54,300
these different attack scenarios so that

75
00:02:54,300 --> 00:02:57,540
we don't need to worry about the future

76
00:02:57,540 --> 00:02:59,700
here in patch peninsula of course first

77
00:02:59,700 --> 00:03:01,440
we want to achieve state-of-the-art

78
00:03:01,440 --> 00:03:02,879
study viable robustness against

79
00:03:02,879 --> 00:03:04,860
adversary patches to have strong

80
00:03:04,860 --> 00:03:06,959
robustness guarantees

81
00:03:06,959 --> 00:03:08,760
being addition to that we want to

82
00:03:08,760 --> 00:03:10,680
highlight that our certifiable

83
00:03:10,680 --> 00:03:13,200
robustness comes at a minimum cost of

84
00:03:13,200 --> 00:03:14,819
clean performance

85
00:03:14,819 --> 00:03:17,040
unlike a prior war that is induced

86
00:03:17,040 --> 00:03:19,560
usually like 20 clean accuracy job for

87
00:03:19,560 --> 00:03:21,659
imagenety does that from the undefended

88
00:03:21,659 --> 00:03:22,620
model

89
00:03:22,620 --> 00:03:25,819
our patch cleanser can still maintain

90
00:03:25,819 --> 00:03:28,080
state-of-the-art clean accuracy even

91
00:03:28,080 --> 00:03:30,480
among undefended model

92
00:03:30,480 --> 00:03:33,000
this is the first time we find a defense

93
00:03:33,000 --> 00:03:34,680
with state-of-the-art certifiable

94
00:03:34,680 --> 00:03:37,019
robustness and clean accuracy at the

95
00:03:37,019 --> 00:03:38,819
same time

96
00:03:38,819 --> 00:03:42,000
and next I will talk about Defender also

97
00:03:42,000 --> 00:03:44,220
so impact cleanser we propose a pixel

98
00:03:44,220 --> 00:03:46,260
masking strategy the idea is very simple

99
00:03:46,260 --> 00:03:48,540
we try to mask out the entire patch to

100
00:03:48,540 --> 00:03:50,519
neutralize the adversary effect

101
00:03:50,519 --> 00:03:53,700
once we remove our third pixels then we

102
00:03:53,700 --> 00:03:55,680
can recover the correct prediction using

103
00:03:55,680 --> 00:03:58,200
any state-of-the-art image classifier

104
00:03:58,200 --> 00:04:00,120
the main challenge is

105
00:04:00,120 --> 00:04:02,159
how can we mask out the patch without

106
00:04:02,159 --> 00:04:04,080
knowing where the patch is

107
00:04:04,080 --> 00:04:06,299
and especially we need to do it in a

108
00:04:06,299 --> 00:04:08,760
certifiably robust manner

109
00:04:08,760 --> 00:04:10,260
so here I will give you some

110
00:04:10,260 --> 00:04:13,379
instructions that motivate our design

111
00:04:13,379 --> 00:04:15,720
first we observe that applying small

112
00:04:15,720 --> 00:04:17,820
mods to clean image purely change the

113
00:04:17,820 --> 00:04:20,040
model prediction because we can still

114
00:04:20,040 --> 00:04:21,660
recognize the door even with the small

115
00:04:21,660 --> 00:04:24,540
mask on input image

116
00:04:24,540 --> 00:04:26,220
on the other hand

117
00:04:26,220 --> 00:04:27,900
when there is an adversary patch on

118
00:04:27,900 --> 00:04:28,860
image

119
00:04:28,860 --> 00:04:31,080
we find applying small Mass would change

120
00:04:31,080 --> 00:04:33,419
the prediction because when we mask out

121
00:04:33,419 --> 00:04:34,919
the patch we can get the correct

122
00:04:34,919 --> 00:04:36,419
prediction back

123
00:04:36,419 --> 00:04:39,840
then apply mask to adversary image will

124
00:04:39,840 --> 00:04:42,060
have a disagreement in all these

125
00:04:42,060 --> 00:04:43,860
different muscle predictions

126
00:04:43,860 --> 00:04:45,660
Harry there's a talk I will focus on the

127
00:04:45,660 --> 00:04:47,400
case that we consider one patch and in

128
00:04:47,400 --> 00:04:48,720
the paper we discuss how to handle

129
00:04:48,720 --> 00:04:50,280
multiple patches

130
00:04:50,280 --> 00:04:52,620
then going back to this slide given this

131
00:04:52,620 --> 00:04:55,500
disagreement the question is how can we

132
00:04:55,500 --> 00:04:56,759
settle it

133
00:04:56,759 --> 00:04:59,460
or how can we identify the correct

134
00:04:59,460 --> 00:05:01,380
prediction label

135
00:05:01,380 --> 00:05:03,600
in this example we might just we might

136
00:05:03,600 --> 00:05:05,340
want to say hey how about we output the

137
00:05:05,340 --> 00:05:07,259
disagree with the minority dog because

138
00:05:07,259 --> 00:05:09,780
we only find one mask that removes the

139
00:05:09,780 --> 00:05:11,100
patch

140
00:05:11,100 --> 00:05:12,840
but the problem is

141
00:05:12,840 --> 00:05:15,180
what if the attacker knows about the

142
00:05:15,180 --> 00:05:16,919
defense strategy and they introduced

143
00:05:16,919 --> 00:05:18,600
other protection labels

144
00:05:18,600 --> 00:05:21,419
just to be noted that all the remaining

145
00:05:21,419 --> 00:05:23,160
eight predictions are all under

146
00:05:23,160 --> 00:05:25,560
attackers control because their patch

147
00:05:25,560 --> 00:05:27,660
doesn't remove the path but their mask

148
00:05:27,660 --> 00:05:29,400
doesn't remove the patch

149
00:05:29,400 --> 00:05:31,020
so the attacker maybe they can change

150
00:05:31,020 --> 00:05:33,060
the cash you fox and maybe move this

151
00:05:33,060 --> 00:05:34,919
Force at different locations

152
00:05:34,919 --> 00:05:37,440
our question is how can we distinguish

153
00:05:37,440 --> 00:05:40,199
this this dog and fox

154
00:05:40,199 --> 00:05:43,259
or equivalently how can we identify the

155
00:05:43,259 --> 00:05:45,660
mask that removes the patch from other

156
00:05:45,660 --> 00:05:48,199
masks

157
00:05:48,479 --> 00:05:51,240
our solution is to add a second mask we

158
00:05:51,240 --> 00:05:53,160
will apply a second mask to different

159
00:05:53,160 --> 00:05:55,680
image location and then we analyze the

160
00:05:55,680 --> 00:05:57,539
model predictions on image with two

161
00:05:57,539 --> 00:06:00,240
masks we will show that this analysis

162
00:06:00,240 --> 00:06:02,940
allows us to determine if the first mask

163
00:06:02,940 --> 00:06:05,280
removes the patch or not

164
00:06:05,280 --> 00:06:07,860
here I will give some more intuitions

165
00:06:07,860 --> 00:06:09,660
say the first Mouse already removed the

166
00:06:09,660 --> 00:06:13,320
patch the our second mask is going to be

167
00:06:13,320 --> 00:06:15,360
applied to a clean image

168
00:06:15,360 --> 00:06:17,400
based on our previous discussion masking

169
00:06:17,400 --> 00:06:19,020
unclean image purely changed the model

170
00:06:19,020 --> 00:06:21,240
prediction so usually we will see a

171
00:06:21,240 --> 00:06:23,100
prediction agreement

172
00:06:23,100 --> 00:06:25,259
therefore in practice when we deployed

173
00:06:25,259 --> 00:06:28,380
our defense if we see an agreement in

174
00:06:28,380 --> 00:06:29,819
too much prediction

175
00:06:29,819 --> 00:06:31,680
we will know probably the first amount

176
00:06:31,680 --> 00:06:33,360
already removed the patch so we can

177
00:06:33,360 --> 00:06:36,060
cross this agreed predictions

178
00:06:36,060 --> 00:06:38,100
on the other hand if the first mod

179
00:06:38,100 --> 00:06:40,860
doesn't remove the patch that means the

180
00:06:40,860 --> 00:06:43,259
second mod is applied to an adversarial

181
00:06:43,259 --> 00:06:45,539
image based on our previous discussion

182
00:06:45,539 --> 00:06:48,180
we will see a prediction disagreement

183
00:06:48,180 --> 00:06:50,419
therefore in defense in our in practice

184
00:06:50,419 --> 00:06:52,919
a disagreement means that they are still

185
00:06:52,919 --> 00:06:54,479
out of a zero pixel left and we

186
00:06:54,479 --> 00:06:57,120
shouldn't trust this prediction at all

187
00:06:57,120 --> 00:06:59,819
so this difference allows us to develop

188
00:06:59,819 --> 00:07:02,160
our final four different algorithms we

189
00:07:02,160 --> 00:07:04,560
call Double masking that is with defense

190
00:07:04,560 --> 00:07:07,500
with two rounds of masking

191
00:07:07,500 --> 00:07:09,240
as the first step we will generate a

192
00:07:09,240 --> 00:07:12,120
mask set and we recall that at least one

193
00:07:12,120 --> 00:07:14,280
mod can remove the patch regardless of

194
00:07:14,280 --> 00:07:16,740
the patch location we discuss how to do

195
00:07:16,740 --> 00:07:18,000
that in the paper

196
00:07:18,000 --> 00:07:20,639
then we will apply every mask

197
00:07:20,639 --> 00:07:22,440
from The Mask set to the include image

198
00:07:22,440 --> 00:07:25,020
then we evaluate the model predictions

199
00:07:25,020 --> 00:07:26,580
on muscular image

200
00:07:26,580 --> 00:07:29,699
if we see an agreement we will know it

201
00:07:29,699 --> 00:07:31,500
is probably a clean image so we will

202
00:07:31,500 --> 00:07:33,479
trust this agreed prediction

203
00:07:33,479 --> 00:07:36,300
on the other hand a disagreement means

204
00:07:36,300 --> 00:07:38,039
that it's probably on another bizarre

205
00:07:38,039 --> 00:07:40,020
patch there we will trigger the second

206
00:07:40,020 --> 00:07:41,699
round masking to settle this

207
00:07:41,699 --> 00:07:43,020
disagreement

208
00:07:43,020 --> 00:07:45,419
and we note that the second mask is

209
00:07:45,419 --> 00:07:47,520
applied to every first round musket

210
00:07:47,520 --> 00:07:50,940
image now we will evaluate prediction

211
00:07:50,940 --> 00:07:53,639
um image with two masks and check the

212
00:07:53,639 --> 00:07:54,780
agreement again

213
00:07:54,780 --> 00:07:57,240
if there is an agreement we will know

214
00:07:57,240 --> 00:07:59,160
the first must already removed the patch

215
00:07:59,160 --> 00:08:01,139
so we can trust and output these agreed

216
00:08:01,139 --> 00:08:02,039
predictions

217
00:08:02,039 --> 00:08:03,960
on the other hand if there is a

218
00:08:03,960 --> 00:08:05,460
disagreement that means there are still

219
00:08:05,460 --> 00:08:08,220
adversary pixel left then we'll just

220
00:08:08,220 --> 00:08:10,139
discard these labels and then we will

221
00:08:10,139 --> 00:08:12,720
move on to the next musket image

222
00:08:12,720 --> 00:08:14,099
so in the paper we have additional

223
00:08:14,099 --> 00:08:16,680
details to ensure that the algorithm

224
00:08:16,680 --> 00:08:18,360
will always output label

225
00:08:18,360 --> 00:08:20,639
so that is a defense algorithm

226
00:08:20,639 --> 00:08:22,979
how can we reason about the robustness

227
00:08:22,979 --> 00:08:25,319
how can we certify the robustness

228
00:08:25,319 --> 00:08:27,840
in the paper we showed that two months

229
00:08:27,840 --> 00:08:29,879
correctness implies the certifiable

230
00:08:29,879 --> 00:08:31,259
robustness

231
00:08:31,259 --> 00:08:33,059
that is we consider all too much

232
00:08:33,059 --> 00:08:35,039
combinations and we apply to the image

233
00:08:35,039 --> 00:08:37,020
and if the model predictions are all of

234
00:08:37,020 --> 00:08:40,440
them are correct we have the robustness

235
00:08:40,440 --> 00:08:43,020
how can we prove that so I would really

236
00:08:43,020 --> 00:08:44,399
encourage people to check out the proof

237
00:08:44,399 --> 00:08:46,500
in the paper it's pretty fun and we

238
00:08:46,500 --> 00:08:48,839
don't need any complex math to prove the

239
00:08:48,839 --> 00:08:50,700
robustness we just do some pure simple

240
00:08:50,700 --> 00:08:53,459
logical reasoning and here I will give

241
00:08:53,459 --> 00:08:55,560
some intrusion about why patch cleanser

242
00:08:55,560 --> 00:08:58,440
would never return an incorrect label

243
00:08:58,440 --> 00:09:00,480
so for a proof we have two conditions

244
00:09:00,480 --> 00:09:02,640
the first one is we know at least one

245
00:09:02,640 --> 00:09:04,680
mass will remove the patch and the

246
00:09:04,680 --> 00:09:06,240
second one is the two Mass correctness

247
00:09:06,240 --> 00:09:07,500
which means

248
00:09:07,500 --> 00:09:09,660
the prediction of muscular image without

249
00:09:09,660 --> 00:09:13,080
any adversary pixels are correct

250
00:09:13,080 --> 00:09:15,779
then look at our algorithm

251
00:09:15,779 --> 00:09:17,640
so we can see that we will output

252
00:09:17,640 --> 00:09:19,620
prediction based on either one mod

253
00:09:19,620 --> 00:09:21,959
protection or too much protection

254
00:09:21,959 --> 00:09:23,820
for the Walmart prediction we can claim

255
00:09:23,820 --> 00:09:25,320
the idea is actually to one correct one

256
00:09:25,320 --> 00:09:27,060
Mass prediction because there is at

257
00:09:27,060 --> 00:09:29,100
least one first round mass that removes

258
00:09:29,100 --> 00:09:30,360
the patch

259
00:09:30,360 --> 00:09:32,399
and based on too much protection we know

260
00:09:32,399 --> 00:09:34,500
when there is no adversary pixels the

261
00:09:34,500 --> 00:09:36,060
musk prediction are correct so we can

262
00:09:36,060 --> 00:09:38,700
find one correct protection here

263
00:09:38,700 --> 00:09:40,860
this correct prediction will enforce

264
00:09:40,860 --> 00:09:42,540
this agreement with any different label

265
00:09:42,540 --> 00:09:44,100
if there is any introduced by the

266
00:09:44,100 --> 00:09:45,899
attacker and we know disagreement we

267
00:09:45,899 --> 00:09:48,120
will not never out for a predictions so

268
00:09:48,120 --> 00:09:51,420
we were not in return incorrect label

269
00:09:51,420 --> 00:09:53,580
and yeah and we can do stimula analysis

270
00:09:53,580 --> 00:09:55,440
for the too much prediction we will know

271
00:09:55,440 --> 00:09:57,420
there is at least one correct too much

272
00:09:57,420 --> 00:09:59,519
prediction because at least one second

273
00:09:59,519 --> 00:10:02,279
raw Mass can remove the patch this to

274
00:10:02,279 --> 00:10:04,200
enforce a disagreement and ensures that

275
00:10:04,200 --> 00:10:06,300
we not we will not return the label

276
00:10:06,300 --> 00:10:08,160
that's the high level idea and more

277
00:10:08,160 --> 00:10:10,920
details are in our payload

278
00:10:10,920 --> 00:10:12,360
then next we will talk about our

279
00:10:12,360 --> 00:10:14,279
evaluation we will focused on two

280
00:10:14,279 --> 00:10:16,440
metrics the first one is clean accuracy

281
00:10:16,440 --> 00:10:18,899
that is the fraction of the image that

282
00:10:18,899 --> 00:10:20,580
can be correctly classified when there

283
00:10:20,580 --> 00:10:22,620
is no attack the second one is the

284
00:10:22,620 --> 00:10:25,080
certified robust accuracy that is how

285
00:10:25,080 --> 00:10:27,240
many image we can certify the robustness

286
00:10:27,240 --> 00:10:29,279
for or just how many image we can have

287
00:10:29,279 --> 00:10:31,860
the two most correctness

288
00:10:31,860 --> 00:10:33,899
here we will report the performance for

289
00:10:33,899 --> 00:10:36,060
the image now data set and robustness is

290
00:10:36,060 --> 00:10:37,800
considered for a two percent pixel

291
00:10:37,800 --> 00:10:39,420
Square patch with any content and

292
00:10:39,420 --> 00:10:42,240
anywhere on image

293
00:10:42,240 --> 00:10:45,540
so there this is where the prior works

294
00:10:45,540 --> 00:10:48,660
are and this also marks the range of

295
00:10:48,660 --> 00:10:50,760
clean accuracy for the undefended model

296
00:10:50,760 --> 00:10:53,339
it's usually larger than 80 and this is

297
00:10:53,339 --> 00:10:55,079
our price cleanser

298
00:10:55,079 --> 00:10:57,180
so from the figure we can show that the

299
00:10:57,180 --> 00:10:58,920
patch cleanser has a very high clean

300
00:10:58,920 --> 00:11:01,320
accuracy it even Falls between the range

301
00:11:01,320 --> 00:11:03,779
of state-of-the-art undefended model

302
00:11:03,779 --> 00:11:06,360
in addition to that we have a strong

303
00:11:06,360 --> 00:11:09,420
certified robust accuracy and our robust

304
00:11:09,420 --> 00:11:11,640
accuracy in this evaluation setting is

305
00:11:11,640 --> 00:11:14,640
even higher than the prior Works uh the

306
00:11:14,640 --> 00:11:16,740
clean accuracy of the prior works

307
00:11:16,740 --> 00:11:19,019
so all this results demonstrates the

308
00:11:19,019 --> 00:11:22,260
strengths of our attachment design

309
00:11:22,260 --> 00:11:24,120
finally take away

310
00:11:24,120 --> 00:11:26,940
um yeah so in this talk I talked about

311
00:11:26,940 --> 00:11:28,620
we designed the patch cleanser defense

312
00:11:28,620 --> 00:11:31,260
which use a pixel masking strategy

313
00:11:31,260 --> 00:11:33,420
and we achieved certifiable robustness

314
00:11:33,420 --> 00:11:35,100
for recovering the correct prediction

315
00:11:35,100 --> 00:11:36,180
labels

316
00:11:36,180 --> 00:11:38,579
and we are the first to achieve such a

317
00:11:38,579 --> 00:11:40,260
high clean accuracy on the image that

318
00:11:40,260 --> 00:11:42,120
data set and of course as well as the

319
00:11:42,120 --> 00:11:44,940
state of the rx35 robust accuracy

320
00:11:44,940 --> 00:11:47,160
and addition to that I want to highlight

321
00:11:47,160 --> 00:11:48,720
the patch things that is compatible with

322
00:11:48,720 --> 00:11:51,180
any state of the image Cloud file while

323
00:11:51,180 --> 00:11:53,519
prior words they all rely on particular

324
00:11:53,519 --> 00:11:55,980
types of model architecture

325
00:11:55,980 --> 00:11:58,140
and we have released the artifact on the

326
00:11:58,140 --> 00:11:59,640
GitHub we are also maintaining a

327
00:11:59,640 --> 00:12:01,440
leaderboard for the robustness and a

328
00:12:01,440 --> 00:12:03,899
paper list for Relevant research just

329
00:12:03,899 --> 00:12:06,120
feel free to check it out and that's the

330
00:12:06,120 --> 00:12:10,279
talk and happy to take any question

