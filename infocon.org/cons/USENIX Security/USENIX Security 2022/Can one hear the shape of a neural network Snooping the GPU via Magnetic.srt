1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:08,000 --> 00:00:10,380
I just want to say first that my

3
00:00:10,380 --> 00:00:12,360
background is in computer graphics

4
00:00:12,360 --> 00:00:14,400
and so regularly if I'm you're working

5
00:00:14,400 --> 00:00:15,900
on Research that looks a little bit more

6
00:00:15,900 --> 00:00:18,480
like this but I've had so much fun being

7
00:00:18,480 --> 00:00:20,460
here everyone here has been super

8
00:00:20,460 --> 00:00:23,160
welcoming and I was half expecting to

9
00:00:23,160 --> 00:00:25,019
get hacked at some point during the

10
00:00:25,019 --> 00:00:26,880
conference but luckily I think I've made

11
00:00:26,880 --> 00:00:28,680
it through unscathed

12
00:00:28,680 --> 00:00:31,279
but

13
00:00:31,380 --> 00:00:33,180
why am I up here

14
00:00:33,180 --> 00:00:34,380
well

15
00:00:34,380 --> 00:00:37,739
in graphics we use a lot of gpus we love

16
00:00:37,739 --> 00:00:39,920
gpus love love love love

17
00:00:39,920 --> 00:00:42,840
and they're not just for mining we use

18
00:00:42,840 --> 00:00:44,399
them from everything from training

19
00:00:44,399 --> 00:00:46,320
robots in Virtual environments to

20
00:00:46,320 --> 00:00:48,719
ruining sand castles to running neural

21
00:00:48,719 --> 00:00:51,660
networks for model reduction and so on

22
00:00:51,660 --> 00:00:54,120
and so one day I go to install extra

23
00:00:54,120 --> 00:00:56,399
hard disk for more storage so I can save

24
00:00:56,399 --> 00:00:58,020
all this wonderful work

25
00:00:58,020 --> 00:01:00,000
and I hear some coil wine from the fans

26
00:01:00,000 --> 00:01:01,680
coming from my GPU

27
00:01:01,680 --> 00:01:03,600
and it got me thinking about what other

28
00:01:03,600 --> 00:01:07,080
signals might be coming off the GPU

29
00:01:07,080 --> 00:01:09,180
and even though I'm coming from Graphics

30
00:01:09,180 --> 00:01:11,159
I'd heard about these other great works

31
00:01:11,159 --> 00:01:13,380
like how it's possible to use audio to

32
00:01:13,380 --> 00:01:15,900
uncover cryptographic keys or more

33
00:01:15,900 --> 00:01:17,460
recently how when it comes to neural

34
00:01:17,460 --> 00:01:19,500
networks by probing Arduino you can

35
00:01:19,500 --> 00:01:20,700
uncover some of the weights from a

36
00:01:20,700 --> 00:01:22,020
handful of layers

37
00:01:22,020 --> 00:01:24,360
and so I Revisited these along with many

38
00:01:24,360 --> 00:01:26,460
other inspiring works and came to

39
00:01:26,460 --> 00:01:28,080
realized that previous methods have only

40
00:01:28,080 --> 00:01:29,820
ever looked at small networks or Edge

41
00:01:29,820 --> 00:01:31,080
devices

42
00:01:31,080 --> 00:01:33,000
so this likely probably wouldn't apply

43
00:01:33,000 --> 00:01:36,720
to gpus right they're too fast no

44
00:01:36,720 --> 00:01:39,000
and so I did what any good PhD student

45
00:01:39,000 --> 00:01:41,340
does and I bought the cheapest possible

46
00:01:41,340 --> 00:01:43,380
sensor I could on the internet

47
00:01:43,380 --> 00:01:45,360
literally there was no world in which I

48
00:01:45,360 --> 00:01:46,740
could convince my advisors that I was

49
00:01:46,740 --> 00:01:49,560
doing anything useful and so I just went

50
00:01:49,560 --> 00:01:52,920
for it and here it is a 47 kilohertz

51
00:01:52,920 --> 00:01:55,259
little flux magnetic sensor that set me

52
00:01:55,259 --> 00:01:58,020
back a full three dollars

53
00:01:58,020 --> 00:02:00,240
and I put it up to my computer and you

54
00:02:00,240 --> 00:02:01,979
can see it there along with a DAC

55
00:02:01,979 --> 00:02:03,540
converter that someone had lying around

56
00:02:03,540 --> 00:02:05,820
and I ran some GPU code to get things

57
00:02:05,820 --> 00:02:08,220
cooking I happen to be training a neural

58
00:02:08,220 --> 00:02:09,899
network classifier at the time and so I

59
00:02:09,899 --> 00:02:11,099
wanted to check if there was anything

60
00:02:11,099 --> 00:02:12,660
coming out of my machine or if it was

61
00:02:12,660 --> 00:02:14,220
all just a bunch of noise

62
00:02:14,220 --> 00:02:16,080
and what did I find

63
00:02:16,080 --> 00:02:19,140
patterns but not just any patterns

64
00:02:19,140 --> 00:02:22,680
clearly identifiable patterns and that

65
00:02:22,680 --> 00:02:25,319
should not have been very surprising why

66
00:02:25,319 --> 00:02:27,120
well Machine learning models running on

67
00:02:27,120 --> 00:02:29,220
a GPU are not just some magical random

68
00:02:29,220 --> 00:02:30,720
calculations happening in parallel

69
00:02:30,720 --> 00:02:32,940
neural networks describe a computational

70
00:02:32,940 --> 00:02:35,160
pipeline a framework of building blocks

71
00:02:35,160 --> 00:02:37,379
and operations composed as layers one

72
00:02:37,379 --> 00:02:39,180
after the other

73
00:02:39,180 --> 00:02:41,099
where each new layer has to wait for the

74
00:02:41,099 --> 00:02:42,840
output of the layer before it before it

75
00:02:42,840 --> 00:02:45,180
has the values ready and it can continue

76
00:02:45,180 --> 00:02:47,160
and the same logic applies for any sub

77
00:02:47,160 --> 00:02:49,019
components that make up Network models

78
00:02:49,019 --> 00:02:51,300
whether it's pooling operators adding

79
00:02:51,300 --> 00:02:53,160
multiplying non-linear activation

80
00:02:53,160 --> 00:02:55,440
functions what have you they all have

81
00:02:55,440 --> 00:02:57,180
this dependency on the neighboring

82
00:02:57,180 --> 00:02:58,800
layer's outcome

83
00:02:58,800 --> 00:03:01,500
which means that even the case of larger

84
00:03:01,500 --> 00:03:04,260
fancier networks and even in the case of

85
00:03:04,260 --> 00:03:07,260
parallel gpus a processor still has to

86
00:03:07,260 --> 00:03:08,580
synchronize and wait for the output of

87
00:03:08,580 --> 00:03:10,739
each layer which sorts any complicated

88
00:03:10,739 --> 00:03:13,140
Network into a topological ordering that

89
00:03:13,140 --> 00:03:15,360
we'll take advantage of later on

90
00:03:15,360 --> 00:03:17,340
and by studying these en traces

91
00:03:17,340 --> 00:03:18,720
literally with these synchronization

92
00:03:18,720 --> 00:03:21,120
points it comes as little surprise to

93
00:03:21,120 --> 00:03:22,200
anyone in the side Channel community

94
00:03:22,200 --> 00:03:24,060
that each specific layer in fact

95
00:03:24,060 --> 00:03:26,519
produces an easily identifiable magnetic

96
00:03:26,519 --> 00:03:28,680
signature

97
00:03:28,680 --> 00:03:30,720
so why does this matter so you can see

98
00:03:30,720 --> 00:03:33,480
my network so what well networks are

99
00:03:33,480 --> 00:03:36,200
taking over everything

100
00:03:36,239 --> 00:03:38,220
from Vision applications to controlling

101
00:03:38,220 --> 00:03:40,200
robots they're finding their ways into

102
00:03:40,200 --> 00:03:42,120
security and authentication Frameworks

103
00:03:42,120 --> 00:03:44,519
as well they're everywhere

104
00:03:44,519 --> 00:03:46,560
there's numerous motives why someone

105
00:03:46,560 --> 00:03:47,819
might want to uncover a neural

106
00:03:47,819 --> 00:03:49,860
architecture an attacker May wanted to

107
00:03:49,860 --> 00:03:51,360
steal some intellectual property for

108
00:03:51,360 --> 00:03:53,519
profit or they may want to circumvent

109
00:03:53,519 --> 00:03:55,560
some payment where a company May invest

110
00:03:55,560 --> 00:03:58,220
some resources to develop a robust model

111
00:03:58,220 --> 00:04:01,260
and hope to recover those resources and

112
00:04:01,260 --> 00:04:03,120
by profiting when charging clients for

113
00:04:03,120 --> 00:04:05,280
its use but by stealing the model an

114
00:04:05,280 --> 00:04:06,840
attacker avoids payment for these

115
00:04:06,840 --> 00:04:07,920
services

116
00:04:07,920 --> 00:04:09,659
or perhaps they're looking to violate

117
00:04:09,659 --> 00:04:11,519
some centuries and this comes up in the

118
00:04:11,519 --> 00:04:12,900
cases where a machine learning approach

119
00:04:12,900 --> 00:04:15,420
is taken to identify viruses or spam and

120
00:04:15,420 --> 00:04:16,918
attacker increases their chances of

121
00:04:16,918 --> 00:04:18,720
deceiving or bypassing the model by

122
00:04:18,720 --> 00:04:20,339
uncovering its underlying neural network

123
00:04:20,339 --> 00:04:22,380
architecture

124
00:04:22,380 --> 00:04:24,180
another thing to keep in mind is that

125
00:04:24,180 --> 00:04:25,860
the graphics processing unit is a

126
00:04:25,860 --> 00:04:27,600
favored vehicle for executing a neural

127
00:04:27,600 --> 00:04:28,259
network

128
00:04:28,259 --> 00:04:30,300
GPU chips are even being redesigned

129
00:04:30,300 --> 00:04:31,919
specifically with machine learning in

130
00:04:31,919 --> 00:04:32,759
mind

131
00:04:32,759 --> 00:04:34,620
and as networks continue to grow and get

132
00:04:34,620 --> 00:04:36,960
deeper and gpus continue to get cheaper

133
00:04:36,960 --> 00:04:39,000
and the popularity of running Nets on

134
00:04:39,000 --> 00:04:41,400
these accelera Hardware it makes sense

135
00:04:41,400 --> 00:04:43,320
to explore these two together since any

136
00:04:43,320 --> 00:04:44,639
sort of vulnerability concerning these

137
00:04:44,639 --> 00:04:47,220
two is hot on the horizon

138
00:04:47,220 --> 00:04:49,139
so in this case we've arrived at a very

139
00:04:49,139 --> 00:04:50,639
capable threat model for a side Channel

140
00:04:50,639 --> 00:04:52,979
attack our proposed attacker has no

141
00:04:52,979 --> 00:04:54,900
prior knowledge of the target neural

142
00:04:54,900 --> 00:04:56,220
network the model is taken to be

143
00:04:56,220 --> 00:04:57,660
developed trained and validated

144
00:04:57,660 --> 00:04:59,759
elsewhere and since it's a side Channel

145
00:04:59,759 --> 00:05:01,620
snooping the network leaves no digital

146
00:05:01,620 --> 00:05:04,259
trace the only accessible result will be

147
00:05:04,259 --> 00:05:06,060
a single inference we don't even look at

148
00:05:06,060 --> 00:05:07,740
training just the runtime use of the

149
00:05:07,740 --> 00:05:09,360
model and so we'll assume that the

150
00:05:09,360 --> 00:05:11,340
model's code memory and design cannot be

151
00:05:11,340 --> 00:05:13,800
accessed without tampering the device or

152
00:05:13,800 --> 00:05:16,080
otherwise alerting the provider and the

153
00:05:16,080 --> 00:05:18,540
attack is both non-invasive and passive

154
00:05:18,540 --> 00:05:20,759
and in this case we use a cheap sensor

155
00:05:20,759 --> 00:05:22,620
with no expertise required to operate

156
00:05:22,620 --> 00:05:25,020
and it's very easy to acquire and we

157
00:05:25,020 --> 00:05:26,759
treat the network as a black box while

158
00:05:26,759 --> 00:05:28,440
providing an input to it as someone

159
00:05:28,440 --> 00:05:30,240
might had they downloaded a network from

160
00:05:30,240 --> 00:05:32,400
some binary online

161
00:05:32,400 --> 00:05:34,080
so what can we hope to gain from only

162
00:05:34,080 --> 00:05:35,639
the side Channel information leaked from

163
00:05:35,639 --> 00:05:38,639
this targeted GPU in only a single scan

164
00:05:38,639 --> 00:05:40,800
well the target is to extract everything

165
00:05:40,800 --> 00:05:41,820
we need to know about the network

166
00:05:41,820 --> 00:05:43,800
architecture in order to recreate it and

167
00:05:43,800 --> 00:05:45,660
train our own version meaning that we'll

168
00:05:45,660 --> 00:05:48,419
need the layer and the order the

169
00:05:48,419 --> 00:05:49,740
function types

170
00:05:49,740 --> 00:05:51,780
along with the function with

171
00:05:51,780 --> 00:05:54,300
and anything that you need to define the

172
00:05:54,300 --> 00:05:56,280
individual layers parameters

173
00:05:56,280 --> 00:05:58,620
so that way we can recover any arbitrary

174
00:05:58,620 --> 00:06:01,320
network with any arbitrary layers and

175
00:06:01,320 --> 00:06:03,060
depth of layers

176
00:06:03,060 --> 00:06:04,800
and our scope will Target

177
00:06:04,800 --> 00:06:07,199
fully connected convolutional recurrent

178
00:06:07,199 --> 00:06:08,940
layers along with activation

179
00:06:08,940 --> 00:06:10,860
normalization pooling layers anything

180
00:06:10,860 --> 00:06:13,139
associated with ML architectures because

181
00:06:13,139 --> 00:06:15,419
together these individual blocks span

182
00:06:15,419 --> 00:06:17,160
the components used by a variety of

183
00:06:17,160 --> 00:06:19,500
machine learning applications

184
00:06:19,500 --> 00:06:21,419
and so now that we have the incentive to

185
00:06:21,419 --> 00:06:23,280
extract the model an understanding of

186
00:06:23,280 --> 00:06:24,780
what it is that we're after

187
00:06:24,780 --> 00:06:26,580
we present our approach

188
00:06:26,580 --> 00:06:29,280
it contains four steps

189
00:06:29,280 --> 00:06:32,160
first offline we generate a signal

190
00:06:32,160 --> 00:06:33,479
database by running lots of network

191
00:06:33,479 --> 00:06:35,580
designs and testing out models of

192
00:06:35,580 --> 00:06:38,039
various sizes and shapes we also include

193
00:06:38,039 --> 00:06:39,060
some procedurally generated

194
00:06:39,060 --> 00:06:40,680
architectures where we stack together

195
00:06:40,680 --> 00:06:42,780
layers of random types sizes and

196
00:06:42,780 --> 00:06:45,120
parameters and combinatory throw them in

197
00:06:45,120 --> 00:06:47,520
ways that might not be common practice

198
00:06:47,520 --> 00:06:49,080
this helps us sample the possible

199
00:06:49,080 --> 00:06:51,660
interplays of em flavors that can incur

200
00:06:51,660 --> 00:06:54,120
between two adjacent flavored layers in

201
00:06:54,120 --> 00:06:57,120
order to help us spice up our data set

202
00:06:57,120 --> 00:06:58,860
then whenever we want to extract a

203
00:06:58,860 --> 00:07:00,900
network we acquire a trace from the

204
00:07:00,900 --> 00:07:03,600
target GPU or its power cable and then

205
00:07:03,600 --> 00:07:05,940
we run a window over the signal which

206
00:07:05,940 --> 00:07:07,860
serves as input to a trained recurrent

207
00:07:07,860 --> 00:07:10,919
Network namely a biostm model that

208
00:07:10,919 --> 00:07:12,900
converts a single scan into categorical

209
00:07:12,900 --> 00:07:13,979
labels

210
00:07:13,979 --> 00:07:15,960
this classification converts the signal

211
00:07:15,960 --> 00:07:18,360
layers signals into layers directly in

212
00:07:18,360 --> 00:07:20,580
place and thanks to that computational

213
00:07:20,580 --> 00:07:22,020
unraveling that we talked about earlier

214
00:07:22,020 --> 00:07:23,639
where the GPU has to compute the

215
00:07:23,639 --> 00:07:25,740
inference step by step we also get the

216
00:07:25,740 --> 00:07:27,539
topological sequence as well of the

217
00:07:27,539 --> 00:07:29,880
network as a result

218
00:07:29,880 --> 00:07:32,039
but at this point we only have the types

219
00:07:32,039 --> 00:07:34,440
of layers and the order that they're in

220
00:07:34,440 --> 00:07:36,180
now we need to estimate the layer

221
00:07:36,180 --> 00:07:37,800
specific parameter information that

222
00:07:37,800 --> 00:07:40,020
helps to find each layer and we do this

223
00:07:40,020 --> 00:07:42,780
by leveraging our data set yet again and

224
00:07:42,780 --> 00:07:44,460
looking for hints in the context of

225
00:07:44,460 --> 00:07:45,720
neighboring layers to the ones that

226
00:07:45,720 --> 00:07:47,039
we've identified

227
00:07:47,039 --> 00:07:49,620
so by creating a feature of layer types

228
00:07:49,620 --> 00:07:50,819
along with voltage and timing

229
00:07:50,819 --> 00:07:52,560
information along with adjacent

230
00:07:52,560 --> 00:07:54,060
neighbors we could approximate the

231
00:07:54,060 --> 00:07:55,800
numerous parameters associated with this

232
00:07:55,800 --> 00:07:57,840
specific layer whether it's the kernel

233
00:07:57,840 --> 00:07:59,639
size of a convolution the output Channel

234
00:07:59,639 --> 00:08:01,440
dimensions of an image or how many

235
00:08:01,440 --> 00:08:05,039
neurons in a fully connected layer Etc

236
00:08:05,039 --> 00:08:07,440
now because we can sample networks

237
00:08:07,440 --> 00:08:09,660
arbitrarily offline the accuracy on

238
00:08:09,660 --> 00:08:11,340
these parameters is pretty good

239
00:08:11,340 --> 00:08:13,139
but if we're off we end up coming up

240
00:08:13,139 --> 00:08:15,060
with an invalid layer where the output

241
00:08:15,060 --> 00:08:16,860
to one and the input to another don't

242
00:08:16,860 --> 00:08:19,199
line up meaning that these estimates

243
00:08:19,199 --> 00:08:21,060
cannot guarantee consistency between the

244
00:08:21,060 --> 00:08:23,400
layers even one wrong can lead to an

245
00:08:23,400 --> 00:08:25,259
invalid Network design which is why

246
00:08:25,259 --> 00:08:27,720
prior methods have decided either not to

247
00:08:27,720 --> 00:08:30,300
fully recover or to rely on an expert in

248
00:08:30,300 --> 00:08:32,279
the loop to fully recover the network

249
00:08:32,279 --> 00:08:34,740
layers or sometimes assign them from a

250
00:08:34,740 --> 00:08:36,958
predetermined set of values

251
00:08:36,958 --> 00:08:39,120
but getting this close and stopping just

252
00:08:39,120 --> 00:08:41,399
here is akin to playing a plumbing fall

253
00:08:41,399 --> 00:08:43,440
game where you start playing and then

254
00:08:43,440 --> 00:08:44,940
you call it a day after just a few

255
00:08:44,940 --> 00:08:45,959
clicks

256
00:08:45,959 --> 00:08:48,600
which is why we additionally verify and

257
00:08:48,600 --> 00:08:50,220
clean up these proposed hyper parameter

258
00:08:50,220 --> 00:08:52,500
specifications by setting up an integer

259
00:08:52,500 --> 00:08:54,240
programming problem and performing a

260
00:08:54,240 --> 00:08:55,860
joint optimization

261
00:08:55,860 --> 00:08:59,040
using IBM C like Sauber we do this but

262
00:08:59,040 --> 00:09:00,839
in the interest of time I'll defer to

263
00:09:00,839 --> 00:09:03,000
the paper for full details on how we can

264
00:09:03,000 --> 00:09:05,220
do this for all the different parameters

265
00:09:05,220 --> 00:09:07,320
but it's worth noting that by estimating

266
00:09:07,320 --> 00:09:09,300
valid architectures for any shape and

267
00:09:09,300 --> 00:09:10,920
size layer and by using this

268
00:09:10,920 --> 00:09:12,959
optimization we make our approach more

269
00:09:12,959 --> 00:09:14,519
resilient to the network designs of

270
00:09:14,519 --> 00:09:15,959
tomorrow

271
00:09:15,959 --> 00:09:18,120
and with that the pipeline is complete

272
00:09:18,120 --> 00:09:19,860
and we have a network model filled in

273
00:09:19,860 --> 00:09:22,320
that we can train and run so let's see

274
00:09:22,320 --> 00:09:24,180
some results

275
00:09:24,180 --> 00:09:25,920
we applied this method on a data set

276
00:09:25,920 --> 00:09:27,600
generated from the single gpus traces

277
00:09:27,600 --> 00:09:30,660
starting with a GTX 1080. we also

278
00:09:30,660 --> 00:09:32,700
acquire a twin identical make of a model

279
00:09:32,700 --> 00:09:35,339
of a GTX 1080 and we use the signals

280
00:09:35,339 --> 00:09:36,839
gathered from one to extract models

281
00:09:36,839 --> 00:09:40,019
running on the other GPU and we find

282
00:09:40,019 --> 00:09:42,120
that our attack is still successful even

283
00:09:42,120 --> 00:09:43,920
on different new instances of the same

284
00:09:43,920 --> 00:09:45,660
Hardware

285
00:09:45,660 --> 00:09:47,880
we also test the gpu's transferability

286
00:09:47,880 --> 00:09:50,100
to quantify the importance of training

287
00:09:50,100 --> 00:09:51,899
on the right Hardware by collecting

288
00:09:51,899 --> 00:09:53,580
signals from a bunch of gpus and

289
00:09:53,580 --> 00:09:55,380
combining them in training in this case

290
00:09:55,380 --> 00:09:57,480
we hold out the data from the target GPU

291
00:09:57,480 --> 00:09:59,880
and we see how we would have fared

292
00:09:59,880 --> 00:10:02,160
we find that never having seen a

293
00:10:02,160 --> 00:10:05,279
specific card we can still pose a threat

294
00:10:05,279 --> 00:10:07,740
by training on other cards uh similar to

295
00:10:07,740 --> 00:10:08,820
it

296
00:10:08,820 --> 00:10:11,040
but as expected having access to the

297
00:10:11,040 --> 00:10:13,080
traces from the target GPU for our

298
00:10:13,080 --> 00:10:16,399
training produces even better results

299
00:10:16,440 --> 00:10:18,000
we also look at our ability to perform

300
00:10:18,000 --> 00:10:20,399
transfer attacks where we can look at

301
00:10:20,399 --> 00:10:21,899
how models perform when they attack

302
00:10:21,899 --> 00:10:23,820
themselves compared to the scores of

303
00:10:23,820 --> 00:10:25,440
attacking other models

304
00:10:25,440 --> 00:10:27,600
and then we can see what happens when we

305
00:10:27,600 --> 00:10:30,300
try to extract models as a proxy

306
00:10:30,300 --> 00:10:32,880
we find that our individual extracted

307
00:10:32,880 --> 00:10:34,620
models are akin to having the original

308
00:10:34,620 --> 00:10:35,519
model

309
00:10:35,519 --> 00:10:38,660
because of their fidelity

310
00:10:40,920 --> 00:10:43,680
so in summary gpus can get faster and

311
00:10:43,680 --> 00:10:45,300
networks can get deeper but that doesn't

312
00:10:45,300 --> 00:10:46,800
mean they're not vulnerable

313
00:10:46,800 --> 00:10:49,140
a cheap three dollar off the shelf 47

314
00:10:49,140 --> 00:10:51,480
kilohertz sensor is all you need

315
00:10:51,480 --> 00:10:54,000
and this works across GPU processors of

316
00:10:54,000 --> 00:10:56,700
all different makes and brands

317
00:10:56,700 --> 00:10:58,560
and across also the popular Network

318
00:10:58,560 --> 00:10:59,880
libraries and Frameworks that people

319
00:10:59,880 --> 00:11:02,339
like to use for training

320
00:11:02,339 --> 00:11:04,079
in the end we recovered networks with

321
00:11:04,079 --> 00:11:07,339
hundreds of layers accurately

322
00:11:07,680 --> 00:11:09,480
so I just want to give a big thanks to

323
00:11:09,480 --> 00:11:11,519
my co-authors and my advisors as well as

324
00:11:11,519 --> 00:11:13,680
all the funding sources and sponsors and

325
00:11:13,680 --> 00:11:15,060
the libraries with which this wouldn't

326
00:11:15,060 --> 00:11:16,920
have been possible and if you want to

327
00:11:16,920 --> 00:11:18,300
reach me my contact information is all

328
00:11:18,300 --> 00:11:20,459
on my website and with that thank you

329
00:11:20,459 --> 00:11:21,899
guys for your attention and I'll open it

330
00:11:21,899 --> 00:11:25,279
up to any questions thank you

