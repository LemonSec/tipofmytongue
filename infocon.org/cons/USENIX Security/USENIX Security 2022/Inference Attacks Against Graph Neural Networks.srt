1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:09,260 --> 00:00:12,179
this is Ming and today I'm going to

3
00:00:12,179 --> 00:00:15,719
introduce you our work inference attacks

4
00:00:15,719 --> 00:00:18,539
against graph neural networks this is a

5
00:00:18,539 --> 00:00:22,920
joint work from cispa and Norton first I

6
00:00:22,920 --> 00:00:25,199
will give you some examples of graph

7
00:00:25,199 --> 00:00:28,560
data many Real World Systems many river

8
00:00:28,560 --> 00:00:32,340
systems consider a consist of notes and

9
00:00:32,340 --> 00:00:35,340
edges and thus can be represented as

10
00:00:35,340 --> 00:00:37,739
graphs for example The Social Network

11
00:00:37,739 --> 00:00:41,940
Financial Network and chemical Network

12
00:00:41,940 --> 00:00:44,820
two of the most important tasks of

13
00:00:44,820 --> 00:00:47,940
analyzing graph data node classification

14
00:00:47,940 --> 00:00:50,760
and graph classification

15
00:00:50,760 --> 00:00:53,520
the objective of node classification is

16
00:00:53,520 --> 00:00:55,980
to determine the label of each node in

17
00:00:55,980 --> 00:00:59,039
the graph and the objective of the graph

18
00:00:59,039 --> 00:01:01,320
classification is to determine the label

19
00:01:01,320 --> 00:01:04,578
of the whole graph

20
00:01:04,920 --> 00:01:08,040
and to the most straightforward method

21
00:01:08,040 --> 00:01:10,380
for the node classification is just

22
00:01:10,380 --> 00:01:13,200
ignore the edge and directly fitted node

23
00:01:13,200 --> 00:01:16,500
features into an MLP model and this

24
00:01:16,500 --> 00:01:18,479
model can predict the label for the

25
00:01:18,479 --> 00:01:21,540
nodes however this method ignores Edge

26
00:01:21,540 --> 00:01:23,820
information can only explore the

27
00:01:23,820 --> 00:01:26,159
information cannot explore the

28
00:01:26,159 --> 00:01:27,720
information from the neighboring nodes

29
00:01:27,720 --> 00:01:30,900
which has shown is important for

30
00:01:30,900 --> 00:01:33,420
non-classification

31
00:01:33,420 --> 00:01:36,119
so here comes the graph neural networks

32
00:01:36,119 --> 00:01:38,759
which is also called genes the general

33
00:01:38,759 --> 00:01:41,460
idea of GN is to aggregate the

34
00:01:41,460 --> 00:01:43,259
information from the neighboring nodes

35
00:01:43,259 --> 00:01:45,900
and generate node embeddings for each

36
00:01:45,900 --> 00:01:49,200
node the node embeddings can soon be

37
00:01:49,200 --> 00:01:53,399
used to conduct some Downstream tasks

38
00:01:53,399 --> 00:01:56,880
for example the node classification or

39
00:01:56,880 --> 00:01:59,100
link prediction

40
00:01:59,100 --> 00:02:03,240
the most important component of gene is

41
00:02:03,240 --> 00:02:04,860
the message passing

42
00:02:04,860 --> 00:02:07,680
so there are multiple algorithms for

43
00:02:07,680 --> 00:02:09,598
aggregating the information from

44
00:02:09,598 --> 00:02:12,260
neighboring nodes for example the graph

45
00:02:12,260 --> 00:02:14,940
conversion neural network the graph

46
00:02:14,940 --> 00:02:17,760
isomorphism neural network and the graph

47
00:02:17,760 --> 00:02:20,480
attention Network

48
00:02:20,819 --> 00:02:21,900
and

49
00:02:21,900 --> 00:02:25,200
the node embeddings we got can further

50
00:02:25,200 --> 00:02:27,780
be summarized as graph embedding for

51
00:02:27,780 --> 00:02:30,480
some graph classification or graph

52
00:02:30,480 --> 00:02:33,480
management tasks and this process is

53
00:02:33,480 --> 00:02:36,959
called graph pooling and there are many

54
00:02:36,959 --> 00:02:39,599
are two categories of black graphical

55
00:02:39,599 --> 00:02:42,000
methods the global pulling and

56
00:02:42,000 --> 00:02:44,640
hierarchical pulling so the global

57
00:02:44,640 --> 00:02:46,800
pulling is quite easy just directly

58
00:02:46,800 --> 00:02:49,019
summarize all node embeddings to a

59
00:02:49,019 --> 00:02:51,300
single graph embedding Vector such as

60
00:02:51,300 --> 00:02:55,260
take the average or take the maximum

61
00:02:55,260 --> 00:02:58,140
and here is a pipeline of generating the

62
00:02:58,140 --> 00:03:00,360
graph embedding in a hierarchical way

63
00:03:00,360 --> 00:03:03,900
first we use message passing modules to

64
00:03:03,900 --> 00:03:06,480
obtain the node embeddings and then we

65
00:03:06,480 --> 00:03:08,940
find multiple clusters according to the

66
00:03:08,940 --> 00:03:12,180
node embeddings we formed and next we

67
00:03:12,180 --> 00:03:15,360
treat each cluster as a node and node

68
00:03:15,360 --> 00:03:18,000
feature here becomes to the graph

69
00:03:18,000 --> 00:03:21,840
embedding of that cluster when then we

70
00:03:21,840 --> 00:03:24,300
iteratively apply the message passing

71
00:03:24,300 --> 00:03:28,080
and clustering operations until there is

72
00:03:28,080 --> 00:03:31,640
only one graph embedding

73
00:03:31,800 --> 00:03:34,980
so in this paper we focus on the whole

74
00:03:34,980 --> 00:03:36,959
graph and matting which is usually

75
00:03:36,959 --> 00:03:39,900
computed on sensitive graph

76
00:03:39,900 --> 00:03:42,780
and such graph embedding is empirically

77
00:03:42,780 --> 00:03:45,000
considered sentinelized since the whole

78
00:03:45,000 --> 00:03:47,879
graph is comprised into a single Vector

79
00:03:47,879 --> 00:03:51,540
in practice it has been shared with the

80
00:03:51,540 --> 00:03:54,000
third parties to conduct some Downstream

81
00:03:54,000 --> 00:03:55,920
tasks for example the graph

82
00:03:55,920 --> 00:03:58,080
classification graph matching or the

83
00:03:58,080 --> 00:04:01,200
graph realization our threat model here

84
00:04:01,200 --> 00:04:04,440
is that when an adversary can assigns to

85
00:04:04,440 --> 00:04:07,739
a graph embedding the information of the

86
00:04:07,739 --> 00:04:11,099
original graph that used to generate a

87
00:04:11,099 --> 00:04:13,640
graph embedding can be linked

88
00:04:13,640 --> 00:04:16,560
specifically we conduct three influence

89
00:04:16,560 --> 00:04:18,660
attacks the property influence attack

90
00:04:18,660 --> 00:04:22,500
the subgraph influencer tag and a graph

91
00:04:22,500 --> 00:04:24,900
reconstruction attack and we introduce

92
00:04:24,900 --> 00:04:27,739
them one by one

93
00:04:28,139 --> 00:04:30,600
so the first one graph property

94
00:04:30,600 --> 00:04:33,540
inference attack and attack objective is

95
00:04:33,540 --> 00:04:36,120
that given the target graph embedding

96
00:04:36,120 --> 00:04:38,400
infer the basic properties of the text

97
00:04:38,400 --> 00:04:41,100
graph such as the graph density number

98
00:04:41,100 --> 00:04:44,160
of nodes and number of ads to achieve

99
00:04:44,160 --> 00:04:46,979
this goal we use a multi-output

100
00:04:46,979 --> 00:04:50,100
classifier as the attack model the tech

101
00:04:50,100 --> 00:04:52,400
model consists of a feature extractor

102
00:04:52,400 --> 00:04:55,919
and multiple output layers each output

103
00:04:55,919 --> 00:04:59,220
layer predicts one graph property

104
00:04:59,220 --> 00:05:02,160
so to train this attack model we can use

105
00:05:02,160 --> 00:05:05,460
a set of auxiliary graph that comes from

106
00:05:05,460 --> 00:05:07,080
either the seam or different

107
00:05:07,080 --> 00:05:10,380
distributions as the target graph

108
00:05:10,380 --> 00:05:12,720
and here is the attack performance we

109
00:05:12,720 --> 00:05:16,440
evalued on five data sites and three

110
00:05:16,440 --> 00:05:19,620
properties so we'll first bucketize each

111
00:05:19,620 --> 00:05:22,199
property into four different bins we

112
00:05:22,199 --> 00:05:24,539
consider two baselines in our experiment

113
00:05:24,539 --> 00:05:26,820
the first one is quite easy just a

114
00:05:26,820 --> 00:05:29,220
random guessing the second one is

115
00:05:29,220 --> 00:05:31,800
directly summarize the properties from

116
00:05:31,800 --> 00:05:33,479
the auxiliary data set

117
00:05:33,479 --> 00:05:35,960
we also consider three pulling my third

118
00:05:35,960 --> 00:05:38,759
Ming pool is a global pulling method

119
00:05:38,759 --> 00:05:41,400
deep pool and minkapu are the

120
00:05:41,400 --> 00:05:44,400
hierarchical ones by comparing the first

121
00:05:44,400 --> 00:05:48,000
three and last two bars we show that our

122
00:05:48,000 --> 00:05:49,860
proposed attack consistently

123
00:05:49,860 --> 00:05:52,860
outperformed Baseline

124
00:05:52,860 --> 00:05:55,800
yeah here's the first conclusion and the

125
00:05:55,800 --> 00:05:58,979
second we observe that larger

126
00:05:58,979 --> 00:06:01,800
bacterization can lead to worse

127
00:06:01,800 --> 00:06:04,500
performance this is expected because

128
00:06:04,500 --> 00:06:07,380
larger bacterization requires higher

129
00:06:07,380 --> 00:06:09,180
granularity of the graph structure

130
00:06:09,180 --> 00:06:11,940
information and is more difficult for

131
00:06:11,940 --> 00:06:14,580
the classifier to distinguish in

132
00:06:14,580 --> 00:06:16,560
addition

133
00:06:16,560 --> 00:06:18,960
we found that attack performance on

134
00:06:18,960 --> 00:06:21,960
minpool is worse than the other two and

135
00:06:21,960 --> 00:06:25,380
this is because that means directly

136
00:06:25,380 --> 00:06:27,780
average all the nodes from the embedding

137
00:06:27,780 --> 00:06:30,479
into a whole graph embedding which might

138
00:06:30,479 --> 00:06:34,159
do some graph structure information

139
00:06:34,680 --> 00:06:37,199
okay so here is the second attack

140
00:06:37,199 --> 00:06:39,479
subgraph influencer tag

141
00:06:39,479 --> 00:06:42,120
the objective here is that given the

142
00:06:42,120 --> 00:06:44,639
target graph embedding and a subgraph of

143
00:06:44,639 --> 00:06:47,400
Interest the attacker aims to infer

144
00:06:47,400 --> 00:06:49,680
whether the subgraph of interest is

145
00:06:49,680 --> 00:06:52,319
contained in the tiger graph since the

146
00:06:52,319 --> 00:06:54,660
tiger graph embedding and the subgraph

147
00:06:54,660 --> 00:06:57,000
of interest is in are in different

148
00:06:57,000 --> 00:07:00,259
formats we cannot directly compare them

149
00:07:00,259 --> 00:07:04,020
to address this issue we first use an

150
00:07:04,020 --> 00:07:06,479
embedding instructor to transform the

151
00:07:06,479 --> 00:07:09,360
subgraph of Interest into subgraph

152
00:07:09,360 --> 00:07:13,080
embedding then we merge the target graph

153
00:07:13,080 --> 00:07:15,360
embedding and the sub graph embedding

154
00:07:15,360 --> 00:07:19,560
and feed them into a MLP model the MLP

155
00:07:19,560 --> 00:07:21,599
model can tell whether the subgraph of

156
00:07:21,599 --> 00:07:24,180
interest is contained in the tiger graph

157
00:07:24,180 --> 00:07:27,539
or not the whole attack model consists

158
00:07:27,539 --> 00:07:30,840
of the embedding structure and the MLP

159
00:07:30,840 --> 00:07:33,960
model and the action together on a local

160
00:07:33,960 --> 00:07:37,280
auxiliary site

161
00:07:38,639 --> 00:07:41,400
to train the attack model we need both

162
00:07:41,400 --> 00:07:44,580
positive pair and negative power the

163
00:07:44,580 --> 00:07:46,800
positive the positive pair means the sub

164
00:07:46,800 --> 00:07:49,020
graph is in the Target Model and

165
00:07:49,020 --> 00:07:50,880
negative higher means the subgraph is

166
00:07:50,880 --> 00:07:54,060
not contained integral Target graph

167
00:07:54,060 --> 00:07:56,699
and to generate the positive pair we use

168
00:07:56,699 --> 00:07:59,039
graph sampling method such as random

169
00:07:59,039 --> 00:08:01,259
work to generate a subgraph from the

170
00:08:01,259 --> 00:08:02,520
tagraph

171
00:08:02,520 --> 00:08:05,220
to generate the negative pair we

172
00:08:05,220 --> 00:08:07,680
randomly sample graph from the auxiliary

173
00:08:07,680 --> 00:08:09,960
data set that is different from the

174
00:08:09,960 --> 00:08:13,020
target graph and we sample a subgraph

175
00:08:13,020 --> 00:08:16,199
from this graph the target graph and the

176
00:08:16,199 --> 00:08:18,539
sample subgraph together to be the

177
00:08:18,539 --> 00:08:21,199
negative pair

178
00:08:21,199 --> 00:08:25,259
and here is the result of the subgraphy

179
00:08:25,259 --> 00:08:27,539
inference attack we consider four

180
00:08:27,539 --> 00:08:29,699
different sample ratios sampling ratios

181
00:08:29,699 --> 00:08:32,219
which indicates the proportion of the

182
00:08:32,219 --> 00:08:36,200
subgraph in the type in the tiger graph

183
00:08:36,299 --> 00:08:39,659
here yeah to conduct the subgraph

184
00:08:39,659 --> 00:08:41,940
influence attack the most read forward

185
00:08:41,940 --> 00:08:44,099
method is to generate the subgraph

186
00:08:44,099 --> 00:08:46,500
embedding directly using the Target

187
00:08:46,500 --> 00:08:50,100
Model and the only Trend MLP At as the

188
00:08:50,100 --> 00:08:52,560
target model which is this method as the

189
00:08:52,560 --> 00:08:55,620
Baseline by comparing the dark bars and

190
00:08:55,620 --> 00:08:57,839
corresponding shallow bars we observe

191
00:08:57,839 --> 00:09:00,420
that our proposed attack consistently

192
00:09:00,420 --> 00:09:03,959
outperformed the Baseline second larger

193
00:09:03,959 --> 00:09:05,700
sampling ratio

194
00:09:05,700 --> 00:09:08,880
leads to better performance and this is

195
00:09:08,880 --> 00:09:11,940
also expected as the positive sample and

196
00:09:11,940 --> 00:09:14,279
negative sample tends to be more similar

197
00:09:14,279 --> 00:09:16,860
to each other on smaller subgraphs

198
00:09:16,860 --> 00:09:19,019
making the attack model more difficult

199
00:09:19,019 --> 00:09:21,180
to distinguish between them

200
00:09:21,180 --> 00:09:24,480
and in addition the attack accuracy on

201
00:09:24,480 --> 00:09:26,880
improve is better than the others we

202
00:09:26,880 --> 00:09:29,459
suspect this is because tifu and minkapu

203
00:09:29,459 --> 00:09:31,620
decompose the graph structure during

204
00:09:31,620 --> 00:09:34,920
their protein process so the subgraph as

205
00:09:34,920 --> 00:09:37,140
a whole may never be seen by the Target

206
00:09:37,140 --> 00:09:40,380
Model and this makes the it harder for

207
00:09:40,380 --> 00:09:42,540
the graph embedded imagine

208
00:09:42,540 --> 00:09:45,740
to be effective

209
00:09:46,080 --> 00:09:50,100
okay here is the third attack is graph

210
00:09:50,100 --> 00:09:52,980
reconstruction attack the objective is

211
00:09:52,980 --> 00:09:55,160
that given the technical embedding

212
00:09:55,160 --> 00:09:57,959
reconstructs a graph that shares similar

213
00:09:57,959 --> 00:10:00,660
graph structures statistics with the

214
00:10:00,660 --> 00:10:02,339
telegraph

215
00:10:02,339 --> 00:10:05,519
and we use graph Auto encoder to serve

216
00:10:05,519 --> 00:10:08,160
as the attack model different from

217
00:10:08,160 --> 00:10:10,260
traditional to encoder in image domain

218
00:10:10,260 --> 00:10:12,959
the graph Auto encoder here has an

219
00:10:12,959 --> 00:10:16,560
addition graph matching component the

220
00:10:16,560 --> 00:10:18,980
reason is that there is a

221
00:10:18,980 --> 00:10:22,560
knows explicit note utterance enforced

222
00:10:22,560 --> 00:10:24,959
in the graph so to guarantee that

223
00:10:24,959 --> 00:10:27,899
similar graph has similar adjacency

224
00:10:27,899 --> 00:10:31,260
Matrix the other is very important after

225
00:10:31,260 --> 00:10:33,420
the auto encoder is tuned we use the

226
00:10:33,420 --> 00:10:36,300
decoder as our attack model so given the

227
00:10:36,300 --> 00:10:38,880
target graph embedding the decoder can

228
00:10:38,880 --> 00:10:42,260
reconstruct the graph

229
00:10:42,300 --> 00:10:45,959
and here is a experimental result we use

230
00:10:45,959 --> 00:10:47,940
two types of metrics to evaluate the

231
00:10:47,940 --> 00:10:50,880
tech performance the first one is graph

232
00:10:50,880 --> 00:10:53,820
osmorphism it directly Compares two

233
00:10:53,820 --> 00:10:56,040
graphs using graph kernel

234
00:10:56,040 --> 00:10:59,100
the graphic isomorphism of one indicate

235
00:10:59,100 --> 00:11:01,740
to graph are the same

236
00:11:01,740 --> 00:11:04,200
and the second metric is the macro level

237
00:11:04,200 --> 00:11:07,140
graph statistics we adopted two widely

238
00:11:07,140 --> 00:11:10,019
used statistics here such as degree

239
00:11:10,019 --> 00:11:12,839
distribution Etc we use cosine

240
00:11:12,839 --> 00:11:14,459
similarity to measure the similarity

241
00:11:14,459 --> 00:11:16,740
between the typograph and constructed

242
00:11:16,740 --> 00:11:17,880
graph

243
00:11:17,880 --> 00:11:19,500
which shows that

244
00:11:19,500 --> 00:11:23,120
our proposed graph reconstruction attack

245
00:11:23,120 --> 00:11:25,740
achieves good performance for both

246
00:11:25,740 --> 00:11:27,420
metrics

247
00:11:27,420 --> 00:11:30,959
and finally we also invested investigate

248
00:11:30,959 --> 00:11:34,320
a simple defense mechanism which is at

249
00:11:34,320 --> 00:11:37,800
running editing random LaPlace noise on

250
00:11:37,800 --> 00:11:39,540
the graph embedding before it is

251
00:11:39,540 --> 00:11:43,500
published The xx here is the standard

252
00:11:43,500 --> 00:11:46,320
division of The Last Action noise the

253
00:11:46,320 --> 00:11:50,180
y-axis is the model performance

254
00:11:50,459 --> 00:11:52,620
we observe that the funds can

255
00:11:52,620 --> 00:11:54,959
effectively mitigate the text while

256
00:11:54,959 --> 00:11:57,600
maintaining acceptable accuracy job for

257
00:11:57,600 --> 00:12:01,680
normal graph graph classification task

258
00:12:01,680 --> 00:12:04,440
so to conclude through our attacks

259
00:12:04,440 --> 00:12:06,000
against the graph embedding we found

260
00:12:06,000 --> 00:12:08,160
that directly sharing graph embedding is

261
00:12:08,160 --> 00:12:10,160
not safe and we show that

262
00:12:10,160 --> 00:12:12,300
probation-based method

263
00:12:12,300 --> 00:12:15,240
can mitigate the inference attack but

264
00:12:15,240 --> 00:12:17,700
sometimes at the cost of model utility

265
00:12:17,700 --> 00:12:20,760
degradation and in this paper we only

266
00:12:20,760 --> 00:12:23,160
consider reconstructed the graph

267
00:12:23,160 --> 00:12:25,500
structure one interesting future work

268
00:12:25,500 --> 00:12:27,839
might be reconstruct another feature as

269
00:12:27,839 --> 00:12:28,680
well

270
00:12:28,680 --> 00:12:31,440
besides when the adversary can access

271
00:12:31,440 --> 00:12:34,079
different information with leakage be

272
00:12:34,079 --> 00:12:36,000
more severe or not it's also very

273
00:12:36,000 --> 00:12:39,240
interesting and we have evaluated graph

274
00:12:39,240 --> 00:12:41,640
recovery attack against the known

275
00:12:41,640 --> 00:12:44,100
embeddings and for more details you can

276
00:12:44,100 --> 00:12:47,459
check out paper Finding Nemo

277
00:12:47,459 --> 00:12:50,040
and that's it thanks for your listening

278
00:12:50,040 --> 00:12:53,779
and now I'm happy to take any questions

