1
00:00:11,100 --> 00:00:16,000
thank you the provenance of our data
matters in 2009

2
00:00:16,000 --> 00:00:19,680
illegal thousands of emails exchanged
between climate research scientists

3
00:00:19,680 --> 00:00:22,560
called into question the validity of
their results

4
00:00:22,560 --> 00:00:27,880
leaving them scrambling to refute the
accusations of skeptics a year later the

5
00:00:27,880 --> 00:00:33,320
seemingly benign Lady Gaga CD after a
military checkpoint unbeknownst to the

6
00:00:33,320 --> 00:00:36,840
guard station to that checkpoint the
city had just been used to extract trade

7
00:00:36,840 --> 00:00:41,750
thousands of classified documents more
recently lab technicians in

8
00:00:41,750 --> 00:00:45,250
massachusetts were arrested for
tampering with court-ordered drug tests

9
00:00:45,250 --> 00:00:49,190
in the aftermath of lab couldn't even
say with certainty which tests had been

10
00:00:49,190 --> 00:00:53,459
accessed by these technicians calling
into question tens of thousands of

11
00:00:53,460 --> 00:00:58,500
drug-related convictions now and all of
these cases a lack of data provenance

12
00:00:58,500 --> 00:01:03,309
led to disaster with this climate
scientists needed was a description of

13
00:01:03,309 --> 00:01:07,420
their data that explained the series of
analyses they performed to lead to a

14
00:01:07,420 --> 00:01:11,570
particular conclusion that checkpoint
needed was a monitoring system that

15
00:01:11,570 --> 00:01:16,350
track the flow of transient data through
the base and the drug laboratory you too

16
00:01:16,350 --> 00:01:20,580
detailed documentation of every lab
tests workflow so they can precisely

17
00:01:20,580 --> 00:01:25,590
identify which results may be corrupted
by those technicians provenance matters

18
00:01:25,590 --> 00:01:29,680
because it provides all of these things
now unfortunately another common link

19
00:01:29,680 --> 00:01:33,470
between each of these scenarios is that
they reside in malicious environments

20
00:01:33,470 --> 00:01:38,090
the trustworthiness of the scientists
the whistleblower and the laptop are all

21
00:01:38,090 --> 00:01:41,960
in question so if provenance had been
available these agents may have been

22
00:01:41,960 --> 00:01:45,789
strongly incentivized to tamper with
that provenance trail in order to hide

23
00:01:45,790 --> 00:01:50,570
evidence of their misdeeds in light of
his knowledge it quickly becomes

24
00:01:50,570 --> 00:01:55,258
apparent only valuable when we're
confident of its integrity authenticity

25
00:01:55,259 --> 00:02:00,000
and completeness up to now the little
attention has been paid to the design of

26
00:02:00,000 --> 00:02:04,540
secure and resilient provenance
collection mechanisms know we tackle

27
00:02:04,540 --> 00:02:08,299
this challenge in this work creating a
secure prominent somewhere system

28
00:02:08,299 --> 00:02:13,110
represent lennox province modules or LPN
a kernel patch to provide some

29
00:02:13,110 --> 00:02:15,519
prominence monitor for the Linux kernel

30
00:02:15,519 --> 00:02:19,329
p.m. introduces a provenance layer to
the operating system that permits the

31
00:02:19,329 --> 00:02:24,250
observation of all system events related
to the manipulation of objects rather

32
00:02:24,250 --> 00:02:27,580
than introduce redundant protections we
demonstrate that LPM could be secured

33
00:02:27,580 --> 00:02:28,880
using the existing

34
00:02:28,880 --> 00:02:33,590
like security mechanisms to further
motivate the capabilities of our system

35
00:02:33,590 --> 00:02:37,990
we going to present a scheme for
provenance based data loss prevention in

36
00:02:37,990 --> 00:02:41,040
contrast to existing enterprise
solutions which are often based on

37
00:02:41,040 --> 00:02:45,019
heuristic in error-prone primitives are
GOP mechanism is able to make

38
00:02:45,020 --> 00:02:48,880
enforcement decisions based on the true
flow of information through the system

39
00:02:48,880 --> 00:02:53,549
offering dramatic improvement over the
state of the art without p.m. prominence

40
00:02:53,550 --> 00:02:57,360
has not only been made it easy it's also
cheaper we show that our system imposes

41
00:02:57,360 --> 00:02:59,540
as little as 3% over time

42
00:02:59,540 --> 00:03:04,989
computational overhead pardon and
realistic workloads and that our data

43
00:03:04,990 --> 00:03:09,750
loss prevention mechanism can search
complex ancestries for system objects in

44
00:03:09,750 --> 00:03:14,070
just a handful of milliseconds the rest
of the talk is gonna be structured as

45
00:03:14,070 --> 00:03:14,650
follows

46
00:03:14,650 --> 00:03:20,090
first your brief primer on provenance
followed by the design and deployment of

47
00:03:20,090 --> 00:03:21,550
LPN

48
00:03:21,550 --> 00:03:25,230
introduce our data loss prevention
mechanism described our values and our

49
00:03:25,230 --> 00:03:31,530
results include so data provenance is
meditate under describes the history of

50
00:03:31,530 --> 00:03:35,420
an object that's been processed on a
computer system this history details

51
00:03:35,420 --> 00:03:38,579
what's happened to the object from the
time it was created including how it

52
00:03:38,580 --> 00:03:43,430
came to exist in its present state
province can be expressed as a graph in

53
00:03:43,430 --> 00:03:49,300
which nodes are users data objects and
processes while edges conveyed

54
00:03:49,300 --> 00:03:53,020
relationship between these notes on the
provenance crash here we see that

55
00:03:53,020 --> 00:03:56,750
there's been an output that was
generated by a process and that process

56
00:03:56,750 --> 00:03:58,860
used to inputs in order to create it

57
00:03:58,860 --> 00:04:04,390
meanwhile that process was controlled by
a particular user so given its value in

58
00:04:04,390 --> 00:04:08,640
this work we assume that provinces on
right candidate for attack to

59
00:04:08,640 --> 00:04:13,359
conceptualize this week introduced a
provenance aware adversary no provenance

60
00:04:13,360 --> 00:04:17,320
or adversaries just wanted recognizes
that provinces being collected on the

61
00:04:17,320 --> 00:04:21,599
system so they might take any number of
actions including attempting to disable

62
00:04:21,600 --> 00:04:25,590
our province collection mechanism
avoiding taking certain actions on the

63
00:04:25,590 --> 00:04:27,619
system in order to remain invisible

64
00:04:27,620 --> 00:04:32,000
provenance monitor when they might set
up the corruptor falsify the provenance

65
00:04:32,000 --> 00:04:36,030
the tardy been collected we make the
following assumptions in the design of

66
00:04:36,030 --> 00:04:40,549
our system we considered all user space
applications to be potentially subject

67
00:04:40,550 --> 00:04:44,880
a compromise and therefore potentially
untrustworthy once compromise an

68
00:04:44,880 --> 00:04:48,600
application might begin to emit false
provenance that lies about its actions

69
00:04:48,600 --> 00:04:52,750
on the system we also considered the
kernel to be vulnerable to run 10

70
00:04:52,750 --> 00:04:58,740
compromises we assume that the kernel as
distributed installed within a known

71
00:04:58,740 --> 00:05:03,420
good configuration finally resumed the
presence of some public infrastructure

72
00:05:03,420 --> 00:05:06,720
that is going to assist us in allowing
provinces warehouse to be able to

73
00:05:06,720 --> 00:05:12,040
communicate since we want prominence
that comprehensive enough to reflect any

74
00:05:12,040 --> 00:05:16,250
possible action taken by an attacker
it's necessary to explicitly defined

75
00:05:16,250 --> 00:05:20,020
what type of provenance were trying to
collect a to do so we define whole

76
00:05:20,020 --> 00:05:25,130
system prominence as the complete an
exhaustive description of how agents had

77
00:05:25,130 --> 00:05:28,560
controlled activities in order to
manipulate controlled data types during

78
00:05:28,560 --> 00:05:33,390
system execution now the significance of
this definition is that I control data

79
00:05:33,390 --> 00:05:37,830
types or concept from the Linux security
and literature a numerate all of the

80
00:05:37,830 --> 00:05:41,669
kernel objects that informed the system
security state so if we capture

81
00:05:41,670 --> 00:05:45,190
provenance for all interaction to these
objects we should have complete

82
00:05:45,190 --> 00:05:50,920
visibility into the actions of the
attacker on the system so with this in

83
00:05:50,920 --> 00:05:54,000
mind we set out to design a system that
possess the following security

84
00:05:54,000 --> 00:05:57,900
properties first there are problems
mechanism satisfied the reference

85
00:05:57,900 --> 00:06:02,419
monitor concept that had complete
observation of all system events that he

86
00:06:02,420 --> 00:06:06,710
be tamper-proof and that these
properties were verifiable beyond the

87
00:06:06,710 --> 00:06:10,060
reference monitor guarantees our system
used to facilitate the deployment of

88
00:06:10,060 --> 00:06:14,740
distributed systems providing a channel
that ensure the authenticity and

89
00:06:14,740 --> 00:06:18,490
integrity of provenance that was
transmitted over an untrusted network

90
00:06:18,490 --> 00:06:23,750
final consideration during design was
that a provenance layering so in order

91
00:06:23,750 --> 00:06:27,480
to bridge the semantic gap that exists
between the operating system and

92
00:06:27,480 --> 00:06:31,520
applications workflows we can create
provinces where applications that

93
00:06:31,520 --> 00:06:35,240
voluntary unpaid voluntary early
disclose information about their

94
00:06:35,240 --> 00:06:40,320
activities however in our threat model
we assume any obligation is potentially

95
00:06:40,320 --> 00:06:44,099
under the control of the attacker so our
system must offer a gateway that can

96
00:06:44,100 --> 00:06:49,080
upgrade disclosed provenance to a high
integrity state

97
00:06:49,080 --> 00:06:52,800
described p.m. I'm gonna be internally
introducing system components to

98
00:06:52,800 --> 00:06:57,300
describe diagram over here on the right
and the foundation of LBM is currently

99
00:06:57,300 --> 00:07:01,639
shown on the screen a set of hooks are
placed around the kernel their

100
00:07:01,639 --> 00:07:06,240
registered by a provenance module and
that module subsequently generates

101
00:07:06,240 --> 00:07:12,650
problems and transmits it to user space
for storage IBM introduced p.m.

102
00:07:12,650 --> 00:07:16,409
instruments the kernel with one hundred
and seventy dedicated prominent sucks

103
00:07:16,409 --> 00:07:21,190
unlike the Linux security modules
authorization hooks problems are not

104
00:07:21,190 --> 00:07:25,139
intended to mediate access or alter the
control path of the function in any way

105
00:07:25,139 --> 00:07:32,180
but instead to possibly observe system
activity before during the last two pics

106
00:07:32,180 --> 00:07:37,159
how provenance affect the open system
call when a process in userspace

107
00:07:37,159 --> 00:07:40,960
attempts to open a document it's risking
to go through a series of air jacks and

108
00:07:40,960 --> 00:07:44,840
discretionary access control checks
followed by the Linux security module

109
00:07:44,840 --> 00:07:49,429
authorization but there's an LSM enabled
and this activity has been approved

110
00:07:49,430 --> 00:07:55,029
vendor products such as reached LPM
examines the concept of the context

111
00:07:55,029 --> 00:07:59,599
increase in New Providence record now
with a problem in SoCal p.m. actually

112
00:07:59,599 --> 00:08:03,960
has the ability to deny access to her
last however in practice this only

113
00:08:03,960 --> 00:08:09,378
happens in the event that LPM fails to
allocate memory to create a new problems

114
00:08:09,379 --> 00:08:14,969
record at which point he needs to the
ninety activity by mirroring these Linux

115
00:08:14,969 --> 00:08:19,370
security module authorization hooks the
produxer able to observe all security

116
00:08:19,370 --> 00:08:22,969
sensitive operations to control data
types this includes things like

117
00:08:22,969 --> 00:08:27,469
operations on files sockets
shared-memory inter-process

118
00:08:27,469 --> 00:08:31,169
communication basically all of the major
operating system primitives you can

119
00:08:31,169 --> 00:08:37,478
think of so for surveying the prominence
it quickly became apparent to us that

120
00:08:37,479 --> 00:08:41,789
reaches her job dramatically different
needs from Providence so this motivated

121
00:08:41,789 --> 00:08:46,459
the modular design of our system
currently offers support for the hi-fi

122
00:08:46,459 --> 00:08:51,079
system which was originally based on
analysis module the spade scientific

123
00:08:51,079 --> 00:08:52,949
auditing project

124
00:08:52,950 --> 00:08:57,790
also been used by researchers in the
up-tempo project which seeks to use

125
00:08:57,790 --> 00:09:01,439
provenance to perform automatic
generation mandatory access control

126
00:09:01,440 --> 00:09:07,390
policies now when prominent events are
generated mcconnell there immediately

127
00:09:07,390 --> 00:09:12,360
transmitted to user space for storage
this province reporter interprets the

128
00:09:12,360 --> 00:09:15,930
kernel stream and record the provenance
into one of a number of different

129
00:09:15,930 --> 00:09:21,270
storage back ends welfare support for
writing to compressed binary files with

130
00:09:21,270 --> 00:09:26,590
the gzip library relational databases
with Postgres equal no seat will be the

131
00:09:26,590 --> 00:09:31,800
basis with new 4G and also an in-memory
graph database that makes you said the

132
00:09:31,800 --> 00:09:38,550
Snapdragon library when considering
distributed environments is necessary

133
00:09:38,550 --> 00:09:42,839
for each network message to be
associated with a unique identifier this

134
00:09:42,840 --> 00:09:46,400
is going to allow the recipient of a
message to later recovered provenance

135
00:09:46,400 --> 00:09:51,480
from the center of the message however
if we're going over an untrusted network

136
00:09:51,480 --> 00:09:55,460
there might be an attacker dare that
could strip or modified that prominence

137
00:09:55,460 --> 00:10:00,730
identifier so to address this problem
also registers a set of net filter box

138
00:10:00,730 --> 00:10:04,880
which it uses to implement a message
commitment protocol that reside entirely

139
00:10:04,880 --> 00:10:10,450
in the kernel for each outbound packet
LPM signs over the immutable feels about

140
00:10:10,450 --> 00:10:15,160
IP packet header as well as its contents
and then embed the signature into the

141
00:10:15,160 --> 00:10:20,520
options field of the IP header when the
packets received LPM first to verify the

142
00:10:20,520 --> 00:10:24,750
signature if the signature verification
fails I'll p.m. can be optionally

143
00:10:24,750 --> 00:10:29,310
configured to drop the packet this is so
that the provenance record remains a

144
00:10:29,310 --> 00:10:34,910
complete and total reflection of the
events that happened on the system so

145
00:10:34,910 --> 00:10:38,860
tired of this message transmission
sequence as shown above all PM's the

146
00:10:38,860 --> 00:10:43,020
last hurdle component to modify the
packet prior to transmission and the

147
00:10:43,020 --> 00:10:47,130
first colonel component to handle the
pack up and reception at no time can

148
00:10:47,130 --> 00:10:52,700
user space processes view or modify the
content of this IP options field nor can

149
00:10:52,700 --> 00:10:56,740
be circumvented arnett filter mechanism
so in this way our message commitment

150
00:10:56,740 --> 00:11:00,220
protocol is both mandatory and invisible
to userspace

151
00:11:00,220 --> 00:11:05,290
ensuring its compatibility with all
applications and other important

152
00:11:05,290 --> 00:11:09,380
considerations design about p.m. is that
of layered provenance capturing

153
00:11:09,380 --> 00:11:13,220
provenance of the kernel error offers
all of these wonderful properties most

154
00:11:13,220 --> 00:11:17,020
foremost among them being its
completeness that gives us a total UV

155
00:11:17,020 --> 00:11:20,870
system activity now unfortunately the
Colonels unable to understand the

156
00:11:20,870 --> 00:11:25,000
internal semantics of individual
applications leading to a need for

157
00:11:25,000 --> 00:11:30,020
layered provenance architectures however
since our threat model assumes that you

158
00:11:30,020 --> 00:11:33,360
just be supplications are prone to
compromise how can we trust the

159
00:11:33,360 --> 00:11:37,510
provenance they disclosed to preserve
our security goes we need a means of

160
00:11:37,510 --> 00:11:43,520
evaluating the integrity of prominence
over applications our solution is to

161
00:11:43,520 --> 00:11:47,870
turn to the Linux integrity measurement
architecture or keema human-computer

162
00:11:47,870 --> 00:11:52,470
cryptographic hash to each binary before
its execution and extends this

163
00:11:52,470 --> 00:11:56,660
measurement to register in the system's
Trusted Platform Module at which point

164
00:11:56,660 --> 00:12:01,500
it publishes the memory of the
measurement 22 kernel memory so an

165
00:12:01,500 --> 00:12:05,020
application wishes to disclose
provenance it first contact the

166
00:12:05,020 --> 00:12:09,610
provenance recorder the recorder looks
up the process idea of the center which

167
00:12:09,610 --> 00:12:12,420
it uses to check the email measurement
against a list of known good

168
00:12:12,420 --> 00:12:17,180
configurations and see if a binary had
been modified by the adversary in order

169
00:12:17,180 --> 00:12:21,439
to commit false provenance information
this activity would be detected and the

170
00:12:21,440 --> 00:12:26,200
province would be rejected this provides
an assurance that layered for provenance

171
00:12:26,200 --> 00:12:32,470
remains in a high integrity rubble we
consider the following secure deployment

172
00:12:32,470 --> 00:12:38,130
of LPM in our security analysis using
the Trusted Platform Module we ensure

173
00:12:38,130 --> 00:12:42,800
the platform integrated LPM using an
Intel Trusted boot sequence Crested

174
00:12:42,800 --> 00:12:45,870
Butte measures each system component
before loading it

175
00:12:45,870 --> 00:12:50,360
attackers from being able to force a
reboot into a different kernel the

176
00:12:50,360 --> 00:12:54,530
trusted platform modules also used to
prove to remote house that IBM's in a

177
00:12:54,530 --> 00:12:59,250
known good configuration what's the
correct colonel is loaded we need to

178
00:12:59,250 --> 00:13:03,820
preserve its runtime integrity after
this we turn to SELinux we make you sit

179
00:13:03,820 --> 00:13:10,070
at the SELinux MLS policy and write some
small policy modules to protect all of

180
00:13:10,070 --> 00:13:11,320
the userspace

181
00:13:11,320 --> 00:13:17,120
are introduced by OPM our arguments for
the security of this deployment are as

182
00:13:17,120 --> 00:13:22,160
follows for completeness albums
provenance folks are a superset of the

183
00:13:22,160 --> 00:13:26,420
Alice an authorization codes with each
authorization are gonna colonel being

184
00:13:26,420 --> 00:13:31,089
immediately followed by a province of
the placement of the Allison

185
00:13:31,090 --> 00:13:33,950
authorization codes have been repeatedly
validated through both static and

186
00:13:33,950 --> 00:13:37,610
dynamic analysis techniques notably
through the efforts of Edwards ignorance

187
00:13:37,610 --> 00:13:43,560
on concluding that there is an
authorization hook on the control path

188
00:13:43,560 --> 00:13:48,800
of every sentence colonel operation that
manipulates control data therefore since

189
00:13:48,800 --> 00:13:52,650
the provenance Oaks Ave near the
authorization hooks LPM offers the

190
00:13:52,650 --> 00:13:57,890
ability to record complete provenance
for those same operations for

191
00:13:57,890 --> 00:14:01,880
tamper-proof earnest LPNs runtime
integrity is assured through the SELinux

192
00:14:01,880 --> 00:14:06,540
MLS policy in particular excellent
exists there to protect the trusted

193
00:14:06,540 --> 00:14:09,680
computing base of the colonel and also
about p.m.

194
00:14:09,680 --> 00:14:15,130
space applications and attacked her
can't reboot into another crown due to

195
00:14:15,130 --> 00:14:19,530
the presence of trusted be technologies
one of the most viable attacks is that

196
00:14:19,530 --> 00:14:25,050
of a resource exhaustion you know das
the system until there's no more room to

197
00:14:25,050 --> 00:14:29,699
crack collector provenance however LPM
can deny system activities in the event

198
00:14:29,700 --> 00:14:34,350
that it's unable to allocate memory for
provenance so for as long as the system

199
00:14:34,350 --> 00:14:40,350
is usable the provenance record remains
complete rpm so placement inherits the

200
00:14:40,350 --> 00:14:45,840
formally verified properties of projects
as well as the runtime integrity

201
00:14:45,840 --> 00:14:51,580
assurance his past work with analysis of
the next MLS policy however the

202
00:14:51,580 --> 00:14:55,490
verification of individual provenance
modules is not addressed in our work so

203
00:14:55,490 --> 00:14:59,550
we leave that to future where r message
commitment protocol which is implemented

204
00:14:59,550 --> 00:15:03,280
entirely in the journal provides an
authenticated channel between provinces

205
00:15:03,280 --> 00:15:08,459
where hosts applications running on
either hoster unable to interfere with

206
00:15:08,460 --> 00:15:12,080
this protocol because it occurs
immediately prior to transmission

207
00:15:12,080 --> 00:15:17,140
and immediately upon reception and
because LBM suppresses application's

208
00:15:17,140 --> 00:15:22,250
ability to access to get and set IP
option system calls an adversary in the

209
00:15:22,250 --> 00:15:26,800
network cannot tamper with a message for
us the signature verified which point

210
00:15:26,800 --> 00:15:31,500
the message will be delivered to the
application are secured disclosure

211
00:15:31,500 --> 00:15:35,520
mechanism provides assurance that our
applications are known good

212
00:15:35,520 --> 00:15:39,460
configuration of the time that they're
loaded unfortunately it doesn't address

213
00:15:39,460 --> 00:15:42,890
the possibility of a long-running
program being compromised during

214
00:15:42,890 --> 00:15:47,380
execution runtime integrity attestation
is what we need for that and that's a

215
00:15:47,380 --> 00:15:52,420
challenging problem that was simply
outside of the scope of this work right

216
00:15:52,420 --> 00:15:55,990
so next time going to further motivate
our system by telling you about data

217
00:15:55,990 --> 00:16:03,260
loss prevention tool that we built using
LPM data loss prevention is monitoring

218
00:16:03,260 --> 00:16:07,310
software that seeks to minimize the
spread of sensitive data in large

219
00:16:07,310 --> 00:16:12,119
complex organizations however current
enterprise software often makes use of

220
00:16:12,120 --> 00:16:15,610
heuristic approaches that are rooted in
the true flow of information through the

221
00:16:15,610 --> 00:16:19,680
system we addressed this by creating a
new prominence paste data loss

222
00:16:19,680 --> 00:16:23,250
prevention primitive the tour was
comprised of both the monitoring and

223
00:16:23,250 --> 00:16:28,620
enforcement phase in the monitoring and
administrators able to reason about the

224
00:16:28,620 --> 00:16:33,320
flow of information on an existing
system for example if a database on the

225
00:16:33,320 --> 00:16:37,650
system contains sensitive information
they could search the system for all

226
00:16:37,650 --> 00:16:41,569
objects that were derived from that
database using this information the

227
00:16:41,570 --> 00:16:45,800
administrator complete informed policy
about which objects are not allowed to

228
00:16:45,800 --> 00:16:50,599
leave the system we implemented an
enforcement phase as a file transfer

229
00:16:50,600 --> 00:16:55,500
mechanism that shown here on the screen
now prior to committing a file to leave

230
00:16:55,500 --> 00:17:00,250
the system this applications going to
scan the provenance of that file if one

231
00:17:00,250 --> 00:17:05,069
of them is one of the ancestors of this
file matches the policy then

232
00:17:05,069 --> 00:17:09,059
transferring this file is going to be
blocked

233
00:17:09,059 --> 00:17:13,529
by surveying publicly available white
papers describing existing DLP products

234
00:17:13,529 --> 00:17:17,878
we conducted an informal analysis that
compared our problem in space primitive

235
00:17:17,878 --> 00:17:23,658
with existing approaches must GOP
systems made using regular expressions

236
00:17:23,659 --> 00:17:27,749
direct nice data the major problem with
this of course is that it can't handle

237
00:17:27,749 --> 00:17:33,029
any sort of data transformation have a
credit card number and I encrypted or I

238
00:17:33,029 --> 00:17:37,950
compress it then it's no longer gonna
look like a credit card number some

239
00:17:37,950 --> 00:17:43,529
peoples in contrast make use of security
labels but revenues system later labels

240
00:17:43,529 --> 00:17:48,149
like we might be familiar with they do
this entirely in userspace and do this

241
00:17:48,149 --> 00:17:52,899
by it manually annotating the metadata
Office documents so I might have

242
00:17:52,899 --> 00:17:57,090
microsoft office with a plugin that
allows me to select a security label for

243
00:17:57,090 --> 00:18:01,779
any file that I create it so because
these labels are manually annotated it's

244
00:18:01,779 --> 00:18:04,759
unlikely that they're gonna be able to
provide complete enforcement of any

245
00:18:04,759 --> 00:18:09,279
policy and they might especially
struggle in the case where we have dated

246
00:18:09,279 --> 00:18:13,879
fusions in which a new file is derived
from multiple previous files with

247
00:18:13,879 --> 00:18:19,100
different security levels now in
contrast our province based mechanism

248
00:18:19,100 --> 00:18:24,230
allows the it follows the actual flow of
data the system so in our case were

249
00:18:24,230 --> 00:18:29,610
undeterred by data transformations and
diffusion is because at all times we

250
00:18:29,610 --> 00:18:36,258
maintain a complete record of the
ancestry of every object to get a better

251
00:18:36,259 --> 00:18:40,059
sense of the performance cost of
collecting province without p.m. we're

252
00:18:40,059 --> 00:18:44,320
in a series of benchmark workloads the
first was the classic Cornel Campbell

253
00:18:44,320 --> 00:18:48,019
compilation Accra benchmark and would
resolve that provenance collection and

254
00:18:48,019 --> 00:18:54,850
post 2.7% performance overhead compared
to compilation on a vanilla kernel we

255
00:18:54,850 --> 00:19:01,049
next ran the blast genomics benchmarks
which are simulating typical biological

256
00:19:01,049 --> 00:19:05,830
sequence in workloads as published by
the National Institute of Health Center

257
00:19:05,830 --> 00:19:11,779
4.8% overhead finally ran the bookmark
tool which simulates and i own workload

258
00:19:11,779 --> 00:19:16,200
for an email server observes 7.5% over

259
00:19:16,200 --> 00:19:19,909
had and the reason for the increase is
due to the manner in which the Linux

260
00:19:19,909 --> 00:19:24,630
kernel and particularly LPM receives my
life so if we're opening a file in this

261
00:19:24,630 --> 00:19:30,649
directory is actually gonna be an access
check for every single directory path so

262
00:19:30,649 --> 00:19:34,139
when that occurs rather than create a
single New Providence event we actually

263
00:19:34,139 --> 00:19:39,110
create multiple events in contrast to
other system calls were there might only

264
00:19:39,110 --> 00:19:44,120
be a single New Providence record
created for a particular event and I'll

265
00:19:44,120 --> 00:19:49,049
intensive workloads are going to
experience slightly higher overheads now

266
00:19:49,049 --> 00:19:53,350
a major challenge to performing
automated provenance collection which is

267
00:19:53,350 --> 00:19:58,209
what we're trying to do here is the high
storage overhead and current so to

268
00:19:58,210 --> 00:20:03,169
capture that we perform the kernel
compilation benchmark again and plotted

269
00:20:03,169 --> 00:20:08,940
the growth of our carbon storage using
different reporters so this top line

270
00:20:08,940 --> 00:20:16,690
here is the raw data was generated by
the kernel which was about 4.5% in only

271
00:20:16,690 --> 00:20:23,159
10 minutes now fortunately only required
for 250 megabytes of storage using are

272
00:20:23,159 --> 00:20:27,269
compressed recorder but province in this
format couldn't be efficiently queried

273
00:20:27,269 --> 00:20:32,630
so we also turn to our in memory Graford
border in the middle there since this

274
00:20:32,630 --> 00:20:36,159
was an in-memory graph we estimated the
amount of storage required by pulling

275
00:20:36,159 --> 00:20:41,480
the process system we found that it
required approximately 1.6 gigabytes

276
00:20:41,480 --> 00:20:45,940
just doing this in memory graph these
results merely served to show that

277
00:20:45,940 --> 00:20:50,820
problem in storage overheads are major
consideration and that incidentally

278
00:20:50,820 --> 00:20:55,210
these overheads are roughly consistent
with past work in developing provenance

279
00:20:55,210 --> 00:20:58,730
aware systems it's just that does pass
systems haven't had all of these

280
00:20:58,730 --> 00:21:03,049
wonderful security guarantees to go on
with these overheads can be managed

281
00:21:03,049 --> 00:21:06,418
through compression and garbage
collection techniques that have been

282
00:21:06,419 --> 00:21:12,289
proposed elsewhere in the literature on
a second series of tests we attempted to

283
00:21:12,289 --> 00:21:17,389
characterize the query performance of
the products we collect with LPN we did

284
00:21:17,389 --> 00:21:21,418
so using our data loss prevention tool
first we built a provenance craft by

285
00:21:21,419 --> 00:21:23,840
replaying replaying the logs

286
00:21:23,840 --> 00:21:28,199
the kernel compilation trial
uncompressed this log was four point

287
00:21:28,200 --> 00:21:33,600
eight gigabytes in size the resulting
staff contained roughly 6.5 million

288
00:21:33,600 --> 00:21:35,270
nodes

289
00:21:35,270 --> 00:21:40,270
everyday object in this graph we then
issued a query that traverse the objects

290
00:21:40,270 --> 00:21:47,279
complete ancestry its total history and
then returned back to us to keep short

291
00:21:47,279 --> 00:21:51,390
lived objects to didn't have many incest
ancestors from favorably securing our

292
00:21:51,390 --> 00:21:56,309
results we ignored responses from data
objects that had less than 50 ancestors

293
00:21:56,309 --> 00:22:01,830
so the cumulative density function for
that is shown here after 98% of the

294
00:22:01,830 --> 00:22:06,399
objects in this graph degree return in
just about two and a half milliseconds

295
00:22:06,399 --> 00:22:12,120
in the worst case which was a Date
object width 17,700 ancestors which

296
00:22:12,120 --> 00:22:16,840
since it was a compilation trial was the
kernel itself that returned in just 21

297
00:22:16,840 --> 00:22:20,470
milliseconds so this is a really
exciting because it's an indication that

298
00:22:20,470 --> 00:22:26,710
no we can do real-time nearly real-time
analysis of Providence and even when we

299
00:22:26,710 --> 00:22:32,029
have these huge graphs finally we
benchmark the performance of our message

300
00:22:32,029 --> 00:22:36,750
commitment protocol using I perf
unfortunately we found that rdsa signing

301
00:22:36,750 --> 00:22:42,320
scheme resulted in dramatically reduced
throughput which naturally follows due

302
00:22:42,320 --> 00:22:46,799
to the fact that we were doing it is a
signature for every single packet we

303
00:22:46,799 --> 00:22:50,168
considered the possibility of using
batch signatures to reduce this burden

304
00:22:50,169 --> 00:22:55,210
and we did find that that significantly
improves throughput at the end of the

305
00:22:55,210 --> 00:23:00,350
day this message commitment protocol is
a strong tool for providing network

306
00:23:00,350 --> 00:23:04,469
integrity for data provenance and
transmit and in some environments it

307
00:23:04,470 --> 00:23:09,120
might impose an acceptable cost other
possibilities that we did consider would

308
00:23:09,120 --> 00:23:14,149
be the use of IPsec or symmetric
signatures to speed things up our free

309
00:23:14,149 --> 00:23:19,959
the paper for our rationale for why we
chose to go with this approach to wrap

310
00:23:19,960 --> 00:23:23,809
things up in this work we've identified
the requirements for trustworthy

311
00:23:23,809 --> 00:23:27,250
provenance in distributed in complex
computing

312
00:23:27,250 --> 00:23:31,290
its we present the design implementation
and deployment of the world's first

313
00:23:31,290 --> 00:23:35,840
fully realized provenance monitor that
is a province collection mechanism that

314
00:23:35,840 --> 00:23:41,100
provides reference monitor guarantees
and we go a step further by showing the

315
00:23:41,100 --> 00:23:44,810
provenance can be used to improve the
state of the art and data loss

316
00:23:44,810 --> 00:23:50,540
prevention software if the things I've
been talking about today that you are

317
00:23:50,540 --> 00:23:54,980
source code is currently available at
Lenox provenance . word we have a copy

318
00:23:54,980 --> 00:23:59,330
of our kernel on RedHat will shortly be
releasing a copy of our kernel for the

319
00:23:59,330 --> 00:24:04,120
mainline Linux kernel and we also have
all of our User space four quarters and

320
00:24:04,120 --> 00:24:07,899
storage and software there with that
thanks so much for your time this

321
00:24:07,900 --> 00:24:16,050
morning I'll be happy to take your
questions now

322
00:24:16,050 --> 00:24:57,260
we actually did something called the
hi-fi system and in particular that one

323
00:24:57,260 --> 00:25:01,390
difference between the security in the
province Sox were that they were

324
00:25:01,390 --> 00:25:05,800
interested in ordering a packets
received in that actually couldn't be

325
00:25:05,800 --> 00:25:09,950
done using the existing so vain
georgia's introduced a couple of new

326
00:25:09,950 --> 00:25:15,190
security hooks to the networking stacked
in their work and we hear them into our

327
00:25:15,190 --> 00:25:19,000
work so there's some cases where the
context of Providence's slightly

328
00:25:19,000 --> 00:25:31,320
different than what the security system
needs a wonderful time to move to one of

329
00:25:31,320 --> 00:25:37,850
my backup slides so one of the main
reasons that we initially chose to use a

330
00:25:37,850 --> 00:25:43,199
dedicated system over instrumental in
turning on provenance of shredded in

331
00:25:43,200 --> 00:25:46,560
using these issues including security
module architecture was due to the fact

332
00:25:46,560 --> 00:25:50,530
that stacking up modules was not
natively supported like you need to use

333
00:25:50,530 --> 00:25:54,260
a community patch in order to provide
that we were concerned about going that

334
00:25:54,260 --> 00:25:58,090
route because we didn't know what it
means meant to have no multiple modules

335
00:25:58,090 --> 00:26:02,419
multiple reference monitors enabled on a
system at the same time that could

336
00:26:02,420 --> 00:26:07,010
potentially lead to predicted behavior
now just this summer there's been a push

337
00:26:07,010 --> 00:26:12,400
to the mainline kernel to enable support
for security module stacking

338
00:26:12,400 --> 00:26:16,960
so we're currently investigating using
that in place of the province of

339
00:26:16,960 --> 00:26:22,150
instrumented because at the end of the
day that would provide easier using the

340
00:26:22,150 --> 00:26:36,310
system working on a great question

341
00:26:36,310 --> 00:26:48,860
so the main primitive that we're
tracking is colonel forks so we'd expect

342
00:26:48,860 --> 00:26:52,840
you know every single process is going
to be associated with a particular

343
00:26:52,840 --> 00:26:57,870
colonel credential but not every
potential is associated with the process

344
00:26:57,870 --> 00:27:02,399
so every single time there's a fork in
the kernel we signed that a new province

345
00:27:02,400 --> 00:27:07,970
I D and then collects you know you've
heard it was in control of that province

346
00:27:07,970 --> 00:27:15,320
I D the binaries that reliance on the
files that were read and written

347
00:27:15,320 --> 00:27:25,389
so we configured in SELinux policy
module for that

348
00:27:25,390 --> 00:27:28,900
provenance reporter that I showed on the
screen and in our storage basically

349
00:27:28,900 --> 00:27:32,770
saying that you know there was only one
party that could create the kernel

350
00:27:32,770 --> 00:27:38,010
stream from userspace there was only one
party that could write to storage so we

351
00:27:38,010 --> 00:27:42,780
sort of we isolated our you know
provenance computing base from the rest

352
00:27:42,780 --> 00:27:47,920
of the system

353
00:27:47,920 --> 00:27:51,570
what do you do

354
00:27:51,570 --> 00:27:58,490
yeah that's that's a very very question
we don't have an optimal solution for

355
00:27:58,490 --> 00:28:05,880
side channels or not we're not alone in
that right so so one thing the one thing

356
00:28:05,880 --> 00:28:09,260
that we did think about and we discuss
it in the paper just you know things

357
00:28:09,260 --> 00:28:13,000
like the copy and paste buffer is a
pretty big side channel right that know

358
00:28:13,000 --> 00:28:20,980
something that could be singularly
addressed those seem very very hard to

359
00:28:20,980 --> 00:28:21,730
do any

360
00:28:21,730 --> 00:28:25,260
yea yea certainly don't know that was
running out of scope we did think about

361
00:28:25,260 --> 00:28:29,960
you know we're concerned with semantic
information that we're losing by being

362
00:28:29,960 --> 00:28:33,660
in the operating system so we did think
about you know like can we have a

363
00:28:33,660 --> 00:28:37,410
problem somewhere X windowing system
right and then we'd have some of these

364
00:28:37,410 --> 00:28:41,770
other things but now you know like
hardware side channels we don't we don't

365
00:28:41,770 --> 00:28:42,720
address in this work

366
00:28:42,720 --> 00:28:43,260
thank you

