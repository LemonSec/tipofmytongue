1
00:00:10,600 --> 00:00:15,610
sorry I'm really delighted to be here
this afternoon to see so many people

2
00:00:15,610 --> 00:00:23,670
here and I'm gonna talk today about work
that was done not just by me but by a

3
00:00:23,670 --> 00:00:27,970
lot of my students and colleagues at
Carnegie Mellon as well so let's get

4
00:00:27,970 --> 00:00:28,689
started

5
00:00:28,690 --> 00:00:34,200
have you ever wondered is there anything
we can do to get people to pay attention

6
00:00:34,200 --> 00:00:39,750
to web browser certificate warnings why
do people keep calling for phishing

7
00:00:39,750 --> 00:00:46,010
attacks are people willing to pay for
privacy

8
00:00:46,010 --> 00:00:49,420
why do people keep posting photos on
Facebook to get themselves into trouble

9
00:00:49,420 --> 00:00:56,690
or passphrase is really easier for
people to remember them passwords and

10
00:00:56,690 --> 00:01:02,089
why after all these years has johnnie
still not learn to encrypt and I had

11
00:01:02,089 --> 00:01:06,070
this light before I knew that this paper
was receiving an award this morning so

12
00:01:06,070 --> 00:01:12,250
it's very nice to see that we're more
than a decade my students and colleagues

13
00:01:12,250 --> 00:01:16,289
have conducted user studies to
investigate these and other questions we

14
00:01:16,289 --> 00:01:21,210
have conducted surveys interviews field
studies online studies in laboratory

15
00:01:21,210 --> 00:01:26,579
studies many involving a little bit of
deception now there's no textbook know

16
00:01:26,579 --> 00:01:31,359
how to manual for how to do research in
this area we draw lot from other fields

17
00:01:31,359 --> 00:01:38,339
and we come up with many new creative
ways to approach our problems are

18
00:01:38,340 --> 00:01:42,140
researchers covered a wide range of
topics in the usual privacy and security

19
00:01:42,140 --> 00:01:47,189
area one of the topics on which we have
published the most is usable and secure

20
00:01:47,189 --> 00:01:52,479
passwords only talked about that briefly
today as my student blazer will be

21
00:01:52,479 --> 00:01:56,350
presenting our latest passwords research
paper here tomorrow so come here that is

22
00:01:56,350 --> 00:02:02,780
well we've also published a number of
papers unusable access control security

23
00:02:02,780 --> 00:02:08,169
warnings and making privacy policy is
more usable in this talk today I'll be

24
00:02:08,169 --> 00:02:11,940
giving you a whirlwind tour through over
a decade of usable privacy and security

25
00:02:11,940 --> 00:02:16,790
research my focus is going to be on some
of the interesting user studies we have

26
00:02:16,790 --> 00:02:22,220
conducted and the challenges we face
along the way in the process I hope to

27
00:02:22,220 --> 00:02:24,050
teach you a little bit about doing

28
00:02:24,050 --> 00:02:27,970
usable security studies and hopefully
convince you that they're actually

29
00:02:27,970 --> 00:02:34,609
pretty worthwhile to do so here's the
road map for your tour we're gonna start

30
00:02:34,610 --> 00:02:40,550
by a little bit of usable security
studies 101 will talk about some studies

31
00:02:40,550 --> 00:02:45,750
we've done to evaluate computer security
warnings will talk about privacy

32
00:02:45,750 --> 00:02:51,480
indicators and willingness to pay for
privacy and how we measure that will

33
00:02:51,480 --> 00:02:57,530
talk about the design and evaluation of
privacy nudges for social media and will

34
00:02:57,530 --> 00:03:04,970
investigate that xkcd password
passphrase assertion so let's start with

35
00:03:04,970 --> 00:03:13,390
you security studies 101 alright so why
should we do usable security studies

36
00:03:13,390 --> 00:03:19,019
there are a variety of reasons that
people do these studies and i wanna talk

37
00:03:19,020 --> 00:03:25,180
about four main types of studies that
that we tend to do the first one is a

38
00:03:25,180 --> 00:03:29,840
needs assessment this is where we try to
understand what problems people are

39
00:03:29,840 --> 00:03:34,690
having and decide what to build what
tools does the world actually need what

40
00:03:34,690 --> 00:03:41,109
unsolved problems do we have the next
type of study is is an evaluation study

41
00:03:41,110 --> 00:03:48,540
so we want to measure the usability of a
particular system of particular features

42
00:03:48,540 --> 00:03:54,980
approaches sometimes were were
evaluating our own systems that we've

43
00:03:54,980 --> 00:04:00,290
built some time for evaluating systems
that other people have built the third

44
00:04:00,290 --> 00:04:05,620
type of study is useful to understand
tradeoff sometimes you can't get usable

45
00:04:05,620 --> 00:04:09,430
security on its own without trading off
something else and so it's good to

46
00:04:09,430 --> 00:04:14,310
understand any tradeoffs whether they be
security versus usability privacy

47
00:04:14,310 --> 00:04:18,320
convenience price or whatever

48
00:04:18,320 --> 00:04:22,460
finally their study said or done to
understand the root causes of problems

49
00:04:22,460 --> 00:04:26,120
so we already know there's a problem we
already know that something is not

50
00:04:26,120 --> 00:04:30,870
working but the question is why is it
not working and can we understand the

51
00:04:30,870 --> 00:04:34,640
root cause of the problem and that will
help give us some clues as to how to

52
00:04:34,640 --> 00:04:35,610
address the

53
00:04:35,610 --> 00:04:42,580
I'm not there are a lot of excuses for
not doing usable security studies and

54
00:04:42,580 --> 00:04:48,609
you may have heard some of them I will
share a few with you today so sometimes

55
00:04:48,610 --> 00:04:53,389
people say well you know people tell me
they can't use the system but of our

56
00:04:53,389 --> 00:04:58,560
users just weren't so lazy or stupid or
careless the system really would work

57
00:04:58,560 --> 00:05:00,020
just fine

58
00:05:00,020 --> 00:05:06,669
another popular 1 I'm a cryptographer
naughty usability expert so the

59
00:05:06,669 --> 00:05:11,128
implication is that somebody else should
do the usability study which may in fact

60
00:05:11,129 --> 00:05:17,870
be the case but I think it's useful for
the system developers to make friends

61
00:05:17,870 --> 00:05:22,150
with the people who might do the studies
and actually work with rather known just

62
00:05:22,150 --> 00:05:29,888
assuming they will come along on their
own later and usability studies I hear

63
00:05:29,889 --> 00:05:33,830
people say well yeah uses a SIM I know
what people want I'm just building what

64
00:05:33,830 --> 00:05:38,599
I know people want without feeling the
need to actually run a study and find

65
00:05:38,599 --> 00:05:45,409
out for themselves and I find the system
et tu so it must be usable in my office

66
00:05:45,409 --> 00:05:51,690
made also find it easy to use and my
kids can use the system so it definitely

67
00:05:51,690 --> 00:05:56,919
must be usable right now I don't know
about you but my fourteen year old son

68
00:05:56,919 --> 00:06:01,299
he's been using a computer since he was
chill and he's not really a typical user

69
00:06:01,300 --> 00:06:06,330
in fact none of my three kids are
typical users which is really too bad

70
00:06:06,330 --> 00:06:11,258
because if they were I could run three
participate user studies all the time in

71
00:06:11,259 --> 00:06:15,870
my house and it would be awfully
convenient and hear the gratuitous

72
00:06:15,870 --> 00:06:18,930
photos all three of them because I
showed you just a picture of my son it

73
00:06:18,930 --> 00:06:22,210
wouldn't be fair to the other two so
there you go

74
00:06:22,210 --> 00:06:31,039
but I'm not joking that people do test
their systems on their kids in an

75
00:06:31,039 --> 00:06:35,878
otherwise fine paper published at his
neck security 2000 and for the authors

76
00:06:35,879 --> 00:06:39,949
included a short section on usability
complete with a drawing done by the

77
00:06:39,949 --> 00:06:43,569
four-year-old son of one of the authors
here it is and

78
00:06:43,569 --> 00:06:48,860
let me quote a bit from this paper they
wrote while a full usability test is

79
00:06:48,860 --> 00:06:53,449
beyond the scope of this paper we did
perform a very informal usability test

80
00:06:53,449 --> 00:06:59,439
Wesley van der Burgh age for created the
drawing in Figure 5 wesley is in many

81
00:06:59,439 --> 00:07:07,439
respects representative of potential end
users then the authors go on to say on a

82
00:07:07,439 --> 00:07:11,360
more serious note the window system
described here has been used in

83
00:07:11,360 --> 00:07:15,939
presentations to darba without
difficulty are noticeable interactive

84
00:07:15,939 --> 00:07:22,180
performance deficiencies very ago we've
tested it now to be fair this paper was

85
00:07:22,180 --> 00:07:28,490
not about usability but the authors did
have a section on usability in the paper

86
00:07:28,490 --> 00:07:32,610
I think what would have been better in
this case would have been to talk about

87
00:07:32,610 --> 00:07:37,169
usability in their limitations section
and just acknowledge that they hadn't

88
00:07:37,169 --> 00:07:40,998
really done a usability study but
they're doing one would be useful for

89
00:07:40,999 --> 00:07:48,709
future work so in general it is really
pretty difficult to extrapolate on

90
00:07:48,709 --> 00:07:52,479
usability based on the experiences of
yourself your office mates and your

91
00:07:52,479 --> 00:07:59,558
children you tend not to be a typical
user and i know i am not a typical user

92
00:07:59,559 --> 00:08:04,779
speaking for myself and anytime I'm
involved in doing a user study I always

93
00:08:04,779 --> 00:08:08,699
learn a lot of things and I'm very
surprised when I see what other people

94
00:08:08,699 --> 00:08:09,809
do with the system

95
00:08:09,809 --> 00:08:14,939
things that I would never have thought
to do or impressions ideas mental model

96
00:08:14,939 --> 00:08:19,539
that they have that don't really
coincide with my own so many show you an

97
00:08:19,539 --> 00:08:23,789
example from one of the first user study
said I did back in two thousand and two

98
00:08:23,789 --> 00:08:32,399
so I had built a piece of software
called privacy bird and this was an icon

99
00:08:32,399 --> 00:08:38,509
that sat in the corner of your web
browser and the the purpose of this

100
00:08:38,509 --> 00:08:43,479
software was to notify people as to
whether websites match their personal

101
00:08:43,479 --> 00:08:48,290
privacy preferences so I did a user
study where people came into the lab and

102
00:08:48,290 --> 00:08:52,380
I showed them a picture of these two
icons and they weren't labeled they were

103
00:08:52,380 --> 00:08:53,880
just the icons

104
00:08:53,880 --> 00:08:58,910
I asked them if they could tell me what
they thought the icons might mean I

105
00:08:58,910 --> 00:09:03,370
asked Ms after I explained to them that
the purpose of the software was to

106
00:09:03,370 --> 00:09:09,140
evaluate privacy policies so the
participants all looked at it and some

107
00:09:09,140 --> 00:09:13,670
of them are very confident when they
told me that Greenberg with the music

108
00:09:13,670 --> 00:09:16,540
that must mean that you're visiting a
website where you can download music

109
00:09:16,540 --> 00:09:23,560
files and the Redbird obviously that's
for websites that have a lot of foul

110
00:09:23,560 --> 00:09:29,300
language and you shouldn't let your kids
go there more than one person said this

111
00:09:29,300 --> 00:09:35,770
like more than five people said this and
I would never have occurred to me until

112
00:09:35,770 --> 00:09:39,579
they said it that this is how other
people would you these icons so

113
00:09:39,580 --> 00:09:45,700
subsequently we redesign the icons but
this is the sort of thing that until you

114
00:09:45,700 --> 00:09:49,870
actually put it in front of people who
are not you you don't get those kind of

115
00:09:49,870 --> 00:09:55,870
observations ok so let's say you want to
do a user study what are the steps

116
00:09:55,870 --> 00:10:03,630
involved in doing a user study so the
first thing is to identify the research

117
00:10:03,630 --> 00:10:07,689
question that you are trying to answer
why are you actually doing this user

118
00:10:07,690 --> 00:10:12,260
study and you need to be actually fairly
specific what exactly are you trying to

119
00:10:12,260 --> 00:10:17,760
measure here what are the use cases that
are important are you trying to find out

120
00:10:17,760 --> 00:10:22,850
whether any general person can use this
particular piece of software or do you

121
00:10:22,850 --> 00:10:28,810
expect users to be a particular class of
people senior citizens children maybe

122
00:10:28,810 --> 00:10:32,219
this is actually for security
professionals it so you need to

123
00:10:32,220 --> 00:10:38,680
understand what it is that you're trying
to find out then you need to decide on

124
00:10:38,680 --> 00:10:44,219
the type of study that you're going to
do and design a study protocol so study

125
00:10:44,220 --> 00:10:49,400
maybe just a series of interviews it may
be a survey it may be a lab study where

126
00:10:49,400 --> 00:10:54,240
participants are actually conducting
tests it may be an online study it may

127
00:10:54,240 --> 00:10:58,730
be a field study where people are using
a piece of software system in their own

128
00:10:58,730 --> 00:11:02,089
environments

129
00:11:02,089 --> 00:11:08,249
then you have to develop the detailed
surveys the scripts of exactly what

130
00:11:08,249 --> 00:11:12,439
you're gonna say two participants during
a study the prototypes that you're going

131
00:11:12,439 --> 00:11:18,110
to test the announcement you're going to
use to recruit participants all of the

132
00:11:18,110 --> 00:11:25,360
detailed artifacts that you need to
actually carry out a study then you have

133
00:11:25,360 --> 00:11:29,709
to get ethics approval if you're at a
university that means getting approval

134
00:11:29,709 --> 00:11:33,859
from your institutional review board
some companies now have a higher bees as

135
00:11:33,860 --> 00:11:37,350
well and there's a whole complicated
process there and I could give a whole

136
00:11:37,350 --> 00:11:41,040
nother talk about that that's that's
that's about all I'm going to say about

137
00:11:41,040 --> 00:11:46,910
IRB for now other than that it is an
important step and it's important step

138
00:11:46,910 --> 00:11:53,850
to leave time for in the process of
doing a study

139
00:11:53,850 --> 00:11:58,949
you pilot and iterate on the design I
never get a user study design right

140
00:11:58,949 --> 00:12:04,040
hundred-percent the first time even
after you have written a survey that you

141
00:12:04,040 --> 00:12:08,679
think like this is a great service the
questions all make perfect sense and

142
00:12:08,679 --> 00:12:11,689
then you pile it and you discover that
there is a question on there that your

143
00:12:11,689 --> 00:12:15,899
participants didn't really understand it
didn't know how to answer so sometimes

144
00:12:15,899 --> 00:12:19,720
it's small tweaks sometimes when you
pilot you end up completely redesigning

145
00:12:19,720 --> 00:12:29,149
your study finally you collect data and
then you analyze the results and that

146
00:12:29,149 --> 00:12:34,120
may be a quantitative analysis or maybe
a qualitative analysis or some

147
00:12:34,120 --> 00:12:40,790
combination and then you often end up
repeating some or all of these steps as

148
00:12:40,790 --> 00:12:45,279
needed depending on how things went so
that's the overview of doing a study and

149
00:12:45,279 --> 00:12:50,279
I'm gonna show you a number of different
studies that we did and I'm not going to

150
00:12:50,279 --> 00:12:53,920
walk through all of these steps for all
of these studies because there's not

151
00:12:53,920 --> 00:12:58,579
time but I'm going to highlight some of
the interesting parts of us are going to

152
00:12:58,579 --> 00:13:04,120
highlight some of the challenges that we
face in doing usable security studies

153
00:13:04,120 --> 00:13:11,120
and you know a lot of these challenges
can can come up doing really any kind of

154
00:13:11,120 --> 00:13:15,490
user study but but these come up I think
even more

155
00:13:15,490 --> 00:13:20,010
when dealing with usable security
studies so a big one with usable

156
00:13:20,010 --> 00:13:24,500
security studies is that we wanted to be
realistic we wanted to have its own a

157
00:13:24,500 --> 00:13:28,959
technological validity and insecurity
studies that often means testing a

158
00:13:28,959 --> 00:13:35,229
system in the face of risk or an
adversary or an attacker some sort so

159
00:13:35,230 --> 00:13:39,959
you may need to make your users believe
that something is actually at risk

160
00:13:39,959 --> 00:13:44,170
there's a reason why they need to
protect security but the same time you

161
00:13:44,170 --> 00:13:49,260
can't ethically put your users at any
greater risk than they normally would be

162
00:13:49,260 --> 00:13:56,260
and so this can be tricky to actually
carry out you need to be careful that

163
00:13:56,260 --> 00:13:59,819
the incentive structure that they have
in the in the study

164
00:13:59,820 --> 00:14:04,620
matches their incentives in real life
and that they're not biased it's very

165
00:14:04,620 --> 00:14:08,670
easy to bias people by feeling safe
because they're actually in a study

166
00:14:08,670 --> 00:14:12,550
environment and you know the this nice
person running the study is not gonna

167
00:14:12,550 --> 00:14:17,329
let anything bad happen to me or they
may be extra aware of certain things you

168
00:14:17,330 --> 00:14:21,920
it's a privacy study I'm going to tell
them how much I like privacy you can you

169
00:14:21,920 --> 00:14:26,510
have to be careful about those sorts of
things you have to make sure that you're

170
00:14:26,510 --> 00:14:31,069
measuring the right thing there have
been many studies that we've done that

171
00:14:31,070 --> 00:14:35,890
we get to the end of the study we
collected our data and then we say no we

172
00:14:35,890 --> 00:14:39,360
didn't actually measure we really needed
to measure to get to the answer our

173
00:14:39,360 --> 00:14:44,020
question and answer it I'll show you
some examples of some of those things

174
00:14:44,020 --> 00:14:48,089
but it's important to really think about
you know i i want to know that this is

175
00:14:48,089 --> 00:14:52,839
usable what do we mean usable so for
example if you're evaluating

176
00:14:52,839 --> 00:14:58,220
authentication system do you want to
measure how easy it is for people to

177
00:14:58,220 --> 00:15:03,360
authenticate or how easy it is for them
to enroll in the system or both in some

178
00:15:03,360 --> 00:15:09,170
cases it may be much more important that
the routine authentication is easy and

179
00:15:09,170 --> 00:15:13,620
in other cases it may be the enrollment
that's really the critical part so there

180
00:15:13,620 --> 00:15:19,100
are a lot of different things that you
could measure it's also important that

181
00:15:19,100 --> 00:15:24,670
you can control the variables so if you
have you know this realistic real-world

182
00:15:24,670 --> 00:15:28,640
element the real world is actually kind
of variable

183
00:15:28,640 --> 00:15:33,640
and sometimes you have to actually take
extra steps to make sure that you that

184
00:15:33,640 --> 00:15:37,189
you don't have too many things changing
out from under you so that you know what

185
00:15:37,190 --> 00:15:42,050
you're actually able to what what is
being held constant and and what what is

186
00:15:42,050 --> 00:15:46,479
changing and you need to make sure that
you have instrumented things so that you

187
00:15:46,480 --> 00:15:50,800
can collect your data we've definitely
done studies where we collect a lot of

188
00:15:50,800 --> 00:15:54,380
data and then discover that we had an
instrumented everything that we wanted

189
00:15:54,380 --> 00:15:58,210
to instrument and so we didn't actually
get some of the critical pieces of data

190
00:15:58,210 --> 00:16:05,190
that we needed another issue that comes
up in in usable security studies is that

191
00:16:05,190 --> 00:16:09,770
sometimes what we're trying to measure
is related to events that don't happen

192
00:16:09,770 --> 00:16:15,860
very often that actually usually the
world is fairly safe and that the

193
00:16:15,860 --> 00:16:20,410
security problem is not the regular
event it's actually the occasional event

194
00:16:20,410 --> 00:16:25,870
and so observing that event may be
difficult in a real-world scenario and

195
00:16:25,870 --> 00:16:29,970
that may require running a study over a
long period of time or with a lot of

196
00:16:29,970 --> 00:16:36,010
users so that you have enough samples of
the interesting event finally there are

197
00:16:36,010 --> 00:16:40,110
legal ethical and practical issues and
there are lots and lots of them I'm

198
00:16:40,110 --> 00:16:43,660
going to highlight a few examples of
these things that come up and I'm sure

199
00:16:43,660 --> 00:16:47,510
they come up actually in all kinds of
user studies but in security and privacy

200
00:16:47,510 --> 00:16:52,010
studies I think there are some extra
issues that come up in this category

201
00:16:52,010 --> 00:16:59,319
right so let's start with evaluating
security warnings I'm gonna talk about

202
00:16:59,320 --> 00:17:03,310
for security warning studies that I've
conducted with my students and

203
00:17:03,310 --> 00:17:07,448
colleagues and these illustrates several
of the challenges that I just talked

204
00:17:07,449 --> 00:17:12,030
about including how to maintain
ecological validity measuring the right

205
00:17:12,030 --> 00:17:17,940
thing and a few legal and practical
issues so I'm sure you're all familiar

206
00:17:17,940 --> 00:17:23,010
with browser security dialogues they all
look like this you don't need to read it

207
00:17:23,010 --> 00:17:27,720
because you already know that this is
what it says

208
00:17:27,720 --> 00:17:32,059
and we've all gotten used to just
swatting these warning dialog away

209
00:17:32,059 --> 00:17:34,330
without reading them

210
00:17:34,330 --> 00:17:43,689
fact I've done it quite a bit myself in
2007 we conducted a study of web browser

211
00:17:43,690 --> 00:17:49,570
fishing warnings and serious right here
was one of the co-authors and Jason home

212
00:17:49,570 --> 00:17:54,639
with the other one we were interested in
evaluating the web browser fishing

213
00:17:54,640 --> 00:18:00,409
warnings for the web browsers of the
time so we wanted to know how well they

214
00:18:00,409 --> 00:18:06,539
were working but we also were interested
in understanding the root causes for why

215
00:18:06,539 --> 00:18:13,400
people seem to be ignoring these
warnings are big challenge here was that

216
00:18:13,400 --> 00:18:18,320
we wanted to observe users interacting
with these warnings but we didn't want

217
00:18:18,320 --> 00:18:21,760
them to know that the reason we were
watching them is we wanted to see them

218
00:18:21,760 --> 00:18:26,510
interacting with the morning so we had
to have a Russo that they they would

219
00:18:26,510 --> 00:18:30,309
think we are watching them for some
other reason we also had to make them

220
00:18:30,309 --> 00:18:35,750
feel like they were actually involved in
a phishing attack you're actually under

221
00:18:35,750 --> 00:18:39,419
attack that there was actually something
address but we couldn't actually put

222
00:18:39,419 --> 00:18:45,039
them at risk so this required some
deception so we did a study on online

223
00:18:45,039 --> 00:18:48,240
shopping within our lab and we asked
people to purchase paper clips from

224
00:18:48,240 --> 00:18:52,559
amazon.com paper clips are good because
they were inexpensive him so he could

225
00:18:52,559 --> 00:19:00,539
afford to have a lot of people come to
our 11 by paper clips they made the

226
00:19:00,539 --> 00:19:04,129
purchase and then after they finish
making the purchase

227
00:19:04,130 --> 00:19:08,289
we gave them a survey about online
shopping this is actually a real survey

228
00:19:08,289 --> 00:19:11,129
about online shopping that some people
in our business goals we're actually

229
00:19:11,130 --> 00:19:17,850
doing that's when we fished them while
they were filling out the survey we sent

230
00:19:17,850 --> 00:19:22,539
them an email that looks like it came
from amazon.com telling them that there

231
00:19:22,539 --> 00:19:29,260
was a problem with their order after
they finished filling out the survey we

232
00:19:29,260 --> 00:19:32,980
told them to check their email so they
could get the receipt for the order and

233
00:19:32,980 --> 00:19:37,659
printed out that we could reimburse them
for their paper clips and that's when

234
00:19:37,659 --> 00:19:39,600
they fell for it

235
00:19:39,600 --> 00:19:44,230
so this is what the email look like in
the key part says please approve this

236
00:19:44,230 --> 00:19:47,240
delay so that we can continue processing
your order

237
00:19:47,240 --> 00:19:50,380
note that if you haven't received your
approval by the end of business tomorrow

238
00:19:50,380 --> 00:19:55,360
the item will be canceled so this is an
urgent message to log into the website

239
00:19:55,360 --> 00:20:01,809
and the Euro we gave them was at
www.amazon.com net it was in plain text

240
00:20:01,809 --> 00:20:07,549
that wasn't hit and so quite clear that
it with Amazon accounts dotnet right so

241
00:20:07,549 --> 00:20:13,129
this was the study design we we started
doing the study was to piloting it and

242
00:20:13,130 --> 00:20:18,750
we ran into some issues first with
anti-phishing systems actually snagged

243
00:20:18,750 --> 00:20:24,549
our emails and so we had to actually do
what the fissures do try to figure out

244
00:20:24,549 --> 00:20:30,418
how to get our emails through we also
have this problem that Amazon discovered

245
00:20:30,419 --> 00:20:36,480
our Amazon accounts dotnet website and
contacted the CMU lawyers about that so

246
00:20:36,480 --> 00:20:42,020
we had to address that problem as well
but eventually we were able to conduct

247
00:20:42,020 --> 00:20:44,700
the study and it was successful

248
00:20:44,700 --> 00:20:48,169
fact we fished almost all of our
participants we're pretty pleased with

249
00:20:48,169 --> 00:20:54,659
ourselves we we we did fish a lot of
participants but we were able to measure

250
00:20:54,659 --> 00:20:58,520
a difference between conditions we found
that some of the browser fishing

251
00:20:58,520 --> 00:21:03,830
warnings were actually significantly
worse than others besides this

252
00:21:03,830 --> 00:21:08,090
quantitative results though we observed
a lot of interesting things by watching

253
00:21:08,090 --> 00:21:16,158
people interact with these warnings so
when people clicked on the link the

254
00:21:16,159 --> 00:21:18,520
warning pops up in their web browser

255
00:21:18,520 --> 00:21:23,360
one person said the address in the
browser with Amazon accounts that net

256
00:21:23,360 --> 00:21:26,979
which is a genuine address so what we
found is that people didn't understand

257
00:21:26,980 --> 00:21:31,700
domain names some of them thought Amazon
account that net sounds like Amazon must

258
00:21:31,700 --> 00:21:33,530
be the same thing

259
00:21:33,530 --> 00:21:37,178
other people didn't even look at the
domain name they had no idea that the

260
00:21:37,179 --> 00:21:41,429
domain name was something important that
they should be paying attention to in

261
00:21:41,429 --> 00:21:45,430
order to protect themselves from fishing

262
00:21:45,430 --> 00:21:50,590
we had some people who just trust that
the browser was going to keep them safe

263
00:21:50,590 --> 00:21:55,050
so they saw that the warning said that
it was not recommended to continue to

264
00:21:55,050 --> 00:21:59,300
the website but they told us things like
well it gave me the option of still

265
00:21:59,300 --> 00:22:05,260
proceeding to the website so I figured
it couldn't be that bad and then one of

266
00:22:05,260 --> 00:22:08,930
the most interesting observations I
think was that we saw people had some

267
00:22:08,930 --> 00:22:16,260
confused mental models so we saw
multiple people who when the browser

268
00:22:16,260 --> 00:22:20,560
warning popped up they close the warning
then they went back to their email click

269
00:22:20,560 --> 00:22:24,560
on the link again the browser warning
popped up again they closed it repeat

270
00:22:24,560 --> 00:22:31,350
repeat I remember when that happened and
shares who had been running the studies

271
00:22:31,350 --> 00:22:35,050
and he came in he said I am observing
this really strange behavior and I don't

272
00:22:35,050 --> 00:22:39,050
fully understand what's going on here
until we talked about it and said well

273
00:22:39,050 --> 00:22:42,129
we're gonna have to ask people what
their thinking and so we added some

274
00:22:42,130 --> 00:22:47,280
questions in the debriefing surveyed the
end and what we found is that people

275
00:22:47,280 --> 00:22:52,139
were not associating this browser
warning with the email as far as they

276
00:22:52,140 --> 00:22:56,190
were concerned the warning had something
to do with either the web browser or the

277
00:22:56,190 --> 00:23:00,010
website it had nothing to do with the
email and so why wouldn't you go back to

278
00:23:00,010 --> 00:23:06,450
the email and try again and maybe the
website will be working this time so the

279
00:23:06,450 --> 00:23:11,380
good news is that this research both the
research that we did as well as research

280
00:23:11,380 --> 00:23:15,580
that other usable security researchers
have done has actually led to better

281
00:23:15,580 --> 00:23:18,679
fishing warnings and the fishing
warnings that are in the web browsers

282
00:23:18,680 --> 00:23:23,330
today I think in all the major browsers
are significantly better than what we

283
00:23:23,330 --> 00:23:28,480
tested back in 2007 I think it was doing
these sorts of studies that actually

284
00:23:28,480 --> 00:23:34,080
helped advance the state of the art in
this area

285
00:23:34,080 --> 00:23:38,110
certificate warnings are another story
there are also better than they used to

286
00:23:38,110 --> 00:23:45,080
be but arguably they're still somewhat
of an unsolved problem in 2008 we did a

287
00:23:45,080 --> 00:23:50,080
study to test certificate warnings and
tried to design a better warning this

288
00:23:50,080 --> 00:23:55,290
actually came out of a class project in
my usable privacy and security class and

289
00:23:55,290 --> 00:23:58,740
husband Josh you see here after the
class was over

290
00:23:58,740 --> 00:24:03,960
were they did some more work on this
along with theirs and we publish this

291
00:24:03,960 --> 00:24:12,520
2009 alright so the reason why SSL
certificate warnings are tricky is

292
00:24:12,520 --> 00:24:16,410
because it's often difficult to know
whether you're actually at risk when you

293
00:24:16,410 --> 00:24:21,510
see one of these warnings and this is
different than a lot of the warning that

294
00:24:21,510 --> 00:24:25,920
we see in the physical world so there
has some hazards that are always

295
00:24:25,920 --> 00:24:31,590
dangerous it is always dangerous to put
a baby in a coffee maker it is always

296
00:24:31,590 --> 00:24:36,100
dangerous to drink poison and it's
always dangerous to go we're walking an

297
00:24:36,100 --> 00:24:40,559
old abandoned mines these things these
things are charges you know if you want

298
00:24:40,559 --> 00:24:49,678
to avoid danger you should just not do
these sorts of things but there are

299
00:24:49,679 --> 00:24:54,220
other types of hazards that are context
dependent so drinking wine in moderation

300
00:24:54,220 --> 00:24:59,910
is not particularly hazardous for most
people however if you were driving a car

301
00:24:59,910 --> 00:25:04,700
then it is hazardous to be drinking wine
or hear pregnant then it is hazardous to

302
00:25:04,700 --> 00:25:09,970
be drinking wine so it is context
dependent now fortunately most wine

303
00:25:09,970 --> 00:25:14,630
drinkers are aware of whether they are
either pregnant or driving a car and so

304
00:25:14,630 --> 00:25:18,940
they can make an informed decision about
whether they want to assume this risk or

305
00:25:18,940 --> 00:25:27,559
not a computer security dialogues tend
to be more like alcohol warnings then

306
00:25:27,559 --> 00:25:32,460
like poison warnings rate their context
dependent but in the case of security

307
00:25:32,460 --> 00:25:35,660
dialogues it often actually really
difficult for the person who receives

308
00:25:35,660 --> 00:25:39,429
one of these warnings to know whether
there's a risk that they need to worry

309
00:25:39,429 --> 00:25:45,679
about so good morning would help users
here would help them determine whether

310
00:25:45,679 --> 00:25:50,280
they're actually at risk in the case
that there is something dangerous here

311
00:25:50,280 --> 00:25:56,309
it should actually prevent the user from
acting in a rescue way but if there's

312
00:25:56,309 --> 00:26:01,580
nothing dangerous going on it shouldn't
interfere in those sorts of contacts in

313
00:26:01,580 --> 00:26:05,699
so we need to test the warnings in both
are risky and an on risky context to

314
00:26:05,700 --> 00:26:10,700
make sure it works both ways so for the
study that we did we wanted to come up

315
00:26:10,700 --> 00:26:12,050
with two contexts

316
00:26:12,050 --> 00:26:15,530
it was pretty easy for us to come up
with an onerous he contacts because at

317
00:26:15,530 --> 00:26:21,230
the time the Carnegie Mellon library
website called cameo had a self-signed

318
00:26:21,230 --> 00:26:25,400
certificate and people at CMU were used
to every time you went to the library

319
00:26:25,400 --> 00:26:29,380
library website you get the certificate
warning and everybody knew you could

320
00:26:29,380 --> 00:26:33,940
just swatted away and nothing bad
whatever happened so that was our non

321
00:26:33,940 --> 00:26:41,080
risky context for a risky context we
thought well if people logged into their

322
00:26:41,080 --> 00:26:45,030
online bank account and they got a
warning that that should surprise them

323
00:26:45,030 --> 00:26:51,330
and they actually have something at risk
at their bank and so we decided that we

324
00:26:51,330 --> 00:26:55,889
would we would trigger a warning when
people log in their online bank so we

325
00:26:55,890 --> 00:26:59,800
created a task where we asked people to
check their bank account balance in our

326
00:26:59,800 --> 00:27:01,960
lab using our lab computer

327
00:27:01,960 --> 00:27:07,080
we'd put a web proxy on the lab computer
which he did a man in the mill middle

328
00:27:07,080 --> 00:27:15,220
attack and triggered the warning that
was our plan however the CMU lawyers

329
00:27:15,220 --> 00:27:20,810
said that this may or may not interfere
or conflict with the Pennsylvania State

330
00:27:20,810 --> 00:27:33,780
wiretap laws and it might not be a legal
thing to do so we we did not do the

331
00:27:33,780 --> 00:27:38,760
experiment in this way because it
wouldn't let us and we had to come up

332
00:27:38,760 --> 00:27:42,810
with another way that we could achieve
what we wanted to achieve we we didn't

333
00:27:42,810 --> 00:27:46,629
really need to do a manual attack we
just needed to make the warning popup

334
00:27:46,630 --> 00:27:52,340
and so what we ended up doing with
removing the root certificate from the

335
00:27:52,340 --> 00:27:57,459
browser and once the retreat of it was
removed the web site you type it

336
00:27:57,460 --> 00:28:01,900
couldn't be verified and any visit to a
secure site where trigger warnings and

337
00:28:01,900 --> 00:28:06,810
so this made the lawyers happy and we
were able to proceed with actually doing

338
00:28:06,810 --> 00:28:12,879
the study we had some other challenges
we had to overcome with doing this is a

339
00:28:12,880 --> 00:28:17,460
lab study so because it was in our lab
were we're very concerned that

340
00:28:17,460 --> 00:28:23,260
participants would feel safe during the
study and

341
00:28:23,260 --> 00:28:27,530
we also felt that you know they're
they're mostly to get paid and so we

342
00:28:27,530 --> 00:28:30,730
give them some task they're just going
to try to get through the test as fast

343
00:28:30,730 --> 00:28:31,700
as they can

344
00:28:31,700 --> 00:28:34,980
that's going to be their priority so we
needed to find a way of constructing

345
00:28:34,980 --> 00:28:41,350
task so that people would feel like
there if they if they felt unsafe they

346
00:28:41,350 --> 00:28:44,610
felt this was risky that they could
still complete the task and get paid

347
00:28:44,610 --> 00:28:51,080
without taking the risk so we thought
this so that there would be easy

348
00:28:51,080 --> 00:28:55,210
alternative test the refrain the studies
and information seeking study there were

349
00:28:55,210 --> 00:28:59,100
four pieces of information they had to
seek to the two that we cared about

350
00:28:59,100 --> 00:29:03,300
where they had to find their their the
last two digits of their bank account

351
00:29:03,300 --> 00:29:08,159
balance and they had to look up some
information the CMU library and then we

352
00:29:08,160 --> 00:29:11,720
had two other tests just to kind of had
it out and make it less obvious what

353
00:29:11,720 --> 00:29:16,160
we're doing and two for each task they
could do it either on the computer we

354
00:29:16,160 --> 00:29:20,220
provided or on the telephone we provided
and so for each of them we provided

355
00:29:20,220 --> 00:29:27,100
instructions including URLs and phone
numbers for how to complete the task so

356
00:29:27,100 --> 00:29:32,439
what we're interested in was for the
library task how many people completed

357
00:29:32,440 --> 00:29:36,340
it using the computer versus how many of
them ended up calling the library and

358
00:29:36,340 --> 00:29:41,909
for the bank account cast the same thing
how many of them use the computer versus

359
00:29:41,910 --> 00:29:47,760
use the telephone banking so what
happened we had a hundred users who came

360
00:29:47,760 --> 00:29:54,530
to our lab for the study we tested
Firefox to Firefox 3 i7 and then to new

361
00:29:54,530 --> 00:29:58,860
warnings we had come up with ourselves
that we had hoped would be better than

362
00:29:58,860 --> 00:30:04,570
the existing browser warnings in a
nutshell what we found was the i seven

363
00:30:04,570 --> 00:30:09,520
and Firefox users ignored all the
warnings they ignored the warnings at

364
00:30:09,520 --> 00:30:12,870
the library and they ignored the
warnings on their online bank account

365
00:30:12,870 --> 00:30:15,699
and they just went through anyway

366
00:30:15,700 --> 00:30:19,880
Firefox today was a really interesting
case Firefox 3 had just come out and if

367
00:30:19,880 --> 00:30:23,520
you wanted to override the morning it
was a four-step process and none of our

368
00:30:23,520 --> 00:30:28,030
users knew how to do it yet they would
learn but this was the first time most

369
00:30:28,030 --> 00:30:31,830
of them had seen this and they didn't
know how to override the morning and so

370
00:30:31,830 --> 00:30:35,439
most of them couldn't do it and they
gave up

371
00:30:35,440 --> 00:30:40,870
and used the phone for both the bank
account and the library so that's not an

372
00:30:40,870 --> 00:30:44,969
effective warning either the new
warnings that we do we designed it the

373
00:30:44,970 --> 00:30:50,110
best buy if you look at the numbers you
know they're the winners however you

374
00:30:50,110 --> 00:30:54,199
look at the numbers would also see that
they were only effective for about half

375
00:30:54,200 --> 00:30:58,330
of our users so it was an improvement
over the other warnings but there were

376
00:30:58,330 --> 00:31:07,600
still have our users who would have
fallen for this attack so after this

377
00:31:07,600 --> 00:31:13,789
study we decide to continue on and do
some more warning studies so we did some

378
00:31:13,789 --> 00:31:14,220
work

379
00:31:14,220 --> 00:31:19,549
Cristian Bravo low who's shown here
Julie down so my colleague and other

380
00:31:19,549 --> 00:31:24,730
students are ongoing commentary and Rob
reader and Stewart schachter and another

381
00:31:24,730 --> 00:31:30,730
said Monday sleeper were involved in
these studies so Christian spent a lot

382
00:31:30,730 --> 00:31:34,259
of time looking at the different
elements of the warnings and looking at

383
00:31:34,259 --> 00:31:40,899
what contribution each each element of
the warning made to being an effective

384
00:31:40,899 --> 00:31:46,639
warning and he focused on the part of
the warning that gives the user the

385
00:31:46,639 --> 00:31:51,620
critical information they need to make a
decision and he wondered how can we make

386
00:31:51,620 --> 00:31:56,629
users pay attention to that critical
piece of information so here's an

387
00:31:56,629 --> 00:32:02,830
example here are two warnings one of us
but nine and 12 suspicious take a look

388
00:32:02,830 --> 00:32:09,379
raise your hand if you think you can see
which is a suspicious one which one the

389
00:32:09,379 --> 00:32:17,189
one on the right correct and why is that
the publisher exactly so the publisher

390
00:32:17,190 --> 00:32:22,320
line is the key piece of information
here and if you know if you want if

391
00:32:22,320 --> 00:32:26,000
you're an expert and you look at it you
very quickly we'll see

392
00:32:26,000 --> 00:32:30,419
publisher but looks really suspicious
I'm not going to trust that that

393
00:32:30,419 --> 00:32:35,960
particular website so now that we know
that that's what experts do the question

394
00:32:35,960 --> 00:32:40,769
is how can we get everybody to look at
the publisher line in this morning and

395
00:32:40,769 --> 00:32:47,200
make an informed decision so we tried a
variety of different things but an area

396
00:32:47,200 --> 00:32:48,519
that we focused a lot on

397
00:32:48,519 --> 00:32:52,509
with the use of attractors so these are
user interface elements that attract

398
00:32:52,509 --> 00:32:58,159
your attention to a particular part of
the interface how we also looked at

399
00:32:58,159 --> 00:33:02,849
using a tractors in combination with
forcing a delay before you'd like people

400
00:33:02,849 --> 00:33:07,928
override and install so five-second
delay we also tried forcing an

401
00:33:07,929 --> 00:33:11,549
interaction with the warning and I'll
show you some examples of that and we

402
00:33:11,549 --> 00:33:15,589
also tried forcing people to actually
read the publisher name but this was a

403
00:33:15,589 --> 00:33:19,908
challenge how do you force somebody to
read something and i cant force my kids

404
00:33:19,909 --> 00:33:26,959
to read anything you make it but that is
all we did we meet with them type it

405
00:33:26,959 --> 00:33:33,879
back and you'll see that example so so
this is an example of one of our

406
00:33:33,879 --> 00:33:39,099
attractors this week all the ANSI
standard because it uses the colors that

407
00:33:39,099 --> 00:33:43,658
are an anti standard warning it's really
ugly but we thought it might attract

408
00:33:43,659 --> 00:33:48,959
people's attention here we have the
animated connector that we had to

409
00:33:48,959 --> 00:33:53,320
animate this yellow blob coming up and
surrounding the important part of the

410
00:33:53,320 --> 00:33:58,649
warning the slow reveal where the
publisher name is revealed slowly

411
00:33:58,649 --> 00:34:06,079
dramatic this one here requires you to
swipe your mouse over the publisher name

412
00:34:06,079 --> 00:34:11,710
at first we thought maybe that would
force people to read it but when we we

413
00:34:11,710 --> 00:34:14,859
built it and tried it we realized that
we did you get good at it you don't

414
00:34:14,859 --> 00:34:18,399
actually have to look at what your
swiping over and so people might not be

415
00:34:18,399 --> 00:34:22,270
reading it while swiping their mouse and
then we had this version where you

416
00:34:22,270 --> 00:34:27,770
actually had to retype the publisher
name and you know that was pretty

417
00:34:27,770 --> 00:34:33,629
annoying but we figured you have to read
it while you're retyping so we wanted to

418
00:34:33,629 --> 00:34:38,348
know whether any of these worked we
wanted to know whether they prevented

419
00:34:38,349 --> 00:34:44,859
the suspicious installs while not
preventing the benign installs and in

420
00:34:44,859 --> 00:34:48,520
the benign case since since many of
these detractors with slow people down

421
00:34:48,520 --> 00:34:52,409
we wanted to know how much were they
slowing people down what was the cost

422
00:34:52,409 --> 00:34:58,930
and in the case in the previous study we
had a hundred people come to our lab to

423
00:34:58,930 --> 00:35:01,960
test five conditions in this case we had
actually a lot

424
00:35:01,960 --> 00:35:06,050
more conditions we wanted to test and it
wasn't gonna be realistic to have

425
00:35:06,050 --> 00:35:10,770
hundreds or maybe even thousands of
people come to our lab and so we decided

426
00:35:10,770 --> 00:35:16,089
we needed to try to do something a bit
different and we wanted to find a way

427
00:35:16,089 --> 00:35:20,560
that we could recruit a lot of people
quickly that wouldn't cost too much

428
00:35:20,560 --> 00:35:26,279
money that we could observe what they
were doing what they were clicking on we

429
00:35:26,280 --> 00:35:33,250
also need to make sure that participants
would accurately feel the safety risk

430
00:35:33,250 --> 00:35:38,490
depending on the scenario and we also
need to not really put our participants

431
00:35:38,490 --> 00:35:46,209
at risk so we use Amazon Mechanical Turk
to recruit people for an online study so

432
00:35:46,210 --> 00:35:50,510
for those of you who are not familiar
with M Turk it's a great platform where

433
00:35:50,510 --> 00:35:56,330
you can recruit workers to do tasks that
take a few minutes an hour and you can

434
00:35:56,330 --> 00:36:03,150
pay them $10 a few dollars depending on
how long the task is an Amazon takes

435
00:36:03,150 --> 00:36:06,430
care of doing all of those payments so
you don't have to figure out how to get

436
00:36:06,430 --> 00:36:11,690
these small payments to hundreds or
thousands of people so in this study

437
00:36:11,690 --> 00:36:16,630
people came to our study were presented
with a consent form that inform them

438
00:36:16,630 --> 00:36:21,599
that they were going to be doing a study
on online games will be evaluating free

439
00:36:21,599 --> 00:36:25,960
online games they were given
instructions first they were told they

440
00:36:25,960 --> 00:36:28,440
would be assigned to their game number
one

441
00:36:28,440 --> 00:36:33,830
mars buggy on mine and we told them the
website whose URL appears above its

442
00:36:33,830 --> 00:36:38,109
external to the study by researchers do
not control its contents right this was

443
00:36:38,109 --> 00:36:43,049
our our way of making sure that they
didn't say this website is part of the

444
00:36:43,050 --> 00:36:46,770
study so I should feel safe here we
wanted them to know that they were going

445
00:36:46,770 --> 00:36:53,099
to external websites for this so they
would go they'd play mars buggy which

446
00:36:53,099 --> 00:36:58,580
was a real game that we did not make up
more control than they've answered some

447
00:36:58,580 --> 00:37:01,740
questions the first question was very
important we asked them if they were

448
00:37:01,740 --> 00:37:05,609
able to play the game and more
importantly we were informing them with

449
00:37:05,609 --> 00:37:09,009
the answer choices that if there were
some reason they couldn't play the game

450
00:37:09,010 --> 00:37:12,560
it wasn't a problem we would just give
them another game to

451
00:37:12,560 --> 00:37:18,170
so we wanted them to know this then they
entered a bunch of questions actually

452
00:37:18,170 --> 00:37:24,930
related to the game and then when they
were done they were assigned their next

453
00:37:24,930 --> 00:37:29,960
game so game number two the Tom and
Jerry refrigerator a game looks like fun

454
00:37:29,960 --> 00:37:37,180
for some questions about that then they
get a sign in number three and when they

455
00:37:37,180 --> 00:37:41,790
get there they're told that they
actually have to download Silverlight in

456
00:37:41,790 --> 00:37:47,360
order to play this game and they get a
warning depending on what condition

457
00:37:47,360 --> 00:37:50,990
they're in they saw warnings with
different tractors and also either

458
00:37:50,990 --> 00:37:56,750
benign or suspicious so this is an
example of a benign warning and here's

459
00:37:56,750 --> 00:38:04,480
an example of a suspicious warning right
we had over 2,000 people who

460
00:38:04,480 --> 00:38:10,030
participated in the study online who
actually made it through to the third

461
00:38:10,030 --> 00:38:16,520
game and encountered these dialogues in
the benign scenario we found that with

462
00:38:16,520 --> 00:38:21,980
most of our different formats of
dialogues the the installation was not

463
00:38:21,980 --> 00:38:23,670
prevented so that's good

464
00:38:23,670 --> 00:38:30,190
basically the 930 the warnings did not
get in the way in some cases it slowed

465
00:38:30,190 --> 00:38:34,360
people down they had to wait or they had
to type the name but it they did not get

466
00:38:34,360 --> 00:38:39,800
in the way in the suspicious scenario we
found that most of the new dialogue that

467
00:38:39,800 --> 00:38:44,860
we tried actually did reduce the number
of installations significantly and that

468
00:38:44,860 --> 00:38:50,200
some of them were particularly good
swiping typing and adding delay were

469
00:38:50,200 --> 00:38:53,399
particularly effective but there were a
lot of them that they were pretty good

470
00:38:53,400 --> 00:39:00,750
so this seemed like success but we
wondered maybe the effect we're seeing

471
00:39:00,750 --> 00:39:04,860
here is that this is the first time
people saw these dialogues and so it was

472
00:39:04,860 --> 00:39:09,230
it was novel and they slow down and they
paid attention what would happen if

473
00:39:09,230 --> 00:39:14,110
these dialogues were normal and every
time they got a browser warning they got

474
00:39:14,110 --> 00:39:18,750
a dialogue that was a red and black or
and that required you to swipe over it

475
00:39:18,750 --> 00:39:25,490
or whatever and so we thought yeah you
know we can't we can't claim victory

476
00:39:25,490 --> 00:39:32,810
we need to see what would happen in the
face of habituation so we wanted to do

477
00:39:32,810 --> 00:39:36,150
an experiment where people would see
this dialogue over and over and over

478
00:39:36,150 --> 00:39:40,820
again but it was really hard to think
how we would make that realistic

479
00:39:40,820 --> 00:39:46,020
how many different game websites what
they have to go to and encounter these

480
00:39:46,020 --> 00:39:50,250
dialogues and this would take a really
long time because we didn't just want it

481
00:39:50,250 --> 00:39:53,750
like two or three you know we wanted
them to encounter these dialogues like

482
00:39:53,750 --> 00:40:01,470
20 times at least and so we decided to
change the approach to the experiment

483
00:40:01,470 --> 00:40:06,779
and tell them that we were doing a study
that was about swatting away dialogues

484
00:40:06,780 --> 00:40:12,200
and the idea here is that if we could
have people spend a few minutes swatting

485
00:40:12,200 --> 00:40:16,560
away dialogues and then after a few
minutes changed the dialogue we could

486
00:40:16,560 --> 00:40:20,700
test to see whether they noticed the
change dialogue and that would be a test

487
00:40:20,700 --> 00:40:23,100
of habituation

488
00:40:23,100 --> 00:40:26,710
so we did this we we'd had nine
different types of warnings that we

489
00:40:26,710 --> 00:40:31,050
experimented with and we had eight
hundred and seventy-two people do the

490
00:40:31,050 --> 00:40:35,619
study so in this case we tell them
directly your task is to respond to as

491
00:40:35,619 --> 00:40:42,270
many dialogues as you can before the
timer goes off and we tell them that

492
00:40:42,270 --> 00:40:48,220
they'll have an opportunity to finish
the study early and still get paid for

493
00:40:48,220 --> 00:40:53,709
this is what it looked like they they
see this dialogue it has a status bar

494
00:40:53,710 --> 00:40:59,190
that says you have now dismissed 0 of
these pop-up windows and then the

495
00:40:59,190 --> 00:41:03,270
question would you like to see another
popup window if they say no it doesn't

496
00:41:03,270 --> 00:41:08,570
let them say no so you have to say yes
right and so they very quickly get used

497
00:41:08,570 --> 00:41:13,790
to I just have to click Yes thats status
changed a little bit you know it's a

498
00:41:13,790 --> 00:41:21,200
scene 01 you nothing to moved around the
screen and then at some point the status

499
00:41:21,200 --> 00:41:27,140
changed and said press the no option
below to finish the study early and at

500
00:41:27,140 --> 00:41:31,500
that point we hope that people would
notice it and if they click know it

501
00:41:31,500 --> 00:41:35,940
would work and they could lead the study
if they didn't click know they'd have

502
00:41:35,940 --> 00:41:38,530
another opportunity click know they had
a few opportunity

503
00:41:38,530 --> 00:41:43,920
when they finally click now our time ran
out then they had a survey what we're

504
00:41:43,920 --> 00:41:47,990
most interested in here is how many
people click know the first time they

505
00:41:47,990 --> 00:41:52,779
had the opportunity to do so and so
that's the green bars you see here so

506
00:41:52,780 --> 00:41:56,550
you can see it actually varies quite a
bit depending on which condition people

507
00:41:56,550 --> 00:42:00,780
were in but there were several
conditions where a large fraction of

508
00:42:00,780 --> 00:42:06,290
people did click know right away but
they're also the control condition and

509
00:42:06,290 --> 00:42:10,890
like the ANSI condition which had done
pretty well when people only saw the

510
00:42:10,890 --> 00:42:18,990
dialogue once really didn't do well at
all in the face of habituation so this

511
00:42:18,990 --> 00:42:27,770
this was a pretty promising results and
this was published at 2013 and actually

512
00:42:27,770 --> 00:42:34,340
one thing which paper award but there is
one problem with the study that we

513
00:42:34,340 --> 00:42:39,720
wanted to revisit we wanted to be able
to claim that our best detractors here

514
00:42:39,720 --> 00:42:44,140
we're actually eliminating or reducing
habituation and we couldn't make that

515
00:42:44,140 --> 00:42:50,370
claim because we had only tested them in
the face of habituation we had we didn't

516
00:42:50,370 --> 00:42:55,200
have the comparison in the same study
without habituation so we couldn't make

517
00:42:55,200 --> 00:43:00,029
a stronger claim as you wanted to make
so we ran another study where instead of

518
00:43:00,030 --> 00:43:04,320
having everybody click these warnings
for several minutes

519
00:43:04,320 --> 00:43:09,070
some people saw the warning only once
some people saw it three times twenty

520
00:43:09,070 --> 00:43:14,890
times or a hundred and fifty seconds of
exposure and so this allowed us to see

521
00:43:14,890 --> 00:43:21,990
over time how well people could could
comply with with the request the press

522
00:43:21,990 --> 00:43:27,790
the no button so here's what happened so
in the control and Nancy condition you

523
00:43:27,790 --> 00:43:32,850
see that even when they've seen it only
once only about half the people are

524
00:43:32,850 --> 00:43:37,520
complying with the request and then as
they see it more times it's falling off

525
00:43:37,520 --> 00:43:44,160
and we're really not doing well at all
once we've had a lot of exposures

526
00:43:44,160 --> 00:43:48,230
in some of our other conditions to
reveal the animated connector with a

527
00:43:48,230 --> 00:43:52,380
delay we start out a lot higher we have
about eighty percent of the people who

528
00:43:52,380 --> 00:43:56,920
noticed the first time at the beginning
but again it's dropping off its not

529
00:43:56,920 --> 00:44:03,230
dropping off as badly as the red lines
but it still dropping off the swiping

530
00:44:03,230 --> 00:44:06,839
type conditions something different is
happening here they're not dropping off

531
00:44:06,839 --> 00:44:11,020
and in some cases are actually been
improving a little bit and so what we

532
00:44:11,020 --> 00:44:17,759
see here is that swype and type actually
do seem to be resilient to habituation

533
00:44:17,760 --> 00:44:22,289
so this was a really interesting finding
and something that we could not have

534
00:44:22,289 --> 00:44:26,910
predicted using only the results of our
previous experiment we actually had to

535
00:44:26,910 --> 00:44:32,069
go and do another experiment in order to
be able to make this sort of a claim

536
00:44:32,069 --> 00:44:39,750
alright so I want to talk a little bit
about privacy and in particular privacy

537
00:44:39,750 --> 00:44:45,619
indicators and how we can study the
effect of privacy indicators and

538
00:44:45,619 --> 00:44:52,190
people's willingness to pay for privacy
so there have been many surveys which

539
00:44:52,190 --> 00:44:56,859
show that people value privacy a lot
they come out every year

540
00:44:56,859 --> 00:45:01,640
multiple times a year however we also
all the time see lots of examples of

541
00:45:01,640 --> 00:45:06,319
people voluntarily giving up their
privacy are doing things that are

542
00:45:06,319 --> 00:45:12,329
basically thwarting their own privacy
and there's been a lot of question of

543
00:45:12,329 --> 00:45:17,809
whether people really value privacy as
much as they say they do but one reason

544
00:45:17,809 --> 00:45:22,039
that we have this apparent contradiction
maybe that protecting privacy is

545
00:45:22,039 --> 00:45:27,140
actually hard to do and it's often hard
to figure out what is the path that will

546
00:45:27,140 --> 00:45:31,868
allow you to protect your privacy so we
wondered what if we could make privacy

547
00:45:31,869 --> 00:45:37,319
protection really easy in a particular
domain what people actually protect

548
00:45:37,319 --> 00:45:41,950
their privacy and how much would they be
willing to go out of their way to do it

549
00:45:41,950 --> 00:45:46,970
in a good way to measure that is in
terms of money how much would they pay

550
00:45:46,970 --> 00:45:53,990
to protect their privacy so we built a
search engine called privacy finder that

551
00:45:53,990 --> 00:45:57,629
was based on that privacy birds offer
that showed you

552
00:45:57,630 --> 00:46:02,799
beginning and the idea here is that you
do a search and initially I think you

553
00:46:02,799 --> 00:46:08,349
could do it on google or yahoo and in
addition to your search results you saw

554
00:46:08,349 --> 00:46:13,170
a privacy meter down the left side that
gave you some indicator of how good the

555
00:46:13,170 --> 00:46:19,970
privacy policy was at each other
websites so we thought well with that

556
00:46:19,970 --> 00:46:23,828
privacy meter it should be really easy
to identify the sites with the good

557
00:46:23,829 --> 00:46:29,660
privacy would people actually pay more
to shop at those websites so we've

558
00:46:29,660 --> 00:46:34,808
talked about how we could test this and
the first idea is that we do is study we

559
00:46:34,809 --> 00:46:38,309
invite people into the lab and we asked
them to do searches with our search

560
00:46:38,309 --> 00:46:43,109
engine and then we asked them will which
website where you shop out the problem

561
00:46:43,109 --> 00:46:46,140
with this is that it completely
hypothetical and you're not actually

562
00:46:46,140 --> 00:46:50,348
risking anything you're not actually
facing a trade off because it's it's all

563
00:46:50,349 --> 00:46:54,539
hypothetical so why wouldn't you say oh
I'm just gonna shop at the one with the

564
00:46:54,539 --> 00:47:02,160
best privacy policy so another way to do
it is to make it a real task where you

565
00:47:02,160 --> 00:47:05,700
ask them which one would you purchased
from and then you ask them to actually

566
00:47:05,700 --> 00:47:09,919
use their credit card and make a
purchase now the problem with that is

567
00:47:09,920 --> 00:47:13,529
that it's going to cost us more money to
do the study would have to pay people

568
00:47:13,529 --> 00:47:16,930
enough that they're willing to come and
make purchases and buy things that they

569
00:47:16,930 --> 00:47:22,879
may not actually want it's also the case
that we're dealing with real websites

570
00:47:22,880 --> 00:47:26,460
and they can real purchases that they're
gonna be a lot of variables that we may

571
00:47:26,460 --> 00:47:32,490
not be able to control but we decided to
give it a tried and between 2005 and

572
00:47:32,490 --> 00:47:39,649
2009 we did a number of these studies
genocide in surgical men were involved

573
00:47:39,650 --> 00:47:45,579
along with my colleague Alessandro
acquisti so the first of these studies

574
00:47:45,579 --> 00:47:51,059
with notable first for having a really
great name power strips prophylactics

575
00:47:51,059 --> 00:47:52,220
and privacy oh my

576
00:47:52,220 --> 00:47:57,140
but it also was a really interesting
study in one of the first studies to

577
00:47:57,140 --> 00:48:02,400
demonstrate that people might be willing
to pay for privacy so it was advertised

578
00:48:02,400 --> 00:48:06,819
as an online shopping study in our lab
we recruited 24 students for the study

579
00:48:06,819 --> 00:48:11,120
we paid them $10 + reimbursed for the
items that they perch

580
00:48:11,120 --> 00:48:15,430
gist they all use a search engine called
shopping finder

581
00:48:15,430 --> 00:48:19,919
half the participants saw privacy icons
in their search results

582
00:48:19,920 --> 00:48:23,490
half the participants did not see
privacy icons in the search results but

583
00:48:23,490 --> 00:48:27,779
they otherwise all the same search
results and there were two particular

584
00:48:27,780 --> 00:48:31,770
products react in the search for one
that was privacy sensitive and one that

585
00:48:31,770 --> 00:48:37,340
was not privacy sensitive so the
nonpartisan sensitive item with a six

586
00:48:37,340 --> 00:48:42,370
outlet surge suppressor power strip and
the privacy sensitive item was a box of

587
00:48:42,370 --> 00:48:48,920
Trojan condoms and these items were
selected mostly because they were within

588
00:48:48,920 --> 00:48:55,130
our price range and it with our budget
for the project and met our criteria and

589
00:48:55,130 --> 00:48:59,160
were available from a lot of websites
that we could find privacy policies for

590
00:48:59,160 --> 00:49:06,089
participants as I said had to use their
own credit cards to make the purchase

591
00:49:06,090 --> 00:49:10,000
but we gave them the option of shipping
items to our lab if they wanted to and

592
00:49:10,000 --> 00:49:16,400
as you can see a lot of them decided to
ship the items to us we had a lot of fun

593
00:49:16,400 --> 00:49:25,280
and arlen surgeries all that they
thought look something like this you can

594
00:49:25,280 --> 00:49:30,430
see the red and green bird icons that we
used in this this particular study we

595
00:49:30,430 --> 00:49:36,549
found that the privacy icons influence
the purchases so the people who were in

596
00:49:36,550 --> 00:49:40,240
the privacy icon condition were more
likely to make purchases from the

597
00:49:40,240 --> 00:49:42,370
website that had green birds next to
them

598
00:49:42,370 --> 00:49:47,220
people in the other condition were less
likely to make purchases from those same

599
00:49:47,220 --> 00:49:51,069
web sites that were not a door and with
green birds but were the same websites

600
00:49:51,070 --> 00:49:56,940
and we saw the effects seem to be larger
for privacy sensitive purchases but

601
00:49:56,940 --> 00:50:01,820
there are a lot of problems with the
study so first of all of our

602
00:50:01,820 --> 00:50:07,060
participants were students so not a very
diverse sample because we reimburse

603
00:50:07,060 --> 00:50:11,150
people for their purchases there's
really no incentive for them to save

604
00:50:11,150 --> 00:50:15,480
money we were gonna pay for whatever it
was that they purchased

605
00:50:15,480 --> 00:50:20,030
also the price and privacy tradeoff for
not very obvious because in order to

606
00:50:20,030 --> 00:50:24,310
find out how much the item cost city's
website they had to click through to the

607
00:50:24,310 --> 00:50:24,570
web

608
00:50:24,570 --> 00:50:28,760
site and then look up not only the price
but also the shipping costs and added

609
00:50:28,760 --> 00:50:39,060
together in order to make that
determination yeah that is true that

610
00:50:39,060 --> 00:50:44,170
predates the Angry Birds game

611
00:50:44,170 --> 00:50:53,250
good point so another a limitation that
we had was that we couldn't really be

612
00:50:53,250 --> 00:50:56,380
sure that people were purchasing from
the green bird websites because they

613
00:50:56,380 --> 00:51:03,720
like privacy maybe they just like green
birds and we also anecdotally thought

614
00:51:03,720 --> 00:51:08,140
that perhaps the privacy sensitive I
don't was not actually privacy sensitive

615
00:51:08,140 --> 00:51:12,870
enough especially with our group of
participants who were students there did

616
00:51:12,870 --> 00:51:19,150
not seem particularly fazed about buying
condoms so we decided to try the study

617
00:51:19,150 --> 00:51:25,380
again this time we use 72 Pittsburg
resident recruited from all over the

618
00:51:25,380 --> 00:51:31,880
city we helped make the price privacy
tradeoff more clear and I'll show you

619
00:51:31,880 --> 00:51:36,770
how we did that we used a fixed payment
and told them to keep the change so that

620
00:51:36,770 --> 00:51:41,110
they would have an incentive to save
money and we had new icons new products

621
00:51:41,110 --> 00:51:46,700
new conditions in order to pick the
product we did a pre study where we

622
00:51:46,700 --> 00:51:50,850
asked people to imagine that they were
participating in a user study at

623
00:51:50,850 --> 00:51:55,400
Carnegie Mellon and we're going to be
asked to purchase items in our lab and

624
00:51:55,400 --> 00:51:59,470
we asked them how concerned they would
be about purchasing a wide variety of

625
00:51:59,470 --> 00:52:04,540
items and you can see we got a wide
variety of responses so office supplies

626
00:52:04,540 --> 00:52:09,259
textbook flowers they had no problem
with bad when we started getting into

627
00:52:09,260 --> 00:52:18,830
items that were related to sex drugs
violence people have more concerns so we

628
00:52:18,830 --> 00:52:24,410
pick two points on this graph one
representing very little concern one

629
00:52:24,410 --> 00:52:28,910
representing a lot of concern but not so
much concern that we thought people

630
00:52:28,910 --> 00:52:35,779
would refuse to do it and so we picked
office supplies and sex toys from there

631
00:52:35,780 --> 00:52:37,760
we selected to particular I

632
00:52:37,760 --> 00:52:42,710
items items we selected were a four pack
of Duracell batteries and the pocket

633
00:52:42,710 --> 00:52:54,550
rocket junior vibrator and yes they were
shipped or lab we selected 10 merchants

634
00:52:54,550 --> 00:53:00,040
for each product and so when people ran
the search is the first 10 sort results

635
00:53:00,040 --> 00:53:04,220
were those merchants rather than
whatever they got four if anybody click

636
00:53:04,220 --> 00:53:08,029
beyond merchants that they would get
random or whatever cites the search

637
00:53:08,030 --> 00:53:13,780
engine would bring up we specifically
ordered the first for results

638
00:53:13,780 --> 00:53:19,590
the first result was the website that
had both the worst privacy policy and

639
00:53:19,590 --> 00:53:24,860
the cheapest price the fourth one had
the best privacy policy and the most

640
00:53:24,860 --> 00:53:29,320
expensive price the difference between
the cheapest and most expensive was

641
00:53:29,320 --> 00:53:34,400
about 69 cents I say about because this
was these are real companies and

642
00:53:34,400 --> 00:53:39,240
sometimes they change their prices in
the middle of the study we found that

643
00:53:39,240 --> 00:53:44,189
for the sex toys it was it was fairly
similar but actually a bigger price

644
00:53:44,190 --> 00:53:50,360
differential I think it's a little bit
over $1 this is what our interface looks

645
00:53:50,360 --> 00:53:56,000
like so on the right side you can see
that we had the price including shipping

646
00:53:56,000 --> 00:54:00,760
it was all calculated to make it really
obvious for people and then we had the

647
00:54:00,760 --> 00:54:06,150
privacy report with our Privacy meter so
you could see at a glance which had more

648
00:54:06,150 --> 00:54:13,300
privacy we compare this to another
condition so that we could rule out the

649
00:54:13,300 --> 00:54:16,380
the idea that people just like the
pretty indicator

650
00:54:16,380 --> 00:54:19,260
well first of all we made in the
careless pretty it was boxes instead of

651
00:54:19,260 --> 00:54:25,230
a bird but we also had the handicapped
accessibility condition so the

652
00:54:25,230 --> 00:54:28,220
indicators were exactly the same but
they were labeled handicapped

653
00:54:28,220 --> 00:54:32,509
accessibility and in another pre study
we tested it and found out that nobody

654
00:54:32,510 --> 00:54:36,090
cares about handicapped accessibility on
websites unless they actually need that

655
00:54:36,090 --> 00:54:37,590
yeah

656
00:54:37,590 --> 00:54:54,400
so we picked to all companies that were
not well known brands and not not

657
00:54:54,400 --> 00:54:59,010
companies that people would likely would
ever have dealt with and we actually

658
00:54:59,010 --> 00:55:01,620
pick companies whose privacy policies

659
00:55:01,620 --> 00:55:06,509
match the label that we gave them which
did add to the complexity of doing this

660
00:55:06,510 --> 00:55:11,690
study so then we had a third condition
which with no information condition

661
00:55:11,690 --> 00:55:16,650
where they didn't get a meter so what we
found was that the privacy I can't

662
00:55:16,650 --> 00:55:21,830
actually did influence purchases the
handicapped accessibility icons did not

663
00:55:21,830 --> 00:55:28,240
mean there was actually something there
with the privacy icons and so with

664
00:55:28,240 --> 00:55:31,850
privacy information people were more
likely to purchase at the more expensive

665
00:55:31,850 --> 00:55:36,660
websites and we had a pretty conclusive
results for that one thing that we

666
00:55:36,660 --> 00:55:42,509
couldn't demonstrate though with the
privacy sensitivity of the product made

667
00:55:42,510 --> 00:55:47,090
a difference and when we went back and
looked at why we couldn't measure

668
00:55:47,090 --> 00:55:50,790
difference here one of the things we
realize with the privacy premium was

669
00:55:50,790 --> 00:55:54,730
different between the two products and
so we hadn't properly controlled for

670
00:55:54,730 --> 00:56:01,200
that so we did a study one more time and
this time we decided to put the vendors

671
00:56:01,200 --> 00:56:07,379
in the loop so genocide actually
contacted 46 vendors and found eight of

672
00:56:07,380 --> 00:56:12,560
them that were willing to participate in
our study and basically we asked them to

673
00:56:12,560 --> 00:56:18,040
adjust their prices so we have need 25
cent increments and the prices and to

674
00:56:18,040 --> 00:56:19,259
hold that price

675
00:56:19,260 --> 00:56:23,040
constant for the course of the study
which is about a month most of the

676
00:56:23,040 --> 00:56:28,800
vendors we asked to raise their price
$0.05 or so there was one sex toys under

677
00:56:28,800 --> 00:56:32,820
that we needed to lower their price and
the way we got them to do that is we

678
00:56:32,820 --> 00:56:36,920
promised that we would pay them for the
lost sales or the lost money may

679
00:56:36,920 --> 00:56:42,870
actually think sold more sex toys that
month when they had a long time but at a

680
00:56:42,870 --> 00:56:48,970
loss for the phrase and so at the end of
the study we owe them a hundred and

681
00:56:48,970 --> 00:56:53,020
forty dollars and I had to go to CMU
accounting and get them to cut a check

682
00:56:53,020 --> 00:57:01,290
to the dirty money for research project
assistance received it but this is

683
00:57:01,290 --> 00:57:06,580
complicated but it was worth while we
actually were able to demonstrate that

684
00:57:06,580 --> 00:57:09,819
the privacy sensitive nature of the
product made made a difference and that

685
00:57:09,820 --> 00:57:14,320
people were more likely to pay more for
privacy in when they were purchasing

686
00:57:14,320 --> 00:57:18,270
that privacy sensitive product once we
were able to hold that privacy premium

687
00:57:18,270 --> 00:57:24,660
constant ok I want to talk about
designing and evaluating privacy nudges

688
00:57:24,660 --> 00:57:30,920
so in the economics literature there's
been a lot of interest in the concept of

689
00:57:30,920 --> 00:57:36,630
nudging we're rather than telling people
you have to do this you can't do that we

690
00:57:36,630 --> 00:57:41,630
can another reason to do what we think
is is the right thing and so they

691
00:57:41,630 --> 00:57:45,790
approach in this project is that we try
to understand whether to what are the

692
00:57:45,790 --> 00:57:51,350
sort of biases that people have and how
we might counteract those biases then we

693
00:57:51,350 --> 00:57:56,250
looked for some regrets people had in
the privacy space this case social

694
00:57:56,250 --> 00:58:01,550
networks and then we we looked at what
kind of nudges could we realistically

695
00:58:01,550 --> 00:58:07,170
build that might help avoid those
regrets so here's an example of a

696
00:58:07,170 --> 00:58:12,240
real-world knowledge and so here we have
a train station that has an escalator

697
00:58:12,240 --> 00:58:16,379
and a set of stairs and they want to
nudge people to take the stairs instead

698
00:58:16,380 --> 00:58:20,300
of the escalator so they painted the
stairs pretty color and it actually says

699
00:58:20,300 --> 00:58:25,860
I want to climb the stairs to fitness
and apparently it worked that people are

700
00:58:25,860 --> 00:58:31,070
much more willing to walk up these
stairs than the typical stares at the

701
00:58:31,070 --> 00:58:35,900
train station so we wanted to do the
same kind of thing for privacy so we

702
00:58:35,900 --> 00:58:40,640
started by trying to understand what
what would be a good thing to know about

703
00:58:40,640 --> 00:58:47,680
so we did a big study on Facebook about
what things people regretted doing on

704
00:58:47,680 --> 00:58:51,160
Facebook right now there is no shortage
of things people regret doing on

705
00:58:51,160 --> 00:58:56,710
Facebook and we very easily collected
several hundred examples through through

706
00:58:56,710 --> 00:58:59,430
our interviews and surveys

707
00:58:59,430 --> 00:59:03,980
and we ask them a number of questions
about the each of these examples so that

708
00:59:03,980 --> 00:59:07,230
we could understand the root causes of
why they were doing things they regret

709
00:59:07,230 --> 00:59:07,800
it

710
00:59:07,800 --> 00:59:14,589
some of the underlying causes that we
heard a lot of very big one was not

711
00:59:14,589 --> 00:59:18,900
thinking a lot of people post without
thinking often it's because they are

712
00:59:18,900 --> 00:59:24,329
either very excited very angry causes
them to post without thinking we also

713
00:59:24,329 --> 00:59:29,500
found a lack of awareness of how other
people would perceive their post so

714
00:59:29,500 --> 00:59:33,619
people said yeah I posted it I thought
it was a joke and other people thought

715
00:59:33,619 --> 00:59:37,250
it was not funny and I i didnt mean it
to be offensive but apparently it was

716
00:59:37,250 --> 00:59:42,520
there is also a lack of awareness of the
audience people couldn't remember who

717
00:59:42,520 --> 00:59:46,490
they had friended on Facebook and so
they would forget that they were still

718
00:59:46,490 --> 00:59:50,808
friends with their mother on Facebook
mothers came up a lot in the study also

719
00:59:50,809 --> 00:59:55,369
people would forget they friended their
boss their ex girlfriend ex boyfriend

720
00:59:55,369 --> 01:00:01,880
and all sorts of other people so we
wanted to develop nudges that would

721
01:00:01,880 --> 01:00:07,369
address these three big problems so we
want to encourage people to stop and

722
01:00:07,369 --> 01:00:12,280
think we want to make people aware of
how others might perceive their post and

723
01:00:12,280 --> 01:00:21,750
remind people about the audience we came
up with the timer knowledge where when

724
01:00:21,750 --> 01:00:28,579
you click Post it showed a countdown
timer and you had 10 seconds to look at

725
01:00:28,579 --> 01:00:33,609
your post again decide if you really
want to post it now there is also here

726
01:00:33,609 --> 01:00:39,200
oppose now button which would allow you
to just override that an Edit button so

727
01:00:39,200 --> 01:00:43,480
that you can go back and edit it or
cancel button if you want to stop it

728
01:00:43,480 --> 01:01:05,900
altogether we also had yeah yeah yeah
yeah we did talk about we didn't

729
01:01:05,900 --> 01:01:10,690
actually try it but that was an idea
that we went on our list of possible

730
01:01:10,690 --> 01:01:17,230
things to try so we are trying three
things we did that said the timer we did

731
01:01:17,230 --> 01:01:22,540
a sentiment nudge the idea here was to
try to address people not not

732
01:01:22,540 --> 01:01:26,980
anticipating what people would think of
it so we did a very simple analysis of

733
01:01:26,980 --> 01:01:31,540
the words and we said here you know
other people may perceive your post as

734
01:01:31,540 --> 01:01:36,540
the choices were either negative
positive very negative or very positive

735
01:01:36,540 --> 01:01:44,220
this one was highly successful nudged in
part because the simple algorithm that

736
01:01:44,220 --> 01:01:48,669
we used was really not very smart and in
particular it really didn't get sarcasm

737
01:01:48,670 --> 01:01:56,799
and then we had our profile picture
nudge where as people were typing it

738
01:01:56,799 --> 01:02:02,180
would select five random profile photos
from the people in the audience so if

739
01:02:02,180 --> 01:02:06,540
you are posting publicly it would be
five Facebook photos if it was your

740
01:02:06,540 --> 01:02:11,150
posting to your friends it would be five
photos of your friends and this was

741
01:02:11,150 --> 01:02:15,980
really useful because you know if you
have friends who are following you but

742
01:02:15,980 --> 01:02:19,750
don't post themselves you forget that
they're your friends and every time you

743
01:02:19,750 --> 01:02:24,880
post a different five people would come
up and I i use this and I certainly when

744
01:02:24,880 --> 01:02:29,750
like my grandmother's picture popped up
I was like what I just typed again

745
01:02:29,750 --> 01:02:37,180
alright so our target effective what we
wanted to do a user study to find out

746
01:02:37,180 --> 01:02:47,230
this study we decided with a three-week
field study we had 21 participants and

747
01:02:47,230 --> 01:02:51,970
we we were interested in finding out
whether the whether people thought it

748
01:02:51,970 --> 01:02:55,899
was useful in like the night just but
also whether it change people's behavior

749
01:02:55,900 --> 01:03:02,240
so this was rather challenging it was
difficult to find participants we

750
01:03:02,240 --> 01:03:02,959
implemented

751
01:03:02,960 --> 01:03:07,920
edges on Chrome so we need to find
people who only posted to Facebook from

752
01:03:07,920 --> 01:03:11,610
chrome who are willing to let us monitor
everything they typed on Facebook for

753
01:03:11,610 --> 01:03:17,710
three weeks it was hard to find people
who would do that it was also difficult

754
01:03:17,710 --> 01:03:22,600
study because while we all know about
lots of examples of things people regret

755
01:03:22,600 --> 01:03:27,160
doing on Facebook most people don't
regret things everyday like its

756
01:03:27,160 --> 01:03:34,069
occasional occasional thing and so even
in a three-week period not everybody in

757
01:03:34,070 --> 01:03:38,890
our study is naturally going to try to
post something that they would regret

758
01:03:38,890 --> 01:03:44,710
and then the other thing that really
gotten our way is that Facebook changes

759
01:03:44,710 --> 01:03:49,470
constantly and every time Facebook
changed it broke our nudges and our

760
01:03:49,470 --> 01:03:54,899
instrumentation and so we we actually
lost a lot of data that way so our

761
01:03:54,900 --> 01:03:59,240
results were that the profile picture
Niger seem to increase awareness and

762
01:03:59,240 --> 01:04:02,620
audience now most of these results we
have not so much from our quantitative

763
01:04:02,620 --> 01:04:06,970
data but from our post study interviews
where people talked about what what they

764
01:04:06,970 --> 01:04:12,200
experience so picture and I do it was
reasonably successful but I bernard was

765
01:04:12,200 --> 01:04:16,660
also successful for some people other
people found it highly annoying there

766
01:04:16,660 --> 01:04:21,140
are people who didn't notice that they
could actually just post now was there

767
01:04:21,140 --> 01:04:26,400
they were especially annoyed by this and
then the sentiment nudge really annoyed

768
01:04:26,400 --> 01:04:30,920
people in part because of the sarcasm
thing but also because people said

769
01:04:30,920 --> 01:04:35,560
alright so I was angry and I type this
message and it says your post seem

770
01:04:35,560 --> 01:04:43,279
really negative and I was like oh yes I
didn't really have the desired effect so

771
01:04:43,280 --> 01:04:46,680
we decided to focus on the picture
knowledge and the timer notes I think

772
01:04:46,680 --> 01:04:49,589
this sentiment now just so probably
worth doing but it needs to be done in a

773
01:04:49,590 --> 01:04:54,920
very different way than how we
approached it we did another field study

774
01:04:54,920 --> 01:05:00,340
we we combined the the picture and and
and the time in argentina won combined

775
01:05:00,340 --> 01:05:05,650
that that everybody saw we had and
improve data collection and event

776
01:05:05,650 --> 01:05:11,300
logging we we hired students to every
day go through and test every feature of

777
01:05:11,300 --> 01:05:16,359
Facebook that interacted with our judges
it actually was

778
01:05:16,359 --> 01:05:20,400
in our testing every day and if they
found that anything had changed would

779
01:05:20,400 --> 01:05:24,039
call the programmer and immediately get
them to work and trying to figure out

780
01:05:24,039 --> 01:05:27,749
whether it had broken our software and
what we needed to do and this was a

781
01:05:27,749 --> 01:05:33,129
six-week studies we did this every day
for six weeks we we had 28 participants

782
01:05:33,130 --> 01:05:37,279
who made it to the end of the study they
spent three weeks just being

783
01:05:37,279 --> 01:05:41,519
instrumented with known IDs and then
three weeks where where we showed them

784
01:05:41,519 --> 01:05:50,538
that this is what our new combined age
look like and once again we have the

785
01:05:50,539 --> 01:05:55,380
problem of sparse data so I just want
you to see how sparse it is you don't

786
01:05:55,380 --> 01:05:59,069
need to actually look in detail but over
here you see these are all the

787
01:05:59,069 --> 01:06:03,239
participants in the study and the other
all the days where they saw in the study

788
01:06:03,239 --> 01:06:07,380
and if you see a colored circle it means
that there were some interesting event

789
01:06:07,380 --> 01:06:11,829
that happened that day and it was
interesting is if they were posting and

790
01:06:11,829 --> 01:06:16,099
they clicked at it because of the night
or they click cancel because another

791
01:06:16,099 --> 01:06:20,499
judge or they went and changed their
privacy settings after a nudge appeared

792
01:06:20,499 --> 01:06:24,509
so you can see these things actually
don't happen that often there are some

793
01:06:24,509 --> 01:06:28,140
participants like christmas nineteen
over here almost every day something

794
01:06:28,140 --> 01:06:32,839
happens but there are many of our
participants that not very much happens

795
01:06:32,839 --> 01:06:37,470
so this is really sparse data and it's
really hard to make any quantitative

796
01:06:37,470 --> 01:06:44,569
conclusions based on this but we have
all sorts of anecdotal data and and from

797
01:06:44,569 --> 01:06:49,038
our interviews people telling us that
that this feature improved their

798
01:06:49,039 --> 01:06:54,809
awareness of the audience we had people
who gave us examples of how it encourage

799
01:06:54,809 --> 01:07:00,239
them to stop and think and then change
what they had posted something that they

800
01:07:00,239 --> 01:07:07,029
thought was gonna be more acceptable to
people in the audience again we had some

801
01:07:07,029 --> 01:07:12,089
people who weren't fans because they
will they didn't like the delay one

802
01:07:12,089 --> 01:07:14,900
person said there's no way to protect
people from posting embarrassing

803
01:07:14,900 --> 01:07:18,809
information online will matter up said
it's human nature to be stupid sometimes

804
01:07:18,809 --> 01:07:26,569
so what can you do but overall we
concluded that the idea of nudges seem

805
01:07:26,569 --> 01:07:28,820
like a promising approach and it's
something

806
01:07:28,820 --> 01:07:33,970
that we have subsequently used not just
on social networks were experimenting

807
01:07:33,970 --> 01:07:38,870
with it right now with Android app
installation to give people to nudge

808
01:07:38,870 --> 01:07:44,380
people about the all the things that
they're absurd doing and the idea has

809
01:07:44,380 --> 01:07:45,230
been picked up

810
01:07:45,230 --> 01:07:50,010
Facebook has actually implemented a
nudge we think they might have been

811
01:07:50,010 --> 01:07:56,240
influenced by our papers and they have
this Privacy checkup dinosaur which will

812
01:07:56,240 --> 01:08:02,979
pop up if you start posting things
publicly on Facebook alright so the last

813
01:08:02,980 --> 01:08:07,440
30 I want to talk about is our
investigation of the xkcd passphrase

814
01:08:07,440 --> 01:08:15,400
assertion so this is an example of doing
a user study to get some empirical data

815
01:08:15,400 --> 01:08:23,420
to test a claim has been made so
probably a lot of you have seen the sexy

816
01:08:23,420 --> 01:08:29,330
KCD comment basically what the
cartoonist is asserting here is that

817
01:08:29,330 --> 01:08:35,930
memorizing a random password is really
hard which there are plenty of other

818
01:08:35,930 --> 01:08:43,330
stuff that's true but member memorizing
a random passphrase is easy and so

819
01:08:43,330 --> 01:08:48,220
therefore we should use random pass
phrases rather than random passwords and

820
01:08:48,220 --> 01:08:53,800
after this came out quite a few people
who knew that that we were working on

821
01:08:53,800 --> 01:08:58,650
password research came up to us and said
yes see that passphrase think you should

822
01:08:58,649 --> 01:09:03,729
just have people to that problem salt
and so we thought about it and said well

823
01:09:03,729 --> 01:09:08,009
I mean it's an interesting hypothesis
but it's not obvious to us that it right

824
01:09:08,010 --> 01:09:12,830
and let's collect some empirical data
and find out whether this really solved

825
01:09:12,830 --> 01:09:18,850
the problem so we did a study there were
a whole bunch of people involved

826
01:09:18,850 --> 01:09:24,770
including a bunch you are in this room
or otherwise at the conference so rich

827
01:09:24,770 --> 01:09:28,859
shay patrick kelly is an ongoing
commentary Michelle measure it please

828
01:09:28,859 --> 01:09:32,140
urge and Leo Brouwer Nicholas Christian

829
01:09:32,140 --> 01:09:39,130
authors of this paper so we we wanted to
look at

830
01:09:39,130 --> 01:09:44,710
system a sign past phrases and compare
them to system sign passwords we were

831
01:09:44,710 --> 01:09:49,370
not comparing it with user-selected
password or passphrase is we're only

832
01:09:49,370 --> 01:09:53,010
looking at the random passwords in past
races because that was the claim made in

833
01:09:53,010 --> 01:09:59,620
the cartoon in addition we found earlier
that people are really not very random

834
01:09:59,620 --> 01:10:04,070
and if you ask people to randomly pick a
pass phrase they they cannot randomly

835
01:10:04,070 --> 01:10:07,340
pick a pass phrase they they will pick
something that is most definitely not

836
01:10:07,340 --> 01:10:14,220
random in their past raise this was done
as a Mechanical Turk study we had about

837
01:10:14,220 --> 01:10:19,900
1,500 participants it participate was
randomly assigned either a password or

838
01:10:19,900 --> 01:10:23,650
passphrase from one of several
conditions which I'll show you in a

839
01:10:23,650 --> 01:10:28,849
minute what they did is they were
assigned a password or passphrase they

840
01:10:28,850 --> 01:10:35,930
typed it in they took a survey they
typed it in again they went home then

841
01:10:35,930 --> 01:10:43,880
then two days later we emailed them and
ask them to come back to our website

842
01:10:43,880 --> 01:10:48,500
again and ask them to try to remember
their password or passphrase at that

843
01:10:48,500 --> 01:10:52,580
point we give them another survey which
included the question of did you write

844
01:10:52,580 --> 01:11:01,040
it down or did you remember it and then
we paid them we had eight different

845
01:11:01,040 --> 01:11:04,490
passphrase conditions and three password
conditions I'm just gonna show you a

846
01:11:04,490 --> 01:11:09,679
couple of time to go through it in
detail we varied a bunch of things

847
01:11:09,680 --> 01:11:16,290
because we want to go maybe it makes a
difference if we draw our random words

848
01:11:16,290 --> 01:11:20,900
from a really big dictionary or really
small dictionary or maybe it matters if

849
01:11:20,900 --> 01:11:24,950
we use different parts of speech or
different number of words or we change

850
01:11:24,950 --> 01:11:28,580
the instructions we give people about
how to remember so we tried a whole

851
01:11:28,580 --> 01:11:34,440
bunch of different things so here's one
example we had a condition that was for

852
01:11:34,440 --> 01:11:38,750
common words so this was from a
dictionary of a hundred and eighty one

853
01:11:38,750 --> 01:11:44,320
of the most common English words so here
you get past phrases like try their

854
01:11:44,320 --> 01:11:45,200
three come

855
01:11:45,200 --> 01:11:50,970
and one between high tell and we look at
those in third strike us as being

856
01:11:50,970 --> 01:11:56,240
particularly memorable movie maybe our
study participants will be different but

857
01:11:56,240 --> 01:12:02,019
tell us they didn't really seem very
memorable so we try to condition that we

858
01:12:02,020 --> 01:12:07,320
thought was more memorable this was noun
verb adjective noun so instead of

859
01:12:07,320 --> 01:12:11,219
drawing the words completely randomly we
segregate them into nouns verbs and

860
01:12:11,220 --> 01:12:16,280
adjectives and drew them from those
dictionaries so here we get plant plan

861
01:12:16,280 --> 01:12:24,620
build sure power that it to me seem bike
could could be more memorable we wanted

862
01:12:24,620 --> 01:12:29,260
to compare this with system assign
passwords so these are just plain old

863
01:12:29,260 --> 01:12:34,460
random passwords worth the same amount
of entropy as a passphrase is these

864
01:12:34,460 --> 01:12:39,670
don't seem all that memorable but at
least they're short we decided we wanted

865
01:12:39,670 --> 01:12:45,520
to try a random password that seemed to
have some logic to it and might have

866
01:12:45,520 --> 01:12:50,460
some some might be more memorable and so
we discovered pronounced of all

867
01:12:50,460 --> 01:12:54,400
passwords there's actually a new
standard from the nineteen seventies

868
01:12:54,400 --> 01:13:01,190
unpronounceable passwords and basically
there you have a dictionary of syllables

869
01:13:01,190 --> 01:13:05,589
and you randomly draw the syllables
rather than the characters and then you

870
01:13:05,590 --> 01:13:10,000
string them together and so you can get
a pass for TWO Frickley your daughter

871
01:13:10,000 --> 01:13:18,090
Sabi Sabi Sabi kind of has a ring to it
and so we decided to test that is well I

872
01:13:18,090 --> 01:13:22,060
kind of like the presidential palace
words my students were definitely of the

873
01:13:22,060 --> 01:13:28,890
mind like no one's gonna it's gonna like
this and so we're really curious to see

874
01:13:28,890 --> 01:13:33,990
what would happen if you ran the study
and there is actually no clear favorite

875
01:13:33,990 --> 01:13:41,969
past phrases were not more memorable
than passwords we found that there were

876
01:13:41,970 --> 01:13:45,770
actually not a whole lot of differences
between our conditions in terms of

877
01:13:45,770 --> 01:13:49,960
memorability we did find that passed
phrases took longer to type because

878
01:13:49,960 --> 01:13:54,730
they're more characters people made more
mistakes entering them we did come up

879
01:13:54,730 --> 01:13:58,009
with a way that we could do some error
correction so that people could enter

880
01:13:58,010 --> 01:13:58,369
their

881
01:13:58,369 --> 01:14:02,398
passphrase even making mistakes and we
could accept it without a loss without a

882
01:14:02,399 --> 01:14:07,209
loss of security and we found that
pronounce your passwords actually people

883
01:14:07,209 --> 01:14:12,969
could enter a little bit more quickly
with fewer mistakes and there were a

884
01:14:12,969 --> 01:14:15,610
number of reasons why pronounced will
password actually showed some promise

885
01:14:15,610 --> 01:14:20,360
and seem like this is something people
should look at some more and see if you

886
01:14:20,360 --> 01:14:29,289
can make them work so in that particular
study and and in all the cities I just

887
01:14:29,289 --> 01:14:36,749
told you about we found these studies
whereas I advertised complicated they

888
01:14:36,749 --> 01:14:42,269
were challenging to design but there are
some really interesting and necessary in

889
01:14:42,269 --> 01:14:46,530
order to understand what's really going
on here so rather than making claims

890
01:14:46,530 --> 01:14:50,119
that we can't support but instead may
actually be wrong

891
01:14:50,119 --> 01:14:55,539
we need to do the study in some cases we
actually have an idea already that

892
01:14:55,539 --> 01:14:59,389
things are going wrong but we don't know
why and we really need to do the study

893
01:14:59,389 --> 01:15:03,860
so hopefully I have convinced you of
that and you'll have some interest in

894
01:15:03,860 --> 01:15:08,829
learning more about usable security
studies and maybe trying them if you

895
01:15:08,829 --> 01:15:12,308
would like to learn a lot more about
them I encourage you to come to the

896
01:15:12,309 --> 01:15:17,530
soups conference which will be in Denver
in June starting this year

897
01:15:17,530 --> 01:15:20,869
USENIX will be running the soups
conference which makes me very happy

898
01:15:20,869 --> 01:15:25,938
because it means I'm not running this
week's conference anymore I've been the

899
01:15:25,939 --> 01:15:31,530
general chair for a long time but yeah
it's a really great conference Matthew

900
01:15:31,530 --> 01:15:37,320
Smith is one of the program chairs he's
here he was here earlier so I encourage

901
01:15:37,320 --> 01:15:44,730
you to check that out and I'll be happy
to take your questions

902
01:15:44,730 --> 01:16:09,750
there are a million

903
01:16:09,750 --> 01:16:33,160
brainstorming $739,000 actually we're
not just limited by money we're also

904
01:16:33,160 --> 01:16:36,809
limited by time and the fact that when
you do a study and recruit participants

905
01:16:36,810 --> 01:16:42,710
at some point during the participant
pool and you can't just an unlimited

906
01:16:42,710 --> 01:16:50,540
number of participants for us you can't
so we tend to start reasoning about the

907
01:16:50,540 --> 01:16:55,440
different variables and seeing how
interesting do we think they are which

908
01:16:55,440 --> 01:16:59,599
ones do we from previous studies can we
eliminate because we already have a

909
01:16:59,600 --> 01:17:04,560
pretty good idea of what it's going to
do we look at how we can do a study

910
01:17:04,560 --> 01:17:07,739
designed so we don't have to do a full
factorial design of the study and with

911
01:17:07,739 --> 01:17:13,459
every combination but instead can
compare hold hold everything constant

912
01:17:13,460 --> 01:17:19,680
except one thing and then just change
that one thing so yeah usually the group

913
01:17:19,680 --> 01:17:24,070
of us working together and kind of all
weighing in different people will be

914
01:17:24,070 --> 01:17:44,289
arguing for different conditions

915
01:17:44,289 --> 01:17:55,969
certainly there are differences in
people's ability to detect phishing

916
01:17:55,969 --> 01:18:02,159
based on the types of changes you make
to your in other work that I done we we

917
01:18:02,159 --> 01:18:06,898
did actually do a lot of tests of why
people were falling for phishing attacks

918
01:18:06,899 --> 01:18:11,349
the biggest factor with falling her
phishing attacks was really people who

919
01:18:11,349 --> 01:18:15,260
don't know even look at the Euro once
you get to the people who are paying

920
01:18:15,260 --> 01:18:20,919
attention to the Euro it's actually a
much smaller difference as far as

921
01:18:20,919 --> 01:18:25,398
whether people are falling 400 W zombies
or whatever

922
01:18:25,399 --> 01:18:30,439
some of those are trickier than others
but but the big differences that at

923
01:18:30,439 --> 01:18:33,619
least the time we did the studies most
people didn't even know to look at the

924
01:18:33,619 --> 01:18:58,499
Euro stuff you know the bad guys are the
ones who are basically doing the user

925
01:18:58,499 --> 01:19:01,679
study you know they try to fish people
and if it works it will let's do that

926
01:19:01,679 --> 01:19:03,469
again if it doesn't work they have

927
01:19:03,469 --> 01:19:06,699
ok we're not gonna do that so look to
see what real Fisher doing because

928
01:19:06,699 --> 01:19:10,039
that's what they know

929
01:19:10,039 --> 01:19:13,739
yeah

930
01:19:13,739 --> 01:19:25,440
so there's a little magic there I think
so there there certainly are statistical

931
01:19:25,440 --> 01:19:30,129
tests that can be run to give you an
idea of how many participants you need

932
01:19:30,130 --> 01:19:33,079
to get a statistically significant
result but in order to run the

933
01:19:33,079 --> 01:19:36,759
statistical tests you have to have some
ideas of what sort of effect size you're

934
01:19:36,760 --> 01:19:40,199
going to have which you don't
necessarily always know in advance you

935
01:19:40,199 --> 01:19:45,428
can pilot in there but there there are a
lot of kind of guesswork and looking at

936
01:19:45,429 --> 01:19:49,820
it will last time we need a password
study you know we're and 500

937
01:19:49,820 --> 01:19:53,389
participants and it was enough this time
we're gonna have twice as many

938
01:19:53,389 --> 01:19:57,639
conditions so probably a thousand
participants in which we definitely do

939
01:19:57,639 --> 01:20:02,619
that sort of thing as far as you know
how long to run the study it depends on

940
01:20:02,619 --> 01:20:06,989
what we're trying to observe and how
frequently we think it will happen and

941
01:20:06,989 --> 01:20:12,190
whether their effects like it is there a
weekend effect to do we expect that

942
01:20:12,190 --> 01:20:16,030
people are going to behave differently
on the weekend they do in the week we

943
01:20:16,030 --> 01:20:20,749
want to make sure that our study covers
goes beyond the weekend so that we can

944
01:20:20,749 --> 01:20:26,139
catch weekday and weekend behavior
things like that

945
01:20:26,139 --> 01:20:52,180
yeah

946
01:20:52,180 --> 01:21:12,750
inexpensive but that would be i think a
good example of if people felt it was

947
01:21:12,750 --> 01:21:18,340
risky to give their bank account number
than what they pay more not to do that

948
01:21:18,340 --> 01:21:54,440
and yeah yeah I think I would ya no no I
haven't Yeah Yeah Yeah Yeah Yeah Yeahs

949
01:21:54,440 --> 01:21:59,360
may be completely unaware of the risk
yea or sometimes they they key in on

950
01:21:59,360 --> 01:22:02,349
things that are that expert with they
are not risky but they've somehow

951
01:22:02,350 --> 01:22:27,449
convince themselves are risky yeah yeah

952
01:22:27,449 --> 01:22:36,329
yeah so we actually did a study on touch
I D and face recognition for cell phones

953
01:22:36,329 --> 01:22:41,570
and so we we were focusing on the
usability there have been other studies

954
01:22:41,570 --> 01:22:46,728
on the security and touch I D and face
recognition on phones do not have great

955
01:22:46,729 --> 01:22:51,849
security I haven't done those studies
but I read them and they're easily

956
01:22:51,849 --> 01:22:57,459
spoofed i've i've encountered I
installed face idea on my android phone

957
01:22:57,459 --> 01:23:02,909
to play with it and the time
seven-year-old daughter picked up my

958
01:23:02,909 --> 01:23:09,219
phone and widen as me immediately my
other two kids then tried it either

959
01:23:09,219 --> 01:23:16,559
now didn't work for them but as far as
usability the touch I D people it is

960
01:23:16,559 --> 01:23:22,699
very usable and people like it it easy
we found in our study we tested logging

961
01:23:22,699 --> 01:23:28,449
in with with both touch idea face I D
while walking or carrying things which

962
01:23:28,449 --> 01:23:33,339
we made people smear moisturizer on
their hands and it didn't matter

963
01:23:33,340 --> 01:23:40,190
touch idea what it was pretty easy face
I D was not so easy in any other

964
01:23:40,190 --> 01:23:45,759
circumstances and then in a dark room
even a dim room face I did as completely

965
01:23:45,760 --> 01:23:52,639
falls apart so that that's what we found
but I think touch I D

966
01:23:52,639 --> 01:24:02,670
not that secure but yeah convenient

967
01:24:02,670 --> 01:24:14,020
yeah so we've used and hurt alot and
overall i I recommend targets its you

968
01:24:14,020 --> 01:24:19,969
can you can do studies on Amtrak that
just wouldn't be feasible and economical

969
01:24:19,969 --> 01:24:26,690
to do a lot of other ways any sample
that you have is gonna be biased in some

970
01:24:26,690 --> 01:24:31,889
way and so the key is to understand what
sort of bias you're dealing with so an

971
01:24:31,890 --> 01:24:35,760
empty queue of people who are relatively
technologically sophisticated because

972
01:24:35,760 --> 01:24:40,199
they're spending their free time on an
online service like trying to make a

973
01:24:40,199 --> 01:24:45,509
little bit of money and there there are
actually some studies which have tried

974
01:24:45,510 --> 01:24:49,170
to characterize their demographics and
see how they're different from the

975
01:24:49,170 --> 01:24:52,800
population there there a little bit
better educated there are few other

976
01:24:52,800 --> 01:24:57,010
things about them that are different
they also tend to do a lot of studies

977
01:24:57,010 --> 01:25:02,600
and so there's some types of studies
that they're kind of immune to because

978
01:25:02,600 --> 01:25:06,840
they've done them so many times so you
can have to keep those are two things in

979
01:25:06,840 --> 01:25:12,280
mind but when we're doing a
between-subjects study where we have

980
01:25:12,280 --> 01:25:16,239
different conditions and everybody does
one condition and we want to see the

981
01:25:16,239 --> 01:25:22,199
differences between them I think it's
been actually pretty useful to use hers

982
01:25:22,199 --> 01:25:31,330
for those studies

983
01:25:31,330 --> 01:25:46,890
yeah yeah so there are an increasing
number of corporate researchers who do

984
01:25:46,890 --> 01:25:53,630
work in this area including story you
can give you his perspective on security

985
01:25:53,630 --> 01:25:57,250
at Microsoft there's a whole big team at
Google who has been doing is all

986
01:25:57,250 --> 01:26:03,360
security studies and I'm sure there are
other companies as well I think that

987
01:26:03,360 --> 01:26:07,910
there we would like to see more usable
security studies at some of the

988
01:26:07,910 --> 01:26:13,269
traditional security companies who I
think are still kind of slow to adopt

989
01:26:13,270 --> 01:26:18,310
and especially small companies tend to
not to do that but some of the bigger

990
01:26:18,310 --> 01:26:23,740
companies definitely are a big advantage
that some of these big companies have is

991
01:26:23,740 --> 01:26:27,830
that they have access to large pools of
users and they can do a beach house

992
01:26:27,830 --> 01:26:34,990
where they can they can change 11
feature 400,000 users and you know

993
01:26:34,990 --> 01:26:39,830
tomorrow tell you what what happens when
you do that and it makes us academics

994
01:26:39,830 --> 01:26:44,970
very jealous we look for ways that we
can collaborate with them so that we can

995
01:26:44,970 --> 01:26:50,790
do that sort of thing to the back

996
01:26:50,790 --> 01:27:10,110
yeah

997
01:27:10,110 --> 01:27:17,780
yeah I discovered the warnings were
about ten years ago and I got a copy of

998
01:27:17,780 --> 01:27:21,910
the handbook of warnings and tell my
desk and all my students have carried

999
01:27:21,910 --> 01:27:27,450
this time around and you know these
bitter people from psychology mostly who

1000
01:27:27,450 --> 01:27:32,290
studied the brain processes when we
encounter a railroad crossing sign you

1001
01:27:32,290 --> 01:27:38,790
know medicine bottle or whatever and
most of the work that they've done is on

1002
01:27:38,790 --> 01:27:46,030
paper or signs and not electronic but
their cognitive models applied very well

1003
01:27:46,030 --> 01:27:51,200
make a lot of sense and so if you look
at a lot of certainly are warning

1004
01:27:51,200 --> 01:27:55,630
studies and other researchers as well
they cite that work and we we've

1005
01:27:55,630 --> 01:28:17,860
definitely tried to use those models and
they seem to apply pretty well yeah

1006
01:28:17,860 --> 01:28:29,219
yeah so I think usability for security
developers is an important area it is

1007
01:28:29,219 --> 01:28:33,440
not an area that I myself have worked in
and I don't think there's actually been

1008
01:28:33,440 --> 01:28:38,469
a lot of work in it but it it's
something that we're starting to get

1009
01:28:38,469 --> 01:28:43,560
exactly so Matthew Smith has has done
some workers to a couple of others were

1010
01:28:43,560 --> 01:28:50,280
starting to see work in this area and I
think about references attempt to extort

1011
01:28:50,280 --> 01:28:59,679
may have with your preferences but by
yes I think it is an area that I hope we

1012
01:28:59,679 --> 01:29:29,590
see more working

1013
01:29:29,590 --> 01:29:58,480
yeah I think there's a lot of before we
even get to that question which as you

1014
01:29:58,480 --> 01:30:03,440
know is it more secure when there's
there's issues of whether users can deal

1015
01:30:03,440 --> 01:30:09,110
with it when they have anything slightly
out of the ordinary so I myself have

1016
01:30:09,110 --> 01:30:14,339
experience I have to factor on on my
Google account and I have an Android

1017
01:30:14,340 --> 01:30:17,730
phone and I went to the AT&T store to
get a new phone and I'm standing there

1018
01:30:17,730 --> 01:30:20,830
in the store with my old phone in one
hand on my new phone in the other and

1019
01:30:20,830 --> 01:30:24,929
AT&T ladies says okay I've activated
your new phone you know log into your

1020
01:30:24,929 --> 01:30:26,070
Google account

1021
01:30:26,070 --> 01:30:29,509
oh wait I can't I have to do the
two-factor authentication except that

1022
01:30:29,510 --> 01:30:34,060
it's going to text me a code but my
phone cant receive text shiite so i cant

1023
01:30:34,060 --> 01:30:40,349
get the code and I'm gonna have to wait
for the iPhone and AT&T store had no

1024
01:30:40,349 --> 01:30:44,310
idea what I was talking about and and I
finally said you know what I'm just

1025
01:30:44,310 --> 01:30:49,570
going to go home and I'm sure in half an
hour that's what we're gonna be fine and

1026
01:30:49,570 --> 01:30:53,500
so I did and it was fine but I could
imagine a lot of people standing there

1027
01:30:53,500 --> 01:30:55,080
and panicking and not knowing what to do

1028
01:30:55,080 --> 01:31:05,309
sort of situation

1029
01:31:05,310 --> 01:31:13,360
so in some studies it's especially
important so in our passwords studies we

1030
01:31:13,360 --> 01:31:18,240
actually try to avoid having the same
Mechanical Turk worker more than once

1031
01:31:18,240 --> 01:31:22,920
now we've been doing passwords daddy's
friend Alex six years and we have a

1032
01:31:22,920 --> 01:31:26,750
database of the turkey or ideas of
everybody who's done our studies and

1033
01:31:26,750 --> 01:31:32,010
when we do a study we check the idea and
if even six years ago one of our studies

1034
01:31:32,010 --> 01:31:38,010
we don't like you do it and so we now
have like 60,000 people have done our

1035
01:31:38,010 --> 01:31:39,900
studies

1036
01:31:39,900 --> 01:31:44,040
well so they're they're certainly maybe
truckers who have created multiple ideas

1037
01:31:44,040 --> 01:31:49,960
and we can't catch that when it comes to
our lab yet so it depends on the study

1038
01:31:49,960 --> 01:31:53,820
but certainly if we're doing a series of
studies like when we didn't multiple

1039
01:31:53,820 --> 01:31:57,759
privacy finders that is if somebody was
involved in study 1 we did not let them

1040
01:31:57,760 --> 01:32:02,880
participate in two or three yeah so we
we are careful about that some of the

1041
01:32:02,880 --> 01:32:06,670
places that we recruit participants we
know there are a lot of people who just

1042
01:32:06,670 --> 01:32:09,670
love doing studies at Carnegie Mellon
and I'll keep signing up for Carnegie

1043
01:32:09,670 --> 01:32:14,860
Mellon studies and so we we need to be
careful about screening out people who

1044
01:32:14,860 --> 01:32:33,940
have already done a similar study see
over on the side

1045
01:32:33,940 --> 01:32:41,440
so they're definitely privacy tools that
you see that that are not very usable

1046
01:32:41,440 --> 01:32:44,830
and yet some people use them and so
there must be some reason that they're

1047
01:32:44,830 --> 01:32:49,760
using them and you know either of those
people are different than the rest of us

1048
01:32:49,760 --> 01:32:54,219
in that they don't find it unusable or
they care so much about the privacy

1049
01:32:54,219 --> 01:33:03,890
protection that are willing to put up
with that

1050
01:33:03,890 --> 01:33:08,600
well so I think anybody who puts up with
torture has to be concerned about

1051
01:33:08,600 --> 01:33:11,470
something there have to be some reason
they're putting up with torture

1052
01:33:11,470 --> 01:33:16,160
concerned about privacy may be one of
those reasons there may be other reasons

1053
01:33:16,160 --> 01:33:20,790
that they put up with her church here
but but but yeah but I think there are

1054
01:33:20,790 --> 01:33:26,080
also people who care quite a bit about
privacy who won't put up with that and

1055
01:33:26,080 --> 01:33:29,710
and if they care so much about privacy
they may do something like just withdraw

1056
01:33:29,710 --> 01:33:33,930
and say I just not going to use the
computer much because I don't think I

1057
01:33:33,930 --> 01:33:38,380
can protect my privacy and it is always
so they're different approaches that

1058
01:33:38,380 --> 01:34:04,280
privacy conscious people take their
definitely when you're doing data

1059
01:34:04,280 --> 01:34:09,660
analysis the utility of the data may be
inversely with privacy but as far as the

1060
01:34:09,660 --> 01:34:13,000
usability of privacy tools I mean there
are a lot of privacy tools that there's

1061
01:34:13,000 --> 01:34:17,600
really no excuse for how about they are
that they they can there's a lot of

1062
01:34:17,600 --> 01:34:21,490
low-hanging fruit as far as improving
the usability of these tools we did a

1063
01:34:21,490 --> 01:34:31,420
study by johnnie study why Johnny can't
opt out that looked at this case it was

1064
01:34:31,420 --> 01:34:37,410
johnnie but your journey would be nice
too but we we did look at usability of

1065
01:34:37,410 --> 01:34:42,570
privacy tools and we found a lot of
things that we're just like really easy

1066
01:34:42,570 --> 01:34:48,230
to fix and after we published our study
some of the tool vendors did fix some of

1067
01:34:48,230 --> 01:34:54,440
the things we'd we pointed out

1068
01:34:54,440 --> 01:35:10,629
so those of you who are academics you
know go find the the user interface HCI

1069
01:35:10,630 --> 01:35:16,510
group in your department and and talk to
them and find out

1070
01:35:16,510 --> 01:35:20,630
get some advice from them find out if
they'd be interested in partnering in

1071
01:35:20,630 --> 01:35:26,760
doing a study those of you in industry
you know many companies have groups that

1072
01:35:26,760 --> 01:35:31,050
are very interested in user interface
and user interaction but often those

1073
01:35:31,050 --> 01:35:35,710
groups don't talk to the security groups
they should talk they should talk in the

1074
01:35:35,710 --> 01:35:39,800
design phase because it's very difficult
once you have a completely implemented

1075
01:35:39,800 --> 01:35:43,730
product to completely change the user
interface when you're usability group

1076
01:35:43,730 --> 01:35:49,839
tells you he nobody can use that so
that's that's why big advice on you know

1077
01:35:49,840 --> 01:35:52,910
if you want to learn how to do the
studies yourself i mean that's a lot of

1078
01:35:52,910 --> 01:36:00,139
work but you can do that to one of the
fun things at Carnegie Mellon I got

1079
01:36:00,140 --> 01:36:05,750
there eleven years ago and went and made
friends with the security group and we

1080
01:36:05,750 --> 01:36:11,360
started working together on user studies
and some of some of those people who had

1081
01:36:11,360 --> 01:36:16,830
never done a user study before now doing
user studies you know ten years later

1082
01:36:16,830 --> 01:36:24,820
they're very comfortable doing it
themselves to

1083
01:36:24,820 --> 01:36:48,929
ok yeah actually given your particular
information it was interesting to see

1084
01:36:48,930 --> 01:36:52,650
that in in the study with the meter
people could click on the meter to get

1085
01:36:52,650 --> 01:36:57,070
detailed information but hardly anyone
did they were just happy with the notion

1086
01:36:57,070 --> 01:37:01,440
of somebody evaluated this and said the
party was good that's all i care about

1087
01:37:01,440 --> 01:37:07,820
but we're actually working on a study
right now because we're trying to design

1088
01:37:07,820 --> 01:37:12,700
a tool that basically read the Privacy
Policy for you and pulls out the most

1089
01:37:12,700 --> 01:37:16,590
useful information and displays it so we
want to know what is the most useful

1090
01:37:16,590 --> 01:37:20,310
information that people would want to
see displayed and so so we're working on

1091
01:37:20,310 --> 01:37:39,050
that

1092
01:37:39,050 --> 01:38:17,530
yeah yeah yeah you can read for hours
but yes we we have looked at the impact

1093
01:38:17,530 --> 01:38:23,969
of all different kinds of rules on both
memorability and security it turns out

1094
01:38:23,969 --> 01:38:26,760
there are some rules that are put in
place to try to make password more

1095
01:38:26,760 --> 01:38:30,440
secure that actually have the opposite
effect because they're so hard that it

1096
01:38:30,440 --> 01:38:37,190
causes users to like try to be as lazy
as they can and and and not actually do

1097
01:38:37,190 --> 01:39:19,329
security yeah

1098
01:39:19,329 --> 01:39:35,929
password when you go longer it might
make more of a difference and I think

1099
01:39:35,929 --> 01:39:40,499
that that there may be more need for
past phrases when you're when you're

1100
01:39:40,499 --> 01:39:55,050
dealing with with more entropy and
remember yeah so we were testing memory

1101
01:39:55,050 --> 01:40:03,339
after two days and there's another study
that I was involved in that that looked

1102
01:40:03,339 --> 01:40:11,269
at past phrases that had images that
were related to them and was looking

1103
01:40:11,269 --> 01:40:15,669
bigger whole way of helping users learn
to remember them and it seems like you

1104
01:40:15,669 --> 01:40:19,898
do something like that can actually give
you some advantages of passphrase

1105
01:40:19,899 --> 01:40:24,879
because there's this whole process where
the user has to associate the passphrase

1106
01:40:24,879 --> 01:40:31,939
with an image and so so I think there
there definitely are some scenarios

1107
01:40:31,939 --> 01:40:37,659
where passphrase it helps them potential
but just as a simple you know a sign a

1108
01:40:37,659 --> 01:40:41,589
passphrase rather the password and
everything will be happy it's not quite

1109
01:40:41,589 --> 01:40:46,269
that great

1110
01:40:46,270 --> 01:41:27,890
Yeah Yeah Yeah Yeah Yeahs see in the
corner

1111
01:41:27,890 --> 01:41:39,660
we didn't have that in this study
because it was testing something else

1112
01:41:39,660 --> 01:41:43,880
with you know whether users can actually
make judgments about publishers that

1113
01:41:43,880 --> 01:41:49,540
they hadn't heard of we were taking kind
of the easier case of assuming it was

1114
01:41:49,540 --> 01:41:53,220
clear cut if you would only look at it
was clear cut this one's good ones bad

1115
01:41:53,220 --> 01:41:57,750
but it is definitely the case that the
user may see a publisher they haven't

1116
01:41:57,750 --> 01:42:20,610
heard of and then what are they going to
do to make a judgment

1117
01:42:20,610 --> 01:42:39,239
ability in general

1118
01:42:39,239 --> 01:42:45,839
definitely our guidelines and you can
find any number of design guidelines for

1119
01:42:45,840 --> 01:42:50,150
websites and four different kinds of
software there are some heuristics that

1120
01:42:50,150 --> 01:42:53,879
they're they're called discount
usability testing where they say you

1121
01:42:53,880 --> 01:42:58,420
systematically applied the heuristics
and go through and and see which of the

1122
01:42:58,420 --> 01:43:02,889
roles have been violated here and if a
rule is violated it not a guarantee that

1123
01:43:02,889 --> 01:43:06,400
it's bad but chances are it's an
indicator that there may be a usability

1124
01:43:06,400 --> 01:43:11,679
problem and if you run through it and
everything checked checks off then your

1125
01:43:11,679 --> 01:43:15,460
chances are you don't have usability
problems so I think first

1126
01:43:15,460 --> 01:43:20,730
straightforward usability testing it's a
good start

1127
01:43:20,730 --> 01:43:26,489
it's an indicator anyway things can get
messier and usable security because

1128
01:43:26,489 --> 01:43:30,869
we're dealing with this kind of trade
off and and we need the users not to be

1129
01:43:30,869 --> 01:43:36,080
able to just you know like use the word
processor and find the bold button but

1130
01:43:36,080 --> 01:43:39,840
we need them to be able to use something
safely while potentially under attack

1131
01:43:39,840 --> 01:43:46,480
and that is not always so obvious but as
the usable security literature is

1132
01:43:46,480 --> 01:43:51,718
growing we have many case studies
involving different studies that people

1133
01:43:51,719 --> 01:43:54,940
have done and so you come up with
something new you may be able to say

1134
01:43:54,940 --> 01:43:59,110
that seems sort of like this other thing
that has already been tested what was

1135
01:43:59,110 --> 01:44:04,699
found in that test and can I extrapolate
to my new situation to get an idea of it

1136
01:44:04,699 --> 01:44:28,320
and so I think we definitely should
learn from these things

1137
01:44:28,320 --> 01:44:40,719
culturally dependent we we've done some
privacy studies in India and China and

1138
01:44:40,719 --> 01:44:43,320
definitely some very different things

1139
01:44:43,320 --> 01:44:49,549
some of our entered studies we do
internationally some we limit to the us-

1140
01:44:49,550 --> 01:44:54,079
we usually limit them to English
speakers because our interfaces are in

1141
01:44:54,079 --> 01:45:01,199
English this summer I supervised a class
project team is doing a project for

1142
01:45:01,199 --> 01:45:05,619
Lufthansa Airlines and on privacy and
left-hander was very interested in what

1143
01:45:05,619 --> 01:45:06,730
germans

1144
01:45:06,730 --> 01:45:11,869
think they have a lot of customers are
German so they actually translated all

1145
01:45:11,869 --> 01:45:16,829
of our study materials into German and
we just heard you yet very difficult to

1146
01:45:16,829 --> 01:45:20,750
recruit germans on M Turk but there's
another crowdsourcing site that we were

1147
01:45:20,750 --> 01:45:23,739
able to use to recruit German
participants and we did find some

1148
01:45:23,739 --> 01:45:31,769
differences between our American and
German participants really out of

1149
01:45:31,770 --> 01:45:33,570
questions alright

