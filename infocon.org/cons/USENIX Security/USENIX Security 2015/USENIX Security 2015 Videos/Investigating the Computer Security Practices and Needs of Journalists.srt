1
00:00:11,250 --> 00:00:15,660
ISO enojar peter is investigating the
computer security needs and

2
00:00:15,660 --> 00:00:20,950
journalists and money Mrs McGregor I'm
from Columbia Journalism School this is

3
00:00:20,950 --> 00:00:24,310
work that I did with my co-author
Francisco rosner from University of

4
00:00:24,310 --> 00:00:28,299
Washington and Polina charters in 10 min
holiday who are part of the HCI and

5
00:00:28,300 --> 00:00:33,260
design program also at the University of
Washington said talk a little bit about

6
00:00:33,260 --> 00:00:39,220
the motivation for this work last year
at Black Hat Asia she not only in our

7
00:00:39,220 --> 00:00:45,420
data presentation about the fact that 21
out of the 25 top news sites in the

8
00:00:45,420 --> 00:00:50,149
world were subject to state level
attacks and this is something that was

9
00:00:50,149 --> 00:00:53,090
already kind of known in the media the
previous year

10
00:00:53,090 --> 00:00:56,180
news agencies have reported themselves
that they had been the subject of

11
00:00:56,180 --> 00:00:59,930
attacks the Wall Street Journal The
Washington Post Bloomberg New York Times

12
00:00:59,930 --> 00:01:05,099
and in a couple of those cases the New
York Times and Bloomberg in particular

13
00:01:05,099 --> 00:01:09,880
it was surmised that the motivation for
these attacks was actually just cover

14
00:01:09,880 --> 00:01:14,170
the sources for stories as particularly
in respect to China of course we also

15
00:01:14,170 --> 00:01:19,390
know that the impetus to discover
sources is not exclusive to these kinds

16
00:01:19,390 --> 00:01:24,440
of attacks on the domestic front
metadata and kind of legal attacks have

17
00:01:24,440 --> 00:01:29,440
been used extensively in recent years to
identify journalists sources this was

18
00:01:29,440 --> 00:01:36,069
very much the case in James rise in New
York Times and others so there are a

19
00:01:36,069 --> 00:01:39,310
range of kind of computer security
issues that journalists and journalistic

20
00:01:39,310 --> 00:01:44,670
organizations face in trying to conduct
their intent to conduct their work and

21
00:01:44,670 --> 00:01:48,560
there are in fact quite a number of
robust technical tools that help address

22
00:01:48,560 --> 00:01:54,179
these these issues individually right so
we have things like GPG that can provide

23
00:01:54,179 --> 00:01:59,690
in a static encryption tour secure drop
which is actually designed specifically

24
00:01:59,690 --> 00:02:05,270
for journalism for kind of a document
that dropped case and yet recent

25
00:02:05,270 --> 00:02:09,929
research this was a Pew survey done with
specifically investigative reporters

26
00:02:09,929 --> 00:02:15,800
globally others released in February
found that 50% of them used no computer

27
00:02:15,800 --> 00:02:21,280
security tools whatsoever in conducting
their work another 30% used one or two

28
00:02:21,280 --> 00:02:26,250
but not with great regularity and so the
real question is why not

29
00:02:26,250 --> 00:02:30,340
it's clear that journalists are targets
that there are a wide range of kind of

30
00:02:30,340 --> 00:02:33,780
threat vectors that they face there are
tools that can help address some of

31
00:02:33,780 --> 00:02:37,680
these things and yet for some reason
they're not doing anything about it and

32
00:02:37,680 --> 00:02:42,000
so our observation was that maybe
there's something for understanding

33
00:02:42,000 --> 00:02:45,330
there's something about the journalistic
process that makes it difficult for

34
00:02:45,330 --> 00:02:49,600
journalists to effectively apply these
tools in the way that they need to in

35
00:02:49,600 --> 00:02:54,329
order to conduct more securely so also
drawing from that Pew Research survey

36
00:02:54,330 --> 00:02:58,930
this is a quote from our respondents
saying I report on another as immigrants

37
00:02:58,930 --> 00:03:01,870
a great deal and have concerns about how
to communicate with them without putting

38
00:03:01,870 --> 00:03:06,110
them at risk you know many of them are
not extremely computer savvy and this is

39
00:03:06,110 --> 00:03:11,620
something I struggle with a great deal
so we're experts from the journalism in

40
00:03:11,620 --> 00:03:14,950
computer security communities we went
about this work by conducting in-depth

41
00:03:14,950 --> 00:03:19,488
interviews with 15 full-time journalists
and news media organizations in the USA

42
00:03:19,489 --> 00:03:23,290
and France and in doing this we want to
just focus on kind of the typical

43
00:03:23,290 --> 00:03:27,320
workflow system understand with a
general journalists across was maybe

44
00:03:27,320 --> 00:03:31,390
where there were gaps in that weren't
being addressed by existing computer

45
00:03:31,390 --> 00:03:35,480
security tools we didn't ask journalists
directly about their threat models

46
00:03:35,480 --> 00:03:39,390
because that's really not a common term
in journalism so instead we designed a

47
00:03:39,390 --> 00:03:45,480
set of questions about their risks and
consequences of those risks to kind of

48
00:03:45,480 --> 00:03:50,048
illicit a threat models there and we did
this as a qualitative data analysis so

49
00:03:50,049 --> 00:03:52,830
conducted the interviews had to
independent voters go through the

50
00:03:52,830 --> 00:03:58,540
results in some cases and we have tables
of all these results in the paper you

51
00:03:58,540 --> 00:04:01,620
know had some qualitative data about you
know how these people used a particular

52
00:04:01,620 --> 00:04:05,540
to learn how many of you that particular
tool but also qualitative thing for

53
00:04:05,540 --> 00:04:08,630
themes about what their security
concerns are there general practices

54
00:04:08,630 --> 00:04:13,400
some of the ad hoc techniques they used
to secure their information just a few

55
00:04:13,400 --> 00:04:18,459
more details about the participants
organizations really really range we had

56
00:04:18,459 --> 00:04:22,419
large multinational media organizations
that operate in dozens of countries we

57
00:04:22,419 --> 00:04:27,099
had really small kind of subject area
focused organizations and we also

58
00:04:27,100 --> 00:04:31,650
intentionally wanted to serve a
journalist who covered a range of eats

59
00:04:31,650 --> 00:04:34,710
rights and not just the people who have
perhaps the highest sensitivity

60
00:04:34,710 --> 00:04:39,310
information but also kind of just doing
sort of regular regular reporting

61
00:04:39,310 --> 00:04:43,330
so we did have some people who were
covering government for example you know

62
00:04:43,330 --> 00:04:47,140
we also have others who are doing sort
of lifestyle and city issues even within

63
00:04:47,140 --> 00:04:51,950
this we did find some pretty interesting
pretty interesting anecdotes come out

64
00:04:51,950 --> 00:04:55,349
for example we did have one participants
who reported that his home had been

65
00:04:55,350 --> 00:04:58,980
broken into and only his laptop was
stolen

66
00:04:58,980 --> 00:05:04,650
another reported in working in doing
community telephone communications with

67
00:05:04,650 --> 00:05:08,710
colleagues and services overseas that
technical glitches would often come up

68
00:05:08,710 --> 00:05:12,580
when they started talking about
sensitive information so online or call

69
00:05:12,580 --> 00:05:16,640
is being dropped and others actually
reported being physically surveilled as

70
00:05:16,640 --> 00:05:20,080
if they were aware that they were being
followed and resources are being

71
00:05:20,080 --> 00:05:21,070
followed

72
00:05:21,070 --> 00:05:24,610
restructure this in terms of kind of
general practices security concerns

73
00:05:24,610 --> 00:05:29,490
defensive strategies and also maybe some
computer security needs that that were

74
00:05:29,490 --> 00:05:33,150
as yet unidentified right things that
have an article have been reported are

75
00:05:33,150 --> 00:05:38,900
identified up to this point in terms of
general practices we found that

76
00:05:38,900 --> 00:05:42,780
participants use a wide range of
communication methods so email as a mass

77
00:05:42,780 --> 00:05:47,270
sometimes even social media telephone
but in general

78
00:05:47,270 --> 00:05:51,640
their tendency was to lean towards the
preference of the source we heard

79
00:05:51,640 --> 00:05:55,469
repeatedly that you know the main thing
was even don't want to raise any

80
00:05:55,470 --> 00:06:00,540
barriers to communication with us or so
is one of our participants noted you

81
00:06:00,540 --> 00:06:03,090
know there's also the idea that maybe
the source understands better than you

82
00:06:03,090 --> 00:06:06,810
do what their weather threat model is
and so you know we're going to go along

83
00:06:06,810 --> 00:06:11,880
with the source feels comfortable also
another important finding was that

84
00:06:11,880 --> 00:06:16,710
long-term sources are by far the
dominant case when doing daily

85
00:06:16,710 --> 00:06:20,909
journalism so of course we're all
familiar with you know snowden kind of

86
00:06:20,910 --> 00:06:25,660
big document that drops the reality is
that this is a very very small portion

87
00:06:25,660 --> 00:06:30,360
of how data journalism even
investigative journalism accountability

88
00:06:30,360 --> 00:06:35,990
journalism really happens and so you
know that also raises

89
00:06:35,990 --> 00:06:40,530
sort of a new set of security issues in
and how do you kind of maintain you know

90
00:06:40,530 --> 00:06:44,200
protect the identity of a source perhaps
over a long-term digital communication

91
00:06:44,200 --> 00:06:49,039
audio recording and note-taking digital
note-taking with a primary for his forms

92
00:06:49,040 --> 00:06:53,280
of interview documentation we had a lot
of people using their phones to record

93
00:06:53,280 --> 00:06:58,500
interviews digital note-taking sometimes
on a hard drive sometimes turn down with

94
00:06:58,500 --> 00:07:02,770
through third-party services and that
was also really common thing there's a

95
00:07:02,770 --> 00:07:09,130
lot of yousef third-party cloud services
to to kind of think information and

96
00:07:09,130 --> 00:07:14,020
we'll talk about that a little bit more
later but very few of our respondents I

97
00:07:14,020 --> 00:07:18,390
you know articulated active security
concerns about using third-party

98
00:07:18,390 --> 00:07:23,750
services in terms of the security of
their information and the concerns that

99
00:07:23,750 --> 00:07:28,280
they did have the primary concern if
information will be to be weird to be

100
00:07:28,280 --> 00:07:33,169
exposed was the effect it will have on a
source and that fact could range from

101
00:07:33,170 --> 00:07:36,530
you know everything in the most extreme
cases to a source being jailed or

102
00:07:36,530 --> 00:07:42,059
suffering physical harm but you know
just a sort of plain old embarrassment

103
00:07:42,060 --> 00:07:44,990
you know that there is a lot of
information that they have much of which

104
00:07:44,990 --> 00:07:50,490
often doesn't make it into a published
story and then you know is is retained

105
00:07:50,490 --> 00:07:56,240
but you were to be exposed wood would be
highly embarrassing to the source and a

106
00:07:56,240 --> 00:08:00,340
side of the sort of you know personal
feelings about having damaged someone

107
00:08:00,340 --> 00:08:09,590
else if the loss of credibility the idea
that if you burning a source furnace or

108
00:08:09,590 --> 00:08:14,750
other people who talk to you right
you'll become known as somebody who does

109
00:08:14,750 --> 00:08:17,490
that and it's going to hurt your ability
to do your job it's gonna hurt the

110
00:08:17,490 --> 00:08:21,650
reputation of your organization and so
that's a that's a really big problem

111
00:08:21,650 --> 00:08:25,969
government identification of sources was
also something that came up as kind of a

112
00:08:25,970 --> 00:08:30,900
background anxiety as well as
disciplinary action that both of the

113
00:08:30,900 --> 00:08:34,409
journalists might sort might face within
their organization for having failed to

114
00:08:34,409 --> 00:08:38,240
protect their information out quickly
but also disciplinary actions

115
00:08:38,240 --> 00:08:42,219
again at the source might suffer such as
losing their job you know losing

116
00:08:42,219 --> 00:08:47,170
standing within their community and some
additional concerns over a little bit

117
00:08:47,170 --> 00:08:51,000
more of a surprise where the loss of
competitive advantage this was again

118
00:08:51,000 --> 00:08:55,860
something that was forced to repeatedly
and that was again partly due to the

119
00:08:55,860 --> 00:09:00,200
idea that with if you lose credibility
you know where the source that sources

120
00:09:00,200 --> 00:09:03,000
gonna take their information somewhere
else right in your competitors gonna get

121
00:09:03,000 --> 00:09:06,910
the story that you thought you had a lot
on but additionally that here just kind

122
00:09:06,910 --> 00:09:09,199
of generally sloppy with your
information

123
00:09:09,200 --> 00:09:13,640
a competitor might discover what you're
working on right and b2 publication and

124
00:09:13,640 --> 00:09:17,970
you know that's a case where foreign
organization is also an individual using

125
00:09:17,970 --> 00:09:21,029
credit you know you can have a case for
you spent weeks months sometimes years

126
00:09:21,029 --> 00:09:26,120
on a story and you essentially lose the
advantage of that because someone else

127
00:09:26,120 --> 00:09:33,649
beat you to the punch and financial
consequences so we did have some sort of

128
00:09:33,649 --> 00:09:37,550
talk about having potentially
embarrassing information about a major

129
00:09:37,550 --> 00:09:41,529
advertisers and that if that were known
and I might influence the business

130
00:09:41,529 --> 00:09:48,250
relationship that the organization had
been that advertiser in terms of

131
00:09:48,250 --> 00:09:52,870
strategy is that we found journalists
employing the vast majority of them are

132
00:09:52,870 --> 00:09:57,270
non technical meeting with sources in
person was a very very common practice

133
00:09:57,270 --> 00:10:01,899
but it was almost it was rarely
articulated as a security strategy for

134
00:10:01,899 --> 00:10:06,700
the most part it was because they said
we get our stories this way you get

135
00:10:06,700 --> 00:10:11,940
better stories when you talk to people
face to face something so maybe some

136
00:10:11,940 --> 00:10:16,240
possibly has a security property
although he later that not a complete

137
00:10:16,240 --> 00:10:20,459
security property but I do things like
using code names and no ticking for

138
00:10:20,459 --> 00:10:23,959
example so you know not writing down
someone's true name maybe only

139
00:10:23,959 --> 00:10:28,170
describing them with you know salient
details that they feel are only sort of

140
00:10:28,170 --> 00:10:32,719
interpretable themselves in an extreme
case using kind of intermediaries and

141
00:10:32,720 --> 00:10:36,529
preset protocols to arrange meetings and
make contact

142
00:10:36,529 --> 00:10:41,670
authentication which is actually also a
really big thing that came up so in this

143
00:10:41,670 --> 00:10:45,260
case being contacted by somebody from a
sort of unrecognized

144
00:10:45,260 --> 00:10:51,880
you know digital digital identity and
the journalists saying okay well if

145
00:10:51,880 --> 00:10:56,890
you're really really are who you say you
are then you know post this line to you

146
00:10:56,890 --> 00:11:01,850
know your Twitter account at this time
and using that as a as a form of hot and

147
00:11:01,850 --> 00:11:07,590
hot indication looking at the technical
strategies they use the most common by

148
00:11:07,590 --> 00:11:13,850
far was encrypted chat and email but
importantly this attended to apply to

149
00:11:13,850 --> 00:11:18,370
communications within a team or
organization right so you know maybe in

150
00:11:18,370 --> 00:11:22,630
talking to each other throughout the day
you know this is a journalist sees but

151
00:11:22,630 --> 00:11:26,850
it was much less common in communication
with sources some file encryption as

152
00:11:26,850 --> 00:11:28,310
well so you know

153
00:11:28,310 --> 00:11:33,030
correction device encryption thing they
didn't come up a lot where things like

154
00:11:33,030 --> 00:11:39,100
tour bTW and other secure messaging apps
and you know we have again all of the

155
00:11:39,100 --> 00:11:44,700
details and the use in this paper so
obviously we're interested in why

156
00:11:44,700 --> 00:11:47,830
journalists aren't using his computer
security tools despite the fact that

157
00:11:47,830 --> 00:11:52,640
they have these these needs and there
are tools out there but we also

158
00:11:52,640 --> 00:11:56,520
identified some of the main themes
around why they did you security tools

159
00:11:56,520 --> 00:11:59,780
and for the most part I had to do with
coming into direct contact with people

160
00:11:59,780 --> 00:12:04,780
who explained how to use them or ask
them to use them so very often it was

161
00:12:04,780 --> 00:12:09,260
because I source required it so that I
won't talk to you unless you can do

162
00:12:09,260 --> 00:12:13,200
whatever and chattering email or they
had had some kind of explicit digital

163
00:12:13,200 --> 00:12:17,220
security training to familiarize them
with these tools so some some of them

164
00:12:17,220 --> 00:12:20,530
are internal or external basically
someone had said to this person directly

165
00:12:20,530 --> 00:12:26,500
you should be doing this and here's how
we're participants did make efforts to

166
00:12:26,500 --> 00:12:30,050
protect information there was a sort of
general awareness of like I should be

167
00:12:30,050 --> 00:12:33,760
doing this and I know I know I should be
doing this and maybe I tried to do this

168
00:12:33,760 --> 00:12:39,800
you know a couple of things here and
there but as it was earlier often

169
00:12:39,800 --> 00:12:44,270
incomplete right so they might try to
protect information in one contacts and

170
00:12:44,270 --> 00:12:48,069
buy it but leave it vulnerable and
another so you know going to me to

171
00:12:48,070 --> 00:12:52,470
source in person by taking their their
mobile phone along a mobile device along

172
00:12:52,470 --> 00:12:56,730
or even sort of trying to protect their
communications with that person but then

173
00:12:56,730 --> 00:13:01,130
doing a phone you using a phone to
record the interview which may or may

174
00:13:01,130 --> 00:13:06,779
not be sinks to like iCloud or something
like that so even though you know the

175
00:13:06,779 --> 00:13:09,550
tools are a bus it's very often
difficult it is clear that you don't

176
00:13:09,550 --> 00:13:13,240
have a hard time kind of kind of putting
them together

177
00:13:13,240 --> 00:13:17,529
them together in a way that really
effectively protects all of protecting

178
00:13:17,529 --> 00:13:24,279
against all of the risks they face so in
terms of the obstacles again a really

179
00:13:24,279 --> 00:13:28,519
common theme that came up was the
digital divide is a very significant

180
00:13:28,519 --> 00:13:32,980
barrier to better communication security
because again journalists are often

181
00:13:32,980 --> 00:13:38,070
going with the sources comfortable with
what the source prefers so its

182
00:13:38,070 --> 00:13:41,500
participants noted you know most of the
sensitive sources I worked with are also

183
00:13:41,500 --> 00:13:45,240
people who probably aren't very tech
savvy I don't know what is secured non

184
00:13:45,240 --> 00:13:49,220
intimidating Lee tacky way would be to
communicate with them some of them don't

185
00:13:49,220 --> 00:13:51,930
even necessarily have email addresses

186
00:13:51,930 --> 00:13:57,790
another big issue was the absence of
clear authoritative indicators about the

187
00:13:57,790 --> 00:14:02,769
protections offered by particular tools
so as I'm sure many of you are where you

188
00:14:02,769 --> 00:14:07,839
know there are lots and lots of new
services chat services you know absolve

189
00:14:07,839 --> 00:14:11,680
this that have come out in the last
couple of years in particular saying you

190
00:14:11,680 --> 00:14:17,089
know this is accurate as private as a
protected and you don't have a timer the

191
00:14:17,089 --> 00:14:21,180
expertise to figure out which which ones
are actually worth using

192
00:14:21,180 --> 00:14:25,930
and even if the ones that might be
considered you know that may be

193
00:14:25,930 --> 00:14:29,930
considered quality really understand the
nuances of what they do and don't

194
00:14:29,930 --> 00:14:33,359
protect right and so how to actually
operationalize their use in a way that

195
00:14:33,360 --> 00:14:38,089
successfully protect their information
in the way that they need another thing

196
00:14:38,089 --> 00:14:42,279
was a lack of institutional support so

197
00:14:42,279 --> 00:14:46,550
you know for obvious security reasons a
lot of reporters don't have admin rights

198
00:14:46,550 --> 00:14:50,649
on their work machines right at a
sensible thing to do at the same time

199
00:14:50,649 --> 00:14:55,240
they also don't necessarily they often
don't have you know secure

200
00:14:55,240 --> 00:14:58,550
communications tools installed in as
machines for them even if they can get

201
00:14:58,550 --> 00:15:02,680
permission to install it they don't a
lot of folks don't how somebody or at

202
00:15:02,680 --> 00:15:10,189
least I mean whose job it is to help an
isolated practices in making sure that

203
00:15:10,189 --> 00:15:12,899
are configured correctly etcetera
etcetera so there's a real lack of

204
00:15:12,899 --> 00:15:20,480
resources the institutional level as
well so why don't journalists security

205
00:15:20,480 --> 00:15:25,339
tools I think there's a lot of talk
about usability and and while that does

206
00:15:25,339 --> 00:15:29,860
remain an issue the truth is actually
that journalists used terrible terrible

207
00:15:29,860 --> 00:15:35,819
and usable software all the time and
they fit perfectly well but there's a

208
00:15:35,819 --> 00:15:38,920
big issue of how these things fit into
the journalistic process are sometimes

209
00:15:38,920 --> 00:15:44,180
impede the journalistic process for
example document drops you know might be

210
00:15:44,180 --> 00:15:49,099
great for even for a source like snowden
but they're really not great for the

211
00:15:49,100 --> 00:15:54,509
long-term source that the more common
situation of long-term sources that you

212
00:15:54,509 --> 00:15:57,550
know that don't initially provide
sensitive information so this is a

213
00:15:57,550 --> 00:16:00,949
really important point I think which is
it you never really know when a source

214
00:16:00,949 --> 00:16:05,849
is going to turn sensitive you know most
sensitive sources are the result of

215
00:16:05,850 --> 00:16:09,529
long-term relationships that have been
cultivated over time which means that

216
00:16:09,529 --> 00:16:13,930
you know you might be having a
communication with somebody in all of a

217
00:16:13,930 --> 00:16:17,709
sudden they say something that is you
know that proves to be approved in

218
00:16:17,709 --> 00:16:21,128
retrospect even to be centered over
eight it by that time it's too late to

219
00:16:21,129 --> 00:16:25,120
switch communication channels but that's
very much how that that kind of

220
00:16:25,120 --> 00:16:31,819
information tends to happen the other
thing is that in the vast majority of

221
00:16:31,819 --> 00:16:33,740
cases on a daily basis

222
00:16:33,740 --> 00:16:38,089
you know journalists depend for
authentication for verification

223
00:16:38,089 --> 00:16:42,089
unknowing their sources real identity
yes of course it's possible to

224
00:16:42,089 --> 00:16:46,850
authenticate information even if you
don't know a person's real identity but

225
00:16:46,850 --> 00:16:47,570
it takes

226
00:16:47,570 --> 00:16:51,880
much larger investment of time and
effort to do that and it's simply not

227
00:16:51,880 --> 00:16:58,140
practical to do that for every single
story every single day just a point of

228
00:16:58,140 --> 00:17:02,439
clarification you know so I mean is one
of our participants out here you know if

229
00:17:02,440 --> 00:17:05,209
I don't know who they are and I can
check their background I'm not gonna use

230
00:17:05,209 --> 00:17:10,380
the information right nine times out of
ten it's not gonna be worth it to to do

231
00:17:10,380 --> 00:17:15,490
that actually work and discipline and
verification about terminology here

232
00:17:15,490 --> 00:17:20,490
journalists talk about an anonymous
source saying what they mean is I did

233
00:17:20,490 --> 00:17:24,500
not publish their name it does not mean
they don't know who that person is

234
00:17:24,500 --> 00:17:28,780
anonymous sources in journalism just
means I didn't print it or put it online

235
00:17:28,780 --> 00:17:34,740
but that reporter definitely knows who
that person is editor probably knows who

236
00:17:34,740 --> 00:17:40,710
that person is just sort of facilitate
the conversation there and in the

237
00:17:40,710 --> 00:17:43,910
confirmation and verification is really
at the heart of a journalist you every

238
00:17:43,910 --> 00:17:49,150
day so there's kind of a quick my
colleague uses which is you know the

239
00:17:49,150 --> 00:17:53,020
rules of journalism is if your mother
tells you she loves you confirm it right

240
00:17:53,020 --> 00:17:56,220
so you can never afford to take someone
at their word which means that this

241
00:17:56,220 --> 00:17:59,430
process of finding out whether or not
they have the information they say they

242
00:17:59,430 --> 00:18:06,160
do it whether or not they're really
important so thinking about future tool

243
00:18:06,160 --> 00:18:10,740
design and some of the work that we see
is you know could be informed by the by

244
00:18:10,740 --> 00:18:17,110
what we found here is to think about the
the barriers that are raised to people

245
00:18:17,110 --> 00:18:22,300
who are less expert on lower-income
icing computer security tools right a

246
00:18:22,300 --> 00:18:25,060
lot of people are working with feature
phones they're working with SMS they

247
00:18:25,060 --> 00:18:30,070
don't have exclusive control over a
computer so what are the opportunities

248
00:18:30,070 --> 00:18:31,240
for a meeting

249
00:18:31,240 --> 00:18:34,690
secure communications tools that work
for people who are existing in the

250
00:18:34,690 --> 00:18:40,100
circumstances are a lot of the people
that journalists are working with and

251
00:18:40,100 --> 00:18:44,110
beyond usability also thinking about
stability and reliability in a

252
00:18:44,110 --> 00:18:47,179
journalistic organizations are
businesses even if many of them are not

253
00:18:47,180 --> 00:18:49,470
very profitable ones

254
00:18:49,470 --> 00:18:53,780
and you know what time accessibility
consistency is important for them which

255
00:18:53,780 --> 00:18:57,639
means that they have the same concerns
as other businesses when adopting

256
00:18:57,640 --> 00:19:01,240
software which means that even if the
software is free it's obviously not free

257
00:19:01,240 --> 00:19:05,740
to implement right it's not free to
deploy to train to maintain very often

258
00:19:05,740 --> 00:19:08,830
they don't necessarily have the
connections at the service connections

259
00:19:08,830 --> 00:19:15,470
to have someone who is going to be able
to catch support etc that . software

260
00:19:15,470 --> 00:19:22,140
right so it's very difficult for
organizations to adopt these things that

261
00:19:22,140 --> 00:19:27,130
we sort of identified as existing unmet
needs that hadn't already been clear

262
00:19:27,130 --> 00:19:32,010
before you know it and sort of obvious
risks with knowledge management so it

263
00:19:32,010 --> 00:19:37,120
came up many times as issue of
journalists when they report on stories

264
00:19:37,120 --> 00:19:40,070
they collect a lot of information from
different places across different media

265
00:19:40,070 --> 00:19:43,590
gonna be web footings Notes emails you
know

266
00:19:43,590 --> 00:19:48,760
audiophiles all of these things and
that's touring sinking searching tagging

267
00:19:48,760 --> 00:19:53,789
these these materials were a really I
was just a general challenge of doing

268
00:19:53,789 --> 00:19:58,799
their work in salon of the ad hoc
solutions that we sought to dealing with

269
00:19:58,799 --> 00:20:02,940
our where security vulnerabilities were
introduced so that's what we saw a lot

270
00:20:02,940 --> 00:20:05,750
of third-party a lot of cloud software
come in

271
00:20:05,750 --> 00:20:10,289
was trying to address these issues not
so much as computer security thing but

272
00:20:10,289 --> 00:20:16,690
audio transcription was also just kind
of a big you know that that's a nut to

273
00:20:16,690 --> 00:20:22,740
crack but you know highlighted here by
one of our respondents was you know

274
00:20:22,740 --> 00:20:26,360
someone who had worked in the legal
field for a while you know as a lawyer

275
00:20:26,360 --> 00:20:29,790
you know what say you have a massive
case you the document under 15,000

276
00:20:29,790 --> 00:20:32,710
documents their programs to help you
consolidate and put them into a secure

277
00:20:32,710 --> 00:20:37,220
database I don't know of anything like
that for journalism so this is a really

278
00:20:37,220 --> 00:20:41,429
massive opportunity to not only remove a
whole slew of vulnerabilities that

279
00:20:41,429 --> 00:20:46,030
journalists are creating when they do
this work but also to make sure that the

280
00:20:46,030 --> 00:20:50,260
system that does fill that gap is one
that has strong security properties

281
00:20:50,260 --> 00:20:55,360
secure first contact and managing the
method the tension between anonymity and

282
00:20:55,360 --> 00:20:58,469
authentication came up more than once

283
00:20:58,470 --> 00:21:01,750
responded noting that the first contact
is never very rarely anonymous are

284
00:21:01,750 --> 00:21:07,610
protected it's very difficult to have
that secure and that's often because it

285
00:21:07,610 --> 00:21:11,909
has also because especially in the
Western contacts in journalism more

286
00:21:11,910 --> 00:21:17,350
often than not it is journalists who are
contacting sources first right so again

287
00:21:17,350 --> 00:21:22,159
the major case is you're covering a
story you're probably your first caller

288
00:21:22,160 --> 00:21:26,340
first contacted you within existing
source on that topic and then you're

289
00:21:26,340 --> 00:21:30,100
going to find out from there but most of
it is not people coming to you and so in

290
00:21:30,100 --> 00:21:36,020
that sense it again very source
dependent problems and other things that

291
00:21:36,020 --> 00:21:39,400
i dont metadata protection so again
knowing that especially with long-term

292
00:21:39,400 --> 00:21:43,710
sources you know we don't have widely
adopted ways of communicating that

293
00:21:43,710 --> 00:21:49,090
protect mad at us you know if you do a
really good job for a long time at one

294
00:21:49,090 --> 00:21:53,879
time it feels kind of all falls apart as
a deeper exploration of the journalistic

295
00:21:53,880 --> 00:21:59,610
process and the needs of sources so most
of the folks that we spoke with were not

296
00:21:59,610 --> 00:22:03,280
dealing with a lot of video and photo
for example right and obviously those

297
00:22:03,280 --> 00:22:07,160
needs the table mediator dealing with
changes they need that they have in

298
00:22:07,160 --> 00:22:10,610
terms of security it's something very
different to talk about and correcting

299
00:22:10,610 --> 00:22:16,780
things when you've got hours and hours
of video footage video it or you know

300
00:22:16,780 --> 00:22:20,000
for example you can't exactly use a
pseudonym when you have somebody's face

301
00:22:20,000 --> 00:22:22,740
in material

302
00:22:22,740 --> 00:22:27,040
and then finally understanding the role
of journalists organizations and how

303
00:22:27,040 --> 00:22:31,889
some of some changes within that might
be able to help inform that so are going

304
00:22:31,890 --> 00:22:35,309
this is a sad it was really to kind of
try to bring together these two

305
00:22:35,309 --> 00:22:40,370
communities and and and highlight some
of you that we may be able to make

306
00:22:40,370 --> 00:22:52,300
progress on this front so that's all in
question

307
00:22:52,300 --> 00:23:22,800
over the age of 21

308
00:23:22,800 --> 00:23:29,659
been convicted of a firearm whatever
those sorts of things

309
00:23:29,660 --> 00:23:37,320
technologies be useful to a journalist
in considering a source to be able to

310
00:23:37,320 --> 00:23:43,250
prove that this person is in this role
without knowing who that person is

311
00:23:43,250 --> 00:23:48,370
culturally so foreign to a journalist
that they would never even consider a

312
00:23:48,370 --> 00:23:52,800
technology like that I think it's a
really good question I mean I think you

313
00:23:52,800 --> 00:23:56,800
know definitely but I it's hard for me
to think about what the technical ways

314
00:23:56,800 --> 00:23:59,830
of doing that might be a mean if you
think about just the simple fact of like

315
00:23:59,830 --> 00:24:04,870
somebody having a like at whatever email
address right that's a sort of simple

316
00:24:04,870 --> 00:24:07,389
way of like if I successfully
communicate with you there

317
00:24:07,390 --> 00:24:11,180
you know unless you are super
sophisticated probably you know you're

318
00:24:11,180 --> 00:24:15,980
there so no and I think actually in fact
this is just a lot of journalists would

319
00:24:15,980 --> 00:24:20,100
welcome that opportunity right if I
could know that this person you know

320
00:24:20,100 --> 00:24:25,240
this aspect of their credentials is no
provable without having to know who they

321
00:24:25,240 --> 00:24:27,940
are I think that would be I think that
would be very welcome

322
00:24:27,940 --> 00:24:31,750
technology does exist today it's not in
widespread use but it does exist

323
00:24:31,750 --> 00:24:34,550
yeah no I mean I as i say i mean I think
it's I think it's just a question of

324
00:24:34,550 --> 00:24:40,750
making sure that that maps to the
characteristics that are important to

325
00:24:40,750 --> 00:24:48,660
journalists to verify right I was
wondering about forward secrecy and kind

326
00:24:48,660 --> 00:24:52,800
of ephemeral tools is that on the radar
of journalists at all cuz there's kind

327
00:24:52,800 --> 00:24:56,780
of a big divide between tools of the
femoral jet tools

328
00:24:56,780 --> 00:25:01,550
my message tech secure and then pgp a
lot of times saying journalists are

329
00:25:01,550 --> 00:25:04,680
worried about making sure that they keep
a record of everything and organize it

330
00:25:04,680 --> 00:25:09,180
was there did they think about forward
secrecy at all is that maybe they don't

331
00:25:09,180 --> 00:25:13,580
know the term but do they think in those
general idea yeah I mean I think the way

332
00:25:13,580 --> 00:25:17,330
that they think in foreign secrecy is
you know somebody finds out after the

333
00:25:17,330 --> 00:25:24,010
fact I was talking to the person they're
gonna get in trouble and I think you

334
00:25:24,010 --> 00:25:32,520
know I think I mean definitely chat
tools I mean have been the thing that I

335
00:25:32,520 --> 00:25:38,110
know have have had the most success in
using but a lot of that has to do with

336
00:25:38,110 --> 00:25:42,370
kind of the the lower overhead and a
lower advance work that has to do with

337
00:25:42,370 --> 00:25:46,919
it right you know if you look at
something like you know right it's a

338
00:25:46,920 --> 00:25:49,850
browser plug-in like most people can do
that they can probably do it without

339
00:25:49,850 --> 00:25:53,649
admin rights it's pretty simple to be
like and now go to the chat room with

340
00:25:53,650 --> 00:25:58,160
this name you know as opposed to
explaining a whole occasion flow or

341
00:25:58,160 --> 00:26:03,050
signup sign up processes are a barrier
you know there's something else that I

342
00:26:03,050 --> 00:26:07,340
don't like part of this is also kind of
two cases when you're dealing with

343
00:26:07,340 --> 00:26:10,679
sensitive information and one of them is
that the source and always know that

344
00:26:10,680 --> 00:26:17,280
what they're sharing is sensitive and so
this idea switching channels right and

345
00:26:17,280 --> 00:26:23,590
there's kinda like whoa so there is that
notion but it's not something that I

346
00:26:23,590 --> 00:26:26,879
mean having records is important but I
mean I don't know anybody who wouldn't

347
00:26:26,880 --> 00:26:31,680
be just taking notes on it at the same
time if it was even a recorded telephone

348
00:26:31,680 --> 00:26:45,370
conversations but they might just taking
notes so surprising is just how

349
00:26:45,370 --> 00:26:52,709
self-serving some of the answers
journalists gave securing the sources

350
00:26:52,710 --> 00:26:57,240
say we don't want to speak to source we
don't want to create barriers even if

351
00:26:57,240 --> 00:26:59,830
the source perhapsmaybe you do you think
there

352
00:26:59,830 --> 00:27:06,629
r ethical issues how journalists should
protect sources I mean absolutely like I

353
00:27:06,630 --> 00:27:10,840
think I think it is an ethical issue you
know some of the work that I have done

354
00:27:10,840 --> 00:27:15,360
this was exactly be for that reason I
think the problem is that there's a

355
00:27:15,360 --> 00:27:19,610
tension between you know if if we
restrict ourselves to covering only

356
00:27:19,610 --> 00:27:24,678
sensitive stories that involve people
who also had the saudi and the resources

357
00:27:24,679 --> 00:27:29,500
right i mean we're not just talking
about you know intellectual resources

358
00:27:29,500 --> 00:27:37,159
talking about financial resources do you
own a computer access to a computer you

359
00:27:37,159 --> 00:27:40,600
know then we'd be missing a lot of the
most important accountability journalism

360
00:27:40,600 --> 00:27:44,719
you know and and sort of coverage of the
most important populations right so many

361
00:27:44,720 --> 00:27:48,470
of our most vulnerable populations who
simply need to be covered

362
00:27:48,470 --> 00:27:55,159
you know we are the ones who don't have
access to these tools and resources and

363
00:27:55,159 --> 00:27:58,019
you know and frankly like there's
there's a mean journalistic

364
00:27:58,019 --> 00:28:01,460
organizations themselves are strapped
rate you know you can't necessarily do

365
00:28:01,460 --> 00:28:03,799
you can always fly to Hong Kong to meet
the source

366
00:28:03,799 --> 00:28:07,100
you know you may not even be able to
drive up state to be the source so

367
00:28:07,100 --> 00:28:10,219
there's there's definitely attention
there and there are ethical implications

368
00:28:10,220 --> 00:28:14,320
but I think you know the end of the day
for most journalists about it's about

369
00:28:14,320 --> 00:28:17,399
doing your job and everyone has
concerned about

370
00:28:17,399 --> 00:28:22,418
unduly exposing people but at the same
time you know it's at some point if you

371
00:28:22,419 --> 00:28:34,659
stop doing it work like the list is a
clear where the journalists themselves

372
00:28:34,659 --> 00:28:41,320
strength of published research so the
Jets like

373
00:28:41,320 --> 00:28:50,139
article in the internet right about a
year that compares like jets like both

374
00:28:50,139 --> 00:28:55,158
but never published article about which
tools you know you would use to protect

375
00:28:55,159 --> 00:28:59,590
this also

376
00:28:59,590 --> 00:29:02,639
I mean yes there certainly illiteracy I
mean I think the only thing I would say

377
00:29:02,640 --> 00:29:06,330
is that my experience is also that the
computer security security community has

378
00:29:06,330 --> 00:29:13,260
been kind of a literate about the
journalistic process to know if there

379
00:29:13,260 --> 00:29:17,629
are organizations and public action in a
really great comparison chart of various

380
00:29:17,630 --> 00:29:22,779
tools but I mean this is exactly why
we're here is a kind of open up a

381
00:29:22,779 --> 00:29:27,370
dialogue because you know we feel that
there's interest on both sides and yeah

382
00:29:27,370 --> 00:29:33,120
very much looking forward to trying to
facilitate that thank you one more time

