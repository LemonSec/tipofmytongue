1
00:00:02,510 --> 00:00:22,689
<font color="#E5E5E5">his Clarence thank you hi okay</font><font color="#CCCCCC"> hi I'm</font>

2
00:00:19,369 --> 00:00:26,420
Clarence thanks for thanks first thing

3
00:00:22,690 --> 00:00:28,100
for<font color="#E5E5E5"> this talk so I just</font><font color="#CCCCCC"> want to get</font><font color="#E5E5E5"> a</font>

4
00:00:26,420 --> 00:00:29,630
quick poll of the audience how many<font color="#E5E5E5"> of</font>

5
00:00:28,100 --> 00:00:31,789
you have used machine learning before

6
00:00:29,630 --> 00:00:35,839
have played with it like taking classes

7
00:00:31,789 --> 00:00:38,690
or something<font color="#CCCCCC"> oh yeah it's good number so</font>

8
00:00:35,839 --> 00:00:41,170
today I'm going<font color="#E5E5E5"> to be talking about how</font>

9
00:00:38,690 --> 00:00:43,550
<font color="#E5E5E5">to bypass machine learning systems</font>

10
00:00:41,170 --> 00:00:46,129
machine learning<font color="#CCCCCC"> has been has been in</font>

11
00:00:43,550 --> 00:00:50,138
the in<font color="#E5E5E5"> the news and in everywhere in</font>

12
00:00:46,129 --> 00:00:55,909
tech over the last two to five years<font color="#E5E5E5"> I</font>

13
00:00:50,139 --> 00:00:58,339
did a quick crawl off RSA and black hat

14
00:00:55,909 --> 00:01:00,319
the expo floor you know how companies

15
00:00:58,339 --> 00:01:03,440
always have to publish some snippets

16
00:01:00,319 --> 00:01:05,299
about what they do<font color="#CCCCCC"> and I was just</font>

17
00:01:03,440 --> 00:01:07,039
looking for<font color="#E5E5E5"> how many companies claim</font>

18
00:01:05,299 --> 00:01:08,540
they were<font color="#E5E5E5"> doing something in the area of</font>

19
00:01:07,040 --> 00:01:11,300
machine learning or deep learning or

20
00:01:08,540 --> 00:01:14,720
data analytics to solve security

21
00:01:11,300 --> 00:01:17,600
problems so this<font color="#E5E5E5"> is just a particular</font>

22
00:01:14,720 --> 00:01:20,750
instance of a company that claims to do

23
00:01:17,600 --> 00:01:22,640
like deep learning for for mail or

24
00:01:20,750 --> 00:01:25,850
<font color="#E5E5E5">detection and apt attacks that was</font>

25
00:01:22,640 --> 00:01:29,110
interesting<font color="#E5E5E5"> so over the years you can</font>

26
00:01:25,850 --> 00:01:31,669
see the the number<font color="#E5E5E5"> of instances that</font>

27
00:01:29,110 --> 00:01:34,220
companies have claimed to<font color="#CCCCCC"> do machine</font>

28
00:01:31,670 --> 00:01:36,110
learning as<font color="#E5E5E5"> in security i think is</font>

29
00:01:34,220 --> 00:01:39,820
pretty interesting from twenty twelve to

30
00:01:36,110 --> 00:01:44,870
twenty fifteen there was pretty<font color="#CCCCCC"> much a</font>

31
00:01:39,820 --> 00:01:46,550
6x increase and 2016 is it's a bit lower

32
00:01:44,870 --> 00:01:54,350
maybe because people realize<font color="#CCCCCC"> that it's</font>

33
00:01:46,550 --> 00:01:56,149
it's not that easy so this<font color="#CCCCCC"> is a neural</font>

34
00:01:54,350 --> 00:01:58,130
network<font color="#CCCCCC"> i I won't spend</font><font color="#E5E5E5"> too much time</font>

35
00:01:56,150 --> 00:02:00,740
going to detail about what a neural

36
00:01:58,130 --> 00:02:02,780
network does or how it works exactly you

37
00:02:00,740 --> 00:02:05,589
can find<font color="#E5E5E5"> it online yourself but</font>

38
00:02:02,780 --> 00:02:08,590
<font color="#E5E5E5">basically the idea is that you have a</font>

39
00:02:05,590 --> 00:02:12,319
standard infrastructure consisting of

40
00:02:08,590 --> 00:02:14,810
input units and hidden layers and an

41
00:02:12,319 --> 00:02:15,810
output layers and activation functions

42
00:02:14,810 --> 00:02:18,750
which are each one

43
00:02:15,810 --> 00:02:21,330
the small circles there and each one of

44
00:02:18,750 --> 00:02:24,599
these units is connected to each other

45
00:02:21,330 --> 00:02:27,180
by by a weight and so when you want to

46
00:02:24,599 --> 00:02:31,530
find the output of a neural net all you

47
00:02:27,180 --> 00:02:34,470
have to do is to check which activation

48
00:02:31,530 --> 00:02:37,530
functions are actually activated and use

49
00:02:34,470 --> 00:02:40,230
and then you would<font color="#E5E5E5"> result the result</font>

50
00:02:37,530 --> 00:02:41,849
would be this output vector called

51
00:02:40,230 --> 00:02:43,470
logits and then you would basically<font color="#E5E5E5"> feed</font>

52
00:02:41,849 --> 00:02:46,829
it through a softmax function to get the

53
00:02:43,470 --> 00:02:52,049
prediction and so neural nets are all

54
00:02:46,830 --> 00:02:53,610
about distributing blame so during<font color="#CCCCCC"> the</font>

55
00:02:52,049 --> 00:02:58,140
training of the model when you feed in

56
00:02:53,610 --> 00:02:59,940
data when you get a wrong prediction

57
00:02:58,140 --> 00:03:02,339
during the training phase you would

58
00:02:59,940 --> 00:03:04,019
basically go through the model do it in

59
00:03:02,340 --> 00:03:06,290
the process of back propagation to

60
00:03:04,019 --> 00:03:09,360
assign blame to see which units were

61
00:03:06,290 --> 00:03:11,819
were responsible for making this wrong

62
00:03:09,360 --> 00:03:13,650
decision<font color="#E5E5E5"> and so when you find a units</font>

63
00:03:11,819 --> 00:03:17,040
that make the wrong decision then<font color="#E5E5E5"> you</font>

64
00:03:13,650 --> 00:03:19,110
<font color="#E5E5E5">would basically reduce the amount of</font>

65
00:03:17,040 --> 00:03:21,540
decision-making power that they have by

66
00:03:19,110 --> 00:03:24,390
decreasing their weights and biases by

67
00:03:21,540 --> 00:03:25,798
the upper body appropriate<font color="#CCCCCC"> amount to how</font>

68
00:03:24,390 --> 00:03:29,190
much they contributed to this wrong

69
00:03:25,799 --> 00:03:31,560
decision<font color="#E5E5E5"> and of course</font><font color="#CCCCCC"> you can do</font><font color="#E5E5E5"> this</font>

70
00:03:29,190 --> 00:03:32,730
efficiently with optimization algorithms

71
00:03:31,560 --> 00:03:34,739
such as stochastic gradient descent

72
00:03:32,730 --> 00:03:38,000
which you may<font color="#CCCCCC"> have heard of in other</font>

73
00:03:34,739 --> 00:03:40,560
machine learning algorithms as well so

74
00:03:38,000 --> 00:03:43,140
the main<font color="#CCCCCC"> focus on my of my presentation</font>

75
00:03:40,560 --> 00:03:44,790
will be how to bypass this if there any

76
00:03:43,140 --> 00:03:46,858
questions about<font color="#E5E5E5"> how this works at all</font>

77
00:03:44,790 --> 00:03:48,209
i'll be<font color="#E5E5E5"> going to further detail this is</font>

78
00:03:46,859 --> 00:03:50,400
not just about bypassing deep learning

79
00:03:48,209 --> 00:03:52,560
system it's also about bypassing just

80
00:03:50,400 --> 00:03:56,069
any general machine learning algorithm

81
00:03:52,560 --> 00:03:59,970
which<font color="#CCCCCC"> i will go into detail later i have</font>

82
00:03:56,069 --> 00:04:01,589
a quick demo here on how to bypass in an

83
00:03:59,970 --> 00:04:04,019
image classification and gene which is

84
00:04:01,590 --> 00:04:07,319
one<font color="#CCCCCC"> of</font><font color="#E5E5E5"> the most classic deep learning</font>

85
00:04:04,019 --> 00:04:10,440
problems the the thing<font color="#E5E5E5"> is that there's</font>

86
00:04:07,319 --> 00:04:14,220
this data set called the CFA are 10 and

87
00:04:10,440 --> 00:04:18,209
what this<font color="#E5E5E5"> is a stress 60,000 images that</font>

88
00:04:14,220 --> 00:04:21,478
are very small 32 by 32 pixels and they

89
00:04:18,209 --> 00:04:24,090
each belong to one of<font color="#E5E5E5"> ten classes so I</font>

90
00:04:21,478 --> 00:04:27,810
chose randomly two classes here a dog

91
00:04:24,090 --> 00:04:31,169
and an automobile or a car so

92
00:04:27,810 --> 00:04:33,840
what I have<font color="#CCCCCC"> here is basically two kinds</font>

93
00:04:31,169 --> 00:04:37,590
of images than the normal and the

94
00:04:33,840 --> 00:04:39,270
adversarial and so I<font color="#E5E5E5"> pre-trained a model</font>

95
00:04:37,590 --> 00:04:42,030
that<font color="#E5E5E5"> would well recognize this properly</font>

96
00:04:39,270 --> 00:04:44,969
so if I if I try to evaluate what this

97
00:04:42,030 --> 00:04:46,559
what this image is it's a dog you can

98
00:04:44,970 --> 00:04:53,280
see from the from the image on<font color="#CCCCCC"> the right</font>

99
00:04:46,560 --> 00:04:55,380
that it is a dog but if you see here dog

100
00:04:53,280 --> 00:04:57,719
f10 also looks<font color="#CCCCCC"> like a top it's a</font><font color="#E5E5E5"> little</font>

101
00:04:55,380 --> 00:04:59,700
<font color="#E5E5E5">bit more noisy but you know it still</font>

102
00:04:57,720 --> 00:05:01,169
looks like<font color="#CCCCCC"> a</font><font color="#E5E5E5"> doctor to you and me if we</font>

103
00:04:59,700 --> 00:05:04,349
actually<font color="#E5E5E5"> run this through</font><font color="#CCCCCC"> the same</font>

104
00:05:01,169 --> 00:05:08,940
evaluation algorithm it says it's a ship

105
00:05:04,350 --> 00:05:10,800
so that's weird right and so this is

106
00:05:08,940 --> 00:05:13,440
<font color="#E5E5E5">just a standard pre-trained model that</font>

107
00:05:10,800 --> 00:05:15,600
you can<font color="#E5E5E5"> download from tensorflow and you</font>

108
00:05:13,440 --> 00:05:18,000
can see that<font color="#E5E5E5"> you can generate</font><font color="#CCCCCC"> images</font>

109
00:05:15,600 --> 00:05:20,580
that trick these models that are trained

110
00:05:18,000 --> 00:05:22,470
using<font color="#CCCCCC"> Google's own engines they have</font>

111
00:05:20,580 --> 00:05:24,090
been trained using gpus for thousands

112
00:05:22,470 --> 00:05:25,530
and thousands<font color="#CCCCCC"> of hours things that I</font>

113
00:05:24,090 --> 00:05:30,030
would<font color="#E5E5E5"> not be able to afford on my own</font>

114
00:05:25,530 --> 00:05:33,450
AWS account similarly for<font color="#E5E5E5"> the automobile</font>

115
00:05:30,030 --> 00:05:34,919
just to<font color="#CCCCCC"> show you again it's a car I run</font>

116
00:05:33,450 --> 00:05:38,400
than the normal image it says<font color="#E5E5E5"> it's</font><font color="#CCCCCC"> an</font>

117
00:05:34,919 --> 00:05:46,919
automobile and I ran the<font color="#E5E5E5"> other one it</font>

118
00:05:38,400 --> 00:05:50,250
says it's a<font color="#E5E5E5"> Pepsi okay</font><font color="#CCCCCC"> so you see it</font>

119
00:05:46,919 --> 00:05:55,799
looks<font color="#E5E5E5"> it looks slightly different but it</font>

120
00:05:50,250 --> 00:05:57,510
says the cat so that's just<font color="#E5E5E5"> some small</font>

121
00:05:55,800 --> 00:05:59,729
example we'll dive into each of these

122
00:05:57,510 --> 00:06:02,960
images and see exactly what is different

123
00:05:59,729 --> 00:06:06,599
exactly how we're generating this later

124
00:06:02,960 --> 00:06:09,599
let's open up a<font color="#E5E5E5"> Python console and let's</font>

125
00:06:06,600 --> 00:06:11,960
look into how these images are different

126
00:06:09,600 --> 00:06:11,960
for each other

127
00:06:15,270 --> 00:06:22,120
sometimes importing the standard

128
00:06:17,410 --> 00:06:25,450
libraries i'm<font color="#E5E5E5"> going to read in the</font>

129
00:06:22,120 --> 00:06:29,590
normal dog image just<font color="#E5E5E5"> going to be a 32</font>

130
00:06:25,450 --> 00:06:33,190
<font color="#CCCCCC">by 32 x 3 vector because there's RGB</font>

131
00:06:29,590 --> 00:06:37,359
values so you can see it's nothing

132
00:06:33,190 --> 00:06:43,780
nothing unexpected you see the shape is

133
00:06:37,360 --> 00:06:47,950
32 x 32 x 3 and the minimum<font color="#CCCCCC"> value is</font><font color="#E5E5E5"> 0</font>

134
00:06:43,780 --> 00:06:50,799
and the<font color="#CCCCCC"> maximum value is 255 as expected</font>

135
00:06:47,950 --> 00:06:54,729
<font color="#E5E5E5">so the values for r g and b brain from 0</font>

136
00:06:50,800 --> 00:07:01,870
to 255<font color="#CCCCCC"> ok so what we're going to do now</font>

137
00:06:54,730 --> 00:07:03,130
is<font color="#E5E5E5"> to read in the adversarial image so i</font>

138
00:07:01,870 --> 00:07:07,180
want to<font color="#E5E5E5"> read in the adversarial image</font>

139
00:07:03,130 --> 00:07:08,980
and<font color="#CCCCCC"> ok so this is what ever it may look</font>

140
00:07:07,180 --> 00:07:12,450
like we're going<font color="#E5E5E5"> to take the diff of</font>

141
00:07:08,980 --> 00:07:15,750
these two and see how<font color="#E5E5E5"> different they are</font>

142
00:07:12,450 --> 00:07:15,750
same shape

143
00:07:32,860 --> 00:07:43,360
ok so the tip is just minus 1 to 2

144
00:07:40,000 --> 00:07:45,819
pixels difference and and yet we can

145
00:07:43,360 --> 00:07:50,860
make the model make a wrong prediction

146
00:07:45,819 --> 00:07:52,780
or a run classification so let's save

147
00:07:50,860 --> 00:08:00,240
this diff and let's see what what the

148
00:07:52,780 --> 00:08:00,239
diff actually looks like<font color="#E5E5E5"> ok</font>

149
00:08:04,580 --> 00:08:10,349
and the difficulty shows up here so we

150
00:08:07,560 --> 00:08:12,720
can see it it's just rubbish it's a

151
00:08:10,350 --> 00:08:14,730
rubbish vector that we can add to<font color="#E5E5E5"> an</font>

152
00:08:12,720 --> 00:08:27,050
image to make the model make a wrong

153
00:08:14,730 --> 00:08:35,190
decision yeah so same<font color="#CCCCCC"> thing for the</font>

154
00:08:27,050 --> 00:08:37,260
other one doc f10 is is another example

155
00:08:35,190 --> 00:08:41,990
of an adversarial image that is

156
00:08:37,260 --> 00:08:41,990
generated<font color="#CCCCCC"> in the same in the same method</font>

157
00:08:42,020 --> 00:08:49,530
and the difference between<font color="#E5E5E5"> da f 1 and f</font>

158
00:08:46,170 --> 00:08:52,349
<font color="#E5E5E5">10 is</font><font color="#CCCCCC"> just how big the perturbation is</font>

159
00:08:49,530 --> 00:08:55,230
so how many pixels it differ from the

160
00:08:52,350 --> 00:08:57,270
original image and the reason why we

161
00:08:55,230 --> 00:08:59,700
want<font color="#E5E5E5"> to change the</font><font color="#CCCCCC"> amount of</font>

162
00:08:57,270 --> 00:09:01,920
perturbation between the<font color="#CCCCCC"> original image</font>

163
00:08:59,700 --> 00:09:05,280
and adversarial images because when you

164
00:09:01,920 --> 00:09:08,790
alter an image x 1 pixel for each color

165
00:09:05,280 --> 00:09:11,610
value then it's not that easy to<font color="#E5E5E5"> be</font>

166
00:09:08,790 --> 00:09:13,860
detected<font color="#CCCCCC"> by by a human but if you if you</font>

167
00:09:11,610 --> 00:09:15,990
change it by like say 10 pixels then

168
00:09:13,860 --> 00:09:17,400
<font color="#CCCCCC">it's more</font><font color="#E5E5E5"> possible that the human will</font>

169
00:09:15,990 --> 00:09:21,240
be<font color="#CCCCCC"> able</font><font color="#E5E5E5"> to tell that something weird is</font>

170
00:09:17,400 --> 00:09:23,490
going on with it but the reason why<font color="#E5E5E5"> you</font>

171
00:09:21,240 --> 00:09:25,560
would<font color="#E5E5E5"> change you would change an image</font>

172
00:09:23,490 --> 00:09:27,750
by a larger amount<font color="#E5E5E5"> is because it's more</font>

173
00:09:25,560 --> 00:09:29,819
likely for you to trick a machine

174
00:09:27,750 --> 00:09:33,720
learning model that something is

175
00:09:29,820 --> 00:09:36,080
actually<font color="#CCCCCC"> different so as you can see</font>

176
00:09:33,720 --> 00:09:40,830
<font color="#CCCCCC">that</font><font color="#E5E5E5"> the diff is is much larger now and</font>

177
00:09:36,080 --> 00:09:43,950
<font color="#E5E5E5">we'll go on so i'll go ahead and explain</font>

178
00:09:40,830 --> 00:09:46,500
how these attack works and and why they

179
00:09:43,950 --> 00:09:48,330
work but first<font color="#E5E5E5"> of all let's look at the</font>

180
00:09:46,500 --> 00:09:50,370
kinds of attack that you can have on on

181
00:09:48,330 --> 00:09:52,980
machine learning models in general there

182
00:09:50,370 --> 00:09:55,020
are three kinds<font color="#E5E5E5"> of attacks you can have</font>

183
00:09:52,980 --> 00:09:57,600
positive attacks and exploratory attacks

184
00:09:55,020 --> 00:10:00,000
closet if attacks just<font color="#E5E5E5"> means that you</font>

185
00:09:57,600 --> 00:10:02,130
have<font color="#CCCCCC"> access to the training the training</font>

186
00:10:00,000 --> 00:10:04,560
stage so in training<font color="#CCCCCC"> a machine learning</font>

187
00:10:02,130 --> 00:10:06,240
model what you usually have in a

188
00:10:04,560 --> 00:10:07,619
supervised case is you have a<font color="#E5E5E5"> bunch of</font>

189
00:10:06,240 --> 00:10:10,380
input data and you want to train the

190
00:10:07,620 --> 00:10:13,410
model to do something that<font color="#CCCCCC"> is that you</font>

191
00:10:10,380 --> 00:10:16,380
do not explicitly program rules for and

192
00:10:13,410 --> 00:10:19,350
this could be the example of if you have

193
00:10:16,380 --> 00:10:23,340
<font color="#E5E5E5">a thousand images of</font><font color="#CCCCCC"> ten classes</font><font color="#E5E5E5"> as you</font>

194
00:10:19,350 --> 00:10:26,280
see as you see before then<font color="#CCCCCC"> you have you</font>

195
00:10:23,340 --> 00:10:28,050
<font color="#CCCCCC">have</font><font color="#E5E5E5"> 600 images of each class and you</font>

196
00:10:26,280 --> 00:10:31,560
would<font color="#E5E5E5"> basically train the model to</font>

197
00:10:28,050 --> 00:10:33,870
recognize each one of these models and

198
00:10:31,560 --> 00:10:35,790
recognize<font color="#CCCCCC"> their characteristics the the</font>

199
00:10:33,870 --> 00:10:37,230
kinds of pixels that make up a dog and

200
00:10:35,790 --> 00:10:40,170
the kinds of pixels that make up a shit

201
00:10:37,230 --> 00:10:41,790
and then you basically go on and go on

202
00:10:40,170 --> 00:10:44,130
to the<font color="#E5E5E5"> training phase after the model is</font>

203
00:10:41,790 --> 00:10:45,660
strained after the model is<font color="#E5E5E5"> trained to</font>

204
00:10:44,130 --> 00:10:47,880
<font color="#E5E5E5">go into the testing phase and then he</font>

205
00:10:45,660 --> 00:10:49,380
will feed it unseen data and see if it

206
00:10:47,880 --> 00:10:51,630
actually generalized as well and makes

207
00:10:49,380 --> 00:10:54,000
the right decisions the exploratory

208
00:10:51,630 --> 00:10:55,170
phase is the export area text are more

209
00:10:54,000 --> 00:10:58,020
interesting because<font color="#E5E5E5"> you don't</font><font color="#CCCCCC"> actually</font>

210
00:10:55,170 --> 00:11:00,569
have<font color="#CCCCCC"> to</font><font color="#E5E5E5"> have access to the training</font>

211
00:10:58,020 --> 00:11:04,710
phase so this is applicable in most

212
00:11:00,570 --> 00:11:06,690
instances let's say you have a an online

213
00:11:04,710 --> 00:11:09,900
<font color="#CCCCCC">Google Translate service that famously</font>

214
00:11:06,690 --> 00:11:12,060
uses machine learning models to to learn

215
00:11:09,900 --> 00:11:13,560
<font color="#E5E5E5">from to learn from its past mistakes and</font>

216
00:11:12,060 --> 00:11:16,410
to make better<font color="#E5E5E5"> predictions as time goes</font>

217
00:11:13,560 --> 00:11:18,150
on you<font color="#E5E5E5"> do</font><font color="#CCCCCC"> not you</font><font color="#E5E5E5"> don't actually have</font>

218
00:11:16,410 --> 00:11:19,920
access to the<font color="#E5E5E5"> training phase because</font>

219
00:11:18,150 --> 00:11:21,600
most of the time when machine learning

220
00:11:19,920 --> 00:11:23,939
models are the plot in<font color="#E5E5E5"> a while they</font>

221
00:11:21,600 --> 00:11:27,600
<font color="#CCCCCC">aren't they don't</font><font color="#E5E5E5"> take in general and</font>

222
00:11:23,940 --> 00:11:29,760
public input in immediate training most

223
00:11:27,600 --> 00:11:31,400
of the time you just get the results and

224
00:11:29,760 --> 00:11:34,380
then the training is done separately

225
00:11:31,400 --> 00:11:36,329
<font color="#E5E5E5">after the data is cleaned so you can</font>

226
00:11:34,380 --> 00:11:38,250
still attack models like that by just

227
00:11:36,330 --> 00:11:42,510
crafting input samples that are more

228
00:11:38,250 --> 00:11:43,830
likely to trick these models and then of

229
00:11:42,510 --> 00:11:45,660
course<font color="#CCCCCC"> you also have the other kind</font><font color="#E5E5E5"> of</font>

230
00:11:43,830 --> 00:11:47,430
attack which is analogous to to<font color="#CCCCCC"> just</font>

231
00:11:45,660 --> 00:11:48,870
DDoS attacks basically we're trying<font color="#E5E5E5"> to</font>

232
00:11:47,430 --> 00:11:51,689
decrease the reliability of these

233
00:11:48,870 --> 00:11:54,420
classifiers and to make them<font color="#CCCCCC"> more</font>

234
00:11:51,690 --> 00:11:56,970
unreliable you want to make everything

235
00:11:54,420 --> 00:12:01,469
for example be classified as a ship no

236
00:11:56,970 --> 00:12:05,130
matter what<font color="#E5E5E5"> it looks like so why can we</font>

237
00:12:01,470 --> 00:12:06,510
do this the<font color="#E5E5E5"> reason that you can trick</font>

238
00:12:05,130 --> 00:12:09,360
machine learning models because they

239
00:12:06,510 --> 00:12:12,300
<font color="#CCCCCC">don't learn like that like how humans</font>

240
00:12:09,360 --> 00:12:14,370
learn and this might be surprising to

241
00:12:12,300 --> 00:12:15,540
too many people<font color="#CCCCCC"> even those working in</font>

242
00:12:14,370 --> 00:12:17,580
the field even those who are very

243
00:12:15,540 --> 00:12:19,530
familiar with machine learning when you

244
00:12:17,580 --> 00:12:22,080
see that classifier successfully

245
00:12:19,530 --> 00:12:23,730
predicts so successfully classifies

246
00:12:22,080 --> 00:12:26,100
images within ninety nine percent

247
00:12:23,730 --> 00:12:29,040
accuracy then you kind of think that

248
00:12:26,100 --> 00:12:29,950
these classifiers are recognizing ships

249
00:12:29,040 --> 00:12:33,250
and dogs<font color="#E5E5E5"> and cats</font>

250
00:12:29,950 --> 00:12:37,060
the same<font color="#CCCCCC"> ways</font><font color="#E5E5E5"> that humans</font><font color="#CCCCCC"> do</font><font color="#E5E5E5"> but they</font>

251
00:12:33,250 --> 00:12:38,410
actually<font color="#E5E5E5"> don't so what classifiers</font><font color="#CCCCCC"> and</font>

252
00:12:37,060 --> 00:12:40,719
deep learning models do is they just

253
00:12:38,410 --> 00:12:44,230
<font color="#E5E5E5">look for characteristics in the pixels</font>

254
00:12:40,720 --> 00:12:46,510
<font color="#E5E5E5">that happen to be common among among</font>

255
00:12:44,230 --> 00:12:47,890
images of the same class and humans

256
00:12:46,510 --> 00:12:50,530
don't think like that<font color="#E5E5E5"> for example when</font>

257
00:12:47,890 --> 00:12:53,920
we look<font color="#E5E5E5"> at a dog even</font><font color="#CCCCCC"> if the the color</font>

258
00:12:50,530 --> 00:12:56,620
of the dog were different even if there

259
00:12:53,920 --> 00:12:58,420
were a large pixel differences from all

260
00:12:56,620 --> 00:13:00,430
examples of dogs that we've seen before

261
00:12:58,420 --> 00:13:02,319
<font color="#E5E5E5">in the past we're still able to</font>

262
00:13:00,430 --> 00:13:04,300
<font color="#E5E5E5">generalize well and look for certain</font>

263
00:13:02,320 --> 00:13:06,430
characteristics of the image there are

264
00:13:04,300 --> 00:13:09,130
more high level that we're able to draw

265
00:13:06,430 --> 00:13:12,430
general associations from and concluded

266
00:13:09,130 --> 00:13:15,010
that<font color="#E5E5E5"> is still a dog so this</font><font color="#CCCCCC"> is this is a</font>

267
00:13:12,430 --> 00:13:17,020
this<font color="#CCCCCC"> is an important thing that we</font>

268
00:13:15,010 --> 00:13:18,550
should take note off no<font color="#E5E5E5"> matter what</font>

269
00:13:17,020 --> 00:13:20,530
<font color="#E5E5E5">we're designing classifiers for we</font>

270
00:13:18,550 --> 00:13:22,630
<font color="#E5E5E5">should make sure that the true positives</font>

271
00:13:20,530 --> 00:13:24,400
are real positives and there can be

272
00:13:22,630 --> 00:13:28,240
false positives in any predictions that

273
00:13:24,400 --> 00:13:29,709
these machine learning models make so

274
00:13:28,240 --> 00:13:33,040
basically<font color="#CCCCCC"> the intuitions for Hari</font>

275
00:13:29,710 --> 00:13:37,210
<font color="#E5E5E5">generate examples for adversarial deep</font>

276
00:13:33,040 --> 00:13:38,860
learning are you run an input model you

277
00:13:37,210 --> 00:13:40,690
want an input sample through the

278
00:13:38,860 --> 00:13:42,850
classifier model if<font color="#E5E5E5"> you don't have</font>

279
00:13:40,690 --> 00:13:45,280
access to the classifier model let's say

280
00:13:42,850 --> 00:13:47,950
you are trying to<font color="#E5E5E5"> do it for</font><font color="#CCCCCC"> a</font><font color="#E5E5E5"> an online</font>

281
00:13:45,280 --> 00:13:49,209
malware classifier that<font color="#E5E5E5"> you don't know</font>

282
00:13:47,950 --> 00:13:50,710
what models are using you don't know

283
00:13:49,210 --> 00:13:52,570
what data they used to train the model

284
00:13:50,710 --> 00:13:55,030
then you just<font color="#CCCCCC"> have to use a substitute</font>

285
00:13:52,570 --> 00:13:56,680
model which are<font color="#E5E5E5"> going to later the</font>

286
00:13:55,030 --> 00:13:59,230
second step is to based on the model

287
00:13:56,680 --> 00:14:01,479
prediction derive a perturbation tensor

288
00:13:59,230 --> 00:14:04,240
<font color="#E5E5E5">or vector or matrix that maximizes the</font>

289
00:14:01,480 --> 00:14:06,040
chances of<font color="#E5E5E5"> Miss classification so I'll</font>

290
00:14:04,240 --> 00:14:08,560
go into more detail about how to do this

291
00:14:06,040 --> 00:14:10,449
later and the third step is to scale

292
00:14:08,560 --> 00:14:12,819
this perturbation tensor by some

293
00:14:10,450 --> 00:14:14,770
arbitrary magnitude the larger magnitude

294
00:14:12,820 --> 00:14:16,150
you scale this by the more chance you

295
00:14:14,770 --> 00:14:19,390
have of tricking the machine learning

296
00:14:16,150 --> 00:14:21,970
model the smaller magnitude you you tune

297
00:14:19,390 --> 00:14:23,380
<font color="#E5E5E5">this by the smaller chance you have of</font>

298
00:14:21,970 --> 00:14:25,000
tricking the machine learning model but

299
00:14:23,380 --> 00:14:26,439
also the smaller chance that human

300
00:14:25,000 --> 00:14:31,300
<font color="#E5E5E5">looking at this input will realize that</font>

301
00:14:26,440 --> 00:14:33,940
<font color="#E5E5E5">something wrong is</font><font color="#CCCCCC"> going on so the most</font>

302
00:14:31,300 --> 00:14:36,069
simple way that we can generate this

303
00:14:33,940 --> 00:14:37,930
adversarial images is to basically

304
00:14:36,070 --> 00:14:40,240
traverse the manifold and find blind

305
00:14:37,930 --> 00:14:44,019
spots in input<font color="#E5E5E5"> and what this means is</font>

306
00:14:40,240 --> 00:14:46,449
just let's say you have a 32 by 32

307
00:14:44,019 --> 00:14:51,189
pixel matrix and you're trying to<font color="#E5E5E5"> look</font>

308
00:14:46,449 --> 00:14:53,170
for<font color="#E5E5E5"> misclassifications of an adult image</font>

309
00:14:51,189 --> 00:14:55,660
so what you can do is to<font color="#CCCCCC"> just brute</font>

310
00:14:53,170 --> 00:14:59,949
force to<font color="#CCCCCC"> change each each pixel one by</font>

311
00:14:55,660 --> 00:15:03,009
one increment each value from from<font color="#E5E5E5"> 0 to</font>

312
00:14:59,949 --> 00:15:05,219
255 and just make small small

313
00:15:03,009 --> 00:15:08,829
perturbations in the input to find out

314
00:15:05,220 --> 00:15:11,199
which image of a dog with<font color="#CCCCCC"> would be</font>

315
00:15:08,829 --> 00:15:13,899
misclassified by this model and this is

316
00:15:11,199 --> 00:15:16,420
this<font color="#E5E5E5"> is okay the people used to do this</font>

317
00:15:13,899 --> 00:15:17,980
in<font color="#E5E5E5"> the past but it would take a long</font>

318
00:15:16,420 --> 00:15:19,689
<font color="#E5E5E5">time especially if you're looking at</font>

319
00:15:17,980 --> 00:15:21,579
larger images we're looking at malware

320
00:15:19,689 --> 00:15:23,290
classification for example then

321
00:15:21,579 --> 00:15:24,878
typically you have<font color="#CCCCCC"> to change millions</font>

322
00:15:23,290 --> 00:15:28,360
<font color="#E5E5E5">and millions of bytes and it's just a</font>

323
00:15:24,879 --> 00:15:30,489
combinatorial explosion so to optimize

324
00:15:28,360 --> 00:15:32,110
this search you can take the input<font color="#E5E5E5"> of</font>

325
00:15:30,489 --> 00:15:34,209
<font color="#E5E5E5">the target with respect to that target</font>

326
00:15:32,110 --> 00:15:36,850
output class which is what<font color="#CCCCCC"> inspired the</font>

327
00:15:34,209 --> 00:15:38,920
next the next<font color="#E5E5E5"> method which is linear</font>

328
00:15:36,850 --> 00:15:41,559
adversarial perturbation so if you can

329
00:15:38,920 --> 00:15:43,959
think<font color="#E5E5E5"> of a neural network as basically a</font>

330
00:15:41,559 --> 00:15:46,719
black box when you have input and an

331
00:15:43,959 --> 00:15:50,559
output and basically what you're trying

332
00:15:46,720 --> 00:15:51,999
to do is to<font color="#E5E5E5"> feed in inputs that would</font>

333
00:15:50,559 --> 00:15:53,709
result in a different output from the

334
00:15:51,999 --> 00:15:55,329
previous input of<font color="#CCCCCC"> the same class and</font>

335
00:15:53,709 --> 00:15:58,388
then you would take<font color="#CCCCCC"> the cross the cross</font>

336
00:15:55,329 --> 00:16:02,709
entropy of the<font color="#E5E5E5"> input sample and the</font>

337
00:15:58,389 --> 00:16:06,160
<font color="#E5E5E5">predicted class and this can be done</font>

338
00:16:02,709 --> 00:16:09,040
without the actual model as<font color="#CCCCCC"> well but you</font>

339
00:16:06,160 --> 00:16:10,660
have to create<font color="#E5E5E5"> a substitute model in</font>

340
00:16:09,040 --> 00:16:13,179
order<font color="#CCCCCC"> to get access to the inside</font>

341
00:16:10,660 --> 00:16:16,240
parameters it's easily fun bye bye that

342
00:16:13,179 --> 00:16:18,249
propagation and I have some sample code

343
00:16:16,240 --> 00:16:19,929
on this on a repo that I published

344
00:16:18,249 --> 00:16:22,600
online and you can play of it yourself

345
00:16:19,929 --> 00:16:24,189
<font color="#E5E5E5">to find out how it works and to see if</font>

346
00:16:22,600 --> 00:16:26,829
you can generate adverse our samples for

347
00:16:24,189 --> 00:16:29,889
arbitrary machine learning classifiers

348
00:16:26,829 --> 00:16:31,929
yourself the last method is is a little

349
00:16:29,889 --> 00:16:34,059
bit more complicated and the motivation

350
00:16:31,929 --> 00:16:35,740
<font color="#CCCCCC">behind this method is simply to reduce</font>

351
00:16:34,059 --> 00:16:38,199
the amount of perturbation that you have

352
00:16:35,740 --> 00:16:41,919
to add to<font color="#CCCCCC"> a to a valid input in order to</font>

353
00:16:38,199 --> 00:16:43,569
trick the machine learning model the

354
00:16:41,919 --> 00:16:46,869
basic<font color="#E5E5E5"> idea is that you are you want to</font>

355
00:16:43,569 --> 00:16:49,719
try<font color="#CCCCCC"> to find points</font><font color="#E5E5E5"> in this in this input</font>

356
00:16:46,869 --> 00:16:52,779
vector that<font color="#E5E5E5"> will allow you to make small</font>

357
00:16:49,720 --> 00:16:56,049
changes to<font color="#E5E5E5"> the input but then result in</font>

358
00:16:52,779 --> 00:16:57,459
a larger change to the to<font color="#CCCCCC"> the output so</font>

359
00:16:56,049 --> 00:16:58,899
this is<font color="#CCCCCC"> a</font><font color="#E5E5E5"> example</font>

360
00:16:57,459 --> 00:17:04,359
<font color="#E5E5E5">all</font><font color="#CCCCCC"> salience II</font><font color="#E5E5E5"> met which is what we</font>

361
00:16:58,899 --> 00:17:06,640
call the measure of how much each pixel

362
00:17:04,359 --> 00:17:09,339
actually affects the output result as

363
00:17:06,640 --> 00:17:11,500
you<font color="#E5E5E5"> can see in the in the center of the</font>

364
00:17:09,339 --> 00:17:13,148
image<font color="#E5E5E5"> you actually have the largest</font>

365
00:17:11,500 --> 00:17:15,699
salience<font color="#E5E5E5"> II which means</font><font color="#CCCCCC"> that if you</font>

366
00:17:13,148 --> 00:17:17,469
<font color="#E5E5E5">change a pixel if you change</font><font color="#CCCCCC"> the pixel</font>

367
00:17:15,699 --> 00:17:19,600
in<font color="#CCCCCC"> the center of</font><font color="#E5E5E5"> the image by a value of</font>

368
00:17:17,470 --> 00:17:21,069
one it's going<font color="#CCCCCC"> to affect the output with</font>

369
00:17:19,599 --> 00:17:22,569
a higher probability than if you change

370
00:17:21,069 --> 00:17:24,459
a pixel by the<font color="#E5E5E5"> value of one on the very</font>

371
00:17:22,569 --> 00:17:25,869
edge of the<font color="#E5E5E5"> image which kind of makes</font>

372
00:17:24,459 --> 00:17:27,909
sense because<font color="#E5E5E5"> of you if you think about</font>

373
00:17:25,869 --> 00:17:29,439
it<font color="#E5E5E5"> just most of the time they contain</font>

374
00:17:27,909 --> 00:17:31,120
backgrounds most<font color="#E5E5E5"> of the time it's not</font>

375
00:17:29,440 --> 00:17:34,600
<font color="#E5E5E5">important things</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> will result in a</font>

376
00:17:31,120 --> 00:17:36,959
difference in the classification so

377
00:17:34,600 --> 00:17:40,330
we'll go on to<font color="#E5E5E5"> the threat model</font>

378
00:17:36,960 --> 00:17:41,649
basically this<font color="#CCCCCC"> is the different kind</font><font color="#E5E5E5"> of</font>

379
00:17:40,330 --> 00:17:42,879
things that<font color="#E5E5E5"> you have the two given the</font>

380
00:17:41,649 --> 00:17:47,260
different<font color="#E5E5E5"> kinds of knowledge that you</font>

381
00:17:42,880 --> 00:17:49,450
<font color="#CCCCCC">have in each attack scenario the most</font>

382
00:17:47,260 --> 00:17:51,610
easy way to attack something is if you

383
00:17:49,450 --> 00:17:52,840
have<font color="#E5E5E5"> the model hyper parameters if</font>

384
00:17:51,610 --> 00:17:55,240
you're training<font color="#CCCCCC"> a neural network for</font>

385
00:17:52,840 --> 00:17:57,100
example if you know the kinds of tools

386
00:17:55,240 --> 00:17:59,260
<font color="#E5E5E5">that we use to</font><font color="#CCCCCC"> Train it the the</font>

387
00:17:57,100 --> 00:18:01,090
frameworks that we use a train it each

388
00:17:59,260 --> 00:18:02,710
value of the weights in this neural

389
00:18:01,090 --> 00:18:05,889
network and the activation function

390
00:18:02,710 --> 00:18:07,029
<font color="#E5E5E5">values the biases for example and then</font>

391
00:18:05,890 --> 00:18:08,860
if you have slightly less information

392
00:18:07,029 --> 00:18:11,230
you only have access to the architecture

393
00:18:08,860 --> 00:18:13,149
you for example know that this is train

394
00:18:11,230 --> 00:18:15,309
of a neural network it's a three layer

395
00:18:13,149 --> 00:18:18,370
on your own net for example and then

396
00:18:15,309 --> 00:18:19,990
slightly less data you have only access

397
00:18:18,370 --> 00:18:24,399
<font color="#CCCCCC">to the training data and even slightly</font>

398
00:18:19,990 --> 00:18:26,260
less examples you have it's basically a

399
00:18:24,399 --> 00:18:28,449
black box so all you<font color="#CCCCCC"> have</font><font color="#E5E5E5"> is the ability</font>

400
00:18:26,260 --> 00:18:30,549
to input stuff into this classifier<font color="#CCCCCC"> and</font>

401
00:18:28,450 --> 00:18:32,200
then you can see the outputs and we can

402
00:18:30,549 --> 00:18:34,779
see<font color="#E5E5E5"> that in this case you</font><font color="#CCCCCC"> can still</font>

403
00:18:32,200 --> 00:18:37,870
generate you can still generate adverse

404
00:18:34,779 --> 00:18:39,700
or examples so to trick these models so

405
00:18:37,870 --> 00:18:43,389
this<font color="#E5E5E5"> is just a study of what people have</font>

406
00:18:39,700 --> 00:18:45,429
done in this<font color="#CCCCCC"> area in</font><font color="#E5E5E5"> general the efforts</font>

407
00:18:43,390 --> 00:18:47,070
in a<font color="#E5E5E5"> veteran machine learning have been</font>

408
00:18:45,429 --> 00:18:49,750
have<font color="#CCCCCC"> been pretty having pretty scattered</font>

409
00:18:47,070 --> 00:18:52,620
there's not<font color="#E5E5E5"> a lot of a lot</font><font color="#CCCCCC"> of new</font>

410
00:18:49,750 --> 00:18:54,610
developments which is why I started

411
00:18:52,620 --> 00:18:57,279
talking<font color="#E5E5E5"> about this thing because I feel</font>

412
00:18:54,610 --> 00:18:59,949
like the security industry needs to look

413
00:18:57,279 --> 00:19:01,659
at machine learning models since a lot

414
00:18:59,950 --> 00:19:03,039
of<font color="#E5E5E5"> the critical infrastructure will be</font>

415
00:19:01,659 --> 00:19:05,409
shifting to machine learning in the

416
00:19:03,039 --> 00:19:08,950
future for example self-driving cars

417
00:19:05,409 --> 00:19:11,110
medical devices and image recognition

418
00:19:08,950 --> 00:19:13,330
speech translation all of the

419
00:19:11,110 --> 00:19:16,990
are powered by machine learning and soon

420
00:19:13,330 --> 00:19:19,330
as<font color="#CCCCCC"> sooner or later we'll be relying on</font>

421
00:19:16,990 --> 00:19:20,790
these more than we would<font color="#CCCCCC"> like to</font><font color="#E5E5E5"> and the</font>

422
00:19:19,330 --> 00:19:25,449
<font color="#E5E5E5">thing is that</font><font color="#CCCCCC"> a lot of these are</font><font color="#E5E5E5"> not</font>

423
00:19:20,790 --> 00:19:26,860
very reliable if if someone were to<font color="#E5E5E5"> go</font>

424
00:19:25,450 --> 00:19:28,840
in and<font color="#E5E5E5"> try to temper of such a system</font>

425
00:19:26,860 --> 00:19:30,879
<font color="#E5E5E5">you can find that they can easily be</font>

426
00:19:28,840 --> 00:19:33,970
tempered some medical imaging devices

427
00:19:30,880 --> 00:19:36,250
that try<font color="#E5E5E5"> to find for example if a cell</font>

428
00:19:33,970 --> 00:19:39,490
in a patient is banana<font color="#E5E5E5"> is benign or not</font>

429
00:19:36,250 --> 00:19:42,490
by looking at the x-ray output can be

430
00:19:39,490 --> 00:19:44,710
easily fooled by something<font color="#E5E5E5"> like that and</font>

431
00:19:42,490 --> 00:19:46,179
you can see that<font color="#E5E5E5"> there can be dire</font>

432
00:19:44,710 --> 00:19:48,700
consequences<font color="#E5E5E5"> they've been examples</font>

433
00:19:46,179 --> 00:19:50,320
online and papers published on how deep

434
00:19:48,700 --> 00:19:53,470
learning engines used by self-driving

435
00:19:50,320 --> 00:19:55,840
cars can be fooled<font color="#E5E5E5"> by images that are</font>

436
00:19:53,470 --> 00:19:58,210
just held<font color="#E5E5E5"> up by a person with a green</font>

437
00:19:55,840 --> 00:20:01,629
light even though<font color="#CCCCCC"> there's</font><font color="#E5E5E5"> a red light in</font>

438
00:19:58,210 --> 00:20:03,520
<font color="#E5E5E5">real life but the the models this sees</font>

439
00:20:01,630 --> 00:20:05,230
the green light or sees a red light with

440
00:20:03,520 --> 00:20:06,879
a slight perturbation and things<font color="#E5E5E5"> that is</font>

441
00:20:05,230 --> 00:20:09,160
not a red light and then it<font color="#CCCCCC"> is proceeds</font>

442
00:20:06,880 --> 00:20:12,040
so you can see<font color="#CCCCCC"> how this will result in</font>

443
00:20:09,160 --> 00:20:13,960
human loss of life or<font color="#CCCCCC"> are other dire</font>

444
00:20:12,040 --> 00:20:15,070
consequences and this is why i<font color="#CCCCCC"> think</font>

445
00:20:13,960 --> 00:20:17,350
<font color="#E5E5E5">that the security industry needs to</font>

446
00:20:15,070 --> 00:20:19,149
start to audit these these statistical

447
00:20:17,350 --> 00:20:20,889
methods need to start looking at how

448
00:20:19,150 --> 00:20:24,640
reliable and how tamper-proof these

449
00:20:20,890 --> 00:20:26,230
machine learning models are so what can

450
00:20:24,640 --> 00:20:27,610
you<font color="#E5E5E5"> do with limited knowledge in most</font>

451
00:20:26,230 --> 00:20:29,950
real-world systems we don't actually

452
00:20:27,610 --> 00:20:32,110
have access to the model<font color="#E5E5E5"> parameters we</font>

453
00:20:29,950 --> 00:20:33,580
don't have access to how the model was

454
00:20:32,110 --> 00:20:35,860
actually trained or what input was

455
00:20:33,580 --> 00:20:37,990
actually used to this so a lot of the

456
00:20:35,860 --> 00:20:40,240
time you<font color="#CCCCCC"> can make good guesses</font><font color="#E5E5E5"> and this</font>

457
00:20:37,990 --> 00:20:42,100
is kind of a black art but as you come

458
00:20:40,240 --> 00:20:43,600
as you come into<font color="#CCCCCC"> contact with more and</font>

459
00:20:42,100 --> 00:20:45,909
more systems that are using machine

460
00:20:43,600 --> 00:20:47,889
learning you can<font color="#E5E5E5"> you</font><font color="#CCCCCC"> can make some some</font>

461
00:20:45,910 --> 00:20:50,710
good guesses for example image

462
00:20:47,890 --> 00:20:53,040
classifications the state of<font color="#E5E5E5"> the art</font>

463
00:20:50,710 --> 00:20:55,390
would be convolutional neural nets and

464
00:20:53,040 --> 00:20:57,010
if you can guess that an image

465
00:20:55,390 --> 00:20:59,200
classification is done by convolutional

466
00:20:57,010 --> 00:21:01,330
neural net then<font color="#E5E5E5"> you will be able to</font>

467
00:20:59,200 --> 00:21:03,640
generate<font color="#E5E5E5"> adversary examples that are</font>

468
00:21:01,330 --> 00:21:05,740
more accurate speech<font color="#CCCCCC"> recognition they</font>

469
00:21:03,640 --> 00:21:08,110
are done using lstm and recursive neural

470
00:21:05,740 --> 00:21:10,230
networks a lot<font color="#E5E5E5"> of the time anything</font>

471
00:21:08,110 --> 00:21:13,120
<font color="#CCCCCC">that's there's a do with temporal inputs</font>

472
00:21:10,230 --> 00:21:15,130
are trained using long short term memory

473
00:21:13,120 --> 00:21:17,500
networks and recursive neural networks

474
00:21:15,130 --> 00:21:19,120
if you're<font color="#E5E5E5"> looking at more general</font>

475
00:21:17,500 --> 00:21:21,010
purpose machine learning frameworks like

476
00:21:19,120 --> 00:21:22,949
Amazon machine learning or any<font color="#E5E5E5"> kind of</font>

477
00:21:21,010 --> 00:21:24,810
machine learning as a service

478
00:21:22,950 --> 00:21:26,700
then a lot<font color="#CCCCCC"> of the</font><font color="#E5E5E5"> time because they have</font>

479
00:21:24,810 --> 00:21:28,860
to generalize well you're looking<font color="#E5E5E5"> at</font>

480
00:21:26,700 --> 00:21:31,410
very shallow networks since having a

481
00:21:28,860 --> 00:21:36,030
deeper network will result in more

482
00:21:31,410 --> 00:21:37,950
probable overfitting so that's if you

483
00:21:36,030 --> 00:21:40,170
can<font color="#E5E5E5"> guess but what if you can't guess or</font>

484
00:21:37,950 --> 00:21:43,470
you're<font color="#E5E5E5"> not very sure of the guess you</font>

485
00:21:40,170 --> 00:21:46,740
can still you can still own it so<font color="#CCCCCC"> I have</font>

486
00:21:43,470 --> 00:21:49,440
a second demo here to show captures are

487
00:21:46,740 --> 00:21:52,860
one of the most<font color="#E5E5E5"> interesting ways that</font>

488
00:21:49,440 --> 00:21:55,170
are most commonly used to differentiate

489
00:21:52,860 --> 00:21:56,699
a human from a bug right so most

490
00:21:55,170 --> 00:21:59,370
websites you go to if you want to tell

491
00:21:56,700 --> 00:22:00,930
that it's a it's a real human that<font color="#E5E5E5"> you</font>

492
00:21:59,370 --> 00:22:02,879
would put a capture there and obviously

493
00:22:00,930 --> 00:22:04,920
there have been many services online

494
00:22:02,880 --> 00:22:07,410
like that by capture that you can pay

495
00:22:04,920 --> 00:22:09,270
for maybe a few cents for for a thousand

496
00:22:07,410 --> 00:22:12,690
capped resolved to<font color="#CCCCCC"> helped with your</font>

497
00:22:09,270 --> 00:22:14,970
automation but this is not<font color="#CCCCCC"> very scalable</font>

498
00:22:12,690 --> 00:22:16,650
because like how many<font color="#E5E5E5"> people are willing</font>

499
00:22:14,970 --> 00:22:18,540
to<font color="#E5E5E5"> sit</font><font color="#CCCCCC"> down at the computer solving</font>

500
00:22:16,650 --> 00:22:22,230
CAPTCHAs all day for for one dollar a

501
00:22:18,540 --> 00:22:26,159
day so it's<font color="#CCCCCC"> not very practical</font><font color="#E5E5E5"> and it's</font>

502
00:22:22,230 --> 00:22:28,080
<font color="#E5E5E5">a it's a matching problem so excuse me</font>

503
00:22:26,160 --> 00:22:30,120
so there have been solutions out there

504
00:22:28,080 --> 00:22:32,129
that use deep learning to solve these

505
00:22:30,120 --> 00:22:34,919
these things and<font color="#E5E5E5"> so this</font><font color="#CCCCCC"> is an</font>

506
00:22:32,130 --> 00:22:37,170
<font color="#CCCCCC">open-source</font><font color="#E5E5E5"> example it's called capture</font>

507
00:22:34,920 --> 00:22:38,400
<font color="#E5E5E5">crusher it's pretty good I didn't write</font>

508
00:22:37,170 --> 00:22:42,840
this this is<font color="#CCCCCC"> this is just something</font>

509
00:22:38,400 --> 00:22:45,390
online<font color="#E5E5E5"> and this is just</font><font color="#CCCCCC"> a tool that I'm</font>

510
00:22:42,840 --> 00:22:47,250
using to generate captures pretty pretty

511
00:22:45,390 --> 00:22:49,850
generic captures they are not the most

512
00:22:47,250 --> 00:22:52,530
complicated captures but as you can see

513
00:22:49,850 --> 00:22:54,030
the complication that the complexity of

514
00:22:52,530 --> 00:22:56,129
the<font color="#CCCCCC"> other captures doesn't actually</font>

515
00:22:54,030 --> 00:23:04,139
matter this is what the model actually

516
00:22:56,130 --> 00:23:05,610
looks like and so I won't<font color="#E5E5E5"> go into into</font>

517
00:23:04,140 --> 00:23:09,600
detail I cleaning<font color="#E5E5E5"> the model but what is</font>

518
00:23:05,610 --> 00:23:12,840
basically doing is to predict the actual

519
00:23:09,600 --> 00:23:14,129
is is<font color="#E5E5E5"> to show</font><font color="#CCCCCC"> the actual labels for for</font>

520
00:23:12,840 --> 00:23:19,159
this input and then and then show the

521
00:23:14,130 --> 00:23:21,990
prediction and we're going<font color="#CCCCCC"> to show that</font>

522
00:23:19,160 --> 00:23:25,170
this actually does solve captives with a

523
00:23:21,990 --> 00:23:27,360
certain degree of accuracy in most real

524
00:23:25,170 --> 00:23:29,250
cases especially since you don't have

525
00:23:27,360 --> 00:23:32,040
<font color="#E5E5E5">access to how the captors that you're</font>

526
00:23:29,250 --> 00:23:34,020
<font color="#E5E5E5">seeing online are actually generated you</font>

527
00:23:32,040 --> 00:23:36,780
can't train a perfect<font color="#E5E5E5"> model for for</font>

528
00:23:34,020 --> 00:23:38,610
solving something like that so

529
00:23:36,780 --> 00:23:40,139
there'll be there'll be high accuracy

530
00:23:38,610 --> 00:23:42,179
but but there will be still things that

531
00:23:40,140 --> 00:23:44,010
you get wrong depending on what the

532
00:23:42,180 --> 00:23:47,010
characters look like let's generate some

533
00:23:44,010 --> 00:23:48,540
captors here we generate at ten you<font color="#E5E5E5"> can</font>

534
00:23:47,010 --> 00:23:49,920
see they<font color="#CCCCCC"> look like pretty generic</font>

535
00:23:48,540 --> 00:23:51,389
captors I think they're pretty

536
00:23:49,920 --> 00:23:53,340
complicated and I might get some of

537
00:23:51,390 --> 00:23:58,830
these wrong if I were<font color="#E5E5E5"> looking at these</font>

538
00:23:53,340 --> 00:24:02,340
and<font color="#E5E5E5"> try to type them out yeah there's</font>

539
00:23:58,830 --> 00:24:07,770
generated more so I'm deleting them and

540
00:24:02,340 --> 00:24:09,990
I'm<font color="#E5E5E5"> gonna show that you can actually</font>

541
00:24:07,770 --> 00:24:11,460
generate captures that trick this model

542
00:24:09,990 --> 00:24:13,590
that solves captures using deep learning

543
00:24:11,460 --> 00:24:16,230
so it's kind of it's kind of a matter

544
00:24:13,590 --> 00:24:17,909
because what we're doing is there's a

545
00:24:16,230 --> 00:24:19,710
deep learning model train to solve these

546
00:24:17,910 --> 00:24:21,240
captors and we want to generate<font color="#CCCCCC"> Capra's</font>

547
00:24:19,710 --> 00:24:22,440
that can trick this deep drilling<font color="#E5E5E5"> model</font>

548
00:24:21,240 --> 00:24:24,630
<font color="#E5E5E5">so it doesn't successfully solve</font>

549
00:24:22,440 --> 00:24:26,610
captures so this was the<font color="#E5E5E5"> training of the</font>

550
00:24:24,630 --> 00:24:29,970
model you can see training<font color="#E5E5E5"> deep learning</font>

551
00:24:26,610 --> 00:24:31,379
models are our kind of they take a long

552
00:24:29,970 --> 00:24:34,290
time especially if you're looking at<font color="#E5E5E5"> a</font>

553
00:24:31,380 --> 00:24:38,880
little bit<font color="#E5E5E5"> larger images I started on</font>

554
00:24:34,290 --> 00:24:40,290
July 12 at 553 a.m. and by the time the

555
00:24:38,880 --> 00:24:42,870
model finish it was<font color="#CCCCCC"> realized our team</font>

556
00:24:40,290 --> 00:24:45,300
from<font color="#E5E5E5"> 9am so it took took a long time i</font>

557
00:24:42,870 --> 00:24:49,649
did this online on amazon GPU clusters

558
00:24:45,300 --> 00:24:54,570
so it is cost up 120 dollars so it

559
00:24:49,650 --> 00:25:01,440
better work well and let's try it out

560
00:24:54,570 --> 00:25:03,389
and<font color="#CCCCCC"> we're evaluating this model for now</font>

561
00:25:01,440 --> 00:25:06,720
<font color="#E5E5E5">from this</font><font color="#CCCCCC"> 10 captures that that we that</font>

562
00:25:03,390 --> 00:25:13,620
we just<font color="#E5E5E5"> generated just to see then it</font>

563
00:25:06,720 --> 00:25:15,570
may omit the right decisions<font color="#CCCCCC"> okay it's</font>

564
00:25:13,620 --> 00:25:17,969
going to<font color="#E5E5E5"> wait for</font><font color="#CCCCCC"> it to dissolve it's</font>

565
00:25:15,570 --> 00:25:19,830
taking some time this is<font color="#E5E5E5"> already pretty</font>

566
00:25:17,970 --> 00:25:21,690
fast<font color="#CCCCCC"> i guess it's it</font><font color="#E5E5E5"> is faster</font><font color="#CCCCCC"> than</font>

567
00:25:19,830 --> 00:25:23,429
using like that<font color="#E5E5E5"> i capture or online</font>

568
00:25:21,690 --> 00:25:25,740
services with a<font color="#E5E5E5"> human behind the</font>

569
00:25:23,430 --> 00:25:28,350
computer ok so i predicted all of them

570
00:25:25,740 --> 00:25:30,240
correctly this is this<font color="#E5E5E5"> is lucky like if</font>

571
00:25:28,350 --> 00:25:31,800
you if you generate more more images

572
00:25:30,240 --> 00:25:35,040
<font color="#E5E5E5">sometimes they'll be like point</font><font color="#CCCCCC"> 9</font>

573
00:25:31,800 --> 00:25:37,770
accuracy so let's let's look at one of

574
00:25:35,040 --> 00:25:42,440
<font color="#E5E5E5">them must look at the</font><font color="#CCCCCC"> the first one so</font>

575
00:25:37,770 --> 00:25:42,440
the first one you see I actg be

576
00:25:42,720 --> 00:26:01,520
and the prediction is right and so let's

577
00:25:55,350 --> 00:26:06,240
look at this actual image it is I ECT gb

578
00:26:01,520 --> 00:26:07,980
yeah so what<font color="#E5E5E5"> we're going to do now</font><font color="#CCCCCC"> is to</font>

579
00:26:06,240 --> 00:26:10,080
generate the adversarial version of

580
00:26:07,980 --> 00:26:12,210
these<font color="#CCCCCC"> examples so they're going</font><font color="#E5E5E5"> to look</font>

581
00:26:10,080 --> 00:26:16,520
similar to the original image that the

582
00:26:12,210 --> 00:26:18,809
this framework can solve very well but a

583
00:26:16,520 --> 00:26:20,190
human<font color="#E5E5E5"> is still going to</font><font color="#CCCCCC"> be able</font><font color="#E5E5E5"> to read</font>

584
00:26:18,809 --> 00:26:25,559
them correctly but the deep learning

585
00:26:20,190 --> 00:26:27,960
model is not so we're going to generate

586
00:26:25,559 --> 00:26:31,350
these images these<font color="#E5E5E5"> are this the same pan</font>

587
00:26:27,960 --> 00:26:33,570
<font color="#CCCCCC">captors and what we're doing is we're</font>

588
00:26:31,350 --> 00:26:36,629
running it through a substitute deep

589
00:26:33,570 --> 00:26:38,520
learning model that<font color="#E5E5E5"> I wrote that is</font>

590
00:26:36,630 --> 00:26:39,929
different<font color="#E5E5E5"> from the original</font><font color="#CCCCCC"> capra</font>

591
00:26:38,520 --> 00:26:42,179
framework<font color="#E5E5E5"> that's used to solve these</font>

592
00:26:39,929 --> 00:26:46,830
these things so this is<font color="#CCCCCC"> a substitute</font>

593
00:26:42,179 --> 00:26:48,360
model is an example of a black box so

594
00:26:46,830 --> 00:26:51,389
what this does<font color="#E5E5E5"> is just five lines of</font>

595
00:26:48,360 --> 00:26:52,469
code that<font color="#CCCCCC"> generates this that's a bet in</font>

596
00:26:51,390 --> 00:26:56,549
a code base that i'll be<font color="#E5E5E5"> releasing</font>

597
00:26:52,470 --> 00:27:01,080
online that<font color="#E5E5E5"> i've released online and so</font>

598
00:26:56,549 --> 00:27:03,150
you see this is<font color="#CCCCCC"> the normal example and</font>

599
00:27:01,080 --> 00:27:04,980
this<font color="#E5E5E5"> is the adversarial example they</font>

600
00:27:03,150 --> 00:27:09,960
look alike it's slightly different if

601
00:27:04,980 --> 00:27:11,370
you look<font color="#E5E5E5"> at</font><font color="#CCCCCC"> the difference the first one</font>

602
00:27:09,960 --> 00:27:13,380
you can<font color="#E5E5E5"> really tell at all which means</font>

603
00:27:11,370 --> 00:27:15,360
<font color="#CCCCCC">that the pixel differences are actually</font>

604
00:27:13,380 --> 00:27:19,470
in a white space and the human eye is

605
00:27:15,360 --> 00:27:22,199
pretty bad at at differentiating white

606
00:27:19,470 --> 00:27:27,809
and slightly off-white so that's

607
00:27:22,200 --> 00:27:31,200
probably why and so if<font color="#E5E5E5"> you do an</font>

608
00:27:27,809 --> 00:27:33,480
evaluation of the adversarial examples

609
00:27:31,200 --> 00:27:36,980
you see that it doesn't perform nearly

610
00:27:33,480 --> 00:27:42,390
as well as with the original examples

611
00:27:36,980 --> 00:27:43,470
and let's<font color="#CCCCCC"> just run these go to a couple</font>

612
00:27:42,390 --> 00:27:50,159
<font color="#E5E5E5">minutes</font>

613
00:27:43,470 --> 00:27:52,190
<font color="#CCCCCC">naw couple months couple seconds and so</font>

614
00:27:50,159 --> 00:27:55,559
it's going<font color="#E5E5E5"> to give</font><font color="#CCCCCC"> me a score at the end</font>

615
00:27:52,190 --> 00:27:57,450
and it's going<font color="#E5E5E5"> to show me at each</font>

616
00:27:55,559 --> 00:27:59,639
position in this<font color="#CCCCCC"> CAPTCHA there are six</font>

617
00:27:57,450 --> 00:28:04,140
there are six digits in each capture how

618
00:27:59,640 --> 00:28:06,150
many got right so let's see a position

619
00:28:04,140 --> 00:28:07,650
one<font color="#E5E5E5"> it</font><font color="#CCCCCC"> only got one out of</font><font color="#E5E5E5"> the 10 right</font>

620
00:28:06,150 --> 00:28:11,220
<font color="#E5E5E5">president to only got two out of the</font><font color="#CCCCCC"> 10</font>

621
00:28:07,650 --> 00:28:14,580
right at position 3 it actually got

622
00:28:11,220 --> 00:28:15,870
seven out of<font color="#CCCCCC"> 10 right so but the thing</font>

623
00:28:14,580 --> 00:28:18,210
<font color="#CCCCCC">is</font><font color="#E5E5E5"> that we've captures as long as you</font>

624
00:28:15,870 --> 00:28:21,289
get one out of<font color="#CCCCCC"> the</font><font color="#E5E5E5"> 10 one out one out of</font>

625
00:28:18,210 --> 00:28:26,640
six digits wrong then you'd be blocked

626
00:28:21,289 --> 00:28:28,140
so we can see that these actually look

627
00:28:26,640 --> 00:28:29,700
the same to a human we'd be able<font color="#E5E5E5"> to</font>

628
00:28:28,140 --> 00:28:37,200
solve them but it successfully tricks

629
00:28:29,700 --> 00:28:38,549
the deep learning model so only an elder

630
00:28:37,200 --> 00:28:41,039
in a presentation I mentioned something

631
00:28:38,549 --> 00:28:42,840
about transferability<font color="#E5E5E5"> I basically these</font>

632
00:28:41,039 --> 00:28:44,070
attacks are not<font color="#E5E5E5"> only applicable to deep</font>

633
00:28:42,840 --> 00:28:45,270
learning models they are applicable to

634
00:28:44,070 --> 00:28:47,610
<font color="#E5E5E5">all kinds of machine learning models</font>

635
00:28:45,270 --> 00:28:49,320
these<font color="#E5E5E5"> are</font><font color="#CCCCCC"> some examples of pressuring</font>

636
00:28:47,610 --> 00:28:52,439
models that have been<font color="#E5E5E5"> studied for</font>

637
00:28:49,320 --> 00:28:54,210
example linear classifiers spectral

638
00:28:52,440 --> 00:28:56,309
clustering algorithms support vector

639
00:28:54,210 --> 00:28:58,440
machines<font color="#CCCCCC"> there's decision trees the</font>

640
00:28:56,309 --> 00:29:00,210
thing is that and this is the one open

641
00:28:58,440 --> 00:29:02,400
research problem as to why this happens

642
00:29:00,210 --> 00:29:04,530
but adversarial samples are generated

643
00:29:02,400 --> 00:29:06,659
using deep learning have a pretty good

644
00:29:04,530 --> 00:29:08,760
<font color="#E5E5E5">chance of fooling a previously unseen</font>

645
00:29:06,659 --> 00:29:10,200
model even<font color="#CCCCCC"> if it's a totally different</font>

646
00:29:08,760 --> 00:29:11,700
model train using totally different

647
00:29:10,200 --> 00:29:14,760
things and in a totally different

648
00:29:11,700 --> 00:29:18,900
<font color="#E5E5E5">infrastructure you can see that this is</font>

649
00:29:14,760 --> 00:29:21,419
a matrix<font color="#CCCCCC"> of adversarial samples are</font>

650
00:29:18,900 --> 00:29:22,980
generated using one technique and test

651
00:29:21,419 --> 00:29:25,460
it on a different<font color="#CCCCCC"> machine learning</font>

652
00:29:22,980 --> 00:29:28,710
technique so on the<font color="#E5E5E5"> y-axis you have the</font>

653
00:29:25,460 --> 00:29:31,020
the adversarial generation method and on

654
00:29:28,710 --> 00:29:32,789
the<font color="#E5E5E5"> x-axis you have the technique that</font>

655
00:29:31,020 --> 00:29:36,030
is used to test these adversarial

656
00:29:32,789 --> 00:29:38,039
samples you<font color="#CCCCCC"> can see that L R stands for</font>

657
00:29:36,030 --> 00:29:40,309
linear regression and svm stands for

658
00:29:38,039 --> 00:29:42,629
support vector machines if you train a

659
00:29:40,309 --> 00:29:44,520
adversarial sample on a logistic

660
00:29:42,630 --> 00:29:45,990
regression and you test it on the

661
00:29:44,520 --> 00:29:48,299
<font color="#E5E5E5">support vector machine which is totally</font>

662
00:29:45,990 --> 00:29:51,179
<font color="#CCCCCC">different in terms of instructor and the</font>

663
00:29:48,299 --> 00:29:52,559
math behind it even then<font color="#E5E5E5"> you see that it</font>

664
00:29:51,179 --> 00:29:53,970
has successfully tricks the model with a

665
00:29:52,559 --> 00:29:55,950
ninety-one percent accuracy which is

666
00:29:53,970 --> 00:29:56,909
pretty<font color="#E5E5E5"> good and the neural networks are</font>

667
00:29:55,950 --> 00:29:58,919
just a

668
00:29:56,909 --> 00:30:00,239
easy way<font color="#CCCCCC"> of training models since you</font>

669
00:29:58,919 --> 00:30:04,470
don't<font color="#CCCCCC"> have</font><font color="#E5E5E5"> the mettle of too many model</font>

670
00:30:00,239 --> 00:30:06,720
parameters and stuff and substitute

671
00:30:04,470 --> 00:30:10,379
models are another thing<font color="#E5E5E5"> that</font><font color="#CCCCCC"> we can use</font>

672
00:30:06,720 --> 00:30:11,720
to<font color="#CCCCCC"> train adversarial samples on a black</font>

673
00:30:10,379 --> 00:30:14,279
box model that we have never<font color="#E5E5E5"> seen</font><font color="#CCCCCC"> before</font>

674
00:30:11,720 --> 00:30:18,929
basically you<font color="#E5E5E5"> want to treat the new</font>

675
00:30:14,279 --> 00:30:22,080
models input with the output labels of

676
00:30:18,929 --> 00:30:23,999
<font color="#E5E5E5">the of the previous model and you are</font>

677
00:30:22,080 --> 00:30:25,259
training a new model based on the a

678
00:30:23,999 --> 00:30:27,239
model that<font color="#CCCCCC"> you don't have access to</font>

679
00:30:25,259 --> 00:30:30,899
which which<font color="#CCCCCC"> is why it's called a</font>

680
00:30:27,239 --> 00:30:33,419
substitute model and why is this much as

681
00:30:30,899 --> 00:30:34,529
possible so transferability as<font color="#CCCCCC"> i</font>

682
00:30:33,419 --> 00:30:37,979
mentioned before is still an open

683
00:30:34,529 --> 00:30:39,779
research problem the fact that you can

684
00:30:37,979 --> 00:30:41,340
trick machine learning models is because

685
00:30:39,779 --> 00:30:44,249
there are many blind spots in<font color="#CCCCCC"> the data</font>

686
00:30:41,340 --> 00:30:46,080
when when<font color="#E5E5E5"> you train a classifier to</font>

687
00:30:44,249 --> 00:30:48,960
learn something<font color="#E5E5E5"> it is not learning</font>

688
00:30:46,080 --> 00:30:51,449
everything about<font color="#E5E5E5"> a dull or it cannot</font>

689
00:30:48,960 --> 00:30:54,539
possibly<font color="#E5E5E5"> classify every single image</font>

690
00:30:51,450 --> 00:30:56,159
that exists of a dog as a dog correctly

691
00:30:54,539 --> 00:30:58,200
because there are certain blind spots

692
00:30:56,159 --> 00:31:00,179
that this model learns so there's<font color="#E5E5E5"> this</font>

693
00:30:58,200 --> 00:31:02,970
model versus reality dimensionality

694
00:31:00,179 --> 00:31:04,049
mismatch and people sort the question is

695
00:31:02,970 --> 00:31:08,789
this model actually<font color="#E5E5E5"> not learning</font>

696
00:31:04,049 --> 00:31:10,259
anything meaningful at all so what this

697
00:31:08,789 --> 00:31:11,759
means is that when you whenever you

698
00:31:10,259 --> 00:31:14,549
deploy machine learning models you

699
00:31:11,759 --> 00:31:17,399
should use with caution in especially in

700
00:31:14,549 --> 00:31:19,619
critical deployments where there's where

701
00:31:17,399 --> 00:31:21,449
there's human life at stake or when

702
00:31:19,619 --> 00:31:23,428
there's large financial<font color="#E5E5E5"> loss at stake</font>

703
00:31:21,450 --> 00:31:25,080
you shouldn't make false assumptions

704
00:31:23,429 --> 00:31:27,119
about what the model learns and you

705
00:31:25,080 --> 00:31:31,139
should always evaluate how resilient a

706
00:31:27,119 --> 00:31:32,908
model is in adversarial in the face of a

707
00:31:31,139 --> 00:31:35,309
visceral attacks and you should spend

708
00:31:32,909 --> 00:31:38,879
efforts to make models more robust so

709
00:31:35,309 --> 00:31:40,559
how the most straightforward one is to

710
00:31:38,879 --> 00:31:41,879
basically generate adversarial samples

711
00:31:40,559 --> 00:31:43,979
for machine learning models and then

712
00:31:41,879 --> 00:31:46,168
train your model using these adversarial

713
00:31:43,979 --> 00:31:49,409
samples basically are trying to iron out

714
00:31:46,169 --> 00:31:50,519
any kind<font color="#E5E5E5"> of imperfect knowledge there</font>

715
00:31:49,409 --> 00:31:53,070
are other techniques that can be used

716
00:31:50,519 --> 00:31:55,080
like distillation which means you train

717
00:31:53,070 --> 00:31:58,019
the model two times first feeding the

718
00:31:55,080 --> 00:32:00,449
first feeding the original images into

719
00:31:58,019 --> 00:32:02,849
into the<font color="#E5E5E5"> first model and then you train</font>

720
00:32:00,450 --> 00:32:04,409
the second<font color="#E5E5E5"> model with a with</font><font color="#CCCCCC"> the output</font>

721
00:32:02,849 --> 00:32:05,970
of<font color="#E5E5E5"> the first model and this helps to</font>

722
00:32:04,409 --> 00:32:08,339
distill the amount<font color="#E5E5E5"> of information that</font>

723
00:32:05,970 --> 00:32:10,780
is stored inside a neural network this

724
00:32:08,339 --> 00:32:12,730
was initially used<font color="#E5E5E5"> to reduce model sizes</font>

725
00:32:10,780 --> 00:32:15,850
they can fit on your mobile phone for

726
00:32:12,730 --> 00:32:18,400
<font color="#CCCCCC">example because</font><font color="#E5E5E5"> if you train on a large</font>

727
00:32:15,850 --> 00:32:20,590
<font color="#E5E5E5">GPU cluster there will be many many did</font>

728
00:32:18,400 --> 00:32:22,030
you need many many dead units that are

729
00:32:20,590 --> 00:32:24,520
not actually used in the actual

730
00:32:22,030 --> 00:32:26,980
prediction and you can also use<font color="#CCCCCC"> other</font>

731
00:32:24,520 --> 00:32:31,420
methods like using loss function and

732
00:32:26,980 --> 00:32:36,100
regularization techniques so the honing

733
00:32:31,420 --> 00:32:37,680
is say is a is something that I that I

734
00:32:36,100 --> 00:32:40,810
release at Def Con this year and

735
00:32:37,680 --> 00:32:43,570
basically it aims<font color="#E5E5E5"> to be the metas plot</font>

736
00:32:40,810 --> 00:32:46,780
of machine learning so what it has it's

737
00:32:43,570 --> 00:32:48,700
basically some some structured code on

738
00:32:46,780 --> 00:32:51,310
how you can generate machine learning

739
00:32:48,700 --> 00:32:56,650
some generate adversarial samples on

740
00:32:51,310 --> 00:32:58,899
machine learning models and basically

741
00:32:56,650 --> 00:33:00,970
you can input your own machine<font color="#E5E5E5"> learning</font>

742
00:32:58,900 --> 00:33:03,400
models the with with your<font color="#E5E5E5"> own</font>

743
00:33:00,970 --> 00:33:05,110
infrastructure and then use standard

744
00:33:03,400 --> 00:33:07,450
techniques for generating adversarial

745
00:33:05,110 --> 00:33:10,179
samples that these that is<font color="#CCCCCC"> a framework</font>

746
00:33:07,450 --> 00:33:13,480
provides and it's pretty<font color="#E5E5E5"> well documented</font>

747
00:33:10,180 --> 00:33:15,970
I think there<font color="#E5E5E5"> are some links to data</font>

748
00:33:13,480 --> 00:33:18,760
sets and papers that are used and also

749
00:33:15,970 --> 00:33:22,000
other sources of code<font color="#CCCCCC"> that you can use</font>

750
00:33:18,760 --> 00:33:23,530
so please check<font color="#E5E5E5"> it out just play around</font>

751
00:33:22,000 --> 00:33:31,680
with it and<font color="#CCCCCC"> and if you want</font><font color="#E5E5E5"> to help</font>

752
00:33:23,530 --> 00:33:34,030
contribute to it they'll be w it so<font color="#CCCCCC"> I</font>

753
00:33:31,680 --> 00:33:35,440
think that<font color="#E5E5E5"> a nutrition testing of</font>

754
00:33:34,030 --> 00:33:38,379
statistical models and machine learning

755
00:33:35,440 --> 00:33:40,090
systems is it's important because<font color="#CCCCCC"> as I</font>

756
00:33:38,380 --> 00:33:41,920
mentioned before more and<font color="#CCCCCC"> more of the</font>

757
00:33:40,090 --> 00:33:43,840
time were relying on<font color="#E5E5E5"> these systems and</font>

758
00:33:41,920 --> 00:33:45,340
we a lot of<font color="#E5E5E5"> people don't</font><font color="#CCCCCC"> have a good</font>

759
00:33:43,840 --> 00:33:48,790
understanding of how<font color="#E5E5E5"> these things work</font>

760
00:33:45,340 --> 00:33:51,189
so having<font color="#E5E5E5"> a good understanding as what's</font>

761
00:33:48,790 --> 00:33:52,899
required to<font color="#E5E5E5"> really evaluate how</font>

762
00:33:51,190 --> 00:33:54,940
resilient these models are too

763
00:33:52,900 --> 00:33:56,560
adversarial input and you should train

764
00:33:54,940 --> 00:33:59,710
these models with adversarial samples

765
00:33:56,560 --> 00:34:03,100
for increased robustness so please play

766
00:33:59,710 --> 00:34:05,380
everything contribute there's this a

767
00:34:03,100 --> 00:34:08,110
last interesting note that i think is an

768
00:34:05,380 --> 00:34:10,960
interesting research area deep learning

769
00:34:08,110 --> 00:34:12,700
and privacy is an interesting<font color="#E5E5E5"> area that</font>

770
00:34:10,960 --> 00:34:15,850
has been up and coming in the past few

771
00:34:12,699 --> 00:34:18,639
years basically there have been

772
00:34:15,850 --> 00:34:20,350
researched as that's done that shows

773
00:34:18,639 --> 00:34:22,839
<font color="#CCCCCC">that you can reconstitute training</font>

774
00:34:20,350 --> 00:34:23,980
samples from a train black box model so

775
00:34:22,840 --> 00:34:24,580
if you can think<font color="#E5E5E5"> about a facial</font>

776
00:34:23,980 --> 00:34:29,050
recognition

777
00:34:24,580 --> 00:34:31,750
engine that that recognizes your face

778
00:34:29,050 --> 00:34:34,590
and<font color="#E5E5E5"> in my face and these models are</font>

779
00:34:31,750 --> 00:34:36,370
stored on in a<font color="#E5E5E5"> mobile phone for example</font>

780
00:34:34,590 --> 00:34:39,790
researchers have shown that you can

781
00:34:36,370 --> 00:34:44,980
regenerate the faces that<font color="#E5E5E5"> I use the tree</font>

782
00:34:39,790 --> 00:34:46,989
in this model pretty pretty easily just

783
00:34:44,980 --> 00:34:48,880
by access to dance with access to<font color="#E5E5E5"> these</font>

784
00:34:46,989 --> 00:34:51,310
models which are distributed<font color="#E5E5E5"> to every</font>

785
00:34:48,880 --> 00:34:53,590
month devices so can<font color="#E5E5E5"> we precisely</font>

786
00:34:51,310 --> 00:34:55,330
control the learning objective of these

787
00:34:53,590 --> 00:34:57,400
models can we train the model without

788
00:34:55,330 --> 00:34:59,290
having complete access<font color="#CCCCCC"> to training data</font>

789
00:34:57,400 --> 00:35:01,150
these are interesting research questions

790
00:34:59,290 --> 00:35:03,460
<font color="#E5E5E5">I think have to be solved before we can</font>

791
00:35:01,150 --> 00:35:08,200
actually rely heavily on on machine

792
00:35:03,460 --> 00:35:09,970
learning in<font color="#E5E5E5"> everyday systems so this</font><font color="#CCCCCC"> is</font>

793
00:35:08,200 --> 00:35:11,980
important because more critical systems

794
00:35:09,970 --> 00:35:14,049
rely on it and we<font color="#E5E5E5"> need people with both</font>

795
00:35:11,980 --> 00:35:15,820
<font color="#E5E5E5">statistic oh and security skill sets to</font>

796
00:35:14,050 --> 00:35:18,250
develop robust systems and evaluate

797
00:35:15,820 --> 00:35:20,290
these infrastructures so in other words

798
00:35:18,250 --> 00:35:21,850
<font color="#CCCCCC">things should learn it in the next</font><font color="#E5E5E5"> five</font>

799
00:35:20,290 --> 00:35:24,930
years it's going to be it's going to be

800
00:35:21,850 --> 00:35:27,930
very important and that's<font color="#E5E5E5"> all I have</font>

801
00:35:24,930 --> 00:35:27,930
<font color="#E5E5E5">thank</font>

802
00:35:32,490 --> 00:35:49,569
hi so any questions<font color="#E5E5E5"> they're there yeah</font>

803
00:35:47,500 --> 00:35:52,150
so the<font color="#CCCCCC"> question is are</font><font color="#E5E5E5"> there any</font>

804
00:35:49,570 --> 00:35:55,150
effective countermeasures that i have

805
00:35:52,150 --> 00:35:56,440
found to be effective during the

806
00:35:55,150 --> 00:36:01,500
development of machine learning systems

807
00:35:56,440 --> 00:36:04,360
yeah so as<font color="#CCCCCC"> i mentioned the adversarial</font>

808
00:36:01,500 --> 00:36:08,200
attack vector is only one very

809
00:36:04,360 --> 00:36:10,600
particular attack vector against against

810
00:36:08,200 --> 00:36:12,759
machine learning there<font color="#E5E5E5"> are other like</font>

811
00:36:10,600 --> 00:36:14,440
system level vulnerabilities in many

812
00:36:12,760 --> 00:36:16,570
machine learning frameworks that affect

813
00:36:14,440 --> 00:36:19,090
how accurate the training is done

814
00:36:16,570 --> 00:36:21,130
previously<font color="#E5E5E5"> I did research in</font><font color="#CCCCCC"> Fresno</font>

815
00:36:19,090 --> 00:36:23,020
component analysis poisoning which is

816
00:36:21,130 --> 00:36:25,450
basically<font color="#E5E5E5"> a poisoning during it during</font>

817
00:36:23,020 --> 00:36:26,920
the training phase so principal

818
00:36:25,450 --> 00:36:30,040
component analysis is a technique that's

819
00:36:26,920 --> 00:36:33,370
used to select dimensions it's used for

820
00:36:30,040 --> 00:36:35,800
dimensionality reduction so you run your

821
00:36:33,370 --> 00:36:37,569
your input through this algorithm and it

822
00:36:35,800 --> 00:36:39,310
will tell you which dimensions can help

823
00:36:37,570 --> 00:36:41,860
you make<font color="#CCCCCC"> a better classification and</font>

824
00:36:39,310 --> 00:36:43,990
these these can be poisoned very easily

825
00:36:41,860 --> 00:36:46,660
as well i think<font color="#E5E5E5"> that the point behind</font>

826
00:36:43,990 --> 00:36:48,970
this is the same as any other kind of

827
00:36:46,660 --> 00:36:50,799
kind of system you have to do some<font color="#CCCCCC"> kind</font>

828
00:36:48,970 --> 00:36:52,569
of penetration testing because<font color="#E5E5E5"> different</font>

829
00:36:50,800 --> 00:36:54,340
systems have different objectives and

830
00:36:52,570 --> 00:36:56,200
the attack vectors for each of these

831
00:36:54,340 --> 00:36:59,380
<font color="#E5E5E5">systems are different in some models</font>

832
00:36:56,200 --> 00:37:01,830
<font color="#E5E5E5">it's applicable that adversarial samples</font>

833
00:36:59,380 --> 00:37:04,480
should be should<font color="#CCCCCC"> be protected against</font>

834
00:37:01,830 --> 00:37:06,580
for example if you<font color="#CCCCCC"> look at the</font><font color="#E5E5E5"> gmail</font>

835
00:37:04,480 --> 00:37:10,350
mail<font color="#CCCCCC"> rare classifier which classifies</font>

836
00:37:06,580 --> 00:37:12,610
PDF documents for for for malware

837
00:37:10,350 --> 00:37:14,680
different researchers that showed that

838
00:37:12,610 --> 00:37:17,170
<font color="#E5E5E5">they can generate malware samples that</font>

839
00:37:14,680 --> 00:37:18,640
will actually bypass this and the<font color="#E5E5E5"> prize</font>

840
00:37:17,170 --> 00:37:21,460
<font color="#E5E5E5">is that they actually get their samples</font>

841
00:37:18,640 --> 00:37:23,170
on to someone else's machine and it gets

842
00:37:21,460 --> 00:37:24,970
past emails memory classifier which is

843
00:37:23,170 --> 00:37:28,990
which is done using a deep learning

844
00:37:24,970 --> 00:37:30,700
model so the<font color="#E5E5E5"> way</font><font color="#CCCCCC"> that you make these</font>

845
00:37:28,990 --> 00:37:33,729
better is to basically generate

846
00:37:30,700 --> 00:37:35,740
adversarial samples and to train your

847
00:37:33,730 --> 00:37:37,290
model with these adversarial samples and

848
00:37:35,740 --> 00:37:39,790
basically it's like<font color="#E5E5E5"> telling a child that</font>

849
00:37:37,290 --> 00:37:42,279
you<font color="#E5E5E5"> made this you made this mistake in</font>

850
00:37:39,790 --> 00:37:45,250
the past but then I'm going<font color="#E5E5E5"> to teach</font><font color="#CCCCCC"> you</font>

851
00:37:42,280 --> 00:37:46,090
that this actually<font color="#E5E5E5"> is a ship this is</font>

852
00:37:45,250 --> 00:37:48,280
actually a dog

853
00:37:46,090 --> 00:37:50,110
and you shouldn't<font color="#E5E5E5"> make the wrong you</font>

854
00:37:48,280 --> 00:37:52,840
shouldn't make the wrong decision again

855
00:37:50,110 --> 00:37:54,310
in the future so that's just<font color="#E5E5E5"> one of the</font>

856
00:37:52,840 --> 00:38:03,840
ways that they can be used and<font color="#E5E5E5"> their</font>

857
00:37:54,310 --> 00:38:07,710
many other okay any more questions cool

858
00:38:03,840 --> 00:38:07,710
thank you<font color="#E5E5E5"> thanks</font>

