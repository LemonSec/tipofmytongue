1
00:00:02,530 --> 00:00:11,200
right hello thank you very much for the

2
00:00:08,530 --> 00:00:13,690
introduction so as you heard my name is

3
00:00:11,200 --> 00:00:15,809
Marco and this is my colleague anis and

4
00:00:13,690 --> 00:00:18,730
today we will talk a little bit about

5
00:00:15,810 --> 00:00:23,740
challenges and pitfalls of modern

6
00:00:18,730 --> 00:00:25,710
computer vision models so we are coming

7
00:00:23,740 --> 00:00:29,140
from data science team from stevia group

8
00:00:25,710 --> 00:00:31,420
if you haven't heard about Styria it's

9
00:00:29,140 --> 00:00:34,449
the biggest regional and biggest

10
00:00:31,420 --> 00:00:36,460
Croatian media company so if you are

11
00:00:34,449 --> 00:00:39,339
from the region then you probably heard

12
00:00:36,460 --> 00:00:41,860
of valujet rosada which released new

13
00:00:39,340 --> 00:00:46,450
chicago post on dynamic and these are

14
00:00:41,860 --> 00:00:49,809
all stereo brands so today we will talk

15
00:00:46,450 --> 00:00:52,539
about four start just briefly about the

16
00:00:49,809 --> 00:00:55,360
evolution of computer vision models how

17
00:00:52,539 --> 00:00:57,340
these models evolved from handcrafted

18
00:00:55,360 --> 00:00:59,640
features to powerful deep learning

19
00:00:57,340 --> 00:01:02,340
models this is important to understand

20
00:00:59,640 --> 00:01:05,860
the problems that my colleague will

21
00:01:02,340 --> 00:01:07,750
demonstrate later just briefly about

22
00:01:05,860 --> 00:01:10,810
high dependency on the training set

23
00:01:07,750 --> 00:01:13,000
distribution of these models a bit some

24
00:01:10,810 --> 00:01:16,240
practical examples of specific

25
00:01:13,000 --> 00:01:19,750
classifiers and then we will speak a

26
00:01:16,240 --> 00:01:23,259
little bit about self-driving cars as an

27
00:01:19,750 --> 00:01:25,049
attractive use case in this field and

28
00:01:23,259 --> 00:01:27,820
after that my colleague anis will

29
00:01:25,049 --> 00:01:29,590
demonstrate some other serial attacks on

30
00:01:27,820 --> 00:01:32,860
deep learning models we even made some

31
00:01:29,590 --> 00:01:38,400
small demo demo project just for this

32
00:01:32,860 --> 00:01:41,860
lecture and he will also present some

33
00:01:38,400 --> 00:01:46,180
strategies for defending against this

34
00:01:41,860 --> 00:01:48,850
kind of attacks so for start regarding

35
00:01:46,180 --> 00:01:51,369
the evolution of computer vision models

36
00:01:48,850 --> 00:01:54,429
in general we had big breakthroughs and

37
00:01:51,369 --> 00:01:59,490
I would even say revolution back five

38
00:01:54,430 --> 00:02:03,930
years ago somewhere around to 2011 2012

39
00:01:59,490 --> 00:02:06,969
with the emergence of new technologies

40
00:02:03,930 --> 00:02:09,280
convolutional neural networks and the

41
00:02:06,969 --> 00:02:11,269
reason for this is twofold the first is

42
00:02:09,280 --> 00:02:14,120
that we got our

43
00:02:11,270 --> 00:02:16,190
hands on the big amounts of data so big

44
00:02:14,120 --> 00:02:19,310
data sits most notably image metadata

45
00:02:16,190 --> 00:02:23,840
set that is used for the most scientific

46
00:02:19,310 --> 00:02:27,710
research and also the evolvement of the

47
00:02:23,840 --> 00:02:29,630
of the harder' so before that we had

48
00:02:27,710 --> 00:02:32,420
handcrafted features the pictures like

49
00:02:29,630 --> 00:02:34,820
the ones on the image so here we can see

50
00:02:32,420 --> 00:02:37,850
some edge detectors and some safety

51
00:02:34,820 --> 00:02:39,950
features that detected on the object but

52
00:02:37,850 --> 00:02:42,590
as you can see if we slightly move the

53
00:02:39,950 --> 00:02:45,739
object around we lose some features and

54
00:02:42,590 --> 00:02:48,710
get some other ones recognized and these

55
00:02:45,740 --> 00:02:50,540
features are really unstable and shallow

56
00:02:48,710 --> 00:02:54,200
machine learning models place on top of

57
00:02:50,540 --> 00:02:56,630
this just didn't work quite well so with

58
00:02:54,200 --> 00:02:57,850
the emergence of powerful hardware and

59
00:02:56,630 --> 00:03:01,940
lots of data

60
00:02:57,850 --> 00:03:04,880
we got a resurrection of neural network

61
00:03:01,940 --> 00:03:07,700
models most notably C NS which are

62
00:03:04,880 --> 00:03:10,100
trying to mimic the way how the visual

63
00:03:07,700 --> 00:03:12,049
cortex of mammals works so these

64
00:03:10,100 --> 00:03:14,359
networks have two-dimensional layers

65
00:03:12,050 --> 00:03:17,770
which we call convolutional filters

66
00:03:14,360 --> 00:03:21,709
placed in this in a series of layers and

67
00:03:17,770 --> 00:03:24,890
with each layer the network uses more

68
00:03:21,709 --> 00:03:28,010
and more abstract features so for the

69
00:03:24,890 --> 00:03:30,649
image on the input first layers detect

70
00:03:28,010 --> 00:03:33,500
primitive and less complex features and

71
00:03:30,650 --> 00:03:35,450
the next layers detect more and more

72
00:03:33,500 --> 00:03:38,390
complicated and abstract features and in

73
00:03:35,450 --> 00:03:41,030
the end at the end we have this standard

74
00:03:38,390 --> 00:03:43,790
traditional model airplanes perceptron a

75
00:03:41,030 --> 00:03:45,680
neural network and and the exit of the

76
00:03:43,790 --> 00:03:48,769
network we have probabilities for

77
00:03:45,680 --> 00:03:52,459
classes presented in the training data

78
00:03:48,770 --> 00:03:53,810
set so these networks are really

79
00:03:52,459 --> 00:03:55,280
powerful

80
00:03:53,810 --> 00:03:58,340
they we say that they are really

81
00:03:55,280 --> 00:04:00,740
descriptive they can really well fit the

82
00:03:58,340 --> 00:04:04,070
data but at the same time this is the

83
00:04:00,740 --> 00:04:04,880
their drawback because they tend to over

84
00:04:04,070 --> 00:04:08,930
fit the data

85
00:04:04,880 --> 00:04:11,060
describe it too well and for specific

86
00:04:08,930 --> 00:04:14,209
use cases we need to have specific

87
00:04:11,060 --> 00:04:16,190
models trained on specific data and this

88
00:04:14,209 --> 00:04:19,100
is the reason why just plugging in some

89
00:04:16,190 --> 00:04:22,250
third party API won't work for your

90
00:04:19,100 --> 00:04:24,620
specific use case and the next reason

91
00:04:22,250 --> 00:04:26,660
why this would be a bad idea is because

92
00:04:24,620 --> 00:04:30,530
we do not have this API under our

93
00:04:26,660 --> 00:04:32,389
control so we do not know which data set

94
00:04:30,530 --> 00:04:34,669
was used for training and what

95
00:04:32,389 --> 00:04:36,470
distribution of this data set is and

96
00:04:34,669 --> 00:04:38,169
even more important we do not know how

97
00:04:36,470 --> 00:04:41,540
this model works we even do not know

98
00:04:38,169 --> 00:04:44,840
sometimes this with our own models so

99
00:04:41,540 --> 00:04:48,199
this is one example of of a model that

100
00:04:44,840 --> 00:04:50,719
we trained on our own data and when

101
00:04:48,199 --> 00:04:54,979
presented these images as you can see

102
00:04:50,720 --> 00:04:57,320
both images show BMW car and when we ask

103
00:04:54,979 --> 00:04:59,900
our model what is this on the image for

104
00:04:57,320 --> 00:05:01,940
the left image it says BMW car but for

105
00:04:59,900 --> 00:05:04,940
the right image it says PlayStation

106
00:05:01,940 --> 00:05:05,300
video game why is it so and is it good

107
00:05:04,940 --> 00:05:08,510
for us

108
00:05:05,300 --> 00:05:12,380
well we train this on our own data set

109
00:05:08,510 --> 00:05:14,810
where we also had some PlayStation or

110
00:05:12,380 --> 00:05:18,500
computer video games as objects in the

111
00:05:14,810 --> 00:05:21,289
database and this image on the right is

112
00:05:18,500 --> 00:05:24,410
not the real photo of a car it's a

113
00:05:21,289 --> 00:05:28,340
render or a drawing and it reminds the

114
00:05:24,410 --> 00:05:30,620
model probably on the need for speed or

115
00:05:28,340 --> 00:05:32,900
some similar color from the videogame

116
00:05:30,620 --> 00:05:35,419
and this is the exactly behavior that we

117
00:05:32,900 --> 00:05:38,388
want but if we plug in some third party

118
00:05:35,419 --> 00:05:40,669
API probably for both of these images we

119
00:05:38,389 --> 00:05:42,650
will get result that it's a BMW car and

120
00:05:40,669 --> 00:05:49,669
this is not something that we want to

121
00:05:42,650 --> 00:05:51,919
achieve of course so these problems are

122
00:05:49,669 --> 00:05:54,109
present in various different use cases

123
00:05:51,919 --> 00:05:56,810
like security and surveillance systems

124
00:05:54,110 --> 00:05:59,510
fraud detection drones self-driving

125
00:05:56,810 --> 00:06:01,849
vehicles and so on so we will now focus

126
00:05:59,510 --> 00:06:04,940
a little bit on self-driving vehicles

127
00:06:01,849 --> 00:06:06,469
because they are so attractive now as

128
00:06:04,940 --> 00:06:08,800
you can see here on the image we have

129
00:06:06,470 --> 00:06:12,169
some interesting interesting result

130
00:06:08,800 --> 00:06:14,150
getting back a few months ago Elon Musk

131
00:06:12,169 --> 00:06:16,070
said that probably we will not need

132
00:06:14,150 --> 00:06:19,400
lighter systems on self-driving cars

133
00:06:16,070 --> 00:06:22,070
that we can rely completely on computer

134
00:06:19,400 --> 00:06:24,380
vision and cameras and why he said so

135
00:06:22,070 --> 00:06:27,080
because he realized that how how

136
00:06:24,380 --> 00:06:30,050
powerful these model models are and the

137
00:06:27,080 --> 00:06:32,240
fact that humans also drive the vehicles

138
00:06:30,050 --> 00:06:34,250
relying only on the cars okay we are

139
00:06:32,240 --> 00:06:37,280
only on the eyes okay we have two eyes

140
00:06:34,250 --> 00:06:38,449
and we can we have stereo vision so we

141
00:06:37,280 --> 00:06:41,659
have also some that

142
00:06:38,449 --> 00:06:43,580
but not just the depth that is important

143
00:06:41,659 --> 00:06:46,009
but also the context in the realized

144
00:06:43,580 --> 00:06:48,710
realization and prior knowledge that we

145
00:06:46,009 --> 00:06:51,289
have about the world world around us so

146
00:06:48,710 --> 00:06:54,650
here for example you can see a person

147
00:06:51,289 --> 00:06:57,139
and bicycle imprinted actually on the

148
00:06:54,650 --> 00:06:59,120
back of the car and the model thinks

149
00:06:57,139 --> 00:07:01,159
that it's bicycle or person in front of

150
00:06:59,120 --> 00:07:04,279
it so it's not the behavior that you

151
00:07:01,159 --> 00:07:05,930
want and this is just one example of

152
00:07:04,279 --> 00:07:08,479
potential problems there are many other

153
00:07:05,930 --> 00:07:10,969
problems and for some of them my

154
00:07:08,479 --> 00:07:24,438
colleague Ennis will now show you a few

155
00:07:10,969 --> 00:07:28,370
more examples hello I'm going to show

156
00:07:24,439 --> 00:07:34,909
you how to train a neural network that

157
00:07:28,370 --> 00:07:37,460
can drive a car so what what you need

158
00:07:34,909 --> 00:07:40,819
for portraying any network you need some

159
00:07:37,460 --> 00:07:45,739
training data and in this case what you

160
00:07:40,819 --> 00:07:49,580
need is some video footage of car

161
00:07:45,740 --> 00:07:54,069
driving and for each of the frames of

162
00:07:49,580 --> 00:08:01,370
this video you need information about

163
00:07:54,069 --> 00:08:04,360
velocity a gas brake and what you would

164
00:08:01,370 --> 00:08:06,639
do you would create an architect's

165
00:08:04,360 --> 00:08:11,509
conversion neural network architecture

166
00:08:06,639 --> 00:08:16,909
which takes as an input image or couple

167
00:08:11,509 --> 00:08:23,270
of images from the video and as an

168
00:08:16,909 --> 00:08:29,629
output it's a returns velocity steering

169
00:08:23,270 --> 00:08:35,568
angle brake gas and also we can have an

170
00:08:29,629 --> 00:08:37,449
as an output we want to recognize the

171
00:08:35,568 --> 00:08:41,299
objects for example traffic signs

172
00:08:37,448 --> 00:08:43,958
pedestrians and so on so you can do this

173
00:08:41,299 --> 00:08:47,290
in one convolutional network

174
00:08:43,958 --> 00:08:49,469
optimization objective for this model

175
00:08:47,290 --> 00:08:53,579
would be

176
00:08:49,470 --> 00:08:57,060
you try to to predict the values for

177
00:08:53,580 --> 00:08:59,580
steering angle and so on that are closed

178
00:08:57,060 --> 00:09:02,520
as as close as possible to to your

179
00:08:59,580 --> 00:09:05,400
training data from so real driving

180
00:09:02,520 --> 00:09:08,819
footage or some data that was created in

181
00:09:05,400 --> 00:09:10,650
a self-driving cars simulator for

182
00:09:08,820 --> 00:09:14,670
example for simulating dangerous

183
00:09:10,650 --> 00:09:19,020
situations to enrich your data set on

184
00:09:14,670 --> 00:09:21,510
the other hand for object recognition

185
00:09:19,020 --> 00:09:27,350
for example traffic signs

186
00:09:21,510 --> 00:09:30,420
what you need is which you need is to

187
00:09:27,350 --> 00:09:33,630
classification eed to classify you need

188
00:09:30,420 --> 00:09:34,140
to classify them as a as accurately as

189
00:09:33,630 --> 00:09:38,130
possible

190
00:09:34,140 --> 00:09:41,699
and here we use usually a cross entropy

191
00:09:38,130 --> 00:09:48,180
loss function the details are not that

192
00:09:41,700 --> 00:09:49,410
important so here are some results for

193
00:09:48,180 --> 00:09:54,239
example this is a publicly available

194
00:09:49,410 --> 00:09:58,500
data set and it is of German traffic

195
00:09:54,240 --> 00:10:03,060
signs and as you can see these designs

196
00:09:58,500 --> 00:10:06,630
are not really clear images yet the

197
00:10:03,060 --> 00:10:09,510
classification accuracy is above 99% and

198
00:10:06,630 --> 00:10:14,420
I have personally trained the model to

199
00:10:09,510 --> 00:10:20,310
do this and it works really well also

200
00:10:14,420 --> 00:10:24,390
you you can using this method as I've

201
00:10:20,310 --> 00:10:28,410
mentioned earlier you can really train

202
00:10:24,390 --> 00:10:31,560
the car to to drive reasonably well in a

203
00:10:28,410 --> 00:10:38,670
in a simulator so it sounds simple

204
00:10:31,560 --> 00:10:42,420
enough so so now let's the recent

205
00:10:38,670 --> 00:10:45,120
research has shown that these neural

206
00:10:42,420 --> 00:10:47,819
networks are can be easily fooled so

207
00:10:45,120 --> 00:10:52,530
let's say you have a classifier which

208
00:10:47,820 --> 00:10:54,720
needs to predict 1000 classes which

209
00:10:52,530 --> 00:10:58,529
means that you want to predict if

210
00:10:54,720 --> 00:11:02,970
something is an animal car

211
00:10:58,529 --> 00:11:05,999
type of animal for example a toy or

212
00:11:02,970 --> 00:11:08,550
something like that and then you would

213
00:11:05,999 --> 00:11:12,149
expect that if you put some kind of a

214
00:11:08,550 --> 00:11:15,628
random image for example like the noise

215
00:11:12,149 --> 00:11:18,180
above or something like these funny

216
00:11:15,629 --> 00:11:25,050
structures that the network will be

217
00:11:18,180 --> 00:11:28,769
unsure in its predictions but they they

218
00:11:25,050 --> 00:11:30,719
have actually created these examples

219
00:11:28,769 --> 00:11:35,839
specifically using evolutionary

220
00:11:30,720 --> 00:11:41,220
algorithms and the networks achieved

221
00:11:35,839 --> 00:11:43,649
were confident above 99.6% that this

222
00:11:41,220 --> 00:11:47,999
belongs to some class for example king

223
00:11:43,649 --> 00:11:51,329
penguin down left starfish baseball

224
00:11:47,999 --> 00:11:53,279
which actually makes some sense and the

225
00:11:51,329 --> 00:11:56,489
noise doesn't make any sense but the

226
00:11:53,279 --> 00:12:00,680
network is very confident that it saw

227
00:11:56,490 --> 00:12:05,009
something that isn't there the more the

228
00:12:00,680 --> 00:12:13,128
more interesting example of this is that

229
00:12:05,009 --> 00:12:15,470
we try to change an input image in

230
00:12:13,129 --> 00:12:22,319
indistinguishable way to human eye and

231
00:12:15,470 --> 00:12:24,949
then this produces the the image which

232
00:12:22,319 --> 00:12:27,329
will be totally misclassified by a

233
00:12:24,949 --> 00:12:33,620
trained neural network which achieves

234
00:12:27,329 --> 00:12:38,519
high accuracies on the normal data so

235
00:12:33,620 --> 00:12:42,209
we've actually created a demo not not

236
00:12:38,519 --> 00:12:45,929
trained on traffic signs but on human

237
00:12:42,209 --> 00:12:47,719
faces the data sets used was faces in

238
00:12:45,929 --> 00:12:52,559
the wild which is a publicly available

239
00:12:47,720 --> 00:12:55,290
data set of human faces and the model

240
00:12:52,559 --> 00:12:59,100
achieves more than 95% of the accuracy

241
00:12:55,290 --> 00:13:02,279
actually on on good images this

242
00:12:59,100 --> 00:13:07,110
percentage would be much higher and also

243
00:13:02,279 --> 00:13:09,809
there was you could train that to go up

244
00:13:07,110 --> 00:13:12,440
and I believe that you can agree that

245
00:13:09,809 --> 00:13:17,650
95% is a good accuracy

246
00:13:12,440 --> 00:13:17,650
now these four images that you can see

247
00:13:17,740 --> 00:13:25,400
below are the images that were not part

248
00:13:22,250 --> 00:13:30,830
of the phases in the wild data set and

249
00:13:25,400 --> 00:13:33,730
we want to create images which would be

250
00:13:30,830 --> 00:13:36,620
indistinguishable from these images and

251
00:13:33,730 --> 00:13:42,550
the network would classify them

252
00:13:36,620 --> 00:13:46,100
completely wrongly so how do you do this

253
00:13:42,550 --> 00:13:50,660
you take your original model and then

254
00:13:46,100 --> 00:13:53,930
you try to create an another model which

255
00:13:50,660 --> 00:13:58,939
takes an images and input and out with

256
00:13:53,930 --> 00:14:02,060
another image so this this other image

257
00:13:58,940 --> 00:14:05,630
has a constraint and when you train

258
00:14:02,060 --> 00:14:08,109
neural networks you you this constraint

259
00:14:05,630 --> 00:14:14,689
means means that you punish the network

260
00:14:08,110 --> 00:14:17,930
if it turns away from these from this

261
00:14:14,690 --> 00:14:21,710
constraint and you and you reward the

262
00:14:17,930 --> 00:14:24,859
network if these constraints are

263
00:14:21,710 --> 00:14:27,620
satisfied so the constraint was that the

264
00:14:24,860 --> 00:14:31,100
image as close as possible to the

265
00:14:27,620 --> 00:14:35,180
original image and that the original

266
00:14:31,100 --> 00:14:37,220
classifier makes the mistake is as large

267
00:14:35,180 --> 00:14:40,099
as possible so these are these are the

268
00:14:37,220 --> 00:14:42,620
constraints just explain what does it

269
00:14:40,100 --> 00:14:48,500
mean to do to have a how do we measure

270
00:14:42,620 --> 00:14:51,110
is the image visually close you you look

271
00:14:48,500 --> 00:14:53,960
at pixel values for example you know

272
00:14:51,110 --> 00:15:02,570
that images are are encoded as an array

273
00:14:53,960 --> 00:15:04,339
of 0 to 120 1 to 255 and we try to

274
00:15:02,570 --> 00:15:06,020
reduce the mean squared error between

275
00:15:04,339 --> 00:15:10,070
the original image and the refined

276
00:15:06,020 --> 00:15:12,699
images much as possible so let's let's

277
00:15:10,070 --> 00:15:12,700
go into demo

278
00:15:19,890 --> 00:15:27,340
okay so some Python code I don't don't

279
00:15:23,350 --> 00:15:33,250
don't get scared will load the networks

280
00:15:27,340 --> 00:15:35,530
both the refined network and the

281
00:15:33,250 --> 00:15:38,320
original network now we'll try to

282
00:15:35,530 --> 00:15:42,040
classify the images you've seen in the

283
00:15:38,320 --> 00:15:50,620
previous slide and see what does our

284
00:15:42,040 --> 00:15:53,589
network tell us so we have so just

285
00:15:50,620 --> 00:15:57,850
explain for classifier for binary

286
00:15:53,590 --> 00:16:02,680
classifier if some class is above 50%

287
00:15:57,850 --> 00:16:04,900
then we designated as that class so you

288
00:16:02,680 --> 00:16:06,280
can look at these numbers as

289
00:16:04,900 --> 00:16:11,949
probabilities although I would not

290
00:16:06,280 --> 00:16:14,500
recommend so so the first image was well

291
00:16:11,950 --> 00:16:18,940
classified so it's a female the second

292
00:16:14,500 --> 00:16:20,890
image is a male the third image was

293
00:16:18,940 --> 00:16:24,010
classified as female and the fourth

294
00:16:20,890 --> 00:16:26,500
image was classified as as male now

295
00:16:24,010 --> 00:16:29,170
let's let's try to create an adversarial

296
00:16:26,500 --> 00:16:32,080
example so the adversarial example

297
00:16:29,170 --> 00:16:35,290
should get misclassified by this network

298
00:16:32,080 --> 00:16:37,450
so these adversarial Network examples

299
00:16:35,290 --> 00:16:40,209
were created from one network and then

300
00:16:37,450 --> 00:16:43,240
are put as an input to another network

301
00:16:40,210 --> 00:16:45,580
so let's see the results now so these

302
00:16:43,240 --> 00:16:48,970
are the refined images so these are the

303
00:16:45,580 --> 00:16:54,220
refined images and look at the

304
00:16:48,970 --> 00:16:58,300
predictions now the Lydia Lydia is 98%

305
00:16:54,220 --> 00:17:03,370
male this gentleman here is 98% female

306
00:16:58,300 --> 00:17:06,609
this lady here is again about about 98%

307
00:17:03,370 --> 00:17:12,010
and this gentleman here is also

308
00:17:06,609 --> 00:17:15,629
misclassified let's look at the original

309
00:17:12,010 --> 00:17:18,579
images one more time so you can see that

310
00:17:15,630 --> 00:17:20,920
obviously there's insignificant

311
00:17:18,579 --> 00:17:22,389
difference between these images and we

312
00:17:20,920 --> 00:17:26,589
can actually calculate for the last

313
00:17:22,390 --> 00:17:29,940
image I don't know if the projector can

314
00:17:26,589 --> 00:17:31,760
allow us to see these changes

315
00:17:29,940 --> 00:17:37,529
[Music]

316
00:17:31,760 --> 00:17:40,019
so you see some almost random noise and

317
00:17:37,529 --> 00:17:45,779
this is the difference this is different

318
00:17:40,019 --> 00:17:48,090
so it's it's minimal oh I almost forgot

319
00:17:45,779 --> 00:17:50,460
now you might think that these images

320
00:17:48,090 --> 00:17:56,610
are that these networks are very

321
00:17:50,460 --> 00:17:58,320
sensitive to to these modifications to

322
00:17:56,610 --> 00:18:00,299
these modifications obviously they are

323
00:17:58,320 --> 00:18:02,820
but two random modifications or

324
00:18:00,299 --> 00:18:03,960
modifications in general just just to

325
00:18:02,820 --> 00:18:05,639
tell you something about the training

326
00:18:03,960 --> 00:18:08,309
process when you train the neural

327
00:18:05,639 --> 00:18:11,279
network you usually want to enlarge your

328
00:18:08,309 --> 00:18:15,210
data set by doing some transformations

329
00:18:11,279 --> 00:18:17,789
which will simulate the stuff that

330
00:18:15,210 --> 00:18:21,960
really happens in you know reality for

331
00:18:17,789 --> 00:18:25,320
example if you take this image and then

332
00:18:21,960 --> 00:18:27,539
you add some brightness or reduce some

333
00:18:25,320 --> 00:18:30,120
brightness you can imagine that this

334
00:18:27,539 --> 00:18:32,669
image would be taken under under

335
00:18:30,120 --> 00:18:36,000
different lighting lighting conditions

336
00:18:32,669 --> 00:18:38,639
and both of these images would be

337
00:18:36,000 --> 00:18:40,769
recognized recognizable to us and we

338
00:18:38,639 --> 00:18:43,820
would classify it correctly so the

339
00:18:40,769 --> 00:18:48,179
algorithm needs to be aware of these

340
00:18:43,820 --> 00:18:52,529
small changes but here I'm going to to

341
00:18:48,179 --> 00:18:55,200
destroy the image by adding a large

342
00:18:52,529 --> 00:18:57,480
vertical stripe or horizontal stripe

343
00:18:55,200 --> 00:19:00,720
across the image and then put it to

344
00:18:57,480 --> 00:19:05,669
original network and try to get try to

345
00:19:00,720 --> 00:19:08,070
see what the classification is so here's

346
00:19:05,669 --> 00:19:09,419
what we've done so this is now input to

347
00:19:08,070 --> 00:19:12,240
our original network

348
00:19:09,419 --> 00:19:14,190
Lydia is still classified as female this

349
00:19:12,240 --> 00:19:15,529
gentleman is still classified as male

350
00:19:14,190 --> 00:19:20,549
this lady

351
00:19:15,529 --> 00:19:25,169
female and male so this this huge

352
00:19:20,549 --> 00:19:27,779
transformation has not made any has

353
00:19:25,169 --> 00:19:30,720
reduced the confidences and probably

354
00:19:27,779 --> 00:19:33,320
would reduce the accuracy of a model but

355
00:19:30,720 --> 00:19:39,080
model is fairly robust to even such a

356
00:19:33,320 --> 00:19:39,080
huge change so now let's try

357
00:19:40,160 --> 00:19:54,540
I can't type with one hand okay let's

358
00:19:50,340 --> 00:19:59,760
try this one okay again as as you can

359
00:19:54,540 --> 00:20:02,310
see everything is as we would expect the

360
00:19:59,760 --> 00:20:07,940
third image was almost classified as

361
00:20:02,310 --> 00:20:19,500
mail but our network defeated it so

362
00:20:07,940 --> 00:20:20,310
let's continue representation okay how

363
00:20:19,500 --> 00:20:24,540
is this possible

364
00:20:20,310 --> 00:20:26,220
well let's let's take a two-dimensional

365
00:20:24,540 --> 00:20:28,470
data and let's say that you have two

366
00:20:26,220 --> 00:20:33,270
classes one are X's and the other ones

367
00:20:28,470 --> 00:20:35,220
are circles so the dashed line is the

368
00:20:33,270 --> 00:20:40,860
real line that's separating the classes

369
00:20:35,220 --> 00:20:43,050
the full line is our model decision

370
00:20:40,860 --> 00:20:45,270
boundary which which was created during

371
00:20:43,050 --> 00:20:49,620
the training process as you can see that

372
00:20:45,270 --> 00:20:53,250
some some of the data points are close

373
00:20:49,620 --> 00:20:54,840
to decision boundary and you need to you

374
00:20:53,250 --> 00:20:57,450
can imagine that these are images but

375
00:20:54,840 --> 00:21:02,370
just two dimensional data now you can

376
00:20:57,450 --> 00:21:09,270
expect to make some small perturbation

377
00:21:02,370 --> 00:21:13,919
or small nudge in a in a in a direction

378
00:21:09,270 --> 00:21:16,170
of changing the class and and this this

379
00:21:13,920 --> 00:21:18,180
would be this would still be a point

380
00:21:16,170 --> 00:21:22,950
with closed coordinates

381
00:21:18,180 --> 00:21:24,630
so close image visually but the class

382
00:21:22,950 --> 00:21:27,780
would be different and now you might ask

383
00:21:24,630 --> 00:21:31,020
okay but you have these images that are

384
00:21:27,780 --> 00:21:33,990
far away from the from decision boundary

385
00:21:31,020 --> 00:21:40,310
but you can imagine that this is just 2d

386
00:21:33,990 --> 00:21:46,080
this is just a 2d and images are usually

387
00:21:40,310 --> 00:21:47,669
256 x 256 x free channels which is about

388
00:21:46,080 --> 00:21:50,100
two hundred thousand and this is a

389
00:21:47,670 --> 00:21:52,390
standard input to a convolutional neural

390
00:21:50,100 --> 00:21:54,250
network so in

391
00:21:52,390 --> 00:21:56,470
in two hundred thousand dimensional

392
00:21:54,250 --> 00:21:59,800
space there are a lot of directions

393
00:21:56,470 --> 00:22:02,080
where you can push your point your data

394
00:21:59,800 --> 00:22:07,860
point and cross the decision boundary

395
00:22:02,080 --> 00:22:10,419
and this is how it gets misclassified so

396
00:22:07,860 --> 00:22:13,600
one might think okay these are

397
00:22:10,420 --> 00:22:15,480
artificial examples can we do

398
00:22:13,600 --> 00:22:20,230
modifications in physical world and

399
00:22:15,480 --> 00:22:25,350
still get this still get this problem so

400
00:22:20,230 --> 00:22:29,560
the recent research was published and

401
00:22:25,350 --> 00:22:34,719
what they did is found which areas of

402
00:22:29,560 --> 00:22:37,830
the image could be altered by by putting

403
00:22:34,720 --> 00:22:40,090
some stickers or something some small

404
00:22:37,830 --> 00:22:42,389
modifications which would resemble

405
00:22:40,090 --> 00:22:48,250
something that you can find in reality

406
00:22:42,390 --> 00:22:52,140
and if this could confuse the network to

407
00:22:48,250 --> 00:22:57,100
misclassify and they actually found that

408
00:22:52,140 --> 00:23:00,640
these example are fairly robust to to to

409
00:22:57,100 --> 00:23:03,219
for example rotations of design and

410
00:23:00,640 --> 00:23:06,820
these are real images and the stop sign

411
00:23:03,220 --> 00:23:10,510
is misclassified as speed limit 45 which

412
00:23:06,820 --> 00:23:13,990
you don't want so getting back to a

413
00:23:10,510 --> 00:23:16,330
software and car example so you know how

414
00:23:13,990 --> 00:23:20,230
do we defend against these attacks so

415
00:23:16,330 --> 00:23:22,480
there's no universal defense but here

416
00:23:20,230 --> 00:23:27,100
here are some examples for example we

417
00:23:22,480 --> 00:23:29,500
can enrich our train data with with

418
00:23:27,100 --> 00:23:31,139
adversarial examples and then our models

419
00:23:29,500 --> 00:23:35,830
will become more robust to these

420
00:23:31,140 --> 00:23:39,930
examples unfortunately this has some

421
00:23:35,830 --> 00:23:44,020
drawbacks and this is that although we

422
00:23:39,930 --> 00:23:49,720
although your network is now more robust

423
00:23:44,020 --> 00:23:52,570
to do two specific methods you still get

424
00:23:49,720 --> 00:23:56,410
the you can still train the adversarial

425
00:23:52,570 --> 00:23:57,939
network - it will just take longer or

426
00:23:56,410 --> 00:24:00,940
you can take a totally different

427
00:23:57,940 --> 00:24:02,890
approach in training these were serial

428
00:24:00,940 --> 00:24:03,930
examples and then you would fool the

429
00:24:02,890 --> 00:24:06,210
network anyway

430
00:24:03,930 --> 00:24:09,780
you're still operating in a very high

431
00:24:06,210 --> 00:24:11,820
dimensional space where there are a lot

432
00:24:09,780 --> 00:24:16,080
of ways in which you can jitter the data

433
00:24:11,820 --> 00:24:19,110
and still get the adversarial examples

434
00:24:16,080 --> 00:24:21,929
one one other way is to hide the

435
00:24:19,110 --> 00:24:26,659
gradients what does this mean well our

436
00:24:21,930 --> 00:24:30,510
networks as if as you've seen output

437
00:24:26,660 --> 00:24:34,440
probabilities so confidences confidences

438
00:24:30,510 --> 00:24:36,930
the output confidences and if you change

439
00:24:34,440 --> 00:24:39,540
the image a bit for example you change

440
00:24:36,930 --> 00:24:41,760
one pixel then you will you will see

441
00:24:39,540 --> 00:24:45,330
that the confidences also changes it

442
00:24:41,760 --> 00:24:52,700
will not change in any significant way

443
00:24:45,330 --> 00:24:55,800
for some random random change but if you

444
00:24:52,700 --> 00:24:58,560
if you use this information how the

445
00:24:55,800 --> 00:25:00,600
change occurred then you can you can

446
00:24:58,560 --> 00:25:02,340
take the gradient you can taint the

447
00:25:00,600 --> 00:25:04,590
direction of biggest changing negative

448
00:25:02,340 --> 00:25:07,500
derivative direction which will reduce

449
00:25:04,590 --> 00:25:11,600
the the confidence as much as possible

450
00:25:07,500 --> 00:25:11,600
dude it's enough times and you will get

451
00:25:12,110 --> 00:25:19,530
you will get the adversarial examples so

452
00:25:16,370 --> 00:25:22,260
why not only return the classes why not

453
00:25:19,530 --> 00:25:24,990
our previous training model why not say

454
00:25:22,260 --> 00:25:28,260
this is a stop sign this is a a woman

455
00:25:24,990 --> 00:25:31,140
this is a man so we could we could do

456
00:25:28,260 --> 00:25:34,860
that and then we would not have a

457
00:25:31,140 --> 00:25:37,590
gradient which we could which we could

458
00:25:34,860 --> 00:25:41,610
use so the neural networks are trained

459
00:25:37,590 --> 00:25:44,760
in a way that you take many little steps

460
00:25:41,610 --> 00:25:47,100
in the direction which maximizes or

461
00:25:44,760 --> 00:25:53,129
minimizes your loss minimize your loss

462
00:25:47,100 --> 00:25:55,250
function so you can you can still fold

463
00:25:53,130 --> 00:26:00,810
these networks so this is not enough

464
00:25:55,250 --> 00:26:06,870
because it has been shown that that

465
00:26:00,810 --> 00:26:09,690
adversarial examples are can be

466
00:26:06,870 --> 00:26:14,610
transferred across different

467
00:26:09,690 --> 00:26:17,010
architectures so the DC these examples

468
00:26:14,610 --> 00:26:18,810
can be used on other networks and they

469
00:26:17,010 --> 00:26:22,440
would still be

470
00:26:18,810 --> 00:26:25,980
not always not perhaps as perfect as in

471
00:26:22,440 --> 00:26:29,400
example which over attacking against but

472
00:26:25,980 --> 00:26:31,830
you can do it and so you can train a

473
00:26:29,400 --> 00:26:33,720
subset substitute network to do some job

474
00:26:31,830 --> 00:26:35,730
and then you can train another several

475
00:26:33,720 --> 00:26:39,990
network for this substitute network

476
00:26:35,730 --> 00:26:42,180
where you have have the gradients and

477
00:26:39,990 --> 00:26:45,030
then you can use these adversarial

478
00:26:42,180 --> 00:26:48,870
examples on your own network you want to

479
00:26:45,030 --> 00:26:52,410
to attack so yeah defensive distillation

480
00:26:48,870 --> 00:26:58,649
so this is another technique you can

481
00:26:52,410 --> 00:27:00,210
train another classifier if not you have

482
00:26:58,650 --> 00:27:01,860
your original model then you can train

483
00:27:00,210 --> 00:27:04,410
another model which which doesn't

484
00:27:01,860 --> 00:27:07,139
predict the classes but which predicts

485
00:27:04,410 --> 00:27:09,900
the probabilities of your network which

486
00:27:07,140 --> 00:27:14,820
is trained on some tasks these results

487
00:27:09,900 --> 00:27:17,520
in a smaller decision boundary but what

488
00:27:14,820 --> 00:27:21,360
what this means is that the gradients

489
00:27:17,520 --> 00:27:24,750
will have smaller the changes in the

490
00:27:21,360 --> 00:27:27,510
input data will will make smaller

491
00:27:24,750 --> 00:27:31,490
changes to the output of the of the

492
00:27:27,510 --> 00:27:34,650
network so the gradients are smaller and

493
00:27:31,490 --> 00:27:37,920
then it's you need much more time to

494
00:27:34,650 --> 00:27:43,080
train on an adversarial model to to use

495
00:27:37,920 --> 00:27:44,940
these gradients okay but there are also

496
00:27:43,080 --> 00:27:47,550
drawbacks you can reduce the accuracy of

497
00:27:44,940 --> 00:27:50,310
the original model that's one and the

498
00:27:47,550 --> 00:27:53,370
the other one is that attacker again you

499
00:27:50,310 --> 00:27:55,050
can just use more resources and there's

500
00:27:53,370 --> 00:27:58,590
also something that's that's not here

501
00:27:55,050 --> 00:28:00,840
and that's that we've recently started

502
00:27:58,590 --> 00:28:04,199
using optimization methods which which

503
00:28:00,840 --> 00:28:07,820
are highly adaptive so if they come to

504
00:28:04,200 --> 00:28:11,490
an optimization or landscape which is

505
00:28:07,820 --> 00:28:15,450
fairly flat then the learning rate of an

506
00:28:11,490 --> 00:28:18,060
algorithm is is boosted by a number or

507
00:28:15,450 --> 00:28:23,910
two to number of orders of magnitudes

508
00:28:18,060 --> 00:28:26,639
and and so this landscape is can be you

509
00:28:23,910 --> 00:28:29,710
can go away from this landscape in small

510
00:28:26,640 --> 00:28:32,529
number of steps so you can still train

511
00:28:29,710 --> 00:28:36,370
their network okay there's some few

512
00:28:32,529 --> 00:28:38,740
examples I don't have much time so you

513
00:28:36,370 --> 00:28:43,029
can create adversarial networks for for

514
00:28:38,740 --> 00:28:47,289
example avoiding spam filters for making

515
00:28:43,029 --> 00:28:49,090
illegal content illegal for design web

516
00:28:47,289 --> 00:28:52,360
page which will artificially increase

517
00:28:49,090 --> 00:28:59,139
the ranking ruling out education systems

518
00:28:52,360 --> 00:29:02,168
and so on so back to Marco so judge just

519
00:28:59,140 --> 00:29:04,690
to recap what we just said so we gave a

520
00:29:02,169 --> 00:29:07,630
brief overview of how these models

521
00:29:04,690 --> 00:29:09,159
evolved at all we described how they

522
00:29:07,630 --> 00:29:12,070
depend heavily on the training set

523
00:29:09,159 --> 00:29:13,720
distribution which then translate to all

524
00:29:12,070 --> 00:29:17,230
of the all these problems that we heard

525
00:29:13,720 --> 00:29:19,480
later we take took a look at

526
00:29:17,230 --> 00:29:22,330
self-driving cars as an attractive

527
00:29:19,480 --> 00:29:24,220
example of this and we show some

528
00:29:22,330 --> 00:29:30,070
practical examples of other stereo

529
00:29:24,220 --> 00:29:32,950
Tech's for face detection gender gender

530
00:29:30,070 --> 00:29:36,250
recognition actually then just briefly

531
00:29:32,950 --> 00:29:38,399
we talked about defense strategies how

532
00:29:36,250 --> 00:29:40,630
to defend yourself against these attacks

533
00:29:38,399 --> 00:29:44,229
so hopefully this was an interesting

534
00:29:40,630 --> 00:29:48,309
lecture for you and hopefully we put

535
00:29:44,230 --> 00:29:50,880
some top some stress on importance of

536
00:29:48,309 --> 00:29:53,678
this topic because these models are

537
00:29:50,880 --> 00:29:56,289
becoming more and more popular and

538
00:29:53,679 --> 00:29:58,690
people tend to use them just take them

539
00:29:56,289 --> 00:30:00,600
for granted and just plug it plug them

540
00:29:58,690 --> 00:30:04,870
in to their data set in their use case

541
00:30:00,600 --> 00:30:07,270
but it's not such that just a simple is

542
00:30:04,870 --> 00:30:12,610
that it should be done with a bit more

543
00:30:07,270 --> 00:30:14,139
cautious courses so these are some of

544
00:30:12,610 --> 00:30:17,320
our team members this is not

545
00:30:14,140 --> 00:30:19,450
photoshopped you can find us here in the

546
00:30:17,320 --> 00:30:22,120
conference and ask some questions

547
00:30:19,450 --> 00:30:25,210
and of course if we have some time I

548
00:30:22,120 --> 00:30:29,340
think we have for few questions now we

549
00:30:25,210 --> 00:30:38,140
will be happy to answer thank you

550
00:30:29,340 --> 00:30:38,139
[Applause]

551
00:30:38,370 --> 00:30:41,370
yeah

552
00:31:26,700 --> 00:31:33,789
the next topic is maleo teaching a

553
00:31:29,799 --> 00:31:41,679
machine to detect malware the presenter

554
00:31:33,789 --> 00:31:43,390
is Tony Greenwich about Molly Oh Molly

555
00:31:41,679 --> 00:31:45,330
oh is machine learning framework that

556
00:31:43,390 --> 00:31:47,380
successfully detects new malware samples

557
00:31:45,330 --> 00:31:49,510
despite the fact that malware is

558
00:31:47,380 --> 00:31:51,100
constantly changing and non-stationary

559
00:31:49,510 --> 00:31:52,960
malware distribution presents a

560
00:31:51,100 --> 00:31:55,209
challenge for statistical methods due to

561
00:31:52,960 --> 00:31:57,130
the phenomenal concept drift Molly o

562
00:31:55,210 --> 00:31:59,380
helps to overcome these problems and

563
00:31:57,130 --> 00:32:01,990
help and can help to better detect new

564
00:31:59,380 --> 00:32:04,690
malware samples at large scale about

565
00:32:01,990 --> 00:32:07,000
Tony Tony is helping organizations to

566
00:32:04,690 --> 00:32:08,799
become more secure working as

567
00:32:07,000 --> 00:32:11,769
information security consultant

568
00:32:08,799 --> 00:32:14,950
consultant in divert Oh besides that he

569
00:32:11,769 --> 00:32:17,760
manages to write his he managed to write

570
00:32:14,950 --> 00:32:21,389
his doctoral thesis also Tony bill

571
00:32:17,760 --> 00:32:26,049
performing music at the after-party

572
00:32:21,389 --> 00:32:27,908
so the talk will be short I will not

573
00:32:26,049 --> 00:32:30,809
take any questions because we have to

574
00:32:27,909 --> 00:32:33,730
prepare the after-party so I'm Tony and

575
00:32:30,809 --> 00:32:36,519
I will be talking about Molly Oh Molly's

576
00:32:33,730 --> 00:32:39,399
machine learning framework - the tech

577
00:32:36,519 --> 00:32:43,570
smaller it basically detects always

578
00:32:39,399 --> 00:32:49,209
portable executable mother this is my

579
00:32:43,570 --> 00:32:52,658
agenda this morning morning I didn't

580
00:32:49,210 --> 00:32:55,860
have any slide I built all flights when

581
00:32:52,659 --> 00:32:59,470
I was listen to the politicians so

582
00:32:55,860 --> 00:33:05,559
detect the talk is not prepared so stand

583
00:32:59,470 --> 00:33:08,950
up and enjoy back to 2014 I was trying

584
00:33:05,559 --> 00:33:12,908
to write my PhD thesis and fight and

585
00:33:08,950 --> 00:33:17,950
find the topic basically I was trying to

586
00:33:12,909 --> 00:33:21,370
detect a normal is in time serious I was

587
00:33:17,950 --> 00:33:23,639
analyzing Glocks and I have a big

588
00:33:21,370 --> 00:33:23,639
problem

589
00:33:24,450 --> 00:33:34,270
RDD know I was worth working at carnot

590
00:33:28,630 --> 00:33:36,610
and basically I had a lot of data it was

591
00:33:34,270 --> 00:33:38,960
cool and I was doing math which I love

592
00:33:36,610 --> 00:33:41,899
but have one big problem

593
00:33:38,960 --> 00:33:44,870
the biggest curse of big data is when

594
00:33:41,900 --> 00:33:47,990
you don't have enough data the carnet

595
00:33:44,870 --> 00:33:51,379
NOC the Network Operations Center wasn't

596
00:33:47,990 --> 00:33:54,290
summer holidays I didn't have any data

597
00:33:51,380 --> 00:33:58,220
the notation of the data set was painful

598
00:33:54,290 --> 00:34:01,190
I was waiting for the takers to attack

599
00:33:58,220 --> 00:34:03,920
our network to annotate this say the day

600
00:34:01,190 --> 00:34:07,490
the data set and to run my models so I

601
00:34:03,920 --> 00:34:09,560
have so much problems so I was talking

602
00:34:07,490 --> 00:34:12,800
to Tony mere twenty meter is my mentor

603
00:34:09,560 --> 00:34:16,009
on the PhD thesis and he said to me I

604
00:34:12,800 --> 00:34:20,500
don't know it's your problem be creative

605
00:34:16,010 --> 00:34:25,870
so I changed the idea at that time

606
00:34:20,500 --> 00:34:29,420
GAMEOVER Zeus button was disrupted so I

607
00:34:25,870 --> 00:34:33,139
said okay that's a bit big thing let's

608
00:34:29,420 --> 00:34:35,150
see what we can do here I was listening

609
00:34:33,139 --> 00:34:38,770
to the end game talk about binary Pig

610
00:34:35,150 --> 00:34:44,600
binary pig is a Hadoop framework to

611
00:34:38,770 --> 00:34:48,710
extract data from from Portugal portable

612
00:34:44,600 --> 00:34:50,989
executable files and I said whom that's

613
00:34:48,710 --> 00:34:54,770
not so interesting and it's better than

614
00:34:50,989 --> 00:34:56,870
the normally detection thing because it

615
00:34:54,770 --> 00:34:59,000
doesn't have so much Matt it's easier

616
00:34:56,870 --> 00:35:06,790
and it's more sellable

617
00:34:59,000 --> 00:35:11,330
so what problem I'm trying to solve here

618
00:35:06,790 --> 00:35:13,700
so the problem with antivirus is a

619
00:35:11,330 --> 00:35:15,850
number of signatures the antivirus

620
00:35:13,700 --> 00:35:20,270
companies cannot stand the competition

621
00:35:15,850 --> 00:35:23,210
with with the marvel mother routers

622
00:35:20,270 --> 00:35:28,810
because they're so creative it's

623
00:35:23,210 --> 00:35:31,940
basically a catch-up game so every year

624
00:35:28,810 --> 00:35:35,720
the number of signatures goes up its

625
00:35:31,940 --> 00:35:39,980
cope goes up exponentially this is

626
00:35:35,720 --> 00:35:43,490
basically a picture for the Romanian

627
00:35:39,980 --> 00:35:47,090
cert about the evolution of ransomware

628
00:35:43,490 --> 00:35:51,319
around somewhere so you can see it's so

629
00:35:47,090 --> 00:35:52,490
much samples new things and stuff but

630
00:35:51,320 --> 00:35:55,280
Chris Mallya

631
00:35:52,490 --> 00:35:58,970
whose mother is basically valley's my

632
00:35:55,280 --> 00:36:03,710
grandpa what is malya malya is a

633
00:35:58,970 --> 00:36:07,279
framework I said that earlier uses a

634
00:36:03,710 --> 00:36:09,980
bunch of classifiers to detect mothers

635
00:36:07,280 --> 00:36:13,609
so it uses the static analysis

636
00:36:09,980 --> 00:36:16,580
classifier thanks Terra Herrera and his

637
00:36:13,609 --> 00:36:21,319
amazing work and the dynamic analysis

638
00:36:16,580 --> 00:36:26,390
which is based on cooks and books so I

639
00:36:21,320 --> 00:36:31,510
take different classifiers and I fashion

640
00:36:26,390 --> 00:36:35,210
I function it together so basically

641
00:36:31,510 --> 00:36:38,390
every type of analysis has some type of

642
00:36:35,210 --> 00:36:42,650
problem static analysis is bad is bad

643
00:36:38,390 --> 00:36:46,730
for text files and dynamic analysis is

644
00:36:42,650 --> 00:36:49,880
bad for anted debug and TVM methods so

645
00:36:46,730 --> 00:36:54,220
when I find it but when I try to combine

646
00:36:49,880 --> 00:36:56,900
them together I get a better result

647
00:36:54,220 --> 00:37:00,140
basically the data problem was solved

648
00:36:56,900 --> 00:37:00,560
pretty easy thanks to the guy from from

649
00:37:00,140 --> 00:37:06,259
Twitter

650
00:37:00,560 --> 00:37:09,740
mouthal is great it has many samples and

651
00:37:06,260 --> 00:37:13,609
Mira share is also pretty great if it

652
00:37:09,740 --> 00:37:18,189
has I don't know 100 terabytes of

653
00:37:13,609 --> 00:37:23,810
smaller samples basically this is my

654
00:37:18,190 --> 00:37:24,830
workflow of my framework basically it

655
00:37:23,810 --> 00:37:28,640
used cuckoo

656
00:37:24,830 --> 00:37:32,270
it launched the model file in the

657
00:37:28,640 --> 00:37:36,618
virtual machines it's order analysis

658
00:37:32,270 --> 00:37:39,680
result in MongoDB the best script it's a

659
00:37:36,619 --> 00:37:42,770
simple batch script which runs all the

660
00:37:39,680 --> 00:37:43,810
analysis and Franco which is written in

661
00:37:42,770 --> 00:37:47,450
Python

662
00:37:43,810 --> 00:37:52,490
does all the crunching basically this is

663
00:37:47,450 --> 00:37:55,129
the storage one function and this is the

664
00:37:52,490 --> 00:37:58,339
largest one it creates the matrixes with

665
00:37:55,130 --> 00:38:01,310
the features then we learn the model and

666
00:37:58,339 --> 00:38:04,350
when we have the models we can't

667
00:38:01,310 --> 00:38:07,080
classify the mother it's simple

668
00:38:04,350 --> 00:38:10,080
so I'm basically using all the

669
00:38:07,080 --> 00:38:12,870
open-source I'm standing on shoulder

670
00:38:10,080 --> 00:38:15,960
shoulders of giants basically the

671
00:38:12,870 --> 00:38:19,799
classic scientific Python stack I'm

672
00:38:15,960 --> 00:38:21,590
using numpy ipad ipython pandas

673
00:38:19,800 --> 00:38:24,480
scikit-learn

674
00:38:21,590 --> 00:38:27,990
i'm running then and now assisting in

675
00:38:24,480 --> 00:38:31,380
KVM i men shouldn't cook away earlier

676
00:38:27,990 --> 00:38:35,279
and basically I use Mongo and was great

677
00:38:31,380 --> 00:38:40,650
for storage at the moment I have three

678
00:38:35,280 --> 00:38:43,910
classifiers the first one use all static

679
00:38:40,650 --> 00:38:48,350
features the second one is dynamic which

680
00:38:43,910 --> 00:38:51,629
use some system calls properties like

681
00:38:48,350 --> 00:38:56,130
arguments properties and the second one

682
00:38:51,630 --> 00:38:58,800
is basically a system call broth which

683
00:38:56,130 --> 00:39:03,900
is basically a matrix of all system

684
00:38:58,800 --> 00:39:05,820
calls basically the idea is based on one

685
00:39:03,900 --> 00:39:09,660
idea what one great scientist it's

686
00:39:05,820 --> 00:39:14,850
called walpert and about anybody heard

687
00:39:09,660 --> 00:39:18,120
from about Walbert he wrote about the no

688
00:39:14,850 --> 00:39:23,060
free lunch theorem so he also wrote

689
00:39:18,120 --> 00:39:25,680
about stack generalization and basically

690
00:39:23,060 --> 00:39:27,750
marco talked about convolutional

691
00:39:25,680 --> 00:39:31,799
networks deep learning this is an

692
00:39:27,750 --> 00:39:34,110
old-school technique from from 1992 it's

693
00:39:31,800 --> 00:39:37,830
basically was multiple classifiers it

694
00:39:34,110 --> 00:39:40,200
takes their predictions uses a meta

695
00:39:37,830 --> 00:39:43,560
classifier and give us the final

696
00:39:40,200 --> 00:39:45,720
prediction so the training Delta the

697
00:39:43,560 --> 00:39:51,360
training data was derived from a big

698
00:39:45,720 --> 00:39:52,350
data set of 50 hundred fifty thousand

699
00:39:51,360 --> 00:39:56,490
samples

700
00:39:52,350 --> 00:39:59,339
it was stratified by EUR and by Bob

701
00:39:56,490 --> 00:40:02,790
mother type the model type was derived

702
00:39:59,340 --> 00:40:07,110
by the antivirus Kaspersky thanks to

703
00:40:02,790 --> 00:40:11,279
Kaspersky it helped me a lot so let's

704
00:40:07,110 --> 00:40:15,320
benchmark molly-o to others this is my

705
00:40:11,280 --> 00:40:17,850
PhD topic and I can show all the results

706
00:40:15,320 --> 00:40:19,620
basically one guy

707
00:40:17,850 --> 00:40:22,770
who's working at the Los Alamos

708
00:40:19,620 --> 00:40:26,580
laboratory now is in Cisco was the best

709
00:40:22,770 --> 00:40:30,870
is it was my aisle and I had so much

710
00:40:26,580 --> 00:40:34,950
problem with that paper and heat wasting

711
00:40:30,870 --> 00:40:38,759
a lot of time so finally he was the best

712
00:40:34,950 --> 00:40:41,220
one but I managed with some

713
00:40:38,760 --> 00:40:43,920
transformation of data it's not about

714
00:40:41,220 --> 00:40:45,660
the result machine learning in the

715
00:40:43,920 --> 00:40:50,790
scientific community it's all about the

716
00:40:45,660 --> 00:40:52,799
results if you want to have a good

717
00:40:50,790 --> 00:40:56,670
classifier you have to have 99%

718
00:40:52,800 --> 00:40:59,700
basically I think it's better to to do a

719
00:40:56,670 --> 00:41:02,930
good idea that can classify malvern and

720
00:40:59,700 --> 00:41:06,950
its creative than to have a 99% result

721
00:41:02,930 --> 00:41:12,390
because you have to have a resilient

722
00:41:06,950 --> 00:41:16,859
classifier I do at the end so some of

723
00:41:12,390 --> 00:41:20,279
the problems of this domain are the

724
00:41:16,860 --> 00:41:22,770
following vm evasion in dynamic analysis

725
00:41:20,280 --> 00:41:28,470
this was a big problem I patched the

726
00:41:22,770 --> 00:41:30,150
cuckoo monitor function anyone look at

727
00:41:28,470 --> 00:41:32,160
the source of the cuckoo monitor

728
00:41:30,150 --> 00:41:34,320
function guys for reversible ops I

729
00:41:32,160 --> 00:41:38,700
pitched it a lot and I have so many

730
00:41:34,320 --> 00:41:41,640
problems with that I basically the

731
00:41:38,700 --> 00:41:44,370
cuckoo monitor it's not that that bad

732
00:41:41,640 --> 00:41:49,069
but Mulder routers are so creative in

733
00:41:44,370 --> 00:41:52,740
escaping sandboxing environment so they

734
00:41:49,070 --> 00:41:57,810
they are finding everyday new ways to

735
00:41:52,740 --> 00:42:01,410
escape sandboxing also this model has to

736
00:41:57,810 --> 00:42:01,770
have a human in the loop guys from

737
00:42:01,410 --> 00:42:06,839
Google

738
00:42:01,770 --> 00:42:11,820
Scully wrote about scallion codes wrote

739
00:42:06,840 --> 00:42:14,820
about detection of spam adverts missed

740
00:42:11,820 --> 00:42:17,550
advertisements and they said they

741
00:42:14,820 --> 00:42:18,000
without a human without an analyst in

742
00:42:17,550 --> 00:42:22,020
the loop

743
00:42:18,000 --> 00:42:26,700
you cannot detect basically that which

744
00:42:22,020 --> 00:42:29,259
are malicious also the temporal

745
00:42:26,700 --> 00:42:35,680
component I had so many problems

746
00:42:29,260 --> 00:42:39,510
while while finding some neurons murmurs

747
00:42:35,680 --> 00:42:41,740
putting it in the data set and with the

748
00:42:39,510 --> 00:42:45,160
results were so bad

749
00:42:41,740 --> 00:42:51,759
basically new Malvar drifts your

750
00:42:45,160 --> 00:42:56,009
distribution of of data and excuse your

751
00:42:51,760 --> 00:43:00,270
results so this is a big problem also

752
00:42:56,010 --> 00:43:04,120
one idea what I plan to do next is to

753
00:43:00,270 --> 00:43:06,100
build a phylogeny at extreme hours

754
00:43:04,120 --> 00:43:09,009
basically we have one branch like you

755
00:43:06,100 --> 00:43:14,470
saw on the cert Romanian cert picture

756
00:43:09,010 --> 00:43:17,230
you have one one tree with all my other

757
00:43:14,470 --> 00:43:24,009
samples and ancestors and stuff like

758
00:43:17,230 --> 00:43:26,740
that so future of maleo I plan to

759
00:43:24,010 --> 00:43:29,140
support more formats for it

760
00:43:26,740 --> 00:43:31,290
use this assembly and build another

761
00:43:29,140 --> 00:43:36,400
classifier which uses this assembly

762
00:43:31,290 --> 00:43:38,650
provisioning to the cloud and change

763
00:43:36,400 --> 00:43:41,860
posture with some other technologies

764
00:43:38,650 --> 00:43:45,010
like click house it's a Yandex database

765
00:43:41,860 --> 00:43:47,770
column variant and I think spark dusk

766
00:43:45,010 --> 00:43:52,440
basically I'm benchmarking this together

767
00:43:47,770 --> 00:43:56,170
to see which shoots more for my music

768
00:43:52,440 --> 00:43:58,270
this is my talk so first questions if

769
00:43:56,170 --> 00:44:05,670
somebody wants to ask me on or you can

770
00:43:58,270 --> 00:44:05,670
ask me any questions on my email anyone

771
00:44:11,100 --> 00:44:13,368
yeah

772
00:44:12,340 --> 00:44:16,219
you can basically

773
00:44:13,369 --> 00:44:18,680
the fur this is why I call it framework

774
00:44:16,219 --> 00:44:20,329
you can extend the framework with any

775
00:44:18,680 --> 00:44:24,160
customer you want

776
00:44:20,329 --> 00:44:27,739
you've been the symbolic execution

777
00:44:24,160 --> 00:44:30,049
classifier and then you incorporate in

778
00:44:27,739 --> 00:44:33,739
the framework the only problem is you

779
00:44:30,049 --> 00:44:35,569
cannot have many classifiers when you go

780
00:44:33,739 --> 00:44:38,299
to five classes there's a problem

781
00:44:35,569 --> 00:44:40,759
because you have two problem to manage

782
00:44:38,299 --> 00:44:43,339
all that that's the only problem but

783
00:44:40,759 --> 00:44:47,210
it's one idea not the symbolic execution

784
00:44:43,339 --> 00:44:52,660
one but dynamic instrumentation with pin

785
00:44:47,210 --> 00:44:52,660
and stuff like that other questions okay

786
00:44:54,069 --> 00:45:00,279
thank you

787
00:44:56,000 --> 00:45:00,280
[Applause]

