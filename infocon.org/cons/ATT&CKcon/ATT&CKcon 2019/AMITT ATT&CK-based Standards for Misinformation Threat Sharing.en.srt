1
00:00:01,200 --> 00:00:03,020
- It's my pleasure to
introduce our next speakers,

2
00:00:03,020 --> 00:00:05,830
Sara-Jayne Terp and John Gray,

3
00:00:05,830 --> 00:00:06,930
and the program committee

4
00:00:06,930 --> 00:00:09,100
was really excited to get this submission

5
00:00:09,100 --> 00:00:11,519
because we so often get questions

6
00:00:11,519 --> 00:00:14,340
from folks about, hey, can you create

7
00:00:14,340 --> 00:00:16,509
ATTACK for toaster ovens, for example.

8
00:00:16,510 --> 00:00:19,780
And as you kind of have
seen in our day one intros

9
00:00:19,780 --> 00:00:21,310
and you've heard during our updates,

10
00:00:21,310 --> 00:00:23,299
we have a lot on our plates
and we can't do everything

11
00:00:23,300 --> 00:00:26,710
and we've chosen to scope
ATTACK a little more narrowly

12
00:00:26,710 --> 00:00:29,570
but that doesn't mean you
can't take our methodology.

13
00:00:29,570 --> 00:00:30,650
Take our philosophy.

14
00:00:30,650 --> 00:00:33,300
Blake led us in publishing our
philosophy paper last year,

15
00:00:33,300 --> 00:00:36,940
explaining kind of the why and
what and how behind ATTACK.

16
00:00:36,940 --> 00:00:40,540
So awesome, take that and
apply it to new domains

17
00:00:40,540 --> 00:00:42,100
and our next speakers have done that

18
00:00:42,100 --> 00:00:44,580
in the domain of misinformation.

19
00:00:44,580 --> 00:00:46,989
This is, obviously, a huge
topic on a lot of our minds

20
00:00:46,990 --> 00:00:49,550
with the election coming up
here in the United States.

21
00:00:49,550 --> 00:00:51,620
And so, we were so, so pleased

22
00:00:51,620 --> 00:00:54,424
to have this talk kicking off our day two,

23
00:00:54,424 --> 00:00:56,320
where Sara and John are gonna be talking

24
00:00:56,320 --> 00:00:58,320
about how they took ATTACK methodology

25
00:00:58,320 --> 00:01:00,440
and applied it to misinformation.

26
00:01:00,440 --> 00:01:01,650
Please join me in welcoming

27
00:01:01,650 --> 00:01:04,173
Sara-Jayne Terp and John Gray.

28
00:01:04,173 --> 00:01:07,340
(audience applauding)

29
00:01:09,910 --> 00:01:12,551
- Ooh, clickers.
- Yes, people get them.

30
00:01:12,552 --> 00:01:13,385
- Hey.

31
00:01:13,385 --> 00:01:14,940
- Hey, so I'm SJ and that's John,

32
00:01:14,940 --> 00:01:17,450
just in case you can't
tell the difference,

33
00:01:17,450 --> 00:01:20,180
and we took your baby
and did things to it.

34
00:01:20,180 --> 00:01:24,123
So there are now three layers of infosec.

35
00:01:25,110 --> 00:01:28,110
There's cyber security,

36
00:01:28,110 --> 00:01:28,943
which you're kinda used to.

37
00:01:28,943 --> 00:01:29,776
There's physical security,

38
00:01:29,776 --> 00:01:32,350
and there's now cognitive security.

39
00:01:32,350 --> 00:01:34,050
This is that.

40
00:01:34,050 --> 00:01:34,960
John, your stage.

41
00:01:34,960 --> 00:01:36,149
- Just a little background,

42
00:01:36,150 --> 00:01:37,420
'cause I'm sure all of you know

43
00:01:37,420 --> 00:01:39,133
what the Credibility Coalition is.

44
00:01:40,040 --> 00:01:42,210
So we're a research community,

45
00:01:42,210 --> 00:01:46,259
startup in 2017 out of the
first MisinfoCon events,

46
00:01:46,260 --> 00:01:49,550
so it's a collection of
journalists, researchers,

47
00:01:49,550 --> 00:01:51,740
and ultimately people
that have gotten together

48
00:01:51,740 --> 00:01:54,039
that care about issues around veracity,

49
00:01:54,040 --> 00:01:56,350
quality and the credibility
of online information.

50
00:01:56,350 --> 00:01:57,949
And somehow we also believe

51
00:01:57,950 --> 00:01:59,780
it's pretty important to have

52
00:01:59,780 --> 00:02:02,453
a element of civil society.

53
00:02:04,250 --> 00:02:07,470
So ultimately, it's really
about creating standards

54
00:02:07,470 --> 00:02:10,449
and again, all about credibility.

55
00:02:10,449 --> 00:02:13,679
And we're fortunate that
the Credibility Coalition

56
00:02:13,680 --> 00:02:16,660
has incubated this project.

57
00:02:16,660 --> 00:02:18,697
- [SJ] Well, we helped start
the Credibility Coalition,

58
00:02:18,697 --> 00:02:19,530
so kind of

59
00:02:19,530 --> 00:02:20,363
they let us in (laughs)
- Yes, yeah, yeah.

60
00:02:20,363 --> 00:02:22,790
Well, SJ knows how to knock on doors

61
00:02:22,790 --> 00:02:25,940
and barge, move her way through them

62
00:02:25,940 --> 00:02:28,480
so there's six different working groups

63
00:02:28,480 --> 00:02:29,470
of which we're one of them

64
00:02:29,470 --> 00:02:32,160
that are currently supported
by the Credibility Coalition.

65
00:02:32,160 --> 00:02:33,720
As well, part of our project,

66
00:02:33,720 --> 00:02:35,160
I think we've gotta give a little shoutout

67
00:02:35,160 --> 00:02:37,350
to the Newmark Foundation for some funding

68
00:02:37,350 --> 00:02:40,570
that helped us move this thing along.

69
00:02:40,570 --> 00:02:44,140
And in terms of people involved,

70
00:02:44,140 --> 00:02:46,329
we have to acknowledge some of the folks

71
00:02:46,330 --> 00:02:47,560
that have supported us,

72
00:02:47,560 --> 00:02:49,700
it's not just SJ and myself,

73
00:02:49,700 --> 00:02:51,470
we've got a pretty unique blend of people

74
00:02:51,470 --> 00:02:53,910
from academic, government, others,

75
00:02:53,910 --> 00:02:55,070
and a few of us,

76
00:02:55,070 --> 00:02:57,549
from the other hats we wear,

77
00:02:57,550 --> 00:02:59,140
the companies we also run,

78
00:02:59,140 --> 00:03:00,933
and think about these issues.

79
00:03:01,938 --> 00:03:03,320
So, overall,

80
00:03:03,320 --> 00:03:07,209
our big challenge in terms
of how we've approached this

81
00:03:07,210 --> 00:03:11,130
is thinking through it,
creating a framework

82
00:03:11,130 --> 00:03:13,680
and working to understand this notion

83
00:03:13,680 --> 00:03:16,493
of how communities are organizing attacks,

84
00:03:16,493 --> 00:03:19,329
information-based attacks,
be them disinformation,

85
00:03:19,330 --> 00:03:21,653
misinformation and network propaganda.

86
00:03:23,030 --> 00:03:25,610
So a big part of this
was exercising some rigor

87
00:03:25,610 --> 00:03:28,123
and trying to classify a
number of these issues.

88
00:03:29,430 --> 00:03:30,431
- We probably need to explain

89
00:03:30,431 --> 00:03:32,983
what misinformation, disinformation are.

90
00:03:32,983 --> 00:03:34,110
- How about it?
- Oh, okay, it's on me?

91
00:03:34,110 --> 00:03:36,070
- Yes it is, as we wing it.

92
00:03:36,070 --> 00:03:37,530
- So this is not just fake news,

93
00:03:37,530 --> 00:03:39,090
so when I started

94
00:03:40,500 --> 00:03:43,120
way back, way back, I mean
we started way way back.

95
00:03:43,120 --> 00:03:43,953
- [John] The olden days

96
00:03:43,953 --> 00:03:48,260
- By 2016, 2017, people were
talking about fake news.

97
00:03:48,260 --> 00:03:50,079
They thought it was just content,

98
00:03:50,080 --> 00:03:52,350
that people were typing
things that were false,

99
00:03:52,350 --> 00:03:53,690
putting those out.

100
00:03:53,690 --> 00:03:54,722
It's not that.

101
00:03:56,120 --> 00:03:58,700
The falsity can be in the content,

102
00:03:58,700 --> 00:04:00,299
it can be in the context,

103
00:04:00,300 --> 00:04:01,720
so disinformation attack,

104
00:04:01,720 --> 00:04:04,240
what we've been looking at specifically

105
00:04:04,240 --> 00:04:08,810
is large scale coordinated
inauthentic behaviors,

106
00:04:08,810 --> 00:04:10,130
so we're looking at

107
00:04:10,130 --> 00:04:11,480
where the content might be fake,

108
00:04:11,480 --> 00:04:12,649
it might be in the text,

109
00:04:12,650 --> 00:04:16,850
it might be in imaging,
it might be in video,

110
00:04:16,850 --> 00:04:19,697
so you've seen deep fakes,
that comes under the umbrella.

111
00:04:19,697 --> 00:04:22,100
- Cheap fakes.
- Cheap fakes, yep.

112
00:04:22,100 --> 00:04:22,933
- Pelosi video

113
00:04:24,330 --> 00:04:25,520
Slow it down.

114
00:04:25,520 --> 00:04:29,340
- Okay, it may be in attributions,

115
00:04:29,340 --> 00:04:31,890
it may be in fake groups,

116
00:04:31,890 --> 00:04:35,530
it may be in fake users, so
bot nets come under that.

117
00:04:35,530 --> 00:04:37,573
But also trolls, cyborgs,

118
00:04:38,720 --> 00:04:41,930
the way that these types of
accounts are being set up

119
00:04:41,930 --> 00:04:45,450
is changing from being
really, really, really,

120
00:04:45,450 --> 00:04:47,969
like glaring, easy to find bots

121
00:04:47,970 --> 00:04:50,140
and bot nets just on all the time

122
00:04:50,140 --> 00:04:52,719
and connected to a hashtag you can find,

123
00:04:52,720 --> 00:04:56,650
they're now starting to
go nice, low, slow attacks

124
00:04:56,650 --> 00:05:01,570
they're ditching the reach they could get

125
00:05:01,570 --> 00:05:04,853
across communities for
being harder to find.

126
00:05:06,081 --> 00:05:10,650
- Just like the problems that
were talked about yesterday,

127
00:05:10,650 --> 00:05:13,299
techniques, tactics, they keep changing.

128
00:05:13,300 --> 00:05:14,940
Our advisories are pretty creative

129
00:05:14,940 --> 00:05:17,332
at exploiting some of the vulnerabilities.

130
00:05:18,750 --> 00:05:20,940
So again, we've really
tried to look at this

131
00:05:20,940 --> 00:05:22,730
through a lens of asking questions

132
00:05:22,730 --> 00:05:24,673
around social and cognitive factors,

133
00:05:26,490 --> 00:05:29,280
thinking through some key actions tactics,

134
00:05:29,280 --> 00:05:31,119
obviously some strategies involved.

135
00:05:31,120 --> 00:05:32,950
I'm gonna a little bit sprint

136
00:05:32,950 --> 00:05:34,110
through the who we are,

137
00:05:34,110 --> 00:05:35,130
how we thought through this

138
00:05:35,130 --> 00:05:36,320
and how we got here,

139
00:05:36,320 --> 00:05:38,770
mainly because SJ and some other folks

140
00:05:38,770 --> 00:05:39,719
have done a lot of work

141
00:05:39,720 --> 00:05:42,060
that I think some people
wanna really nerd out on

142
00:05:42,060 --> 00:05:43,463
to see how we built it.
- We've got some time.

143
00:05:43,463 --> 00:05:44,296
We got some time.

144
00:05:44,296 --> 00:05:45,570
we're good.
- True, we do.

145
00:05:45,570 --> 00:05:47,022
I'm looking at the timer.

146
00:05:48,150 --> 00:05:49,950
So we do have a few objectives

147
00:05:49,950 --> 00:05:52,530
and one of the things
that we're working for

148
00:05:52,530 --> 00:05:56,739
is obviously how we're
describing the assets,

149
00:05:56,740 --> 00:05:58,710
describing the techniques and tactics

150
00:05:58,710 --> 00:06:01,109
that we've worked through on the framework

151
00:06:01,110 --> 00:06:05,110
and moving forward where we're at,

152
00:06:05,110 --> 00:06:06,800
is now we've essentially

153
00:06:06,800 --> 00:06:08,720
gone through this red team exercise

154
00:06:08,720 --> 00:06:10,440
and now we're working towards

155
00:06:11,770 --> 00:06:13,270
now the blue team side of things,

156
00:06:13,270 --> 00:06:14,740
as looking through the techniques

157
00:06:14,740 --> 00:06:17,210
that we've identified and figuring,

158
00:06:17,210 --> 00:06:18,969
what are gonna be some counters for that,

159
00:06:18,970 --> 00:06:22,253
so that's on the agenda.

160
00:06:23,240 --> 00:06:27,150
Really quickly, as SJ noted,

161
00:06:27,150 --> 00:06:28,560
she herself personally and some others

162
00:06:28,560 --> 00:06:30,920
have been in this space for some time now,

163
00:06:30,920 --> 00:06:32,270
I feel like I'm a bit of a latecomer,

164
00:06:32,270 --> 00:06:35,620
but sort of the timeline
of our working group--

165
00:06:35,620 --> 00:06:36,550
- [SJ] You've been in the space,

166
00:06:36,550 --> 00:06:37,383
we just didn't bring you

167
00:06:37,383 --> 00:06:38,830
to the infosec dark side until later.

168
00:06:38,830 --> 00:06:42,203
- I'm late being pulled
into the dark side, yes.

169
00:06:43,090 --> 00:06:44,919
So this is just a timeline
of the work we've done

170
00:06:44,920 --> 00:06:47,100
and really what it boiled down to for us

171
00:06:47,100 --> 00:06:49,250
is that we sort of have this mantras

172
00:06:49,250 --> 00:06:53,760
that there's been a lot of
admiration about the problem

173
00:06:53,760 --> 00:06:55,483
but not enough action being taken.

174
00:06:56,440 --> 00:06:59,120
We don't think that, in essence,

175
00:06:59,120 --> 00:07:02,140
the erosion, the attack on democracy,

176
00:07:02,140 --> 00:07:03,640
which if we think about this problem

177
00:07:03,640 --> 00:07:06,820
through the lens of state
actors and their proxies,

178
00:07:06,820 --> 00:07:09,500
we don't think democracy's
basically got 27 years

179
00:07:09,500 --> 00:07:12,320
to wait for a framework,
wait for a proper process,

180
00:07:12,320 --> 00:07:15,360
and this is what we've endeavored to do,

181
00:07:15,360 --> 00:07:19,360
so born from sort of our start-up thing,

182
00:07:19,360 --> 00:07:20,780
we've sat down,

183
00:07:20,780 --> 00:07:22,700
a group of us have really worked hard

184
00:07:22,700 --> 00:07:25,099
to try to get some stuff done.

185
00:07:25,100 --> 00:07:26,870
We've delivered numerous papers,

186
00:07:26,870 --> 00:07:29,340
we've done numerous talks,

187
00:07:29,340 --> 00:07:31,840
SJ will share stories
at least from the couch

188
00:07:31,840 --> 00:07:35,349
of her zip code's been her license plate

189
00:07:35,350 --> 00:07:37,010
and she's been amazing,

190
00:07:37,010 --> 00:07:37,880
driving around the country,

191
00:07:37,880 --> 00:07:39,560
giving a lot of talks.

192
00:07:39,560 --> 00:07:42,030
So this is the timeline,

193
00:07:42,030 --> 00:07:44,080
and coming in a couple of weeks,

194
00:07:44,080 --> 00:07:45,229
we're doing our workshop

195
00:07:45,230 --> 00:07:46,910
and then we'll continue to refine things

196
00:07:46,910 --> 00:07:47,850
as we move through.

197
00:07:47,850 --> 00:07:50,040
And in all fairness,

198
00:07:50,040 --> 00:07:51,930
a lot of this has been done

199
00:07:51,930 --> 00:07:54,110
in essence as a volunteer project.

200
00:07:54,110 --> 00:07:55,090
A lot of our teammates,

201
00:07:55,090 --> 00:07:56,780
a lot of people that have been behind it,

202
00:07:56,780 --> 00:08:00,429
have been doing this while having jobs

203
00:08:01,280 --> 00:08:04,909
and doing it out of their own pockets,

204
00:08:04,910 --> 00:08:06,970
and they're doing it 'cause they care.

205
00:08:06,970 --> 00:08:08,020
We're doing 'cause we care.

206
00:08:08,020 --> 00:08:09,969
So in the first six months,

207
00:08:09,970 --> 00:08:12,300
ultimately what we did
is we open sourced out

208
00:08:12,300 --> 00:08:14,750
when we documented what we identified

209
00:08:14,750 --> 00:08:17,703
as 63 different incidents or campaigns.

210
00:08:19,180 --> 00:08:22,563
SJ'll talk about
developing the STIX format.

211
00:08:24,200 --> 00:08:25,990
So really the first six months

212
00:08:25,990 --> 00:08:27,470
has been creating the foundation,

213
00:08:27,470 --> 00:08:29,130
we felt really good in August,

214
00:08:29,130 --> 00:08:31,580
we delivered the first
version of AMITT to Github

215
00:08:33,330 --> 00:08:35,567
and then, again, here's what
we've got moving forward

216
00:08:35,567 --> 00:08:37,860
and it's gonna be an evolution.

217
00:08:37,860 --> 00:08:40,750
I mean, I think about
what I heard yesterday,

218
00:08:40,750 --> 00:08:44,100
a lot of you have been in
this space for a long time,

219
00:08:44,100 --> 00:08:46,030
you've got a real head
start on those of us

220
00:08:46,030 --> 00:08:48,120
that look at what we call misinfosec,

221
00:08:48,120 --> 00:08:50,430
so the intersection of misinformation

222
00:08:50,430 --> 00:08:52,270
and information security,

223
00:08:52,270 --> 00:08:55,480
so let's just say that
we've gotta move quickly

224
00:08:55,480 --> 00:08:56,820
and we've gotta keep evolving

225
00:08:56,820 --> 00:09:00,623
and we also see this as a
definitely a function community.

226
00:09:02,560 --> 00:09:05,792
So just to move through the next slide,

227
00:09:07,500 --> 00:09:08,440
a big part of this,

228
00:09:08,440 --> 00:09:10,040
and I was impressed
hearing this yesterday,

229
00:09:10,040 --> 00:09:11,560
'cause obviously it's this issue

230
00:09:11,560 --> 00:09:14,619
of having common language
across communities,

231
00:09:14,620 --> 00:09:17,763
thinking through better defending,

232
00:09:18,850 --> 00:09:19,970
creating better tools,

233
00:09:19,970 --> 00:09:23,710
and ultimately looking
at this through the lens

234
00:09:23,710 --> 00:09:27,613
of a problem that's happening at scale.

235
00:09:28,610 --> 00:09:31,540
And as SJ noted,

236
00:09:31,540 --> 00:09:33,620
the whole machine learning and AI issues

237
00:09:33,620 --> 00:09:37,313
are gonna just continue
to drive this problem.

238
00:09:38,300 --> 00:09:39,132
I hate to say it,

239
00:09:39,133 --> 00:09:41,523
I'm rather bullish on
our career, sadly to say.

240
00:09:44,950 --> 00:09:46,570
We've taken a really

241
00:09:46,570 --> 00:09:48,430
multi-disciplinary approach of things,

242
00:09:48,430 --> 00:09:49,300
so we've had to look at it

243
00:09:49,300 --> 00:09:51,370
through a number of
different points of view

244
00:09:51,370 --> 00:09:53,540
in terms of the misinformation community,

245
00:09:53,540 --> 00:09:56,670
so information security,
information operations,

246
00:09:56,670 --> 00:09:58,750
I'm sure some of you might
recognize a name or two

247
00:09:58,750 --> 00:10:01,790
we've tagged with the
information security folks,

248
00:10:01,790 --> 00:10:05,182
looking at it through the lens of conflict

249
00:10:05,182 --> 00:10:07,350
and then again, you know, I think arguably

250
00:10:07,350 --> 00:10:09,280
this has been happening since well before,

251
00:10:09,280 --> 00:10:11,720
at least in the online space, before 2016,

252
00:10:11,720 --> 00:10:13,810
but obviously since 2016

253
00:10:13,810 --> 00:10:16,599
and events of 2016 being looked at,

254
00:10:16,600 --> 00:10:17,890
is both a social problem

255
00:10:17,890 --> 00:10:20,770
and one as an information space

256
00:10:20,770 --> 00:10:22,870
that's just full of pollution.

257
00:10:22,870 --> 00:10:26,660
And ultimately a lot of
this is going at scale,

258
00:10:26,660 --> 00:10:28,079
so SJ is gonna take it away.

259
00:10:28,080 --> 00:10:29,922
I've been Ed McMann today.
- Let's go techy.

260
00:10:29,922 --> 00:10:31,330
(laughs)

261
00:10:31,330 --> 00:10:33,110
So we've got a problem.

262
00:10:33,110 --> 00:10:37,060
We've got large-scale coordinated
misinformation attacks.

263
00:10:37,060 --> 00:10:40,502
It looks very similar to
early infosec attacks.

264
00:10:41,500 --> 00:10:44,030
I said at the beginning of last year

265
00:10:44,030 --> 00:10:45,520
that we're pretty much at the cuckoo's egg

266
00:10:45,520 --> 00:10:47,750
stage of development of this,

267
00:10:47,750 --> 00:10:50,030
as from the bad guys' point of view,

268
00:10:50,030 --> 00:10:52,069
I think we've gone past
the Morris worm now

269
00:10:52,070 --> 00:10:55,430
we're kind of heading over
into early virus checkers,

270
00:10:55,430 --> 00:10:58,089
but we don't have time

271
00:10:58,090 --> 00:11:00,570
to create an entire new response field,

272
00:11:00,570 --> 00:11:02,100
so we've stolen yours.

273
00:11:02,100 --> 00:11:07,100
So we've got a large scale problem.

274
00:11:09,280 --> 00:11:11,120
It's a whole system problem,

275
00:11:11,120 --> 00:11:12,750
we need a whole system response,

276
00:11:12,750 --> 00:11:14,850
we need to do it at speed, at scale,

277
00:11:14,850 --> 00:11:18,170
where our end points are human
beings, human communities,

278
00:11:18,170 --> 00:11:20,050
who have all sorts of fun things

279
00:11:20,050 --> 00:11:22,103
like those cognitive biases
you were talking about.

280
00:11:22,103 --> 00:11:23,550
We have about 200,

281
00:11:23,550 --> 00:11:25,729
they are all individual vulnerabilities,

282
00:11:25,730 --> 00:11:28,010
each of which we can
hit in different ways.

283
00:11:28,010 --> 00:11:31,052
So yeah, OODA loops, we got 'em,

284
00:11:31,052 --> 00:11:32,623
we've got them in many scales.

285
00:11:33,610 --> 00:11:36,060
So systems, we're looking at these systems

286
00:11:36,060 --> 00:11:37,359
and we didn't have a framework,

287
00:11:37,360 --> 00:11:39,410
so we needed to go look for one.

288
00:11:39,410 --> 00:11:40,630
We needed to create communities,

289
00:11:40,630 --> 00:11:42,550
so those, all those different viewpoints,

290
00:11:42,550 --> 00:11:44,500
we needed to combine in some way,

291
00:11:44,500 --> 00:11:46,340
so we went out and started creating them.

292
00:11:46,340 --> 00:11:49,360
So we created misinfosec,
misinfosec working group,

293
00:11:49,360 --> 00:11:51,640
but also misinfosec in larger community.

294
00:11:51,640 --> 00:11:53,370
We found everybody we could

295
00:11:53,370 --> 00:11:54,560
who was working on this boundary

296
00:11:54,560 --> 00:11:56,780
between misinformation and infosec,

297
00:11:56,780 --> 00:11:57,800
threw them into one place,

298
00:11:57,800 --> 00:12:00,060
started talking to each other.

299
00:12:00,060 --> 00:12:03,113
People-Centered Internet,
the Vint Cerf organization,

300
00:12:04,190 --> 00:12:05,610
some of us are part of that as well,

301
00:12:05,610 --> 00:12:07,950
we're looking at how we
can work from the bottom,

302
00:12:07,950 --> 00:12:11,880
from human communities, to connect those

303
00:12:11,880 --> 00:12:14,410
into whatever grid we've been, we produce.

304
00:12:14,410 --> 00:12:19,023
IRC, there is now a
cognitive security ISAP,

305
00:12:19,960 --> 00:12:21,670
you probably haven't heard of it yet.

306
00:12:21,670 --> 00:12:24,050
We're still kind of
writing the stuff on it.

307
00:12:24,050 --> 00:12:28,959
But devs down at the Florida
Space Center is running it,

308
00:12:28,960 --> 00:12:31,020
and it's putting out reports already,

309
00:12:31,020 --> 00:12:32,610
it just needs some messaging formats,

310
00:12:32,610 --> 00:12:34,920
that's kind of on us,

311
00:12:34,920 --> 00:12:36,069
but we need to connect together

312
00:12:36,070 --> 00:12:38,280
all of these different groups in some way

313
00:12:38,280 --> 00:12:40,380
and get them talking the same languages.

314
00:12:40,380 --> 00:12:43,570
So one of the problems we have,

315
00:12:43,570 --> 00:12:46,070
all we had when we started working out

316
00:12:46,070 --> 00:12:47,040
how all this connects together

317
00:12:47,040 --> 00:12:48,949
and what we needed to put together,

318
00:12:48,950 --> 00:12:52,830
was we had no big lists of threats

319
00:12:52,830 --> 00:12:54,630
and we had no big lists of incidents,

320
00:12:54,630 --> 00:12:55,950
we had to go build them,

321
00:12:55,950 --> 00:12:56,930
and as we built them,

322
00:12:56,930 --> 00:12:59,209
we realized that we were
looking at different views,

323
00:12:59,210 --> 00:13:02,400
so we had the attacker view.

324
00:13:02,400 --> 00:13:06,910
So the attacker sees the whole space

325
00:13:07,810 --> 00:13:09,430
and what they see,

326
00:13:09,430 --> 00:13:10,939
so we started originally looking at,

327
00:13:10,940 --> 00:13:12,243
we talked about attacks,

328
00:13:13,750 --> 00:13:15,643
and we were trying to work out what,

329
00:13:17,711 --> 00:13:20,900
a attack as a time-limited thing,

330
00:13:20,900 --> 00:13:22,500
and we discovered there
were different sizes,

331
00:13:22,500 --> 00:13:26,100
so, for instance, 2016 election,

332
00:13:26,100 --> 00:13:27,340
the internet research agency,

333
00:13:27,340 --> 00:13:29,100
Russian internet research agency

334
00:13:29,100 --> 00:13:31,570
on the American elections
was about two years long,

335
00:13:31,570 --> 00:13:34,110
it was different to
something like Pizzagate,

336
00:13:34,110 --> 00:13:36,700
it was different to something
like Columbia Chemicals,

337
00:13:36,700 --> 00:13:38,900
our favorite example.

338
00:13:38,900 --> 00:13:41,079
So we decided the big ones were campaigns,

339
00:13:41,080 --> 00:13:42,860
we had advanced persistent threats,

340
00:13:42,860 --> 00:13:44,550
which I think what's now called

341
00:13:44,550 --> 00:13:46,589
advanced persistent manipulators,

342
00:13:46,590 --> 00:13:49,510
and then we have these
incidents within those,

343
00:13:49,510 --> 00:13:51,150
but as human beings,

344
00:13:51,150 --> 00:13:54,750
human beings base themselves
on stories, narratives,

345
00:13:54,750 --> 00:13:56,360
so narratives are the stories

346
00:13:56,360 --> 00:13:58,980
that we base our belonging on.

347
00:13:58,980 --> 00:14:01,850
We base our sense of how
we belong to communities,

348
00:14:01,850 --> 00:14:04,850
our in groups, our out groups, who we are,

349
00:14:04,850 --> 00:14:07,720
so narrative is the
level at which we fight.

350
00:14:07,720 --> 00:14:10,540
So there's lot of work happening
now on narrative tracking,

351
00:14:10,540 --> 00:14:13,530
and we look at and we're
starting to report at narratives,

352
00:14:13,530 --> 00:14:15,709
but what you see as a data scientist

353
00:14:15,710 --> 00:14:18,090
on the internet tracking through this,

354
00:14:18,090 --> 00:14:19,250
and I am a data scientist

355
00:14:19,250 --> 00:14:21,810
so I spend a lot of
time down in the weeds,

356
00:14:21,810 --> 00:14:25,699
is you see artifacts, you see messages,

357
00:14:25,700 --> 00:14:28,870
you see users

358
00:14:28,870 --> 00:14:31,420
well, accounts, not necessarily users,

359
00:14:31,420 --> 00:14:33,150
you see connections between these,

360
00:14:33,150 --> 00:14:35,230
you see connections across platforms,

361
00:14:35,230 --> 00:14:37,297
so you see a bunch of stuff at the bottom

362
00:14:37,297 --> 00:14:38,990
and you have to work your way back to

363
00:14:38,990 --> 00:14:39,960
is this an incident,

364
00:14:39,960 --> 00:14:42,510
is this a coordinated campaign?

365
00:14:42,510 --> 00:14:45,870
So this is one of the important diagrams

366
00:14:45,870 --> 00:14:50,230
because this is, we track
at the incident level,

367
00:14:50,230 --> 00:14:51,520
we fight at the narrative level,

368
00:14:51,520 --> 00:14:53,540
but we basically get stuff
at the artifact level

369
00:14:53,540 --> 00:14:55,360
and if we're really, really, really lucky

370
00:14:55,360 --> 00:14:57,154
and we get intelligence information,

371
00:14:57,154 --> 00:14:59,890
we get information at the campaign level.

372
00:14:59,890 --> 00:15:01,910
So stuff we built.

373
00:15:01,910 --> 00:15:04,535
Luckily we don't tell
you about this stuff.

374
00:15:04,535 --> 00:15:06,360
(laughs) So stage-based models.

375
00:15:06,360 --> 00:15:08,650
We looked at a huge pile of models.

376
00:15:08,650 --> 00:15:12,520
So we looked at models for marketing,

377
00:15:12,520 --> 00:15:15,310
because marketing models
are really, really useful

378
00:15:15,310 --> 00:15:18,900
for things like extremism,

379
00:15:18,900 --> 00:15:21,079
so it turns out that a
sales conversion model

380
00:15:21,080 --> 00:15:22,800
is exactly what you need

381
00:15:22,800 --> 00:15:26,402
when you are looking at how
to do extremist conversions.

382
00:15:28,170 --> 00:15:32,069
We looked at psyops models,
information security models,

383
00:15:32,070 --> 00:15:34,700
sorry, information operations models,

384
00:15:34,700 --> 00:15:37,430
so psyops models are already
used to run campaigns,

385
00:15:37,430 --> 00:15:38,680
so okay, let's go look at these.

386
00:15:38,680 --> 00:15:39,579
Can we use these?

387
00:15:39,580 --> 00:15:42,030
Can we use these to map
misinformation campaigns?

388
00:15:42,990 --> 00:15:46,810
We looked at existing misinfosec models,

389
00:15:46,810 --> 00:15:48,609
there were a couple around.

390
00:15:48,610 --> 00:15:50,580
The justice department
had a really lovely one.

391
00:15:50,580 --> 00:15:53,120
If you're in the room,
thank you, you're great.

392
00:15:53,120 --> 00:15:56,680
And we looked at
information security models,

393
00:15:56,680 --> 00:15:59,714
and we looked at the cyclic models

394
00:15:59,715 --> 00:16:01,130
and we looked at stage-based models

395
00:16:01,130 --> 00:16:03,780
and we absolutely adored
the Cyber Kill Chain,

396
00:16:03,780 --> 00:16:04,770
'cause it covered everything,

397
00:16:04,770 --> 00:16:06,939
but we really adored ATTACK,

398
00:16:06,940 --> 00:16:10,740
because we love the idea
of having the stages

399
00:16:10,740 --> 00:16:13,270
and then this breakdown the TDPs,

400
00:16:13,270 --> 00:16:15,300
because that's what we needed,

401
00:16:15,300 --> 00:16:16,900
you actually had what we needed.

402
00:16:18,291 --> 00:16:20,880
And we tried really,
really, really hard to fit

403
00:16:20,880 --> 00:16:23,910
misinformation into the
ATTACK framework as it exists

404
00:16:23,910 --> 00:16:25,790
and we couldn't make it work.

405
00:16:25,790 --> 00:16:27,069
I'm sorry, we tried.

406
00:16:27,070 --> 00:16:28,730
We really tried

407
00:16:28,730 --> 00:16:30,700
We think that actually

408
00:16:30,700 --> 00:16:33,190
ultimately we'll probably
end up as parallel branches,

409
00:16:33,190 --> 00:16:34,520
we'll end up doing our thing,

410
00:16:34,520 --> 00:16:36,090
you'll end up doing your thing,

411
00:16:36,090 --> 00:16:37,820
we'll both end up doing
hierarchical things

412
00:16:37,820 --> 00:16:39,960
and we'll end up merging in the end,

413
00:16:39,960 --> 00:16:42,600
but for the moment, we're different.

414
00:16:42,600 --> 00:16:44,103
So yeah, we extended you.

415
00:16:45,940 --> 00:16:47,640
And one of the problems,

416
00:16:47,640 --> 00:16:49,460
we didn't have these catalogs

417
00:16:49,460 --> 00:16:50,810
so we had to go build them.

418
00:16:51,760 --> 00:16:54,939
There were pieces of
catalogs all over the world,

419
00:16:54,940 --> 00:16:57,560
the Oxford Internet Institute had one,

420
00:16:57,560 --> 00:16:59,099
there's some at NATO.

421
00:16:59,100 --> 00:17:00,540
Nobody had a standard for it,

422
00:17:00,540 --> 00:17:01,723
so we built a standard.

423
00:17:03,290 --> 00:17:07,173
There are two STIX
extensions we're putting in,

424
00:17:08,140 --> 00:17:10,330
can't find you, MITRE person that said

425
00:17:10,329 --> 00:17:11,923
we'll come help with that.

426
00:17:13,490 --> 00:17:17,819
We're putting in for
incident and narrative,

427
00:17:17,819 --> 00:17:19,240
those are the two pieces we need

428
00:17:19,240 --> 00:17:22,780
to carry misinformation
messages across inter STIX

429
00:17:22,780 --> 00:17:24,913
and across all of the other,

430
00:17:26,099 --> 00:17:28,973
taxi and misb and all the rest of it,

431
00:17:29,990 --> 00:17:30,950
but this is one of them.

432
00:17:30,950 --> 00:17:32,230
So we needed to populate,

433
00:17:32,230 --> 00:17:34,300
so I've already talked about Internet,

434
00:17:34,300 --> 00:17:36,480
the elections, Columbia Chemicals,

435
00:17:36,480 --> 00:17:38,170
but another thing that was a useful source

436
00:17:38,170 --> 00:17:39,980
of information was failed attempts.

437
00:17:39,980 --> 00:17:43,220
Russia does not have a
lot of luck in France.

438
00:17:43,220 --> 00:17:44,870
French education system,

439
00:17:44,870 --> 00:17:45,929
France gets prepared,

440
00:17:45,930 --> 00:17:48,560
has some really good
techs, yada yada yada.

441
00:17:48,560 --> 00:17:52,129
But just like infosec,
failures tell us things.

442
00:17:52,130 --> 00:17:53,640
One of the things it tells us

443
00:17:53,640 --> 00:17:56,630
is resilience and counters

444
00:17:56,630 --> 00:17:57,879
We're, in two weeks time,

445
00:17:57,879 --> 00:18:00,020
we're doing workshop on counters,

446
00:18:00,020 --> 00:18:02,720
so I'm busy collecting those,
which is kind of useful.

447
00:18:03,710 --> 00:18:04,900
Again, we learnt from you,

448
00:18:04,900 --> 00:18:06,880
so this is what one of the
daily sheets looks like,

449
00:18:06,880 --> 00:18:09,560
this is our favorite example.

450
00:18:09,560 --> 00:18:10,393
If you're on Twitter,

451
00:18:10,393 --> 00:18:14,380
you'll see this as a STIX graph,

452
00:18:14,380 --> 00:18:17,010
so just a very small example,

453
00:18:17,010 --> 00:18:18,670
one day people in an area woke up,

454
00:18:18,670 --> 00:18:20,280
there are chemical factories in the area,

455
00:18:20,280 --> 00:18:21,570
there was a message on their thing saying,

456
00:18:21,570 --> 00:18:22,909
stay inside, there's a fire.

457
00:18:22,910 --> 00:18:23,810
There was no fire.

458
00:18:24,830 --> 00:18:27,500
We think it was a really
early Russian test message.

459
00:18:27,500 --> 00:18:32,500
We've been seeing these
since 2010, the BP oil spill,

460
00:18:32,840 --> 00:18:36,100
so Kate Starbird up at the
University of Washington

461
00:18:36,100 --> 00:18:37,560
has been tracking those,

462
00:18:37,560 --> 00:18:39,720
does really good work on that,

463
00:18:39,720 --> 00:18:44,040
and we started feeding those,

464
00:18:44,040 --> 00:18:44,920
started looking at techniques,

465
00:18:44,920 --> 00:18:46,530
so this is one of the techniques,

466
00:18:46,530 --> 00:18:48,343
paid targeted ads is this technique,

467
00:18:49,930 --> 00:18:51,970
so we pulled all this together,

468
00:18:51,970 --> 00:18:55,980
and I'll head forwards because time,

469
00:18:55,980 --> 00:18:57,530
so we built this.

470
00:18:57,530 --> 00:18:58,543
So this is our baby.

471
00:19:00,720 --> 00:19:03,130
It's not as big as ATTACK,

472
00:19:03,130 --> 00:19:08,130
because we've just built it
off 22 different incidents,

473
00:19:09,260 --> 00:19:11,300
a day with a bunch of experts

474
00:19:11,300 --> 00:19:13,443
and an awful lot of post its on a wall.

475
00:19:15,428 --> 00:19:18,730
So we pulled out all of the tactics

476
00:19:18,730 --> 00:19:21,430
and techniques from 22 incidents

477
00:19:22,760 --> 00:19:25,280
we post-it'd the heck out of it,

478
00:19:25,280 --> 00:19:28,500
we grouped them, we
went through the stages,

479
00:19:28,500 --> 00:19:31,030
then we looked at all of the
other models that existed,

480
00:19:31,030 --> 00:19:31,863
including ATTACK ,

481
00:19:31,863 --> 00:19:33,030
and we went back and said,

482
00:19:33,030 --> 00:19:35,260
do these things contain,

483
00:19:35,260 --> 00:19:36,670
this one might help more,

484
00:19:36,670 --> 00:19:38,253
these are the stages we have.

485
00:19:39,200 --> 00:19:44,200
The big purple and red
things, those are phases.

486
00:19:45,130 --> 00:19:47,170
So while we were looking,

487
00:19:47,170 --> 00:19:50,490
so we realized that
there were other pieces

488
00:19:50,490 --> 00:19:54,430
from the models that we could include.

489
00:19:54,430 --> 00:19:55,263
So, for example,

490
00:19:55,263 --> 00:19:58,750
the planning stages came
from the psyops models.

491
00:19:58,750 --> 00:20:03,750
The evaluation came from the
adtech, the advertising models.

492
00:20:04,440 --> 00:20:07,800
So if you have an incident
that's part of a campaign,

493
00:20:07,800 --> 00:20:09,639
you're going to run it

494
00:20:09,640 --> 00:20:11,620
but you're also gonna check what worked

495
00:20:11,620 --> 00:20:13,780
to feed into the next incident.

496
00:20:13,780 --> 00:20:18,500
So we, again this is kind of not pretty,

497
00:20:18,500 --> 00:20:21,760
but near the blue line is the tactics,

498
00:20:21,760 --> 00:20:23,510
the grays are the techniques,

499
00:20:23,510 --> 00:20:25,860
I will give you the
address at the end of this,

500
00:20:26,890 --> 00:20:28,880
and the purple at the left of boom,

501
00:20:28,880 --> 00:20:30,590
the red at the right of boom.

502
00:20:30,590 --> 00:20:32,659
So I believe in your parlance,

503
00:20:32,660 --> 00:20:37,280
the purple are the pre-attacks
and the red is the attacks.

504
00:20:37,280 --> 00:20:39,190
We need help with this

505
00:20:39,190 --> 00:20:41,560
and this is just where we start

506
00:20:41,560 --> 00:20:44,090
and I suspect some of
you will probably go back

507
00:20:44,090 --> 00:20:46,929
and think, hey cool, we can do this too.

508
00:20:46,930 --> 00:20:49,423
That would be absolutely bloody awesome.

509
00:20:50,980 --> 00:20:54,340
So this is where we are at the moment,

510
00:20:54,340 --> 00:20:57,800
and we've already had
people starting to use this

511
00:20:57,800 --> 00:21:01,300
and starting to pick out techniques

512
00:21:01,300 --> 00:21:03,360
and starting to share techniques

513
00:21:03,360 --> 00:21:05,689
used in the individual attacks,

514
00:21:05,690 --> 00:21:08,940
and the next thing that
we've started doing

515
00:21:08,940 --> 00:21:11,030
is the STIX part of this.

516
00:21:11,030 --> 00:21:13,320
So we did the mapping,

517
00:21:13,320 --> 00:21:14,810
so this is the mapping between

518
00:21:14,810 --> 00:21:17,950
misinformation STIX and infosec STIX,

519
00:21:17,950 --> 00:21:21,613
and again we got an almost one
to one map between the two.

520
00:21:22,570 --> 00:21:27,409
We can carry misinformation
on straight STIX

521
00:21:27,410 --> 00:21:31,090
as long as we put the
incidents into intrusion sets

522
00:21:31,090 --> 00:21:32,743
and narratives into malware.

523
00:21:33,840 --> 00:21:35,379
It's not pretty.

524
00:21:35,380 --> 00:21:37,860
The diagram I put up
this morning on Twitter

525
00:21:37,860 --> 00:21:39,219
uses intrusion sets and malware,

526
00:21:39,220 --> 00:21:42,320
it would be nicer to have
two separate objects,

527
00:21:42,320 --> 00:21:43,710
incident and narrative,

528
00:21:43,710 --> 00:21:45,480
make that a lot cleaner,

529
00:21:45,480 --> 00:21:49,700
but it works, it works, it's beautiful,

530
00:21:49,700 --> 00:21:52,200
we can carry that, we can
match that into infosec,

531
00:21:54,060 --> 00:21:56,513
and it also means that finally, finally,

532
00:21:57,440 --> 00:22:00,680
we can match the work on the
data science people are doing

533
00:22:00,680 --> 00:22:03,120
to the work that the information
operations people are doing

534
00:22:03,120 --> 00:22:04,469
and we can talk to each other.

535
00:22:04,470 --> 00:22:05,920
We've got a language at last.

536
00:22:07,300 --> 00:22:09,500
STIX graphs, so yeah.

537
00:22:09,500 --> 00:22:12,280
We can do this, we can
talk about thread actors,

538
00:22:12,280 --> 00:22:13,960
same way you've been doing for years.

539
00:22:13,960 --> 00:22:16,970
Finally we can do this
and we can do it formally

540
00:22:16,970 --> 00:22:18,150
and we can reason about it

541
00:22:18,150 --> 00:22:20,053
and we can start doing the science.

542
00:22:21,450 --> 00:22:22,713
It just becomes cleaner.

543
00:22:23,860 --> 00:22:26,642
And we can feed that to the ISAOs.

544
00:22:27,660 --> 00:22:30,220
At the moment there are email reports

545
00:22:30,220 --> 00:22:33,960
going out to the ISAOs and
ISACs on misinformation.

546
00:22:33,960 --> 00:22:36,633
Now we can start doing
the real time feeds,

547
00:22:36,633 --> 00:22:38,200
soon as we get this stuff.

548
00:22:38,200 --> 00:22:40,503
So this is where we're going next.

549
00:22:41,520 --> 00:22:43,400
We've got that blue
team workshop coming up

550
00:22:43,400 --> 00:22:44,690
talking about counters

551
00:22:44,690 --> 00:22:46,520
so just as you were picking off

552
00:22:46,520 --> 00:22:48,920
all of the techniques one by one,

553
00:22:48,920 --> 00:22:52,090
we are doing one by one
technique level counters,

554
00:22:52,090 --> 00:22:53,720
we're doing tactic level counters,

555
00:22:53,720 --> 00:22:56,530
we're doing big process
procedure level counters,

556
00:22:56,530 --> 00:22:59,080
and we're looking for
anything that is interesting

557
00:22:59,080 --> 00:23:00,423
and weird on top of that.

558
00:23:02,050 --> 00:23:04,810
We'd like AMITT, or some form of AMITT,

559
00:23:04,810 --> 00:23:07,710
to be in the misinformation
response centers coming through,

560
00:23:07,710 --> 00:23:10,860
we've been talking not
just in this country

561
00:23:10,860 --> 00:23:12,382
but to other bodies as well,

562
00:23:13,290 --> 00:23:16,520
and we'd like to start
testing against new incidents,

563
00:23:16,520 --> 00:23:18,389
we've been so busy,

564
00:23:18,390 --> 00:23:21,020
wrapped up in just trying
to get a standard going.

565
00:23:21,020 --> 00:23:22,310
Literally, we gave ourselves a year

566
00:23:22,310 --> 00:23:23,940
to try to build some sort of response,

567
00:23:23,940 --> 00:23:25,920
some sort of way of responding formally

568
00:23:25,920 --> 00:23:28,070
and well and bringing
in a whole new crowd.

569
00:23:29,430 --> 00:23:31,660
So yeah, finding response populations,

570
00:23:31,660 --> 00:23:33,600
finding people looking to use this

571
00:23:33,600 --> 00:23:36,070
and get going with it,

572
00:23:36,070 --> 00:23:38,639
and you wanna go find updates,

573
00:23:38,640 --> 00:23:42,220
misinfotec.org is one of the group sites.

574
00:23:42,220 --> 00:23:43,770
It's got a page in there for AMITT,

575
00:23:43,770 --> 00:23:45,629
there's a Github repo,

576
00:23:45,630 --> 00:23:48,340
what we do on AMITT ends up in the repo.

577
00:23:48,340 --> 00:23:49,602
So that's us.

578
00:23:52,109 --> 00:23:55,275
(audience applauding)

