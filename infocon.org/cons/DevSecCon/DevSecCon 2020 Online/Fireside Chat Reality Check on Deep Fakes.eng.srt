1
00:00:00,000 --> 00:00:05,220
hello hey how are you hey hey I could

2
00:00:05,220 --> 00:00:07,170
hear you yeah this is exciting you were

3
00:00:07,170 --> 00:00:08,730
talking about deep fakes you're a minute

4
00:00:08,730 --> 00:00:11,700
with our friends yeah okay one of my

5
00:00:11,700 --> 00:00:16,049
favorite little topics okay yeah we're

6
00:00:16,049 --> 00:00:17,369
gonna dig in a little bit we'll talk a

7
00:00:17,369 --> 00:00:18,750
little bit about the tech we'll talk

8
00:00:18,750 --> 00:00:21,630
definitely about the ethics and even

9
00:00:21,630 --> 00:00:24,300
some of the threat vectors and just you

10
00:00:24,300 --> 00:00:27,000
know how realistic is it or isn't it so

11
00:00:27,000 --> 00:00:29,010
well I I think I think you got the

12
00:00:29,010 --> 00:00:30,630
controls here so I think I'll just gonna

13
00:00:30,630 --> 00:00:31,920
drop and let you do your thing

14
00:00:31,920 --> 00:00:37,890
awesome well I appreciate it so there we

15
00:00:37,890 --> 00:00:41,460
go you many buttons to click at the same

16
00:00:41,460 --> 00:00:43,379
time I have all these awesome I don't

17
00:00:43,379 --> 00:00:44,370
know where to start

18
00:00:44,370 --> 00:00:47,579
um so first of all let me introduce

19
00:00:47,579 --> 00:00:49,890
myself everybody if you don't know who I

20
00:00:49,890 --> 00:00:52,710
am already from earlier in the

21
00:00:52,710 --> 00:00:54,539
conference I'm Alyssa Miller I'm an

22
00:00:54,539 --> 00:00:56,629
application security advocate at sneaked

23
00:00:56,629 --> 00:01:00,989
I'm also uh you know a hacker researcher

24
00:01:00,989 --> 00:01:02,910
public speaker all that happy fun stuff

25
00:01:02,910 --> 00:01:06,689
but anyone who follows me knows that I

26
00:01:06,689 --> 00:01:08,700
have a particular passion for deep fakes

27
00:01:08,700 --> 00:01:11,580
and I've been doing some different

28
00:01:11,580 --> 00:01:13,500
research I've definitely been speaking

29
00:01:13,500 --> 00:01:16,409
on it at a number of conferences and so

30
00:01:16,409 --> 00:01:19,350
I was asked today to put together this

31
00:01:19,350 --> 00:01:21,689
fireside chat with a couple of my

32
00:01:21,689 --> 00:01:25,140
wonderful colleagues to just talk

33
00:01:25,140 --> 00:01:27,030
through some of the the various issues

34
00:01:27,030 --> 00:01:29,189
and things surrounding this technology

35
00:01:29,189 --> 00:01:32,070
and where things seem to be going so

36
00:01:32,070 --> 00:01:34,680
with that I'm gonna jam why don't you go

37
00:01:34,680 --> 00:01:36,720
ahead and introduce yourself first and

38
00:01:36,720 --> 00:01:38,909
we'll just go around the horn here sure

39
00:01:38,909 --> 00:01:39,680
thank you Lisa

40
00:01:39,680 --> 00:01:42,659
so my name is Jan paurav I'm a cyber

41
00:01:42,659 --> 00:01:44,490
security and national security reporter

42
00:01:44,490 --> 00:01:46,020
New York I've written about deep fakes

43
00:01:46,020 --> 00:01:48,060
and journalists are concerned about how

44
00:01:48,060 --> 00:01:52,740
to authenticate video is real on very

45
00:01:52,740 --> 00:01:55,350
short deadlines to ensure that what they

46
00:01:55,350 --> 00:01:57,210
publish is in fact correct and I'll be

47
00:01:57,210 --> 00:01:58,469
bringing that perspective to the

48
00:01:58,469 --> 00:02:02,430
conversation thank you and also with us

49
00:02:02,430 --> 00:02:04,590
we have andrea andrea go ahead introduce

50
00:02:04,590 --> 00:02:07,500
yourself hey everyone my name is andrea

51
00:02:07,500 --> 00:02:10,709
downing i am an advocate for people who

52
00:02:10,709 --> 00:02:13,920
carry hereditary cancer mutations so

53
00:02:13,920 --> 00:02:16,500
of my work in healthcare it has to do

54
00:02:16,500 --> 00:02:19,530
with how we use social media and put our

55
00:02:19,530 --> 00:02:21,300
digital selves out there and how that

56
00:02:21,300 --> 00:02:24,000
can be weaponized against us so you know

57
00:02:24,000 --> 00:02:27,120
for us and my community as an advocate

58
00:02:27,120 --> 00:02:28,860
the stakes are really high when we think

59
00:02:28,860 --> 00:02:31,080
about how future technologies can affect

60
00:02:31,080 --> 00:02:34,560
us so very interesting topic and thank

61
00:02:34,560 --> 00:02:37,920
you Alissa for having me awesome and

62
00:02:37,920 --> 00:02:41,640
then last and certainly not least we've

63
00:02:41,640 --> 00:02:44,489
got Lisa Johnson Lisa go ahead and tell

64
00:02:44,489 --> 00:02:46,010
us a little bit about yourself as well

65
00:02:46,010 --> 00:02:50,459
hi I'm Rita Johnson and I have a

66
00:02:50,459 --> 00:02:53,250
master's degree in cyber security policy

67
00:02:53,250 --> 00:02:56,040
in governance and I have a particular

68
00:02:56,040 --> 00:02:59,340
interest in privacy law and similar to

69
00:02:59,340 --> 00:03:02,400
and Andrea I come from a healthcare IT

70
00:03:02,400 --> 00:03:05,640
background so I have a lot of questions

71
00:03:05,640 --> 00:03:09,959
around DNA rare disease patients being

72
00:03:09,959 --> 00:03:12,720
able to anonymize patient data that sort

73
00:03:12,720 --> 00:03:17,430
of thing awesome so let's just dive

74
00:03:17,430 --> 00:03:19,769
right in and I actually thought by the

75
00:03:19,769 --> 00:03:21,810
first place that would be best to start

76
00:03:21,810 --> 00:03:24,209
is just really to define what a deep

77
00:03:24,209 --> 00:03:26,700
fake is and like I said I do a number of

78
00:03:26,700 --> 00:03:29,730
talks on this and I I don't know that

79
00:03:29,730 --> 00:03:31,070
there's necessarily a hard-and-fast

80
00:03:31,070 --> 00:03:33,299
definition but the way I put it and

81
00:03:33,299 --> 00:03:34,709
we'll see what you know some of my

82
00:03:34,709 --> 00:03:36,570
colleagues think here and we thought

83
00:03:36,570 --> 00:03:38,100
about deep fakes what I'm usually

84
00:03:38,100 --> 00:03:42,120
referring to is any form of media that's

85
00:03:42,120 --> 00:03:44,250
been artificially generated using deep

86
00:03:44,250 --> 00:03:46,500
learning neural networks so it includes

87
00:03:46,500 --> 00:03:50,640
still images videos and audio and indeed

88
00:03:50,640 --> 00:03:53,100
that's kind of what we're seeing in the

89
00:03:53,100 --> 00:03:55,620
market space but I want to throw it out

90
00:03:55,620 --> 00:03:57,060
to the group based on what you've seen

91
00:03:57,060 --> 00:03:59,100
is there anything you would add to that

92
00:03:59,100 --> 00:04:01,170
definition or anything that you think is

93
00:04:01,170 --> 00:04:02,579
notable that we should really draw out

94
00:04:02,579 --> 00:04:08,220
of that sure I'll go and I you know I

95
00:04:08,220 --> 00:04:12,389
think a lot of the way these the this

96
00:04:12,389 --> 00:04:15,329
new technology can affect us is unknown

97
00:04:15,329 --> 00:04:19,228
and one of the things as as somebody who

98
00:04:19,228 --> 00:04:21,180
kind of delved into the world of

99
00:04:21,180 --> 00:04:23,010
security research two years ago and

100
00:04:23,010 --> 00:04:26,580
didn't really understand how social

101
00:04:26,580 --> 00:04:27,570
networks would

102
00:04:27,570 --> 00:04:29,730
fact by a community I start to think

103
00:04:29,730 --> 00:04:34,290
about how the fakes can create a kind of

104
00:04:34,290 --> 00:04:37,470
a paradigm where our images can be used

105
00:04:37,470 --> 00:04:40,190
against us and in ways that we don't

106
00:04:40,190 --> 00:04:44,850
expect when we have every single video

107
00:04:44,850 --> 00:04:47,900
from you know our child's birth to our

108
00:04:47,900 --> 00:04:50,760
family reunions out there on Facebook

109
00:04:50,760 --> 00:04:53,820
you know how do those things get used in

110
00:04:53,820 --> 00:04:54,930
the future so that's one of the

111
00:04:54,930 --> 00:04:59,310
questions I have I would add it's

112
00:04:59,310 --> 00:05:01,290
important to distinguish between deep

113
00:05:01,290 --> 00:05:03,000
fakes and what I would call shallow

114
00:05:03,000 --> 00:05:05,190
fakes that is to say if I Photoshop a

115
00:05:05,190 --> 00:05:07,560
bogus image and then try to sell it to a

116
00:05:07,560 --> 00:05:10,020
reporter that's not generated by machine

117
00:05:10,020 --> 00:05:11,610
learning or a neural network it's just

118
00:05:11,610 --> 00:05:14,160
good old fashioned Photoshop if I chop

119
00:05:14,160 --> 00:05:16,350
an audio file or a video file to remove

120
00:05:16,350 --> 00:05:18,090
the keyword to make someone look like

121
00:05:18,090 --> 00:05:19,350
they're being a jerk when they're not

122
00:05:19,350 --> 00:05:23,010
that's bad it's fake media but again

123
00:05:23,010 --> 00:05:25,860
it's a shallow fake it's not a deep fake

124
00:05:25,860 --> 00:05:27,930
and I think that that's an important

125
00:05:27,930 --> 00:05:29,460
distinction to draw when having this

126
00:05:29,460 --> 00:05:33,900
conversation yeah I would agree and I

127
00:05:33,900 --> 00:05:35,340
think actually at least here in the

128
00:05:35,340 --> 00:05:37,980
United States we had one really good

129
00:05:37,980 --> 00:05:40,500
example of that not all that long ago

130
00:05:40,500 --> 00:05:42,540
and it's one that I usually like to draw

131
00:05:42,540 --> 00:05:44,490
on because it's important and it does

132
00:05:44,490 --> 00:05:46,440
frame a lot of the conversation around

133
00:05:46,440 --> 00:05:48,180
what you're seeing and that's this video

134
00:05:48,180 --> 00:05:50,730
that's surfaced of Nancy Pelosi where

135
00:05:50,730 --> 00:05:54,180
she was doing an interview and someone

136
00:05:54,180 --> 00:05:57,030
took the video and slowed it down to 75%

137
00:05:57,030 --> 00:06:00,600
speed and when you do that and you you

138
00:06:00,600 --> 00:06:02,490
know you keep the audio normalized so

139
00:06:02,490 --> 00:06:04,670
that pitch stays the same but it slows

140
00:06:04,670 --> 00:06:07,500
yeah it makes it appear that she was

141
00:06:07,500 --> 00:06:09,240
inebriated in some way and so of course

142
00:06:09,240 --> 00:06:13,560
that that flew across the internet with

143
00:06:13,560 --> 00:06:15,270
reckless abandon and in fact people are

144
00:06:15,270 --> 00:06:17,550
still sharing it today I actually did a

145
00:06:17,550 --> 00:06:19,950
search earlier today people are still

146
00:06:19,950 --> 00:06:21,570
sharing that video claiming that it's

147
00:06:21,570 --> 00:06:24,240
real and now I don't know if they maybe

148
00:06:24,240 --> 00:06:25,590
know that it's not and they're just

149
00:06:25,590 --> 00:06:27,750
trying to convince people but it's an

150
00:06:27,750 --> 00:06:30,240
interesting fact because yeah that's not

151
00:06:30,240 --> 00:06:32,160
a deep fake in fact I think some people

152
00:06:32,160 --> 00:06:35,100
originally claimed it was but it's not

153
00:06:35,100 --> 00:06:36,960
it's just hey I took this video and I

154
00:06:36,960 --> 00:06:41,520
slowed it down 75% so when we talk

155
00:06:41,520 --> 00:06:44,610
you know deep fakes we're talking hey I

156
00:06:44,610 --> 00:06:47,940
created something that didn't exist and

157
00:06:47,940 --> 00:06:51,120
I created it using deep neural networks

158
00:06:51,120 --> 00:06:54,720
not just you know cool audio and visual

159
00:06:54,720 --> 00:06:59,400
tricks so Lisa I'm curious from your

160
00:06:59,400 --> 00:07:01,169
perspective I know you've looked into

161
00:07:01,169 --> 00:07:04,310
this area some what what is it that

162
00:07:04,310 --> 00:07:06,330
especially coming from the healthcare

163
00:07:06,330 --> 00:07:07,949
background what is it that catches your

164
00:07:07,949 --> 00:07:14,430
eye about deep fakes in general uh what

165
00:07:14,430 --> 00:07:16,229
would be interesting right would be to

166
00:07:16,229 --> 00:07:18,360
take the example that we had yesterday

167
00:07:18,360 --> 00:07:20,699
of Donald Trump walking slowly down a

168
00:07:20,699 --> 00:07:25,289
ramp and taking a medical diagnosis

169
00:07:25,289 --> 00:07:28,740
deriving that and then using the video

170
00:07:28,740 --> 00:07:30,949
and altering it some way to sort of

171
00:07:30,949 --> 00:07:34,289
deepen the possibility of that diagnosis

172
00:07:34,289 --> 00:07:41,930
existing so let's say there God

173
00:07:41,930 --> 00:07:44,159
Parkinson's let's say it was Parkinson's

174
00:07:44,159 --> 00:07:47,280
right you could take that video and

175
00:07:47,280 --> 00:07:50,099
alter it so that the symptoms seemed to

176
00:07:50,099 --> 00:07:52,740
be more in the Parkinson's way and then

177
00:07:52,740 --> 00:07:53,669
put that out there

178
00:07:53,669 --> 00:07:55,710
and have that be part of the political

179
00:07:55,710 --> 00:07:59,460
discussion it would be an issue for

180
00:07:59,460 --> 00:08:02,699
medical disclosure and you know how

181
00:08:02,699 --> 00:08:07,919
would we work around that interesting

182
00:08:07,919 --> 00:08:10,590
go ahead Andrea yeah that kind of makes

183
00:08:10,590 --> 00:08:13,020
me think of so many permutations of how

184
00:08:13,020 --> 00:08:16,349
this could be potentially weaponized in

185
00:08:16,349 --> 00:08:20,729
the future and so so I my personal

186
00:08:20,729 --> 00:08:22,919
favorite deep fake is the one that where

187
00:08:22,919 --> 00:08:26,580
Mark Zuckerberg was making an

188
00:08:26,580 --> 00:08:29,639
announcement about Facebook and we all

189
00:08:29,639 --> 00:08:31,579
know that has he has had some pretty

190
00:08:31,579 --> 00:08:35,549
strong stances about his unwillingness

191
00:08:35,549 --> 00:08:38,520
to regulate anything that is real or

192
00:08:38,520 --> 00:08:41,219
fake on his platform and what happened

193
00:08:41,219 --> 00:08:42,958
in response to that was there was a

194
00:08:42,958 --> 00:08:45,270
couple of artists who created a deep

195
00:08:45,270 --> 00:08:47,570
fake of him essentially talking about

196
00:08:47,570 --> 00:08:50,250
stealing our data and weaponizing it and

197
00:08:50,250 --> 00:08:53,610
using it against us so while I laughed

198
00:08:53,610 --> 00:08:54,660
so hard and there was

199
00:08:54,660 --> 00:08:56,459
like a bit of shot in from I thinking

200
00:08:56,459 --> 00:08:58,860
about oh you know this this is being

201
00:08:58,860 --> 00:09:02,850
used in a way that can you know harm his

202
00:09:02,850 --> 00:09:04,980
reputation or his image if people don't

203
00:09:04,980 --> 00:09:07,740
know what's real or fake I think when

204
00:09:07,740 --> 00:09:09,660
you translate that back to well what

205
00:09:09,660 --> 00:09:11,430
could happen to a regular person if that

206
00:09:11,430 --> 00:09:14,790
is used against you oh my gosh there's

207
00:09:14,790 --> 00:09:16,769
there are just so many ways in which I

208
00:09:16,769 --> 00:09:18,600
can think of these these videos on

209
00:09:18,600 --> 00:09:22,500
social media and and you know something

210
00:09:22,500 --> 00:09:24,000
that I might have posted five years ago

211
00:09:24,000 --> 00:09:28,709
that could be used to you know let's say

212
00:09:28,709 --> 00:09:30,839
I was in an active court case and some

213
00:09:30,839 --> 00:09:33,329
of you wanted to create evidence where

214
00:09:33,329 --> 00:09:35,970
we don't know what's real or fake and I

215
00:09:35,970 --> 00:09:39,750
think so much of this this you know

216
00:09:39,750 --> 00:09:42,689
current landscape you know where we have

217
00:09:42,689 --> 00:09:45,480
active disinformation campaigns from

218
00:09:45,480 --> 00:09:48,689
static factors on social media the more

219
00:09:48,689 --> 00:09:50,279
that we give them the more ammo that we

220
00:09:50,279 --> 00:09:52,709
give them in terms of our own digital

221
00:09:52,709 --> 00:09:55,560
samples the things that we we put out on

222
00:09:55,560 --> 00:09:57,509
the internet those are all things that

223
00:09:57,509 --> 00:09:59,579
I've been very concerned about from

224
00:09:59,579 --> 00:10:03,300
myself in my community well I have a

225
00:10:03,300 --> 00:10:05,370
question for you Alyssa my understanding

226
00:10:05,370 --> 00:10:08,040
is that academics academic researchers

227
00:10:08,040 --> 00:10:10,680
have had good success finding ways to

228
00:10:10,680 --> 00:10:13,069
authenticate video and to conclusively

229
00:10:13,069 --> 00:10:16,319
identify deep fake videos I think you

230
00:10:16,319 --> 00:10:18,329
know more about than I do is is is that

231
00:10:18,329 --> 00:10:20,420
your conclusion as well can a

232
00:10:20,420 --> 00:10:22,980
professional expert say on the witness

233
00:10:22,980 --> 00:10:25,170
stand Your Honor I've examined this

234
00:10:25,170 --> 00:10:26,880
video and I believe it with confidence

235
00:10:26,880 --> 00:10:29,100
to be a deep fake or is there still like

236
00:10:29,100 --> 00:10:32,009
we just don't know so there there's

237
00:10:32,009 --> 00:10:33,480
definitely been a lot of research you're

238
00:10:33,480 --> 00:10:36,089
right and I think we are at a point

239
00:10:36,089 --> 00:10:38,579
still where it is actually ultimately

240
00:10:38,579 --> 00:10:42,209
pretty easy to identify deep fakes you

241
00:10:42,209 --> 00:10:44,610
know some of the things and of course a

242
00:10:44,610 --> 00:10:46,019
lot of it depends on the quality of the

243
00:10:46,019 --> 00:10:49,290
deep fake - and I mean it it that's

244
00:10:49,290 --> 00:10:50,730
where a lot of my research has been

245
00:10:50,730 --> 00:10:52,800
around is looking at okay how difficult

246
00:10:52,800 --> 00:10:54,649
is it really to create these and

247
00:10:54,649 --> 00:10:57,089
surprisingly despite having apps

248
00:10:57,089 --> 00:10:59,430
everything else that will create them it

249
00:10:59,430 --> 00:11:01,759
can be difficult to create them and

250
00:11:01,759 --> 00:11:06,269
especially from scratch but it's getting

251
00:11:06,269 --> 00:11:08,579
better so you know some of the thing

252
00:11:08,579 --> 00:11:11,220
that are still very apparent in most of

253
00:11:11,220 --> 00:11:12,899
these deep fakes

254
00:11:12,899 --> 00:11:16,829
there are definitely visual artifacts

255
00:11:16,829 --> 00:11:19,319
that make it very clear that it's deep

256
00:11:19,319 --> 00:11:21,809
fake there are issues of resolution

257
00:11:21,809 --> 00:11:25,199
issues of coloration things like that

258
00:11:25,199 --> 00:11:27,839
where you know this masked in phase

259
00:11:27,839 --> 00:11:30,449
transitions to the rest of the body and

260
00:11:30,449 --> 00:11:35,249
face behind it you know that become

261
00:11:35,249 --> 00:11:39,600
problematic and you can identify one of

262
00:11:39,600 --> 00:11:43,410
the other things that oh shoot and I'm

263
00:11:43,410 --> 00:11:46,139
gonna forget which school it is now but

264
00:11:46,139 --> 00:11:47,639
going beyond just looking for different

265
00:11:47,639 --> 00:11:49,610
artifacts and different things like that

266
00:11:49,610 --> 00:11:51,869
there is research out there it was

267
00:11:51,869 --> 00:11:53,730
released last year and it's really

268
00:11:53,730 --> 00:11:55,290
driving me nuts I can't remember who the

269
00:11:55,290 --> 00:11:56,759
the school is next I'd love to give them

270
00:11:56,759 --> 00:11:58,019
credit for this it's actually two

271
00:11:58,019 --> 00:12:00,989
schools if I remember right but what

272
00:12:00,989 --> 00:12:02,220
they're looking at is behavioral

273
00:12:02,220 --> 00:12:05,220
analytics so take you know a video of

274
00:12:05,220 --> 00:12:08,579
Barack Obama services and he's talking

275
00:12:08,579 --> 00:12:10,649
and and that this is one of the examples

276
00:12:10,649 --> 00:12:14,399
they used in their training was you know

277
00:12:14,399 --> 00:12:17,579
when he's delivering a message that's

278
00:12:17,579 --> 00:12:21,269
very you know I guess more tender and

279
00:12:21,269 --> 00:12:24,389
empathetic you know his eyebrows are low

280
00:12:24,389 --> 00:12:27,720
you know the just the natural motions of

281
00:12:27,720 --> 00:12:29,309
his face and everything are very

282
00:12:29,309 --> 00:12:30,899
different than like for instance they

283
00:12:30,899 --> 00:12:32,610
compared that to a video where he was

284
00:12:32,610 --> 00:12:35,189
delivering a stern warning to North

285
00:12:35,189 --> 00:12:37,860
Korea and suddenly you know is he's got

286
00:12:37,860 --> 00:12:39,569
the furrowed eyebrows he's got it

287
00:12:39,569 --> 00:12:41,850
there's a little twitch that he has

288
00:12:41,850 --> 00:12:45,209
things like that and so what they were

289
00:12:45,209 --> 00:12:47,449
reporting as of last year was 95%

290
00:12:47,449 --> 00:12:49,799
accuracy and being able to identify deep

291
00:12:49,799 --> 00:12:53,369
fakes and they expected that to be 99

292
00:12:53,369 --> 00:12:55,980
percent by about this time this year I

293
00:12:55,980 --> 00:12:58,949
haven't seen an update on that yet but

294
00:12:58,949 --> 00:13:03,959
that's encouraging um but on the flip

295
00:13:03,959 --> 00:13:05,579
side this stuff's getting better and

296
00:13:05,579 --> 00:13:08,519
better I mean yeah there's an app out

297
00:13:08,519 --> 00:13:11,639
there now called impressions that you

298
00:13:11,639 --> 00:13:15,899
can download on your iPhone iPad you

299
00:13:15,899 --> 00:13:17,699
record a video of yourself and you can

300
00:13:17,699 --> 00:13:20,069
choose what celebrity you want to place

301
00:13:20,069 --> 00:13:22,110
on top of your face

302
00:13:22,110 --> 00:13:24,690
and in less than five minutes after you

303
00:13:24,690 --> 00:13:26,820
record this 10 to 20 second video

304
00:13:26,820 --> 00:13:28,019
depending on if you're paying for it or

305
00:13:28,019 --> 00:13:30,120
not less than five minutes they send you

306
00:13:30,120 --> 00:13:34,040
back a deep fake of you as that person

307
00:13:34,040 --> 00:13:36,660
that's impressive given how quickly they

308
00:13:36,660 --> 00:13:40,110
do it on the flip side there's

309
00:13:40,110 --> 00:13:41,820
definitely visual artifacts that make it

310
00:13:41,820 --> 00:13:45,120
obvious that it's still deep fade now if

311
00:13:45,120 --> 00:13:46,890
we really wanted to make this fun we

312
00:13:46,890 --> 00:13:48,829
would have taken impressions and like

313
00:13:48,829 --> 00:13:51,269
started this whole panel with our

314
00:13:51,269 --> 00:13:58,440
favorite celebrities so off to do with

315
00:13:58,440 --> 00:14:00,720
impressions because it is a it's a you

316
00:14:00,720 --> 00:14:02,490
know it's not real time but that bring

317
00:14:02,490 --> 00:14:05,100
like no point wouldn't that be fun like

318
00:14:05,100 --> 00:14:07,260
maybe in a conference of the future we

319
00:14:07,260 --> 00:14:10,970
can go dressed as our person of choice

320
00:14:10,970 --> 00:14:14,040
you don't have to wear our own faces so

321
00:14:14,040 --> 00:14:16,680
Lisa's gonna say something here well I

322
00:14:16,680 --> 00:14:18,510
would say that I think you're going to

323
00:14:18,510 --> 00:14:21,390
see just like you do in all other forms

324
00:14:21,390 --> 00:14:23,120
of bad actors you're going to see a

325
00:14:23,120 --> 00:14:25,440
stratification of sophistication right

326
00:14:25,440 --> 00:14:28,199
so there'll be the crappy free apps with

327
00:14:28,199 --> 00:14:30,209
the kid just starting out trying to

328
00:14:30,209 --> 00:14:31,980
figure it figure out how to do it

329
00:14:31,980 --> 00:14:35,279
that'll be bad and then you'll get up to

330
00:14:35,279 --> 00:14:37,980
you know the state actor or high-level

331
00:14:37,980 --> 00:14:41,850
cyber crime group and they're gonna have

332
00:14:41,850 --> 00:14:46,790
access to CGI access to Hollywood level

333
00:14:46,790 --> 00:14:49,079
manipulations and those are going to be

334
00:14:49,079 --> 00:14:51,959
a lot harder to to distinguish and

335
00:14:51,959 --> 00:14:54,959
defined so I will say there is real-time

336
00:14:54,959 --> 00:14:57,779
capability now hmm and that's what's a

337
00:14:57,779 --> 00:15:01,290
little scary there is a plug-in for

338
00:15:01,290 --> 00:15:03,800
video conferencing tools like

339
00:15:03,800 --> 00:15:06,000
everybody's favorite lately right under

340
00:15:06,000 --> 00:15:09,209
kovat has been zoom that do allow you to

341
00:15:09,209 --> 00:15:14,399
take a single image of a person and make

342
00:15:14,399 --> 00:15:17,010
that what's displayed and then that

343
00:15:17,010 --> 00:15:20,459
image gets manipulated based on what

344
00:15:20,459 --> 00:15:23,970
you're doing as you're talking so that's

345
00:15:23,970 --> 00:15:25,199
kind of an interesting what's called

346
00:15:25,199 --> 00:15:29,130
Avatar Fi and they're their famous video

347
00:15:29,130 --> 00:15:31,190
is that one of the guys who worked on it

348
00:15:31,190 --> 00:15:34,770
used it with a colleague of his he set

349
00:15:34,770 --> 00:15:35,490
up the zoom me

350
00:15:35,490 --> 00:15:37,980
had a third colleague join as Elon Musk

351
00:15:37,980 --> 00:15:40,770
and this other colleague didn't she

352
00:15:40,770 --> 00:15:43,080
didn't know that you know about this

353
00:15:43,080 --> 00:15:45,510
plug-in whatever and it's but there

354
00:15:45,510 --> 00:15:46,890
again there's weaknesses like I've

355
00:15:46,890 --> 00:15:48,450
watched different videos and one of the

356
00:15:48,450 --> 00:15:50,279
things you notice is it it very much

357
00:15:50,279 --> 00:15:51,630
looks like a still image that's more

358
00:15:51,630 --> 00:15:54,270
faint like if the head turns too far you

359
00:15:54,270 --> 00:15:56,820
see the background kind of get you know

360
00:15:56,820 --> 00:15:58,830
warped with it because it's basically

361
00:15:58,830 --> 00:16:00,450
just taking this still image and moving

362
00:16:00,450 --> 00:16:03,050
it but what's impressive about is

363
00:16:03,050 --> 00:16:04,709
similar to what's happening with

364
00:16:04,709 --> 00:16:08,370
impressions app is that it's real-time

365
00:16:08,370 --> 00:16:12,240
it Maps your your face landmarks the you

366
00:16:12,240 --> 00:16:14,850
know typical 68 landmarks of facial

367
00:16:14,850 --> 00:16:17,040
recognition and it's able to apply that

368
00:16:17,040 --> 00:16:22,560
and morph the image in in real time so

369
00:16:22,560 --> 00:16:26,970
oh go ahead I have a question and and

370
00:16:26,970 --> 00:16:29,580
you know I'm just with the way my mind

371
00:16:29,580 --> 00:16:30,350
works

372
00:16:30,350 --> 00:16:33,660
asking myself you know what's gonna

373
00:16:33,660 --> 00:16:36,180
happen when this evolved a little bit

374
00:16:36,180 --> 00:16:39,480
further and I just read this fantastic

375
00:16:39,480 --> 00:16:41,390
paper and tying it back to the health

376
00:16:41,390 --> 00:16:43,350
you know thread that we were talking

377
00:16:43,350 --> 00:16:45,450
about a little bit earlier there are

378
00:16:45,450 --> 00:16:48,230
these these really complex

379
00:16:48,230 --> 00:16:51,209
disinformation networks that are now

380
00:16:51,209 --> 00:16:53,300
being mapped on Twitter and Facebook

381
00:16:53,300 --> 00:16:59,250
where we see the the advanced a you know

382
00:16:59,250 --> 00:17:03,029
power of taking fake information where

383
00:17:03,029 --> 00:17:05,670
if you don't know a social media profile

384
00:17:05,670 --> 00:17:09,240
is real or not for somebody like this

385
00:17:09,240 --> 00:17:11,609
panel to understand oh that's definitely

386
00:17:11,609 --> 00:17:14,699
a bot versus that's not I think the

387
00:17:14,699 --> 00:17:18,059
level of savviness of a regular person

388
00:17:18,059 --> 00:17:20,160
not knowing what's real in fake that's

389
00:17:20,160 --> 00:17:22,189
where it gets really scary to me because

390
00:17:22,189 --> 00:17:25,890
let's say for example we see protests

391
00:17:25,890 --> 00:17:29,880
out there in every city right now what

392
00:17:29,880 --> 00:17:33,030
happens when there is a deep fake of you

393
00:17:33,030 --> 00:17:36,780
know a public official or you know some

394
00:17:36,780 --> 00:17:39,720
rumor that is being spread in a way that

395
00:17:39,720 --> 00:17:42,390
could incite heart those are the

396
00:17:42,390 --> 00:17:45,120
questions I have when people are already

397
00:17:45,120 --> 00:17:48,790
not sure of who and what to trust

398
00:17:48,790 --> 00:17:52,420
when they go on social media you know

399
00:17:52,420 --> 00:17:54,370
how quickly those things could spread or

400
00:17:54,370 --> 00:17:57,880
how how that could be used in a way that

401
00:17:57,880 --> 00:18:04,000
causes confusion at a rapid pace where

402
00:18:04,000 --> 00:18:07,360
you don't have the experts involved so

403
00:18:07,360 --> 00:18:09,010
Jim I think you've actually got some

404
00:18:09,010 --> 00:18:12,340
opinions on this just in terms of and

405
00:18:12,340 --> 00:18:14,230
I'll actually tie it to a question we

406
00:18:14,230 --> 00:18:15,700
have waiting in the chat - somebody's

407
00:18:15,700 --> 00:18:17,560
asking who's winning deep fakes or

408
00:18:17,560 --> 00:18:21,790
debunkers well Mark Twain famously said

409
00:18:21,790 --> 00:18:23,590
that a lie will get halfway around the

410
00:18:23,590 --> 00:18:25,450
world before the truth can get its boots

411
00:18:25,450 --> 00:18:26,980
on and he said that one hundred and

412
00:18:26,980 --> 00:18:28,930
thirtysomething years ago and not much

413
00:18:28,930 --> 00:18:32,230
has changed for all the reasons Andrea

414
00:18:32,230 --> 00:18:34,150
just stated which I wholeheartedly agree

415
00:18:34,150 --> 00:18:36,490
with I am not overly concerned about

416
00:18:36,490 --> 00:18:40,630
deep fakes because you can I we've all

417
00:18:40,630 --> 00:18:44,320
seen the viral bad tweet containing

418
00:18:44,320 --> 00:18:45,490
false information with a hundred

419
00:18:45,490 --> 00:18:46,900
thousand retweets and then the

420
00:18:46,900 --> 00:18:48,280
correction that had like five hundred

421
00:18:48,280 --> 00:18:51,130
retweets because that's how the world

422
00:18:51,130 --> 00:18:53,110
works do I need a deep fake to do that

423
00:18:53,110 --> 00:18:55,480
do I need a do I need a deep neural

424
00:18:55,480 --> 00:18:57,490
network with a fake video to do that I

425
00:18:57,490 --> 00:19:00,940
mean like Fox News Fox News is less

426
00:19:00,940 --> 00:19:03,010
reputable or propaganda arm published a

427
00:19:03,010 --> 00:19:06,070
photo two days ago photoshopping a gun

428
00:19:06,070 --> 00:19:09,550
into a peaceful protester in Seattle

429
00:19:09,550 --> 00:19:14,440
because they hate the protesters we

430
00:19:14,440 --> 00:19:16,240
don't need deep fakes to do that now

431
00:19:16,240 --> 00:19:17,800
people believe what they want to believe

432
00:19:17,800 --> 00:19:19,060
and we live in this increasingly

433
00:19:19,060 --> 00:19:21,340
polarized news world where people

434
00:19:21,340 --> 00:19:22,660
believe what they want to believe

435
00:19:22,660 --> 00:19:25,900
confirmation bias rules and no one wants

436
00:19:25,900 --> 00:19:28,930
to have their mind changed so and that's

437
00:19:28,930 --> 00:19:31,270
bad that's super bad that's bad that for

438
00:19:31,270 --> 00:19:32,950
democracy that's bad for everybody but

439
00:19:32,950 --> 00:19:35,050
do we need deep fakes to get that level

440
00:19:35,050 --> 00:19:38,890
of success I mean I see people being

441
00:19:38,890 --> 00:19:41,110
successful with you dr. Jim Acosta video

442
00:19:41,110 --> 00:19:43,860
doctor doctor video of Nancy Pelosi

443
00:19:43,860 --> 00:19:47,800
doctored Photoshop like a 12 year old 10

444
00:19:47,800 --> 00:19:50,230
year old could do it Photoshop picture

445
00:19:50,230 --> 00:19:54,070
of a gun with like bad Photoshop two

446
00:19:54,070 --> 00:19:55,840
days ago you know you don't need deep

447
00:19:55,840 --> 00:19:58,180
fakes for that now I mean the root cause

448
00:19:58,180 --> 00:20:00,580
here is is not technology the root cause

449
00:20:00,580 --> 00:20:02,320
here is deep

450
00:20:02,320 --> 00:20:05,620
in our society in terms of of education

451
00:20:05,620 --> 00:20:08,490
and politics and public discourse and

452
00:20:08,490 --> 00:20:12,070
deep deep fakes are just like the the

453
00:20:12,070 --> 00:20:16,149
the tiniest symptom of a deep savage

454
00:20:16,149 --> 00:20:19,600
malaise in our current society I that's

455
00:20:19,600 --> 00:20:21,279
not an easy answer but I'm afraid it's

456
00:20:21,279 --> 00:20:22,360
the one I think it's true

457
00:20:22,360 --> 00:20:25,450
no and I agree with you in fact it's

458
00:20:25,450 --> 00:20:27,130
something that I bring up pretty

459
00:20:27,130 --> 00:20:29,169
regularly when I talk about this is that

460
00:20:29,169 --> 00:20:32,019
you know deep fakes ultimately as far as

461
00:20:32,019 --> 00:20:34,269
a threat they're misinformation or just

462
00:20:34,269 --> 00:20:35,559
information whichever you want to call

463
00:20:35,559 --> 00:20:38,110
it and Andrew you were spot-on when you

464
00:20:38,110 --> 00:20:40,600
said that and the fact of the matter is

465
00:20:40,600 --> 00:20:42,220
all these technological advances and

466
00:20:42,220 --> 00:20:43,330
detecting them and everything else are

467
00:20:43,330 --> 00:20:45,250
great but those are all technological

468
00:20:45,250 --> 00:20:46,929
solutions to something it's not a

469
00:20:46,929 --> 00:20:48,850
technological problem as you said what I

470
00:20:48,850 --> 00:20:49,870
tell people all the time is

471
00:20:49,870 --> 00:20:52,210
misinformation is a human problem and it

472
00:20:52,210 --> 00:20:55,179
requires human solutions it's not

473
00:20:55,179 --> 00:20:57,639
something we're gonna fix with the fakes

474
00:20:57,639 --> 00:21:00,149
yes that it could be helpful you know

475
00:21:00,149 --> 00:21:03,009
Microsoft and Facebook and Amazon and a

476
00:21:03,009 --> 00:21:04,149
few others have sponsored this

477
00:21:04,149 --> 00:21:07,059
million-dollar challenge for researchers

478
00:21:07,059 --> 00:21:09,129
to come up with detection techniques

479
00:21:09,129 --> 00:21:12,730
that they can incorporate into you know

480
00:21:12,730 --> 00:21:16,509
their platforms great that's wonderful

481
00:21:16,509 --> 00:21:18,100
that would be wonderful that if feel

482
00:21:18,100 --> 00:21:19,570
these deep face starts showing up in

483
00:21:19,570 --> 00:21:21,220
Facebook that they're immediately tagged

484
00:21:21,220 --> 00:21:22,840
as this is a deep fake because it does

485
00:21:22,840 --> 00:21:26,080
help if you can hit it right away and

486
00:21:26,080 --> 00:21:29,740
prevent you know that immediate spread

487
00:21:29,740 --> 00:21:30,909
like you were saying but yes

488
00:21:30,909 --> 00:21:33,789
unfortunately what happens today is that

489
00:21:33,789 --> 00:21:35,830
spread happens and by the time you can

490
00:21:35,830 --> 00:21:38,620
get a real story out there you know that

491
00:21:38,620 --> 00:21:41,379
misinformation it's basically propaganda

492
00:21:41,379 --> 00:21:42,909
like you said it plays on people's

493
00:21:42,909 --> 00:21:45,220
existing prejudices and now it's just

494
00:21:45,220 --> 00:21:47,200
buried in their minds and trying to

495
00:21:47,200 --> 00:21:50,399
dislodge that with the truth is

496
00:21:50,399 --> 00:21:55,539
exceptionally difficult yeah and so yeah

497
00:21:55,539 --> 00:21:57,340
I you know it's it's one of those things

498
00:21:57,340 --> 00:22:01,899
I think I honestly uh I'll be honest I

499
00:22:01,899 --> 00:22:04,629
everybody's blown up about the political

500
00:22:04,629 --> 00:22:06,730
ramifications of this right everybody

501
00:22:06,730 --> 00:22:08,169
talks about how it can affect an

502
00:22:08,169 --> 00:22:10,960
election and sure yeah it could you know

503
00:22:10,960 --> 00:22:14,980
we talked about how it could impact you

504
00:22:14,980 --> 00:22:16,220
know amiri

505
00:22:16,220 --> 00:22:17,630
of all the things but where I'm more

506
00:22:17,630 --> 00:22:19,640
worried about it is more in the private

507
00:22:19,640 --> 00:22:21,230
space as this stuff becomes more

508
00:22:21,230 --> 00:22:23,870
consumer eyes I mean weird effects

509
00:22:23,870 --> 00:22:25,250
started they started with deep faked

510
00:22:25,250 --> 00:22:28,400
porn right yeah actual abuse actual

511
00:22:28,400 --> 00:22:31,490
victims right now today yeah so I work

512
00:22:31,490 --> 00:22:34,100
all right I don't work I have a

513
00:22:34,100 --> 00:22:38,480
colleague who runs a an organization

514
00:22:38,480 --> 00:22:41,809
focused on revenge part and you know

515
00:22:41,809 --> 00:22:44,059
that whole concept now when you start to

516
00:22:44,059 --> 00:22:46,610
stir in okay that happens today because

517
00:22:46,610 --> 00:22:49,429
you know someone texts a naked picture

518
00:22:49,429 --> 00:22:51,860
to their partner or they they allow

519
00:22:51,860 --> 00:22:53,450
their partner to make video of them and

520
00:22:53,450 --> 00:22:55,909
then when they know things go self that

521
00:22:55,909 --> 00:22:57,470
partner shares it out to the world as

522
00:22:57,470 --> 00:22:59,690
revenge but now you have that situation

523
00:22:59,690 --> 00:23:01,520
where that video that picture doesn't

524
00:23:01,520 --> 00:23:03,770
even have to exist their partner can

525
00:23:03,770 --> 00:23:05,570
create a fake one and even if everybody

526
00:23:05,570 --> 00:23:07,309
knows it's fake it doesn't matter

527
00:23:07,309 --> 00:23:10,760
because I mean what that does to a human

528
00:23:10,760 --> 00:23:13,520
being when your face is being purported

529
00:23:13,520 --> 00:23:15,590
to be out there you know in this video

530
00:23:15,590 --> 00:23:18,309
you're you're naked and doing whatever

531
00:23:18,309 --> 00:23:20,870
that's powerful stuff and that that's

532
00:23:20,870 --> 00:23:23,059
honestly where I have probably a bigger

533
00:23:23,059 --> 00:23:26,390
concern that's also where it's promising

534
00:23:26,390 --> 00:23:27,770
that at least right now the technology

535
00:23:27,770 --> 00:23:32,030
isn't there to do that as easily as one

536
00:23:32,030 --> 00:23:35,720
might hope what does we said it keeps

537
00:23:35,720 --> 00:23:38,240
getting better I mean this I think

538
00:23:38,240 --> 00:23:41,419
raises all the questions about human

539
00:23:41,419 --> 00:23:44,600
rights and you know what are what are

540
00:23:44,600 --> 00:23:46,820
the next sets of human rights that we

541
00:23:46,820 --> 00:23:49,750
need as these technologies develop and

542
00:23:49,750 --> 00:23:53,570
you know I I would be curious what Lisa

543
00:23:53,570 --> 00:23:58,280
has to think about this I have I have

544
00:23:58,280 --> 00:24:00,020
you know thinking about health data

545
00:24:00,020 --> 00:24:02,900
sharing for the BRCA community and you

546
00:24:02,900 --> 00:24:04,700
know coming from a place where our genes

547
00:24:04,700 --> 00:24:08,900
were patented back in 2013 started to

548
00:24:08,900 --> 00:24:11,900
understand from experts how access to

549
00:24:11,900 --> 00:24:14,059
knowledge about our health or our bodies

550
00:24:14,059 --> 00:24:16,909
is a human right and I'm what beginning

551
00:24:16,909 --> 00:24:18,890
to wonder will manipulate can

552
00:24:18,890 --> 00:24:23,929
manipulation of data and images from our

553
00:24:23,929 --> 00:24:25,880
body also be considered some sort of

554
00:24:25,880 --> 00:24:29,900
right that we need to develop or

555
00:24:29,900 --> 00:24:33,710
yet I know for a fact because I have you

556
00:24:33,710 --> 00:24:36,350
know fought Facebook and and gone all

557
00:24:36,350 --> 00:24:37,790
the way to the FCC and there was a

558
00:24:37,790 --> 00:24:39,640
congressional inquiry there's no

559
00:24:39,640 --> 00:24:42,200
comprehensive legislation coming down

560
00:24:42,200 --> 00:24:44,420
the pipes for this any time soon it's

561
00:24:44,420 --> 00:24:46,190
gonna be the Wild West for a long time

562
00:24:46,190 --> 00:24:48,920
but what would the what were the rights

563
00:24:48,920 --> 00:24:53,540
look like what should they look like so

564
00:24:53,540 --> 00:24:55,670
actually spent a lot of time writing

565
00:24:55,670 --> 00:24:59,870
about this so if you look at the

566
00:24:59,870 --> 00:25:03,350
original premise of privacy law it was

567
00:25:03,350 --> 00:25:07,480
based on primarily white cisgendered men

568
00:25:07,480 --> 00:25:11,600
and it was the right it was the idea

569
00:25:11,600 --> 00:25:15,020
that your home is your castle and within

570
00:25:15,020 --> 00:25:19,250
your home as a as an white cisgendered

571
00:25:19,250 --> 00:25:22,910
land owning mail you have the you have

572
00:25:22,910 --> 00:25:24,740
the right to privacy within those four

573
00:25:24,740 --> 00:25:28,540
walls so and that that was back when

574
00:25:28,540 --> 00:25:32,330
women as wives were property children

575
00:25:32,330 --> 00:25:36,520
and farm animals were essentially equal

576
00:25:36,520 --> 00:25:41,750
it goes back to that original premise if

577
00:25:41,750 --> 00:25:43,990
you take that and evolve that now into

578
00:25:43,990 --> 00:25:46,430
technology and the way that it's entered

579
00:25:46,430 --> 00:25:49,550
our lives you know we're all walking

580
00:25:49,550 --> 00:25:54,340
around with with cameras in our house

581
00:25:54,340 --> 00:25:56,270
photographing things sharing things as

582
00:25:56,270 --> 00:25:59,840
we go so does the parameters of having

583
00:25:59,840 --> 00:26:03,380
four walls make sense anymore and I

584
00:26:03,380 --> 00:26:05,960
would posit that it does not and that

585
00:26:05,960 --> 00:26:08,470
what we need to think about is our

586
00:26:08,470 --> 00:26:12,140
stories that make us unique and that

587
00:26:12,140 --> 00:26:15,680
make up the facets of ourselves so like

588
00:26:15,680 --> 00:26:19,250
those 68 data points on your face we

589
00:26:19,250 --> 00:26:22,010
would have tens of thousands hundreds of

590
00:26:22,010 --> 00:26:24,560
thousands of data points that belong to

591
00:26:24,560 --> 00:26:27,650
us and they're ours and we have the

592
00:26:27,650 --> 00:26:30,020
right to share them or not share them as

593
00:26:30,020 --> 00:26:34,340
we see fit so I would evolve the concept

594
00:26:34,340 --> 00:26:37,760
of a man in this castle into we are the

595
00:26:37,760 --> 00:26:39,650
keepers of our story and how we share

596
00:26:39,650 --> 00:26:42,179
that is up top

597
00:26:42,179 --> 00:26:43,679
and that's a really good point and it

598
00:26:43,679 --> 00:26:44,879
brings up an interesting point that

599
00:26:44,879 --> 00:26:46,860
actually I love the way this is flowing

600
00:26:46,860 --> 00:26:48,570
because we've got a question in the chat

601
00:26:48,570 --> 00:26:51,210
right now asking about where do we stand

602
00:26:51,210 --> 00:26:54,269
on legislation of owning your own

603
00:26:54,269 --> 00:26:57,080
likeness and honestly it's a minefield

604
00:26:57,080 --> 00:26:59,159
because there is no legislation

605
00:26:59,159 --> 00:27:03,330
currently and defects really gray a lot

606
00:27:03,330 --> 00:27:06,149
of areas the only legislation to date

607
00:27:06,149 --> 00:27:08,100
that has even been proposed and passed

608
00:27:08,100 --> 00:27:11,159
is specifically around election security

609
00:27:11,159 --> 00:27:14,730
and it's woefully inadequate it was

610
00:27:14,730 --> 00:27:16,860
basically just to create monitoring for

611
00:27:16,860 --> 00:27:19,649
this but you know if I go back to my

612
00:27:19,649 --> 00:27:22,139
example of like the revenge porn one of

613
00:27:22,139 --> 00:27:24,899
the common ways to combat that is the

614
00:27:24,899 --> 00:27:27,360
use of the let's see if I get this right

615
00:27:27,360 --> 00:27:29,909
the DMCA yes I had to think about it I

616
00:27:29,909 --> 00:27:33,779
always get it wrong the DMCA has you

617
00:27:33,779 --> 00:27:36,419
know specific requirements for taking

618
00:27:36,419 --> 00:27:38,629
down material when it's proven that it's

619
00:27:38,629 --> 00:27:42,980
copyright of somebody else who hasn't

620
00:27:42,980 --> 00:27:45,600
consented to that being posted so you

621
00:27:45,600 --> 00:27:47,129
can do that because there's an implicit

622
00:27:47,129 --> 00:27:49,529
copyright if I take a picture of myself

623
00:27:49,529 --> 00:27:51,749
and I send it to you or if I have appear

624
00:27:51,749 --> 00:27:53,759
in a video and I used to you know I send

625
00:27:53,759 --> 00:27:56,820
it to you we can you know these

626
00:27:56,820 --> 00:27:58,710
organizations use those make down

627
00:27:58,710 --> 00:28:01,259
requests to get rid of that video but

628
00:28:01,259 --> 00:28:02,789
now you could be talking about a video

629
00:28:02,789 --> 00:28:05,669
that the victim doesn't own and it's

630
00:28:05,669 --> 00:28:08,309
just their face and so there's this

631
00:28:08,309 --> 00:28:12,600
really tough gray area of can you even

632
00:28:12,600 --> 00:28:19,590
use a DMC or a DCM jeez I can't because

633
00:28:19,590 --> 00:28:21,690
if you can see a take down to take those

634
00:28:21,690 --> 00:28:23,789
off because they're not really the

635
00:28:23,789 --> 00:28:26,210
copyright owner that's complaining and

636
00:28:26,210 --> 00:28:29,249
there's also the question of what's the

637
00:28:29,249 --> 00:28:33,299
border between satire and libelous

638
00:28:33,299 --> 00:28:36,090
behavior and we've been fighting that

639
00:28:36,090 --> 00:28:39,029
I'm Jam I'm sure you know you at least

640
00:28:39,029 --> 00:28:41,490
are probably both having worked in more

641
00:28:41,490 --> 00:28:42,960
of a journalistic space in particular

642
00:28:42,960 --> 00:28:44,399
you're very familiar with that struggle

643
00:28:44,399 --> 00:28:48,570
all the time um so yeah I mean there's

644
00:28:48,570 --> 00:28:51,629
challenges there well I mean I'll leave

645
00:28:51,629 --> 00:28:53,940
the the satire question even though I'm

646
00:28:53,940 --> 00:28:56,179
tempted but what's really on my mine

647
00:28:56,179 --> 00:28:58,309
right now and this is a debate that's

648
00:28:58,309 --> 00:29:00,679
raging among journalists mostly me

649
00:29:00,679 --> 00:29:03,529
raging I have to be honest but you know

650
00:29:03,529 --> 00:29:04,700
so we have these these protests

651
00:29:04,700 --> 00:29:06,230
happening and for me as a cybersecurity

652
00:29:06,230 --> 00:29:08,360
professional as an engineer and a

653
00:29:08,360 --> 00:29:10,519
reporter you know taking my picture is

654
00:29:10,519 --> 00:29:12,860
is to plunder my biometrics without my

655
00:29:12,860 --> 00:29:16,009
consent you know you you are robbing me

656
00:29:16,009 --> 00:29:17,990
of some that is mine if I don't give my

657
00:29:17,990 --> 00:29:19,759
consent you are taking it from me by

658
00:29:19,759 --> 00:29:22,700
force without asking and if you want to

659
00:29:22,700 --> 00:29:25,700
hear in a public face this is exactly

660
00:29:25,700 --> 00:29:28,009
what comes next because what does it

661
00:29:28,009 --> 00:29:30,649
mean to be private in public private in

662
00:29:30,649 --> 00:29:33,230
public before ubiquitous cameras and

663
00:29:33,230 --> 00:29:35,450
facial recognition was the fleeting

664
00:29:35,450 --> 00:29:37,940
memory in somebody else's brain of your

665
00:29:37,940 --> 00:29:40,309
presence and activity now we have

666
00:29:40,309 --> 00:29:42,529
always-on cameras recording everything

667
00:29:42,529 --> 00:29:44,600
you do with facial recognition that can

668
00:29:44,600 --> 00:29:47,480
correlate your entire history to law

669
00:29:47,480 --> 00:29:49,759
enforcement both good and bad as we see

670
00:29:49,759 --> 00:29:51,799
happening in China which really raises

671
00:29:51,799 --> 00:29:55,009
the question what does privacy and

672
00:29:55,009 --> 00:29:58,249
public mean if we want to have any kind

673
00:29:58,249 --> 00:30:00,559
of democracy because journalists are

674
00:30:00,559 --> 00:30:02,240
telling me people who are going to film

675
00:30:02,240 --> 00:30:04,850
protesters I'm telling them you are

676
00:30:04,850 --> 00:30:07,369
serving as a de-facto branch of law

677
00:30:07,369 --> 00:30:10,610
enforcement as a snitch turning in

678
00:30:10,610 --> 00:30:13,519
nonviolent protesters to racist thug who

679
00:30:13,519 --> 00:30:14,779
are going to come to your house and

680
00:30:14,779 --> 00:30:17,119
harass you and kill you and you are

681
00:30:17,119 --> 00:30:19,220
aiding and abetting these cops by doing

682
00:30:19,220 --> 00:30:21,169
so and they're like look these people

683
00:30:21,169 --> 00:30:23,450
are in public how am I supposed to do my

684
00:30:23,450 --> 00:30:26,119
job as a journalist if I can't film

685
00:30:26,119 --> 00:30:28,399
crowds and we're both right I don't have

686
00:30:28,399 --> 00:30:30,320
an easy answer because there are equal

687
00:30:30,320 --> 00:30:32,929
competing concerns here there is an

688
00:30:32,929 --> 00:30:35,210
there is a valid newsgathering concern

689
00:30:35,210 --> 00:30:37,340
to film public gatherings that are

690
00:30:37,340 --> 00:30:39,320
newsworthy there is a valid public

691
00:30:39,320 --> 00:30:41,990
concern that we not be filmed because

692
00:30:41,990 --> 00:30:44,899
you know bad law enforcement will hurt

693
00:30:44,899 --> 00:30:47,509
people there is no easy and obvious

694
00:30:47,509 --> 00:30:50,720
answer here and I don't have a solution

695
00:30:50,720 --> 00:30:52,879
but we're reaching a threshold in

696
00:30:52,879 --> 00:30:54,559
society with facial recognition deep

697
00:30:54,559 --> 00:30:57,559
fakes were the norms and the laws may

698
00:30:57,559 --> 00:31:01,330
need to change to address these issues

699
00:31:01,669 --> 00:31:04,609
I would actually posit that we've been

700
00:31:04,609 --> 00:31:07,879
collecting on facial data on people

701
00:31:07,879 --> 00:31:08,990
since we had

702
00:31:08,990 --> 00:31:11,000
decent photography and the FBI figured

703
00:31:11,000 --> 00:31:13,309
out that they could have the wrong

704
00:31:13,309 --> 00:31:17,450
whichever agency it can collect and use

705
00:31:17,450 --> 00:31:20,270
that information it's definitely more

706
00:31:20,270 --> 00:31:23,000
sophisticated now it's always always

707
00:31:23,000 --> 00:31:24,529
with technology it's a measure of

708
00:31:24,529 --> 00:31:28,190
degrees right so we could take you know

709
00:31:28,190 --> 00:31:31,190
1964 protest marches and pictures from

710
00:31:31,190 --> 00:31:33,110
them and they were definitely used and

711
00:31:33,110 --> 00:31:35,600
implemented at the time by various

712
00:31:35,600 --> 00:31:37,429
three-letter agencies to go after people

713
00:31:37,429 --> 00:31:39,679
that they shouldn't have gone after now

714
00:31:39,679 --> 00:31:42,529
you can just scrape the local traffic

715
00:31:42,529 --> 00:31:44,480
camera dump it right into the database

716
00:31:44,480 --> 00:31:47,149
and you do the work immediately so to me

717
00:31:47,149 --> 00:31:49,820
it's more a matter of degrees and I

718
00:31:49,820 --> 00:31:51,380
think the way that you come back that is

719
00:31:51,380 --> 00:31:56,240
with education if I have an I work in IT

720
00:31:56,240 --> 00:31:58,309
risk now and you're always looking to

721
00:31:58,309 --> 00:32:00,710
mitigate the risk and the easiest

722
00:32:00,710 --> 00:32:02,299
cheapest way to do that is through

723
00:32:02,299 --> 00:32:05,059
education so letting people know you

724
00:32:05,059 --> 00:32:06,950
know you are making this choice by going

725
00:32:06,950 --> 00:32:08,779
to this rally you were making this

726
00:32:08,779 --> 00:32:10,760
choice by holding the sign up in front

727
00:32:10,760 --> 00:32:13,250
of a camera and I think you thought

728
00:32:13,250 --> 00:32:15,140
people making those choices when I was

729
00:32:15,140 --> 00:32:18,409
watching things happen in Minneapolis

730
00:32:18,409 --> 00:32:20,630
you could see people choosing to stand

731
00:32:20,630 --> 00:32:23,179
behind the cameraman and holding up a

732
00:32:23,179 --> 00:32:25,250
sign or waving to their mom I saw some

733
00:32:25,250 --> 00:32:26,419
fellow moms which I thought was

734
00:32:26,419 --> 00:32:30,200
hilarious or or you could see the camera

735
00:32:30,200 --> 00:32:31,700
panning in their direction and you saw

736
00:32:31,700 --> 00:32:35,360
them back up so so I think it's a matter

737
00:32:35,360 --> 00:32:38,090
of Education and choice but if you're in

738
00:32:38,090 --> 00:32:41,120
the public your face is there to be used

739
00:32:41,120 --> 00:32:43,070
however anyone sees fit

740
00:32:43,070 --> 00:32:46,010
yeah that's got to change strong

741
00:32:46,010 --> 00:32:47,960
disagree there that's gotta change look

742
00:32:47,960 --> 00:32:50,330
at what's happening in China you know

743
00:32:50,330 --> 00:32:52,279
what happens when we have nano cameras

744
00:32:52,279 --> 00:32:56,779
everywhere but you're at I would say you

745
00:32:56,779 --> 00:32:58,760
being in public is a choice that you're

746
00:32:58,760 --> 00:33:02,270
making how the technology is being used

747
00:33:02,270 --> 00:33:06,020
behind you is a different issue I don't

748
00:33:06,020 --> 00:33:07,700
have a choice to stay home I have to go

749
00:33:07,700 --> 00:33:09,470
to work I have to go shopping I got to

750
00:33:09,470 --> 00:33:11,240
see people like that is not like a

751
00:33:11,240 --> 00:33:14,450
choice I have to go outside that that's

752
00:33:14,450 --> 00:33:18,559
not a choice same argument I mean we've

753
00:33:18,559 --> 00:33:19,970
been talking about this I mean let's

754
00:33:19,970 --> 00:33:21,980
take it out of like worst case scenario

755
00:33:21,980 --> 00:33:25,110
for him I know but let's even think

756
00:33:25,110 --> 00:33:27,210
about you know what we've been talking

757
00:33:27,210 --> 00:33:30,929
about for a number of years in the UK in

758
00:33:30,929 --> 00:33:33,990
particular and indeed all over Europe as

759
00:33:33,990 --> 00:33:38,720
well where your cameras in public just

760
00:33:38,720 --> 00:33:41,669
surveillance cameras have become so

761
00:33:41,669 --> 00:33:43,860
ubiquitous that you know I mean you

762
00:33:43,860 --> 00:33:46,950
can't go from your home to the store and

763
00:33:46,950 --> 00:33:49,140
back without being filmed seven

764
00:33:49,140 --> 00:33:51,990
different times and yeah there's there

765
00:33:51,990 --> 00:33:55,289
is a you know suddenly it's become your

766
00:33:55,289 --> 00:33:57,870
only privacy in which it doesn't even

767
00:33:57,870 --> 00:33:59,309
exist there because we all connect

768
00:33:59,309 --> 00:34:00,390
ourselves to the internet and do things

769
00:34:00,390 --> 00:34:04,020
is within your home but you know I I do

770
00:34:04,020 --> 00:34:06,809
agree with Lisa from the perspective of

771
00:34:06,809 --> 00:34:10,710
it being degrees because with deep fakes

772
00:34:10,710 --> 00:34:13,080
that's even what's occurring when we

773
00:34:13,080 --> 00:34:16,050
just think about that the video itself

774
00:34:16,050 --> 00:34:19,440
so you think in terms of like the

775
00:34:19,440 --> 00:34:24,570
justice system we go back 20 30 40 50

776
00:34:24,570 --> 00:34:27,000
years whatever it is to the point where

777
00:34:27,000 --> 00:34:29,699
I witness accounts were the gold

778
00:34:29,699 --> 00:34:32,010
standard of evidence like that was the

779
00:34:32,010 --> 00:34:33,629
best possible evidence you could have

780
00:34:33,629 --> 00:34:35,940
well then that kind of started to fade

781
00:34:35,940 --> 00:34:37,710
we started realize there were biases and

782
00:34:37,710 --> 00:34:41,280
other issues with with what I witnesses

783
00:34:41,280 --> 00:34:43,260
and this thing surfaced we called video

784
00:34:43,260 --> 00:34:46,500
and now if you had video evidence that

785
00:34:46,500 --> 00:34:48,960
was the gold standard I mean if we saw

786
00:34:48,960 --> 00:34:52,230
it on video and had to be true and then

787
00:34:52,230 --> 00:34:54,869
we started to see that get picked apart

788
00:34:54,869 --> 00:34:58,080
and torn down and and now a deep face it

789
00:34:58,080 --> 00:35:00,660
really accelerates that because now it

790
00:35:00,660 --> 00:35:02,280
makes even harder yet to trust that

791
00:35:02,280 --> 00:35:04,710
video so the question kind of becomes

792
00:35:04,710 --> 00:35:07,710
what's next you know what is that next

793
00:35:07,710 --> 00:35:11,160
layer and from a privacy perspective I

794
00:35:11,160 --> 00:35:12,690
think we have to you know that's where

795
00:35:12,690 --> 00:35:16,650
we as technologists we in the security

796
00:35:16,650 --> 00:35:18,930
space can start to impact that I think

797
00:35:18,930 --> 00:35:21,330
starting to really help people see that

798
00:35:21,330 --> 00:35:25,290
ethical side of okay this is cool that

799
00:35:25,290 --> 00:35:26,760
we're doing all this really neat stuff

800
00:35:26,760 --> 00:35:28,770
with deep learning but now how would we

801
00:35:28,770 --> 00:35:32,730
use it in a way that improves life and

802
00:35:32,730 --> 00:35:35,740
you know maybe improve some

803
00:35:35,740 --> 00:35:39,070
that that privacy factor and India I

804
00:35:39,070 --> 00:35:40,630
mean there are positive uses of this

805
00:35:40,630 --> 00:35:44,980
technology we already see it today as I

806
00:35:44,980 --> 00:35:46,600
was listening to everybody talk one

807
00:35:46,600 --> 00:35:48,700
thing that popped into my mind was what

808
00:35:48,700 --> 00:35:51,490
happens when we get arrested right we

809
00:35:51,490 --> 00:35:55,510
all read our Miranda rights and that is

810
00:35:55,510 --> 00:35:57,520
you have the right to remain silent

811
00:35:57,520 --> 00:35:59,500
everything you say and do can and will

812
00:35:59,500 --> 00:36:02,020
be used against you so my question for

813
00:36:02,020 --> 00:36:06,100
everybody is as as we step into a public

814
00:36:06,100 --> 00:36:10,150
domain as we go peacefully assemble do

815
00:36:10,150 --> 00:36:12,340
we really have the the rights we think

816
00:36:12,340 --> 00:36:14,280
we have anymore

817
00:36:14,280 --> 00:36:17,440
when so much of this is being used

818
00:36:17,440 --> 00:36:19,960
against us in ways that were not even

819
00:36:19,960 --> 00:36:24,250
aware of now there's an interesting

820
00:36:24,250 --> 00:36:27,850
philosophical question because you can

821
00:36:27,850 --> 00:36:32,400
be silent verbally but yeah your actions

822
00:36:32,400 --> 00:36:36,310
tell stories that may or may not

823
00:36:36,310 --> 00:36:38,320
actually be true but could definitely be

824
00:36:38,320 --> 00:36:40,570
used against you and do get used against

825
00:36:40,570 --> 00:36:43,920
you and so that that that does

826
00:36:43,920 --> 00:36:46,240
additionally complicate things so how do

827
00:36:46,240 --> 00:36:48,820
we I guess what I'm trying to figure out

828
00:36:48,820 --> 00:36:50,440
is how do we now take this cool

829
00:36:50,440 --> 00:36:54,160
technology and start using it in ways

830
00:36:54,160 --> 00:36:56,590
that address that and that I think is

831
00:36:56,590 --> 00:36:59,530
gonna be very telling moving forward

832
00:36:59,530 --> 00:37:02,800
because yeah I know there's that's a

833
00:37:02,800 --> 00:37:04,060
really good point Andrea and I don't

834
00:37:04,060 --> 00:37:07,090
know how we we really move forward on

835
00:37:07,090 --> 00:37:10,000
that and it probably you know going back

836
00:37:10,000 --> 00:37:12,670
to what Lisa and JM you guys were

837
00:37:12,670 --> 00:37:15,520
talking back and forth on how do we you

838
00:37:15,520 --> 00:37:18,780
know do we need to morph how we look at

839
00:37:18,780 --> 00:37:21,369
liberties and rights and then now

840
00:37:21,369 --> 00:37:23,320
technology supports that and plays into

841
00:37:23,320 --> 00:37:25,690
that and then that become clear topic

842
00:37:25,690 --> 00:37:29,140
than just videos right I mean the

843
00:37:29,140 --> 00:37:32,260
ubiquity of video has a lot of downsides

844
00:37:32,260 --> 00:37:35,290
but there's some positive news as well

845
00:37:35,290 --> 00:37:39,820
the cataclysmic shift that we have had

846
00:37:39,820 --> 00:37:43,510
since May 25th has been because of video

847
00:37:43,510 --> 00:37:46,200
that wouldn't have been available to us

848
00:37:46,200 --> 00:37:48,970
in an earlier time and it has a

849
00:37:48,970 --> 00:37:51,130
us to go out and seek justice for people

850
00:37:51,130 --> 00:37:54,880
who you know who died and so there is

851
00:37:54,880 --> 00:37:58,990
some good there for that oh yeah two

852
00:37:58,990 --> 00:38:00,670
years ago the Apple's Police Department

853
00:38:00,670 --> 00:38:03,070
did not have body cameras

854
00:38:03,070 --> 00:38:04,690
now we've sought we saw the whole thing

855
00:38:04,690 --> 00:38:07,780
we've seen countless videos of what's

856
00:38:07,780 --> 00:38:10,290
going on during protests come up

857
00:38:10,290 --> 00:38:12,700
highlighting some of the major issues

858
00:38:12,700 --> 00:38:14,440
that people are concerned with and it

859
00:38:14,440 --> 00:38:19,660
has had positive impact you know moving

860
00:38:19,660 --> 00:38:21,550
beyond just video is a you know kind of

861
00:38:21,550 --> 00:38:23,830
to the deep learning inside of it you

862
00:38:23,830 --> 00:38:25,570
know you I've got two folks on here

863
00:38:25,570 --> 00:38:29,020
really you're in the heat of the

864
00:38:29,020 --> 00:38:32,859
healthcare situations and what deep face

865
00:38:32,859 --> 00:38:35,470
mean in terms of medical imaging the

866
00:38:35,470 --> 00:38:37,570
ability of deep fake technology to

867
00:38:37,570 --> 00:38:41,950
identify tumors and a size far smaller

868
00:38:41,950 --> 00:38:44,619
than even the best trained doctors are

869
00:38:44,619 --> 00:38:46,960
able to see and not only detect them but

870
00:38:46,960 --> 00:38:49,119
be able to predict their growth far more

871
00:38:49,119 --> 00:38:51,910
accurately than any doctor could you

872
00:38:51,910 --> 00:38:54,940
know that that kind of thing is exciting

873
00:38:54,940 --> 00:38:57,460
but that's where again I'm back to okay

874
00:38:57,460 --> 00:38:59,320
so we can pivot technology that way how

875
00:38:59,320 --> 00:39:00,849
can we take these other portions of it

876
00:39:00,849 --> 00:39:02,710
and pivot them towards things that are

877
00:39:02,710 --> 00:39:05,230
you know useful and are actually going

878
00:39:05,230 --> 00:39:09,099
to help improve not necessarily detract

879
00:39:09,099 --> 00:39:11,800
from what we look at in terms of privacy

880
00:39:11,800 --> 00:39:15,450
and how do we use the stuff ethically so

881
00:39:15,450 --> 00:39:20,109
and maybe I I you know I I don't

882
00:39:20,109 --> 00:39:23,109
understand the deep fake world as well

883
00:39:23,109 --> 00:39:25,180
as I do the world of cancer variant

884
00:39:25,180 --> 00:39:28,540
classification in genomics but let me

885
00:39:28,540 --> 00:39:30,670
take that healthcare example and kind of

886
00:39:30,670 --> 00:39:32,520
stretch it out a little bit because

887
00:39:32,520 --> 00:39:35,020
absolutely um machine learning and

888
00:39:35,020 --> 00:39:38,170
neural networks husband very effective

889
00:39:38,170 --> 00:39:41,710
in being able to for example classify

890
00:39:41,710 --> 00:39:44,080
variants of uncertain significance in a

891
00:39:44,080 --> 00:39:47,800
tumor far better than humans can at a

892
00:39:47,800 --> 00:39:52,420
faster rate in fact there are and Lisa

893
00:39:52,420 --> 00:39:54,250
might know this well in in genomics the

894
00:39:54,250 --> 00:39:56,200
Kliq cancer very it caught the cancer

895
00:39:56,200 --> 00:39:59,740
variants are classified it by a certain

896
00:39:59,740 --> 00:40:01,359
processor standards in or these

897
00:40:01,359 --> 00:40:03,099
standards that I might

898
00:40:03,099 --> 00:40:07,450
we're google's deepmind came along and

899
00:40:07,450 --> 00:40:09,790
figured out that they were 95% more

900
00:40:09,790 --> 00:40:11,820
effective than the existing human

901
00:40:11,820 --> 00:40:14,530
standards or the way they're proper the

902
00:40:14,530 --> 00:40:17,710
process that was used to classify

903
00:40:17,710 --> 00:40:20,500
variants my problem with that is though

904
00:40:20,500 --> 00:40:22,510
you know D fix is about creating

905
00:40:22,510 --> 00:40:24,849
something that is fake it you know neuro

906
00:40:24,849 --> 00:40:27,940
we can separate machine learning and and

907
00:40:27,940 --> 00:40:30,490
classification of variants from taking

908
00:40:30,490 --> 00:40:34,089
something and then producing a fake so I

909
00:40:34,089 --> 00:40:36,250
would I would pause it to all of you and

910
00:40:36,250 --> 00:40:38,890
feel free to debate or disagree but you

911
00:40:38,890 --> 00:40:40,420
know that's a whole other ball of wax

912
00:40:40,420 --> 00:40:42,670
when we're talking about you know using

913
00:40:42,670 --> 00:40:44,349
machine learning to create something

914
00:40:44,349 --> 00:40:47,109
that does not exist versus using machine

915
00:40:47,109 --> 00:40:50,560
learning in health to identify something

916
00:40:50,560 --> 00:40:52,780
I move too far into that but would you

917
00:40:52,780 --> 00:40:56,070
just agree Lisa what what do you think

918
00:40:56,070 --> 00:40:58,930
that whole area is kind of new to me but

919
00:40:58,930 --> 00:41:00,700
I was I was following your dialogue I

920
00:41:00,700 --> 00:41:03,160
was thinking that that could be used as

921
00:41:03,160 --> 00:41:05,560
a very effective training tool for

922
00:41:05,560 --> 00:41:08,680
medical professionals and that was what

923
00:41:08,680 --> 00:41:09,970
was coming to mind I thought that would

924
00:41:09,970 --> 00:41:14,260
be super cool so as I've been like how

925
00:41:14,260 --> 00:41:16,630
to how'd it die know something or how to

926
00:41:16,630 --> 00:41:19,390
create something that's a fake can use

927
00:41:19,390 --> 00:41:23,349
it as a training tool to use it as one

928
00:41:23,349 --> 00:41:27,130
of the things that I saw go by was the

929
00:41:27,130 --> 00:41:33,010
difference with black and sorry

930
00:41:33,010 --> 00:41:36,550
most medical diagnoses that they use for

931
00:41:36,550 --> 00:41:39,150
training purposes is with white patients

932
00:41:39,150 --> 00:41:42,970
and and so people of color any color

933
00:41:42,970 --> 00:41:45,280
other than white there's there's not as

934
00:41:45,280 --> 00:41:47,140
much information for medical

935
00:41:47,140 --> 00:41:50,380
practitioners to use and so maybe we

936
00:41:50,380 --> 00:41:52,630
could do some deep fakes and and apply

937
00:41:52,630 --> 00:41:57,880
some some genetics to that and and come

938
00:41:57,880 --> 00:42:00,040
up with some training principles of what

939
00:42:00,040 --> 00:42:01,660
I'm literally just thinking off the top

940
00:42:01,660 --> 00:42:07,240
of my head this is what I want people to

941
00:42:07,240 --> 00:42:08,710
think about no unfortunately where our

942
00:42:08,710 --> 00:42:11,710
time is up so we've got to wrap it up

943
00:42:11,710 --> 00:42:13,060
which I would love to keep this

944
00:42:13,060 --> 00:42:14,810
conversation going cuz it's been amazing

945
00:42:14,810 --> 00:42:16,670
that's I think what you touched on

946
00:42:16,670 --> 00:42:18,230
Lisa's just that mindset is what I

947
00:42:18,230 --> 00:42:19,760
really challenge people to think about

948
00:42:19,760 --> 00:42:21,920
when it comes to deep fakes is alright

949
00:42:21,920 --> 00:42:23,540
yeah this is really cool technology it's

950
00:42:23,540 --> 00:42:26,300
a little scary at times as well but how

951
00:42:26,300 --> 00:42:30,320
can it become something that is you know

952
00:42:30,320 --> 00:42:32,330
used for the right purposes there's a

953
00:42:32,330 --> 00:42:35,570
number of examples out there now and you

954
00:42:35,570 --> 00:42:37,370
know I challenge people just to think

955
00:42:37,370 --> 00:42:38,930
about that I've got you know we've got a

956
00:42:38,930 --> 00:42:40,520
conference call security folks and

957
00:42:40,520 --> 00:42:43,580
developers on this you know on this live

958
00:42:43,580 --> 00:42:48,560
stream we're turning to you all to help

959
00:42:48,560 --> 00:42:51,860
with that so so with that like I said

960
00:42:51,860 --> 00:42:54,110
we're unfortunately out of time so I

961
00:42:54,110 --> 00:42:55,790
want to thank you guys so much

962
00:42:55,790 --> 00:42:58,340
JME Andrea Lisa thank you for joining me

963
00:42:58,340 --> 00:43:02,060
I know this was probably not what we all

964
00:43:02,060 --> 00:43:04,010
expected we be doing this afternoon and

965
00:43:04,010 --> 00:43:05,750
actually I think this was a wonderful

966
00:43:05,750 --> 00:43:07,400
discussion I hope the folks out there

967
00:43:07,400 --> 00:43:09,350
enjoyed it too

968
00:43:09,350 --> 00:43:13,880
with that so thank you so much and we'll

969
00:43:13,880 --> 00:43:15,130
see you all real soon

970
00:43:15,130 --> 00:43:20,620
thank you take care everybody all right

