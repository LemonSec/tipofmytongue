1
00:00:00,000 --> 00:00:04,500
about deep fakes so for those of you

2
00:00:04,500 --> 00:00:07,620
that don't know just really quickly a

3
00:00:07,620 --> 00:00:10,469
very quick intro I am Melissa Miller I'm

4
00:00:10,469 --> 00:00:12,420
a hacker and a researcher I'm also an

5
00:00:12,420 --> 00:00:15,529
application security advocate for sneek

6
00:00:15,529 --> 00:00:18,750
many of you might be familiar with us by

7
00:00:18,750 --> 00:00:25,830
this point so additionally I am an

8
00:00:25,830 --> 00:00:27,180
author and a blogger I'm currently

9
00:00:27,180 --> 00:00:29,970
working on a book to serve as a guide to

10
00:00:29,970 --> 00:00:32,159
help people get their career started in

11
00:00:32,159 --> 00:00:33,540
information security

12
00:00:33,540 --> 00:00:36,149
I do also blog both within the sneek

13
00:00:36,149 --> 00:00:41,760
blog as well as you know on my personal

14
00:00:41,760 --> 00:00:43,710
website as well and I will share that

15
00:00:43,710 --> 00:00:47,989
information with you at the end of this

16
00:00:47,989 --> 00:00:52,110
then finally I am the co-host of a

17
00:00:52,110 --> 00:00:54,329
podcast and itsp magazine called the

18
00:00:54,329 --> 00:00:57,420
uncommon journey if you've been on the

19
00:00:57,420 --> 00:00:58,980
conference for a while tonight

20
00:00:58,980 --> 00:01:03,329
this morning whatever you actually saw

21
00:01:03,329 --> 00:01:06,450
my two co-hosts Chloe Misaki and Phillip

22
00:01:06,450 --> 00:01:08,310
Wylie who also presented in this track

23
00:01:08,310 --> 00:01:10,979
as well but I'm gonna take you on a

24
00:01:10,979 --> 00:01:13,020
journey now and it's a journey of sight

25
00:01:13,020 --> 00:01:17,700
and sound now when I was a kid my

26
00:01:17,700 --> 00:01:19,650
parents took me to what we call here in

27
00:01:19,650 --> 00:01:21,869
the States a fun house now I don't know

28
00:01:21,869 --> 00:01:23,430
why they call this thing a fun house

29
00:01:23,430 --> 00:01:26,310
it's basically a gigantic maze that you

30
00:01:26,310 --> 00:01:28,860
walk through and it's filled with all

31
00:01:28,860 --> 00:01:33,299
sorts of optical illusions and other

32
00:01:33,299 --> 00:01:35,549
things to disorient you as you try to

33
00:01:35,549 --> 00:01:38,549
find your way out of this fun house and

34
00:01:38,549 --> 00:01:40,770
I just remember being a very young child

35
00:01:40,770 --> 00:01:42,270
and going through this funhouse and

36
00:01:42,270 --> 00:01:44,460
every time I thought I found the exit

37
00:01:44,460 --> 00:01:46,229
here I thought I found it there I'd run

38
00:01:46,229 --> 00:01:49,020
up and I would realize that my senses

39
00:01:49,020 --> 00:01:51,390
were playing tricks on me I couldn't

40
00:01:51,390 --> 00:01:54,329
trust anything that I was perceiving and

41
00:01:54,329 --> 00:01:56,399
when we started to talk about this

42
00:01:56,399 --> 00:01:59,040
concept of deep fakes it really

43
00:01:59,040 --> 00:02:00,689
represents a new paradigm of attack

44
00:02:00,689 --> 00:02:03,840
where instead of attacking systems or

45
00:02:03,840 --> 00:02:06,299
attacking people now we're attacking

46
00:02:06,299 --> 00:02:08,669
people's senses the things that we use

47
00:02:08,669 --> 00:02:12,640
every day to determine and

48
00:02:12,640 --> 00:02:14,410
what's real and what's fake all around

49
00:02:14,410 --> 00:02:18,250
us so when I say deep fakes what do I

50
00:02:18,250 --> 00:02:21,340
mean by that well I am talking about any

51
00:02:21,340 --> 00:02:25,270
form of artificial media created by deep

52
00:02:25,270 --> 00:02:27,910
learning neural networks so it might be

53
00:02:27,910 --> 00:02:31,540
images it might be audio or the most

54
00:02:31,540 --> 00:02:33,430
common one that people think of when

55
00:02:33,430 --> 00:02:37,390
they hear deep fakes is video now we're

56
00:02:37,390 --> 00:02:41,920
too deep fakes begin well you know as is

57
00:02:41,920 --> 00:02:43,690
often the case a lot of times when we

58
00:02:43,690 --> 00:02:45,550
have promising technology the first

59
00:02:45,550 --> 00:02:47,440
industry that really picks up on it is

60
00:02:47,440 --> 00:02:50,110
the porn industry and indeed that's what

61
00:02:50,110 --> 00:02:52,930
happened with deep fakes as researchers

62
00:02:52,930 --> 00:02:55,690
early on czar coming out with deep fake

63
00:02:55,690 --> 00:02:58,810
technology in 2017 I didn't take long

64
00:02:58,810 --> 00:03:01,420
before we started seeing websites making

65
00:03:01,420 --> 00:03:03,700
deep fake videos where they took a

66
00:03:03,700 --> 00:03:06,430
normal pornographic video they took

67
00:03:06,430 --> 00:03:09,250
their actress out of it and put the face

68
00:03:09,250 --> 00:03:12,130
of a celebrity like Natalie Portman or

69
00:03:12,130 --> 00:03:15,310
gal gadot or Natalie Dormer Emma Watson

70
00:03:15,310 --> 00:03:17,620
and others and in fact still today

71
00:03:17,620 --> 00:03:19,150
there's websites that will do this and

72
00:03:19,150 --> 00:03:21,820
some of them even take requests and you

73
00:03:21,820 --> 00:03:24,070
can pay an amount of money and they'll

74
00:03:24,070 --> 00:03:26,380
they'll make a deep fake porn video of

75
00:03:26,380 --> 00:03:29,280
whatever celebrity it is you want to see

76
00:03:29,280 --> 00:03:34,269
now it didn't take long from that humble

77
00:03:34,269 --> 00:03:36,100
beginning to jump into politics and

78
00:03:36,100 --> 00:03:38,350
certainly I think many people have seen

79
00:03:38,350 --> 00:03:40,269
a lot of the videos that are out there

80
00:03:40,269 --> 00:03:43,600
of different characters being deep faked

81
00:03:43,600 --> 00:03:46,090
within videos if you don't recognize any

82
00:03:46,090 --> 00:03:48,220
of these you might recognize one from

83
00:03:48,220 --> 00:03:52,299
2018 where they deep fake Barack Obama

84
00:03:52,299 --> 00:03:55,329
saying some less than kind things about

85
00:03:55,329 --> 00:03:56,950
the current President of the United

86
00:03:56,950 --> 00:04:00,370
States and so forth but the attack

87
00:04:00,370 --> 00:04:02,590
vectors extend beyond deep fakes

88
00:04:02,590 --> 00:04:05,680
so I mentioned audio and deep fake audio

89
00:04:05,680 --> 00:04:08,470
before social engineering is a real

90
00:04:08,470 --> 00:04:11,320
concern when we think about deep phase

91
00:04:11,320 --> 00:04:14,170
so there's an example that comes to us

92
00:04:14,170 --> 00:04:17,108
out of the UK of an energy company where

93
00:04:17,108 --> 00:04:22,349
the CEO was tricked into transferring

94
00:04:22,349 --> 00:04:25,400
200 million euros to attackers

95
00:04:25,400 --> 00:04:27,500
because they use deep fake audio to

96
00:04:27,500 --> 00:04:29,900
convince him that the president of that

97
00:04:29,900 --> 00:04:32,990
energy company's parent company was

98
00:04:32,990 --> 00:04:34,580
calling him and instructing them to make

99
00:04:34,580 --> 00:04:38,930
this wire transfer so as this technology

100
00:04:38,930 --> 00:04:42,530
goes beyond just video and into audio it

101
00:04:42,530 --> 00:04:45,050
represents a real threat in terms of

102
00:04:45,050 --> 00:04:49,520
social engineering and so as we start to

103
00:04:49,520 --> 00:04:51,410
think about okay that how this applies

104
00:04:51,410 --> 00:04:55,750
to visit the business environment the

105
00:04:55,750 --> 00:04:58,510
potentially uses for extortion is

106
00:04:58,510 --> 00:05:02,000
another real threat now I use these I

107
00:05:02,000 --> 00:05:04,100
saw this example up here obviously I

108
00:05:04,100 --> 00:05:05,539
think we probably all recognize that

109
00:05:05,539 --> 00:05:10,610
face the head of Amazon you know indeed

110
00:05:10,610 --> 00:05:13,220
you know everything I understand of this

111
00:05:13,220 --> 00:05:15,139
that all the videos and stuff are real

112
00:05:15,139 --> 00:05:18,710
nothing was deep fake but how much of a

113
00:05:18,710 --> 00:05:20,599
leap is it that someone looking to

114
00:05:20,599 --> 00:05:23,990
extort a high-ranking CEO might use deep

115
00:05:23,990 --> 00:05:25,729
fake the threat of a deep fake video

116
00:05:25,729 --> 00:05:28,759
being released to try to extort money or

117
00:05:28,759 --> 00:05:32,120
other actions from that particular CEO

118
00:05:32,120 --> 00:05:34,789
even if they know it's fake just getting

119
00:05:34,789 --> 00:05:36,949
that out into the media could be

120
00:05:36,949 --> 00:05:39,650
damaging and if their spouse sees it it

121
00:05:39,650 --> 00:05:41,930
could be impactful to their marriage as

122
00:05:41,930 --> 00:05:44,060
well and the fact that it comes out

123
00:05:44,060 --> 00:05:47,710
later that it's false might be too late

124
00:05:47,710 --> 00:05:51,110
but the the threats extend beyond just

125
00:05:51,110 --> 00:05:52,940
extortion when we start talking business

126
00:05:52,940 --> 00:05:56,060
and there's this idea that I call

127
00:05:56,060 --> 00:05:58,820
outsider trading so we're all probably

128
00:05:58,820 --> 00:06:01,070
familiar with insider trading where

129
00:06:01,070 --> 00:06:03,530
somebody uses insider knowledge of an

130
00:06:03,530 --> 00:06:07,000
organization to make stock market trades

131
00:06:07,000 --> 00:06:09,560
based on their anticipation of stocks

132
00:06:09,560 --> 00:06:13,599
going up or down so consider this now

133
00:06:13,599 --> 00:06:16,310
we'll take Elon Musk and Tesla and let's

134
00:06:16,310 --> 00:06:18,010
say Tesla is getting ready to launch

135
00:06:18,010 --> 00:06:21,380
their newest model we'll say the Model Q

136
00:06:21,380 --> 00:06:24,110
and the night before they're ready to

137
00:06:24,110 --> 00:06:28,099
have their big launch a deep fake video

138
00:06:28,099 --> 00:06:30,620
of Elon Musk talking to investors

139
00:06:30,620 --> 00:06:32,479
telling them how the lying and is

140
00:06:32,479 --> 00:06:34,909
experiencing problems and they're gonna

141
00:06:34,909 --> 00:06:36,710
be late and shipping and they're gonna

142
00:06:36,710 --> 00:06:38,150
have all sorts of issues

143
00:06:38,150 --> 00:06:40,100
that makes its way onto the Internet the

144
00:06:40,100 --> 00:06:41,780
night before well what happens to that

145
00:06:41,780 --> 00:06:44,540
stock that was soaring because of this

146
00:06:44,540 --> 00:06:46,250
pending announcement well that stock is

147
00:06:46,250 --> 00:06:49,820
gonna drop and now that attacker can buy

148
00:06:49,820 --> 00:06:53,900
that shorted stock and after news comes

149
00:06:53,900 --> 00:06:56,090
out that this video was phony it was

150
00:06:56,090 --> 00:06:58,060
just a deep fake the stocks rise

151
00:06:58,060 --> 00:07:01,400
attacker sells off that stock you now

152
00:07:01,400 --> 00:07:03,680
have this concept of outsider trading

153
00:07:03,680 --> 00:07:05,480
but if I can do that to a large

154
00:07:05,480 --> 00:07:08,390
organization can I also do that to whole

155
00:07:08,390 --> 00:07:11,870
industry's entire markets what about

156
00:07:11,870 --> 00:07:13,790
maybe even in the economies of small

157
00:07:13,790 --> 00:07:17,540
countries the fact is we have so much in

158
00:07:17,540 --> 00:07:20,030
our world that's dependent on our trust

159
00:07:20,030 --> 00:07:22,130
in information that when that

160
00:07:22,130 --> 00:07:25,790
information is threatened it it goes

161
00:07:25,790 --> 00:07:27,380
right to the core of how these systems

162
00:07:27,380 --> 00:07:32,900
work so what is real how do we define it

163
00:07:32,900 --> 00:07:36,200
we use our senses every day to perceive

164
00:07:36,200 --> 00:07:38,630
the world around us and to make those

165
00:07:38,630 --> 00:07:41,180
judgments of what is real and what is

166
00:07:41,180 --> 00:07:45,590
fake so let's talk about how these

167
00:07:45,590 --> 00:07:48,350
videos for instance are created and

168
00:07:48,350 --> 00:07:50,360
they're ultimately created using what we

169
00:07:50,360 --> 00:07:52,760
call a generative adversarial networker

170
00:07:52,760 --> 00:07:55,430
again now if I break that down its

171
00:07:55,430 --> 00:07:57,560
generative in that it's creating

172
00:07:57,560 --> 00:08:00,230
something it's adversarial in that

173
00:08:00,230 --> 00:08:02,570
there's two networks within this game

174
00:08:02,570 --> 00:08:05,120
the first is called the generator the

175
00:08:05,120 --> 00:08:06,950
generators job as the name would imply

176
00:08:06,950 --> 00:08:09,860
is to create the phony images or the

177
00:08:09,860 --> 00:08:13,160
phony video the discriminator then is

178
00:08:13,160 --> 00:08:16,370
the adversary to that Network and the

179
00:08:16,370 --> 00:08:19,010
discriminator is there to determine how

180
00:08:19,010 --> 00:08:22,240
realistic those images actually look

181
00:08:22,240 --> 00:08:25,010
both sides of this equation are trained

182
00:08:25,010 --> 00:08:27,800
with training images and so the

183
00:08:27,800 --> 00:08:29,720
discriminator understands what it's

184
00:08:29,720 --> 00:08:31,810
trying to match up to the generator

185
00:08:31,810 --> 00:08:34,220
understands effectively what it's trying

186
00:08:34,220 --> 00:08:37,789
to create we pass it a target media so

187
00:08:37,789 --> 00:08:38,870
that's the media that we're going to

188
00:08:38,870 --> 00:08:42,110
replace a person in and we're trying to

189
00:08:42,110 --> 00:08:44,570
create this subject so the generator

190
00:08:44,570 --> 00:08:46,550
creates these images the discriminator

191
00:08:46,550 --> 00:08:48,530
looks at them and says either yeah that

192
00:08:48,530 --> 00:08:51,880
looks real or no that looks pretty fake

193
00:08:51,880 --> 00:08:54,430
and it then passes information back to

194
00:08:54,430 --> 00:08:57,370
the generator so the generator can

195
00:08:57,370 --> 00:08:59,500
improve what it's doing now how does

196
00:08:59,500 --> 00:09:01,090
this actually work

197
00:09:01,090 --> 00:09:03,180
so we're gonna look at some deep fake

198
00:09:03,180 --> 00:09:07,030
media that I did of myself and so what

199
00:09:07,030 --> 00:09:10,450
we see is as we're we're training this

200
00:09:10,450 --> 00:09:13,360
within this network of the generator

201
00:09:13,360 --> 00:09:15,910
there are there's this idea of encoding

202
00:09:15,910 --> 00:09:17,920
and what encoding really is what the

203
00:09:17,920 --> 00:09:20,020
encoder does is it takes an image and

204
00:09:20,020 --> 00:09:22,060
you can see from the way this image is

205
00:09:22,060 --> 00:09:23,380
shaped that these images get

206
00:09:23,380 --> 00:09:25,450
standardized to a square image of a

207
00:09:25,450 --> 00:09:28,900
specific size and that's so that as

208
00:09:28,900 --> 00:09:31,360
we're building these models in our deep

209
00:09:31,360 --> 00:09:33,280
learning networks they're working with

210
00:09:33,280 --> 00:09:35,830
consistent data that's all normalized so

211
00:09:35,830 --> 00:09:37,990
the encoder then takes that image and

212
00:09:37,990 --> 00:09:40,480
tries to describe it in a mathematical

213
00:09:40,480 --> 00:09:43,600
sense so everything is numeric it's

214
00:09:43,600 --> 00:09:47,070
taking all the aspects of that face 68

215
00:09:47,070 --> 00:09:50,020
typically 68 different landmark points

216
00:09:50,020 --> 00:09:52,900
in facial recognition other information

217
00:09:52,900 --> 00:09:55,810
about expressions and so forth angles

218
00:09:55,810 --> 00:09:57,880
the alignment of the face all of that

219
00:09:57,880 --> 00:10:01,240
gets encoded into a numeric model then

220
00:10:01,240 --> 00:10:04,270
we have this decoder and that decoder is

221
00:10:04,270 --> 00:10:07,090
trained to use that model to regenerate

222
00:10:07,090 --> 00:10:09,730
that same image and then the

223
00:10:09,730 --> 00:10:11,140
discriminator takes a look at it and

224
00:10:11,140 --> 00:10:13,060
says okay how much does this really look

225
00:10:13,060 --> 00:10:15,370
like the original image it calculates

226
00:10:15,370 --> 00:10:17,200
this thing called loss which is just a

227
00:10:17,200 --> 00:10:20,620
measure of how realistic how good is the

228
00:10:20,620 --> 00:10:23,680
result and then it updates that model

229
00:10:23,680 --> 00:10:25,780
and it updates what we call the weights

230
00:10:25,780 --> 00:10:29,130
and the weights are what impact those

231
00:10:29,130 --> 00:10:32,400
numeric values and provide that feedback

232
00:10:32,400 --> 00:10:35,500
so when we're actually training again

233
00:10:35,500 --> 00:10:37,750
what we're doing is we're actually

234
00:10:37,750 --> 00:10:39,850
training it on two sets we're training

235
00:10:39,850 --> 00:10:43,120
it on our target face which is the face

236
00:10:43,120 --> 00:10:44,890
that we're going to replace and the

237
00:10:44,890 --> 00:10:47,530
subject face but we're training one

238
00:10:47,530 --> 00:10:50,350
encoder to be able to encode both of

239
00:10:50,350 --> 00:10:53,110
those into that numeric model but then

240
00:10:53,110 --> 00:10:55,030
we're training our decoders to be able

241
00:10:55,030 --> 00:10:57,400
to take that common numeric model and

242
00:10:57,400 --> 00:11:01,330
create either image a or image B and so

243
00:11:01,330 --> 00:11:03,190
as we are in that training and it's

244
00:11:03,190 --> 00:11:04,870
getting better and better we see over

245
00:11:04,870 --> 00:11:05,620
time

246
00:11:05,620 --> 00:11:09,130
that those images improve now what we do

247
00:11:09,130 --> 00:11:10,570
when we feel we have a strong enough

248
00:11:10,570 --> 00:11:13,360
model when that loss value is low enough

249
00:11:13,360 --> 00:11:16,000
that we and the results look good enough

250
00:11:16,000 --> 00:11:18,700
we do a conversion and the conversion is

251
00:11:18,700 --> 00:11:20,350
what actually will create the final

252
00:11:20,350 --> 00:11:22,750
media and all we're doing there is we're

253
00:11:22,750 --> 00:11:25,480
taking that that image of the target

254
00:11:25,480 --> 00:11:28,150
we're passing it to the encoder but then

255
00:11:28,150 --> 00:11:31,600
we're using the decoder for the subject

256
00:11:31,600 --> 00:11:34,540
and so now it's able to take all that

257
00:11:34,540 --> 00:11:36,100
information about expressions and

258
00:11:36,100 --> 00:11:38,200
alignment and everything else of my face

259
00:11:38,200 --> 00:11:41,080
as you see here on the left and turn

260
00:11:41,080 --> 00:11:44,230
that into Alyssa Milano's face that you

261
00:11:44,230 --> 00:11:49,330
see on the right so this presents some

262
00:11:49,330 --> 00:11:52,510
issues and we're able to detect visual

263
00:11:52,510 --> 00:11:54,760
cues in some cases and other things that

264
00:11:54,760 --> 00:11:57,970
tell us that videos are fake initially

265
00:11:57,970 --> 00:11:59,529
we just were able to look at the fit of

266
00:11:59,529 --> 00:12:01,480
a face because we were using

267
00:12:01,480 --> 00:12:04,000
standardized sizes and everybody's faces

268
00:12:04,000 --> 00:12:05,920
different size and shape there was

269
00:12:05,920 --> 00:12:07,810
warping and other artifacts that could

270
00:12:07,810 --> 00:12:10,420
be detected very early on but the fake

271
00:12:10,420 --> 00:12:13,410
technology has gotten better so

272
00:12:13,410 --> 00:12:16,300
researchers next decided to look at

273
00:12:16,300 --> 00:12:19,000
blinking because they noticed that with

274
00:12:19,000 --> 00:12:22,540
deep fake videos the blinking the amount

275
00:12:22,540 --> 00:12:24,730
of blinking that the final subject was

276
00:12:24,730 --> 00:12:27,700
doing didn't match up and wasn't wasn't

277
00:12:27,700 --> 00:12:29,740
realistic because if you think about it

278
00:12:29,740 --> 00:12:31,510
most people don't have a lot of pictures

279
00:12:31,510 --> 00:12:33,310
of themselves out there on the internet

280
00:12:33,310 --> 00:12:34,959
that could be used for training that

281
00:12:34,959 --> 00:12:36,250
show their eyes closed

282
00:12:36,250 --> 00:12:38,950
well indeed the University at Albany

283
00:12:38,950 --> 00:12:41,709
released this study and within months

284
00:12:41,709 --> 00:12:44,440
deep fake technology was now replicating

285
00:12:44,440 --> 00:12:46,779
blinking so they did some things they

286
00:12:46,779 --> 00:12:49,240
started looking at velocities and other

287
00:12:49,240 --> 00:12:51,100
things about the nature of blinking but

288
00:12:51,100 --> 00:12:54,279
at the end of the day the ability to use

289
00:12:54,279 --> 00:12:57,430
blinking as an indicator is no longer a

290
00:12:57,430 --> 00:12:59,589
viable or accurate option for detecting

291
00:12:59,589 --> 00:13:02,860
deep things so some other issues we have

292
00:13:02,860 --> 00:13:05,560
I mentioned the standardized size again

293
00:13:05,560 --> 00:13:08,800
of these images that we use and so here

294
00:13:08,800 --> 00:13:10,240
you have an example of Steve Buscemi's

295
00:13:10,240 --> 00:13:13,440
face being placed on Sharon Stone and

296
00:13:13,440 --> 00:13:17,140
it's in this scene in the movie that it

297
00:13:17,140 --> 00:13:19,089
starts with this image which

298
00:13:19,089 --> 00:13:22,480
looks pretty realistic her her face is

299
00:13:22,480 --> 00:13:24,639
far enough away that using that

300
00:13:24,639 --> 00:13:28,629
standardized size of that image this

301
00:13:28,629 --> 00:13:31,449
looks pretty realistic now the issue is

302
00:13:31,449 --> 00:13:34,660
later in the scene the camera zooms in

303
00:13:34,660 --> 00:13:38,800
and because now that deep fake has to

304
00:13:38,800 --> 00:13:41,889
stretch and resize that image we lose

305
00:13:41,889 --> 00:13:44,139
fidelity and you can see the difference

306
00:13:44,139 --> 00:13:47,889
in terms of just the resolution of the

307
00:13:47,889 --> 00:13:51,730
face region versus the rest of her head

308
00:13:51,730 --> 00:13:54,339
you also see some of the deformations

309
00:13:54,339 --> 00:13:56,620
that show up in the cheekbones and other

310
00:13:56,620 --> 00:13:59,259
things as a result of having to

311
00:13:59,259 --> 00:14:01,449
manipulate that standardized square

312
00:14:01,449 --> 00:14:05,620
image to fit a new size and a new shape

313
00:14:05,620 --> 00:14:07,059
and a new angle that doesn't necessarily

314
00:14:07,059 --> 00:14:11,680
match up exactly so this is been useful

315
00:14:11,680 --> 00:14:14,889
and it does make it harder when we have

316
00:14:14,889 --> 00:14:18,490
video that changes in that focal length

317
00:14:18,490 --> 00:14:20,620
but if we've got a standard focal length

318
00:14:20,620 --> 00:14:22,420
like say a press-conference being given

319
00:14:22,420 --> 00:14:25,179
by a president it's still not an

320
00:14:25,179 --> 00:14:27,879
effective way to detect a deep fake so

321
00:14:27,879 --> 00:14:29,829
some promising stuff that's being done

322
00:14:29,829 --> 00:14:32,350
is actually looking at modeling behavior

323
00:14:32,350 --> 00:14:35,439
so instead of trying to really just

324
00:14:35,439 --> 00:14:37,959
detect things the artifacts and other

325
00:14:37,959 --> 00:14:41,079
things within the video researchers are

326
00:14:41,079 --> 00:14:45,850
looking at can we detect behaviors of an

327
00:14:45,850 --> 00:14:49,870
individual but how their face changes

328
00:14:49,870 --> 00:14:51,399
how their facial expressions other

329
00:14:51,399 --> 00:14:53,470
things that they do when they're

330
00:14:53,470 --> 00:14:56,290
delivering content of a certain type so

331
00:14:56,290 --> 00:14:58,329
if they're very emotional if they're

332
00:14:58,329 --> 00:15:00,929
very angry or they're very you know very

333
00:15:00,929 --> 00:15:05,350
mundane or subdued how does their face

334
00:15:05,350 --> 00:15:06,069
change

335
00:15:06,069 --> 00:15:07,990
and does that match up to what we see in

336
00:15:07,990 --> 00:15:11,679
that video now this is this is very

337
00:15:11,679 --> 00:15:14,139
promising in that Dartmouth recently

338
00:15:14,139 --> 00:15:16,389
released their results on this and they

339
00:15:16,389 --> 00:15:19,120
are seeing 95% accuracy they expect to

340
00:15:19,120 --> 00:15:21,730
have 99% accuracy and detecting deep

341
00:15:21,730 --> 00:15:24,759
fakes by the time the 2020 US

342
00:15:24,759 --> 00:15:27,250
presidential elections roll around now

343
00:15:27,250 --> 00:15:28,899
the problem with this of course is you

344
00:15:28,899 --> 00:15:30,549
have to do a lot of analysis of the

345
00:15:30,549 --> 00:15:32,480
subject so it works great

346
00:15:32,480 --> 00:15:35,779
we're talking about yo high-profile

347
00:15:35,779 --> 00:15:38,120
celebrities and so forth but when it

348
00:15:38,120 --> 00:15:39,980
comes to the common individual this type

349
00:15:39,980 --> 00:15:42,680
of behavioral information just might not

350
00:15:42,680 --> 00:15:46,970
even be available for analysis so

351
00:15:46,970 --> 00:15:48,199
researchers are also looking at how can

352
00:15:48,199 --> 00:15:49,730
we just prevent the fakes from being

353
00:15:49,730 --> 00:15:52,550
created in the first place and what

354
00:15:52,550 --> 00:15:55,279
they're using in this research is what

355
00:15:55,279 --> 00:15:57,579
we call adversarial / tribution 'z

356
00:15:57,579 --> 00:16:00,500
perturbations excuse me basically what

357
00:16:00,500 --> 00:16:03,740
it is it's unperceptive all noise that's

358
00:16:03,740 --> 00:16:06,560
introduced into these images so you can

359
00:16:06,560 --> 00:16:08,420
see this top row is the original image

360
00:16:08,420 --> 00:16:10,910
and you can see from the rectangle that

361
00:16:10,910 --> 00:16:14,329
the facial recognition libraries easily

362
00:16:14,329 --> 00:16:17,240
identify that face but when I drop down

363
00:16:17,240 --> 00:16:19,579
I see what the image looks like after

364
00:16:19,579 --> 00:16:21,410
that noise has been injected in that

365
00:16:21,410 --> 00:16:23,899
second row and I see that all of a

366
00:16:23,899 --> 00:16:25,370
sudden the facial recognition is

367
00:16:25,370 --> 00:16:28,699
identifying tons of different faces that

368
00:16:28,699 --> 00:16:31,250
don't exist and it really is not

369
00:16:31,250 --> 00:16:33,709
identifying the real face and then that

370
00:16:33,709 --> 00:16:35,540
final row shows you what that noise

371
00:16:35,540 --> 00:16:37,550
actually looks like that's being

372
00:16:37,550 --> 00:16:39,860
inserted to that image so human eye

373
00:16:39,860 --> 00:16:42,230
doesn't perceive this at all but when it

374
00:16:42,230 --> 00:16:43,389
comes to the facial recognition

375
00:16:43,389 --> 00:16:46,300
libraries that deep fakes rely on today

376
00:16:46,300 --> 00:16:48,829
we see how it prevents them from being

377
00:16:48,829 --> 00:16:51,230
able to read those images and therefore

378
00:16:51,230 --> 00:16:55,190
they're unable to create or to do that

379
00:16:55,190 --> 00:16:57,079
training and create that model and to

380
00:16:57,079 --> 00:17:00,829
create images from it so these are lots

381
00:17:00,829 --> 00:17:02,480
of technological solutions and they're

382
00:17:02,480 --> 00:17:06,980
great but the problem is this is

383
00:17:06,980 --> 00:17:09,500
misinformation ultimately that we're

384
00:17:09,500 --> 00:17:11,510
dealing with and misinformation is not a

385
00:17:11,510 --> 00:17:13,280
technological problem it's a human

386
00:17:13,280 --> 00:17:15,770
problem and so we need to address the

387
00:17:15,770 --> 00:17:19,099
human element in all of this so the

388
00:17:19,099 --> 00:17:21,079
problem with misinformation first of all

389
00:17:21,079 --> 00:17:24,589
is that it's very sticky it is hard to

390
00:17:24,589 --> 00:17:27,169
dislodge misinformation once it's in

391
00:17:27,169 --> 00:17:29,150
somebody's brain and that's because

392
00:17:29,150 --> 00:17:32,570
first of all disinformation it plays off

393
00:17:32,570 --> 00:17:34,640
of people's existing thoughts and

394
00:17:34,640 --> 00:17:37,270
beliefs and their their predisposed

395
00:17:37,270 --> 00:17:40,070
knowledge and so forth basically it

396
00:17:40,070 --> 00:17:41,270
gives them something they want to

397
00:17:41,270 --> 00:17:44,570
believe is real and what happens then is

398
00:17:44,570 --> 00:17:46,400
that information gets in

399
00:17:46,400 --> 00:17:49,220
into a logic chain in our brains that

400
00:17:49,220 --> 00:17:51,169
just follows because it's already

401
00:17:51,169 --> 00:17:52,880
information that fits with everything

402
00:17:52,880 --> 00:17:54,559
else we believe everything else were

403
00:17:54,559 --> 00:17:57,080
prejudiced to see and everything else

404
00:17:57,080 --> 00:17:59,450
that we want to believe so as a result

405
00:17:59,450 --> 00:18:01,820
trying to remove that information that

406
00:18:01,820 --> 00:18:04,279
now as a part of this logic chain in our

407
00:18:04,279 --> 00:18:08,779
brain is extremely difficult so how do

408
00:18:08,779 --> 00:18:11,570
we go about this well to debunk

409
00:18:11,570 --> 00:18:13,370
misinformation the first thing we have

410
00:18:13,370 --> 00:18:14,900
to do is we have to put the warnings out

411
00:18:14,900 --> 00:18:18,080
there that misinformation exists so

412
00:18:18,080 --> 00:18:20,510
hearing about deep fakes and media and

413
00:18:20,510 --> 00:18:22,010
understanding that this is something

414
00:18:22,010 --> 00:18:24,740
that is being created with increasing

415
00:18:24,740 --> 00:18:28,010
reality is great and indeed when videos

416
00:18:28,010 --> 00:18:31,880
come up that are fake getting the news

417
00:18:31,880 --> 00:18:33,649
out there as quickly as possible that

418
00:18:33,649 --> 00:18:35,630
hey this is a deep fake video this is

419
00:18:35,630 --> 00:18:38,419
not the real thing is crucial it's also

420
00:18:38,419 --> 00:18:40,669
crucial that we repeat that information

421
00:18:40,669 --> 00:18:42,409
over and over again that this is a deep

422
00:18:42,409 --> 00:18:45,230
fake and we have to reinforce it again

423
00:18:45,230 --> 00:18:47,539
and again and again so the media is

424
00:18:47,539 --> 00:18:49,159
getting better at doing this we see

425
00:18:49,159 --> 00:18:51,080
social media is taking care of this as

426
00:18:51,080 --> 00:18:54,200
well but the one piece that we still

427
00:18:54,200 --> 00:18:56,750
have to get better at is in order to

428
00:18:56,750 --> 00:18:59,390
dislodge that information we need to

429
00:18:59,390 --> 00:19:01,610
replace it with an alternative narrative

430
00:19:01,610 --> 00:19:04,549
but that alternative truthful narrative

431
00:19:04,549 --> 00:19:08,299
has to fit that logic chain so we have

432
00:19:08,299 --> 00:19:11,390
to understand from the original mythical

433
00:19:11,390 --> 00:19:14,450
information how does this fit in with

434
00:19:14,450 --> 00:19:18,080
the belief systems and the prejudices of

435
00:19:18,080 --> 00:19:20,390
the target audience that's actually

436
00:19:20,390 --> 00:19:23,059
believing this information and then and

437
00:19:23,059 --> 00:19:24,679
only then can we craft the narrative

438
00:19:24,679 --> 00:19:27,890
around the truth that will fit into that

439
00:19:27,890 --> 00:19:30,460
logic chain and replace that information

440
00:19:30,460 --> 00:19:33,799
now this is all really scary stuff the

441
00:19:33,799 --> 00:19:35,659
good news is there are positive

442
00:19:35,659 --> 00:19:37,460
intentions here and there's already

443
00:19:37,460 --> 00:19:39,710
we're seeing positive uses of this

444
00:19:39,710 --> 00:19:42,110
technology first and foremost for all

445
00:19:42,110 --> 00:19:44,120
the Star Wars fans out there you might

446
00:19:44,120 --> 00:19:46,279
remember these two faces making some

447
00:19:46,279 --> 00:19:48,230
appearances and some of the more recent

448
00:19:48,230 --> 00:19:53,750
videos so for instance we have the young

449
00:19:53,750 --> 00:19:55,460
Princess Leia or we have Grand Moff

450
00:19:55,460 --> 00:19:57,799
Tarkin both being introduced into these

451
00:19:57,799 --> 00:19:59,990
videos

452
00:19:59,990 --> 00:20:03,290
that was deep fake technology with Ganz

453
00:20:03,290 --> 00:20:05,990
that was used to create them we're also

454
00:20:05,990 --> 00:20:09,380
seeing ganz used in medical imaging ganz

455
00:20:09,380 --> 00:20:13,400
are insanely more accurate than a doctor

456
00:20:13,400 --> 00:20:17,090
looking at an MRI in both identifying a

457
00:20:17,090 --> 00:20:19,760
tumor and predicting the future growth

458
00:20:19,760 --> 00:20:22,400
of that particular tumor based on what's

459
00:20:22,400 --> 00:20:25,850
seen in the MRI we're also seeing

460
00:20:25,850 --> 00:20:29,470
research done around augmented reality

461
00:20:29,470 --> 00:20:32,270
so here we see a form of augmented

462
00:20:32,270 --> 00:20:34,880
reality telehealth where this doctor

463
00:20:34,880 --> 00:20:37,070
isn't even a human being at all that's

464
00:20:37,070 --> 00:20:40,090
just a deep fake generated image of

465
00:20:40,090 --> 00:20:44,390
common pleasing facial features and so

466
00:20:44,390 --> 00:20:46,580
forth that are meant to open up a

467
00:20:46,580 --> 00:20:49,910
particular person to be able to more

468
00:20:49,910 --> 00:20:51,890
easily talk with that doctor and so

469
00:20:51,890 --> 00:20:53,510
forth so you think about automated

470
00:20:53,510 --> 00:20:55,550
telehealth that's what they're looking

471
00:20:55,550 --> 00:20:57,800
at here and then finally returning to

472
00:20:57,800 --> 00:20:59,809
politics this is one that's kind of

473
00:20:59,809 --> 00:21:01,490
impressive but there's some argument as

474
00:21:01,490 --> 00:21:04,760
to how good or bad this really is this

475
00:21:04,760 --> 00:21:07,370
is a political candidate from India and

476
00:21:07,370 --> 00:21:11,210
his constituency involves people that

477
00:21:11,210 --> 00:21:13,309
speak seven or eight different dialects

478
00:21:13,309 --> 00:21:17,030
of the languages in India so he wanted

479
00:21:17,030 --> 00:21:18,440
to be able to connect with all of his

480
00:21:18,440 --> 00:21:20,960
constituents and so what he did was he

481
00:21:20,960 --> 00:21:23,300
recorded a video and then had that video

482
00:21:23,300 --> 00:21:27,230
deep faked both audio and video to now

483
00:21:27,230 --> 00:21:30,530
show him communicating the same message

484
00:21:30,530 --> 00:21:32,809
in all of those different dialects and

485
00:21:32,809 --> 00:21:35,360
indeed it worked he was very popular he

486
00:21:35,360 --> 00:21:37,580
won the election by almost a landslide

487
00:21:37,580 --> 00:21:41,000
and it was a really effective way the

488
00:21:41,000 --> 00:21:42,290
last one I mentioned I don't have an

489
00:21:42,290 --> 00:21:44,179
image here for you is that we also see

490
00:21:44,179 --> 00:21:46,280
this now in terms of course security

491
00:21:46,280 --> 00:21:48,590
we've all heard about deep learning and

492
00:21:48,590 --> 00:21:52,250
AI being used to make better security

493
00:21:52,250 --> 00:21:52,820
tools

494
00:21:52,820 --> 00:21:56,150
well imagine Ganz being able to analyze

495
00:21:56,150 --> 00:21:59,179
source code and identify whether source

496
00:21:59,179 --> 00:22:02,480
code is secure or not not even thinking

497
00:22:02,480 --> 00:22:04,580
about specific types of vulnerabilities

498
00:22:04,580 --> 00:22:06,320
but just being able to develop an

499
00:22:06,320 --> 00:22:09,710
overall model of what secure code looks

500
00:22:09,710 --> 00:22:12,290
like versus what insecure code looks

501
00:22:12,290 --> 00:22:13,309
like

502
00:22:13,309 --> 00:22:15,889
indeed we've also seen research being

503
00:22:15,889 --> 00:22:17,950
done already by McAfee

504
00:22:17,950 --> 00:22:22,940
to use Gans and digital imaging to look

505
00:22:22,940 --> 00:22:25,970
to analyze malware both fireless and

506
00:22:25,970 --> 00:22:28,850
self morphing malware and it's indeed

507
00:22:28,850 --> 00:22:30,830
it's able to identify patterns and it

508
00:22:30,830 --> 00:22:33,649
can pick out malware that otherwise

509
00:22:33,649 --> 00:22:38,299
would go undetected so before I leave

510
00:22:38,299 --> 00:22:41,240
you here I want to hand you this quote

511
00:22:41,240 --> 00:22:43,429
this is from Yuri bisman of he's a

512
00:22:43,429 --> 00:22:47,090
former KGB agent this is from 1984 so

513
00:22:47,090 --> 00:22:50,389
this goes back to when the then Soviet

514
00:22:50,389 --> 00:22:53,990
Union was engaged in some pretty heavy

515
00:22:53,990 --> 00:22:57,249
efforts for mass brainwashing both

516
00:22:57,249 --> 00:22:59,179
domestically within their own country

517
00:22:59,179 --> 00:23:01,340
and in other countries in the Western

518
00:23:01,340 --> 00:23:06,860
world and this is this goes to the heart

519
00:23:06,860 --> 00:23:09,710
of what misinformation is all about and

520
00:23:09,710 --> 00:23:13,240
what the goals of misinformation are

521
00:23:13,240 --> 00:23:14,899
finally if you want some more

522
00:23:14,899 --> 00:23:16,700
information you can follow me on social

523
00:23:16,700 --> 00:23:20,389
media hashtag project deep fake you'll

524
00:23:20,389 --> 00:23:22,789
see lots of different information out

525
00:23:22,789 --> 00:23:24,619
there from me on Twitter or LinkedIn

526
00:23:24,619 --> 00:23:27,320
with those hashtags there's URLs here

527
00:23:27,320 --> 00:23:28,639
that you can follow that will take you

528
00:23:28,639 --> 00:23:30,559
right to those hashtags there's also a

529
00:23:30,559 --> 00:23:33,139
URL I told you I've done some generation

530
00:23:33,139 --> 00:23:36,919
of videos you'll you'll see the good and

531
00:23:36,919 --> 00:23:39,559
the bad how how easy it is to generate

532
00:23:39,559 --> 00:23:41,840
and yet also some of the challenges in

533
00:23:41,840 --> 00:23:44,830
terms of how good or bad that can be I

534
00:23:44,830 --> 00:23:48,619
here's some references just some of the

535
00:23:48,619 --> 00:23:50,899
research papers that I referenced in

536
00:23:50,899 --> 00:23:53,509
this talk today I encourage you to go

537
00:23:53,509 --> 00:23:55,519
check them out I'll also mention one

538
00:23:55,519 --> 00:23:58,490
that's not here which is mi t--'s intro

539
00:23:58,490 --> 00:24:01,100
to deep learning is a course that is

540
00:24:01,100 --> 00:24:04,940
available to be viewed online I I

541
00:24:04,940 --> 00:24:07,610
believe the website is intro to deep

542
00:24:07,610 --> 00:24:09,559
learning com

543
00:24:09,559 --> 00:24:11,269
but if you do a search for that some

544
00:24:11,269 --> 00:24:12,679
great material there if you want to

545
00:24:12,679 --> 00:24:15,019
learn more about not just Gans and how

546
00:24:15,019 --> 00:24:16,759
they apply to deep fix but other deep

547
00:24:16,759 --> 00:24:19,610
learning concepts as well and then

548
00:24:19,610 --> 00:24:22,159
finally I invite you to continue the

549
00:24:22,159 --> 00:24:25,190
conversation with me you see here my

550
00:24:25,190 --> 00:24:27,140
twitter handle my

551
00:24:27,140 --> 00:24:29,540
LinkedIn address also my website that I

552
00:24:29,540 --> 00:24:32,330
promised you I would share with you feel

553
00:24:32,330 --> 00:24:34,790
free to reach out follow me

554
00:24:34,790 --> 00:24:38,600
send me a DM my DMS are always open you

555
00:24:38,600 --> 00:24:40,070
know hit me up through LinkedIn if you

556
00:24:40,070 --> 00:24:42,890
prefer that route always happy to

557
00:24:42,890 --> 00:24:45,350
discuss any security topic certainly

558
00:24:45,350 --> 00:24:48,380
discuss deep fakes there's some you know

559
00:24:48,380 --> 00:24:50,420
new braking technologies I was just

560
00:24:50,420 --> 00:24:51,740
looking at Twitter a little while ago

561
00:24:51,740 --> 00:24:55,400
and somebody's come out with it it's

562
00:24:55,400 --> 00:24:57,970
arguable if it's really deep fake but a

563
00:24:57,970 --> 00:25:00,500
library that's capable of doing some

564
00:25:00,500 --> 00:25:03,500
form of deep faking in real time within

565
00:25:03,500 --> 00:25:06,610
video conferencing software like zoom so

566
00:25:06,610 --> 00:25:08,930
happy to talk to you more about that as

567
00:25:08,930 --> 00:25:10,850
I even do some more analysis into some

568
00:25:10,850 --> 00:25:13,100
of those things so with that I want to

569
00:25:13,100 --> 00:25:16,190
say thank you thank you for me first of

570
00:25:16,190 --> 00:25:19,220
all for attending this talk thank you

571
00:25:19,220 --> 00:25:23,570
from sneaked from all the talks from all

572
00:25:23,570 --> 00:25:26,420
of the sponsors of all the talks for

573
00:25:26,420 --> 00:25:29,540
joining us for this conference this has

574
00:25:29,540 --> 00:25:32,210
been a really important effort for us we

575
00:25:32,210 --> 00:25:34,370
wanted to build something community

576
00:25:34,370 --> 00:25:37,100
driven that brought together developers

577
00:25:37,100 --> 00:25:39,830
and security and operational people all

578
00:25:39,830 --> 00:25:42,530
into one space and for the good of the

579
00:25:42,530 --> 00:25:45,890
community we've raised over $70,000 I

580
00:25:45,890 --> 00:25:47,660
think last number I saw we were

581
00:25:47,660 --> 00:25:49,520
approaching 80,000 and that was a few

582
00:25:49,520 --> 00:25:52,730
hours ago we've raised a lot of money to

583
00:25:52,730 --> 00:25:54,590
help the World Health Organization

584
00:25:54,590 --> 00:25:58,340
combate covet 19 we're helping people

585
00:25:58,340 --> 00:26:00,260
because that's what we do as a community

586
00:26:00,260 --> 00:26:04,160
and so it's so important to us to have

587
00:26:04,160 --> 00:26:06,470
you here the attendance has been

588
00:26:06,470 --> 00:26:10,010
wonderful we cannot thank you enough I

589
00:26:10,010 --> 00:26:12,410
want to also say a great big thank you

590
00:26:12,410 --> 00:26:16,790
to the entire crew of all the talks we

591
00:26:16,790 --> 00:26:20,240
have had stellar performance the streams

592
00:26:20,240 --> 00:26:22,430
have been available very consistently

593
00:26:22,430 --> 00:26:24,230
throughout I think we had one little

594
00:26:24,230 --> 00:26:28,220
blip at one point on one stream we

595
00:26:28,220 --> 00:26:31,490
certainly had some challenges with

596
00:26:31,490 --> 00:26:33,740
technological issues with speakers who

597
00:26:33,740 --> 00:26:35,510
we were able to overcome quickly and

598
00:26:35,510 --> 00:26:38,750
ensure their talks still happened that's

599
00:26:38,750 --> 00:26:40,580
really impressive for something that

600
00:26:40,580 --> 00:26:43,970
we put together in five weeks so to all

601
00:26:43,970 --> 00:26:46,190
the crew out there Sam and Patrick who

602
00:26:46,190 --> 00:26:48,289
spent countless hours putting this all

603
00:26:48,289 --> 00:26:50,360
together everybody who had a part in

604
00:26:50,360 --> 00:26:52,399
making this a success a big thank you

605
00:26:52,399 --> 00:26:54,769
and again thank you to everybody who's

606
00:26:54,769 --> 00:26:58,250
joined us here this evening or this

607
00:26:58,250 --> 00:27:01,070
morning all throughout the this 24-hour

608
00:27:01,070 --> 00:27:03,409
conference or coming up now in twenty

609
00:27:03,409 --> 00:27:08,330
three hours nine nine nine so from me

610
00:27:08,330 --> 00:27:11,179
from everybody here thank you so much

611
00:27:11,179 --> 00:27:15,100
again for being a part of our conference

