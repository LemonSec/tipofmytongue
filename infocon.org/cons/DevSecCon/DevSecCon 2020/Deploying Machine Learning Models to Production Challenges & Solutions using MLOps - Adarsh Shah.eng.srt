1
00:00:00,050 --> 00:00:07,140
other so Howie I'm fine thanks and so

2
00:00:07,140 --> 00:00:09,660
you show is all yours

3
00:00:09,660 --> 00:00:15,030
thank you hello everyone thank you

4
00:00:15,030 --> 00:00:28,460
joining let me share my screen all right

5
00:00:28,460 --> 00:00:31,260
so yeah hello everyone thanks for

6
00:00:31,260 --> 00:00:34,950
joining my session your last name say my

7
00:00:34,950 --> 00:00:37,440
name is Audra I'm an independent

8
00:00:37,440 --> 00:00:39,629
concerned who works with various clients

9
00:00:39,629 --> 00:00:42,090
helping them with engineering leadership

10
00:00:42,090 --> 00:00:44,270
coaching and hands-on architecture needs

11
00:00:44,270 --> 00:00:46,950
you can find more info about me on my

12
00:00:46,950 --> 00:00:49,500
website so today I am going to talk

13
00:00:49,500 --> 00:00:51,149
about deploying machine learning models

14
00:00:51,149 --> 00:00:53,190
to production what are the various

15
00:00:53,190 --> 00:00:54,809
challenges that you face there are

16
00:00:54,809 --> 00:00:57,510
machine learning specific and solutions

17
00:00:57,510 --> 00:01:02,300
that I've found useful are using NLRB's

18
00:01:02,570 --> 00:01:05,220
first let's talk about difference

19
00:01:05,220 --> 00:01:06,930
between traditional software engineering

20
00:01:06,930 --> 00:01:08,760
and machine learning so as you can see

21
00:01:08,760 --> 00:01:11,520
here that traditional software

22
00:01:11,520 --> 00:01:14,939
development includes writing some

23
00:01:14,939 --> 00:01:18,570
program providing some data and then you

24
00:01:18,570 --> 00:01:20,490
get the results out of it if there are

25
00:01:20,490 --> 00:01:24,689
changes in the program if their changes

26
00:01:24,689 --> 00:01:26,670
need to make you need to make those

27
00:01:26,670 --> 00:01:30,240
changes in the logic of the program or

28
00:01:30,240 --> 00:01:32,579
in the data and the then you get

29
00:01:32,579 --> 00:01:36,329
different results and in the machine

30
00:01:36,329 --> 00:01:39,780
learning model you have two steps here

31
00:01:39,780 --> 00:01:42,180
are the first step you actually provide

32
00:01:42,180 --> 00:01:44,790
the desired results that you want and

33
00:01:44,790 --> 00:01:46,979
the training data set and I'll talk more

34
00:01:46,979 --> 00:01:49,649
about training a little later but

35
00:01:49,649 --> 00:01:52,439
basically you use the desired results

36
00:01:52,439 --> 00:01:55,259
and the training data and the training

37
00:01:55,259 --> 00:01:57,659
code to generate the program in the

38
00:01:57,659 --> 00:02:00,090
training step and then use that program

39
00:02:00,090 --> 00:02:02,340
in the next step which is the model

40
00:02:02,340 --> 00:02:07,020
serving phase where you use that program

41
00:02:07,020 --> 00:02:10,800
use live data in production and then get

42
00:02:10,800 --> 00:02:13,920
the results which sounds pretty similar

43
00:02:13,920 --> 00:02:16,459
what a traditional software development

44
00:02:16,459 --> 00:02:19,440
does ii state i am talking about model

45
00:02:19,440 --> 00:02:22,560
serving i but as you can see the program

46
00:02:22,560 --> 00:02:25,590
is actually generated in the training

47
00:02:25,590 --> 00:02:29,280
step so even though they are

48
00:02:29,280 --> 00:02:32,400
fundamentally different software

49
00:02:32,400 --> 00:02:33,750
development traditional software

50
00:02:33,750 --> 00:02:36,600
development and machine learning as most

51
00:02:36,600 --> 00:02:38,370
of the best practice that you can use

52
00:02:38,370 --> 00:02:40,890
for software development can be used in

53
00:02:40,890 --> 00:02:44,880
machine learning faith as well let's

54
00:02:44,880 --> 00:02:46,739
start by understanding a typical machine

55
00:02:46,739 --> 00:02:49,860
learning workflow so as you can see

56
00:02:49,860 --> 00:02:51,630
there are three different parts to it

57
00:02:51,630 --> 00:02:54,600
first is data management second is

58
00:02:54,600 --> 00:02:56,519
experimentation and then production

59
00:02:56,519 --> 00:02:59,459
deployment so in the Dayman management

60
00:02:59,459 --> 00:03:01,950
phase you are looking at acquiring data

61
00:03:01,950 --> 00:03:05,160
and data scientists spend a lot of time

62
00:03:05,160 --> 00:03:07,470
because data is really important and

63
00:03:07,470 --> 00:03:10,140
having the right data and right amount

64
00:03:10,140 --> 00:03:12,540
of data is really important to get the

65
00:03:12,540 --> 00:03:14,610
training right and get your algorithm

66
00:03:14,610 --> 00:03:16,110
that was generated in the training phase

67
00:03:16,110 --> 00:03:18,690
right and then you prepare for the data

68
00:03:18,690 --> 00:03:20,790
so if you have to massage the date and

69
00:03:20,790 --> 00:03:23,459
make any changes i can give you one

70
00:03:23,459 --> 00:03:25,890
example in in one of the projects we had

71
00:03:25,890 --> 00:03:28,040
we were getting images from the client

72
00:03:28,040 --> 00:03:31,019
but the labels of the image were in a

73
00:03:31,019 --> 00:03:32,700
different language so we had to do the

74
00:03:32,700 --> 00:03:35,430
translation before we could use it in

75
00:03:35,430 --> 00:03:40,410
the final phase and then once you're

76
00:03:40,410 --> 00:03:42,900
done preparing the data or you go to the

77
00:03:42,900 --> 00:03:44,970
experiment and experimentation phase

78
00:03:44,970 --> 00:03:47,700
where data scientists spend a lot of

79
00:03:47,700 --> 00:03:50,790
time then being the model they use

80
00:03:50,790 --> 00:03:54,329
various algorithms various model

81
00:03:54,329 --> 00:03:57,720
architectures and then they also try

82
00:03:57,720 --> 00:03:59,489
different data sets different version of

83
00:03:59,489 --> 00:04:02,910
data and then they train the model train

84
00:04:02,910 --> 00:04:05,220
and see if it's meeting their

85
00:04:05,220 --> 00:04:07,230
expectations if it's hitting their

86
00:04:07,230 --> 00:04:10,400
criteria including

87
00:04:10,400 --> 00:04:13,650
various ways of percentage of acceptance

88
00:04:13,650 --> 00:04:17,220
right haters once they do that they go

89
00:04:17,220 --> 00:04:19,978
to the production deployment phase where

90
00:04:19,978 --> 00:04:23,250
the model is free and and is deployed in

91
00:04:23,250 --> 00:04:25,890
production and it starts serving life

92
00:04:25,890 --> 00:04:27,210
traffic

93
00:04:27,210 --> 00:04:31,020
once that happens you constantly keep on

94
00:04:31,020 --> 00:04:34,039
checking for accuracy of the model

95
00:04:34,039 --> 00:04:36,600
accuracy is pretty important because as

96
00:04:36,600 --> 00:04:38,940
I mentioned that the Agra rhythm is

97
00:04:38,940 --> 00:04:41,490
generated during the training phase if

98
00:04:41,490 --> 00:04:44,009
things change in productions for example

99
00:04:44,009 --> 00:04:46,949
you start supporting new products or you

100
00:04:46,949 --> 00:04:49,229
your environment in production changes a

101
00:04:49,229 --> 00:04:51,660
bit you need to train the model again so

102
00:04:51,660 --> 00:04:53,070
as we can see there's an arrow going

103
00:04:53,070 --> 00:04:56,880
back for retraining or any code changes

104
00:04:56,880 --> 00:04:59,370
that you need to make so all of that

105
00:04:59,370 --> 00:05:02,070
needs to happen and you need to go back

106
00:05:02,070 --> 00:05:05,490
to the experimentation phase and then

107
00:05:05,490 --> 00:05:07,889
you retrain the model and then try it

108
00:05:07,889 --> 00:05:08,370
again

109
00:05:08,370 --> 00:05:10,740
I'll give you an example here which

110
00:05:10,740 --> 00:05:15,570
we'll use going forward so one of the

111
00:05:15,570 --> 00:05:17,789
examples of things that we have looked

112
00:05:17,789 --> 00:05:21,030
at is identifying defects in assembly

113
00:05:21,030 --> 00:05:24,509
lines so let's say you're making mobile

114
00:05:24,509 --> 00:05:29,160
phones and in the assembly line you have

115
00:05:29,160 --> 00:05:32,280
a QA phase Quality Assurance phase where

116
00:05:32,280 --> 00:05:34,830
you want to make sure that the items are

117
00:05:34,830 --> 00:05:37,380
not defective in this case mobile phones

118
00:05:37,380 --> 00:05:39,780
and you can create machine learning

119
00:05:39,780 --> 00:05:43,110
models that actually look at images of

120
00:05:43,110 --> 00:05:44,699
the mobile phones that are coming in

121
00:05:44,699 --> 00:05:47,130
from different angles and then they look

122
00:05:47,130 --> 00:05:49,710
at any defects within that so that a

123
00:05:49,710 --> 00:05:52,620
human doesn't have to do that and we can

124
00:05:52,620 --> 00:05:54,750
automate those things and humans can

125
00:05:54,750 --> 00:05:57,479
spend time doing more productive things

126
00:05:57,479 --> 00:06:00,090
because this one is more repetitive and

127
00:06:00,090 --> 00:06:03,720
something that computers can can handle

128
00:06:03,720 --> 00:06:09,180
easily so here's a fun fact about model

129
00:06:09,180 --> 00:06:10,470
training since we are talking about

130
00:06:10,470 --> 00:06:14,070
models have you ever helped train a

131
00:06:14,070 --> 00:06:18,240
model guess what everybody has they just

132
00:06:18,240 --> 00:06:19,889
don't realize it

133
00:06:19,889 --> 00:06:22,800
have you seen captures have you ever

134
00:06:22,800 --> 00:06:25,800
filled up forms while filling up forms

135
00:06:25,800 --> 00:06:27,900
on various websites to verify that

136
00:06:27,900 --> 00:06:29,610
you're a human have you filled up this

137
00:06:29,610 --> 00:06:31,860
capture where you have two different

138
00:06:31,860 --> 00:06:37,020
words this is basically Google using you

139
00:06:37,020 --> 00:06:40,490
to label their data and labeling data

140
00:06:40,490 --> 00:06:43,030
means you are actually helping identify

141
00:06:43,030 --> 00:06:46,819
the image with actual words and then

142
00:06:46,819 --> 00:06:48,590
they use that during the training phase

143
00:06:48,590 --> 00:06:51,349
to train the models similarly if you

144
00:06:51,349 --> 00:06:53,360
have seen this where you are actually

145
00:06:53,360 --> 00:06:56,360
identifying let's say street signs it's

146
00:06:56,360 --> 00:06:59,479
actually used by Google to train their

147
00:06:59,479 --> 00:07:01,280
module for self writing and other

148
00:07:01,280 --> 00:07:04,069
capabilities so just something I wanted

149
00:07:04,069 --> 00:07:05,930
to share a fun fact which I knew last

150
00:07:05,930 --> 00:07:10,520
year one of known truths of the machine

151
00:07:10,520 --> 00:07:12,530
learning world is that it takes a lot

152
00:07:12,530 --> 00:07:14,659
longer to deploy machine learning models

153
00:07:14,659 --> 00:07:16,630
to production than to develop it

154
00:07:16,630 --> 00:07:19,220
according to the paper hidden technical

155
00:07:19,220 --> 00:07:21,919
that in machine learning systems only a

156
00:07:21,919 --> 00:07:24,020
small fraction of real-world machine

157
00:07:24,020 --> 00:07:26,780
learning systems are composed of ML code

158
00:07:26,780 --> 00:07:29,870
as you can see it in the small black box

159
00:07:29,870 --> 00:07:32,750
in the middle the required surrounding

160
00:07:32,750 --> 00:07:34,729
infrastructure and all the other

161
00:07:34,729 --> 00:07:37,490
supporting aspects they take vast they

162
00:07:37,490 --> 00:07:41,509
really vast and complex so so you need

163
00:07:41,509 --> 00:07:43,370
to focus a lot of time making those

164
00:07:43,370 --> 00:07:45,280
things right for your machine learning

165
00:07:45,280 --> 00:07:49,729
models to work in production so now

166
00:07:49,729 --> 00:07:51,650
let's talk about some unique challenges

167
00:07:51,650 --> 00:07:56,020
that our machine learning specific then

168
00:07:56,020 --> 00:07:58,099
usually what you find in traditional

169
00:07:58,099 --> 00:08:01,400
software development first one being

170
00:08:01,400 --> 00:08:04,009
date management as I mentioned earlier

171
00:08:04,009 --> 00:08:06,620
that data is pretty important for the

172
00:08:06,620 --> 00:08:09,409
training phase and having the data right

173
00:08:09,409 --> 00:08:12,409
and and the right amount of data is

174
00:08:12,409 --> 00:08:15,669
pretty important so as you can see

175
00:08:15,669 --> 00:08:19,819
usually unique large data sets which is

176
00:08:19,819 --> 00:08:21,949
usually hundreds of gigabytes or even

177
00:08:21,949 --> 00:08:25,400
larger in size so you have to keep that

178
00:08:25,400 --> 00:08:26,990
in mind when you are building those

179
00:08:26,990 --> 00:08:29,919
systems to deploy and in those models

180
00:08:29,919 --> 00:08:33,020
data location is another thing you might

181
00:08:33,020 --> 00:08:36,020
have data in some cloud provider on an

182
00:08:36,020 --> 00:08:38,870
s3 bucket let's say or you have it on on

183
00:08:38,870 --> 00:08:42,409
M location or at some edge location if

184
00:08:42,409 --> 00:08:44,839
you're working with retail clients your

185
00:08:44,839 --> 00:08:47,600
data might actually be a closer to where

186
00:08:47,600 --> 00:08:49,670
the store is so you have to keep that in

187
00:08:49,670 --> 00:08:51,290
mind when you're building the system

188
00:08:51,290 --> 00:08:54,260
moving large amount of data

189
00:08:54,260 --> 00:08:56,720
across the wire is expensive and

190
00:08:56,720 --> 00:08:59,030
time-consuming so you might want to run

191
00:08:59,030 --> 00:09:01,910
your training or even a model serving

192
00:09:01,910 --> 00:09:04,300
closer to where the data resides and

193
00:09:04,300 --> 00:09:07,010
then security and compliance obviously

194
00:09:07,010 --> 00:09:09,440
when you talk about data you have to

195
00:09:09,440 --> 00:09:11,690
think about security and compliance this

196
00:09:11,690 --> 00:09:13,490
is not necessarily machine learning

197
00:09:13,490 --> 00:09:16,970
specific challenge but since data is of

198
00:09:16,970 --> 00:09:19,370
key and there's a lot of data involved

199
00:09:19,370 --> 00:09:21,860
in machine learning these are important

200
00:09:21,860 --> 00:09:25,610
aspects next one being constant research

201
00:09:25,610 --> 00:09:28,520
and experimentation as I mentioned data

202
00:09:28,520 --> 00:09:29,900
scientists spend a lot of time

203
00:09:29,900 --> 00:09:32,510
experimenting they try different data

204
00:09:32,510 --> 00:09:34,430
sets different versions of blessed's

205
00:09:34,430 --> 00:09:37,100
they try different model architecture to

206
00:09:37,100 --> 00:09:39,440
see what works and fits their needs they

207
00:09:39,440 --> 00:09:41,900
usually use Jupiter notebook which is

208
00:09:41,900 --> 00:09:44,900
one of the tools for experimentation and

209
00:09:44,900 --> 00:09:46,940
writing code and while they are doing

210
00:09:46,940 --> 00:09:49,640
that their focus is not code quality

211
00:09:49,640 --> 00:09:52,100
their focus is doing the research and

212
00:09:52,100 --> 00:09:54,230
experimentation and seeing what works

213
00:09:54,230 --> 00:09:55,610
for them and what doesn't

214
00:09:55,610 --> 00:09:58,850
so the code quality during that phase is

215
00:09:58,850 --> 00:10:01,310
actually not as good so you have to keep

216
00:10:01,310 --> 00:10:04,370
that in mind before you take that model

217
00:10:04,370 --> 00:10:06,140
and deploy to production

218
00:10:06,140 --> 00:10:08,420
and in any other code that's surrounding

219
00:10:08,420 --> 00:10:12,530
that so yeah next one being training

220
00:10:12,530 --> 00:10:14,600
process this is unique through machine

221
00:10:14,600 --> 00:10:17,590
learning and usually training takes time

222
00:10:17,590 --> 00:10:19,520
let me give you an example

223
00:10:19,520 --> 00:10:23,450
Tesla's autopilot a neural network it

224
00:10:23,450 --> 00:10:27,620
takes about 70,000 Jeep two hours to

225
00:10:27,620 --> 00:10:31,880
train the full model now it doesn't take

226
00:10:31,880 --> 00:10:34,610
70,000 hours to train them they just

227
00:10:34,610 --> 00:10:37,520
paralyze that with multiple GPUs but

228
00:10:37,520 --> 00:10:40,280
still you can still see the gravity and

229
00:10:40,280 --> 00:10:42,560
and and how long it takes to actually

230
00:10:42,560 --> 00:10:46,880
train it I have had cases where we had -

231
00:10:46,880 --> 00:10:49,940
it took hours and some some cases 2 to 3

232
00:10:49,940 --> 00:10:53,420
days to train the full model and we had

233
00:10:53,420 --> 00:10:55,160
to make sure that we support that in our

234
00:10:55,160 --> 00:10:55,880
platform

235
00:10:55,880 --> 00:10:59,180
I think produce ability you train with

236
00:10:59,180 --> 00:11:01,790
different data sets change few things

237
00:11:01,790 --> 00:11:03,950
you should be able to reproduce the

238
00:11:03,950 --> 00:11:06,410
training process and then compare the

239
00:11:06,410 --> 00:11:07,939
differences so that you know

240
00:11:07,939 --> 00:11:10,759
one better and which ones are not so the

241
00:11:10,759 --> 00:11:13,699
producer reproducibility of this step is

242
00:11:13,699 --> 00:11:16,970
really important and you need to be able

243
00:11:16,970 --> 00:11:18,979
to track experiment as I mentioned that

244
00:11:18,979 --> 00:11:20,959
if you try with different data sets and

245
00:11:20,959 --> 00:11:23,749
different combinations you want to be

246
00:11:23,749 --> 00:11:25,699
tracking those and making sure that you

247
00:11:25,699 --> 00:11:27,649
can go back to the previous experiments

248
00:11:27,649 --> 00:11:31,099
if you had to and then retraining the

249
00:11:31,099 --> 00:11:33,289
models if let's say the performance and

250
00:11:33,289 --> 00:11:36,199
accuracy of the model reduces you should

251
00:11:36,199 --> 00:11:40,879
be able to train them again there are

252
00:11:40,879 --> 00:11:43,489
specific infrastructure requirements for

253
00:11:43,489 --> 00:11:46,189
machine learning especially GPU and

254
00:11:46,189 --> 00:11:49,099
high-density course if you as graphics

255
00:11:49,099 --> 00:11:53,899
processing unit what a CPU does is it

256
00:11:53,899 --> 00:11:56,149
works together with GPU to increase the

257
00:11:56,149 --> 00:11:58,249
throughput of data and the number of

258
00:11:58,249 --> 00:11:59,929
congruent calculations within an

259
00:11:59,929 --> 00:12:02,839
application so as I mentioned previously

260
00:12:02,839 --> 00:12:05,929
that if it takes a lot longer to train

261
00:12:05,929 --> 00:12:09,259
the model and even in cases in some

262
00:12:09,259 --> 00:12:11,389
cases in in production way of serving

263
00:12:11,389 --> 00:12:13,699
the model it takes longer so having CPU

264
00:12:13,699 --> 00:12:16,039
helps you run concurrent

265
00:12:16,039 --> 00:12:19,399
calculation and reduce that time so GPUs

266
00:12:19,399 --> 00:12:21,049
were originally designed to create

267
00:12:21,049 --> 00:12:23,629
images for computer graphics and video

268
00:12:23,629 --> 00:12:27,139
game consoles but since 2010 GPUs can

269
00:12:27,139 --> 00:12:29,809
also be used to accelerate calculations

270
00:12:29,809 --> 00:12:31,879
evolving massive amounts of data which

271
00:12:31,879 --> 00:12:35,059
is exactly our use case in in in most

272
00:12:35,059 --> 00:12:38,569
machine learning use cases and then

273
00:12:38,569 --> 00:12:42,919
elasticity since training needs to

274
00:12:42,919 --> 00:12:44,959
happen for a certain period of time and

275
00:12:44,959 --> 00:12:48,679
then you don't need those resources even

276
00:12:48,679 --> 00:12:51,489
inference you might want to increase

277
00:12:51,489 --> 00:12:54,889
elasticity and support more traffic in

278
00:12:54,889 --> 00:12:57,709
certain cases so having an option to

279
00:12:57,709 --> 00:12:59,989
bring provision and deprovision your

280
00:12:59,989 --> 00:13:04,339
infrastructure makes you like it helps

281
00:13:04,339 --> 00:13:06,739
you reduce your cost because especially

282
00:13:06,739 --> 00:13:07,939
if you're running in cloud you're

283
00:13:07,939 --> 00:13:09,709
actually playing by the hour

284
00:13:09,709 --> 00:13:12,409
and these GPU nodes are really expensive

285
00:13:12,409 --> 00:13:16,999
so having that automation to provision

286
00:13:16,999 --> 00:13:18,859
and then be provision once you're done

287
00:13:18,859 --> 00:13:21,560
your infrastructure will have really

288
00:13:21,560 --> 00:13:25,520
helps reduce cost we always whenever I

289
00:13:25,520 --> 00:13:27,610
look at a proposal and work with clients

290
00:13:27,610 --> 00:13:31,430
I always look at this aspect really

291
00:13:31,430 --> 00:13:34,700
closely because depending on what's the

292
00:13:34,700 --> 00:13:37,810
uptime and how much traffic you might

293
00:13:37,810 --> 00:13:40,670
you should look at optimizing how much

294
00:13:40,670 --> 00:13:44,510
GPU you need in the cloud and then edge

295
00:13:44,510 --> 00:13:47,029
devices especially in computer vision

296
00:13:47,029 --> 00:13:49,310
where you have a camera and where you

297
00:13:49,310 --> 00:13:51,860
are processing images so in in our case

298
00:13:51,860 --> 00:13:54,770
that we talked about for detecting

299
00:13:54,770 --> 00:13:57,140
defects in the assembly line

300
00:13:57,140 --> 00:14:00,380
we usually you usually have cameras and

301
00:14:00,380 --> 00:14:02,779
that camera take images and you want to

302
00:14:02,779 --> 00:14:05,750
process those right away so there are

303
00:14:05,750 --> 00:14:08,510
devices by Nvidia and other companies

304
00:14:08,510 --> 00:14:11,240
that have GPU support so for example and

305
00:14:11,240 --> 00:14:14,330
we Jets and nano is one of the devices I

306
00:14:14,330 --> 00:14:17,450
have used in the past which is like

307
00:14:17,450 --> 00:14:19,640
Raspberry Pi but has GPUs so that you

308
00:14:19,640 --> 00:14:22,160
can run machine learning workloads so

309
00:14:22,160 --> 00:14:25,520
I've used that for model serving so

310
00:14:25,520 --> 00:14:27,680
basically taking there's a camera

311
00:14:27,680 --> 00:14:29,960
running you take those images of items

312
00:14:29,960 --> 00:14:33,560
and then process it right there so your

313
00:14:33,560 --> 00:14:36,110
Nvidia jets and nano is attached to your

314
00:14:36,110 --> 00:14:40,940
camera testing now obviously in

315
00:14:40,940 --> 00:14:42,650
traditional software dortmund you you

316
00:14:42,650 --> 00:14:45,140
you should have testing as well but

317
00:14:45,140 --> 00:14:48,440
there are some special tests that apply

318
00:14:48,440 --> 00:14:50,570
to machine learning and I'm gonna be

319
00:14:50,570 --> 00:14:52,460
talking about those so the validation is

320
00:14:52,460 --> 00:14:55,940
one where in your data pipeline and we

321
00:14:55,940 --> 00:14:59,120
looked at that in detail next but you

322
00:14:59,120 --> 00:15:00,830
should be validating your data whenever

323
00:15:00,830 --> 00:15:02,930
you get you might be getting data from

324
00:15:02,930 --> 00:15:06,350
different sources so so have a self feel

325
00:15:06,350 --> 00:15:08,060
you're validating your data that it

326
00:15:08,060 --> 00:15:11,950
meets the schema and other requirements

327
00:15:11,950 --> 00:15:17,660
model bias and fitness having these

328
00:15:17,660 --> 00:15:20,120
tests and making sure that your model is

329
00:15:20,120 --> 00:15:24,950
fair and and without bias bias for race

330
00:15:24,950 --> 00:15:28,040
gender age income groups and those

331
00:15:28,040 --> 00:15:29,780
things so make sure that those things

332
00:15:29,780 --> 00:15:33,920
are not your model is bias and fair it

333
00:15:33,920 --> 00:15:35,509
is very important

334
00:15:35,509 --> 00:15:37,519
to make sure that you're not

335
00:15:37,519 --> 00:15:40,519
discriminating against a certain group

336
00:15:40,519 --> 00:15:44,600
and then model accuracy so validating

337
00:15:44,600 --> 00:15:46,730
whether your model is accurate and not

338
00:15:46,730 --> 00:15:49,670
just during the training process or

339
00:15:49,670 --> 00:15:51,679
after the framing process but also when

340
00:15:51,679 --> 00:15:53,689
you're serving the model so make sure

341
00:15:53,689 --> 00:15:56,600
that if anything changes you go back and

342
00:15:56,600 --> 00:15:58,999
retrain the model and in improve

343
00:15:58,999 --> 00:15:59,860
accuracy

344
00:15:59,860 --> 00:16:03,799
and last but not the least is dependency

345
00:16:03,799 --> 00:16:05,929
huh now this is something from

346
00:16:05,929 --> 00:16:08,649
experience working with various

347
00:16:08,649 --> 00:16:12,199
libraries that that you need to work

348
00:16:12,199 --> 00:16:15,559
with GPU and and and on some of these

349
00:16:15,559 --> 00:16:18,679
hardware's ah yeah it is believe me I

350
00:16:18,679 --> 00:16:20,509
have spent a lot of time working with

351
00:16:20,509 --> 00:16:23,689
these and and and certain dependencies

352
00:16:23,689 --> 00:16:26,149
only work with another dependency

353
00:16:26,149 --> 00:16:30,139
certain version CUDA being one so CUDA

354
00:16:30,139 --> 00:16:32,689
is a library from Nvidia that helps you

355
00:16:32,689 --> 00:16:35,899
paralyze some of this processing and it

356
00:16:35,899 --> 00:16:39,799
is pain to work with and there are the

357
00:16:39,799 --> 00:16:41,629
dependencies as well in that ecosystem

358
00:16:41,629 --> 00:16:44,179
that are painful so keep that in mind

359
00:16:44,179 --> 00:16:48,559
helps you build a solution that that

360
00:16:48,559 --> 00:16:52,970
eases the death pain and then if you

361
00:16:52,970 --> 00:16:54,470
especially if you're working with edge

362
00:16:54,470 --> 00:16:56,899
devices they usually run on ARM

363
00:16:56,899 --> 00:16:59,239
architecture not x86 which is what we

364
00:16:59,239 --> 00:17:01,939
are used to and the dependencies for ARM

365
00:17:01,939 --> 00:17:05,179
architecture are different and then your

366
00:17:05,179 --> 00:17:08,029
x86 and and and there are ways you can

367
00:17:08,029 --> 00:17:10,539
do stuff like in darker you can actually

368
00:17:10,539 --> 00:17:13,929
build stuff for ARM architecture in your

369
00:17:13,929 --> 00:17:17,269
x86 machine but you have to do work

370
00:17:17,269 --> 00:17:19,638
could do that and make it happen so

371
00:17:19,638 --> 00:17:21,500
keeping that in mind and learning is

372
00:17:21,500 --> 00:17:27,169
really important so yeah here which we

373
00:17:27,169 --> 00:17:28,459
talked about all the challenges that

374
00:17:28,459 --> 00:17:31,490
I've seen and and let's now talk about

375
00:17:31,490 --> 00:17:35,779
solutions and well this is this is one

376
00:17:35,779 --> 00:17:38,120
of the buzzwords where people use ml

377
00:17:38,120 --> 00:17:42,139
offs what exactly is that well ops it is

378
00:17:42,139 --> 00:17:44,299
nothing but applying the DevOps

379
00:17:44,299 --> 00:17:46,250
principles and practices that we have

380
00:17:46,250 --> 00:17:48,020
used in traditional software to oisin

381
00:17:48,020 --> 00:17:48,920
learning

382
00:17:48,920 --> 00:17:51,050
but since as I mentioned that these

383
00:17:51,050 --> 00:17:53,120
challenges are some challenges they

384
00:17:53,120 --> 00:17:55,430
unique to machine learning Emma laughs

385
00:17:55,430 --> 00:17:58,820
helps you some of those challenges so as

386
00:17:58,820 --> 00:18:01,370
we talked about talked about people

387
00:18:01,370 --> 00:18:03,460
process and technology in DevOps

388
00:18:03,460 --> 00:18:05,780
for traditional software same thing

389
00:18:05,780 --> 00:18:09,800
applies here but what is different is is

390
00:18:09,800 --> 00:18:11,780
what we will talk about next

391
00:18:11,780 --> 00:18:14,810
in terms of solution so there are

392
00:18:14,810 --> 00:18:16,910
certain roles that are different you

393
00:18:16,910 --> 00:18:19,430
look at ml researchers who's focused on

394
00:18:19,430 --> 00:18:22,310
doing research experimentation and they

395
00:18:22,310 --> 00:18:24,920
they don't focus on writing production

396
00:18:24,920 --> 00:18:26,840
quality code but they focus on getting

397
00:18:26,840 --> 00:18:29,690
the research and experimentation side of

398
00:18:29,690 --> 00:18:32,330
things right you have an ml engineer

399
00:18:32,330 --> 00:18:35,360
who's basically in green machine

400
00:18:35,360 --> 00:18:36,890
learning researchers and software

401
00:18:36,890 --> 00:18:39,740
development so they have a machine

402
00:18:39,740 --> 00:18:41,360
learning knowledge but also good

403
00:18:41,360 --> 00:18:42,950
software development practice knowledge

404
00:18:42,950 --> 00:18:47,060
where they help bridge the gap and and

405
00:18:47,060 --> 00:18:49,100
production is the code and then data

406
00:18:49,100 --> 00:18:51,110
engineer who's mainly focused on getting

407
00:18:51,110 --> 00:18:53,390
the data right and right in pipelines

408
00:18:53,390 --> 00:18:57,080
and ETL and other things to get get good

409
00:18:57,080 --> 00:19:00,880
data for testing and even inference and

410
00:19:00,880 --> 00:19:04,250
then I'm a half's engineer now I was

411
00:19:04,250 --> 00:19:06,440
right there office engineer but since

412
00:19:06,440 --> 00:19:09,380
we're talking about Emma I just call it

413
00:19:09,380 --> 00:19:12,920
an office engineer you need to

414
00:19:12,920 --> 00:19:14,990
understand that there are practices and

415
00:19:14,990 --> 00:19:16,730
principles but also have some

416
00:19:16,730 --> 00:19:18,290
understanding of machine learning and

417
00:19:18,290 --> 00:19:21,200
the challenge is standard drinks so yeah

418
00:19:21,200 --> 00:19:24,080
like those are the roles and then let's

419
00:19:24,080 --> 00:19:25,790
talk about some team structure

420
00:19:25,790 --> 00:19:29,390
considerations so as in traditional

421
00:19:29,390 --> 00:19:31,460
software development cross-functional

422
00:19:31,460 --> 00:19:34,850
teams work best where you have a team

423
00:19:34,850 --> 00:19:38,120
that includes your researcher your data

424
00:19:38,120 --> 00:19:40,420
engineer your machine learning engineer

425
00:19:40,420 --> 00:19:43,490
even your DevOps or m/l ops folks and

426
00:19:43,490 --> 00:19:45,530
software engineer bringing them all

427
00:19:45,530 --> 00:19:47,900
together and building a cross-functional

428
00:19:47,900 --> 00:19:50,180
team that is focused on building the

429
00:19:50,180 --> 00:19:52,430
product right and having all the

430
00:19:52,430 --> 00:19:55,630
different components is always great so

431
00:19:55,630 --> 00:19:57,800
the challenges that we talked about

432
00:19:57,800 --> 00:20:00,230
about support for constant research and

433
00:20:00,230 --> 00:20:01,840
experimentation and

434
00:20:01,840 --> 00:20:03,909
then the code quality and things that we

435
00:20:03,909 --> 00:20:06,190
talked about if the team is

436
00:20:06,190 --> 00:20:07,480
cross-functional there is better

437
00:20:07,480 --> 00:20:09,640
collaboration and they have common goals

438
00:20:09,640 --> 00:20:12,730
and incentives even the infrastructure

439
00:20:12,730 --> 00:20:14,409
requirements are unique and challenging

440
00:20:14,409 --> 00:20:16,779
so having someone who understands

441
00:20:16,779 --> 00:20:18,760
infrastructure in your team also helps

442
00:20:18,760 --> 00:20:22,450
so I would recommend building a

443
00:20:22,450 --> 00:20:25,179
cross-functional team if you you have

444
00:20:25,179 --> 00:20:26,260
you're working with machine learning

445
00:20:26,260 --> 00:20:30,130
projects but in cases where you can't

446
00:20:30,130 --> 00:20:32,529
have that and you need to have a

447
00:20:32,529 --> 00:20:34,750
separate data science team which is okay

448
00:20:34,750 --> 00:20:37,330
as well but you need to make sure that

449
00:20:37,330 --> 00:20:40,299
you promote collaboration between teams

450
00:20:40,299 --> 00:20:42,520
I have seen places where the data

451
00:20:42,520 --> 00:20:44,620
science team is doing their own thing

452
00:20:44,620 --> 00:20:46,990
and don't collaborate closely with other

453
00:20:46,990 --> 00:20:49,600
teams and that causes issues because

454
00:20:49,600 --> 00:20:51,580
they're working on things that the

455
00:20:51,580 --> 00:20:54,399
platform can't support and there's not

456
00:20:54,399 --> 00:20:56,110
enough collaboration so even the

457
00:20:56,110 --> 00:20:58,510
incentives for various including data

458
00:20:58,510 --> 00:21:01,059
science should be should be in a way

459
00:21:01,059 --> 00:21:04,570
where they go towards same instant

460
00:21:04,570 --> 00:21:08,710
incentives towards the end then this is

461
00:21:08,710 --> 00:21:10,659
basically a platforming training team

462
00:21:10,659 --> 00:21:13,990
but focused on machine learning so as I

463
00:21:13,990 --> 00:21:18,039
said that if you have a GPU and other

464
00:21:18,039 --> 00:21:20,260
needs which most of the ml projects have

465
00:21:20,260 --> 00:21:22,899
you might want to have a focus team that

466
00:21:22,899 --> 00:21:25,539
that fills this platform and I'll talk

467
00:21:25,539 --> 00:21:29,080
about it in a bit about that as well so

468
00:21:29,080 --> 00:21:32,110
here's a typical data pipeline you have

469
00:21:32,110 --> 00:21:34,149
a data source you validate the data the

470
00:21:34,149 --> 00:21:37,210
test that I talked about previously then

471
00:21:37,210 --> 00:21:40,240
you acquire the data and then and then

472
00:21:40,240 --> 00:21:42,630
you prepare the data before it can be

473
00:21:42,630 --> 00:21:46,450
used for testing and then you apply that

474
00:21:46,450 --> 00:21:48,549
and you might have the resources you

475
00:21:48,549 --> 00:21:49,840
might be getting from different cloud

476
00:21:49,840 --> 00:21:53,049
providers open source data that's out

477
00:21:53,049 --> 00:21:55,750
out there open data and other things so

478
00:21:55,750 --> 00:21:57,880
having a pipeline through this and

479
00:21:57,880 --> 00:22:00,070
automation that helps you build the

480
00:22:00,070 --> 00:22:02,289
training data set is really important

481
00:22:02,289 --> 00:22:04,690
and then that becomes an input in your

482
00:22:04,690 --> 00:22:09,159
training process now let's look at the

483
00:22:09,159 --> 00:22:11,230
training pipeline which is another

484
00:22:11,230 --> 00:22:13,690
crucial and a lot various challenges

485
00:22:13,690 --> 00:22:15,549
that we talked about look

486
00:22:15,549 --> 00:22:18,429
addressing here so sisters let's say

487
00:22:18,429 --> 00:22:21,279
your data scientist or someone who wants

488
00:22:21,279 --> 00:22:23,200
to schedule a training uses a tubular

489
00:22:23,200 --> 00:22:26,529
notebook or a UI command to you I are a

490
00:22:26,529 --> 00:22:29,440
commando schedule the training you have

491
00:22:29,440 --> 00:22:31,149
your training code that's coming from

492
00:22:31,149 --> 00:22:33,879
your repo you do your typical CI with

493
00:22:33,879 --> 00:22:36,909
that code as well and then you provision

494
00:22:36,909 --> 00:22:39,129
and a training environment as you can

495
00:22:39,129 --> 00:22:41,259
see on the right you have infra

496
00:22:41,259 --> 00:22:44,309
provisioning automation with GPU support

497
00:22:44,309 --> 00:22:47,649
what I have used in the past is I used Q

498
00:22:47,649 --> 00:22:50,190
flow and we'll talk about that next

499
00:22:50,190 --> 00:22:53,230
which is a platform that helps you get

500
00:22:53,230 --> 00:22:56,980
install some of these challenges so yes

501
00:22:56,980 --> 00:22:59,080
having that automation to provision and

502
00:22:59,080 --> 00:23:00,820
then once we're done B provision is

503
00:23:00,820 --> 00:23:03,279
important to save cost but also on

504
00:23:03,279 --> 00:23:06,999
aspects and then you provides the pre

505
00:23:06,999 --> 00:23:09,549
trained due dates which are basically

506
00:23:09,549 --> 00:23:12,429
ways for you to get started with the

507
00:23:12,429 --> 00:23:15,549
training process and then the training

508
00:23:15,549 --> 00:23:17,289
data set that was created at the data

509
00:23:17,289 --> 00:23:20,190
pipeline stage goes as an input as well

510
00:23:20,190 --> 00:23:22,899
you provide monitoring logging and

511
00:23:22,899 --> 00:23:25,179
alerting as I mentioned previously that

512
00:23:25,179 --> 00:23:28,749
training takes time and if things go

513
00:23:28,749 --> 00:23:30,879
wrong you should get notified right way

514
00:23:30,879 --> 00:23:33,279
so that you can resolve them and

515
00:23:33,279 --> 00:23:36,100
continue training or restart them based

516
00:23:36,100 --> 00:23:37,119
on your use case

517
00:23:37,119 --> 00:23:39,759
so having logging L also helps and

518
00:23:39,759 --> 00:23:42,340
alerting so having all of these things

519
00:23:42,340 --> 00:23:46,600
so that you you're not stuck there in

520
00:23:46,600 --> 00:23:48,279
our cases we used to start the training

521
00:23:48,279 --> 00:23:50,980
go home come and then when we come next

522
00:23:50,980 --> 00:23:52,899
day we might have some good progress but

523
00:23:52,899 --> 00:23:55,539
if something fails we used to know the

524
00:23:55,539 --> 00:23:59,559
facts because because you you don't want

525
00:23:59,559 --> 00:24:02,799
to wait till later to find out issues us

526
00:24:02,799 --> 00:24:05,970
yeah like it's really crucial to have a

527
00:24:05,970 --> 00:24:08,200
monitoring logging and alerting at the

528
00:24:08,200 --> 00:24:11,230
stage and you validate whether the

529
00:24:11,230 --> 00:24:13,239
training actually worked and if the

530
00:24:13,239 --> 00:24:16,210
accuracy is good or not and then if that

531
00:24:16,210 --> 00:24:16,659
works

532
00:24:16,659 --> 00:24:19,119
run bias and fairness testing having

533
00:24:19,119 --> 00:24:22,269
automation here really helps and then

534
00:24:22,269 --> 00:24:24,659
build and world on your model so that

535
00:24:24,659 --> 00:24:28,509
you can push it to docker if you feel

536
00:24:28,509 --> 00:24:29,470
like with

537
00:24:29,470 --> 00:24:31,929
helps you have versioning and other

538
00:24:31,929 --> 00:24:34,809
aspects and then you push your image to

539
00:24:34,809 --> 00:24:37,659
artifact tree so this is a typical

540
00:24:37,659 --> 00:24:40,780
training pipeline and we kind of touched

541
00:24:40,780 --> 00:24:42,880
some of these various aspects of

542
00:24:42,880 --> 00:24:45,730
challenges that we had including maybe

543
00:24:45,730 --> 00:24:47,740
packaging in your docker container so

544
00:24:47,740 --> 00:24:50,289
that you don't have the dependency hell

545
00:24:50,289 --> 00:24:53,220
and you know which versions you're using

546
00:24:53,220 --> 00:24:55,539
okay so next stage is deployment

547
00:24:55,539 --> 00:24:58,809
pipeline the deployment pipe and you and

548
00:24:58,809 --> 00:25:01,090
this is a use case that I have used you

549
00:25:01,090 --> 00:25:04,390
don't have to get hops but we used get

550
00:25:04,390 --> 00:25:06,789
ups which works pretty well with with

551
00:25:06,789 --> 00:25:08,409
with queue flow which is what we have

552
00:25:08,409 --> 00:25:12,700
used in the past but push to master in

553
00:25:12,700 --> 00:25:13,240
your github

554
00:25:13,240 --> 00:25:16,720
github let's say it triggers a build

555
00:25:16,720 --> 00:25:19,570
with our go and it was your model in

556
00:25:19,570 --> 00:25:22,150
production you basically pull the image

557
00:25:22,150 --> 00:25:24,220
from artifact repository that it pushed

558
00:25:24,220 --> 00:25:27,370
in in the previous stage and again

559
00:25:27,370 --> 00:25:29,730
provisioning your mod your

560
00:25:29,730 --> 00:25:32,289
infrastructure automatically helps here

561
00:25:32,289 --> 00:25:36,159
and you might want GP support again

562
00:25:36,159 --> 00:25:37,929
muttering logging alerting having all of

563
00:25:37,929 --> 00:25:40,179
those aspects there and then

564
00:25:40,179 --> 00:25:42,340
periodically evaluate your model

565
00:25:42,340 --> 00:25:47,320
accuracy see if things are going down if

566
00:25:47,320 --> 00:25:49,240
the accuracy is going down and if you

567
00:25:49,240 --> 00:25:53,080
need to train the model again so that's

568
00:25:53,080 --> 00:25:55,150
the typical deployment pipeline I

569
00:25:55,150 --> 00:25:57,460
haven't mentioned a lot of usual things

570
00:25:57,460 --> 00:26:00,490
like test automation stutter that I also

571
00:26:00,490 --> 00:26:03,730
common to software software development

572
00:26:03,730 --> 00:26:05,830
and traditional software development and

573
00:26:05,830 --> 00:26:10,210
talked only about ml specific stuff so

574
00:26:10,210 --> 00:26:11,890
let's quickly talk about I know we are

575
00:26:11,890 --> 00:26:13,750
running out of time but I'll take 30

576
00:26:13,750 --> 00:26:15,850
seconds to talk about vary platforms

577
00:26:15,850 --> 00:26:18,580
that are available so the ones from the

578
00:26:18,580 --> 00:26:22,320
top ml spoke you flow and Apache airflow

579
00:26:22,320 --> 00:26:24,669
agnostic and and you need to manage

580
00:26:24,669 --> 00:26:27,429
those platform and then the ones at the

581
00:26:27,429 --> 00:26:29,620
bottom are what cloud why does provide

582
00:26:29,620 --> 00:26:33,400
and they are managed so here's queue

583
00:26:33,400 --> 00:26:37,179
flow which is what my experience most of

584
00:26:37,179 --> 00:26:38,980
my experiences and as you can see

585
00:26:38,980 --> 00:26:40,830
supports things like pipelines

586
00:26:40,830 --> 00:26:43,840
experiments tracking experiments

587
00:26:43,840 --> 00:26:47,110
serving the models and answering PI -

588
00:26:47,110 --> 00:26:50,320
model and tensorflow models and then

589
00:26:50,320 --> 00:26:52,710
model training and all of these aspects

590
00:26:52,710 --> 00:26:55,090
working with Jupiter notebooks and

591
00:26:55,090 --> 00:26:57,520
providing a Jupiter lab so yeah like a

592
00:26:57,520 --> 00:26:59,740
lot of those things are supported in

593
00:26:59,740 --> 00:27:02,440
queue flow but but also having good

594
00:27:02,440 --> 00:27:06,430
practices in terms of automation team

595
00:27:06,430 --> 00:27:08,830
structures and people process part of it

596
00:27:08,830 --> 00:27:11,710
is really important okay here's the

597
00:27:11,710 --> 00:27:13,270
summary I'm not gonna go through each

598
00:27:13,270 --> 00:27:16,320
one but that's what we talked about and

599
00:27:16,320 --> 00:27:19,570
that's all that's when I talk thanks for

600
00:27:19,570 --> 00:27:21,340
listening and happy to take any

601
00:27:21,340 --> 00:27:24,670
questions but again my name is other

602
00:27:24,670 --> 00:27:38,170
Shah thank you hello so we have

603
00:27:38,170 --> 00:27:42,490
questions yeah I'll put them on screen

604
00:27:42,490 --> 00:27:44,550
so it's it's better for you to

605
00:27:44,550 --> 00:27:49,300
understand my english sounds good so we

606
00:27:49,300 --> 00:27:53,340
have two questions from the query

607
00:27:53,340 --> 00:27:57,100
section of your website there are both

608
00:27:57,100 --> 00:28:01,900
form wallet so the first one is how

609
00:28:01,900 --> 00:28:05,680
about the ops from Nvidia in her red

610
00:28:05,680 --> 00:28:09,130
head data hub do you see any community

611
00:28:09,130 --> 00:28:11,590
support and prove it were close you do

612
00:28:11,590 --> 00:28:14,620
not utilize in them if not what do you

613
00:28:14,620 --> 00:28:16,230
recommend

614
00:28:16,230 --> 00:28:19,840
yes there is good community support I

615
00:28:19,840 --> 00:28:22,900
have definitely used a lot of Nvidia

616
00:28:22,900 --> 00:28:25,540
forms and an Nvidia folks are pretty

617
00:28:25,540 --> 00:28:30,610
helpful there the support for some like

618
00:28:30,610 --> 00:28:33,190
things like docker and and container

619
00:28:33,190 --> 00:28:35,830
izing things with especially edge

620
00:28:35,830 --> 00:28:39,310
devices are challenges so I mentioned in

621
00:28:39,310 --> 00:28:42,610
medium Jetson Nano when I was talking

622
00:28:42,610 --> 00:28:46,720
about edge devices it's very painful to

623
00:28:46,720 --> 00:28:49,120
make docker or other things work there

624
00:28:49,120 --> 00:28:51,850
so I have used community there and they

625
00:28:51,850 --> 00:28:53,500
are actually working on improving those

626
00:28:53,500 --> 00:28:56,920
things so yeah III don't know much about

627
00:28:56,920 --> 00:28:57,779
Red Hat

628
00:28:57,779 --> 00:29:00,710
era hub I haven't used it but in media

629
00:29:00,710 --> 00:29:03,599
support and community forums are

630
00:29:03,599 --> 00:29:06,509
definitely helpful and I have used and

631
00:29:06,509 --> 00:29:11,399
even contributed a bit along the way

632
00:29:11,399 --> 00:29:13,580
hope that will answer their question to

633
00:29:13,580 --> 00:29:18,090
me at least he'd answer it maybe wallet

634
00:29:18,090 --> 00:29:19,440
can talk to you later

635
00:29:19,440 --> 00:29:22,799
it's like so the next question from

636
00:29:22,799 --> 00:29:27,080
hearing is mid support the hopes and

637
00:29:27,080 --> 00:29:31,159
infrastructure of MLD out which I think

638
00:29:31,159 --> 00:29:35,029
it's machine learning and deep learning

639
00:29:35,029 --> 00:29:38,759
solutions to users how can i enhance my

640
00:29:38,759 --> 00:29:39,239
skills

641
00:29:39,239 --> 00:29:41,820
what if suture and the opposite sources

642
00:29:41,820 --> 00:29:46,019
do you recommend yeah question I I went

643
00:29:46,019 --> 00:29:49,739
through that journey so yeah yeah so I

644
00:29:49,739 --> 00:29:51,960
and that is actually one of the things

645
00:29:51,960 --> 00:29:55,739
about us talk up understanding those

646
00:29:55,739 --> 00:29:59,009
challenges I mentioned I I can share

647
00:29:59,009 --> 00:30:00,719
some of the resources I have used on

648
00:30:00,719 --> 00:30:04,219
slack links to it basically

649
00:30:04,219 --> 00:30:08,070
understanding those challenges on I have

650
00:30:08,070 --> 00:30:11,519
a in our UC especially we had a need run

651
00:30:11,519 --> 00:30:13,799
that in different environments cloud

652
00:30:13,799 --> 00:30:16,679
versus our own Ram because if the data

653
00:30:16,679 --> 00:30:18,419
is on trend had to run those things

654
00:30:18,419 --> 00:30:22,619
there so use tools like you flow but but

655
00:30:22,619 --> 00:30:25,429
being understanding of how GPU works

656
00:30:25,429 --> 00:30:28,469
parallelization and and some of the

657
00:30:28,469 --> 00:30:31,129
challenges that I talked about are

658
00:30:31,129 --> 00:30:34,200
something that if you focus on and learn

659
00:30:34,200 --> 00:30:37,349
will help you understand what to do when

660
00:30:37,349 --> 00:30:38,669
working with machine learning deep

661
00:30:38,669 --> 00:30:40,919
learning let me share some of the

662
00:30:40,919 --> 00:30:43,950
resources that I've used on slack pretty

663
00:30:43,950 --> 00:30:50,509
easy okay so thank you a lot

664
00:30:50,509 --> 00:30:57,239
here you have anything to say no that's

665
00:30:57,239 --> 00:30:59,070
all I had fun getting the talk and

666
00:30:59,070 --> 00:31:01,200
thanks everyone for listening and thank

667
00:31:01,200 --> 00:31:03,359
you to you for introduction and helping

668
00:31:03,359 --> 00:31:06,299
with questions up alright any other

669
00:31:06,299 --> 00:31:10,169
questions let me know on slide thank you

670
00:31:10,169 --> 00:31:13,070
Thanks

