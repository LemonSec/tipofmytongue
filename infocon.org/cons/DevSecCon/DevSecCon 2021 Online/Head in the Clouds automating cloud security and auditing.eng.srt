1
00:00:07,530 --> 00:00:10,730
[Music]

2
00:00:12,000 --> 00:00:12,320
hi

3
00:00:12,320 --> 00:00:15,839
my name is max hi everyone i'm natalie

4
00:00:15,839 --> 00:00:17,600
and we're so excited to have you here

5
00:00:17,600 --> 00:00:19,920
today

6
00:00:20,080 --> 00:00:22,880
so a little bit about myself and i'm a

7
00:00:22,880 --> 00:00:24,080
tech lead at

8
00:00:24,080 --> 00:00:27,279
babylon um at data science and

9
00:00:27,279 --> 00:00:29,519
analytics platform and my interests

10
00:00:29,519 --> 00:00:31,039
include infrastructure

11
00:00:31,039 --> 00:00:34,160
data and machine learning and i'm

12
00:00:34,160 --> 00:00:36,239
natalie godick i'm a cloud engineer in

13
00:00:36,239 --> 00:00:38,719
the data platform foundations team

14
00:00:38,719 --> 00:00:40,640
i love all things infrastructure and

15
00:00:40,640 --> 00:00:42,960
have a particular thing for security

16
00:00:42,960 --> 00:00:44,960
so overseeing our data platform really

17
00:00:44,960 --> 00:00:46,399
allows me to draw

18
00:00:46,399 --> 00:00:49,360
these passions of myself and to worst

19
00:00:49,360 --> 00:00:51,199
about babylon who we are

20
00:00:51,199 --> 00:00:54,239
we are a digital first value-based

21
00:00:54,239 --> 00:00:55,360
healthcare company

22
00:00:55,360 --> 00:00:56,719
and we're on a mission to bring

23
00:00:56,719 --> 00:00:58,719
accessible and affordable health service

24
00:00:58,719 --> 00:00:59,840
into the hands of

25
00:00:59,840 --> 00:01:02,559
every single person on earth now that's

26
00:01:02,559 --> 00:01:03,600
quite a challenge

27
00:01:03,600 --> 00:01:06,000
right and as you might imagine as a

28
00:01:06,000 --> 00:01:07,520
healthcare company we deal with

29
00:01:07,520 --> 00:01:10,320
loads of sensitive data that data needs

30
00:01:10,320 --> 00:01:12,320
to stay secured and protected at all

31
00:01:12,320 --> 00:01:13,040
times

32
00:01:13,040 --> 00:01:15,520
while also allowing us to innovate and

33
00:01:15,520 --> 00:01:17,119
create products that will bring

34
00:01:17,119 --> 00:01:20,080
us down the road of our mission our

35
00:01:20,080 --> 00:01:21,360
security policies

36
00:01:21,360 --> 00:01:23,360
tend to get very complicated and we

37
00:01:23,360 --> 00:01:24,640
often find ourselves

38
00:01:24,640 --> 00:01:27,280
challenging the limits of our cloud

39
00:01:27,280 --> 00:01:29,439
providers and our product providers of

40
00:01:29,439 --> 00:01:31,680
the technologies that we use

41
00:01:31,680 --> 00:01:34,240
and on that over to max to introduce you

42
00:01:34,240 --> 00:01:34,880
to your

43
00:01:34,880 --> 00:01:38,000
very complex security policies

44
00:01:38,000 --> 00:01:40,240
thanks a lot natalie and a lot of

45
00:01:40,240 --> 00:01:41,759
companies have seen the benefits of

46
00:01:41,759 --> 00:01:42,960
using cloud for

47
00:01:42,960 --> 00:01:45,040
everything from storing data through

48
00:01:45,040 --> 00:01:47,119
running microservice infrastructures

49
00:01:47,119 --> 00:01:50,079
to machine learning and so much more but

50
00:01:50,079 --> 00:01:52,320
once you store your data in the cloud

51
00:01:52,320 --> 00:01:55,920
all the apis that protect it are public

52
00:01:55,920 --> 00:01:57,439
natalie already explained what our

53
00:01:57,439 --> 00:02:00,079
mission is and while we're fulfilling it

54
00:02:00,079 --> 00:02:02,159
we cannot compromise security of our

55
00:02:02,159 --> 00:02:03,439
members data

56
00:02:03,439 --> 00:02:06,159
we cannot keep the public apis public

57
00:02:06,159 --> 00:02:08,000
much longer

58
00:02:08,000 --> 00:02:10,239
so how do you do that most of the things

59
00:02:10,239 --> 00:02:11,760
we're going to cover today

60
00:02:11,760 --> 00:02:13,520
are going to talk about our adventure

61
00:02:13,520 --> 00:02:14,800
with gcp

62
00:02:14,800 --> 00:02:16,800
but these concepts are not foreign to

63
00:02:16,800 --> 00:02:18,720
other platforms and you will be able to

64
00:02:18,720 --> 00:02:19,920
take

65
00:02:19,920 --> 00:02:21,680
anything that you've seen here and

66
00:02:21,680 --> 00:02:25,040
adapted in your own context

67
00:02:25,040 --> 00:02:27,120
number one let's take a look at a

68
00:02:27,120 --> 00:02:29,680
sampler request hitting a public api

69
00:02:29,680 --> 00:02:31,519
we need to make sure that we consider

70
00:02:31,519 --> 00:02:33,920
every single aspect of these requests

71
00:02:33,920 --> 00:02:36,400
in a classic rbac scenario a user makes

72
00:02:36,400 --> 00:02:37,280
a request

73
00:02:37,280 --> 00:02:39,519
we check the identity of the user and we

74
00:02:39,519 --> 00:02:41,120
compare it against the list of users

75
00:02:41,120 --> 00:02:42,640
that should be able to access given

76
00:02:42,640 --> 00:02:43,920
resource

77
00:02:43,920 --> 00:02:46,319
but what if your resources in us and

78
00:02:46,319 --> 00:02:48,480
data sovereignty loss prohibit any uk

79
00:02:48,480 --> 00:02:51,360
employees from accessing it on uk soil

80
00:02:51,360 --> 00:02:53,200
we need to make sure that any policies

81
00:02:53,200 --> 00:02:55,680
that are legal compliance privacy and

82
00:02:55,680 --> 00:02:57,200
security teams craft

83
00:02:57,200 --> 00:03:01,200
can accommodate these sort of nuances

84
00:03:02,640 --> 00:03:04,400
and it isn't uncommon in the world of

85
00:03:04,400 --> 00:03:05,920
healthcare to have many

86
00:03:05,920 --> 00:03:08,560
vastly deferring regulations that across

87
00:03:08,560 --> 00:03:10,640
borders will tell you that the data of a

88
00:03:10,640 --> 00:03:12,080
citizen of one country

89
00:03:12,080 --> 00:03:14,080
need to be processed in a certain way

90
00:03:14,080 --> 00:03:15,760
that is different to data of a citizen

91
00:03:15,760 --> 00:03:17,440
in another country

92
00:03:17,440 --> 00:03:19,680
if you thought gdpr was bad this stuff

93
00:03:19,680 --> 00:03:22,000
can give you proper nightmares

94
00:03:22,000 --> 00:03:23,680
some of countries will have very

95
00:03:23,680 --> 00:03:25,599
prohibitive laws while others

96
00:03:25,599 --> 00:03:27,120
will allow you to be a little bit more

97
00:03:27,120 --> 00:03:29,200
adult on your own

98
00:03:29,200 --> 00:03:31,280
we wanted to make sure that we don't

99
00:03:31,280 --> 00:03:33,200
just comply with the rules we're giving

100
00:03:33,200 --> 00:03:34,879
in any given country

101
00:03:34,879 --> 00:03:36,560
we want to make sure we act in the

102
00:03:36,560 --> 00:03:38,239
spirit of the regulation

103
00:03:38,239 --> 00:03:40,000
if the regulations want us to make our

104
00:03:40,000 --> 00:03:41,840
members data secure

105
00:03:41,840 --> 00:03:43,599
we can't do the bare minimum to make

106
00:03:43,599 --> 00:03:46,799
that happen we will do everything we can

107
00:03:46,799 --> 00:03:48,879
so instead of considering just the

108
00:03:48,879 --> 00:03:50,799
identity and the role of the user

109
00:03:50,799 --> 00:03:53,360
we want to make take a look at as many

110
00:03:53,360 --> 00:03:54,319
things as we can

111
00:03:54,319 --> 00:03:57,040
including yes the identity but also

112
00:03:57,040 --> 00:03:57,840
things like

113
00:03:57,840 --> 00:03:59,920
where is the request coming from what

114
00:03:59,920 --> 00:04:01,920
kind of data is being requested

115
00:04:01,920 --> 00:04:04,159
as this might affect when we should say

116
00:04:04,159 --> 00:04:05,760
yes and when we should say no to a

117
00:04:05,760 --> 00:04:07,840
request coming in

118
00:04:07,840 --> 00:04:10,000
we also want to know things like intent

119
00:04:10,000 --> 00:04:11,840
as data might be available for one

120
00:04:11,840 --> 00:04:12,400
purpose

121
00:04:12,400 --> 00:04:15,200
but not another and finally for for

122
00:04:15,200 --> 00:04:16,959
instance auditing purposes

123
00:04:16,959 --> 00:04:19,440
we want to be able to tell why was a

124
00:04:19,440 --> 00:04:22,880
permission granted in the first place

125
00:04:22,880 --> 00:04:24,320
so let's start delving into the

126
00:04:24,320 --> 00:04:26,080
resources that you can create to make

127
00:04:26,080 --> 00:04:27,360
all of that happen

128
00:04:27,360 --> 00:04:30,240
first up using blue dot i am but already

129
00:04:30,240 --> 00:04:31,520
here there is something that

130
00:04:31,520 --> 00:04:33,680
if you are not doing it right now you

131
00:04:33,680 --> 00:04:36,000
should start as soon as possible

132
00:04:36,000 --> 00:04:38,720
because the way you can grant a person a

133
00:04:38,720 --> 00:04:39,120
role

134
00:04:39,120 --> 00:04:41,280
is made so easy it's an absolute miracle

135
00:04:41,280 --> 00:04:43,040
of modern cloud apis

136
00:04:43,040 --> 00:04:45,120
but even in a simple example like this

137
00:04:45,120 --> 00:04:46,880
you can already start encoding some

138
00:04:46,880 --> 00:04:48,320
additional information

139
00:04:48,320 --> 00:04:50,400
about for instance why was the world

140
00:04:50,400 --> 00:04:51,919
granted in the first place

141
00:04:51,919 --> 00:04:54,880
and very importantly when should the

142
00:04:54,880 --> 00:04:56,479
permission expire

143
00:04:56,479 --> 00:04:58,400
because if you give somebody a row with

144
00:04:58,400 --> 00:04:59,759
indefinite duration

145
00:04:59,759 --> 00:05:01,840
at some point somebody will forget that

146
00:05:01,840 --> 00:05:03,360
that role was granted

147
00:05:03,360 --> 00:05:06,400
and that the user had it

148
00:05:06,400 --> 00:05:08,080
this complicates the entire security

149
00:05:08,080 --> 00:05:10,320
infrastructure and reasoning about it

150
00:05:10,320 --> 00:05:12,720
and so again if you're not doing this

151
00:05:12,720 --> 00:05:14,400
this is one of the simplest things that

152
00:05:14,400 --> 00:05:15,199
you can do

153
00:05:15,199 --> 00:05:16,960
that will remove a massive chunk of your

154
00:05:16,960 --> 00:05:18,800
potential attack surface

155
00:05:18,800 --> 00:05:21,520
because of all that zombie users that

156
00:05:21,520 --> 00:05:23,440
have access to data that they don't even

157
00:05:23,440 --> 00:05:27,039
use can be used against you

158
00:05:27,039 --> 00:05:29,840
moving on in the same spirit let's say

159
00:05:29,840 --> 00:05:30,960
that a user

160
00:05:30,960 --> 00:05:33,280
actually needs access to some data set

161
00:05:33,280 --> 00:05:35,759
so we want her to get permission

162
00:05:35,759 --> 00:05:38,160
but that she needs access to all of the

163
00:05:38,160 --> 00:05:39,120
data

164
00:05:39,120 --> 00:05:41,680
most of the time no she doesn't and

165
00:05:41,680 --> 00:05:43,360
what's more or she shouldn't

166
00:05:43,360 --> 00:05:45,680
raw data especially in healthcare can be

167
00:05:45,680 --> 00:05:47,520
avoided to be seen and that's a big

168
00:05:47,520 --> 00:05:49,280
story for a completely different time

169
00:05:49,280 --> 00:05:51,199
but in this session i'm just going to

170
00:05:51,199 --> 00:05:53,759
show you in a very simplified example

171
00:05:53,759 --> 00:05:55,120
how you can manipulate the data

172
00:05:55,120 --> 00:05:56,960
dynamically when the user is requesting

173
00:05:56,960 --> 00:05:58,000
access

174
00:05:58,000 --> 00:06:00,080
and yes you can do this by creating a

175
00:06:00,080 --> 00:06:02,319
new data set that is maybe anonymized

176
00:06:02,319 --> 00:06:03,120
and so on

177
00:06:03,120 --> 00:06:05,680
but that means that you have to have all

178
00:06:05,680 --> 00:06:07,120
of these data sets ready

179
00:06:07,120 --> 00:06:09,039
you have to predict what the user is

180
00:06:09,039 --> 00:06:10,160
going to want to see

181
00:06:10,160 --> 00:06:11,840
what the regulations allow them to see

182
00:06:11,840 --> 00:06:13,280
etc which

183
00:06:13,280 --> 00:06:15,039
to be honest arguably is not the job for

184
00:06:15,039 --> 00:06:17,039
data engineering teams

185
00:06:17,039 --> 00:06:18,960
all of that can introduce delays and

186
00:06:18,960 --> 00:06:20,400
delays cost money

187
00:06:20,400 --> 00:06:22,319
and in some cases they reduce security

188
00:06:22,319 --> 00:06:23,840
as well

189
00:06:23,840 --> 00:06:25,600
what you can do instead is to mask the

190
00:06:25,600 --> 00:06:27,680
columns or even rows dynamically

191
00:06:27,680 --> 00:06:29,120
so that the final data set that the

192
00:06:29,120 --> 00:06:31,280
users get to see includes all the data

193
00:06:31,280 --> 00:06:32,960
that they need to do their job

194
00:06:32,960 --> 00:06:36,080
but it doesn't include anything else and

195
00:06:36,080 --> 00:06:36,800
so now

196
00:06:36,800 --> 00:06:38,960
if somebody tries to attack you they

197
00:06:38,960 --> 00:06:40,639
won't be able to target any of your

198
00:06:40,639 --> 00:06:42,400
zombie users as they do not have

199
00:06:42,400 --> 00:06:43,759
indefinite permissions

200
00:06:43,759 --> 00:06:45,759
but if even if they get access to

201
00:06:45,759 --> 00:06:47,280
somebody potentially

202
00:06:47,280 --> 00:06:49,280
senior the data that they see is going

203
00:06:49,280 --> 00:06:51,199
to be relevant only to the particular

204
00:06:51,199 --> 00:06:52,960
task that the senior person is doing

205
00:06:52,960 --> 00:06:55,440
right now

206
00:06:56,960 --> 00:06:58,639
the third thing i'm going to mention is

207
00:06:58,639 --> 00:07:01,039
google's vpc access controls

208
00:07:01,039 --> 00:07:02,880
this can be trickier to implement

209
00:07:02,880 --> 00:07:04,800
without that particular api

210
00:07:04,800 --> 00:07:07,120
but you can do it using other tools like

211
00:07:07,120 --> 00:07:09,440
dpc's or firewall rules

212
00:07:09,440 --> 00:07:11,199
service parameters give you direct

213
00:07:11,199 --> 00:07:13,440
control over who can access which apis

214
00:07:13,440 --> 00:07:14,880
and in which context

215
00:07:14,880 --> 00:07:16,880
and that's massive because as i've

216
00:07:16,880 --> 00:07:17,919
mentioned before

217
00:07:17,919 --> 00:07:20,000
giving one user access to a particular

218
00:07:20,000 --> 00:07:21,680
data set is not enough

219
00:07:21,680 --> 00:07:24,080
as when uk employees working from us

220
00:07:24,080 --> 00:07:24,720
office

221
00:07:24,720 --> 00:07:26,560
even though they retain their original

222
00:07:26,560 --> 00:07:28,160
identity i mean

223
00:07:28,160 --> 00:07:29,599
nobody really changes their login

224
00:07:29,599 --> 00:07:31,919
credentials while crossing the border

225
00:07:31,919 --> 00:07:34,080
in that case they should lose access as

226
00:07:34,080 --> 00:07:35,599
it might be a massive violation to

227
00:07:35,599 --> 00:07:36,880
access uk members

228
00:07:36,880 --> 00:07:40,160
data on u.s soil access controls give

229
00:07:40,160 --> 00:07:41,680
you the power to make these nuances

230
00:07:41,680 --> 00:07:42,800
enforceable

231
00:07:42,800 --> 00:07:44,879
we can now very easily say not only that

232
00:07:44,879 --> 00:07:46,639
the user has access to some data

233
00:07:46,639 --> 00:07:49,680
but also where from

234
00:07:49,840 --> 00:07:52,960
this is the customary visual aid for

235
00:07:52,960 --> 00:07:54,479
for all the folks that are more visual

236
00:07:54,479 --> 00:07:58,720
in the audience

237
00:07:58,720 --> 00:08:01,280
and so now you can take all of these

238
00:08:01,280 --> 00:08:02,319
small things

239
00:08:02,319 --> 00:08:04,400
which can and often are managed in

240
00:08:04,400 --> 00:08:06,319
different ways in different places

241
00:08:06,319 --> 00:08:08,400
and you can really just make them one

242
00:08:08,400 --> 00:08:10,639
quick and simple process

243
00:08:10,639 --> 00:08:13,360
a user submits a request to access data

244
00:08:13,360 --> 00:08:14,160
which can be done

245
00:08:14,160 --> 00:08:17,440
very easily through git then in a cicd

246
00:08:17,440 --> 00:08:17,840
tool

247
00:08:17,840 --> 00:08:19,440
we can get the full context of that

248
00:08:19,440 --> 00:08:21,280
request and validate it

249
00:08:21,280 --> 00:08:23,199
potentially we might have to intercept

250
00:08:23,199 --> 00:08:25,120
the request and edit it in some ways

251
00:08:25,120 --> 00:08:27,039
like for instance removing a user

252
00:08:27,039 --> 00:08:29,039
if they don't satisfy our policies or

253
00:08:29,039 --> 00:08:30,720
changing which ips the data will be

254
00:08:30,720 --> 00:08:32,399
accessible from depending on what the

255
00:08:32,399 --> 00:08:33,839
data includes

256
00:08:33,839 --> 00:08:35,440
and then we orchestrate all the cloud

257
00:08:35,440 --> 00:08:38,479
resources using paraform and paragraphs

258
00:08:38,479 --> 00:08:40,559
end to end in potentially five minutes

259
00:08:40,559 --> 00:08:42,399
so when your users are asking for access

260
00:08:42,399 --> 00:08:43,120
to data

261
00:08:43,120 --> 00:08:45,360
you can very quickly tell them what they

262
00:08:45,360 --> 00:08:47,040
can and cannot access

263
00:08:47,040 --> 00:08:49,200
and then give them their own private

264
00:08:49,200 --> 00:08:53,120
google project or kubernetes namespace

265
00:08:53,519 --> 00:08:55,839
this is how a sample policy might look

266
00:08:55,839 --> 00:08:57,200
like it can

267
00:08:57,200 --> 00:09:00,160
really be just a simple hd yaml file

268
00:09:00,160 --> 00:09:01,440
that lets you specify

269
00:09:01,440 --> 00:09:03,760
which attributes of data and users can

270
00:09:03,760 --> 00:09:05,839
go together and which doms

271
00:09:05,839 --> 00:09:07,760
some of these fields will depend on your

272
00:09:07,760 --> 00:09:09,680
organization and you should really make

273
00:09:09,680 --> 00:09:11,360
it as custom fit to your needs as

274
00:09:11,360 --> 00:09:12,160
possible

275
00:09:12,160 --> 00:09:13,760
this is a very simplified version of

276
00:09:13,760 --> 00:09:15,440
something most companies might want to

277
00:09:15,440 --> 00:09:16,399
codify

278
00:09:16,399 --> 00:09:18,000
in this particular case we're looking at

279
00:09:18,000 --> 00:09:20,240
a dummy policy which allows access to

280
00:09:20,240 --> 00:09:20,800
raw

281
00:09:20,800 --> 00:09:23,920
cities and diseases in uk as long as the

282
00:09:23,920 --> 00:09:25,680
patients they refer to

283
00:09:25,680 --> 00:09:27,440
have given an explicit ai learning

284
00:09:27,440 --> 00:09:28,800
consent by any

285
00:09:28,800 --> 00:09:31,200
uk-based full-time employees with

286
00:09:31,200 --> 00:09:32,720
enhanced dbs check

287
00:09:32,720 --> 00:09:35,839
if they wanted to use it for ai model

288
00:09:35,839 --> 00:09:37,519
the access is also going to happen

289
00:09:37,519 --> 00:09:39,600
through a virtual desktop infrastructure

290
00:09:39,600 --> 00:09:41,600
because the data is raw and there is

291
00:09:41,600 --> 00:09:44,720
going to be no internet access

292
00:09:44,720 --> 00:09:46,880
now any teams that need to make these

293
00:09:46,880 --> 00:09:49,040
decisions can get involved directly

294
00:09:49,040 --> 00:09:50,399
impacting how the data

295
00:09:50,399 --> 00:09:52,800
can be accessed across the organization

296
00:09:52,800 --> 00:09:54,560
but without ever having to talk to

297
00:09:54,560 --> 00:09:56,320
engineers or getting familiar with the

298
00:09:56,320 --> 00:09:58,720
data

299
00:09:59,279 --> 00:10:01,279
and from our user's perspective this is

300
00:10:01,279 --> 00:10:03,440
how a sample request might look like

301
00:10:03,440 --> 00:10:05,839
i want person one two and three to be

302
00:10:05,839 --> 00:10:07,839
able to access this particular table

303
00:10:07,839 --> 00:10:09,760
but only the rows that were consented

304
00:10:09,760 --> 00:10:11,920
for and with dynamically masked columns

305
00:10:11,920 --> 00:10:13,680
in this particular way

306
00:10:13,680 --> 00:10:15,600
in addition my project is going to

307
00:10:15,600 --> 00:10:18,079
expire on the first of july 2021

308
00:10:18,079 --> 00:10:19,920
the data is going to be used for ai

309
00:10:19,920 --> 00:10:21,120
model improvement

310
00:10:21,120 --> 00:10:23,120
and a reason for approval and maybe

311
00:10:23,120 --> 00:10:24,800
description of the project is included

312
00:10:24,800 --> 00:10:26,959
in jira ticket

313
00:10:26,959 --> 00:10:30,800
0123 and all of that is a direct input

314
00:10:30,800 --> 00:10:32,160
to a terraform module

315
00:10:32,160 --> 00:10:34,160
that can then take all of the resources

316
00:10:34,160 --> 00:10:35,920
i showed you before and many more

317
00:10:35,920 --> 00:10:37,920
and provision them together so that all

318
00:10:37,920 --> 00:10:39,519
of the complexities are hidden from

319
00:10:39,519 --> 00:10:40,079
people

320
00:10:40,079 --> 00:10:43,279
that do not need to see them

321
00:10:43,279 --> 00:10:46,000
as i mentioned before in the cicd tool

322
00:10:46,000 --> 00:10:47,760
we're going to then enrich the request

323
00:10:47,760 --> 00:10:49,360
with all the metadata we need to make

324
00:10:49,360 --> 00:10:51,200
the decision based on policies

325
00:10:51,200 --> 00:10:53,920
and then we can directly compare the two

326
00:10:53,920 --> 00:10:54,959
and that's me

327
00:10:54,959 --> 00:10:56,720
so now let's see what natalie has

328
00:10:56,720 --> 00:10:59,120
prepared for us

329
00:10:59,120 --> 00:11:02,160
thanks max right so now you have

330
00:11:02,160 --> 00:11:04,959
uh seen how to analyze your requirements

331
00:11:04,959 --> 00:11:06,399
in terms of security

332
00:11:06,399 --> 00:11:09,120
and how to uh construct a policy that

333
00:11:09,120 --> 00:11:11,440
will be flexible and will encompass all

334
00:11:11,440 --> 00:11:12,839
of the needs that you have in your

335
00:11:12,839 --> 00:11:14,320
organization

336
00:11:14,320 --> 00:11:16,399
now it's time to shift the focus onto

337
00:11:16,399 --> 00:11:17,519
what happens

338
00:11:17,519 --> 00:11:20,320
after you see when you use a cloud

339
00:11:20,320 --> 00:11:20,959
platform

340
00:11:20,959 --> 00:11:22,720
all of the requests that happen and all

341
00:11:22,720 --> 00:11:24,800
of the events that happen within it

342
00:11:24,800 --> 00:11:28,079
are api requests and each api request

343
00:11:28,079 --> 00:11:31,440
produces a log logs contain

344
00:11:31,440 --> 00:11:33,839
masses of very important data that you

345
00:11:33,839 --> 00:11:34,480
definitely

346
00:11:34,480 --> 00:11:37,040
should store centrally and use to

347
00:11:37,040 --> 00:11:39,519
produce insights analytics alarms and

348
00:11:39,519 --> 00:11:41,440
have a really good visibility onto your

349
00:11:41,440 --> 00:11:43,120
platform

350
00:11:43,120 --> 00:11:45,760
now i will continue using the example of

351
00:11:45,760 --> 00:11:46,480
gcp

352
00:11:46,480 --> 00:11:48,959
but you can apply the same principles

353
00:11:48,959 --> 00:11:51,519
whichever platform you use

354
00:11:51,519 --> 00:11:54,079
in gcp each resource in each project

355
00:11:54,079 --> 00:11:55,040
will produce

356
00:11:55,040 --> 00:11:57,120
a log and you can use something called

357
00:11:57,120 --> 00:11:58,959
logging syncs

358
00:11:58,959 --> 00:12:01,680
to route those logs into a specific

359
00:12:01,680 --> 00:12:02,800
location

360
00:12:02,800 --> 00:12:04,399
these logging things are just log

361
00:12:04,399 --> 00:12:06,480
pipelines and if you use logstash or

362
00:12:06,480 --> 00:12:08,320
cloudtrail or anything else you will

363
00:12:08,320 --> 00:12:10,079
likely have a similar

364
00:12:10,079 --> 00:12:12,720
pipeline now i would suggest that you

365
00:12:12,720 --> 00:12:14,480
would want to have your logs in a

366
00:12:14,480 --> 00:12:16,000
central location

367
00:12:16,000 --> 00:12:18,639
such as a centralized logging project in

368
00:12:18,639 --> 00:12:20,880
order to enable the analytics uh

369
00:12:20,880 --> 00:12:24,079
later down the line and this is quite

370
00:12:24,079 --> 00:12:28,720
easy to set up until you start having

371
00:12:28,720 --> 00:12:30,639
vpc service controls and service

372
00:12:30,639 --> 00:12:32,000
parameters

373
00:12:32,000 --> 00:12:34,320
you see when you have your data that is

374
00:12:34,320 --> 00:12:35,040
sensitive

375
00:12:35,040 --> 00:12:36,560
you will want to protect it with service

376
00:12:36,560 --> 00:12:38,639
parameters like max described

377
00:12:38,639 --> 00:12:41,200
but also logs are sensitive data as well

378
00:12:41,200 --> 00:12:42,959
and specific to your organization

379
00:12:42,959 --> 00:12:44,320
so you will want to have a service

380
00:12:44,320 --> 00:12:46,480
parameter around the logging project

381
00:12:46,480 --> 00:12:49,200
too and then you start having an

382
00:12:49,200 --> 00:12:50,959
architecture that is quite complex with

383
00:12:50,959 --> 00:12:52,079
some projects within

384
00:12:52,079 --> 00:12:54,399
different service perimeters others

385
00:12:54,399 --> 00:12:56,560
outsides because not all services have

386
00:12:56,560 --> 00:12:58,000
to be restricted

387
00:12:58,000 --> 00:13:00,240
and then the logging project inside

388
00:13:00,240 --> 00:13:02,079
perimeters too

389
00:13:02,079 --> 00:13:04,240
and how do you enable communication

390
00:13:04,240 --> 00:13:05,680
since the perimeters

391
00:13:05,680 --> 00:13:07,839
serve for this particular purpose of not

392
00:13:07,839 --> 00:13:09,600
allowing any data to go

393
00:13:09,600 --> 00:13:13,120
outside of it well you can use

394
00:13:13,120 --> 00:13:16,240
egress rules to allow specific data

395
00:13:16,240 --> 00:13:19,200
to go into a specific place in this case

396
00:13:19,200 --> 00:13:20,880
let's say that we want

397
00:13:20,880 --> 00:13:23,440
the logging sync to route the logs from

398
00:13:23,440 --> 00:13:24,720
our project a

399
00:13:24,720 --> 00:13:27,040
into the logging project and the egress

400
00:13:27,040 --> 00:13:28,959
rule will say that the logging api

401
00:13:28,959 --> 00:13:32,079
is allowed to exit the perimeter if the

402
00:13:32,079 --> 00:13:33,120
data is going

403
00:13:33,120 --> 00:13:36,320
into the login project and then on the

404
00:13:36,320 --> 00:13:38,480
side of the login service parameter you

405
00:13:38,480 --> 00:13:39,519
will want to say

406
00:13:39,519 --> 00:13:42,480
that the identity of the login sync is

407
00:13:42,480 --> 00:13:43,279
allowed to

408
00:13:43,279 --> 00:13:47,279
enter data inside the service parameter

409
00:13:47,279 --> 00:13:49,279
whatever logging pipeline you use you

410
00:13:49,279 --> 00:13:51,040
will have an identity

411
00:13:51,040 --> 00:13:53,839
of the of this pipeline and this

412
00:13:53,839 --> 00:13:55,120
identity can be used

413
00:13:55,120 --> 00:13:58,720
to uh to set up iem roles permissions

414
00:13:58,720 --> 00:14:01,680
and also network traffic rules and if if

415
00:14:01,680 --> 00:14:03,360
your platform allows for that

416
00:14:03,360 --> 00:14:04,880
and this is exactly what we're going to

417
00:14:04,880 --> 00:14:06,639
do with gcp

418
00:14:06,639 --> 00:14:09,120
we are going to allow the traffic with

419
00:14:09,120 --> 00:14:09,839
the logs

420
00:14:09,839 --> 00:14:12,800
from the originating perimeters to exit

421
00:14:12,800 --> 00:14:14,399
if they are going into the logging

422
00:14:14,399 --> 00:14:15,279
project

423
00:14:15,279 --> 00:14:17,199
and also allow the logging things to

424
00:14:17,199 --> 00:14:21,439
write into the logging service perimeter

425
00:14:21,519 --> 00:14:24,000
in order to make this a setup automated

426
00:14:24,000 --> 00:14:25,440
and a little bit more

427
00:14:25,440 --> 00:14:28,639
simplified you will want to consider

428
00:14:28,639 --> 00:14:31,040
having top level folders in your

429
00:14:31,040 --> 00:14:32,720
organization

430
00:14:32,720 --> 00:14:35,600
in general the more high level your

431
00:14:35,600 --> 00:14:36,480
setup can be

432
00:14:36,480 --> 00:14:39,199
the better because then you can just set

433
00:14:39,199 --> 00:14:41,120
up the login configuration on the top

434
00:14:41,120 --> 00:14:42,880
level and forget about it and everything

435
00:14:42,880 --> 00:14:44,399
else inside your platform

436
00:14:44,399 --> 00:14:47,360
will be already covered but maybe you

437
00:14:47,360 --> 00:14:49,199
have a requirement to store logs

438
00:14:49,199 --> 00:14:50,079
separately

439
00:14:50,079 --> 00:14:52,320
depending on the environment maybe

440
00:14:52,320 --> 00:14:54,000
non-production production

441
00:14:54,000 --> 00:14:56,639
maybe location or maybe department needs

442
00:14:56,639 --> 00:14:58,480
to be stored separately

443
00:14:58,480 --> 00:15:00,800
in that case you can say okay let's have

444
00:15:00,800 --> 00:15:02,000
top-level project

445
00:15:02,000 --> 00:15:05,279
folders that are that signify those

446
00:15:05,279 --> 00:15:08,079
separated spaces environments in this

447
00:15:08,079 --> 00:15:09,920
case i will use an environment as an

448
00:15:09,920 --> 00:15:12,480
example production and non-production

449
00:15:12,480 --> 00:15:15,600
then we create a folder logging sync and

450
00:15:15,600 --> 00:15:16,399
we say that

451
00:15:16,399 --> 00:15:18,959
all children are included in the logging

452
00:15:18,959 --> 00:15:20,240
sync

453
00:15:20,240 --> 00:15:22,079
and we create separate spaces for

454
00:15:22,079 --> 00:15:24,240
storing those logs to comply with a

455
00:15:24,240 --> 00:15:25,199
different

456
00:15:25,199 --> 00:15:28,160
data locality or storage requirements

457
00:15:28,160 --> 00:15:29,759
each of the logging syncs again will

458
00:15:29,759 --> 00:15:30,959
have a service account

459
00:15:30,959 --> 00:15:33,120
the its writer identity that will be

460
00:15:33,120 --> 00:15:34,000
used to

461
00:15:34,000 --> 00:15:36,720
set up access if we look at this

462
00:15:36,720 --> 00:15:38,720
architecture in a little bit more detail

463
00:15:38,720 --> 00:15:40,639
it will look something like this

464
00:15:40,639 --> 00:15:43,440
you have your gcp organization and you

465
00:15:43,440 --> 00:15:43,920
enable

466
00:15:43,920 --> 00:15:46,160
audit logging on all services this is

467
00:15:46,160 --> 00:15:48,959
what is going to give you the most data

468
00:15:48,959 --> 00:15:50,959
then you say that you have top level

469
00:15:50,959 --> 00:15:53,199
folders for example production

470
00:15:53,199 --> 00:15:54,959
and all of the projects that go

471
00:15:54,959 --> 00:15:57,120
underneath will have to live within

472
00:15:57,120 --> 00:15:59,839
folders or underneath just the top level

473
00:15:59,839 --> 00:16:01,759
folder

474
00:16:01,759 --> 00:16:03,759
then on the right hand side you create

475
00:16:03,759 --> 00:16:05,360
the audit log

476
00:16:05,360 --> 00:16:08,240
prod project which will for example

477
00:16:08,240 --> 00:16:08,800
contain

478
00:16:08,800 --> 00:16:11,199
a bigquery data set and we create a

479
00:16:11,199 --> 00:16:13,120
folder logging sync from the production

480
00:16:13,120 --> 00:16:14,320
folder to this

481
00:16:14,320 --> 00:16:17,120
bigquery data set to store logs in

482
00:16:17,120 --> 00:16:18,320
bigquery

483
00:16:18,320 --> 00:16:21,279
why bigquery well because it's a store a

484
00:16:21,279 --> 00:16:22,240
data store

485
00:16:22,240 --> 00:16:24,639
that is extremely scalable and enables

486
00:16:24,639 --> 00:16:26,560
you to do analytics that we will

487
00:16:26,560 --> 00:16:29,759
take a look just in a second

488
00:16:29,759 --> 00:16:31,680
whatever service parameters you will

489
00:16:31,680 --> 00:16:34,079
have around your source projects

490
00:16:34,079 --> 00:16:39,120
uh will have to have an egress policy

491
00:16:39,120 --> 00:16:41,120
that egress policy will allow logs and

492
00:16:41,120 --> 00:16:42,320
the query api

493
00:16:42,320 --> 00:16:44,959
to go outside of the perimeter into the

494
00:16:44,959 --> 00:16:46,639
logging project

495
00:16:46,639 --> 00:16:49,120
and on the side of the logging service

496
00:16:49,120 --> 00:16:51,040
parameter you will have an access level

497
00:16:51,040 --> 00:16:52,800
and you will add an access level

498
00:16:52,800 --> 00:16:55,279
condition for each of the logging sync

499
00:16:55,279 --> 00:16:58,079
identities and also an iem role for the

500
00:16:58,079 --> 00:16:59,199
identity to write

501
00:16:59,199 --> 00:17:01,440
into the data set and everything will be

502
00:17:01,440 --> 00:17:03,839
encrypted end to end thanks to gcp

503
00:17:03,839 --> 00:17:06,000
encrypting everything um you know all

504
00:17:06,000 --> 00:17:08,559
data in transit and us using a

505
00:17:08,559 --> 00:17:10,720
service identity a service account

506
00:17:10,720 --> 00:17:13,039
identity

507
00:17:13,039 --> 00:17:15,359
when you use terraform you can actually

508
00:17:15,359 --> 00:17:16,079
simplify

509
00:17:16,079 --> 00:17:19,039
the setup and make it nice and automated

510
00:17:19,039 --> 00:17:20,480
you will have a module

511
00:17:20,480 --> 00:17:22,480
terraform allows you to create a module

512
00:17:22,480 --> 00:17:24,640
which groups resources that have to be

513
00:17:24,640 --> 00:17:26,559
deployed together

514
00:17:26,559 --> 00:17:28,720
in this case we have seen that we create

515
00:17:28,720 --> 00:17:29,840
a folder

516
00:17:29,840 --> 00:17:32,960
a folder logging sync a bigquery data

517
00:17:32,960 --> 00:17:34,000
set

518
00:17:34,000 --> 00:17:36,240
an access level condition and some iem

519
00:17:36,240 --> 00:17:38,320
permissions for the identity of the

520
00:17:38,320 --> 00:17:40,480
logging sync to write into the target

521
00:17:40,480 --> 00:17:41,840
dataset

522
00:17:41,840 --> 00:17:44,720
that will be our gcp folder module and

523
00:17:44,720 --> 00:17:46,799
then you code it in a way that

524
00:17:46,799 --> 00:17:49,280
in the input variables or telegram

525
00:17:49,280 --> 00:17:50,880
diffuse telegrams

526
00:17:50,880 --> 00:17:52,400
you will just have a list of folders

527
00:17:52,400 --> 00:17:54,240
that you want to create and you might

528
00:17:54,240 --> 00:17:55,520
want to have a flag

529
00:17:55,520 --> 00:17:59,440
that says that a folder is audited or

530
00:17:59,440 --> 00:18:01,440
maybe logs enabled something like that

531
00:18:01,440 --> 00:18:03,039
something descriptive of saying that

532
00:18:03,039 --> 00:18:05,120
this folder is the top-level folder

533
00:18:05,120 --> 00:18:05,679
where the

534
00:18:05,679 --> 00:18:08,160
logging syncs will be configured and

535
00:18:08,160 --> 00:18:09,919
then of course

536
00:18:09,919 --> 00:18:12,160
the module will create your folder and

537
00:18:12,160 --> 00:18:14,160
create all of the login configuration

538
00:18:14,160 --> 00:18:16,320
for you and then you don't have to worry

539
00:18:16,320 --> 00:18:17,360
about anything else

540
00:18:17,360 --> 00:18:19,280
all of the projects that will be created

541
00:18:19,280 --> 00:18:20,880
underneath will have will be covered

542
00:18:20,880 --> 00:18:23,039
with the logging syncs automatically

543
00:18:23,039 --> 00:18:25,039
and when you need a new folder you just

544
00:18:25,039 --> 00:18:27,520
add one line in the input

545
00:18:27,520 --> 00:18:31,360
very simple very nice like that

546
00:18:31,360 --> 00:18:33,679
now i've mentioned a couple of times

547
00:18:33,679 --> 00:18:35,760
that bigquery analytics

548
00:18:35,760 --> 00:18:39,200
query what do i mean what if you want to

549
00:18:39,200 --> 00:18:42,080
see who access to data and production

550
00:18:42,080 --> 00:18:44,559
how do you do this now is it easy to do

551
00:18:44,559 --> 00:18:45,840
do you have to go and

552
00:18:45,840 --> 00:18:48,640
search somewhere through the log trails

553
00:18:48,640 --> 00:18:49,600
or maybe some

554
00:18:49,600 --> 00:18:52,720
special specialized security software

555
00:18:52,720 --> 00:18:53,919
well if you have your logs in the

556
00:18:53,919 --> 00:18:55,919
central place in a store that is

557
00:18:55,919 --> 00:18:56,720
queryable

558
00:18:56,720 --> 00:18:58,559
you can have a really simple query like

559
00:18:58,559 --> 00:19:00,640
this and select

560
00:19:00,640 --> 00:19:04,480
user emails from cloud audit data access

561
00:19:04,480 --> 00:19:06,160
in this particular case this is

562
00:19:06,160 --> 00:19:09,200
a google cloud logging

563
00:19:09,200 --> 00:19:10,960
table that contains all of the data

564
00:19:10,960 --> 00:19:12,559
access slots

565
00:19:12,559 --> 00:19:13,919
and you might want to might want to

566
00:19:13,919 --> 00:19:16,240
filter it by resource or email or time

567
00:19:16,240 --> 00:19:16,960
zone

568
00:19:16,960 --> 00:19:20,720
or region or maybe you want to see

569
00:19:20,720 --> 00:19:23,520
um policy violation statistics across

570
00:19:23,520 --> 00:19:25,039
your entire platform

571
00:19:25,039 --> 00:19:26,720
so you've written all of those policies

572
00:19:26,720 --> 00:19:28,480
and now you want to monitor them

573
00:19:28,480 --> 00:19:30,080
this particular query actually comes

574
00:19:30,080 --> 00:19:32,480
from a really cool medium blog post

575
00:19:32,480 --> 00:19:34,480
that describes how to create a dashboard

576
00:19:34,480 --> 00:19:38,000
about vpc policy violations in gcp and

577
00:19:38,000 --> 00:19:39,600
this really just gives you all of the

578
00:19:39,600 --> 00:19:40,080
data

579
00:19:40,080 --> 00:19:42,320
that you need now you can't really set

580
00:19:42,320 --> 00:19:45,039
up a real-time alerting on this because

581
00:19:45,039 --> 00:19:47,840
logs come in not in real time but every

582
00:19:47,840 --> 00:19:49,440
hour i believe

583
00:19:49,440 --> 00:19:52,320
but this will still allow you to create

584
00:19:52,320 --> 00:19:54,400
really nice visibility and really nice

585
00:19:54,400 --> 00:19:56,080
dashboards across your platform and

586
00:19:56,080 --> 00:19:57,120
understand

587
00:19:57,120 --> 00:19:59,120
who is trying to access what where they

588
00:19:59,120 --> 00:20:00,480
are not allowed to

589
00:20:00,480 --> 00:20:03,120
which regions might be affected more

590
00:20:03,120 --> 00:20:05,440
maybe there are trends in violations and

591
00:20:05,440 --> 00:20:06,159
you need to

592
00:20:06,159 --> 00:20:08,080
kind of keep in mind keep keep that in

593
00:20:08,080 --> 00:20:09,760
mind

594
00:20:09,760 --> 00:20:11,360
and the third thing that i want to show

595
00:20:11,360 --> 00:20:13,520
is usage statistics

596
00:20:13,520 --> 00:20:15,600
now this query starts to get a little

597
00:20:15,600 --> 00:20:17,840
bit more complicated

598
00:20:17,840 --> 00:20:21,039
but fear not it's actually really easy

599
00:20:21,039 --> 00:20:24,240
you can take for bigquery for example

600
00:20:24,240 --> 00:20:27,280
you can take which user

601
00:20:27,280 --> 00:20:30,640
has queried how much data per day

602
00:20:30,640 --> 00:20:32,320
and then you can create dashboards like

603
00:20:32,320 --> 00:20:34,880
this now all of the numbers here are

604
00:20:34,880 --> 00:20:36,880
randomly generated just to illustrate

605
00:20:36,880 --> 00:20:38,880
the possibility to you

606
00:20:38,880 --> 00:20:42,000
but you can actually classify your

607
00:20:42,000 --> 00:20:44,640
usage and the costs of each project or

608
00:20:44,640 --> 00:20:46,000
per region or

609
00:20:46,000 --> 00:20:49,039
product or gcp project

610
00:20:49,039 --> 00:20:51,679
see the stats and the differences

611
00:20:51,679 --> 00:20:53,360
between user names

612
00:20:53,360 --> 00:20:55,440
and this starts to get really really

613
00:20:55,440 --> 00:20:57,679
interesting when you start comparing

614
00:20:57,679 --> 00:20:58,000
this

615
00:20:58,000 --> 00:21:01,039
this data over time so you can see how

616
00:21:01,039 --> 00:21:02,480
much the usage of your platform is

617
00:21:02,480 --> 00:21:03,840
growing for example

618
00:21:03,840 --> 00:21:06,480
or which project and with or which team

619
00:21:06,480 --> 00:21:06,880
is

620
00:21:06,880 --> 00:21:10,240
you is being the most active um or maybe

621
00:21:10,240 --> 00:21:12,320
which query or which analytical project

622
00:21:12,320 --> 00:21:14,799
is uh starting to cost a little bit more

623
00:21:14,799 --> 00:21:16,080
than expected

624
00:21:16,080 --> 00:21:18,240
and this kind of data really will allow

625
00:21:18,240 --> 00:21:20,080
your analytics teams

626
00:21:20,080 --> 00:21:22,480
to be more independent and to optimize

627
00:21:22,480 --> 00:21:24,000
the way they work

628
00:21:24,000 --> 00:21:25,840
and really have a good visibility into

629
00:21:25,840 --> 00:21:28,080
what they do on the platform

630
00:21:28,080 --> 00:21:30,159
and so this is a bigquery usage example

631
00:21:30,159 --> 00:21:31,280
but similarly

632
00:21:31,280 --> 00:21:33,679
like all of your logs contains so much

633
00:21:33,679 --> 00:21:35,600
data and so much insight

634
00:21:35,600 --> 00:21:37,600
that you should really really think

635
00:21:37,600 --> 00:21:40,000
about using the using it and building

636
00:21:40,000 --> 00:21:41,679
some dashboards that are useful for your

637
00:21:41,679 --> 00:21:45,039
company with that

638
00:21:45,039 --> 00:21:47,200
and that is all from us if you have any

639
00:21:47,200 --> 00:21:49,200
questions we're really happy to answer

640
00:21:49,200 --> 00:21:50,000
them

641
00:21:50,000 --> 00:21:52,880
and would also like to say that we're

642
00:21:52,880 --> 00:21:53,840
hiring

643
00:21:53,840 --> 00:21:59,679
thank you very much

644
00:21:59,679 --> 00:22:01,760
you

