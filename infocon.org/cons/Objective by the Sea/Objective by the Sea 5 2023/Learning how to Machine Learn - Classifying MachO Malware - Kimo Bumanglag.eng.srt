1
00:00:00,659 --> 00:00:03,559
bye

2
00:00:07,919 --> 00:00:09,780
all right next up we have a talk by

3
00:00:09,780 --> 00:00:12,540
chemo about classifying Mac malware via

4
00:00:12,540 --> 00:00:15,000
machine learning this is also a very

5
00:00:15,000 --> 00:00:16,859
interesting topic to me because on

6
00:00:16,859 --> 00:00:19,980
Windows we've seen a lot of research

7
00:00:19,980 --> 00:00:21,960
gone into classifying Windows malware

8
00:00:21,960 --> 00:00:24,359
but we have not recently seen a lot of

9
00:00:24,359 --> 00:00:26,760
that in the Mac OS space and I'm sure

10
00:00:26,760 --> 00:00:27,900
chemo will talk about some of the

11
00:00:27,900 --> 00:00:29,400
challenges but it is really neat to see

12
00:00:29,400 --> 00:00:31,920
that we're gaining some finally some

13
00:00:31,920 --> 00:00:34,800
parity between Windows and Mac in terms

14
00:00:34,800 --> 00:00:37,260
of the Mac research that's coming out so

15
00:00:37,260 --> 00:00:40,640
let's give chemo a round of applause

16
00:00:45,600 --> 00:00:48,360
okay it's good all right thanks Patrick

17
00:00:48,360 --> 00:00:50,700
I'm excited to be here as you said my

18
00:00:50,700 --> 00:00:52,320
name is Kimo and the talk that I have

19
00:00:52,320 --> 00:00:54,120
for you today is called learning how to

20
00:00:54,120 --> 00:00:56,460
machine learn and as a background is

21
00:00:56,460 --> 00:00:57,420
this

22
00:00:57,420 --> 00:00:59,699
um I have had a Mac for quite a long

23
00:00:59,699 --> 00:01:02,219
time and I've been doing security stuff

24
00:01:02,219 --> 00:01:04,080
for a pretty long time but it's only

25
00:01:04,080 --> 00:01:06,360
been in the last couple of years that I

26
00:01:06,360 --> 00:01:08,159
started to get interested in like Mac OS

27
00:01:08,159 --> 00:01:10,500
security and around the same time I was

28
00:01:10,500 --> 00:01:12,240
taking a class in machine learning so I

29
00:01:12,240 --> 00:01:13,380
was trying to figure out like how can we

30
00:01:13,380 --> 00:01:16,260
use ml to solve security problems like

31
00:01:16,260 --> 00:01:17,640
Patrick mentioned I was doing some

32
00:01:17,640 --> 00:01:19,439
research and I found quite a bit of like

33
00:01:19,439 --> 00:01:21,299
published work on this is how you can

34
00:01:21,299 --> 00:01:24,479
use ml to classify a PE file as

35
00:01:24,479 --> 00:01:26,939
malicious or how to classify a PE file

36
00:01:26,939 --> 00:01:29,100
as packed so looking around I didn't see

37
00:01:29,100 --> 00:01:31,200
a whole lot of similar research being

38
00:01:31,200 --> 00:01:33,600
applied to Macos and the Mac space

39
00:01:33,600 --> 00:01:35,579
that's kind of like how this whole thing

40
00:01:35,579 --> 00:01:37,500
got started and if you all are

41
00:01:37,500 --> 00:01:39,659
interested in going on that Journey as

42
00:01:39,659 --> 00:01:41,759
well I've put together a GitHub with a

43
00:01:41,759 --> 00:01:43,380
jupyter notebook where I'm explaining

44
00:01:43,380 --> 00:01:45,840
like how all the stuff works and it's

45
00:01:45,840 --> 00:01:48,540
got some sample data that you can use

46
00:01:48,540 --> 00:01:50,520
and we'll talk about how to use it

47
00:01:50,520 --> 00:01:52,740
so with that the agenda for today is

48
00:01:52,740 --> 00:01:54,780
First downloading some malware samples

49
00:01:54,780 --> 00:01:56,220
so in order to do machine learning we've

50
00:01:56,220 --> 00:01:58,020
got to have data in order to have data

51
00:01:58,020 --> 00:01:59,340
we need to download some malware so I'll

52
00:01:59,340 --> 00:02:01,740
talk about how I did that data

53
00:02:01,740 --> 00:02:04,740
pre-processing once we have Macco files

54
00:02:04,740 --> 00:02:06,240
we need to extract information out of

55
00:02:06,240 --> 00:02:08,699
those files so that we can apply some ml

56
00:02:08,699 --> 00:02:11,099
against it and feature selection and

57
00:02:11,099 --> 00:02:12,780
Engineering we'll talk about how to

58
00:02:12,780 --> 00:02:14,760
transform the data so that's more

59
00:02:14,760 --> 00:02:16,920
applicable to different algorithms fill

60
00:02:16,920 --> 00:02:18,440
in missing values that kind of stuff

61
00:02:18,440 --> 00:02:21,360
hyper parameter optimization every

62
00:02:21,360 --> 00:02:23,160
machine learning algorithm has a set of

63
00:02:23,160 --> 00:02:24,840
hyper parameters that are going to

64
00:02:24,840 --> 00:02:27,000
control how that model Works internally

65
00:02:27,000 --> 00:02:29,040
so we'll look at how to automatically

66
00:02:29,040 --> 00:02:30,540
find the best parameters for our

67
00:02:30,540 --> 00:02:32,879
particular problem and then model

68
00:02:32,879 --> 00:02:34,140
training we'll take all that stuff put

69
00:02:34,140 --> 00:02:35,760
it together make a machine learning

70
00:02:35,760 --> 00:02:37,140
model that can do some predictions on

71
00:02:37,140 --> 00:02:39,060
whether something is malware or not and

72
00:02:39,060 --> 00:02:40,620
then lastly we'll look at ways to assess

73
00:02:40,620 --> 00:02:42,780
and validate those models with a special

74
00:02:42,780 --> 00:02:44,760
emphasis towards like visualizing how

75
00:02:44,760 --> 00:02:46,860
well it's doing

76
00:02:46,860 --> 00:02:49,080
starting off with the malware sample

77
00:02:49,080 --> 00:02:50,879
collection it's my experience going

78
00:02:50,879 --> 00:02:52,440
through some machine learning courses

79
00:02:52,440 --> 00:02:55,500
was like intro to machine learning go to

80
00:02:55,500 --> 00:02:58,680
kaggle and download the CSV that has all

81
00:02:58,680 --> 00:03:00,360
this information about people who are

82
00:03:00,360 --> 00:03:02,879
like on the Titanic and then make a

83
00:03:02,879 --> 00:03:04,019
machine learning model to predict

84
00:03:04,019 --> 00:03:05,340
whether they would have survived or not

85
00:03:05,340 --> 00:03:07,980
then go do a machine learning course

86
00:03:07,980 --> 00:03:09,360
that's like focused on cyber security

87
00:03:09,360 --> 00:03:11,819
here's a data set from nist with like

88
00:03:11,819 --> 00:03:14,459
all this information about Network

89
00:03:14,459 --> 00:03:16,319
traffic in this network and it's already

90
00:03:16,319 --> 00:03:19,319
labeled for you so in all those cases

91
00:03:19,319 --> 00:03:21,480
right the data was provided to me so

92
00:03:21,480 --> 00:03:23,099
when it came time to do my own research

93
00:03:23,099 --> 00:03:25,080
I had to figure out how to go and get my

94
00:03:25,080 --> 00:03:27,900
own data so the processes uh the the

95
00:03:27,900 --> 00:03:29,400
ways that I went about doing that went

96
00:03:29,400 --> 00:03:31,080
to some of the repositories that are

97
00:03:31,080 --> 00:03:33,780
online so the first one is birashare you

98
00:03:33,780 --> 00:03:35,099
can email them and they'll hook you up

99
00:03:35,099 --> 00:03:36,840
with an account once you have access

100
00:03:36,840 --> 00:03:39,420
into their repo you can search through

101
00:03:39,420 --> 00:03:41,940
it to find some malware and the way that

102
00:03:41,940 --> 00:03:44,099
they kind of extract information from

103
00:03:44,099 --> 00:03:45,840
their files they essentially run like

104
00:03:45,840 --> 00:03:47,760
file against all their samples and store

105
00:03:47,760 --> 00:03:50,099
that string inside of their database and

106
00:03:50,099 --> 00:03:51,480
then you can write a regex to search

107
00:03:51,480 --> 00:03:53,640
that string for specific file types so

108
00:03:53,640 --> 00:03:55,500
just an example of searching their repo

109
00:03:55,500 --> 00:03:58,739
to get mock-ofiles with a virus total at

110
00:03:58,739 --> 00:04:00,720
the time I was affiliated with the

111
00:04:00,720 --> 00:04:04,140
university so VT gave me access to their

112
00:04:04,140 --> 00:04:07,080
academic API what that means is I had a

113
00:04:07,080 --> 00:04:09,780
higher like API quota and they also gave

114
00:04:09,780 --> 00:04:11,700
me access to a Google Drive where they

115
00:04:11,700 --> 00:04:13,379
have some zip files that they've made

116
00:04:13,379 --> 00:04:14,299
with different types of network

117
00:04:14,299 --> 00:04:16,440
limitation that I found there is that

118
00:04:16,440 --> 00:04:18,779
the zip files were actually like empty

119
00:04:18,779 --> 00:04:22,100
or did not have Macos a lot of Macos

120
00:04:22,100 --> 00:04:24,259
specifically in their Mako collection

121
00:04:24,259 --> 00:04:26,520
now through work and looking have to

122
00:04:26,520 --> 00:04:28,320
have access to VT Enterprise and if you

123
00:04:28,320 --> 00:04:30,300
are as well then you can use a query

124
00:04:30,300 --> 00:04:31,680
like this to search

125
00:04:31,680 --> 00:04:34,680
for Macos and specifically for samples

126
00:04:34,680 --> 00:04:35,759
that are going to have three or more

127
00:04:35,759 --> 00:04:37,620
detections from the antivirus engines

128
00:04:37,620 --> 00:04:39,900
that vt's running and then last but not

129
00:04:39,900 --> 00:04:42,540
least is objective c so Patrick curates

130
00:04:42,540 --> 00:04:45,660
maybe like 150 or so malware samples

131
00:04:45,660 --> 00:04:48,120
right now so a One-Stop shop to get some

132
00:04:48,120 --> 00:04:50,580
like high quality Mac malware

133
00:04:50,580 --> 00:04:52,440
now I was doing a supervised machine

134
00:04:52,440 --> 00:04:54,120
learning problem which means that I need

135
00:04:54,120 --> 00:04:56,580
both malware samples and benign samples

136
00:04:56,580 --> 00:04:58,440
and then I need to label them as whether

137
00:04:58,440 --> 00:05:00,540
they are malware or not so I need to go

138
00:05:00,540 --> 00:05:02,460
and download some benign samples some

139
00:05:02,460 --> 00:05:03,660
ways that you could do that you could

140
00:05:03,660 --> 00:05:05,759
just use the Macos that are going to be

141
00:05:05,759 --> 00:05:08,880
present on a base install of Mac OS but

142
00:05:08,880 --> 00:05:10,199
one of the things to consider there is

143
00:05:10,199 --> 00:05:11,820
that the stuff that's in like slash user

144
00:05:11,820 --> 00:05:13,740
slash pin kind of follows that Unix

145
00:05:13,740 --> 00:05:15,419
philosophy of like a small program that

146
00:05:15,419 --> 00:05:17,580
does one thing so it's not going to

147
00:05:17,580 --> 00:05:19,800
generalize out to applications that a

148
00:05:19,800 --> 00:05:21,300
user is going to download or the

149
00:05:21,300 --> 00:05:23,460
functionality that malware might have

150
00:05:23,460 --> 00:05:26,160
so what I wanted to do was go find some

151
00:05:26,160 --> 00:05:27,600
other stuff so I went to the Mac App

152
00:05:27,600 --> 00:05:30,120
Store and softronic to find more

153
00:05:30,120 --> 00:05:31,620
different types of things as an example

154
00:05:31,620 --> 00:05:34,080
if we look at malware and it's a piece

155
00:05:34,080 --> 00:05:37,560
of ransomware it's going to load a

156
00:05:37,560 --> 00:05:39,180
library that contains cryptographic

157
00:05:39,180 --> 00:05:40,320
functions and it's going to call those

158
00:05:40,320 --> 00:05:42,479
functions to encrypt data I wanted to

159
00:05:42,479 --> 00:05:44,400
find something like a password manager

160
00:05:44,400 --> 00:05:46,500
which will load cryptographic libraries

161
00:05:46,500 --> 00:05:49,199
encrypt things but for good purposes and

162
00:05:49,199 --> 00:05:50,699
so that way when we build the models

163
00:05:50,699 --> 00:05:52,380
right we can discriminate between

164
00:05:52,380 --> 00:05:55,220
malicious and benign use of those apis

165
00:05:55,220 --> 00:05:57,960
instead of just the presence of those

166
00:05:57,960 --> 00:05:59,160
apis

167
00:05:59,160 --> 00:06:00,900
so I went to the Mac App Store went to a

168
00:06:00,900 --> 00:06:03,120
website called softronic they categorize

169
00:06:03,120 --> 00:06:04,919
their application so I just downloaded

170
00:06:04,919 --> 00:06:06,780
like the first 20 things that showed up

171
00:06:06,780 --> 00:06:09,859
in each of those categories

172
00:06:10,259 --> 00:06:12,840
now I have a bunch of Macco files

173
00:06:12,840 --> 00:06:15,240
sitting on a hard drive but I still need

174
00:06:15,240 --> 00:06:17,580
to take information out of those files

175
00:06:17,580 --> 00:06:20,340
so that I can do something with it so as

176
00:06:20,340 --> 00:06:22,080
I kind of covered before by some other

177
00:06:22,080 --> 00:06:24,060
presenters Mako consists of a header

178
00:06:24,060 --> 00:06:26,100
load commands and then data in the

179
00:06:26,100 --> 00:06:27,180
header we're going to find things like

180
00:06:27,180 --> 00:06:29,400
flags for how the library should be or

181
00:06:29,400 --> 00:06:31,500
the executable should be loaded load

182
00:06:31,500 --> 00:06:32,880
commands are going to have things that

183
00:06:32,880 --> 00:06:34,979
specify like the minimum Mac OS version

184
00:06:34,979 --> 00:06:37,680
or here's the dynamic library to load or

185
00:06:37,680 --> 00:06:40,259
here's a section or a segment so load

186
00:06:40,259 --> 00:06:42,780
from this library or this file and map

187
00:06:42,780 --> 00:06:45,240
it into memory for execution and then in

188
00:06:45,240 --> 00:06:46,380
the data section we're actually going to

189
00:06:46,380 --> 00:06:48,360
have the different segments and sections

190
00:06:48,360 --> 00:06:50,940
that compose that file

191
00:06:50,940 --> 00:06:52,620
so you could write your own tool to do

192
00:06:52,620 --> 00:06:53,580
that you could use something like

193
00:06:53,580 --> 00:06:56,819
O'Toole or J Tool and parse it challenge

194
00:06:56,819 --> 00:06:59,759
there is you get text and I wanted

195
00:06:59,759 --> 00:07:01,680
something a little bit more organized so

196
00:07:01,680 --> 00:07:03,660
I found a python Library called Macho

197
00:07:03,660 --> 00:07:07,100
Libre and Macho Libre will parse the

198
00:07:07,100 --> 00:07:10,620
executable and then return a Json object

199
00:07:10,620 --> 00:07:13,680
to you so sweet exactly what I need

200
00:07:13,680 --> 00:07:15,720
then I ran into another problem where

201
00:07:15,720 --> 00:07:18,360
with machine learning we have matrices

202
00:07:18,360 --> 00:07:21,360
of data not Json files of data so I

203
00:07:21,360 --> 00:07:23,220
needed to take the Json files and then

204
00:07:23,220 --> 00:07:25,500
load them up into a pandas data frame

205
00:07:25,500 --> 00:07:27,840
and the pandas library has the ability

206
00:07:27,840 --> 00:07:30,720
to read Json files but with Macho Libre

207
00:07:30,720 --> 00:07:33,060
it's giving you these kind of like

208
00:07:33,060 --> 00:07:35,880
recursive Json objects because for

209
00:07:35,880 --> 00:07:37,380
example it's going to parse the section

210
00:07:37,380 --> 00:07:38,880
and that section is going to get the

211
00:07:38,880 --> 00:07:40,560
size and the name of the section and the

212
00:07:40,560 --> 00:07:41,699
entropy of the section and the

213
00:07:41,699 --> 00:07:44,039
protections for memory when it gets

214
00:07:44,039 --> 00:07:47,460
mapped and all of that is a sub key of

215
00:07:47,460 --> 00:07:49,560
the segment and that's the sub key of

216
00:07:49,560 --> 00:07:51,660
the Mako which is a sub key of the file

217
00:07:51,660 --> 00:07:54,360
and so it's difficult to parse all that

218
00:07:54,360 --> 00:07:56,639
stuff out with pandas so the approach

219
00:07:56,639 --> 00:07:58,259
that I took to solve that was just write

220
00:07:58,259 --> 00:08:00,599
my own python that'll iterate through

221
00:08:00,599 --> 00:08:02,819
all the keys and sub Keys grab each of

222
00:08:02,819 --> 00:08:04,740
those objects and then just make like

223
00:08:04,740 --> 00:08:07,680
one big flat Json file with all that

224
00:08:07,680 --> 00:08:09,919
information

225
00:08:11,039 --> 00:08:12,780
so once all that was loaded up into

226
00:08:12,780 --> 00:08:14,639
pandas you could start asking some

227
00:08:14,639 --> 00:08:16,560
questions and poking at the data there

228
00:08:16,560 --> 00:08:17,880
are three questions that I wanted to be

229
00:08:17,880 --> 00:08:20,340
able to answer with this data so that

230
00:08:20,340 --> 00:08:23,039
was one is this file malware two is this

231
00:08:23,039 --> 00:08:25,500
file packed and then three is this file

232
00:08:25,500 --> 00:08:27,840
packed malware

233
00:08:27,840 --> 00:08:29,699
in order to poke at that we're going to

234
00:08:29,699 --> 00:08:32,580
run some essentially equations against

235
00:08:32,580 --> 00:08:35,760
it and just like back in school we have

236
00:08:35,760 --> 00:08:39,240
Y equals X plus something our Y in this

237
00:08:39,240 --> 00:08:42,000
case is going to be the column from our

238
00:08:42,000 --> 00:08:45,060
data that we want to find the answer to

239
00:08:45,060 --> 00:08:46,800
so in my case I had a column named

240
00:08:46,800 --> 00:08:48,660
malware column name pact and then one

241
00:08:48,660 --> 00:08:50,760
named pack mower so those are each of my

242
00:08:50,760 --> 00:08:52,920
y values and then we're also going to

243
00:08:52,920 --> 00:08:55,740
have x x being basically all the other

244
00:08:55,740 --> 00:08:57,540
features that we've pulled out of these

245
00:08:57,540 --> 00:08:59,279
files

246
00:08:59,279 --> 00:09:01,740
and then inside of the scikit-learn

247
00:09:01,740 --> 00:09:03,899
framework they have a nice little

248
00:09:03,899 --> 00:09:05,339
function to split the data between

249
00:09:05,339 --> 00:09:07,080
training and testing and then we're

250
00:09:07,080 --> 00:09:09,000
doing that so that we don't like pollute

251
00:09:09,000 --> 00:09:12,920
answers with training data

252
00:09:14,279 --> 00:09:15,959
so next was a feature selection

253
00:09:15,959 --> 00:09:17,820
engineering we've got data inside of a

254
00:09:17,820 --> 00:09:20,040
data frame it's all loaded up with split

255
00:09:20,040 --> 00:09:22,500
off now we need to do some more stuff to

256
00:09:22,500 --> 00:09:24,720
it the stuff that we need to do is fill

257
00:09:24,720 --> 00:09:27,180
in missing values scale numbers and then

258
00:09:27,180 --> 00:09:29,459
encode strings I don't do a whole lot of

259
00:09:29,459 --> 00:09:31,620
memes and a lot of my slides but this

260
00:09:31,620 --> 00:09:34,080
one did really speak to me because when

261
00:09:34,080 --> 00:09:35,459
I started doing machine learning it's

262
00:09:35,459 --> 00:09:36,839
what I felt like I was just throwing

263
00:09:36,839 --> 00:09:39,300
random stuff at a wall and then getting

264
00:09:39,300 --> 00:09:40,860
like an answer and saying okay cool that

265
00:09:40,860 --> 00:09:44,160
worked but I don't know how and it took

266
00:09:44,160 --> 00:09:45,420
a little bit of time to come to terms

267
00:09:45,420 --> 00:09:47,399
with it because there's like a voice in

268
00:09:47,399 --> 00:09:48,420
the back of my head that was like hey

269
00:09:48,420 --> 00:09:49,980
you've been doing security for a while

270
00:09:49,980 --> 00:09:51,720
and you're supposed to be like an expert

271
00:09:51,720 --> 00:09:53,760
so shouldn't you just figure out what's

272
00:09:53,760 --> 00:09:55,440
the important stuff from all these files

273
00:09:55,440 --> 00:09:56,880
to call it malware

274
00:09:56,880 --> 00:09:58,320
but the whole purpose of machine

275
00:09:58,320 --> 00:10:00,420
learning is you have all this data and

276
00:10:00,420 --> 00:10:01,980
you just let the computer figure out

277
00:10:01,980 --> 00:10:04,200
like what each of those features mean

278
00:10:04,200 --> 00:10:05,940
how they're related and how they answer

279
00:10:05,940 --> 00:10:08,160
the question that we're asking of it so

280
00:10:08,160 --> 00:10:09,600
once I did that it made a little bit

281
00:10:09,600 --> 00:10:12,060
more sense and I felt less awkward doing

282
00:10:12,060 --> 00:10:12,839
it

283
00:10:12,839 --> 00:10:15,120
so filling in missing values inside of

284
00:10:15,120 --> 00:10:17,399
our big Matrix if our machine learning

285
00:10:17,399 --> 00:10:19,320
algorithm encounters like a null or like

286
00:10:19,320 --> 00:10:21,480
a not a number type of value it's going

287
00:10:21,480 --> 00:10:23,459
to error out so we got to fix that the

288
00:10:23,459 --> 00:10:24,779
process that we use to fix that is

289
00:10:24,779 --> 00:10:26,459
called imputation a few different

290
00:10:26,459 --> 00:10:28,080
strategies that you can employ there you

291
00:10:28,080 --> 00:10:30,000
can either fill in just a constant value

292
00:10:30,000 --> 00:10:32,640
for anything that's missing if it's a

293
00:10:32,640 --> 00:10:34,500
numeric data type then you can take all

294
00:10:34,500 --> 00:10:36,779
of the values inside of that column do

295
00:10:36,779 --> 00:10:38,339
something like average them out and then

296
00:10:38,339 --> 00:10:40,080
fill in missing values with the average

297
00:10:40,080 --> 00:10:42,540
or if you've used Excel and there's that

298
00:10:42,540 --> 00:10:44,100
little black box in the bottom right

299
00:10:44,100 --> 00:10:46,019
corner of a cell and you can like click

300
00:10:46,019 --> 00:10:47,160
it and drag it and then it just

301
00:10:47,160 --> 00:10:49,380
populates the last cell down through the

302
00:10:49,380 --> 00:10:51,180
next cells you can take a similar

303
00:10:51,180 --> 00:10:53,160
approach as that

304
00:10:53,160 --> 00:10:54,959
um in scaling numbers so some machine

305
00:10:54,959 --> 00:10:57,720
learning algorithms are going to want

306
00:10:57,720 --> 00:11:00,000
data to be normally distributed or it'll

307
00:11:00,000 --> 00:11:01,680
be sensitive to outliers so this is just

308
00:11:01,680 --> 00:11:03,540
taking numeric information and then

309
00:11:03,540 --> 00:11:06,000
normalizing it and then encoding strings

310
00:11:06,000 --> 00:11:08,820
so ml is using numbers to do math it

311
00:11:08,820 --> 00:11:10,980
can't do math on a string so if we have

312
00:11:10,980 --> 00:11:12,720
a string value like the name of a

313
00:11:12,720 --> 00:11:15,480
dynamic library that gets loaded by a

314
00:11:15,480 --> 00:11:17,640
binary we need to take that and do

315
00:11:17,640 --> 00:11:19,980
something with it we can binaurize it or

316
00:11:19,980 --> 00:11:21,660
do something called one hot encoding

317
00:11:21,660 --> 00:11:24,180
that takes all of the values that are in

318
00:11:24,180 --> 00:11:26,519
one column and then turns those into

319
00:11:26,519 --> 00:11:30,180
individual columns so if we had a DOT

320
00:11:30,180 --> 00:11:32,459
dilib gets loaded a DOT dilib would

321
00:11:32,459 --> 00:11:34,019
become its own column and then we'd fill

322
00:11:34,019 --> 00:11:35,760
in one or zero to indicate the presence

323
00:11:35,760 --> 00:11:38,220
of that particular library in that

324
00:11:38,220 --> 00:11:40,260
particular sample

325
00:11:40,260 --> 00:11:42,420
so once all that's done we have some

326
00:11:42,420 --> 00:11:44,779
features and when I did this I had about

327
00:11:44,779 --> 00:11:48,779
2400 features so 2400 Columns of data

328
00:11:48,779 --> 00:11:51,720
and um not all of that's going to be

329
00:11:51,720 --> 00:11:53,700
important so we want to find only the

330
00:11:53,700 --> 00:11:55,740
parts that are important and use that

331
00:11:55,740 --> 00:11:58,620
and then the reason why we want to do

332
00:11:58,620 --> 00:12:00,480
this is because this was a pretty like

333
00:12:00,480 --> 00:12:01,860
limited data set that just used

334
00:12:01,860 --> 00:12:03,600
information that you can parse out of a

335
00:12:03,600 --> 00:12:05,760
mako but it didn't include things like

336
00:12:05,760 --> 00:12:07,740
the strings that are present if you ran

337
00:12:07,740 --> 00:12:09,720
strings and it also didn't disassemble

338
00:12:09,720 --> 00:12:12,420
files and then use bytecode sequences to

339
00:12:12,420 --> 00:12:14,220
say whether this was malicious or not if

340
00:12:14,220 --> 00:12:16,200
you did those things you might have a

341
00:12:16,200 --> 00:12:19,560
lot more columns and as you add in more

342
00:12:19,560 --> 00:12:21,540
data whether like horizontally or

343
00:12:21,540 --> 00:12:23,220
vertically you increase the amount of

344
00:12:23,220 --> 00:12:24,480
memory you need you increase the amount

345
00:12:24,480 --> 00:12:26,279
of compute that you need and again a lot

346
00:12:26,279 --> 00:12:28,019
of it's just not going to be meaningful

347
00:12:28,019 --> 00:12:30,420
to answer the questions that we have so

348
00:12:30,420 --> 00:12:33,240
we can use some features from scigit to

349
00:12:33,240 --> 00:12:34,800
find the features that are going to be

350
00:12:34,800 --> 00:12:37,140
relevant to us some ones that I tried

351
00:12:37,140 --> 00:12:39,300
are select K best which just gives you

352
00:12:39,300 --> 00:12:41,220
measures like the information gained

353
00:12:41,220 --> 00:12:44,459
from each of the features and then gives

354
00:12:44,459 --> 00:12:46,260
you the K number of those that you asked

355
00:12:46,260 --> 00:12:48,839
for select from model will instantiate a

356
00:12:48,839 --> 00:12:50,700
machine learning model feed in random

357
00:12:50,700 --> 00:12:52,079
features to those models get the

358
00:12:52,079 --> 00:12:53,880
predictions and then from those most

359
00:12:53,880 --> 00:12:55,800
accurate predictions give you the number

360
00:12:55,800 --> 00:12:58,019
of features that gave you those

361
00:12:58,019 --> 00:13:00,120
predictions and then sequential feature

362
00:13:00,120 --> 00:13:01,320
selection is kind of similar in that

363
00:13:01,320 --> 00:13:03,180
it's going to instantiate a model and

364
00:13:03,180 --> 00:13:04,320
then you're going to either do forward

365
00:13:04,320 --> 00:13:05,940
sequential feature selection where you

366
00:13:05,940 --> 00:13:07,740
start from zero features you take the

367
00:13:07,740 --> 00:13:08,940
best performing and add it best

368
00:13:08,940 --> 00:13:10,560
performing at it add it at it until you

369
00:13:10,560 --> 00:13:12,540
get the number you ask for or reverse

370
00:13:12,540 --> 00:13:13,680
where you start from all your features

371
00:13:13,680 --> 00:13:14,880
and then you take away the least

372
00:13:14,880 --> 00:13:16,019
performing take away the least

373
00:13:16,019 --> 00:13:17,579
performing until you get down to the

374
00:13:17,579 --> 00:13:18,839
number that you wanted

375
00:13:18,839 --> 00:13:20,519
so you pick one or the other based off

376
00:13:20,519 --> 00:13:22,139
of how many features you have total and

377
00:13:22,139 --> 00:13:23,459
how many features you want to get down

378
00:13:23,459 --> 00:13:24,600
to

379
00:13:24,600 --> 00:13:26,160
and then the last thing I tried was a

380
00:13:26,160 --> 00:13:27,779
principal component analysis which is

381
00:13:27,779 --> 00:13:29,279
not like strictly feature engineering

382
00:13:29,279 --> 00:13:31,200
but it does give you a measure of like

383
00:13:31,200 --> 00:13:33,300
how much that component contributes to

384
00:13:33,300 --> 00:13:35,100
the change in the amount of information

385
00:13:35,100 --> 00:13:37,740
between them so you can either ask for n

386
00:13:37,740 --> 00:13:39,839
components so a component is essentially

387
00:13:39,839 --> 00:13:42,720
a feature or you can ask for a float and

388
00:13:42,720 --> 00:13:44,940
if you ask for a float it will give you

389
00:13:44,940 --> 00:13:47,459
the number of features that explain that

390
00:13:47,459 --> 00:13:49,680
amount of information within the data

391
00:13:49,680 --> 00:13:52,200
set so as an example when I used PCA

392
00:13:52,200 --> 00:13:55,680
with a 0.95 so I asked for 95 percent of

393
00:13:55,680 --> 00:13:56,720
the information

394
00:13:56,720 --> 00:14:00,180
explanation it gave me like 250 or so

395
00:14:00,180 --> 00:14:02,639
components so out of 2400 went down to

396
00:14:02,639 --> 00:14:05,040
250 10 percent

397
00:14:05,040 --> 00:14:07,740
uh that's about 10 right so then 10 of

398
00:14:07,740 --> 00:14:11,399
the features explained 95 of uh the

399
00:14:11,399 --> 00:14:13,519
information

400
00:14:13,740 --> 00:14:16,500
when we're doing that

401
00:14:16,500 --> 00:14:18,180
um feature selection and Engineering

402
00:14:18,180 --> 00:14:19,500
Process we'll want to use something

403
00:14:19,500 --> 00:14:21,240
called a pipeline and that just

404
00:14:21,240 --> 00:14:23,399
sequences all these steps together so

405
00:14:23,399 --> 00:14:25,139
that we perform it in a repeatable

406
00:14:25,139 --> 00:14:27,600
manner on the data that makes sense so

407
00:14:27,600 --> 00:14:29,279
this is just an example of selecting all

408
00:14:29,279 --> 00:14:30,839
of the numeric features and then all the

409
00:14:30,839 --> 00:14:32,160
categorical features which are like

410
00:14:32,160 --> 00:14:34,500
strings and objects and then for the

411
00:14:34,500 --> 00:14:36,180
numerics we're going to fill in any

412
00:14:36,180 --> 00:14:37,800
missing values with just a constant of

413
00:14:37,800 --> 00:14:39,959
zero and then apply a standard scale to

414
00:14:39,959 --> 00:14:42,300
all of those columns and then for the

415
00:14:42,300 --> 00:14:44,940
categorical features filling a constant

416
00:14:44,940 --> 00:14:46,920
value of missing and then performing one

417
00:14:46,920 --> 00:14:48,420
hot encoding to again take those

418
00:14:48,420 --> 00:14:50,220
individual values and then make new

419
00:14:50,220 --> 00:14:52,940
columns out of them

420
00:14:53,160 --> 00:14:55,260
so now let's talk about hyper parameter

421
00:14:55,260 --> 00:14:58,079
optimization all that stuff was having

422
00:14:58,079 --> 00:14:59,880
to deal with the data and then this is

423
00:14:59,880 --> 00:15:01,560
having to deal with the model

424
00:15:01,560 --> 00:15:03,540
every machine learning model again is

425
00:15:03,540 --> 00:15:04,800
going to have its own set of hyper

426
00:15:04,800 --> 00:15:06,120
parameters that you'd want to look for

427
00:15:06,120 --> 00:15:09,420
use the random force classifier in this

428
00:15:09,420 --> 00:15:11,940
research and a random Forest is composed

429
00:15:11,940 --> 00:15:14,579
of decision trees so we see here are a

430
00:15:14,579 --> 00:15:16,380
couple of decision trees that are going

431
00:15:16,380 --> 00:15:19,019
to make a determination on each node

432
00:15:19,019 --> 00:15:22,019
whether it should Branch off into a new

433
00:15:22,019 --> 00:15:25,199
set of features and as we so the random

434
00:15:25,199 --> 00:15:27,600
force is a bunch of decision trees that

435
00:15:27,600 --> 00:15:29,519
get made and that's our forest forestry

436
00:15:29,519 --> 00:15:32,639
trees we feed in random features to each

437
00:15:32,639 --> 00:15:34,500
of those trees get predictions from them

438
00:15:34,500 --> 00:15:36,120
and then once they've all made their

439
00:15:36,120 --> 00:15:39,300
predictions we either average average it

440
00:15:39,300 --> 00:15:40,800
out if it's like a regression problem or

441
00:15:40,800 --> 00:15:42,839
we're trying to find a number or we just

442
00:15:42,839 --> 00:15:44,959
vote on like the highest number of

443
00:15:44,959 --> 00:15:47,760
classes for a classification problem and

444
00:15:47,760 --> 00:15:49,740
then the output of that is the answer

445
00:15:49,740 --> 00:15:51,660
from the model

446
00:15:51,660 --> 00:15:53,279
so the hyper parameters for random

447
00:15:53,279 --> 00:15:56,100
Forest are what you see on this slide

448
00:15:56,100 --> 00:15:57,959
there's n estimators that's the number

449
00:15:57,959 --> 00:16:01,019
of trees to build for the forest so in

450
00:16:01,019 --> 00:16:02,339
this case you see two trees so it could

451
00:16:02,339 --> 00:16:04,339
have been an estimator as was set to two

452
00:16:04,339 --> 00:16:06,720
Criterion is going to be how we measure

453
00:16:06,720 --> 00:16:09,360
the how we measure how we split at each

454
00:16:09,360 --> 00:16:11,220
branch you can have either a genie

455
00:16:11,220 --> 00:16:13,920
coefficient or an entropy value Min

456
00:16:13,920 --> 00:16:15,660
sample split is going to Define how many

457
00:16:15,660 --> 00:16:17,880
samples need to be present at a node in

458
00:16:17,880 --> 00:16:19,380
order for us to split so if we had like

459
00:16:19,380 --> 00:16:21,779
Min sample splits up to five and only

460
00:16:21,779 --> 00:16:23,940
four samples make it down to that node

461
00:16:23,940 --> 00:16:25,680
we don't split further because we

462
00:16:25,680 --> 00:16:28,260
haven't met that threshold

463
00:16:28,260 --> 00:16:30,000
um I said it you're going to get random

464
00:16:30,000 --> 00:16:32,040
features assigned to each decision tree

465
00:16:32,040 --> 00:16:34,920
and then Max features will Define the

466
00:16:34,920 --> 00:16:37,079
maximum number of features that can be

467
00:16:37,079 --> 00:16:39,060
considered at any given node for

468
00:16:39,060 --> 00:16:41,220
splitting and then lastly is max depth

469
00:16:41,220 --> 00:16:42,720
and that's how tall our decision tree

470
00:16:42,720 --> 00:16:44,519
can go basically how many times it can

471
00:16:44,519 --> 00:16:46,620
Splinter Branch off

472
00:16:46,620 --> 00:16:48,720
so hopefully this helps to visualize how

473
00:16:48,720 --> 00:16:51,120
these uh things that you would try to

474
00:16:51,120 --> 00:16:53,399
solve for feed into the actual like

475
00:16:53,399 --> 00:16:55,920
algorithm

476
00:16:55,920 --> 00:16:58,920
in order to search over and find the

477
00:16:58,920 --> 00:17:00,959
optimal hyper parameters we can use a

478
00:17:00,959 --> 00:17:03,660
function called grid search CV so it's

479
00:17:03,660 --> 00:17:06,179
going to search a grid and that grid is

480
00:17:06,179 --> 00:17:09,540
a like an object or a dictionary of the

481
00:17:09,540 --> 00:17:11,819
hyper parameters we want to look for and

482
00:17:11,819 --> 00:17:13,980
then so if we said our grid is going to

483
00:17:13,980 --> 00:17:16,079
be Criterion and we want to search for

484
00:17:16,079 --> 00:17:18,660
either Genie or for entropy and then

485
00:17:18,660 --> 00:17:20,400
we're going to have max depth and we

486
00:17:20,400 --> 00:17:22,199
want to do a range from like 4 to 10

487
00:17:22,199 --> 00:17:24,240
maybe I would spell all those out as

488
00:17:24,240 --> 00:17:26,400
part of the grid search and then we do

489
00:17:26,400 --> 00:17:29,280
CV is cross validation so this is going

490
00:17:29,280 --> 00:17:30,780
to take our training data and do some

491
00:17:30,780 --> 00:17:32,580
further splits off into some stuff

492
00:17:32,580 --> 00:17:35,100
called folds so we by default do like

493
00:17:35,100 --> 00:17:37,080
five-fold cross validation so we take

494
00:17:37,080 --> 00:17:39,120
our data split it off into five folds

495
00:17:39,120 --> 00:17:42,539
and then say then fold one maybe data

496
00:17:42,539 --> 00:17:45,179
set one of that is going to be our

497
00:17:45,179 --> 00:17:47,160
testing data and the other four are

498
00:17:47,160 --> 00:17:48,840
training and then as we go through each

499
00:17:48,840 --> 00:17:51,179
fold we swap out the testing set with a

500
00:17:51,179 --> 00:17:53,700
different one and that way we can make

501
00:17:53,700 --> 00:17:56,039
the model try out the parameters get

502
00:17:56,039 --> 00:17:58,140
predictions and then test it against the

503
00:17:58,140 --> 00:17:59,460
answers and then see how it performs

504
00:17:59,460 --> 00:18:01,440
that's how we get those optimal

505
00:18:01,440 --> 00:18:02,820
parameters

506
00:18:02,820 --> 00:18:04,620
once they've been found you can

507
00:18:04,620 --> 00:18:06,960
visualize them in pandas so as an

508
00:18:06,960 --> 00:18:08,039
example here

509
00:18:08,039 --> 00:18:11,220
we can look at the score and I found in

510
00:18:11,220 --> 00:18:13,440
this particular case I found three

511
00:18:13,440 --> 00:18:15,419
particular combinations of hyper

512
00:18:15,419 --> 00:18:19,080
parameters and features that would

513
00:18:19,080 --> 00:18:21,120
answer a particular question at 100

514
00:18:21,120 --> 00:18:23,280
accuracy so that's pretty good some of

515
00:18:23,280 --> 00:18:26,539
the other ones not as good

516
00:18:28,020 --> 00:18:30,360
okay now we'll talk about model training

517
00:18:30,360 --> 00:18:32,400
which is the easy part because we've

518
00:18:32,400 --> 00:18:34,080
already done all the stuff that we

519
00:18:34,080 --> 00:18:36,120
needed to do to build the model and make

520
00:18:36,120 --> 00:18:40,320
predictions so after having got the data

521
00:18:40,320 --> 00:18:41,640
through feature selection and

522
00:18:41,640 --> 00:18:43,380
engineering and then having got the

523
00:18:43,380 --> 00:18:44,880
optimal hyper parameters through that

524
00:18:44,880 --> 00:18:47,940
hpo process we can either just manually

525
00:18:47,940 --> 00:18:50,700
type in like the specific features and

526
00:18:50,700 --> 00:18:52,620
the specific combination of hyper

527
00:18:52,620 --> 00:18:54,600
parameters I like to just store them as

528
00:18:54,600 --> 00:18:56,640
like dictionaries and then index into

529
00:18:56,640 --> 00:18:58,799
them to get the right stuff

530
00:18:58,799 --> 00:19:01,080
make our pipeline again again taking the

531
00:19:01,080 --> 00:19:02,600
numeric data and the category

532
00:19:02,600 --> 00:19:04,799
categorical data applying those

533
00:19:04,799 --> 00:19:06,720
Transformations and then when we call

534
00:19:06,720 --> 00:19:08,760
Dot fit that's the function that's

535
00:19:08,760 --> 00:19:10,919
actually going to train the model once

536
00:19:10,919 --> 00:19:12,480
that's done we have a model that can do

537
00:19:12,480 --> 00:19:14,340
predictions so you see at the bottom we

538
00:19:14,340 --> 00:19:17,160
just call Dot predict so that case at

539
00:19:17,160 --> 00:19:19,679
the end of that we've got a an array of

540
00:19:19,679 --> 00:19:21,600
like ones or zeros to indicate whether

541
00:19:21,600 --> 00:19:24,000
that sample is predicted to be malware

542
00:19:24,000 --> 00:19:25,799
or not

543
00:19:25,799 --> 00:19:28,020
and now we want to see how good it did

544
00:19:28,020 --> 00:19:29,640
so we're going to do some model

545
00:19:29,640 --> 00:19:31,559
evaluation we've got a few different

546
00:19:31,559 --> 00:19:33,120
ways that we can do this first is going

547
00:19:33,120 --> 00:19:34,740
to be confusion Matrix which will plot

548
00:19:34,740 --> 00:19:36,299
like our true positives false positives

549
00:19:36,299 --> 00:19:38,360
true negatives and false negatives

550
00:19:38,360 --> 00:19:41,039
Precision recall will plot the Precision

551
00:19:41,039 --> 00:19:43,559
and recall values on a curve and then we

552
00:19:43,559 --> 00:19:45,000
can use that to figure out a threshold

553
00:19:45,000 --> 00:19:47,039
for actual like production purposes

554
00:19:47,039 --> 00:19:50,039
receiver operator characteristic curve

555
00:19:50,039 --> 00:19:52,020
is similar it's going to plot our true

556
00:19:52,020 --> 00:19:53,820
positive rate versus our false positive

557
00:19:53,820 --> 00:19:55,500
rate and again then we can find like

558
00:19:55,500 --> 00:19:57,419
where we want to set the threshold for

559
00:19:57,419 --> 00:19:59,460
that and then some numeric values are

560
00:19:59,460 --> 00:20:02,039
just the accuracy and then F1 score and

561
00:20:02,039 --> 00:20:04,020
F1 is like a balance between the

562
00:20:04,020 --> 00:20:05,940
Precision and recall values so if you

563
00:20:05,940 --> 00:20:08,039
want just a single number that tells you

564
00:20:08,039 --> 00:20:09,419
like how well your model is doing you

565
00:20:09,419 --> 00:20:12,179
could just get with the F1 score

566
00:20:12,179 --> 00:20:14,039
and an example of what that might look

567
00:20:14,039 --> 00:20:16,380
like is here on the left hand side we've

568
00:20:16,380 --> 00:20:18,660
got our confusion Matrix and what we

569
00:20:18,660 --> 00:20:20,640
want to see is the basically a diagonal

570
00:20:20,640 --> 00:20:23,700
of high saturation so that's a kind of

571
00:20:23,700 --> 00:20:25,500
visual cue to us that we're getting a

572
00:20:25,500 --> 00:20:27,059
high number of predictions and the true

573
00:20:27,059 --> 00:20:30,360
positives and true negative columns and

574
00:20:30,360 --> 00:20:32,880
then low predictions and the false

575
00:20:32,880 --> 00:20:35,160
positive and false negative columns in

576
00:20:35,160 --> 00:20:36,660
this particular model you can see that

577
00:20:36,660 --> 00:20:38,580
like with a false negative has a little

578
00:20:38,580 --> 00:20:40,440
bit of light blue so it didn't perform

579
00:20:40,440 --> 00:20:41,640
super great

580
00:20:41,640 --> 00:20:43,919
and then in the middle we've got a

581
00:20:43,919 --> 00:20:45,840
Precision recall curve this one just

582
00:20:45,840 --> 00:20:48,179
kind of takes a jump off a cliff and

583
00:20:48,179 --> 00:20:49,860
goes straight over to the right hand

584
00:20:49,860 --> 00:20:50,940
side there

585
00:20:50,940 --> 00:20:52,559
normally this would be a little bit more

586
00:20:52,559 --> 00:20:54,600
Jagged and would have like individual

587
00:20:54,600 --> 00:20:56,760
steps between each of the different uh

588
00:20:56,760 --> 00:20:58,620
thresholds there and then for the rock

589
00:20:58,620 --> 00:21:00,059
curve again it kind of just goes up and

590
00:21:00,059 --> 00:21:01,380
over to the right where normally we'd

591
00:21:01,380 --> 00:21:03,660
see a bit more of a jagged line and then

592
00:21:03,660 --> 00:21:07,799
accuracy and F1 score less than 95 so

593
00:21:07,799 --> 00:21:10,260
not actually super great on figuring out

594
00:21:10,260 --> 00:21:12,600
if a sample was malware or not but when

595
00:21:12,600 --> 00:21:15,360
I ran it against the packed question got

596
00:21:15,360 --> 00:21:16,559
really really good results on

597
00:21:16,559 --> 00:21:18,600
determining if a file was packed and

598
00:21:18,600 --> 00:21:19,980
then just kind of at that intersection

599
00:21:19,980 --> 00:21:23,100
somehow the uh packed malware

600
00:21:23,100 --> 00:21:24,660
predictions actually did pretty pretty

601
00:21:24,660 --> 00:21:26,700
good as well so in this case it might

602
00:21:26,700 --> 00:21:28,380
indicate that we need a different set of

603
00:21:28,380 --> 00:21:29,940
features so again maybe going back and

604
00:21:29,940 --> 00:21:31,440
looking at the strings that are present

605
00:21:31,440 --> 00:21:33,600
in the Macco files maybe doing some

606
00:21:33,600 --> 00:21:35,580
disassembly and bytecode sequencing and

607
00:21:35,580 --> 00:21:36,720
all that kind of stuff and adding those

608
00:21:36,720 --> 00:21:39,059
in its features alternatively you could

609
00:21:39,059 --> 00:21:41,520
do some like Dynamic runtime analysis

610
00:21:41,520 --> 00:21:43,140
and pull some features out of that like

611
00:21:43,140 --> 00:21:44,520
what files does it create or what

612
00:21:44,520 --> 00:21:46,080
network connections does it make and

613
00:21:46,080 --> 00:21:48,299
then use those for a separate set of

614
00:21:48,299 --> 00:21:49,500
features

615
00:21:49,500 --> 00:21:52,559
so that's kind of end to end of what I

616
00:21:52,559 --> 00:21:54,900
learned from starting off with machine

617
00:21:54,900 --> 00:21:56,460
learning and then kind of putting it

618
00:21:56,460 --> 00:21:59,039
into a practical application and it was

619
00:21:59,039 --> 00:22:01,500
just a few minutes left here I thought I

620
00:22:01,500 --> 00:22:03,600
would show you how easy it is to take

621
00:22:03,600 --> 00:22:05,940
all that work and then to go deploy it

622
00:22:05,940 --> 00:22:09,299
into production into Amazon sagemaker

623
00:22:09,299 --> 00:22:11,820
and ignoring this thing for a second I

624
00:22:11,820 --> 00:22:13,620
do think that sagemaker is a super cool

625
00:22:13,620 --> 00:22:15,840
product to make it easy to do a whole

626
00:22:15,840 --> 00:22:18,059
bunch of this machine learning stuff and

627
00:22:18,059 --> 00:22:20,039
now that I work there and I get to use

628
00:22:20,039 --> 00:22:23,059
it for free it's even better

629
00:22:23,059 --> 00:22:25,799
uh but it is really simple to take all

630
00:22:25,799 --> 00:22:28,140
of that work and then go deploy it so a

631
00:22:28,140 --> 00:22:29,400
couple things first we're going to

632
00:22:29,400 --> 00:22:31,140
install sagemaker into this Jupiter

633
00:22:31,140 --> 00:22:33,780
notebook next we import joblib which is

634
00:22:33,780 --> 00:22:36,059
going to let me export my model into a

635
00:22:36,059 --> 00:22:37,860
single file I'm going to take the

636
00:22:37,860 --> 00:22:39,780
features that I had done the engineering

637
00:22:39,780 --> 00:22:41,820
and selection on export that as a Json

638
00:22:41,820 --> 00:22:43,799
file the only thing you don't see on

639
00:22:43,799 --> 00:22:46,500
this slide is all of that python that I

640
00:22:46,500 --> 00:22:48,240
wrote that I told you about to kind of

641
00:22:48,240 --> 00:22:50,100
take the Json objects and then make a

642
00:22:50,100 --> 00:22:52,500
flat Json just took that copy and pasted

643
00:22:52,500 --> 00:22:54,240
it out into a separate file called

644
00:22:54,240 --> 00:22:56,700
script.pi and then you have to make four

645
00:22:56,700 --> 00:22:58,620
functions for sagemaker but it's like 10

646
00:22:58,620 --> 00:23:00,480
lines of code total so it's uh pretty

647
00:23:00,480 --> 00:23:01,620
easy there

648
00:23:01,620 --> 00:23:04,919
so with the model with the features and

649
00:23:04,919 --> 00:23:07,740
then with the script.py

650
00:23:07,740 --> 00:23:09,900
we're going to Tar all of that up

651
00:23:09,900 --> 00:23:13,020
and then upload it into S3 we import

652
00:23:13,020 --> 00:23:15,720
sagemaker and then call sklearn model

653
00:23:15,720 --> 00:23:18,600
pointing it to that tar file that was

654
00:23:18,600 --> 00:23:21,360
uploaded into S3 tell it when you start

655
00:23:21,360 --> 00:23:24,360
the container run the script.pi that

656
00:23:24,360 --> 00:23:26,520
we've uploaded to S3 and then lastly

657
00:23:26,520 --> 00:23:28,260
just tell it to deploy our model for

658
00:23:28,260 --> 00:23:30,360
real-time inference and so once that

659
00:23:30,360 --> 00:23:32,159
completes you have something that's in

660
00:23:32,159 --> 00:23:33,780
the cloud that you can send data to and

661
00:23:33,780 --> 00:23:36,659
then you can get predictions back from

662
00:23:36,659 --> 00:23:38,100
oops

663
00:23:38,100 --> 00:23:40,860
and so here's an example of that I had a

664
00:23:40,860 --> 00:23:42,780
separate set of data where I just went

665
00:23:42,780 --> 00:23:44,340
through virus total downloaded some new

666
00:23:44,340 --> 00:23:46,200
malware files ran them through Macho

667
00:23:46,200 --> 00:23:48,720
Libre so read the Json file and then

668
00:23:48,720 --> 00:23:51,600
just call Dot predict on my predictor it

669
00:23:51,600 --> 00:23:54,840
sends the Json up into sagemaker within

670
00:23:54,840 --> 00:23:56,520
a second it process it and then gives a

671
00:23:56,520 --> 00:23:58,799
result back in this case in Json and we

672
00:23:58,799 --> 00:24:00,419
see that the verdict was malware which

673
00:24:00,419 --> 00:24:01,860
is exactly what I was expecting from

674
00:24:01,860 --> 00:24:04,860
this case so yeah that's that's how easy

675
00:24:04,860 --> 00:24:06,480
it is to take all of that work that

676
00:24:06,480 --> 00:24:08,159
you've already done and then just go

677
00:24:08,159 --> 00:24:10,080
upload it to there so that was really

678
00:24:10,080 --> 00:24:13,140
cool and then um as kind of mentioned by

679
00:24:13,140 --> 00:24:15,780
Kristen earlier what you don't see is

680
00:24:15,780 --> 00:24:17,340
all the time that I spent banging my

681
00:24:17,340 --> 00:24:18,659
head against the desk trying to figure

682
00:24:18,659 --> 00:24:20,280
out how this stuff works there's

683
00:24:20,280 --> 00:24:22,380
certainly a learning curve to all of the

684
00:24:22,380 --> 00:24:24,720
things and I am by far not smart enough

685
00:24:24,720 --> 00:24:27,059
to learn it by myself so I'd like to

686
00:24:27,059 --> 00:24:28,260
give a shout out to the people that you

687
00:24:28,260 --> 00:24:29,640
see on this slide for helping me out

688
00:24:29,640 --> 00:24:31,860
along the way and there's many more who

689
00:24:31,860 --> 00:24:33,960
uh didn't make this cut but they're so

690
00:24:33,960 --> 00:24:36,419
cool and so thank you to them as well

691
00:24:36,419 --> 00:24:38,400
and for you all for being here and

692
00:24:38,400 --> 00:24:41,059
listening to me

693
00:24:49,919 --> 00:24:51,900
and I guess I have time for questions if

694
00:24:51,900 --> 00:24:55,080
anybody has any yes sir

695
00:24:55,080 --> 00:24:57,260
foreign

696
00:24:57,440 --> 00:24:59,940
do I have the Jupiter notebook up in

697
00:24:59,940 --> 00:25:03,480
Google collab I don't I do have it up in

698
00:25:03,480 --> 00:25:06,480
GitHub so you can pull it down and I run

699
00:25:06,480 --> 00:25:08,159
Jupiter either in my home lab or in

700
00:25:08,159 --> 00:25:10,080
sagemaker and then you can run that

701
00:25:10,080 --> 00:25:13,100
notebook wherever you'd like to

702
00:25:23,340 --> 00:25:25,140
yeah so the question was like how often

703
00:25:25,140 --> 00:25:27,840
do you have to update the model and uh

704
00:25:27,840 --> 00:25:30,299
like all good security questions the

705
00:25:30,299 --> 00:25:33,179
answer is it depends so you'd want to

706
00:25:33,179 --> 00:25:34,919
evaluate the performance of that model

707
00:25:34,919 --> 00:25:37,679
over time and then determine your

708
00:25:37,679 --> 00:25:39,539
criteria for when you'd want to retrain

709
00:25:39,539 --> 00:25:41,520
it so let's say it's going for a month

710
00:25:41,520 --> 00:25:43,260
and then you see like the accuracy

711
00:25:43,260 --> 00:25:44,580
scores are starting to dip for whatever

712
00:25:44,580 --> 00:25:46,440
reason that could be an indicator that

713
00:25:46,440 --> 00:25:48,659
you'd want to go and retrain it if

714
00:25:48,659 --> 00:25:51,360
you're dealing with like time-based data

715
00:25:51,360 --> 00:25:54,179
you might have some sort of indicator in

716
00:25:54,179 --> 00:25:56,640
that time like seasonality or something

717
00:25:56,640 --> 00:25:58,320
like that that might make you want to

718
00:25:58,320 --> 00:26:00,120
like retrain it

719
00:26:00,120 --> 00:26:02,400
so I know it's not super prescriptive

720
00:26:02,400 --> 00:26:05,120
sorry

721
00:26:21,720 --> 00:26:23,460
uh yeah so the question was uh did I

722
00:26:23,460 --> 00:26:25,200
find like any super sketchy

723
00:26:25,200 --> 00:26:27,600
uh samples while downloading all that

724
00:26:27,600 --> 00:26:30,020
malware

725
00:26:31,320 --> 00:26:33,539
okay or if it was a benign file but it

726
00:26:33,539 --> 00:26:35,220
just looked kind of sketchy

727
00:26:35,220 --> 00:26:37,679
um I didn't you know dive deep into each

728
00:26:37,679 --> 00:26:41,220
individual sample so I couldn't say it

729
00:26:41,220 --> 00:26:43,020
gave me a feeling but that's probably

730
00:26:43,020 --> 00:26:46,020
reflected in some of those uh accuracy

731
00:26:46,020 --> 00:26:47,460
scores right because

732
00:26:47,460 --> 00:26:49,500
um the machine learned what it thinks

733
00:26:49,500 --> 00:26:50,820
was sketchy based off of what I

734
00:26:50,820 --> 00:26:51,779
downloaded so some of those

735
00:26:51,779 --> 00:26:54,059
misclassifications are probably because

736
00:26:54,059 --> 00:26:55,980
there were sketchy things in those

737
00:26:55,980 --> 00:26:57,840
benign files that just made the computer

738
00:26:57,840 --> 00:27:01,039
think that they were malicious

739
00:27:03,900 --> 00:27:06,299
you mentioned the challenges that have

740
00:27:06,299 --> 00:27:09,059
worked of first Gathering sufficient

741
00:27:09,059 --> 00:27:12,000
sample size essentially uh do you think

742
00:27:12,000 --> 00:27:13,919
that your models will improve or we'll

743
00:27:13,919 --> 00:27:15,480
need some tweaks kind of to fall off

744
00:27:15,480 --> 00:27:17,820
that question as you collect more mac

745
00:27:17,820 --> 00:27:20,520
malware or do you think that you have a

746
00:27:20,520 --> 00:27:21,779
pretty solid Baseline and it'll more

747
00:27:21,779 --> 00:27:23,460
just be kind of like it depends like you

748
00:27:23,460 --> 00:27:26,220
said yeah um so it wasn't a huge data

749
00:27:26,220 --> 00:27:27,659
set it was like a thousand samples an

750
00:27:27,659 --> 00:27:29,940
hour and thousand benign files so if you

751
00:27:29,940 --> 00:27:31,380
did something like if you had VT

752
00:27:31,380 --> 00:27:33,059
Enterprise and you could download all of

753
00:27:33,059 --> 00:27:36,059
their Macos having more data is almost

754
00:27:36,059 --> 00:27:38,520
always better and then what I didn't

755
00:27:38,520 --> 00:27:40,559
touch on was uh another way to collect

756
00:27:40,559 --> 00:27:42,240
samples right is if you're really

757
00:27:42,240 --> 00:27:44,159
hardcore you just go set up a botnet or

758
00:27:44,159 --> 00:27:47,340
a honeynet and uh let people give you

759
00:27:47,340 --> 00:27:49,799
samples on the internet and so or about

760
00:27:49,799 --> 00:27:52,380
now too and then whatever

761
00:27:52,380 --> 00:27:54,960
no judgment

762
00:27:54,960 --> 00:27:56,700
have you talked to any of the antivirus

763
00:27:56,700 --> 00:27:58,020
companies because I imagine they have

764
00:27:58,020 --> 00:28:00,659
pretty good sample sets I know they're

765
00:28:00,659 --> 00:28:02,820
not always willing to share but you know

766
00:28:02,820 --> 00:28:04,860
if you're developing this lovely model

767
00:28:04,860 --> 00:28:06,419
and this machine learning stuff maybe

768
00:28:06,419 --> 00:28:07,980
they'd be open collaboration have you

769
00:28:07,980 --> 00:28:09,900
thought about that or talked to them or

770
00:28:09,900 --> 00:28:11,279
tried and failed like what are your

771
00:28:11,279 --> 00:28:13,620
thoughts on that yeah I did not but that

772
00:28:13,620 --> 00:28:16,279
is a good idea

773
00:28:17,179 --> 00:28:19,500
let's give chemo again a round of

774
00:28:19,500 --> 00:28:19,930
applause

775
00:28:19,930 --> 00:28:27,779
[Applause]

