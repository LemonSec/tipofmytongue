1
00:00:04,859 --> 00:00:07,700
this one

2
00:00:08,400 --> 00:00:11,540
I see Barry right

3
00:00:14,059 --> 00:00:18,960
yeah it's uh pretty empty room so far

4
00:00:18,960 --> 00:00:21,560
yeah

5
00:00:27,599 --> 00:00:29,580
so it's a little awkward doing the uh

6
00:00:29,580 --> 00:00:31,500
the chairing when the chair is remotes

7
00:00:31,500 --> 00:00:34,579
but the speakers are local

8
00:00:34,680 --> 00:00:37,260
I don't know if I hope the speakers are

9
00:00:37,260 --> 00:00:39,440
local

10
00:00:44,470 --> 00:00:47,740
[Music]

11
00:00:50,500 --> 00:00:53,579
[Music]

12
00:01:05,339 --> 00:01:08,339
foreign

13
00:01:57,479 --> 00:02:00,479
foreign

14
00:02:39,599 --> 00:02:42,599
foreign

15
00:03:51,480 --> 00:03:54,200
foreign

16
00:03:57,000 --> 00:03:58,920
questions

17
00:03:58,920 --> 00:04:03,000
um I see some human touches along with

18
00:04:03,000 --> 00:04:06,080
the other presents are in the room

19
00:04:44,639 --> 00:04:47,639
exactly

20
00:05:19,680 --> 00:05:21,860
foreign

21
00:06:01,259 --> 00:06:04,100
laughs

22
00:06:26,759 --> 00:06:28,940
okay

23
00:06:40,560 --> 00:06:42,360
so

24
00:06:42,360 --> 00:06:44,780
okay

25
00:07:33,000 --> 00:07:35,840
I don't know

26
00:08:12,780 --> 00:08:14,900
thank you

27
00:08:17,300 --> 00:08:21,020
okay I could get a few minutes

28
00:08:21,020 --> 00:08:24,620
you get started it's uh

29
00:08:24,620 --> 00:08:27,419
there and if so do you want to come up

30
00:08:27,419 --> 00:08:31,020
and get ready for the microphone

31
00:08:31,020 --> 00:08:35,179
while I do the introductory slates

32
00:09:04,450 --> 00:09:07,080
[Music]

33
00:09:07,080 --> 00:09:09,260
thank you

34
00:09:19,339 --> 00:09:23,279
okay Tasha can you do another check I

35
00:09:23,279 --> 00:09:25,380
see you're there now it's a cute little

36
00:09:25,380 --> 00:09:28,399
camera to work

37
00:09:38,279 --> 00:09:41,180
foreign

38
00:10:06,800 --> 00:10:11,700
Perkins I'm the irtf chair hopefully you

39
00:10:11,700 --> 00:10:15,180
cannot see it and hear me in the room

40
00:10:15,180 --> 00:10:17,779
I'm remote

41
00:10:18,660 --> 00:10:20,899
foreign

42
00:10:25,260 --> 00:10:28,260
slides

43
00:10:33,720 --> 00:10:35,779
um

44
00:10:37,620 --> 00:10:40,200
okay let's make her but hopefully it's

45
00:10:40,200 --> 00:10:43,500
not too bad uh so uh this is the irtf

46
00:10:43,500 --> 00:10:46,640
open meeting the the irtf follows the

47
00:10:46,640 --> 00:10:48,839
itf's intellectual property rights

48
00:10:48,839 --> 00:10:51,720
disclosure rules uh and a reminder that

49
00:10:51,720 --> 00:10:54,240
by participating in this meeting and by

50
00:10:54,240 --> 00:10:56,220
commenting on the presentations that you

51
00:10:56,220 --> 00:10:59,760
you agree to follow the irtf processes

52
00:10:59,760 --> 00:11:02,459
and procedures including disclosing any

53
00:11:02,459 --> 00:11:05,040
intellectual property relating to the

54
00:11:05,040 --> 00:11:07,200
contributions that you make

55
00:11:07,200 --> 00:11:09,360
uh I'm sure most of you have seen these

56
00:11:09,360 --> 00:11:11,760
slides before the the details are in the

57
00:11:11,760 --> 00:11:13,800
documents linked but uh

58
00:11:13,800 --> 00:11:16,320
essentially if if you have IPR on the

59
00:11:16,320 --> 00:11:17,760
documents you're talking about you you

60
00:11:17,760 --> 00:11:19,079
need to disclose that if you're

61
00:11:19,079 --> 00:11:22,939
commenting a microphone

62
00:11:23,940 --> 00:11:27,480
in addition a reminder that uh the iitf

63
00:11:27,480 --> 00:11:29,700
routinely makes recordings of these

64
00:11:29,700 --> 00:11:32,040
meetings uh available both the the

65
00:11:32,040 --> 00:11:34,440
online and the in-person person meetings

66
00:11:34,440 --> 00:11:37,320
including this one and this meeting is

67
00:11:37,320 --> 00:11:41,940
being streamed uh live on YouTube as

68
00:11:41,940 --> 00:11:46,440
well as via the usual meat echo system

69
00:11:46,440 --> 00:11:48,480
um if you're participating in person and

70
00:11:48,480 --> 00:11:50,940
you are not wearing one of the red uh do

71
00:11:50,940 --> 00:11:52,740
not photograph lanyards then you can

72
00:11:52,740 --> 00:11:54,959
send to appear in these recordings and

73
00:11:54,959 --> 00:11:57,420
if you speak at the microphones

74
00:11:57,420 --> 00:12:00,660
um then again you're consenting to being

75
00:12:00,660 --> 00:12:03,120
recorded and as I say the recording is

76
00:12:03,120 --> 00:12:06,060
being made available on YouTube

77
00:12:06,060 --> 00:12:07,980
equally uh if you're participating

78
00:12:07,980 --> 00:12:10,380
online and you turn on your camera or

79
00:12:10,380 --> 00:12:11,700
your microphone that will make a

80
00:12:11,700 --> 00:12:14,579
contribution then that is being recorded

81
00:12:14,579 --> 00:12:17,000
and you can consent to being recorded

82
00:12:17,000 --> 00:12:20,220
and also the the chat is also being

83
00:12:20,220 --> 00:12:22,500
recorded and will be made available in

84
00:12:22,500 --> 00:12:25,079
the the usual jabber archives

85
00:12:25,079 --> 00:12:27,680
all right

86
00:12:27,959 --> 00:12:32,100
that's a participant in the iitf

87
00:12:32,100 --> 00:12:35,100
as I say you uh acknowledge that uh

88
00:12:35,100 --> 00:12:37,200
recordings of the meeting may be made

89
00:12:37,200 --> 00:12:39,660
available and that the previous that any

90
00:12:39,660 --> 00:12:41,279
personal information you provide will be

91
00:12:41,279 --> 00:12:42,660
handled in accordance with the privacy

92
00:12:42,660 --> 00:12:43,800
policy

93
00:12:43,800 --> 00:12:46,380
and you also agree to work respectfully

94
00:12:46,380 --> 00:12:48,720
with the other participants in the ietf

95
00:12:48,720 --> 00:12:51,360
and the irtf and if you have any uh

96
00:12:51,360 --> 00:12:54,839
issues or concerns about that speak to

97
00:12:54,839 --> 00:12:57,300
me or speak with the onwards team

98
00:12:57,300 --> 00:13:00,060
uh and the the itf's code of conduct and

99
00:13:00,060 --> 00:13:03,180
anti-harassment procedures uh linked on

100
00:13:03,180 --> 00:13:07,219
the slide also applied to the irtf

101
00:13:09,000 --> 00:13:12,480
for those of you participating in person

102
00:13:12,480 --> 00:13:14,700
um please sign in using the the mobile

103
00:13:14,700 --> 00:13:18,060
meter code that the meteco light Tool uh

104
00:13:18,060 --> 00:13:20,160
we're running the queue electronically

105
00:13:20,160 --> 00:13:21,899
so if you have questions then we're

106
00:13:21,899 --> 00:13:23,459
using the electronic queue that's

107
00:13:23,459 --> 00:13:26,820
accessed via the meteco tool

108
00:13:26,820 --> 00:13:28,800
um and keeps the audio and video off if

109
00:13:28,800 --> 00:13:30,540
you're using the on-site version that

110
00:13:30,540 --> 00:13:32,639
the meateker light tool

111
00:13:32,639 --> 00:13:35,160
uh remote participants uh please leave

112
00:13:35,160 --> 00:13:36,839
your audio and video off and unless

113
00:13:36,839 --> 00:13:38,880
you're you're presenting

114
00:13:38,880 --> 00:13:39,480
um

115
00:13:39,480 --> 00:13:42,839
uh or asking a question uh just to avoid

116
00:13:42,839 --> 00:13:45,740
feedback and

117
00:13:46,880 --> 00:13:49,980
also a reminder for those of you who are

118
00:13:49,980 --> 00:13:53,579
attending the meeting in person uh as a

119
00:13:53,579 --> 00:13:56,339
covet safety measure the ITF is

120
00:13:56,339 --> 00:13:59,040
requiring those those of you attending

121
00:13:59,040 --> 00:14:02,279
the meeting in person to wear an ffp2

122
00:14:02,279 --> 00:14:04,160
and 95 mask

123
00:14:04,160 --> 00:14:06,839
or its equivalent uh and the only

124
00:14:06,839 --> 00:14:09,360
exception for that is the the chairs and

125
00:14:09,360 --> 00:14:12,060
the presenters who are actively speaking

126
00:14:12,060 --> 00:14:14,220
uh in particular participants who are

127
00:14:14,220 --> 00:14:15,899
making comments or asking questions from

128
00:14:15,899 --> 00:14:18,300
the floor microphones are expected to

129
00:14:18,300 --> 00:14:20,399
wear a mask at all times including while

130
00:14:20,399 --> 00:14:22,920
they're asking those questions

131
00:14:22,920 --> 00:14:24,839
as I said the only exception of for that

132
00:14:24,839 --> 00:14:27,180
is the the active presenter at the front

133
00:14:27,180 --> 00:14:29,719
of the room

134
00:14:32,880 --> 00:14:36,180
okay so uh as I say this is the the irtf

135
00:14:36,180 --> 00:14:39,120
open meeting uh the goals of the irtf

136
00:14:39,120 --> 00:14:42,000
are to complement the standards work

137
00:14:42,000 --> 00:14:44,220
being done in the ietf by focusing on

138
00:14:44,220 --> 00:14:47,399
some of the longer term research issues

139
00:14:47,399 --> 00:14:49,500
uh the iitf is very much a research

140
00:14:49,500 --> 00:14:51,600
organization it's not a standards

141
00:14:51,600 --> 00:14:54,480
development organization and while it

142
00:14:54,480 --> 00:14:57,240
can publish rfcs and and we we do

143
00:14:57,240 --> 00:14:58,860
publish both experimental and

144
00:14:58,860 --> 00:15:00,720
informational documents on the RFC

145
00:15:00,720 --> 00:15:03,000
series that the primary outputs of the

146
00:15:03,000 --> 00:15:05,760
irtf is research is understanding his

147
00:15:05,760 --> 00:15:08,779
research papers

148
00:15:12,360 --> 00:15:15,720
the irtf is organized as a series of

149
00:15:15,720 --> 00:15:17,339
research groups

150
00:15:17,339 --> 00:15:19,860
um hopefully you you can see them on the

151
00:15:19,860 --> 00:15:22,019
slide here the the crypto Forum group

152
00:15:22,019 --> 00:15:24,720
and the uh privacy enhance enhancements

153
00:15:24,720 --> 00:15:27,959
and assessments groups met earlier today

154
00:15:27,959 --> 00:15:30,120
um the the other groups men uh so

155
00:15:30,120 --> 00:15:32,040
highlighted in dark blue on the slider

156
00:15:32,040 --> 00:15:34,620
meeting later in this week uh so please

157
00:15:34,620 --> 00:15:35,880
do

158
00:15:35,880 --> 00:15:38,279
um look out for those groups uh this

159
00:15:38,279 --> 00:15:40,079
week and try and attend the sessions if

160
00:15:40,079 --> 00:15:43,699
you're interested in those topics

161
00:15:44,880 --> 00:15:48,120
a little bit of research groups news uh

162
00:15:48,120 --> 00:15:50,759
I'd like to welcome Curtis heimerel

163
00:15:50,759 --> 00:15:54,000
who's recently joined as coacher of the

164
00:15:54,000 --> 00:15:56,759
Gaia group the global access to the

165
00:15:56,759 --> 00:15:59,579
internet for all research group

166
00:15:59,579 --> 00:16:02,160
um Curtis will be joining uh Leandro and

167
00:16:02,160 --> 00:16:04,199
Navarro who is

168
00:16:04,199 --> 00:16:06,720
um planning on stepping down from from

169
00:16:06,720 --> 00:16:08,459
sharing that group after this meeting

170
00:16:08,459 --> 00:16:12,420
and Jane coffin who is continuing so I'd

171
00:16:12,420 --> 00:16:17,100
like to welcome uh Curtis uh

172
00:16:17,100 --> 00:16:20,339
and um thank him for his service and

173
00:16:20,339 --> 00:16:22,500
thankfully Andrew for his his many years

174
00:16:22,500 --> 00:16:24,839
of service to the group

175
00:16:24,839 --> 00:16:28,199
I very much appreciate the efforts the

176
00:16:28,199 --> 00:16:30,480
Endo has put into chair in the group and

177
00:16:30,480 --> 00:16:32,519
I look forward to to working with Curtis

178
00:16:32,519 --> 00:16:34,380
going forward

179
00:16:34,380 --> 00:16:38,720
so thank you both thank you both

180
00:16:42,060 --> 00:16:45,120
did I say the irtf is primarily a

181
00:16:45,120 --> 00:16:47,220
research organization which have not

182
00:16:47,220 --> 00:16:50,399
published many rfcs we've had one RFC uh

183
00:16:50,399 --> 00:16:52,440
published since the last meeting

184
00:16:52,440 --> 00:16:53,759
um from the information Centric

185
00:16:53,759 --> 00:16:55,560
networking group looking at

186
00:16:55,560 --> 00:16:59,040
architectural considerations for using

187
00:16:59,040 --> 00:17:01,680
an ICN main resolution service

188
00:17:01,680 --> 00:17:05,400
but primarily the the iitf tends not to

189
00:17:05,400 --> 00:17:07,380
publish much in the RFC series and the

190
00:17:07,380 --> 00:17:09,720
output is more in form of interesting

191
00:17:09,720 --> 00:17:11,520
presentations and understanding and

192
00:17:11,520 --> 00:17:14,220
research papers

193
00:17:14,220 --> 00:17:16,400
foreign

194
00:17:21,140 --> 00:17:24,660
networking research price and the the

195
00:17:24,660 --> 00:17:27,419
goal of this prize is to recognize that

196
00:17:27,419 --> 00:17:29,700
some of the best recent results in

197
00:17:29,700 --> 00:17:33,200
applied networking research uh is to to

198
00:17:33,200 --> 00:17:35,400
recognize some interesting new ideas

199
00:17:35,400 --> 00:17:37,799
which are potentially relevant to the

200
00:17:37,799 --> 00:17:39,600
internet standards Community going

201
00:17:39,600 --> 00:17:41,700
forward is to recognize up and coming

202
00:17:41,700 --> 00:17:43,860
people who are likely to have an impact

203
00:17:43,860 --> 00:17:46,200
on the internet standards process and

204
00:17:46,200 --> 00:17:48,360
internet technology

205
00:17:48,360 --> 00:17:50,580
uh we're very grateful to the internet

206
00:17:50,580 --> 00:17:53,880
Society to Comcast and NBC Universal for

207
00:17:53,880 --> 00:17:57,000
their sponsorship of the anip that

208
00:17:57,000 --> 00:18:01,080
allows us to make these Awards

209
00:18:01,080 --> 00:18:03,120
bring to bring people to give these

210
00:18:03,120 --> 00:18:05,539
thoughts

211
00:18:07,320 --> 00:18:09,059
and uh

212
00:18:09,059 --> 00:18:12,600
what we're doing today is uh the goal of

213
00:18:12,600 --> 00:18:15,120
this session is to to make some of these

214
00:18:15,120 --> 00:18:17,280
Awards so I would like to congratulate

215
00:18:17,280 --> 00:18:21,480
uh Tasha Swami and some Kumar who will

216
00:18:21,480 --> 00:18:25,580
be giving that there are World talks

217
00:18:26,000 --> 00:18:29,160
this session today

218
00:18:29,160 --> 00:18:31,320
um Tasha will be talking first in a

219
00:18:31,320 --> 00:18:33,179
couple of minutes uh talking about his

220
00:18:33,179 --> 00:18:35,520
work on data plane architectures lots of

221
00:18:35,520 --> 00:18:37,620
the line rates in for insurance

222
00:18:37,620 --> 00:18:40,380
um and Sam will be following later in

223
00:18:40,380 --> 00:18:42,539
the session talking about TCP low

224
00:18:42,539 --> 00:18:44,700
powerlessness

225
00:18:44,700 --> 00:18:46,559
uh we've got two really really good

226
00:18:46,559 --> 00:18:50,039
talks coming so please do uh please

227
00:18:50,039 --> 00:18:51,780
Instagram pay attention to those and

228
00:18:51,780 --> 00:18:55,260
again congratulations to Natasha and to

229
00:18:55,260 --> 00:18:57,919
sex

230
00:18:58,860 --> 00:19:00,539
going forward

231
00:19:00,539 --> 00:19:03,000
um look out for a little bit more more

232
00:19:03,000 --> 00:19:05,480
talks uh go to some

233
00:19:05,480 --> 00:19:08,340
current cash and Daniel Wagner will be

234
00:19:08,340 --> 00:19:11,520
doing the talks as it f115 and the

235
00:19:11,520 --> 00:19:13,440
nominations for the

236
00:19:13,440 --> 00:19:14,460
um

237
00:19:14,460 --> 00:19:17,100
nominations for this for 2023 Awards

238
00:19:17,100 --> 00:19:20,100
will be opening in September 2022 so to

239
00:19:20,100 --> 00:19:22,919
look out for those

240
00:19:22,919 --> 00:19:26,240
um and we'll care for the nominations

241
00:19:32,940 --> 00:19:35,820
okay did the audio improve trip meeting

242
00:19:35,820 --> 00:19:39,799
and restarting hopefully you can hear me

243
00:19:41,640 --> 00:19:44,039
okay

244
00:19:44,039 --> 00:19:46,200
okay hopefully that's better as I was

245
00:19:46,200 --> 00:19:48,299
saying look out for the nomination to

246
00:19:48,299 --> 00:19:50,760
the 2023 anrp

247
00:19:50,760 --> 00:19:54,000
um in September this year and

248
00:19:54,000 --> 00:19:56,460
um congratulations to tusha and to Sam

249
00:19:56,460 --> 00:19:59,039
who will be giving their their NRP talks

250
00:19:59,039 --> 00:20:01,220
today

251
00:20:02,280 --> 00:20:05,700
in addition to the applied networking

252
00:20:05,700 --> 00:20:09,539
research prize we also host the applied

253
00:20:09,539 --> 00:20:11,340
networking research Workshop which we

254
00:20:11,340 --> 00:20:14,659
organize in conjunction with ACM sitcom

255
00:20:14,659 --> 00:20:16,980
this Workshop is taking place tomorrow

256
00:20:16,980 --> 00:20:19,260
it's co-locating with the ITF in

257
00:20:19,260 --> 00:20:22,740
Philadelphia so thank you to TJ Chang

258
00:20:22,740 --> 00:20:25,140
and Marwan fired who the chairs this

259
00:20:25,140 --> 00:20:26,520
year and who've been organizing that

260
00:20:26,520 --> 00:20:28,320
Workshop

261
00:20:28,320 --> 00:20:31,260
um we've got a program of uh I think

262
00:20:31,260 --> 00:20:33,600
there are four four really nice research

263
00:20:33,600 --> 00:20:35,940
papers a keynote and some Innovative

264
00:20:35,940 --> 00:20:38,280
talks on novel approaches to protocol

265
00:20:38,280 --> 00:20:40,980
specification as I say that the

266
00:20:40,980 --> 00:20:42,840
workshop's happening tomorrow

267
00:20:42,840 --> 00:20:45,780
um if you're there in in person then

268
00:20:45,780 --> 00:20:48,419
please do consider attending if you're

269
00:20:48,419 --> 00:20:50,220
attending remotely then you can register

270
00:20:50,220 --> 00:20:51,780
and attend

271
00:20:51,780 --> 00:20:54,419
um registration is free for anyone who's

272
00:20:54,419 --> 00:20:56,760
also registered for the ITF although we

273
00:20:56,760 --> 00:20:58,799
we do ask you to to register separately

274
00:20:58,799 --> 00:21:02,700
so we know who's attending the workshop

275
00:21:02,700 --> 00:21:05,700
um and the anow next year will be again

276
00:21:05,700 --> 00:21:08,640
co-locating with the the ATF in July

277
00:21:08,640 --> 00:21:10,080
2023

278
00:21:10,080 --> 00:21:11,700
um which is planned to be in San

279
00:21:11,700 --> 00:21:13,820
Francisco

280
00:21:15,799 --> 00:21:18,419
and to finish up before we get to the

281
00:21:18,419 --> 00:21:21,240
talks uh I'd just like to

282
00:21:21,240 --> 00:21:23,340
um note that we we are very pleased to

283
00:21:23,340 --> 00:21:24,840
offer a number of travel grants for

284
00:21:24,840 --> 00:21:26,580
these meetings

285
00:21:26,580 --> 00:21:29,100
um both to support early career

286
00:21:29,100 --> 00:21:32,400
academics and and PhD students from

287
00:21:32,400 --> 00:21:34,679
underrepresented groups to to attend the

288
00:21:34,679 --> 00:21:37,679
irtf research groups and a number of

289
00:21:37,679 --> 00:21:40,320
travel grants for the applied networking

290
00:21:40,320 --> 00:21:42,140
research Workshop

291
00:21:42,140 --> 00:21:45,539
thank you very much to the travel Grant

292
00:21:45,539 --> 00:21:48,240
sponsors to Akamai Comcast cloudflare

293
00:21:48,240 --> 00:21:51,659
and Netflix for supporting that

294
00:21:51,659 --> 00:21:54,240
um if you're you know please see the the

295
00:21:54,240 --> 00:21:56,220
travel credits page linked from the

296
00:21:56,220 --> 00:21:57,120
website

297
00:21:57,120 --> 00:22:01,200
um the details of that and if if you're

298
00:22:01,200 --> 00:22:04,500
interested in sponsoring the travel

299
00:22:04,500 --> 00:22:05,940
grants in the future or if you're

300
00:22:05,940 --> 00:22:07,200
interested in applying for a travel

301
00:22:07,200 --> 00:22:10,200
Grant see that webpage or contact me for

302
00:22:10,200 --> 00:22:12,059
for details of the sponsorship

303
00:22:12,059 --> 00:22:13,559
opportunities

304
00:22:13,559 --> 00:22:15,419
and again thank you very much for the

305
00:22:15,419 --> 00:22:17,720
spices

306
00:22:19,559 --> 00:22:21,840
so that's uh essentially all I have to

307
00:22:21,840 --> 00:22:22,980
say today

308
00:22:22,980 --> 00:22:23,700
um

309
00:22:23,700 --> 00:22:26,940
the agenda for the remainder of the day

310
00:22:26,940 --> 00:22:29,760
um we have the the two anrp award talks

311
00:22:29,760 --> 00:22:33,360
uh Tasha Swami will be first talking

312
00:22:33,360 --> 00:22:35,820
about Taurus a data playing architecture

313
00:22:35,820 --> 00:22:38,640
for per packet machine learning and that

314
00:22:38,640 --> 00:22:40,799
will be followed by Sam Sam Kumar's talk

315
00:22:40,799 --> 00:22:43,440
on performance TCP for for low power

316
00:22:43,440 --> 00:22:46,460
wireless networks

317
00:22:47,340 --> 00:22:50,480
okay um I will at this point switch over

318
00:22:50,480 --> 00:22:54,539
to uh Tasha can you check the microphone

319
00:22:54,539 --> 00:22:57,360
when I get the slides up

320
00:22:57,360 --> 00:23:00,199
yes it's okay

321
00:23:02,280 --> 00:23:06,539
uh yep I can hear you remotely

322
00:23:06,539 --> 00:23:09,740
is it working in the room

323
00:23:14,640 --> 00:23:16,500
should I get started

324
00:23:16,500 --> 00:23:19,679
yes just one one second if you have a

325
00:23:19,679 --> 00:23:21,900
phone I can pass you control so you can

326
00:23:21,900 --> 00:23:23,640
control the slides yourself if you have

327
00:23:23,640 --> 00:23:26,580
the meat Echo light uh if not then

328
00:23:26,580 --> 00:23:28,559
um shout when you want to go to the next

329
00:23:28,559 --> 00:23:31,039
slide

330
00:23:36,120 --> 00:23:40,799
okay so I should have control over that

331
00:23:40,799 --> 00:23:43,679
um well uh Tisha is checking to see if

332
00:23:43,679 --> 00:23:45,960
that works uh I'd just like to say that

333
00:23:45,960 --> 00:23:49,260
uh the as I said the first talk hey it's

334
00:23:49,260 --> 00:23:50,820
Tasha Swami who'll be talking about

335
00:23:50,820 --> 00:23:53,460
Taurus a data plane architecture for per

336
00:23:53,460 --> 00:23:57,179
packet ml uh Tasha is a PhD candidate in

337
00:23:57,179 --> 00:23:58,799
the electrical engineering department at

338
00:23:58,799 --> 00:24:02,760
Stanford uh his research is focusing on

339
00:24:02,760 --> 00:24:04,200
the intersection of machine learning

340
00:24:04,200 --> 00:24:06,779
networking and architecture and he works

341
00:24:06,779 --> 00:24:08,820
on the hardware software stack for data

342
00:24:08,820 --> 00:24:10,020
plane based machine learning

343
00:24:10,020 --> 00:24:12,299
infrastructure and applications

344
00:24:12,299 --> 00:24:14,940
uh Cheshire is due to graduate this year

345
00:24:14,940 --> 00:24:17,100
I understand he's on the job market so

346
00:24:17,100 --> 00:24:19,320
uh if if you like this work then please

347
00:24:19,320 --> 00:24:22,620
do uh talk to him uh he'll be around at

348
00:24:22,620 --> 00:24:24,600
the ATF all week and if you find this

349
00:24:24,600 --> 00:24:26,760
talk interesting uh I believe he's also

350
00:24:26,760 --> 00:24:28,740
going to be presenting in the koinagi

351
00:24:28,740 --> 00:24:31,880
session later this week

352
00:24:32,159 --> 00:24:35,100
um Tasha over to you

353
00:24:35,100 --> 00:24:39,539
awesome thanks Colin uh cool so

354
00:24:39,539 --> 00:24:41,220
um I'm going to be talking about Taurus

355
00:24:41,220 --> 00:24:43,620
which is a project that uh me and my

356
00:24:43,620 --> 00:24:45,299
colleagues have been working on and so

357
00:24:45,299 --> 00:24:47,220
Taurus is essentially a data plane

358
00:24:47,220 --> 00:24:49,620
architecture for per packet machine

359
00:24:49,620 --> 00:24:52,260
learning and

360
00:24:52,260 --> 00:24:54,620
foreign

361
00:24:54,620 --> 00:24:58,500
that means all right so this here is a

362
00:24:58,500 --> 00:25:02,039
quote from a 2015 Google Blog and at

363
00:25:02,039 --> 00:25:04,740
that time uh Google is already dealing

364
00:25:04,740 --> 00:25:07,740
with uh over one petabit per second of

365
00:25:07,740 --> 00:25:10,080
total bisection bandwidth

366
00:25:10,080 --> 00:25:12,720
um and it's only grown larger and harder

367
00:25:12,720 --> 00:25:14,940
to scale since so what we're essentially

368
00:25:14,940 --> 00:25:17,460
dealing with here is a situation where

369
00:25:17,460 --> 00:25:19,620
networks require more and more complex

370
00:25:19,620 --> 00:25:21,539
management with higher and higher

371
00:25:21,539 --> 00:25:23,220
performance

372
00:25:23,220 --> 00:25:26,760
um and so it's uh the time is ripe for

373
00:25:26,760 --> 00:25:30,440
finding new Solutions here

374
00:25:30,679 --> 00:25:34,440
and uh one of the promising Solutions in

375
00:25:34,440 --> 00:25:38,039
this area is machine learning so machine

376
00:25:38,039 --> 00:25:41,520
learning can allow us to

377
00:25:41,520 --> 00:25:43,320
um essentially take in data from the

378
00:25:43,320 --> 00:25:45,000
network and make progressively better

379
00:25:45,000 --> 00:25:46,860
and better decisions as we train our

380
00:25:46,860 --> 00:25:50,039
models and these machine learning

381
00:25:50,039 --> 00:25:51,840
algorithms can approximate Network

382
00:25:51,840 --> 00:25:55,080
functions based on the data they see and

383
00:25:55,080 --> 00:25:57,480
they're also going to customize their

384
00:25:57,480 --> 00:25:59,580
operation to the data that they're

385
00:25:59,580 --> 00:26:02,279
training on which in turn means that

386
00:26:02,279 --> 00:26:04,380
these machine learning algorithms are

387
00:26:04,380 --> 00:26:06,960
actually customizing their models to the

388
00:26:06,960 --> 00:26:08,340
network itself

389
00:26:08,340 --> 00:26:12,600
and so we're sort of uh doing elements

390
00:26:12,600 --> 00:26:14,279
of this already with handwritten

391
00:26:14,279 --> 00:26:16,440
heuristics in the network so something

392
00:26:16,440 --> 00:26:17,640
like an active queue management

393
00:26:17,640 --> 00:26:20,460
algorithm or uh hashing and load

394
00:26:20,460 --> 00:26:23,039
balancing and playing with operator tune

395
00:26:23,039 --> 00:26:27,179
parameters so all machine learning uh is

396
00:26:27,179 --> 00:26:29,220
doing here is taking the next step by

397
00:26:29,220 --> 00:26:31,380
automating the

398
00:26:31,380 --> 00:26:33,779
um the search for these kind of

399
00:26:33,779 --> 00:26:35,880
parameters that allow to work well

400
00:26:35,880 --> 00:26:38,600
within your network

401
00:26:40,799 --> 00:26:43,860
so uh if we're okay with using machine

402
00:26:43,860 --> 00:26:46,320
learning we now need to examine where

403
00:26:46,320 --> 00:26:48,120
exactly in the network it has to happen

404
00:26:48,120 --> 00:26:50,580
so I'm sure many of you already familiar

405
00:26:50,580 --> 00:26:52,500
with software-defined networks

406
00:26:52,500 --> 00:26:54,480
essentially the control plane and the

407
00:26:54,480 --> 00:26:57,240
data plane are split and the control

408
00:26:57,240 --> 00:27:01,020
plane is responsible for policy creation

409
00:27:01,020 --> 00:27:02,700
um essentially in the form of flow rules

410
00:27:02,700 --> 00:27:04,440
which are installed into a data plane

411
00:27:04,440 --> 00:27:06,059
where that's where you're going to find

412
00:27:06,059 --> 00:27:08,460
your switches and they're doing packet

413
00:27:08,460 --> 00:27:10,860
forwarding via match action

414
00:27:10,860 --> 00:27:12,480
so

415
00:27:12,480 --> 00:27:14,640
um right off the bat there are two good

416
00:27:14,640 --> 00:27:16,740
candidates for where we should operate

417
00:27:16,740 --> 00:27:19,760
with machine learning

418
00:27:20,580 --> 00:27:24,179
and uh on the left here I have a diagram

419
00:27:24,179 --> 00:27:26,940
of the same typical defined software

420
00:27:26,940 --> 00:27:28,020
defined Network

421
00:27:28,020 --> 00:27:30,899
but on the right uh I have a software

422
00:27:30,899 --> 00:27:32,279
defined network with the Taurus

423
00:27:32,279 --> 00:27:36,120
worldview and so what we've actually

424
00:27:36,120 --> 00:27:38,100
done here is we've split the machine

425
00:27:38,100 --> 00:27:40,919
learning operation into training which

426
00:27:40,919 --> 00:27:42,360
is going to happen in the control plane

427
00:27:42,360 --> 00:27:44,340
and then inference which is going to

428
00:27:44,340 --> 00:27:46,679
happen in the data plane so in the

429
00:27:46,679 --> 00:27:49,080
control plane policy creation is going

430
00:27:49,080 --> 00:27:51,960
to take the form of flow rules plus ml

431
00:27:51,960 --> 00:27:54,360
training and when installing this

432
00:27:54,360 --> 00:27:56,279
information into the data plane it's

433
00:27:56,279 --> 00:27:58,679
going to be sending flow rules as usual

434
00:27:58,679 --> 00:28:02,700
but also the ml model weights and in the

435
00:28:02,700 --> 00:28:04,440
data plane we're going to be doing our

436
00:28:04,440 --> 00:28:06,659
typical match action packet forwarding

437
00:28:06,659 --> 00:28:08,159
but we're also going to be doing

438
00:28:08,159 --> 00:28:12,559
decision making with ML inference

439
00:28:14,760 --> 00:28:17,399
and so that brings me to one of the core

440
00:28:17,399 --> 00:28:20,039
tenets of Taurus and that's essentially

441
00:28:20,039 --> 00:28:22,140
that ml inference should happen per

442
00:28:22,140 --> 00:28:24,840
packet in the data plane

443
00:28:24,840 --> 00:28:27,779
um and so the the intuition here is

444
00:28:27,779 --> 00:28:29,880
relatively straightforward you want to

445
00:28:29,880 --> 00:28:31,919
be able to do per packet operation

446
00:28:31,919 --> 00:28:34,380
because that is the finest granularity

447
00:28:34,380 --> 00:28:36,720
of traffic essentially operating on a

448
00:28:36,720 --> 00:28:39,179
packet scale now not every application

449
00:28:39,179 --> 00:28:41,899
may need per packet

450
00:28:41,899 --> 00:28:45,899
level operation but the there are

451
00:28:45,899 --> 00:28:48,120
applications that need it and so the

452
00:28:48,120 --> 00:28:50,159
platform should be able to support per

453
00:28:50,159 --> 00:28:52,559
packet operation and then the data plane

454
00:28:52,559 --> 00:28:54,360
that's where the packets are so if we're

455
00:28:54,360 --> 00:28:56,159
going to be doing decisions on packets

456
00:28:56,159 --> 00:28:59,779
it should happen in the data plane

457
00:29:01,020 --> 00:29:04,500
oh I think uh PowerPoint animations

458
00:29:04,500 --> 00:29:06,899
don't play well with the PDF

459
00:29:06,899 --> 00:29:11,039
um so that's okay uh what what's

460
00:29:11,039 --> 00:29:13,679
basically happening in here is that

461
00:29:13,679 --> 00:29:16,380
um if we were to do just a rough off the

462
00:29:16,380 --> 00:29:19,200
uh off the cuff math here so you have

463
00:29:19,200 --> 00:29:21,120
traffic at one gigapacket per second

464
00:29:21,120 --> 00:29:24,179
moving through your data plane now in

465
00:29:24,179 --> 00:29:26,399
the time it takes you to send a packet

466
00:29:26,399 --> 00:29:28,020
digest from the data plane up to the

467
00:29:28,020 --> 00:29:31,320
control plane calculate flow rules and

468
00:29:31,320 --> 00:29:33,000
then install it back into the data plane

469
00:29:33,000 --> 00:29:35,399
in this case we've assumed

470
00:29:35,399 --> 00:29:38,340
um a half millisecond for each step so

471
00:29:38,340 --> 00:29:42,539
we've now missed 1.5 million packets in

472
00:29:42,539 --> 00:29:46,140
our traffic stream by the time uh we had

473
00:29:46,140 --> 00:29:48,120
flow rules installed into the data plane

474
00:29:48,120 --> 00:29:51,120
so in the example here we're doing

475
00:29:51,120 --> 00:29:52,799
anomaly detection so we're trying to

476
00:29:52,799 --> 00:29:54,539
find out if incoming packets are

477
00:29:54,539 --> 00:29:57,720
malicious or benign and maybe if we find

478
00:29:57,720 --> 00:29:59,279
that it's malicious we're going to

479
00:29:59,279 --> 00:30:02,700
install some rule to say block that IP

480
00:30:02,700 --> 00:30:05,460
um and by the so if we've missed 1.5

481
00:30:05,460 --> 00:30:07,380
million packets during this flow rule

482
00:30:07,380 --> 00:30:08,940
installation time by the time we block

483
00:30:08,940 --> 00:30:11,539
that IP we've already let a ton of

484
00:30:11,539 --> 00:30:14,340
potentially malicious traffic into the

485
00:30:14,340 --> 00:30:16,020
network so

486
00:30:16,020 --> 00:30:18,059
the whole takeaway here is really just

487
00:30:18,059 --> 00:30:23,760
to show you why we can't let our

488
00:30:23,760 --> 00:30:26,340
operation for these kind of this level

489
00:30:26,340 --> 00:30:28,860
of application happen in the control

490
00:30:28,860 --> 00:30:30,960
plane and if we're committing to using

491
00:30:30,960 --> 00:30:33,299
machine learning we can't have inference

492
00:30:33,299 --> 00:30:36,740
happen in the control plane

493
00:30:38,340 --> 00:30:39,840
so

494
00:30:39,840 --> 00:30:42,000
fundamentally the conclusion here is

495
00:30:42,000 --> 00:30:43,500
that the robustness and performance of

496
00:30:43,500 --> 00:30:45,419
your network are going to be determined

497
00:30:45,419 --> 00:30:47,399
by the quality of your reaction and the

498
00:30:47,399 --> 00:30:48,779
speed of your reaction

499
00:30:48,779 --> 00:30:52,559
so in the machine learning worldview the

500
00:30:52,559 --> 00:30:54,000
quality of the reaction is going to be

501
00:30:54,000 --> 00:30:57,059
determined by your training data so how

502
00:30:57,059 --> 00:30:59,100
much do you have what kind of cases does

503
00:30:59,100 --> 00:31:01,500
it cover how well is it cleaned but also

504
00:31:01,500 --> 00:31:03,240
your speed of reaction so in the case of

505
00:31:03,240 --> 00:31:05,580
the anomaly detection

506
00:31:05,580 --> 00:31:07,620
um you want to act on a malicious packet

507
00:31:07,620 --> 00:31:09,000
the moment you see it you don't want to

508
00:31:09,000 --> 00:31:10,440
have to go to the control plane and come

509
00:31:10,440 --> 00:31:12,620
back and install any sort of flow rules

510
00:31:12,620 --> 00:31:16,019
and this is essentially the per packet

511
00:31:16,019 --> 00:31:19,279
operation in the data plane

512
00:31:21,960 --> 00:31:26,220
so zooming in on uh the control plane

513
00:31:26,220 --> 00:31:27,899
let's talk a little bit about the actual

514
00:31:27,899 --> 00:31:31,140
implementation of how you do this so I

515
00:31:31,140 --> 00:31:32,460
mentioned before that we're going to

516
00:31:32,460 --> 00:31:35,039
split our machine learning uh into

517
00:31:35,039 --> 00:31:36,600
training in the control plane and

518
00:31:36,600 --> 00:31:38,940
inference to the data plane and so the

519
00:31:38,940 --> 00:31:40,919
key here is that training is off the

520
00:31:40,919 --> 00:31:42,600
critical path if packet forward is

521
00:31:42,600 --> 00:31:44,220
Packet forwarding is happening in the

522
00:31:44,220 --> 00:31:46,500
data plane then

523
00:31:46,500 --> 00:31:49,200
um the control plane is not uh

524
00:31:49,200 --> 00:31:51,480
responsible for making uh per packet

525
00:31:51,480 --> 00:31:53,460
level decisions which means that we can

526
00:31:53,460 --> 00:31:55,799
do our machine learning training there

527
00:31:55,799 --> 00:31:58,559
at leisure and

528
00:31:58,559 --> 00:32:00,779
um essentially we can we can put in

529
00:32:00,779 --> 00:32:02,700
whatever the latest and greatest ml

530
00:32:02,700 --> 00:32:04,740
accelerators are whatever your favorite

531
00:32:04,740 --> 00:32:07,500
ml framework is installed in a control

532
00:32:07,500 --> 00:32:10,559
plane server and have it training models

533
00:32:10,559 --> 00:32:12,000
offline

534
00:32:12,000 --> 00:32:15,559
the trickier part comes in The Next Step

535
00:32:15,559 --> 00:32:19,080
where now uh we need to deal with the

536
00:32:19,080 --> 00:32:21,899
actual critical path basically

537
00:32:21,899 --> 00:32:24,360
um tackling packets as they come so

538
00:32:24,360 --> 00:32:26,880
machine learning interference here is

539
00:32:26,880 --> 00:32:28,679
going to happen in the data plane like I

540
00:32:28,679 --> 00:32:31,860
mentioned and the the final outstanding

541
00:32:31,860 --> 00:32:34,500
question here then is if we're okay with

542
00:32:34,500 --> 00:32:37,799
doing uh training in the control plane

543
00:32:37,799 --> 00:32:40,320
we can use whatever existing Hardware we

544
00:32:40,320 --> 00:32:43,380
want and then what do we do about the

545
00:32:43,380 --> 00:32:46,740
data plane do we have say a switch that

546
00:32:46,740 --> 00:32:50,220
can do inference at line rate per packet

547
00:32:50,220 --> 00:32:51,539
operation

548
00:32:51,539 --> 00:32:53,279
and

549
00:32:53,279 --> 00:32:55,919
so this is really the the Crux of Taurus

550
00:32:55,919 --> 00:32:57,659
and that's what it aims to do so

551
00:32:57,659 --> 00:32:59,580
tortoise is an architecture for per

552
00:32:59,580 --> 00:33:01,860
packet machine learning inference in the

553
00:33:01,860 --> 00:33:04,340
data plane

554
00:33:06,419 --> 00:33:08,760
so uh let's jump into the actual

555
00:33:08,760 --> 00:33:10,080
Hardware

556
00:33:10,080 --> 00:33:12,059
um and how we enable this kind of

557
00:33:12,059 --> 00:33:14,640
machine learning inference at line rate

558
00:33:14,640 --> 00:33:17,820
so I have a picture here of a piece of

559
00:33:17,820 --> 00:33:19,799
pipeline a protocol independent switch

560
00:33:19,799 --> 00:33:24,120
architecture so this is the typical uh

561
00:33:24,120 --> 00:33:27,240
programmable structures you'll find in

562
00:33:27,240 --> 00:33:30,539
these kind of switches so some sort of

563
00:33:30,539 --> 00:33:33,000
programmable packet parser match action

564
00:33:33,000 --> 00:33:34,620
tables that allow you to encode your

565
00:33:34,620 --> 00:33:36,779
network functions and then uh maybe a

566
00:33:36,779 --> 00:33:39,059
programmable traffic manager

567
00:33:39,059 --> 00:33:40,799
so

568
00:33:40,799 --> 00:33:43,140
we're going to actually keep most of

569
00:33:43,140 --> 00:33:46,380
these elements and just make a

570
00:33:46,380 --> 00:33:48,179
modification of additional Hardware

571
00:33:48,179 --> 00:33:50,159
that'll allow us to do our machine

572
00:33:50,159 --> 00:33:53,059
learning inference

573
00:33:53,159 --> 00:33:56,100
um but the natural question is if we're

574
00:33:56,100 --> 00:33:58,260
committing to adding Hardware into the

575
00:33:58,260 --> 00:34:00,120
switch pipeline

576
00:34:00,120 --> 00:34:02,220
um what does that look like and more

577
00:34:02,220 --> 00:34:03,779
specifically what is the abstraction

578
00:34:03,779 --> 00:34:06,360
here with which we're going to create

579
00:34:06,360 --> 00:34:09,139
our programmable machine learning

580
00:34:09,139 --> 00:34:12,960
Fabric and so in Taurus we use the

581
00:34:12,960 --> 00:34:16,500
mapreduce abstraction so mapreduce is

582
00:34:16,500 --> 00:34:17,879
really useful for machine learning

583
00:34:17,879 --> 00:34:20,099
because it supports a lot of the common

584
00:34:20,099 --> 00:34:22,139
linear algebra operations that you need

585
00:34:22,139 --> 00:34:24,540
for your ml algorithm so this covers

586
00:34:24,540 --> 00:34:27,000
everything from neural network support

587
00:34:27,000 --> 00:34:29,639
Vector machines k-means all these

588
00:34:29,639 --> 00:34:32,280
different kind of applications and just

589
00:34:32,280 --> 00:34:35,219
as an example I have here in the picture

590
00:34:35,219 --> 00:34:38,520
um an example of a single neuron from a

591
00:34:38,520 --> 00:34:41,159
deep neural network so you can see

592
00:34:41,159 --> 00:34:43,679
exactly how map and reduce her applied

593
00:34:43,679 --> 00:34:46,679
here in this case in the blue box

594
00:34:46,679 --> 00:34:48,780
um we are doing an element-wise

595
00:34:48,780 --> 00:34:50,520
multiplication that's our mapped with

596
00:34:50,520 --> 00:34:53,579
multiplication with inputs and weights

597
00:34:53,579 --> 00:34:56,639
and then we're applying a reduction and

598
00:34:56,639 --> 00:35:00,060
so this is going to essentially add all

599
00:35:00,060 --> 00:35:02,640
the values together and

600
00:35:02,640 --> 00:35:05,220
um you're going to produce a scalar

601
00:35:05,220 --> 00:35:08,220
value from your vector of inputs and

602
00:35:08,220 --> 00:35:09,300
then finally we're going to apply an

603
00:35:09,300 --> 00:35:11,760
activation function and so that suffices

604
00:35:11,760 --> 00:35:14,520
for a single neuron but you can mix and

605
00:35:14,520 --> 00:35:17,400
match this pattern ad nauseum uh to

606
00:35:17,400 --> 00:35:20,400
create a full neural network so by

607
00:35:20,400 --> 00:35:24,420
stacking extra of these blocks

608
00:35:24,420 --> 00:35:27,240
um in parallel you'll be creating a

609
00:35:27,240 --> 00:35:29,339
layer of neurons and then stacking them

610
00:35:29,339 --> 00:35:31,680
sequentially you'll be creating multiple

611
00:35:31,680 --> 00:35:33,300
layers and so that's how you can create

612
00:35:33,300 --> 00:35:36,500
say a deep neural network

613
00:35:38,460 --> 00:35:40,800
so the other advantage of the mapreduce

614
00:35:40,800 --> 00:35:43,740
pattern is uh comes from the kind of

615
00:35:43,740 --> 00:35:46,680
performance that it enables primarily

616
00:35:46,680 --> 00:35:48,839
it's a from the the simdi parallelism

617
00:35:48,839 --> 00:35:51,720
that same instruction multiple data so

618
00:35:51,720 --> 00:35:53,880
we can get a lot of performance out of

619
00:35:53,880 --> 00:35:57,420
the parallelism with minimal logic and

620
00:35:57,420 --> 00:35:59,280
this is as opposed to what you might

621
00:35:59,280 --> 00:36:02,760
find in a say like a a typical like

622
00:36:02,760 --> 00:36:04,980
Tofino pipeline where they have vliw

623
00:36:04,980 --> 00:36:07,560
pipelines which give you much more

624
00:36:07,560 --> 00:36:10,440
flexibility but the cost here is that

625
00:36:10,440 --> 00:36:12,960
there's a lot of logic that's needed for

626
00:36:12,960 --> 00:36:15,839
the communication hardware and

627
00:36:15,839 --> 00:36:18,300
um that ends up taking up a lot of the

628
00:36:18,300 --> 00:36:22,260
the overall on-chip area and uh in

629
00:36:22,260 --> 00:36:24,599
addition simply parallelism gives us the

630
00:36:24,599 --> 00:36:30,000
ability to unroll the loops in our

631
00:36:30,000 --> 00:36:33,720
uh in our algorithms so the the idea of

632
00:36:33,720 --> 00:36:36,660
unrolling here if we take the example of

633
00:36:36,660 --> 00:36:38,400
um say a single layer of a neural

634
00:36:38,400 --> 00:36:40,980
network and say you have four neurons in

635
00:36:40,980 --> 00:36:43,920
your network you can either

636
00:36:43,920 --> 00:36:46,619
um execute them sequentially you're

637
00:36:46,619 --> 00:36:49,380
doing one neuron after the other or if

638
00:36:49,380 --> 00:36:51,180
you have the resources you can

639
00:36:51,180 --> 00:36:53,280
instantiate all four of them in parallel

640
00:36:53,280 --> 00:36:56,640
and so the trade-off here is that more

641
00:36:56,640 --> 00:36:58,140
on rolling is going to give you better

642
00:36:58,140 --> 00:36:59,760
performance essentially doing all four

643
00:36:59,760 --> 00:37:02,220
of those neurons at once while less

644
00:37:02,220 --> 00:37:04,500
enrolling means you only need the

645
00:37:04,500 --> 00:37:07,560
hardware for one single neuron's worth

646
00:37:07,560 --> 00:37:09,720
of operations but it's going to take you

647
00:37:09,720 --> 00:37:12,180
four times as long so it's less resource

648
00:37:12,180 --> 00:37:15,260
intensive but it's also uh

649
00:37:15,260 --> 00:37:17,940
uh much less

650
00:37:17,940 --> 00:37:21,839
um a much higher latency and uh but we

651
00:37:21,839 --> 00:37:23,579
get this kind of control with the CMD

652
00:37:23,579 --> 00:37:25,260
pattern by

653
00:37:25,260 --> 00:37:27,240
um essentially un adjusting unrolling

654
00:37:27,240 --> 00:37:29,660
factors

655
00:37:30,800 --> 00:37:32,460
so

656
00:37:32,460 --> 00:37:35,520
we went ahead and we uh essentially

657
00:37:35,520 --> 00:37:38,820
adjusted the switch Pipeline with a

658
00:37:38,820 --> 00:37:42,119
mapreduce unit that uh implements the

659
00:37:42,119 --> 00:37:45,359
patterns that I just described so the

660
00:37:45,359 --> 00:37:47,599
the a we still have our typical

661
00:37:47,599 --> 00:37:50,099
programmable elements we have a

662
00:37:50,099 --> 00:37:52,200
programmable packet parser match action

663
00:37:52,200 --> 00:37:54,300
tables and traffic manager but you can

664
00:37:54,300 --> 00:37:56,760
see in the center we have this mapreduce

665
00:37:56,760 --> 00:37:58,859
unit and that's essentially what's going

666
00:37:58,859 --> 00:38:01,980
to do our machine learning inference and

667
00:38:01,980 --> 00:38:03,839
so there are a couple uh little

668
00:38:03,839 --> 00:38:06,420
idiosyncrasies about the um the

669
00:38:06,420 --> 00:38:07,740
arrangement of the pipeline that I want

670
00:38:07,740 --> 00:38:10,380
to point out and uh and that's how we

671
00:38:10,380 --> 00:38:12,660
use these different elements for machine

672
00:38:12,660 --> 00:38:13,980
learning context even if they're

673
00:38:13,980 --> 00:38:16,980
typically Network elements so a packet

674
00:38:16,980 --> 00:38:18,839
parser is normally for pulling out your

675
00:38:18,839 --> 00:38:21,480
headers from your packets

676
00:38:21,480 --> 00:38:23,220
um and doing whatever you want with your

677
00:38:23,220 --> 00:38:25,320
match action rules in this case packet

678
00:38:25,320 --> 00:38:27,420
parsing is also pulling out the features

679
00:38:27,420 --> 00:38:30,420
for our machine learning inference and

680
00:38:30,420 --> 00:38:32,339
then we have match action tables before

681
00:38:32,339 --> 00:38:34,980
and after the mapreduce unit and so

682
00:38:34,980 --> 00:38:37,079
these are doing different types of

683
00:38:37,079 --> 00:38:40,260
rule-based pre and post-processing on

684
00:38:40,260 --> 00:38:41,940
our machine learning inputs and outputs

685
00:38:41,940 --> 00:38:45,359
so uh the match action tables before

686
00:38:45,359 --> 00:38:47,640
mapreduce can be doing some sort of

687
00:38:47,640 --> 00:38:50,040
cleaning on the features and then the

688
00:38:50,040 --> 00:38:53,760
match action tables on the output on the

689
00:38:53,760 --> 00:38:55,859
right side of the mapreduce unit can be

690
00:38:55,859 --> 00:38:58,380
doing some sort of interpretation of the

691
00:38:58,380 --> 00:38:59,460
results

692
00:38:59,460 --> 00:39:02,640
and uh so when we actually went to

693
00:39:02,640 --> 00:39:05,040
design this mapreduce unit

694
00:39:05,040 --> 00:39:07,140
um there's a couple of things that came

695
00:39:07,140 --> 00:39:09,359
up it turns out you can't really just

696
00:39:09,359 --> 00:39:12,119
stick an accelerator into the switch

697
00:39:12,119 --> 00:39:14,240
pipeline so

698
00:39:14,240 --> 00:39:17,460
uh what we did was we kind of

699
00:39:17,460 --> 00:39:19,200
established what were the the points

700
00:39:19,200 --> 00:39:21,359
that we wanted our mapreduce block to

701
00:39:21,359 --> 00:39:24,300
fit and so most of all we wanted it to

702
00:39:24,300 --> 00:39:26,579
be reconfigurable so essentially you

703
00:39:26,579 --> 00:39:28,320
should be able to program it it can't be

704
00:39:28,320 --> 00:39:30,960
a an Asic for a single type of machine

705
00:39:30,960 --> 00:39:32,579
learning application you should be able

706
00:39:32,579 --> 00:39:35,040
to put in whatever or program whatever

707
00:39:35,040 --> 00:39:38,960
application you want oops

708
00:39:39,420 --> 00:39:42,300
and um it has some neat line rate with

709
00:39:42,300 --> 00:39:46,320
the fixed clock so this essentially uh

710
00:39:46,320 --> 00:39:48,480
rules out in fpga because in fpga will

711
00:39:48,480 --> 00:39:50,820
give you a variable clock we want it to

712
00:39:50,820 --> 00:39:52,740
be deterministic

713
00:39:52,740 --> 00:39:55,380
um and of course line rate is our

714
00:39:55,380 --> 00:39:58,440
performance requirement and then minimal

715
00:39:58,440 --> 00:40:00,780
area and power overhead we don't want to

716
00:40:00,780 --> 00:40:03,599
blow up the entire chip area adding in

717
00:40:03,599 --> 00:40:06,720
like a mapreduce block it should be

718
00:40:06,720 --> 00:40:10,079
something that is a small but gives you

719
00:40:10,079 --> 00:40:13,920
access to a whole class of applications

720
00:40:13,920 --> 00:40:17,099
and so finally the one little thing to

721
00:40:17,099 --> 00:40:19,079
note here that's kind of interesting is

722
00:40:19,079 --> 00:40:21,780
that most of these ml accelerators are

723
00:40:21,780 --> 00:40:24,839
built to do uh batch processing in an

724
00:40:24,839 --> 00:40:27,960
effort to get high throughput but in the

725
00:40:27,960 --> 00:40:29,599
network pipeline you're actually

726
00:40:29,599 --> 00:40:31,800
processing packets as they're coming

727
00:40:31,800 --> 00:40:34,319
which means that you're operating on a

728
00:40:34,319 --> 00:40:36,660
batch size of one

729
00:40:36,660 --> 00:40:39,960
um which is uh turns out puts a lot of

730
00:40:39,960 --> 00:40:41,640
different performance demands on the

731
00:40:41,640 --> 00:40:43,740
hardware than a typical accelerator

732
00:40:43,740 --> 00:40:46,459
would see

733
00:40:47,040 --> 00:40:49,500
uh yeah so

734
00:40:49,500 --> 00:40:51,540
um I have a quick example here just to

735
00:40:51,540 --> 00:40:52,800
make this a little more concrete going

736
00:40:52,800 --> 00:40:56,280
back to anomaly detection

737
00:40:56,280 --> 00:40:59,940
um so you can you can uh imagine say a

738
00:40:59,940 --> 00:41:01,500
packet coming into the switch Pipeline

739
00:41:01,500 --> 00:41:03,480
and we want to see essentially whether

740
00:41:03,480 --> 00:41:07,020
it's malicious or benign so the packet

741
00:41:07,020 --> 00:41:10,020
hits the first stage and that's where

742
00:41:10,020 --> 00:41:11,579
we're going to

743
00:41:11,579 --> 00:41:13,619
um do our packet parsing so we're going

744
00:41:13,619 --> 00:41:16,440
to read local features say our IP

745
00:41:16,440 --> 00:41:18,960
whatever information we can extract from

746
00:41:18,960 --> 00:41:20,280
the packet itself

747
00:41:20,280 --> 00:41:22,260
the packet is going to move to the

748
00:41:22,260 --> 00:41:23,940
second stage which are the match action

749
00:41:23,940 --> 00:41:27,060
tables and from there maybe we're going

750
00:41:27,060 --> 00:41:29,579
to do some sort of uh retrieval of out

751
00:41:29,579 --> 00:41:31,020
of network events so these would be

752
00:41:31,020 --> 00:41:33,359
different kinds of uh elements of

753
00:41:33,359 --> 00:41:35,099
metadata that the control plane may have

754
00:41:35,099 --> 00:41:37,440
installed into the match action tables

755
00:41:37,440 --> 00:41:39,540
so something like the failed logins per

756
00:41:39,540 --> 00:41:40,740
IP

757
00:41:40,740 --> 00:41:42,780
the packet then moves to the the center

758
00:41:42,780 --> 00:41:45,060
block of the mapreduce unit that's where

759
00:41:45,060 --> 00:41:46,680
we're going to apply our learned anomaly

760
00:41:46,680 --> 00:41:48,420
detection so you can imagine this is

761
00:41:48,420 --> 00:41:50,160
maybe a binary neural network and it

762
00:41:50,160 --> 00:41:52,980
gives it a a score from zero to one on

763
00:41:52,980 --> 00:41:55,800
how anomalous it is so one is definitely

764
00:41:55,800 --> 00:41:58,040
anomalous zero is benign

765
00:41:58,040 --> 00:42:01,319
and finally the match action or the the

766
00:42:01,319 --> 00:42:03,420
packet will move to the post-processing

767
00:42:03,420 --> 00:42:05,819
match action tables and that's where we

768
00:42:05,819 --> 00:42:07,920
do our interpretation so say we got a

769
00:42:07,920 --> 00:42:10,040
score of 0.8 so it's pretty anomalous

770
00:42:10,040 --> 00:42:13,200
and uh now we want to drop it or

771
00:42:13,200 --> 00:42:15,240
quarantine it this is where the match

772
00:42:15,240 --> 00:42:17,040
action table will set a rule for that

773
00:42:17,040 --> 00:42:19,380
such that when the packet now moves to

774
00:42:19,380 --> 00:42:21,420
the traffic manager it's going to go to

775
00:42:21,420 --> 00:42:24,560
the appropriate destination

776
00:42:25,980 --> 00:42:31,680
thank you so uh in the paper we actually

777
00:42:31,680 --> 00:42:33,300
did a full

778
00:42:33,300 --> 00:42:37,020
um Asic analysis of uh this Taurus

779
00:42:37,020 --> 00:42:40,619
hardware and how we can um we wanted to

780
00:42:40,619 --> 00:42:42,180
show essentially that it has minimal

781
00:42:42,180 --> 00:42:44,160
overhead and it's feasible to to build

782
00:42:44,160 --> 00:42:46,440
something like this and so we based our

783
00:42:46,440 --> 00:42:49,140
evaluation platform on a coarse grain

784
00:42:49,140 --> 00:42:50,700
reconfigurable architecture called

785
00:42:50,700 --> 00:42:53,880
plasticine and we programmed our

786
00:42:53,880 --> 00:42:55,740
applications in the spatial Hardware

787
00:42:55,740 --> 00:42:58,260
description language and so spatials is

788
00:42:58,260 --> 00:43:01,260
just an HDL that lets you use these kind

789
00:43:01,260 --> 00:43:03,900
of uh parallel patterns like map and

790
00:43:03,900 --> 00:43:07,740
reduce to program your

791
00:43:07,740 --> 00:43:09,540
um your your reconfigurable

792
00:43:09,540 --> 00:43:13,560
architectures at the Loop level and so

793
00:43:13,560 --> 00:43:15,960
the the basic architecture the mapreduce

794
00:43:15,960 --> 00:43:18,180
unit here is really just a grid of

795
00:43:18,180 --> 00:43:21,420
compute and memory tiles so easily

796
00:43:21,420 --> 00:43:24,200
scalable and very very straightforward

797
00:43:24,200 --> 00:43:28,319
in the compute units we have Cindy lanes

798
00:43:28,319 --> 00:43:30,839
that are operating in parallel and a

799
00:43:30,839 --> 00:43:32,640
reduction Network that allows us to

800
00:43:32,640 --> 00:43:35,160
implement the reduce operation and the

801
00:43:35,160 --> 00:43:38,040
memory units are just blocks of banked

802
00:43:38,040 --> 00:43:42,240
SRAM so uh we're doing severe pipelining

803
00:43:42,240 --> 00:43:44,400
within the compute unit but then we're

804
00:43:44,400 --> 00:43:46,380
also doing pipelining one level higher

805
00:43:46,380 --> 00:43:48,480
between the compute and memory units so

806
00:43:48,480 --> 00:43:50,940
the AED here is simdi parallelism

807
00:43:50,940 --> 00:43:53,520
everywhere uh and then pipeline

808
00:43:53,520 --> 00:43:55,200
parallelism everywhere and that's how

809
00:43:55,200 --> 00:43:58,399
you get your performance really

810
00:44:00,660 --> 00:44:04,560
so uh we went through a set of real

811
00:44:04,560 --> 00:44:07,560
world applications and

812
00:44:07,560 --> 00:44:11,339
um program them onto our Asic and we

813
00:44:11,339 --> 00:44:13,140
ended up using a 12 by 10 grid to

814
00:44:13,140 --> 00:44:15,599
support all of them and

815
00:44:15,599 --> 00:44:17,760
um we compared it to state-of-the-art

816
00:44:17,760 --> 00:44:22,680
switches with four uh pipelines and

817
00:44:22,680 --> 00:44:25,500
um our reference which was 500 square

818
00:44:25,500 --> 00:44:28,680
millimeters and we found that our grid

819
00:44:28,680 --> 00:44:29,940
which could support these different

820
00:44:29,940 --> 00:44:32,819
applications was only adding a 3.8

821
00:44:32,819 --> 00:44:35,099
percent overhead or 4.8 millimeters per

822
00:44:35,099 --> 00:44:37,140
pipeline so

823
00:44:37,140 --> 00:44:40,260
um again earlier I said we want minimal

824
00:44:40,260 --> 00:44:44,880
area overhead so 3.8 is pretty low given

825
00:44:44,880 --> 00:44:46,440
that you're now getting an entire class

826
00:44:46,440 --> 00:44:50,180
of machine learning applications

827
00:44:52,200 --> 00:44:54,839
um and jumping into one of these uh

828
00:44:54,839 --> 00:44:56,940
applications I've been using anomaly

829
00:44:56,940 --> 00:45:00,420
detection as a recurring example here

830
00:45:00,420 --> 00:45:01,980
um we tried out two different types of

831
00:45:01,980 --> 00:45:04,140
anomaly detection with support Vector

832
00:45:04,140 --> 00:45:07,380
machines and deep neural network and so

833
00:45:07,380 --> 00:45:10,560
uh for both models you can see in the

834
00:45:10,560 --> 00:45:12,599
throughput it's one giga packet per

835
00:45:12,599 --> 00:45:15,780
second which is the line rate for

836
00:45:15,780 --> 00:45:18,480
um high-end uh switch pipelines like

837
00:45:18,480 --> 00:45:21,420
your tofinos and broadcoms uh the

838
00:45:21,420 --> 00:45:23,599
latency that we added was in the

839
00:45:23,599 --> 00:45:26,339
hundreds of nanoseconds or less

840
00:45:26,339 --> 00:45:28,859
so in this case you would choose your

841
00:45:28,859 --> 00:45:30,540
application you can see here that the

842
00:45:30,540 --> 00:45:33,000
bsvm requires 83 nanoseconds while the

843
00:45:33,000 --> 00:45:35,819
DNN requires 221 nanoseconds so

844
00:45:35,819 --> 00:45:37,920
depending on your slos and what kind of

845
00:45:37,920 --> 00:45:39,780
requirements you have to meet you can

846
00:45:39,780 --> 00:45:43,680
choose your algorithm to reduce latency

847
00:45:43,680 --> 00:45:46,500
um and then in both cases the area and

848
00:45:46,500 --> 00:45:48,960
power overhead required for the hardware

849
00:45:48,960 --> 00:45:52,920
to implement just these applications is

850
00:45:52,920 --> 00:45:57,440
um single digits or uh or less a 0.6

851
00:45:57,440 --> 00:45:59,760
power overhead point five percent area

852
00:45:59,760 --> 00:46:02,880
overhead or 0.8 and point uh and 1.0

853
00:46:02,880 --> 00:46:04,319
respectively

854
00:46:04,319 --> 00:46:07,560
uh again if you don't need

855
00:46:07,560 --> 00:46:10,859
um say the full Suite of benchmarks you

856
00:46:10,859 --> 00:46:13,740
only want a reconfigurable fabric that

857
00:46:13,740 --> 00:46:15,660
will let you do anomaly detection you

858
00:46:15,660 --> 00:46:17,339
can do it with

859
00:46:17,339 --> 00:46:20,940
um minimal overhead here and so in the

860
00:46:20,940 --> 00:46:22,920
paper there's a several more

861
00:46:22,920 --> 00:46:24,599
applications if people are are

862
00:46:24,599 --> 00:46:27,480
interested such as a congestion control

863
00:46:27,480 --> 00:46:29,579
Network and a traffic classification

864
00:46:29,579 --> 00:46:32,000
Network

865
00:46:35,880 --> 00:46:39,660
so uh we went through this whole process

866
00:46:39,660 --> 00:46:42,839
of doing an Asic analysis to prove that

867
00:46:42,839 --> 00:46:44,579
it could be done

868
00:46:44,579 --> 00:46:47,520
um but as far as research goes we don't

869
00:46:47,520 --> 00:46:49,619
really want anyone waiting for some sort

870
00:46:49,619 --> 00:46:52,440
of mass-produced Taurus Asics so we've

871
00:46:52,440 --> 00:46:55,200
put out an open source fpga based test

872
00:46:55,200 --> 00:46:56,099
bed

873
00:46:56,099 --> 00:46:57,900
um and so this is just a rough diagram

874
00:46:57,900 --> 00:47:00,720
of what it looks like at the control

875
00:47:00,720 --> 00:47:03,180
plane we're using your typical Network

876
00:47:03,180 --> 00:47:05,359
OS like onos

877
00:47:05,359 --> 00:47:08,940
we're using a Tofino switch to to mimic

878
00:47:08,940 --> 00:47:11,220
the piece of pipeline elements like your

879
00:47:11,220 --> 00:47:13,319
program mobile packet parsers match

880
00:47:13,319 --> 00:47:15,599
action tables and traffic managers and

881
00:47:15,599 --> 00:47:18,960
then we're using an fpga to

882
00:47:18,960 --> 00:47:21,720
um uh to mimic the mapreduce unit so we

883
00:47:21,720 --> 00:47:23,220
set it up in this bump in the wire

884
00:47:23,220 --> 00:47:25,260
configuration

885
00:47:25,260 --> 00:47:28,559
um and so uh because of the limits of an

886
00:47:28,559 --> 00:47:30,180
fpga you're not going to be able to hit

887
00:47:30,180 --> 00:47:32,040
the same performance as you're going to

888
00:47:32,040 --> 00:47:34,859
get with the Asic core screen

889
00:47:34,859 --> 00:47:37,920
reconfigurable architecture but it's

890
00:47:37,920 --> 00:47:41,460
there to serve as a proof of concept for

891
00:47:41,460 --> 00:47:44,540
the functionality

892
00:47:47,400 --> 00:47:50,040
so just a quick demonstration of this

893
00:47:50,040 --> 00:47:51,900
test bed

894
00:47:51,900 --> 00:47:53,819
um we did an example

895
00:47:53,819 --> 00:47:55,619
essentially the example I mentioned

896
00:47:55,619 --> 00:47:57,599
earlier about anomaly detection where

897
00:47:57,599 --> 00:48:00,420
we're trying to do uh detection of

898
00:48:00,420 --> 00:48:02,359
anomalous packets in the control plane

899
00:48:02,359 --> 00:48:05,839
or we're trying to use Taurus and do

900
00:48:05,839 --> 00:48:08,940
anomaly detection in the data plane and

901
00:48:08,940 --> 00:48:10,559
so with the test bed that I just showed

902
00:48:10,559 --> 00:48:12,300
you you can do

903
00:48:12,300 --> 00:48:13,559
um

904
00:48:13,559 --> 00:48:15,900
you can do either so you so in the case

905
00:48:15,900 --> 00:48:18,720
of Taurus we'd be uh placing our anomaly

906
00:48:18,720 --> 00:48:22,020
detection application on the fpga while

907
00:48:22,020 --> 00:48:24,059
if we're trying to do control plane

908
00:48:24,059 --> 00:48:26,400
based anomaly detection we would run it

909
00:48:26,400 --> 00:48:31,220
at the uh the controller on the CPU

910
00:48:31,440 --> 00:48:34,800
so uh the takeaway here is is the same

911
00:48:34,800 --> 00:48:36,839
sort of message on

912
00:48:36,839 --> 00:48:40,020
um why you really can't use the control

913
00:48:40,020 --> 00:48:42,359
plane for efficient

914
00:48:42,359 --> 00:48:44,819
um machine learning just based uh

915
00:48:44,819 --> 00:48:46,079
decision making

916
00:48:46,079 --> 00:48:50,280
and um if you take a look at the very uh

917
00:48:50,280 --> 00:48:54,119
last two columns the F1 Square

918
00:48:54,119 --> 00:48:57,720
um now this is the F1 score for the

919
00:48:57,720 --> 00:48:59,760
model when it was implemented on the

920
00:48:59,760 --> 00:49:01,200
Baseline which is control plane or

921
00:49:01,200 --> 00:49:03,720
Taurus which was in the data plane and

922
00:49:03,720 --> 00:49:07,920
in software in tensorflow uh the F1

923
00:49:07,920 --> 00:49:11,400
score is 71.1 so you can see that Taurus

924
00:49:11,400 --> 00:49:14,819
on the far right side of the uh the the

925
00:49:14,819 --> 00:49:16,260
uh the table

926
00:49:16,260 --> 00:49:19,140
is achieving an F1 square of 71.1 so

927
00:49:19,140 --> 00:49:22,319
it's Faithfully recreating the model as

928
00:49:22,319 --> 00:49:25,440
it was in software and

929
00:49:25,440 --> 00:49:27,359
um we're processing packets as they're

930
00:49:27,359 --> 00:49:29,579
coming in whereas in the control plane

931
00:49:29,579 --> 00:49:32,760
we actually had two sample packets from

932
00:49:32,760 --> 00:49:34,980
the network and

933
00:49:34,980 --> 00:49:36,780
um run it through the control plane and

934
00:49:36,780 --> 00:49:39,420
run it through an ml framework and try

935
00:49:39,420 --> 00:49:42,660
to install flow rules and what ends up

936
00:49:42,660 --> 00:49:45,000
happening is that you miss so many

937
00:49:45,000 --> 00:49:48,000
packets while doing this operation that

938
00:49:48,000 --> 00:49:50,760
your effective F1 score drops pretty

939
00:49:50,760 --> 00:49:53,579
heavily so you can see on the far right

940
00:49:53,579 --> 00:49:56,000
column the F1 score for the Baseline

941
00:49:56,000 --> 00:50:00,800
ranges from 1.5 to almost almost zero so

942
00:50:00,800 --> 00:50:03,300
you're effectively throwing away your

943
00:50:03,300 --> 00:50:06,780
model because of the added latency

944
00:50:06,780 --> 00:50:09,480
so that's just uh one example of what

945
00:50:09,480 --> 00:50:11,460
you know what we did with our fpga test

946
00:50:11,460 --> 00:50:12,660
bed

947
00:50:12,660 --> 00:50:14,040
um there's of course lots of other

948
00:50:14,040 --> 00:50:15,960
things you can do but the just to

949
00:50:15,960 --> 00:50:17,760
reinforce the point why you have to

950
00:50:17,760 --> 00:50:21,020
operate in the data plane

951
00:50:22,980 --> 00:50:27,240
cool so yes that's mostly it uh for me

952
00:50:27,240 --> 00:50:29,160
um I have my contact information here

953
00:50:29,160 --> 00:50:33,059
and I have at the bottom the gitlab link

954
00:50:33,059 --> 00:50:36,119
for the fpga test bed we hope people

955
00:50:36,119 --> 00:50:39,119
wanna can try it out and there's the

956
00:50:39,119 --> 00:50:42,480
link to the full paper in this easy to

957
00:50:42,480 --> 00:50:44,400
memorize URL

958
00:50:44,400 --> 00:50:48,000
so uh yeah I'm happy to take any

959
00:50:48,000 --> 00:50:50,240
questions

960
00:50:50,819 --> 00:50:54,960
okay uh thank you very much yeah the

961
00:50:54,960 --> 00:50:56,460
excellent talk

962
00:50:56,460 --> 00:50:57,119
um

963
00:50:57,119 --> 00:50:59,420
since we we have a some people remote

964
00:50:59,420 --> 00:51:02,160
some in the room I think if if we can

965
00:51:02,160 --> 00:51:05,579
manage the queue using miteko uh let me

966
00:51:05,579 --> 00:51:07,020
take a queuing tool I think that would

967
00:51:07,020 --> 00:51:10,020
be helpful uh I do see I guess it's

968
00:51:10,020 --> 00:51:13,640
Barry at the microphone there

969
00:51:13,980 --> 00:51:16,440
okay actually I'm being uh Dave Oran

970
00:51:16,440 --> 00:51:17,880
right now

971
00:51:17,880 --> 00:51:20,220
um Dave Aran asks I assume the class of

972
00:51:20,220 --> 00:51:22,619
anomalies you can detect are those that

973
00:51:22,619 --> 00:51:24,599
can be detected by header Fields within

974
00:51:24,599 --> 00:51:27,540
the width of the ALU of the switch

975
00:51:27,540 --> 00:51:29,520
things in the packet data beyond the

976
00:51:29,520 --> 00:51:32,220
headers won't be seen is that correct

977
00:51:32,220 --> 00:51:34,260
um so the in the case of anomaly

978
00:51:34,260 --> 00:51:38,520
detection we used uh the KDE NSL data

979
00:51:38,520 --> 00:51:41,220
set which had a a record of different

980
00:51:41,220 --> 00:51:44,040
um attacks that were calculated from

981
00:51:44,040 --> 00:51:47,099
like you said either header fields or

982
00:51:47,099 --> 00:51:49,380
um you can also actually calculate

983
00:51:49,380 --> 00:51:51,780
aggregate fields from across headers so

984
00:51:51,780 --> 00:51:53,280
you can

985
00:51:53,280 --> 00:51:56,280
um uh say create like a histogram using

986
00:51:56,280 --> 00:51:57,720
the matte checking tables across

987
00:51:57,720 --> 00:51:59,460
different packets

988
00:51:59,460 --> 00:52:03,240
um and the the packets the uh the packet

989
00:52:03,240 --> 00:52:04,800
headers

990
00:52:04,800 --> 00:52:06,900
um are going to be limited by the packet

991
00:52:06,900 --> 00:52:09,000
header Vector size that's moving between

992
00:52:09,000 --> 00:52:11,880
stages in the switch pipeline but you

993
00:52:11,880 --> 00:52:13,980
don't necessarily need to be limited to

994
00:52:13,980 --> 00:52:16,319
features in the header because the

995
00:52:16,319 --> 00:52:18,480
control plane can install different

996
00:52:18,480 --> 00:52:20,339
types of metadata into the magician

997
00:52:20,339 --> 00:52:22,700
tables and you can do your own

998
00:52:22,700 --> 00:52:25,760
processing in the match action tables

999
00:52:25,760 --> 00:52:29,400
over time or whatever other kind of

1000
00:52:29,400 --> 00:52:30,660
calculations you want to do on your

1001
00:52:30,660 --> 00:52:32,400
headers so the headers are just the

1002
00:52:32,400 --> 00:52:37,160
starting point for the the features here

1003
00:52:39,660 --> 00:52:40,619
hi

1004
00:52:40,619 --> 00:52:43,440
is this working yeah

1005
00:52:43,440 --> 00:52:46,800
George Michaelson can I expect to can I

1006
00:52:46,800 --> 00:52:50,280
sneak two questions in is that okay yeah

1007
00:52:50,280 --> 00:52:53,700
so the first one and this is the naive

1008
00:52:53,700 --> 00:52:56,819
attendee question I suspect the paper is

1009
00:52:56,819 --> 00:52:58,920
very important for interpreting that

1010
00:52:58,920 --> 00:53:02,280
last table it was really quite opaque

1011
00:53:02,280 --> 00:53:05,040
how to understand the meaning of the

1012
00:53:05,040 --> 00:53:07,920
columns and their impact on a comparison

1013
00:53:07,920 --> 00:53:09,720
to the Baseline I think there's a lot of

1014
00:53:09,720 --> 00:53:12,119
implicit knowledge in your table

1015
00:53:12,119 --> 00:53:15,000
structure sure the paper explains it the

1016
00:53:15,000 --> 00:53:16,859
slide where it was just a bit of juice

1017
00:53:16,859 --> 00:53:18,839
to a naive reader

1018
00:53:18,839 --> 00:53:21,180
so at the start of your talk that was

1019
00:53:21,180 --> 00:53:24,240
the first point you made a case to say

1020
00:53:24,240 --> 00:53:26,760
that the delay between doing a packet

1021
00:53:26,760 --> 00:53:29,819
sample constructing table match rules in

1022
00:53:29,819 --> 00:53:31,740
the controller injecting those rules

1023
00:53:31,740 --> 00:53:34,140
down into the functional plane and

1024
00:53:34,140 --> 00:53:37,800
applying them had a huge packet loss and

1025
00:53:37,800 --> 00:53:41,460
mismatch interval but it seems to me the

1026
00:53:41,460 --> 00:53:43,859
delay to perform the ml operation and

1027
00:53:43,859 --> 00:53:46,380
tune your ml have a model that is

1028
00:53:46,380 --> 00:53:48,540
representative of the condition you want

1029
00:53:48,540 --> 00:53:51,540
to model and then install that has a

1030
00:53:51,540 --> 00:53:53,040
similar cost

1031
00:53:53,040 --> 00:53:55,680
it's not to say there's no benefit of ml

1032
00:53:55,680 --> 00:53:57,900
I think it's huge but the component

1033
00:53:57,900 --> 00:54:00,180
that's about the delay cost of doing an

1034
00:54:00,180 --> 00:54:03,000
instantiation of rules I don't think is

1035
00:54:03,000 --> 00:54:05,220
a basis of doing it I think you're on

1036
00:54:05,220 --> 00:54:07,020
stronger ground arguing it's about the

1037
00:54:07,020 --> 00:54:09,839
ability to do complex match at line rate

1038
00:54:09,839 --> 00:54:12,660
than the static cost of doing the rule

1039
00:54:12,660 --> 00:54:15,960
installation right so

1040
00:54:15,960 --> 00:54:17,400
um the installation you're right about

1041
00:54:17,400 --> 00:54:19,559
the installing the model itself so the

1042
00:54:19,559 --> 00:54:21,240
idea is that you could be taking

1043
00:54:21,240 --> 00:54:24,000
sampling uh packets from the your

1044
00:54:24,000 --> 00:54:25,980
network and be sending different kinds

1045
00:54:25,980 --> 00:54:27,839
of metadata to the control plane and

1046
00:54:27,839 --> 00:54:29,160
essentially be doing your training

1047
00:54:29,160 --> 00:54:31,500
offline and you can install model

1048
00:54:31,500 --> 00:54:35,160
weights or replace model weights as uh

1049
00:54:35,160 --> 00:54:37,500
as needed the idea is that whatever is

1050
00:54:37,500 --> 00:54:40,140
operating in the data plane itself has

1051
00:54:40,140 --> 00:54:41,940
nothing to do with the installation of

1052
00:54:41,940 --> 00:54:43,920
model weights yeah completely agnostic

1053
00:54:43,920 --> 00:54:46,319
and I thought I thought that idea that

1054
00:54:46,319 --> 00:54:47,579
you could do the model training

1055
00:54:47,579 --> 00:54:49,559
asynchronously the sample exactly is

1056
00:54:49,559 --> 00:54:52,319
very beneficial but if you consider a

1057
00:54:52,319 --> 00:54:54,540
new class of attack attack that you have

1058
00:54:54,540 --> 00:54:57,059
to understand it and do some form of

1059
00:54:57,059 --> 00:54:58,920
Bayesian analysis and classification

1060
00:54:58,920 --> 00:55:01,260
which is completely unmodeled here

1061
00:55:01,260 --> 00:55:04,380
exactly how you do that training unknown

1062
00:55:04,380 --> 00:55:06,960
how long that takes it's not about the

1063
00:55:06,960 --> 00:55:08,700
speed of the chipset it's about your

1064
00:55:08,700 --> 00:55:10,500
ability to do the good bad

1065
00:55:10,500 --> 00:55:13,559
classification a priority to inform the

1066
00:55:13,559 --> 00:55:16,260
model and then download it that's quite

1067
00:55:16,260 --> 00:55:20,099
a high cost in time yeah so so this is

1068
00:55:20,099 --> 00:55:22,200
always like kind of the uh the trouble

1069
00:55:22,200 --> 00:55:24,180
with security rate like if you want to

1070
00:55:24,180 --> 00:55:27,180
do an on-the-fly analysis of a brand new

1071
00:55:27,180 --> 00:55:30,119
attack that's not really uh what we're

1072
00:55:30,119 --> 00:55:31,619
what we're targeting at the moment here

1073
00:55:31,619 --> 00:55:34,500
but uh but in engineering terms your

1074
00:55:34,500 --> 00:55:36,900
case this is extremely fast that line

1075
00:55:36,900 --> 00:55:39,420
right well made I enjoyed listening to

1076
00:55:39,420 --> 00:55:40,440
it a lot

1077
00:55:40,440 --> 00:55:43,640
thank you thank you

1078
00:55:44,760 --> 00:55:49,260
so excellent work to share

1079
00:55:49,260 --> 00:55:51,420
I have a question during machine

1080
00:55:51,420 --> 00:55:53,940
learning and datablane will consume more

1081
00:55:53,940 --> 00:55:55,980
energy

1082
00:55:55,980 --> 00:55:58,760
oh sorry I can't uh

1083
00:55:58,760 --> 00:56:01,500
using machine learning and the datablane

1084
00:56:01,500 --> 00:56:04,440
will consume more energy

1085
00:56:04,440 --> 00:56:06,720
so and we would like to reduce the

1086
00:56:06,720 --> 00:56:08,579
energy consumption of filters and

1087
00:56:08,579 --> 00:56:10,920
switches so have you looked at this

1088
00:56:10,920 --> 00:56:14,940
issue yeah so I think the the for energy

1089
00:56:14,940 --> 00:56:17,220
consumption needs to be looked at maybe

1090
00:56:17,220 --> 00:56:19,619
more holistically so while you are

1091
00:56:19,619 --> 00:56:22,980
increasing by some small percentage the

1092
00:56:22,980 --> 00:56:25,619
energy that you'd be consuming in the

1093
00:56:25,619 --> 00:56:28,200
the switch itself you can consider that

1094
00:56:28,200 --> 00:56:29,579
say if you're doing anomaly detection

1095
00:56:29,579 --> 00:56:32,940
you're removing the cost of running an

1096
00:56:32,940 --> 00:56:34,559
anomaly detection application and

1097
00:56:34,559 --> 00:56:37,380
software on a server somewhere else so

1098
00:56:37,380 --> 00:56:39,660
with this like specialized Hardware here

1099
00:56:39,660 --> 00:56:42,180
you're consuming less power in the

1100
00:56:42,180 --> 00:56:44,400
switch than you would running it in

1101
00:56:44,400 --> 00:56:47,460
software elsewhere so on the whole

1102
00:56:47,460 --> 00:56:50,700
you're reducing power cost but for the

1103
00:56:50,700 --> 00:56:52,440
switch itself yeah you'd be increasing

1104
00:56:52,440 --> 00:56:56,299
it minimally okay thank you

1105
00:57:00,240 --> 00:57:04,799
okay thank you uh questions

1106
00:57:09,380 --> 00:57:12,319
and I guess I'll wait

1107
00:57:12,319 --> 00:57:15,960
a question uh I mean you know this this

1108
00:57:15,960 --> 00:57:19,319
is a an iitf uh meeting which is which

1109
00:57:19,319 --> 00:57:21,599
is co-locating with the ietf

1110
00:57:21,599 --> 00:57:22,680
um

1111
00:57:22,680 --> 00:57:25,500
and obviously you know since this kind

1112
00:57:25,500 --> 00:57:26,880
of case with the ITF the question is

1113
00:57:26,880 --> 00:57:29,160
then you know to what extent have you

1114
00:57:29,160 --> 00:57:31,680
given any thought towards how the how

1115
00:57:31,680 --> 00:57:34,920
this might change or affect the type of

1116
00:57:34,920 --> 00:57:37,680
work the ATF does are there any

1117
00:57:37,680 --> 00:57:40,079
implications of these types of systems

1118
00:57:40,079 --> 00:57:43,319
for the way way we design standards or

1119
00:57:43,319 --> 00:57:46,740
other types of protocols the ATF designs

1120
00:57:46,740 --> 00:57:49,500
or is this just a an optimization that

1121
00:57:49,500 --> 00:57:52,140
fits within the existing architecture

1122
00:57:52,140 --> 00:57:55,980
yeah I think uh so one of the the things

1123
00:57:55,980 --> 00:57:59,520
that uh actually uh hashemu who uh asked

1124
00:57:59,520 --> 00:58:01,380
a question earlier

1125
00:58:01,380 --> 00:58:04,680
um brought to my attention was that was

1126
00:58:04,680 --> 00:58:06,299
what kind of

1127
00:58:06,299 --> 00:58:09,299
um standardization is needed for packet

1128
00:58:09,299 --> 00:58:11,700
headers if we're going to be using them

1129
00:58:11,700 --> 00:58:15,599
as features or carrying model weights or

1130
00:58:15,599 --> 00:58:18,900
basically doing kind of this um like ml

1131
00:58:18,900 --> 00:58:22,079
uh ml assist type operations

1132
00:58:22,079 --> 00:58:23,760
um so I think there's probably something

1133
00:58:23,760 --> 00:58:27,240
there as far as making a cleaner

1134
00:58:27,240 --> 00:58:30,119
definition of what what has to happen at

1135
00:58:30,119 --> 00:58:32,760
the uh the the packet standardization

1136
00:58:32,760 --> 00:58:34,260
level

1137
00:58:34,260 --> 00:58:35,700
um to support this kind of machine

1138
00:58:35,700 --> 00:58:38,040
learning and make it uh

1139
00:58:38,040 --> 00:58:40,200
easier for different different types of

1140
00:58:40,200 --> 00:58:43,720
ml systems to interoperate

1141
00:58:43,720 --> 00:58:46,500
[Music]

1142
00:58:46,500 --> 00:58:50,299
yeah that makes a lot of

1143
00:58:50,520 --> 00:58:52,740
um presumably there's also something in

1144
00:58:52,740 --> 00:58:54,359
terms of the control plane and the

1145
00:58:54,359 --> 00:58:55,619
standardized

1146
00:58:55,619 --> 00:58:58,020
um so programming model for that in

1147
00:58:58,020 --> 00:59:01,500
order to to to specify the the model is

1148
00:59:01,500 --> 00:59:02,579
that right

1149
00:59:02,579 --> 00:59:06,260
uh sorry uh

1150
00:59:06,480 --> 00:59:08,460
um I I mean I'm thinking that your

1151
00:59:08,460 --> 00:59:11,040
traditional programmable switch uses P4

1152
00:59:11,040 --> 00:59:12,660
or something like that as a programming

1153
00:59:12,660 --> 00:59:14,819
model do do we need a similar

1154
00:59:14,819 --> 00:59:16,980
standardized programming model for these

1155
00:59:16,980 --> 00:59:19,559
types of ml switches

1156
00:59:19,559 --> 00:59:21,540
oh yeah so

1157
00:59:21,540 --> 00:59:24,299
um yeah so so is like kind of a

1158
00:59:24,299 --> 00:59:26,640
compliment to to P4

1159
00:59:26,640 --> 00:59:28,740
um we went with mapreduce so we're not

1160
00:59:28,740 --> 00:59:31,980
necessarily married to the idea of using

1161
00:59:31,980 --> 00:59:34,079
um a map produce block or anything the

1162
00:59:34,079 --> 00:59:35,819
the bigger idea here is just doing

1163
00:59:35,819 --> 00:59:38,520
inference in the data plane

1164
00:59:38,520 --> 00:59:40,200
um so but yeah it could definitely help

1165
00:59:40,200 --> 00:59:42,180
to have some sort of standardization in

1166
00:59:42,180 --> 00:59:46,140
the way that people works but for

1167
00:59:46,140 --> 00:59:50,520
um the the mapreduce elements so uh you

1168
00:59:50,520 --> 00:59:51,839
could even consider like an extra

1169
00:59:51,839 --> 00:59:54,660
control Block in P4 as mapreduce and we

1170
00:59:54,660 --> 00:59:55,920
actually we have another paper

1171
00:59:55,920 --> 00:59:57,780
intermission on

1172
00:59:57,780 --> 00:59:59,339
um what the the language level

1173
00:59:59,339 --> 01:00:01,500
constructs here look like so yeah

1174
01:00:01,500 --> 01:00:03,480
there's it's definitely an area for

1175
01:00:03,480 --> 01:00:06,619
standardization as well

1176
01:00:07,819 --> 01:00:11,819
all right great thank you are there any

1177
01:00:11,819 --> 01:00:15,380
any final questions

1178
01:00:20,059 --> 01:00:23,280
okay uh I don't see anything thank you

1179
01:00:23,280 --> 01:00:25,579
very much

1180
01:00:29,700 --> 01:00:33,180
all right Sam if you can come up while I

1181
01:00:33,180 --> 01:00:34,799
try and

1182
01:00:34,799 --> 01:00:38,000
share the slides

1183
01:00:39,299 --> 01:00:41,420
thank you

1184
01:00:51,980 --> 01:00:56,040
all right can you see the slights

1185
01:00:56,040 --> 01:00:58,380
yes we can see the slides here

1186
01:00:58,380 --> 01:01:01,160
okay

1187
01:01:03,059 --> 01:01:05,700
all right so with any luck you should

1188
01:01:05,700 --> 01:01:08,099
now have control over the oh

1189
01:01:08,099 --> 01:01:12,319
slide so they gone away

1190
01:01:23,099 --> 01:01:26,000
that's

1191
01:01:26,040 --> 01:01:28,140
is it working

1192
01:01:28,140 --> 01:01:30,480
I can see it on my phone oh yeah I see I

1193
01:01:30,480 --> 01:01:32,160
had to Click Share okay there we are

1194
01:01:32,160 --> 01:01:34,619
just took a little while yeah

1195
01:01:34,619 --> 01:01:36,720
okay

1196
01:01:36,720 --> 01:01:39,240
okay great all right so the second talk

1197
01:01:39,240 --> 01:01:40,799
today is

1198
01:01:40,799 --> 01:01:42,720
um focusing I think on a very different

1199
01:01:42,720 --> 01:01:44,160
problem domain

1200
01:01:44,160 --> 01:01:46,440
um so in this talk Sam Kumar will talk

1201
01:01:46,440 --> 01:01:49,500
about his paper on performance TCP for

1202
01:01:49,500 --> 01:01:51,660
for low power wireless networks

1203
01:01:51,660 --> 01:01:54,119
uh this was originally presented at the

1204
01:01:54,119 --> 01:01:57,420
nsdi conference in 2020 if I if I

1205
01:01:57,420 --> 01:01:58,619
remember correctly

1206
01:01:58,619 --> 01:02:01,859
uh Sam is a PhD student at UC Berkeley

1207
01:02:01,859 --> 01:02:05,480
uh advised by David color and relax

1208
01:02:05,480 --> 01:02:07,799
he's broadly interested in system

1209
01:02:07,799 --> 01:02:10,559
security and networking and his research

1210
01:02:10,559 --> 01:02:13,079
focuses on rethinking systems designed

1211
01:02:13,079 --> 01:02:15,059
to manage the overhead of using

1212
01:02:15,059 --> 01:02:17,640
cryptography and presumably also

1213
01:02:17,640 --> 01:02:20,160
improving the performance of TCP for low

1214
01:02:20,160 --> 01:02:21,960
power wireless networks

1215
01:02:21,960 --> 01:02:23,339
so

1216
01:02:23,339 --> 01:02:26,000
um Sam over to you

1217
01:02:26,000 --> 01:02:27,599
okay

1218
01:02:27,599 --> 01:02:29,700
um thanks Colin for the introduction

1219
01:02:29,700 --> 01:02:31,440
um as you said I'm Sam and I'm going to

1220
01:02:31,440 --> 01:02:33,839
present uh my research on performing TCP

1221
01:02:33,839 --> 01:02:36,000
for low power wireless networks and this

1222
01:02:36,000 --> 01:02:37,559
is a joint work with macular Arbiters at

1223
01:02:37,559 --> 01:02:39,539
UC Berkeley uh and as you mentioned it

1224
01:02:39,539 --> 01:02:43,859
was published in 2020 at nsdi so I'm

1225
01:02:43,859 --> 01:02:45,720
going to begin by giving a brief

1226
01:02:45,720 --> 01:02:48,599
overview of of history of research in

1227
01:02:48,599 --> 01:02:50,039
low power Wireless personal area

1228
01:02:50,039 --> 01:02:52,559
networks or low pens uh to put our

1229
01:02:52,559 --> 01:02:54,960
research in context so Loop band

1230
01:02:54,960 --> 01:02:57,180
research began in the late 1990s and at

1231
01:02:57,180 --> 01:02:58,920
this point in time researchers

1232
01:02:58,920 --> 01:03:00,780
deliberately Cast Away the internet

1233
01:03:00,780 --> 01:03:03,180
architecture based on the idea that the

1234
01:03:03,180 --> 01:03:04,859
load pads may have to operate in two

1235
01:03:04,859 --> 01:03:06,720
extreme environments and two different

1236
01:03:06,720 --> 01:03:09,299
uh from regular networks in order for

1237
01:03:09,299 --> 01:03:10,799
the internet architecture to directly

1238
01:03:10,799 --> 01:03:13,500
apply so many of the early protocols

1239
01:03:13,500 --> 01:03:16,260
like s-mat dmac and so on and the early

1240
01:03:16,260 --> 01:03:19,260
systems like tiny OS and contiki did not

1241
01:03:19,260 --> 01:03:20,940
conform to any particular standard or

1242
01:03:20,940 --> 01:03:22,799
architecture and this allowed the

1243
01:03:22,799 --> 01:03:24,780
researchers to nicely explore how to

1244
01:03:24,780 --> 01:03:26,220
tackle the challenges of Lopez chance

1245
01:03:26,220 --> 01:03:28,200
without being hindered by having to

1246
01:03:28,200 --> 01:03:30,240
conform to an architecture

1247
01:03:30,240 --> 01:03:33,660
about a decade later in 2008 IP the

1248
01:03:33,660 --> 01:03:35,520
Internet Protocol was first introduced

1249
01:03:35,520 --> 01:03:37,380
in the space largely enabled by the six

1250
01:03:37,380 --> 01:03:39,180
low pan adaptation layers standardized

1251
01:03:39,180 --> 01:03:40,680
by the ietf

1252
01:03:40,680 --> 01:03:42,420
and what happened here is that people

1253
01:03:42,420 --> 01:03:44,460
found ways to take the lessons that were

1254
01:03:44,460 --> 01:03:46,619
learned in the earlier systems and

1255
01:03:46,619 --> 01:03:48,839
applied them within an ipa-based

1256
01:03:48,839 --> 01:03:51,180
architecture and this essentially caught

1257
01:03:51,180 --> 01:03:54,660
on in a few years by about 2012 IEP had

1258
01:03:54,660 --> 01:03:56,579
essentially become the standard in the

1259
01:03:56,579 --> 01:03:57,420
space

1260
01:03:57,420 --> 01:04:00,420
but surprisingly the adoption of IEP did

1261
01:04:00,420 --> 01:04:03,000
not come with TCP for example open

1262
01:04:03,000 --> 01:04:05,640
thread a low pan Network stack developed

1263
01:04:05,640 --> 01:04:08,339
by nest and used in uh in the smart home

1264
01:04:08,339 --> 01:04:11,280
space didn't even support TCP and

1265
01:04:11,280 --> 01:04:13,319
instead the community has come to rely

1266
01:04:13,319 --> 01:04:16,020
on protocols like co-app which are

1267
01:04:16,020 --> 01:04:18,180
specialized low-fance protocols based on

1268
01:04:18,180 --> 01:04:19,680
UDP

1269
01:04:19,680 --> 01:04:21,299
also worth pointing out that during this

1270
01:04:21,299 --> 01:04:23,760
time low Pens have not yet achieved the

1271
01:04:23,760 --> 01:04:25,559
same kind of pervasive adoption that

1272
01:04:25,559 --> 01:04:28,680
we've seen in other protocols like Wi-Fi

1273
01:04:28,680 --> 01:04:30,480
at least in the context of branding

1274
01:04:30,480 --> 01:04:32,339
internet access to devices

1275
01:04:32,339 --> 01:04:34,619
so a natural question is whether to get

1276
01:04:34,619 --> 01:04:36,539
that kind of pervasive adoption of low

1277
01:04:36,539 --> 01:04:39,780
pens we should adopt not only IP but

1278
01:04:39,780 --> 01:04:41,819
also the broader set of Ip based

1279
01:04:41,819 --> 01:04:44,579
protocols including TCP

1280
01:04:44,579 --> 01:04:47,039
in this context our work completes the

1281
01:04:47,039 --> 01:04:49,559
transition of low pens to an ip-based

1282
01:04:49,559 --> 01:04:52,020
architecture by showing how to make TCP

1283
01:04:52,020 --> 01:04:54,539
work well in low pants and a research

1284
01:04:54,539 --> 01:04:57,839
artifact is tcplp a performant TCP stack

1285
01:04:57,839 --> 01:04:59,039
for low pens

1286
01:04:59,039 --> 01:05:01,079
so what exactly do I mean when I say

1287
01:05:01,079 --> 01:05:02,520
performant

1288
01:05:02,520 --> 01:05:05,099
well one metric is good but and that's

1289
01:05:05,099 --> 01:05:06,420
the amount of bandwidth that an

1290
01:05:06,420 --> 01:05:08,160
application is able to get when

1291
01:05:08,160 --> 01:05:10,380
operating over a TCP connection

1292
01:05:10,380 --> 01:05:12,359
now there have been a few prior attempts

1293
01:05:12,359 --> 01:05:15,359
to use TCP in the space typically based

1294
01:05:15,359 --> 01:05:17,400
on a simplified embedded TCT stack like

1295
01:05:17,400 --> 01:05:19,980
micro IP or blip and what we can see in

1296
01:05:19,980 --> 01:05:22,140
this graph is that our work tcplp

1297
01:05:22,140 --> 01:05:23,940
achieved significantly higher good but

1298
01:05:23,940 --> 01:05:25,920
than prior attempts to use TCP in this

1299
01:05:25,920 --> 01:05:26,880
space

1300
01:05:26,880 --> 01:05:28,799
in fact we can calculate an upper bound

1301
01:05:28,799 --> 01:05:30,900
on goodput shown by these dashed lines

1302
01:05:30,900 --> 01:05:33,660
based on measurements of how fast the

1303
01:05:33,660 --> 01:05:35,280
radio can send out packets and how much

1304
01:05:35,280 --> 01:05:37,500
overhead is lost to headers and acts and

1305
01:05:37,500 --> 01:05:39,720
so on and our work comes quite close to

1306
01:05:39,720 --> 01:05:42,420
these upper bounds

1307
01:05:42,420 --> 01:05:43,740
um I'd also like to share an update

1308
01:05:43,740 --> 01:05:45,299
that's happened since we published This

1309
01:05:45,299 --> 01:05:47,220
research which is that open thread the

1310
01:05:47,220 --> 01:05:49,020
low power Network stack that I mentioned

1311
01:05:49,020 --> 01:05:51,240
that's used in the smart home space we

1312
01:05:51,240 --> 01:05:53,460
simply adopted TCP directly based on our

1313
01:05:53,460 --> 01:05:56,220
research it uses tcplp as its TCP

1314
01:05:56,220 --> 01:05:58,319
implementation and the research also

1315
01:05:58,319 --> 01:06:00,420
influenced thread the network standard

1316
01:06:00,420 --> 01:06:02,520
that open thread implements so I'm

1317
01:06:02,520 --> 01:06:04,619
delighted to have been invited to spear

1318
01:06:04,619 --> 01:06:06,539
help spearhead this process and I am

1319
01:06:06,539 --> 01:06:09,059
hopeful that uh that the adoption of TCP

1320
01:06:09,059 --> 01:06:10,920
in this space will help improve the

1321
01:06:10,920 --> 01:06:12,960
adoption of low pants more broadly in

1322
01:06:12,960 --> 01:06:15,660
the smart home space

1323
01:06:15,660 --> 01:06:17,640
so now I'm going to take a step back and

1324
01:06:17,640 --> 01:06:19,020
provide some more context as to what

1325
01:06:19,020 --> 01:06:20,880
exactly low pens are and what some of

1326
01:06:20,880 --> 01:06:23,220
the challenges are with using low pens

1327
01:06:23,220 --> 01:06:25,260
and I can do that by comparing low pens

1328
01:06:25,260 --> 01:06:27,059
to other Wireless technologies that you

1329
01:06:27,059 --> 01:06:29,339
might be more familiar with so on the

1330
01:06:29,339 --> 01:06:31,500
left Wi-Fi provides a host with internet

1331
01:06:31,500 --> 01:06:33,660
access via an access point

1332
01:06:33,660 --> 01:06:35,940
in the middle Bluetooth uh doesn't

1333
01:06:35,940 --> 01:06:37,920
really provide full internet access it's

1334
01:06:37,920 --> 01:06:39,780
more like a cable replacement Channel a

1335
01:06:39,780 --> 01:06:42,000
wireless USB of sorts

1336
01:06:42,000 --> 01:06:43,799
and then on the right we have low pans

1337
01:06:43,799 --> 01:06:45,059
which aim to provide internet

1338
01:06:45,059 --> 01:06:47,220
connectivity at the same level as Wi-Fi

1339
01:06:47,220 --> 01:06:49,740
would but to embedded devices and while

1340
01:06:49,740 --> 01:06:51,780
operating within the constraints of low

1341
01:06:51,780 --> 01:06:53,940
power for example having a transmit data

1342
01:06:53,940 --> 01:06:56,099
over multiple Wireless hops to set up an

1343
01:06:56,099 --> 01:06:58,220
embedded mesh Network

1344
01:06:58,220 --> 01:07:00,900
so low pass have been used in a variety

1345
01:07:00,900 --> 01:07:02,940
of applications for example scientific

1346
01:07:02,940 --> 01:07:04,559
applications like environmental

1347
01:07:04,559 --> 01:07:06,780
monitoring structural monitoring of a

1348
01:07:06,780 --> 01:07:08,099
bridge

1349
01:07:08,099 --> 01:07:09,660
um and it's also been deployed in the

1350
01:07:09,660 --> 01:07:11,160
indoor environment in a smart grid

1351
01:07:11,160 --> 01:07:13,680
context and recently there's been a push

1352
01:07:13,680 --> 01:07:15,780
to deploy it in a smart home and iot

1353
01:07:15,780 --> 01:07:17,880
space and the thread and open third

1354
01:07:17,880 --> 01:07:19,440
efforts I mentioned earlier are one such

1355
01:07:19,440 --> 01:07:20,460
attempt

1356
01:07:20,460 --> 01:07:22,619
but despite being useful for all these

1357
01:07:22,619 --> 01:07:24,299
applications it's difficult to use low

1358
01:07:24,299 --> 01:07:25,980
pants because they also come with a set

1359
01:07:25,980 --> 01:07:27,839
of challenges

1360
01:07:27,839 --> 01:07:29,520
the first set of challenges come from

1361
01:07:29,520 --> 01:07:31,079
the resource constraints the fact that

1362
01:07:31,079 --> 01:07:33,059
the embedded hosts have limited CPU and

1363
01:07:33,059 --> 01:07:35,520
memory resources uh the second set of

1364
01:07:35,520 --> 01:07:37,380
constraints come from the link layer a

1365
01:07:37,380 --> 01:07:39,000
low band link clear like for example

1366
01:07:39,000 --> 01:07:43,079
IEEE 802.15.4 has a small MTU of only

1367
01:07:43,079 --> 01:07:45,420
about 100 bytes and has low wireless

1368
01:07:45,420 --> 01:07:47,640
range which means that in order to in

1369
01:07:47,640 --> 01:07:49,079
order to get connectivity over a large

1370
01:07:49,079 --> 01:07:50,760
area you need to transmit data over

1371
01:07:50,760 --> 01:07:52,619
multiple Wireless hops

1372
01:07:52,619 --> 01:07:54,720
and finally energy constraints are also

1373
01:07:54,720 --> 01:07:56,940
an issue uh you typically don't have

1374
01:07:56,940 --> 01:07:58,680
enough energy to keep your radio on and

1375
01:07:58,680 --> 01:08:00,539
listening all the time so you do recycle

1376
01:08:00,539 --> 01:08:02,640
your radio what that means is that your

1377
01:08:02,640 --> 01:08:04,799
radio is actually in a low power sleep

1378
01:08:04,799 --> 01:08:08,160
state for say 99 of the time and then

1379
01:08:08,160 --> 01:08:09,839
one percent of the time you can turn on

1380
01:08:09,839 --> 01:08:12,839
your radio to send or receive packets

1381
01:08:12,839 --> 01:08:14,819
and in order to provide an always-on

1382
01:08:14,819 --> 01:08:16,738
allusion to Applications despite doing

1383
01:08:16,738 --> 01:08:18,899
this to save power we need some careful

1384
01:08:18,899 --> 01:08:20,399
scheduling at the link player in order

1385
01:08:20,399 --> 01:08:22,620
to make sure the data is only sent to a

1386
01:08:22,620 --> 01:08:24,719
node when this radio is on and ready to

1387
01:08:24,719 --> 01:08:27,238
receive that data

1388
01:08:27,238 --> 01:08:30,179
so to make this more concrete uh I'm

1389
01:08:30,179 --> 01:08:31,439
going to tell you about the platform we

1390
01:08:31,439 --> 01:08:33,238
use in our research it's called Hamilton

1391
01:08:33,238 --> 01:08:35,100
and some of the stats of this platform

1392
01:08:35,100 --> 01:08:38,399
are on the slide the key Point here is

1393
01:08:38,399 --> 01:08:39,899
that this kind of device is more

1394
01:08:39,899 --> 01:08:41,819
powerful than the devices we had when

1395
01:08:41,819 --> 01:08:43,679
load band research first got started in

1396
01:08:43,679 --> 01:08:45,719
the early 2000s but it's still

1397
01:08:45,719 --> 01:08:47,880
substantially less powerful than even a

1398
01:08:47,880 --> 01:08:50,399
Raspberry Pi you cannot run Linux on a

1399
01:08:50,399 --> 01:08:52,259
device like this instead you have to run

1400
01:08:52,259 --> 01:08:54,658
a specialized embedded operating system

1401
01:08:54,658 --> 01:08:56,520
and you can understand our researchers

1402
01:08:56,520 --> 01:08:58,738
tackling the central question of how

1403
01:08:58,738 --> 01:09:00,420
should a device like this connect to the

1404
01:09:00,420 --> 01:09:01,380
internet

1405
01:09:01,380 --> 01:09:03,600
and the result of our research is that

1406
01:09:03,600 --> 01:09:06,479
we show that tcpip works well

1407
01:09:06,479 --> 01:09:08,279
now as I mentioned earlier the adoption

1408
01:09:08,279 --> 01:09:11,580
of Ip in this space did not include TCP

1409
01:09:11,580 --> 01:09:14,100
and that was no accident the reason is

1410
01:09:14,100 --> 01:09:16,319
that researchers had doubts as to

1411
01:09:16,319 --> 01:09:17,880
whether TCP would work well and they

1412
01:09:17,880 --> 01:09:19,859
expected it to not work well given the

1413
01:09:19,859 --> 01:09:21,839
challenges of low pants

1414
01:09:21,839 --> 01:09:23,819
so here are some quotes I've taken from

1415
01:09:23,819 --> 01:09:25,500
some research papers to show some of the

1416
01:09:25,500 --> 01:09:26,880
concerns that the community has had

1417
01:09:26,880 --> 01:09:29,399
about using TCP the first one is that

1418
01:09:29,399 --> 01:09:31,500
TCP is not lightweight and may not be

1419
01:09:31,500 --> 01:09:33,238
suitable for implementation in low-cost

1420
01:09:33,238 --> 01:09:34,620
sensor nodes with limiting processing

1421
01:09:34,620 --> 01:09:36,600
memory and energy resources

1422
01:09:36,600 --> 01:09:38,399
the second one is that certain features

1423
01:09:38,399 --> 01:09:40,979
of TCP may cause harm like for example

1424
01:09:40,979 --> 01:09:42,658
that the connection oriented protocol

1425
01:09:42,658 --> 01:09:45,238
aspect of TCP is a poor mesh for

1426
01:09:45,238 --> 01:09:47,279
wireless sensor networks where actual

1427
01:09:47,279 --> 01:09:48,960
data may only be in the order of a few

1428
01:09:48,960 --> 01:09:50,040
bytes

1429
01:09:50,040 --> 01:09:51,960
and finally there's the wireless TCP

1430
01:09:51,960 --> 01:09:54,360
problem the idea that TCP may use a

1431
01:09:54,360 --> 01:09:55,980
single packet drop to infer the network

1432
01:09:55,980 --> 01:09:58,080
is congested which can result in

1433
01:09:58,080 --> 01:09:59,460
extremely poor performance because

1434
01:09:59,460 --> 01:10:01,020
Wireless links tend to exhibit

1435
01:10:01,020 --> 01:10:03,420
relatively High packet loss rates so

1436
01:10:03,420 --> 01:10:06,179
again to summarize more simply there's a

1437
01:10:06,179 --> 01:10:08,460
concern that TCP is too heavy that its

1438
01:10:08,460 --> 01:10:10,679
features are necessary and that it'll

1439
01:10:10,679 --> 01:10:12,000
perform poorly in the presence of

1440
01:10:12,000 --> 01:10:14,100
Wireless loss so

1441
01:10:14,100 --> 01:10:15,659
Central to where research was

1442
01:10:15,659 --> 01:10:18,179
understanding tcp's performance and what

1443
01:10:18,179 --> 01:10:19,920
we did is we did a study where we

1444
01:10:19,920 --> 01:10:22,560
actually ran TCP in a low pan measured

1445
01:10:22,560 --> 01:10:24,900
its performance and tried to draw

1446
01:10:24,900 --> 01:10:27,000
conclusions about how well TCP really

1447
01:10:27,000 --> 01:10:29,100
does or does not perform

1448
01:10:29,100 --> 01:10:31,400
and what we found is that out of the box

1449
01:10:31,400 --> 01:10:35,040
TCP indeed performs poorly but it turns

1450
01:10:35,040 --> 01:10:37,320
out it's not due to the expected reasons

1451
01:10:37,320 --> 01:10:38,820
that people had

1452
01:10:38,820 --> 01:10:40,620
the actual reasons were somewhat

1453
01:10:40,620 --> 01:10:41,820
different

1454
01:10:41,820 --> 01:10:44,580
okay so the actual reasons are that low

1455
01:10:44,580 --> 01:10:46,980
pants have a small L2 frame size

1456
01:10:46,980 --> 01:10:49,199
basically a small MTU and this results

1457
01:10:49,199 --> 01:10:51,239
in very high header overhead

1458
01:10:51,239 --> 01:10:52,860
the second problem is that hidden

1459
01:10:52,860 --> 01:10:54,780
terminals are a serious issue for TCP

1460
01:10:54,780 --> 01:10:56,280
when operating over multiple Wireless

1461
01:10:56,280 --> 01:10:57,360
hops

1462
01:10:57,360 --> 01:10:59,100
and finally that the kind of scheduling

1463
01:10:59,100 --> 01:11:00,719
at the link clear needed to support a

1464
01:11:00,719 --> 01:11:02,340
low duty cycle and low energy

1465
01:11:02,340 --> 01:11:05,400
consumption interact poorly with TCP

1466
01:11:05,400 --> 01:11:07,199
now there's a key difference between the

1467
01:11:07,199 --> 01:11:08,580
issues on the left

1468
01:11:08,580 --> 01:11:10,920
and the issues on the right the issues

1469
01:11:10,920 --> 01:11:12,540
on the left if they were to exist would

1470
01:11:12,540 --> 01:11:14,520
be fundamental issues there's no clear

1471
01:11:14,520 --> 01:11:16,860
way to adapt TCP or the link layer to

1472
01:11:16,860 --> 01:11:18,659
eliminate those issues

1473
01:11:18,659 --> 01:11:21,480
but the issues on the right it turns out

1474
01:11:21,480 --> 01:11:23,640
are fixable within the Paradigm of TCP

1475
01:11:23,640 --> 01:11:26,159
or a fairly straightforward techniques

1476
01:11:26,159 --> 01:11:29,100
so in our research we show why the

1477
01:11:29,100 --> 01:11:31,920
expected reasons don't actually apply we

1478
01:11:31,920 --> 01:11:33,719
demonstrate techniques to address the

1479
01:11:33,719 --> 01:11:35,460
actual issues causing poor TCP

1480
01:11:35,460 --> 01:11:37,620
performance and our overall conclusion

1481
01:11:37,620 --> 01:11:39,780
is that TCP can perform well in low

1482
01:11:39,780 --> 01:11:42,360
bands after all

1483
01:11:42,360 --> 01:11:45,719
so that's an overview of what I'm going

1484
01:11:45,719 --> 01:11:47,520
to be telling you about uh and they're

1485
01:11:47,520 --> 01:11:49,320
also by the way a set of techniques that

1486
01:11:49,320 --> 01:11:51,420
we propose in order to make low pens

1487
01:11:51,420 --> 01:11:52,739
work well which I'll go over in the

1488
01:11:52,739 --> 01:11:54,719
course of the talk

1489
01:11:54,719 --> 01:11:56,820
okay in the next part of the talk I'm

1490
01:11:56,820 --> 01:11:58,620
going to focus on the expected reasons

1491
01:11:58,620 --> 01:12:02,400
for uh why uh or why the expected uses

1492
01:12:02,400 --> 01:12:04,920
for performance don't apply

1493
01:12:04,920 --> 01:12:06,420
um and to go back here I'll be talking

1494
01:12:06,420 --> 01:12:08,100
about this technique in this part of the

1495
01:12:08,100 --> 01:12:09,719
talk and the reason is that this part of

1496
01:12:09,719 --> 01:12:11,520
the talk is more about our experiments

1497
01:12:11,520 --> 01:12:13,199
and our observations about the expected

1498
01:12:13,199 --> 01:12:15,060
reasons this technique has to do with

1499
01:12:15,060 --> 01:12:16,320
their implementation which is why it's

1500
01:12:16,320 --> 01:12:18,179
included I'll talk about the remaining

1501
01:12:18,179 --> 01:12:19,980
techniques in the next part of the talk

1502
01:12:19,980 --> 01:12:21,719
where I dive into how to affix the

1503
01:12:21,719 --> 01:12:24,719
actual reasons for for performance

1504
01:12:24,719 --> 01:12:27,120
so our methodology is based on a

1505
01:12:27,120 --> 01:12:28,739
Hamilton platform as I mentioned earlier

1506
01:12:28,739 --> 01:12:30,659
you can see the picture there this is a

1507
01:12:30,659 --> 01:12:31,920
Hamilton platform connected to a

1508
01:12:31,920 --> 01:12:33,840
Raspberry Pi and the Raspberry Pi is

1509
01:12:33,840 --> 01:12:35,820
just there as a back channel to collect

1510
01:12:35,820 --> 01:12:38,940
logs and so on and measurements uh the

1511
01:12:38,940 --> 01:12:40,739
TCP stack was of course running on the

1512
01:12:40,739 --> 01:12:43,380
Hamilton platform directly our software

1513
01:12:43,380 --> 01:12:45,719
stack is using open thread with Riot OS

1514
01:12:45,719 --> 01:12:47,760
and we used a wireless test Twitter

1515
01:12:47,760 --> 01:12:49,380
collector data where each of those

1516
01:12:49,380 --> 01:12:52,199
numbers is one of our Hamilton nodes uh

1517
01:12:52,199 --> 01:12:53,640
the lines connecting them Show an

1518
01:12:53,640 --> 01:12:55,980
example of a topology in reality

1519
01:12:55,980 --> 01:12:57,420
openthair is going to generate this

1520
01:12:57,420 --> 01:12:59,040
dynamically this is just a snapshot of

1521
01:12:59,040 --> 01:13:00,960
what it might look like

1522
01:13:00,960 --> 01:13:04,739
and we ran TCP where one TCP endpoint is

1523
01:13:04,739 --> 01:13:06,120
in the wireless mesh on one of the

1524
01:13:06,120 --> 01:13:08,460
Hamilton nodes and the other TCP

1525
01:13:08,460 --> 01:13:10,199
endpoint is hosted on the cloud and

1526
01:13:10,199 --> 01:13:12,980
Amazon ec2

1527
01:13:13,320 --> 01:13:14,580
so

1528
01:13:14,580 --> 01:13:15,960
um one of the first things we had to do

1529
01:13:15,960 --> 01:13:18,600
was to implement TCP uh now as I

1530
01:13:18,600 --> 01:13:19,560
mentioned earlier there have been

1531
01:13:19,560 --> 01:13:21,540
several prior attempts to use TCP in

1532
01:13:21,540 --> 01:13:23,219
this space based on simplified embedded

1533
01:13:23,219 --> 01:13:25,679
PCP Stacks but we wanted to use a

1534
01:13:25,679 --> 01:13:28,800
full-scale TCP stack in our study now

1535
01:13:28,800 --> 01:13:30,120
the challenge is implementing a full

1536
01:13:30,120 --> 01:13:32,460
scale TCP stack is hard and in fact

1537
01:13:32,460 --> 01:13:34,620
there's an entire RFC devoted to all

1538
01:13:34,620 --> 01:13:36,300
describing all the problems that people

1539
01:13:36,300 --> 01:13:38,340
were seeing in full scale TCP stacks

1540
01:13:38,340 --> 01:13:41,760
back in 1999 even though these TCP sets

1541
01:13:41,760 --> 01:13:43,679
had matured for at least a decade by

1542
01:13:43,679 --> 01:13:44,940
this point

1543
01:13:44,940 --> 01:13:46,560
so

1544
01:13:46,560 --> 01:13:48,420
um our approach was not to implement a

1545
01:13:48,420 --> 01:13:50,340
TCP stack from scratch since we felt it

1546
01:13:50,340 --> 01:13:52,520
would be too error prone uh to do

1547
01:13:52,520 --> 01:13:54,600
instead we started with the mature

1548
01:13:54,600 --> 01:13:57,120
full-scale TCP implementation in FreeBSD

1549
01:13:57,120 --> 01:13:59,699
and re-engineered key parts of it so it

1550
01:13:59,699 --> 01:14:01,140
would work well in an embedded platform

1551
01:14:01,140 --> 01:14:03,440
and we call our resulting implementation

1552
01:14:03,440 --> 01:14:08,299
tcplp where the lp stands for low power

1553
01:14:08,340 --> 01:14:10,020
so now that we have our implementation

1554
01:14:10,020 --> 01:14:12,120
of TCP we can concretely answer the

1555
01:14:12,120 --> 01:14:14,340
question of what are the resource

1556
01:14:14,340 --> 01:14:17,760
requirements of running TCP so what we

1557
01:14:17,760 --> 01:14:19,860
found is that tcplp requires 32

1558
01:14:19,860 --> 01:14:21,600
kilobytes of code memory and about half

1559
01:14:21,600 --> 01:14:23,280
a kilobyte of data memory per connection

1560
01:14:23,280 --> 01:14:25,320
to store all of the TCP connection state

1561
01:14:25,320 --> 01:14:27,719
in a full-scale TCP implementation

1562
01:14:27,719 --> 01:14:29,400
while our platform has substantially

1563
01:14:29,400 --> 01:14:31,560
more code and data memory than that now

1564
01:14:31,560 --> 01:14:33,239
as an optimization we use separate

1565
01:14:33,239 --> 01:14:34,920
structures for active sockets that are

1566
01:14:34,920 --> 01:14:36,480
actually endpoints of a TCP connection

1567
01:14:36,480 --> 01:14:37,920
and passive sockets that are just

1568
01:14:37,920 --> 01:14:39,300
listening for new connections which

1569
01:14:39,300 --> 01:14:41,880
helps to save a bunch of memory as well

1570
01:14:41,880 --> 01:14:43,620
um but the point here is that you know

1571
01:14:43,620 --> 01:14:45,179
at least in terms of connection State

1572
01:14:45,179 --> 01:14:46,860
we're well within the bounds of the

1573
01:14:46,860 --> 01:14:49,080
available memory so natural question is

1574
01:14:49,080 --> 01:14:51,060
what about the actual buffers used to

1575
01:14:51,060 --> 01:14:53,280
send and receive data

1576
01:14:53,280 --> 01:14:54,719
so

1577
01:14:54,719 --> 01:14:56,520
um the TCB buffers need to be the

1578
01:14:56,520 --> 01:14:58,260
bandwidth delay product and size in

1579
01:14:58,260 --> 01:14:59,880
order to be able to send at full speed

1580
01:14:59,880 --> 01:15:00,840
of the network

1581
01:15:00,840 --> 01:15:02,520
and we empirically determine the

1582
01:15:02,520 --> 01:15:03,840
bandwidth delayed product as two to

1583
01:15:03,840 --> 01:15:05,520
three kilobytes and we can see in the

1584
01:15:05,520 --> 01:15:07,020
graph here how we experimentally did

1585
01:15:07,020 --> 01:15:08,580
that you can see it two to three

1586
01:15:08,580 --> 01:15:10,500
kilobytes of buffer size the available

1587
01:15:10,500 --> 01:15:13,020
could put over TCP levels off

1588
01:15:13,020 --> 01:15:14,520
so

1589
01:15:14,520 --> 01:15:17,219
here is a TCP including the size of the

1590
01:15:17,219 --> 01:15:19,500
buffers fits comfortably in memory and

1591
01:15:19,500 --> 01:15:20,820
in fact there's another conclusion to be

1592
01:15:20,820 --> 01:15:22,739
drawn here which is that if you notice

1593
01:15:22,739 --> 01:15:24,840
the the size of the buffers is actually

1594
01:15:24,840 --> 01:15:26,219
much bigger than a connection state

1595
01:15:26,219 --> 01:15:28,140
which suggests that most of the overhead

1596
01:15:28,140 --> 01:15:29,940
of TCP doesn't come with the complexity

1597
01:15:29,940 --> 01:15:32,219
of the protocol is from the buffers and

1598
01:15:32,219 --> 01:15:34,500
any performant bulk transfer protocol

1599
01:15:34,500 --> 01:15:36,300
would need these buffers in order to

1600
01:15:36,300 --> 01:15:38,880
transmit at the bdp so in some sense the

1601
01:15:38,880 --> 01:15:40,500
overhead really isn't bottlenecked by

1602
01:15:40,500 --> 01:15:43,760
tcps complexity at all

1603
01:15:44,520 --> 01:15:46,320
um there's also some uh we also

1604
01:15:46,320 --> 01:15:48,060
introduced a technique here in order to

1605
01:15:48,060 --> 01:15:49,860
reduce the memory used for the buffers

1606
01:15:49,860 --> 01:15:52,739
uh and part of this has to rely on TCP

1607
01:15:52,739 --> 01:15:55,440
having both a receive buffer and a

1608
01:15:55,440 --> 01:15:57,540
reassembly buffer to store in sequence

1609
01:15:57,540 --> 01:15:59,040
data and auto sequence data for

1610
01:15:59,040 --> 01:16:02,219
reassembly now full scale TCP Stacks

1611
01:16:02,219 --> 01:16:04,320
like FreeBSD use packet cues there's a

1612
01:16:04,320 --> 01:16:05,760
separate queue of packets for each of

1613
01:16:05,760 --> 01:16:07,980
these but in the embedded setting we

1614
01:16:07,980 --> 01:16:09,840
don't want to use dynamically allocated

1615
01:16:09,840 --> 01:16:11,460
packets because if we hold on to

1616
01:16:11,460 --> 01:16:13,320
dynamically allocated packets in a

1617
01:16:13,320 --> 01:16:15,179
memory constrained setting we may cause

1618
01:16:15,179 --> 01:16:16,800
other memory allocations to fail so

1619
01:16:16,800 --> 01:16:19,199
instead we want to use flat arrays and

1620
01:16:19,199 --> 01:16:20,880
the naive strategy would be to have a

1621
01:16:20,880 --> 01:16:22,560
separate flat array for your receive

1622
01:16:22,560 --> 01:16:25,140
queue and for the reassembly queue now

1623
01:16:25,140 --> 01:16:27,000
to optimizes what we observe is that

1624
01:16:27,000 --> 01:16:28,260
there's an interesting relationship

1625
01:16:28,260 --> 01:16:31,140
between the advertised window size the

1626
01:16:31,140 --> 01:16:33,420
number of buys we currently have and the

1627
01:16:33,420 --> 01:16:34,860
total size of that buffer which is that

1628
01:16:34,860 --> 01:16:36,420
the number of received bytes plus the

1629
01:16:36,420 --> 01:16:38,159
advertise window size is equal to the

1630
01:16:38,159 --> 01:16:41,100
total size of a receive buffer now the

1631
01:16:41,100 --> 01:16:43,320
observation we make on top of this is

1632
01:16:43,320 --> 01:16:45,300
that all of the data we may possibly get

1633
01:16:45,300 --> 01:16:47,580
for reassembly has to fit within the

1634
01:16:47,580 --> 01:16:48,960
advertised window size that's the

1635
01:16:48,960 --> 01:16:50,640
contract of TCP that if you're sending

1636
01:16:50,640 --> 01:16:52,440
to a recipient you should not go past

1637
01:16:52,440 --> 01:16:54,600
their advertised window

1638
01:16:54,600 --> 01:16:57,000
so this allows us to actually store the

1639
01:16:57,000 --> 01:16:59,400
receive buffer and the reassembly queue

1640
01:16:59,400 --> 01:17:02,699
in a single flat array okay so the way

1641
01:17:02,699 --> 01:17:04,260
this works is that we have our flat

1642
01:17:04,260 --> 01:17:06,060
array and the yellow region with the

1643
01:17:06,060 --> 01:17:07,440
start and end pointers is just a

1644
01:17:07,440 --> 01:17:09,239
circular buffer destroyer in sequence

1645
01:17:09,239 --> 01:17:12,179
data then as we receive Auto sequence

1646
01:17:12,179 --> 01:17:14,159
data that needs to be reassembled we

1647
01:17:14,159 --> 01:17:16,080
store it in the same array past the end

1648
01:17:16,080 --> 01:17:18,420
of the circular buffer using a bitmap to

1649
01:17:18,420 --> 01:17:19,920
keep track of which of these bytes are

1650
01:17:19,920 --> 01:17:21,719
active corresponding to received Auto

1651
01:17:21,719 --> 01:17:23,760
sequence data and which of them are just

1652
01:17:23,760 --> 01:17:25,620
empty slots on the array where new data

1653
01:17:25,620 --> 01:17:26,820
can be stored

1654
01:17:26,820 --> 01:17:28,920
okay so in this way we can significantly

1655
01:17:28,920 --> 01:17:31,260
reduce the memory for buffers by in some

1656
01:17:31,260 --> 01:17:33,120
sense not having to allocate a separate

1657
01:17:33,120 --> 01:17:34,860
buffer for a reassembly queue and just

1658
01:17:34,860 --> 01:17:36,179
sharing that with the buffer we've

1659
01:17:36,179 --> 01:17:39,420
allocated for the received queue

1660
01:17:39,420 --> 01:17:41,280
okay next I'm going to talk about the

1661
01:17:41,280 --> 01:17:43,500
wireless TCP problem and before we talk

1662
01:17:43,500 --> 01:17:45,000
about that let me tell you about the

1663
01:17:45,000 --> 01:17:46,500
number of in-flight segments since that

1664
01:17:46,500 --> 01:17:48,719
affects tcp's congestion control

1665
01:17:48,719 --> 01:17:50,219
so as I mentioned the Batman's live

1666
01:17:50,219 --> 01:17:52,560
product is two to three kilobytes each

1667
01:17:52,560 --> 01:17:55,260
segment is sized to about 250 to 500

1668
01:17:55,260 --> 01:17:57,060
bytes and this was chosen carefully it's

1669
01:17:57,060 --> 01:17:58,320
actually based on the technique I'll

1670
01:17:58,320 --> 01:18:00,120
tell you about later on in the talk or

1671
01:18:00,120 --> 01:18:01,739
coping with a small MTU of these

1672
01:18:01,739 --> 01:18:03,540
networks uh so I'll come back and

1673
01:18:03,540 --> 01:18:05,040
explain this but for now take it as a

1674
01:18:05,040 --> 01:18:06,780
given that our segments are 250 bytes to

1675
01:18:06,780 --> 01:18:09,420
500 bytes and what this works out too is

1676
01:18:09,420 --> 01:18:11,940
we have 4 to 12 in-flight TCP segments

1677
01:18:11,940 --> 01:18:13,440
at any one point in time

1678
01:18:13,440 --> 01:18:16,080
now this is different from other higher

1679
01:18:16,080 --> 01:18:17,520
bandwidth networks you might imagine if

1680
01:18:17,520 --> 01:18:18,719
you're transmitting over a higher

1681
01:18:18,719 --> 01:18:20,159
bandwidth Network or over a longer

1682
01:18:20,159 --> 01:18:21,840
distance you may have hundreds or

1683
01:18:21,840 --> 01:18:23,460
thousands or tens of thousands of

1684
01:18:23,460 --> 01:18:25,920
packets in flight and in comparison 4 to

1685
01:18:25,920 --> 01:18:28,440
12 is is very small and that profoundly

1686
01:18:28,440 --> 01:18:30,300
affects how tcp's congestion control

1687
01:18:30,300 --> 01:18:32,280
operates

1688
01:18:32,280 --> 01:18:35,760
so here are some examples of how of TCP

1689
01:18:35,760 --> 01:18:37,380
neureno's behavior in a low pattern and

1690
01:18:37,380 --> 01:18:39,780
for now focus on the left graph

1691
01:18:39,780 --> 01:18:42,540
um here uh our maximum segment size is

1692
01:18:42,540 --> 01:18:44,880
462 bytes

1693
01:18:44,880 --> 01:18:47,400
um and what's going on MSA and active

1694
01:18:47,400 --> 01:18:48,900
segment says I'm actually subtracting

1695
01:18:48,900 --> 01:18:50,699
the space for TCP options so this is how

1696
01:18:50,699 --> 01:18:54,239
much data is sent in htcp packet and our

1697
01:18:54,239 --> 01:18:56,159
Bama delay product is filled by just

1698
01:18:56,159 --> 01:18:59,400
four kcp segments so what ends up

1699
01:18:59,400 --> 01:19:01,560
happening is that yeah our losses are

1700
01:19:01,560 --> 01:19:03,900
very frequent but because we only need a

1701
01:19:03,900 --> 01:19:05,699
connection window of four segments in

1702
01:19:05,699 --> 01:19:09,120
order to fill up uh the bdp and standard

1703
01:19:09,120 --> 01:19:11,040
line rate gcp's condition control

1704
01:19:11,040 --> 01:19:13,440
actually is actually able to recover

1705
01:19:13,440 --> 01:19:15,360
from losses extremely quickly and we

1706
01:19:15,360 --> 01:19:17,340
spend most of our time actually sending

1707
01:19:17,340 --> 01:19:19,620
at a full window uh despite the losses

1708
01:19:19,620 --> 01:19:22,080
in the wireless medium being frequent on

1709
01:19:22,080 --> 01:19:23,280
the right we have a more challenging

1710
01:19:23,280 --> 01:19:25,440
scenario where we size our MSS to be

1711
01:19:25,440 --> 01:19:27,239
smaller and we use some active Q

1712
01:19:27,239 --> 01:19:28,800
management which induces some more lost

1713
01:19:28,800 --> 01:19:31,739
events but we still find that TCP is

1714
01:19:31,739 --> 01:19:33,840
able to reach a full window and operate

1715
01:19:33,840 --> 01:19:35,880
there most of the time despite seeing

1716
01:19:35,880 --> 01:19:37,800
treatment losses

1717
01:19:37,800 --> 01:19:40,320
so somewhat counter-intuitively we find

1718
01:19:40,320 --> 01:19:42,540
that because our bandwidth in these

1719
01:19:42,540 --> 01:19:45,060
networks is so small our bandwidth delay

1720
01:19:45,060 --> 01:19:47,100
products are small and as a result we

1721
01:19:47,100 --> 01:19:49,140
can recover to a full bdp quickly after

1722
01:19:49,140 --> 01:19:51,239
a loss and this means that the wireless

1723
01:19:51,239 --> 01:19:53,460
TCP problem actually does not affect

1724
01:19:53,460 --> 01:19:55,500
tcp's performance significantly in these

1725
01:19:55,500 --> 01:19:57,960
networks and it's much more resilient to

1726
01:19:57,960 --> 01:19:59,580
wireless losses in a low pan than it is

1727
01:19:59,580 --> 01:20:01,020
in a higher bandwidth wireless network

1728
01:20:01,020 --> 01:20:03,360
so that was a surprising result but one

1729
01:20:03,360 --> 01:20:04,739
that works well for us because it

1730
01:20:04,739 --> 01:20:06,000
removes one of the obstacles we

1731
01:20:06,000 --> 01:20:07,800
ordinarily would have faced in getting

1732
01:20:07,800 --> 01:20:10,500
TCP to work

1733
01:20:10,500 --> 01:20:12,420
so now I've talked about the expect why

1734
01:20:12,420 --> 01:20:14,400
the expected reasons don't apply in the

1735
01:20:14,400 --> 01:20:15,480
next part of the talk I'm going to tell

1736
01:20:15,480 --> 01:20:17,159
you about the actual reasons for poor

1737
01:20:17,159 --> 01:20:19,440
performance and going back to our slide

1738
01:20:19,440 --> 01:20:20,699
with our techniques on it I'll be

1739
01:20:20,699 --> 01:20:22,320
telling you about these three techniques

1740
01:20:22,320 --> 01:20:23,940
now there are a couple I didn't get to

1741
01:20:23,940 --> 01:20:25,679
the zero copy send buffer the link to

1742
01:20:25,679 --> 01:20:26,940
your queue management and that's because

1743
01:20:26,940 --> 01:20:28,440
I don't have the time in this talk to

1744
01:20:28,440 --> 01:20:30,300
talk about it but if you want to chat

1745
01:20:30,300 --> 01:20:32,280
about it afterwards I'll be around or if

1746
01:20:32,280 --> 01:20:33,719
you or you can look in the paper to find

1747
01:20:33,719 --> 01:20:36,540
some details about those

1748
01:20:36,540 --> 01:20:39,320
so first dealing with the MTU problem

1749
01:20:39,320 --> 01:20:42,060
here's a graphic showing the size of the

1750
01:20:42,060 --> 01:20:45,000
MTU in Ethernet Wi-Fi and I've Tripoli

1751
01:20:45,000 --> 01:20:46,980
irritated 15.4 which is an example of a

1752
01:20:46,980 --> 01:20:49,380
load pan link layer and what we can see

1753
01:20:49,380 --> 01:20:50,940
is that

1754
01:20:50,940 --> 01:20:53,760
um TCP IP headers are very small

1755
01:20:53,760 --> 01:20:55,860
compared to the ethernet and Wi-Fi mtus

1756
01:20:55,860 --> 01:20:58,500
but there's significant compared to the

1757
01:20:58,500 --> 01:21:01,800
IEEE inner Toyota 15.4 MTU

1758
01:21:01,800 --> 01:21:03,239
and this is going to result in large

1759
01:21:03,239 --> 01:21:05,340
header overhead okay normally we size

1760
01:21:05,340 --> 01:21:07,860
TCP segments to be as large the link

1761
01:21:07,860 --> 01:21:09,900
supports but no larger this is standard

1762
01:21:09,900 --> 01:21:11,100
this is what's used in Ethernet and

1763
01:21:11,100 --> 01:21:12,060
Wi-Fi

1764
01:21:12,060 --> 01:21:15,659
but in the case of IEEE 802.15.4 it's

1765
01:21:15,659 --> 01:21:18,900
only 104 bytes right our MTU is small

1766
01:21:18,900 --> 01:21:21,540
and our TCP IP headers can actually take

1767
01:21:21,540 --> 01:21:23,400
up more than half of that if you include

1768
01:21:23,400 --> 01:21:25,920
the cost of TCP options even if you use

1769
01:21:25,920 --> 01:21:28,080
a standard IP header compression that's

1770
01:21:28,080 --> 01:21:30,300
part of six low pan and what that means

1771
01:21:30,300 --> 01:21:31,860
is that if you're transmitting data in a

1772
01:21:31,860 --> 01:21:33,780
TCP connection more than half of the

1773
01:21:33,780 --> 01:21:35,219
data you're setting out are just these

1774
01:21:35,219 --> 01:21:37,080
headers and your good put a severely

1775
01:21:37,080 --> 01:21:39,300
affected by that

1776
01:21:39,300 --> 01:21:42,060
so in order to overcome this we break

1777
01:21:42,060 --> 01:21:43,620
this conventional wisdom and instead

1778
01:21:43,620 --> 01:21:47,100
allow tcplp to have TCP segments that

1779
01:21:47,100 --> 01:21:49,320
span multiple link layer frames okay

1780
01:21:49,320 --> 01:21:51,840
what that means is that we're relying on

1781
01:21:51,840 --> 01:21:53,580
the six low pen adaptation layer to

1782
01:21:53,580 --> 01:21:55,440
handle fragmentation and reassembly for

1783
01:21:55,440 --> 01:21:58,199
us which adds some overhead but it means

1784
01:21:58,199 --> 01:22:00,179
that the overhead of our headers is now

1785
01:22:00,179 --> 01:22:02,580
amortized over multiple frames allowing

1786
01:22:02,580 --> 01:22:04,320
us to get some good good put

1787
01:22:04,320 --> 01:22:06,600
now there is a trade-off here

1788
01:22:06,600 --> 01:22:09,719
um if we use too much fragmentation if

1789
01:22:09,719 --> 01:22:12,840
we set our our MTU I mean if we set rtcp

1790
01:22:12,840 --> 01:22:14,760
segments to be way too large what's

1791
01:22:14,760 --> 01:22:16,440
going to end up happening is that we

1792
01:22:16,440 --> 01:22:18,000
rely on too much fragmentation and

1793
01:22:18,000 --> 01:22:20,100
that's bad because now one fragment gets

1794
01:22:20,100 --> 01:22:22,260
lost we lose the entire packet so what

1795
01:22:22,260 --> 01:22:24,120
we want to do is we want to choose a TCP

1796
01:22:24,120 --> 01:22:26,280
segments to be as large as possible to

1797
01:22:26,280 --> 01:22:27,780
effectively amortize the overhead

1798
01:22:27,780 --> 01:22:30,179
without incurring more fragmentation

1799
01:22:30,179 --> 01:22:32,699
beyond that okay and this graph was an

1800
01:22:32,699 --> 01:22:35,040
experiment where we where we measured

1801
01:22:35,040 --> 01:22:36,960
the maximum segment size and the good

1802
01:22:36,960 --> 01:22:39,179
put that results and we found that the

1803
01:22:39,179 --> 01:22:40,800
gains essentially level off around three

1804
01:22:40,800 --> 01:22:43,560
to five frames uh so that's what we use

1805
01:22:43,560 --> 01:22:45,239
for our future experiments and it shows

1806
01:22:45,239 --> 01:22:46,620
that you know there's a good trade-off

1807
01:22:46,620 --> 01:22:48,420
to be made here where we can get good

1808
01:22:48,420 --> 01:22:51,179
good put despite the despite the header

1809
01:22:51,179 --> 01:22:53,520
sizes now one thing that we didn't do

1810
01:22:53,520 --> 01:22:55,199
but could potentially help in agree

1811
01:22:55,199 --> 01:22:57,600
that's orthogonal to this is to get good

1812
01:22:57,600 --> 01:22:59,820
TCP header compression right because six

1813
01:22:59,820 --> 01:23:02,100
low band currently standardizes UDP

1814
01:23:02,100 --> 01:23:03,719
hetero compression with six load pen but

1815
01:23:03,719 --> 01:23:05,760
not TCP hetero compression and that's

1816
01:23:05,760 --> 01:23:07,199
another opportunity to reduce these

1817
01:23:07,199 --> 01:23:09,840
overheads further

1818
01:23:09,840 --> 01:23:12,239
okay now I'll talk about how the link

1819
01:23:12,239 --> 01:23:14,159
layer scheduling to support a low Julius

1820
01:23:14,159 --> 01:23:16,860
cycle interacts poorly with TCP so

1821
01:23:16,860 --> 01:23:18,840
recall that these devices often don't

1822
01:23:18,840 --> 01:23:20,280
have enough energy to keep their videos

1823
01:23:20,280 --> 01:23:22,920
on listening all the time so we Define

1824
01:23:22,920 --> 01:23:24,960
the duty cycle as the proportion of time

1825
01:23:24,960 --> 01:23:26,760
that the radio is listening or

1826
01:23:26,760 --> 01:23:28,440
transmitting basically the percent of

1827
01:23:28,440 --> 01:23:30,960
time where the radio is not in a low

1828
01:23:30,960 --> 01:23:33,179
power sleep state okay and in order to

1829
01:23:33,179 --> 01:23:34,620
get good energy Concentra we want the

1830
01:23:34,620 --> 01:23:36,900
duty cycle to be as close to zero as

1831
01:23:36,900 --> 01:23:37,980
possible

1832
01:23:37,980 --> 01:23:40,199
uh now there are several ways in order

1833
01:23:40,199 --> 01:23:41,940
to support this uh in the session of

1834
01:23:41,940 --> 01:23:43,860
literature open third uses a particular

1835
01:23:43,860 --> 01:23:45,719
Duty cycling mechanism that's called a

1836
01:23:45,719 --> 01:23:47,580
receiver initiated duty cycle protocol

1837
01:23:47,580 --> 01:23:50,100
which I'll now explain

1838
01:23:50,100 --> 01:23:52,320
so in open thread you have two kinds of

1839
01:23:52,320 --> 01:23:54,120
nodes you have battery powered nodes

1840
01:23:54,120 --> 01:23:55,920
where we want to minimize the duty cycle

1841
01:23:55,920 --> 01:23:58,020
and wall power nodes that are plugged

1842
01:23:58,020 --> 01:23:59,699
into a wall outlet and have enough power

1843
01:23:59,699 --> 01:24:02,219
to keep their videos always on

1844
01:24:02,219 --> 01:24:06,600
okay now sending a frame from B to W is

1845
01:24:06,600 --> 01:24:09,239
easy because W3 audio is always on so we

1846
01:24:09,239 --> 01:24:11,760
can just send the frame whenever we like

1847
01:24:11,760 --> 01:24:14,400
more challenging is the reverse getting

1848
01:24:14,400 --> 01:24:16,620
a frame from W to B

1849
01:24:16,620 --> 01:24:19,800
okay so what has to happen is that W has

1850
01:24:19,800 --> 01:24:22,440
to wait until B is Radio is listening

1851
01:24:22,440 --> 01:24:24,060
and how does it know when boost radio is

1852
01:24:24,060 --> 01:24:25,800
listening well this is where the

1853
01:24:25,800 --> 01:24:27,780
protocol comes in what B does is that it

1854
01:24:27,780 --> 01:24:30,000
never returns on S3 audio to listen for

1855
01:24:30,000 --> 01:24:32,580
a for a frame it'll send a data request

1856
01:24:32,580 --> 01:24:35,400
packet to W informing it that it's now

1857
01:24:35,400 --> 01:24:37,920
listening so w has to wait until it

1858
01:24:37,920 --> 01:24:39,780
guesses data request packet and once it

1859
01:24:39,780 --> 01:24:42,000
does then it can go ahead and send the

1860
01:24:42,000 --> 01:24:44,100
frame to B and B will listen and receive

1861
01:24:44,100 --> 01:24:46,320
the frame okay so what's the key Point

1862
01:24:46,320 --> 01:24:48,239
here the key point I want to emphasize

1863
01:24:48,239 --> 01:24:50,460
is that these idle duty cycle is

1864
01:24:50,460 --> 01:24:52,260
directly related to how frequently it

1865
01:24:52,260 --> 01:24:54,659
sends data request frames B can choose

1866
01:24:54,659 --> 01:24:56,460
to send data request frames very rarely

1867
01:24:56,460 --> 01:24:58,440
which allow you to get very good energy

1868
01:24:58,440 --> 01:25:01,320
consumption but doing so what uh what

1869
01:25:01,320 --> 01:25:02,639
we're doing so will cause more of a

1870
01:25:02,639 --> 01:25:04,500
delay in getting frames to it since W

1871
01:25:04,500 --> 01:25:06,239
has to wait for the data request frame

1872
01:25:06,239 --> 01:25:09,120
in order to send it one of the uh one of

1873
01:25:09,120 --> 01:25:10,380
the data frames

1874
01:25:10,380 --> 01:25:13,020
okay so now let me talk about what this

1875
01:25:13,020 --> 01:25:15,480
means for TCP operation and I'll do this

1876
01:25:15,480 --> 01:25:18,719
by comparing HTTP over TCP to co-app

1877
01:25:18,719 --> 01:25:20,880
okay and co-app is a rest-based protocol

1878
01:25:20,880 --> 01:25:23,820
running on top of UDP and in our setup

1879
01:25:23,820 --> 01:25:26,340
we had bsnw we did a request frame every

1880
01:25:26,340 --> 01:25:28,199
one second basically it basically it's

1881
01:25:28,199 --> 01:25:30,120
in for packets every one second and that

1882
01:25:30,120 --> 01:25:32,820
allows it to get a really low duty cycle

1883
01:25:32,820 --> 01:25:34,800
now the key difference between HTTP and

1884
01:25:34,800 --> 01:25:36,719
co-app here is that the HTTP requires

1885
01:25:36,719 --> 01:25:39,000
two round trips whereas co-app only

1886
01:25:39,000 --> 01:25:41,760
requires one round trip okay so for the

1887
01:25:41,760 --> 01:25:43,679
first round trip right you start at a

1888
01:25:43,679 --> 01:25:46,139
random uh phase within the 100 Milli

1889
01:25:46,139 --> 01:25:47,820
within a thousand millisecond sleep

1890
01:25:47,820 --> 01:25:49,980
interval so you'd expect on average a

1891
01:25:49,980 --> 01:25:52,139
500 millisecond delay and Co-op is

1892
01:25:52,139 --> 01:25:54,719
consistent with that for HTTP what

1893
01:25:54,719 --> 01:25:56,219
happens is that for the first round trip

1894
01:25:56,219 --> 01:25:58,620
we see 500 milliseconds but the second

1895
01:25:58,620 --> 01:26:01,020
round trip starts right at the beginning

1896
01:26:01,020 --> 01:26:02,760
of the next leap interval so the second

1897
01:26:02,760 --> 01:26:05,340
round trip consistently sees the worst

1898
01:26:05,340 --> 01:26:07,739
case latency when transmitting the

1899
01:26:07,739 --> 01:26:11,100
packet from W to B okay and as a result

1900
01:26:11,100 --> 01:26:13,500
HTTP performs more than twice as poorly

1901
01:26:13,500 --> 01:26:16,560
as Co-op uh on this workload now I want

1902
01:26:16,560 --> 01:26:17,460
to point out that there have been some

1903
01:26:17,460 --> 01:26:20,040
recent extensions to TCP for example TCP

1904
01:26:20,040 --> 01:26:22,139
fast open which you can use to eliminate

1905
01:26:22,139 --> 01:26:23,340
the second round trip and get

1906
01:26:23,340 --> 01:26:25,139
performance parity between coapp and

1907
01:26:25,139 --> 01:26:26,280
http

1908
01:26:26,280 --> 01:26:28,980
but this problem also happens for both

1909
01:26:28,980 --> 01:26:30,960
transfers where the ACT clock nature of

1910
01:26:30,960 --> 01:26:33,719
TCP causes it to consistently experience

1911
01:26:33,719 --> 01:26:35,820
the worst case latency even for bulk

1912
01:26:35,820 --> 01:26:37,380
transfers so this is an important

1913
01:26:37,380 --> 01:26:40,020
problem to solve regardless of that

1914
01:26:40,020 --> 01:26:42,000
and our approach to solving it is to use

1915
01:26:42,000 --> 01:26:44,639
an Adaptive duty cycle the idea is that

1916
01:26:44,639 --> 01:26:47,040
we can use the TCP and HTTP protocol

1917
01:26:47,040 --> 01:26:49,980
state in order to vary how often we send

1918
01:26:49,980 --> 01:26:52,139
data request frames the idea being when

1919
01:26:52,139 --> 01:26:54,420
we expect a packet we want to send data

1920
01:26:54,420 --> 01:26:56,460
request frames more frequently so for

1921
01:26:56,460 --> 01:26:58,620
example if I'm an HTTP server one of

1922
01:26:58,620 --> 01:27:00,719
these battery-powered devices and I just

1923
01:27:00,719 --> 01:27:03,060
accepted a TCP connection I can be

1924
01:27:03,060 --> 01:27:04,860
pretty sure that that I'm going to soon

1925
01:27:04,860 --> 01:27:06,480
receive an HTTP request on that

1926
01:27:06,480 --> 01:27:08,760
connection so I may choose to send data

1927
01:27:08,760 --> 01:27:10,560
request frames more frequently at that

1928
01:27:10,560 --> 01:27:13,260
point in time and doing this nearly

1929
01:27:13,260 --> 01:27:15,420
entirely eliminates the gap between

1930
01:27:15,420 --> 01:27:19,139
Co-op and HTTP intros or performance

1931
01:27:19,139 --> 01:27:20,940
so if we zoom out and look at the

1932
01:27:20,940 --> 01:27:22,860
overall Network this adapter Junior

1933
01:27:22,860 --> 01:27:25,139
cycle technique works well for the last

1934
01:27:25,139 --> 01:27:27,239
hop going from a wall powered node to a

1935
01:27:27,239 --> 01:27:28,800
battery-powered node node but the

1936
01:27:28,800 --> 01:27:30,360
overall network has the last operator

1937
01:27:30,360 --> 01:27:32,400
over multiple Wireless hops to even get

1938
01:27:32,400 --> 01:27:33,780
to that last hop

1939
01:27:33,780 --> 01:27:35,520
and what we observed with the TCP

1940
01:27:35,520 --> 01:27:37,920
performs poorly over this chain of wall

1941
01:27:37,920 --> 01:27:40,940
powered nodes due to Hidden terminals

1942
01:27:40,940 --> 01:27:42,719
so

1943
01:27:42,719 --> 01:27:44,760
let me step back and go over hidden

1944
01:27:44,760 --> 01:27:46,320
terminals to provide some background on

1945
01:27:46,320 --> 01:27:47,699
that for those who aren't familiar with

1946
01:27:47,699 --> 01:27:48,420
it

1947
01:27:48,420 --> 01:27:50,280
uh we can understand the wireless range

1948
01:27:50,280 --> 01:27:51,960
of a node is looking something like this

1949
01:27:51,960 --> 01:27:53,820
uh the unit disk model is a

1950
01:27:53,820 --> 01:27:55,860
simplification where we consider this to

1951
01:27:55,860 --> 01:27:58,260
be in some sort of the perfect circle uh

1952
01:27:58,260 --> 01:28:00,179
in practice of course it can be more

1953
01:28:00,179 --> 01:28:01,739
complex depending on the exact

1954
01:28:01,739 --> 01:28:04,800
environment your deployment is in uh but

1955
01:28:04,800 --> 01:28:07,080
unit just model is is going to be enough

1956
01:28:07,080 --> 01:28:08,520
for us to capture the phenomena of

1957
01:28:08,520 --> 01:28:11,219
Interest here so we'll go with that

1958
01:28:11,219 --> 01:28:12,960
um so imagine you have four segments in

1959
01:28:12,960 --> 01:28:15,360
a line I mean four four nodes in a line

1960
01:28:15,360 --> 01:28:17,159
with their uh with their transmission

1961
01:28:17,159 --> 01:28:18,840
ranges shown here and we want to

1962
01:28:18,840 --> 01:28:21,659
transmit data from a to d now the nature

1963
01:28:21,659 --> 01:28:24,540
of TCP is that we have multiple segments

1964
01:28:24,540 --> 01:28:26,219
in flight at the same time for a single

1965
01:28:26,219 --> 01:28:27,900
connection and that's why we have

1966
01:28:27,900 --> 01:28:29,760
segment one being sent from C to D and

1967
01:28:29,760 --> 01:28:32,280
segment two being sent from A to B

1968
01:28:32,280 --> 01:28:34,860
but unfortunately this is bad because

1969
01:28:34,860 --> 01:28:36,120
the wireless Rangers are going to

1970
01:28:36,120 --> 01:28:37,920
overlap at B so the two packets are

1971
01:28:37,920 --> 01:28:40,020
going to interfere there okay now in the

1972
01:28:40,020 --> 01:28:42,360
context of Wi-Fi we typically overcome

1973
01:28:42,360 --> 01:28:44,699
this using a protocol based on RTS and

1974
01:28:44,699 --> 01:28:47,340
CTS frames that allow us to mitigate the

1975
01:28:47,340 --> 01:28:49,560
hidden terminal problem in most cases

1976
01:28:49,560 --> 01:28:52,260
but in the context of low pens the small

1977
01:28:52,260 --> 01:28:55,860
MTU means that RDS CTS typically has too

1978
01:28:55,860 --> 01:28:58,139
high of an overhead as a result most

1979
01:28:58,139 --> 01:29:01,100
uses of it don't use RTS and CTS packets

1980
01:29:01,100 --> 01:29:03,360
so as a result we're only relying on

1981
01:29:03,360 --> 01:29:07,260
csma right so at a csma can't detect uh

1982
01:29:07,260 --> 01:29:10,679
C's transmission because uh it's all the

1983
01:29:10,679 --> 01:29:13,440
way because a is out of range of c and

1984
01:29:13,440 --> 01:29:15,659
csma at C can't detect A's transmission

1985
01:29:15,659 --> 01:29:18,540
because C is out of range of a

1986
01:29:18,540 --> 01:29:20,340
but both of the packets end up

1987
01:29:20,340 --> 01:29:22,320
interfering at B and the packet gets

1988
01:29:22,320 --> 01:29:24,420
lost

1989
01:29:24,420 --> 01:29:26,219
um this also happens because of data

1990
01:29:26,219 --> 01:29:27,780
packets and acts going in opposite

1991
01:29:27,780 --> 01:29:30,840
directions so for example here uh what

1992
01:29:30,840 --> 01:29:33,780
we'll ultimately see is that

1993
01:29:33,780 --> 01:29:34,620
um

1994
01:29:34,620 --> 01:29:36,840
you get the same problem with b and d

1995
01:29:36,840 --> 01:29:38,520
both setting at the same time to C

1996
01:29:38,520 --> 01:29:41,280
because each of their csmas can't hear

1997
01:29:41,280 --> 01:29:43,620
the other

1998
01:29:43,620 --> 01:29:46,020
so to mitigate this our approach is to

1999
01:29:46,020 --> 01:29:48,600
add a new random back off delay between

2000
01:29:48,600 --> 01:29:50,760
link layer rate price okay so the idea

2001
01:29:50,760 --> 01:29:53,340
is if you transmit a frame and it fails

2002
01:29:53,340 --> 01:29:54,360
but you know because you don't get a

2003
01:29:54,360 --> 01:29:56,639
link layer acknowledgment for it then

2004
01:29:56,639 --> 01:29:59,159
you wait a random amount and retry the

2005
01:29:59,159 --> 01:30:00,719
transmission and this is different from

2006
01:30:00,719 --> 01:30:03,239
csma in two respects the first respect

2007
01:30:03,239 --> 01:30:06,540
is that in csma you do this randomized

2008
01:30:06,540 --> 01:30:09,600
uh delay with exponential back off if

2009
01:30:09,600 --> 01:30:12,000
the channel appears busy in this case

2010
01:30:12,000 --> 01:30:14,340
even the channel appears clear if your

2011
01:30:14,340 --> 01:30:16,739
transmission fails we still do the back

2012
01:30:16,739 --> 01:30:18,600
off so it's different in regards of what

2013
01:30:18,600 --> 01:30:20,580
triggers the transmission

2014
01:30:20,580 --> 01:30:22,620
and second it's a much longer delay

2015
01:30:22,620 --> 01:30:24,719
right because in csma you can rely on

2016
01:30:24,719 --> 01:30:26,340
hearing a concurrent transmission you

2017
01:30:26,340 --> 01:30:28,199
can transmit immediately if a channel

2018
01:30:28,199 --> 01:30:30,540
appears clearer in this new delay that

2019
01:30:30,540 --> 01:30:32,639
we're adding this link free trial delay

2020
01:30:32,639 --> 01:30:34,679
what we're seeing is that we want to

2021
01:30:34,679 --> 01:30:38,100
have a delay that's chosen between 0 and

2022
01:30:38,100 --> 01:30:39,900
10 times the time to transmitter frame

2023
01:30:39,900 --> 01:30:41,699
the idea being even if there are two

2024
01:30:41,699 --> 01:30:43,080
concurrent Transmissions that can't hear

2025
01:30:43,080 --> 01:30:44,880
each other with high probability they

2026
01:30:44,880 --> 01:30:46,679
won't overlap in time

2027
01:30:46,679 --> 01:30:48,780
okay so

2028
01:30:48,780 --> 01:30:50,940
um the way this would work is that uh

2029
01:30:50,940 --> 01:30:52,860
each of these two nodes would send its

2030
01:30:52,860 --> 01:30:55,199
data once in order to go and they will

2031
01:30:55,199 --> 01:30:57,600
collide but then when they retry the

2032
01:30:57,600 --> 01:30:59,280
transmit a second time at hopefully

2033
01:30:59,280 --> 01:31:00,780
different intervals and they won't

2034
01:31:00,780 --> 01:31:02,639
overlap in time and the transmission

2035
01:31:02,639 --> 01:31:06,360
will succeed Okay so

2036
01:31:06,360 --> 01:31:07,920
um we did a measurements for you to

2037
01:31:07,920 --> 01:31:09,300
understand what kind of Link delays

2038
01:31:09,300 --> 01:31:10,739
would be appropriate and what would work

2039
01:31:10,739 --> 01:31:12,900
what we observe is that there's a huge

2040
01:31:12,900 --> 01:31:15,300
reduction in packet loss even from a

2041
01:31:15,300 --> 01:31:17,400
small delay and as we increase the delay

2042
01:31:17,400 --> 01:31:19,139
too much it starts to iterate your good

2043
01:31:19,139 --> 01:31:20,820
bud because now you're beating a lot

2044
01:31:20,820 --> 01:31:22,800
when transmitting your packets

2045
01:31:22,800 --> 01:31:24,300
so we found that there's a sweet spot

2046
01:31:24,300 --> 01:31:26,219
here at around 40 milliseconds which is

2047
01:31:26,219 --> 01:31:28,020
about 10 times the time to transmit a

2048
01:31:28,020 --> 01:31:29,639
single frame and I have Triple A root

2049
01:31:29,639 --> 01:31:31,080
2.15.4

2050
01:31:31,080 --> 01:31:34,440
uh so that's what we used in our study

2051
01:31:34,440 --> 01:31:36,480
um and this reduced a packet cluster

2052
01:31:36,480 --> 01:31:38,159
from six percent to one percent which

2053
01:31:38,159 --> 01:31:40,020
was we should consider a significant

2054
01:31:40,020 --> 01:31:41,880
Improvement

2055
01:31:41,880 --> 01:31:43,860
so finally I'm going to summarize our

2056
01:31:43,860 --> 01:31:47,219
evaluation and and conclusions so first

2057
01:31:47,219 --> 01:31:48,600
I previewed this result at the beginning

2058
01:31:48,600 --> 01:31:50,340
we're able to achieve significantly

2059
01:31:50,340 --> 01:31:51,840
higher good but than prior attempts at

2060
01:31:51,840 --> 01:31:54,360
using TCP and we're very close to a

2061
01:31:54,360 --> 01:31:55,920
reasonable upper bound that we computed

2062
01:31:55,920 --> 01:31:57,300
based on measurements of how fast the

2063
01:31:57,300 --> 01:31:59,159
radio can send out packets and the

2064
01:31:59,159 --> 01:32:02,780
overhead loss to headers and acts

2065
01:32:03,179 --> 01:32:04,980
um we also did a measurement study to

2066
01:32:04,980 --> 01:32:07,020
study the Energy Efficiency so we use

2067
01:32:07,020 --> 01:32:09,480
TCP and Co-op for a sentence and task

2068
01:32:09,480 --> 01:32:11,580
and measure the radio dutious cycle over

2069
01:32:11,580 --> 01:32:14,040
a 24-hour period and you can see the

2070
01:32:14,040 --> 01:32:16,500
radio duty cycle here the key point is

2071
01:32:16,500 --> 01:32:18,780
that TCP is not significantly worse than

2072
01:32:18,780 --> 01:32:20,580
co-op in fact they perform comparably

2073
01:32:20,580 --> 01:32:22,440
for the duration of the experiment at

2074
01:32:22,440 --> 01:32:24,659
about a two percent duty cycle and be

2075
01:32:24,659 --> 01:32:26,520
considered this a success because TCP is

2076
01:32:26,520 --> 01:32:28,920
able to perform essentially on par as a

2077
01:32:28,920 --> 01:32:30,900
protocol over UDP developed specifically

2078
01:32:30,900 --> 01:32:33,420
for low pens

2079
01:32:33,420 --> 01:32:35,760
so now the TCP is a viable option what

2080
01:32:35,760 --> 01:32:37,620
does this mean well first we should

2081
01:32:37,620 --> 01:32:39,000
reconsider the use of lightweight

2082
01:32:39,000 --> 01:32:40,860
protocols that emulate part of tcp's

2083
01:32:40,860 --> 01:32:42,420
functionality

2084
01:32:42,420 --> 01:32:43,980
um in the sense that you know if you

2085
01:32:43,980 --> 01:32:45,420
have a protocol that's specialized and

2086
01:32:45,420 --> 01:32:46,980
performs just as well as a general

2087
01:32:46,980 --> 01:32:49,080
protocol that's more interoperable and

2088
01:32:49,080 --> 01:32:50,880
used more broadly we should perhaps

2089
01:32:50,880 --> 01:32:52,560
prefer the one that's used more broadly

2090
01:32:52,560 --> 01:32:54,300
and is more interoperable

2091
01:32:54,300 --> 01:32:56,580
second we think that TCP May influence

2092
01:32:56,580 --> 01:32:58,920
the design of low-pad network systems in

2093
01:32:58,920 --> 01:33:00,239
the sense that you know for a long time

2094
01:33:00,239 --> 01:33:01,860
it's been the case that many smart home

2095
01:33:01,860 --> 01:33:03,179
devices that you buy on the market

2096
01:33:03,179 --> 01:33:05,520
require a specialized gateway to get

2097
01:33:05,520 --> 01:33:07,560
internet connectivity

2098
01:33:07,560 --> 01:33:09,900
um and TCP gives us the opportunity to

2099
01:33:09,900 --> 01:33:11,400
allow these devices to connect end to

2100
01:33:11,400 --> 01:33:14,040
end to any uh Services externally that

2101
01:33:14,040 --> 01:33:15,900
they may depend on

2102
01:33:15,900 --> 01:33:18,179
and finally I just want to mention the

2103
01:33:18,179 --> 01:33:19,920
UDP based protocols I think will still

2104
01:33:19,920 --> 01:33:22,020
be used in low pens but just in the same

2105
01:33:22,020 --> 01:33:23,400
sense that they're used broadly in the

2106
01:33:23,400 --> 01:33:24,780
internet for applications with

2107
01:33:24,780 --> 01:33:26,219
specialized protocols substantially

2108
01:33:26,219 --> 01:33:29,219
outperform TCP in cases where TCP

2109
01:33:29,219 --> 01:33:30,780
performs on par with the specialized

2110
01:33:30,780 --> 01:33:33,239
protocols using TCP is now a viable

2111
01:33:33,239 --> 01:33:35,219
option

2112
01:33:35,219 --> 01:33:37,440
so just to talk a little more about the

2113
01:33:37,440 --> 01:33:39,480
about the middle point about how TCB May

2114
01:33:39,480 --> 01:33:40,620
influence the design of low-pad network

2115
01:33:40,620 --> 01:33:43,199
systems when I say Gateway architecture

2116
01:33:43,199 --> 01:33:44,940
I mean a setup like this where you have

2117
01:33:44,940 --> 01:33:46,440
your devices these smartphone devices

2118
01:33:46,440 --> 01:33:48,719
you bought on the market and in order to

2119
01:33:48,719 --> 01:33:50,040
allow them to communicate with an

2120
01:33:50,040 --> 01:33:51,420
application server in a data center

2121
01:33:51,420 --> 01:33:53,159
somewhere you have to install some

2122
01:33:53,159 --> 01:33:55,260
specific Gateway in your home that is

2123
01:33:55,260 --> 01:33:56,580
some protocol translation and

2124
01:33:56,580 --> 01:33:58,500
application logic in order to bring

2125
01:33:58,500 --> 01:34:00,360
connectivity to those devices

2126
01:34:00,360 --> 01:34:02,100
uh what this means is it's often the

2127
01:34:02,100 --> 01:34:02,940
case for some of you may have

2128
01:34:02,940 --> 01:34:04,920
experienced this is that if you go buy a

2129
01:34:04,920 --> 01:34:07,980
smart devices from a new vendor uh now

2130
01:34:07,980 --> 01:34:10,260
all of a sudden you need another Gateway

2131
01:34:10,260 --> 01:34:12,719
for those new devices or even maybe the

2132
01:34:12,719 --> 01:34:14,219
newer versions of devices from the same

2133
01:34:14,219 --> 01:34:16,500
vendor like for example uh for a long

2134
01:34:16,500 --> 01:34:19,320
time it was a case that for life that if

2135
01:34:19,320 --> 01:34:21,840
you have bulbs from say lifx and Bulbs

2136
01:34:21,840 --> 01:34:23,100
from Philips you would need separate

2137
01:34:23,100 --> 01:34:26,639
gateways for both of those devices

2138
01:34:26,639 --> 01:34:27,360
um

2139
01:34:27,360 --> 01:34:30,480
so uh the the introduction of Ip in this

2140
01:34:30,480 --> 01:34:32,760
space didn't really change this in the

2141
01:34:32,760 --> 01:34:34,500
sense that now your application protocol

2142
01:34:34,500 --> 01:34:36,239
on the left is now implemented over IP

2143
01:34:36,239 --> 01:34:37,860
but you still need the application layer

2144
01:34:37,860 --> 01:34:40,139
Gateway and the missing piece I think

2145
01:34:40,139 --> 01:34:42,780
that would allow an end-to-end uh a

2146
01:34:42,780 --> 01:34:44,280
connection here would be to have a

2147
01:34:44,280 --> 01:34:45,840
transfer protocol that's supported on

2148
01:34:45,840 --> 01:34:48,780
both sides namely TCP and once you do

2149
01:34:48,780 --> 01:34:50,280
this your application layer again please

2150
01:34:50,280 --> 01:34:51,960
become regular border routers and you

2151
01:34:51,960 --> 01:34:53,340
could potentially consolidate these

2152
01:34:53,340 --> 01:34:56,159
together into a single border router

2153
01:34:56,159 --> 01:34:57,540
so

2154
01:34:57,540 --> 01:35:00,780
um in conclusion uh we implemented tcplp

2155
01:35:00,780 --> 01:35:02,760
a full scale TCP stack for low pan

2156
01:35:02,760 --> 01:35:05,340
devices uh we explained why the expected

2157
01:35:05,340 --> 01:35:06,900
reasons for Port TCP performance don't

2158
01:35:06,900 --> 01:35:09,239
apply uh we show how to address the

2159
01:35:09,239 --> 01:35:11,159
actual reasons for poor TCP performance

2160
01:35:11,159 --> 01:35:13,020
and we show that once the issues are

2161
01:35:13,020 --> 01:35:15,300
resolved TCP can perform comparably to

2162
01:35:15,300 --> 01:35:17,580
low band specialized protocols that's

2163
01:35:17,580 --> 01:35:18,960
all I have prepared I'm happy to take

2164
01:35:18,960 --> 01:35:21,800
any questions now

2165
01:35:23,639 --> 01:35:25,820
foreign

2166
01:35:30,380 --> 01:35:32,520
excellent talk

2167
01:35:32,520 --> 01:35:34,920
um I see we have a couple of people in

2168
01:35:34,920 --> 01:35:37,080
the online queue and a couple of people

2169
01:35:37,080 --> 01:35:39,000
at the microphone

2170
01:35:39,000 --> 01:35:41,639
um which we do the uh I guess that we'll

2171
01:35:41,639 --> 01:35:44,520
do the microphone first so uh I can see

2172
01:35:44,520 --> 01:35:46,080
who that is but if if you can go ahead

2173
01:35:46,080 --> 01:35:50,059
and see your name in your question

2174
01:35:51,780 --> 01:35:54,300
so hi I'm Matthias I'm one of the

2175
01:35:54,300 --> 01:35:56,159
co-founders of white great work thanks a

2176
01:35:56,159 --> 01:35:57,659
lot

2177
01:35:57,659 --> 01:35:59,760
um one remark and two questions a

2178
01:35:59,760 --> 01:36:01,080
question first

2179
01:36:01,080 --> 01:36:03,960
um so you argued that supporting TCP is

2180
01:36:03,960 --> 01:36:06,000
important because it's popular now Creek

2181
01:36:06,000 --> 01:36:08,219
becomes popular did you work on any

2182
01:36:08,219 --> 01:36:11,219
comparison from a system point of view

2183
01:36:11,219 --> 01:36:14,040
um sorry I didn't quite hear what they

2184
01:36:14,040 --> 01:36:16,260
said becomes popular uh you said that

2185
01:36:16,260 --> 01:36:18,540
TCP is quite popular but quick also

2186
01:36:18,540 --> 01:36:21,360
becomes popular in the internet quick

2187
01:36:21,360 --> 01:36:24,019
you know

2188
01:36:27,860 --> 01:36:30,239
uh so we didn't do a comparison against

2189
01:36:30,239 --> 01:36:31,800
quick but I like to comment on that

2190
01:36:31,800 --> 01:36:33,060
because that's a good point that other

2191
01:36:33,060 --> 01:36:35,460
transports are becoming popular many of

2192
01:36:35,460 --> 01:36:37,500
the issues that we addressed aren't

2193
01:36:37,500 --> 01:36:39,780
specific to TCP they apply broadly to

2194
01:36:39,780 --> 01:36:42,239
TCP and other protocols needed for bulk

2195
01:36:42,239 --> 01:36:44,400
transfer like for example

2196
01:36:44,400 --> 01:36:46,139
um the main issues getting it to work

2197
01:36:46,139 --> 01:36:47,940
with hidden terminals getting it to play

2198
01:36:47,940 --> 01:36:50,100
well with link clear scheduling and so

2199
01:36:50,100 --> 01:36:52,500
on apply broadly to any protocol that's

2200
01:36:52,500 --> 01:36:54,840
transmitting a lot of data and wants a

2201
01:36:54,840 --> 01:36:56,040
significant amount of bandwidth

2202
01:36:56,040 --> 01:36:57,600
therefore I think that many of our

2203
01:36:57,600 --> 01:36:59,159
conclusions would actually apply equally

2204
01:36:59,159 --> 01:37:01,500
well to quick as they do to TCP

2205
01:37:01,500 --> 01:37:03,420
okay

2206
01:37:03,420 --> 01:37:05,639
um and another question I mean in your

2207
01:37:05,639 --> 01:37:07,380
paper you note that you also have an

2208
01:37:07,380 --> 01:37:09,300
implementation for gnac the default

2209
01:37:09,300 --> 01:37:11,460
networks they can write do you also plan

2210
01:37:11,460 --> 01:37:13,320
to submit the Pierre to upstream's

2211
01:37:13,320 --> 01:37:14,460
implementation

2212
01:37:14,460 --> 01:37:15,480
um

2213
01:37:15,480 --> 01:37:17,400
at some point we did have plans for that

2214
01:37:17,400 --> 01:37:18,960
but what happened is that Riot OS

2215
01:37:18,960 --> 01:37:20,760
already adopted a different TCP stack

2216
01:37:20,760 --> 01:37:21,960
and it seemed a bit redundant to

2217
01:37:21,960 --> 01:37:24,179
contribute a second one uh recently what

2218
01:37:24,179 --> 01:37:25,800
we've done is we have we must have

2219
01:37:25,800 --> 01:37:27,360
contributed our code to open thread

2220
01:37:27,360 --> 01:37:29,280
which now uses it as its default TCP

2221
01:37:29,280 --> 01:37:31,560
stack okay first did I highly encourage

2222
01:37:31,560 --> 01:37:34,800
you to submit the PM and final remark um

2223
01:37:34,800 --> 01:37:38,460
you said that the fragment needs to be a

2224
01:37:38,460 --> 01:37:40,199
packet needs to be is lost when the

2225
01:37:40,199 --> 01:37:42,239
fragment is lost I mean this depends a

2226
01:37:42,239 --> 01:37:43,560
little bit on the fragmentation screen

2227
01:37:43,560 --> 01:37:45,380
right if you consider for example

2228
01:37:45,380 --> 01:37:48,540
selective fragment recovery

2229
01:37:48,540 --> 01:37:50,580
um it doesn't matter too much whether

2230
01:37:50,580 --> 01:37:52,920
the fragment is lost or not for the

2231
01:37:52,920 --> 01:37:55,199
whole packet yeah so

2232
01:37:55,199 --> 01:37:57,119
um my understanding about the basics

2233
01:37:57,119 --> 01:37:58,619
load pan work at least the way it was

2234
01:37:58,619 --> 01:38:00,119
implemented in the operating systems we

2235
01:38:00,119 --> 01:38:01,739
looked at was indeed that if a fragment

2236
01:38:01,739 --> 01:38:04,020
is lost you lose the whole packet but I

2237
01:38:04,020 --> 01:38:05,699
do agree that there are protocols you

2238
01:38:05,699 --> 01:38:07,619
can use to recover a loss for Apple

2239
01:38:07,619 --> 01:38:09,420
without losing the entire packet and

2240
01:38:09,420 --> 01:38:10,800
those could also help with the problem

2241
01:38:10,800 --> 01:38:12,659
allowing you to make the packet bigger

2242
01:38:12,659 --> 01:38:17,000
and amortize TCP IP headers even better

2243
01:38:17,280 --> 01:38:20,219
cool all right hello

2244
01:38:20,219 --> 01:38:22,500
um Tommy Pauley from Apple thank you for

2245
01:38:22,500 --> 01:38:24,480
doing this talk I'm very

2246
01:38:24,480 --> 01:38:27,659
interesting I'm super happy to see the

2247
01:38:27,659 --> 01:38:30,239
use of TCP here um I just had a couple

2248
01:38:30,239 --> 01:38:34,020
questions from the presentation um

2249
01:38:34,020 --> 01:38:36,060
way earlier and you don't have to go

2250
01:38:36,060 --> 01:38:37,920
back when you're talking about

2251
01:38:37,920 --> 01:38:39,780
um the memory saving aspects and the

2252
01:38:39,780 --> 01:38:41,820
ability to have the flat

2253
01:38:41,820 --> 01:38:44,040
buffer you had the diagram there of you

2254
01:38:44,040 --> 01:38:45,719
know essentially here's

2255
01:38:45,719 --> 01:38:47,820
kind of what's in flight and then

2256
01:38:47,820 --> 01:38:50,400
there's the out of order bits

2257
01:38:50,400 --> 01:38:53,040
and there are gaps in there as well

2258
01:38:53,040 --> 01:38:54,659
um when you're doing this are you able

2259
01:38:54,659 --> 01:38:56,940
to essentially guarantee 100 of the time

2260
01:38:56,940 --> 01:38:58,260
that you'll never need to allocate

2261
01:38:58,260 --> 01:39:00,659
memory or is it like just most of the

2262
01:39:00,659 --> 01:39:01,860
time and then there would be a failover

2263
01:39:01,860 --> 01:39:03,540
case where you do need to have Dynamic

2264
01:39:03,540 --> 01:39:06,179
allocation uh that's a great question uh

2265
01:39:06,179 --> 01:39:07,560
we ensure that you never have your

2266
01:39:07,560 --> 01:39:09,480
dynamically allocated memory cool and

2267
01:39:09,480 --> 01:39:10,920
the way we do it is that you store the

2268
01:39:10,920 --> 01:39:12,900
data there you have a bitmap to keep

2269
01:39:12,900 --> 01:39:14,699
track of which bits contain the

2270
01:39:14,699 --> 01:39:16,619
out-of-order data but the bitmap can

2271
01:39:16,619 --> 01:39:18,300
also be sized statically because it

2272
01:39:18,300 --> 01:39:20,219
depends only on the array size which is

2273
01:39:20,219 --> 01:39:24,480
also static got it okay cool

2274
01:39:24,480 --> 01:39:26,400
um and then the other question is more

2275
01:39:26,400 --> 01:39:27,960
about kind of what you're ending with

2276
01:39:27,960 --> 01:39:30,480
talking about how you can use this to

2277
01:39:30,480 --> 01:39:33,900
get to internet hosts end to end um and

2278
01:39:33,900 --> 01:39:36,179
I believe in your tests

2279
01:39:36,179 --> 01:39:37,920
you were testing against

2280
01:39:37,920 --> 01:39:40,860
um end-to-end internet connections

2281
01:39:40,860 --> 01:39:43,699
for that do you need to modify anything

2282
01:39:43,699 --> 01:39:46,980
on the TCP implementation on the

2283
01:39:46,980 --> 01:39:48,420
internet servers like as we were

2284
01:39:48,420 --> 01:39:49,619
mentioning things like timing the

2285
01:39:49,619 --> 01:39:51,420
retransmit timing schedule so you want

2286
01:39:51,420 --> 01:39:52,980
to add Randomness so you're not

2287
01:39:52,980 --> 01:39:55,199
colliding

2288
01:39:55,199 --> 01:39:58,199
um is there something that needs tuning

2289
01:39:58,199 --> 01:40:01,500
on the internet hosts to make sure that

2290
01:40:01,500 --> 01:40:04,679
they are friendly to the low pan devices

2291
01:40:04,679 --> 01:40:07,800
or can you use completely unmodified

2292
01:40:07,800 --> 01:40:10,020
um internet hosts to talk to yeah that's

2293
01:40:10,020 --> 01:40:11,280
an excellent question and the short

2294
01:40:11,280 --> 01:40:13,199
answer is that the hosts on Linux side

2295
01:40:13,199 --> 01:40:15,600
were completely unmodified great

2296
01:40:15,600 --> 01:40:18,420
um I mean that's to say a little bit

2297
01:40:18,420 --> 01:40:20,280
more about that uh the timing that we

2298
01:40:20,280 --> 01:40:22,080
adjusted for like the randomized delay

2299
01:40:22,080 --> 01:40:23,699
was none of the TCP level it was at the

2300
01:40:23,699 --> 01:40:26,219
link layer so as a result the the other

2301
01:40:26,219 --> 01:40:27,780
side actually doesn't see any of that

2302
01:40:27,780 --> 01:40:29,760
got it um there's also one of the

2303
01:40:29,760 --> 01:40:31,920
advantages to us using a full scale TCP

2304
01:40:31,920 --> 01:40:33,540
stack like the one from FreeBSD because

2305
01:40:33,540 --> 01:40:34,800
it's been battle tested in the real

2306
01:40:34,800 --> 01:40:36,600
world and it's interoperable with all

2307
01:40:36,600 --> 01:40:38,400
the major TCP Stacks that are out there

2308
01:40:38,400 --> 01:40:40,320
and I just want to say that uh

2309
01:40:40,320 --> 01:40:41,940
interoperability is actually a problem

2310
01:40:41,940 --> 01:40:43,800
in the embedded space many of the

2311
01:40:43,800 --> 01:40:46,800
ability CPS tags you find are have inter

2312
01:40:46,800 --> 01:40:48,420
have interoperability problems in pretty

2313
01:40:48,420 --> 01:40:50,460
subtle ways with the real TCP Stacks

2314
01:40:50,460 --> 01:40:52,739
that are used and that's something we

2315
01:40:52,739 --> 01:40:54,600
managed to sidestep by using a battle

2316
01:40:54,600 --> 01:40:56,639
test the TCP implementation as the basis

2317
01:40:56,639 --> 01:41:00,250
of our study cool thank you

2318
01:41:00,250 --> 01:41:01,860
[Music]

2319
01:41:01,860 --> 01:41:04,080
so hello this is Thomas also from the

2320
01:41:04,080 --> 01:41:06,000
riot Community thanks again for this

2321
01:41:06,000 --> 01:41:09,239
work thanks for using Riot uh there's uh

2322
01:41:09,239 --> 01:41:11,699
another encouragement using Junior C

2323
01:41:11,699 --> 01:41:13,980
because you have a generic packet buffer

2324
01:41:13,980 --> 01:41:16,619
here which you could reuse that even

2325
01:41:16,619 --> 01:41:19,280
reduces your memory over it even further

2326
01:41:19,280 --> 01:41:24,000
uh just just a remark one question about

2327
01:41:24,000 --> 01:41:28,080
uh about your multi-hop experiments you

2328
01:41:28,080 --> 01:41:31,500
showed us nicely how by jittering the

2329
01:41:31,500 --> 01:41:34,800
the TCP forwarding how you could avoid

2330
01:41:34,800 --> 01:41:36,360
the hidden terminal problem

2331
01:41:36,360 --> 01:41:38,400
was that in a cleaner environment

2332
01:41:38,400 --> 01:41:41,219
without cross traffic with only a single

2333
01:41:41,219 --> 01:41:43,139
TCP connection

2334
01:41:43,139 --> 01:41:46,260
yeah so uh the hidden terminal problem

2335
01:41:46,260 --> 01:41:48,600
affects even a single TCP connection in

2336
01:41:48,600 --> 01:41:50,400
isolation

2337
01:41:50,400 --> 01:41:53,520
um and we verified that our randomized

2338
01:41:53,520 --> 01:41:54,659
back off

2339
01:41:54,659 --> 01:41:56,940
fixes the problem in that case yeah but

2340
01:41:56,940 --> 01:41:58,560
only in this case I mean the normal

2341
01:41:58,560 --> 01:42:00,119
cases that you have background traffic

2342
01:42:00,119 --> 01:42:02,820
right so yeah yeah so I mean if you have

2343
01:42:02,820 --> 01:42:04,440
background traffic this is also why we

2344
01:42:04,440 --> 01:42:06,000
used randomized delays instead of fixed

2345
01:42:06,000 --> 01:42:08,460
delays because if we had a randomized

2346
01:42:08,460 --> 01:42:09,719
back off it doesn't matter the

2347
01:42:09,719 --> 01:42:11,460
interference is coming from the same

2348
01:42:11,460 --> 01:42:13,920
stream or a different stream right in

2349
01:42:13,920 --> 01:42:15,300
both cases you'll back up a random

2350
01:42:15,300 --> 01:42:16,800
amount and hopefully transmit again

2351
01:42:16,800 --> 01:42:19,020
without colliding

2352
01:42:19,020 --> 01:42:20,820
um this is also why we did it without

2353
01:42:20,820 --> 01:42:22,380
because I mean there are several

2354
01:42:22,380 --> 01:42:24,600
protocols you could use that look at TCP

2355
01:42:24,600 --> 01:42:26,639
state in some way

2356
01:42:26,639 --> 01:42:28,800
um and having it just be a randomized

2357
01:42:28,800 --> 01:42:30,239
related link there gives us some

2358
01:42:30,239 --> 01:42:32,340
confidence that it would work across TCP

2359
01:42:32,340 --> 01:42:33,960
extremes and regardless of the source of

2360
01:42:33,960 --> 01:42:36,420
traffic whether it's TCP different TCP

2361
01:42:36,420 --> 01:42:38,400
streams or even something else

2362
01:42:38,400 --> 01:42:41,600
in this context did you also consider

2363
01:42:41,600 --> 01:42:44,820
experimenting with more flexible uh um

2364
01:42:44,820 --> 01:42:48,659
link layer Mech layers then just a csma

2365
01:42:48,659 --> 01:42:51,360
CA for instance the dsme Mac layer which

2366
01:42:51,360 --> 01:42:54,119
is also supported by riot no we didn't

2367
01:42:54,119 --> 01:42:55,860
experiment with that uh we looked at

2368
01:42:55,860 --> 01:42:57,480
csme because that was the most common

2369
01:42:57,480 --> 01:42:59,340
one supported across all the operating

2370
01:42:59,340 --> 01:43:01,500
systems and networking protocols that we

2371
01:43:01,500 --> 01:43:03,600
tried across tiny OS Riot and open

2372
01:43:03,600 --> 01:43:06,000
thread so it's the most natural to focus

2373
01:43:06,000 --> 01:43:06,960
on that

2374
01:43:06,960 --> 01:43:11,179
okay thank you thank you

2375
01:43:11,179 --> 01:43:13,619
all right I think I can't I think we

2376
01:43:13,619 --> 01:43:17,360
have a remote question

2377
01:43:19,260 --> 01:43:22,920
am I unmuted finally

2378
01:43:22,920 --> 01:43:24,900
so uh this is following up on the

2379
01:43:24,900 --> 01:43:27,900
multi-hop case uh so in these

2380
01:43:27,900 --> 01:43:30,239
environments the uh forwarding devices

2381
01:43:30,239 --> 01:43:33,480
are in fact also very low power low

2382
01:43:33,480 --> 01:43:35,820
um resource devices

2383
01:43:35,820 --> 01:43:38,219
um did you see or could you speculate on

2384
01:43:38,219 --> 01:43:40,800
what you might see as to whether TCP

2385
01:43:40,800 --> 01:43:43,739
traffic would have more stress on the

2386
01:43:43,739 --> 01:43:46,619
buffers of the forwarding multi-hop

2387
01:43:46,619 --> 01:43:48,719
Wireless nodes

2388
01:43:48,719 --> 01:43:51,179
us so that's a great question

2389
01:43:51,179 --> 01:43:53,460
um first I want to I mean so first I

2390
01:43:53,460 --> 01:43:55,619
just want to clarify that the buffers

2391
01:43:55,619 --> 01:43:57,480
use of the intermediate routers these

2392
01:43:57,480 --> 01:43:59,340
aren't TCP layer buffers is just like

2393
01:43:59,340 --> 01:44:00,840
the general packet buffers used for

2394
01:44:00,840 --> 01:44:01,980
forwarding because you know an

2395
01:44:01,980 --> 01:44:03,900
end-to-end TCP connection there's no TCP

2396
01:44:03,900 --> 01:44:06,840
State sure sure but it made for the

2397
01:44:06,840 --> 01:44:08,460
different may put a different low

2398
01:44:08,460 --> 01:44:10,679
aggregate load

2399
01:44:10,679 --> 01:44:13,199
um on those buffers then say Co-op

2400
01:44:13,199 --> 01:44:15,719
traffic or something that's more you

2401
01:44:15,719 --> 01:44:18,719
know simple request response related

2402
01:44:18,719 --> 01:44:21,179
yeah so I mean of course that's the case

2403
01:44:21,179 --> 01:44:22,500
that when you're transmitting at higher

2404
01:44:22,500 --> 01:44:23,639
bandwidth you're going to place some

2405
01:44:23,639 --> 01:44:25,500
more stress on the on the buffers of the

2406
01:44:25,500 --> 01:44:27,780
intermediate uh all of the intermediate

2407
01:44:27,780 --> 01:44:28,920
routers and there are a couple things

2408
01:44:28,920 --> 01:44:31,199
that that we do in that we actually did

2409
01:44:31,199 --> 01:44:32,820
in our study in order to help mitigate

2410
01:44:32,820 --> 01:44:34,860
that the first one is that we added some

2411
01:44:34,860 --> 01:44:36,540
active queue management functionality to

2412
01:44:36,540 --> 01:44:38,340
those intermediate routers where you

2413
01:44:38,340 --> 01:44:40,500
mark packets as congested using explicit

2414
01:44:40,500 --> 01:44:42,360
using explicit condition notification

2415
01:44:42,360 --> 01:44:44,820
and so on in order to prevent TCP from

2416
01:44:44,820 --> 01:44:46,500
filling up the entire buffer and keeping

2417
01:44:46,500 --> 01:44:48,300
your cues short

2418
01:44:48,300 --> 01:44:50,100
um the primary reason we did this was to

2419
01:44:50,100 --> 01:44:52,199
improve fairness of different TCP flows

2420
01:44:52,199 --> 01:44:53,699
that are competing for buffer space of

2421
01:44:53,699 --> 01:44:55,679
these intermediate routers and also to

2422
01:44:55,679 --> 01:44:57,900
reduce the and also reduce the latency

2423
01:44:57,900 --> 01:45:00,600
of traffic but it also has a side effect

2424
01:45:00,600 --> 01:45:02,520
of limiting the amount of buffer space

2425
01:45:02,520 --> 01:45:04,619
that's being used by a single TCP flow

2426
01:45:04,619 --> 01:45:06,239
to address some of the concerns that you

2427
01:45:06,239 --> 01:45:08,280
brought up

2428
01:45:08,280 --> 01:45:11,760
thanks I was looking for the aqm uh

2429
01:45:11,760 --> 01:45:14,360
angle on that

2430
01:45:17,159 --> 01:45:22,080
all right uh so I have a question

2431
01:45:22,080 --> 01:45:26,219
um does the uh I I I very much like the

2432
01:45:26,219 --> 01:45:29,820
idea of the head of multiple link their

2433
01:45:29,820 --> 01:45:30,960
frames

2434
01:45:30,960 --> 01:45:33,119
um does this put any constraints on the

2435
01:45:33,119 --> 01:45:35,219
link there or or does the six Loop

2436
01:45:35,219 --> 01:45:36,300
Handler

2437
01:45:36,300 --> 01:45:39,659
um so handle all of that

2438
01:45:39,659 --> 01:45:41,520
um that's a great question so some of

2439
01:45:41,520 --> 01:45:43,500
these uh can potentially be handled at

2440
01:45:43,500 --> 01:45:45,659
the sixth low pan layer but others do

2441
01:45:45,659 --> 01:45:48,840
indeed have to do with with the uh with

2442
01:45:48,840 --> 01:45:50,340
the link layer directly like for example

2443
01:45:50,340 --> 01:45:53,159
the randomized delayed that we added to

2444
01:45:53,159 --> 01:45:55,199
avoid hidden terminals is something that

2445
01:45:55,199 --> 01:45:56,820
would operate at the link layer right

2446
01:45:56,820 --> 01:45:58,860
because of the six little band layer uh

2447
01:45:58,860 --> 01:46:00,719
you don't have or at least you don't

2448
01:46:00,719 --> 01:46:01,980
naturally have the same kind of

2449
01:46:01,980 --> 01:46:04,199
visibility into you know when your link

2450
01:46:04,199 --> 01:46:05,880
layer acts are coming in and so on

2451
01:46:05,880 --> 01:46:07,500
whereas you would need that to determine

2452
01:46:07,500 --> 01:46:09,540
uh that a transmission failed and how

2453
01:46:09,540 --> 01:46:11,159
much to back off on the re-transmission

2454
01:46:11,159 --> 01:46:13,199
and so on so some of them do indeed

2455
01:46:13,199 --> 01:46:15,060
affect the link layer

2456
01:46:15,060 --> 01:46:17,159
yeah other requirements that the link

2457
01:46:17,159 --> 01:46:19,440
layer delivers packet in order

2458
01:46:19,440 --> 01:46:22,400
um to avoid um damaging the headers or

2459
01:46:22,400 --> 01:46:25,739
os6 weapon handling

2460
01:46:25,739 --> 01:46:27,840
uh sorry I didn't understand your

2461
01:46:27,840 --> 01:46:28,980
question

2462
01:46:28,980 --> 01:46:32,460
is there a requirement that the links

2463
01:46:32,460 --> 01:46:35,400
packets in order in order because of the

2464
01:46:35,400 --> 01:46:37,320
way you've sent the head of split across

2465
01:46:37,320 --> 01:46:39,540
multiple link their frames or is that

2466
01:46:39,540 --> 01:46:41,400
all that the reordering handles by

2467
01:46:41,400 --> 01:46:44,219
sixfloper oh yeah so the reordering and

2468
01:46:44,219 --> 01:46:46,320
reassembly is handled by six low pan and

2469
01:46:46,320 --> 01:46:48,060
there's no strict requirement that you

2470
01:46:48,060 --> 01:46:49,860
have to transmit the frames of a packet

2471
01:46:49,860 --> 01:46:51,239
directly one after the other

2472
01:46:51,239 --> 01:46:53,340
consecutively in fact one of the things

2473
01:46:53,340 --> 01:46:54,719
that I skipped because of the time limit

2474
01:46:54,719 --> 01:46:57,239
was another set of techniques we have uh

2475
01:46:57,239 --> 01:47:00,000
at the level of managing how to deal

2476
01:47:00,000 --> 01:47:02,280
with concurrent frames basically how to

2477
01:47:02,280 --> 01:47:03,659
schedule frames when you're some of them

2478
01:47:03,659 --> 01:47:04,800
are going through other wall power

2479
01:47:04,800 --> 01:47:06,239
devices some of the battery powered

2480
01:47:06,239 --> 01:47:08,760
devices and in effect what we do is if

2481
01:47:08,760 --> 01:47:10,500
you if you receive a data requesting

2482
01:47:10,500 --> 01:47:12,540
from a battery powered device then you

2483
01:47:12,540 --> 01:47:14,460
prioritize sending frames to Tech in

2484
01:47:14,460 --> 01:47:16,260
order to reduce its duty cycle and let

2485
01:47:16,260 --> 01:47:18,780
it go to sleep as fast as possible and

2486
01:47:18,780 --> 01:47:20,460
that's one case where we specifically

2487
01:47:20,460 --> 01:47:23,159
might interrupt another transmission and

2488
01:47:23,159 --> 01:47:25,020
not send its frames concurrently I mean

2489
01:47:25,020 --> 01:47:28,639
nothing streams consecutively

2490
01:47:30,239 --> 01:47:32,960
oh okay yeah that makes sense

2491
01:47:32,960 --> 01:47:36,860
uh Gabrielle

2492
01:47:38,639 --> 01:47:41,060
me

2493
01:47:41,159 --> 01:47:43,199
one

2494
01:47:43,199 --> 01:47:45,119
okay

2495
01:47:45,119 --> 01:47:47,280
um yeah thank you very much for for this

2496
01:47:47,280 --> 01:47:50,159
work this is great stuff

2497
01:47:50,159 --> 01:47:54,840
um I did have a comment on um the

2498
01:47:54,840 --> 01:47:57,540
comparison with Co-op I think the

2499
01:47:57,540 --> 01:48:01,820
justification for Co-Op was not entirely

2500
01:48:01,820 --> 01:48:05,280
based on we can't use TCP type type

2501
01:48:05,280 --> 01:48:07,560
thing it was more based on we can't use

2502
01:48:07,560 --> 01:48:10,260
HTTP because of the justification for it

2503
01:48:10,260 --> 01:48:11,460
was for

2504
01:48:11,460 --> 01:48:13,560
folks who wanted to use a restful

2505
01:48:13,560 --> 01:48:15,480
interface for the application layer not

2506
01:48:15,480 --> 01:48:17,820
every application of the year in iot

2507
01:48:17,820 --> 01:48:20,280
wishes to do that but there's certainly

2508
01:48:20,280 --> 01:48:24,780
a lot of a lot of incentive to use

2509
01:48:24,780 --> 01:48:30,800
restful so when when the restful folks

2510
01:48:30,800 --> 01:48:32,900
started uh

2511
01:48:32,900 --> 01:48:35,760
to become interested in iot the only

2512
01:48:35,760 --> 01:48:39,480
alternative was http11 which uh I

2513
01:48:39,480 --> 01:48:42,239
completely agree is terrible uh it's my

2514
01:48:42,239 --> 01:48:44,460
Sardar it's textual based protocol you

2515
01:48:44,460 --> 01:48:46,800
cannot compress it it's it's very

2516
01:48:46,800 --> 01:48:50,400
verbose Etc it's it's terrible

2517
01:48:50,400 --> 01:48:52,619
um we subsequently had HTTP 2 which

2518
01:48:52,619 --> 01:48:54,239
became a binary protocol and we actually

2519
01:48:54,239 --> 01:48:55,560
had a paper

2520
01:48:55,560 --> 01:48:58,739
uh three years ago in an RW about you

2521
01:48:58,739 --> 01:49:00,119
know how to use that over something like

2522
01:49:00,119 --> 01:49:02,760
six low pen for example uh some just

2523
01:49:02,760 --> 01:49:04,679
initial scratching the surface but now

2524
01:49:04,679 --> 01:49:07,199
we have HTTP 3 and quick and it's all

2525
01:49:07,199 --> 01:49:08,760
binary so

2526
01:49:08,760 --> 01:49:12,659
um and I um I understand you you guys

2527
01:49:12,659 --> 01:49:14,699
haven't had a chance to go after the

2528
01:49:14,699 --> 01:49:16,020
excellent work you've done to look at

2529
01:49:16,020 --> 01:49:17,099
those layers but I would highly

2530
01:49:17,099 --> 01:49:19,020
encourage you to do that because that

2531
01:49:19,020 --> 01:49:20,699
would that would address

2532
01:49:20,699 --> 01:49:23,880
um a significant portion of the of the

2533
01:49:23,880 --> 01:49:26,580
application layer incentives

2534
01:49:26,580 --> 01:49:30,719
um to um for iot as well

2535
01:49:30,719 --> 01:49:33,480
yeah so uh so thanks for for clarifying

2536
01:49:33,480 --> 01:49:34,440
that

2537
01:49:34,440 --> 01:49:36,360
um I do acknowledge that Co-op has has

2538
01:49:36,360 --> 01:49:37,920
evolved quite a bit in a few years some

2539
01:49:37,920 --> 01:49:40,020
of those Evolutions happen after after

2540
01:49:40,020 --> 01:49:43,199
we published this work uh but I do want

2541
01:49:43,199 --> 01:49:46,679
to uh to clarify my position on Co-op a

2542
01:49:46,679 --> 01:49:48,540
little bit uh based on what you said

2543
01:49:48,540 --> 01:49:50,699
it's that uh indeed I think that Co-op

2544
01:49:50,699 --> 01:49:52,619
is useful and it has its uses and it's

2545
01:49:52,619 --> 01:49:54,239
very flexible it's been evolving a lot

2546
01:49:54,239 --> 01:49:56,400
over the years and that's great

2547
01:49:56,400 --> 01:49:58,739
um I do I mean I have noticed that Co-op

2548
01:49:58,739 --> 01:50:00,900
has been evolving in some sense more and

2549
01:50:00,900 --> 01:50:02,219
more towards the same kind of

2550
01:50:02,219 --> 01:50:04,199
abstraction that TCP provides right in

2551
01:50:04,199 --> 01:50:06,300
some sense uh the ability like for

2552
01:50:06,300 --> 01:50:07,860
example with some of the recent work on

2553
01:50:07,860 --> 01:50:09,960
on streaming on streaming block

2554
01:50:09,960 --> 01:50:11,940
transfers and so on

2555
01:50:11,940 --> 01:50:13,500
um all I'm saying here is that I think

2556
01:50:13,500 --> 01:50:15,119
that an application that's built on

2557
01:50:15,119 --> 01:50:17,580
co-app in these kinds of networks with

2558
01:50:17,580 --> 01:50:19,139
all the latest features like for example

2559
01:50:19,139 --> 01:50:20,760
the ability to have multiple blocks in

2560
01:50:20,760 --> 01:50:23,340
Flight uh concurrently and so on would

2561
01:50:23,340 --> 01:50:25,139
also be wise to potentially consider

2562
01:50:25,139 --> 01:50:28,139
using TCP directly itself given that TCP

2563
01:50:28,139 --> 01:50:29,520
is also a viable option in these

2564
01:50:29,520 --> 01:50:31,880
Networks

2565
01:50:32,099 --> 01:50:35,420
but thanks for that comment

2566
01:50:36,179 --> 01:50:38,940
all right thank you uh one more question

2567
01:50:38,940 --> 01:50:43,159
uh Benjamin uh

2568
01:50:44,099 --> 01:50:45,599
hi thank you so much for the

2569
01:50:45,599 --> 01:50:47,699
presentation I really appreciate it

2570
01:50:47,699 --> 01:50:49,619
um I was just wondering you talked

2571
01:50:49,619 --> 01:50:52,619
mainly about the applications of this in

2572
01:50:52,619 --> 01:50:54,599
um in lands do you see any application

2573
01:50:54,599 --> 01:50:57,480
for longer range networks like um like

2574
01:50:57,480 --> 01:50:59,520
mobile ad hoc networks or anything of

2575
01:50:59,520 --> 01:51:01,260
that sort

2576
01:51:01,260 --> 01:51:04,500
uh that's a great question so all of our

2577
01:51:04,500 --> 01:51:07,320
experimentation was uh was done using

2578
01:51:07,320 --> 01:51:10,139
iFly 82.15-4 which is a personal area

2579
01:51:10,139 --> 01:51:12,480
network protocol and that was motivated

2580
01:51:12,480 --> 01:51:14,520
by the recent interest in it in adopting

2581
01:51:14,520 --> 01:51:16,860
some of that technology uh to work in

2582
01:51:16,860 --> 01:51:19,260
the smart home and iot space

2583
01:51:19,260 --> 01:51:21,480
um some of the I mean some of these

2584
01:51:21,480 --> 01:51:22,800
lessons might carry you over to the

2585
01:51:22,800 --> 01:51:25,139
mobile and and ad hoc network space like

2586
01:51:25,139 --> 01:51:27,300
LP Vans and so on

2587
01:51:27,300 --> 01:51:28,739
um I'm not sure I'll be able to tell you

2588
01:51:28,739 --> 01:51:30,119
any specifics given that I don't have

2589
01:51:30,119 --> 01:51:32,520
much experience with those networks

2590
01:51:32,520 --> 01:51:34,980
um but I mean by first gut would be

2591
01:51:34,980 --> 01:51:37,139
there's probably a way to make TCP work

2592
01:51:37,139 --> 01:51:38,520
well given that it's been adapted to

2593
01:51:38,520 --> 01:51:39,840
work in so many different kinds of

2594
01:51:39,840 --> 01:51:42,179
networks uh in all kinds of different

2595
01:51:42,179 --> 01:51:44,340
environments but other than that I'm not

2596
01:51:44,340 --> 01:51:45,960
sure if any of this which of the

2597
01:51:45,960 --> 01:51:47,460
specific techniques would directly carry

2598
01:51:47,460 --> 01:51:49,679
over there

2599
01:51:49,679 --> 01:51:52,639
thank you so much

2600
01:51:57,300 --> 01:52:02,000
all right uh thank you very much

2601
01:52:02,000 --> 01:52:04,170
excellent talk

2602
01:52:04,170 --> 01:52:08,219
[Applause]

2603
01:52:08,219 --> 01:52:11,820
uh and thank you again to to both of the

2604
01:52:11,820 --> 01:52:13,500
speakers I think that there were two two

2605
01:52:13,500 --> 01:52:15,480
really great talks there

2606
01:52:15,480 --> 01:52:19,619
um both uh Sam and Tasha will be around

2607
01:52:19,619 --> 01:52:21,420
all week uh I'm sure they'll be very

2608
01:52:21,420 --> 01:52:23,639
happy to to talk with people more about

2609
01:52:23,639 --> 01:52:27,119
their work uh so please do do you find

2610
01:52:27,119 --> 01:52:28,860
them have a chat chat about their work

2611
01:52:28,860 --> 01:52:31,679
make them welcome to the the ietf and to

2612
01:52:31,679 --> 01:52:36,119
the irtf uh congratulations both to uh

2613
01:52:36,119 --> 01:52:39,060
Simon satosha for the award of the anrp

2614
01:52:39,060 --> 01:52:40,679
this time

2615
01:52:40,679 --> 01:52:43,619
um as I said earlier looking up for

2616
01:52:43,619 --> 01:52:46,020
um more anrp award talks

2617
01:52:46,020 --> 01:52:49,380
um at the the uh itf15 in London in

2618
01:52:49,380 --> 01:52:52,800
November uh the nominations for the uh

2619
01:52:52,800 --> 01:52:55,920
2023 uh nip Awards will be opening in

2620
01:52:55,920 --> 01:52:58,619
September so if you know any good work

2621
01:52:58,619 --> 01:53:01,080
please think about nominating that work

2622
01:53:01,080 --> 01:53:03,300
and look out for the uh applied

2623
01:53:03,300 --> 01:53:05,159
networking research Workshop which is

2624
01:53:05,159 --> 01:53:07,679
taking place uh co-locating with the ITF

2625
01:53:07,679 --> 01:53:10,440
in Philadelphia tomorrow

2626
01:53:10,440 --> 01:53:12,300
um thank you again everybody

2627
01:53:12,300 --> 01:53:15,540
um hopefully I will see some oral of you

2628
01:53:15,540 --> 01:53:18,900
in London or at the nrw tomorrow or

2629
01:53:18,900 --> 01:53:20,280
later this week

2630
01:53:20,280 --> 01:53:22,880
thanks everyone

