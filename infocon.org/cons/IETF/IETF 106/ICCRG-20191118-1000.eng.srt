1
00:00:04,590 --> 00:00:32,099
a good morning good people can you hear

2
00:00:26,400 --> 00:00:35,220
me I can hear myself just not a good

3
00:00:32,098 --> 00:00:39,870
thing anyways good morning and welcome

4
00:00:35,220 --> 00:00:42,600
to the IETF and welcome to IC crg if you

5
00:00:39,870 --> 00:00:45,180
are here if you don't believe you're

6
00:00:42,600 --> 00:00:46,650
here for IC crg stay on anyways this

7
00:00:45,180 --> 00:00:52,200
might be more interesting than anything

8
00:00:46,650 --> 00:00:55,580
else you are going to go to before we

9
00:00:52,200 --> 00:00:59,360
start I need the standard things I need

10
00:00:55,580 --> 00:01:11,310
jabber scribe and I need a minute taker

11
00:00:59,360 --> 00:01:13,460
volunteers come on I can start naming

12
00:01:11,310 --> 00:01:13,460
names

13
00:01:17,570 --> 00:01:21,869
Gouri you're looking at me like you want

14
00:01:20,219 --> 00:01:24,809
to do some Janna I'll take the minutes

15
00:01:21,869 --> 00:01:28,770
but I don't have a jarek on what do you

16
00:01:24,810 --> 00:01:30,030
say Andrew I see it I'll take the

17
00:01:28,770 --> 00:01:32,130
minutes but I don't have a jabber client

18
00:01:30,030 --> 00:01:36,060
so someone needs to JavaScript Thank You

19
00:01:32,130 --> 00:01:37,229
Andrews gonna do minutes jabber thank

20
00:01:36,060 --> 00:01:38,999
you sir

21
00:01:37,229 --> 00:01:42,270
so we've got a jabber scribe and we've

22
00:01:38,999 --> 00:01:50,280
got minute minute taker so we can move

23
00:01:42,270 --> 00:01:53,639
on so we have a pretty packed agenda let

24
00:01:50,280 --> 00:01:56,999
me see how do I do this but before we

25
00:01:53,639 --> 00:02:01,109
start we will just briefly I will flash

26
00:01:56,999 --> 00:02:03,600
you the note well very very briefly we

27
00:02:01,109 --> 00:02:05,699
follow the IETF IPR disclosure rules and

28
00:02:03,600 --> 00:02:08,429
if you do not know what the note well is

29
00:02:05,700 --> 00:02:12,229
you should go read it before you come up

30
00:02:08,429 --> 00:02:12,229
to the mic and say anything at all

31
00:02:12,709 --> 00:02:17,430
there's also privacy and code of conduct

32
00:02:15,239 --> 00:02:18,090
which again I will not go into right now

33
00:02:17,430 --> 00:02:19,650
but

34
00:02:18,090 --> 00:02:21,540
you're welcome to read it their slides

35
00:02:19,650 --> 00:02:24,120
are available online and the code of

36
00:02:21,540 --> 00:02:27,840
conduct is available online at that URL

37
00:02:24,120 --> 00:02:29,910
and the goal of the IRT F just to be

38
00:02:27,840 --> 00:02:31,769
very clear we do research we don't do

39
00:02:29,910 --> 00:02:33,720
standards development so if you are here

40
00:02:31,769 --> 00:02:35,160
to push a standard Strack RFC or on the

41
00:02:33,720 --> 00:02:36,739
wrong room

42
00:02:35,160 --> 00:02:40,470
but if you are here to hear about

43
00:02:36,739 --> 00:02:43,010
research ideas and not just sound let's

44
00:02:40,470 --> 00:02:45,750
work then you're on the right rope and

45
00:02:43,010 --> 00:02:47,310
there's our agenda for today so we're

46
00:02:45,750 --> 00:02:51,060
gonna basically we have a fairly packed

47
00:02:47,310 --> 00:02:53,069
agenda and I'm not going to walk through

48
00:02:51,060 --> 00:02:57,299
the entire agenda but we're gonna try to

49
00:02:53,069 --> 00:03:01,560
get done on time here so to the speakers

50
00:02:57,299 --> 00:03:03,120
please come up just as your talk is

51
00:03:01,560 --> 00:03:07,829
about thought and please make sure to

52
00:03:03,120 --> 00:03:10,079
stick to the allotted time and that

53
00:03:07,829 --> 00:03:15,150
includes me so I'm going to go through

54
00:03:10,079 --> 00:03:17,970
this fairly quickly the first thing to

55
00:03:15,150 --> 00:03:20,549
note here is we have to site meetings we

56
00:03:17,970 --> 00:03:22,170
couldn't really bring them into this

57
00:03:20,549 --> 00:03:24,510
meeting but they are very relevant and

58
00:03:22,170 --> 00:03:27,059
you may be interested in them so do go

59
00:03:24,510 --> 00:03:29,160
to them if you think you are interested

60
00:03:27,060 --> 00:03:31,310
the first one is on data center

61
00:03:29,160 --> 00:03:33,420
condition control and Paul Congdon is

62
00:03:31,310 --> 00:03:37,530
organizing this Paul's there raising his

63
00:03:33,420 --> 00:03:39,238
hand and please ask him speak with him

64
00:03:37,530 --> 00:03:42,090
if you have any questions otherwise the

65
00:03:39,239 --> 00:03:44,940
those details are sill correct Paul okay

66
00:03:42,090 --> 00:03:47,280
lovely thank you so that's tomorrow

67
00:03:44,940 --> 00:03:50,549
morning in room VIP a early in the

68
00:03:47,280 --> 00:03:53,940
morning the side meeting time slot and

69
00:03:50,549 --> 00:03:57,540
then you have l4s on Thursday in this

70
00:03:53,940 --> 00:03:59,790
very room at the same time at 8:30 in

71
00:03:57,540 --> 00:04:03,298
the morning Bob are those details still

72
00:03:59,790 --> 00:04:08,489
correct so this will basically be an

73
00:04:03,299 --> 00:04:10,769
update on l4 s yes o l4 s and okay TCP

74
00:04:08,489 --> 00:04:13,440
Prague it's about TCP Prague not a

75
00:04:10,769 --> 00:04:17,478
bottle for s not just sell for us yeah

76
00:04:13,440 --> 00:04:23,280
okay but itself for SNP CP Prague euro

77
00:04:17,478 --> 00:04:28,500
the details are that the l4s is mostly

78
00:04:23,280 --> 00:04:32,000
in the th vwg meeting on 3rd on Thursday

79
00:04:28,500 --> 00:04:33,960
this side meeting is mostly TCP Prague

80
00:04:32,000 --> 00:04:37,560
fair enough thank you for that

81
00:04:33,960 --> 00:04:40,409
clarification David and that's happening

82
00:04:37,560 --> 00:04:44,760
Thursday in the morning so again these

83
00:04:40,410 --> 00:04:46,590
are very relevant to ICC RG so we just

84
00:04:44,760 --> 00:04:49,230
don't have enough time in here to do do

85
00:04:46,590 --> 00:04:51,150
all of these conversations so these side

86
00:04:49,230 --> 00:04:56,460
meetings are you're encouraged to go to

87
00:04:51,150 --> 00:04:58,710
them I will be there and with that I'm

88
00:04:56,460 --> 00:05:01,430
gonna start off on just a one topic I'm

89
00:04:58,710 --> 00:05:05,390
gonna spend at most five minutes on this

90
00:05:01,430 --> 00:05:10,440
we had started a conversation last time

91
00:05:05,390 --> 00:05:15,599
how to do this beautiful fit the page

92
00:05:10,440 --> 00:05:19,050
maybe okay that works we had I had

93
00:05:15,600 --> 00:05:21,600
broached the topic of doing documents in

94
00:05:19,050 --> 00:05:23,580
IC crg publishing documents through ICC

95
00:05:21,600 --> 00:05:25,890
RG adopting them in ICC RG

96
00:05:23,580 --> 00:05:30,150
I've had a number of conversations with

97
00:05:25,890 --> 00:05:32,430
various folks since then and I'm

98
00:05:30,150 --> 00:05:34,440
basically here to say now that we will

99
00:05:32,430 --> 00:05:37,200
start adopting documents in this

100
00:05:34,440 --> 00:05:40,410
research group I intend to use

101
00:05:37,200 --> 00:05:44,909
discretion and the RG is guidance in

102
00:05:40,410 --> 00:05:47,550
proposing what documents makes sense but

103
00:05:44,910 --> 00:05:49,740
that's the the goal is to adopt and

104
00:05:47,550 --> 00:05:51,660
publish documents here we've done this

105
00:05:49,740 --> 00:05:53,970
way back in the past I'm gonna start

106
00:05:51,660 --> 00:05:57,030
doing it again what does it mean to

107
00:05:53,970 --> 00:05:58,500
adopt documents well broadly the

108
00:05:57,030 --> 00:06:01,469
document is the reasonable fit for what

109
00:05:58,500 --> 00:06:03,360
we do in this room and by what I what I

110
00:06:01,470 --> 00:06:05,040
mean by what we do in this room is not

111
00:06:03,360 --> 00:06:07,530
just if it fits the letter of the

112
00:06:05,040 --> 00:06:09,150
Charter but if there's energy in the

113
00:06:07,530 --> 00:06:11,969
room and if this is the right sort of

114
00:06:09,150 --> 00:06:14,940
audience to do this work or look at

115
00:06:11,970 --> 00:06:16,700
these documents typically any condition

116
00:06:14,940 --> 00:06:18,750
control other fits in internet-like

117
00:06:16,700 --> 00:06:21,539
environments is an absolutely wonderful

118
00:06:18,750 --> 00:06:23,700
fit for example but we need to see

119
00:06:21,540 --> 00:06:25,110
author engagement we need to see author

120
00:06:23,700 --> 00:06:26,159
engagement with the research group and

121
00:06:25,110 --> 00:06:29,370
to show that there's interested in

122
00:06:26,160 --> 00:06:32,010
continuing that engagement to be adopted

123
00:06:29,370 --> 00:06:33,780
in this research group and again I

124
00:06:32,010 --> 00:06:35,099
intend to use discussion and research

125
00:06:33,780 --> 00:06:38,340
group guidance in determining relevance

126
00:06:35,100 --> 00:06:40,260
here what does it mean to publish a

127
00:06:38,340 --> 00:06:41,580
document it just means that the research

128
00:06:40,260 --> 00:06:45,150
of things the document is in good enough

129
00:06:41,580 --> 00:06:45,479
shape yes this doesn't seem like a very

130
00:06:45,150 --> 00:06:47,250
firm

131
00:06:45,480 --> 00:06:48,810
process but that's by design this was

132
00:06:47,250 --> 00:06:52,350
not intended to be a heavy process this

133
00:06:48,810 --> 00:06:54,240
is not the ietf again I expect to call

134
00:06:52,350 --> 00:06:57,300
upon the RG folks in the RG for reviews

135
00:06:54,240 --> 00:06:58,410
and for other decisions and we will

136
00:06:57,300 --> 00:07:01,470
figure this out as we go

137
00:06:58,410 --> 00:07:04,440
there's I have faith in the community

138
00:07:01,470 --> 00:07:06,090
here that we can make this work

139
00:07:04,440 --> 00:07:07,410
I feel great saying this by the way this

140
00:07:06,090 --> 00:07:10,020
is a good medium to say this because I

141
00:07:07,410 --> 00:07:11,820
feel like I'm I'm handing off this I'm

142
00:07:10,020 --> 00:07:19,169
doing this thing from upon high looking

143
00:07:11,820 --> 00:07:20,760
down at all of you I will decide but

144
00:07:19,170 --> 00:07:24,170
this is this is the plan going forward

145
00:07:20,760 --> 00:07:26,670
is to publish documents in ICC RG and

146
00:07:24,170 --> 00:07:27,960
starting off I'm gonna propose adopting

147
00:07:26,670 --> 00:07:28,770
the following documents we will hear

148
00:07:27,960 --> 00:07:33,450
about them today

149
00:07:28,770 --> 00:07:35,340
the first one is led back plus plus the

150
00:07:33,450 --> 00:07:37,320
second one is our LED bat if you haven't

151
00:07:35,340 --> 00:07:39,690
read these documents please do I think

152
00:07:37,320 --> 00:07:42,630
they are fairly good and I think

153
00:07:39,690 --> 00:07:44,730
laid-back + + s is actually I'd say it's

154
00:07:42,630 --> 00:07:47,490
it's almost an update to let that and

155
00:07:44,730 --> 00:07:50,670
it's it's a very important update to

156
00:07:47,490 --> 00:07:52,230
like that itself so I'm going to propose

157
00:07:50,670 --> 00:07:55,110
this I will also send this out on the

158
00:07:52,230 --> 00:07:58,440
mailing list and we'll adopt it when

159
00:07:55,110 --> 00:08:00,390
unless I hear somebody yelling and

160
00:07:58,440 --> 00:08:05,190
saying this is a terrible idea that's

161
00:08:00,390 --> 00:08:07,530
the plan going forward with that well

162
00:08:05,190 --> 00:08:14,880
any any quick thoughts I give you 30

163
00:08:07,530 --> 00:08:18,239
seconds and your 30 seconds are up

164
00:08:14,880 --> 00:08:21,590
Thank You Corey sorry go on Corey first

165
00:08:18,240 --> 00:08:21,590
I'm cold account real quick

166
00:08:21,620 --> 00:08:25,560
III think this is a good idea

167
00:08:23,730 --> 00:08:28,140
I like this and these two these two

168
00:08:25,560 --> 00:08:29,790
documents fit well because they are less

169
00:08:28,140 --> 00:08:31,890
they are more conservative in other

170
00:08:29,790 --> 00:08:33,360
transports therefore they definitely

171
00:08:31,890 --> 00:08:35,340
fall within something that could be done

172
00:08:33,360 --> 00:08:37,349
in a research group other things we

173
00:08:35,340 --> 00:08:38,700
don't have to tread carefully on and I

174
00:08:37,349 --> 00:08:40,800
guess we'll figure it out as we go on

175
00:08:38,700 --> 00:08:43,080
but these two documents seem like good

176
00:08:40,799 --> 00:08:44,370
candidates for here to me yeah I think

177
00:08:43,080 --> 00:08:49,340
we will have to figure this out as we go

178
00:08:44,370 --> 00:08:52,440
so question is the Martin Duke f5 is the

179
00:08:49,340 --> 00:08:54,480
desired outcome of these documents to

180
00:08:52,440 --> 00:08:56,660
produce informational RFC s or something

181
00:08:54,480 --> 00:08:56,660
else

182
00:08:57,180 --> 00:09:02,069
I yes I would think so I don't know can

183
00:09:00,660 --> 00:09:05,850
we publish experimental a lot of C's

184
00:09:02,070 --> 00:09:09,060
here anybody no we can't do that here

185
00:09:05,850 --> 00:09:11,910
right who says we can Andrew says we can

186
00:09:09,060 --> 00:09:14,880
I come on Perkins yes you can publish

187
00:09:11,910 --> 00:09:16,260
experimental or informational RFC Thank

188
00:09:14,880 --> 00:09:19,080
You Colin that's the voice of authority

189
00:09:16,260 --> 00:09:22,050
there so yeah I would I'd be interested

190
00:09:19,080 --> 00:09:25,800
in experimental but that's what I'm

191
00:09:22,050 --> 00:09:29,780
gonna be pushing folks for anyways thank

192
00:09:25,800 --> 00:09:32,520
you for that with that I am going to

193
00:09:29,780 --> 00:09:33,120
move on with the agenda again this is

194
00:09:32,520 --> 00:09:37,260
the agenda

195
00:09:33,120 --> 00:09:41,130
pravin Europe first you're gonna talk

196
00:09:37,260 --> 00:09:54,330
about that bed plus plus and I'm gonna

197
00:09:41,130 --> 00:10:11,700
give the slides up morning everyone can

198
00:09:54,330 --> 00:10:13,560
you hear me morning everyone I'm here to

199
00:10:11,700 --> 00:10:15,090
talk about led by plus-plus this is now

200
00:10:13,560 --> 00:10:17,400
a draft if you haven't read the draft

201
00:10:15,090 --> 00:10:19,740
there's version zero one that was

202
00:10:17,400 --> 00:10:22,439
published recently so please read it

203
00:10:19,740 --> 00:10:24,480
LED bed plus plus is a less than best

204
00:10:22,440 --> 00:10:26,790
effort congestion control so the goal is

205
00:10:24,480 --> 00:10:29,100
to use unused capacity bus but quickly

206
00:10:26,790 --> 00:10:31,709
yield to for ground flows example

207
00:10:29,100 --> 00:10:34,020
workloads include operating system

208
00:10:31,710 --> 00:10:36,840
application after it updates or backup

209
00:10:34,020 --> 00:10:38,960
replicas sort of burglars next slide

210
00:10:36,840 --> 00:10:38,960
please

211
00:10:43,290 --> 00:10:49,380
slide is a summary of led by plasmus the

212
00:10:46,740 --> 00:10:51,690
original RFC 68 17 has a bunch of

213
00:10:49,380 --> 00:10:53,670
problems which were found in prior

214
00:10:51,690 --> 00:10:55,110
research as well as in our efforts to

215
00:10:53,670 --> 00:10:58,199
implement the RFC in the Windows

216
00:10:55,110 --> 00:10:59,760
operating system I'll quickly walk

217
00:10:58,200 --> 00:11:01,950
through the problems and the mechanisms

218
00:10:59,760 --> 00:11:03,810
that had bad plus plus proposes to fix

219
00:11:01,950 --> 00:11:06,150
those problems one way delay

220
00:11:03,810 --> 00:11:08,670
measurements are hard in practice for

221
00:11:06,150 --> 00:11:10,770
example in TCP the clock frequency is

222
00:11:08,670 --> 00:11:12,780
not known of the pier as well as there

223
00:11:10,770 --> 00:11:15,300
is a clock drift problem let my plus

224
00:11:12,780 --> 00:11:17,750
plus users RTT there's a downside which

225
00:11:15,300 --> 00:11:20,430
means on the downlink you may measure

226
00:11:17,750 --> 00:11:21,780
congestion and react to it but because

227
00:11:20,430 --> 00:11:24,560
this is less than best effort in

228
00:11:21,780 --> 00:11:26,730
practice this doesn't cause problems

229
00:11:24,560 --> 00:11:28,680
there's a late comer advantage problem

230
00:11:26,730 --> 00:11:31,170
if there are multiple flows already in

231
00:11:28,680 --> 00:11:34,439
the bottleneck link a late comer or let

232
00:11:31,170 --> 00:11:36,329
bad flow would end up getting more than

233
00:11:34,440 --> 00:11:38,070
its fair share led by plus plus

234
00:11:36,330 --> 00:11:39,390
introduces two mechanisms to address

235
00:11:38,070 --> 00:11:41,010
this problem one is the multiplicative

236
00:11:39,390 --> 00:11:43,260
decrease instead of additive degrees

237
00:11:41,010 --> 00:11:45,360
giving congestion avoidance as well as

238
00:11:43,260 --> 00:11:48,689
initial and periodic slowdowns the

239
00:11:45,360 --> 00:11:50,760
original RFC said that natural gaps in

240
00:11:48,690 --> 00:11:52,350
application traffic would allow led by

241
00:11:50,760 --> 00:11:54,030
to measure the base delay but that

242
00:11:52,350 --> 00:11:56,070
doesn't happen in practice so led by

243
00:11:54,030 --> 00:11:58,319
plus plus immediately after slow start

244
00:11:56,070 --> 00:12:00,740
would enter a slowdown period and then

245
00:11:58,320 --> 00:12:04,740
would enter subsequent slow long periods

246
00:12:00,740 --> 00:12:07,230
with a goal to sacrifice throughput of

247
00:12:04,740 --> 00:12:11,690
at most to be at least 90% of standard

248
00:12:07,230 --> 00:12:13,920
tcp when there is no competing traffic

249
00:12:11,690 --> 00:12:15,840
there's a internet but fairness problem

250
00:12:13,920 --> 00:12:17,069
as well this is related to the late

251
00:12:15,840 --> 00:12:19,230
comer advantage problem and you can

252
00:12:17,070 --> 00:12:21,660
measure the base delay but although the

253
00:12:19,230 --> 00:12:24,000
queue is stable the flow is bound fair

254
00:12:21,660 --> 00:12:25,410
sure the led by plus plus introduces

255
00:12:24,000 --> 00:12:28,860
multiplicative decrease to solve this

256
00:12:25,410 --> 00:12:30,860
problem the original RFC does not

257
00:12:28,860 --> 00:12:33,270
specify slow start in very great detail

258
00:12:30,860 --> 00:12:35,040
there's two things that led by plus plus

259
00:12:33,270 --> 00:12:37,380
does it does slower than we know ramp up

260
00:12:35,040 --> 00:12:39,180
during slow start and it also does a

261
00:12:37,380 --> 00:12:40,800
variant of high start to exit slow start

262
00:12:39,180 --> 00:12:43,319
when it senses delay increases on the

263
00:12:40,800 --> 00:12:44,760
bottleneck link there's also a latency

264
00:12:43,320 --> 00:12:46,590
drift problem when you let connections

265
00:12:44,760 --> 00:12:49,950
run for a long time there's a ratchet

266
00:12:46,590 --> 00:12:51,240
ratcheting effect where the measurement

267
00:12:49,950 --> 00:12:54,630
of the basically key keeps increasing

268
00:12:51,240 --> 00:12:56,780
and that causes us to keep increasing

269
00:12:54,630 --> 00:12:58,550
latency from the link

270
00:12:56,780 --> 00:13:00,800
initial and periodic slowdowns help

271
00:12:58,550 --> 00:13:02,870
address this problem there's also a low

272
00:13:00,800 --> 00:13:05,599
latency computation problem if you have

273
00:13:02,870 --> 00:13:07,550
a bottleneck link where you can't reach

274
00:13:05,600 --> 00:13:10,430
target delay for example if the buffer

275
00:13:07,550 --> 00:13:12,949
is too small that causes LED bad to

276
00:13:10,430 --> 00:13:14,630
basically just compete with standard TCP

277
00:13:12,950 --> 00:13:16,490
and the bottleneck link so slower than

278
00:13:14,630 --> 00:13:21,950
Reno increase during condition avoidance

279
00:13:16,490 --> 00:13:23,930
addresses this problem there was on the

280
00:13:21,950 --> 00:13:25,430
mailing list there was a problem that

281
00:13:23,930 --> 00:13:27,859
was brought up I think need card will

282
00:13:25,430 --> 00:13:30,260
brought this problem up there what

283
00:13:27,860 --> 00:13:31,700
happens when there are multiple flows or

284
00:13:30,260 --> 00:13:33,890
large number of flows on the bottleneck

285
00:13:31,700 --> 00:13:35,690
link do we see a ratcheting effect

286
00:13:33,890 --> 00:13:38,810
problem or not so we actually did this

287
00:13:35,690 --> 00:13:41,330
experiment recently so this is a hundred

288
00:13:38,810 --> 00:13:42,439
millisecond or titi link we actually

289
00:13:41,330 --> 00:13:44,150
changed the target in later hundred

290
00:13:42,440 --> 00:13:45,950
millisecond for this experiment but yeah

291
00:13:44,150 --> 00:13:48,680
the results are similar with the default

292
00:13:45,950 --> 00:13:51,230
of 60 milliseconds and this is a 10x 10x

293
00:13:48,680 --> 00:13:52,550
bdp bottleneck buffer so even with the

294
00:13:51,230 --> 00:13:55,400
increase in number of connections we see

295
00:13:52,550 --> 00:13:58,819
that the latency remains near target at

296
00:13:55,400 --> 00:14:00,410
or near target so when we look at traces

297
00:13:58,820 --> 00:14:01,910
there's two reasons one is when the new

298
00:14:00,410 --> 00:14:04,040
flow starts even though the slow start

299
00:14:01,910 --> 00:14:06,170
is slower than Reno it still causes a

300
00:14:04,040 --> 00:14:08,120
deal increase and that causes other

301
00:14:06,170 --> 00:14:10,130
flows to back off and then the newcomer

302
00:14:08,120 --> 00:14:11,780
flow would immediately enter it slow

303
00:14:10,130 --> 00:14:14,270
down phase allowing all the flows to

304
00:14:11,780 --> 00:14:15,680
measure phase doing we find that all the

305
00:14:14,270 --> 00:14:19,400
flows are able to successfully measured

306
00:14:15,680 --> 00:14:22,910
by anyway there is another experiment

307
00:14:19,400 --> 00:14:24,500
this is with both 1x + 10 X B DPS again

308
00:14:22,910 --> 00:14:25,760
with large number of flows as we

309
00:14:24,500 --> 00:14:28,430
increase the number of flows we find

310
00:14:25,760 --> 00:14:30,860
that things mostly stabilize around

311
00:14:28,430 --> 00:14:34,819
target so this is 100 millisecond or TTN

312
00:14:30,860 --> 00:14:37,640
and the D for 60 millisecond target so

313
00:14:34,820 --> 00:14:39,170
we would like to experiment more there

314
00:14:37,640 --> 00:14:41,300
was a suggestion that we use B be our

315
00:14:39,170 --> 00:14:42,650
probe our TT phase and see how that

316
00:14:41,300 --> 00:14:43,939
doesn't comparison that would be a very

317
00:14:42,650 --> 00:14:45,770
interesting experiment to do in the

318
00:14:43,940 --> 00:14:51,410
future but currently in practice we

319
00:14:45,770 --> 00:14:53,210
don't see a problem what are the changes

320
00:14:51,410 --> 00:14:54,530
since the first draft so I read and

321
00:14:53,210 --> 00:14:56,840
rearranged some sections to make

322
00:14:54,530 --> 00:14:58,640
readability easier and thanks to

323
00:14:56,840 --> 00:15:00,800
Marshalls review so I fixed all the

324
00:14:58,640 --> 00:15:03,920
feedback that he gave to make the text

325
00:15:00,800 --> 00:15:06,170
better the multiplicative decrease cap

326
00:15:03,920 --> 00:15:08,360
was not specified very well so the

327
00:15:06,170 --> 00:15:09,839
intention is to basically ensure that we

328
00:15:08,360 --> 00:15:13,530
never react more than

329
00:15:09,840 --> 00:15:15,810
what Reno would do in if it sees a last

330
00:15:13,530 --> 00:15:17,819
signal so the delay increased signal the

331
00:15:15,810 --> 00:15:19,680
worst-case decrease even with

332
00:15:17,820 --> 00:15:22,440
multiplicative decrease would be half

333
00:15:19,680 --> 00:15:25,050
the condition window removed the

334
00:15:22,440 --> 00:15:27,120
reduction factor just to simplify so if

335
00:15:25,050 --> 00:15:33,900
the reader has read that bad we just use

336
00:15:27,120 --> 00:15:35,280
the gain parameter now one of the

337
00:15:33,900 --> 00:15:37,439
requests has been to make the document

338
00:15:35,280 --> 00:15:40,079
stand alone currently it's a addendum on

339
00:15:37,440 --> 00:15:42,260
top of the original RFC so the goal is

340
00:15:40,080 --> 00:15:44,970
to make the document stand alone so that

341
00:15:42,260 --> 00:15:46,920
any implementer could just read this RFC

342
00:15:44,970 --> 00:15:49,350
and be able to implement the algorithm

343
00:15:46,920 --> 00:15:51,569
we also want to add pseudocode so that

344
00:15:49,350 --> 00:15:52,800
it makes it easy for implementers so I

345
00:15:51,570 --> 00:15:54,840
request to folks in the room would be

346
00:15:52,800 --> 00:15:56,250
please review this draft I think like

347
00:15:54,840 --> 00:15:59,190
less than best-effort congestion control

348
00:15:56,250 --> 00:16:01,200
is extremely important so please review

349
00:15:59,190 --> 00:16:03,480
this we provide feedback if you could

350
00:16:01,200 --> 00:16:05,820
implement this and do some experiments

351
00:16:03,480 --> 00:16:08,510
on your own that would be great any

352
00:16:05,820 --> 00:16:08,510
questions

353
00:16:13,850 --> 00:16:21,000
Jake on I was curious whether you had a

354
00:16:17,850 --> 00:16:25,290
chance to examine the impact of a

355
00:16:21,000 --> 00:16:28,770
queuing of FQ systems on on this

356
00:16:25,290 --> 00:16:30,270
strategy we have not so I call that out

357
00:16:28,770 --> 00:16:31,920
in one of the prior presentations it's

358
00:16:30,270 --> 00:16:34,110
also an active area of research I think

359
00:16:31,920 --> 00:16:35,969
like both testing with a QM as well as

360
00:16:34,110 --> 00:16:37,980
newer congestion controls like VB are

361
00:16:35,970 --> 00:16:42,420
they currently are testing is limited to

362
00:16:37,980 --> 00:16:43,710
cubic and you know so yeah testing with

363
00:16:42,420 --> 00:16:45,180
other delay ways congestion control

364
00:16:43,710 --> 00:16:47,870
algorithms which are aggressive would

365
00:16:45,180 --> 00:16:50,959
also be I think part of that future work

366
00:16:47,870 --> 00:16:50,960
thank you

367
00:16:50,990 --> 00:16:54,049
[Music]

368
00:16:57,280 --> 00:17:09,069
other questions we have a couple of

369
00:17:00,520 --> 00:17:37,750
minutes all right all right thank you

370
00:17:09,069 --> 00:17:40,899
thank you a presentation Marcelo morning

371
00:17:37,750 --> 00:17:44,250
hey I'm Marcelo Angelo going to talk

372
00:17:40,900 --> 00:17:47,380
about the receiver driven led but so I

373
00:17:44,250 --> 00:17:49,270
represented this in the last IDF so

374
00:17:47,380 --> 00:17:52,570
basically this is an update of the

375
00:17:49,270 --> 00:17:57,670
changes that I'd done in the draft so if

376
00:17:52,570 --> 00:18:00,669
you remember LED body is a set of

377
00:17:57,670 --> 00:18:02,920
mekinese that allow to run a less than

378
00:18:00,670 --> 00:18:05,200
best-effort congestion control algorithm

379
00:18:02,920 --> 00:18:10,510
at the receiver in order to throttle the

380
00:18:05,200 --> 00:18:13,120
control centers rate right so in the

381
00:18:10,510 --> 00:18:15,879
original document I mean the in the

382
00:18:13,120 --> 00:18:18,669
version zero document that I represented

383
00:18:15,880 --> 00:18:21,040
last time the document contained both

384
00:18:18,670 --> 00:18:22,750
the mechanism to implement this in the

385
00:18:21,040 --> 00:18:25,899
receiver and the congestion and

386
00:18:22,750 --> 00:18:30,930
congestion control algorithm controller

387
00:18:25,900 --> 00:18:33,520
is now move to to the level plus plus a

388
00:18:30,930 --> 00:18:35,800
document that Praveen just presented so

389
00:18:33,520 --> 00:18:38,520
in this document we only kept the set of

390
00:18:35,800 --> 00:18:41,260
mechanics that allow the receiver to

391
00:18:38,520 --> 00:18:44,920
exert control over the sender's rate

392
00:18:41,260 --> 00:18:47,170
right so in particular in this document

393
00:18:44,920 --> 00:18:49,720
we kept how to use the receive window

394
00:18:47,170 --> 00:18:52,930
how to manage it safely in order to

395
00:18:49,720 --> 00:18:55,330
avoid window shrinking handling the

396
00:18:52,930 --> 00:18:57,400
window scaled option and this type of

397
00:18:55,330 --> 00:19:01,780
things in order to control the sender's

398
00:18:57,400 --> 00:19:03,640
rate right in addition we work out the

399
00:19:01,780 --> 00:19:05,860
different dock mechanics in order to

400
00:19:03,640 --> 00:19:08,679
feed the information that the congestion

401
00:19:05,860 --> 00:19:10,510
control economy needs including the

402
00:19:08,679 --> 00:19:11,110
Artic measurements in order to estimate

403
00:19:10,510 --> 00:19:13,450
queuing

404
00:19:11,110 --> 00:19:16,419
lay and the detection of retransmissions

405
00:19:13,450 --> 00:19:20,940
at the receiver so basically the the

406
00:19:16,420 --> 00:19:24,730
input that that the congestion control

407
00:19:20,940 --> 00:19:26,590
algorithm form of RTD measurements delay

408
00:19:24,730 --> 00:19:31,030
measurement and some form of loss

409
00:19:26,590 --> 00:19:32,860
transmission detection so the changes

410
00:19:31,030 --> 00:19:34,178
since the zero zero ssin is we remove

411
00:19:32,860 --> 00:19:37,330
all the kind of the part of the

412
00:19:34,179 --> 00:19:40,360
congestion control algorithm we defined

413
00:19:37,330 --> 00:19:42,460
so that elaborate is compatible with any

414
00:19:40,360 --> 00:19:44,949
less than best F for congestion control

415
00:19:42,460 --> 00:19:47,230
as long as it uses some form of delay

416
00:19:44,950 --> 00:19:50,260
estimation and all packet losses in

417
00:19:47,230 --> 00:19:53,110
order to work and we currently are

418
00:19:50,260 --> 00:19:56,290
referring to labelled plus plus or any

419
00:19:53,110 --> 00:20:00,189
other a congestion control that uses

420
00:19:56,290 --> 00:20:02,918
this type of signaling we could point to

421
00:20:00,190 --> 00:20:04,870
LED but if we consider that using LED

422
00:20:02,919 --> 00:20:07,630
but with RTT instead of one-way delay

423
00:20:04,870 --> 00:20:09,129
make sense assume I mean consuming that

424
00:20:07,630 --> 00:20:12,070
leopard plus plus is essentially an

425
00:20:09,130 --> 00:20:14,140
upgrade on on lead but we believe that

426
00:20:12,070 --> 00:20:16,330
this may not make a lot of sense but we

427
00:20:14,140 --> 00:20:21,640
could do it if people feel strongly

428
00:20:16,330 --> 00:20:23,530
about LED but and regarding our LED what

429
00:20:21,640 --> 00:20:25,120
implementation we're currently working I

430
00:20:23,530 --> 00:20:26,918
mean we haven't an implementation we

431
00:20:25,120 --> 00:20:30,189
presented some results in the in the

432
00:20:26,919 --> 00:20:32,530
last IDF we're now working in order to

433
00:20:30,190 --> 00:20:34,600
make it perfectly aligned with the level

434
00:20:32,530 --> 00:20:36,129
plus plus latest version that Praveen

435
00:20:34,600 --> 00:20:38,760
just presented this is this is ongoing

436
00:20:36,130 --> 00:20:42,850
work we hope to be able to present a

437
00:20:38,760 --> 00:20:45,370
results in the next meeting and we also

438
00:20:42,850 --> 00:20:48,360
became aware that a apple have a

439
00:20:45,370 --> 00:20:50,530
receiver based less than best effort

440
00:20:48,360 --> 00:20:52,389
implementation that is available in this

441
00:20:50,530 --> 00:20:55,690
in the it's open source and available

442
00:20:52,390 --> 00:21:00,100
here it seems somehow aligned in the

443
00:20:55,690 --> 00:21:01,570
sense that the use RTT use timestamps

444
00:21:00,100 --> 00:21:03,730
and receive window basically the tools

445
00:21:01,570 --> 00:21:06,990
that we are that we are describing the

446
00:21:03,730 --> 00:21:09,850
draft but it would be interesting to

447
00:21:06,990 --> 00:21:12,630
actually have some input from from from

448
00:21:09,850 --> 00:21:15,100
apple to see if they actually are

449
00:21:12,630 --> 00:21:19,480
aligned to to what we present here and

450
00:21:15,100 --> 00:21:22,240
if not to be able to align properly the

451
00:21:19,480 --> 00:21:24,010
less than best effort control congestion

452
00:21:22,240 --> 00:21:24,970
control agon that they use clearly it

453
00:21:24,010 --> 00:21:27,190
seems different

454
00:21:24,970 --> 00:21:29,320
then LED but misplaced but as as I said

455
00:21:27,190 --> 00:21:33,479
are LED but is compatible with different

456
00:21:29,320 --> 00:21:33,479
congestion control I'll go and examine

457
00:21:34,799 --> 00:21:40,539
xed steps will be will be very useful to

458
00:21:37,659 --> 00:21:43,179
have reviews comments from from the from

459
00:21:40,539 --> 00:21:44,710
the research group and feel free to

460
00:21:43,179 --> 00:21:49,779
contact me if you have further questions

461
00:21:44,710 --> 00:21:54,909
or comments and with that I available

462
00:21:49,779 --> 00:22:08,950
for questions or I can leave questions

463
00:21:54,909 --> 00:22:12,940
folks come on it's Monday morning do

464
00:22:08,950 --> 00:22:19,090
that the mic is all I'm saying some

465
00:22:12,940 --> 00:22:21,330
engagement people well I actually do

466
00:22:19,090 --> 00:22:24,418
have one question for you myself

467
00:22:21,330 --> 00:22:26,769
have you considered expanding this to

468
00:22:24,419 --> 00:22:28,450
the mechanisms would just be slightly

469
00:22:26,769 --> 00:22:33,070
different but I think the concepts still

470
00:22:28,450 --> 00:22:35,409
hold for quick yes do you plan to write

471
00:22:33,070 --> 00:22:36,849
that in the draft or do you think in

472
00:22:35,409 --> 00:22:40,929
this particular draft yeah yeah I'm

473
00:22:36,849 --> 00:22:42,428
asking ah I was thinking to I mean once

474
00:22:40,929 --> 00:22:44,470
I'm done with this do it in a different

475
00:22:42,429 --> 00:22:46,210
document but if you love define do I

476
00:22:44,470 --> 00:22:48,729
mean I mean if you want me to do this

477
00:22:46,210 --> 00:22:50,440
talk mean I'm happy to it it seems

478
00:22:48,729 --> 00:22:52,720
reasonable to do it if you're gonna

479
00:22:50,440 --> 00:22:54,249
cover both TCP and quick in the same if

480
00:22:52,720 --> 00:22:57,029
it's possible to write I don't think the

481
00:22:54,249 --> 00:23:00,190
differences should be that much okay but

482
00:22:57,029 --> 00:23:02,200
yeah if it becomes big enough it might

483
00:23:00,190 --> 00:23:06,489
be worth separating it out okay yeah

484
00:23:02,200 --> 00:23:08,830
sure thank you gory Fair has just asking

485
00:23:06,489 --> 00:23:13,029
about what you actually said at the mic

486
00:23:08,830 --> 00:23:15,189
were you asking about strange Eckles

487
00:23:13,029 --> 00:23:16,929
were you asking about whether this

488
00:23:15,190 --> 00:23:19,479
document could describe a method that

489
00:23:16,929 --> 00:23:20,769
could be applied to quake or were you

490
00:23:19,479 --> 00:23:22,929
saying the document would include a

491
00:23:20,769 --> 00:23:25,619
mechanism described how it applied to

492
00:23:22,929 --> 00:23:28,720
quick I see the two is quite different

493
00:23:25,619 --> 00:23:32,228
that's fair I was asking how you would

494
00:23:28,720 --> 00:23:34,599
apply it so the mechanism itself is not

495
00:23:32,229 --> 00:23:36,360
an underwire change it's entirely a

496
00:23:34,599 --> 00:23:39,639
receiver side

497
00:23:36,360 --> 00:23:41,678
it's its management of the receiving

498
00:23:39,639 --> 00:23:44,500
door in the world of quick it would be

499
00:23:41,679 --> 00:23:46,690
how you advertise flow control credit

500
00:23:44,500 --> 00:23:47,799
and so it would be slightly different in

501
00:23:46,690 --> 00:23:49,120
the sense that you're not just talking

502
00:23:47,799 --> 00:23:50,710
about modulating the window but you're

503
00:23:49,120 --> 00:23:53,500
talking about modulating the size of the

504
00:23:50,710 --> 00:23:55,480
credit that even advertised and there is

505
00:23:53,500 --> 00:23:57,370
no equivalent I mean you can actually

506
00:23:55,480 --> 00:24:00,039
shrink effectively so that gives you a

507
00:23:57,370 --> 00:24:01,570
degree of freedom that is available in

508
00:24:00,039 --> 00:24:03,429
quit because what I mean by shrinking is

509
00:24:01,570 --> 00:24:04,539
you advertise lesser credit in the

510
00:24:03,429 --> 00:24:09,309
future right because you can actually

511
00:24:04,539 --> 00:24:10,809
shrink the window so if that I don't

512
00:24:09,309 --> 00:24:12,820
need that's that's what I was asking me

513
00:24:10,809 --> 00:24:15,940
this right the way it applies to quick

514
00:24:12,820 --> 00:24:17,620
would be slightly different so if I

515
00:24:15,940 --> 00:24:20,080
understand that that means this is a

516
00:24:17,620 --> 00:24:21,459
mechanism and you're not going to

517
00:24:20,080 --> 00:24:22,960
describe the protocol but you're going

518
00:24:21,460 --> 00:24:25,210
to describe the mechanism we saw what

519
00:24:22,960 --> 00:24:26,889
you were talking about yeah so so maybe

520
00:24:25,210 --> 00:24:29,590
maybe then what I'm asking for thank you

521
00:24:26,889 --> 00:24:30,789
for for for teasing this out Cory what

522
00:24:29,590 --> 00:24:33,100
I'm asking for is how would this apply

523
00:24:30,789 --> 00:24:35,529
to like a credit-based flow controller

524
00:24:33,100 --> 00:24:37,449
as against there in the window waste one

525
00:24:35,529 --> 00:24:44,559
okay that's the difference I'm looking

526
00:24:37,450 --> 00:24:46,659
at clear thank you Robi Praveen from

527
00:24:44,559 --> 00:24:48,759
Microsoft I do think that both of these

528
00:24:46,659 --> 00:24:51,220
RFC's are kind sort of transport

529
00:24:48,759 --> 00:24:53,769
agnostic there are certain like

530
00:24:51,220 --> 00:24:55,299
transport mechanisms were using to

531
00:24:53,769 --> 00:24:56,710
communicate information as you point out

532
00:24:55,299 --> 00:24:58,269
but yeah I think they could be written

533
00:24:56,710 --> 00:25:02,950
aware that they could be applied to both

534
00:24:58,269 --> 00:25:04,419
TCP so that's true but does the avoiding

535
00:25:02,950 --> 00:25:07,750
of the risk of the window shrinking

536
00:25:04,419 --> 00:25:10,990
right which is very tcp specific it's

537
00:25:07,750 --> 00:25:15,759
it's very ecosystem specific to now

538
00:25:10,990 --> 00:25:17,769
that's not applicable to quick possibly

539
00:25:15,759 --> 00:25:23,740
not but III don't think it's illegal to

540
00:25:17,769 --> 00:25:25,870
shrink it's what's not illegal it's it's

541
00:25:23,740 --> 00:25:27,309
not illegal to avoid it certainly but

542
00:25:25,870 --> 00:25:33,939
I'm saying it you have the degree of

543
00:25:27,309 --> 00:25:36,158
freedom if you want it okay Nokia versa

544
00:25:33,940 --> 00:25:40,990
I'm wondering about the windows

545
00:25:36,159 --> 00:25:44,139
shrinking thing is that like what I mean

546
00:25:40,990 --> 00:25:46,480
I understand what it is but I don't know

547
00:25:44,139 --> 00:25:49,149
what what would current TCP sender

548
00:25:46,480 --> 00:25:50,080
applications to if you if you put you

549
00:25:49,149 --> 00:25:51,760
know if you would submit

550
00:25:50,080 --> 00:25:53,710
the window that is too small and you

551
00:25:51,760 --> 00:25:54,908
would actually shrink it they might just

552
00:25:53,710 --> 00:25:56,740
ignore it right because they have to

553
00:25:54,909 --> 00:25:58,960
have a way of handling it on the sender

554
00:25:56,740 --> 00:26:07,029
side anyway if that happens what did

555
00:25:58,960 --> 00:26:15,940
send a reset or sorry buddy no so so if

556
00:26:07,029 --> 00:26:17,110
you so as far as I can tell there are

557
00:26:15,940 --> 00:26:18,789
some situations where the window

558
00:26:17,110 --> 00:26:20,469
shrinking is general when you're using

559
00:26:18,789 --> 00:26:22,899
very large window scale for instance

560
00:26:20,470 --> 00:26:26,230
that the granularity of the window that

561
00:26:22,899 --> 00:26:27,639
you are able to announce will will

562
00:26:26,230 --> 00:26:29,620
result in window shrinking so I

563
00:26:27,639 --> 00:26:31,689
understand that implementations usually

564
00:26:29,620 --> 00:26:34,149
can handle window shrinking without

565
00:26:31,690 --> 00:26:39,490
losing packets but that's that's what I

566
00:26:34,149 --> 00:26:41,139
think Praveen Tony so gory Fairhurst not

567
00:26:39,490 --> 00:26:43,299
answering the question directly but

568
00:26:41,139 --> 00:26:45,010
saying that will because I'm going to

569
00:26:43,299 --> 00:26:47,230
talk in a side meeting on quick for

570
00:26:45,010 --> 00:26:48,399
satellite later in the week and one of

571
00:26:47,230 --> 00:26:52,080
the things I'm going to talk about is

572
00:26:48,399 --> 00:26:54,459
the way tcp windows are managed in bsd

573
00:26:52,080 --> 00:26:56,830
using receiver window advertisements and

574
00:26:54,460 --> 00:26:58,330
how you grow and shrink the receiver

575
00:26:56,830 --> 00:27:00,820
versions and how that relates to what

576
00:26:58,330 --> 00:27:02,320
quick does so this is an area where yes

577
00:27:00,820 --> 00:27:06,279
you can't change the receiver window and

578
00:27:02,320 --> 00:27:07,779
it doesn't will impact maybe that causes

579
00:27:06,279 --> 00:27:09,580
you to send more than you're expecting

580
00:27:07,779 --> 00:27:10,990
it ends that we ditched or whatever but

581
00:27:09,580 --> 00:27:15,279
this is a mechanism that does work in

582
00:27:10,990 --> 00:27:16,360
real implementations and and talk more

583
00:27:15,279 --> 00:27:25,179
to me if you wanted to find out what I

584
00:27:16,360 --> 00:27:32,620
was saying thanks Maya

585
00:27:25,179 --> 00:27:35,980
okay thank you so these are the two

586
00:27:32,620 --> 00:27:37,418
drafts that we are planning to adopt if

587
00:27:35,980 --> 00:27:39,760
you have any thoughts please send them

588
00:27:37,419 --> 00:27:41,679
to me or send them on the list like I

589
00:27:39,760 --> 00:27:45,250
said I will send out an email I enter

590
00:27:41,679 --> 00:27:49,090
the list after this meeting and I see

591
00:27:45,250 --> 00:27:52,510
news already there and are there miles

592
00:27:49,090 --> 00:27:53,199
on you guys can you guys hear me yes we

593
00:27:52,510 --> 00:27:57,309
can hear you

594
00:27:53,200 --> 00:27:58,029
okay and your slides are up gonio all

595
00:27:57,309 --> 00:28:01,299
right great

596
00:27:58,029 --> 00:28:04,030
hi my name is Neel Cardwell and I'm

597
00:28:01,299 --> 00:28:07,240
gonna talk today but with a brief update

598
00:28:04,030 --> 00:28:09,129
VBR version to work at google we think

599
00:28:07,240 --> 00:28:11,560
of EVR to as a model based congestion

600
00:28:09,130 --> 00:28:13,750
control and today I'm going to focus on

601
00:28:11,560 --> 00:28:16,179
some recent performance optimization

602
00:28:13,750 --> 00:28:18,400
work that we've done this is joint work

603
00:28:16,180 --> 00:28:21,880
with my colleagues at Google you Chong

604
00:28:18,400 --> 00:28:25,000
and Sohail Puri Ranjan you suck in Kevin

605
00:28:21,880 --> 00:28:27,430
the quick folks in Victor and bin a

606
00:28:25,000 --> 00:28:31,270
summer intern I had this year Luke and

607
00:28:27,430 --> 00:28:33,010
Matt Mathis and magic I'm hearing

608
00:28:31,270 --> 00:28:38,410
substantial echo in my hand are you guys

609
00:28:33,010 --> 00:28:40,720
okay yeah we're fine go but that says

610
00:28:38,410 --> 00:28:44,680
the function of the room okay great

611
00:28:40,720 --> 00:28:47,350
all right next slide please yeah so just

612
00:28:44,680 --> 00:28:48,610
a quick outline I'm gonna talk for a

613
00:28:47,350 --> 00:28:50,530
while about the performance

614
00:28:48,610 --> 00:28:52,899
optimizations I mentioned and then I'll

615
00:28:50,530 --> 00:28:55,600
give a quick rundown of the status of

616
00:28:52,900 --> 00:28:57,810
the VBR version to code and the

617
00:28:55,600 --> 00:29:00,389
deployment at Google and then wrap up

618
00:28:57,810 --> 00:29:03,909
next slide please

619
00:29:00,390 --> 00:29:07,780
so yeah performance optimizations next

620
00:29:03,910 --> 00:29:09,880
slide so what are we doing here what's

621
00:29:07,780 --> 00:29:13,060
the goal here well we basically want to

622
00:29:09,880 --> 00:29:15,550
ensure that bbr version 2 gets to a

623
00:29:13,060 --> 00:29:17,919
point where it's doing a good job as a

624
00:29:15,550 --> 00:29:22,570
general-purpose congestion control for

625
00:29:17,920 --> 00:29:24,190
both TCP and quick for the environments

626
00:29:22,570 --> 00:29:24,669
where those transport protocols are used

627
00:29:24,190 --> 00:29:27,520
today

628
00:29:24,670 --> 00:29:31,300
so that means LAN and when networks data

629
00:29:27,520 --> 00:29:33,010
center networks VM guests and we want to

630
00:29:31,300 --> 00:29:35,800
get to a point where it's a good drop-in

631
00:29:33,010 --> 00:29:39,070
replacement for the predominant

632
00:29:35,800 --> 00:29:41,560
algorithms used today so reno Kubik DC

633
00:29:39,070 --> 00:29:43,360
TCP and we want to make sure that it's

634
00:29:41,560 --> 00:29:46,889
providing in performance improvements

635
00:29:43,360 --> 00:29:49,689
across all of those environments and has

636
00:29:46,890 --> 00:29:51,940
acceptable coexistence properties when

637
00:29:49,690 --> 00:29:54,460
it shares with common congestion control

638
00:29:51,940 --> 00:29:57,640
algorithms like renown cubic and of

639
00:29:54,460 --> 00:30:01,210
course as a stepping stone to number one

640
00:29:57,640 --> 00:30:03,370
we are deploying bbr version 2 for all

641
00:30:01,210 --> 00:30:06,100
the TCP and quick traffic at Google and

642
00:30:03,370 --> 00:30:08,679
we are working on that as as we speak

643
00:30:06,100 --> 00:30:10,959
and as part of that of course that

644
00:30:08,680 --> 00:30:13,960
involves a fair amount of testing and

645
00:30:10,960 --> 00:30:16,810
that so we basically want to ensure that

646
00:30:13,960 --> 00:30:17,730
PBR is doing a good job on both the

647
00:30:16,810 --> 00:30:20,040
production where

648
00:30:17,730 --> 00:30:24,780
clothes we see at Google and a wide

649
00:30:20,040 --> 00:30:26,790
range of test matrices in synthetic or

650
00:30:24,780 --> 00:30:28,110
lab tests and we want to make sure it's

651
00:30:26,790 --> 00:30:32,970
doing at least as well as we know and

652
00:30:28,110 --> 00:30:35,340
cubic and DC TCP next slide please so I

653
00:30:32,970 --> 00:30:37,950
mentioned some tests of production

654
00:30:35,340 --> 00:30:41,220
workloads a cool so so what's that so at

655
00:30:37,950 --> 00:30:43,860
at Google we do a fair amount of testing

656
00:30:41,220 --> 00:30:46,290
on our production kernel changes this

657
00:30:43,860 --> 00:30:48,449
includes TCP loss recovery and

658
00:30:46,290 --> 00:30:51,090
congestion control including VBR changes

659
00:30:48,450 --> 00:30:53,120
and there's a fairly rigorous suite of

660
00:30:51,090 --> 00:30:55,649
application benchmarks that we apply

661
00:30:53,120 --> 00:30:59,129
obviously web search is included but

662
00:30:55,650 --> 00:31:01,200
also databases and storage on and we

663
00:30:59,130 --> 00:31:05,430
look at a fair number of scenarios and

664
00:31:01,200 --> 00:31:08,070
not just sort of simple bulk transfer

665
00:31:05,430 --> 00:31:10,560
dumbell tests and not just looking at

666
00:31:08,070 --> 00:31:13,770
only traditional congestion control

667
00:31:10,560 --> 00:31:16,500
metrics like loss rate or throughput

668
00:31:13,770 --> 00:31:20,120
fairness queuing delays and so forth we

669
00:31:16,500 --> 00:31:24,630
also look a fair amount at CPU usage and

670
00:31:20,120 --> 00:31:26,939
median and tail RPC latency and why we

671
00:31:24,630 --> 00:31:28,260
do that well you know it when you

672
00:31:26,940 --> 00:31:29,790
operate these things at scale in

673
00:31:28,260 --> 00:31:32,370
production environments of course the

674
00:31:29,790 --> 00:31:34,220
details matter a lot and in particular

675
00:31:32,370 --> 00:31:36,659
one of the important details that

676
00:31:34,220 --> 00:31:40,140
doesn't always get as much attention as

677
00:31:36,660 --> 00:31:42,150
it should is is CPU usage and the sort

678
00:31:40,140 --> 00:31:45,020
of contributing factors including data

679
00:31:42,150 --> 00:31:47,880
packet send rates acknowledgement rates

680
00:31:45,020 --> 00:31:50,700
offload birth sizes interrupt rates all

681
00:31:47,880 --> 00:31:52,560
the things that feed into CPU usage and

682
00:31:50,700 --> 00:31:54,540
in addition there are a lot of sort of

683
00:31:52,560 --> 00:31:56,550
tricky and stressful scenarios that are

684
00:31:54,540 --> 00:31:58,649
important to test like when you have

685
00:31:56,550 --> 00:32:02,460
thousands of flows coming from a single

686
00:31:58,650 --> 00:32:04,230
sending host or sharing a small BDP in a

687
00:32:02,460 --> 00:32:07,050
data center like environment those are

688
00:32:04,230 --> 00:32:08,760
also important to test and as we've been

689
00:32:07,050 --> 00:32:12,540
running bbr version 2 through these

690
00:32:08,760 --> 00:32:16,020
kinds of tests at Google we've developed

691
00:32:12,540 --> 00:32:17,879
a number of performance improvements to

692
00:32:16,020 --> 00:32:20,190
tackle various issues that we've seen in

693
00:32:17,880 --> 00:32:21,780
these tests and so I'll go through these

694
00:32:20,190 --> 00:32:24,060
in a little bit of detail but just to

695
00:32:21,780 --> 00:32:27,510
give a quick summary I could say that

696
00:32:24,060 --> 00:32:30,389
basically the the first two improvements

697
00:32:27,510 --> 00:32:31,710
are in order to match the CPU usage for

698
00:32:30,390 --> 00:32:34,860
Reno or a cubic

699
00:32:31,710 --> 00:32:37,920
DC TCP and number one we developed a

700
00:32:34,860 --> 00:32:40,679
fast path for PBR on number two we did

701
00:32:37,920 --> 00:32:43,380
some improvements in the TSO auto sizing

702
00:32:40,680 --> 00:32:45,510
calculation and then the third

703
00:32:43,380 --> 00:32:47,280
improvement was to fix in a sort of

704
00:32:45,510 --> 00:32:50,640
interesting issue we uncovered with the

705
00:32:47,280 --> 00:32:52,580
Linux TCP receive code path and for that

706
00:32:50,640 --> 00:32:56,460
we've developed a mechanism that

707
00:32:52,580 --> 00:33:00,300
generates faster acts and then finally

708
00:32:56,460 --> 00:33:02,430
the fourth one is to enhance PBR version

709
00:33:00,300 --> 00:33:05,030
to you so that it gets better

710
00:33:02,430 --> 00:33:07,320
performance than the widely deployed

711
00:33:05,030 --> 00:33:09,149
algorithms like Reno and cubic and DC

712
00:33:07,320 --> 00:33:11,550
TCP and the sort of interesting case

713
00:33:09,150 --> 00:33:14,250
where there are more flows sharing a

714
00:33:11,550 --> 00:33:16,770
bottleneck than the bdp of the path

715
00:33:14,250 --> 00:33:20,660
expressed in packets

716
00:33:16,770 --> 00:33:23,129
all right next slide so the first

717
00:33:20,660 --> 00:33:26,420
improvement that we worked on was a fast

718
00:33:23,130 --> 00:33:30,060
path for bbr so why did we do this well

719
00:33:26,420 --> 00:33:33,570
much of the traffic in the real world is

720
00:33:30,060 --> 00:33:37,440
application limited so web transfers RPC

721
00:33:33,570 --> 00:33:40,409
traffic adaptive bitrate video and if

722
00:33:37,440 --> 00:33:44,070
you think about it cubic and Reno and DC

723
00:33:40,410 --> 00:33:48,150
TCP when they're not receiving ecn or

724
00:33:44,070 --> 00:33:50,280
lost signals they take a essentially a

725
00:33:48,150 --> 00:33:52,290
fast path when they have application

726
00:33:50,280 --> 00:33:54,510
limited traffic the the first couple

727
00:33:52,290 --> 00:33:56,850
lines of most of them basically say if

728
00:33:54,510 --> 00:33:58,710
this if we're if the flow is not

729
00:33:56,850 --> 00:34:01,159
currently see when limited and then

730
00:33:58,710 --> 00:34:03,090
let's return and not do any of the other

731
00:34:01,160 --> 00:34:05,370
processing that we might normally do for

732
00:34:03,090 --> 00:34:08,610
the congestion control the problem here

733
00:34:05,370 --> 00:34:10,380
is that bbr when run on these very

734
00:34:08,610 --> 00:34:12,780
simple application limited workloads I

735
00:34:10,380 --> 00:34:15,270
ran into CPU usage issues and even

736
00:34:12,780 --> 00:34:17,910
through progressions and some of these

737
00:34:15,270 --> 00:34:21,150
simple application limited workloads and

738
00:34:17,909 --> 00:34:23,149
why is this well thus far the BB our

739
00:34:21,150 --> 00:34:25,920
code base has tried to prioritize

740
00:34:23,150 --> 00:34:27,720
simplicity and basically ran the entire

741
00:34:25,920 --> 00:34:30,200
algorithm on every act so that means

742
00:34:27,719 --> 00:34:33,029
updating the entire model of the path

743
00:34:30,199 --> 00:34:34,859
updating the probing state machine and

744
00:34:33,030 --> 00:34:37,500
then adjusting all of the control

745
00:34:34,860 --> 00:34:40,110
parameters the pasting rate congestion

746
00:34:37,500 --> 00:34:43,050
window offload chunk size and this was

747
00:34:40,110 --> 00:34:45,310
causing up to sort of 2 to 5% CPU and

748
00:34:43,050 --> 00:34:48,490
throughput regressions on some of these

749
00:34:45,310 --> 00:34:51,310
synthetic tests in these workloads and

750
00:34:48,489 --> 00:34:54,639
to tackle this we basically constructed

751
00:34:51,310 --> 00:34:57,279
a fast path for bbr where the idea is is

752
00:34:54,639 --> 00:35:00,129
that we only run the portions of the

753
00:34:57,280 --> 00:35:03,040
algorithm that are strictly needed based

754
00:35:00,130 --> 00:35:04,990
on the properties of the information

755
00:35:03,040 --> 00:35:08,170
being conveyed by a given acknowledgment

756
00:35:04,990 --> 00:35:10,629
and this resolved those cpu and through

757
00:35:08,170 --> 00:35:13,540
progressions without sacrificing

758
00:35:10,630 --> 00:35:15,720
throughput or latency an excellent

759
00:35:13,540 --> 00:35:15,720
please

760
00:35:18,030 --> 00:35:26,260
so we also worked on some improvements

761
00:35:21,520 --> 00:35:29,440
in TSO auto sizing so what is TSO auto

762
00:35:26,260 --> 00:35:32,590
sizing so in high-performance transport

763
00:35:29,440 --> 00:35:34,630
stacks often they achieve a very big

764
00:35:32,590 --> 00:35:37,360
performance improvement by bundling

765
00:35:34,630 --> 00:35:39,430
together consecutive segments into a

766
00:35:37,360 --> 00:35:42,190
single unit when passing them down to

767
00:35:39,430 --> 00:35:46,960
lower layers like the IP layer and the

768
00:35:42,190 --> 00:35:50,290
the NIC driver and in particular the the

769
00:35:46,960 --> 00:35:52,900
core Linux TCP stack has for about seven

770
00:35:50,290 --> 00:35:56,560
years or so used a TSO auto sizing

771
00:35:52,900 --> 00:35:59,050
algorithm that adapts the offload chunk

772
00:35:56,560 --> 00:36:01,029
size largely based on the pacing rate

773
00:35:59,050 --> 00:36:03,760
and it goes something like this so the

774
00:36:01,030 --> 00:36:05,980
the core TCP stack computes a pacing

775
00:36:03,760 --> 00:36:08,650
rate and whether that's used for pacing

776
00:36:05,980 --> 00:36:11,830
or not it still uses that pacing rate

777
00:36:08,650 --> 00:36:13,600
for the offload sizing decision so it

778
00:36:11,830 --> 00:36:15,819
computes the pacing rate as some

779
00:36:13,600 --> 00:36:17,650
constant scale factor times the

780
00:36:15,820 --> 00:36:21,280
congestion window divided by the smooth

781
00:36:17,650 --> 00:36:23,740
round-trip time and then it takes that

782
00:36:21,280 --> 00:36:26,020
pacing rate and it calculates how much

783
00:36:23,740 --> 00:36:28,299
data it thinks would be paste out in one

784
00:36:26,020 --> 00:36:31,150
millisecond and then it applies a floor

785
00:36:28,300 --> 00:36:33,970
of two segments and a ceiling of 64

786
00:36:31,150 --> 00:36:37,750
kilobytes that's determined by the the

787
00:36:33,970 --> 00:36:39,700
offload mechanism itself and an

788
00:36:37,750 --> 00:36:41,650
interesting thing happens if you have a

789
00:36:39,700 --> 00:36:43,890
workload where the sender host is the

790
00:36:41,650 --> 00:36:47,590
bottleneck for a large number of flows

791
00:36:43,890 --> 00:36:49,270
and in that kind of scenario because the

792
00:36:47,590 --> 00:36:52,150
sender host itself is the bottleneck

793
00:36:49,270 --> 00:36:54,700
there tends to not be any easy on or

794
00:36:52,150 --> 00:36:58,480
lost signals that the sender's get and

795
00:36:54,700 --> 00:36:59,250
so with an algorithm like DC TCP or a

796
00:36:58,480 --> 00:37:02,490
cube occur we

797
00:36:59,250 --> 00:37:04,560
the Seawind can be surprisingly high in

798
00:37:02,490 --> 00:37:06,569
these cases which then generates a large

799
00:37:04,560 --> 00:37:10,680
pacing rate and then a large offload

800
00:37:06,569 --> 00:37:12,869
chunk size and that large offload chunk

801
00:37:10,680 --> 00:37:17,750
size allows very low a very efficient

802
00:37:12,869 --> 00:37:21,390
CPU usage and then the problem is if we

803
00:37:17,750 --> 00:37:23,280
if we take a b b our scenario where we

804
00:37:21,390 --> 00:37:25,890
have a large number of BB r senders and

805
00:37:23,280 --> 00:37:28,500
they try to use reuse this auto sizing

806
00:37:25,890 --> 00:37:31,230
algorithm now BB r is computing its

807
00:37:28,500 --> 00:37:34,650
pacing rate based on its estimate of the

808
00:37:31,230 --> 00:37:36,450
flows fair share of the bandwidth coming

809
00:37:34,650 --> 00:37:39,810
out of that sender host so if you've got

810
00:37:36,450 --> 00:37:42,118
thousands of flows the fair share for a

811
00:37:39,810 --> 00:37:44,520
given flow is quite small so PBR was

812
00:37:42,119 --> 00:37:47,210
choosing small offload chunks and using

813
00:37:44,520 --> 00:37:50,670
a lot of CPU and so to solve this issue

814
00:37:47,210 --> 00:37:53,670
we developed a mechanism that adds a

815
00:37:50,670 --> 00:37:55,680
term into the auto sizing calculation

816
00:37:53,670 --> 00:37:58,200
that is a function of the minimum

817
00:37:55,680 --> 00:38:02,569
round-trip time so that as the minimum

818
00:37:58,200 --> 00:38:06,540
round-trip time goes the saw the size of

819
00:38:02,569 --> 00:38:10,349
the offload budget that is added for the

820
00:38:06,540 --> 00:38:11,700
min ITT falls off rapidly but for very

821
00:38:10,349 --> 00:38:14,040
small men are TT's there is a

822
00:38:11,700 --> 00:38:16,290
considerable budget that's allocated for

823
00:38:14,040 --> 00:38:19,650
the the TSO offload size and you can see

824
00:38:16,290 --> 00:38:21,480
the exact formula there and you can read

825
00:38:19,650 --> 00:38:25,339
the the code for the the details on the

826
00:38:21,480 --> 00:38:25,339
constant next slide please

827
00:38:25,609 --> 00:38:31,980
so the the third issue that we tackled

828
00:38:30,359 --> 00:38:34,650
was and it's sort of an interesting

829
00:38:31,980 --> 00:38:37,260
issue involving the specifics of the

830
00:38:34,650 --> 00:38:42,359
Linux TCP receive code path and the

831
00:38:37,260 --> 00:38:44,880
delay dock logic and DC TCP and BP are

832
00:38:42,359 --> 00:38:47,490
v2 both have a nice property where they

833
00:38:44,880 --> 00:38:50,250
emit an immediate act when the incoming

834
00:38:47,490 --> 00:38:53,009
stream of congestion experience gets

835
00:38:50,250 --> 00:38:54,869
change but that mechanism doesn't always

836
00:38:53,010 --> 00:38:56,160
save you because if you're under heavy

837
00:38:54,869 --> 00:38:58,230
enough congestion then there's

838
00:38:56,160 --> 00:39:00,569
continuous C II marking and that

839
00:38:58,230 --> 00:39:04,020
mechanism doesn't kick in so it can

840
00:39:00,569 --> 00:39:06,240
happen is that you can actually get into

841
00:39:04,020 --> 00:39:10,259
trouble as a Linux TCP receiver because

842
00:39:06,240 --> 00:39:12,770
the the details of the code for the

843
00:39:10,260 --> 00:39:14,690
traditional TCP every other

844
00:39:12,770 --> 00:39:16,009
Paquette mechanism are actually a little

845
00:39:14,690 --> 00:39:18,590
surprising and they're actually two

846
00:39:16,010 --> 00:39:21,740
conditions the first one is what you

847
00:39:18,590 --> 00:39:23,900
might expect which is to say if more

848
00:39:21,740 --> 00:39:26,750
than one MSS has been received since the

849
00:39:23,900 --> 00:39:28,760
last time we sent an AK then that

850
00:39:26,750 --> 00:39:30,950
condition needs to be met but it also

851
00:39:28,760 --> 00:39:32,300
has to be the case that the next

852
00:39:30,950 --> 00:39:33,740
received window that we're about to

853
00:39:32,300 --> 00:39:36,170
offer is at least as big as the previous

854
00:39:33,740 --> 00:39:38,689
one and the problem you get into is if

855
00:39:36,170 --> 00:39:41,330
the receive window stops growing

856
00:39:38,690 --> 00:39:44,360
this causes check number two to fail

857
00:39:41,330 --> 00:39:46,580
which causes the receiver to wait until

858
00:39:44,360 --> 00:39:48,800
the application actually reads some data

859
00:39:46,580 --> 00:39:52,819
out of the receive buffer to allow the

860
00:39:48,800 --> 00:39:55,550
receive window to advance and then

861
00:39:52,820 --> 00:39:58,670
trigger and acknowledgment and this was

862
00:39:55,550 --> 00:40:02,300
actually this sort of surprising detail

863
00:39:58,670 --> 00:40:04,670
was actually causing 2x higher p99 RPC

864
00:40:02,300 --> 00:40:06,680
latency x' was sustained congestion and

865
00:40:04,670 --> 00:40:09,500
some of the tests we were running and as

866
00:40:06,680 --> 00:40:11,660
as a solution what we've done at least

867
00:40:09,500 --> 00:40:15,170
thus far is to just remove truck number

868
00:40:11,660 --> 00:40:17,540
two and this is taking care of those RPC

869
00:40:15,170 --> 00:40:19,550
latency issues and we should say that if

870
00:40:17,540 --> 00:40:21,650
this works well in practice as we deploy

871
00:40:19,550 --> 00:40:24,130
I think will propose this as a general

872
00:40:21,650 --> 00:40:27,310
fix for the Linux TCP receive code path

873
00:40:24,130 --> 00:40:29,690
next slide please

874
00:40:27,310 --> 00:40:31,070
so this is just a quick picture to give

875
00:40:29,690 --> 00:40:33,140
a better sense of what's going on there

876
00:40:31,070 --> 00:40:35,240
because I know the the text doesn't make

877
00:40:33,140 --> 00:40:37,430
it entirely clear so here we have a

878
00:40:35,240 --> 00:40:39,529
picture that's a time sequence diagram

879
00:40:37,430 --> 00:40:40,640
of one of these scenarios and you can

880
00:40:39,530 --> 00:40:44,060
see at the beginning of the connection

881
00:40:40,640 --> 00:40:46,100
the receive window is is opening up you

882
00:40:44,060 --> 00:40:48,320
can see that as the received window the

883
00:40:46,100 --> 00:40:50,410
the yellow line is growing further away

884
00:40:48,320 --> 00:40:53,450
from the act line the green line and

885
00:40:50,410 --> 00:40:54,680
during that phase the receiver is

886
00:40:53,450 --> 00:40:56,180
allowing itself to acknowledge

887
00:40:54,680 --> 00:40:58,580
immediately just giving a high

888
00:40:56,180 --> 00:41:01,190
throughput which is reflected in the lot

889
00:40:58,580 --> 00:41:03,319
and the high slope of that line but then

890
00:41:01,190 --> 00:41:06,020
as soon as the receive window stops

891
00:41:03,320 --> 00:41:08,570
growing the yellow line sort of plateaus

892
00:41:06,020 --> 00:41:10,790
you can see that the acts are all of a

893
00:41:08,570 --> 00:41:12,710
sudden very delayed because it the stack

894
00:41:10,790 --> 00:41:14,960
has suddenly waiting for the application

895
00:41:12,710 --> 00:41:16,940
to read some data before it sends an

896
00:41:14,960 --> 00:41:19,610
acknowledgement so that's just an

897
00:41:16,940 --> 00:41:23,840
illustration of an example of this next

898
00:41:19,610 --> 00:41:25,670
slide please so finally the final

899
00:41:23,840 --> 00:41:26,390
mechanism I'm going to talk about is is

900
00:41:25,670 --> 00:41:29,450
some work

901
00:41:26,390 --> 00:41:31,460
to some work in progress that we have to

902
00:41:29,450 --> 00:41:35,689
reduce the queuing that we see when we

903
00:41:31,460 --> 00:41:37,100
have lots of flows so next slide so

904
00:41:35,690 --> 00:41:41,170
what's the scenario we're talking about

905
00:41:37,100 --> 00:41:43,549
here so this is a scenario that's been

906
00:41:41,170 --> 00:41:45,290
documented and and talked about for a

907
00:41:43,550 --> 00:41:48,080
quite a long time and I think it goes at

908
00:41:45,290 --> 00:41:50,630
least as far back as Robert Morris's

909
00:41:48,080 --> 00:41:52,940
paper on TCP behavior with many flows in

910
00:41:50,630 --> 00:41:56,000
1997 and maybe there are earlier papers

911
00:41:52,940 --> 00:41:59,410
as well but the basic issue is that when

912
00:41:56,000 --> 00:42:02,480
you've got mini flows and al-obeidi P

913
00:41:59,410 --> 00:42:04,730
then most existing widely deployed

914
00:42:02,480 --> 00:42:07,940
congestion control algorithms like DC

915
00:42:04,730 --> 00:42:11,360
TCP Kubik you know and BB are thus far

916
00:42:07,940 --> 00:42:13,190
tend to run window limited and they're

917
00:42:11,360 --> 00:42:14,810
gonna have a standing queue when the

918
00:42:13,190 --> 00:42:16,250
number of flows times the minimum

919
00:42:14,810 --> 00:42:20,029
congestion window of those flows is

920
00:42:16,250 --> 00:42:22,070
bigger than the PDP and the problem that

921
00:42:20,030 --> 00:42:24,890
you then get is that there's a standing

922
00:42:22,070 --> 00:42:26,840
queue that's basically that budget of

923
00:42:24,890 --> 00:42:29,390
the aggregate in flight the number of

924
00:42:26,840 --> 00:42:31,070
flows times minimum Sealand minus the

925
00:42:29,390 --> 00:42:33,770
bandwidth delay product and that can be

926
00:42:31,070 --> 00:42:36,290
fairly substantial and just as an aside

927
00:42:33,770 --> 00:42:38,650
here the to give some sense of scale the

928
00:42:36,290 --> 00:42:43,279
the B be our minimum C wins

929
00:42:38,650 --> 00:42:44,900
aside from RTO recovery is for segments

930
00:42:43,280 --> 00:42:47,930
and that's to avoid stop-and-wait

931
00:42:44,900 --> 00:42:51,350
behavior with TCP s every other packet

932
00:42:47,930 --> 00:42:52,910
policy so the diagram here sort of gives

933
00:42:51,350 --> 00:42:56,660
you an intuition about what's going on

934
00:42:52,910 --> 00:42:57,859
you can sort of see that the Q of excess

935
00:42:56,660 --> 00:42:59,390
packets here is building up at the

936
00:42:57,860 --> 00:43:02,210
bottleneck due to the aggregate

937
00:42:59,390 --> 00:43:04,549
in-flight being larger than the BGP of

938
00:43:02,210 --> 00:43:07,750
the path and all of these flows running

939
00:43:04,550 --> 00:43:10,340
when they limited next slide please

940
00:43:07,750 --> 00:43:14,300
so the solution that we've been

941
00:43:10,340 --> 00:43:16,670
investigating is to improve PBRs use of

942
00:43:14,300 --> 00:43:18,170
ecn and loss signals to make sure that

943
00:43:16,670 --> 00:43:19,910
we do a better job of adapting the

944
00:43:18,170 --> 00:43:21,590
pacing rate to match the available

945
00:43:19,910 --> 00:43:24,819
bandwidth and so there's sort of two

946
00:43:21,590 --> 00:43:27,020
core pieces of this the first is a

947
00:43:24,820 --> 00:43:30,620
multiplicative decrease in the pacing

948
00:43:27,020 --> 00:43:33,860
rate and in-flight data when we see ecn

949
00:43:30,620 --> 00:43:36,049
marks as well as loss and that allows us

950
00:43:33,860 --> 00:43:38,810
to sort of quickly match the available

951
00:43:36,050 --> 00:43:40,340
bandwidth and then drain the queue and

952
00:43:38,810 --> 00:43:43,759
also helps

953
00:43:40,340 --> 00:43:47,120
in converting converging toward a fair

954
00:43:43,760 --> 00:43:50,060
allocation bandwidth and then when the

955
00:43:47,120 --> 00:43:52,790
cue seems to be low enough based on the

956
00:43:50,060 --> 00:43:54,830
EC on signals we're getting we then do a

957
00:43:52,790 --> 00:43:57,740
small additive increase and then

958
00:43:54,830 --> 00:44:00,980
continue in the BB our bandwidth Pro

959
00:43:57,740 --> 00:44:03,859
beam growth curve and this is a paper

960
00:44:00,980 --> 00:44:06,530
this is a sort of approach that's been

961
00:44:03,860 --> 00:44:09,710
discussed before you know there's the

962
00:44:06,530 --> 00:44:12,260
the google timely paper from 2015

963
00:44:09,710 --> 00:44:14,570
discusses this kind of approach and I

964
00:44:12,260 --> 00:44:17,240
also believe Bob Brisco has done some

965
00:44:14,570 --> 00:44:19,820
work in this area for TCP Prague if I'm

966
00:44:17,240 --> 00:44:21,589
not mistaken and the picture here just

967
00:44:19,820 --> 00:44:23,690
sort of gives you an intuition about the

968
00:44:21,590 --> 00:44:26,660
the dynamics that we'd like to see where

969
00:44:23,690 --> 00:44:29,120
all of the flows are matching their

970
00:44:26,660 --> 00:44:31,790
pacing rate to the available bandwidth

971
00:44:29,120 --> 00:44:33,470
of the ebon link so you can try to

972
00:44:31,790 --> 00:44:35,890
reduce the queuing there at the

973
00:44:33,470 --> 00:44:39,290
bottleneck next slide please

974
00:44:35,890 --> 00:44:42,710
so this slide has some details about the

975
00:44:39,290 --> 00:44:44,960
the mechanisms that we are experimenting

976
00:44:42,710 --> 00:44:47,840
with in the PBR version two code base

977
00:44:44,960 --> 00:44:52,400
and the at the top there's a sort of

978
00:44:47,840 --> 00:44:55,010
summary of the core mechanism as ecn

979
00:44:52,400 --> 00:44:59,900
marks are arriving once per round trip

980
00:44:55,010 --> 00:45:03,130
time bbr v2 has thus far been reducing

981
00:44:59,900 --> 00:45:05,120
it's in fly cap or in-flight low

982
00:45:03,130 --> 00:45:06,950
multiplicatively based on an

983
00:45:05,120 --> 00:45:10,339
exponentially weighted moving average of

984
00:45:06,950 --> 00:45:14,470
the ECM mark rate and then for this

985
00:45:10,340 --> 00:45:17,810
experimental variation we are also

986
00:45:14,470 --> 00:45:19,490
capping the the pacing rate or bandwidth

987
00:45:17,810 --> 00:45:23,540
low as as we call it in the code base

988
00:45:19,490 --> 00:45:25,189
and decreasing that likewise and the

989
00:45:23,540 --> 00:45:28,220
other sort of pieces of the puzzle here

990
00:45:25,190 --> 00:45:31,600
as mentioned before we're cutting the

991
00:45:28,220 --> 00:45:35,870
the bandwidth low multiplicatively and

992
00:45:31,600 --> 00:45:37,940
we're as as another piece of this we're

993
00:45:35,870 --> 00:45:41,380
increasing the the ecn factor which is

994
00:45:37,940 --> 00:45:46,190
the sort of magnitude or scaling factor

995
00:45:41,380 --> 00:45:49,430
for scaling the ecn response into the

996
00:45:46,190 --> 00:45:52,580
pacing and in flight response and then

997
00:45:49,430 --> 00:45:54,089
we're also decoupling the bandwidth high

998
00:45:52,580 --> 00:45:57,240
parameter which was already

999
00:45:54,090 --> 00:45:59,580
in the in the BB rv2 model from the

1000
00:45:57,240 --> 00:46:01,709
maximum bandwidth sample that we've seen

1001
00:45:59,580 --> 00:46:03,480
so that we're sort of refining the

1002
00:46:01,710 --> 00:46:05,820
notion of this bandwidth high parameter

1003
00:46:03,480 --> 00:46:07,830
whereas before it was just the maximum

1004
00:46:05,820 --> 00:46:10,740
bandwidth that we'd seen recently and

1005
00:46:07,830 --> 00:46:12,720
now we're turning it into a more refined

1006
00:46:10,740 --> 00:46:14,609
notion that says it's tracking the

1007
00:46:12,720 --> 00:46:17,549
maximum bandwidth that seems consistent

1008
00:46:14,610 --> 00:46:21,000
with the tolerable level of EC on and

1009
00:46:17,550 --> 00:46:22,890
loss marks and if we're not seeing a

1010
00:46:21,000 --> 00:46:25,440
tolerable level of those signals then we

1011
00:46:22,890 --> 00:46:31,400
do a multiplicative cut in the parameter

1012
00:46:25,440 --> 00:46:35,430
and then finally when it seems like the

1013
00:46:31,400 --> 00:46:37,770
level of queuing is low enough based on

1014
00:46:35,430 --> 00:46:39,839
the EC on mark rate then we go ahead and

1015
00:46:37,770 --> 00:46:42,210
do a very small additive increase and

1016
00:46:39,840 --> 00:46:44,430
then continue on in the bandwidth

1017
00:46:42,210 --> 00:46:46,700
probing state machine so those are just

1018
00:46:44,430 --> 00:46:53,089
to give you a sense of the details and

1019
00:46:46,700 --> 00:46:57,540
next slide please so I'll present some

1020
00:46:53,090 --> 00:47:00,950
synthetic lab test results to give a

1021
00:46:57,540 --> 00:47:03,570
sense of the properties of the

1022
00:47:00,950 --> 00:47:05,490
experimental algorithm here here we have

1023
00:47:03,570 --> 00:47:09,270
two Center machines and one receiver all

1024
00:47:05,490 --> 00:47:12,330
on the same switch all with a 50 gigabit

1025
00:47:09,270 --> 00:47:15,840
Nick and the switch here is configured

1026
00:47:12,330 --> 00:47:18,240
to use DC TCP style ecn marking where

1027
00:47:15,840 --> 00:47:21,600
the congestion experienced bit is that

1028
00:47:18,240 --> 00:47:23,939
if and only if the instantaneous Q is

1029
00:47:21,600 --> 00:47:28,080
greater than 80 kilobytes in this

1030
00:47:23,940 --> 00:47:31,500
particular case and we've got 60 second

1031
00:47:28,080 --> 00:47:33,480
bulk knepper transfers here and we've

1032
00:47:31,500 --> 00:47:35,400
collected a number of typical metrics

1033
00:47:33,480 --> 00:47:36,930
which I'll go through as they talk about

1034
00:47:35,400 --> 00:47:38,580
the results and then when we're

1035
00:47:36,930 --> 00:47:41,279
comparing three different congestion

1036
00:47:38,580 --> 00:47:45,210
control variants here one is the Linux

1037
00:47:41,280 --> 00:47:48,690
DC TCP code the very latest and then one

1038
00:47:45,210 --> 00:47:51,210
is the VBR version 2 baseline which is

1039
00:47:48,690 --> 00:47:53,550
the algorithm without the changes I just

1040
00:47:51,210 --> 00:47:55,290
described and then BB are two new is the

1041
00:47:53,550 --> 00:47:58,550
algorithm with the changes I just

1042
00:47:55,290 --> 00:48:02,870
described next slide please

1043
00:47:58,550 --> 00:48:07,230
all right so here the results next slide

1044
00:48:02,870 --> 00:48:08,150
so the most interesting metric here in

1045
00:48:07,230 --> 00:48:11,029
these experiments

1046
00:48:08,150 --> 00:48:12,140
is that if we look at the queueing

1047
00:48:11,029 --> 00:48:14,750
pressure as reflected in the

1048
00:48:12,140 --> 00:48:18,710
retransmission rate as we scale up the

1049
00:48:14,750 --> 00:48:20,210
number of flows with notices we're sort

1050
00:48:18,710 --> 00:48:23,000
of Inc doubling the number of flows in

1051
00:48:20,210 --> 00:48:24,589
each experiment you can see with a small

1052
00:48:23,000 --> 00:48:26,809
number of flows all of the algorithms

1053
00:48:24,589 --> 00:48:30,859
are doing quite good at keeping

1054
00:48:26,809 --> 00:48:33,410
retransmission reloj and as more flows

1055
00:48:30,859 --> 00:48:35,750
are added there's sort of varying

1056
00:48:33,410 --> 00:48:38,058
degrees of retransmission rates with the

1057
00:48:35,750 --> 00:48:41,359
largest number of flows you can see d

1058
00:48:38,059 --> 00:48:45,170
c-- tcp because it's necessarily running

1059
00:48:41,359 --> 00:48:47,538
window limited tends to have a fairly

1060
00:48:45,170 --> 00:48:49,869
high retransmission rate around sixteen

1061
00:48:47,539 --> 00:48:53,960
percent in this experiment experiment

1062
00:48:49,869 --> 00:48:56,210
and the newer version of EVR is able to

1063
00:48:53,960 --> 00:48:58,190
keep the queuing pressure lower and has

1064
00:48:56,210 --> 00:49:03,490
a lower retransmission rate of around

1065
00:48:58,190 --> 00:49:06,140
1.6 percent and a next slide please

1066
00:49:03,490 --> 00:49:08,180
similarly if we look at other metrics

1067
00:49:06,140 --> 00:49:10,368
that reflect the queueing pressure we

1068
00:49:08,180 --> 00:49:12,259
can see that on the newer version of

1069
00:49:10,369 --> 00:49:14,869
BiBi ours is doing a slightly better job

1070
00:49:12,260 --> 00:49:20,599
at keeping the queues low this is the

1071
00:49:14,869 --> 00:49:23,690
average RT t next slide and here's the

1072
00:49:20,599 --> 00:49:26,990
95th percentile RT t it's sort of a

1073
00:49:23,690 --> 00:49:31,339
similar pattern next slide please

1074
00:49:26,990 --> 00:49:34,279
and then the throughputs are relatively

1075
00:49:31,339 --> 00:49:36,650
similar for the for the variance the

1076
00:49:34,279 --> 00:49:38,569
older version of e b r2 is not quite as

1077
00:49:36,650 --> 00:49:41,059
high as the others but otherwise they're

1078
00:49:38,569 --> 00:49:45,799
fairly similar next slide please

1079
00:49:41,059 --> 00:49:47,420
and then you can look at sort of the C e

1080
00:49:45,799 --> 00:49:50,569
mark rate is sort of an interesting way

1081
00:49:47,420 --> 00:49:52,970
to look at how good the algorithms are

1082
00:49:50,569 --> 00:49:58,069
doing at avoiding saturating the signal

1083
00:49:52,970 --> 00:49:59,629
they're using and you can see the PVR

1084
00:49:58,069 --> 00:50:02,630
the newer version of PBR is doing

1085
00:49:59,630 --> 00:50:06,500
reasonably well here next slide please

1086
00:50:02,630 --> 00:50:10,460
and then finally we also look at the

1087
00:50:06,500 --> 00:50:11,809
fairness the jain fairness index for

1088
00:50:10,460 --> 00:50:14,150
lips small number of flows that are all

1089
00:50:11,809 --> 00:50:16,789
doing quite well with larger number of

1090
00:50:14,150 --> 00:50:18,109
flows they're all not quite ideal and

1091
00:50:16,789 --> 00:50:21,680
there's definitely some room for

1092
00:50:18,109 --> 00:50:23,330
improvement and we'll be looking at

1093
00:50:21,680 --> 00:50:27,529
proving that but it looks to me like

1094
00:50:23,330 --> 00:50:29,930
their use ibly fair and all you know in

1095
00:50:27,530 --> 00:50:32,630
the same ballpark but there's some work

1096
00:50:29,930 --> 00:50:34,270
that we'll be doing in that area next

1097
00:50:32,630 --> 00:50:40,160
slide please

1098
00:50:34,270 --> 00:50:42,950
so wrapping up next slide so where are

1099
00:50:40,160 --> 00:50:45,589
we in terms of the status of the code so

1100
00:50:42,950 --> 00:50:48,200
the performance improvements that I was

1101
00:50:45,590 --> 00:50:51,740
just discussing we pushed those to the

1102
00:50:48,200 --> 00:50:55,069
Google will be our github repo you can

1103
00:50:51,740 --> 00:50:56,990
follow the link there for instructions

1104
00:50:55,070 --> 00:50:59,630
on how to check out and play with the

1105
00:50:56,990 --> 00:51:01,729
code as we mentioned in July and there's

1106
00:50:59,630 --> 00:51:03,470
also a bbr aversion to implementation

1107
00:51:01,730 --> 00:51:07,780
for quick and you can follow the URL

1108
00:51:03,470 --> 00:51:11,200
there and we again we encourage folks to

1109
00:51:07,780 --> 00:51:13,820
take the code for a spin try it out

1110
00:51:11,200 --> 00:51:16,120
share any test results they have our

1111
00:51:13,820 --> 00:51:19,130
packet traces share any ideas about

1112
00:51:16,120 --> 00:51:22,540
issues they run into or ways to improve

1113
00:51:19,130 --> 00:51:25,280
the code we're always looking for a

1114
00:51:22,540 --> 00:51:27,800
feedback from the community and then we

1115
00:51:25,280 --> 00:51:29,450
have links to the slides and video from

1116
00:51:27,800 --> 00:51:34,400
a previous IDF's

1117
00:51:29,450 --> 00:51:35,720
on next slide please so where are we in

1118
00:51:34,400 --> 00:51:38,900
terms of the deployment status

1119
00:51:35,720 --> 00:51:40,609
we've got PBR version two running for a

1120
00:51:38,900 --> 00:51:43,910
small percentage of users on YouTube

1121
00:51:40,610 --> 00:51:46,760
where we see lower queuing delays than

1122
00:51:43,910 --> 00:51:49,549
both PBR version one and cubic and

1123
00:51:46,760 --> 00:51:52,970
considerably reduced packet loss closer

1124
00:51:49,550 --> 00:51:56,690
to cubic than VB R version one and then

1125
00:51:52,970 --> 00:52:00,319
we're in the process of a large-scale

1126
00:51:56,690 --> 00:52:02,330
test pilot program pushing BB r v2 out

1127
00:52:00,320 --> 00:52:05,810
as the default congestion control for

1128
00:52:02,330 --> 00:52:08,420
TCP within and between Google Data

1129
00:52:05,810 --> 00:52:10,670
Centers and we're seeing some nice

1130
00:52:08,420 --> 00:52:13,100
results there and we're continuing to

1131
00:52:10,670 --> 00:52:18,440
iterate both in production and lab tests

1132
00:52:13,100 --> 00:52:20,600
next slide please so finally in summary

1133
00:52:18,440 --> 00:52:23,900
we're actively working on VB our version

1134
00:52:20,600 --> 00:52:26,839
2 at Google and doing some tuning of

1135
00:52:23,900 --> 00:52:29,150
performance especially in data center

1136
00:52:26,840 --> 00:52:32,980
contexts high-speed data center networks

1137
00:52:29,150 --> 00:52:34,840
so that we can enable VB our version 2

1138
00:52:32,980 --> 00:52:37,360
globally at

1139
00:52:34,840 --> 00:52:39,160
Google for internal traffic and we're

1140
00:52:37,360 --> 00:52:40,900
working as you saw on improving the

1141
00:52:39,160 --> 00:52:43,270
algorithm to scale to larger numbers of

1142
00:52:40,900 --> 00:52:45,400
flows and as always we invite the

1143
00:52:43,270 --> 00:52:48,250
community to share any test results or

1144
00:52:45,400 --> 00:52:50,170
issues or patches or ideas they have we

1145
00:52:48,250 --> 00:52:53,530
also wanted to give a shout out to the

1146
00:52:50,170 --> 00:52:55,540
freebsd tcp community and the team at

1147
00:52:53,530 --> 00:52:58,180
netflix which has been working on bbr

1148
00:52:55,540 --> 00:53:01,270
and they actually released a an ability

1149
00:52:58,180 --> 00:53:03,850
shinobi be our version one for FreeBSD a

1150
00:53:01,270 --> 00:53:05,530
couple of weeks ago and we wanted to

1151
00:53:03,850 --> 00:53:07,750
finally leave you with some food for

1152
00:53:05,530 --> 00:53:11,800
thought it's sort of an interesting

1153
00:53:07,750 --> 00:53:14,530
question of if you have to design a KPI

1154
00:53:11,800 --> 00:53:16,930
or key performance indicator for

1155
00:53:14,530 --> 00:53:17,980
congestion control what would that look

1156
00:53:16,930 --> 00:53:21,220
like

1157
00:53:17,980 --> 00:53:24,010
basically the question is how would you

1158
00:53:21,220 --> 00:53:26,230
tell if your congestion control is doing

1159
00:53:24,010 --> 00:53:28,270
well in a production environment given

1160
00:53:26,230 --> 00:53:30,370
that in a production environment the

1161
00:53:28,270 --> 00:53:32,650
traffic is dynamic the routing is

1162
00:53:30,370 --> 00:53:35,680
dynamic topologies are dynamic and so

1163
00:53:32,650 --> 00:53:39,640
it's it's it's non-trivial as far as we

1164
00:53:35,680 --> 00:53:41,770
can tell to to develop a good key

1165
00:53:39,640 --> 00:53:45,400
performance indicator and we encourage

1166
00:53:41,770 --> 00:53:47,440
folks to tackle that as a research

1167
00:53:45,400 --> 00:53:50,130
problem if they're interested we think

1168
00:53:47,440 --> 00:53:53,290
it's a valuable area for contributions

1169
00:53:50,130 --> 00:54:02,860
so thank you very much any questions or

1170
00:53:53,290 --> 00:54:06,700
comments I I new mardukas

1171
00:54:02,860 --> 00:54:09,730
thanks for giving up your Sunday night I

1172
00:54:06,700 --> 00:54:12,549
have two questions one is is there a

1173
00:54:09,730 --> 00:54:15,250
plan to update the BB r Draft at some

1174
00:54:12,550 --> 00:54:17,860
point we're trying to track with an

1175
00:54:15,250 --> 00:54:19,810
implementation I know obviously Netflix

1176
00:54:17,860 --> 00:54:25,420
as well we really nice them a fixed

1177
00:54:19,810 --> 00:54:27,850
target yeah an update of the internet

1178
00:54:25,420 --> 00:54:30,130
draft to reflect version two is

1179
00:54:27,850 --> 00:54:32,230
absolutely on the to-do list and I

1180
00:54:30,130 --> 00:54:35,590
apologize we weren't able to get it done

1181
00:54:32,230 --> 00:54:37,300
by this ITF but rest assured it's

1182
00:54:35,590 --> 00:54:40,090
definitely on the to-do list

1183
00:54:37,300 --> 00:54:42,790
yeah okay thanks the second can you

1184
00:54:40,090 --> 00:54:53,920
return to the sequence number graph you

1185
00:54:42,790 --> 00:54:55,800
had yeah so um maybe I'm missing

1186
00:54:53,920 --> 00:54:59,710
something here so this strikes me is a

1187
00:54:55,800 --> 00:55:01,480
for certainly for legacy t speak gesture

1188
00:54:59,710 --> 00:55:05,020
controls this is a feature not a bug

1189
00:55:01,480 --> 00:55:07,180
like you're not getting pulling more

1190
00:55:05,020 --> 00:55:10,360
data out of the system when the app is

1191
00:55:07,180 --> 00:55:12,279
not able to consume it is this a problem

1192
00:55:10,360 --> 00:55:14,260
in bbr just because the act pacing

1193
00:55:12,280 --> 00:55:19,420
messes up your bbr or your bandwidth

1194
00:55:14,260 --> 00:55:23,350
estimation no I don't think so the it

1195
00:55:19,420 --> 00:55:30,540
it's it turned out to be a problem in

1196
00:55:23,350 --> 00:55:33,190
practice in these RPC workloads and it's

1197
00:55:30,540 --> 00:55:37,660
you know it showed up in the detail

1198
00:55:33,190 --> 00:55:38,920
Layton sees for one of their storage

1199
00:55:37,660 --> 00:55:41,890
applications that we're benching

1200
00:55:38,920 --> 00:55:46,290
benchmarking so we could definitely go

1201
00:55:41,890 --> 00:55:50,410
into detail offline if would be helpful

1202
00:55:46,290 --> 00:55:52,660
but yeah not sure what else to add it at

1203
00:55:50,410 --> 00:55:54,310
this point okay well I mean I guess III

1204
00:55:52,660 --> 00:55:56,680
think you were proposing that maybe this

1205
00:55:54,310 --> 00:55:58,660
should just be a blanket changed TCP to

1206
00:55:56,680 --> 00:56:02,589
remove that second condition about the

1207
00:55:58,660 --> 00:56:06,069
window increase and I I don't have a lot

1208
00:56:02,590 --> 00:56:07,840
of experiment remember why exactly that

1209
00:56:06,070 --> 00:56:11,530
was put in but it strikes me as it as a

1210
00:56:07,840 --> 00:56:15,400
fairly obvious thing for for the reason

1211
00:56:11,530 --> 00:56:16,470
I just said which is to not just pile

1212
00:56:15,400 --> 00:56:18,750
more stuff on

1213
00:56:16,470 --> 00:56:23,368
on a backlog client on a backlog

1214
00:56:18,750 --> 00:56:27,300
receiver right I mean I think there are

1215
00:56:23,369 --> 00:56:29,339
probably a number of trade offs here and

1216
00:56:27,300 --> 00:56:32,369
I guess it you know at the end of the

1217
00:56:29,339 --> 00:56:33,980
day maybe it's something that is going

1218
00:56:32,369 --> 00:56:37,050
to be context dependent whether it's a

1219
00:56:33,980 --> 00:56:38,750
an improvement or not and so that's you

1220
00:56:37,050 --> 00:56:40,770
know why we haven't offered it

1221
00:56:38,750 --> 00:56:42,630
immediately as a fix for upstream we

1222
00:56:40,770 --> 00:56:45,980
want to get some experience with it in

1223
00:56:42,630 --> 00:56:49,140
production workloads and see if it is

1224
00:56:45,980 --> 00:56:50,430
you know see if it is a net win we think

1225
00:56:49,140 --> 00:56:54,118
that you have good reason to believe

1226
00:56:50,430 --> 00:56:56,848
that it will be an at least acceptable

1227
00:56:54,119 --> 00:56:59,760
if not an improvement in every case

1228
00:56:56,849 --> 00:57:02,190
because we the the previous congestion

1229
00:56:59,760 --> 00:57:08,150
control that was deployed in Google

1230
00:57:02,190 --> 00:57:10,859
actually had a de facto behavior that in

1231
00:57:08,150 --> 00:57:13,800
congested cases actually did essentially

1232
00:57:10,859 --> 00:57:15,660
bypass check number two so we do have a

1233
00:57:13,800 --> 00:57:18,480
fair amount of experience at least in

1234
00:57:15,660 --> 00:57:20,790
the congested case with it helping but

1235
00:57:18,480 --> 00:57:22,859
you know well reserve final judgment

1236
00:57:20,790 --> 00:57:24,779
until we've pushed this for all of our

1237
00:57:22,859 --> 00:57:29,160
workloads and and we don't see any

1238
00:57:24,780 --> 00:57:30,599
regressions thanks Neal right there just

1239
00:57:29,160 --> 00:57:33,000
to ask a follow-up question do you know

1240
00:57:30,599 --> 00:57:37,349
why that second condition was was

1241
00:57:33,000 --> 00:57:38,790
instead in the kernel right now we don't

1242
00:57:37,349 --> 00:57:42,420
know for sure I mean as Martin said

1243
00:57:38,790 --> 00:57:45,599
there there is an argument that you can

1244
00:57:42,420 --> 00:57:48,180
make about trying not to overload the

1245
00:57:45,599 --> 00:57:52,920
receiver another argument that Eric

1246
00:57:48,180 --> 00:57:56,879
dooms a pointed out was that this check

1247
00:57:52,920 --> 00:58:00,060
works nicely with the underside fast

1248
00:57:56,880 --> 00:58:03,990
path which has only taken if the receive

1249
00:58:00,060 --> 00:58:06,990
window is is constant and so this can

1250
00:58:03,990 --> 00:58:10,439
save some CPU on the sender side if they

1251
00:58:06,990 --> 00:58:13,919
received window stays constant yeah

1252
00:58:10,440 --> 00:58:15,930
but where this this check predates they

1253
00:58:13,920 --> 00:58:18,599
get history in Linux so it's a little a

1254
00:58:15,930 --> 00:58:22,410
little tough to talk Wow

1255
00:58:18,599 --> 00:58:24,210
all right well thank you so I'm I'm very

1256
00:58:22,410 --> 00:58:25,740
amused of course because my question was

1257
00:58:24,210 --> 00:58:29,710
what was condition to expected to help

1258
00:58:25,740 --> 00:58:31,299
solve Roberto who are you hi

1259
00:58:29,710 --> 00:58:38,470
I'm Roberto and I'm with Facebook right

1260
00:58:31,300 --> 00:58:39,790
now and III you know one comment is say

1261
00:58:38,470 --> 00:58:43,870
hi to everybody

1262
00:58:39,790 --> 00:58:46,870
the other comment I love the data-driven

1263
00:58:43,870 --> 00:58:49,779
approach and in cases where there is no

1264
00:58:46,870 --> 00:58:52,480
clear answer why I'm really hoping that

1265
00:58:49,780 --> 00:58:55,210
you will be able to drive changes in

1266
00:58:52,480 --> 00:58:57,430
behavior this particular one with number

1267
00:58:55,210 --> 00:59:01,110
two here seems exceptionally odd to me

1268
00:58:57,430 --> 00:59:04,299
because it it it provides a cliff of

1269
00:59:01,110 --> 00:59:07,420
behavior and we know cliffs and in

1270
00:59:04,300 --> 00:59:08,710
control theory tends to be bad for being

1271
00:59:07,420 --> 00:59:11,860
able to understand the systemic

1272
00:59:08,710 --> 00:59:14,080
properties so I think it's super awesome

1273
00:59:11,860 --> 00:59:16,210
to try and either eliminate to or prove

1274
00:59:14,080 --> 00:59:18,190
that there's a good reason for it to

1275
00:59:16,210 --> 00:59:27,190
exist given that nobody seems to know

1276
00:59:18,190 --> 00:59:28,300
why so anyway thank you thank you a

1277
00:59:27,190 --> 00:59:31,060
couple of questions

1278
00:59:28,300 --> 00:59:33,370
so the minimum congestion window for so

1279
00:59:31,060 --> 00:59:35,740
TCP traditionally had that value one

1280
00:59:33,370 --> 00:59:36,910
matter standing is quick is using two

1281
00:59:35,740 --> 00:59:38,169
because of the problem that you

1282
00:59:36,910 --> 00:59:41,520
mentioned in the slide I'm just curious

1283
00:59:38,170 --> 00:59:45,520
why you would pick four instead of two

1284
00:59:41,520 --> 00:59:48,820
so the details have to do with the

1285
00:59:45,520 --> 00:59:51,370
dynamics that you get into given the

1286
00:59:48,820 --> 00:59:55,840
delayed act behavior so the basic idea

1287
00:59:51,370 --> 00:59:58,930
is that you want to have four packets in

1288
00:59:55,840 --> 01:00:00,610
flight so that two of those packets well

1289
00:59:58,930 --> 01:00:02,049
the simplest scenario to imagine is

1290
01:00:00,610 --> 01:00:04,600
basically that two of those packets are

1291
01:00:02,050 --> 01:00:07,000
in flight in the data transmission

1292
01:00:04,600 --> 01:00:09,520
direction and because you have two of

1293
01:00:07,000 --> 01:00:13,900
them that's going to force out quickly

1294
01:00:09,520 --> 01:00:15,700
quickly force out a delayed ACK or

1295
01:00:13,900 --> 01:00:17,950
quickly force out in a community rather

1296
01:00:15,700 --> 01:00:21,700
than delaying the ACK rather so then

1297
01:00:17,950 --> 01:00:24,069
you've got also two a two packets worth

1298
01:00:21,700 --> 01:00:25,960
of budget coming back in the reverse

1299
01:00:24,070 --> 01:00:29,200
direction in the form of acknowledgments

1300
01:00:25,960 --> 01:00:31,240
so that those packets when the windows

1301
01:00:29,200 --> 01:00:33,460
acts when they arrive at the sender they

1302
01:00:31,240 --> 01:00:35,770
can immediately release the next two

1303
01:00:33,460 --> 01:00:38,560
data packets and you basically want to

1304
01:00:35,770 --> 01:00:42,130
have all all four packets worth of

1305
01:00:38,560 --> 01:00:43,120
budget in flight so that there's always

1306
01:00:42,130 --> 01:00:45,070
data going

1307
01:00:43,120 --> 01:00:46,930
in the transmission direction and always

1308
01:00:45,070 --> 01:00:49,600
acts flowing in the reverse direction

1309
01:00:46,930 --> 01:00:52,540
and the only way you can sort of ensure

1310
01:00:49,600 --> 01:00:55,180
that that happens is if if the receiver

1311
01:00:52,540 --> 01:00:59,110
is acknowledging every other packet is

1312
01:00:55,180 --> 01:01:04,870
to have 4x or sorry 4 packets worth of

1313
01:00:59,110 --> 01:01:09,310
budget in-flight on and yeah so

1314
01:01:04,870 --> 01:01:10,750
obviously there's that does increase the

1315
01:01:09,310 --> 01:01:13,060
aggregate in-flight in these sorts of

1316
01:01:10,750 --> 01:01:14,890
scenarios but the I think architectural

1317
01:01:13,060 --> 01:01:17,850
II my sense would be that a better

1318
01:01:14,890 --> 01:01:21,540
solution is ensuring that the flows

1319
01:01:17,850 --> 01:01:24,900
tends to run pacing limited because

1320
01:01:21,540 --> 01:01:27,610
that's a more complete solution and

1321
01:01:24,900 --> 01:01:28,810
you're always going to have there's no

1322
01:01:27,610 --> 01:01:30,490
matter what your minimum sea wind is

1323
01:01:28,810 --> 01:01:32,680
there's always going to be if you're

1324
01:01:30,490 --> 01:01:35,740
running window there's always going to

1325
01:01:32,680 --> 01:01:37,930
be some number of flows and some B DPS

1326
01:01:35,740 --> 01:01:40,330
at which you get into trouble so it

1327
01:01:37,930 --> 01:01:43,540
seems like a more general fix is to to

1328
01:01:40,330 --> 01:01:46,600
run pacing limited with a C a minimum C

1329
01:01:43,540 --> 01:01:48,640
wind that's robust ok I'm gonna ask

1330
01:01:46,600 --> 01:01:51,370
folks to be super quick because we are

1331
01:01:48,640 --> 01:01:54,129
well over time at this point and I'm

1332
01:01:51,370 --> 01:01:57,730
closing the lines one more question was

1333
01:01:54,130 --> 01:01:59,680
the target loss rate so currently how do

1334
01:01:57,730 --> 01:02:01,750
you determine that for various workloads

1335
01:01:59,680 --> 01:02:05,680
to do pickup constant value or do you

1336
01:02:01,750 --> 01:02:09,880
tune it based on some parameters right

1337
01:02:05,680 --> 01:02:13,870
so far we would like well so far we are

1338
01:02:09,880 --> 01:02:16,680
using a constant value in our deployment

1339
01:02:13,870 --> 01:02:21,100
but we are leaving the door open to

1340
01:02:16,680 --> 01:02:24,910
making that something that could be

1341
01:02:21,100 --> 01:02:27,970
conceivably tuned by deployments for

1342
01:02:24,910 --> 01:02:31,020
example if you have a background file

1343
01:02:27,970 --> 01:02:35,140
transfer like a less than best-effort

1344
01:02:31,020 --> 01:02:39,580
transfer you could that is isolated to

1345
01:02:35,140 --> 01:02:42,100
its own cue in your quality of service

1346
01:02:39,580 --> 01:02:43,930
implementation in your switches you

1347
01:02:42,100 --> 01:02:49,920
might want to tolerate a higher loss

1348
01:02:43,930 --> 01:02:54,310
rate for example but yeah the we the

1349
01:02:49,920 --> 01:02:56,800
final loss threshold that we'd like to

1350
01:02:54,310 --> 01:02:59,740
target for the public Internet sort of

1351
01:02:56,800 --> 01:03:03,750
yet to be determined and we're open to

1352
01:02:59,740 --> 01:03:06,810
ideas from the community about that yeah

1353
01:03:03,750 --> 01:03:06,810
thank you

1354
01:03:07,480 --> 01:03:11,410
I came up it when you put up the thing

1355
01:03:09,970 --> 01:03:14,078
about key performance indicators which

1356
01:03:11,410 --> 01:03:17,410
was quite a time ago now just that just

1357
01:03:14,079 --> 01:03:20,470
to say to people go and have a look at

1358
01:03:17,410 --> 01:03:23,589
there's a hot Nets paper in on its this

1359
01:03:20,470 --> 01:03:27,368
year yeah called beyond Jane's fairness

1360
01:03:23,589 --> 01:03:28,750
index which is quite interesting sort of

1361
01:03:27,369 --> 01:03:30,250
thoughts on how to do that being hot

1362
01:03:28,750 --> 01:03:34,000
Nets obviously it's not completed work

1363
01:03:30,250 --> 01:03:35,500
but it's more about measuring the harm

1364
01:03:34,000 --> 01:03:41,109
one flow causes others rather than

1365
01:03:35,500 --> 01:03:43,540
equality and they'll be on that

1366
01:03:41,109 --> 01:03:47,529
conversation about packet windows and

1367
01:03:43,540 --> 01:03:50,579
things like that I'm gonna just forward

1368
01:03:47,530 --> 01:03:54,010
reference to the slot about TCP Prague

1369
01:03:50,579 --> 01:03:58,540
Assad's finished his and defended his

1370
01:03:54,010 --> 01:04:01,060
thesis in September on that work and the

1371
01:03:58,540 --> 01:04:03,790
code will be available hopefully before

1372
01:04:01,060 --> 01:04:06,880
Thursday and well it I mean it's it's

1373
01:04:03,790 --> 01:04:09,220
completed but just chickened out on

1374
01:04:06,880 --> 01:04:11,349
everything and it I'll give a link to

1375
01:04:09,220 --> 01:04:12,790
his thesis and things that we we did

1376
01:04:11,349 --> 01:04:15,369
manage to get it working with a window

1377
01:04:12,790 --> 01:04:18,430
based algorithm inside the base TCP

1378
01:04:15,369 --> 01:04:20,140
stack in Linux quite quite nicely but

1379
01:04:18,430 --> 01:04:23,020
there are other ways you can think about

1380
01:04:20,140 --> 01:04:26,250
it so I'd like to hear when we talk

1381
01:04:23,020 --> 01:04:29,500
about it on Thursday more about it okay

1382
01:04:26,250 --> 01:04:31,030
tests on that particular paper but

1383
01:04:29,500 --> 01:04:34,349
you're gonna have that presentation in

1384
01:04:31,030 --> 01:04:37,480
Vancouver I think the Hartman speaker

1385
01:04:34,349 --> 01:04:39,730
initiative I have very simple question

1386
01:04:37,480 --> 01:04:43,390
so you mentioned about the documenting

1387
01:04:39,730 --> 01:04:46,480
baby I'll be too but I might be wrong

1388
01:04:43,390 --> 01:04:48,690
but my impression is birria bytes

1389
01:04:46,480 --> 01:04:51,730
consists of lots of small hello

1390
01:04:48,690 --> 01:04:53,890
complicated Rosic and then some

1391
01:04:51,730 --> 01:04:59,020
sounds like our implementation tricks

1392
01:04:53,890 --> 01:05:01,450
and then so I'm wondering how do you are

1393
01:04:59,020 --> 01:05:07,660
you going to document spec over the

1394
01:05:01,450 --> 01:05:10,029
b-b-b our beta I yes we as we mentioned

1395
01:05:07,660 --> 01:05:12,790
with when Martin Duke

1396
01:05:10,030 --> 01:05:14,950
came to the mic we definitely intend to

1397
01:05:12,790 --> 01:05:23,200
document the algorithm in an internet

1398
01:05:14,950 --> 01:05:26,620
draft yeah how you know strike a hospice

1399
01:05:23,200 --> 01:05:29,310
think something like Fast Pass seems to

1400
01:05:26,620 --> 01:05:33,700
be an implementation trick from my sure

1401
01:05:29,310 --> 01:05:35,320
right so that is not something that

1402
01:05:33,700 --> 01:05:37,060
would necessarily need to be in the

1403
01:05:35,320 --> 01:05:39,490
draft because as you say that's an

1404
01:05:37,060 --> 01:05:45,250
implementation detail

1405
01:05:39,490 --> 01:05:47,470
I think the much as the you know the

1406
01:05:45,250 --> 01:05:50,830
traditional TCP fast path is also not in

1407
01:05:47,470 --> 01:05:52,839
the the RFC 568 Lauren's back I don't

1408
01:05:50,830 --> 01:05:58,500
think that the VR fast path would be in

1409
01:05:52,840 --> 01:06:01,350
the internet draft but I do think it's

1410
01:05:58,500 --> 01:06:05,470
you know we definitely intend to to

1411
01:06:01,350 --> 01:06:09,339
document the the high level algorithm in

1412
01:06:05,470 --> 01:06:11,680
the internet draft yeah if the

1413
01:06:09,340 --> 01:06:14,920
implementation Yoshi we we have to move

1414
01:06:11,680 --> 01:06:17,740
on I'm sorry Jake we've closed I close

1415
01:06:14,920 --> 01:06:19,300
the lines earlier okay last question if

1416
01:06:17,740 --> 01:06:20,859
you have you are still in the line

1417
01:06:19,300 --> 01:06:23,680
before it closed it if you have you have

1418
01:06:20,860 --> 01:06:26,530
ten seconds okay yeah John's Norton I'm

1419
01:06:23,680 --> 01:06:30,910
just wanting to ask whether you cope

1420
01:06:26,530 --> 01:06:34,800
well with our cc-16 eight style ecn for

1421
01:06:30,910 --> 01:06:37,379
example if a TC and is not negotiated uh

1422
01:06:34,800 --> 01:06:42,100
that's far we're not planning on

1423
01:06:37,380 --> 01:06:47,410
integrating bbr with RFC three 168 style

1424
01:06:42,100 --> 01:06:52,060
ecn we expect that thus far people using

1425
01:06:47,410 --> 01:06:55,990
VBR would not negotiate three 168 ecn

1426
01:06:52,060 --> 01:07:00,880
that's that's the current plan thank you

1427
01:06:55,990 --> 01:07:03,759
so much I am going to thank mean for

1428
01:07:00,880 --> 01:07:06,100
staying up late and doing this and I am

1429
01:07:03,760 --> 01:07:07,240
hoping that we'll see the draft sooner

1430
01:07:06,100 --> 01:07:09,690
than later

1431
01:07:07,240 --> 01:07:11,319
but we should talk offline about that

1432
01:07:09,690 --> 01:07:13,540
especially now that we're talking about

1433
01:07:11,320 --> 01:07:17,820
adoption of documents in this in his

1434
01:07:13,540 --> 01:07:19,900
research group and yeah thank you knew

1435
01:07:17,820 --> 01:07:23,020
thank you

1436
01:07:19,900 --> 01:07:23,460
I'm going to now handoff to Nathan Gert

1437
01:07:23,020 --> 01:07:25,140
from

1438
01:07:23,460 --> 01:07:35,580
Facebook who's going to talk about their

1439
01:07:25,140 --> 01:07:37,980
experiences with Coppa hello everyone

1440
01:07:35,580 --> 01:07:41,450
can you guys hear me should I move

1441
01:07:37,980 --> 01:07:45,960
closer or further away close is better

1442
01:07:41,450 --> 01:07:48,270
okay so yeah little bit about myself

1443
01:07:45,960 --> 01:07:51,300
my name is Nathan GUG as you can see

1444
01:07:48,270 --> 01:07:53,340
it's my first ITF here and I'm super

1445
01:07:51,300 --> 01:07:56,160
excited to present some of the work we

1446
01:07:53,340 --> 01:07:58,230
did at Facebook comparing Coppa with

1447
01:07:56,160 --> 01:08:00,960
cubic and VBR for live video upload

1448
01:07:58,230 --> 01:08:05,340
use-case I work in the videos

1449
01:08:00,960 --> 01:08:06,840
infrastructure team at Facebook this is

1450
01:08:05,340 --> 01:08:08,970
the rough outline of my presentation

1451
01:08:06,840 --> 01:08:11,160
I'll start with the motivation for our

1452
01:08:08,970 --> 01:08:13,680
work then I'll go over the experiment

1453
01:08:11,160 --> 01:08:16,050
setup we used and the results we saw on

1454
01:08:13,680 --> 01:08:18,660
the application side then I'll do a

1455
01:08:16,050 --> 01:08:20,340
brief overview of Coppa how that works

1456
01:08:18,660 --> 01:08:21,899
although that is not the focus of this

1457
01:08:20,340 --> 01:08:24,720
presentation but I think it's important

1458
01:08:21,899 --> 01:08:26,910
to go over our understanding of Coppa

1459
01:08:24,720 --> 01:08:28,620
and how we applied it and then Indian

1460
01:08:26,910 --> 01:08:32,010
I'll go over the conclusion and the

1461
01:08:28,620 --> 01:08:34,859
future work that we apply so motivation

1462
01:08:32,010 --> 01:08:36,600
in videos infrastructure team we work

1463
01:08:34,859 --> 01:08:38,549
with a lot of product teams who work on

1464
01:08:36,600 --> 01:08:40,590
videos and give realized the different

1465
01:08:38,550 --> 01:08:42,360
type of video experiences require

1466
01:08:40,590 --> 01:08:44,940
different quality and latency trade-off

1467
01:08:42,359 --> 01:08:47,759
so on one end of the spectrum you have

1468
01:08:44,939 --> 01:08:49,679
applications such as video calling where

1469
01:08:47,760 --> 01:08:51,840
the end-to-end glass to glass latency is

1470
01:08:49,680 --> 01:08:53,760
super important and these applications

1471
01:08:51,840 --> 01:08:55,500
are usually willing to tolerate some

1472
01:08:53,760 --> 01:08:58,200
quality loss in order to get that

1473
01:08:55,500 --> 01:08:59,910
desired latency on the other end of the

1474
01:08:58,200 --> 01:09:01,859
spectrum you have applications which

1475
01:08:59,910 --> 01:09:02,939
don't care as much about latency but

1476
01:09:01,859 --> 01:09:06,179
they require extremely high quality

1477
01:09:02,939 --> 01:09:08,189
playback and then there are applications

1478
01:09:06,180 --> 01:09:10,470
which falling between which are kind of

1479
01:09:08,189 --> 01:09:12,599
like yeah we need low latency but we

1480
01:09:10,470 --> 01:09:14,700
also need high quality playback so

1481
01:09:12,600 --> 01:09:16,260
normally you would need different type

1482
01:09:14,700 --> 01:09:18,149
of condition control algorithms

1483
01:09:16,260 --> 01:09:20,700
compatible with different trade of

1484
01:09:18,149 --> 01:09:23,490
scenarios what we wanted to find out was

1485
01:09:20,700 --> 01:09:25,229
is it possible for a single congestion

1486
01:09:23,490 --> 01:09:27,179
control algorithm to offer a dial to the

1487
01:09:25,229 --> 01:09:30,269
application and the application could

1488
01:09:27,180 --> 01:09:33,150
use that dial to just make it compatible

1489
01:09:30,270 --> 01:09:35,310
with the desired quality versus latency

1490
01:09:33,149 --> 01:09:37,028
trade-off this is where we evaluated

1491
01:09:35,310 --> 01:09:38,799
Coppa which is a delay based

1492
01:09:37,029 --> 01:09:41,199
condition control algorithm it has a

1493
01:09:38,799 --> 01:09:43,238
dial which is in the form of parameter

1494
01:09:41,198 --> 01:09:44,799
called Delta which you can use to

1495
01:09:43,238 --> 01:09:48,729
control the delay sensitivity of the

1496
01:09:44,799 --> 01:09:50,049
algorithm so in our experiment setup we

1497
01:09:48,729 --> 01:09:52,269
wanted to start with something so we

1498
01:09:50,049 --> 01:09:54,038
started with the big one extreme where

1499
01:09:52,270 --> 01:09:56,710
we are optimizing for throughput at the

1500
01:09:54,038 --> 01:09:59,139
expense of delay so we tune Copa to

1501
01:09:56,710 --> 01:10:01,360
optimize for throughput and we did some

1502
01:09:59,139 --> 01:10:03,610
comparisons with bbr and cubic which

1503
01:10:01,360 --> 01:10:07,750
were which are two popular algorithms

1504
01:10:03,610 --> 01:10:09,670
available and a for testing we use the

1505
01:10:07,750 --> 01:10:11,409
Facebook live streaming application so

1506
01:10:09,670 --> 01:10:13,719
in this application any user with their

1507
01:10:11,409 --> 01:10:15,789
mobile phone they can go live they can

1508
01:10:13,719 --> 01:10:18,909
broadcast their live feed to their

1509
01:10:15,789 --> 01:10:20,530
friends and followers and one thing to

1510
01:10:18,909 --> 01:10:23,379
note here is that like this is a very

1511
01:10:20,530 --> 01:10:26,289
this is very different scenario as

1512
01:10:23,380 --> 01:10:28,750
compared to HTTP style traffic HTTP

1513
01:10:26,289 --> 01:10:30,550
traffic would be short and burst years

1514
01:10:28,750 --> 01:10:32,860
well could be app limited a lot of times

1515
01:10:30,550 --> 01:10:34,570
as well in this case we have

1516
01:10:32,860 --> 01:10:37,089
long-running flows over a single quick

1517
01:10:34,570 --> 01:10:38,799
connection no multiplexing with the mean

1518
01:10:37,090 --> 01:10:40,900
duration of around 3 minutes so you

1519
01:10:38,800 --> 01:10:42,599
always have a constant flow of data like

1520
01:10:40,900 --> 01:10:44,710
which you are pushing with the wire

1521
01:10:42,599 --> 01:10:46,719
other a different thing about this

1522
01:10:44,710 --> 01:10:47,860
application is there is an adaptive bit

1523
01:10:46,719 --> 01:10:49,659
rate algorithm running in the

1524
01:10:47,860 --> 01:10:51,549
application which would change the

1525
01:10:49,659 --> 01:10:53,650
encoded bit rate in response to the

1526
01:10:51,550 --> 01:10:54,880
network conditions such an example if it

1527
01:10:53,650 --> 01:10:56,949
sees that the network queues are

1528
01:10:54,880 --> 01:10:59,440
building up and the throughput or the

1529
01:10:56,949 --> 01:11:01,178
goodput it's seeing is lower then it

1530
01:10:59,440 --> 01:11:03,400
could change the encoder to produce less

1531
01:11:01,179 --> 01:11:05,440
bytes and hence that which that will

1532
01:11:03,400 --> 01:11:09,879
result in less bytes being written on

1533
01:11:05,440 --> 01:11:11,409
the wire and we implemented a coupon in

1534
01:11:09,880 --> 01:11:13,809
the Facebook quick library like quick

1535
01:11:11,409 --> 01:11:16,570
because the transport is in user space

1536
01:11:13,809 --> 01:11:19,329
it allowed us to implement Coppa quickly

1537
01:11:16,570 --> 01:11:21,190
and experiment with it quickly and we

1538
01:11:19,329 --> 01:11:22,840
use the bbr in cubic implementations

1539
01:11:21,190 --> 01:11:24,789
which were already implemented and

1540
01:11:22,840 --> 01:11:29,380
deployed for several other use cases

1541
01:11:24,789 --> 01:11:31,809
with quick so for conducting the

1542
01:11:29,380 --> 01:11:34,030
experiment we used facebook's eb testing

1543
01:11:31,809 --> 01:11:35,889
framework so this framework allowed us

1544
01:11:34,030 --> 01:11:38,469
to conduct this experiment all over the

1545
01:11:35,889 --> 01:11:41,320
world this framework

1546
01:11:38,469 --> 01:11:44,050
helped us divide users into three random

1547
01:11:41,320 --> 01:11:46,268
groups a one random group got a copa the

1548
01:11:44,050 --> 01:11:49,599
other group got a b BR and the third

1549
01:11:46,269 --> 01:11:50,860
group got a cubic we conducted this

1550
01:11:49,599 --> 01:11:52,480
experiment for

1551
01:11:50,860 --> 01:11:56,530
three two weeks and we collected roughly

1552
01:11:52,480 --> 01:11:58,150
four million samples in each group so

1553
01:11:56,530 --> 01:11:59,469
before diving into the results I want to

1554
01:11:58,150 --> 01:12:01,888
spend a little bit of time explaining

1555
01:11:59,469 --> 01:12:04,630
the metrics and how how we define them

1556
01:12:01,889 --> 01:12:06,520
so we mainly focused on two metrics on

1557
01:12:04,630 --> 01:12:08,710
the application side to evaluate the

1558
01:12:06,520 --> 01:12:10,570
condition general algorithm the one

1559
01:12:08,710 --> 01:12:12,969
metric is average good word so the way

1560
01:12:10,570 --> 01:12:14,710
we define it as as you guys might

1561
01:12:12,969 --> 01:12:16,480
already know number of application by

1562
01:12:14,710 --> 01:12:19,929
its sixth sense successfully over time

1563
01:12:16,480 --> 01:12:21,879
so because they do not cast could be

1564
01:12:19,929 --> 01:12:23,619
long-running we calculated the total

1565
01:12:21,880 --> 01:12:27,219
number of audio and video bytes that

1566
01:12:23,619 --> 01:12:28,900
were sent during the entire cast and

1567
01:12:27,219 --> 01:12:31,780
then we divided that by the duration of

1568
01:12:28,900 --> 01:12:32,589
the broadcast in seconds that gives us

1569
01:12:31,780 --> 01:12:35,920
average code word

1570
01:12:32,590 --> 01:12:38,290
the second metric we used is average

1571
01:12:35,920 --> 01:12:41,530
application observed oddity this is our

1572
01:12:38,290 --> 01:12:43,510
proxy for video ingest latency to

1573
01:12:41,530 --> 01:12:46,420
explain how this is calculated the

1574
01:12:43,510 --> 01:12:48,940
there's a diagram here to calculate this

1575
01:12:46,420 --> 01:12:51,400
the live streaming application inserts a

1576
01:12:48,940 --> 01:12:53,650
pink frame with basically no payload

1577
01:12:51,400 --> 01:12:56,469
just a bunch of headers in between the

1578
01:12:53,650 --> 01:12:58,750
audio and video data now a dark pink

1579
01:12:56,469 --> 01:13:00,429
frame travels with the rest of the audio

1580
01:12:58,750 --> 01:13:02,920
and video data through the transport

1581
01:13:00,429 --> 01:13:04,840
send buffer through the kernel through

1582
01:13:02,920 --> 01:13:06,790
the same network through the proxies and

1583
01:13:04,840 --> 01:13:08,710
it reaches the Facebook live server

1584
01:13:06,790 --> 01:13:12,340
which is processing the audio and video

1585
01:13:08,710 --> 01:13:14,619
data then when the server receives the

1586
01:13:12,340 --> 01:13:16,090
ping frame it immediately responds back

1587
01:13:14,619 --> 01:13:17,980
with an acknowledgement frame for that

1588
01:13:16,090 --> 01:13:19,630
ping frame which goes all the way back

1589
01:13:17,980 --> 01:13:21,730
to the live streaming application now

1590
01:13:19,630 --> 01:13:23,679
the application uses a timing of sending

1591
01:13:21,730 --> 01:13:25,989
the frame ping frame and getting the ACK

1592
01:13:23,679 --> 01:13:27,639
it calculates an RTT which is its

1593
01:13:25,989 --> 01:13:30,909
picture of what is the round-trip time

1594
01:13:27,639 --> 01:13:33,219
for the application so the application

1595
01:13:30,909 --> 01:13:35,259
does this measurement every second and

1596
01:13:33,219 --> 01:13:37,300
it gets roughly one sample every second

1597
01:13:35,260 --> 01:13:39,100
and to calculate average application

1598
01:13:37,300 --> 01:13:42,670
observed RTD we take the average of all

1599
01:13:39,100 --> 01:13:45,340
of these samples now let's look at the

1600
01:13:42,670 --> 01:13:46,600
results clarification question method so

1601
01:13:45,340 --> 01:13:52,929
these are basically measured on the

1602
01:13:46,600 --> 01:13:55,780
upload this is site yeah so looking at

1603
01:13:52,929 --> 01:13:58,060
the results for goodput we saw that both

1604
01:13:55,780 --> 01:14:00,639
coppa and VB are provided better goodput

1605
01:13:58,060 --> 01:14:02,770
as compared to cubic however we saw that

1606
01:14:00,639 --> 01:14:04,900
coppa provided much better good word as

1607
01:14:02,770 --> 01:14:08,349
compared to BB re not

1608
01:14:04,900 --> 01:14:11,888
to be specific of the p50 good port was

1609
01:14:08,349 --> 01:14:14,710
increased by 5% for bbr and it increased

1610
01:14:11,889 --> 01:14:16,869
by 16% for Coppa and similar trend

1611
01:14:14,710 --> 01:14:18,909
followed even if you look at the look

1612
01:14:16,869 --> 01:14:23,139
the verse connections which have a lower

1613
01:14:18,909 --> 01:14:25,329
quality and as you can see like at p90

1614
01:14:23,139 --> 01:14:28,150
it maxes out because our encoded bit

1615
01:14:25,329 --> 01:14:29,739
rate is capped at 3 Mbps because that's

1616
01:14:28,150 --> 01:14:31,719
pretty much like a reasonable max

1617
01:14:29,739 --> 01:14:35,949
quality that we want to have from from

1618
01:14:31,719 --> 01:14:38,829
our our users and this this improvement

1619
01:14:35,949 --> 01:14:41,379
was massive for us and we also saw

1620
01:14:38,829 --> 01:14:44,558
positive impact on some of the top line

1621
01:14:41,380 --> 01:14:46,179
video metrics that we observe which tell

1622
01:14:44,559 --> 01:14:48,039
us how much people are watching those

1623
01:14:46,179 --> 01:14:52,300
videos and how much people are engaging

1624
01:14:48,039 --> 01:14:54,429
with those videos the second metric we

1625
01:14:52,300 --> 01:14:56,980
used is the video ingest latency or

1626
01:14:54,429 --> 01:15:00,250
application observe data team in this a

1627
01:14:56,980 --> 01:15:02,379
metric what we saw was a bbr was able to

1628
01:15:00,250 --> 01:15:04,780
improve it slightly more for the best

1629
01:15:02,380 --> 01:15:08,020
connections so as you can see in the

1630
01:15:04,780 --> 01:15:11,650
left graph bbr reduced it the bbr a bar

1631
01:15:08,020 --> 01:15:15,130
length is smaller for p50 BBI reduced it

1632
01:15:11,650 --> 01:15:17,199
by 8% Coppa reduced it by 4% but but

1633
01:15:15,130 --> 01:15:20,260
those connections already had like quite

1634
01:15:17,199 --> 01:15:21,820
low RTD and the reduction was not as

1635
01:15:20,260 --> 01:15:24,429
visible for our users

1636
01:15:21,820 --> 01:15:27,460
if you look at the verse connections P

1637
01:15:24,429 --> 01:15:28,150
75 P 90 and above are Coppa reductions

1638
01:15:27,460 --> 01:15:31,059
were highest

1639
01:15:28,150 --> 01:15:34,058
for P 90 Copa reduced the application

1640
01:15:31,059 --> 01:15:36,820
ITT by 27 percent whereas BB our

1641
01:15:34,059 --> 01:15:40,960
application RTT was same as cubic in our

1642
01:15:36,820 --> 01:15:42,880
tests um so this told us so in the

1643
01:15:40,960 --> 01:15:45,670
previous slide you saw that Coppa had

1644
01:15:42,880 --> 01:15:47,800
better good put so that meant that Coppa

1645
01:15:45,670 --> 01:15:49,360
was able to help like it helped

1646
01:15:47,800 --> 01:15:51,070
transport to send more bytes

1647
01:15:49,360 --> 01:15:53,679
successfully over the wire and it also

1648
01:15:51,070 --> 01:15:55,539
impacted the application ABR to produce

1649
01:15:53,679 --> 01:15:58,270
more bytes in response to a better

1650
01:15:55,539 --> 01:16:00,550
better transport but it was able to do

1651
01:15:58,270 --> 01:16:02,619
it at the same time as keeping the

1652
01:16:00,550 --> 01:16:05,559
latency slow especially for the users

1653
01:16:02,619 --> 01:16:07,530
like that had high latencies and where

1654
01:16:05,559 --> 01:16:10,360
it mattered the most

1655
01:16:07,530 --> 01:16:12,880
so to be honest like this result was

1656
01:16:10,360 --> 01:16:15,159
quite surprising for us we did not

1657
01:16:12,880 --> 01:16:17,440
expect both goodput and latency to be

1658
01:16:15,159 --> 01:16:17,759
better so we spent some time trying to

1659
01:16:17,440 --> 01:16:19,530
under

1660
01:16:17,760 --> 01:16:23,130
and more like why that might be

1661
01:16:19,530 --> 01:16:24,809
happening so in in the process we found

1662
01:16:23,130 --> 01:16:26,940
some interesting observations on the

1663
01:16:24,810 --> 01:16:28,410
transport side but before going into

1664
01:16:26,940 --> 01:16:30,629
those I want to spend some time

1665
01:16:28,410 --> 01:16:32,400
explaining how Cooper works I'm sure a

1666
01:16:30,630 --> 01:16:34,500
lot of you are already already now aware

1667
01:16:32,400 --> 01:16:37,190
but I want to explain like how we

1668
01:16:34,500 --> 01:16:39,630
applied it and how we understood it so

1669
01:16:37,190 --> 01:16:41,759
it's a tunable delay based congestion

1670
01:16:39,630 --> 01:16:44,280
control algorithm the tunability kappahd

1671
01:16:41,760 --> 01:16:47,460
comes from the delta parameter which

1672
01:16:44,280 --> 01:16:49,590
varies from 0 to 1 the closer it is to 0

1673
01:16:47,460 --> 01:16:51,780
the more the algorithm will optimize for

1674
01:16:49,590 --> 01:16:55,110
throughput the closer it is to 1 the

1675
01:16:51,780 --> 01:16:56,730
more it will optimize for delay and it's

1676
01:16:55,110 --> 01:16:58,380
a delay based algorithm which means it

1677
01:16:56,730 --> 01:17:00,389
uses RTT variation and signal of

1678
01:16:58,380 --> 01:17:02,940
congestion and not loss or something

1679
01:17:00,390 --> 01:17:05,010
else in order to do that it maintains

1680
01:17:02,940 --> 01:17:07,049
two key variables one is RT t minimum

1681
01:17:05,010 --> 01:17:09,180
which is the minimum RTD the flow has

1682
01:17:07,050 --> 01:17:11,730
observed over a period of 10 seconds

1683
01:17:09,180 --> 01:17:13,440
it's its estimate of the two-way

1684
01:17:11,730 --> 01:17:16,830
propagation delay which is a property of

1685
01:17:13,440 --> 01:17:18,809
the network path the second metric the

1686
01:17:16,830 --> 01:17:21,210
second variable it maintains is RTD

1687
01:17:18,810 --> 01:17:24,000
standing this is a minimum RT t observed

1688
01:17:21,210 --> 01:17:27,090
over a much smaller period s RT T by 2

1689
01:17:24,000 --> 01:17:29,160
to be precise this is a copepod estimate

1690
01:17:27,090 --> 01:17:31,110
of what is the current round-trip time

1691
01:17:29,160 --> 01:17:33,420
including any queuing delays in deep

1692
01:17:31,110 --> 01:17:35,969
water like links so the reason it does

1693
01:17:33,420 --> 01:17:38,580
it chooses RT t by 2 is to make sure

1694
01:17:35,970 --> 01:17:40,830
like the measurement is correct if there

1695
01:17:38,580 --> 01:17:42,750
are some jitters or there are this

1696
01:17:40,830 --> 01:17:45,000
compression on the on the transport

1697
01:17:42,750 --> 01:17:46,980
which might mess up which might make the

1698
01:17:45,000 --> 01:17:50,010
algorithm think that there is queueing

1699
01:17:46,980 --> 01:17:51,900
even when there is not then it uses

1700
01:17:50,010 --> 01:17:54,360
these two to calculate a queuing delay

1701
01:17:51,900 --> 01:17:56,670
which is RTD standing - Artie T minimum

1702
01:17:54,360 --> 01:17:58,110
so on every ACK the algorithm the main

1703
01:17:56,670 --> 01:18:00,270
controller calculates the queuing delay

1704
01:17:58,110 --> 01:18:03,179
and then it calculates a target rate

1705
01:18:00,270 --> 01:18:06,240
using this formula the formula is 1

1706
01:18:03,180 --> 01:18:08,130
divided by delta x queuing delay i am

1707
01:18:06,240 --> 01:18:09,360
not going to go in details of this there

1708
01:18:08,130 --> 01:18:12,120
is a long mathematical justification

1709
01:18:09,360 --> 01:18:13,500
given in the paper they show that like

1710
01:18:12,120 --> 01:18:15,900
you can critically reach Nash

1711
01:18:13,500 --> 01:18:17,220
equilibrium if you have certain modeling

1712
01:18:15,900 --> 01:18:20,070
assumptions on the packet arrival

1713
01:18:17,220 --> 01:18:22,410
pattern so please do read more about it

1714
01:18:20,070 --> 01:18:24,059
if you are interested and then it

1715
01:18:22,410 --> 01:18:25,920
compares the target rate with the

1716
01:18:24,060 --> 01:18:27,480
current rate and accordingly it adjusts

1717
01:18:25,920 --> 01:18:29,400
the condition window to what said by

1718
01:18:27,480 --> 01:18:31,089
using an additive increase additive

1719
01:18:29,400 --> 01:18:34,059
decrease a variant

1720
01:18:31,090 --> 01:18:36,520
so it moves it by a V divided by Delta

1721
01:18:34,060 --> 01:18:38,620
times condition window so condition

1722
01:18:36,520 --> 01:18:40,600
window factor in the denominator helps

1723
01:18:38,620 --> 01:18:43,720
ensure that the change in one RT t is

1724
01:18:40,600 --> 01:18:46,810
going to be at max V divided by Delta

1725
01:18:43,720 --> 01:18:49,900
packets V here is a velocity parameter

1726
01:18:46,810 --> 01:18:51,910
it is 1 by default and this a parameter

1727
01:18:49,900 --> 01:18:54,309
comes in handy when Co pi detects that

1728
01:18:51,910 --> 01:18:56,440
it has been trying to converge towards a

1729
01:18:54,310 --> 01:18:58,150
target rate for far too long and it

1730
01:18:56,440 --> 01:19:00,339
needs to do a better job at converging

1731
01:18:58,150 --> 01:19:02,259
at it faster in that case it starts

1732
01:19:00,340 --> 01:19:06,220
doubling the velocity parameter I think

1733
01:19:02,260 --> 01:19:08,320
every RTT the other unique thing or

1734
01:19:06,220 --> 01:19:10,360
different thing about this algorithm is

1735
01:19:08,320 --> 01:19:13,769
that it has a competitive mode normally

1736
01:19:10,360 --> 01:19:17,080
deliveries condition control algorithms

1737
01:19:13,770 --> 01:19:19,510
we lose too will lose in the presence of

1738
01:19:17,080 --> 01:19:21,780
buffer filling flows in the owner in the

1739
01:19:19,510 --> 01:19:24,490
battle in the bottleneck link and

1740
01:19:21,780 --> 01:19:27,790
because if the buffers are filling up

1741
01:19:24,490 --> 01:19:30,730
for no fault of the deliberate sender

1742
01:19:27,790 --> 01:19:34,120
then the deliberate sender will think

1743
01:19:30,730 --> 01:19:35,440
that this congestion or it's the delays

1744
01:19:34,120 --> 01:19:37,450
are building up and it's going to back

1745
01:19:35,440 --> 01:19:39,849
off before the lost beast flows are

1746
01:19:37,450 --> 01:19:43,510
going to back off hence it loses some of

1747
01:19:39,850 --> 01:19:45,190
the throughput coppa gets around this

1748
01:19:43,510 --> 01:19:47,380
problem by using something called a

1749
01:19:45,190 --> 01:19:50,950
competitive mode so in competitive mode

1750
01:19:47,380 --> 01:19:53,410
it uses a heuristic which I will explain

1751
01:19:50,950 --> 01:19:55,330
a little bit more in the next slide to

1752
01:19:53,410 --> 01:19:58,720
detect the presence of buffer filling

1753
01:19:55,330 --> 01:20:00,309
flows and if it detects a buffer filling

1754
01:19:58,720 --> 01:20:02,140
flow then it adjust the Delta to be more

1755
01:20:00,310 --> 01:20:04,270
aggressive basically optimize more for

1756
01:20:02,140 --> 01:20:06,910
throughput and perform better in the as

1757
01:20:04,270 --> 01:20:08,890
compared to fulfilling flows however in

1758
01:20:06,910 --> 01:20:10,840
our experimentation we did not implement

1759
01:20:08,890 --> 01:20:12,910
competitive mode we wanted our

1760
01:20:10,840 --> 01:20:14,920
experiment to be like smaller in scope

1761
01:20:12,910 --> 01:20:17,139
so we just tested like what will happen

1762
01:20:14,920 --> 01:20:22,660
if you use aggressive value of Delta

1763
01:20:17,140 --> 01:20:25,270
without using competitive mode so this

1764
01:20:22,660 --> 01:20:26,950
is the steady state dynamics of cope

1765
01:20:25,270 --> 01:20:29,890
algorithm this is what the bottleneck

1766
01:20:26,950 --> 01:20:32,050
you looks like in steady state so the

1767
01:20:29,890 --> 01:20:34,960
bottleneck you goes oscillates between

1768
01:20:32,050 --> 01:20:38,350
having zero packets up to 2.5 times

1769
01:20:34,960 --> 01:20:40,750
Delta inverts packets and this the

1770
01:20:38,350 --> 01:20:44,200
entire pattern repeats itself every 5 RT

1771
01:20:40,750 --> 01:20:45,780
so what I was saying before was that the

1772
01:20:44,200 --> 01:20:47,980
competitive mode and

1773
01:20:45,780 --> 01:20:49,990
computing flow detection so Coppa

1774
01:20:47,980 --> 01:20:51,610
actually exploits this property that the

1775
01:20:49,990 --> 01:20:54,610
queue is going to empty every five RTD

1776
01:20:51,610 --> 01:20:56,080
and if it's not doing that then it it

1777
01:20:54,610 --> 01:20:57,160
thinks that there is a buffer filling

1778
01:20:56,080 --> 01:20:59,320
flow which is preventing this from

1779
01:20:57,160 --> 01:21:01,769
happening so hence it concludes that

1780
01:20:59,320 --> 01:21:05,679
there is a buffer filling flowing in

1781
01:21:01,770 --> 01:21:08,010
sharing the bottleneck link so here like

1782
01:21:05,680 --> 01:21:10,150
if you see let's say at T equal to zero

1783
01:21:08,010 --> 01:21:11,950
the queue starts to increase for the

1784
01:21:10,150 --> 01:21:14,620
first time so it will take about half

1785
01:21:11,950 --> 01:21:16,450
our TT for the RTD standing measurement

1786
01:21:14,620 --> 01:21:18,880
to be available and the target rate to

1787
01:21:16,450 --> 01:21:21,190
reflect this change and the target rate

1788
01:21:18,880 --> 01:21:23,620
will be lower than the current rate at

1789
01:21:21,190 --> 01:21:24,849
that time because of the inherent RT t

1790
01:21:23,620 --> 01:21:28,349
delay in the network it will take one

1791
01:21:24,850 --> 01:21:30,880
oddity for that change to be effective

1792
01:21:28,350 --> 01:21:32,950
basically the change of reducing the

1793
01:21:30,880 --> 01:21:34,630
target rate and then in another oddity

1794
01:21:32,950 --> 01:21:36,820
the queues will start to drain again and

1795
01:21:34,630 --> 01:21:38,380
then the entire cycle repeats itself

1796
01:21:36,820 --> 01:21:40,179
like half our TT for the architects

1797
01:21:38,380 --> 01:21:42,670
timing measurement to reflect the change

1798
01:21:40,180 --> 01:21:44,890
and then another oddity for the

1799
01:21:42,670 --> 01:21:47,530
congestion window increase or the target

1800
01:21:44,890 --> 01:21:50,170
rate increase to result in the queue

1801
01:21:47,530 --> 01:21:52,690
length to start increasing in our

1802
01:21:50,170 --> 01:21:55,090
experiment V and then the results that

1803
01:21:52,690 --> 01:21:57,240
I'm presenting they used a delta value

1804
01:21:55,090 --> 01:22:00,420
of point zero four which means roughly

1805
01:21:57,240 --> 01:22:02,830
twenty-five packets in the equilibrium

1806
01:22:00,420 --> 01:22:04,240
twenty five packets equilibrium queue

1807
01:22:02,830 --> 01:22:06,880
length and the maximum queue length is

1808
01:22:04,240 --> 01:22:11,200
like two point five times twenty five

1809
01:22:06,880 --> 01:22:13,300
now let's look at some of the transport

1810
01:22:11,200 --> 01:22:14,860
level stuff we observed so one metric we

1811
01:22:13,300 --> 01:22:17,770
looked at is how the how is the

1812
01:22:14,860 --> 01:22:19,719
transport Arcadia varying so we saw so

1813
01:22:17,770 --> 01:22:21,490
this is measured as just taking the

1814
01:22:19,720 --> 01:22:23,170
average of all the quake RTD

1815
01:22:21,490 --> 01:22:26,710
measurements over the duration of the

1816
01:22:23,170 --> 01:22:28,750
broadcast so the trend we saw was a very

1817
01:22:26,710 --> 01:22:30,580
similar kind of similar to application

1818
01:22:28,750 --> 01:22:32,530
architecture meant bbr for the best

1819
01:22:30,580 --> 01:22:34,390
connections which already had pretty low

1820
01:22:32,530 --> 01:22:36,900
RTT bbr was able to reduce it slightly

1821
01:22:34,390 --> 01:22:39,970
more as compared to a copan cubic

1822
01:22:36,900 --> 01:22:43,030
however if you look at the tale cases 75

1823
01:22:39,970 --> 01:22:46,060
1995 Copa reductions were very higher as

1824
01:22:43,030 --> 01:22:48,790
compared to VB are so for a p90 for

1825
01:22:46,060 --> 01:22:50,650
example Copa reduced the RTD by 38

1826
01:22:48,790 --> 01:22:53,230
percent whereas bbr reduced it by eight

1827
01:22:50,650 --> 01:22:55,960
point eight percent so this was pretty

1828
01:22:53,230 --> 01:22:57,400
good to see and this told us little bit

1829
01:22:55,960 --> 01:22:59,170
like where the application IDT

1830
01:22:57,400 --> 01:23:02,440
reductions might be coming from

1831
01:22:59,170 --> 01:23:04,930
and it also showed us that like it Koopa

1832
01:23:02,440 --> 01:23:07,059
flow is not just improving the

1833
01:23:04,930 --> 01:23:08,890
application latency but it's also

1834
01:23:07,060 --> 01:23:10,750
improving the RTT for the network so if

1835
01:23:08,890 --> 01:23:12,580
there are any other flows which are also

1836
01:23:10,750 --> 01:23:15,600
sharing the bottleneck length like they

1837
01:23:12,580 --> 01:23:17,860
will also start seeing a lower equities

1838
01:23:15,600 --> 01:23:21,190
the other metric we looked at is the

1839
01:23:17,860 --> 01:23:22,839
retransmission overhead so this we

1840
01:23:21,190 --> 01:23:25,059
defined it as the total number of bytes

1841
01:23:22,840 --> 01:23:26,950
retransmitted by the transport during

1842
01:23:25,060 --> 01:23:28,780
the course of the broadcast divided by

1843
01:23:26,950 --> 01:23:30,519
total number of bytes acknowledged

1844
01:23:28,780 --> 01:23:32,679
during the course of the broadcast I

1845
01:23:30,520 --> 01:23:34,090
think it's a pretty important metric to

1846
01:23:32,680 --> 01:23:35,650
look at because it tells you how

1847
01:23:34,090 --> 01:23:37,630
efficient the transport is if you are

1848
01:23:35,650 --> 01:23:39,309
wasting resources and bandwidth

1849
01:23:37,630 --> 01:23:40,930
rewriting the same bytes over the wire

1850
01:23:39,310 --> 01:23:43,930
again and again it's just not good for

1851
01:23:40,930 --> 01:23:46,750
anyone so what we found here was that

1852
01:23:43,930 --> 01:23:48,790
for 90% of users coppa retransmission

1853
01:23:46,750 --> 01:23:51,670
overhead was available as compared to

1854
01:23:48,790 --> 01:23:54,400
both be brn cubic it was about half of

1855
01:23:51,670 --> 01:23:58,420
what bbr had and around 1/4 of what

1856
01:23:54,400 --> 01:24:00,219
sorry cubic hard so this gave us an idea

1857
01:23:58,420 --> 01:24:01,810
like why application goodput might have

1858
01:24:00,220 --> 01:24:03,970
been better like this might be one of

1859
01:24:01,810 --> 01:24:05,230
the reasons why we were able to send

1860
01:24:03,970 --> 01:24:08,200
mode by it successfully for the

1861
01:24:05,230 --> 01:24:10,330
application however for the last 10% of

1862
01:24:08,200 --> 01:24:12,550
users we saw a different trend that

1863
01:24:10,330 --> 01:24:14,140
coppa retransmission overhead grew very

1864
01:24:12,550 --> 01:24:16,510
rapidly and it became three to four

1865
01:24:14,140 --> 01:24:19,210
times as compared to cope and cubic and

1866
01:24:16,510 --> 01:24:20,920
VBR which was concerning as well as

1867
01:24:19,210 --> 01:24:23,110
surprising because we did not see a

1868
01:24:20,920 --> 01:24:26,020
corresponding proportional degradation

1869
01:24:23,110 --> 01:24:29,559
in the application metrics for any other

1870
01:24:26,020 --> 01:24:32,080
tail users so we spend some time

1871
01:24:29,560 --> 01:24:34,900
debugging into it and the first thing we

1872
01:24:32,080 --> 01:24:37,750
did was we sample a few cases some some

1873
01:24:34,900 --> 01:24:41,019
broadcasts which had a very high loss

1874
01:24:37,750 --> 01:24:43,330
rate for coppa so we notice two things

1875
01:24:41,020 --> 01:24:45,640
one thing was that these flows had very

1876
01:24:43,330 --> 01:24:47,140
constant or throughput as you can see by

1877
01:24:45,640 --> 01:24:49,720
the red line here the number of bytes

1878
01:24:47,140 --> 01:24:51,610
act over time it's it's a it's a

1879
01:24:49,720 --> 01:24:53,290
straight line and the second thing you

1880
01:24:51,610 --> 01:24:55,990
observe is that these flows actually

1881
01:24:53,290 --> 01:24:58,540
have very low arteries so in this sample

1882
01:24:55,990 --> 01:25:02,200
except the one spike the RTD stays

1883
01:24:58,540 --> 01:25:04,630
constant around 75 milliseconds so this

1884
01:25:02,200 --> 01:25:06,580
did not look like congestion or loss is

1885
01:25:04,630 --> 01:25:08,950
happening because of buffers filling up

1886
01:25:06,580 --> 01:25:11,080
this look more like network policing to

1887
01:25:08,950 --> 01:25:12,730
us because if there's a token bucket a

1888
01:25:11,080 --> 01:25:14,830
police are in effect which is

1889
01:25:12,730 --> 01:25:17,360
identifying your traffic and and

1890
01:25:14,830 --> 01:25:19,610
limiting it as a bit at a bit rate it's

1891
01:25:17,360 --> 01:25:21,259
not going to result in any increase in

1892
01:25:19,610 --> 01:25:23,299
queuing delays or equities but it will

1893
01:25:21,260 --> 01:25:25,160
look more like this

1894
01:25:23,300 --> 01:25:27,140
we also grouped our retransmission

1895
01:25:25,160 --> 01:25:29,360
overhead numbers by ESN and we found

1896
01:25:27,140 --> 01:25:30,860
that they do vary greatly depending on

1897
01:25:29,360 --> 01:25:32,150
which s and you're looking at some

1898
01:25:30,860 --> 01:25:34,610
essence which are known to police

1899
01:25:32,150 --> 01:25:36,410
Facebook traffic they had very higher

1900
01:25:34,610 --> 01:25:39,589
retransmission overhead for tail cases

1901
01:25:36,410 --> 01:25:41,090
for Koopa and there was some other a

1902
01:25:39,590 --> 01:25:44,210
essence where the retransmission

1903
01:25:41,090 --> 01:25:46,610
overheads look very similar so to

1904
01:25:44,210 --> 01:25:48,650
generalize our finding we did some more

1905
01:25:46,610 --> 01:25:50,630
aggregated analysis we looked at the

1906
01:25:48,650 --> 01:25:52,460
relationship between artery transmission

1907
01:25:50,630 --> 01:25:54,950
overhead and our TT queuing delays and

1908
01:25:52,460 --> 01:25:57,320
we found that for cubic they are highly

1909
01:25:54,950 --> 01:26:00,410
correlated as the retransmission

1910
01:25:57,320 --> 01:26:01,940
overhead increases our TT and queuing

1911
01:26:00,410 --> 01:26:03,920
delays also increase for cubic which

1912
01:26:01,940 --> 01:26:06,349
indicates that all the losses are

1913
01:26:03,920 --> 01:26:09,530
happening because of congestion instead

1914
01:26:06,350 --> 01:26:11,750
for but for Coppa we saw similar trend

1915
01:26:09,530 --> 01:26:13,610
for the first part of the graph but

1916
01:26:11,750 --> 01:26:15,740
later on we see that the retransmission

1917
01:26:13,610 --> 01:26:17,540
as the retransmission overhead increases

1918
01:26:15,740 --> 01:26:21,080
the arc titties and queuing delays

1919
01:26:17,540 --> 01:26:24,110
actually start to come down so this kind

1920
01:26:21,080 --> 01:26:26,510
of solidified our hypothesis that this

1921
01:26:24,110 --> 01:26:29,150
net the network policing is a big factor

1922
01:26:26,510 --> 01:26:31,850
being playing a role here in the high

1923
01:26:29,150 --> 01:26:33,559
retransmissions although it's also

1924
01:26:31,850 --> 01:26:35,180
possible that there are some other

1925
01:26:33,560 --> 01:26:36,800
reasons at play here and network

1926
01:26:35,180 --> 01:26:39,020
policing is not the only one for example

1927
01:26:36,800 --> 01:26:41,000
short buffers if there's a short buffer

1928
01:26:39,020 --> 01:26:42,800
and Coppa sender is trying to maintain

1929
01:26:41,000 --> 01:26:45,200
an equilibrium queue length of 25

1930
01:26:42,800 --> 01:26:47,240
packets then the buffer is smaller than

1931
01:26:45,200 --> 01:26:51,080
that then like definitely going to run

1932
01:26:47,240 --> 01:26:53,000
into losses so definitely there are

1933
01:26:51,080 --> 01:26:55,490
improvements are possible in Coppa to

1934
01:26:53,000 --> 01:26:57,380
handle this case better competitive mode

1935
01:26:55,490 --> 01:26:59,570
as I described before could help because

1936
01:26:57,380 --> 01:27:04,010
it could adjust the target Delta based

1937
01:26:59,570 --> 01:27:05,480
on whether it sees loss or success we

1938
01:27:04,010 --> 01:27:06,980
could also add a heuristic to change

1939
01:27:05,480 --> 01:27:08,870
congestion window based on something

1940
01:27:06,980 --> 01:27:11,269
simple like multiplicative decrease

1941
01:27:08,870 --> 01:27:13,930
based on target loss just like cubic or

1942
01:27:11,270 --> 01:27:16,130
Renault or even BB are now is doing

1943
01:27:13,930 --> 01:27:18,080
other option is to have an explicit

1944
01:27:16,130 --> 01:27:21,770
network policy detection similar to how

1945
01:27:18,080 --> 01:27:24,230
BB R by V one had it so in conclusion

1946
01:27:21,770 --> 01:27:26,210
the aggregated results showed us that

1947
01:27:24,230 --> 01:27:28,610
Coppa provided better

1948
01:27:26,210 --> 01:27:30,260
and lower latencies in artists these

1949
01:27:28,610 --> 01:27:32,210
tests were for mobile broadcasts for

1950
01:27:30,260 --> 01:27:32,540
uploads and we compared with cubic and

1951
01:27:32,210 --> 01:27:35,090
VBR

1952
01:27:32,540 --> 01:27:37,100
with quake but one thing to note is VB

1953
01:27:35,090 --> 01:27:39,680
are like via tuning VB are in our

1954
01:27:37,100 --> 01:27:41,540
internal implementation and also like

1955
01:27:39,680 --> 01:27:44,300
Beiber veto is happening so the results

1956
01:27:41,540 --> 01:27:45,769
may differ in future one of the big

1957
01:27:44,300 --> 01:27:47,150
future work items is a better

1958
01:27:45,770 --> 01:27:49,969
understanding of the reasons behind

1959
01:27:47,150 --> 01:27:50,989
these improvements could are these

1960
01:27:49,969 --> 01:27:53,060
improvements because of lower

1961
01:27:50,989 --> 01:27:55,160
retransmission overhead or is it because

1962
01:27:53,060 --> 01:27:57,140
Kupa is just doing better target rate

1963
01:27:55,160 --> 01:27:59,960
estimation by sending fewer packets and

1964
01:27:57,140 --> 01:28:02,960
it's converging to that faster or it

1965
01:27:59,960 --> 01:28:04,730
could be something else in the beginning

1966
01:28:02,960 --> 01:28:06,110
of presentation I spoke about our goal

1967
01:28:04,730 --> 01:28:07,580
of having a tunable delay based

1968
01:28:06,110 --> 01:28:09,880
condition enroll algorithm which could

1969
01:28:07,580 --> 01:28:13,219
be compatible with all the video

1970
01:28:09,880 --> 01:28:15,200
experiences and so far we only ran

1971
01:28:13,219 --> 01:28:17,150
experiments on one extreme where we are

1972
01:28:15,200 --> 01:28:19,489
optimizing the throughput we would also

1973
01:28:17,150 --> 01:28:22,070
like to run some experiments with other

1974
01:28:19,489 --> 01:28:24,200
with goobie condition control as a

1975
01:28:22,070 --> 01:28:26,360
comparison where the end-to-end ultra

1976
01:28:24,200 --> 01:28:27,650
low latency super important and we would

1977
01:28:26,360 --> 01:28:31,969
also like to test for more use cases

1978
01:28:27,650 --> 01:28:33,679
like playback and all the traffic this

1979
01:28:31,969 --> 01:28:36,910
is these are a few links where you can

1980
01:28:33,680 --> 01:28:39,410
read and learn more my email is here and

1981
01:28:36,910 --> 01:28:43,580
I'm happy to answer any questions now or

1982
01:28:39,410 --> 01:28:45,950
offline thank you so much nothing

1983
01:28:43,580 --> 01:28:47,030
I'll intake myself in the front of the

1984
01:28:45,950 --> 01:28:49,219
queue

1985
01:28:47,030 --> 01:28:54,830
so I'm asking this question from the

1986
01:28:49,219 --> 01:28:56,989
floor basically the scenario that using

1987
01:28:54,830 --> 01:28:59,660
this under right now is when the client

1988
01:28:56,989 --> 01:29:01,639
is brought is uploading video up to the

1989
01:28:59,660 --> 01:29:04,969
server and this is almost a textbook

1990
01:29:01,640 --> 01:29:06,469
example of a situation where there is no

1991
01:29:04,969 --> 01:29:10,310
cross traffic or there's no other

1992
01:29:06,469 --> 01:29:12,860
traffic that is going to be competing

1993
01:29:10,310 --> 01:29:14,090
with Copa at a bottleneck because I

1994
01:29:12,860 --> 01:29:16,370
imagine that when people are uploading

1995
01:29:14,090 --> 01:29:19,489
video they're not doing anything else on

1996
01:29:16,370 --> 01:29:21,290
the phone so it seems to make sense that

1997
01:29:19,489 --> 01:29:23,360
the that even without the competitive

1998
01:29:21,290 --> 01:29:27,880
mode which I like to think of as TCP

1999
01:29:23,360 --> 01:29:31,040
friendly mode because that's what it is

2000
01:29:27,880 --> 01:29:33,140
no that's what cubicles it but we don't

2001
01:29:31,040 --> 01:29:37,489
get there the the point is that that's

2002
01:29:33,140 --> 01:29:39,770
that's simply a it makes sense that it

2003
01:29:37,489 --> 01:29:40,099
works without that so I'd be interested

2004
01:29:39,770 --> 01:29:42,050
in

2005
01:29:40,100 --> 01:29:43,430
seeing more evaluation on on what

2006
01:29:42,050 --> 01:29:45,830
happens when you when you have

2007
01:29:43,430 --> 01:29:47,420
competition with cubic with PBR with

2008
01:29:45,830 --> 01:29:48,500
other things in the network because

2009
01:29:47,420 --> 01:29:51,890
that's going to be useful for

2010
01:29:48,500 --> 01:29:53,210
downloading download streams and in the

2011
01:29:51,890 --> 01:29:57,200
common case that's gonna be an important

2012
01:29:53,210 --> 01:29:58,700
factor definitely like that's like I

2013
01:29:57,200 --> 01:30:01,010
said that's one of the future work items

2014
01:29:58,700 --> 01:30:02,510
although then to be honest when we

2015
01:30:01,010 --> 01:30:04,280
started working on this like we did not

2016
01:30:02,510 --> 01:30:08,150
really know that there is going to be no

2017
01:30:04,280 --> 01:30:10,370
competing traffic and like even now like

2018
01:30:08,150 --> 01:30:13,969
I'm not totally sure like if there is

2019
01:30:10,370 --> 01:30:15,349
any evidence like a real life evidence

2020
01:30:13,970 --> 01:30:17,450
of that I would love to read more about

2021
01:30:15,350 --> 01:30:19,610
it I know from textbook that it is the

2022
01:30:17,450 --> 01:30:23,450
case but like yeah this is one of the

2023
01:30:19,610 --> 01:30:26,599
future work items thank you Michael

2024
01:30:23,450 --> 01:30:28,700
Mike reverse begin by saying that I

2025
01:30:26,600 --> 01:30:31,400
greatly appreciate Facebook coming here

2026
01:30:28,700 --> 01:30:33,110
and bringing this here and some some

2027
01:30:31,400 --> 01:30:37,639
good results and I guess the mechanism

2028
01:30:33,110 --> 01:30:40,130
is not altogether unreasonable I have to

2029
01:30:37,640 --> 01:30:44,270
say sorry for that I have to say that

2030
01:30:40,130 --> 01:30:47,810
the base idea or that this you know it's

2031
01:30:44,270 --> 01:30:50,420
based upon of a delta that that reflects

2032
01:30:47,810 --> 01:30:52,790
a trade-off between having high delay

2033
01:30:50,420 --> 01:30:56,470
and good throughput or low delay and not

2034
01:30:52,790 --> 01:30:59,450
so much support is fundamentally wrong

2035
01:30:56,470 --> 01:31:01,370
just because well I mean if you look at

2036
01:30:59,450 --> 01:31:04,790
the Intuit the base intuition between

2037
01:31:01,370 --> 01:31:06,200
mechanisms like bbr or alpha s or things

2038
01:31:04,790 --> 01:31:09,280
like that you know they aim at having

2039
01:31:06,200 --> 01:31:11,750
high throughput and having a low q and

2040
01:31:09,280 --> 01:31:14,000
having a large q just isn't good for

2041
01:31:11,750 --> 01:31:15,740
anybody I think there is such a

2042
01:31:14,000 --> 01:31:18,050
trade-off when you're trying to compete

2043
01:31:15,740 --> 01:31:19,760
with TCP so there is you know you had

2044
01:31:18,050 --> 01:31:21,590
this in your in your competitive mode

2045
01:31:19,760 --> 01:31:23,090
you can try to be a bit more or less

2046
01:31:21,590 --> 01:31:26,240
aggressive which will produce more or

2047
01:31:23,090 --> 01:31:28,280
less queuing so I guess there you have

2048
01:31:26,240 --> 01:31:29,809
to trade off but whenever you are able

2049
01:31:28,280 --> 01:31:31,910
to detect that there isn't only any

2050
01:31:29,810 --> 01:31:34,130
other traffic competing with you I don't

2051
01:31:31,910 --> 01:31:36,740
think the idea of have of including such

2052
01:31:34,130 --> 01:31:39,920
a trade-off in the design as a base idea

2053
01:31:36,740 --> 01:31:41,990
is good at all so my answer to that is

2054
01:31:39,920 --> 01:31:44,960
like maybe you're right

2055
01:31:41,990 --> 01:31:47,139
like I don't know for sure but the

2056
01:31:44,960 --> 01:31:49,460
trade-off does exist on the product side

2057
01:31:47,140 --> 01:31:51,140
because on the other hand we have video

2058
01:31:49,460 --> 01:31:53,480
calling applications which have not been

2059
01:31:51,140 --> 01:31:53,960
able to use cubic or bbr so far as far

2060
01:31:53,480 --> 01:31:56,330
as I know

2061
01:31:53,960 --> 01:31:57,980
they have to use something like Google

2062
01:31:56,330 --> 01:31:59,989
condition control where they have to

2063
01:31:57,980 --> 01:32:04,280
really optimize for end to end delays

2064
01:31:59,989 --> 01:32:06,349
right on the transport side like we did

2065
01:32:04,280 --> 01:32:08,630
run some limited experiments by using

2066
01:32:06,350 --> 01:32:10,310
like a very high value of Delta right

2067
01:32:08,630 --> 01:32:12,290
and I did see that queuing delays

2068
01:32:10,310 --> 01:32:15,110
decreased even more so I mean I don't

2069
01:32:12,290 --> 01:32:16,400
know like fundamentally if like it's a

2070
01:32:15,110 --> 01:32:17,900
good idea or not but from the

2071
01:32:16,400 --> 01:32:20,179
application point of view this

2072
01:32:17,900 --> 01:32:22,219
definitely a trade-off and if the

2073
01:32:20,180 --> 01:32:23,540
condition control could also kind of

2074
01:32:22,219 --> 01:32:26,330
like give you a varying performance

2075
01:32:23,540 --> 01:32:27,920
depending on like what like it could

2076
01:32:26,330 --> 01:32:31,850
also be expressed in a different way for

2077
01:32:27,920 --> 01:32:33,830
example prop 8 exposes a target delay to

2078
01:32:31,850 --> 01:32:36,650
the application and it will just try to

2079
01:32:33,830 --> 01:32:38,300
optimize and limit the queue the

2080
01:32:36,650 --> 01:32:39,860
bottleneck you delay it to that like

2081
01:32:38,300 --> 01:32:42,560
that could be an alternative way for

2082
01:32:39,860 --> 01:32:44,059
application to benefit from it like I

2083
01:32:42,560 --> 01:32:47,060
honestly don't know

2084
01:32:44,060 --> 01:32:59,500
I can maybe point out some other work

2085
01:32:47,060 --> 01:33:03,340
offline yeah Stuart Cheshire from Apple

2086
01:32:59,500 --> 01:33:05,930
you presented some results showing

2087
01:33:03,340 --> 01:33:08,690
marginally better application throughput

2088
01:33:05,930 --> 01:33:10,640
which is interesting I think it would be

2089
01:33:08,690 --> 01:33:11,509
interesting to understand where that's

2090
01:33:10,640 --> 01:33:15,530
coming from

2091
01:33:11,510 --> 01:33:18,140
because either the other congestion

2092
01:33:15,530 --> 01:33:21,290
control algorithms were letting the line

2093
01:33:18,140 --> 01:33:23,210
go idle and wasting capacity or they

2094
01:33:21,290 --> 01:33:25,840
were retransmitting unnecessary and

2095
01:33:23,210 --> 01:33:28,969
wasting capacity with duplicates or

2096
01:33:25,840 --> 01:33:31,190
something else and it would be really

2097
01:33:28,969 --> 01:33:39,850
interesting to know where that extra is

2098
01:33:31,190 --> 01:33:44,629
coming from where these tests done on

2099
01:33:39,850 --> 01:33:47,600
Wi-Fi links or LTE good questions so we

2100
01:33:44,630 --> 01:33:50,330
ran these tests globally and when I

2101
01:33:47,600 --> 01:33:55,010
looked at the data like about 3/4 of the

2102
01:33:50,330 --> 01:33:56,690
users were using cellular I did not I

2103
01:33:55,010 --> 01:33:59,150
don't know like LTE versus 3G or

2104
01:33:56,690 --> 01:34:01,700
something else and about 1/4 were using

2105
01:33:59,150 --> 01:34:03,049
Wi-Fi but in the Wi-Fi case there would

2106
01:34:01,700 --> 01:34:04,580
be competing traffic at the bottleneck

2107
01:34:03,050 --> 01:34:06,950
because it would be probably the first

2108
01:34:04,580 --> 01:34:09,200
or probably

2109
01:34:06,950 --> 01:34:11,030
and when I looked at the results and

2110
01:34:09,200 --> 01:34:13,309
compared them like the trend was similar

2111
01:34:11,030 --> 01:34:15,050
for Wi-Fi and cellular although we saw

2112
01:34:13,310 --> 01:34:17,870
more wins on cellular as compared to

2113
01:34:15,050 --> 01:34:22,900
Wi-Fi and what is the Delta value for

2114
01:34:17,870 --> 01:34:22,900
these experiments no 2.04 Thank You

2115
01:34:24,370 --> 01:34:29,780
Roberta they own

2116
01:34:26,270 --> 01:34:31,460
I just wanted to push back on the labor

2117
01:34:29,780 --> 01:34:34,340
sis bandwidth not making sense

2118
01:34:31,460 --> 01:34:37,670
I also want to agree with delay versus

2119
01:34:34,340 --> 01:34:39,380
bandwidth not making sense I'm holding

2120
01:34:37,670 --> 01:34:42,440
both of these opinions simultaneously

2121
01:34:39,380 --> 01:34:44,690
because it's hard to define what the

2122
01:34:42,440 --> 01:34:46,160
network is does the network diff'ent is

2123
01:34:44,690 --> 01:34:48,589
the boundary of the network between the

2124
01:34:46,160 --> 01:34:51,680
application and the operating system or

2125
01:34:48,590 --> 01:34:56,680
is it between the nic sending packets

2126
01:34:51,680 --> 01:34:58,850
and the fabric to another NIC right the

2127
01:34:56,680 --> 01:35:00,410
you know depending on where we are in

2128
01:34:58,850 --> 01:35:01,970
the stack we tend to think of either of

2129
01:35:00,410 --> 01:35:04,730
these as the network and try to exclude

2130
01:35:01,970 --> 01:35:07,190
one of the others generally the internal

2131
01:35:04,730 --> 01:35:10,129
to the host part so we know from

2132
01:35:07,190 --> 01:35:12,799
previous presentation by Neil Cardwell

2133
01:35:10,130 --> 01:35:14,060
for instance that there are parts of the

2134
01:35:12,800 --> 01:35:16,850
stack that are trying to reduce the

2135
01:35:14,060 --> 01:35:20,420
amount of overhead that we are

2136
01:35:16,850 --> 01:35:22,190
experiencing within the host and the

2137
01:35:20,420 --> 01:35:24,620
effort by which we do this and of course

2138
01:35:22,190 --> 01:35:27,440
there's Nagle's etc right the effort by

2139
01:35:24,620 --> 01:35:31,280
which we do this is really a trade-off

2140
01:35:27,440 --> 01:35:32,540
of how expensive it is in CPU versus how

2141
01:35:31,280 --> 01:35:38,660
important it is to get the information

2142
01:35:32,540 --> 01:35:40,840
through so there you go sorry thank you

2143
01:35:38,660 --> 01:35:43,730
thank you so much folks and I want to

2144
01:35:40,840 --> 01:35:45,710
thank mitten for showing up here and

2145
01:35:43,730 --> 01:35:48,440
presenting this please make him feel

2146
01:35:45,710 --> 01:35:55,880
welcome it's his first IETF /i r TF /

2147
01:35:48,440 --> 01:35:59,120
ICC RG meeting and for everybody else

2148
01:35:55,880 --> 01:36:00,320
watching I love to see more

2149
01:35:59,120 --> 01:36:03,530
presentations like this if you are doing

2150
01:36:00,320 --> 01:36:06,650
in experiments in your house or in your

2151
01:36:03,530 --> 01:36:07,790
lab or at your company come share them

2152
01:36:06,650 --> 01:36:13,700
with us

2153
01:36:07,790 --> 01:36:18,070
and we now move on to Marcus and get

2154
01:36:13,700 --> 01:36:18,070
this up mc/dc CP

2155
01:36:20,930 --> 01:36:27,390
there you go take it away my name is

2156
01:36:25,320 --> 01:36:31,019
Marcus Armand from Deutsche Telekom and

2157
01:36:27,390 --> 01:36:33,120
today I want to present some joint work

2158
01:36:31,020 --> 01:36:35,750
with Karcher at University and City

2159
01:36:33,120 --> 01:36:42,000
University London on multipass

2160
01:36:35,750 --> 01:36:45,000
multi-party ccp that we intend to to use

2161
01:36:42,000 --> 01:36:48,630
for providing multiple capabilities for

2162
01:36:45,000 --> 01:36:52,980
UDP and IP traffic why I want to

2163
01:36:48,630 --> 01:36:55,950
presented today here at ICC RG is that

2164
01:36:52,980 --> 01:36:59,009
we see a challenge there in respect to

2165
01:36:55,950 --> 01:37:02,340
congestion control and I think you you

2166
01:36:59,010 --> 01:37:05,430
are the right group to become aware of

2167
01:37:02,340 --> 01:37:07,620
this and maybe work together with us on

2168
01:37:05,430 --> 01:37:11,160
a solution B and we also think that this

2169
01:37:07,620 --> 01:37:14,160
is not a limited issue to the multipass

2170
01:37:11,160 --> 01:37:16,500
tcp maybe that is also something which

2171
01:37:14,160 --> 01:37:21,150
affects in future the community we come

2172
01:37:16,500 --> 01:37:22,920
to this during the presentation first of

2173
01:37:21,150 --> 01:37:25,349
all I want to give you a short

2174
01:37:22,920 --> 01:37:27,890
introduction why are we working on multi

2175
01:37:25,350 --> 01:37:30,180
pass DCP what is the reason behind I

2176
01:37:27,890 --> 01:37:34,050
want to make you aware about the

2177
01:37:30,180 --> 01:37:36,540
development we did so far and then

2178
01:37:34,050 --> 01:37:42,330
coming quickly to the point where we see

2179
01:37:36,540 --> 01:37:46,650
the issue of congestion control the

2180
01:37:42,330 --> 01:37:50,190
multipath TCP as such is usually pushed

2181
01:37:46,650 --> 01:37:55,049
at TS vwg where we also have some

2182
01:37:50,190 --> 01:37:57,750
updates in the afternoon session okay

2183
01:37:55,050 --> 01:38:01,950
starting with them motivation why are we

2184
01:37:57,750 --> 01:38:04,860
working on on a solution for providing

2185
01:38:01,950 --> 01:38:09,139
multiple capabilities to two UDP or IP

2186
01:38:04,860 --> 01:38:12,509
traffic so as you may know there are

2187
01:38:09,140 --> 01:38:17,460
specifications ongoing in the 3gpp area

2188
01:38:12,510 --> 01:38:19,980
and also in the BBF proppant forum on

2189
01:38:17,460 --> 01:38:24,600
multi connectivity architectures to

2190
01:38:19,980 --> 01:38:27,959
provide multi-part usage to either

2191
01:38:24,600 --> 01:38:30,420
mobile phones though that is on the

2192
01:38:27,960 --> 01:38:31,890
upper part that is directly happening at

2193
01:38:30,420 --> 01:38:34,380
3gpp

2194
01:38:31,890 --> 01:38:39,570
released 16 so within the 5g

2195
01:38:34,380 --> 01:38:41,880
standardization and they're discussed in

2196
01:38:39,570 --> 01:38:44,820
in the area of 80 s SS XS traffic

2197
01:38:41,880 --> 01:38:47,700
steering switching splitting it's an

2198
01:38:44,820 --> 01:38:49,230
operator controlled approach so that's

2199
01:38:47,700 --> 01:38:50,340
why we are so telecom are very

2200
01:38:49,230 --> 01:38:53,250
interested in that

2201
01:38:50,340 --> 01:38:56,910
that means multi connectivity is

2202
01:38:53,250 --> 01:39:01,200
terminated in the operator network the

2203
01:38:56,910 --> 01:39:04,610
same is also happening for the hybrid

2204
01:39:01,200 --> 01:39:09,720
access use case where multi connectivity

2205
01:39:04,610 --> 01:39:13,980
is applied between a multi path capable

2206
01:39:09,720 --> 01:39:17,580
CPE using cellular and fixed access and

2207
01:39:13,980 --> 01:39:20,730
again the operator network in future the

2208
01:39:17,580 --> 01:39:25,640
IP Texas will most totally also rely on

2209
01:39:20,730 --> 01:39:28,320
the 3gpp a TSSs specification and

2210
01:39:25,640 --> 01:39:32,400
currently in release sixteen multiple

2211
01:39:28,320 --> 01:39:38,150
CCP is a protocol which is defined for

2212
01:39:32,400 --> 01:39:38,150
making simultaneous use of multiple X's

2213
01:39:38,750 --> 01:39:45,900
if you start now with having a look on

2214
01:39:42,390 --> 01:39:48,540
how the traffic mix has changed over the

2215
01:39:45,900 --> 01:39:51,360
last years I I think I don't tell tell

2216
01:39:48,540 --> 01:39:54,170
you something you we see that the quick

2217
01:39:51,360 --> 01:39:56,519
traffic is coming up more and more

2218
01:39:54,170 --> 01:40:01,890
though that is some statistic from our

2219
01:39:56,520 --> 01:40:07,320
fixed network in 2018 where we already

2220
01:40:01,890 --> 01:40:12,150
see more or less 12% of quick traffic

2221
01:40:07,320 --> 01:40:14,250
and we think with HTTP 3 deployment this

2222
01:40:12,150 --> 01:40:16,950
will become more and more and maybe it

2223
01:40:14,250 --> 01:40:20,280
comes to the point where Creek or UDP

2224
01:40:16,950 --> 01:40:22,019
traffic in future dominate the TCP so

2225
01:40:20,280 --> 01:40:25,259
that is totally different to what we

2226
01:40:22,020 --> 01:40:28,370
have seen maybe three four or five years

2227
01:40:25,260 --> 01:40:32,940
ago where TCP where the dominating

2228
01:40:28,370 --> 01:40:38,370
protocol with almost 100 percent with

2229
01:40:32,940 --> 01:40:41,759
all almost 100 percent share but coming

2230
01:40:38,370 --> 01:40:43,620
now back to to these multi connectivity

2231
01:40:41,760 --> 01:40:46,460
architectures we have seen before

2232
01:40:43,620 --> 01:40:50,070
within the a TSSs

2233
01:40:46,460 --> 01:40:54,570
we're multipass TCP the protocol for

2234
01:40:50,070 --> 01:40:59,090
providing multipath transmission this

2235
01:40:54,570 --> 01:41:04,139
means that it cannot cover anymore all

2236
01:40:59,090 --> 01:41:06,090
the traffic which is today generated

2237
01:41:04,139 --> 01:41:10,199
between customers and and the internet

2238
01:41:06,090 --> 01:41:12,030
the Gothic week is coming so our finding

2239
01:41:10,199 --> 01:41:16,349
so far as we have a long lasting

2240
01:41:12,030 --> 01:41:17,759
experience was not a past TCP it's a

2241
01:41:16,350 --> 01:41:21,360
very well working protocol it's

2242
01:41:17,760 --> 01:41:27,440
efficient it's a good candidate to use

2243
01:41:21,360 --> 01:41:31,830
or to provide yeah multi connectivity

2244
01:41:27,440 --> 01:41:34,230
services and there's also finding that

2245
01:41:31,830 --> 01:41:37,050
the congestion control should buy TCP

2246
01:41:34,230 --> 01:41:41,159
and also used with a multi pass TCP for

2247
01:41:37,050 --> 01:41:45,510
scheduling decisions and so on it's very

2248
01:41:41,159 --> 01:41:47,250
beneficial another finding is so far if

2249
01:41:45,510 --> 01:41:51,360
you look at IKEA for example there is no

2250
01:41:47,250 --> 01:41:56,940
multi part of a call for UDP or IP

2251
01:41:51,360 --> 01:41:59,909
traffic and an encapsulation into

2252
01:41:56,940 --> 01:42:02,280
multiple TCP of such protocols it's not

2253
01:41:59,909 --> 01:42:04,440
an option at least that is our opinion

2254
01:42:02,280 --> 01:42:06,750
and that was also something discussed

2255
01:42:04,440 --> 01:42:09,570
years ago in the multiple TCP mailing

2256
01:42:06,750 --> 01:42:12,420
list that this does not make sense

2257
01:42:09,570 --> 01:42:15,150
because if you encapsulate UDP or IP

2258
01:42:12,420 --> 01:42:20,010
traffic into multiple TCP then you

2259
01:42:15,150 --> 01:42:22,349
impose the reliable nature of TCP so a

2260
01:42:20,010 --> 01:42:26,370
potential multipath solution for a UDP

2261
01:42:22,350 --> 01:42:29,489
or even IP traffic must not impose TCP

2262
01:42:26,370 --> 01:42:30,750
like reliability additional high latency

2263
01:42:29,489 --> 01:42:33,360
packets cramping or head-of-line

2264
01:42:30,750 --> 01:42:35,670
blocking otherwise it Prague the UDP and

2265
01:42:33,360 --> 01:42:40,380
IP principles on transportation and

2266
01:42:35,670 --> 01:42:42,179
service expectations before I come now

2267
01:42:40,380 --> 01:42:46,340
to the solution and talk about the

2268
01:42:42,179 --> 01:42:49,290
multipath DCP itself I want to present

2269
01:42:46,340 --> 01:42:53,340
what what are the key components for a

2270
01:42:49,290 --> 01:42:55,620
multipath transmission sure for sure you

2271
01:42:53,340 --> 01:42:58,170
have a generator and a receiver part in

2272
01:42:55,620 --> 01:42:59,719
between you have multiple paths or

2273
01:42:58,170 --> 01:43:04,639
multiple X

2274
01:42:59,719 --> 01:43:11,239
so at least more than one path to

2275
01:43:04,639 --> 01:43:14,780
provide multi-multi connectivity and you

2276
01:43:11,239 --> 01:43:18,129
you need a you need a scheduling unit to

2277
01:43:14,780 --> 01:43:21,110
distribute traffic over a multiple part

2278
01:43:18,130 --> 01:43:24,170
and it is very beneficial to have a path

2279
01:43:21,110 --> 01:43:26,000
estimation for the scheduling unit which

2280
01:43:24,170 --> 01:43:27,920
gives you information about the path

2281
01:43:26,000 --> 01:43:30,920
characteristic latency is data rate

2282
01:43:27,920 --> 01:43:33,080
available capacities so that the

2283
01:43:30,920 --> 01:43:36,020
sketching unit can make proper decisions

2284
01:43:33,080 --> 01:43:39,170
efficient decisions and that's not

2285
01:43:36,020 --> 01:43:41,540
overload the path may be according to

2286
01:43:39,170 --> 01:43:44,139
two policies preferring paths with low

2287
01:43:41,540 --> 01:43:49,340
latencies and it needs this information

2288
01:43:44,139 --> 01:43:56,210
which which latencies are applied to the

2289
01:43:49,340 --> 01:43:58,010
individual path and also in most

2290
01:43:56,210 --> 01:44:01,040
scenarios and real scenarios you need

2291
01:43:58,010 --> 01:44:02,719
some we or a queue to compensate and the

2292
01:44:01,040 --> 01:44:05,600
different paths or icterus takes to

2293
01:44:02,719 --> 01:44:09,050
compensate the latency differences and

2294
01:44:05,600 --> 01:44:12,530
so on for the three ordering unit or

2295
01:44:09,050 --> 01:44:14,270
poor for making this reordering working

2296
01:44:12,530 --> 01:44:17,509
for sure you need also some some

2297
01:44:14,270 --> 01:44:24,440
sequencing but that is that is very

2298
01:44:17,510 --> 01:44:28,159
simple as such so the the the most

2299
01:44:24,440 --> 01:44:29,839
critical components I see are the

2300
01:44:28,159 --> 01:44:31,879
scheduling the path estimation in the

2301
01:44:29,840 --> 01:44:34,670
reordering so all of this has worked

2302
01:44:31,880 --> 01:44:38,179
together and to make efficient

2303
01:44:34,670 --> 01:44:46,310
multi-party which possible now coming to

2304
01:44:38,179 --> 01:44:49,219
the solution multipass DCP so at the ITF

2305
01:44:46,310 --> 01:44:51,590
which which types of protocols are

2306
01:44:49,219 --> 01:44:57,170
available which can support us in

2307
01:44:51,590 --> 01:45:00,980
finding a solution to transport UDP IP

2308
01:44:57,170 --> 01:45:04,040
traffic over multiple paths and there we

2309
01:45:00,980 --> 01:45:08,690
came up with the DCP protocol which is a

2310
01:45:04,040 --> 01:45:11,030
mix between TCP and UDP it's it's of our

2311
01:45:08,690 --> 01:45:12,659
unreliable nature but it has a

2312
01:45:11,030 --> 01:45:16,590
congestion control

2313
01:45:12,659 --> 01:45:21,750
and that we think we can exploit for our

2314
01:45:16,590 --> 01:45:25,159
purpose purposes so what we imagine is

2315
01:45:21,750 --> 01:45:29,659
we have the multipath DC CP as such

2316
01:45:25,159 --> 01:45:33,059
which by itself is only possible to to

2317
01:45:29,659 --> 01:45:34,379
distribute a TCP traffic over multiple

2318
01:45:33,060 --> 01:45:38,429
paths but if you combine it with a

2319
01:45:34,380 --> 01:45:40,699
multipath framework which more or less

2320
01:45:38,429 --> 01:45:43,860
means we have virtual network interfaces

2321
01:45:40,699 --> 01:45:48,839
which can consume any kind of IP traffic

2322
01:45:43,860 --> 01:45:51,120
and the the traffic which is is incoming

2323
01:45:48,840 --> 01:45:54,170
into this virtual network interface then

2324
01:45:51,120 --> 01:45:57,989
encapsulated into the multi pass TCP

2325
01:45:54,170 --> 01:46:03,929
distributed and then on receiver side

2326
01:45:57,989 --> 01:46:07,199
again may be reordered and then stripped

2327
01:46:03,929 --> 01:46:10,429
all the DCP information and send out of

2328
01:46:07,199 --> 01:46:14,719
the outgoing virtual network interface

2329
01:46:10,429 --> 01:46:20,159
so that is a rough description of how we

2330
01:46:14,719 --> 01:46:22,230
imagining a solution you can you can

2331
01:46:20,159 --> 01:46:26,929
find more details in all the draft I

2332
01:46:22,230 --> 01:46:30,500
have mentioned here on this slide and

2333
01:46:26,929 --> 01:46:32,880
that is our prototype and also our

2334
01:46:30,500 --> 01:46:36,110
simulation test that we have available

2335
01:46:32,880 --> 01:46:39,480
so we have a Linux kernel implementation

2336
01:46:36,110 --> 01:46:46,290
extending the the available DCC keep of

2337
01:46:39,480 --> 01:46:48,120
a call by multi part capability and the

2338
01:46:46,290 --> 01:46:54,360
same we have available in nf3

2339
01:46:48,120 --> 01:46:58,349
for simulation purposes and it's the

2340
01:46:54,360 --> 01:47:00,900
descender side you can see on the on the

2341
01:46:58,350 --> 01:47:03,570
left side the multiple CCP home gateway

2342
01:47:00,900 --> 01:47:07,949
as we have called it and that is very

2343
01:47:03,570 --> 01:47:11,699
similar to to multi pass tcp we have a

2344
01:47:07,949 --> 01:47:14,009
modular scapula which can load different

2345
01:47:11,699 --> 01:47:16,949
kind of scheduling algorithms

2346
01:47:14,010 --> 01:47:19,159
round-robin for example a cheapest pipe

2347
01:47:16,949 --> 01:47:21,949
first which is a path prioritization

2348
01:47:19,159 --> 01:47:25,199
scheduler but also more sophisticated

2349
01:47:21,949 --> 01:47:27,400
ODS for example out of order for inorder

2350
01:47:25,199 --> 01:47:30,070
arrival schedule

2351
01:47:27,400 --> 01:47:32,909
on receiver side and that is something

2352
01:47:30,070 --> 01:47:35,500
you compare to to multiple CCP we have

2353
01:47:32,909 --> 01:47:38,790
reordering modules so we can apply

2354
01:47:35,500 --> 01:47:42,969
different kind of reordering mechanisms

2355
01:47:38,790 --> 01:47:48,239
we have a passive one which more or less

2356
01:47:42,969 --> 01:47:52,330
does nothing we have a adaptive fixed

2357
01:47:48,239 --> 01:47:55,650
algorithm which waits for a fixed time

2358
01:47:52,330 --> 01:47:59,559
when when packets are missed and then

2359
01:47:55,650 --> 01:48:02,259
skips possible gaps in the reordering

2360
01:47:59,560 --> 01:48:05,650
queue we have a depth effective so that

2361
01:48:02,260 --> 01:48:08,110
is a dynamic approach using the RTT

2362
01:48:05,650 --> 01:48:10,920
informality the latency information from

2363
01:48:08,110 --> 01:48:13,269
the congestion control put a ten-time

2364
01:48:10,920 --> 01:48:17,699
and last but not least we have delay

2365
01:48:13,270 --> 01:48:20,340
equalization which is more or less no

2366
01:48:17,699 --> 01:48:23,830
reordering approach it just tries to

2367
01:48:20,340 --> 01:48:26,650
delay the packet on the faster path by

2368
01:48:23,830 --> 01:48:29,380
the latency difference we have a path

2369
01:48:26,650 --> 01:48:33,120
manager we have this virtual network

2370
01:48:29,380 --> 01:48:35,830
interfaces from the multi path framework

2371
01:48:33,120 --> 01:48:38,790
implemented so it's more or less

2372
01:48:35,830 --> 01:48:47,139
complete a set up which we can use for

2373
01:48:38,790 --> 01:48:51,540
testing presenting now some results in

2374
01:48:47,139 --> 01:48:54,639
short that is really a short sort of

2375
01:48:51,540 --> 01:48:58,239
results just to make you aware what is

2376
01:48:54,639 --> 01:49:03,060
already working and then coming to the

2377
01:48:58,239 --> 01:49:08,138
challenge we seen so that is that is our

2378
01:49:03,060 --> 01:49:12,300
na 3 setup where you see that we have

2379
01:49:08,139 --> 01:49:15,790
plenty of possibilities we can simulate

2380
01:49:12,300 --> 01:49:18,340
different you ease we we can simulate

2381
01:49:15,790 --> 01:49:21,820
you know tea or cellular axis we can

2382
01:49:18,340 --> 01:49:25,120
simulate Wi-Fi access and also and we

2383
01:49:21,820 --> 01:49:28,150
can simulate moving scenarios where we

2384
01:49:25,120 --> 01:49:30,639
move you ease from one point to the

2385
01:49:28,150 --> 01:49:32,888
other which maybe lead to the effect and

2386
01:49:30,639 --> 01:49:37,780
therefore I have results in the next

2387
01:49:32,889 --> 01:49:39,790
slide that we lose connectivity or that

2388
01:49:37,780 --> 01:49:44,580
we enter

2389
01:49:39,790 --> 01:49:52,360
you access nodes and we can get

2390
01:49:44,580 --> 01:49:56,620
connectivity so that that our results

2391
01:49:52,360 --> 01:50:01,570
from last ITF but again just to remember

2392
01:49:56,620 --> 01:50:04,059
you so switching and aggregation is it's

2393
01:50:01,570 --> 01:50:07,299
possible with our multi-party ccp setup

2394
01:50:04,060 --> 01:50:16,840
so on the top you see when we move and

2395
01:50:07,300 --> 01:50:18,520
you e out of a Wi-Fi access so that the

2396
01:50:16,840 --> 01:50:19,000
the Wi-Fi access is not available

2397
01:50:18,520 --> 01:50:23,080
anymore

2398
01:50:19,000 --> 01:50:25,780
then the ue automatically connects to

2399
01:50:23,080 --> 01:50:28,510
the LTE and the multi-party Suzuki is

2400
01:50:25,780 --> 01:50:32,110
capable of doing this in a in a seamless

2401
01:50:28,510 --> 01:50:35,500
way and with the fewest switching

2402
01:50:32,110 --> 01:50:38,019
approach you see between switching from

2403
01:50:35,500 --> 01:50:39,880
by factory suddenly you have a get with

2404
01:50:38,020 --> 01:50:42,460
aggregation mode which was also

2405
01:50:39,880 --> 01:50:48,490
supported by multiple CCP you see a more

2406
01:50:42,460 --> 01:50:54,490
or less as seamless hand over without

2407
01:50:48,490 --> 01:50:57,519
having a gap between switching from

2408
01:50:54,490 --> 01:51:02,710
Wi-Fi to LTE we think that is a very

2409
01:50:57,520 --> 01:51:04,930
beneficial scenario the same test we

2410
01:51:02,710 --> 01:51:08,950
also did and MP think that is very

2411
01:51:04,930 --> 01:51:11,230
realistic maybe I have to switch back so

2412
01:51:08,950 --> 01:51:15,820
that what was pure UDP traffic it was

2413
01:51:11,230 --> 01:51:20,169
some some iperf UDP traffic we generated

2414
01:51:15,820 --> 01:51:24,070
but that is not very realistic so we did

2415
01:51:20,170 --> 01:51:28,180
the same test with UDP combined with

2416
01:51:24,070 --> 01:51:31,059
Nara congestion control most of the UDP

2417
01:51:28,180 --> 01:51:33,850
traffic we see in the field is to some

2418
01:51:31,060 --> 01:51:36,940
extent congestion controlled and if you

2419
01:51:33,850 --> 01:51:38,410
talk about quick then it's it's pretty

2420
01:51:36,940 --> 01:51:42,129
clear that this is congestion control

2421
01:51:38,410 --> 01:51:45,340
and you see here in the same scenario

2422
01:51:42,130 --> 01:51:51,660
you're switching or aggregation it also

2423
01:51:45,340 --> 01:51:53,530
works to some extent but you see some

2424
01:51:51,660 --> 01:51:56,410
some interact

2425
01:51:53,530 --> 01:51:59,320
between the Nara congestion control and

2426
01:51:56,410 --> 01:52:03,639
also the D CCP congestion control you

2427
01:51:59,320 --> 01:52:07,660
have this black reference line so that

2428
01:52:03,640 --> 01:52:13,930
is from the previous slide the results

2429
01:52:07,660 --> 01:52:16,889
where you have seen more or less yeah

2430
01:52:13,930 --> 01:52:20,920
maybe where you have seen that after

2431
01:52:16,890 --> 01:52:24,850
switching from one access to the other a

2432
01:52:20,920 --> 01:52:27,810
direct use of the full capacity was

2433
01:52:24,850 --> 01:52:31,210
possible whereas whereas now with the

2434
01:52:27,810 --> 01:52:37,240
nadir controlled UDP traffic you see it

2435
01:52:31,210 --> 01:52:40,570
takes on time to to adapt this one I I

2436
01:52:37,240 --> 01:52:43,599
will skip for time reasons and that is

2437
01:52:40,570 --> 01:52:49,389
now also exploring a little bit what's

2438
01:52:43,600 --> 01:52:53,370
going on in in terms of delay that is

2439
01:52:49,390 --> 01:52:58,180
also some result from from last our ITF

2440
01:52:53,370 --> 01:53:02,200
where will be proved that reordering

2441
01:52:58,180 --> 01:53:05,230
makes sense we need reordering in a in a

2442
01:53:02,200 --> 01:53:11,139
multi pass set up supporting UDP traffic

2443
01:53:05,230 --> 01:53:15,190
and the cream bar here that is proving

2444
01:53:11,140 --> 01:53:18,670
that reordering is beneficial over

2445
01:53:15,190 --> 01:53:21,339
scenarios without reordering and you can

2446
01:53:18,670 --> 01:53:28,960
find the details in the slides from from

2447
01:53:21,340 --> 01:53:33,100
last ITF what is new we have some real

2448
01:53:28,960 --> 01:53:36,460
world results with YouTube traffic based

2449
01:53:33,100 --> 01:53:38,560
on the call now and you see here the set

2450
01:53:36,460 --> 01:53:40,870
up we have the multi Party CCP client

2451
01:53:38,560 --> 01:53:43,870
and multi pass TCP server that is a

2452
01:53:40,870 --> 01:53:46,390
local set up and but connected to the

2453
01:53:43,870 --> 01:53:48,790
internet connected to the multiple TCP

2454
01:53:46,390 --> 01:53:52,690
client we have a laptop running the

2455
01:53:48,790 --> 01:53:57,370
Chrome browser and between the both

2456
01:53:52,690 --> 01:54:01,419
multiple TCP boxes we have two links to

2457
01:53:57,370 --> 01:54:04,170
ethernet links going through so-called

2458
01:54:01,420 --> 01:54:07,300
TC boxes but which we can shape the

2459
01:54:04,170 --> 01:54:12,490
links and where we can change the path

2460
01:54:07,300 --> 01:54:15,810
characteristics as I said we use the

2461
01:54:12,490 --> 01:54:20,610
Chrome browser with that we are able to

2462
01:54:15,810 --> 01:54:28,180
request YouTube videos using the quick

2463
01:54:20,610 --> 01:54:32,139
protocol we requested a static video

2464
01:54:28,180 --> 01:54:36,310
with a fixed resolution and we skipped

2465
01:54:32,140 --> 01:54:43,000
this video at ten thirty fifty 70 90 and

2466
01:54:36,310 --> 01:54:45,640
110 seconds and to force a buffering the

2467
01:54:43,000 --> 01:54:49,150
total curation of the test were 120

2468
01:54:45,640 --> 01:54:51,790
seconds and the network condition

2469
01:54:49,150 --> 01:54:55,870
chained at 60 seconds we see on the next

2470
01:54:51,790 --> 01:54:59,920
slide again so we had to part both at

2471
01:54:55,870 --> 01:55:03,849
one megabit per second and a latency of

2472
01:54:59,920 --> 01:55:07,270
10 on the first path and 50 milliseconds

2473
01:55:03,850 --> 01:55:10,000
on on the second path of the 60 second

2474
01:55:07,270 --> 01:55:12,760
be changed from 10 to 90 milliseconds on

2475
01:55:10,000 --> 01:55:15,610
the first path and on the right side

2476
01:55:12,760 --> 01:55:24,270
view you can see the playback ratio but

2477
01:55:15,610 --> 01:55:31,200
is a data rate where you can see that

2478
01:55:24,270 --> 01:55:35,520
the green circles that are the one using

2479
01:55:31,200 --> 01:55:39,780
multipath in combination with reordering

2480
01:55:35,520 --> 01:55:44,020
promise promises the highest gain

2481
01:55:39,780 --> 01:55:50,500
whereas other scenarios like single pass

2482
01:55:44,020 --> 01:55:59,140
usage or Multi multi Party CCP without

2483
01:55:50,500 --> 01:56:03,960
reordering ya have have some have some

2484
01:55:59,140 --> 01:56:06,370
lower playback ratio compared to this

2485
01:56:03,960 --> 01:56:08,940
multi-party which with with reordering

2486
01:56:06,370 --> 01:56:12,519
however that is not part of this slide

2487
01:56:08,940 --> 01:56:17,740
the detailed evaluation revealed and the

2488
01:56:12,520 --> 01:56:20,680
imperfect parts use it even in the best

2489
01:56:17,740 --> 01:56:23,230
performing use case which means

2490
01:56:20,680 --> 01:56:26,950
even if you have reordering and

2491
01:56:23,230 --> 01:56:29,440
multi-part usage combined there are

2492
01:56:26,950 --> 01:56:33,490
still some room to improve the customer

2493
01:56:29,440 --> 01:56:35,230
experience and now coming to an

2494
01:56:33,490 --> 01:56:37,960
intermediate conclusion before coming

2495
01:56:35,230 --> 01:56:43,719
later to the final conclusion so our

2496
01:56:37,960 --> 01:56:46,270
multiple CCCP prototype and also the the

2497
01:56:43,720 --> 01:56:51,100
the standards or the the troughs we have

2498
01:56:46,270 --> 01:56:54,070
available at TS vwg making very clear

2499
01:56:51,100 --> 01:56:57,520
that this is something which can brought

2500
01:56:54,070 --> 01:57:01,090
a work scheduling profits from the TCP

2501
01:56:57,520 --> 01:57:05,290
congestion control and can therefore

2502
01:57:01,090 --> 01:57:07,660
perform similar to multipath TCP the

2503
01:57:05,290 --> 01:57:09,400
reordering on receiver side is is proof

2504
01:57:07,660 --> 01:57:12,160
mandatory for efficient multipath

2505
01:57:09,400 --> 01:57:14,440
transmission of unreliable traffic smart

2506
01:57:12,160 --> 01:57:16,599
algorithm to keep a traffic flow

2507
01:57:14,440 --> 01:57:18,610
smoothly ongoing are required and can

2508
01:57:16,600 --> 01:57:21,330
make use of congestion control

2509
01:57:18,610 --> 01:57:24,580
information head-of-line blocking is not

2510
01:57:21,330 --> 01:57:26,950
purposeful and should always assess if

2511
01:57:24,580 --> 01:57:29,080
it is worth to wait for missing out of

2512
01:57:26,950 --> 01:57:31,450
other information which means

2513
01:57:29,080 --> 01:57:36,010
encapsulation into multiple TCP doesn't

2514
01:57:31,450 --> 01:57:38,130
make sense for for UDP or IP traffic you

2515
01:57:36,010 --> 01:57:41,610
have also seen that we have plenty of

2516
01:57:38,130 --> 01:57:47,320
scheduling and and reordering algorithms

2517
01:57:41,610 --> 01:57:51,130
in our prototype and all of this

2518
01:57:47,320 --> 01:57:55,690
together benefits from from the

2519
01:57:51,130 --> 01:57:58,960
congestion control the ship with TCP so

2520
01:57:55,690 --> 01:58:04,629
from this perspective it was a very

2521
01:57:58,960 --> 01:58:06,250
clever decision to use the TCP and also

2522
01:58:04,630 --> 01:58:08,560
with the congestion control we can make

2523
01:58:06,250 --> 01:58:10,960
sure that we do not overload paths

2524
01:58:08,560 --> 01:58:14,770
though that we do not force a packet

2525
01:58:10,960 --> 01:58:17,080
loss however the main target protocols

2526
01:58:14,770 --> 01:58:21,370
transmitted over the multipath TCP

2527
01:58:17,080 --> 01:58:24,760
framework like quick employee and own

2528
01:58:21,370 --> 01:58:27,580
congestion control in combination with

2529
01:58:24,760 --> 01:58:29,440
multiple TCP this leads to a kind of

2530
01:58:27,580 --> 01:58:31,630
congestion control over congestion

2531
01:58:29,440 --> 01:58:35,259
control scenario with well known issues

2532
01:58:31,630 --> 01:58:37,840
from literature so

2533
01:58:35,260 --> 01:58:40,890
now the point where I think it becomes

2534
01:58:37,840 --> 01:58:43,900
very interesting for ICC at ICC Archie

2535
01:58:40,890 --> 01:58:46,870
so in all of our tests we used so far

2536
01:58:43,900 --> 01:58:53,290
the CCI d2 which is a TCP like a contest

2537
01:58:46,870 --> 01:58:58,420
in control and we asked ourselves if it

2538
01:58:53,290 --> 01:59:01,989
makes sense to use CC ID - what could be

2539
01:58:58,420 --> 01:59:03,610
the alternative and how intra neural we

2540
01:59:01,989 --> 01:59:04,989
tackle the problem of congestion control

2541
01:59:03,610 --> 01:59:07,599
over congestion control and such

2542
01:59:04,989 --> 01:59:10,089
scenarios if we assume that combustion

2543
01:59:07,600 --> 01:59:25,360
control for multipath purposes with the

2544
01:59:10,090 --> 01:59:27,340
right way to go okay so as I said we

2545
01:59:25,360 --> 01:59:30,280
have this issue of congestion control

2546
01:59:27,340 --> 01:59:33,580
congestion control identified we think

2547
01:59:30,280 --> 01:59:36,070
also that affect the the community so

2548
01:59:33,580 --> 01:59:41,290
there are troughs regarding quick

2549
01:59:36,070 --> 01:59:45,219
tunneling where at the end you you have

2550
01:59:41,290 --> 01:59:46,780
the same question depending on which

2551
01:59:45,220 --> 01:59:50,160
type of traffic you want to tunnel

2552
01:59:46,780 --> 01:59:50,160
through through Creek

2553
01:59:50,310 --> 01:59:57,130
my main question today is ICC actually

2554
01:59:53,739 --> 01:59:58,660
the right place and dealing with this

2555
01:59:57,130 --> 02:00:01,180
question of congestion control over

2556
01:59:58,660 --> 02:00:03,610
congestion control and who is interested

2557
02:00:01,180 --> 02:00:05,500
in elaborating this topic further not

2558
02:00:03,610 --> 02:00:08,019
dedicated to this multi parties yeah I

2559
02:00:05,500 --> 02:00:10,450
think it's a general question but for

2560
02:00:08,020 --> 02:00:13,870
sure it would help us in pushing the

2561
02:00:10,450 --> 02:00:15,970
multi processes EP further thank you

2562
02:00:13,870 --> 02:00:17,260
very pleasant Dacian Marcus I think just

2563
02:00:15,970 --> 02:00:20,200
very quickly to answer the question

2564
02:00:17,260 --> 02:00:23,440
about is this ICC has the right place it

2565
02:00:20,200 --> 02:00:25,660
can be is the Shorin said that I have I

2566
02:00:23,440 --> 02:00:27,309
think the question I would have is other

2567
02:00:25,660 --> 02:00:29,230
other folks who are interested in in

2568
02:00:27,310 --> 02:00:31,050
this problem and I'm not just speaking

2569
02:00:29,230 --> 02:00:34,269
academically is this a real problem

2570
02:00:31,050 --> 02:00:36,160
folks interested in deploying our face

2571
02:00:34,270 --> 02:00:38,350
are they facing this problem in

2572
02:00:36,160 --> 02:00:39,670
production that sort of question if you

2573
02:00:38,350 --> 02:00:40,840
can you know you could you could have

2574
02:00:39,670 --> 02:00:43,060
this conversation on the mailing list

2575
02:00:40,840 --> 02:00:46,320
and see there are people who are

2576
02:00:43,060 --> 02:00:48,480
interested in working with you on on

2577
02:00:46,320 --> 02:00:54,660
establishing relevance

2578
02:00:48,480 --> 02:00:55,549
any other questions we have - Anita a

2579
02:00:54,660 --> 02:00:59,910
quick question

2580
02:00:55,550 --> 02:01:03,060
have you ever experiment with that pass

2581
02:00:59,910 --> 02:01:07,290
is not disjoint no for example with some

2582
02:01:03,060 --> 02:01:08,219
savvy multipass DCC subfloors share the

2583
02:01:07,290 --> 02:01:10,440
same bottleneck

2584
02:01:08,219 --> 02:01:12,690
have you ever experiment with that kind

2585
02:01:10,440 --> 02:01:14,549
of environment the question was will be

2586
02:01:12,690 --> 02:01:17,610
experimented with couple congestion

2587
02:01:14,550 --> 02:01:22,350
control like multiple TCP uses no that

2588
02:01:17,610 --> 02:01:25,380
is on our to do this but I'm not sure at

2589
02:01:22,350 --> 02:01:27,929
the end it also ends up in congestion

2590
02:01:25,380 --> 02:01:30,780
control over congestion control for sure

2591
02:01:27,929 --> 02:01:35,070
we have to do this we should do the such

2592
02:01:30,780 --> 02:01:36,870
test what we have done but what was not

2593
02:01:35,070 --> 02:01:39,840
able to to show up that we have

2594
02:01:36,870 --> 02:01:41,370
implemented PBR 40 CCP and we did some

2595
02:01:39,840 --> 02:01:44,070
tests with that that was very promising

2596
02:01:41,370 --> 02:01:46,019
but copy congestion control is on our

2597
02:01:44,070 --> 02:01:49,559
to-do list we have the issue that we

2598
02:01:46,020 --> 02:01:52,770
have to report everything into the DCP

2599
02:01:49,560 --> 02:01:55,500
world to make such tests I think that's

2600
02:01:52,770 --> 02:02:01,320
important make sure it's on our to-do

2601
02:01:55,500 --> 02:02:04,110
list but again the problem stays at the

2602
02:02:01,320 --> 02:02:06,090
end then we have inner congestion

2603
02:02:04,110 --> 02:02:08,519
control of the piggyback traffic then

2604
02:02:06,090 --> 02:02:11,550
over couple congestion control where

2605
02:02:08,520 --> 02:02:16,080
also think that this end ends up in some

2606
02:02:11,550 --> 02:02:19,650
challenges or conflicts thank you so

2607
02:02:16,080 --> 02:02:20,960
much Marcus and with that we end the

2608
02:02:19,650 --> 02:02:24,199
session I'm going to quickly note that

2609
02:02:20,960 --> 02:02:26,820
in Vancouver I'm trying to get together

2610
02:02:24,199 --> 02:02:29,519
there's been a recent spate of academic

2611
02:02:26,820 --> 02:02:31,949
work on EPR analysis so I'm gonna try

2612
02:02:29,520 --> 02:02:34,320
and get those authors and even need to

2613
02:02:31,949 --> 02:02:37,589
show up or BBF folks to show up in

2614
02:02:34,320 --> 02:02:39,719
Vancouver so I'm hoping to do two ICC re

2615
02:02:37,590 --> 02:02:42,030
sessions in Vancouver four of those

2616
02:02:39,719 --> 02:02:44,640
panel this is tentative of course one of

2617
02:02:42,030 --> 02:02:46,830
them will be around bbr specifically and

2618
02:02:44,640 --> 02:02:49,020
the other one will be more generic but

2619
02:02:46,830 --> 02:02:51,239
if you have any work around that if you

2620
02:02:49,020 --> 02:02:54,179
have any analysis that you've been doing

2621
02:02:51,239 --> 02:02:56,879
with DVR or vrv - please write to me I'd

2622
02:02:54,179 --> 02:03:00,840
love to have that be a part of the

2623
02:02:56,880 --> 02:03:02,510
program and that we will end thank you

2624
02:03:00,840 --> 02:03:04,430
so much

2625
02:03:02,510 --> 02:03:06,440
specifically tank Marcus for winding

2626
02:03:04,430 --> 02:03:10,150
down his kind of his presentation

2627
02:03:06,440 --> 02:03:13,389
quickly and we'll see you in Vancouver

2628
02:03:10,150 --> 02:03:13,389
thank you folks

