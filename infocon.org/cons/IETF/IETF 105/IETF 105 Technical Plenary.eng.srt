1
00:00:10,480 --> 00:00:17,680
we have an announcement to make in the

2
00:00:14,049 --> 00:00:19,390
fall and I'll say kefka's shows we're

3
00:00:17,680 --> 00:00:21,550
having a bit of a technical problem up

4
00:00:19,390 --> 00:00:24,700
front with the projectors these lovely

5
00:00:21,550 --> 00:00:29,290
people are working on it but we want to

6
00:00:24,700 --> 00:00:31,390
let you know that the download all of

7
00:00:29,290 --> 00:00:33,879
the presentations from the from the

8
00:00:31,390 --> 00:00:37,480
website now this is a stress test for

9
00:00:33,880 --> 00:00:40,300
the network because we may have to go

10
00:00:37,480 --> 00:00:44,379
forward with the rest of the program

11
00:00:40,300 --> 00:00:48,459
with those in your laptops as opposed to

12
00:00:44,380 --> 00:00:50,200
on these nice big screens so if you

13
00:00:48,460 --> 00:00:52,380
would like to find your way to the

14
00:00:50,200 --> 00:00:55,000
agenda page now and start downloading

15
00:00:52,380 --> 00:00:57,489
the next step will be Brian Trammell who

16
00:00:55,000 --> 00:00:59,769
will introduce our speakers and provide

17
00:00:57,489 --> 00:01:01,239
backing interpretive dance I've been

18
00:00:59,770 --> 00:01:05,350
joking about that but there's an actual

19
00:01:01,239 --> 00:01:08,289
separate stage for it normally we would

20
00:01:05,349 --> 00:01:10,210
let this debugging go on a little bit

21
00:01:08,290 --> 00:01:12,520
longer but we have a quite a tough

22
00:01:10,210 --> 00:01:15,249
schedule tonight and a lot of people

23
00:01:12,520 --> 00:01:19,090
would be upset if we ran late so we're

24
00:01:15,249 --> 00:01:21,908
going to try it this way please do

25
00:01:19,090 --> 00:01:24,219
download them and please do give your

26
00:01:21,909 --> 00:01:28,289
attention to the speakers even if there

27
00:01:24,219 --> 00:01:28,288
aren't any pretty lights many thanks

28
00:01:51,830 --> 00:01:56,480
so I have the slides here on the monitor

29
00:01:54,710 --> 00:02:00,889
in front of me so if everyone could

30
00:01:56,480 --> 00:02:03,080
please come to the front hi I'm Brian

31
00:02:00,890 --> 00:02:06,140
Trammell I be the emcee for the

32
00:02:03,080 --> 00:02:07,670
technical portion of tonight's whoo or

33
00:02:06,140 --> 00:02:12,290
you can hang out on me Tyco which will

34
00:02:07,670 --> 00:02:15,830
also be a stress test for me deco also a

35
00:02:12,290 --> 00:02:18,230
stress test so as a beginning note uh or

36
00:02:15,830 --> 00:02:20,570
you'll notice on the agendas that we're

37
00:02:18,230 --> 00:02:22,970
trying out something new this time we're

38
00:02:20,570 --> 00:02:25,220
more explicitly splitting the Technic

39
00:02:22,970 --> 00:02:27,200
and tech and admin plenaries um you know

40
00:02:25,220 --> 00:02:29,420
way way way back in the past these were

41
00:02:27,200 --> 00:02:30,739
on separate evenings and then they sort

42
00:02:29,420 --> 00:02:32,869
of came together and then they sort of

43
00:02:30,740 --> 00:02:37,130
came even farther together into a single

44
00:02:32,870 --> 00:02:38,390
session we are splitting these out

45
00:02:37,130 --> 00:02:40,370
explicitly to make sure that we have

46
00:02:38,390 --> 00:02:42,369
enough time for the tech plenary as well

47
00:02:40,370 --> 00:02:45,290
as enough time for the admin plenary

48
00:02:42,370 --> 00:02:46,610
this part of the session is one hour

49
00:02:45,290 --> 00:02:49,070
long which is why we were going ahead

50
00:02:46,610 --> 00:02:53,180
and getting started even though we don't

51
00:02:49,070 --> 00:02:55,910
have video yet um we're going to be

52
00:02:53,180 --> 00:02:57,170
holding pretty strictly to time there

53
00:02:55,910 --> 00:03:00,709
will be time for questions and

54
00:02:57,170 --> 00:03:02,869
discussion at the end um so um you know

55
00:03:00,709 --> 00:03:07,840
clarifying questions only but please

56
00:03:02,870 --> 00:03:07,840
know clarifying questions um with that

57
00:03:08,739 --> 00:03:16,550
privacy question mark what's the delay

58
00:03:12,739 --> 00:03:20,480
on okay all right

59
00:03:16,550 --> 00:03:23,810
yes next slide please as some of you may

60
00:03:20,480 --> 00:03:26,209
be aware the IB and the ITF at least we

61
00:03:23,810 --> 00:03:28,130
hope many in the IETF is deeply

62
00:03:26,209 --> 00:03:29,450
interested in confidentiality on the

63
00:03:28,130 --> 00:03:33,500
internet this is a conversation that

64
00:03:29,450 --> 00:03:34,700
we've had ongoing for a while we were

65
00:03:33,500 --> 00:03:37,910
interested in this in large part for

66
00:03:34,700 --> 00:03:41,320
reasons of privacy um we spend a lot of

67
00:03:37,910 --> 00:03:43,609
time in the working groups these days

68
00:03:41,320 --> 00:03:45,049
that is the most applause I thought I

69
00:03:43,610 --> 00:03:48,050
would ever get for privacy with a

70
00:03:45,050 --> 00:03:51,890
question mark and an IETF meeting post

71
00:03:48,050 --> 00:03:55,580
Vancouver um I'll go ahead and it gets

72
00:03:51,890 --> 00:03:57,380
better um uh we've been talking a lot

73
00:03:55,580 --> 00:03:59,220
about in the in the the working groups

74
00:03:57,380 --> 00:04:00,840
and in the hallways and

75
00:03:59,220 --> 00:04:03,810
you know around the working groups and

76
00:04:00,840 --> 00:04:05,460
in the press and sort of everywhere it's

77
00:04:03,810 --> 00:04:08,340
been a while since we've addressed it in

78
00:04:05,460 --> 00:04:10,320
plenary so we'd like to change that

79
00:04:08,340 --> 00:04:13,320
tonight we have a program where we'd

80
00:04:10,320 --> 00:04:16,469
like to talk about current issues and

81
00:04:13,320 --> 00:04:19,170
eternal issues in Internet privacy with

82
00:04:16,470 --> 00:04:21,840
Arvin drivin and Ryan I worked so hard

83
00:04:19,170 --> 00:04:25,770
to get their name right and Steve

84
00:04:21,839 --> 00:04:27,090
pehlivan Arvin Ryan is an associate

85
00:04:25,770 --> 00:04:29,460
professor of computer science at

86
00:04:27,090 --> 00:04:31,380
Princeton he leads the Princeton web

87
00:04:29,460 --> 00:04:32,820
transparency and accountability project

88
00:04:31,380 --> 00:04:34,140
he's also the recipient of the

89
00:04:32,820 --> 00:04:38,610
presidential Early Career Award for

90
00:04:34,140 --> 00:04:40,349
scientists and engineers there's a word

91
00:04:38,610 --> 00:04:42,090
ceremony for this that he's missing to

92
00:04:40,350 --> 00:04:44,160
be with us tonight so we're very very

93
00:04:42,090 --> 00:04:45,030
honored to have him here he'll be

94
00:04:44,160 --> 00:04:46,680
talking about some of the implications

95
00:04:45,030 --> 00:04:49,679
and current trends on communicate on

96
00:04:46,680 --> 00:04:52,710
communications privacy in large current

97
00:04:49,680 --> 00:04:54,900
trends on communication privacy internet

98
00:04:52,710 --> 00:04:57,840
sort of in the large so he's sort of a

99
00:04:54,900 --> 00:04:59,520
contextual look at this Steve pehlivan

100
00:04:57,840 --> 00:05:03,419
needs no introduction in this room but

101
00:04:59,520 --> 00:05:05,609
I'm gonna try to do so anyway um he is a

102
00:05:03,420 --> 00:05:07,560
former member of the IAB a former

103
00:05:05,610 --> 00:05:09,210
security Area Director he was

104
00:05:07,560 --> 00:05:12,020
instrumental in the creation of Usenet

105
00:05:09,210 --> 00:05:14,489
um some of you may have heard of that

106
00:05:12,020 --> 00:05:16,859
he's currently professor of computer

107
00:05:14,490 --> 00:05:19,590
science at Columbia and affiliate at

108
00:05:16,860 --> 00:05:21,600
Columbia Law he'll be showing us tonight

109
00:05:19,590 --> 00:05:24,090
that an Internet privacy everything old

110
00:05:21,600 --> 00:05:26,480
is new again so thank you both Arvind

111
00:05:24,090 --> 00:05:26,479
come on

112
00:05:31,729 --> 00:05:43,349
all right hi folks can you hear me

113
00:05:34,080 --> 00:05:45,599
I guess not thank you Brian thank you

114
00:05:43,349 --> 00:05:47,429
everyone so I'd like to share with you

115
00:05:45,599 --> 00:05:49,830
what I've learned from a decade of doing

116
00:05:47,429 --> 00:05:52,169
privacy measurement privacy measurement

117
00:05:49,830 --> 00:05:53,849
is kind of a boring sounding term but

118
00:05:52,169 --> 00:05:56,099
what it really means is trying to find

119
00:05:53,849 --> 00:05:57,839
his privacy vulnerabilities ideally on a

120
00:05:56,099 --> 00:05:59,759
large scale and talking millions of

121
00:05:57,839 --> 00:06:02,099
endpoints in an automated or mostly

122
00:05:59,759 --> 00:06:04,800
automated way before I do that I'll

123
00:06:02,099 --> 00:06:07,020
start with a couple of caveats one is I

124
00:06:04,800 --> 00:06:09,119
want to be really upfront that most of

125
00:06:07,020 --> 00:06:11,159
my work has been in the web space and my

126
00:06:09,119 --> 00:06:13,439
prior engagement with standards agencies

127
00:06:11,159 --> 00:06:14,998
has been with the w3c and that's what a

128
00:06:13,439 --> 00:06:16,289
lot of this is going to be informed by

129
00:06:14,999 --> 00:06:18,209
but nonetheless what I'm gonna try

130
00:06:16,289 --> 00:06:19,589
really hard to do is extract some

131
00:06:18,209 --> 00:06:21,330
principles that are much more broadly

132
00:06:19,589 --> 00:06:23,459
applicable and that's what I'd like to

133
00:06:21,330 --> 00:06:24,899
share with you today and another thing

134
00:06:23,459 --> 00:06:26,789
that's gonna be a common theme of their

135
00:06:24,899 --> 00:06:28,199
presentation is that I'm gonna be

136
00:06:26,789 --> 00:06:29,669
talking about issues beyond an

137
00:06:28,199 --> 00:06:32,189
encryption I'm gonna assume that we're

138
00:06:29,669 --> 00:06:33,330
in a world with pervasive encryption and

139
00:06:32,189 --> 00:06:35,550
in fact some of the things that I'll

140
00:06:33,330 --> 00:06:37,800
touch upon are perhaps some downsides of

141
00:06:35,550 --> 00:06:39,779
encryption for privacy and how we can

142
00:06:37,800 --> 00:06:41,099
try to mitigate this so that already

143
00:06:39,779 --> 00:06:42,389
sounds surprising to some of you so I

144
00:06:41,099 --> 00:06:45,330
hope this will be an interesting

145
00:06:42,389 --> 00:06:47,339
discussion I should also say that as you

146
00:06:45,330 --> 00:06:49,109
heard from the intro I'm an academic so

147
00:06:47,339 --> 00:06:50,909
my job is to think you know idealistic

148
00:06:49,110 --> 00:06:52,139
blueSky thoughts there are going to be

149
00:06:50,909 --> 00:06:53,308
points where you're going to feel oh

150
00:06:52,139 --> 00:06:55,289
this will never work in the real world

151
00:06:53,309 --> 00:06:56,969
and you're welcome to come say that in

152
00:06:55,289 --> 00:06:57,628
QA that's totally their game and I

153
00:06:56,969 --> 00:07:00,329
appreciate that

154
00:06:57,629 --> 00:07:01,829
okay so with those caveats here are

155
00:07:00,329 --> 00:07:04,259
three things that I want to share with

156
00:07:01,829 --> 00:07:05,819
you the first thing is an issue that

157
00:07:04,259 --> 00:07:07,529
very often comes up when we're talking

158
00:07:05,819 --> 00:07:09,860
about privacy beyond encryption

159
00:07:07,529 --> 00:07:12,479
especially when we're talking about more

160
00:07:09,860 --> 00:07:14,819
subtle privacy threats such as device

161
00:07:12,479 --> 00:07:16,349
fingerprinting an argument that often

162
00:07:14,819 --> 00:07:18,629
comes up is oh forget about

163
00:07:16,349 --> 00:07:20,909
fingerprinting that ship has sailed

164
00:07:18,629 --> 00:07:22,860
the horses left the barn it's too late

165
00:07:20,909 --> 00:07:24,659
for fingerprinting defenses there are

166
00:07:22,860 --> 00:07:26,369
too many fingerprinting vectors it's too

167
00:07:24,659 --> 00:07:28,139
easy to do tracking so we should just

168
00:07:26,369 --> 00:07:29,759
forget about that and accept that we're

169
00:07:28,139 --> 00:07:32,279
gonna be in a world where it's really

170
00:07:29,759 --> 00:07:33,839
easy to track and profile people and

171
00:07:32,279 --> 00:07:36,119
that is a point of view that I actually

172
00:07:33,839 --> 00:07:38,249
used to subscribe to and what I want to

173
00:07:36,119 --> 00:07:40,349
tell you about is why I changed my mind

174
00:07:38,249 --> 00:07:41,430
and what I learned from that so that's

175
00:07:40,349 --> 00:07:43,740
the first thing I want to tell you about

176
00:07:41,430 --> 00:07:45,720
and specifically this this is come

177
00:07:43,740 --> 00:07:47,910
a lot in the web context so let me start

178
00:07:45,720 --> 00:07:51,150
with that now web fingerprinting as many

179
00:07:47,910 --> 00:07:53,639
of you may know I really came to broad

180
00:07:51,150 --> 00:07:55,318
attention about a decade ago with the

181
00:07:53,639 --> 00:07:57,150
very cool work of the Electronic

182
00:07:55,319 --> 00:07:59,460
Frontier Foundation they made a project

183
00:07:57,150 --> 00:08:01,590
called pit not to click users could go

184
00:07:59,460 --> 00:08:03,150
click the button in it and in fact still

185
00:08:01,590 --> 00:08:07,500
online you can go click the button and

186
00:08:03,150 --> 00:08:11,310
if you did the script on the web page is

187
00:08:07,500 --> 00:08:13,259
going to grab a lot of information from

188
00:08:11,310 --> 00:08:16,740
your web browser like the user agent

189
00:08:13,259 --> 00:08:18,720
various HTTP headers the list of fonts

190
00:08:16,740 --> 00:08:20,400
and plugins that you have installed etc

191
00:08:18,720 --> 00:08:21,870
it's going to use that to constructive

192
00:08:20,400 --> 00:08:24,150
fingerprints and it's gonna measure

193
00:08:21,870 --> 00:08:26,280
among the you know million other people

194
00:08:24,150 --> 00:08:28,349
who have taken the same test as you have

195
00:08:26,280 --> 00:08:30,809
how unique is your fingerprint how many

196
00:08:28,349 --> 00:08:32,669
others in that dataset share the same

197
00:08:30,810 --> 00:08:34,110
fingerprint that you do and the very

198
00:08:32,669 --> 00:08:36,120
interesting thing was it depends on how

199
00:08:34,110 --> 00:08:38,219
you measure depending on whether users

200
00:08:36,120 --> 00:08:41,820
have Flash installed or not at least

201
00:08:38,219 --> 00:08:43,620
back in 2009 over 90% of users had a

202
00:08:41,820 --> 00:08:45,690
unique browser fingerprint and this was

203
00:08:43,620 --> 00:08:47,730
very concerning for privacy advocates

204
00:08:45,690 --> 00:08:49,649
because fingerprinting a lot of people

205
00:08:47,730 --> 00:08:51,420
would consider to be a privacy violation

206
00:08:49,649 --> 00:08:53,250
it cannot be seen or controlled by the

207
00:08:51,420 --> 00:08:55,140
user you can't get rid of it in the same

208
00:08:53,250 --> 00:08:57,390
way that you can clear third-party

209
00:08:55,140 --> 00:08:59,310
cookies so this was a big concern there

210
00:08:57,390 --> 00:09:01,230
was another project called mi unique dot

211
00:08:59,310 --> 00:09:03,599
org it's also still out there by

212
00:09:01,230 --> 00:09:05,339
researchers in INRIA and France also

213
00:09:03,600 --> 00:09:07,440
came to some very broadly similar

214
00:09:05,339 --> 00:09:09,240
conclusions so the question is what do

215
00:09:07,440 --> 00:09:11,520
we do about this how should standards

216
00:09:09,240 --> 00:09:12,899
agencies respond to this ease of

217
00:09:11,520 --> 00:09:15,149
fingerprinting how should browser

218
00:09:12,899 --> 00:09:16,589
vendors respond to this and one way you

219
00:09:15,149 --> 00:09:18,810
could think about this is that there are

220
00:09:16,589 --> 00:09:20,279
way too many fingerprinting vectors and

221
00:09:18,810 --> 00:09:22,260
in fact it is true there are way too

222
00:09:20,279 --> 00:09:24,870
many fingerprinting vectors this is a

223
00:09:22,260 --> 00:09:27,209
partial list and as we're adding a new

224
00:09:24,870 --> 00:09:28,440
features to the web like canvas it only

225
00:09:27,209 --> 00:09:30,239
increases the number of features

226
00:09:28,440 --> 00:09:32,040
available for fingerprinting and

227
00:09:30,240 --> 00:09:34,020
ironically you can see on the list

228
00:09:32,040 --> 00:09:35,579
privacy features like do-not-track

229
00:09:34,020 --> 00:09:37,260
also contribute to fingerprinting

230
00:09:35,579 --> 00:09:39,510
because do you have do not track enable

231
00:09:37,260 --> 00:09:42,510
do not have it enable so that's a small

232
00:09:39,510 --> 00:09:44,700
amount of entropy etc so one thing you

233
00:09:42,510 --> 00:09:46,589
might conclude from this is you know the

234
00:09:44,700 --> 00:09:48,270
horses left the bar and fingerprinting

235
00:09:46,589 --> 00:09:50,490
is devastatingly effective

236
00:09:48,270 --> 00:09:53,130
we shouldn't even try to minimize the

237
00:09:50,490 --> 00:09:55,650
fingerprint ability of new features that

238
00:09:53,130 --> 00:09:56,699
we put into the standard now the w3c to

239
00:09:55,650 --> 00:09:57,360
their credit there were a lot of people

240
00:09:56,699 --> 00:09:59,459
who

241
00:09:57,360 --> 00:10:01,950
I still try to minimize the fingerprint

242
00:09:59,460 --> 00:10:03,300
ability of new features and I don't want

243
00:10:01,950 --> 00:10:05,760
to present this as a criticism of other

244
00:10:03,300 --> 00:10:07,949
people this was absolutely me up until

245
00:10:05,760 --> 00:10:09,660
about a year ago so this is what I

246
00:10:07,950 --> 00:10:10,080
believed and here's why I changed my

247
00:10:09,660 --> 00:10:12,060
mind

248
00:10:10,080 --> 00:10:14,970
so those studies that I present it they

249
00:10:12,060 --> 00:10:17,160
were really you know excellent studies

250
00:10:14,970 --> 00:10:18,600
but there was something wrong with the

251
00:10:17,160 --> 00:10:20,459
way that a lot of people interpreted

252
00:10:18,600 --> 00:10:22,680
them one weird thing about those studies

253
00:10:20,460 --> 00:10:25,290
is that the users who participated were

254
00:10:22,680 --> 00:10:26,939
self selected so does that mean that the

255
00:10:25,290 --> 00:10:28,560
results could be non representative in

256
00:10:26,940 --> 00:10:30,930
some way what could be different about

257
00:10:28,560 --> 00:10:32,880
self selected users one possibility is

258
00:10:30,930 --> 00:10:34,530
that all the users who had self select

259
00:10:32,880 --> 00:10:36,390
into a study like that are actually

260
00:10:34,530 --> 00:10:37,770
really tech savvy people the kind of

261
00:10:36,390 --> 00:10:39,660
people who are likely to make a lot of

262
00:10:37,770 --> 00:10:41,130
modifications in their browsers that

263
00:10:39,660 --> 00:10:43,949
would make them more unique and more

264
00:10:41,130 --> 00:10:45,689
fingerprint Abal so that was one

265
00:10:43,950 --> 00:10:47,490
interesting kind of bias that some

266
00:10:45,690 --> 00:10:48,870
researchers suspected could be in those

267
00:10:47,490 --> 00:10:50,250
studies including some of the

268
00:10:48,870 --> 00:10:51,570
researchers at INRIA who were

269
00:10:50,250 --> 00:10:53,010
responsible for one of those two these

270
00:10:51,570 --> 00:10:54,570
and then what they did was they

271
00:10:53,010 --> 00:10:56,939
partnered with a major french website

272
00:10:54,570 --> 00:10:59,100
and fingerprinted all of the users of

273
00:10:56,940 --> 00:11:00,750
the off that web site without really

274
00:10:59,100 --> 00:11:02,310
telling them so by doing this lightly

275
00:11:00,750 --> 00:11:04,290
ethically questionable thing they did a

276
00:11:02,310 --> 00:11:06,270
statistically much more rigorous thing

277
00:11:04,290 --> 00:11:08,790
and they published that study which

278
00:11:06,270 --> 00:11:10,590
found in fact contrary to some of the

279
00:11:08,790 --> 00:11:13,040
previous findings only a third of the

280
00:11:10,590 --> 00:11:15,630
users were unique and as more and more

281
00:11:13,040 --> 00:11:17,010
activity shifts to mobile less than a

282
00:11:15,630 --> 00:11:18,270
fifth of mobile users were unique

283
00:11:17,010 --> 00:11:21,240
because those devices are less

284
00:11:18,270 --> 00:11:23,880
customizable and further has flash and

285
00:11:21,240 --> 00:11:25,650
Java and other old plugins are getting

286
00:11:23,880 --> 00:11:27,660
phased out that number is actually going

287
00:11:25,650 --> 00:11:29,850
down and if you look at for me when I

288
00:11:27,660 --> 00:11:31,620
looked at the findings of this study the

289
00:11:29,850 --> 00:11:33,420
conclusion was very different even

290
00:11:31,620 --> 00:11:35,100
little things that browsers can do in

291
00:11:33,420 --> 00:11:36,540
order to minimize the fingerprint

292
00:11:35,100 --> 00:11:39,150
ability of features are going to have a

293
00:11:36,540 --> 00:11:41,520
big impact and so I came away with this

294
00:11:39,150 --> 00:11:44,130
with a very different point of view then

295
00:11:41,520 --> 00:11:45,569
a lot of people had had before before

296
00:11:44,130 --> 00:11:47,939
the study which is let's not even bother

297
00:11:45,570 --> 00:11:51,330
let's not cripple features for the sake

298
00:11:47,940 --> 00:11:53,460
of privacy but instead after the study

299
00:11:51,330 --> 00:11:55,770
what I concluded was quite the opposite

300
00:11:53,460 --> 00:11:57,960
so I think this is a much more general

301
00:11:55,770 --> 00:11:59,310
principle and a lot of contexts we hear

302
00:11:57,960 --> 00:12:01,530
the ship has sailed the horse has left

303
00:11:59,310 --> 00:12:04,680
the bar in kind of argument and if you

304
00:12:01,530 --> 00:12:06,930
were at Pete Snyder's talk at PRG

305
00:12:04,680 --> 00:12:08,229
earlier today you heard a lot of similar

306
00:12:06,930 --> 00:12:09,930
points

307
00:12:08,230 --> 00:12:12,399
from him as well you know and the the

308
00:12:09,930 --> 00:12:18,370
same kind of thing that that I'm saying

309
00:12:12,399 --> 00:12:20,200
here so that's the first kind of insight

310
00:12:18,370 --> 00:12:23,829
that I want to that I want to give you

311
00:12:20,200 --> 00:12:25,149
the ship has not sailed and one of the

312
00:12:23,829 --> 00:12:27,099
reasons that people will say that the

313
00:12:25,149 --> 00:12:28,660
ship has sailed is that if you don't

314
00:12:27,100 --> 00:12:30,430
have a perfect defense even if you try

315
00:12:28,660 --> 00:12:31,870
to mitigate fingerprint ability oh

316
00:12:30,430 --> 00:12:33,579
here's a clever way that somebody can

317
00:12:31,870 --> 00:12:37,120
get around that well I have an imperfect

318
00:12:33,579 --> 00:12:40,329
defense at all is it is it not better to

319
00:12:37,120 --> 00:12:43,750
you know to not give people a false

320
00:12:40,329 --> 00:12:45,430
sense of security so that is a point on

321
00:12:43,750 --> 00:12:46,870
which I will disagree I think that

322
00:12:45,430 --> 00:12:49,000
imperfect defenses are is still very

323
00:12:46,870 --> 00:12:51,579
useful and one reason I believe that is

324
00:12:49,000 --> 00:12:53,649
because technology doesn't have to bear

325
00:12:51,579 --> 00:12:55,329
the full burden of privacy protection

326
00:12:53,649 --> 00:12:57,430
what do I mean by this here's an

327
00:12:55,329 --> 00:12:59,290
interesting example Safari has third

328
00:12:57,430 --> 00:13:01,209
plot third party cookie blocking as you

329
00:12:59,290 --> 00:13:03,310
might know and it's not a perfect

330
00:13:01,209 --> 00:13:05,260
defense it can be circumvented in fact

331
00:13:03,310 --> 00:13:07,089
Google decided to do exactly that Google

332
00:13:05,260 --> 00:13:09,069
decided to circumvent it and once they

333
00:13:07,089 --> 00:13:10,690
did something interesting happened in

334
00:13:09,070 --> 00:13:12,490
the US the Federal Trade Commission got

335
00:13:10,690 --> 00:13:14,769
involved they said hey you can't do that

336
00:13:12,490 --> 00:13:16,630
you can't circumvent a privacy measure

337
00:13:14,769 --> 00:13:18,399
that's actually a violation of the law

338
00:13:16,630 --> 00:13:20,260
and they went after Google and they find

339
00:13:18,399 --> 00:13:22,149
Google so that's an interesting

340
00:13:20,260 --> 00:13:24,220
phenomenon where the technology itself

341
00:13:22,149 --> 00:13:26,199
was not bulletproof but it turns out

342
00:13:24,220 --> 00:13:28,000
that circumventing even a weak privacy

343
00:13:26,199 --> 00:13:29,740
protection measure can actually get

344
00:13:28,000 --> 00:13:31,990
companies into trouble with the law it

345
00:13:29,740 --> 00:13:33,640
can also be a reputational harm so when

346
00:13:31,990 --> 00:13:35,500
we're talking about the privacy

347
00:13:33,640 --> 00:13:36,760
adversaries here we're talking about the

348
00:13:35,500 --> 00:13:38,860
Facebook's and Google's of the world

349
00:13:36,760 --> 00:13:40,230
we're not talking about somebody you

350
00:13:38,860 --> 00:13:42,160
know from a poorly regulated

351
00:13:40,230 --> 00:13:44,110
jurisdiction somewhere out there in the

352
00:13:42,160 --> 00:13:46,000
world and therefore technology doesn't

353
00:13:44,110 --> 00:13:48,670
have to bear the full burden and perfect

354
00:13:46,000 --> 00:13:50,589
defenses can still be useful even if all

355
00:13:48,670 --> 00:13:52,089
that it does is raise the cost of some

356
00:13:50,589 --> 00:13:53,620
of these fingerprinting and privacy

357
00:13:52,089 --> 00:13:56,800
invasive features and it takes a couple

358
00:13:53,620 --> 00:13:58,390
more years for those kind of tracking

359
00:13:56,800 --> 00:14:00,699
technologies to become very widespread

360
00:13:58,390 --> 00:14:02,350
that is still useful because it gives a

361
00:14:00,699 --> 00:14:04,120
couple of years for new defenses to be

362
00:14:02,350 --> 00:14:06,639
developed whether they may be technical

363
00:14:04,120 --> 00:14:08,290
or legal or something like that so that

364
00:14:06,639 --> 00:14:12,279
was point number one point number two

365
00:14:08,290 --> 00:14:14,800
that I want to talk about is we're in a

366
00:14:12,279 --> 00:14:16,630
world where what privacy means to people

367
00:14:14,800 --> 00:14:18,370
changes very quickly whether or not

368
00:14:16,630 --> 00:14:20,560
something as a privacy breach changes

369
00:14:18,370 --> 00:14:21,290
very quickly so both privacy attitudes

370
00:14:20,560 --> 00:14:23,209
and private

371
00:14:21,290 --> 00:14:25,130
infringing technology's changed pretty

372
00:14:23,209 --> 00:14:26,479
quickly how can standards cope in this

373
00:14:25,130 --> 00:14:30,500
world given that standards are intended

374
00:14:26,480 --> 00:14:32,060
to be pretty long-lasting documents and

375
00:14:30,500 --> 00:14:36,290
so how do you resolve the tension

376
00:14:32,060 --> 00:14:38,089
between these two so one good example of

377
00:14:36,290 --> 00:14:40,279
this is one of my favorite examples of

378
00:14:38,089 --> 00:14:42,769
how privacy attitudes evolved quickly is

379
00:14:40,279 --> 00:14:44,930
that if you thought about privacy ten

380
00:14:42,769 --> 00:14:46,940
years ago most users would have been

381
00:14:44,930 --> 00:14:49,219
concerned with what are the individual

382
00:14:46,940 --> 00:14:51,230
harms that can accrue to me out of all

383
00:14:49,220 --> 00:14:54,079
of those data collection out of all of

384
00:14:51,230 --> 00:14:56,060
the databases owned by companies that

385
00:14:54,079 --> 00:14:58,069
have my personal information is that

386
00:14:56,060 --> 00:15:00,138
identity theft is the data breaches is

387
00:14:58,069 --> 00:15:01,699
it perhaps targeted price discrimination

388
00:15:00,139 --> 00:15:03,560
what should I be worried about those

389
00:15:01,699 --> 00:15:05,149
were the kinds of privacy questions that

390
00:15:03,560 --> 00:15:07,279
people were asking people are still

391
00:15:05,149 --> 00:15:09,319
worried about those privacy issues but

392
00:15:07,279 --> 00:15:10,970
now people are increasingly worried

393
00:15:09,319 --> 00:15:12,889
about a very different kind of privacy

394
00:15:10,970 --> 00:15:15,649
issue which is what are the threats to

395
00:15:12,889 --> 00:15:17,509
society overall and perhaps to democracy

396
00:15:15,649 --> 00:15:19,699
from all of these massive collections of

397
00:15:17,509 --> 00:15:24,050
personal information especially after a

398
00:15:19,699 --> 00:15:25,760
recent recent stories like Cambridge

399
00:15:24,050 --> 00:15:27,560
analytic apeople are very concerned

400
00:15:25,760 --> 00:15:30,410
about what is the potential of hyper

401
00:15:27,560 --> 00:15:32,359
personalized targeting to affect you

402
00:15:30,410 --> 00:15:34,370
know the overall society that we live in

403
00:15:32,360 --> 00:15:36,649
so I want to say that there has been a

404
00:15:34,370 --> 00:15:38,360
shift from these very individualized

405
00:15:36,649 --> 00:15:40,100
concerns about privacy to more

406
00:15:38,360 --> 00:15:42,470
collective societal concerns about

407
00:15:40,100 --> 00:15:44,389
privacy among privacy scholars and

408
00:15:42,470 --> 00:15:46,069
privacy advocates that shift has been

409
00:15:44,389 --> 00:15:47,240
pretty stark and even among the general

410
00:15:46,069 --> 00:15:49,099
public I think there has been a

411
00:15:47,240 --> 00:15:50,600
substantial shift and so what this means

412
00:15:49,100 --> 00:15:52,490
is that a certain type of data

413
00:15:50,600 --> 00:15:54,980
collection that might have seemed pretty

414
00:15:52,490 --> 00:15:57,290
innocuous ten years ago begins to look

415
00:15:54,980 --> 00:15:58,699
very different today so that was one

416
00:15:57,290 --> 00:16:01,279
example I have a couple of other

417
00:15:58,699 --> 00:16:02,930
examples that I'll skip but the result

418
00:16:01,279 --> 00:16:04,910
of this is that it's very hard in a

419
00:16:02,930 --> 00:16:06,949
standards document to write down a fixed

420
00:16:04,910 --> 00:16:08,630
privacy definition and then say that

421
00:16:06,949 --> 00:16:10,310
I've analyzed this protocol with respect

422
00:16:08,630 --> 00:16:11,810
to this privacy definition and I'm

423
00:16:10,310 --> 00:16:14,420
confident that this is going to be a

424
00:16:11,810 --> 00:16:16,969
privacy respecting protocol now and for

425
00:16:14,420 --> 00:16:18,620
all time to come and so going back to

426
00:16:16,970 --> 00:16:20,120
that example of individual versus

427
00:16:18,620 --> 00:16:22,130
collective harms let me show you very

428
00:16:20,120 --> 00:16:24,439
quickly the paper by Cambridge

429
00:16:22,130 --> 00:16:26,209
researchers this was in 2013 this was

430
00:16:24,439 --> 00:16:28,219
the paper that realised that you could

431
00:16:26,209 --> 00:16:29,839
take people's Facebook Likes which is a

432
00:16:28,220 --> 00:16:31,880
very innocuous sounding type of

433
00:16:29,839 --> 00:16:33,680
information and use that to predict

434
00:16:31,880 --> 00:16:34,939
their so-called Big Five personality

435
00:16:33,680 --> 00:16:37,180
traits and though

436
00:16:34,940 --> 00:16:39,560
are things like emotional stability

437
00:16:37,180 --> 00:16:40,880
agreeableness extraversion and so on the

438
00:16:39,560 --> 00:16:42,500
stuff that you see in green over there

439
00:16:40,880 --> 00:16:43,790
if you can even read that text sorry

440
00:16:42,500 --> 00:16:45,980
about that the font size is a little

441
00:16:43,790 --> 00:16:47,959
small and this is exactly the research

442
00:16:45,980 --> 00:16:50,600
that was allegedly weaponized by

443
00:16:47,960 --> 00:16:52,180
Cambridge analytic ofor psychographic

444
00:16:50,600 --> 00:16:54,470
targeting so this was not necessarily

445
00:16:52,180 --> 00:16:55,819
anticipated a few years ago there are

446
00:16:54,470 --> 00:16:57,560
many other examples of this of

447
00:16:55,820 --> 00:16:59,900
improvements in machine learning

448
00:16:57,560 --> 00:17:02,630
turning innocuous data into something

449
00:16:59,900 --> 00:17:05,089
that can be used for something much more

450
00:17:02,630 --> 00:17:07,849
problematic this was a headline from a

451
00:17:05,089 --> 00:17:09,500
few years ago statistician said target

452
00:17:07,849 --> 00:17:11,270
had figured out how to use a person's

453
00:17:09,500 --> 00:17:14,420
shopping records to figure out whether

454
00:17:11,270 --> 00:17:16,959
they were pregnant or not and so a one

455
00:17:14,420 --> 00:17:19,699
concrete threat along these lines is

456
00:17:16,959 --> 00:17:21,709
well stated by Paul ohm who's a legal

457
00:17:19,699 --> 00:17:23,540
scholar who calls this the database of

458
00:17:21,709 --> 00:17:25,250
Rouen he asks us to imagine the

459
00:17:23,540 --> 00:17:27,290
consequences of a single massive

460
00:17:25,250 --> 00:17:29,690
database containing secrets about every

461
00:17:27,290 --> 00:17:32,090
individual formed by linking different

462
00:17:29,690 --> 00:17:33,710
companies data stores and I think one of

463
00:17:32,090 --> 00:17:35,449
the technologies that is enabling

464
00:17:33,710 --> 00:17:37,400
something like this today is cross

465
00:17:35,450 --> 00:17:39,140
device tracking techniques that enable

466
00:17:37,400 --> 00:17:41,210
the linking of our activities between

467
00:17:39,140 --> 00:17:43,190
different devices even if we're not

468
00:17:41,210 --> 00:17:45,230
identifying ourselves using explicit

469
00:17:43,190 --> 00:17:47,420
identify areas that allow such linkage

470
00:17:45,230 --> 00:17:49,310
just using statistical patterns to link

471
00:17:47,420 --> 00:17:50,900
these different devices together and I

472
00:17:49,310 --> 00:17:53,090
think these types of concerns should

473
00:17:50,900 --> 00:17:55,610
perhaps be at the forefront of some of

474
00:17:53,090 --> 00:17:57,740
our privacy efforts including in

475
00:17:55,610 --> 00:17:59,330
standards efforts but these are not

476
00:17:57,740 --> 00:18:01,970
things that we really recognized as

477
00:17:59,330 --> 00:18:04,310
privacy concerns maybe ten years ago as

478
00:18:01,970 --> 00:18:06,440
much as we do today so that's kind of

479
00:18:04,310 --> 00:18:08,389
what I mean by the landscape of privacy

480
00:18:06,440 --> 00:18:10,190
is shifting pretty quickly and this is a

481
00:18:08,390 --> 00:18:11,960
challenge for a standards document of

482
00:18:10,190 --> 00:18:14,240
standards process which needs to be

483
00:18:11,960 --> 00:18:16,310
really long lived so we thought about

484
00:18:14,240 --> 00:18:19,120
this in a paper recently where we looked

485
00:18:16,310 --> 00:18:22,190
at specifically the battery status API

486
00:18:19,120 --> 00:18:24,770
in the web context and this was an API

487
00:18:22,190 --> 00:18:26,780
that turned out to have much more

488
00:18:24,770 --> 00:18:28,389
serious fingerprint ability privacy

489
00:18:26,780 --> 00:18:30,649
consequences then was realized and

490
00:18:28,390 --> 00:18:32,480
therefore was taken out of a number of

491
00:18:30,650 --> 00:18:34,220
browsers after it had shipped and after

492
00:18:32,480 --> 00:18:36,080
people had started using it that was

493
00:18:34,220 --> 00:18:38,180
kind of fun precedented so we looked at

494
00:18:36,080 --> 00:18:39,980
how did this go wrong and how can we be

495
00:18:38,180 --> 00:18:42,800
more aware of these potential and

496
00:18:39,980 --> 00:18:45,850
misuses during the standards process and

497
00:18:42,800 --> 00:18:48,560
so here's a paper citation at the bottom

498
00:18:45,850 --> 00:18:48,919
and what we proposed in this paper at a

499
00:18:48,560 --> 00:18:51,590
high

500
00:18:48,920 --> 00:18:53,930
level what we called for is a much

501
00:18:51,590 --> 00:18:56,060
tighter loop between standards agencies

502
00:18:53,930 --> 00:18:57,950
as well as researchers and developers

503
00:18:56,060 --> 00:18:59,690
and by developers I mean both

504
00:18:57,950 --> 00:19:01,280
implementers and also developers in a

505
00:18:59,690 --> 00:19:03,860
much more general sense people who are

506
00:19:01,280 --> 00:19:06,950
using the api's that are you know

507
00:19:03,860 --> 00:19:09,500
implemented by by the browser vendors

508
00:19:06,950 --> 00:19:12,380
for example and as part of this we think

509
00:19:09,500 --> 00:19:14,330
that it would be really useful to

510
00:19:12,380 --> 00:19:16,370
incentivize academics to do two things

511
00:19:14,330 --> 00:19:18,020
one is to get involved in the standards

512
00:19:16,370 --> 00:19:19,870
process and do privacy reviews of

513
00:19:18,020 --> 00:19:23,240
standards and the other one this is

514
00:19:19,870 --> 00:19:25,699
perhaps still quite missing which is

515
00:19:23,240 --> 00:19:27,470
once an API is out in the wild and once

516
00:19:25,700 --> 00:19:29,660
people are using it to do regular

517
00:19:27,470 --> 00:19:31,970
privacy audits of how it's being used

518
00:19:29,660 --> 00:19:34,100
and abused I've talked about this a few

519
00:19:31,970 --> 00:19:36,890
times and one question that I get sure

520
00:19:34,100 --> 00:19:38,689
this sounds good in theory but it's hard

521
00:19:36,890 --> 00:19:41,480
to convince researchers to do this how

522
00:19:38,690 --> 00:19:43,250
do we do that now one good thing I'll

523
00:19:41,480 --> 00:19:44,660
say about this this actually sounds like

524
00:19:43,250 --> 00:19:46,850
a horrible thing but I'll claim it's a

525
00:19:44,660 --> 00:19:49,690
good thing is that it's fairly easy to

526
00:19:46,850 --> 00:19:51,830
influence academic researchers influence

527
00:19:49,690 --> 00:19:53,570
influenced them not in the sense of what

528
00:19:51,830 --> 00:19:55,580
they'll say but influence them in the

529
00:19:53,570 --> 00:19:57,260
sense of what they want to work on by

530
00:19:55,580 --> 00:19:59,179
funding certain work or by making it

531
00:19:57,260 --> 00:20:01,400
more prestigious by creating awards for

532
00:19:59,180 --> 00:20:03,440
example for certain types of work such

533
00:20:01,400 --> 00:20:07,130
as privacy reviews of standards I think

534
00:20:03,440 --> 00:20:09,890
it's there's a there's a fairly

535
00:20:07,130 --> 00:20:11,900
straightforward path to incentivizing

536
00:20:09,890 --> 00:20:13,370
much more academic work as part of the

537
00:20:11,900 --> 00:20:15,770
standards process which I think will be

538
00:20:13,370 --> 00:20:17,510
a good thing another thing that I think

539
00:20:15,770 --> 00:20:19,639
would be useful is as part of the

540
00:20:17,510 --> 00:20:21,890
standards process to be explicit about

541
00:20:19,640 --> 00:20:24,440
assumptions because privacy changes so

542
00:20:21,890 --> 00:20:26,570
quickly because we can't anticipate what

543
00:20:24,440 --> 00:20:28,610
new privacy infringing technologies will

544
00:20:26,570 --> 00:20:30,860
be out there in five years it helps to

545
00:20:28,610 --> 00:20:34,129
be explicit about assumptions as part of

546
00:20:30,860 --> 00:20:36,830
the standards process and that is to be

547
00:20:34,130 --> 00:20:40,160
able to explicitly say we have created

548
00:20:36,830 --> 00:20:42,169
the standard assuming that this API will

549
00:20:40,160 --> 00:20:43,940
not be highly susceptible to fingerprint

550
00:20:42,170 --> 00:20:46,160
ability but if it turns out that that's

551
00:20:43,940 --> 00:20:48,350
the case if it turns out that this is

552
00:20:46,160 --> 00:20:49,820
being exploited in the wild here are

553
00:20:48,350 --> 00:20:51,770
some things that implementers could do

554
00:20:49,820 --> 00:20:54,530
to mitigate that risk so that's the

555
00:20:51,770 --> 00:20:56,000
second point okay and the third and

556
00:20:54,530 --> 00:20:58,520
final point that I want to talk about is

557
00:20:56,000 --> 00:21:00,320
that this idea of measurement which is

558
00:20:58,520 --> 00:21:01,700
finding these privacy violations on a

559
00:21:00,320 --> 00:21:03,439
large scale I'm clay

560
00:21:01,700 --> 00:21:06,470
that it's been really useful for privacy

561
00:21:03,440 --> 00:21:07,909
but unfortunately it's going away and I

562
00:21:06,470 --> 00:21:10,399
want to talk about whether there is a

563
00:21:07,909 --> 00:21:12,200
way to preserve it I don't want to make

564
00:21:10,399 --> 00:21:14,209
the sound like a sky is falling kind of

565
00:21:12,200 --> 00:21:15,860
claim but in my little corner of the

566
00:21:14,210 --> 00:21:17,299
research world the sky has already

567
00:21:15,860 --> 00:21:19,490
fallen and a lot of fesses that have

568
00:21:17,299 --> 00:21:20,809
moved on to other research areas so let

569
00:21:19,490 --> 00:21:22,039
me tell you why that is and why that

570
00:21:20,809 --> 00:21:23,928
should worry us from a privacy

571
00:21:22,039 --> 00:21:26,779
perspective and to see whether there's a

572
00:21:23,929 --> 00:21:28,669
way to preserve it so I'm claiming that

573
00:21:26,779 --> 00:21:30,559
at least in the web context measurement

574
00:21:28,669 --> 00:21:32,779
has played a very key role in keeping

575
00:21:30,559 --> 00:21:35,090
the worst of the privacy abuses in check

576
00:21:32,779 --> 00:21:37,010
many teams around the world have been

577
00:21:35,090 --> 00:21:39,080
working on web privacy measurement I'll

578
00:21:37,010 --> 00:21:41,120
tell you a tiny bit about my own team's

579
00:21:39,080 --> 00:21:43,730
work something that we built is a tool

580
00:21:41,120 --> 00:21:45,139
called open wpm this is a the github

581
00:21:43,730 --> 00:21:46,909
page if you want to check it out as you

582
00:21:45,139 --> 00:21:48,590
can see it's an actively developed

583
00:21:46,909 --> 00:21:50,720
open-source project it was developed at

584
00:21:48,590 --> 00:21:52,129
Princeton and now the main developer

585
00:21:50,720 --> 00:21:54,710
Steve Englehart has moved to Missoula

586
00:21:52,130 --> 00:21:56,149
it's maintained by Missoula now so what

587
00:21:54,710 --> 00:21:57,740
it is I don't mean for any of the

588
00:21:56,149 --> 00:22:00,229
details on this page to be important is

589
00:21:57,740 --> 00:22:03,350
it's just the URL if you want to look at

590
00:22:00,230 --> 00:22:05,090
it or the name open wpm now what it is

591
00:22:03,350 --> 00:22:07,399
is an instrumented version of Firefox

592
00:22:05,090 --> 00:22:09,289
it's basically a bot that visits the

593
00:22:07,399 --> 00:22:12,379
web's top 1 million web sites every

594
00:22:09,289 --> 00:22:15,080
month and looks at what kind of privacy

595
00:22:12,380 --> 00:22:17,299
violation violating techniques are out

596
00:22:15,080 --> 00:22:19,490
there it even does things like put in

597
00:22:17,299 --> 00:22:20,330
fake PII into various forums and tries

598
00:22:19,490 --> 00:22:22,309
to see where they go

599
00:22:20,330 --> 00:22:24,110
and it saves all that data we have half

600
00:22:22,309 --> 00:22:26,360
a terabyte of data per month and then we

601
00:22:24,110 --> 00:22:28,219
run various scripts on that data to try

602
00:22:26,360 --> 00:22:29,840
to find privacy violations and publicize

603
00:22:28,220 --> 00:22:31,610
them and get people to change their

604
00:22:29,840 --> 00:22:33,620
practices we've written a number of

605
00:22:31,610 --> 00:22:35,330
papers based on this data this is one

606
00:22:33,620 --> 00:22:37,158
example it's called online tracking of 1

607
00:22:35,330 --> 00:22:38,330
million site measurement analysis and as

608
00:22:37,159 --> 00:22:40,250
you can see one of the key things here

609
00:22:38,330 --> 00:22:43,129
is to be able to do this on a large

610
00:22:40,250 --> 00:22:45,260
scale in a mostly automated way it's had

611
00:22:43,130 --> 00:22:47,600
a number of positive impacts on privacy

612
00:22:45,260 --> 00:22:49,519
one of them is enhancing block lists for

613
00:22:47,600 --> 00:22:51,620
example if you use adblock plus or you

614
00:22:49,519 --> 00:22:53,029
block origin those tools use filter

615
00:22:51,620 --> 00:22:55,309
lists and the developers of this filter

616
00:22:53,029 --> 00:22:57,110
lists often look to research like hours

617
00:22:55,309 --> 00:22:59,240
to try to figure out what are some of

618
00:22:57,110 --> 00:23:01,459
the new privacy violating endpoints in

619
00:22:59,240 --> 00:23:04,100
the URLs in order to add them to their

620
00:23:01,460 --> 00:23:05,779
block lists various other things for

621
00:23:04,100 --> 00:23:07,519
example in some cases there's not

622
00:23:05,779 --> 00:23:09,169
there's been an enforcement action by

623
00:23:07,519 --> 00:23:11,269
data protection authorities by the

624
00:23:09,169 --> 00:23:12,429
federal agency and things like that so

625
00:23:11,269 --> 00:23:14,379
I'm claiming

626
00:23:12,429 --> 00:23:15,789
the the this kind of research that's

627
00:23:14,379 --> 00:23:18,309
been done by many groups around the

628
00:23:15,789 --> 00:23:21,369
world is one of the main reasons why web

629
00:23:18,309 --> 00:23:23,710
privacy has been you know kind of at an

630
00:23:21,369 --> 00:23:24,580
equilibrium hasn't been even worse than

631
00:23:23,710 --> 00:23:28,389
it already is

632
00:23:24,580 --> 00:23:31,240
now the important point though is that

633
00:23:28,389 --> 00:23:33,129
five years ago a lot of fuss that we

634
00:23:31,240 --> 00:23:34,990
were going to do this very same kind of

635
00:23:33,129 --> 00:23:37,090
work for IOT because we were hearing

636
00:23:34,990 --> 00:23:39,490
that a lot of the IOT devices in our

637
00:23:37,090 --> 00:23:40,928
homes have occasionally have

638
00:23:39,490 --> 00:23:43,299
surreptitious data collection that

639
00:23:40,929 --> 00:23:44,950
consumers did not know about and so we

640
00:23:43,299 --> 00:23:46,539
wanted to do this kind of work but we

641
00:23:44,950 --> 00:23:48,399
very quickly realized that we can't

642
00:23:46,539 --> 00:23:50,499
actually do this work because of one

643
00:23:48,399 --> 00:23:52,719
very simple reason which is that most

644
00:23:50,499 --> 00:23:54,820
devices are end-to-end encrypted which

645
00:23:52,720 --> 00:23:56,649
to be very clear is a great thing it's

646
00:23:54,820 --> 00:23:57,149
great for privacy they should be into an

647
00:23:56,649 --> 00:23:59,258
encrypted

648
00:23:57,149 --> 00:24:01,658
unfortunately the downside of that is

649
00:23:59,259 --> 00:24:03,220
that the two ends of course F into an

650
00:24:01,659 --> 00:24:05,320
encryption are the device in the server

651
00:24:03,220 --> 00:24:07,720
it doesn't involve the user it doesn't

652
00:24:05,320 --> 00:24:09,700
involve a researcher a researcher can't

653
00:24:07,720 --> 00:24:11,049
Smitham these devices a researcher can't

654
00:24:09,700 --> 00:24:14,139
figure out what data is being collected

655
00:24:11,049 --> 00:24:16,360
and where it's being sent and we think

656
00:24:14,139 --> 00:24:17,769
this is you know kind of a crisis for

657
00:24:16,360 --> 00:24:19,899
this kind of research it makes

658
00:24:17,769 --> 00:24:23,320
meaningful privacy measurement basically

659
00:24:19,899 --> 00:24:24,820
infeasible the public is very interested

660
00:24:23,320 --> 00:24:26,830
in these questions for example there was

661
00:24:24,820 --> 00:24:28,600
this article called the house that spied

662
00:24:26,830 --> 00:24:30,570
on me that just looked at what are the

663
00:24:28,600 --> 00:24:33,129
endpoints of communication of various

664
00:24:30,570 --> 00:24:35,200
IOT devices including sex toys why is

665
00:24:33,129 --> 00:24:37,029
that contacting 13 different servers you

666
00:24:35,200 --> 00:24:39,309
know people want to know what data is

667
00:24:37,029 --> 00:24:41,049
going out there and this is important

668
00:24:39,309 --> 00:24:43,658
not just from a privacy advocate point

669
00:24:41,049 --> 00:24:45,129
of view if you're a company and you're a

670
00:24:43,659 --> 00:24:46,960
reputable company and you want to be

671
00:24:45,129 --> 00:24:49,389
able to show your users that your data

672
00:24:46,960 --> 00:24:51,580
collection is completely you know

673
00:24:49,389 --> 00:24:53,498
according to your specified privacy

674
00:24:51,580 --> 00:24:55,809
policies there's no good way to do that

675
00:24:53,499 --> 00:24:57,909
today because researchers can't examine

676
00:24:55,809 --> 00:25:00,309
the plain text of these communications

677
00:24:57,909 --> 00:25:02,200
and I think this is a serious issue for

678
00:25:00,309 --> 00:25:03,850
example if we wanted to know if the

679
00:25:02,200 --> 00:25:05,830
smart light bulbs in our homes are

680
00:25:03,850 --> 00:25:07,869
transmitting conversations because they

681
00:25:05,830 --> 00:25:09,189
actually have microphones we really

682
00:25:07,869 --> 00:25:11,619
don't have a good way to check that

683
00:25:09,190 --> 00:25:13,600
today and this is not a paranoid

684
00:25:11,619 --> 00:25:15,850
scenario something somewhat similar has

685
00:25:13,600 --> 00:25:17,949
happened for example this interesting

686
00:25:15,850 --> 00:25:20,529
thing happened a few months ago where

687
00:25:17,950 --> 00:25:22,539
Google sent an email to all of the

688
00:25:20,529 --> 00:25:25,130
owners of nest thermostats and said hey

689
00:25:22,539 --> 00:25:28,100
your nest thermostat is also a Google

690
00:25:25,130 --> 00:25:29,270
voices just now and people like what how

691
00:25:28,100 --> 00:25:31,189
is that possible it doesn't have a

692
00:25:29,270 --> 00:25:33,200
microphone and Google said no it does

693
00:25:31,190 --> 00:25:34,670
have a microphone and people said what

694
00:25:33,200 --> 00:25:36,860
we didn't know it had a microphone and

695
00:25:34,670 --> 00:25:39,560
Google said yes it does check the

696
00:25:36,860 --> 00:25:41,270
privacy policy and people said what

697
00:25:39,560 --> 00:25:42,889
privacy policy nobody reads the privacy

698
00:25:41,270 --> 00:25:44,810
policy also when they read the privacy

699
00:25:42,890 --> 00:25:46,670
policy it was actually not in there and

700
00:25:44,810 --> 00:25:48,379
then Google said oh we meant to disclose

701
00:25:46,670 --> 00:25:52,100
that in the privacy policy sorry that

702
00:25:48,380 --> 00:25:53,510
was an oversight so of course in this

703
00:25:52,100 --> 00:25:55,459
case I'm willing to believe that it was

704
00:25:53,510 --> 00:25:57,710
an oversight on the part of Google but

705
00:25:55,460 --> 00:25:59,960
if there was a malicious you know vendor

706
00:25:57,710 --> 00:26:02,120
who put microphones and devices that are

707
00:25:59,960 --> 00:26:03,830
in millions of people homes we literally

708
00:26:02,120 --> 00:26:05,540
don't have a good way to know about it

709
00:26:03,830 --> 00:26:07,490
this measurement research has been in

710
00:26:05,540 --> 00:26:09,800
the past one way to know about it but it

711
00:26:07,490 --> 00:26:11,900
doesn't work for IMT so with that I'll

712
00:26:09,800 --> 00:26:14,360
just end by saying that what it likes a

713
00:26:11,900 --> 00:26:16,310
call for is some kind of debug mode for

714
00:26:14,360 --> 00:26:18,310
IOT devices I think this is a critical

715
00:26:16,310 --> 00:26:21,770
need the idea being that when you enable

716
00:26:18,310 --> 00:26:24,440
this kind of debug mode the user or more

717
00:26:21,770 --> 00:26:25,910
likely a researcher you know the details

718
00:26:24,440 --> 00:26:27,710
and user experience will depend on the

719
00:26:25,910 --> 00:26:29,930
device but some way to be able to

720
00:26:27,710 --> 00:26:31,880
intercept the plaintext in order to be

721
00:26:29,930 --> 00:26:34,190
able to audit what's going on out there

722
00:26:31,880 --> 00:26:38,390
there's a Stanford project related to

723
00:26:34,190 --> 00:26:42,380
this called TLS TLS what is it called

724
00:26:38,390 --> 00:26:44,300
TLS replays something and so what I'm

725
00:26:42,380 --> 00:26:46,340
proposing is slightly different I'm

726
00:26:44,300 --> 00:26:48,409
happy to hash out the details later this

727
00:26:46,340 --> 00:26:50,780
is not necessarily the time for that but

728
00:26:48,410 --> 00:26:52,670
I think some way of being able to

729
00:26:50,780 --> 00:26:54,800
examine the communications of IOT

730
00:26:52,670 --> 00:26:57,230
devices is critical and I think there's

731
00:26:54,800 --> 00:26:58,639
a role for Standardization here with

732
00:26:57,230 --> 00:27:00,920
that I'll just put the summary back up

733
00:26:58,640 --> 00:27:01,420
and thank you for your time

734
00:27:00,920 --> 00:27:09,360
[Music]

735
00:27:01,420 --> 00:27:11,679
[Applause]

736
00:27:09,360 --> 00:27:18,399
Thanks

737
00:27:11,679 --> 00:27:21,610
okay okay that's so to talk about some

738
00:27:18,400 --> 00:27:25,000
modern issues in privacy today and you

739
00:27:21,610 --> 00:27:26,770
know privacy is not a new issue when I

740
00:27:25,000 --> 00:27:28,330
started doing the research that led to

741
00:27:26,770 --> 00:27:30,900
this talk and by the way these slides

742
00:27:28,330 --> 00:27:33,059
were already on my web pages and

743
00:27:30,900 --> 00:27:36,610
references linked to references a

744
00:27:33,059 --> 00:27:40,360
technical class legal document is also

745
00:27:36,610 --> 00:27:44,199
on my web page a lot of this stuff goes

746
00:27:40,360 --> 00:27:46,330
back to the 1960s you know the New York

747
00:27:44,200 --> 00:27:50,260
City Bar Association started studying

748
00:27:46,330 --> 00:27:51,908
computers in privacy in 1962 Allen

749
00:27:50,260 --> 00:27:53,649
Weston prepared basically a report of

750
00:27:51,909 --> 00:27:56,380
that committee in 67 been very

751
00:27:53,649 --> 00:27:58,510
influential the US Congress held

752
00:27:56,380 --> 00:28:01,450
hearings on this legal academics for

753
00:27:58,510 --> 00:28:04,510
writing papers on this all in the 1960s

754
00:28:01,450 --> 00:28:05,950
and it actually goes back the right to

755
00:28:04,510 --> 00:28:07,809
privacy is mentioned in Jewish

756
00:28:05,950 --> 00:28:11,169
literature about eighteen hundred years

757
00:28:07,809 --> 00:28:13,990
ago so it's not a new issue and the

758
00:28:11,169 --> 00:28:16,990
privacy that we work used today the

759
00:28:13,990 --> 00:28:21,700
privacy paradigm called notice and

760
00:28:16,990 --> 00:28:23,289
consent goes back to Westen's 1967 book

761
00:28:21,700 --> 00:28:25,149
which is the report of this Bar

762
00:28:23,289 --> 00:28:28,779
Association the city of New York

763
00:28:25,149 --> 00:28:31,959
Committee that users individuals can

764
00:28:28,779 --> 00:28:34,539
determine for themselves what they want

765
00:28:31,960 --> 00:28:39,640
to share and what they're willing to

766
00:28:34,539 --> 00:28:42,460
reveal and this statement from 1967 has

767
00:28:39,640 --> 00:28:47,350
been the basis for virtually all privacy

768
00:28:42,460 --> 00:28:49,299
regulation since then and yet you look

769
00:28:47,350 --> 00:28:52,830
at the timeline he published this book

770
00:28:49,299 --> 00:28:56,168
in 67 six years later a US government

771
00:28:52,830 --> 00:28:57,760
committee came up with what became known

772
00:28:56,169 --> 00:29:00,610
as the Fair Information practice

773
00:28:57,760 --> 00:29:04,330
principles of consent of security of

774
00:29:00,610 --> 00:29:08,139
openness of use specification and so on

775
00:29:04,330 --> 00:29:10,539
in 1974 a year later the US government

776
00:29:08,140 --> 00:29:13,120
actually enacted this into law but only

777
00:29:10,539 --> 00:29:15,370
is applied to the US government didn't

778
00:29:13,120 --> 00:29:17,169
apply to private corporations not the

779
00:29:15,370 --> 00:29:21,669
American Way

780
00:29:17,170 --> 00:29:23,320
a few years later the OECD suggested

781
00:29:21,670 --> 00:29:27,820
more or less the same thing but applying

782
00:29:23,320 --> 00:29:31,629
to the private sector in 94 the EU and

783
00:29:27,820 --> 00:29:34,030
active data privacy directive seven

784
00:29:31,630 --> 00:29:37,230
years ago the gdpr was enacted when it

785
00:29:34,030 --> 00:29:40,080
to affect a couple of years ago but from

786
00:29:37,230 --> 00:29:41,650
10,000 meters all of these are

787
00:29:40,080 --> 00:29:44,040
substantially the same yeah

788
00:29:41,650 --> 00:29:47,800
tremendous differences in details but

789
00:29:44,040 --> 00:29:51,250
fundamentally if you consent the data

790
00:29:47,800 --> 00:29:54,760
that you have the data about you can and

791
00:29:51,250 --> 00:29:58,960
will be collected and notice and consent

792
00:29:54,760 --> 00:30:01,600
and so notice and consent is sites tell

793
00:29:58,960 --> 00:30:04,300
you what they're going to collect and

794
00:30:01,600 --> 00:30:06,719
what they're going to do with it and by

795
00:30:04,300 --> 00:30:10,540
using the website by using the device

796
00:30:06,720 --> 00:30:16,300
you are deemed to have consented to this

797
00:30:10,540 --> 00:30:21,340
policy and some of the risks were known

798
00:30:16,300 --> 00:30:23,409
back in the 1960s academics law

799
00:30:21,340 --> 00:30:24,909
professors wrote people are just going

800
00:30:23,410 --> 00:30:27,910
to go along with the requests because

801
00:30:24,910 --> 00:30:30,100
they want the service 1960's we didn't

802
00:30:27,910 --> 00:30:32,710
have Google we didn't have Facebook they

803
00:30:30,100 --> 00:30:36,219
realized people are going to go along to

804
00:30:32,710 --> 00:30:37,900
get the benefits they realized they told

805
00:30:36,220 --> 00:30:39,670
the US Congress people are gonna share

806
00:30:37,900 --> 00:30:44,170
passwords maybe we need multi-factor

807
00:30:39,670 --> 00:30:46,590
authentication 1967 folks how many such

808
00:30:44,170 --> 00:30:49,090
you log in to adjust a password today

809
00:30:46,590 --> 00:30:51,909
they worried about hackers they even

810
00:30:49,090 --> 00:30:55,419
cited MIT the MIT students breaking into

811
00:30:51,910 --> 00:30:59,110
systems for fun insider threats why are

812
00:30:55,420 --> 00:31:01,150
tapping the need for encryption the

813
00:30:59,110 --> 00:31:04,060
importance of metadata and the

814
00:31:01,150 --> 00:31:08,940
inferences you can draw from metadata in

815
00:31:04,060 --> 00:31:12,610
again 1967-1969 the danger of large

816
00:31:08,940 --> 00:31:17,790
searchable aggregate able databases all

817
00:31:12,610 --> 00:31:20,590
of this was known and largely forgotten

818
00:31:17,790 --> 00:31:22,600
so we haven't solved the technical

819
00:31:20,590 --> 00:31:24,820
problems of with for more than 50 years

820
00:31:22,600 --> 00:31:26,810
ago we still have noticed in consent

821
00:31:24,820 --> 00:31:31,129
though does it work

822
00:31:26,810 --> 00:31:32,929
not even close to working there's a

823
00:31:31,130 --> 00:31:36,020
tremendous amount of data is collected

824
00:31:32,930 --> 00:31:37,820
and we don't know who was collecting it

825
00:31:36,020 --> 00:31:40,460
we have privacy policies we have

826
00:31:37,820 --> 00:31:44,450
location data and of course there are

827
00:31:40,460 --> 00:31:46,220
the governments of the world there's a

828
00:31:44,450 --> 00:31:48,530
tremendous amount of over collection

829
00:31:46,220 --> 00:31:49,400
apart from all the folks whom you give

830
00:31:48,530 --> 00:31:51,710
consent

831
00:31:49,400 --> 00:31:55,550
there are the data brokers outside

832
00:31:51,710 --> 00:31:59,870
parties who business is to collect data

833
00:31:55,550 --> 00:32:02,600
about people and sell it they collect it

834
00:31:59,870 --> 00:32:04,909
they buy it and they sell it sometimes

835
00:32:02,600 --> 00:32:07,129
from public records sometimes from

836
00:32:04,910 --> 00:32:10,520
private transactions that you know

837
00:32:07,130 --> 00:32:12,500
nothing about last year I sold the car I

838
00:32:10,520 --> 00:32:18,770
found out that my odometer readings had

839
00:32:12,500 --> 00:32:20,900
been sold by my mechanic well yeah did I

840
00:32:18,770 --> 00:32:22,550
could send to it no that was a private

841
00:32:20,900 --> 00:32:26,720
deal between the mechanic and some data

842
00:32:22,550 --> 00:32:28,310
collection company the ads that you see

843
00:32:26,720 --> 00:32:29,900
on the web they're not generally not

844
00:32:28,310 --> 00:32:32,090
coming from the website you're visiting

845
00:32:29,900 --> 00:32:34,720
they're coming from ad brokers often

846
00:32:32,090 --> 00:32:38,209
multiple levels of ad brokers who do

847
00:32:34,720 --> 00:32:41,540
HTTP redirects each one is a separate

848
00:32:38,210 --> 00:32:43,820
website and collect and set cookies so

849
00:32:41,540 --> 00:32:45,920
lots of folks are gathering data about

850
00:32:43,820 --> 00:32:49,010
you and you don't even know who they are

851
00:32:45,920 --> 00:32:51,890
and the third-party like buttons like

852
00:32:49,010 --> 00:32:54,800
Facebook and Twitter and the third-party

853
00:32:51,890 --> 00:32:58,190
authentication Facebook and Google tells

854
00:32:54,800 --> 00:33:02,690
these collection sites what sites on the

855
00:32:58,190 --> 00:33:06,230
web you're visiting these analytic

856
00:33:02,690 --> 00:33:09,260
platforms are used to build the profiles

857
00:33:06,230 --> 00:33:11,210
on people and their companies in calling

858
00:33:09,260 --> 00:33:16,340
out rubicon simply because they're cited

859
00:33:11,210 --> 00:33:18,320
in New York Times article they take what

860
00:33:16,340 --> 00:33:20,929
they know about you from the tracking

861
00:33:18,320 --> 00:33:22,790
cookies they combine that with

862
00:33:20,930 --> 00:33:26,240
information from the third-party data

863
00:33:22,790 --> 00:33:28,460
aggregators an estimate based your age

864
00:33:26,240 --> 00:33:30,860
your gender your income and use that to

865
00:33:28,460 --> 00:33:33,020
say how valuable a customer are you and

866
00:33:30,860 --> 00:33:37,330
therefore what ad is appropriate to show

867
00:33:33,020 --> 00:33:37,330
you and you don't see any of this

868
00:33:37,720 --> 00:33:43,210
ah but we have privacy policies no

869
00:33:40,780 --> 00:33:45,790
curiosity who in this room reads every

870
00:33:43,210 --> 00:33:48,720
privacy policy they encounter I'm

871
00:33:45,790 --> 00:33:52,990
impressed I have seriously impressed

872
00:33:48,720 --> 00:33:55,360
security of these hands down there are a

873
00:33:52,990 --> 00:33:57,760
few my hand was not raised there are

874
00:33:55,360 --> 00:33:59,560
lori crater and her colleagues at to

875
00:33:57,760 --> 00:34:01,660
Carnegie Mellon estimated that the

876
00:33:59,560 --> 00:34:04,179
opportunity cost for reading all the

877
00:34:01,660 --> 00:34:06,040
privacy policies you encounter would be

878
00:34:04,180 --> 00:34:09,159
about thirty five hundred US dollars per

879
00:34:06,040 --> 00:34:12,250
year and they're deliberately vague

880
00:34:09,159 --> 00:34:14,590
deliberately expansive because at least

881
00:34:12,250 --> 00:34:16,389
in the US regulators will come down on

882
00:34:14,590 --> 00:34:19,510
you not for what they collect not which

883
00:34:16,389 --> 00:34:21,370
way you collect but from when you break

884
00:34:19,510 --> 00:34:23,290
your promise that's an unfair and

885
00:34:21,370 --> 00:34:25,569
deceptive trade practice according to US

886
00:34:23,290 --> 00:34:28,060
law so if you say you might do

887
00:34:25,570 --> 00:34:31,780
everything then you don't lie when you

888
00:34:28,060 --> 00:34:33,429
do everything you know we may collect

889
00:34:31,780 --> 00:34:35,679
personal information and other

890
00:34:33,429 --> 00:34:36,940
information about you remember the date

891
00:34:35,679 --> 00:34:38,860
of brokers remember the analytic

892
00:34:36,940 --> 00:34:41,590
platforms from business partners

893
00:34:38,860 --> 00:34:46,300
contractors and other third parties in

894
00:34:41,590 --> 00:34:48,730
other words the world quota Advisory

895
00:34:46,300 --> 00:34:50,590
Committee report to President Obama

896
00:34:48,730 --> 00:34:53,139
about five years ago only in some

897
00:34:50,590 --> 00:34:54,550
fantasy world do users actually read

898
00:34:53,139 --> 00:34:56,500
these notices and understand their

899
00:34:54,550 --> 00:34:59,350
implications before clicking to indicate

900
00:34:56,500 --> 00:35:02,470
their consent by and large that's true

901
00:34:59,350 --> 00:35:04,240
and remember because of all these third

902
00:35:02,470 --> 00:35:06,399
and fourth and fifth and sixth parties

903
00:35:04,240 --> 00:35:09,279
on the web you don't even know what

904
00:35:06,400 --> 00:35:12,190
websites you're consenting to you go to

905
00:35:09,280 --> 00:35:15,700
a news site a sports site what-have-you

906
00:35:12,190 --> 00:35:17,500
and you you're careful you read it and

907
00:35:15,700 --> 00:35:20,319
you look at the fine print says by the

908
00:35:17,500 --> 00:35:22,750
way reader advertising partners privacy

909
00:35:20,320 --> 00:35:27,490
policies - who are they good luck

910
00:35:22,750 --> 00:35:30,700
finding out location data it's a huge

911
00:35:27,490 --> 00:35:33,160
issue for mobile devices lots of apps

912
00:35:30,700 --> 00:35:36,879
are collecting and analyzing this kind

913
00:35:33,160 --> 00:35:39,759
of data and even if the app is not doing

914
00:35:36,880 --> 00:35:44,890
the collection and transmission IP

915
00:35:39,760 --> 00:35:47,320
geolocation a very mature technology

916
00:35:44,890 --> 00:35:50,230
reveals a lot is it perfect

917
00:35:47,320 --> 00:35:52,600
no is it very very good yes

918
00:35:50,230 --> 00:35:57,400
and this stuff doesn't have to be

919
00:35:52,600 --> 00:35:59,890
perfect if data exists it's available to

920
00:35:57,400 --> 00:36:01,210
governments sometimes in some

921
00:35:59,890 --> 00:36:03,609
governments you've got a complex

922
00:36:01,210 --> 00:36:05,950
restricted and somewhat painful process

923
00:36:03,610 --> 00:36:08,320
to gain access to your data I said the

924
00:36:05,950 --> 00:36:10,359
US government at this this 45 year old

925
00:36:08,320 --> 00:36:12,160
privacy law you can under certain

926
00:36:10,360 --> 00:36:14,470
circumstances gain access to certain

927
00:36:12,160 --> 00:36:16,930
information about the held about you

928
00:36:14,470 --> 00:36:19,060
other governments don't really care

929
00:36:16,930 --> 00:36:22,990
about the niceties of privacy policies

930
00:36:19,060 --> 00:36:27,240
and access in it is your data we wanted

931
00:36:22,990 --> 00:36:33,520
we haven't go away and of course that's

932
00:36:27,240 --> 00:36:36,520
even ignoring what you know 193 nations

933
00:36:33,520 --> 00:36:39,100
in the in the UN I think about 192 of

934
00:36:36,520 --> 00:36:41,770
them have espionage agencies they

935
00:36:39,100 --> 00:36:44,140
collect data via technical means and

936
00:36:41,770 --> 00:36:49,360
other means and this you don't get to

937
00:36:44,140 --> 00:36:52,480
look at it all the privacy laws that we

938
00:36:49,360 --> 00:36:55,530
have are largely based on what's called

939
00:36:52,480 --> 00:36:58,710
PII personally identifiable information

940
00:36:55,530 --> 00:37:02,410
your name your email address a

941
00:36:58,710 --> 00:37:04,630
government ID number of some sort the

942
00:37:02,410 --> 00:37:07,480
definition varies the EU considers IP

943
00:37:04,630 --> 00:37:10,810
addresses PII the much of the United

944
00:37:07,480 --> 00:37:12,550
States government does not I'm someplace

945
00:37:10,810 --> 00:37:14,799
to be I think they're both rights under

946
00:37:12,550 --> 00:37:18,100
depending on the circumstances but it

947
00:37:14,800 --> 00:37:21,550
turns out you don't need PII to invade

948
00:37:18,100 --> 00:37:23,890
somebody's privacy Amazon doesn't need

949
00:37:21,550 --> 00:37:27,190
your name and address to recommend

950
00:37:23,890 --> 00:37:29,080
products oh they might like it oh you

951
00:37:27,190 --> 00:37:30,580
live in a well-to-do neighborhood we're

952
00:37:29,080 --> 00:37:34,020
going to recommend more expensive

953
00:37:30,580 --> 00:37:36,880
products you have an ethnic surname

954
00:37:34,020 --> 00:37:39,370
family name let me go recommend products

955
00:37:36,880 --> 00:37:40,840
that appeals to that ethnic group so I

956
00:37:39,370 --> 00:37:43,270
can help but they don't really need that

957
00:37:40,840 --> 00:37:46,270
you know people who bought this also

958
00:37:43,270 --> 00:37:48,610
bought that Netflix doesn't need to know

959
00:37:46,270 --> 00:37:50,440
who you are to recommend movies TiVo

960
00:37:48,610 --> 00:37:53,980
doesn't need to know who you are to

961
00:37:50,440 --> 00:37:56,350
recommend TV shows it's a great essay

962
00:37:53,980 --> 00:38:00,299
out there you can find search for it

963
00:37:56,350 --> 00:38:02,890
called my TiVo thinks I'm gay

964
00:38:00,299 --> 00:38:04,900
somebody overreacted when he started

965
00:38:02,890 --> 00:38:07,480
getting recommendations from TiVo

966
00:38:04,900 --> 00:38:09,490
freegate themed movies so he started

967
00:38:07,480 --> 00:38:12,339
this line to overcorrect by watching

968
00:38:09,490 --> 00:38:14,290
manly he-man movie's war movies and so

969
00:38:12,339 --> 00:38:21,040
on at that point you started showing him

970
00:38:14,290 --> 00:38:23,529
Nazi propaganda movies in PII is

971
00:38:21,040 --> 00:38:25,329
actually just a database key but the

972
00:38:23,530 --> 00:38:26,380
database records exist on their own can

973
00:38:25,329 --> 00:38:29,410
be used for lots of things

974
00:38:26,380 --> 00:38:33,670
even without the key to look it up and

975
00:38:29,410 --> 00:38:35,890
to merge it if you're worried about PII

976
00:38:33,670 --> 00:38:38,380
some people try to anonymize the data

977
00:38:35,890 --> 00:38:42,220
what will strip off the identifying

978
00:38:38,380 --> 00:38:45,220
information it doesn't work first of all

979
00:38:42,220 --> 00:38:48,240
for most kinds of anonymization the real

980
00:38:45,220 --> 00:38:50,589
world has shown is easy to re identify

981
00:38:48,240 --> 00:38:54,759
or even you've done some of that as I

982
00:38:50,589 --> 00:38:56,950
recall haven't you and if you do too

983
00:38:54,760 --> 00:38:59,079
good a job of anonymization you may

984
00:38:56,950 --> 00:39:00,939
actually destroy the utility of the data

985
00:38:59,079 --> 00:39:04,270
for certain very important things for

986
00:39:00,940 --> 00:39:06,069
example some medical dosage calculations

987
00:39:04,270 --> 00:39:07,799
done based on machine learning on a

988
00:39:06,069 --> 00:39:10,359
large database of patient information

989
00:39:07,799 --> 00:39:12,819
very successful to calculating the

990
00:39:10,359 --> 00:39:17,529
proper dose of warfarin aver which have

991
00:39:12,819 --> 00:39:19,509
been a very tricky problem but some

992
00:39:17,530 --> 00:39:21,099
academics showed that if you anonymize

993
00:39:19,510 --> 00:39:23,380
the data well enough to really hide the

994
00:39:21,099 --> 00:39:25,960
patient's identity the calculations

995
00:39:23,380 --> 00:39:28,000
wouldn't work you've hidden too much of

996
00:39:25,960 --> 00:39:30,910
the subtle details about the patient's

997
00:39:28,000 --> 00:39:34,809
medical condition so take your choice

998
00:39:30,910 --> 00:39:36,420
identification utility even for things

999
00:39:34,809 --> 00:39:40,619
that we all agree are useful like

1000
00:39:36,420 --> 00:39:40,619
medical research to benefit everybody

1001
00:39:41,940 --> 00:39:47,109
PII

1002
00:39:43,150 --> 00:39:49,690
focusing on PII also misses the

1003
00:39:47,109 --> 00:39:53,650
importance today of machine learning and

1004
00:39:49,690 --> 00:39:56,799
the inferences that it can make you can

1005
00:39:53,650 --> 00:39:59,079
tell someone's sexual orientation from

1006
00:39:56,799 --> 00:40:03,030
the kinds of things they do I will

1007
00:39:59,079 --> 00:40:06,760
ignore them my Tivo thinks I'm gay but

1008
00:40:03,030 --> 00:40:08,890
you can infer this is this good as a fan

1009
00:40:06,760 --> 00:40:11,470
it's private information to a lot of

1010
00:40:08,890 --> 00:40:16,180
people whether or not

1011
00:40:11,470 --> 00:40:18,339
it should be it's much much harder to

1012
00:40:16,180 --> 00:40:20,740
control because it's not based on data

1013
00:40:18,339 --> 00:40:24,009
directly collected you can say you

1014
00:40:20,740 --> 00:40:28,709
cannot collect information say from my

1015
00:40:24,010 --> 00:40:31,720
doctor on my sexual orientation but

1016
00:40:28,710 --> 00:40:37,480
maybe there are proxy variables that

1017
00:40:31,720 --> 00:40:42,189
tend to indicate it the foods that I buy

1018
00:40:37,480 --> 00:40:44,530
might indicate my ethnicity proxy

1019
00:40:42,190 --> 00:40:48,160
variables are a very powerful thing

1020
00:40:44,530 --> 00:40:49,839
there was a study done by the Federal

1021
00:40:48,160 --> 00:40:52,328
Trade US Federal Trade Commission about

1022
00:40:49,839 --> 00:40:54,700
ten years ago they discovered that auto

1023
00:40:52,329 --> 00:40:57,819
insurance companies were using credit

1024
00:40:54,700 --> 00:40:59,680
scores to set rates what is your ability

1025
00:40:57,819 --> 00:41:00,910
or willingness to pay a debt have to do

1026
00:40:59,680 --> 00:41:02,828
with whether or not you're going to get

1027
00:41:00,910 --> 00:41:06,670
into an auto mode whatever bial accident

1028
00:41:02,829 --> 00:41:09,430
and the FTC staff came to three

1029
00:41:06,670 --> 00:41:11,559
conclusions one it was a valid predictor

1030
00:41:09,430 --> 00:41:13,240
why what machine learning doesn't tell

1031
00:41:11,559 --> 00:41:15,430
us why it just says there's a

1032
00:41:13,240 --> 00:41:17,379
correlation this is a good predictor and

1033
00:41:15,430 --> 00:41:21,759
insurance is about prediction and

1034
00:41:17,380 --> 00:41:24,069
statistics not about causation two there

1035
00:41:21,760 --> 00:41:27,670
was also a correlation with that with

1036
00:41:24,069 --> 00:41:29,950
ethnicity the higher rates were going to

1037
00:41:27,670 --> 00:41:33,520
certain ethnic groups based on credit

1038
00:41:29,950 --> 00:41:36,069
scores well that's bad social policy so

1039
00:41:33,520 --> 00:41:38,079
the FTC staff said we're going to solve

1040
00:41:36,069 --> 00:41:40,150
this we're going to try to build a model

1041
00:41:38,079 --> 00:41:42,460
that's just as predictive but not

1042
00:41:40,150 --> 00:41:46,270
discriminatory and guess what they

1043
00:41:42,460 --> 00:41:50,490
couldn't do it there was something deep

1044
00:41:46,270 --> 00:41:52,630
in the data that said yes there is this

1045
00:41:50,490 --> 00:41:54,220
true correlation that's going to

1046
00:41:52,630 --> 00:41:56,680
discriminate if we don't do something

1047
00:41:54,220 --> 00:41:59,009
regular in a regulatory fashion against

1048
00:41:56,680 --> 00:42:02,230
certain ethnic groups in setting rates

1049
00:41:59,010 --> 00:42:07,740
it's very hard to find and eliminate all

1050
00:42:02,230 --> 00:42:07,740
of this and got nothing to do with PII

1051
00:42:09,390 --> 00:42:17,529
so to me notice and consent is dead no

1052
00:42:14,380 --> 00:42:19,420
one knows who collects the data no one

1053
00:42:17,529 --> 00:42:21,910
knows what they'll do with it no one

1054
00:42:19,420 --> 00:42:24,140
knows where it's stored and some of the

1055
00:42:21,910 --> 00:42:27,370
most sensitive stuff like location is

1056
00:42:24,140 --> 00:42:30,799
use it's used for your benefit you know

1057
00:42:27,370 --> 00:42:32,990
how do I get from point A to point B in

1058
00:42:30,800 --> 00:42:34,480
a map program and it's part of what's

1059
00:42:32,990 --> 00:42:37,129
called you data shadow

1060
00:42:34,480 --> 00:42:40,070
you know even the US Supreme Court has

1061
00:42:37,130 --> 00:42:42,830
noted how sensitive location data can be

1062
00:42:40,070 --> 00:42:45,440
in the aggregate so if we don't have

1063
00:42:42,830 --> 00:42:48,650
notice and consent what should we do

1064
00:42:45,440 --> 00:42:51,350
what should we replace it with one

1065
00:42:48,650 --> 00:42:54,620
answer is use control it's controversial

1066
00:42:51,350 --> 00:42:56,960
but give up on data collection

1067
00:42:54,620 --> 00:42:59,770
restriction it doesn't work better in

1068
00:42:56,960 --> 00:43:03,530
the EU but still doesn't work that well

1069
00:42:59,770 --> 00:43:05,450
instead let people specify how their

1070
00:43:03,530 --> 00:43:07,960
data can be used not what can be

1071
00:43:05,450 --> 00:43:10,580
collected but what it can be used for

1072
00:43:07,960 --> 00:43:12,860
targeted advertising statistical

1073
00:43:10,580 --> 00:43:15,710
analysis medical research what-have-you

1074
00:43:12,860 --> 00:43:19,670
it sounds like a great idea it's not

1075
00:43:15,710 --> 00:43:22,970
that easy how do you define your use

1076
00:43:19,670 --> 00:43:26,830
categories how do you give people a

1077
00:43:22,970 --> 00:43:28,850
really usable interface to specify

1078
00:43:26,830 --> 00:43:32,240
here's a kind of data we're collecting

1079
00:43:28,850 --> 00:43:34,430
and here's a kind of use that you may or

1080
00:43:32,240 --> 00:43:39,680
may not want to permit for this kind of

1081
00:43:34,430 --> 00:43:43,819
data usability of privacy settings is a

1082
00:43:39,680 --> 00:43:46,549
fiendishly difficult problem very few of

1083
00:43:43,820 --> 00:43:48,110
anyone has gotten that right you've got

1084
00:43:46,550 --> 00:43:51,740
to give consent across long time

1085
00:43:48,110 --> 00:43:55,450
intervals you know I have been posting

1086
00:43:51,740 --> 00:43:57,770
stuff on the net for about 40 years now

1087
00:43:55,450 --> 00:43:59,450
it's prime mentions one of the people

1088
00:43:57,770 --> 00:44:02,030
created net dues which went live in

1089
00:43:59,450 --> 00:44:03,500
January of 1980 and I was one of the

1090
00:44:02,030 --> 00:44:08,120
founders so I was out there for the very

1091
00:44:03,500 --> 00:44:10,820
beginning do I have the same preferences

1092
00:44:08,120 --> 00:44:13,880
today as I had 40 years ago well I was

1093
00:44:10,820 --> 00:44:15,650
lucky my first boss at Bell Labs when I

1094
00:44:13,880 --> 00:44:17,000
walked into his office a few years after

1095
00:44:15,650 --> 00:44:20,300
that said I've seen your flames on net

1096
00:44:17,000 --> 00:44:24,890
no Steve yeah okay upper management

1097
00:44:20,300 --> 00:44:27,050
reads these things yeah okay data that

1098
00:44:24,890 --> 00:44:29,930
exists can be abused by hackers

1099
00:44:27,050 --> 00:44:33,020
scofflaws governments or simply through

1100
00:44:29,930 --> 00:44:35,870
a change in the law and it turns out

1101
00:44:33,020 --> 00:44:36,690
that the under US law it may be

1102
00:44:35,870 --> 00:44:38,970
impossible

1103
00:44:36,690 --> 00:44:41,579
two mandates use restrictions for

1104
00:44:38,970 --> 00:44:43,890
companies they could adopt it but you

1105
00:44:41,579 --> 00:44:47,700
may not be able to mandate it under US

1106
00:44:43,890 --> 00:44:49,589
law so how do we implement this use

1107
00:44:47,700 --> 00:44:52,919
control you can start with a privacy

1108
00:44:49,589 --> 00:44:56,130
preserving credential scheme tag all the

1109
00:44:52,920 --> 00:44:58,619
data that you create with a privacy

1110
00:44:56,130 --> 00:45:01,890
preserving sub identity and a data type

1111
00:44:58,619 --> 00:45:06,540
and you can publish tuple saying data

1112
00:45:01,890 --> 00:45:08,759
type anonymous identity allowed uses and

1113
00:45:06,540 --> 00:45:10,859
all digitally signed with your anonymous

1114
00:45:08,760 --> 00:45:12,359
credential swaps credential and where we

1115
00:45:10,859 --> 00:45:20,180
put it well gee do we put it in the

1116
00:45:12,359 --> 00:45:20,180
blockchain no no tomatoes please and if

1117
00:45:20,510 --> 00:45:24,329
if you change your mind about something

1118
00:45:22,740 --> 00:45:26,279
you just push out something it's your

1119
00:45:24,329 --> 00:45:29,640
newest it's you new a statement that

1120
00:45:26,280 --> 00:45:31,920
wins enforcement well it's often pointed

1121
00:45:29,640 --> 00:45:34,379
out governments have a role if you break

1122
00:45:31,920 --> 00:45:38,300
a legally binding promise if you break a

1123
00:45:34,380 --> 00:45:38,300
law and governments can come down on you

1124
00:45:38,660 --> 00:45:43,399
difficult but might be doable might be a

1125
00:45:41,280 --> 00:45:46,650
worth examining as a research project

1126
00:45:43,400 --> 00:45:50,730
what we really need is a new privacy

1127
00:45:46,650 --> 00:45:53,520
paradigm it's got a scale so very many

1128
00:45:50,730 --> 00:45:56,010
data collectors known unknown in the

1129
00:45:53,520 --> 00:45:57,509
future it has to scale across time

1130
00:45:56,010 --> 00:45:59,640
it's got to be comprehensible by

1131
00:45:57,510 --> 00:46:02,280
individuals it's got to account for

1132
00:45:59,640 --> 00:46:03,990
inferences it's got to trade-off the

1133
00:46:02,280 --> 00:46:06,750
harms and benefits of different kinds of

1134
00:46:03,990 --> 00:46:08,669
data use and I have no idea what such a

1135
00:46:06,750 --> 00:46:10,470
paradigm would look like where's that

1136
00:46:08,670 --> 00:46:12,210
for you me as an academic that's great

1137
00:46:10,470 --> 00:46:15,149
no if we knew the answer it wouldn't be

1138
00:46:12,210 --> 00:46:17,569
research but you know this is the real

1139
00:46:15,150 --> 00:46:22,140
challenge how do we do this

1140
00:46:17,569 --> 00:46:24,509
so what should the IETF do obviously

1141
00:46:22,140 --> 00:46:26,040
encrypt as much as possible the IETF has

1142
00:46:24,510 --> 00:46:28,069
been moving in that direction for more

1143
00:46:26,040 --> 00:46:32,339
than 20 years and that's great

1144
00:46:28,069 --> 00:46:35,670
avoid creating unnecessary third party

1145
00:46:32,339 --> 00:46:39,270
metadata one place this really shows up

1146
00:46:35,670 --> 00:46:41,670
in protocol definitions is stuff that's

1147
00:46:39,270 --> 00:46:45,569
left to the implementation because that

1148
00:46:41,670 --> 00:46:48,150
becomes finger printable how about to

1149
00:46:45,569 --> 00:46:50,820
pick one random example if the HTTP

1150
00:46:48,150 --> 00:46:53,220
header headers could only be a nicer

1151
00:46:50,820 --> 00:46:54,960
a specified order and even if they win

1152
00:46:53,220 --> 00:46:57,359
all you have to specify it and didn't

1153
00:46:54,960 --> 00:47:01,280
and have you know just a semicolon or

1154
00:46:57,360 --> 00:47:04,830
something design more privacy protocols

1155
00:47:01,280 --> 00:47:07,920
do a privacy analysis of protocols

1156
00:47:04,830 --> 00:47:10,020
similar to what is done for security

1157
00:47:07,920 --> 00:47:11,700
considerations today you know some years

1158
00:47:10,020 --> 00:47:16,620
ago there was the geo-print working

1159
00:47:11,700 --> 00:47:17,819
group geolocation said ok we this is

1160
00:47:16,620 --> 00:47:22,410
dangerous stuff from a privacy

1161
00:47:17,820 --> 00:47:24,270
perspective let's look at it first

1162
00:47:22,410 --> 00:47:27,210
tagging might help create some more

1163
00:47:24,270 --> 00:47:29,490
metadata so here are the references most

1164
00:47:27,210 --> 00:47:31,200
of the quotes are from my comments on

1165
00:47:29,490 --> 00:47:34,140
privacy getting written for us legal

1166
00:47:31,200 --> 00:47:35,640
context I apologize quoted assorted

1167
00:47:34,140 --> 00:47:37,279
academics from fifty years ago

1168
00:47:35,640 --> 00:47:44,240
thank you

1169
00:47:37,280 --> 00:47:47,400
[Applause]

1170
00:47:44,240 --> 00:47:48,720
thank you very much with that we open

1171
00:47:47,400 --> 00:47:53,550
the floor to questions

1172
00:47:48,720 --> 00:47:55,799
I have Eliot Steve Arvind thanks very

1173
00:47:53,550 --> 00:48:00,510
much for your presentations two comments

1174
00:47:55,800 --> 00:48:02,670
first of all related to Arbenz work I'm

1175
00:48:00,510 --> 00:48:05,010
very pleased to fund a colleague of

1176
00:48:02,670 --> 00:48:06,740
yours surgical min at Berkeley who's

1177
00:48:05,010 --> 00:48:08,760
done a lot of work in this space

1178
00:48:06,740 --> 00:48:12,839
particularly around linkages on

1179
00:48:08,760 --> 00:48:17,160
cellphones and the idea that you have

1180
00:48:12,840 --> 00:48:19,230
about TLS and privacy of IOT is

1181
00:48:17,160 --> 00:48:21,839
something that I that I am deeply

1182
00:48:19,230 --> 00:48:24,390
involved in and one of the things that

1183
00:48:21,840 --> 00:48:27,090
it raises the question of privacy

1184
00:48:24,390 --> 00:48:30,000
brokerage we release this information

1185
00:48:27,090 --> 00:48:32,160
something often times we release

1186
00:48:30,000 --> 00:48:33,960
information for a purpose and the notion

1187
00:48:32,160 --> 00:48:36,810
of contextual privacy is something that

1188
00:48:33,960 --> 00:48:38,540
I think is a relatively nascent

1189
00:48:36,810 --> 00:48:41,279
if I understand in terms of the research

1190
00:48:38,540 --> 00:48:42,870
and I'd be very interested to see us

1191
00:48:41,280 --> 00:48:45,420
continue that discussion here at the

1192
00:48:42,870 --> 00:48:47,670
IETF or at least at the IRT F as to how

1193
00:48:45,420 --> 00:48:50,280
as to what that means and this goes to

1194
00:48:47,670 --> 00:48:52,500
the tagging that you mentioned Steve so

1195
00:48:50,280 --> 00:48:54,870
thanks for your research and if people

1196
00:48:52,500 --> 00:48:55,950
haven't looked at surges work too he's

1197
00:48:54,870 --> 00:48:59,880
done a lot of work particularly around

1198
00:48:55,950 --> 00:49:05,490
the Amazon echo recently that's very

1199
00:48:59,880 --> 00:49:08,190
useful Thanks Thank You Barry hi this is

1200
00:49:05,490 --> 00:49:09,509
berrin Lee this is a very live mic hi

1201
00:49:08,190 --> 00:49:14,430
this is Barry liebe Stephan

1202
00:49:09,510 --> 00:49:17,190
how does notice and consent ng DPR work

1203
00:49:14,430 --> 00:49:19,410
with being tracked by Facebook and

1204
00:49:17,190 --> 00:49:20,880
Twitter icons and the like that's

1205
00:49:19,410 --> 00:49:23,129
another reason notice the consent

1206
00:49:20,880 --> 00:49:24,690
doesn't work you're being tracked by

1207
00:49:23,130 --> 00:49:27,450
lots of people with whom you don't have

1208
00:49:24,690 --> 00:49:29,520
a direct relationship and how can you

1209
00:49:27,450 --> 00:49:32,640
consent I don't can can I say I don't

1210
00:49:29,520 --> 00:49:36,120
consent to seeing a facebook like button

1211
00:49:32,640 --> 00:49:39,150
on a web page I want to visit do you do

1212
00:49:36,120 --> 00:49:42,930
you have any idea how they have not been

1213
00:49:39,150 --> 00:49:45,180
attacked by the GDP our people I'm not a

1214
00:49:42,930 --> 00:49:46,009
lawyer I will let the lawyers answer

1215
00:49:45,180 --> 00:49:55,129
that one

1216
00:49:46,010 --> 00:49:57,920
that's fair you have Aaron so so this is

1217
00:49:55,130 --> 00:49:59,150
actually I'm interested in responses

1218
00:49:57,920 --> 00:50:01,940
from both of you that there's been a lot

1219
00:49:59,150 --> 00:50:04,160
of interest in the IHF in privacy issues

1220
00:50:01,940 --> 00:50:06,920
around the DNS and so I'm wondering what

1221
00:50:04,160 --> 00:50:09,980
you think of the the risks the privacy

1222
00:50:06,920 --> 00:50:14,300
risks around that and whether dot and Oh

1223
00:50:09,980 --> 00:50:16,100
provide useful solutions privacy like

1224
00:50:14,300 --> 00:50:18,020
any other security problem has to be

1225
00:50:16,100 --> 00:50:20,870
done in the context of a threat model

1226
00:50:18,020 --> 00:50:22,910
who is trying to collect this data what

1227
00:50:20,870 --> 00:50:29,569
are they going to do with it you know

1228
00:50:22,910 --> 00:50:31,250
with DNS over HTTP over TLS you know you

1229
00:50:29,570 --> 00:50:33,200
might get a central aggregation point

1230
00:50:31,250 --> 00:50:36,080
and do you trust them to be honest

1231
00:50:33,200 --> 00:50:37,220
secure against government's secure

1232
00:50:36,080 --> 00:50:39,560
against government's will come armed

1233
00:50:37,220 --> 00:50:40,490
with legal process and you don't

1234
00:50:39,560 --> 00:50:44,690
necessarily have a business relationship

1235
00:50:40,490 --> 00:50:50,990
it's not clear to me that guarding

1236
00:50:44,690 --> 00:50:54,380
against the NSA or GCHQ or the FSB or

1237
00:50:50,990 --> 00:50:56,450
GRU the Mossad or whomever is the best

1238
00:50:54,380 --> 00:50:58,160
threat model versus the commercial

1239
00:50:56,450 --> 00:51:01,490
threat model that one might actually be

1240
00:50:58,160 --> 00:51:04,460
best dealt with with laws saying your

1241
00:51:01,490 --> 00:51:06,649
ISP can't used collect or use this data

1242
00:51:04,460 --> 00:51:09,140
in any way rather than this technical

1243
00:51:06,650 --> 00:51:11,120
mechanism and avoids the central point

1244
00:51:09,140 --> 00:51:12,890
of collection which is a greater threat

1245
00:51:11,120 --> 00:51:20,150
for against certain threat modes watch

1246
00:51:12,890 --> 00:51:22,400
the threat model marry my questions for

1247
00:51:20,150 --> 00:51:25,030
Arvind I really liked your example of

1248
00:51:22,400 --> 00:51:27,680
how a limited technical mitigation was

1249
00:51:25,030 --> 00:51:29,930
encouraged a strong policy that then

1250
00:51:27,680 --> 00:51:31,220
filled that gap for user privacy this

1251
00:51:29,930 --> 00:51:33,770
was like about a third through your

1252
00:51:31,220 --> 00:51:35,359
presentation and I think that like

1253
00:51:33,770 --> 00:51:37,850
paralleling that example with your

1254
00:51:35,360 --> 00:51:40,250
conclusion is also interesting so I

1255
00:51:37,850 --> 00:51:41,720
guess my question would be and then I

1256
00:51:40,250 --> 00:51:45,290
can explain a bit more if that's helpful

1257
00:51:41,720 --> 00:51:48,379
is I mean I think there's there is a

1258
00:51:45,290 --> 00:51:50,990
role that that measurement and research

1259
00:51:48,380 --> 00:51:52,579
can still play even if it's limited now

1260
00:51:50,990 --> 00:51:55,399
because of the privacy

1261
00:51:52,579 --> 00:51:57,349
enhanced protocols that were using how

1262
00:51:55,400 --> 00:51:59,420
can we instead pivot instead of trying

1263
00:51:57,349 --> 00:52:00,890
to bargain like in the stages of grief

1264
00:51:59,420 --> 00:52:03,079
or in the bargaining stage like how can

1265
00:52:00,890 --> 00:52:05,660
we do both of these things when there's

1266
00:52:03,079 --> 00:52:09,019
an actual inherent technical paradox

1267
00:52:05,660 --> 00:52:12,348
between the two and rather go into pivot

1268
00:52:09,019 --> 00:52:15,758
into a more complicated relationship

1269
00:52:12,349 --> 00:52:18,579
between policy incentives policy sticks

1270
00:52:15,759 --> 00:52:21,380
and then you know so I wonder how

1271
00:52:18,579 --> 00:52:24,949
academic researchers can help for

1272
00:52:21,380 --> 00:52:27,670
example human right or not human rights

1273
00:52:24,949 --> 00:52:29,709
but like impact assessments or getting

1274
00:52:27,670 --> 00:52:33,079
companies to take more responsibility

1275
00:52:29,709 --> 00:52:36,019
for doing privacy audits security

1276
00:52:33,079 --> 00:52:38,209
audience and it's a slower approach it's

1277
00:52:36,019 --> 00:52:40,939
not as fast as scanning a million

1278
00:52:38,209 --> 00:52:43,249
websites every month but I think that

1279
00:52:40,939 --> 00:52:46,249
where we're at now and the way that

1280
00:52:43,249 --> 00:52:49,339
we've advanced privacy for end-users to

1281
00:52:46,249 --> 00:52:51,910
get the higher hanging fruit we actually

1282
00:52:49,339 --> 00:52:55,640
have to have more complicated approaches

1283
00:52:51,910 --> 00:52:58,428
that's great thank you the way in which

1284
00:52:55,640 --> 00:53:01,939
I think measurement research has helped

1285
00:52:58,429 --> 00:53:02,390
at a very high level is in economic

1286
00:53:01,939 --> 00:53:04,819
terms

1287
00:53:02,390 --> 00:53:08,509
closing the information asymmetry and

1288
00:53:04,819 --> 00:53:11,109
what I mean by that is when a product

1289
00:53:08,509 --> 00:53:14,749
doesn't live up to its privacy claims

1290
00:53:11,109 --> 00:53:16,699
oftentimes the buyers users consumers of

1291
00:53:14,749 --> 00:53:20,328
that products don't know and have no way

1292
00:53:16,699 --> 00:53:22,670
of knowing and this parallels in in the

1293
00:53:20,329 --> 00:53:24,890
United States one of the critical

1294
00:53:22,670 --> 00:53:27,769
situations that we had with respect to

1295
00:53:24,890 --> 00:53:29,868
used car sales 40 years ago and the

1296
00:53:27,769 --> 00:53:32,209
matter became so critical that buyers of

1297
00:53:29,869 --> 00:53:34,039
used cars would not know if the car had

1298
00:53:32,209 --> 00:53:35,899
a critical defect whereas the seller

1299
00:53:34,039 --> 00:53:38,089
would know and would not tell them and

1300
00:53:35,900 --> 00:53:39,769
it was exactly the same kind of problem

1301
00:53:38,089 --> 00:53:41,660
that was faced economists call this an

1302
00:53:39,769 --> 00:53:43,428
information asymmetry and that

1303
00:53:41,660 --> 00:53:45,739
particular information asymmetry was

1304
00:53:43,429 --> 00:53:48,619
closed by lemon laws that mandated

1305
00:53:45,739 --> 00:53:51,469
certain information exclosure disclosure

1306
00:53:48,619 --> 00:53:53,809
pardon me that guaranteed the right for

1307
00:53:51,469 --> 00:53:56,179
buyers of cars to first take it to

1308
00:53:53,809 --> 00:53:58,670
mechanics to be inspected and so on so

1309
00:53:56,179 --> 00:54:00,199
broadly to your question as long as we

1310
00:53:58,670 --> 00:54:02,479
have some way of closing this

1311
00:54:00,199 --> 00:54:04,559
information asymmetry that exists

1312
00:54:02,479 --> 00:54:06,448
between the sellers of

1313
00:54:04,559 --> 00:54:08,549
products and services and the people who

1314
00:54:06,449 --> 00:54:10,349
use them I think we're in good shape and

1315
00:54:08,549 --> 00:54:11,939
one of the ways we've been doing that is

1316
00:54:10,349 --> 00:54:13,890
with academic research that's been you

1317
00:54:11,939 --> 00:54:15,719
know scanning a million endpoints at

1318
00:54:13,890 --> 00:54:18,808
once but it doesn't have to be the only

1319
00:54:15,719 --> 00:54:20,880
way another critical way to do that has

1320
00:54:18,809 --> 00:54:22,949
been journalists have been you know

1321
00:54:20,880 --> 00:54:25,079
individually examining these products in

1322
00:54:22,949 --> 00:54:27,209
a lot of detail and holding companies

1323
00:54:25,079 --> 00:54:28,890
feet to the fire so as long as we have

1324
00:54:27,209 --> 00:54:30,538
some oversight mechanism whether that

1325
00:54:28,890 --> 00:54:32,489
comes from law whether that comes from

1326
00:54:30,539 --> 00:54:34,410
academia whether that comes from

1327
00:54:32,489 --> 00:54:37,049
journalism or whether it simply comes

1328
00:54:34,410 --> 00:54:39,269
from a more informed public that helps

1329
00:54:37,049 --> 00:54:42,170
close this information asymmetry then I

1330
00:54:39,269 --> 00:54:46,529
think we'll be in better shape thank you

1331
00:54:42,170 --> 00:54:49,709
okay front okay so Peter file Deutsche

1332
00:54:46,529 --> 00:54:51,749
Telekom so from my point of view that

1333
00:54:49,709 --> 00:54:56,578
was an excellent presentation but it was

1334
00:54:51,749 --> 00:55:00,390
very technology oriented so we have an

1335
00:54:56,579 --> 00:55:03,239
also very North American oriented so in

1336
00:55:00,390 --> 00:55:08,999
in Europe especially in Germany we have

1337
00:55:03,239 --> 00:55:12,719
very severe laws regarding privacy so

1338
00:55:08,999 --> 00:55:14,729
you mentioned the GD P R which is in

1339
00:55:12,719 --> 00:55:19,319
effect since since last year basically

1340
00:55:14,729 --> 00:55:21,178
and I don't think that we can solve this

1341
00:55:19,319 --> 00:55:25,170
issue from a technical point of view

1342
00:55:21,179 --> 00:55:26,609
it's a legal issue so in Europe I don't

1343
00:55:25,170 --> 00:55:30,029
know if you were at the news but

1344
00:55:26,609 --> 00:55:32,910
Facebook will probably have to pay some

1345
00:55:30,029 --> 00:55:37,469
billions because they did not follow the

1346
00:55:32,910 --> 00:55:40,890
rules of this law and in Europe the any

1347
00:55:37,469 --> 00:55:43,709
data I give to any any company is owned

1348
00:55:40,890 --> 00:55:46,199
by me and not by the company who can

1349
00:55:43,709 --> 00:55:49,140
catch this data so this is something

1350
00:55:46,199 --> 00:55:54,359
that has to be changed worldwide so

1351
00:55:49,140 --> 00:55:56,129
especially in the US and again I just

1352
00:55:54,359 --> 00:55:58,038
wanted to point out that it's not a

1353
00:55:56,130 --> 00:56:01,459
technical issue it's a legal issue

1354
00:55:58,039 --> 00:56:05,729
thank you I agree I agree completely the

1355
00:56:01,459 --> 00:56:08,038
document that that I wrote to it my talk

1356
00:56:05,729 --> 00:56:10,319
was derived from was a submission to a

1357
00:56:08,039 --> 00:56:12,420
u.s. government process on privacy

1358
00:56:10,319 --> 00:56:15,690
because I agree completely that is a

1359
00:56:12,420 --> 00:56:17,860
very important legal role for the

1360
00:56:15,690 --> 00:56:21,130
legalities here in the governments

1361
00:56:17,860 --> 00:56:23,020
around the world but I think that trying

1362
00:56:21,130 --> 00:56:25,630
to base your privacy on notice and

1363
00:56:23,020 --> 00:56:28,750
consent from a technical perspective is

1364
00:56:25,630 --> 00:56:31,930
not going to work and I want what I was

1365
00:56:28,750 --> 00:56:35,050
what my paper said is we need to find a

1366
00:56:31,930 --> 00:56:39,460
different paradigm for regulators and

1367
00:56:35,050 --> 00:56:40,960
legislators to mandate so thank you for

1368
00:56:39,460 --> 00:56:44,200
your comment I completely take your

1369
00:56:40,960 --> 00:56:46,120
point that the role of regulation and

1370
00:56:44,200 --> 00:56:47,770
the law is very critical here thank you

1371
00:56:46,120 --> 00:56:49,540
for bringing up the gdpr

1372
00:56:47,770 --> 00:56:51,009
one thing I want to slightly push back

1373
00:56:49,540 --> 00:56:53,950
on is that I wouldn't see it as a

1374
00:56:51,010 --> 00:56:55,870
technical or legal issue I don't think

1375
00:56:53,950 --> 00:56:57,460
it's a dichotomy in fact a lot of the

1376
00:56:55,870 --> 00:56:59,770
investigations that have come about

1377
00:56:57,460 --> 00:57:02,170
under the gdpr and the fines that have

1378
00:56:59,770 --> 00:57:03,790
resulted from that those privacy issues

1379
00:57:02,170 --> 00:57:05,140
only came to be known because of the

1380
00:57:03,790 --> 00:57:06,550
kind of research that I described

1381
00:57:05,140 --> 00:57:08,319
whether it was done by academics

1382
00:57:06,550 --> 00:57:10,150
journalists or some other third parties

1383
00:57:08,320 --> 00:57:12,370
so that's technical work in a sense and

1384
00:57:10,150 --> 00:57:14,470
for me the real success stories involve

1385
00:57:12,370 --> 00:57:18,640
the collaboration between technical

1386
00:57:14,470 --> 00:57:19,810
teams and legal measures thank you we're

1387
00:57:18,640 --> 00:57:21,460
gonna hide and close the mic line so

1388
00:57:19,810 --> 00:57:24,910
we'll bring the queues please be brief

1389
00:57:21,460 --> 00:57:27,280
okay folks thank you very much for your

1390
00:57:24,910 --> 00:57:28,810
talks to both Stephen Arvind

1391
00:57:27,280 --> 00:57:30,130
I want to actually follow up on this

1392
00:57:28,810 --> 00:57:32,259
discussion because in fact that's

1393
00:57:30,130 --> 00:57:33,940
precisely what I wanted to say in this

1394
00:57:32,260 --> 00:57:35,800
world where we believe privacy is

1395
00:57:33,940 --> 00:57:38,260
getting harder the battle is lost I very

1396
00:57:35,800 --> 00:57:39,760
much appreciate the message of no it's

1397
00:57:38,260 --> 00:57:42,760
not lost we can keep improving things

1398
00:57:39,760 --> 00:57:45,310
and also just following up from the

1399
00:57:42,760 --> 00:57:47,020
previous speaker that we are going all

1400
00:57:45,310 --> 00:57:49,330
the way from the extremes of regulations

1401
00:57:47,020 --> 00:57:51,400
towards the purely technical and I think

1402
00:57:49,330 --> 00:57:53,740
they both have to marry at some point in

1403
00:57:51,400 --> 00:57:55,990
time it's true we don't have the answers

1404
00:57:53,740 --> 00:57:57,939
to all the questions but in it's true

1405
00:57:55,990 --> 00:57:59,859
that for instance I was recently in a

1406
00:57:57,940 --> 00:58:02,920
regulatory discussion where they were

1407
00:57:59,860 --> 00:58:07,180
just discussing okay in a world where

1408
00:58:02,920 --> 00:58:08,650
the watch is checking your vital signs

1409
00:58:07,180 --> 00:58:10,120
and then it's sending it to your phone

1410
00:58:08,650 --> 00:58:11,110
and then that's anything to an app and

1411
00:58:10,120 --> 00:58:13,870
that's sending it to a cloud provider

1412
00:58:11,110 --> 00:58:16,750
and who do you regulate it's not anymore

1413
00:58:13,870 --> 00:58:19,270
the world of one piece does one thing so

1414
00:58:16,750 --> 00:58:20,770
it's important I agree with Arvind and

1415
00:58:19,270 --> 00:58:24,130
message that we can help their

1416
00:58:20,770 --> 00:58:25,360
regulatory bodies understand that is the

1417
00:58:24,130 --> 00:58:26,320
service provider probably the most

1418
00:58:25,360 --> 00:58:28,090
account or wand

1419
00:58:26,320 --> 00:58:31,090
we'll make sure that the information

1420
00:58:28,090 --> 00:58:34,870
flows down and also on our side as a

1421
00:58:31,090 --> 00:58:37,180
technical writers we do have indeed the

1422
00:58:34,870 --> 00:58:40,060
the the role of writing the right

1423
00:58:37,180 --> 00:58:41,049
standard but also educating people as

1424
00:58:40,060 --> 00:58:44,560
much as we can

1425
00:58:41,050 --> 00:58:45,880
regulatory it could be a section Steven

1426
00:58:44,560 --> 00:58:47,200
you mentioned the privacy considerations

1427
00:58:45,880 --> 00:58:49,330
I think that's a great way to

1428
00:58:47,200 --> 00:58:51,129
communicate what the standard should do

1429
00:58:49,330 --> 00:58:54,610
what what are the issues what should be

1430
00:58:51,130 --> 00:58:56,440
looked after and then follow up and make

1431
00:58:54,610 --> 00:58:58,960
sure that we do improve because

1432
00:58:56,440 --> 00:59:00,640
definitely the the battle is not lost

1433
00:58:58,960 --> 00:59:07,480
and there's a lot of things we can keep

1434
00:59:00,640 --> 00:59:09,400
doing hi riad Wahby Arvind you mentioned

1435
00:59:07,480 --> 00:59:12,160
at the very end of your talk a project

1436
00:59:09,400 --> 00:59:13,390
of Stanford TLS our AR rotate and

1437
00:59:12,160 --> 00:59:16,000
release I was one of the authors on that

1438
00:59:13,390 --> 00:59:18,100
so I think you're absolutely right that

1439
00:59:16,000 --> 00:59:19,540
there are some technical measures like

1440
00:59:18,100 --> 00:59:21,100
that but just to provide a little

1441
00:59:19,540 --> 00:59:23,140
background in kind of a counterpoint

1442
00:59:21,100 --> 00:59:24,339
while we were working on that we

1443
00:59:23,140 --> 00:59:25,870
actually spoke with some of the people

1444
00:59:24,340 --> 00:59:27,370
on the TLS working group and said hey

1445
00:59:25,870 --> 00:59:29,080
look it might be the case that like a

1446
00:59:27,370 --> 00:59:32,589
small change to TLS would actually make

1447
00:59:29,080 --> 00:59:34,660
this easier and we very rightly got

1448
00:59:32,590 --> 00:59:36,280
pushback from from the TLS working group

1449
00:59:34,660 --> 00:59:38,200
who said yeah but we don't want to make

1450
00:59:36,280 --> 00:59:39,970
this easier because yeah you might want

1451
00:59:38,200 --> 00:59:41,830
to use it for watching your own devices

1452
00:59:39,970 --> 00:59:43,330
but anything that we make easier for you

1453
00:59:41,830 --> 00:59:46,330
is going to also be easier for somebody

1454
00:59:43,330 --> 00:59:49,569
who's spying on you so while it's true

1455
00:59:46,330 --> 00:59:51,850
that we want to look at our devices it

1456
00:59:49,570 --> 00:59:53,770
seems like technical measures at the

1457
00:59:51,850 --> 00:59:56,230
level of you know the encryption

1458
00:59:53,770 --> 00:59:58,210
standards maybe not the right way to go

1459
00:59:56,230 --> 01:00:00,040
we may be in some sense at the mercy of

1460
00:59:58,210 --> 01:00:02,440
the people who are building the device

1461
01:00:00,040 --> 01:00:04,570
is almost no matter what we do because

1462
01:00:02,440 --> 01:00:06,490
you know we shouldn't insert back doors

1463
01:00:04,570 --> 01:00:08,530
into TLS for our own good they will hurt

1464
01:00:06,490 --> 01:00:10,899
us more than they will help us so just

1465
01:00:08,530 --> 01:00:12,880
yeah thank you for your comment and

1466
01:00:10,900 --> 01:00:15,510
somewhat aware if the debates that have

1467
01:00:12,880 --> 01:00:18,640
gone on in the in the TLS working group

1468
01:00:15,510 --> 01:00:20,260
my main goal was to call attention to

1469
01:00:18,640 --> 01:00:21,250
the severity of the problem I'm not

1470
01:00:20,260 --> 01:00:23,530
claiming that I know what the right

1471
01:00:21,250 --> 01:00:28,260
solution is but I think the current

1472
01:00:23,530 --> 01:00:30,880
situation is perhaps not optimal okay

1473
01:00:28,260 --> 01:00:32,860
max Paula CableLabs thanks for the talks

1474
01:00:30,880 --> 01:00:36,620
I would like to ask you a question

1475
01:00:32,860 --> 01:00:41,240
following the the gentleman from Ridge

1476
01:00:36,620 --> 01:00:43,490
Telekom about ownership of the data this

1477
01:00:41,240 --> 01:00:45,620
is a very big difference in the US and

1478
01:00:43,490 --> 01:00:47,870
Europe for example where ownership of

1479
01:00:45,620 --> 01:00:50,660
the data is always about me when I'm in

1480
01:00:47,870 --> 01:00:52,609
Europe and once is collected in the u.s.

1481
01:00:50,660 --> 01:00:56,899
is property of who collected the data

1482
01:00:52,610 --> 01:00:59,990
and this I think is the biggest issue in

1483
01:00:56,900 --> 01:01:02,360
when privacy instead of giving out your

1484
01:00:59,990 --> 01:01:05,120
data you can borrow my data but I can

1485
01:01:02,360 --> 01:01:08,090
always ask you to remove your my data

1486
01:01:05,120 --> 01:01:09,109
from your system whenever I want to you

1487
01:01:08,090 --> 01:01:13,130
know I don't want to have business

1488
01:01:09,110 --> 01:01:14,690
relationship with us etc and and if you

1489
01:01:13,130 --> 01:01:17,360
can elaborate on that if you think that

1490
01:01:14,690 --> 01:01:19,340
this might be a tool that announced

1491
01:01:17,360 --> 01:01:21,890
privacy from a legal standpoint of

1492
01:01:19,340 --> 01:01:24,080
unisys that give me give me as a user

1493
01:01:21,890 --> 01:01:26,089
a possible recourse of action in case

1494
01:01:24,080 --> 01:01:29,060
you're you're failing to protect my

1495
01:01:26,090 --> 01:01:31,010
privacy and the second point is about

1496
01:01:29,060 --> 01:01:33,259
the measurement you say that you know

1497
01:01:31,010 --> 01:01:35,540
this type of privacy issue came out

1498
01:01:33,260 --> 01:01:37,760
because of measurements maybe it's time

1499
01:01:35,540 --> 01:01:41,300
to talk about having measurements as

1500
01:01:37,760 --> 01:01:43,730
part of the legal framework so that is

1501
01:01:41,300 --> 01:01:45,680
not left to academic to expose this but

1502
01:01:43,730 --> 01:01:51,620
with bodies that actually have authority

1503
01:01:45,680 --> 01:01:54,020
to and to follow up on that data

1504
01:01:51,620 --> 01:01:57,080
ownership is a really complicated

1505
01:01:54,020 --> 01:01:59,720
question there's a fair amount of legal

1506
01:01:57,080 --> 01:02:02,120
writing lately legal academic writing on

1507
01:01:59,720 --> 01:02:04,790
why trying to treat data as property can

1508
01:02:02,120 --> 01:02:07,130
have bad side effects one of the

1509
01:02:04,790 --> 01:02:09,710
interesting things from US law is that a

1510
01:02:07,130 --> 01:02:12,860
lot of these transactions there are two

1511
01:02:09,710 --> 01:02:15,310
different parties that have ownership so

1512
01:02:12,860 --> 01:02:18,050
I mentioned about the my mechanics

1513
01:02:15,310 --> 01:02:21,110
uploading or selling my odometer

1514
01:02:18,050 --> 01:02:22,940
readings well yes my ODOT my mechanic is

1515
01:02:21,110 --> 01:02:24,080
recording the odometer reading to go let

1516
01:02:22,940 --> 01:02:26,780
me know when I should change my oil

1517
01:02:24,080 --> 01:02:28,910
again and that's perfectly that becomes

1518
01:02:26,780 --> 01:02:32,360
a business record of the mechanic and

1519
01:02:28,910 --> 01:02:34,310
that's the property the data belong to

1520
01:02:32,360 --> 01:02:37,100
the mechanic and therefore the mechanic

1521
01:02:34,310 --> 01:02:38,960
can sell it as well as me and my privacy

1522
01:02:37,100 --> 01:02:41,960
problem is that gets aggregated and

1523
01:02:38,960 --> 01:02:44,120
attributed to me as well so there are

1524
01:02:41,960 --> 01:02:46,910
very complicated questions with trying

1525
01:02:44,120 --> 01:02:49,680
to treat this as property even apart

1526
01:02:46,910 --> 01:02:53,279
from the international

1527
01:02:49,680 --> 01:02:55,399
issues of different philosophies there's

1528
01:02:53,280 --> 01:02:58,680
a lot of data that businesses very

1529
01:02:55,400 --> 01:03:01,350
legitimately have to collect and medical

1530
01:02:58,680 --> 01:03:03,419
personnel utterly rely on it they need

1531
01:03:01,350 --> 01:03:05,790
this to keep you healthy they have to

1532
01:03:03,420 --> 01:03:08,880
have this data and then who owns it so

1533
01:03:05,790 --> 01:03:11,430
it's it's not an easy question so which

1534
01:03:08,880 --> 01:03:14,880
is why I like the notion of user control

1535
01:03:11,430 --> 01:03:16,410
instead I just want to say a couple

1536
01:03:14,880 --> 01:03:19,080
sentences about the measurement issue

1537
01:03:16,410 --> 01:03:20,790
that you raised I agree 200 percent that

1538
01:03:19,080 --> 01:03:23,490
measurement should be a standard part of

1539
01:03:20,790 --> 01:03:25,080
the regulatory process and just to tell

1540
01:03:23,490 --> 01:03:26,609
you how much I agree with that at

1541
01:03:25,080 --> 01:03:29,580
Princeton and part of the Center for

1542
01:03:26,610 --> 01:03:31,830
Information Technology Policy and it was

1543
01:03:29,580 --> 01:03:33,630
started 15 years ago with precisely the

1544
01:03:31,830 --> 01:03:36,150
notion that there need to be more

1545
01:03:33,630 --> 01:03:38,070
technologists in government exactly

1546
01:03:36,150 --> 01:03:39,300
because we can do more of the sort of

1547
01:03:38,070 --> 01:03:41,220
things you're calling for it because

1548
01:03:39,300 --> 01:03:43,260
today the main limitation is just the

1549
01:03:41,220 --> 01:03:45,990
technical expertise that exists in

1550
01:03:43,260 --> 01:03:47,850
regulatory agencies if you're a

1551
01:03:45,990 --> 01:03:48,689
technical person get involved with your

1552
01:03:47,850 --> 01:03:51,690
own government

1553
01:03:48,690 --> 01:03:53,700
make sure the lawyers judges legislators

1554
01:03:51,690 --> 01:03:56,100
regulation someone understand the

1555
01:03:53,700 --> 01:04:01,620
technology I've done that I've done this

1556
01:03:56,100 --> 01:04:04,380
twice I highly recommend it and now with

1557
01:04:01,620 --> 01:04:07,230
the last word Phil you have negative

1558
01:04:04,380 --> 01:04:08,730
four minutes so please be extremely

1559
01:04:07,230 --> 01:04:10,290
brief I'm sorry back Mike we caught that

1560
01:04:08,730 --> 01:04:13,710
Mike lines a while ago okay so the name

1561
01:04:10,290 --> 01:04:15,810
withheld actually so Steve said we had

1562
01:04:13,710 --> 01:04:19,350
the wrong trust model you know we got

1563
01:04:15,810 --> 01:04:21,529
thinkable the CIA etc attacking I think

1564
01:04:19,350 --> 01:04:24,450
goes beyond that these third-party

1565
01:04:21,530 --> 01:04:28,610
databases they are a national security

1566
01:04:24,450 --> 01:04:33,529
threat and we saw them weaponized in

1567
01:04:28,610 --> 01:04:37,260
2016 and it isn't just personal data I

1568
01:04:33,530 --> 01:04:42,300
know of an insurance company that is

1569
01:04:37,260 --> 01:04:46,080
operated by an individual who is widely

1570
01:04:42,300 --> 01:04:49,620
believed to be operating on behalf of an

1571
01:04:46,080 --> 01:04:52,110
intelligence agency a hostile one this

1572
01:04:49,620 --> 01:04:55,620
insurance agency specializes in

1573
01:04:52,110 --> 01:04:58,080
commercial vehicles if you think about

1574
01:04:55,620 --> 01:05:01,150
what such an insurance agency would be

1575
01:04:58,080 --> 01:05:04,930
doing it is collecting data

1576
01:05:01,150 --> 01:05:08,079
on all the trucks there's a moving in

1577
01:05:04,930 --> 01:05:10,419
that country and they managed to get 70%

1578
01:05:08,079 --> 01:05:13,119
of the market in markedly short time

1579
01:05:10,420 --> 01:05:16,450
because businesses are very price

1580
01:05:13,119 --> 01:05:20,079
sensitive so when we're thinking about

1581
01:05:16,450 --> 01:05:23,288
this thing it is no longer just us as

1582
01:05:20,079 --> 01:05:26,500
individuals having concern about our

1583
01:05:23,289 --> 01:05:30,819
personal privacy it is also a matter of

1584
01:05:26,500 --> 01:05:35,079
national security and patriotism two

1585
01:05:30,819 --> 01:05:39,038
major data breaches of multinational US

1586
01:05:35,079 --> 01:05:42,130
firms are frequently a thought to have

1587
01:05:39,039 --> 01:05:45,760
been perpetrated by a foreign

1588
01:05:42,130 --> 01:05:48,099
intelligence agency equi fact the

1589
01:05:45,760 --> 01:05:49,690
Equifax and the Marriott breach have

1590
01:05:48,099 --> 01:05:53,289
been attributed to foreign intelligence

1591
01:05:49,690 --> 01:05:56,260
agencies in fact someone Equifax said

1592
01:05:53,289 --> 01:05:59,260
justice just yesterday they seen zero

1593
01:05:56,260 --> 01:06:01,119
evidence that any of the stolen data has

1594
01:05:59,260 --> 01:06:03,190
been used commercially for identity

1595
01:06:01,119 --> 01:06:04,750
theft or anything else which kind of

1596
01:06:03,190 --> 01:06:06,640
goes along pretty well with the notion

1597
01:06:04,750 --> 01:06:10,950
that there's an intelligence agency that

1598
01:06:06,640 --> 01:06:14,200
took it so yeah this is very plausible

1599
01:06:10,950 --> 01:06:16,450
so thank you very much we now have a

1600
01:06:14,200 --> 01:06:18,250
four minute break I think before the

1601
01:06:16,450 --> 01:06:20,680
administrative plenary I'd like to thank

1602
01:06:18,250 --> 01:06:23,140
again very much both Steve and Arvind

1603
01:06:20,680 --> 01:06:26,169
this is an excellent evening I had I

1604
01:06:23,140 --> 01:06:28,058
learned a lot um I especially appreciate

1605
01:06:26,170 --> 01:06:29,589
the challenges to the IETF from both of

1606
01:06:28,059 --> 01:06:33,549
you and we hope to live up to them so

1607
01:06:29,589 --> 01:06:36,029
thank you very much we'll see you in 180

1608
01:06:33,549 --> 01:06:36,029
seconds

1609
01:06:46,260 --> 01:06:48,320
you

